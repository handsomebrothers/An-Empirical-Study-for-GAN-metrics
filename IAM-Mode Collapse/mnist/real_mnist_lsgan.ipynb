{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import util\n",
    "import tensorflow as tf\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "def EuclideanDistances(A, B):\n",
    "\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(),ED.shape[0]*ED.shape[1])\n",
    "def cal_distance_image_real(images,labels):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "def cal_distance_image_fake(images):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    labels = tf.Session().run(tf.argmax(y_logits, 1))\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D loss: 0.394064, acc.: 67.19%] [G loss: 1.276596]\n",
      "epoch:0 step:2 [D loss: 2.043125, acc.: 36.72%] [G loss: 1.187498]\n",
      "epoch:0 step:3 [D loss: 0.361388, acc.: 54.69%] [G loss: 0.890847]\n",
      "epoch:0 step:4 [D loss: 0.214958, acc.: 67.97%] [G loss: 0.847916]\n",
      "epoch:0 step:5 [D loss: 0.250463, acc.: 64.84%] [G loss: 0.879900]\n",
      "epoch:0 step:6 [D loss: 0.160147, acc.: 79.69%] [G loss: 0.777655]\n",
      "epoch:0 step:7 [D loss: 0.136644, acc.: 85.16%] [G loss: 0.862127]\n",
      "epoch:0 step:8 [D loss: 0.109987, acc.: 88.28%] [G loss: 0.900392]\n",
      "epoch:0 step:9 [D loss: 0.118005, acc.: 84.38%] [G loss: 0.928360]\n",
      "epoch:0 step:10 [D loss: 0.104095, acc.: 91.41%] [G loss: 1.012202]\n",
      "epoch:0 step:11 [D loss: 0.094292, acc.: 89.84%] [G loss: 0.906884]\n",
      "epoch:0 step:12 [D loss: 0.111513, acc.: 89.06%] [G loss: 1.030546]\n",
      "epoch:0 step:13 [D loss: 0.099113, acc.: 88.28%] [G loss: 1.043573]\n",
      "epoch:0 step:14 [D loss: 0.139999, acc.: 85.16%] [G loss: 0.994667]\n",
      "epoch:0 step:15 [D loss: 0.171760, acc.: 75.00%] [G loss: 1.072548]\n",
      "epoch:0 step:16 [D loss: 0.370756, acc.: 42.19%] [G loss: 0.908745]\n",
      "epoch:0 step:17 [D loss: 0.763459, acc.: 44.53%] [G loss: 1.136410]\n",
      "epoch:0 step:18 [D loss: 1.024173, acc.: 32.81%] [G loss: 0.833045]\n",
      "epoch:0 step:19 [D loss: 0.486593, acc.: 38.28%] [G loss: 1.178346]\n",
      "epoch:0 step:20 [D loss: 0.104077, acc.: 90.62%] [G loss: 1.025155]\n",
      "epoch:0 step:21 [D loss: 0.117353, acc.: 89.06%] [G loss: 1.179181]\n",
      "epoch:0 step:22 [D loss: 0.098318, acc.: 89.06%] [G loss: 1.012341]\n",
      "epoch:0 step:23 [D loss: 0.066906, acc.: 95.31%] [G loss: 1.016895]\n",
      "epoch:0 step:24 [D loss: 0.091009, acc.: 89.84%] [G loss: 1.056516]\n",
      "epoch:0 step:25 [D loss: 0.090397, acc.: 92.19%] [G loss: 0.956020]\n",
      "epoch:0 step:26 [D loss: 0.116664, acc.: 87.50%] [G loss: 1.068492]\n",
      "epoch:0 step:27 [D loss: 0.144638, acc.: 84.38%] [G loss: 1.099379]\n",
      "epoch:0 step:28 [D loss: 0.078419, acc.: 90.62%] [G loss: 1.241996]\n",
      "epoch:0 step:29 [D loss: 0.081731, acc.: 91.41%] [G loss: 1.003167]\n",
      "epoch:0 step:30 [D loss: 0.102881, acc.: 88.28%] [G loss: 1.053353]\n",
      "epoch:0 step:31 [D loss: 0.084475, acc.: 90.62%] [G loss: 1.440557]\n",
      "epoch:0 step:32 [D loss: 0.093284, acc.: 92.97%] [G loss: 1.092665]\n",
      "epoch:0 step:33 [D loss: 0.075937, acc.: 89.84%] [G loss: 0.951184]\n",
      "epoch:0 step:34 [D loss: 0.074678, acc.: 89.84%] [G loss: 1.057177]\n",
      "epoch:0 step:35 [D loss: 0.099894, acc.: 85.94%] [G loss: 1.112847]\n",
      "epoch:0 step:36 [D loss: 0.088525, acc.: 88.28%] [G loss: 0.971068]\n",
      "epoch:0 step:37 [D loss: 0.109002, acc.: 85.94%] [G loss: 0.952461]\n",
      "epoch:0 step:38 [D loss: 0.087260, acc.: 89.84%] [G loss: 0.944686]\n",
      "epoch:0 step:39 [D loss: 0.078634, acc.: 91.41%] [G loss: 1.015043]\n",
      "epoch:0 step:40 [D loss: 0.104060, acc.: 87.50%] [G loss: 1.133183]\n",
      "epoch:0 step:41 [D loss: 0.081316, acc.: 91.41%] [G loss: 1.084771]\n",
      "epoch:0 step:42 [D loss: 0.073967, acc.: 91.41%] [G loss: 0.971687]\n",
      "epoch:0 step:43 [D loss: 0.103550, acc.: 86.72%] [G loss: 0.950838]\n",
      "epoch:0 step:44 [D loss: 0.081285, acc.: 89.06%] [G loss: 1.028086]\n",
      "epoch:0 step:45 [D loss: 0.076992, acc.: 91.41%] [G loss: 1.195260]\n",
      "epoch:0 step:46 [D loss: 0.103195, acc.: 89.84%] [G loss: 1.131558]\n",
      "epoch:0 step:47 [D loss: 0.071518, acc.: 92.19%] [G loss: 1.011786]\n",
      "epoch:0 step:48 [D loss: 0.078907, acc.: 91.41%] [G loss: 1.074069]\n",
      "epoch:0 step:49 [D loss: 0.089321, acc.: 91.41%] [G loss: 0.964021]\n",
      "epoch:0 step:50 [D loss: 0.115404, acc.: 87.50%] [G loss: 0.977562]\n",
      "epoch:0 step:51 [D loss: 0.071270, acc.: 92.97%] [G loss: 1.111287]\n",
      "epoch:0 step:52 [D loss: 0.102103, acc.: 89.84%] [G loss: 1.045592]\n",
      "epoch:0 step:53 [D loss: 0.124536, acc.: 89.06%] [G loss: 1.141150]\n",
      "epoch:0 step:54 [D loss: 0.093987, acc.: 90.62%] [G loss: 1.062520]\n",
      "epoch:0 step:55 [D loss: 0.112286, acc.: 85.94%] [G loss: 0.922091]\n",
      "epoch:0 step:56 [D loss: 0.108767, acc.: 83.59%] [G loss: 1.123418]\n",
      "epoch:0 step:57 [D loss: 0.080084, acc.: 92.97%] [G loss: 1.017429]\n",
      "epoch:0 step:58 [D loss: 0.109283, acc.: 85.16%] [G loss: 0.938650]\n",
      "epoch:0 step:59 [D loss: 0.103247, acc.: 85.16%] [G loss: 0.906473]\n",
      "epoch:0 step:60 [D loss: 0.061840, acc.: 93.75%] [G loss: 0.934986]\n",
      "epoch:0 step:61 [D loss: 0.062052, acc.: 91.41%] [G loss: 1.042451]\n",
      "epoch:0 step:62 [D loss: 0.058275, acc.: 94.53%] [G loss: 1.025155]\n",
      "epoch:0 step:63 [D loss: 0.099798, acc.: 89.84%] [G loss: 1.100880]\n",
      "epoch:0 step:64 [D loss: 0.105129, acc.: 85.16%] [G loss: 0.976318]\n",
      "epoch:0 step:65 [D loss: 0.092768, acc.: 92.19%] [G loss: 1.260118]\n",
      "epoch:0 step:66 [D loss: 0.082049, acc.: 92.19%] [G loss: 1.014143]\n",
      "epoch:0 step:67 [D loss: 0.081461, acc.: 95.31%] [G loss: 1.038721]\n",
      "epoch:0 step:68 [D loss: 0.063539, acc.: 93.75%] [G loss: 0.979210]\n",
      "epoch:0 step:69 [D loss: 0.053120, acc.: 94.53%] [G loss: 0.981171]\n",
      "epoch:0 step:70 [D loss: 0.080162, acc.: 90.62%] [G loss: 1.090625]\n",
      "epoch:0 step:71 [D loss: 0.098997, acc.: 89.84%] [G loss: 0.975690]\n",
      "epoch:0 step:72 [D loss: 0.100742, acc.: 86.72%] [G loss: 0.975338]\n",
      "epoch:0 step:73 [D loss: 0.091036, acc.: 87.50%] [G loss: 1.163139]\n",
      "epoch:0 step:74 [D loss: 0.128723, acc.: 85.16%] [G loss: 1.056049]\n",
      "epoch:0 step:75 [D loss: 0.167461, acc.: 82.03%] [G loss: 0.828295]\n",
      "epoch:0 step:76 [D loss: 0.196314, acc.: 74.22%] [G loss: 0.654649]\n",
      "epoch:0 step:77 [D loss: 0.099406, acc.: 92.19%] [G loss: 1.032170]\n",
      "epoch:0 step:78 [D loss: 0.099487, acc.: 91.41%] [G loss: 0.955012]\n",
      "epoch:0 step:79 [D loss: 0.079783, acc.: 96.88%] [G loss: 1.002862]\n",
      "epoch:0 step:80 [D loss: 0.093526, acc.: 93.75%] [G loss: 1.090581]\n",
      "epoch:0 step:81 [D loss: 0.075689, acc.: 96.09%] [G loss: 1.196587]\n",
      "epoch:0 step:82 [D loss: 0.051327, acc.: 97.66%] [G loss: 0.833849]\n",
      "epoch:0 step:83 [D loss: 0.062017, acc.: 92.97%] [G loss: 1.128407]\n",
      "epoch:0 step:84 [D loss: 0.062808, acc.: 95.31%] [G loss: 1.093109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:85 [D loss: 0.070844, acc.: 92.19%] [G loss: 1.001496]\n",
      "epoch:0 step:86 [D loss: 0.126322, acc.: 87.50%] [G loss: 0.885132]\n",
      "epoch:0 step:87 [D loss: 0.122390, acc.: 82.81%] [G loss: 0.903748]\n",
      "epoch:0 step:88 [D loss: 0.173703, acc.: 77.34%] [G loss: 0.949748]\n",
      "epoch:0 step:89 [D loss: 0.254989, acc.: 71.09%] [G loss: 1.477472]\n",
      "epoch:0 step:90 [D loss: 0.338487, acc.: 67.97%] [G loss: 0.995543]\n",
      "epoch:0 step:91 [D loss: 0.384140, acc.: 66.41%] [G loss: 0.863377]\n",
      "epoch:0 step:92 [D loss: 0.245592, acc.: 75.00%] [G loss: 0.664815]\n",
      "epoch:0 step:93 [D loss: 0.147238, acc.: 80.47%] [G loss: 0.971815]\n",
      "epoch:0 step:94 [D loss: 0.097720, acc.: 89.06%] [G loss: 0.860874]\n",
      "epoch:0 step:95 [D loss: 0.060567, acc.: 98.44%] [G loss: 1.083434]\n",
      "epoch:0 step:96 [D loss: 0.059065, acc.: 96.88%] [G loss: 1.048466]\n",
      "epoch:0 step:97 [D loss: 0.040200, acc.: 96.09%] [G loss: 1.011873]\n",
      "epoch:0 step:98 [D loss: 0.034540, acc.: 99.22%] [G loss: 0.934219]\n",
      "epoch:0 step:99 [D loss: 0.049864, acc.: 94.53%] [G loss: 0.963029]\n",
      "epoch:0 step:100 [D loss: 0.051136, acc.: 94.53%] [G loss: 0.929234]\n",
      "epoch:0 step:101 [D loss: 0.053444, acc.: 96.09%] [G loss: 1.114479]\n",
      "epoch:0 step:102 [D loss: 0.050204, acc.: 96.88%] [G loss: 0.935205]\n",
      "epoch:0 step:103 [D loss: 0.032469, acc.: 98.44%] [G loss: 1.039127]\n",
      "epoch:0 step:104 [D loss: 0.034797, acc.: 100.00%] [G loss: 0.968761]\n",
      "epoch:0 step:105 [D loss: 0.045216, acc.: 97.66%] [G loss: 1.012086]\n",
      "epoch:0 step:106 [D loss: 0.042586, acc.: 99.22%] [G loss: 1.087719]\n",
      "epoch:0 step:107 [D loss: 0.036189, acc.: 98.44%] [G loss: 1.068708]\n",
      "epoch:0 step:108 [D loss: 0.048119, acc.: 96.09%] [G loss: 0.882339]\n",
      "epoch:0 step:109 [D loss: 0.037652, acc.: 98.44%] [G loss: 0.979468]\n",
      "epoch:0 step:110 [D loss: 0.061344, acc.: 95.31%] [G loss: 0.893360]\n",
      "epoch:0 step:111 [D loss: 0.047093, acc.: 97.66%] [G loss: 0.857167]\n",
      "epoch:0 step:112 [D loss: 0.085643, acc.: 90.62%] [G loss: 0.973212]\n",
      "epoch:0 step:113 [D loss: 0.129563, acc.: 80.47%] [G loss: 1.145396]\n",
      "epoch:0 step:114 [D loss: 0.189112, acc.: 71.88%] [G loss: 1.015496]\n",
      "epoch:0 step:115 [D loss: 0.227938, acc.: 68.75%] [G loss: 1.019857]\n",
      "epoch:0 step:116 [D loss: 0.297892, acc.: 69.53%] [G loss: 0.843669]\n",
      "epoch:0 step:117 [D loss: 0.332579, acc.: 63.28%] [G loss: 0.652110]\n",
      "epoch:0 step:118 [D loss: 0.207953, acc.: 74.22%] [G loss: 0.753732]\n",
      "epoch:0 step:119 [D loss: 0.047282, acc.: 96.88%] [G loss: 1.043662]\n",
      "epoch:0 step:120 [D loss: 0.054339, acc.: 96.88%] [G loss: 0.883069]\n",
      "epoch:0 step:121 [D loss: 0.040995, acc.: 98.44%] [G loss: 1.000039]\n",
      "epoch:0 step:122 [D loss: 0.036995, acc.: 98.44%] [G loss: 0.899786]\n",
      "epoch:0 step:123 [D loss: 0.027088, acc.: 99.22%] [G loss: 0.979936]\n",
      "epoch:0 step:124 [D loss: 0.039106, acc.: 97.66%] [G loss: 1.031602]\n",
      "epoch:0 step:125 [D loss: 0.036985, acc.: 97.66%] [G loss: 1.113243]\n",
      "epoch:0 step:126 [D loss: 0.029492, acc.: 100.00%] [G loss: 0.903731]\n",
      "epoch:0 step:127 [D loss: 0.031198, acc.: 100.00%] [G loss: 0.942962]\n",
      "epoch:0 step:128 [D loss: 0.029463, acc.: 100.00%] [G loss: 0.866434]\n",
      "epoch:0 step:129 [D loss: 0.033028, acc.: 100.00%] [G loss: 1.094229]\n",
      "epoch:0 step:130 [D loss: 0.042729, acc.: 97.66%] [G loss: 0.985080]\n",
      "epoch:0 step:131 [D loss: 0.029998, acc.: 99.22%] [G loss: 0.983504]\n",
      "epoch:0 step:132 [D loss: 0.040239, acc.: 97.66%] [G loss: 0.927967]\n",
      "epoch:0 step:133 [D loss: 0.029905, acc.: 100.00%] [G loss: 0.847910]\n",
      "epoch:0 step:134 [D loss: 0.030254, acc.: 99.22%] [G loss: 0.989619]\n",
      "epoch:0 step:135 [D loss: 0.041738, acc.: 97.66%] [G loss: 0.877152]\n",
      "epoch:0 step:136 [D loss: 0.033887, acc.: 98.44%] [G loss: 1.024043]\n",
      "epoch:0 step:137 [D loss: 0.040834, acc.: 100.00%] [G loss: 0.772137]\n",
      "epoch:0 step:138 [D loss: 0.050448, acc.: 98.44%] [G loss: 1.001377]\n",
      "epoch:0 step:139 [D loss: 0.069679, acc.: 97.66%] [G loss: 1.061047]\n",
      "epoch:0 step:140 [D loss: 0.053862, acc.: 100.00%] [G loss: 1.106228]\n",
      "epoch:0 step:141 [D loss: 0.056357, acc.: 96.09%] [G loss: 0.870140]\n",
      "epoch:0 step:142 [D loss: 0.044456, acc.: 100.00%] [G loss: 1.072553]\n",
      "epoch:0 step:143 [D loss: 0.047922, acc.: 100.00%] [G loss: 0.911073]\n",
      "epoch:0 step:144 [D loss: 0.052963, acc.: 97.66%] [G loss: 0.971282]\n",
      "epoch:0 step:145 [D loss: 0.076596, acc.: 90.62%] [G loss: 1.001956]\n",
      "epoch:0 step:146 [D loss: 0.141867, acc.: 76.56%] [G loss: 0.766467]\n",
      "epoch:0 step:147 [D loss: 0.236856, acc.: 74.22%] [G loss: 0.636158]\n",
      "epoch:0 step:148 [D loss: 0.192308, acc.: 78.12%] [G loss: 0.645143]\n",
      "epoch:0 step:149 [D loss: 0.077543, acc.: 91.41%] [G loss: 0.876128]\n",
      "epoch:0 step:150 [D loss: 0.041995, acc.: 97.66%] [G loss: 0.801594]\n",
      "epoch:0 step:151 [D loss: 0.046085, acc.: 98.44%] [G loss: 0.794603]\n",
      "epoch:0 step:152 [D loss: 0.038624, acc.: 98.44%] [G loss: 0.863720]\n",
      "epoch:0 step:153 [D loss: 0.047679, acc.: 98.44%] [G loss: 0.957221]\n",
      "epoch:0 step:154 [D loss: 0.027634, acc.: 99.22%] [G loss: 0.999184]\n",
      "epoch:0 step:155 [D loss: 0.031669, acc.: 99.22%] [G loss: 0.967286]\n",
      "epoch:0 step:156 [D loss: 0.025116, acc.: 98.44%] [G loss: 0.919631]\n",
      "epoch:0 step:157 [D loss: 0.028962, acc.: 99.22%] [G loss: 1.007197]\n",
      "epoch:0 step:158 [D loss: 0.038658, acc.: 99.22%] [G loss: 0.965827]\n",
      "epoch:0 step:159 [D loss: 0.067106, acc.: 99.22%] [G loss: 0.803709]\n",
      "epoch:0 step:160 [D loss: 0.056091, acc.: 100.00%] [G loss: 0.979817]\n",
      "epoch:0 step:161 [D loss: 0.071321, acc.: 92.97%] [G loss: 0.813218]\n",
      "epoch:0 step:162 [D loss: 0.066074, acc.: 98.44%] [G loss: 1.073542]\n",
      "epoch:0 step:163 [D loss: 0.087156, acc.: 94.53%] [G loss: 0.890787]\n",
      "epoch:0 step:164 [D loss: 0.057971, acc.: 99.22%] [G loss: 1.087577]\n",
      "epoch:0 step:165 [D loss: 0.053427, acc.: 98.44%] [G loss: 0.926745]\n",
      "epoch:0 step:166 [D loss: 0.036456, acc.: 99.22%] [G loss: 0.967733]\n",
      "epoch:0 step:167 [D loss: 0.035535, acc.: 100.00%] [G loss: 0.870342]\n",
      "epoch:0 step:168 [D loss: 0.022990, acc.: 100.00%] [G loss: 0.997953]\n",
      "epoch:0 step:169 [D loss: 0.034018, acc.: 100.00%] [G loss: 0.939458]\n",
      "epoch:0 step:170 [D loss: 0.033680, acc.: 98.44%] [G loss: 0.926966]\n",
      "epoch:0 step:171 [D loss: 0.033522, acc.: 98.44%] [G loss: 1.059546]\n",
      "epoch:0 step:172 [D loss: 0.027740, acc.: 99.22%] [G loss: 1.095240]\n",
      "epoch:0 step:173 [D loss: 0.035219, acc.: 98.44%] [G loss: 0.978898]\n",
      "epoch:0 step:174 [D loss: 0.044035, acc.: 98.44%] [G loss: 1.024450]\n",
      "epoch:0 step:175 [D loss: 0.060845, acc.: 97.66%] [G loss: 0.841168]\n",
      "epoch:0 step:176 [D loss: 0.084496, acc.: 89.06%] [G loss: 0.765160]\n",
      "epoch:0 step:177 [D loss: 0.095490, acc.: 85.94%] [G loss: 1.011245]\n",
      "epoch:0 step:178 [D loss: 0.057027, acc.: 92.19%] [G loss: 1.063047]\n",
      "epoch:0 step:179 [D loss: 0.077369, acc.: 92.97%] [G loss: 0.937866]\n",
      "epoch:0 step:180 [D loss: 0.079093, acc.: 91.41%] [G loss: 1.008371]\n",
      "epoch:0 step:181 [D loss: 0.100397, acc.: 87.50%] [G loss: 0.896028]\n",
      "epoch:0 step:182 [D loss: 0.102020, acc.: 86.72%] [G loss: 0.812769]\n",
      "epoch:0 step:183 [D loss: 0.100040, acc.: 85.94%] [G loss: 0.897846]\n",
      "epoch:0 step:184 [D loss: 0.026825, acc.: 100.00%] [G loss: 0.990965]\n",
      "epoch:0 step:185 [D loss: 0.034664, acc.: 99.22%] [G loss: 0.894009]\n",
      "epoch:0 step:186 [D loss: 0.026252, acc.: 99.22%] [G loss: 0.937004]\n",
      "epoch:0 step:187 [D loss: 0.031242, acc.: 99.22%] [G loss: 0.878174]\n",
      "epoch:0 step:188 [D loss: 0.029395, acc.: 99.22%] [G loss: 0.970371]\n",
      "epoch:0 step:189 [D loss: 0.026439, acc.: 99.22%] [G loss: 0.923460]\n",
      "epoch:0 step:190 [D loss: 0.022965, acc.: 100.00%] [G loss: 0.979277]\n",
      "epoch:0 step:191 [D loss: 0.024046, acc.: 99.22%] [G loss: 0.892301]\n",
      "epoch:0 step:192 [D loss: 0.021944, acc.: 99.22%] [G loss: 1.058697]\n",
      "epoch:0 step:193 [D loss: 0.026141, acc.: 98.44%] [G loss: 0.940393]\n",
      "epoch:0 step:194 [D loss: 0.022421, acc.: 100.00%] [G loss: 0.977228]\n",
      "epoch:0 step:195 [D loss: 0.025438, acc.: 97.66%] [G loss: 1.052484]\n",
      "epoch:0 step:196 [D loss: 0.021263, acc.: 100.00%] [G loss: 0.881224]\n",
      "epoch:0 step:197 [D loss: 0.023366, acc.: 99.22%] [G loss: 0.939806]\n",
      "epoch:0 step:198 [D loss: 0.033609, acc.: 97.66%] [G loss: 0.806713]\n",
      "epoch:0 step:199 [D loss: 0.023029, acc.: 100.00%] [G loss: 1.030585]\n",
      "epoch:0 step:200 [D loss: 0.062991, acc.: 94.53%] [G loss: 0.753826]\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[ 7.88050253  7.59974359 13.25367035 11.80824     9.02500699 11.98333983\n",
      " 11.27914699 10.77839851  9.47523762 10.29102348]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.060375, acc.: 100.00%] [G loss: 1.137801]\n",
      "epoch:0 step:202 [D loss: 0.196748, acc.: 64.06%] [G loss: 0.731908]\n",
      "epoch:0 step:203 [D loss: 0.169371, acc.: 67.19%] [G loss: 1.202554]\n",
      "epoch:0 step:204 [D loss: 0.272716, acc.: 53.91%] [G loss: 0.683808]\n",
      "epoch:0 step:205 [D loss: 0.159003, acc.: 68.75%] [G loss: 1.019059]\n",
      "epoch:0 step:206 [D loss: 0.096903, acc.: 95.31%] [G loss: 0.787873]\n",
      "epoch:0 step:207 [D loss: 0.039472, acc.: 97.66%] [G loss: 1.045727]\n",
      "epoch:0 step:208 [D loss: 0.023851, acc.: 100.00%] [G loss: 0.950552]\n",
      "epoch:0 step:209 [D loss: 0.028771, acc.: 97.66%] [G loss: 0.852569]\n",
      "epoch:0 step:210 [D loss: 0.034208, acc.: 96.09%] [G loss: 0.930938]\n",
      "epoch:0 step:211 [D loss: 0.027797, acc.: 100.00%] [G loss: 0.944287]\n",
      "epoch:0 step:212 [D loss: 0.031121, acc.: 97.66%] [G loss: 0.863395]\n",
      "epoch:0 step:213 [D loss: 0.022691, acc.: 99.22%] [G loss: 0.940557]\n",
      "epoch:0 step:214 [D loss: 0.030163, acc.: 97.66%] [G loss: 0.991055]\n",
      "epoch:0 step:215 [D loss: 0.022736, acc.: 98.44%] [G loss: 0.897566]\n",
      "epoch:0 step:216 [D loss: 0.026487, acc.: 99.22%] [G loss: 0.999871]\n",
      "epoch:0 step:217 [D loss: 0.030060, acc.: 98.44%] [G loss: 0.956071]\n",
      "epoch:0 step:218 [D loss: 0.023091, acc.: 99.22%] [G loss: 0.937068]\n",
      "epoch:0 step:219 [D loss: 0.023307, acc.: 98.44%] [G loss: 0.973116]\n",
      "epoch:0 step:220 [D loss: 0.050678, acc.: 96.88%] [G loss: 0.864531]\n",
      "epoch:0 step:221 [D loss: 0.024800, acc.: 99.22%] [G loss: 0.943877]\n",
      "epoch:0 step:222 [D loss: 0.028127, acc.: 96.88%] [G loss: 0.931669]\n",
      "epoch:0 step:223 [D loss: 0.032517, acc.: 96.88%] [G loss: 0.937562]\n",
      "epoch:0 step:224 [D loss: 0.018337, acc.: 100.00%] [G loss: 0.938186]\n",
      "epoch:0 step:225 [D loss: 0.035094, acc.: 97.66%] [G loss: 0.922116]\n",
      "epoch:0 step:226 [D loss: 0.024054, acc.: 98.44%] [G loss: 1.059175]\n",
      "epoch:0 step:227 [D loss: 0.033536, acc.: 98.44%] [G loss: 0.883707]\n",
      "epoch:0 step:228 [D loss: 0.023117, acc.: 99.22%] [G loss: 0.976322]\n",
      "epoch:0 step:229 [D loss: 0.068636, acc.: 92.97%] [G loss: 0.828250]\n",
      "epoch:0 step:230 [D loss: 0.035544, acc.: 97.66%] [G loss: 0.981082]\n",
      "epoch:0 step:231 [D loss: 0.059105, acc.: 96.88%] [G loss: 0.967255]\n",
      "epoch:0 step:232 [D loss: 0.042379, acc.: 97.66%] [G loss: 1.044466]\n",
      "epoch:0 step:233 [D loss: 0.049331, acc.: 99.22%] [G loss: 0.841420]\n",
      "epoch:0 step:234 [D loss: 0.047512, acc.: 99.22%] [G loss: 1.072980]\n",
      "epoch:0 step:235 [D loss: 0.062350, acc.: 97.66%] [G loss: 0.887955]\n",
      "epoch:0 step:236 [D loss: 0.038454, acc.: 98.44%] [G loss: 0.874514]\n",
      "epoch:0 step:237 [D loss: 0.060369, acc.: 95.31%] [G loss: 0.987866]\n",
      "epoch:0 step:238 [D loss: 0.044665, acc.: 96.88%] [G loss: 1.014145]\n",
      "epoch:0 step:239 [D loss: 0.044059, acc.: 97.66%] [G loss: 0.839853]\n",
      "epoch:0 step:240 [D loss: 0.049659, acc.: 96.88%] [G loss: 0.955685]\n",
      "epoch:0 step:241 [D loss: 0.066730, acc.: 96.88%] [G loss: 0.802327]\n",
      "epoch:0 step:242 [D loss: 0.030470, acc.: 99.22%] [G loss: 0.847162]\n",
      "epoch:0 step:243 [D loss: 0.045611, acc.: 96.09%] [G loss: 0.861967]\n",
      "epoch:0 step:244 [D loss: 0.025787, acc.: 98.44%] [G loss: 1.001479]\n",
      "epoch:0 step:245 [D loss: 0.068973, acc.: 91.41%] [G loss: 0.966240]\n",
      "epoch:0 step:246 [D loss: 0.035481, acc.: 96.88%] [G loss: 0.908207]\n",
      "epoch:0 step:247 [D loss: 0.025938, acc.: 98.44%] [G loss: 0.902305]\n",
      "epoch:0 step:248 [D loss: 0.032271, acc.: 97.66%] [G loss: 0.862572]\n",
      "epoch:0 step:249 [D loss: 0.031066, acc.: 96.88%] [G loss: 0.817016]\n",
      "epoch:0 step:250 [D loss: 0.037487, acc.: 96.88%] [G loss: 0.872232]\n",
      "epoch:0 step:251 [D loss: 0.050745, acc.: 96.09%] [G loss: 0.904470]\n",
      "epoch:0 step:252 [D loss: 0.046601, acc.: 96.09%] [G loss: 0.945963]\n",
      "epoch:0 step:253 [D loss: 0.028677, acc.: 96.88%] [G loss: 1.012835]\n",
      "epoch:0 step:254 [D loss: 0.040333, acc.: 95.31%] [G loss: 0.935948]\n",
      "epoch:0 step:255 [D loss: 0.034204, acc.: 96.09%] [G loss: 0.927762]\n",
      "epoch:0 step:256 [D loss: 0.051164, acc.: 93.75%] [G loss: 0.857380]\n",
      "epoch:0 step:257 [D loss: 0.038153, acc.: 94.53%] [G loss: 0.924720]\n",
      "epoch:0 step:258 [D loss: 0.071785, acc.: 90.62%] [G loss: 0.806179]\n",
      "epoch:0 step:259 [D loss: 0.035071, acc.: 96.88%] [G loss: 0.973611]\n",
      "epoch:0 step:260 [D loss: 0.123124, acc.: 84.38%] [G loss: 0.795665]\n",
      "epoch:0 step:261 [D loss: 0.083074, acc.: 96.09%] [G loss: 1.181604]\n",
      "epoch:0 step:262 [D loss: 0.234616, acc.: 55.47%] [G loss: 0.667851]\n",
      "epoch:0 step:263 [D loss: 0.165939, acc.: 70.31%] [G loss: 1.120192]\n",
      "epoch:0 step:264 [D loss: 0.196662, acc.: 68.75%] [G loss: 0.725766]\n",
      "epoch:0 step:265 [D loss: 0.059459, acc.: 94.53%] [G loss: 0.829094]\n",
      "epoch:0 step:266 [D loss: 0.047269, acc.: 95.31%] [G loss: 0.921869]\n",
      "epoch:0 step:267 [D loss: 0.046964, acc.: 94.53%] [G loss: 0.893263]\n",
      "epoch:0 step:268 [D loss: 0.048323, acc.: 96.88%] [G loss: 0.906752]\n",
      "epoch:0 step:269 [D loss: 0.066588, acc.: 89.84%] [G loss: 0.888675]\n",
      "epoch:0 step:270 [D loss: 0.049326, acc.: 92.97%] [G loss: 0.873144]\n",
      "epoch:0 step:271 [D loss: 0.055654, acc.: 92.19%] [G loss: 0.844582]\n",
      "epoch:0 step:272 [D loss: 0.039454, acc.: 96.09%] [G loss: 0.857915]\n",
      "epoch:0 step:273 [D loss: 0.051970, acc.: 95.31%] [G loss: 0.812695]\n",
      "epoch:0 step:274 [D loss: 0.040194, acc.: 95.31%] [G loss: 0.890918]\n",
      "epoch:0 step:275 [D loss: 0.054510, acc.: 92.97%] [G loss: 0.901643]\n",
      "epoch:0 step:276 [D loss: 0.034600, acc.: 97.66%] [G loss: 0.845965]\n",
      "epoch:0 step:277 [D loss: 0.050217, acc.: 93.75%] [G loss: 0.909940]\n",
      "epoch:0 step:278 [D loss: 0.047513, acc.: 95.31%] [G loss: 0.845267]\n",
      "epoch:0 step:279 [D loss: 0.054526, acc.: 94.53%] [G loss: 0.876259]\n",
      "epoch:0 step:280 [D loss: 0.043218, acc.: 95.31%] [G loss: 0.907293]\n",
      "epoch:0 step:281 [D loss: 0.094964, acc.: 85.94%] [G loss: 0.832891]\n",
      "epoch:0 step:282 [D loss: 0.044574, acc.: 96.09%] [G loss: 0.864425]\n",
      "epoch:0 step:283 [D loss: 0.110092, acc.: 88.28%] [G loss: 0.715745]\n",
      "epoch:0 step:284 [D loss: 0.039250, acc.: 96.88%] [G loss: 0.969611]\n",
      "epoch:0 step:285 [D loss: 0.115408, acc.: 83.59%] [G loss: 0.786756]\n",
      "epoch:0 step:286 [D loss: 0.057502, acc.: 93.75%] [G loss: 0.943574]\n",
      "epoch:0 step:287 [D loss: 0.123353, acc.: 82.81%] [G loss: 0.703264]\n",
      "epoch:0 step:288 [D loss: 0.053526, acc.: 92.97%] [G loss: 0.956582]\n",
      "epoch:0 step:289 [D loss: 0.098742, acc.: 88.28%] [G loss: 0.731699]\n",
      "epoch:0 step:290 [D loss: 0.053014, acc.: 96.88%] [G loss: 0.994289]\n",
      "epoch:0 step:291 [D loss: 0.091961, acc.: 88.28%] [G loss: 0.773988]\n",
      "epoch:0 step:292 [D loss: 0.098376, acc.: 87.50%] [G loss: 0.918464]\n",
      "epoch:0 step:293 [D loss: 0.112510, acc.: 85.16%] [G loss: 0.748908]\n",
      "epoch:0 step:294 [D loss: 0.042123, acc.: 93.75%] [G loss: 0.935398]\n",
      "epoch:0 step:295 [D loss: 0.092469, acc.: 87.50%] [G loss: 0.812567]\n",
      "epoch:0 step:296 [D loss: 0.081951, acc.: 89.84%] [G loss: 0.954661]\n",
      "epoch:0 step:297 [D loss: 0.067459, acc.: 95.31%] [G loss: 0.885554]\n",
      "epoch:0 step:298 [D loss: 0.051258, acc.: 94.53%] [G loss: 0.996439]\n",
      "epoch:0 step:299 [D loss: 0.153179, acc.: 82.03%] [G loss: 0.717942]\n",
      "epoch:0 step:300 [D loss: 0.068313, acc.: 93.75%] [G loss: 1.012414]\n",
      "epoch:0 step:301 [D loss: 0.217000, acc.: 70.31%] [G loss: 0.619551]\n",
      "epoch:0 step:302 [D loss: 0.098662, acc.: 83.59%] [G loss: 0.934265]\n",
      "epoch:0 step:303 [D loss: 0.145942, acc.: 82.03%] [G loss: 0.782229]\n",
      "epoch:0 step:304 [D loss: 0.100608, acc.: 88.28%] [G loss: 0.812757]\n",
      "epoch:0 step:305 [D loss: 0.119169, acc.: 88.28%] [G loss: 0.986052]\n",
      "epoch:0 step:306 [D loss: 0.076225, acc.: 91.41%] [G loss: 0.967113]\n",
      "epoch:0 step:307 [D loss: 0.056058, acc.: 91.41%] [G loss: 0.884504]\n",
      "epoch:0 step:308 [D loss: 0.112212, acc.: 87.50%] [G loss: 0.797544]\n",
      "epoch:0 step:309 [D loss: 0.069658, acc.: 89.06%] [G loss: 0.837915]\n",
      "epoch:0 step:310 [D loss: 0.109682, acc.: 87.50%] [G loss: 0.768570]\n",
      "epoch:0 step:311 [D loss: 0.058877, acc.: 92.19%] [G loss: 0.867251]\n",
      "epoch:0 step:312 [D loss: 0.120988, acc.: 83.59%] [G loss: 0.840509]\n",
      "epoch:0 step:313 [D loss: 0.062811, acc.: 87.50%] [G loss: 0.928023]\n",
      "epoch:0 step:314 [D loss: 0.102739, acc.: 88.28%] [G loss: 0.765875]\n",
      "epoch:0 step:315 [D loss: 0.040595, acc.: 92.97%] [G loss: 0.898366]\n",
      "epoch:0 step:316 [D loss: 0.104071, acc.: 89.84%] [G loss: 0.764193]\n",
      "epoch:0 step:317 [D loss: 0.036809, acc.: 96.09%] [G loss: 0.983322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:318 [D loss: 0.094507, acc.: 85.16%] [G loss: 0.735082]\n",
      "epoch:0 step:319 [D loss: 0.083810, acc.: 85.16%] [G loss: 0.890853]\n",
      "epoch:0 step:320 [D loss: 0.070266, acc.: 90.62%] [G loss: 0.889535]\n",
      "epoch:0 step:321 [D loss: 0.059037, acc.: 89.84%] [G loss: 0.781754]\n",
      "epoch:0 step:322 [D loss: 0.079054, acc.: 87.50%] [G loss: 0.840330]\n",
      "epoch:0 step:323 [D loss: 0.077891, acc.: 91.41%] [G loss: 0.886175]\n",
      "epoch:0 step:324 [D loss: 0.055181, acc.: 89.84%] [G loss: 0.955115]\n",
      "epoch:0 step:325 [D loss: 0.096903, acc.: 90.62%] [G loss: 0.720278]\n",
      "epoch:0 step:326 [D loss: 0.077445, acc.: 88.28%] [G loss: 0.982658]\n",
      "epoch:0 step:327 [D loss: 0.143362, acc.: 78.12%] [G loss: 0.755494]\n",
      "epoch:0 step:328 [D loss: 0.068749, acc.: 88.28%] [G loss: 0.851471]\n",
      "epoch:0 step:329 [D loss: 0.102724, acc.: 85.16%] [G loss: 0.783758]\n",
      "epoch:0 step:330 [D loss: 0.093742, acc.: 82.03%] [G loss: 0.930415]\n",
      "epoch:0 step:331 [D loss: 0.115190, acc.: 82.81%] [G loss: 0.827893]\n",
      "epoch:0 step:332 [D loss: 0.059555, acc.: 91.41%] [G loss: 0.923632]\n",
      "epoch:0 step:333 [D loss: 0.129212, acc.: 82.81%] [G loss: 0.713029]\n",
      "epoch:0 step:334 [D loss: 0.058249, acc.: 93.75%] [G loss: 1.051179]\n",
      "epoch:0 step:335 [D loss: 0.122896, acc.: 82.03%] [G loss: 0.747031]\n",
      "epoch:0 step:336 [D loss: 0.095028, acc.: 85.94%] [G loss: 0.756285]\n",
      "epoch:0 step:337 [D loss: 0.111757, acc.: 86.72%] [G loss: 0.823611]\n",
      "epoch:0 step:338 [D loss: 0.050678, acc.: 89.84%] [G loss: 0.851002]\n",
      "epoch:0 step:339 [D loss: 0.114879, acc.: 85.94%] [G loss: 0.747100]\n",
      "epoch:0 step:340 [D loss: 0.072171, acc.: 85.16%] [G loss: 0.885243]\n",
      "epoch:0 step:341 [D loss: 0.202094, acc.: 67.97%] [G loss: 0.674079]\n",
      "epoch:0 step:342 [D loss: 0.062448, acc.: 90.62%] [G loss: 0.988051]\n",
      "epoch:0 step:343 [D loss: 0.168618, acc.: 73.44%] [G loss: 0.754765]\n",
      "epoch:0 step:344 [D loss: 0.067172, acc.: 89.06%] [G loss: 0.955897]\n",
      "epoch:0 step:345 [D loss: 0.161233, acc.: 74.22%] [G loss: 0.754327]\n",
      "epoch:0 step:346 [D loss: 0.064504, acc.: 88.28%] [G loss: 0.952520]\n",
      "epoch:0 step:347 [D loss: 0.094987, acc.: 89.84%] [G loss: 0.854482]\n",
      "epoch:0 step:348 [D loss: 0.078349, acc.: 86.72%] [G loss: 0.948581]\n",
      "epoch:0 step:349 [D loss: 0.082011, acc.: 89.06%] [G loss: 0.859848]\n",
      "epoch:0 step:350 [D loss: 0.056116, acc.: 91.41%] [G loss: 0.861612]\n",
      "epoch:0 step:351 [D loss: 0.082740, acc.: 87.50%] [G loss: 0.858861]\n",
      "epoch:0 step:352 [D loss: 0.066853, acc.: 89.06%] [G loss: 0.820211]\n",
      "epoch:0 step:353 [D loss: 0.100308, acc.: 82.03%] [G loss: 0.742501]\n",
      "epoch:0 step:354 [D loss: 0.099334, acc.: 85.16%] [G loss: 0.834511]\n",
      "epoch:0 step:355 [D loss: 0.094515, acc.: 82.81%] [G loss: 0.784557]\n",
      "epoch:0 step:356 [D loss: 0.079911, acc.: 87.50%] [G loss: 0.908881]\n",
      "epoch:0 step:357 [D loss: 0.072544, acc.: 89.84%] [G loss: 0.872585]\n",
      "epoch:0 step:358 [D loss: 0.082907, acc.: 85.16%] [G loss: 0.832840]\n",
      "epoch:0 step:359 [D loss: 0.062188, acc.: 90.62%] [G loss: 0.948287]\n",
      "epoch:0 step:360 [D loss: 0.086327, acc.: 88.28%] [G loss: 0.828955]\n",
      "epoch:0 step:361 [D loss: 0.105693, acc.: 82.81%] [G loss: 0.847913]\n",
      "epoch:0 step:362 [D loss: 0.107688, acc.: 82.81%] [G loss: 0.784078]\n",
      "epoch:0 step:363 [D loss: 0.061140, acc.: 91.41%] [G loss: 0.835639]\n",
      "epoch:0 step:364 [D loss: 0.099400, acc.: 85.16%] [G loss: 0.790902]\n",
      "epoch:0 step:365 [D loss: 0.083537, acc.: 82.03%] [G loss: 0.893096]\n",
      "epoch:0 step:366 [D loss: 0.117625, acc.: 82.03%] [G loss: 0.746471]\n",
      "epoch:0 step:367 [D loss: 0.053947, acc.: 94.53%] [G loss: 0.943098]\n",
      "epoch:0 step:368 [D loss: 0.098062, acc.: 82.03%] [G loss: 0.840311]\n",
      "epoch:0 step:369 [D loss: 0.073645, acc.: 89.06%] [G loss: 0.868962]\n",
      "epoch:0 step:370 [D loss: 0.062989, acc.: 89.06%] [G loss: 0.803987]\n",
      "epoch:0 step:371 [D loss: 0.130446, acc.: 81.25%] [G loss: 0.883659]\n",
      "epoch:0 step:372 [D loss: 0.080138, acc.: 84.38%] [G loss: 0.933650]\n",
      "epoch:0 step:373 [D loss: 0.113053, acc.: 82.81%] [G loss: 0.715384]\n",
      "epoch:0 step:374 [D loss: 0.059012, acc.: 92.97%] [G loss: 1.016403]\n",
      "epoch:0 step:375 [D loss: 0.178579, acc.: 75.78%] [G loss: 0.846808]\n",
      "epoch:0 step:376 [D loss: 0.070534, acc.: 95.31%] [G loss: 1.146825]\n",
      "epoch:0 step:377 [D loss: 0.203825, acc.: 63.28%] [G loss: 0.644578]\n",
      "epoch:0 step:378 [D loss: 0.082929, acc.: 94.53%] [G loss: 1.292802]\n",
      "epoch:0 step:379 [D loss: 0.193153, acc.: 67.97%] [G loss: 0.820577]\n",
      "epoch:0 step:380 [D loss: 0.065397, acc.: 85.16%] [G loss: 0.890518]\n",
      "epoch:0 step:381 [D loss: 0.063203, acc.: 96.88%] [G loss: 0.928071]\n",
      "epoch:0 step:382 [D loss: 0.074422, acc.: 90.62%] [G loss: 0.808312]\n",
      "epoch:0 step:383 [D loss: 0.086139, acc.: 89.06%] [G loss: 0.781961]\n",
      "epoch:0 step:384 [D loss: 0.084635, acc.: 86.72%] [G loss: 0.890637]\n",
      "epoch:0 step:385 [D loss: 0.079048, acc.: 88.28%] [G loss: 0.818832]\n",
      "epoch:0 step:386 [D loss: 0.052377, acc.: 93.75%] [G loss: 0.848181]\n",
      "epoch:0 step:387 [D loss: 0.070112, acc.: 91.41%] [G loss: 0.913601]\n",
      "epoch:0 step:388 [D loss: 0.054534, acc.: 90.62%] [G loss: 0.861734]\n",
      "epoch:0 step:389 [D loss: 0.084896, acc.: 90.62%] [G loss: 0.819186]\n",
      "epoch:0 step:390 [D loss: 0.065372, acc.: 92.97%] [G loss: 1.073440]\n",
      "epoch:0 step:391 [D loss: 0.053349, acc.: 96.09%] [G loss: 0.719010]\n",
      "epoch:0 step:392 [D loss: 0.062216, acc.: 97.66%] [G loss: 1.003998]\n",
      "epoch:0 step:393 [D loss: 0.207239, acc.: 65.62%] [G loss: 0.768401]\n",
      "epoch:0 step:394 [D loss: 0.036306, acc.: 100.00%] [G loss: 1.112023]\n",
      "epoch:0 step:395 [D loss: 0.155734, acc.: 77.34%] [G loss: 0.690333]\n",
      "epoch:0 step:396 [D loss: 0.052444, acc.: 94.53%] [G loss: 1.006263]\n",
      "epoch:0 step:397 [D loss: 0.072523, acc.: 99.22%] [G loss: 0.857870]\n",
      "epoch:0 step:398 [D loss: 0.093201, acc.: 93.75%] [G loss: 0.893837]\n",
      "epoch:0 step:399 [D loss: 0.048559, acc.: 96.88%] [G loss: 0.980760]\n",
      "epoch:0 step:400 [D loss: 0.115780, acc.: 83.59%] [G loss: 0.868655]\n",
      "##############\n",
      "[ 8.27053875  7.59974359 13.25367035 10.13752535  9.50549665 11.98333983\n",
      " 11.27914699 10.08946663  9.56312182 10.29102348]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.047268, acc.: 96.09%] [G loss: 1.032305]\n",
      "epoch:0 step:402 [D loss: 0.079297, acc.: 90.62%] [G loss: 0.812227]\n",
      "epoch:0 step:403 [D loss: 0.065008, acc.: 90.62%] [G loss: 0.995057]\n",
      "epoch:0 step:404 [D loss: 0.103095, acc.: 82.81%] [G loss: 0.936865]\n",
      "epoch:0 step:405 [D loss: 0.041869, acc.: 96.88%] [G loss: 1.036552]\n",
      "epoch:0 step:406 [D loss: 0.121155, acc.: 78.91%] [G loss: 0.890068]\n",
      "epoch:0 step:407 [D loss: 0.043926, acc.: 100.00%] [G loss: 0.982813]\n",
      "epoch:0 step:408 [D loss: 0.129583, acc.: 80.47%] [G loss: 0.942163]\n",
      "epoch:0 step:409 [D loss: 0.039358, acc.: 100.00%] [G loss: 1.009604]\n",
      "epoch:0 step:410 [D loss: 0.118602, acc.: 87.50%] [G loss: 0.875882]\n",
      "epoch:0 step:411 [D loss: 0.058087, acc.: 93.75%] [G loss: 0.907633]\n",
      "epoch:0 step:412 [D loss: 0.119196, acc.: 86.72%] [G loss: 0.950325]\n",
      "epoch:0 step:413 [D loss: 0.047120, acc.: 93.75%] [G loss: 0.974121]\n",
      "epoch:0 step:414 [D loss: 0.094971, acc.: 85.16%] [G loss: 0.823304]\n",
      "epoch:0 step:415 [D loss: 0.070998, acc.: 92.19%] [G loss: 0.940060]\n",
      "epoch:0 step:416 [D loss: 0.093371, acc.: 91.41%] [G loss: 0.881345]\n",
      "epoch:0 step:417 [D loss: 0.061331, acc.: 90.62%] [G loss: 1.080611]\n",
      "epoch:0 step:418 [D loss: 0.123634, acc.: 78.91%] [G loss: 0.836720]\n",
      "epoch:0 step:419 [D loss: 0.066371, acc.: 90.62%] [G loss: 0.900178]\n",
      "epoch:0 step:420 [D loss: 0.068838, acc.: 96.09%] [G loss: 0.877173]\n",
      "epoch:0 step:421 [D loss: 0.100541, acc.: 82.03%] [G loss: 0.874317]\n",
      "epoch:0 step:422 [D loss: 0.070047, acc.: 92.97%] [G loss: 0.903990]\n",
      "epoch:0 step:423 [D loss: 0.067024, acc.: 90.62%] [G loss: 0.937770]\n",
      "epoch:0 step:424 [D loss: 0.108268, acc.: 82.81%] [G loss: 0.869121]\n",
      "epoch:0 step:425 [D loss: 0.093063, acc.: 85.16%] [G loss: 0.785523]\n",
      "epoch:0 step:426 [D loss: 0.071581, acc.: 96.88%] [G loss: 0.971484]\n",
      "epoch:0 step:427 [D loss: 0.111724, acc.: 80.47%] [G loss: 0.858101]\n",
      "epoch:0 step:428 [D loss: 0.057839, acc.: 97.66%] [G loss: 1.074691]\n",
      "epoch:0 step:429 [D loss: 0.084529, acc.: 89.06%] [G loss: 0.837411]\n",
      "epoch:0 step:430 [D loss: 0.081230, acc.: 86.72%] [G loss: 0.981206]\n",
      "epoch:0 step:431 [D loss: 0.084883, acc.: 89.84%] [G loss: 0.890781]\n",
      "epoch:0 step:432 [D loss: 0.042016, acc.: 96.09%] [G loss: 1.024742]\n",
      "epoch:0 step:433 [D loss: 0.097762, acc.: 86.72%] [G loss: 0.847400]\n",
      "epoch:0 step:434 [D loss: 0.055595, acc.: 92.97%] [G loss: 1.109603]\n",
      "epoch:0 step:435 [D loss: 0.143134, acc.: 80.47%] [G loss: 0.841779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:436 [D loss: 0.043642, acc.: 100.00%] [G loss: 0.993621]\n",
      "epoch:0 step:437 [D loss: 0.158502, acc.: 78.12%] [G loss: 0.810969]\n",
      "epoch:0 step:438 [D loss: 0.036087, acc.: 99.22%] [G loss: 1.087070]\n",
      "epoch:0 step:439 [D loss: 0.144976, acc.: 79.69%] [G loss: 0.788410]\n",
      "epoch:0 step:440 [D loss: 0.056598, acc.: 96.09%] [G loss: 0.983879]\n",
      "epoch:0 step:441 [D loss: 0.157409, acc.: 75.78%] [G loss: 0.839864]\n",
      "epoch:0 step:442 [D loss: 0.049674, acc.: 98.44%] [G loss: 1.077475]\n",
      "epoch:0 step:443 [D loss: 0.123513, acc.: 81.25%] [G loss: 0.721986]\n",
      "epoch:0 step:444 [D loss: 0.056888, acc.: 95.31%] [G loss: 0.958360]\n",
      "epoch:0 step:445 [D loss: 0.082224, acc.: 96.88%] [G loss: 0.855983]\n",
      "epoch:0 step:446 [D loss: 0.076326, acc.: 89.06%] [G loss: 0.971661]\n",
      "epoch:0 step:447 [D loss: 0.050291, acc.: 100.00%] [G loss: 0.933890]\n",
      "epoch:0 step:448 [D loss: 0.107430, acc.: 82.81%] [G loss: 0.716665]\n",
      "epoch:0 step:449 [D loss: 0.053996, acc.: 99.22%] [G loss: 1.027480]\n",
      "epoch:0 step:450 [D loss: 0.098906, acc.: 90.62%] [G loss: 0.892702]\n",
      "epoch:0 step:451 [D loss: 0.048427, acc.: 100.00%] [G loss: 0.978803]\n",
      "epoch:0 step:452 [D loss: 0.053589, acc.: 99.22%] [G loss: 0.885775]\n",
      "epoch:0 step:453 [D loss: 0.061073, acc.: 95.31%] [G loss: 0.935724]\n",
      "epoch:0 step:454 [D loss: 0.060692, acc.: 98.44%] [G loss: 0.953863]\n",
      "epoch:0 step:455 [D loss: 0.064557, acc.: 96.09%] [G loss: 1.152740]\n",
      "epoch:0 step:456 [D loss: 0.108147, acc.: 86.72%] [G loss: 0.873657]\n",
      "epoch:0 step:457 [D loss: 0.040188, acc.: 100.00%] [G loss: 1.058201]\n",
      "epoch:0 step:458 [D loss: 0.128246, acc.: 84.38%] [G loss: 0.734317]\n",
      "epoch:0 step:459 [D loss: 0.036264, acc.: 98.44%] [G loss: 1.098007]\n",
      "epoch:0 step:460 [D loss: 0.115026, acc.: 85.16%] [G loss: 0.787310]\n",
      "epoch:0 step:461 [D loss: 0.034742, acc.: 100.00%] [G loss: 1.100371]\n",
      "epoch:0 step:462 [D loss: 0.179326, acc.: 71.88%] [G loss: 0.942987]\n",
      "epoch:0 step:463 [D loss: 0.038449, acc.: 100.00%] [G loss: 0.786988]\n",
      "epoch:0 step:464 [D loss: 0.111173, acc.: 83.59%] [G loss: 0.863617]\n",
      "epoch:0 step:465 [D loss: 0.049101, acc.: 96.88%] [G loss: 0.988255]\n",
      "epoch:0 step:466 [D loss: 0.053657, acc.: 92.97%] [G loss: 0.916436]\n",
      "epoch:0 step:467 [D loss: 0.119838, acc.: 81.25%] [G loss: 1.057065]\n",
      "epoch:0 step:468 [D loss: 0.066262, acc.: 94.53%] [G loss: 0.939043]\n",
      "epoch:0 step:469 [D loss: 0.070337, acc.: 96.09%] [G loss: 0.923463]\n",
      "epoch:0 step:470 [D loss: 0.082851, acc.: 92.19%] [G loss: 0.932598]\n",
      "epoch:0 step:471 [D loss: 0.053124, acc.: 98.44%] [G loss: 1.015847]\n",
      "epoch:0 step:472 [D loss: 0.058942, acc.: 96.88%] [G loss: 0.951555]\n",
      "epoch:0 step:473 [D loss: 0.069825, acc.: 89.06%] [G loss: 0.946950]\n",
      "epoch:0 step:474 [D loss: 0.068769, acc.: 89.06%] [G loss: 0.918942]\n",
      "epoch:0 step:475 [D loss: 0.087117, acc.: 85.16%] [G loss: 0.918718]\n",
      "epoch:0 step:476 [D loss: 0.092917, acc.: 87.50%] [G loss: 0.942468]\n",
      "epoch:0 step:477 [D loss: 0.053405, acc.: 98.44%] [G loss: 0.921155]\n",
      "epoch:0 step:478 [D loss: 0.114435, acc.: 82.03%] [G loss: 0.885157]\n",
      "epoch:0 step:479 [D loss: 0.059663, acc.: 96.09%] [G loss: 0.923934]\n",
      "epoch:0 step:480 [D loss: 0.062492, acc.: 94.53%] [G loss: 0.924032]\n",
      "epoch:0 step:481 [D loss: 0.069349, acc.: 92.19%] [G loss: 1.066222]\n",
      "epoch:0 step:482 [D loss: 0.114830, acc.: 85.16%] [G loss: 0.815918]\n",
      "epoch:0 step:483 [D loss: 0.048362, acc.: 96.09%] [G loss: 1.054201]\n",
      "epoch:0 step:484 [D loss: 0.090890, acc.: 88.28%] [G loss: 0.894672]\n",
      "epoch:0 step:485 [D loss: 0.055961, acc.: 96.09%] [G loss: 1.082337]\n",
      "epoch:0 step:486 [D loss: 0.099189, acc.: 85.16%] [G loss: 0.948654]\n",
      "epoch:0 step:487 [D loss: 0.069694, acc.: 99.22%] [G loss: 0.985087]\n",
      "epoch:0 step:488 [D loss: 0.059642, acc.: 96.09%] [G loss: 0.881922]\n",
      "epoch:0 step:489 [D loss: 0.082585, acc.: 85.16%] [G loss: 0.971474]\n",
      "epoch:0 step:490 [D loss: 0.072795, acc.: 90.62%] [G loss: 1.084561]\n",
      "epoch:0 step:491 [D loss: 0.057515, acc.: 95.31%] [G loss: 0.967852]\n",
      "epoch:0 step:492 [D loss: 0.073869, acc.: 93.75%] [G loss: 1.175442]\n",
      "epoch:0 step:493 [D loss: 0.071162, acc.: 94.53%] [G loss: 0.882275]\n",
      "epoch:0 step:494 [D loss: 0.056273, acc.: 93.75%] [G loss: 1.015102]\n",
      "epoch:0 step:495 [D loss: 0.138863, acc.: 75.00%] [G loss: 0.957684]\n",
      "epoch:0 step:496 [D loss: 0.062770, acc.: 97.66%] [G loss: 1.027405]\n",
      "epoch:0 step:497 [D loss: 0.071901, acc.: 94.53%] [G loss: 0.966089]\n",
      "epoch:0 step:498 [D loss: 0.050624, acc.: 97.66%] [G loss: 0.900250]\n",
      "epoch:0 step:499 [D loss: 0.073289, acc.: 91.41%] [G loss: 1.201493]\n",
      "epoch:0 step:500 [D loss: 0.086412, acc.: 88.28%] [G loss: 0.970204]\n",
      "epoch:0 step:501 [D loss: 0.107061, acc.: 86.72%] [G loss: 1.391172]\n",
      "epoch:0 step:502 [D loss: 0.125517, acc.: 88.28%] [G loss: 1.029238]\n",
      "epoch:0 step:503 [D loss: 0.087702, acc.: 96.09%] [G loss: 0.932105]\n",
      "epoch:0 step:504 [D loss: 0.131922, acc.: 82.81%] [G loss: 0.983021]\n",
      "epoch:0 step:505 [D loss: 0.048938, acc.: 100.00%] [G loss: 0.867291]\n",
      "epoch:0 step:506 [D loss: 0.138864, acc.: 79.69%] [G loss: 0.913296]\n",
      "epoch:0 step:507 [D loss: 0.066165, acc.: 97.66%] [G loss: 1.017524]\n",
      "epoch:0 step:508 [D loss: 0.099973, acc.: 88.28%] [G loss: 0.906978]\n",
      "epoch:0 step:509 [D loss: 0.082376, acc.: 83.59%] [G loss: 1.007479]\n",
      "epoch:0 step:510 [D loss: 0.109630, acc.: 85.94%] [G loss: 0.968420]\n",
      "epoch:0 step:511 [D loss: 0.061825, acc.: 100.00%] [G loss: 0.896100]\n",
      "epoch:0 step:512 [D loss: 0.120169, acc.: 79.69%] [G loss: 0.943122]\n",
      "epoch:0 step:513 [D loss: 0.060211, acc.: 100.00%] [G loss: 0.993613]\n",
      "epoch:0 step:514 [D loss: 0.111222, acc.: 91.41%] [G loss: 0.881738]\n",
      "epoch:0 step:515 [D loss: 0.057334, acc.: 100.00%] [G loss: 0.997324]\n",
      "epoch:0 step:516 [D loss: 0.096556, acc.: 90.62%] [G loss: 0.944810]\n",
      "epoch:0 step:517 [D loss: 0.095340, acc.: 89.84%] [G loss: 0.931591]\n",
      "epoch:0 step:518 [D loss: 0.037644, acc.: 100.00%] [G loss: 1.004385]\n",
      "epoch:0 step:519 [D loss: 0.076399, acc.: 96.09%] [G loss: 0.868313]\n",
      "epoch:0 step:520 [D loss: 0.054651, acc.: 99.22%] [G loss: 0.991130]\n",
      "epoch:0 step:521 [D loss: 0.096443, acc.: 96.09%] [G loss: 0.904809]\n",
      "epoch:0 step:522 [D loss: 0.059758, acc.: 96.09%] [G loss: 1.021289]\n",
      "epoch:0 step:523 [D loss: 0.064652, acc.: 97.66%] [G loss: 0.922586]\n",
      "epoch:0 step:524 [D loss: 0.044522, acc.: 100.00%] [G loss: 1.000900]\n",
      "epoch:0 step:525 [D loss: 0.078780, acc.: 95.31%] [G loss: 0.908183]\n",
      "epoch:0 step:526 [D loss: 0.039256, acc.: 100.00%] [G loss: 0.986182]\n",
      "epoch:0 step:527 [D loss: 0.117675, acc.: 83.59%] [G loss: 0.983538]\n",
      "epoch:0 step:528 [D loss: 0.046061, acc.: 99.22%] [G loss: 0.955155]\n",
      "epoch:0 step:529 [D loss: 0.072555, acc.: 97.66%] [G loss: 0.846213]\n",
      "epoch:0 step:530 [D loss: 0.053238, acc.: 100.00%] [G loss: 1.040957]\n",
      "epoch:0 step:531 [D loss: 0.124910, acc.: 79.69%] [G loss: 0.865782]\n",
      "epoch:0 step:532 [D loss: 0.046825, acc.: 100.00%] [G loss: 0.997568]\n",
      "epoch:0 step:533 [D loss: 0.092866, acc.: 92.97%] [G loss: 0.865156]\n",
      "epoch:0 step:534 [D loss: 0.040440, acc.: 100.00%] [G loss: 1.010634]\n",
      "epoch:0 step:535 [D loss: 0.080681, acc.: 93.75%] [G loss: 0.969193]\n",
      "epoch:0 step:536 [D loss: 0.038527, acc.: 100.00%] [G loss: 1.065820]\n",
      "epoch:0 step:537 [D loss: 0.087655, acc.: 96.88%] [G loss: 0.928449]\n",
      "epoch:0 step:538 [D loss: 0.045616, acc.: 98.44%] [G loss: 0.986120]\n",
      "epoch:0 step:539 [D loss: 0.040803, acc.: 100.00%] [G loss: 1.001062]\n",
      "epoch:0 step:540 [D loss: 0.081197, acc.: 92.97%] [G loss: 1.095860]\n",
      "epoch:0 step:541 [D loss: 0.056915, acc.: 96.88%] [G loss: 1.011206]\n",
      "epoch:0 step:542 [D loss: 0.063300, acc.: 96.88%] [G loss: 0.979585]\n",
      "epoch:0 step:543 [D loss: 0.111058, acc.: 93.75%] [G loss: 0.925705]\n",
      "epoch:0 step:544 [D loss: 0.041399, acc.: 99.22%] [G loss: 1.002097]\n",
      "epoch:0 step:545 [D loss: 0.039031, acc.: 99.22%] [G loss: 1.062659]\n",
      "epoch:0 step:546 [D loss: 0.053467, acc.: 97.66%] [G loss: 1.168596]\n",
      "epoch:0 step:547 [D loss: 0.104493, acc.: 89.84%] [G loss: 0.996452]\n",
      "epoch:0 step:548 [D loss: 0.039133, acc.: 100.00%] [G loss: 1.040310]\n",
      "epoch:0 step:549 [D loss: 0.055444, acc.: 98.44%] [G loss: 1.020820]\n",
      "epoch:0 step:550 [D loss: 0.048867, acc.: 100.00%] [G loss: 1.123787]\n",
      "epoch:0 step:551 [D loss: 0.078637, acc.: 92.19%] [G loss: 0.970876]\n",
      "epoch:0 step:552 [D loss: 0.064840, acc.: 96.09%] [G loss: 1.015263]\n",
      "epoch:0 step:553 [D loss: 0.066019, acc.: 99.22%] [G loss: 1.009106]\n",
      "epoch:0 step:554 [D loss: 0.082422, acc.: 91.41%] [G loss: 1.040428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:555 [D loss: 0.061291, acc.: 97.66%] [G loss: 0.991079]\n",
      "epoch:0 step:556 [D loss: 0.101546, acc.: 94.53%] [G loss: 1.118898]\n",
      "epoch:0 step:557 [D loss: 0.079575, acc.: 92.97%] [G loss: 1.042138]\n",
      "epoch:0 step:558 [D loss: 0.069783, acc.: 95.31%] [G loss: 0.990749]\n",
      "epoch:0 step:559 [D loss: 0.077144, acc.: 96.09%] [G loss: 1.060361]\n",
      "epoch:0 step:560 [D loss: 0.055595, acc.: 98.44%] [G loss: 0.956049]\n",
      "epoch:0 step:561 [D loss: 0.110238, acc.: 83.59%] [G loss: 0.970747]\n",
      "epoch:0 step:562 [D loss: 0.123648, acc.: 85.16%] [G loss: 0.970280]\n",
      "epoch:0 step:563 [D loss: 0.056746, acc.: 100.00%] [G loss: 0.975288]\n",
      "epoch:0 step:564 [D loss: 0.099989, acc.: 84.38%] [G loss: 0.794466]\n",
      "epoch:0 step:565 [D loss: 0.036636, acc.: 99.22%] [G loss: 1.094144]\n",
      "epoch:0 step:566 [D loss: 0.048134, acc.: 99.22%] [G loss: 0.964342]\n",
      "epoch:0 step:567 [D loss: 0.048520, acc.: 97.66%] [G loss: 0.997210]\n",
      "epoch:0 step:568 [D loss: 0.126398, acc.: 83.59%] [G loss: 0.929892]\n",
      "epoch:0 step:569 [D loss: 0.088519, acc.: 94.53%] [G loss: 0.938616]\n",
      "epoch:0 step:570 [D loss: 0.040563, acc.: 99.22%] [G loss: 1.009530]\n",
      "epoch:0 step:571 [D loss: 0.072866, acc.: 95.31%] [G loss: 0.953970]\n",
      "epoch:0 step:572 [D loss: 0.057493, acc.: 98.44%] [G loss: 1.089537]\n",
      "epoch:0 step:573 [D loss: 0.057963, acc.: 98.44%] [G loss: 0.987796]\n",
      "epoch:0 step:574 [D loss: 0.054537, acc.: 98.44%] [G loss: 0.974128]\n",
      "epoch:0 step:575 [D loss: 0.059052, acc.: 97.66%] [G loss: 0.990544]\n",
      "epoch:0 step:576 [D loss: 0.062157, acc.: 96.09%] [G loss: 0.968160]\n",
      "epoch:0 step:577 [D loss: 0.083935, acc.: 87.50%] [G loss: 0.951206]\n",
      "epoch:0 step:578 [D loss: 0.044522, acc.: 96.88%] [G loss: 0.983432]\n",
      "epoch:0 step:579 [D loss: 0.072090, acc.: 93.75%] [G loss: 0.991164]\n",
      "epoch:0 step:580 [D loss: 0.061767, acc.: 98.44%] [G loss: 1.075219]\n",
      "epoch:0 step:581 [D loss: 0.063059, acc.: 98.44%] [G loss: 0.910798]\n",
      "epoch:0 step:582 [D loss: 0.054085, acc.: 94.53%] [G loss: 1.014970]\n",
      "epoch:0 step:583 [D loss: 0.110778, acc.: 85.16%] [G loss: 0.887982]\n",
      "epoch:0 step:584 [D loss: 0.046036, acc.: 99.22%] [G loss: 1.042109]\n",
      "epoch:0 step:585 [D loss: 0.070592, acc.: 94.53%] [G loss: 0.996655]\n",
      "epoch:0 step:586 [D loss: 0.065752, acc.: 94.53%] [G loss: 0.999469]\n",
      "epoch:0 step:587 [D loss: 0.079416, acc.: 91.41%] [G loss: 0.999208]\n",
      "epoch:0 step:588 [D loss: 0.039394, acc.: 99.22%] [G loss: 1.024669]\n",
      "epoch:0 step:589 [D loss: 0.073461, acc.: 96.09%] [G loss: 1.036725]\n",
      "epoch:0 step:590 [D loss: 0.074856, acc.: 96.09%] [G loss: 1.068576]\n",
      "epoch:0 step:591 [D loss: 0.075616, acc.: 96.09%] [G loss: 1.052129]\n",
      "epoch:0 step:592 [D loss: 0.041813, acc.: 98.44%] [G loss: 0.919563]\n",
      "epoch:0 step:593 [D loss: 0.133841, acc.: 83.59%] [G loss: 1.065607]\n",
      "epoch:0 step:594 [D loss: 0.081045, acc.: 96.88%] [G loss: 1.121468]\n",
      "epoch:0 step:595 [D loss: 0.056012, acc.: 98.44%] [G loss: 0.987135]\n",
      "epoch:0 step:596 [D loss: 0.092133, acc.: 95.31%] [G loss: 1.051770]\n",
      "epoch:0 step:597 [D loss: 0.109923, acc.: 89.06%] [G loss: 0.938602]\n",
      "epoch:0 step:598 [D loss: 0.047891, acc.: 98.44%] [G loss: 1.138335]\n",
      "epoch:0 step:599 [D loss: 0.094527, acc.: 88.28%] [G loss: 0.881447]\n",
      "epoch:0 step:600 [D loss: 0.039933, acc.: 99.22%] [G loss: 1.129153]\n",
      "##############\n",
      "[ 9.27053875  7.59974359 13.25367035 10.10885742  9.02935692 11.98333983\n",
      " 11.27914699  9.92963694  9.14768427  9.29033295]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.122274, acc.: 85.16%] [G loss: 0.946652]\n",
      "epoch:0 step:602 [D loss: 0.033317, acc.: 99.22%] [G loss: 1.043136]\n",
      "epoch:0 step:603 [D loss: 0.090283, acc.: 92.97%] [G loss: 0.935176]\n",
      "epoch:0 step:604 [D loss: 0.069039, acc.: 97.66%] [G loss: 1.025331]\n",
      "epoch:0 step:605 [D loss: 0.052110, acc.: 99.22%] [G loss: 1.041109]\n",
      "epoch:0 step:606 [D loss: 0.057955, acc.: 98.44%] [G loss: 1.045297]\n",
      "epoch:0 step:607 [D loss: 0.036473, acc.: 100.00%] [G loss: 1.042659]\n",
      "epoch:0 step:608 [D loss: 0.044688, acc.: 99.22%] [G loss: 1.132361]\n",
      "epoch:0 step:609 [D loss: 0.088446, acc.: 92.19%] [G loss: 1.106007]\n",
      "epoch:0 step:610 [D loss: 0.051033, acc.: 97.66%] [G loss: 1.071890]\n",
      "epoch:0 step:611 [D loss: 0.033482, acc.: 100.00%] [G loss: 1.140687]\n",
      "epoch:0 step:612 [D loss: 0.136205, acc.: 84.38%] [G loss: 1.200665]\n",
      "epoch:0 step:613 [D loss: 0.060030, acc.: 98.44%] [G loss: 0.950483]\n",
      "epoch:0 step:614 [D loss: 0.096920, acc.: 89.84%] [G loss: 1.048313]\n",
      "epoch:0 step:615 [D loss: 0.096261, acc.: 89.06%] [G loss: 0.905639]\n",
      "epoch:0 step:616 [D loss: 0.065122, acc.: 98.44%] [G loss: 0.980694]\n",
      "epoch:0 step:617 [D loss: 0.177489, acc.: 72.66%] [G loss: 0.782742]\n",
      "epoch:0 step:618 [D loss: 0.048803, acc.: 98.44%] [G loss: 1.111788]\n",
      "epoch:0 step:619 [D loss: 0.070182, acc.: 96.09%] [G loss: 0.957934]\n",
      "epoch:0 step:620 [D loss: 0.036863, acc.: 99.22%] [G loss: 1.091062]\n",
      "epoch:0 step:621 [D loss: 0.114472, acc.: 84.38%] [G loss: 0.953691]\n",
      "epoch:0 step:622 [D loss: 0.077516, acc.: 96.88%] [G loss: 0.998121]\n",
      "epoch:0 step:623 [D loss: 0.052705, acc.: 97.66%] [G loss: 0.888230]\n",
      "epoch:0 step:624 [D loss: 0.110418, acc.: 82.81%] [G loss: 1.012085]\n",
      "epoch:0 step:625 [D loss: 0.054259, acc.: 97.66%] [G loss: 1.073546]\n",
      "epoch:0 step:626 [D loss: 0.071352, acc.: 96.09%] [G loss: 1.032866]\n",
      "epoch:0 step:627 [D loss: 0.045301, acc.: 99.22%] [G loss: 0.966651]\n",
      "epoch:0 step:628 [D loss: 0.123763, acc.: 81.25%] [G loss: 1.536952]\n",
      "epoch:0 step:629 [D loss: 0.172609, acc.: 79.69%] [G loss: 1.036752]\n",
      "epoch:0 step:630 [D loss: 0.098424, acc.: 91.41%] [G loss: 1.091940]\n",
      "epoch:0 step:631 [D loss: 0.151938, acc.: 75.00%] [G loss: 1.047721]\n",
      "epoch:0 step:632 [D loss: 0.109903, acc.: 87.50%] [G loss: 0.918334]\n",
      "epoch:0 step:633 [D loss: 0.055592, acc.: 96.88%] [G loss: 1.011612]\n",
      "epoch:0 step:634 [D loss: 0.095590, acc.: 92.19%] [G loss: 1.009720]\n",
      "epoch:0 step:635 [D loss: 0.081007, acc.: 93.75%] [G loss: 0.991459]\n",
      "epoch:0 step:636 [D loss: 0.098660, acc.: 88.28%] [G loss: 1.054890]\n",
      "epoch:0 step:637 [D loss: 0.093953, acc.: 89.06%] [G loss: 0.955350]\n",
      "epoch:0 step:638 [D loss: 0.057767, acc.: 93.75%] [G loss: 0.989595]\n",
      "epoch:0 step:639 [D loss: 0.060324, acc.: 95.31%] [G loss: 1.048067]\n",
      "epoch:0 step:640 [D loss: 0.103907, acc.: 90.62%] [G loss: 0.950783]\n",
      "epoch:0 step:641 [D loss: 0.037051, acc.: 99.22%] [G loss: 1.022878]\n",
      "epoch:0 step:642 [D loss: 0.129081, acc.: 85.94%] [G loss: 1.041925]\n",
      "epoch:0 step:643 [D loss: 0.069301, acc.: 97.66%] [G loss: 0.949029]\n",
      "epoch:0 step:644 [D loss: 0.065290, acc.: 96.09%] [G loss: 1.051076]\n",
      "epoch:0 step:645 [D loss: 0.086920, acc.: 90.62%] [G loss: 1.023019]\n",
      "epoch:0 step:646 [D loss: 0.096224, acc.: 85.16%] [G loss: 0.997910]\n",
      "epoch:0 step:647 [D loss: 0.070398, acc.: 93.75%] [G loss: 1.123683]\n",
      "epoch:0 step:648 [D loss: 0.056200, acc.: 98.44%] [G loss: 0.973443]\n",
      "epoch:0 step:649 [D loss: 0.098381, acc.: 95.31%] [G loss: 1.037276]\n",
      "epoch:0 step:650 [D loss: 0.052442, acc.: 96.88%] [G loss: 1.075008]\n",
      "epoch:0 step:651 [D loss: 0.114125, acc.: 85.94%] [G loss: 0.827969]\n",
      "epoch:0 step:652 [D loss: 0.090958, acc.: 88.28%] [G loss: 1.026456]\n",
      "epoch:0 step:653 [D loss: 0.147696, acc.: 75.78%] [G loss: 0.899762]\n",
      "epoch:0 step:654 [D loss: 0.049651, acc.: 97.66%] [G loss: 1.014992]\n",
      "epoch:0 step:655 [D loss: 0.085303, acc.: 92.97%] [G loss: 1.022823]\n",
      "epoch:0 step:656 [D loss: 0.062579, acc.: 94.53%] [G loss: 0.913951]\n",
      "epoch:0 step:657 [D loss: 0.066406, acc.: 94.53%] [G loss: 0.999720]\n",
      "epoch:0 step:658 [D loss: 0.090808, acc.: 90.62%] [G loss: 0.912946]\n",
      "epoch:0 step:659 [D loss: 0.052996, acc.: 97.66%] [G loss: 0.970714]\n",
      "epoch:0 step:660 [D loss: 0.103555, acc.: 89.06%] [G loss: 0.976035]\n",
      "epoch:0 step:661 [D loss: 0.061260, acc.: 97.66%] [G loss: 0.917488]\n",
      "epoch:0 step:662 [D loss: 0.072619, acc.: 96.09%] [G loss: 0.932035]\n",
      "epoch:0 step:663 [D loss: 0.069661, acc.: 96.09%] [G loss: 1.008741]\n",
      "epoch:0 step:664 [D loss: 0.066171, acc.: 93.75%] [G loss: 0.981286]\n",
      "epoch:0 step:665 [D loss: 0.038904, acc.: 99.22%] [G loss: 1.018798]\n",
      "epoch:0 step:666 [D loss: 0.075188, acc.: 94.53%] [G loss: 0.985943]\n",
      "epoch:0 step:667 [D loss: 0.087332, acc.: 92.19%] [G loss: 0.998299]\n",
      "epoch:0 step:668 [D loss: 0.059756, acc.: 96.88%] [G loss: 1.103984]\n",
      "epoch:0 step:669 [D loss: 0.070759, acc.: 95.31%] [G loss: 1.025702]\n",
      "epoch:0 step:670 [D loss: 0.096763, acc.: 89.84%] [G loss: 1.064395]\n",
      "epoch:0 step:671 [D loss: 0.142700, acc.: 80.47%] [G loss: 1.002713]\n",
      "epoch:0 step:672 [D loss: 0.057657, acc.: 96.88%] [G loss: 1.003465]\n",
      "epoch:0 step:673 [D loss: 0.062814, acc.: 95.31%] [G loss: 0.910072]\n",
      "epoch:0 step:674 [D loss: 0.083756, acc.: 92.19%] [G loss: 1.126767]\n",
      "epoch:0 step:675 [D loss: 0.055718, acc.: 96.09%] [G loss: 0.959669]\n",
      "epoch:0 step:676 [D loss: 0.064257, acc.: 96.88%] [G loss: 1.026762]\n",
      "epoch:0 step:677 [D loss: 0.032299, acc.: 98.44%] [G loss: 0.948213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:678 [D loss: 0.101451, acc.: 92.19%] [G loss: 1.171958]\n",
      "epoch:0 step:679 [D loss: 0.101883, acc.: 89.84%] [G loss: 0.948738]\n",
      "epoch:0 step:680 [D loss: 0.066993, acc.: 96.88%] [G loss: 1.008322]\n",
      "epoch:0 step:681 [D loss: 0.114396, acc.: 84.38%] [G loss: 0.951259]\n",
      "epoch:0 step:682 [D loss: 0.039579, acc.: 97.66%] [G loss: 0.968188]\n",
      "epoch:0 step:683 [D loss: 0.092864, acc.: 92.19%] [G loss: 0.930891]\n",
      "epoch:0 step:684 [D loss: 0.043296, acc.: 99.22%] [G loss: 1.103459]\n",
      "epoch:0 step:685 [D loss: 0.119815, acc.: 81.25%] [G loss: 1.002599]\n",
      "epoch:0 step:686 [D loss: 0.098703, acc.: 87.50%] [G loss: 0.960306]\n",
      "epoch:0 step:687 [D loss: 0.062132, acc.: 93.75%] [G loss: 0.921923]\n",
      "epoch:0 step:688 [D loss: 0.087020, acc.: 91.41%] [G loss: 1.054985]\n",
      "epoch:0 step:689 [D loss: 0.092700, acc.: 87.50%] [G loss: 1.014224]\n",
      "epoch:0 step:690 [D loss: 0.095128, acc.: 86.72%] [G loss: 0.954765]\n",
      "epoch:0 step:691 [D loss: 0.084031, acc.: 91.41%] [G loss: 1.008096]\n",
      "epoch:0 step:692 [D loss: 0.078609, acc.: 93.75%] [G loss: 1.028263]\n",
      "epoch:0 step:693 [D loss: 0.100763, acc.: 86.72%] [G loss: 0.987589]\n",
      "epoch:0 step:694 [D loss: 0.073939, acc.: 93.75%] [G loss: 1.068310]\n",
      "epoch:0 step:695 [D loss: 0.073889, acc.: 93.75%] [G loss: 0.868106]\n",
      "epoch:0 step:696 [D loss: 0.050085, acc.: 99.22%] [G loss: 1.013481]\n",
      "epoch:0 step:697 [D loss: 0.114225, acc.: 82.81%] [G loss: 1.102432]\n",
      "epoch:0 step:698 [D loss: 0.050273, acc.: 99.22%] [G loss: 1.066540]\n",
      "epoch:0 step:699 [D loss: 0.138499, acc.: 82.03%] [G loss: 1.101994]\n",
      "epoch:0 step:700 [D loss: 0.077839, acc.: 92.97%] [G loss: 0.863799]\n",
      "epoch:0 step:701 [D loss: 0.126556, acc.: 82.03%] [G loss: 1.039431]\n",
      "epoch:0 step:702 [D loss: 0.120271, acc.: 85.16%] [G loss: 0.911165]\n",
      "epoch:0 step:703 [D loss: 0.043130, acc.: 99.22%] [G loss: 1.061472]\n",
      "epoch:0 step:704 [D loss: 0.152246, acc.: 74.22%] [G loss: 0.847881]\n",
      "epoch:0 step:705 [D loss: 0.048641, acc.: 99.22%] [G loss: 1.078686]\n",
      "epoch:0 step:706 [D loss: 0.124171, acc.: 84.38%] [G loss: 0.870330]\n",
      "epoch:0 step:707 [D loss: 0.059923, acc.: 97.66%] [G loss: 1.163707]\n",
      "epoch:0 step:708 [D loss: 0.131409, acc.: 81.25%] [G loss: 0.882472]\n",
      "epoch:0 step:709 [D loss: 0.063050, acc.: 98.44%] [G loss: 1.078483]\n",
      "epoch:0 step:710 [D loss: 0.231338, acc.: 59.38%] [G loss: 0.812790]\n",
      "epoch:0 step:711 [D loss: 0.069721, acc.: 94.53%] [G loss: 0.996121]\n",
      "epoch:0 step:712 [D loss: 0.083789, acc.: 92.19%] [G loss: 0.996161]\n",
      "epoch:0 step:713 [D loss: 0.095175, acc.: 89.84%] [G loss: 0.923804]\n",
      "epoch:0 step:714 [D loss: 0.072477, acc.: 93.75%] [G loss: 0.945211]\n",
      "epoch:0 step:715 [D loss: 0.066803, acc.: 96.88%] [G loss: 0.988312]\n",
      "epoch:0 step:716 [D loss: 0.118051, acc.: 78.91%] [G loss: 0.927577]\n",
      "epoch:0 step:717 [D loss: 0.078349, acc.: 94.53%] [G loss: 1.024915]\n",
      "epoch:0 step:718 [D loss: 0.139955, acc.: 82.81%] [G loss: 0.980036]\n",
      "epoch:0 step:719 [D loss: 0.108899, acc.: 86.72%] [G loss: 0.808815]\n",
      "epoch:0 step:720 [D loss: 0.062832, acc.: 93.75%] [G loss: 1.133034]\n",
      "epoch:0 step:721 [D loss: 0.167617, acc.: 75.00%] [G loss: 0.958184]\n",
      "epoch:0 step:722 [D loss: 0.047891, acc.: 99.22%] [G loss: 0.942021]\n",
      "epoch:0 step:723 [D loss: 0.129277, acc.: 82.03%] [G loss: 0.938266]\n",
      "epoch:0 step:724 [D loss: 0.070705, acc.: 94.53%] [G loss: 0.942271]\n",
      "epoch:0 step:725 [D loss: 0.093274, acc.: 90.62%] [G loss: 0.881307]\n",
      "epoch:0 step:726 [D loss: 0.075400, acc.: 97.66%] [G loss: 1.113425]\n",
      "epoch:0 step:727 [D loss: 0.109578, acc.: 89.84%] [G loss: 0.902837]\n",
      "epoch:0 step:728 [D loss: 0.079724, acc.: 93.75%] [G loss: 1.011124]\n",
      "epoch:0 step:729 [D loss: 0.091870, acc.: 95.31%] [G loss: 0.954023]\n",
      "epoch:0 step:730 [D loss: 0.062609, acc.: 95.31%] [G loss: 1.046154]\n",
      "epoch:0 step:731 [D loss: 0.118587, acc.: 85.94%] [G loss: 0.932618]\n",
      "epoch:0 step:732 [D loss: 0.053275, acc.: 97.66%] [G loss: 1.036996]\n",
      "epoch:0 step:733 [D loss: 0.098965, acc.: 90.62%] [G loss: 1.046859]\n",
      "epoch:0 step:734 [D loss: 0.096934, acc.: 87.50%] [G loss: 1.041163]\n",
      "epoch:0 step:735 [D loss: 0.072601, acc.: 96.09%] [G loss: 1.032995]\n",
      "epoch:0 step:736 [D loss: 0.112981, acc.: 84.38%] [G loss: 0.971932]\n",
      "epoch:0 step:737 [D loss: 0.115369, acc.: 84.38%] [G loss: 1.039749]\n",
      "epoch:0 step:738 [D loss: 0.135626, acc.: 83.59%] [G loss: 0.965439]\n",
      "epoch:0 step:739 [D loss: 0.067779, acc.: 93.75%] [G loss: 1.103059]\n",
      "epoch:0 step:740 [D loss: 0.105107, acc.: 86.72%] [G loss: 0.955257]\n",
      "epoch:0 step:741 [D loss: 0.146595, acc.: 83.59%] [G loss: 1.156511]\n",
      "epoch:0 step:742 [D loss: 0.220909, acc.: 60.16%] [G loss: 0.738384]\n",
      "epoch:0 step:743 [D loss: 0.076483, acc.: 93.75%] [G loss: 1.168914]\n",
      "epoch:0 step:744 [D loss: 0.111003, acc.: 84.38%] [G loss: 0.866955]\n",
      "epoch:0 step:745 [D loss: 0.063224, acc.: 92.97%] [G loss: 1.045777]\n",
      "epoch:0 step:746 [D loss: 0.085577, acc.: 91.41%] [G loss: 0.817884]\n",
      "epoch:0 step:747 [D loss: 0.070817, acc.: 93.75%] [G loss: 1.088248]\n",
      "epoch:0 step:748 [D loss: 0.112645, acc.: 89.06%] [G loss: 0.854157]\n",
      "epoch:0 step:749 [D loss: 0.066694, acc.: 96.09%] [G loss: 1.062640]\n",
      "epoch:0 step:750 [D loss: 0.125597, acc.: 85.94%] [G loss: 0.858907]\n",
      "epoch:0 step:751 [D loss: 0.078437, acc.: 89.84%] [G loss: 0.951103]\n",
      "epoch:0 step:752 [D loss: 0.080921, acc.: 91.41%] [G loss: 0.920533]\n",
      "epoch:0 step:753 [D loss: 0.080472, acc.: 92.97%] [G loss: 1.011710]\n",
      "epoch:0 step:754 [D loss: 0.098658, acc.: 91.41%] [G loss: 1.045085]\n",
      "epoch:0 step:755 [D loss: 0.087733, acc.: 90.62%] [G loss: 0.957987]\n",
      "epoch:0 step:756 [D loss: 0.098677, acc.: 87.50%] [G loss: 0.957023]\n",
      "epoch:0 step:757 [D loss: 0.068348, acc.: 96.09%] [G loss: 0.888145]\n",
      "epoch:0 step:758 [D loss: 0.103282, acc.: 92.97%] [G loss: 1.037615]\n",
      "epoch:0 step:759 [D loss: 0.085462, acc.: 92.97%] [G loss: 0.916796]\n",
      "epoch:0 step:760 [D loss: 0.069792, acc.: 94.53%] [G loss: 0.968072]\n",
      "epoch:0 step:761 [D loss: 0.100867, acc.: 89.84%] [G loss: 0.960119]\n",
      "epoch:0 step:762 [D loss: 0.075595, acc.: 92.97%] [G loss: 1.021174]\n",
      "epoch:0 step:763 [D loss: 0.074466, acc.: 92.97%] [G loss: 1.071327]\n",
      "epoch:0 step:764 [D loss: 0.091475, acc.: 90.62%] [G loss: 0.977668]\n",
      "epoch:0 step:765 [D loss: 0.114726, acc.: 85.94%] [G loss: 1.062864]\n",
      "epoch:0 step:766 [D loss: 0.074914, acc.: 92.19%] [G loss: 0.862504]\n",
      "epoch:0 step:767 [D loss: 0.125411, acc.: 84.38%] [G loss: 1.184405]\n",
      "epoch:0 step:768 [D loss: 0.092790, acc.: 90.62%] [G loss: 0.747887]\n",
      "epoch:0 step:769 [D loss: 0.096275, acc.: 94.53%] [G loss: 1.142503]\n",
      "epoch:0 step:770 [D loss: 0.144465, acc.: 78.91%] [G loss: 0.918097]\n",
      "epoch:0 step:771 [D loss: 0.060618, acc.: 95.31%] [G loss: 1.089672]\n",
      "epoch:0 step:772 [D loss: 0.064687, acc.: 92.97%] [G loss: 0.945234]\n",
      "epoch:0 step:773 [D loss: 0.090957, acc.: 89.06%] [G loss: 0.959227]\n",
      "epoch:0 step:774 [D loss: 0.071542, acc.: 91.41%] [G loss: 0.907696]\n",
      "epoch:0 step:775 [D loss: 0.061759, acc.: 96.09%] [G loss: 1.065180]\n",
      "epoch:0 step:776 [D loss: 0.128937, acc.: 82.81%] [G loss: 0.896435]\n",
      "epoch:0 step:777 [D loss: 0.087751, acc.: 92.19%] [G loss: 1.058305]\n",
      "epoch:0 step:778 [D loss: 0.079257, acc.: 97.66%] [G loss: 0.888534]\n",
      "epoch:0 step:779 [D loss: 0.137733, acc.: 82.81%] [G loss: 1.129203]\n",
      "epoch:0 step:780 [D loss: 0.076463, acc.: 96.09%] [G loss: 0.914629]\n",
      "epoch:0 step:781 [D loss: 0.061020, acc.: 96.88%] [G loss: 1.029355]\n",
      "epoch:0 step:782 [D loss: 0.101965, acc.: 85.94%] [G loss: 0.991574]\n",
      "epoch:0 step:783 [D loss: 0.127292, acc.: 85.94%] [G loss: 0.937855]\n",
      "epoch:0 step:784 [D loss: 0.075618, acc.: 96.88%] [G loss: 1.108627]\n",
      "epoch:0 step:785 [D loss: 0.128842, acc.: 85.94%] [G loss: 0.918805]\n",
      "epoch:0 step:786 [D loss: 0.065137, acc.: 95.31%] [G loss: 0.996648]\n",
      "epoch:0 step:787 [D loss: 0.112653, acc.: 85.94%] [G loss: 0.890509]\n",
      "epoch:0 step:788 [D loss: 0.103391, acc.: 88.28%] [G loss: 0.970707]\n",
      "epoch:0 step:789 [D loss: 0.072963, acc.: 96.88%] [G loss: 0.986356]\n",
      "epoch:0 step:790 [D loss: 0.086259, acc.: 96.88%] [G loss: 0.876350]\n",
      "epoch:0 step:791 [D loss: 0.088521, acc.: 92.97%] [G loss: 1.026119]\n",
      "epoch:0 step:792 [D loss: 0.074543, acc.: 95.31%] [G loss: 0.952604]\n",
      "epoch:0 step:793 [D loss: 0.099639, acc.: 89.06%] [G loss: 1.032120]\n",
      "epoch:0 step:794 [D loss: 0.092108, acc.: 93.75%] [G loss: 0.921749]\n",
      "epoch:0 step:795 [D loss: 0.098624, acc.: 89.84%] [G loss: 1.149338]\n",
      "epoch:0 step:796 [D loss: 0.130875, acc.: 82.81%] [G loss: 0.998937]\n",
      "epoch:0 step:797 [D loss: 0.108949, acc.: 89.84%] [G loss: 1.019956]\n",
      "epoch:0 step:798 [D loss: 0.057582, acc.: 96.88%] [G loss: 0.942903]\n",
      "epoch:0 step:799 [D loss: 0.098086, acc.: 88.28%] [G loss: 1.008281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:800 [D loss: 0.087611, acc.: 92.97%] [G loss: 0.935183]\n",
      "##############\n",
      "[ 5.96007938  4.90106171 10.74090719  8.25204755  8.51743494  9.09544056\n",
      "  9.51754179 11.77839851  8.9988672   7.22196145]\n",
      "##########\n",
      "epoch:0 step:801 [D loss: 0.096082, acc.: 89.84%] [G loss: 1.050837]\n",
      "epoch:0 step:802 [D loss: 0.094175, acc.: 89.06%] [G loss: 0.944970]\n",
      "epoch:0 step:803 [D loss: 0.067761, acc.: 95.31%] [G loss: 0.948326]\n",
      "epoch:0 step:804 [D loss: 0.118008, acc.: 78.91%] [G loss: 1.024060]\n",
      "epoch:0 step:805 [D loss: 0.080625, acc.: 92.97%] [G loss: 0.946953]\n",
      "epoch:0 step:806 [D loss: 0.105313, acc.: 85.94%] [G loss: 1.073689]\n",
      "epoch:0 step:807 [D loss: 0.095738, acc.: 89.84%] [G loss: 0.830876]\n",
      "epoch:0 step:808 [D loss: 0.074179, acc.: 94.53%] [G loss: 1.070626]\n",
      "epoch:0 step:809 [D loss: 0.099600, acc.: 87.50%] [G loss: 0.824932]\n",
      "epoch:0 step:810 [D loss: 0.065862, acc.: 93.75%] [G loss: 1.076824]\n",
      "epoch:0 step:811 [D loss: 0.119343, acc.: 82.81%] [G loss: 0.922931]\n",
      "epoch:0 step:812 [D loss: 0.055528, acc.: 96.88%] [G loss: 1.077219]\n",
      "epoch:0 step:813 [D loss: 0.123550, acc.: 78.91%] [G loss: 0.876581]\n",
      "epoch:0 step:814 [D loss: 0.085186, acc.: 92.97%] [G loss: 0.986437]\n",
      "epoch:0 step:815 [D loss: 0.084471, acc.: 91.41%] [G loss: 0.914972]\n",
      "epoch:0 step:816 [D loss: 0.070837, acc.: 92.97%] [G loss: 1.043027]\n",
      "epoch:0 step:817 [D loss: 0.098045, acc.: 89.84%] [G loss: 0.878563]\n",
      "epoch:0 step:818 [D loss: 0.070583, acc.: 93.75%] [G loss: 1.007134]\n",
      "epoch:0 step:819 [D loss: 0.088438, acc.: 89.84%] [G loss: 0.896590]\n",
      "epoch:0 step:820 [D loss: 0.075613, acc.: 92.97%] [G loss: 0.981069]\n",
      "epoch:0 step:821 [D loss: 0.090806, acc.: 90.62%] [G loss: 0.877094]\n",
      "epoch:0 step:822 [D loss: 0.070119, acc.: 96.88%] [G loss: 0.884729]\n",
      "epoch:0 step:823 [D loss: 0.094696, acc.: 89.06%] [G loss: 1.036356]\n",
      "epoch:0 step:824 [D loss: 0.090305, acc.: 92.19%] [G loss: 0.887976]\n",
      "epoch:0 step:825 [D loss: 0.077273, acc.: 95.31%] [G loss: 0.946246]\n",
      "epoch:0 step:826 [D loss: 0.079214, acc.: 96.09%] [G loss: 0.925697]\n",
      "epoch:0 step:827 [D loss: 0.091261, acc.: 92.19%] [G loss: 1.003780]\n",
      "epoch:0 step:828 [D loss: 0.052486, acc.: 96.09%] [G loss: 1.025225]\n",
      "epoch:0 step:829 [D loss: 0.080467, acc.: 92.97%] [G loss: 0.974458]\n",
      "epoch:0 step:830 [D loss: 0.071797, acc.: 96.09%] [G loss: 1.038601]\n",
      "epoch:0 step:831 [D loss: 0.074262, acc.: 94.53%] [G loss: 0.959480]\n",
      "epoch:0 step:832 [D loss: 0.068060, acc.: 93.75%] [G loss: 0.955409]\n",
      "epoch:0 step:833 [D loss: 0.099041, acc.: 92.19%] [G loss: 0.999680]\n",
      "epoch:0 step:834 [D loss: 0.097506, acc.: 90.62%] [G loss: 1.029362]\n",
      "epoch:0 step:835 [D loss: 0.072386, acc.: 93.75%] [G loss: 0.868913]\n",
      "epoch:0 step:836 [D loss: 0.106350, acc.: 89.06%] [G loss: 1.045225]\n",
      "epoch:0 step:837 [D loss: 0.096920, acc.: 92.97%] [G loss: 0.950345]\n",
      "epoch:0 step:838 [D loss: 0.069341, acc.: 96.09%] [G loss: 0.956754]\n",
      "epoch:0 step:839 [D loss: 0.073477, acc.: 95.31%] [G loss: 0.963746]\n",
      "epoch:0 step:840 [D loss: 0.086369, acc.: 91.41%] [G loss: 0.938326]\n",
      "epoch:0 step:841 [D loss: 0.084189, acc.: 93.75%] [G loss: 0.943204]\n",
      "epoch:0 step:842 [D loss: 0.058021, acc.: 96.88%] [G loss: 0.964700]\n",
      "epoch:0 step:843 [D loss: 0.081662, acc.: 94.53%] [G loss: 0.974774]\n",
      "epoch:0 step:844 [D loss: 0.100667, acc.: 88.28%] [G loss: 1.023743]\n",
      "epoch:0 step:845 [D loss: 0.064170, acc.: 94.53%] [G loss: 1.045348]\n",
      "epoch:0 step:846 [D loss: 0.083503, acc.: 93.75%] [G loss: 1.067418]\n",
      "epoch:0 step:847 [D loss: 0.068894, acc.: 96.09%] [G loss: 0.957315]\n",
      "epoch:0 step:848 [D loss: 0.062650, acc.: 97.66%] [G loss: 1.082527]\n",
      "epoch:0 step:849 [D loss: 0.119104, acc.: 88.28%] [G loss: 1.126648]\n",
      "epoch:0 step:850 [D loss: 0.071722, acc.: 93.75%] [G loss: 1.013363]\n",
      "epoch:0 step:851 [D loss: 0.056809, acc.: 97.66%] [G loss: 1.035769]\n",
      "epoch:0 step:852 [D loss: 0.062728, acc.: 98.44%] [G loss: 1.095776]\n",
      "epoch:0 step:853 [D loss: 0.083235, acc.: 89.06%] [G loss: 1.096183]\n",
      "epoch:0 step:854 [D loss: 0.059832, acc.: 96.09%] [G loss: 0.990382]\n",
      "epoch:0 step:855 [D loss: 0.073359, acc.: 93.75%] [G loss: 1.053662]\n",
      "epoch:0 step:856 [D loss: 0.094981, acc.: 88.28%] [G loss: 1.115998]\n",
      "epoch:0 step:857 [D loss: 0.085200, acc.: 91.41%] [G loss: 1.047875]\n",
      "epoch:0 step:858 [D loss: 0.129515, acc.: 82.03%] [G loss: 1.190727]\n",
      "epoch:0 step:859 [D loss: 0.067577, acc.: 97.66%] [G loss: 1.007153]\n",
      "epoch:0 step:860 [D loss: 0.102084, acc.: 90.62%] [G loss: 1.114074]\n",
      "epoch:0 step:861 [D loss: 0.104239, acc.: 85.94%] [G loss: 0.982676]\n",
      "epoch:0 step:862 [D loss: 0.071549, acc.: 93.75%] [G loss: 1.242305]\n",
      "epoch:0 step:863 [D loss: 0.110835, acc.: 87.50%] [G loss: 1.051181]\n",
      "epoch:0 step:864 [D loss: 0.065638, acc.: 96.88%] [G loss: 1.106332]\n",
      "epoch:0 step:865 [D loss: 0.074569, acc.: 95.31%] [G loss: 1.196482]\n",
      "epoch:0 step:866 [D loss: 0.075324, acc.: 92.97%] [G loss: 1.016425]\n",
      "epoch:0 step:867 [D loss: 0.084094, acc.: 95.31%] [G loss: 1.127962]\n",
      "epoch:0 step:868 [D loss: 0.089911, acc.: 92.97%] [G loss: 1.204648]\n",
      "epoch:0 step:869 [D loss: 0.076549, acc.: 92.19%] [G loss: 0.980805]\n",
      "epoch:0 step:870 [D loss: 0.151036, acc.: 76.56%] [G loss: 1.054552]\n",
      "epoch:0 step:871 [D loss: 0.099521, acc.: 94.53%] [G loss: 0.984367]\n",
      "epoch:0 step:872 [D loss: 0.103542, acc.: 89.84%] [G loss: 1.055700]\n",
      "epoch:0 step:873 [D loss: 0.103335, acc.: 88.28%] [G loss: 0.982021]\n",
      "epoch:0 step:874 [D loss: 0.087537, acc.: 92.97%] [G loss: 1.091360]\n",
      "epoch:0 step:875 [D loss: 0.066412, acc.: 96.88%] [G loss: 1.050340]\n",
      "epoch:0 step:876 [D loss: 0.091077, acc.: 90.62%] [G loss: 1.089367]\n",
      "epoch:0 step:877 [D loss: 0.073478, acc.: 94.53%] [G loss: 1.137239]\n",
      "epoch:0 step:878 [D loss: 0.095062, acc.: 94.53%] [G loss: 1.073025]\n",
      "epoch:0 step:879 [D loss: 0.066227, acc.: 97.66%] [G loss: 1.017001]\n",
      "epoch:0 step:880 [D loss: 0.135840, acc.: 79.69%] [G loss: 1.159484]\n",
      "epoch:0 step:881 [D loss: 0.070481, acc.: 98.44%] [G loss: 0.897678]\n",
      "epoch:0 step:882 [D loss: 0.119460, acc.: 78.12%] [G loss: 1.142152]\n",
      "epoch:0 step:883 [D loss: 0.110141, acc.: 92.19%] [G loss: 0.981687]\n",
      "epoch:0 step:884 [D loss: 0.062204, acc.: 96.09%] [G loss: 0.981714]\n",
      "epoch:0 step:885 [D loss: 0.092281, acc.: 93.75%] [G loss: 1.026192]\n",
      "epoch:0 step:886 [D loss: 0.120197, acc.: 82.81%] [G loss: 1.114471]\n",
      "epoch:0 step:887 [D loss: 0.050304, acc.: 98.44%] [G loss: 0.935227]\n",
      "epoch:0 step:888 [D loss: 0.103385, acc.: 87.50%] [G loss: 1.191373]\n",
      "epoch:0 step:889 [D loss: 0.117658, acc.: 88.28%] [G loss: 0.988860]\n",
      "epoch:0 step:890 [D loss: 0.075124, acc.: 93.75%] [G loss: 1.042310]\n",
      "epoch:0 step:891 [D loss: 0.124902, acc.: 82.03%] [G loss: 0.995343]\n",
      "epoch:0 step:892 [D loss: 0.099373, acc.: 87.50%] [G loss: 0.930527]\n",
      "epoch:0 step:893 [D loss: 0.057206, acc.: 96.09%] [G loss: 1.060025]\n",
      "epoch:0 step:894 [D loss: 0.097749, acc.: 91.41%] [G loss: 1.021825]\n",
      "epoch:0 step:895 [D loss: 0.093068, acc.: 92.97%] [G loss: 1.004065]\n",
      "epoch:0 step:896 [D loss: 0.080641, acc.: 96.09%] [G loss: 1.181096]\n",
      "epoch:0 step:897 [D loss: 0.111067, acc.: 87.50%] [G loss: 0.929962]\n",
      "epoch:0 step:898 [D loss: 0.053059, acc.: 98.44%] [G loss: 1.228992]\n",
      "epoch:0 step:899 [D loss: 0.136842, acc.: 83.59%] [G loss: 1.169779]\n",
      "epoch:0 step:900 [D loss: 0.075658, acc.: 96.09%] [G loss: 1.027783]\n",
      "epoch:0 step:901 [D loss: 0.077700, acc.: 95.31%] [G loss: 1.073049]\n",
      "epoch:0 step:902 [D loss: 0.161487, acc.: 75.78%] [G loss: 1.119476]\n",
      "epoch:0 step:903 [D loss: 0.068847, acc.: 95.31%] [G loss: 0.974223]\n",
      "epoch:0 step:904 [D loss: 0.111371, acc.: 91.41%] [G loss: 1.189914]\n",
      "epoch:0 step:905 [D loss: 0.121977, acc.: 86.72%] [G loss: 0.881091]\n",
      "epoch:0 step:906 [D loss: 0.064515, acc.: 94.53%] [G loss: 1.097583]\n",
      "epoch:0 step:907 [D loss: 0.187532, acc.: 75.78%] [G loss: 1.158946]\n",
      "epoch:0 step:908 [D loss: 0.079940, acc.: 95.31%] [G loss: 0.966724]\n",
      "epoch:0 step:909 [D loss: 0.108753, acc.: 88.28%] [G loss: 1.060213]\n",
      "epoch:0 step:910 [D loss: 0.085521, acc.: 93.75%] [G loss: 1.025092]\n",
      "epoch:0 step:911 [D loss: 0.099631, acc.: 94.53%] [G loss: 1.104770]\n",
      "epoch:0 step:912 [D loss: 0.094690, acc.: 92.19%] [G loss: 1.055559]\n",
      "epoch:0 step:913 [D loss: 0.054781, acc.: 96.09%] [G loss: 1.052413]\n",
      "epoch:0 step:914 [D loss: 0.072645, acc.: 95.31%] [G loss: 1.211384]\n",
      "epoch:0 step:915 [D loss: 0.238229, acc.: 63.28%] [G loss: 1.314358]\n",
      "epoch:0 step:916 [D loss: 0.096353, acc.: 90.62%] [G loss: 0.897451]\n",
      "epoch:0 step:917 [D loss: 0.146499, acc.: 80.47%] [G loss: 1.307651]\n",
      "epoch:0 step:918 [D loss: 0.116960, acc.: 82.81%] [G loss: 0.863509]\n",
      "epoch:0 step:919 [D loss: 0.109142, acc.: 83.59%] [G loss: 1.029761]\n",
      "epoch:0 step:920 [D loss: 0.127538, acc.: 83.59%] [G loss: 0.997645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:921 [D loss: 0.058741, acc.: 98.44%] [G loss: 1.129024]\n",
      "epoch:0 step:922 [D loss: 0.145025, acc.: 85.16%] [G loss: 1.051423]\n",
      "epoch:0 step:923 [D loss: 0.070184, acc.: 94.53%] [G loss: 1.237632]\n",
      "epoch:0 step:924 [D loss: 0.080708, acc.: 92.19%] [G loss: 0.964948]\n",
      "epoch:0 step:925 [D loss: 0.044673, acc.: 97.66%] [G loss: 0.995140]\n",
      "epoch:0 step:926 [D loss: 0.078287, acc.: 94.53%] [G loss: 1.065743]\n",
      "epoch:0 step:927 [D loss: 0.095110, acc.: 89.84%] [G loss: 1.080067]\n",
      "epoch:0 step:928 [D loss: 0.098657, acc.: 91.41%] [G loss: 1.112643]\n",
      "epoch:0 step:929 [D loss: 0.043525, acc.: 100.00%] [G loss: 1.021037]\n",
      "epoch:0 step:930 [D loss: 0.200174, acc.: 64.84%] [G loss: 0.880584]\n",
      "epoch:0 step:931 [D loss: 0.114238, acc.: 89.84%] [G loss: 0.892518]\n",
      "epoch:0 step:932 [D loss: 0.115325, acc.: 90.62%] [G loss: 0.916528]\n",
      "epoch:0 step:933 [D loss: 0.074701, acc.: 90.62%] [G loss: 1.028561]\n",
      "epoch:0 step:934 [D loss: 0.124003, acc.: 80.47%] [G loss: 0.979103]\n",
      "epoch:0 step:935 [D loss: 0.052193, acc.: 97.66%] [G loss: 0.938741]\n",
      "epoch:0 step:936 [D loss: 0.059576, acc.: 98.44%] [G loss: 1.036204]\n",
      "epoch:0 step:937 [D loss: 0.166912, acc.: 74.22%] [G loss: 0.984002]\n",
      "epoch:1 step:938 [D loss: 0.066551, acc.: 94.53%] [G loss: 1.051997]\n",
      "epoch:1 step:939 [D loss: 0.121287, acc.: 86.72%] [G loss: 0.992061]\n",
      "epoch:1 step:940 [D loss: 0.099707, acc.: 89.06%] [G loss: 0.970482]\n",
      "epoch:1 step:941 [D loss: 0.094575, acc.: 90.62%] [G loss: 1.001640]\n",
      "epoch:1 step:942 [D loss: 0.083637, acc.: 94.53%] [G loss: 1.140992]\n",
      "epoch:1 step:943 [D loss: 0.129996, acc.: 81.25%] [G loss: 0.901624]\n",
      "epoch:1 step:944 [D loss: 0.087349, acc.: 96.88%] [G loss: 1.146618]\n",
      "epoch:1 step:945 [D loss: 0.142103, acc.: 75.78%] [G loss: 1.096172]\n",
      "epoch:1 step:946 [D loss: 0.073944, acc.: 93.75%] [G loss: 1.108114]\n",
      "epoch:1 step:947 [D loss: 0.130779, acc.: 82.81%] [G loss: 1.021276]\n",
      "epoch:1 step:948 [D loss: 0.081097, acc.: 94.53%] [G loss: 1.364922]\n",
      "epoch:1 step:949 [D loss: 0.158332, acc.: 77.34%] [G loss: 0.998248]\n",
      "epoch:1 step:950 [D loss: 0.071541, acc.: 92.97%] [G loss: 1.154654]\n",
      "epoch:1 step:951 [D loss: 0.090874, acc.: 93.75%] [G loss: 1.066484]\n",
      "epoch:1 step:952 [D loss: 0.108724, acc.: 90.62%] [G loss: 1.092439]\n",
      "epoch:1 step:953 [D loss: 0.088327, acc.: 92.97%] [G loss: 1.177027]\n",
      "epoch:1 step:954 [D loss: 0.104091, acc.: 91.41%] [G loss: 1.236574]\n",
      "epoch:1 step:955 [D loss: 0.127467, acc.: 85.16%] [G loss: 1.378903]\n",
      "epoch:1 step:956 [D loss: 0.122463, acc.: 87.50%] [G loss: 1.049325]\n",
      "epoch:1 step:957 [D loss: 0.071724, acc.: 95.31%] [G loss: 1.278674]\n",
      "epoch:1 step:958 [D loss: 0.154859, acc.: 77.34%] [G loss: 1.231446]\n",
      "epoch:1 step:959 [D loss: 0.079059, acc.: 93.75%] [G loss: 1.188086]\n",
      "epoch:1 step:960 [D loss: 0.243248, acc.: 57.81%] [G loss: 1.264554]\n",
      "epoch:1 step:961 [D loss: 0.085226, acc.: 92.19%] [G loss: 0.734206]\n",
      "epoch:1 step:962 [D loss: 0.138979, acc.: 78.12%] [G loss: 1.426517]\n",
      "epoch:1 step:963 [D loss: 0.178866, acc.: 71.88%] [G loss: 0.731568]\n",
      "epoch:1 step:964 [D loss: 0.064727, acc.: 96.88%] [G loss: 1.158982]\n",
      "epoch:1 step:965 [D loss: 0.161307, acc.: 78.12%] [G loss: 0.851945]\n",
      "epoch:1 step:966 [D loss: 0.053175, acc.: 96.88%] [G loss: 1.132383]\n",
      "epoch:1 step:967 [D loss: 0.125419, acc.: 87.50%] [G loss: 1.024674]\n",
      "epoch:1 step:968 [D loss: 0.070680, acc.: 96.88%] [G loss: 0.990928]\n",
      "epoch:1 step:969 [D loss: 0.067793, acc.: 91.41%] [G loss: 1.083990]\n",
      "epoch:1 step:970 [D loss: 0.092980, acc.: 92.19%] [G loss: 1.074784]\n",
      "epoch:1 step:971 [D loss: 0.080191, acc.: 90.62%] [G loss: 0.953465]\n",
      "epoch:1 step:972 [D loss: 0.104298, acc.: 89.84%] [G loss: 1.105472]\n",
      "epoch:1 step:973 [D loss: 0.066847, acc.: 96.88%] [G loss: 0.957809]\n",
      "epoch:1 step:974 [D loss: 0.124730, acc.: 85.16%] [G loss: 1.217686]\n",
      "epoch:1 step:975 [D loss: 0.117612, acc.: 82.03%] [G loss: 0.896682]\n",
      "epoch:1 step:976 [D loss: 0.077401, acc.: 94.53%] [G loss: 1.148839]\n",
      "epoch:1 step:977 [D loss: 0.103446, acc.: 89.84%] [G loss: 1.075185]\n",
      "epoch:1 step:978 [D loss: 0.166489, acc.: 79.69%] [G loss: 1.106820]\n",
      "epoch:1 step:979 [D loss: 0.099308, acc.: 89.84%] [G loss: 1.182964]\n",
      "epoch:1 step:980 [D loss: 0.105331, acc.: 89.06%] [G loss: 0.899210]\n",
      "epoch:1 step:981 [D loss: 0.072021, acc.: 96.88%] [G loss: 1.057562]\n",
      "epoch:1 step:982 [D loss: 0.084843, acc.: 91.41%] [G loss: 1.062022]\n",
      "epoch:1 step:983 [D loss: 0.074695, acc.: 96.09%] [G loss: 1.130567]\n",
      "epoch:1 step:984 [D loss: 0.132264, acc.: 84.38%] [G loss: 1.049757]\n",
      "epoch:1 step:985 [D loss: 0.075647, acc.: 93.75%] [G loss: 0.965664]\n",
      "epoch:1 step:986 [D loss: 0.111335, acc.: 87.50%] [G loss: 1.184942]\n",
      "epoch:1 step:987 [D loss: 0.086191, acc.: 89.84%] [G loss: 0.980192]\n",
      "epoch:1 step:988 [D loss: 0.075540, acc.: 92.19%] [G loss: 1.226634]\n",
      "epoch:1 step:989 [D loss: 0.100161, acc.: 89.06%] [G loss: 0.960969]\n",
      "epoch:1 step:990 [D loss: 0.085858, acc.: 94.53%] [G loss: 1.291137]\n",
      "epoch:1 step:991 [D loss: 0.097798, acc.: 93.75%] [G loss: 0.812032]\n",
      "epoch:1 step:992 [D loss: 0.126928, acc.: 85.94%] [G loss: 1.293375]\n",
      "epoch:1 step:993 [D loss: 0.122278, acc.: 85.16%] [G loss: 0.972675]\n",
      "epoch:1 step:994 [D loss: 0.095471, acc.: 88.28%] [G loss: 1.160722]\n",
      "epoch:1 step:995 [D loss: 0.144903, acc.: 77.34%] [G loss: 1.019164]\n",
      "epoch:1 step:996 [D loss: 0.087504, acc.: 92.19%] [G loss: 1.038796]\n",
      "epoch:1 step:997 [D loss: 0.097033, acc.: 85.94%] [G loss: 1.016727]\n",
      "epoch:1 step:998 [D loss: 0.082613, acc.: 92.19%] [G loss: 1.071428]\n",
      "epoch:1 step:999 [D loss: 0.125755, acc.: 85.16%] [G loss: 1.139529]\n",
      "epoch:1 step:1000 [D loss: 0.072088, acc.: 93.75%] [G loss: 1.143352]\n",
      "##############\n",
      "[ 7.32433081  7.59974359  8.70058574  8.05998005  7.79195674  9.0330042\n",
      " 11.27914699  8.38150999  7.91068535  6.85598957]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.129310, acc.: 85.16%] [G loss: 1.163212]\n",
      "epoch:1 step:1002 [D loss: 0.107018, acc.: 89.06%] [G loss: 0.964417]\n",
      "epoch:1 step:1003 [D loss: 0.063939, acc.: 97.66%] [G loss: 1.033852]\n",
      "epoch:1 step:1004 [D loss: 0.112765, acc.: 86.72%] [G loss: 1.101203]\n",
      "epoch:1 step:1005 [D loss: 0.102804, acc.: 88.28%] [G loss: 1.088039]\n",
      "epoch:1 step:1006 [D loss: 0.068790, acc.: 96.09%] [G loss: 1.168232]\n",
      "epoch:1 step:1007 [D loss: 0.128500, acc.: 84.38%] [G loss: 1.034959]\n",
      "epoch:1 step:1008 [D loss: 0.078862, acc.: 92.19%] [G loss: 1.145082]\n",
      "epoch:1 step:1009 [D loss: 0.108811, acc.: 90.62%] [G loss: 1.028224]\n",
      "epoch:1 step:1010 [D loss: 0.053058, acc.: 98.44%] [G loss: 1.023544]\n",
      "epoch:1 step:1011 [D loss: 0.099381, acc.: 90.62%] [G loss: 1.114650]\n",
      "epoch:1 step:1012 [D loss: 0.142398, acc.: 83.59%] [G loss: 1.068476]\n",
      "epoch:1 step:1013 [D loss: 0.079371, acc.: 92.97%] [G loss: 1.084342]\n",
      "epoch:1 step:1014 [D loss: 0.068297, acc.: 92.19%] [G loss: 1.143919]\n",
      "epoch:1 step:1015 [D loss: 0.193077, acc.: 71.88%] [G loss: 0.959625]\n",
      "epoch:1 step:1016 [D loss: 0.085380, acc.: 89.06%] [G loss: 0.931719]\n",
      "epoch:1 step:1017 [D loss: 0.056018, acc.: 97.66%] [G loss: 1.062222]\n",
      "epoch:1 step:1018 [D loss: 0.195738, acc.: 69.53%] [G loss: 1.019206]\n",
      "epoch:1 step:1019 [D loss: 0.110999, acc.: 88.28%] [G loss: 0.973269]\n",
      "epoch:1 step:1020 [D loss: 0.058855, acc.: 96.09%] [G loss: 1.042250]\n",
      "epoch:1 step:1021 [D loss: 0.100149, acc.: 91.41%] [G loss: 1.159646]\n",
      "epoch:1 step:1022 [D loss: 0.105454, acc.: 85.94%] [G loss: 1.121590]\n",
      "epoch:1 step:1023 [D loss: 0.087951, acc.: 90.62%] [G loss: 0.947591]\n",
      "epoch:1 step:1024 [D loss: 0.090812, acc.: 94.53%] [G loss: 1.165529]\n",
      "epoch:1 step:1025 [D loss: 0.088950, acc.: 92.97%] [G loss: 0.910007]\n",
      "epoch:1 step:1026 [D loss: 0.112565, acc.: 88.28%] [G loss: 1.171241]\n",
      "epoch:1 step:1027 [D loss: 0.069180, acc.: 93.75%] [G loss: 1.047775]\n",
      "epoch:1 step:1028 [D loss: 0.115715, acc.: 87.50%] [G loss: 0.995774]\n",
      "epoch:1 step:1029 [D loss: 0.095557, acc.: 89.84%] [G loss: 1.071094]\n",
      "epoch:1 step:1030 [D loss: 0.093177, acc.: 91.41%] [G loss: 1.133732]\n",
      "epoch:1 step:1031 [D loss: 0.092990, acc.: 92.19%] [G loss: 1.152758]\n",
      "epoch:1 step:1032 [D loss: 0.088484, acc.: 89.84%] [G loss: 1.090005]\n",
      "epoch:1 step:1033 [D loss: 0.099656, acc.: 90.62%] [G loss: 1.145398]\n",
      "epoch:1 step:1034 [D loss: 0.148835, acc.: 80.47%] [G loss: 1.136621]\n",
      "epoch:1 step:1035 [D loss: 0.071810, acc.: 91.41%] [G loss: 1.089370]\n",
      "epoch:1 step:1036 [D loss: 0.155407, acc.: 76.56%] [G loss: 0.963632]\n",
      "epoch:1 step:1037 [D loss: 0.074821, acc.: 94.53%] [G loss: 1.163239]\n",
      "epoch:1 step:1038 [D loss: 0.130547, acc.: 83.59%] [G loss: 1.139027]\n",
      "epoch:1 step:1039 [D loss: 0.089518, acc.: 91.41%] [G loss: 1.001726]\n",
      "epoch:1 step:1040 [D loss: 0.064567, acc.: 95.31%] [G loss: 1.262498]\n",
      "epoch:1 step:1041 [D loss: 0.145881, acc.: 81.25%] [G loss: 0.981160]\n",
      "epoch:1 step:1042 [D loss: 0.105421, acc.: 85.16%] [G loss: 1.217288]\n",
      "epoch:1 step:1043 [D loss: 0.155202, acc.: 77.34%] [G loss: 0.965646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1044 [D loss: 0.097696, acc.: 89.84%] [G loss: 1.326779]\n",
      "epoch:1 step:1045 [D loss: 0.110132, acc.: 85.94%] [G loss: 1.075843]\n",
      "epoch:1 step:1046 [D loss: 0.095389, acc.: 89.84%] [G loss: 1.081520]\n",
      "epoch:1 step:1047 [D loss: 0.123228, acc.: 83.59%] [G loss: 0.962383]\n",
      "epoch:1 step:1048 [D loss: 0.123166, acc.: 85.16%] [G loss: 1.079179]\n",
      "epoch:1 step:1049 [D loss: 0.143518, acc.: 79.69%] [G loss: 1.172673]\n",
      "epoch:1 step:1050 [D loss: 0.110739, acc.: 87.50%] [G loss: 1.047932]\n",
      "epoch:1 step:1051 [D loss: 0.136858, acc.: 82.03%] [G loss: 1.274798]\n",
      "epoch:1 step:1052 [D loss: 0.114606, acc.: 88.28%] [G loss: 0.977298]\n",
      "epoch:1 step:1053 [D loss: 0.085563, acc.: 92.97%] [G loss: 1.133815]\n",
      "epoch:1 step:1054 [D loss: 0.191814, acc.: 72.66%] [G loss: 1.108799]\n",
      "epoch:1 step:1055 [D loss: 0.123231, acc.: 85.16%] [G loss: 1.145490]\n",
      "epoch:1 step:1056 [D loss: 0.124640, acc.: 85.94%] [G loss: 1.110873]\n",
      "epoch:1 step:1057 [D loss: 0.159323, acc.: 77.34%] [G loss: 1.060371]\n",
      "epoch:1 step:1058 [D loss: 0.145861, acc.: 78.12%] [G loss: 1.147540]\n",
      "epoch:1 step:1059 [D loss: 0.153115, acc.: 79.69%] [G loss: 1.026354]\n",
      "epoch:1 step:1060 [D loss: 0.096526, acc.: 89.84%] [G loss: 0.940663]\n",
      "epoch:1 step:1061 [D loss: 0.121245, acc.: 89.84%] [G loss: 1.137413]\n",
      "epoch:1 step:1062 [D loss: 0.105097, acc.: 90.62%] [G loss: 1.051763]\n",
      "epoch:1 step:1063 [D loss: 0.151299, acc.: 81.25%] [G loss: 1.053241]\n",
      "epoch:1 step:1064 [D loss: 0.097621, acc.: 89.06%] [G loss: 1.075749]\n",
      "epoch:1 step:1065 [D loss: 0.140962, acc.: 78.91%] [G loss: 1.013312]\n",
      "epoch:1 step:1066 [D loss: 0.081289, acc.: 92.97%] [G loss: 0.999368]\n",
      "epoch:1 step:1067 [D loss: 0.105594, acc.: 89.84%] [G loss: 0.997748]\n",
      "epoch:1 step:1068 [D loss: 0.121189, acc.: 85.94%] [G loss: 1.161548]\n",
      "epoch:1 step:1069 [D loss: 0.163536, acc.: 77.34%] [G loss: 0.953060]\n",
      "epoch:1 step:1070 [D loss: 0.054228, acc.: 96.88%] [G loss: 1.043561]\n",
      "epoch:1 step:1071 [D loss: 0.111752, acc.: 89.06%] [G loss: 1.082493]\n",
      "epoch:1 step:1072 [D loss: 0.118474, acc.: 86.72%] [G loss: 1.016477]\n",
      "epoch:1 step:1073 [D loss: 0.141255, acc.: 83.59%] [G loss: 1.044096]\n",
      "epoch:1 step:1074 [D loss: 0.090885, acc.: 88.28%] [G loss: 1.116178]\n",
      "epoch:1 step:1075 [D loss: 0.094110, acc.: 88.28%] [G loss: 1.017248]\n",
      "epoch:1 step:1076 [D loss: 0.196797, acc.: 67.19%] [G loss: 1.024376]\n",
      "epoch:1 step:1077 [D loss: 0.158334, acc.: 75.78%] [G loss: 0.911762]\n",
      "epoch:1 step:1078 [D loss: 0.082292, acc.: 93.75%] [G loss: 1.125191]\n",
      "epoch:1 step:1079 [D loss: 0.174401, acc.: 75.00%] [G loss: 0.886410]\n",
      "epoch:1 step:1080 [D loss: 0.168536, acc.: 75.78%] [G loss: 1.146328]\n",
      "epoch:1 step:1081 [D loss: 0.133658, acc.: 84.38%] [G loss: 0.992414]\n",
      "epoch:1 step:1082 [D loss: 0.118056, acc.: 85.16%] [G loss: 1.076799]\n",
      "epoch:1 step:1083 [D loss: 0.141276, acc.: 79.69%] [G loss: 1.017921]\n",
      "epoch:1 step:1084 [D loss: 0.115740, acc.: 85.94%] [G loss: 1.005236]\n",
      "epoch:1 step:1085 [D loss: 0.135967, acc.: 82.03%] [G loss: 1.139434]\n",
      "epoch:1 step:1086 [D loss: 0.178643, acc.: 75.78%] [G loss: 1.038634]\n",
      "epoch:1 step:1087 [D loss: 0.120174, acc.: 83.59%] [G loss: 1.051272]\n",
      "epoch:1 step:1088 [D loss: 0.116292, acc.: 89.06%] [G loss: 1.190355]\n",
      "epoch:1 step:1089 [D loss: 0.119752, acc.: 89.06%] [G loss: 1.102624]\n",
      "epoch:1 step:1090 [D loss: 0.169564, acc.: 79.69%] [G loss: 1.027858]\n",
      "epoch:1 step:1091 [D loss: 0.115990, acc.: 82.81%] [G loss: 1.007975]\n",
      "epoch:1 step:1092 [D loss: 0.107941, acc.: 83.59%] [G loss: 1.285150]\n",
      "epoch:1 step:1093 [D loss: 0.154623, acc.: 78.12%] [G loss: 1.033408]\n",
      "epoch:1 step:1094 [D loss: 0.084468, acc.: 96.09%] [G loss: 1.117231]\n",
      "epoch:1 step:1095 [D loss: 0.197644, acc.: 65.62%] [G loss: 1.115679]\n",
      "epoch:1 step:1096 [D loss: 0.112265, acc.: 87.50%] [G loss: 1.040383]\n",
      "epoch:1 step:1097 [D loss: 0.155191, acc.: 82.03%] [G loss: 1.116303]\n",
      "epoch:1 step:1098 [D loss: 0.090784, acc.: 92.97%] [G loss: 0.920879]\n",
      "epoch:1 step:1099 [D loss: 0.124813, acc.: 85.16%] [G loss: 1.245782]\n",
      "epoch:1 step:1100 [D loss: 0.139113, acc.: 85.16%] [G loss: 1.062998]\n",
      "epoch:1 step:1101 [D loss: 0.073791, acc.: 96.09%] [G loss: 0.952427]\n",
      "epoch:1 step:1102 [D loss: 0.138237, acc.: 86.72%] [G loss: 1.044737]\n",
      "epoch:1 step:1103 [D loss: 0.167118, acc.: 78.12%] [G loss: 0.987345]\n",
      "epoch:1 step:1104 [D loss: 0.151278, acc.: 77.34%] [G loss: 1.007542]\n",
      "epoch:1 step:1105 [D loss: 0.141917, acc.: 82.81%] [G loss: 1.006289]\n",
      "epoch:1 step:1106 [D loss: 0.099960, acc.: 91.41%] [G loss: 0.957494]\n",
      "epoch:1 step:1107 [D loss: 0.071825, acc.: 94.53%] [G loss: 1.040838]\n",
      "epoch:1 step:1108 [D loss: 0.129623, acc.: 86.72%] [G loss: 0.855897]\n",
      "epoch:1 step:1109 [D loss: 0.098940, acc.: 92.19%] [G loss: 0.981725]\n",
      "epoch:1 step:1110 [D loss: 0.104306, acc.: 90.62%] [G loss: 0.953968]\n",
      "epoch:1 step:1111 [D loss: 0.196544, acc.: 73.44%] [G loss: 0.801772]\n",
      "epoch:1 step:1112 [D loss: 0.080290, acc.: 94.53%] [G loss: 1.018604]\n",
      "epoch:1 step:1113 [D loss: 0.094142, acc.: 91.41%] [G loss: 0.891673]\n",
      "epoch:1 step:1114 [D loss: 0.083922, acc.: 94.53%] [G loss: 1.015866]\n",
      "epoch:1 step:1115 [D loss: 0.110327, acc.: 89.84%] [G loss: 1.006397]\n",
      "epoch:1 step:1116 [D loss: 0.113380, acc.: 87.50%] [G loss: 0.970122]\n",
      "epoch:1 step:1117 [D loss: 0.136917, acc.: 79.69%] [G loss: 1.027887]\n",
      "epoch:1 step:1118 [D loss: 0.107633, acc.: 90.62%] [G loss: 1.045519]\n",
      "epoch:1 step:1119 [D loss: 0.159729, acc.: 80.47%] [G loss: 0.892438]\n",
      "epoch:1 step:1120 [D loss: 0.097107, acc.: 91.41%] [G loss: 1.105163]\n",
      "epoch:1 step:1121 [D loss: 0.159232, acc.: 76.56%] [G loss: 0.891230]\n",
      "epoch:1 step:1122 [D loss: 0.108605, acc.: 89.84%] [G loss: 0.965895]\n",
      "epoch:1 step:1123 [D loss: 0.087511, acc.: 92.19%] [G loss: 0.999783]\n",
      "epoch:1 step:1124 [D loss: 0.126669, acc.: 88.28%] [G loss: 1.057546]\n",
      "epoch:1 step:1125 [D loss: 0.110858, acc.: 87.50%] [G loss: 0.958733]\n",
      "epoch:1 step:1126 [D loss: 0.141178, acc.: 85.94%] [G loss: 1.060215]\n",
      "epoch:1 step:1127 [D loss: 0.087524, acc.: 92.19%] [G loss: 0.997692]\n",
      "epoch:1 step:1128 [D loss: 0.105548, acc.: 91.41%] [G loss: 0.879981]\n",
      "epoch:1 step:1129 [D loss: 0.129438, acc.: 85.94%] [G loss: 0.993070]\n",
      "epoch:1 step:1130 [D loss: 0.121935, acc.: 88.28%] [G loss: 0.908840]\n",
      "epoch:1 step:1131 [D loss: 0.082808, acc.: 96.88%] [G loss: 1.037036]\n",
      "epoch:1 step:1132 [D loss: 0.153485, acc.: 83.59%] [G loss: 0.936938]\n",
      "epoch:1 step:1133 [D loss: 0.142471, acc.: 84.38%] [G loss: 1.030294]\n",
      "epoch:1 step:1134 [D loss: 0.127823, acc.: 85.16%] [G loss: 0.978667]\n",
      "epoch:1 step:1135 [D loss: 0.124358, acc.: 86.72%] [G loss: 0.982971]\n",
      "epoch:1 step:1136 [D loss: 0.123734, acc.: 87.50%] [G loss: 0.927696]\n",
      "epoch:1 step:1137 [D loss: 0.069798, acc.: 92.97%] [G loss: 1.038141]\n",
      "epoch:1 step:1138 [D loss: 0.114301, acc.: 90.62%] [G loss: 0.898401]\n",
      "epoch:1 step:1139 [D loss: 0.114788, acc.: 86.72%] [G loss: 1.024480]\n",
      "epoch:1 step:1140 [D loss: 0.130057, acc.: 85.94%] [G loss: 0.943699]\n",
      "epoch:1 step:1141 [D loss: 0.090721, acc.: 91.41%] [G loss: 1.036057]\n",
      "epoch:1 step:1142 [D loss: 0.101243, acc.: 90.62%] [G loss: 1.043449]\n",
      "epoch:1 step:1143 [D loss: 0.129144, acc.: 86.72%] [G loss: 1.032517]\n",
      "epoch:1 step:1144 [D loss: 0.153237, acc.: 80.47%] [G loss: 0.916354]\n",
      "epoch:1 step:1145 [D loss: 0.120668, acc.: 85.94%] [G loss: 0.991293]\n",
      "epoch:1 step:1146 [D loss: 0.106179, acc.: 88.28%] [G loss: 1.036707]\n",
      "epoch:1 step:1147 [D loss: 0.156852, acc.: 76.56%] [G loss: 1.120400]\n",
      "epoch:1 step:1148 [D loss: 0.107896, acc.: 89.84%] [G loss: 1.045959]\n",
      "epoch:1 step:1149 [D loss: 0.145302, acc.: 80.47%] [G loss: 1.020046]\n",
      "epoch:1 step:1150 [D loss: 0.104445, acc.: 88.28%] [G loss: 0.990680]\n",
      "epoch:1 step:1151 [D loss: 0.261798, acc.: 54.69%] [G loss: 1.012817]\n",
      "epoch:1 step:1152 [D loss: 0.124593, acc.: 79.69%] [G loss: 0.958912]\n",
      "epoch:1 step:1153 [D loss: 0.144273, acc.: 78.12%] [G loss: 0.942224]\n",
      "epoch:1 step:1154 [D loss: 0.090550, acc.: 93.75%] [G loss: 0.965971]\n",
      "epoch:1 step:1155 [D loss: 0.111413, acc.: 84.38%] [G loss: 0.974281]\n",
      "epoch:1 step:1156 [D loss: 0.148410, acc.: 79.69%] [G loss: 0.903575]\n",
      "epoch:1 step:1157 [D loss: 0.199056, acc.: 67.19%] [G loss: 0.958679]\n",
      "epoch:1 step:1158 [D loss: 0.076743, acc.: 95.31%] [G loss: 1.081944]\n",
      "epoch:1 step:1159 [D loss: 0.122807, acc.: 85.94%] [G loss: 0.871025]\n",
      "epoch:1 step:1160 [D loss: 0.117242, acc.: 87.50%] [G loss: 1.048279]\n",
      "epoch:1 step:1161 [D loss: 0.180703, acc.: 75.00%] [G loss: 0.918943]\n",
      "epoch:1 step:1162 [D loss: 0.105338, acc.: 89.06%] [G loss: 1.032139]\n",
      "epoch:1 step:1163 [D loss: 0.195987, acc.: 70.31%] [G loss: 0.897217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1164 [D loss: 0.156524, acc.: 82.03%] [G loss: 0.967992]\n",
      "epoch:1 step:1165 [D loss: 0.142826, acc.: 78.91%] [G loss: 0.957477]\n",
      "epoch:1 step:1166 [D loss: 0.138423, acc.: 82.03%] [G loss: 0.918028]\n",
      "epoch:1 step:1167 [D loss: 0.099695, acc.: 90.62%] [G loss: 0.948470]\n",
      "epoch:1 step:1168 [D loss: 0.136707, acc.: 82.03%] [G loss: 0.862259]\n",
      "epoch:1 step:1169 [D loss: 0.116754, acc.: 85.94%] [G loss: 0.984122]\n",
      "epoch:1 step:1170 [D loss: 0.161296, acc.: 78.91%] [G loss: 0.953334]\n",
      "epoch:1 step:1171 [D loss: 0.153235, acc.: 80.47%] [G loss: 0.851599]\n",
      "epoch:1 step:1172 [D loss: 0.169247, acc.: 77.34%] [G loss: 0.984668]\n",
      "epoch:1 step:1173 [D loss: 0.118146, acc.: 85.94%] [G loss: 0.863604]\n",
      "epoch:1 step:1174 [D loss: 0.196859, acc.: 75.00%] [G loss: 0.961138]\n",
      "epoch:1 step:1175 [D loss: 0.116748, acc.: 91.41%] [G loss: 0.890075]\n",
      "epoch:1 step:1176 [D loss: 0.194897, acc.: 70.31%] [G loss: 0.909288]\n",
      "epoch:1 step:1177 [D loss: 0.100372, acc.: 88.28%] [G loss: 1.023997]\n",
      "epoch:1 step:1178 [D loss: 0.223548, acc.: 60.94%] [G loss: 0.901015]\n",
      "epoch:1 step:1179 [D loss: 0.088853, acc.: 95.31%] [G loss: 0.889303]\n",
      "epoch:1 step:1180 [D loss: 0.180705, acc.: 74.22%] [G loss: 0.815944]\n",
      "epoch:1 step:1181 [D loss: 0.122696, acc.: 88.28%] [G loss: 0.947804]\n",
      "epoch:1 step:1182 [D loss: 0.189371, acc.: 75.00%] [G loss: 1.014259]\n",
      "epoch:1 step:1183 [D loss: 0.142299, acc.: 79.69%] [G loss: 0.961203]\n",
      "epoch:1 step:1184 [D loss: 0.159607, acc.: 69.53%] [G loss: 1.025604]\n",
      "epoch:1 step:1185 [D loss: 0.135142, acc.: 82.81%] [G loss: 0.937618]\n",
      "epoch:1 step:1186 [D loss: 0.157061, acc.: 79.69%] [G loss: 0.915105]\n",
      "epoch:1 step:1187 [D loss: 0.126864, acc.: 85.16%] [G loss: 1.049610]\n",
      "epoch:1 step:1188 [D loss: 0.218075, acc.: 67.19%] [G loss: 0.878561]\n",
      "epoch:1 step:1189 [D loss: 0.161311, acc.: 78.12%] [G loss: 0.903408]\n",
      "epoch:1 step:1190 [D loss: 0.127896, acc.: 87.50%] [G loss: 0.936219]\n",
      "epoch:1 step:1191 [D loss: 0.153686, acc.: 82.81%] [G loss: 0.902426]\n",
      "epoch:1 step:1192 [D loss: 0.129012, acc.: 88.28%] [G loss: 1.004228]\n",
      "epoch:1 step:1193 [D loss: 0.167656, acc.: 76.56%] [G loss: 0.910476]\n",
      "epoch:1 step:1194 [D loss: 0.119630, acc.: 85.16%] [G loss: 0.903890]\n",
      "epoch:1 step:1195 [D loss: 0.181317, acc.: 73.44%] [G loss: 0.889955]\n",
      "epoch:1 step:1196 [D loss: 0.090765, acc.: 92.97%] [G loss: 0.967880]\n",
      "epoch:1 step:1197 [D loss: 0.135836, acc.: 87.50%] [G loss: 0.892943]\n",
      "epoch:1 step:1198 [D loss: 0.127332, acc.: 86.72%] [G loss: 0.967149]\n",
      "epoch:1 step:1199 [D loss: 0.166575, acc.: 74.22%] [G loss: 0.906260]\n",
      "epoch:1 step:1200 [D loss: 0.246141, acc.: 58.59%] [G loss: 0.879772]\n",
      "##############\n",
      "[5.1357423  3.43501822 8.76340165 6.64976598 5.77419793 8.26131279\n",
      " 7.66554518 7.16481344 6.98651061 5.30791487]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.101183, acc.: 89.06%] [G loss: 0.908876]\n",
      "epoch:1 step:1202 [D loss: 0.134700, acc.: 86.72%] [G loss: 0.946002]\n",
      "epoch:1 step:1203 [D loss: 0.163305, acc.: 80.47%] [G loss: 0.832945]\n",
      "epoch:1 step:1204 [D loss: 0.107715, acc.: 92.19%] [G loss: 1.031918]\n",
      "epoch:1 step:1205 [D loss: 0.130073, acc.: 82.81%] [G loss: 0.945594]\n",
      "epoch:1 step:1206 [D loss: 0.208170, acc.: 65.62%] [G loss: 0.789293]\n",
      "epoch:1 step:1207 [D loss: 0.124321, acc.: 87.50%] [G loss: 0.895338]\n",
      "epoch:1 step:1208 [D loss: 0.152035, acc.: 79.69%] [G loss: 0.830053]\n",
      "epoch:1 step:1209 [D loss: 0.099498, acc.: 88.28%] [G loss: 1.019163]\n",
      "epoch:1 step:1210 [D loss: 0.153596, acc.: 78.91%] [G loss: 0.879615]\n",
      "epoch:1 step:1211 [D loss: 0.103266, acc.: 92.19%] [G loss: 0.930980]\n",
      "epoch:1 step:1212 [D loss: 0.247392, acc.: 60.16%] [G loss: 0.741902]\n",
      "epoch:1 step:1213 [D loss: 0.111916, acc.: 89.06%] [G loss: 1.107549]\n",
      "epoch:1 step:1214 [D loss: 0.182917, acc.: 74.22%] [G loss: 0.863462]\n",
      "epoch:1 step:1215 [D loss: 0.082177, acc.: 94.53%] [G loss: 0.967705]\n",
      "epoch:1 step:1216 [D loss: 0.162937, acc.: 77.34%] [G loss: 0.911550]\n",
      "epoch:1 step:1217 [D loss: 0.124757, acc.: 85.94%] [G loss: 0.821677]\n",
      "epoch:1 step:1218 [D loss: 0.206036, acc.: 69.53%] [G loss: 0.874319]\n",
      "epoch:1 step:1219 [D loss: 0.136915, acc.: 88.28%] [G loss: 0.886967]\n",
      "epoch:1 step:1220 [D loss: 0.121787, acc.: 86.72%] [G loss: 0.851604]\n",
      "epoch:1 step:1221 [D loss: 0.119729, acc.: 91.41%] [G loss: 0.852446]\n",
      "epoch:1 step:1222 [D loss: 0.114491, acc.: 92.19%] [G loss: 0.984407]\n",
      "epoch:1 step:1223 [D loss: 0.203199, acc.: 68.75%] [G loss: 0.852449]\n",
      "epoch:1 step:1224 [D loss: 0.093473, acc.: 91.41%] [G loss: 0.961230]\n",
      "epoch:1 step:1225 [D loss: 0.131479, acc.: 84.38%] [G loss: 0.909007]\n",
      "epoch:1 step:1226 [D loss: 0.129487, acc.: 89.84%] [G loss: 0.921019]\n",
      "epoch:1 step:1227 [D loss: 0.170538, acc.: 78.12%] [G loss: 0.898592]\n",
      "epoch:1 step:1228 [D loss: 0.187000, acc.: 71.09%] [G loss: 0.917291]\n",
      "epoch:1 step:1229 [D loss: 0.151820, acc.: 77.34%] [G loss: 0.978237]\n",
      "epoch:1 step:1230 [D loss: 0.132948, acc.: 84.38%] [G loss: 0.842176]\n",
      "epoch:1 step:1231 [D loss: 0.098485, acc.: 91.41%] [G loss: 0.955834]\n",
      "epoch:1 step:1232 [D loss: 0.137331, acc.: 85.94%] [G loss: 1.031099]\n",
      "epoch:1 step:1233 [D loss: 0.109009, acc.: 92.19%] [G loss: 1.032995]\n",
      "epoch:1 step:1234 [D loss: 0.140927, acc.: 83.59%] [G loss: 1.007204]\n",
      "epoch:1 step:1235 [D loss: 0.121161, acc.: 86.72%] [G loss: 0.987273]\n",
      "epoch:1 step:1236 [D loss: 0.150002, acc.: 82.03%] [G loss: 1.109948]\n",
      "epoch:1 step:1237 [D loss: 0.129328, acc.: 87.50%] [G loss: 0.986199]\n",
      "epoch:1 step:1238 [D loss: 0.227955, acc.: 68.75%] [G loss: 0.846488]\n",
      "epoch:1 step:1239 [D loss: 0.131082, acc.: 86.72%] [G loss: 0.958575]\n",
      "epoch:1 step:1240 [D loss: 0.156012, acc.: 82.81%] [G loss: 0.868704]\n",
      "epoch:1 step:1241 [D loss: 0.117860, acc.: 89.84%] [G loss: 0.962017]\n",
      "epoch:1 step:1242 [D loss: 0.145624, acc.: 85.16%] [G loss: 0.845857]\n",
      "epoch:1 step:1243 [D loss: 0.099761, acc.: 92.19%] [G loss: 0.989677]\n",
      "epoch:1 step:1244 [D loss: 0.145066, acc.: 83.59%] [G loss: 0.913734]\n",
      "epoch:1 step:1245 [D loss: 0.124186, acc.: 87.50%] [G loss: 0.911282]\n",
      "epoch:1 step:1246 [D loss: 0.116521, acc.: 89.84%] [G loss: 0.933731]\n",
      "epoch:1 step:1247 [D loss: 0.119127, acc.: 88.28%] [G loss: 1.045585]\n",
      "epoch:1 step:1248 [D loss: 0.191384, acc.: 78.12%] [G loss: 0.942221]\n",
      "epoch:1 step:1249 [D loss: 0.072165, acc.: 92.97%] [G loss: 1.021903]\n",
      "epoch:1 step:1250 [D loss: 0.146719, acc.: 78.91%] [G loss: 0.844903]\n",
      "epoch:1 step:1251 [D loss: 0.100417, acc.: 89.84%] [G loss: 1.068819]\n",
      "epoch:1 step:1252 [D loss: 0.122872, acc.: 83.59%] [G loss: 1.005650]\n",
      "epoch:1 step:1253 [D loss: 0.234196, acc.: 60.16%] [G loss: 0.990864]\n",
      "epoch:1 step:1254 [D loss: 0.124895, acc.: 84.38%] [G loss: 1.065047]\n",
      "epoch:1 step:1255 [D loss: 0.180972, acc.: 75.78%] [G loss: 0.901979]\n",
      "epoch:1 step:1256 [D loss: 0.146590, acc.: 83.59%] [G loss: 1.089948]\n",
      "epoch:1 step:1257 [D loss: 0.150458, acc.: 84.38%] [G loss: 1.019837]\n",
      "epoch:1 step:1258 [D loss: 0.152863, acc.: 78.91%] [G loss: 1.079880]\n",
      "epoch:1 step:1259 [D loss: 0.155711, acc.: 76.56%] [G loss: 1.098426]\n",
      "epoch:1 step:1260 [D loss: 0.147500, acc.: 82.81%] [G loss: 0.944944]\n",
      "epoch:1 step:1261 [D loss: 0.098412, acc.: 92.19%] [G loss: 1.038241]\n",
      "epoch:1 step:1262 [D loss: 0.230981, acc.: 64.06%] [G loss: 0.832869]\n",
      "epoch:1 step:1263 [D loss: 0.179053, acc.: 75.78%] [G loss: 0.967761]\n",
      "epoch:1 step:1264 [D loss: 0.144689, acc.: 82.81%] [G loss: 0.953185]\n",
      "epoch:1 step:1265 [D loss: 0.188913, acc.: 75.00%] [G loss: 0.983521]\n",
      "epoch:1 step:1266 [D loss: 0.191639, acc.: 72.66%] [G loss: 1.034508]\n",
      "epoch:1 step:1267 [D loss: 0.161312, acc.: 75.78%] [G loss: 0.936420]\n",
      "epoch:1 step:1268 [D loss: 0.128763, acc.: 85.94%] [G loss: 0.946555]\n",
      "epoch:1 step:1269 [D loss: 0.153559, acc.: 82.81%] [G loss: 0.993988]\n",
      "epoch:1 step:1270 [D loss: 0.240099, acc.: 62.50%] [G loss: 1.031668]\n",
      "epoch:1 step:1271 [D loss: 0.182224, acc.: 74.22%] [G loss: 0.940938]\n",
      "epoch:1 step:1272 [D loss: 0.131234, acc.: 85.94%] [G loss: 0.950702]\n",
      "epoch:1 step:1273 [D loss: 0.122238, acc.: 89.84%] [G loss: 0.924974]\n",
      "epoch:1 step:1274 [D loss: 0.109311, acc.: 94.53%] [G loss: 0.887600]\n",
      "epoch:1 step:1275 [D loss: 0.139623, acc.: 85.16%] [G loss: 1.034861]\n",
      "epoch:1 step:1276 [D loss: 0.132280, acc.: 85.16%] [G loss: 0.936988]\n",
      "epoch:1 step:1277 [D loss: 0.187320, acc.: 78.91%] [G loss: 0.931850]\n",
      "epoch:1 step:1278 [D loss: 0.245573, acc.: 64.06%] [G loss: 0.845002]\n",
      "epoch:1 step:1279 [D loss: 0.109316, acc.: 85.16%] [G loss: 1.057365]\n",
      "epoch:1 step:1280 [D loss: 0.137020, acc.: 82.03%] [G loss: 0.872774]\n",
      "epoch:1 step:1281 [D loss: 0.138341, acc.: 86.72%] [G loss: 0.807900]\n",
      "epoch:1 step:1282 [D loss: 0.206220, acc.: 65.62%] [G loss: 0.814828]\n",
      "epoch:1 step:1283 [D loss: 0.145596, acc.: 82.03%] [G loss: 0.885057]\n",
      "epoch:1 step:1284 [D loss: 0.127165, acc.: 81.25%] [G loss: 0.913156]\n",
      "epoch:1 step:1285 [D loss: 0.194733, acc.: 71.88%] [G loss: 0.832938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1286 [D loss: 0.179972, acc.: 73.44%] [G loss: 0.877991]\n",
      "epoch:1 step:1287 [D loss: 0.084371, acc.: 94.53%] [G loss: 0.965441]\n",
      "epoch:1 step:1288 [D loss: 0.182301, acc.: 74.22%] [G loss: 0.834578]\n",
      "epoch:1 step:1289 [D loss: 0.131841, acc.: 86.72%] [G loss: 0.891587]\n",
      "epoch:1 step:1290 [D loss: 0.172397, acc.: 71.88%] [G loss: 0.844861]\n",
      "epoch:1 step:1291 [D loss: 0.094016, acc.: 92.97%] [G loss: 1.008751]\n",
      "epoch:1 step:1292 [D loss: 0.175665, acc.: 72.66%] [G loss: 0.829423]\n",
      "epoch:1 step:1293 [D loss: 0.133680, acc.: 84.38%] [G loss: 0.880681]\n",
      "epoch:1 step:1294 [D loss: 0.131895, acc.: 85.16%] [G loss: 0.824802]\n",
      "epoch:1 step:1295 [D loss: 0.115983, acc.: 88.28%] [G loss: 0.869058]\n",
      "epoch:1 step:1296 [D loss: 0.155261, acc.: 79.69%] [G loss: 0.845779]\n",
      "epoch:1 step:1297 [D loss: 0.118037, acc.: 86.72%] [G loss: 0.971000]\n",
      "epoch:1 step:1298 [D loss: 0.149198, acc.: 81.25%] [G loss: 0.843669]\n",
      "epoch:1 step:1299 [D loss: 0.133892, acc.: 84.38%] [G loss: 0.882260]\n",
      "epoch:1 step:1300 [D loss: 0.189510, acc.: 68.75%] [G loss: 0.877356]\n",
      "epoch:1 step:1301 [D loss: 0.112677, acc.: 87.50%] [G loss: 0.847589]\n",
      "epoch:1 step:1302 [D loss: 0.157482, acc.: 78.91%] [G loss: 0.889304]\n",
      "epoch:1 step:1303 [D loss: 0.111385, acc.: 88.28%] [G loss: 0.879439]\n",
      "epoch:1 step:1304 [D loss: 0.188351, acc.: 75.00%] [G loss: 0.805936]\n",
      "epoch:1 step:1305 [D loss: 0.086104, acc.: 93.75%] [G loss: 0.966894]\n",
      "epoch:1 step:1306 [D loss: 0.173732, acc.: 81.25%] [G loss: 0.783897]\n",
      "epoch:1 step:1307 [D loss: 0.165776, acc.: 80.47%] [G loss: 0.885881]\n",
      "epoch:1 step:1308 [D loss: 0.115313, acc.: 90.62%] [G loss: 0.890412]\n",
      "epoch:1 step:1309 [D loss: 0.165176, acc.: 82.03%] [G loss: 0.845947]\n",
      "epoch:1 step:1310 [D loss: 0.114521, acc.: 89.06%] [G loss: 0.853925]\n",
      "epoch:1 step:1311 [D loss: 0.163343, acc.: 82.81%] [G loss: 0.789741]\n",
      "epoch:1 step:1312 [D loss: 0.118116, acc.: 87.50%] [G loss: 0.899034]\n",
      "epoch:1 step:1313 [D loss: 0.112353, acc.: 88.28%] [G loss: 0.860041]\n",
      "epoch:1 step:1314 [D loss: 0.115046, acc.: 85.94%] [G loss: 0.882545]\n",
      "epoch:1 step:1315 [D loss: 0.104854, acc.: 87.50%] [G loss: 0.996328]\n",
      "epoch:1 step:1316 [D loss: 0.177867, acc.: 73.44%] [G loss: 0.807665]\n",
      "epoch:1 step:1317 [D loss: 0.174863, acc.: 79.69%] [G loss: 0.828037]\n",
      "epoch:1 step:1318 [D loss: 0.104032, acc.: 93.75%] [G loss: 0.944813]\n",
      "epoch:1 step:1319 [D loss: 0.157416, acc.: 82.03%] [G loss: 0.862014]\n",
      "epoch:1 step:1320 [D loss: 0.123502, acc.: 88.28%] [G loss: 0.924429]\n",
      "epoch:1 step:1321 [D loss: 0.111767, acc.: 91.41%] [G loss: 0.882654]\n",
      "epoch:1 step:1322 [D loss: 0.157450, acc.: 80.47%] [G loss: 0.780593]\n",
      "epoch:1 step:1323 [D loss: 0.115576, acc.: 89.84%] [G loss: 0.973041]\n",
      "epoch:1 step:1324 [D loss: 0.132070, acc.: 85.94%] [G loss: 0.857782]\n",
      "epoch:1 step:1325 [D loss: 0.096071, acc.: 90.62%] [G loss: 0.932895]\n",
      "epoch:1 step:1326 [D loss: 0.170274, acc.: 76.56%] [G loss: 0.886534]\n",
      "epoch:1 step:1327 [D loss: 0.216319, acc.: 65.62%] [G loss: 0.825818]\n",
      "epoch:1 step:1328 [D loss: 0.106752, acc.: 87.50%] [G loss: 0.957386]\n",
      "epoch:1 step:1329 [D loss: 0.127650, acc.: 84.38%] [G loss: 0.911168]\n",
      "epoch:1 step:1330 [D loss: 0.100335, acc.: 92.19%] [G loss: 0.944802]\n",
      "epoch:1 step:1331 [D loss: 0.153172, acc.: 78.12%] [G loss: 0.953212]\n",
      "epoch:1 step:1332 [D loss: 0.123848, acc.: 85.16%] [G loss: 0.921275]\n",
      "epoch:1 step:1333 [D loss: 0.184538, acc.: 76.56%] [G loss: 0.800200]\n",
      "epoch:1 step:1334 [D loss: 0.061605, acc.: 96.88%] [G loss: 1.105232]\n",
      "epoch:1 step:1335 [D loss: 0.120242, acc.: 89.06%] [G loss: 0.804833]\n",
      "epoch:1 step:1336 [D loss: 0.084358, acc.: 93.75%] [G loss: 0.940476]\n",
      "epoch:1 step:1337 [D loss: 0.215100, acc.: 73.44%] [G loss: 0.741507]\n",
      "epoch:1 step:1338 [D loss: 0.115717, acc.: 91.41%] [G loss: 0.886177]\n",
      "epoch:1 step:1339 [D loss: 0.086585, acc.: 92.19%] [G loss: 0.918380]\n",
      "epoch:1 step:1340 [D loss: 0.187714, acc.: 75.00%] [G loss: 0.801183]\n",
      "epoch:1 step:1341 [D loss: 0.098813, acc.: 89.06%] [G loss: 0.920531]\n",
      "epoch:1 step:1342 [D loss: 0.125508, acc.: 85.94%] [G loss: 0.866163]\n",
      "epoch:1 step:1343 [D loss: 0.124571, acc.: 85.16%] [G loss: 0.905581]\n",
      "epoch:1 step:1344 [D loss: 0.142834, acc.: 82.81%] [G loss: 0.880084]\n",
      "epoch:1 step:1345 [D loss: 0.105599, acc.: 91.41%] [G loss: 0.990826]\n",
      "epoch:1 step:1346 [D loss: 0.125116, acc.: 84.38%] [G loss: 0.992780]\n",
      "epoch:1 step:1347 [D loss: 0.131072, acc.: 82.81%] [G loss: 0.913836]\n",
      "epoch:1 step:1348 [D loss: 0.130135, acc.: 87.50%] [G loss: 0.931199]\n",
      "epoch:1 step:1349 [D loss: 0.140063, acc.: 83.59%] [G loss: 0.893765]\n",
      "epoch:1 step:1350 [D loss: 0.087804, acc.: 92.19%] [G loss: 0.877125]\n",
      "epoch:1 step:1351 [D loss: 0.206873, acc.: 69.53%] [G loss: 0.793603]\n",
      "epoch:1 step:1352 [D loss: 0.092665, acc.: 93.75%] [G loss: 1.068724]\n",
      "epoch:1 step:1353 [D loss: 0.164191, acc.: 84.38%] [G loss: 0.896716]\n",
      "epoch:1 step:1354 [D loss: 0.079507, acc.: 92.97%] [G loss: 0.972108]\n",
      "epoch:1 step:1355 [D loss: 0.123840, acc.: 87.50%] [G loss: 0.937235]\n",
      "epoch:1 step:1356 [D loss: 0.069305, acc.: 96.09%] [G loss: 0.966147]\n",
      "epoch:1 step:1357 [D loss: 0.106445, acc.: 84.38%] [G loss: 0.951816]\n",
      "epoch:1 step:1358 [D loss: 0.155640, acc.: 77.34%] [G loss: 0.818695]\n",
      "epoch:1 step:1359 [D loss: 0.114960, acc.: 85.94%] [G loss: 0.988963]\n",
      "epoch:1 step:1360 [D loss: 0.194853, acc.: 75.00%] [G loss: 0.807615]\n",
      "epoch:1 step:1361 [D loss: 0.095876, acc.: 92.19%] [G loss: 0.919811]\n",
      "epoch:1 step:1362 [D loss: 0.131990, acc.: 85.94%] [G loss: 0.888299]\n",
      "epoch:1 step:1363 [D loss: 0.095830, acc.: 92.97%] [G loss: 0.963815]\n",
      "epoch:1 step:1364 [D loss: 0.113589, acc.: 89.06%] [G loss: 0.971170]\n",
      "epoch:1 step:1365 [D loss: 0.106289, acc.: 89.06%] [G loss: 0.978103]\n",
      "epoch:1 step:1366 [D loss: 0.167375, acc.: 75.78%] [G loss: 0.914345]\n",
      "epoch:1 step:1367 [D loss: 0.115372, acc.: 84.38%] [G loss: 1.039183]\n",
      "epoch:1 step:1368 [D loss: 0.112164, acc.: 87.50%] [G loss: 0.849779]\n",
      "epoch:1 step:1369 [D loss: 0.140146, acc.: 81.25%] [G loss: 0.987441]\n",
      "epoch:1 step:1370 [D loss: 0.126938, acc.: 85.16%] [G loss: 0.927176]\n",
      "epoch:1 step:1371 [D loss: 0.141350, acc.: 85.94%] [G loss: 0.871082]\n",
      "epoch:1 step:1372 [D loss: 0.130880, acc.: 84.38%] [G loss: 0.960451]\n",
      "epoch:1 step:1373 [D loss: 0.118009, acc.: 89.06%] [G loss: 0.943025]\n",
      "epoch:1 step:1374 [D loss: 0.108182, acc.: 89.06%] [G loss: 0.984542]\n",
      "epoch:1 step:1375 [D loss: 0.143569, acc.: 82.03%] [G loss: 0.933389]\n",
      "epoch:1 step:1376 [D loss: 0.113527, acc.: 89.06%] [G loss: 0.903491]\n",
      "epoch:1 step:1377 [D loss: 0.140647, acc.: 82.81%] [G loss: 0.910721]\n",
      "epoch:1 step:1378 [D loss: 0.127592, acc.: 84.38%] [G loss: 0.866742]\n",
      "epoch:1 step:1379 [D loss: 0.095433, acc.: 89.84%] [G loss: 0.957170]\n",
      "epoch:1 step:1380 [D loss: 0.120950, acc.: 85.94%] [G loss: 0.880015]\n",
      "epoch:1 step:1381 [D loss: 0.130126, acc.: 85.16%] [G loss: 0.911148]\n",
      "epoch:1 step:1382 [D loss: 0.111993, acc.: 89.06%] [G loss: 0.927822]\n",
      "epoch:1 step:1383 [D loss: 0.101854, acc.: 89.84%] [G loss: 0.937967]\n",
      "epoch:1 step:1384 [D loss: 0.083882, acc.: 94.53%] [G loss: 0.949767]\n",
      "epoch:1 step:1385 [D loss: 0.143347, acc.: 82.81%] [G loss: 0.856838]\n",
      "epoch:1 step:1386 [D loss: 0.156336, acc.: 83.59%] [G loss: 0.815478]\n",
      "epoch:1 step:1387 [D loss: 0.082977, acc.: 92.19%] [G loss: 0.902820]\n",
      "epoch:1 step:1388 [D loss: 0.129253, acc.: 86.72%] [G loss: 0.876179]\n",
      "epoch:1 step:1389 [D loss: 0.125617, acc.: 86.72%] [G loss: 0.924059]\n",
      "epoch:1 step:1390 [D loss: 0.116840, acc.: 89.84%] [G loss: 0.881100]\n",
      "epoch:1 step:1391 [D loss: 0.131509, acc.: 85.94%] [G loss: 0.847847]\n",
      "epoch:1 step:1392 [D loss: 0.159050, acc.: 78.91%] [G loss: 0.933526]\n",
      "epoch:1 step:1393 [D loss: 0.160811, acc.: 75.78%] [G loss: 0.864894]\n",
      "epoch:1 step:1394 [D loss: 0.131936, acc.: 83.59%] [G loss: 0.896221]\n",
      "epoch:1 step:1395 [D loss: 0.111793, acc.: 89.06%] [G loss: 1.021888]\n",
      "epoch:1 step:1396 [D loss: 0.121690, acc.: 85.16%] [G loss: 0.864259]\n",
      "epoch:1 step:1397 [D loss: 0.101804, acc.: 89.84%] [G loss: 0.931339]\n",
      "epoch:1 step:1398 [D loss: 0.113845, acc.: 87.50%] [G loss: 0.902014]\n",
      "epoch:1 step:1399 [D loss: 0.178267, acc.: 71.88%] [G loss: 0.788061]\n",
      "epoch:1 step:1400 [D loss: 0.136596, acc.: 81.25%] [G loss: 0.963215]\n",
      "##############\n",
      "[4.83641146 3.92237227 8.75703018 6.51608675 6.06600484 7.32383249\n",
      " 6.84001051 6.25837074 6.9042851  5.39647078]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.133772, acc.: 82.81%] [G loss: 0.883126]\n",
      "epoch:1 step:1402 [D loss: 0.175039, acc.: 78.91%] [G loss: 0.788092]\n",
      "epoch:1 step:1403 [D loss: 0.084862, acc.: 92.97%] [G loss: 0.988336]\n",
      "epoch:1 step:1404 [D loss: 0.172260, acc.: 78.91%] [G loss: 0.778000]\n",
      "epoch:1 step:1405 [D loss: 0.113296, acc.: 91.41%] [G loss: 0.930071]\n",
      "epoch:1 step:1406 [D loss: 0.162485, acc.: 78.12%] [G loss: 0.965632]\n",
      "epoch:1 step:1407 [D loss: 0.098986, acc.: 89.84%] [G loss: 0.944355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1408 [D loss: 0.105453, acc.: 89.84%] [G loss: 0.902369]\n",
      "epoch:1 step:1409 [D loss: 0.116251, acc.: 90.62%] [G loss: 0.884395]\n",
      "epoch:1 step:1410 [D loss: 0.172814, acc.: 75.00%] [G loss: 0.801231]\n",
      "epoch:1 step:1411 [D loss: 0.096276, acc.: 89.84%] [G loss: 0.906178]\n",
      "epoch:1 step:1412 [D loss: 0.115680, acc.: 86.72%] [G loss: 0.880271]\n",
      "epoch:1 step:1413 [D loss: 0.130204, acc.: 82.81%] [G loss: 0.952059]\n",
      "epoch:1 step:1414 [D loss: 0.160285, acc.: 78.91%] [G loss: 0.773986]\n",
      "epoch:1 step:1415 [D loss: 0.146548, acc.: 82.81%] [G loss: 0.966136]\n",
      "epoch:1 step:1416 [D loss: 0.192951, acc.: 70.31%] [G loss: 0.868994]\n",
      "epoch:1 step:1417 [D loss: 0.110243, acc.: 89.06%] [G loss: 0.907708]\n",
      "epoch:1 step:1418 [D loss: 0.131995, acc.: 83.59%] [G loss: 0.851118]\n",
      "epoch:1 step:1419 [D loss: 0.098063, acc.: 91.41%] [G loss: 0.882671]\n",
      "epoch:1 step:1420 [D loss: 0.176245, acc.: 75.00%] [G loss: 0.774148]\n",
      "epoch:1 step:1421 [D loss: 0.093327, acc.: 88.28%] [G loss: 0.964269]\n",
      "epoch:1 step:1422 [D loss: 0.127850, acc.: 86.72%] [G loss: 0.773078]\n",
      "epoch:1 step:1423 [D loss: 0.141303, acc.: 85.94%] [G loss: 0.893414]\n",
      "epoch:1 step:1424 [D loss: 0.108897, acc.: 89.06%] [G loss: 0.940698]\n",
      "epoch:1 step:1425 [D loss: 0.125528, acc.: 90.62%] [G loss: 0.788392]\n",
      "epoch:1 step:1426 [D loss: 0.118439, acc.: 91.41%] [G loss: 0.901236]\n",
      "epoch:1 step:1427 [D loss: 0.113526, acc.: 86.72%] [G loss: 0.948132]\n",
      "epoch:1 step:1428 [D loss: 0.116309, acc.: 88.28%] [G loss: 0.809648]\n",
      "epoch:1 step:1429 [D loss: 0.126819, acc.: 84.38%] [G loss: 0.930017]\n",
      "epoch:1 step:1430 [D loss: 0.152421, acc.: 77.34%] [G loss: 0.766585]\n",
      "epoch:1 step:1431 [D loss: 0.123646, acc.: 88.28%] [G loss: 0.905844]\n",
      "epoch:1 step:1432 [D loss: 0.122472, acc.: 85.94%] [G loss: 0.840174]\n",
      "epoch:1 step:1433 [D loss: 0.145063, acc.: 85.16%] [G loss: 0.831887]\n",
      "epoch:1 step:1434 [D loss: 0.100598, acc.: 91.41%] [G loss: 0.890550]\n",
      "epoch:1 step:1435 [D loss: 0.108435, acc.: 90.62%] [G loss: 0.883647]\n",
      "epoch:1 step:1436 [D loss: 0.138869, acc.: 83.59%] [G loss: 0.828377]\n",
      "epoch:1 step:1437 [D loss: 0.155205, acc.: 78.91%] [G loss: 0.865947]\n",
      "epoch:1 step:1438 [D loss: 0.128267, acc.: 83.59%] [G loss: 0.808641]\n",
      "epoch:1 step:1439 [D loss: 0.083435, acc.: 92.19%] [G loss: 0.861610]\n",
      "epoch:1 step:1440 [D loss: 0.125494, acc.: 85.16%] [G loss: 0.865367]\n",
      "epoch:1 step:1441 [D loss: 0.129555, acc.: 83.59%] [G loss: 0.870194]\n",
      "epoch:1 step:1442 [D loss: 0.177733, acc.: 76.56%] [G loss: 0.791366]\n",
      "epoch:1 step:1443 [D loss: 0.109611, acc.: 89.84%] [G loss: 0.832021]\n",
      "epoch:1 step:1444 [D loss: 0.098670, acc.: 89.84%] [G loss: 0.838374]\n",
      "epoch:1 step:1445 [D loss: 0.121559, acc.: 86.72%] [G loss: 0.893682]\n",
      "epoch:1 step:1446 [D loss: 0.186950, acc.: 75.00%] [G loss: 0.712804]\n",
      "epoch:1 step:1447 [D loss: 0.105046, acc.: 92.19%] [G loss: 0.878664]\n",
      "epoch:1 step:1448 [D loss: 0.148107, acc.: 77.34%] [G loss: 0.869522]\n",
      "epoch:1 step:1449 [D loss: 0.201199, acc.: 68.75%] [G loss: 0.820598]\n",
      "epoch:1 step:1450 [D loss: 0.128266, acc.: 86.72%] [G loss: 0.870054]\n",
      "epoch:1 step:1451 [D loss: 0.148630, acc.: 82.81%] [G loss: 0.807116]\n",
      "epoch:1 step:1452 [D loss: 0.137111, acc.: 85.94%] [G loss: 0.824345]\n",
      "epoch:1 step:1453 [D loss: 0.118523, acc.: 82.81%] [G loss: 0.907198]\n",
      "epoch:1 step:1454 [D loss: 0.191805, acc.: 71.88%] [G loss: 0.800849]\n",
      "epoch:1 step:1455 [D loss: 0.105917, acc.: 88.28%] [G loss: 1.011727]\n",
      "epoch:1 step:1456 [D loss: 0.125957, acc.: 85.16%] [G loss: 0.832262]\n",
      "epoch:1 step:1457 [D loss: 0.128106, acc.: 83.59%] [G loss: 0.819485]\n",
      "epoch:1 step:1458 [D loss: 0.122002, acc.: 85.16%] [G loss: 0.878726]\n",
      "epoch:1 step:1459 [D loss: 0.174065, acc.: 81.25%] [G loss: 0.802531]\n",
      "epoch:1 step:1460 [D loss: 0.118844, acc.: 85.94%] [G loss: 0.909636]\n",
      "epoch:1 step:1461 [D loss: 0.168294, acc.: 78.12%] [G loss: 0.777797]\n",
      "epoch:1 step:1462 [D loss: 0.164459, acc.: 78.12%] [G loss: 0.953955]\n",
      "epoch:1 step:1463 [D loss: 0.166914, acc.: 79.69%] [G loss: 0.820662]\n",
      "epoch:1 step:1464 [D loss: 0.111670, acc.: 89.06%] [G loss: 0.840448]\n",
      "epoch:1 step:1465 [D loss: 0.149303, acc.: 80.47%] [G loss: 0.865098]\n",
      "epoch:1 step:1466 [D loss: 0.139169, acc.: 80.47%] [G loss: 0.921411]\n",
      "epoch:1 step:1467 [D loss: 0.107898, acc.: 89.06%] [G loss: 0.920721]\n",
      "epoch:1 step:1468 [D loss: 0.162459, acc.: 79.69%] [G loss: 0.798690]\n",
      "epoch:1 step:1469 [D loss: 0.138021, acc.: 85.16%] [G loss: 0.877589]\n",
      "epoch:1 step:1470 [D loss: 0.092851, acc.: 89.84%] [G loss: 0.862323]\n",
      "epoch:1 step:1471 [D loss: 0.131273, acc.: 82.03%] [G loss: 0.998087]\n",
      "epoch:1 step:1472 [D loss: 0.185622, acc.: 72.66%] [G loss: 0.716314]\n",
      "epoch:1 step:1473 [D loss: 0.167947, acc.: 78.91%] [G loss: 0.821324]\n",
      "epoch:1 step:1474 [D loss: 0.115633, acc.: 84.38%] [G loss: 0.876896]\n",
      "epoch:1 step:1475 [D loss: 0.178146, acc.: 71.09%] [G loss: 0.826781]\n",
      "epoch:1 step:1476 [D loss: 0.117577, acc.: 87.50%] [G loss: 0.909201]\n",
      "epoch:1 step:1477 [D loss: 0.124228, acc.: 82.03%] [G loss: 0.824152]\n",
      "epoch:1 step:1478 [D loss: 0.082086, acc.: 92.97%] [G loss: 0.896566]\n",
      "epoch:1 step:1479 [D loss: 0.185435, acc.: 74.22%] [G loss: 0.798152]\n",
      "epoch:1 step:1480 [D loss: 0.107371, acc.: 91.41%] [G loss: 0.975334]\n",
      "epoch:1 step:1481 [D loss: 0.185700, acc.: 73.44%] [G loss: 0.857089]\n",
      "epoch:1 step:1482 [D loss: 0.150179, acc.: 79.69%] [G loss: 0.830715]\n",
      "epoch:1 step:1483 [D loss: 0.108671, acc.: 87.50%] [G loss: 0.977016]\n",
      "epoch:1 step:1484 [D loss: 0.122104, acc.: 89.06%] [G loss: 0.798458]\n",
      "epoch:1 step:1485 [D loss: 0.119848, acc.: 85.94%] [G loss: 0.920886]\n",
      "epoch:1 step:1486 [D loss: 0.127469, acc.: 85.94%] [G loss: 0.897092]\n",
      "epoch:1 step:1487 [D loss: 0.159812, acc.: 78.91%] [G loss: 0.828054]\n",
      "epoch:1 step:1488 [D loss: 0.147643, acc.: 82.03%] [G loss: 0.950014]\n",
      "epoch:1 step:1489 [D loss: 0.180795, acc.: 73.44%] [G loss: 0.830976]\n",
      "epoch:1 step:1490 [D loss: 0.133428, acc.: 80.47%] [G loss: 0.840570]\n",
      "epoch:1 step:1491 [D loss: 0.100173, acc.: 91.41%] [G loss: 0.865345]\n",
      "epoch:1 step:1492 [D loss: 0.130381, acc.: 84.38%] [G loss: 0.862336]\n",
      "epoch:1 step:1493 [D loss: 0.100155, acc.: 92.97%] [G loss: 0.856329]\n",
      "epoch:1 step:1494 [D loss: 0.136755, acc.: 87.50%] [G loss: 0.842033]\n",
      "epoch:1 step:1495 [D loss: 0.090178, acc.: 89.84%] [G loss: 0.949556]\n",
      "epoch:1 step:1496 [D loss: 0.200533, acc.: 67.19%] [G loss: 0.694242]\n",
      "epoch:1 step:1497 [D loss: 0.106052, acc.: 92.19%] [G loss: 0.917320]\n",
      "epoch:1 step:1498 [D loss: 0.163959, acc.: 81.25%] [G loss: 0.841595]\n",
      "epoch:1 step:1499 [D loss: 0.198084, acc.: 69.53%] [G loss: 0.827995]\n",
      "epoch:1 step:1500 [D loss: 0.103683, acc.: 92.19%] [G loss: 0.955195]\n",
      "epoch:1 step:1501 [D loss: 0.145891, acc.: 83.59%] [G loss: 0.824715]\n",
      "epoch:1 step:1502 [D loss: 0.169371, acc.: 80.47%] [G loss: 0.909585]\n",
      "epoch:1 step:1503 [D loss: 0.089650, acc.: 91.41%] [G loss: 0.961176]\n",
      "epoch:1 step:1504 [D loss: 0.145616, acc.: 79.69%] [G loss: 0.808809]\n",
      "epoch:1 step:1505 [D loss: 0.108615, acc.: 89.84%] [G loss: 0.943608]\n",
      "epoch:1 step:1506 [D loss: 0.175605, acc.: 77.34%] [G loss: 0.770025]\n",
      "epoch:1 step:1507 [D loss: 0.094356, acc.: 91.41%] [G loss: 0.938341]\n",
      "epoch:1 step:1508 [D loss: 0.127465, acc.: 83.59%] [G loss: 0.811070]\n",
      "epoch:1 step:1509 [D loss: 0.122174, acc.: 86.72%] [G loss: 0.917397]\n",
      "epoch:1 step:1510 [D loss: 0.116384, acc.: 86.72%] [G loss: 0.948539]\n",
      "epoch:1 step:1511 [D loss: 0.130414, acc.: 84.38%] [G loss: 0.873299]\n",
      "epoch:1 step:1512 [D loss: 0.097495, acc.: 89.06%] [G loss: 0.915737]\n",
      "epoch:1 step:1513 [D loss: 0.163980, acc.: 81.25%] [G loss: 0.796511]\n",
      "epoch:1 step:1514 [D loss: 0.123765, acc.: 89.84%] [G loss: 0.931420]\n",
      "epoch:1 step:1515 [D loss: 0.136399, acc.: 81.25%] [G loss: 0.932957]\n",
      "epoch:1 step:1516 [D loss: 0.169742, acc.: 75.00%] [G loss: 0.850371]\n",
      "epoch:1 step:1517 [D loss: 0.134433, acc.: 81.25%] [G loss: 0.929240]\n",
      "epoch:1 step:1518 [D loss: 0.145938, acc.: 81.25%] [G loss: 0.828768]\n",
      "epoch:1 step:1519 [D loss: 0.095897, acc.: 92.19%] [G loss: 0.973675]\n",
      "epoch:1 step:1520 [D loss: 0.199145, acc.: 70.31%] [G loss: 0.726898]\n",
      "epoch:1 step:1521 [D loss: 0.191526, acc.: 71.88%] [G loss: 0.754033]\n",
      "epoch:1 step:1522 [D loss: 0.140866, acc.: 82.03%] [G loss: 0.979377]\n",
      "epoch:1 step:1523 [D loss: 0.149595, acc.: 80.47%] [G loss: 0.817121]\n",
      "epoch:1 step:1524 [D loss: 0.221678, acc.: 66.41%] [G loss: 0.775451]\n",
      "epoch:1 step:1525 [D loss: 0.147341, acc.: 80.47%] [G loss: 0.944154]\n",
      "epoch:1 step:1526 [D loss: 0.152456, acc.: 78.91%] [G loss: 0.864066]\n",
      "epoch:1 step:1527 [D loss: 0.153674, acc.: 78.12%] [G loss: 0.891755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1528 [D loss: 0.149077, acc.: 78.91%] [G loss: 0.786313]\n",
      "epoch:1 step:1529 [D loss: 0.096151, acc.: 91.41%] [G loss: 0.883854]\n",
      "epoch:1 step:1530 [D loss: 0.189989, acc.: 72.66%] [G loss: 0.742676]\n",
      "epoch:1 step:1531 [D loss: 0.154095, acc.: 75.78%] [G loss: 0.876429]\n",
      "epoch:1 step:1532 [D loss: 0.159742, acc.: 80.47%] [G loss: 0.793188]\n",
      "epoch:1 step:1533 [D loss: 0.182402, acc.: 76.56%] [G loss: 0.821467]\n",
      "epoch:1 step:1534 [D loss: 0.118076, acc.: 85.94%] [G loss: 0.859531]\n",
      "epoch:1 step:1535 [D loss: 0.126481, acc.: 86.72%] [G loss: 0.836080]\n",
      "epoch:1 step:1536 [D loss: 0.162728, acc.: 82.81%] [G loss: 0.784013]\n",
      "epoch:1 step:1537 [D loss: 0.201506, acc.: 68.75%] [G loss: 0.877942]\n",
      "epoch:1 step:1538 [D loss: 0.181919, acc.: 76.56%] [G loss: 0.779528]\n",
      "epoch:1 step:1539 [D loss: 0.127448, acc.: 88.28%] [G loss: 0.876844]\n",
      "epoch:1 step:1540 [D loss: 0.133316, acc.: 83.59%] [G loss: 0.867320]\n",
      "epoch:1 step:1541 [D loss: 0.124239, acc.: 85.16%] [G loss: 0.882638]\n",
      "epoch:1 step:1542 [D loss: 0.099860, acc.: 89.84%] [G loss: 0.939865]\n",
      "epoch:1 step:1543 [D loss: 0.156696, acc.: 81.25%] [G loss: 0.800343]\n",
      "epoch:1 step:1544 [D loss: 0.114366, acc.: 92.19%] [G loss: 0.925699]\n",
      "epoch:1 step:1545 [D loss: 0.160001, acc.: 77.34%] [G loss: 0.759401]\n",
      "epoch:1 step:1546 [D loss: 0.123977, acc.: 87.50%] [G loss: 0.868568]\n",
      "epoch:1 step:1547 [D loss: 0.141560, acc.: 85.16%] [G loss: 0.790695]\n",
      "epoch:1 step:1548 [D loss: 0.120199, acc.: 83.59%] [G loss: 0.887291]\n",
      "epoch:1 step:1549 [D loss: 0.138236, acc.: 85.16%] [G loss: 0.817179]\n",
      "epoch:1 step:1550 [D loss: 0.086127, acc.: 93.75%] [G loss: 0.905445]\n",
      "epoch:1 step:1551 [D loss: 0.122038, acc.: 84.38%] [G loss: 0.799938]\n",
      "epoch:1 step:1552 [D loss: 0.137088, acc.: 85.16%] [G loss: 0.838260]\n",
      "epoch:1 step:1553 [D loss: 0.121846, acc.: 88.28%] [G loss: 0.932020]\n",
      "epoch:1 step:1554 [D loss: 0.126725, acc.: 87.50%] [G loss: 0.811404]\n",
      "epoch:1 step:1555 [D loss: 0.137032, acc.: 85.16%] [G loss: 0.870441]\n",
      "epoch:1 step:1556 [D loss: 0.133898, acc.: 83.59%] [G loss: 0.866150]\n",
      "epoch:1 step:1557 [D loss: 0.126028, acc.: 84.38%] [G loss: 0.864210]\n",
      "epoch:1 step:1558 [D loss: 0.187523, acc.: 75.00%] [G loss: 0.689657]\n",
      "epoch:1 step:1559 [D loss: 0.239558, acc.: 60.16%] [G loss: 0.740492]\n",
      "epoch:1 step:1560 [D loss: 0.109541, acc.: 88.28%] [G loss: 0.899893]\n",
      "epoch:1 step:1561 [D loss: 0.170759, acc.: 82.81%] [G loss: 0.700815]\n",
      "epoch:1 step:1562 [D loss: 0.113867, acc.: 86.72%] [G loss: 0.804267]\n",
      "epoch:1 step:1563 [D loss: 0.117056, acc.: 89.84%] [G loss: 0.778790]\n",
      "epoch:1 step:1564 [D loss: 0.109240, acc.: 89.06%] [G loss: 0.831746]\n",
      "epoch:1 step:1565 [D loss: 0.093012, acc.: 89.06%] [G loss: 0.881537]\n",
      "epoch:1 step:1566 [D loss: 0.134651, acc.: 82.81%] [G loss: 0.748070]\n",
      "epoch:1 step:1567 [D loss: 0.096428, acc.: 91.41%] [G loss: 0.863746]\n",
      "epoch:1 step:1568 [D loss: 0.115996, acc.: 89.84%] [G loss: 0.815659]\n",
      "epoch:1 step:1569 [D loss: 0.099731, acc.: 89.06%] [G loss: 0.879986]\n",
      "epoch:1 step:1570 [D loss: 0.109742, acc.: 89.84%] [G loss: 0.858422]\n",
      "epoch:1 step:1571 [D loss: 0.126984, acc.: 87.50%] [G loss: 0.826469]\n",
      "epoch:1 step:1572 [D loss: 0.128907, acc.: 82.81%] [G loss: 0.794631]\n",
      "epoch:1 step:1573 [D loss: 0.153982, acc.: 80.47%] [G loss: 0.809612]\n",
      "epoch:1 step:1574 [D loss: 0.125845, acc.: 87.50%] [G loss: 0.811781]\n",
      "epoch:1 step:1575 [D loss: 0.122569, acc.: 87.50%] [G loss: 0.775790]\n",
      "epoch:1 step:1576 [D loss: 0.113982, acc.: 90.62%] [G loss: 0.843529]\n",
      "epoch:1 step:1577 [D loss: 0.151619, acc.: 81.25%] [G loss: 0.781121]\n",
      "epoch:1 step:1578 [D loss: 0.099340, acc.: 91.41%] [G loss: 0.881705]\n",
      "epoch:1 step:1579 [D loss: 0.134421, acc.: 85.16%] [G loss: 0.824960]\n",
      "epoch:1 step:1580 [D loss: 0.147085, acc.: 83.59%] [G loss: 0.796543]\n",
      "epoch:1 step:1581 [D loss: 0.115570, acc.: 84.38%] [G loss: 0.822993]\n",
      "epoch:1 step:1582 [D loss: 0.163038, acc.: 76.56%] [G loss: 0.781208]\n",
      "epoch:1 step:1583 [D loss: 0.129125, acc.: 85.16%] [G loss: 0.837755]\n",
      "epoch:1 step:1584 [D loss: 0.088848, acc.: 93.75%] [G loss: 0.901263]\n",
      "epoch:1 step:1585 [D loss: 0.086565, acc.: 90.62%] [G loss: 0.915017]\n",
      "epoch:1 step:1586 [D loss: 0.138634, acc.: 78.12%] [G loss: 0.837489]\n",
      "epoch:1 step:1587 [D loss: 0.132327, acc.: 85.94%] [G loss: 0.838163]\n",
      "epoch:1 step:1588 [D loss: 0.161320, acc.: 80.47%] [G loss: 0.797294]\n",
      "epoch:1 step:1589 [D loss: 0.227143, acc.: 66.41%] [G loss: 0.748301]\n",
      "epoch:1 step:1590 [D loss: 0.135159, acc.: 82.03%] [G loss: 0.915086]\n",
      "epoch:1 step:1591 [D loss: 0.130321, acc.: 88.28%] [G loss: 0.837204]\n",
      "epoch:1 step:1592 [D loss: 0.138533, acc.: 86.72%] [G loss: 0.844787]\n",
      "epoch:1 step:1593 [D loss: 0.116545, acc.: 87.50%] [G loss: 0.836192]\n",
      "epoch:1 step:1594 [D loss: 0.131780, acc.: 83.59%] [G loss: 0.832121]\n",
      "epoch:1 step:1595 [D loss: 0.104240, acc.: 88.28%] [G loss: 0.948061]\n",
      "epoch:1 step:1596 [D loss: 0.126919, acc.: 85.16%] [G loss: 0.767012]\n",
      "epoch:1 step:1597 [D loss: 0.125882, acc.: 87.50%] [G loss: 0.966150]\n",
      "epoch:1 step:1598 [D loss: 0.104568, acc.: 86.72%] [G loss: 0.863469]\n",
      "epoch:1 step:1599 [D loss: 0.159706, acc.: 79.69%] [G loss: 0.771567]\n",
      "epoch:1 step:1600 [D loss: 0.118972, acc.: 87.50%] [G loss: 0.876817]\n",
      "##############\n",
      "[4.59592637 3.60085156 8.32258506 7.39450727 5.85569513 7.66536253\n",
      " 6.95802323 7.11431795 6.48986998 5.12490385]\n",
      "##########\n",
      "epoch:1 step:1601 [D loss: 0.165763, acc.: 71.88%] [G loss: 0.734217]\n",
      "epoch:1 step:1602 [D loss: 0.126946, acc.: 84.38%] [G loss: 0.835217]\n",
      "epoch:1 step:1603 [D loss: 0.087075, acc.: 93.75%] [G loss: 0.990163]\n",
      "epoch:1 step:1604 [D loss: 0.147382, acc.: 84.38%] [G loss: 0.794785]\n",
      "epoch:1 step:1605 [D loss: 0.165904, acc.: 80.47%] [G loss: 0.766466]\n",
      "epoch:1 step:1606 [D loss: 0.090981, acc.: 89.84%] [G loss: 1.017543]\n",
      "epoch:1 step:1607 [D loss: 0.185117, acc.: 72.66%] [G loss: 0.703175]\n",
      "epoch:1 step:1608 [D loss: 0.153615, acc.: 77.34%] [G loss: 0.804765]\n",
      "epoch:1 step:1609 [D loss: 0.196691, acc.: 73.44%] [G loss: 0.754012]\n",
      "epoch:1 step:1610 [D loss: 0.146723, acc.: 81.25%] [G loss: 0.897741]\n",
      "epoch:1 step:1611 [D loss: 0.163241, acc.: 75.78%] [G loss: 0.837869]\n",
      "epoch:1 step:1612 [D loss: 0.109764, acc.: 92.19%] [G loss: 0.881214]\n",
      "epoch:1 step:1613 [D loss: 0.116757, acc.: 86.72%] [G loss: 0.851179]\n",
      "epoch:1 step:1614 [D loss: 0.123151, acc.: 86.72%] [G loss: 0.894889]\n",
      "epoch:1 step:1615 [D loss: 0.190606, acc.: 74.22%] [G loss: 0.730018]\n",
      "epoch:1 step:1616 [D loss: 0.117701, acc.: 84.38%] [G loss: 0.940615]\n",
      "epoch:1 step:1617 [D loss: 0.093664, acc.: 90.62%] [G loss: 0.883879]\n",
      "epoch:1 step:1618 [D loss: 0.163469, acc.: 79.69%] [G loss: 0.756319]\n",
      "epoch:1 step:1619 [D loss: 0.108953, acc.: 87.50%] [G loss: 0.953164]\n",
      "epoch:1 step:1620 [D loss: 0.124702, acc.: 86.72%] [G loss: 0.832843]\n",
      "epoch:1 step:1621 [D loss: 0.130976, acc.: 87.50%] [G loss: 0.805995]\n",
      "epoch:1 step:1622 [D loss: 0.144240, acc.: 83.59%] [G loss: 0.898587]\n",
      "epoch:1 step:1623 [D loss: 0.127255, acc.: 87.50%] [G loss: 0.843578]\n",
      "epoch:1 step:1624 [D loss: 0.142137, acc.: 82.81%] [G loss: 0.820172]\n",
      "epoch:1 step:1625 [D loss: 0.132665, acc.: 85.16%] [G loss: 0.832913]\n",
      "epoch:1 step:1626 [D loss: 0.117899, acc.: 88.28%] [G loss: 0.847251]\n",
      "epoch:1 step:1627 [D loss: 0.097148, acc.: 89.06%] [G loss: 0.879461]\n",
      "epoch:1 step:1628 [D loss: 0.148409, acc.: 82.03%] [G loss: 0.829325]\n",
      "epoch:1 step:1629 [D loss: 0.107699, acc.: 89.84%] [G loss: 0.890566]\n",
      "epoch:1 step:1630 [D loss: 0.102902, acc.: 92.97%] [G loss: 0.871005]\n",
      "epoch:1 step:1631 [D loss: 0.129749, acc.: 85.94%] [G loss: 0.825506]\n",
      "epoch:1 step:1632 [D loss: 0.115288, acc.: 89.84%] [G loss: 0.905297]\n",
      "epoch:1 step:1633 [D loss: 0.103128, acc.: 89.84%] [G loss: 0.886350]\n",
      "epoch:1 step:1634 [D loss: 0.103962, acc.: 89.06%] [G loss: 0.920742]\n",
      "epoch:1 step:1635 [D loss: 0.138473, acc.: 80.47%] [G loss: 0.809117]\n",
      "epoch:1 step:1636 [D loss: 0.110618, acc.: 89.06%] [G loss: 0.936619]\n",
      "epoch:1 step:1637 [D loss: 0.118754, acc.: 85.94%] [G loss: 0.889687]\n",
      "epoch:1 step:1638 [D loss: 0.118437, acc.: 86.72%] [G loss: 0.814038]\n",
      "epoch:1 step:1639 [D loss: 0.151203, acc.: 76.56%] [G loss: 0.948885]\n",
      "epoch:1 step:1640 [D loss: 0.102333, acc.: 88.28%] [G loss: 0.910913]\n",
      "epoch:1 step:1641 [D loss: 0.141014, acc.: 82.81%] [G loss: 0.810369]\n",
      "epoch:1 step:1642 [D loss: 0.087435, acc.: 92.97%] [G loss: 1.045658]\n",
      "epoch:1 step:1643 [D loss: 0.109641, acc.: 89.84%] [G loss: 0.876659]\n",
      "epoch:1 step:1644 [D loss: 0.062014, acc.: 94.53%] [G loss: 0.996560]\n",
      "epoch:1 step:1645 [D loss: 0.120472, acc.: 85.16%] [G loss: 0.848453]\n",
      "epoch:1 step:1646 [D loss: 0.092764, acc.: 91.41%] [G loss: 0.962690]\n",
      "epoch:1 step:1647 [D loss: 0.201325, acc.: 69.53%] [G loss: 0.763589]\n",
      "epoch:1 step:1648 [D loss: 0.146245, acc.: 82.03%] [G loss: 0.848529]\n",
      "epoch:1 step:1649 [D loss: 0.103856, acc.: 88.28%] [G loss: 0.968036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1650 [D loss: 0.135807, acc.: 84.38%] [G loss: 0.842125]\n",
      "epoch:1 step:1651 [D loss: 0.091182, acc.: 90.62%] [G loss: 0.924811]\n",
      "epoch:1 step:1652 [D loss: 0.175545, acc.: 76.56%] [G loss: 0.793459]\n",
      "epoch:1 step:1653 [D loss: 0.152741, acc.: 79.69%] [G loss: 0.828731]\n",
      "epoch:1 step:1654 [D loss: 0.148240, acc.: 81.25%] [G loss: 0.832632]\n",
      "epoch:1 step:1655 [D loss: 0.135962, acc.: 84.38%] [G loss: 0.810827]\n",
      "epoch:1 step:1656 [D loss: 0.138391, acc.: 84.38%] [G loss: 0.931899]\n",
      "epoch:1 step:1657 [D loss: 0.138434, acc.: 83.59%] [G loss: 0.884603]\n",
      "epoch:1 step:1658 [D loss: 0.196421, acc.: 72.66%] [G loss: 0.786259]\n",
      "epoch:1 step:1659 [D loss: 0.077740, acc.: 89.84%] [G loss: 1.006196]\n",
      "epoch:1 step:1660 [D loss: 0.222933, acc.: 67.97%] [G loss: 0.682859]\n",
      "epoch:1 step:1661 [D loss: 0.103918, acc.: 91.41%] [G loss: 0.902617]\n",
      "epoch:1 step:1662 [D loss: 0.114623, acc.: 89.06%] [G loss: 0.860018]\n",
      "epoch:1 step:1663 [D loss: 0.099278, acc.: 88.28%] [G loss: 0.910817]\n",
      "epoch:1 step:1664 [D loss: 0.169049, acc.: 75.78%] [G loss: 0.757646]\n",
      "epoch:1 step:1665 [D loss: 0.115333, acc.: 87.50%] [G loss: 0.836054]\n",
      "epoch:1 step:1666 [D loss: 0.119191, acc.: 85.16%] [G loss: 0.845371]\n",
      "epoch:1 step:1667 [D loss: 0.117903, acc.: 88.28%] [G loss: 0.977917]\n",
      "epoch:1 step:1668 [D loss: 0.182063, acc.: 75.78%] [G loss: 0.770510]\n",
      "epoch:1 step:1669 [D loss: 0.069991, acc.: 92.97%] [G loss: 1.000352]\n",
      "epoch:1 step:1670 [D loss: 0.147743, acc.: 83.59%] [G loss: 0.821597]\n",
      "epoch:1 step:1671 [D loss: 0.114469, acc.: 88.28%] [G loss: 0.840136]\n",
      "epoch:1 step:1672 [D loss: 0.176925, acc.: 75.78%] [G loss: 0.755690]\n",
      "epoch:1 step:1673 [D loss: 0.113405, acc.: 86.72%] [G loss: 0.848801]\n",
      "epoch:1 step:1674 [D loss: 0.132959, acc.: 84.38%] [G loss: 0.825576]\n",
      "epoch:1 step:1675 [D loss: 0.165785, acc.: 84.38%] [G loss: 0.725597]\n",
      "epoch:1 step:1676 [D loss: 0.154906, acc.: 78.91%] [G loss: 0.811154]\n",
      "epoch:1 step:1677 [D loss: 0.165181, acc.: 73.44%] [G loss: 0.807073]\n",
      "epoch:1 step:1678 [D loss: 0.135790, acc.: 82.03%] [G loss: 0.929659]\n",
      "epoch:1 step:1679 [D loss: 0.170850, acc.: 75.00%] [G loss: 0.735731]\n",
      "epoch:1 step:1680 [D loss: 0.123764, acc.: 82.03%] [G loss: 0.931282]\n",
      "epoch:1 step:1681 [D loss: 0.080398, acc.: 92.19%] [G loss: 0.923239]\n",
      "epoch:1 step:1682 [D loss: 0.111810, acc.: 88.28%] [G loss: 0.867874]\n",
      "epoch:1 step:1683 [D loss: 0.098593, acc.: 89.84%] [G loss: 0.926101]\n",
      "epoch:1 step:1684 [D loss: 0.087103, acc.: 90.62%] [G loss: 0.912896]\n",
      "epoch:1 step:1685 [D loss: 0.153689, acc.: 82.03%] [G loss: 0.817972]\n",
      "epoch:1 step:1686 [D loss: 0.116511, acc.: 85.16%] [G loss: 0.812730]\n",
      "epoch:1 step:1687 [D loss: 0.156181, acc.: 77.34%] [G loss: 0.853750]\n",
      "epoch:1 step:1688 [D loss: 0.108728, acc.: 88.28%] [G loss: 0.914684]\n",
      "epoch:1 step:1689 [D loss: 0.128289, acc.: 88.28%] [G loss: 0.843709]\n",
      "epoch:1 step:1690 [D loss: 0.107253, acc.: 90.62%] [G loss: 0.932452]\n",
      "epoch:1 step:1691 [D loss: 0.133072, acc.: 84.38%] [G loss: 0.926784]\n",
      "epoch:1 step:1692 [D loss: 0.112419, acc.: 88.28%] [G loss: 0.910686]\n",
      "epoch:1 step:1693 [D loss: 0.133587, acc.: 84.38%] [G loss: 0.877968]\n",
      "epoch:1 step:1694 [D loss: 0.102099, acc.: 88.28%] [G loss: 0.932461]\n",
      "epoch:1 step:1695 [D loss: 0.103150, acc.: 89.84%] [G loss: 0.829233]\n",
      "epoch:1 step:1696 [D loss: 0.125723, acc.: 84.38%] [G loss: 0.903792]\n",
      "epoch:1 step:1697 [D loss: 0.130184, acc.: 85.94%] [G loss: 0.845850]\n",
      "epoch:1 step:1698 [D loss: 0.139987, acc.: 83.59%] [G loss: 0.878521]\n",
      "epoch:1 step:1699 [D loss: 0.145045, acc.: 82.03%] [G loss: 0.868379]\n",
      "epoch:1 step:1700 [D loss: 0.104235, acc.: 89.84%] [G loss: 0.953925]\n",
      "epoch:1 step:1701 [D loss: 0.108571, acc.: 89.06%] [G loss: 0.890632]\n",
      "epoch:1 step:1702 [D loss: 0.211931, acc.: 70.31%] [G loss: 0.794955]\n",
      "epoch:1 step:1703 [D loss: 0.170239, acc.: 78.91%] [G loss: 0.822485]\n",
      "epoch:1 step:1704 [D loss: 0.152892, acc.: 82.03%] [G loss: 0.882141]\n",
      "epoch:1 step:1705 [D loss: 0.140892, acc.: 81.25%] [G loss: 0.905373]\n",
      "epoch:1 step:1706 [D loss: 0.109127, acc.: 86.72%] [G loss: 0.849882]\n",
      "epoch:1 step:1707 [D loss: 0.154941, acc.: 86.72%] [G loss: 0.830666]\n",
      "epoch:1 step:1708 [D loss: 0.136219, acc.: 85.16%] [G loss: 0.859044]\n",
      "epoch:1 step:1709 [D loss: 0.146359, acc.: 82.81%] [G loss: 0.925670]\n",
      "epoch:1 step:1710 [D loss: 0.155886, acc.: 79.69%] [G loss: 0.828174]\n",
      "epoch:1 step:1711 [D loss: 0.204107, acc.: 65.62%] [G loss: 0.783588]\n",
      "epoch:1 step:1712 [D loss: 0.100175, acc.: 85.94%] [G loss: 1.033994]\n",
      "epoch:1 step:1713 [D loss: 0.187865, acc.: 71.09%] [G loss: 0.764840]\n",
      "epoch:1 step:1714 [D loss: 0.114047, acc.: 89.84%] [G loss: 0.883540]\n",
      "epoch:1 step:1715 [D loss: 0.096823, acc.: 89.84%] [G loss: 0.860900]\n",
      "epoch:1 step:1716 [D loss: 0.138492, acc.: 82.81%] [G loss: 0.876345]\n",
      "epoch:1 step:1717 [D loss: 0.159694, acc.: 80.47%] [G loss: 0.852531]\n",
      "epoch:1 step:1718 [D loss: 0.119869, acc.: 86.72%] [G loss: 0.955023]\n",
      "epoch:1 step:1719 [D loss: 0.164632, acc.: 77.34%] [G loss: 0.886333]\n",
      "epoch:1 step:1720 [D loss: 0.215444, acc.: 65.62%] [G loss: 0.825905]\n",
      "epoch:1 step:1721 [D loss: 0.142484, acc.: 81.25%] [G loss: 0.925969]\n",
      "epoch:1 step:1722 [D loss: 0.161081, acc.: 79.69%] [G loss: 0.873800]\n",
      "epoch:1 step:1723 [D loss: 0.111248, acc.: 88.28%] [G loss: 0.908585]\n",
      "epoch:1 step:1724 [D loss: 0.150391, acc.: 80.47%] [G loss: 0.795211]\n",
      "epoch:1 step:1725 [D loss: 0.174130, acc.: 76.56%] [G loss: 0.823481]\n",
      "epoch:1 step:1726 [D loss: 0.127741, acc.: 87.50%] [G loss: 0.868926]\n",
      "epoch:1 step:1727 [D loss: 0.118689, acc.: 85.94%] [G loss: 0.845383]\n",
      "epoch:1 step:1728 [D loss: 0.126223, acc.: 85.16%] [G loss: 0.868981]\n",
      "epoch:1 step:1729 [D loss: 0.111034, acc.: 86.72%] [G loss: 0.968582]\n",
      "epoch:1 step:1730 [D loss: 0.187507, acc.: 71.09%] [G loss: 0.699064]\n",
      "epoch:1 step:1731 [D loss: 0.113849, acc.: 89.06%] [G loss: 0.939691]\n",
      "epoch:1 step:1732 [D loss: 0.118520, acc.: 85.94%] [G loss: 0.866374]\n",
      "epoch:1 step:1733 [D loss: 0.112539, acc.: 89.06%] [G loss: 0.955345]\n",
      "epoch:1 step:1734 [D loss: 0.204996, acc.: 71.09%] [G loss: 0.755731]\n",
      "epoch:1 step:1735 [D loss: 0.107473, acc.: 84.38%] [G loss: 0.891997]\n",
      "epoch:1 step:1736 [D loss: 0.139901, acc.: 85.16%] [G loss: 0.825329]\n",
      "epoch:1 step:1737 [D loss: 0.165048, acc.: 77.34%] [G loss: 0.775198]\n",
      "epoch:1 step:1738 [D loss: 0.119027, acc.: 87.50%] [G loss: 0.953055]\n",
      "epoch:1 step:1739 [D loss: 0.137019, acc.: 82.81%] [G loss: 0.846799]\n",
      "epoch:1 step:1740 [D loss: 0.108601, acc.: 86.72%] [G loss: 0.822080]\n",
      "epoch:1 step:1741 [D loss: 0.130825, acc.: 90.62%] [G loss: 0.816586]\n",
      "epoch:1 step:1742 [D loss: 0.120979, acc.: 82.81%] [G loss: 0.852798]\n",
      "epoch:1 step:1743 [D loss: 0.132665, acc.: 82.81%] [G loss: 0.823327]\n",
      "epoch:1 step:1744 [D loss: 0.153697, acc.: 78.91%] [G loss: 0.874043]\n",
      "epoch:1 step:1745 [D loss: 0.150467, acc.: 77.34%] [G loss: 0.789888]\n",
      "epoch:1 step:1746 [D loss: 0.135588, acc.: 82.03%] [G loss: 0.879070]\n",
      "epoch:1 step:1747 [D loss: 0.113294, acc.: 85.94%] [G loss: 0.889376]\n",
      "epoch:1 step:1748 [D loss: 0.177544, acc.: 78.12%] [G loss: 0.723630]\n",
      "epoch:1 step:1749 [D loss: 0.140959, acc.: 84.38%] [G loss: 0.833385]\n",
      "epoch:1 step:1750 [D loss: 0.138047, acc.: 85.16%] [G loss: 0.869619]\n",
      "epoch:1 step:1751 [D loss: 0.150892, acc.: 82.81%] [G loss: 0.852953]\n",
      "epoch:1 step:1752 [D loss: 0.137977, acc.: 80.47%] [G loss: 0.825051]\n",
      "epoch:1 step:1753 [D loss: 0.141751, acc.: 78.91%] [G loss: 0.924657]\n",
      "epoch:1 step:1754 [D loss: 0.167662, acc.: 75.00%] [G loss: 0.742438]\n",
      "epoch:1 step:1755 [D loss: 0.132782, acc.: 85.16%] [G loss: 0.845461]\n",
      "epoch:1 step:1756 [D loss: 0.115988, acc.: 85.94%] [G loss: 0.836181]\n",
      "epoch:1 step:1757 [D loss: 0.156384, acc.: 78.12%] [G loss: 0.781312]\n",
      "epoch:1 step:1758 [D loss: 0.132993, acc.: 82.81%] [G loss: 0.863786]\n",
      "epoch:1 step:1759 [D loss: 0.109256, acc.: 88.28%] [G loss: 0.834570]\n",
      "epoch:1 step:1760 [D loss: 0.163819, acc.: 78.12%] [G loss: 0.762579]\n",
      "epoch:1 step:1761 [D loss: 0.162955, acc.: 77.34%] [G loss: 0.778767]\n",
      "epoch:1 step:1762 [D loss: 0.118482, acc.: 84.38%] [G loss: 0.846723]\n",
      "epoch:1 step:1763 [D loss: 0.143224, acc.: 82.81%] [G loss: 0.834472]\n",
      "epoch:1 step:1764 [D loss: 0.190103, acc.: 74.22%] [G loss: 0.751260]\n",
      "epoch:1 step:1765 [D loss: 0.153228, acc.: 78.12%] [G loss: 0.767682]\n",
      "epoch:1 step:1766 [D loss: 0.149516, acc.: 85.16%] [G loss: 0.781412]\n",
      "epoch:1 step:1767 [D loss: 0.123135, acc.: 84.38%] [G loss: 0.836274]\n",
      "epoch:1 step:1768 [D loss: 0.132486, acc.: 84.38%] [G loss: 0.842324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1769 [D loss: 0.137927, acc.: 81.25%] [G loss: 0.771802]\n",
      "epoch:1 step:1770 [D loss: 0.118288, acc.: 89.06%] [G loss: 0.851360]\n",
      "epoch:1 step:1771 [D loss: 0.123740, acc.: 83.59%] [G loss: 0.801905]\n",
      "epoch:1 step:1772 [D loss: 0.128714, acc.: 85.16%] [G loss: 0.855321]\n",
      "epoch:1 step:1773 [D loss: 0.142933, acc.: 81.25%] [G loss: 0.714309]\n",
      "epoch:1 step:1774 [D loss: 0.151205, acc.: 81.25%] [G loss: 0.782416]\n",
      "epoch:1 step:1775 [D loss: 0.181106, acc.: 71.88%] [G loss: 0.812548]\n",
      "epoch:1 step:1776 [D loss: 0.181809, acc.: 75.00%] [G loss: 0.736071]\n",
      "epoch:1 step:1777 [D loss: 0.124312, acc.: 87.50%] [G loss: 0.807424]\n",
      "epoch:1 step:1778 [D loss: 0.141249, acc.: 85.16%] [G loss: 0.772626]\n",
      "epoch:1 step:1779 [D loss: 0.123201, acc.: 86.72%] [G loss: 0.790800]\n",
      "epoch:1 step:1780 [D loss: 0.176234, acc.: 77.34%] [G loss: 0.763723]\n",
      "epoch:1 step:1781 [D loss: 0.125868, acc.: 83.59%] [G loss: 0.834323]\n",
      "epoch:1 step:1782 [D loss: 0.123173, acc.: 86.72%] [G loss: 0.816480]\n",
      "epoch:1 step:1783 [D loss: 0.140048, acc.: 83.59%] [G loss: 0.828383]\n",
      "epoch:1 step:1784 [D loss: 0.124037, acc.: 83.59%] [G loss: 0.790452]\n",
      "epoch:1 step:1785 [D loss: 0.138782, acc.: 83.59%] [G loss: 0.804009]\n",
      "epoch:1 step:1786 [D loss: 0.189692, acc.: 69.53%] [G loss: 0.737285]\n",
      "epoch:1 step:1787 [D loss: 0.150977, acc.: 81.25%] [G loss: 0.771724]\n",
      "epoch:1 step:1788 [D loss: 0.152423, acc.: 82.81%] [G loss: 0.804103]\n",
      "epoch:1 step:1789 [D loss: 0.142892, acc.: 82.81%] [G loss: 0.874043]\n",
      "epoch:1 step:1790 [D loss: 0.168530, acc.: 79.69%] [G loss: 0.798683]\n",
      "epoch:1 step:1791 [D loss: 0.134152, acc.: 83.59%] [G loss: 0.887849]\n",
      "epoch:1 step:1792 [D loss: 0.173499, acc.: 76.56%] [G loss: 0.743962]\n",
      "epoch:1 step:1793 [D loss: 0.203476, acc.: 68.75%] [G loss: 0.804315]\n",
      "epoch:1 step:1794 [D loss: 0.136754, acc.: 82.03%] [G loss: 0.889305]\n",
      "epoch:1 step:1795 [D loss: 0.232262, acc.: 64.84%] [G loss: 0.734428]\n",
      "epoch:1 step:1796 [D loss: 0.173316, acc.: 72.66%] [G loss: 0.813027]\n",
      "epoch:1 step:1797 [D loss: 0.122239, acc.: 84.38%] [G loss: 0.934877]\n",
      "epoch:1 step:1798 [D loss: 0.241890, acc.: 61.72%] [G loss: 0.644836]\n",
      "epoch:1 step:1799 [D loss: 0.133305, acc.: 84.38%] [G loss: 0.842913]\n",
      "epoch:1 step:1800 [D loss: 0.170498, acc.: 78.91%] [G loss: 0.761711]\n",
      "##############\n",
      "[5.11127132 3.62991864 7.770109   6.23071406 5.44568374 7.11595902\n",
      " 7.80055116 6.41564002 6.67724023 4.86703973]\n",
      "##########\n",
      "epoch:1 step:1801 [D loss: 0.179991, acc.: 71.09%] [G loss: 0.826733]\n",
      "epoch:1 step:1802 [D loss: 0.161146, acc.: 77.34%] [G loss: 0.763469]\n",
      "epoch:1 step:1803 [D loss: 0.125297, acc.: 85.16%] [G loss: 0.883831]\n",
      "epoch:1 step:1804 [D loss: 0.240916, acc.: 57.81%] [G loss: 0.742333]\n",
      "epoch:1 step:1805 [D loss: 0.141747, acc.: 85.94%] [G loss: 0.820325]\n",
      "epoch:1 step:1806 [D loss: 0.176308, acc.: 73.44%] [G loss: 0.762678]\n",
      "epoch:1 step:1807 [D loss: 0.112920, acc.: 88.28%] [G loss: 0.929752]\n",
      "epoch:1 step:1808 [D loss: 0.156534, acc.: 81.25%] [G loss: 0.747338]\n",
      "epoch:1 step:1809 [D loss: 0.143041, acc.: 82.03%] [G loss: 0.812947]\n",
      "epoch:1 step:1810 [D loss: 0.168062, acc.: 76.56%] [G loss: 0.748567]\n",
      "epoch:1 step:1811 [D loss: 0.128049, acc.: 82.81%] [G loss: 0.817169]\n",
      "epoch:1 step:1812 [D loss: 0.119583, acc.: 85.94%] [G loss: 0.885920]\n",
      "epoch:1 step:1813 [D loss: 0.160731, acc.: 77.34%] [G loss: 0.751906]\n",
      "epoch:1 step:1814 [D loss: 0.144780, acc.: 84.38%] [G loss: 0.847125]\n",
      "epoch:1 step:1815 [D loss: 0.140978, acc.: 82.03%] [G loss: 0.837717]\n",
      "epoch:1 step:1816 [D loss: 0.194384, acc.: 68.75%] [G loss: 0.766486]\n",
      "epoch:1 step:1817 [D loss: 0.201885, acc.: 67.19%] [G loss: 0.724957]\n",
      "epoch:1 step:1818 [D loss: 0.130850, acc.: 81.25%] [G loss: 0.853797]\n",
      "epoch:1 step:1819 [D loss: 0.160181, acc.: 77.34%] [G loss: 0.794794]\n",
      "epoch:1 step:1820 [D loss: 0.180622, acc.: 70.31%] [G loss: 0.790367]\n",
      "epoch:1 step:1821 [D loss: 0.097920, acc.: 90.62%] [G loss: 0.949394]\n",
      "epoch:1 step:1822 [D loss: 0.187260, acc.: 78.12%] [G loss: 0.758217]\n",
      "epoch:1 step:1823 [D loss: 0.131265, acc.: 84.38%] [G loss: 0.932736]\n",
      "epoch:1 step:1824 [D loss: 0.123036, acc.: 83.59%] [G loss: 0.849452]\n",
      "epoch:1 step:1825 [D loss: 0.144115, acc.: 81.25%] [G loss: 0.772939]\n",
      "epoch:1 step:1826 [D loss: 0.129427, acc.: 83.59%] [G loss: 0.828276]\n",
      "epoch:1 step:1827 [D loss: 0.153208, acc.: 83.59%] [G loss: 0.835106]\n",
      "epoch:1 step:1828 [D loss: 0.157622, acc.: 83.59%] [G loss: 0.734385]\n",
      "epoch:1 step:1829 [D loss: 0.167655, acc.: 78.12%] [G loss: 0.777971]\n",
      "epoch:1 step:1830 [D loss: 0.145528, acc.: 79.69%] [G loss: 0.813991]\n",
      "epoch:1 step:1831 [D loss: 0.122778, acc.: 85.94%] [G loss: 0.825589]\n",
      "epoch:1 step:1832 [D loss: 0.119986, acc.: 88.28%] [G loss: 0.858180]\n",
      "epoch:1 step:1833 [D loss: 0.143924, acc.: 82.81%] [G loss: 0.739087]\n",
      "epoch:1 step:1834 [D loss: 0.126334, acc.: 85.94%] [G loss: 0.894162]\n",
      "epoch:1 step:1835 [D loss: 0.160147, acc.: 79.69%] [G loss: 0.774131]\n",
      "epoch:1 step:1836 [D loss: 0.107060, acc.: 86.72%] [G loss: 0.899857]\n",
      "epoch:1 step:1837 [D loss: 0.177112, acc.: 78.91%] [G loss: 0.734015]\n",
      "epoch:1 step:1838 [D loss: 0.164178, acc.: 79.69%] [G loss: 0.805194]\n",
      "epoch:1 step:1839 [D loss: 0.203377, acc.: 72.66%] [G loss: 0.699106]\n",
      "epoch:1 step:1840 [D loss: 0.171799, acc.: 75.00%] [G loss: 0.832449]\n",
      "epoch:1 step:1841 [D loss: 0.154916, acc.: 82.03%] [G loss: 0.793977]\n",
      "epoch:1 step:1842 [D loss: 0.173141, acc.: 82.03%] [G loss: 0.733155]\n",
      "epoch:1 step:1843 [D loss: 0.136821, acc.: 85.94%] [G loss: 0.795652]\n",
      "epoch:1 step:1844 [D loss: 0.177273, acc.: 78.12%] [G loss: 0.808446]\n",
      "epoch:1 step:1845 [D loss: 0.165234, acc.: 75.78%] [G loss: 0.837468]\n",
      "epoch:1 step:1846 [D loss: 0.117966, acc.: 85.94%] [G loss: 0.858458]\n",
      "epoch:1 step:1847 [D loss: 0.141412, acc.: 84.38%] [G loss: 0.810012]\n",
      "epoch:1 step:1848 [D loss: 0.103093, acc.: 89.06%] [G loss: 0.883942]\n",
      "epoch:1 step:1849 [D loss: 0.072445, acc.: 94.53%] [G loss: 0.916975]\n",
      "epoch:1 step:1850 [D loss: 0.172682, acc.: 75.00%] [G loss: 0.807591]\n",
      "epoch:1 step:1851 [D loss: 0.087495, acc.: 92.97%] [G loss: 0.906367]\n",
      "epoch:1 step:1852 [D loss: 0.155794, acc.: 80.47%] [G loss: 0.747680]\n",
      "epoch:1 step:1853 [D loss: 0.136020, acc.: 85.16%] [G loss: 0.798304]\n",
      "epoch:1 step:1854 [D loss: 0.201133, acc.: 67.19%] [G loss: 0.719086]\n",
      "epoch:1 step:1855 [D loss: 0.104124, acc.: 91.41%] [G loss: 0.918153]\n",
      "epoch:1 step:1856 [D loss: 0.108551, acc.: 85.94%] [G loss: 0.852282]\n",
      "epoch:1 step:1857 [D loss: 0.285949, acc.: 60.94%] [G loss: 0.722142]\n",
      "epoch:1 step:1858 [D loss: 0.107967, acc.: 87.50%] [G loss: 1.014043]\n",
      "epoch:1 step:1859 [D loss: 0.162172, acc.: 78.91%] [G loss: 0.720697]\n",
      "epoch:1 step:1860 [D loss: 0.088858, acc.: 93.75%] [G loss: 0.940722]\n",
      "epoch:1 step:1861 [D loss: 0.100139, acc.: 86.72%] [G loss: 0.822344]\n",
      "epoch:1 step:1862 [D loss: 0.056331, acc.: 95.31%] [G loss: 0.936437]\n",
      "epoch:1 step:1863 [D loss: 0.074909, acc.: 92.97%] [G loss: 0.874960]\n",
      "epoch:1 step:1864 [D loss: 0.083286, acc.: 96.09%] [G loss: 0.909255]\n",
      "epoch:1 step:1865 [D loss: 0.229336, acc.: 70.31%] [G loss: 0.906045]\n",
      "epoch:1 step:1866 [D loss: 0.109893, acc.: 83.59%] [G loss: 1.050539]\n",
      "epoch:1 step:1867 [D loss: 0.184890, acc.: 74.22%] [G loss: 0.724180]\n",
      "epoch:1 step:1868 [D loss: 0.156740, acc.: 79.69%] [G loss: 0.780438]\n",
      "epoch:1 step:1869 [D loss: 0.130690, acc.: 88.28%] [G loss: 0.788455]\n",
      "epoch:1 step:1870 [D loss: 0.118229, acc.: 84.38%] [G loss: 0.898601]\n",
      "epoch:1 step:1871 [D loss: 0.169836, acc.: 78.12%] [G loss: 0.701558]\n",
      "epoch:1 step:1872 [D loss: 0.091601, acc.: 89.84%] [G loss: 0.889650]\n",
      "epoch:1 step:1873 [D loss: 0.067380, acc.: 91.41%] [G loss: 1.035713]\n",
      "epoch:1 step:1874 [D loss: 0.214560, acc.: 67.97%] [G loss: 0.735351]\n",
      "epoch:2 step:1875 [D loss: 0.179062, acc.: 78.91%] [G loss: 1.019703]\n",
      "epoch:2 step:1876 [D loss: 0.163075, acc.: 78.12%] [G loss: 0.741634]\n",
      "epoch:2 step:1877 [D loss: 0.199429, acc.: 71.09%] [G loss: 0.696421]\n",
      "epoch:2 step:1878 [D loss: 0.110738, acc.: 89.06%] [G loss: 0.983556]\n",
      "epoch:2 step:1879 [D loss: 0.185149, acc.: 76.56%] [G loss: 0.697807]\n",
      "epoch:2 step:1880 [D loss: 0.139197, acc.: 85.94%] [G loss: 0.825485]\n",
      "epoch:2 step:1881 [D loss: 0.189016, acc.: 71.88%] [G loss: 0.746459]\n",
      "epoch:2 step:1882 [D loss: 0.126953, acc.: 87.50%] [G loss: 0.832875]\n",
      "epoch:2 step:1883 [D loss: 0.150829, acc.: 79.69%] [G loss: 0.818777]\n",
      "epoch:2 step:1884 [D loss: 0.155892, acc.: 78.12%] [G loss: 0.792034]\n",
      "epoch:2 step:1885 [D loss: 0.103494, acc.: 89.84%] [G loss: 0.889618]\n",
      "epoch:2 step:1886 [D loss: 0.135989, acc.: 82.03%] [G loss: 0.776295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1887 [D loss: 0.134630, acc.: 84.38%] [G loss: 0.861828]\n",
      "epoch:2 step:1888 [D loss: 0.157645, acc.: 81.25%] [G loss: 0.807683]\n",
      "epoch:2 step:1889 [D loss: 0.098295, acc.: 86.72%] [G loss: 0.934070]\n",
      "epoch:2 step:1890 [D loss: 0.162341, acc.: 83.59%] [G loss: 0.766735]\n",
      "epoch:2 step:1891 [D loss: 0.158962, acc.: 78.91%] [G loss: 0.789617]\n",
      "epoch:2 step:1892 [D loss: 0.158111, acc.: 78.12%] [G loss: 0.808708]\n",
      "epoch:2 step:1893 [D loss: 0.201156, acc.: 78.12%] [G loss: 0.728123]\n",
      "epoch:2 step:1894 [D loss: 0.157417, acc.: 78.91%] [G loss: 0.785438]\n",
      "epoch:2 step:1895 [D loss: 0.122390, acc.: 85.16%] [G loss: 0.964422]\n",
      "epoch:2 step:1896 [D loss: 0.131705, acc.: 82.03%] [G loss: 0.868721]\n",
      "epoch:2 step:1897 [D loss: 0.186167, acc.: 73.44%] [G loss: 0.743319]\n",
      "epoch:2 step:1898 [D loss: 0.143306, acc.: 82.03%] [G loss: 0.822012]\n",
      "epoch:2 step:1899 [D loss: 0.142715, acc.: 82.81%] [G loss: 0.811538]\n",
      "epoch:2 step:1900 [D loss: 0.172637, acc.: 76.56%] [G loss: 0.705187]\n",
      "epoch:2 step:1901 [D loss: 0.110573, acc.: 88.28%] [G loss: 0.918786]\n",
      "epoch:2 step:1902 [D loss: 0.176516, acc.: 76.56%] [G loss: 0.747200]\n",
      "epoch:2 step:1903 [D loss: 0.106015, acc.: 88.28%] [G loss: 0.869003]\n",
      "epoch:2 step:1904 [D loss: 0.164055, acc.: 80.47%] [G loss: 0.781243]\n",
      "epoch:2 step:1905 [D loss: 0.132799, acc.: 83.59%] [G loss: 0.764434]\n",
      "epoch:2 step:1906 [D loss: 0.138159, acc.: 81.25%] [G loss: 0.921940]\n",
      "epoch:2 step:1907 [D loss: 0.115542, acc.: 87.50%] [G loss: 0.877003]\n",
      "epoch:2 step:1908 [D loss: 0.111950, acc.: 85.94%] [G loss: 0.870759]\n",
      "epoch:2 step:1909 [D loss: 0.152107, acc.: 80.47%] [G loss: 0.765625]\n",
      "epoch:2 step:1910 [D loss: 0.164588, acc.: 75.00%] [G loss: 0.853438]\n",
      "epoch:2 step:1911 [D loss: 0.172452, acc.: 75.78%] [G loss: 0.741943]\n",
      "epoch:2 step:1912 [D loss: 0.169226, acc.: 75.78%] [G loss: 0.874483]\n",
      "epoch:2 step:1913 [D loss: 0.147210, acc.: 78.91%] [G loss: 0.773207]\n",
      "epoch:2 step:1914 [D loss: 0.123074, acc.: 86.72%] [G loss: 0.789445]\n",
      "epoch:2 step:1915 [D loss: 0.109349, acc.: 90.62%] [G loss: 0.887504]\n",
      "epoch:2 step:1916 [D loss: 0.132743, acc.: 83.59%] [G loss: 0.796237]\n",
      "epoch:2 step:1917 [D loss: 0.129561, acc.: 85.94%] [G loss: 0.728387]\n",
      "epoch:2 step:1918 [D loss: 0.155246, acc.: 78.12%] [G loss: 0.780218]\n",
      "epoch:2 step:1919 [D loss: 0.107890, acc.: 89.84%] [G loss: 0.895896]\n",
      "epoch:2 step:1920 [D loss: 0.147558, acc.: 78.12%] [G loss: 0.760742]\n",
      "epoch:2 step:1921 [D loss: 0.168721, acc.: 76.56%] [G loss: 0.755055]\n",
      "epoch:2 step:1922 [D loss: 0.167391, acc.: 75.00%] [G loss: 0.782568]\n",
      "epoch:2 step:1923 [D loss: 0.171515, acc.: 75.00%] [G loss: 0.761577]\n",
      "epoch:2 step:1924 [D loss: 0.175224, acc.: 78.91%] [G loss: 0.748883]\n",
      "epoch:2 step:1925 [D loss: 0.167490, acc.: 77.34%] [G loss: 0.780061]\n",
      "epoch:2 step:1926 [D loss: 0.144157, acc.: 80.47%] [G loss: 0.796968]\n",
      "epoch:2 step:1927 [D loss: 0.135424, acc.: 85.94%] [G loss: 0.782109]\n",
      "epoch:2 step:1928 [D loss: 0.148875, acc.: 79.69%] [G loss: 0.757303]\n",
      "epoch:2 step:1929 [D loss: 0.174244, acc.: 78.91%] [G loss: 0.776524]\n",
      "epoch:2 step:1930 [D loss: 0.133791, acc.: 79.69%] [G loss: 0.766190]\n",
      "epoch:2 step:1931 [D loss: 0.158366, acc.: 80.47%] [G loss: 0.861766]\n",
      "epoch:2 step:1932 [D loss: 0.131979, acc.: 81.25%] [G loss: 0.744661]\n",
      "epoch:2 step:1933 [D loss: 0.136875, acc.: 83.59%] [G loss: 0.761506]\n",
      "epoch:2 step:1934 [D loss: 0.128947, acc.: 84.38%] [G loss: 0.817129]\n",
      "epoch:2 step:1935 [D loss: 0.154319, acc.: 82.03%] [G loss: 0.769661]\n",
      "epoch:2 step:1936 [D loss: 0.137100, acc.: 85.94%] [G loss: 0.775599]\n",
      "epoch:2 step:1937 [D loss: 0.147695, acc.: 84.38%] [G loss: 0.748378]\n",
      "epoch:2 step:1938 [D loss: 0.139403, acc.: 82.03%] [G loss: 0.820087]\n",
      "epoch:2 step:1939 [D loss: 0.154839, acc.: 76.56%] [G loss: 0.760780]\n",
      "epoch:2 step:1940 [D loss: 0.127636, acc.: 85.94%] [G loss: 0.789467]\n",
      "epoch:2 step:1941 [D loss: 0.153565, acc.: 83.59%] [G loss: 0.770646]\n",
      "epoch:2 step:1942 [D loss: 0.129982, acc.: 84.38%] [G loss: 0.782305]\n",
      "epoch:2 step:1943 [D loss: 0.165188, acc.: 79.69%] [G loss: 0.744952]\n",
      "epoch:2 step:1944 [D loss: 0.121456, acc.: 90.62%] [G loss: 0.873089]\n",
      "epoch:2 step:1945 [D loss: 0.122181, acc.: 89.06%] [G loss: 0.878498]\n",
      "epoch:2 step:1946 [D loss: 0.107914, acc.: 86.72%] [G loss: 0.814187]\n",
      "epoch:2 step:1947 [D loss: 0.137624, acc.: 85.16%] [G loss: 0.768918]\n",
      "epoch:2 step:1948 [D loss: 0.117158, acc.: 88.28%] [G loss: 0.795005]\n",
      "epoch:2 step:1949 [D loss: 0.167698, acc.: 78.91%] [G loss: 0.790212]\n",
      "epoch:2 step:1950 [D loss: 0.146674, acc.: 78.91%] [G loss: 0.848844]\n",
      "epoch:2 step:1951 [D loss: 0.112949, acc.: 88.28%] [G loss: 0.847952]\n",
      "epoch:2 step:1952 [D loss: 0.171366, acc.: 78.91%] [G loss: 0.668667]\n",
      "epoch:2 step:1953 [D loss: 0.148076, acc.: 84.38%] [G loss: 0.753167]\n",
      "epoch:2 step:1954 [D loss: 0.136310, acc.: 79.69%] [G loss: 0.774009]\n",
      "epoch:2 step:1955 [D loss: 0.140600, acc.: 80.47%] [G loss: 0.718946]\n",
      "epoch:2 step:1956 [D loss: 0.128547, acc.: 89.84%] [G loss: 0.743152]\n",
      "epoch:2 step:1957 [D loss: 0.114401, acc.: 88.28%] [G loss: 0.841101]\n",
      "epoch:2 step:1958 [D loss: 0.148355, acc.: 82.03%] [G loss: 0.713556]\n",
      "epoch:2 step:1959 [D loss: 0.120568, acc.: 84.38%] [G loss: 0.748210]\n",
      "epoch:2 step:1960 [D loss: 0.090965, acc.: 94.53%] [G loss: 0.803829]\n",
      "epoch:2 step:1961 [D loss: 0.134071, acc.: 83.59%] [G loss: 0.821715]\n",
      "epoch:2 step:1962 [D loss: 0.134033, acc.: 85.16%] [G loss: 0.761116]\n",
      "epoch:2 step:1963 [D loss: 0.143755, acc.: 77.34%] [G loss: 0.732035]\n",
      "epoch:2 step:1964 [D loss: 0.152783, acc.: 82.03%] [G loss: 0.719231]\n",
      "epoch:2 step:1965 [D loss: 0.186172, acc.: 77.34%] [G loss: 0.698887]\n",
      "epoch:2 step:1966 [D loss: 0.143112, acc.: 85.16%] [G loss: 0.759778]\n",
      "epoch:2 step:1967 [D loss: 0.133133, acc.: 83.59%] [G loss: 0.791477]\n",
      "epoch:2 step:1968 [D loss: 0.121804, acc.: 85.16%] [G loss: 0.805132]\n",
      "epoch:2 step:1969 [D loss: 0.165797, acc.: 78.12%] [G loss: 0.806358]\n",
      "epoch:2 step:1970 [D loss: 0.118754, acc.: 87.50%] [G loss: 0.853800]\n",
      "epoch:2 step:1971 [D loss: 0.154317, acc.: 81.25%] [G loss: 0.742379]\n",
      "epoch:2 step:1972 [D loss: 0.116742, acc.: 85.94%] [G loss: 0.779430]\n",
      "epoch:2 step:1973 [D loss: 0.126084, acc.: 84.38%] [G loss: 0.764016]\n",
      "epoch:2 step:1974 [D loss: 0.130723, acc.: 87.50%] [G loss: 0.818022]\n",
      "epoch:2 step:1975 [D loss: 0.127362, acc.: 85.94%] [G loss: 0.760782]\n",
      "epoch:2 step:1976 [D loss: 0.154013, acc.: 78.91%] [G loss: 0.685850]\n",
      "epoch:2 step:1977 [D loss: 0.098200, acc.: 89.06%] [G loss: 0.850271]\n",
      "epoch:2 step:1978 [D loss: 0.123771, acc.: 86.72%] [G loss: 0.784138]\n",
      "epoch:2 step:1979 [D loss: 0.206849, acc.: 64.84%] [G loss: 0.676809]\n",
      "epoch:2 step:1980 [D loss: 0.142250, acc.: 81.25%] [G loss: 0.814737]\n",
      "epoch:2 step:1981 [D loss: 0.228570, acc.: 60.94%] [G loss: 0.727746]\n",
      "epoch:2 step:1982 [D loss: 0.127402, acc.: 87.50%] [G loss: 0.815897]\n",
      "epoch:2 step:1983 [D loss: 0.154831, acc.: 78.12%] [G loss: 0.775182]\n",
      "epoch:2 step:1984 [D loss: 0.129664, acc.: 82.03%] [G loss: 0.832354]\n",
      "epoch:2 step:1985 [D loss: 0.184173, acc.: 80.47%] [G loss: 0.721933]\n",
      "epoch:2 step:1986 [D loss: 0.142457, acc.: 82.81%] [G loss: 0.774395]\n",
      "epoch:2 step:1987 [D loss: 0.215193, acc.: 63.28%] [G loss: 0.651312]\n",
      "epoch:2 step:1988 [D loss: 0.164388, acc.: 75.78%] [G loss: 0.780616]\n",
      "epoch:2 step:1989 [D loss: 0.158467, acc.: 78.91%] [G loss: 0.791781]\n",
      "epoch:2 step:1990 [D loss: 0.169039, acc.: 75.00%] [G loss: 0.833768]\n",
      "epoch:2 step:1991 [D loss: 0.157373, acc.: 80.47%] [G loss: 0.754154]\n",
      "epoch:2 step:1992 [D loss: 0.119757, acc.: 87.50%] [G loss: 0.794716]\n",
      "epoch:2 step:1993 [D loss: 0.101882, acc.: 89.84%] [G loss: 0.816873]\n",
      "epoch:2 step:1994 [D loss: 0.215210, acc.: 68.75%] [G loss: 0.678679]\n",
      "epoch:2 step:1995 [D loss: 0.142703, acc.: 85.16%] [G loss: 0.832862]\n",
      "epoch:2 step:1996 [D loss: 0.173123, acc.: 77.34%] [G loss: 0.769077]\n",
      "epoch:2 step:1997 [D loss: 0.162198, acc.: 76.56%] [G loss: 0.728209]\n",
      "epoch:2 step:1998 [D loss: 0.136548, acc.: 82.03%] [G loss: 0.800337]\n",
      "epoch:2 step:1999 [D loss: 0.151319, acc.: 82.03%] [G loss: 0.709353]\n",
      "epoch:2 step:2000 [D loss: 0.126983, acc.: 86.72%] [G loss: 0.759091]\n",
      "##############\n",
      "[5.44623485 3.22997948 8.11640779 5.95911761 5.45832897 7.71450301\n",
      " 7.11221155 6.83953516 6.24393081 4.67723961]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.150979, acc.: 80.47%] [G loss: 0.750175]\n",
      "epoch:2 step:2002 [D loss: 0.146442, acc.: 80.47%] [G loss: 0.773039]\n",
      "epoch:2 step:2003 [D loss: 0.209126, acc.: 66.41%] [G loss: 0.716858]\n",
      "epoch:2 step:2004 [D loss: 0.112246, acc.: 89.06%] [G loss: 0.861308]\n",
      "epoch:2 step:2005 [D loss: 0.139987, acc.: 82.81%] [G loss: 0.789870]\n",
      "epoch:2 step:2006 [D loss: 0.164944, acc.: 78.91%] [G loss: 0.760234]\n",
      "epoch:2 step:2007 [D loss: 0.224990, acc.: 68.75%] [G loss: 0.732121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2008 [D loss: 0.115101, acc.: 88.28%] [G loss: 0.909380]\n",
      "epoch:2 step:2009 [D loss: 0.156672, acc.: 80.47%] [G loss: 0.688389]\n",
      "epoch:2 step:2010 [D loss: 0.152123, acc.: 81.25%] [G loss: 0.731143]\n",
      "epoch:2 step:2011 [D loss: 0.119880, acc.: 87.50%] [G loss: 0.795929]\n",
      "epoch:2 step:2012 [D loss: 0.133443, acc.: 82.03%] [G loss: 0.765857]\n",
      "epoch:2 step:2013 [D loss: 0.173006, acc.: 77.34%] [G loss: 0.669372]\n",
      "epoch:2 step:2014 [D loss: 0.134802, acc.: 86.72%] [G loss: 0.780893]\n",
      "epoch:2 step:2015 [D loss: 0.123683, acc.: 82.81%] [G loss: 0.814301]\n",
      "epoch:2 step:2016 [D loss: 0.113832, acc.: 84.38%] [G loss: 0.865382]\n",
      "epoch:2 step:2017 [D loss: 0.209673, acc.: 71.88%] [G loss: 0.649296]\n",
      "epoch:2 step:2018 [D loss: 0.122635, acc.: 84.38%] [G loss: 0.828501]\n",
      "epoch:2 step:2019 [D loss: 0.159490, acc.: 81.25%] [G loss: 0.750123]\n",
      "epoch:2 step:2020 [D loss: 0.146226, acc.: 82.03%] [G loss: 0.811798]\n",
      "epoch:2 step:2021 [D loss: 0.190142, acc.: 73.44%] [G loss: 0.748518]\n",
      "epoch:2 step:2022 [D loss: 0.104121, acc.: 92.19%] [G loss: 0.814382]\n",
      "epoch:2 step:2023 [D loss: 0.107301, acc.: 87.50%] [G loss: 0.830354]\n",
      "epoch:2 step:2024 [D loss: 0.233544, acc.: 63.28%] [G loss: 0.647190]\n",
      "epoch:2 step:2025 [D loss: 0.146863, acc.: 80.47%] [G loss: 0.815588]\n",
      "epoch:2 step:2026 [D loss: 0.094107, acc.: 91.41%] [G loss: 0.916592]\n",
      "epoch:2 step:2027 [D loss: 0.181163, acc.: 76.56%] [G loss: 0.635022]\n",
      "epoch:2 step:2028 [D loss: 0.111468, acc.: 88.28%] [G loss: 0.849198]\n",
      "epoch:2 step:2029 [D loss: 0.111736, acc.: 85.16%] [G loss: 0.841659]\n",
      "epoch:2 step:2030 [D loss: 0.185671, acc.: 77.34%] [G loss: 0.651800]\n",
      "epoch:2 step:2031 [D loss: 0.121114, acc.: 85.94%] [G loss: 0.766303]\n",
      "epoch:2 step:2032 [D loss: 0.154195, acc.: 76.56%] [G loss: 0.816123]\n",
      "epoch:2 step:2033 [D loss: 0.148793, acc.: 80.47%] [G loss: 0.766019]\n",
      "epoch:2 step:2034 [D loss: 0.200978, acc.: 72.66%] [G loss: 0.703491]\n",
      "epoch:2 step:2035 [D loss: 0.143812, acc.: 83.59%] [G loss: 0.784274]\n",
      "epoch:2 step:2036 [D loss: 0.131123, acc.: 85.94%] [G loss: 0.878367]\n",
      "epoch:2 step:2037 [D loss: 0.140189, acc.: 80.47%] [G loss: 0.724746]\n",
      "epoch:2 step:2038 [D loss: 0.142260, acc.: 81.25%] [G loss: 0.732568]\n",
      "epoch:2 step:2039 [D loss: 0.134776, acc.: 85.16%] [G loss: 0.775590]\n",
      "epoch:2 step:2040 [D loss: 0.143251, acc.: 80.47%] [G loss: 0.739191]\n",
      "epoch:2 step:2041 [D loss: 0.167880, acc.: 75.78%] [G loss: 0.676632]\n",
      "epoch:2 step:2042 [D loss: 0.164482, acc.: 77.34%] [G loss: 0.697018]\n",
      "epoch:2 step:2043 [D loss: 0.156994, acc.: 79.69%] [G loss: 0.744263]\n",
      "epoch:2 step:2044 [D loss: 0.152758, acc.: 75.78%] [G loss: 0.722385]\n",
      "epoch:2 step:2045 [D loss: 0.154997, acc.: 78.12%] [G loss: 0.701214]\n",
      "epoch:2 step:2046 [D loss: 0.118977, acc.: 88.28%] [G loss: 0.722938]\n",
      "epoch:2 step:2047 [D loss: 0.148354, acc.: 78.91%] [G loss: 0.796497]\n",
      "epoch:2 step:2048 [D loss: 0.160820, acc.: 77.34%] [G loss: 0.722448]\n",
      "epoch:2 step:2049 [D loss: 0.121614, acc.: 85.94%] [G loss: 0.767223]\n",
      "epoch:2 step:2050 [D loss: 0.121078, acc.: 89.84%] [G loss: 0.755378]\n",
      "epoch:2 step:2051 [D loss: 0.130154, acc.: 85.16%] [G loss: 0.737196]\n",
      "epoch:2 step:2052 [D loss: 0.142434, acc.: 82.81%] [G loss: 0.710850]\n",
      "epoch:2 step:2053 [D loss: 0.147953, acc.: 81.25%] [G loss: 0.730844]\n",
      "epoch:2 step:2054 [D loss: 0.135984, acc.: 84.38%] [G loss: 0.737188]\n",
      "epoch:2 step:2055 [D loss: 0.147068, acc.: 82.81%] [G loss: 0.748229]\n",
      "epoch:2 step:2056 [D loss: 0.147228, acc.: 82.03%] [G loss: 0.776204]\n",
      "epoch:2 step:2057 [D loss: 0.134738, acc.: 84.38%] [G loss: 0.782584]\n",
      "epoch:2 step:2058 [D loss: 0.172898, acc.: 74.22%] [G loss: 0.682508]\n",
      "epoch:2 step:2059 [D loss: 0.126023, acc.: 84.38%] [G loss: 0.769797]\n",
      "epoch:2 step:2060 [D loss: 0.179055, acc.: 72.66%] [G loss: 0.693734]\n",
      "epoch:2 step:2061 [D loss: 0.152513, acc.: 81.25%] [G loss: 0.765251]\n",
      "epoch:2 step:2062 [D loss: 0.152150, acc.: 83.59%] [G loss: 0.728276]\n",
      "epoch:2 step:2063 [D loss: 0.125116, acc.: 85.16%] [G loss: 0.755566]\n",
      "epoch:2 step:2064 [D loss: 0.112392, acc.: 89.06%] [G loss: 0.855742]\n",
      "epoch:2 step:2065 [D loss: 0.134371, acc.: 83.59%] [G loss: 0.804978]\n",
      "epoch:2 step:2066 [D loss: 0.123009, acc.: 86.72%] [G loss: 0.798649]\n",
      "epoch:2 step:2067 [D loss: 0.163343, acc.: 81.25%] [G loss: 0.732940]\n",
      "epoch:2 step:2068 [D loss: 0.100065, acc.: 89.84%] [G loss: 0.823553]\n",
      "epoch:2 step:2069 [D loss: 0.134003, acc.: 79.69%] [G loss: 0.771395]\n",
      "epoch:2 step:2070 [D loss: 0.122355, acc.: 85.16%] [G loss: 0.723400]\n",
      "epoch:2 step:2071 [D loss: 0.108869, acc.: 88.28%] [G loss: 0.786400]\n",
      "epoch:2 step:2072 [D loss: 0.088807, acc.: 89.84%] [G loss: 0.877209]\n",
      "epoch:2 step:2073 [D loss: 0.140358, acc.: 83.59%] [G loss: 0.661309]\n",
      "epoch:2 step:2074 [D loss: 0.139152, acc.: 83.59%] [G loss: 0.776463]\n",
      "epoch:2 step:2075 [D loss: 0.138847, acc.: 82.03%] [G loss: 0.795717]\n",
      "epoch:2 step:2076 [D loss: 0.119165, acc.: 85.16%] [G loss: 0.772128]\n",
      "epoch:2 step:2077 [D loss: 0.185119, acc.: 74.22%] [G loss: 0.707019]\n",
      "epoch:2 step:2078 [D loss: 0.110012, acc.: 88.28%] [G loss: 0.863241]\n",
      "epoch:2 step:2079 [D loss: 0.110467, acc.: 89.06%] [G loss: 0.850154]\n",
      "epoch:2 step:2080 [D loss: 0.156899, acc.: 78.12%] [G loss: 0.661347]\n",
      "epoch:2 step:2081 [D loss: 0.091714, acc.: 90.62%] [G loss: 0.884084]\n",
      "epoch:2 step:2082 [D loss: 0.131629, acc.: 83.59%] [G loss: 0.813972]\n",
      "epoch:2 step:2083 [D loss: 0.144745, acc.: 81.25%] [G loss: 0.737855]\n",
      "epoch:2 step:2084 [D loss: 0.180866, acc.: 72.66%] [G loss: 0.772256]\n",
      "epoch:2 step:2085 [D loss: 0.139447, acc.: 80.47%] [G loss: 0.778987]\n",
      "epoch:2 step:2086 [D loss: 0.125368, acc.: 86.72%] [G loss: 0.808851]\n",
      "epoch:2 step:2087 [D loss: 0.154855, acc.: 77.34%] [G loss: 0.780088]\n",
      "epoch:2 step:2088 [D loss: 0.224110, acc.: 62.50%] [G loss: 0.663640]\n",
      "epoch:2 step:2089 [D loss: 0.138618, acc.: 81.25%] [G loss: 0.724645]\n",
      "epoch:2 step:2090 [D loss: 0.137716, acc.: 82.03%] [G loss: 0.751869]\n",
      "epoch:2 step:2091 [D loss: 0.143988, acc.: 82.03%] [G loss: 0.708619]\n",
      "epoch:2 step:2092 [D loss: 0.126932, acc.: 84.38%] [G loss: 0.756776]\n",
      "epoch:2 step:2093 [D loss: 0.170881, acc.: 77.34%] [G loss: 0.731577]\n",
      "epoch:2 step:2094 [D loss: 0.163140, acc.: 79.69%] [G loss: 0.757504]\n",
      "epoch:2 step:2095 [D loss: 0.129110, acc.: 85.94%] [G loss: 0.777243]\n",
      "epoch:2 step:2096 [D loss: 0.162234, acc.: 75.00%] [G loss: 0.688936]\n",
      "epoch:2 step:2097 [D loss: 0.117433, acc.: 84.38%] [G loss: 0.800098]\n",
      "epoch:2 step:2098 [D loss: 0.184746, acc.: 75.00%] [G loss: 0.713529]\n",
      "epoch:2 step:2099 [D loss: 0.179952, acc.: 70.31%] [G loss: 0.730051]\n",
      "epoch:2 step:2100 [D loss: 0.169038, acc.: 78.91%] [G loss: 0.763855]\n",
      "epoch:2 step:2101 [D loss: 0.211571, acc.: 67.97%] [G loss: 0.613321]\n",
      "epoch:2 step:2102 [D loss: 0.184255, acc.: 75.78%] [G loss: 0.750007]\n",
      "epoch:2 step:2103 [D loss: 0.173271, acc.: 77.34%] [G loss: 0.730277]\n",
      "epoch:2 step:2104 [D loss: 0.166019, acc.: 76.56%] [G loss: 0.736614]\n",
      "epoch:2 step:2105 [D loss: 0.143131, acc.: 80.47%] [G loss: 0.810703]\n",
      "epoch:2 step:2106 [D loss: 0.127856, acc.: 84.38%] [G loss: 0.774053]\n",
      "epoch:2 step:2107 [D loss: 0.257438, acc.: 57.03%] [G loss: 0.573723]\n",
      "epoch:2 step:2108 [D loss: 0.179419, acc.: 73.44%] [G loss: 0.692180]\n",
      "epoch:2 step:2109 [D loss: 0.168443, acc.: 76.56%] [G loss: 0.716434]\n",
      "epoch:2 step:2110 [D loss: 0.143392, acc.: 80.47%] [G loss: 0.722145]\n",
      "epoch:2 step:2111 [D loss: 0.199342, acc.: 71.88%] [G loss: 0.644810]\n",
      "epoch:2 step:2112 [D loss: 0.136452, acc.: 82.81%] [G loss: 0.749679]\n",
      "epoch:2 step:2113 [D loss: 0.166694, acc.: 73.44%] [G loss: 0.664357]\n",
      "epoch:2 step:2114 [D loss: 0.134297, acc.: 82.03%] [G loss: 0.778347]\n",
      "epoch:2 step:2115 [D loss: 0.189585, acc.: 69.53%] [G loss: 0.738239]\n",
      "epoch:2 step:2116 [D loss: 0.140156, acc.: 80.47%] [G loss: 0.755838]\n",
      "epoch:2 step:2117 [D loss: 0.176036, acc.: 78.12%] [G loss: 0.721614]\n",
      "epoch:2 step:2118 [D loss: 0.156527, acc.: 76.56%] [G loss: 0.751898]\n",
      "epoch:2 step:2119 [D loss: 0.176413, acc.: 74.22%] [G loss: 0.768528]\n",
      "epoch:2 step:2120 [D loss: 0.191874, acc.: 71.88%] [G loss: 0.733041]\n",
      "epoch:2 step:2121 [D loss: 0.206242, acc.: 67.19%] [G loss: 0.670466]\n",
      "epoch:2 step:2122 [D loss: 0.168885, acc.: 74.22%] [G loss: 0.709691]\n",
      "epoch:2 step:2123 [D loss: 0.143594, acc.: 83.59%] [G loss: 0.729528]\n",
      "epoch:2 step:2124 [D loss: 0.157122, acc.: 82.03%] [G loss: 0.661341]\n",
      "epoch:2 step:2125 [D loss: 0.167045, acc.: 77.34%] [G loss: 0.717887]\n",
      "epoch:2 step:2126 [D loss: 0.144931, acc.: 82.81%] [G loss: 0.765141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2127 [D loss: 0.151528, acc.: 79.69%] [G loss: 0.744204]\n",
      "epoch:2 step:2128 [D loss: 0.130684, acc.: 83.59%] [G loss: 0.805115]\n",
      "epoch:2 step:2129 [D loss: 0.135349, acc.: 84.38%] [G loss: 0.743827]\n",
      "epoch:2 step:2130 [D loss: 0.168498, acc.: 77.34%] [G loss: 0.707055]\n",
      "epoch:2 step:2131 [D loss: 0.139007, acc.: 82.03%] [G loss: 0.729262]\n",
      "epoch:2 step:2132 [D loss: 0.142876, acc.: 82.03%] [G loss: 0.727593]\n",
      "epoch:2 step:2133 [D loss: 0.114402, acc.: 91.41%] [G loss: 0.774505]\n",
      "epoch:2 step:2134 [D loss: 0.153170, acc.: 81.25%] [G loss: 0.676463]\n",
      "epoch:2 step:2135 [D loss: 0.131593, acc.: 86.72%] [G loss: 0.722393]\n",
      "epoch:2 step:2136 [D loss: 0.165940, acc.: 81.25%] [G loss: 0.668365]\n",
      "epoch:2 step:2137 [D loss: 0.223700, acc.: 68.75%] [G loss: 0.604535]\n",
      "epoch:2 step:2138 [D loss: 0.154704, acc.: 78.91%] [G loss: 0.694877]\n",
      "epoch:2 step:2139 [D loss: 0.149478, acc.: 81.25%] [G loss: 0.717302]\n",
      "epoch:2 step:2140 [D loss: 0.178296, acc.: 74.22%] [G loss: 0.650373]\n",
      "epoch:2 step:2141 [D loss: 0.155007, acc.: 81.25%] [G loss: 0.680459]\n",
      "epoch:2 step:2142 [D loss: 0.173187, acc.: 75.00%] [G loss: 0.672123]\n",
      "epoch:2 step:2143 [D loss: 0.157119, acc.: 74.22%] [G loss: 0.716957]\n",
      "epoch:2 step:2144 [D loss: 0.140215, acc.: 85.16%] [G loss: 0.799795]\n",
      "epoch:2 step:2145 [D loss: 0.147621, acc.: 80.47%] [G loss: 0.690214]\n",
      "epoch:2 step:2146 [D loss: 0.176841, acc.: 75.78%] [G loss: 0.655875]\n",
      "epoch:2 step:2147 [D loss: 0.149285, acc.: 82.81%] [G loss: 0.772506]\n",
      "epoch:2 step:2148 [D loss: 0.172496, acc.: 78.12%] [G loss: 0.716110]\n",
      "epoch:2 step:2149 [D loss: 0.175420, acc.: 76.56%] [G loss: 0.666065]\n",
      "epoch:2 step:2150 [D loss: 0.164421, acc.: 78.91%] [G loss: 0.741547]\n",
      "epoch:2 step:2151 [D loss: 0.179871, acc.: 77.34%] [G loss: 0.761790]\n",
      "epoch:2 step:2152 [D loss: 0.218578, acc.: 67.19%] [G loss: 0.667056]\n",
      "epoch:2 step:2153 [D loss: 0.136156, acc.: 82.81%] [G loss: 0.813696]\n",
      "epoch:2 step:2154 [D loss: 0.163815, acc.: 77.34%] [G loss: 0.722055]\n",
      "epoch:2 step:2155 [D loss: 0.171470, acc.: 76.56%] [G loss: 0.685077]\n",
      "epoch:2 step:2156 [D loss: 0.159093, acc.: 78.12%] [G loss: 0.708417]\n",
      "epoch:2 step:2157 [D loss: 0.164297, acc.: 77.34%] [G loss: 0.741904]\n",
      "epoch:2 step:2158 [D loss: 0.128678, acc.: 82.81%] [G loss: 0.770696]\n",
      "epoch:2 step:2159 [D loss: 0.106043, acc.: 88.28%] [G loss: 0.785095]\n",
      "epoch:2 step:2160 [D loss: 0.130783, acc.: 86.72%] [G loss: 0.750831]\n",
      "epoch:2 step:2161 [D loss: 0.142888, acc.: 83.59%] [G loss: 0.711691]\n",
      "epoch:2 step:2162 [D loss: 0.170186, acc.: 78.91%] [G loss: 0.725098]\n",
      "epoch:2 step:2163 [D loss: 0.160901, acc.: 75.78%] [G loss: 0.711706]\n",
      "epoch:2 step:2164 [D loss: 0.138713, acc.: 83.59%] [G loss: 0.736328]\n",
      "epoch:2 step:2165 [D loss: 0.154480, acc.: 78.91%] [G loss: 0.744875]\n",
      "epoch:2 step:2166 [D loss: 0.184948, acc.: 75.78%] [G loss: 0.735716]\n",
      "epoch:2 step:2167 [D loss: 0.178562, acc.: 71.09%] [G loss: 0.687576]\n",
      "epoch:2 step:2168 [D loss: 0.158481, acc.: 77.34%] [G loss: 0.707343]\n",
      "epoch:2 step:2169 [D loss: 0.157581, acc.: 81.25%] [G loss: 0.667029]\n",
      "epoch:2 step:2170 [D loss: 0.159446, acc.: 81.25%] [G loss: 0.700613]\n",
      "epoch:2 step:2171 [D loss: 0.171244, acc.: 71.09%] [G loss: 0.654191]\n",
      "epoch:2 step:2172 [D loss: 0.194360, acc.: 70.31%] [G loss: 0.626312]\n",
      "epoch:2 step:2173 [D loss: 0.144493, acc.: 82.03%] [G loss: 0.726114]\n",
      "epoch:2 step:2174 [D loss: 0.150409, acc.: 82.81%] [G loss: 0.731811]\n",
      "epoch:2 step:2175 [D loss: 0.168518, acc.: 75.00%] [G loss: 0.675166]\n",
      "epoch:2 step:2176 [D loss: 0.110193, acc.: 87.50%] [G loss: 0.770466]\n",
      "epoch:2 step:2177 [D loss: 0.145340, acc.: 82.03%] [G loss: 0.733810]\n",
      "epoch:2 step:2178 [D loss: 0.138029, acc.: 84.38%] [G loss: 0.802654]\n",
      "epoch:2 step:2179 [D loss: 0.163570, acc.: 79.69%] [G loss: 0.678935]\n",
      "epoch:2 step:2180 [D loss: 0.170327, acc.: 77.34%] [G loss: 0.708888]\n",
      "epoch:2 step:2181 [D loss: 0.132934, acc.: 85.94%] [G loss: 0.754290]\n",
      "epoch:2 step:2182 [D loss: 0.136100, acc.: 83.59%] [G loss: 0.700532]\n",
      "epoch:2 step:2183 [D loss: 0.120428, acc.: 85.16%] [G loss: 0.779732]\n",
      "epoch:2 step:2184 [D loss: 0.155221, acc.: 78.91%] [G loss: 0.704226]\n",
      "epoch:2 step:2185 [D loss: 0.126925, acc.: 82.03%] [G loss: 0.745615]\n",
      "epoch:2 step:2186 [D loss: 0.140169, acc.: 82.81%] [G loss: 0.715091]\n",
      "epoch:2 step:2187 [D loss: 0.152647, acc.: 80.47%] [G loss: 0.805247]\n",
      "epoch:2 step:2188 [D loss: 0.114988, acc.: 88.28%] [G loss: 0.783880]\n",
      "epoch:2 step:2189 [D loss: 0.162406, acc.: 78.12%] [G loss: 0.674153]\n",
      "epoch:2 step:2190 [D loss: 0.261901, acc.: 58.59%] [G loss: 0.573941]\n",
      "epoch:2 step:2191 [D loss: 0.189694, acc.: 71.88%] [G loss: 0.675822]\n",
      "epoch:2 step:2192 [D loss: 0.179731, acc.: 75.00%] [G loss: 0.625954]\n",
      "epoch:2 step:2193 [D loss: 0.147697, acc.: 82.03%] [G loss: 0.645498]\n",
      "epoch:2 step:2194 [D loss: 0.137570, acc.: 85.94%] [G loss: 0.708330]\n",
      "epoch:2 step:2195 [D loss: 0.125502, acc.: 85.16%] [G loss: 0.770563]\n",
      "epoch:2 step:2196 [D loss: 0.146641, acc.: 81.25%] [G loss: 0.676481]\n",
      "epoch:2 step:2197 [D loss: 0.154006, acc.: 82.03%] [G loss: 0.766158]\n",
      "epoch:2 step:2198 [D loss: 0.176895, acc.: 71.09%] [G loss: 0.728004]\n",
      "epoch:2 step:2199 [D loss: 0.143246, acc.: 83.59%] [G loss: 0.645299]\n",
      "epoch:2 step:2200 [D loss: 0.163537, acc.: 75.78%] [G loss: 0.657741]\n",
      "##############\n",
      "[5.17069485 2.45914101 7.63866205 6.47857573 5.01488129 7.65120312\n",
      " 7.36013082 5.67121755 6.48822352 4.61205347]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.154480, acc.: 80.47%] [G loss: 0.688451]\n",
      "epoch:2 step:2202 [D loss: 0.161776, acc.: 78.12%] [G loss: 0.760260]\n",
      "epoch:2 step:2203 [D loss: 0.157766, acc.: 78.91%] [G loss: 0.681019]\n",
      "epoch:2 step:2204 [D loss: 0.162132, acc.: 78.91%] [G loss: 0.677770]\n",
      "epoch:2 step:2205 [D loss: 0.148144, acc.: 81.25%] [G loss: 0.667720]\n",
      "epoch:2 step:2206 [D loss: 0.126972, acc.: 88.28%] [G loss: 0.735362]\n",
      "epoch:2 step:2207 [D loss: 0.177453, acc.: 77.34%] [G loss: 0.626774]\n",
      "epoch:2 step:2208 [D loss: 0.161377, acc.: 73.44%] [G loss: 0.660317]\n",
      "epoch:2 step:2209 [D loss: 0.135379, acc.: 80.47%] [G loss: 0.758133]\n",
      "epoch:2 step:2210 [D loss: 0.130466, acc.: 85.16%] [G loss: 0.752402]\n",
      "epoch:2 step:2211 [D loss: 0.147335, acc.: 80.47%] [G loss: 0.693515]\n",
      "epoch:2 step:2212 [D loss: 0.144109, acc.: 79.69%] [G loss: 0.765951]\n",
      "epoch:2 step:2213 [D loss: 0.134893, acc.: 87.50%] [G loss: 0.687204]\n",
      "epoch:2 step:2214 [D loss: 0.173790, acc.: 75.78%] [G loss: 0.705284]\n",
      "epoch:2 step:2215 [D loss: 0.154446, acc.: 78.91%] [G loss: 0.643453]\n",
      "epoch:2 step:2216 [D loss: 0.131119, acc.: 83.59%] [G loss: 0.658026]\n",
      "epoch:2 step:2217 [D loss: 0.104538, acc.: 89.84%] [G loss: 0.803161]\n",
      "epoch:2 step:2218 [D loss: 0.123606, acc.: 85.94%] [G loss: 0.777714]\n",
      "epoch:2 step:2219 [D loss: 0.155022, acc.: 76.56%] [G loss: 0.707445]\n",
      "epoch:2 step:2220 [D loss: 0.137466, acc.: 83.59%] [G loss: 0.723269]\n",
      "epoch:2 step:2221 [D loss: 0.116777, acc.: 86.72%] [G loss: 0.810783]\n",
      "epoch:2 step:2222 [D loss: 0.186128, acc.: 75.78%] [G loss: 0.714965]\n",
      "epoch:2 step:2223 [D loss: 0.228104, acc.: 61.72%] [G loss: 0.607811]\n",
      "epoch:2 step:2224 [D loss: 0.144357, acc.: 81.25%] [G loss: 0.723909]\n",
      "epoch:2 step:2225 [D loss: 0.137885, acc.: 85.16%] [G loss: 0.794759]\n",
      "epoch:2 step:2226 [D loss: 0.210332, acc.: 68.75%] [G loss: 0.602055]\n",
      "epoch:2 step:2227 [D loss: 0.144847, acc.: 81.25%] [G loss: 0.715191]\n",
      "epoch:2 step:2228 [D loss: 0.115738, acc.: 84.38%] [G loss: 0.748009]\n",
      "epoch:2 step:2229 [D loss: 0.137816, acc.: 82.03%] [G loss: 0.713275]\n",
      "epoch:2 step:2230 [D loss: 0.142032, acc.: 85.94%] [G loss: 0.697177]\n",
      "epoch:2 step:2231 [D loss: 0.158095, acc.: 78.91%] [G loss: 0.731109]\n",
      "epoch:2 step:2232 [D loss: 0.124941, acc.: 87.50%] [G loss: 0.732820]\n",
      "epoch:2 step:2233 [D loss: 0.159139, acc.: 78.91%] [G loss: 0.720675]\n",
      "epoch:2 step:2234 [D loss: 0.134407, acc.: 84.38%] [G loss: 0.740323]\n",
      "epoch:2 step:2235 [D loss: 0.160158, acc.: 80.47%] [G loss: 0.660468]\n",
      "epoch:2 step:2236 [D loss: 0.143301, acc.: 77.34%] [G loss: 0.693933]\n",
      "epoch:2 step:2237 [D loss: 0.133509, acc.: 83.59%] [G loss: 0.731506]\n",
      "epoch:2 step:2238 [D loss: 0.154317, acc.: 75.78%] [G loss: 0.700249]\n",
      "epoch:2 step:2239 [D loss: 0.138519, acc.: 78.91%] [G loss: 0.770031]\n",
      "epoch:2 step:2240 [D loss: 0.136895, acc.: 75.00%] [G loss: 0.731132]\n",
      "epoch:2 step:2241 [D loss: 0.153176, acc.: 76.56%] [G loss: 0.755593]\n",
      "epoch:2 step:2242 [D loss: 0.130668, acc.: 85.16%] [G loss: 0.717239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2243 [D loss: 0.170140, acc.: 76.56%] [G loss: 0.647181]\n",
      "epoch:2 step:2244 [D loss: 0.151116, acc.: 79.69%] [G loss: 0.708601]\n",
      "epoch:2 step:2245 [D loss: 0.141205, acc.: 82.81%] [G loss: 0.726710]\n",
      "epoch:2 step:2246 [D loss: 0.147683, acc.: 83.59%] [G loss: 0.706496]\n",
      "epoch:2 step:2247 [D loss: 0.161080, acc.: 80.47%] [G loss: 0.620192]\n",
      "epoch:2 step:2248 [D loss: 0.155861, acc.: 75.00%] [G loss: 0.715204]\n",
      "epoch:2 step:2249 [D loss: 0.142417, acc.: 82.03%] [G loss: 0.727465]\n",
      "epoch:2 step:2250 [D loss: 0.215484, acc.: 69.53%] [G loss: 0.622146]\n",
      "epoch:2 step:2251 [D loss: 0.136710, acc.: 81.25%] [G loss: 0.736656]\n",
      "epoch:2 step:2252 [D loss: 0.144554, acc.: 78.91%] [G loss: 0.771920]\n",
      "epoch:2 step:2253 [D loss: 0.161012, acc.: 81.25%] [G loss: 0.666201]\n",
      "epoch:2 step:2254 [D loss: 0.172231, acc.: 77.34%] [G loss: 0.653023]\n",
      "epoch:2 step:2255 [D loss: 0.130088, acc.: 83.59%] [G loss: 0.752761]\n",
      "epoch:2 step:2256 [D loss: 0.140731, acc.: 85.16%] [G loss: 0.701725]\n",
      "epoch:2 step:2257 [D loss: 0.179010, acc.: 76.56%] [G loss: 0.631844]\n",
      "epoch:2 step:2258 [D loss: 0.134715, acc.: 80.47%] [G loss: 0.676521]\n",
      "epoch:2 step:2259 [D loss: 0.142659, acc.: 82.03%] [G loss: 0.665908]\n",
      "epoch:2 step:2260 [D loss: 0.187401, acc.: 77.34%] [G loss: 0.642747]\n",
      "epoch:2 step:2261 [D loss: 0.169408, acc.: 75.78%] [G loss: 0.676734]\n",
      "epoch:2 step:2262 [D loss: 0.145234, acc.: 81.25%] [G loss: 0.715869]\n",
      "epoch:2 step:2263 [D loss: 0.162806, acc.: 81.25%] [G loss: 0.619658]\n",
      "epoch:2 step:2264 [D loss: 0.172396, acc.: 75.00%] [G loss: 0.671602]\n",
      "epoch:2 step:2265 [D loss: 0.124447, acc.: 88.28%] [G loss: 0.711486]\n",
      "epoch:2 step:2266 [D loss: 0.116628, acc.: 86.72%] [G loss: 0.770951]\n",
      "epoch:2 step:2267 [D loss: 0.163999, acc.: 78.12%] [G loss: 0.652548]\n",
      "epoch:2 step:2268 [D loss: 0.105725, acc.: 89.84%] [G loss: 0.752727]\n",
      "epoch:2 step:2269 [D loss: 0.144997, acc.: 86.72%] [G loss: 0.753037]\n",
      "epoch:2 step:2270 [D loss: 0.180279, acc.: 72.66%] [G loss: 0.626673]\n",
      "epoch:2 step:2271 [D loss: 0.102925, acc.: 90.62%] [G loss: 0.831335]\n",
      "epoch:2 step:2272 [D loss: 0.128917, acc.: 81.25%] [G loss: 0.745478]\n",
      "epoch:2 step:2273 [D loss: 0.138212, acc.: 85.94%] [G loss: 0.784679]\n",
      "epoch:2 step:2274 [D loss: 0.147297, acc.: 77.34%] [G loss: 0.675202]\n",
      "epoch:2 step:2275 [D loss: 0.127961, acc.: 85.16%] [G loss: 0.742452]\n",
      "epoch:2 step:2276 [D loss: 0.123001, acc.: 86.72%] [G loss: 0.710549]\n",
      "epoch:2 step:2277 [D loss: 0.136275, acc.: 82.03%] [G loss: 0.710645]\n",
      "epoch:2 step:2278 [D loss: 0.201795, acc.: 69.53%] [G loss: 0.652085]\n",
      "epoch:2 step:2279 [D loss: 0.158893, acc.: 77.34%] [G loss: 0.753542]\n",
      "epoch:2 step:2280 [D loss: 0.177833, acc.: 71.09%] [G loss: 0.698757]\n",
      "epoch:2 step:2281 [D loss: 0.133053, acc.: 82.81%] [G loss: 0.772367]\n",
      "epoch:2 step:2282 [D loss: 0.186610, acc.: 75.00%] [G loss: 0.694114]\n",
      "epoch:2 step:2283 [D loss: 0.128938, acc.: 85.94%] [G loss: 0.759009]\n",
      "epoch:2 step:2284 [D loss: 0.194630, acc.: 75.78%] [G loss: 0.613031]\n",
      "epoch:2 step:2285 [D loss: 0.172150, acc.: 71.88%] [G loss: 0.665572]\n",
      "epoch:2 step:2286 [D loss: 0.157612, acc.: 80.47%] [G loss: 0.760693]\n",
      "epoch:2 step:2287 [D loss: 0.166798, acc.: 78.91%] [G loss: 0.701988]\n",
      "epoch:2 step:2288 [D loss: 0.174205, acc.: 72.66%] [G loss: 0.676889]\n",
      "epoch:2 step:2289 [D loss: 0.149485, acc.: 78.91%] [G loss: 0.753201]\n",
      "epoch:2 step:2290 [D loss: 0.175791, acc.: 75.00%] [G loss: 0.754628]\n",
      "epoch:2 step:2291 [D loss: 0.228361, acc.: 65.62%] [G loss: 0.626153]\n",
      "epoch:2 step:2292 [D loss: 0.168843, acc.: 77.34%] [G loss: 0.674683]\n",
      "epoch:2 step:2293 [D loss: 0.130720, acc.: 85.16%] [G loss: 0.719247]\n",
      "epoch:2 step:2294 [D loss: 0.178009, acc.: 71.88%] [G loss: 0.743133]\n",
      "epoch:2 step:2295 [D loss: 0.152912, acc.: 79.69%] [G loss: 0.647989]\n",
      "epoch:2 step:2296 [D loss: 0.124898, acc.: 84.38%] [G loss: 0.721007]\n",
      "epoch:2 step:2297 [D loss: 0.128019, acc.: 84.38%] [G loss: 0.714325]\n",
      "epoch:2 step:2298 [D loss: 0.133253, acc.: 85.16%] [G loss: 0.728455]\n",
      "epoch:2 step:2299 [D loss: 0.134171, acc.: 85.94%] [G loss: 0.777505]\n",
      "epoch:2 step:2300 [D loss: 0.122530, acc.: 84.38%] [G loss: 0.726639]\n",
      "epoch:2 step:2301 [D loss: 0.131236, acc.: 79.69%] [G loss: 0.833516]\n",
      "epoch:2 step:2302 [D loss: 0.125657, acc.: 85.94%] [G loss: 0.765439]\n",
      "epoch:2 step:2303 [D loss: 0.142484, acc.: 81.25%] [G loss: 0.731911]\n",
      "epoch:2 step:2304 [D loss: 0.092952, acc.: 91.41%] [G loss: 0.807302]\n",
      "epoch:2 step:2305 [D loss: 0.156584, acc.: 83.59%] [G loss: 0.737009]\n",
      "epoch:2 step:2306 [D loss: 0.147705, acc.: 81.25%] [G loss: 0.659542]\n",
      "epoch:2 step:2307 [D loss: 0.165359, acc.: 81.25%] [G loss: 0.658658]\n",
      "epoch:2 step:2308 [D loss: 0.140203, acc.: 81.25%] [G loss: 0.722358]\n",
      "epoch:2 step:2309 [D loss: 0.158252, acc.: 78.91%] [G loss: 0.702666]\n",
      "epoch:2 step:2310 [D loss: 0.131201, acc.: 85.94%] [G loss: 0.751307]\n",
      "epoch:2 step:2311 [D loss: 0.226334, acc.: 68.75%] [G loss: 0.597581]\n",
      "epoch:2 step:2312 [D loss: 0.186188, acc.: 70.31%] [G loss: 0.714023]\n",
      "epoch:2 step:2313 [D loss: 0.164203, acc.: 75.00%] [G loss: 0.725295]\n",
      "epoch:2 step:2314 [D loss: 0.167384, acc.: 77.34%] [G loss: 0.693770]\n",
      "epoch:2 step:2315 [D loss: 0.146580, acc.: 79.69%] [G loss: 0.752804]\n",
      "epoch:2 step:2316 [D loss: 0.179402, acc.: 74.22%] [G loss: 0.686536]\n",
      "epoch:2 step:2317 [D loss: 0.148227, acc.: 78.12%] [G loss: 0.692084]\n",
      "epoch:2 step:2318 [D loss: 0.164047, acc.: 75.00%] [G loss: 0.730928]\n",
      "epoch:2 step:2319 [D loss: 0.136620, acc.: 84.38%] [G loss: 0.725740]\n",
      "epoch:2 step:2320 [D loss: 0.157702, acc.: 81.25%] [G loss: 0.691455]\n",
      "epoch:2 step:2321 [D loss: 0.130164, acc.: 84.38%] [G loss: 0.671834]\n",
      "epoch:2 step:2322 [D loss: 0.182296, acc.: 77.34%] [G loss: 0.675778]\n",
      "epoch:2 step:2323 [D loss: 0.142756, acc.: 82.81%] [G loss: 0.736853]\n",
      "epoch:2 step:2324 [D loss: 0.104806, acc.: 88.28%] [G loss: 0.737727]\n",
      "epoch:2 step:2325 [D loss: 0.100936, acc.: 88.28%] [G loss: 0.809749]\n",
      "epoch:2 step:2326 [D loss: 0.149163, acc.: 85.16%] [G loss: 0.743293]\n",
      "epoch:2 step:2327 [D loss: 0.133927, acc.: 82.81%] [G loss: 0.748774]\n",
      "epoch:2 step:2328 [D loss: 0.170270, acc.: 81.25%] [G loss: 0.659601]\n",
      "epoch:2 step:2329 [D loss: 0.148955, acc.: 83.59%] [G loss: 0.703210]\n",
      "epoch:2 step:2330 [D loss: 0.175554, acc.: 72.66%] [G loss: 0.635064]\n",
      "epoch:2 step:2331 [D loss: 0.136050, acc.: 81.25%] [G loss: 0.742816]\n",
      "epoch:2 step:2332 [D loss: 0.153015, acc.: 77.34%] [G loss: 0.701688]\n",
      "epoch:2 step:2333 [D loss: 0.133499, acc.: 80.47%] [G loss: 0.717774]\n",
      "epoch:2 step:2334 [D loss: 0.154055, acc.: 79.69%] [G loss: 0.705234]\n",
      "epoch:2 step:2335 [D loss: 0.132571, acc.: 84.38%] [G loss: 0.752973]\n",
      "epoch:2 step:2336 [D loss: 0.134281, acc.: 82.81%] [G loss: 0.740911]\n",
      "epoch:2 step:2337 [D loss: 0.165685, acc.: 78.91%] [G loss: 0.685358]\n",
      "epoch:2 step:2338 [D loss: 0.129925, acc.: 85.94%] [G loss: 0.751684]\n",
      "epoch:2 step:2339 [D loss: 0.215902, acc.: 66.41%] [G loss: 0.602781]\n",
      "epoch:2 step:2340 [D loss: 0.136740, acc.: 79.69%] [G loss: 0.749573]\n",
      "epoch:2 step:2341 [D loss: 0.152193, acc.: 80.47%] [G loss: 0.729285]\n",
      "epoch:2 step:2342 [D loss: 0.151640, acc.: 79.69%] [G loss: 0.751862]\n",
      "epoch:2 step:2343 [D loss: 0.149360, acc.: 79.69%] [G loss: 0.688661]\n",
      "epoch:2 step:2344 [D loss: 0.140502, acc.: 82.03%] [G loss: 0.702745]\n",
      "epoch:2 step:2345 [D loss: 0.117669, acc.: 85.94%] [G loss: 0.779401]\n",
      "epoch:2 step:2346 [D loss: 0.137308, acc.: 85.16%] [G loss: 0.752566]\n",
      "epoch:2 step:2347 [D loss: 0.173506, acc.: 72.66%] [G loss: 0.719553]\n",
      "epoch:2 step:2348 [D loss: 0.130713, acc.: 85.16%] [G loss: 0.749190]\n",
      "epoch:2 step:2349 [D loss: 0.154674, acc.: 78.91%] [G loss: 0.716579]\n",
      "epoch:2 step:2350 [D loss: 0.182663, acc.: 75.00%] [G loss: 0.683766]\n",
      "epoch:2 step:2351 [D loss: 0.199401, acc.: 71.09%] [G loss: 0.625988]\n",
      "epoch:2 step:2352 [D loss: 0.163097, acc.: 73.44%] [G loss: 0.646189]\n",
      "epoch:2 step:2353 [D loss: 0.210236, acc.: 69.53%] [G loss: 0.590851]\n",
      "epoch:2 step:2354 [D loss: 0.165779, acc.: 76.56%] [G loss: 0.666470]\n",
      "epoch:2 step:2355 [D loss: 0.199902, acc.: 69.53%] [G loss: 0.676868]\n",
      "epoch:2 step:2356 [D loss: 0.149678, acc.: 78.91%] [G loss: 0.686362]\n",
      "epoch:2 step:2357 [D loss: 0.175412, acc.: 83.59%] [G loss: 0.646278]\n",
      "epoch:2 step:2358 [D loss: 0.140095, acc.: 81.25%] [G loss: 0.724175]\n",
      "epoch:2 step:2359 [D loss: 0.141445, acc.: 82.03%] [G loss: 0.706661]\n",
      "epoch:2 step:2360 [D loss: 0.130025, acc.: 83.59%] [G loss: 0.724005]\n",
      "epoch:2 step:2361 [D loss: 0.155152, acc.: 80.47%] [G loss: 0.721412]\n",
      "epoch:2 step:2362 [D loss: 0.167668, acc.: 74.22%] [G loss: 0.681138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2363 [D loss: 0.163247, acc.: 78.91%] [G loss: 0.645455]\n",
      "epoch:2 step:2364 [D loss: 0.175695, acc.: 75.78%] [G loss: 0.667587]\n",
      "epoch:2 step:2365 [D loss: 0.152800, acc.: 78.91%] [G loss: 0.700987]\n",
      "epoch:2 step:2366 [D loss: 0.154351, acc.: 82.03%] [G loss: 0.662349]\n",
      "epoch:2 step:2367 [D loss: 0.222928, acc.: 64.84%] [G loss: 0.601592]\n",
      "epoch:2 step:2368 [D loss: 0.137879, acc.: 82.03%] [G loss: 0.749215]\n",
      "epoch:2 step:2369 [D loss: 0.190941, acc.: 71.88%] [G loss: 0.681182]\n",
      "epoch:2 step:2370 [D loss: 0.194078, acc.: 71.09%] [G loss: 0.628919]\n",
      "epoch:2 step:2371 [D loss: 0.170024, acc.: 80.47%] [G loss: 0.716780]\n",
      "epoch:2 step:2372 [D loss: 0.152631, acc.: 78.12%] [G loss: 0.723095]\n",
      "epoch:2 step:2373 [D loss: 0.135702, acc.: 86.72%] [G loss: 0.692174]\n",
      "epoch:2 step:2374 [D loss: 0.221178, acc.: 67.19%] [G loss: 0.600154]\n",
      "epoch:2 step:2375 [D loss: 0.190695, acc.: 73.44%] [G loss: 0.594864]\n",
      "epoch:2 step:2376 [D loss: 0.151532, acc.: 80.47%] [G loss: 0.698011]\n",
      "epoch:2 step:2377 [D loss: 0.142191, acc.: 81.25%] [G loss: 0.699921]\n",
      "epoch:2 step:2378 [D loss: 0.133302, acc.: 79.69%] [G loss: 0.684503]\n",
      "epoch:2 step:2379 [D loss: 0.185262, acc.: 78.91%] [G loss: 0.692162]\n",
      "epoch:2 step:2380 [D loss: 0.132798, acc.: 85.16%] [G loss: 0.739939]\n",
      "epoch:2 step:2381 [D loss: 0.173751, acc.: 78.91%] [G loss: 0.606469]\n",
      "epoch:2 step:2382 [D loss: 0.130563, acc.: 85.16%] [G loss: 0.699436]\n",
      "epoch:2 step:2383 [D loss: 0.130419, acc.: 82.03%] [G loss: 0.715925]\n",
      "epoch:2 step:2384 [D loss: 0.153275, acc.: 78.12%] [G loss: 0.668000]\n",
      "epoch:2 step:2385 [D loss: 0.124699, acc.: 83.59%] [G loss: 0.809968]\n",
      "epoch:2 step:2386 [D loss: 0.168263, acc.: 77.34%] [G loss: 0.647716]\n",
      "epoch:2 step:2387 [D loss: 0.140357, acc.: 79.69%] [G loss: 0.736209]\n",
      "epoch:2 step:2388 [D loss: 0.152548, acc.: 79.69%] [G loss: 0.695675]\n",
      "epoch:2 step:2389 [D loss: 0.126940, acc.: 82.81%] [G loss: 0.756161]\n",
      "epoch:2 step:2390 [D loss: 0.178162, acc.: 75.78%] [G loss: 0.658100]\n",
      "epoch:2 step:2391 [D loss: 0.179974, acc.: 72.66%] [G loss: 0.657575]\n",
      "epoch:2 step:2392 [D loss: 0.133286, acc.: 83.59%] [G loss: 0.705792]\n",
      "epoch:2 step:2393 [D loss: 0.167463, acc.: 74.22%] [G loss: 0.678977]\n",
      "epoch:2 step:2394 [D loss: 0.133840, acc.: 85.94%] [G loss: 0.699587]\n",
      "epoch:2 step:2395 [D loss: 0.145540, acc.: 81.25%] [G loss: 0.698644]\n",
      "epoch:2 step:2396 [D loss: 0.168894, acc.: 73.44%] [G loss: 0.662472]\n",
      "epoch:2 step:2397 [D loss: 0.140287, acc.: 83.59%] [G loss: 0.717445]\n",
      "epoch:2 step:2398 [D loss: 0.142391, acc.: 78.12%] [G loss: 0.688138]\n",
      "epoch:2 step:2399 [D loss: 0.165686, acc.: 73.44%] [G loss: 0.651500]\n",
      "epoch:2 step:2400 [D loss: 0.147319, acc.: 79.69%] [G loss: 0.735819]\n",
      "##############\n",
      "[3.80354622 2.1591819  7.50982176 5.9559431  4.86315765 7.17769842\n",
      " 6.23866093 5.95333601 6.26730262 4.60576422]\n",
      "##########\n",
      "epoch:2 step:2401 [D loss: 0.202033, acc.: 72.66%] [G loss: 0.685710]\n",
      "epoch:2 step:2402 [D loss: 0.194278, acc.: 71.88%] [G loss: 0.614982]\n",
      "epoch:2 step:2403 [D loss: 0.156920, acc.: 78.12%] [G loss: 0.672150]\n",
      "epoch:2 step:2404 [D loss: 0.155943, acc.: 82.03%] [G loss: 0.679901]\n",
      "epoch:2 step:2405 [D loss: 0.220908, acc.: 65.62%] [G loss: 0.579880]\n",
      "epoch:2 step:2406 [D loss: 0.160262, acc.: 79.69%] [G loss: 0.704293]\n",
      "epoch:2 step:2407 [D loss: 0.163211, acc.: 74.22%] [G loss: 0.682753]\n",
      "epoch:2 step:2408 [D loss: 0.121909, acc.: 88.28%] [G loss: 0.740212]\n",
      "epoch:2 step:2409 [D loss: 0.184315, acc.: 74.22%] [G loss: 0.598796]\n",
      "epoch:2 step:2410 [D loss: 0.106621, acc.: 85.94%] [G loss: 0.732466]\n",
      "epoch:2 step:2411 [D loss: 0.154451, acc.: 75.78%] [G loss: 0.719779]\n",
      "epoch:2 step:2412 [D loss: 0.170578, acc.: 71.88%] [G loss: 0.659056]\n",
      "epoch:2 step:2413 [D loss: 0.142418, acc.: 85.94%] [G loss: 0.648124]\n",
      "epoch:2 step:2414 [D loss: 0.144892, acc.: 76.56%] [G loss: 0.690174]\n",
      "epoch:2 step:2415 [D loss: 0.154344, acc.: 80.47%] [G loss: 0.688126]\n",
      "epoch:2 step:2416 [D loss: 0.169720, acc.: 78.91%] [G loss: 0.642819]\n",
      "epoch:2 step:2417 [D loss: 0.190079, acc.: 71.88%] [G loss: 0.631709]\n",
      "epoch:2 step:2418 [D loss: 0.165001, acc.: 77.34%] [G loss: 0.681162]\n",
      "epoch:2 step:2419 [D loss: 0.176257, acc.: 74.22%] [G loss: 0.658987]\n",
      "epoch:2 step:2420 [D loss: 0.122910, acc.: 83.59%] [G loss: 0.709870]\n",
      "epoch:2 step:2421 [D loss: 0.127785, acc.: 83.59%] [G loss: 0.747608]\n",
      "epoch:2 step:2422 [D loss: 0.114402, acc.: 82.81%] [G loss: 0.707958]\n",
      "epoch:2 step:2423 [D loss: 0.160203, acc.: 75.00%] [G loss: 0.668509]\n",
      "epoch:2 step:2424 [D loss: 0.159138, acc.: 79.69%] [G loss: 0.659554]\n",
      "epoch:2 step:2425 [D loss: 0.149815, acc.: 82.03%] [G loss: 0.734799]\n",
      "epoch:2 step:2426 [D loss: 0.157054, acc.: 78.91%] [G loss: 0.725421]\n",
      "epoch:2 step:2427 [D loss: 0.178381, acc.: 72.66%] [G loss: 0.624388]\n",
      "epoch:2 step:2428 [D loss: 0.142845, acc.: 84.38%] [G loss: 0.661475]\n",
      "epoch:2 step:2429 [D loss: 0.136082, acc.: 83.59%] [G loss: 0.708318]\n",
      "epoch:2 step:2430 [D loss: 0.142821, acc.: 79.69%] [G loss: 0.661423]\n",
      "epoch:2 step:2431 [D loss: 0.146251, acc.: 84.38%] [G loss: 0.667615]\n",
      "epoch:2 step:2432 [D loss: 0.130093, acc.: 85.16%] [G loss: 0.714667]\n",
      "epoch:2 step:2433 [D loss: 0.177368, acc.: 75.00%] [G loss: 0.642854]\n",
      "epoch:2 step:2434 [D loss: 0.167362, acc.: 77.34%] [G loss: 0.674265]\n",
      "epoch:2 step:2435 [D loss: 0.147271, acc.: 78.91%] [G loss: 0.763702]\n",
      "epoch:2 step:2436 [D loss: 0.208705, acc.: 71.09%] [G loss: 0.652506]\n",
      "epoch:2 step:2437 [D loss: 0.145020, acc.: 78.12%] [G loss: 0.714757]\n",
      "epoch:2 step:2438 [D loss: 0.171054, acc.: 74.22%] [G loss: 0.686863]\n",
      "epoch:2 step:2439 [D loss: 0.156728, acc.: 80.47%] [G loss: 0.653065]\n",
      "epoch:2 step:2440 [D loss: 0.183110, acc.: 77.34%] [G loss: 0.641303]\n",
      "epoch:2 step:2441 [D loss: 0.120776, acc.: 88.28%] [G loss: 0.741190]\n",
      "epoch:2 step:2442 [D loss: 0.136558, acc.: 82.81%] [G loss: 0.702818]\n",
      "epoch:2 step:2443 [D loss: 0.217121, acc.: 66.41%] [G loss: 0.569194]\n",
      "epoch:2 step:2444 [D loss: 0.152076, acc.: 79.69%] [G loss: 0.696798]\n",
      "epoch:2 step:2445 [D loss: 0.160241, acc.: 77.34%] [G loss: 0.653867]\n",
      "epoch:2 step:2446 [D loss: 0.204029, acc.: 69.53%] [G loss: 0.658642]\n",
      "epoch:2 step:2447 [D loss: 0.155349, acc.: 78.91%] [G loss: 0.711028]\n",
      "epoch:2 step:2448 [D loss: 0.141183, acc.: 82.81%] [G loss: 0.653408]\n",
      "epoch:2 step:2449 [D loss: 0.126548, acc.: 83.59%] [G loss: 0.702332]\n",
      "epoch:2 step:2450 [D loss: 0.164268, acc.: 76.56%] [G loss: 0.696829]\n",
      "epoch:2 step:2451 [D loss: 0.128693, acc.: 85.16%] [G loss: 0.716300]\n",
      "epoch:2 step:2452 [D loss: 0.153482, acc.: 75.78%] [G loss: 0.711928]\n",
      "epoch:2 step:2453 [D loss: 0.163517, acc.: 77.34%] [G loss: 0.687781]\n",
      "epoch:2 step:2454 [D loss: 0.200977, acc.: 70.31%] [G loss: 0.614012]\n",
      "epoch:2 step:2455 [D loss: 0.141854, acc.: 79.69%] [G loss: 0.685330]\n",
      "epoch:2 step:2456 [D loss: 0.165640, acc.: 78.91%] [G loss: 0.662555]\n",
      "epoch:2 step:2457 [D loss: 0.142442, acc.: 81.25%] [G loss: 0.713578]\n",
      "epoch:2 step:2458 [D loss: 0.197530, acc.: 71.09%] [G loss: 0.628864]\n",
      "epoch:2 step:2459 [D loss: 0.155390, acc.: 81.25%] [G loss: 0.706695]\n",
      "epoch:2 step:2460 [D loss: 0.150522, acc.: 83.59%] [G loss: 0.694756]\n",
      "epoch:2 step:2461 [D loss: 0.150012, acc.: 80.47%] [G loss: 0.692400]\n",
      "epoch:2 step:2462 [D loss: 0.172001, acc.: 72.66%] [G loss: 0.689848]\n",
      "epoch:2 step:2463 [D loss: 0.136506, acc.: 82.03%] [G loss: 0.776578]\n",
      "epoch:2 step:2464 [D loss: 0.218415, acc.: 67.19%] [G loss: 0.634864]\n",
      "epoch:2 step:2465 [D loss: 0.126296, acc.: 85.16%] [G loss: 0.739484]\n",
      "epoch:2 step:2466 [D loss: 0.137953, acc.: 83.59%] [G loss: 0.723653]\n",
      "epoch:2 step:2467 [D loss: 0.194231, acc.: 70.31%] [G loss: 0.602787]\n",
      "epoch:2 step:2468 [D loss: 0.165810, acc.: 78.91%] [G loss: 0.678756]\n",
      "epoch:2 step:2469 [D loss: 0.144687, acc.: 81.25%] [G loss: 0.664932]\n",
      "epoch:2 step:2470 [D loss: 0.147742, acc.: 81.25%] [G loss: 0.707521]\n",
      "epoch:2 step:2471 [D loss: 0.165991, acc.: 74.22%] [G loss: 0.632394]\n",
      "epoch:2 step:2472 [D loss: 0.133770, acc.: 78.91%] [G loss: 0.715416]\n",
      "epoch:2 step:2473 [D loss: 0.165882, acc.: 77.34%] [G loss: 0.692642]\n",
      "epoch:2 step:2474 [D loss: 0.188262, acc.: 71.09%] [G loss: 0.644910]\n",
      "epoch:2 step:2475 [D loss: 0.166968, acc.: 74.22%] [G loss: 0.702771]\n",
      "epoch:2 step:2476 [D loss: 0.162677, acc.: 77.34%] [G loss: 0.704623]\n",
      "epoch:2 step:2477 [D loss: 0.158677, acc.: 78.12%] [G loss: 0.686031]\n",
      "epoch:2 step:2478 [D loss: 0.140699, acc.: 81.25%] [G loss: 0.774301]\n",
      "epoch:2 step:2479 [D loss: 0.120360, acc.: 90.62%] [G loss: 0.746806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2480 [D loss: 0.142696, acc.: 80.47%] [G loss: 0.672366]\n",
      "epoch:2 step:2481 [D loss: 0.172772, acc.: 77.34%] [G loss: 0.654160]\n",
      "epoch:2 step:2482 [D loss: 0.133420, acc.: 81.25%] [G loss: 0.758667]\n",
      "epoch:2 step:2483 [D loss: 0.143240, acc.: 83.59%] [G loss: 0.751269]\n",
      "epoch:2 step:2484 [D loss: 0.185458, acc.: 75.00%] [G loss: 0.616428]\n",
      "epoch:2 step:2485 [D loss: 0.155696, acc.: 76.56%] [G loss: 0.687863]\n",
      "epoch:2 step:2486 [D loss: 0.150198, acc.: 78.91%] [G loss: 0.773440]\n",
      "epoch:2 step:2487 [D loss: 0.144473, acc.: 80.47%] [G loss: 0.702396]\n",
      "epoch:2 step:2488 [D loss: 0.156129, acc.: 76.56%] [G loss: 0.676816]\n",
      "epoch:2 step:2489 [D loss: 0.176766, acc.: 72.66%] [G loss: 0.648946]\n",
      "epoch:2 step:2490 [D loss: 0.168901, acc.: 77.34%] [G loss: 0.764663]\n",
      "epoch:2 step:2491 [D loss: 0.188610, acc.: 74.22%] [G loss: 0.591882]\n",
      "epoch:2 step:2492 [D loss: 0.173750, acc.: 73.44%] [G loss: 0.620893]\n",
      "epoch:2 step:2493 [D loss: 0.142162, acc.: 78.91%] [G loss: 0.718390]\n",
      "epoch:2 step:2494 [D loss: 0.159473, acc.: 80.47%] [G loss: 0.716423]\n",
      "epoch:2 step:2495 [D loss: 0.153571, acc.: 78.12%] [G loss: 0.686459]\n",
      "epoch:2 step:2496 [D loss: 0.209923, acc.: 65.62%] [G loss: 0.593447]\n",
      "epoch:2 step:2497 [D loss: 0.134504, acc.: 85.16%] [G loss: 0.725712]\n",
      "epoch:2 step:2498 [D loss: 0.157639, acc.: 79.69%] [G loss: 0.778106]\n",
      "epoch:2 step:2499 [D loss: 0.187544, acc.: 71.88%] [G loss: 0.650374]\n",
      "epoch:2 step:2500 [D loss: 0.123946, acc.: 85.16%] [G loss: 0.709045]\n",
      "epoch:2 step:2501 [D loss: 0.140962, acc.: 80.47%] [G loss: 0.718534]\n",
      "epoch:2 step:2502 [D loss: 0.165583, acc.: 78.91%] [G loss: 0.639240]\n",
      "epoch:2 step:2503 [D loss: 0.156751, acc.: 75.00%] [G loss: 0.676530]\n",
      "epoch:2 step:2504 [D loss: 0.165747, acc.: 77.34%] [G loss: 0.675263]\n",
      "epoch:2 step:2505 [D loss: 0.130168, acc.: 86.72%] [G loss: 0.655609]\n",
      "epoch:2 step:2506 [D loss: 0.138622, acc.: 82.03%] [G loss: 0.699011]\n",
      "epoch:2 step:2507 [D loss: 0.153377, acc.: 78.12%] [G loss: 0.699782]\n",
      "epoch:2 step:2508 [D loss: 0.139133, acc.: 82.81%] [G loss: 0.749633]\n",
      "epoch:2 step:2509 [D loss: 0.140818, acc.: 80.47%] [G loss: 0.668499]\n",
      "epoch:2 step:2510 [D loss: 0.210337, acc.: 70.31%] [G loss: 0.598846]\n",
      "epoch:2 step:2511 [D loss: 0.139732, acc.: 82.03%] [G loss: 0.674322]\n",
      "epoch:2 step:2512 [D loss: 0.137171, acc.: 84.38%] [G loss: 0.707181]\n",
      "epoch:2 step:2513 [D loss: 0.124950, acc.: 87.50%] [G loss: 0.783038]\n",
      "epoch:2 step:2514 [D loss: 0.128542, acc.: 82.81%] [G loss: 0.763514]\n",
      "epoch:2 step:2515 [D loss: 0.148188, acc.: 80.47%] [G loss: 0.666874]\n",
      "epoch:2 step:2516 [D loss: 0.146415, acc.: 79.69%] [G loss: 0.778678]\n",
      "epoch:2 step:2517 [D loss: 0.202363, acc.: 71.09%] [G loss: 0.595231]\n",
      "epoch:2 step:2518 [D loss: 0.165317, acc.: 75.00%] [G loss: 0.666317]\n",
      "epoch:2 step:2519 [D loss: 0.190983, acc.: 71.88%] [G loss: 0.695477]\n",
      "epoch:2 step:2520 [D loss: 0.185375, acc.: 72.66%] [G loss: 0.662168]\n",
      "epoch:2 step:2521 [D loss: 0.158815, acc.: 79.69%] [G loss: 0.729636]\n",
      "epoch:2 step:2522 [D loss: 0.135389, acc.: 84.38%] [G loss: 0.740411]\n",
      "epoch:2 step:2523 [D loss: 0.144848, acc.: 82.03%] [G loss: 0.700020]\n",
      "epoch:2 step:2524 [D loss: 0.138497, acc.: 82.81%] [G loss: 0.736754]\n",
      "epoch:2 step:2525 [D loss: 0.147427, acc.: 78.12%] [G loss: 0.682671]\n",
      "epoch:2 step:2526 [D loss: 0.207327, acc.: 67.19%] [G loss: 0.592904]\n",
      "epoch:2 step:2527 [D loss: 0.161672, acc.: 76.56%] [G loss: 0.686684]\n",
      "epoch:2 step:2528 [D loss: 0.183047, acc.: 75.78%] [G loss: 0.657253]\n",
      "epoch:2 step:2529 [D loss: 0.194720, acc.: 76.56%] [G loss: 0.669399]\n",
      "epoch:2 step:2530 [D loss: 0.161574, acc.: 77.34%] [G loss: 0.681612]\n",
      "epoch:2 step:2531 [D loss: 0.148614, acc.: 85.94%] [G loss: 0.697234]\n",
      "epoch:2 step:2532 [D loss: 0.168009, acc.: 75.00%] [G loss: 0.694023]\n",
      "epoch:2 step:2533 [D loss: 0.174325, acc.: 74.22%] [G loss: 0.662921]\n",
      "epoch:2 step:2534 [D loss: 0.114795, acc.: 87.50%] [G loss: 0.733401]\n",
      "epoch:2 step:2535 [D loss: 0.138059, acc.: 84.38%] [G loss: 0.698382]\n",
      "epoch:2 step:2536 [D loss: 0.166706, acc.: 76.56%] [G loss: 0.674538]\n",
      "epoch:2 step:2537 [D loss: 0.153942, acc.: 79.69%] [G loss: 0.757059]\n",
      "epoch:2 step:2538 [D loss: 0.142098, acc.: 79.69%] [G loss: 0.764033]\n",
      "epoch:2 step:2539 [D loss: 0.147264, acc.: 79.69%] [G loss: 0.689951]\n",
      "epoch:2 step:2540 [D loss: 0.139308, acc.: 82.03%] [G loss: 0.685529]\n",
      "epoch:2 step:2541 [D loss: 0.157395, acc.: 76.56%] [G loss: 0.680804]\n",
      "epoch:2 step:2542 [D loss: 0.160917, acc.: 82.03%] [G loss: 0.649611]\n",
      "epoch:2 step:2543 [D loss: 0.119477, acc.: 85.16%] [G loss: 0.782332]\n",
      "epoch:2 step:2544 [D loss: 0.110841, acc.: 87.50%] [G loss: 0.715837]\n",
      "epoch:2 step:2545 [D loss: 0.160430, acc.: 78.12%] [G loss: 0.671585]\n",
      "epoch:2 step:2546 [D loss: 0.201676, acc.: 68.75%] [G loss: 0.616011]\n",
      "epoch:2 step:2547 [D loss: 0.161380, acc.: 77.34%] [G loss: 0.709529]\n",
      "epoch:2 step:2548 [D loss: 0.162160, acc.: 79.69%] [G loss: 0.745853]\n",
      "epoch:2 step:2549 [D loss: 0.174118, acc.: 77.34%] [G loss: 0.691237]\n",
      "epoch:2 step:2550 [D loss: 0.165992, acc.: 75.00%] [G loss: 0.722876]\n",
      "epoch:2 step:2551 [D loss: 0.127922, acc.: 82.81%] [G loss: 0.701834]\n",
      "epoch:2 step:2552 [D loss: 0.170246, acc.: 78.91%] [G loss: 0.637396]\n",
      "epoch:2 step:2553 [D loss: 0.110626, acc.: 85.94%] [G loss: 0.679519]\n",
      "epoch:2 step:2554 [D loss: 0.120709, acc.: 87.50%] [G loss: 0.694125]\n",
      "epoch:2 step:2555 [D loss: 0.140957, acc.: 85.94%] [G loss: 0.723552]\n",
      "epoch:2 step:2556 [D loss: 0.126801, acc.: 83.59%] [G loss: 0.721851]\n",
      "epoch:2 step:2557 [D loss: 0.142991, acc.: 83.59%] [G loss: 0.695820]\n",
      "epoch:2 step:2558 [D loss: 0.158303, acc.: 75.78%] [G loss: 0.665337]\n",
      "epoch:2 step:2559 [D loss: 0.151837, acc.: 80.47%] [G loss: 0.670998]\n",
      "epoch:2 step:2560 [D loss: 0.161478, acc.: 75.00%] [G loss: 0.632092]\n",
      "epoch:2 step:2561 [D loss: 0.132059, acc.: 84.38%] [G loss: 0.685773]\n",
      "epoch:2 step:2562 [D loss: 0.183056, acc.: 74.22%] [G loss: 0.675795]\n",
      "epoch:2 step:2563 [D loss: 0.191881, acc.: 72.66%] [G loss: 0.642536]\n",
      "epoch:2 step:2564 [D loss: 0.121186, acc.: 86.72%] [G loss: 0.728153]\n",
      "epoch:2 step:2565 [D loss: 0.127510, acc.: 87.50%] [G loss: 0.741961]\n",
      "epoch:2 step:2566 [D loss: 0.151285, acc.: 82.03%] [G loss: 0.674418]\n",
      "epoch:2 step:2567 [D loss: 0.133379, acc.: 81.25%] [G loss: 0.715957]\n",
      "epoch:2 step:2568 [D loss: 0.125202, acc.: 86.72%] [G loss: 0.699644]\n",
      "epoch:2 step:2569 [D loss: 0.139331, acc.: 83.59%] [G loss: 0.654636]\n",
      "epoch:2 step:2570 [D loss: 0.153471, acc.: 78.91%] [G loss: 0.697136]\n",
      "epoch:2 step:2571 [D loss: 0.130588, acc.: 83.59%] [G loss: 0.755759]\n",
      "epoch:2 step:2572 [D loss: 0.140255, acc.: 83.59%] [G loss: 0.663221]\n",
      "epoch:2 step:2573 [D loss: 0.127619, acc.: 82.81%] [G loss: 0.698163]\n",
      "epoch:2 step:2574 [D loss: 0.148637, acc.: 78.91%] [G loss: 0.715426]\n",
      "epoch:2 step:2575 [D loss: 0.127529, acc.: 85.16%] [G loss: 0.816880]\n",
      "epoch:2 step:2576 [D loss: 0.195014, acc.: 75.78%] [G loss: 0.653858]\n",
      "epoch:2 step:2577 [D loss: 0.124609, acc.: 80.47%] [G loss: 0.685788]\n",
      "epoch:2 step:2578 [D loss: 0.211142, acc.: 71.09%] [G loss: 0.641285]\n",
      "epoch:2 step:2579 [D loss: 0.154255, acc.: 80.47%] [G loss: 0.671701]\n",
      "epoch:2 step:2580 [D loss: 0.127463, acc.: 85.16%] [G loss: 0.717694]\n",
      "epoch:2 step:2581 [D loss: 0.116365, acc.: 85.94%] [G loss: 0.718798]\n",
      "epoch:2 step:2582 [D loss: 0.119540, acc.: 87.50%] [G loss: 0.703156]\n",
      "epoch:2 step:2583 [D loss: 0.115902, acc.: 85.16%] [G loss: 0.758876]\n",
      "epoch:2 step:2584 [D loss: 0.215316, acc.: 67.97%] [G loss: 0.610363]\n",
      "epoch:2 step:2585 [D loss: 0.186458, acc.: 72.66%] [G loss: 0.625194]\n",
      "epoch:2 step:2586 [D loss: 0.127214, acc.: 85.94%] [G loss: 0.709706]\n",
      "epoch:2 step:2587 [D loss: 0.143731, acc.: 79.69%] [G loss: 0.636247]\n",
      "epoch:2 step:2588 [D loss: 0.128822, acc.: 85.94%] [G loss: 0.768171]\n",
      "epoch:2 step:2589 [D loss: 0.130783, acc.: 82.81%] [G loss: 0.722428]\n",
      "epoch:2 step:2590 [D loss: 0.200646, acc.: 67.97%] [G loss: 0.611047]\n",
      "epoch:2 step:2591 [D loss: 0.134867, acc.: 84.38%] [G loss: 0.693815]\n",
      "epoch:2 step:2592 [D loss: 0.149394, acc.: 80.47%] [G loss: 0.692087]\n",
      "epoch:2 step:2593 [D loss: 0.196917, acc.: 67.19%] [G loss: 0.677863]\n",
      "epoch:2 step:2594 [D loss: 0.186346, acc.: 69.53%] [G loss: 0.677002]\n",
      "epoch:2 step:2595 [D loss: 0.167930, acc.: 78.12%] [G loss: 0.679964]\n",
      "epoch:2 step:2596 [D loss: 0.159449, acc.: 76.56%] [G loss: 0.664281]\n",
      "epoch:2 step:2597 [D loss: 0.167230, acc.: 75.00%] [G loss: 0.685772]\n",
      "epoch:2 step:2598 [D loss: 0.145434, acc.: 78.91%] [G loss: 0.675422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2599 [D loss: 0.127096, acc.: 82.81%] [G loss: 0.757130]\n",
      "epoch:2 step:2600 [D loss: 0.176188, acc.: 76.56%] [G loss: 0.703417]\n",
      "##############\n",
      "[4.15313551 2.45308986 7.42495712 6.19158349 4.84046002 7.09446537\n",
      " 6.43186102 5.68399644 5.78545136 4.51977043]\n",
      "##########\n",
      "epoch:2 step:2601 [D loss: 0.145994, acc.: 78.91%] [G loss: 0.657620]\n",
      "epoch:2 step:2602 [D loss: 0.141902, acc.: 79.69%] [G loss: 0.740067]\n",
      "epoch:2 step:2603 [D loss: 0.156043, acc.: 78.91%] [G loss: 0.707352]\n",
      "epoch:2 step:2604 [D loss: 0.134011, acc.: 82.81%] [G loss: 0.771957]\n",
      "epoch:2 step:2605 [D loss: 0.156310, acc.: 78.91%] [G loss: 0.713309]\n",
      "epoch:2 step:2606 [D loss: 0.110703, acc.: 87.50%] [G loss: 0.695942]\n",
      "epoch:2 step:2607 [D loss: 0.166981, acc.: 79.69%] [G loss: 0.659681]\n",
      "epoch:2 step:2608 [D loss: 0.145190, acc.: 81.25%] [G loss: 0.693661]\n",
      "epoch:2 step:2609 [D loss: 0.167152, acc.: 79.69%] [G loss: 0.611353]\n",
      "epoch:2 step:2610 [D loss: 0.129444, acc.: 83.59%] [G loss: 0.713441]\n",
      "epoch:2 step:2611 [D loss: 0.171755, acc.: 75.78%] [G loss: 0.732835]\n",
      "epoch:2 step:2612 [D loss: 0.225357, acc.: 63.28%] [G loss: 0.556322]\n",
      "epoch:2 step:2613 [D loss: 0.162039, acc.: 79.69%] [G loss: 0.633362]\n",
      "epoch:2 step:2614 [D loss: 0.198535, acc.: 70.31%] [G loss: 0.621498]\n",
      "epoch:2 step:2615 [D loss: 0.171431, acc.: 73.44%] [G loss: 0.725605]\n",
      "epoch:2 step:2616 [D loss: 0.166672, acc.: 76.56%] [G loss: 0.699789]\n",
      "epoch:2 step:2617 [D loss: 0.185238, acc.: 72.66%] [G loss: 0.716349]\n",
      "epoch:2 step:2618 [D loss: 0.174882, acc.: 75.00%] [G loss: 0.726344]\n",
      "epoch:2 step:2619 [D loss: 0.137738, acc.: 82.81%] [G loss: 0.763662]\n",
      "epoch:2 step:2620 [D loss: 0.128899, acc.: 84.38%] [G loss: 0.787950]\n",
      "epoch:2 step:2621 [D loss: 0.125144, acc.: 84.38%] [G loss: 0.802229]\n",
      "epoch:2 step:2622 [D loss: 0.165516, acc.: 71.88%] [G loss: 0.678428]\n",
      "epoch:2 step:2623 [D loss: 0.162450, acc.: 78.91%] [G loss: 0.643320]\n",
      "epoch:2 step:2624 [D loss: 0.149619, acc.: 78.12%] [G loss: 0.682933]\n",
      "epoch:2 step:2625 [D loss: 0.172248, acc.: 78.12%] [G loss: 0.661959]\n",
      "epoch:2 step:2626 [D loss: 0.154494, acc.: 78.91%] [G loss: 0.689653]\n",
      "epoch:2 step:2627 [D loss: 0.155301, acc.: 75.78%] [G loss: 0.696658]\n",
      "epoch:2 step:2628 [D loss: 0.180886, acc.: 75.00%] [G loss: 0.651436]\n",
      "epoch:2 step:2629 [D loss: 0.146252, acc.: 75.78%] [G loss: 0.704486]\n",
      "epoch:2 step:2630 [D loss: 0.131599, acc.: 81.25%] [G loss: 0.731226]\n",
      "epoch:2 step:2631 [D loss: 0.151545, acc.: 81.25%] [G loss: 0.743280]\n",
      "epoch:2 step:2632 [D loss: 0.137926, acc.: 82.03%] [G loss: 0.734875]\n",
      "epoch:2 step:2633 [D loss: 0.159480, acc.: 78.91%] [G loss: 0.662367]\n",
      "epoch:2 step:2634 [D loss: 0.177339, acc.: 74.22%] [G loss: 0.681116]\n",
      "epoch:2 step:2635 [D loss: 0.145672, acc.: 79.69%] [G loss: 0.676272]\n",
      "epoch:2 step:2636 [D loss: 0.160229, acc.: 79.69%] [G loss: 0.663736]\n",
      "epoch:2 step:2637 [D loss: 0.165838, acc.: 75.78%] [G loss: 0.752025]\n",
      "epoch:2 step:2638 [D loss: 0.174885, acc.: 73.44%] [G loss: 0.682568]\n",
      "epoch:2 step:2639 [D loss: 0.249733, acc.: 60.16%] [G loss: 0.592191]\n",
      "epoch:2 step:2640 [D loss: 0.229590, acc.: 66.41%] [G loss: 0.633646]\n",
      "epoch:2 step:2641 [D loss: 0.144764, acc.: 84.38%] [G loss: 0.740312]\n",
      "epoch:2 step:2642 [D loss: 0.186969, acc.: 68.75%] [G loss: 0.698941]\n",
      "epoch:2 step:2643 [D loss: 0.127916, acc.: 84.38%] [G loss: 0.757204]\n",
      "epoch:2 step:2644 [D loss: 0.166265, acc.: 79.69%] [G loss: 0.685207]\n",
      "epoch:2 step:2645 [D loss: 0.159533, acc.: 82.03%] [G loss: 0.652673]\n",
      "epoch:2 step:2646 [D loss: 0.143291, acc.: 85.16%] [G loss: 0.691331]\n",
      "epoch:2 step:2647 [D loss: 0.126115, acc.: 84.38%] [G loss: 0.734789]\n",
      "epoch:2 step:2648 [D loss: 0.212934, acc.: 67.19%] [G loss: 0.554653]\n",
      "epoch:2 step:2649 [D loss: 0.130866, acc.: 85.16%] [G loss: 0.718894]\n",
      "epoch:2 step:2650 [D loss: 0.167296, acc.: 75.00%] [G loss: 0.679551]\n",
      "epoch:2 step:2651 [D loss: 0.160014, acc.: 78.91%] [G loss: 0.659052]\n",
      "epoch:2 step:2652 [D loss: 0.137332, acc.: 85.16%] [G loss: 0.634097]\n",
      "epoch:2 step:2653 [D loss: 0.164880, acc.: 75.00%] [G loss: 0.636183]\n",
      "epoch:2 step:2654 [D loss: 0.164646, acc.: 75.00%] [G loss: 0.673593]\n",
      "epoch:2 step:2655 [D loss: 0.134397, acc.: 83.59%] [G loss: 0.722443]\n",
      "epoch:2 step:2656 [D loss: 0.153523, acc.: 80.47%] [G loss: 0.748685]\n",
      "epoch:2 step:2657 [D loss: 0.142841, acc.: 82.81%] [G loss: 0.690273]\n",
      "epoch:2 step:2658 [D loss: 0.140275, acc.: 80.47%] [G loss: 0.734350]\n",
      "epoch:2 step:2659 [D loss: 0.162121, acc.: 78.91%] [G loss: 0.669324]\n",
      "epoch:2 step:2660 [D loss: 0.142204, acc.: 78.12%] [G loss: 0.741904]\n",
      "epoch:2 step:2661 [D loss: 0.208500, acc.: 69.53%] [G loss: 0.667243]\n",
      "epoch:2 step:2662 [D loss: 0.230562, acc.: 64.06%] [G loss: 0.653383]\n",
      "epoch:2 step:2663 [D loss: 0.156204, acc.: 78.91%] [G loss: 0.666255]\n",
      "epoch:2 step:2664 [D loss: 0.143508, acc.: 81.25%] [G loss: 0.711162]\n",
      "epoch:2 step:2665 [D loss: 0.169332, acc.: 75.78%] [G loss: 0.668687]\n",
      "epoch:2 step:2666 [D loss: 0.125255, acc.: 82.81%] [G loss: 0.775962]\n",
      "epoch:2 step:2667 [D loss: 0.237959, acc.: 61.72%] [G loss: 0.598514]\n",
      "epoch:2 step:2668 [D loss: 0.169654, acc.: 75.00%] [G loss: 0.638348]\n",
      "epoch:2 step:2669 [D loss: 0.178133, acc.: 75.78%] [G loss: 0.727105]\n",
      "epoch:2 step:2670 [D loss: 0.133004, acc.: 84.38%] [G loss: 0.730214]\n",
      "epoch:2 step:2671 [D loss: 0.210931, acc.: 67.97%] [G loss: 0.624039]\n",
      "epoch:2 step:2672 [D loss: 0.154745, acc.: 75.78%] [G loss: 0.687822]\n",
      "epoch:2 step:2673 [D loss: 0.196444, acc.: 71.88%] [G loss: 0.593765]\n",
      "epoch:2 step:2674 [D loss: 0.185875, acc.: 73.44%] [G loss: 0.648475]\n",
      "epoch:2 step:2675 [D loss: 0.171822, acc.: 78.91%] [G loss: 0.708631]\n",
      "epoch:2 step:2676 [D loss: 0.159854, acc.: 76.56%] [G loss: 0.686388]\n",
      "epoch:2 step:2677 [D loss: 0.152899, acc.: 79.69%] [G loss: 0.644869]\n",
      "epoch:2 step:2678 [D loss: 0.153124, acc.: 78.91%] [G loss: 0.647704]\n",
      "epoch:2 step:2679 [D loss: 0.129205, acc.: 82.81%] [G loss: 0.734221]\n",
      "epoch:2 step:2680 [D loss: 0.148952, acc.: 80.47%] [G loss: 0.735346]\n",
      "epoch:2 step:2681 [D loss: 0.145192, acc.: 81.25%] [G loss: 0.720494]\n",
      "epoch:2 step:2682 [D loss: 0.182467, acc.: 74.22%] [G loss: 0.621079]\n",
      "epoch:2 step:2683 [D loss: 0.204027, acc.: 64.84%] [G loss: 0.665806]\n",
      "epoch:2 step:2684 [D loss: 0.155626, acc.: 76.56%] [G loss: 0.682938]\n",
      "epoch:2 step:2685 [D loss: 0.193624, acc.: 65.62%] [G loss: 0.609139]\n",
      "epoch:2 step:2686 [D loss: 0.196271, acc.: 68.75%] [G loss: 0.622070]\n",
      "epoch:2 step:2687 [D loss: 0.141382, acc.: 79.69%] [G loss: 0.714580]\n",
      "epoch:2 step:2688 [D loss: 0.154544, acc.: 78.91%] [G loss: 0.679662]\n",
      "epoch:2 step:2689 [D loss: 0.201924, acc.: 67.19%] [G loss: 0.629998]\n",
      "epoch:2 step:2690 [D loss: 0.148635, acc.: 79.69%] [G loss: 0.700161]\n",
      "epoch:2 step:2691 [D loss: 0.190686, acc.: 70.31%] [G loss: 0.602647]\n",
      "epoch:2 step:2692 [D loss: 0.167576, acc.: 77.34%] [G loss: 0.637309]\n",
      "epoch:2 step:2693 [D loss: 0.140778, acc.: 80.47%] [G loss: 0.667002]\n",
      "epoch:2 step:2694 [D loss: 0.173593, acc.: 75.00%] [G loss: 0.676985]\n",
      "epoch:2 step:2695 [D loss: 0.134695, acc.: 84.38%] [G loss: 0.696946]\n",
      "epoch:2 step:2696 [D loss: 0.139331, acc.: 82.03%] [G loss: 0.699865]\n",
      "epoch:2 step:2697 [D loss: 0.136357, acc.: 82.81%] [G loss: 0.725978]\n",
      "epoch:2 step:2698 [D loss: 0.214809, acc.: 68.75%] [G loss: 0.529315]\n",
      "epoch:2 step:2699 [D loss: 0.150915, acc.: 77.34%] [G loss: 0.683807]\n",
      "epoch:2 step:2700 [D loss: 0.186558, acc.: 72.66%] [G loss: 0.626892]\n",
      "epoch:2 step:2701 [D loss: 0.200115, acc.: 70.31%] [G loss: 0.632886]\n",
      "epoch:2 step:2702 [D loss: 0.210221, acc.: 70.31%] [G loss: 0.622364]\n",
      "epoch:2 step:2703 [D loss: 0.148264, acc.: 79.69%] [G loss: 0.691465]\n",
      "epoch:2 step:2704 [D loss: 0.150211, acc.: 76.56%] [G loss: 0.717156]\n",
      "epoch:2 step:2705 [D loss: 0.168115, acc.: 76.56%] [G loss: 0.664048]\n",
      "epoch:2 step:2706 [D loss: 0.150470, acc.: 79.69%] [G loss: 0.737489]\n",
      "epoch:2 step:2707 [D loss: 0.140427, acc.: 82.03%] [G loss: 0.694779]\n",
      "epoch:2 step:2708 [D loss: 0.131782, acc.: 81.25%] [G loss: 0.752788]\n",
      "epoch:2 step:2709 [D loss: 0.165449, acc.: 79.69%] [G loss: 0.637405]\n",
      "epoch:2 step:2710 [D loss: 0.172649, acc.: 74.22%] [G loss: 0.655117]\n",
      "epoch:2 step:2711 [D loss: 0.174554, acc.: 75.00%] [G loss: 0.676072]\n",
      "epoch:2 step:2712 [D loss: 0.157791, acc.: 79.69%] [G loss: 0.721964]\n",
      "epoch:2 step:2713 [D loss: 0.147412, acc.: 82.03%] [G loss: 0.641732]\n",
      "epoch:2 step:2714 [D loss: 0.203577, acc.: 69.53%] [G loss: 0.634992]\n",
      "epoch:2 step:2715 [D loss: 0.165828, acc.: 75.78%] [G loss: 0.656367]\n",
      "epoch:2 step:2716 [D loss: 0.152612, acc.: 79.69%] [G loss: 0.638298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2717 [D loss: 0.201792, acc.: 69.53%] [G loss: 0.615629]\n",
      "epoch:2 step:2718 [D loss: 0.150221, acc.: 82.03%] [G loss: 0.641046]\n",
      "epoch:2 step:2719 [D loss: 0.178357, acc.: 71.88%] [G loss: 0.644237]\n",
      "epoch:2 step:2720 [D loss: 0.162644, acc.: 75.00%] [G loss: 0.708372]\n",
      "epoch:2 step:2721 [D loss: 0.142736, acc.: 79.69%] [G loss: 0.723300]\n",
      "epoch:2 step:2722 [D loss: 0.138945, acc.: 82.81%] [G loss: 0.663761]\n",
      "epoch:2 step:2723 [D loss: 0.168842, acc.: 72.66%] [G loss: 0.620384]\n",
      "epoch:2 step:2724 [D loss: 0.124845, acc.: 87.50%] [G loss: 0.699472]\n",
      "epoch:2 step:2725 [D loss: 0.187373, acc.: 71.09%] [G loss: 0.653018]\n",
      "epoch:2 step:2726 [D loss: 0.156037, acc.: 74.22%] [G loss: 0.691617]\n",
      "epoch:2 step:2727 [D loss: 0.145736, acc.: 79.69%] [G loss: 0.707295]\n",
      "epoch:2 step:2728 [D loss: 0.107412, acc.: 86.72%] [G loss: 0.726782]\n",
      "epoch:2 step:2729 [D loss: 0.154471, acc.: 74.22%] [G loss: 0.608900]\n",
      "epoch:2 step:2730 [D loss: 0.167805, acc.: 76.56%] [G loss: 0.605900]\n",
      "epoch:2 step:2731 [D loss: 0.137793, acc.: 81.25%] [G loss: 0.702998]\n",
      "epoch:2 step:2732 [D loss: 0.259964, acc.: 59.38%] [G loss: 0.625539]\n",
      "epoch:2 step:2733 [D loss: 0.180738, acc.: 72.66%] [G loss: 0.637501]\n",
      "epoch:2 step:2734 [D loss: 0.151282, acc.: 82.03%] [G loss: 0.689267]\n",
      "epoch:2 step:2735 [D loss: 0.239265, acc.: 61.72%] [G loss: 0.556697]\n",
      "epoch:2 step:2736 [D loss: 0.179237, acc.: 72.66%] [G loss: 0.582295]\n",
      "epoch:2 step:2737 [D loss: 0.170188, acc.: 78.12%] [G loss: 0.690632]\n",
      "epoch:2 step:2738 [D loss: 0.155732, acc.: 79.69%] [G loss: 0.641167]\n",
      "epoch:2 step:2739 [D loss: 0.157135, acc.: 75.78%] [G loss: 0.666027]\n",
      "epoch:2 step:2740 [D loss: 0.173276, acc.: 74.22%] [G loss: 0.697938]\n",
      "epoch:2 step:2741 [D loss: 0.185317, acc.: 75.00%] [G loss: 0.607478]\n",
      "epoch:2 step:2742 [D loss: 0.150456, acc.: 77.34%] [G loss: 0.618980]\n",
      "epoch:2 step:2743 [D loss: 0.183653, acc.: 72.66%] [G loss: 0.616980]\n",
      "epoch:2 step:2744 [D loss: 0.156673, acc.: 81.25%] [G loss: 0.716649]\n",
      "epoch:2 step:2745 [D loss: 0.152601, acc.: 82.03%] [G loss: 0.692312]\n",
      "epoch:2 step:2746 [D loss: 0.143081, acc.: 79.69%] [G loss: 0.713403]\n",
      "epoch:2 step:2747 [D loss: 0.197574, acc.: 69.53%] [G loss: 0.603280]\n",
      "epoch:2 step:2748 [D loss: 0.178449, acc.: 71.88%] [G loss: 0.606915]\n",
      "epoch:2 step:2749 [D loss: 0.146971, acc.: 79.69%] [G loss: 0.693785]\n",
      "epoch:2 step:2750 [D loss: 0.164264, acc.: 77.34%] [G loss: 0.654195]\n",
      "epoch:2 step:2751 [D loss: 0.168419, acc.: 72.66%] [G loss: 0.678609]\n",
      "epoch:2 step:2752 [D loss: 0.177517, acc.: 72.66%] [G loss: 0.614615]\n",
      "epoch:2 step:2753 [D loss: 0.162426, acc.: 75.78%] [G loss: 0.629062]\n",
      "epoch:2 step:2754 [D loss: 0.228964, acc.: 64.84%] [G loss: 0.601479]\n",
      "epoch:2 step:2755 [D loss: 0.159434, acc.: 83.59%] [G loss: 0.616391]\n",
      "epoch:2 step:2756 [D loss: 0.193267, acc.: 69.53%] [G loss: 0.612390]\n",
      "epoch:2 step:2757 [D loss: 0.156186, acc.: 75.00%] [G loss: 0.663725]\n",
      "epoch:2 step:2758 [D loss: 0.138520, acc.: 83.59%] [G loss: 0.731972]\n",
      "epoch:2 step:2759 [D loss: 0.150207, acc.: 78.12%] [G loss: 0.675048]\n",
      "epoch:2 step:2760 [D loss: 0.122616, acc.: 82.03%] [G loss: 0.725626]\n",
      "epoch:2 step:2761 [D loss: 0.114723, acc.: 86.72%] [G loss: 0.782866]\n",
      "epoch:2 step:2762 [D loss: 0.162514, acc.: 75.00%] [G loss: 0.685776]\n",
      "epoch:2 step:2763 [D loss: 0.163518, acc.: 74.22%] [G loss: 0.701994]\n",
      "epoch:2 step:2764 [D loss: 0.103524, acc.: 92.19%] [G loss: 0.750953]\n",
      "epoch:2 step:2765 [D loss: 0.180831, acc.: 76.56%] [G loss: 0.649561]\n",
      "epoch:2 step:2766 [D loss: 0.240352, acc.: 60.94%] [G loss: 0.562583]\n",
      "epoch:2 step:2767 [D loss: 0.171838, acc.: 71.88%] [G loss: 0.648200]\n",
      "epoch:2 step:2768 [D loss: 0.169462, acc.: 76.56%] [G loss: 0.655801]\n",
      "epoch:2 step:2769 [D loss: 0.150100, acc.: 81.25%] [G loss: 0.643383]\n",
      "epoch:2 step:2770 [D loss: 0.164963, acc.: 75.00%] [G loss: 0.651476]\n",
      "epoch:2 step:2771 [D loss: 0.155863, acc.: 78.12%] [G loss: 0.678270]\n",
      "epoch:2 step:2772 [D loss: 0.136436, acc.: 85.94%] [G loss: 0.654070]\n",
      "epoch:2 step:2773 [D loss: 0.146489, acc.: 81.25%] [G loss: 0.661373]\n",
      "epoch:2 step:2774 [D loss: 0.170255, acc.: 73.44%] [G loss: 0.635325]\n",
      "epoch:2 step:2775 [D loss: 0.177427, acc.: 73.44%] [G loss: 0.648710]\n",
      "epoch:2 step:2776 [D loss: 0.179365, acc.: 75.78%] [G loss: 0.617954]\n",
      "epoch:2 step:2777 [D loss: 0.182094, acc.: 71.88%] [G loss: 0.621671]\n",
      "epoch:2 step:2778 [D loss: 0.154279, acc.: 78.91%] [G loss: 0.653773]\n",
      "epoch:2 step:2779 [D loss: 0.190769, acc.: 68.75%] [G loss: 0.663735]\n",
      "epoch:2 step:2780 [D loss: 0.142265, acc.: 82.03%] [G loss: 0.700867]\n",
      "epoch:2 step:2781 [D loss: 0.174220, acc.: 74.22%] [G loss: 0.625766]\n",
      "epoch:2 step:2782 [D loss: 0.148301, acc.: 79.69%] [G loss: 0.676654]\n",
      "epoch:2 step:2783 [D loss: 0.146114, acc.: 80.47%] [G loss: 0.724590]\n",
      "epoch:2 step:2784 [D loss: 0.144332, acc.: 79.69%] [G loss: 0.678422]\n",
      "epoch:2 step:2785 [D loss: 0.158641, acc.: 78.12%] [G loss: 0.682391]\n",
      "epoch:2 step:2786 [D loss: 0.113584, acc.: 87.50%] [G loss: 0.833130]\n",
      "epoch:2 step:2787 [D loss: 0.212551, acc.: 68.75%] [G loss: 0.617015]\n",
      "epoch:2 step:2788 [D loss: 0.123743, acc.: 84.38%] [G loss: 0.754618]\n",
      "epoch:2 step:2789 [D loss: 0.179593, acc.: 69.53%] [G loss: 0.692247]\n",
      "epoch:2 step:2790 [D loss: 0.157941, acc.: 76.56%] [G loss: 0.645721]\n",
      "epoch:2 step:2791 [D loss: 0.193726, acc.: 71.09%] [G loss: 0.674803]\n",
      "epoch:2 step:2792 [D loss: 0.145290, acc.: 83.59%] [G loss: 0.727243]\n",
      "epoch:2 step:2793 [D loss: 0.209636, acc.: 70.31%] [G loss: 0.650049]\n",
      "epoch:2 step:2794 [D loss: 0.238076, acc.: 71.09%] [G loss: 0.664093]\n",
      "epoch:2 step:2795 [D loss: 0.115893, acc.: 88.28%] [G loss: 0.819898]\n",
      "epoch:2 step:2796 [D loss: 0.230279, acc.: 62.50%] [G loss: 0.602494]\n",
      "epoch:2 step:2797 [D loss: 0.124820, acc.: 83.59%] [G loss: 0.802383]\n",
      "epoch:2 step:2798 [D loss: 0.128836, acc.: 82.03%] [G loss: 0.758883]\n",
      "epoch:2 step:2799 [D loss: 0.092527, acc.: 90.62%] [G loss: 0.822953]\n",
      "epoch:2 step:2800 [D loss: 0.110475, acc.: 87.50%] [G loss: 0.726815]\n",
      "##############\n",
      "[4.29306048 2.24156152 7.49839946 6.10906704 4.7385271  6.82522954\n",
      " 5.79697001 5.68074977 5.77827754 4.63492654]\n",
      "##########\n",
      "epoch:2 step:2801 [D loss: 0.110352, acc.: 85.16%] [G loss: 0.783592]\n",
      "epoch:2 step:2802 [D loss: 0.190160, acc.: 76.56%] [G loss: 0.783606]\n",
      "epoch:2 step:2803 [D loss: 0.090664, acc.: 89.84%] [G loss: 1.038070]\n",
      "epoch:2 step:2804 [D loss: 0.154090, acc.: 76.56%] [G loss: 0.638077]\n",
      "epoch:2 step:2805 [D loss: 0.144196, acc.: 83.59%] [G loss: 0.664999]\n",
      "epoch:2 step:2806 [D loss: 0.141718, acc.: 81.25%] [G loss: 0.725424]\n",
      "epoch:2 step:2807 [D loss: 0.140533, acc.: 81.25%] [G loss: 0.681598]\n",
      "epoch:2 step:2808 [D loss: 0.153541, acc.: 83.59%] [G loss: 0.691728]\n",
      "epoch:2 step:2809 [D loss: 0.150315, acc.: 78.91%] [G loss: 0.725197]\n",
      "epoch:2 step:2810 [D loss: 0.089944, acc.: 91.41%] [G loss: 0.805595]\n",
      "epoch:2 step:2811 [D loss: 0.185574, acc.: 72.66%] [G loss: 0.735792]\n",
      "epoch:3 step:2812 [D loss: 0.190239, acc.: 79.69%] [G loss: 0.725042]\n",
      "epoch:3 step:2813 [D loss: 0.144084, acc.: 82.81%] [G loss: 0.724282]\n",
      "epoch:3 step:2814 [D loss: 0.189344, acc.: 74.22%] [G loss: 0.607136]\n",
      "epoch:3 step:2815 [D loss: 0.139979, acc.: 83.59%] [G loss: 0.656615]\n",
      "epoch:3 step:2816 [D loss: 0.145739, acc.: 78.12%] [G loss: 0.661881]\n",
      "epoch:3 step:2817 [D loss: 0.170987, acc.: 74.22%] [G loss: 0.729879]\n",
      "epoch:3 step:2818 [D loss: 0.174579, acc.: 77.34%] [G loss: 0.674896]\n",
      "epoch:3 step:2819 [D loss: 0.176597, acc.: 73.44%] [G loss: 0.656144]\n",
      "epoch:3 step:2820 [D loss: 0.167894, acc.: 79.69%] [G loss: 0.702491]\n",
      "epoch:3 step:2821 [D loss: 0.194943, acc.: 74.22%] [G loss: 0.633385]\n",
      "epoch:3 step:2822 [D loss: 0.146605, acc.: 81.25%] [G loss: 0.699469]\n",
      "epoch:3 step:2823 [D loss: 0.182837, acc.: 69.53%] [G loss: 0.644544]\n",
      "epoch:3 step:2824 [D loss: 0.177185, acc.: 75.00%] [G loss: 0.654516]\n",
      "epoch:3 step:2825 [D loss: 0.189284, acc.: 69.53%] [G loss: 0.666926]\n",
      "epoch:3 step:2826 [D loss: 0.142077, acc.: 79.69%] [G loss: 0.707959]\n",
      "epoch:3 step:2827 [D loss: 0.182560, acc.: 74.22%] [G loss: 0.670605]\n",
      "epoch:3 step:2828 [D loss: 0.226397, acc.: 63.28%] [G loss: 0.616520]\n",
      "epoch:3 step:2829 [D loss: 0.218797, acc.: 65.62%] [G loss: 0.635028]\n",
      "epoch:3 step:2830 [D loss: 0.165732, acc.: 76.56%] [G loss: 0.635517]\n",
      "epoch:3 step:2831 [D loss: 0.187663, acc.: 72.66%] [G loss: 0.586837]\n",
      "epoch:3 step:2832 [D loss: 0.164401, acc.: 77.34%] [G loss: 0.714895]\n",
      "epoch:3 step:2833 [D loss: 0.129547, acc.: 86.72%] [G loss: 0.738202]\n",
      "epoch:3 step:2834 [D loss: 0.201448, acc.: 64.06%] [G loss: 0.635725]\n",
      "epoch:3 step:2835 [D loss: 0.174549, acc.: 77.34%] [G loss: 0.648855]\n",
      "epoch:3 step:2836 [D loss: 0.153799, acc.: 81.25%] [G loss: 0.676645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2837 [D loss: 0.190759, acc.: 68.75%] [G loss: 0.616101]\n",
      "epoch:3 step:2838 [D loss: 0.149580, acc.: 75.78%] [G loss: 0.659842]\n",
      "epoch:3 step:2839 [D loss: 0.196036, acc.: 71.09%] [G loss: 0.578094]\n",
      "epoch:3 step:2840 [D loss: 0.154112, acc.: 80.47%] [G loss: 0.635834]\n",
      "epoch:3 step:2841 [D loss: 0.174811, acc.: 78.12%] [G loss: 0.653099]\n",
      "epoch:3 step:2842 [D loss: 0.191752, acc.: 69.53%] [G loss: 0.636642]\n",
      "epoch:3 step:2843 [D loss: 0.224511, acc.: 64.84%] [G loss: 0.583983]\n",
      "epoch:3 step:2844 [D loss: 0.146944, acc.: 78.91%] [G loss: 0.662394]\n",
      "epoch:3 step:2845 [D loss: 0.189457, acc.: 71.09%] [G loss: 0.646907]\n",
      "epoch:3 step:2846 [D loss: 0.149573, acc.: 81.25%] [G loss: 0.699905]\n",
      "epoch:3 step:2847 [D loss: 0.143815, acc.: 78.91%] [G loss: 0.679940]\n",
      "epoch:3 step:2848 [D loss: 0.169382, acc.: 78.91%] [G loss: 0.659501]\n",
      "epoch:3 step:2849 [D loss: 0.188795, acc.: 71.09%] [G loss: 0.664737]\n",
      "epoch:3 step:2850 [D loss: 0.166381, acc.: 76.56%] [G loss: 0.638369]\n",
      "epoch:3 step:2851 [D loss: 0.121278, acc.: 88.28%] [G loss: 0.730898]\n",
      "epoch:3 step:2852 [D loss: 0.221226, acc.: 69.53%] [G loss: 0.633121]\n",
      "epoch:3 step:2853 [D loss: 0.153010, acc.: 75.78%] [G loss: 0.646400]\n",
      "epoch:3 step:2854 [D loss: 0.181886, acc.: 71.09%] [G loss: 0.674405]\n",
      "epoch:3 step:2855 [D loss: 0.219018, acc.: 64.84%] [G loss: 0.621656]\n",
      "epoch:3 step:2856 [D loss: 0.191658, acc.: 74.22%] [G loss: 0.630321]\n",
      "epoch:3 step:2857 [D loss: 0.177072, acc.: 75.78%] [G loss: 0.635667]\n",
      "epoch:3 step:2858 [D loss: 0.162796, acc.: 75.00%] [G loss: 0.629072]\n",
      "epoch:3 step:2859 [D loss: 0.193674, acc.: 69.53%] [G loss: 0.602907]\n",
      "epoch:3 step:2860 [D loss: 0.208859, acc.: 65.62%] [G loss: 0.598219]\n",
      "epoch:3 step:2861 [D loss: 0.182984, acc.: 70.31%] [G loss: 0.664896]\n",
      "epoch:3 step:2862 [D loss: 0.161416, acc.: 76.56%] [G loss: 0.687826]\n",
      "epoch:3 step:2863 [D loss: 0.176108, acc.: 72.66%] [G loss: 0.651805]\n",
      "epoch:3 step:2864 [D loss: 0.193109, acc.: 75.00%] [G loss: 0.653229]\n",
      "epoch:3 step:2865 [D loss: 0.164866, acc.: 75.78%] [G loss: 0.672946]\n",
      "epoch:3 step:2866 [D loss: 0.190324, acc.: 75.00%] [G loss: 0.742414]\n",
      "epoch:3 step:2867 [D loss: 0.174239, acc.: 75.00%] [G loss: 0.670577]\n",
      "epoch:3 step:2868 [D loss: 0.160405, acc.: 81.25%] [G loss: 0.584877]\n",
      "epoch:3 step:2869 [D loss: 0.143185, acc.: 78.12%] [G loss: 0.684276]\n",
      "epoch:3 step:2870 [D loss: 0.144136, acc.: 78.91%] [G loss: 0.661810]\n",
      "epoch:3 step:2871 [D loss: 0.179977, acc.: 73.44%] [G loss: 0.611881]\n",
      "epoch:3 step:2872 [D loss: 0.171275, acc.: 79.69%] [G loss: 0.653713]\n",
      "epoch:3 step:2873 [D loss: 0.201799, acc.: 68.75%] [G loss: 0.597757]\n",
      "epoch:3 step:2874 [D loss: 0.174513, acc.: 75.00%] [G loss: 0.658851]\n",
      "epoch:3 step:2875 [D loss: 0.226095, acc.: 62.50%] [G loss: 0.683863]\n",
      "epoch:3 step:2876 [D loss: 0.195081, acc.: 71.88%] [G loss: 0.654844]\n",
      "epoch:3 step:2877 [D loss: 0.197842, acc.: 67.97%] [G loss: 0.586386]\n",
      "epoch:3 step:2878 [D loss: 0.204864, acc.: 67.19%] [G loss: 0.601837]\n",
      "epoch:3 step:2879 [D loss: 0.167739, acc.: 78.12%] [G loss: 0.662667]\n",
      "epoch:3 step:2880 [D loss: 0.203899, acc.: 67.19%] [G loss: 0.621835]\n",
      "epoch:3 step:2881 [D loss: 0.146828, acc.: 83.59%] [G loss: 0.646348]\n",
      "epoch:3 step:2882 [D loss: 0.151365, acc.: 82.03%] [G loss: 0.688394]\n",
      "epoch:3 step:2883 [D loss: 0.150346, acc.: 78.91%] [G loss: 0.690263]\n",
      "epoch:3 step:2884 [D loss: 0.183003, acc.: 72.66%] [G loss: 0.613701]\n",
      "epoch:3 step:2885 [D loss: 0.143472, acc.: 82.03%] [G loss: 0.642444]\n",
      "epoch:3 step:2886 [D loss: 0.155126, acc.: 79.69%] [G loss: 0.621225]\n",
      "epoch:3 step:2887 [D loss: 0.184868, acc.: 73.44%] [G loss: 0.674811]\n",
      "epoch:3 step:2888 [D loss: 0.135078, acc.: 85.16%] [G loss: 0.711507]\n",
      "epoch:3 step:2889 [D loss: 0.224537, acc.: 65.62%] [G loss: 0.593389]\n",
      "epoch:3 step:2890 [D loss: 0.182633, acc.: 71.09%] [G loss: 0.596595]\n",
      "epoch:3 step:2891 [D loss: 0.204803, acc.: 72.66%] [G loss: 0.577797]\n",
      "epoch:3 step:2892 [D loss: 0.175907, acc.: 77.34%] [G loss: 0.639067]\n",
      "epoch:3 step:2893 [D loss: 0.152119, acc.: 80.47%] [G loss: 0.650242]\n",
      "epoch:3 step:2894 [D loss: 0.140124, acc.: 78.91%] [G loss: 0.718181]\n",
      "epoch:3 step:2895 [D loss: 0.186602, acc.: 70.31%] [G loss: 0.609752]\n",
      "epoch:3 step:2896 [D loss: 0.153823, acc.: 78.12%] [G loss: 0.644512]\n",
      "epoch:3 step:2897 [D loss: 0.155203, acc.: 78.12%] [G loss: 0.654315]\n",
      "epoch:3 step:2898 [D loss: 0.162418, acc.: 78.91%] [G loss: 0.673046]\n",
      "epoch:3 step:2899 [D loss: 0.132126, acc.: 82.81%] [G loss: 0.680617]\n",
      "epoch:3 step:2900 [D loss: 0.179502, acc.: 78.91%] [G loss: 0.656980]\n",
      "epoch:3 step:2901 [D loss: 0.167029, acc.: 78.12%] [G loss: 0.610219]\n",
      "epoch:3 step:2902 [D loss: 0.191925, acc.: 68.75%] [G loss: 0.629804]\n",
      "epoch:3 step:2903 [D loss: 0.174018, acc.: 73.44%] [G loss: 0.659685]\n",
      "epoch:3 step:2904 [D loss: 0.188081, acc.: 76.56%] [G loss: 0.631136]\n",
      "epoch:3 step:2905 [D loss: 0.154138, acc.: 78.91%] [G loss: 0.625120]\n",
      "epoch:3 step:2906 [D loss: 0.167861, acc.: 78.12%] [G loss: 0.668411]\n",
      "epoch:3 step:2907 [D loss: 0.123289, acc.: 82.03%] [G loss: 0.636630]\n",
      "epoch:3 step:2908 [D loss: 0.165578, acc.: 80.47%] [G loss: 0.618766]\n",
      "epoch:3 step:2909 [D loss: 0.164525, acc.: 76.56%] [G loss: 0.645998]\n",
      "epoch:3 step:2910 [D loss: 0.165769, acc.: 76.56%] [G loss: 0.655979]\n",
      "epoch:3 step:2911 [D loss: 0.126424, acc.: 84.38%] [G loss: 0.728954]\n",
      "epoch:3 step:2912 [D loss: 0.172002, acc.: 73.44%] [G loss: 0.624829]\n",
      "epoch:3 step:2913 [D loss: 0.216153, acc.: 67.97%] [G loss: 0.552225]\n",
      "epoch:3 step:2914 [D loss: 0.130094, acc.: 84.38%] [G loss: 0.660090]\n",
      "epoch:3 step:2915 [D loss: 0.169769, acc.: 75.00%] [G loss: 0.589697]\n",
      "epoch:3 step:2916 [D loss: 0.203574, acc.: 70.31%] [G loss: 0.550538]\n",
      "epoch:3 step:2917 [D loss: 0.232374, acc.: 61.72%] [G loss: 0.508327]\n",
      "epoch:3 step:2918 [D loss: 0.228040, acc.: 67.19%] [G loss: 0.607069]\n",
      "epoch:3 step:2919 [D loss: 0.232906, acc.: 62.50%] [G loss: 0.632811]\n",
      "epoch:3 step:2920 [D loss: 0.195749, acc.: 70.31%] [G loss: 0.632504]\n",
      "epoch:3 step:2921 [D loss: 0.180186, acc.: 74.22%] [G loss: 0.620224]\n",
      "epoch:3 step:2922 [D loss: 0.173269, acc.: 74.22%] [G loss: 0.595079]\n",
      "epoch:3 step:2923 [D loss: 0.162494, acc.: 75.78%] [G loss: 0.601943]\n",
      "epoch:3 step:2924 [D loss: 0.231425, acc.: 64.84%] [G loss: 0.562638]\n",
      "epoch:3 step:2925 [D loss: 0.178405, acc.: 71.09%] [G loss: 0.600455]\n",
      "epoch:3 step:2926 [D loss: 0.212941, acc.: 71.09%] [G loss: 0.623347]\n",
      "epoch:3 step:2927 [D loss: 0.218212, acc.: 69.53%] [G loss: 0.594107]\n",
      "epoch:3 step:2928 [D loss: 0.192438, acc.: 69.53%] [G loss: 0.596221]\n",
      "epoch:3 step:2929 [D loss: 0.178758, acc.: 75.00%] [G loss: 0.593104]\n",
      "epoch:3 step:2930 [D loss: 0.145917, acc.: 79.69%] [G loss: 0.679849]\n",
      "epoch:3 step:2931 [D loss: 0.231281, acc.: 63.28%] [G loss: 0.566321]\n",
      "epoch:3 step:2932 [D loss: 0.170975, acc.: 74.22%] [G loss: 0.612162]\n",
      "epoch:3 step:2933 [D loss: 0.184959, acc.: 75.78%] [G loss: 0.653728]\n",
      "epoch:3 step:2934 [D loss: 0.185563, acc.: 74.22%] [G loss: 0.607337]\n",
      "epoch:3 step:2935 [D loss: 0.212653, acc.: 70.31%] [G loss: 0.616070]\n",
      "epoch:3 step:2936 [D loss: 0.178400, acc.: 73.44%] [G loss: 0.596135]\n",
      "epoch:3 step:2937 [D loss: 0.193994, acc.: 71.88%] [G loss: 0.636648]\n",
      "epoch:3 step:2938 [D loss: 0.170003, acc.: 71.88%] [G loss: 0.632121]\n",
      "epoch:3 step:2939 [D loss: 0.193919, acc.: 70.31%] [G loss: 0.609191]\n",
      "epoch:3 step:2940 [D loss: 0.177033, acc.: 73.44%] [G loss: 0.567492]\n",
      "epoch:3 step:2941 [D loss: 0.158682, acc.: 79.69%] [G loss: 0.572341]\n",
      "epoch:3 step:2942 [D loss: 0.152208, acc.: 75.78%] [G loss: 0.664488]\n",
      "epoch:3 step:2943 [D loss: 0.171154, acc.: 77.34%] [G loss: 0.617872]\n",
      "epoch:3 step:2944 [D loss: 0.223107, acc.: 64.84%] [G loss: 0.541770]\n",
      "epoch:3 step:2945 [D loss: 0.172796, acc.: 74.22%] [G loss: 0.579658]\n",
      "epoch:3 step:2946 [D loss: 0.153214, acc.: 75.00%] [G loss: 0.668605]\n",
      "epoch:3 step:2947 [D loss: 0.191347, acc.: 73.44%] [G loss: 0.651205]\n",
      "epoch:3 step:2948 [D loss: 0.189273, acc.: 77.34%] [G loss: 0.630318]\n",
      "epoch:3 step:2949 [D loss: 0.176334, acc.: 73.44%] [G loss: 0.586646]\n",
      "epoch:3 step:2950 [D loss: 0.190893, acc.: 74.22%] [G loss: 0.627611]\n",
      "epoch:3 step:2951 [D loss: 0.160636, acc.: 74.22%] [G loss: 0.596209]\n",
      "epoch:3 step:2952 [D loss: 0.145993, acc.: 79.69%] [G loss: 0.662130]\n",
      "epoch:3 step:2953 [D loss: 0.148647, acc.: 83.59%] [G loss: 0.621917]\n",
      "epoch:3 step:2954 [D loss: 0.188248, acc.: 75.78%] [G loss: 0.552055]\n",
      "epoch:3 step:2955 [D loss: 0.127163, acc.: 87.50%] [G loss: 0.650829]\n",
      "epoch:3 step:2956 [D loss: 0.192370, acc.: 72.66%] [G loss: 0.578468]\n",
      "epoch:3 step:2957 [D loss: 0.166735, acc.: 79.69%] [G loss: 0.608252]\n",
      "epoch:3 step:2958 [D loss: 0.175310, acc.: 79.69%] [G loss: 0.618544]\n",
      "epoch:3 step:2959 [D loss: 0.152036, acc.: 78.12%] [G loss: 0.618681]\n",
      "epoch:3 step:2960 [D loss: 0.162156, acc.: 77.34%] [G loss: 0.672714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2961 [D loss: 0.206498, acc.: 70.31%] [G loss: 0.656419]\n",
      "epoch:3 step:2962 [D loss: 0.157654, acc.: 76.56%] [G loss: 0.662333]\n",
      "epoch:3 step:2963 [D loss: 0.147421, acc.: 78.12%] [G loss: 0.700188]\n",
      "epoch:3 step:2964 [D loss: 0.207952, acc.: 73.44%] [G loss: 0.538145]\n",
      "epoch:3 step:2965 [D loss: 0.138386, acc.: 82.03%] [G loss: 0.666013]\n",
      "epoch:3 step:2966 [D loss: 0.140616, acc.: 82.81%] [G loss: 0.653824]\n",
      "epoch:3 step:2967 [D loss: 0.194950, acc.: 72.66%] [G loss: 0.623718]\n",
      "epoch:3 step:2968 [D loss: 0.160076, acc.: 77.34%] [G loss: 0.682633]\n",
      "epoch:3 step:2969 [D loss: 0.184643, acc.: 73.44%] [G loss: 0.584202]\n",
      "epoch:3 step:2970 [D loss: 0.144195, acc.: 79.69%] [G loss: 0.668661]\n",
      "epoch:3 step:2971 [D loss: 0.206133, acc.: 67.97%] [G loss: 0.580317]\n",
      "epoch:3 step:2972 [D loss: 0.202749, acc.: 69.53%] [G loss: 0.663131]\n",
      "epoch:3 step:2973 [D loss: 0.110610, acc.: 86.72%] [G loss: 0.756486]\n",
      "epoch:3 step:2974 [D loss: 0.188931, acc.: 71.88%] [G loss: 0.664156]\n",
      "epoch:3 step:2975 [D loss: 0.198143, acc.: 69.53%] [G loss: 0.670461]\n",
      "epoch:3 step:2976 [D loss: 0.169904, acc.: 74.22%] [G loss: 0.642625]\n",
      "epoch:3 step:2977 [D loss: 0.140915, acc.: 81.25%] [G loss: 0.632077]\n",
      "epoch:3 step:2978 [D loss: 0.189983, acc.: 75.00%] [G loss: 0.645948]\n",
      "epoch:3 step:2979 [D loss: 0.188218, acc.: 73.44%] [G loss: 0.602203]\n",
      "epoch:3 step:2980 [D loss: 0.189628, acc.: 71.09%] [G loss: 0.635176]\n",
      "epoch:3 step:2981 [D loss: 0.163424, acc.: 78.12%] [G loss: 0.622352]\n",
      "epoch:3 step:2982 [D loss: 0.168578, acc.: 75.00%] [G loss: 0.604632]\n",
      "epoch:3 step:2983 [D loss: 0.181844, acc.: 76.56%] [G loss: 0.618820]\n",
      "epoch:3 step:2984 [D loss: 0.193582, acc.: 70.31%] [G loss: 0.623651]\n",
      "epoch:3 step:2985 [D loss: 0.201937, acc.: 66.41%] [G loss: 0.562220]\n",
      "epoch:3 step:2986 [D loss: 0.193500, acc.: 72.66%] [G loss: 0.617676]\n",
      "epoch:3 step:2987 [D loss: 0.165611, acc.: 75.78%] [G loss: 0.658932]\n",
      "epoch:3 step:2988 [D loss: 0.185498, acc.: 74.22%] [G loss: 0.622760]\n",
      "epoch:3 step:2989 [D loss: 0.161996, acc.: 75.78%] [G loss: 0.654264]\n",
      "epoch:3 step:2990 [D loss: 0.158742, acc.: 81.25%] [G loss: 0.642663]\n",
      "epoch:3 step:2991 [D loss: 0.173174, acc.: 75.00%] [G loss: 0.580089]\n",
      "epoch:3 step:2992 [D loss: 0.182314, acc.: 70.31%] [G loss: 0.614192]\n",
      "epoch:3 step:2993 [D loss: 0.183419, acc.: 71.88%] [G loss: 0.620982]\n",
      "epoch:3 step:2994 [D loss: 0.189221, acc.: 67.97%] [G loss: 0.622418]\n",
      "epoch:3 step:2995 [D loss: 0.229710, acc.: 62.50%] [G loss: 0.612379]\n",
      "epoch:3 step:2996 [D loss: 0.179025, acc.: 77.34%] [G loss: 0.621657]\n",
      "epoch:3 step:2997 [D loss: 0.194769, acc.: 73.44%] [G loss: 0.576745]\n",
      "epoch:3 step:2998 [D loss: 0.172231, acc.: 77.34%] [G loss: 0.605339]\n",
      "epoch:3 step:2999 [D loss: 0.167916, acc.: 78.12%] [G loss: 0.647812]\n",
      "epoch:3 step:3000 [D loss: 0.152251, acc.: 80.47%] [G loss: 0.721597]\n",
      "##############\n",
      "[3.55094498 1.67638195 7.31535227 5.58851722 4.68646301 6.69269825\n",
      " 5.70892667 5.41990969 5.57579267 4.41992817]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.136328, acc.: 79.69%] [G loss: 0.699116]\n",
      "epoch:3 step:3002 [D loss: 0.165408, acc.: 79.69%] [G loss: 0.641571]\n",
      "epoch:3 step:3003 [D loss: 0.173553, acc.: 74.22%] [G loss: 0.566968]\n",
      "epoch:3 step:3004 [D loss: 0.163139, acc.: 80.47%] [G loss: 0.652559]\n",
      "epoch:3 step:3005 [D loss: 0.128541, acc.: 83.59%] [G loss: 0.691704]\n",
      "epoch:3 step:3006 [D loss: 0.205164, acc.: 69.53%] [G loss: 0.615515]\n",
      "epoch:3 step:3007 [D loss: 0.196442, acc.: 69.53%] [G loss: 0.584001]\n",
      "epoch:3 step:3008 [D loss: 0.149073, acc.: 77.34%] [G loss: 0.662999]\n",
      "epoch:3 step:3009 [D loss: 0.146101, acc.: 78.12%] [G loss: 0.709300]\n",
      "epoch:3 step:3010 [D loss: 0.194460, acc.: 71.09%] [G loss: 0.615047]\n",
      "epoch:3 step:3011 [D loss: 0.225323, acc.: 62.50%] [G loss: 0.533944]\n",
      "epoch:3 step:3012 [D loss: 0.162238, acc.: 78.91%] [G loss: 0.639059]\n",
      "epoch:3 step:3013 [D loss: 0.157840, acc.: 77.34%] [G loss: 0.641862]\n",
      "epoch:3 step:3014 [D loss: 0.221207, acc.: 62.50%] [G loss: 0.601701]\n",
      "epoch:3 step:3015 [D loss: 0.164714, acc.: 78.12%] [G loss: 0.667015]\n",
      "epoch:3 step:3016 [D loss: 0.174039, acc.: 75.78%] [G loss: 0.688403]\n",
      "epoch:3 step:3017 [D loss: 0.183926, acc.: 76.56%] [G loss: 0.612483]\n",
      "epoch:3 step:3018 [D loss: 0.128510, acc.: 85.16%] [G loss: 0.671914]\n",
      "epoch:3 step:3019 [D loss: 0.128527, acc.: 84.38%] [G loss: 0.689721]\n",
      "epoch:3 step:3020 [D loss: 0.150515, acc.: 78.91%] [G loss: 0.653375]\n",
      "epoch:3 step:3021 [D loss: 0.211801, acc.: 67.19%] [G loss: 0.609786]\n",
      "epoch:3 step:3022 [D loss: 0.153389, acc.: 80.47%] [G loss: 0.580083]\n",
      "epoch:3 step:3023 [D loss: 0.176565, acc.: 75.78%] [G loss: 0.607096]\n",
      "epoch:3 step:3024 [D loss: 0.164356, acc.: 78.12%] [G loss: 0.659990]\n",
      "epoch:3 step:3025 [D loss: 0.252510, acc.: 60.94%] [G loss: 0.517344]\n",
      "epoch:3 step:3026 [D loss: 0.185785, acc.: 71.09%] [G loss: 0.543908]\n",
      "epoch:3 step:3027 [D loss: 0.176814, acc.: 75.00%] [G loss: 0.624107]\n",
      "epoch:3 step:3028 [D loss: 0.161857, acc.: 81.25%] [G loss: 0.714338]\n",
      "epoch:3 step:3029 [D loss: 0.131871, acc.: 85.16%] [G loss: 0.713497]\n",
      "epoch:3 step:3030 [D loss: 0.137531, acc.: 82.03%] [G loss: 0.643943]\n",
      "epoch:3 step:3031 [D loss: 0.195212, acc.: 66.41%] [G loss: 0.587425]\n",
      "epoch:3 step:3032 [D loss: 0.174820, acc.: 75.78%] [G loss: 0.713123]\n",
      "epoch:3 step:3033 [D loss: 0.167879, acc.: 75.00%] [G loss: 0.718691]\n",
      "epoch:3 step:3034 [D loss: 0.154895, acc.: 80.47%] [G loss: 0.625196]\n",
      "epoch:3 step:3035 [D loss: 0.209192, acc.: 67.97%] [G loss: 0.612625]\n",
      "epoch:3 step:3036 [D loss: 0.196491, acc.: 71.88%] [G loss: 0.615334]\n",
      "epoch:3 step:3037 [D loss: 0.216199, acc.: 67.19%] [G loss: 0.552129]\n",
      "epoch:3 step:3038 [D loss: 0.199579, acc.: 71.88%] [G loss: 0.585210]\n",
      "epoch:3 step:3039 [D loss: 0.170649, acc.: 78.91%] [G loss: 0.602677]\n",
      "epoch:3 step:3040 [D loss: 0.165189, acc.: 75.78%] [G loss: 0.652158]\n",
      "epoch:3 step:3041 [D loss: 0.123458, acc.: 85.94%] [G loss: 0.704543]\n",
      "epoch:3 step:3042 [D loss: 0.148878, acc.: 77.34%] [G loss: 0.706063]\n",
      "epoch:3 step:3043 [D loss: 0.128917, acc.: 82.81%] [G loss: 0.737172]\n",
      "epoch:3 step:3044 [D loss: 0.199145, acc.: 71.88%] [G loss: 0.573641]\n",
      "epoch:3 step:3045 [D loss: 0.242711, acc.: 57.03%] [G loss: 0.581231]\n",
      "epoch:3 step:3046 [D loss: 0.174505, acc.: 77.34%] [G loss: 0.597467]\n",
      "epoch:3 step:3047 [D loss: 0.166976, acc.: 75.00%] [G loss: 0.567070]\n",
      "epoch:3 step:3048 [D loss: 0.189361, acc.: 68.75%] [G loss: 0.589130]\n",
      "epoch:3 step:3049 [D loss: 0.176331, acc.: 75.78%] [G loss: 0.586296]\n",
      "epoch:3 step:3050 [D loss: 0.159799, acc.: 79.69%] [G loss: 0.607163]\n",
      "epoch:3 step:3051 [D loss: 0.173827, acc.: 75.78%] [G loss: 0.656295]\n",
      "epoch:3 step:3052 [D loss: 0.170250, acc.: 74.22%] [G loss: 0.654635]\n",
      "epoch:3 step:3053 [D loss: 0.173646, acc.: 78.12%] [G loss: 0.633583]\n",
      "epoch:3 step:3054 [D loss: 0.169050, acc.: 75.78%] [G loss: 0.606217]\n",
      "epoch:3 step:3055 [D loss: 0.170008, acc.: 78.12%] [G loss: 0.639330]\n",
      "epoch:3 step:3056 [D loss: 0.152157, acc.: 82.81%] [G loss: 0.624472]\n",
      "epoch:3 step:3057 [D loss: 0.195599, acc.: 71.09%] [G loss: 0.618404]\n",
      "epoch:3 step:3058 [D loss: 0.233269, acc.: 62.50%] [G loss: 0.589741]\n",
      "epoch:3 step:3059 [D loss: 0.151936, acc.: 77.34%] [G loss: 0.670650]\n",
      "epoch:3 step:3060 [D loss: 0.205042, acc.: 69.53%] [G loss: 0.613391]\n",
      "epoch:3 step:3061 [D loss: 0.230667, acc.: 64.06%] [G loss: 0.531308]\n",
      "epoch:3 step:3062 [D loss: 0.205973, acc.: 64.06%] [G loss: 0.597208]\n",
      "epoch:3 step:3063 [D loss: 0.211075, acc.: 66.41%] [G loss: 0.592538]\n",
      "epoch:3 step:3064 [D loss: 0.187594, acc.: 67.97%] [G loss: 0.611962]\n",
      "epoch:3 step:3065 [D loss: 0.175531, acc.: 77.34%] [G loss: 0.625862]\n",
      "epoch:3 step:3066 [D loss: 0.175892, acc.: 76.56%] [G loss: 0.704041]\n",
      "epoch:3 step:3067 [D loss: 0.193303, acc.: 67.19%] [G loss: 0.637102]\n",
      "epoch:3 step:3068 [D loss: 0.199094, acc.: 70.31%] [G loss: 0.579505]\n",
      "epoch:3 step:3069 [D loss: 0.143621, acc.: 82.81%] [G loss: 0.634067]\n",
      "epoch:3 step:3070 [D loss: 0.186342, acc.: 71.88%] [G loss: 0.634665]\n",
      "epoch:3 step:3071 [D loss: 0.182637, acc.: 75.00%] [G loss: 0.672232]\n",
      "epoch:3 step:3072 [D loss: 0.187779, acc.: 74.22%] [G loss: 0.622697]\n",
      "epoch:3 step:3073 [D loss: 0.179503, acc.: 76.56%] [G loss: 0.633972]\n",
      "epoch:3 step:3074 [D loss: 0.213449, acc.: 67.97%] [G loss: 0.602108]\n",
      "epoch:3 step:3075 [D loss: 0.120808, acc.: 85.94%] [G loss: 0.686939]\n",
      "epoch:3 step:3076 [D loss: 0.237874, acc.: 63.28%] [G loss: 0.533410]\n",
      "epoch:3 step:3077 [D loss: 0.185535, acc.: 72.66%] [G loss: 0.590216]\n",
      "epoch:3 step:3078 [D loss: 0.207398, acc.: 64.06%] [G loss: 0.573147]\n",
      "epoch:3 step:3079 [D loss: 0.214888, acc.: 70.31%] [G loss: 0.589920]\n",
      "epoch:3 step:3080 [D loss: 0.176319, acc.: 73.44%] [G loss: 0.625609]\n",
      "epoch:3 step:3081 [D loss: 0.185334, acc.: 70.31%] [G loss: 0.619359]\n",
      "epoch:3 step:3082 [D loss: 0.155159, acc.: 78.12%] [G loss: 0.683627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3083 [D loss: 0.220504, acc.: 65.62%] [G loss: 0.594999]\n",
      "epoch:3 step:3084 [D loss: 0.165690, acc.: 77.34%] [G loss: 0.628903]\n",
      "epoch:3 step:3085 [D loss: 0.178840, acc.: 73.44%] [G loss: 0.604835]\n",
      "epoch:3 step:3086 [D loss: 0.210521, acc.: 70.31%] [G loss: 0.588025]\n",
      "epoch:3 step:3087 [D loss: 0.173066, acc.: 76.56%] [G loss: 0.630307]\n",
      "epoch:3 step:3088 [D loss: 0.227336, acc.: 67.19%] [G loss: 0.585284]\n",
      "epoch:3 step:3089 [D loss: 0.204747, acc.: 64.06%] [G loss: 0.563920]\n",
      "epoch:3 step:3090 [D loss: 0.177249, acc.: 76.56%] [G loss: 0.670032]\n",
      "epoch:3 step:3091 [D loss: 0.172599, acc.: 74.22%] [G loss: 0.611587]\n",
      "epoch:3 step:3092 [D loss: 0.224666, acc.: 64.06%] [G loss: 0.577265]\n",
      "epoch:3 step:3093 [D loss: 0.164649, acc.: 75.00%] [G loss: 0.624570]\n",
      "epoch:3 step:3094 [D loss: 0.136342, acc.: 79.69%] [G loss: 0.663692]\n",
      "epoch:3 step:3095 [D loss: 0.161033, acc.: 75.00%] [G loss: 0.615334]\n",
      "epoch:3 step:3096 [D loss: 0.155912, acc.: 78.12%] [G loss: 0.632811]\n",
      "epoch:3 step:3097 [D loss: 0.130526, acc.: 84.38%] [G loss: 0.684768]\n",
      "epoch:3 step:3098 [D loss: 0.210498, acc.: 64.06%] [G loss: 0.558079]\n",
      "epoch:3 step:3099 [D loss: 0.190501, acc.: 66.41%] [G loss: 0.590546]\n",
      "epoch:3 step:3100 [D loss: 0.152370, acc.: 82.03%] [G loss: 0.675045]\n",
      "epoch:3 step:3101 [D loss: 0.155444, acc.: 76.56%] [G loss: 0.660682]\n",
      "epoch:3 step:3102 [D loss: 0.185051, acc.: 71.88%] [G loss: 0.652964]\n",
      "epoch:3 step:3103 [D loss: 0.200184, acc.: 72.66%] [G loss: 0.584325]\n",
      "epoch:3 step:3104 [D loss: 0.178875, acc.: 68.75%] [G loss: 0.604990]\n",
      "epoch:3 step:3105 [D loss: 0.183986, acc.: 73.44%] [G loss: 0.560218]\n",
      "epoch:3 step:3106 [D loss: 0.176625, acc.: 71.88%] [G loss: 0.629027]\n",
      "epoch:3 step:3107 [D loss: 0.163106, acc.: 75.78%] [G loss: 0.640149]\n",
      "epoch:3 step:3108 [D loss: 0.183588, acc.: 75.00%] [G loss: 0.591992]\n",
      "epoch:3 step:3109 [D loss: 0.172817, acc.: 73.44%] [G loss: 0.630504]\n",
      "epoch:3 step:3110 [D loss: 0.165243, acc.: 77.34%] [G loss: 0.580603]\n",
      "epoch:3 step:3111 [D loss: 0.170090, acc.: 77.34%] [G loss: 0.603801]\n",
      "epoch:3 step:3112 [D loss: 0.234018, acc.: 69.53%] [G loss: 0.618018]\n",
      "epoch:3 step:3113 [D loss: 0.185506, acc.: 73.44%] [G loss: 0.585049]\n",
      "epoch:3 step:3114 [D loss: 0.187190, acc.: 67.97%] [G loss: 0.641907]\n",
      "epoch:3 step:3115 [D loss: 0.154190, acc.: 82.03%] [G loss: 0.655160]\n",
      "epoch:3 step:3116 [D loss: 0.152433, acc.: 81.25%] [G loss: 0.630230]\n",
      "epoch:3 step:3117 [D loss: 0.176389, acc.: 75.00%] [G loss: 0.597856]\n",
      "epoch:3 step:3118 [D loss: 0.149534, acc.: 78.12%] [G loss: 0.655405]\n",
      "epoch:3 step:3119 [D loss: 0.167263, acc.: 76.56%] [G loss: 0.615110]\n",
      "epoch:3 step:3120 [D loss: 0.145550, acc.: 81.25%] [G loss: 0.643506]\n",
      "epoch:3 step:3121 [D loss: 0.140932, acc.: 84.38%] [G loss: 0.661442]\n",
      "epoch:3 step:3122 [D loss: 0.165198, acc.: 72.66%] [G loss: 0.686638]\n",
      "epoch:3 step:3123 [D loss: 0.123220, acc.: 84.38%] [G loss: 0.715359]\n",
      "epoch:3 step:3124 [D loss: 0.154793, acc.: 72.66%] [G loss: 0.690359]\n",
      "epoch:3 step:3125 [D loss: 0.126742, acc.: 85.16%] [G loss: 0.731741]\n",
      "epoch:3 step:3126 [D loss: 0.171636, acc.: 75.78%] [G loss: 0.658658]\n",
      "epoch:3 step:3127 [D loss: 0.232518, acc.: 62.50%] [G loss: 0.609591]\n",
      "epoch:3 step:3128 [D loss: 0.183245, acc.: 75.78%] [G loss: 0.555526]\n",
      "epoch:3 step:3129 [D loss: 0.147056, acc.: 82.81%] [G loss: 0.603531]\n",
      "epoch:3 step:3130 [D loss: 0.191338, acc.: 71.88%] [G loss: 0.658576]\n",
      "epoch:3 step:3131 [D loss: 0.153849, acc.: 77.34%] [G loss: 0.636501]\n",
      "epoch:3 step:3132 [D loss: 0.133938, acc.: 80.47%] [G loss: 0.671086]\n",
      "epoch:3 step:3133 [D loss: 0.176653, acc.: 75.78%] [G loss: 0.616185]\n",
      "epoch:3 step:3134 [D loss: 0.191399, acc.: 74.22%] [G loss: 0.581550]\n",
      "epoch:3 step:3135 [D loss: 0.182028, acc.: 75.00%] [G loss: 0.576873]\n",
      "epoch:3 step:3136 [D loss: 0.173917, acc.: 75.00%] [G loss: 0.573751]\n",
      "epoch:3 step:3137 [D loss: 0.170143, acc.: 76.56%] [G loss: 0.583329]\n",
      "epoch:3 step:3138 [D loss: 0.189735, acc.: 72.66%] [G loss: 0.561522]\n",
      "epoch:3 step:3139 [D loss: 0.180625, acc.: 71.88%] [G loss: 0.620606]\n",
      "epoch:3 step:3140 [D loss: 0.161523, acc.: 81.25%] [G loss: 0.632127]\n",
      "epoch:3 step:3141 [D loss: 0.162490, acc.: 78.91%] [G loss: 0.589716]\n",
      "epoch:3 step:3142 [D loss: 0.180884, acc.: 73.44%] [G loss: 0.605230]\n",
      "epoch:3 step:3143 [D loss: 0.153449, acc.: 82.03%] [G loss: 0.632526]\n",
      "epoch:3 step:3144 [D loss: 0.159604, acc.: 78.91%] [G loss: 0.661160]\n",
      "epoch:3 step:3145 [D loss: 0.224421, acc.: 67.19%] [G loss: 0.580986]\n",
      "epoch:3 step:3146 [D loss: 0.162586, acc.: 75.00%] [G loss: 0.668447]\n",
      "epoch:3 step:3147 [D loss: 0.165298, acc.: 76.56%] [G loss: 0.637601]\n",
      "epoch:3 step:3148 [D loss: 0.158760, acc.: 80.47%] [G loss: 0.624587]\n",
      "epoch:3 step:3149 [D loss: 0.203674, acc.: 65.62%] [G loss: 0.601498]\n",
      "epoch:3 step:3150 [D loss: 0.180424, acc.: 72.66%] [G loss: 0.584804]\n",
      "epoch:3 step:3151 [D loss: 0.162556, acc.: 79.69%] [G loss: 0.567508]\n",
      "epoch:3 step:3152 [D loss: 0.208403, acc.: 66.41%] [G loss: 0.547504]\n",
      "epoch:3 step:3153 [D loss: 0.155509, acc.: 81.25%] [G loss: 0.631573]\n",
      "epoch:3 step:3154 [D loss: 0.144648, acc.: 77.34%] [G loss: 0.739171]\n",
      "epoch:3 step:3155 [D loss: 0.209065, acc.: 71.88%] [G loss: 0.628002]\n",
      "epoch:3 step:3156 [D loss: 0.181159, acc.: 73.44%] [G loss: 0.614757]\n",
      "epoch:3 step:3157 [D loss: 0.167010, acc.: 75.00%] [G loss: 0.673197]\n",
      "epoch:3 step:3158 [D loss: 0.169257, acc.: 71.09%] [G loss: 0.668187]\n",
      "epoch:3 step:3159 [D loss: 0.219530, acc.: 66.41%] [G loss: 0.608695]\n",
      "epoch:3 step:3160 [D loss: 0.254065, acc.: 57.03%] [G loss: 0.548230]\n",
      "epoch:3 step:3161 [D loss: 0.185235, acc.: 73.44%] [G loss: 0.512599]\n",
      "epoch:3 step:3162 [D loss: 0.168137, acc.: 75.00%] [G loss: 0.628248]\n",
      "epoch:3 step:3163 [D loss: 0.237108, acc.: 62.50%] [G loss: 0.555625]\n",
      "epoch:3 step:3164 [D loss: 0.179793, acc.: 71.88%] [G loss: 0.557288]\n",
      "epoch:3 step:3165 [D loss: 0.142501, acc.: 81.25%] [G loss: 0.628192]\n",
      "epoch:3 step:3166 [D loss: 0.184485, acc.: 73.44%] [G loss: 0.578835]\n",
      "epoch:3 step:3167 [D loss: 0.220039, acc.: 67.19%] [G loss: 0.577175]\n",
      "epoch:3 step:3168 [D loss: 0.162334, acc.: 82.03%] [G loss: 0.615981]\n",
      "epoch:3 step:3169 [D loss: 0.143210, acc.: 83.59%] [G loss: 0.640927]\n",
      "epoch:3 step:3170 [D loss: 0.156743, acc.: 76.56%] [G loss: 0.620489]\n",
      "epoch:3 step:3171 [D loss: 0.154940, acc.: 77.34%] [G loss: 0.640387]\n",
      "epoch:3 step:3172 [D loss: 0.180788, acc.: 73.44%] [G loss: 0.597179]\n",
      "epoch:3 step:3173 [D loss: 0.160943, acc.: 73.44%] [G loss: 0.574404]\n",
      "epoch:3 step:3174 [D loss: 0.184269, acc.: 75.00%] [G loss: 0.546820]\n",
      "epoch:3 step:3175 [D loss: 0.165044, acc.: 78.12%] [G loss: 0.603285]\n",
      "epoch:3 step:3176 [D loss: 0.198124, acc.: 70.31%] [G loss: 0.602761]\n",
      "epoch:3 step:3177 [D loss: 0.161189, acc.: 80.47%] [G loss: 0.670654]\n",
      "epoch:3 step:3178 [D loss: 0.195717, acc.: 69.53%] [G loss: 0.623065]\n",
      "epoch:3 step:3179 [D loss: 0.183313, acc.: 76.56%] [G loss: 0.597039]\n",
      "epoch:3 step:3180 [D loss: 0.213662, acc.: 62.50%] [G loss: 0.555259]\n",
      "epoch:3 step:3181 [D loss: 0.183284, acc.: 74.22%] [G loss: 0.613095]\n",
      "epoch:3 step:3182 [D loss: 0.199332, acc.: 68.75%] [G loss: 0.561097]\n",
      "epoch:3 step:3183 [D loss: 0.167503, acc.: 76.56%] [G loss: 0.592788]\n",
      "epoch:3 step:3184 [D loss: 0.214395, acc.: 68.75%] [G loss: 0.551481]\n",
      "epoch:3 step:3185 [D loss: 0.174610, acc.: 75.00%] [G loss: 0.610788]\n",
      "epoch:3 step:3186 [D loss: 0.169235, acc.: 77.34%] [G loss: 0.618432]\n",
      "epoch:3 step:3187 [D loss: 0.243346, acc.: 60.94%] [G loss: 0.512962]\n",
      "epoch:3 step:3188 [D loss: 0.195202, acc.: 71.88%] [G loss: 0.595798]\n",
      "epoch:3 step:3189 [D loss: 0.167323, acc.: 78.12%] [G loss: 0.584229]\n",
      "epoch:3 step:3190 [D loss: 0.175399, acc.: 72.66%] [G loss: 0.566387]\n",
      "epoch:3 step:3191 [D loss: 0.189866, acc.: 69.53%] [G loss: 0.586574]\n",
      "epoch:3 step:3192 [D loss: 0.151932, acc.: 80.47%] [G loss: 0.643394]\n",
      "epoch:3 step:3193 [D loss: 0.174747, acc.: 75.78%] [G loss: 0.559138]\n",
      "epoch:3 step:3194 [D loss: 0.200074, acc.: 68.75%] [G loss: 0.528538]\n",
      "epoch:3 step:3195 [D loss: 0.159238, acc.: 74.22%] [G loss: 0.587942]\n",
      "epoch:3 step:3196 [D loss: 0.155980, acc.: 75.00%] [G loss: 0.603274]\n",
      "epoch:3 step:3197 [D loss: 0.197925, acc.: 65.62%] [G loss: 0.563666]\n",
      "epoch:3 step:3198 [D loss: 0.188448, acc.: 70.31%] [G loss: 0.546467]\n",
      "epoch:3 step:3199 [D loss: 0.156042, acc.: 78.91%] [G loss: 0.627528]\n",
      "epoch:3 step:3200 [D loss: 0.173234, acc.: 75.00%] [G loss: 0.615769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[3.83095402 1.41597215 7.0045515  5.53623199 4.56936316 6.33115678\n",
      " 5.53658213 5.31957562 5.51916415 3.97270136]\n",
      "##########\n",
      "epoch:3 step:3201 [D loss: 0.178772, acc.: 76.56%] [G loss: 0.550563]\n",
      "epoch:3 step:3202 [D loss: 0.181660, acc.: 71.09%] [G loss: 0.593866]\n",
      "epoch:3 step:3203 [D loss: 0.158608, acc.: 75.00%] [G loss: 0.616967]\n",
      "epoch:3 step:3204 [D loss: 0.181647, acc.: 73.44%] [G loss: 0.607025]\n",
      "epoch:3 step:3205 [D loss: 0.169460, acc.: 71.88%] [G loss: 0.611749]\n",
      "epoch:3 step:3206 [D loss: 0.170247, acc.: 80.47%] [G loss: 0.603117]\n",
      "epoch:3 step:3207 [D loss: 0.230882, acc.: 63.28%] [G loss: 0.570839]\n",
      "epoch:3 step:3208 [D loss: 0.154041, acc.: 78.12%] [G loss: 0.675792]\n",
      "epoch:3 step:3209 [D loss: 0.169942, acc.: 75.78%] [G loss: 0.671992]\n",
      "epoch:3 step:3210 [D loss: 0.165629, acc.: 75.78%] [G loss: 0.656888]\n",
      "epoch:3 step:3211 [D loss: 0.210266, acc.: 67.97%] [G loss: 0.583086]\n",
      "epoch:3 step:3212 [D loss: 0.151972, acc.: 77.34%] [G loss: 0.648747]\n",
      "epoch:3 step:3213 [D loss: 0.126687, acc.: 82.03%] [G loss: 0.673149]\n",
      "epoch:3 step:3214 [D loss: 0.171212, acc.: 76.56%] [G loss: 0.619769]\n",
      "epoch:3 step:3215 [D loss: 0.217560, acc.: 64.84%] [G loss: 0.608942]\n",
      "epoch:3 step:3216 [D loss: 0.182352, acc.: 75.78%] [G loss: 0.584098]\n",
      "epoch:3 step:3217 [D loss: 0.166625, acc.: 75.00%] [G loss: 0.620012]\n",
      "epoch:3 step:3218 [D loss: 0.190262, acc.: 69.53%] [G loss: 0.672459]\n",
      "epoch:3 step:3219 [D loss: 0.167994, acc.: 79.69%] [G loss: 0.592939]\n",
      "epoch:3 step:3220 [D loss: 0.171261, acc.: 78.91%] [G loss: 0.601046]\n",
      "epoch:3 step:3221 [D loss: 0.180898, acc.: 71.88%] [G loss: 0.603989]\n",
      "epoch:3 step:3222 [D loss: 0.174141, acc.: 74.22%] [G loss: 0.596371]\n",
      "epoch:3 step:3223 [D loss: 0.183034, acc.: 73.44%] [G loss: 0.572618]\n",
      "epoch:3 step:3224 [D loss: 0.178198, acc.: 74.22%] [G loss: 0.584539]\n",
      "epoch:3 step:3225 [D loss: 0.199259, acc.: 69.53%] [G loss: 0.596642]\n",
      "epoch:3 step:3226 [D loss: 0.153805, acc.: 78.91%] [G loss: 0.663232]\n",
      "epoch:3 step:3227 [D loss: 0.199112, acc.: 68.75%] [G loss: 0.637829]\n",
      "epoch:3 step:3228 [D loss: 0.240888, acc.: 61.72%] [G loss: 0.599374]\n",
      "epoch:3 step:3229 [D loss: 0.204948, acc.: 69.53%] [G loss: 0.584196]\n",
      "epoch:3 step:3230 [D loss: 0.182700, acc.: 73.44%] [G loss: 0.603697]\n",
      "epoch:3 step:3231 [D loss: 0.181128, acc.: 72.66%] [G loss: 0.604589]\n",
      "epoch:3 step:3232 [D loss: 0.243886, acc.: 63.28%] [G loss: 0.528246]\n",
      "epoch:3 step:3233 [D loss: 0.214886, acc.: 61.72%] [G loss: 0.575593]\n",
      "epoch:3 step:3234 [D loss: 0.175838, acc.: 73.44%] [G loss: 0.547355]\n",
      "epoch:3 step:3235 [D loss: 0.219446, acc.: 69.53%] [G loss: 0.545843]\n",
      "epoch:3 step:3236 [D loss: 0.174032, acc.: 72.66%] [G loss: 0.640713]\n",
      "epoch:3 step:3237 [D loss: 0.143382, acc.: 78.91%] [G loss: 0.688892]\n",
      "epoch:3 step:3238 [D loss: 0.143348, acc.: 82.81%] [G loss: 0.718539]\n",
      "epoch:3 step:3239 [D loss: 0.138001, acc.: 83.59%] [G loss: 0.643841]\n",
      "epoch:3 step:3240 [D loss: 0.178026, acc.: 74.22%] [G loss: 0.593042]\n",
      "epoch:3 step:3241 [D loss: 0.150330, acc.: 80.47%] [G loss: 0.656765]\n",
      "epoch:3 step:3242 [D loss: 0.184894, acc.: 75.78%] [G loss: 0.609616]\n",
      "epoch:3 step:3243 [D loss: 0.197580, acc.: 63.28%] [G loss: 0.571193]\n",
      "epoch:3 step:3244 [D loss: 0.171643, acc.: 76.56%] [G loss: 0.596487]\n",
      "epoch:3 step:3245 [D loss: 0.192061, acc.: 69.53%] [G loss: 0.594517]\n",
      "epoch:3 step:3246 [D loss: 0.199623, acc.: 72.66%] [G loss: 0.593503]\n",
      "epoch:3 step:3247 [D loss: 0.189038, acc.: 69.53%] [G loss: 0.576994]\n",
      "epoch:3 step:3248 [D loss: 0.264736, acc.: 56.25%] [G loss: 0.551561]\n",
      "epoch:3 step:3249 [D loss: 0.172874, acc.: 77.34%] [G loss: 0.579981]\n",
      "epoch:3 step:3250 [D loss: 0.191500, acc.: 67.19%] [G loss: 0.593271]\n",
      "epoch:3 step:3251 [D loss: 0.182770, acc.: 73.44%] [G loss: 0.658193]\n",
      "epoch:3 step:3252 [D loss: 0.184488, acc.: 72.66%] [G loss: 0.559536]\n",
      "epoch:3 step:3253 [D loss: 0.185847, acc.: 71.88%] [G loss: 0.605258]\n",
      "epoch:3 step:3254 [D loss: 0.192660, acc.: 72.66%] [G loss: 0.562454]\n",
      "epoch:3 step:3255 [D loss: 0.206876, acc.: 71.09%] [G loss: 0.585472]\n",
      "epoch:3 step:3256 [D loss: 0.162628, acc.: 75.00%] [G loss: 0.624032]\n",
      "epoch:3 step:3257 [D loss: 0.146937, acc.: 81.25%] [G loss: 0.632016]\n",
      "epoch:3 step:3258 [D loss: 0.207150, acc.: 71.09%] [G loss: 0.597620]\n",
      "epoch:3 step:3259 [D loss: 0.224637, acc.: 58.59%] [G loss: 0.523291]\n",
      "epoch:3 step:3260 [D loss: 0.226612, acc.: 64.84%] [G loss: 0.567049]\n",
      "epoch:3 step:3261 [D loss: 0.158751, acc.: 74.22%] [G loss: 0.611357]\n",
      "epoch:3 step:3262 [D loss: 0.136463, acc.: 79.69%] [G loss: 0.668935]\n",
      "epoch:3 step:3263 [D loss: 0.180940, acc.: 76.56%] [G loss: 0.572421]\n",
      "epoch:3 step:3264 [D loss: 0.207174, acc.: 70.31%] [G loss: 0.559426]\n",
      "epoch:3 step:3265 [D loss: 0.216901, acc.: 65.62%] [G loss: 0.604435]\n",
      "epoch:3 step:3266 [D loss: 0.219073, acc.: 71.09%] [G loss: 0.586628]\n",
      "epoch:3 step:3267 [D loss: 0.193604, acc.: 72.66%] [G loss: 0.555930]\n",
      "epoch:3 step:3268 [D loss: 0.198012, acc.: 67.97%] [G loss: 0.583857]\n",
      "epoch:3 step:3269 [D loss: 0.239424, acc.: 68.75%] [G loss: 0.552082]\n",
      "epoch:3 step:3270 [D loss: 0.212115, acc.: 64.06%] [G loss: 0.548970]\n",
      "epoch:3 step:3271 [D loss: 0.180232, acc.: 74.22%] [G loss: 0.605824]\n",
      "epoch:3 step:3272 [D loss: 0.199334, acc.: 68.75%] [G loss: 0.584260]\n",
      "epoch:3 step:3273 [D loss: 0.177502, acc.: 75.78%] [G loss: 0.604906]\n",
      "epoch:3 step:3274 [D loss: 0.204961, acc.: 67.19%] [G loss: 0.560377]\n",
      "epoch:3 step:3275 [D loss: 0.172433, acc.: 80.47%] [G loss: 0.560498]\n",
      "epoch:3 step:3276 [D loss: 0.219627, acc.: 62.50%] [G loss: 0.547254]\n",
      "epoch:3 step:3277 [D loss: 0.174252, acc.: 77.34%] [G loss: 0.584782]\n",
      "epoch:3 step:3278 [D loss: 0.176015, acc.: 71.88%] [G loss: 0.621930]\n",
      "epoch:3 step:3279 [D loss: 0.214195, acc.: 65.62%] [G loss: 0.565331]\n",
      "epoch:3 step:3280 [D loss: 0.183142, acc.: 71.09%] [G loss: 0.608828]\n",
      "epoch:3 step:3281 [D loss: 0.218359, acc.: 62.50%] [G loss: 0.561928]\n",
      "epoch:3 step:3282 [D loss: 0.156535, acc.: 78.12%] [G loss: 0.630186]\n",
      "epoch:3 step:3283 [D loss: 0.156512, acc.: 76.56%] [G loss: 0.630143]\n",
      "epoch:3 step:3284 [D loss: 0.189796, acc.: 75.00%] [G loss: 0.576026]\n",
      "epoch:3 step:3285 [D loss: 0.157907, acc.: 80.47%] [G loss: 0.665429]\n",
      "epoch:3 step:3286 [D loss: 0.125197, acc.: 84.38%] [G loss: 0.684438]\n",
      "epoch:3 step:3287 [D loss: 0.214172, acc.: 67.97%] [G loss: 0.612504]\n",
      "epoch:3 step:3288 [D loss: 0.206908, acc.: 67.97%] [G loss: 0.530899]\n",
      "epoch:3 step:3289 [D loss: 0.193529, acc.: 71.09%] [G loss: 0.576523]\n",
      "epoch:3 step:3290 [D loss: 0.194839, acc.: 73.44%] [G loss: 0.542032]\n",
      "epoch:3 step:3291 [D loss: 0.222811, acc.: 67.19%] [G loss: 0.590562]\n",
      "epoch:3 step:3292 [D loss: 0.169507, acc.: 75.00%] [G loss: 0.603088]\n",
      "epoch:3 step:3293 [D loss: 0.225093, acc.: 61.72%] [G loss: 0.524893]\n",
      "epoch:3 step:3294 [D loss: 0.192315, acc.: 71.88%] [G loss: 0.520112]\n",
      "epoch:3 step:3295 [D loss: 0.174403, acc.: 71.88%] [G loss: 0.567785]\n",
      "epoch:3 step:3296 [D loss: 0.168525, acc.: 75.00%] [G loss: 0.581104]\n",
      "epoch:3 step:3297 [D loss: 0.209952, acc.: 71.88%] [G loss: 0.594454]\n",
      "epoch:3 step:3298 [D loss: 0.176392, acc.: 78.12%] [G loss: 0.561215]\n",
      "epoch:3 step:3299 [D loss: 0.175733, acc.: 70.31%] [G loss: 0.612705]\n",
      "epoch:3 step:3300 [D loss: 0.197629, acc.: 64.84%] [G loss: 0.603116]\n",
      "epoch:3 step:3301 [D loss: 0.189423, acc.: 69.53%] [G loss: 0.545623]\n",
      "epoch:3 step:3302 [D loss: 0.189143, acc.: 72.66%] [G loss: 0.606329]\n",
      "epoch:3 step:3303 [D loss: 0.181770, acc.: 75.00%] [G loss: 0.598213]\n",
      "epoch:3 step:3304 [D loss: 0.204896, acc.: 66.41%] [G loss: 0.529972]\n",
      "epoch:3 step:3305 [D loss: 0.163282, acc.: 78.91%] [G loss: 0.598329]\n",
      "epoch:3 step:3306 [D loss: 0.140656, acc.: 85.16%] [G loss: 0.646884]\n",
      "epoch:3 step:3307 [D loss: 0.197073, acc.: 68.75%] [G loss: 0.602215]\n",
      "epoch:3 step:3308 [D loss: 0.157424, acc.: 78.91%] [G loss: 0.646667]\n",
      "epoch:3 step:3309 [D loss: 0.143592, acc.: 82.03%] [G loss: 0.703659]\n",
      "epoch:3 step:3310 [D loss: 0.157547, acc.: 76.56%] [G loss: 0.720169]\n",
      "epoch:3 step:3311 [D loss: 0.293042, acc.: 53.91%] [G loss: 0.516195]\n",
      "epoch:3 step:3312 [D loss: 0.222817, acc.: 60.16%] [G loss: 0.518081]\n",
      "epoch:3 step:3313 [D loss: 0.206667, acc.: 62.50%] [G loss: 0.583650]\n",
      "epoch:3 step:3314 [D loss: 0.144541, acc.: 80.47%] [G loss: 0.666835]\n",
      "epoch:3 step:3315 [D loss: 0.152667, acc.: 81.25%] [G loss: 0.624512]\n",
      "epoch:3 step:3316 [D loss: 0.182029, acc.: 71.09%] [G loss: 0.625386]\n",
      "epoch:3 step:3317 [D loss: 0.225720, acc.: 65.62%] [G loss: 0.533481]\n",
      "epoch:3 step:3318 [D loss: 0.156903, acc.: 78.12%] [G loss: 0.606907]\n",
      "epoch:3 step:3319 [D loss: 0.135952, acc.: 80.47%] [G loss: 0.683418]\n",
      "epoch:3 step:3320 [D loss: 0.160363, acc.: 81.25%] [G loss: 0.630992]\n",
      "epoch:3 step:3321 [D loss: 0.190930, acc.: 75.00%] [G loss: 0.594363]\n",
      "epoch:3 step:3322 [D loss: 0.177190, acc.: 73.44%] [G loss: 0.602729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3323 [D loss: 0.179676, acc.: 76.56%] [G loss: 0.628160]\n",
      "epoch:3 step:3324 [D loss: 0.157949, acc.: 78.91%] [G loss: 0.598923]\n",
      "epoch:3 step:3325 [D loss: 0.153196, acc.: 76.56%] [G loss: 0.638429]\n",
      "epoch:3 step:3326 [D loss: 0.153678, acc.: 81.25%] [G loss: 0.644974]\n",
      "epoch:3 step:3327 [D loss: 0.166264, acc.: 75.00%] [G loss: 0.614152]\n",
      "epoch:3 step:3328 [D loss: 0.171561, acc.: 76.56%] [G loss: 0.615003]\n",
      "epoch:3 step:3329 [D loss: 0.173787, acc.: 78.91%] [G loss: 0.617456]\n",
      "epoch:3 step:3330 [D loss: 0.153841, acc.: 80.47%] [G loss: 0.590862]\n",
      "epoch:3 step:3331 [D loss: 0.152322, acc.: 81.25%] [G loss: 0.638157]\n",
      "epoch:3 step:3332 [D loss: 0.163998, acc.: 77.34%] [G loss: 0.590920]\n",
      "epoch:3 step:3333 [D loss: 0.156335, acc.: 78.12%] [G loss: 0.610652]\n",
      "epoch:3 step:3334 [D loss: 0.145712, acc.: 80.47%] [G loss: 0.624113]\n",
      "epoch:3 step:3335 [D loss: 0.172561, acc.: 74.22%] [G loss: 0.617316]\n",
      "epoch:3 step:3336 [D loss: 0.196780, acc.: 67.97%] [G loss: 0.604985]\n",
      "epoch:3 step:3337 [D loss: 0.178347, acc.: 77.34%] [G loss: 0.592941]\n",
      "epoch:3 step:3338 [D loss: 0.170461, acc.: 76.56%] [G loss: 0.614126]\n",
      "epoch:3 step:3339 [D loss: 0.205683, acc.: 71.88%] [G loss: 0.545660]\n",
      "epoch:3 step:3340 [D loss: 0.174124, acc.: 75.78%] [G loss: 0.580587]\n",
      "epoch:3 step:3341 [D loss: 0.156184, acc.: 82.81%] [G loss: 0.656527]\n",
      "epoch:3 step:3342 [D loss: 0.233477, acc.: 64.06%] [G loss: 0.538046]\n",
      "epoch:3 step:3343 [D loss: 0.169923, acc.: 74.22%] [G loss: 0.563598]\n",
      "epoch:3 step:3344 [D loss: 0.197041, acc.: 67.19%] [G loss: 0.598578]\n",
      "epoch:3 step:3345 [D loss: 0.141685, acc.: 82.03%] [G loss: 0.632788]\n",
      "epoch:3 step:3346 [D loss: 0.199098, acc.: 67.97%] [G loss: 0.557949]\n",
      "epoch:3 step:3347 [D loss: 0.191777, acc.: 67.97%] [G loss: 0.625332]\n",
      "epoch:3 step:3348 [D loss: 0.183867, acc.: 73.44%] [G loss: 0.637971]\n",
      "epoch:3 step:3349 [D loss: 0.206130, acc.: 71.09%] [G loss: 0.547773]\n",
      "epoch:3 step:3350 [D loss: 0.206887, acc.: 67.19%] [G loss: 0.542764]\n",
      "epoch:3 step:3351 [D loss: 0.154643, acc.: 84.38%] [G loss: 0.607766]\n",
      "epoch:3 step:3352 [D loss: 0.136990, acc.: 82.81%] [G loss: 0.603742]\n",
      "epoch:3 step:3353 [D loss: 0.216960, acc.: 66.41%] [G loss: 0.543010]\n",
      "epoch:3 step:3354 [D loss: 0.205270, acc.: 69.53%] [G loss: 0.539335]\n",
      "epoch:3 step:3355 [D loss: 0.188758, acc.: 76.56%] [G loss: 0.566454]\n",
      "epoch:3 step:3356 [D loss: 0.192221, acc.: 71.09%] [G loss: 0.576259]\n",
      "epoch:3 step:3357 [D loss: 0.176484, acc.: 72.66%] [G loss: 0.655813]\n",
      "epoch:3 step:3358 [D loss: 0.147159, acc.: 84.38%] [G loss: 0.614710]\n",
      "epoch:3 step:3359 [D loss: 0.173163, acc.: 75.78%] [G loss: 0.588260]\n",
      "epoch:3 step:3360 [D loss: 0.179575, acc.: 76.56%] [G loss: 0.623165]\n",
      "epoch:3 step:3361 [D loss: 0.171015, acc.: 73.44%] [G loss: 0.563953]\n",
      "epoch:3 step:3362 [D loss: 0.188633, acc.: 71.09%] [G loss: 0.599953]\n",
      "epoch:3 step:3363 [D loss: 0.132516, acc.: 88.28%] [G loss: 0.596814]\n",
      "epoch:3 step:3364 [D loss: 0.231518, acc.: 64.06%] [G loss: 0.510732]\n",
      "epoch:3 step:3365 [D loss: 0.153259, acc.: 80.47%] [G loss: 0.593626]\n",
      "epoch:3 step:3366 [D loss: 0.173522, acc.: 75.78%] [G loss: 0.600692]\n",
      "epoch:3 step:3367 [D loss: 0.160129, acc.: 80.47%] [G loss: 0.552997]\n",
      "epoch:3 step:3368 [D loss: 0.161752, acc.: 77.34%] [G loss: 0.582933]\n",
      "epoch:3 step:3369 [D loss: 0.147413, acc.: 79.69%] [G loss: 0.613789]\n",
      "epoch:3 step:3370 [D loss: 0.170688, acc.: 77.34%] [G loss: 0.605188]\n",
      "epoch:3 step:3371 [D loss: 0.199469, acc.: 70.31%] [G loss: 0.580675]\n",
      "epoch:3 step:3372 [D loss: 0.157076, acc.: 74.22%] [G loss: 0.581669]\n",
      "epoch:3 step:3373 [D loss: 0.195650, acc.: 67.19%] [G loss: 0.568407]\n",
      "epoch:3 step:3374 [D loss: 0.189747, acc.: 68.75%] [G loss: 0.583868]\n",
      "epoch:3 step:3375 [D loss: 0.173516, acc.: 79.69%] [G loss: 0.653864]\n",
      "epoch:3 step:3376 [D loss: 0.185118, acc.: 75.78%] [G loss: 0.644491]\n",
      "epoch:3 step:3377 [D loss: 0.198206, acc.: 70.31%] [G loss: 0.522047]\n",
      "epoch:3 step:3378 [D loss: 0.187235, acc.: 71.09%] [G loss: 0.573807]\n",
      "epoch:3 step:3379 [D loss: 0.193151, acc.: 71.09%] [G loss: 0.593157]\n",
      "epoch:3 step:3380 [D loss: 0.178215, acc.: 73.44%] [G loss: 0.562770]\n",
      "epoch:3 step:3381 [D loss: 0.180517, acc.: 71.88%] [G loss: 0.566317]\n",
      "epoch:3 step:3382 [D loss: 0.172683, acc.: 76.56%] [G loss: 0.602984]\n",
      "epoch:3 step:3383 [D loss: 0.197390, acc.: 67.19%] [G loss: 0.525569]\n",
      "epoch:3 step:3384 [D loss: 0.177848, acc.: 74.22%] [G loss: 0.577246]\n",
      "epoch:3 step:3385 [D loss: 0.183639, acc.: 74.22%] [G loss: 0.587436]\n",
      "epoch:3 step:3386 [D loss: 0.154009, acc.: 82.03%] [G loss: 0.643741]\n",
      "epoch:3 step:3387 [D loss: 0.209120, acc.: 62.50%] [G loss: 0.570676]\n",
      "epoch:3 step:3388 [D loss: 0.184719, acc.: 76.56%] [G loss: 0.586692]\n",
      "epoch:3 step:3389 [D loss: 0.173979, acc.: 78.12%] [G loss: 0.566213]\n",
      "epoch:3 step:3390 [D loss: 0.207653, acc.: 65.62%] [G loss: 0.587674]\n",
      "epoch:3 step:3391 [D loss: 0.186331, acc.: 73.44%] [G loss: 0.593009]\n",
      "epoch:3 step:3392 [D loss: 0.153403, acc.: 78.91%] [G loss: 0.615088]\n",
      "epoch:3 step:3393 [D loss: 0.174222, acc.: 76.56%] [G loss: 0.595795]\n",
      "epoch:3 step:3394 [D loss: 0.194293, acc.: 67.97%] [G loss: 0.597068]\n",
      "epoch:3 step:3395 [D loss: 0.187217, acc.: 71.09%] [G loss: 0.609595]\n",
      "epoch:3 step:3396 [D loss: 0.185556, acc.: 70.31%] [G loss: 0.535667]\n",
      "epoch:3 step:3397 [D loss: 0.188874, acc.: 73.44%] [G loss: 0.564953]\n",
      "epoch:3 step:3398 [D loss: 0.175129, acc.: 77.34%] [G loss: 0.648112]\n",
      "epoch:3 step:3399 [D loss: 0.185177, acc.: 70.31%] [G loss: 0.682449]\n",
      "epoch:3 step:3400 [D loss: 0.136843, acc.: 85.16%] [G loss: 0.623179]\n",
      "##############\n",
      "[3.43855593 1.54245295 6.82624666 5.38828744 4.55052261 6.20007511\n",
      " 5.49501463 5.04326128 5.33396828 4.06220319]\n",
      "##########\n",
      "epoch:3 step:3401 [D loss: 0.216455, acc.: 68.75%] [G loss: 0.532478]\n",
      "epoch:3 step:3402 [D loss: 0.177501, acc.: 78.12%] [G loss: 0.553773]\n",
      "epoch:3 step:3403 [D loss: 0.142103, acc.: 78.91%] [G loss: 0.582741]\n",
      "epoch:3 step:3404 [D loss: 0.209048, acc.: 64.06%] [G loss: 0.588755]\n",
      "epoch:3 step:3405 [D loss: 0.189636, acc.: 74.22%] [G loss: 0.549866]\n",
      "epoch:3 step:3406 [D loss: 0.178847, acc.: 75.00%] [G loss: 0.606687]\n",
      "epoch:3 step:3407 [D loss: 0.198880, acc.: 71.88%] [G loss: 0.604348]\n",
      "epoch:3 step:3408 [D loss: 0.182152, acc.: 76.56%] [G loss: 0.602270]\n",
      "epoch:3 step:3409 [D loss: 0.177426, acc.: 74.22%] [G loss: 0.611116]\n",
      "epoch:3 step:3410 [D loss: 0.173164, acc.: 76.56%] [G loss: 0.583418]\n",
      "epoch:3 step:3411 [D loss: 0.238908, acc.: 61.72%] [G loss: 0.508592]\n",
      "epoch:3 step:3412 [D loss: 0.211721, acc.: 63.28%] [G loss: 0.536539]\n",
      "epoch:3 step:3413 [D loss: 0.165166, acc.: 77.34%] [G loss: 0.611879]\n",
      "epoch:3 step:3414 [D loss: 0.184280, acc.: 75.00%] [G loss: 0.621658]\n",
      "epoch:3 step:3415 [D loss: 0.217362, acc.: 64.06%] [G loss: 0.582565]\n",
      "epoch:3 step:3416 [D loss: 0.190331, acc.: 71.88%] [G loss: 0.567534]\n",
      "epoch:3 step:3417 [D loss: 0.172059, acc.: 77.34%] [G loss: 0.538826]\n",
      "epoch:3 step:3418 [D loss: 0.191459, acc.: 67.19%] [G loss: 0.518161]\n",
      "epoch:3 step:3419 [D loss: 0.143664, acc.: 83.59%] [G loss: 0.611633]\n",
      "epoch:3 step:3420 [D loss: 0.149978, acc.: 79.69%] [G loss: 0.661460]\n",
      "epoch:3 step:3421 [D loss: 0.162856, acc.: 81.25%] [G loss: 0.605979]\n",
      "epoch:3 step:3422 [D loss: 0.200321, acc.: 67.97%] [G loss: 0.573250]\n",
      "epoch:3 step:3423 [D loss: 0.157526, acc.: 74.22%] [G loss: 0.616257]\n",
      "epoch:3 step:3424 [D loss: 0.175852, acc.: 71.09%] [G loss: 0.614492]\n",
      "epoch:3 step:3425 [D loss: 0.175781, acc.: 73.44%] [G loss: 0.552314]\n",
      "epoch:3 step:3426 [D loss: 0.189764, acc.: 71.88%] [G loss: 0.535562]\n",
      "epoch:3 step:3427 [D loss: 0.191907, acc.: 70.31%] [G loss: 0.558641]\n",
      "epoch:3 step:3428 [D loss: 0.176476, acc.: 75.78%] [G loss: 0.642555]\n",
      "epoch:3 step:3429 [D loss: 0.217636, acc.: 69.53%] [G loss: 0.542938]\n",
      "epoch:3 step:3430 [D loss: 0.180596, acc.: 75.78%] [G loss: 0.598164]\n",
      "epoch:3 step:3431 [D loss: 0.172040, acc.: 75.00%] [G loss: 0.570225]\n",
      "epoch:3 step:3432 [D loss: 0.173174, acc.: 77.34%] [G loss: 0.558232]\n",
      "epoch:3 step:3433 [D loss: 0.244361, acc.: 56.25%] [G loss: 0.552032]\n",
      "epoch:3 step:3434 [D loss: 0.176277, acc.: 73.44%] [G loss: 0.589769]\n",
      "epoch:3 step:3435 [D loss: 0.173347, acc.: 77.34%] [G loss: 0.595966]\n",
      "epoch:3 step:3436 [D loss: 0.240137, acc.: 59.38%] [G loss: 0.504520]\n",
      "epoch:3 step:3437 [D loss: 0.200312, acc.: 68.75%] [G loss: 0.547049]\n",
      "epoch:3 step:3438 [D loss: 0.191614, acc.: 71.09%] [G loss: 0.580647]\n",
      "epoch:3 step:3439 [D loss: 0.192702, acc.: 74.22%] [G loss: 0.570561]\n",
      "epoch:3 step:3440 [D loss: 0.182643, acc.: 72.66%] [G loss: 0.551661]\n",
      "epoch:3 step:3441 [D loss: 0.182712, acc.: 72.66%] [G loss: 0.535352]\n",
      "epoch:3 step:3442 [D loss: 0.157111, acc.: 78.91%] [G loss: 0.565813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3443 [D loss: 0.153079, acc.: 78.91%] [G loss: 0.635400]\n",
      "epoch:3 step:3444 [D loss: 0.152053, acc.: 82.03%] [G loss: 0.612867]\n",
      "epoch:3 step:3445 [D loss: 0.176492, acc.: 74.22%] [G loss: 0.609350]\n",
      "epoch:3 step:3446 [D loss: 0.157817, acc.: 75.78%] [G loss: 0.597605]\n",
      "epoch:3 step:3447 [D loss: 0.190248, acc.: 67.97%] [G loss: 0.571539]\n",
      "epoch:3 step:3448 [D loss: 0.165358, acc.: 75.00%] [G loss: 0.602069]\n",
      "epoch:3 step:3449 [D loss: 0.170753, acc.: 74.22%] [G loss: 0.622602]\n",
      "epoch:3 step:3450 [D loss: 0.136777, acc.: 82.81%] [G loss: 0.654521]\n",
      "epoch:3 step:3451 [D loss: 0.166540, acc.: 72.66%] [G loss: 0.619662]\n",
      "epoch:3 step:3452 [D loss: 0.163425, acc.: 78.91%] [G loss: 0.653327]\n",
      "epoch:3 step:3453 [D loss: 0.133995, acc.: 83.59%] [G loss: 0.667653]\n",
      "epoch:3 step:3454 [D loss: 0.203551, acc.: 71.09%] [G loss: 0.581805]\n",
      "epoch:3 step:3455 [D loss: 0.167442, acc.: 78.91%] [G loss: 0.613641]\n",
      "epoch:3 step:3456 [D loss: 0.189285, acc.: 71.88%] [G loss: 0.576593]\n",
      "epoch:3 step:3457 [D loss: 0.177269, acc.: 71.09%] [G loss: 0.582416]\n",
      "epoch:3 step:3458 [D loss: 0.165851, acc.: 77.34%] [G loss: 0.630008]\n",
      "epoch:3 step:3459 [D loss: 0.151136, acc.: 78.91%] [G loss: 0.694216]\n",
      "epoch:3 step:3460 [D loss: 0.161596, acc.: 75.78%] [G loss: 0.572648]\n",
      "epoch:3 step:3461 [D loss: 0.145547, acc.: 82.03%] [G loss: 0.644247]\n",
      "epoch:3 step:3462 [D loss: 0.187289, acc.: 71.88%] [G loss: 0.580008]\n",
      "epoch:3 step:3463 [D loss: 0.188141, acc.: 71.09%] [G loss: 0.619303]\n",
      "epoch:3 step:3464 [D loss: 0.191963, acc.: 71.09%] [G loss: 0.604298]\n",
      "epoch:3 step:3465 [D loss: 0.177317, acc.: 75.00%] [G loss: 0.618429]\n",
      "epoch:3 step:3466 [D loss: 0.226426, acc.: 67.19%] [G loss: 0.545352]\n",
      "epoch:3 step:3467 [D loss: 0.200575, acc.: 69.53%] [G loss: 0.576871]\n",
      "epoch:3 step:3468 [D loss: 0.165882, acc.: 76.56%] [G loss: 0.585412]\n",
      "epoch:3 step:3469 [D loss: 0.176250, acc.: 75.00%] [G loss: 0.613682]\n",
      "epoch:3 step:3470 [D loss: 0.183459, acc.: 72.66%] [G loss: 0.509591]\n",
      "epoch:3 step:3471 [D loss: 0.151442, acc.: 78.91%] [G loss: 0.644059]\n",
      "epoch:3 step:3472 [D loss: 0.161365, acc.: 73.44%] [G loss: 0.606873]\n",
      "epoch:3 step:3473 [D loss: 0.197404, acc.: 69.53%] [G loss: 0.581427]\n",
      "epoch:3 step:3474 [D loss: 0.161340, acc.: 75.78%] [G loss: 0.582480]\n",
      "epoch:3 step:3475 [D loss: 0.198757, acc.: 70.31%] [G loss: 0.606605]\n",
      "epoch:3 step:3476 [D loss: 0.170598, acc.: 75.00%] [G loss: 0.628539]\n",
      "epoch:3 step:3477 [D loss: 0.188810, acc.: 73.44%] [G loss: 0.527920]\n",
      "epoch:3 step:3478 [D loss: 0.189569, acc.: 68.75%] [G loss: 0.534663]\n",
      "epoch:3 step:3479 [D loss: 0.224551, acc.: 64.84%] [G loss: 0.571576]\n",
      "epoch:3 step:3480 [D loss: 0.174172, acc.: 77.34%] [G loss: 0.663846]\n",
      "epoch:3 step:3481 [D loss: 0.195179, acc.: 73.44%] [G loss: 0.563829]\n",
      "epoch:3 step:3482 [D loss: 0.214088, acc.: 70.31%] [G loss: 0.579982]\n",
      "epoch:3 step:3483 [D loss: 0.214583, acc.: 66.41%] [G loss: 0.504345]\n",
      "epoch:3 step:3484 [D loss: 0.201331, acc.: 68.75%] [G loss: 0.554651]\n",
      "epoch:3 step:3485 [D loss: 0.184206, acc.: 71.09%] [G loss: 0.621680]\n",
      "epoch:3 step:3486 [D loss: 0.186932, acc.: 74.22%] [G loss: 0.657664]\n",
      "epoch:3 step:3487 [D loss: 0.185282, acc.: 69.53%] [G loss: 0.589681]\n",
      "epoch:3 step:3488 [D loss: 0.136144, acc.: 84.38%] [G loss: 0.638351]\n",
      "epoch:3 step:3489 [D loss: 0.181938, acc.: 75.78%] [G loss: 0.603112]\n",
      "epoch:3 step:3490 [D loss: 0.175145, acc.: 78.91%] [G loss: 0.607365]\n",
      "epoch:3 step:3491 [D loss: 0.158983, acc.: 76.56%] [G loss: 0.608019]\n",
      "epoch:3 step:3492 [D loss: 0.178679, acc.: 75.00%] [G loss: 0.598385]\n",
      "epoch:3 step:3493 [D loss: 0.180883, acc.: 73.44%] [G loss: 0.551371]\n",
      "epoch:3 step:3494 [D loss: 0.150375, acc.: 75.78%] [G loss: 0.596958]\n",
      "epoch:3 step:3495 [D loss: 0.178721, acc.: 69.53%] [G loss: 0.585991]\n",
      "epoch:3 step:3496 [D loss: 0.178522, acc.: 74.22%] [G loss: 0.597703]\n",
      "epoch:3 step:3497 [D loss: 0.210300, acc.: 67.19%] [G loss: 0.527393]\n",
      "epoch:3 step:3498 [D loss: 0.193297, acc.: 75.00%] [G loss: 0.515604]\n",
      "epoch:3 step:3499 [D loss: 0.187468, acc.: 69.53%] [G loss: 0.535940]\n",
      "epoch:3 step:3500 [D loss: 0.173909, acc.: 75.00%] [G loss: 0.627756]\n",
      "epoch:3 step:3501 [D loss: 0.134853, acc.: 83.59%] [G loss: 0.716618]\n",
      "epoch:3 step:3502 [D loss: 0.171253, acc.: 72.66%] [G loss: 0.574474]\n",
      "epoch:3 step:3503 [D loss: 0.170940, acc.: 70.31%] [G loss: 0.543770]\n",
      "epoch:3 step:3504 [D loss: 0.177906, acc.: 74.22%] [G loss: 0.634829]\n",
      "epoch:3 step:3505 [D loss: 0.162711, acc.: 78.12%] [G loss: 0.648868]\n",
      "epoch:3 step:3506 [D loss: 0.185634, acc.: 70.31%] [G loss: 0.609088]\n",
      "epoch:3 step:3507 [D loss: 0.205548, acc.: 67.97%] [G loss: 0.600089]\n",
      "epoch:3 step:3508 [D loss: 0.160956, acc.: 78.91%] [G loss: 0.622950]\n",
      "epoch:3 step:3509 [D loss: 0.170966, acc.: 71.09%] [G loss: 0.614217]\n",
      "epoch:3 step:3510 [D loss: 0.145042, acc.: 79.69%] [G loss: 0.617183]\n",
      "epoch:3 step:3511 [D loss: 0.182959, acc.: 75.00%] [G loss: 0.637003]\n",
      "epoch:3 step:3512 [D loss: 0.192330, acc.: 71.88%] [G loss: 0.599663]\n",
      "epoch:3 step:3513 [D loss: 0.211080, acc.: 64.84%] [G loss: 0.580059]\n",
      "epoch:3 step:3514 [D loss: 0.203923, acc.: 70.31%] [G loss: 0.554279]\n",
      "epoch:3 step:3515 [D loss: 0.201198, acc.: 66.41%] [G loss: 0.564283]\n",
      "epoch:3 step:3516 [D loss: 0.179681, acc.: 71.09%] [G loss: 0.543705]\n",
      "epoch:3 step:3517 [D loss: 0.151877, acc.: 80.47%] [G loss: 0.641217]\n",
      "epoch:3 step:3518 [D loss: 0.140490, acc.: 82.81%] [G loss: 0.661926]\n",
      "epoch:3 step:3519 [D loss: 0.184200, acc.: 71.88%] [G loss: 0.574774]\n",
      "epoch:3 step:3520 [D loss: 0.162823, acc.: 78.91%] [G loss: 0.632052]\n",
      "epoch:3 step:3521 [D loss: 0.238061, acc.: 60.94%] [G loss: 0.568201]\n",
      "epoch:3 step:3522 [D loss: 0.205050, acc.: 63.28%] [G loss: 0.591719]\n",
      "epoch:3 step:3523 [D loss: 0.199408, acc.: 70.31%] [G loss: 0.573316]\n",
      "epoch:3 step:3524 [D loss: 0.197030, acc.: 70.31%] [G loss: 0.544680]\n",
      "epoch:3 step:3525 [D loss: 0.144004, acc.: 78.12%] [G loss: 0.612681]\n",
      "epoch:3 step:3526 [D loss: 0.196387, acc.: 72.66%] [G loss: 0.593790]\n",
      "epoch:3 step:3527 [D loss: 0.184415, acc.: 73.44%] [G loss: 0.559165]\n",
      "epoch:3 step:3528 [D loss: 0.183363, acc.: 71.09%] [G loss: 0.576461]\n",
      "epoch:3 step:3529 [D loss: 0.219867, acc.: 67.19%] [G loss: 0.507847]\n",
      "epoch:3 step:3530 [D loss: 0.170000, acc.: 76.56%] [G loss: 0.610455]\n",
      "epoch:3 step:3531 [D loss: 0.214202, acc.: 68.75%] [G loss: 0.590149]\n",
      "epoch:3 step:3532 [D loss: 0.207984, acc.: 66.41%] [G loss: 0.576130]\n",
      "epoch:3 step:3533 [D loss: 0.195475, acc.: 70.31%] [G loss: 0.541331]\n",
      "epoch:3 step:3534 [D loss: 0.214764, acc.: 63.28%] [G loss: 0.574994]\n",
      "epoch:3 step:3535 [D loss: 0.175447, acc.: 72.66%] [G loss: 0.589292]\n",
      "epoch:3 step:3536 [D loss: 0.186524, acc.: 71.88%] [G loss: 0.596207]\n",
      "epoch:3 step:3537 [D loss: 0.205182, acc.: 69.53%] [G loss: 0.549377]\n",
      "epoch:3 step:3538 [D loss: 0.213764, acc.: 66.41%] [G loss: 0.536360]\n",
      "epoch:3 step:3539 [D loss: 0.138455, acc.: 81.25%] [G loss: 0.593504]\n",
      "epoch:3 step:3540 [D loss: 0.170152, acc.: 71.09%] [G loss: 0.615832]\n",
      "epoch:3 step:3541 [D loss: 0.159898, acc.: 77.34%] [G loss: 0.624664]\n",
      "epoch:3 step:3542 [D loss: 0.158454, acc.: 80.47%] [G loss: 0.582618]\n",
      "epoch:3 step:3543 [D loss: 0.162709, acc.: 80.47%] [G loss: 0.588827]\n",
      "epoch:3 step:3544 [D loss: 0.212799, acc.: 70.31%] [G loss: 0.578809]\n",
      "epoch:3 step:3545 [D loss: 0.224369, acc.: 64.84%] [G loss: 0.523794]\n",
      "epoch:3 step:3546 [D loss: 0.176230, acc.: 78.91%] [G loss: 0.591586]\n",
      "epoch:3 step:3547 [D loss: 0.155355, acc.: 78.12%] [G loss: 0.635705]\n",
      "epoch:3 step:3548 [D loss: 0.158031, acc.: 76.56%] [G loss: 0.610552]\n",
      "epoch:3 step:3549 [D loss: 0.212845, acc.: 67.19%] [G loss: 0.530918]\n",
      "epoch:3 step:3550 [D loss: 0.223801, acc.: 64.84%] [G loss: 0.549000]\n",
      "epoch:3 step:3551 [D loss: 0.190403, acc.: 71.09%] [G loss: 0.573844]\n",
      "epoch:3 step:3552 [D loss: 0.174890, acc.: 75.00%] [G loss: 0.598412]\n",
      "epoch:3 step:3553 [D loss: 0.200570, acc.: 69.53%] [G loss: 0.556127]\n",
      "epoch:3 step:3554 [D loss: 0.152710, acc.: 80.47%] [G loss: 0.594785]\n",
      "epoch:3 step:3555 [D loss: 0.184961, acc.: 70.31%] [G loss: 0.603297]\n",
      "epoch:3 step:3556 [D loss: 0.171599, acc.: 73.44%] [G loss: 0.608204]\n",
      "epoch:3 step:3557 [D loss: 0.129997, acc.: 81.25%] [G loss: 0.644795]\n",
      "epoch:3 step:3558 [D loss: 0.147862, acc.: 81.25%] [G loss: 0.657957]\n",
      "epoch:3 step:3559 [D loss: 0.179643, acc.: 71.88%] [G loss: 0.588352]\n",
      "epoch:3 step:3560 [D loss: 0.180383, acc.: 75.00%] [G loss: 0.583238]\n",
      "epoch:3 step:3561 [D loss: 0.169246, acc.: 72.66%] [G loss: 0.600705]\n",
      "epoch:3 step:3562 [D loss: 0.163129, acc.: 77.34%] [G loss: 0.627459]\n",
      "epoch:3 step:3563 [D loss: 0.172580, acc.: 75.00%] [G loss: 0.584490]\n",
      "epoch:3 step:3564 [D loss: 0.141291, acc.: 79.69%] [G loss: 0.603797]\n",
      "epoch:3 step:3565 [D loss: 0.154941, acc.: 78.91%] [G loss: 0.622063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3566 [D loss: 0.149539, acc.: 78.91%] [G loss: 0.659631]\n",
      "epoch:3 step:3567 [D loss: 0.188189, acc.: 75.78%] [G loss: 0.563474]\n",
      "epoch:3 step:3568 [D loss: 0.166860, acc.: 72.66%] [G loss: 0.646372]\n",
      "epoch:3 step:3569 [D loss: 0.142485, acc.: 80.47%] [G loss: 0.599711]\n",
      "epoch:3 step:3570 [D loss: 0.184511, acc.: 75.00%] [G loss: 0.573651]\n",
      "epoch:3 step:3571 [D loss: 0.180186, acc.: 73.44%] [G loss: 0.615162]\n",
      "epoch:3 step:3572 [D loss: 0.173880, acc.: 75.00%] [G loss: 0.631510]\n",
      "epoch:3 step:3573 [D loss: 0.192737, acc.: 75.00%] [G loss: 0.571523]\n",
      "epoch:3 step:3574 [D loss: 0.159884, acc.: 78.12%] [G loss: 0.618653]\n",
      "epoch:3 step:3575 [D loss: 0.163737, acc.: 78.12%] [G loss: 0.611884]\n",
      "epoch:3 step:3576 [D loss: 0.247856, acc.: 62.50%] [G loss: 0.606083]\n",
      "epoch:3 step:3577 [D loss: 0.204336, acc.: 72.66%] [G loss: 0.587265]\n",
      "epoch:3 step:3578 [D loss: 0.171768, acc.: 74.22%] [G loss: 0.616044]\n",
      "epoch:3 step:3579 [D loss: 0.187826, acc.: 76.56%] [G loss: 0.620479]\n",
      "epoch:3 step:3580 [D loss: 0.163335, acc.: 75.78%] [G loss: 0.632687]\n",
      "epoch:3 step:3581 [D loss: 0.169948, acc.: 72.66%] [G loss: 0.647055]\n",
      "epoch:3 step:3582 [D loss: 0.199784, acc.: 70.31%] [G loss: 0.559944]\n",
      "epoch:3 step:3583 [D loss: 0.144668, acc.: 80.47%] [G loss: 0.683200]\n",
      "epoch:3 step:3584 [D loss: 0.208569, acc.: 67.19%] [G loss: 0.618461]\n",
      "epoch:3 step:3585 [D loss: 0.211463, acc.: 64.06%] [G loss: 0.548343]\n",
      "epoch:3 step:3586 [D loss: 0.161159, acc.: 77.34%] [G loss: 0.592396]\n",
      "epoch:3 step:3587 [D loss: 0.192893, acc.: 72.66%] [G loss: 0.609481]\n",
      "epoch:3 step:3588 [D loss: 0.188599, acc.: 70.31%] [G loss: 0.521436]\n",
      "epoch:3 step:3589 [D loss: 0.174857, acc.: 76.56%] [G loss: 0.550784]\n",
      "epoch:3 step:3590 [D loss: 0.178189, acc.: 78.12%] [G loss: 0.558321]\n",
      "epoch:3 step:3591 [D loss: 0.137795, acc.: 82.03%] [G loss: 0.638319]\n",
      "epoch:3 step:3592 [D loss: 0.133352, acc.: 85.94%] [G loss: 0.673710]\n",
      "epoch:3 step:3593 [D loss: 0.139151, acc.: 82.81%] [G loss: 0.707226]\n",
      "epoch:3 step:3594 [D loss: 0.193089, acc.: 69.53%] [G loss: 0.575170]\n",
      "epoch:3 step:3595 [D loss: 0.212071, acc.: 71.88%] [G loss: 0.546716]\n",
      "epoch:3 step:3596 [D loss: 0.181567, acc.: 73.44%] [G loss: 0.551332]\n",
      "epoch:3 step:3597 [D loss: 0.144962, acc.: 78.12%] [G loss: 0.592809]\n",
      "epoch:3 step:3598 [D loss: 0.218253, acc.: 67.97%] [G loss: 0.555744]\n",
      "epoch:3 step:3599 [D loss: 0.215856, acc.: 64.06%] [G loss: 0.545032]\n",
      "epoch:3 step:3600 [D loss: 0.212612, acc.: 65.62%] [G loss: 0.542269]\n",
      "##############\n",
      "[3.28847115 1.34045818 7.03223837 5.2946542  4.33904157 6.55545596\n",
      " 5.11742151 4.96247268 5.45493045 3.85024398]\n",
      "##########\n",
      "epoch:3 step:3601 [D loss: 0.185378, acc.: 75.78%] [G loss: 0.632979]\n",
      "epoch:3 step:3602 [D loss: 0.203858, acc.: 74.22%] [G loss: 0.570209]\n",
      "epoch:3 step:3603 [D loss: 0.138240, acc.: 85.94%] [G loss: 0.638245]\n",
      "epoch:3 step:3604 [D loss: 0.195557, acc.: 71.88%] [G loss: 0.637377]\n",
      "epoch:3 step:3605 [D loss: 0.191949, acc.: 70.31%] [G loss: 0.570399]\n",
      "epoch:3 step:3606 [D loss: 0.164617, acc.: 74.22%] [G loss: 0.626370]\n",
      "epoch:3 step:3607 [D loss: 0.141139, acc.: 85.16%] [G loss: 0.623715]\n",
      "epoch:3 step:3608 [D loss: 0.191658, acc.: 66.41%] [G loss: 0.604892]\n",
      "epoch:3 step:3609 [D loss: 0.145328, acc.: 78.91%] [G loss: 0.596918]\n",
      "epoch:3 step:3610 [D loss: 0.209463, acc.: 69.53%] [G loss: 0.563976]\n",
      "epoch:3 step:3611 [D loss: 0.185163, acc.: 75.00%] [G loss: 0.578790]\n",
      "epoch:3 step:3612 [D loss: 0.156770, acc.: 79.69%] [G loss: 0.631630]\n",
      "epoch:3 step:3613 [D loss: 0.141756, acc.: 85.16%] [G loss: 0.620145]\n",
      "epoch:3 step:3614 [D loss: 0.161783, acc.: 77.34%] [G loss: 0.621466]\n",
      "epoch:3 step:3615 [D loss: 0.160701, acc.: 76.56%] [G loss: 0.582344]\n",
      "epoch:3 step:3616 [D loss: 0.140697, acc.: 83.59%] [G loss: 0.685380]\n",
      "epoch:3 step:3617 [D loss: 0.170578, acc.: 73.44%] [G loss: 0.617879]\n",
      "epoch:3 step:3618 [D loss: 0.176965, acc.: 69.53%] [G loss: 0.623229]\n",
      "epoch:3 step:3619 [D loss: 0.190892, acc.: 74.22%] [G loss: 0.599962]\n",
      "epoch:3 step:3620 [D loss: 0.199452, acc.: 67.19%] [G loss: 0.561516]\n",
      "epoch:3 step:3621 [D loss: 0.177250, acc.: 76.56%] [G loss: 0.523080]\n",
      "epoch:3 step:3622 [D loss: 0.192101, acc.: 72.66%] [G loss: 0.584022]\n",
      "epoch:3 step:3623 [D loss: 0.196632, acc.: 71.09%] [G loss: 0.529483]\n",
      "epoch:3 step:3624 [D loss: 0.167669, acc.: 78.91%] [G loss: 0.563794]\n",
      "epoch:3 step:3625 [D loss: 0.177448, acc.: 74.22%] [G loss: 0.623014]\n",
      "epoch:3 step:3626 [D loss: 0.225122, acc.: 64.06%] [G loss: 0.540988]\n",
      "epoch:3 step:3627 [D loss: 0.165973, acc.: 72.66%] [G loss: 0.687539]\n",
      "epoch:3 step:3628 [D loss: 0.233465, acc.: 60.94%] [G loss: 0.604480]\n",
      "epoch:3 step:3629 [D loss: 0.205549, acc.: 69.53%] [G loss: 0.552509]\n",
      "epoch:3 step:3630 [D loss: 0.161385, acc.: 82.81%] [G loss: 0.592800]\n",
      "epoch:3 step:3631 [D loss: 0.184216, acc.: 78.12%] [G loss: 0.572859]\n",
      "epoch:3 step:3632 [D loss: 0.184018, acc.: 75.00%] [G loss: 0.577662]\n",
      "epoch:3 step:3633 [D loss: 0.148516, acc.: 82.81%] [G loss: 0.626992]\n",
      "epoch:3 step:3634 [D loss: 0.170504, acc.: 75.78%] [G loss: 0.615364]\n",
      "epoch:3 step:3635 [D loss: 0.209507, acc.: 67.19%] [G loss: 0.549345]\n",
      "epoch:3 step:3636 [D loss: 0.191555, acc.: 67.97%] [G loss: 0.522802]\n",
      "epoch:3 step:3637 [D loss: 0.170545, acc.: 75.00%] [G loss: 0.596510]\n",
      "epoch:3 step:3638 [D loss: 0.220502, acc.: 63.28%] [G loss: 0.537295]\n",
      "epoch:3 step:3639 [D loss: 0.217182, acc.: 64.84%] [G loss: 0.560456]\n",
      "epoch:3 step:3640 [D loss: 0.186236, acc.: 73.44%] [G loss: 0.595991]\n",
      "epoch:3 step:3641 [D loss: 0.192275, acc.: 72.66%] [G loss: 0.619945]\n",
      "epoch:3 step:3642 [D loss: 0.183690, acc.: 71.88%] [G loss: 0.536988]\n",
      "epoch:3 step:3643 [D loss: 0.167763, acc.: 78.12%] [G loss: 0.605640]\n",
      "epoch:3 step:3644 [D loss: 0.170577, acc.: 76.56%] [G loss: 0.596876]\n",
      "epoch:3 step:3645 [D loss: 0.149439, acc.: 79.69%] [G loss: 0.612883]\n",
      "epoch:3 step:3646 [D loss: 0.147054, acc.: 82.03%] [G loss: 0.623892]\n",
      "epoch:3 step:3647 [D loss: 0.172769, acc.: 73.44%] [G loss: 0.622176]\n",
      "epoch:3 step:3648 [D loss: 0.179826, acc.: 71.09%] [G loss: 0.649822]\n",
      "epoch:3 step:3649 [D loss: 0.144629, acc.: 82.81%] [G loss: 0.663491]\n",
      "epoch:3 step:3650 [D loss: 0.218552, acc.: 62.50%] [G loss: 0.556321]\n",
      "epoch:3 step:3651 [D loss: 0.198863, acc.: 68.75%] [G loss: 0.532628]\n",
      "epoch:3 step:3652 [D loss: 0.190247, acc.: 72.66%] [G loss: 0.525821]\n",
      "epoch:3 step:3653 [D loss: 0.170362, acc.: 76.56%] [G loss: 0.600610]\n",
      "epoch:3 step:3654 [D loss: 0.202119, acc.: 71.09%] [G loss: 0.563538]\n",
      "epoch:3 step:3655 [D loss: 0.186335, acc.: 75.00%] [G loss: 0.633117]\n",
      "epoch:3 step:3656 [D loss: 0.225802, acc.: 62.50%] [G loss: 0.563481]\n",
      "epoch:3 step:3657 [D loss: 0.179836, acc.: 69.53%] [G loss: 0.574734]\n",
      "epoch:3 step:3658 [D loss: 0.196107, acc.: 69.53%] [G loss: 0.526663]\n",
      "epoch:3 step:3659 [D loss: 0.179146, acc.: 74.22%] [G loss: 0.583457]\n",
      "epoch:3 step:3660 [D loss: 0.178792, acc.: 78.91%] [G loss: 0.613800]\n",
      "epoch:3 step:3661 [D loss: 0.200293, acc.: 72.66%] [G loss: 0.582320]\n",
      "epoch:3 step:3662 [D loss: 0.176700, acc.: 75.00%] [G loss: 0.591195]\n",
      "epoch:3 step:3663 [D loss: 0.149857, acc.: 81.25%] [G loss: 0.663578]\n",
      "epoch:3 step:3664 [D loss: 0.177314, acc.: 71.09%] [G loss: 0.629764]\n",
      "epoch:3 step:3665 [D loss: 0.151980, acc.: 82.81%] [G loss: 0.592963]\n",
      "epoch:3 step:3666 [D loss: 0.184333, acc.: 73.44%] [G loss: 0.497477]\n",
      "epoch:3 step:3667 [D loss: 0.192818, acc.: 71.88%] [G loss: 0.541332]\n",
      "epoch:3 step:3668 [D loss: 0.169098, acc.: 72.66%] [G loss: 0.590175]\n",
      "epoch:3 step:3669 [D loss: 0.277944, acc.: 55.47%] [G loss: 0.557022]\n",
      "epoch:3 step:3670 [D loss: 0.178021, acc.: 73.44%] [G loss: 0.601499]\n",
      "epoch:3 step:3671 [D loss: 0.153341, acc.: 80.47%] [G loss: 0.679897]\n",
      "epoch:3 step:3672 [D loss: 0.245610, acc.: 61.72%] [G loss: 0.522533]\n",
      "epoch:3 step:3673 [D loss: 0.196052, acc.: 67.97%] [G loss: 0.567810]\n",
      "epoch:3 step:3674 [D loss: 0.149453, acc.: 81.25%] [G loss: 0.605804]\n",
      "epoch:3 step:3675 [D loss: 0.183009, acc.: 71.88%] [G loss: 0.563960]\n",
      "epoch:3 step:3676 [D loss: 0.183253, acc.: 73.44%] [G loss: 0.543600]\n",
      "epoch:3 step:3677 [D loss: 0.199522, acc.: 69.53%] [G loss: 0.563238]\n",
      "epoch:3 step:3678 [D loss: 0.218839, acc.: 67.19%] [G loss: 0.536827]\n",
      "epoch:3 step:3679 [D loss: 0.223023, acc.: 64.84%] [G loss: 0.532851]\n",
      "epoch:3 step:3680 [D loss: 0.172330, acc.: 73.44%] [G loss: 0.619104]\n",
      "epoch:3 step:3681 [D loss: 0.189916, acc.: 69.53%] [G loss: 0.546646]\n",
      "epoch:3 step:3682 [D loss: 0.140802, acc.: 85.94%] [G loss: 0.579173]\n",
      "epoch:3 step:3683 [D loss: 0.165218, acc.: 81.25%] [G loss: 0.600290]\n",
      "epoch:3 step:3684 [D loss: 0.199783, acc.: 71.88%] [G loss: 0.579668]\n",
      "epoch:3 step:3685 [D loss: 0.182399, acc.: 75.78%] [G loss: 0.568374]\n",
      "epoch:3 step:3686 [D loss: 0.159534, acc.: 77.34%] [G loss: 0.600802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3687 [D loss: 0.192386, acc.: 67.19%] [G loss: 0.590174]\n",
      "epoch:3 step:3688 [D loss: 0.158582, acc.: 78.91%] [G loss: 0.598456]\n",
      "epoch:3 step:3689 [D loss: 0.220627, acc.: 64.84%] [G loss: 0.523449]\n",
      "epoch:3 step:3690 [D loss: 0.179783, acc.: 73.44%] [G loss: 0.577333]\n",
      "epoch:3 step:3691 [D loss: 0.251415, acc.: 60.16%] [G loss: 0.512982]\n",
      "epoch:3 step:3692 [D loss: 0.178717, acc.: 73.44%] [G loss: 0.561538]\n",
      "epoch:3 step:3693 [D loss: 0.178530, acc.: 74.22%] [G loss: 0.554290]\n",
      "epoch:3 step:3694 [D loss: 0.208934, acc.: 65.62%] [G loss: 0.572480]\n",
      "epoch:3 step:3695 [D loss: 0.154271, acc.: 79.69%] [G loss: 0.660758]\n",
      "epoch:3 step:3696 [D loss: 0.171093, acc.: 75.78%] [G loss: 0.654475]\n",
      "epoch:3 step:3697 [D loss: 0.165166, acc.: 75.78%] [G loss: 0.611664]\n",
      "epoch:3 step:3698 [D loss: 0.177361, acc.: 76.56%] [G loss: 0.595481]\n",
      "epoch:3 step:3699 [D loss: 0.178189, acc.: 73.44%] [G loss: 0.560220]\n",
      "epoch:3 step:3700 [D loss: 0.165790, acc.: 75.78%] [G loss: 0.648039]\n",
      "epoch:3 step:3701 [D loss: 0.145904, acc.: 83.59%] [G loss: 0.677422]\n",
      "epoch:3 step:3702 [D loss: 0.233235, acc.: 67.19%] [G loss: 0.534673]\n",
      "epoch:3 step:3703 [D loss: 0.219101, acc.: 67.19%] [G loss: 0.535319]\n",
      "epoch:3 step:3704 [D loss: 0.157348, acc.: 83.59%] [G loss: 0.592914]\n",
      "epoch:3 step:3705 [D loss: 0.174253, acc.: 72.66%] [G loss: 0.621960]\n",
      "epoch:3 step:3706 [D loss: 0.179526, acc.: 79.69%] [G loss: 0.602366]\n",
      "epoch:3 step:3707 [D loss: 0.178300, acc.: 73.44%] [G loss: 0.555922]\n",
      "epoch:3 step:3708 [D loss: 0.163067, acc.: 78.12%] [G loss: 0.594609]\n",
      "epoch:3 step:3709 [D loss: 0.152896, acc.: 78.12%] [G loss: 0.632357]\n",
      "epoch:3 step:3710 [D loss: 0.146471, acc.: 83.59%] [G loss: 0.627718]\n",
      "epoch:3 step:3711 [D loss: 0.155674, acc.: 75.00%] [G loss: 0.594640]\n",
      "epoch:3 step:3712 [D loss: 0.179178, acc.: 71.88%] [G loss: 0.536518]\n",
      "epoch:3 step:3713 [D loss: 0.200514, acc.: 67.97%] [G loss: 0.542251]\n",
      "epoch:3 step:3714 [D loss: 0.168709, acc.: 78.12%] [G loss: 0.621481]\n",
      "epoch:3 step:3715 [D loss: 0.159908, acc.: 77.34%] [G loss: 0.619491]\n",
      "epoch:3 step:3716 [D loss: 0.147079, acc.: 78.91%] [G loss: 0.646764]\n",
      "epoch:3 step:3717 [D loss: 0.175912, acc.: 75.78%] [G loss: 0.607305]\n",
      "epoch:3 step:3718 [D loss: 0.187551, acc.: 72.66%] [G loss: 0.588357]\n",
      "epoch:3 step:3719 [D loss: 0.200487, acc.: 68.75%] [G loss: 0.552292]\n",
      "epoch:3 step:3720 [D loss: 0.153646, acc.: 79.69%] [G loss: 0.649919]\n",
      "epoch:3 step:3721 [D loss: 0.169241, acc.: 71.88%] [G loss: 0.589659]\n",
      "epoch:3 step:3722 [D loss: 0.178349, acc.: 75.00%] [G loss: 0.615527]\n",
      "epoch:3 step:3723 [D loss: 0.172600, acc.: 75.78%] [G loss: 0.665417]\n",
      "epoch:3 step:3724 [D loss: 0.188031, acc.: 71.88%] [G loss: 0.623931]\n",
      "epoch:3 step:3725 [D loss: 0.171681, acc.: 74.22%] [G loss: 0.605083]\n",
      "epoch:3 step:3726 [D loss: 0.202566, acc.: 68.75%] [G loss: 0.617373]\n",
      "epoch:3 step:3727 [D loss: 0.196141, acc.: 72.66%] [G loss: 0.576222]\n",
      "epoch:3 step:3728 [D loss: 0.201995, acc.: 64.84%] [G loss: 0.612811]\n",
      "epoch:3 step:3729 [D loss: 0.146865, acc.: 78.91%] [G loss: 0.696080]\n",
      "epoch:3 step:3730 [D loss: 0.176326, acc.: 71.88%] [G loss: 0.664223]\n",
      "epoch:3 step:3731 [D loss: 0.317979, acc.: 53.91%] [G loss: 0.558552]\n",
      "epoch:3 step:3732 [D loss: 0.124705, acc.: 84.38%] [G loss: 0.709452]\n",
      "epoch:3 step:3733 [D loss: 0.259137, acc.: 58.59%] [G loss: 0.551828]\n",
      "epoch:3 step:3734 [D loss: 0.125691, acc.: 85.94%] [G loss: 0.681239]\n",
      "epoch:3 step:3735 [D loss: 0.128873, acc.: 82.03%] [G loss: 0.676705]\n",
      "epoch:3 step:3736 [D loss: 0.137097, acc.: 82.03%] [G loss: 0.718249]\n",
      "epoch:3 step:3737 [D loss: 0.133683, acc.: 82.03%] [G loss: 0.725578]\n",
      "epoch:3 step:3738 [D loss: 0.127270, acc.: 82.81%] [G loss: 0.709697]\n",
      "epoch:3 step:3739 [D loss: 0.300100, acc.: 60.16%] [G loss: 0.625721]\n",
      "epoch:3 step:3740 [D loss: 0.123338, acc.: 83.59%] [G loss: 0.780020]\n",
      "epoch:3 step:3741 [D loss: 0.199179, acc.: 68.75%] [G loss: 0.652509]\n",
      "epoch:3 step:3742 [D loss: 0.178915, acc.: 71.88%] [G loss: 0.540163]\n",
      "epoch:3 step:3743 [D loss: 0.172336, acc.: 74.22%] [G loss: 0.528358]\n",
      "epoch:3 step:3744 [D loss: 0.148102, acc.: 81.25%] [G loss: 0.628002]\n",
      "epoch:3 step:3745 [D loss: 0.182352, acc.: 73.44%] [G loss: 0.644518]\n",
      "epoch:3 step:3746 [D loss: 0.201093, acc.: 69.53%] [G loss: 0.646007]\n",
      "epoch:3 step:3747 [D loss: 0.148808, acc.: 77.34%] [G loss: 0.734651]\n",
      "epoch:3 step:3748 [D loss: 0.135485, acc.: 83.59%] [G loss: 0.730596]\n",
      "epoch:4 step:3749 [D loss: 0.189766, acc.: 76.56%] [G loss: 0.576452]\n",
      "epoch:4 step:3750 [D loss: 0.198734, acc.: 71.88%] [G loss: 0.623725]\n",
      "epoch:4 step:3751 [D loss: 0.206583, acc.: 66.41%] [G loss: 0.601781]\n",
      "epoch:4 step:3752 [D loss: 0.182377, acc.: 74.22%] [G loss: 0.619614]\n",
      "epoch:4 step:3753 [D loss: 0.196121, acc.: 67.97%] [G loss: 0.535150]\n",
      "epoch:4 step:3754 [D loss: 0.182601, acc.: 75.78%] [G loss: 0.571850]\n",
      "epoch:4 step:3755 [D loss: 0.162739, acc.: 79.69%] [G loss: 0.637689]\n",
      "epoch:4 step:3756 [D loss: 0.218411, acc.: 64.06%] [G loss: 0.567176]\n",
      "epoch:4 step:3757 [D loss: 0.176213, acc.: 72.66%] [G loss: 0.603705]\n",
      "epoch:4 step:3758 [D loss: 0.187843, acc.: 73.44%] [G loss: 0.560229]\n",
      "epoch:4 step:3759 [D loss: 0.173406, acc.: 74.22%] [G loss: 0.594261]\n",
      "epoch:4 step:3760 [D loss: 0.189687, acc.: 71.88%] [G loss: 0.625907]\n",
      "epoch:4 step:3761 [D loss: 0.178759, acc.: 74.22%] [G loss: 0.590704]\n",
      "epoch:4 step:3762 [D loss: 0.188698, acc.: 70.31%] [G loss: 0.599969]\n",
      "epoch:4 step:3763 [D loss: 0.138167, acc.: 83.59%] [G loss: 0.639658]\n",
      "epoch:4 step:3764 [D loss: 0.159063, acc.: 75.00%] [G loss: 0.652693]\n",
      "epoch:4 step:3765 [D loss: 0.217897, acc.: 65.62%] [G loss: 0.543112]\n",
      "epoch:4 step:3766 [D loss: 0.202159, acc.: 68.75%] [G loss: 0.521461]\n",
      "epoch:4 step:3767 [D loss: 0.177333, acc.: 70.31%] [G loss: 0.568654]\n",
      "epoch:4 step:3768 [D loss: 0.219263, acc.: 65.62%] [G loss: 0.551506]\n",
      "epoch:4 step:3769 [D loss: 0.178220, acc.: 73.44%] [G loss: 0.608373]\n",
      "epoch:4 step:3770 [D loss: 0.170285, acc.: 71.09%] [G loss: 0.636347]\n",
      "epoch:4 step:3771 [D loss: 0.185453, acc.: 75.00%] [G loss: 0.573072]\n",
      "epoch:4 step:3772 [D loss: 0.186799, acc.: 75.78%] [G loss: 0.604405]\n",
      "epoch:4 step:3773 [D loss: 0.150960, acc.: 79.69%] [G loss: 0.658625]\n",
      "epoch:4 step:3774 [D loss: 0.183951, acc.: 78.12%] [G loss: 0.596900]\n",
      "epoch:4 step:3775 [D loss: 0.187160, acc.: 71.88%] [G loss: 0.590486]\n",
      "epoch:4 step:3776 [D loss: 0.178697, acc.: 73.44%] [G loss: 0.610100]\n",
      "epoch:4 step:3777 [D loss: 0.157804, acc.: 76.56%] [G loss: 0.601910]\n",
      "epoch:4 step:3778 [D loss: 0.213349, acc.: 64.06%] [G loss: 0.581239]\n",
      "epoch:4 step:3779 [D loss: 0.163436, acc.: 77.34%] [G loss: 0.558528]\n",
      "epoch:4 step:3780 [D loss: 0.187176, acc.: 72.66%] [G loss: 0.529722]\n",
      "epoch:4 step:3781 [D loss: 0.142258, acc.: 83.59%] [G loss: 0.607860]\n",
      "epoch:4 step:3782 [D loss: 0.158749, acc.: 76.56%] [G loss: 0.617802]\n",
      "epoch:4 step:3783 [D loss: 0.163928, acc.: 80.47%] [G loss: 0.584486]\n",
      "epoch:4 step:3784 [D loss: 0.149349, acc.: 81.25%] [G loss: 0.603098]\n",
      "epoch:4 step:3785 [D loss: 0.148741, acc.: 78.12%] [G loss: 0.628871]\n",
      "epoch:4 step:3786 [D loss: 0.183271, acc.: 75.00%] [G loss: 0.594329]\n",
      "epoch:4 step:3787 [D loss: 0.167138, acc.: 75.78%] [G loss: 0.588121]\n",
      "epoch:4 step:3788 [D loss: 0.121684, acc.: 85.94%] [G loss: 0.711176]\n",
      "epoch:4 step:3789 [D loss: 0.204559, acc.: 74.22%] [G loss: 0.600704]\n",
      "epoch:4 step:3790 [D loss: 0.158835, acc.: 80.47%] [G loss: 0.610816]\n",
      "epoch:4 step:3791 [D loss: 0.151399, acc.: 80.47%] [G loss: 0.619347]\n",
      "epoch:4 step:3792 [D loss: 0.183374, acc.: 69.53%] [G loss: 0.614644]\n",
      "epoch:4 step:3793 [D loss: 0.213363, acc.: 65.62%] [G loss: 0.568750]\n",
      "epoch:4 step:3794 [D loss: 0.189292, acc.: 74.22%] [G loss: 0.580753]\n",
      "epoch:4 step:3795 [D loss: 0.176975, acc.: 72.66%] [G loss: 0.559785]\n",
      "epoch:4 step:3796 [D loss: 0.200586, acc.: 70.31%] [G loss: 0.594338]\n",
      "epoch:4 step:3797 [D loss: 0.175327, acc.: 73.44%] [G loss: 0.567135]\n",
      "epoch:4 step:3798 [D loss: 0.173334, acc.: 73.44%] [G loss: 0.623813]\n",
      "epoch:4 step:3799 [D loss: 0.179446, acc.: 71.09%] [G loss: 0.619685]\n",
      "epoch:4 step:3800 [D loss: 0.153653, acc.: 81.25%] [G loss: 0.625308]\n",
      "##############\n",
      "[3.18735602 1.46607365 6.70321269 5.26919728 4.20721771 6.13356458\n",
      " 5.09722751 5.12506646 5.10944699 3.94556843]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.195845, acc.: 72.66%] [G loss: 0.596651]\n",
      "epoch:4 step:3802 [D loss: 0.177702, acc.: 77.34%] [G loss: 0.587445]\n",
      "epoch:4 step:3803 [D loss: 0.169107, acc.: 75.00%] [G loss: 0.592335]\n",
      "epoch:4 step:3804 [D loss: 0.188266, acc.: 71.88%] [G loss: 0.572812]\n",
      "epoch:4 step:3805 [D loss: 0.168683, acc.: 80.47%] [G loss: 0.584903]\n",
      "epoch:4 step:3806 [D loss: 0.185676, acc.: 70.31%] [G loss: 0.611316]\n",
      "epoch:4 step:3807 [D loss: 0.165161, acc.: 75.78%] [G loss: 0.549015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3808 [D loss: 0.163268, acc.: 78.91%] [G loss: 0.593336]\n",
      "epoch:4 step:3809 [D loss: 0.200017, acc.: 71.09%] [G loss: 0.590681]\n",
      "epoch:4 step:3810 [D loss: 0.184596, acc.: 69.53%] [G loss: 0.550883]\n",
      "epoch:4 step:3811 [D loss: 0.211502, acc.: 63.28%] [G loss: 0.512663]\n",
      "epoch:4 step:3812 [D loss: 0.168152, acc.: 78.12%] [G loss: 0.575664]\n",
      "epoch:4 step:3813 [D loss: 0.209962, acc.: 67.97%] [G loss: 0.553323]\n",
      "epoch:4 step:3814 [D loss: 0.185075, acc.: 73.44%] [G loss: 0.572459]\n",
      "epoch:4 step:3815 [D loss: 0.171809, acc.: 75.00%] [G loss: 0.568806]\n",
      "epoch:4 step:3816 [D loss: 0.171919, acc.: 74.22%] [G loss: 0.600161]\n",
      "epoch:4 step:3817 [D loss: 0.167858, acc.: 75.78%] [G loss: 0.581735]\n",
      "epoch:4 step:3818 [D loss: 0.170758, acc.: 79.69%] [G loss: 0.599610]\n",
      "epoch:4 step:3819 [D loss: 0.167944, acc.: 74.22%] [G loss: 0.609143]\n",
      "epoch:4 step:3820 [D loss: 0.183909, acc.: 76.56%] [G loss: 0.617175]\n",
      "epoch:4 step:3821 [D loss: 0.174279, acc.: 73.44%] [G loss: 0.622460]\n",
      "epoch:4 step:3822 [D loss: 0.191402, acc.: 69.53%] [G loss: 0.562852]\n",
      "epoch:4 step:3823 [D loss: 0.171642, acc.: 78.12%] [G loss: 0.600382]\n",
      "epoch:4 step:3824 [D loss: 0.161747, acc.: 74.22%] [G loss: 0.615671]\n",
      "epoch:4 step:3825 [D loss: 0.144998, acc.: 78.91%] [G loss: 0.662945]\n",
      "epoch:4 step:3826 [D loss: 0.256301, acc.: 59.38%] [G loss: 0.528161]\n",
      "epoch:4 step:3827 [D loss: 0.200029, acc.: 70.31%] [G loss: 0.520617]\n",
      "epoch:4 step:3828 [D loss: 0.188794, acc.: 68.75%] [G loss: 0.575126]\n",
      "epoch:4 step:3829 [D loss: 0.203006, acc.: 70.31%] [G loss: 0.549666]\n",
      "epoch:4 step:3830 [D loss: 0.213022, acc.: 61.72%] [G loss: 0.553095]\n",
      "epoch:4 step:3831 [D loss: 0.147252, acc.: 78.91%] [G loss: 0.649523]\n",
      "epoch:4 step:3832 [D loss: 0.212609, acc.: 66.41%] [G loss: 0.567451]\n",
      "epoch:4 step:3833 [D loss: 0.198987, acc.: 67.19%] [G loss: 0.571911]\n",
      "epoch:4 step:3834 [D loss: 0.179788, acc.: 71.09%] [G loss: 0.592948]\n",
      "epoch:4 step:3835 [D loss: 0.170938, acc.: 74.22%] [G loss: 0.564321]\n",
      "epoch:4 step:3836 [D loss: 0.167040, acc.: 75.00%] [G loss: 0.646907]\n",
      "epoch:4 step:3837 [D loss: 0.175491, acc.: 77.34%] [G loss: 0.603385]\n",
      "epoch:4 step:3838 [D loss: 0.187601, acc.: 70.31%] [G loss: 0.549956]\n",
      "epoch:4 step:3839 [D loss: 0.229043, acc.: 63.28%] [G loss: 0.555006]\n",
      "epoch:4 step:3840 [D loss: 0.160691, acc.: 78.91%] [G loss: 0.622801]\n",
      "epoch:4 step:3841 [D loss: 0.204941, acc.: 71.09%] [G loss: 0.583809]\n",
      "epoch:4 step:3842 [D loss: 0.156930, acc.: 78.12%] [G loss: 0.633166]\n",
      "epoch:4 step:3843 [D loss: 0.179896, acc.: 72.66%] [G loss: 0.638856]\n",
      "epoch:4 step:3844 [D loss: 0.182650, acc.: 74.22%] [G loss: 0.594386]\n",
      "epoch:4 step:3845 [D loss: 0.179121, acc.: 73.44%] [G loss: 0.620006]\n",
      "epoch:4 step:3846 [D loss: 0.179516, acc.: 77.34%] [G loss: 0.577958]\n",
      "epoch:4 step:3847 [D loss: 0.201002, acc.: 68.75%] [G loss: 0.531852]\n",
      "epoch:4 step:3848 [D loss: 0.168212, acc.: 74.22%] [G loss: 0.652406]\n",
      "epoch:4 step:3849 [D loss: 0.179146, acc.: 75.00%] [G loss: 0.625399]\n",
      "epoch:4 step:3850 [D loss: 0.175356, acc.: 75.00%] [G loss: 0.563218]\n",
      "epoch:4 step:3851 [D loss: 0.145252, acc.: 81.25%] [G loss: 0.580243]\n",
      "epoch:4 step:3852 [D loss: 0.154785, acc.: 80.47%] [G loss: 0.611272]\n",
      "epoch:4 step:3853 [D loss: 0.182290, acc.: 75.78%] [G loss: 0.584063]\n",
      "epoch:4 step:3854 [D loss: 0.161642, acc.: 79.69%] [G loss: 0.621504]\n",
      "epoch:4 step:3855 [D loss: 0.207908, acc.: 62.50%] [G loss: 0.618301]\n",
      "epoch:4 step:3856 [D loss: 0.222732, acc.: 65.62%] [G loss: 0.600492]\n",
      "epoch:4 step:3857 [D loss: 0.235066, acc.: 64.06%] [G loss: 0.538057]\n",
      "epoch:4 step:3858 [D loss: 0.234314, acc.: 64.06%] [G loss: 0.552357]\n",
      "epoch:4 step:3859 [D loss: 0.169113, acc.: 76.56%] [G loss: 0.670862]\n",
      "epoch:4 step:3860 [D loss: 0.172115, acc.: 80.47%] [G loss: 0.612300]\n",
      "epoch:4 step:3861 [D loss: 0.210292, acc.: 65.62%] [G loss: 0.527202]\n",
      "epoch:4 step:3862 [D loss: 0.189030, acc.: 77.34%] [G loss: 0.571861]\n",
      "epoch:4 step:3863 [D loss: 0.194145, acc.: 64.84%] [G loss: 0.635713]\n",
      "epoch:4 step:3864 [D loss: 0.215637, acc.: 57.03%] [G loss: 0.595948]\n",
      "epoch:4 step:3865 [D loss: 0.195292, acc.: 67.19%] [G loss: 0.565608]\n",
      "epoch:4 step:3866 [D loss: 0.178410, acc.: 77.34%] [G loss: 0.581603]\n",
      "epoch:4 step:3867 [D loss: 0.160392, acc.: 80.47%] [G loss: 0.650358]\n",
      "epoch:4 step:3868 [D loss: 0.244449, acc.: 62.50%] [G loss: 0.578947]\n",
      "epoch:4 step:3869 [D loss: 0.195181, acc.: 70.31%] [G loss: 0.584227]\n",
      "epoch:4 step:3870 [D loss: 0.141426, acc.: 83.59%] [G loss: 0.661425]\n",
      "epoch:4 step:3871 [D loss: 0.177820, acc.: 75.78%] [G loss: 0.623200]\n",
      "epoch:4 step:3872 [D loss: 0.197614, acc.: 70.31%] [G loss: 0.533533]\n",
      "epoch:4 step:3873 [D loss: 0.200841, acc.: 66.41%] [G loss: 0.600619]\n",
      "epoch:4 step:3874 [D loss: 0.168350, acc.: 74.22%] [G loss: 0.600678]\n",
      "epoch:4 step:3875 [D loss: 0.197158, acc.: 68.75%] [G loss: 0.539477]\n",
      "epoch:4 step:3876 [D loss: 0.190582, acc.: 70.31%] [G loss: 0.559835]\n",
      "epoch:4 step:3877 [D loss: 0.203469, acc.: 67.97%] [G loss: 0.543796]\n",
      "epoch:4 step:3878 [D loss: 0.147673, acc.: 82.81%] [G loss: 0.567508]\n",
      "epoch:4 step:3879 [D loss: 0.171156, acc.: 77.34%] [G loss: 0.608324]\n",
      "epoch:4 step:3880 [D loss: 0.151783, acc.: 78.91%] [G loss: 0.622162]\n",
      "epoch:4 step:3881 [D loss: 0.183312, acc.: 70.31%] [G loss: 0.595572]\n",
      "epoch:4 step:3882 [D loss: 0.169411, acc.: 77.34%] [G loss: 0.623389]\n",
      "epoch:4 step:3883 [D loss: 0.163875, acc.: 77.34%] [G loss: 0.589981]\n",
      "epoch:4 step:3884 [D loss: 0.215737, acc.: 67.97%] [G loss: 0.564388]\n",
      "epoch:4 step:3885 [D loss: 0.186358, acc.: 69.53%] [G loss: 0.542662]\n",
      "epoch:4 step:3886 [D loss: 0.212899, acc.: 67.97%] [G loss: 0.583519]\n",
      "epoch:4 step:3887 [D loss: 0.201332, acc.: 66.41%] [G loss: 0.614413]\n",
      "epoch:4 step:3888 [D loss: 0.204255, acc.: 67.19%] [G loss: 0.546990]\n",
      "epoch:4 step:3889 [D loss: 0.166678, acc.: 74.22%] [G loss: 0.590389]\n",
      "epoch:4 step:3890 [D loss: 0.171531, acc.: 74.22%] [G loss: 0.592485]\n",
      "epoch:4 step:3891 [D loss: 0.190831, acc.: 67.19%] [G loss: 0.581285]\n",
      "epoch:4 step:3892 [D loss: 0.140903, acc.: 81.25%] [G loss: 0.621238]\n",
      "epoch:4 step:3893 [D loss: 0.195119, acc.: 72.66%] [G loss: 0.582764]\n",
      "epoch:4 step:3894 [D loss: 0.186748, acc.: 76.56%] [G loss: 0.543390]\n",
      "epoch:4 step:3895 [D loss: 0.209247, acc.: 67.19%] [G loss: 0.519046]\n",
      "epoch:4 step:3896 [D loss: 0.208992, acc.: 66.41%] [G loss: 0.541157]\n",
      "epoch:4 step:3897 [D loss: 0.174087, acc.: 73.44%] [G loss: 0.607588]\n",
      "epoch:4 step:3898 [D loss: 0.213262, acc.: 70.31%] [G loss: 0.583359]\n",
      "epoch:4 step:3899 [D loss: 0.172848, acc.: 75.78%] [G loss: 0.620649]\n",
      "epoch:4 step:3900 [D loss: 0.137637, acc.: 87.50%] [G loss: 0.656325]\n",
      "epoch:4 step:3901 [D loss: 0.242015, acc.: 67.19%] [G loss: 0.548561]\n",
      "epoch:4 step:3902 [D loss: 0.184519, acc.: 73.44%] [G loss: 0.582143]\n",
      "epoch:4 step:3903 [D loss: 0.156845, acc.: 78.12%] [G loss: 0.589259]\n",
      "epoch:4 step:3904 [D loss: 0.150748, acc.: 78.91%] [G loss: 0.624659]\n",
      "epoch:4 step:3905 [D loss: 0.179813, acc.: 77.34%] [G loss: 0.605552]\n",
      "epoch:4 step:3906 [D loss: 0.176931, acc.: 72.66%] [G loss: 0.583292]\n",
      "epoch:4 step:3907 [D loss: 0.168266, acc.: 78.91%] [G loss: 0.584290]\n",
      "epoch:4 step:3908 [D loss: 0.195012, acc.: 68.75%] [G loss: 0.649322]\n",
      "epoch:4 step:3909 [D loss: 0.186278, acc.: 71.09%] [G loss: 0.633578]\n",
      "epoch:4 step:3910 [D loss: 0.172472, acc.: 72.66%] [G loss: 0.659994]\n",
      "epoch:4 step:3911 [D loss: 0.163847, acc.: 74.22%] [G loss: 0.607331]\n",
      "epoch:4 step:3912 [D loss: 0.158581, acc.: 78.91%] [G loss: 0.609443]\n",
      "epoch:4 step:3913 [D loss: 0.167372, acc.: 76.56%] [G loss: 0.592119]\n",
      "epoch:4 step:3914 [D loss: 0.173079, acc.: 78.12%] [G loss: 0.571617]\n",
      "epoch:4 step:3915 [D loss: 0.186174, acc.: 69.53%] [G loss: 0.555587]\n",
      "epoch:4 step:3916 [D loss: 0.169283, acc.: 78.91%] [G loss: 0.591984]\n",
      "epoch:4 step:3917 [D loss: 0.210791, acc.: 71.88%] [G loss: 0.558419]\n",
      "epoch:4 step:3918 [D loss: 0.197595, acc.: 67.19%] [G loss: 0.560239]\n",
      "epoch:4 step:3919 [D loss: 0.186047, acc.: 67.19%] [G loss: 0.573581]\n",
      "epoch:4 step:3920 [D loss: 0.179405, acc.: 75.00%] [G loss: 0.562784]\n",
      "epoch:4 step:3921 [D loss: 0.183123, acc.: 71.88%] [G loss: 0.559324]\n",
      "epoch:4 step:3922 [D loss: 0.205680, acc.: 69.53%] [G loss: 0.544517]\n",
      "epoch:4 step:3923 [D loss: 0.172480, acc.: 75.78%] [G loss: 0.566573]\n",
      "epoch:4 step:3924 [D loss: 0.198793, acc.: 68.75%] [G loss: 0.516475]\n",
      "epoch:4 step:3925 [D loss: 0.178125, acc.: 68.75%] [G loss: 0.583479]\n",
      "epoch:4 step:3926 [D loss: 0.168344, acc.: 78.91%] [G loss: 0.622230]\n",
      "epoch:4 step:3927 [D loss: 0.159696, acc.: 75.78%] [G loss: 0.602950]\n",
      "epoch:4 step:3928 [D loss: 0.191866, acc.: 70.31%] [G loss: 0.587467]\n",
      "epoch:4 step:3929 [D loss: 0.228844, acc.: 64.84%] [G loss: 0.536190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3930 [D loss: 0.193727, acc.: 70.31%] [G loss: 0.529686]\n",
      "epoch:4 step:3931 [D loss: 0.193336, acc.: 67.97%] [G loss: 0.602092]\n",
      "epoch:4 step:3932 [D loss: 0.186918, acc.: 71.09%] [G loss: 0.587672]\n",
      "epoch:4 step:3933 [D loss: 0.196367, acc.: 70.31%] [G loss: 0.563249]\n",
      "epoch:4 step:3934 [D loss: 0.212169, acc.: 68.75%] [G loss: 0.569861]\n",
      "epoch:4 step:3935 [D loss: 0.190758, acc.: 71.09%] [G loss: 0.540655]\n",
      "epoch:4 step:3936 [D loss: 0.234259, acc.: 64.84%] [G loss: 0.507137]\n",
      "epoch:4 step:3937 [D loss: 0.206095, acc.: 70.31%] [G loss: 0.595812]\n",
      "epoch:4 step:3938 [D loss: 0.156428, acc.: 79.69%] [G loss: 0.587368]\n",
      "epoch:4 step:3939 [D loss: 0.176587, acc.: 74.22%] [G loss: 0.572771]\n",
      "epoch:4 step:3940 [D loss: 0.188942, acc.: 67.19%] [G loss: 0.556886]\n",
      "epoch:4 step:3941 [D loss: 0.174308, acc.: 75.78%] [G loss: 0.584600]\n",
      "epoch:4 step:3942 [D loss: 0.141200, acc.: 83.59%] [G loss: 0.619386]\n",
      "epoch:4 step:3943 [D loss: 0.185764, acc.: 72.66%] [G loss: 0.594537]\n",
      "epoch:4 step:3944 [D loss: 0.208980, acc.: 67.19%] [G loss: 0.548977]\n",
      "epoch:4 step:3945 [D loss: 0.181871, acc.: 68.75%] [G loss: 0.564028]\n",
      "epoch:4 step:3946 [D loss: 0.152671, acc.: 79.69%] [G loss: 0.669298]\n",
      "epoch:4 step:3947 [D loss: 0.182016, acc.: 75.00%] [G loss: 0.585493]\n",
      "epoch:4 step:3948 [D loss: 0.238712, acc.: 65.62%] [G loss: 0.547539]\n",
      "epoch:4 step:3949 [D loss: 0.187619, acc.: 72.66%] [G loss: 0.560527]\n",
      "epoch:4 step:3950 [D loss: 0.176584, acc.: 75.78%] [G loss: 0.634635]\n",
      "epoch:4 step:3951 [D loss: 0.198992, acc.: 70.31%] [G loss: 0.537310]\n",
      "epoch:4 step:3952 [D loss: 0.180608, acc.: 73.44%] [G loss: 0.575947]\n",
      "epoch:4 step:3953 [D loss: 0.136446, acc.: 81.25%] [G loss: 0.640291]\n",
      "epoch:4 step:3954 [D loss: 0.205968, acc.: 70.31%] [G loss: 0.583195]\n",
      "epoch:4 step:3955 [D loss: 0.157110, acc.: 81.25%] [G loss: 0.582684]\n",
      "epoch:4 step:3956 [D loss: 0.141423, acc.: 82.03%] [G loss: 0.634610]\n",
      "epoch:4 step:3957 [D loss: 0.183213, acc.: 74.22%] [G loss: 0.564335]\n",
      "epoch:4 step:3958 [D loss: 0.217060, acc.: 64.84%] [G loss: 0.553224]\n",
      "epoch:4 step:3959 [D loss: 0.208988, acc.: 65.62%] [G loss: 0.542250]\n",
      "epoch:4 step:3960 [D loss: 0.203942, acc.: 71.09%] [G loss: 0.551616]\n",
      "epoch:4 step:3961 [D loss: 0.175673, acc.: 70.31%] [G loss: 0.570277]\n",
      "epoch:4 step:3962 [D loss: 0.225120, acc.: 64.84%] [G loss: 0.526999]\n",
      "epoch:4 step:3963 [D loss: 0.225701, acc.: 61.72%] [G loss: 0.526209]\n",
      "epoch:4 step:3964 [D loss: 0.171818, acc.: 70.31%] [G loss: 0.552613]\n",
      "epoch:4 step:3965 [D loss: 0.167462, acc.: 77.34%] [G loss: 0.625610]\n",
      "epoch:4 step:3966 [D loss: 0.172026, acc.: 72.66%] [G loss: 0.585501]\n",
      "epoch:4 step:3967 [D loss: 0.143640, acc.: 80.47%] [G loss: 0.590981]\n",
      "epoch:4 step:3968 [D loss: 0.257025, acc.: 60.94%] [G loss: 0.523257]\n",
      "epoch:4 step:3969 [D loss: 0.145113, acc.: 78.91%] [G loss: 0.674356]\n",
      "epoch:4 step:3970 [D loss: 0.173407, acc.: 71.88%] [G loss: 0.632565]\n",
      "epoch:4 step:3971 [D loss: 0.148161, acc.: 81.25%] [G loss: 0.677869]\n",
      "epoch:4 step:3972 [D loss: 0.239893, acc.: 64.06%] [G loss: 0.536631]\n",
      "epoch:4 step:3973 [D loss: 0.185715, acc.: 70.31%] [G loss: 0.565516]\n",
      "epoch:4 step:3974 [D loss: 0.220097, acc.: 71.09%] [G loss: 0.532050]\n",
      "epoch:4 step:3975 [D loss: 0.191911, acc.: 71.88%] [G loss: 0.548617]\n",
      "epoch:4 step:3976 [D loss: 0.203219, acc.: 69.53%] [G loss: 0.530232]\n",
      "epoch:4 step:3977 [D loss: 0.164293, acc.: 77.34%] [G loss: 0.590859]\n",
      "epoch:4 step:3978 [D loss: 0.159957, acc.: 74.22%] [G loss: 0.679186]\n",
      "epoch:4 step:3979 [D loss: 0.173986, acc.: 72.66%] [G loss: 0.615808]\n",
      "epoch:4 step:3980 [D loss: 0.135502, acc.: 82.03%] [G loss: 0.681847]\n",
      "epoch:4 step:3981 [D loss: 0.212876, acc.: 64.06%] [G loss: 0.574003]\n",
      "epoch:4 step:3982 [D loss: 0.192325, acc.: 74.22%] [G loss: 0.559915]\n",
      "epoch:4 step:3983 [D loss: 0.192339, acc.: 71.09%] [G loss: 0.551161]\n",
      "epoch:4 step:3984 [D loss: 0.170175, acc.: 76.56%] [G loss: 0.627658]\n",
      "epoch:4 step:3985 [D loss: 0.216783, acc.: 71.88%] [G loss: 0.564162]\n",
      "epoch:4 step:3986 [D loss: 0.171493, acc.: 75.78%] [G loss: 0.558933]\n",
      "epoch:4 step:3987 [D loss: 0.200131, acc.: 67.97%] [G loss: 0.551235]\n",
      "epoch:4 step:3988 [D loss: 0.143967, acc.: 82.03%] [G loss: 0.617201]\n",
      "epoch:4 step:3989 [D loss: 0.171481, acc.: 75.00%] [G loss: 0.548777]\n",
      "epoch:4 step:3990 [D loss: 0.170145, acc.: 75.78%] [G loss: 0.558628]\n",
      "epoch:4 step:3991 [D loss: 0.171221, acc.: 75.78%] [G loss: 0.600140]\n",
      "epoch:4 step:3992 [D loss: 0.183853, acc.: 74.22%] [G loss: 0.641441]\n",
      "epoch:4 step:3993 [D loss: 0.187897, acc.: 73.44%] [G loss: 0.545458]\n",
      "epoch:4 step:3994 [D loss: 0.214002, acc.: 67.19%] [G loss: 0.558221]\n",
      "epoch:4 step:3995 [D loss: 0.201897, acc.: 66.41%] [G loss: 0.607409]\n",
      "epoch:4 step:3996 [D loss: 0.198026, acc.: 70.31%] [G loss: 0.605932]\n",
      "epoch:4 step:3997 [D loss: 0.224407, acc.: 63.28%] [G loss: 0.582383]\n",
      "epoch:4 step:3998 [D loss: 0.206527, acc.: 66.41%] [G loss: 0.586669]\n",
      "epoch:4 step:3999 [D loss: 0.170485, acc.: 71.88%] [G loss: 0.595860]\n",
      "epoch:4 step:4000 [D loss: 0.222506, acc.: 68.75%] [G loss: 0.580743]\n",
      "##############\n",
      "[3.16776302 1.36291502 6.86815834 5.07222315 4.17092874 6.06149818\n",
      " 4.9277894  4.9139697  5.09546503 3.75404168]\n",
      "##########\n",
      "epoch:4 step:4001 [D loss: 0.182978, acc.: 74.22%] [G loss: 0.561873]\n",
      "epoch:4 step:4002 [D loss: 0.167224, acc.: 77.34%] [G loss: 0.565035]\n",
      "epoch:4 step:4003 [D loss: 0.148496, acc.: 78.12%] [G loss: 0.588214]\n",
      "epoch:4 step:4004 [D loss: 0.181151, acc.: 80.47%] [G loss: 0.554388]\n",
      "epoch:4 step:4005 [D loss: 0.194695, acc.: 73.44%] [G loss: 0.577508]\n",
      "epoch:4 step:4006 [D loss: 0.157947, acc.: 79.69%] [G loss: 0.602674]\n",
      "epoch:4 step:4007 [D loss: 0.154883, acc.: 81.25%] [G loss: 0.602576]\n",
      "epoch:4 step:4008 [D loss: 0.182040, acc.: 73.44%] [G loss: 0.557078]\n",
      "epoch:4 step:4009 [D loss: 0.177218, acc.: 78.12%] [G loss: 0.640363]\n",
      "epoch:4 step:4010 [D loss: 0.171617, acc.: 75.78%] [G loss: 0.611220]\n",
      "epoch:4 step:4011 [D loss: 0.231773, acc.: 66.41%] [G loss: 0.520601]\n",
      "epoch:4 step:4012 [D loss: 0.174912, acc.: 78.12%] [G loss: 0.649115]\n",
      "epoch:4 step:4013 [D loss: 0.240154, acc.: 63.28%] [G loss: 0.519513]\n",
      "epoch:4 step:4014 [D loss: 0.196392, acc.: 69.53%] [G loss: 0.553152]\n",
      "epoch:4 step:4015 [D loss: 0.181541, acc.: 74.22%] [G loss: 0.550033]\n",
      "epoch:4 step:4016 [D loss: 0.191359, acc.: 67.97%] [G loss: 0.557884]\n",
      "epoch:4 step:4017 [D loss: 0.224534, acc.: 64.06%] [G loss: 0.572740]\n",
      "epoch:4 step:4018 [D loss: 0.176965, acc.: 71.09%] [G loss: 0.613898]\n",
      "epoch:4 step:4019 [D loss: 0.146580, acc.: 80.47%] [G loss: 0.641894]\n",
      "epoch:4 step:4020 [D loss: 0.231505, acc.: 62.50%] [G loss: 0.535385]\n",
      "epoch:4 step:4021 [D loss: 0.191629, acc.: 69.53%] [G loss: 0.541479]\n",
      "epoch:4 step:4022 [D loss: 0.196694, acc.: 71.09%] [G loss: 0.610795]\n",
      "epoch:4 step:4023 [D loss: 0.208928, acc.: 65.62%] [G loss: 0.509261]\n",
      "epoch:4 step:4024 [D loss: 0.201136, acc.: 66.41%] [G loss: 0.560686]\n",
      "epoch:4 step:4025 [D loss: 0.232972, acc.: 65.62%] [G loss: 0.561832]\n",
      "epoch:4 step:4026 [D loss: 0.199379, acc.: 66.41%] [G loss: 0.539802]\n",
      "epoch:4 step:4027 [D loss: 0.188140, acc.: 70.31%] [G loss: 0.631704]\n",
      "epoch:4 step:4028 [D loss: 0.177907, acc.: 71.09%] [G loss: 0.591492]\n",
      "epoch:4 step:4029 [D loss: 0.241836, acc.: 64.06%] [G loss: 0.489845]\n",
      "epoch:4 step:4030 [D loss: 0.209744, acc.: 65.62%] [G loss: 0.533279]\n",
      "epoch:4 step:4031 [D loss: 0.176215, acc.: 72.66%] [G loss: 0.545870]\n",
      "epoch:4 step:4032 [D loss: 0.173643, acc.: 75.00%] [G loss: 0.588663]\n",
      "epoch:4 step:4033 [D loss: 0.166991, acc.: 78.91%] [G loss: 0.570673]\n",
      "epoch:4 step:4034 [D loss: 0.168855, acc.: 72.66%] [G loss: 0.606371]\n",
      "epoch:4 step:4035 [D loss: 0.211191, acc.: 65.62%] [G loss: 0.567912]\n",
      "epoch:4 step:4036 [D loss: 0.206153, acc.: 67.97%] [G loss: 0.548018]\n",
      "epoch:4 step:4037 [D loss: 0.186192, acc.: 70.31%] [G loss: 0.634635]\n",
      "epoch:4 step:4038 [D loss: 0.195713, acc.: 69.53%] [G loss: 0.591380]\n",
      "epoch:4 step:4039 [D loss: 0.196335, acc.: 73.44%] [G loss: 0.626038]\n",
      "epoch:4 step:4040 [D loss: 0.207736, acc.: 67.97%] [G loss: 0.603198]\n",
      "epoch:4 step:4041 [D loss: 0.171309, acc.: 74.22%] [G loss: 0.630303]\n",
      "epoch:4 step:4042 [D loss: 0.190577, acc.: 74.22%] [G loss: 0.592858]\n",
      "epoch:4 step:4043 [D loss: 0.181974, acc.: 72.66%] [G loss: 0.603343]\n",
      "epoch:4 step:4044 [D loss: 0.179082, acc.: 73.44%] [G loss: 0.589813]\n",
      "epoch:4 step:4045 [D loss: 0.180973, acc.: 73.44%] [G loss: 0.585554]\n",
      "epoch:4 step:4046 [D loss: 0.156227, acc.: 78.91%] [G loss: 0.583408]\n",
      "epoch:4 step:4047 [D loss: 0.207049, acc.: 72.66%] [G loss: 0.525581]\n",
      "epoch:4 step:4048 [D loss: 0.163955, acc.: 77.34%] [G loss: 0.624176]\n",
      "epoch:4 step:4049 [D loss: 0.214593, acc.: 67.19%] [G loss: 0.539321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4050 [D loss: 0.195395, acc.: 68.75%] [G loss: 0.593436]\n",
      "epoch:4 step:4051 [D loss: 0.183315, acc.: 74.22%] [G loss: 0.600453]\n",
      "epoch:4 step:4052 [D loss: 0.144887, acc.: 76.56%] [G loss: 0.655719]\n",
      "epoch:4 step:4053 [D loss: 0.158123, acc.: 81.25%] [G loss: 0.628508]\n",
      "epoch:4 step:4054 [D loss: 0.176031, acc.: 75.78%] [G loss: 0.616482]\n",
      "epoch:4 step:4055 [D loss: 0.185801, acc.: 73.44%] [G loss: 0.584910]\n",
      "epoch:4 step:4056 [D loss: 0.184300, acc.: 70.31%] [G loss: 0.615690]\n",
      "epoch:4 step:4057 [D loss: 0.143551, acc.: 82.81%] [G loss: 0.600664]\n",
      "epoch:4 step:4058 [D loss: 0.169326, acc.: 78.12%] [G loss: 0.592767]\n",
      "epoch:4 step:4059 [D loss: 0.175039, acc.: 76.56%] [G loss: 0.595389]\n",
      "epoch:4 step:4060 [D loss: 0.130640, acc.: 80.47%] [G loss: 0.676142]\n",
      "epoch:4 step:4061 [D loss: 0.149741, acc.: 80.47%] [G loss: 0.645607]\n",
      "epoch:4 step:4062 [D loss: 0.152251, acc.: 80.47%] [G loss: 0.624503]\n",
      "epoch:4 step:4063 [D loss: 0.146756, acc.: 78.91%] [G loss: 0.644323]\n",
      "epoch:4 step:4064 [D loss: 0.290048, acc.: 51.56%] [G loss: 0.526005]\n",
      "epoch:4 step:4065 [D loss: 0.203633, acc.: 66.41%] [G loss: 0.564564]\n",
      "epoch:4 step:4066 [D loss: 0.180921, acc.: 75.00%] [G loss: 0.593412]\n",
      "epoch:4 step:4067 [D loss: 0.189680, acc.: 71.88%] [G loss: 0.596727]\n",
      "epoch:4 step:4068 [D loss: 0.170030, acc.: 72.66%] [G loss: 0.597853]\n",
      "epoch:4 step:4069 [D loss: 0.148081, acc.: 81.25%] [G loss: 0.670646]\n",
      "epoch:4 step:4070 [D loss: 0.181460, acc.: 72.66%] [G loss: 0.578712]\n",
      "epoch:4 step:4071 [D loss: 0.198842, acc.: 75.00%] [G loss: 0.571332]\n",
      "epoch:4 step:4072 [D loss: 0.208773, acc.: 61.72%] [G loss: 0.557087]\n",
      "epoch:4 step:4073 [D loss: 0.162707, acc.: 75.78%] [G loss: 0.588631]\n",
      "epoch:4 step:4074 [D loss: 0.164078, acc.: 79.69%] [G loss: 0.589062]\n",
      "epoch:4 step:4075 [D loss: 0.201605, acc.: 68.75%] [G loss: 0.562454]\n",
      "epoch:4 step:4076 [D loss: 0.172355, acc.: 72.66%] [G loss: 0.605670]\n",
      "epoch:4 step:4077 [D loss: 0.213074, acc.: 67.19%] [G loss: 0.606950]\n",
      "epoch:4 step:4078 [D loss: 0.171678, acc.: 74.22%] [G loss: 0.644885]\n",
      "epoch:4 step:4079 [D loss: 0.191164, acc.: 73.44%] [G loss: 0.584891]\n",
      "epoch:4 step:4080 [D loss: 0.149809, acc.: 79.69%] [G loss: 0.600387]\n",
      "epoch:4 step:4081 [D loss: 0.183597, acc.: 73.44%] [G loss: 0.586461]\n",
      "epoch:4 step:4082 [D loss: 0.208788, acc.: 69.53%] [G loss: 0.569445]\n",
      "epoch:4 step:4083 [D loss: 0.172973, acc.: 75.00%] [G loss: 0.644688]\n",
      "epoch:4 step:4084 [D loss: 0.162407, acc.: 75.00%] [G loss: 0.572690]\n",
      "epoch:4 step:4085 [D loss: 0.170892, acc.: 71.09%] [G loss: 0.571659]\n",
      "epoch:4 step:4086 [D loss: 0.182312, acc.: 75.00%] [G loss: 0.613836]\n",
      "epoch:4 step:4087 [D loss: 0.170877, acc.: 74.22%] [G loss: 0.599544]\n",
      "epoch:4 step:4088 [D loss: 0.196301, acc.: 71.88%] [G loss: 0.596987]\n",
      "epoch:4 step:4089 [D loss: 0.216011, acc.: 63.28%] [G loss: 0.525374]\n",
      "epoch:4 step:4090 [D loss: 0.162026, acc.: 78.12%] [G loss: 0.552995]\n",
      "epoch:4 step:4091 [D loss: 0.131734, acc.: 85.16%] [G loss: 0.676097]\n",
      "epoch:4 step:4092 [D loss: 0.141349, acc.: 83.59%] [G loss: 0.630451]\n",
      "epoch:4 step:4093 [D loss: 0.202804, acc.: 72.66%] [G loss: 0.534533]\n",
      "epoch:4 step:4094 [D loss: 0.166782, acc.: 75.00%] [G loss: 0.637248]\n",
      "epoch:4 step:4095 [D loss: 0.156176, acc.: 80.47%] [G loss: 0.654971]\n",
      "epoch:4 step:4096 [D loss: 0.302723, acc.: 50.78%] [G loss: 0.513805]\n",
      "epoch:4 step:4097 [D loss: 0.231251, acc.: 63.28%] [G loss: 0.503163]\n",
      "epoch:4 step:4098 [D loss: 0.163682, acc.: 75.78%] [G loss: 0.558787]\n",
      "epoch:4 step:4099 [D loss: 0.172158, acc.: 77.34%] [G loss: 0.570419]\n",
      "epoch:4 step:4100 [D loss: 0.219272, acc.: 67.19%] [G loss: 0.568365]\n",
      "epoch:4 step:4101 [D loss: 0.201844, acc.: 68.75%] [G loss: 0.565211]\n",
      "epoch:4 step:4102 [D loss: 0.162985, acc.: 80.47%] [G loss: 0.626617]\n",
      "epoch:4 step:4103 [D loss: 0.191291, acc.: 68.75%] [G loss: 0.617323]\n",
      "epoch:4 step:4104 [D loss: 0.192191, acc.: 70.31%] [G loss: 0.596097]\n",
      "epoch:4 step:4105 [D loss: 0.194569, acc.: 67.19%] [G loss: 0.581986]\n",
      "epoch:4 step:4106 [D loss: 0.163941, acc.: 78.12%] [G loss: 0.601921]\n",
      "epoch:4 step:4107 [D loss: 0.169001, acc.: 75.00%] [G loss: 0.593104]\n",
      "epoch:4 step:4108 [D loss: 0.157772, acc.: 78.91%] [G loss: 0.602812]\n",
      "epoch:4 step:4109 [D loss: 0.160117, acc.: 79.69%] [G loss: 0.609233]\n",
      "epoch:4 step:4110 [D loss: 0.193037, acc.: 69.53%] [G loss: 0.540548]\n",
      "epoch:4 step:4111 [D loss: 0.192444, acc.: 70.31%] [G loss: 0.574255]\n",
      "epoch:4 step:4112 [D loss: 0.167708, acc.: 79.69%] [G loss: 0.585374]\n",
      "epoch:4 step:4113 [D loss: 0.183698, acc.: 75.78%] [G loss: 0.593131]\n",
      "epoch:4 step:4114 [D loss: 0.158305, acc.: 76.56%] [G loss: 0.608062]\n",
      "epoch:4 step:4115 [D loss: 0.179121, acc.: 75.78%] [G loss: 0.608156]\n",
      "epoch:4 step:4116 [D loss: 0.197385, acc.: 73.44%] [G loss: 0.527716]\n",
      "epoch:4 step:4117 [D loss: 0.220931, acc.: 59.38%] [G loss: 0.502767]\n",
      "epoch:4 step:4118 [D loss: 0.170544, acc.: 75.78%] [G loss: 0.529423]\n",
      "epoch:4 step:4119 [D loss: 0.177619, acc.: 76.56%] [G loss: 0.585799]\n",
      "epoch:4 step:4120 [D loss: 0.179092, acc.: 74.22%] [G loss: 0.584194]\n",
      "epoch:4 step:4121 [D loss: 0.221943, acc.: 66.41%] [G loss: 0.553803]\n",
      "epoch:4 step:4122 [D loss: 0.156700, acc.: 79.69%] [G loss: 0.601878]\n",
      "epoch:4 step:4123 [D loss: 0.173866, acc.: 78.91%] [G loss: 0.587528]\n",
      "epoch:4 step:4124 [D loss: 0.239090, acc.: 62.50%] [G loss: 0.511808]\n",
      "epoch:4 step:4125 [D loss: 0.213914, acc.: 61.72%] [G loss: 0.505403]\n",
      "epoch:4 step:4126 [D loss: 0.204610, acc.: 68.75%] [G loss: 0.551799]\n",
      "epoch:4 step:4127 [D loss: 0.185781, acc.: 73.44%] [G loss: 0.554338]\n",
      "epoch:4 step:4128 [D loss: 0.200081, acc.: 68.75%] [G loss: 0.542935]\n",
      "epoch:4 step:4129 [D loss: 0.157128, acc.: 75.00%] [G loss: 0.642414]\n",
      "epoch:4 step:4130 [D loss: 0.188749, acc.: 71.88%] [G loss: 0.548162]\n",
      "epoch:4 step:4131 [D loss: 0.226919, acc.: 64.06%] [G loss: 0.516525]\n",
      "epoch:4 step:4132 [D loss: 0.214628, acc.: 64.84%] [G loss: 0.491552]\n",
      "epoch:4 step:4133 [D loss: 0.190575, acc.: 69.53%] [G loss: 0.575874]\n",
      "epoch:4 step:4134 [D loss: 0.216423, acc.: 64.84%] [G loss: 0.530053]\n",
      "epoch:4 step:4135 [D loss: 0.197534, acc.: 66.41%] [G loss: 0.521565]\n",
      "epoch:4 step:4136 [D loss: 0.188892, acc.: 69.53%] [G loss: 0.582525]\n",
      "epoch:4 step:4137 [D loss: 0.214905, acc.: 64.84%] [G loss: 0.521857]\n",
      "epoch:4 step:4138 [D loss: 0.200643, acc.: 69.53%] [G loss: 0.533711]\n",
      "epoch:4 step:4139 [D loss: 0.180181, acc.: 73.44%] [G loss: 0.613987]\n",
      "epoch:4 step:4140 [D loss: 0.180183, acc.: 73.44%] [G loss: 0.577758]\n",
      "epoch:4 step:4141 [D loss: 0.192389, acc.: 68.75%] [G loss: 0.535270]\n",
      "epoch:4 step:4142 [D loss: 0.197847, acc.: 73.44%] [G loss: 0.519248]\n",
      "epoch:4 step:4143 [D loss: 0.170150, acc.: 78.12%] [G loss: 0.573000]\n",
      "epoch:4 step:4144 [D loss: 0.259754, acc.: 58.59%] [G loss: 0.534180]\n",
      "epoch:4 step:4145 [D loss: 0.171156, acc.: 75.00%] [G loss: 0.590212]\n",
      "epoch:4 step:4146 [D loss: 0.172493, acc.: 75.00%] [G loss: 0.611764]\n",
      "epoch:4 step:4147 [D loss: 0.147331, acc.: 79.69%] [G loss: 0.645947]\n",
      "epoch:4 step:4148 [D loss: 0.248378, acc.: 59.38%] [G loss: 0.511905]\n",
      "epoch:4 step:4149 [D loss: 0.197847, acc.: 66.41%] [G loss: 0.563259]\n",
      "epoch:4 step:4150 [D loss: 0.153807, acc.: 77.34%] [G loss: 0.613266]\n",
      "epoch:4 step:4151 [D loss: 0.182077, acc.: 67.97%] [G loss: 0.562523]\n",
      "epoch:4 step:4152 [D loss: 0.200311, acc.: 69.53%] [G loss: 0.594557]\n",
      "epoch:4 step:4153 [D loss: 0.151025, acc.: 81.25%] [G loss: 0.541072]\n",
      "epoch:4 step:4154 [D loss: 0.173543, acc.: 72.66%] [G loss: 0.644978]\n",
      "epoch:4 step:4155 [D loss: 0.193066, acc.: 71.09%] [G loss: 0.553035]\n",
      "epoch:4 step:4156 [D loss: 0.226624, acc.: 67.19%] [G loss: 0.541311]\n",
      "epoch:4 step:4157 [D loss: 0.179427, acc.: 74.22%] [G loss: 0.592553]\n",
      "epoch:4 step:4158 [D loss: 0.203107, acc.: 67.19%] [G loss: 0.532153]\n",
      "epoch:4 step:4159 [D loss: 0.199256, acc.: 68.75%] [G loss: 0.594856]\n",
      "epoch:4 step:4160 [D loss: 0.214716, acc.: 68.75%] [G loss: 0.556388]\n",
      "epoch:4 step:4161 [D loss: 0.187026, acc.: 71.09%] [G loss: 0.552147]\n",
      "epoch:4 step:4162 [D loss: 0.190730, acc.: 71.88%] [G loss: 0.574291]\n",
      "epoch:4 step:4163 [D loss: 0.172397, acc.: 74.22%] [G loss: 0.582457]\n",
      "epoch:4 step:4164 [D loss: 0.196920, acc.: 71.88%] [G loss: 0.613057]\n",
      "epoch:4 step:4165 [D loss: 0.223372, acc.: 67.97%] [G loss: 0.608698]\n",
      "epoch:4 step:4166 [D loss: 0.258778, acc.: 64.06%] [G loss: 0.524259]\n",
      "epoch:4 step:4167 [D loss: 0.173630, acc.: 74.22%] [G loss: 0.492927]\n",
      "epoch:4 step:4168 [D loss: 0.175654, acc.: 73.44%] [G loss: 0.566434]\n",
      "epoch:4 step:4169 [D loss: 0.211730, acc.: 67.97%] [G loss: 0.505694]\n",
      "epoch:4 step:4170 [D loss: 0.196020, acc.: 70.31%] [G loss: 0.571078]\n",
      "epoch:4 step:4171 [D loss: 0.208677, acc.: 67.97%] [G loss: 0.561111]\n",
      "epoch:4 step:4172 [D loss: 0.223895, acc.: 65.62%] [G loss: 0.543951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4173 [D loss: 0.162835, acc.: 77.34%] [G loss: 0.553970]\n",
      "epoch:4 step:4174 [D loss: 0.180484, acc.: 74.22%] [G loss: 0.553658]\n",
      "epoch:4 step:4175 [D loss: 0.151666, acc.: 79.69%] [G loss: 0.677037]\n",
      "epoch:4 step:4176 [D loss: 0.198332, acc.: 69.53%] [G loss: 0.608657]\n",
      "epoch:4 step:4177 [D loss: 0.152552, acc.: 79.69%] [G loss: 0.624226]\n",
      "epoch:4 step:4178 [D loss: 0.197561, acc.: 73.44%] [G loss: 0.577648]\n",
      "epoch:4 step:4179 [D loss: 0.174198, acc.: 75.00%] [G loss: 0.568142]\n",
      "epoch:4 step:4180 [D loss: 0.211818, acc.: 65.62%] [G loss: 0.557280]\n",
      "epoch:4 step:4181 [D loss: 0.192492, acc.: 68.75%] [G loss: 0.534300]\n",
      "epoch:4 step:4182 [D loss: 0.184684, acc.: 76.56%] [G loss: 0.575983]\n",
      "epoch:4 step:4183 [D loss: 0.203096, acc.: 71.09%] [G loss: 0.579083]\n",
      "epoch:4 step:4184 [D loss: 0.181626, acc.: 67.19%] [G loss: 0.592807]\n",
      "epoch:4 step:4185 [D loss: 0.249264, acc.: 57.03%] [G loss: 0.542750]\n",
      "epoch:4 step:4186 [D loss: 0.191251, acc.: 71.88%] [G loss: 0.580228]\n",
      "epoch:4 step:4187 [D loss: 0.163386, acc.: 75.78%] [G loss: 0.573398]\n",
      "epoch:4 step:4188 [D loss: 0.187429, acc.: 73.44%] [G loss: 0.593023]\n",
      "epoch:4 step:4189 [D loss: 0.227594, acc.: 64.06%] [G loss: 0.561910]\n",
      "epoch:4 step:4190 [D loss: 0.193990, acc.: 72.66%] [G loss: 0.600274]\n",
      "epoch:4 step:4191 [D loss: 0.211569, acc.: 67.19%] [G loss: 0.516402]\n",
      "epoch:4 step:4192 [D loss: 0.202952, acc.: 67.97%] [G loss: 0.609399]\n",
      "epoch:4 step:4193 [D loss: 0.163252, acc.: 78.12%] [G loss: 0.578303]\n",
      "epoch:4 step:4194 [D loss: 0.189099, acc.: 73.44%] [G loss: 0.570608]\n",
      "epoch:4 step:4195 [D loss: 0.200394, acc.: 71.09%] [G loss: 0.573268]\n",
      "epoch:4 step:4196 [D loss: 0.226676, acc.: 63.28%] [G loss: 0.588231]\n",
      "epoch:4 step:4197 [D loss: 0.178483, acc.: 71.09%] [G loss: 0.577826]\n",
      "epoch:4 step:4198 [D loss: 0.156873, acc.: 78.91%] [G loss: 0.592352]\n",
      "epoch:4 step:4199 [D loss: 0.179242, acc.: 73.44%] [G loss: 0.620351]\n",
      "epoch:4 step:4200 [D loss: 0.182775, acc.: 75.00%] [G loss: 0.635411]\n",
      "##############\n",
      "[3.26290799 1.35282828 6.65400654 5.00865649 4.26561544 6.13040437\n",
      " 4.97852709 4.84763036 5.08998515 3.88577559]\n",
      "##########\n",
      "epoch:4 step:4201 [D loss: 0.187611, acc.: 72.66%] [G loss: 0.560126]\n",
      "epoch:4 step:4202 [D loss: 0.205243, acc.: 63.28%] [G loss: 0.544985]\n",
      "epoch:4 step:4203 [D loss: 0.212952, acc.: 66.41%] [G loss: 0.527115]\n",
      "epoch:4 step:4204 [D loss: 0.204037, acc.: 68.75%] [G loss: 0.535510]\n",
      "epoch:4 step:4205 [D loss: 0.176388, acc.: 72.66%] [G loss: 0.611116]\n",
      "epoch:4 step:4206 [D loss: 0.189482, acc.: 75.78%] [G loss: 0.580675]\n",
      "epoch:4 step:4207 [D loss: 0.200370, acc.: 67.97%] [G loss: 0.535383]\n",
      "epoch:4 step:4208 [D loss: 0.171561, acc.: 72.66%] [G loss: 0.617978]\n",
      "epoch:4 step:4209 [D loss: 0.168098, acc.: 74.22%] [G loss: 0.593309]\n",
      "epoch:4 step:4210 [D loss: 0.182105, acc.: 74.22%] [G loss: 0.568519]\n",
      "epoch:4 step:4211 [D loss: 0.174967, acc.: 71.09%] [G loss: 0.564562]\n",
      "epoch:4 step:4212 [D loss: 0.194845, acc.: 75.78%] [G loss: 0.558586]\n",
      "epoch:4 step:4213 [D loss: 0.246346, acc.: 57.81%] [G loss: 0.531411]\n",
      "epoch:4 step:4214 [D loss: 0.171497, acc.: 73.44%] [G loss: 0.656105]\n",
      "epoch:4 step:4215 [D loss: 0.188054, acc.: 72.66%] [G loss: 0.584872]\n",
      "epoch:4 step:4216 [D loss: 0.202130, acc.: 69.53%] [G loss: 0.583446]\n",
      "epoch:4 step:4217 [D loss: 0.208556, acc.: 67.19%] [G loss: 0.559785]\n",
      "epoch:4 step:4218 [D loss: 0.189723, acc.: 74.22%] [G loss: 0.596637]\n",
      "epoch:4 step:4219 [D loss: 0.161207, acc.: 75.00%] [G loss: 0.619647]\n",
      "epoch:4 step:4220 [D loss: 0.168268, acc.: 74.22%] [G loss: 0.598542]\n",
      "epoch:4 step:4221 [D loss: 0.202112, acc.: 71.09%] [G loss: 0.573756]\n",
      "epoch:4 step:4222 [D loss: 0.176744, acc.: 74.22%] [G loss: 0.563178]\n",
      "epoch:4 step:4223 [D loss: 0.160737, acc.: 77.34%] [G loss: 0.620089]\n",
      "epoch:4 step:4224 [D loss: 0.190920, acc.: 70.31%] [G loss: 0.580660]\n",
      "epoch:4 step:4225 [D loss: 0.266121, acc.: 57.81%] [G loss: 0.545744]\n",
      "epoch:4 step:4226 [D loss: 0.189339, acc.: 70.31%] [G loss: 0.547485]\n",
      "epoch:4 step:4227 [D loss: 0.215753, acc.: 65.62%] [G loss: 0.513173]\n",
      "epoch:4 step:4228 [D loss: 0.168722, acc.: 73.44%] [G loss: 0.545189]\n",
      "epoch:4 step:4229 [D loss: 0.162102, acc.: 78.12%] [G loss: 0.553141]\n",
      "epoch:4 step:4230 [D loss: 0.234669, acc.: 61.72%] [G loss: 0.510125]\n",
      "epoch:4 step:4231 [D loss: 0.201121, acc.: 69.53%] [G loss: 0.584474]\n",
      "epoch:4 step:4232 [D loss: 0.171931, acc.: 75.78%] [G loss: 0.587673]\n",
      "epoch:4 step:4233 [D loss: 0.204119, acc.: 67.97%] [G loss: 0.544823]\n",
      "epoch:4 step:4234 [D loss: 0.204593, acc.: 66.41%] [G loss: 0.524747]\n",
      "epoch:4 step:4235 [D loss: 0.193426, acc.: 75.00%] [G loss: 0.617082]\n",
      "epoch:4 step:4236 [D loss: 0.185559, acc.: 76.56%] [G loss: 0.614681]\n",
      "epoch:4 step:4237 [D loss: 0.228959, acc.: 68.75%] [G loss: 0.586663]\n",
      "epoch:4 step:4238 [D loss: 0.185244, acc.: 71.09%] [G loss: 0.585562]\n",
      "epoch:4 step:4239 [D loss: 0.173045, acc.: 76.56%] [G loss: 0.578705]\n",
      "epoch:4 step:4240 [D loss: 0.217425, acc.: 67.97%] [G loss: 0.521387]\n",
      "epoch:4 step:4241 [D loss: 0.193834, acc.: 71.09%] [G loss: 0.550741]\n",
      "epoch:4 step:4242 [D loss: 0.202771, acc.: 67.97%] [G loss: 0.584449]\n",
      "epoch:4 step:4243 [D loss: 0.183257, acc.: 75.00%] [G loss: 0.641901]\n",
      "epoch:4 step:4244 [D loss: 0.177038, acc.: 77.34%] [G loss: 0.589146]\n",
      "epoch:4 step:4245 [D loss: 0.171687, acc.: 75.00%] [G loss: 0.573197]\n",
      "epoch:4 step:4246 [D loss: 0.180635, acc.: 74.22%] [G loss: 0.605153]\n",
      "epoch:4 step:4247 [D loss: 0.156280, acc.: 77.34%] [G loss: 0.645272]\n",
      "epoch:4 step:4248 [D loss: 0.232963, acc.: 60.16%] [G loss: 0.486662]\n",
      "epoch:4 step:4249 [D loss: 0.244380, acc.: 63.28%] [G loss: 0.489922]\n",
      "epoch:4 step:4250 [D loss: 0.182897, acc.: 70.31%] [G loss: 0.596087]\n",
      "epoch:4 step:4251 [D loss: 0.142236, acc.: 82.03%] [G loss: 0.625500]\n",
      "epoch:4 step:4252 [D loss: 0.194004, acc.: 72.66%] [G loss: 0.565467]\n",
      "epoch:4 step:4253 [D loss: 0.193143, acc.: 73.44%] [G loss: 0.603290]\n",
      "epoch:4 step:4254 [D loss: 0.252163, acc.: 56.25%] [G loss: 0.529464]\n",
      "epoch:4 step:4255 [D loss: 0.204193, acc.: 72.66%] [G loss: 0.589439]\n",
      "epoch:4 step:4256 [D loss: 0.144855, acc.: 82.03%] [G loss: 0.640103]\n",
      "epoch:4 step:4257 [D loss: 0.180723, acc.: 75.78%] [G loss: 0.613170]\n",
      "epoch:4 step:4258 [D loss: 0.214829, acc.: 64.84%] [G loss: 0.505167]\n",
      "epoch:4 step:4259 [D loss: 0.196858, acc.: 71.09%] [G loss: 0.551957]\n",
      "epoch:4 step:4260 [D loss: 0.172010, acc.: 73.44%] [G loss: 0.584026]\n",
      "epoch:4 step:4261 [D loss: 0.174117, acc.: 75.78%] [G loss: 0.638102]\n",
      "epoch:4 step:4262 [D loss: 0.177804, acc.: 75.00%] [G loss: 0.585030]\n",
      "epoch:4 step:4263 [D loss: 0.160255, acc.: 79.69%] [G loss: 0.567748]\n",
      "epoch:4 step:4264 [D loss: 0.182260, acc.: 69.53%] [G loss: 0.558600]\n",
      "epoch:4 step:4265 [D loss: 0.194896, acc.: 67.97%] [G loss: 0.582335]\n",
      "epoch:4 step:4266 [D loss: 0.170787, acc.: 76.56%] [G loss: 0.604645]\n",
      "epoch:4 step:4267 [D loss: 0.166599, acc.: 77.34%] [G loss: 0.616018]\n",
      "epoch:4 step:4268 [D loss: 0.163784, acc.: 75.00%] [G loss: 0.614960]\n",
      "epoch:4 step:4269 [D loss: 0.185229, acc.: 70.31%] [G loss: 0.577652]\n",
      "epoch:4 step:4270 [D loss: 0.175494, acc.: 71.88%] [G loss: 0.607442]\n",
      "epoch:4 step:4271 [D loss: 0.185974, acc.: 74.22%] [G loss: 0.600004]\n",
      "epoch:4 step:4272 [D loss: 0.180325, acc.: 70.31%] [G loss: 0.614266]\n",
      "epoch:4 step:4273 [D loss: 0.205537, acc.: 68.75%] [G loss: 0.570409]\n",
      "epoch:4 step:4274 [D loss: 0.161223, acc.: 77.34%] [G loss: 0.589594]\n",
      "epoch:4 step:4275 [D loss: 0.223041, acc.: 66.41%] [G loss: 0.571856]\n",
      "epoch:4 step:4276 [D loss: 0.254927, acc.: 57.81%] [G loss: 0.516159]\n",
      "epoch:4 step:4277 [D loss: 0.195087, acc.: 72.66%] [G loss: 0.530214]\n",
      "epoch:4 step:4278 [D loss: 0.178588, acc.: 74.22%] [G loss: 0.627459]\n",
      "epoch:4 step:4279 [D loss: 0.203195, acc.: 71.09%] [G loss: 0.566330]\n",
      "epoch:4 step:4280 [D loss: 0.205180, acc.: 67.97%] [G loss: 0.493293]\n",
      "epoch:4 step:4281 [D loss: 0.184372, acc.: 72.66%] [G loss: 0.576398]\n",
      "epoch:4 step:4282 [D loss: 0.162132, acc.: 75.00%] [G loss: 0.541640]\n",
      "epoch:4 step:4283 [D loss: 0.186212, acc.: 71.09%] [G loss: 0.572284]\n",
      "epoch:4 step:4284 [D loss: 0.163748, acc.: 74.22%] [G loss: 0.574723]\n",
      "epoch:4 step:4285 [D loss: 0.195764, acc.: 71.09%] [G loss: 0.576584]\n",
      "epoch:4 step:4286 [D loss: 0.221572, acc.: 66.41%] [G loss: 0.552275]\n",
      "epoch:4 step:4287 [D loss: 0.207011, acc.: 62.50%] [G loss: 0.543892]\n",
      "epoch:4 step:4288 [D loss: 0.162928, acc.: 78.12%] [G loss: 0.568997]\n",
      "epoch:4 step:4289 [D loss: 0.173161, acc.: 74.22%] [G loss: 0.613564]\n",
      "epoch:4 step:4290 [D loss: 0.250079, acc.: 64.84%] [G loss: 0.534150]\n",
      "epoch:4 step:4291 [D loss: 0.219669, acc.: 62.50%] [G loss: 0.533860]\n",
      "epoch:4 step:4292 [D loss: 0.205462, acc.: 74.22%] [G loss: 0.509117]\n",
      "epoch:4 step:4293 [D loss: 0.176444, acc.: 75.00%] [G loss: 0.579527]\n",
      "epoch:4 step:4294 [D loss: 0.193501, acc.: 71.88%] [G loss: 0.612379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4295 [D loss: 0.170107, acc.: 77.34%] [G loss: 0.579153]\n",
      "epoch:4 step:4296 [D loss: 0.163137, acc.: 77.34%] [G loss: 0.622857]\n",
      "epoch:4 step:4297 [D loss: 0.162331, acc.: 75.00%] [G loss: 0.625993]\n",
      "epoch:4 step:4298 [D loss: 0.196845, acc.: 66.41%] [G loss: 0.529968]\n",
      "epoch:4 step:4299 [D loss: 0.205968, acc.: 67.97%] [G loss: 0.550758]\n",
      "epoch:4 step:4300 [D loss: 0.157282, acc.: 75.78%] [G loss: 0.610673]\n",
      "epoch:4 step:4301 [D loss: 0.207140, acc.: 68.75%] [G loss: 0.551480]\n",
      "epoch:4 step:4302 [D loss: 0.148864, acc.: 80.47%] [G loss: 0.608222]\n",
      "epoch:4 step:4303 [D loss: 0.169434, acc.: 78.12%] [G loss: 0.618900]\n",
      "epoch:4 step:4304 [D loss: 0.189183, acc.: 74.22%] [G loss: 0.561467]\n",
      "epoch:4 step:4305 [D loss: 0.201047, acc.: 68.75%] [G loss: 0.569889]\n",
      "epoch:4 step:4306 [D loss: 0.183632, acc.: 72.66%] [G loss: 0.593447]\n",
      "epoch:4 step:4307 [D loss: 0.218856, acc.: 65.62%] [G loss: 0.534319]\n",
      "epoch:4 step:4308 [D loss: 0.217646, acc.: 66.41%] [G loss: 0.520681]\n",
      "epoch:4 step:4309 [D loss: 0.154111, acc.: 78.12%] [G loss: 0.572498]\n",
      "epoch:4 step:4310 [D loss: 0.205654, acc.: 65.62%] [G loss: 0.565292]\n",
      "epoch:4 step:4311 [D loss: 0.194981, acc.: 71.88%] [G loss: 0.562303]\n",
      "epoch:4 step:4312 [D loss: 0.174661, acc.: 74.22%] [G loss: 0.664699]\n",
      "epoch:4 step:4313 [D loss: 0.191022, acc.: 70.31%] [G loss: 0.615577]\n",
      "epoch:4 step:4314 [D loss: 0.234404, acc.: 60.94%] [G loss: 0.562032]\n",
      "epoch:4 step:4315 [D loss: 0.156280, acc.: 78.91%] [G loss: 0.590467]\n",
      "epoch:4 step:4316 [D loss: 0.182004, acc.: 75.78%] [G loss: 0.610619]\n",
      "epoch:4 step:4317 [D loss: 0.247440, acc.: 59.38%] [G loss: 0.502372]\n",
      "epoch:4 step:4318 [D loss: 0.201561, acc.: 67.97%] [G loss: 0.551368]\n",
      "epoch:4 step:4319 [D loss: 0.182992, acc.: 71.09%] [G loss: 0.566887]\n",
      "epoch:4 step:4320 [D loss: 0.189061, acc.: 72.66%] [G loss: 0.581600]\n",
      "epoch:4 step:4321 [D loss: 0.197913, acc.: 69.53%] [G loss: 0.576128]\n",
      "epoch:4 step:4322 [D loss: 0.143873, acc.: 83.59%] [G loss: 0.622297]\n",
      "epoch:4 step:4323 [D loss: 0.188914, acc.: 71.88%] [G loss: 0.595066]\n",
      "epoch:4 step:4324 [D loss: 0.221026, acc.: 65.62%] [G loss: 0.555810]\n",
      "epoch:4 step:4325 [D loss: 0.212598, acc.: 67.19%] [G loss: 0.543985]\n",
      "epoch:4 step:4326 [D loss: 0.194066, acc.: 71.09%] [G loss: 0.574959]\n",
      "epoch:4 step:4327 [D loss: 0.211690, acc.: 64.06%] [G loss: 0.543289]\n",
      "epoch:4 step:4328 [D loss: 0.185737, acc.: 71.09%] [G loss: 0.565648]\n",
      "epoch:4 step:4329 [D loss: 0.191374, acc.: 69.53%] [G loss: 0.577190]\n",
      "epoch:4 step:4330 [D loss: 0.171533, acc.: 71.09%] [G loss: 0.556669]\n",
      "epoch:4 step:4331 [D loss: 0.214259, acc.: 67.97%] [G loss: 0.538475]\n",
      "epoch:4 step:4332 [D loss: 0.220107, acc.: 60.94%] [G loss: 0.595219]\n",
      "epoch:4 step:4333 [D loss: 0.204345, acc.: 67.19%] [G loss: 0.568539]\n",
      "epoch:4 step:4334 [D loss: 0.195625, acc.: 71.09%] [G loss: 0.557582]\n",
      "epoch:4 step:4335 [D loss: 0.177054, acc.: 77.34%] [G loss: 0.616742]\n",
      "epoch:4 step:4336 [D loss: 0.196206, acc.: 69.53%] [G loss: 0.608796]\n",
      "epoch:4 step:4337 [D loss: 0.176781, acc.: 75.78%] [G loss: 0.640102]\n",
      "epoch:4 step:4338 [D loss: 0.207809, acc.: 67.97%] [G loss: 0.592056]\n",
      "epoch:4 step:4339 [D loss: 0.204401, acc.: 66.41%] [G loss: 0.543647]\n",
      "epoch:4 step:4340 [D loss: 0.161984, acc.: 76.56%] [G loss: 0.604890]\n",
      "epoch:4 step:4341 [D loss: 0.212183, acc.: 64.84%] [G loss: 0.555331]\n",
      "epoch:4 step:4342 [D loss: 0.193867, acc.: 68.75%] [G loss: 0.532105]\n",
      "epoch:4 step:4343 [D loss: 0.194529, acc.: 70.31%] [G loss: 0.593980]\n",
      "epoch:4 step:4344 [D loss: 0.182332, acc.: 72.66%] [G loss: 0.561080]\n",
      "epoch:4 step:4345 [D loss: 0.189852, acc.: 72.66%] [G loss: 0.584453]\n",
      "epoch:4 step:4346 [D loss: 0.162874, acc.: 78.12%] [G loss: 0.622844]\n",
      "epoch:4 step:4347 [D loss: 0.205465, acc.: 66.41%] [G loss: 0.543497]\n",
      "epoch:4 step:4348 [D loss: 0.209307, acc.: 68.75%] [G loss: 0.517327]\n",
      "epoch:4 step:4349 [D loss: 0.214429, acc.: 61.72%] [G loss: 0.521694]\n",
      "epoch:4 step:4350 [D loss: 0.193123, acc.: 71.88%] [G loss: 0.547867]\n",
      "epoch:4 step:4351 [D loss: 0.183021, acc.: 74.22%] [G loss: 0.555491]\n",
      "epoch:4 step:4352 [D loss: 0.172977, acc.: 79.69%] [G loss: 0.552538]\n",
      "epoch:4 step:4353 [D loss: 0.199007, acc.: 71.09%] [G loss: 0.535786]\n",
      "epoch:4 step:4354 [D loss: 0.216598, acc.: 69.53%] [G loss: 0.510650]\n",
      "epoch:4 step:4355 [D loss: 0.213094, acc.: 64.84%] [G loss: 0.515119]\n",
      "epoch:4 step:4356 [D loss: 0.185889, acc.: 74.22%] [G loss: 0.556592]\n",
      "epoch:4 step:4357 [D loss: 0.185474, acc.: 71.09%] [G loss: 0.589444]\n",
      "epoch:4 step:4358 [D loss: 0.171620, acc.: 76.56%] [G loss: 0.571523]\n",
      "epoch:4 step:4359 [D loss: 0.157563, acc.: 78.91%] [G loss: 0.555879]\n",
      "epoch:4 step:4360 [D loss: 0.194654, acc.: 71.09%] [G loss: 0.524248]\n",
      "epoch:4 step:4361 [D loss: 0.152326, acc.: 79.69%] [G loss: 0.574183]\n",
      "epoch:4 step:4362 [D loss: 0.204546, acc.: 65.62%] [G loss: 0.563188]\n",
      "epoch:4 step:4363 [D loss: 0.223745, acc.: 62.50%] [G loss: 0.489214]\n",
      "epoch:4 step:4364 [D loss: 0.204508, acc.: 67.97%] [G loss: 0.553391]\n",
      "epoch:4 step:4365 [D loss: 0.189920, acc.: 71.88%] [G loss: 0.552717]\n",
      "epoch:4 step:4366 [D loss: 0.212392, acc.: 72.66%] [G loss: 0.537137]\n",
      "epoch:4 step:4367 [D loss: 0.170852, acc.: 72.66%] [G loss: 0.620564]\n",
      "epoch:4 step:4368 [D loss: 0.189599, acc.: 69.53%] [G loss: 0.528078]\n",
      "epoch:4 step:4369 [D loss: 0.199597, acc.: 74.22%] [G loss: 0.526234]\n",
      "epoch:4 step:4370 [D loss: 0.230227, acc.: 66.41%] [G loss: 0.509851]\n",
      "epoch:4 step:4371 [D loss: 0.147362, acc.: 79.69%] [G loss: 0.627553]\n",
      "epoch:4 step:4372 [D loss: 0.176396, acc.: 73.44%] [G loss: 0.590984]\n",
      "epoch:4 step:4373 [D loss: 0.243026, acc.: 56.25%] [G loss: 0.489988]\n",
      "epoch:4 step:4374 [D loss: 0.211741, acc.: 64.84%] [G loss: 0.532541]\n",
      "epoch:4 step:4375 [D loss: 0.185611, acc.: 71.09%] [G loss: 0.573528]\n",
      "epoch:4 step:4376 [D loss: 0.225989, acc.: 66.41%] [G loss: 0.491930]\n",
      "epoch:4 step:4377 [D loss: 0.174824, acc.: 77.34%] [G loss: 0.537303]\n",
      "epoch:4 step:4378 [D loss: 0.207827, acc.: 67.19%] [G loss: 0.516476]\n",
      "epoch:4 step:4379 [D loss: 0.156295, acc.: 81.25%] [G loss: 0.586992]\n",
      "epoch:4 step:4380 [D loss: 0.158608, acc.: 76.56%] [G loss: 0.632756]\n",
      "epoch:4 step:4381 [D loss: 0.166863, acc.: 78.91%] [G loss: 0.598288]\n",
      "epoch:4 step:4382 [D loss: 0.176206, acc.: 75.78%] [G loss: 0.567242]\n",
      "epoch:4 step:4383 [D loss: 0.165396, acc.: 74.22%] [G loss: 0.539531]\n",
      "epoch:4 step:4384 [D loss: 0.212659, acc.: 64.84%] [G loss: 0.505097]\n",
      "epoch:4 step:4385 [D loss: 0.178450, acc.: 77.34%] [G loss: 0.543982]\n",
      "epoch:4 step:4386 [D loss: 0.229543, acc.: 62.50%] [G loss: 0.477492]\n",
      "epoch:4 step:4387 [D loss: 0.164028, acc.: 79.69%] [G loss: 0.569098]\n",
      "epoch:4 step:4388 [D loss: 0.161417, acc.: 75.00%] [G loss: 0.631825]\n",
      "epoch:4 step:4389 [D loss: 0.202804, acc.: 69.53%] [G loss: 0.638055]\n",
      "epoch:4 step:4390 [D loss: 0.165443, acc.: 75.00%] [G loss: 0.594836]\n",
      "epoch:4 step:4391 [D loss: 0.196283, acc.: 72.66%] [G loss: 0.618722]\n",
      "epoch:4 step:4392 [D loss: 0.183905, acc.: 75.00%] [G loss: 0.527562]\n",
      "epoch:4 step:4393 [D loss: 0.216587, acc.: 64.84%] [G loss: 0.529060]\n",
      "epoch:4 step:4394 [D loss: 0.176062, acc.: 72.66%] [G loss: 0.562218]\n",
      "epoch:4 step:4395 [D loss: 0.195263, acc.: 70.31%] [G loss: 0.588693]\n",
      "epoch:4 step:4396 [D loss: 0.183385, acc.: 73.44%] [G loss: 0.688464]\n",
      "epoch:4 step:4397 [D loss: 0.209882, acc.: 69.53%] [G loss: 0.579168]\n",
      "epoch:4 step:4398 [D loss: 0.181761, acc.: 71.09%] [G loss: 0.580311]\n",
      "epoch:4 step:4399 [D loss: 0.203710, acc.: 70.31%] [G loss: 0.553608]\n",
      "epoch:4 step:4400 [D loss: 0.205899, acc.: 66.41%] [G loss: 0.625583]\n",
      "##############\n",
      "[2.81270029 1.35925589 6.70754292 5.17911051 4.20496962 5.93288509\n",
      " 4.67608557 4.96702623 4.88882573 3.6779751 ]\n",
      "##########\n",
      "epoch:4 step:4401 [D loss: 0.220676, acc.: 61.72%] [G loss: 0.527371]\n",
      "epoch:4 step:4402 [D loss: 0.178969, acc.: 73.44%] [G loss: 0.602111]\n",
      "epoch:4 step:4403 [D loss: 0.197428, acc.: 75.00%] [G loss: 0.558345]\n",
      "epoch:4 step:4404 [D loss: 0.193230, acc.: 66.41%] [G loss: 0.530701]\n",
      "epoch:4 step:4405 [D loss: 0.183356, acc.: 71.09%] [G loss: 0.541040]\n",
      "epoch:4 step:4406 [D loss: 0.228656, acc.: 65.62%] [G loss: 0.493614]\n",
      "epoch:4 step:4407 [D loss: 0.172996, acc.: 73.44%] [G loss: 0.533250]\n",
      "epoch:4 step:4408 [D loss: 0.188040, acc.: 71.09%] [G loss: 0.557567]\n",
      "epoch:4 step:4409 [D loss: 0.198119, acc.: 71.09%] [G loss: 0.564455]\n",
      "epoch:4 step:4410 [D loss: 0.159399, acc.: 79.69%] [G loss: 0.611810]\n",
      "epoch:4 step:4411 [D loss: 0.198817, acc.: 67.97%] [G loss: 0.593680]\n",
      "epoch:4 step:4412 [D loss: 0.186609, acc.: 71.09%] [G loss: 0.609909]\n",
      "epoch:4 step:4413 [D loss: 0.212901, acc.: 66.41%] [G loss: 0.541554]\n",
      "epoch:4 step:4414 [D loss: 0.203201, acc.: 67.97%] [G loss: 0.531525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4415 [D loss: 0.206883, acc.: 65.62%] [G loss: 0.542714]\n",
      "epoch:4 step:4416 [D loss: 0.208882, acc.: 68.75%] [G loss: 0.531150]\n",
      "epoch:4 step:4417 [D loss: 0.161793, acc.: 82.03%] [G loss: 0.617458]\n",
      "epoch:4 step:4418 [D loss: 0.197845, acc.: 68.75%] [G loss: 0.542255]\n",
      "epoch:4 step:4419 [D loss: 0.171877, acc.: 71.88%] [G loss: 0.576673]\n",
      "epoch:4 step:4420 [D loss: 0.228158, acc.: 61.72%] [G loss: 0.550764]\n",
      "epoch:4 step:4421 [D loss: 0.236590, acc.: 63.28%] [G loss: 0.595712]\n",
      "epoch:4 step:4422 [D loss: 0.169968, acc.: 73.44%] [G loss: 0.581452]\n",
      "epoch:4 step:4423 [D loss: 0.217221, acc.: 71.09%] [G loss: 0.506033]\n",
      "epoch:4 step:4424 [D loss: 0.204527, acc.: 68.75%] [G loss: 0.571609]\n",
      "epoch:4 step:4425 [D loss: 0.169662, acc.: 73.44%] [G loss: 0.592082]\n",
      "epoch:4 step:4426 [D loss: 0.176459, acc.: 77.34%] [G loss: 0.550686]\n",
      "epoch:4 step:4427 [D loss: 0.197558, acc.: 67.97%] [G loss: 0.554908]\n",
      "epoch:4 step:4428 [D loss: 0.170756, acc.: 77.34%] [G loss: 0.517250]\n",
      "epoch:4 step:4429 [D loss: 0.174983, acc.: 75.00%] [G loss: 0.535990]\n",
      "epoch:4 step:4430 [D loss: 0.186300, acc.: 71.09%] [G loss: 0.543688]\n",
      "epoch:4 step:4431 [D loss: 0.175366, acc.: 75.78%] [G loss: 0.560435]\n",
      "epoch:4 step:4432 [D loss: 0.196639, acc.: 68.75%] [G loss: 0.516820]\n",
      "epoch:4 step:4433 [D loss: 0.161937, acc.: 78.91%] [G loss: 0.553915]\n",
      "epoch:4 step:4434 [D loss: 0.222061, acc.: 71.09%] [G loss: 0.552074]\n",
      "epoch:4 step:4435 [D loss: 0.202883, acc.: 66.41%] [G loss: 0.533075]\n",
      "epoch:4 step:4436 [D loss: 0.198103, acc.: 69.53%] [G loss: 0.627147]\n",
      "epoch:4 step:4437 [D loss: 0.187553, acc.: 75.78%] [G loss: 0.627697]\n",
      "epoch:4 step:4438 [D loss: 0.187376, acc.: 72.66%] [G loss: 0.588437]\n",
      "epoch:4 step:4439 [D loss: 0.164932, acc.: 77.34%] [G loss: 0.591978]\n",
      "epoch:4 step:4440 [D loss: 0.156869, acc.: 80.47%] [G loss: 0.588170]\n",
      "epoch:4 step:4441 [D loss: 0.161711, acc.: 80.47%] [G loss: 0.583741]\n",
      "epoch:4 step:4442 [D loss: 0.151893, acc.: 79.69%] [G loss: 0.653609]\n",
      "epoch:4 step:4443 [D loss: 0.202809, acc.: 70.31%] [G loss: 0.590090]\n",
      "epoch:4 step:4444 [D loss: 0.197488, acc.: 67.97%] [G loss: 0.618181]\n",
      "epoch:4 step:4445 [D loss: 0.163542, acc.: 82.03%] [G loss: 0.648045]\n",
      "epoch:4 step:4446 [D loss: 0.216037, acc.: 64.06%] [G loss: 0.586637]\n",
      "epoch:4 step:4447 [D loss: 0.160098, acc.: 78.12%] [G loss: 0.622320]\n",
      "epoch:4 step:4448 [D loss: 0.171662, acc.: 77.34%] [G loss: 0.625465]\n",
      "epoch:4 step:4449 [D loss: 0.174633, acc.: 75.78%] [G loss: 0.594843]\n",
      "epoch:4 step:4450 [D loss: 0.249009, acc.: 56.25%] [G loss: 0.562462]\n",
      "epoch:4 step:4451 [D loss: 0.194433, acc.: 70.31%] [G loss: 0.558864]\n",
      "epoch:4 step:4452 [D loss: 0.200414, acc.: 69.53%] [G loss: 0.560922]\n",
      "epoch:4 step:4453 [D loss: 0.176382, acc.: 74.22%] [G loss: 0.563820]\n",
      "epoch:4 step:4454 [D loss: 0.178550, acc.: 76.56%] [G loss: 0.621896]\n",
      "epoch:4 step:4455 [D loss: 0.160400, acc.: 75.00%] [G loss: 0.605588]\n",
      "epoch:4 step:4456 [D loss: 0.184597, acc.: 72.66%] [G loss: 0.582190]\n",
      "epoch:4 step:4457 [D loss: 0.163221, acc.: 75.00%] [G loss: 0.612176]\n",
      "epoch:4 step:4458 [D loss: 0.217326, acc.: 67.97%] [G loss: 0.521080]\n",
      "epoch:4 step:4459 [D loss: 0.206233, acc.: 61.72%] [G loss: 0.547992]\n",
      "epoch:4 step:4460 [D loss: 0.177609, acc.: 79.69%] [G loss: 0.618777]\n",
      "epoch:4 step:4461 [D loss: 0.173476, acc.: 74.22%] [G loss: 0.606367]\n",
      "epoch:4 step:4462 [D loss: 0.181315, acc.: 73.44%] [G loss: 0.571350]\n",
      "epoch:4 step:4463 [D loss: 0.217448, acc.: 68.75%] [G loss: 0.527552]\n",
      "epoch:4 step:4464 [D loss: 0.222609, acc.: 62.50%] [G loss: 0.509895]\n",
      "epoch:4 step:4465 [D loss: 0.172687, acc.: 78.12%] [G loss: 0.529221]\n",
      "epoch:4 step:4466 [D loss: 0.225241, acc.: 64.06%] [G loss: 0.541365]\n",
      "epoch:4 step:4467 [D loss: 0.188741, acc.: 71.09%] [G loss: 0.582386]\n",
      "epoch:4 step:4468 [D loss: 0.198985, acc.: 72.66%] [G loss: 0.561391]\n",
      "epoch:4 step:4469 [D loss: 0.193477, acc.: 71.09%] [G loss: 0.526824]\n",
      "epoch:4 step:4470 [D loss: 0.210780, acc.: 69.53%] [G loss: 0.509725]\n",
      "epoch:4 step:4471 [D loss: 0.189759, acc.: 69.53%] [G loss: 0.588830]\n",
      "epoch:4 step:4472 [D loss: 0.199025, acc.: 67.19%] [G loss: 0.563013]\n",
      "epoch:4 step:4473 [D loss: 0.169642, acc.: 73.44%] [G loss: 0.645122]\n",
      "epoch:4 step:4474 [D loss: 0.229394, acc.: 66.41%] [G loss: 0.539658]\n",
      "epoch:4 step:4475 [D loss: 0.221805, acc.: 66.41%] [G loss: 0.506085]\n",
      "epoch:4 step:4476 [D loss: 0.178395, acc.: 75.00%] [G loss: 0.571875]\n",
      "epoch:4 step:4477 [D loss: 0.201773, acc.: 67.97%] [G loss: 0.533514]\n",
      "epoch:4 step:4478 [D loss: 0.158563, acc.: 81.25%] [G loss: 0.581659]\n",
      "epoch:4 step:4479 [D loss: 0.197081, acc.: 67.19%] [G loss: 0.556567]\n",
      "epoch:4 step:4480 [D loss: 0.176233, acc.: 77.34%] [G loss: 0.587447]\n",
      "epoch:4 step:4481 [D loss: 0.200751, acc.: 69.53%] [G loss: 0.562724]\n",
      "epoch:4 step:4482 [D loss: 0.216595, acc.: 72.66%] [G loss: 0.527429]\n",
      "epoch:4 step:4483 [D loss: 0.204951, acc.: 68.75%] [G loss: 0.528485]\n",
      "epoch:4 step:4484 [D loss: 0.189548, acc.: 66.41%] [G loss: 0.600131]\n",
      "epoch:4 step:4485 [D loss: 0.183202, acc.: 70.31%] [G loss: 0.571391]\n",
      "epoch:4 step:4486 [D loss: 0.236621, acc.: 60.16%] [G loss: 0.498060]\n",
      "epoch:4 step:4487 [D loss: 0.220734, acc.: 67.19%] [G loss: 0.543998]\n",
      "epoch:4 step:4488 [D loss: 0.197547, acc.: 69.53%] [G loss: 0.532706]\n",
      "epoch:4 step:4489 [D loss: 0.208480, acc.: 67.19%] [G loss: 0.569609]\n",
      "epoch:4 step:4490 [D loss: 0.229942, acc.: 64.06%] [G loss: 0.510071]\n",
      "epoch:4 step:4491 [D loss: 0.175772, acc.: 69.53%] [G loss: 0.553886]\n",
      "epoch:4 step:4492 [D loss: 0.207060, acc.: 70.31%] [G loss: 0.543085]\n",
      "epoch:4 step:4493 [D loss: 0.172437, acc.: 75.78%] [G loss: 0.543479]\n",
      "epoch:4 step:4494 [D loss: 0.194961, acc.: 70.31%] [G loss: 0.595189]\n",
      "epoch:4 step:4495 [D loss: 0.163668, acc.: 75.78%] [G loss: 0.592840]\n",
      "epoch:4 step:4496 [D loss: 0.179804, acc.: 74.22%] [G loss: 0.567755]\n",
      "epoch:4 step:4497 [D loss: 0.208962, acc.: 67.19%] [G loss: 0.559819]\n",
      "epoch:4 step:4498 [D loss: 0.192798, acc.: 70.31%] [G loss: 0.545026]\n",
      "epoch:4 step:4499 [D loss: 0.184077, acc.: 73.44%] [G loss: 0.553296]\n",
      "epoch:4 step:4500 [D loss: 0.183560, acc.: 69.53%] [G loss: 0.579148]\n",
      "epoch:4 step:4501 [D loss: 0.181616, acc.: 75.00%] [G loss: 0.501882]\n",
      "epoch:4 step:4502 [D loss: 0.154898, acc.: 75.78%] [G loss: 0.544219]\n",
      "epoch:4 step:4503 [D loss: 0.187862, acc.: 71.88%] [G loss: 0.552483]\n",
      "epoch:4 step:4504 [D loss: 0.198715, acc.: 67.19%] [G loss: 0.574333]\n",
      "epoch:4 step:4505 [D loss: 0.181954, acc.: 71.88%] [G loss: 0.587869]\n",
      "epoch:4 step:4506 [D loss: 0.198518, acc.: 71.88%] [G loss: 0.541814]\n",
      "epoch:4 step:4507 [D loss: 0.209972, acc.: 67.97%] [G loss: 0.552029]\n",
      "epoch:4 step:4508 [D loss: 0.188767, acc.: 73.44%] [G loss: 0.562023]\n",
      "epoch:4 step:4509 [D loss: 0.182204, acc.: 70.31%] [G loss: 0.554505]\n",
      "epoch:4 step:4510 [D loss: 0.189442, acc.: 73.44%] [G loss: 0.579699]\n",
      "epoch:4 step:4511 [D loss: 0.178979, acc.: 73.44%] [G loss: 0.588595]\n",
      "epoch:4 step:4512 [D loss: 0.175878, acc.: 74.22%] [G loss: 0.577014]\n",
      "epoch:4 step:4513 [D loss: 0.249064, acc.: 63.28%] [G loss: 0.508915]\n",
      "epoch:4 step:4514 [D loss: 0.229623, acc.: 61.72%] [G loss: 0.501816]\n",
      "epoch:4 step:4515 [D loss: 0.146956, acc.: 75.78%] [G loss: 0.532364]\n",
      "epoch:4 step:4516 [D loss: 0.191933, acc.: 72.66%] [G loss: 0.535463]\n",
      "epoch:4 step:4517 [D loss: 0.179340, acc.: 72.66%] [G loss: 0.580024]\n",
      "epoch:4 step:4518 [D loss: 0.202205, acc.: 66.41%] [G loss: 0.592098]\n",
      "epoch:4 step:4519 [D loss: 0.188702, acc.: 74.22%] [G loss: 0.599423]\n",
      "epoch:4 step:4520 [D loss: 0.191591, acc.: 67.19%] [G loss: 0.537123]\n",
      "epoch:4 step:4521 [D loss: 0.198408, acc.: 70.31%] [G loss: 0.545124]\n",
      "epoch:4 step:4522 [D loss: 0.234700, acc.: 61.72%] [G loss: 0.539923]\n",
      "epoch:4 step:4523 [D loss: 0.178720, acc.: 71.88%] [G loss: 0.575634]\n",
      "epoch:4 step:4524 [D loss: 0.223204, acc.: 62.50%] [G loss: 0.493725]\n",
      "epoch:4 step:4525 [D loss: 0.187482, acc.: 68.75%] [G loss: 0.535611]\n",
      "epoch:4 step:4526 [D loss: 0.175216, acc.: 75.78%] [G loss: 0.566696]\n",
      "epoch:4 step:4527 [D loss: 0.219771, acc.: 67.97%] [G loss: 0.513895]\n",
      "epoch:4 step:4528 [D loss: 0.149736, acc.: 82.03%] [G loss: 0.576641]\n",
      "epoch:4 step:4529 [D loss: 0.135424, acc.: 86.72%] [G loss: 0.670038]\n",
      "epoch:4 step:4530 [D loss: 0.187170, acc.: 74.22%] [G loss: 0.613551]\n",
      "epoch:4 step:4531 [D loss: 0.214261, acc.: 70.31%] [G loss: 0.605808]\n",
      "epoch:4 step:4532 [D loss: 0.232663, acc.: 64.06%] [G loss: 0.525859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4533 [D loss: 0.208305, acc.: 67.97%] [G loss: 0.480022]\n",
      "epoch:4 step:4534 [D loss: 0.167077, acc.: 77.34%] [G loss: 0.611624]\n",
      "epoch:4 step:4535 [D loss: 0.245582, acc.: 61.72%] [G loss: 0.534491]\n",
      "epoch:4 step:4536 [D loss: 0.230936, acc.: 65.62%] [G loss: 0.488813]\n",
      "epoch:4 step:4537 [D loss: 0.180924, acc.: 71.88%] [G loss: 0.571009]\n",
      "epoch:4 step:4538 [D loss: 0.187322, acc.: 74.22%] [G loss: 0.549426]\n",
      "epoch:4 step:4539 [D loss: 0.248960, acc.: 63.28%] [G loss: 0.505603]\n",
      "epoch:4 step:4540 [D loss: 0.138582, acc.: 82.81%] [G loss: 0.594196]\n",
      "epoch:4 step:4541 [D loss: 0.189823, acc.: 72.66%] [G loss: 0.595004]\n",
      "epoch:4 step:4542 [D loss: 0.222738, acc.: 65.62%] [G loss: 0.551153]\n",
      "epoch:4 step:4543 [D loss: 0.206863, acc.: 63.28%] [G loss: 0.564825]\n",
      "epoch:4 step:4544 [D loss: 0.172756, acc.: 75.00%] [G loss: 0.552096]\n",
      "epoch:4 step:4545 [D loss: 0.180110, acc.: 71.09%] [G loss: 0.572086]\n",
      "epoch:4 step:4546 [D loss: 0.200584, acc.: 68.75%] [G loss: 0.572750]\n",
      "epoch:4 step:4547 [D loss: 0.185228, acc.: 75.78%] [G loss: 0.579874]\n",
      "epoch:4 step:4548 [D loss: 0.196374, acc.: 75.00%] [G loss: 0.554415]\n",
      "epoch:4 step:4549 [D loss: 0.200779, acc.: 69.53%] [G loss: 0.544788]\n",
      "epoch:4 step:4550 [D loss: 0.165440, acc.: 75.78%] [G loss: 0.664199]\n",
      "epoch:4 step:4551 [D loss: 0.178126, acc.: 73.44%] [G loss: 0.598133]\n",
      "epoch:4 step:4552 [D loss: 0.206976, acc.: 67.97%] [G loss: 0.524906]\n",
      "epoch:4 step:4553 [D loss: 0.189956, acc.: 76.56%] [G loss: 0.597264]\n",
      "epoch:4 step:4554 [D loss: 0.180652, acc.: 70.31%] [G loss: 0.609827]\n",
      "epoch:4 step:4555 [D loss: 0.184866, acc.: 75.00%] [G loss: 0.603034]\n",
      "epoch:4 step:4556 [D loss: 0.171576, acc.: 73.44%] [G loss: 0.597564]\n",
      "epoch:4 step:4557 [D loss: 0.182454, acc.: 72.66%] [G loss: 0.568683]\n",
      "epoch:4 step:4558 [D loss: 0.173565, acc.: 73.44%] [G loss: 0.571320]\n",
      "epoch:4 step:4559 [D loss: 0.207273, acc.: 66.41%] [G loss: 0.609253]\n",
      "epoch:4 step:4560 [D loss: 0.251652, acc.: 56.25%] [G loss: 0.488884]\n",
      "epoch:4 step:4561 [D loss: 0.229265, acc.: 67.19%] [G loss: 0.508318]\n",
      "epoch:4 step:4562 [D loss: 0.185701, acc.: 76.56%] [G loss: 0.560021]\n",
      "epoch:4 step:4563 [D loss: 0.174672, acc.: 75.00%] [G loss: 0.629309]\n",
      "epoch:4 step:4564 [D loss: 0.192221, acc.: 71.88%] [G loss: 0.579276]\n",
      "epoch:4 step:4565 [D loss: 0.220251, acc.: 66.41%] [G loss: 0.519733]\n",
      "epoch:4 step:4566 [D loss: 0.194189, acc.: 71.09%] [G loss: 0.532773]\n",
      "epoch:4 step:4567 [D loss: 0.192556, acc.: 73.44%] [G loss: 0.513406]\n",
      "epoch:4 step:4568 [D loss: 0.213546, acc.: 69.53%] [G loss: 0.514158]\n",
      "epoch:4 step:4569 [D loss: 0.197423, acc.: 69.53%] [G loss: 0.520864]\n",
      "epoch:4 step:4570 [D loss: 0.179733, acc.: 72.66%] [G loss: 0.576328]\n",
      "epoch:4 step:4571 [D loss: 0.170298, acc.: 71.88%] [G loss: 0.578430]\n",
      "epoch:4 step:4572 [D loss: 0.242277, acc.: 64.84%] [G loss: 0.517521]\n",
      "epoch:4 step:4573 [D loss: 0.186156, acc.: 74.22%] [G loss: 0.565601]\n",
      "epoch:4 step:4574 [D loss: 0.184409, acc.: 71.88%] [G loss: 0.549954]\n",
      "epoch:4 step:4575 [D loss: 0.206571, acc.: 67.19%] [G loss: 0.530581]\n",
      "epoch:4 step:4576 [D loss: 0.237347, acc.: 60.94%] [G loss: 0.466830]\n",
      "epoch:4 step:4577 [D loss: 0.190769, acc.: 70.31%] [G loss: 0.548747]\n",
      "epoch:4 step:4578 [D loss: 0.202534, acc.: 73.44%] [G loss: 0.572701]\n",
      "epoch:4 step:4579 [D loss: 0.165459, acc.: 76.56%] [G loss: 0.586091]\n",
      "epoch:4 step:4580 [D loss: 0.205268, acc.: 70.31%] [G loss: 0.564867]\n",
      "epoch:4 step:4581 [D loss: 0.157845, acc.: 80.47%] [G loss: 0.592577]\n",
      "epoch:4 step:4582 [D loss: 0.171502, acc.: 76.56%] [G loss: 0.529767]\n",
      "epoch:4 step:4583 [D loss: 0.190649, acc.: 71.88%] [G loss: 0.545149]\n",
      "epoch:4 step:4584 [D loss: 0.194035, acc.: 69.53%] [G loss: 0.558993]\n",
      "epoch:4 step:4585 [D loss: 0.148726, acc.: 78.12%] [G loss: 0.642668]\n",
      "epoch:4 step:4586 [D loss: 0.189672, acc.: 71.09%] [G loss: 0.567073]\n",
      "epoch:4 step:4587 [D loss: 0.185509, acc.: 72.66%] [G loss: 0.575315]\n",
      "epoch:4 step:4588 [D loss: 0.192575, acc.: 71.88%] [G loss: 0.551698]\n",
      "epoch:4 step:4589 [D loss: 0.226234, acc.: 60.16%] [G loss: 0.561315]\n",
      "epoch:4 step:4590 [D loss: 0.133478, acc.: 86.72%] [G loss: 0.622283]\n",
      "epoch:4 step:4591 [D loss: 0.199135, acc.: 71.09%] [G loss: 0.549914]\n",
      "epoch:4 step:4592 [D loss: 0.210179, acc.: 69.53%] [G loss: 0.563737]\n",
      "epoch:4 step:4593 [D loss: 0.198961, acc.: 67.19%] [G loss: 0.567281]\n",
      "epoch:4 step:4594 [D loss: 0.199630, acc.: 71.09%] [G loss: 0.576167]\n",
      "epoch:4 step:4595 [D loss: 0.200671, acc.: 71.09%] [G loss: 0.518346]\n",
      "epoch:4 step:4596 [D loss: 0.216753, acc.: 64.84%] [G loss: 0.542388]\n",
      "epoch:4 step:4597 [D loss: 0.194404, acc.: 68.75%] [G loss: 0.530839]\n",
      "epoch:4 step:4598 [D loss: 0.196086, acc.: 71.09%] [G loss: 0.532762]\n",
      "epoch:4 step:4599 [D loss: 0.203129, acc.: 64.84%] [G loss: 0.515223]\n",
      "epoch:4 step:4600 [D loss: 0.162544, acc.: 76.56%] [G loss: 0.531814]\n",
      "##############\n",
      "[2.90008035 1.44247164 6.7324433  5.00527068 4.2401579  6.05357205\n",
      " 5.08950282 4.77876193 5.06529292 3.65427548]\n",
      "##########\n",
      "epoch:4 step:4601 [D loss: 0.184198, acc.: 76.56%] [G loss: 0.554166]\n",
      "epoch:4 step:4602 [D loss: 0.182293, acc.: 73.44%] [G loss: 0.611876]\n",
      "epoch:4 step:4603 [D loss: 0.207390, acc.: 68.75%] [G loss: 0.505371]\n",
      "epoch:4 step:4604 [D loss: 0.222175, acc.: 64.06%] [G loss: 0.510748]\n",
      "epoch:4 step:4605 [D loss: 0.171970, acc.: 73.44%] [G loss: 0.567694]\n",
      "epoch:4 step:4606 [D loss: 0.273102, acc.: 53.91%] [G loss: 0.505760]\n",
      "epoch:4 step:4607 [D loss: 0.183558, acc.: 75.78%] [G loss: 0.609370]\n",
      "epoch:4 step:4608 [D loss: 0.173118, acc.: 75.78%] [G loss: 0.655175]\n",
      "epoch:4 step:4609 [D loss: 0.229327, acc.: 62.50%] [G loss: 0.567356]\n",
      "epoch:4 step:4610 [D loss: 0.216073, acc.: 63.28%] [G loss: 0.507313]\n",
      "epoch:4 step:4611 [D loss: 0.201393, acc.: 70.31%] [G loss: 0.514012]\n",
      "epoch:4 step:4612 [D loss: 0.204685, acc.: 71.09%] [G loss: 0.573450]\n",
      "epoch:4 step:4613 [D loss: 0.209012, acc.: 64.06%] [G loss: 0.522974]\n",
      "epoch:4 step:4614 [D loss: 0.203987, acc.: 65.62%] [G loss: 0.541896]\n",
      "epoch:4 step:4615 [D loss: 0.226085, acc.: 64.84%] [G loss: 0.544596]\n",
      "epoch:4 step:4616 [D loss: 0.184224, acc.: 75.78%] [G loss: 0.546229]\n",
      "epoch:4 step:4617 [D loss: 0.222090, acc.: 64.06%] [G loss: 0.495224]\n",
      "epoch:4 step:4618 [D loss: 0.170012, acc.: 72.66%] [G loss: 0.590757]\n",
      "epoch:4 step:4619 [D loss: 0.159973, acc.: 78.12%] [G loss: 0.583568]\n",
      "epoch:4 step:4620 [D loss: 0.199591, acc.: 69.53%] [G loss: 0.559624]\n",
      "epoch:4 step:4621 [D loss: 0.215545, acc.: 64.84%] [G loss: 0.518865]\n",
      "epoch:4 step:4622 [D loss: 0.228118, acc.: 61.72%] [G loss: 0.511716]\n",
      "epoch:4 step:4623 [D loss: 0.224296, acc.: 65.62%] [G loss: 0.567393]\n",
      "epoch:4 step:4624 [D loss: 0.193170, acc.: 68.75%] [G loss: 0.585240]\n",
      "epoch:4 step:4625 [D loss: 0.214859, acc.: 66.41%] [G loss: 0.548165]\n",
      "epoch:4 step:4626 [D loss: 0.182436, acc.: 74.22%] [G loss: 0.564481]\n",
      "epoch:4 step:4627 [D loss: 0.200003, acc.: 69.53%] [G loss: 0.581093]\n",
      "epoch:4 step:4628 [D loss: 0.254930, acc.: 57.81%] [G loss: 0.481235]\n",
      "epoch:4 step:4629 [D loss: 0.199165, acc.: 71.09%] [G loss: 0.539920]\n",
      "epoch:4 step:4630 [D loss: 0.204073, acc.: 67.19%] [G loss: 0.536520]\n",
      "epoch:4 step:4631 [D loss: 0.211637, acc.: 69.53%] [G loss: 0.602111]\n",
      "epoch:4 step:4632 [D loss: 0.153176, acc.: 78.12%] [G loss: 0.612518]\n",
      "epoch:4 step:4633 [D loss: 0.198206, acc.: 67.19%] [G loss: 0.572581]\n",
      "epoch:4 step:4634 [D loss: 0.175437, acc.: 73.44%] [G loss: 0.608147]\n",
      "epoch:4 step:4635 [D loss: 0.213159, acc.: 70.31%] [G loss: 0.527639]\n",
      "epoch:4 step:4636 [D loss: 0.163111, acc.: 78.12%] [G loss: 0.606783]\n",
      "epoch:4 step:4637 [D loss: 0.173647, acc.: 72.66%] [G loss: 0.533271]\n",
      "epoch:4 step:4638 [D loss: 0.146437, acc.: 83.59%] [G loss: 0.592397]\n",
      "epoch:4 step:4639 [D loss: 0.232932, acc.: 62.50%] [G loss: 0.513794]\n",
      "epoch:4 step:4640 [D loss: 0.236005, acc.: 64.06%] [G loss: 0.511866]\n",
      "epoch:4 step:4641 [D loss: 0.171296, acc.: 75.78%] [G loss: 0.569188]\n",
      "epoch:4 step:4642 [D loss: 0.167504, acc.: 75.78%] [G loss: 0.617675]\n",
      "epoch:4 step:4643 [D loss: 0.200096, acc.: 72.66%] [G loss: 0.581391]\n",
      "epoch:4 step:4644 [D loss: 0.195009, acc.: 68.75%] [G loss: 0.537887]\n",
      "epoch:4 step:4645 [D loss: 0.175219, acc.: 73.44%] [G loss: 0.553785]\n",
      "epoch:4 step:4646 [D loss: 0.200017, acc.: 71.09%] [G loss: 0.551354]\n",
      "epoch:4 step:4647 [D loss: 0.158811, acc.: 79.69%] [G loss: 0.587265]\n",
      "epoch:4 step:4648 [D loss: 0.180303, acc.: 73.44%] [G loss: 0.554052]\n",
      "epoch:4 step:4649 [D loss: 0.199118, acc.: 71.09%] [G loss: 0.511465]\n",
      "epoch:4 step:4650 [D loss: 0.224483, acc.: 67.19%] [G loss: 0.563638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4651 [D loss: 0.210526, acc.: 66.41%] [G loss: 0.525759]\n",
      "epoch:4 step:4652 [D loss: 0.173068, acc.: 77.34%] [G loss: 0.557986]\n",
      "epoch:4 step:4653 [D loss: 0.196489, acc.: 70.31%] [G loss: 0.580999]\n",
      "epoch:4 step:4654 [D loss: 0.181214, acc.: 72.66%] [G loss: 0.651883]\n",
      "epoch:4 step:4655 [D loss: 0.207030, acc.: 71.88%] [G loss: 0.519489]\n",
      "epoch:4 step:4656 [D loss: 0.180742, acc.: 72.66%] [G loss: 0.521990]\n",
      "epoch:4 step:4657 [D loss: 0.198627, acc.: 68.75%] [G loss: 0.570267]\n",
      "epoch:4 step:4658 [D loss: 0.184530, acc.: 73.44%] [G loss: 0.550606]\n",
      "epoch:4 step:4659 [D loss: 0.165647, acc.: 76.56%] [G loss: 0.619256]\n",
      "epoch:4 step:4660 [D loss: 0.118439, acc.: 83.59%] [G loss: 0.671863]\n",
      "epoch:4 step:4661 [D loss: 0.230043, acc.: 67.19%] [G loss: 0.613950]\n",
      "epoch:4 step:4662 [D loss: 0.178629, acc.: 71.88%] [G loss: 0.592571]\n",
      "epoch:4 step:4663 [D loss: 0.209411, acc.: 68.75%] [G loss: 0.518221]\n",
      "epoch:4 step:4664 [D loss: 0.180981, acc.: 74.22%] [G loss: 0.606990]\n",
      "epoch:4 step:4665 [D loss: 0.204082, acc.: 65.62%] [G loss: 0.546227]\n",
      "epoch:4 step:4666 [D loss: 0.180021, acc.: 78.12%] [G loss: 0.608666]\n",
      "epoch:4 step:4667 [D loss: 0.195645, acc.: 66.41%] [G loss: 0.585837]\n",
      "epoch:4 step:4668 [D loss: 0.297524, acc.: 51.56%] [G loss: 0.458930]\n",
      "epoch:4 step:4669 [D loss: 0.173196, acc.: 73.44%] [G loss: 0.586063]\n",
      "epoch:4 step:4670 [D loss: 0.230368, acc.: 67.97%] [G loss: 0.533876]\n",
      "epoch:4 step:4671 [D loss: 0.166005, acc.: 76.56%] [G loss: 0.626784]\n",
      "epoch:4 step:4672 [D loss: 0.138925, acc.: 87.50%] [G loss: 0.690840]\n",
      "epoch:4 step:4673 [D loss: 0.114576, acc.: 86.72%] [G loss: 0.734504]\n",
      "epoch:4 step:4674 [D loss: 0.128792, acc.: 81.25%] [G loss: 0.675032]\n",
      "epoch:4 step:4675 [D loss: 0.150729, acc.: 75.00%] [G loss: 0.707263]\n",
      "epoch:4 step:4676 [D loss: 0.321988, acc.: 60.94%] [G loss: 0.546453]\n",
      "epoch:4 step:4677 [D loss: 0.113687, acc.: 83.59%] [G loss: 0.758583]\n",
      "epoch:4 step:4678 [D loss: 0.238211, acc.: 64.84%] [G loss: 0.598181]\n",
      "epoch:4 step:4679 [D loss: 0.188458, acc.: 68.75%] [G loss: 0.586701]\n",
      "epoch:4 step:4680 [D loss: 0.202558, acc.: 67.97%] [G loss: 0.573734]\n",
      "epoch:4 step:4681 [D loss: 0.151659, acc.: 79.69%] [G loss: 0.592264]\n",
      "epoch:4 step:4682 [D loss: 0.214378, acc.: 71.09%] [G loss: 0.577865]\n",
      "epoch:4 step:4683 [D loss: 0.183867, acc.: 68.75%] [G loss: 0.600334]\n",
      "epoch:4 step:4684 [D loss: 0.128500, acc.: 83.59%] [G loss: 0.695222]\n",
      "epoch:4 step:4685 [D loss: 0.155248, acc.: 77.34%] [G loss: 0.618216]\n",
      "epoch:5 step:4686 [D loss: 0.216438, acc.: 66.41%] [G loss: 0.686955]\n",
      "epoch:5 step:4687 [D loss: 0.216400, acc.: 70.31%] [G loss: 0.629703]\n",
      "epoch:5 step:4688 [D loss: 0.213333, acc.: 61.72%] [G loss: 0.638964]\n",
      "epoch:5 step:4689 [D loss: 0.191647, acc.: 72.66%] [G loss: 0.604887]\n",
      "epoch:5 step:4690 [D loss: 0.198157, acc.: 64.84%] [G loss: 0.559539]\n",
      "epoch:5 step:4691 [D loss: 0.169174, acc.: 74.22%] [G loss: 0.656323]\n",
      "epoch:5 step:4692 [D loss: 0.168040, acc.: 75.78%] [G loss: 0.602996]\n",
      "epoch:5 step:4693 [D loss: 0.192761, acc.: 71.09%] [G loss: 0.556538]\n",
      "epoch:5 step:4694 [D loss: 0.176600, acc.: 72.66%] [G loss: 0.582863]\n",
      "epoch:5 step:4695 [D loss: 0.185272, acc.: 75.00%] [G loss: 0.604196]\n",
      "epoch:5 step:4696 [D loss: 0.185771, acc.: 66.41%] [G loss: 0.611100]\n",
      "epoch:5 step:4697 [D loss: 0.195499, acc.: 73.44%] [G loss: 0.546613]\n",
      "epoch:5 step:4698 [D loss: 0.196859, acc.: 70.31%] [G loss: 0.535568]\n",
      "epoch:5 step:4699 [D loss: 0.181925, acc.: 73.44%] [G loss: 0.552451]\n",
      "epoch:5 step:4700 [D loss: 0.158496, acc.: 76.56%] [G loss: 0.629913]\n",
      "epoch:5 step:4701 [D loss: 0.168586, acc.: 75.78%] [G loss: 0.587526]\n",
      "epoch:5 step:4702 [D loss: 0.245608, acc.: 60.16%] [G loss: 0.495507]\n",
      "epoch:5 step:4703 [D loss: 0.234057, acc.: 63.28%] [G loss: 0.520393]\n",
      "epoch:5 step:4704 [D loss: 0.203693, acc.: 69.53%] [G loss: 0.584874]\n",
      "epoch:5 step:4705 [D loss: 0.215689, acc.: 63.28%] [G loss: 0.542820]\n",
      "epoch:5 step:4706 [D loss: 0.204167, acc.: 69.53%] [G loss: 0.553217]\n",
      "epoch:5 step:4707 [D loss: 0.172674, acc.: 76.56%] [G loss: 0.593324]\n",
      "epoch:5 step:4708 [D loss: 0.185189, acc.: 71.88%] [G loss: 0.569181]\n",
      "epoch:5 step:4709 [D loss: 0.172311, acc.: 79.69%] [G loss: 0.572201]\n",
      "epoch:5 step:4710 [D loss: 0.171119, acc.: 75.78%] [G loss: 0.573296]\n",
      "epoch:5 step:4711 [D loss: 0.185116, acc.: 75.78%] [G loss: 0.587145]\n",
      "epoch:5 step:4712 [D loss: 0.177386, acc.: 71.09%] [G loss: 0.596238]\n",
      "epoch:5 step:4713 [D loss: 0.183384, acc.: 71.09%] [G loss: 0.563710]\n",
      "epoch:5 step:4714 [D loss: 0.178011, acc.: 76.56%] [G loss: 0.581614]\n",
      "epoch:5 step:4715 [D loss: 0.195735, acc.: 69.53%] [G loss: 0.570289]\n",
      "epoch:5 step:4716 [D loss: 0.210101, acc.: 65.62%] [G loss: 0.543531]\n",
      "epoch:5 step:4717 [D loss: 0.192637, acc.: 72.66%] [G loss: 0.549838]\n",
      "epoch:5 step:4718 [D loss: 0.142594, acc.: 81.25%] [G loss: 0.621817]\n",
      "epoch:5 step:4719 [D loss: 0.210659, acc.: 64.84%] [G loss: 0.569831]\n",
      "epoch:5 step:4720 [D loss: 0.232225, acc.: 63.28%] [G loss: 0.538862]\n",
      "epoch:5 step:4721 [D loss: 0.153982, acc.: 78.12%] [G loss: 0.623024]\n",
      "epoch:5 step:4722 [D loss: 0.203459, acc.: 72.66%] [G loss: 0.570606]\n",
      "epoch:5 step:4723 [D loss: 0.205663, acc.: 68.75%] [G loss: 0.577725]\n",
      "epoch:5 step:4724 [D loss: 0.168779, acc.: 75.00%] [G loss: 0.623414]\n",
      "epoch:5 step:4725 [D loss: 0.140361, acc.: 82.03%] [G loss: 0.656225]\n",
      "epoch:5 step:4726 [D loss: 0.194038, acc.: 67.97%] [G loss: 0.589544]\n",
      "epoch:5 step:4727 [D loss: 0.164601, acc.: 78.91%] [G loss: 0.571720]\n",
      "epoch:5 step:4728 [D loss: 0.196716, acc.: 67.19%] [G loss: 0.534049]\n",
      "epoch:5 step:4729 [D loss: 0.219052, acc.: 62.50%] [G loss: 0.556890]\n",
      "epoch:5 step:4730 [D loss: 0.200411, acc.: 66.41%] [G loss: 0.607965]\n",
      "epoch:5 step:4731 [D loss: 0.212227, acc.: 70.31%] [G loss: 0.578585]\n",
      "epoch:5 step:4732 [D loss: 0.187764, acc.: 71.88%] [G loss: 0.567103]\n",
      "epoch:5 step:4733 [D loss: 0.174107, acc.: 78.91%] [G loss: 0.623231]\n",
      "epoch:5 step:4734 [D loss: 0.178111, acc.: 74.22%] [G loss: 0.641605]\n",
      "epoch:5 step:4735 [D loss: 0.188580, acc.: 67.97%] [G loss: 0.586079]\n",
      "epoch:5 step:4736 [D loss: 0.191580, acc.: 73.44%] [G loss: 0.542157]\n",
      "epoch:5 step:4737 [D loss: 0.183185, acc.: 73.44%] [G loss: 0.565782]\n",
      "epoch:5 step:4738 [D loss: 0.166223, acc.: 76.56%] [G loss: 0.632450]\n",
      "epoch:5 step:4739 [D loss: 0.168151, acc.: 75.00%] [G loss: 0.579246]\n",
      "epoch:5 step:4740 [D loss: 0.204996, acc.: 64.06%] [G loss: 0.573827]\n",
      "epoch:5 step:4741 [D loss: 0.174144, acc.: 75.78%] [G loss: 0.611512]\n",
      "epoch:5 step:4742 [D loss: 0.195583, acc.: 67.19%] [G loss: 0.538899]\n",
      "epoch:5 step:4743 [D loss: 0.199701, acc.: 68.75%] [G loss: 0.548481]\n",
      "epoch:5 step:4744 [D loss: 0.163655, acc.: 77.34%] [G loss: 0.612502]\n",
      "epoch:5 step:4745 [D loss: 0.200850, acc.: 67.19%] [G loss: 0.576143]\n",
      "epoch:5 step:4746 [D loss: 0.239626, acc.: 64.84%] [G loss: 0.536690]\n",
      "epoch:5 step:4747 [D loss: 0.188205, acc.: 75.00%] [G loss: 0.617396]\n",
      "epoch:5 step:4748 [D loss: 0.206153, acc.: 66.41%] [G loss: 0.552400]\n",
      "epoch:5 step:4749 [D loss: 0.171477, acc.: 73.44%] [G loss: 0.544598]\n",
      "epoch:5 step:4750 [D loss: 0.205098, acc.: 67.19%] [G loss: 0.559437]\n",
      "epoch:5 step:4751 [D loss: 0.206052, acc.: 67.19%] [G loss: 0.547523]\n",
      "epoch:5 step:4752 [D loss: 0.169173, acc.: 73.44%] [G loss: 0.581680]\n",
      "epoch:5 step:4753 [D loss: 0.200041, acc.: 69.53%] [G loss: 0.512947]\n",
      "epoch:5 step:4754 [D loss: 0.194319, acc.: 69.53%] [G loss: 0.572633]\n",
      "epoch:5 step:4755 [D loss: 0.174930, acc.: 75.78%] [G loss: 0.591830]\n",
      "epoch:5 step:4756 [D loss: 0.178442, acc.: 71.88%] [G loss: 0.577088]\n",
      "epoch:5 step:4757 [D loss: 0.194654, acc.: 71.09%] [G loss: 0.536562]\n",
      "epoch:5 step:4758 [D loss: 0.168775, acc.: 75.00%] [G loss: 0.607377]\n",
      "epoch:5 step:4759 [D loss: 0.161485, acc.: 80.47%] [G loss: 0.557481]\n",
      "epoch:5 step:4760 [D loss: 0.183702, acc.: 72.66%] [G loss: 0.559975]\n",
      "epoch:5 step:4761 [D loss: 0.154032, acc.: 79.69%] [G loss: 0.633508]\n",
      "epoch:5 step:4762 [D loss: 0.150144, acc.: 83.59%] [G loss: 0.622459]\n",
      "epoch:5 step:4763 [D loss: 0.280717, acc.: 58.59%] [G loss: 0.479235]\n",
      "epoch:5 step:4764 [D loss: 0.196038, acc.: 72.66%] [G loss: 0.514534]\n",
      "epoch:5 step:4765 [D loss: 0.175859, acc.: 70.31%] [G loss: 0.580236]\n",
      "epoch:5 step:4766 [D loss: 0.223749, acc.: 64.84%] [G loss: 0.511051]\n",
      "epoch:5 step:4767 [D loss: 0.155013, acc.: 77.34%] [G loss: 0.627949]\n",
      "epoch:5 step:4768 [D loss: 0.164322, acc.: 76.56%] [G loss: 0.594934]\n",
      "epoch:5 step:4769 [D loss: 0.192890, acc.: 68.75%] [G loss: 0.569486]\n",
      "epoch:5 step:4770 [D loss: 0.215477, acc.: 65.62%] [G loss: 0.549032]\n",
      "epoch:5 step:4771 [D loss: 0.180968, acc.: 74.22%] [G loss: 0.569847]\n",
      "epoch:5 step:4772 [D loss: 0.211785, acc.: 71.88%] [G loss: 0.548066]\n",
      "epoch:5 step:4773 [D loss: 0.163145, acc.: 78.91%] [G loss: 0.584362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4774 [D loss: 0.201595, acc.: 67.19%] [G loss: 0.546751]\n",
      "epoch:5 step:4775 [D loss: 0.192344, acc.: 68.75%] [G loss: 0.561517]\n",
      "epoch:5 step:4776 [D loss: 0.207034, acc.: 67.97%] [G loss: 0.555043]\n",
      "epoch:5 step:4777 [D loss: 0.185391, acc.: 71.88%] [G loss: 0.591458]\n",
      "epoch:5 step:4778 [D loss: 0.181619, acc.: 75.00%] [G loss: 0.595335]\n",
      "epoch:5 step:4779 [D loss: 0.194332, acc.: 68.75%] [G loss: 0.526676]\n",
      "epoch:5 step:4780 [D loss: 0.200986, acc.: 68.75%] [G loss: 0.560876]\n",
      "epoch:5 step:4781 [D loss: 0.184053, acc.: 71.88%] [G loss: 0.565630]\n",
      "epoch:5 step:4782 [D loss: 0.169794, acc.: 77.34%] [G loss: 0.595160]\n",
      "epoch:5 step:4783 [D loss: 0.204138, acc.: 67.19%] [G loss: 0.552352]\n",
      "epoch:5 step:4784 [D loss: 0.199659, acc.: 68.75%] [G loss: 0.544702]\n",
      "epoch:5 step:4785 [D loss: 0.174076, acc.: 77.34%] [G loss: 0.604647]\n",
      "epoch:5 step:4786 [D loss: 0.200909, acc.: 65.62%] [G loss: 0.616326]\n",
      "epoch:5 step:4787 [D loss: 0.226133, acc.: 63.28%] [G loss: 0.529300]\n",
      "epoch:5 step:4788 [D loss: 0.181618, acc.: 74.22%] [G loss: 0.527014]\n",
      "epoch:5 step:4789 [D loss: 0.167342, acc.: 81.25%] [G loss: 0.585213]\n",
      "epoch:5 step:4790 [D loss: 0.209192, acc.: 71.88%] [G loss: 0.557839]\n",
      "epoch:5 step:4791 [D loss: 0.177346, acc.: 76.56%] [G loss: 0.566783]\n",
      "epoch:5 step:4792 [D loss: 0.242124, acc.: 57.81%] [G loss: 0.586739]\n",
      "epoch:5 step:4793 [D loss: 0.251567, acc.: 57.03%] [G loss: 0.490514]\n",
      "epoch:5 step:4794 [D loss: 0.201329, acc.: 67.97%] [G loss: 0.539525]\n",
      "epoch:5 step:4795 [D loss: 0.202119, acc.: 69.53%] [G loss: 0.539626]\n",
      "epoch:5 step:4796 [D loss: 0.171025, acc.: 78.91%] [G loss: 0.592458]\n",
      "epoch:5 step:4797 [D loss: 0.188045, acc.: 69.53%] [G loss: 0.552949]\n",
      "epoch:5 step:4798 [D loss: 0.212790, acc.: 69.53%] [G loss: 0.566434]\n",
      "epoch:5 step:4799 [D loss: 0.183349, acc.: 76.56%] [G loss: 0.541048]\n",
      "epoch:5 step:4800 [D loss: 0.190805, acc.: 72.66%] [G loss: 0.543448]\n",
      "##############\n",
      "[2.46300594 1.30804109 6.75106711 5.06043349 3.96071126 6.26260555\n",
      " 4.51636734 4.70803327 4.939151   3.89829352]\n",
      "##########\n",
      "epoch:5 step:4801 [D loss: 0.209537, acc.: 62.50%] [G loss: 0.629444]\n",
      "epoch:5 step:4802 [D loss: 0.212250, acc.: 66.41%] [G loss: 0.566676]\n",
      "epoch:5 step:4803 [D loss: 0.223032, acc.: 63.28%] [G loss: 0.550911]\n",
      "epoch:5 step:4804 [D loss: 0.171974, acc.: 75.00%] [G loss: 0.594885]\n",
      "epoch:5 step:4805 [D loss: 0.280858, acc.: 54.69%] [G loss: 0.489468]\n",
      "epoch:5 step:4806 [D loss: 0.190973, acc.: 75.00%] [G loss: 0.537259]\n",
      "epoch:5 step:4807 [D loss: 0.162600, acc.: 76.56%] [G loss: 0.605165]\n",
      "epoch:5 step:4808 [D loss: 0.217591, acc.: 65.62%] [G loss: 0.592885]\n",
      "epoch:5 step:4809 [D loss: 0.223935, acc.: 67.97%] [G loss: 0.578472]\n",
      "epoch:5 step:4810 [D loss: 0.195718, acc.: 71.09%] [G loss: 0.582182]\n",
      "epoch:5 step:4811 [D loss: 0.166693, acc.: 78.91%] [G loss: 0.571213]\n",
      "epoch:5 step:4812 [D loss: 0.186534, acc.: 75.00%] [G loss: 0.567609]\n",
      "epoch:5 step:4813 [D loss: 0.195213, acc.: 69.53%] [G loss: 0.503838]\n",
      "epoch:5 step:4814 [D loss: 0.216985, acc.: 64.84%] [G loss: 0.516047]\n",
      "epoch:5 step:4815 [D loss: 0.168670, acc.: 78.12%] [G loss: 0.557149]\n",
      "epoch:5 step:4816 [D loss: 0.186516, acc.: 71.88%] [G loss: 0.564136]\n",
      "epoch:5 step:4817 [D loss: 0.186521, acc.: 73.44%] [G loss: 0.533259]\n",
      "epoch:5 step:4818 [D loss: 0.210176, acc.: 70.31%] [G loss: 0.515847]\n",
      "epoch:5 step:4819 [D loss: 0.180542, acc.: 71.88%] [G loss: 0.556276]\n",
      "epoch:5 step:4820 [D loss: 0.169750, acc.: 74.22%] [G loss: 0.563877]\n",
      "epoch:5 step:4821 [D loss: 0.188074, acc.: 75.78%] [G loss: 0.569807]\n",
      "epoch:5 step:4822 [D loss: 0.166932, acc.: 73.44%] [G loss: 0.545000]\n",
      "epoch:5 step:4823 [D loss: 0.208091, acc.: 67.97%] [G loss: 0.508593]\n",
      "epoch:5 step:4824 [D loss: 0.217444, acc.: 68.75%] [G loss: 0.563897]\n",
      "epoch:5 step:4825 [D loss: 0.175181, acc.: 76.56%] [G loss: 0.616564]\n",
      "epoch:5 step:4826 [D loss: 0.165533, acc.: 78.91%] [G loss: 0.558562]\n",
      "epoch:5 step:4827 [D loss: 0.222931, acc.: 67.97%] [G loss: 0.520468]\n",
      "epoch:5 step:4828 [D loss: 0.215129, acc.: 67.97%] [G loss: 0.517146]\n",
      "epoch:5 step:4829 [D loss: 0.160132, acc.: 82.03%] [G loss: 0.627992]\n",
      "epoch:5 step:4830 [D loss: 0.194186, acc.: 69.53%] [G loss: 0.568677]\n",
      "epoch:5 step:4831 [D loss: 0.202988, acc.: 66.41%] [G loss: 0.531130]\n",
      "epoch:5 step:4832 [D loss: 0.227619, acc.: 59.38%] [G loss: 0.501619]\n",
      "epoch:5 step:4833 [D loss: 0.205989, acc.: 71.09%] [G loss: 0.534566]\n",
      "epoch:5 step:4834 [D loss: 0.177256, acc.: 74.22%] [G loss: 0.578965]\n",
      "epoch:5 step:4835 [D loss: 0.223209, acc.: 68.75%] [G loss: 0.514959]\n",
      "epoch:5 step:4836 [D loss: 0.183307, acc.: 71.88%] [G loss: 0.511650]\n",
      "epoch:5 step:4837 [D loss: 0.154307, acc.: 82.03%] [G loss: 0.612439]\n",
      "epoch:5 step:4838 [D loss: 0.227368, acc.: 66.41%] [G loss: 0.525741]\n",
      "epoch:5 step:4839 [D loss: 0.213834, acc.: 68.75%] [G loss: 0.568313]\n",
      "epoch:5 step:4840 [D loss: 0.145731, acc.: 82.81%] [G loss: 0.638603]\n",
      "epoch:5 step:4841 [D loss: 0.185963, acc.: 73.44%] [G loss: 0.591477]\n",
      "epoch:5 step:4842 [D loss: 0.203218, acc.: 72.66%] [G loss: 0.502047]\n",
      "epoch:5 step:4843 [D loss: 0.187799, acc.: 75.00%] [G loss: 0.553616]\n",
      "epoch:5 step:4844 [D loss: 0.176917, acc.: 77.34%] [G loss: 0.599508]\n",
      "epoch:5 step:4845 [D loss: 0.229291, acc.: 67.97%] [G loss: 0.548261]\n",
      "epoch:5 step:4846 [D loss: 0.188947, acc.: 73.44%] [G loss: 0.593079]\n",
      "epoch:5 step:4847 [D loss: 0.185776, acc.: 73.44%] [G loss: 0.592315]\n",
      "epoch:5 step:4848 [D loss: 0.179940, acc.: 72.66%] [G loss: 0.557144]\n",
      "epoch:5 step:4849 [D loss: 0.207762, acc.: 71.88%] [G loss: 0.558863]\n",
      "epoch:5 step:4850 [D loss: 0.194582, acc.: 68.75%] [G loss: 0.547628]\n",
      "epoch:5 step:4851 [D loss: 0.188871, acc.: 72.66%] [G loss: 0.592263]\n",
      "epoch:5 step:4852 [D loss: 0.222477, acc.: 61.72%] [G loss: 0.498347]\n",
      "epoch:5 step:4853 [D loss: 0.178869, acc.: 74.22%] [G loss: 0.582569]\n",
      "epoch:5 step:4854 [D loss: 0.199684, acc.: 66.41%] [G loss: 0.532852]\n",
      "epoch:5 step:4855 [D loss: 0.235893, acc.: 60.16%] [G loss: 0.480164]\n",
      "epoch:5 step:4856 [D loss: 0.173546, acc.: 73.44%] [G loss: 0.523699]\n",
      "epoch:5 step:4857 [D loss: 0.190241, acc.: 74.22%] [G loss: 0.557914]\n",
      "epoch:5 step:4858 [D loss: 0.203145, acc.: 70.31%] [G loss: 0.549002]\n",
      "epoch:5 step:4859 [D loss: 0.231645, acc.: 62.50%] [G loss: 0.537973]\n",
      "epoch:5 step:4860 [D loss: 0.199861, acc.: 67.19%] [G loss: 0.516638]\n",
      "epoch:5 step:4861 [D loss: 0.206797, acc.: 66.41%] [G loss: 0.508297]\n",
      "epoch:5 step:4862 [D loss: 0.173790, acc.: 75.78%] [G loss: 0.562553]\n",
      "epoch:5 step:4863 [D loss: 0.207248, acc.: 66.41%] [G loss: 0.549660]\n",
      "epoch:5 step:4864 [D loss: 0.182409, acc.: 73.44%] [G loss: 0.541528]\n",
      "epoch:5 step:4865 [D loss: 0.197184, acc.: 67.97%] [G loss: 0.556888]\n",
      "epoch:5 step:4866 [D loss: 0.203014, acc.: 70.31%] [G loss: 0.538769]\n",
      "epoch:5 step:4867 [D loss: 0.220364, acc.: 65.62%] [G loss: 0.507250]\n",
      "epoch:5 step:4868 [D loss: 0.229145, acc.: 60.94%] [G loss: 0.539919]\n",
      "epoch:5 step:4869 [D loss: 0.230858, acc.: 62.50%] [G loss: 0.548341]\n",
      "epoch:5 step:4870 [D loss: 0.200945, acc.: 70.31%] [G loss: 0.556445]\n",
      "epoch:5 step:4871 [D loss: 0.235779, acc.: 66.41%] [G loss: 0.558187]\n",
      "epoch:5 step:4872 [D loss: 0.218735, acc.: 68.75%] [G loss: 0.561883]\n",
      "epoch:5 step:4873 [D loss: 0.227545, acc.: 62.50%] [G loss: 0.523982]\n",
      "epoch:5 step:4874 [D loss: 0.232894, acc.: 60.94%] [G loss: 0.527477]\n",
      "epoch:5 step:4875 [D loss: 0.162786, acc.: 77.34%] [G loss: 0.574728]\n",
      "epoch:5 step:4876 [D loss: 0.196719, acc.: 68.75%] [G loss: 0.541228]\n",
      "epoch:5 step:4877 [D loss: 0.202880, acc.: 67.97%] [G loss: 0.578982]\n",
      "epoch:5 step:4878 [D loss: 0.172182, acc.: 71.09%] [G loss: 0.575803]\n",
      "epoch:5 step:4879 [D loss: 0.190670, acc.: 69.53%] [G loss: 0.580542]\n",
      "epoch:5 step:4880 [D loss: 0.184439, acc.: 72.66%] [G loss: 0.519085]\n",
      "epoch:5 step:4881 [D loss: 0.205829, acc.: 71.88%] [G loss: 0.523278]\n",
      "epoch:5 step:4882 [D loss: 0.202426, acc.: 68.75%] [G loss: 0.499696]\n",
      "epoch:5 step:4883 [D loss: 0.165115, acc.: 78.12%] [G loss: 0.607052]\n",
      "epoch:5 step:4884 [D loss: 0.190521, acc.: 72.66%] [G loss: 0.575493]\n",
      "epoch:5 step:4885 [D loss: 0.245245, acc.: 57.81%] [G loss: 0.544061]\n",
      "epoch:5 step:4886 [D loss: 0.232757, acc.: 64.06%] [G loss: 0.488167]\n",
      "epoch:5 step:4887 [D loss: 0.166909, acc.: 75.00%] [G loss: 0.606064]\n",
      "epoch:5 step:4888 [D loss: 0.273458, acc.: 52.34%] [G loss: 0.504649]\n",
      "epoch:5 step:4889 [D loss: 0.188853, acc.: 71.09%] [G loss: 0.554027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4890 [D loss: 0.177459, acc.: 75.78%] [G loss: 0.563802]\n",
      "epoch:5 step:4891 [D loss: 0.190312, acc.: 71.09%] [G loss: 0.570052]\n",
      "epoch:5 step:4892 [D loss: 0.171614, acc.: 71.88%] [G loss: 0.596941]\n",
      "epoch:5 step:4893 [D loss: 0.152157, acc.: 78.91%] [G loss: 0.612160]\n",
      "epoch:5 step:4894 [D loss: 0.211183, acc.: 67.97%] [G loss: 0.582300]\n",
      "epoch:5 step:4895 [D loss: 0.210630, acc.: 67.97%] [G loss: 0.565191]\n",
      "epoch:5 step:4896 [D loss: 0.195609, acc.: 69.53%] [G loss: 0.538609]\n",
      "epoch:5 step:4897 [D loss: 0.220655, acc.: 67.97%] [G loss: 0.511112]\n",
      "epoch:5 step:4898 [D loss: 0.198653, acc.: 68.75%] [G loss: 0.533456]\n",
      "epoch:5 step:4899 [D loss: 0.242844, acc.: 60.94%] [G loss: 0.482073]\n",
      "epoch:5 step:4900 [D loss: 0.221673, acc.: 64.84%] [G loss: 0.505337]\n",
      "epoch:5 step:4901 [D loss: 0.230825, acc.: 56.25%] [G loss: 0.526407]\n",
      "epoch:5 step:4902 [D loss: 0.207148, acc.: 66.41%] [G loss: 0.525328]\n",
      "epoch:5 step:4903 [D loss: 0.173738, acc.: 78.12%] [G loss: 0.576232]\n",
      "epoch:5 step:4904 [D loss: 0.182407, acc.: 73.44%] [G loss: 0.599241]\n",
      "epoch:5 step:4905 [D loss: 0.265868, acc.: 57.03%] [G loss: 0.526752]\n",
      "epoch:5 step:4906 [D loss: 0.147404, acc.: 77.34%] [G loss: 0.584040]\n",
      "epoch:5 step:4907 [D loss: 0.160845, acc.: 78.91%] [G loss: 0.664133]\n",
      "epoch:5 step:4908 [D loss: 0.147201, acc.: 82.03%] [G loss: 0.587880]\n",
      "epoch:5 step:4909 [D loss: 0.248942, acc.: 60.94%] [G loss: 0.536595]\n",
      "epoch:5 step:4910 [D loss: 0.224351, acc.: 64.06%] [G loss: 0.521328]\n",
      "epoch:5 step:4911 [D loss: 0.210964, acc.: 65.62%] [G loss: 0.497437]\n",
      "epoch:5 step:4912 [D loss: 0.227765, acc.: 58.59%] [G loss: 0.489053]\n",
      "epoch:5 step:4913 [D loss: 0.241677, acc.: 60.16%] [G loss: 0.462326]\n",
      "epoch:5 step:4914 [D loss: 0.198476, acc.: 70.31%] [G loss: 0.488956]\n",
      "epoch:5 step:4915 [D loss: 0.164216, acc.: 79.69%] [G loss: 0.590317]\n",
      "epoch:5 step:4916 [D loss: 0.173281, acc.: 72.66%] [G loss: 0.568946]\n",
      "epoch:5 step:4917 [D loss: 0.176238, acc.: 75.78%] [G loss: 0.625099]\n",
      "epoch:5 step:4918 [D loss: 0.221015, acc.: 66.41%] [G loss: 0.549422]\n",
      "epoch:5 step:4919 [D loss: 0.229115, acc.: 63.28%] [G loss: 0.507739]\n",
      "epoch:5 step:4920 [D loss: 0.235436, acc.: 61.72%] [G loss: 0.534093]\n",
      "epoch:5 step:4921 [D loss: 0.191228, acc.: 74.22%] [G loss: 0.532896]\n",
      "epoch:5 step:4922 [D loss: 0.224161, acc.: 66.41%] [G loss: 0.585668]\n",
      "epoch:5 step:4923 [D loss: 0.216716, acc.: 61.72%] [G loss: 0.544878]\n",
      "epoch:5 step:4924 [D loss: 0.195376, acc.: 73.44%] [G loss: 0.534238]\n",
      "epoch:5 step:4925 [D loss: 0.193751, acc.: 71.88%] [G loss: 0.491931]\n",
      "epoch:5 step:4926 [D loss: 0.184339, acc.: 71.88%] [G loss: 0.576647]\n",
      "epoch:5 step:4927 [D loss: 0.215809, acc.: 66.41%] [G loss: 0.579046]\n",
      "epoch:5 step:4928 [D loss: 0.200919, acc.: 66.41%] [G loss: 0.582028]\n",
      "epoch:5 step:4929 [D loss: 0.199064, acc.: 70.31%] [G loss: 0.550161]\n",
      "epoch:5 step:4930 [D loss: 0.171310, acc.: 75.78%] [G loss: 0.533804]\n",
      "epoch:5 step:4931 [D loss: 0.194861, acc.: 74.22%] [G loss: 0.527422]\n",
      "epoch:5 step:4932 [D loss: 0.227912, acc.: 64.84%] [G loss: 0.523273]\n",
      "epoch:5 step:4933 [D loss: 0.198720, acc.: 69.53%] [G loss: 0.564364]\n",
      "epoch:5 step:4934 [D loss: 0.229218, acc.: 65.62%] [G loss: 0.575210]\n",
      "epoch:5 step:4935 [D loss: 0.226487, acc.: 60.94%] [G loss: 0.544292]\n",
      "epoch:5 step:4936 [D loss: 0.196846, acc.: 71.09%] [G loss: 0.552069]\n",
      "epoch:5 step:4937 [D loss: 0.214373, acc.: 67.19%] [G loss: 0.520325]\n",
      "epoch:5 step:4938 [D loss: 0.191739, acc.: 71.09%] [G loss: 0.543539]\n",
      "epoch:5 step:4939 [D loss: 0.183439, acc.: 71.88%] [G loss: 0.561455]\n",
      "epoch:5 step:4940 [D loss: 0.190419, acc.: 74.22%] [G loss: 0.586999]\n",
      "epoch:5 step:4941 [D loss: 0.199598, acc.: 73.44%] [G loss: 0.524108]\n",
      "epoch:5 step:4942 [D loss: 0.208239, acc.: 65.62%] [G loss: 0.522827]\n",
      "epoch:5 step:4943 [D loss: 0.193898, acc.: 70.31%] [G loss: 0.528008]\n",
      "epoch:5 step:4944 [D loss: 0.201571, acc.: 67.97%] [G loss: 0.571990]\n",
      "epoch:5 step:4945 [D loss: 0.209743, acc.: 65.62%] [G loss: 0.591075]\n",
      "epoch:5 step:4946 [D loss: 0.207964, acc.: 65.62%] [G loss: 0.545824]\n",
      "epoch:5 step:4947 [D loss: 0.187785, acc.: 71.09%] [G loss: 0.577713]\n",
      "epoch:5 step:4948 [D loss: 0.222893, acc.: 66.41%] [G loss: 0.531940]\n",
      "epoch:5 step:4949 [D loss: 0.172197, acc.: 74.22%] [G loss: 0.568058]\n",
      "epoch:5 step:4950 [D loss: 0.268181, acc.: 54.69%] [G loss: 0.491470]\n",
      "epoch:5 step:4951 [D loss: 0.238355, acc.: 60.94%] [G loss: 0.488421]\n",
      "epoch:5 step:4952 [D loss: 0.206894, acc.: 67.19%] [G loss: 0.570402]\n",
      "epoch:5 step:4953 [D loss: 0.207353, acc.: 68.75%] [G loss: 0.554791]\n",
      "epoch:5 step:4954 [D loss: 0.199956, acc.: 68.75%] [G loss: 0.524353]\n",
      "epoch:5 step:4955 [D loss: 0.181637, acc.: 72.66%] [G loss: 0.545020]\n",
      "epoch:5 step:4956 [D loss: 0.169570, acc.: 75.78%] [G loss: 0.559437]\n",
      "epoch:5 step:4957 [D loss: 0.194331, acc.: 72.66%] [G loss: 0.535509]\n",
      "epoch:5 step:4958 [D loss: 0.191322, acc.: 73.44%] [G loss: 0.545781]\n",
      "epoch:5 step:4959 [D loss: 0.220722, acc.: 66.41%] [G loss: 0.555673]\n",
      "epoch:5 step:4960 [D loss: 0.209461, acc.: 67.19%] [G loss: 0.527520]\n",
      "epoch:5 step:4961 [D loss: 0.178916, acc.: 73.44%] [G loss: 0.536107]\n",
      "epoch:5 step:4962 [D loss: 0.211642, acc.: 67.19%] [G loss: 0.507833]\n",
      "epoch:5 step:4963 [D loss: 0.204661, acc.: 68.75%] [G loss: 0.496909]\n",
      "epoch:5 step:4964 [D loss: 0.191333, acc.: 70.31%] [G loss: 0.561659]\n",
      "epoch:5 step:4965 [D loss: 0.185396, acc.: 69.53%] [G loss: 0.632995]\n",
      "epoch:5 step:4966 [D loss: 0.231908, acc.: 62.50%] [G loss: 0.515363]\n",
      "epoch:5 step:4967 [D loss: 0.222743, acc.: 61.72%] [G loss: 0.504031]\n",
      "epoch:5 step:4968 [D loss: 0.179154, acc.: 71.88%] [G loss: 0.550918]\n",
      "epoch:5 step:4969 [D loss: 0.180784, acc.: 76.56%] [G loss: 0.544566]\n",
      "epoch:5 step:4970 [D loss: 0.199978, acc.: 65.62%] [G loss: 0.546862]\n",
      "epoch:5 step:4971 [D loss: 0.174435, acc.: 73.44%] [G loss: 0.585778]\n",
      "epoch:5 step:4972 [D loss: 0.212530, acc.: 67.97%] [G loss: 0.538741]\n",
      "epoch:5 step:4973 [D loss: 0.207376, acc.: 70.31%] [G loss: 0.523396]\n",
      "epoch:5 step:4974 [D loss: 0.194207, acc.: 68.75%] [G loss: 0.535090]\n",
      "epoch:5 step:4975 [D loss: 0.183759, acc.: 74.22%] [G loss: 0.544199]\n",
      "epoch:5 step:4976 [D loss: 0.201501, acc.: 69.53%] [G loss: 0.502922]\n",
      "epoch:5 step:4977 [D loss: 0.203841, acc.: 64.06%] [G loss: 0.504535]\n",
      "epoch:5 step:4978 [D loss: 0.176641, acc.: 74.22%] [G loss: 0.610870]\n",
      "epoch:5 step:4979 [D loss: 0.209690, acc.: 64.84%] [G loss: 0.511999]\n",
      "epoch:5 step:4980 [D loss: 0.181734, acc.: 71.09%] [G loss: 0.552196]\n",
      "epoch:5 step:4981 [D loss: 0.157567, acc.: 76.56%] [G loss: 0.591331]\n",
      "epoch:5 step:4982 [D loss: 0.195466, acc.: 69.53%] [G loss: 0.554683]\n",
      "epoch:5 step:4983 [D loss: 0.192811, acc.: 70.31%] [G loss: 0.563638]\n",
      "epoch:5 step:4984 [D loss: 0.181355, acc.: 76.56%] [G loss: 0.540629]\n",
      "epoch:5 step:4985 [D loss: 0.195690, acc.: 69.53%] [G loss: 0.555063]\n",
      "epoch:5 step:4986 [D loss: 0.198449, acc.: 68.75%] [G loss: 0.563632]\n",
      "epoch:5 step:4987 [D loss: 0.197420, acc.: 65.62%] [G loss: 0.526644]\n",
      "epoch:5 step:4988 [D loss: 0.224397, acc.: 60.16%] [G loss: 0.520001]\n",
      "epoch:5 step:4989 [D loss: 0.166571, acc.: 72.66%] [G loss: 0.593956]\n",
      "epoch:5 step:4990 [D loss: 0.177515, acc.: 75.00%] [G loss: 0.581608]\n",
      "epoch:5 step:4991 [D loss: 0.191053, acc.: 74.22%] [G loss: 0.561314]\n",
      "epoch:5 step:4992 [D loss: 0.177409, acc.: 69.53%] [G loss: 0.592869]\n",
      "epoch:5 step:4993 [D loss: 0.218653, acc.: 70.31%] [G loss: 0.517284]\n",
      "epoch:5 step:4994 [D loss: 0.162120, acc.: 74.22%] [G loss: 0.554756]\n",
      "epoch:5 step:4995 [D loss: 0.198324, acc.: 68.75%] [G loss: 0.578551]\n",
      "epoch:5 step:4996 [D loss: 0.184897, acc.: 75.00%] [G loss: 0.568036]\n",
      "epoch:5 step:4997 [D loss: 0.166892, acc.: 76.56%] [G loss: 0.558855]\n",
      "epoch:5 step:4998 [D loss: 0.173444, acc.: 71.09%] [G loss: 0.676071]\n",
      "epoch:5 step:4999 [D loss: 0.184855, acc.: 71.09%] [G loss: 0.607294]\n",
      "epoch:5 step:5000 [D loss: 0.179159, acc.: 78.12%] [G loss: 0.596338]\n",
      "##############\n",
      "[3.05195866 1.4046344  6.76480409 5.00749039 4.14002361 5.87240525\n",
      " 4.81393516 4.95577101 4.94889374 3.99015242]\n",
      "##########\n",
      "epoch:5 step:5001 [D loss: 0.258992, acc.: 58.59%] [G loss: 0.499609]\n",
      "epoch:5 step:5002 [D loss: 0.243402, acc.: 62.50%] [G loss: 0.475063]\n",
      "epoch:5 step:5003 [D loss: 0.218653, acc.: 68.75%] [G loss: 0.542688]\n",
      "epoch:5 step:5004 [D loss: 0.183575, acc.: 72.66%] [G loss: 0.553524]\n",
      "epoch:5 step:5005 [D loss: 0.202626, acc.: 69.53%] [G loss: 0.542863]\n",
      "epoch:5 step:5006 [D loss: 0.153284, acc.: 78.91%] [G loss: 0.597673]\n",
      "epoch:5 step:5007 [D loss: 0.180511, acc.: 78.12%] [G loss: 0.584046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5008 [D loss: 0.206459, acc.: 70.31%] [G loss: 0.524771]\n",
      "epoch:5 step:5009 [D loss: 0.181625, acc.: 72.66%] [G loss: 0.546405]\n",
      "epoch:5 step:5010 [D loss: 0.215365, acc.: 60.94%] [G loss: 0.529244]\n",
      "epoch:5 step:5011 [D loss: 0.177212, acc.: 75.00%] [G loss: 0.517782]\n",
      "epoch:5 step:5012 [D loss: 0.235342, acc.: 59.38%] [G loss: 0.514275]\n",
      "epoch:5 step:5013 [D loss: 0.175656, acc.: 74.22%] [G loss: 0.570484]\n",
      "epoch:5 step:5014 [D loss: 0.199798, acc.: 69.53%] [G loss: 0.540375]\n",
      "epoch:5 step:5015 [D loss: 0.197089, acc.: 70.31%] [G loss: 0.515469]\n",
      "epoch:5 step:5016 [D loss: 0.174671, acc.: 75.78%] [G loss: 0.549159]\n",
      "epoch:5 step:5017 [D loss: 0.188737, acc.: 67.97%] [G loss: 0.551369]\n",
      "epoch:5 step:5018 [D loss: 0.174898, acc.: 74.22%] [G loss: 0.546898]\n",
      "epoch:5 step:5019 [D loss: 0.211277, acc.: 68.75%] [G loss: 0.516292]\n",
      "epoch:5 step:5020 [D loss: 0.198231, acc.: 71.09%] [G loss: 0.547050]\n",
      "epoch:5 step:5021 [D loss: 0.177585, acc.: 81.25%] [G loss: 0.581652]\n",
      "epoch:5 step:5022 [D loss: 0.197187, acc.: 70.31%] [G loss: 0.581082]\n",
      "epoch:5 step:5023 [D loss: 0.185513, acc.: 77.34%] [G loss: 0.514429]\n",
      "epoch:5 step:5024 [D loss: 0.168599, acc.: 80.47%] [G loss: 0.564325]\n",
      "epoch:5 step:5025 [D loss: 0.194948, acc.: 73.44%] [G loss: 0.559623]\n",
      "epoch:5 step:5026 [D loss: 0.222093, acc.: 67.19%] [G loss: 0.494813]\n",
      "epoch:5 step:5027 [D loss: 0.181558, acc.: 71.88%] [G loss: 0.551270]\n",
      "epoch:5 step:5028 [D loss: 0.136292, acc.: 80.47%] [G loss: 0.636638]\n",
      "epoch:5 step:5029 [D loss: 0.156387, acc.: 79.69%] [G loss: 0.585508]\n",
      "epoch:5 step:5030 [D loss: 0.217020, acc.: 66.41%] [G loss: 0.543978]\n",
      "epoch:5 step:5031 [D loss: 0.149049, acc.: 82.03%] [G loss: 0.653744]\n",
      "epoch:5 step:5032 [D loss: 0.162860, acc.: 71.09%] [G loss: 0.606937]\n",
      "epoch:5 step:5033 [D loss: 0.291318, acc.: 53.91%] [G loss: 0.496318]\n",
      "epoch:5 step:5034 [D loss: 0.245102, acc.: 61.72%] [G loss: 0.496161]\n",
      "epoch:5 step:5035 [D loss: 0.176193, acc.: 75.00%] [G loss: 0.554844]\n",
      "epoch:5 step:5036 [D loss: 0.179900, acc.: 73.44%] [G loss: 0.613818]\n",
      "epoch:5 step:5037 [D loss: 0.240193, acc.: 60.94%] [G loss: 0.580000]\n",
      "epoch:5 step:5038 [D loss: 0.186343, acc.: 72.66%] [G loss: 0.581251]\n",
      "epoch:5 step:5039 [D loss: 0.173176, acc.: 75.00%] [G loss: 0.524456]\n",
      "epoch:5 step:5040 [D loss: 0.203178, acc.: 69.53%] [G loss: 0.536695]\n",
      "epoch:5 step:5041 [D loss: 0.224553, acc.: 67.97%] [G loss: 0.505122]\n",
      "epoch:5 step:5042 [D loss: 0.180030, acc.: 71.88%] [G loss: 0.571982]\n",
      "epoch:5 step:5043 [D loss: 0.175988, acc.: 75.78%] [G loss: 0.588369]\n",
      "epoch:5 step:5044 [D loss: 0.172179, acc.: 71.88%] [G loss: 0.560639]\n",
      "epoch:5 step:5045 [D loss: 0.187299, acc.: 67.97%] [G loss: 0.546106]\n",
      "epoch:5 step:5046 [D loss: 0.206008, acc.: 70.31%] [G loss: 0.553440]\n",
      "epoch:5 step:5047 [D loss: 0.191817, acc.: 67.19%] [G loss: 0.554938]\n",
      "epoch:5 step:5048 [D loss: 0.170568, acc.: 76.56%] [G loss: 0.524047]\n",
      "epoch:5 step:5049 [D loss: 0.148659, acc.: 82.03%] [G loss: 0.522819]\n",
      "epoch:5 step:5050 [D loss: 0.229175, acc.: 60.16%] [G loss: 0.516847]\n",
      "epoch:5 step:5051 [D loss: 0.172119, acc.: 72.66%] [G loss: 0.609206]\n",
      "epoch:5 step:5052 [D loss: 0.215768, acc.: 63.28%] [G loss: 0.544634]\n",
      "epoch:5 step:5053 [D loss: 0.202734, acc.: 73.44%] [G loss: 0.527673]\n",
      "epoch:5 step:5054 [D loss: 0.200365, acc.: 67.97%] [G loss: 0.581411]\n",
      "epoch:5 step:5055 [D loss: 0.164977, acc.: 79.69%] [G loss: 0.600049]\n",
      "epoch:5 step:5056 [D loss: 0.172218, acc.: 78.12%] [G loss: 0.606275]\n",
      "epoch:5 step:5057 [D loss: 0.202230, acc.: 69.53%] [G loss: 0.558622]\n",
      "epoch:5 step:5058 [D loss: 0.247051, acc.: 60.16%] [G loss: 0.536880]\n",
      "epoch:5 step:5059 [D loss: 0.182229, acc.: 71.09%] [G loss: 0.587585]\n",
      "epoch:5 step:5060 [D loss: 0.205418, acc.: 65.62%] [G loss: 0.569246]\n",
      "epoch:5 step:5061 [D loss: 0.285206, acc.: 48.44%] [G loss: 0.507677]\n",
      "epoch:5 step:5062 [D loss: 0.238294, acc.: 59.38%] [G loss: 0.500772]\n",
      "epoch:5 step:5063 [D loss: 0.211047, acc.: 64.84%] [G loss: 0.538567]\n",
      "epoch:5 step:5064 [D loss: 0.211854, acc.: 68.75%] [G loss: 0.540736]\n",
      "epoch:5 step:5065 [D loss: 0.201477, acc.: 67.19%] [G loss: 0.542264]\n",
      "epoch:5 step:5066 [D loss: 0.160525, acc.: 76.56%] [G loss: 0.570194]\n",
      "epoch:5 step:5067 [D loss: 0.224148, acc.: 61.72%] [G loss: 0.503164]\n",
      "epoch:5 step:5068 [D loss: 0.225063, acc.: 67.97%] [G loss: 0.530703]\n",
      "epoch:5 step:5069 [D loss: 0.188768, acc.: 70.31%] [G loss: 0.486215]\n",
      "epoch:5 step:5070 [D loss: 0.176670, acc.: 72.66%] [G loss: 0.549525]\n",
      "epoch:5 step:5071 [D loss: 0.227573, acc.: 60.94%] [G loss: 0.500315]\n",
      "epoch:5 step:5072 [D loss: 0.205283, acc.: 64.84%] [G loss: 0.535919]\n",
      "epoch:5 step:5073 [D loss: 0.199519, acc.: 70.31%] [G loss: 0.553936]\n",
      "epoch:5 step:5074 [D loss: 0.202246, acc.: 71.09%] [G loss: 0.583617]\n",
      "epoch:5 step:5075 [D loss: 0.213622, acc.: 67.97%] [G loss: 0.573485]\n",
      "epoch:5 step:5076 [D loss: 0.211529, acc.: 67.97%] [G loss: 0.592833]\n",
      "epoch:5 step:5077 [D loss: 0.175589, acc.: 75.78%] [G loss: 0.539265]\n",
      "epoch:5 step:5078 [D loss: 0.205728, acc.: 69.53%] [G loss: 0.534874]\n",
      "epoch:5 step:5079 [D loss: 0.213297, acc.: 67.97%] [G loss: 0.508547]\n",
      "epoch:5 step:5080 [D loss: 0.196001, acc.: 71.88%] [G loss: 0.502357]\n",
      "epoch:5 step:5081 [D loss: 0.236016, acc.: 63.28%] [G loss: 0.548378]\n",
      "epoch:5 step:5082 [D loss: 0.215306, acc.: 63.28%] [G loss: 0.560703]\n",
      "epoch:5 step:5083 [D loss: 0.167383, acc.: 73.44%] [G loss: 0.593881]\n",
      "epoch:5 step:5084 [D loss: 0.165808, acc.: 78.12%] [G loss: 0.586621]\n",
      "epoch:5 step:5085 [D loss: 0.205952, acc.: 67.97%] [G loss: 0.489601]\n",
      "epoch:5 step:5086 [D loss: 0.213487, acc.: 69.53%] [G loss: 0.545977]\n",
      "epoch:5 step:5087 [D loss: 0.178448, acc.: 74.22%] [G loss: 0.563913]\n",
      "epoch:5 step:5088 [D loss: 0.207089, acc.: 66.41%] [G loss: 0.568731]\n",
      "epoch:5 step:5089 [D loss: 0.237960, acc.: 60.16%] [G loss: 0.503862]\n",
      "epoch:5 step:5090 [D loss: 0.188736, acc.: 75.00%] [G loss: 0.574790]\n",
      "epoch:5 step:5091 [D loss: 0.182611, acc.: 70.31%] [G loss: 0.575493]\n",
      "epoch:5 step:5092 [D loss: 0.208528, acc.: 62.50%] [G loss: 0.536649]\n",
      "epoch:5 step:5093 [D loss: 0.223274, acc.: 67.97%] [G loss: 0.540992]\n",
      "epoch:5 step:5094 [D loss: 0.217857, acc.: 67.19%] [G loss: 0.519113]\n",
      "epoch:5 step:5095 [D loss: 0.221505, acc.: 65.62%] [G loss: 0.483332]\n",
      "epoch:5 step:5096 [D loss: 0.228438, acc.: 58.59%] [G loss: 0.476720]\n",
      "epoch:5 step:5097 [D loss: 0.197073, acc.: 69.53%] [G loss: 0.506835]\n",
      "epoch:5 step:5098 [D loss: 0.206570, acc.: 72.66%] [G loss: 0.478335]\n",
      "epoch:5 step:5099 [D loss: 0.218925, acc.: 64.06%] [G loss: 0.500589]\n",
      "epoch:5 step:5100 [D loss: 0.186494, acc.: 71.09%] [G loss: 0.548656]\n",
      "epoch:5 step:5101 [D loss: 0.190819, acc.: 69.53%] [G loss: 0.572400]\n",
      "epoch:5 step:5102 [D loss: 0.261852, acc.: 59.38%] [G loss: 0.550948]\n",
      "epoch:5 step:5103 [D loss: 0.256418, acc.: 62.50%] [G loss: 0.474041]\n",
      "epoch:5 step:5104 [D loss: 0.231443, acc.: 60.94%] [G loss: 0.528392]\n",
      "epoch:5 step:5105 [D loss: 0.225549, acc.: 60.94%] [G loss: 0.557849]\n",
      "epoch:5 step:5106 [D loss: 0.227535, acc.: 65.62%] [G loss: 0.542391]\n",
      "epoch:5 step:5107 [D loss: 0.212583, acc.: 65.62%] [G loss: 0.519668]\n",
      "epoch:5 step:5108 [D loss: 0.229074, acc.: 62.50%] [G loss: 0.523438]\n",
      "epoch:5 step:5109 [D loss: 0.218453, acc.: 61.72%] [G loss: 0.467453]\n",
      "epoch:5 step:5110 [D loss: 0.202781, acc.: 69.53%] [G loss: 0.547199]\n",
      "epoch:5 step:5111 [D loss: 0.165411, acc.: 75.00%] [G loss: 0.546908]\n",
      "epoch:5 step:5112 [D loss: 0.196124, acc.: 72.66%] [G loss: 0.623340]\n",
      "epoch:5 step:5113 [D loss: 0.159585, acc.: 79.69%] [G loss: 0.623101]\n",
      "epoch:5 step:5114 [D loss: 0.175361, acc.: 77.34%] [G loss: 0.621282]\n",
      "epoch:5 step:5115 [D loss: 0.191037, acc.: 71.09%] [G loss: 0.534815]\n",
      "epoch:5 step:5116 [D loss: 0.202863, acc.: 70.31%] [G loss: 0.536013]\n",
      "epoch:5 step:5117 [D loss: 0.212655, acc.: 63.28%] [G loss: 0.483469]\n",
      "epoch:5 step:5118 [D loss: 0.238925, acc.: 62.50%] [G loss: 0.505896]\n",
      "epoch:5 step:5119 [D loss: 0.153064, acc.: 77.34%] [G loss: 0.574542]\n",
      "epoch:5 step:5120 [D loss: 0.199272, acc.: 73.44%] [G loss: 0.580351]\n",
      "epoch:5 step:5121 [D loss: 0.211360, acc.: 67.19%] [G loss: 0.522574]\n",
      "epoch:5 step:5122 [D loss: 0.288340, acc.: 54.69%] [G loss: 0.524394]\n",
      "epoch:5 step:5123 [D loss: 0.222249, acc.: 64.06%] [G loss: 0.543783]\n",
      "epoch:5 step:5124 [D loss: 0.171362, acc.: 75.78%] [G loss: 0.563118]\n",
      "epoch:5 step:5125 [D loss: 0.214729, acc.: 63.28%] [G loss: 0.561284]\n",
      "epoch:5 step:5126 [D loss: 0.238385, acc.: 64.06%] [G loss: 0.514513]\n",
      "epoch:5 step:5127 [D loss: 0.205862, acc.: 68.75%] [G loss: 0.547820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5128 [D loss: 0.192026, acc.: 72.66%] [G loss: 0.515286]\n",
      "epoch:5 step:5129 [D loss: 0.218465, acc.: 68.75%] [G loss: 0.525294]\n",
      "epoch:5 step:5130 [D loss: 0.185180, acc.: 71.88%] [G loss: 0.552670]\n",
      "epoch:5 step:5131 [D loss: 0.202176, acc.: 70.31%] [G loss: 0.570594]\n",
      "epoch:5 step:5132 [D loss: 0.197294, acc.: 67.97%] [G loss: 0.510098]\n",
      "epoch:5 step:5133 [D loss: 0.230088, acc.: 60.94%] [G loss: 0.502575]\n",
      "epoch:5 step:5134 [D loss: 0.205023, acc.: 71.09%] [G loss: 0.489210]\n",
      "epoch:5 step:5135 [D loss: 0.190189, acc.: 73.44%] [G loss: 0.571905]\n",
      "epoch:5 step:5136 [D loss: 0.169705, acc.: 76.56%] [G loss: 0.541711]\n",
      "epoch:5 step:5137 [D loss: 0.205367, acc.: 72.66%] [G loss: 0.570511]\n",
      "epoch:5 step:5138 [D loss: 0.203319, acc.: 68.75%] [G loss: 0.575732]\n",
      "epoch:5 step:5139 [D loss: 0.222365, acc.: 67.97%] [G loss: 0.552555]\n",
      "epoch:5 step:5140 [D loss: 0.226511, acc.: 62.50%] [G loss: 0.489739]\n",
      "epoch:5 step:5141 [D loss: 0.223148, acc.: 59.38%] [G loss: 0.505037]\n",
      "epoch:5 step:5142 [D loss: 0.221106, acc.: 67.19%] [G loss: 0.509672]\n",
      "epoch:5 step:5143 [D loss: 0.218008, acc.: 71.09%] [G loss: 0.537198]\n",
      "epoch:5 step:5144 [D loss: 0.227030, acc.: 60.94%] [G loss: 0.521950]\n",
      "epoch:5 step:5145 [D loss: 0.228657, acc.: 57.03%] [G loss: 0.493343]\n",
      "epoch:5 step:5146 [D loss: 0.217964, acc.: 67.19%] [G loss: 0.557202]\n",
      "epoch:5 step:5147 [D loss: 0.209976, acc.: 67.97%] [G loss: 0.582001]\n",
      "epoch:5 step:5148 [D loss: 0.204506, acc.: 69.53%] [G loss: 0.504860]\n",
      "epoch:5 step:5149 [D loss: 0.199347, acc.: 69.53%] [G loss: 0.505577]\n",
      "epoch:5 step:5150 [D loss: 0.225174, acc.: 64.06%] [G loss: 0.510698]\n",
      "epoch:5 step:5151 [D loss: 0.230879, acc.: 61.72%] [G loss: 0.517960]\n",
      "epoch:5 step:5152 [D loss: 0.202383, acc.: 70.31%] [G loss: 0.545982]\n",
      "epoch:5 step:5153 [D loss: 0.199145, acc.: 70.31%] [G loss: 0.609980]\n",
      "epoch:5 step:5154 [D loss: 0.227613, acc.: 65.62%] [G loss: 0.559049]\n",
      "epoch:5 step:5155 [D loss: 0.214390, acc.: 67.19%] [G loss: 0.580148]\n",
      "epoch:5 step:5156 [D loss: 0.151478, acc.: 78.91%] [G loss: 0.625321]\n",
      "epoch:5 step:5157 [D loss: 0.212722, acc.: 70.31%] [G loss: 0.573564]\n",
      "epoch:5 step:5158 [D loss: 0.247820, acc.: 60.16%] [G loss: 0.484914]\n",
      "epoch:5 step:5159 [D loss: 0.184957, acc.: 72.66%] [G loss: 0.567994]\n",
      "epoch:5 step:5160 [D loss: 0.161664, acc.: 72.66%] [G loss: 0.631396]\n",
      "epoch:5 step:5161 [D loss: 0.246114, acc.: 60.94%] [G loss: 0.521986]\n",
      "epoch:5 step:5162 [D loss: 0.223543, acc.: 64.84%] [G loss: 0.456643]\n",
      "epoch:5 step:5163 [D loss: 0.191156, acc.: 73.44%] [G loss: 0.493557]\n",
      "epoch:5 step:5164 [D loss: 0.208734, acc.: 68.75%] [G loss: 0.486663]\n",
      "epoch:5 step:5165 [D loss: 0.222991, acc.: 65.62%] [G loss: 0.479217]\n",
      "epoch:5 step:5166 [D loss: 0.203003, acc.: 69.53%] [G loss: 0.552253]\n",
      "epoch:5 step:5167 [D loss: 0.244775, acc.: 67.97%] [G loss: 0.504428]\n",
      "epoch:5 step:5168 [D loss: 0.217855, acc.: 68.75%] [G loss: 0.526029]\n",
      "epoch:5 step:5169 [D loss: 0.176105, acc.: 71.88%] [G loss: 0.620456]\n",
      "epoch:5 step:5170 [D loss: 0.208386, acc.: 67.97%] [G loss: 0.559826]\n",
      "epoch:5 step:5171 [D loss: 0.224308, acc.: 59.38%] [G loss: 0.475314]\n",
      "epoch:5 step:5172 [D loss: 0.211682, acc.: 67.19%] [G loss: 0.521773]\n",
      "epoch:5 step:5173 [D loss: 0.199514, acc.: 67.97%] [G loss: 0.551648]\n",
      "epoch:5 step:5174 [D loss: 0.231986, acc.: 62.50%] [G loss: 0.524400]\n",
      "epoch:5 step:5175 [D loss: 0.214004, acc.: 67.97%] [G loss: 0.554314]\n",
      "epoch:5 step:5176 [D loss: 0.194040, acc.: 73.44%] [G loss: 0.542758]\n",
      "epoch:5 step:5177 [D loss: 0.214607, acc.: 68.75%] [G loss: 0.505637]\n",
      "epoch:5 step:5178 [D loss: 0.212081, acc.: 68.75%] [G loss: 0.522222]\n",
      "epoch:5 step:5179 [D loss: 0.196916, acc.: 74.22%] [G loss: 0.561269]\n",
      "epoch:5 step:5180 [D loss: 0.185222, acc.: 73.44%] [G loss: 0.602451]\n",
      "epoch:5 step:5181 [D loss: 0.214759, acc.: 70.31%] [G loss: 0.524527]\n",
      "epoch:5 step:5182 [D loss: 0.205464, acc.: 64.06%] [G loss: 0.549997]\n",
      "epoch:5 step:5183 [D loss: 0.188299, acc.: 77.34%] [G loss: 0.594949]\n",
      "epoch:5 step:5184 [D loss: 0.162374, acc.: 75.78%] [G loss: 0.608596]\n",
      "epoch:5 step:5185 [D loss: 0.267595, acc.: 51.56%] [G loss: 0.469640]\n",
      "epoch:5 step:5186 [D loss: 0.240922, acc.: 63.28%] [G loss: 0.477086]\n",
      "epoch:5 step:5187 [D loss: 0.230466, acc.: 64.84%] [G loss: 0.485811]\n",
      "epoch:5 step:5188 [D loss: 0.182522, acc.: 70.31%] [G loss: 0.573728]\n",
      "epoch:5 step:5189 [D loss: 0.183935, acc.: 71.88%] [G loss: 0.553457]\n",
      "epoch:5 step:5190 [D loss: 0.184476, acc.: 69.53%] [G loss: 0.572448]\n",
      "epoch:5 step:5191 [D loss: 0.228359, acc.: 64.06%] [G loss: 0.499683]\n",
      "epoch:5 step:5192 [D loss: 0.195588, acc.: 69.53%] [G loss: 0.573209]\n",
      "epoch:5 step:5193 [D loss: 0.152015, acc.: 82.81%] [G loss: 0.640788]\n",
      "epoch:5 step:5194 [D loss: 0.231455, acc.: 64.84%] [G loss: 0.534197]\n",
      "epoch:5 step:5195 [D loss: 0.213238, acc.: 64.84%] [G loss: 0.502232]\n",
      "epoch:5 step:5196 [D loss: 0.217295, acc.: 65.62%] [G loss: 0.547927]\n",
      "epoch:5 step:5197 [D loss: 0.186078, acc.: 77.34%] [G loss: 0.525333]\n",
      "epoch:5 step:5198 [D loss: 0.174958, acc.: 72.66%] [G loss: 0.547455]\n",
      "epoch:5 step:5199 [D loss: 0.191082, acc.: 73.44%] [G loss: 0.598149]\n",
      "epoch:5 step:5200 [D loss: 0.180800, acc.: 73.44%] [G loss: 0.572181]\n",
      "##############\n",
      "[2.69335322 1.42343166 6.57206126 5.03491687 4.30141366 5.73908133\n",
      " 4.8439238  4.8648391  4.90175046 3.93328343]\n",
      "##########\n",
      "epoch:5 step:5201 [D loss: 0.159100, acc.: 77.34%] [G loss: 0.544305]\n",
      "epoch:5 step:5202 [D loss: 0.211472, acc.: 67.19%] [G loss: 0.530376]\n",
      "epoch:5 step:5203 [D loss: 0.186989, acc.: 73.44%] [G loss: 0.555487]\n",
      "epoch:5 step:5204 [D loss: 0.171849, acc.: 76.56%] [G loss: 0.581477]\n",
      "epoch:5 step:5205 [D loss: 0.180576, acc.: 75.78%] [G loss: 0.586596]\n",
      "epoch:5 step:5206 [D loss: 0.185555, acc.: 73.44%] [G loss: 0.523748]\n",
      "epoch:5 step:5207 [D loss: 0.225436, acc.: 64.84%] [G loss: 0.479975]\n",
      "epoch:5 step:5208 [D loss: 0.186149, acc.: 73.44%] [G loss: 0.546829]\n",
      "epoch:5 step:5209 [D loss: 0.193001, acc.: 71.88%] [G loss: 0.521459]\n",
      "epoch:5 step:5210 [D loss: 0.218923, acc.: 63.28%] [G loss: 0.525720]\n",
      "epoch:5 step:5211 [D loss: 0.203654, acc.: 66.41%] [G loss: 0.565979]\n",
      "epoch:5 step:5212 [D loss: 0.193315, acc.: 71.88%] [G loss: 0.572197]\n",
      "epoch:5 step:5213 [D loss: 0.268946, acc.: 51.56%] [G loss: 0.524008]\n",
      "epoch:5 step:5214 [D loss: 0.206272, acc.: 71.88%] [G loss: 0.529955]\n",
      "epoch:5 step:5215 [D loss: 0.183615, acc.: 72.66%] [G loss: 0.576559]\n",
      "epoch:5 step:5216 [D loss: 0.227044, acc.: 66.41%] [G loss: 0.527375]\n",
      "epoch:5 step:5217 [D loss: 0.211230, acc.: 71.09%] [G loss: 0.517479]\n",
      "epoch:5 step:5218 [D loss: 0.215797, acc.: 67.19%] [G loss: 0.489655]\n",
      "epoch:5 step:5219 [D loss: 0.177671, acc.: 79.69%] [G loss: 0.554652]\n",
      "epoch:5 step:5220 [D loss: 0.231872, acc.: 58.59%] [G loss: 0.554951]\n",
      "epoch:5 step:5221 [D loss: 0.179892, acc.: 75.00%] [G loss: 0.550011]\n",
      "epoch:5 step:5222 [D loss: 0.217560, acc.: 68.75%] [G loss: 0.553104]\n",
      "epoch:5 step:5223 [D loss: 0.231172, acc.: 60.94%] [G loss: 0.512183]\n",
      "epoch:5 step:5224 [D loss: 0.227840, acc.: 58.59%] [G loss: 0.479578]\n",
      "epoch:5 step:5225 [D loss: 0.197969, acc.: 71.09%] [G loss: 0.490922]\n",
      "epoch:5 step:5226 [D loss: 0.184391, acc.: 72.66%] [G loss: 0.524672]\n",
      "epoch:5 step:5227 [D loss: 0.244760, acc.: 56.25%] [G loss: 0.481365]\n",
      "epoch:5 step:5228 [D loss: 0.209278, acc.: 63.28%] [G loss: 0.529104]\n",
      "epoch:5 step:5229 [D loss: 0.237355, acc.: 57.81%] [G loss: 0.500808]\n",
      "epoch:5 step:5230 [D loss: 0.171101, acc.: 74.22%] [G loss: 0.550083]\n",
      "epoch:5 step:5231 [D loss: 0.193231, acc.: 68.75%] [G loss: 0.496843]\n",
      "epoch:5 step:5232 [D loss: 0.180152, acc.: 71.88%] [G loss: 0.554815]\n",
      "epoch:5 step:5233 [D loss: 0.199384, acc.: 70.31%] [G loss: 0.547722]\n",
      "epoch:5 step:5234 [D loss: 0.195937, acc.: 71.09%] [G loss: 0.541240]\n",
      "epoch:5 step:5235 [D loss: 0.207682, acc.: 67.97%] [G loss: 0.534320]\n",
      "epoch:5 step:5236 [D loss: 0.218547, acc.: 61.72%] [G loss: 0.497421]\n",
      "epoch:5 step:5237 [D loss: 0.146905, acc.: 79.69%] [G loss: 0.583794]\n",
      "epoch:5 step:5238 [D loss: 0.237903, acc.: 66.41%] [G loss: 0.514107]\n",
      "epoch:5 step:5239 [D loss: 0.165018, acc.: 77.34%] [G loss: 0.561825]\n",
      "epoch:5 step:5240 [D loss: 0.165133, acc.: 73.44%] [G loss: 0.581003]\n",
      "epoch:5 step:5241 [D loss: 0.188141, acc.: 74.22%] [G loss: 0.563987]\n",
      "epoch:5 step:5242 [D loss: 0.210107, acc.: 64.06%] [G loss: 0.498679]\n",
      "epoch:5 step:5243 [D loss: 0.169356, acc.: 75.00%] [G loss: 0.517233]\n",
      "epoch:5 step:5244 [D loss: 0.204775, acc.: 67.19%] [G loss: 0.552512]\n",
      "epoch:5 step:5245 [D loss: 0.222670, acc.: 60.94%] [G loss: 0.537686]\n",
      "epoch:5 step:5246 [D loss: 0.190667, acc.: 70.31%] [G loss: 0.549408]\n",
      "epoch:5 step:5247 [D loss: 0.235413, acc.: 60.16%] [G loss: 0.497914]\n",
      "epoch:5 step:5248 [D loss: 0.215203, acc.: 68.75%] [G loss: 0.551902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5249 [D loss: 0.156350, acc.: 76.56%] [G loss: 0.616124]\n",
      "epoch:5 step:5250 [D loss: 0.213208, acc.: 67.19%] [G loss: 0.549857]\n",
      "epoch:5 step:5251 [D loss: 0.247298, acc.: 57.03%] [G loss: 0.524240]\n",
      "epoch:5 step:5252 [D loss: 0.162398, acc.: 80.47%] [G loss: 0.584725]\n",
      "epoch:5 step:5253 [D loss: 0.232822, acc.: 64.06%] [G loss: 0.540163]\n",
      "epoch:5 step:5254 [D loss: 0.216867, acc.: 63.28%] [G loss: 0.525645]\n",
      "epoch:5 step:5255 [D loss: 0.215222, acc.: 67.97%] [G loss: 0.502881]\n",
      "epoch:5 step:5256 [D loss: 0.170285, acc.: 74.22%] [G loss: 0.572086]\n",
      "epoch:5 step:5257 [D loss: 0.207024, acc.: 67.97%] [G loss: 0.477569]\n",
      "epoch:5 step:5258 [D loss: 0.194301, acc.: 71.88%] [G loss: 0.501706]\n",
      "epoch:5 step:5259 [D loss: 0.149918, acc.: 76.56%] [G loss: 0.579463]\n",
      "epoch:5 step:5260 [D loss: 0.178676, acc.: 72.66%] [G loss: 0.571301]\n",
      "epoch:5 step:5261 [D loss: 0.227816, acc.: 65.62%] [G loss: 0.502720]\n",
      "epoch:5 step:5262 [D loss: 0.217328, acc.: 67.19%] [G loss: 0.514798]\n",
      "epoch:5 step:5263 [D loss: 0.223042, acc.: 64.06%] [G loss: 0.508590]\n",
      "epoch:5 step:5264 [D loss: 0.211011, acc.: 61.72%] [G loss: 0.494169]\n",
      "epoch:5 step:5265 [D loss: 0.214866, acc.: 64.06%] [G loss: 0.502723]\n",
      "epoch:5 step:5266 [D loss: 0.182356, acc.: 69.53%] [G loss: 0.573636]\n",
      "epoch:5 step:5267 [D loss: 0.197996, acc.: 67.19%] [G loss: 0.590158]\n",
      "epoch:5 step:5268 [D loss: 0.218426, acc.: 66.41%] [G loss: 0.504044]\n",
      "epoch:5 step:5269 [D loss: 0.225420, acc.: 61.72%] [G loss: 0.512728]\n",
      "epoch:5 step:5270 [D loss: 0.198478, acc.: 71.09%] [G loss: 0.483982]\n",
      "epoch:5 step:5271 [D loss: 0.203984, acc.: 74.22%] [G loss: 0.503281]\n",
      "epoch:5 step:5272 [D loss: 0.206823, acc.: 64.06%] [G loss: 0.507687]\n",
      "epoch:5 step:5273 [D loss: 0.190657, acc.: 75.78%] [G loss: 0.566678]\n",
      "epoch:5 step:5274 [D loss: 0.170794, acc.: 71.88%] [G loss: 0.578656]\n",
      "epoch:5 step:5275 [D loss: 0.233525, acc.: 67.19%] [G loss: 0.496736]\n",
      "epoch:5 step:5276 [D loss: 0.222992, acc.: 66.41%] [G loss: 0.522023]\n",
      "epoch:5 step:5277 [D loss: 0.171338, acc.: 78.12%] [G loss: 0.557533]\n",
      "epoch:5 step:5278 [D loss: 0.214488, acc.: 64.84%] [G loss: 0.526318]\n",
      "epoch:5 step:5279 [D loss: 0.220050, acc.: 60.16%] [G loss: 0.517371]\n",
      "epoch:5 step:5280 [D loss: 0.212720, acc.: 67.19%] [G loss: 0.490797]\n",
      "epoch:5 step:5281 [D loss: 0.225856, acc.: 64.84%] [G loss: 0.489880]\n",
      "epoch:5 step:5282 [D loss: 0.206560, acc.: 69.53%] [G loss: 0.521636]\n",
      "epoch:5 step:5283 [D loss: 0.175527, acc.: 77.34%] [G loss: 0.534199]\n",
      "epoch:5 step:5284 [D loss: 0.224509, acc.: 67.97%] [G loss: 0.509929]\n",
      "epoch:5 step:5285 [D loss: 0.259201, acc.: 56.25%] [G loss: 0.491076]\n",
      "epoch:5 step:5286 [D loss: 0.221475, acc.: 68.75%] [G loss: 0.513548]\n",
      "epoch:5 step:5287 [D loss: 0.207841, acc.: 66.41%] [G loss: 0.560357]\n",
      "epoch:5 step:5288 [D loss: 0.189582, acc.: 72.66%] [G loss: 0.575415]\n",
      "epoch:5 step:5289 [D loss: 0.219207, acc.: 61.72%] [G loss: 0.464216]\n",
      "epoch:5 step:5290 [D loss: 0.215968, acc.: 67.97%] [G loss: 0.488814]\n",
      "epoch:5 step:5291 [D loss: 0.203380, acc.: 67.19%] [G loss: 0.504831]\n",
      "epoch:5 step:5292 [D loss: 0.218909, acc.: 64.06%] [G loss: 0.525133]\n",
      "epoch:5 step:5293 [D loss: 0.176545, acc.: 73.44%] [G loss: 0.532368]\n",
      "epoch:5 step:5294 [D loss: 0.187092, acc.: 67.97%] [G loss: 0.524016]\n",
      "epoch:5 step:5295 [D loss: 0.218265, acc.: 64.84%] [G loss: 0.506422]\n",
      "epoch:5 step:5296 [D loss: 0.191414, acc.: 68.75%] [G loss: 0.517505]\n",
      "epoch:5 step:5297 [D loss: 0.195552, acc.: 70.31%] [G loss: 0.508760]\n",
      "epoch:5 step:5298 [D loss: 0.162266, acc.: 80.47%] [G loss: 0.558924]\n",
      "epoch:5 step:5299 [D loss: 0.214698, acc.: 67.97%] [G loss: 0.470069]\n",
      "epoch:5 step:5300 [D loss: 0.226166, acc.: 63.28%] [G loss: 0.482607]\n",
      "epoch:5 step:5301 [D loss: 0.200544, acc.: 65.62%] [G loss: 0.518955]\n",
      "epoch:5 step:5302 [D loss: 0.202652, acc.: 67.19%] [G loss: 0.518656]\n",
      "epoch:5 step:5303 [D loss: 0.176606, acc.: 78.91%] [G loss: 0.565331]\n",
      "epoch:5 step:5304 [D loss: 0.202091, acc.: 66.41%] [G loss: 0.522648]\n",
      "epoch:5 step:5305 [D loss: 0.194640, acc.: 69.53%] [G loss: 0.513943]\n",
      "epoch:5 step:5306 [D loss: 0.229982, acc.: 63.28%] [G loss: 0.510773]\n",
      "epoch:5 step:5307 [D loss: 0.249749, acc.: 60.94%] [G loss: 0.483956]\n",
      "epoch:5 step:5308 [D loss: 0.168983, acc.: 73.44%] [G loss: 0.555672]\n",
      "epoch:5 step:5309 [D loss: 0.172518, acc.: 79.69%] [G loss: 0.596660]\n",
      "epoch:5 step:5310 [D loss: 0.242384, acc.: 60.16%] [G loss: 0.499497]\n",
      "epoch:5 step:5311 [D loss: 0.212275, acc.: 68.75%] [G loss: 0.525661]\n",
      "epoch:5 step:5312 [D loss: 0.148189, acc.: 81.25%] [G loss: 0.560867]\n",
      "epoch:5 step:5313 [D loss: 0.248011, acc.: 58.59%] [G loss: 0.502287]\n",
      "epoch:5 step:5314 [D loss: 0.203081, acc.: 69.53%] [G loss: 0.513218]\n",
      "epoch:5 step:5315 [D loss: 0.216779, acc.: 67.97%] [G loss: 0.494661]\n",
      "epoch:5 step:5316 [D loss: 0.200469, acc.: 70.31%] [G loss: 0.555829]\n",
      "epoch:5 step:5317 [D loss: 0.202197, acc.: 73.44%] [G loss: 0.530914]\n",
      "epoch:5 step:5318 [D loss: 0.174688, acc.: 75.78%] [G loss: 0.545825]\n",
      "epoch:5 step:5319 [D loss: 0.174040, acc.: 75.78%] [G loss: 0.553971]\n",
      "epoch:5 step:5320 [D loss: 0.212665, acc.: 65.62%] [G loss: 0.521298]\n",
      "epoch:5 step:5321 [D loss: 0.259263, acc.: 62.50%] [G loss: 0.482437]\n",
      "epoch:5 step:5322 [D loss: 0.195180, acc.: 72.66%] [G loss: 0.502783]\n",
      "epoch:5 step:5323 [D loss: 0.202069, acc.: 68.75%] [G loss: 0.489237]\n",
      "epoch:5 step:5324 [D loss: 0.187189, acc.: 70.31%] [G loss: 0.532748]\n",
      "epoch:5 step:5325 [D loss: 0.196817, acc.: 67.97%] [G loss: 0.513642]\n",
      "epoch:5 step:5326 [D loss: 0.178348, acc.: 69.53%] [G loss: 0.592824]\n",
      "epoch:5 step:5327 [D loss: 0.152017, acc.: 80.47%] [G loss: 0.628916]\n",
      "epoch:5 step:5328 [D loss: 0.192587, acc.: 69.53%] [G loss: 0.555924]\n",
      "epoch:5 step:5329 [D loss: 0.213937, acc.: 66.41%] [G loss: 0.458530]\n",
      "epoch:5 step:5330 [D loss: 0.210077, acc.: 64.84%] [G loss: 0.515818]\n",
      "epoch:5 step:5331 [D loss: 0.188475, acc.: 75.00%] [G loss: 0.576057]\n",
      "epoch:5 step:5332 [D loss: 0.186923, acc.: 74.22%] [G loss: 0.562542]\n",
      "epoch:5 step:5333 [D loss: 0.176063, acc.: 78.12%] [G loss: 0.605686]\n",
      "epoch:5 step:5334 [D loss: 0.189748, acc.: 71.09%] [G loss: 0.589650]\n",
      "epoch:5 step:5335 [D loss: 0.184168, acc.: 75.00%] [G loss: 0.554739]\n",
      "epoch:5 step:5336 [D loss: 0.192238, acc.: 70.31%] [G loss: 0.545174]\n",
      "epoch:5 step:5337 [D loss: 0.257103, acc.: 54.69%] [G loss: 0.521651]\n",
      "epoch:5 step:5338 [D loss: 0.220639, acc.: 65.62%] [G loss: 0.547567]\n",
      "epoch:5 step:5339 [D loss: 0.192863, acc.: 72.66%] [G loss: 0.532726]\n",
      "epoch:5 step:5340 [D loss: 0.210026, acc.: 66.41%] [G loss: 0.540919]\n",
      "epoch:5 step:5341 [D loss: 0.204327, acc.: 68.75%] [G loss: 0.519690]\n",
      "epoch:5 step:5342 [D loss: 0.194765, acc.: 67.97%] [G loss: 0.557399]\n",
      "epoch:5 step:5343 [D loss: 0.217160, acc.: 61.72%] [G loss: 0.563985]\n",
      "epoch:5 step:5344 [D loss: 0.201218, acc.: 67.19%] [G loss: 0.497260]\n",
      "epoch:5 step:5345 [D loss: 0.190156, acc.: 70.31%] [G loss: 0.519776]\n",
      "epoch:5 step:5346 [D loss: 0.205684, acc.: 69.53%] [G loss: 0.546911]\n",
      "epoch:5 step:5347 [D loss: 0.197759, acc.: 69.53%] [G loss: 0.558107]\n",
      "epoch:5 step:5348 [D loss: 0.208236, acc.: 67.19%] [G loss: 0.496626]\n",
      "epoch:5 step:5349 [D loss: 0.221648, acc.: 58.59%] [G loss: 0.539712]\n",
      "epoch:5 step:5350 [D loss: 0.234882, acc.: 60.94%] [G loss: 0.564940]\n",
      "epoch:5 step:5351 [D loss: 0.216149, acc.: 64.84%] [G loss: 0.553644]\n",
      "epoch:5 step:5352 [D loss: 0.234343, acc.: 62.50%] [G loss: 0.491578]\n",
      "epoch:5 step:5353 [D loss: 0.228863, acc.: 67.97%] [G loss: 0.471190]\n",
      "epoch:5 step:5354 [D loss: 0.183325, acc.: 71.09%] [G loss: 0.536011]\n",
      "epoch:5 step:5355 [D loss: 0.232778, acc.: 60.94%] [G loss: 0.484153]\n",
      "epoch:5 step:5356 [D loss: 0.222714, acc.: 65.62%] [G loss: 0.513262]\n",
      "epoch:5 step:5357 [D loss: 0.214225, acc.: 67.19%] [G loss: 0.548087]\n",
      "epoch:5 step:5358 [D loss: 0.227232, acc.: 69.53%] [G loss: 0.548136]\n",
      "epoch:5 step:5359 [D loss: 0.232718, acc.: 61.72%] [G loss: 0.553472]\n",
      "epoch:5 step:5360 [D loss: 0.240637, acc.: 61.72%] [G loss: 0.587206]\n",
      "epoch:5 step:5361 [D loss: 0.204748, acc.: 71.88%] [G loss: 0.533720]\n",
      "epoch:5 step:5362 [D loss: 0.201457, acc.: 72.66%] [G loss: 0.534031]\n",
      "epoch:5 step:5363 [D loss: 0.194461, acc.: 75.00%] [G loss: 0.545237]\n",
      "epoch:5 step:5364 [D loss: 0.187695, acc.: 73.44%] [G loss: 0.570099]\n",
      "epoch:5 step:5365 [D loss: 0.226629, acc.: 60.16%] [G loss: 0.461397]\n",
      "epoch:5 step:5366 [D loss: 0.178208, acc.: 75.78%] [G loss: 0.537196]\n",
      "epoch:5 step:5367 [D loss: 0.208295, acc.: 70.31%] [G loss: 0.508259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5368 [D loss: 0.184632, acc.: 72.66%] [G loss: 0.513206]\n",
      "epoch:5 step:5369 [D loss: 0.195372, acc.: 69.53%] [G loss: 0.509224]\n",
      "epoch:5 step:5370 [D loss: 0.193151, acc.: 69.53%] [G loss: 0.496956]\n",
      "epoch:5 step:5371 [D loss: 0.217595, acc.: 61.72%] [G loss: 0.536380]\n",
      "epoch:5 step:5372 [D loss: 0.209452, acc.: 70.31%] [G loss: 0.532153]\n",
      "epoch:5 step:5373 [D loss: 0.190830, acc.: 72.66%] [G loss: 0.515009]\n",
      "epoch:5 step:5374 [D loss: 0.209679, acc.: 68.75%] [G loss: 0.554233]\n",
      "epoch:5 step:5375 [D loss: 0.189150, acc.: 69.53%] [G loss: 0.594054]\n",
      "epoch:5 step:5376 [D loss: 0.156993, acc.: 77.34%] [G loss: 0.586749]\n",
      "epoch:5 step:5377 [D loss: 0.166474, acc.: 76.56%] [G loss: 0.569830]\n",
      "epoch:5 step:5378 [D loss: 0.175187, acc.: 74.22%] [G loss: 0.570530]\n",
      "epoch:5 step:5379 [D loss: 0.185263, acc.: 74.22%] [G loss: 0.561101]\n",
      "epoch:5 step:5380 [D loss: 0.205342, acc.: 67.97%] [G loss: 0.557220]\n",
      "epoch:5 step:5381 [D loss: 0.210345, acc.: 67.19%] [G loss: 0.537358]\n",
      "epoch:5 step:5382 [D loss: 0.173867, acc.: 75.78%] [G loss: 0.584516]\n",
      "epoch:5 step:5383 [D loss: 0.223402, acc.: 65.62%] [G loss: 0.508349]\n",
      "epoch:5 step:5384 [D loss: 0.169423, acc.: 73.44%] [G loss: 0.548105]\n",
      "epoch:5 step:5385 [D loss: 0.175724, acc.: 75.00%] [G loss: 0.578226]\n",
      "epoch:5 step:5386 [D loss: 0.215531, acc.: 65.62%] [G loss: 0.585872]\n",
      "epoch:5 step:5387 [D loss: 0.237769, acc.: 60.16%] [G loss: 0.554322]\n",
      "epoch:5 step:5388 [D loss: 0.222635, acc.: 64.06%] [G loss: 0.517150]\n",
      "epoch:5 step:5389 [D loss: 0.210057, acc.: 64.84%] [G loss: 0.517325]\n",
      "epoch:5 step:5390 [D loss: 0.192042, acc.: 69.53%] [G loss: 0.534944]\n",
      "epoch:5 step:5391 [D loss: 0.197989, acc.: 71.88%] [G loss: 0.514826]\n",
      "epoch:5 step:5392 [D loss: 0.172305, acc.: 73.44%] [G loss: 0.547167]\n",
      "epoch:5 step:5393 [D loss: 0.213170, acc.: 67.97%] [G loss: 0.541816]\n",
      "epoch:5 step:5394 [D loss: 0.195149, acc.: 68.75%] [G loss: 0.579889]\n",
      "epoch:5 step:5395 [D loss: 0.249078, acc.: 55.47%] [G loss: 0.487727]\n",
      "epoch:5 step:5396 [D loss: 0.206352, acc.: 68.75%] [G loss: 0.522997]\n",
      "epoch:5 step:5397 [D loss: 0.184163, acc.: 70.31%] [G loss: 0.573645]\n",
      "epoch:5 step:5398 [D loss: 0.227345, acc.: 57.81%] [G loss: 0.523051]\n",
      "epoch:5 step:5399 [D loss: 0.174276, acc.: 75.00%] [G loss: 0.564999]\n",
      "epoch:5 step:5400 [D loss: 0.214880, acc.: 64.06%] [G loss: 0.512849]\n",
      "##############\n",
      "[2.87153368 1.93790503 6.6505663  4.89886592 3.95427346 5.84534175\n",
      " 4.57042868 4.87059128 4.68117463 3.89154909]\n",
      "##########\n",
      "epoch:5 step:5401 [D loss: 0.255948, acc.: 58.59%] [G loss: 0.530830]\n",
      "epoch:5 step:5402 [D loss: 0.206100, acc.: 69.53%] [G loss: 0.492358]\n",
      "epoch:5 step:5403 [D loss: 0.247709, acc.: 60.16%] [G loss: 0.515225]\n",
      "epoch:5 step:5404 [D loss: 0.202520, acc.: 67.97%] [G loss: 0.569949]\n",
      "epoch:5 step:5405 [D loss: 0.252256, acc.: 60.16%] [G loss: 0.522168]\n",
      "epoch:5 step:5406 [D loss: 0.228560, acc.: 60.16%] [G loss: 0.501471]\n",
      "epoch:5 step:5407 [D loss: 0.227043, acc.: 60.94%] [G loss: 0.508478]\n",
      "epoch:5 step:5408 [D loss: 0.203879, acc.: 68.75%] [G loss: 0.499091]\n",
      "epoch:5 step:5409 [D loss: 0.200737, acc.: 68.75%] [G loss: 0.500731]\n",
      "epoch:5 step:5410 [D loss: 0.171947, acc.: 73.44%] [G loss: 0.588908]\n",
      "epoch:5 step:5411 [D loss: 0.218357, acc.: 63.28%] [G loss: 0.561311]\n",
      "epoch:5 step:5412 [D loss: 0.222471, acc.: 64.84%] [G loss: 0.505777]\n",
      "epoch:5 step:5413 [D loss: 0.237929, acc.: 61.72%] [G loss: 0.515370]\n",
      "epoch:5 step:5414 [D loss: 0.174895, acc.: 76.56%] [G loss: 0.596260]\n",
      "epoch:5 step:5415 [D loss: 0.184159, acc.: 72.66%] [G loss: 0.535855]\n",
      "epoch:5 step:5416 [D loss: 0.207553, acc.: 67.19%] [G loss: 0.552768]\n",
      "epoch:5 step:5417 [D loss: 0.194579, acc.: 75.00%] [G loss: 0.541532]\n",
      "epoch:5 step:5418 [D loss: 0.199671, acc.: 68.75%] [G loss: 0.533153]\n",
      "epoch:5 step:5419 [D loss: 0.218674, acc.: 67.97%] [G loss: 0.488774]\n",
      "epoch:5 step:5420 [D loss: 0.206358, acc.: 67.97%] [G loss: 0.504090]\n",
      "epoch:5 step:5421 [D loss: 0.185043, acc.: 71.09%] [G loss: 0.494313]\n",
      "epoch:5 step:5422 [D loss: 0.187696, acc.: 67.19%] [G loss: 0.543863]\n",
      "epoch:5 step:5423 [D loss: 0.213060, acc.: 65.62%] [G loss: 0.516139]\n",
      "epoch:5 step:5424 [D loss: 0.232118, acc.: 64.84%] [G loss: 0.504443]\n",
      "epoch:5 step:5425 [D loss: 0.237592, acc.: 61.72%] [G loss: 0.546465]\n",
      "epoch:5 step:5426 [D loss: 0.218408, acc.: 67.97%] [G loss: 0.539888]\n",
      "epoch:5 step:5427 [D loss: 0.222531, acc.: 66.41%] [G loss: 0.519636]\n",
      "epoch:5 step:5428 [D loss: 0.192382, acc.: 65.62%] [G loss: 0.499434]\n",
      "epoch:5 step:5429 [D loss: 0.204609, acc.: 67.97%] [G loss: 0.537576]\n",
      "epoch:5 step:5430 [D loss: 0.207066, acc.: 68.75%] [G loss: 0.524872]\n",
      "epoch:5 step:5431 [D loss: 0.210793, acc.: 71.09%] [G loss: 0.526186]\n",
      "epoch:5 step:5432 [D loss: 0.184559, acc.: 70.31%] [G loss: 0.533249]\n",
      "epoch:5 step:5433 [D loss: 0.195039, acc.: 70.31%] [G loss: 0.560686]\n",
      "epoch:5 step:5434 [D loss: 0.213070, acc.: 73.44%] [G loss: 0.503575]\n",
      "epoch:5 step:5435 [D loss: 0.193548, acc.: 70.31%] [G loss: 0.505551]\n",
      "epoch:5 step:5436 [D loss: 0.211509, acc.: 67.19%] [G loss: 0.524134]\n",
      "epoch:5 step:5437 [D loss: 0.209608, acc.: 64.06%] [G loss: 0.498645]\n",
      "epoch:5 step:5438 [D loss: 0.179782, acc.: 73.44%] [G loss: 0.551973]\n",
      "epoch:5 step:5439 [D loss: 0.178116, acc.: 66.41%] [G loss: 0.534187]\n",
      "epoch:5 step:5440 [D loss: 0.194233, acc.: 67.97%] [G loss: 0.550170]\n",
      "epoch:5 step:5441 [D loss: 0.193011, acc.: 67.19%] [G loss: 0.528721]\n",
      "epoch:5 step:5442 [D loss: 0.189318, acc.: 70.31%] [G loss: 0.497772]\n",
      "epoch:5 step:5443 [D loss: 0.179850, acc.: 73.44%] [G loss: 0.534085]\n",
      "epoch:5 step:5444 [D loss: 0.229919, acc.: 62.50%] [G loss: 0.515587]\n",
      "epoch:5 step:5445 [D loss: 0.214848, acc.: 69.53%] [G loss: 0.482888]\n",
      "epoch:5 step:5446 [D loss: 0.197207, acc.: 72.66%] [G loss: 0.498692]\n",
      "epoch:5 step:5447 [D loss: 0.207357, acc.: 66.41%] [G loss: 0.497748]\n",
      "epoch:5 step:5448 [D loss: 0.193819, acc.: 67.97%] [G loss: 0.474363]\n",
      "epoch:5 step:5449 [D loss: 0.213413, acc.: 62.50%] [G loss: 0.467630]\n",
      "epoch:5 step:5450 [D loss: 0.283286, acc.: 50.78%] [G loss: 0.485158]\n",
      "epoch:5 step:5451 [D loss: 0.222241, acc.: 65.62%] [G loss: 0.481947]\n",
      "epoch:5 step:5452 [D loss: 0.191644, acc.: 74.22%] [G loss: 0.477842]\n",
      "epoch:5 step:5453 [D loss: 0.246873, acc.: 57.03%] [G loss: 0.492546]\n",
      "epoch:5 step:5454 [D loss: 0.169479, acc.: 75.78%] [G loss: 0.569477]\n",
      "epoch:5 step:5455 [D loss: 0.204638, acc.: 67.19%] [G loss: 0.538513]\n",
      "epoch:5 step:5456 [D loss: 0.210766, acc.: 69.53%] [G loss: 0.526213]\n",
      "epoch:5 step:5457 [D loss: 0.209193, acc.: 70.31%] [G loss: 0.545094]\n",
      "epoch:5 step:5458 [D loss: 0.212387, acc.: 67.97%] [G loss: 0.520911]\n",
      "epoch:5 step:5459 [D loss: 0.208047, acc.: 68.75%] [G loss: 0.501150]\n",
      "epoch:5 step:5460 [D loss: 0.166253, acc.: 75.78%] [G loss: 0.551336]\n",
      "epoch:5 step:5461 [D loss: 0.232402, acc.: 62.50%] [G loss: 0.497175]\n",
      "epoch:5 step:5462 [D loss: 0.204397, acc.: 63.28%] [G loss: 0.560330]\n",
      "epoch:5 step:5463 [D loss: 0.183935, acc.: 72.66%] [G loss: 0.538507]\n",
      "epoch:5 step:5464 [D loss: 0.241041, acc.: 63.28%] [G loss: 0.489567]\n",
      "epoch:5 step:5465 [D loss: 0.189550, acc.: 73.44%] [G loss: 0.493221]\n",
      "epoch:5 step:5466 [D loss: 0.186700, acc.: 71.88%] [G loss: 0.575335]\n",
      "epoch:5 step:5467 [D loss: 0.190021, acc.: 71.88%] [G loss: 0.576745]\n",
      "epoch:5 step:5468 [D loss: 0.224190, acc.: 61.72%] [G loss: 0.533960]\n",
      "epoch:5 step:5469 [D loss: 0.249055, acc.: 57.81%] [G loss: 0.495659]\n",
      "epoch:5 step:5470 [D loss: 0.218475, acc.: 67.19%] [G loss: 0.475098]\n",
      "epoch:5 step:5471 [D loss: 0.172274, acc.: 76.56%] [G loss: 0.559331]\n",
      "epoch:5 step:5472 [D loss: 0.242646, acc.: 64.84%] [G loss: 0.522485]\n",
      "epoch:5 step:5473 [D loss: 0.237693, acc.: 63.28%] [G loss: 0.503371]\n",
      "epoch:5 step:5474 [D loss: 0.214634, acc.: 64.84%] [G loss: 0.499403]\n",
      "epoch:5 step:5475 [D loss: 0.161979, acc.: 79.69%] [G loss: 0.566130]\n",
      "epoch:5 step:5476 [D loss: 0.242582, acc.: 60.16%] [G loss: 0.500464]\n",
      "epoch:5 step:5477 [D loss: 0.161341, acc.: 77.34%] [G loss: 0.524610]\n",
      "epoch:5 step:5478 [D loss: 0.209929, acc.: 64.84%] [G loss: 0.494328]\n",
      "epoch:5 step:5479 [D loss: 0.253823, acc.: 52.34%] [G loss: 0.504135]\n",
      "epoch:5 step:5480 [D loss: 0.202797, acc.: 71.88%] [G loss: 0.534626]\n",
      "epoch:5 step:5481 [D loss: 0.203882, acc.: 69.53%] [G loss: 0.560814]\n",
      "epoch:5 step:5482 [D loss: 0.227875, acc.: 60.94%] [G loss: 0.553188]\n",
      "epoch:5 step:5483 [D loss: 0.227836, acc.: 63.28%] [G loss: 0.518116]\n",
      "epoch:5 step:5484 [D loss: 0.219075, acc.: 64.84%] [G loss: 0.484507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5485 [D loss: 0.203134, acc.: 67.97%] [G loss: 0.477175]\n",
      "epoch:5 step:5486 [D loss: 0.212849, acc.: 65.62%] [G loss: 0.506771]\n",
      "epoch:5 step:5487 [D loss: 0.175602, acc.: 70.31%] [G loss: 0.578534]\n",
      "epoch:5 step:5488 [D loss: 0.220157, acc.: 64.06%] [G loss: 0.494006]\n",
      "epoch:5 step:5489 [D loss: 0.231121, acc.: 60.16%] [G loss: 0.496354]\n",
      "epoch:5 step:5490 [D loss: 0.210609, acc.: 65.62%] [G loss: 0.485841]\n",
      "epoch:5 step:5491 [D loss: 0.205411, acc.: 71.09%] [G loss: 0.551713]\n",
      "epoch:5 step:5492 [D loss: 0.176220, acc.: 75.00%] [G loss: 0.547789]\n",
      "epoch:5 step:5493 [D loss: 0.226623, acc.: 66.41%] [G loss: 0.519356]\n",
      "epoch:5 step:5494 [D loss: 0.213568, acc.: 61.72%] [G loss: 0.549624]\n",
      "epoch:5 step:5495 [D loss: 0.193924, acc.: 75.00%] [G loss: 0.502648]\n",
      "epoch:5 step:5496 [D loss: 0.227085, acc.: 57.03%] [G loss: 0.528179]\n",
      "epoch:5 step:5497 [D loss: 0.269519, acc.: 53.91%] [G loss: 0.473009]\n",
      "epoch:5 step:5498 [D loss: 0.208126, acc.: 72.66%] [G loss: 0.509511]\n",
      "epoch:5 step:5499 [D loss: 0.201875, acc.: 65.62%] [G loss: 0.551639]\n",
      "epoch:5 step:5500 [D loss: 0.215404, acc.: 65.62%] [G loss: 0.526936]\n",
      "epoch:5 step:5501 [D loss: 0.161532, acc.: 78.12%] [G loss: 0.628064]\n",
      "epoch:5 step:5502 [D loss: 0.255888, acc.: 52.34%] [G loss: 0.475392]\n",
      "epoch:5 step:5503 [D loss: 0.206259, acc.: 63.28%] [G loss: 0.505168]\n",
      "epoch:5 step:5504 [D loss: 0.212096, acc.: 62.50%] [G loss: 0.469963]\n",
      "epoch:5 step:5505 [D loss: 0.212083, acc.: 70.31%] [G loss: 0.552575]\n",
      "epoch:5 step:5506 [D loss: 0.201491, acc.: 64.84%] [G loss: 0.467379]\n",
      "epoch:5 step:5507 [D loss: 0.181536, acc.: 78.91%] [G loss: 0.521254]\n",
      "epoch:5 step:5508 [D loss: 0.196497, acc.: 67.19%] [G loss: 0.532560]\n",
      "epoch:5 step:5509 [D loss: 0.237737, acc.: 63.28%] [G loss: 0.485501]\n",
      "epoch:5 step:5510 [D loss: 0.184165, acc.: 73.44%] [G loss: 0.518790]\n",
      "epoch:5 step:5511 [D loss: 0.217516, acc.: 64.06%] [G loss: 0.499839]\n",
      "epoch:5 step:5512 [D loss: 0.254556, acc.: 54.69%] [G loss: 0.494291]\n",
      "epoch:5 step:5513 [D loss: 0.216877, acc.: 64.84%] [G loss: 0.511095]\n",
      "epoch:5 step:5514 [D loss: 0.198037, acc.: 70.31%] [G loss: 0.547323]\n",
      "epoch:5 step:5515 [D loss: 0.229269, acc.: 66.41%] [G loss: 0.490474]\n",
      "epoch:5 step:5516 [D loss: 0.205162, acc.: 65.62%] [G loss: 0.545902]\n",
      "epoch:5 step:5517 [D loss: 0.194621, acc.: 70.31%] [G loss: 0.561885]\n",
      "epoch:5 step:5518 [D loss: 0.197593, acc.: 64.84%] [G loss: 0.606689]\n",
      "epoch:5 step:5519 [D loss: 0.212716, acc.: 64.84%] [G loss: 0.530300]\n",
      "epoch:5 step:5520 [D loss: 0.195439, acc.: 71.09%] [G loss: 0.508837]\n",
      "epoch:5 step:5521 [D loss: 0.202234, acc.: 66.41%] [G loss: 0.569028]\n",
      "epoch:5 step:5522 [D loss: 0.181710, acc.: 69.53%] [G loss: 0.590328]\n",
      "epoch:5 step:5523 [D loss: 0.188511, acc.: 70.31%] [G loss: 0.540301]\n",
      "epoch:5 step:5524 [D loss: 0.227221, acc.: 65.62%] [G loss: 0.487257]\n",
      "epoch:5 step:5525 [D loss: 0.221047, acc.: 61.72%] [G loss: 0.465326]\n",
      "epoch:5 step:5526 [D loss: 0.188261, acc.: 71.88%] [G loss: 0.533781]\n",
      "epoch:5 step:5527 [D loss: 0.206430, acc.: 68.75%] [G loss: 0.540222]\n",
      "epoch:5 step:5528 [D loss: 0.200384, acc.: 70.31%] [G loss: 0.532793]\n",
      "epoch:5 step:5529 [D loss: 0.215638, acc.: 67.19%] [G loss: 0.523195]\n",
      "epoch:5 step:5530 [D loss: 0.224887, acc.: 60.16%] [G loss: 0.471368]\n",
      "epoch:5 step:5531 [D loss: 0.209396, acc.: 66.41%] [G loss: 0.490805]\n",
      "epoch:5 step:5532 [D loss: 0.214970, acc.: 64.06%] [G loss: 0.478708]\n",
      "epoch:5 step:5533 [D loss: 0.224925, acc.: 64.06%] [G loss: 0.489783]\n",
      "epoch:5 step:5534 [D loss: 0.201567, acc.: 64.84%] [G loss: 0.548725]\n",
      "epoch:5 step:5535 [D loss: 0.219351, acc.: 64.06%] [G loss: 0.515192]\n",
      "epoch:5 step:5536 [D loss: 0.209566, acc.: 65.62%] [G loss: 0.519551]\n",
      "epoch:5 step:5537 [D loss: 0.203710, acc.: 73.44%] [G loss: 0.507236]\n",
      "epoch:5 step:5538 [D loss: 0.201084, acc.: 69.53%] [G loss: 0.556015]\n",
      "epoch:5 step:5539 [D loss: 0.205168, acc.: 72.66%] [G loss: 0.555367]\n",
      "epoch:5 step:5540 [D loss: 0.220353, acc.: 67.97%] [G loss: 0.526439]\n",
      "epoch:5 step:5541 [D loss: 0.230927, acc.: 60.16%] [G loss: 0.488211]\n",
      "epoch:5 step:5542 [D loss: 0.179124, acc.: 73.44%] [G loss: 0.519386]\n",
      "epoch:5 step:5543 [D loss: 0.233667, acc.: 58.59%] [G loss: 0.482076]\n",
      "epoch:5 step:5544 [D loss: 0.223801, acc.: 60.94%] [G loss: 0.570334]\n",
      "epoch:5 step:5545 [D loss: 0.163542, acc.: 81.25%] [G loss: 0.612498]\n",
      "epoch:5 step:5546 [D loss: 0.256105, acc.: 57.03%] [G loss: 0.485332]\n",
      "epoch:5 step:5547 [D loss: 0.206894, acc.: 67.97%] [G loss: 0.453881]\n",
      "epoch:5 step:5548 [D loss: 0.212962, acc.: 67.97%] [G loss: 0.472032]\n",
      "epoch:5 step:5549 [D loss: 0.205998, acc.: 70.31%] [G loss: 0.472737]\n",
      "epoch:5 step:5550 [D loss: 0.211490, acc.: 63.28%] [G loss: 0.498409]\n",
      "epoch:5 step:5551 [D loss: 0.222925, acc.: 64.84%] [G loss: 0.514322]\n",
      "epoch:5 step:5552 [D loss: 0.245626, acc.: 55.47%] [G loss: 0.465084]\n",
      "epoch:5 step:5553 [D loss: 0.226923, acc.: 64.84%] [G loss: 0.489490]\n",
      "epoch:5 step:5554 [D loss: 0.237771, acc.: 60.94%] [G loss: 0.498532]\n",
      "epoch:5 step:5555 [D loss: 0.175755, acc.: 72.66%] [G loss: 0.536252]\n",
      "epoch:5 step:5556 [D loss: 0.186420, acc.: 71.09%] [G loss: 0.524302]\n",
      "epoch:5 step:5557 [D loss: 0.214929, acc.: 64.84%] [G loss: 0.479528]\n",
      "epoch:5 step:5558 [D loss: 0.209628, acc.: 67.19%] [G loss: 0.500486]\n",
      "epoch:5 step:5559 [D loss: 0.240208, acc.: 59.38%] [G loss: 0.453632]\n",
      "epoch:5 step:5560 [D loss: 0.163364, acc.: 74.22%] [G loss: 0.560924]\n",
      "epoch:5 step:5561 [D loss: 0.242868, acc.: 60.16%] [G loss: 0.467483]\n",
      "epoch:5 step:5562 [D loss: 0.217783, acc.: 64.06%] [G loss: 0.478218]\n",
      "epoch:5 step:5563 [D loss: 0.194927, acc.: 73.44%] [G loss: 0.517627]\n",
      "epoch:5 step:5564 [D loss: 0.227967, acc.: 60.94%] [G loss: 0.541337]\n",
      "epoch:5 step:5565 [D loss: 0.229810, acc.: 59.38%] [G loss: 0.485699]\n",
      "epoch:5 step:5566 [D loss: 0.207784, acc.: 69.53%] [G loss: 0.499203]\n",
      "epoch:5 step:5567 [D loss: 0.211049, acc.: 69.53%] [G loss: 0.511707]\n",
      "epoch:5 step:5568 [D loss: 0.225879, acc.: 63.28%] [G loss: 0.508249]\n",
      "epoch:5 step:5569 [D loss: 0.153485, acc.: 81.25%] [G loss: 0.547497]\n",
      "epoch:5 step:5570 [D loss: 0.206695, acc.: 65.62%] [G loss: 0.550082]\n",
      "epoch:5 step:5571 [D loss: 0.189731, acc.: 71.88%] [G loss: 0.584823]\n",
      "epoch:5 step:5572 [D loss: 0.211059, acc.: 67.19%] [G loss: 0.523973]\n",
      "epoch:5 step:5573 [D loss: 0.170367, acc.: 78.12%] [G loss: 0.555710]\n",
      "epoch:5 step:5574 [D loss: 0.189935, acc.: 71.88%] [G loss: 0.526553]\n",
      "epoch:5 step:5575 [D loss: 0.179594, acc.: 73.44%] [G loss: 0.567083]\n",
      "epoch:5 step:5576 [D loss: 0.255378, acc.: 53.91%] [G loss: 0.491574]\n",
      "epoch:5 step:5577 [D loss: 0.233613, acc.: 59.38%] [G loss: 0.507077]\n",
      "epoch:5 step:5578 [D loss: 0.182478, acc.: 72.66%] [G loss: 0.544464]\n",
      "epoch:5 step:5579 [D loss: 0.161103, acc.: 74.22%] [G loss: 0.571625]\n",
      "epoch:5 step:5580 [D loss: 0.239528, acc.: 64.84%] [G loss: 0.548303]\n",
      "epoch:5 step:5581 [D loss: 0.203520, acc.: 69.53%] [G loss: 0.542944]\n",
      "epoch:5 step:5582 [D loss: 0.202990, acc.: 69.53%] [G loss: 0.551561]\n",
      "epoch:5 step:5583 [D loss: 0.191740, acc.: 71.88%] [G loss: 0.553602]\n",
      "epoch:5 step:5584 [D loss: 0.171510, acc.: 75.00%] [G loss: 0.552228]\n",
      "epoch:5 step:5585 [D loss: 0.171572, acc.: 74.22%] [G loss: 0.613638]\n",
      "epoch:5 step:5586 [D loss: 0.204665, acc.: 67.97%] [G loss: 0.546239]\n",
      "epoch:5 step:5587 [D loss: 0.232174, acc.: 66.41%] [G loss: 0.515096]\n",
      "epoch:5 step:5588 [D loss: 0.196676, acc.: 67.97%] [G loss: 0.504353]\n",
      "epoch:5 step:5589 [D loss: 0.200374, acc.: 73.44%] [G loss: 0.544438]\n",
      "epoch:5 step:5590 [D loss: 0.200995, acc.: 67.19%] [G loss: 0.568778]\n",
      "epoch:5 step:5591 [D loss: 0.206441, acc.: 73.44%] [G loss: 0.596147]\n",
      "epoch:5 step:5592 [D loss: 0.230026, acc.: 64.84%] [G loss: 0.492820]\n",
      "epoch:5 step:5593 [D loss: 0.210442, acc.: 68.75%] [G loss: 0.503745]\n",
      "epoch:5 step:5594 [D loss: 0.209318, acc.: 65.62%] [G loss: 0.522805]\n",
      "epoch:5 step:5595 [D loss: 0.181223, acc.: 75.00%] [G loss: 0.537218]\n",
      "epoch:5 step:5596 [D loss: 0.187018, acc.: 67.97%] [G loss: 0.612507]\n",
      "epoch:5 step:5597 [D loss: 0.158910, acc.: 75.00%] [G loss: 0.611717]\n",
      "epoch:5 step:5598 [D loss: 0.206756, acc.: 70.31%] [G loss: 0.525861]\n",
      "epoch:5 step:5599 [D loss: 0.220308, acc.: 64.06%] [G loss: 0.521624]\n",
      "epoch:5 step:5600 [D loss: 0.247038, acc.: 57.81%] [G loss: 0.510976]\n",
      "##############\n",
      "[2.66641985 1.88836093 6.35625406 4.94331079 3.97480986 5.98877585\n",
      " 4.81530303 4.79104922 4.8342976  3.88107214]\n",
      "##########\n",
      "epoch:5 step:5601 [D loss: 0.183768, acc.: 71.88%] [G loss: 0.519094]\n",
      "epoch:5 step:5602 [D loss: 0.244633, acc.: 58.59%] [G loss: 0.481905]\n",
      "epoch:5 step:5603 [D loss: 0.157558, acc.: 81.25%] [G loss: 0.606706]\n",
      "epoch:5 step:5604 [D loss: 0.192996, acc.: 68.75%] [G loss: 0.648306]\n",
      "epoch:5 step:5605 [D loss: 0.295723, acc.: 57.03%] [G loss: 0.543071]\n",
      "epoch:5 step:5606 [D loss: 0.192385, acc.: 66.41%] [G loss: 0.561225]\n",
      "epoch:5 step:5607 [D loss: 0.201542, acc.: 69.53%] [G loss: 0.548745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5608 [D loss: 0.181614, acc.: 75.78%] [G loss: 0.535128]\n",
      "epoch:5 step:5609 [D loss: 0.168783, acc.: 78.91%] [G loss: 0.561405]\n",
      "epoch:5 step:5610 [D loss: 0.128505, acc.: 82.03%] [G loss: 0.659722]\n",
      "epoch:5 step:5611 [D loss: 0.163347, acc.: 76.56%] [G loss: 0.622783]\n",
      "epoch:5 step:5612 [D loss: 0.176235, acc.: 76.56%] [G loss: 0.661885]\n",
      "epoch:5 step:5613 [D loss: 0.346878, acc.: 56.25%] [G loss: 0.582693]\n",
      "epoch:5 step:5614 [D loss: 0.165817, acc.: 78.91%] [G loss: 0.675667]\n",
      "epoch:5 step:5615 [D loss: 0.180260, acc.: 74.22%] [G loss: 0.647090]\n",
      "epoch:5 step:5616 [D loss: 0.217863, acc.: 60.94%] [G loss: 0.504452]\n",
      "epoch:5 step:5617 [D loss: 0.216575, acc.: 67.19%] [G loss: 0.450203]\n",
      "epoch:5 step:5618 [D loss: 0.184557, acc.: 70.31%] [G loss: 0.564178]\n",
      "epoch:5 step:5619 [D loss: 0.216648, acc.: 69.53%] [G loss: 0.575514]\n",
      "epoch:5 step:5620 [D loss: 0.186312, acc.: 71.88%] [G loss: 0.583315]\n",
      "epoch:5 step:5621 [D loss: 0.164502, acc.: 76.56%] [G loss: 0.629975]\n",
      "epoch:5 step:5622 [D loss: 0.151761, acc.: 78.12%] [G loss: 0.645882]\n",
      "epoch:6 step:5623 [D loss: 0.249030, acc.: 64.06%] [G loss: 0.609667]\n",
      "epoch:6 step:5624 [D loss: 0.238787, acc.: 63.28%] [G loss: 0.534299]\n",
      "epoch:6 step:5625 [D loss: 0.222619, acc.: 63.28%] [G loss: 0.501140]\n",
      "epoch:6 step:5626 [D loss: 0.210226, acc.: 66.41%] [G loss: 0.502464]\n",
      "epoch:6 step:5627 [D loss: 0.226483, acc.: 58.59%] [G loss: 0.487745]\n",
      "epoch:6 step:5628 [D loss: 0.214305, acc.: 61.72%] [G loss: 0.556931]\n",
      "epoch:6 step:5629 [D loss: 0.181381, acc.: 75.00%] [G loss: 0.576136]\n",
      "epoch:6 step:5630 [D loss: 0.203040, acc.: 72.66%] [G loss: 0.500322]\n",
      "epoch:6 step:5631 [D loss: 0.201124, acc.: 70.31%] [G loss: 0.487565]\n",
      "epoch:6 step:5632 [D loss: 0.182351, acc.: 74.22%] [G loss: 0.495555]\n",
      "epoch:6 step:5633 [D loss: 0.182786, acc.: 71.88%] [G loss: 0.562650]\n",
      "epoch:6 step:5634 [D loss: 0.212059, acc.: 67.97%] [G loss: 0.536988]\n",
      "epoch:6 step:5635 [D loss: 0.205729, acc.: 70.31%] [G loss: 0.524416]\n",
      "epoch:6 step:5636 [D loss: 0.209891, acc.: 60.94%] [G loss: 0.517929]\n",
      "epoch:6 step:5637 [D loss: 0.176385, acc.: 72.66%] [G loss: 0.554799]\n",
      "epoch:6 step:5638 [D loss: 0.195942, acc.: 74.22%] [G loss: 0.590249]\n",
      "epoch:6 step:5639 [D loss: 0.229085, acc.: 59.38%] [G loss: 0.538521]\n",
      "epoch:6 step:5640 [D loss: 0.234261, acc.: 64.06%] [G loss: 0.516215]\n",
      "epoch:6 step:5641 [D loss: 0.236442, acc.: 60.94%] [G loss: 0.475147]\n",
      "epoch:6 step:5642 [D loss: 0.226740, acc.: 60.94%] [G loss: 0.485553]\n",
      "epoch:6 step:5643 [D loss: 0.200693, acc.: 68.75%] [G loss: 0.542750]\n",
      "epoch:6 step:5644 [D loss: 0.169343, acc.: 76.56%] [G loss: 0.589657]\n",
      "epoch:6 step:5645 [D loss: 0.218153, acc.: 60.94%] [G loss: 0.520694]\n",
      "epoch:6 step:5646 [D loss: 0.205770, acc.: 67.19%] [G loss: 0.511996]\n",
      "epoch:6 step:5647 [D loss: 0.192271, acc.: 68.75%] [G loss: 0.504260]\n",
      "epoch:6 step:5648 [D loss: 0.199537, acc.: 71.88%] [G loss: 0.542413]\n",
      "epoch:6 step:5649 [D loss: 0.236730, acc.: 60.94%] [G loss: 0.515007]\n",
      "epoch:6 step:5650 [D loss: 0.197799, acc.: 66.41%] [G loss: 0.515049]\n",
      "epoch:6 step:5651 [D loss: 0.179130, acc.: 71.88%] [G loss: 0.515444]\n",
      "epoch:6 step:5652 [D loss: 0.223314, acc.: 62.50%] [G loss: 0.522262]\n",
      "epoch:6 step:5653 [D loss: 0.223680, acc.: 65.62%] [G loss: 0.470793]\n",
      "epoch:6 step:5654 [D loss: 0.202121, acc.: 65.62%] [G loss: 0.556447]\n",
      "epoch:6 step:5655 [D loss: 0.173888, acc.: 71.88%] [G loss: 0.570915]\n",
      "epoch:6 step:5656 [D loss: 0.224651, acc.: 63.28%] [G loss: 0.525247]\n",
      "epoch:6 step:5657 [D loss: 0.236327, acc.: 62.50%] [G loss: 0.453073]\n",
      "epoch:6 step:5658 [D loss: 0.202384, acc.: 71.09%] [G loss: 0.581929]\n",
      "epoch:6 step:5659 [D loss: 0.184416, acc.: 71.09%] [G loss: 0.561151]\n",
      "epoch:6 step:5660 [D loss: 0.258759, acc.: 53.12%] [G loss: 0.476911]\n",
      "epoch:6 step:5661 [D loss: 0.174935, acc.: 74.22%] [G loss: 0.545278]\n",
      "epoch:6 step:5662 [D loss: 0.148986, acc.: 81.25%] [G loss: 0.567507]\n",
      "epoch:6 step:5663 [D loss: 0.215805, acc.: 68.75%] [G loss: 0.557044]\n",
      "epoch:6 step:5664 [D loss: 0.203168, acc.: 64.84%] [G loss: 0.530060]\n",
      "epoch:6 step:5665 [D loss: 0.191305, acc.: 71.09%] [G loss: 0.519751]\n",
      "epoch:6 step:5666 [D loss: 0.229510, acc.: 64.06%] [G loss: 0.510527]\n",
      "epoch:6 step:5667 [D loss: 0.217814, acc.: 66.41%] [G loss: 0.485975]\n",
      "epoch:6 step:5668 [D loss: 0.224701, acc.: 60.16%] [G loss: 0.492118]\n",
      "epoch:6 step:5669 [D loss: 0.184366, acc.: 73.44%] [G loss: 0.557335]\n",
      "epoch:6 step:5670 [D loss: 0.200638, acc.: 67.97%] [G loss: 0.555365]\n",
      "epoch:6 step:5671 [D loss: 0.209942, acc.: 65.62%] [G loss: 0.541247]\n",
      "epoch:6 step:5672 [D loss: 0.210298, acc.: 69.53%] [G loss: 0.522073]\n",
      "epoch:6 step:5673 [D loss: 0.197055, acc.: 70.31%] [G loss: 0.537983]\n",
      "epoch:6 step:5674 [D loss: 0.216949, acc.: 63.28%] [G loss: 0.465825]\n",
      "epoch:6 step:5675 [D loss: 0.190515, acc.: 74.22%] [G loss: 0.580248]\n",
      "epoch:6 step:5676 [D loss: 0.212996, acc.: 68.75%] [G loss: 0.581356]\n",
      "epoch:6 step:5677 [D loss: 0.187227, acc.: 71.09%] [G loss: 0.565262]\n",
      "epoch:6 step:5678 [D loss: 0.205301, acc.: 70.31%] [G loss: 0.531744]\n",
      "epoch:6 step:5679 [D loss: 0.216403, acc.: 66.41%] [G loss: 0.494984]\n",
      "epoch:6 step:5680 [D loss: 0.202075, acc.: 69.53%] [G loss: 0.508394]\n",
      "epoch:6 step:5681 [D loss: 0.221333, acc.: 63.28%] [G loss: 0.517825]\n",
      "epoch:6 step:5682 [D loss: 0.219972, acc.: 62.50%] [G loss: 0.497676]\n",
      "epoch:6 step:5683 [D loss: 0.232601, acc.: 62.50%] [G loss: 0.501510]\n",
      "epoch:6 step:5684 [D loss: 0.212495, acc.: 68.75%] [G loss: 0.542959]\n",
      "epoch:6 step:5685 [D loss: 0.225047, acc.: 61.72%] [G loss: 0.518150]\n",
      "epoch:6 step:5686 [D loss: 0.178866, acc.: 76.56%] [G loss: 0.550852]\n",
      "epoch:6 step:5687 [D loss: 0.231881, acc.: 64.06%] [G loss: 0.481947]\n",
      "epoch:6 step:5688 [D loss: 0.213784, acc.: 61.72%] [G loss: 0.497867]\n",
      "epoch:6 step:5689 [D loss: 0.188313, acc.: 74.22%] [G loss: 0.512654]\n",
      "epoch:6 step:5690 [D loss: 0.198399, acc.: 67.97%] [G loss: 0.504810]\n",
      "epoch:6 step:5691 [D loss: 0.179994, acc.: 71.88%] [G loss: 0.496055]\n",
      "epoch:6 step:5692 [D loss: 0.182860, acc.: 70.31%] [G loss: 0.553486]\n",
      "epoch:6 step:5693 [D loss: 0.216928, acc.: 64.84%] [G loss: 0.537627]\n",
      "epoch:6 step:5694 [D loss: 0.217130, acc.: 64.84%] [G loss: 0.525603]\n",
      "epoch:6 step:5695 [D loss: 0.199975, acc.: 68.75%] [G loss: 0.503817]\n",
      "epoch:6 step:5696 [D loss: 0.215425, acc.: 69.53%] [G loss: 0.494766]\n",
      "epoch:6 step:5697 [D loss: 0.200717, acc.: 71.88%] [G loss: 0.537405]\n",
      "epoch:6 step:5698 [D loss: 0.164937, acc.: 76.56%] [G loss: 0.564668]\n",
      "epoch:6 step:5699 [D loss: 0.172925, acc.: 73.44%] [G loss: 0.549084]\n",
      "epoch:6 step:5700 [D loss: 0.278757, acc.: 58.59%] [G loss: 0.444922]\n",
      "epoch:6 step:5701 [D loss: 0.213525, acc.: 64.06%] [G loss: 0.482838]\n",
      "epoch:6 step:5702 [D loss: 0.194828, acc.: 71.88%] [G loss: 0.499928]\n",
      "epoch:6 step:5703 [D loss: 0.250996, acc.: 58.59%] [G loss: 0.513866]\n",
      "epoch:6 step:5704 [D loss: 0.201929, acc.: 71.09%] [G loss: 0.527810]\n",
      "epoch:6 step:5705 [D loss: 0.202488, acc.: 67.97%] [G loss: 0.515547]\n",
      "epoch:6 step:5706 [D loss: 0.220953, acc.: 63.28%] [G loss: 0.538066]\n",
      "epoch:6 step:5707 [D loss: 0.212164, acc.: 67.19%] [G loss: 0.504480]\n",
      "epoch:6 step:5708 [D loss: 0.188345, acc.: 74.22%] [G loss: 0.474060]\n",
      "epoch:6 step:5709 [D loss: 0.220650, acc.: 68.75%] [G loss: 0.485463]\n",
      "epoch:6 step:5710 [D loss: 0.188303, acc.: 75.00%] [G loss: 0.532403]\n",
      "epoch:6 step:5711 [D loss: 0.217520, acc.: 64.06%] [G loss: 0.521155]\n",
      "epoch:6 step:5712 [D loss: 0.206780, acc.: 68.75%] [G loss: 0.552529]\n",
      "epoch:6 step:5713 [D loss: 0.200312, acc.: 71.88%] [G loss: 0.497457]\n",
      "epoch:6 step:5714 [D loss: 0.189442, acc.: 69.53%] [G loss: 0.524710]\n",
      "epoch:6 step:5715 [D loss: 0.206512, acc.: 67.97%] [G loss: 0.509730]\n",
      "epoch:6 step:5716 [D loss: 0.194676, acc.: 68.75%] [G loss: 0.530189]\n",
      "epoch:6 step:5717 [D loss: 0.189861, acc.: 75.00%] [G loss: 0.525365]\n",
      "epoch:6 step:5718 [D loss: 0.192855, acc.: 71.09%] [G loss: 0.519780]\n",
      "epoch:6 step:5719 [D loss: 0.197925, acc.: 67.97%] [G loss: 0.562373]\n",
      "epoch:6 step:5720 [D loss: 0.250477, acc.: 56.25%] [G loss: 0.479640]\n",
      "epoch:6 step:5721 [D loss: 0.218700, acc.: 64.06%] [G loss: 0.483205]\n",
      "epoch:6 step:5722 [D loss: 0.184745, acc.: 72.66%] [G loss: 0.544661]\n",
      "epoch:6 step:5723 [D loss: 0.220445, acc.: 68.75%] [G loss: 0.543833]\n",
      "epoch:6 step:5724 [D loss: 0.215610, acc.: 70.31%] [G loss: 0.470880]\n",
      "epoch:6 step:5725 [D loss: 0.210992, acc.: 67.19%] [G loss: 0.450654]\n",
      "epoch:6 step:5726 [D loss: 0.168489, acc.: 74.22%] [G loss: 0.541480]\n",
      "epoch:6 step:5727 [D loss: 0.228446, acc.: 60.94%] [G loss: 0.447680]\n",
      "epoch:6 step:5728 [D loss: 0.164556, acc.: 78.12%] [G loss: 0.529221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5729 [D loss: 0.208900, acc.: 64.06%] [G loss: 0.575372]\n",
      "epoch:6 step:5730 [D loss: 0.254526, acc.: 61.72%] [G loss: 0.509222]\n",
      "epoch:6 step:5731 [D loss: 0.238336, acc.: 60.94%] [G loss: 0.486202]\n",
      "epoch:6 step:5732 [D loss: 0.224284, acc.: 64.06%] [G loss: 0.513825]\n",
      "epoch:6 step:5733 [D loss: 0.204338, acc.: 64.84%] [G loss: 0.532098]\n",
      "epoch:6 step:5734 [D loss: 0.208210, acc.: 68.75%] [G loss: 0.531557]\n",
      "epoch:6 step:5735 [D loss: 0.224336, acc.: 64.84%] [G loss: 0.500895]\n",
      "epoch:6 step:5736 [D loss: 0.211523, acc.: 64.06%] [G loss: 0.540575]\n",
      "epoch:6 step:5737 [D loss: 0.216117, acc.: 63.28%] [G loss: 0.565169]\n",
      "epoch:6 step:5738 [D loss: 0.198574, acc.: 68.75%] [G loss: 0.588530]\n",
      "epoch:6 step:5739 [D loss: 0.235683, acc.: 60.16%] [G loss: 0.566500]\n",
      "epoch:6 step:5740 [D loss: 0.204177, acc.: 70.31%] [G loss: 0.545441]\n",
      "epoch:6 step:5741 [D loss: 0.172800, acc.: 75.00%] [G loss: 0.539777]\n",
      "epoch:6 step:5742 [D loss: 0.287686, acc.: 54.69%] [G loss: 0.510900]\n",
      "epoch:6 step:5743 [D loss: 0.233093, acc.: 63.28%] [G loss: 0.581642]\n",
      "epoch:6 step:5744 [D loss: 0.174254, acc.: 74.22%] [G loss: 0.596504]\n",
      "epoch:6 step:5745 [D loss: 0.225706, acc.: 64.84%] [G loss: 0.538396]\n",
      "epoch:6 step:5746 [D loss: 0.213884, acc.: 67.97%] [G loss: 0.536532]\n",
      "epoch:6 step:5747 [D loss: 0.205303, acc.: 66.41%] [G loss: 0.509989]\n",
      "epoch:6 step:5748 [D loss: 0.174088, acc.: 75.78%] [G loss: 0.556641]\n",
      "epoch:6 step:5749 [D loss: 0.237446, acc.: 60.94%] [G loss: 0.477315]\n",
      "epoch:6 step:5750 [D loss: 0.222993, acc.: 63.28%] [G loss: 0.489886]\n",
      "epoch:6 step:5751 [D loss: 0.245030, acc.: 56.25%] [G loss: 0.466973]\n",
      "epoch:6 step:5752 [D loss: 0.178351, acc.: 73.44%] [G loss: 0.492297]\n",
      "epoch:6 step:5753 [D loss: 0.198904, acc.: 69.53%] [G loss: 0.516143]\n",
      "epoch:6 step:5754 [D loss: 0.198290, acc.: 71.09%] [G loss: 0.504984]\n",
      "epoch:6 step:5755 [D loss: 0.225435, acc.: 60.94%] [G loss: 0.501227]\n",
      "epoch:6 step:5756 [D loss: 0.207649, acc.: 68.75%] [G loss: 0.540807]\n",
      "epoch:6 step:5757 [D loss: 0.185005, acc.: 78.12%] [G loss: 0.491476]\n",
      "epoch:6 step:5758 [D loss: 0.208326, acc.: 67.97%] [G loss: 0.478225]\n",
      "epoch:6 step:5759 [D loss: 0.197219, acc.: 67.97%] [G loss: 0.472290]\n",
      "epoch:6 step:5760 [D loss: 0.265735, acc.: 58.59%] [G loss: 0.432949]\n",
      "epoch:6 step:5761 [D loss: 0.216491, acc.: 66.41%] [G loss: 0.492749]\n",
      "epoch:6 step:5762 [D loss: 0.233058, acc.: 62.50%] [G loss: 0.500962]\n",
      "epoch:6 step:5763 [D loss: 0.209042, acc.: 65.62%] [G loss: 0.519883]\n",
      "epoch:6 step:5764 [D loss: 0.213264, acc.: 71.88%] [G loss: 0.465038]\n",
      "epoch:6 step:5765 [D loss: 0.202949, acc.: 69.53%] [G loss: 0.499082]\n",
      "epoch:6 step:5766 [D loss: 0.179338, acc.: 75.78%] [G loss: 0.520690]\n",
      "epoch:6 step:5767 [D loss: 0.199428, acc.: 68.75%] [G loss: 0.558166]\n",
      "epoch:6 step:5768 [D loss: 0.235750, acc.: 60.16%] [G loss: 0.505618]\n",
      "epoch:6 step:5769 [D loss: 0.250762, acc.: 59.38%] [G loss: 0.506985]\n",
      "epoch:6 step:5770 [D loss: 0.204762, acc.: 68.75%] [G loss: 0.513179]\n",
      "epoch:6 step:5771 [D loss: 0.215590, acc.: 64.06%] [G loss: 0.507972]\n",
      "epoch:6 step:5772 [D loss: 0.215896, acc.: 67.19%] [G loss: 0.515592]\n",
      "epoch:6 step:5773 [D loss: 0.191955, acc.: 74.22%] [G loss: 0.504293]\n",
      "epoch:6 step:5774 [D loss: 0.234616, acc.: 64.84%] [G loss: 0.533903]\n",
      "epoch:6 step:5775 [D loss: 0.231768, acc.: 63.28%] [G loss: 0.546103]\n",
      "epoch:6 step:5776 [D loss: 0.198855, acc.: 69.53%] [G loss: 0.520855]\n",
      "epoch:6 step:5777 [D loss: 0.177663, acc.: 78.12%] [G loss: 0.507807]\n",
      "epoch:6 step:5778 [D loss: 0.186942, acc.: 72.66%] [G loss: 0.505019]\n",
      "epoch:6 step:5779 [D loss: 0.227593, acc.: 67.97%] [G loss: 0.483922]\n",
      "epoch:6 step:5780 [D loss: 0.208997, acc.: 65.62%] [G loss: 0.505746]\n",
      "epoch:6 step:5781 [D loss: 0.219511, acc.: 65.62%] [G loss: 0.506491]\n",
      "epoch:6 step:5782 [D loss: 0.240363, acc.: 62.50%] [G loss: 0.516110]\n",
      "epoch:6 step:5783 [D loss: 0.199329, acc.: 64.84%] [G loss: 0.556376]\n",
      "epoch:6 step:5784 [D loss: 0.198979, acc.: 69.53%] [G loss: 0.554911]\n",
      "epoch:6 step:5785 [D loss: 0.203463, acc.: 68.75%] [G loss: 0.528633]\n",
      "epoch:6 step:5786 [D loss: 0.183246, acc.: 72.66%] [G loss: 0.519187]\n",
      "epoch:6 step:5787 [D loss: 0.179291, acc.: 73.44%] [G loss: 0.522223]\n",
      "epoch:6 step:5788 [D loss: 0.193358, acc.: 67.19%] [G loss: 0.506027]\n",
      "epoch:6 step:5789 [D loss: 0.226881, acc.: 67.19%] [G loss: 0.474637]\n",
      "epoch:6 step:5790 [D loss: 0.184222, acc.: 75.00%] [G loss: 0.536523]\n",
      "epoch:6 step:5791 [D loss: 0.210177, acc.: 67.97%] [G loss: 0.481146]\n",
      "epoch:6 step:5792 [D loss: 0.207090, acc.: 67.97%] [G loss: 0.480198]\n",
      "epoch:6 step:5793 [D loss: 0.200586, acc.: 67.19%] [G loss: 0.495768]\n",
      "epoch:6 step:5794 [D loss: 0.202836, acc.: 68.75%] [G loss: 0.500707]\n",
      "epoch:6 step:5795 [D loss: 0.199202, acc.: 66.41%] [G loss: 0.525390]\n",
      "epoch:6 step:5796 [D loss: 0.263001, acc.: 55.47%] [G loss: 0.513389]\n",
      "epoch:6 step:5797 [D loss: 0.209710, acc.: 64.84%] [G loss: 0.503881]\n",
      "epoch:6 step:5798 [D loss: 0.214665, acc.: 62.50%] [G loss: 0.483215]\n",
      "epoch:6 step:5799 [D loss: 0.207170, acc.: 65.62%] [G loss: 0.546461]\n",
      "epoch:6 step:5800 [D loss: 0.222826, acc.: 65.62%] [G loss: 0.539558]\n",
      "##############\n",
      "[2.58249711 1.55191029 6.70548406 4.76548888 4.01077126 5.86772327\n",
      " 4.56567451 4.85095495 4.61765977 3.63147181]\n",
      "##########\n",
      "epoch:6 step:5801 [D loss: 0.220891, acc.: 64.84%] [G loss: 0.475542]\n",
      "epoch:6 step:5802 [D loss: 0.240795, acc.: 57.03%] [G loss: 0.462919]\n",
      "epoch:6 step:5803 [D loss: 0.220085, acc.: 67.97%] [G loss: 0.529833]\n",
      "epoch:6 step:5804 [D loss: 0.251262, acc.: 60.16%] [G loss: 0.470847]\n",
      "epoch:6 step:5805 [D loss: 0.227558, acc.: 64.06%] [G loss: 0.488382]\n",
      "epoch:6 step:5806 [D loss: 0.224694, acc.: 61.72%] [G loss: 0.462771]\n",
      "epoch:6 step:5807 [D loss: 0.206603, acc.: 69.53%] [G loss: 0.509799]\n",
      "epoch:6 step:5808 [D loss: 0.243683, acc.: 60.16%] [G loss: 0.496311]\n",
      "epoch:6 step:5809 [D loss: 0.241246, acc.: 57.81%] [G loss: 0.471980]\n",
      "epoch:6 step:5810 [D loss: 0.222943, acc.: 64.06%] [G loss: 0.520780]\n",
      "epoch:6 step:5811 [D loss: 0.227833, acc.: 55.47%] [G loss: 0.513947]\n",
      "epoch:6 step:5812 [D loss: 0.198140, acc.: 67.19%] [G loss: 0.491219]\n",
      "epoch:6 step:5813 [D loss: 0.209995, acc.: 64.06%] [G loss: 0.499520]\n",
      "epoch:6 step:5814 [D loss: 0.179387, acc.: 74.22%] [G loss: 0.522599]\n",
      "epoch:6 step:5815 [D loss: 0.187948, acc.: 73.44%] [G loss: 0.546043]\n",
      "epoch:6 step:5816 [D loss: 0.184734, acc.: 70.31%] [G loss: 0.573091]\n",
      "epoch:6 step:5817 [D loss: 0.209263, acc.: 67.97%] [G loss: 0.535805]\n",
      "epoch:6 step:5818 [D loss: 0.204647, acc.: 66.41%] [G loss: 0.490926]\n",
      "epoch:6 step:5819 [D loss: 0.219986, acc.: 67.97%] [G loss: 0.481242]\n",
      "epoch:6 step:5820 [D loss: 0.154202, acc.: 77.34%] [G loss: 0.584032]\n",
      "epoch:6 step:5821 [D loss: 0.208059, acc.: 70.31%] [G loss: 0.566300]\n",
      "epoch:6 step:5822 [D loss: 0.259292, acc.: 60.16%] [G loss: 0.500040]\n",
      "epoch:6 step:5823 [D loss: 0.205721, acc.: 69.53%] [G loss: 0.513393]\n",
      "epoch:6 step:5824 [D loss: 0.240195, acc.: 56.25%] [G loss: 0.480343]\n",
      "epoch:6 step:5825 [D loss: 0.257870, acc.: 53.12%] [G loss: 0.476077]\n",
      "epoch:6 step:5826 [D loss: 0.180744, acc.: 73.44%] [G loss: 0.506325]\n",
      "epoch:6 step:5827 [D loss: 0.198125, acc.: 70.31%] [G loss: 0.559269]\n",
      "epoch:6 step:5828 [D loss: 0.203308, acc.: 71.88%] [G loss: 0.550281]\n",
      "epoch:6 step:5829 [D loss: 0.151815, acc.: 75.00%] [G loss: 0.584421]\n",
      "epoch:6 step:5830 [D loss: 0.185969, acc.: 71.88%] [G loss: 0.546179]\n",
      "epoch:6 step:5831 [D loss: 0.177877, acc.: 71.88%] [G loss: 0.525474]\n",
      "epoch:6 step:5832 [D loss: 0.217600, acc.: 60.94%] [G loss: 0.464127]\n",
      "epoch:6 step:5833 [D loss: 0.229561, acc.: 61.72%] [G loss: 0.514736]\n",
      "epoch:6 step:5834 [D loss: 0.256543, acc.: 54.69%] [G loss: 0.458052]\n",
      "epoch:6 step:5835 [D loss: 0.182991, acc.: 75.78%] [G loss: 0.520728]\n",
      "epoch:6 step:5836 [D loss: 0.239155, acc.: 64.06%] [G loss: 0.456702]\n",
      "epoch:6 step:5837 [D loss: 0.230466, acc.: 62.50%] [G loss: 0.449447]\n",
      "epoch:6 step:5838 [D loss: 0.212186, acc.: 65.62%] [G loss: 0.524933]\n",
      "epoch:6 step:5839 [D loss: 0.201699, acc.: 69.53%] [G loss: 0.506862]\n",
      "epoch:6 step:5840 [D loss: 0.205065, acc.: 70.31%] [G loss: 0.528287]\n",
      "epoch:6 step:5841 [D loss: 0.194678, acc.: 68.75%] [G loss: 0.552398]\n",
      "epoch:6 step:5842 [D loss: 0.266684, acc.: 57.03%] [G loss: 0.495161]\n",
      "epoch:6 step:5843 [D loss: 0.193447, acc.: 64.84%] [G loss: 0.508145]\n",
      "epoch:6 step:5844 [D loss: 0.177274, acc.: 75.00%] [G loss: 0.566623]\n",
      "epoch:6 step:5845 [D loss: 0.187105, acc.: 75.00%] [G loss: 0.583134]\n",
      "epoch:6 step:5846 [D loss: 0.235354, acc.: 59.38%] [G loss: 0.508876]\n",
      "epoch:6 step:5847 [D loss: 0.221913, acc.: 67.97%] [G loss: 0.475995]\n",
      "epoch:6 step:5848 [D loss: 0.215681, acc.: 64.84%] [G loss: 0.483400]\n",
      "epoch:6 step:5849 [D loss: 0.205579, acc.: 67.19%] [G loss: 0.467526]\n",
      "epoch:6 step:5850 [D loss: 0.215793, acc.: 64.84%] [G loss: 0.523222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5851 [D loss: 0.206226, acc.: 67.19%] [G loss: 0.522860]\n",
      "epoch:6 step:5852 [D loss: 0.172231, acc.: 75.78%] [G loss: 0.522572]\n",
      "epoch:6 step:5853 [D loss: 0.195079, acc.: 66.41%] [G loss: 0.533760]\n",
      "epoch:6 step:5854 [D loss: 0.185774, acc.: 70.31%] [G loss: 0.635081]\n",
      "epoch:6 step:5855 [D loss: 0.245468, acc.: 63.28%] [G loss: 0.524549]\n",
      "epoch:6 step:5856 [D loss: 0.224077, acc.: 61.72%] [G loss: 0.450345]\n",
      "epoch:6 step:5857 [D loss: 0.202403, acc.: 68.75%] [G loss: 0.505106]\n",
      "epoch:6 step:5858 [D loss: 0.184045, acc.: 70.31%] [G loss: 0.501043]\n",
      "epoch:6 step:5859 [D loss: 0.236180, acc.: 62.50%] [G loss: 0.511530]\n",
      "epoch:6 step:5860 [D loss: 0.220234, acc.: 67.19%] [G loss: 0.506422]\n",
      "epoch:6 step:5861 [D loss: 0.213275, acc.: 64.84%] [G loss: 0.511983]\n",
      "epoch:6 step:5862 [D loss: 0.249385, acc.: 57.03%] [G loss: 0.511129]\n",
      "epoch:6 step:5863 [D loss: 0.212811, acc.: 62.50%] [G loss: 0.489113]\n",
      "epoch:6 step:5864 [D loss: 0.209751, acc.: 65.62%] [G loss: 0.486744]\n",
      "epoch:6 step:5865 [D loss: 0.173942, acc.: 76.56%] [G loss: 0.523123]\n",
      "epoch:6 step:5866 [D loss: 0.190156, acc.: 75.00%] [G loss: 0.522508]\n",
      "epoch:6 step:5867 [D loss: 0.194493, acc.: 69.53%] [G loss: 0.562281]\n",
      "epoch:6 step:5868 [D loss: 0.204615, acc.: 69.53%] [G loss: 0.495875]\n",
      "epoch:6 step:5869 [D loss: 0.219421, acc.: 65.62%] [G loss: 0.524426]\n",
      "epoch:6 step:5870 [D loss: 0.224507, acc.: 62.50%] [G loss: 0.538717]\n",
      "epoch:6 step:5871 [D loss: 0.212701, acc.: 63.28%] [G loss: 0.546821]\n",
      "epoch:6 step:5872 [D loss: 0.251838, acc.: 59.38%] [G loss: 0.482926]\n",
      "epoch:6 step:5873 [D loss: 0.222085, acc.: 67.97%] [G loss: 0.484742]\n",
      "epoch:6 step:5874 [D loss: 0.215468, acc.: 69.53%] [G loss: 0.500816]\n",
      "epoch:6 step:5875 [D loss: 0.221273, acc.: 63.28%] [G loss: 0.505088]\n",
      "epoch:6 step:5876 [D loss: 0.184049, acc.: 71.88%] [G loss: 0.560439]\n",
      "epoch:6 step:5877 [D loss: 0.212708, acc.: 67.97%] [G loss: 0.510388]\n",
      "epoch:6 step:5878 [D loss: 0.211775, acc.: 64.84%] [G loss: 0.500571]\n",
      "epoch:6 step:5879 [D loss: 0.232613, acc.: 60.94%] [G loss: 0.491279]\n",
      "epoch:6 step:5880 [D loss: 0.194396, acc.: 73.44%] [G loss: 0.498108]\n",
      "epoch:6 step:5881 [D loss: 0.183756, acc.: 73.44%] [G loss: 0.530795]\n",
      "epoch:6 step:5882 [D loss: 0.205809, acc.: 67.97%] [G loss: 0.560312]\n",
      "epoch:6 step:5883 [D loss: 0.203181, acc.: 68.75%] [G loss: 0.557116]\n",
      "epoch:6 step:5884 [D loss: 0.184418, acc.: 77.34%] [G loss: 0.548601]\n",
      "epoch:6 step:5885 [D loss: 0.252523, acc.: 60.16%] [G loss: 0.479630]\n",
      "epoch:6 step:5886 [D loss: 0.192639, acc.: 71.09%] [G loss: 0.519476]\n",
      "epoch:6 step:5887 [D loss: 0.268159, acc.: 53.91%] [G loss: 0.460395]\n",
      "epoch:6 step:5888 [D loss: 0.208235, acc.: 66.41%] [G loss: 0.504531]\n",
      "epoch:6 step:5889 [D loss: 0.203544, acc.: 68.75%] [G loss: 0.486469]\n",
      "epoch:6 step:5890 [D loss: 0.207455, acc.: 67.97%] [G loss: 0.534461]\n",
      "epoch:6 step:5891 [D loss: 0.221703, acc.: 65.62%] [G loss: 0.511230]\n",
      "epoch:6 step:5892 [D loss: 0.185073, acc.: 75.00%] [G loss: 0.527284]\n",
      "epoch:6 step:5893 [D loss: 0.195376, acc.: 67.19%] [G loss: 0.540611]\n",
      "epoch:6 step:5894 [D loss: 0.227114, acc.: 63.28%] [G loss: 0.518253]\n",
      "epoch:6 step:5895 [D loss: 0.220815, acc.: 66.41%] [G loss: 0.509844]\n",
      "epoch:6 step:5896 [D loss: 0.201333, acc.: 71.09%] [G loss: 0.515823]\n",
      "epoch:6 step:5897 [D loss: 0.237081, acc.: 60.94%] [G loss: 0.444573]\n",
      "epoch:6 step:5898 [D loss: 0.219482, acc.: 67.97%] [G loss: 0.518166]\n",
      "epoch:6 step:5899 [D loss: 0.236024, acc.: 60.94%] [G loss: 0.492426]\n",
      "epoch:6 step:5900 [D loss: 0.191358, acc.: 73.44%] [G loss: 0.523450]\n",
      "epoch:6 step:5901 [D loss: 0.186633, acc.: 75.78%] [G loss: 0.499690]\n",
      "epoch:6 step:5902 [D loss: 0.185363, acc.: 69.53%] [G loss: 0.523001]\n",
      "epoch:6 step:5903 [D loss: 0.268381, acc.: 50.78%] [G loss: 0.510003]\n",
      "epoch:6 step:5904 [D loss: 0.200334, acc.: 68.75%] [G loss: 0.511716]\n",
      "epoch:6 step:5905 [D loss: 0.178948, acc.: 75.00%] [G loss: 0.483727]\n",
      "epoch:6 step:5906 [D loss: 0.187477, acc.: 75.00%] [G loss: 0.535016]\n",
      "epoch:6 step:5907 [D loss: 0.205914, acc.: 71.09%] [G loss: 0.488887]\n",
      "epoch:6 step:5908 [D loss: 0.190219, acc.: 71.88%] [G loss: 0.564367]\n",
      "epoch:6 step:5909 [D loss: 0.220794, acc.: 62.50%] [G loss: 0.517178]\n",
      "epoch:6 step:5910 [D loss: 0.224187, acc.: 68.75%] [G loss: 0.529918]\n",
      "epoch:6 step:5911 [D loss: 0.203979, acc.: 71.09%] [G loss: 0.536702]\n",
      "epoch:6 step:5912 [D loss: 0.211116, acc.: 70.31%] [G loss: 0.484884]\n",
      "epoch:6 step:5913 [D loss: 0.213449, acc.: 66.41%] [G loss: 0.524581]\n",
      "epoch:6 step:5914 [D loss: 0.242854, acc.: 56.25%] [G loss: 0.510124]\n",
      "epoch:6 step:5915 [D loss: 0.227002, acc.: 64.84%] [G loss: 0.559482]\n",
      "epoch:6 step:5916 [D loss: 0.220741, acc.: 68.75%] [G loss: 0.512539]\n",
      "epoch:6 step:5917 [D loss: 0.219416, acc.: 62.50%] [G loss: 0.484252]\n",
      "epoch:6 step:5918 [D loss: 0.173343, acc.: 77.34%] [G loss: 0.525901]\n",
      "epoch:6 step:5919 [D loss: 0.186430, acc.: 68.75%] [G loss: 0.528725]\n",
      "epoch:6 step:5920 [D loss: 0.177256, acc.: 74.22%] [G loss: 0.509162]\n",
      "epoch:6 step:5921 [D loss: 0.179485, acc.: 75.00%] [G loss: 0.535786]\n",
      "epoch:6 step:5922 [D loss: 0.181174, acc.: 73.44%] [G loss: 0.545848]\n",
      "epoch:6 step:5923 [D loss: 0.226252, acc.: 65.62%] [G loss: 0.495030]\n",
      "epoch:6 step:5924 [D loss: 0.219573, acc.: 64.06%] [G loss: 0.519736]\n",
      "epoch:6 step:5925 [D loss: 0.195205, acc.: 71.88%] [G loss: 0.523643]\n",
      "epoch:6 step:5926 [D loss: 0.199070, acc.: 66.41%] [G loss: 0.541749]\n",
      "epoch:6 step:5927 [D loss: 0.198396, acc.: 69.53%] [G loss: 0.535824]\n",
      "epoch:6 step:5928 [D loss: 0.237023, acc.: 59.38%] [G loss: 0.521254]\n",
      "epoch:6 step:5929 [D loss: 0.196344, acc.: 68.75%] [G loss: 0.565694]\n",
      "epoch:6 step:5930 [D loss: 0.191708, acc.: 71.09%] [G loss: 0.514043]\n",
      "epoch:6 step:5931 [D loss: 0.163460, acc.: 75.00%] [G loss: 0.500138]\n",
      "epoch:6 step:5932 [D loss: 0.195318, acc.: 69.53%] [G loss: 0.534308]\n",
      "epoch:6 step:5933 [D loss: 0.157894, acc.: 78.12%] [G loss: 0.563751]\n",
      "epoch:6 step:5934 [D loss: 0.145013, acc.: 82.81%] [G loss: 0.572837]\n",
      "epoch:6 step:5935 [D loss: 0.194705, acc.: 66.41%] [G loss: 0.552179]\n",
      "epoch:6 step:5936 [D loss: 0.170066, acc.: 75.00%] [G loss: 0.667527]\n",
      "epoch:6 step:5937 [D loss: 0.174107, acc.: 74.22%] [G loss: 0.615339]\n",
      "epoch:6 step:5938 [D loss: 0.279247, acc.: 56.25%] [G loss: 0.457977]\n",
      "epoch:6 step:5939 [D loss: 0.213328, acc.: 67.97%] [G loss: 0.476067]\n",
      "epoch:6 step:5940 [D loss: 0.195667, acc.: 70.31%] [G loss: 0.489309]\n",
      "epoch:6 step:5941 [D loss: 0.220887, acc.: 65.62%] [G loss: 0.504926]\n",
      "epoch:6 step:5942 [D loss: 0.179113, acc.: 71.09%] [G loss: 0.513179]\n",
      "epoch:6 step:5943 [D loss: 0.174343, acc.: 71.88%] [G loss: 0.560921]\n",
      "epoch:6 step:5944 [D loss: 0.217577, acc.: 64.84%] [G loss: 0.516073]\n",
      "epoch:6 step:5945 [D loss: 0.239884, acc.: 56.25%] [G loss: 0.462040]\n",
      "epoch:6 step:5946 [D loss: 0.215150, acc.: 64.84%] [G loss: 0.504997]\n",
      "epoch:6 step:5947 [D loss: 0.222200, acc.: 67.19%] [G loss: 0.539041]\n",
      "epoch:6 step:5948 [D loss: 0.192247, acc.: 67.97%] [G loss: 0.533341]\n",
      "epoch:6 step:5949 [D loss: 0.228772, acc.: 62.50%] [G loss: 0.517346]\n",
      "epoch:6 step:5950 [D loss: 0.157358, acc.: 78.12%] [G loss: 0.540036]\n",
      "epoch:6 step:5951 [D loss: 0.209380, acc.: 69.53%] [G loss: 0.494299]\n",
      "epoch:6 step:5952 [D loss: 0.200280, acc.: 67.19%] [G loss: 0.540759]\n",
      "epoch:6 step:5953 [D loss: 0.215604, acc.: 64.06%] [G loss: 0.511276]\n",
      "epoch:6 step:5954 [D loss: 0.183837, acc.: 74.22%] [G loss: 0.533628]\n",
      "epoch:6 step:5955 [D loss: 0.189395, acc.: 70.31%] [G loss: 0.559590]\n",
      "epoch:6 step:5956 [D loss: 0.213556, acc.: 67.97%] [G loss: 0.511678]\n",
      "epoch:6 step:5957 [D loss: 0.219331, acc.: 64.06%] [G loss: 0.542121]\n",
      "epoch:6 step:5958 [D loss: 0.218831, acc.: 66.41%] [G loss: 0.534037]\n",
      "epoch:6 step:5959 [D loss: 0.185387, acc.: 75.78%] [G loss: 0.511015]\n",
      "epoch:6 step:5960 [D loss: 0.205182, acc.: 67.97%] [G loss: 0.518835]\n",
      "epoch:6 step:5961 [D loss: 0.181590, acc.: 73.44%] [G loss: 0.505341]\n",
      "epoch:6 step:5962 [D loss: 0.213612, acc.: 72.66%] [G loss: 0.481569]\n",
      "epoch:6 step:5963 [D loss: 0.218202, acc.: 66.41%] [G loss: 0.524753]\n",
      "epoch:6 step:5964 [D loss: 0.214340, acc.: 63.28%] [G loss: 0.551031]\n",
      "epoch:6 step:5965 [D loss: 0.171763, acc.: 77.34%] [G loss: 0.578434]\n",
      "epoch:6 step:5966 [D loss: 0.199498, acc.: 72.66%] [G loss: 0.539017]\n",
      "epoch:6 step:5967 [D loss: 0.187038, acc.: 73.44%] [G loss: 0.584165]\n",
      "epoch:6 step:5968 [D loss: 0.156602, acc.: 78.12%] [G loss: 0.591976]\n",
      "epoch:6 step:5969 [D loss: 0.196709, acc.: 70.31%] [G loss: 0.566107]\n",
      "epoch:6 step:5970 [D loss: 0.280347, acc.: 56.25%] [G loss: 0.494416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5971 [D loss: 0.259821, acc.: 50.78%] [G loss: 0.440203]\n",
      "epoch:6 step:5972 [D loss: 0.211584, acc.: 68.75%] [G loss: 0.469961]\n",
      "epoch:6 step:5973 [D loss: 0.203181, acc.: 69.53%] [G loss: 0.546757]\n",
      "epoch:6 step:5974 [D loss: 0.234599, acc.: 66.41%] [G loss: 0.484945]\n",
      "epoch:6 step:5975 [D loss: 0.186075, acc.: 71.09%] [G loss: 0.517513]\n",
      "epoch:6 step:5976 [D loss: 0.166663, acc.: 73.44%] [G loss: 0.562900]\n",
      "epoch:6 step:5977 [D loss: 0.240366, acc.: 60.94%] [G loss: 0.505369]\n",
      "epoch:6 step:5978 [D loss: 0.232244, acc.: 64.84%] [G loss: 0.504869]\n",
      "epoch:6 step:5979 [D loss: 0.170941, acc.: 74.22%] [G loss: 0.573528]\n",
      "epoch:6 step:5980 [D loss: 0.169045, acc.: 80.47%] [G loss: 0.581365]\n",
      "epoch:6 step:5981 [D loss: 0.185087, acc.: 76.56%] [G loss: 0.517952]\n",
      "epoch:6 step:5982 [D loss: 0.199616, acc.: 64.06%] [G loss: 0.504520]\n",
      "epoch:6 step:5983 [D loss: 0.216377, acc.: 69.53%] [G loss: 0.542588]\n",
      "epoch:6 step:5984 [D loss: 0.245019, acc.: 64.06%] [G loss: 0.468990]\n",
      "epoch:6 step:5985 [D loss: 0.197200, acc.: 72.66%] [G loss: 0.471585]\n",
      "epoch:6 step:5986 [D loss: 0.200136, acc.: 67.19%] [G loss: 0.495633]\n",
      "epoch:6 step:5987 [D loss: 0.233253, acc.: 63.28%] [G loss: 0.484438]\n",
      "epoch:6 step:5988 [D loss: 0.207394, acc.: 67.19%] [G loss: 0.494868]\n",
      "epoch:6 step:5989 [D loss: 0.223240, acc.: 69.53%] [G loss: 0.558249]\n",
      "epoch:6 step:5990 [D loss: 0.225593, acc.: 61.72%] [G loss: 0.489382]\n",
      "epoch:6 step:5991 [D loss: 0.202393, acc.: 70.31%] [G loss: 0.513438]\n",
      "epoch:6 step:5992 [D loss: 0.199440, acc.: 70.31%] [G loss: 0.475116]\n",
      "epoch:6 step:5993 [D loss: 0.190356, acc.: 71.09%] [G loss: 0.519378]\n",
      "epoch:6 step:5994 [D loss: 0.196046, acc.: 69.53%] [G loss: 0.581658]\n",
      "epoch:6 step:5995 [D loss: 0.242124, acc.: 59.38%] [G loss: 0.473279]\n",
      "epoch:6 step:5996 [D loss: 0.202925, acc.: 68.75%] [G loss: 0.509226]\n",
      "epoch:6 step:5997 [D loss: 0.239033, acc.: 56.25%] [G loss: 0.503557]\n",
      "epoch:6 step:5998 [D loss: 0.274580, acc.: 53.12%] [G loss: 0.467951]\n",
      "epoch:6 step:5999 [D loss: 0.229365, acc.: 61.72%] [G loss: 0.474044]\n",
      "epoch:6 step:6000 [D loss: 0.204003, acc.: 70.31%] [G loss: 0.505709]\n",
      "##############\n",
      "[2.79840514 1.72845562 6.40149937 5.19462005 3.94075553 5.80925242\n",
      " 4.48545175 4.71830991 4.82532779 3.80049342]\n",
      "##########\n",
      "epoch:6 step:6001 [D loss: 0.218452, acc.: 68.75%] [G loss: 0.498267]\n",
      "epoch:6 step:6002 [D loss: 0.210918, acc.: 64.06%] [G loss: 0.486394]\n",
      "epoch:6 step:6003 [D loss: 0.163134, acc.: 74.22%] [G loss: 0.529358]\n",
      "epoch:6 step:6004 [D loss: 0.210911, acc.: 66.41%] [G loss: 0.558639]\n",
      "epoch:6 step:6005 [D loss: 0.262006, acc.: 57.03%] [G loss: 0.527667]\n",
      "epoch:6 step:6006 [D loss: 0.227745, acc.: 61.72%] [G loss: 0.510409]\n",
      "epoch:6 step:6007 [D loss: 0.205095, acc.: 65.62%] [G loss: 0.512998]\n",
      "epoch:6 step:6008 [D loss: 0.231496, acc.: 57.81%] [G loss: 0.499687]\n",
      "epoch:6 step:6009 [D loss: 0.204659, acc.: 68.75%] [G loss: 0.502788]\n",
      "epoch:6 step:6010 [D loss: 0.201646, acc.: 69.53%] [G loss: 0.497294]\n",
      "epoch:6 step:6011 [D loss: 0.201404, acc.: 67.97%] [G loss: 0.505898]\n",
      "epoch:6 step:6012 [D loss: 0.242962, acc.: 58.59%] [G loss: 0.465405]\n",
      "epoch:6 step:6013 [D loss: 0.187289, acc.: 75.00%] [G loss: 0.519691]\n",
      "epoch:6 step:6014 [D loss: 0.187943, acc.: 75.00%] [G loss: 0.510329]\n",
      "epoch:6 step:6015 [D loss: 0.218536, acc.: 65.62%] [G loss: 0.509429]\n",
      "epoch:6 step:6016 [D loss: 0.225449, acc.: 67.97%] [G loss: 0.500287]\n",
      "epoch:6 step:6017 [D loss: 0.176680, acc.: 71.09%] [G loss: 0.533504]\n",
      "epoch:6 step:6018 [D loss: 0.266388, acc.: 53.12%] [G loss: 0.498504]\n",
      "epoch:6 step:6019 [D loss: 0.191811, acc.: 73.44%] [G loss: 0.571112]\n",
      "epoch:6 step:6020 [D loss: 0.190409, acc.: 73.44%] [G loss: 0.537679]\n",
      "epoch:6 step:6021 [D loss: 0.168496, acc.: 77.34%] [G loss: 0.555526]\n",
      "epoch:6 step:6022 [D loss: 0.258504, acc.: 52.34%] [G loss: 0.471163]\n",
      "epoch:6 step:6023 [D loss: 0.227328, acc.: 60.94%] [G loss: 0.461313]\n",
      "epoch:6 step:6024 [D loss: 0.182350, acc.: 73.44%] [G loss: 0.524845]\n",
      "epoch:6 step:6025 [D loss: 0.226078, acc.: 65.62%] [G loss: 0.477042]\n",
      "epoch:6 step:6026 [D loss: 0.238560, acc.: 58.59%] [G loss: 0.472509]\n",
      "epoch:6 step:6027 [D loss: 0.191951, acc.: 69.53%] [G loss: 0.487939]\n",
      "epoch:6 step:6028 [D loss: 0.188494, acc.: 72.66%] [G loss: 0.540129]\n",
      "epoch:6 step:6029 [D loss: 0.232477, acc.: 64.84%] [G loss: 0.535813]\n",
      "epoch:6 step:6030 [D loss: 0.242271, acc.: 64.84%] [G loss: 0.476447]\n",
      "epoch:6 step:6031 [D loss: 0.229906, acc.: 67.19%] [G loss: 0.508438]\n",
      "epoch:6 step:6032 [D loss: 0.203233, acc.: 71.09%] [G loss: 0.534177]\n",
      "epoch:6 step:6033 [D loss: 0.225463, acc.: 60.94%] [G loss: 0.534043]\n",
      "epoch:6 step:6034 [D loss: 0.224604, acc.: 64.84%] [G loss: 0.482283]\n",
      "epoch:6 step:6035 [D loss: 0.244545, acc.: 58.59%] [G loss: 0.499281]\n",
      "epoch:6 step:6036 [D loss: 0.207920, acc.: 69.53%] [G loss: 0.515690]\n",
      "epoch:6 step:6037 [D loss: 0.192330, acc.: 68.75%] [G loss: 0.514857]\n",
      "epoch:6 step:6038 [D loss: 0.201204, acc.: 71.88%] [G loss: 0.560095]\n",
      "epoch:6 step:6039 [D loss: 0.239758, acc.: 63.28%] [G loss: 0.515145]\n",
      "epoch:6 step:6040 [D loss: 0.271249, acc.: 50.00%] [G loss: 0.482633]\n",
      "epoch:6 step:6041 [D loss: 0.210152, acc.: 64.84%] [G loss: 0.500446]\n",
      "epoch:6 step:6042 [D loss: 0.202697, acc.: 70.31%] [G loss: 0.558913]\n",
      "epoch:6 step:6043 [D loss: 0.220632, acc.: 69.53%] [G loss: 0.535399]\n",
      "epoch:6 step:6044 [D loss: 0.216927, acc.: 63.28%] [G loss: 0.536332]\n",
      "epoch:6 step:6045 [D loss: 0.237992, acc.: 59.38%] [G loss: 0.466434]\n",
      "epoch:6 step:6046 [D loss: 0.222945, acc.: 66.41%] [G loss: 0.491395]\n",
      "epoch:6 step:6047 [D loss: 0.195084, acc.: 64.06%] [G loss: 0.468106]\n",
      "epoch:6 step:6048 [D loss: 0.202084, acc.: 71.09%] [G loss: 0.494965]\n",
      "epoch:6 step:6049 [D loss: 0.174135, acc.: 78.91%] [G loss: 0.517132]\n",
      "epoch:6 step:6050 [D loss: 0.158955, acc.: 80.47%] [G loss: 0.575300]\n",
      "epoch:6 step:6051 [D loss: 0.165183, acc.: 78.91%] [G loss: 0.578761]\n",
      "epoch:6 step:6052 [D loss: 0.208329, acc.: 71.09%] [G loss: 0.556597]\n",
      "epoch:6 step:6053 [D loss: 0.224609, acc.: 65.62%] [G loss: 0.566375]\n",
      "epoch:6 step:6054 [D loss: 0.255003, acc.: 55.47%] [G loss: 0.479640]\n",
      "epoch:6 step:6055 [D loss: 0.215586, acc.: 66.41%] [G loss: 0.530685]\n",
      "epoch:6 step:6056 [D loss: 0.193386, acc.: 71.88%] [G loss: 0.485846]\n",
      "epoch:6 step:6057 [D loss: 0.214916, acc.: 68.75%] [G loss: 0.535599]\n",
      "epoch:6 step:6058 [D loss: 0.206501, acc.: 65.62%] [G loss: 0.561550]\n",
      "epoch:6 step:6059 [D loss: 0.256414, acc.: 58.59%] [G loss: 0.471388]\n",
      "epoch:6 step:6060 [D loss: 0.237454, acc.: 60.94%] [G loss: 0.473428]\n",
      "epoch:6 step:6061 [D loss: 0.172394, acc.: 77.34%] [G loss: 0.498664]\n",
      "epoch:6 step:6062 [D loss: 0.217377, acc.: 64.06%] [G loss: 0.484185]\n",
      "epoch:6 step:6063 [D loss: 0.200860, acc.: 70.31%] [G loss: 0.523752]\n",
      "epoch:6 step:6064 [D loss: 0.225748, acc.: 64.06%] [G loss: 0.481529]\n",
      "epoch:6 step:6065 [D loss: 0.212670, acc.: 67.19%] [G loss: 0.494481]\n",
      "epoch:6 step:6066 [D loss: 0.217402, acc.: 62.50%] [G loss: 0.537431]\n",
      "epoch:6 step:6067 [D loss: 0.190139, acc.: 71.09%] [G loss: 0.606618]\n",
      "epoch:6 step:6068 [D loss: 0.206085, acc.: 65.62%] [G loss: 0.529498]\n",
      "epoch:6 step:6069 [D loss: 0.203359, acc.: 71.88%] [G loss: 0.504692]\n",
      "epoch:6 step:6070 [D loss: 0.221338, acc.: 64.84%] [G loss: 0.470787]\n",
      "epoch:6 step:6071 [D loss: 0.195340, acc.: 70.31%] [G loss: 0.486353]\n",
      "epoch:6 step:6072 [D loss: 0.173685, acc.: 67.97%] [G loss: 0.544110]\n",
      "epoch:6 step:6073 [D loss: 0.170593, acc.: 75.78%] [G loss: 0.533827]\n",
      "epoch:6 step:6074 [D loss: 0.214099, acc.: 67.97%] [G loss: 0.522336]\n",
      "epoch:6 step:6075 [D loss: 0.190001, acc.: 72.66%] [G loss: 0.535645]\n",
      "epoch:6 step:6076 [D loss: 0.232538, acc.: 64.84%] [G loss: 0.509971]\n",
      "epoch:6 step:6077 [D loss: 0.223849, acc.: 58.59%] [G loss: 0.504277]\n",
      "epoch:6 step:6078 [D loss: 0.234344, acc.: 60.94%] [G loss: 0.450492]\n",
      "epoch:6 step:6079 [D loss: 0.207753, acc.: 67.19%] [G loss: 0.521366]\n",
      "epoch:6 step:6080 [D loss: 0.250521, acc.: 55.47%] [G loss: 0.474819]\n",
      "epoch:6 step:6081 [D loss: 0.208300, acc.: 63.28%] [G loss: 0.488442]\n",
      "epoch:6 step:6082 [D loss: 0.219812, acc.: 66.41%] [G loss: 0.529038]\n",
      "epoch:6 step:6083 [D loss: 0.216989, acc.: 69.53%] [G loss: 0.493691]\n",
      "epoch:6 step:6084 [D loss: 0.216088, acc.: 60.94%] [G loss: 0.468680]\n",
      "epoch:6 step:6085 [D loss: 0.226010, acc.: 64.84%] [G loss: 0.463808]\n",
      "epoch:6 step:6086 [D loss: 0.212433, acc.: 66.41%] [G loss: 0.492159]\n",
      "epoch:6 step:6087 [D loss: 0.223959, acc.: 60.16%] [G loss: 0.468868]\n",
      "epoch:6 step:6088 [D loss: 0.216114, acc.: 66.41%] [G loss: 0.529376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6089 [D loss: 0.259288, acc.: 55.47%] [G loss: 0.534257]\n",
      "epoch:6 step:6090 [D loss: 0.215159, acc.: 64.84%] [G loss: 0.535744]\n",
      "epoch:6 step:6091 [D loss: 0.277651, acc.: 57.03%] [G loss: 0.462997]\n",
      "epoch:6 step:6092 [D loss: 0.197484, acc.: 69.53%] [G loss: 0.586081]\n",
      "epoch:6 step:6093 [D loss: 0.167107, acc.: 79.69%] [G loss: 0.610627]\n",
      "epoch:6 step:6094 [D loss: 0.167252, acc.: 77.34%] [G loss: 0.560652]\n",
      "epoch:6 step:6095 [D loss: 0.255512, acc.: 57.81%] [G loss: 0.482441]\n",
      "epoch:6 step:6096 [D loss: 0.199559, acc.: 64.84%] [G loss: 0.524679]\n",
      "epoch:6 step:6097 [D loss: 0.204404, acc.: 67.19%] [G loss: 0.521668]\n",
      "epoch:6 step:6098 [D loss: 0.234213, acc.: 64.84%] [G loss: 0.517682]\n",
      "epoch:6 step:6099 [D loss: 0.216726, acc.: 64.06%] [G loss: 0.498669]\n",
      "epoch:6 step:6100 [D loss: 0.227330, acc.: 61.72%] [G loss: 0.451733]\n",
      "epoch:6 step:6101 [D loss: 0.177406, acc.: 75.78%] [G loss: 0.560648]\n",
      "epoch:6 step:6102 [D loss: 0.229034, acc.: 64.84%] [G loss: 0.493873]\n",
      "epoch:6 step:6103 [D loss: 0.162135, acc.: 80.47%] [G loss: 0.530009]\n",
      "epoch:6 step:6104 [D loss: 0.241381, acc.: 57.81%] [G loss: 0.500959]\n",
      "epoch:6 step:6105 [D loss: 0.242332, acc.: 61.72%] [G loss: 0.472215]\n",
      "epoch:6 step:6106 [D loss: 0.163808, acc.: 75.78%] [G loss: 0.597067]\n",
      "epoch:6 step:6107 [D loss: 0.214653, acc.: 67.19%] [G loss: 0.568388]\n",
      "epoch:6 step:6108 [D loss: 0.241848, acc.: 59.38%] [G loss: 0.482672]\n",
      "epoch:6 step:6109 [D loss: 0.251885, acc.: 60.16%] [G loss: 0.494348]\n",
      "epoch:6 step:6110 [D loss: 0.197310, acc.: 66.41%] [G loss: 0.522220]\n",
      "epoch:6 step:6111 [D loss: 0.249288, acc.: 60.16%] [G loss: 0.505317]\n",
      "epoch:6 step:6112 [D loss: 0.211622, acc.: 66.41%] [G loss: 0.526820]\n",
      "epoch:6 step:6113 [D loss: 0.201577, acc.: 74.22%] [G loss: 0.512118]\n",
      "epoch:6 step:6114 [D loss: 0.257461, acc.: 60.16%] [G loss: 0.479961]\n",
      "epoch:6 step:6115 [D loss: 0.225098, acc.: 67.97%] [G loss: 0.475359]\n",
      "epoch:6 step:6116 [D loss: 0.230763, acc.: 63.28%] [G loss: 0.521641]\n",
      "epoch:6 step:6117 [D loss: 0.188138, acc.: 71.09%] [G loss: 0.529822]\n",
      "epoch:6 step:6118 [D loss: 0.197058, acc.: 69.53%] [G loss: 0.477865]\n",
      "epoch:6 step:6119 [D loss: 0.196756, acc.: 72.66%] [G loss: 0.514390]\n",
      "epoch:6 step:6120 [D loss: 0.203775, acc.: 67.19%] [G loss: 0.523703]\n",
      "epoch:6 step:6121 [D loss: 0.182682, acc.: 73.44%] [G loss: 0.561313]\n",
      "epoch:6 step:6122 [D loss: 0.283006, acc.: 52.34%] [G loss: 0.471570]\n",
      "epoch:6 step:6123 [D loss: 0.268547, acc.: 52.34%] [G loss: 0.476089]\n",
      "epoch:6 step:6124 [D loss: 0.240398, acc.: 64.84%] [G loss: 0.467474]\n",
      "epoch:6 step:6125 [D loss: 0.191659, acc.: 72.66%] [G loss: 0.521062]\n",
      "epoch:6 step:6126 [D loss: 0.175484, acc.: 76.56%] [G loss: 0.576136]\n",
      "epoch:6 step:6127 [D loss: 0.199126, acc.: 71.09%] [G loss: 0.534220]\n",
      "epoch:6 step:6128 [D loss: 0.236877, acc.: 64.06%] [G loss: 0.480410]\n",
      "epoch:6 step:6129 [D loss: 0.227193, acc.: 66.41%] [G loss: 0.489814]\n",
      "epoch:6 step:6130 [D loss: 0.184018, acc.: 71.88%] [G loss: 0.582995]\n",
      "epoch:6 step:6131 [D loss: 0.243895, acc.: 60.94%] [G loss: 0.491070]\n",
      "epoch:6 step:6132 [D loss: 0.198090, acc.: 69.53%] [G loss: 0.524035]\n",
      "epoch:6 step:6133 [D loss: 0.234702, acc.: 60.16%] [G loss: 0.473459]\n",
      "epoch:6 step:6134 [D loss: 0.191773, acc.: 71.09%] [G loss: 0.514403]\n",
      "epoch:6 step:6135 [D loss: 0.194985, acc.: 68.75%] [G loss: 0.522984]\n",
      "epoch:6 step:6136 [D loss: 0.227519, acc.: 63.28%] [G loss: 0.558987]\n",
      "epoch:6 step:6137 [D loss: 0.171930, acc.: 80.47%] [G loss: 0.514381]\n",
      "epoch:6 step:6138 [D loss: 0.195360, acc.: 67.97%] [G loss: 0.565799]\n",
      "epoch:6 step:6139 [D loss: 0.220872, acc.: 57.81%] [G loss: 0.494825]\n",
      "epoch:6 step:6140 [D loss: 0.187128, acc.: 71.09%] [G loss: 0.544613]\n",
      "epoch:6 step:6141 [D loss: 0.180086, acc.: 72.66%] [G loss: 0.532504]\n",
      "epoch:6 step:6142 [D loss: 0.185535, acc.: 73.44%] [G loss: 0.526859]\n",
      "epoch:6 step:6143 [D loss: 0.201556, acc.: 70.31%] [G loss: 0.508840]\n",
      "epoch:6 step:6144 [D loss: 0.207057, acc.: 71.88%] [G loss: 0.507014]\n",
      "epoch:6 step:6145 [D loss: 0.187808, acc.: 71.09%] [G loss: 0.569403]\n",
      "epoch:6 step:6146 [D loss: 0.236400, acc.: 61.72%] [G loss: 0.500327]\n",
      "epoch:6 step:6147 [D loss: 0.224501, acc.: 64.06%] [G loss: 0.519823]\n",
      "epoch:6 step:6148 [D loss: 0.224477, acc.: 60.16%] [G loss: 0.460527]\n",
      "epoch:6 step:6149 [D loss: 0.192391, acc.: 70.31%] [G loss: 0.547021]\n",
      "epoch:6 step:6150 [D loss: 0.276934, acc.: 50.78%] [G loss: 0.447136]\n",
      "epoch:6 step:6151 [D loss: 0.241463, acc.: 58.59%] [G loss: 0.499304]\n",
      "epoch:6 step:6152 [D loss: 0.180009, acc.: 75.78%] [G loss: 0.560981]\n",
      "epoch:6 step:6153 [D loss: 0.278347, acc.: 57.03%] [G loss: 0.460706]\n",
      "epoch:6 step:6154 [D loss: 0.199620, acc.: 66.41%] [G loss: 0.504349]\n",
      "epoch:6 step:6155 [D loss: 0.196715, acc.: 71.88%] [G loss: 0.502975]\n",
      "epoch:6 step:6156 [D loss: 0.169709, acc.: 75.00%] [G loss: 0.569703]\n",
      "epoch:6 step:6157 [D loss: 0.269722, acc.: 53.12%] [G loss: 0.460193]\n",
      "epoch:6 step:6158 [D loss: 0.192407, acc.: 67.19%] [G loss: 0.532175]\n",
      "epoch:6 step:6159 [D loss: 0.197652, acc.: 72.66%] [G loss: 0.485341]\n",
      "epoch:6 step:6160 [D loss: 0.241307, acc.: 57.81%] [G loss: 0.505071]\n",
      "epoch:6 step:6161 [D loss: 0.223177, acc.: 67.19%] [G loss: 0.485007]\n",
      "epoch:6 step:6162 [D loss: 0.245197, acc.: 62.50%] [G loss: 0.522152]\n",
      "epoch:6 step:6163 [D loss: 0.208959, acc.: 69.53%] [G loss: 0.498002]\n",
      "epoch:6 step:6164 [D loss: 0.262497, acc.: 57.81%] [G loss: 0.482899]\n",
      "epoch:6 step:6165 [D loss: 0.227166, acc.: 61.72%] [G loss: 0.522000]\n",
      "epoch:6 step:6166 [D loss: 0.235368, acc.: 60.94%] [G loss: 0.444890]\n",
      "epoch:6 step:6167 [D loss: 0.205184, acc.: 69.53%] [G loss: 0.514243]\n",
      "epoch:6 step:6168 [D loss: 0.217517, acc.: 65.62%] [G loss: 0.523781]\n",
      "epoch:6 step:6169 [D loss: 0.176049, acc.: 72.66%] [G loss: 0.541045]\n",
      "epoch:6 step:6170 [D loss: 0.206557, acc.: 69.53%] [G loss: 0.508621]\n",
      "epoch:6 step:6171 [D loss: 0.199009, acc.: 65.62%] [G loss: 0.484761]\n",
      "epoch:6 step:6172 [D loss: 0.210068, acc.: 70.31%] [G loss: 0.498507]\n",
      "epoch:6 step:6173 [D loss: 0.222341, acc.: 63.28%] [G loss: 0.528807]\n",
      "epoch:6 step:6174 [D loss: 0.191994, acc.: 69.53%] [G loss: 0.569346]\n",
      "epoch:6 step:6175 [D loss: 0.232840, acc.: 61.72%] [G loss: 0.487301]\n",
      "epoch:6 step:6176 [D loss: 0.169404, acc.: 75.78%] [G loss: 0.528802]\n",
      "epoch:6 step:6177 [D loss: 0.174528, acc.: 75.78%] [G loss: 0.553438]\n",
      "epoch:6 step:6178 [D loss: 0.199149, acc.: 67.97%] [G loss: 0.523639]\n",
      "epoch:6 step:6179 [D loss: 0.188603, acc.: 68.75%] [G loss: 0.508284]\n",
      "epoch:6 step:6180 [D loss: 0.217228, acc.: 64.06%] [G loss: 0.516527]\n",
      "epoch:6 step:6181 [D loss: 0.228293, acc.: 61.72%] [G loss: 0.485172]\n",
      "epoch:6 step:6182 [D loss: 0.226726, acc.: 62.50%] [G loss: 0.485581]\n",
      "epoch:6 step:6183 [D loss: 0.180935, acc.: 67.97%] [G loss: 0.519887]\n",
      "epoch:6 step:6184 [D loss: 0.234371, acc.: 62.50%] [G loss: 0.475517]\n",
      "epoch:6 step:6185 [D loss: 0.177445, acc.: 71.88%] [G loss: 0.498326]\n",
      "epoch:6 step:6186 [D loss: 0.179812, acc.: 72.66%] [G loss: 0.601213]\n",
      "epoch:6 step:6187 [D loss: 0.251480, acc.: 61.72%] [G loss: 0.485987]\n",
      "epoch:6 step:6188 [D loss: 0.244489, acc.: 59.38%] [G loss: 0.468682]\n",
      "epoch:6 step:6189 [D loss: 0.203069, acc.: 67.97%] [G loss: 0.522137]\n",
      "epoch:6 step:6190 [D loss: 0.200171, acc.: 68.75%] [G loss: 0.601335]\n",
      "epoch:6 step:6191 [D loss: 0.267182, acc.: 51.56%] [G loss: 0.472830]\n",
      "epoch:6 step:6192 [D loss: 0.222788, acc.: 63.28%] [G loss: 0.482812]\n",
      "epoch:6 step:6193 [D loss: 0.199700, acc.: 67.97%] [G loss: 0.520782]\n",
      "epoch:6 step:6194 [D loss: 0.194341, acc.: 75.78%] [G loss: 0.488441]\n",
      "epoch:6 step:6195 [D loss: 0.215445, acc.: 73.44%] [G loss: 0.476095]\n",
      "epoch:6 step:6196 [D loss: 0.156726, acc.: 79.69%] [G loss: 0.566957]\n",
      "epoch:6 step:6197 [D loss: 0.196276, acc.: 73.44%] [G loss: 0.542513]\n",
      "epoch:6 step:6198 [D loss: 0.228290, acc.: 57.03%] [G loss: 0.520078]\n",
      "epoch:6 step:6199 [D loss: 0.230915, acc.: 59.38%] [G loss: 0.484309]\n",
      "epoch:6 step:6200 [D loss: 0.192984, acc.: 70.31%] [G loss: 0.508406]\n",
      "##############\n",
      "[2.70714209 1.38120676 6.17895428 4.74944695 3.83467551 5.75083216\n",
      " 4.59268125 4.77411867 4.52665636 4.02496898]\n",
      "##########\n",
      "epoch:6 step:6201 [D loss: 0.240094, acc.: 56.25%] [G loss: 0.469831]\n",
      "epoch:6 step:6202 [D loss: 0.230102, acc.: 60.16%] [G loss: 0.467346]\n",
      "epoch:6 step:6203 [D loss: 0.214646, acc.: 68.75%] [G loss: 0.469558]\n",
      "epoch:6 step:6204 [D loss: 0.215476, acc.: 67.97%] [G loss: 0.474668]\n",
      "epoch:6 step:6205 [D loss: 0.228172, acc.: 60.94%] [G loss: 0.499648]\n",
      "epoch:6 step:6206 [D loss: 0.239641, acc.: 61.72%] [G loss: 0.480355]\n",
      "epoch:6 step:6207 [D loss: 0.184191, acc.: 70.31%] [G loss: 0.489047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6208 [D loss: 0.215148, acc.: 69.53%] [G loss: 0.498803]\n",
      "epoch:6 step:6209 [D loss: 0.220170, acc.: 64.84%] [G loss: 0.464257]\n",
      "epoch:6 step:6210 [D loss: 0.205486, acc.: 70.31%] [G loss: 0.523894]\n",
      "epoch:6 step:6211 [D loss: 0.187408, acc.: 71.88%] [G loss: 0.619224]\n",
      "epoch:6 step:6212 [D loss: 0.239704, acc.: 65.62%] [G loss: 0.561495]\n",
      "epoch:6 step:6213 [D loss: 0.226823, acc.: 64.06%] [G loss: 0.480218]\n",
      "epoch:6 step:6214 [D loss: 0.186610, acc.: 74.22%] [G loss: 0.522345]\n",
      "epoch:6 step:6215 [D loss: 0.225057, acc.: 64.06%] [G loss: 0.533200]\n",
      "epoch:6 step:6216 [D loss: 0.219662, acc.: 64.06%] [G loss: 0.481223]\n",
      "epoch:6 step:6217 [D loss: 0.201073, acc.: 71.09%] [G loss: 0.472705]\n",
      "epoch:6 step:6218 [D loss: 0.241403, acc.: 57.81%] [G loss: 0.472495]\n",
      "epoch:6 step:6219 [D loss: 0.217362, acc.: 62.50%] [G loss: 0.483843]\n",
      "epoch:6 step:6220 [D loss: 0.215681, acc.: 70.31%] [G loss: 0.507025]\n",
      "epoch:6 step:6221 [D loss: 0.228426, acc.: 61.72%] [G loss: 0.488109]\n",
      "epoch:6 step:6222 [D loss: 0.239078, acc.: 56.25%] [G loss: 0.479887]\n",
      "epoch:6 step:6223 [D loss: 0.217835, acc.: 67.97%] [G loss: 0.433600]\n",
      "epoch:6 step:6224 [D loss: 0.212820, acc.: 64.84%] [G loss: 0.511020]\n",
      "epoch:6 step:6225 [D loss: 0.202311, acc.: 73.44%] [G loss: 0.468222]\n",
      "epoch:6 step:6226 [D loss: 0.235983, acc.: 61.72%] [G loss: 0.518496]\n",
      "epoch:6 step:6227 [D loss: 0.223960, acc.: 64.06%] [G loss: 0.476566]\n",
      "epoch:6 step:6228 [D loss: 0.218627, acc.: 65.62%] [G loss: 0.524022]\n",
      "epoch:6 step:6229 [D loss: 0.200861, acc.: 69.53%] [G loss: 0.540009]\n",
      "epoch:6 step:6230 [D loss: 0.235198, acc.: 62.50%] [G loss: 0.485920]\n",
      "epoch:6 step:6231 [D loss: 0.211719, acc.: 67.19%] [G loss: 0.497936]\n",
      "epoch:6 step:6232 [D loss: 0.219040, acc.: 65.62%] [G loss: 0.460506]\n",
      "epoch:6 step:6233 [D loss: 0.223362, acc.: 66.41%] [G loss: 0.478155]\n",
      "epoch:6 step:6234 [D loss: 0.205270, acc.: 67.97%] [G loss: 0.510430]\n",
      "epoch:6 step:6235 [D loss: 0.158757, acc.: 76.56%] [G loss: 0.583840]\n",
      "epoch:6 step:6236 [D loss: 0.226310, acc.: 60.94%] [G loss: 0.473377]\n",
      "epoch:6 step:6237 [D loss: 0.240909, acc.: 53.91%] [G loss: 0.432537]\n",
      "epoch:6 step:6238 [D loss: 0.205358, acc.: 68.75%] [G loss: 0.495361]\n",
      "epoch:6 step:6239 [D loss: 0.226107, acc.: 64.84%] [G loss: 0.494670]\n",
      "epoch:6 step:6240 [D loss: 0.213514, acc.: 66.41%] [G loss: 0.502230]\n",
      "epoch:6 step:6241 [D loss: 0.182400, acc.: 69.53%] [G loss: 0.545077]\n",
      "epoch:6 step:6242 [D loss: 0.197717, acc.: 74.22%] [G loss: 0.477889]\n",
      "epoch:6 step:6243 [D loss: 0.221920, acc.: 64.06%] [G loss: 0.461430]\n",
      "epoch:6 step:6244 [D loss: 0.253974, acc.: 57.81%] [G loss: 0.445636]\n",
      "epoch:6 step:6245 [D loss: 0.215986, acc.: 67.19%] [G loss: 0.513761]\n",
      "epoch:6 step:6246 [D loss: 0.180754, acc.: 74.22%] [G loss: 0.546411]\n",
      "epoch:6 step:6247 [D loss: 0.278196, acc.: 50.78%] [G loss: 0.492431]\n",
      "epoch:6 step:6248 [D loss: 0.221977, acc.: 65.62%] [G loss: 0.451244]\n",
      "epoch:6 step:6249 [D loss: 0.205305, acc.: 68.75%] [G loss: 0.547299]\n",
      "epoch:6 step:6250 [D loss: 0.255607, acc.: 57.81%] [G loss: 0.491293]\n",
      "epoch:6 step:6251 [D loss: 0.203254, acc.: 67.97%] [G loss: 0.492579]\n",
      "epoch:6 step:6252 [D loss: 0.212055, acc.: 60.94%] [G loss: 0.500744]\n",
      "epoch:6 step:6253 [D loss: 0.188350, acc.: 72.66%] [G loss: 0.533821]\n",
      "epoch:6 step:6254 [D loss: 0.184040, acc.: 77.34%] [G loss: 0.523808]\n",
      "epoch:6 step:6255 [D loss: 0.218728, acc.: 60.16%] [G loss: 0.517518]\n",
      "epoch:6 step:6256 [D loss: 0.200055, acc.: 75.00%] [G loss: 0.476877]\n",
      "epoch:6 step:6257 [D loss: 0.193625, acc.: 71.88%] [G loss: 0.548999]\n",
      "epoch:6 step:6258 [D loss: 0.220553, acc.: 62.50%] [G loss: 0.489482]\n",
      "epoch:6 step:6259 [D loss: 0.213911, acc.: 67.19%] [G loss: 0.475381]\n",
      "epoch:6 step:6260 [D loss: 0.222676, acc.: 60.16%] [G loss: 0.459255]\n",
      "epoch:6 step:6261 [D loss: 0.224062, acc.: 64.84%] [G loss: 0.485260]\n",
      "epoch:6 step:6262 [D loss: 0.204045, acc.: 65.62%] [G loss: 0.556871]\n",
      "epoch:6 step:6263 [D loss: 0.181748, acc.: 75.78%] [G loss: 0.550000]\n",
      "epoch:6 step:6264 [D loss: 0.179552, acc.: 72.66%] [G loss: 0.594097]\n",
      "epoch:6 step:6265 [D loss: 0.224593, acc.: 66.41%] [G loss: 0.518991]\n",
      "epoch:6 step:6266 [D loss: 0.233788, acc.: 63.28%] [G loss: 0.481439]\n",
      "epoch:6 step:6267 [D loss: 0.206611, acc.: 65.62%] [G loss: 0.478381]\n",
      "epoch:6 step:6268 [D loss: 0.198615, acc.: 71.09%] [G loss: 0.528167]\n",
      "epoch:6 step:6269 [D loss: 0.154310, acc.: 80.47%] [G loss: 0.575761]\n",
      "epoch:6 step:6270 [D loss: 0.184273, acc.: 73.44%] [G loss: 0.578931]\n",
      "epoch:6 step:6271 [D loss: 0.193147, acc.: 70.31%] [G loss: 0.542203]\n",
      "epoch:6 step:6272 [D loss: 0.198280, acc.: 69.53%] [G loss: 0.500466]\n",
      "epoch:6 step:6273 [D loss: 0.211473, acc.: 67.97%] [G loss: 0.527409]\n",
      "epoch:6 step:6274 [D loss: 0.232928, acc.: 62.50%] [G loss: 0.482191]\n",
      "epoch:6 step:6275 [D loss: 0.222372, acc.: 67.19%] [G loss: 0.523772]\n",
      "epoch:6 step:6276 [D loss: 0.217539, acc.: 65.62%] [G loss: 0.508473]\n",
      "epoch:6 step:6277 [D loss: 0.221029, acc.: 64.06%] [G loss: 0.525165]\n",
      "epoch:6 step:6278 [D loss: 0.225852, acc.: 66.41%] [G loss: 0.467468]\n",
      "epoch:6 step:6279 [D loss: 0.194773, acc.: 69.53%] [G loss: 0.557399]\n",
      "epoch:6 step:6280 [D loss: 0.240693, acc.: 60.94%] [G loss: 0.468539]\n",
      "epoch:6 step:6281 [D loss: 0.233823, acc.: 62.50%] [G loss: 0.487872]\n",
      "epoch:6 step:6282 [D loss: 0.183792, acc.: 71.88%] [G loss: 0.551275]\n",
      "epoch:6 step:6283 [D loss: 0.201459, acc.: 67.19%] [G loss: 0.538775]\n",
      "epoch:6 step:6284 [D loss: 0.204136, acc.: 66.41%] [G loss: 0.511775]\n",
      "epoch:6 step:6285 [D loss: 0.214287, acc.: 63.28%] [G loss: 0.498033]\n",
      "epoch:6 step:6286 [D loss: 0.217209, acc.: 64.84%] [G loss: 0.494251]\n",
      "epoch:6 step:6287 [D loss: 0.223449, acc.: 65.62%] [G loss: 0.506539]\n",
      "epoch:6 step:6288 [D loss: 0.222330, acc.: 67.19%] [G loss: 0.518804]\n",
      "epoch:6 step:6289 [D loss: 0.222171, acc.: 64.06%] [G loss: 0.488258]\n",
      "epoch:6 step:6290 [D loss: 0.232509, acc.: 61.72%] [G loss: 0.479463]\n",
      "epoch:6 step:6291 [D loss: 0.195440, acc.: 71.88%] [G loss: 0.493736]\n",
      "epoch:6 step:6292 [D loss: 0.206498, acc.: 69.53%] [G loss: 0.527308]\n",
      "epoch:6 step:6293 [D loss: 0.216637, acc.: 62.50%] [G loss: 0.475095]\n",
      "epoch:6 step:6294 [D loss: 0.257736, acc.: 57.03%] [G loss: 0.475535]\n",
      "epoch:6 step:6295 [D loss: 0.214997, acc.: 64.06%] [G loss: 0.471039]\n",
      "epoch:6 step:6296 [D loss: 0.208009, acc.: 67.19%] [G loss: 0.525499]\n",
      "epoch:6 step:6297 [D loss: 0.223942, acc.: 66.41%] [G loss: 0.517944]\n",
      "epoch:6 step:6298 [D loss: 0.246343, acc.: 61.72%] [G loss: 0.502828]\n",
      "epoch:6 step:6299 [D loss: 0.178899, acc.: 78.91%] [G loss: 0.501276]\n",
      "epoch:6 step:6300 [D loss: 0.206470, acc.: 69.53%] [G loss: 0.517267]\n",
      "epoch:6 step:6301 [D loss: 0.203583, acc.: 67.19%] [G loss: 0.470976]\n",
      "epoch:6 step:6302 [D loss: 0.202786, acc.: 67.97%] [G loss: 0.497371]\n",
      "epoch:6 step:6303 [D loss: 0.157025, acc.: 81.25%] [G loss: 0.543063]\n",
      "epoch:6 step:6304 [D loss: 0.217127, acc.: 60.94%] [G loss: 0.499935]\n",
      "epoch:6 step:6305 [D loss: 0.217240, acc.: 64.84%] [G loss: 0.481354]\n",
      "epoch:6 step:6306 [D loss: 0.205929, acc.: 69.53%] [G loss: 0.502008]\n",
      "epoch:6 step:6307 [D loss: 0.188108, acc.: 75.00%] [G loss: 0.490074]\n",
      "epoch:6 step:6308 [D loss: 0.215399, acc.: 61.72%] [G loss: 0.491554]\n",
      "epoch:6 step:6309 [D loss: 0.207977, acc.: 64.06%] [G loss: 0.498023]\n",
      "epoch:6 step:6310 [D loss: 0.193145, acc.: 71.09%] [G loss: 0.510646]\n",
      "epoch:6 step:6311 [D loss: 0.198425, acc.: 74.22%] [G loss: 0.479380]\n",
      "epoch:6 step:6312 [D loss: 0.170279, acc.: 75.78%] [G loss: 0.607524]\n",
      "epoch:6 step:6313 [D loss: 0.182075, acc.: 73.44%] [G loss: 0.562378]\n",
      "epoch:6 step:6314 [D loss: 0.181751, acc.: 73.44%] [G loss: 0.573390]\n",
      "epoch:6 step:6315 [D loss: 0.195053, acc.: 66.41%] [G loss: 0.528339]\n",
      "epoch:6 step:6316 [D loss: 0.206932, acc.: 66.41%] [G loss: 0.573368]\n",
      "epoch:6 step:6317 [D loss: 0.194671, acc.: 70.31%] [G loss: 0.592098]\n",
      "epoch:6 step:6318 [D loss: 0.225221, acc.: 63.28%] [G loss: 0.488316]\n",
      "epoch:6 step:6319 [D loss: 0.193870, acc.: 72.66%] [G loss: 0.505530]\n",
      "epoch:6 step:6320 [D loss: 0.221524, acc.: 60.94%] [G loss: 0.485202]\n",
      "epoch:6 step:6321 [D loss: 0.205240, acc.: 71.88%] [G loss: 0.525181]\n",
      "epoch:6 step:6322 [D loss: 0.229351, acc.: 64.84%] [G loss: 0.503878]\n",
      "epoch:6 step:6323 [D loss: 0.219983, acc.: 63.28%] [G loss: 0.534333]\n",
      "epoch:6 step:6324 [D loss: 0.250199, acc.: 56.25%] [G loss: 0.515210]\n",
      "epoch:6 step:6325 [D loss: 0.241418, acc.: 60.16%] [G loss: 0.491888]\n",
      "epoch:6 step:6326 [D loss: 0.245775, acc.: 58.59%] [G loss: 0.471005]\n",
      "epoch:6 step:6327 [D loss: 0.223145, acc.: 63.28%] [G loss: 0.458841]\n",
      "epoch:6 step:6328 [D loss: 0.218768, acc.: 57.81%] [G loss: 0.466077]\n",
      "epoch:6 step:6329 [D loss: 0.205888, acc.: 67.19%] [G loss: 0.484873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6330 [D loss: 0.191991, acc.: 72.66%] [G loss: 0.523397]\n",
      "epoch:6 step:6331 [D loss: 0.208446, acc.: 69.53%] [G loss: 0.512395]\n",
      "epoch:6 step:6332 [D loss: 0.235952, acc.: 61.72%] [G loss: 0.472352]\n",
      "epoch:6 step:6333 [D loss: 0.209928, acc.: 66.41%] [G loss: 0.473115]\n",
      "epoch:6 step:6334 [D loss: 0.176263, acc.: 73.44%] [G loss: 0.546032]\n",
      "epoch:6 step:6335 [D loss: 0.196078, acc.: 70.31%] [G loss: 0.509845]\n",
      "epoch:6 step:6336 [D loss: 0.194608, acc.: 71.88%] [G loss: 0.502351]\n",
      "epoch:6 step:6337 [D loss: 0.241993, acc.: 59.38%] [G loss: 0.499760]\n",
      "epoch:6 step:6338 [D loss: 0.269067, acc.: 51.56%] [G loss: 0.480765]\n",
      "epoch:6 step:6339 [D loss: 0.213947, acc.: 64.84%] [G loss: 0.449551]\n",
      "epoch:6 step:6340 [D loss: 0.241576, acc.: 63.28%] [G loss: 0.513993]\n",
      "epoch:6 step:6341 [D loss: 0.190202, acc.: 70.31%] [G loss: 0.546803]\n",
      "epoch:6 step:6342 [D loss: 0.225719, acc.: 59.38%] [G loss: 0.503225]\n",
      "epoch:6 step:6343 [D loss: 0.220214, acc.: 60.94%] [G loss: 0.501409]\n",
      "epoch:6 step:6344 [D loss: 0.235059, acc.: 60.94%] [G loss: 0.479240]\n",
      "epoch:6 step:6345 [D loss: 0.244054, acc.: 57.81%] [G loss: 0.500763]\n",
      "epoch:6 step:6346 [D loss: 0.200780, acc.: 67.19%] [G loss: 0.489708]\n",
      "epoch:6 step:6347 [D loss: 0.213200, acc.: 64.84%] [G loss: 0.503759]\n",
      "epoch:6 step:6348 [D loss: 0.216006, acc.: 62.50%] [G loss: 0.497287]\n",
      "epoch:6 step:6349 [D loss: 0.266797, acc.: 53.12%] [G loss: 0.442819]\n",
      "epoch:6 step:6350 [D loss: 0.205370, acc.: 64.84%] [G loss: 0.502569]\n",
      "epoch:6 step:6351 [D loss: 0.214039, acc.: 67.19%] [G loss: 0.485648]\n",
      "epoch:6 step:6352 [D loss: 0.191725, acc.: 70.31%] [G loss: 0.564146]\n",
      "epoch:6 step:6353 [D loss: 0.203353, acc.: 64.84%] [G loss: 0.515848]\n",
      "epoch:6 step:6354 [D loss: 0.233147, acc.: 60.94%] [G loss: 0.533332]\n",
      "epoch:6 step:6355 [D loss: 0.203451, acc.: 64.84%] [G loss: 0.524929]\n",
      "epoch:6 step:6356 [D loss: 0.230456, acc.: 60.94%] [G loss: 0.523882]\n",
      "epoch:6 step:6357 [D loss: 0.222811, acc.: 64.84%] [G loss: 0.454008]\n",
      "epoch:6 step:6358 [D loss: 0.204559, acc.: 65.62%] [G loss: 0.543968]\n",
      "epoch:6 step:6359 [D loss: 0.191072, acc.: 70.31%] [G loss: 0.529602]\n",
      "epoch:6 step:6360 [D loss: 0.238779, acc.: 60.16%] [G loss: 0.476607]\n",
      "epoch:6 step:6361 [D loss: 0.258484, acc.: 67.19%] [G loss: 0.441923]\n",
      "epoch:6 step:6362 [D loss: 0.222947, acc.: 62.50%] [G loss: 0.486604]\n",
      "epoch:6 step:6363 [D loss: 0.221670, acc.: 63.28%] [G loss: 0.520305]\n",
      "epoch:6 step:6364 [D loss: 0.215884, acc.: 62.50%] [G loss: 0.533594]\n",
      "epoch:6 step:6365 [D loss: 0.177358, acc.: 75.78%] [G loss: 0.552656]\n",
      "epoch:6 step:6366 [D loss: 0.252181, acc.: 57.81%] [G loss: 0.478902]\n",
      "epoch:6 step:6367 [D loss: 0.214332, acc.: 66.41%] [G loss: 0.476577]\n",
      "epoch:6 step:6368 [D loss: 0.193455, acc.: 68.75%] [G loss: 0.516847]\n",
      "epoch:6 step:6369 [D loss: 0.190441, acc.: 76.56%] [G loss: 0.496687]\n",
      "epoch:6 step:6370 [D loss: 0.198568, acc.: 68.75%] [G loss: 0.529979]\n",
      "epoch:6 step:6371 [D loss: 0.188486, acc.: 71.09%] [G loss: 0.473857]\n",
      "epoch:6 step:6372 [D loss: 0.219839, acc.: 67.97%] [G loss: 0.478802]\n",
      "epoch:6 step:6373 [D loss: 0.197628, acc.: 70.31%] [G loss: 0.527558]\n",
      "epoch:6 step:6374 [D loss: 0.226021, acc.: 61.72%] [G loss: 0.502621]\n",
      "epoch:6 step:6375 [D loss: 0.232919, acc.: 62.50%] [G loss: 0.465512]\n",
      "epoch:6 step:6376 [D loss: 0.179733, acc.: 78.12%] [G loss: 0.567325]\n",
      "epoch:6 step:6377 [D loss: 0.194404, acc.: 69.53%] [G loss: 0.542748]\n",
      "epoch:6 step:6378 [D loss: 0.250452, acc.: 60.16%] [G loss: 0.473879]\n",
      "epoch:6 step:6379 [D loss: 0.236641, acc.: 66.41%] [G loss: 0.476858]\n",
      "epoch:6 step:6380 [D loss: 0.221848, acc.: 67.19%] [G loss: 0.513877]\n",
      "epoch:6 step:6381 [D loss: 0.236521, acc.: 60.16%] [G loss: 0.504483]\n",
      "epoch:6 step:6382 [D loss: 0.227253, acc.: 64.84%] [G loss: 0.503797]\n",
      "epoch:6 step:6383 [D loss: 0.209527, acc.: 65.62%] [G loss: 0.506360]\n",
      "epoch:6 step:6384 [D loss: 0.230321, acc.: 57.81%] [G loss: 0.499922]\n",
      "epoch:6 step:6385 [D loss: 0.222617, acc.: 64.84%] [G loss: 0.468061]\n",
      "epoch:6 step:6386 [D loss: 0.218971, acc.: 64.84%] [G loss: 0.445351]\n",
      "epoch:6 step:6387 [D loss: 0.280918, acc.: 50.78%] [G loss: 0.425087]\n",
      "epoch:6 step:6388 [D loss: 0.257475, acc.: 52.34%] [G loss: 0.444687]\n",
      "epoch:6 step:6389 [D loss: 0.206616, acc.: 65.62%] [G loss: 0.492384]\n",
      "epoch:6 step:6390 [D loss: 0.207871, acc.: 67.97%] [G loss: 0.472358]\n",
      "epoch:6 step:6391 [D loss: 0.191610, acc.: 70.31%] [G loss: 0.560138]\n",
      "epoch:6 step:6392 [D loss: 0.216964, acc.: 66.41%] [G loss: 0.528689]\n",
      "epoch:6 step:6393 [D loss: 0.213937, acc.: 64.06%] [G loss: 0.485085]\n",
      "epoch:6 step:6394 [D loss: 0.194243, acc.: 68.75%] [G loss: 0.491479]\n",
      "epoch:6 step:6395 [D loss: 0.194890, acc.: 67.97%] [G loss: 0.489193]\n",
      "epoch:6 step:6396 [D loss: 0.199304, acc.: 72.66%] [G loss: 0.483236]\n",
      "epoch:6 step:6397 [D loss: 0.194061, acc.: 67.97%] [G loss: 0.498750]\n",
      "epoch:6 step:6398 [D loss: 0.210483, acc.: 67.19%] [G loss: 0.495433]\n",
      "epoch:6 step:6399 [D loss: 0.213316, acc.: 64.84%] [G loss: 0.520171]\n",
      "epoch:6 step:6400 [D loss: 0.217674, acc.: 65.62%] [G loss: 0.466092]\n",
      "##############\n",
      "[2.69857104 1.74449239 6.25671981 4.60260512 3.87875216 5.6400479\n",
      " 4.62045645 4.87328258 4.60176277 3.51757061]\n",
      "##########\n",
      "epoch:6 step:6401 [D loss: 0.207485, acc.: 68.75%] [G loss: 0.501497]\n",
      "epoch:6 step:6402 [D loss: 0.196927, acc.: 68.75%] [G loss: 0.475698]\n",
      "epoch:6 step:6403 [D loss: 0.175952, acc.: 71.88%] [G loss: 0.535487]\n",
      "epoch:6 step:6404 [D loss: 0.180450, acc.: 78.12%] [G loss: 0.589235]\n",
      "epoch:6 step:6405 [D loss: 0.229489, acc.: 62.50%] [G loss: 0.545901]\n",
      "epoch:6 step:6406 [D loss: 0.268992, acc.: 60.94%] [G loss: 0.486501]\n",
      "epoch:6 step:6407 [D loss: 0.208830, acc.: 65.62%] [G loss: 0.497257]\n",
      "epoch:6 step:6408 [D loss: 0.207751, acc.: 68.75%] [G loss: 0.454200]\n",
      "epoch:6 step:6409 [D loss: 0.245805, acc.: 57.81%] [G loss: 0.464700]\n",
      "epoch:6 step:6410 [D loss: 0.264016, acc.: 56.25%] [G loss: 0.449967]\n",
      "epoch:6 step:6411 [D loss: 0.212679, acc.: 67.97%] [G loss: 0.510352]\n",
      "epoch:6 step:6412 [D loss: 0.240608, acc.: 60.16%] [G loss: 0.499200]\n",
      "epoch:6 step:6413 [D loss: 0.235925, acc.: 63.28%] [G loss: 0.473236]\n",
      "epoch:6 step:6414 [D loss: 0.184540, acc.: 71.88%] [G loss: 0.545627]\n",
      "epoch:6 step:6415 [D loss: 0.231070, acc.: 64.06%] [G loss: 0.515723]\n",
      "epoch:6 step:6416 [D loss: 0.277311, acc.: 48.44%] [G loss: 0.481300]\n",
      "epoch:6 step:6417 [D loss: 0.268728, acc.: 52.34%] [G loss: 0.434588]\n",
      "epoch:6 step:6418 [D loss: 0.202254, acc.: 63.28%] [G loss: 0.546354]\n",
      "epoch:6 step:6419 [D loss: 0.226392, acc.: 60.16%] [G loss: 0.504466]\n",
      "epoch:6 step:6420 [D loss: 0.197587, acc.: 71.09%] [G loss: 0.498549]\n",
      "epoch:6 step:6421 [D loss: 0.209744, acc.: 66.41%] [G loss: 0.487740]\n",
      "epoch:6 step:6422 [D loss: 0.218768, acc.: 65.62%] [G loss: 0.495230]\n",
      "epoch:6 step:6423 [D loss: 0.209496, acc.: 69.53%] [G loss: 0.504622]\n",
      "epoch:6 step:6424 [D loss: 0.191144, acc.: 72.66%] [G loss: 0.532860]\n",
      "epoch:6 step:6425 [D loss: 0.210423, acc.: 66.41%] [G loss: 0.571323]\n",
      "epoch:6 step:6426 [D loss: 0.215078, acc.: 64.06%] [G loss: 0.475526]\n",
      "epoch:6 step:6427 [D loss: 0.216588, acc.: 69.53%] [G loss: 0.491007]\n",
      "epoch:6 step:6428 [D loss: 0.202964, acc.: 69.53%] [G loss: 0.494978]\n",
      "epoch:6 step:6429 [D loss: 0.192021, acc.: 70.31%] [G loss: 0.523619]\n",
      "epoch:6 step:6430 [D loss: 0.225783, acc.: 65.62%] [G loss: 0.532842]\n",
      "epoch:6 step:6431 [D loss: 0.228681, acc.: 60.16%] [G loss: 0.511864]\n",
      "epoch:6 step:6432 [D loss: 0.200909, acc.: 71.88%] [G loss: 0.505491]\n",
      "epoch:6 step:6433 [D loss: 0.210235, acc.: 64.84%] [G loss: 0.484830]\n",
      "epoch:6 step:6434 [D loss: 0.254070, acc.: 59.38%] [G loss: 0.439959]\n",
      "epoch:6 step:6435 [D loss: 0.262472, acc.: 61.72%] [G loss: 0.452510]\n",
      "epoch:6 step:6436 [D loss: 0.206616, acc.: 64.84%] [G loss: 0.514849]\n",
      "epoch:6 step:6437 [D loss: 0.210970, acc.: 66.41%] [G loss: 0.506956]\n",
      "epoch:6 step:6438 [D loss: 0.214590, acc.: 67.19%] [G loss: 0.543952]\n",
      "epoch:6 step:6439 [D loss: 0.251758, acc.: 59.38%] [G loss: 0.484366]\n",
      "epoch:6 step:6440 [D loss: 0.209446, acc.: 61.72%] [G loss: 0.490387]\n",
      "epoch:6 step:6441 [D loss: 0.181214, acc.: 76.56%] [G loss: 0.523208]\n",
      "epoch:6 step:6442 [D loss: 0.216253, acc.: 66.41%] [G loss: 0.497156]\n",
      "epoch:6 step:6443 [D loss: 0.230926, acc.: 62.50%] [G loss: 0.460805]\n",
      "epoch:6 step:6444 [D loss: 0.208218, acc.: 63.28%] [G loss: 0.489633]\n",
      "epoch:6 step:6445 [D loss: 0.182798, acc.: 75.00%] [G loss: 0.536067]\n",
      "epoch:6 step:6446 [D loss: 0.225018, acc.: 64.06%] [G loss: 0.497018]\n",
      "epoch:6 step:6447 [D loss: 0.221964, acc.: 64.06%] [G loss: 0.468997]\n",
      "epoch:6 step:6448 [D loss: 0.213565, acc.: 65.62%] [G loss: 0.461629]\n",
      "epoch:6 step:6449 [D loss: 0.261811, acc.: 55.47%] [G loss: 0.462012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6450 [D loss: 0.223855, acc.: 63.28%] [G loss: 0.457724]\n",
      "epoch:6 step:6451 [D loss: 0.221411, acc.: 64.06%] [G loss: 0.488360]\n",
      "epoch:6 step:6452 [D loss: 0.207210, acc.: 67.97%] [G loss: 0.482138]\n",
      "epoch:6 step:6453 [D loss: 0.213838, acc.: 66.41%] [G loss: 0.467624]\n",
      "epoch:6 step:6454 [D loss: 0.196009, acc.: 71.09%] [G loss: 0.503369]\n",
      "epoch:6 step:6455 [D loss: 0.189050, acc.: 74.22%] [G loss: 0.526001]\n",
      "epoch:6 step:6456 [D loss: 0.199400, acc.: 70.31%] [G loss: 0.516685]\n",
      "epoch:6 step:6457 [D loss: 0.188502, acc.: 71.09%] [G loss: 0.527072]\n",
      "epoch:6 step:6458 [D loss: 0.195313, acc.: 71.88%] [G loss: 0.498697]\n",
      "epoch:6 step:6459 [D loss: 0.183780, acc.: 77.34%] [G loss: 0.543907]\n",
      "epoch:6 step:6460 [D loss: 0.197204, acc.: 70.31%] [G loss: 0.524689]\n",
      "epoch:6 step:6461 [D loss: 0.232904, acc.: 61.72%] [G loss: 0.468441]\n",
      "epoch:6 step:6462 [D loss: 0.236880, acc.: 54.69%] [G loss: 0.478085]\n",
      "epoch:6 step:6463 [D loss: 0.198012, acc.: 67.19%] [G loss: 0.476716]\n",
      "epoch:6 step:6464 [D loss: 0.182771, acc.: 76.56%] [G loss: 0.528623]\n",
      "epoch:6 step:6465 [D loss: 0.206356, acc.: 67.19%] [G loss: 0.539652]\n",
      "epoch:6 step:6466 [D loss: 0.233536, acc.: 59.38%] [G loss: 0.509587]\n",
      "epoch:6 step:6467 [D loss: 0.220980, acc.: 63.28%] [G loss: 0.490590]\n",
      "epoch:6 step:6468 [D loss: 0.222639, acc.: 69.53%] [G loss: 0.502769]\n",
      "epoch:6 step:6469 [D loss: 0.243773, acc.: 57.81%] [G loss: 0.452848]\n",
      "epoch:6 step:6470 [D loss: 0.204376, acc.: 63.28%] [G loss: 0.502362]\n",
      "epoch:6 step:6471 [D loss: 0.215567, acc.: 65.62%] [G loss: 0.485182]\n",
      "epoch:6 step:6472 [D loss: 0.223752, acc.: 64.06%] [G loss: 0.536198]\n",
      "epoch:6 step:6473 [D loss: 0.231152, acc.: 59.38%] [G loss: 0.478903]\n",
      "epoch:6 step:6474 [D loss: 0.210791, acc.: 70.31%] [G loss: 0.471052]\n",
      "epoch:6 step:6475 [D loss: 0.208054, acc.: 67.19%] [G loss: 0.534914]\n",
      "epoch:6 step:6476 [D loss: 0.211054, acc.: 71.09%] [G loss: 0.506265]\n",
      "epoch:6 step:6477 [D loss: 0.238775, acc.: 58.59%] [G loss: 0.492866]\n",
      "epoch:6 step:6478 [D loss: 0.222182, acc.: 62.50%] [G loss: 0.505433]\n",
      "epoch:6 step:6479 [D loss: 0.243016, acc.: 63.28%] [G loss: 0.500192]\n",
      "epoch:6 step:6480 [D loss: 0.244638, acc.: 59.38%] [G loss: 0.490199]\n",
      "epoch:6 step:6481 [D loss: 0.234959, acc.: 65.62%] [G loss: 0.479620]\n",
      "epoch:6 step:6482 [D loss: 0.162235, acc.: 78.12%] [G loss: 0.613770]\n",
      "epoch:6 step:6483 [D loss: 0.240486, acc.: 58.59%] [G loss: 0.466526]\n",
      "epoch:6 step:6484 [D loss: 0.224788, acc.: 65.62%] [G loss: 0.461245]\n",
      "epoch:6 step:6485 [D loss: 0.215317, acc.: 70.31%] [G loss: 0.486696]\n",
      "epoch:6 step:6486 [D loss: 0.225246, acc.: 66.41%] [G loss: 0.465939]\n",
      "epoch:6 step:6487 [D loss: 0.215218, acc.: 70.31%] [G loss: 0.475506]\n",
      "epoch:6 step:6488 [D loss: 0.216230, acc.: 62.50%] [G loss: 0.459998]\n",
      "epoch:6 step:6489 [D loss: 0.238629, acc.: 58.59%] [G loss: 0.456422]\n",
      "epoch:6 step:6490 [D loss: 0.231990, acc.: 65.62%] [G loss: 0.488270]\n",
      "epoch:6 step:6491 [D loss: 0.248365, acc.: 60.94%] [G loss: 0.483628]\n",
      "epoch:6 step:6492 [D loss: 0.221167, acc.: 63.28%] [G loss: 0.520644]\n",
      "epoch:6 step:6493 [D loss: 0.213912, acc.: 66.41%] [G loss: 0.496152]\n",
      "epoch:6 step:6494 [D loss: 0.194232, acc.: 72.66%] [G loss: 0.514695]\n",
      "epoch:6 step:6495 [D loss: 0.218754, acc.: 67.97%] [G loss: 0.491319]\n",
      "epoch:6 step:6496 [D loss: 0.238553, acc.: 63.28%] [G loss: 0.508931]\n",
      "epoch:6 step:6497 [D loss: 0.201658, acc.: 67.97%] [G loss: 0.517676]\n",
      "epoch:6 step:6498 [D loss: 0.231816, acc.: 59.38%] [G loss: 0.499000]\n",
      "epoch:6 step:6499 [D loss: 0.241409, acc.: 59.38%] [G loss: 0.464061]\n",
      "epoch:6 step:6500 [D loss: 0.218344, acc.: 60.16%] [G loss: 0.441180]\n",
      "epoch:6 step:6501 [D loss: 0.215295, acc.: 66.41%] [G loss: 0.510040]\n",
      "epoch:6 step:6502 [D loss: 0.263556, acc.: 52.34%] [G loss: 0.453740]\n",
      "epoch:6 step:6503 [D loss: 0.218713, acc.: 62.50%] [G loss: 0.466086]\n",
      "epoch:6 step:6504 [D loss: 0.221792, acc.: 63.28%] [G loss: 0.481328]\n",
      "epoch:6 step:6505 [D loss: 0.268096, acc.: 48.44%] [G loss: 0.465489]\n",
      "epoch:6 step:6506 [D loss: 0.175252, acc.: 79.69%] [G loss: 0.515662]\n",
      "epoch:6 step:6507 [D loss: 0.210493, acc.: 64.84%] [G loss: 0.515942]\n",
      "epoch:6 step:6508 [D loss: 0.182583, acc.: 71.88%] [G loss: 0.570031]\n",
      "epoch:6 step:6509 [D loss: 0.195761, acc.: 71.09%] [G loss: 0.518746]\n",
      "epoch:6 step:6510 [D loss: 0.207514, acc.: 66.41%] [G loss: 0.517685]\n",
      "epoch:6 step:6511 [D loss: 0.210243, acc.: 67.19%] [G loss: 0.510143]\n",
      "epoch:6 step:6512 [D loss: 0.180260, acc.: 73.44%] [G loss: 0.526084]\n",
      "epoch:6 step:6513 [D loss: 0.261767, acc.: 57.03%] [G loss: 0.456775]\n",
      "epoch:6 step:6514 [D loss: 0.233577, acc.: 56.25%] [G loss: 0.495329]\n",
      "epoch:6 step:6515 [D loss: 0.193599, acc.: 71.88%] [G loss: 0.527131]\n",
      "epoch:6 step:6516 [D loss: 0.172731, acc.: 75.78%] [G loss: 0.569111]\n",
      "epoch:6 step:6517 [D loss: 0.188020, acc.: 71.88%] [G loss: 0.558586]\n",
      "epoch:6 step:6518 [D loss: 0.187102, acc.: 71.09%] [G loss: 0.470935]\n",
      "epoch:6 step:6519 [D loss: 0.191010, acc.: 74.22%] [G loss: 0.550735]\n",
      "epoch:6 step:6520 [D loss: 0.183879, acc.: 74.22%] [G loss: 0.529341]\n",
      "epoch:6 step:6521 [D loss: 0.162648, acc.: 75.00%] [G loss: 0.553850]\n",
      "epoch:6 step:6522 [D loss: 0.184021, acc.: 72.66%] [G loss: 0.542635]\n",
      "epoch:6 step:6523 [D loss: 0.215433, acc.: 66.41%] [G loss: 0.500469]\n",
      "epoch:6 step:6524 [D loss: 0.225828, acc.: 64.06%] [G loss: 0.508378]\n",
      "epoch:6 step:6525 [D loss: 0.228542, acc.: 61.72%] [G loss: 0.462757]\n",
      "epoch:6 step:6526 [D loss: 0.238555, acc.: 63.28%] [G loss: 0.522184]\n",
      "epoch:6 step:6527 [D loss: 0.207417, acc.: 70.31%] [G loss: 0.535969]\n",
      "epoch:6 step:6528 [D loss: 0.217058, acc.: 68.75%] [G loss: 0.516009]\n",
      "epoch:6 step:6529 [D loss: 0.223254, acc.: 65.62%] [G loss: 0.456785]\n",
      "epoch:6 step:6530 [D loss: 0.200422, acc.: 69.53%] [G loss: 0.504851]\n",
      "epoch:6 step:6531 [D loss: 0.212372, acc.: 66.41%] [G loss: 0.497520]\n",
      "epoch:6 step:6532 [D loss: 0.203496, acc.: 74.22%] [G loss: 0.521769]\n",
      "epoch:6 step:6533 [D loss: 0.176309, acc.: 75.78%] [G loss: 0.532077]\n",
      "epoch:6 step:6534 [D loss: 0.161151, acc.: 77.34%] [G loss: 0.575984]\n",
      "epoch:6 step:6535 [D loss: 0.222463, acc.: 67.97%] [G loss: 0.521383]\n",
      "epoch:6 step:6536 [D loss: 0.240279, acc.: 64.84%] [G loss: 0.515973]\n",
      "epoch:6 step:6537 [D loss: 0.236662, acc.: 63.28%] [G loss: 0.504244]\n",
      "epoch:6 step:6538 [D loss: 0.215400, acc.: 66.41%] [G loss: 0.462816]\n",
      "epoch:6 step:6539 [D loss: 0.255002, acc.: 55.47%] [G loss: 0.460414]\n",
      "epoch:6 step:6540 [D loss: 0.181580, acc.: 75.00%] [G loss: 0.557170]\n",
      "epoch:6 step:6541 [D loss: 0.205434, acc.: 70.31%] [G loss: 0.536768]\n",
      "epoch:6 step:6542 [D loss: 0.282419, acc.: 56.25%] [G loss: 0.475665]\n",
      "epoch:6 step:6543 [D loss: 0.200589, acc.: 67.97%] [G loss: 0.525545]\n",
      "epoch:6 step:6544 [D loss: 0.214964, acc.: 69.53%] [G loss: 0.475915]\n",
      "epoch:6 step:6545 [D loss: 0.164006, acc.: 76.56%] [G loss: 0.538937]\n",
      "epoch:6 step:6546 [D loss: 0.176508, acc.: 72.66%] [G loss: 0.526640]\n",
      "epoch:6 step:6547 [D loss: 0.142249, acc.: 81.25%] [G loss: 0.701405]\n",
      "epoch:6 step:6548 [D loss: 0.149679, acc.: 84.38%] [G loss: 0.702232]\n",
      "epoch:6 step:6549 [D loss: 0.187660, acc.: 68.75%] [G loss: 0.606265]\n",
      "epoch:6 step:6550 [D loss: 0.332860, acc.: 54.69%] [G loss: 0.515049]\n",
      "epoch:6 step:6551 [D loss: 0.143239, acc.: 81.25%] [G loss: 0.632424]\n",
      "epoch:6 step:6552 [D loss: 0.216834, acc.: 68.75%] [G loss: 0.559411]\n",
      "epoch:6 step:6553 [D loss: 0.226798, acc.: 60.16%] [G loss: 0.501878]\n",
      "epoch:6 step:6554 [D loss: 0.229007, acc.: 60.94%] [G loss: 0.489371]\n",
      "epoch:6 step:6555 [D loss: 0.166264, acc.: 75.00%] [G loss: 0.572791]\n",
      "epoch:6 step:6556 [D loss: 0.221616, acc.: 71.09%] [G loss: 0.551903]\n",
      "epoch:6 step:6557 [D loss: 0.178665, acc.: 71.09%] [G loss: 0.582802]\n",
      "epoch:6 step:6558 [D loss: 0.169052, acc.: 72.66%] [G loss: 0.586857]\n",
      "epoch:6 step:6559 [D loss: 0.142263, acc.: 82.81%] [G loss: 0.657372]\n",
      "epoch:7 step:6560 [D loss: 0.263304, acc.: 67.19%] [G loss: 0.539082]\n",
      "epoch:7 step:6561 [D loss: 0.243907, acc.: 60.16%] [G loss: 0.504775]\n",
      "epoch:7 step:6562 [D loss: 0.252130, acc.: 57.03%] [G loss: 0.504555]\n",
      "epoch:7 step:6563 [D loss: 0.236092, acc.: 57.03%] [G loss: 0.479740]\n",
      "epoch:7 step:6564 [D loss: 0.249189, acc.: 60.16%] [G loss: 0.506887]\n",
      "epoch:7 step:6565 [D loss: 0.220763, acc.: 63.28%] [G loss: 0.536647]\n",
      "epoch:7 step:6566 [D loss: 0.199057, acc.: 67.19%] [G loss: 0.540331]\n",
      "epoch:7 step:6567 [D loss: 0.244685, acc.: 57.03%] [G loss: 0.465327]\n",
      "epoch:7 step:6568 [D loss: 0.196678, acc.: 71.88%] [G loss: 0.536284]\n",
      "epoch:7 step:6569 [D loss: 0.234582, acc.: 60.16%] [G loss: 0.532751]\n",
      "epoch:7 step:6570 [D loss: 0.195269, acc.: 68.75%] [G loss: 0.509685]\n",
      "epoch:7 step:6571 [D loss: 0.204622, acc.: 68.75%] [G loss: 0.530502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6572 [D loss: 0.202427, acc.: 65.62%] [G loss: 0.519281]\n",
      "epoch:7 step:6573 [D loss: 0.203900, acc.: 70.31%] [G loss: 0.480129]\n",
      "epoch:7 step:6574 [D loss: 0.191520, acc.: 69.53%] [G loss: 0.512415]\n",
      "epoch:7 step:6575 [D loss: 0.209460, acc.: 63.28%] [G loss: 0.548538]\n",
      "epoch:7 step:6576 [D loss: 0.247632, acc.: 58.59%] [G loss: 0.507212]\n",
      "epoch:7 step:6577 [D loss: 0.239718, acc.: 64.06%] [G loss: 0.479910]\n",
      "epoch:7 step:6578 [D loss: 0.219676, acc.: 65.62%] [G loss: 0.493980]\n",
      "epoch:7 step:6579 [D loss: 0.244262, acc.: 55.47%] [G loss: 0.465849]\n",
      "epoch:7 step:6580 [D loss: 0.230235, acc.: 64.06%] [G loss: 0.540066]\n",
      "epoch:7 step:6581 [D loss: 0.182175, acc.: 72.66%] [G loss: 0.557478]\n",
      "epoch:7 step:6582 [D loss: 0.203831, acc.: 71.09%] [G loss: 0.494057]\n",
      "epoch:7 step:6583 [D loss: 0.210855, acc.: 66.41%] [G loss: 0.481428]\n",
      "epoch:7 step:6584 [D loss: 0.199448, acc.: 71.09%] [G loss: 0.535896]\n",
      "epoch:7 step:6585 [D loss: 0.218078, acc.: 67.97%] [G loss: 0.520957]\n",
      "epoch:7 step:6586 [D loss: 0.223590, acc.: 64.06%] [G loss: 0.495791]\n",
      "epoch:7 step:6587 [D loss: 0.196501, acc.: 66.41%] [G loss: 0.534752]\n",
      "epoch:7 step:6588 [D loss: 0.197970, acc.: 71.09%] [G loss: 0.512038]\n",
      "epoch:7 step:6589 [D loss: 0.219020, acc.: 63.28%] [G loss: 0.543329]\n",
      "epoch:7 step:6590 [D loss: 0.217047, acc.: 64.06%] [G loss: 0.485206]\n",
      "epoch:7 step:6591 [D loss: 0.236458, acc.: 60.94%] [G loss: 0.454408]\n",
      "epoch:7 step:6592 [D loss: 0.180585, acc.: 70.31%] [G loss: 0.521670]\n",
      "epoch:7 step:6593 [D loss: 0.235220, acc.: 58.59%] [G loss: 0.511602]\n",
      "epoch:7 step:6594 [D loss: 0.220136, acc.: 64.84%] [G loss: 0.479247]\n",
      "epoch:7 step:6595 [D loss: 0.199734, acc.: 67.97%] [G loss: 0.491133]\n",
      "epoch:7 step:6596 [D loss: 0.211790, acc.: 64.06%] [G loss: 0.474580]\n",
      "epoch:7 step:6597 [D loss: 0.253970, acc.: 58.59%] [G loss: 0.506645]\n",
      "epoch:7 step:6598 [D loss: 0.207668, acc.: 70.31%] [G loss: 0.508932]\n",
      "epoch:7 step:6599 [D loss: 0.164100, acc.: 78.91%] [G loss: 0.561852]\n",
      "epoch:7 step:6600 [D loss: 0.247277, acc.: 61.72%] [G loss: 0.517828]\n",
      "##############\n",
      "[2.69809632 1.4480364  6.30439503 4.89164247 3.57919253 5.81783871\n",
      " 4.73450168 4.68253393 4.6494614  3.69981143]\n",
      "##########\n",
      "epoch:7 step:6601 [D loss: 0.216527, acc.: 65.62%] [G loss: 0.516929]\n",
      "epoch:7 step:6602 [D loss: 0.206229, acc.: 67.19%] [G loss: 0.492553]\n",
      "epoch:7 step:6603 [D loss: 0.239790, acc.: 60.94%] [G loss: 0.451521]\n",
      "epoch:7 step:6604 [D loss: 0.210493, acc.: 61.72%] [G loss: 0.471344]\n",
      "epoch:7 step:6605 [D loss: 0.230359, acc.: 60.94%] [G loss: 0.482803]\n",
      "epoch:7 step:6606 [D loss: 0.213133, acc.: 67.97%] [G loss: 0.516082]\n",
      "epoch:7 step:6607 [D loss: 0.190663, acc.: 72.66%] [G loss: 0.535291]\n",
      "epoch:7 step:6608 [D loss: 0.209975, acc.: 71.09%] [G loss: 0.484508]\n",
      "epoch:7 step:6609 [D loss: 0.200566, acc.: 71.09%] [G loss: 0.490479]\n",
      "epoch:7 step:6610 [D loss: 0.239263, acc.: 56.25%] [G loss: 0.477717]\n",
      "epoch:7 step:6611 [D loss: 0.224421, acc.: 64.84%] [G loss: 0.488131]\n",
      "epoch:7 step:6612 [D loss: 0.201347, acc.: 69.53%] [G loss: 0.550292]\n",
      "epoch:7 step:6613 [D loss: 0.225330, acc.: 62.50%] [G loss: 0.492146]\n",
      "epoch:7 step:6614 [D loss: 0.177787, acc.: 74.22%] [G loss: 0.511526]\n",
      "epoch:7 step:6615 [D loss: 0.232363, acc.: 61.72%] [G loss: 0.506006]\n",
      "epoch:7 step:6616 [D loss: 0.210853, acc.: 67.19%] [G loss: 0.504022]\n",
      "epoch:7 step:6617 [D loss: 0.207586, acc.: 67.19%] [G loss: 0.504155]\n",
      "epoch:7 step:6618 [D loss: 0.195918, acc.: 68.75%] [G loss: 0.507605]\n",
      "epoch:7 step:6619 [D loss: 0.217947, acc.: 61.72%] [G loss: 0.433509]\n",
      "epoch:7 step:6620 [D loss: 0.255509, acc.: 57.81%] [G loss: 0.434280]\n",
      "epoch:7 step:6621 [D loss: 0.226122, acc.: 65.62%] [G loss: 0.469054]\n",
      "epoch:7 step:6622 [D loss: 0.220794, acc.: 63.28%] [G loss: 0.543337]\n",
      "epoch:7 step:6623 [D loss: 0.201986, acc.: 64.06%] [G loss: 0.513650]\n",
      "epoch:7 step:6624 [D loss: 0.218009, acc.: 63.28%] [G loss: 0.492913]\n",
      "epoch:7 step:6625 [D loss: 0.212790, acc.: 65.62%] [G loss: 0.478161]\n",
      "epoch:7 step:6626 [D loss: 0.204712, acc.: 70.31%] [G loss: 0.503641]\n",
      "epoch:7 step:6627 [D loss: 0.237163, acc.: 60.16%] [G loss: 0.459395]\n",
      "epoch:7 step:6628 [D loss: 0.209586, acc.: 66.41%] [G loss: 0.496148]\n",
      "epoch:7 step:6629 [D loss: 0.203638, acc.: 70.31%] [G loss: 0.509507]\n",
      "epoch:7 step:6630 [D loss: 0.207243, acc.: 65.62%] [G loss: 0.505207]\n",
      "epoch:7 step:6631 [D loss: 0.228479, acc.: 67.19%] [G loss: 0.497596]\n",
      "epoch:7 step:6632 [D loss: 0.224245, acc.: 68.75%] [G loss: 0.537233]\n",
      "epoch:7 step:6633 [D loss: 0.184717, acc.: 75.00%] [G loss: 0.537944]\n",
      "epoch:7 step:6634 [D loss: 0.206046, acc.: 69.53%] [G loss: 0.518195]\n",
      "epoch:7 step:6635 [D loss: 0.203938, acc.: 64.06%] [G loss: 0.532631]\n",
      "epoch:7 step:6636 [D loss: 0.167148, acc.: 75.00%] [G loss: 0.562075]\n",
      "epoch:7 step:6637 [D loss: 0.260377, acc.: 60.16%] [G loss: 0.428272]\n",
      "epoch:7 step:6638 [D loss: 0.229537, acc.: 62.50%] [G loss: 0.413262]\n",
      "epoch:7 step:6639 [D loss: 0.214313, acc.: 63.28%] [G loss: 0.470540]\n",
      "epoch:7 step:6640 [D loss: 0.223276, acc.: 60.16%] [G loss: 0.482913]\n",
      "epoch:7 step:6641 [D loss: 0.223139, acc.: 57.81%] [G loss: 0.492320]\n",
      "epoch:7 step:6642 [D loss: 0.180786, acc.: 73.44%] [G loss: 0.557382]\n",
      "epoch:7 step:6643 [D loss: 0.217775, acc.: 64.06%] [G loss: 0.545564]\n",
      "epoch:7 step:6644 [D loss: 0.223138, acc.: 67.19%] [G loss: 0.523690]\n",
      "epoch:7 step:6645 [D loss: 0.236173, acc.: 64.84%] [G loss: 0.481317]\n",
      "epoch:7 step:6646 [D loss: 0.214492, acc.: 67.19%] [G loss: 0.497261]\n",
      "epoch:7 step:6647 [D loss: 0.183045, acc.: 71.09%] [G loss: 0.531803]\n",
      "epoch:7 step:6648 [D loss: 0.211345, acc.: 67.19%] [G loss: 0.489746]\n",
      "epoch:7 step:6649 [D loss: 0.207255, acc.: 67.19%] [G loss: 0.509944]\n",
      "epoch:7 step:6650 [D loss: 0.206074, acc.: 71.88%] [G loss: 0.498759]\n",
      "epoch:7 step:6651 [D loss: 0.195576, acc.: 73.44%] [G loss: 0.523476]\n",
      "epoch:7 step:6652 [D loss: 0.227267, acc.: 66.41%] [G loss: 0.465575]\n",
      "epoch:7 step:6653 [D loss: 0.215252, acc.: 64.06%] [G loss: 0.499090]\n",
      "epoch:7 step:6654 [D loss: 0.214141, acc.: 60.94%] [G loss: 0.516935]\n",
      "epoch:7 step:6655 [D loss: 0.181222, acc.: 67.97%] [G loss: 0.521933]\n",
      "epoch:7 step:6656 [D loss: 0.207231, acc.: 66.41%] [G loss: 0.530173]\n",
      "epoch:7 step:6657 [D loss: 0.232498, acc.: 63.28%] [G loss: 0.466501]\n",
      "epoch:7 step:6658 [D loss: 0.217637, acc.: 67.97%] [G loss: 0.513889]\n",
      "epoch:7 step:6659 [D loss: 0.166197, acc.: 69.53%] [G loss: 0.578065]\n",
      "epoch:7 step:6660 [D loss: 0.226378, acc.: 61.72%] [G loss: 0.499317]\n",
      "epoch:7 step:6661 [D loss: 0.277533, acc.: 53.12%] [G loss: 0.447319]\n",
      "epoch:7 step:6662 [D loss: 0.191963, acc.: 71.09%] [G loss: 0.459020]\n",
      "epoch:7 step:6663 [D loss: 0.195626, acc.: 69.53%] [G loss: 0.490265]\n",
      "epoch:7 step:6664 [D loss: 0.223341, acc.: 64.84%] [G loss: 0.481927]\n",
      "epoch:7 step:6665 [D loss: 0.212535, acc.: 68.75%] [G loss: 0.501224]\n",
      "epoch:7 step:6666 [D loss: 0.201267, acc.: 70.31%] [G loss: 0.521546]\n",
      "epoch:7 step:6667 [D loss: 0.259266, acc.: 57.03%] [G loss: 0.479835]\n",
      "epoch:7 step:6668 [D loss: 0.264938, acc.: 53.12%] [G loss: 0.456265]\n",
      "epoch:7 step:6669 [D loss: 0.231914, acc.: 64.06%] [G loss: 0.529247]\n",
      "epoch:7 step:6670 [D loss: 0.207192, acc.: 68.75%] [G loss: 0.520498]\n",
      "epoch:7 step:6671 [D loss: 0.202170, acc.: 66.41%] [G loss: 0.468782]\n",
      "epoch:7 step:6672 [D loss: 0.201836, acc.: 69.53%] [G loss: 0.508984]\n",
      "epoch:7 step:6673 [D loss: 0.198530, acc.: 69.53%] [G loss: 0.535948]\n",
      "epoch:7 step:6674 [D loss: 0.238362, acc.: 67.19%] [G loss: 0.491993]\n",
      "epoch:7 step:6675 [D loss: 0.222510, acc.: 60.94%] [G loss: 0.517130]\n",
      "epoch:7 step:6676 [D loss: 0.222113, acc.: 64.84%] [G loss: 0.552517]\n",
      "epoch:7 step:6677 [D loss: 0.227434, acc.: 64.06%] [G loss: 0.545729]\n",
      "epoch:7 step:6678 [D loss: 0.171180, acc.: 75.00%] [G loss: 0.559224]\n",
      "epoch:7 step:6679 [D loss: 0.283026, acc.: 58.59%] [G loss: 0.428733]\n",
      "epoch:7 step:6680 [D loss: 0.215845, acc.: 63.28%] [G loss: 0.478517]\n",
      "epoch:7 step:6681 [D loss: 0.205937, acc.: 71.88%] [G loss: 0.528602]\n",
      "epoch:7 step:6682 [D loss: 0.195946, acc.: 69.53%] [G loss: 0.537832]\n",
      "epoch:7 step:6683 [D loss: 0.216643, acc.: 62.50%] [G loss: 0.525315]\n",
      "epoch:7 step:6684 [D loss: 0.231087, acc.: 63.28%] [G loss: 0.418869]\n",
      "epoch:7 step:6685 [D loss: 0.178415, acc.: 71.09%] [G loss: 0.536893]\n",
      "epoch:7 step:6686 [D loss: 0.198167, acc.: 64.84%] [G loss: 0.490815]\n",
      "epoch:7 step:6687 [D loss: 0.234728, acc.: 59.38%] [G loss: 0.456147]\n",
      "epoch:7 step:6688 [D loss: 0.250362, acc.: 58.59%] [G loss: 0.449790]\n",
      "epoch:7 step:6689 [D loss: 0.185208, acc.: 73.44%] [G loss: 0.528340]\n",
      "epoch:7 step:6690 [D loss: 0.222477, acc.: 60.16%] [G loss: 0.522331]\n",
      "epoch:7 step:6691 [D loss: 0.213522, acc.: 67.19%] [G loss: 0.486129]\n",
      "epoch:7 step:6692 [D loss: 0.225951, acc.: 62.50%] [G loss: 0.482575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6693 [D loss: 0.207304, acc.: 65.62%] [G loss: 0.501766]\n",
      "epoch:7 step:6694 [D loss: 0.187667, acc.: 75.00%] [G loss: 0.484198]\n",
      "epoch:7 step:6695 [D loss: 0.211713, acc.: 65.62%] [G loss: 0.478957]\n",
      "epoch:7 step:6696 [D loss: 0.246086, acc.: 61.72%] [G loss: 0.494371]\n",
      "epoch:7 step:6697 [D loss: 0.236195, acc.: 64.06%] [G loss: 0.444294]\n",
      "epoch:7 step:6698 [D loss: 0.195510, acc.: 71.09%] [G loss: 0.497955]\n",
      "epoch:7 step:6699 [D loss: 0.243317, acc.: 53.12%] [G loss: 0.462443]\n",
      "epoch:7 step:6700 [D loss: 0.244367, acc.: 54.69%] [G loss: 0.473666]\n",
      "epoch:7 step:6701 [D loss: 0.250803, acc.: 60.94%] [G loss: 0.472893]\n",
      "epoch:7 step:6702 [D loss: 0.245328, acc.: 56.25%] [G loss: 0.499702]\n",
      "epoch:7 step:6703 [D loss: 0.188957, acc.: 71.09%] [G loss: 0.539428]\n",
      "epoch:7 step:6704 [D loss: 0.220983, acc.: 62.50%] [G loss: 0.525787]\n",
      "epoch:7 step:6705 [D loss: 0.235015, acc.: 57.81%] [G loss: 0.491500]\n",
      "epoch:7 step:6706 [D loss: 0.237044, acc.: 63.28%] [G loss: 0.492216]\n",
      "epoch:7 step:6707 [D loss: 0.230352, acc.: 62.50%] [G loss: 0.472999]\n",
      "epoch:7 step:6708 [D loss: 0.210969, acc.: 71.88%] [G loss: 0.497772]\n",
      "epoch:7 step:6709 [D loss: 0.252713, acc.: 53.91%] [G loss: 0.451953]\n",
      "epoch:7 step:6710 [D loss: 0.221659, acc.: 69.53%] [G loss: 0.487678]\n",
      "epoch:7 step:6711 [D loss: 0.175559, acc.: 74.22%] [G loss: 0.534170]\n",
      "epoch:7 step:6712 [D loss: 0.234625, acc.: 61.72%] [G loss: 0.465186]\n",
      "epoch:7 step:6713 [D loss: 0.220387, acc.: 60.94%] [G loss: 0.486577]\n",
      "epoch:7 step:6714 [D loss: 0.206350, acc.: 67.19%] [G loss: 0.537917]\n",
      "epoch:7 step:6715 [D loss: 0.210294, acc.: 65.62%] [G loss: 0.494803]\n",
      "epoch:7 step:6716 [D loss: 0.249370, acc.: 57.03%] [G loss: 0.499182]\n",
      "epoch:7 step:6717 [D loss: 0.213196, acc.: 68.75%] [G loss: 0.484631]\n",
      "epoch:7 step:6718 [D loss: 0.225081, acc.: 68.75%] [G loss: 0.479829]\n",
      "epoch:7 step:6719 [D loss: 0.242768, acc.: 57.81%] [G loss: 0.467894]\n",
      "epoch:7 step:6720 [D loss: 0.186934, acc.: 71.88%] [G loss: 0.540954]\n",
      "epoch:7 step:6721 [D loss: 0.172789, acc.: 73.44%] [G loss: 0.547176]\n",
      "epoch:7 step:6722 [D loss: 0.201780, acc.: 66.41%] [G loss: 0.498829]\n",
      "epoch:7 step:6723 [D loss: 0.200151, acc.: 65.62%] [G loss: 0.505133]\n",
      "epoch:7 step:6724 [D loss: 0.197475, acc.: 66.41%] [G loss: 0.532453]\n",
      "epoch:7 step:6725 [D loss: 0.200839, acc.: 69.53%] [G loss: 0.495231]\n",
      "epoch:7 step:6726 [D loss: 0.226556, acc.: 59.38%] [G loss: 0.461835]\n",
      "epoch:7 step:6727 [D loss: 0.186806, acc.: 72.66%] [G loss: 0.529505]\n",
      "epoch:7 step:6728 [D loss: 0.235543, acc.: 58.59%] [G loss: 0.484570]\n",
      "epoch:7 step:6729 [D loss: 0.231790, acc.: 61.72%] [G loss: 0.450964]\n",
      "epoch:7 step:6730 [D loss: 0.206464, acc.: 71.09%] [G loss: 0.495134]\n",
      "epoch:7 step:6731 [D loss: 0.218994, acc.: 63.28%] [G loss: 0.464178]\n",
      "epoch:7 step:6732 [D loss: 0.212412, acc.: 67.97%] [G loss: 0.474299]\n",
      "epoch:7 step:6733 [D loss: 0.260731, acc.: 58.59%] [G loss: 0.457519]\n",
      "epoch:7 step:6734 [D loss: 0.211483, acc.: 64.06%] [G loss: 0.454541]\n",
      "epoch:7 step:6735 [D loss: 0.216348, acc.: 63.28%] [G loss: 0.461421]\n",
      "epoch:7 step:6736 [D loss: 0.193436, acc.: 73.44%] [G loss: 0.495485]\n",
      "epoch:7 step:6737 [D loss: 0.218408, acc.: 60.94%] [G loss: 0.483858]\n",
      "epoch:7 step:6738 [D loss: 0.235547, acc.: 57.81%] [G loss: 0.502256]\n",
      "epoch:7 step:6739 [D loss: 0.212381, acc.: 65.62%] [G loss: 0.481371]\n",
      "epoch:7 step:6740 [D loss: 0.199695, acc.: 67.97%] [G loss: 0.464508]\n",
      "epoch:7 step:6741 [D loss: 0.254561, acc.: 56.25%] [G loss: 0.460706]\n",
      "epoch:7 step:6742 [D loss: 0.254350, acc.: 58.59%] [G loss: 0.492314]\n",
      "epoch:7 step:6743 [D loss: 0.209559, acc.: 67.97%] [G loss: 0.510489]\n",
      "epoch:7 step:6744 [D loss: 0.234547, acc.: 63.28%] [G loss: 0.471449]\n",
      "epoch:7 step:6745 [D loss: 0.260026, acc.: 62.50%] [G loss: 0.448062]\n",
      "epoch:7 step:6746 [D loss: 0.219248, acc.: 63.28%] [G loss: 0.518145]\n",
      "epoch:7 step:6747 [D loss: 0.212312, acc.: 65.62%] [G loss: 0.484055]\n",
      "epoch:7 step:6748 [D loss: 0.240012, acc.: 54.69%] [G loss: 0.459901]\n",
      "epoch:7 step:6749 [D loss: 0.204405, acc.: 69.53%] [G loss: 0.499750]\n",
      "epoch:7 step:6750 [D loss: 0.186471, acc.: 72.66%] [G loss: 0.511222]\n",
      "epoch:7 step:6751 [D loss: 0.196984, acc.: 73.44%] [G loss: 0.479599]\n",
      "epoch:7 step:6752 [D loss: 0.222352, acc.: 71.09%] [G loss: 0.481780]\n",
      "epoch:7 step:6753 [D loss: 0.206001, acc.: 68.75%] [G loss: 0.510372]\n",
      "epoch:7 step:6754 [D loss: 0.235871, acc.: 58.59%] [G loss: 0.517851]\n",
      "epoch:7 step:6755 [D loss: 0.217171, acc.: 64.06%] [G loss: 0.488971]\n",
      "epoch:7 step:6756 [D loss: 0.202720, acc.: 65.62%] [G loss: 0.467544]\n",
      "epoch:7 step:6757 [D loss: 0.192687, acc.: 72.66%] [G loss: 0.522244]\n",
      "epoch:7 step:6758 [D loss: 0.218758, acc.: 64.84%] [G loss: 0.504243]\n",
      "epoch:7 step:6759 [D loss: 0.242828, acc.: 57.81%] [G loss: 0.462800]\n",
      "epoch:7 step:6760 [D loss: 0.213513, acc.: 67.97%] [G loss: 0.479905]\n",
      "epoch:7 step:6761 [D loss: 0.231612, acc.: 63.28%] [G loss: 0.455522]\n",
      "epoch:7 step:6762 [D loss: 0.245397, acc.: 60.16%] [G loss: 0.466941]\n",
      "epoch:7 step:6763 [D loss: 0.213695, acc.: 68.75%] [G loss: 0.506985]\n",
      "epoch:7 step:6764 [D loss: 0.193553, acc.: 71.09%] [G loss: 0.539605]\n",
      "epoch:7 step:6765 [D loss: 0.183191, acc.: 71.09%] [G loss: 0.507128]\n",
      "epoch:7 step:6766 [D loss: 0.179609, acc.: 74.22%] [G loss: 0.519451]\n",
      "epoch:7 step:6767 [D loss: 0.180266, acc.: 71.09%] [G loss: 0.532826]\n",
      "epoch:7 step:6768 [D loss: 0.186914, acc.: 71.88%] [G loss: 0.520971]\n",
      "epoch:7 step:6769 [D loss: 0.233564, acc.: 64.84%] [G loss: 0.464268]\n",
      "epoch:7 step:6770 [D loss: 0.242727, acc.: 60.94%] [G loss: 0.419186]\n",
      "epoch:7 step:6771 [D loss: 0.235158, acc.: 60.94%] [G loss: 0.466475]\n",
      "epoch:7 step:6772 [D loss: 0.214866, acc.: 64.06%] [G loss: 0.477263]\n",
      "epoch:7 step:6773 [D loss: 0.240518, acc.: 54.69%] [G loss: 0.473935]\n",
      "epoch:7 step:6774 [D loss: 0.236309, acc.: 60.94%] [G loss: 0.454815]\n",
      "epoch:7 step:6775 [D loss: 0.218360, acc.: 65.62%] [G loss: 0.507415]\n",
      "epoch:7 step:6776 [D loss: 0.210120, acc.: 65.62%] [G loss: 0.489941]\n",
      "epoch:7 step:6777 [D loss: 0.196649, acc.: 70.31%] [G loss: 0.496486]\n",
      "epoch:7 step:6778 [D loss: 0.186405, acc.: 72.66%] [G loss: 0.559282]\n",
      "epoch:7 step:6779 [D loss: 0.259535, acc.: 58.59%] [G loss: 0.475781]\n",
      "epoch:7 step:6780 [D loss: 0.186368, acc.: 72.66%] [G loss: 0.533289]\n",
      "epoch:7 step:6781 [D loss: 0.207168, acc.: 64.06%] [G loss: 0.557242]\n",
      "epoch:7 step:6782 [D loss: 0.199500, acc.: 70.31%] [G loss: 0.524219]\n",
      "epoch:7 step:6783 [D loss: 0.244765, acc.: 56.25%] [G loss: 0.486045]\n",
      "epoch:7 step:6784 [D loss: 0.238432, acc.: 60.16%] [G loss: 0.475131]\n",
      "epoch:7 step:6785 [D loss: 0.235094, acc.: 61.72%] [G loss: 0.466819]\n",
      "epoch:7 step:6786 [D loss: 0.227998, acc.: 62.50%] [G loss: 0.463272]\n",
      "epoch:7 step:6787 [D loss: 0.251881, acc.: 57.03%] [G loss: 0.439887]\n",
      "epoch:7 step:6788 [D loss: 0.190343, acc.: 72.66%] [G loss: 0.546434]\n",
      "epoch:7 step:6789 [D loss: 0.177932, acc.: 76.56%] [G loss: 0.532046]\n",
      "epoch:7 step:6790 [D loss: 0.191749, acc.: 69.53%] [G loss: 0.544688]\n",
      "epoch:7 step:6791 [D loss: 0.172087, acc.: 71.88%] [G loss: 0.597780]\n",
      "epoch:7 step:6792 [D loss: 0.275849, acc.: 57.03%] [G loss: 0.491874]\n",
      "epoch:7 step:6793 [D loss: 0.221900, acc.: 60.16%] [G loss: 0.463588]\n",
      "epoch:7 step:6794 [D loss: 0.217982, acc.: 64.06%] [G loss: 0.450036]\n",
      "epoch:7 step:6795 [D loss: 0.170594, acc.: 71.88%] [G loss: 0.520806]\n",
      "epoch:7 step:6796 [D loss: 0.231319, acc.: 61.72%] [G loss: 0.460589]\n",
      "epoch:7 step:6797 [D loss: 0.228936, acc.: 65.62%] [G loss: 0.495900]\n",
      "epoch:7 step:6798 [D loss: 0.211020, acc.: 64.84%] [G loss: 0.484942]\n",
      "epoch:7 step:6799 [D loss: 0.251817, acc.: 55.47%] [G loss: 0.484962]\n",
      "epoch:7 step:6800 [D loss: 0.200003, acc.: 67.19%] [G loss: 0.502357]\n",
      "##############\n",
      "[2.56177271 1.40766339 6.44235905 4.92831178 4.02643392 5.94520294\n",
      " 4.47589115 4.86415681 4.67964066 3.73587437]\n",
      "##########\n",
      "epoch:7 step:6801 [D loss: 0.201497, acc.: 67.97%] [G loss: 0.494670]\n",
      "epoch:7 step:6802 [D loss: 0.232976, acc.: 60.94%] [G loss: 0.476029]\n",
      "epoch:7 step:6803 [D loss: 0.188006, acc.: 78.12%] [G loss: 0.546844]\n",
      "epoch:7 step:6804 [D loss: 0.223511, acc.: 64.84%] [G loss: 0.516077]\n",
      "epoch:7 step:6805 [D loss: 0.202770, acc.: 69.53%] [G loss: 0.541942]\n",
      "epoch:7 step:6806 [D loss: 0.234689, acc.: 60.16%] [G loss: 0.504871]\n",
      "epoch:7 step:6807 [D loss: 0.200390, acc.: 70.31%] [G loss: 0.543315]\n",
      "epoch:7 step:6808 [D loss: 0.253268, acc.: 57.03%] [G loss: 0.505439]\n",
      "epoch:7 step:6809 [D loss: 0.288652, acc.: 41.41%] [G loss: 0.454443]\n",
      "epoch:7 step:6810 [D loss: 0.244751, acc.: 60.94%] [G loss: 0.458448]\n",
      "epoch:7 step:6811 [D loss: 0.244961, acc.: 61.72%] [G loss: 0.490428]\n",
      "epoch:7 step:6812 [D loss: 0.195939, acc.: 71.09%] [G loss: 0.493430]\n",
      "epoch:7 step:6813 [D loss: 0.203633, acc.: 70.31%] [G loss: 0.459220]\n",
      "epoch:7 step:6814 [D loss: 0.203704, acc.: 64.84%] [G loss: 0.495303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6815 [D loss: 0.221177, acc.: 62.50%] [G loss: 0.507723]\n",
      "epoch:7 step:6816 [D loss: 0.219358, acc.: 62.50%] [G loss: 0.495899]\n",
      "epoch:7 step:6817 [D loss: 0.192968, acc.: 71.88%] [G loss: 0.498078]\n",
      "epoch:7 step:6818 [D loss: 0.198055, acc.: 64.84%] [G loss: 0.471712]\n",
      "epoch:7 step:6819 [D loss: 0.215662, acc.: 63.28%] [G loss: 0.550212]\n",
      "epoch:7 step:6820 [D loss: 0.207459, acc.: 64.84%] [G loss: 0.522545]\n",
      "epoch:7 step:6821 [D loss: 0.232256, acc.: 64.06%] [G loss: 0.503369]\n",
      "epoch:7 step:6822 [D loss: 0.227762, acc.: 64.84%] [G loss: 0.509035]\n",
      "epoch:7 step:6823 [D loss: 0.208608, acc.: 67.97%] [G loss: 0.510585]\n",
      "epoch:7 step:6824 [D loss: 0.256603, acc.: 58.59%] [G loss: 0.446893]\n",
      "epoch:7 step:6825 [D loss: 0.251941, acc.: 57.03%] [G loss: 0.446606]\n",
      "epoch:7 step:6826 [D loss: 0.209232, acc.: 69.53%] [G loss: 0.485533]\n",
      "epoch:7 step:6827 [D loss: 0.213658, acc.: 67.19%] [G loss: 0.477425]\n",
      "epoch:7 step:6828 [D loss: 0.225033, acc.: 63.28%] [G loss: 0.480629]\n",
      "epoch:7 step:6829 [D loss: 0.196529, acc.: 67.97%] [G loss: 0.484826]\n",
      "epoch:7 step:6830 [D loss: 0.169149, acc.: 76.56%] [G loss: 0.543433]\n",
      "epoch:7 step:6831 [D loss: 0.243694, acc.: 57.03%] [G loss: 0.459159]\n",
      "epoch:7 step:6832 [D loss: 0.217768, acc.: 63.28%] [G loss: 0.553119]\n",
      "epoch:7 step:6833 [D loss: 0.217617, acc.: 64.84%] [G loss: 0.465679]\n",
      "epoch:7 step:6834 [D loss: 0.221412, acc.: 64.06%] [G loss: 0.432432]\n",
      "epoch:7 step:6835 [D loss: 0.194762, acc.: 68.75%] [G loss: 0.506850]\n",
      "epoch:7 step:6836 [D loss: 0.241409, acc.: 57.81%] [G loss: 0.490445]\n",
      "epoch:7 step:6837 [D loss: 0.231428, acc.: 63.28%] [G loss: 0.481989]\n",
      "epoch:7 step:6838 [D loss: 0.215276, acc.: 66.41%] [G loss: 0.507869]\n",
      "epoch:7 step:6839 [D loss: 0.213363, acc.: 64.06%] [G loss: 0.490685]\n",
      "epoch:7 step:6840 [D loss: 0.242750, acc.: 62.50%] [G loss: 0.461830]\n",
      "epoch:7 step:6841 [D loss: 0.227037, acc.: 63.28%] [G loss: 0.452008]\n",
      "epoch:7 step:6842 [D loss: 0.177186, acc.: 76.56%] [G loss: 0.519594]\n",
      "epoch:7 step:6843 [D loss: 0.212756, acc.: 60.94%] [G loss: 0.475378]\n",
      "epoch:7 step:6844 [D loss: 0.216283, acc.: 69.53%] [G loss: 0.471200]\n",
      "epoch:7 step:6845 [D loss: 0.169802, acc.: 78.12%] [G loss: 0.534141]\n",
      "epoch:7 step:6846 [D loss: 0.227800, acc.: 63.28%] [G loss: 0.483924]\n",
      "epoch:7 step:6847 [D loss: 0.219303, acc.: 65.62%] [G loss: 0.491014]\n",
      "epoch:7 step:6848 [D loss: 0.206161, acc.: 69.53%] [G loss: 0.489066]\n",
      "epoch:7 step:6849 [D loss: 0.202214, acc.: 68.75%] [G loss: 0.504078]\n",
      "epoch:7 step:6850 [D loss: 0.223500, acc.: 65.62%] [G loss: 0.475327]\n",
      "epoch:7 step:6851 [D loss: 0.210775, acc.: 64.06%] [G loss: 0.514381]\n",
      "epoch:7 step:6852 [D loss: 0.224068, acc.: 65.62%] [G loss: 0.485382]\n",
      "epoch:7 step:6853 [D loss: 0.215678, acc.: 61.72%] [G loss: 0.524544]\n",
      "epoch:7 step:6854 [D loss: 0.211586, acc.: 65.62%] [G loss: 0.481707]\n",
      "epoch:7 step:6855 [D loss: 0.166071, acc.: 74.22%] [G loss: 0.554398]\n",
      "epoch:7 step:6856 [D loss: 0.188416, acc.: 70.31%] [G loss: 0.542639]\n",
      "epoch:7 step:6857 [D loss: 0.169897, acc.: 75.00%] [G loss: 0.506555]\n",
      "epoch:7 step:6858 [D loss: 0.220592, acc.: 66.41%] [G loss: 0.499341]\n",
      "epoch:7 step:6859 [D loss: 0.201024, acc.: 71.88%] [G loss: 0.528232]\n",
      "epoch:7 step:6860 [D loss: 0.229643, acc.: 60.94%] [G loss: 0.511252]\n",
      "epoch:7 step:6861 [D loss: 0.216036, acc.: 61.72%] [G loss: 0.507162]\n",
      "epoch:7 step:6862 [D loss: 0.221099, acc.: 67.97%] [G loss: 0.496069]\n",
      "epoch:7 step:6863 [D loss: 0.202900, acc.: 65.62%] [G loss: 0.484268]\n",
      "epoch:7 step:6864 [D loss: 0.168325, acc.: 75.00%] [G loss: 0.530372]\n",
      "epoch:7 step:6865 [D loss: 0.194669, acc.: 72.66%] [G loss: 0.511176]\n",
      "epoch:7 step:6866 [D loss: 0.205206, acc.: 69.53%] [G loss: 0.504909]\n",
      "epoch:7 step:6867 [D loss: 0.222934, acc.: 68.75%] [G loss: 0.506284]\n",
      "epoch:7 step:6868 [D loss: 0.188901, acc.: 73.44%] [G loss: 0.485465]\n",
      "epoch:7 step:6869 [D loss: 0.225674, acc.: 60.94%] [G loss: 0.551003]\n",
      "epoch:7 step:6870 [D loss: 0.190647, acc.: 71.09%] [G loss: 0.549667]\n",
      "epoch:7 step:6871 [D loss: 0.188394, acc.: 71.09%] [G loss: 0.545291]\n",
      "epoch:7 step:6872 [D loss: 0.186140, acc.: 69.53%] [G loss: 0.591924]\n",
      "epoch:7 step:6873 [D loss: 0.176595, acc.: 71.88%] [G loss: 0.573664]\n",
      "epoch:7 step:6874 [D loss: 0.186431, acc.: 75.00%] [G loss: 0.576451]\n",
      "epoch:7 step:6875 [D loss: 0.279140, acc.: 58.59%] [G loss: 0.466132]\n",
      "epoch:7 step:6876 [D loss: 0.231522, acc.: 64.06%] [G loss: 0.428254]\n",
      "epoch:7 step:6877 [D loss: 0.184770, acc.: 74.22%] [G loss: 0.478463]\n",
      "epoch:7 step:6878 [D loss: 0.209715, acc.: 63.28%] [G loss: 0.514517]\n",
      "epoch:7 step:6879 [D loss: 0.175431, acc.: 75.00%] [G loss: 0.542453]\n",
      "epoch:7 step:6880 [D loss: 0.180633, acc.: 70.31%] [G loss: 0.533364]\n",
      "epoch:7 step:6881 [D loss: 0.236327, acc.: 56.25%] [G loss: 0.522793]\n",
      "epoch:7 step:6882 [D loss: 0.243143, acc.: 57.81%] [G loss: 0.489751]\n",
      "epoch:7 step:6883 [D loss: 0.206294, acc.: 67.19%] [G loss: 0.521522]\n",
      "epoch:7 step:6884 [D loss: 0.199651, acc.: 64.06%] [G loss: 0.515059]\n",
      "epoch:7 step:6885 [D loss: 0.192039, acc.: 69.53%] [G loss: 0.511731]\n",
      "epoch:7 step:6886 [D loss: 0.235310, acc.: 57.81%] [G loss: 0.472242]\n",
      "epoch:7 step:6887 [D loss: 0.198828, acc.: 71.09%] [G loss: 0.580752]\n",
      "epoch:7 step:6888 [D loss: 0.228821, acc.: 60.94%] [G loss: 0.546220]\n",
      "epoch:7 step:6889 [D loss: 0.222577, acc.: 61.72%] [G loss: 0.531763]\n",
      "epoch:7 step:6890 [D loss: 0.221925, acc.: 62.50%] [G loss: 0.485930]\n",
      "epoch:7 step:6891 [D loss: 0.213242, acc.: 64.84%] [G loss: 0.485340]\n",
      "epoch:7 step:6892 [D loss: 0.192467, acc.: 69.53%] [G loss: 0.533735]\n",
      "epoch:7 step:6893 [D loss: 0.216921, acc.: 65.62%] [G loss: 0.498896]\n",
      "epoch:7 step:6894 [D loss: 0.204719, acc.: 68.75%] [G loss: 0.479275]\n",
      "epoch:7 step:6895 [D loss: 0.191528, acc.: 67.97%] [G loss: 0.548884]\n",
      "epoch:7 step:6896 [D loss: 0.198649, acc.: 71.09%] [G loss: 0.514609]\n",
      "epoch:7 step:6897 [D loss: 0.210226, acc.: 72.66%] [G loss: 0.479124]\n",
      "epoch:7 step:6898 [D loss: 0.185498, acc.: 75.00%] [G loss: 0.536676]\n",
      "epoch:7 step:6899 [D loss: 0.196282, acc.: 72.66%] [G loss: 0.527722]\n",
      "epoch:7 step:6900 [D loss: 0.247692, acc.: 63.28%] [G loss: 0.499463]\n",
      "epoch:7 step:6901 [D loss: 0.212651, acc.: 67.97%] [G loss: 0.514747]\n",
      "epoch:7 step:6902 [D loss: 0.166148, acc.: 74.22%] [G loss: 0.550414]\n",
      "epoch:7 step:6903 [D loss: 0.182123, acc.: 74.22%] [G loss: 0.548315]\n",
      "epoch:7 step:6904 [D loss: 0.228142, acc.: 61.72%] [G loss: 0.569884]\n",
      "epoch:7 step:6905 [D loss: 0.166613, acc.: 75.00%] [G loss: 0.544354]\n",
      "epoch:7 step:6906 [D loss: 0.182068, acc.: 70.31%] [G loss: 0.578806]\n",
      "epoch:7 step:6907 [D loss: 0.292219, acc.: 57.81%] [G loss: 0.488417]\n",
      "epoch:7 step:6908 [D loss: 0.268417, acc.: 53.91%] [G loss: 0.423223]\n",
      "epoch:7 step:6909 [D loss: 0.203716, acc.: 70.31%] [G loss: 0.504813]\n",
      "epoch:7 step:6910 [D loss: 0.203910, acc.: 67.19%] [G loss: 0.524651]\n",
      "epoch:7 step:6911 [D loss: 0.232225, acc.: 61.72%] [G loss: 0.461334]\n",
      "epoch:7 step:6912 [D loss: 0.193633, acc.: 75.78%] [G loss: 0.479731]\n",
      "epoch:7 step:6913 [D loss: 0.179929, acc.: 71.09%] [G loss: 0.529633]\n",
      "epoch:7 step:6914 [D loss: 0.234273, acc.: 69.53%] [G loss: 0.534959]\n",
      "epoch:7 step:6915 [D loss: 0.251303, acc.: 63.28%] [G loss: 0.499537]\n",
      "epoch:7 step:6916 [D loss: 0.201413, acc.: 67.97%] [G loss: 0.468156]\n",
      "epoch:7 step:6917 [D loss: 0.185918, acc.: 71.88%] [G loss: 0.526532]\n",
      "epoch:7 step:6918 [D loss: 0.216323, acc.: 64.84%] [G loss: 0.519500]\n",
      "epoch:7 step:6919 [D loss: 0.213538, acc.: 63.28%] [G loss: 0.524742]\n",
      "epoch:7 step:6920 [D loss: 0.218833, acc.: 62.50%] [G loss: 0.484706]\n",
      "epoch:7 step:6921 [D loss: 0.251397, acc.: 57.81%] [G loss: 0.444438]\n",
      "epoch:7 step:6922 [D loss: 0.211399, acc.: 71.88%] [G loss: 0.492346]\n",
      "epoch:7 step:6923 [D loss: 0.201017, acc.: 68.75%] [G loss: 0.462162]\n",
      "epoch:7 step:6924 [D loss: 0.222633, acc.: 65.62%] [G loss: 0.475819]\n",
      "epoch:7 step:6925 [D loss: 0.217279, acc.: 62.50%] [G loss: 0.465318]\n",
      "epoch:7 step:6926 [D loss: 0.228709, acc.: 57.81%] [G loss: 0.513692]\n",
      "epoch:7 step:6927 [D loss: 0.223151, acc.: 66.41%] [G loss: 0.464341]\n",
      "epoch:7 step:6928 [D loss: 0.247841, acc.: 59.38%] [G loss: 0.485840]\n",
      "epoch:7 step:6929 [D loss: 0.212908, acc.: 67.19%] [G loss: 0.495213]\n",
      "epoch:7 step:6930 [D loss: 0.193395, acc.: 68.75%] [G loss: 0.536091]\n",
      "epoch:7 step:6931 [D loss: 0.212323, acc.: 66.41%] [G loss: 0.539025]\n",
      "epoch:7 step:6932 [D loss: 0.238592, acc.: 62.50%] [G loss: 0.507235]\n",
      "epoch:7 step:6933 [D loss: 0.192787, acc.: 71.09%] [G loss: 0.472554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6934 [D loss: 0.234770, acc.: 61.72%] [G loss: 0.486975]\n",
      "epoch:7 step:6935 [D loss: 0.236396, acc.: 65.62%] [G loss: 0.450795]\n",
      "epoch:7 step:6936 [D loss: 0.231892, acc.: 63.28%] [G loss: 0.449500]\n",
      "epoch:7 step:6937 [D loss: 0.232318, acc.: 60.16%] [G loss: 0.469941]\n",
      "epoch:7 step:6938 [D loss: 0.246088, acc.: 57.81%] [G loss: 0.486111]\n",
      "epoch:7 step:6939 [D loss: 0.237444, acc.: 59.38%] [G loss: 0.486794]\n",
      "epoch:7 step:6940 [D loss: 0.173526, acc.: 71.09%] [G loss: 0.536423]\n",
      "epoch:7 step:6941 [D loss: 0.213797, acc.: 67.97%] [G loss: 0.459558]\n",
      "epoch:7 step:6942 [D loss: 0.251674, acc.: 57.81%] [G loss: 0.476740]\n",
      "epoch:7 step:6943 [D loss: 0.223898, acc.: 61.72%] [G loss: 0.486229]\n",
      "epoch:7 step:6944 [D loss: 0.221473, acc.: 62.50%] [G loss: 0.466147]\n",
      "epoch:7 step:6945 [D loss: 0.232920, acc.: 60.16%] [G loss: 0.461260]\n",
      "epoch:7 step:6946 [D loss: 0.224056, acc.: 59.38%] [G loss: 0.460218]\n",
      "epoch:7 step:6947 [D loss: 0.227224, acc.: 63.28%] [G loss: 0.502777]\n",
      "epoch:7 step:6948 [D loss: 0.227468, acc.: 63.28%] [G loss: 0.465695]\n",
      "epoch:7 step:6949 [D loss: 0.219540, acc.: 66.41%] [G loss: 0.481638]\n",
      "epoch:7 step:6950 [D loss: 0.196918, acc.: 72.66%] [G loss: 0.478882]\n",
      "epoch:7 step:6951 [D loss: 0.211504, acc.: 69.53%] [G loss: 0.507176]\n",
      "epoch:7 step:6952 [D loss: 0.232184, acc.: 64.06%] [G loss: 0.519226]\n",
      "epoch:7 step:6953 [D loss: 0.221091, acc.: 63.28%] [G loss: 0.522866]\n",
      "epoch:7 step:6954 [D loss: 0.249559, acc.: 59.38%] [G loss: 0.469209]\n",
      "epoch:7 step:6955 [D loss: 0.238874, acc.: 59.38%] [G loss: 0.475605]\n",
      "epoch:7 step:6956 [D loss: 0.211506, acc.: 65.62%] [G loss: 0.481848]\n",
      "epoch:7 step:6957 [D loss: 0.189057, acc.: 68.75%] [G loss: 0.533195]\n",
      "epoch:7 step:6958 [D loss: 0.191884, acc.: 70.31%] [G loss: 0.504035]\n",
      "epoch:7 step:6959 [D loss: 0.253640, acc.: 53.91%] [G loss: 0.462012]\n",
      "epoch:7 step:6960 [D loss: 0.225621, acc.: 63.28%] [G loss: 0.447120]\n",
      "epoch:7 step:6961 [D loss: 0.236401, acc.: 64.84%] [G loss: 0.431855]\n",
      "epoch:7 step:6962 [D loss: 0.194540, acc.: 71.09%] [G loss: 0.511645]\n",
      "epoch:7 step:6963 [D loss: 0.237840, acc.: 57.03%] [G loss: 0.468374]\n",
      "epoch:7 step:6964 [D loss: 0.207445, acc.: 66.41%] [G loss: 0.501444]\n",
      "epoch:7 step:6965 [D loss: 0.167991, acc.: 75.00%] [G loss: 0.562224]\n",
      "epoch:7 step:6966 [D loss: 0.221744, acc.: 64.84%] [G loss: 0.542825]\n",
      "epoch:7 step:6967 [D loss: 0.250103, acc.: 58.59%] [G loss: 0.498214]\n",
      "epoch:7 step:6968 [D loss: 0.242303, acc.: 59.38%] [G loss: 0.501357]\n",
      "epoch:7 step:6969 [D loss: 0.220726, acc.: 65.62%] [G loss: 0.453692]\n",
      "epoch:7 step:6970 [D loss: 0.217049, acc.: 64.84%] [G loss: 0.461662]\n",
      "epoch:7 step:6971 [D loss: 0.229674, acc.: 61.72%] [G loss: 0.459742]\n",
      "epoch:7 step:6972 [D loss: 0.232722, acc.: 60.94%] [G loss: 0.475004]\n",
      "epoch:7 step:6973 [D loss: 0.225074, acc.: 63.28%] [G loss: 0.509706]\n",
      "epoch:7 step:6974 [D loss: 0.220209, acc.: 67.19%] [G loss: 0.470948]\n",
      "epoch:7 step:6975 [D loss: 0.214330, acc.: 66.41%] [G loss: 0.494629]\n",
      "epoch:7 step:6976 [D loss: 0.233336, acc.: 58.59%] [G loss: 0.519531]\n",
      "epoch:7 step:6977 [D loss: 0.228939, acc.: 63.28%] [G loss: 0.475965]\n",
      "epoch:7 step:6978 [D loss: 0.226884, acc.: 67.97%] [G loss: 0.430845]\n",
      "epoch:7 step:6979 [D loss: 0.209976, acc.: 64.84%] [G loss: 0.465054]\n",
      "epoch:7 step:6980 [D loss: 0.248840, acc.: 60.16%] [G loss: 0.499661]\n",
      "epoch:7 step:6981 [D loss: 0.226553, acc.: 59.38%] [G loss: 0.477178]\n",
      "epoch:7 step:6982 [D loss: 0.219114, acc.: 63.28%] [G loss: 0.466586]\n",
      "epoch:7 step:6983 [D loss: 0.229852, acc.: 66.41%] [G loss: 0.470348]\n",
      "epoch:7 step:6984 [D loss: 0.190306, acc.: 71.88%] [G loss: 0.485799]\n",
      "epoch:7 step:6985 [D loss: 0.188504, acc.: 76.56%] [G loss: 0.541927]\n",
      "epoch:7 step:6986 [D loss: 0.192909, acc.: 73.44%] [G loss: 0.504702]\n",
      "epoch:7 step:6987 [D loss: 0.185174, acc.: 75.78%] [G loss: 0.566816]\n",
      "epoch:7 step:6988 [D loss: 0.205597, acc.: 71.88%] [G loss: 0.557572]\n",
      "epoch:7 step:6989 [D loss: 0.197223, acc.: 71.88%] [G loss: 0.551763]\n",
      "epoch:7 step:6990 [D loss: 0.242495, acc.: 60.16%] [G loss: 0.459646]\n",
      "epoch:7 step:6991 [D loss: 0.238079, acc.: 60.94%] [G loss: 0.486923]\n",
      "epoch:7 step:6992 [D loss: 0.217288, acc.: 63.28%] [G loss: 0.480337]\n",
      "epoch:7 step:6993 [D loss: 0.192702, acc.: 70.31%] [G loss: 0.519943]\n",
      "epoch:7 step:6994 [D loss: 0.211946, acc.: 67.97%] [G loss: 0.488336]\n",
      "epoch:7 step:6995 [D loss: 0.213870, acc.: 64.06%] [G loss: 0.498054]\n",
      "epoch:7 step:6996 [D loss: 0.244530, acc.: 61.72%] [G loss: 0.471704]\n",
      "epoch:7 step:6997 [D loss: 0.210675, acc.: 64.06%] [G loss: 0.471749]\n",
      "epoch:7 step:6998 [D loss: 0.181476, acc.: 71.09%] [G loss: 0.483011]\n",
      "epoch:7 step:6999 [D loss: 0.220898, acc.: 66.41%] [G loss: 0.505330]\n",
      "epoch:7 step:7000 [D loss: 0.236777, acc.: 57.03%] [G loss: 0.516506]\n",
      "##############\n",
      "[2.65172352 1.46138167 6.307927   4.75784028 3.85488304 5.75018351\n",
      " 4.49963429 4.84741519 4.72766514 3.70512458]\n",
      "##########\n",
      "epoch:7 step:7001 [D loss: 0.216701, acc.: 64.84%] [G loss: 0.545996]\n",
      "epoch:7 step:7002 [D loss: 0.221885, acc.: 66.41%] [G loss: 0.463318]\n",
      "epoch:7 step:7003 [D loss: 0.219808, acc.: 61.72%] [G loss: 0.504967]\n",
      "epoch:7 step:7004 [D loss: 0.216848, acc.: 64.84%] [G loss: 0.496466]\n",
      "epoch:7 step:7005 [D loss: 0.211345, acc.: 63.28%] [G loss: 0.494408]\n",
      "epoch:7 step:7006 [D loss: 0.194163, acc.: 72.66%] [G loss: 0.521876]\n",
      "epoch:7 step:7007 [D loss: 0.227673, acc.: 62.50%] [G loss: 0.455839]\n",
      "epoch:7 step:7008 [D loss: 0.218095, acc.: 67.97%] [G loss: 0.496988]\n",
      "epoch:7 step:7009 [D loss: 0.206189, acc.: 70.31%] [G loss: 0.517682]\n",
      "epoch:7 step:7010 [D loss: 0.177257, acc.: 72.66%] [G loss: 0.567624]\n",
      "epoch:7 step:7011 [D loss: 0.219203, acc.: 67.19%] [G loss: 0.511459]\n",
      "epoch:7 step:7012 [D loss: 0.194799, acc.: 67.97%] [G loss: 0.545609]\n",
      "epoch:7 step:7013 [D loss: 0.230603, acc.: 64.84%] [G loss: 0.500220]\n",
      "epoch:7 step:7014 [D loss: 0.256403, acc.: 57.81%] [G loss: 0.451873]\n",
      "epoch:7 step:7015 [D loss: 0.252200, acc.: 59.38%] [G loss: 0.459167]\n",
      "epoch:7 step:7016 [D loss: 0.220827, acc.: 67.19%] [G loss: 0.522373]\n",
      "epoch:7 step:7017 [D loss: 0.253460, acc.: 53.12%] [G loss: 0.460787]\n",
      "epoch:7 step:7018 [D loss: 0.233681, acc.: 54.69%] [G loss: 0.468499]\n",
      "epoch:7 step:7019 [D loss: 0.213312, acc.: 67.97%] [G loss: 0.490206]\n",
      "epoch:7 step:7020 [D loss: 0.212522, acc.: 67.19%] [G loss: 0.487586]\n",
      "epoch:7 step:7021 [D loss: 0.227311, acc.: 62.50%] [G loss: 0.479234]\n",
      "epoch:7 step:7022 [D loss: 0.206110, acc.: 69.53%] [G loss: 0.472673]\n",
      "epoch:7 step:7023 [D loss: 0.257415, acc.: 56.25%] [G loss: 0.415120]\n",
      "epoch:7 step:7024 [D loss: 0.257236, acc.: 57.81%] [G loss: 0.471086]\n",
      "epoch:7 step:7025 [D loss: 0.223560, acc.: 67.19%] [G loss: 0.480714]\n",
      "epoch:7 step:7026 [D loss: 0.222566, acc.: 67.97%] [G loss: 0.479451]\n",
      "epoch:7 step:7027 [D loss: 0.237089, acc.: 60.16%] [G loss: 0.489643]\n",
      "epoch:7 step:7028 [D loss: 0.242772, acc.: 60.94%] [G loss: 0.474476]\n",
      "epoch:7 step:7029 [D loss: 0.203068, acc.: 70.31%] [G loss: 0.556219]\n",
      "epoch:7 step:7030 [D loss: 0.205810, acc.: 71.09%] [G loss: 0.568968]\n",
      "epoch:7 step:7031 [D loss: 0.216425, acc.: 67.19%] [G loss: 0.553359]\n",
      "epoch:7 step:7032 [D loss: 0.238544, acc.: 57.81%] [G loss: 0.527601]\n",
      "epoch:7 step:7033 [D loss: 0.201313, acc.: 70.31%] [G loss: 0.529432]\n",
      "epoch:7 step:7034 [D loss: 0.191757, acc.: 69.53%] [G loss: 0.516463]\n",
      "epoch:7 step:7035 [D loss: 0.268051, acc.: 55.47%] [G loss: 0.443097]\n",
      "epoch:7 step:7036 [D loss: 0.257412, acc.: 55.47%] [G loss: 0.451376]\n",
      "epoch:7 step:7037 [D loss: 0.225362, acc.: 62.50%] [G loss: 0.455043]\n",
      "epoch:7 step:7038 [D loss: 0.209568, acc.: 65.62%] [G loss: 0.487806]\n",
      "epoch:7 step:7039 [D loss: 0.228445, acc.: 61.72%] [G loss: 0.439069]\n",
      "epoch:7 step:7040 [D loss: 0.192711, acc.: 71.09%] [G loss: 0.506976]\n",
      "epoch:7 step:7041 [D loss: 0.259509, acc.: 54.69%] [G loss: 0.448517]\n",
      "epoch:7 step:7042 [D loss: 0.255853, acc.: 54.69%] [G loss: 0.433925]\n",
      "epoch:7 step:7043 [D loss: 0.222258, acc.: 62.50%] [G loss: 0.523120]\n",
      "epoch:7 step:7044 [D loss: 0.214518, acc.: 59.38%] [G loss: 0.539236]\n",
      "epoch:7 step:7045 [D loss: 0.242824, acc.: 61.72%] [G loss: 0.481648]\n",
      "epoch:7 step:7046 [D loss: 0.223865, acc.: 65.62%] [G loss: 0.466117]\n",
      "epoch:7 step:7047 [D loss: 0.194808, acc.: 65.62%] [G loss: 0.485423]\n",
      "epoch:7 step:7048 [D loss: 0.241825, acc.: 57.81%] [G loss: 0.444110]\n",
      "epoch:7 step:7049 [D loss: 0.216505, acc.: 70.31%] [G loss: 0.489588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7050 [D loss: 0.202353, acc.: 72.66%] [G loss: 0.501345]\n",
      "epoch:7 step:7051 [D loss: 0.266040, acc.: 58.59%] [G loss: 0.447999]\n",
      "epoch:7 step:7052 [D loss: 0.225292, acc.: 57.03%] [G loss: 0.460282]\n",
      "epoch:7 step:7053 [D loss: 0.213159, acc.: 68.75%] [G loss: 0.484125]\n",
      "epoch:7 step:7054 [D loss: 0.197359, acc.: 67.19%] [G loss: 0.547806]\n",
      "epoch:7 step:7055 [D loss: 0.210618, acc.: 68.75%] [G loss: 0.478081]\n",
      "epoch:7 step:7056 [D loss: 0.198649, acc.: 75.00%] [G loss: 0.499020]\n",
      "epoch:7 step:7057 [D loss: 0.190000, acc.: 78.12%] [G loss: 0.511316]\n",
      "epoch:7 step:7058 [D loss: 0.172045, acc.: 77.34%] [G loss: 0.537983]\n",
      "epoch:7 step:7059 [D loss: 0.274196, acc.: 47.66%] [G loss: 0.526501]\n",
      "epoch:7 step:7060 [D loss: 0.243156, acc.: 61.72%] [G loss: 0.478758]\n",
      "epoch:7 step:7061 [D loss: 0.225925, acc.: 66.41%] [G loss: 0.484010]\n",
      "epoch:7 step:7062 [D loss: 0.189896, acc.: 71.88%] [G loss: 0.491977]\n",
      "epoch:7 step:7063 [D loss: 0.167986, acc.: 77.34%] [G loss: 0.564219]\n",
      "epoch:7 step:7064 [D loss: 0.203970, acc.: 64.84%] [G loss: 0.521376]\n",
      "epoch:7 step:7065 [D loss: 0.265967, acc.: 53.91%] [G loss: 0.462798]\n",
      "epoch:7 step:7066 [D loss: 0.207753, acc.: 64.84%] [G loss: 0.481004]\n",
      "epoch:7 step:7067 [D loss: 0.189471, acc.: 67.19%] [G loss: 0.530814]\n",
      "epoch:7 step:7068 [D loss: 0.235584, acc.: 61.72%] [G loss: 0.497716]\n",
      "epoch:7 step:7069 [D loss: 0.251397, acc.: 53.91%] [G loss: 0.460346]\n",
      "epoch:7 step:7070 [D loss: 0.227495, acc.: 67.19%] [G loss: 0.446281]\n",
      "epoch:7 step:7071 [D loss: 0.206044, acc.: 67.97%] [G loss: 0.524299]\n",
      "epoch:7 step:7072 [D loss: 0.231919, acc.: 65.62%] [G loss: 0.538073]\n",
      "epoch:7 step:7073 [D loss: 0.211966, acc.: 65.62%] [G loss: 0.482845]\n",
      "epoch:7 step:7074 [D loss: 0.210753, acc.: 65.62%] [G loss: 0.544446]\n",
      "epoch:7 step:7075 [D loss: 0.195511, acc.: 74.22%] [G loss: 0.549517]\n",
      "epoch:7 step:7076 [D loss: 0.214461, acc.: 64.84%] [G loss: 0.475315]\n",
      "epoch:7 step:7077 [D loss: 0.234581, acc.: 57.03%] [G loss: 0.439098]\n",
      "epoch:7 step:7078 [D loss: 0.189810, acc.: 70.31%] [G loss: 0.516319]\n",
      "epoch:7 step:7079 [D loss: 0.189938, acc.: 71.09%] [G loss: 0.586777]\n",
      "epoch:7 step:7080 [D loss: 0.234809, acc.: 63.28%] [G loss: 0.478954]\n",
      "epoch:7 step:7081 [D loss: 0.212793, acc.: 64.84%] [G loss: 0.489451]\n",
      "epoch:7 step:7082 [D loss: 0.196097, acc.: 71.09%] [G loss: 0.534666]\n",
      "epoch:7 step:7083 [D loss: 0.215511, acc.: 60.94%] [G loss: 0.492560]\n",
      "epoch:7 step:7084 [D loss: 0.197321, acc.: 73.44%] [G loss: 0.487038]\n",
      "epoch:7 step:7085 [D loss: 0.238474, acc.: 61.72%] [G loss: 0.474304]\n",
      "epoch:7 step:7086 [D loss: 0.208959, acc.: 71.88%] [G loss: 0.484290]\n",
      "epoch:7 step:7087 [D loss: 0.285113, acc.: 50.78%] [G loss: 0.465344]\n",
      "epoch:7 step:7088 [D loss: 0.217217, acc.: 60.94%] [G loss: 0.495999]\n",
      "epoch:7 step:7089 [D loss: 0.191936, acc.: 70.31%] [G loss: 0.545628]\n",
      "epoch:7 step:7090 [D loss: 0.236562, acc.: 60.16%] [G loss: 0.469518]\n",
      "epoch:7 step:7091 [D loss: 0.202435, acc.: 69.53%] [G loss: 0.475655]\n",
      "epoch:7 step:7092 [D loss: 0.219778, acc.: 57.03%] [G loss: 0.492784]\n",
      "epoch:7 step:7093 [D loss: 0.193577, acc.: 71.09%] [G loss: 0.530003]\n",
      "epoch:7 step:7094 [D loss: 0.261659, acc.: 53.91%] [G loss: 0.504555]\n",
      "epoch:7 step:7095 [D loss: 0.217402, acc.: 60.16%] [G loss: 0.519618]\n",
      "epoch:7 step:7096 [D loss: 0.207879, acc.: 67.97%] [G loss: 0.459562]\n",
      "epoch:7 step:7097 [D loss: 0.278157, acc.: 53.12%] [G loss: 0.478669]\n",
      "epoch:7 step:7098 [D loss: 0.244504, acc.: 60.16%] [G loss: 0.495208]\n",
      "epoch:7 step:7099 [D loss: 0.223923, acc.: 62.50%] [G loss: 0.512485]\n",
      "epoch:7 step:7100 [D loss: 0.205424, acc.: 67.19%] [G loss: 0.465599]\n",
      "epoch:7 step:7101 [D loss: 0.257946, acc.: 53.91%] [G loss: 0.415539]\n",
      "epoch:7 step:7102 [D loss: 0.228422, acc.: 64.84%] [G loss: 0.464320]\n",
      "epoch:7 step:7103 [D loss: 0.216266, acc.: 69.53%] [G loss: 0.454566]\n",
      "epoch:7 step:7104 [D loss: 0.197756, acc.: 71.09%] [G loss: 0.473123]\n",
      "epoch:7 step:7105 [D loss: 0.226428, acc.: 63.28%] [G loss: 0.479097]\n",
      "epoch:7 step:7106 [D loss: 0.201960, acc.: 65.62%] [G loss: 0.534781]\n",
      "epoch:7 step:7107 [D loss: 0.209418, acc.: 66.41%] [G loss: 0.570882]\n",
      "epoch:7 step:7108 [D loss: 0.197481, acc.: 66.41%] [G loss: 0.516446]\n",
      "epoch:7 step:7109 [D loss: 0.199993, acc.: 69.53%] [G loss: 0.496296]\n",
      "epoch:7 step:7110 [D loss: 0.213169, acc.: 67.97%] [G loss: 0.507285]\n",
      "epoch:7 step:7111 [D loss: 0.188264, acc.: 70.31%] [G loss: 0.504124]\n",
      "epoch:7 step:7112 [D loss: 0.224263, acc.: 64.06%] [G loss: 0.464260]\n",
      "epoch:7 step:7113 [D loss: 0.208340, acc.: 66.41%] [G loss: 0.456716]\n",
      "epoch:7 step:7114 [D loss: 0.183083, acc.: 72.66%] [G loss: 0.553206]\n",
      "epoch:7 step:7115 [D loss: 0.220738, acc.: 72.66%] [G loss: 0.533876]\n",
      "epoch:7 step:7116 [D loss: 0.203061, acc.: 64.84%] [G loss: 0.494923]\n",
      "epoch:7 step:7117 [D loss: 0.207900, acc.: 62.50%] [G loss: 0.505866]\n",
      "epoch:7 step:7118 [D loss: 0.216423, acc.: 68.75%] [G loss: 0.492943]\n",
      "epoch:7 step:7119 [D loss: 0.224794, acc.: 58.59%] [G loss: 0.488423]\n",
      "epoch:7 step:7120 [D loss: 0.203064, acc.: 68.75%] [G loss: 0.547745]\n",
      "epoch:7 step:7121 [D loss: 0.233410, acc.: 60.16%] [G loss: 0.487611]\n",
      "epoch:7 step:7122 [D loss: 0.194244, acc.: 65.62%] [G loss: 0.521736]\n",
      "epoch:7 step:7123 [D loss: 0.181764, acc.: 71.09%] [G loss: 0.590358]\n",
      "epoch:7 step:7124 [D loss: 0.248543, acc.: 62.50%] [G loss: 0.488916]\n",
      "epoch:7 step:7125 [D loss: 0.272509, acc.: 60.16%] [G loss: 0.432618]\n",
      "epoch:7 step:7126 [D loss: 0.219539, acc.: 64.84%] [G loss: 0.468119]\n",
      "epoch:7 step:7127 [D loss: 0.187827, acc.: 78.12%] [G loss: 0.503888]\n",
      "epoch:7 step:7128 [D loss: 0.237588, acc.: 59.38%] [G loss: 0.479544]\n",
      "epoch:7 step:7129 [D loss: 0.215534, acc.: 64.06%] [G loss: 0.451752]\n",
      "epoch:7 step:7130 [D loss: 0.185718, acc.: 73.44%] [G loss: 0.492696]\n",
      "epoch:7 step:7131 [D loss: 0.197298, acc.: 73.44%] [G loss: 0.468907]\n",
      "epoch:7 step:7132 [D loss: 0.221048, acc.: 64.84%] [G loss: 0.470842]\n",
      "epoch:7 step:7133 [D loss: 0.170775, acc.: 71.88%] [G loss: 0.510522]\n",
      "epoch:7 step:7134 [D loss: 0.186094, acc.: 75.00%] [G loss: 0.522374]\n",
      "epoch:7 step:7135 [D loss: 0.242688, acc.: 57.81%] [G loss: 0.470127]\n",
      "epoch:7 step:7136 [D loss: 0.204567, acc.: 72.66%] [G loss: 0.489813]\n",
      "epoch:7 step:7137 [D loss: 0.234739, acc.: 64.84%] [G loss: 0.474261]\n",
      "epoch:7 step:7138 [D loss: 0.221706, acc.: 62.50%] [G loss: 0.503416]\n",
      "epoch:7 step:7139 [D loss: 0.226033, acc.: 63.28%] [G loss: 0.509340]\n",
      "epoch:7 step:7140 [D loss: 0.179609, acc.: 72.66%] [G loss: 0.494017]\n",
      "epoch:7 step:7141 [D loss: 0.193413, acc.: 67.19%] [G loss: 0.520585]\n",
      "epoch:7 step:7142 [D loss: 0.241663, acc.: 64.84%] [G loss: 0.429677]\n",
      "epoch:7 step:7143 [D loss: 0.239333, acc.: 58.59%] [G loss: 0.474457]\n",
      "epoch:7 step:7144 [D loss: 0.231847, acc.: 60.94%] [G loss: 0.480621]\n",
      "epoch:7 step:7145 [D loss: 0.233222, acc.: 57.81%] [G loss: 0.472111]\n",
      "epoch:7 step:7146 [D loss: 0.210832, acc.: 65.62%] [G loss: 0.472218]\n",
      "epoch:7 step:7147 [D loss: 0.213044, acc.: 66.41%] [G loss: 0.501697]\n",
      "epoch:7 step:7148 [D loss: 0.199004, acc.: 67.19%] [G loss: 0.540314]\n",
      "epoch:7 step:7149 [D loss: 0.243161, acc.: 62.50%] [G loss: 0.508700]\n",
      "epoch:7 step:7150 [D loss: 0.237543, acc.: 61.72%] [G loss: 0.497009]\n",
      "epoch:7 step:7151 [D loss: 0.177787, acc.: 75.78%] [G loss: 0.521973]\n",
      "epoch:7 step:7152 [D loss: 0.192100, acc.: 68.75%] [G loss: 0.500091]\n",
      "epoch:7 step:7153 [D loss: 0.218052, acc.: 64.06%] [G loss: 0.462815]\n",
      "epoch:7 step:7154 [D loss: 0.189543, acc.: 75.00%] [G loss: 0.517994]\n",
      "epoch:7 step:7155 [D loss: 0.241501, acc.: 62.50%] [G loss: 0.475157]\n",
      "epoch:7 step:7156 [D loss: 0.241870, acc.: 59.38%] [G loss: 0.481057]\n",
      "epoch:7 step:7157 [D loss: 0.181456, acc.: 78.12%] [G loss: 0.520308]\n",
      "epoch:7 step:7158 [D loss: 0.248947, acc.: 53.91%] [G loss: 0.468194]\n",
      "epoch:7 step:7159 [D loss: 0.265304, acc.: 54.69%] [G loss: 0.465040]\n",
      "epoch:7 step:7160 [D loss: 0.242029, acc.: 61.72%] [G loss: 0.471918]\n",
      "epoch:7 step:7161 [D loss: 0.212377, acc.: 64.84%] [G loss: 0.498581]\n",
      "epoch:7 step:7162 [D loss: 0.217283, acc.: 67.97%] [G loss: 0.474227]\n",
      "epoch:7 step:7163 [D loss: 0.211159, acc.: 64.06%] [G loss: 0.510245]\n",
      "epoch:7 step:7164 [D loss: 0.188089, acc.: 71.88%] [G loss: 0.462377]\n",
      "epoch:7 step:7165 [D loss: 0.232264, acc.: 59.38%] [G loss: 0.473604]\n",
      "epoch:7 step:7166 [D loss: 0.212789, acc.: 64.06%] [G loss: 0.497621]\n",
      "epoch:7 step:7167 [D loss: 0.226865, acc.: 64.84%] [G loss: 0.454815]\n",
      "epoch:7 step:7168 [D loss: 0.197239, acc.: 68.75%] [G loss: 0.476033]\n",
      "epoch:7 step:7169 [D loss: 0.241513, acc.: 57.03%] [G loss: 0.477286]\n",
      "epoch:7 step:7170 [D loss: 0.203877, acc.: 67.97%] [G loss: 0.488599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7171 [D loss: 0.205358, acc.: 69.53%] [G loss: 0.488719]\n",
      "epoch:7 step:7172 [D loss: 0.193928, acc.: 68.75%] [G loss: 0.467416]\n",
      "epoch:7 step:7173 [D loss: 0.223690, acc.: 57.81%] [G loss: 0.490359]\n",
      "epoch:7 step:7174 [D loss: 0.245060, acc.: 50.00%] [G loss: 0.459910]\n",
      "epoch:7 step:7175 [D loss: 0.242696, acc.: 63.28%] [G loss: 0.479722]\n",
      "epoch:7 step:7176 [D loss: 0.200840, acc.: 72.66%] [G loss: 0.454839]\n",
      "epoch:7 step:7177 [D loss: 0.236381, acc.: 60.94%] [G loss: 0.479675]\n",
      "epoch:7 step:7178 [D loss: 0.197894, acc.: 69.53%] [G loss: 0.455543]\n",
      "epoch:7 step:7179 [D loss: 0.211957, acc.: 66.41%] [G loss: 0.488806]\n",
      "epoch:7 step:7180 [D loss: 0.232186, acc.: 60.16%] [G loss: 0.464384]\n",
      "epoch:7 step:7181 [D loss: 0.248734, acc.: 57.03%] [G loss: 0.464801]\n",
      "epoch:7 step:7182 [D loss: 0.181036, acc.: 74.22%] [G loss: 0.451273]\n",
      "epoch:7 step:7183 [D loss: 0.179450, acc.: 74.22%] [G loss: 0.492292]\n",
      "epoch:7 step:7184 [D loss: 0.239210, acc.: 66.41%] [G loss: 0.473732]\n",
      "epoch:7 step:7185 [D loss: 0.225951, acc.: 61.72%] [G loss: 0.472906]\n",
      "epoch:7 step:7186 [D loss: 0.227818, acc.: 67.97%] [G loss: 0.493550]\n",
      "epoch:7 step:7187 [D loss: 0.246795, acc.: 62.50%] [G loss: 0.474666]\n",
      "epoch:7 step:7188 [D loss: 0.238316, acc.: 60.94%] [G loss: 0.477482]\n",
      "epoch:7 step:7189 [D loss: 0.215791, acc.: 65.62%] [G loss: 0.488348]\n",
      "epoch:7 step:7190 [D loss: 0.200383, acc.: 70.31%] [G loss: 0.478588]\n",
      "epoch:7 step:7191 [D loss: 0.212024, acc.: 66.41%] [G loss: 0.496804]\n",
      "epoch:7 step:7192 [D loss: 0.202672, acc.: 67.97%] [G loss: 0.519380]\n",
      "epoch:7 step:7193 [D loss: 0.200331, acc.: 70.31%] [G loss: 0.500632]\n",
      "epoch:7 step:7194 [D loss: 0.191611, acc.: 71.09%] [G loss: 0.534160]\n",
      "epoch:7 step:7195 [D loss: 0.232438, acc.: 62.50%] [G loss: 0.492333]\n",
      "epoch:7 step:7196 [D loss: 0.211654, acc.: 67.97%] [G loss: 0.497015]\n",
      "epoch:7 step:7197 [D loss: 0.225809, acc.: 60.16%] [G loss: 0.414602]\n",
      "epoch:7 step:7198 [D loss: 0.205669, acc.: 67.19%] [G loss: 0.513725]\n",
      "epoch:7 step:7199 [D loss: 0.210004, acc.: 65.62%] [G loss: 0.488553]\n",
      "epoch:7 step:7200 [D loss: 0.209700, acc.: 61.72%] [G loss: 0.504582]\n",
      "##############\n",
      "[2.48800801 1.61010761 6.15529267 4.73619959 3.69493008 5.61673095\n",
      " 4.46491359 4.59449824 4.53973721 3.66577765]\n",
      "##########\n",
      "epoch:7 step:7201 [D loss: 0.172148, acc.: 75.78%] [G loss: 0.542555]\n",
      "epoch:7 step:7202 [D loss: 0.191975, acc.: 71.88%] [G loss: 0.553484]\n",
      "epoch:7 step:7203 [D loss: 0.224445, acc.: 67.97%] [G loss: 0.467412]\n",
      "epoch:7 step:7204 [D loss: 0.219861, acc.: 65.62%] [G loss: 0.494712]\n",
      "epoch:7 step:7205 [D loss: 0.186929, acc.: 71.09%] [G loss: 0.520079]\n",
      "epoch:7 step:7206 [D loss: 0.197128, acc.: 67.19%] [G loss: 0.519738]\n",
      "epoch:7 step:7207 [D loss: 0.164816, acc.: 78.12%] [G loss: 0.578897]\n",
      "epoch:7 step:7208 [D loss: 0.188938, acc.: 73.44%] [G loss: 0.540834]\n",
      "epoch:7 step:7209 [D loss: 0.201096, acc.: 71.88%] [G loss: 0.500894]\n",
      "epoch:7 step:7210 [D loss: 0.229167, acc.: 60.94%] [G loss: 0.486561]\n",
      "epoch:7 step:7211 [D loss: 0.220931, acc.: 62.50%] [G loss: 0.478746]\n",
      "epoch:7 step:7212 [D loss: 0.237195, acc.: 63.28%] [G loss: 0.461347]\n",
      "epoch:7 step:7213 [D loss: 0.226863, acc.: 64.06%] [G loss: 0.475410]\n",
      "epoch:7 step:7214 [D loss: 0.232455, acc.: 68.75%] [G loss: 0.492059]\n",
      "epoch:7 step:7215 [D loss: 0.202247, acc.: 67.19%] [G loss: 0.484853]\n",
      "epoch:7 step:7216 [D loss: 0.219426, acc.: 64.84%] [G loss: 0.483789]\n",
      "epoch:7 step:7217 [D loss: 0.219529, acc.: 62.50%] [G loss: 0.536075]\n",
      "epoch:7 step:7218 [D loss: 0.218315, acc.: 63.28%] [G loss: 0.491478]\n",
      "epoch:7 step:7219 [D loss: 0.215336, acc.: 68.75%] [G loss: 0.511983]\n",
      "epoch:7 step:7220 [D loss: 0.199397, acc.: 70.31%] [G loss: 0.507229]\n",
      "epoch:7 step:7221 [D loss: 0.214228, acc.: 66.41%] [G loss: 0.497125]\n",
      "epoch:7 step:7222 [D loss: 0.212242, acc.: 67.97%] [G loss: 0.454704]\n",
      "epoch:7 step:7223 [D loss: 0.252741, acc.: 58.59%] [G loss: 0.456500]\n",
      "epoch:7 step:7224 [D loss: 0.211916, acc.: 66.41%] [G loss: 0.561482]\n",
      "epoch:7 step:7225 [D loss: 0.224533, acc.: 65.62%] [G loss: 0.475980]\n",
      "epoch:7 step:7226 [D loss: 0.263387, acc.: 54.69%] [G loss: 0.459033]\n",
      "epoch:7 step:7227 [D loss: 0.243105, acc.: 58.59%] [G loss: 0.451893]\n",
      "epoch:7 step:7228 [D loss: 0.225867, acc.: 66.41%] [G loss: 0.505979]\n",
      "epoch:7 step:7229 [D loss: 0.233064, acc.: 63.28%] [G loss: 0.497581]\n",
      "epoch:7 step:7230 [D loss: 0.226177, acc.: 60.94%] [G loss: 0.456986]\n",
      "epoch:7 step:7231 [D loss: 0.240110, acc.: 60.94%] [G loss: 0.471559]\n",
      "epoch:7 step:7232 [D loss: 0.219214, acc.: 67.19%] [G loss: 0.467282]\n",
      "epoch:7 step:7233 [D loss: 0.203919, acc.: 64.06%] [G loss: 0.489214]\n",
      "epoch:7 step:7234 [D loss: 0.213447, acc.: 66.41%] [G loss: 0.508194]\n",
      "epoch:7 step:7235 [D loss: 0.245177, acc.: 55.47%] [G loss: 0.468737]\n",
      "epoch:7 step:7236 [D loss: 0.234617, acc.: 64.06%] [G loss: 0.512370]\n",
      "epoch:7 step:7237 [D loss: 0.203747, acc.: 69.53%] [G loss: 0.489717]\n",
      "epoch:7 step:7238 [D loss: 0.223114, acc.: 64.06%] [G loss: 0.509012]\n",
      "epoch:7 step:7239 [D loss: 0.250265, acc.: 56.25%] [G loss: 0.443288]\n",
      "epoch:7 step:7240 [D loss: 0.205887, acc.: 69.53%] [G loss: 0.468589]\n",
      "epoch:7 step:7241 [D loss: 0.224126, acc.: 58.59%] [G loss: 0.449094]\n",
      "epoch:7 step:7242 [D loss: 0.247309, acc.: 59.38%] [G loss: 0.440749]\n",
      "epoch:7 step:7243 [D loss: 0.222726, acc.: 62.50%] [G loss: 0.484335]\n",
      "epoch:7 step:7244 [D loss: 0.211408, acc.: 66.41%] [G loss: 0.483267]\n",
      "epoch:7 step:7245 [D loss: 0.235374, acc.: 57.03%] [G loss: 0.413720]\n",
      "epoch:7 step:7246 [D loss: 0.216040, acc.: 65.62%] [G loss: 0.453259]\n",
      "epoch:7 step:7247 [D loss: 0.207142, acc.: 69.53%] [G loss: 0.473629]\n",
      "epoch:7 step:7248 [D loss: 0.219347, acc.: 64.84%] [G loss: 0.505655]\n",
      "epoch:7 step:7249 [D loss: 0.168532, acc.: 77.34%] [G loss: 0.528153]\n",
      "epoch:7 step:7250 [D loss: 0.207225, acc.: 66.41%] [G loss: 0.559795]\n",
      "epoch:7 step:7251 [D loss: 0.196765, acc.: 68.75%] [G loss: 0.554045]\n",
      "epoch:7 step:7252 [D loss: 0.197608, acc.: 72.66%] [G loss: 0.539747]\n",
      "epoch:7 step:7253 [D loss: 0.191609, acc.: 67.19%] [G loss: 0.529530]\n",
      "epoch:7 step:7254 [D loss: 0.231716, acc.: 63.28%] [G loss: 0.528986]\n",
      "epoch:7 step:7255 [D loss: 0.236017, acc.: 57.03%] [G loss: 0.458616]\n",
      "epoch:7 step:7256 [D loss: 0.225781, acc.: 60.94%] [G loss: 0.478181]\n",
      "epoch:7 step:7257 [D loss: 0.226934, acc.: 62.50%] [G loss: 0.511131]\n",
      "epoch:7 step:7258 [D loss: 0.197467, acc.: 69.53%] [G loss: 0.534391]\n",
      "epoch:7 step:7259 [D loss: 0.222301, acc.: 65.62%] [G loss: 0.551681]\n",
      "epoch:7 step:7260 [D loss: 0.209203, acc.: 67.97%] [G loss: 0.548692]\n",
      "epoch:7 step:7261 [D loss: 0.231278, acc.: 60.94%] [G loss: 0.529204]\n",
      "epoch:7 step:7262 [D loss: 0.239830, acc.: 62.50%] [G loss: 0.481059]\n",
      "epoch:7 step:7263 [D loss: 0.271474, acc.: 54.69%] [G loss: 0.481350]\n",
      "epoch:7 step:7264 [D loss: 0.175071, acc.: 75.78%] [G loss: 0.507402]\n",
      "epoch:7 step:7265 [D loss: 0.243547, acc.: 62.50%] [G loss: 0.426594]\n",
      "epoch:7 step:7266 [D loss: 0.223511, acc.: 64.06%] [G loss: 0.482993]\n",
      "epoch:7 step:7267 [D loss: 0.183409, acc.: 72.66%] [G loss: 0.508162]\n",
      "epoch:7 step:7268 [D loss: 0.186906, acc.: 71.88%] [G loss: 0.541765]\n",
      "epoch:7 step:7269 [D loss: 0.253345, acc.: 58.59%] [G loss: 0.474264]\n",
      "epoch:7 step:7270 [D loss: 0.205988, acc.: 68.75%] [G loss: 0.497194]\n",
      "epoch:7 step:7271 [D loss: 0.203884, acc.: 66.41%] [G loss: 0.526036]\n",
      "epoch:7 step:7272 [D loss: 0.224085, acc.: 67.19%] [G loss: 0.470891]\n",
      "epoch:7 step:7273 [D loss: 0.233790, acc.: 57.81%] [G loss: 0.450991]\n",
      "epoch:7 step:7274 [D loss: 0.232781, acc.: 59.38%] [G loss: 0.493173]\n",
      "epoch:7 step:7275 [D loss: 0.229215, acc.: 65.62%] [G loss: 0.439624]\n",
      "epoch:7 step:7276 [D loss: 0.221490, acc.: 70.31%] [G loss: 0.452489]\n",
      "epoch:7 step:7277 [D loss: 0.208191, acc.: 69.53%] [G loss: 0.439905]\n",
      "epoch:7 step:7278 [D loss: 0.204293, acc.: 65.62%] [G loss: 0.497657]\n",
      "epoch:7 step:7279 [D loss: 0.231630, acc.: 61.72%] [G loss: 0.565448]\n",
      "epoch:7 step:7280 [D loss: 0.241703, acc.: 57.81%] [G loss: 0.512175]\n",
      "epoch:7 step:7281 [D loss: 0.251177, acc.: 53.91%] [G loss: 0.491860]\n",
      "epoch:7 step:7282 [D loss: 0.232426, acc.: 60.16%] [G loss: 0.451991]\n",
      "epoch:7 step:7283 [D loss: 0.209985, acc.: 66.41%] [G loss: 0.494375]\n",
      "epoch:7 step:7284 [D loss: 0.199602, acc.: 71.88%] [G loss: 0.533469]\n",
      "epoch:7 step:7285 [D loss: 0.231040, acc.: 61.72%] [G loss: 0.465181]\n",
      "epoch:7 step:7286 [D loss: 0.218844, acc.: 63.28%] [G loss: 0.485122]\n",
      "epoch:7 step:7287 [D loss: 0.224560, acc.: 60.94%] [G loss: 0.439846]\n",
      "epoch:7 step:7288 [D loss: 0.195606, acc.: 68.75%] [G loss: 0.461335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7289 [D loss: 0.207404, acc.: 67.97%] [G loss: 0.489849]\n",
      "epoch:7 step:7290 [D loss: 0.195967, acc.: 71.09%] [G loss: 0.516123]\n",
      "epoch:7 step:7291 [D loss: 0.198147, acc.: 72.66%] [G loss: 0.521056]\n",
      "epoch:7 step:7292 [D loss: 0.228377, acc.: 68.75%] [G loss: 0.479446]\n",
      "epoch:7 step:7293 [D loss: 0.235017, acc.: 63.28%] [G loss: 0.491954]\n",
      "epoch:7 step:7294 [D loss: 0.219212, acc.: 63.28%] [G loss: 0.465349]\n",
      "epoch:7 step:7295 [D loss: 0.211491, acc.: 68.75%] [G loss: 0.510375]\n",
      "epoch:7 step:7296 [D loss: 0.194026, acc.: 69.53%] [G loss: 0.493557]\n",
      "epoch:7 step:7297 [D loss: 0.246827, acc.: 53.91%] [G loss: 0.465372]\n",
      "epoch:7 step:7298 [D loss: 0.241669, acc.: 60.94%] [G loss: 0.421241]\n",
      "epoch:7 step:7299 [D loss: 0.232013, acc.: 57.81%] [G loss: 0.445043]\n",
      "epoch:7 step:7300 [D loss: 0.210363, acc.: 64.06%] [G loss: 0.482653]\n",
      "epoch:7 step:7301 [D loss: 0.209444, acc.: 67.19%] [G loss: 0.495971]\n",
      "epoch:7 step:7302 [D loss: 0.185596, acc.: 71.88%] [G loss: 0.519247]\n",
      "epoch:7 step:7303 [D loss: 0.229182, acc.: 59.38%] [G loss: 0.518620]\n",
      "epoch:7 step:7304 [D loss: 0.247935, acc.: 65.62%] [G loss: 0.477631]\n",
      "epoch:7 step:7305 [D loss: 0.217627, acc.: 63.28%] [G loss: 0.467339]\n",
      "epoch:7 step:7306 [D loss: 0.179325, acc.: 73.44%] [G loss: 0.498526]\n",
      "epoch:7 step:7307 [D loss: 0.229400, acc.: 60.16%] [G loss: 0.465032]\n",
      "epoch:7 step:7308 [D loss: 0.250659, acc.: 64.84%] [G loss: 0.458015]\n",
      "epoch:7 step:7309 [D loss: 0.232050, acc.: 57.81%] [G loss: 0.452232]\n",
      "epoch:7 step:7310 [D loss: 0.201947, acc.: 72.66%] [G loss: 0.479218]\n",
      "epoch:7 step:7311 [D loss: 0.222814, acc.: 65.62%] [G loss: 0.434813]\n",
      "epoch:7 step:7312 [D loss: 0.184924, acc.: 70.31%] [G loss: 0.439274]\n",
      "epoch:7 step:7313 [D loss: 0.199740, acc.: 67.97%] [G loss: 0.477713]\n",
      "epoch:7 step:7314 [D loss: 0.212778, acc.: 67.19%] [G loss: 0.486277]\n",
      "epoch:7 step:7315 [D loss: 0.228856, acc.: 65.62%] [G loss: 0.517325]\n",
      "epoch:7 step:7316 [D loss: 0.229299, acc.: 67.97%] [G loss: 0.491035]\n",
      "epoch:7 step:7317 [D loss: 0.235312, acc.: 60.16%] [G loss: 0.466290]\n",
      "epoch:7 step:7318 [D loss: 0.226542, acc.: 57.81%] [G loss: 0.450976]\n",
      "epoch:7 step:7319 [D loss: 0.213504, acc.: 66.41%] [G loss: 0.480089]\n",
      "epoch:7 step:7320 [D loss: 0.217313, acc.: 60.94%] [G loss: 0.512668]\n",
      "epoch:7 step:7321 [D loss: 0.231417, acc.: 64.84%] [G loss: 0.491914]\n",
      "epoch:7 step:7322 [D loss: 0.214158, acc.: 68.75%] [G loss: 0.479238]\n",
      "epoch:7 step:7323 [D loss: 0.228836, acc.: 58.59%] [G loss: 0.465583]\n",
      "epoch:7 step:7324 [D loss: 0.305521, acc.: 49.22%] [G loss: 0.442368]\n",
      "epoch:7 step:7325 [D loss: 0.263836, acc.: 48.44%] [G loss: 0.401866]\n",
      "epoch:7 step:7326 [D loss: 0.205303, acc.: 68.75%] [G loss: 0.479417]\n",
      "epoch:7 step:7327 [D loss: 0.218162, acc.: 67.19%] [G loss: 0.466079]\n",
      "epoch:7 step:7328 [D loss: 0.239191, acc.: 60.16%] [G loss: 0.454105]\n",
      "epoch:7 step:7329 [D loss: 0.222615, acc.: 60.16%] [G loss: 0.511941]\n",
      "epoch:7 step:7330 [D loss: 0.228009, acc.: 65.62%] [G loss: 0.438005]\n",
      "epoch:7 step:7331 [D loss: 0.225049, acc.: 62.50%] [G loss: 0.457952]\n",
      "epoch:7 step:7332 [D loss: 0.206166, acc.: 67.97%] [G loss: 0.470109]\n",
      "epoch:7 step:7333 [D loss: 0.211707, acc.: 71.09%] [G loss: 0.483621]\n",
      "epoch:7 step:7334 [D loss: 0.198829, acc.: 64.84%] [G loss: 0.522541]\n",
      "epoch:7 step:7335 [D loss: 0.234937, acc.: 56.25%] [G loss: 0.475670]\n",
      "epoch:7 step:7336 [D loss: 0.217818, acc.: 58.59%] [G loss: 0.445519]\n",
      "epoch:7 step:7337 [D loss: 0.223201, acc.: 60.94%] [G loss: 0.483424]\n",
      "epoch:7 step:7338 [D loss: 0.226059, acc.: 69.53%] [G loss: 0.464320]\n",
      "epoch:7 step:7339 [D loss: 0.201049, acc.: 67.19%] [G loss: 0.466380]\n",
      "epoch:7 step:7340 [D loss: 0.188120, acc.: 73.44%] [G loss: 0.611596]\n",
      "epoch:7 step:7341 [D loss: 0.165224, acc.: 76.56%] [G loss: 0.584844]\n",
      "epoch:7 step:7342 [D loss: 0.249230, acc.: 58.59%] [G loss: 0.466924]\n",
      "epoch:7 step:7343 [D loss: 0.258922, acc.: 55.47%] [G loss: 0.439691]\n",
      "epoch:7 step:7344 [D loss: 0.225868, acc.: 67.19%] [G loss: 0.466041]\n",
      "epoch:7 step:7345 [D loss: 0.169552, acc.: 73.44%] [G loss: 0.489976]\n",
      "epoch:7 step:7346 [D loss: 0.284333, acc.: 52.34%] [G loss: 0.416382]\n",
      "epoch:7 step:7347 [D loss: 0.265450, acc.: 55.47%] [G loss: 0.420861]\n",
      "epoch:7 step:7348 [D loss: 0.221152, acc.: 62.50%] [G loss: 0.482085]\n",
      "epoch:7 step:7349 [D loss: 0.185725, acc.: 72.66%] [G loss: 0.525644]\n",
      "epoch:7 step:7350 [D loss: 0.276023, acc.: 53.91%] [G loss: 0.472635]\n",
      "epoch:7 step:7351 [D loss: 0.174117, acc.: 76.56%] [G loss: 0.476976]\n",
      "epoch:7 step:7352 [D loss: 0.209382, acc.: 67.19%] [G loss: 0.464401]\n",
      "epoch:7 step:7353 [D loss: 0.254341, acc.: 60.16%] [G loss: 0.456991]\n",
      "epoch:7 step:7354 [D loss: 0.230965, acc.: 63.28%] [G loss: 0.467157]\n",
      "epoch:7 step:7355 [D loss: 0.217470, acc.: 65.62%] [G loss: 0.471569]\n",
      "epoch:7 step:7356 [D loss: 0.229902, acc.: 57.03%] [G loss: 0.490879]\n",
      "epoch:7 step:7357 [D loss: 0.202067, acc.: 68.75%] [G loss: 0.479436]\n",
      "epoch:7 step:7358 [D loss: 0.196866, acc.: 70.31%] [G loss: 0.493239]\n",
      "epoch:7 step:7359 [D loss: 0.253283, acc.: 57.03%] [G loss: 0.478170]\n",
      "epoch:7 step:7360 [D loss: 0.186174, acc.: 71.88%] [G loss: 0.499071]\n",
      "epoch:7 step:7361 [D loss: 0.217248, acc.: 68.75%] [G loss: 0.548481]\n",
      "epoch:7 step:7362 [D loss: 0.225638, acc.: 64.06%] [G loss: 0.539760]\n",
      "epoch:7 step:7363 [D loss: 0.230194, acc.: 58.59%] [G loss: 0.472640]\n",
      "epoch:7 step:7364 [D loss: 0.214457, acc.: 70.31%] [G loss: 0.442724]\n",
      "epoch:7 step:7365 [D loss: 0.188297, acc.: 70.31%] [G loss: 0.491783]\n",
      "epoch:7 step:7366 [D loss: 0.190637, acc.: 72.66%] [G loss: 0.509783]\n",
      "epoch:7 step:7367 [D loss: 0.236742, acc.: 60.16%] [G loss: 0.458530]\n",
      "epoch:7 step:7368 [D loss: 0.225091, acc.: 58.59%] [G loss: 0.492733]\n",
      "epoch:7 step:7369 [D loss: 0.213337, acc.: 67.97%] [G loss: 0.465144]\n",
      "epoch:7 step:7370 [D loss: 0.232907, acc.: 61.72%] [G loss: 0.483837]\n",
      "epoch:7 step:7371 [D loss: 0.263341, acc.: 58.59%] [G loss: 0.463707]\n",
      "epoch:7 step:7372 [D loss: 0.220609, acc.: 64.06%] [G loss: 0.474906]\n",
      "epoch:7 step:7373 [D loss: 0.226076, acc.: 64.06%] [G loss: 0.471931]\n",
      "epoch:7 step:7374 [D loss: 0.212129, acc.: 65.62%] [G loss: 0.507173]\n",
      "epoch:7 step:7375 [D loss: 0.223066, acc.: 64.84%] [G loss: 0.503870]\n",
      "epoch:7 step:7376 [D loss: 0.244886, acc.: 56.25%] [G loss: 0.514737]\n",
      "epoch:7 step:7377 [D loss: 0.233274, acc.: 64.06%] [G loss: 0.449320]\n",
      "epoch:7 step:7378 [D loss: 0.217922, acc.: 66.41%] [G loss: 0.463278]\n",
      "epoch:7 step:7379 [D loss: 0.238722, acc.: 57.81%] [G loss: 0.511448]\n",
      "epoch:7 step:7380 [D loss: 0.228325, acc.: 60.94%] [G loss: 0.438096]\n",
      "epoch:7 step:7381 [D loss: 0.183651, acc.: 71.09%] [G loss: 0.469173]\n",
      "epoch:7 step:7382 [D loss: 0.208339, acc.: 71.88%] [G loss: 0.468895]\n",
      "epoch:7 step:7383 [D loss: 0.252907, acc.: 53.91%] [G loss: 0.470546]\n",
      "epoch:7 step:7384 [D loss: 0.201350, acc.: 72.66%] [G loss: 0.498349]\n",
      "epoch:7 step:7385 [D loss: 0.221128, acc.: 65.62%] [G loss: 0.453385]\n",
      "epoch:7 step:7386 [D loss: 0.249427, acc.: 53.91%] [G loss: 0.447804]\n",
      "epoch:7 step:7387 [D loss: 0.237049, acc.: 54.69%] [G loss: 0.466114]\n",
      "epoch:7 step:7388 [D loss: 0.219670, acc.: 61.72%] [G loss: 0.486398]\n",
      "epoch:7 step:7389 [D loss: 0.214408, acc.: 72.66%] [G loss: 0.429880]\n",
      "epoch:7 step:7390 [D loss: 0.217978, acc.: 64.06%] [G loss: 0.474137]\n",
      "epoch:7 step:7391 [D loss: 0.195717, acc.: 71.09%] [G loss: 0.500460]\n",
      "epoch:7 step:7392 [D loss: 0.194400, acc.: 71.88%] [G loss: 0.470504]\n",
      "epoch:7 step:7393 [D loss: 0.221278, acc.: 64.84%] [G loss: 0.476260]\n",
      "epoch:7 step:7394 [D loss: 0.209398, acc.: 67.19%] [G loss: 0.460381]\n",
      "epoch:7 step:7395 [D loss: 0.203519, acc.: 69.53%] [G loss: 0.511784]\n",
      "epoch:7 step:7396 [D loss: 0.194725, acc.: 72.66%] [G loss: 0.509601]\n",
      "epoch:7 step:7397 [D loss: 0.200783, acc.: 70.31%] [G loss: 0.475258]\n",
      "epoch:7 step:7398 [D loss: 0.222742, acc.: 63.28%] [G loss: 0.473754]\n",
      "epoch:7 step:7399 [D loss: 0.221630, acc.: 60.94%] [G loss: 0.456688]\n",
      "epoch:7 step:7400 [D loss: 0.218745, acc.: 64.06%] [G loss: 0.438306]\n",
      "##############\n",
      "[2.53427421 1.59864192 6.21633259 4.93067795 3.76908945 5.57705775\n",
      " 4.60713972 4.36810002 4.58438023 3.65857561]\n",
      "##########\n",
      "epoch:7 step:7401 [D loss: 0.192552, acc.: 72.66%] [G loss: 0.471836]\n",
      "epoch:7 step:7402 [D loss: 0.221876, acc.: 67.19%] [G loss: 0.483272]\n",
      "epoch:7 step:7403 [D loss: 0.236264, acc.: 61.72%] [G loss: 0.510800]\n",
      "epoch:7 step:7404 [D loss: 0.201240, acc.: 71.88%] [G loss: 0.492328]\n",
      "epoch:7 step:7405 [D loss: 0.239781, acc.: 59.38%] [G loss: 0.445869]\n",
      "epoch:7 step:7406 [D loss: 0.244179, acc.: 57.03%] [G loss: 0.420024]\n",
      "epoch:7 step:7407 [D loss: 0.233992, acc.: 60.94%] [G loss: 0.432887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7408 [D loss: 0.190339, acc.: 74.22%] [G loss: 0.478552]\n",
      "epoch:7 step:7409 [D loss: 0.216840, acc.: 64.06%] [G loss: 0.497620]\n",
      "epoch:7 step:7410 [D loss: 0.229317, acc.: 65.62%] [G loss: 0.463089]\n",
      "epoch:7 step:7411 [D loss: 0.224654, acc.: 67.97%] [G loss: 0.462529]\n",
      "epoch:7 step:7412 [D loss: 0.218130, acc.: 61.72%] [G loss: 0.497547]\n",
      "epoch:7 step:7413 [D loss: 0.217972, acc.: 67.97%] [G loss: 0.525294]\n",
      "epoch:7 step:7414 [D loss: 0.237352, acc.: 59.38%] [G loss: 0.500578]\n",
      "epoch:7 step:7415 [D loss: 0.225943, acc.: 61.72%] [G loss: 0.440560]\n",
      "epoch:7 step:7416 [D loss: 0.218362, acc.: 60.94%] [G loss: 0.494227]\n",
      "epoch:7 step:7417 [D loss: 0.253091, acc.: 50.00%] [G loss: 0.462356]\n",
      "epoch:7 step:7418 [D loss: 0.260896, acc.: 55.47%] [G loss: 0.468493]\n",
      "epoch:7 step:7419 [D loss: 0.166214, acc.: 79.69%] [G loss: 0.520953]\n",
      "epoch:7 step:7420 [D loss: 0.250858, acc.: 57.81%] [G loss: 0.452883]\n",
      "epoch:7 step:7421 [D loss: 0.246815, acc.: 56.25%] [G loss: 0.421736]\n",
      "epoch:7 step:7422 [D loss: 0.227961, acc.: 67.19%] [G loss: 0.457204]\n",
      "epoch:7 step:7423 [D loss: 0.215204, acc.: 68.75%] [G loss: 0.471396]\n",
      "epoch:7 step:7424 [D loss: 0.238115, acc.: 53.91%] [G loss: 0.453087]\n",
      "epoch:7 step:7425 [D loss: 0.206982, acc.: 68.75%] [G loss: 0.457430]\n",
      "epoch:7 step:7426 [D loss: 0.254376, acc.: 57.03%] [G loss: 0.443657]\n",
      "epoch:7 step:7427 [D loss: 0.225849, acc.: 64.06%] [G loss: 0.470491]\n",
      "epoch:7 step:7428 [D loss: 0.237451, acc.: 61.72%] [G loss: 0.425182]\n",
      "epoch:7 step:7429 [D loss: 0.211339, acc.: 69.53%] [G loss: 0.493736]\n",
      "epoch:7 step:7430 [D loss: 0.199784, acc.: 68.75%] [G loss: 0.543348]\n",
      "epoch:7 step:7431 [D loss: 0.207322, acc.: 64.84%] [G loss: 0.504929]\n",
      "epoch:7 step:7432 [D loss: 0.219893, acc.: 63.28%] [G loss: 0.503810]\n",
      "epoch:7 step:7433 [D loss: 0.235810, acc.: 57.81%] [G loss: 0.472369]\n",
      "epoch:7 step:7434 [D loss: 0.201121, acc.: 68.75%] [G loss: 0.516532]\n",
      "epoch:7 step:7435 [D loss: 0.250554, acc.: 54.69%] [G loss: 0.518040]\n",
      "epoch:7 step:7436 [D loss: 0.224391, acc.: 60.94%] [G loss: 0.432541]\n",
      "epoch:7 step:7437 [D loss: 0.241663, acc.: 64.06%] [G loss: 0.419899]\n",
      "epoch:7 step:7438 [D loss: 0.228723, acc.: 60.16%] [G loss: 0.467233]\n",
      "epoch:7 step:7439 [D loss: 0.240065, acc.: 57.03%] [G loss: 0.465004]\n",
      "epoch:7 step:7440 [D loss: 0.213899, acc.: 66.41%] [G loss: 0.484033]\n",
      "epoch:7 step:7441 [D loss: 0.216158, acc.: 63.28%] [G loss: 0.449603]\n",
      "epoch:7 step:7442 [D loss: 0.231921, acc.: 63.28%] [G loss: 0.442720]\n",
      "epoch:7 step:7443 [D loss: 0.205400, acc.: 72.66%] [G loss: 0.527380]\n",
      "epoch:7 step:7444 [D loss: 0.228092, acc.: 58.59%] [G loss: 0.537067]\n",
      "epoch:7 step:7445 [D loss: 0.188502, acc.: 75.78%] [G loss: 0.512788]\n",
      "epoch:7 step:7446 [D loss: 0.232493, acc.: 60.16%] [G loss: 0.457138]\n",
      "epoch:7 step:7447 [D loss: 0.222375, acc.: 64.84%] [G loss: 0.514430]\n",
      "epoch:7 step:7448 [D loss: 0.199177, acc.: 69.53%] [G loss: 0.493533]\n",
      "epoch:7 step:7449 [D loss: 0.180889, acc.: 75.78%] [G loss: 0.504908]\n",
      "epoch:7 step:7450 [D loss: 0.259374, acc.: 53.12%] [G loss: 0.447094]\n",
      "epoch:7 step:7451 [D loss: 0.235311, acc.: 57.03%] [G loss: 0.435158]\n",
      "epoch:7 step:7452 [D loss: 0.179755, acc.: 74.22%] [G loss: 0.511395]\n",
      "epoch:7 step:7453 [D loss: 0.205095, acc.: 71.09%] [G loss: 0.515528]\n",
      "epoch:7 step:7454 [D loss: 0.218145, acc.: 63.28%] [G loss: 0.496459]\n",
      "epoch:7 step:7455 [D loss: 0.223336, acc.: 64.06%] [G loss: 0.491467]\n",
      "epoch:7 step:7456 [D loss: 0.187698, acc.: 72.66%] [G loss: 0.535016]\n",
      "epoch:7 step:7457 [D loss: 0.193955, acc.: 71.09%] [G loss: 0.487209]\n",
      "epoch:7 step:7458 [D loss: 0.182586, acc.: 71.88%] [G loss: 0.492566]\n",
      "epoch:7 step:7459 [D loss: 0.189370, acc.: 73.44%] [G loss: 0.502909]\n",
      "epoch:7 step:7460 [D loss: 0.207379, acc.: 69.53%] [G loss: 0.502943]\n",
      "epoch:7 step:7461 [D loss: 0.246280, acc.: 57.03%] [G loss: 0.469938]\n",
      "epoch:7 step:7462 [D loss: 0.209891, acc.: 64.84%] [G loss: 0.467373]\n",
      "epoch:7 step:7463 [D loss: 0.202956, acc.: 65.62%] [G loss: 0.492929]\n",
      "epoch:7 step:7464 [D loss: 0.226668, acc.: 66.41%] [G loss: 0.508463]\n",
      "epoch:7 step:7465 [D loss: 0.199250, acc.: 71.88%] [G loss: 0.526467]\n",
      "epoch:7 step:7466 [D loss: 0.215779, acc.: 65.62%] [G loss: 0.506110]\n",
      "epoch:7 step:7467 [D loss: 0.200797, acc.: 71.09%] [G loss: 0.459239]\n",
      "epoch:7 step:7468 [D loss: 0.202036, acc.: 71.09%] [G loss: 0.464785]\n",
      "epoch:7 step:7469 [D loss: 0.215863, acc.: 61.72%] [G loss: 0.455827]\n",
      "epoch:7 step:7470 [D loss: 0.188591, acc.: 69.53%] [G loss: 0.484955]\n",
      "epoch:7 step:7471 [D loss: 0.167462, acc.: 72.66%] [G loss: 0.521564]\n",
      "epoch:7 step:7472 [D loss: 0.242636, acc.: 59.38%] [G loss: 0.547189]\n",
      "epoch:7 step:7473 [D loss: 0.216683, acc.: 66.41%] [G loss: 0.522385]\n",
      "epoch:7 step:7474 [D loss: 0.258826, acc.: 58.59%] [G loss: 0.492785]\n",
      "epoch:7 step:7475 [D loss: 0.210579, acc.: 66.41%] [G loss: 0.506822]\n",
      "epoch:7 step:7476 [D loss: 0.277720, acc.: 49.22%] [G loss: 0.470040]\n",
      "epoch:7 step:7477 [D loss: 0.197677, acc.: 70.31%] [G loss: 0.492347]\n",
      "epoch:7 step:7478 [D loss: 0.171183, acc.: 75.00%] [G loss: 0.515145]\n",
      "epoch:7 step:7479 [D loss: 0.339507, acc.: 44.53%] [G loss: 0.417214]\n",
      "epoch:7 step:7480 [D loss: 0.224264, acc.: 61.72%] [G loss: 0.493764]\n",
      "epoch:7 step:7481 [D loss: 0.196908, acc.: 71.09%] [G loss: 0.501725]\n",
      "epoch:7 step:7482 [D loss: 0.180223, acc.: 75.78%] [G loss: 0.503561]\n",
      "epoch:7 step:7483 [D loss: 0.181937, acc.: 79.69%] [G loss: 0.531741]\n",
      "epoch:7 step:7484 [D loss: 0.142501, acc.: 82.03%] [G loss: 0.566084]\n",
      "epoch:7 step:7485 [D loss: 0.167735, acc.: 76.56%] [G loss: 0.567803]\n",
      "epoch:7 step:7486 [D loss: 0.183830, acc.: 70.31%] [G loss: 0.606854]\n",
      "epoch:7 step:7487 [D loss: 0.317008, acc.: 51.56%] [G loss: 0.594536]\n",
      "epoch:7 step:7488 [D loss: 0.209576, acc.: 69.53%] [G loss: 0.656190]\n",
      "epoch:7 step:7489 [D loss: 0.197646, acc.: 66.41%] [G loss: 0.557113]\n",
      "epoch:7 step:7490 [D loss: 0.282175, acc.: 63.28%] [G loss: 0.501719]\n",
      "epoch:7 step:7491 [D loss: 0.222671, acc.: 63.28%] [G loss: 0.482299]\n",
      "epoch:7 step:7492 [D loss: 0.217906, acc.: 64.84%] [G loss: 0.533792]\n",
      "epoch:7 step:7493 [D loss: 0.238120, acc.: 65.62%] [G loss: 0.479095]\n",
      "epoch:7 step:7494 [D loss: 0.193082, acc.: 70.31%] [G loss: 0.530239]\n",
      "epoch:7 step:7495 [D loss: 0.158048, acc.: 78.12%] [G loss: 0.580886]\n",
      "epoch:7 step:7496 [D loss: 0.150250, acc.: 81.25%] [G loss: 0.625195]\n",
      "epoch:8 step:7497 [D loss: 0.233237, acc.: 64.06%] [G loss: 0.555100]\n",
      "epoch:8 step:7498 [D loss: 0.269211, acc.: 58.59%] [G loss: 0.445205]\n",
      "epoch:8 step:7499 [D loss: 0.252468, acc.: 57.03%] [G loss: 0.501163]\n",
      "epoch:8 step:7500 [D loss: 0.233068, acc.: 62.50%] [G loss: 0.496998]\n",
      "epoch:8 step:7501 [D loss: 0.220894, acc.: 64.06%] [G loss: 0.442396]\n",
      "epoch:8 step:7502 [D loss: 0.216529, acc.: 66.41%] [G loss: 0.479593]\n",
      "epoch:8 step:7503 [D loss: 0.200268, acc.: 69.53%] [G loss: 0.452644]\n",
      "epoch:8 step:7504 [D loss: 0.202393, acc.: 67.97%] [G loss: 0.443740]\n",
      "epoch:8 step:7505 [D loss: 0.190263, acc.: 69.53%] [G loss: 0.464023]\n",
      "epoch:8 step:7506 [D loss: 0.241897, acc.: 60.16%] [G loss: 0.489985]\n",
      "epoch:8 step:7507 [D loss: 0.200944, acc.: 71.09%] [G loss: 0.501383]\n",
      "epoch:8 step:7508 [D loss: 0.219997, acc.: 66.41%] [G loss: 0.485557]\n",
      "epoch:8 step:7509 [D loss: 0.208053, acc.: 64.84%] [G loss: 0.487980]\n",
      "epoch:8 step:7510 [D loss: 0.218604, acc.: 64.84%] [G loss: 0.469296]\n",
      "epoch:8 step:7511 [D loss: 0.209847, acc.: 68.75%] [G loss: 0.482149]\n",
      "epoch:8 step:7512 [D loss: 0.204991, acc.: 69.53%] [G loss: 0.489299]\n",
      "epoch:8 step:7513 [D loss: 0.236324, acc.: 56.25%] [G loss: 0.507041]\n",
      "epoch:8 step:7514 [D loss: 0.241025, acc.: 60.16%] [G loss: 0.529492]\n",
      "epoch:8 step:7515 [D loss: 0.252524, acc.: 57.81%] [G loss: 0.531042]\n",
      "epoch:8 step:7516 [D loss: 0.233039, acc.: 63.28%] [G loss: 0.484746]\n",
      "epoch:8 step:7517 [D loss: 0.217572, acc.: 61.72%] [G loss: 0.478979]\n",
      "epoch:8 step:7518 [D loss: 0.181316, acc.: 71.09%] [G loss: 0.529568]\n",
      "epoch:8 step:7519 [D loss: 0.222093, acc.: 61.72%] [G loss: 0.485179]\n",
      "epoch:8 step:7520 [D loss: 0.203349, acc.: 70.31%] [G loss: 0.442688]\n",
      "epoch:8 step:7521 [D loss: 0.195817, acc.: 67.19%] [G loss: 0.518940]\n",
      "epoch:8 step:7522 [D loss: 0.225321, acc.: 60.16%] [G loss: 0.477743]\n",
      "epoch:8 step:7523 [D loss: 0.230344, acc.: 56.25%] [G loss: 0.443719]\n",
      "epoch:8 step:7524 [D loss: 0.209034, acc.: 70.31%] [G loss: 0.462056]\n",
      "epoch:8 step:7525 [D loss: 0.181577, acc.: 71.88%] [G loss: 0.525902]\n",
      "epoch:8 step:7526 [D loss: 0.216970, acc.: 58.59%] [G loss: 0.489753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7527 [D loss: 0.221661, acc.: 60.16%] [G loss: 0.487836]\n",
      "epoch:8 step:7528 [D loss: 0.244238, acc.: 63.28%] [G loss: 0.481450]\n",
      "epoch:8 step:7529 [D loss: 0.224977, acc.: 64.06%] [G loss: 0.486768]\n",
      "epoch:8 step:7530 [D loss: 0.211185, acc.: 66.41%] [G loss: 0.456863]\n",
      "epoch:8 step:7531 [D loss: 0.233818, acc.: 59.38%] [G loss: 0.443739]\n",
      "epoch:8 step:7532 [D loss: 0.189321, acc.: 68.75%] [G loss: 0.488791]\n",
      "epoch:8 step:7533 [D loss: 0.224514, acc.: 67.19%] [G loss: 0.447478]\n",
      "epoch:8 step:7534 [D loss: 0.243123, acc.: 54.69%] [G loss: 0.469341]\n",
      "epoch:8 step:7535 [D loss: 0.212906, acc.: 64.06%] [G loss: 0.507578]\n",
      "epoch:8 step:7536 [D loss: 0.165623, acc.: 75.78%] [G loss: 0.539598]\n",
      "epoch:8 step:7537 [D loss: 0.214345, acc.: 63.28%] [G loss: 0.456734]\n",
      "epoch:8 step:7538 [D loss: 0.182103, acc.: 71.09%] [G loss: 0.472618]\n",
      "epoch:8 step:7539 [D loss: 0.213890, acc.: 71.09%] [G loss: 0.435557]\n",
      "epoch:8 step:7540 [D loss: 0.250806, acc.: 49.22%] [G loss: 0.429113]\n",
      "epoch:8 step:7541 [D loss: 0.216519, acc.: 60.94%] [G loss: 0.508298]\n",
      "epoch:8 step:7542 [D loss: 0.250460, acc.: 57.03%] [G loss: 0.476764]\n",
      "epoch:8 step:7543 [D loss: 0.187327, acc.: 71.09%] [G loss: 0.517361]\n",
      "epoch:8 step:7544 [D loss: 0.199294, acc.: 75.00%] [G loss: 0.564937]\n",
      "epoch:8 step:7545 [D loss: 0.210513, acc.: 65.62%] [G loss: 0.528937]\n",
      "epoch:8 step:7546 [D loss: 0.206994, acc.: 71.88%] [G loss: 0.480943]\n",
      "epoch:8 step:7547 [D loss: 0.245411, acc.: 56.25%] [G loss: 0.458508]\n",
      "epoch:8 step:7548 [D loss: 0.212746, acc.: 65.62%] [G loss: 0.510591]\n",
      "epoch:8 step:7549 [D loss: 0.195143, acc.: 71.88%] [G loss: 0.483779]\n",
      "epoch:8 step:7550 [D loss: 0.223127, acc.: 62.50%] [G loss: 0.463612]\n",
      "epoch:8 step:7551 [D loss: 0.191024, acc.: 71.88%] [G loss: 0.533838]\n",
      "epoch:8 step:7552 [D loss: 0.242559, acc.: 61.72%] [G loss: 0.463134]\n",
      "epoch:8 step:7553 [D loss: 0.212482, acc.: 68.75%] [G loss: 0.461960]\n",
      "epoch:8 step:7554 [D loss: 0.212112, acc.: 65.62%] [G loss: 0.517570]\n",
      "epoch:8 step:7555 [D loss: 0.238672, acc.: 62.50%] [G loss: 0.449110]\n",
      "epoch:8 step:7556 [D loss: 0.240254, acc.: 58.59%] [G loss: 0.433030]\n",
      "epoch:8 step:7557 [D loss: 0.219609, acc.: 63.28%] [G loss: 0.466549]\n",
      "epoch:8 step:7558 [D loss: 0.220533, acc.: 70.31%] [G loss: 0.420927]\n",
      "epoch:8 step:7559 [D loss: 0.227429, acc.: 64.06%] [G loss: 0.468499]\n",
      "epoch:8 step:7560 [D loss: 0.224786, acc.: 67.97%] [G loss: 0.524664]\n",
      "epoch:8 step:7561 [D loss: 0.238078, acc.: 56.25%] [G loss: 0.451017]\n",
      "epoch:8 step:7562 [D loss: 0.215287, acc.: 64.06%] [G loss: 0.447918]\n",
      "epoch:8 step:7563 [D loss: 0.235688, acc.: 63.28%] [G loss: 0.443561]\n",
      "epoch:8 step:7564 [D loss: 0.231822, acc.: 59.38%] [G loss: 0.474214]\n",
      "epoch:8 step:7565 [D loss: 0.211929, acc.: 69.53%] [G loss: 0.450095]\n",
      "epoch:8 step:7566 [D loss: 0.195036, acc.: 75.00%] [G loss: 0.508579]\n",
      "epoch:8 step:7567 [D loss: 0.224231, acc.: 60.94%] [G loss: 0.448415]\n",
      "epoch:8 step:7568 [D loss: 0.228867, acc.: 59.38%] [G loss: 0.444842]\n",
      "epoch:8 step:7569 [D loss: 0.217024, acc.: 60.94%] [G loss: 0.479832]\n",
      "epoch:8 step:7570 [D loss: 0.177953, acc.: 74.22%] [G loss: 0.452367]\n",
      "epoch:8 step:7571 [D loss: 0.206099, acc.: 65.62%] [G loss: 0.483934]\n",
      "epoch:8 step:7572 [D loss: 0.179551, acc.: 67.97%] [G loss: 0.517143]\n",
      "epoch:8 step:7573 [D loss: 0.182657, acc.: 73.44%] [G loss: 0.512266]\n",
      "epoch:8 step:7574 [D loss: 0.298420, acc.: 54.69%] [G loss: 0.464629]\n",
      "epoch:8 step:7575 [D loss: 0.209879, acc.: 70.31%] [G loss: 0.476998]\n",
      "epoch:8 step:7576 [D loss: 0.212684, acc.: 67.19%] [G loss: 0.454114]\n",
      "epoch:8 step:7577 [D loss: 0.249833, acc.: 60.94%] [G loss: 0.470241]\n",
      "epoch:8 step:7578 [D loss: 0.188922, acc.: 63.28%] [G loss: 0.475124]\n",
      "epoch:8 step:7579 [D loss: 0.185306, acc.: 75.78%] [G loss: 0.511368]\n",
      "epoch:8 step:7580 [D loss: 0.206709, acc.: 68.75%] [G loss: 0.503763]\n",
      "epoch:8 step:7581 [D loss: 0.229122, acc.: 63.28%] [G loss: 0.470377]\n",
      "epoch:8 step:7582 [D loss: 0.206973, acc.: 66.41%] [G loss: 0.481252]\n",
      "epoch:8 step:7583 [D loss: 0.191576, acc.: 72.66%] [G loss: 0.473592]\n",
      "epoch:8 step:7584 [D loss: 0.195276, acc.: 73.44%] [G loss: 0.504622]\n",
      "epoch:8 step:7585 [D loss: 0.187444, acc.: 71.09%] [G loss: 0.472952]\n",
      "epoch:8 step:7586 [D loss: 0.213374, acc.: 67.19%] [G loss: 0.456353]\n",
      "epoch:8 step:7587 [D loss: 0.240718, acc.: 64.84%] [G loss: 0.436532]\n",
      "epoch:8 step:7588 [D loss: 0.198453, acc.: 65.62%] [G loss: 0.512480]\n",
      "epoch:8 step:7589 [D loss: 0.200572, acc.: 66.41%] [G loss: 0.486366]\n",
      "epoch:8 step:7590 [D loss: 0.223139, acc.: 63.28%] [G loss: 0.496083]\n",
      "epoch:8 step:7591 [D loss: 0.223720, acc.: 65.62%] [G loss: 0.479291]\n",
      "epoch:8 step:7592 [D loss: 0.204487, acc.: 70.31%] [G loss: 0.497504]\n",
      "epoch:8 step:7593 [D loss: 0.201059, acc.: 71.09%] [G loss: 0.527961]\n",
      "epoch:8 step:7594 [D loss: 0.222836, acc.: 65.62%] [G loss: 0.475015]\n",
      "epoch:8 step:7595 [D loss: 0.203393, acc.: 66.41%] [G loss: 0.479228]\n",
      "epoch:8 step:7596 [D loss: 0.173497, acc.: 72.66%] [G loss: 0.512616]\n",
      "epoch:8 step:7597 [D loss: 0.242143, acc.: 65.62%] [G loss: 0.496485]\n",
      "epoch:8 step:7598 [D loss: 0.232079, acc.: 59.38%] [G loss: 0.482606]\n",
      "epoch:8 step:7599 [D loss: 0.235550, acc.: 62.50%] [G loss: 0.419405]\n",
      "epoch:8 step:7600 [D loss: 0.220721, acc.: 64.84%] [G loss: 0.475688]\n",
      "##############\n",
      "[2.52441626 1.35685956 6.09534933 4.7799185  3.66262678 5.69871857\n",
      " 4.3875721  4.51914586 4.45473979 3.67075903]\n",
      "##########\n",
      "epoch:8 step:7601 [D loss: 0.220377, acc.: 62.50%] [G loss: 0.462214]\n",
      "epoch:8 step:7602 [D loss: 0.218600, acc.: 64.06%] [G loss: 0.476441]\n",
      "epoch:8 step:7603 [D loss: 0.205459, acc.: 69.53%] [G loss: 0.553260]\n",
      "epoch:8 step:7604 [D loss: 0.283250, acc.: 53.91%] [G loss: 0.431616]\n",
      "epoch:8 step:7605 [D loss: 0.257708, acc.: 57.03%] [G loss: 0.441745]\n",
      "epoch:8 step:7606 [D loss: 0.229816, acc.: 63.28%] [G loss: 0.427346]\n",
      "epoch:8 step:7607 [D loss: 0.210466, acc.: 67.19%] [G loss: 0.479044]\n",
      "epoch:8 step:7608 [D loss: 0.199046, acc.: 71.09%] [G loss: 0.478156]\n",
      "epoch:8 step:7609 [D loss: 0.222067, acc.: 63.28%] [G loss: 0.496227]\n",
      "epoch:8 step:7610 [D loss: 0.237038, acc.: 59.38%] [G loss: 0.501629]\n",
      "epoch:8 step:7611 [D loss: 0.200448, acc.: 68.75%] [G loss: 0.512887]\n",
      "epoch:8 step:7612 [D loss: 0.200334, acc.: 71.09%] [G loss: 0.515293]\n",
      "epoch:8 step:7613 [D loss: 0.218605, acc.: 67.19%] [G loss: 0.521299]\n",
      "epoch:8 step:7614 [D loss: 0.231367, acc.: 61.72%] [G loss: 0.485658]\n",
      "epoch:8 step:7615 [D loss: 0.180077, acc.: 73.44%] [G loss: 0.516701]\n",
      "epoch:8 step:7616 [D loss: 0.253522, acc.: 57.81%] [G loss: 0.497681]\n",
      "epoch:8 step:7617 [D loss: 0.238361, acc.: 58.59%] [G loss: 0.473510]\n",
      "epoch:8 step:7618 [D loss: 0.181916, acc.: 72.66%] [G loss: 0.485296]\n",
      "epoch:8 step:7619 [D loss: 0.222283, acc.: 61.72%] [G loss: 0.486163]\n",
      "epoch:8 step:7620 [D loss: 0.192760, acc.: 69.53%] [G loss: 0.504533]\n",
      "epoch:8 step:7621 [D loss: 0.236196, acc.: 61.72%] [G loss: 0.440866]\n",
      "epoch:8 step:7622 [D loss: 0.210631, acc.: 66.41%] [G loss: 0.455132]\n",
      "epoch:8 step:7623 [D loss: 0.229317, acc.: 60.94%] [G loss: 0.474255]\n",
      "epoch:8 step:7624 [D loss: 0.231900, acc.: 60.94%] [G loss: 0.468557]\n",
      "epoch:8 step:7625 [D loss: 0.236241, acc.: 60.16%] [G loss: 0.435364]\n",
      "epoch:8 step:7626 [D loss: 0.211581, acc.: 65.62%] [G loss: 0.452958]\n",
      "epoch:8 step:7627 [D loss: 0.260456, acc.: 57.03%] [G loss: 0.445507]\n",
      "epoch:8 step:7628 [D loss: 0.216246, acc.: 68.75%] [G loss: 0.514537]\n",
      "epoch:8 step:7629 [D loss: 0.194413, acc.: 71.09%] [G loss: 0.538710]\n",
      "epoch:8 step:7630 [D loss: 0.203181, acc.: 63.28%] [G loss: 0.493214]\n",
      "epoch:8 step:7631 [D loss: 0.196708, acc.: 72.66%] [G loss: 0.469032]\n",
      "epoch:8 step:7632 [D loss: 0.231495, acc.: 64.06%] [G loss: 0.496070]\n",
      "epoch:8 step:7633 [D loss: 0.256507, acc.: 62.50%] [G loss: 0.477367]\n",
      "epoch:8 step:7634 [D loss: 0.258594, acc.: 57.81%] [G loss: 0.433150]\n",
      "epoch:8 step:7635 [D loss: 0.201666, acc.: 67.19%] [G loss: 0.506961]\n",
      "epoch:8 step:7636 [D loss: 0.219775, acc.: 61.72%] [G loss: 0.473986]\n",
      "epoch:8 step:7637 [D loss: 0.205769, acc.: 69.53%] [G loss: 0.478666]\n",
      "epoch:8 step:7638 [D loss: 0.208304, acc.: 69.53%] [G loss: 0.472163]\n",
      "epoch:8 step:7639 [D loss: 0.248821, acc.: 60.94%] [G loss: 0.463230]\n",
      "epoch:8 step:7640 [D loss: 0.195568, acc.: 67.19%] [G loss: 0.523432]\n",
      "epoch:8 step:7641 [D loss: 0.216237, acc.: 67.97%] [G loss: 0.504179]\n",
      "epoch:8 step:7642 [D loss: 0.228967, acc.: 62.50%] [G loss: 0.483874]\n",
      "epoch:8 step:7643 [D loss: 0.265089, acc.: 57.81%] [G loss: 0.470601]\n",
      "epoch:8 step:7644 [D loss: 0.239116, acc.: 60.16%] [G loss: 0.436998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7645 [D loss: 0.181113, acc.: 75.00%] [G loss: 0.516811]\n",
      "epoch:8 step:7646 [D loss: 0.244070, acc.: 58.59%] [G loss: 0.476479]\n",
      "epoch:8 step:7647 [D loss: 0.205178, acc.: 68.75%] [G loss: 0.459019]\n",
      "epoch:8 step:7648 [D loss: 0.199782, acc.: 71.88%] [G loss: 0.501736]\n",
      "epoch:8 step:7649 [D loss: 0.219416, acc.: 64.84%] [G loss: 0.441888]\n",
      "epoch:8 step:7650 [D loss: 0.215649, acc.: 65.62%] [G loss: 0.473379]\n",
      "epoch:8 step:7651 [D loss: 0.212302, acc.: 65.62%] [G loss: 0.441219]\n",
      "epoch:8 step:7652 [D loss: 0.206784, acc.: 63.28%] [G loss: 0.458254]\n",
      "epoch:8 step:7653 [D loss: 0.246285, acc.: 58.59%] [G loss: 0.466089]\n",
      "epoch:8 step:7654 [D loss: 0.215766, acc.: 67.19%] [G loss: 0.436231]\n",
      "epoch:8 step:7655 [D loss: 0.223396, acc.: 64.06%] [G loss: 0.459669]\n",
      "epoch:8 step:7656 [D loss: 0.255007, acc.: 54.69%] [G loss: 0.463059]\n",
      "epoch:8 step:7657 [D loss: 0.226336, acc.: 65.62%] [G loss: 0.463894]\n",
      "epoch:8 step:7658 [D loss: 0.205938, acc.: 67.97%] [G loss: 0.531284]\n",
      "epoch:8 step:7659 [D loss: 0.225716, acc.: 60.16%] [G loss: 0.512603]\n",
      "epoch:8 step:7660 [D loss: 0.230337, acc.: 58.59%] [G loss: 0.520235]\n",
      "epoch:8 step:7661 [D loss: 0.244346, acc.: 59.38%] [G loss: 0.449691]\n",
      "epoch:8 step:7662 [D loss: 0.207200, acc.: 67.19%] [G loss: 0.496019]\n",
      "epoch:8 step:7663 [D loss: 0.194529, acc.: 75.00%] [G loss: 0.448125]\n",
      "epoch:8 step:7664 [D loss: 0.199094, acc.: 72.66%] [G loss: 0.482022]\n",
      "epoch:8 step:7665 [D loss: 0.231827, acc.: 61.72%] [G loss: 0.474673]\n",
      "epoch:8 step:7666 [D loss: 0.251855, acc.: 58.59%] [G loss: 0.463637]\n",
      "epoch:8 step:7667 [D loss: 0.206300, acc.: 72.66%] [G loss: 0.465092]\n",
      "epoch:8 step:7668 [D loss: 0.230042, acc.: 65.62%] [G loss: 0.441381]\n",
      "epoch:8 step:7669 [D loss: 0.233047, acc.: 63.28%] [G loss: 0.489600]\n",
      "epoch:8 step:7670 [D loss: 0.227172, acc.: 61.72%] [G loss: 0.466643]\n",
      "epoch:8 step:7671 [D loss: 0.219038, acc.: 60.16%] [G loss: 0.457247]\n",
      "epoch:8 step:7672 [D loss: 0.226368, acc.: 57.03%] [G loss: 0.477429]\n",
      "epoch:8 step:7673 [D loss: 0.219024, acc.: 64.84%] [G loss: 0.476602]\n",
      "epoch:8 step:7674 [D loss: 0.217002, acc.: 65.62%] [G loss: 0.443855]\n",
      "epoch:8 step:7675 [D loss: 0.223991, acc.: 64.84%] [G loss: 0.482850]\n",
      "epoch:8 step:7676 [D loss: 0.222272, acc.: 64.84%] [G loss: 0.474717]\n",
      "epoch:8 step:7677 [D loss: 0.214056, acc.: 66.41%] [G loss: 0.453123]\n",
      "epoch:8 step:7678 [D loss: 0.265431, acc.: 52.34%] [G loss: 0.451797]\n",
      "epoch:8 step:7679 [D loss: 0.262709, acc.: 56.25%] [G loss: 0.476738]\n",
      "epoch:8 step:7680 [D loss: 0.246042, acc.: 55.47%] [G loss: 0.436876]\n",
      "epoch:8 step:7681 [D loss: 0.227086, acc.: 60.16%] [G loss: 0.462045]\n",
      "epoch:8 step:7682 [D loss: 0.259433, acc.: 58.59%] [G loss: 0.427720]\n",
      "epoch:8 step:7683 [D loss: 0.237771, acc.: 55.47%] [G loss: 0.478748]\n",
      "epoch:8 step:7684 [D loss: 0.207823, acc.: 66.41%] [G loss: 0.503785]\n",
      "epoch:8 step:7685 [D loss: 0.229902, acc.: 60.94%] [G loss: 0.475607]\n",
      "epoch:8 step:7686 [D loss: 0.220367, acc.: 65.62%] [G loss: 0.429919]\n",
      "epoch:8 step:7687 [D loss: 0.202345, acc.: 71.09%] [G loss: 0.514011]\n",
      "epoch:8 step:7688 [D loss: 0.232033, acc.: 61.72%] [G loss: 0.444324]\n",
      "epoch:8 step:7689 [D loss: 0.215034, acc.: 65.62%] [G loss: 0.486348]\n",
      "epoch:8 step:7690 [D loss: 0.187989, acc.: 72.66%] [G loss: 0.501008]\n",
      "epoch:8 step:7691 [D loss: 0.220975, acc.: 67.19%] [G loss: 0.491264]\n",
      "epoch:8 step:7692 [D loss: 0.201411, acc.: 67.97%] [G loss: 0.503935]\n",
      "epoch:8 step:7693 [D loss: 0.191359, acc.: 71.88%] [G loss: 0.472700]\n",
      "epoch:8 step:7694 [D loss: 0.159692, acc.: 77.34%] [G loss: 0.540479]\n",
      "epoch:8 step:7695 [D loss: 0.209685, acc.: 64.84%] [G loss: 0.481232]\n",
      "epoch:8 step:7696 [D loss: 0.246731, acc.: 57.03%] [G loss: 0.450060]\n",
      "epoch:8 step:7697 [D loss: 0.245839, acc.: 60.94%] [G loss: 0.440518]\n",
      "epoch:8 step:7698 [D loss: 0.218348, acc.: 64.06%] [G loss: 0.499065]\n",
      "epoch:8 step:7699 [D loss: 0.278614, acc.: 51.56%] [G loss: 0.461854]\n",
      "epoch:8 step:7700 [D loss: 0.216996, acc.: 62.50%] [G loss: 0.470696]\n",
      "epoch:8 step:7701 [D loss: 0.226629, acc.: 64.84%] [G loss: 0.505328]\n",
      "epoch:8 step:7702 [D loss: 0.187527, acc.: 75.00%] [G loss: 0.520702]\n",
      "epoch:8 step:7703 [D loss: 0.192879, acc.: 73.44%] [G loss: 0.520443]\n",
      "epoch:8 step:7704 [D loss: 0.188692, acc.: 67.97%] [G loss: 0.500781]\n",
      "epoch:8 step:7705 [D loss: 0.186322, acc.: 71.88%] [G loss: 0.505319]\n",
      "epoch:8 step:7706 [D loss: 0.252705, acc.: 57.81%] [G loss: 0.461033]\n",
      "epoch:8 step:7707 [D loss: 0.245731, acc.: 57.81%] [G loss: 0.423677]\n",
      "epoch:8 step:7708 [D loss: 0.225571, acc.: 59.38%] [G loss: 0.455229]\n",
      "epoch:8 step:7709 [D loss: 0.238147, acc.: 60.94%] [G loss: 0.442189]\n",
      "epoch:8 step:7710 [D loss: 0.252537, acc.: 57.03%] [G loss: 0.434144]\n",
      "epoch:8 step:7711 [D loss: 0.245985, acc.: 58.59%] [G loss: 0.469258]\n",
      "epoch:8 step:7712 [D loss: 0.235613, acc.: 58.59%] [G loss: 0.485332]\n",
      "epoch:8 step:7713 [D loss: 0.216598, acc.: 64.06%] [G loss: 0.493223]\n",
      "epoch:8 step:7714 [D loss: 0.189048, acc.: 70.31%] [G loss: 0.457396]\n",
      "epoch:8 step:7715 [D loss: 0.196363, acc.: 67.97%] [G loss: 0.515950]\n",
      "epoch:8 step:7716 [D loss: 0.288614, acc.: 50.78%] [G loss: 0.434165]\n",
      "epoch:8 step:7717 [D loss: 0.195829, acc.: 74.22%] [G loss: 0.514675]\n",
      "epoch:8 step:7718 [D loss: 0.197104, acc.: 68.75%] [G loss: 0.476054]\n",
      "epoch:8 step:7719 [D loss: 0.193987, acc.: 75.00%] [G loss: 0.480114]\n",
      "epoch:8 step:7720 [D loss: 0.234786, acc.: 62.50%] [G loss: 0.460304]\n",
      "epoch:8 step:7721 [D loss: 0.244565, acc.: 60.16%] [G loss: 0.460849]\n",
      "epoch:8 step:7722 [D loss: 0.247493, acc.: 61.72%] [G loss: 0.481420]\n",
      "epoch:8 step:7723 [D loss: 0.242648, acc.: 60.16%] [G loss: 0.432485]\n",
      "epoch:8 step:7724 [D loss: 0.261944, acc.: 50.78%] [G loss: 0.439990]\n",
      "epoch:8 step:7725 [D loss: 0.203468, acc.: 68.75%] [G loss: 0.478207]\n",
      "epoch:8 step:7726 [D loss: 0.198502, acc.: 70.31%] [G loss: 0.488245]\n",
      "epoch:8 step:7727 [D loss: 0.169064, acc.: 76.56%] [G loss: 0.545942]\n",
      "epoch:8 step:7728 [D loss: 0.170096, acc.: 75.00%] [G loss: 0.589522]\n",
      "epoch:8 step:7729 [D loss: 0.259881, acc.: 61.72%] [G loss: 0.509003]\n",
      "epoch:8 step:7730 [D loss: 0.235324, acc.: 64.06%] [G loss: 0.430336]\n",
      "epoch:8 step:7731 [D loss: 0.215848, acc.: 66.41%] [G loss: 0.474738]\n",
      "epoch:8 step:7732 [D loss: 0.189981, acc.: 69.53%] [G loss: 0.476331]\n",
      "epoch:8 step:7733 [D loss: 0.212732, acc.: 66.41%] [G loss: 0.474121]\n",
      "epoch:8 step:7734 [D loss: 0.231186, acc.: 64.84%] [G loss: 0.491370]\n",
      "epoch:8 step:7735 [D loss: 0.204923, acc.: 63.28%] [G loss: 0.527496]\n",
      "epoch:8 step:7736 [D loss: 0.216226, acc.: 63.28%] [G loss: 0.494103]\n",
      "epoch:8 step:7737 [D loss: 0.208321, acc.: 65.62%] [G loss: 0.471625]\n",
      "epoch:8 step:7738 [D loss: 0.194688, acc.: 68.75%] [G loss: 0.511437]\n",
      "epoch:8 step:7739 [D loss: 0.243536, acc.: 56.25%] [G loss: 0.459010]\n",
      "epoch:8 step:7740 [D loss: 0.199471, acc.: 67.97%] [G loss: 0.528795]\n",
      "epoch:8 step:7741 [D loss: 0.224444, acc.: 64.84%] [G loss: 0.510666]\n",
      "epoch:8 step:7742 [D loss: 0.226816, acc.: 57.03%] [G loss: 0.484715]\n",
      "epoch:8 step:7743 [D loss: 0.238928, acc.: 62.50%] [G loss: 0.488669]\n",
      "epoch:8 step:7744 [D loss: 0.225425, acc.: 62.50%] [G loss: 0.457025]\n",
      "epoch:8 step:7745 [D loss: 0.254961, acc.: 57.03%] [G loss: 0.504412]\n",
      "epoch:8 step:7746 [D loss: 0.253786, acc.: 50.78%] [G loss: 0.469632]\n",
      "epoch:8 step:7747 [D loss: 0.245249, acc.: 60.16%] [G loss: 0.431520]\n",
      "epoch:8 step:7748 [D loss: 0.217495, acc.: 66.41%] [G loss: 0.500127]\n",
      "epoch:8 step:7749 [D loss: 0.215049, acc.: 63.28%] [G loss: 0.464127]\n",
      "epoch:8 step:7750 [D loss: 0.217348, acc.: 66.41%] [G loss: 0.447867]\n",
      "epoch:8 step:7751 [D loss: 0.205664, acc.: 65.62%] [G loss: 0.489878]\n",
      "epoch:8 step:7752 [D loss: 0.243565, acc.: 58.59%] [G loss: 0.431406]\n",
      "epoch:8 step:7753 [D loss: 0.237014, acc.: 61.72%] [G loss: 0.451669]\n",
      "epoch:8 step:7754 [D loss: 0.216769, acc.: 62.50%] [G loss: 0.480833]\n",
      "epoch:8 step:7755 [D loss: 0.196159, acc.: 67.97%] [G loss: 0.483507]\n",
      "epoch:8 step:7756 [D loss: 0.227828, acc.: 60.94%] [G loss: 0.476019]\n",
      "epoch:8 step:7757 [D loss: 0.205500, acc.: 68.75%] [G loss: 0.497604]\n",
      "epoch:8 step:7758 [D loss: 0.192761, acc.: 65.62%] [G loss: 0.489325]\n",
      "epoch:8 step:7759 [D loss: 0.270968, acc.: 50.00%] [G loss: 0.484754]\n",
      "epoch:8 step:7760 [D loss: 0.182363, acc.: 72.66%] [G loss: 0.470617]\n",
      "epoch:8 step:7761 [D loss: 0.282727, acc.: 49.22%] [G loss: 0.445974]\n",
      "epoch:8 step:7762 [D loss: 0.227099, acc.: 64.84%] [G loss: 0.434919]\n",
      "epoch:8 step:7763 [D loss: 0.247234, acc.: 55.47%] [G loss: 0.410575]\n",
      "epoch:8 step:7764 [D loss: 0.201574, acc.: 69.53%] [G loss: 0.464491]\n",
      "epoch:8 step:7765 [D loss: 0.231131, acc.: 61.72%] [G loss: 0.462260]\n",
      "epoch:8 step:7766 [D loss: 0.223097, acc.: 67.19%] [G loss: 0.478818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7767 [D loss: 0.186617, acc.: 75.00%] [G loss: 0.511025]\n",
      "epoch:8 step:7768 [D loss: 0.215276, acc.: 65.62%] [G loss: 0.510542]\n",
      "epoch:8 step:7769 [D loss: 0.195165, acc.: 69.53%] [G loss: 0.473549]\n",
      "epoch:8 step:7770 [D loss: 0.169821, acc.: 75.00%] [G loss: 0.472437]\n",
      "epoch:8 step:7771 [D loss: 0.276051, acc.: 55.47%] [G loss: 0.442577]\n",
      "epoch:8 step:7772 [D loss: 0.198912, acc.: 72.66%] [G loss: 0.503385]\n",
      "epoch:8 step:7773 [D loss: 0.247679, acc.: 60.94%] [G loss: 0.452402]\n",
      "epoch:8 step:7774 [D loss: 0.242358, acc.: 64.84%] [G loss: 0.424765]\n",
      "epoch:8 step:7775 [D loss: 0.225042, acc.: 60.94%] [G loss: 0.480688]\n",
      "epoch:8 step:7776 [D loss: 0.181800, acc.: 70.31%] [G loss: 0.494087]\n",
      "epoch:8 step:7777 [D loss: 0.256097, acc.: 53.12%] [G loss: 0.410610]\n",
      "epoch:8 step:7778 [D loss: 0.225537, acc.: 63.28%] [G loss: 0.456563]\n",
      "epoch:8 step:7779 [D loss: 0.194057, acc.: 66.41%] [G loss: 0.448587]\n",
      "epoch:8 step:7780 [D loss: 0.210823, acc.: 67.19%] [G loss: 0.483001]\n",
      "epoch:8 step:7781 [D loss: 0.209827, acc.: 68.75%] [G loss: 0.512129]\n",
      "epoch:8 step:7782 [D loss: 0.191286, acc.: 74.22%] [G loss: 0.530511]\n",
      "epoch:8 step:7783 [D loss: 0.218865, acc.: 64.84%] [G loss: 0.496640]\n",
      "epoch:8 step:7784 [D loss: 0.203238, acc.: 65.62%] [G loss: 0.480131]\n",
      "epoch:8 step:7785 [D loss: 0.202773, acc.: 67.19%] [G loss: 0.530909]\n",
      "epoch:8 step:7786 [D loss: 0.226709, acc.: 63.28%] [G loss: 0.515543]\n",
      "epoch:8 step:7787 [D loss: 0.212576, acc.: 68.75%] [G loss: 0.493409]\n",
      "epoch:8 step:7788 [D loss: 0.230755, acc.: 60.16%] [G loss: 0.460185]\n",
      "epoch:8 step:7789 [D loss: 0.245102, acc.: 57.03%] [G loss: 0.442107]\n",
      "epoch:8 step:7790 [D loss: 0.241771, acc.: 62.50%] [G loss: 0.452198]\n",
      "epoch:8 step:7791 [D loss: 0.209572, acc.: 64.84%] [G loss: 0.459035]\n",
      "epoch:8 step:7792 [D loss: 0.185818, acc.: 71.09%] [G loss: 0.480767]\n",
      "epoch:8 step:7793 [D loss: 0.216765, acc.: 66.41%] [G loss: 0.436629]\n",
      "epoch:8 step:7794 [D loss: 0.201042, acc.: 71.09%] [G loss: 0.463052]\n",
      "epoch:8 step:7795 [D loss: 0.211989, acc.: 66.41%] [G loss: 0.520845]\n",
      "epoch:8 step:7796 [D loss: 0.207107, acc.: 69.53%] [G loss: 0.511787]\n",
      "epoch:8 step:7797 [D loss: 0.226624, acc.: 62.50%] [G loss: 0.468308]\n",
      "epoch:8 step:7798 [D loss: 0.237658, acc.: 57.81%] [G loss: 0.418375]\n",
      "epoch:8 step:7799 [D loss: 0.260968, acc.: 53.12%] [G loss: 0.454396]\n",
      "epoch:8 step:7800 [D loss: 0.209081, acc.: 66.41%] [G loss: 0.507817]\n",
      "##############\n",
      "[2.67560576 1.49844391 6.02245103 4.57546244 3.7239308  5.70790061\n",
      " 4.55251785 4.41221818 4.46181722 3.74600559]\n",
      "##########\n",
      "epoch:8 step:7801 [D loss: 0.213176, acc.: 68.75%] [G loss: 0.481904]\n",
      "epoch:8 step:7802 [D loss: 0.243629, acc.: 57.81%] [G loss: 0.477347]\n",
      "epoch:8 step:7803 [D loss: 0.211727, acc.: 65.62%] [G loss: 0.435203]\n",
      "epoch:8 step:7804 [D loss: 0.258777, acc.: 53.91%] [G loss: 0.434493]\n",
      "epoch:8 step:7805 [D loss: 0.172757, acc.: 73.44%] [G loss: 0.489808]\n",
      "epoch:8 step:7806 [D loss: 0.203061, acc.: 67.97%] [G loss: 0.503187]\n",
      "epoch:8 step:7807 [D loss: 0.195161, acc.: 67.97%] [G loss: 0.509859]\n",
      "epoch:8 step:7808 [D loss: 0.172545, acc.: 75.78%] [G loss: 0.539926]\n",
      "epoch:8 step:7809 [D loss: 0.211069, acc.: 65.62%] [G loss: 0.538653]\n",
      "epoch:8 step:7810 [D loss: 0.195782, acc.: 70.31%] [G loss: 0.579433]\n",
      "epoch:8 step:7811 [D loss: 0.202080, acc.: 67.97%] [G loss: 0.504448]\n",
      "epoch:8 step:7812 [D loss: 0.262502, acc.: 54.69%] [G loss: 0.416274]\n",
      "epoch:8 step:7813 [D loss: 0.269764, acc.: 52.34%] [G loss: 0.365987]\n",
      "epoch:8 step:7814 [D loss: 0.222660, acc.: 63.28%] [G loss: 0.478898]\n",
      "epoch:8 step:7815 [D loss: 0.201066, acc.: 68.75%] [G loss: 0.518788]\n",
      "epoch:8 step:7816 [D loss: 0.205752, acc.: 67.19%] [G loss: 0.422038]\n",
      "epoch:8 step:7817 [D loss: 0.185867, acc.: 71.88%] [G loss: 0.482790]\n",
      "epoch:8 step:7818 [D loss: 0.217052, acc.: 70.31%] [G loss: 0.555141]\n",
      "epoch:8 step:7819 [D loss: 0.273501, acc.: 50.00%] [G loss: 0.452256]\n",
      "epoch:8 step:7820 [D loss: 0.216193, acc.: 66.41%] [G loss: 0.453501]\n",
      "epoch:8 step:7821 [D loss: 0.221060, acc.: 60.94%] [G loss: 0.442713]\n",
      "epoch:8 step:7822 [D loss: 0.212772, acc.: 63.28%] [G loss: 0.466654]\n",
      "epoch:8 step:7823 [D loss: 0.205180, acc.: 66.41%] [G loss: 0.498562]\n",
      "epoch:8 step:7824 [D loss: 0.201981, acc.: 71.88%] [G loss: 0.519126]\n",
      "epoch:8 step:7825 [D loss: 0.208295, acc.: 66.41%] [G loss: 0.507678]\n",
      "epoch:8 step:7826 [D loss: 0.215992, acc.: 62.50%] [G loss: 0.449309]\n",
      "epoch:8 step:7827 [D loss: 0.222654, acc.: 64.06%] [G loss: 0.450503]\n",
      "epoch:8 step:7828 [D loss: 0.188500, acc.: 74.22%] [G loss: 0.508728]\n",
      "epoch:8 step:7829 [D loss: 0.232598, acc.: 69.53%] [G loss: 0.524396]\n",
      "epoch:8 step:7830 [D loss: 0.221017, acc.: 63.28%] [G loss: 0.466276]\n",
      "epoch:8 step:7831 [D loss: 0.212527, acc.: 67.19%] [G loss: 0.484022]\n",
      "epoch:8 step:7832 [D loss: 0.207380, acc.: 71.88%] [G loss: 0.467929]\n",
      "epoch:8 step:7833 [D loss: 0.214798, acc.: 67.19%] [G loss: 0.479222]\n",
      "epoch:8 step:7834 [D loss: 0.224928, acc.: 62.50%] [G loss: 0.446689]\n",
      "epoch:8 step:7835 [D loss: 0.191247, acc.: 71.09%] [G loss: 0.439650]\n",
      "epoch:8 step:7836 [D loss: 0.211904, acc.: 66.41%] [G loss: 0.435220]\n",
      "epoch:8 step:7837 [D loss: 0.259628, acc.: 60.94%] [G loss: 0.464199]\n",
      "epoch:8 step:7838 [D loss: 0.231097, acc.: 63.28%] [G loss: 0.470885]\n",
      "epoch:8 step:7839 [D loss: 0.187922, acc.: 71.88%] [G loss: 0.495265]\n",
      "epoch:8 step:7840 [D loss: 0.191331, acc.: 71.88%] [G loss: 0.539563]\n",
      "epoch:8 step:7841 [D loss: 0.208740, acc.: 65.62%] [G loss: 0.459136]\n",
      "epoch:8 step:7842 [D loss: 0.200704, acc.: 65.62%] [G loss: 0.464642]\n",
      "epoch:8 step:7843 [D loss: 0.171909, acc.: 76.56%] [G loss: 0.549413]\n",
      "epoch:8 step:7844 [D loss: 0.267757, acc.: 56.25%] [G loss: 0.466442]\n",
      "epoch:8 step:7845 [D loss: 0.294456, acc.: 48.44%] [G loss: 0.408349]\n",
      "epoch:8 step:7846 [D loss: 0.218714, acc.: 65.62%] [G loss: 0.428461]\n",
      "epoch:8 step:7847 [D loss: 0.230851, acc.: 63.28%] [G loss: 0.462189]\n",
      "epoch:8 step:7848 [D loss: 0.213036, acc.: 68.75%] [G loss: 0.498509]\n",
      "epoch:8 step:7849 [D loss: 0.224064, acc.: 61.72%] [G loss: 0.524779]\n",
      "epoch:8 step:7850 [D loss: 0.174460, acc.: 75.78%] [G loss: 0.557002]\n",
      "epoch:8 step:7851 [D loss: 0.237326, acc.: 60.16%] [G loss: 0.489374]\n",
      "epoch:8 step:7852 [D loss: 0.218362, acc.: 67.19%] [G loss: 0.429346]\n",
      "epoch:8 step:7853 [D loss: 0.225640, acc.: 61.72%] [G loss: 0.451668]\n",
      "epoch:8 step:7854 [D loss: 0.203390, acc.: 68.75%] [G loss: 0.548299]\n",
      "epoch:8 step:7855 [D loss: 0.207114, acc.: 70.31%] [G loss: 0.527437]\n",
      "epoch:8 step:7856 [D loss: 0.206510, acc.: 67.97%] [G loss: 0.543776]\n",
      "epoch:8 step:7857 [D loss: 0.208940, acc.: 67.97%] [G loss: 0.498992]\n",
      "epoch:8 step:7858 [D loss: 0.219265, acc.: 59.38%] [G loss: 0.468330]\n",
      "epoch:8 step:7859 [D loss: 0.222006, acc.: 67.19%] [G loss: 0.433308]\n",
      "epoch:8 step:7860 [D loss: 0.217227, acc.: 62.50%] [G loss: 0.454351]\n",
      "epoch:8 step:7861 [D loss: 0.192041, acc.: 68.75%] [G loss: 0.475458]\n",
      "epoch:8 step:7862 [D loss: 0.211465, acc.: 67.97%] [G loss: 0.496855]\n",
      "epoch:8 step:7863 [D loss: 0.230009, acc.: 58.59%] [G loss: 0.492333]\n",
      "epoch:8 step:7864 [D loss: 0.222528, acc.: 62.50%] [G loss: 0.451629]\n",
      "epoch:8 step:7865 [D loss: 0.206064, acc.: 66.41%] [G loss: 0.471060]\n",
      "epoch:8 step:7866 [D loss: 0.198160, acc.: 68.75%] [G loss: 0.490853]\n",
      "epoch:8 step:7867 [D loss: 0.225237, acc.: 64.84%] [G loss: 0.510956]\n",
      "epoch:8 step:7868 [D loss: 0.200003, acc.: 71.09%] [G loss: 0.519907]\n",
      "epoch:8 step:7869 [D loss: 0.236185, acc.: 54.69%] [G loss: 0.515265]\n",
      "epoch:8 step:7870 [D loss: 0.227305, acc.: 64.84%] [G loss: 0.500732]\n",
      "epoch:8 step:7871 [D loss: 0.218366, acc.: 71.09%] [G loss: 0.491210]\n",
      "epoch:8 step:7872 [D loss: 0.244774, acc.: 52.34%] [G loss: 0.428176]\n",
      "epoch:8 step:7873 [D loss: 0.255809, acc.: 53.91%] [G loss: 0.412527]\n",
      "epoch:8 step:7874 [D loss: 0.220103, acc.: 61.72%] [G loss: 0.495337]\n",
      "epoch:8 step:7875 [D loss: 0.216701, acc.: 61.72%] [G loss: 0.491850]\n",
      "epoch:8 step:7876 [D loss: 0.208056, acc.: 67.19%] [G loss: 0.475082]\n",
      "epoch:8 step:7877 [D loss: 0.192349, acc.: 70.31%] [G loss: 0.521741]\n",
      "epoch:8 step:7878 [D loss: 0.234038, acc.: 62.50%] [G loss: 0.511488]\n",
      "epoch:8 step:7879 [D loss: 0.231874, acc.: 64.06%] [G loss: 0.468993]\n",
      "epoch:8 step:7880 [D loss: 0.224684, acc.: 62.50%] [G loss: 0.488477]\n",
      "epoch:8 step:7881 [D loss: 0.206510, acc.: 65.62%] [G loss: 0.466992]\n",
      "epoch:8 step:7882 [D loss: 0.235606, acc.: 60.16%] [G loss: 0.445120]\n",
      "epoch:8 step:7883 [D loss: 0.208762, acc.: 67.19%] [G loss: 0.508584]\n",
      "epoch:8 step:7884 [D loss: 0.227502, acc.: 64.06%] [G loss: 0.456368]\n",
      "epoch:8 step:7885 [D loss: 0.226850, acc.: 63.28%] [G loss: 0.519684]\n",
      "epoch:8 step:7886 [D loss: 0.254469, acc.: 58.59%] [G loss: 0.450925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7887 [D loss: 0.215314, acc.: 68.75%] [G loss: 0.453782]\n",
      "epoch:8 step:7888 [D loss: 0.230191, acc.: 63.28%] [G loss: 0.481283]\n",
      "epoch:8 step:7889 [D loss: 0.229808, acc.: 60.94%] [G loss: 0.449360]\n",
      "epoch:8 step:7890 [D loss: 0.237655, acc.: 62.50%] [G loss: 0.461895]\n",
      "epoch:8 step:7891 [D loss: 0.218732, acc.: 66.41%] [G loss: 0.458343]\n",
      "epoch:8 step:7892 [D loss: 0.254865, acc.: 53.91%] [G loss: 0.469845]\n",
      "epoch:8 step:7893 [D loss: 0.210320, acc.: 65.62%] [G loss: 0.511113]\n",
      "epoch:8 step:7894 [D loss: 0.188767, acc.: 71.09%] [G loss: 0.531340]\n",
      "epoch:8 step:7895 [D loss: 0.186484, acc.: 72.66%] [G loss: 0.520837]\n",
      "epoch:8 step:7896 [D loss: 0.248238, acc.: 57.03%] [G loss: 0.435758]\n",
      "epoch:8 step:7897 [D loss: 0.234135, acc.: 59.38%] [G loss: 0.466614]\n",
      "epoch:8 step:7898 [D loss: 0.223993, acc.: 64.84%] [G loss: 0.450787]\n",
      "epoch:8 step:7899 [D loss: 0.211588, acc.: 64.06%] [G loss: 0.481067]\n",
      "epoch:8 step:7900 [D loss: 0.220188, acc.: 62.50%] [G loss: 0.474090]\n",
      "epoch:8 step:7901 [D loss: 0.193228, acc.: 71.88%] [G loss: 0.531223]\n",
      "epoch:8 step:7902 [D loss: 0.216937, acc.: 67.19%] [G loss: 0.551535]\n",
      "epoch:8 step:7903 [D loss: 0.214684, acc.: 64.84%] [G loss: 0.541862]\n",
      "epoch:8 step:7904 [D loss: 0.266525, acc.: 56.25%] [G loss: 0.449843]\n",
      "epoch:8 step:7905 [D loss: 0.231041, acc.: 60.94%] [G loss: 0.449237]\n",
      "epoch:8 step:7906 [D loss: 0.219441, acc.: 65.62%] [G loss: 0.442703]\n",
      "epoch:8 step:7907 [D loss: 0.203600, acc.: 70.31%] [G loss: 0.483692]\n",
      "epoch:8 step:7908 [D loss: 0.229335, acc.: 60.16%] [G loss: 0.434012]\n",
      "epoch:8 step:7909 [D loss: 0.232690, acc.: 58.59%] [G loss: 0.431463]\n",
      "epoch:8 step:7910 [D loss: 0.235698, acc.: 56.25%] [G loss: 0.455782]\n",
      "epoch:8 step:7911 [D loss: 0.239915, acc.: 65.62%] [G loss: 0.461502]\n",
      "epoch:8 step:7912 [D loss: 0.202271, acc.: 70.31%] [G loss: 0.516111]\n",
      "epoch:8 step:7913 [D loss: 0.243240, acc.: 60.94%] [G loss: 0.480321]\n",
      "epoch:8 step:7914 [D loss: 0.264065, acc.: 53.91%] [G loss: 0.431175]\n",
      "epoch:8 step:7915 [D loss: 0.256725, acc.: 57.03%] [G loss: 0.418439]\n",
      "epoch:8 step:7916 [D loss: 0.216816, acc.: 64.84%] [G loss: 0.493896]\n",
      "epoch:8 step:7917 [D loss: 0.238405, acc.: 64.84%] [G loss: 0.470150]\n",
      "epoch:8 step:7918 [D loss: 0.227149, acc.: 59.38%] [G loss: 0.512409]\n",
      "epoch:8 step:7919 [D loss: 0.207748, acc.: 67.19%] [G loss: 0.472809]\n",
      "epoch:8 step:7920 [D loss: 0.268876, acc.: 54.69%] [G loss: 0.394220]\n",
      "epoch:8 step:7921 [D loss: 0.217758, acc.: 66.41%] [G loss: 0.415298]\n",
      "epoch:8 step:7922 [D loss: 0.199229, acc.: 65.62%] [G loss: 0.517771]\n",
      "epoch:8 step:7923 [D loss: 0.179860, acc.: 71.88%] [G loss: 0.503317]\n",
      "epoch:8 step:7924 [D loss: 0.181397, acc.: 70.31%] [G loss: 0.481802]\n",
      "epoch:8 step:7925 [D loss: 0.176345, acc.: 78.91%] [G loss: 0.507502]\n",
      "epoch:8 step:7926 [D loss: 0.203246, acc.: 67.97%] [G loss: 0.497516]\n",
      "epoch:8 step:7927 [D loss: 0.239881, acc.: 57.03%] [G loss: 0.520122]\n",
      "epoch:8 step:7928 [D loss: 0.235518, acc.: 60.16%] [G loss: 0.489102]\n",
      "epoch:8 step:7929 [D loss: 0.238432, acc.: 60.94%] [G loss: 0.465018]\n",
      "epoch:8 step:7930 [D loss: 0.222880, acc.: 67.19%] [G loss: 0.453367]\n",
      "epoch:8 step:7931 [D loss: 0.213832, acc.: 67.97%] [G loss: 0.492375]\n",
      "epoch:8 step:7932 [D loss: 0.200889, acc.: 70.31%] [G loss: 0.516028]\n",
      "epoch:8 step:7933 [D loss: 0.254918, acc.: 53.12%] [G loss: 0.474713]\n",
      "epoch:8 step:7934 [D loss: 0.265085, acc.: 56.25%] [G loss: 0.421656]\n",
      "epoch:8 step:7935 [D loss: 0.189842, acc.: 68.75%] [G loss: 0.491349]\n",
      "epoch:8 step:7936 [D loss: 0.206254, acc.: 64.84%] [G loss: 0.482280]\n",
      "epoch:8 step:7937 [D loss: 0.244830, acc.: 58.59%] [G loss: 0.467518]\n",
      "epoch:8 step:7938 [D loss: 0.227895, acc.: 64.06%] [G loss: 0.508036]\n",
      "epoch:8 step:7939 [D loss: 0.230565, acc.: 63.28%] [G loss: 0.465256]\n",
      "epoch:8 step:7940 [D loss: 0.202422, acc.: 71.09%] [G loss: 0.487760]\n",
      "epoch:8 step:7941 [D loss: 0.203643, acc.: 64.06%] [G loss: 0.471943]\n",
      "epoch:8 step:7942 [D loss: 0.202629, acc.: 71.09%] [G loss: 0.475838]\n",
      "epoch:8 step:7943 [D loss: 0.211320, acc.: 66.41%] [G loss: 0.517479]\n",
      "epoch:8 step:7944 [D loss: 0.229707, acc.: 63.28%] [G loss: 0.446639]\n",
      "epoch:8 step:7945 [D loss: 0.210659, acc.: 64.84%] [G loss: 0.479905]\n",
      "epoch:8 step:7946 [D loss: 0.214272, acc.: 67.97%] [G loss: 0.447386]\n",
      "epoch:8 step:7947 [D loss: 0.160343, acc.: 80.47%] [G loss: 0.495816]\n",
      "epoch:8 step:7948 [D loss: 0.189818, acc.: 71.09%] [G loss: 0.493793]\n",
      "epoch:8 step:7949 [D loss: 0.198545, acc.: 74.22%] [G loss: 0.531098]\n",
      "epoch:8 step:7950 [D loss: 0.219601, acc.: 67.19%] [G loss: 0.517735]\n",
      "epoch:8 step:7951 [D loss: 0.221291, acc.: 62.50%] [G loss: 0.511653]\n",
      "epoch:8 step:7952 [D loss: 0.265240, acc.: 55.47%] [G loss: 0.444518]\n",
      "epoch:8 step:7953 [D loss: 0.202358, acc.: 69.53%] [G loss: 0.497278]\n",
      "epoch:8 step:7954 [D loss: 0.266233, acc.: 48.44%] [G loss: 0.442256]\n",
      "epoch:8 step:7955 [D loss: 0.251526, acc.: 54.69%] [G loss: 0.474708]\n",
      "epoch:8 step:7956 [D loss: 0.227755, acc.: 62.50%] [G loss: 0.455575]\n",
      "epoch:8 step:7957 [D loss: 0.229859, acc.: 59.38%] [G loss: 0.482024]\n",
      "epoch:8 step:7958 [D loss: 0.233779, acc.: 59.38%] [G loss: 0.495908]\n",
      "epoch:8 step:7959 [D loss: 0.236891, acc.: 56.25%] [G loss: 0.450112]\n",
      "epoch:8 step:7960 [D loss: 0.221142, acc.: 66.41%] [G loss: 0.429858]\n",
      "epoch:8 step:7961 [D loss: 0.258403, acc.: 54.69%] [G loss: 0.420210]\n",
      "epoch:8 step:7962 [D loss: 0.211574, acc.: 69.53%] [G loss: 0.481982]\n",
      "epoch:8 step:7963 [D loss: 0.245556, acc.: 60.16%] [G loss: 0.449093]\n",
      "epoch:8 step:7964 [D loss: 0.197494, acc.: 66.41%] [G loss: 0.482643]\n",
      "epoch:8 step:7965 [D loss: 0.215447, acc.: 60.16%] [G loss: 0.513844]\n",
      "epoch:8 step:7966 [D loss: 0.209778, acc.: 69.53%] [G loss: 0.488571]\n",
      "epoch:8 step:7967 [D loss: 0.178966, acc.: 75.78%] [G loss: 0.568526]\n",
      "epoch:8 step:7968 [D loss: 0.214329, acc.: 67.97%] [G loss: 0.537594]\n",
      "epoch:8 step:7969 [D loss: 0.248713, acc.: 57.81%] [G loss: 0.434703]\n",
      "epoch:8 step:7970 [D loss: 0.201784, acc.: 67.97%] [G loss: 0.483407]\n",
      "epoch:8 step:7971 [D loss: 0.188566, acc.: 71.09%] [G loss: 0.494917]\n",
      "epoch:8 step:7972 [D loss: 0.273866, acc.: 57.81%] [G loss: 0.461931]\n",
      "epoch:8 step:7973 [D loss: 0.274073, acc.: 47.66%] [G loss: 0.465013]\n",
      "epoch:8 step:7974 [D loss: 0.255424, acc.: 54.69%] [G loss: 0.421902]\n",
      "epoch:8 step:7975 [D loss: 0.197220, acc.: 68.75%] [G loss: 0.470748]\n",
      "epoch:8 step:7976 [D loss: 0.229784, acc.: 60.94%] [G loss: 0.429157]\n",
      "epoch:8 step:7977 [D loss: 0.199464, acc.: 70.31%] [G loss: 0.459057]\n",
      "epoch:8 step:7978 [D loss: 0.282691, acc.: 48.44%] [G loss: 0.441473]\n",
      "epoch:8 step:7979 [D loss: 0.246179, acc.: 60.16%] [G loss: 0.435829]\n",
      "epoch:8 step:7980 [D loss: 0.175183, acc.: 78.91%] [G loss: 0.510721]\n",
      "epoch:8 step:7981 [D loss: 0.215438, acc.: 68.75%] [G loss: 0.449317]\n",
      "epoch:8 step:7982 [D loss: 0.234494, acc.: 61.72%] [G loss: 0.465080]\n",
      "epoch:8 step:7983 [D loss: 0.208389, acc.: 67.19%] [G loss: 0.466466]\n",
      "epoch:8 step:7984 [D loss: 0.201947, acc.: 61.72%] [G loss: 0.517925]\n",
      "epoch:8 step:7985 [D loss: 0.230702, acc.: 64.84%] [G loss: 0.448001]\n",
      "epoch:8 step:7986 [D loss: 0.225018, acc.: 63.28%] [G loss: 0.477071]\n",
      "epoch:8 step:7987 [D loss: 0.240251, acc.: 65.62%] [G loss: 0.506836]\n",
      "epoch:8 step:7988 [D loss: 0.236565, acc.: 57.81%] [G loss: 0.493965]\n",
      "epoch:8 step:7989 [D loss: 0.231656, acc.: 59.38%] [G loss: 0.440720]\n",
      "epoch:8 step:7990 [D loss: 0.219976, acc.: 62.50%] [G loss: 0.471418]\n",
      "epoch:8 step:7991 [D loss: 0.180160, acc.: 76.56%] [G loss: 0.496480]\n",
      "epoch:8 step:7992 [D loss: 0.205661, acc.: 70.31%] [G loss: 0.494738]\n",
      "epoch:8 step:7993 [D loss: 0.225241, acc.: 63.28%] [G loss: 0.495621]\n",
      "epoch:8 step:7994 [D loss: 0.214147, acc.: 65.62%] [G loss: 0.502667]\n",
      "epoch:8 step:7995 [D loss: 0.216430, acc.: 67.19%] [G loss: 0.513856]\n",
      "epoch:8 step:7996 [D loss: 0.269667, acc.: 56.25%] [G loss: 0.455867]\n",
      "epoch:8 step:7997 [D loss: 0.289162, acc.: 54.69%] [G loss: 0.440172]\n",
      "epoch:8 step:7998 [D loss: 0.237313, acc.: 59.38%] [G loss: 0.454277]\n",
      "epoch:8 step:7999 [D loss: 0.220687, acc.: 57.81%] [G loss: 0.477682]\n",
      "epoch:8 step:8000 [D loss: 0.194233, acc.: 71.09%] [G loss: 0.483411]\n",
      "##############\n",
      "[2.61586429 1.7377057  6.21444571 4.68270356 3.82336086 5.84678606\n",
      " 4.50015775 4.54563694 4.63837333 3.86323406]\n",
      "##########\n",
      "epoch:8 step:8001 [D loss: 0.212231, acc.: 67.19%] [G loss: 0.497023]\n",
      "epoch:8 step:8002 [D loss: 0.225394, acc.: 60.94%] [G loss: 0.481387]\n",
      "epoch:8 step:8003 [D loss: 0.242563, acc.: 64.06%] [G loss: 0.456651]\n",
      "epoch:8 step:8004 [D loss: 0.212874, acc.: 64.84%] [G loss: 0.541803]\n",
      "epoch:8 step:8005 [D loss: 0.214604, acc.: 69.53%] [G loss: 0.485618]\n",
      "epoch:8 step:8006 [D loss: 0.240476, acc.: 62.50%] [G loss: 0.473254]\n",
      "epoch:8 step:8007 [D loss: 0.263000, acc.: 53.91%] [G loss: 0.446812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8008 [D loss: 0.247923, acc.: 53.91%] [G loss: 0.444606]\n",
      "epoch:8 step:8009 [D loss: 0.217969, acc.: 66.41%] [G loss: 0.526452]\n",
      "epoch:8 step:8010 [D loss: 0.200143, acc.: 64.84%] [G loss: 0.475291]\n",
      "epoch:8 step:8011 [D loss: 0.174548, acc.: 78.12%] [G loss: 0.524450]\n",
      "epoch:8 step:8012 [D loss: 0.225731, acc.: 71.09%] [G loss: 0.460235]\n",
      "epoch:8 step:8013 [D loss: 0.223102, acc.: 63.28%] [G loss: 0.484852]\n",
      "epoch:8 step:8014 [D loss: 0.240197, acc.: 63.28%] [G loss: 0.486165]\n",
      "epoch:8 step:8015 [D loss: 0.203897, acc.: 70.31%] [G loss: 0.503811]\n",
      "epoch:8 step:8016 [D loss: 0.183249, acc.: 74.22%] [G loss: 0.518075]\n",
      "epoch:8 step:8017 [D loss: 0.220537, acc.: 66.41%] [G loss: 0.480130]\n",
      "epoch:8 step:8018 [D loss: 0.202523, acc.: 64.84%] [G loss: 0.487249]\n",
      "epoch:8 step:8019 [D loss: 0.204348, acc.: 71.88%] [G loss: 0.463397]\n",
      "epoch:8 step:8020 [D loss: 0.225878, acc.: 61.72%] [G loss: 0.448909]\n",
      "epoch:8 step:8021 [D loss: 0.215772, acc.: 69.53%] [G loss: 0.459051]\n",
      "epoch:8 step:8022 [D loss: 0.212160, acc.: 69.53%] [G loss: 0.485184]\n",
      "epoch:8 step:8023 [D loss: 0.222324, acc.: 66.41%] [G loss: 0.510251]\n",
      "epoch:8 step:8024 [D loss: 0.288791, acc.: 53.91%] [G loss: 0.443136]\n",
      "epoch:8 step:8025 [D loss: 0.223436, acc.: 65.62%] [G loss: 0.490177]\n",
      "epoch:8 step:8026 [D loss: 0.197415, acc.: 67.97%] [G loss: 0.507862]\n",
      "epoch:8 step:8027 [D loss: 0.238132, acc.: 61.72%] [G loss: 0.484027]\n",
      "epoch:8 step:8028 [D loss: 0.230652, acc.: 64.06%] [G loss: 0.470540]\n",
      "epoch:8 step:8029 [D loss: 0.216668, acc.: 65.62%] [G loss: 0.521775]\n",
      "epoch:8 step:8030 [D loss: 0.184741, acc.: 71.88%] [G loss: 0.530944]\n",
      "epoch:8 step:8031 [D loss: 0.246128, acc.: 58.59%] [G loss: 0.455124]\n",
      "epoch:8 step:8032 [D loss: 0.196645, acc.: 67.97%] [G loss: 0.444772]\n",
      "epoch:8 step:8033 [D loss: 0.206239, acc.: 69.53%] [G loss: 0.513574]\n",
      "epoch:8 step:8034 [D loss: 0.244300, acc.: 64.06%] [G loss: 0.480484]\n",
      "epoch:8 step:8035 [D loss: 0.219804, acc.: 60.94%] [G loss: 0.472716]\n",
      "epoch:8 step:8036 [D loss: 0.245416, acc.: 57.03%] [G loss: 0.447059]\n",
      "epoch:8 step:8037 [D loss: 0.226867, acc.: 69.53%] [G loss: 0.495587]\n",
      "epoch:8 step:8038 [D loss: 0.251609, acc.: 62.50%] [G loss: 0.450076]\n",
      "epoch:8 step:8039 [D loss: 0.217520, acc.: 68.75%] [G loss: 0.458735]\n",
      "epoch:8 step:8040 [D loss: 0.240588, acc.: 59.38%] [G loss: 0.454182]\n",
      "epoch:8 step:8041 [D loss: 0.203818, acc.: 73.44%] [G loss: 0.485512]\n",
      "epoch:8 step:8042 [D loss: 0.231914, acc.: 60.94%] [G loss: 0.469069]\n",
      "epoch:8 step:8043 [D loss: 0.215380, acc.: 68.75%] [G loss: 0.473529]\n",
      "epoch:8 step:8044 [D loss: 0.192854, acc.: 71.88%] [G loss: 0.475119]\n",
      "epoch:8 step:8045 [D loss: 0.209227, acc.: 67.19%] [G loss: 0.502271]\n",
      "epoch:8 step:8046 [D loss: 0.187119, acc.: 72.66%] [G loss: 0.489265]\n",
      "epoch:8 step:8047 [D loss: 0.206729, acc.: 66.41%] [G loss: 0.500484]\n",
      "epoch:8 step:8048 [D loss: 0.212771, acc.: 63.28%] [G loss: 0.507859]\n",
      "epoch:8 step:8049 [D loss: 0.244080, acc.: 55.47%] [G loss: 0.476265]\n",
      "epoch:8 step:8050 [D loss: 0.193262, acc.: 73.44%] [G loss: 0.495783]\n",
      "epoch:8 step:8051 [D loss: 0.207744, acc.: 68.75%] [G loss: 0.490024]\n",
      "epoch:8 step:8052 [D loss: 0.202799, acc.: 71.88%] [G loss: 0.518733]\n",
      "epoch:8 step:8053 [D loss: 0.205633, acc.: 64.06%] [G loss: 0.439840]\n",
      "epoch:8 step:8054 [D loss: 0.194069, acc.: 71.09%] [G loss: 0.469147]\n",
      "epoch:8 step:8055 [D loss: 0.238801, acc.: 60.16%] [G loss: 0.470232]\n",
      "epoch:8 step:8056 [D loss: 0.229865, acc.: 60.16%] [G loss: 0.458998]\n",
      "epoch:8 step:8057 [D loss: 0.202471, acc.: 67.19%] [G loss: 0.499284]\n",
      "epoch:8 step:8058 [D loss: 0.248112, acc.: 53.91%] [G loss: 0.440392]\n",
      "epoch:8 step:8059 [D loss: 0.197454, acc.: 67.19%] [G loss: 0.505222]\n",
      "epoch:8 step:8060 [D loss: 0.198269, acc.: 68.75%] [G loss: 0.475482]\n",
      "epoch:8 step:8061 [D loss: 0.239288, acc.: 64.84%] [G loss: 0.523576]\n",
      "epoch:8 step:8062 [D loss: 0.248242, acc.: 54.69%] [G loss: 0.445526]\n",
      "epoch:8 step:8063 [D loss: 0.203256, acc.: 69.53%] [G loss: 0.476268]\n",
      "epoch:8 step:8064 [D loss: 0.203239, acc.: 67.97%] [G loss: 0.504843]\n",
      "epoch:8 step:8065 [D loss: 0.267031, acc.: 55.47%] [G loss: 0.418362]\n",
      "epoch:8 step:8066 [D loss: 0.223162, acc.: 64.84%] [G loss: 0.436173]\n",
      "epoch:8 step:8067 [D loss: 0.193447, acc.: 69.53%] [G loss: 0.445213]\n",
      "epoch:8 step:8068 [D loss: 0.201850, acc.: 74.22%] [G loss: 0.480342]\n",
      "epoch:8 step:8069 [D loss: 0.211689, acc.: 64.06%] [G loss: 0.468333]\n",
      "epoch:8 step:8070 [D loss: 0.195867, acc.: 69.53%] [G loss: 0.519952]\n",
      "epoch:8 step:8071 [D loss: 0.198205, acc.: 70.31%] [G loss: 0.490139]\n",
      "epoch:8 step:8072 [D loss: 0.233081, acc.: 60.94%] [G loss: 0.496846]\n",
      "epoch:8 step:8073 [D loss: 0.233241, acc.: 64.84%] [G loss: 0.411055]\n",
      "epoch:8 step:8074 [D loss: 0.212371, acc.: 65.62%] [G loss: 0.463536]\n",
      "epoch:8 step:8075 [D loss: 0.251148, acc.: 49.22%] [G loss: 0.447474]\n",
      "epoch:8 step:8076 [D loss: 0.228975, acc.: 57.81%] [G loss: 0.424377]\n",
      "epoch:8 step:8077 [D loss: 0.222701, acc.: 67.97%] [G loss: 0.455755]\n",
      "epoch:8 step:8078 [D loss: 0.190982, acc.: 70.31%] [G loss: 0.455835]\n",
      "epoch:8 step:8079 [D loss: 0.244539, acc.: 58.59%] [G loss: 0.477817]\n",
      "epoch:8 step:8080 [D loss: 0.227099, acc.: 66.41%] [G loss: 0.453492]\n",
      "epoch:8 step:8081 [D loss: 0.247880, acc.: 58.59%] [G loss: 0.465638]\n",
      "epoch:8 step:8082 [D loss: 0.228350, acc.: 66.41%] [G loss: 0.461110]\n",
      "epoch:8 step:8083 [D loss: 0.244819, acc.: 57.03%] [G loss: 0.429663]\n",
      "epoch:8 step:8084 [D loss: 0.227044, acc.: 65.62%] [G loss: 0.489416]\n",
      "epoch:8 step:8085 [D loss: 0.191255, acc.: 71.09%] [G loss: 0.512078]\n",
      "epoch:8 step:8086 [D loss: 0.217244, acc.: 64.84%] [G loss: 0.485596]\n",
      "epoch:8 step:8087 [D loss: 0.229653, acc.: 62.50%] [G loss: 0.473099]\n",
      "epoch:8 step:8088 [D loss: 0.188726, acc.: 74.22%] [G loss: 0.483455]\n",
      "epoch:8 step:8089 [D loss: 0.217230, acc.: 64.84%] [G loss: 0.507364]\n",
      "epoch:8 step:8090 [D loss: 0.234196, acc.: 63.28%] [G loss: 0.426664]\n",
      "epoch:8 step:8091 [D loss: 0.215835, acc.: 67.19%] [G loss: 0.463550]\n",
      "epoch:8 step:8092 [D loss: 0.255556, acc.: 57.81%] [G loss: 0.460205]\n",
      "epoch:8 step:8093 [D loss: 0.213875, acc.: 65.62%] [G loss: 0.479877]\n",
      "epoch:8 step:8094 [D loss: 0.201140, acc.: 72.66%] [G loss: 0.459371]\n",
      "epoch:8 step:8095 [D loss: 0.245092, acc.: 59.38%] [G loss: 0.443934]\n",
      "epoch:8 step:8096 [D loss: 0.245019, acc.: 59.38%] [G loss: 0.447975]\n",
      "epoch:8 step:8097 [D loss: 0.201531, acc.: 67.19%] [G loss: 0.464755]\n",
      "epoch:8 step:8098 [D loss: 0.226029, acc.: 64.06%] [G loss: 0.459185]\n",
      "epoch:8 step:8099 [D loss: 0.215240, acc.: 67.19%] [G loss: 0.483934]\n",
      "epoch:8 step:8100 [D loss: 0.240800, acc.: 58.59%] [G loss: 0.454188]\n",
      "epoch:8 step:8101 [D loss: 0.224211, acc.: 64.84%] [G loss: 0.463409]\n",
      "epoch:8 step:8102 [D loss: 0.258972, acc.: 55.47%] [G loss: 0.470515]\n",
      "epoch:8 step:8103 [D loss: 0.217825, acc.: 60.16%] [G loss: 0.437223]\n",
      "epoch:8 step:8104 [D loss: 0.203595, acc.: 71.09%] [G loss: 0.464249]\n",
      "epoch:8 step:8105 [D loss: 0.222640, acc.: 66.41%] [G loss: 0.463369]\n",
      "epoch:8 step:8106 [D loss: 0.242499, acc.: 56.25%] [G loss: 0.446286]\n",
      "epoch:8 step:8107 [D loss: 0.228859, acc.: 59.38%] [G loss: 0.424009]\n",
      "epoch:8 step:8108 [D loss: 0.228438, acc.: 58.59%] [G loss: 0.467755]\n",
      "epoch:8 step:8109 [D loss: 0.211932, acc.: 68.75%] [G loss: 0.450675]\n",
      "epoch:8 step:8110 [D loss: 0.236268, acc.: 58.59%] [G loss: 0.450969]\n",
      "epoch:8 step:8111 [D loss: 0.265485, acc.: 49.22%] [G loss: 0.401255]\n",
      "epoch:8 step:8112 [D loss: 0.257504, acc.: 58.59%] [G loss: 0.446417]\n",
      "epoch:8 step:8113 [D loss: 0.197973, acc.: 71.88%] [G loss: 0.485979]\n",
      "epoch:8 step:8114 [D loss: 0.223874, acc.: 66.41%] [G loss: 0.498781]\n",
      "epoch:8 step:8115 [D loss: 0.222493, acc.: 65.62%] [G loss: 0.465712]\n",
      "epoch:8 step:8116 [D loss: 0.204471, acc.: 67.19%] [G loss: 0.443511]\n",
      "epoch:8 step:8117 [D loss: 0.239027, acc.: 56.25%] [G loss: 0.411615]\n",
      "epoch:8 step:8118 [D loss: 0.254999, acc.: 54.69%] [G loss: 0.441533]\n",
      "epoch:8 step:8119 [D loss: 0.230076, acc.: 64.06%] [G loss: 0.465797]\n",
      "epoch:8 step:8120 [D loss: 0.179191, acc.: 75.78%] [G loss: 0.469146]\n",
      "epoch:8 step:8121 [D loss: 0.274406, acc.: 50.00%] [G loss: 0.440792]\n",
      "epoch:8 step:8122 [D loss: 0.244300, acc.: 61.72%] [G loss: 0.454342]\n",
      "epoch:8 step:8123 [D loss: 0.196706, acc.: 73.44%] [G loss: 0.503733]\n",
      "epoch:8 step:8124 [D loss: 0.235720, acc.: 63.28%] [G loss: 0.496748]\n",
      "epoch:8 step:8125 [D loss: 0.190616, acc.: 71.88%] [G loss: 0.458063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8126 [D loss: 0.225834, acc.: 60.94%] [G loss: 0.438603]\n",
      "epoch:8 step:8127 [D loss: 0.189228, acc.: 69.53%] [G loss: 0.466985]\n",
      "epoch:8 step:8128 [D loss: 0.186470, acc.: 74.22%] [G loss: 0.528771]\n",
      "epoch:8 step:8129 [D loss: 0.231619, acc.: 60.94%] [G loss: 0.488310]\n",
      "epoch:8 step:8130 [D loss: 0.189530, acc.: 71.88%] [G loss: 0.538157]\n",
      "epoch:8 step:8131 [D loss: 0.209037, acc.: 67.97%] [G loss: 0.527902]\n",
      "epoch:8 step:8132 [D loss: 0.239548, acc.: 57.81%] [G loss: 0.482414]\n",
      "epoch:8 step:8133 [D loss: 0.231804, acc.: 62.50%] [G loss: 0.469293]\n",
      "epoch:8 step:8134 [D loss: 0.213404, acc.: 64.84%] [G loss: 0.393807]\n",
      "epoch:8 step:8135 [D loss: 0.234803, acc.: 55.47%] [G loss: 0.436659]\n",
      "epoch:8 step:8136 [D loss: 0.206512, acc.: 66.41%] [G loss: 0.547707]\n",
      "epoch:8 step:8137 [D loss: 0.209113, acc.: 62.50%] [G loss: 0.541422]\n",
      "epoch:8 step:8138 [D loss: 0.182086, acc.: 75.00%] [G loss: 0.558143]\n",
      "epoch:8 step:8139 [D loss: 0.219763, acc.: 63.28%] [G loss: 0.470955]\n",
      "epoch:8 step:8140 [D loss: 0.222158, acc.: 66.41%] [G loss: 0.477555]\n",
      "epoch:8 step:8141 [D loss: 0.234018, acc.: 65.62%] [G loss: 0.428302]\n",
      "epoch:8 step:8142 [D loss: 0.222769, acc.: 60.94%] [G loss: 0.472847]\n",
      "epoch:8 step:8143 [D loss: 0.205877, acc.: 69.53%] [G loss: 0.535098]\n",
      "epoch:8 step:8144 [D loss: 0.219375, acc.: 68.75%] [G loss: 0.546790]\n",
      "epoch:8 step:8145 [D loss: 0.213503, acc.: 68.75%] [G loss: 0.535698]\n",
      "epoch:8 step:8146 [D loss: 0.211113, acc.: 71.09%] [G loss: 0.507235]\n",
      "epoch:8 step:8147 [D loss: 0.208116, acc.: 64.84%] [G loss: 0.459003]\n",
      "epoch:8 step:8148 [D loss: 0.221878, acc.: 65.62%] [G loss: 0.490299]\n",
      "epoch:8 step:8149 [D loss: 0.249965, acc.: 59.38%] [G loss: 0.470277]\n",
      "epoch:8 step:8150 [D loss: 0.189780, acc.: 71.88%] [G loss: 0.514357]\n",
      "epoch:8 step:8151 [D loss: 0.237819, acc.: 57.81%] [G loss: 0.441315]\n",
      "epoch:8 step:8152 [D loss: 0.229497, acc.: 63.28%] [G loss: 0.449970]\n",
      "epoch:8 step:8153 [D loss: 0.222742, acc.: 64.06%] [G loss: 0.503638]\n",
      "epoch:8 step:8154 [D loss: 0.219541, acc.: 63.28%] [G loss: 0.461594]\n",
      "epoch:8 step:8155 [D loss: 0.209006, acc.: 64.06%] [G loss: 0.454919]\n",
      "epoch:8 step:8156 [D loss: 0.225997, acc.: 64.06%] [G loss: 0.480663]\n",
      "epoch:8 step:8157 [D loss: 0.228745, acc.: 67.97%] [G loss: 0.464706]\n",
      "epoch:8 step:8158 [D loss: 0.230898, acc.: 63.28%] [G loss: 0.478905]\n",
      "epoch:8 step:8159 [D loss: 0.193333, acc.: 67.97%] [G loss: 0.495650]\n",
      "epoch:8 step:8160 [D loss: 0.217085, acc.: 62.50%] [G loss: 0.471988]\n",
      "epoch:8 step:8161 [D loss: 0.218809, acc.: 62.50%] [G loss: 0.492007]\n",
      "epoch:8 step:8162 [D loss: 0.253123, acc.: 64.84%] [G loss: 0.439267]\n",
      "epoch:8 step:8163 [D loss: 0.234609, acc.: 57.03%] [G loss: 0.447592]\n",
      "epoch:8 step:8164 [D loss: 0.214479, acc.: 69.53%] [G loss: 0.469370]\n",
      "epoch:8 step:8165 [D loss: 0.208424, acc.: 67.97%] [G loss: 0.478610]\n",
      "epoch:8 step:8166 [D loss: 0.238752, acc.: 63.28%] [G loss: 0.438140]\n",
      "epoch:8 step:8167 [D loss: 0.229083, acc.: 59.38%] [G loss: 0.447197]\n",
      "epoch:8 step:8168 [D loss: 0.228455, acc.: 59.38%] [G loss: 0.469662]\n",
      "epoch:8 step:8169 [D loss: 0.220786, acc.: 69.53%] [G loss: 0.471565]\n",
      "epoch:8 step:8170 [D loss: 0.221880, acc.: 67.19%] [G loss: 0.490499]\n",
      "epoch:8 step:8171 [D loss: 0.220496, acc.: 62.50%] [G loss: 0.486575]\n",
      "epoch:8 step:8172 [D loss: 0.225578, acc.: 67.97%] [G loss: 0.437313]\n",
      "epoch:8 step:8173 [D loss: 0.199279, acc.: 70.31%] [G loss: 0.456375]\n",
      "epoch:8 step:8174 [D loss: 0.207141, acc.: 69.53%] [G loss: 0.513317]\n",
      "epoch:8 step:8175 [D loss: 0.216901, acc.: 65.62%] [G loss: 0.488721]\n",
      "epoch:8 step:8176 [D loss: 0.222872, acc.: 65.62%] [G loss: 0.463330]\n",
      "epoch:8 step:8177 [D loss: 0.200321, acc.: 68.75%] [G loss: 0.474975]\n",
      "epoch:8 step:8178 [D loss: 0.206638, acc.: 67.97%] [G loss: 0.494596]\n",
      "epoch:8 step:8179 [D loss: 0.236890, acc.: 60.94%] [G loss: 0.439634]\n",
      "epoch:8 step:8180 [D loss: 0.209892, acc.: 65.62%] [G loss: 0.452965]\n",
      "epoch:8 step:8181 [D loss: 0.208234, acc.: 67.19%] [G loss: 0.454695]\n",
      "epoch:8 step:8182 [D loss: 0.227742, acc.: 64.06%] [G loss: 0.455765]\n",
      "epoch:8 step:8183 [D loss: 0.207823, acc.: 70.31%] [G loss: 0.445666]\n",
      "epoch:8 step:8184 [D loss: 0.212881, acc.: 71.09%] [G loss: 0.469344]\n",
      "epoch:8 step:8185 [D loss: 0.223099, acc.: 63.28%] [G loss: 0.482599]\n",
      "epoch:8 step:8186 [D loss: 0.191766, acc.: 72.66%] [G loss: 0.501351]\n",
      "epoch:8 step:8187 [D loss: 0.190472, acc.: 73.44%] [G loss: 0.527443]\n",
      "epoch:8 step:8188 [D loss: 0.194571, acc.: 70.31%] [G loss: 0.487606]\n",
      "epoch:8 step:8189 [D loss: 0.216001, acc.: 69.53%] [G loss: 0.428445]\n",
      "epoch:8 step:8190 [D loss: 0.192364, acc.: 71.09%] [G loss: 0.528379]\n",
      "epoch:8 step:8191 [D loss: 0.225284, acc.: 65.62%] [G loss: 0.495965]\n",
      "epoch:8 step:8192 [D loss: 0.246149, acc.: 56.25%] [G loss: 0.494377]\n",
      "epoch:8 step:8193 [D loss: 0.216801, acc.: 68.75%] [G loss: 0.475190]\n",
      "epoch:8 step:8194 [D loss: 0.201170, acc.: 67.19%] [G loss: 0.459082]\n",
      "epoch:8 step:8195 [D loss: 0.243548, acc.: 63.28%] [G loss: 0.457754]\n",
      "epoch:8 step:8196 [D loss: 0.209284, acc.: 65.62%] [G loss: 0.530188]\n",
      "epoch:8 step:8197 [D loss: 0.207640, acc.: 66.41%] [G loss: 0.562595]\n",
      "epoch:8 step:8198 [D loss: 0.269655, acc.: 52.34%] [G loss: 0.507072]\n",
      "epoch:8 step:8199 [D loss: 0.263722, acc.: 56.25%] [G loss: 0.430343]\n",
      "epoch:8 step:8200 [D loss: 0.261507, acc.: 53.91%] [G loss: 0.412705]\n",
      "##############\n",
      "[2.40637235 1.46348952 6.38609528 4.68664823 3.58639981 5.57435005\n",
      " 4.54616901 4.76572351 4.50784851 3.65367273]\n",
      "##########\n",
      "epoch:8 step:8201 [D loss: 0.212456, acc.: 71.09%] [G loss: 0.443100]\n",
      "epoch:8 step:8202 [D loss: 0.193719, acc.: 71.09%] [G loss: 0.507992]\n",
      "epoch:8 step:8203 [D loss: 0.249828, acc.: 58.59%] [G loss: 0.450790]\n",
      "epoch:8 step:8204 [D loss: 0.175564, acc.: 73.44%] [G loss: 0.512553]\n",
      "epoch:8 step:8205 [D loss: 0.196270, acc.: 69.53%] [G loss: 0.493084]\n",
      "epoch:8 step:8206 [D loss: 0.263342, acc.: 57.03%] [G loss: 0.441277]\n",
      "epoch:8 step:8207 [D loss: 0.218193, acc.: 67.97%] [G loss: 0.465874]\n",
      "epoch:8 step:8208 [D loss: 0.202932, acc.: 67.19%] [G loss: 0.481395]\n",
      "epoch:8 step:8209 [D loss: 0.250928, acc.: 57.03%] [G loss: 0.441873]\n",
      "epoch:8 step:8210 [D loss: 0.243507, acc.: 66.41%] [G loss: 0.497621]\n",
      "epoch:8 step:8211 [D loss: 0.246214, acc.: 59.38%] [G loss: 0.428293]\n",
      "epoch:8 step:8212 [D loss: 0.258739, acc.: 57.81%] [G loss: 0.474387]\n",
      "epoch:8 step:8213 [D loss: 0.238073, acc.: 64.06%] [G loss: 0.440580]\n",
      "epoch:8 step:8214 [D loss: 0.225566, acc.: 61.72%] [G loss: 0.449751]\n",
      "epoch:8 step:8215 [D loss: 0.225345, acc.: 58.59%] [G loss: 0.464383]\n",
      "epoch:8 step:8216 [D loss: 0.207511, acc.: 72.66%] [G loss: 0.483945]\n",
      "epoch:8 step:8217 [D loss: 0.219457, acc.: 60.94%] [G loss: 0.479375]\n",
      "epoch:8 step:8218 [D loss: 0.248807, acc.: 54.69%] [G loss: 0.409787]\n",
      "epoch:8 step:8219 [D loss: 0.211486, acc.: 67.97%] [G loss: 0.466238]\n",
      "epoch:8 step:8220 [D loss: 0.196355, acc.: 70.31%] [G loss: 0.450798]\n",
      "epoch:8 step:8221 [D loss: 0.200996, acc.: 71.88%] [G loss: 0.469364]\n",
      "epoch:8 step:8222 [D loss: 0.222453, acc.: 60.94%] [G loss: 0.473282]\n",
      "epoch:8 step:8223 [D loss: 0.224468, acc.: 64.06%] [G loss: 0.435732]\n",
      "epoch:8 step:8224 [D loss: 0.225094, acc.: 60.16%] [G loss: 0.450538]\n",
      "epoch:8 step:8225 [D loss: 0.222461, acc.: 64.06%] [G loss: 0.476736]\n",
      "epoch:8 step:8226 [D loss: 0.201888, acc.: 71.09%] [G loss: 0.478353]\n",
      "epoch:8 step:8227 [D loss: 0.207178, acc.: 70.31%] [G loss: 0.489064]\n",
      "epoch:8 step:8228 [D loss: 0.210539, acc.: 65.62%] [G loss: 0.492676]\n",
      "epoch:8 step:8229 [D loss: 0.217852, acc.: 65.62%] [G loss: 0.501253]\n",
      "epoch:8 step:8230 [D loss: 0.235546, acc.: 67.19%] [G loss: 0.495831]\n",
      "epoch:8 step:8231 [D loss: 0.262581, acc.: 54.69%] [G loss: 0.435792]\n",
      "epoch:8 step:8232 [D loss: 0.197554, acc.: 67.97%] [G loss: 0.529882]\n",
      "epoch:8 step:8233 [D loss: 0.203962, acc.: 68.75%] [G loss: 0.491892]\n",
      "epoch:8 step:8234 [D loss: 0.261665, acc.: 54.69%] [G loss: 0.413850]\n",
      "epoch:8 step:8235 [D loss: 0.261459, acc.: 54.69%] [G loss: 0.428038]\n",
      "epoch:8 step:8236 [D loss: 0.251002, acc.: 57.81%] [G loss: 0.446922]\n",
      "epoch:8 step:8237 [D loss: 0.257224, acc.: 61.72%] [G loss: 0.465436]\n",
      "epoch:8 step:8238 [D loss: 0.219707, acc.: 63.28%] [G loss: 0.452113]\n",
      "epoch:8 step:8239 [D loss: 0.237727, acc.: 59.38%] [G loss: 0.436000]\n",
      "epoch:8 step:8240 [D loss: 0.230811, acc.: 62.50%] [G loss: 0.489831]\n",
      "epoch:8 step:8241 [D loss: 0.229803, acc.: 58.59%] [G loss: 0.456054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8242 [D loss: 0.188716, acc.: 73.44%] [G loss: 0.487965]\n",
      "epoch:8 step:8243 [D loss: 0.230931, acc.: 68.75%] [G loss: 0.482660]\n",
      "epoch:8 step:8244 [D loss: 0.241697, acc.: 57.81%] [G loss: 0.434988]\n",
      "epoch:8 step:8245 [D loss: 0.239167, acc.: 58.59%] [G loss: 0.415728]\n",
      "epoch:8 step:8246 [D loss: 0.214091, acc.: 61.72%] [G loss: 0.452130]\n",
      "epoch:8 step:8247 [D loss: 0.208015, acc.: 65.62%] [G loss: 0.444335]\n",
      "epoch:8 step:8248 [D loss: 0.204409, acc.: 71.09%] [G loss: 0.468101]\n",
      "epoch:8 step:8249 [D loss: 0.205511, acc.: 65.62%] [G loss: 0.469572]\n",
      "epoch:8 step:8250 [D loss: 0.209403, acc.: 67.97%] [G loss: 0.501970]\n",
      "epoch:8 step:8251 [D loss: 0.211295, acc.: 65.62%] [G loss: 0.492757]\n",
      "epoch:8 step:8252 [D loss: 0.219188, acc.: 64.84%] [G loss: 0.475151]\n",
      "epoch:8 step:8253 [D loss: 0.267490, acc.: 54.69%] [G loss: 0.472225]\n",
      "epoch:8 step:8254 [D loss: 0.224089, acc.: 60.16%] [G loss: 0.461033]\n",
      "epoch:8 step:8255 [D loss: 0.239730, acc.: 60.94%] [G loss: 0.444214]\n",
      "epoch:8 step:8256 [D loss: 0.228828, acc.: 62.50%] [G loss: 0.450648]\n",
      "epoch:8 step:8257 [D loss: 0.241437, acc.: 60.16%] [G loss: 0.438331]\n",
      "epoch:8 step:8258 [D loss: 0.237617, acc.: 61.72%] [G loss: 0.461961]\n",
      "epoch:8 step:8259 [D loss: 0.224903, acc.: 64.06%] [G loss: 0.465315]\n",
      "epoch:8 step:8260 [D loss: 0.223835, acc.: 65.62%] [G loss: 0.467489]\n",
      "epoch:8 step:8261 [D loss: 0.278525, acc.: 53.12%] [G loss: 0.414043]\n",
      "epoch:8 step:8262 [D loss: 0.231197, acc.: 60.94%] [G loss: 0.473188]\n",
      "epoch:8 step:8263 [D loss: 0.203928, acc.: 63.28%] [G loss: 0.478193]\n",
      "epoch:8 step:8264 [D loss: 0.220326, acc.: 67.97%] [G loss: 0.502328]\n",
      "epoch:8 step:8265 [D loss: 0.226736, acc.: 62.50%] [G loss: 0.531397]\n",
      "epoch:8 step:8266 [D loss: 0.229946, acc.: 57.81%] [G loss: 0.478720]\n",
      "epoch:8 step:8267 [D loss: 0.226513, acc.: 60.94%] [G loss: 0.466664]\n",
      "epoch:8 step:8268 [D loss: 0.236633, acc.: 60.16%] [G loss: 0.466238]\n",
      "epoch:8 step:8269 [D loss: 0.192621, acc.: 69.53%] [G loss: 0.493365]\n",
      "epoch:8 step:8270 [D loss: 0.248796, acc.: 56.25%] [G loss: 0.460572]\n",
      "epoch:8 step:8271 [D loss: 0.209058, acc.: 65.62%] [G loss: 0.510719]\n",
      "epoch:8 step:8272 [D loss: 0.226290, acc.: 61.72%] [G loss: 0.457739]\n",
      "epoch:8 step:8273 [D loss: 0.206550, acc.: 67.19%] [G loss: 0.485854]\n",
      "epoch:8 step:8274 [D loss: 0.240392, acc.: 61.72%] [G loss: 0.450410]\n",
      "epoch:8 step:8275 [D loss: 0.216760, acc.: 66.41%] [G loss: 0.478217]\n",
      "epoch:8 step:8276 [D loss: 0.199442, acc.: 67.19%] [G loss: 0.452023]\n",
      "epoch:8 step:8277 [D loss: 0.205527, acc.: 66.41%] [G loss: 0.526122]\n",
      "epoch:8 step:8278 [D loss: 0.187913, acc.: 68.75%] [G loss: 0.552576]\n",
      "epoch:8 step:8279 [D loss: 0.223859, acc.: 63.28%] [G loss: 0.527819]\n",
      "epoch:8 step:8280 [D loss: 0.234366, acc.: 60.94%] [G loss: 0.437610]\n",
      "epoch:8 step:8281 [D loss: 0.221917, acc.: 58.59%] [G loss: 0.482656]\n",
      "epoch:8 step:8282 [D loss: 0.201610, acc.: 63.28%] [G loss: 0.496794]\n",
      "epoch:8 step:8283 [D loss: 0.274641, acc.: 54.69%] [G loss: 0.464744]\n",
      "epoch:8 step:8284 [D loss: 0.270370, acc.: 57.81%] [G loss: 0.394032]\n",
      "epoch:8 step:8285 [D loss: 0.233811, acc.: 56.25%] [G loss: 0.448766]\n",
      "epoch:8 step:8286 [D loss: 0.203825, acc.: 70.31%] [G loss: 0.492901]\n",
      "epoch:8 step:8287 [D loss: 0.274035, acc.: 54.69%] [G loss: 0.449474]\n",
      "epoch:8 step:8288 [D loss: 0.186173, acc.: 70.31%] [G loss: 0.476415]\n",
      "epoch:8 step:8289 [D loss: 0.195353, acc.: 70.31%] [G loss: 0.501234]\n",
      "epoch:8 step:8290 [D loss: 0.272502, acc.: 46.88%] [G loss: 0.432882]\n",
      "epoch:8 step:8291 [D loss: 0.242017, acc.: 61.72%] [G loss: 0.449567]\n",
      "epoch:8 step:8292 [D loss: 0.201480, acc.: 69.53%] [G loss: 0.529022]\n",
      "epoch:8 step:8293 [D loss: 0.238278, acc.: 62.50%] [G loss: 0.479047]\n",
      "epoch:8 step:8294 [D loss: 0.244069, acc.: 52.34%] [G loss: 0.441904]\n",
      "epoch:8 step:8295 [D loss: 0.228097, acc.: 65.62%] [G loss: 0.486208]\n",
      "epoch:8 step:8296 [D loss: 0.224232, acc.: 62.50%] [G loss: 0.503027]\n",
      "epoch:8 step:8297 [D loss: 0.206403, acc.: 64.84%] [G loss: 0.513945]\n",
      "epoch:8 step:8298 [D loss: 0.186756, acc.: 72.66%] [G loss: 0.518686]\n",
      "epoch:8 step:8299 [D loss: 0.198429, acc.: 67.19%] [G loss: 0.534770]\n",
      "epoch:8 step:8300 [D loss: 0.244055, acc.: 61.72%] [G loss: 0.403911]\n",
      "epoch:8 step:8301 [D loss: 0.233773, acc.: 57.03%] [G loss: 0.474453]\n",
      "epoch:8 step:8302 [D loss: 0.223018, acc.: 62.50%] [G loss: 0.484496]\n",
      "epoch:8 step:8303 [D loss: 0.215083, acc.: 62.50%] [G loss: 0.483944]\n",
      "epoch:8 step:8304 [D loss: 0.232812, acc.: 61.72%] [G loss: 0.417112]\n",
      "epoch:8 step:8305 [D loss: 0.219963, acc.: 64.84%] [G loss: 0.439875]\n",
      "epoch:8 step:8306 [D loss: 0.211544, acc.: 69.53%] [G loss: 0.453860]\n",
      "epoch:8 step:8307 [D loss: 0.216813, acc.: 71.88%] [G loss: 0.468386]\n",
      "epoch:8 step:8308 [D loss: 0.256952, acc.: 54.69%] [G loss: 0.423958]\n",
      "epoch:8 step:8309 [D loss: 0.217947, acc.: 71.09%] [G loss: 0.447810]\n",
      "epoch:8 step:8310 [D loss: 0.207632, acc.: 68.75%] [G loss: 0.444692]\n",
      "epoch:8 step:8311 [D loss: 0.198285, acc.: 71.09%] [G loss: 0.500207]\n",
      "epoch:8 step:8312 [D loss: 0.220673, acc.: 64.06%] [G loss: 0.530364]\n",
      "epoch:8 step:8313 [D loss: 0.254223, acc.: 53.91%] [G loss: 0.464347]\n",
      "epoch:8 step:8314 [D loss: 0.223077, acc.: 65.62%] [G loss: 0.456214]\n",
      "epoch:8 step:8315 [D loss: 0.217477, acc.: 66.41%] [G loss: 0.452920]\n",
      "epoch:8 step:8316 [D loss: 0.271756, acc.: 51.56%] [G loss: 0.437622]\n",
      "epoch:8 step:8317 [D loss: 0.219033, acc.: 68.75%] [G loss: 0.476658]\n",
      "epoch:8 step:8318 [D loss: 0.222332, acc.: 61.72%] [G loss: 0.452233]\n",
      "epoch:8 step:8319 [D loss: 0.186301, acc.: 74.22%] [G loss: 0.502707]\n",
      "epoch:8 step:8320 [D loss: 0.229811, acc.: 64.84%] [G loss: 0.461914]\n",
      "epoch:8 step:8321 [D loss: 0.233378, acc.: 56.25%] [G loss: 0.456856]\n",
      "epoch:8 step:8322 [D loss: 0.204714, acc.: 72.66%] [G loss: 0.516092]\n",
      "epoch:8 step:8323 [D loss: 0.238547, acc.: 57.03%] [G loss: 0.462680]\n",
      "epoch:8 step:8324 [D loss: 0.239343, acc.: 55.47%] [G loss: 0.413402]\n",
      "epoch:8 step:8325 [D loss: 0.234968, acc.: 60.94%] [G loss: 0.429047]\n",
      "epoch:8 step:8326 [D loss: 0.212564, acc.: 65.62%] [G loss: 0.469509]\n",
      "epoch:8 step:8327 [D loss: 0.227966, acc.: 64.84%] [G loss: 0.458184]\n",
      "epoch:8 step:8328 [D loss: 0.209243, acc.: 64.06%] [G loss: 0.460576]\n",
      "epoch:8 step:8329 [D loss: 0.193456, acc.: 69.53%] [G loss: 0.494499]\n",
      "epoch:8 step:8330 [D loss: 0.217442, acc.: 67.19%] [G loss: 0.437920]\n",
      "epoch:8 step:8331 [D loss: 0.200165, acc.: 70.31%] [G loss: 0.492951]\n",
      "epoch:8 step:8332 [D loss: 0.205735, acc.: 68.75%] [G loss: 0.477433]\n",
      "epoch:8 step:8333 [D loss: 0.215342, acc.: 67.97%] [G loss: 0.446093]\n",
      "epoch:8 step:8334 [D loss: 0.190368, acc.: 69.53%] [G loss: 0.466327]\n",
      "epoch:8 step:8335 [D loss: 0.223141, acc.: 62.50%] [G loss: 0.434649]\n",
      "epoch:8 step:8336 [D loss: 0.205162, acc.: 65.62%] [G loss: 0.456247]\n",
      "epoch:8 step:8337 [D loss: 0.208733, acc.: 67.19%] [G loss: 0.470156]\n",
      "epoch:8 step:8338 [D loss: 0.190334, acc.: 71.09%] [G loss: 0.528811]\n",
      "epoch:8 step:8339 [D loss: 0.226305, acc.: 67.97%] [G loss: 0.504588]\n",
      "epoch:8 step:8340 [D loss: 0.213172, acc.: 67.19%] [G loss: 0.484852]\n",
      "epoch:8 step:8341 [D loss: 0.215417, acc.: 66.41%] [G loss: 0.479075]\n",
      "epoch:8 step:8342 [D loss: 0.228093, acc.: 57.03%] [G loss: 0.489425]\n",
      "epoch:8 step:8343 [D loss: 0.221265, acc.: 60.94%] [G loss: 0.461654]\n",
      "epoch:8 step:8344 [D loss: 0.220213, acc.: 63.28%] [G loss: 0.480133]\n",
      "epoch:8 step:8345 [D loss: 0.215482, acc.: 67.19%] [G loss: 0.447385]\n",
      "epoch:8 step:8346 [D loss: 0.220625, acc.: 69.53%] [G loss: 0.483986]\n",
      "epoch:8 step:8347 [D loss: 0.213982, acc.: 67.19%] [G loss: 0.459624]\n",
      "epoch:8 step:8348 [D loss: 0.227292, acc.: 60.94%] [G loss: 0.428962]\n",
      "epoch:8 step:8349 [D loss: 0.197521, acc.: 68.75%] [G loss: 0.499087]\n",
      "epoch:8 step:8350 [D loss: 0.207127, acc.: 67.19%] [G loss: 0.513770]\n",
      "epoch:8 step:8351 [D loss: 0.212317, acc.: 68.75%] [G loss: 0.491560]\n",
      "epoch:8 step:8352 [D loss: 0.244310, acc.: 59.38%] [G loss: 0.482392]\n",
      "epoch:8 step:8353 [D loss: 0.193854, acc.: 72.66%] [G loss: 0.450325]\n",
      "epoch:8 step:8354 [D loss: 0.269137, acc.: 46.88%] [G loss: 0.471703]\n",
      "epoch:8 step:8355 [D loss: 0.216845, acc.: 62.50%] [G loss: 0.484420]\n",
      "epoch:8 step:8356 [D loss: 0.185513, acc.: 67.97%] [G loss: 0.535946]\n",
      "epoch:8 step:8357 [D loss: 0.259679, acc.: 57.81%] [G loss: 0.489968]\n",
      "epoch:8 step:8358 [D loss: 0.215944, acc.: 64.84%] [G loss: 0.438515]\n",
      "epoch:8 step:8359 [D loss: 0.221941, acc.: 62.50%] [G loss: 0.469548]\n",
      "epoch:8 step:8360 [D loss: 0.223257, acc.: 63.28%] [G loss: 0.431838]\n",
      "epoch:8 step:8361 [D loss: 0.226464, acc.: 63.28%] [G loss: 0.441544]\n",
      "epoch:8 step:8362 [D loss: 0.222535, acc.: 63.28%] [G loss: 0.440833]\n",
      "epoch:8 step:8363 [D loss: 0.251667, acc.: 49.22%] [G loss: 0.439529]\n",
      "epoch:8 step:8364 [D loss: 0.213151, acc.: 69.53%] [G loss: 0.463025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8365 [D loss: 0.220768, acc.: 65.62%] [G loss: 0.462342]\n",
      "epoch:8 step:8366 [D loss: 0.207831, acc.: 71.09%] [G loss: 0.515163]\n",
      "epoch:8 step:8367 [D loss: 0.220724, acc.: 65.62%] [G loss: 0.471566]\n",
      "epoch:8 step:8368 [D loss: 0.183113, acc.: 73.44%] [G loss: 0.522303]\n",
      "epoch:8 step:8369 [D loss: 0.220892, acc.: 66.41%] [G loss: 0.437646]\n",
      "epoch:8 step:8370 [D loss: 0.204155, acc.: 64.84%] [G loss: 0.512021]\n",
      "epoch:8 step:8371 [D loss: 0.192210, acc.: 71.09%] [G loss: 0.519468]\n",
      "epoch:8 step:8372 [D loss: 0.224130, acc.: 66.41%] [G loss: 0.462146]\n",
      "epoch:8 step:8373 [D loss: 0.224375, acc.: 64.84%] [G loss: 0.455484]\n",
      "epoch:8 step:8374 [D loss: 0.215693, acc.: 60.16%] [G loss: 0.466097]\n",
      "epoch:8 step:8375 [D loss: 0.236742, acc.: 63.28%] [G loss: 0.440499]\n",
      "epoch:8 step:8376 [D loss: 0.243486, acc.: 57.81%] [G loss: 0.446423]\n",
      "epoch:8 step:8377 [D loss: 0.225084, acc.: 64.06%] [G loss: 0.435062]\n",
      "epoch:8 step:8378 [D loss: 0.233297, acc.: 56.25%] [G loss: 0.479711]\n",
      "epoch:8 step:8379 [D loss: 0.254657, acc.: 59.38%] [G loss: 0.468494]\n",
      "epoch:8 step:8380 [D loss: 0.197504, acc.: 67.19%] [G loss: 0.496542]\n",
      "epoch:8 step:8381 [D loss: 0.195035, acc.: 69.53%] [G loss: 0.525808]\n",
      "epoch:8 step:8382 [D loss: 0.187346, acc.: 72.66%] [G loss: 0.568407]\n",
      "epoch:8 step:8383 [D loss: 0.248667, acc.: 59.38%] [G loss: 0.483623]\n",
      "epoch:8 step:8384 [D loss: 0.216488, acc.: 69.53%] [G loss: 0.455094]\n",
      "epoch:8 step:8385 [D loss: 0.210577, acc.: 66.41%] [G loss: 0.495019]\n",
      "epoch:8 step:8386 [D loss: 0.191496, acc.: 72.66%] [G loss: 0.541832]\n",
      "epoch:8 step:8387 [D loss: 0.265958, acc.: 52.34%] [G loss: 0.485927]\n",
      "epoch:8 step:8388 [D loss: 0.265410, acc.: 50.78%] [G loss: 0.451605]\n",
      "epoch:8 step:8389 [D loss: 0.195433, acc.: 73.44%] [G loss: 0.487273]\n",
      "epoch:8 step:8390 [D loss: 0.215812, acc.: 69.53%] [G loss: 0.466274]\n",
      "epoch:8 step:8391 [D loss: 0.208710, acc.: 64.84%] [G loss: 0.499573]\n",
      "epoch:8 step:8392 [D loss: 0.231328, acc.: 58.59%] [G loss: 0.442588]\n",
      "epoch:8 step:8393 [D loss: 0.208701, acc.: 68.75%] [G loss: 0.457802]\n",
      "epoch:8 step:8394 [D loss: 0.237024, acc.: 64.84%] [G loss: 0.484958]\n",
      "epoch:8 step:8395 [D loss: 0.184604, acc.: 77.34%] [G loss: 0.531235]\n",
      "epoch:8 step:8396 [D loss: 0.200383, acc.: 72.66%] [G loss: 0.473948]\n",
      "epoch:8 step:8397 [D loss: 0.209126, acc.: 64.06%] [G loss: 0.490219]\n",
      "epoch:8 step:8398 [D loss: 0.261078, acc.: 54.69%] [G loss: 0.442279]\n",
      "epoch:8 step:8399 [D loss: 0.215916, acc.: 64.84%] [G loss: 0.450522]\n",
      "epoch:8 step:8400 [D loss: 0.245738, acc.: 58.59%] [G loss: 0.472536]\n",
      "##############\n",
      "[2.5420945  1.97131538 6.04233005 4.74698371 3.62360189 5.79114028\n",
      " 4.6450942  4.4829144  4.56844189 3.96001217]\n",
      "##########\n",
      "epoch:8 step:8401 [D loss: 0.209365, acc.: 64.84%] [G loss: 0.510656]\n",
      "epoch:8 step:8402 [D loss: 0.196388, acc.: 71.09%] [G loss: 0.505903]\n",
      "epoch:8 step:8403 [D loss: 0.249915, acc.: 56.25%] [G loss: 0.463962]\n",
      "epoch:8 step:8404 [D loss: 0.203309, acc.: 68.75%] [G loss: 0.475554]\n",
      "epoch:8 step:8405 [D loss: 0.213068, acc.: 68.75%] [G loss: 0.451568]\n",
      "epoch:8 step:8406 [D loss: 0.217677, acc.: 66.41%] [G loss: 0.492649]\n",
      "epoch:8 step:8407 [D loss: 0.206915, acc.: 65.62%] [G loss: 0.516487]\n",
      "epoch:8 step:8408 [D loss: 0.171520, acc.: 77.34%] [G loss: 0.508498]\n",
      "epoch:8 step:8409 [D loss: 0.254732, acc.: 57.81%] [G loss: 0.517160]\n",
      "epoch:8 step:8410 [D loss: 0.240840, acc.: 61.72%] [G loss: 0.475403]\n",
      "epoch:8 step:8411 [D loss: 0.251453, acc.: 60.16%] [G loss: 0.463376]\n",
      "epoch:8 step:8412 [D loss: 0.234882, acc.: 60.16%] [G loss: 0.434776]\n",
      "epoch:8 step:8413 [D loss: 0.225189, acc.: 62.50%] [G loss: 0.495141]\n",
      "epoch:8 step:8414 [D loss: 0.186932, acc.: 72.66%] [G loss: 0.586695]\n",
      "epoch:8 step:8415 [D loss: 0.219747, acc.: 68.75%] [G loss: 0.523055]\n",
      "epoch:8 step:8416 [D loss: 0.287933, acc.: 53.91%] [G loss: 0.445221]\n",
      "epoch:8 step:8417 [D loss: 0.195034, acc.: 69.53%] [G loss: 0.492902]\n",
      "epoch:8 step:8418 [D loss: 0.211048, acc.: 68.75%] [G loss: 0.445346]\n",
      "epoch:8 step:8419 [D loss: 0.236728, acc.: 63.28%] [G loss: 0.477737]\n",
      "epoch:8 step:8420 [D loss: 0.175796, acc.: 77.34%] [G loss: 0.523867]\n",
      "epoch:8 step:8421 [D loss: 0.158802, acc.: 80.47%] [G loss: 0.526997]\n",
      "epoch:8 step:8422 [D loss: 0.173502, acc.: 78.91%] [G loss: 0.556918]\n",
      "epoch:8 step:8423 [D loss: 0.184302, acc.: 70.31%] [G loss: 0.573577]\n",
      "epoch:8 step:8424 [D loss: 0.315880, acc.: 59.38%] [G loss: 0.567409]\n",
      "epoch:8 step:8425 [D loss: 0.224240, acc.: 67.97%] [G loss: 0.580014]\n",
      "epoch:8 step:8426 [D loss: 0.200441, acc.: 64.06%] [G loss: 0.515350]\n",
      "epoch:8 step:8427 [D loss: 0.275933, acc.: 53.91%] [G loss: 0.453691]\n",
      "epoch:8 step:8428 [D loss: 0.245460, acc.: 61.72%] [G loss: 0.457070]\n",
      "epoch:8 step:8429 [D loss: 0.186029, acc.: 75.00%] [G loss: 0.528514]\n",
      "epoch:8 step:8430 [D loss: 0.231148, acc.: 63.28%] [G loss: 0.505292]\n",
      "epoch:8 step:8431 [D loss: 0.221291, acc.: 62.50%] [G loss: 0.497039]\n",
      "epoch:8 step:8432 [D loss: 0.148883, acc.: 78.91%] [G loss: 0.558437]\n",
      "epoch:8 step:8433 [D loss: 0.180777, acc.: 75.00%] [G loss: 0.537760]\n",
      "epoch:9 step:8434 [D loss: 0.259275, acc.: 56.25%] [G loss: 0.552431]\n",
      "epoch:9 step:8435 [D loss: 0.243221, acc.: 60.94%] [G loss: 0.498766]\n",
      "epoch:9 step:8436 [D loss: 0.234633, acc.: 59.38%] [G loss: 0.501757]\n",
      "epoch:9 step:8437 [D loss: 0.257693, acc.: 60.94%] [G loss: 0.412177]\n",
      "epoch:9 step:8438 [D loss: 0.219204, acc.: 62.50%] [G loss: 0.490175]\n",
      "epoch:9 step:8439 [D loss: 0.198488, acc.: 66.41%] [G loss: 0.486992]\n",
      "epoch:9 step:8440 [D loss: 0.214279, acc.: 65.62%] [G loss: 0.502545]\n",
      "epoch:9 step:8441 [D loss: 0.211941, acc.: 67.19%] [G loss: 0.485549]\n",
      "epoch:9 step:8442 [D loss: 0.187934, acc.: 74.22%] [G loss: 0.493397]\n",
      "epoch:9 step:8443 [D loss: 0.221830, acc.: 64.84%] [G loss: 0.492339]\n",
      "epoch:9 step:8444 [D loss: 0.196669, acc.: 70.31%] [G loss: 0.511614]\n",
      "epoch:9 step:8445 [D loss: 0.231490, acc.: 60.94%] [G loss: 0.535622]\n",
      "epoch:9 step:8446 [D loss: 0.201202, acc.: 66.41%] [G loss: 0.487065]\n",
      "epoch:9 step:8447 [D loss: 0.206301, acc.: 63.28%] [G loss: 0.431020]\n",
      "epoch:9 step:8448 [D loss: 0.195415, acc.: 72.66%] [G loss: 0.534648]\n",
      "epoch:9 step:8449 [D loss: 0.192833, acc.: 71.09%] [G loss: 0.508853]\n",
      "epoch:9 step:8450 [D loss: 0.234927, acc.: 60.16%] [G loss: 0.488829]\n",
      "epoch:9 step:8451 [D loss: 0.248861, acc.: 57.03%] [G loss: 0.465304]\n",
      "epoch:9 step:8452 [D loss: 0.242983, acc.: 60.16%] [G loss: 0.496980]\n",
      "epoch:9 step:8453 [D loss: 0.276694, acc.: 48.44%] [G loss: 0.449775]\n",
      "epoch:9 step:8454 [D loss: 0.234520, acc.: 60.16%] [G loss: 0.506100]\n",
      "epoch:9 step:8455 [D loss: 0.206724, acc.: 67.19%] [G loss: 0.522177]\n",
      "epoch:9 step:8456 [D loss: 0.213121, acc.: 69.53%] [G loss: 0.488784]\n",
      "epoch:9 step:8457 [D loss: 0.227999, acc.: 65.62%] [G loss: 0.426985]\n",
      "epoch:9 step:8458 [D loss: 0.209620, acc.: 62.50%] [G loss: 0.460390]\n",
      "epoch:9 step:8459 [D loss: 0.226612, acc.: 60.16%] [G loss: 0.445910]\n",
      "epoch:9 step:8460 [D loss: 0.236980, acc.: 55.47%] [G loss: 0.432830]\n",
      "epoch:9 step:8461 [D loss: 0.210632, acc.: 65.62%] [G loss: 0.498503]\n",
      "epoch:9 step:8462 [D loss: 0.203891, acc.: 66.41%] [G loss: 0.495665]\n",
      "epoch:9 step:8463 [D loss: 0.221507, acc.: 60.16%] [G loss: 0.491453]\n",
      "epoch:9 step:8464 [D loss: 0.225946, acc.: 64.06%] [G loss: 0.444841]\n",
      "epoch:9 step:8465 [D loss: 0.230427, acc.: 57.81%] [G loss: 0.436391]\n",
      "epoch:9 step:8466 [D loss: 0.214074, acc.: 68.75%] [G loss: 0.446466]\n",
      "epoch:9 step:8467 [D loss: 0.227517, acc.: 57.81%] [G loss: 0.443869]\n",
      "epoch:9 step:8468 [D loss: 0.198943, acc.: 71.09%] [G loss: 0.517704]\n",
      "epoch:9 step:8469 [D loss: 0.228279, acc.: 62.50%] [G loss: 0.464701]\n",
      "epoch:9 step:8470 [D loss: 0.235744, acc.: 57.81%] [G loss: 0.459319]\n",
      "epoch:9 step:8471 [D loss: 0.234612, acc.: 57.03%] [G loss: 0.457813]\n",
      "epoch:9 step:8472 [D loss: 0.207420, acc.: 68.75%] [G loss: 0.462230]\n",
      "epoch:9 step:8473 [D loss: 0.189035, acc.: 73.44%] [G loss: 0.494362]\n",
      "epoch:9 step:8474 [D loss: 0.244191, acc.: 60.16%] [G loss: 0.459811]\n",
      "epoch:9 step:8475 [D loss: 0.226497, acc.: 63.28%] [G loss: 0.450115]\n",
      "epoch:9 step:8476 [D loss: 0.217412, acc.: 64.06%] [G loss: 0.459119]\n",
      "epoch:9 step:8477 [D loss: 0.243780, acc.: 57.81%] [G loss: 0.446982]\n",
      "epoch:9 step:8478 [D loss: 0.198440, acc.: 71.88%] [G loss: 0.466909]\n",
      "epoch:9 step:8479 [D loss: 0.218661, acc.: 59.38%] [G loss: 0.431441]\n",
      "epoch:9 step:8480 [D loss: 0.224239, acc.: 66.41%] [G loss: 0.441512]\n",
      "epoch:9 step:8481 [D loss: 0.224087, acc.: 61.72%] [G loss: 0.475884]\n",
      "epoch:9 step:8482 [D loss: 0.198572, acc.: 66.41%] [G loss: 0.510217]\n",
      "epoch:9 step:8483 [D loss: 0.192468, acc.: 73.44%] [G loss: 0.512446]\n",
      "epoch:9 step:8484 [D loss: 0.218101, acc.: 66.41%] [G loss: 0.472541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8485 [D loss: 0.215312, acc.: 69.53%] [G loss: 0.469302]\n",
      "epoch:9 step:8486 [D loss: 0.195232, acc.: 75.78%] [G loss: 0.529350]\n",
      "epoch:9 step:8487 [D loss: 0.222630, acc.: 64.06%] [G loss: 0.478376]\n",
      "epoch:9 step:8488 [D loss: 0.218001, acc.: 65.62%] [G loss: 0.516836]\n",
      "epoch:9 step:8489 [D loss: 0.235165, acc.: 63.28%] [G loss: 0.482563]\n",
      "epoch:9 step:8490 [D loss: 0.215142, acc.: 63.28%] [G loss: 0.424206]\n",
      "epoch:9 step:8491 [D loss: 0.206950, acc.: 69.53%] [G loss: 0.485748]\n",
      "epoch:9 step:8492 [D loss: 0.223024, acc.: 64.06%] [G loss: 0.431884]\n",
      "epoch:9 step:8493 [D loss: 0.243028, acc.: 58.59%] [G loss: 0.438779]\n",
      "epoch:9 step:8494 [D loss: 0.257718, acc.: 57.03%] [G loss: 0.457348]\n",
      "epoch:9 step:8495 [D loss: 0.223977, acc.: 62.50%] [G loss: 0.452522]\n",
      "epoch:9 step:8496 [D loss: 0.241664, acc.: 61.72%] [G loss: 0.434775]\n",
      "epoch:9 step:8497 [D loss: 0.224146, acc.: 67.19%] [G loss: 0.427748]\n",
      "epoch:9 step:8498 [D loss: 0.216573, acc.: 64.06%] [G loss: 0.459158]\n",
      "epoch:9 step:8499 [D loss: 0.216304, acc.: 63.28%] [G loss: 0.463894]\n",
      "epoch:9 step:8500 [D loss: 0.227555, acc.: 59.38%] [G loss: 0.464348]\n",
      "epoch:9 step:8501 [D loss: 0.181770, acc.: 70.31%] [G loss: 0.492374]\n",
      "epoch:9 step:8502 [D loss: 0.203060, acc.: 67.19%] [G loss: 0.448297]\n",
      "epoch:9 step:8503 [D loss: 0.195357, acc.: 68.75%] [G loss: 0.528813]\n",
      "epoch:9 step:8504 [D loss: 0.240155, acc.: 61.72%] [G loss: 0.440573]\n",
      "epoch:9 step:8505 [D loss: 0.214150, acc.: 66.41%] [G loss: 0.481775]\n",
      "epoch:9 step:8506 [D loss: 0.255409, acc.: 59.38%] [G loss: 0.413947]\n",
      "epoch:9 step:8507 [D loss: 0.196270, acc.: 71.09%] [G loss: 0.486136]\n",
      "epoch:9 step:8508 [D loss: 0.217551, acc.: 61.72%] [G loss: 0.496883]\n",
      "epoch:9 step:8509 [D loss: 0.201003, acc.: 69.53%] [G loss: 0.494646]\n",
      "epoch:9 step:8510 [D loss: 0.186968, acc.: 74.22%] [G loss: 0.493900]\n",
      "epoch:9 step:8511 [D loss: 0.258595, acc.: 58.59%] [G loss: 0.444398]\n",
      "epoch:9 step:8512 [D loss: 0.234433, acc.: 60.94%] [G loss: 0.416562]\n",
      "epoch:9 step:8513 [D loss: 0.209173, acc.: 67.97%] [G loss: 0.453427]\n",
      "epoch:9 step:8514 [D loss: 0.236891, acc.: 60.94%] [G loss: 0.460474]\n",
      "epoch:9 step:8515 [D loss: 0.194764, acc.: 71.88%] [G loss: 0.493842]\n",
      "epoch:9 step:8516 [D loss: 0.199474, acc.: 69.53%] [G loss: 0.506711]\n",
      "epoch:9 step:8517 [D loss: 0.217398, acc.: 64.06%] [G loss: 0.510676]\n",
      "epoch:9 step:8518 [D loss: 0.230919, acc.: 60.94%] [G loss: 0.477479]\n",
      "epoch:9 step:8519 [D loss: 0.225804, acc.: 64.06%] [G loss: 0.453226]\n",
      "epoch:9 step:8520 [D loss: 0.230906, acc.: 64.06%] [G loss: 0.466112]\n",
      "epoch:9 step:8521 [D loss: 0.203039, acc.: 69.53%] [G loss: 0.476918]\n",
      "epoch:9 step:8522 [D loss: 0.212024, acc.: 67.19%] [G loss: 0.463567]\n",
      "epoch:9 step:8523 [D loss: 0.215850, acc.: 64.06%] [G loss: 0.458717]\n",
      "epoch:9 step:8524 [D loss: 0.203218, acc.: 68.75%] [G loss: 0.476069]\n",
      "epoch:9 step:8525 [D loss: 0.203809, acc.: 61.72%] [G loss: 0.423174]\n",
      "epoch:9 step:8526 [D loss: 0.210220, acc.: 67.97%] [G loss: 0.469010]\n",
      "epoch:9 step:8527 [D loss: 0.245120, acc.: 63.28%] [G loss: 0.413791]\n",
      "epoch:9 step:8528 [D loss: 0.198866, acc.: 65.62%] [G loss: 0.472180]\n",
      "epoch:9 step:8529 [D loss: 0.194514, acc.: 68.75%] [G loss: 0.494481]\n",
      "epoch:9 step:8530 [D loss: 0.191482, acc.: 70.31%] [G loss: 0.470592]\n",
      "epoch:9 step:8531 [D loss: 0.240011, acc.: 59.38%] [G loss: 0.441838]\n",
      "epoch:9 step:8532 [D loss: 0.218015, acc.: 67.97%] [G loss: 0.488584]\n",
      "epoch:9 step:8533 [D loss: 0.194317, acc.: 71.09%] [G loss: 0.531779]\n",
      "epoch:9 step:8534 [D loss: 0.234706, acc.: 62.50%] [G loss: 0.486856]\n",
      "epoch:9 step:8535 [D loss: 0.232990, acc.: 59.38%] [G loss: 0.465501]\n",
      "epoch:9 step:8536 [D loss: 0.254909, acc.: 57.81%] [G loss: 0.403146]\n",
      "epoch:9 step:8537 [D loss: 0.199702, acc.: 68.75%] [G loss: 0.454886]\n",
      "epoch:9 step:8538 [D loss: 0.231775, acc.: 57.03%] [G loss: 0.449995]\n",
      "epoch:9 step:8539 [D loss: 0.214017, acc.: 61.72%] [G loss: 0.471627]\n",
      "epoch:9 step:8540 [D loss: 0.207708, acc.: 67.97%] [G loss: 0.482398]\n",
      "epoch:9 step:8541 [D loss: 0.266618, acc.: 54.69%] [G loss: 0.447166]\n",
      "epoch:9 step:8542 [D loss: 0.257969, acc.: 57.03%] [G loss: 0.479260]\n",
      "epoch:9 step:8543 [D loss: 0.224465, acc.: 64.84%] [G loss: 0.471079]\n",
      "epoch:9 step:8544 [D loss: 0.200141, acc.: 71.09%] [G loss: 0.467916]\n",
      "epoch:9 step:8545 [D loss: 0.193042, acc.: 69.53%] [G loss: 0.493877]\n",
      "epoch:9 step:8546 [D loss: 0.201533, acc.: 67.97%] [G loss: 0.477327]\n",
      "epoch:9 step:8547 [D loss: 0.216224, acc.: 62.50%] [G loss: 0.495905]\n",
      "epoch:9 step:8548 [D loss: 0.202003, acc.: 74.22%] [G loss: 0.527780]\n",
      "epoch:9 step:8549 [D loss: 0.210034, acc.: 64.06%] [G loss: 0.500435]\n",
      "epoch:9 step:8550 [D loss: 0.196375, acc.: 67.97%] [G loss: 0.481454]\n",
      "epoch:9 step:8551 [D loss: 0.217717, acc.: 69.53%] [G loss: 0.450474]\n",
      "epoch:9 step:8552 [D loss: 0.169951, acc.: 77.34%] [G loss: 0.570385]\n",
      "epoch:9 step:8553 [D loss: 0.260408, acc.: 57.81%] [G loss: 0.460723]\n",
      "epoch:9 step:8554 [D loss: 0.203383, acc.: 64.84%] [G loss: 0.486464]\n",
      "epoch:9 step:8555 [D loss: 0.210560, acc.: 68.75%] [G loss: 0.491511]\n",
      "epoch:9 step:8556 [D loss: 0.207909, acc.: 67.97%] [G loss: 0.523666]\n",
      "epoch:9 step:8557 [D loss: 0.219296, acc.: 67.19%] [G loss: 0.466938]\n",
      "epoch:9 step:8558 [D loss: 0.220494, acc.: 61.72%] [G loss: 0.472173]\n",
      "epoch:9 step:8559 [D loss: 0.187210, acc.: 71.88%] [G loss: 0.459777]\n",
      "epoch:9 step:8560 [D loss: 0.229900, acc.: 61.72%] [G loss: 0.457646]\n",
      "epoch:9 step:8561 [D loss: 0.239491, acc.: 60.94%] [G loss: 0.456103]\n",
      "epoch:9 step:8562 [D loss: 0.235732, acc.: 59.38%] [G loss: 0.449544]\n",
      "epoch:9 step:8563 [D loss: 0.201547, acc.: 66.41%] [G loss: 0.455835]\n",
      "epoch:9 step:8564 [D loss: 0.237788, acc.: 63.28%] [G loss: 0.453700]\n",
      "epoch:9 step:8565 [D loss: 0.209606, acc.: 73.44%] [G loss: 0.473798]\n",
      "epoch:9 step:8566 [D loss: 0.222383, acc.: 64.06%] [G loss: 0.480728]\n",
      "epoch:9 step:8567 [D loss: 0.215770, acc.: 67.19%] [G loss: 0.473914]\n",
      "epoch:9 step:8568 [D loss: 0.220126, acc.: 66.41%] [G loss: 0.453827]\n",
      "epoch:9 step:8569 [D loss: 0.228642, acc.: 60.94%] [G loss: 0.471689]\n",
      "epoch:9 step:8570 [D loss: 0.248821, acc.: 56.25%] [G loss: 0.410988]\n",
      "epoch:9 step:8571 [D loss: 0.257659, acc.: 54.69%] [G loss: 0.440498]\n",
      "epoch:9 step:8572 [D loss: 0.231642, acc.: 60.94%] [G loss: 0.431691]\n",
      "epoch:9 step:8573 [D loss: 0.231801, acc.: 61.72%] [G loss: 0.459449]\n",
      "epoch:9 step:8574 [D loss: 0.220467, acc.: 67.97%] [G loss: 0.446297]\n",
      "epoch:9 step:8575 [D loss: 0.224384, acc.: 63.28%] [G loss: 0.414902]\n",
      "epoch:9 step:8576 [D loss: 0.245913, acc.: 61.72%] [G loss: 0.446615]\n",
      "epoch:9 step:8577 [D loss: 0.210185, acc.: 68.75%] [G loss: 0.489022]\n",
      "epoch:9 step:8578 [D loss: 0.238696, acc.: 60.16%] [G loss: 0.464416]\n",
      "epoch:9 step:8579 [D loss: 0.223713, acc.: 64.06%] [G loss: 0.446186]\n",
      "epoch:9 step:8580 [D loss: 0.276203, acc.: 51.56%] [G loss: 0.463275]\n",
      "epoch:9 step:8581 [D loss: 0.233361, acc.: 61.72%] [G loss: 0.439461]\n",
      "epoch:9 step:8582 [D loss: 0.203907, acc.: 70.31%] [G loss: 0.464381]\n",
      "epoch:9 step:8583 [D loss: 0.234355, acc.: 64.06%] [G loss: 0.411208]\n",
      "epoch:9 step:8584 [D loss: 0.194668, acc.: 68.75%] [G loss: 0.495201]\n",
      "epoch:9 step:8585 [D loss: 0.204116, acc.: 69.53%] [G loss: 0.456569]\n",
      "epoch:9 step:8586 [D loss: 0.247934, acc.: 56.25%] [G loss: 0.459353]\n",
      "epoch:9 step:8587 [D loss: 0.222545, acc.: 63.28%] [G loss: 0.492817]\n",
      "epoch:9 step:8588 [D loss: 0.191031, acc.: 71.09%] [G loss: 0.479805]\n",
      "epoch:9 step:8589 [D loss: 0.209492, acc.: 66.41%] [G loss: 0.473186]\n",
      "epoch:9 step:8590 [D loss: 0.226629, acc.: 64.84%] [G loss: 0.439191]\n",
      "epoch:9 step:8591 [D loss: 0.223276, acc.: 62.50%] [G loss: 0.458141]\n",
      "epoch:9 step:8592 [D loss: 0.209257, acc.: 68.75%] [G loss: 0.463779]\n",
      "epoch:9 step:8593 [D loss: 0.277818, acc.: 53.12%] [G loss: 0.430063]\n",
      "epoch:9 step:8594 [D loss: 0.210591, acc.: 60.94%] [G loss: 0.525320]\n",
      "epoch:9 step:8595 [D loss: 0.212528, acc.: 69.53%] [G loss: 0.468280]\n",
      "epoch:9 step:8596 [D loss: 0.208136, acc.: 67.97%] [G loss: 0.487896]\n",
      "epoch:9 step:8597 [D loss: 0.202923, acc.: 68.75%] [G loss: 0.484466]\n",
      "epoch:9 step:8598 [D loss: 0.220133, acc.: 66.41%] [G loss: 0.429711]\n",
      "epoch:9 step:8599 [D loss: 0.218217, acc.: 70.31%] [G loss: 0.494777]\n",
      "epoch:9 step:8600 [D loss: 0.201587, acc.: 69.53%] [G loss: 0.496288]\n",
      "##############\n",
      "[2.42072093 1.44864993 6.20623901 4.64737347 3.60591126 5.76003964\n",
      " 4.53759987 4.6097278  4.32257196 3.72157483]\n",
      "##########\n",
      "epoch:9 step:8601 [D loss: 0.205298, acc.: 71.09%] [G loss: 0.473322]\n",
      "epoch:9 step:8602 [D loss: 0.248963, acc.: 60.94%] [G loss: 0.477165]\n",
      "epoch:9 step:8603 [D loss: 0.220128, acc.: 60.94%] [G loss: 0.478989]\n",
      "epoch:9 step:8604 [D loss: 0.231809, acc.: 61.72%] [G loss: 0.447584]\n",
      "epoch:9 step:8605 [D loss: 0.202617, acc.: 67.97%] [G loss: 0.462777]\n",
      "epoch:9 step:8606 [D loss: 0.234551, acc.: 65.62%] [G loss: 0.451792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8607 [D loss: 0.241067, acc.: 57.81%] [G loss: 0.434080]\n",
      "epoch:9 step:8608 [D loss: 0.213662, acc.: 64.84%] [G loss: 0.465209]\n",
      "epoch:9 step:8609 [D loss: 0.202104, acc.: 69.53%] [G loss: 0.486375]\n",
      "epoch:9 step:8610 [D loss: 0.232236, acc.: 61.72%] [G loss: 0.466956]\n",
      "epoch:9 step:8611 [D loss: 0.233555, acc.: 57.81%] [G loss: 0.471789]\n",
      "epoch:9 step:8612 [D loss: 0.228933, acc.: 62.50%] [G loss: 0.437217]\n",
      "epoch:9 step:8613 [D loss: 0.223379, acc.: 64.06%] [G loss: 0.423199]\n",
      "epoch:9 step:8614 [D loss: 0.238394, acc.: 57.81%] [G loss: 0.448218]\n",
      "epoch:9 step:8615 [D loss: 0.267732, acc.: 53.91%] [G loss: 0.431499]\n",
      "epoch:9 step:8616 [D loss: 0.242389, acc.: 60.16%] [G loss: 0.443301]\n",
      "epoch:9 step:8617 [D loss: 0.207149, acc.: 70.31%] [G loss: 0.440172]\n",
      "epoch:9 step:8618 [D loss: 0.254665, acc.: 55.47%] [G loss: 0.452670]\n",
      "epoch:9 step:8619 [D loss: 0.247264, acc.: 62.50%] [G loss: 0.445887]\n",
      "epoch:9 step:8620 [D loss: 0.229962, acc.: 60.94%] [G loss: 0.455093]\n",
      "epoch:9 step:8621 [D loss: 0.227977, acc.: 61.72%] [G loss: 0.414538]\n",
      "epoch:9 step:8622 [D loss: 0.203154, acc.: 67.97%] [G loss: 0.479962]\n",
      "epoch:9 step:8623 [D loss: 0.211648, acc.: 67.97%] [G loss: 0.447470]\n",
      "epoch:9 step:8624 [D loss: 0.208298, acc.: 66.41%] [G loss: 0.433844]\n",
      "epoch:9 step:8625 [D loss: 0.221335, acc.: 65.62%] [G loss: 0.422534]\n",
      "epoch:9 step:8626 [D loss: 0.177927, acc.: 75.00%] [G loss: 0.518295]\n",
      "epoch:9 step:8627 [D loss: 0.197644, acc.: 71.09%] [G loss: 0.489036]\n",
      "epoch:9 step:8628 [D loss: 0.189399, acc.: 67.97%] [G loss: 0.504397]\n",
      "epoch:9 step:8629 [D loss: 0.255300, acc.: 56.25%] [G loss: 0.451305]\n",
      "epoch:9 step:8630 [D loss: 0.210720, acc.: 63.28%] [G loss: 0.450482]\n",
      "epoch:9 step:8631 [D loss: 0.187139, acc.: 72.66%] [G loss: 0.534540]\n",
      "epoch:9 step:8632 [D loss: 0.220404, acc.: 65.62%] [G loss: 0.494773]\n",
      "epoch:9 step:8633 [D loss: 0.246015, acc.: 60.16%] [G loss: 0.488082]\n",
      "epoch:9 step:8634 [D loss: 0.238644, acc.: 63.28%] [G loss: 0.490361]\n",
      "epoch:9 step:8635 [D loss: 0.209018, acc.: 67.19%] [G loss: 0.475692]\n",
      "epoch:9 step:8636 [D loss: 0.252463, acc.: 60.16%] [G loss: 0.437227]\n",
      "epoch:9 step:8637 [D loss: 0.207164, acc.: 72.66%] [G loss: 0.453528]\n",
      "epoch:9 step:8638 [D loss: 0.195089, acc.: 67.97%] [G loss: 0.485939]\n",
      "epoch:9 step:8639 [D loss: 0.218430, acc.: 67.97%] [G loss: 0.451322]\n",
      "epoch:9 step:8640 [D loss: 0.216220, acc.: 62.50%] [G loss: 0.473416]\n",
      "epoch:9 step:8641 [D loss: 0.167651, acc.: 73.44%] [G loss: 0.553064]\n",
      "epoch:9 step:8642 [D loss: 0.208077, acc.: 65.62%] [G loss: 0.518839]\n",
      "epoch:9 step:8643 [D loss: 0.258077, acc.: 53.12%] [G loss: 0.434546]\n",
      "epoch:9 step:8644 [D loss: 0.249487, acc.: 54.69%] [G loss: 0.433727]\n",
      "epoch:9 step:8645 [D loss: 0.244429, acc.: 58.59%] [G loss: 0.421169]\n",
      "epoch:9 step:8646 [D loss: 0.225828, acc.: 61.72%] [G loss: 0.422237]\n",
      "epoch:9 step:8647 [D loss: 0.235707, acc.: 58.59%] [G loss: 0.433665]\n",
      "epoch:9 step:8648 [D loss: 0.244914, acc.: 53.91%] [G loss: 0.424967]\n",
      "epoch:9 step:8649 [D loss: 0.207723, acc.: 67.19%] [G loss: 0.441474]\n",
      "epoch:9 step:8650 [D loss: 0.232255, acc.: 59.38%] [G loss: 0.457379]\n",
      "epoch:9 step:8651 [D loss: 0.206549, acc.: 72.66%] [G loss: 0.470703]\n",
      "epoch:9 step:8652 [D loss: 0.209707, acc.: 68.75%] [G loss: 0.493694]\n",
      "epoch:9 step:8653 [D loss: 0.287896, acc.: 53.91%] [G loss: 0.439335]\n",
      "epoch:9 step:8654 [D loss: 0.194810, acc.: 64.84%] [G loss: 0.496131]\n",
      "epoch:9 step:8655 [D loss: 0.202692, acc.: 69.53%] [G loss: 0.485096]\n",
      "epoch:9 step:8656 [D loss: 0.215190, acc.: 69.53%] [G loss: 0.543307]\n",
      "epoch:9 step:8657 [D loss: 0.247751, acc.: 55.47%] [G loss: 0.446561]\n",
      "epoch:9 step:8658 [D loss: 0.248683, acc.: 60.94%] [G loss: 0.451663]\n",
      "epoch:9 step:8659 [D loss: 0.246052, acc.: 57.81%] [G loss: 0.441849]\n",
      "epoch:9 step:8660 [D loss: 0.234206, acc.: 64.06%] [G loss: 0.429625]\n",
      "epoch:9 step:8661 [D loss: 0.234772, acc.: 57.03%] [G loss: 0.453155]\n",
      "epoch:9 step:8662 [D loss: 0.202047, acc.: 67.97%] [G loss: 0.466108]\n",
      "epoch:9 step:8663 [D loss: 0.207353, acc.: 65.62%] [G loss: 0.459180]\n",
      "epoch:9 step:8664 [D loss: 0.174186, acc.: 75.78%] [G loss: 0.565517]\n",
      "epoch:9 step:8665 [D loss: 0.174145, acc.: 75.78%] [G loss: 0.508537]\n",
      "epoch:9 step:8666 [D loss: 0.261285, acc.: 61.72%] [G loss: 0.469942]\n",
      "epoch:9 step:8667 [D loss: 0.220817, acc.: 61.72%] [G loss: 0.438166]\n",
      "epoch:9 step:8668 [D loss: 0.214086, acc.: 66.41%] [G loss: 0.464980]\n",
      "epoch:9 step:8669 [D loss: 0.204863, acc.: 65.62%] [G loss: 0.440624]\n",
      "epoch:9 step:8670 [D loss: 0.207428, acc.: 67.97%] [G loss: 0.470954]\n",
      "epoch:9 step:8671 [D loss: 0.195121, acc.: 72.66%] [G loss: 0.504653]\n",
      "epoch:9 step:8672 [D loss: 0.206481, acc.: 67.97%] [G loss: 0.490015]\n",
      "epoch:9 step:8673 [D loss: 0.229183, acc.: 62.50%] [G loss: 0.453443]\n",
      "epoch:9 step:8674 [D loss: 0.197940, acc.: 69.53%] [G loss: 0.476128]\n",
      "epoch:9 step:8675 [D loss: 0.200775, acc.: 71.09%] [G loss: 0.516248]\n",
      "epoch:9 step:8676 [D loss: 0.229010, acc.: 61.72%] [G loss: 0.479766]\n",
      "epoch:9 step:8677 [D loss: 0.208307, acc.: 64.06%] [G loss: 0.491103]\n",
      "epoch:9 step:8678 [D loss: 0.202286, acc.: 71.09%] [G loss: 0.482709]\n",
      "epoch:9 step:8679 [D loss: 0.220828, acc.: 63.28%] [G loss: 0.459465]\n",
      "epoch:9 step:8680 [D loss: 0.232265, acc.: 63.28%] [G loss: 0.457045]\n",
      "epoch:9 step:8681 [D loss: 0.212775, acc.: 64.06%] [G loss: 0.470169]\n",
      "epoch:9 step:8682 [D loss: 0.273027, acc.: 56.25%] [G loss: 0.453126]\n",
      "epoch:9 step:8683 [D loss: 0.278337, acc.: 45.31%] [G loss: 0.479184]\n",
      "epoch:9 step:8684 [D loss: 0.251101, acc.: 60.16%] [G loss: 0.507501]\n",
      "epoch:9 step:8685 [D loss: 0.246590, acc.: 60.94%] [G loss: 0.443664]\n",
      "epoch:9 step:8686 [D loss: 0.208162, acc.: 67.19%] [G loss: 0.484159]\n",
      "epoch:9 step:8687 [D loss: 0.207634, acc.: 68.75%] [G loss: 0.476228]\n",
      "epoch:9 step:8688 [D loss: 0.221273, acc.: 63.28%] [G loss: 0.459322]\n",
      "epoch:9 step:8689 [D loss: 0.226942, acc.: 63.28%] [G loss: 0.432460]\n",
      "epoch:9 step:8690 [D loss: 0.230645, acc.: 60.94%] [G loss: 0.466555]\n",
      "epoch:9 step:8691 [D loss: 0.197097, acc.: 67.97%] [G loss: 0.474226]\n",
      "epoch:9 step:8692 [D loss: 0.213164, acc.: 63.28%] [G loss: 0.441069]\n",
      "epoch:9 step:8693 [D loss: 0.218631, acc.: 66.41%] [G loss: 0.490710]\n",
      "epoch:9 step:8694 [D loss: 0.227969, acc.: 60.16%] [G loss: 0.450925]\n",
      "epoch:9 step:8695 [D loss: 0.220414, acc.: 65.62%] [G loss: 0.491282]\n",
      "epoch:9 step:8696 [D loss: 0.255833, acc.: 52.34%] [G loss: 0.471150]\n",
      "epoch:9 step:8697 [D loss: 0.208225, acc.: 68.75%] [G loss: 0.458764]\n",
      "epoch:9 step:8698 [D loss: 0.234993, acc.: 57.81%] [G loss: 0.436785]\n",
      "epoch:9 step:8699 [D loss: 0.270881, acc.: 51.56%] [G loss: 0.430291]\n",
      "epoch:9 step:8700 [D loss: 0.219595, acc.: 62.50%] [G loss: 0.428719]\n",
      "epoch:9 step:8701 [D loss: 0.232074, acc.: 58.59%] [G loss: 0.458456]\n",
      "epoch:9 step:8702 [D loss: 0.228806, acc.: 58.59%] [G loss: 0.412522]\n",
      "epoch:9 step:8703 [D loss: 0.204923, acc.: 66.41%] [G loss: 0.494676]\n",
      "epoch:9 step:8704 [D loss: 0.207921, acc.: 67.19%] [G loss: 0.505069]\n",
      "epoch:9 step:8705 [D loss: 0.214623, acc.: 65.62%] [G loss: 0.502241]\n",
      "epoch:9 step:8706 [D loss: 0.242737, acc.: 63.28%] [G loss: 0.449860]\n",
      "epoch:9 step:8707 [D loss: 0.217509, acc.: 67.97%] [G loss: 0.484348]\n",
      "epoch:9 step:8708 [D loss: 0.234416, acc.: 60.94%] [G loss: 0.508201]\n",
      "epoch:9 step:8709 [D loss: 0.235581, acc.: 64.84%] [G loss: 0.505867]\n",
      "epoch:9 step:8710 [D loss: 0.260519, acc.: 51.56%] [G loss: 0.435742]\n",
      "epoch:9 step:8711 [D loss: 0.228425, acc.: 65.62%] [G loss: 0.450530]\n",
      "epoch:9 step:8712 [D loss: 0.212411, acc.: 64.84%] [G loss: 0.502777]\n",
      "epoch:9 step:8713 [D loss: 0.205689, acc.: 63.28%] [G loss: 0.466930]\n",
      "epoch:9 step:8714 [D loss: 0.246209, acc.: 60.94%] [G loss: 0.458877]\n",
      "epoch:9 step:8715 [D loss: 0.233025, acc.: 60.16%] [G loss: 0.441946]\n",
      "epoch:9 step:8716 [D loss: 0.203530, acc.: 71.09%] [G loss: 0.463751]\n",
      "epoch:9 step:8717 [D loss: 0.221266, acc.: 64.84%] [G loss: 0.465466]\n",
      "epoch:9 step:8718 [D loss: 0.221336, acc.: 64.06%] [G loss: 0.443522]\n",
      "epoch:9 step:8719 [D loss: 0.217798, acc.: 67.97%] [G loss: 0.460251]\n",
      "epoch:9 step:8720 [D loss: 0.218635, acc.: 64.06%] [G loss: 0.482975]\n",
      "epoch:9 step:8721 [D loss: 0.214687, acc.: 65.62%] [G loss: 0.491943]\n",
      "epoch:9 step:8722 [D loss: 0.215506, acc.: 67.19%] [G loss: 0.511827]\n",
      "epoch:9 step:8723 [D loss: 0.222818, acc.: 60.16%] [G loss: 0.500816]\n",
      "epoch:9 step:8724 [D loss: 0.235428, acc.: 62.50%] [G loss: 0.430400]\n",
      "epoch:9 step:8725 [D loss: 0.231740, acc.: 60.16%] [G loss: 0.466568]\n",
      "epoch:9 step:8726 [D loss: 0.241174, acc.: 58.59%] [G loss: 0.449207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8727 [D loss: 0.247523, acc.: 56.25%] [G loss: 0.446944]\n",
      "epoch:9 step:8728 [D loss: 0.227993, acc.: 63.28%] [G loss: 0.458684]\n",
      "epoch:9 step:8729 [D loss: 0.204161, acc.: 66.41%] [G loss: 0.460572]\n",
      "epoch:9 step:8730 [D loss: 0.220580, acc.: 70.31%] [G loss: 0.468798]\n",
      "epoch:9 step:8731 [D loss: 0.206297, acc.: 67.97%] [G loss: 0.459701]\n",
      "epoch:9 step:8732 [D loss: 0.231450, acc.: 64.06%] [G loss: 0.434812]\n",
      "epoch:9 step:8733 [D loss: 0.199584, acc.: 76.56%] [G loss: 0.462008]\n",
      "epoch:9 step:8734 [D loss: 0.241964, acc.: 60.94%] [G loss: 0.436716]\n",
      "epoch:9 step:8735 [D loss: 0.220334, acc.: 62.50%] [G loss: 0.449094]\n",
      "epoch:9 step:8736 [D loss: 0.223296, acc.: 64.06%] [G loss: 0.539221]\n",
      "epoch:9 step:8737 [D loss: 0.203293, acc.: 64.06%] [G loss: 0.525523]\n",
      "epoch:9 step:8738 [D loss: 0.205523, acc.: 67.19%] [G loss: 0.457323]\n",
      "epoch:9 step:8739 [D loss: 0.205605, acc.: 66.41%] [G loss: 0.465591]\n",
      "epoch:9 step:8740 [D loss: 0.193185, acc.: 71.88%] [G loss: 0.478008]\n",
      "epoch:9 step:8741 [D loss: 0.227303, acc.: 61.72%] [G loss: 0.457394]\n",
      "epoch:9 step:8742 [D loss: 0.202108, acc.: 67.19%] [G loss: 0.485915]\n",
      "epoch:9 step:8743 [D loss: 0.240887, acc.: 55.47%] [G loss: 0.471295]\n",
      "epoch:9 step:8744 [D loss: 0.203661, acc.: 68.75%] [G loss: 0.499815]\n",
      "epoch:9 step:8745 [D loss: 0.176099, acc.: 75.00%] [G loss: 0.523925]\n",
      "epoch:9 step:8746 [D loss: 0.182540, acc.: 67.97%] [G loss: 0.525867]\n",
      "epoch:9 step:8747 [D loss: 0.198930, acc.: 65.62%] [G loss: 0.516858]\n",
      "epoch:9 step:8748 [D loss: 0.205586, acc.: 72.66%] [G loss: 0.499064]\n",
      "epoch:9 step:8749 [D loss: 0.267696, acc.: 56.25%] [G loss: 0.427948]\n",
      "epoch:9 step:8750 [D loss: 0.241889, acc.: 62.50%] [G loss: 0.435092]\n",
      "epoch:9 step:8751 [D loss: 0.209166, acc.: 64.06%] [G loss: 0.470933]\n",
      "epoch:9 step:8752 [D loss: 0.213184, acc.: 67.97%] [G loss: 0.441356]\n",
      "epoch:9 step:8753 [D loss: 0.203356, acc.: 67.19%] [G loss: 0.498631]\n",
      "epoch:9 step:8754 [D loss: 0.164433, acc.: 78.12%] [G loss: 0.549500]\n",
      "epoch:9 step:8755 [D loss: 0.242783, acc.: 62.50%] [G loss: 0.487800]\n",
      "epoch:9 step:8756 [D loss: 0.240219, acc.: 62.50%] [G loss: 0.469731]\n",
      "epoch:9 step:8757 [D loss: 0.205587, acc.: 69.53%] [G loss: 0.472215]\n",
      "epoch:9 step:8758 [D loss: 0.223847, acc.: 65.62%] [G loss: 0.467609]\n",
      "epoch:9 step:8759 [D loss: 0.219619, acc.: 64.06%] [G loss: 0.501029]\n",
      "epoch:9 step:8760 [D loss: 0.222405, acc.: 62.50%] [G loss: 0.496142]\n",
      "epoch:9 step:8761 [D loss: 0.214644, acc.: 73.44%] [G loss: 0.452637]\n",
      "epoch:9 step:8762 [D loss: 0.235700, acc.: 60.94%] [G loss: 0.506785]\n",
      "epoch:9 step:8763 [D loss: 0.228132, acc.: 60.94%] [G loss: 0.456815]\n",
      "epoch:9 step:8764 [D loss: 0.210130, acc.: 63.28%] [G loss: 0.475520]\n",
      "epoch:9 step:8765 [D loss: 0.219942, acc.: 61.72%] [G loss: 0.431567]\n",
      "epoch:9 step:8766 [D loss: 0.223827, acc.: 65.62%] [G loss: 0.464272]\n",
      "epoch:9 step:8767 [D loss: 0.234737, acc.: 66.41%] [G loss: 0.486853]\n",
      "epoch:9 step:8768 [D loss: 0.199787, acc.: 69.53%] [G loss: 0.503743]\n",
      "epoch:9 step:8769 [D loss: 0.197507, acc.: 70.31%] [G loss: 0.482222]\n",
      "epoch:9 step:8770 [D loss: 0.201031, acc.: 70.31%] [G loss: 0.469868]\n",
      "epoch:9 step:8771 [D loss: 0.202388, acc.: 68.75%] [G loss: 0.451003]\n",
      "epoch:9 step:8772 [D loss: 0.210755, acc.: 70.31%] [G loss: 0.469583]\n",
      "epoch:9 step:8773 [D loss: 0.195326, acc.: 71.88%] [G loss: 0.491848]\n",
      "epoch:9 step:8774 [D loss: 0.278444, acc.: 53.12%] [G loss: 0.452538]\n",
      "epoch:9 step:8775 [D loss: 0.232054, acc.: 62.50%] [G loss: 0.469882]\n",
      "epoch:9 step:8776 [D loss: 0.204668, acc.: 67.97%] [G loss: 0.483622]\n",
      "epoch:9 step:8777 [D loss: 0.199172, acc.: 67.19%] [G loss: 0.560406]\n",
      "epoch:9 step:8778 [D loss: 0.229453, acc.: 63.28%] [G loss: 0.492393]\n",
      "epoch:9 step:8779 [D loss: 0.197522, acc.: 71.09%] [G loss: 0.548386]\n",
      "epoch:9 step:8780 [D loss: 0.157351, acc.: 77.34%] [G loss: 0.553753]\n",
      "epoch:9 step:8781 [D loss: 0.274175, acc.: 57.03%] [G loss: 0.484114]\n",
      "epoch:9 step:8782 [D loss: 0.277835, acc.: 46.88%] [G loss: 0.436729]\n",
      "epoch:9 step:8783 [D loss: 0.198905, acc.: 67.97%] [G loss: 0.483136]\n",
      "epoch:9 step:8784 [D loss: 0.232080, acc.: 57.81%] [G loss: 0.509699]\n",
      "epoch:9 step:8785 [D loss: 0.228032, acc.: 68.75%] [G loss: 0.444977]\n",
      "epoch:9 step:8786 [D loss: 0.236372, acc.: 60.94%] [G loss: 0.443321]\n",
      "epoch:9 step:8787 [D loss: 0.189982, acc.: 71.09%] [G loss: 0.511824]\n",
      "epoch:9 step:8788 [D loss: 0.239822, acc.: 62.50%] [G loss: 0.468611]\n",
      "epoch:9 step:8789 [D loss: 0.244229, acc.: 58.59%] [G loss: 0.459996]\n",
      "epoch:9 step:8790 [D loss: 0.211035, acc.: 67.97%] [G loss: 0.452036]\n",
      "epoch:9 step:8791 [D loss: 0.175598, acc.: 76.56%] [G loss: 0.483047]\n",
      "epoch:9 step:8792 [D loss: 0.193192, acc.: 74.22%] [G loss: 0.496472]\n",
      "epoch:9 step:8793 [D loss: 0.197875, acc.: 71.88%] [G loss: 0.470789]\n",
      "epoch:9 step:8794 [D loss: 0.225177, acc.: 64.06%] [G loss: 0.444089]\n",
      "epoch:9 step:8795 [D loss: 0.221125, acc.: 64.84%] [G loss: 0.441415]\n",
      "epoch:9 step:8796 [D loss: 0.231112, acc.: 60.16%] [G loss: 0.453521]\n",
      "epoch:9 step:8797 [D loss: 0.223548, acc.: 60.94%] [G loss: 0.458867]\n",
      "epoch:9 step:8798 [D loss: 0.227058, acc.: 57.81%] [G loss: 0.452166]\n",
      "epoch:9 step:8799 [D loss: 0.215814, acc.: 62.50%] [G loss: 0.447474]\n",
      "epoch:9 step:8800 [D loss: 0.218918, acc.: 64.84%] [G loss: 0.482564]\n",
      "##############\n",
      "[2.45840014 1.68339677 6.1065622  4.76316515 3.64410296 5.71933252\n",
      " 4.49920822 4.61963662 4.55748631 3.51021465]\n",
      "##########\n",
      "epoch:9 step:8801 [D loss: 0.218606, acc.: 65.62%] [G loss: 0.493280]\n",
      "epoch:9 step:8802 [D loss: 0.257772, acc.: 50.00%] [G loss: 0.443855]\n",
      "epoch:9 step:8803 [D loss: 0.214259, acc.: 70.31%] [G loss: 0.508489]\n",
      "epoch:9 step:8804 [D loss: 0.215757, acc.: 65.62%] [G loss: 0.484597]\n",
      "epoch:9 step:8805 [D loss: 0.223909, acc.: 66.41%] [G loss: 0.496150]\n",
      "epoch:9 step:8806 [D loss: 0.236648, acc.: 60.94%] [G loss: 0.437716]\n",
      "epoch:9 step:8807 [D loss: 0.204409, acc.: 69.53%] [G loss: 0.458082]\n",
      "epoch:9 step:8808 [D loss: 0.236367, acc.: 63.28%] [G loss: 0.456438]\n",
      "epoch:9 step:8809 [D loss: 0.261020, acc.: 59.38%] [G loss: 0.427853]\n",
      "epoch:9 step:8810 [D loss: 0.232780, acc.: 61.72%] [G loss: 0.455039]\n",
      "epoch:9 step:8811 [D loss: 0.232315, acc.: 65.62%] [G loss: 0.438245]\n",
      "epoch:9 step:8812 [D loss: 0.228293, acc.: 61.72%] [G loss: 0.479683]\n",
      "epoch:9 step:8813 [D loss: 0.229768, acc.: 62.50%] [G loss: 0.503657]\n",
      "epoch:9 step:8814 [D loss: 0.196565, acc.: 67.97%] [G loss: 0.496766]\n",
      "epoch:9 step:8815 [D loss: 0.225787, acc.: 63.28%] [G loss: 0.469636]\n",
      "epoch:9 step:8816 [D loss: 0.220042, acc.: 68.75%] [G loss: 0.451781]\n",
      "epoch:9 step:8817 [D loss: 0.201125, acc.: 66.41%] [G loss: 0.459542]\n",
      "epoch:9 step:8818 [D loss: 0.216746, acc.: 63.28%] [G loss: 0.453922]\n",
      "epoch:9 step:8819 [D loss: 0.218774, acc.: 64.06%] [G loss: 0.453321]\n",
      "epoch:9 step:8820 [D loss: 0.229954, acc.: 58.59%] [G loss: 0.436909]\n",
      "epoch:9 step:8821 [D loss: 0.199907, acc.: 72.66%] [G loss: 0.464473]\n",
      "epoch:9 step:8822 [D loss: 0.240824, acc.: 53.12%] [G loss: 0.443990]\n",
      "epoch:9 step:8823 [D loss: 0.237309, acc.: 60.16%] [G loss: 0.449957]\n",
      "epoch:9 step:8824 [D loss: 0.216556, acc.: 63.28%] [G loss: 0.453988]\n",
      "epoch:9 step:8825 [D loss: 0.232727, acc.: 64.84%] [G loss: 0.470617]\n",
      "epoch:9 step:8826 [D loss: 0.239222, acc.: 61.72%] [G loss: 0.473135]\n",
      "epoch:9 step:8827 [D loss: 0.246046, acc.: 57.03%] [G loss: 0.436805]\n",
      "epoch:9 step:8828 [D loss: 0.221891, acc.: 62.50%] [G loss: 0.479085]\n",
      "epoch:9 step:8829 [D loss: 0.215333, acc.: 67.19%] [G loss: 0.459319]\n",
      "epoch:9 step:8830 [D loss: 0.232180, acc.: 59.38%] [G loss: 0.443482]\n",
      "epoch:9 step:8831 [D loss: 0.185167, acc.: 71.88%] [G loss: 0.506610]\n",
      "epoch:9 step:8832 [D loss: 0.188261, acc.: 73.44%] [G loss: 0.483542]\n",
      "epoch:9 step:8833 [D loss: 0.279300, acc.: 52.34%] [G loss: 0.419394]\n",
      "epoch:9 step:8834 [D loss: 0.234601, acc.: 60.94%] [G loss: 0.473756]\n",
      "epoch:9 step:8835 [D loss: 0.202616, acc.: 72.66%] [G loss: 0.441018]\n",
      "epoch:9 step:8836 [D loss: 0.229065, acc.: 64.84%] [G loss: 0.477042]\n",
      "epoch:9 step:8837 [D loss: 0.234337, acc.: 61.72%] [G loss: 0.470215]\n",
      "epoch:9 step:8838 [D loss: 0.223619, acc.: 61.72%] [G loss: 0.505733]\n",
      "epoch:9 step:8839 [D loss: 0.218174, acc.: 66.41%] [G loss: 0.478736]\n",
      "epoch:9 step:8840 [D loss: 0.238114, acc.: 60.16%] [G loss: 0.449456]\n",
      "epoch:9 step:8841 [D loss: 0.289074, acc.: 47.66%] [G loss: 0.416716]\n",
      "epoch:9 step:8842 [D loss: 0.244569, acc.: 59.38%] [G loss: 0.470132]\n",
      "epoch:9 step:8843 [D loss: 0.242304, acc.: 60.94%] [G loss: 0.433984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8844 [D loss: 0.246657, acc.: 51.56%] [G loss: 0.428220]\n",
      "epoch:9 step:8845 [D loss: 0.232951, acc.: 59.38%] [G loss: 0.440257]\n",
      "epoch:9 step:8846 [D loss: 0.236586, acc.: 58.59%] [G loss: 0.435281]\n",
      "epoch:9 step:8847 [D loss: 0.248717, acc.: 52.34%] [G loss: 0.460197]\n",
      "epoch:9 step:8848 [D loss: 0.215438, acc.: 62.50%] [G loss: 0.482471]\n",
      "epoch:9 step:8849 [D loss: 0.200881, acc.: 69.53%] [G loss: 0.552263]\n",
      "epoch:9 step:8850 [D loss: 0.238915, acc.: 60.16%] [G loss: 0.482278]\n",
      "epoch:9 step:8851 [D loss: 0.264481, acc.: 52.34%] [G loss: 0.439841]\n",
      "epoch:9 step:8852 [D loss: 0.235372, acc.: 62.50%] [G loss: 0.426144]\n",
      "epoch:9 step:8853 [D loss: 0.234404, acc.: 60.94%] [G loss: 0.433560]\n",
      "epoch:9 step:8854 [D loss: 0.235926, acc.: 60.16%] [G loss: 0.463085]\n",
      "epoch:9 step:8855 [D loss: 0.246355, acc.: 51.56%] [G loss: 0.436020]\n",
      "epoch:9 step:8856 [D loss: 0.217560, acc.: 64.84%] [G loss: 0.457761]\n",
      "epoch:9 step:8857 [D loss: 0.240510, acc.: 59.38%] [G loss: 0.451035]\n",
      "epoch:9 step:8858 [D loss: 0.216169, acc.: 66.41%] [G loss: 0.450309]\n",
      "epoch:9 step:8859 [D loss: 0.236029, acc.: 57.81%] [G loss: 0.412792]\n",
      "epoch:9 step:8860 [D loss: 0.172271, acc.: 75.00%] [G loss: 0.504876]\n",
      "epoch:9 step:8861 [D loss: 0.203957, acc.: 70.31%] [G loss: 0.524843]\n",
      "epoch:9 step:8862 [D loss: 0.172895, acc.: 76.56%] [G loss: 0.497715]\n",
      "epoch:9 step:8863 [D loss: 0.192545, acc.: 70.31%] [G loss: 0.494554]\n",
      "epoch:9 step:8864 [D loss: 0.233264, acc.: 61.72%] [G loss: 0.452131]\n",
      "epoch:9 step:8865 [D loss: 0.229879, acc.: 59.38%] [G loss: 0.435407]\n",
      "epoch:9 step:8866 [D loss: 0.213726, acc.: 67.97%] [G loss: 0.439480]\n",
      "epoch:9 step:8867 [D loss: 0.209756, acc.: 68.75%] [G loss: 0.459929]\n",
      "epoch:9 step:8868 [D loss: 0.210071, acc.: 65.62%] [G loss: 0.499966]\n",
      "epoch:9 step:8869 [D loss: 0.206275, acc.: 70.31%] [G loss: 0.491658]\n",
      "epoch:9 step:8870 [D loss: 0.257215, acc.: 54.69%] [G loss: 0.468587]\n",
      "epoch:9 step:8871 [D loss: 0.233448, acc.: 60.94%] [G loss: 0.452563]\n",
      "epoch:9 step:8872 [D loss: 0.199413, acc.: 68.75%] [G loss: 0.482271]\n",
      "epoch:9 step:8873 [D loss: 0.212059, acc.: 66.41%] [G loss: 0.487641]\n",
      "epoch:9 step:8874 [D loss: 0.230141, acc.: 57.81%] [G loss: 0.473896]\n",
      "epoch:9 step:8875 [D loss: 0.231072, acc.: 59.38%] [G loss: 0.489434]\n",
      "epoch:9 step:8876 [D loss: 0.248524, acc.: 59.38%] [G loss: 0.455649]\n",
      "epoch:9 step:8877 [D loss: 0.222789, acc.: 64.84%] [G loss: 0.504589]\n",
      "epoch:9 step:8878 [D loss: 0.215262, acc.: 61.72%] [G loss: 0.450701]\n",
      "epoch:9 step:8879 [D loss: 0.217877, acc.: 60.94%] [G loss: 0.496207]\n",
      "epoch:9 step:8880 [D loss: 0.237627, acc.: 63.28%] [G loss: 0.494509]\n",
      "epoch:9 step:8881 [D loss: 0.272116, acc.: 52.34%] [G loss: 0.425239]\n",
      "epoch:9 step:8882 [D loss: 0.209644, acc.: 64.06%] [G loss: 0.483608]\n",
      "epoch:9 step:8883 [D loss: 0.220813, acc.: 62.50%] [G loss: 0.462269]\n",
      "epoch:9 step:8884 [D loss: 0.177802, acc.: 74.22%] [G loss: 0.488487]\n",
      "epoch:9 step:8885 [D loss: 0.219324, acc.: 62.50%] [G loss: 0.473549]\n",
      "epoch:9 step:8886 [D loss: 0.198293, acc.: 67.19%] [G loss: 0.490815]\n",
      "epoch:9 step:8887 [D loss: 0.224739, acc.: 62.50%] [G loss: 0.480075]\n",
      "epoch:9 step:8888 [D loss: 0.227773, acc.: 58.59%] [G loss: 0.474669]\n",
      "epoch:9 step:8889 [D loss: 0.241850, acc.: 58.59%] [G loss: 0.455459]\n",
      "epoch:9 step:8890 [D loss: 0.211138, acc.: 64.06%] [G loss: 0.503114]\n",
      "epoch:9 step:8891 [D loss: 0.266747, acc.: 53.91%] [G loss: 0.469832]\n",
      "epoch:9 step:8892 [D loss: 0.242845, acc.: 57.03%] [G loss: 0.411421]\n",
      "epoch:9 step:8893 [D loss: 0.231150, acc.: 59.38%] [G loss: 0.440379]\n",
      "epoch:9 step:8894 [D loss: 0.221186, acc.: 62.50%] [G loss: 0.492810]\n",
      "epoch:9 step:8895 [D loss: 0.222385, acc.: 62.50%] [G loss: 0.467810]\n",
      "epoch:9 step:8896 [D loss: 0.224639, acc.: 63.28%] [G loss: 0.429915]\n",
      "epoch:9 step:8897 [D loss: 0.223630, acc.: 65.62%] [G loss: 0.438245]\n",
      "epoch:9 step:8898 [D loss: 0.254965, acc.: 53.91%] [G loss: 0.438218]\n",
      "epoch:9 step:8899 [D loss: 0.204970, acc.: 67.97%] [G loss: 0.441849]\n",
      "epoch:9 step:8900 [D loss: 0.230888, acc.: 64.06%] [G loss: 0.432374]\n",
      "epoch:9 step:8901 [D loss: 0.208743, acc.: 64.06%] [G loss: 0.473621]\n",
      "epoch:9 step:8902 [D loss: 0.193580, acc.: 73.44%] [G loss: 0.525216]\n",
      "epoch:9 step:8903 [D loss: 0.205601, acc.: 67.97%] [G loss: 0.515174]\n",
      "epoch:9 step:8904 [D loss: 0.187940, acc.: 69.53%] [G loss: 0.520984]\n",
      "epoch:9 step:8905 [D loss: 0.200744, acc.: 70.31%] [G loss: 0.523131]\n",
      "epoch:9 step:8906 [D loss: 0.245233, acc.: 60.16%] [G loss: 0.470151]\n",
      "epoch:9 step:8907 [D loss: 0.198997, acc.: 71.09%] [G loss: 0.472415]\n",
      "epoch:9 step:8908 [D loss: 0.184859, acc.: 71.88%] [G loss: 0.531970]\n",
      "epoch:9 step:8909 [D loss: 0.262428, acc.: 60.94%] [G loss: 0.476473]\n",
      "epoch:9 step:8910 [D loss: 0.259295, acc.: 55.47%] [G loss: 0.453963]\n",
      "epoch:9 step:8911 [D loss: 0.252801, acc.: 52.34%] [G loss: 0.399849]\n",
      "epoch:9 step:8912 [D loss: 0.231846, acc.: 58.59%] [G loss: 0.412262]\n",
      "epoch:9 step:8913 [D loss: 0.247100, acc.: 57.81%] [G loss: 0.460109]\n",
      "epoch:9 step:8914 [D loss: 0.205918, acc.: 65.62%] [G loss: 0.470359]\n",
      "epoch:9 step:8915 [D loss: 0.265520, acc.: 49.22%] [G loss: 0.438438]\n",
      "epoch:9 step:8916 [D loss: 0.206093, acc.: 67.97%] [G loss: 0.427997]\n",
      "epoch:9 step:8917 [D loss: 0.178128, acc.: 75.78%] [G loss: 0.481747]\n",
      "epoch:9 step:8918 [D loss: 0.204908, acc.: 69.53%] [G loss: 0.500970]\n",
      "epoch:9 step:8919 [D loss: 0.232043, acc.: 60.94%] [G loss: 0.448489]\n",
      "epoch:9 step:8920 [D loss: 0.233973, acc.: 63.28%] [G loss: 0.464810]\n",
      "epoch:9 step:8921 [D loss: 0.222870, acc.: 60.94%] [G loss: 0.505277]\n",
      "epoch:9 step:8922 [D loss: 0.242125, acc.: 58.59%] [G loss: 0.470316]\n",
      "epoch:9 step:8923 [D loss: 0.212796, acc.: 69.53%] [G loss: 0.476277]\n",
      "epoch:9 step:8924 [D loss: 0.224552, acc.: 64.84%] [G loss: 0.489623]\n",
      "epoch:9 step:8925 [D loss: 0.247469, acc.: 59.38%] [G loss: 0.483178]\n",
      "epoch:9 step:8926 [D loss: 0.224372, acc.: 64.84%] [G loss: 0.461171]\n",
      "epoch:9 step:8927 [D loss: 0.237071, acc.: 58.59%] [G loss: 0.468513]\n",
      "epoch:9 step:8928 [D loss: 0.226929, acc.: 60.94%] [G loss: 0.464615]\n",
      "epoch:9 step:8929 [D loss: 0.211422, acc.: 65.62%] [G loss: 0.440340]\n",
      "epoch:9 step:8930 [D loss: 0.203137, acc.: 67.97%] [G loss: 0.464016]\n",
      "epoch:9 step:8931 [D loss: 0.202664, acc.: 69.53%] [G loss: 0.468063]\n",
      "epoch:9 step:8932 [D loss: 0.191374, acc.: 73.44%] [G loss: 0.540973]\n",
      "epoch:9 step:8933 [D loss: 0.283173, acc.: 50.78%] [G loss: 0.444739]\n",
      "epoch:9 step:8934 [D loss: 0.283064, acc.: 50.78%] [G loss: 0.401109]\n",
      "epoch:9 step:8935 [D loss: 0.222964, acc.: 64.06%] [G loss: 0.454111]\n",
      "epoch:9 step:8936 [D loss: 0.200572, acc.: 64.84%] [G loss: 0.443163]\n",
      "epoch:9 step:8937 [D loss: 0.208964, acc.: 67.97%] [G loss: 0.496324]\n",
      "epoch:9 step:8938 [D loss: 0.192529, acc.: 67.97%] [G loss: 0.500162]\n",
      "epoch:9 step:8939 [D loss: 0.232663, acc.: 56.25%] [G loss: 0.477325]\n",
      "epoch:9 step:8940 [D loss: 0.236524, acc.: 60.94%] [G loss: 0.495490]\n",
      "epoch:9 step:8941 [D loss: 0.176595, acc.: 75.78%] [G loss: 0.553005]\n",
      "epoch:9 step:8942 [D loss: 0.260389, acc.: 57.81%] [G loss: 0.436942]\n",
      "epoch:9 step:8943 [D loss: 0.259854, acc.: 52.34%] [G loss: 0.406781]\n",
      "epoch:9 step:8944 [D loss: 0.263882, acc.: 55.47%] [G loss: 0.444658]\n",
      "epoch:9 step:8945 [D loss: 0.223610, acc.: 60.16%] [G loss: 0.478730]\n",
      "epoch:9 step:8946 [D loss: 0.219487, acc.: 65.62%] [G loss: 0.448920]\n",
      "epoch:9 step:8947 [D loss: 0.212640, acc.: 62.50%] [G loss: 0.481428]\n",
      "epoch:9 step:8948 [D loss: 0.228330, acc.: 62.50%] [G loss: 0.454005]\n",
      "epoch:9 step:8949 [D loss: 0.219008, acc.: 64.84%] [G loss: 0.479707]\n",
      "epoch:9 step:8950 [D loss: 0.250446, acc.: 60.16%] [G loss: 0.469886]\n",
      "epoch:9 step:8951 [D loss: 0.226194, acc.: 69.53%] [G loss: 0.485479]\n",
      "epoch:9 step:8952 [D loss: 0.194988, acc.: 73.44%] [G loss: 0.472975]\n",
      "epoch:9 step:8953 [D loss: 0.203351, acc.: 72.66%] [G loss: 0.480032]\n",
      "epoch:9 step:8954 [D loss: 0.219727, acc.: 67.97%] [G loss: 0.447791]\n",
      "epoch:9 step:8955 [D loss: 0.200467, acc.: 71.88%] [G loss: 0.440869]\n",
      "epoch:9 step:8956 [D loss: 0.202668, acc.: 70.31%] [G loss: 0.477434]\n",
      "epoch:9 step:8957 [D loss: 0.248642, acc.: 56.25%] [G loss: 0.459319]\n",
      "epoch:9 step:8958 [D loss: 0.226896, acc.: 67.19%] [G loss: 0.460622]\n",
      "epoch:9 step:8959 [D loss: 0.218219, acc.: 62.50%] [G loss: 0.507365]\n",
      "epoch:9 step:8960 [D loss: 0.250871, acc.: 58.59%] [G loss: 0.416397]\n",
      "epoch:9 step:8961 [D loss: 0.289676, acc.: 46.09%] [G loss: 0.405343]\n",
      "epoch:9 step:8962 [D loss: 0.243669, acc.: 60.16%] [G loss: 0.445108]\n",
      "epoch:9 step:8963 [D loss: 0.209341, acc.: 67.19%] [G loss: 0.508818]\n",
      "epoch:9 step:8964 [D loss: 0.269631, acc.: 57.03%] [G loss: 0.418597]\n",
      "epoch:9 step:8965 [D loss: 0.227527, acc.: 61.72%] [G loss: 0.426547]\n",
      "epoch:9 step:8966 [D loss: 0.205329, acc.: 67.19%] [G loss: 0.480181]\n",
      "epoch:9 step:8967 [D loss: 0.187770, acc.: 67.19%] [G loss: 0.460112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8968 [D loss: 0.244837, acc.: 58.59%] [G loss: 0.454354]\n",
      "epoch:9 step:8969 [D loss: 0.201663, acc.: 69.53%] [G loss: 0.498096]\n",
      "epoch:9 step:8970 [D loss: 0.234229, acc.: 64.84%] [G loss: 0.470253]\n",
      "epoch:9 step:8971 [D loss: 0.251300, acc.: 54.69%] [G loss: 0.482496]\n",
      "epoch:9 step:8972 [D loss: 0.225632, acc.: 64.06%] [G loss: 0.486019]\n",
      "epoch:9 step:8973 [D loss: 0.214343, acc.: 64.06%] [G loss: 0.483726]\n",
      "epoch:9 step:8974 [D loss: 0.225640, acc.: 63.28%] [G loss: 0.464410]\n",
      "epoch:9 step:8975 [D loss: 0.281993, acc.: 49.22%] [G loss: 0.431448]\n",
      "epoch:9 step:8976 [D loss: 0.202207, acc.: 70.31%] [G loss: 0.428951]\n",
      "epoch:9 step:8977 [D loss: 0.226699, acc.: 62.50%] [G loss: 0.432503]\n",
      "epoch:9 step:8978 [D loss: 0.220629, acc.: 64.06%] [G loss: 0.411067]\n",
      "epoch:9 step:8979 [D loss: 0.200105, acc.: 68.75%] [G loss: 0.417793]\n",
      "epoch:9 step:8980 [D loss: 0.217527, acc.: 67.19%] [G loss: 0.479792]\n",
      "epoch:9 step:8981 [D loss: 0.210409, acc.: 64.06%] [G loss: 0.447884]\n",
      "epoch:9 step:8982 [D loss: 0.218874, acc.: 64.84%] [G loss: 0.471923]\n",
      "epoch:9 step:8983 [D loss: 0.209269, acc.: 65.62%] [G loss: 0.462605]\n",
      "epoch:9 step:8984 [D loss: 0.208917, acc.: 64.06%] [G loss: 0.488569]\n",
      "epoch:9 step:8985 [D loss: 0.188379, acc.: 72.66%] [G loss: 0.513562]\n",
      "epoch:9 step:8986 [D loss: 0.232549, acc.: 63.28%] [G loss: 0.459017]\n",
      "epoch:9 step:8987 [D loss: 0.215041, acc.: 67.19%] [G loss: 0.445688]\n",
      "epoch:9 step:8988 [D loss: 0.177164, acc.: 77.34%] [G loss: 0.476338]\n",
      "epoch:9 step:8989 [D loss: 0.221156, acc.: 67.97%] [G loss: 0.476922]\n",
      "epoch:9 step:8990 [D loss: 0.230244, acc.: 64.84%] [G loss: 0.493961]\n",
      "epoch:9 step:8991 [D loss: 0.207563, acc.: 68.75%] [G loss: 0.497971]\n",
      "epoch:9 step:8992 [D loss: 0.252832, acc.: 54.69%] [G loss: 0.442450]\n",
      "epoch:9 step:8993 [D loss: 0.219652, acc.: 63.28%] [G loss: 0.481452]\n",
      "epoch:9 step:8994 [D loss: 0.193441, acc.: 68.75%] [G loss: 0.476559]\n",
      "epoch:9 step:8995 [D loss: 0.232763, acc.: 64.06%] [G loss: 0.505620]\n",
      "epoch:9 step:8996 [D loss: 0.195372, acc.: 66.41%] [G loss: 0.508149]\n",
      "epoch:9 step:8997 [D loss: 0.202365, acc.: 64.06%] [G loss: 0.529965]\n",
      "epoch:9 step:8998 [D loss: 0.236747, acc.: 58.59%] [G loss: 0.492501]\n",
      "epoch:9 step:8999 [D loss: 0.252631, acc.: 59.38%] [G loss: 0.476118]\n",
      "epoch:9 step:9000 [D loss: 0.220121, acc.: 61.72%] [G loss: 0.445686]\n",
      "##############\n",
      "[2.54134049 1.42545289 6.21456421 4.61035156 3.79866917 5.57860063\n",
      " 4.49759689 4.729073   4.71534358 3.81896945]\n",
      "##########\n",
      "epoch:9 step:9001 [D loss: 0.192240, acc.: 72.66%] [G loss: 0.492762]\n",
      "epoch:9 step:9002 [D loss: 0.268083, acc.: 53.12%] [G loss: 0.445135]\n",
      "epoch:9 step:9003 [D loss: 0.206033, acc.: 67.19%] [G loss: 0.481647]\n",
      "epoch:9 step:9004 [D loss: 0.226371, acc.: 65.62%] [G loss: 0.427251]\n",
      "epoch:9 step:9005 [D loss: 0.209115, acc.: 70.31%] [G loss: 0.456776]\n",
      "epoch:9 step:9006 [D loss: 0.203694, acc.: 67.19%] [G loss: 0.474912]\n",
      "epoch:9 step:9007 [D loss: 0.168698, acc.: 73.44%] [G loss: 0.494053]\n",
      "epoch:9 step:9008 [D loss: 0.219509, acc.: 67.19%] [G loss: 0.479788]\n",
      "epoch:9 step:9009 [D loss: 0.248818, acc.: 53.12%] [G loss: 0.485080]\n",
      "epoch:9 step:9010 [D loss: 0.243534, acc.: 62.50%] [G loss: 0.425390]\n",
      "epoch:9 step:9011 [D loss: 0.213258, acc.: 63.28%] [G loss: 0.456309]\n",
      "epoch:9 step:9012 [D loss: 0.225889, acc.: 60.16%] [G loss: 0.458186]\n",
      "epoch:9 step:9013 [D loss: 0.239340, acc.: 61.72%] [G loss: 0.418905]\n",
      "epoch:9 step:9014 [D loss: 0.236578, acc.: 63.28%] [G loss: 0.414287]\n",
      "epoch:9 step:9015 [D loss: 0.223629, acc.: 58.59%] [G loss: 0.466278]\n",
      "epoch:9 step:9016 [D loss: 0.243805, acc.: 63.28%] [G loss: 0.507229]\n",
      "epoch:9 step:9017 [D loss: 0.235264, acc.: 58.59%] [G loss: 0.435170]\n",
      "epoch:9 step:9018 [D loss: 0.214580, acc.: 65.62%] [G loss: 0.432285]\n",
      "epoch:9 step:9019 [D loss: 0.224141, acc.: 64.06%] [G loss: 0.423807]\n",
      "epoch:9 step:9020 [D loss: 0.226854, acc.: 59.38%] [G loss: 0.419668]\n",
      "epoch:9 step:9021 [D loss: 0.200691, acc.: 66.41%] [G loss: 0.476918]\n",
      "epoch:9 step:9022 [D loss: 0.187194, acc.: 73.44%] [G loss: 0.479019]\n",
      "epoch:9 step:9023 [D loss: 0.224371, acc.: 64.84%] [G loss: 0.473491]\n",
      "epoch:9 step:9024 [D loss: 0.223172, acc.: 66.41%] [G loss: 0.435075]\n",
      "epoch:9 step:9025 [D loss: 0.174750, acc.: 74.22%] [G loss: 0.479448]\n",
      "epoch:9 step:9026 [D loss: 0.220987, acc.: 63.28%] [G loss: 0.458296]\n",
      "epoch:9 step:9027 [D loss: 0.246491, acc.: 53.91%] [G loss: 0.435779]\n",
      "epoch:9 step:9028 [D loss: 0.226974, acc.: 57.81%] [G loss: 0.417215]\n",
      "epoch:9 step:9029 [D loss: 0.238641, acc.: 64.84%] [G loss: 0.443109]\n",
      "epoch:9 step:9030 [D loss: 0.207871, acc.: 69.53%] [G loss: 0.458742]\n",
      "epoch:9 step:9031 [D loss: 0.211382, acc.: 67.97%] [G loss: 0.439482]\n",
      "epoch:9 step:9032 [D loss: 0.217546, acc.: 70.31%] [G loss: 0.456912]\n",
      "epoch:9 step:9033 [D loss: 0.246933, acc.: 57.03%] [G loss: 0.418351]\n",
      "epoch:9 step:9034 [D loss: 0.241440, acc.: 56.25%] [G loss: 0.418466]\n",
      "epoch:9 step:9035 [D loss: 0.224530, acc.: 65.62%] [G loss: 0.474101]\n",
      "epoch:9 step:9036 [D loss: 0.225679, acc.: 64.06%] [G loss: 0.481865]\n",
      "epoch:9 step:9037 [D loss: 0.207124, acc.: 66.41%] [G loss: 0.486705]\n",
      "epoch:9 step:9038 [D loss: 0.230011, acc.: 65.62%] [G loss: 0.453160]\n",
      "epoch:9 step:9039 [D loss: 0.218756, acc.: 67.19%] [G loss: 0.408990]\n",
      "epoch:9 step:9040 [D loss: 0.220836, acc.: 63.28%] [G loss: 0.466591]\n",
      "epoch:9 step:9041 [D loss: 0.203934, acc.: 73.44%] [G loss: 0.442642]\n",
      "epoch:9 step:9042 [D loss: 0.212261, acc.: 65.62%] [G loss: 0.467481]\n",
      "epoch:9 step:9043 [D loss: 0.245036, acc.: 58.59%] [G loss: 0.424933]\n",
      "epoch:9 step:9044 [D loss: 0.213971, acc.: 67.97%] [G loss: 0.428860]\n",
      "epoch:9 step:9045 [D loss: 0.209122, acc.: 70.31%] [G loss: 0.455768]\n",
      "epoch:9 step:9046 [D loss: 0.212701, acc.: 62.50%] [G loss: 0.415718]\n",
      "epoch:9 step:9047 [D loss: 0.222912, acc.: 66.41%] [G loss: 0.489939]\n",
      "epoch:9 step:9048 [D loss: 0.244416, acc.: 61.72%] [G loss: 0.454647]\n",
      "epoch:9 step:9049 [D loss: 0.250046, acc.: 56.25%] [G loss: 0.434804]\n",
      "epoch:9 step:9050 [D loss: 0.229718, acc.: 61.72%] [G loss: 0.453626]\n",
      "epoch:9 step:9051 [D loss: 0.223573, acc.: 62.50%] [G loss: 0.459554]\n",
      "epoch:9 step:9052 [D loss: 0.227773, acc.: 61.72%] [G loss: 0.440253]\n",
      "epoch:9 step:9053 [D loss: 0.232934, acc.: 67.19%] [G loss: 0.455050]\n",
      "epoch:9 step:9054 [D loss: 0.241089, acc.: 51.56%] [G loss: 0.463484]\n",
      "epoch:9 step:9055 [D loss: 0.245832, acc.: 63.28%] [G loss: 0.476091]\n",
      "epoch:9 step:9056 [D loss: 0.206168, acc.: 67.97%] [G loss: 0.451344]\n",
      "epoch:9 step:9057 [D loss: 0.204198, acc.: 66.41%] [G loss: 0.491765]\n",
      "epoch:9 step:9058 [D loss: 0.262580, acc.: 52.34%] [G loss: 0.474309]\n",
      "epoch:9 step:9059 [D loss: 0.192355, acc.: 72.66%] [G loss: 0.493988]\n",
      "epoch:9 step:9060 [D loss: 0.201603, acc.: 71.88%] [G loss: 0.457384]\n",
      "epoch:9 step:9061 [D loss: 0.236841, acc.: 65.62%] [G loss: 0.479922]\n",
      "epoch:9 step:9062 [D loss: 0.196074, acc.: 75.78%] [G loss: 0.501868]\n",
      "epoch:9 step:9063 [D loss: 0.216984, acc.: 69.53%] [G loss: 0.428485]\n",
      "epoch:9 step:9064 [D loss: 0.173636, acc.: 79.69%] [G loss: 0.464115]\n",
      "epoch:9 step:9065 [D loss: 0.193006, acc.: 75.00%] [G loss: 0.481851]\n",
      "epoch:9 step:9066 [D loss: 0.226048, acc.: 60.94%] [G loss: 0.465353]\n",
      "epoch:9 step:9067 [D loss: 0.203901, acc.: 71.09%] [G loss: 0.507968]\n",
      "epoch:9 step:9068 [D loss: 0.210737, acc.: 64.84%] [G loss: 0.500722]\n",
      "epoch:9 step:9069 [D loss: 0.217865, acc.: 61.72%] [G loss: 0.471096]\n",
      "epoch:9 step:9070 [D loss: 0.202510, acc.: 70.31%] [G loss: 0.483243]\n",
      "epoch:9 step:9071 [D loss: 0.223034, acc.: 61.72%] [G loss: 0.446231]\n",
      "epoch:9 step:9072 [D loss: 0.225244, acc.: 59.38%] [G loss: 0.438912]\n",
      "epoch:9 step:9073 [D loss: 0.221382, acc.: 63.28%] [G loss: 0.483684]\n",
      "epoch:9 step:9074 [D loss: 0.188812, acc.: 69.53%] [G loss: 0.473045]\n",
      "epoch:9 step:9075 [D loss: 0.181124, acc.: 75.00%] [G loss: 0.519423]\n",
      "epoch:9 step:9076 [D loss: 0.250453, acc.: 57.81%] [G loss: 0.502830]\n",
      "epoch:9 step:9077 [D loss: 0.250217, acc.: 57.03%] [G loss: 0.454690]\n",
      "epoch:9 step:9078 [D loss: 0.232458, acc.: 60.16%] [G loss: 0.438817]\n",
      "epoch:9 step:9079 [D loss: 0.219952, acc.: 64.84%] [G loss: 0.467335]\n",
      "epoch:9 step:9080 [D loss: 0.187175, acc.: 72.66%] [G loss: 0.518687]\n",
      "epoch:9 step:9081 [D loss: 0.173095, acc.: 75.78%] [G loss: 0.575484]\n",
      "epoch:9 step:9082 [D loss: 0.184177, acc.: 78.12%] [G loss: 0.516672]\n",
      "epoch:9 step:9083 [D loss: 0.205503, acc.: 68.75%] [G loss: 0.479090]\n",
      "epoch:9 step:9084 [D loss: 0.200344, acc.: 66.41%] [G loss: 0.493953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9085 [D loss: 0.235460, acc.: 60.16%] [G loss: 0.467414]\n",
      "epoch:9 step:9086 [D loss: 0.226724, acc.: 61.72%] [G loss: 0.455831]\n",
      "epoch:9 step:9087 [D loss: 0.199328, acc.: 68.75%] [G loss: 0.481648]\n",
      "epoch:9 step:9088 [D loss: 0.247974, acc.: 62.50%] [G loss: 0.482068]\n",
      "epoch:9 step:9089 [D loss: 0.219818, acc.: 62.50%] [G loss: 0.493119]\n",
      "epoch:9 step:9090 [D loss: 0.211024, acc.: 65.62%] [G loss: 0.459705]\n",
      "epoch:9 step:9091 [D loss: 0.236452, acc.: 60.94%] [G loss: 0.419274]\n",
      "epoch:9 step:9092 [D loss: 0.217999, acc.: 63.28%] [G loss: 0.481866]\n",
      "epoch:9 step:9093 [D loss: 0.198500, acc.: 69.53%] [G loss: 0.467741]\n",
      "epoch:9 step:9094 [D loss: 0.203415, acc.: 67.97%] [G loss: 0.477398]\n",
      "epoch:9 step:9095 [D loss: 0.251763, acc.: 54.69%] [G loss: 0.448761]\n",
      "epoch:9 step:9096 [D loss: 0.235947, acc.: 62.50%] [G loss: 0.427747]\n",
      "epoch:9 step:9097 [D loss: 0.225674, acc.: 63.28%] [G loss: 0.511891]\n",
      "epoch:9 step:9098 [D loss: 0.231008, acc.: 64.06%] [G loss: 0.498540]\n",
      "epoch:9 step:9099 [D loss: 0.221488, acc.: 67.19%] [G loss: 0.502417]\n",
      "epoch:9 step:9100 [D loss: 0.249728, acc.: 59.38%] [G loss: 0.449649]\n",
      "epoch:9 step:9101 [D loss: 0.221464, acc.: 71.09%] [G loss: 0.477870]\n",
      "epoch:9 step:9102 [D loss: 0.214273, acc.: 68.75%] [G loss: 0.478123]\n",
      "epoch:9 step:9103 [D loss: 0.232882, acc.: 64.06%] [G loss: 0.466935]\n",
      "epoch:9 step:9104 [D loss: 0.222420, acc.: 61.72%] [G loss: 0.477125]\n",
      "epoch:9 step:9105 [D loss: 0.239855, acc.: 58.59%] [G loss: 0.493066]\n",
      "epoch:9 step:9106 [D loss: 0.218931, acc.: 68.75%] [G loss: 0.471886]\n",
      "epoch:9 step:9107 [D loss: 0.218892, acc.: 65.62%] [G loss: 0.442640]\n",
      "epoch:9 step:9108 [D loss: 0.236206, acc.: 59.38%] [G loss: 0.431341]\n",
      "epoch:9 step:9109 [D loss: 0.244235, acc.: 64.06%] [G loss: 0.442407]\n",
      "epoch:9 step:9110 [D loss: 0.199694, acc.: 71.09%] [G loss: 0.428407]\n",
      "epoch:9 step:9111 [D loss: 0.223760, acc.: 63.28%] [G loss: 0.457148]\n",
      "epoch:9 step:9112 [D loss: 0.198508, acc.: 69.53%] [G loss: 0.460920]\n",
      "epoch:9 step:9113 [D loss: 0.220196, acc.: 63.28%] [G loss: 0.444452]\n",
      "epoch:9 step:9114 [D loss: 0.202109, acc.: 68.75%] [G loss: 0.465686]\n",
      "epoch:9 step:9115 [D loss: 0.221129, acc.: 60.16%] [G loss: 0.436642]\n",
      "epoch:9 step:9116 [D loss: 0.238199, acc.: 61.72%] [G loss: 0.395857]\n",
      "epoch:9 step:9117 [D loss: 0.230068, acc.: 66.41%] [G loss: 0.411566]\n",
      "epoch:9 step:9118 [D loss: 0.203470, acc.: 68.75%] [G loss: 0.413656]\n",
      "epoch:9 step:9119 [D loss: 0.221260, acc.: 67.97%] [G loss: 0.443891]\n",
      "epoch:9 step:9120 [D loss: 0.217772, acc.: 61.72%] [G loss: 0.438654]\n",
      "epoch:9 step:9121 [D loss: 0.211277, acc.: 68.75%] [G loss: 0.441995]\n",
      "epoch:9 step:9122 [D loss: 0.214720, acc.: 64.84%] [G loss: 0.483107]\n",
      "epoch:9 step:9123 [D loss: 0.212828, acc.: 64.84%] [G loss: 0.493519]\n",
      "epoch:9 step:9124 [D loss: 0.203527, acc.: 69.53%] [G loss: 0.517396]\n",
      "epoch:9 step:9125 [D loss: 0.204120, acc.: 67.97%] [G loss: 0.509312]\n",
      "epoch:9 step:9126 [D loss: 0.208526, acc.: 64.84%] [G loss: 0.464047]\n",
      "epoch:9 step:9127 [D loss: 0.194433, acc.: 68.75%] [G loss: 0.486965]\n",
      "epoch:9 step:9128 [D loss: 0.250007, acc.: 60.94%] [G loss: 0.468999]\n",
      "epoch:9 step:9129 [D loss: 0.232741, acc.: 59.38%] [G loss: 0.468341]\n",
      "epoch:9 step:9130 [D loss: 0.206635, acc.: 70.31%] [G loss: 0.465421]\n",
      "epoch:9 step:9131 [D loss: 0.229734, acc.: 61.72%] [G loss: 0.435087]\n",
      "epoch:9 step:9132 [D loss: 0.203005, acc.: 66.41%] [G loss: 0.480616]\n",
      "epoch:9 step:9133 [D loss: 0.215564, acc.: 61.72%] [G loss: 0.486122]\n",
      "epoch:9 step:9134 [D loss: 0.208066, acc.: 66.41%] [G loss: 0.519565]\n",
      "epoch:9 step:9135 [D loss: 0.255344, acc.: 57.81%] [G loss: 0.515172]\n",
      "epoch:9 step:9136 [D loss: 0.239666, acc.: 61.72%] [G loss: 0.452660]\n",
      "epoch:9 step:9137 [D loss: 0.264352, acc.: 55.47%] [G loss: 0.408157]\n",
      "epoch:9 step:9138 [D loss: 0.218139, acc.: 65.62%] [G loss: 0.416737]\n",
      "epoch:9 step:9139 [D loss: 0.209668, acc.: 67.19%] [G loss: 0.477078]\n",
      "epoch:9 step:9140 [D loss: 0.202746, acc.: 67.97%] [G loss: 0.480629]\n",
      "epoch:9 step:9141 [D loss: 0.191054, acc.: 71.88%] [G loss: 0.510846]\n",
      "epoch:9 step:9142 [D loss: 0.183791, acc.: 73.44%] [G loss: 0.530023]\n",
      "epoch:9 step:9143 [D loss: 0.228692, acc.: 63.28%] [G loss: 0.449495]\n",
      "epoch:9 step:9144 [D loss: 0.229268, acc.: 61.72%] [G loss: 0.449508]\n",
      "epoch:9 step:9145 [D loss: 0.199685, acc.: 65.62%] [G loss: 0.538294]\n",
      "epoch:9 step:9146 [D loss: 0.232898, acc.: 60.94%] [G loss: 0.438906]\n",
      "epoch:9 step:9147 [D loss: 0.213781, acc.: 62.50%] [G loss: 0.438793]\n",
      "epoch:9 step:9148 [D loss: 0.210837, acc.: 68.75%] [G loss: 0.428219]\n",
      "epoch:9 step:9149 [D loss: 0.256130, acc.: 53.91%] [G loss: 0.430320]\n",
      "epoch:9 step:9150 [D loss: 0.229331, acc.: 66.41%] [G loss: 0.405245]\n",
      "epoch:9 step:9151 [D loss: 0.244471, acc.: 52.34%] [G loss: 0.432539]\n",
      "epoch:9 step:9152 [D loss: 0.192558, acc.: 72.66%] [G loss: 0.470372]\n",
      "epoch:9 step:9153 [D loss: 0.220178, acc.: 60.16%] [G loss: 0.440012]\n",
      "epoch:9 step:9154 [D loss: 0.219911, acc.: 59.38%] [G loss: 0.500954]\n",
      "epoch:9 step:9155 [D loss: 0.241468, acc.: 53.91%] [G loss: 0.461673]\n",
      "epoch:9 step:9156 [D loss: 0.211774, acc.: 67.19%] [G loss: 0.450744]\n",
      "epoch:9 step:9157 [D loss: 0.204816, acc.: 69.53%] [G loss: 0.480235]\n",
      "epoch:9 step:9158 [D loss: 0.204584, acc.: 70.31%] [G loss: 0.485202]\n",
      "epoch:9 step:9159 [D loss: 0.224108, acc.: 64.84%] [G loss: 0.494046]\n",
      "epoch:9 step:9160 [D loss: 0.245956, acc.: 62.50%] [G loss: 0.433018]\n",
      "epoch:9 step:9161 [D loss: 0.213048, acc.: 66.41%] [G loss: 0.476763]\n",
      "epoch:9 step:9162 [D loss: 0.222171, acc.: 61.72%] [G loss: 0.467818]\n",
      "epoch:9 step:9163 [D loss: 0.203892, acc.: 69.53%] [G loss: 0.515068]\n",
      "epoch:9 step:9164 [D loss: 0.188353, acc.: 74.22%] [G loss: 0.519373]\n",
      "epoch:9 step:9165 [D loss: 0.255155, acc.: 57.81%] [G loss: 0.411058]\n",
      "epoch:9 step:9166 [D loss: 0.203285, acc.: 70.31%] [G loss: 0.468512]\n",
      "epoch:9 step:9167 [D loss: 0.246605, acc.: 57.81%] [G loss: 0.428546]\n",
      "epoch:9 step:9168 [D loss: 0.235996, acc.: 60.16%] [G loss: 0.428513]\n",
      "epoch:9 step:9169 [D loss: 0.190640, acc.: 71.88%] [G loss: 0.435240]\n",
      "epoch:9 step:9170 [D loss: 0.208456, acc.: 64.06%] [G loss: 0.452574]\n",
      "epoch:9 step:9171 [D loss: 0.236756, acc.: 56.25%] [G loss: 0.466542]\n",
      "epoch:9 step:9172 [D loss: 0.228175, acc.: 65.62%] [G loss: 0.447969]\n",
      "epoch:9 step:9173 [D loss: 0.236099, acc.: 61.72%] [G loss: 0.455090]\n",
      "epoch:9 step:9174 [D loss: 0.236656, acc.: 60.94%] [G loss: 0.456772]\n",
      "epoch:9 step:9175 [D loss: 0.224490, acc.: 64.84%] [G loss: 0.503684]\n",
      "epoch:9 step:9176 [D loss: 0.217052, acc.: 60.94%] [G loss: 0.457419]\n",
      "epoch:9 step:9177 [D loss: 0.230424, acc.: 57.81%] [G loss: 0.415547]\n",
      "epoch:9 step:9178 [D loss: 0.216129, acc.: 64.06%] [G loss: 0.436709]\n",
      "epoch:9 step:9179 [D loss: 0.200599, acc.: 67.19%] [G loss: 0.474870]\n",
      "epoch:9 step:9180 [D loss: 0.198918, acc.: 70.31%] [G loss: 0.467167]\n",
      "epoch:9 step:9181 [D loss: 0.218839, acc.: 62.50%] [G loss: 0.469639]\n",
      "epoch:9 step:9182 [D loss: 0.204730, acc.: 67.19%] [G loss: 0.464771]\n",
      "epoch:9 step:9183 [D loss: 0.204708, acc.: 65.62%] [G loss: 0.447776]\n",
      "epoch:9 step:9184 [D loss: 0.211871, acc.: 67.97%] [G loss: 0.452791]\n",
      "epoch:9 step:9185 [D loss: 0.233057, acc.: 64.06%] [G loss: 0.485678]\n",
      "epoch:9 step:9186 [D loss: 0.212019, acc.: 60.16%] [G loss: 0.448152]\n",
      "epoch:9 step:9187 [D loss: 0.210770, acc.: 67.19%] [G loss: 0.470623]\n",
      "epoch:9 step:9188 [D loss: 0.205889, acc.: 71.09%] [G loss: 0.464810]\n",
      "epoch:9 step:9189 [D loss: 0.227111, acc.: 61.72%] [G loss: 0.504572]\n",
      "epoch:9 step:9190 [D loss: 0.207675, acc.: 67.97%] [G loss: 0.493380]\n",
      "epoch:9 step:9191 [D loss: 0.246615, acc.: 59.38%] [G loss: 0.438086]\n",
      "epoch:9 step:9192 [D loss: 0.244183, acc.: 62.50%] [G loss: 0.448798]\n",
      "epoch:9 step:9193 [D loss: 0.204183, acc.: 67.97%] [G loss: 0.471770]\n",
      "epoch:9 step:9194 [D loss: 0.252407, acc.: 51.56%] [G loss: 0.410241]\n",
      "epoch:9 step:9195 [D loss: 0.237643, acc.: 57.81%] [G loss: 0.474478]\n",
      "epoch:9 step:9196 [D loss: 0.238947, acc.: 64.06%] [G loss: 0.423090]\n",
      "epoch:9 step:9197 [D loss: 0.217327, acc.: 65.62%] [G loss: 0.449173]\n",
      "epoch:9 step:9198 [D loss: 0.287940, acc.: 53.12%] [G loss: 0.395275]\n",
      "epoch:9 step:9199 [D loss: 0.247990, acc.: 59.38%] [G loss: 0.431371]\n",
      "epoch:9 step:9200 [D loss: 0.216361, acc.: 68.75%] [G loss: 0.478261]\n",
      "##############\n",
      "[2.53922802 1.65900858 6.21324501 4.79923947 3.6581118  5.63458985\n",
      " 4.44820966 4.29109135 4.5655806  3.66912671]\n",
      "##########\n",
      "epoch:9 step:9201 [D loss: 0.196872, acc.: 70.31%] [G loss: 0.474987]\n",
      "epoch:9 step:9202 [D loss: 0.211278, acc.: 69.53%] [G loss: 0.515700]\n",
      "epoch:9 step:9203 [D loss: 0.212363, acc.: 65.62%] [G loss: 0.498441]\n",
      "epoch:9 step:9204 [D loss: 0.218806, acc.: 67.97%] [G loss: 0.449910]\n",
      "epoch:9 step:9205 [D loss: 0.215465, acc.: 64.06%] [G loss: 0.443466]\n",
      "epoch:9 step:9206 [D loss: 0.223326, acc.: 66.41%] [G loss: 0.443721]\n",
      "epoch:9 step:9207 [D loss: 0.221310, acc.: 65.62%] [G loss: 0.448742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9208 [D loss: 0.206433, acc.: 67.97%] [G loss: 0.494990]\n",
      "epoch:9 step:9209 [D loss: 0.232859, acc.: 59.38%] [G loss: 0.469110]\n",
      "epoch:9 step:9210 [D loss: 0.230000, acc.: 57.03%] [G loss: 0.468696]\n",
      "epoch:9 step:9211 [D loss: 0.205598, acc.: 71.09%] [G loss: 0.463298]\n",
      "epoch:9 step:9212 [D loss: 0.249842, acc.: 59.38%] [G loss: 0.437402]\n",
      "epoch:9 step:9213 [D loss: 0.203813, acc.: 69.53%] [G loss: 0.437290]\n",
      "epoch:9 step:9214 [D loss: 0.169692, acc.: 76.56%] [G loss: 0.557535]\n",
      "epoch:9 step:9215 [D loss: 0.203131, acc.: 71.09%] [G loss: 0.510298]\n",
      "epoch:9 step:9216 [D loss: 0.233600, acc.: 59.38%] [G loss: 0.514459]\n",
      "epoch:9 step:9217 [D loss: 0.265033, acc.: 50.00%] [G loss: 0.430604]\n",
      "epoch:9 step:9218 [D loss: 0.237141, acc.: 60.94%] [G loss: 0.436464]\n",
      "epoch:9 step:9219 [D loss: 0.183499, acc.: 71.88%] [G loss: 0.520345]\n",
      "epoch:9 step:9220 [D loss: 0.275111, acc.: 52.34%] [G loss: 0.434710]\n",
      "epoch:9 step:9221 [D loss: 0.275484, acc.: 52.34%] [G loss: 0.413795]\n",
      "epoch:9 step:9222 [D loss: 0.235229, acc.: 58.59%] [G loss: 0.452405]\n",
      "epoch:9 step:9223 [D loss: 0.219171, acc.: 62.50%] [G loss: 0.484372]\n",
      "epoch:9 step:9224 [D loss: 0.240834, acc.: 59.38%] [G loss: 0.436535]\n",
      "epoch:9 step:9225 [D loss: 0.184486, acc.: 71.09%] [G loss: 0.510998]\n",
      "epoch:9 step:9226 [D loss: 0.220029, acc.: 63.28%] [G loss: 0.487044]\n",
      "epoch:9 step:9227 [D loss: 0.294451, acc.: 46.88%] [G loss: 0.433203]\n",
      "epoch:9 step:9228 [D loss: 0.239242, acc.: 58.59%] [G loss: 0.428354]\n",
      "epoch:9 step:9229 [D loss: 0.208430, acc.: 65.62%] [G loss: 0.493589]\n",
      "epoch:9 step:9230 [D loss: 0.222014, acc.: 64.84%] [G loss: 0.469576]\n",
      "epoch:9 step:9231 [D loss: 0.247337, acc.: 59.38%] [G loss: 0.430598]\n",
      "epoch:9 step:9232 [D loss: 0.229464, acc.: 60.94%] [G loss: 0.471507]\n",
      "epoch:9 step:9233 [D loss: 0.246547, acc.: 61.72%] [G loss: 0.515331]\n",
      "epoch:9 step:9234 [D loss: 0.212321, acc.: 68.75%] [G loss: 0.503886]\n",
      "epoch:9 step:9235 [D loss: 0.193119, acc.: 70.31%] [G loss: 0.500957]\n",
      "epoch:9 step:9236 [D loss: 0.205927, acc.: 63.28%] [G loss: 0.482666]\n",
      "epoch:9 step:9237 [D loss: 0.207672, acc.: 70.31%] [G loss: 0.488663]\n",
      "epoch:9 step:9238 [D loss: 0.219497, acc.: 64.84%] [G loss: 0.431053]\n",
      "epoch:9 step:9239 [D loss: 0.198975, acc.: 70.31%] [G loss: 0.454701]\n",
      "epoch:9 step:9240 [D loss: 0.197195, acc.: 68.75%] [G loss: 0.461351]\n",
      "epoch:9 step:9241 [D loss: 0.237442, acc.: 57.03%] [G loss: 0.430794]\n",
      "epoch:9 step:9242 [D loss: 0.249576, acc.: 59.38%] [G loss: 0.461125]\n",
      "epoch:9 step:9243 [D loss: 0.208135, acc.: 67.19%] [G loss: 0.477213]\n",
      "epoch:9 step:9244 [D loss: 0.227754, acc.: 64.06%] [G loss: 0.454011]\n",
      "epoch:9 step:9245 [D loss: 0.259210, acc.: 53.91%] [G loss: 0.442861]\n",
      "epoch:9 step:9246 [D loss: 0.239239, acc.: 58.59%] [G loss: 0.430531]\n",
      "epoch:9 step:9247 [D loss: 0.240775, acc.: 53.12%] [G loss: 0.429306]\n",
      "epoch:9 step:9248 [D loss: 0.225133, acc.: 65.62%] [G loss: 0.510329]\n",
      "epoch:9 step:9249 [D loss: 0.219197, acc.: 62.50%] [G loss: 0.507603]\n",
      "epoch:9 step:9250 [D loss: 0.264447, acc.: 50.78%] [G loss: 0.448044]\n",
      "epoch:9 step:9251 [D loss: 0.250431, acc.: 53.12%] [G loss: 0.442793]\n",
      "epoch:9 step:9252 [D loss: 0.222935, acc.: 62.50%] [G loss: 0.420071]\n",
      "epoch:9 step:9253 [D loss: 0.261056, acc.: 55.47%] [G loss: 0.433234]\n",
      "epoch:9 step:9254 [D loss: 0.222928, acc.: 64.06%] [G loss: 0.439942]\n",
      "epoch:9 step:9255 [D loss: 0.200733, acc.: 71.09%] [G loss: 0.461326]\n",
      "epoch:9 step:9256 [D loss: 0.210566, acc.: 71.88%] [G loss: 0.486273]\n",
      "epoch:9 step:9257 [D loss: 0.253674, acc.: 57.81%] [G loss: 0.456590]\n",
      "epoch:9 step:9258 [D loss: 0.216866, acc.: 67.19%] [G loss: 0.436852]\n",
      "epoch:9 step:9259 [D loss: 0.231442, acc.: 64.84%] [G loss: 0.452395]\n",
      "epoch:9 step:9260 [D loss: 0.260967, acc.: 53.91%] [G loss: 0.451522]\n",
      "epoch:9 step:9261 [D loss: 0.253083, acc.: 53.12%] [G loss: 0.431262]\n",
      "epoch:9 step:9262 [D loss: 0.225858, acc.: 63.28%] [G loss: 0.415934]\n",
      "epoch:9 step:9263 [D loss: 0.222270, acc.: 64.84%] [G loss: 0.467902]\n",
      "epoch:9 step:9264 [D loss: 0.210516, acc.: 67.97%] [G loss: 0.448312]\n",
      "epoch:9 step:9265 [D loss: 0.206270, acc.: 67.97%] [G loss: 0.469222]\n",
      "epoch:9 step:9266 [D loss: 0.189239, acc.: 67.97%] [G loss: 0.493044]\n",
      "epoch:9 step:9267 [D loss: 0.256633, acc.: 55.47%] [G loss: 0.478068]\n",
      "epoch:9 step:9268 [D loss: 0.223511, acc.: 65.62%] [G loss: 0.433406]\n",
      "epoch:9 step:9269 [D loss: 0.223617, acc.: 64.84%] [G loss: 0.443248]\n",
      "epoch:9 step:9270 [D loss: 0.204494, acc.: 67.19%] [G loss: 0.467753]\n",
      "epoch:9 step:9271 [D loss: 0.216963, acc.: 61.72%] [G loss: 0.466312]\n",
      "epoch:9 step:9272 [D loss: 0.213898, acc.: 67.19%] [G loss: 0.448074]\n",
      "epoch:9 step:9273 [D loss: 0.219386, acc.: 60.94%] [G loss: 0.448040]\n",
      "epoch:9 step:9274 [D loss: 0.206044, acc.: 67.97%] [G loss: 0.468353]\n",
      "epoch:9 step:9275 [D loss: 0.191407, acc.: 78.12%] [G loss: 0.452103]\n",
      "epoch:9 step:9276 [D loss: 0.232658, acc.: 60.16%] [G loss: 0.457581]\n",
      "epoch:9 step:9277 [D loss: 0.229005, acc.: 64.06%] [G loss: 0.454900]\n",
      "epoch:9 step:9278 [D loss: 0.209099, acc.: 67.19%] [G loss: 0.483984]\n",
      "epoch:9 step:9279 [D loss: 0.238197, acc.: 56.25%] [G loss: 0.459796]\n",
      "epoch:9 step:9280 [D loss: 0.224177, acc.: 60.16%] [G loss: 0.447385]\n",
      "epoch:9 step:9281 [D loss: 0.213047, acc.: 69.53%] [G loss: 0.447646]\n",
      "epoch:9 step:9282 [D loss: 0.208741, acc.: 68.75%] [G loss: 0.449431]\n",
      "epoch:9 step:9283 [D loss: 0.253606, acc.: 55.47%] [G loss: 0.457432]\n",
      "epoch:9 step:9284 [D loss: 0.244118, acc.: 56.25%] [G loss: 0.456318]\n",
      "epoch:9 step:9285 [D loss: 0.243701, acc.: 57.81%] [G loss: 0.462844]\n",
      "epoch:9 step:9286 [D loss: 0.215351, acc.: 67.97%] [G loss: 0.478196]\n",
      "epoch:9 step:9287 [D loss: 0.216228, acc.: 65.62%] [G loss: 0.474585]\n",
      "epoch:9 step:9288 [D loss: 0.233181, acc.: 58.59%] [G loss: 0.439956]\n",
      "epoch:9 step:9289 [D loss: 0.233935, acc.: 51.56%] [G loss: 0.444769]\n",
      "epoch:9 step:9290 [D loss: 0.216589, acc.: 70.31%] [G loss: 0.460937]\n",
      "epoch:9 step:9291 [D loss: 0.252659, acc.: 59.38%] [G loss: 0.456438]\n",
      "epoch:9 step:9292 [D loss: 0.251968, acc.: 57.03%] [G loss: 0.428553]\n",
      "epoch:9 step:9293 [D loss: 0.193555, acc.: 71.09%] [G loss: 0.489793]\n",
      "epoch:9 step:9294 [D loss: 0.276577, acc.: 53.12%] [G loss: 0.462752]\n",
      "epoch:9 step:9295 [D loss: 0.241971, acc.: 58.59%] [G loss: 0.425340]\n",
      "epoch:9 step:9296 [D loss: 0.213689, acc.: 67.97%] [G loss: 0.460431]\n",
      "epoch:9 step:9297 [D loss: 0.240767, acc.: 60.16%] [G loss: 0.439229]\n",
      "epoch:9 step:9298 [D loss: 0.242786, acc.: 63.28%] [G loss: 0.427029]\n",
      "epoch:9 step:9299 [D loss: 0.247703, acc.: 58.59%] [G loss: 0.418905]\n",
      "epoch:9 step:9300 [D loss: 0.227005, acc.: 64.84%] [G loss: 0.440197]\n",
      "epoch:9 step:9301 [D loss: 0.234085, acc.: 59.38%] [G loss: 0.464327]\n",
      "epoch:9 step:9302 [D loss: 0.242777, acc.: 55.47%] [G loss: 0.431967]\n",
      "epoch:9 step:9303 [D loss: 0.228592, acc.: 61.72%] [G loss: 0.447901]\n",
      "epoch:9 step:9304 [D loss: 0.193863, acc.: 67.19%] [G loss: 0.507056]\n",
      "epoch:9 step:9305 [D loss: 0.213262, acc.: 61.72%] [G loss: 0.479705]\n",
      "epoch:9 step:9306 [D loss: 0.217085, acc.: 63.28%] [G loss: 0.443208]\n",
      "epoch:9 step:9307 [D loss: 0.223505, acc.: 62.50%] [G loss: 0.450727]\n",
      "epoch:9 step:9308 [D loss: 0.199000, acc.: 74.22%] [G loss: 0.473831]\n",
      "epoch:9 step:9309 [D loss: 0.226306, acc.: 64.84%] [G loss: 0.493241]\n",
      "epoch:9 step:9310 [D loss: 0.217871, acc.: 62.50%] [G loss: 0.460618]\n",
      "epoch:9 step:9311 [D loss: 0.234051, acc.: 61.72%] [G loss: 0.465529]\n",
      "epoch:9 step:9312 [D loss: 0.207044, acc.: 64.84%] [G loss: 0.472864]\n",
      "epoch:9 step:9313 [D loss: 0.248192, acc.: 57.81%] [G loss: 0.400444]\n",
      "epoch:9 step:9314 [D loss: 0.225288, acc.: 67.19%] [G loss: 0.444207]\n",
      "epoch:9 step:9315 [D loss: 0.226059, acc.: 60.16%] [G loss: 0.452720]\n",
      "epoch:9 step:9316 [D loss: 0.222814, acc.: 59.38%] [G loss: 0.506913]\n",
      "epoch:9 step:9317 [D loss: 0.219094, acc.: 63.28%] [G loss: 0.479144]\n",
      "epoch:9 step:9318 [D loss: 0.208020, acc.: 68.75%] [G loss: 0.525952]\n",
      "epoch:9 step:9319 [D loss: 0.173992, acc.: 75.00%] [G loss: 0.497745]\n",
      "epoch:9 step:9320 [D loss: 0.247036, acc.: 57.81%] [G loss: 0.458876]\n",
      "epoch:9 step:9321 [D loss: 0.233892, acc.: 61.72%] [G loss: 0.480477]\n",
      "epoch:9 step:9322 [D loss: 0.214877, acc.: 67.19%] [G loss: 0.464624]\n",
      "epoch:9 step:9323 [D loss: 0.198883, acc.: 71.88%] [G loss: 0.490060]\n",
      "epoch:9 step:9324 [D loss: 0.253612, acc.: 54.69%] [G loss: 0.446597]\n",
      "epoch:9 step:9325 [D loss: 0.261241, acc.: 53.12%] [G loss: 0.421586]\n",
      "epoch:9 step:9326 [D loss: 0.208846, acc.: 67.19%] [G loss: 0.439397]\n",
      "epoch:9 step:9327 [D loss: 0.200446, acc.: 70.31%] [G loss: 0.450815]\n",
      "epoch:9 step:9328 [D loss: 0.213764, acc.: 61.72%] [G loss: 0.476649]\n",
      "epoch:9 step:9329 [D loss: 0.211532, acc.: 67.19%] [G loss: 0.523952]\n",
      "epoch:9 step:9330 [D loss: 0.206609, acc.: 66.41%] [G loss: 0.503141]\n",
      "epoch:9 step:9331 [D loss: 0.200093, acc.: 68.75%] [G loss: 0.491524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9332 [D loss: 0.175037, acc.: 73.44%] [G loss: 0.541098]\n",
      "epoch:9 step:9333 [D loss: 0.203162, acc.: 70.31%] [G loss: 0.512381]\n",
      "epoch:9 step:9334 [D loss: 0.210514, acc.: 65.62%] [G loss: 0.504631]\n",
      "epoch:9 step:9335 [D loss: 0.249436, acc.: 56.25%] [G loss: 0.438462]\n",
      "epoch:9 step:9336 [D loss: 0.217348, acc.: 64.84%] [G loss: 0.475504]\n",
      "epoch:9 step:9337 [D loss: 0.219325, acc.: 67.97%] [G loss: 0.483890]\n",
      "epoch:9 step:9338 [D loss: 0.210759, acc.: 66.41%] [G loss: 0.505945]\n",
      "epoch:9 step:9339 [D loss: 0.205518, acc.: 65.62%] [G loss: 0.498400]\n",
      "epoch:9 step:9340 [D loss: 0.215841, acc.: 67.19%] [G loss: 0.476987]\n",
      "epoch:9 step:9341 [D loss: 0.206022, acc.: 67.19%] [G loss: 0.502667]\n",
      "epoch:9 step:9342 [D loss: 0.201888, acc.: 67.97%] [G loss: 0.492995]\n",
      "epoch:9 step:9343 [D loss: 0.230495, acc.: 62.50%] [G loss: 0.460346]\n",
      "epoch:9 step:9344 [D loss: 0.211982, acc.: 72.66%] [G loss: 0.463175]\n",
      "epoch:9 step:9345 [D loss: 0.197641, acc.: 67.19%] [G loss: 0.492725]\n",
      "epoch:9 step:9346 [D loss: 0.255544, acc.: 60.16%] [G loss: 0.481456]\n",
      "epoch:9 step:9347 [D loss: 0.258442, acc.: 59.38%] [G loss: 0.480194]\n",
      "epoch:9 step:9348 [D loss: 0.271655, acc.: 55.47%] [G loss: 0.459716]\n",
      "epoch:9 step:9349 [D loss: 0.210670, acc.: 62.50%] [G loss: 0.493695]\n",
      "epoch:9 step:9350 [D loss: 0.240954, acc.: 55.47%] [G loss: 0.488485]\n",
      "epoch:9 step:9351 [D loss: 0.186989, acc.: 73.44%] [G loss: 0.546977]\n",
      "epoch:9 step:9352 [D loss: 0.212898, acc.: 65.62%] [G loss: 0.508318]\n",
      "epoch:9 step:9353 [D loss: 0.294713, acc.: 44.53%] [G loss: 0.434476]\n",
      "epoch:9 step:9354 [D loss: 0.193963, acc.: 74.22%] [G loss: 0.406455]\n",
      "epoch:9 step:9355 [D loss: 0.213823, acc.: 67.97%] [G loss: 0.475104]\n",
      "epoch:9 step:9356 [D loss: 0.181486, acc.: 71.09%] [G loss: 0.488346]\n",
      "epoch:9 step:9357 [D loss: 0.173674, acc.: 79.69%] [G loss: 0.478013]\n",
      "epoch:9 step:9358 [D loss: 0.194021, acc.: 73.44%] [G loss: 0.531013]\n",
      "epoch:9 step:9359 [D loss: 0.173342, acc.: 73.44%] [G loss: 0.596787]\n",
      "epoch:9 step:9360 [D loss: 0.184488, acc.: 71.88%] [G loss: 0.568031]\n",
      "epoch:9 step:9361 [D loss: 0.311236, acc.: 53.12%] [G loss: 0.503691]\n",
      "epoch:9 step:9362 [D loss: 0.224562, acc.: 60.16%] [G loss: 0.554956]\n",
      "epoch:9 step:9363 [D loss: 0.207112, acc.: 69.53%] [G loss: 0.490155]\n",
      "epoch:9 step:9364 [D loss: 0.254461, acc.: 52.34%] [G loss: 0.488646]\n",
      "epoch:9 step:9365 [D loss: 0.285434, acc.: 51.56%] [G loss: 0.506899]\n",
      "epoch:9 step:9366 [D loss: 0.211847, acc.: 67.19%] [G loss: 0.498247]\n",
      "epoch:9 step:9367 [D loss: 0.234822, acc.: 65.62%] [G loss: 0.488882]\n",
      "epoch:9 step:9368 [D loss: 0.246256, acc.: 66.41%] [G loss: 0.509998]\n",
      "epoch:9 step:9369 [D loss: 0.183161, acc.: 71.88%] [G loss: 0.549651]\n",
      "epoch:9 step:9370 [D loss: 0.182282, acc.: 72.66%] [G loss: 0.607328]\n",
      "epoch:10 step:9371 [D loss: 0.225540, acc.: 67.19%] [G loss: 0.524785]\n",
      "epoch:10 step:9372 [D loss: 0.297414, acc.: 53.12%] [G loss: 0.463826]\n",
      "epoch:10 step:9373 [D loss: 0.249638, acc.: 60.94%] [G loss: 0.509604]\n",
      "epoch:10 step:9374 [D loss: 0.204940, acc.: 75.00%] [G loss: 0.509353]\n",
      "epoch:10 step:9375 [D loss: 0.224170, acc.: 61.72%] [G loss: 0.428460]\n",
      "epoch:10 step:9376 [D loss: 0.201552, acc.: 65.62%] [G loss: 0.471783]\n",
      "epoch:10 step:9377 [D loss: 0.205890, acc.: 67.19%] [G loss: 0.486046]\n",
      "epoch:10 step:9378 [D loss: 0.203328, acc.: 64.84%] [G loss: 0.491647]\n",
      "epoch:10 step:9379 [D loss: 0.203063, acc.: 67.19%] [G loss: 0.481577]\n",
      "epoch:10 step:9380 [D loss: 0.229947, acc.: 63.28%] [G loss: 0.482425]\n",
      "epoch:10 step:9381 [D loss: 0.191482, acc.: 67.19%] [G loss: 0.473444]\n",
      "epoch:10 step:9382 [D loss: 0.220617, acc.: 64.84%] [G loss: 0.452433]\n",
      "epoch:10 step:9383 [D loss: 0.247713, acc.: 60.16%] [G loss: 0.477203]\n",
      "epoch:10 step:9384 [D loss: 0.193152, acc.: 69.53%] [G loss: 0.486725]\n",
      "epoch:10 step:9385 [D loss: 0.204682, acc.: 72.66%] [G loss: 0.520104]\n",
      "epoch:10 step:9386 [D loss: 0.190216, acc.: 73.44%] [G loss: 0.523682]\n",
      "epoch:10 step:9387 [D loss: 0.244532, acc.: 54.69%] [G loss: 0.463335]\n",
      "epoch:10 step:9388 [D loss: 0.217180, acc.: 66.41%] [G loss: 0.449661]\n",
      "epoch:10 step:9389 [D loss: 0.262276, acc.: 53.12%] [G loss: 0.435827]\n",
      "epoch:10 step:9390 [D loss: 0.245923, acc.: 59.38%] [G loss: 0.426985]\n",
      "epoch:10 step:9391 [D loss: 0.213956, acc.: 60.16%] [G loss: 0.468583]\n",
      "epoch:10 step:9392 [D loss: 0.191165, acc.: 68.75%] [G loss: 0.513716]\n",
      "epoch:10 step:9393 [D loss: 0.266633, acc.: 56.25%] [G loss: 0.467227]\n",
      "epoch:10 step:9394 [D loss: 0.226808, acc.: 64.06%] [G loss: 0.480803]\n",
      "epoch:10 step:9395 [D loss: 0.205177, acc.: 68.75%] [G loss: 0.510520]\n",
      "epoch:10 step:9396 [D loss: 0.213622, acc.: 68.75%] [G loss: 0.503556]\n",
      "epoch:10 step:9397 [D loss: 0.245723, acc.: 62.50%] [G loss: 0.432084]\n",
      "epoch:10 step:9398 [D loss: 0.245135, acc.: 58.59%] [G loss: 0.410608]\n",
      "epoch:10 step:9399 [D loss: 0.229603, acc.: 60.16%] [G loss: 0.502641]\n",
      "epoch:10 step:9400 [D loss: 0.221554, acc.: 61.72%] [G loss: 0.490092]\n",
      "##############\n",
      "[2.59782366 1.5172828  5.89936952 4.70222318 3.71342023 5.50740533\n",
      " 4.66863322 4.87513057 4.56258611 3.60372875]\n",
      "##########\n",
      "epoch:10 step:9401 [D loss: 0.238408, acc.: 60.94%] [G loss: 0.497767]\n",
      "epoch:10 step:9402 [D loss: 0.240298, acc.: 57.81%] [G loss: 0.436407]\n",
      "epoch:10 step:9403 [D loss: 0.246176, acc.: 59.38%] [G loss: 0.436060]\n",
      "epoch:10 step:9404 [D loss: 0.235803, acc.: 64.06%] [G loss: 0.459544]\n",
      "epoch:10 step:9405 [D loss: 0.235471, acc.: 56.25%] [G loss: 0.456323]\n",
      "epoch:10 step:9406 [D loss: 0.215456, acc.: 67.97%] [G loss: 0.475246]\n",
      "epoch:10 step:9407 [D loss: 0.250572, acc.: 56.25%] [G loss: 0.436040]\n",
      "epoch:10 step:9408 [D loss: 0.253792, acc.: 56.25%] [G loss: 0.456124]\n",
      "epoch:10 step:9409 [D loss: 0.217905, acc.: 64.84%] [G loss: 0.428355]\n",
      "epoch:10 step:9410 [D loss: 0.211832, acc.: 65.62%] [G loss: 0.462974]\n",
      "epoch:10 step:9411 [D loss: 0.215435, acc.: 64.06%] [G loss: 0.485576]\n",
      "epoch:10 step:9412 [D loss: 0.218035, acc.: 60.94%] [G loss: 0.438928]\n",
      "epoch:10 step:9413 [D loss: 0.234662, acc.: 65.62%] [G loss: 0.397287]\n",
      "epoch:10 step:9414 [D loss: 0.250699, acc.: 54.69%] [G loss: 0.430436]\n",
      "epoch:10 step:9415 [D loss: 0.208472, acc.: 65.62%] [G loss: 0.468177]\n",
      "epoch:10 step:9416 [D loss: 0.229384, acc.: 57.03%] [G loss: 0.436542]\n",
      "epoch:10 step:9417 [D loss: 0.206905, acc.: 71.88%] [G loss: 0.419893]\n",
      "epoch:10 step:9418 [D loss: 0.205349, acc.: 68.75%] [G loss: 0.435434]\n",
      "epoch:10 step:9419 [D loss: 0.203276, acc.: 64.84%] [G loss: 0.468269]\n",
      "epoch:10 step:9420 [D loss: 0.211708, acc.: 67.19%] [G loss: 0.482755]\n",
      "epoch:10 step:9421 [D loss: 0.252099, acc.: 59.38%] [G loss: 0.462068]\n",
      "epoch:10 step:9422 [D loss: 0.239520, acc.: 63.28%] [G loss: 0.434804]\n",
      "epoch:10 step:9423 [D loss: 0.226334, acc.: 66.41%] [G loss: 0.506105]\n",
      "epoch:10 step:9424 [D loss: 0.229020, acc.: 60.16%] [G loss: 0.472335]\n",
      "epoch:10 step:9425 [D loss: 0.212482, acc.: 65.62%] [G loss: 0.521456]\n",
      "epoch:10 step:9426 [D loss: 0.229747, acc.: 63.28%] [G loss: 0.472949]\n",
      "epoch:10 step:9427 [D loss: 0.258377, acc.: 60.16%] [G loss: 0.431352]\n",
      "epoch:10 step:9428 [D loss: 0.217374, acc.: 62.50%] [G loss: 0.484307]\n",
      "epoch:10 step:9429 [D loss: 0.222882, acc.: 61.72%] [G loss: 0.419052]\n",
      "epoch:10 step:9430 [D loss: 0.209944, acc.: 67.19%] [G loss: 0.450350]\n",
      "epoch:10 step:9431 [D loss: 0.226963, acc.: 58.59%] [G loss: 0.425616]\n",
      "epoch:10 step:9432 [D loss: 0.217119, acc.: 64.84%] [G loss: 0.422239]\n",
      "epoch:10 step:9433 [D loss: 0.235019, acc.: 64.06%] [G loss: 0.396582]\n",
      "epoch:10 step:9434 [D loss: 0.206566, acc.: 66.41%] [G loss: 0.456409]\n",
      "epoch:10 step:9435 [D loss: 0.219635, acc.: 59.38%] [G loss: 0.423797]\n",
      "epoch:10 step:9436 [D loss: 0.224992, acc.: 61.72%] [G loss: 0.444752]\n",
      "epoch:10 step:9437 [D loss: 0.238333, acc.: 64.84%] [G loss: 0.406734]\n",
      "epoch:10 step:9438 [D loss: 0.217330, acc.: 66.41%] [G loss: 0.436216]\n",
      "epoch:10 step:9439 [D loss: 0.190761, acc.: 75.00%] [G loss: 0.479878]\n",
      "epoch:10 step:9440 [D loss: 0.199432, acc.: 69.53%] [G loss: 0.498529]\n",
      "epoch:10 step:9441 [D loss: 0.254152, acc.: 53.91%] [G loss: 0.451740]\n",
      "epoch:10 step:9442 [D loss: 0.244925, acc.: 60.16%] [G loss: 0.449162]\n",
      "epoch:10 step:9443 [D loss: 0.211503, acc.: 60.16%] [G loss: 0.445375]\n",
      "epoch:10 step:9444 [D loss: 0.194774, acc.: 71.09%] [G loss: 0.496168]\n",
      "epoch:10 step:9445 [D loss: 0.215684, acc.: 67.19%] [G loss: 0.498836]\n",
      "epoch:10 step:9446 [D loss: 0.184714, acc.: 74.22%] [G loss: 0.507305]\n",
      "epoch:10 step:9447 [D loss: 0.184207, acc.: 75.78%] [G loss: 0.513561]\n",
      "epoch:10 step:9448 [D loss: 0.281378, acc.: 55.47%] [G loss: 0.447033]\n",
      "epoch:10 step:9449 [D loss: 0.223787, acc.: 58.59%] [G loss: 0.447581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9450 [D loss: 0.232356, acc.: 62.50%] [G loss: 0.416534]\n",
      "epoch:10 step:9451 [D loss: 0.240860, acc.: 62.50%] [G loss: 0.409510]\n",
      "epoch:10 step:9452 [D loss: 0.191793, acc.: 71.09%] [G loss: 0.461290]\n",
      "epoch:10 step:9453 [D loss: 0.198359, acc.: 68.75%] [G loss: 0.471816]\n",
      "epoch:10 step:9454 [D loss: 0.226437, acc.: 63.28%] [G loss: 0.448259]\n",
      "epoch:10 step:9455 [D loss: 0.235070, acc.: 61.72%] [G loss: 0.438133]\n",
      "epoch:10 step:9456 [D loss: 0.205795, acc.: 72.66%] [G loss: 0.484006]\n",
      "epoch:10 step:9457 [D loss: 0.207146, acc.: 67.19%] [G loss: 0.427933]\n",
      "epoch:10 step:9458 [D loss: 0.197359, acc.: 66.41%] [G loss: 0.485202]\n",
      "epoch:10 step:9459 [D loss: 0.214979, acc.: 67.19%] [G loss: 0.425502]\n",
      "epoch:10 step:9460 [D loss: 0.214819, acc.: 60.16%] [G loss: 0.476545]\n",
      "epoch:10 step:9461 [D loss: 0.240589, acc.: 60.94%] [G loss: 0.484673]\n",
      "epoch:10 step:9462 [D loss: 0.189770, acc.: 71.09%] [G loss: 0.481587]\n",
      "epoch:10 step:9463 [D loss: 0.202204, acc.: 64.84%] [G loss: 0.510829]\n",
      "epoch:10 step:9464 [D loss: 0.218063, acc.: 64.06%] [G loss: 0.500806]\n",
      "epoch:10 step:9465 [D loss: 0.194703, acc.: 67.19%] [G loss: 0.489499]\n",
      "epoch:10 step:9466 [D loss: 0.224405, acc.: 63.28%] [G loss: 0.467238]\n",
      "epoch:10 step:9467 [D loss: 0.192479, acc.: 71.88%] [G loss: 0.496851]\n",
      "epoch:10 step:9468 [D loss: 0.229423, acc.: 60.16%] [G loss: 0.462483]\n",
      "epoch:10 step:9469 [D loss: 0.227712, acc.: 61.72%] [G loss: 0.409764]\n",
      "epoch:10 step:9470 [D loss: 0.188370, acc.: 71.88%] [G loss: 0.526040]\n",
      "epoch:10 step:9471 [D loss: 0.237754, acc.: 63.28%] [G loss: 0.462743]\n",
      "epoch:10 step:9472 [D loss: 0.266244, acc.: 52.34%] [G loss: 0.421890]\n",
      "epoch:10 step:9473 [D loss: 0.222574, acc.: 66.41%] [G loss: 0.434009]\n",
      "epoch:10 step:9474 [D loss: 0.208477, acc.: 70.31%] [G loss: 0.444147]\n",
      "epoch:10 step:9475 [D loss: 0.237019, acc.: 56.25%] [G loss: 0.461073]\n",
      "epoch:10 step:9476 [D loss: 0.210095, acc.: 71.09%] [G loss: 0.470496]\n",
      "epoch:10 step:9477 [D loss: 0.221187, acc.: 63.28%] [G loss: 0.489975]\n",
      "epoch:10 step:9478 [D loss: 0.263806, acc.: 55.47%] [G loss: 0.496803]\n",
      "epoch:10 step:9479 [D loss: 0.234492, acc.: 60.16%] [G loss: 0.470082]\n",
      "epoch:10 step:9480 [D loss: 0.230574, acc.: 60.94%] [G loss: 0.430408]\n",
      "epoch:10 step:9481 [D loss: 0.199904, acc.: 67.97%] [G loss: 0.453139]\n",
      "epoch:10 step:9482 [D loss: 0.185752, acc.: 71.88%] [G loss: 0.477483]\n",
      "epoch:10 step:9483 [D loss: 0.236149, acc.: 55.47%] [G loss: 0.467678]\n",
      "epoch:10 step:9484 [D loss: 0.233242, acc.: 56.25%] [G loss: 0.471316]\n",
      "epoch:10 step:9485 [D loss: 0.180313, acc.: 78.12%] [G loss: 0.556910]\n",
      "epoch:10 step:9486 [D loss: 0.231483, acc.: 61.72%] [G loss: 0.474277]\n",
      "epoch:10 step:9487 [D loss: 0.200554, acc.: 70.31%] [G loss: 0.481494]\n",
      "epoch:10 step:9488 [D loss: 0.215442, acc.: 67.97%] [G loss: 0.497657]\n",
      "epoch:10 step:9489 [D loss: 0.183402, acc.: 74.22%] [G loss: 0.525249]\n",
      "epoch:10 step:9490 [D loss: 0.243434, acc.: 63.28%] [G loss: 0.461074]\n",
      "epoch:10 step:9491 [D loss: 0.219157, acc.: 62.50%] [G loss: 0.432340]\n",
      "epoch:10 step:9492 [D loss: 0.188929, acc.: 73.44%] [G loss: 0.500931]\n",
      "epoch:10 step:9493 [D loss: 0.207214, acc.: 70.31%] [G loss: 0.514144]\n",
      "epoch:10 step:9494 [D loss: 0.209226, acc.: 64.06%] [G loss: 0.503311]\n",
      "epoch:10 step:9495 [D loss: 0.237405, acc.: 61.72%] [G loss: 0.456371]\n",
      "epoch:10 step:9496 [D loss: 0.197823, acc.: 72.66%] [G loss: 0.459342]\n",
      "epoch:10 step:9497 [D loss: 0.250747, acc.: 53.91%] [G loss: 0.446987]\n",
      "epoch:10 step:9498 [D loss: 0.237311, acc.: 57.03%] [G loss: 0.437720]\n",
      "epoch:10 step:9499 [D loss: 0.230896, acc.: 59.38%] [G loss: 0.422496]\n",
      "epoch:10 step:9500 [D loss: 0.203448, acc.: 66.41%] [G loss: 0.460286]\n",
      "epoch:10 step:9501 [D loss: 0.223010, acc.: 60.16%] [G loss: 0.456564]\n",
      "epoch:10 step:9502 [D loss: 0.213987, acc.: 66.41%] [G loss: 0.456239]\n",
      "epoch:10 step:9503 [D loss: 0.246484, acc.: 54.69%] [G loss: 0.460276]\n",
      "epoch:10 step:9504 [D loss: 0.224774, acc.: 63.28%] [G loss: 0.505915]\n",
      "epoch:10 step:9505 [D loss: 0.210298, acc.: 65.62%] [G loss: 0.501473]\n",
      "epoch:10 step:9506 [D loss: 0.237287, acc.: 64.06%] [G loss: 0.456572]\n",
      "epoch:10 step:9507 [D loss: 0.251264, acc.: 56.25%] [G loss: 0.449980]\n",
      "epoch:10 step:9508 [D loss: 0.257251, acc.: 59.38%] [G loss: 0.397258]\n",
      "epoch:10 step:9509 [D loss: 0.221104, acc.: 59.38%] [G loss: 0.425301]\n",
      "epoch:10 step:9510 [D loss: 0.268257, acc.: 53.91%] [G loss: 0.419836]\n",
      "epoch:10 step:9511 [D loss: 0.201865, acc.: 71.88%] [G loss: 0.482511]\n",
      "epoch:10 step:9512 [D loss: 0.219946, acc.: 62.50%] [G loss: 0.418421]\n",
      "epoch:10 step:9513 [D loss: 0.215711, acc.: 63.28%] [G loss: 0.481612]\n",
      "epoch:10 step:9514 [D loss: 0.190070, acc.: 71.09%] [G loss: 0.437721]\n",
      "epoch:10 step:9515 [D loss: 0.213572, acc.: 71.09%] [G loss: 0.504542]\n",
      "epoch:10 step:9516 [D loss: 0.223293, acc.: 67.19%] [G loss: 0.438505]\n",
      "epoch:10 step:9517 [D loss: 0.246706, acc.: 58.59%] [G loss: 0.433476]\n",
      "epoch:10 step:9518 [D loss: 0.231196, acc.: 66.41%] [G loss: 0.448756]\n",
      "epoch:10 step:9519 [D loss: 0.217556, acc.: 68.75%] [G loss: 0.461693]\n",
      "epoch:10 step:9520 [D loss: 0.215657, acc.: 64.84%] [G loss: 0.471515]\n",
      "epoch:10 step:9521 [D loss: 0.202606, acc.: 68.75%] [G loss: 0.478021]\n",
      "epoch:10 step:9522 [D loss: 0.237011, acc.: 61.72%] [G loss: 0.477411]\n",
      "epoch:10 step:9523 [D loss: 0.244153, acc.: 63.28%] [G loss: 0.480031]\n",
      "epoch:10 step:9524 [D loss: 0.230701, acc.: 64.84%] [G loss: 0.453039]\n",
      "epoch:10 step:9525 [D loss: 0.192089, acc.: 69.53%] [G loss: 0.477860]\n",
      "epoch:10 step:9526 [D loss: 0.210183, acc.: 65.62%] [G loss: 0.478925]\n",
      "epoch:10 step:9527 [D loss: 0.230529, acc.: 60.16%] [G loss: 0.438627]\n",
      "epoch:10 step:9528 [D loss: 0.205044, acc.: 69.53%] [G loss: 0.475499]\n",
      "epoch:10 step:9529 [D loss: 0.222774, acc.: 63.28%] [G loss: 0.425764]\n",
      "epoch:10 step:9530 [D loss: 0.246309, acc.: 62.50%] [G loss: 0.433502]\n",
      "epoch:10 step:9531 [D loss: 0.247442, acc.: 66.41%] [G loss: 0.472384]\n",
      "epoch:10 step:9532 [D loss: 0.223885, acc.: 63.28%] [G loss: 0.491439]\n",
      "epoch:10 step:9533 [D loss: 0.206555, acc.: 65.62%] [G loss: 0.491634]\n",
      "epoch:10 step:9534 [D loss: 0.226118, acc.: 65.62%] [G loss: 0.460752]\n",
      "epoch:10 step:9535 [D loss: 0.217559, acc.: 65.62%] [G loss: 0.462781]\n",
      "epoch:10 step:9536 [D loss: 0.210533, acc.: 71.09%] [G loss: 0.499436]\n",
      "epoch:10 step:9537 [D loss: 0.255500, acc.: 50.78%] [G loss: 0.414619]\n",
      "epoch:10 step:9538 [D loss: 0.210350, acc.: 67.19%] [G loss: 0.460033]\n",
      "epoch:10 step:9539 [D loss: 0.241706, acc.: 62.50%] [G loss: 0.474937]\n",
      "epoch:10 step:9540 [D loss: 0.243036, acc.: 57.81%] [G loss: 0.437471]\n",
      "epoch:10 step:9541 [D loss: 0.215333, acc.: 62.50%] [G loss: 0.445783]\n",
      "epoch:10 step:9542 [D loss: 0.204500, acc.: 64.06%] [G loss: 0.456459]\n",
      "epoch:10 step:9543 [D loss: 0.218200, acc.: 70.31%] [G loss: 0.402352]\n",
      "epoch:10 step:9544 [D loss: 0.244096, acc.: 59.38%] [G loss: 0.503641]\n",
      "epoch:10 step:9545 [D loss: 0.235974, acc.: 58.59%] [G loss: 0.452161]\n",
      "epoch:10 step:9546 [D loss: 0.219030, acc.: 67.19%] [G loss: 0.416175]\n",
      "epoch:10 step:9547 [D loss: 0.227616, acc.: 60.94%] [G loss: 0.427715]\n",
      "epoch:10 step:9548 [D loss: 0.227120, acc.: 63.28%] [G loss: 0.417576]\n",
      "epoch:10 step:9549 [D loss: 0.248104, acc.: 60.16%] [G loss: 0.417956]\n",
      "epoch:10 step:9550 [D loss: 0.224136, acc.: 65.62%] [G loss: 0.453787]\n",
      "epoch:10 step:9551 [D loss: 0.222492, acc.: 64.84%] [G loss: 0.444322]\n",
      "epoch:10 step:9552 [D loss: 0.237668, acc.: 60.94%] [G loss: 0.415015]\n",
      "epoch:10 step:9553 [D loss: 0.232105, acc.: 61.72%] [G loss: 0.483601]\n",
      "epoch:10 step:9554 [D loss: 0.207537, acc.: 67.19%] [G loss: 0.495558]\n",
      "epoch:10 step:9555 [D loss: 0.223850, acc.: 63.28%] [G loss: 0.489573]\n",
      "epoch:10 step:9556 [D loss: 0.246871, acc.: 61.72%] [G loss: 0.445912]\n",
      "epoch:10 step:9557 [D loss: 0.228694, acc.: 59.38%] [G loss: 0.412744]\n",
      "epoch:10 step:9558 [D loss: 0.215617, acc.: 64.84%] [G loss: 0.469872]\n",
      "epoch:10 step:9559 [D loss: 0.227136, acc.: 67.19%] [G loss: 0.422338]\n",
      "epoch:10 step:9560 [D loss: 0.192462, acc.: 71.09%] [G loss: 0.464634]\n",
      "epoch:10 step:9561 [D loss: 0.200427, acc.: 64.84%] [G loss: 0.471341]\n",
      "epoch:10 step:9562 [D loss: 0.224327, acc.: 62.50%] [G loss: 0.454525]\n",
      "epoch:10 step:9563 [D loss: 0.210876, acc.: 63.28%] [G loss: 0.467363]\n",
      "epoch:10 step:9564 [D loss: 0.186982, acc.: 73.44%] [G loss: 0.490847]\n",
      "epoch:10 step:9565 [D loss: 0.198556, acc.: 67.19%] [G loss: 0.465881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9566 [D loss: 0.254571, acc.: 60.16%] [G loss: 0.463307]\n",
      "epoch:10 step:9567 [D loss: 0.209391, acc.: 71.88%] [G loss: 0.462583]\n",
      "epoch:10 step:9568 [D loss: 0.216370, acc.: 65.62%] [G loss: 0.453538]\n",
      "epoch:10 step:9569 [D loss: 0.214232, acc.: 63.28%] [G loss: 0.504971]\n",
      "epoch:10 step:9570 [D loss: 0.254831, acc.: 55.47%] [G loss: 0.409893]\n",
      "epoch:10 step:9571 [D loss: 0.224685, acc.: 63.28%] [G loss: 0.448934]\n",
      "epoch:10 step:9572 [D loss: 0.204142, acc.: 67.97%] [G loss: 0.473765]\n",
      "epoch:10 step:9573 [D loss: 0.244638, acc.: 58.59%] [G loss: 0.451483]\n",
      "epoch:10 step:9574 [D loss: 0.235833, acc.: 60.16%] [G loss: 0.433553]\n",
      "epoch:10 step:9575 [D loss: 0.199217, acc.: 68.75%] [G loss: 0.498371]\n",
      "epoch:10 step:9576 [D loss: 0.212655, acc.: 69.53%] [G loss: 0.490795]\n",
      "epoch:10 step:9577 [D loss: 0.201616, acc.: 65.62%] [G loss: 0.489295]\n",
      "epoch:10 step:9578 [D loss: 0.183948, acc.: 71.09%] [G loss: 0.496124]\n",
      "epoch:10 step:9579 [D loss: 0.191129, acc.: 73.44%] [G loss: 0.519112]\n",
      "epoch:10 step:9580 [D loss: 0.243145, acc.: 57.03%] [G loss: 0.473512]\n",
      "epoch:10 step:9581 [D loss: 0.241675, acc.: 58.59%] [G loss: 0.430207]\n",
      "epoch:10 step:9582 [D loss: 0.256766, acc.: 59.38%] [G loss: 0.422442]\n",
      "epoch:10 step:9583 [D loss: 0.237553, acc.: 63.28%] [G loss: 0.451141]\n",
      "epoch:10 step:9584 [D loss: 0.246222, acc.: 60.94%] [G loss: 0.466117]\n",
      "epoch:10 step:9585 [D loss: 0.244524, acc.: 54.69%] [G loss: 0.426431]\n",
      "epoch:10 step:9586 [D loss: 0.234585, acc.: 65.62%] [G loss: 0.480462]\n",
      "epoch:10 step:9587 [D loss: 0.250675, acc.: 57.81%] [G loss: 0.464998]\n",
      "epoch:10 step:9588 [D loss: 0.185135, acc.: 72.66%] [G loss: 0.493149]\n",
      "epoch:10 step:9589 [D loss: 0.186945, acc.: 73.44%] [G loss: 0.525228]\n",
      "epoch:10 step:9590 [D loss: 0.274972, acc.: 52.34%] [G loss: 0.446488]\n",
      "epoch:10 step:9591 [D loss: 0.219226, acc.: 63.28%] [G loss: 0.405343]\n",
      "epoch:10 step:9592 [D loss: 0.215655, acc.: 64.84%] [G loss: 0.470765]\n",
      "epoch:10 step:9593 [D loss: 0.202506, acc.: 67.97%] [G loss: 0.516347]\n",
      "epoch:10 step:9594 [D loss: 0.265576, acc.: 58.59%] [G loss: 0.429691]\n",
      "epoch:10 step:9595 [D loss: 0.249231, acc.: 57.81%] [G loss: 0.443148]\n",
      "epoch:10 step:9596 [D loss: 0.237757, acc.: 62.50%] [G loss: 0.450808]\n",
      "epoch:10 step:9597 [D loss: 0.231458, acc.: 62.50%] [G loss: 0.421373]\n",
      "epoch:10 step:9598 [D loss: 0.254800, acc.: 54.69%] [G loss: 0.434682]\n",
      "epoch:10 step:9599 [D loss: 0.203450, acc.: 64.06%] [G loss: 0.472909]\n",
      "epoch:10 step:9600 [D loss: 0.230695, acc.: 64.84%] [G loss: 0.493057]\n",
      "##############\n",
      "[2.42867441 1.73802871 6.20195039 4.83316449 3.77423168 5.8152846\n",
      " 4.50052562 4.88978231 4.56265519 4.0354582 ]\n",
      "##########\n",
      "epoch:10 step:9601 [D loss: 0.180619, acc.: 76.56%] [G loss: 0.582559]\n",
      "epoch:10 step:9602 [D loss: 0.183723, acc.: 71.09%] [G loss: 0.530341]\n",
      "epoch:10 step:9603 [D loss: 0.240998, acc.: 63.28%] [G loss: 0.455519]\n",
      "epoch:10 step:9604 [D loss: 0.240727, acc.: 57.81%] [G loss: 0.431339]\n",
      "epoch:10 step:9605 [D loss: 0.257726, acc.: 53.91%] [G loss: 0.425032]\n",
      "epoch:10 step:9606 [D loss: 0.178944, acc.: 70.31%] [G loss: 0.475398]\n",
      "epoch:10 step:9607 [D loss: 0.242527, acc.: 57.81%] [G loss: 0.447000]\n",
      "epoch:10 step:9608 [D loss: 0.212371, acc.: 66.41%] [G loss: 0.500661]\n",
      "epoch:10 step:9609 [D loss: 0.198826, acc.: 65.62%] [G loss: 0.470032]\n",
      "epoch:10 step:9610 [D loss: 0.208421, acc.: 69.53%] [G loss: 0.494269]\n",
      "epoch:10 step:9611 [D loss: 0.194104, acc.: 71.09%] [G loss: 0.460601]\n",
      "epoch:10 step:9612 [D loss: 0.187904, acc.: 75.00%] [G loss: 0.482669]\n",
      "epoch:10 step:9613 [D loss: 0.215084, acc.: 63.28%] [G loss: 0.463269]\n",
      "epoch:10 step:9614 [D loss: 0.212323, acc.: 69.53%] [G loss: 0.457452]\n",
      "epoch:10 step:9615 [D loss: 0.194938, acc.: 74.22%] [G loss: 0.495722]\n",
      "epoch:10 step:9616 [D loss: 0.238644, acc.: 55.47%] [G loss: 0.448922]\n",
      "epoch:10 step:9617 [D loss: 0.227778, acc.: 62.50%] [G loss: 0.454192]\n",
      "epoch:10 step:9618 [D loss: 0.195594, acc.: 68.75%] [G loss: 0.495725]\n",
      "epoch:10 step:9619 [D loss: 0.266847, acc.: 56.25%] [G loss: 0.442932]\n",
      "epoch:10 step:9620 [D loss: 0.254480, acc.: 53.91%] [G loss: 0.404063]\n",
      "epoch:10 step:9621 [D loss: 0.268780, acc.: 55.47%] [G loss: 0.423468]\n",
      "epoch:10 step:9622 [D loss: 0.238329, acc.: 60.16%] [G loss: 0.466383]\n",
      "epoch:10 step:9623 [D loss: 0.225157, acc.: 64.84%] [G loss: 0.451883]\n",
      "epoch:10 step:9624 [D loss: 0.218803, acc.: 63.28%] [G loss: 0.469918]\n",
      "epoch:10 step:9625 [D loss: 0.228804, acc.: 58.59%] [G loss: 0.427666]\n",
      "epoch:10 step:9626 [D loss: 0.254554, acc.: 55.47%] [G loss: 0.419779]\n",
      "epoch:10 step:9627 [D loss: 0.242606, acc.: 56.25%] [G loss: 0.426812]\n",
      "epoch:10 step:9628 [D loss: 0.218343, acc.: 61.72%] [G loss: 0.463692]\n",
      "epoch:10 step:9629 [D loss: 0.182549, acc.: 74.22%] [G loss: 0.494179]\n",
      "epoch:10 step:9630 [D loss: 0.233695, acc.: 63.28%] [G loss: 0.483448]\n",
      "epoch:10 step:9631 [D loss: 0.215891, acc.: 67.97%] [G loss: 0.493771]\n",
      "epoch:10 step:9632 [D loss: 0.212003, acc.: 65.62%] [G loss: 0.521034]\n",
      "epoch:10 step:9633 [D loss: 0.227221, acc.: 61.72%] [G loss: 0.477864]\n",
      "epoch:10 step:9634 [D loss: 0.201342, acc.: 71.88%] [G loss: 0.489202]\n",
      "epoch:10 step:9635 [D loss: 0.285878, acc.: 47.66%] [G loss: 0.457743]\n",
      "epoch:10 step:9636 [D loss: 0.265036, acc.: 46.88%] [G loss: 0.425488]\n",
      "epoch:10 step:9637 [D loss: 0.212721, acc.: 67.97%] [G loss: 0.428290]\n",
      "epoch:10 step:9638 [D loss: 0.239016, acc.: 60.94%] [G loss: 0.443173]\n",
      "epoch:10 step:9639 [D loss: 0.199915, acc.: 71.09%] [G loss: 0.438763]\n",
      "epoch:10 step:9640 [D loss: 0.220948, acc.: 63.28%] [G loss: 0.445663]\n",
      "epoch:10 step:9641 [D loss: 0.191559, acc.: 73.44%] [G loss: 0.496186]\n",
      "epoch:10 step:9642 [D loss: 0.214248, acc.: 62.50%] [G loss: 0.471339]\n",
      "epoch:10 step:9643 [D loss: 0.208610, acc.: 64.06%] [G loss: 0.474850]\n",
      "epoch:10 step:9644 [D loss: 0.198663, acc.: 71.88%] [G loss: 0.464838]\n",
      "epoch:10 step:9645 [D loss: 0.232839, acc.: 61.72%] [G loss: 0.496847]\n",
      "epoch:10 step:9646 [D loss: 0.195411, acc.: 69.53%] [G loss: 0.487672]\n",
      "epoch:10 step:9647 [D loss: 0.257622, acc.: 57.81%] [G loss: 0.449117]\n",
      "epoch:10 step:9648 [D loss: 0.229554, acc.: 60.94%] [G loss: 0.452850]\n",
      "epoch:10 step:9649 [D loss: 0.214905, acc.: 70.31%] [G loss: 0.445343]\n",
      "epoch:10 step:9650 [D loss: 0.212052, acc.: 67.97%] [G loss: 0.458412]\n",
      "epoch:10 step:9651 [D loss: 0.255839, acc.: 58.59%] [G loss: 0.441589]\n",
      "epoch:10 step:9652 [D loss: 0.236275, acc.: 60.94%] [G loss: 0.432510]\n",
      "epoch:10 step:9653 [D loss: 0.204858, acc.: 70.31%] [G loss: 0.441591]\n",
      "epoch:10 step:9654 [D loss: 0.234845, acc.: 57.81%] [G loss: 0.419053]\n",
      "epoch:10 step:9655 [D loss: 0.216801, acc.: 67.97%] [G loss: 0.456573]\n",
      "epoch:10 step:9656 [D loss: 0.201458, acc.: 76.56%] [G loss: 0.464011]\n",
      "epoch:10 step:9657 [D loss: 0.243627, acc.: 59.38%] [G loss: 0.449561]\n",
      "epoch:10 step:9658 [D loss: 0.221431, acc.: 69.53%] [G loss: 0.450709]\n",
      "epoch:10 step:9659 [D loss: 0.199010, acc.: 70.31%] [G loss: 0.485898]\n",
      "epoch:10 step:9660 [D loss: 0.255549, acc.: 50.00%] [G loss: 0.446087]\n",
      "epoch:10 step:9661 [D loss: 0.239232, acc.: 62.50%] [G loss: 0.475242]\n",
      "epoch:10 step:9662 [D loss: 0.267385, acc.: 53.91%] [G loss: 0.418889]\n",
      "epoch:10 step:9663 [D loss: 0.227105, acc.: 57.81%] [G loss: 0.426918]\n",
      "epoch:10 step:9664 [D loss: 0.239589, acc.: 60.16%] [G loss: 0.432414]\n",
      "epoch:10 step:9665 [D loss: 0.245587, acc.: 50.78%] [G loss: 0.432594]\n",
      "epoch:10 step:9666 [D loss: 0.190913, acc.: 72.66%] [G loss: 0.475354]\n",
      "epoch:10 step:9667 [D loss: 0.208935, acc.: 61.72%] [G loss: 0.460050]\n",
      "epoch:10 step:9668 [D loss: 0.196908, acc.: 69.53%] [G loss: 0.479112]\n",
      "epoch:10 step:9669 [D loss: 0.215443, acc.: 68.75%] [G loss: 0.474324]\n",
      "epoch:10 step:9670 [D loss: 0.205449, acc.: 73.44%] [G loss: 0.471864]\n",
      "epoch:10 step:9671 [D loss: 0.249425, acc.: 57.81%] [G loss: 0.424810]\n",
      "epoch:10 step:9672 [D loss: 0.239481, acc.: 57.03%] [G loss: 0.441704]\n",
      "epoch:10 step:9673 [D loss: 0.245214, acc.: 58.59%] [G loss: 0.451806]\n",
      "epoch:10 step:9674 [D loss: 0.220578, acc.: 62.50%] [G loss: 0.463241]\n",
      "epoch:10 step:9675 [D loss: 0.218573, acc.: 66.41%] [G loss: 0.464500]\n",
      "epoch:10 step:9676 [D loss: 0.223552, acc.: 62.50%] [G loss: 0.476736]\n",
      "epoch:10 step:9677 [D loss: 0.214682, acc.: 67.19%] [G loss: 0.442463]\n",
      "epoch:10 step:9678 [D loss: 0.247972, acc.: 60.16%] [G loss: 0.422113]\n",
      "epoch:10 step:9679 [D loss: 0.187898, acc.: 72.66%] [G loss: 0.484759]\n",
      "epoch:10 step:9680 [D loss: 0.204380, acc.: 67.19%] [G loss: 0.482105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9681 [D loss: 0.210431, acc.: 67.19%] [G loss: 0.547346]\n",
      "epoch:10 step:9682 [D loss: 0.177454, acc.: 72.66%] [G loss: 0.545099]\n",
      "epoch:10 step:9683 [D loss: 0.164278, acc.: 75.00%] [G loss: 0.569918]\n",
      "epoch:10 step:9684 [D loss: 0.169658, acc.: 75.00%] [G loss: 0.532032]\n",
      "epoch:10 step:9685 [D loss: 0.220897, acc.: 65.62%] [G loss: 0.445215]\n",
      "epoch:10 step:9686 [D loss: 0.261642, acc.: 57.03%] [G loss: 0.481455]\n",
      "epoch:10 step:9687 [D loss: 0.248462, acc.: 55.47%] [G loss: 0.424361]\n",
      "epoch:10 step:9688 [D loss: 0.214744, acc.: 63.28%] [G loss: 0.478236]\n",
      "epoch:10 step:9689 [D loss: 0.226792, acc.: 64.84%] [G loss: 0.454268]\n",
      "epoch:10 step:9690 [D loss: 0.213028, acc.: 71.09%] [G loss: 0.442448]\n",
      "epoch:10 step:9691 [D loss: 0.200698, acc.: 68.75%] [G loss: 0.472275]\n",
      "epoch:10 step:9692 [D loss: 0.216051, acc.: 67.19%] [G loss: 0.493506]\n",
      "epoch:10 step:9693 [D loss: 0.242521, acc.: 57.03%] [G loss: 0.453517]\n",
      "epoch:10 step:9694 [D loss: 0.211750, acc.: 69.53%] [G loss: 0.389988]\n",
      "epoch:10 step:9695 [D loss: 0.229804, acc.: 64.06%] [G loss: 0.478117]\n",
      "epoch:10 step:9696 [D loss: 0.220574, acc.: 60.16%] [G loss: 0.473111]\n",
      "epoch:10 step:9697 [D loss: 0.230984, acc.: 62.50%] [G loss: 0.464151]\n",
      "epoch:10 step:9698 [D loss: 0.202970, acc.: 71.09%] [G loss: 0.460873]\n",
      "epoch:10 step:9699 [D loss: 0.230973, acc.: 59.38%] [G loss: 0.448594]\n",
      "epoch:10 step:9700 [D loss: 0.221440, acc.: 64.84%] [G loss: 0.415778]\n",
      "epoch:10 step:9701 [D loss: 0.216180, acc.: 65.62%] [G loss: 0.448576]\n",
      "epoch:10 step:9702 [D loss: 0.199707, acc.: 71.09%] [G loss: 0.456877]\n",
      "epoch:10 step:9703 [D loss: 0.201801, acc.: 71.09%] [G loss: 0.495999]\n",
      "epoch:10 step:9704 [D loss: 0.215538, acc.: 67.19%] [G loss: 0.479877]\n",
      "epoch:10 step:9705 [D loss: 0.199412, acc.: 69.53%] [G loss: 0.480131]\n",
      "epoch:10 step:9706 [D loss: 0.173847, acc.: 78.12%] [G loss: 0.521592]\n",
      "epoch:10 step:9707 [D loss: 0.205727, acc.: 67.97%] [G loss: 0.494439]\n",
      "epoch:10 step:9708 [D loss: 0.250192, acc.: 57.03%] [G loss: 0.451602]\n",
      "epoch:10 step:9709 [D loss: 0.197474, acc.: 75.78%] [G loss: 0.509349]\n",
      "epoch:10 step:9710 [D loss: 0.211302, acc.: 66.41%] [G loss: 0.479133]\n",
      "epoch:10 step:9711 [D loss: 0.264524, acc.: 59.38%] [G loss: 0.436252]\n",
      "epoch:10 step:9712 [D loss: 0.226141, acc.: 58.59%] [G loss: 0.490477]\n",
      "epoch:10 step:9713 [D loss: 0.185183, acc.: 73.44%] [G loss: 0.518123]\n",
      "epoch:10 step:9714 [D loss: 0.222229, acc.: 65.62%] [G loss: 0.508879]\n",
      "epoch:10 step:9715 [D loss: 0.227797, acc.: 63.28%] [G loss: 0.512149]\n",
      "epoch:10 step:9716 [D loss: 0.185093, acc.: 70.31%] [G loss: 0.535975]\n",
      "epoch:10 step:9717 [D loss: 0.175361, acc.: 73.44%] [G loss: 0.555084]\n",
      "epoch:10 step:9718 [D loss: 0.266788, acc.: 56.25%] [G loss: 0.462389]\n",
      "epoch:10 step:9719 [D loss: 0.270904, acc.: 50.00%] [G loss: 0.429897]\n",
      "epoch:10 step:9720 [D loss: 0.202923, acc.: 68.75%] [G loss: 0.434420]\n",
      "epoch:10 step:9721 [D loss: 0.206486, acc.: 65.62%] [G loss: 0.445212]\n",
      "epoch:10 step:9722 [D loss: 0.245910, acc.: 54.69%] [G loss: 0.464747]\n",
      "epoch:10 step:9723 [D loss: 0.220104, acc.: 66.41%] [G loss: 0.424455]\n",
      "epoch:10 step:9724 [D loss: 0.175496, acc.: 72.66%] [G loss: 0.511562]\n",
      "epoch:10 step:9725 [D loss: 0.248654, acc.: 54.69%] [G loss: 0.487396]\n",
      "epoch:10 step:9726 [D loss: 0.238499, acc.: 59.38%] [G loss: 0.457223]\n",
      "epoch:10 step:9727 [D loss: 0.210097, acc.: 64.06%] [G loss: 0.461802]\n",
      "epoch:10 step:9728 [D loss: 0.183207, acc.: 73.44%] [G loss: 0.494549]\n",
      "epoch:10 step:9729 [D loss: 0.204685, acc.: 71.09%] [G loss: 0.462169]\n",
      "epoch:10 step:9730 [D loss: 0.197082, acc.: 69.53%] [G loss: 0.476382]\n",
      "epoch:10 step:9731 [D loss: 0.210585, acc.: 68.75%] [G loss: 0.479856]\n",
      "epoch:10 step:9732 [D loss: 0.244071, acc.: 59.38%] [G loss: 0.452076]\n",
      "epoch:10 step:9733 [D loss: 0.200255, acc.: 66.41%] [G loss: 0.463782]\n",
      "epoch:10 step:9734 [D loss: 0.216672, acc.: 60.16%] [G loss: 0.451560]\n",
      "epoch:10 step:9735 [D loss: 0.221084, acc.: 59.38%] [G loss: 0.467615]\n",
      "epoch:10 step:9736 [D loss: 0.226340, acc.: 62.50%] [G loss: 0.469786]\n",
      "epoch:10 step:9737 [D loss: 0.203195, acc.: 67.97%] [G loss: 0.489516]\n",
      "epoch:10 step:9738 [D loss: 0.222815, acc.: 65.62%] [G loss: 0.434196]\n",
      "epoch:10 step:9739 [D loss: 0.231842, acc.: 60.94%] [G loss: 0.417263]\n",
      "epoch:10 step:9740 [D loss: 0.212198, acc.: 65.62%] [G loss: 0.470230]\n",
      "epoch:10 step:9741 [D loss: 0.203403, acc.: 67.97%] [G loss: 0.485098]\n",
      "epoch:10 step:9742 [D loss: 0.216294, acc.: 67.19%] [G loss: 0.467439]\n",
      "epoch:10 step:9743 [D loss: 0.235860, acc.: 64.84%] [G loss: 0.480916]\n",
      "epoch:10 step:9744 [D loss: 0.178406, acc.: 75.00%] [G loss: 0.500688]\n",
      "epoch:10 step:9745 [D loss: 0.245176, acc.: 57.03%] [G loss: 0.452281]\n",
      "epoch:10 step:9746 [D loss: 0.307610, acc.: 46.88%] [G loss: 0.372263]\n",
      "epoch:10 step:9747 [D loss: 0.268984, acc.: 48.44%] [G loss: 0.432483]\n",
      "epoch:10 step:9748 [D loss: 0.216536, acc.: 64.06%] [G loss: 0.423202]\n",
      "epoch:10 step:9749 [D loss: 0.244754, acc.: 57.03%] [G loss: 0.414857]\n",
      "epoch:10 step:9750 [D loss: 0.227571, acc.: 64.84%] [G loss: 0.426086]\n",
      "epoch:10 step:9751 [D loss: 0.184377, acc.: 75.00%] [G loss: 0.443853]\n",
      "epoch:10 step:9752 [D loss: 0.215748, acc.: 64.06%] [G loss: 0.471114]\n",
      "epoch:10 step:9753 [D loss: 0.248938, acc.: 57.81%] [G loss: 0.459211]\n",
      "epoch:10 step:9754 [D loss: 0.213186, acc.: 67.97%] [G loss: 0.473342]\n",
      "epoch:10 step:9755 [D loss: 0.221218, acc.: 67.19%] [G loss: 0.474937]\n",
      "epoch:10 step:9756 [D loss: 0.230285, acc.: 61.72%] [G loss: 0.474418]\n",
      "epoch:10 step:9757 [D loss: 0.235735, acc.: 55.47%] [G loss: 0.426584]\n",
      "epoch:10 step:9758 [D loss: 0.234427, acc.: 51.56%] [G loss: 0.430716]\n",
      "epoch:10 step:9759 [D loss: 0.253110, acc.: 54.69%] [G loss: 0.449520]\n",
      "epoch:10 step:9760 [D loss: 0.256554, acc.: 57.03%] [G loss: 0.429909]\n",
      "epoch:10 step:9761 [D loss: 0.195989, acc.: 71.88%] [G loss: 0.469336]\n",
      "epoch:10 step:9762 [D loss: 0.266118, acc.: 57.81%] [G loss: 0.392081]\n",
      "epoch:10 step:9763 [D loss: 0.236688, acc.: 60.16%] [G loss: 0.441240]\n",
      "epoch:10 step:9764 [D loss: 0.216855, acc.: 71.88%] [G loss: 0.440936]\n",
      "epoch:10 step:9765 [D loss: 0.198143, acc.: 71.88%] [G loss: 0.452197]\n",
      "epoch:10 step:9766 [D loss: 0.245370, acc.: 59.38%] [G loss: 0.458886]\n",
      "epoch:10 step:9767 [D loss: 0.226984, acc.: 58.59%] [G loss: 0.480057]\n",
      "epoch:10 step:9768 [D loss: 0.184039, acc.: 73.44%] [G loss: 0.506322]\n",
      "epoch:10 step:9769 [D loss: 0.209743, acc.: 67.19%] [G loss: 0.499597]\n",
      "epoch:10 step:9770 [D loss: 0.264096, acc.: 54.69%] [G loss: 0.447689]\n",
      "epoch:10 step:9771 [D loss: 0.213095, acc.: 64.84%] [G loss: 0.454675]\n",
      "epoch:10 step:9772 [D loss: 0.235100, acc.: 64.84%] [G loss: 0.452156]\n",
      "epoch:10 step:9773 [D loss: 0.231575, acc.: 62.50%] [G loss: 0.464572]\n",
      "epoch:10 step:9774 [D loss: 0.251433, acc.: 54.69%] [G loss: 0.460502]\n",
      "epoch:10 step:9775 [D loss: 0.184641, acc.: 75.78%] [G loss: 0.498427]\n",
      "epoch:10 step:9776 [D loss: 0.199818, acc.: 71.09%] [G loss: 0.530882]\n",
      "epoch:10 step:9777 [D loss: 0.217511, acc.: 67.19%] [G loss: 0.481158]\n",
      "epoch:10 step:9778 [D loss: 0.270001, acc.: 50.78%] [G loss: 0.441710]\n",
      "epoch:10 step:9779 [D loss: 0.228204, acc.: 67.19%] [G loss: 0.442771]\n",
      "epoch:10 step:9780 [D loss: 0.244744, acc.: 58.59%] [G loss: 0.449251]\n",
      "epoch:10 step:9781 [D loss: 0.217604, acc.: 64.06%] [G loss: 0.419016]\n",
      "epoch:10 step:9782 [D loss: 0.212070, acc.: 62.50%] [G loss: 0.449834]\n",
      "epoch:10 step:9783 [D loss: 0.218962, acc.: 60.94%] [G loss: 0.440579]\n",
      "epoch:10 step:9784 [D loss: 0.234025, acc.: 62.50%] [G loss: 0.463040]\n",
      "epoch:10 step:9785 [D loss: 0.219964, acc.: 67.19%] [G loss: 0.536068]\n",
      "epoch:10 step:9786 [D loss: 0.175965, acc.: 74.22%] [G loss: 0.509167]\n",
      "epoch:10 step:9787 [D loss: 0.244856, acc.: 53.12%] [G loss: 0.507394]\n",
      "epoch:10 step:9788 [D loss: 0.252865, acc.: 56.25%] [G loss: 0.410746]\n",
      "epoch:10 step:9789 [D loss: 0.228270, acc.: 68.75%] [G loss: 0.440934]\n",
      "epoch:10 step:9790 [D loss: 0.225677, acc.: 61.72%] [G loss: 0.483027]\n",
      "epoch:10 step:9791 [D loss: 0.265618, acc.: 53.91%] [G loss: 0.434880]\n",
      "epoch:10 step:9792 [D loss: 0.217031, acc.: 67.97%] [G loss: 0.451224]\n",
      "epoch:10 step:9793 [D loss: 0.222477, acc.: 61.72%] [G loss: 0.444713]\n",
      "epoch:10 step:9794 [D loss: 0.245606, acc.: 60.94%] [G loss: 0.425864]\n",
      "epoch:10 step:9795 [D loss: 0.251969, acc.: 60.16%] [G loss: 0.405620]\n",
      "epoch:10 step:9796 [D loss: 0.201463, acc.: 71.09%] [G loss: 0.477505]\n",
      "epoch:10 step:9797 [D loss: 0.204080, acc.: 71.88%] [G loss: 0.492039]\n",
      "epoch:10 step:9798 [D loss: 0.186246, acc.: 72.66%] [G loss: 0.490683]\n",
      "epoch:10 step:9799 [D loss: 0.189869, acc.: 73.44%] [G loss: 0.518696]\n",
      "epoch:10 step:9800 [D loss: 0.236101, acc.: 65.62%] [G loss: 0.544717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.37038758 1.77207734 6.10718005 4.64765075 3.65072773 5.8830257\n",
      " 4.4250589  5.06491865 4.57009548 3.7843077 ]\n",
      "##########\n",
      "epoch:10 step:9801 [D loss: 0.219947, acc.: 63.28%] [G loss: 0.484795]\n",
      "epoch:10 step:9802 [D loss: 0.233986, acc.: 58.59%] [G loss: 0.463998]\n",
      "epoch:10 step:9803 [D loss: 0.229468, acc.: 63.28%] [G loss: 0.442783]\n",
      "epoch:10 step:9804 [D loss: 0.211240, acc.: 65.62%] [G loss: 0.486350]\n",
      "epoch:10 step:9805 [D loss: 0.213153, acc.: 65.62%] [G loss: 0.487353]\n",
      "epoch:10 step:9806 [D loss: 0.201968, acc.: 70.31%] [G loss: 0.501564]\n",
      "epoch:10 step:9807 [D loss: 0.299637, acc.: 50.00%] [G loss: 0.448585]\n",
      "epoch:10 step:9808 [D loss: 0.238813, acc.: 58.59%] [G loss: 0.464881]\n",
      "epoch:10 step:9809 [D loss: 0.212040, acc.: 66.41%] [G loss: 0.483809]\n",
      "epoch:10 step:9810 [D loss: 0.206393, acc.: 67.19%] [G loss: 0.485564]\n",
      "epoch:10 step:9811 [D loss: 0.206842, acc.: 67.97%] [G loss: 0.484150]\n",
      "epoch:10 step:9812 [D loss: 0.227072, acc.: 64.84%] [G loss: 0.518844]\n",
      "epoch:10 step:9813 [D loss: 0.252370, acc.: 58.59%] [G loss: 0.412045]\n",
      "epoch:10 step:9814 [D loss: 0.229432, acc.: 64.84%] [G loss: 0.470384]\n",
      "epoch:10 step:9815 [D loss: 0.222319, acc.: 64.06%] [G loss: 0.426406]\n",
      "epoch:10 step:9816 [D loss: 0.207119, acc.: 67.97%] [G loss: 0.474738]\n",
      "epoch:10 step:9817 [D loss: 0.198641, acc.: 72.66%] [G loss: 0.488559]\n",
      "epoch:10 step:9818 [D loss: 0.279882, acc.: 49.22%] [G loss: 0.421364]\n",
      "epoch:10 step:9819 [D loss: 0.215604, acc.: 61.72%] [G loss: 0.448529]\n",
      "epoch:10 step:9820 [D loss: 0.195306, acc.: 70.31%] [G loss: 0.472487]\n",
      "epoch:10 step:9821 [D loss: 0.206571, acc.: 64.06%] [G loss: 0.463606]\n",
      "epoch:10 step:9822 [D loss: 0.222081, acc.: 64.06%] [G loss: 0.463846]\n",
      "epoch:10 step:9823 [D loss: 0.205846, acc.: 71.09%] [G loss: 0.481592]\n",
      "epoch:10 step:9824 [D loss: 0.209203, acc.: 68.75%] [G loss: 0.479989]\n",
      "epoch:10 step:9825 [D loss: 0.249588, acc.: 60.94%] [G loss: 0.460864]\n",
      "epoch:10 step:9826 [D loss: 0.244337, acc.: 60.16%] [G loss: 0.432629]\n",
      "epoch:10 step:9827 [D loss: 0.233310, acc.: 64.84%] [G loss: 0.501794]\n",
      "epoch:10 step:9828 [D loss: 0.248377, acc.: 60.16%] [G loss: 0.495682]\n",
      "epoch:10 step:9829 [D loss: 0.239563, acc.: 57.81%] [G loss: 0.439816]\n",
      "epoch:10 step:9830 [D loss: 0.213031, acc.: 68.75%] [G loss: 0.453153]\n",
      "epoch:10 step:9831 [D loss: 0.217287, acc.: 66.41%] [G loss: 0.437995]\n",
      "epoch:10 step:9832 [D loss: 0.242302, acc.: 54.69%] [G loss: 0.427776]\n",
      "epoch:10 step:9833 [D loss: 0.246546, acc.: 60.16%] [G loss: 0.457313]\n",
      "epoch:10 step:9834 [D loss: 0.218018, acc.: 63.28%] [G loss: 0.464578]\n",
      "epoch:10 step:9835 [D loss: 0.254127, acc.: 53.12%] [G loss: 0.443939]\n",
      "epoch:10 step:9836 [D loss: 0.216911, acc.: 66.41%] [G loss: 0.443165]\n",
      "epoch:10 step:9837 [D loss: 0.239506, acc.: 63.28%] [G loss: 0.478496]\n",
      "epoch:10 step:9838 [D loss: 0.215227, acc.: 65.62%] [G loss: 0.450097]\n",
      "epoch:10 step:9839 [D loss: 0.197230, acc.: 68.75%] [G loss: 0.482295]\n",
      "epoch:10 step:9840 [D loss: 0.202366, acc.: 67.97%] [G loss: 0.507450]\n",
      "epoch:10 step:9841 [D loss: 0.184327, acc.: 72.66%] [G loss: 0.506517]\n",
      "epoch:10 step:9842 [D loss: 0.203838, acc.: 67.97%] [G loss: 0.506729]\n",
      "epoch:10 step:9843 [D loss: 0.256980, acc.: 54.69%] [G loss: 0.433273]\n",
      "epoch:10 step:9844 [D loss: 0.224910, acc.: 60.16%] [G loss: 0.465006]\n",
      "epoch:10 step:9845 [D loss: 0.180143, acc.: 78.91%] [G loss: 0.489952]\n",
      "epoch:10 step:9846 [D loss: 0.259346, acc.: 60.16%] [G loss: 0.460139]\n",
      "epoch:10 step:9847 [D loss: 0.285748, acc.: 52.34%] [G loss: 0.394808]\n",
      "epoch:10 step:9848 [D loss: 0.240045, acc.: 59.38%] [G loss: 0.411373]\n",
      "epoch:10 step:9849 [D loss: 0.219603, acc.: 67.19%] [G loss: 0.456999]\n",
      "epoch:10 step:9850 [D loss: 0.215957, acc.: 66.41%] [G loss: 0.436741]\n",
      "epoch:10 step:9851 [D loss: 0.198941, acc.: 70.31%] [G loss: 0.479725]\n",
      "epoch:10 step:9852 [D loss: 0.268090, acc.: 54.69%] [G loss: 0.432884]\n",
      "epoch:10 step:9853 [D loss: 0.215909, acc.: 66.41%] [G loss: 0.444516]\n",
      "epoch:10 step:9854 [D loss: 0.219562, acc.: 67.19%] [G loss: 0.463966]\n",
      "epoch:10 step:9855 [D loss: 0.200376, acc.: 66.41%] [G loss: 0.488800]\n",
      "epoch:10 step:9856 [D loss: 0.229350, acc.: 64.06%] [G loss: 0.481686]\n",
      "epoch:10 step:9857 [D loss: 0.224792, acc.: 59.38%] [G loss: 0.414873]\n",
      "epoch:10 step:9858 [D loss: 0.206419, acc.: 64.06%] [G loss: 0.460713]\n",
      "epoch:10 step:9859 [D loss: 0.231274, acc.: 64.84%] [G loss: 0.444955]\n",
      "epoch:10 step:9860 [D loss: 0.210186, acc.: 67.97%] [G loss: 0.465880]\n",
      "epoch:10 step:9861 [D loss: 0.234667, acc.: 59.38%] [G loss: 0.454339]\n",
      "epoch:10 step:9862 [D loss: 0.238602, acc.: 58.59%] [G loss: 0.426489]\n",
      "epoch:10 step:9863 [D loss: 0.226899, acc.: 63.28%] [G loss: 0.464779]\n",
      "epoch:10 step:9864 [D loss: 0.229856, acc.: 63.28%] [G loss: 0.448070]\n",
      "epoch:10 step:9865 [D loss: 0.197754, acc.: 67.97%] [G loss: 0.483874]\n",
      "epoch:10 step:9866 [D loss: 0.205837, acc.: 67.97%] [G loss: 0.443523]\n",
      "epoch:10 step:9867 [D loss: 0.213320, acc.: 65.62%] [G loss: 0.489060]\n",
      "epoch:10 step:9868 [D loss: 0.211997, acc.: 64.84%] [G loss: 0.472129]\n",
      "epoch:10 step:9869 [D loss: 0.173900, acc.: 76.56%] [G loss: 0.522462]\n",
      "epoch:10 step:9870 [D loss: 0.290535, acc.: 53.91%] [G loss: 0.431314]\n",
      "epoch:10 step:9871 [D loss: 0.276843, acc.: 52.34%] [G loss: 0.400110]\n",
      "epoch:10 step:9872 [D loss: 0.248340, acc.: 55.47%] [G loss: 0.400740]\n",
      "epoch:10 step:9873 [D loss: 0.219294, acc.: 61.72%] [G loss: 0.435270]\n",
      "epoch:10 step:9874 [D loss: 0.204863, acc.: 69.53%] [G loss: 0.433098]\n",
      "epoch:10 step:9875 [D loss: 0.184888, acc.: 70.31%] [G loss: 0.476323]\n",
      "epoch:10 step:9876 [D loss: 0.212047, acc.: 65.62%] [G loss: 0.503849]\n",
      "epoch:10 step:9877 [D loss: 0.209825, acc.: 69.53%] [G loss: 0.502366]\n",
      "epoch:10 step:9878 [D loss: 0.233774, acc.: 63.28%] [G loss: 0.475971]\n",
      "epoch:10 step:9879 [D loss: 0.239791, acc.: 59.38%] [G loss: 0.482522]\n",
      "epoch:10 step:9880 [D loss: 0.243403, acc.: 57.81%] [G loss: 0.434315]\n",
      "epoch:10 step:9881 [D loss: 0.241459, acc.: 57.03%] [G loss: 0.447166]\n",
      "epoch:10 step:9882 [D loss: 0.224951, acc.: 63.28%] [G loss: 0.434188]\n",
      "epoch:10 step:9883 [D loss: 0.228686, acc.: 66.41%] [G loss: 0.440476]\n",
      "epoch:10 step:9884 [D loss: 0.201126, acc.: 67.97%] [G loss: 0.503644]\n",
      "epoch:10 step:9885 [D loss: 0.219523, acc.: 65.62%] [G loss: 0.483120]\n",
      "epoch:10 step:9886 [D loss: 0.215635, acc.: 64.84%] [G loss: 0.478112]\n",
      "epoch:10 step:9887 [D loss: 0.248849, acc.: 61.72%] [G loss: 0.444804]\n",
      "epoch:10 step:9888 [D loss: 0.220959, acc.: 61.72%] [G loss: 0.445106]\n",
      "epoch:10 step:9889 [D loss: 0.199808, acc.: 62.50%] [G loss: 0.432310]\n",
      "epoch:10 step:9890 [D loss: 0.212344, acc.: 67.19%] [G loss: 0.458027]\n",
      "epoch:10 step:9891 [D loss: 0.218418, acc.: 62.50%] [G loss: 0.477335]\n",
      "epoch:10 step:9892 [D loss: 0.204764, acc.: 66.41%] [G loss: 0.472462]\n",
      "epoch:10 step:9893 [D loss: 0.211764, acc.: 65.62%] [G loss: 0.443661]\n",
      "epoch:10 step:9894 [D loss: 0.213572, acc.: 63.28%] [G loss: 0.448839]\n",
      "epoch:10 step:9895 [D loss: 0.223598, acc.: 63.28%] [G loss: 0.440687]\n",
      "epoch:10 step:9896 [D loss: 0.215404, acc.: 65.62%] [G loss: 0.450704]\n",
      "epoch:10 step:9897 [D loss: 0.256786, acc.: 58.59%] [G loss: 0.460316]\n",
      "epoch:10 step:9898 [D loss: 0.271399, acc.: 51.56%] [G loss: 0.482445]\n",
      "epoch:10 step:9899 [D loss: 0.245849, acc.: 56.25%] [G loss: 0.481606]\n",
      "epoch:10 step:9900 [D loss: 0.215318, acc.: 65.62%] [G loss: 0.492499]\n",
      "epoch:10 step:9901 [D loss: 0.265925, acc.: 57.03%] [G loss: 0.431854]\n",
      "epoch:10 step:9902 [D loss: 0.228219, acc.: 60.16%] [G loss: 0.414942]\n",
      "epoch:10 step:9903 [D loss: 0.233774, acc.: 60.94%] [G loss: 0.409099]\n",
      "epoch:10 step:9904 [D loss: 0.184751, acc.: 73.44%] [G loss: 0.460259]\n",
      "epoch:10 step:9905 [D loss: 0.255890, acc.: 50.00%] [G loss: 0.452546]\n",
      "epoch:10 step:9906 [D loss: 0.212820, acc.: 61.72%] [G loss: 0.484775]\n",
      "epoch:10 step:9907 [D loss: 0.212347, acc.: 69.53%] [G loss: 0.483724]\n",
      "epoch:10 step:9908 [D loss: 0.253294, acc.: 55.47%] [G loss: 0.427854]\n",
      "epoch:10 step:9909 [D loss: 0.206676, acc.: 70.31%] [G loss: 0.467220]\n",
      "epoch:10 step:9910 [D loss: 0.233353, acc.: 60.16%] [G loss: 0.459505]\n",
      "epoch:10 step:9911 [D loss: 0.222956, acc.: 67.97%] [G loss: 0.428020]\n",
      "epoch:10 step:9912 [D loss: 0.247063, acc.: 57.81%] [G loss: 0.381997]\n",
      "epoch:10 step:9913 [D loss: 0.235535, acc.: 60.16%] [G loss: 0.417685]\n",
      "epoch:10 step:9914 [D loss: 0.230669, acc.: 60.16%] [G loss: 0.441756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9915 [D loss: 0.192901, acc.: 67.97%] [G loss: 0.457120]\n",
      "epoch:10 step:9916 [D loss: 0.219845, acc.: 62.50%] [G loss: 0.423985]\n",
      "epoch:10 step:9917 [D loss: 0.215261, acc.: 62.50%] [G loss: 0.462886]\n",
      "epoch:10 step:9918 [D loss: 0.209356, acc.: 67.97%] [G loss: 0.477191]\n",
      "epoch:10 step:9919 [D loss: 0.213327, acc.: 67.97%] [G loss: 0.465555]\n",
      "epoch:10 step:9920 [D loss: 0.219627, acc.: 64.84%] [G loss: 0.509464]\n",
      "epoch:10 step:9921 [D loss: 0.202690, acc.: 64.06%] [G loss: 0.534330]\n",
      "epoch:10 step:9922 [D loss: 0.192129, acc.: 72.66%] [G loss: 0.513698]\n",
      "epoch:10 step:9923 [D loss: 0.255243, acc.: 57.03%] [G loss: 0.494157]\n",
      "epoch:10 step:9924 [D loss: 0.199897, acc.: 71.09%] [G loss: 0.484185]\n",
      "epoch:10 step:9925 [D loss: 0.184733, acc.: 69.53%] [G loss: 0.487134]\n",
      "epoch:10 step:9926 [D loss: 0.203276, acc.: 74.22%] [G loss: 0.470925]\n",
      "epoch:10 step:9927 [D loss: 0.207723, acc.: 67.97%] [G loss: 0.497221]\n",
      "epoch:10 step:9928 [D loss: 0.231902, acc.: 64.84%] [G loss: 0.479756]\n",
      "epoch:10 step:9929 [D loss: 0.249267, acc.: 59.38%] [G loss: 0.442946]\n",
      "epoch:10 step:9930 [D loss: 0.247083, acc.: 56.25%] [G loss: 0.425593]\n",
      "epoch:10 step:9931 [D loss: 0.199006, acc.: 73.44%] [G loss: 0.501225]\n",
      "epoch:10 step:9932 [D loss: 0.241390, acc.: 52.34%] [G loss: 0.450387]\n",
      "epoch:10 step:9933 [D loss: 0.210578, acc.: 65.62%] [G loss: 0.433970]\n",
      "epoch:10 step:9934 [D loss: 0.200775, acc.: 67.97%] [G loss: 0.483045]\n",
      "epoch:10 step:9935 [D loss: 0.240246, acc.: 59.38%] [G loss: 0.488770]\n",
      "epoch:10 step:9936 [D loss: 0.271293, acc.: 53.91%] [G loss: 0.471337]\n",
      "epoch:10 step:9937 [D loss: 0.218181, acc.: 64.84%] [G loss: 0.478843]\n",
      "epoch:10 step:9938 [D loss: 0.195213, acc.: 67.19%] [G loss: 0.466532]\n",
      "epoch:10 step:9939 [D loss: 0.264398, acc.: 57.03%] [G loss: 0.379160]\n",
      "epoch:10 step:9940 [D loss: 0.216734, acc.: 62.50%] [G loss: 0.430015]\n",
      "epoch:10 step:9941 [D loss: 0.216940, acc.: 64.84%] [G loss: 0.446262]\n",
      "epoch:10 step:9942 [D loss: 0.228644, acc.: 67.19%] [G loss: 0.436900]\n",
      "epoch:10 step:9943 [D loss: 0.209064, acc.: 64.84%] [G loss: 0.465288]\n",
      "epoch:10 step:9944 [D loss: 0.187854, acc.: 73.44%] [G loss: 0.500020]\n",
      "epoch:10 step:9945 [D loss: 0.203731, acc.: 72.66%] [G loss: 0.496991]\n",
      "epoch:10 step:9946 [D loss: 0.239099, acc.: 58.59%] [G loss: 0.433435]\n",
      "epoch:10 step:9947 [D loss: 0.236594, acc.: 53.12%] [G loss: 0.427112]\n",
      "epoch:10 step:9948 [D loss: 0.234347, acc.: 53.91%] [G loss: 0.452169]\n",
      "epoch:10 step:9949 [D loss: 0.236854, acc.: 57.03%] [G loss: 0.446968]\n",
      "epoch:10 step:9950 [D loss: 0.212404, acc.: 67.19%] [G loss: 0.477922]\n",
      "epoch:10 step:9951 [D loss: 0.231139, acc.: 56.25%] [G loss: 0.474332]\n",
      "epoch:10 step:9952 [D loss: 0.232134, acc.: 65.62%] [G loss: 0.460645]\n",
      "epoch:10 step:9953 [D loss: 0.214805, acc.: 67.19%] [G loss: 0.465653]\n",
      "epoch:10 step:9954 [D loss: 0.282073, acc.: 51.56%] [G loss: 0.431424]\n",
      "epoch:10 step:9955 [D loss: 0.229462, acc.: 63.28%] [G loss: 0.463951]\n",
      "epoch:10 step:9956 [D loss: 0.216099, acc.: 65.62%] [G loss: 0.453000]\n",
      "epoch:10 step:9957 [D loss: 0.276511, acc.: 47.66%] [G loss: 0.420742]\n",
      "epoch:10 step:9958 [D loss: 0.216070, acc.: 68.75%] [G loss: 0.430439]\n",
      "epoch:10 step:9959 [D loss: 0.206774, acc.: 67.97%] [G loss: 0.447022]\n",
      "epoch:10 step:9960 [D loss: 0.253272, acc.: 60.94%] [G loss: 0.455192]\n",
      "epoch:10 step:9961 [D loss: 0.228582, acc.: 62.50%] [G loss: 0.450753]\n",
      "epoch:10 step:9962 [D loss: 0.193253, acc.: 75.00%] [G loss: 0.522571]\n",
      "epoch:10 step:9963 [D loss: 0.240846, acc.: 60.94%] [G loss: 0.465080]\n",
      "epoch:10 step:9964 [D loss: 0.251675, acc.: 53.91%] [G loss: 0.430661]\n",
      "epoch:10 step:9965 [D loss: 0.223754, acc.: 60.16%] [G loss: 0.428211]\n",
      "epoch:10 step:9966 [D loss: 0.222274, acc.: 61.72%] [G loss: 0.475039]\n",
      "epoch:10 step:9967 [D loss: 0.239440, acc.: 59.38%] [G loss: 0.465001]\n",
      "epoch:10 step:9968 [D loss: 0.222160, acc.: 69.53%] [G loss: 0.462193]\n",
      "epoch:10 step:9969 [D loss: 0.221235, acc.: 61.72%] [G loss: 0.478661]\n",
      "epoch:10 step:9970 [D loss: 0.258339, acc.: 54.69%] [G loss: 0.429308]\n",
      "epoch:10 step:9971 [D loss: 0.214194, acc.: 65.62%] [G loss: 0.432036]\n",
      "epoch:10 step:9972 [D loss: 0.218815, acc.: 62.50%] [G loss: 0.418133]\n",
      "epoch:10 step:9973 [D loss: 0.232377, acc.: 63.28%] [G loss: 0.424639]\n",
      "epoch:10 step:9974 [D loss: 0.225696, acc.: 59.38%] [G loss: 0.431440]\n",
      "epoch:10 step:9975 [D loss: 0.227824, acc.: 59.38%] [G loss: 0.432948]\n",
      "epoch:10 step:9976 [D loss: 0.214799, acc.: 66.41%] [G loss: 0.445619]\n",
      "epoch:10 step:9977 [D loss: 0.209599, acc.: 64.84%] [G loss: 0.462212]\n",
      "epoch:10 step:9978 [D loss: 0.200853, acc.: 71.88%] [G loss: 0.462074]\n",
      "epoch:10 step:9979 [D loss: 0.215693, acc.: 63.28%] [G loss: 0.452873]\n",
      "epoch:10 step:9980 [D loss: 0.230217, acc.: 62.50%] [G loss: 0.472069]\n",
      "epoch:10 step:9981 [D loss: 0.211305, acc.: 68.75%] [G loss: 0.441289]\n",
      "epoch:10 step:9982 [D loss: 0.218464, acc.: 66.41%] [G loss: 0.452153]\n",
      "epoch:10 step:9983 [D loss: 0.214447, acc.: 63.28%] [G loss: 0.498053]\n",
      "epoch:10 step:9984 [D loss: 0.238800, acc.: 56.25%] [G loss: 0.483117]\n",
      "epoch:10 step:9985 [D loss: 0.254621, acc.: 62.50%] [G loss: 0.458754]\n",
      "epoch:10 step:9986 [D loss: 0.238536, acc.: 59.38%] [G loss: 0.463048]\n",
      "epoch:10 step:9987 [D loss: 0.217528, acc.: 64.06%] [G loss: 0.478912]\n",
      "epoch:10 step:9988 [D loss: 0.226957, acc.: 61.72%] [G loss: 0.428055]\n",
      "epoch:10 step:9989 [D loss: 0.238852, acc.: 61.72%] [G loss: 0.460906]\n",
      "epoch:10 step:9990 [D loss: 0.214119, acc.: 64.06%] [G loss: 0.458845]\n",
      "epoch:10 step:9991 [D loss: 0.202901, acc.: 70.31%] [G loss: 0.469079]\n",
      "epoch:10 step:9992 [D loss: 0.242636, acc.: 57.81%] [G loss: 0.393193]\n",
      "epoch:10 step:9993 [D loss: 0.209206, acc.: 65.62%] [G loss: 0.482398]\n",
      "epoch:10 step:9994 [D loss: 0.203869, acc.: 65.62%] [G loss: 0.487235]\n",
      "epoch:10 step:9995 [D loss: 0.251721, acc.: 56.25%] [G loss: 0.472661]\n",
      "epoch:10 step:9996 [D loss: 0.211586, acc.: 67.19%] [G loss: 0.439651]\n",
      "epoch:10 step:9997 [D loss: 0.223943, acc.: 60.94%] [G loss: 0.436423]\n",
      "epoch:10 step:9998 [D loss: 0.248555, acc.: 59.38%] [G loss: 0.424276]\n",
      "epoch:10 step:9999 [D loss: 0.192380, acc.: 74.22%] [G loss: 0.440814]\n",
      "epoch:10 step:10000 [D loss: 0.221395, acc.: 61.72%] [G loss: 0.420488]\n",
      "##############\n",
      "[2.29064604 1.62134454 5.91052092 4.67498662 3.75269939 5.48199751\n",
      " 4.46275108 4.71490152 4.3458302  3.86772691]\n",
      "##########\n",
      "epoch:10 step:10001 [D loss: 0.195782, acc.: 70.31%] [G loss: 0.444516]\n",
      "epoch:10 step:10002 [D loss: 0.192057, acc.: 73.44%] [G loss: 0.474208]\n",
      "epoch:10 step:10003 [D loss: 0.215699, acc.: 64.84%] [G loss: 0.465742]\n",
      "epoch:10 step:10004 [D loss: 0.188825, acc.: 75.00%] [G loss: 0.498774]\n",
      "epoch:10 step:10005 [D loss: 0.207876, acc.: 65.62%] [G loss: 0.484593]\n",
      "epoch:10 step:10006 [D loss: 0.228830, acc.: 57.81%] [G loss: 0.447216]\n",
      "epoch:10 step:10007 [D loss: 0.222103, acc.: 60.94%] [G loss: 0.453520]\n",
      "epoch:10 step:10008 [D loss: 0.217621, acc.: 60.16%] [G loss: 0.437523]\n",
      "epoch:10 step:10009 [D loss: 0.212632, acc.: 65.62%] [G loss: 0.467439]\n",
      "epoch:10 step:10010 [D loss: 0.209491, acc.: 64.84%] [G loss: 0.492010]\n",
      "epoch:10 step:10011 [D loss: 0.198393, acc.: 67.19%] [G loss: 0.492472]\n",
      "epoch:10 step:10012 [D loss: 0.199466, acc.: 74.22%] [G loss: 0.546148]\n",
      "epoch:10 step:10013 [D loss: 0.219844, acc.: 64.06%] [G loss: 0.514757]\n",
      "epoch:10 step:10014 [D loss: 0.240578, acc.: 58.59%] [G loss: 0.454518]\n",
      "epoch:10 step:10015 [D loss: 0.226982, acc.: 60.16%] [G loss: 0.478788]\n",
      "epoch:10 step:10016 [D loss: 0.186183, acc.: 75.00%] [G loss: 0.495232]\n",
      "epoch:10 step:10017 [D loss: 0.187662, acc.: 77.34%] [G loss: 0.498123]\n",
      "epoch:10 step:10018 [D loss: 0.174443, acc.: 75.78%] [G loss: 0.543380]\n",
      "epoch:10 step:10019 [D loss: 0.203501, acc.: 71.88%] [G loss: 0.493705]\n",
      "epoch:10 step:10020 [D loss: 0.250484, acc.: 61.72%] [G loss: 0.479339]\n",
      "epoch:10 step:10021 [D loss: 0.217139, acc.: 67.19%] [G loss: 0.516147]\n",
      "epoch:10 step:10022 [D loss: 0.227974, acc.: 57.03%] [G loss: 0.477350]\n",
      "epoch:10 step:10023 [D loss: 0.222770, acc.: 65.62%] [G loss: 0.502234]\n",
      "epoch:10 step:10024 [D loss: 0.210126, acc.: 67.97%] [G loss: 0.495744]\n",
      "epoch:10 step:10025 [D loss: 0.275381, acc.: 53.12%] [G loss: 0.411254]\n",
      "epoch:10 step:10026 [D loss: 0.237869, acc.: 54.69%] [G loss: 0.436353]\n",
      "epoch:10 step:10027 [D loss: 0.228180, acc.: 60.94%] [G loss: 0.481804]\n",
      "epoch:10 step:10028 [D loss: 0.204749, acc.: 68.75%] [G loss: 0.459202]\n",
      "epoch:10 step:10029 [D loss: 0.213756, acc.: 64.84%] [G loss: 0.460731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10030 [D loss: 0.211297, acc.: 67.19%] [G loss: 0.430444]\n",
      "epoch:10 step:10031 [D loss: 0.223410, acc.: 60.16%] [G loss: 0.440174]\n",
      "epoch:10 step:10032 [D loss: 0.228642, acc.: 63.28%] [G loss: 0.459636]\n",
      "epoch:10 step:10033 [D loss: 0.220822, acc.: 60.16%] [G loss: 0.435483]\n",
      "epoch:10 step:10034 [D loss: 0.237358, acc.: 55.47%] [G loss: 0.461283]\n",
      "epoch:10 step:10035 [D loss: 0.228485, acc.: 58.59%] [G loss: 0.433676]\n",
      "epoch:10 step:10036 [D loss: 0.228750, acc.: 58.59%] [G loss: 0.489595]\n",
      "epoch:10 step:10037 [D loss: 0.261545, acc.: 53.12%] [G loss: 0.447110]\n",
      "epoch:10 step:10038 [D loss: 0.219580, acc.: 69.53%] [G loss: 0.464463]\n",
      "epoch:10 step:10039 [D loss: 0.202919, acc.: 65.62%] [G loss: 0.439600]\n",
      "epoch:10 step:10040 [D loss: 0.237666, acc.: 59.38%] [G loss: 0.451619]\n",
      "epoch:10 step:10041 [D loss: 0.236831, acc.: 55.47%] [G loss: 0.401464]\n",
      "epoch:10 step:10042 [D loss: 0.256541, acc.: 54.69%] [G loss: 0.446104]\n",
      "epoch:10 step:10043 [D loss: 0.234590, acc.: 64.06%] [G loss: 0.545333]\n",
      "epoch:10 step:10044 [D loss: 0.197695, acc.: 69.53%] [G loss: 0.477977]\n",
      "epoch:10 step:10045 [D loss: 0.235855, acc.: 60.94%] [G loss: 0.427703]\n",
      "epoch:10 step:10046 [D loss: 0.252111, acc.: 60.94%] [G loss: 0.414976]\n",
      "epoch:10 step:10047 [D loss: 0.199444, acc.: 66.41%] [G loss: 0.436251]\n",
      "epoch:10 step:10048 [D loss: 0.226583, acc.: 64.84%] [G loss: 0.495120]\n",
      "epoch:10 step:10049 [D loss: 0.197012, acc.: 64.84%] [G loss: 0.471393]\n",
      "epoch:10 step:10050 [D loss: 0.216993, acc.: 60.94%] [G loss: 0.452426]\n",
      "epoch:10 step:10051 [D loss: 0.202112, acc.: 70.31%] [G loss: 0.444656]\n",
      "epoch:10 step:10052 [D loss: 0.230459, acc.: 59.38%] [G loss: 0.412295]\n",
      "epoch:10 step:10053 [D loss: 0.238980, acc.: 59.38%] [G loss: 0.409373]\n",
      "epoch:10 step:10054 [D loss: 0.212382, acc.: 64.06%] [G loss: 0.424989]\n",
      "epoch:10 step:10055 [D loss: 0.216762, acc.: 71.09%] [G loss: 0.425019]\n",
      "epoch:10 step:10056 [D loss: 0.225024, acc.: 63.28%] [G loss: 0.455606]\n",
      "epoch:10 step:10057 [D loss: 0.217633, acc.: 65.62%] [G loss: 0.468338]\n",
      "epoch:10 step:10058 [D loss: 0.218502, acc.: 64.84%] [G loss: 0.485580]\n",
      "epoch:10 step:10059 [D loss: 0.207253, acc.: 67.97%] [G loss: 0.473159]\n",
      "epoch:10 step:10060 [D loss: 0.194565, acc.: 67.19%] [G loss: 0.452050]\n",
      "epoch:10 step:10061 [D loss: 0.226365, acc.: 64.84%] [G loss: 0.433245]\n",
      "epoch:10 step:10062 [D loss: 0.201239, acc.: 69.53%] [G loss: 0.466495]\n",
      "epoch:10 step:10063 [D loss: 0.225829, acc.: 65.62%] [G loss: 0.502950]\n",
      "epoch:10 step:10064 [D loss: 0.189033, acc.: 68.75%] [G loss: 0.529119]\n",
      "epoch:10 step:10065 [D loss: 0.216064, acc.: 71.09%] [G loss: 0.513789]\n",
      "epoch:10 step:10066 [D loss: 0.233691, acc.: 64.84%] [G loss: 0.441666]\n",
      "epoch:10 step:10067 [D loss: 0.221144, acc.: 64.84%] [G loss: 0.462415]\n",
      "epoch:10 step:10068 [D loss: 0.221649, acc.: 67.97%] [G loss: 0.442199]\n",
      "epoch:10 step:10069 [D loss: 0.222988, acc.: 64.84%] [G loss: 0.460923]\n",
      "epoch:10 step:10070 [D loss: 0.236709, acc.: 60.94%] [G loss: 0.467142]\n",
      "epoch:10 step:10071 [D loss: 0.233838, acc.: 61.72%] [G loss: 0.462007]\n",
      "epoch:10 step:10072 [D loss: 0.229462, acc.: 59.38%] [G loss: 0.477806]\n",
      "epoch:10 step:10073 [D loss: 0.231282, acc.: 60.94%] [G loss: 0.440880]\n",
      "epoch:10 step:10074 [D loss: 0.240427, acc.: 59.38%] [G loss: 0.406128]\n",
      "epoch:10 step:10075 [D loss: 0.223308, acc.: 68.75%] [G loss: 0.429642]\n",
      "epoch:10 step:10076 [D loss: 0.219097, acc.: 69.53%] [G loss: 0.469989]\n",
      "epoch:10 step:10077 [D loss: 0.223980, acc.: 64.84%] [G loss: 0.450890]\n",
      "epoch:10 step:10078 [D loss: 0.198747, acc.: 71.88%] [G loss: 0.474203]\n",
      "epoch:10 step:10079 [D loss: 0.211277, acc.: 68.75%] [G loss: 0.477084]\n",
      "epoch:10 step:10080 [D loss: 0.242652, acc.: 55.47%] [G loss: 0.438158]\n",
      "epoch:10 step:10081 [D loss: 0.237757, acc.: 57.81%] [G loss: 0.438482]\n",
      "epoch:10 step:10082 [D loss: 0.210722, acc.: 64.06%] [G loss: 0.478317]\n",
      "epoch:10 step:10083 [D loss: 0.246616, acc.: 58.59%] [G loss: 0.442984]\n",
      "epoch:10 step:10084 [D loss: 0.232860, acc.: 58.59%] [G loss: 0.439742]\n",
      "epoch:10 step:10085 [D loss: 0.229238, acc.: 60.16%] [G loss: 0.456380]\n",
      "epoch:10 step:10086 [D loss: 0.236850, acc.: 62.50%] [G loss: 0.447798]\n",
      "epoch:10 step:10087 [D loss: 0.227619, acc.: 63.28%] [G loss: 0.481136]\n",
      "epoch:10 step:10088 [D loss: 0.229487, acc.: 66.41%] [G loss: 0.490513]\n",
      "epoch:10 step:10089 [D loss: 0.251419, acc.: 58.59%] [G loss: 0.453906]\n",
      "epoch:10 step:10090 [D loss: 0.226356, acc.: 53.91%] [G loss: 0.482965]\n",
      "epoch:10 step:10091 [D loss: 0.233151, acc.: 56.25%] [G loss: 0.509373]\n",
      "epoch:10 step:10092 [D loss: 0.232178, acc.: 62.50%] [G loss: 0.427361]\n",
      "epoch:10 step:10093 [D loss: 0.240493, acc.: 54.69%] [G loss: 0.451188]\n",
      "epoch:10 step:10094 [D loss: 0.221079, acc.: 67.19%] [G loss: 0.469732]\n",
      "epoch:10 step:10095 [D loss: 0.201106, acc.: 69.53%] [G loss: 0.490324]\n",
      "epoch:10 step:10096 [D loss: 0.232332, acc.: 56.25%] [G loss: 0.444274]\n",
      "epoch:10 step:10097 [D loss: 0.228289, acc.: 60.94%] [G loss: 0.450676]\n",
      "epoch:10 step:10098 [D loss: 0.228953, acc.: 60.94%] [G loss: 0.458461]\n",
      "epoch:10 step:10099 [D loss: 0.240541, acc.: 60.94%] [G loss: 0.414531]\n",
      "epoch:10 step:10100 [D loss: 0.212698, acc.: 67.97%] [G loss: 0.468314]\n",
      "epoch:10 step:10101 [D loss: 0.234646, acc.: 61.72%] [G loss: 0.453526]\n",
      "epoch:10 step:10102 [D loss: 0.234972, acc.: 61.72%] [G loss: 0.438235]\n",
      "epoch:10 step:10103 [D loss: 0.191353, acc.: 71.09%] [G loss: 0.442797]\n",
      "epoch:10 step:10104 [D loss: 0.242619, acc.: 57.81%] [G loss: 0.438680]\n",
      "epoch:10 step:10105 [D loss: 0.228814, acc.: 64.84%] [G loss: 0.460195]\n",
      "epoch:10 step:10106 [D loss: 0.211752, acc.: 62.50%] [G loss: 0.464592]\n",
      "epoch:10 step:10107 [D loss: 0.226585, acc.: 59.38%] [G loss: 0.465084]\n",
      "epoch:10 step:10108 [D loss: 0.256126, acc.: 53.12%] [G loss: 0.453057]\n",
      "epoch:10 step:10109 [D loss: 0.251079, acc.: 56.25%] [G loss: 0.438579]\n",
      "epoch:10 step:10110 [D loss: 0.222568, acc.: 64.84%] [G loss: 0.428874]\n",
      "epoch:10 step:10111 [D loss: 0.251170, acc.: 54.69%] [G loss: 0.447775]\n",
      "epoch:10 step:10112 [D loss: 0.224794, acc.: 59.38%] [G loss: 0.441029]\n",
      "epoch:10 step:10113 [D loss: 0.218002, acc.: 66.41%] [G loss: 0.481712]\n",
      "epoch:10 step:10114 [D loss: 0.240857, acc.: 60.16%] [G loss: 0.455536]\n",
      "epoch:10 step:10115 [D loss: 0.232665, acc.: 66.41%] [G loss: 0.420053]\n",
      "epoch:10 step:10116 [D loss: 0.185944, acc.: 70.31%] [G loss: 0.454394]\n",
      "epoch:10 step:10117 [D loss: 0.208934, acc.: 67.97%] [G loss: 0.477424]\n",
      "epoch:10 step:10118 [D loss: 0.234416, acc.: 60.16%] [G loss: 0.453629]\n",
      "epoch:10 step:10119 [D loss: 0.240325, acc.: 61.72%] [G loss: 0.429383]\n",
      "epoch:10 step:10120 [D loss: 0.218076, acc.: 62.50%] [G loss: 0.462238]\n",
      "epoch:10 step:10121 [D loss: 0.216328, acc.: 61.72%] [G loss: 0.499887]\n",
      "epoch:10 step:10122 [D loss: 0.239622, acc.: 60.16%] [G loss: 0.432250]\n",
      "epoch:10 step:10123 [D loss: 0.217139, acc.: 65.62%] [G loss: 0.455375]\n",
      "epoch:10 step:10124 [D loss: 0.205549, acc.: 66.41%] [G loss: 0.481174]\n",
      "epoch:10 step:10125 [D loss: 0.214999, acc.: 63.28%] [G loss: 0.496145]\n",
      "epoch:10 step:10126 [D loss: 0.228385, acc.: 63.28%] [G loss: 0.488523]\n",
      "epoch:10 step:10127 [D loss: 0.232602, acc.: 59.38%] [G loss: 0.468740]\n",
      "epoch:10 step:10128 [D loss: 0.254765, acc.: 62.50%] [G loss: 0.443122]\n",
      "epoch:10 step:10129 [D loss: 0.255723, acc.: 52.34%] [G loss: 0.452574]\n",
      "epoch:10 step:10130 [D loss: 0.227928, acc.: 57.81%] [G loss: 0.408073]\n",
      "epoch:10 step:10131 [D loss: 0.231673, acc.: 60.94%] [G loss: 0.418380]\n",
      "epoch:10 step:10132 [D loss: 0.236268, acc.: 57.03%] [G loss: 0.417182]\n",
      "epoch:10 step:10133 [D loss: 0.216608, acc.: 67.97%] [G loss: 0.438332]\n",
      "epoch:10 step:10134 [D loss: 0.213239, acc.: 64.06%] [G loss: 0.457432]\n",
      "epoch:10 step:10135 [D loss: 0.273250, acc.: 50.78%] [G loss: 0.424183]\n",
      "epoch:10 step:10136 [D loss: 0.250507, acc.: 52.34%] [G loss: 0.422626]\n",
      "epoch:10 step:10137 [D loss: 0.206648, acc.: 70.31%] [G loss: 0.441893]\n",
      "epoch:10 step:10138 [D loss: 0.210634, acc.: 62.50%] [G loss: 0.500213]\n",
      "epoch:10 step:10139 [D loss: 0.205983, acc.: 65.62%] [G loss: 0.485406]\n",
      "epoch:10 step:10140 [D loss: 0.218066, acc.: 61.72%] [G loss: 0.464152]\n",
      "epoch:10 step:10141 [D loss: 0.197787, acc.: 69.53%] [G loss: 0.474069]\n",
      "epoch:10 step:10142 [D loss: 0.270411, acc.: 53.12%] [G loss: 0.432568]\n",
      "epoch:10 step:10143 [D loss: 0.204515, acc.: 65.62%] [G loss: 0.482262]\n",
      "epoch:10 step:10144 [D loss: 0.222211, acc.: 67.19%] [G loss: 0.471843]\n",
      "epoch:10 step:10145 [D loss: 0.212862, acc.: 67.97%] [G loss: 0.491598]\n",
      "epoch:10 step:10146 [D loss: 0.221109, acc.: 59.38%] [G loss: 0.448521]\n",
      "epoch:10 step:10147 [D loss: 0.193331, acc.: 70.31%] [G loss: 0.465488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10148 [D loss: 0.221327, acc.: 60.94%] [G loss: 0.468137]\n",
      "epoch:10 step:10149 [D loss: 0.212083, acc.: 68.75%] [G loss: 0.437563]\n",
      "epoch:10 step:10150 [D loss: 0.209681, acc.: 64.84%] [G loss: 0.457662]\n",
      "epoch:10 step:10151 [D loss: 0.200968, acc.: 67.97%] [G loss: 0.475038]\n",
      "epoch:10 step:10152 [D loss: 0.201057, acc.: 69.53%] [G loss: 0.580915]\n",
      "epoch:10 step:10153 [D loss: 0.256654, acc.: 53.91%] [G loss: 0.483982]\n",
      "epoch:10 step:10154 [D loss: 0.272691, acc.: 53.12%] [G loss: 0.413248]\n",
      "epoch:10 step:10155 [D loss: 0.199353, acc.: 64.06%] [G loss: 0.456740]\n",
      "epoch:10 step:10156 [D loss: 0.189740, acc.: 72.66%] [G loss: 0.452826]\n",
      "epoch:10 step:10157 [D loss: 0.284794, acc.: 50.00%] [G loss: 0.406591]\n",
      "epoch:10 step:10158 [D loss: 0.250709, acc.: 55.47%] [G loss: 0.424635]\n",
      "epoch:10 step:10159 [D loss: 0.228846, acc.: 60.16%] [G loss: 0.440203]\n",
      "epoch:10 step:10160 [D loss: 0.199997, acc.: 71.88%] [G loss: 0.420967]\n",
      "epoch:10 step:10161 [D loss: 0.248790, acc.: 55.47%] [G loss: 0.412301]\n",
      "epoch:10 step:10162 [D loss: 0.193114, acc.: 70.31%] [G loss: 0.508712]\n",
      "epoch:10 step:10163 [D loss: 0.214119, acc.: 61.72%] [G loss: 0.476804]\n",
      "epoch:10 step:10164 [D loss: 0.256999, acc.: 53.12%] [G loss: 0.430293]\n",
      "epoch:10 step:10165 [D loss: 0.238585, acc.: 55.47%] [G loss: 0.436631]\n",
      "epoch:10 step:10166 [D loss: 0.247262, acc.: 61.72%] [G loss: 0.458772]\n",
      "epoch:10 step:10167 [D loss: 0.246496, acc.: 56.25%] [G loss: 0.418930]\n",
      "epoch:10 step:10168 [D loss: 0.224510, acc.: 62.50%] [G loss: 0.454005]\n",
      "epoch:10 step:10169 [D loss: 0.224220, acc.: 60.94%] [G loss: 0.430364]\n",
      "epoch:10 step:10170 [D loss: 0.223804, acc.: 64.84%] [G loss: 0.482507]\n",
      "epoch:10 step:10171 [D loss: 0.192920, acc.: 69.53%] [G loss: 0.481318]\n",
      "epoch:10 step:10172 [D loss: 0.231559, acc.: 67.97%] [G loss: 0.450215]\n",
      "epoch:10 step:10173 [D loss: 0.213378, acc.: 69.53%] [G loss: 0.491350]\n",
      "epoch:10 step:10174 [D loss: 0.240711, acc.: 60.16%] [G loss: 0.416597]\n",
      "epoch:10 step:10175 [D loss: 0.225240, acc.: 60.16%] [G loss: 0.422187]\n",
      "epoch:10 step:10176 [D loss: 0.227237, acc.: 59.38%] [G loss: 0.429014]\n",
      "epoch:10 step:10177 [D loss: 0.222218, acc.: 61.72%] [G loss: 0.466533]\n",
      "epoch:10 step:10178 [D loss: 0.242805, acc.: 64.06%] [G loss: 0.425879]\n",
      "epoch:10 step:10179 [D loss: 0.232574, acc.: 61.72%] [G loss: 0.442520]\n",
      "epoch:10 step:10180 [D loss: 0.232229, acc.: 61.72%] [G loss: 0.454516]\n",
      "epoch:10 step:10181 [D loss: 0.215447, acc.: 67.19%] [G loss: 0.466601]\n",
      "epoch:10 step:10182 [D loss: 0.242170, acc.: 57.03%] [G loss: 0.445004]\n",
      "epoch:10 step:10183 [D loss: 0.215461, acc.: 63.28%] [G loss: 0.410195]\n",
      "epoch:10 step:10184 [D loss: 0.236195, acc.: 60.16%] [G loss: 0.438144]\n",
      "epoch:10 step:10185 [D loss: 0.196771, acc.: 72.66%] [G loss: 0.449518]\n",
      "epoch:10 step:10186 [D loss: 0.232097, acc.: 63.28%] [G loss: 0.417553]\n",
      "epoch:10 step:10187 [D loss: 0.267963, acc.: 52.34%] [G loss: 0.451886]\n",
      "epoch:10 step:10188 [D loss: 0.265365, acc.: 53.91%] [G loss: 0.453696]\n",
      "epoch:10 step:10189 [D loss: 0.222736, acc.: 64.06%] [G loss: 0.447429]\n",
      "epoch:10 step:10190 [D loss: 0.273903, acc.: 54.69%] [G loss: 0.419172]\n",
      "epoch:10 step:10191 [D loss: 0.221702, acc.: 64.84%] [G loss: 0.446310]\n",
      "epoch:10 step:10192 [D loss: 0.213151, acc.: 66.41%] [G loss: 0.386093]\n",
      "epoch:10 step:10193 [D loss: 0.209193, acc.: 64.84%] [G loss: 0.483840]\n",
      "epoch:10 step:10194 [D loss: 0.252445, acc.: 57.81%] [G loss: 0.404828]\n",
      "epoch:10 step:10195 [D loss: 0.224716, acc.: 64.06%] [G loss: 0.441601]\n",
      "epoch:10 step:10196 [D loss: 0.236484, acc.: 58.59%] [G loss: 0.412142]\n",
      "epoch:10 step:10197 [D loss: 0.237266, acc.: 60.16%] [G loss: 0.490431]\n",
      "epoch:10 step:10198 [D loss: 0.246860, acc.: 55.47%] [G loss: 0.426149]\n",
      "epoch:10 step:10199 [D loss: 0.240019, acc.: 62.50%] [G loss: 0.419646]\n",
      "epoch:10 step:10200 [D loss: 0.221178, acc.: 65.62%] [G loss: 0.455233]\n",
      "##############\n",
      "[2.37590064 1.61062206 6.06897931 4.70990939 3.79442655 5.59737516\n",
      " 4.28435749 4.67228294 4.46738631 4.04164848]\n",
      "##########\n",
      "epoch:10 step:10201 [D loss: 0.193210, acc.: 69.53%] [G loss: 0.462547]\n",
      "epoch:10 step:10202 [D loss: 0.227460, acc.: 60.16%] [G loss: 0.417054]\n",
      "epoch:10 step:10203 [D loss: 0.204630, acc.: 64.84%] [G loss: 0.444628]\n",
      "epoch:10 step:10204 [D loss: 0.223272, acc.: 64.06%] [G loss: 0.458871]\n",
      "epoch:10 step:10205 [D loss: 0.222157, acc.: 60.94%] [G loss: 0.430011]\n",
      "epoch:10 step:10206 [D loss: 0.230507, acc.: 62.50%] [G loss: 0.408971]\n",
      "epoch:10 step:10207 [D loss: 0.218783, acc.: 62.50%] [G loss: 0.476498]\n",
      "epoch:10 step:10208 [D loss: 0.194615, acc.: 71.88%] [G loss: 0.457035]\n",
      "epoch:10 step:10209 [D loss: 0.227896, acc.: 61.72%] [G loss: 0.431328]\n",
      "epoch:10 step:10210 [D loss: 0.246347, acc.: 53.91%] [G loss: 0.400645]\n",
      "epoch:10 step:10211 [D loss: 0.212738, acc.: 64.06%] [G loss: 0.473572]\n",
      "epoch:10 step:10212 [D loss: 0.190253, acc.: 75.00%] [G loss: 0.490643]\n",
      "epoch:10 step:10213 [D loss: 0.222177, acc.: 66.41%] [G loss: 0.495122]\n",
      "epoch:10 step:10214 [D loss: 0.226461, acc.: 64.06%] [G loss: 0.474725]\n",
      "epoch:10 step:10215 [D loss: 0.200498, acc.: 66.41%] [G loss: 0.462448]\n",
      "epoch:10 step:10216 [D loss: 0.227051, acc.: 60.94%] [G loss: 0.447478]\n",
      "epoch:10 step:10217 [D loss: 0.242820, acc.: 57.03%] [G loss: 0.429594]\n",
      "epoch:10 step:10218 [D loss: 0.189455, acc.: 78.91%] [G loss: 0.451542]\n",
      "epoch:10 step:10219 [D loss: 0.211564, acc.: 67.97%] [G loss: 0.458291]\n",
      "epoch:10 step:10220 [D loss: 0.247918, acc.: 57.03%] [G loss: 0.437730]\n",
      "epoch:10 step:10221 [D loss: 0.248107, acc.: 53.12%] [G loss: 0.442374]\n",
      "epoch:10 step:10222 [D loss: 0.218833, acc.: 60.16%] [G loss: 0.433586]\n",
      "epoch:10 step:10223 [D loss: 0.217699, acc.: 67.97%] [G loss: 0.437156]\n",
      "epoch:10 step:10224 [D loss: 0.226682, acc.: 62.50%] [G loss: 0.455870]\n",
      "epoch:10 step:10225 [D loss: 0.223948, acc.: 62.50%] [G loss: 0.438549]\n",
      "epoch:10 step:10226 [D loss: 0.253163, acc.: 51.56%] [G loss: 0.408605]\n",
      "epoch:10 step:10227 [D loss: 0.208168, acc.: 67.19%] [G loss: 0.474604]\n",
      "epoch:10 step:10228 [D loss: 0.273418, acc.: 53.91%] [G loss: 0.429307]\n",
      "epoch:10 step:10229 [D loss: 0.234481, acc.: 60.94%] [G loss: 0.424780]\n",
      "epoch:10 step:10230 [D loss: 0.193965, acc.: 69.53%] [G loss: 0.496485]\n",
      "epoch:10 step:10231 [D loss: 0.244018, acc.: 60.94%] [G loss: 0.435895]\n",
      "epoch:10 step:10232 [D loss: 0.237159, acc.: 57.81%] [G loss: 0.374063]\n",
      "epoch:10 step:10233 [D loss: 0.235210, acc.: 60.16%] [G loss: 0.398348]\n",
      "epoch:10 step:10234 [D loss: 0.240240, acc.: 64.84%] [G loss: 0.430097]\n",
      "epoch:10 step:10235 [D loss: 0.249636, acc.: 57.03%] [G loss: 0.419162]\n",
      "epoch:10 step:10236 [D loss: 0.232106, acc.: 61.72%] [G loss: 0.444112]\n",
      "epoch:10 step:10237 [D loss: 0.237098, acc.: 60.94%] [G loss: 0.415081]\n",
      "epoch:10 step:10238 [D loss: 0.235124, acc.: 60.94%] [G loss: 0.380899]\n",
      "epoch:10 step:10239 [D loss: 0.253872, acc.: 52.34%] [G loss: 0.425649]\n",
      "epoch:10 step:10240 [D loss: 0.249297, acc.: 60.16%] [G loss: 0.445990]\n",
      "epoch:10 step:10241 [D loss: 0.226872, acc.: 60.94%] [G loss: 0.473987]\n",
      "epoch:10 step:10242 [D loss: 0.230464, acc.: 62.50%] [G loss: 0.416747]\n",
      "epoch:10 step:10243 [D loss: 0.232483, acc.: 61.72%] [G loss: 0.446660]\n",
      "epoch:10 step:10244 [D loss: 0.205281, acc.: 67.19%] [G loss: 0.463212]\n",
      "epoch:10 step:10245 [D loss: 0.179110, acc.: 76.56%] [G loss: 0.509415]\n",
      "epoch:10 step:10246 [D loss: 0.241606, acc.: 55.47%] [G loss: 0.443362]\n",
      "epoch:10 step:10247 [D loss: 0.229771, acc.: 64.84%] [G loss: 0.443994]\n",
      "epoch:10 step:10248 [D loss: 0.219758, acc.: 67.19%] [G loss: 0.452840]\n",
      "epoch:10 step:10249 [D loss: 0.232709, acc.: 59.38%] [G loss: 0.433346]\n",
      "epoch:10 step:10250 [D loss: 0.237862, acc.: 59.38%] [G loss: 0.451156]\n",
      "epoch:10 step:10251 [D loss: 0.232544, acc.: 57.81%] [G loss: 0.431321]\n",
      "epoch:10 step:10252 [D loss: 0.230163, acc.: 62.50%] [G loss: 0.429679]\n",
      "epoch:10 step:10253 [D loss: 0.231574, acc.: 61.72%] [G loss: 0.450861]\n",
      "epoch:10 step:10254 [D loss: 0.213320, acc.: 64.84%] [G loss: 0.472219]\n",
      "epoch:10 step:10255 [D loss: 0.214907, acc.: 67.19%] [G loss: 0.502033]\n",
      "epoch:10 step:10256 [D loss: 0.198606, acc.: 71.09%] [G loss: 0.491475]\n",
      "epoch:10 step:10257 [D loss: 0.244445, acc.: 59.38%] [G loss: 0.498147]\n",
      "epoch:10 step:10258 [D loss: 0.241038, acc.: 55.47%] [G loss: 0.472842]\n",
      "epoch:10 step:10259 [D loss: 0.213572, acc.: 68.75%] [G loss: 0.434152]\n",
      "epoch:10 step:10260 [D loss: 0.232340, acc.: 62.50%] [G loss: 0.492816]\n",
      "epoch:10 step:10261 [D loss: 0.256066, acc.: 53.12%] [G loss: 0.478904]\n",
      "epoch:10 step:10262 [D loss: 0.281734, acc.: 47.66%] [G loss: 0.416739]\n",
      "epoch:10 step:10263 [D loss: 0.191989, acc.: 66.41%] [G loss: 0.475165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10264 [D loss: 0.200328, acc.: 75.78%] [G loss: 0.474554]\n",
      "epoch:10 step:10265 [D loss: 0.211096, acc.: 66.41%] [G loss: 0.472397]\n",
      "epoch:10 step:10266 [D loss: 0.219853, acc.: 69.53%] [G loss: 0.453860]\n",
      "epoch:10 step:10267 [D loss: 0.192876, acc.: 71.09%] [G loss: 0.482995]\n",
      "epoch:10 step:10268 [D loss: 0.225557, acc.: 57.81%] [G loss: 0.436640]\n",
      "epoch:10 step:10269 [D loss: 0.190043, acc.: 72.66%] [G loss: 0.485382]\n",
      "epoch:10 step:10270 [D loss: 0.201654, acc.: 69.53%] [G loss: 0.489131]\n",
      "epoch:10 step:10271 [D loss: 0.222828, acc.: 60.94%] [G loss: 0.430640]\n",
      "epoch:10 step:10272 [D loss: 0.242150, acc.: 59.38%] [G loss: 0.432426]\n",
      "epoch:10 step:10273 [D loss: 0.231911, acc.: 58.59%] [G loss: 0.471599]\n",
      "epoch:10 step:10274 [D loss: 0.221290, acc.: 64.84%] [G loss: 0.479035]\n",
      "epoch:10 step:10275 [D loss: 0.207837, acc.: 67.97%] [G loss: 0.502663]\n",
      "epoch:10 step:10276 [D loss: 0.206966, acc.: 70.31%] [G loss: 0.535372]\n",
      "epoch:10 step:10277 [D loss: 0.226487, acc.: 61.72%] [G loss: 0.435630]\n",
      "epoch:10 step:10278 [D loss: 0.207473, acc.: 67.19%] [G loss: 0.469952]\n",
      "epoch:10 step:10279 [D loss: 0.225416, acc.: 60.16%] [G loss: 0.418027]\n",
      "epoch:10 step:10280 [D loss: 0.231416, acc.: 58.59%] [G loss: 0.458483]\n",
      "epoch:10 step:10281 [D loss: 0.203994, acc.: 67.97%] [G loss: 0.488714]\n",
      "epoch:10 step:10282 [D loss: 0.172178, acc.: 73.44%] [G loss: 0.574040]\n",
      "epoch:10 step:10283 [D loss: 0.262787, acc.: 53.12%] [G loss: 0.492371]\n",
      "epoch:10 step:10284 [D loss: 0.226683, acc.: 62.50%] [G loss: 0.445134]\n",
      "epoch:10 step:10285 [D loss: 0.269320, acc.: 53.12%] [G loss: 0.463642]\n",
      "epoch:10 step:10286 [D loss: 0.242739, acc.: 55.47%] [G loss: 0.421754]\n",
      "epoch:10 step:10287 [D loss: 0.260969, acc.: 60.16%] [G loss: 0.482649]\n",
      "epoch:10 step:10288 [D loss: 0.194819, acc.: 75.00%] [G loss: 0.553830]\n",
      "epoch:10 step:10289 [D loss: 0.206313, acc.: 64.84%] [G loss: 0.507492]\n",
      "epoch:10 step:10290 [D loss: 0.304164, acc.: 50.00%] [G loss: 0.403200]\n",
      "epoch:10 step:10291 [D loss: 0.218412, acc.: 64.06%] [G loss: 0.440959]\n",
      "epoch:10 step:10292 [D loss: 0.225575, acc.: 67.97%] [G loss: 0.426216]\n",
      "epoch:10 step:10293 [D loss: 0.186834, acc.: 72.66%] [G loss: 0.477076]\n",
      "epoch:10 step:10294 [D loss: 0.171022, acc.: 75.00%] [G loss: 0.476803]\n",
      "epoch:10 step:10295 [D loss: 0.190302, acc.: 70.31%] [G loss: 0.513064]\n",
      "epoch:10 step:10296 [D loss: 0.189563, acc.: 68.75%] [G loss: 0.556768]\n",
      "epoch:10 step:10297 [D loss: 0.225354, acc.: 67.97%] [G loss: 0.580264]\n",
      "epoch:10 step:10298 [D loss: 0.309787, acc.: 59.38%] [G loss: 0.538520]\n",
      "epoch:10 step:10299 [D loss: 0.219796, acc.: 67.97%] [G loss: 0.550232]\n",
      "epoch:10 step:10300 [D loss: 0.201119, acc.: 67.19%] [G loss: 0.539207]\n",
      "epoch:10 step:10301 [D loss: 0.261819, acc.: 57.03%] [G loss: 0.452085]\n",
      "epoch:10 step:10302 [D loss: 0.259043, acc.: 54.69%] [G loss: 0.425942]\n",
      "epoch:10 step:10303 [D loss: 0.213447, acc.: 70.31%] [G loss: 0.432612]\n",
      "epoch:10 step:10304 [D loss: 0.237596, acc.: 60.16%] [G loss: 0.444433]\n",
      "epoch:10 step:10305 [D loss: 0.192905, acc.: 73.44%] [G loss: 0.485918]\n",
      "epoch:10 step:10306 [D loss: 0.141754, acc.: 79.69%] [G loss: 0.565139]\n",
      "epoch:10 step:10307 [D loss: 0.181917, acc.: 74.22%] [G loss: 0.554914]\n",
      "epoch:11 step:10308 [D loss: 0.244544, acc.: 61.72%] [G loss: 0.519474]\n",
      "epoch:11 step:10309 [D loss: 0.248690, acc.: 64.84%] [G loss: 0.476732]\n",
      "epoch:11 step:10310 [D loss: 0.239696, acc.: 59.38%] [G loss: 0.458823]\n",
      "epoch:11 step:10311 [D loss: 0.233764, acc.: 61.72%] [G loss: 0.499025]\n",
      "epoch:11 step:10312 [D loss: 0.254784, acc.: 52.34%] [G loss: 0.425341]\n",
      "epoch:11 step:10313 [D loss: 0.211900, acc.: 66.41%] [G loss: 0.478586]\n",
      "epoch:11 step:10314 [D loss: 0.206637, acc.: 68.75%] [G loss: 0.499250]\n",
      "epoch:11 step:10315 [D loss: 0.213527, acc.: 69.53%] [G loss: 0.451704]\n",
      "epoch:11 step:10316 [D loss: 0.204940, acc.: 71.88%] [G loss: 0.465277]\n",
      "epoch:11 step:10317 [D loss: 0.223229, acc.: 66.41%] [G loss: 0.492860]\n",
      "epoch:11 step:10318 [D loss: 0.184446, acc.: 73.44%] [G loss: 0.466450]\n",
      "epoch:11 step:10319 [D loss: 0.211595, acc.: 63.28%] [G loss: 0.490559]\n",
      "epoch:11 step:10320 [D loss: 0.230937, acc.: 59.38%] [G loss: 0.442731]\n",
      "epoch:11 step:10321 [D loss: 0.190893, acc.: 73.44%] [G loss: 0.458780]\n",
      "epoch:11 step:10322 [D loss: 0.192647, acc.: 71.88%] [G loss: 0.473365]\n",
      "epoch:11 step:10323 [D loss: 0.198855, acc.: 68.75%] [G loss: 0.531901]\n",
      "epoch:11 step:10324 [D loss: 0.219798, acc.: 65.62%] [G loss: 0.440624]\n",
      "epoch:11 step:10325 [D loss: 0.264895, acc.: 53.12%] [G loss: 0.442445]\n",
      "epoch:11 step:10326 [D loss: 0.238285, acc.: 62.50%] [G loss: 0.456487]\n",
      "epoch:11 step:10327 [D loss: 0.249679, acc.: 57.03%] [G loss: 0.458158]\n",
      "epoch:11 step:10328 [D loss: 0.231496, acc.: 55.47%] [G loss: 0.454901]\n",
      "epoch:11 step:10329 [D loss: 0.187634, acc.: 76.56%] [G loss: 0.491731]\n",
      "epoch:11 step:10330 [D loss: 0.227761, acc.: 64.06%] [G loss: 0.470436]\n",
      "epoch:11 step:10331 [D loss: 0.222015, acc.: 66.41%] [G loss: 0.457609]\n",
      "epoch:11 step:10332 [D loss: 0.198223, acc.: 71.88%] [G loss: 0.471973]\n",
      "epoch:11 step:10333 [D loss: 0.230736, acc.: 62.50%] [G loss: 0.419843]\n",
      "epoch:11 step:10334 [D loss: 0.214762, acc.: 64.06%] [G loss: 0.467516]\n",
      "epoch:11 step:10335 [D loss: 0.213268, acc.: 64.06%] [G loss: 0.467485]\n",
      "epoch:11 step:10336 [D loss: 0.197993, acc.: 72.66%] [G loss: 0.474493]\n",
      "epoch:11 step:10337 [D loss: 0.240757, acc.: 59.38%] [G loss: 0.439977]\n",
      "epoch:11 step:10338 [D loss: 0.236899, acc.: 58.59%] [G loss: 0.418825]\n",
      "epoch:11 step:10339 [D loss: 0.226885, acc.: 68.75%] [G loss: 0.435512]\n",
      "epoch:11 step:10340 [D loss: 0.221240, acc.: 63.28%] [G loss: 0.457655]\n",
      "epoch:11 step:10341 [D loss: 0.260139, acc.: 47.66%] [G loss: 0.419584]\n",
      "epoch:11 step:10342 [D loss: 0.236372, acc.: 60.16%] [G loss: 0.439169]\n",
      "epoch:11 step:10343 [D loss: 0.195046, acc.: 69.53%] [G loss: 0.475485]\n",
      "epoch:11 step:10344 [D loss: 0.236414, acc.: 62.50%] [G loss: 0.442041]\n",
      "epoch:11 step:10345 [D loss: 0.263249, acc.: 53.12%] [G loss: 0.439250]\n",
      "epoch:11 step:10346 [D loss: 0.219802, acc.: 60.94%] [G loss: 0.490403]\n",
      "epoch:11 step:10347 [D loss: 0.175195, acc.: 74.22%] [G loss: 0.493979]\n",
      "epoch:11 step:10348 [D loss: 0.221188, acc.: 62.50%] [G loss: 0.424291]\n",
      "epoch:11 step:10349 [D loss: 0.190540, acc.: 73.44%] [G loss: 0.467239]\n",
      "epoch:11 step:10350 [D loss: 0.231017, acc.: 65.62%] [G loss: 0.425790]\n",
      "epoch:11 step:10351 [D loss: 0.248124, acc.: 54.69%] [G loss: 0.449129]\n",
      "epoch:11 step:10352 [D loss: 0.215963, acc.: 64.06%] [G loss: 0.483694]\n",
      "epoch:11 step:10353 [D loss: 0.261322, acc.: 52.34%] [G loss: 0.440995]\n",
      "epoch:11 step:10354 [D loss: 0.212573, acc.: 68.75%] [G loss: 0.441752]\n",
      "epoch:11 step:10355 [D loss: 0.213681, acc.: 64.84%] [G loss: 0.453135]\n",
      "epoch:11 step:10356 [D loss: 0.218281, acc.: 64.06%] [G loss: 0.466995]\n",
      "epoch:11 step:10357 [D loss: 0.211753, acc.: 63.28%] [G loss: 0.483709]\n",
      "epoch:11 step:10358 [D loss: 0.259889, acc.: 54.69%] [G loss: 0.415348]\n",
      "epoch:11 step:10359 [D loss: 0.214911, acc.: 67.19%] [G loss: 0.444274]\n",
      "epoch:11 step:10360 [D loss: 0.217800, acc.: 67.19%] [G loss: 0.480506]\n",
      "epoch:11 step:10361 [D loss: 0.236276, acc.: 60.16%] [G loss: 0.465593]\n",
      "epoch:11 step:10362 [D loss: 0.214604, acc.: 67.19%] [G loss: 0.446030]\n",
      "epoch:11 step:10363 [D loss: 0.222785, acc.: 66.41%] [G loss: 0.483932]\n",
      "epoch:11 step:10364 [D loss: 0.240168, acc.: 59.38%] [G loss: 0.441331]\n",
      "epoch:11 step:10365 [D loss: 0.218747, acc.: 62.50%] [G loss: 0.454056]\n",
      "epoch:11 step:10366 [D loss: 0.221237, acc.: 62.50%] [G loss: 0.450527]\n",
      "epoch:11 step:10367 [D loss: 0.239741, acc.: 63.28%] [G loss: 0.409648]\n",
      "epoch:11 step:10368 [D loss: 0.252713, acc.: 57.03%] [G loss: 0.414054]\n",
      "epoch:11 step:10369 [D loss: 0.209699, acc.: 68.75%] [G loss: 0.417449]\n",
      "epoch:11 step:10370 [D loss: 0.231021, acc.: 60.94%] [G loss: 0.403590]\n",
      "epoch:11 step:10371 [D loss: 0.223204, acc.: 60.94%] [G loss: 0.444541]\n",
      "epoch:11 step:10372 [D loss: 0.214644, acc.: 67.97%] [G loss: 0.449582]\n",
      "epoch:11 step:10373 [D loss: 0.208636, acc.: 69.53%] [G loss: 0.432581]\n",
      "epoch:11 step:10374 [D loss: 0.219533, acc.: 64.06%] [G loss: 0.454728]\n",
      "epoch:11 step:10375 [D loss: 0.214180, acc.: 64.06%] [G loss: 0.466099]\n",
      "epoch:11 step:10376 [D loss: 0.191977, acc.: 68.75%] [G loss: 0.462568]\n",
      "epoch:11 step:10377 [D loss: 0.209886, acc.: 69.53%] [G loss: 0.474215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10378 [D loss: 0.233491, acc.: 60.94%] [G loss: 0.430978]\n",
      "epoch:11 step:10379 [D loss: 0.252795, acc.: 63.28%] [G loss: 0.448382]\n",
      "epoch:11 step:10380 [D loss: 0.232100, acc.: 60.94%] [G loss: 0.441827]\n",
      "epoch:11 step:10381 [D loss: 0.222879, acc.: 66.41%] [G loss: 0.441231]\n",
      "epoch:11 step:10382 [D loss: 0.218374, acc.: 64.84%] [G loss: 0.443385]\n",
      "epoch:11 step:10383 [D loss: 0.220478, acc.: 68.75%] [G loss: 0.465934]\n",
      "epoch:11 step:10384 [D loss: 0.175092, acc.: 76.56%] [G loss: 0.525674]\n",
      "epoch:11 step:10385 [D loss: 0.267865, acc.: 53.91%] [G loss: 0.430843]\n",
      "epoch:11 step:10386 [D loss: 0.214493, acc.: 66.41%] [G loss: 0.416890]\n",
      "epoch:11 step:10387 [D loss: 0.209355, acc.: 70.31%] [G loss: 0.437885]\n",
      "epoch:11 step:10388 [D loss: 0.235718, acc.: 59.38%] [G loss: 0.440753]\n",
      "epoch:11 step:10389 [D loss: 0.220254, acc.: 64.84%] [G loss: 0.417481]\n",
      "epoch:11 step:10390 [D loss: 0.219470, acc.: 67.19%] [G loss: 0.467459]\n",
      "epoch:11 step:10391 [D loss: 0.206395, acc.: 68.75%] [G loss: 0.455301]\n",
      "epoch:11 step:10392 [D loss: 0.220955, acc.: 60.16%] [G loss: 0.490623]\n",
      "epoch:11 step:10393 [D loss: 0.243097, acc.: 64.84%] [G loss: 0.412150]\n",
      "epoch:11 step:10394 [D loss: 0.237951, acc.: 61.72%] [G loss: 0.442055]\n",
      "epoch:11 step:10395 [D loss: 0.200305, acc.: 71.09%] [G loss: 0.505226]\n",
      "epoch:11 step:10396 [D loss: 0.192451, acc.: 75.00%] [G loss: 0.490998]\n",
      "epoch:11 step:10397 [D loss: 0.212130, acc.: 63.28%] [G loss: 0.485856]\n",
      "epoch:11 step:10398 [D loss: 0.240304, acc.: 62.50%] [G loss: 0.447445]\n",
      "epoch:11 step:10399 [D loss: 0.194410, acc.: 73.44%] [G loss: 0.520991]\n",
      "epoch:11 step:10400 [D loss: 0.213227, acc.: 65.62%] [G loss: 0.512379]\n",
      "##############\n",
      "[2.42602976 1.65095646 6.24819285 4.85551403 3.77245079 5.680166\n",
      " 4.49184767 4.95249928 4.33643052 3.72264995]\n",
      "##########\n",
      "epoch:11 step:10401 [D loss: 0.221107, acc.: 65.62%] [G loss: 0.480047]\n",
      "epoch:11 step:10402 [D loss: 0.231842, acc.: 61.72%] [G loss: 0.449132]\n",
      "epoch:11 step:10403 [D loss: 0.200116, acc.: 71.09%] [G loss: 0.443680]\n",
      "epoch:11 step:10404 [D loss: 0.193483, acc.: 70.31%] [G loss: 0.506031]\n",
      "epoch:11 step:10405 [D loss: 0.254283, acc.: 52.34%] [G loss: 0.449679]\n",
      "epoch:11 step:10406 [D loss: 0.221735, acc.: 62.50%] [G loss: 0.473657]\n",
      "epoch:11 step:10407 [D loss: 0.175508, acc.: 73.44%] [G loss: 0.522548]\n",
      "epoch:11 step:10408 [D loss: 0.230353, acc.: 65.62%] [G loss: 0.470229]\n",
      "epoch:11 step:10409 [D loss: 0.250064, acc.: 53.12%] [G loss: 0.450218]\n",
      "epoch:11 step:10410 [D loss: 0.225900, acc.: 58.59%] [G loss: 0.419454]\n",
      "epoch:11 step:10411 [D loss: 0.224509, acc.: 67.19%] [G loss: 0.420419]\n",
      "epoch:11 step:10412 [D loss: 0.225887, acc.: 60.94%] [G loss: 0.425467]\n",
      "epoch:11 step:10413 [D loss: 0.200456, acc.: 67.97%] [G loss: 0.466044]\n",
      "epoch:11 step:10414 [D loss: 0.181349, acc.: 76.56%] [G loss: 0.538053]\n",
      "epoch:11 step:10415 [D loss: 0.247300, acc.: 60.16%] [G loss: 0.496195]\n",
      "epoch:11 step:10416 [D loss: 0.254295, acc.: 57.03%] [G loss: 0.454762]\n",
      "epoch:11 step:10417 [D loss: 0.250968, acc.: 57.03%] [G loss: 0.461948]\n",
      "epoch:11 step:10418 [D loss: 0.224472, acc.: 60.16%] [G loss: 0.420249]\n",
      "epoch:11 step:10419 [D loss: 0.194650, acc.: 70.31%] [G loss: 0.468534]\n",
      "epoch:11 step:10420 [D loss: 0.210423, acc.: 66.41%] [G loss: 0.436119]\n",
      "epoch:11 step:10421 [D loss: 0.209583, acc.: 67.97%] [G loss: 0.468858]\n",
      "epoch:11 step:10422 [D loss: 0.214151, acc.: 67.19%] [G loss: 0.499516]\n",
      "epoch:11 step:10423 [D loss: 0.226354, acc.: 65.62%] [G loss: 0.464119]\n",
      "epoch:11 step:10424 [D loss: 0.213834, acc.: 70.31%] [G loss: 0.504846]\n",
      "epoch:11 step:10425 [D loss: 0.218432, acc.: 67.97%] [G loss: 0.530878]\n",
      "epoch:11 step:10426 [D loss: 0.202924, acc.: 66.41%] [G loss: 0.513869]\n",
      "epoch:11 step:10427 [D loss: 0.266614, acc.: 57.81%] [G loss: 0.490303]\n",
      "epoch:11 step:10428 [D loss: 0.251375, acc.: 58.59%] [G loss: 0.447311]\n",
      "epoch:11 step:10429 [D loss: 0.198381, acc.: 72.66%] [G loss: 0.473248]\n",
      "epoch:11 step:10430 [D loss: 0.202074, acc.: 69.53%] [G loss: 0.472626]\n",
      "epoch:11 step:10431 [D loss: 0.262690, acc.: 47.66%] [G loss: 0.442415]\n",
      "epoch:11 step:10432 [D loss: 0.256932, acc.: 55.47%] [G loss: 0.459032]\n",
      "epoch:11 step:10433 [D loss: 0.198517, acc.: 67.97%] [G loss: 0.453769]\n",
      "epoch:11 step:10434 [D loss: 0.221198, acc.: 60.16%] [G loss: 0.474234]\n",
      "epoch:11 step:10435 [D loss: 0.231363, acc.: 60.94%] [G loss: 0.451935]\n",
      "epoch:11 step:10436 [D loss: 0.244210, acc.: 62.50%] [G loss: 0.419489]\n",
      "epoch:11 step:10437 [D loss: 0.222999, acc.: 63.28%] [G loss: 0.436803]\n",
      "epoch:11 step:10438 [D loss: 0.218841, acc.: 65.62%] [G loss: 0.435971]\n",
      "epoch:11 step:10439 [D loss: 0.210789, acc.: 64.84%] [G loss: 0.438619]\n",
      "epoch:11 step:10440 [D loss: 0.267326, acc.: 50.00%] [G loss: 0.443661]\n",
      "epoch:11 step:10441 [D loss: 0.212753, acc.: 67.19%] [G loss: 0.483830]\n",
      "epoch:11 step:10442 [D loss: 0.206767, acc.: 67.97%] [G loss: 0.435884]\n",
      "epoch:11 step:10443 [D loss: 0.253091, acc.: 56.25%] [G loss: 0.402930]\n",
      "epoch:11 step:10444 [D loss: 0.252745, acc.: 49.22%] [G loss: 0.425370]\n",
      "epoch:11 step:10445 [D loss: 0.246137, acc.: 60.94%] [G loss: 0.399592]\n",
      "epoch:11 step:10446 [D loss: 0.234910, acc.: 60.94%] [G loss: 0.420105]\n",
      "epoch:11 step:10447 [D loss: 0.226701, acc.: 60.94%] [G loss: 0.436085]\n",
      "epoch:11 step:10448 [D loss: 0.220897, acc.: 63.28%] [G loss: 0.414661]\n",
      "epoch:11 step:10449 [D loss: 0.224567, acc.: 60.16%] [G loss: 0.399218]\n",
      "epoch:11 step:10450 [D loss: 0.244073, acc.: 56.25%] [G loss: 0.445952]\n",
      "epoch:11 step:10451 [D loss: 0.207241, acc.: 71.09%] [G loss: 0.452199]\n",
      "epoch:11 step:10452 [D loss: 0.237011, acc.: 55.47%] [G loss: 0.454344]\n",
      "epoch:11 step:10453 [D loss: 0.243822, acc.: 53.12%] [G loss: 0.416155]\n",
      "epoch:11 step:10454 [D loss: 0.228588, acc.: 60.94%] [G loss: 0.463151]\n",
      "epoch:11 step:10455 [D loss: 0.245674, acc.: 59.38%] [G loss: 0.457285]\n",
      "epoch:11 step:10456 [D loss: 0.218793, acc.: 65.62%] [G loss: 0.417909]\n",
      "epoch:11 step:10457 [D loss: 0.239249, acc.: 60.16%] [G loss: 0.404974]\n",
      "epoch:11 step:10458 [D loss: 0.217865, acc.: 61.72%] [G loss: 0.420600]\n",
      "epoch:11 step:10459 [D loss: 0.228854, acc.: 63.28%] [G loss: 0.430307]\n",
      "epoch:11 step:10460 [D loss: 0.231863, acc.: 60.16%] [G loss: 0.445501]\n",
      "epoch:11 step:10461 [D loss: 0.209239, acc.: 65.62%] [G loss: 0.437443]\n",
      "epoch:11 step:10462 [D loss: 0.223563, acc.: 64.06%] [G loss: 0.431641]\n",
      "epoch:11 step:10463 [D loss: 0.237435, acc.: 60.16%] [G loss: 0.467638]\n",
      "epoch:11 step:10464 [D loss: 0.258762, acc.: 55.47%] [G loss: 0.443689]\n",
      "epoch:11 step:10465 [D loss: 0.224287, acc.: 65.62%] [G loss: 0.480161]\n",
      "epoch:11 step:10466 [D loss: 0.221892, acc.: 65.62%] [G loss: 0.439274]\n",
      "epoch:11 step:10467 [D loss: 0.268744, acc.: 54.69%] [G loss: 0.456320]\n",
      "epoch:11 step:10468 [D loss: 0.215872, acc.: 65.62%] [G loss: 0.497488]\n",
      "epoch:11 step:10469 [D loss: 0.236641, acc.: 58.59%] [G loss: 0.427266]\n",
      "epoch:11 step:10470 [D loss: 0.200990, acc.: 65.62%] [G loss: 0.452492]\n",
      "epoch:11 step:10471 [D loss: 0.236705, acc.: 63.28%] [G loss: 0.459528]\n",
      "epoch:11 step:10472 [D loss: 0.223765, acc.: 59.38%] [G loss: 0.452244]\n",
      "epoch:11 step:10473 [D loss: 0.235898, acc.: 60.16%] [G loss: 0.468631]\n",
      "epoch:11 step:10474 [D loss: 0.219432, acc.: 60.94%] [G loss: 0.410243]\n",
      "epoch:11 step:10475 [D loss: 0.228330, acc.: 63.28%] [G loss: 0.470866]\n",
      "epoch:11 step:10476 [D loss: 0.246479, acc.: 64.06%] [G loss: 0.457196]\n",
      "epoch:11 step:10477 [D loss: 0.257853, acc.: 54.69%] [G loss: 0.384854]\n",
      "epoch:11 step:10478 [D loss: 0.211858, acc.: 63.28%] [G loss: 0.428203]\n",
      "epoch:11 step:10479 [D loss: 0.216905, acc.: 63.28%] [G loss: 0.412185]\n",
      "epoch:11 step:10480 [D loss: 0.190032, acc.: 71.09%] [G loss: 0.422346]\n",
      "epoch:11 step:10481 [D loss: 0.258540, acc.: 55.47%] [G loss: 0.414636]\n",
      "epoch:11 step:10482 [D loss: 0.217720, acc.: 67.19%] [G loss: 0.440498]\n",
      "epoch:11 step:10483 [D loss: 0.218230, acc.: 60.94%] [G loss: 0.449062]\n",
      "epoch:11 step:10484 [D loss: 0.228503, acc.: 58.59%] [G loss: 0.415530]\n",
      "epoch:11 step:10485 [D loss: 0.224561, acc.: 64.06%] [G loss: 0.419761]\n",
      "epoch:11 step:10486 [D loss: 0.236404, acc.: 64.84%] [G loss: 0.402203]\n",
      "epoch:11 step:10487 [D loss: 0.232679, acc.: 63.28%] [G loss: 0.424888]\n",
      "epoch:11 step:10488 [D loss: 0.214233, acc.: 63.28%] [G loss: 0.455395]\n",
      "epoch:11 step:10489 [D loss: 0.239113, acc.: 57.81%] [G loss: 0.466668]\n",
      "epoch:11 step:10490 [D loss: 0.249591, acc.: 57.81%] [G loss: 0.449194]\n",
      "epoch:11 step:10491 [D loss: 0.193528, acc.: 75.00%] [G loss: 0.488046]\n",
      "epoch:11 step:10492 [D loss: 0.255745, acc.: 59.38%] [G loss: 0.427654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10493 [D loss: 0.251221, acc.: 56.25%] [G loss: 0.402789]\n",
      "epoch:11 step:10494 [D loss: 0.240596, acc.: 53.91%] [G loss: 0.413171]\n",
      "epoch:11 step:10495 [D loss: 0.236751, acc.: 59.38%] [G loss: 0.404503]\n",
      "epoch:11 step:10496 [D loss: 0.219942, acc.: 66.41%] [G loss: 0.430120]\n",
      "epoch:11 step:10497 [D loss: 0.223574, acc.: 63.28%] [G loss: 0.430279]\n",
      "epoch:11 step:10498 [D loss: 0.215037, acc.: 64.06%] [G loss: 0.453532]\n",
      "epoch:11 step:10499 [D loss: 0.200589, acc.: 71.09%] [G loss: 0.458017]\n",
      "epoch:11 step:10500 [D loss: 0.207494, acc.: 67.19%] [G loss: 0.432681]\n",
      "epoch:11 step:10501 [D loss: 0.188703, acc.: 75.78%] [G loss: 0.462305]\n",
      "epoch:11 step:10502 [D loss: 0.216148, acc.: 64.84%] [G loss: 0.477613]\n",
      "epoch:11 step:10503 [D loss: 0.218233, acc.: 63.28%] [G loss: 0.459723]\n",
      "epoch:11 step:10504 [D loss: 0.191714, acc.: 72.66%] [G loss: 0.469327]\n",
      "epoch:11 step:10505 [D loss: 0.192866, acc.: 69.53%] [G loss: 0.489787]\n",
      "epoch:11 step:10506 [D loss: 0.233748, acc.: 59.38%] [G loss: 0.489932]\n",
      "epoch:11 step:10507 [D loss: 0.250436, acc.: 57.03%] [G loss: 0.445554]\n",
      "epoch:11 step:10508 [D loss: 0.251397, acc.: 51.56%] [G loss: 0.383107]\n",
      "epoch:11 step:10509 [D loss: 0.213768, acc.: 66.41%] [G loss: 0.454152]\n",
      "epoch:11 step:10510 [D loss: 0.299453, acc.: 50.78%] [G loss: 0.404561]\n",
      "epoch:11 step:10511 [D loss: 0.203112, acc.: 70.31%] [G loss: 0.448650]\n",
      "epoch:11 step:10512 [D loss: 0.223842, acc.: 66.41%] [G loss: 0.493976]\n",
      "epoch:11 step:10513 [D loss: 0.226473, acc.: 68.75%] [G loss: 0.451081]\n",
      "epoch:11 step:10514 [D loss: 0.189142, acc.: 70.31%] [G loss: 0.522847]\n",
      "epoch:11 step:10515 [D loss: 0.184377, acc.: 78.12%] [G loss: 0.535674]\n",
      "epoch:11 step:10516 [D loss: 0.205863, acc.: 69.53%] [G loss: 0.467108]\n",
      "epoch:11 step:10517 [D loss: 0.271314, acc.: 50.00%] [G loss: 0.428091]\n",
      "epoch:11 step:10518 [D loss: 0.259185, acc.: 51.56%] [G loss: 0.409439]\n",
      "epoch:11 step:10519 [D loss: 0.255203, acc.: 57.81%] [G loss: 0.404462]\n",
      "epoch:11 step:10520 [D loss: 0.232525, acc.: 59.38%] [G loss: 0.426542]\n",
      "epoch:11 step:10521 [D loss: 0.255245, acc.: 55.47%] [G loss: 0.450263]\n",
      "epoch:11 step:10522 [D loss: 0.247059, acc.: 58.59%] [G loss: 0.446221]\n",
      "epoch:11 step:10523 [D loss: 0.224620, acc.: 67.97%] [G loss: 0.477796]\n",
      "epoch:11 step:10524 [D loss: 0.237781, acc.: 63.28%] [G loss: 0.466724]\n",
      "epoch:11 step:10525 [D loss: 0.206778, acc.: 69.53%] [G loss: 0.431514]\n",
      "epoch:11 step:10526 [D loss: 0.191506, acc.: 73.44%] [G loss: 0.506629]\n",
      "epoch:11 step:10527 [D loss: 0.279796, acc.: 50.00%] [G loss: 0.443329]\n",
      "epoch:11 step:10528 [D loss: 0.222038, acc.: 57.81%] [G loss: 0.481194]\n",
      "epoch:11 step:10529 [D loss: 0.190318, acc.: 76.56%] [G loss: 0.501672]\n",
      "epoch:11 step:10530 [D loss: 0.216295, acc.: 64.84%] [G loss: 0.481872]\n",
      "epoch:11 step:10531 [D loss: 0.264676, acc.: 53.91%] [G loss: 0.433075]\n",
      "epoch:11 step:10532 [D loss: 0.244312, acc.: 61.72%] [G loss: 0.429291]\n",
      "epoch:11 step:10533 [D loss: 0.256926, acc.: 52.34%] [G loss: 0.436437]\n",
      "epoch:11 step:10534 [D loss: 0.224947, acc.: 61.72%] [G loss: 0.449454]\n",
      "epoch:11 step:10535 [D loss: 0.223509, acc.: 62.50%] [G loss: 0.453970]\n",
      "epoch:11 step:10536 [D loss: 0.195398, acc.: 68.75%] [G loss: 0.444825]\n",
      "epoch:11 step:10537 [D loss: 0.187019, acc.: 71.88%] [G loss: 0.495618]\n",
      "epoch:11 step:10538 [D loss: 0.174590, acc.: 73.44%] [G loss: 0.482810]\n",
      "epoch:11 step:10539 [D loss: 0.189592, acc.: 71.09%] [G loss: 0.534759]\n",
      "epoch:11 step:10540 [D loss: 0.262252, acc.: 57.03%] [G loss: 0.510254]\n",
      "epoch:11 step:10541 [D loss: 0.229006, acc.: 63.28%] [G loss: 0.425598]\n",
      "epoch:11 step:10542 [D loss: 0.216950, acc.: 66.41%] [G loss: 0.466995]\n",
      "epoch:11 step:10543 [D loss: 0.202294, acc.: 70.31%] [G loss: 0.423664]\n",
      "epoch:11 step:10544 [D loss: 0.222485, acc.: 64.84%] [G loss: 0.441215]\n",
      "epoch:11 step:10545 [D loss: 0.231608, acc.: 67.97%] [G loss: 0.415644]\n",
      "epoch:11 step:10546 [D loss: 0.209416, acc.: 67.97%] [G loss: 0.496445]\n",
      "epoch:11 step:10547 [D loss: 0.229150, acc.: 60.94%] [G loss: 0.413242]\n",
      "epoch:11 step:10548 [D loss: 0.219511, acc.: 60.16%] [G loss: 0.435056]\n",
      "epoch:11 step:10549 [D loss: 0.210354, acc.: 64.06%] [G loss: 0.484459]\n",
      "epoch:11 step:10550 [D loss: 0.213935, acc.: 67.19%] [G loss: 0.461128]\n",
      "epoch:11 step:10551 [D loss: 0.210716, acc.: 71.88%] [G loss: 0.477181]\n",
      "epoch:11 step:10552 [D loss: 0.242128, acc.: 60.94%] [G loss: 0.462213]\n",
      "epoch:11 step:10553 [D loss: 0.229602, acc.: 61.72%] [G loss: 0.502249]\n",
      "epoch:11 step:10554 [D loss: 0.237926, acc.: 60.94%] [G loss: 0.416515]\n",
      "epoch:11 step:10555 [D loss: 0.212504, acc.: 66.41%] [G loss: 0.440202]\n",
      "epoch:11 step:10556 [D loss: 0.251745, acc.: 58.59%] [G loss: 0.500431]\n",
      "epoch:11 step:10557 [D loss: 0.265670, acc.: 50.78%] [G loss: 0.419371]\n",
      "epoch:11 step:10558 [D loss: 0.241846, acc.: 58.59%] [G loss: 0.441721]\n",
      "epoch:11 step:10559 [D loss: 0.213837, acc.: 62.50%] [G loss: 0.458835]\n",
      "epoch:11 step:10560 [D loss: 0.212821, acc.: 69.53%] [G loss: 0.436455]\n",
      "epoch:11 step:10561 [D loss: 0.242241, acc.: 61.72%] [G loss: 0.433501]\n",
      "epoch:11 step:10562 [D loss: 0.228955, acc.: 58.59%] [G loss: 0.446925]\n",
      "epoch:11 step:10563 [D loss: 0.249190, acc.: 59.38%] [G loss: 0.433594]\n",
      "epoch:11 step:10564 [D loss: 0.249409, acc.: 57.03%] [G loss: 0.449310]\n",
      "epoch:11 step:10565 [D loss: 0.233302, acc.: 58.59%] [G loss: 0.447012]\n",
      "epoch:11 step:10566 [D loss: 0.184811, acc.: 75.00%] [G loss: 0.466286]\n",
      "epoch:11 step:10567 [D loss: 0.230061, acc.: 57.81%] [G loss: 0.479469]\n",
      "epoch:11 step:10568 [D loss: 0.217645, acc.: 64.84%] [G loss: 0.473136]\n",
      "epoch:11 step:10569 [D loss: 0.208839, acc.: 67.97%] [G loss: 0.474055]\n",
      "epoch:11 step:10570 [D loss: 0.258403, acc.: 53.91%] [G loss: 0.446540]\n",
      "epoch:11 step:10571 [D loss: 0.229142, acc.: 60.16%] [G loss: 0.459350]\n",
      "epoch:11 step:10572 [D loss: 0.235756, acc.: 63.28%] [G loss: 0.426157]\n",
      "epoch:11 step:10573 [D loss: 0.260148, acc.: 55.47%] [G loss: 0.451137]\n",
      "epoch:11 step:10574 [D loss: 0.215494, acc.: 70.31%] [G loss: 0.417202]\n",
      "epoch:11 step:10575 [D loss: 0.224372, acc.: 63.28%] [G loss: 0.431429]\n",
      "epoch:11 step:10576 [D loss: 0.201509, acc.: 74.22%] [G loss: 0.453273]\n",
      "epoch:11 step:10577 [D loss: 0.222211, acc.: 61.72%] [G loss: 0.422220]\n",
      "epoch:11 step:10578 [D loss: 0.222163, acc.: 64.84%] [G loss: 0.438813]\n",
      "epoch:11 step:10579 [D loss: 0.211669, acc.: 66.41%] [G loss: 0.453436]\n",
      "epoch:11 step:10580 [D loss: 0.218155, acc.: 67.19%] [G loss: 0.429864]\n",
      "epoch:11 step:10581 [D loss: 0.200396, acc.: 72.66%] [G loss: 0.485329]\n",
      "epoch:11 step:10582 [D loss: 0.226802, acc.: 64.06%] [G loss: 0.455435]\n",
      "epoch:11 step:10583 [D loss: 0.233583, acc.: 64.06%] [G loss: 0.493042]\n",
      "epoch:11 step:10584 [D loss: 0.243860, acc.: 59.38%] [G loss: 0.467330]\n",
      "epoch:11 step:10585 [D loss: 0.235350, acc.: 54.69%] [G loss: 0.429706]\n",
      "epoch:11 step:10586 [D loss: 0.216567, acc.: 63.28%] [G loss: 0.442981]\n",
      "epoch:11 step:10587 [D loss: 0.212532, acc.: 68.75%] [G loss: 0.475413]\n",
      "epoch:11 step:10588 [D loss: 0.270811, acc.: 54.69%] [G loss: 0.435559]\n",
      "epoch:11 step:10589 [D loss: 0.242513, acc.: 54.69%] [G loss: 0.416083]\n",
      "epoch:11 step:10590 [D loss: 0.210036, acc.: 71.09%] [G loss: 0.437288]\n",
      "epoch:11 step:10591 [D loss: 0.254247, acc.: 54.69%] [G loss: 0.395425]\n",
      "epoch:11 step:10592 [D loss: 0.223223, acc.: 57.81%] [G loss: 0.450706]\n",
      "epoch:11 step:10593 [D loss: 0.201981, acc.: 66.41%] [G loss: 0.432864]\n",
      "epoch:11 step:10594 [D loss: 0.223385, acc.: 64.06%] [G loss: 0.450438]\n",
      "epoch:11 step:10595 [D loss: 0.219334, acc.: 64.84%] [G loss: 0.454957]\n",
      "epoch:11 step:10596 [D loss: 0.229772, acc.: 63.28%] [G loss: 0.461384]\n",
      "epoch:11 step:10597 [D loss: 0.224493, acc.: 54.69%] [G loss: 0.466071]\n",
      "epoch:11 step:10598 [D loss: 0.226185, acc.: 67.97%] [G loss: 0.462254]\n",
      "epoch:11 step:10599 [D loss: 0.225246, acc.: 62.50%] [G loss: 0.470995]\n",
      "epoch:11 step:10600 [D loss: 0.232218, acc.: 56.25%] [G loss: 0.456324]\n",
      "##############\n",
      "[2.58133274 1.71910758 6.3992019  4.76955787 3.78133075 5.73916754\n",
      " 4.59620058 4.69882402 4.58073277 3.9627381 ]\n",
      "##########\n",
      "epoch:11 step:10601 [D loss: 0.226646, acc.: 64.84%] [G loss: 0.429184]\n",
      "epoch:11 step:10602 [D loss: 0.244178, acc.: 55.47%] [G loss: 0.430099]\n",
      "epoch:11 step:10603 [D loss: 0.198860, acc.: 71.09%] [G loss: 0.472288]\n",
      "epoch:11 step:10604 [D loss: 0.198207, acc.: 75.00%] [G loss: 0.461515]\n",
      "epoch:11 step:10605 [D loss: 0.203401, acc.: 68.75%] [G loss: 0.454973]\n",
      "epoch:11 step:10606 [D loss: 0.211928, acc.: 64.84%] [G loss: 0.452915]\n",
      "epoch:11 step:10607 [D loss: 0.209365, acc.: 67.97%] [G loss: 0.466015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10608 [D loss: 0.263387, acc.: 52.34%] [G loss: 0.430386]\n",
      "epoch:11 step:10609 [D loss: 0.231103, acc.: 60.16%] [G loss: 0.459720]\n",
      "epoch:11 step:10610 [D loss: 0.212935, acc.: 67.19%] [G loss: 0.481631]\n",
      "epoch:11 step:10611 [D loss: 0.214784, acc.: 67.97%] [G loss: 0.420569]\n",
      "epoch:11 step:10612 [D loss: 0.203468, acc.: 69.53%] [G loss: 0.499428]\n",
      "epoch:11 step:10613 [D loss: 0.206440, acc.: 64.84%] [G loss: 0.503785]\n",
      "epoch:11 step:10614 [D loss: 0.228354, acc.: 61.72%] [G loss: 0.456718]\n",
      "epoch:11 step:10615 [D loss: 0.243466, acc.: 61.72%] [G loss: 0.448017]\n",
      "epoch:11 step:10616 [D loss: 0.185730, acc.: 67.97%] [G loss: 0.480607]\n",
      "epoch:11 step:10617 [D loss: 0.219987, acc.: 66.41%] [G loss: 0.430784]\n",
      "epoch:11 step:10618 [D loss: 0.203230, acc.: 71.88%] [G loss: 0.498791]\n",
      "epoch:11 step:10619 [D loss: 0.157506, acc.: 76.56%] [G loss: 0.540434]\n",
      "epoch:11 step:10620 [D loss: 0.174521, acc.: 74.22%] [G loss: 0.540348]\n",
      "epoch:11 step:10621 [D loss: 0.181971, acc.: 68.75%] [G loss: 0.517550]\n",
      "epoch:11 step:10622 [D loss: 0.203026, acc.: 67.97%] [G loss: 0.510898]\n",
      "epoch:11 step:10623 [D loss: 0.291963, acc.: 49.22%] [G loss: 0.426728]\n",
      "epoch:11 step:10624 [D loss: 0.237103, acc.: 60.94%] [G loss: 0.427416]\n",
      "epoch:11 step:10625 [D loss: 0.199622, acc.: 68.75%] [G loss: 0.470637]\n",
      "epoch:11 step:10626 [D loss: 0.206526, acc.: 68.75%] [G loss: 0.472468]\n",
      "epoch:11 step:10627 [D loss: 0.199662, acc.: 65.62%] [G loss: 0.489583]\n",
      "epoch:11 step:10628 [D loss: 0.190185, acc.: 68.75%] [G loss: 0.493629]\n",
      "epoch:11 step:10629 [D loss: 0.188620, acc.: 69.53%] [G loss: 0.493659]\n",
      "epoch:11 step:10630 [D loss: 0.301379, acc.: 45.31%] [G loss: 0.434786]\n",
      "epoch:11 step:10631 [D loss: 0.200495, acc.: 68.75%] [G loss: 0.440245]\n",
      "epoch:11 step:10632 [D loss: 0.221812, acc.: 60.94%] [G loss: 0.452060]\n",
      "epoch:11 step:10633 [D loss: 0.211869, acc.: 63.28%] [G loss: 0.430167]\n",
      "epoch:11 step:10634 [D loss: 0.220771, acc.: 61.72%] [G loss: 0.481190]\n",
      "epoch:11 step:10635 [D loss: 0.224992, acc.: 64.06%] [G loss: 0.472317]\n",
      "epoch:11 step:10636 [D loss: 0.224298, acc.: 62.50%] [G loss: 0.457238]\n",
      "epoch:11 step:10637 [D loss: 0.213291, acc.: 67.97%] [G loss: 0.464043]\n",
      "epoch:11 step:10638 [D loss: 0.220917, acc.: 61.72%] [G loss: 0.387483]\n",
      "epoch:11 step:10639 [D loss: 0.201040, acc.: 68.75%] [G loss: 0.461973]\n",
      "epoch:11 step:10640 [D loss: 0.204359, acc.: 67.19%] [G loss: 0.460946]\n",
      "epoch:11 step:10641 [D loss: 0.214952, acc.: 64.06%] [G loss: 0.452456]\n",
      "epoch:11 step:10642 [D loss: 0.252980, acc.: 54.69%] [G loss: 0.488141]\n",
      "epoch:11 step:10643 [D loss: 0.187357, acc.: 73.44%] [G loss: 0.519441]\n",
      "epoch:11 step:10644 [D loss: 0.211221, acc.: 65.62%] [G loss: 0.527278]\n",
      "epoch:11 step:10645 [D loss: 0.219446, acc.: 69.53%] [G loss: 0.448484]\n",
      "epoch:11 step:10646 [D loss: 0.189097, acc.: 73.44%] [G loss: 0.485010]\n",
      "epoch:11 step:10647 [D loss: 0.215167, acc.: 65.62%] [G loss: 0.448829]\n",
      "epoch:11 step:10648 [D loss: 0.286126, acc.: 54.69%] [G loss: 0.433625]\n",
      "epoch:11 step:10649 [D loss: 0.234483, acc.: 59.38%] [G loss: 0.460512]\n",
      "epoch:11 step:10650 [D loss: 0.188140, acc.: 75.00%] [G loss: 0.461639]\n",
      "epoch:11 step:10651 [D loss: 0.191693, acc.: 70.31%] [G loss: 0.457918]\n",
      "epoch:11 step:10652 [D loss: 0.229476, acc.: 59.38%] [G loss: 0.472416]\n",
      "epoch:11 step:10653 [D loss: 0.198229, acc.: 69.53%] [G loss: 0.527590]\n",
      "epoch:11 step:10654 [D loss: 0.190877, acc.: 73.44%] [G loss: 0.548182]\n",
      "epoch:11 step:10655 [D loss: 0.276124, acc.: 56.25%] [G loss: 0.464088]\n",
      "epoch:11 step:10656 [D loss: 0.243685, acc.: 57.81%] [G loss: 0.432433]\n",
      "epoch:11 step:10657 [D loss: 0.205845, acc.: 65.62%] [G loss: 0.448667]\n",
      "epoch:11 step:10658 [D loss: 0.225928, acc.: 61.72%] [G loss: 0.475366]\n",
      "epoch:11 step:10659 [D loss: 0.236655, acc.: 57.03%] [G loss: 0.434288]\n",
      "epoch:11 step:10660 [D loss: 0.205524, acc.: 66.41%] [G loss: 0.460817]\n",
      "epoch:11 step:10661 [D loss: 0.154462, acc.: 76.56%] [G loss: 0.490436]\n",
      "epoch:11 step:10662 [D loss: 0.230105, acc.: 63.28%] [G loss: 0.455033]\n",
      "epoch:11 step:10663 [D loss: 0.227121, acc.: 67.97%] [G loss: 0.421383]\n",
      "epoch:11 step:10664 [D loss: 0.195846, acc.: 71.88%] [G loss: 0.455260]\n",
      "epoch:11 step:10665 [D loss: 0.179843, acc.: 74.22%] [G loss: 0.483453]\n",
      "epoch:11 step:10666 [D loss: 0.170697, acc.: 77.34%] [G loss: 0.487708]\n",
      "epoch:11 step:10667 [D loss: 0.206572, acc.: 69.53%] [G loss: 0.453105]\n",
      "epoch:11 step:10668 [D loss: 0.204517, acc.: 68.75%] [G loss: 0.483000]\n",
      "epoch:11 step:10669 [D loss: 0.219177, acc.: 63.28%] [G loss: 0.446879]\n",
      "epoch:11 step:10670 [D loss: 0.219301, acc.: 62.50%] [G loss: 0.452073]\n",
      "epoch:11 step:10671 [D loss: 0.199210, acc.: 64.06%] [G loss: 0.443400]\n",
      "epoch:11 step:10672 [D loss: 0.217042, acc.: 65.62%] [G loss: 0.482250]\n",
      "epoch:11 step:10673 [D loss: 0.210181, acc.: 65.62%] [G loss: 0.459894]\n",
      "epoch:11 step:10674 [D loss: 0.209315, acc.: 64.84%] [G loss: 0.446717]\n",
      "epoch:11 step:10675 [D loss: 0.254169, acc.: 57.03%] [G loss: 0.390803]\n",
      "epoch:11 step:10676 [D loss: 0.241750, acc.: 58.59%] [G loss: 0.432516]\n",
      "epoch:11 step:10677 [D loss: 0.184228, acc.: 71.88%] [G loss: 0.475393]\n",
      "epoch:11 step:10678 [D loss: 0.228049, acc.: 58.59%] [G loss: 0.500312]\n",
      "epoch:11 step:10679 [D loss: 0.222307, acc.: 65.62%] [G loss: 0.470513]\n",
      "epoch:11 step:10680 [D loss: 0.238699, acc.: 59.38%] [G loss: 0.479612]\n",
      "epoch:11 step:10681 [D loss: 0.192328, acc.: 67.97%] [G loss: 0.505500]\n",
      "epoch:11 step:10682 [D loss: 0.247306, acc.: 59.38%] [G loss: 0.421879]\n",
      "epoch:11 step:10683 [D loss: 0.268825, acc.: 50.78%] [G loss: 0.409653]\n",
      "epoch:11 step:10684 [D loss: 0.257524, acc.: 51.56%] [G loss: 0.421072]\n",
      "epoch:11 step:10685 [D loss: 0.240587, acc.: 60.16%] [G loss: 0.446454]\n",
      "epoch:11 step:10686 [D loss: 0.210058, acc.: 67.19%] [G loss: 0.435540]\n",
      "epoch:11 step:10687 [D loss: 0.228529, acc.: 60.94%] [G loss: 0.407771]\n",
      "epoch:11 step:10688 [D loss: 0.214347, acc.: 67.97%] [G loss: 0.478220]\n",
      "epoch:11 step:10689 [D loss: 0.206361, acc.: 65.62%] [G loss: 0.448896]\n",
      "epoch:11 step:10690 [D loss: 0.236434, acc.: 62.50%] [G loss: 0.454635]\n",
      "epoch:11 step:10691 [D loss: 0.209178, acc.: 70.31%] [G loss: 0.441853]\n",
      "epoch:11 step:10692 [D loss: 0.207498, acc.: 62.50%] [G loss: 0.491698]\n",
      "epoch:11 step:10693 [D loss: 0.216797, acc.: 66.41%] [G loss: 0.447216]\n",
      "epoch:11 step:10694 [D loss: 0.222080, acc.: 64.06%] [G loss: 0.442016]\n",
      "epoch:11 step:10695 [D loss: 0.208458, acc.: 60.94%] [G loss: 0.451692]\n",
      "epoch:11 step:10696 [D loss: 0.235384, acc.: 61.72%] [G loss: 0.518524]\n",
      "epoch:11 step:10697 [D loss: 0.238867, acc.: 58.59%] [G loss: 0.427564]\n",
      "epoch:11 step:10698 [D loss: 0.214223, acc.: 62.50%] [G loss: 0.450725]\n",
      "epoch:11 step:10699 [D loss: 0.227275, acc.: 61.72%] [G loss: 0.449036]\n",
      "epoch:11 step:10700 [D loss: 0.256983, acc.: 53.91%] [G loss: 0.449572]\n",
      "epoch:11 step:10701 [D loss: 0.208773, acc.: 64.84%] [G loss: 0.485232]\n",
      "epoch:11 step:10702 [D loss: 0.237430, acc.: 54.69%] [G loss: 0.536092]\n",
      "epoch:11 step:10703 [D loss: 0.267731, acc.: 46.09%] [G loss: 0.419135]\n",
      "epoch:11 step:10704 [D loss: 0.214533, acc.: 71.09%] [G loss: 0.443811]\n",
      "epoch:11 step:10705 [D loss: 0.194880, acc.: 69.53%] [G loss: 0.464292]\n",
      "epoch:11 step:10706 [D loss: 0.229133, acc.: 65.62%] [G loss: 0.477931]\n",
      "epoch:11 step:10707 [D loss: 0.236841, acc.: 60.94%] [G loss: 0.452791]\n",
      "epoch:11 step:10708 [D loss: 0.233550, acc.: 60.94%] [G loss: 0.406818]\n",
      "epoch:11 step:10709 [D loss: 0.216866, acc.: 64.06%] [G loss: 0.450455]\n",
      "epoch:11 step:10710 [D loss: 0.217652, acc.: 60.94%] [G loss: 0.466709]\n",
      "epoch:11 step:10711 [D loss: 0.233226, acc.: 59.38%] [G loss: 0.441554]\n",
      "epoch:11 step:10712 [D loss: 0.213108, acc.: 68.75%] [G loss: 0.464451]\n",
      "epoch:11 step:10713 [D loss: 0.211669, acc.: 67.19%] [G loss: 0.481842]\n",
      "epoch:11 step:10714 [D loss: 0.256110, acc.: 59.38%] [G loss: 0.491378]\n",
      "epoch:11 step:10715 [D loss: 0.240645, acc.: 60.16%] [G loss: 0.488155]\n",
      "epoch:11 step:10716 [D loss: 0.229323, acc.: 56.25%] [G loss: 0.426958]\n",
      "epoch:11 step:10717 [D loss: 0.259206, acc.: 53.91%] [G loss: 0.415320]\n",
      "epoch:11 step:10718 [D loss: 0.240353, acc.: 58.59%] [G loss: 0.474379]\n",
      "epoch:11 step:10719 [D loss: 0.219296, acc.: 64.06%] [G loss: 0.435091]\n",
      "epoch:11 step:10720 [D loss: 0.261841, acc.: 57.03%] [G loss: 0.420581]\n",
      "epoch:11 step:10721 [D loss: 0.214266, acc.: 63.28%] [G loss: 0.413651]\n",
      "epoch:11 step:10722 [D loss: 0.223108, acc.: 64.06%] [G loss: 0.453656]\n",
      "epoch:11 step:10723 [D loss: 0.190002, acc.: 70.31%] [G loss: 0.481098]\n",
      "epoch:11 step:10724 [D loss: 0.229871, acc.: 57.81%] [G loss: 0.460832]\n",
      "epoch:11 step:10725 [D loss: 0.235536, acc.: 62.50%] [G loss: 0.421144]\n",
      "epoch:11 step:10726 [D loss: 0.220497, acc.: 66.41%] [G loss: 0.448155]\n",
      "epoch:11 step:10727 [D loss: 0.230564, acc.: 57.81%] [G loss: 0.417543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10728 [D loss: 0.258760, acc.: 57.81%] [G loss: 0.427819]\n",
      "epoch:11 step:10729 [D loss: 0.233874, acc.: 60.94%] [G loss: 0.421856]\n",
      "epoch:11 step:10730 [D loss: 0.226724, acc.: 62.50%] [G loss: 0.447905]\n",
      "epoch:11 step:10731 [D loss: 0.232721, acc.: 60.94%] [G loss: 0.428096]\n",
      "epoch:11 step:10732 [D loss: 0.226065, acc.: 61.72%] [G loss: 0.427109]\n",
      "epoch:11 step:10733 [D loss: 0.213807, acc.: 66.41%] [G loss: 0.441687]\n",
      "epoch:11 step:10734 [D loss: 0.214868, acc.: 65.62%] [G loss: 0.463879]\n",
      "epoch:11 step:10735 [D loss: 0.181680, acc.: 77.34%] [G loss: 0.482290]\n",
      "epoch:11 step:10736 [D loss: 0.215177, acc.: 64.84%] [G loss: 0.481701]\n",
      "epoch:11 step:10737 [D loss: 0.193117, acc.: 75.78%] [G loss: 0.507900]\n",
      "epoch:11 step:10738 [D loss: 0.224053, acc.: 64.06%] [G loss: 0.462843]\n",
      "epoch:11 step:10739 [D loss: 0.236046, acc.: 61.72%] [G loss: 0.450283]\n",
      "epoch:11 step:10740 [D loss: 0.211732, acc.: 64.06%] [G loss: 0.434804]\n",
      "epoch:11 step:10741 [D loss: 0.247675, acc.: 59.38%] [G loss: 0.441059]\n",
      "epoch:11 step:10742 [D loss: 0.236084, acc.: 64.84%] [G loss: 0.457335]\n",
      "epoch:11 step:10743 [D loss: 0.197583, acc.: 70.31%] [G loss: 0.494724]\n",
      "epoch:11 step:10744 [D loss: 0.281564, acc.: 53.12%] [G loss: 0.448136]\n",
      "epoch:11 step:10745 [D loss: 0.230567, acc.: 60.16%] [G loss: 0.421720]\n",
      "epoch:11 step:10746 [D loss: 0.207145, acc.: 71.09%] [G loss: 0.468106]\n",
      "epoch:11 step:10747 [D loss: 0.204758, acc.: 67.97%] [G loss: 0.468915]\n",
      "epoch:11 step:10748 [D loss: 0.217592, acc.: 65.62%] [G loss: 0.486640]\n",
      "epoch:11 step:10749 [D loss: 0.228704, acc.: 59.38%] [G loss: 0.502877]\n",
      "epoch:11 step:10750 [D loss: 0.251972, acc.: 60.94%] [G loss: 0.456808]\n",
      "epoch:11 step:10751 [D loss: 0.254223, acc.: 53.12%] [G loss: 0.433472]\n",
      "epoch:11 step:10752 [D loss: 0.229323, acc.: 62.50%] [G loss: 0.467394]\n",
      "epoch:11 step:10753 [D loss: 0.202739, acc.: 64.06%] [G loss: 0.502738]\n",
      "epoch:11 step:10754 [D loss: 0.217253, acc.: 70.31%] [G loss: 0.469541]\n",
      "epoch:11 step:10755 [D loss: 0.249800, acc.: 54.69%] [G loss: 0.436590]\n",
      "epoch:11 step:10756 [D loss: 0.242958, acc.: 60.16%] [G loss: 0.423296]\n",
      "epoch:11 step:10757 [D loss: 0.224798, acc.: 66.41%] [G loss: 0.473342]\n",
      "epoch:11 step:10758 [D loss: 0.198794, acc.: 69.53%] [G loss: 0.467040]\n",
      "epoch:11 step:10759 [D loss: 0.186565, acc.: 72.66%] [G loss: 0.512396]\n",
      "epoch:11 step:10760 [D loss: 0.225391, acc.: 67.19%] [G loss: 0.447277]\n",
      "epoch:11 step:10761 [D loss: 0.208545, acc.: 66.41%] [G loss: 0.458541]\n",
      "epoch:11 step:10762 [D loss: 0.240695, acc.: 64.84%] [G loss: 0.446407]\n",
      "epoch:11 step:10763 [D loss: 0.240286, acc.: 59.38%] [G loss: 0.457323]\n",
      "epoch:11 step:10764 [D loss: 0.224832, acc.: 67.19%] [G loss: 0.486767]\n",
      "epoch:11 step:10765 [D loss: 0.300855, acc.: 49.22%] [G loss: 0.445743]\n",
      "epoch:11 step:10766 [D loss: 0.248244, acc.: 56.25%] [G loss: 0.418833]\n",
      "epoch:11 step:10767 [D loss: 0.246133, acc.: 59.38%] [G loss: 0.402713]\n",
      "epoch:11 step:10768 [D loss: 0.262068, acc.: 51.56%] [G loss: 0.437420]\n",
      "epoch:11 step:10769 [D loss: 0.253715, acc.: 53.91%] [G loss: 0.427531]\n",
      "epoch:11 step:10770 [D loss: 0.243142, acc.: 54.69%] [G loss: 0.410494]\n",
      "epoch:11 step:10771 [D loss: 0.218167, acc.: 66.41%] [G loss: 0.419846]\n",
      "epoch:11 step:10772 [D loss: 0.252607, acc.: 54.69%] [G loss: 0.390319]\n",
      "epoch:11 step:10773 [D loss: 0.225222, acc.: 63.28%] [G loss: 0.435340]\n",
      "epoch:11 step:10774 [D loss: 0.234450, acc.: 63.28%] [G loss: 0.439480]\n",
      "epoch:11 step:10775 [D loss: 0.229439, acc.: 60.16%] [G loss: 0.441046]\n",
      "epoch:11 step:10776 [D loss: 0.197138, acc.: 68.75%] [G loss: 0.462553]\n",
      "epoch:11 step:10777 [D loss: 0.210257, acc.: 67.19%] [G loss: 0.508685]\n",
      "epoch:11 step:10778 [D loss: 0.182894, acc.: 72.66%] [G loss: 0.528881]\n",
      "epoch:11 step:10779 [D loss: 0.202579, acc.: 68.75%] [G loss: 0.509073]\n",
      "epoch:11 step:10780 [D loss: 0.276774, acc.: 47.66%] [G loss: 0.448379]\n",
      "epoch:11 step:10781 [D loss: 0.209499, acc.: 63.28%] [G loss: 0.478015]\n",
      "epoch:11 step:10782 [D loss: 0.198221, acc.: 69.53%] [G loss: 0.480718]\n",
      "epoch:11 step:10783 [D loss: 0.233461, acc.: 60.94%] [G loss: 0.474475]\n",
      "epoch:11 step:10784 [D loss: 0.275817, acc.: 46.09%] [G loss: 0.453312]\n",
      "epoch:11 step:10785 [D loss: 0.240048, acc.: 56.25%] [G loss: 0.428792]\n",
      "epoch:11 step:10786 [D loss: 0.218416, acc.: 63.28%] [G loss: 0.426881]\n",
      "epoch:11 step:10787 [D loss: 0.224585, acc.: 62.50%] [G loss: 0.423675]\n",
      "epoch:11 step:10788 [D loss: 0.182581, acc.: 74.22%] [G loss: 0.439959]\n",
      "epoch:11 step:10789 [D loss: 0.256499, acc.: 51.56%] [G loss: 0.421376]\n",
      "epoch:11 step:10790 [D loss: 0.234651, acc.: 55.47%] [G loss: 0.387476]\n",
      "epoch:11 step:10791 [D loss: 0.206314, acc.: 67.19%] [G loss: 0.498505]\n",
      "epoch:11 step:10792 [D loss: 0.218272, acc.: 66.41%] [G loss: 0.457194]\n",
      "epoch:11 step:10793 [D loss: 0.220635, acc.: 65.62%] [G loss: 0.431726]\n",
      "epoch:11 step:10794 [D loss: 0.248044, acc.: 55.47%] [G loss: 0.414580]\n",
      "epoch:11 step:10795 [D loss: 0.177378, acc.: 74.22%] [G loss: 0.441245]\n",
      "epoch:11 step:10796 [D loss: 0.257196, acc.: 57.03%] [G loss: 0.419662]\n",
      "epoch:11 step:10797 [D loss: 0.228211, acc.: 64.84%] [G loss: 0.457420]\n",
      "epoch:11 step:10798 [D loss: 0.237896, acc.: 60.16%] [G loss: 0.430218]\n",
      "epoch:11 step:10799 [D loss: 0.242941, acc.: 54.69%] [G loss: 0.463413]\n",
      "epoch:11 step:10800 [D loss: 0.217153, acc.: 62.50%] [G loss: 0.430768]\n",
      "##############\n",
      "[2.4141918  1.678371   6.27477589 4.57449924 3.79382339 5.79168848\n",
      " 4.54291886 4.7258896  4.42682879 3.93337498]\n",
      "##########\n",
      "epoch:11 step:10801 [D loss: 0.219284, acc.: 68.75%] [G loss: 0.463667]\n",
      "epoch:11 step:10802 [D loss: 0.200044, acc.: 67.19%] [G loss: 0.468101]\n",
      "epoch:11 step:10803 [D loss: 0.208973, acc.: 73.44%] [G loss: 0.488823]\n",
      "epoch:11 step:10804 [D loss: 0.232009, acc.: 56.25%] [G loss: 0.469937]\n",
      "epoch:11 step:10805 [D loss: 0.217172, acc.: 64.84%] [G loss: 0.506616]\n",
      "epoch:11 step:10806 [D loss: 0.185833, acc.: 74.22%] [G loss: 0.532958]\n",
      "epoch:11 step:10807 [D loss: 0.275299, acc.: 53.12%] [G loss: 0.444942]\n",
      "epoch:11 step:10808 [D loss: 0.279111, acc.: 53.91%] [G loss: 0.397615]\n",
      "epoch:11 step:10809 [D loss: 0.237181, acc.: 60.16%] [G loss: 0.408596]\n",
      "epoch:11 step:10810 [D loss: 0.204538, acc.: 68.75%] [G loss: 0.466218]\n",
      "epoch:11 step:10811 [D loss: 0.194881, acc.: 69.53%] [G loss: 0.495837]\n",
      "epoch:11 step:10812 [D loss: 0.188383, acc.: 67.19%] [G loss: 0.501557]\n",
      "epoch:11 step:10813 [D loss: 0.240177, acc.: 58.59%] [G loss: 0.486181]\n",
      "epoch:11 step:10814 [D loss: 0.226670, acc.: 61.72%] [G loss: 0.454741]\n",
      "epoch:11 step:10815 [D loss: 0.199473, acc.: 71.09%] [G loss: 0.489433]\n",
      "epoch:11 step:10816 [D loss: 0.235244, acc.: 64.06%] [G loss: 0.448791]\n",
      "epoch:11 step:10817 [D loss: 0.223488, acc.: 64.84%] [G loss: 0.474790]\n",
      "epoch:11 step:10818 [D loss: 0.246345, acc.: 60.16%] [G loss: 0.421959]\n",
      "epoch:11 step:10819 [D loss: 0.233628, acc.: 59.38%] [G loss: 0.446973]\n",
      "epoch:11 step:10820 [D loss: 0.229510, acc.: 62.50%] [G loss: 0.487920]\n",
      "epoch:11 step:10821 [D loss: 0.210204, acc.: 61.72%] [G loss: 0.466178]\n",
      "epoch:11 step:10822 [D loss: 0.202003, acc.: 75.78%] [G loss: 0.469694]\n",
      "epoch:11 step:10823 [D loss: 0.211532, acc.: 66.41%] [G loss: 0.470454]\n",
      "epoch:11 step:10824 [D loss: 0.230024, acc.: 64.84%] [G loss: 0.447347]\n",
      "epoch:11 step:10825 [D loss: 0.251472, acc.: 57.03%] [G loss: 0.461393]\n",
      "epoch:11 step:10826 [D loss: 0.227518, acc.: 59.38%] [G loss: 0.446495]\n",
      "epoch:11 step:10827 [D loss: 0.194107, acc.: 68.75%] [G loss: 0.480969]\n",
      "epoch:11 step:10828 [D loss: 0.225205, acc.: 64.06%] [G loss: 0.449043]\n",
      "epoch:11 step:10829 [D loss: 0.216481, acc.: 63.28%] [G loss: 0.468785]\n",
      "epoch:11 step:10830 [D loss: 0.231191, acc.: 63.28%] [G loss: 0.438482]\n",
      "epoch:11 step:10831 [D loss: 0.232953, acc.: 61.72%] [G loss: 0.437254]\n",
      "epoch:11 step:10832 [D loss: 0.221265, acc.: 60.94%] [G loss: 0.437094]\n",
      "epoch:11 step:10833 [D loss: 0.205153, acc.: 67.97%] [G loss: 0.463185]\n",
      "epoch:11 step:10834 [D loss: 0.240504, acc.: 61.72%] [G loss: 0.444964]\n",
      "epoch:11 step:10835 [D loss: 0.285489, acc.: 53.91%] [G loss: 0.434788]\n",
      "epoch:11 step:10836 [D loss: 0.243115, acc.: 59.38%] [G loss: 0.447555]\n",
      "epoch:11 step:10837 [D loss: 0.201860, acc.: 72.66%] [G loss: 0.497747]\n",
      "epoch:11 step:10838 [D loss: 0.238390, acc.: 58.59%] [G loss: 0.401701]\n",
      "epoch:11 step:10839 [D loss: 0.238957, acc.: 62.50%] [G loss: 0.425432]\n",
      "epoch:11 step:10840 [D loss: 0.235507, acc.: 62.50%] [G loss: 0.410356]\n",
      "epoch:11 step:10841 [D loss: 0.218533, acc.: 65.62%] [G loss: 0.477665]\n",
      "epoch:11 step:10842 [D loss: 0.224591, acc.: 63.28%] [G loss: 0.464063]\n",
      "epoch:11 step:10843 [D loss: 0.201314, acc.: 71.09%] [G loss: 0.435363]\n",
      "epoch:11 step:10844 [D loss: 0.235704, acc.: 59.38%] [G loss: 0.456567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10845 [D loss: 0.237449, acc.: 61.72%] [G loss: 0.484701]\n",
      "epoch:11 step:10846 [D loss: 0.224949, acc.: 62.50%] [G loss: 0.447772]\n",
      "epoch:11 step:10847 [D loss: 0.234434, acc.: 56.25%] [G loss: 0.435485]\n",
      "epoch:11 step:10848 [D loss: 0.212462, acc.: 62.50%] [G loss: 0.441796]\n",
      "epoch:11 step:10849 [D loss: 0.297768, acc.: 44.53%] [G loss: 0.402136]\n",
      "epoch:11 step:10850 [D loss: 0.232097, acc.: 59.38%] [G loss: 0.433991]\n",
      "epoch:11 step:10851 [D loss: 0.228533, acc.: 62.50%] [G loss: 0.389201]\n",
      "epoch:11 step:10852 [D loss: 0.238541, acc.: 56.25%] [G loss: 0.405395]\n",
      "epoch:11 step:10853 [D loss: 0.202881, acc.: 71.09%] [G loss: 0.434351]\n",
      "epoch:11 step:10854 [D loss: 0.226150, acc.: 60.16%] [G loss: 0.421373]\n",
      "epoch:11 step:10855 [D loss: 0.188884, acc.: 68.75%] [G loss: 0.463818]\n",
      "epoch:11 step:10856 [D loss: 0.184854, acc.: 72.66%] [G loss: 0.472139]\n",
      "epoch:11 step:10857 [D loss: 0.223610, acc.: 64.84%] [G loss: 0.443578]\n",
      "epoch:11 step:10858 [D loss: 0.205020, acc.: 65.62%] [G loss: 0.515730]\n",
      "epoch:11 step:10859 [D loss: 0.191887, acc.: 71.09%] [G loss: 0.492479]\n",
      "epoch:11 step:10860 [D loss: 0.228410, acc.: 64.06%] [G loss: 0.461969]\n",
      "epoch:11 step:10861 [D loss: 0.207604, acc.: 64.84%] [G loss: 0.432643]\n",
      "epoch:11 step:10862 [D loss: 0.207983, acc.: 67.19%] [G loss: 0.441010]\n",
      "epoch:11 step:10863 [D loss: 0.218634, acc.: 63.28%] [G loss: 0.444787]\n",
      "epoch:11 step:10864 [D loss: 0.185961, acc.: 72.66%] [G loss: 0.533553]\n",
      "epoch:11 step:10865 [D loss: 0.228926, acc.: 64.84%] [G loss: 0.429051]\n",
      "epoch:11 step:10866 [D loss: 0.271415, acc.: 54.69%] [G loss: 0.421092]\n",
      "epoch:11 step:10867 [D loss: 0.231176, acc.: 60.94%] [G loss: 0.449686]\n",
      "epoch:11 step:10868 [D loss: 0.215346, acc.: 64.06%] [G loss: 0.478181]\n",
      "epoch:11 step:10869 [D loss: 0.235335, acc.: 58.59%] [G loss: 0.470465]\n",
      "epoch:11 step:10870 [D loss: 0.213629, acc.: 68.75%] [G loss: 0.484957]\n",
      "epoch:11 step:10871 [D loss: 0.175532, acc.: 76.56%] [G loss: 0.492014]\n",
      "epoch:11 step:10872 [D loss: 0.271197, acc.: 50.78%] [G loss: 0.460600]\n",
      "epoch:11 step:10873 [D loss: 0.276630, acc.: 52.34%] [G loss: 0.456584]\n",
      "epoch:11 step:10874 [D loss: 0.214275, acc.: 69.53%] [G loss: 0.485178]\n",
      "epoch:11 step:10875 [D loss: 0.191249, acc.: 75.00%] [G loss: 0.493667]\n",
      "epoch:11 step:10876 [D loss: 0.270309, acc.: 55.47%] [G loss: 0.434960]\n",
      "epoch:11 step:10877 [D loss: 0.208289, acc.: 64.84%] [G loss: 0.416748]\n",
      "epoch:11 step:10878 [D loss: 0.223583, acc.: 66.41%] [G loss: 0.433618]\n",
      "epoch:11 step:10879 [D loss: 0.231542, acc.: 61.72%] [G loss: 0.438670]\n",
      "epoch:11 step:10880 [D loss: 0.221614, acc.: 62.50%] [G loss: 0.454225]\n",
      "epoch:11 step:10881 [D loss: 0.177594, acc.: 74.22%] [G loss: 0.486649]\n",
      "epoch:11 step:10882 [D loss: 0.208484, acc.: 66.41%] [G loss: 0.492695]\n",
      "epoch:11 step:10883 [D loss: 0.259396, acc.: 51.56%] [G loss: 0.423859]\n",
      "epoch:11 step:10884 [D loss: 0.248209, acc.: 60.94%] [G loss: 0.455370]\n",
      "epoch:11 step:10885 [D loss: 0.225794, acc.: 58.59%] [G loss: 0.427175]\n",
      "epoch:11 step:10886 [D loss: 0.229409, acc.: 59.38%] [G loss: 0.429080]\n",
      "epoch:11 step:10887 [D loss: 0.219729, acc.: 60.94%] [G loss: 0.439802]\n",
      "epoch:11 step:10888 [D loss: 0.209648, acc.: 63.28%] [G loss: 0.488392]\n",
      "epoch:11 step:10889 [D loss: 0.222656, acc.: 65.62%] [G loss: 0.436915]\n",
      "epoch:11 step:10890 [D loss: 0.219788, acc.: 63.28%] [G loss: 0.486403]\n",
      "epoch:11 step:10891 [D loss: 0.260403, acc.: 56.25%] [G loss: 0.450577]\n",
      "epoch:11 step:10892 [D loss: 0.232642, acc.: 60.94%] [G loss: 0.439255]\n",
      "epoch:11 step:10893 [D loss: 0.228197, acc.: 58.59%] [G loss: 0.432492]\n",
      "epoch:11 step:10894 [D loss: 0.250160, acc.: 50.78%] [G loss: 0.419795]\n",
      "epoch:11 step:10895 [D loss: 0.217003, acc.: 69.53%] [G loss: 0.442472]\n",
      "epoch:11 step:10896 [D loss: 0.202970, acc.: 72.66%] [G loss: 0.441295]\n",
      "epoch:11 step:10897 [D loss: 0.253968, acc.: 60.94%] [G loss: 0.481438]\n",
      "epoch:11 step:10898 [D loss: 0.224752, acc.: 60.94%] [G loss: 0.428229]\n",
      "epoch:11 step:10899 [D loss: 0.199746, acc.: 71.09%] [G loss: 0.462374]\n",
      "epoch:11 step:10900 [D loss: 0.218596, acc.: 64.06%] [G loss: 0.466601]\n",
      "epoch:11 step:10901 [D loss: 0.232994, acc.: 60.94%] [G loss: 0.456836]\n",
      "epoch:11 step:10902 [D loss: 0.231320, acc.: 62.50%] [G loss: 0.423158]\n",
      "epoch:11 step:10903 [D loss: 0.240947, acc.: 58.59%] [G loss: 0.440728]\n",
      "epoch:11 step:10904 [D loss: 0.238502, acc.: 60.16%] [G loss: 0.443707]\n",
      "epoch:11 step:10905 [D loss: 0.220731, acc.: 67.19%] [G loss: 0.447305]\n",
      "epoch:11 step:10906 [D loss: 0.242630, acc.: 57.81%] [G loss: 0.417063]\n",
      "epoch:11 step:10907 [D loss: 0.242236, acc.: 52.34%] [G loss: 0.400245]\n",
      "epoch:11 step:10908 [D loss: 0.211090, acc.: 65.62%] [G loss: 0.432255]\n",
      "epoch:11 step:10909 [D loss: 0.220634, acc.: 68.75%] [G loss: 0.417191]\n",
      "epoch:11 step:10910 [D loss: 0.219771, acc.: 61.72%] [G loss: 0.488825]\n",
      "epoch:11 step:10911 [D loss: 0.211218, acc.: 61.72%] [G loss: 0.464605]\n",
      "epoch:11 step:10912 [D loss: 0.238290, acc.: 60.94%] [G loss: 0.430329]\n",
      "epoch:11 step:10913 [D loss: 0.243737, acc.: 61.72%] [G loss: 0.408780]\n",
      "epoch:11 step:10914 [D loss: 0.201025, acc.: 68.75%] [G loss: 0.434188]\n",
      "epoch:11 step:10915 [D loss: 0.217479, acc.: 67.19%] [G loss: 0.388641]\n",
      "epoch:11 step:10916 [D loss: 0.210597, acc.: 64.84%] [G loss: 0.470947]\n",
      "epoch:11 step:10917 [D loss: 0.220306, acc.: 65.62%] [G loss: 0.450076]\n",
      "epoch:11 step:10918 [D loss: 0.236022, acc.: 60.16%] [G loss: 0.404529]\n",
      "epoch:11 step:10919 [D loss: 0.225432, acc.: 66.41%] [G loss: 0.404256]\n",
      "epoch:11 step:10920 [D loss: 0.225492, acc.: 63.28%] [G loss: 0.438481]\n",
      "epoch:11 step:10921 [D loss: 0.252243, acc.: 53.12%] [G loss: 0.448594]\n",
      "epoch:11 step:10922 [D loss: 0.238320, acc.: 62.50%] [G loss: 0.467937]\n",
      "epoch:11 step:10923 [D loss: 0.224011, acc.: 63.28%] [G loss: 0.443658]\n",
      "epoch:11 step:10924 [D loss: 0.222198, acc.: 64.06%] [G loss: 0.418198]\n",
      "epoch:11 step:10925 [D loss: 0.224383, acc.: 64.84%] [G loss: 0.415497]\n",
      "epoch:11 step:10926 [D loss: 0.247628, acc.: 57.03%] [G loss: 0.424018]\n",
      "epoch:11 step:10927 [D loss: 0.229360, acc.: 63.28%] [G loss: 0.442941]\n",
      "epoch:11 step:10928 [D loss: 0.229211, acc.: 59.38%] [G loss: 0.429951]\n",
      "epoch:11 step:10929 [D loss: 0.232461, acc.: 60.94%] [G loss: 0.448660]\n",
      "epoch:11 step:10930 [D loss: 0.213313, acc.: 69.53%] [G loss: 0.498695]\n",
      "epoch:11 step:10931 [D loss: 0.189939, acc.: 70.31%] [G loss: 0.487956]\n",
      "epoch:11 step:10932 [D loss: 0.250844, acc.: 54.69%] [G loss: 0.455115]\n",
      "epoch:11 step:10933 [D loss: 0.232893, acc.: 58.59%] [G loss: 0.428089]\n",
      "epoch:11 step:10934 [D loss: 0.215118, acc.: 66.41%] [G loss: 0.414862]\n",
      "epoch:11 step:10935 [D loss: 0.239540, acc.: 60.94%] [G loss: 0.392718]\n",
      "epoch:11 step:10936 [D loss: 0.205284, acc.: 71.88%] [G loss: 0.428436]\n",
      "epoch:11 step:10937 [D loss: 0.224697, acc.: 66.41%] [G loss: 0.409984]\n",
      "epoch:11 step:10938 [D loss: 0.189656, acc.: 73.44%] [G loss: 0.426857]\n",
      "epoch:11 step:10939 [D loss: 0.195847, acc.: 71.88%] [G loss: 0.489049]\n",
      "epoch:11 step:10940 [D loss: 0.199886, acc.: 69.53%] [G loss: 0.499055]\n",
      "epoch:11 step:10941 [D loss: 0.227428, acc.: 62.50%] [G loss: 0.438604]\n",
      "epoch:11 step:10942 [D loss: 0.221239, acc.: 62.50%] [G loss: 0.464776]\n",
      "epoch:11 step:10943 [D loss: 0.226574, acc.: 57.81%] [G loss: 0.457619]\n",
      "epoch:11 step:10944 [D loss: 0.234391, acc.: 61.72%] [G loss: 0.425666]\n",
      "epoch:11 step:10945 [D loss: 0.215810, acc.: 65.62%] [G loss: 0.463184]\n",
      "epoch:11 step:10946 [D loss: 0.220992, acc.: 63.28%] [G loss: 0.473427]\n",
      "epoch:11 step:10947 [D loss: 0.211061, acc.: 66.41%] [G loss: 0.485554]\n",
      "epoch:11 step:10948 [D loss: 0.195875, acc.: 71.09%] [G loss: 0.499155]\n",
      "epoch:11 step:10949 [D loss: 0.171152, acc.: 73.44%] [G loss: 0.519892]\n",
      "epoch:11 step:10950 [D loss: 0.236485, acc.: 52.34%] [G loss: 0.478793]\n",
      "epoch:11 step:10951 [D loss: 0.238979, acc.: 56.25%] [G loss: 0.434643]\n",
      "epoch:11 step:10952 [D loss: 0.216071, acc.: 67.19%] [G loss: 0.448271]\n",
      "epoch:11 step:10953 [D loss: 0.208230, acc.: 68.75%] [G loss: 0.422551]\n",
      "epoch:11 step:10954 [D loss: 0.200283, acc.: 67.19%] [G loss: 0.480696]\n",
      "epoch:11 step:10955 [D loss: 0.174936, acc.: 72.66%] [G loss: 0.554343]\n",
      "epoch:11 step:10956 [D loss: 0.202230, acc.: 69.53%] [G loss: 0.517312]\n",
      "epoch:11 step:10957 [D loss: 0.228461, acc.: 64.06%] [G loss: 0.493649]\n",
      "epoch:11 step:10958 [D loss: 0.221797, acc.: 60.94%] [G loss: 0.502143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10959 [D loss: 0.247795, acc.: 57.03%] [G loss: 0.430920]\n",
      "epoch:11 step:10960 [D loss: 0.231108, acc.: 57.81%] [G loss: 0.477189]\n",
      "epoch:11 step:10961 [D loss: 0.229625, acc.: 60.16%] [G loss: 0.430050]\n",
      "epoch:11 step:10962 [D loss: 0.249447, acc.: 61.72%] [G loss: 0.448335]\n",
      "epoch:11 step:10963 [D loss: 0.221996, acc.: 62.50%] [G loss: 0.480223]\n",
      "epoch:11 step:10964 [D loss: 0.220708, acc.: 63.28%] [G loss: 0.444667]\n",
      "epoch:11 step:10965 [D loss: 0.225365, acc.: 64.06%] [G loss: 0.448260]\n",
      "epoch:11 step:10966 [D loss: 0.218873, acc.: 61.72%] [G loss: 0.443885]\n",
      "epoch:11 step:10967 [D loss: 0.226019, acc.: 61.72%] [G loss: 0.459478]\n",
      "epoch:11 step:10968 [D loss: 0.210140, acc.: 67.97%] [G loss: 0.445669]\n",
      "epoch:11 step:10969 [D loss: 0.226946, acc.: 64.06%] [G loss: 0.474385]\n",
      "epoch:11 step:10970 [D loss: 0.221422, acc.: 61.72%] [G loss: 0.466057]\n",
      "epoch:11 step:10971 [D loss: 0.212023, acc.: 60.94%] [G loss: 0.486413]\n",
      "epoch:11 step:10972 [D loss: 0.207076, acc.: 68.75%] [G loss: 0.410424]\n",
      "epoch:11 step:10973 [D loss: 0.222654, acc.: 66.41%] [G loss: 0.437041]\n",
      "epoch:11 step:10974 [D loss: 0.257627, acc.: 54.69%] [G loss: 0.426284]\n",
      "epoch:11 step:10975 [D loss: 0.221828, acc.: 65.62%] [G loss: 0.463542]\n",
      "epoch:11 step:10976 [D loss: 0.219359, acc.: 65.62%] [G loss: 0.471790]\n",
      "epoch:11 step:10977 [D loss: 0.234707, acc.: 64.06%] [G loss: 0.430887]\n",
      "epoch:11 step:10978 [D loss: 0.230158, acc.: 62.50%] [G loss: 0.464640]\n",
      "epoch:11 step:10979 [D loss: 0.237647, acc.: 60.16%] [G loss: 0.460653]\n",
      "epoch:11 step:10980 [D loss: 0.230156, acc.: 61.72%] [G loss: 0.417277]\n",
      "epoch:11 step:10981 [D loss: 0.206179, acc.: 69.53%] [G loss: 0.480865]\n",
      "epoch:11 step:10982 [D loss: 0.254574, acc.: 53.91%] [G loss: 0.426728]\n",
      "epoch:11 step:10983 [D loss: 0.230414, acc.: 66.41%] [G loss: 0.424381]\n",
      "epoch:11 step:10984 [D loss: 0.217274, acc.: 60.16%] [G loss: 0.427972]\n",
      "epoch:11 step:10985 [D loss: 0.250174, acc.: 58.59%] [G loss: 0.424937]\n",
      "epoch:11 step:10986 [D loss: 0.204623, acc.: 71.09%] [G loss: 0.469002]\n",
      "epoch:11 step:10987 [D loss: 0.208590, acc.: 63.28%] [G loss: 0.467572]\n",
      "epoch:11 step:10988 [D loss: 0.205905, acc.: 68.75%] [G loss: 0.461054]\n",
      "epoch:11 step:10989 [D loss: 0.247528, acc.: 57.03%] [G loss: 0.414923]\n",
      "epoch:11 step:10990 [D loss: 0.247234, acc.: 58.59%] [G loss: 0.405233]\n",
      "epoch:11 step:10991 [D loss: 0.220781, acc.: 61.72%] [G loss: 0.433313]\n",
      "epoch:11 step:10992 [D loss: 0.219083, acc.: 61.72%] [G loss: 0.447363]\n",
      "epoch:11 step:10993 [D loss: 0.221026, acc.: 62.50%] [G loss: 0.463541]\n",
      "epoch:11 step:10994 [D loss: 0.226820, acc.: 62.50%] [G loss: 0.454111]\n",
      "epoch:11 step:10995 [D loss: 0.200518, acc.: 69.53%] [G loss: 0.485256]\n",
      "epoch:11 step:10996 [D loss: 0.213465, acc.: 67.97%] [G loss: 0.490470]\n",
      "epoch:11 step:10997 [D loss: 0.173805, acc.: 77.34%] [G loss: 0.477421]\n",
      "epoch:11 step:10998 [D loss: 0.230798, acc.: 64.06%] [G loss: 0.476863]\n",
      "epoch:11 step:10999 [D loss: 0.203237, acc.: 72.66%] [G loss: 0.482028]\n",
      "epoch:11 step:11000 [D loss: 0.226825, acc.: 63.28%] [G loss: 0.478309]\n",
      "##############\n",
      "[2.49806722 1.55031984 5.89649585 4.72162472 3.54989056 5.62241166\n",
      " 4.45451743 4.3482915  4.34332698 3.85407605]\n",
      "##########\n",
      "epoch:11 step:11001 [D loss: 0.210602, acc.: 64.06%] [G loss: 0.508242]\n",
      "epoch:11 step:11002 [D loss: 0.215837, acc.: 67.19%] [G loss: 0.478558]\n",
      "epoch:11 step:11003 [D loss: 0.212755, acc.: 62.50%] [G loss: 0.458379]\n",
      "epoch:11 step:11004 [D loss: 0.209466, acc.: 67.97%] [G loss: 0.481299]\n",
      "epoch:11 step:11005 [D loss: 0.239147, acc.: 60.94%] [G loss: 0.410819]\n",
      "epoch:11 step:11006 [D loss: 0.228780, acc.: 64.06%] [G loss: 0.438569]\n",
      "epoch:11 step:11007 [D loss: 0.222726, acc.: 61.72%] [G loss: 0.465411]\n",
      "epoch:11 step:11008 [D loss: 0.191645, acc.: 70.31%] [G loss: 0.487976]\n",
      "epoch:11 step:11009 [D loss: 0.227687, acc.: 64.84%] [G loss: 0.502347]\n",
      "epoch:11 step:11010 [D loss: 0.246627, acc.: 56.25%] [G loss: 0.469104]\n",
      "epoch:11 step:11011 [D loss: 0.232683, acc.: 63.28%] [G loss: 0.446059]\n",
      "epoch:11 step:11012 [D loss: 0.218716, acc.: 67.19%] [G loss: 0.453029]\n",
      "epoch:11 step:11013 [D loss: 0.221099, acc.: 65.62%] [G loss: 0.418700]\n",
      "epoch:11 step:11014 [D loss: 0.227251, acc.: 66.41%] [G loss: 0.442268]\n",
      "epoch:11 step:11015 [D loss: 0.168318, acc.: 78.91%] [G loss: 0.501140]\n",
      "epoch:11 step:11016 [D loss: 0.181112, acc.: 75.78%] [G loss: 0.462312]\n",
      "epoch:11 step:11017 [D loss: 0.289054, acc.: 50.00%] [G loss: 0.418497]\n",
      "epoch:11 step:11018 [D loss: 0.221318, acc.: 61.72%] [G loss: 0.448829]\n",
      "epoch:11 step:11019 [D loss: 0.196362, acc.: 68.75%] [G loss: 0.507419]\n",
      "epoch:11 step:11020 [D loss: 0.243462, acc.: 57.81%] [G loss: 0.431677]\n",
      "epoch:11 step:11021 [D loss: 0.241040, acc.: 59.38%] [G loss: 0.442330]\n",
      "epoch:11 step:11022 [D loss: 0.228417, acc.: 57.81%] [G loss: 0.442686]\n",
      "epoch:11 step:11023 [D loss: 0.259233, acc.: 52.34%] [G loss: 0.455057]\n",
      "epoch:11 step:11024 [D loss: 0.226181, acc.: 62.50%] [G loss: 0.499142]\n",
      "epoch:11 step:11025 [D loss: 0.217525, acc.: 64.06%] [G loss: 0.479898]\n",
      "epoch:11 step:11026 [D loss: 0.226729, acc.: 60.94%] [G loss: 0.463223]\n",
      "epoch:11 step:11027 [D loss: 0.207715, acc.: 68.75%] [G loss: 0.524686]\n",
      "epoch:11 step:11028 [D loss: 0.249915, acc.: 55.47%] [G loss: 0.476899]\n",
      "epoch:11 step:11029 [D loss: 0.265832, acc.: 55.47%] [G loss: 0.428051]\n",
      "epoch:11 step:11030 [D loss: 0.233390, acc.: 60.94%] [G loss: 0.438201]\n",
      "epoch:11 step:11031 [D loss: 0.208574, acc.: 65.62%] [G loss: 0.472123]\n",
      "epoch:11 step:11032 [D loss: 0.206317, acc.: 71.09%] [G loss: 0.474909]\n",
      "epoch:11 step:11033 [D loss: 0.220188, acc.: 61.72%] [G loss: 0.417575]\n",
      "epoch:11 step:11034 [D loss: 0.244481, acc.: 57.03%] [G loss: 0.435961]\n",
      "epoch:11 step:11035 [D loss: 0.236749, acc.: 60.94%] [G loss: 0.424723]\n",
      "epoch:11 step:11036 [D loss: 0.208543, acc.: 64.84%] [G loss: 0.429959]\n",
      "epoch:11 step:11037 [D loss: 0.212805, acc.: 68.75%] [G loss: 0.434206]\n",
      "epoch:11 step:11038 [D loss: 0.222164, acc.: 60.16%] [G loss: 0.501723]\n",
      "epoch:11 step:11039 [D loss: 0.232199, acc.: 60.94%] [G loss: 0.493625]\n",
      "epoch:11 step:11040 [D loss: 0.210482, acc.: 67.97%] [G loss: 0.447312]\n",
      "epoch:11 step:11041 [D loss: 0.226404, acc.: 63.28%] [G loss: 0.443832]\n",
      "epoch:11 step:11042 [D loss: 0.228748, acc.: 66.41%] [G loss: 0.427986]\n",
      "epoch:11 step:11043 [D loss: 0.225452, acc.: 60.94%] [G loss: 0.438730]\n",
      "epoch:11 step:11044 [D loss: 0.235278, acc.: 61.72%] [G loss: 0.437425]\n",
      "epoch:11 step:11045 [D loss: 0.237596, acc.: 57.03%] [G loss: 0.433607]\n",
      "epoch:11 step:11046 [D loss: 0.232805, acc.: 66.41%] [G loss: 0.410685]\n",
      "epoch:11 step:11047 [D loss: 0.211641, acc.: 64.84%] [G loss: 0.405669]\n",
      "epoch:11 step:11048 [D loss: 0.266113, acc.: 54.69%] [G loss: 0.402149]\n",
      "epoch:11 step:11049 [D loss: 0.232915, acc.: 57.81%] [G loss: 0.445042]\n",
      "epoch:11 step:11050 [D loss: 0.223099, acc.: 64.84%] [G loss: 0.456789]\n",
      "epoch:11 step:11051 [D loss: 0.232387, acc.: 58.59%] [G loss: 0.433170]\n",
      "epoch:11 step:11052 [D loss: 0.230149, acc.: 59.38%] [G loss: 0.413039]\n",
      "epoch:11 step:11053 [D loss: 0.228430, acc.: 64.06%] [G loss: 0.401863]\n",
      "epoch:11 step:11054 [D loss: 0.209839, acc.: 67.19%] [G loss: 0.432015]\n",
      "epoch:11 step:11055 [D loss: 0.222945, acc.: 61.72%] [G loss: 0.413352]\n",
      "epoch:11 step:11056 [D loss: 0.240864, acc.: 60.16%] [G loss: 0.429282]\n",
      "epoch:11 step:11057 [D loss: 0.216349, acc.: 64.84%] [G loss: 0.453087]\n",
      "epoch:11 step:11058 [D loss: 0.247513, acc.: 55.47%] [G loss: 0.446842]\n",
      "epoch:11 step:11059 [D loss: 0.231911, acc.: 64.06%] [G loss: 0.442773]\n",
      "epoch:11 step:11060 [D loss: 0.218203, acc.: 63.28%] [G loss: 0.467527]\n",
      "epoch:11 step:11061 [D loss: 0.232273, acc.: 59.38%] [G loss: 0.480982]\n",
      "epoch:11 step:11062 [D loss: 0.201633, acc.: 68.75%] [G loss: 0.521097]\n",
      "epoch:11 step:11063 [D loss: 0.265947, acc.: 53.91%] [G loss: 0.421669]\n",
      "epoch:11 step:11064 [D loss: 0.223748, acc.: 64.84%] [G loss: 0.499315]\n",
      "epoch:11 step:11065 [D loss: 0.211792, acc.: 68.75%] [G loss: 0.468559]\n",
      "epoch:11 step:11066 [D loss: 0.254176, acc.: 57.03%] [G loss: 0.438304]\n",
      "epoch:11 step:11067 [D loss: 0.227667, acc.: 65.62%] [G loss: 0.420905]\n",
      "epoch:11 step:11068 [D loss: 0.227152, acc.: 60.94%] [G loss: 0.467524]\n",
      "epoch:11 step:11069 [D loss: 0.232852, acc.: 57.03%] [G loss: 0.438851]\n",
      "epoch:11 step:11070 [D loss: 0.245638, acc.: 53.12%] [G loss: 0.436792]\n",
      "epoch:11 step:11071 [D loss: 0.198880, acc.: 70.31%] [G loss: 0.443188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11072 [D loss: 0.254975, acc.: 54.69%] [G loss: 0.411882]\n",
      "epoch:11 step:11073 [D loss: 0.236100, acc.: 57.03%] [G loss: 0.431700]\n",
      "epoch:11 step:11074 [D loss: 0.209705, acc.: 68.75%] [G loss: 0.500902]\n",
      "epoch:11 step:11075 [D loss: 0.227980, acc.: 64.06%] [G loss: 0.477643]\n",
      "epoch:11 step:11076 [D loss: 0.210745, acc.: 62.50%] [G loss: 0.473379]\n",
      "epoch:11 step:11077 [D loss: 0.198221, acc.: 64.84%] [G loss: 0.472140]\n",
      "epoch:11 step:11078 [D loss: 0.224230, acc.: 57.81%] [G loss: 0.432416]\n",
      "epoch:11 step:11079 [D loss: 0.241644, acc.: 56.25%] [G loss: 0.400232]\n",
      "epoch:11 step:11080 [D loss: 0.215392, acc.: 66.41%] [G loss: 0.464195]\n",
      "epoch:11 step:11081 [D loss: 0.215465, acc.: 67.19%] [G loss: 0.447264]\n",
      "epoch:11 step:11082 [D loss: 0.207686, acc.: 64.84%] [G loss: 0.488412]\n",
      "epoch:11 step:11083 [D loss: 0.252792, acc.: 52.34%] [G loss: 0.427089]\n",
      "epoch:11 step:11084 [D loss: 0.224019, acc.: 60.16%] [G loss: 0.420605]\n",
      "epoch:11 step:11085 [D loss: 0.231468, acc.: 64.06%] [G loss: 0.455810]\n",
      "epoch:11 step:11086 [D loss: 0.238969, acc.: 57.81%] [G loss: 0.449190]\n",
      "epoch:11 step:11087 [D loss: 0.250499, acc.: 54.69%] [G loss: 0.424243]\n",
      "epoch:11 step:11088 [D loss: 0.208124, acc.: 69.53%] [G loss: 0.457873]\n",
      "epoch:11 step:11089 [D loss: 0.203859, acc.: 71.88%] [G loss: 0.515360]\n",
      "epoch:11 step:11090 [D loss: 0.245553, acc.: 57.03%] [G loss: 0.501224]\n",
      "epoch:11 step:11091 [D loss: 0.243690, acc.: 61.72%] [G loss: 0.428973]\n",
      "epoch:11 step:11092 [D loss: 0.232395, acc.: 60.94%] [G loss: 0.439338]\n",
      "epoch:11 step:11093 [D loss: 0.220990, acc.: 67.97%] [G loss: 0.420839]\n",
      "epoch:11 step:11094 [D loss: 0.258356, acc.: 51.56%] [G loss: 0.461380]\n",
      "epoch:11 step:11095 [D loss: 0.249443, acc.: 55.47%] [G loss: 0.424393]\n",
      "epoch:11 step:11096 [D loss: 0.232289, acc.: 61.72%] [G loss: 0.460713]\n",
      "epoch:11 step:11097 [D loss: 0.213007, acc.: 71.88%] [G loss: 0.415827]\n",
      "epoch:11 step:11098 [D loss: 0.250647, acc.: 52.34%] [G loss: 0.412985]\n",
      "epoch:11 step:11099 [D loss: 0.188616, acc.: 71.88%] [G loss: 0.449984]\n",
      "epoch:11 step:11100 [D loss: 0.225327, acc.: 63.28%] [G loss: 0.436467]\n",
      "epoch:11 step:11101 [D loss: 0.267751, acc.: 55.47%] [G loss: 0.472567]\n",
      "epoch:11 step:11102 [D loss: 0.229422, acc.: 60.94%] [G loss: 0.414691]\n",
      "epoch:11 step:11103 [D loss: 0.216021, acc.: 66.41%] [G loss: 0.461235]\n",
      "epoch:11 step:11104 [D loss: 0.235918, acc.: 57.81%] [G loss: 0.429295]\n",
      "epoch:11 step:11105 [D loss: 0.205496, acc.: 66.41%] [G loss: 0.438722]\n",
      "epoch:11 step:11106 [D loss: 0.200876, acc.: 67.97%] [G loss: 0.452332]\n",
      "epoch:11 step:11107 [D loss: 0.257590, acc.: 53.91%] [G loss: 0.475543]\n",
      "epoch:11 step:11108 [D loss: 0.224458, acc.: 67.97%] [G loss: 0.438971]\n",
      "epoch:11 step:11109 [D loss: 0.221675, acc.: 62.50%] [G loss: 0.459712]\n",
      "epoch:11 step:11110 [D loss: 0.204341, acc.: 71.09%] [G loss: 0.517001]\n",
      "epoch:11 step:11111 [D loss: 0.270349, acc.: 53.91%] [G loss: 0.413265]\n",
      "epoch:11 step:11112 [D loss: 0.203342, acc.: 67.97%] [G loss: 0.421768]\n",
      "epoch:11 step:11113 [D loss: 0.233346, acc.: 58.59%] [G loss: 0.456407]\n",
      "epoch:11 step:11114 [D loss: 0.198340, acc.: 70.31%] [G loss: 0.430812]\n",
      "epoch:11 step:11115 [D loss: 0.229278, acc.: 60.16%] [G loss: 0.455855]\n",
      "epoch:11 step:11116 [D loss: 0.245734, acc.: 59.38%] [G loss: 0.413314]\n",
      "epoch:11 step:11117 [D loss: 0.213659, acc.: 67.97%] [G loss: 0.402641]\n",
      "epoch:11 step:11118 [D loss: 0.237361, acc.: 55.47%] [G loss: 0.411568]\n",
      "epoch:11 step:11119 [D loss: 0.233000, acc.: 61.72%] [G loss: 0.405527]\n",
      "epoch:11 step:11120 [D loss: 0.238278, acc.: 58.59%] [G loss: 0.440174]\n",
      "epoch:11 step:11121 [D loss: 0.226157, acc.: 66.41%] [G loss: 0.442408]\n",
      "epoch:11 step:11122 [D loss: 0.235966, acc.: 58.59%] [G loss: 0.462283]\n",
      "epoch:11 step:11123 [D loss: 0.215860, acc.: 64.84%] [G loss: 0.503016]\n",
      "epoch:11 step:11124 [D loss: 0.260617, acc.: 53.12%] [G loss: 0.458057]\n",
      "epoch:11 step:11125 [D loss: 0.238147, acc.: 62.50%] [G loss: 0.409010]\n",
      "epoch:11 step:11126 [D loss: 0.230979, acc.: 64.06%] [G loss: 0.399578]\n",
      "epoch:11 step:11127 [D loss: 0.275198, acc.: 50.78%] [G loss: 0.403849]\n",
      "epoch:11 step:11128 [D loss: 0.200921, acc.: 69.53%] [G loss: 0.450709]\n",
      "epoch:11 step:11129 [D loss: 0.205620, acc.: 66.41%] [G loss: 0.441424]\n",
      "epoch:11 step:11130 [D loss: 0.192365, acc.: 71.88%] [G loss: 0.437542]\n",
      "epoch:11 step:11131 [D loss: 0.247971, acc.: 57.03%] [G loss: 0.432832]\n",
      "epoch:11 step:11132 [D loss: 0.223635, acc.: 62.50%] [G loss: 0.445479]\n",
      "epoch:11 step:11133 [D loss: 0.220289, acc.: 65.62%] [G loss: 0.470232]\n",
      "epoch:11 step:11134 [D loss: 0.272064, acc.: 50.78%] [G loss: 0.437824]\n",
      "epoch:11 step:11135 [D loss: 0.267972, acc.: 53.12%] [G loss: 0.363404]\n",
      "epoch:11 step:11136 [D loss: 0.241800, acc.: 58.59%] [G loss: 0.408789]\n",
      "epoch:11 step:11137 [D loss: 0.217256, acc.: 65.62%] [G loss: 0.423272]\n",
      "epoch:11 step:11138 [D loss: 0.214327, acc.: 62.50%] [G loss: 0.460906]\n",
      "epoch:11 step:11139 [D loss: 0.238250, acc.: 64.06%] [G loss: 0.460806]\n",
      "epoch:11 step:11140 [D loss: 0.214136, acc.: 67.97%] [G loss: 0.485168]\n",
      "epoch:11 step:11141 [D loss: 0.210256, acc.: 69.53%] [G loss: 0.461342]\n",
      "epoch:11 step:11142 [D loss: 0.205660, acc.: 71.88%] [G loss: 0.428084]\n",
      "epoch:11 step:11143 [D loss: 0.209701, acc.: 64.84%] [G loss: 0.403681]\n",
      "epoch:11 step:11144 [D loss: 0.200648, acc.: 71.88%] [G loss: 0.429864]\n",
      "epoch:11 step:11145 [D loss: 0.207820, acc.: 64.06%] [G loss: 0.466614]\n",
      "epoch:11 step:11146 [D loss: 0.218547, acc.: 64.06%] [G loss: 0.427566]\n",
      "epoch:11 step:11147 [D loss: 0.226035, acc.: 63.28%] [G loss: 0.473271]\n",
      "epoch:11 step:11148 [D loss: 0.212935, acc.: 65.62%] [G loss: 0.404421]\n",
      "epoch:11 step:11149 [D loss: 0.200891, acc.: 62.50%] [G loss: 0.478942]\n",
      "epoch:11 step:11150 [D loss: 0.236187, acc.: 63.28%] [G loss: 0.447623]\n",
      "epoch:11 step:11151 [D loss: 0.239490, acc.: 59.38%] [G loss: 0.456087]\n",
      "epoch:11 step:11152 [D loss: 0.205805, acc.: 66.41%] [G loss: 0.448063]\n",
      "epoch:11 step:11153 [D loss: 0.217966, acc.: 62.50%] [G loss: 0.440788]\n",
      "epoch:11 step:11154 [D loss: 0.243405, acc.: 57.03%] [G loss: 0.421319]\n",
      "epoch:11 step:11155 [D loss: 0.241260, acc.: 60.16%] [G loss: 0.388776]\n",
      "epoch:11 step:11156 [D loss: 0.204407, acc.: 69.53%] [G loss: 0.425994]\n",
      "epoch:11 step:11157 [D loss: 0.231099, acc.: 67.19%] [G loss: 0.431512]\n",
      "epoch:11 step:11158 [D loss: 0.246076, acc.: 56.25%] [G loss: 0.451924]\n",
      "epoch:11 step:11159 [D loss: 0.219246, acc.: 64.84%] [G loss: 0.461901]\n",
      "epoch:11 step:11160 [D loss: 0.232752, acc.: 59.38%] [G loss: 0.459092]\n",
      "epoch:11 step:11161 [D loss: 0.217325, acc.: 67.19%] [G loss: 0.444732]\n",
      "epoch:11 step:11162 [D loss: 0.233089, acc.: 57.81%] [G loss: 0.427274]\n",
      "epoch:11 step:11163 [D loss: 0.221423, acc.: 61.72%] [G loss: 0.438049]\n",
      "epoch:11 step:11164 [D loss: 0.216474, acc.: 64.06%] [G loss: 0.457015]\n",
      "epoch:11 step:11165 [D loss: 0.230091, acc.: 57.03%] [G loss: 0.464843]\n",
      "epoch:11 step:11166 [D loss: 0.227195, acc.: 58.59%] [G loss: 0.456574]\n",
      "epoch:11 step:11167 [D loss: 0.228777, acc.: 60.16%] [G loss: 0.456613]\n",
      "epoch:11 step:11168 [D loss: 0.262086, acc.: 51.56%] [G loss: 0.434951]\n",
      "epoch:11 step:11169 [D loss: 0.249907, acc.: 57.81%] [G loss: 0.450560]\n",
      "epoch:11 step:11170 [D loss: 0.211533, acc.: 66.41%] [G loss: 0.436691]\n",
      "epoch:11 step:11171 [D loss: 0.222620, acc.: 60.94%] [G loss: 0.454053]\n",
      "epoch:11 step:11172 [D loss: 0.242063, acc.: 59.38%] [G loss: 0.395064]\n",
      "epoch:11 step:11173 [D loss: 0.212097, acc.: 60.16%] [G loss: 0.425628]\n",
      "epoch:11 step:11174 [D loss: 0.234724, acc.: 57.81%] [G loss: 0.450307]\n",
      "epoch:11 step:11175 [D loss: 0.223947, acc.: 64.06%] [G loss: 0.474146]\n",
      "epoch:11 step:11176 [D loss: 0.269248, acc.: 53.12%] [G loss: 0.399886]\n",
      "epoch:11 step:11177 [D loss: 0.237513, acc.: 60.16%] [G loss: 0.451892]\n",
      "epoch:11 step:11178 [D loss: 0.176259, acc.: 69.53%] [G loss: 0.496185]\n",
      "epoch:11 step:11179 [D loss: 0.220333, acc.: 65.62%] [G loss: 0.458312]\n",
      "epoch:11 step:11180 [D loss: 0.238094, acc.: 60.16%] [G loss: 0.415745]\n",
      "epoch:11 step:11181 [D loss: 0.223325, acc.: 61.72%] [G loss: 0.470839]\n",
      "epoch:11 step:11182 [D loss: 0.183668, acc.: 67.97%] [G loss: 0.529349]\n",
      "epoch:11 step:11183 [D loss: 0.240811, acc.: 60.94%] [G loss: 0.457755]\n",
      "epoch:11 step:11184 [D loss: 0.222344, acc.: 60.16%] [G loss: 0.440366]\n",
      "epoch:11 step:11185 [D loss: 0.252901, acc.: 57.81%] [G loss: 0.444829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11186 [D loss: 0.233223, acc.: 59.38%] [G loss: 0.412997]\n",
      "epoch:11 step:11187 [D loss: 0.247577, acc.: 53.91%] [G loss: 0.437701]\n",
      "epoch:11 step:11188 [D loss: 0.232031, acc.: 58.59%] [G loss: 0.455419]\n",
      "epoch:11 step:11189 [D loss: 0.265004, acc.: 50.78%] [G loss: 0.433715]\n",
      "epoch:11 step:11190 [D loss: 0.228626, acc.: 64.84%] [G loss: 0.493751]\n",
      "epoch:11 step:11191 [D loss: 0.198248, acc.: 69.53%] [G loss: 0.512121]\n",
      "epoch:11 step:11192 [D loss: 0.215421, acc.: 64.84%] [G loss: 0.472666]\n",
      "epoch:11 step:11193 [D loss: 0.196076, acc.: 71.09%] [G loss: 0.478573]\n",
      "epoch:11 step:11194 [D loss: 0.250201, acc.: 55.47%] [G loss: 0.475677]\n",
      "epoch:11 step:11195 [D loss: 0.242628, acc.: 58.59%] [G loss: 0.444851]\n",
      "epoch:11 step:11196 [D loss: 0.208244, acc.: 64.84%] [G loss: 0.446005]\n",
      "epoch:11 step:11197 [D loss: 0.208723, acc.: 71.09%] [G loss: 0.443446]\n",
      "epoch:11 step:11198 [D loss: 0.256505, acc.: 52.34%] [G loss: 0.456714]\n",
      "epoch:11 step:11199 [D loss: 0.247682, acc.: 58.59%] [G loss: 0.434497]\n",
      "epoch:11 step:11200 [D loss: 0.204072, acc.: 69.53%] [G loss: 0.456846]\n",
      "##############\n",
      "[2.57437128 1.7911047  6.06720552 4.61373365 3.72437504 5.43589713\n",
      " 4.30782339 4.7218544  4.49552492 3.96721042]\n",
      "##########\n",
      "epoch:11 step:11201 [D loss: 0.210997, acc.: 64.84%] [G loss: 0.498590]\n",
      "epoch:11 step:11202 [D loss: 0.256523, acc.: 51.56%] [G loss: 0.421157]\n",
      "epoch:11 step:11203 [D loss: 0.204305, acc.: 74.22%] [G loss: 0.462729]\n",
      "epoch:11 step:11204 [D loss: 0.208134, acc.: 71.09%] [G loss: 0.508351]\n",
      "epoch:11 step:11205 [D loss: 0.202258, acc.: 67.97%] [G loss: 0.440352]\n",
      "epoch:11 step:11206 [D loss: 0.179742, acc.: 75.78%] [G loss: 0.505926]\n",
      "epoch:11 step:11207 [D loss: 0.195342, acc.: 70.31%] [G loss: 0.495409]\n",
      "epoch:11 step:11208 [D loss: 0.221825, acc.: 61.72%] [G loss: 0.464894]\n",
      "epoch:11 step:11209 [D loss: 0.255618, acc.: 56.25%] [G loss: 0.445161]\n",
      "epoch:11 step:11210 [D loss: 0.209640, acc.: 68.75%] [G loss: 0.450049]\n",
      "epoch:11 step:11211 [D loss: 0.221330, acc.: 64.06%] [G loss: 0.437897]\n",
      "epoch:11 step:11212 [D loss: 0.245130, acc.: 57.81%] [G loss: 0.442230]\n",
      "epoch:11 step:11213 [D loss: 0.199765, acc.: 68.75%] [G loss: 0.521268]\n",
      "epoch:11 step:11214 [D loss: 0.203823, acc.: 64.84%] [G loss: 0.474978]\n",
      "epoch:11 step:11215 [D loss: 0.207759, acc.: 65.62%] [G loss: 0.423447]\n",
      "epoch:11 step:11216 [D loss: 0.222049, acc.: 60.16%] [G loss: 0.460654]\n",
      "epoch:11 step:11217 [D loss: 0.217194, acc.: 64.84%] [G loss: 0.452912]\n",
      "epoch:11 step:11218 [D loss: 0.209377, acc.: 69.53%] [G loss: 0.472008]\n",
      "epoch:11 step:11219 [D loss: 0.177192, acc.: 74.22%] [G loss: 0.517860]\n",
      "epoch:11 step:11220 [D loss: 0.240802, acc.: 63.28%] [G loss: 0.519228]\n",
      "epoch:11 step:11221 [D loss: 0.218388, acc.: 67.19%] [G loss: 0.545954]\n",
      "epoch:11 step:11222 [D loss: 0.247695, acc.: 64.06%] [G loss: 0.441146]\n",
      "epoch:11 step:11223 [D loss: 0.197952, acc.: 70.31%] [G loss: 0.462249]\n",
      "epoch:11 step:11224 [D loss: 0.190231, acc.: 68.75%] [G loss: 0.473839]\n",
      "epoch:11 step:11225 [D loss: 0.196119, acc.: 71.88%] [G loss: 0.467089]\n",
      "epoch:11 step:11226 [D loss: 0.227659, acc.: 63.28%] [G loss: 0.530353]\n",
      "epoch:11 step:11227 [D loss: 0.332241, acc.: 41.41%] [G loss: 0.458217]\n",
      "epoch:11 step:11228 [D loss: 0.207567, acc.: 64.06%] [G loss: 0.476570]\n",
      "epoch:11 step:11229 [D loss: 0.227029, acc.: 62.50%] [G loss: 0.421457]\n",
      "epoch:11 step:11230 [D loss: 0.208760, acc.: 70.31%] [G loss: 0.418807]\n",
      "epoch:11 step:11231 [D loss: 0.187459, acc.: 75.00%] [G loss: 0.468306]\n",
      "epoch:11 step:11232 [D loss: 0.166286, acc.: 79.69%] [G loss: 0.517963]\n",
      "epoch:11 step:11233 [D loss: 0.188997, acc.: 75.78%] [G loss: 0.512596]\n",
      "epoch:11 step:11234 [D loss: 0.212572, acc.: 64.06%] [G loss: 0.541631]\n",
      "epoch:11 step:11235 [D loss: 0.314430, acc.: 51.56%] [G loss: 0.550810]\n",
      "epoch:11 step:11236 [D loss: 0.251611, acc.: 64.06%] [G loss: 0.532948]\n",
      "epoch:11 step:11237 [D loss: 0.252849, acc.: 60.94%] [G loss: 0.498228]\n",
      "epoch:11 step:11238 [D loss: 0.287194, acc.: 48.44%] [G loss: 0.423361]\n",
      "epoch:11 step:11239 [D loss: 0.259328, acc.: 55.47%] [G loss: 0.423967]\n",
      "epoch:11 step:11240 [D loss: 0.241231, acc.: 63.28%] [G loss: 0.456982]\n",
      "epoch:11 step:11241 [D loss: 0.210994, acc.: 69.53%] [G loss: 0.486532]\n",
      "epoch:11 step:11242 [D loss: 0.198641, acc.: 70.31%] [G loss: 0.477703]\n",
      "epoch:11 step:11243 [D loss: 0.177299, acc.: 75.00%] [G loss: 0.515852]\n",
      "epoch:11 step:11244 [D loss: 0.203071, acc.: 71.09%] [G loss: 0.503287]\n",
      "epoch:12 step:11245 [D loss: 0.277468, acc.: 60.94%] [G loss: 0.520459]\n",
      "epoch:12 step:11246 [D loss: 0.247725, acc.: 59.38%] [G loss: 0.442928]\n",
      "epoch:12 step:11247 [D loss: 0.258063, acc.: 54.69%] [G loss: 0.431629]\n",
      "epoch:12 step:11248 [D loss: 0.253368, acc.: 56.25%] [G loss: 0.467091]\n",
      "epoch:12 step:11249 [D loss: 0.236523, acc.: 56.25%] [G loss: 0.455700]\n",
      "epoch:12 step:11250 [D loss: 0.214500, acc.: 64.84%] [G loss: 0.487119]\n",
      "epoch:12 step:11251 [D loss: 0.202058, acc.: 65.62%] [G loss: 0.460299]\n",
      "epoch:12 step:11252 [D loss: 0.228244, acc.: 62.50%] [G loss: 0.440202]\n",
      "epoch:12 step:11253 [D loss: 0.177857, acc.: 72.66%] [G loss: 0.486890]\n",
      "epoch:12 step:11254 [D loss: 0.244985, acc.: 57.81%] [G loss: 0.413533]\n",
      "epoch:12 step:11255 [D loss: 0.198318, acc.: 65.62%] [G loss: 0.465786]\n",
      "epoch:12 step:11256 [D loss: 0.233011, acc.: 62.50%] [G loss: 0.442339]\n",
      "epoch:12 step:11257 [D loss: 0.224256, acc.: 57.81%] [G loss: 0.447046]\n",
      "epoch:12 step:11258 [D loss: 0.223991, acc.: 62.50%] [G loss: 0.437605]\n",
      "epoch:12 step:11259 [D loss: 0.198807, acc.: 71.09%] [G loss: 0.506218]\n",
      "epoch:12 step:11260 [D loss: 0.211470, acc.: 67.19%] [G loss: 0.461240]\n",
      "epoch:12 step:11261 [D loss: 0.253957, acc.: 52.34%] [G loss: 0.490110]\n",
      "epoch:12 step:11262 [D loss: 0.255671, acc.: 52.34%] [G loss: 0.472802]\n",
      "epoch:12 step:11263 [D loss: 0.256130, acc.: 56.25%] [G loss: 0.463648]\n",
      "epoch:12 step:11264 [D loss: 0.265981, acc.: 46.88%] [G loss: 0.439243]\n",
      "epoch:12 step:11265 [D loss: 0.252047, acc.: 51.56%] [G loss: 0.456375]\n",
      "epoch:12 step:11266 [D loss: 0.204954, acc.: 64.84%] [G loss: 0.504682]\n",
      "epoch:12 step:11267 [D loss: 0.261254, acc.: 60.94%] [G loss: 0.433609]\n",
      "epoch:12 step:11268 [D loss: 0.209006, acc.: 63.28%] [G loss: 0.443421]\n",
      "epoch:12 step:11269 [D loss: 0.215357, acc.: 70.31%] [G loss: 0.461959]\n",
      "epoch:12 step:11270 [D loss: 0.254387, acc.: 55.47%] [G loss: 0.467983]\n",
      "epoch:12 step:11271 [D loss: 0.219480, acc.: 60.94%] [G loss: 0.486558]\n",
      "epoch:12 step:11272 [D loss: 0.197824, acc.: 72.66%] [G loss: 0.450474]\n",
      "epoch:12 step:11273 [D loss: 0.206091, acc.: 67.97%] [G loss: 0.464387]\n",
      "epoch:12 step:11274 [D loss: 0.232779, acc.: 58.59%] [G loss: 0.451083]\n",
      "epoch:12 step:11275 [D loss: 0.253065, acc.: 53.91%] [G loss: 0.421179]\n",
      "epoch:12 step:11276 [D loss: 0.226447, acc.: 61.72%] [G loss: 0.441996]\n",
      "epoch:12 step:11277 [D loss: 0.217687, acc.: 67.19%] [G loss: 0.448912]\n",
      "epoch:12 step:11278 [D loss: 0.252505, acc.: 59.38%] [G loss: 0.407742]\n",
      "epoch:12 step:11279 [D loss: 0.230336, acc.: 59.38%] [G loss: 0.427813]\n",
      "epoch:12 step:11280 [D loss: 0.226091, acc.: 60.16%] [G loss: 0.437248]\n",
      "epoch:12 step:11281 [D loss: 0.242665, acc.: 55.47%] [G loss: 0.415292]\n",
      "epoch:12 step:11282 [D loss: 0.239332, acc.: 57.03%] [G loss: 0.429339]\n",
      "epoch:12 step:11283 [D loss: 0.208433, acc.: 71.88%] [G loss: 0.460452]\n",
      "epoch:12 step:11284 [D loss: 0.204428, acc.: 65.62%] [G loss: 0.463491]\n",
      "epoch:12 step:11285 [D loss: 0.224259, acc.: 62.50%] [G loss: 0.449558]\n",
      "epoch:12 step:11286 [D loss: 0.212623, acc.: 70.31%] [G loss: 0.437110]\n",
      "epoch:12 step:11287 [D loss: 0.235880, acc.: 61.72%] [G loss: 0.422887]\n",
      "epoch:12 step:11288 [D loss: 0.253013, acc.: 55.47%] [G loss: 0.425499]\n",
      "epoch:12 step:11289 [D loss: 0.215360, acc.: 62.50%] [G loss: 0.431836]\n",
      "epoch:12 step:11290 [D loss: 0.247947, acc.: 55.47%] [G loss: 0.415141]\n",
      "epoch:12 step:11291 [D loss: 0.213688, acc.: 64.06%] [G loss: 0.455221]\n",
      "epoch:12 step:11292 [D loss: 0.221483, acc.: 65.62%] [G loss: 0.436925]\n",
      "epoch:12 step:11293 [D loss: 0.210108, acc.: 67.19%] [G loss: 0.466600]\n",
      "epoch:12 step:11294 [D loss: 0.206283, acc.: 67.19%] [G loss: 0.445651]\n",
      "epoch:12 step:11295 [D loss: 0.247411, acc.: 53.91%] [G loss: 0.420517]\n",
      "epoch:12 step:11296 [D loss: 0.231421, acc.: 62.50%] [G loss: 0.465952]\n",
      "epoch:12 step:11297 [D loss: 0.216284, acc.: 64.84%] [G loss: 0.514240]\n",
      "epoch:12 step:11298 [D loss: 0.217818, acc.: 67.97%] [G loss: 0.505111]\n",
      "epoch:12 step:11299 [D loss: 0.248071, acc.: 60.94%] [G loss: 0.509420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11300 [D loss: 0.247369, acc.: 61.72%] [G loss: 0.476338]\n",
      "epoch:12 step:11301 [D loss: 0.235547, acc.: 60.94%] [G loss: 0.411484]\n",
      "epoch:12 step:11302 [D loss: 0.202187, acc.: 72.66%] [G loss: 0.447468]\n",
      "epoch:12 step:11303 [D loss: 0.194482, acc.: 67.19%] [G loss: 0.477050]\n",
      "epoch:12 step:11304 [D loss: 0.244291, acc.: 59.38%] [G loss: 0.412283]\n",
      "epoch:12 step:11305 [D loss: 0.246942, acc.: 59.38%] [G loss: 0.427226]\n",
      "epoch:12 step:11306 [D loss: 0.249836, acc.: 59.38%] [G loss: 0.410643]\n",
      "epoch:12 step:11307 [D loss: 0.224782, acc.: 66.41%] [G loss: 0.452990]\n",
      "epoch:12 step:11308 [D loss: 0.208718, acc.: 67.97%] [G loss: 0.447397]\n",
      "epoch:12 step:11309 [D loss: 0.214356, acc.: 64.06%] [G loss: 0.439756]\n",
      "epoch:12 step:11310 [D loss: 0.219579, acc.: 60.94%] [G loss: 0.438995]\n",
      "epoch:12 step:11311 [D loss: 0.216761, acc.: 71.09%] [G loss: 0.413201]\n",
      "epoch:12 step:11312 [D loss: 0.238077, acc.: 61.72%] [G loss: 0.430067]\n",
      "epoch:12 step:11313 [D loss: 0.191143, acc.: 72.66%] [G loss: 0.430897]\n",
      "epoch:12 step:11314 [D loss: 0.229955, acc.: 56.25%] [G loss: 0.487860]\n",
      "epoch:12 step:11315 [D loss: 0.248551, acc.: 54.69%] [G loss: 0.451279]\n",
      "epoch:12 step:11316 [D loss: 0.213674, acc.: 63.28%] [G loss: 0.462407]\n",
      "epoch:12 step:11317 [D loss: 0.227458, acc.: 60.94%] [G loss: 0.434708]\n",
      "epoch:12 step:11318 [D loss: 0.208165, acc.: 69.53%] [G loss: 0.427835]\n",
      "epoch:12 step:11319 [D loss: 0.231262, acc.: 60.94%] [G loss: 0.416631]\n",
      "epoch:12 step:11320 [D loss: 0.193000, acc.: 71.09%] [G loss: 0.523791]\n",
      "epoch:12 step:11321 [D loss: 0.196633, acc.: 68.75%] [G loss: 0.484865]\n",
      "epoch:12 step:11322 [D loss: 0.282866, acc.: 47.66%] [G loss: 0.455098]\n",
      "epoch:12 step:11323 [D loss: 0.227388, acc.: 61.72%] [G loss: 0.410066]\n",
      "epoch:12 step:11324 [D loss: 0.218409, acc.: 68.75%] [G loss: 0.419128]\n",
      "epoch:12 step:11325 [D loss: 0.231206, acc.: 60.94%] [G loss: 0.417989]\n",
      "epoch:12 step:11326 [D loss: 0.227019, acc.: 57.03%] [G loss: 0.427690]\n",
      "epoch:12 step:11327 [D loss: 0.190502, acc.: 73.44%] [G loss: 0.485369]\n",
      "epoch:12 step:11328 [D loss: 0.202537, acc.: 67.97%] [G loss: 0.455958]\n",
      "epoch:12 step:11329 [D loss: 0.214877, acc.: 65.62%] [G loss: 0.490251]\n",
      "epoch:12 step:11330 [D loss: 0.251656, acc.: 60.94%] [G loss: 0.436516]\n",
      "epoch:12 step:11331 [D loss: 0.223231, acc.: 67.97%] [G loss: 0.410158]\n",
      "epoch:12 step:11332 [D loss: 0.205891, acc.: 64.06%] [G loss: 0.468011]\n",
      "epoch:12 step:11333 [D loss: 0.200100, acc.: 71.88%] [G loss: 0.453903]\n",
      "epoch:12 step:11334 [D loss: 0.216434, acc.: 62.50%] [G loss: 0.442372]\n",
      "epoch:12 step:11335 [D loss: 0.221143, acc.: 66.41%] [G loss: 0.459373]\n",
      "epoch:12 step:11336 [D loss: 0.216084, acc.: 65.62%] [G loss: 0.439727]\n",
      "epoch:12 step:11337 [D loss: 0.208838, acc.: 67.19%] [G loss: 0.456047]\n",
      "epoch:12 step:11338 [D loss: 0.244757, acc.: 57.03%] [G loss: 0.481668]\n",
      "epoch:12 step:11339 [D loss: 0.218189, acc.: 67.97%] [G loss: 0.463442]\n",
      "epoch:12 step:11340 [D loss: 0.204554, acc.: 71.88%] [G loss: 0.444197]\n",
      "epoch:12 step:11341 [D loss: 0.195169, acc.: 68.75%] [G loss: 0.523852]\n",
      "epoch:12 step:11342 [D loss: 0.238775, acc.: 59.38%] [G loss: 0.433529]\n",
      "epoch:12 step:11343 [D loss: 0.240077, acc.: 59.38%] [G loss: 0.461254]\n",
      "epoch:12 step:11344 [D loss: 0.182351, acc.: 70.31%] [G loss: 0.502382]\n",
      "epoch:12 step:11345 [D loss: 0.249899, acc.: 54.69%] [G loss: 0.442108]\n",
      "epoch:12 step:11346 [D loss: 0.268173, acc.: 49.22%] [G loss: 0.450826]\n",
      "epoch:12 step:11347 [D loss: 0.219754, acc.: 64.06%] [G loss: 0.448935]\n",
      "epoch:12 step:11348 [D loss: 0.220638, acc.: 64.84%] [G loss: 0.420123]\n",
      "epoch:12 step:11349 [D loss: 0.273473, acc.: 50.00%] [G loss: 0.399089]\n",
      "epoch:12 step:11350 [D loss: 0.221242, acc.: 61.72%] [G loss: 0.432489]\n",
      "epoch:12 step:11351 [D loss: 0.201268, acc.: 67.97%] [G loss: 0.473601]\n",
      "epoch:12 step:11352 [D loss: 0.266691, acc.: 53.12%] [G loss: 0.451867]\n",
      "epoch:12 step:11353 [D loss: 0.246166, acc.: 59.38%] [G loss: 0.420096]\n",
      "epoch:12 step:11354 [D loss: 0.244186, acc.: 52.34%] [G loss: 0.430818]\n",
      "epoch:12 step:11355 [D loss: 0.207988, acc.: 69.53%] [G loss: 0.448957]\n",
      "epoch:12 step:11356 [D loss: 0.193745, acc.: 75.00%] [G loss: 0.469473]\n",
      "epoch:12 step:11357 [D loss: 0.208605, acc.: 65.62%] [G loss: 0.491229]\n",
      "epoch:12 step:11358 [D loss: 0.216881, acc.: 60.94%] [G loss: 0.453329]\n",
      "epoch:12 step:11359 [D loss: 0.197050, acc.: 72.66%] [G loss: 0.489830]\n",
      "epoch:12 step:11360 [D loss: 0.220264, acc.: 63.28%] [G loss: 0.491631]\n",
      "epoch:12 step:11361 [D loss: 0.216626, acc.: 60.94%] [G loss: 0.507689]\n",
      "epoch:12 step:11362 [D loss: 0.231024, acc.: 63.28%] [G loss: 0.498165]\n",
      "epoch:12 step:11363 [D loss: 0.198222, acc.: 70.31%] [G loss: 0.511206]\n",
      "epoch:12 step:11364 [D loss: 0.255742, acc.: 56.25%] [G loss: 0.487548]\n",
      "epoch:12 step:11365 [D loss: 0.233138, acc.: 62.50%] [G loss: 0.461625]\n",
      "epoch:12 step:11366 [D loss: 0.211062, acc.: 65.62%] [G loss: 0.438081]\n",
      "epoch:12 step:11367 [D loss: 0.191461, acc.: 69.53%] [G loss: 0.448311]\n",
      "epoch:12 step:11368 [D loss: 0.255166, acc.: 53.91%] [G loss: 0.425570]\n",
      "epoch:12 step:11369 [D loss: 0.242473, acc.: 59.38%] [G loss: 0.424860]\n",
      "epoch:12 step:11370 [D loss: 0.196299, acc.: 66.41%] [G loss: 0.461586]\n",
      "epoch:12 step:11371 [D loss: 0.213017, acc.: 63.28%] [G loss: 0.425140]\n",
      "epoch:12 step:11372 [D loss: 0.255248, acc.: 57.03%] [G loss: 0.415756]\n",
      "epoch:12 step:11373 [D loss: 0.219023, acc.: 62.50%] [G loss: 0.425301]\n",
      "epoch:12 step:11374 [D loss: 0.222822, acc.: 64.84%] [G loss: 0.439555]\n",
      "epoch:12 step:11375 [D loss: 0.193900, acc.: 71.88%] [G loss: 0.462105]\n",
      "epoch:12 step:11376 [D loss: 0.220864, acc.: 64.84%] [G loss: 0.459607]\n",
      "epoch:12 step:11377 [D loss: 0.246523, acc.: 57.03%] [G loss: 0.460193]\n",
      "epoch:12 step:11378 [D loss: 0.221914, acc.: 63.28%] [G loss: 0.436116]\n",
      "epoch:12 step:11379 [D loss: 0.230747, acc.: 64.06%] [G loss: 0.426695]\n",
      "epoch:12 step:11380 [D loss: 0.230263, acc.: 60.16%] [G loss: 0.477140]\n",
      "epoch:12 step:11381 [D loss: 0.269169, acc.: 53.12%] [G loss: 0.398095]\n",
      "epoch:12 step:11382 [D loss: 0.237772, acc.: 61.72%] [G loss: 0.420539]\n",
      "epoch:12 step:11383 [D loss: 0.224051, acc.: 66.41%] [G loss: 0.449086]\n",
      "epoch:12 step:11384 [D loss: 0.239279, acc.: 54.69%] [G loss: 0.413516]\n",
      "epoch:12 step:11385 [D loss: 0.243932, acc.: 63.28%] [G loss: 0.472883]\n",
      "epoch:12 step:11386 [D loss: 0.227894, acc.: 57.81%] [G loss: 0.425003]\n",
      "epoch:12 step:11387 [D loss: 0.255445, acc.: 52.34%] [G loss: 0.453571]\n",
      "epoch:12 step:11388 [D loss: 0.221097, acc.: 63.28%] [G loss: 0.442354]\n",
      "epoch:12 step:11389 [D loss: 0.232652, acc.: 60.16%] [G loss: 0.451409]\n",
      "epoch:12 step:11390 [D loss: 0.226440, acc.: 61.72%] [G loss: 0.448515]\n",
      "epoch:12 step:11391 [D loss: 0.255961, acc.: 56.25%] [G loss: 0.404277]\n",
      "epoch:12 step:11392 [D loss: 0.243926, acc.: 58.59%] [G loss: 0.421730]\n",
      "epoch:12 step:11393 [D loss: 0.217259, acc.: 64.84%] [G loss: 0.431721]\n",
      "epoch:12 step:11394 [D loss: 0.243072, acc.: 56.25%] [G loss: 0.432992]\n",
      "epoch:12 step:11395 [D loss: 0.215058, acc.: 66.41%] [G loss: 0.420985]\n",
      "epoch:12 step:11396 [D loss: 0.224867, acc.: 68.75%] [G loss: 0.424329]\n",
      "epoch:12 step:11397 [D loss: 0.229612, acc.: 64.06%] [G loss: 0.421086]\n",
      "epoch:12 step:11398 [D loss: 0.201188, acc.: 67.97%] [G loss: 0.426671]\n",
      "epoch:12 step:11399 [D loss: 0.202840, acc.: 71.09%] [G loss: 0.454551]\n",
      "epoch:12 step:11400 [D loss: 0.247509, acc.: 53.12%] [G loss: 0.399743]\n",
      "##############\n",
      "[2.47539835 1.61473252 5.98025955 4.72167425 3.7229271  5.50341385\n",
      " 4.5773246  4.75837557 4.59071559 4.04231612]\n",
      "##########\n",
      "epoch:12 step:11401 [D loss: 0.248029, acc.: 57.81%] [G loss: 0.470654]\n",
      "epoch:12 step:11402 [D loss: 0.248486, acc.: 53.12%] [G loss: 0.438533]\n",
      "epoch:12 step:11403 [D loss: 0.223014, acc.: 64.06%] [G loss: 0.447827]\n",
      "epoch:12 step:11404 [D loss: 0.282958, acc.: 53.91%] [G loss: 0.433180]\n",
      "epoch:12 step:11405 [D loss: 0.221927, acc.: 63.28%] [G loss: 0.508410]\n",
      "epoch:12 step:11406 [D loss: 0.241633, acc.: 60.94%] [G loss: 0.455365]\n",
      "epoch:12 step:11407 [D loss: 0.247669, acc.: 56.25%] [G loss: 0.419262]\n",
      "epoch:12 step:11408 [D loss: 0.204193, acc.: 71.09%] [G loss: 0.468172]\n",
      "epoch:12 step:11409 [D loss: 0.229403, acc.: 63.28%] [G loss: 0.431283]\n",
      "epoch:12 step:11410 [D loss: 0.234798, acc.: 59.38%] [G loss: 0.436229]\n",
      "epoch:12 step:11411 [D loss: 0.224253, acc.: 59.38%] [G loss: 0.409303]\n",
      "epoch:12 step:11412 [D loss: 0.213372, acc.: 67.19%] [G loss: 0.435393]\n",
      "epoch:12 step:11413 [D loss: 0.262963, acc.: 53.12%] [G loss: 0.418303]\n",
      "epoch:12 step:11414 [D loss: 0.271992, acc.: 53.91%] [G loss: 0.378726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11415 [D loss: 0.214826, acc.: 61.72%] [G loss: 0.461708]\n",
      "epoch:12 step:11416 [D loss: 0.226115, acc.: 59.38%] [G loss: 0.441116]\n",
      "epoch:12 step:11417 [D loss: 0.225335, acc.: 67.97%] [G loss: 0.436797]\n",
      "epoch:12 step:11418 [D loss: 0.217273, acc.: 68.75%] [G loss: 0.449940]\n",
      "epoch:12 step:11419 [D loss: 0.219520, acc.: 61.72%] [G loss: 0.442077]\n",
      "epoch:12 step:11420 [D loss: 0.189142, acc.: 68.75%] [G loss: 0.458219]\n",
      "epoch:12 step:11421 [D loss: 0.227333, acc.: 64.84%] [G loss: 0.433354]\n",
      "epoch:12 step:11422 [D loss: 0.220072, acc.: 65.62%] [G loss: 0.446966]\n",
      "epoch:12 step:11423 [D loss: 0.230902, acc.: 63.28%] [G loss: 0.411203]\n",
      "epoch:12 step:11424 [D loss: 0.223190, acc.: 62.50%] [G loss: 0.445153]\n",
      "epoch:12 step:11425 [D loss: 0.237032, acc.: 60.16%] [G loss: 0.386581]\n",
      "epoch:12 step:11426 [D loss: 0.267279, acc.: 54.69%] [G loss: 0.413010]\n",
      "epoch:12 step:11427 [D loss: 0.216356, acc.: 67.97%] [G loss: 0.410936]\n",
      "epoch:12 step:11428 [D loss: 0.218047, acc.: 66.41%] [G loss: 0.410372]\n",
      "epoch:12 step:11429 [D loss: 0.219803, acc.: 67.19%] [G loss: 0.406897]\n",
      "epoch:12 step:11430 [D loss: 0.243455, acc.: 60.94%] [G loss: 0.416978]\n",
      "epoch:12 step:11431 [D loss: 0.223253, acc.: 64.06%] [G loss: 0.431801]\n",
      "epoch:12 step:11432 [D loss: 0.235127, acc.: 60.94%] [G loss: 0.450051]\n",
      "epoch:12 step:11433 [D loss: 0.227698, acc.: 59.38%] [G loss: 0.461502]\n",
      "epoch:12 step:11434 [D loss: 0.223185, acc.: 63.28%] [G loss: 0.434479]\n",
      "epoch:12 step:11435 [D loss: 0.205923, acc.: 67.97%] [G loss: 0.410335]\n",
      "epoch:12 step:11436 [D loss: 0.209588, acc.: 67.19%] [G loss: 0.480438]\n",
      "epoch:12 step:11437 [D loss: 0.208597, acc.: 68.75%] [G loss: 0.437014]\n",
      "epoch:12 step:11438 [D loss: 0.199203, acc.: 71.09%] [G loss: 0.453698]\n",
      "epoch:12 step:11439 [D loss: 0.203164, acc.: 68.75%] [G loss: 0.454854]\n",
      "epoch:12 step:11440 [D loss: 0.237175, acc.: 58.59%] [G loss: 0.470401]\n",
      "epoch:12 step:11441 [D loss: 0.201995, acc.: 67.97%] [G loss: 0.454430]\n",
      "epoch:12 step:11442 [D loss: 0.195466, acc.: 70.31%] [G loss: 0.468553]\n",
      "epoch:12 step:11443 [D loss: 0.211128, acc.: 60.16%] [G loss: 0.466882]\n",
      "epoch:12 step:11444 [D loss: 0.259418, acc.: 55.47%] [G loss: 0.427658]\n",
      "epoch:12 step:11445 [D loss: 0.227878, acc.: 64.84%] [G loss: 0.420386]\n",
      "epoch:12 step:11446 [D loss: 0.227167, acc.: 60.94%] [G loss: 0.445400]\n",
      "epoch:12 step:11447 [D loss: 0.276411, acc.: 46.09%] [G loss: 0.430286]\n",
      "epoch:12 step:11448 [D loss: 0.210040, acc.: 67.19%] [G loss: 0.447177]\n",
      "epoch:12 step:11449 [D loss: 0.238652, acc.: 58.59%] [G loss: 0.446373]\n",
      "epoch:12 step:11450 [D loss: 0.221417, acc.: 62.50%] [G loss: 0.462465]\n",
      "epoch:12 step:11451 [D loss: 0.208922, acc.: 60.94%] [G loss: 0.469493]\n",
      "epoch:12 step:11452 [D loss: 0.188470, acc.: 69.53%] [G loss: 0.489885]\n",
      "epoch:12 step:11453 [D loss: 0.187296, acc.: 76.56%] [G loss: 0.500920]\n",
      "epoch:12 step:11454 [D loss: 0.264449, acc.: 56.25%] [G loss: 0.435012]\n",
      "epoch:12 step:11455 [D loss: 0.239781, acc.: 58.59%] [G loss: 0.440380]\n",
      "epoch:12 step:11456 [D loss: 0.251088, acc.: 61.72%] [G loss: 0.411827]\n",
      "epoch:12 step:11457 [D loss: 0.201427, acc.: 64.84%] [G loss: 0.446424]\n",
      "epoch:12 step:11458 [D loss: 0.268403, acc.: 50.00%] [G loss: 0.387264]\n",
      "epoch:12 step:11459 [D loss: 0.239058, acc.: 60.16%] [G loss: 0.448951]\n",
      "epoch:12 step:11460 [D loss: 0.211821, acc.: 64.06%] [G loss: 0.426962]\n",
      "epoch:12 step:11461 [D loss: 0.213893, acc.: 66.41%] [G loss: 0.440361]\n",
      "epoch:12 step:11462 [D loss: 0.204339, acc.: 68.75%] [G loss: 0.448234]\n",
      "epoch:12 step:11463 [D loss: 0.185948, acc.: 71.88%] [G loss: 0.487687]\n",
      "epoch:12 step:11464 [D loss: 0.294078, acc.: 50.78%] [G loss: 0.463090]\n",
      "epoch:12 step:11465 [D loss: 0.207620, acc.: 69.53%] [G loss: 0.458744]\n",
      "epoch:12 step:11466 [D loss: 0.222435, acc.: 66.41%] [G loss: 0.476876]\n",
      "epoch:12 step:11467 [D loss: 0.190101, acc.: 73.44%] [G loss: 0.526852]\n",
      "epoch:12 step:11468 [D loss: 0.244500, acc.: 60.16%] [G loss: 0.463800]\n",
      "epoch:12 step:11469 [D loss: 0.239233, acc.: 64.06%] [G loss: 0.424952]\n",
      "epoch:12 step:11470 [D loss: 0.224235, acc.: 63.28%] [G loss: 0.402692]\n",
      "epoch:12 step:11471 [D loss: 0.222489, acc.: 60.16%] [G loss: 0.420280]\n",
      "epoch:12 step:11472 [D loss: 0.246785, acc.: 55.47%] [G loss: 0.400011]\n",
      "epoch:12 step:11473 [D loss: 0.215499, acc.: 64.06%] [G loss: 0.421520]\n",
      "epoch:12 step:11474 [D loss: 0.204097, acc.: 64.06%] [G loss: 0.447311]\n",
      "epoch:12 step:11475 [D loss: 0.181762, acc.: 74.22%] [G loss: 0.505427]\n",
      "epoch:12 step:11476 [D loss: 0.173463, acc.: 72.66%] [G loss: 0.565172]\n",
      "epoch:12 step:11477 [D loss: 0.283062, acc.: 53.91%] [G loss: 0.413919]\n",
      "epoch:12 step:11478 [D loss: 0.242882, acc.: 60.94%] [G loss: 0.445593]\n",
      "epoch:12 step:11479 [D loss: 0.226952, acc.: 63.28%] [G loss: 0.449103]\n",
      "epoch:12 step:11480 [D loss: 0.199458, acc.: 66.41%] [G loss: 0.447613]\n",
      "epoch:12 step:11481 [D loss: 0.209029, acc.: 67.19%] [G loss: 0.438635]\n",
      "epoch:12 step:11482 [D loss: 0.229720, acc.: 57.03%] [G loss: 0.449202]\n",
      "epoch:12 step:11483 [D loss: 0.207970, acc.: 67.97%] [G loss: 0.466086]\n",
      "epoch:12 step:11484 [D loss: 0.222059, acc.: 59.38%] [G loss: 0.491866]\n",
      "epoch:12 step:11485 [D loss: 0.213424, acc.: 67.97%] [G loss: 0.474194]\n",
      "epoch:12 step:11486 [D loss: 0.199449, acc.: 72.66%] [G loss: 0.469053]\n",
      "epoch:12 step:11487 [D loss: 0.216898, acc.: 68.75%] [G loss: 0.506858]\n",
      "epoch:12 step:11488 [D loss: 0.198510, acc.: 75.78%] [G loss: 0.463344]\n",
      "epoch:12 step:11489 [D loss: 0.200635, acc.: 72.66%] [G loss: 0.475231]\n",
      "epoch:12 step:11490 [D loss: 0.207168, acc.: 67.19%] [G loss: 0.462136]\n",
      "epoch:12 step:11491 [D loss: 0.221876, acc.: 68.75%] [G loss: 0.427519]\n",
      "epoch:12 step:11492 [D loss: 0.202434, acc.: 66.41%] [G loss: 0.488782]\n",
      "epoch:12 step:11493 [D loss: 0.260541, acc.: 52.34%] [G loss: 0.465803]\n",
      "epoch:12 step:11494 [D loss: 0.252117, acc.: 57.03%] [G loss: 0.455798]\n",
      "epoch:12 step:11495 [D loss: 0.248069, acc.: 57.81%] [G loss: 0.432087]\n",
      "epoch:12 step:11496 [D loss: 0.225808, acc.: 65.62%] [G loss: 0.449255]\n",
      "epoch:12 step:11497 [D loss: 0.242679, acc.: 58.59%] [G loss: 0.478566]\n",
      "epoch:12 step:11498 [D loss: 0.229865, acc.: 62.50%] [G loss: 0.436144]\n",
      "epoch:12 step:11499 [D loss: 0.229854, acc.: 63.28%] [G loss: 0.434019]\n",
      "epoch:12 step:11500 [D loss: 0.239380, acc.: 61.72%] [G loss: 0.391020]\n",
      "epoch:12 step:11501 [D loss: 0.235843, acc.: 60.16%] [G loss: 0.403219]\n",
      "epoch:12 step:11502 [D loss: 0.214651, acc.: 71.09%] [G loss: 0.441091]\n",
      "epoch:12 step:11503 [D loss: 0.198679, acc.: 67.97%] [G loss: 0.459258]\n",
      "epoch:12 step:11504 [D loss: 0.224760, acc.: 64.06%] [G loss: 0.462067]\n",
      "epoch:12 step:11505 [D loss: 0.217473, acc.: 64.06%] [G loss: 0.470248]\n",
      "epoch:12 step:11506 [D loss: 0.217693, acc.: 58.59%] [G loss: 0.435977]\n",
      "epoch:12 step:11507 [D loss: 0.250915, acc.: 54.69%] [G loss: 0.446659]\n",
      "epoch:12 step:11508 [D loss: 0.214045, acc.: 67.97%] [G loss: 0.494436]\n",
      "epoch:12 step:11509 [D loss: 0.235021, acc.: 60.16%] [G loss: 0.408773]\n",
      "epoch:12 step:11510 [D loss: 0.263922, acc.: 53.12%] [G loss: 0.385934]\n",
      "epoch:12 step:11511 [D loss: 0.251105, acc.: 56.25%] [G loss: 0.409882]\n",
      "epoch:12 step:11512 [D loss: 0.233079, acc.: 58.59%] [G loss: 0.438937]\n",
      "epoch:12 step:11513 [D loss: 0.193303, acc.: 72.66%] [G loss: 0.440718]\n",
      "epoch:12 step:11514 [D loss: 0.225077, acc.: 62.50%] [G loss: 0.449818]\n",
      "epoch:12 step:11515 [D loss: 0.220141, acc.: 61.72%] [G loss: 0.461385]\n",
      "epoch:12 step:11516 [D loss: 0.216195, acc.: 67.19%] [G loss: 0.431068]\n",
      "epoch:12 step:11517 [D loss: 0.220467, acc.: 62.50%] [G loss: 0.438382]\n",
      "epoch:12 step:11518 [D loss: 0.183855, acc.: 80.47%] [G loss: 0.459897]\n",
      "epoch:12 step:11519 [D loss: 0.222753, acc.: 60.16%] [G loss: 0.471305]\n",
      "epoch:12 step:11520 [D loss: 0.210956, acc.: 66.41%] [G loss: 0.476703]\n",
      "epoch:12 step:11521 [D loss: 0.276626, acc.: 50.78%] [G loss: 0.417047]\n",
      "epoch:12 step:11522 [D loss: 0.242851, acc.: 57.03%] [G loss: 0.420660]\n",
      "epoch:12 step:11523 [D loss: 0.231739, acc.: 64.84%] [G loss: 0.449922]\n",
      "epoch:12 step:11524 [D loss: 0.191960, acc.: 67.97%] [G loss: 0.478124]\n",
      "epoch:12 step:11525 [D loss: 0.253223, acc.: 55.47%] [G loss: 0.425980]\n",
      "epoch:12 step:11526 [D loss: 0.232719, acc.: 60.94%] [G loss: 0.423168]\n",
      "epoch:12 step:11527 [D loss: 0.224973, acc.: 64.06%] [G loss: 0.397541]\n",
      "epoch:12 step:11528 [D loss: 0.251180, acc.: 52.34%] [G loss: 0.420835]\n",
      "epoch:12 step:11529 [D loss: 0.215697, acc.: 66.41%] [G loss: 0.431649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11530 [D loss: 0.210970, acc.: 70.31%] [G loss: 0.429493]\n",
      "epoch:12 step:11531 [D loss: 0.237365, acc.: 61.72%] [G loss: 0.466940]\n",
      "epoch:12 step:11532 [D loss: 0.219716, acc.: 65.62%] [G loss: 0.475346]\n",
      "epoch:12 step:11533 [D loss: 0.219819, acc.: 64.06%] [G loss: 0.435810]\n",
      "epoch:12 step:11534 [D loss: 0.203104, acc.: 64.06%] [G loss: 0.487435]\n",
      "epoch:12 step:11535 [D loss: 0.258877, acc.: 55.47%] [G loss: 0.414152]\n",
      "epoch:12 step:11536 [D loss: 0.218769, acc.: 61.72%] [G loss: 0.437448]\n",
      "epoch:12 step:11537 [D loss: 0.194164, acc.: 71.09%] [G loss: 0.452318]\n",
      "epoch:12 step:11538 [D loss: 0.239508, acc.: 54.69%] [G loss: 0.422000]\n",
      "epoch:12 step:11539 [D loss: 0.236364, acc.: 64.06%] [G loss: 0.413529]\n",
      "epoch:12 step:11540 [D loss: 0.209879, acc.: 70.31%] [G loss: 0.452099]\n",
      "epoch:12 step:11541 [D loss: 0.209554, acc.: 64.06%] [G loss: 0.474294]\n",
      "epoch:12 step:11542 [D loss: 0.209877, acc.: 67.97%] [G loss: 0.456529]\n",
      "epoch:12 step:11543 [D loss: 0.217714, acc.: 60.94%] [G loss: 0.471975]\n",
      "epoch:12 step:11544 [D loss: 0.222719, acc.: 64.06%] [G loss: 0.435560]\n",
      "epoch:12 step:11545 [D loss: 0.240723, acc.: 58.59%] [G loss: 0.430181]\n",
      "epoch:12 step:11546 [D loss: 0.233600, acc.: 60.16%] [G loss: 0.457514]\n",
      "epoch:12 step:11547 [D loss: 0.223562, acc.: 64.06%] [G loss: 0.439668]\n",
      "epoch:12 step:11548 [D loss: 0.202791, acc.: 67.97%] [G loss: 0.462627]\n",
      "epoch:12 step:11549 [D loss: 0.218258, acc.: 66.41%] [G loss: 0.443116]\n",
      "epoch:12 step:11550 [D loss: 0.216542, acc.: 65.62%] [G loss: 0.454662]\n",
      "epoch:12 step:11551 [D loss: 0.242249, acc.: 58.59%] [G loss: 0.452017]\n",
      "epoch:12 step:11552 [D loss: 0.249337, acc.: 53.91%] [G loss: 0.413049]\n",
      "epoch:12 step:11553 [D loss: 0.184090, acc.: 72.66%] [G loss: 0.473129]\n",
      "epoch:12 step:11554 [D loss: 0.231312, acc.: 61.72%] [G loss: 0.446861]\n",
      "epoch:12 step:11555 [D loss: 0.196986, acc.: 69.53%] [G loss: 0.459625]\n",
      "epoch:12 step:11556 [D loss: 0.189640, acc.: 72.66%] [G loss: 0.507156]\n",
      "epoch:12 step:11557 [D loss: 0.186894, acc.: 71.09%] [G loss: 0.526433]\n",
      "epoch:12 step:11558 [D loss: 0.179614, acc.: 67.97%] [G loss: 0.535815]\n",
      "epoch:12 step:11559 [D loss: 0.192656, acc.: 71.09%] [G loss: 0.486100]\n",
      "epoch:12 step:11560 [D loss: 0.290438, acc.: 47.66%] [G loss: 0.406844]\n",
      "epoch:12 step:11561 [D loss: 0.220884, acc.: 62.50%] [G loss: 0.402545]\n",
      "epoch:12 step:11562 [D loss: 0.221972, acc.: 65.62%] [G loss: 0.425091]\n",
      "epoch:12 step:11563 [D loss: 0.208984, acc.: 69.53%] [G loss: 0.450648]\n",
      "epoch:12 step:11564 [D loss: 0.226010, acc.: 66.41%] [G loss: 0.438731]\n",
      "epoch:12 step:11565 [D loss: 0.155609, acc.: 78.12%] [G loss: 0.498973]\n",
      "epoch:12 step:11566 [D loss: 0.222117, acc.: 66.41%] [G loss: 0.455822]\n",
      "epoch:12 step:11567 [D loss: 0.266342, acc.: 52.34%] [G loss: 0.442208]\n",
      "epoch:12 step:11568 [D loss: 0.229565, acc.: 67.97%] [G loss: 0.431447]\n",
      "epoch:12 step:11569 [D loss: 0.239048, acc.: 57.81%] [G loss: 0.401504]\n",
      "epoch:12 step:11570 [D loss: 0.216031, acc.: 63.28%] [G loss: 0.435861]\n",
      "epoch:12 step:11571 [D loss: 0.208556, acc.: 64.84%] [G loss: 0.445845]\n",
      "epoch:12 step:11572 [D loss: 0.210195, acc.: 67.97%] [G loss: 0.463375]\n",
      "epoch:12 step:11573 [D loss: 0.220058, acc.: 64.84%] [G loss: 0.450890]\n",
      "epoch:12 step:11574 [D loss: 0.220470, acc.: 70.31%] [G loss: 0.443610]\n",
      "epoch:12 step:11575 [D loss: 0.220250, acc.: 62.50%] [G loss: 0.472880]\n",
      "epoch:12 step:11576 [D loss: 0.178642, acc.: 75.00%] [G loss: 0.486332]\n",
      "epoch:12 step:11577 [D loss: 0.224912, acc.: 59.38%] [G loss: 0.488006]\n",
      "epoch:12 step:11578 [D loss: 0.199150, acc.: 67.97%] [G loss: 0.483367]\n",
      "epoch:12 step:11579 [D loss: 0.222707, acc.: 60.94%] [G loss: 0.483694]\n",
      "epoch:12 step:11580 [D loss: 0.206113, acc.: 67.19%] [G loss: 0.493973]\n",
      "epoch:12 step:11581 [D loss: 0.230423, acc.: 60.94%] [G loss: 0.465716]\n",
      "epoch:12 step:11582 [D loss: 0.222424, acc.: 64.06%] [G loss: 0.481387]\n",
      "epoch:12 step:11583 [D loss: 0.218572, acc.: 62.50%] [G loss: 0.469516]\n",
      "epoch:12 step:11584 [D loss: 0.213555, acc.: 67.19%] [G loss: 0.474809]\n",
      "epoch:12 step:11585 [D loss: 0.256304, acc.: 57.03%] [G loss: 0.445452]\n",
      "epoch:12 step:11586 [D loss: 0.248605, acc.: 59.38%] [G loss: 0.427118]\n",
      "epoch:12 step:11587 [D loss: 0.194615, acc.: 71.09%] [G loss: 0.493047]\n",
      "epoch:12 step:11588 [D loss: 0.208380, acc.: 67.97%] [G loss: 0.498808]\n",
      "epoch:12 step:11589 [D loss: 0.238224, acc.: 58.59%] [G loss: 0.464139]\n",
      "epoch:12 step:11590 [D loss: 0.215367, acc.: 60.94%] [G loss: 0.452812]\n",
      "epoch:12 step:11591 [D loss: 0.180582, acc.: 75.78%] [G loss: 0.536902]\n",
      "epoch:12 step:11592 [D loss: 0.259861, acc.: 57.03%] [G loss: 0.443485]\n",
      "epoch:12 step:11593 [D loss: 0.286417, acc.: 46.09%] [G loss: 0.388745]\n",
      "epoch:12 step:11594 [D loss: 0.211979, acc.: 66.41%] [G loss: 0.448195]\n",
      "epoch:12 step:11595 [D loss: 0.207280, acc.: 65.62%] [G loss: 0.474685]\n",
      "epoch:12 step:11596 [D loss: 0.235146, acc.: 57.81%] [G loss: 0.404757]\n",
      "epoch:12 step:11597 [D loss: 0.216533, acc.: 60.94%] [G loss: 0.467185]\n",
      "epoch:12 step:11598 [D loss: 0.191060, acc.: 71.09%] [G loss: 0.498557]\n",
      "epoch:12 step:11599 [D loss: 0.240039, acc.: 59.38%] [G loss: 0.449911]\n",
      "epoch:12 step:11600 [D loss: 0.225965, acc.: 61.72%] [G loss: 0.449256]\n",
      "##############\n",
      "[2.55538975 1.72975619 6.10077972 4.71741326 3.64740556 5.59627437\n",
      " 4.45972552 4.74911862 4.56065209 3.68745912]\n",
      "##########\n",
      "epoch:12 step:11601 [D loss: 0.227881, acc.: 67.19%] [G loss: 0.434609]\n",
      "epoch:12 step:11602 [D loss: 0.203896, acc.: 65.62%] [G loss: 0.482181]\n",
      "epoch:12 step:11603 [D loss: 0.222030, acc.: 61.72%] [G loss: 0.444717]\n",
      "epoch:12 step:11604 [D loss: 0.203294, acc.: 65.62%] [G loss: 0.507994]\n",
      "epoch:12 step:11605 [D loss: 0.211992, acc.: 63.28%] [G loss: 0.451585]\n",
      "epoch:12 step:11606 [D loss: 0.248565, acc.: 54.69%] [G loss: 0.432227]\n",
      "epoch:12 step:11607 [D loss: 0.219383, acc.: 62.50%] [G loss: 0.481613]\n",
      "epoch:12 step:11608 [D loss: 0.210748, acc.: 67.19%] [G loss: 0.434291]\n",
      "epoch:12 step:11609 [D loss: 0.232795, acc.: 57.03%] [G loss: 0.471187]\n",
      "epoch:12 step:11610 [D loss: 0.232347, acc.: 60.16%] [G loss: 0.430088]\n",
      "epoch:12 step:11611 [D loss: 0.219014, acc.: 66.41%] [G loss: 0.477369]\n",
      "epoch:12 step:11612 [D loss: 0.250259, acc.: 57.81%] [G loss: 0.417252]\n",
      "epoch:12 step:11613 [D loss: 0.250100, acc.: 56.25%] [G loss: 0.429351]\n",
      "epoch:12 step:11614 [D loss: 0.186458, acc.: 75.00%] [G loss: 0.441737]\n",
      "epoch:12 step:11615 [D loss: 0.201951, acc.: 66.41%] [G loss: 0.512007]\n",
      "epoch:12 step:11616 [D loss: 0.226621, acc.: 67.19%] [G loss: 0.472097]\n",
      "epoch:12 step:11617 [D loss: 0.282310, acc.: 44.53%] [G loss: 0.430773]\n",
      "epoch:12 step:11618 [D loss: 0.178972, acc.: 71.09%] [G loss: 0.471699]\n",
      "epoch:12 step:11619 [D loss: 0.238226, acc.: 57.03%] [G loss: 0.419146]\n",
      "epoch:12 step:11620 [D loss: 0.251206, acc.: 57.81%] [G loss: 0.410316]\n",
      "epoch:12 step:11621 [D loss: 0.265639, acc.: 50.78%] [G loss: 0.421054]\n",
      "epoch:12 step:11622 [D loss: 0.243621, acc.: 64.06%] [G loss: 0.427274]\n",
      "epoch:12 step:11623 [D loss: 0.225003, acc.: 62.50%] [G loss: 0.462439]\n",
      "epoch:12 step:11624 [D loss: 0.214301, acc.: 64.84%] [G loss: 0.466834]\n",
      "epoch:12 step:11625 [D loss: 0.193109, acc.: 68.75%] [G loss: 0.480404]\n",
      "epoch:12 step:11626 [D loss: 0.222142, acc.: 67.19%] [G loss: 0.444534]\n",
      "epoch:12 step:11627 [D loss: 0.219434, acc.: 65.62%] [G loss: 0.414176]\n",
      "epoch:12 step:11628 [D loss: 0.213349, acc.: 68.75%] [G loss: 0.403156]\n",
      "epoch:12 step:11629 [D loss: 0.204278, acc.: 68.75%] [G loss: 0.421575]\n",
      "epoch:12 step:11630 [D loss: 0.223915, acc.: 62.50%] [G loss: 0.450174]\n",
      "epoch:12 step:11631 [D loss: 0.221163, acc.: 64.84%] [G loss: 0.456970]\n",
      "epoch:12 step:11632 [D loss: 0.211445, acc.: 67.97%] [G loss: 0.457391]\n",
      "epoch:12 step:11633 [D loss: 0.233753, acc.: 63.28%] [G loss: 0.423727]\n",
      "epoch:12 step:11634 [D loss: 0.273543, acc.: 49.22%] [G loss: 0.423576]\n",
      "epoch:12 step:11635 [D loss: 0.227034, acc.: 61.72%] [G loss: 0.428167]\n",
      "epoch:12 step:11636 [D loss: 0.222061, acc.: 60.94%] [G loss: 0.420066]\n",
      "epoch:12 step:11637 [D loss: 0.226240, acc.: 59.38%] [G loss: 0.406962]\n",
      "epoch:12 step:11638 [D loss: 0.236222, acc.: 63.28%] [G loss: 0.426147]\n",
      "epoch:12 step:11639 [D loss: 0.228239, acc.: 62.50%] [G loss: 0.423218]\n",
      "epoch:12 step:11640 [D loss: 0.233081, acc.: 58.59%] [G loss: 0.452427]\n",
      "epoch:12 step:11641 [D loss: 0.224539, acc.: 62.50%] [G loss: 0.449812]\n",
      "epoch:12 step:11642 [D loss: 0.180070, acc.: 75.00%] [G loss: 0.453287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11643 [D loss: 0.198093, acc.: 70.31%] [G loss: 0.428797]\n",
      "epoch:12 step:11644 [D loss: 0.245721, acc.: 59.38%] [G loss: 0.417489]\n",
      "epoch:12 step:11645 [D loss: 0.246463, acc.: 57.03%] [G loss: 0.433852]\n",
      "epoch:12 step:11646 [D loss: 0.197779, acc.: 75.00%] [G loss: 0.411265]\n",
      "epoch:12 step:11647 [D loss: 0.226961, acc.: 58.59%] [G loss: 0.475770]\n",
      "epoch:12 step:11648 [D loss: 0.234113, acc.: 66.41%] [G loss: 0.453764]\n",
      "epoch:12 step:11649 [D loss: 0.220751, acc.: 67.97%] [G loss: 0.523537]\n",
      "epoch:12 step:11650 [D loss: 0.219508, acc.: 65.62%] [G loss: 0.483524]\n",
      "epoch:12 step:11651 [D loss: 0.263193, acc.: 57.03%] [G loss: 0.461259]\n",
      "epoch:12 step:11652 [D loss: 0.267036, acc.: 50.00%] [G loss: 0.427408]\n",
      "epoch:12 step:11653 [D loss: 0.227231, acc.: 55.47%] [G loss: 0.460229]\n",
      "epoch:12 step:11654 [D loss: 0.246453, acc.: 58.59%] [G loss: 0.451363]\n",
      "epoch:12 step:11655 [D loss: 0.244195, acc.: 57.03%] [G loss: 0.428514]\n",
      "epoch:12 step:11656 [D loss: 0.212837, acc.: 67.97%] [G loss: 0.409486]\n",
      "epoch:12 step:11657 [D loss: 0.254325, acc.: 56.25%] [G loss: 0.440934]\n",
      "epoch:12 step:11658 [D loss: 0.222524, acc.: 67.19%] [G loss: 0.460415]\n",
      "epoch:12 step:11659 [D loss: 0.213675, acc.: 69.53%] [G loss: 0.452168]\n",
      "epoch:12 step:11660 [D loss: 0.208587, acc.: 66.41%] [G loss: 0.516529]\n",
      "epoch:12 step:11661 [D loss: 0.245654, acc.: 52.34%] [G loss: 0.474075]\n",
      "epoch:12 step:11662 [D loss: 0.248507, acc.: 60.16%] [G loss: 0.448018]\n",
      "epoch:12 step:11663 [D loss: 0.227496, acc.: 64.84%] [G loss: 0.431974]\n",
      "epoch:12 step:11664 [D loss: 0.221604, acc.: 63.28%] [G loss: 0.487744]\n",
      "epoch:12 step:11665 [D loss: 0.284879, acc.: 52.34%] [G loss: 0.433358]\n",
      "epoch:12 step:11666 [D loss: 0.224299, acc.: 64.06%] [G loss: 0.441140]\n",
      "epoch:12 step:11667 [D loss: 0.235400, acc.: 61.72%] [G loss: 0.431511]\n",
      "epoch:12 step:11668 [D loss: 0.231051, acc.: 59.38%] [G loss: 0.407993]\n",
      "epoch:12 step:11669 [D loss: 0.219952, acc.: 60.94%] [G loss: 0.423350]\n",
      "epoch:12 step:11670 [D loss: 0.220996, acc.: 68.75%] [G loss: 0.453625]\n",
      "epoch:12 step:11671 [D loss: 0.210835, acc.: 71.88%] [G loss: 0.483422]\n",
      "epoch:12 step:11672 [D loss: 0.215837, acc.: 62.50%] [G loss: 0.464356]\n",
      "epoch:12 step:11673 [D loss: 0.207419, acc.: 68.75%] [G loss: 0.492241]\n",
      "epoch:12 step:11674 [D loss: 0.180283, acc.: 77.34%] [G loss: 0.484352]\n",
      "epoch:12 step:11675 [D loss: 0.242233, acc.: 61.72%] [G loss: 0.442696]\n",
      "epoch:12 step:11676 [D loss: 0.231163, acc.: 57.03%] [G loss: 0.449037]\n",
      "epoch:12 step:11677 [D loss: 0.218719, acc.: 66.41%] [G loss: 0.431428]\n",
      "epoch:12 step:11678 [D loss: 0.188264, acc.: 73.44%] [G loss: 0.451472]\n",
      "epoch:12 step:11679 [D loss: 0.215403, acc.: 68.75%] [G loss: 0.476640]\n",
      "epoch:12 step:11680 [D loss: 0.199317, acc.: 71.09%] [G loss: 0.467038]\n",
      "epoch:12 step:11681 [D loss: 0.281784, acc.: 52.34%] [G loss: 0.500922]\n",
      "epoch:12 step:11682 [D loss: 0.246301, acc.: 57.81%] [G loss: 0.466438]\n",
      "epoch:12 step:11683 [D loss: 0.207861, acc.: 64.84%] [G loss: 0.458401]\n",
      "epoch:12 step:11684 [D loss: 0.218295, acc.: 60.16%] [G loss: 0.464041]\n",
      "epoch:12 step:11685 [D loss: 0.229481, acc.: 66.41%] [G loss: 0.434275]\n",
      "epoch:12 step:11686 [D loss: 0.230476, acc.: 61.72%] [G loss: 0.469369]\n",
      "epoch:12 step:11687 [D loss: 0.239398, acc.: 57.81%] [G loss: 0.464592]\n",
      "epoch:12 step:11688 [D loss: 0.235772, acc.: 61.72%] [G loss: 0.441102]\n",
      "epoch:12 step:11689 [D loss: 0.216150, acc.: 62.50%] [G loss: 0.449150]\n",
      "epoch:12 step:11690 [D loss: 0.205307, acc.: 67.19%] [G loss: 0.451973]\n",
      "epoch:12 step:11691 [D loss: 0.214554, acc.: 64.06%] [G loss: 0.436529]\n",
      "epoch:12 step:11692 [D loss: 0.234721, acc.: 63.28%] [G loss: 0.443970]\n",
      "epoch:12 step:11693 [D loss: 0.252654, acc.: 55.47%] [G loss: 0.433326]\n",
      "epoch:12 step:11694 [D loss: 0.205895, acc.: 64.06%] [G loss: 0.471746]\n",
      "epoch:12 step:11695 [D loss: 0.182007, acc.: 73.44%] [G loss: 0.462077]\n",
      "epoch:12 step:11696 [D loss: 0.202576, acc.: 66.41%] [G loss: 0.470445]\n",
      "epoch:12 step:11697 [D loss: 0.219463, acc.: 60.94%] [G loss: 0.487266]\n",
      "epoch:12 step:11698 [D loss: 0.221793, acc.: 65.62%] [G loss: 0.494924]\n",
      "epoch:12 step:11699 [D loss: 0.231983, acc.: 62.50%] [G loss: 0.445665]\n",
      "epoch:12 step:11700 [D loss: 0.252761, acc.: 52.34%] [G loss: 0.423257]\n",
      "epoch:12 step:11701 [D loss: 0.206466, acc.: 66.41%] [G loss: 0.463041]\n",
      "epoch:12 step:11702 [D loss: 0.285342, acc.: 55.47%] [G loss: 0.426049]\n",
      "epoch:12 step:11703 [D loss: 0.233103, acc.: 62.50%] [G loss: 0.405197]\n",
      "epoch:12 step:11704 [D loss: 0.217784, acc.: 65.62%] [G loss: 0.439211]\n",
      "epoch:12 step:11705 [D loss: 0.248170, acc.: 57.81%] [G loss: 0.428121]\n",
      "epoch:12 step:11706 [D loss: 0.233380, acc.: 57.03%] [G loss: 0.433007]\n",
      "epoch:12 step:11707 [D loss: 0.242221, acc.: 60.16%] [G loss: 0.414765]\n",
      "epoch:12 step:11708 [D loss: 0.219134, acc.: 64.06%] [G loss: 0.417712]\n",
      "epoch:12 step:11709 [D loss: 0.229608, acc.: 61.72%] [G loss: 0.462984]\n",
      "epoch:12 step:11710 [D loss: 0.229652, acc.: 65.62%] [G loss: 0.434905]\n",
      "epoch:12 step:11711 [D loss: 0.239596, acc.: 57.03%] [G loss: 0.442561]\n",
      "epoch:12 step:11712 [D loss: 0.220888, acc.: 63.28%] [G loss: 0.451597]\n",
      "epoch:12 step:11713 [D loss: 0.202759, acc.: 67.97%] [G loss: 0.485874]\n",
      "epoch:12 step:11714 [D loss: 0.195125, acc.: 67.19%] [G loss: 0.473167]\n",
      "epoch:12 step:11715 [D loss: 0.183149, acc.: 70.31%] [G loss: 0.498563]\n",
      "epoch:12 step:11716 [D loss: 0.197928, acc.: 74.22%] [G loss: 0.512254]\n",
      "epoch:12 step:11717 [D loss: 0.287624, acc.: 53.91%] [G loss: 0.408447]\n",
      "epoch:12 step:11718 [D loss: 0.197811, acc.: 71.88%] [G loss: 0.452499]\n",
      "epoch:12 step:11719 [D loss: 0.221833, acc.: 66.41%] [G loss: 0.472201]\n",
      "epoch:12 step:11720 [D loss: 0.250895, acc.: 60.16%] [G loss: 0.474513]\n",
      "epoch:12 step:11721 [D loss: 0.269537, acc.: 51.56%] [G loss: 0.434213]\n",
      "epoch:12 step:11722 [D loss: 0.241056, acc.: 56.25%] [G loss: 0.385508]\n",
      "epoch:12 step:11723 [D loss: 0.235360, acc.: 60.94%] [G loss: 0.405618]\n",
      "epoch:12 step:11724 [D loss: 0.205989, acc.: 67.97%] [G loss: 0.404013]\n",
      "epoch:12 step:11725 [D loss: 0.184916, acc.: 71.09%] [G loss: 0.472706]\n",
      "epoch:12 step:11726 [D loss: 0.251168, acc.: 53.91%] [G loss: 0.441191]\n",
      "epoch:12 step:11727 [D loss: 0.216842, acc.: 65.62%] [G loss: 0.431429]\n",
      "epoch:12 step:11728 [D loss: 0.197360, acc.: 71.88%] [G loss: 0.485686]\n",
      "epoch:12 step:11729 [D loss: 0.207200, acc.: 67.97%] [G loss: 0.515623]\n",
      "epoch:12 step:11730 [D loss: 0.263596, acc.: 51.56%] [G loss: 0.442870]\n",
      "epoch:12 step:11731 [D loss: 0.256931, acc.: 54.69%] [G loss: 0.418604]\n",
      "epoch:12 step:11732 [D loss: 0.191764, acc.: 73.44%] [G loss: 0.480046]\n",
      "epoch:12 step:11733 [D loss: 0.252905, acc.: 57.81%] [G loss: 0.435221]\n",
      "epoch:12 step:11734 [D loss: 0.227818, acc.: 64.06%] [G loss: 0.472094]\n",
      "epoch:12 step:11735 [D loss: 0.228603, acc.: 62.50%] [G loss: 0.464631]\n",
      "epoch:12 step:11736 [D loss: 0.237194, acc.: 58.59%] [G loss: 0.439974]\n",
      "epoch:12 step:11737 [D loss: 0.236100, acc.: 60.94%] [G loss: 0.416283]\n",
      "epoch:12 step:11738 [D loss: 0.218028, acc.: 69.53%] [G loss: 0.416713]\n",
      "epoch:12 step:11739 [D loss: 0.209889, acc.: 63.28%] [G loss: 0.478996]\n",
      "epoch:12 step:11740 [D loss: 0.215608, acc.: 67.97%] [G loss: 0.472619]\n",
      "epoch:12 step:11741 [D loss: 0.208548, acc.: 67.19%] [G loss: 0.461780]\n",
      "epoch:12 step:11742 [D loss: 0.200067, acc.: 71.09%] [G loss: 0.507501]\n",
      "epoch:12 step:11743 [D loss: 0.192319, acc.: 72.66%] [G loss: 0.516089]\n",
      "epoch:12 step:11744 [D loss: 0.256864, acc.: 53.12%] [G loss: 0.442901]\n",
      "epoch:12 step:11745 [D loss: 0.270700, acc.: 52.34%] [G loss: 0.385995]\n",
      "epoch:12 step:11746 [D loss: 0.228494, acc.: 64.84%] [G loss: 0.427943]\n",
      "epoch:12 step:11747 [D loss: 0.241376, acc.: 57.03%] [G loss: 0.432551]\n",
      "epoch:12 step:11748 [D loss: 0.201765, acc.: 62.50%] [G loss: 0.461156]\n",
      "epoch:12 step:11749 [D loss: 0.203034, acc.: 70.31%] [G loss: 0.509599]\n",
      "epoch:12 step:11750 [D loss: 0.223508, acc.: 62.50%] [G loss: 0.442413]\n",
      "epoch:12 step:11751 [D loss: 0.241681, acc.: 61.72%] [G loss: 0.434135]\n",
      "epoch:12 step:11752 [D loss: 0.192602, acc.: 69.53%] [G loss: 0.476607]\n",
      "epoch:12 step:11753 [D loss: 0.236372, acc.: 64.06%] [G loss: 0.466149]\n",
      "epoch:12 step:11754 [D loss: 0.244064, acc.: 59.38%] [G loss: 0.433754]\n",
      "epoch:12 step:11755 [D loss: 0.236488, acc.: 57.03%] [G loss: 0.413424]\n",
      "epoch:12 step:11756 [D loss: 0.228303, acc.: 57.03%] [G loss: 0.444256]\n",
      "epoch:12 step:11757 [D loss: 0.209748, acc.: 66.41%] [G loss: 0.448537]\n",
      "epoch:12 step:11758 [D loss: 0.205066, acc.: 68.75%] [G loss: 0.449470]\n",
      "epoch:12 step:11759 [D loss: 0.214857, acc.: 67.97%] [G loss: 0.484893]\n",
      "epoch:12 step:11760 [D loss: 0.224236, acc.: 66.41%] [G loss: 0.510561]\n",
      "epoch:12 step:11761 [D loss: 0.230880, acc.: 62.50%] [G loss: 0.462595]\n",
      "epoch:12 step:11762 [D loss: 0.233535, acc.: 63.28%] [G loss: 0.468759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11763 [D loss: 0.221530, acc.: 61.72%] [G loss: 0.451609]\n",
      "epoch:12 step:11764 [D loss: 0.193638, acc.: 72.66%] [G loss: 0.467264]\n",
      "epoch:12 step:11765 [D loss: 0.215851, acc.: 62.50%] [G loss: 0.413162]\n",
      "epoch:12 step:11766 [D loss: 0.224207, acc.: 64.06%] [G loss: 0.447798]\n",
      "epoch:12 step:11767 [D loss: 0.197068, acc.: 66.41%] [G loss: 0.458677]\n",
      "epoch:12 step:11768 [D loss: 0.263043, acc.: 57.81%] [G loss: 0.438955]\n",
      "epoch:12 step:11769 [D loss: 0.203323, acc.: 69.53%] [G loss: 0.438722]\n",
      "epoch:12 step:11770 [D loss: 0.210857, acc.: 69.53%] [G loss: 0.457691]\n",
      "epoch:12 step:11771 [D loss: 0.242013, acc.: 57.81%] [G loss: 0.460684]\n",
      "epoch:12 step:11772 [D loss: 0.282428, acc.: 49.22%] [G loss: 0.402385]\n",
      "epoch:12 step:11773 [D loss: 0.233012, acc.: 58.59%] [G loss: 0.427856]\n",
      "epoch:12 step:11774 [D loss: 0.218349, acc.: 69.53%] [G loss: 0.494787]\n",
      "epoch:12 step:11775 [D loss: 0.256109, acc.: 58.59%] [G loss: 0.471690]\n",
      "epoch:12 step:11776 [D loss: 0.242504, acc.: 59.38%] [G loss: 0.430936]\n",
      "epoch:12 step:11777 [D loss: 0.224505, acc.: 57.03%] [G loss: 0.436979]\n",
      "epoch:12 step:11778 [D loss: 0.198586, acc.: 68.75%] [G loss: 0.482013]\n",
      "epoch:12 step:11779 [D loss: 0.254568, acc.: 56.25%] [G loss: 0.436927]\n",
      "epoch:12 step:11780 [D loss: 0.196826, acc.: 75.78%] [G loss: 0.464658]\n",
      "epoch:12 step:11781 [D loss: 0.225172, acc.: 57.81%] [G loss: 0.438301]\n",
      "epoch:12 step:11782 [D loss: 0.251931, acc.: 53.12%] [G loss: 0.409915]\n",
      "epoch:12 step:11783 [D loss: 0.217665, acc.: 62.50%] [G loss: 0.457595]\n",
      "epoch:12 step:11784 [D loss: 0.233396, acc.: 57.81%] [G loss: 0.437534]\n",
      "epoch:12 step:11785 [D loss: 0.209111, acc.: 64.84%] [G loss: 0.455017]\n",
      "epoch:12 step:11786 [D loss: 0.267337, acc.: 53.91%] [G loss: 0.409815]\n",
      "epoch:12 step:11787 [D loss: 0.240944, acc.: 60.94%] [G loss: 0.426472]\n",
      "epoch:12 step:11788 [D loss: 0.215027, acc.: 65.62%] [G loss: 0.427929]\n",
      "epoch:12 step:11789 [D loss: 0.212382, acc.: 64.84%] [G loss: 0.458902]\n",
      "epoch:12 step:11790 [D loss: 0.241890, acc.: 57.81%] [G loss: 0.433071]\n",
      "epoch:12 step:11791 [D loss: 0.216147, acc.: 64.84%] [G loss: 0.437100]\n",
      "epoch:12 step:11792 [D loss: 0.221272, acc.: 60.94%] [G loss: 0.427466]\n",
      "epoch:12 step:11793 [D loss: 0.204844, acc.: 70.31%] [G loss: 0.485347]\n",
      "epoch:12 step:11794 [D loss: 0.213662, acc.: 69.53%] [G loss: 0.492624]\n",
      "epoch:12 step:11795 [D loss: 0.211094, acc.: 69.53%] [G loss: 0.483753]\n",
      "epoch:12 step:11796 [D loss: 0.217279, acc.: 60.94%] [G loss: 0.488429]\n",
      "epoch:12 step:11797 [D loss: 0.245713, acc.: 59.38%] [G loss: 0.440664]\n",
      "epoch:12 step:11798 [D loss: 0.220288, acc.: 61.72%] [G loss: 0.448676]\n",
      "epoch:12 step:11799 [D loss: 0.185612, acc.: 76.56%] [G loss: 0.471354]\n",
      "epoch:12 step:11800 [D loss: 0.205538, acc.: 73.44%] [G loss: 0.465969]\n",
      "##############\n",
      "[2.60980277 1.84065247 6.0213868  4.75927567 3.64059294 5.39101884\n",
      " 4.48460503 4.97743179 4.55911194 3.92509716]\n",
      "##########\n",
      "epoch:12 step:11801 [D loss: 0.177889, acc.: 73.44%] [G loss: 0.450352]\n",
      "epoch:12 step:11802 [D loss: 0.205173, acc.: 67.97%] [G loss: 0.411839]\n",
      "epoch:12 step:11803 [D loss: 0.234909, acc.: 58.59%] [G loss: 0.466873]\n",
      "epoch:12 step:11804 [D loss: 0.260411, acc.: 55.47%] [G loss: 0.453520]\n",
      "epoch:12 step:11805 [D loss: 0.232924, acc.: 62.50%] [G loss: 0.484152]\n",
      "epoch:12 step:11806 [D loss: 0.232091, acc.: 59.38%] [G loss: 0.463411]\n",
      "epoch:12 step:11807 [D loss: 0.193027, acc.: 69.53%] [G loss: 0.501531]\n",
      "epoch:12 step:11808 [D loss: 0.201782, acc.: 70.31%] [G loss: 0.537005]\n",
      "epoch:12 step:11809 [D loss: 0.229998, acc.: 64.06%] [G loss: 0.511241]\n",
      "epoch:12 step:11810 [D loss: 0.295153, acc.: 48.44%] [G loss: 0.387891]\n",
      "epoch:12 step:11811 [D loss: 0.204064, acc.: 69.53%] [G loss: 0.475299]\n",
      "epoch:12 step:11812 [D loss: 0.197752, acc.: 71.88%] [G loss: 0.468694]\n",
      "epoch:12 step:11813 [D loss: 0.286083, acc.: 49.22%] [G loss: 0.414101]\n",
      "epoch:12 step:11814 [D loss: 0.223705, acc.: 64.84%] [G loss: 0.401839]\n",
      "epoch:12 step:11815 [D loss: 0.218909, acc.: 64.84%] [G loss: 0.450230]\n",
      "epoch:12 step:11816 [D loss: 0.231387, acc.: 58.59%] [G loss: 0.438716]\n",
      "epoch:12 step:11817 [D loss: 0.236342, acc.: 60.94%] [G loss: 0.442040]\n",
      "epoch:12 step:11818 [D loss: 0.175081, acc.: 75.78%] [G loss: 0.474180]\n",
      "epoch:12 step:11819 [D loss: 0.206402, acc.: 71.88%] [G loss: 0.456052]\n",
      "epoch:12 step:11820 [D loss: 0.241149, acc.: 54.69%] [G loss: 0.464148]\n",
      "epoch:12 step:11821 [D loss: 0.219801, acc.: 60.94%] [G loss: 0.452607]\n",
      "epoch:12 step:11822 [D loss: 0.211107, acc.: 68.75%] [G loss: 0.421038]\n",
      "epoch:12 step:11823 [D loss: 0.253060, acc.: 59.38%] [G loss: 0.441139]\n",
      "epoch:12 step:11824 [D loss: 0.227108, acc.: 60.94%] [G loss: 0.451313]\n",
      "epoch:12 step:11825 [D loss: 0.213693, acc.: 62.50%] [G loss: 0.445768]\n",
      "epoch:12 step:11826 [D loss: 0.212160, acc.: 63.28%] [G loss: 0.445125]\n",
      "epoch:12 step:11827 [D loss: 0.229186, acc.: 64.06%] [G loss: 0.489866]\n",
      "epoch:12 step:11828 [D loss: 0.220554, acc.: 64.84%] [G loss: 0.437942]\n",
      "epoch:12 step:11829 [D loss: 0.258235, acc.: 55.47%] [G loss: 0.410700]\n",
      "epoch:12 step:11830 [D loss: 0.248970, acc.: 59.38%] [G loss: 0.443961]\n",
      "epoch:12 step:11831 [D loss: 0.256146, acc.: 51.56%] [G loss: 0.444367]\n",
      "epoch:12 step:11832 [D loss: 0.218063, acc.: 67.19%] [G loss: 0.444441]\n",
      "epoch:12 step:11833 [D loss: 0.180961, acc.: 77.34%] [G loss: 0.480093]\n",
      "epoch:12 step:11834 [D loss: 0.228922, acc.: 63.28%] [G loss: 0.430328]\n",
      "epoch:12 step:11835 [D loss: 0.261397, acc.: 57.81%] [G loss: 0.452032]\n",
      "epoch:12 step:11836 [D loss: 0.192263, acc.: 79.69%] [G loss: 0.475483]\n",
      "epoch:12 step:11837 [D loss: 0.216989, acc.: 62.50%] [G loss: 0.445542]\n",
      "epoch:12 step:11838 [D loss: 0.237154, acc.: 57.81%] [G loss: 0.399263]\n",
      "epoch:12 step:11839 [D loss: 0.202930, acc.: 71.09%] [G loss: 0.437844]\n",
      "epoch:12 step:11840 [D loss: 0.246820, acc.: 56.25%] [G loss: 0.457569]\n",
      "epoch:12 step:11841 [D loss: 0.232552, acc.: 63.28%] [G loss: 0.427699]\n",
      "epoch:12 step:11842 [D loss: 0.210321, acc.: 68.75%] [G loss: 0.427651]\n",
      "epoch:12 step:11843 [D loss: 0.254713, acc.: 49.22%] [G loss: 0.425815]\n",
      "epoch:12 step:11844 [D loss: 0.240056, acc.: 58.59%] [G loss: 0.431386]\n",
      "epoch:12 step:11845 [D loss: 0.222249, acc.: 60.94%] [G loss: 0.404310]\n",
      "epoch:12 step:11846 [D loss: 0.229563, acc.: 60.94%] [G loss: 0.425799]\n",
      "epoch:12 step:11847 [D loss: 0.222799, acc.: 57.03%] [G loss: 0.438380]\n",
      "epoch:12 step:11848 [D loss: 0.198708, acc.: 68.75%] [G loss: 0.490092]\n",
      "epoch:12 step:11849 [D loss: 0.237114, acc.: 64.84%] [G loss: 0.453357]\n",
      "epoch:12 step:11850 [D loss: 0.223565, acc.: 64.06%] [G loss: 0.435421]\n",
      "epoch:12 step:11851 [D loss: 0.229008, acc.: 57.03%] [G loss: 0.397268]\n",
      "epoch:12 step:11852 [D loss: 0.219367, acc.: 66.41%] [G loss: 0.425998]\n",
      "epoch:12 step:11853 [D loss: 0.215940, acc.: 63.28%] [G loss: 0.446791]\n",
      "epoch:12 step:11854 [D loss: 0.227715, acc.: 60.94%] [G loss: 0.413508]\n",
      "epoch:12 step:11855 [D loss: 0.231485, acc.: 62.50%] [G loss: 0.390818]\n",
      "epoch:12 step:11856 [D loss: 0.242637, acc.: 60.16%] [G loss: 0.424048]\n",
      "epoch:12 step:11857 [D loss: 0.203694, acc.: 67.97%] [G loss: 0.428794]\n",
      "epoch:12 step:11858 [D loss: 0.236049, acc.: 60.94%] [G loss: 0.448400]\n",
      "epoch:12 step:11859 [D loss: 0.248215, acc.: 52.34%] [G loss: 0.434360]\n",
      "epoch:12 step:11860 [D loss: 0.222335, acc.: 62.50%] [G loss: 0.457642]\n",
      "epoch:12 step:11861 [D loss: 0.211619, acc.: 70.31%] [G loss: 0.435638]\n",
      "epoch:12 step:11862 [D loss: 0.215493, acc.: 67.19%] [G loss: 0.462034]\n",
      "epoch:12 step:11863 [D loss: 0.228812, acc.: 60.16%] [G loss: 0.445124]\n",
      "epoch:12 step:11864 [D loss: 0.223402, acc.: 62.50%] [G loss: 0.456977]\n",
      "epoch:12 step:11865 [D loss: 0.239137, acc.: 57.03%] [G loss: 0.450944]\n",
      "epoch:12 step:11866 [D loss: 0.247812, acc.: 55.47%] [G loss: 0.399601]\n",
      "epoch:12 step:11867 [D loss: 0.190449, acc.: 71.09%] [G loss: 0.447590]\n",
      "epoch:12 step:11868 [D loss: 0.188919, acc.: 72.66%] [G loss: 0.455296]\n",
      "epoch:12 step:11869 [D loss: 0.226770, acc.: 59.38%] [G loss: 0.477065]\n",
      "epoch:12 step:11870 [D loss: 0.252005, acc.: 57.03%] [G loss: 0.420164]\n",
      "epoch:12 step:11871 [D loss: 0.223999, acc.: 62.50%] [G loss: 0.436077]\n",
      "epoch:12 step:11872 [D loss: 0.250643, acc.: 53.91%] [G loss: 0.435226]\n",
      "epoch:12 step:11873 [D loss: 0.221643, acc.: 67.97%] [G loss: 0.431650]\n",
      "epoch:12 step:11874 [D loss: 0.219638, acc.: 67.19%] [G loss: 0.472401]\n",
      "epoch:12 step:11875 [D loss: 0.204686, acc.: 70.31%] [G loss: 0.454075]\n",
      "epoch:12 step:11876 [D loss: 0.206336, acc.: 64.84%] [G loss: 0.449599]\n",
      "epoch:12 step:11877 [D loss: 0.224326, acc.: 64.84%] [G loss: 0.475203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11878 [D loss: 0.204322, acc.: 70.31%] [G loss: 0.481269]\n",
      "epoch:12 step:11879 [D loss: 0.222011, acc.: 61.72%] [G loss: 0.474298]\n",
      "epoch:12 step:11880 [D loss: 0.216069, acc.: 65.62%] [G loss: 0.454415]\n",
      "epoch:12 step:11881 [D loss: 0.215782, acc.: 66.41%] [G loss: 0.411583]\n",
      "epoch:12 step:11882 [D loss: 0.216796, acc.: 64.06%] [G loss: 0.434494]\n",
      "epoch:12 step:11883 [D loss: 0.229178, acc.: 60.16%] [G loss: 0.429675]\n",
      "epoch:12 step:11884 [D loss: 0.201081, acc.: 64.84%] [G loss: 0.471428]\n",
      "epoch:12 step:11885 [D loss: 0.228020, acc.: 63.28%] [G loss: 0.469478]\n",
      "epoch:12 step:11886 [D loss: 0.176774, acc.: 73.44%] [G loss: 0.480191]\n",
      "epoch:12 step:11887 [D loss: 0.270122, acc.: 46.88%] [G loss: 0.437118]\n",
      "epoch:12 step:11888 [D loss: 0.239822, acc.: 60.16%] [G loss: 0.489416]\n",
      "epoch:12 step:11889 [D loss: 0.229220, acc.: 63.28%] [G loss: 0.444512]\n",
      "epoch:12 step:11890 [D loss: 0.210518, acc.: 67.97%] [G loss: 0.450067]\n",
      "epoch:12 step:11891 [D loss: 0.222316, acc.: 64.06%] [G loss: 0.456069]\n",
      "epoch:12 step:11892 [D loss: 0.176535, acc.: 75.00%] [G loss: 0.518672]\n",
      "epoch:12 step:11893 [D loss: 0.223047, acc.: 66.41%] [G loss: 0.536749]\n",
      "epoch:12 step:11894 [D loss: 0.226860, acc.: 67.19%] [G loss: 0.529313]\n",
      "epoch:12 step:11895 [D loss: 0.245635, acc.: 59.38%] [G loss: 0.457148]\n",
      "epoch:12 step:11896 [D loss: 0.243522, acc.: 54.69%] [G loss: 0.452924]\n",
      "epoch:12 step:11897 [D loss: 0.204737, acc.: 68.75%] [G loss: 0.458357]\n",
      "epoch:12 step:11898 [D loss: 0.210564, acc.: 64.84%] [G loss: 0.470036]\n",
      "epoch:12 step:11899 [D loss: 0.237708, acc.: 57.81%] [G loss: 0.431031]\n",
      "epoch:12 step:11900 [D loss: 0.221397, acc.: 64.84%] [G loss: 0.469427]\n",
      "epoch:12 step:11901 [D loss: 0.236876, acc.: 54.69%] [G loss: 0.422611]\n",
      "epoch:12 step:11902 [D loss: 0.223920, acc.: 66.41%] [G loss: 0.459430]\n",
      "epoch:12 step:11903 [D loss: 0.207442, acc.: 64.84%] [G loss: 0.459644]\n",
      "epoch:12 step:11904 [D loss: 0.210713, acc.: 66.41%] [G loss: 0.432202]\n",
      "epoch:12 step:11905 [D loss: 0.220321, acc.: 60.94%] [G loss: 0.447023]\n",
      "epoch:12 step:11906 [D loss: 0.215566, acc.: 62.50%] [G loss: 0.464971]\n",
      "epoch:12 step:11907 [D loss: 0.220860, acc.: 59.38%] [G loss: 0.466859]\n",
      "epoch:12 step:11908 [D loss: 0.222678, acc.: 60.94%] [G loss: 0.470387]\n",
      "epoch:12 step:11909 [D loss: 0.219433, acc.: 59.38%] [G loss: 0.455869]\n",
      "epoch:12 step:11910 [D loss: 0.227445, acc.: 59.38%] [G loss: 0.460376]\n",
      "epoch:12 step:11911 [D loss: 0.244059, acc.: 55.47%] [G loss: 0.438706]\n",
      "epoch:12 step:11912 [D loss: 0.237604, acc.: 56.25%] [G loss: 0.406848]\n",
      "epoch:12 step:11913 [D loss: 0.204043, acc.: 71.88%] [G loss: 0.438318]\n",
      "epoch:12 step:11914 [D loss: 0.245745, acc.: 58.59%] [G loss: 0.406768]\n",
      "epoch:12 step:11915 [D loss: 0.233906, acc.: 59.38%] [G loss: 0.438209]\n",
      "epoch:12 step:11916 [D loss: 0.217564, acc.: 60.94%] [G loss: 0.453919]\n",
      "epoch:12 step:11917 [D loss: 0.224872, acc.: 64.84%] [G loss: 0.447193]\n",
      "epoch:12 step:11918 [D loss: 0.227170, acc.: 67.19%] [G loss: 0.462254]\n",
      "epoch:12 step:11919 [D loss: 0.248807, acc.: 59.38%] [G loss: 0.481304]\n",
      "epoch:12 step:11920 [D loss: 0.234256, acc.: 60.94%] [G loss: 0.447306]\n",
      "epoch:12 step:11921 [D loss: 0.204573, acc.: 67.19%] [G loss: 0.403137]\n",
      "epoch:12 step:11922 [D loss: 0.234009, acc.: 60.16%] [G loss: 0.425564]\n",
      "epoch:12 step:11923 [D loss: 0.222021, acc.: 61.72%] [G loss: 0.463080]\n",
      "epoch:12 step:11924 [D loss: 0.208512, acc.: 64.06%] [G loss: 0.452858]\n",
      "epoch:12 step:11925 [D loss: 0.197853, acc.: 70.31%] [G loss: 0.479050]\n",
      "epoch:12 step:11926 [D loss: 0.243135, acc.: 56.25%] [G loss: 0.437379]\n",
      "epoch:12 step:11927 [D loss: 0.238394, acc.: 61.72%] [G loss: 0.426577]\n",
      "epoch:12 step:11928 [D loss: 0.237903, acc.: 60.94%] [G loss: 0.411127]\n",
      "epoch:12 step:11929 [D loss: 0.209700, acc.: 70.31%] [G loss: 0.473905]\n",
      "epoch:12 step:11930 [D loss: 0.217459, acc.: 67.19%] [G loss: 0.440336]\n",
      "epoch:12 step:11931 [D loss: 0.234081, acc.: 60.94%] [G loss: 0.420728]\n",
      "epoch:12 step:11932 [D loss: 0.209899, acc.: 66.41%] [G loss: 0.419303]\n",
      "epoch:12 step:11933 [D loss: 0.235318, acc.: 61.72%] [G loss: 0.458978]\n",
      "epoch:12 step:11934 [D loss: 0.183545, acc.: 67.19%] [G loss: 0.466351]\n",
      "epoch:12 step:11935 [D loss: 0.230871, acc.: 60.94%] [G loss: 0.434559]\n",
      "epoch:12 step:11936 [D loss: 0.217191, acc.: 67.19%] [G loss: 0.482894]\n",
      "epoch:12 step:11937 [D loss: 0.227139, acc.: 64.06%] [G loss: 0.489480]\n",
      "epoch:12 step:11938 [D loss: 0.192263, acc.: 67.97%] [G loss: 0.467999]\n",
      "epoch:12 step:11939 [D loss: 0.207101, acc.: 67.97%] [G loss: 0.503006]\n",
      "epoch:12 step:11940 [D loss: 0.242427, acc.: 52.34%] [G loss: 0.447567]\n",
      "epoch:12 step:11941 [D loss: 0.219781, acc.: 62.50%] [G loss: 0.451101]\n",
      "epoch:12 step:11942 [D loss: 0.223669, acc.: 64.06%] [G loss: 0.446777]\n",
      "epoch:12 step:11943 [D loss: 0.204617, acc.: 64.06%] [G loss: 0.431626]\n",
      "epoch:12 step:11944 [D loss: 0.210613, acc.: 61.72%] [G loss: 0.473465]\n",
      "epoch:12 step:11945 [D loss: 0.206369, acc.: 64.84%] [G loss: 0.506814]\n",
      "epoch:12 step:11946 [D loss: 0.237901, acc.: 60.94%] [G loss: 0.472748]\n",
      "epoch:12 step:11947 [D loss: 0.245895, acc.: 60.94%] [G loss: 0.462875]\n",
      "epoch:12 step:11948 [D loss: 0.244651, acc.: 57.81%] [G loss: 0.451897]\n",
      "epoch:12 step:11949 [D loss: 0.216682, acc.: 67.97%] [G loss: 0.434371]\n",
      "epoch:12 step:11950 [D loss: 0.221523, acc.: 64.84%] [G loss: 0.429887]\n",
      "epoch:12 step:11951 [D loss: 0.208715, acc.: 64.06%] [G loss: 0.465382]\n",
      "epoch:12 step:11952 [D loss: 0.211693, acc.: 66.41%] [G loss: 0.443590]\n",
      "epoch:12 step:11953 [D loss: 0.199394, acc.: 69.53%] [G loss: 0.467878]\n",
      "epoch:12 step:11954 [D loss: 0.284271, acc.: 53.91%] [G loss: 0.420376]\n",
      "epoch:12 step:11955 [D loss: 0.220893, acc.: 61.72%] [G loss: 0.427157]\n",
      "epoch:12 step:11956 [D loss: 0.211737, acc.: 62.50%] [G loss: 0.468003]\n",
      "epoch:12 step:11957 [D loss: 0.235535, acc.: 59.38%] [G loss: 0.470987]\n",
      "epoch:12 step:11958 [D loss: 0.239385, acc.: 60.94%] [G loss: 0.447605]\n",
      "epoch:12 step:11959 [D loss: 0.254517, acc.: 57.81%] [G loss: 0.442009]\n",
      "epoch:12 step:11960 [D loss: 0.256674, acc.: 50.00%] [G loss: 0.413524]\n",
      "epoch:12 step:11961 [D loss: 0.214399, acc.: 67.19%] [G loss: 0.393842]\n",
      "epoch:12 step:11962 [D loss: 0.234756, acc.: 60.94%] [G loss: 0.424935]\n",
      "epoch:12 step:11963 [D loss: 0.228982, acc.: 61.72%] [G loss: 0.445948]\n",
      "epoch:12 step:11964 [D loss: 0.223241, acc.: 63.28%] [G loss: 0.459607]\n",
      "epoch:12 step:11965 [D loss: 0.242893, acc.: 62.50%] [G loss: 0.472414]\n",
      "epoch:12 step:11966 [D loss: 0.264596, acc.: 50.78%] [G loss: 0.422505]\n",
      "epoch:12 step:11967 [D loss: 0.236815, acc.: 60.94%] [G loss: 0.406832]\n",
      "epoch:12 step:11968 [D loss: 0.221711, acc.: 59.38%] [G loss: 0.445111]\n",
      "epoch:12 step:11969 [D loss: 0.204079, acc.: 67.97%] [G loss: 0.487429]\n",
      "epoch:12 step:11970 [D loss: 0.220216, acc.: 64.84%] [G loss: 0.461547]\n",
      "epoch:12 step:11971 [D loss: 0.232733, acc.: 59.38%] [G loss: 0.444969]\n",
      "epoch:12 step:11972 [D loss: 0.225250, acc.: 60.94%] [G loss: 0.393841]\n",
      "epoch:12 step:11973 [D loss: 0.230348, acc.: 59.38%] [G loss: 0.410107]\n",
      "epoch:12 step:11974 [D loss: 0.178327, acc.: 75.00%] [G loss: 0.469174]\n",
      "epoch:12 step:11975 [D loss: 0.224478, acc.: 58.59%] [G loss: 0.441943]\n",
      "epoch:12 step:11976 [D loss: 0.217756, acc.: 64.84%] [G loss: 0.467931]\n",
      "epoch:12 step:11977 [D loss: 0.197701, acc.: 73.44%] [G loss: 0.478068]\n",
      "epoch:12 step:11978 [D loss: 0.250964, acc.: 57.03%] [G loss: 0.419871]\n",
      "epoch:12 step:11979 [D loss: 0.229042, acc.: 67.19%] [G loss: 0.420267]\n",
      "epoch:12 step:11980 [D loss: 0.217525, acc.: 64.06%] [G loss: 0.498147]\n",
      "epoch:12 step:11981 [D loss: 0.210770, acc.: 66.41%] [G loss: 0.467526]\n",
      "epoch:12 step:11982 [D loss: 0.238566, acc.: 63.28%] [G loss: 0.409173]\n",
      "epoch:12 step:11983 [D loss: 0.249635, acc.: 60.16%] [G loss: 0.433560]\n",
      "epoch:12 step:11984 [D loss: 0.240799, acc.: 62.50%] [G loss: 0.421929]\n",
      "epoch:12 step:11985 [D loss: 0.222907, acc.: 63.28%] [G loss: 0.456636]\n",
      "epoch:12 step:11986 [D loss: 0.214595, acc.: 67.19%] [G loss: 0.459009]\n",
      "epoch:12 step:11987 [D loss: 0.213427, acc.: 67.19%] [G loss: 0.440275]\n",
      "epoch:12 step:11988 [D loss: 0.234789, acc.: 57.81%] [G loss: 0.446778]\n",
      "epoch:12 step:11989 [D loss: 0.223726, acc.: 60.16%] [G loss: 0.448512]\n",
      "epoch:12 step:11990 [D loss: 0.206201, acc.: 64.84%] [G loss: 0.439083]\n",
      "epoch:12 step:11991 [D loss: 0.201099, acc.: 65.62%] [G loss: 0.456694]\n",
      "epoch:12 step:11992 [D loss: 0.253850, acc.: 59.38%] [G loss: 0.414727]\n",
      "epoch:12 step:11993 [D loss: 0.242338, acc.: 57.81%] [G loss: 0.428543]\n",
      "epoch:12 step:11994 [D loss: 0.215521, acc.: 66.41%] [G loss: 0.476042]\n",
      "epoch:12 step:11995 [D loss: 0.218171, acc.: 60.16%] [G loss: 0.442806]\n",
      "epoch:12 step:11996 [D loss: 0.257416, acc.: 51.56%] [G loss: 0.380933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11997 [D loss: 0.224823, acc.: 61.72%] [G loss: 0.477294]\n",
      "epoch:12 step:11998 [D loss: 0.196499, acc.: 68.75%] [G loss: 0.465386]\n",
      "epoch:12 step:11999 [D loss: 0.210800, acc.: 67.97%] [G loss: 0.487542]\n",
      "epoch:12 step:12000 [D loss: 0.212843, acc.: 62.50%] [G loss: 0.453647]\n",
      "##############\n",
      "[2.3037388  1.73490431 5.94356083 4.72694596 3.54937307 5.62392769\n",
      " 4.46561921 4.29541465 4.46139049 3.69704076]\n",
      "##########\n",
      "epoch:12 step:12001 [D loss: 0.229753, acc.: 60.94%] [G loss: 0.435418]\n",
      "epoch:12 step:12002 [D loss: 0.216925, acc.: 71.09%] [G loss: 0.450549]\n",
      "epoch:12 step:12003 [D loss: 0.242120, acc.: 58.59%] [G loss: 0.413343]\n",
      "epoch:12 step:12004 [D loss: 0.252577, acc.: 53.91%] [G loss: 0.397733]\n",
      "epoch:12 step:12005 [D loss: 0.224193, acc.: 59.38%] [G loss: 0.451309]\n",
      "epoch:12 step:12006 [D loss: 0.233267, acc.: 56.25%] [G loss: 0.441808]\n",
      "epoch:12 step:12007 [D loss: 0.232230, acc.: 67.19%] [G loss: 0.447178]\n",
      "epoch:12 step:12008 [D loss: 0.229642, acc.: 59.38%] [G loss: 0.449436]\n",
      "epoch:12 step:12009 [D loss: 0.260331, acc.: 56.25%] [G loss: 0.421044]\n",
      "epoch:12 step:12010 [D loss: 0.224087, acc.: 60.94%] [G loss: 0.437511]\n",
      "epoch:12 step:12011 [D loss: 0.205662, acc.: 67.97%] [G loss: 0.443297]\n",
      "epoch:12 step:12012 [D loss: 0.211158, acc.: 67.97%] [G loss: 0.449814]\n",
      "epoch:12 step:12013 [D loss: 0.240168, acc.: 58.59%] [G loss: 0.456574]\n",
      "epoch:12 step:12014 [D loss: 0.210048, acc.: 66.41%] [G loss: 0.501980]\n",
      "epoch:12 step:12015 [D loss: 0.221399, acc.: 61.72%] [G loss: 0.457893]\n",
      "epoch:12 step:12016 [D loss: 0.232120, acc.: 61.72%] [G loss: 0.450267]\n",
      "epoch:12 step:12017 [D loss: 0.202138, acc.: 66.41%] [G loss: 0.477516]\n",
      "epoch:12 step:12018 [D loss: 0.226662, acc.: 60.94%] [G loss: 0.451912]\n",
      "epoch:12 step:12019 [D loss: 0.204833, acc.: 70.31%] [G loss: 0.456955]\n",
      "epoch:12 step:12020 [D loss: 0.222605, acc.: 60.16%] [G loss: 0.443286]\n",
      "epoch:12 step:12021 [D loss: 0.196640, acc.: 68.75%] [G loss: 0.461499]\n",
      "epoch:12 step:12022 [D loss: 0.219112, acc.: 67.19%] [G loss: 0.452233]\n",
      "epoch:12 step:12023 [D loss: 0.234266, acc.: 62.50%] [G loss: 0.428293]\n",
      "epoch:12 step:12024 [D loss: 0.231357, acc.: 59.38%] [G loss: 0.417682]\n",
      "epoch:12 step:12025 [D loss: 0.234578, acc.: 60.94%] [G loss: 0.457476]\n",
      "epoch:12 step:12026 [D loss: 0.197683, acc.: 71.09%] [G loss: 0.502631]\n",
      "epoch:12 step:12027 [D loss: 0.221432, acc.: 61.72%] [G loss: 0.494666]\n",
      "epoch:12 step:12028 [D loss: 0.295539, acc.: 47.66%] [G loss: 0.399655]\n",
      "epoch:12 step:12029 [D loss: 0.225788, acc.: 67.19%] [G loss: 0.450261]\n",
      "epoch:12 step:12030 [D loss: 0.193500, acc.: 69.53%] [G loss: 0.487226]\n",
      "epoch:12 step:12031 [D loss: 0.259257, acc.: 58.59%] [G loss: 0.488487]\n",
      "epoch:12 step:12032 [D loss: 0.251094, acc.: 57.81%] [G loss: 0.455221]\n",
      "epoch:12 step:12033 [D loss: 0.237265, acc.: 56.25%] [G loss: 0.421709]\n",
      "epoch:12 step:12034 [D loss: 0.225377, acc.: 64.84%] [G loss: 0.441921]\n",
      "epoch:12 step:12035 [D loss: 0.256177, acc.: 56.25%] [G loss: 0.405974]\n",
      "epoch:12 step:12036 [D loss: 0.194380, acc.: 75.00%] [G loss: 0.453325]\n",
      "epoch:12 step:12037 [D loss: 0.222863, acc.: 62.50%] [G loss: 0.444074]\n",
      "epoch:12 step:12038 [D loss: 0.265939, acc.: 53.12%] [G loss: 0.431497]\n",
      "epoch:12 step:12039 [D loss: 0.251317, acc.: 54.69%] [G loss: 0.435912]\n",
      "epoch:12 step:12040 [D loss: 0.200268, acc.: 67.97%] [G loss: 0.468285]\n",
      "epoch:12 step:12041 [D loss: 0.242500, acc.: 59.38%] [G loss: 0.432057]\n",
      "epoch:12 step:12042 [D loss: 0.231415, acc.: 64.06%] [G loss: 0.432212]\n",
      "epoch:12 step:12043 [D loss: 0.225523, acc.: 64.06%] [G loss: 0.442486]\n",
      "epoch:12 step:12044 [D loss: 0.254915, acc.: 53.91%] [G loss: 0.459304]\n",
      "epoch:12 step:12045 [D loss: 0.206436, acc.: 67.19%] [G loss: 0.416848]\n",
      "epoch:12 step:12046 [D loss: 0.215975, acc.: 67.19%] [G loss: 0.483606]\n",
      "epoch:12 step:12047 [D loss: 0.194011, acc.: 73.44%] [G loss: 0.511878]\n",
      "epoch:12 step:12048 [D loss: 0.253695, acc.: 52.34%] [G loss: 0.447300]\n",
      "epoch:12 step:12049 [D loss: 0.215653, acc.: 67.97%] [G loss: 0.424462]\n",
      "epoch:12 step:12050 [D loss: 0.264834, acc.: 53.91%] [G loss: 0.421963]\n",
      "epoch:12 step:12051 [D loss: 0.226136, acc.: 64.84%] [G loss: 0.444293]\n",
      "epoch:12 step:12052 [D loss: 0.241957, acc.: 60.94%] [G loss: 0.435009]\n",
      "epoch:12 step:12053 [D loss: 0.230401, acc.: 60.16%] [G loss: 0.442224]\n",
      "epoch:12 step:12054 [D loss: 0.228967, acc.: 66.41%] [G loss: 0.438668]\n",
      "epoch:12 step:12055 [D loss: 0.218382, acc.: 60.94%] [G loss: 0.429229]\n",
      "epoch:12 step:12056 [D loss: 0.233320, acc.: 64.84%] [G loss: 0.457524]\n",
      "epoch:12 step:12057 [D loss: 0.233459, acc.: 59.38%] [G loss: 0.416383]\n",
      "epoch:12 step:12058 [D loss: 0.235909, acc.: 63.28%] [G loss: 0.443502]\n",
      "epoch:12 step:12059 [D loss: 0.217374, acc.: 66.41%] [G loss: 0.466073]\n",
      "epoch:12 step:12060 [D loss: 0.225979, acc.: 66.41%] [G loss: 0.496221]\n",
      "epoch:12 step:12061 [D loss: 0.244924, acc.: 60.16%] [G loss: 0.434038]\n",
      "epoch:12 step:12062 [D loss: 0.258348, acc.: 57.81%] [G loss: 0.447792]\n",
      "epoch:12 step:12063 [D loss: 0.233081, acc.: 63.28%] [G loss: 0.458172]\n",
      "epoch:12 step:12064 [D loss: 0.282029, acc.: 50.78%] [G loss: 0.410056]\n",
      "epoch:12 step:12065 [D loss: 0.216552, acc.: 60.94%] [G loss: 0.419995]\n",
      "epoch:12 step:12066 [D loss: 0.185993, acc.: 68.75%] [G loss: 0.422562]\n",
      "epoch:12 step:12067 [D loss: 0.201085, acc.: 71.88%] [G loss: 0.454854]\n",
      "epoch:12 step:12068 [D loss: 0.242785, acc.: 56.25%] [G loss: 0.429115]\n",
      "epoch:12 step:12069 [D loss: 0.181200, acc.: 75.00%] [G loss: 0.465025]\n",
      "epoch:12 step:12070 [D loss: 0.217056, acc.: 63.28%] [G loss: 0.424216]\n",
      "epoch:12 step:12071 [D loss: 0.262582, acc.: 54.69%] [G loss: 0.452371]\n",
      "epoch:12 step:12072 [D loss: 0.241596, acc.: 58.59%] [G loss: 0.430651]\n",
      "epoch:12 step:12073 [D loss: 0.240817, acc.: 53.12%] [G loss: 0.420255]\n",
      "epoch:12 step:12074 [D loss: 0.227549, acc.: 62.50%] [G loss: 0.418654]\n",
      "epoch:12 step:12075 [D loss: 0.202033, acc.: 71.88%] [G loss: 0.410617]\n",
      "epoch:12 step:12076 [D loss: 0.226525, acc.: 64.84%] [G loss: 0.454957]\n",
      "epoch:12 step:12077 [D loss: 0.198168, acc.: 71.88%] [G loss: 0.469944]\n",
      "epoch:12 step:12078 [D loss: 0.218421, acc.: 62.50%] [G loss: 0.463003]\n",
      "epoch:12 step:12079 [D loss: 0.218598, acc.: 64.06%] [G loss: 0.419415]\n",
      "epoch:12 step:12080 [D loss: 0.237326, acc.: 60.16%] [G loss: 0.439954]\n",
      "epoch:12 step:12081 [D loss: 0.207164, acc.: 69.53%] [G loss: 0.477184]\n",
      "epoch:12 step:12082 [D loss: 0.208031, acc.: 61.72%] [G loss: 0.487641]\n",
      "epoch:12 step:12083 [D loss: 0.209291, acc.: 65.62%] [G loss: 0.439792]\n",
      "epoch:12 step:12084 [D loss: 0.228106, acc.: 61.72%] [G loss: 0.413085]\n",
      "epoch:12 step:12085 [D loss: 0.206605, acc.: 65.62%] [G loss: 0.454091]\n",
      "epoch:12 step:12086 [D loss: 0.189991, acc.: 74.22%] [G loss: 0.502751]\n",
      "epoch:12 step:12087 [D loss: 0.222840, acc.: 64.06%] [G loss: 0.452110]\n",
      "epoch:12 step:12088 [D loss: 0.231616, acc.: 57.81%] [G loss: 0.438421]\n",
      "epoch:12 step:12089 [D loss: 0.223934, acc.: 60.16%] [G loss: 0.446377]\n",
      "epoch:12 step:12090 [D loss: 0.233888, acc.: 54.69%] [G loss: 0.419893]\n",
      "epoch:12 step:12091 [D loss: 0.232866, acc.: 66.41%] [G loss: 0.450247]\n",
      "epoch:12 step:12092 [D loss: 0.224267, acc.: 63.28%] [G loss: 0.475732]\n",
      "epoch:12 step:12093 [D loss: 0.190723, acc.: 71.88%] [G loss: 0.445215]\n",
      "epoch:12 step:12094 [D loss: 0.261952, acc.: 60.16%] [G loss: 0.428097]\n",
      "epoch:12 step:12095 [D loss: 0.232562, acc.: 60.16%] [G loss: 0.454418]\n",
      "epoch:12 step:12096 [D loss: 0.215938, acc.: 63.28%] [G loss: 0.454293]\n",
      "epoch:12 step:12097 [D loss: 0.233342, acc.: 58.59%] [G loss: 0.469944]\n",
      "epoch:12 step:12098 [D loss: 0.223551, acc.: 62.50%] [G loss: 0.407764]\n",
      "epoch:12 step:12099 [D loss: 0.252913, acc.: 55.47%] [G loss: 0.422003]\n",
      "epoch:12 step:12100 [D loss: 0.219412, acc.: 60.94%] [G loss: 0.444618]\n",
      "epoch:12 step:12101 [D loss: 0.239861, acc.: 62.50%] [G loss: 0.435840]\n",
      "epoch:12 step:12102 [D loss: 0.283490, acc.: 45.31%] [G loss: 0.467930]\n",
      "epoch:12 step:12103 [D loss: 0.223682, acc.: 64.84%] [G loss: 0.462584]\n",
      "epoch:12 step:12104 [D loss: 0.200094, acc.: 69.53%] [G loss: 0.485423]\n",
      "epoch:12 step:12105 [D loss: 0.268309, acc.: 51.56%] [G loss: 0.419546]\n",
      "epoch:12 step:12106 [D loss: 0.221223, acc.: 60.16%] [G loss: 0.407240]\n",
      "epoch:12 step:12107 [D loss: 0.213652, acc.: 70.31%] [G loss: 0.396957]\n",
      "epoch:12 step:12108 [D loss: 0.220249, acc.: 63.28%] [G loss: 0.411952]\n",
      "epoch:12 step:12109 [D loss: 0.271437, acc.: 52.34%] [G loss: 0.413906]\n",
      "epoch:12 step:12110 [D loss: 0.231016, acc.: 56.25%] [G loss: 0.406925]\n",
      "epoch:12 step:12111 [D loss: 0.229251, acc.: 61.72%] [G loss: 0.446810]\n",
      "epoch:12 step:12112 [D loss: 0.236410, acc.: 60.94%] [G loss: 0.423236]\n",
      "epoch:12 step:12113 [D loss: 0.231013, acc.: 60.16%] [G loss: 0.428616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12114 [D loss: 0.233287, acc.: 66.41%] [G loss: 0.441671]\n",
      "epoch:12 step:12115 [D loss: 0.216109, acc.: 65.62%] [G loss: 0.480573]\n",
      "epoch:12 step:12116 [D loss: 0.221064, acc.: 60.94%] [G loss: 0.489273]\n",
      "epoch:12 step:12117 [D loss: 0.238568, acc.: 60.94%] [G loss: 0.437580]\n",
      "epoch:12 step:12118 [D loss: 0.222789, acc.: 64.06%] [G loss: 0.441986]\n",
      "epoch:12 step:12119 [D loss: 0.188293, acc.: 72.66%] [G loss: 0.454902]\n",
      "epoch:12 step:12120 [D loss: 0.231950, acc.: 62.50%] [G loss: 0.483676]\n",
      "epoch:12 step:12121 [D loss: 0.237586, acc.: 56.25%] [G loss: 0.429356]\n",
      "epoch:12 step:12122 [D loss: 0.235596, acc.: 61.72%] [G loss: 0.426146]\n",
      "epoch:12 step:12123 [D loss: 0.218687, acc.: 63.28%] [G loss: 0.412125]\n",
      "epoch:12 step:12124 [D loss: 0.230424, acc.: 65.62%] [G loss: 0.441271]\n",
      "epoch:12 step:12125 [D loss: 0.235513, acc.: 58.59%] [G loss: 0.426286]\n",
      "epoch:12 step:12126 [D loss: 0.241698, acc.: 65.62%] [G loss: 0.432724]\n",
      "epoch:12 step:12127 [D loss: 0.231523, acc.: 60.94%] [G loss: 0.471297]\n",
      "epoch:12 step:12128 [D loss: 0.196604, acc.: 73.44%] [G loss: 0.491279]\n",
      "epoch:12 step:12129 [D loss: 0.214776, acc.: 65.62%] [G loss: 0.496521]\n",
      "epoch:12 step:12130 [D loss: 0.205474, acc.: 67.97%] [G loss: 0.505440]\n",
      "epoch:12 step:12131 [D loss: 0.231181, acc.: 60.16%] [G loss: 0.467571]\n",
      "epoch:12 step:12132 [D loss: 0.215437, acc.: 67.97%] [G loss: 0.449503]\n",
      "epoch:12 step:12133 [D loss: 0.203028, acc.: 70.31%] [G loss: 0.440570]\n",
      "epoch:12 step:12134 [D loss: 0.186099, acc.: 75.00%] [G loss: 0.444168]\n",
      "epoch:12 step:12135 [D loss: 0.277454, acc.: 48.44%] [G loss: 0.410294]\n",
      "epoch:12 step:12136 [D loss: 0.264066, acc.: 50.00%] [G loss: 0.447576]\n",
      "epoch:12 step:12137 [D loss: 0.214412, acc.: 71.88%] [G loss: 0.445382]\n",
      "epoch:12 step:12138 [D loss: 0.214838, acc.: 63.28%] [G loss: 0.446263]\n",
      "epoch:12 step:12139 [D loss: 0.218799, acc.: 66.41%] [G loss: 0.467568]\n",
      "epoch:12 step:12140 [D loss: 0.232687, acc.: 61.72%] [G loss: 0.438514]\n",
      "epoch:12 step:12141 [D loss: 0.210405, acc.: 66.41%] [G loss: 0.463156]\n",
      "epoch:12 step:12142 [D loss: 0.210435, acc.: 66.41%] [G loss: 0.490517]\n",
      "epoch:12 step:12143 [D loss: 0.185555, acc.: 76.56%] [G loss: 0.486553]\n",
      "epoch:12 step:12144 [D loss: 0.228015, acc.: 61.72%] [G loss: 0.466096]\n",
      "epoch:12 step:12145 [D loss: 0.213925, acc.: 59.38%] [G loss: 0.447325]\n",
      "epoch:12 step:12146 [D loss: 0.240881, acc.: 61.72%] [G loss: 0.426289]\n",
      "epoch:12 step:12147 [D loss: 0.211547, acc.: 68.75%] [G loss: 0.441488]\n",
      "epoch:12 step:12148 [D loss: 0.235544, acc.: 65.62%] [G loss: 0.430967]\n",
      "epoch:12 step:12149 [D loss: 0.224002, acc.: 61.72%] [G loss: 0.440018]\n",
      "epoch:12 step:12150 [D loss: 0.192081, acc.: 67.97%] [G loss: 0.470487]\n",
      "epoch:12 step:12151 [D loss: 0.237892, acc.: 56.25%] [G loss: 0.447687]\n",
      "epoch:12 step:12152 [D loss: 0.192123, acc.: 72.66%] [G loss: 0.461279]\n",
      "epoch:12 step:12153 [D loss: 0.192811, acc.: 69.53%] [G loss: 0.455021]\n",
      "epoch:12 step:12154 [D loss: 0.233334, acc.: 60.94%] [G loss: 0.435671]\n",
      "epoch:12 step:12155 [D loss: 0.199324, acc.: 68.75%] [G loss: 0.433005]\n",
      "epoch:12 step:12156 [D loss: 0.204370, acc.: 72.66%] [G loss: 0.499882]\n",
      "epoch:12 step:12157 [D loss: 0.229848, acc.: 63.28%] [G loss: 0.504307]\n",
      "epoch:12 step:12158 [D loss: 0.226732, acc.: 63.28%] [G loss: 0.470930]\n",
      "epoch:12 step:12159 [D loss: 0.304395, acc.: 49.22%] [G loss: 0.387386]\n",
      "epoch:12 step:12160 [D loss: 0.233940, acc.: 60.16%] [G loss: 0.427912]\n",
      "epoch:12 step:12161 [D loss: 0.213986, acc.: 68.75%] [G loss: 0.444469]\n",
      "epoch:12 step:12162 [D loss: 0.182453, acc.: 76.56%] [G loss: 0.496732]\n",
      "epoch:12 step:12163 [D loss: 0.219992, acc.: 66.41%] [G loss: 0.519304]\n",
      "epoch:12 step:12164 [D loss: 0.346528, acc.: 37.50%] [G loss: 0.449033]\n",
      "epoch:12 step:12165 [D loss: 0.242427, acc.: 64.84%] [G loss: 0.477906]\n",
      "epoch:12 step:12166 [D loss: 0.221050, acc.: 60.16%] [G loss: 0.428138]\n",
      "epoch:12 step:12167 [D loss: 0.217581, acc.: 65.62%] [G loss: 0.439640]\n",
      "epoch:12 step:12168 [D loss: 0.185193, acc.: 76.56%] [G loss: 0.468338]\n",
      "epoch:12 step:12169 [D loss: 0.192297, acc.: 73.44%] [G loss: 0.475304]\n",
      "epoch:12 step:12170 [D loss: 0.190990, acc.: 73.44%] [G loss: 0.512997]\n",
      "epoch:12 step:12171 [D loss: 0.231843, acc.: 67.97%] [G loss: 0.526990]\n",
      "epoch:12 step:12172 [D loss: 0.293904, acc.: 53.91%] [G loss: 0.550180]\n",
      "epoch:12 step:12173 [D loss: 0.263777, acc.: 59.38%] [G loss: 0.539427]\n",
      "epoch:12 step:12174 [D loss: 0.251501, acc.: 64.06%] [G loss: 0.549608]\n",
      "epoch:12 step:12175 [D loss: 0.253378, acc.: 61.72%] [G loss: 0.465615]\n",
      "epoch:12 step:12176 [D loss: 0.254247, acc.: 57.03%] [G loss: 0.423948]\n",
      "epoch:12 step:12177 [D loss: 0.219309, acc.: 63.28%] [G loss: 0.457252]\n",
      "epoch:12 step:12178 [D loss: 0.217335, acc.: 65.62%] [G loss: 0.445157]\n",
      "epoch:12 step:12179 [D loss: 0.201012, acc.: 71.88%] [G loss: 0.444257]\n",
      "epoch:12 step:12180 [D loss: 0.171213, acc.: 72.66%] [G loss: 0.553200]\n",
      "epoch:12 step:12181 [D loss: 0.171862, acc.: 74.22%] [G loss: 0.542225]\n",
      "epoch:13 step:12182 [D loss: 0.239642, acc.: 60.94%] [G loss: 0.479743]\n",
      "epoch:13 step:12183 [D loss: 0.264896, acc.: 57.03%] [G loss: 0.404292]\n",
      "epoch:13 step:12184 [D loss: 0.212458, acc.: 67.97%] [G loss: 0.497959]\n",
      "epoch:13 step:12185 [D loss: 0.228271, acc.: 64.06%] [G loss: 0.452405]\n",
      "epoch:13 step:12186 [D loss: 0.237657, acc.: 57.81%] [G loss: 0.471956]\n",
      "epoch:13 step:12187 [D loss: 0.212701, acc.: 68.75%] [G loss: 0.486655]\n",
      "epoch:13 step:12188 [D loss: 0.203514, acc.: 65.62%] [G loss: 0.474566]\n",
      "epoch:13 step:12189 [D loss: 0.215657, acc.: 64.84%] [G loss: 0.452433]\n",
      "epoch:13 step:12190 [D loss: 0.200797, acc.: 76.56%] [G loss: 0.462987]\n",
      "epoch:13 step:12191 [D loss: 0.228969, acc.: 60.94%] [G loss: 0.460812]\n",
      "epoch:13 step:12192 [D loss: 0.211610, acc.: 66.41%] [G loss: 0.470234]\n",
      "epoch:13 step:12193 [D loss: 0.211655, acc.: 64.06%] [G loss: 0.499200]\n",
      "epoch:13 step:12194 [D loss: 0.216701, acc.: 62.50%] [G loss: 0.432310]\n",
      "epoch:13 step:12195 [D loss: 0.223276, acc.: 64.06%] [G loss: 0.458547]\n",
      "epoch:13 step:12196 [D loss: 0.185909, acc.: 69.53%] [G loss: 0.476080]\n",
      "epoch:13 step:12197 [D loss: 0.189493, acc.: 71.88%] [G loss: 0.491914]\n",
      "epoch:13 step:12198 [D loss: 0.244771, acc.: 53.91%] [G loss: 0.422354]\n",
      "epoch:13 step:12199 [D loss: 0.250265, acc.: 52.34%] [G loss: 0.421022]\n",
      "epoch:13 step:12200 [D loss: 0.258824, acc.: 51.56%] [G loss: 0.462044]\n",
      "##############\n",
      "[2.42869098 1.752811   5.95336758 4.53199812 3.80618149 5.54502279\n",
      " 4.50200876 4.8012678  4.27500058 4.10965617]\n",
      "##########\n",
      "epoch:13 step:12201 [D loss: 0.286542, acc.: 44.53%] [G loss: 0.454261]\n",
      "epoch:13 step:12202 [D loss: 0.230636, acc.: 60.94%] [G loss: 0.459056]\n",
      "epoch:13 step:12203 [D loss: 0.201496, acc.: 67.19%] [G loss: 0.497247]\n",
      "epoch:13 step:12204 [D loss: 0.243467, acc.: 60.16%] [G loss: 0.437028]\n",
      "epoch:13 step:12205 [D loss: 0.191419, acc.: 74.22%] [G loss: 0.457773]\n",
      "epoch:13 step:12206 [D loss: 0.185027, acc.: 71.88%] [G loss: 0.451519]\n",
      "epoch:13 step:12207 [D loss: 0.240717, acc.: 59.38%] [G loss: 0.385490]\n",
      "epoch:13 step:12208 [D loss: 0.227255, acc.: 63.28%] [G loss: 0.423561]\n",
      "epoch:13 step:12209 [D loss: 0.229915, acc.: 60.94%] [G loss: 0.432014]\n",
      "epoch:13 step:12210 [D loss: 0.203477, acc.: 64.84%] [G loss: 0.451574]\n",
      "epoch:13 step:12211 [D loss: 0.239626, acc.: 59.38%] [G loss: 0.437321]\n",
      "epoch:13 step:12212 [D loss: 0.256311, acc.: 57.81%] [G loss: 0.429232]\n",
      "epoch:13 step:12213 [D loss: 0.221613, acc.: 64.06%] [G loss: 0.472404]\n",
      "epoch:13 step:12214 [D loss: 0.204302, acc.: 69.53%] [G loss: 0.491186]\n",
      "epoch:13 step:12215 [D loss: 0.253710, acc.: 55.47%] [G loss: 0.429815]\n",
      "epoch:13 step:12216 [D loss: 0.227589, acc.: 63.28%] [G loss: 0.381981]\n",
      "epoch:13 step:12217 [D loss: 0.227540, acc.: 66.41%] [G loss: 0.435116]\n",
      "epoch:13 step:12218 [D loss: 0.242478, acc.: 57.03%] [G loss: 0.415222]\n",
      "epoch:13 step:12219 [D loss: 0.255823, acc.: 53.12%] [G loss: 0.399910]\n",
      "epoch:13 step:12220 [D loss: 0.216180, acc.: 64.06%] [G loss: 0.413510]\n",
      "epoch:13 step:12221 [D loss: 0.188287, acc.: 71.09%] [G loss: 0.465369]\n",
      "epoch:13 step:12222 [D loss: 0.244487, acc.: 61.72%] [G loss: 0.447715]\n",
      "epoch:13 step:12223 [D loss: 0.216872, acc.: 66.41%] [G loss: 0.433447]\n",
      "epoch:13 step:12224 [D loss: 0.220177, acc.: 64.84%] [G loss: 0.432880]\n",
      "epoch:13 step:12225 [D loss: 0.266286, acc.: 50.00%] [G loss: 0.439801]\n",
      "epoch:13 step:12226 [D loss: 0.214718, acc.: 67.97%] [G loss: 0.442330]\n",
      "epoch:13 step:12227 [D loss: 0.203407, acc.: 67.97%] [G loss: 0.460769]\n",
      "epoch:13 step:12228 [D loss: 0.212801, acc.: 67.97%] [G loss: 0.430356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12229 [D loss: 0.203331, acc.: 66.41%] [G loss: 0.434934]\n",
      "epoch:13 step:12230 [D loss: 0.201205, acc.: 67.19%] [G loss: 0.464144]\n",
      "epoch:13 step:12231 [D loss: 0.222753, acc.: 61.72%] [G loss: 0.473599]\n",
      "epoch:13 step:12232 [D loss: 0.225603, acc.: 60.94%] [G loss: 0.428547]\n",
      "epoch:13 step:12233 [D loss: 0.250362, acc.: 57.81%] [G loss: 0.398735]\n",
      "epoch:13 step:12234 [D loss: 0.213705, acc.: 69.53%] [G loss: 0.434743]\n",
      "epoch:13 step:12235 [D loss: 0.221981, acc.: 62.50%] [G loss: 0.435734]\n",
      "epoch:13 step:12236 [D loss: 0.216939, acc.: 61.72%] [G loss: 0.449014]\n",
      "epoch:13 step:12237 [D loss: 0.213588, acc.: 65.62%] [G loss: 0.448585]\n",
      "epoch:13 step:12238 [D loss: 0.207594, acc.: 70.31%] [G loss: 0.438308]\n",
      "epoch:13 step:12239 [D loss: 0.241053, acc.: 60.16%] [G loss: 0.392831]\n",
      "epoch:13 step:12240 [D loss: 0.204229, acc.: 69.53%] [G loss: 0.440288]\n",
      "epoch:13 step:12241 [D loss: 0.248265, acc.: 59.38%] [G loss: 0.432425]\n",
      "epoch:13 step:12242 [D loss: 0.234210, acc.: 59.38%] [G loss: 0.416400]\n",
      "epoch:13 step:12243 [D loss: 0.235333, acc.: 59.38%] [G loss: 0.469869]\n",
      "epoch:13 step:12244 [D loss: 0.246507, acc.: 58.59%] [G loss: 0.394760]\n",
      "epoch:13 step:12245 [D loss: 0.229772, acc.: 60.94%] [G loss: 0.451313]\n",
      "epoch:13 step:12246 [D loss: 0.221972, acc.: 60.94%] [G loss: 0.445120]\n",
      "epoch:13 step:12247 [D loss: 0.221807, acc.: 66.41%] [G loss: 0.439047]\n",
      "epoch:13 step:12248 [D loss: 0.229192, acc.: 65.62%] [G loss: 0.419843]\n",
      "epoch:13 step:12249 [D loss: 0.229722, acc.: 58.59%] [G loss: 0.419806]\n",
      "epoch:13 step:12250 [D loss: 0.198292, acc.: 67.97%] [G loss: 0.450280]\n",
      "epoch:13 step:12251 [D loss: 0.209303, acc.: 64.06%] [G loss: 0.477743]\n",
      "epoch:13 step:12252 [D loss: 0.235777, acc.: 58.59%] [G loss: 0.447931]\n",
      "epoch:13 step:12253 [D loss: 0.220385, acc.: 59.38%] [G loss: 0.423335]\n",
      "epoch:13 step:12254 [D loss: 0.248708, acc.: 55.47%] [G loss: 0.422010]\n",
      "epoch:13 step:12255 [D loss: 0.215545, acc.: 70.31%] [G loss: 0.435037]\n",
      "epoch:13 step:12256 [D loss: 0.213214, acc.: 64.84%] [G loss: 0.484489]\n",
      "epoch:13 step:12257 [D loss: 0.205351, acc.: 67.97%] [G loss: 0.466088]\n",
      "epoch:13 step:12258 [D loss: 0.174920, acc.: 74.22%] [G loss: 0.490698]\n",
      "epoch:13 step:12259 [D loss: 0.270527, acc.: 52.34%] [G loss: 0.430530]\n",
      "epoch:13 step:12260 [D loss: 0.253514, acc.: 55.47%] [G loss: 0.390085]\n",
      "epoch:13 step:12261 [D loss: 0.238888, acc.: 63.28%] [G loss: 0.434303]\n",
      "epoch:13 step:12262 [D loss: 0.247430, acc.: 58.59%] [G loss: 0.406789]\n",
      "epoch:13 step:12263 [D loss: 0.193598, acc.: 75.78%] [G loss: 0.450027]\n",
      "epoch:13 step:12264 [D loss: 0.223963, acc.: 59.38%] [G loss: 0.432659]\n",
      "epoch:13 step:12265 [D loss: 0.218464, acc.: 64.84%] [G loss: 0.421771]\n",
      "epoch:13 step:12266 [D loss: 0.226558, acc.: 60.94%] [G loss: 0.454886]\n",
      "epoch:13 step:12267 [D loss: 0.242762, acc.: 61.72%] [G loss: 0.418014]\n",
      "epoch:13 step:12268 [D loss: 0.201925, acc.: 67.19%] [G loss: 0.443724]\n",
      "epoch:13 step:12269 [D loss: 0.199620, acc.: 74.22%] [G loss: 0.440080]\n",
      "epoch:13 step:12270 [D loss: 0.204186, acc.: 67.19%] [G loss: 0.446350]\n",
      "epoch:13 step:12271 [D loss: 0.216704, acc.: 65.62%] [G loss: 0.468292]\n",
      "epoch:13 step:12272 [D loss: 0.229027, acc.: 63.28%] [G loss: 0.408776]\n",
      "epoch:13 step:12273 [D loss: 0.211816, acc.: 67.97%] [G loss: 0.461034]\n",
      "epoch:13 step:12274 [D loss: 0.192124, acc.: 69.53%] [G loss: 0.462422]\n",
      "epoch:13 step:12275 [D loss: 0.247582, acc.: 51.56%] [G loss: 0.469969]\n",
      "epoch:13 step:12276 [D loss: 0.213156, acc.: 62.50%] [G loss: 0.465542]\n",
      "epoch:13 step:12277 [D loss: 0.209366, acc.: 75.00%] [G loss: 0.448441]\n",
      "epoch:13 step:12278 [D loss: 0.230300, acc.: 67.19%] [G loss: 0.449306]\n",
      "epoch:13 step:12279 [D loss: 0.229939, acc.: 60.16%] [G loss: 0.469928]\n",
      "epoch:13 step:12280 [D loss: 0.216064, acc.: 62.50%] [G loss: 0.442646]\n",
      "epoch:13 step:12281 [D loss: 0.183642, acc.: 77.34%] [G loss: 0.452549]\n",
      "epoch:13 step:12282 [D loss: 0.234849, acc.: 65.62%] [G loss: 0.465944]\n",
      "epoch:13 step:12283 [D loss: 0.266927, acc.: 50.00%] [G loss: 0.424134]\n",
      "epoch:13 step:12284 [D loss: 0.261604, acc.: 53.91%] [G loss: 0.465132]\n",
      "epoch:13 step:12285 [D loss: 0.228990, acc.: 60.94%] [G loss: 0.444816]\n",
      "epoch:13 step:12286 [D loss: 0.238552, acc.: 59.38%] [G loss: 0.445900]\n",
      "epoch:13 step:12287 [D loss: 0.222608, acc.: 64.06%] [G loss: 0.425304]\n",
      "epoch:13 step:12288 [D loss: 0.204102, acc.: 63.28%] [G loss: 0.477742]\n",
      "epoch:13 step:12289 [D loss: 0.265361, acc.: 53.91%] [G loss: 0.451014]\n",
      "epoch:13 step:12290 [D loss: 0.241362, acc.: 56.25%] [G loss: 0.425013]\n",
      "epoch:13 step:12291 [D loss: 0.243346, acc.: 59.38%] [G loss: 0.426379]\n",
      "epoch:13 step:12292 [D loss: 0.202543, acc.: 67.97%] [G loss: 0.426794]\n",
      "epoch:13 step:12293 [D loss: 0.211961, acc.: 66.41%] [G loss: 0.441457]\n",
      "epoch:13 step:12294 [D loss: 0.197583, acc.: 69.53%] [G loss: 0.443190]\n",
      "epoch:13 step:12295 [D loss: 0.212511, acc.: 63.28%] [G loss: 0.482455]\n",
      "epoch:13 step:12296 [D loss: 0.211422, acc.: 65.62%] [G loss: 0.434537]\n",
      "epoch:13 step:12297 [D loss: 0.213080, acc.: 62.50%] [G loss: 0.480451]\n",
      "epoch:13 step:12298 [D loss: 0.228461, acc.: 61.72%] [G loss: 0.487842]\n",
      "epoch:13 step:12299 [D loss: 0.219498, acc.: 64.06%] [G loss: 0.474462]\n",
      "epoch:13 step:12300 [D loss: 0.177742, acc.: 73.44%] [G loss: 0.535116]\n",
      "epoch:13 step:12301 [D loss: 0.256764, acc.: 56.25%] [G loss: 0.460895]\n",
      "epoch:13 step:12302 [D loss: 0.241079, acc.: 59.38%] [G loss: 0.422256]\n",
      "epoch:13 step:12303 [D loss: 0.197538, acc.: 69.53%] [G loss: 0.443748]\n",
      "epoch:13 step:12304 [D loss: 0.191208, acc.: 65.62%] [G loss: 0.494435]\n",
      "epoch:13 step:12305 [D loss: 0.246723, acc.: 56.25%] [G loss: 0.463755]\n",
      "epoch:13 step:12306 [D loss: 0.243730, acc.: 60.16%] [G loss: 0.420713]\n",
      "epoch:13 step:12307 [D loss: 0.209898, acc.: 66.41%] [G loss: 0.482444]\n",
      "epoch:13 step:12308 [D loss: 0.225891, acc.: 63.28%] [G loss: 0.426163]\n",
      "epoch:13 step:12309 [D loss: 0.266985, acc.: 52.34%] [G loss: 0.399807]\n",
      "epoch:13 step:12310 [D loss: 0.222438, acc.: 64.06%] [G loss: 0.459143]\n",
      "epoch:13 step:12311 [D loss: 0.210334, acc.: 64.84%] [G loss: 0.437823]\n",
      "epoch:13 step:12312 [D loss: 0.231001, acc.: 67.19%] [G loss: 0.433678]\n",
      "epoch:13 step:12313 [D loss: 0.242248, acc.: 60.94%] [G loss: 0.433099]\n",
      "epoch:13 step:12314 [D loss: 0.271290, acc.: 50.00%] [G loss: 0.489049]\n",
      "epoch:13 step:12315 [D loss: 0.223657, acc.: 59.38%] [G loss: 0.483517]\n",
      "epoch:13 step:12316 [D loss: 0.234256, acc.: 60.16%] [G loss: 0.425404]\n",
      "epoch:13 step:12317 [D loss: 0.239966, acc.: 59.38%] [G loss: 0.411481]\n",
      "epoch:13 step:12318 [D loss: 0.264315, acc.: 53.12%] [G loss: 0.408884]\n",
      "epoch:13 step:12319 [D loss: 0.211163, acc.: 61.72%] [G loss: 0.408775]\n",
      "epoch:13 step:12320 [D loss: 0.227267, acc.: 66.41%] [G loss: 0.445568]\n",
      "epoch:13 step:12321 [D loss: 0.238434, acc.: 57.03%] [G loss: 0.436170]\n",
      "epoch:13 step:12322 [D loss: 0.221747, acc.: 64.84%] [G loss: 0.392233]\n",
      "epoch:13 step:12323 [D loss: 0.223023, acc.: 67.97%] [G loss: 0.386295]\n",
      "epoch:13 step:12324 [D loss: 0.262444, acc.: 51.56%] [G loss: 0.383610]\n",
      "epoch:13 step:12325 [D loss: 0.215683, acc.: 64.84%] [G loss: 0.466741]\n",
      "epoch:13 step:12326 [D loss: 0.216607, acc.: 67.19%] [G loss: 0.439111]\n",
      "epoch:13 step:12327 [D loss: 0.234502, acc.: 59.38%] [G loss: 0.419662]\n",
      "epoch:13 step:12328 [D loss: 0.248345, acc.: 57.81%] [G loss: 0.407667]\n",
      "epoch:13 step:12329 [D loss: 0.251790, acc.: 58.59%] [G loss: 0.388259]\n",
      "epoch:13 step:12330 [D loss: 0.224459, acc.: 67.97%] [G loss: 0.444736]\n",
      "epoch:13 step:12331 [D loss: 0.234803, acc.: 62.50%] [G loss: 0.417765]\n",
      "epoch:13 step:12332 [D loss: 0.217792, acc.: 67.19%] [G loss: 0.468772]\n",
      "epoch:13 step:12333 [D loss: 0.212633, acc.: 67.97%] [G loss: 0.442987]\n",
      "epoch:13 step:12334 [D loss: 0.247117, acc.: 59.38%] [G loss: 0.400764]\n",
      "epoch:13 step:12335 [D loss: 0.221985, acc.: 61.72%] [G loss: 0.416482]\n",
      "epoch:13 step:12336 [D loss: 0.217507, acc.: 65.62%] [G loss: 0.467926]\n",
      "epoch:13 step:12337 [D loss: 0.217802, acc.: 64.06%] [G loss: 0.409446]\n",
      "epoch:13 step:12338 [D loss: 0.231160, acc.: 56.25%] [G loss: 0.431153]\n",
      "epoch:13 step:12339 [D loss: 0.244105, acc.: 57.03%] [G loss: 0.413811]\n",
      "epoch:13 step:12340 [D loss: 0.212911, acc.: 67.19%] [G loss: 0.488248]\n",
      "epoch:13 step:12341 [D loss: 0.271588, acc.: 57.03%] [G loss: 0.452954]\n",
      "epoch:13 step:12342 [D loss: 0.248490, acc.: 57.81%] [G loss: 0.449399]\n",
      "epoch:13 step:12343 [D loss: 0.207213, acc.: 64.06%] [G loss: 0.459213]\n",
      "epoch:13 step:12344 [D loss: 0.247271, acc.: 56.25%] [G loss: 0.450973]\n",
      "epoch:13 step:12345 [D loss: 0.208954, acc.: 66.41%] [G loss: 0.494060]\n",
      "epoch:13 step:12346 [D loss: 0.192133, acc.: 71.88%] [G loss: 0.482116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12347 [D loss: 0.229397, acc.: 59.38%] [G loss: 0.484799]\n",
      "epoch:13 step:12348 [D loss: 0.221859, acc.: 62.50%] [G loss: 0.443886]\n",
      "epoch:13 step:12349 [D loss: 0.228080, acc.: 71.88%] [G loss: 0.428747]\n",
      "epoch:13 step:12350 [D loss: 0.247966, acc.: 58.59%] [G loss: 0.455887]\n",
      "epoch:13 step:12351 [D loss: 0.255212, acc.: 55.47%] [G loss: 0.410734]\n",
      "epoch:13 step:12352 [D loss: 0.224715, acc.: 63.28%] [G loss: 0.408566]\n",
      "epoch:13 step:12353 [D loss: 0.216709, acc.: 67.19%] [G loss: 0.455197]\n",
      "epoch:13 step:12354 [D loss: 0.229370, acc.: 67.97%] [G loss: 0.458225]\n",
      "epoch:13 step:12355 [D loss: 0.274956, acc.: 51.56%] [G loss: 0.393217]\n",
      "epoch:13 step:12356 [D loss: 0.245529, acc.: 57.81%] [G loss: 0.429247]\n",
      "epoch:13 step:12357 [D loss: 0.225529, acc.: 58.59%] [G loss: 0.422379]\n",
      "epoch:13 step:12358 [D loss: 0.243451, acc.: 54.69%] [G loss: 0.440240]\n",
      "epoch:13 step:12359 [D loss: 0.219636, acc.: 64.06%] [G loss: 0.427651]\n",
      "epoch:13 step:12360 [D loss: 0.222574, acc.: 64.06%] [G loss: 0.410425]\n",
      "epoch:13 step:12361 [D loss: 0.237915, acc.: 60.16%] [G loss: 0.396006]\n",
      "epoch:13 step:12362 [D loss: 0.222365, acc.: 64.06%] [G loss: 0.446382]\n",
      "epoch:13 step:12363 [D loss: 0.229975, acc.: 66.41%] [G loss: 0.434909]\n",
      "epoch:13 step:12364 [D loss: 0.236222, acc.: 60.16%] [G loss: 0.430648]\n",
      "epoch:13 step:12365 [D loss: 0.215933, acc.: 61.72%] [G loss: 0.455726]\n",
      "epoch:13 step:12366 [D loss: 0.238557, acc.: 62.50%] [G loss: 0.413161]\n",
      "epoch:13 step:12367 [D loss: 0.257992, acc.: 57.03%] [G loss: 0.408275]\n",
      "epoch:13 step:12368 [D loss: 0.248811, acc.: 58.59%] [G loss: 0.436560]\n",
      "epoch:13 step:12369 [D loss: 0.207584, acc.: 67.97%] [G loss: 0.461508]\n",
      "epoch:13 step:12370 [D loss: 0.232680, acc.: 56.25%] [G loss: 0.410141]\n",
      "epoch:13 step:12371 [D loss: 0.193770, acc.: 72.66%] [G loss: 0.436211]\n",
      "epoch:13 step:12372 [D loss: 0.197351, acc.: 71.09%] [G loss: 0.450738]\n",
      "epoch:13 step:12373 [D loss: 0.217008, acc.: 64.06%] [G loss: 0.461768]\n",
      "epoch:13 step:12374 [D loss: 0.235586, acc.: 60.16%] [G loss: 0.418896]\n",
      "epoch:13 step:12375 [D loss: 0.195304, acc.: 71.09%] [G loss: 0.448906]\n",
      "epoch:13 step:12376 [D loss: 0.211081, acc.: 67.97%] [G loss: 0.468640]\n",
      "epoch:13 step:12377 [D loss: 0.239496, acc.: 64.84%] [G loss: 0.445255]\n",
      "epoch:13 step:12378 [D loss: 0.210780, acc.: 67.19%] [G loss: 0.453754]\n",
      "epoch:13 step:12379 [D loss: 0.195923, acc.: 71.09%] [G loss: 0.457306]\n",
      "epoch:13 step:12380 [D loss: 0.217562, acc.: 60.94%] [G loss: 0.447889]\n",
      "epoch:13 step:12381 [D loss: 0.239825, acc.: 59.38%] [G loss: 0.412205]\n",
      "epoch:13 step:12382 [D loss: 0.234202, acc.: 60.94%] [G loss: 0.438893]\n",
      "epoch:13 step:12383 [D loss: 0.234492, acc.: 59.38%] [G loss: 0.457583]\n",
      "epoch:13 step:12384 [D loss: 0.265765, acc.: 59.38%] [G loss: 0.436881]\n",
      "epoch:13 step:12385 [D loss: 0.227101, acc.: 66.41%] [G loss: 0.430023]\n",
      "epoch:13 step:12386 [D loss: 0.210415, acc.: 68.75%] [G loss: 0.420605]\n",
      "epoch:13 step:12387 [D loss: 0.221072, acc.: 63.28%] [G loss: 0.440832]\n",
      "epoch:13 step:12388 [D loss: 0.195812, acc.: 70.31%] [G loss: 0.510619]\n",
      "epoch:13 step:12389 [D loss: 0.184193, acc.: 73.44%] [G loss: 0.504546]\n",
      "epoch:13 step:12390 [D loss: 0.190610, acc.: 71.88%] [G loss: 0.519489]\n",
      "epoch:13 step:12391 [D loss: 0.270580, acc.: 53.12%] [G loss: 0.435201]\n",
      "epoch:13 step:12392 [D loss: 0.223734, acc.: 67.19%] [G loss: 0.433906]\n",
      "epoch:13 step:12393 [D loss: 0.210836, acc.: 69.53%] [G loss: 0.421532]\n",
      "epoch:13 step:12394 [D loss: 0.246220, acc.: 59.38%] [G loss: 0.419841]\n",
      "epoch:13 step:12395 [D loss: 0.249988, acc.: 57.03%] [G loss: 0.434352]\n",
      "epoch:13 step:12396 [D loss: 0.247038, acc.: 55.47%] [G loss: 0.389810]\n",
      "epoch:13 step:12397 [D loss: 0.220789, acc.: 63.28%] [G loss: 0.477132]\n",
      "epoch:13 step:12398 [D loss: 0.240255, acc.: 53.12%] [G loss: 0.419744]\n",
      "epoch:13 step:12399 [D loss: 0.185580, acc.: 73.44%] [G loss: 0.478956]\n",
      "epoch:13 step:12400 [D loss: 0.180554, acc.: 75.00%] [G loss: 0.487060]\n",
      "##############\n",
      "[2.42965313 1.53410385 6.35811275 4.7464129  3.56979872 5.76436087\n",
      " 4.55328699 4.83294128 4.51903134 3.914639  ]\n",
      "##########\n",
      "epoch:13 step:12401 [D loss: 0.273380, acc.: 57.81%] [G loss: 0.441101]\n",
      "epoch:13 step:12402 [D loss: 0.211728, acc.: 66.41%] [G loss: 0.461815]\n",
      "epoch:13 step:12403 [D loss: 0.212863, acc.: 61.72%] [G loss: 0.464921]\n",
      "epoch:13 step:12404 [D loss: 0.197751, acc.: 64.84%] [G loss: 0.482434]\n",
      "epoch:13 step:12405 [D loss: 0.223407, acc.: 67.97%] [G loss: 0.467721]\n",
      "epoch:13 step:12406 [D loss: 0.205642, acc.: 71.09%] [G loss: 0.431493]\n",
      "epoch:13 step:12407 [D loss: 0.239979, acc.: 60.94%] [G loss: 0.417060]\n",
      "epoch:13 step:12408 [D loss: 0.203925, acc.: 67.97%] [G loss: 0.411447]\n",
      "epoch:13 step:12409 [D loss: 0.224501, acc.: 66.41%] [G loss: 0.427766]\n",
      "epoch:13 step:12410 [D loss: 0.182632, acc.: 73.44%] [G loss: 0.440198]\n",
      "epoch:13 step:12411 [D loss: 0.205036, acc.: 62.50%] [G loss: 0.444992]\n",
      "epoch:13 step:12412 [D loss: 0.187988, acc.: 71.88%] [G loss: 0.477878]\n",
      "epoch:13 step:12413 [D loss: 0.181666, acc.: 73.44%] [G loss: 0.498374]\n",
      "epoch:13 step:12414 [D loss: 0.233313, acc.: 61.72%] [G loss: 0.452402]\n",
      "epoch:13 step:12415 [D loss: 0.267016, acc.: 53.91%] [G loss: 0.452468]\n",
      "epoch:13 step:12416 [D loss: 0.215041, acc.: 68.75%] [G loss: 0.479485]\n",
      "epoch:13 step:12417 [D loss: 0.191514, acc.: 72.66%] [G loss: 0.474323]\n",
      "epoch:13 step:12418 [D loss: 0.229365, acc.: 58.59%] [G loss: 0.432870]\n",
      "epoch:13 step:12419 [D loss: 0.219809, acc.: 66.41%] [G loss: 0.423788]\n",
      "epoch:13 step:12420 [D loss: 0.204000, acc.: 69.53%] [G loss: 0.442223]\n",
      "epoch:13 step:12421 [D loss: 0.226774, acc.: 65.62%] [G loss: 0.445946]\n",
      "epoch:13 step:12422 [D loss: 0.211091, acc.: 72.66%] [G loss: 0.445130]\n",
      "epoch:13 step:12423 [D loss: 0.205026, acc.: 71.09%] [G loss: 0.466354]\n",
      "epoch:13 step:12424 [D loss: 0.231734, acc.: 59.38%] [G loss: 0.443602]\n",
      "epoch:13 step:12425 [D loss: 0.211722, acc.: 67.19%] [G loss: 0.484087]\n",
      "epoch:13 step:12426 [D loss: 0.220038, acc.: 68.75%] [G loss: 0.446091]\n",
      "epoch:13 step:12427 [D loss: 0.205734, acc.: 68.75%] [G loss: 0.465711]\n",
      "epoch:13 step:12428 [D loss: 0.226032, acc.: 63.28%] [G loss: 0.467025]\n",
      "epoch:13 step:12429 [D loss: 0.203348, acc.: 71.88%] [G loss: 0.485413]\n",
      "epoch:13 step:12430 [D loss: 0.271110, acc.: 50.78%] [G loss: 0.439557]\n",
      "epoch:13 step:12431 [D loss: 0.265640, acc.: 58.59%] [G loss: 0.424056]\n",
      "epoch:13 step:12432 [D loss: 0.235260, acc.: 52.34%] [G loss: 0.503207]\n",
      "epoch:13 step:12433 [D loss: 0.250016, acc.: 60.16%] [G loss: 0.465984]\n",
      "epoch:13 step:12434 [D loss: 0.207898, acc.: 63.28%] [G loss: 0.450538]\n",
      "epoch:13 step:12435 [D loss: 0.217385, acc.: 65.62%] [G loss: 0.436727]\n",
      "epoch:13 step:12436 [D loss: 0.222233, acc.: 62.50%] [G loss: 0.440868]\n",
      "epoch:13 step:12437 [D loss: 0.247321, acc.: 57.81%] [G loss: 0.453914]\n",
      "epoch:13 step:12438 [D loss: 0.233758, acc.: 57.81%] [G loss: 0.466911]\n",
      "epoch:13 step:12439 [D loss: 0.220634, acc.: 62.50%] [G loss: 0.430361]\n",
      "epoch:13 step:12440 [D loss: 0.207007, acc.: 67.19%] [G loss: 0.420454]\n",
      "epoch:13 step:12441 [D loss: 0.240207, acc.: 58.59%] [G loss: 0.443228]\n",
      "epoch:13 step:12442 [D loss: 0.212453, acc.: 65.62%] [G loss: 0.461247]\n",
      "epoch:13 step:12443 [D loss: 0.220275, acc.: 60.16%] [G loss: 0.450021]\n",
      "epoch:13 step:12444 [D loss: 0.246942, acc.: 57.03%] [G loss: 0.436650]\n",
      "epoch:13 step:12445 [D loss: 0.191246, acc.: 75.78%] [G loss: 0.485613]\n",
      "epoch:13 step:12446 [D loss: 0.254001, acc.: 53.12%] [G loss: 0.423199]\n",
      "epoch:13 step:12447 [D loss: 0.242237, acc.: 55.47%] [G loss: 0.434333]\n",
      "epoch:13 step:12448 [D loss: 0.233105, acc.: 53.12%] [G loss: 0.408128]\n",
      "epoch:13 step:12449 [D loss: 0.225291, acc.: 64.06%] [G loss: 0.390537]\n",
      "epoch:13 step:12450 [D loss: 0.242607, acc.: 57.03%] [G loss: 0.445158]\n",
      "epoch:13 step:12451 [D loss: 0.204995, acc.: 71.88%] [G loss: 0.468010]\n",
      "epoch:13 step:12452 [D loss: 0.182736, acc.: 75.78%] [G loss: 0.501591]\n",
      "epoch:13 step:12453 [D loss: 0.212127, acc.: 64.06%] [G loss: 0.475131]\n",
      "epoch:13 step:12454 [D loss: 0.234063, acc.: 56.25%] [G loss: 0.416043]\n",
      "epoch:13 step:12455 [D loss: 0.204499, acc.: 66.41%] [G loss: 0.474592]\n",
      "epoch:13 step:12456 [D loss: 0.219974, acc.: 66.41%] [G loss: 0.459459]\n",
      "epoch:13 step:12457 [D loss: 0.204218, acc.: 71.88%] [G loss: 0.425803]\n",
      "epoch:13 step:12458 [D loss: 0.254320, acc.: 57.03%] [G loss: 0.469641]\n",
      "epoch:13 step:12459 [D loss: 0.264100, acc.: 53.12%] [G loss: 0.427479]\n",
      "epoch:13 step:12460 [D loss: 0.211637, acc.: 67.19%] [G loss: 0.433996]\n",
      "epoch:13 step:12461 [D loss: 0.207319, acc.: 67.97%] [G loss: 0.485889]\n",
      "epoch:13 step:12462 [D loss: 0.259239, acc.: 59.38%] [G loss: 0.450392]\n",
      "epoch:13 step:12463 [D loss: 0.253005, acc.: 53.91%] [G loss: 0.416141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12464 [D loss: 0.220769, acc.: 65.62%] [G loss: 0.428208]\n",
      "epoch:13 step:12465 [D loss: 0.244574, acc.: 58.59%] [G loss: 0.440166]\n",
      "epoch:13 step:12466 [D loss: 0.214927, acc.: 64.06%] [G loss: 0.451481]\n",
      "epoch:13 step:12467 [D loss: 0.204122, acc.: 66.41%] [G loss: 0.487135]\n",
      "epoch:13 step:12468 [D loss: 0.255611, acc.: 54.69%] [G loss: 0.407295]\n",
      "epoch:13 step:12469 [D loss: 0.221043, acc.: 67.97%] [G loss: 0.399496]\n",
      "epoch:13 step:12470 [D loss: 0.218736, acc.: 66.41%] [G loss: 0.459254]\n",
      "epoch:13 step:12471 [D loss: 0.205011, acc.: 66.41%] [G loss: 0.467033]\n",
      "epoch:13 step:12472 [D loss: 0.249007, acc.: 60.94%] [G loss: 0.459221]\n",
      "epoch:13 step:12473 [D loss: 0.227324, acc.: 60.94%] [G loss: 0.435588]\n",
      "epoch:13 step:12474 [D loss: 0.217255, acc.: 67.19%] [G loss: 0.433182]\n",
      "epoch:13 step:12475 [D loss: 0.262662, acc.: 50.00%] [G loss: 0.407781]\n",
      "epoch:13 step:12476 [D loss: 0.217286, acc.: 65.62%] [G loss: 0.440924]\n",
      "epoch:13 step:12477 [D loss: 0.208112, acc.: 67.97%] [G loss: 0.443182]\n",
      "epoch:13 step:12478 [D loss: 0.230128, acc.: 62.50%] [G loss: 0.436350]\n",
      "epoch:13 step:12479 [D loss: 0.194457, acc.: 72.66%] [G loss: 0.479774]\n",
      "epoch:13 step:12480 [D loss: 0.221397, acc.: 64.84%] [G loss: 0.466755]\n",
      "epoch:13 step:12481 [D loss: 0.212659, acc.: 65.62%] [G loss: 0.455367]\n",
      "epoch:13 step:12482 [D loss: 0.230715, acc.: 60.16%] [G loss: 0.457373]\n",
      "epoch:13 step:12483 [D loss: 0.201471, acc.: 67.97%] [G loss: 0.461166]\n",
      "epoch:13 step:12484 [D loss: 0.228027, acc.: 57.81%] [G loss: 0.453017]\n",
      "epoch:13 step:12485 [D loss: 0.219668, acc.: 67.19%] [G loss: 0.451367]\n",
      "epoch:13 step:12486 [D loss: 0.225540, acc.: 64.06%] [G loss: 0.452184]\n",
      "epoch:13 step:12487 [D loss: 0.237496, acc.: 58.59%] [G loss: 0.467441]\n",
      "epoch:13 step:12488 [D loss: 0.213775, acc.: 64.84%] [G loss: 0.465488]\n",
      "epoch:13 step:12489 [D loss: 0.247694, acc.: 58.59%] [G loss: 0.438797]\n",
      "epoch:13 step:12490 [D loss: 0.216918, acc.: 67.97%] [G loss: 0.462238]\n",
      "epoch:13 step:12491 [D loss: 0.209479, acc.: 67.19%] [G loss: 0.477256]\n",
      "epoch:13 step:12492 [D loss: 0.227778, acc.: 64.84%] [G loss: 0.438016]\n",
      "epoch:13 step:12493 [D loss: 0.193686, acc.: 71.88%] [G loss: 0.436408]\n",
      "epoch:13 step:12494 [D loss: 0.174580, acc.: 76.56%] [G loss: 0.478404]\n",
      "epoch:13 step:12495 [D loss: 0.181668, acc.: 72.66%] [G loss: 0.535145]\n",
      "epoch:13 step:12496 [D loss: 0.194291, acc.: 69.53%] [G loss: 0.465554]\n",
      "epoch:13 step:12497 [D loss: 0.274460, acc.: 55.47%] [G loss: 0.447054]\n",
      "epoch:13 step:12498 [D loss: 0.240085, acc.: 60.16%] [G loss: 0.418508]\n",
      "epoch:13 step:12499 [D loss: 0.211150, acc.: 67.19%] [G loss: 0.441840]\n",
      "epoch:13 step:12500 [D loss: 0.213969, acc.: 68.75%] [G loss: 0.442884]\n",
      "epoch:13 step:12501 [D loss: 0.214820, acc.: 64.06%] [G loss: 0.424527]\n",
      "epoch:13 step:12502 [D loss: 0.197955, acc.: 71.09%] [G loss: 0.475090]\n",
      "epoch:13 step:12503 [D loss: 0.221544, acc.: 64.84%] [G loss: 0.460462]\n",
      "epoch:13 step:12504 [D loss: 0.235171, acc.: 56.25%] [G loss: 0.476314]\n",
      "epoch:13 step:12505 [D loss: 0.241503, acc.: 60.94%] [G loss: 0.396421]\n",
      "epoch:13 step:12506 [D loss: 0.263857, acc.: 55.47%] [G loss: 0.422263]\n",
      "epoch:13 step:12507 [D loss: 0.216671, acc.: 59.38%] [G loss: 0.439013]\n",
      "epoch:13 step:12508 [D loss: 0.217760, acc.: 66.41%] [G loss: 0.464983]\n",
      "epoch:13 step:12509 [D loss: 0.220447, acc.: 60.94%] [G loss: 0.458406]\n",
      "epoch:13 step:12510 [D loss: 0.205375, acc.: 72.66%] [G loss: 0.480933]\n",
      "epoch:13 step:12511 [D loss: 0.242732, acc.: 57.81%] [G loss: 0.408888]\n",
      "epoch:13 step:12512 [D loss: 0.215562, acc.: 63.28%] [G loss: 0.455304]\n",
      "epoch:13 step:12513 [D loss: 0.209631, acc.: 69.53%] [G loss: 0.460801]\n",
      "epoch:13 step:12514 [D loss: 0.207272, acc.: 71.09%] [G loss: 0.476788]\n",
      "epoch:13 step:12515 [D loss: 0.207586, acc.: 71.88%] [G loss: 0.474998]\n",
      "epoch:13 step:12516 [D loss: 0.231401, acc.: 60.16%] [G loss: 0.447443]\n",
      "epoch:13 step:12517 [D loss: 0.211003, acc.: 69.53%] [G loss: 0.503024]\n",
      "epoch:13 step:12518 [D loss: 0.203865, acc.: 67.97%] [G loss: 0.490950]\n",
      "epoch:13 step:12519 [D loss: 0.222495, acc.: 64.06%] [G loss: 0.453995]\n",
      "epoch:13 step:12520 [D loss: 0.205592, acc.: 69.53%] [G loss: 0.470194]\n",
      "epoch:13 step:12521 [D loss: 0.198920, acc.: 67.97%] [G loss: 0.471570]\n",
      "epoch:13 step:12522 [D loss: 0.279342, acc.: 56.25%] [G loss: 0.451667]\n",
      "epoch:13 step:12523 [D loss: 0.231985, acc.: 61.72%] [G loss: 0.450136]\n",
      "epoch:13 step:12524 [D loss: 0.199889, acc.: 64.06%] [G loss: 0.510844]\n",
      "epoch:13 step:12525 [D loss: 0.206929, acc.: 67.19%] [G loss: 0.484418]\n",
      "epoch:13 step:12526 [D loss: 0.221480, acc.: 63.28%] [G loss: 0.445989]\n",
      "epoch:13 step:12527 [D loss: 0.188975, acc.: 72.66%] [G loss: 0.450523]\n",
      "epoch:13 step:12528 [D loss: 0.189698, acc.: 71.09%] [G loss: 0.517331]\n",
      "epoch:13 step:12529 [D loss: 0.289527, acc.: 54.69%] [G loss: 0.450978]\n",
      "epoch:13 step:12530 [D loss: 0.256695, acc.: 52.34%] [G loss: 0.423021]\n",
      "epoch:13 step:12531 [D loss: 0.234232, acc.: 64.06%] [G loss: 0.423733]\n",
      "epoch:13 step:12532 [D loss: 0.216827, acc.: 66.41%] [G loss: 0.467408]\n",
      "epoch:13 step:12533 [D loss: 0.247023, acc.: 54.69%] [G loss: 0.464661]\n",
      "epoch:13 step:12534 [D loss: 0.230096, acc.: 59.38%] [G loss: 0.450209]\n",
      "epoch:13 step:12535 [D loss: 0.173036, acc.: 73.44%] [G loss: 0.531556]\n",
      "epoch:13 step:12536 [D loss: 0.225255, acc.: 65.62%] [G loss: 0.454360]\n",
      "epoch:13 step:12537 [D loss: 0.245252, acc.: 59.38%] [G loss: 0.433627]\n",
      "epoch:13 step:12538 [D loss: 0.264700, acc.: 58.59%] [G loss: 0.444515]\n",
      "epoch:13 step:12539 [D loss: 0.205477, acc.: 64.84%] [G loss: 0.487857]\n",
      "epoch:13 step:12540 [D loss: 0.182806, acc.: 71.88%] [G loss: 0.489798]\n",
      "epoch:13 step:12541 [D loss: 0.214356, acc.: 64.06%] [G loss: 0.456466]\n",
      "epoch:13 step:12542 [D loss: 0.204420, acc.: 65.62%] [G loss: 0.448511]\n",
      "epoch:13 step:12543 [D loss: 0.211332, acc.: 64.06%] [G loss: 0.443015]\n",
      "epoch:13 step:12544 [D loss: 0.215486, acc.: 68.75%] [G loss: 0.426945]\n",
      "epoch:13 step:12545 [D loss: 0.208895, acc.: 67.19%] [G loss: 0.418072]\n",
      "epoch:13 step:12546 [D loss: 0.228503, acc.: 62.50%] [G loss: 0.422852]\n",
      "epoch:13 step:12547 [D loss: 0.223048, acc.: 64.84%] [G loss: 0.432784]\n",
      "epoch:13 step:12548 [D loss: 0.233046, acc.: 63.28%] [G loss: 0.416115]\n",
      "epoch:13 step:12549 [D loss: 0.228473, acc.: 60.16%] [G loss: 0.443143]\n",
      "epoch:13 step:12550 [D loss: 0.239058, acc.: 61.72%] [G loss: 0.421542]\n",
      "epoch:13 step:12551 [D loss: 0.204785, acc.: 70.31%] [G loss: 0.447166]\n",
      "epoch:13 step:12552 [D loss: 0.198612, acc.: 65.62%] [G loss: 0.484696]\n",
      "epoch:13 step:12553 [D loss: 0.224603, acc.: 62.50%] [G loss: 0.451735]\n",
      "epoch:13 step:12554 [D loss: 0.225911, acc.: 65.62%] [G loss: 0.446064]\n",
      "epoch:13 step:12555 [D loss: 0.188605, acc.: 70.31%] [G loss: 0.465207]\n",
      "epoch:13 step:12556 [D loss: 0.236708, acc.: 57.81%] [G loss: 0.444692]\n",
      "epoch:13 step:12557 [D loss: 0.263211, acc.: 53.91%] [G loss: 0.403390]\n",
      "epoch:13 step:12558 [D loss: 0.257457, acc.: 53.91%] [G loss: 0.422655]\n",
      "epoch:13 step:12559 [D loss: 0.220352, acc.: 64.06%] [G loss: 0.423857]\n",
      "epoch:13 step:12560 [D loss: 0.220942, acc.: 64.84%] [G loss: 0.458435]\n",
      "epoch:13 step:12561 [D loss: 0.239547, acc.: 59.38%] [G loss: 0.447791]\n",
      "epoch:13 step:12562 [D loss: 0.184646, acc.: 74.22%] [G loss: 0.467337]\n",
      "epoch:13 step:12563 [D loss: 0.234839, acc.: 59.38%] [G loss: 0.429038]\n",
      "epoch:13 step:12564 [D loss: 0.242190, acc.: 61.72%] [G loss: 0.413880]\n",
      "epoch:13 step:12565 [D loss: 0.214195, acc.: 67.19%] [G loss: 0.423467]\n",
      "epoch:13 step:12566 [D loss: 0.194094, acc.: 66.41%] [G loss: 0.436700]\n",
      "epoch:13 step:12567 [D loss: 0.220404, acc.: 63.28%] [G loss: 0.465444]\n",
      "epoch:13 step:12568 [D loss: 0.229998, acc.: 60.94%] [G loss: 0.442114]\n",
      "epoch:13 step:12569 [D loss: 0.200243, acc.: 67.19%] [G loss: 0.462435]\n",
      "epoch:13 step:12570 [D loss: 0.236243, acc.: 57.03%] [G loss: 0.471681]\n",
      "epoch:13 step:12571 [D loss: 0.255783, acc.: 59.38%] [G loss: 0.416533]\n",
      "epoch:13 step:12572 [D loss: 0.227683, acc.: 62.50%] [G loss: 0.405508]\n",
      "epoch:13 step:12573 [D loss: 0.236926, acc.: 64.84%] [G loss: 0.397772]\n",
      "epoch:13 step:12574 [D loss: 0.225453, acc.: 67.97%] [G loss: 0.437627]\n",
      "epoch:13 step:12575 [D loss: 0.205084, acc.: 71.88%] [G loss: 0.453110]\n",
      "epoch:13 step:12576 [D loss: 0.230114, acc.: 58.59%] [G loss: 0.465311]\n",
      "epoch:13 step:12577 [D loss: 0.265806, acc.: 50.00%] [G loss: 0.451636]\n",
      "epoch:13 step:12578 [D loss: 0.220007, acc.: 66.41%] [G loss: 0.448003]\n",
      "epoch:13 step:12579 [D loss: 0.192539, acc.: 71.09%] [G loss: 0.455423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12580 [D loss: 0.206291, acc.: 70.31%] [G loss: 0.483350]\n",
      "epoch:13 step:12581 [D loss: 0.267369, acc.: 51.56%] [G loss: 0.427629]\n",
      "epoch:13 step:12582 [D loss: 0.238048, acc.: 57.81%] [G loss: 0.422333]\n",
      "epoch:13 step:12583 [D loss: 0.231442, acc.: 63.28%] [G loss: 0.471213]\n",
      "epoch:13 step:12584 [D loss: 0.236689, acc.: 63.28%] [G loss: 0.445870]\n",
      "epoch:13 step:12585 [D loss: 0.213980, acc.: 68.75%] [G loss: 0.417063]\n",
      "epoch:13 step:12586 [D loss: 0.206619, acc.: 67.19%] [G loss: 0.419449]\n",
      "epoch:13 step:12587 [D loss: 0.209894, acc.: 71.88%] [G loss: 0.483040]\n",
      "epoch:13 step:12588 [D loss: 0.241169, acc.: 57.03%] [G loss: 0.494341]\n",
      "epoch:13 step:12589 [D loss: 0.248510, acc.: 52.34%] [G loss: 0.456466]\n",
      "epoch:13 step:12590 [D loss: 0.225482, acc.: 58.59%] [G loss: 0.445423]\n",
      "epoch:13 step:12591 [D loss: 0.262257, acc.: 59.38%] [G loss: 0.411709]\n",
      "epoch:13 step:12592 [D loss: 0.247558, acc.: 57.03%] [G loss: 0.435016]\n",
      "epoch:13 step:12593 [D loss: 0.196537, acc.: 70.31%] [G loss: 0.466445]\n",
      "epoch:13 step:12594 [D loss: 0.244291, acc.: 60.16%] [G loss: 0.424541]\n",
      "epoch:13 step:12595 [D loss: 0.223317, acc.: 60.94%] [G loss: 0.438227]\n",
      "epoch:13 step:12596 [D loss: 0.204697, acc.: 69.53%] [G loss: 0.456744]\n",
      "epoch:13 step:12597 [D loss: 0.217770, acc.: 65.62%] [G loss: 0.443984]\n",
      "epoch:13 step:12598 [D loss: 0.243410, acc.: 60.16%] [G loss: 0.464284]\n",
      "epoch:13 step:12599 [D loss: 0.266647, acc.: 53.12%] [G loss: 0.445568]\n",
      "epoch:13 step:12600 [D loss: 0.245570, acc.: 57.81%] [G loss: 0.449999]\n",
      "##############\n",
      "[2.58927018 1.79853763 6.0775908  4.68212726 3.72764426 5.54507689\n",
      " 4.42079434 4.89803179 4.51047195 3.64320038]\n",
      "##########\n",
      "epoch:13 step:12601 [D loss: 0.249971, acc.: 53.91%] [G loss: 0.445277]\n",
      "epoch:13 step:12602 [D loss: 0.249608, acc.: 58.59%] [G loss: 0.444281]\n",
      "epoch:13 step:12603 [D loss: 0.240399, acc.: 54.69%] [G loss: 0.434546]\n",
      "epoch:13 step:12604 [D loss: 0.227659, acc.: 61.72%] [G loss: 0.437127]\n",
      "epoch:13 step:12605 [D loss: 0.247171, acc.: 53.91%] [G loss: 0.430694]\n",
      "epoch:13 step:12606 [D loss: 0.238790, acc.: 60.94%] [G loss: 0.402677]\n",
      "epoch:13 step:12607 [D loss: 0.209977, acc.: 66.41%] [G loss: 0.439467]\n",
      "epoch:13 step:12608 [D loss: 0.227470, acc.: 59.38%] [G loss: 0.455684]\n",
      "epoch:13 step:12609 [D loss: 0.202401, acc.: 67.97%] [G loss: 0.453678]\n",
      "epoch:13 step:12610 [D loss: 0.232476, acc.: 63.28%] [G loss: 0.492590]\n",
      "epoch:13 step:12611 [D loss: 0.209456, acc.: 68.75%] [G loss: 0.495050]\n",
      "epoch:13 step:12612 [D loss: 0.205843, acc.: 67.19%] [G loss: 0.442439]\n",
      "epoch:13 step:12613 [D loss: 0.228831, acc.: 60.94%] [G loss: 0.434089]\n",
      "epoch:13 step:12614 [D loss: 0.232387, acc.: 57.81%] [G loss: 0.422594]\n",
      "epoch:13 step:12615 [D loss: 0.214889, acc.: 62.50%] [G loss: 0.477064]\n",
      "epoch:13 step:12616 [D loss: 0.215190, acc.: 65.62%] [G loss: 0.496846]\n",
      "epoch:13 step:12617 [D loss: 0.201564, acc.: 69.53%] [G loss: 0.445626]\n",
      "epoch:13 step:12618 [D loss: 0.257387, acc.: 50.00%] [G loss: 0.423390]\n",
      "epoch:13 step:12619 [D loss: 0.230588, acc.: 59.38%] [G loss: 0.383241]\n",
      "epoch:13 step:12620 [D loss: 0.211528, acc.: 69.53%] [G loss: 0.440695]\n",
      "epoch:13 step:12621 [D loss: 0.200201, acc.: 66.41%] [G loss: 0.482308]\n",
      "epoch:13 step:12622 [D loss: 0.224827, acc.: 63.28%] [G loss: 0.452290]\n",
      "epoch:13 step:12623 [D loss: 0.235995, acc.: 64.06%] [G loss: 0.477582]\n",
      "epoch:13 step:12624 [D loss: 0.219588, acc.: 65.62%] [G loss: 0.447826]\n",
      "epoch:13 step:12625 [D loss: 0.236563, acc.: 60.94%] [G loss: 0.429220]\n",
      "epoch:13 step:12626 [D loss: 0.197517, acc.: 67.19%] [G loss: 0.474834]\n",
      "epoch:13 step:12627 [D loss: 0.244097, acc.: 58.59%] [G loss: 0.451633]\n",
      "epoch:13 step:12628 [D loss: 0.196201, acc.: 74.22%] [G loss: 0.475392]\n",
      "epoch:13 step:12629 [D loss: 0.255967, acc.: 56.25%] [G loss: 0.433085]\n",
      "epoch:13 step:12630 [D loss: 0.198900, acc.: 71.88%] [G loss: 0.486947]\n",
      "epoch:13 step:12631 [D loss: 0.196702, acc.: 67.19%] [G loss: 0.478013]\n",
      "epoch:13 step:12632 [D loss: 0.180863, acc.: 71.88%] [G loss: 0.484880]\n",
      "epoch:13 step:12633 [D loss: 0.196274, acc.: 70.31%] [G loss: 0.468876]\n",
      "epoch:13 step:12634 [D loss: 0.224951, acc.: 64.06%] [G loss: 0.440590]\n",
      "epoch:13 step:12635 [D loss: 0.216128, acc.: 68.75%] [G loss: 0.451410]\n",
      "epoch:13 step:12636 [D loss: 0.245337, acc.: 60.94%] [G loss: 0.448561]\n",
      "epoch:13 step:12637 [D loss: 0.264936, acc.: 52.34%] [G loss: 0.431975]\n",
      "epoch:13 step:12638 [D loss: 0.225400, acc.: 61.72%] [G loss: 0.481310]\n",
      "epoch:13 step:12639 [D loss: 0.274381, acc.: 51.56%] [G loss: 0.431070]\n",
      "epoch:13 step:12640 [D loss: 0.247914, acc.: 47.66%] [G loss: 0.412928]\n",
      "epoch:13 step:12641 [D loss: 0.234198, acc.: 61.72%] [G loss: 0.399223]\n",
      "epoch:13 step:12642 [D loss: 0.226828, acc.: 67.19%] [G loss: 0.393808]\n",
      "epoch:13 step:12643 [D loss: 0.218570, acc.: 62.50%] [G loss: 0.440240]\n",
      "epoch:13 step:12644 [D loss: 0.241161, acc.: 58.59%] [G loss: 0.399950]\n",
      "epoch:13 step:12645 [D loss: 0.243635, acc.: 60.16%] [G loss: 0.421383]\n",
      "epoch:13 step:12646 [D loss: 0.252171, acc.: 56.25%] [G loss: 0.412832]\n",
      "epoch:13 step:12647 [D loss: 0.239728, acc.: 60.16%] [G loss: 0.405748]\n",
      "epoch:13 step:12648 [D loss: 0.228039, acc.: 58.59%] [G loss: 0.454672]\n",
      "epoch:13 step:12649 [D loss: 0.230864, acc.: 57.03%] [G loss: 0.431064]\n",
      "epoch:13 step:12650 [D loss: 0.198827, acc.: 71.88%] [G loss: 0.513294]\n",
      "epoch:13 step:12651 [D loss: 0.205553, acc.: 71.88%] [G loss: 0.507967]\n",
      "epoch:13 step:12652 [D loss: 0.210195, acc.: 70.31%] [G loss: 0.489377]\n",
      "epoch:13 step:12653 [D loss: 0.218064, acc.: 66.41%] [G loss: 0.476529]\n",
      "epoch:13 step:12654 [D loss: 0.236666, acc.: 61.72%] [G loss: 0.464669]\n",
      "epoch:13 step:12655 [D loss: 0.216859, acc.: 63.28%] [G loss: 0.439418]\n",
      "epoch:13 step:12656 [D loss: 0.203374, acc.: 67.97%] [G loss: 0.440423]\n",
      "epoch:13 step:12657 [D loss: 0.247841, acc.: 59.38%] [G loss: 0.439286]\n",
      "epoch:13 step:12658 [D loss: 0.244682, acc.: 56.25%] [G loss: 0.429054]\n",
      "epoch:13 step:12659 [D loss: 0.264197, acc.: 55.47%] [G loss: 0.383961]\n",
      "epoch:13 step:12660 [D loss: 0.228138, acc.: 63.28%] [G loss: 0.394787]\n",
      "epoch:13 step:12661 [D loss: 0.221708, acc.: 66.41%] [G loss: 0.412763]\n",
      "epoch:13 step:12662 [D loss: 0.194990, acc.: 69.53%] [G loss: 0.476688]\n",
      "epoch:13 step:12663 [D loss: 0.254676, acc.: 53.91%] [G loss: 0.441814]\n",
      "epoch:13 step:12664 [D loss: 0.228229, acc.: 64.84%] [G loss: 0.449880]\n",
      "epoch:13 step:12665 [D loss: 0.209448, acc.: 69.53%] [G loss: 0.443482]\n",
      "epoch:13 step:12666 [D loss: 0.202367, acc.: 69.53%] [G loss: 0.459250]\n",
      "epoch:13 step:12667 [D loss: 0.243337, acc.: 57.81%] [G loss: 0.426665]\n",
      "epoch:13 step:12668 [D loss: 0.234308, acc.: 57.03%] [G loss: 0.439516]\n",
      "epoch:13 step:12669 [D loss: 0.211934, acc.: 66.41%] [G loss: 0.429043]\n",
      "epoch:13 step:12670 [D loss: 0.244543, acc.: 58.59%] [G loss: 0.453872]\n",
      "epoch:13 step:12671 [D loss: 0.234063, acc.: 64.84%] [G loss: 0.368996]\n",
      "epoch:13 step:12672 [D loss: 0.206266, acc.: 69.53%] [G loss: 0.455582]\n",
      "epoch:13 step:12673 [D loss: 0.250311, acc.: 54.69%] [G loss: 0.427110]\n",
      "epoch:13 step:12674 [D loss: 0.211520, acc.: 69.53%] [G loss: 0.474696]\n",
      "epoch:13 step:12675 [D loss: 0.220657, acc.: 69.53%] [G loss: 0.452015]\n",
      "epoch:13 step:12676 [D loss: 0.213120, acc.: 63.28%] [G loss: 0.471634]\n",
      "epoch:13 step:12677 [D loss: 0.227757, acc.: 64.84%] [G loss: 0.432061]\n",
      "epoch:13 step:12678 [D loss: 0.212926, acc.: 67.19%] [G loss: 0.488580]\n",
      "epoch:13 step:12679 [D loss: 0.201666, acc.: 71.09%] [G loss: 0.469182]\n",
      "epoch:13 step:12680 [D loss: 0.177962, acc.: 76.56%] [G loss: 0.462510]\n",
      "epoch:13 step:12681 [D loss: 0.243481, acc.: 64.84%] [G loss: 0.404450]\n",
      "epoch:13 step:12682 [D loss: 0.273534, acc.: 54.69%] [G loss: 0.381494]\n",
      "epoch:13 step:12683 [D loss: 0.273932, acc.: 50.78%] [G loss: 0.415492]\n",
      "epoch:13 step:12684 [D loss: 0.226618, acc.: 62.50%] [G loss: 0.450327]\n",
      "epoch:13 step:12685 [D loss: 0.194292, acc.: 73.44%] [G loss: 0.467222]\n",
      "epoch:13 step:12686 [D loss: 0.194476, acc.: 72.66%] [G loss: 0.472774]\n",
      "epoch:13 step:12687 [D loss: 0.218809, acc.: 62.50%] [G loss: 0.475758]\n",
      "epoch:13 step:12688 [D loss: 0.223660, acc.: 59.38%] [G loss: 0.482497]\n",
      "epoch:13 step:12689 [D loss: 0.212282, acc.: 61.72%] [G loss: 0.452719]\n",
      "epoch:13 step:12690 [D loss: 0.237315, acc.: 62.50%] [G loss: 0.448778]\n",
      "epoch:13 step:12691 [D loss: 0.248303, acc.: 60.16%] [G loss: 0.421557]\n",
      "epoch:13 step:12692 [D loss: 0.268243, acc.: 46.09%] [G loss: 0.392459]\n",
      "epoch:13 step:12693 [D loss: 0.197538, acc.: 70.31%] [G loss: 0.432286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12694 [D loss: 0.241964, acc.: 57.81%] [G loss: 0.465528]\n",
      "epoch:13 step:12695 [D loss: 0.196218, acc.: 71.09%] [G loss: 0.441719]\n",
      "epoch:13 step:12696 [D loss: 0.199991, acc.: 71.09%] [G loss: 0.496344]\n",
      "epoch:13 step:12697 [D loss: 0.203168, acc.: 74.22%] [G loss: 0.473194]\n",
      "epoch:13 step:12698 [D loss: 0.230945, acc.: 69.53%] [G loss: 0.415962]\n",
      "epoch:13 step:12699 [D loss: 0.240602, acc.: 58.59%] [G loss: 0.455877]\n",
      "epoch:13 step:12700 [D loss: 0.185402, acc.: 73.44%] [G loss: 0.509815]\n",
      "epoch:13 step:12701 [D loss: 0.214769, acc.: 61.72%] [G loss: 0.450091]\n",
      "epoch:13 step:12702 [D loss: 0.215919, acc.: 64.06%] [G loss: 0.452277]\n",
      "epoch:13 step:12703 [D loss: 0.222930, acc.: 66.41%] [G loss: 0.449738]\n",
      "epoch:13 step:12704 [D loss: 0.232040, acc.: 67.19%] [G loss: 0.437510]\n",
      "epoch:13 step:12705 [D loss: 0.234156, acc.: 64.06%] [G loss: 0.434696]\n",
      "epoch:13 step:12706 [D loss: 0.233844, acc.: 60.16%] [G loss: 0.459285]\n",
      "epoch:13 step:12707 [D loss: 0.238186, acc.: 64.84%] [G loss: 0.463192]\n",
      "epoch:13 step:12708 [D loss: 0.215896, acc.: 67.19%] [G loss: 0.450643]\n",
      "epoch:13 step:12709 [D loss: 0.247517, acc.: 62.50%] [G loss: 0.468819]\n",
      "epoch:13 step:12710 [D loss: 0.239414, acc.: 60.94%] [G loss: 0.429755]\n",
      "epoch:13 step:12711 [D loss: 0.199794, acc.: 69.53%] [G loss: 0.468324]\n",
      "epoch:13 step:12712 [D loss: 0.223654, acc.: 62.50%] [G loss: 0.457688]\n",
      "epoch:13 step:12713 [D loss: 0.229356, acc.: 57.03%] [G loss: 0.411894]\n",
      "epoch:13 step:12714 [D loss: 0.229526, acc.: 60.16%] [G loss: 0.432759]\n",
      "epoch:13 step:12715 [D loss: 0.185185, acc.: 67.97%] [G loss: 0.457981]\n",
      "epoch:13 step:12716 [D loss: 0.256807, acc.: 55.47%] [G loss: 0.432496]\n",
      "epoch:13 step:12717 [D loss: 0.206517, acc.: 67.19%] [G loss: 0.428489]\n",
      "epoch:13 step:12718 [D loss: 0.240133, acc.: 65.62%] [G loss: 0.420873]\n",
      "epoch:13 step:12719 [D loss: 0.236200, acc.: 58.59%] [G loss: 0.437419]\n",
      "epoch:13 step:12720 [D loss: 0.243664, acc.: 56.25%] [G loss: 0.438160]\n",
      "epoch:13 step:12721 [D loss: 0.256335, acc.: 49.22%] [G loss: 0.439750]\n",
      "epoch:13 step:12722 [D loss: 0.215435, acc.: 68.75%] [G loss: 0.483171]\n",
      "epoch:13 step:12723 [D loss: 0.287602, acc.: 48.44%] [G loss: 0.395721]\n",
      "epoch:13 step:12724 [D loss: 0.251813, acc.: 60.16%] [G loss: 0.423896]\n",
      "epoch:13 step:12725 [D loss: 0.235838, acc.: 57.81%] [G loss: 0.419123]\n",
      "epoch:13 step:12726 [D loss: 0.206725, acc.: 63.28%] [G loss: 0.444665]\n",
      "epoch:13 step:12727 [D loss: 0.230962, acc.: 60.94%] [G loss: 0.420309]\n",
      "epoch:13 step:12728 [D loss: 0.245144, acc.: 57.81%] [G loss: 0.446036]\n",
      "epoch:13 step:12729 [D loss: 0.221752, acc.: 63.28%] [G loss: 0.445090]\n",
      "epoch:13 step:12730 [D loss: 0.223586, acc.: 59.38%] [G loss: 0.448323]\n",
      "epoch:13 step:12731 [D loss: 0.203360, acc.: 69.53%] [G loss: 0.441982]\n",
      "epoch:13 step:12732 [D loss: 0.193464, acc.: 64.06%] [G loss: 0.469920]\n",
      "epoch:13 step:12733 [D loss: 0.200264, acc.: 65.62%] [G loss: 0.459970]\n",
      "epoch:13 step:12734 [D loss: 0.225232, acc.: 67.19%] [G loss: 0.432124]\n",
      "epoch:13 step:12735 [D loss: 0.211706, acc.: 67.19%] [G loss: 0.453818]\n",
      "epoch:13 step:12736 [D loss: 0.203613, acc.: 64.06%] [G loss: 0.448454]\n",
      "epoch:13 step:12737 [D loss: 0.202104, acc.: 67.97%] [G loss: 0.471519]\n",
      "epoch:13 step:12738 [D loss: 0.189048, acc.: 70.31%] [G loss: 0.470909]\n",
      "epoch:13 step:12739 [D loss: 0.213474, acc.: 64.84%] [G loss: 0.442338]\n",
      "epoch:13 step:12740 [D loss: 0.235906, acc.: 57.03%] [G loss: 0.441634]\n",
      "epoch:13 step:12741 [D loss: 0.246168, acc.: 57.81%] [G loss: 0.441051]\n",
      "epoch:13 step:12742 [D loss: 0.205573, acc.: 67.19%] [G loss: 0.489210]\n",
      "epoch:13 step:12743 [D loss: 0.254175, acc.: 57.81%] [G loss: 0.454331]\n",
      "epoch:13 step:12744 [D loss: 0.198496, acc.: 69.53%] [G loss: 0.473568]\n",
      "epoch:13 step:12745 [D loss: 0.194138, acc.: 69.53%] [G loss: 0.450075]\n",
      "epoch:13 step:12746 [D loss: 0.247767, acc.: 57.81%] [G loss: 0.469414]\n",
      "epoch:13 step:12747 [D loss: 0.261619, acc.: 52.34%] [G loss: 0.441593]\n",
      "epoch:13 step:12748 [D loss: 0.203084, acc.: 69.53%] [G loss: 0.423153]\n",
      "epoch:13 step:12749 [D loss: 0.217479, acc.: 64.84%] [G loss: 0.443343]\n",
      "epoch:13 step:12750 [D loss: 0.267959, acc.: 50.78%] [G loss: 0.379049]\n",
      "epoch:13 step:12751 [D loss: 0.224166, acc.: 60.94%] [G loss: 0.400081]\n",
      "epoch:13 step:12752 [D loss: 0.208898, acc.: 65.62%] [G loss: 0.435847]\n",
      "epoch:13 step:12753 [D loss: 0.218275, acc.: 64.84%] [G loss: 0.445668]\n",
      "epoch:13 step:12754 [D loss: 0.208969, acc.: 66.41%] [G loss: 0.447646]\n",
      "epoch:13 step:12755 [D loss: 0.177550, acc.: 72.66%] [G loss: 0.486053]\n",
      "epoch:13 step:12756 [D loss: 0.207305, acc.: 67.19%] [G loss: 0.483170]\n",
      "epoch:13 step:12757 [D loss: 0.263374, acc.: 47.66%] [G loss: 0.446878]\n",
      "epoch:13 step:12758 [D loss: 0.244692, acc.: 57.81%] [G loss: 0.410703]\n",
      "epoch:13 step:12759 [D loss: 0.236525, acc.: 59.38%] [G loss: 0.432032]\n",
      "epoch:13 step:12760 [D loss: 0.244414, acc.: 56.25%] [G loss: 0.430431]\n",
      "epoch:13 step:12761 [D loss: 0.240049, acc.: 57.81%] [G loss: 0.463476]\n",
      "epoch:13 step:12762 [D loss: 0.222682, acc.: 61.72%] [G loss: 0.441610]\n",
      "epoch:13 step:12763 [D loss: 0.204125, acc.: 67.97%] [G loss: 0.475051]\n",
      "epoch:13 step:12764 [D loss: 0.238811, acc.: 57.81%] [G loss: 0.451915]\n",
      "epoch:13 step:12765 [D loss: 0.222355, acc.: 64.84%] [G loss: 0.475900]\n",
      "epoch:13 step:12766 [D loss: 0.222024, acc.: 67.97%] [G loss: 0.438721]\n",
      "epoch:13 step:12767 [D loss: 0.256830, acc.: 53.12%] [G loss: 0.417279]\n",
      "epoch:13 step:12768 [D loss: 0.257989, acc.: 52.34%] [G loss: 0.407497]\n",
      "epoch:13 step:12769 [D loss: 0.214628, acc.: 62.50%] [G loss: 0.440130]\n",
      "epoch:13 step:12770 [D loss: 0.196596, acc.: 68.75%] [G loss: 0.465215]\n",
      "epoch:13 step:12771 [D loss: 0.274633, acc.: 54.69%] [G loss: 0.436842]\n",
      "epoch:13 step:12772 [D loss: 0.216693, acc.: 67.19%] [G loss: 0.458136]\n",
      "epoch:13 step:12773 [D loss: 0.191702, acc.: 74.22%] [G loss: 0.495439]\n",
      "epoch:13 step:12774 [D loss: 0.196431, acc.: 71.88%] [G loss: 0.465504]\n",
      "epoch:13 step:12775 [D loss: 0.220170, acc.: 66.41%] [G loss: 0.402752]\n",
      "epoch:13 step:12776 [D loss: 0.216572, acc.: 67.19%] [G loss: 0.438922]\n",
      "epoch:13 step:12777 [D loss: 0.230174, acc.: 60.94%] [G loss: 0.448449]\n",
      "epoch:13 step:12778 [D loss: 0.228043, acc.: 63.28%] [G loss: 0.400463]\n",
      "epoch:13 step:12779 [D loss: 0.222556, acc.: 64.84%] [G loss: 0.456675]\n",
      "epoch:13 step:12780 [D loss: 0.253136, acc.: 53.12%] [G loss: 0.415915]\n",
      "epoch:13 step:12781 [D loss: 0.247708, acc.: 54.69%] [G loss: 0.425608]\n",
      "epoch:13 step:12782 [D loss: 0.237684, acc.: 60.16%] [G loss: 0.441664]\n",
      "epoch:13 step:12783 [D loss: 0.237531, acc.: 60.16%] [G loss: 0.429011]\n",
      "epoch:13 step:12784 [D loss: 0.214026, acc.: 66.41%] [G loss: 0.446592]\n",
      "epoch:13 step:12785 [D loss: 0.223090, acc.: 64.06%] [G loss: 0.442971]\n",
      "epoch:13 step:12786 [D loss: 0.205674, acc.: 67.97%] [G loss: 0.473754]\n",
      "epoch:13 step:12787 [D loss: 0.232069, acc.: 60.16%] [G loss: 0.406176]\n",
      "epoch:13 step:12788 [D loss: 0.215480, acc.: 62.50%] [G loss: 0.436432]\n",
      "epoch:13 step:12789 [D loss: 0.211629, acc.: 67.97%] [G loss: 0.417345]\n",
      "epoch:13 step:12790 [D loss: 0.228148, acc.: 62.50%] [G loss: 0.450903]\n",
      "epoch:13 step:12791 [D loss: 0.228727, acc.: 59.38%] [G loss: 0.457382]\n",
      "epoch:13 step:12792 [D loss: 0.213690, acc.: 64.06%] [G loss: 0.435707]\n",
      "epoch:13 step:12793 [D loss: 0.230087, acc.: 56.25%] [G loss: 0.404275]\n",
      "epoch:13 step:12794 [D loss: 0.209777, acc.: 65.62%] [G loss: 0.431027]\n",
      "epoch:13 step:12795 [D loss: 0.229701, acc.: 60.94%] [G loss: 0.443842]\n",
      "epoch:13 step:12796 [D loss: 0.257502, acc.: 51.56%] [G loss: 0.414219]\n",
      "epoch:13 step:12797 [D loss: 0.238124, acc.: 64.06%] [G loss: 0.421050]\n",
      "epoch:13 step:12798 [D loss: 0.234917, acc.: 55.47%] [G loss: 0.447700]\n",
      "epoch:13 step:12799 [D loss: 0.244687, acc.: 62.50%] [G loss: 0.466294]\n",
      "epoch:13 step:12800 [D loss: 0.238530, acc.: 57.81%] [G loss: 0.403879]\n",
      "##############\n",
      "[2.52255532 1.61028281 5.92728303 4.8370176  3.69724439 5.56649507\n",
      " 4.29429647 4.71285884 4.31433232 3.85967889]\n",
      "##########\n",
      "epoch:13 step:12801 [D loss: 0.224155, acc.: 63.28%] [G loss: 0.421689]\n",
      "epoch:13 step:12802 [D loss: 0.250910, acc.: 50.00%] [G loss: 0.418481]\n",
      "epoch:13 step:12803 [D loss: 0.222820, acc.: 62.50%] [G loss: 0.460441]\n",
      "epoch:13 step:12804 [D loss: 0.200664, acc.: 68.75%] [G loss: 0.473008]\n",
      "epoch:13 step:12805 [D loss: 0.181537, acc.: 75.00%] [G loss: 0.491254]\n",
      "epoch:13 step:12806 [D loss: 0.258248, acc.: 56.25%] [G loss: 0.425147]\n",
      "epoch:13 step:12807 [D loss: 0.225654, acc.: 66.41%] [G loss: 0.459296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12808 [D loss: 0.223258, acc.: 66.41%] [G loss: 0.460455]\n",
      "epoch:13 step:12809 [D loss: 0.237923, acc.: 64.06%] [G loss: 0.458214]\n",
      "epoch:13 step:12810 [D loss: 0.182758, acc.: 76.56%] [G loss: 0.454734]\n",
      "epoch:13 step:12811 [D loss: 0.245741, acc.: 54.69%] [G loss: 0.401289]\n",
      "epoch:13 step:12812 [D loss: 0.190025, acc.: 73.44%] [G loss: 0.468318]\n",
      "epoch:13 step:12813 [D loss: 0.206634, acc.: 67.19%] [G loss: 0.431502]\n",
      "epoch:13 step:12814 [D loss: 0.225634, acc.: 66.41%] [G loss: 0.466397]\n",
      "epoch:13 step:12815 [D loss: 0.188928, acc.: 67.19%] [G loss: 0.496394]\n",
      "epoch:13 step:12816 [D loss: 0.212319, acc.: 66.41%] [G loss: 0.516198]\n",
      "epoch:13 step:12817 [D loss: 0.234316, acc.: 56.25%] [G loss: 0.444636]\n",
      "epoch:13 step:12818 [D loss: 0.214560, acc.: 67.19%] [G loss: 0.415681]\n",
      "epoch:13 step:12819 [D loss: 0.225735, acc.: 57.81%] [G loss: 0.417057]\n",
      "epoch:13 step:12820 [D loss: 0.234681, acc.: 62.50%] [G loss: 0.436724]\n",
      "epoch:13 step:12821 [D loss: 0.239594, acc.: 54.69%] [G loss: 0.445147]\n",
      "epoch:13 step:12822 [D loss: 0.218313, acc.: 66.41%] [G loss: 0.468695]\n",
      "epoch:13 step:12823 [D loss: 0.196709, acc.: 69.53%] [G loss: 0.485439]\n",
      "epoch:13 step:12824 [D loss: 0.218363, acc.: 63.28%] [G loss: 0.490173]\n",
      "epoch:13 step:12825 [D loss: 0.218977, acc.: 67.19%] [G loss: 0.410436]\n",
      "epoch:13 step:12826 [D loss: 0.224632, acc.: 65.62%] [G loss: 0.466530]\n",
      "epoch:13 step:12827 [D loss: 0.195395, acc.: 72.66%] [G loss: 0.503831]\n",
      "epoch:13 step:12828 [D loss: 0.204752, acc.: 66.41%] [G loss: 0.451977]\n",
      "epoch:13 step:12829 [D loss: 0.170054, acc.: 76.56%] [G loss: 0.535380]\n",
      "epoch:13 step:12830 [D loss: 0.211784, acc.: 67.97%] [G loss: 0.520004]\n",
      "epoch:13 step:12831 [D loss: 0.210454, acc.: 65.62%] [G loss: 0.484731]\n",
      "epoch:13 step:12832 [D loss: 0.217306, acc.: 67.19%] [G loss: 0.472487]\n",
      "epoch:13 step:12833 [D loss: 0.235028, acc.: 62.50%] [G loss: 0.457296]\n",
      "epoch:13 step:12834 [D loss: 0.231138, acc.: 59.38%] [G loss: 0.442079]\n",
      "epoch:13 step:12835 [D loss: 0.251540, acc.: 50.00%] [G loss: 0.484805]\n",
      "epoch:13 step:12836 [D loss: 0.245108, acc.: 60.16%] [G loss: 0.446320]\n",
      "epoch:13 step:12837 [D loss: 0.223633, acc.: 66.41%] [G loss: 0.454457]\n",
      "epoch:13 step:12838 [D loss: 0.227504, acc.: 62.50%] [G loss: 0.412697]\n",
      "epoch:13 step:12839 [D loss: 0.236330, acc.: 60.94%] [G loss: 0.446792]\n",
      "epoch:13 step:12840 [D loss: 0.196866, acc.: 71.09%] [G loss: 0.475592]\n",
      "epoch:13 step:12841 [D loss: 0.194899, acc.: 69.53%] [G loss: 0.483352]\n",
      "epoch:13 step:12842 [D loss: 0.217718, acc.: 64.84%] [G loss: 0.475813]\n",
      "epoch:13 step:12843 [D loss: 0.239379, acc.: 59.38%] [G loss: 0.441633]\n",
      "epoch:13 step:12844 [D loss: 0.230546, acc.: 64.06%] [G loss: 0.447621]\n",
      "epoch:13 step:12845 [D loss: 0.227489, acc.: 62.50%] [G loss: 0.448897]\n",
      "epoch:13 step:12846 [D loss: 0.219360, acc.: 64.84%] [G loss: 0.436396]\n",
      "epoch:13 step:12847 [D loss: 0.201904, acc.: 74.22%] [G loss: 0.466225]\n",
      "epoch:13 step:12848 [D loss: 0.283834, acc.: 56.25%] [G loss: 0.412367]\n",
      "epoch:13 step:12849 [D loss: 0.245734, acc.: 57.03%] [G loss: 0.458738]\n",
      "epoch:13 step:12850 [D loss: 0.201422, acc.: 71.09%] [G loss: 0.470738]\n",
      "epoch:13 step:12851 [D loss: 0.247921, acc.: 54.69%] [G loss: 0.431734]\n",
      "epoch:13 step:12852 [D loss: 0.216269, acc.: 65.62%] [G loss: 0.407504]\n",
      "epoch:13 step:12853 [D loss: 0.246043, acc.: 54.69%] [G loss: 0.415109]\n",
      "epoch:13 step:12854 [D loss: 0.224933, acc.: 67.19%] [G loss: 0.456424]\n",
      "epoch:13 step:12855 [D loss: 0.182249, acc.: 77.34%] [G loss: 0.506876]\n",
      "epoch:13 step:12856 [D loss: 0.271789, acc.: 50.78%] [G loss: 0.435759]\n",
      "epoch:13 step:12857 [D loss: 0.248567, acc.: 60.16%] [G loss: 0.442581]\n",
      "epoch:13 step:12858 [D loss: 0.226934, acc.: 60.94%] [G loss: 0.419878]\n",
      "epoch:13 step:12859 [D loss: 0.211638, acc.: 64.06%] [G loss: 0.466202]\n",
      "epoch:13 step:12860 [D loss: 0.233926, acc.: 60.94%] [G loss: 0.421912]\n",
      "epoch:13 step:12861 [D loss: 0.218176, acc.: 65.62%] [G loss: 0.468094]\n",
      "epoch:13 step:12862 [D loss: 0.190096, acc.: 71.88%] [G loss: 0.467385]\n",
      "epoch:13 step:12863 [D loss: 0.236265, acc.: 59.38%] [G loss: 0.473360]\n",
      "epoch:13 step:12864 [D loss: 0.231632, acc.: 59.38%] [G loss: 0.474662]\n",
      "epoch:13 step:12865 [D loss: 0.249632, acc.: 58.59%] [G loss: 0.415227]\n",
      "epoch:13 step:12866 [D loss: 0.226146, acc.: 59.38%] [G loss: 0.454809]\n",
      "epoch:13 step:12867 [D loss: 0.208166, acc.: 64.84%] [G loss: 0.440572]\n",
      "epoch:13 step:12868 [D loss: 0.221896, acc.: 57.81%] [G loss: 0.452026]\n",
      "epoch:13 step:12869 [D loss: 0.217983, acc.: 68.75%] [G loss: 0.450529]\n",
      "epoch:13 step:12870 [D loss: 0.199700, acc.: 69.53%] [G loss: 0.456912]\n",
      "epoch:13 step:12871 [D loss: 0.179722, acc.: 78.91%] [G loss: 0.480954]\n",
      "epoch:13 step:12872 [D loss: 0.213583, acc.: 69.53%] [G loss: 0.500869]\n",
      "epoch:13 step:12873 [D loss: 0.204961, acc.: 67.97%] [G loss: 0.496853]\n",
      "epoch:13 step:12874 [D loss: 0.203989, acc.: 71.09%] [G loss: 0.497596]\n",
      "epoch:13 step:12875 [D loss: 0.181601, acc.: 74.22%] [G loss: 0.503353]\n",
      "epoch:13 step:12876 [D loss: 0.224823, acc.: 64.84%] [G loss: 0.432074]\n",
      "epoch:13 step:12877 [D loss: 0.234811, acc.: 59.38%] [G loss: 0.423565]\n",
      "epoch:13 step:12878 [D loss: 0.219467, acc.: 64.84%] [G loss: 0.416071]\n",
      "epoch:13 step:12879 [D loss: 0.213409, acc.: 65.62%] [G loss: 0.446542]\n",
      "epoch:13 step:12880 [D loss: 0.218227, acc.: 66.41%] [G loss: 0.452204]\n",
      "epoch:13 step:12881 [D loss: 0.196646, acc.: 67.97%] [G loss: 0.512553]\n",
      "epoch:13 step:12882 [D loss: 0.206275, acc.: 70.31%] [G loss: 0.510844]\n",
      "epoch:13 step:12883 [D loss: 0.227318, acc.: 59.38%] [G loss: 0.471105]\n",
      "epoch:13 step:12884 [D loss: 0.238912, acc.: 58.59%] [G loss: 0.433422]\n",
      "epoch:13 step:12885 [D loss: 0.259743, acc.: 53.12%] [G loss: 0.433978]\n",
      "epoch:13 step:12886 [D loss: 0.228311, acc.: 65.62%] [G loss: 0.423001]\n",
      "epoch:13 step:12887 [D loss: 0.207723, acc.: 67.19%] [G loss: 0.437709]\n",
      "epoch:13 step:12888 [D loss: 0.224902, acc.: 61.72%] [G loss: 0.427347]\n",
      "epoch:13 step:12889 [D loss: 0.173352, acc.: 75.78%] [G loss: 0.451033]\n",
      "epoch:13 step:12890 [D loss: 0.209540, acc.: 64.06%] [G loss: 0.501802]\n",
      "epoch:13 step:12891 [D loss: 0.272918, acc.: 50.00%] [G loss: 0.437606]\n",
      "epoch:13 step:12892 [D loss: 0.233400, acc.: 56.25%] [G loss: 0.456766]\n",
      "epoch:13 step:12893 [D loss: 0.206249, acc.: 64.84%] [G loss: 0.489174]\n",
      "epoch:13 step:12894 [D loss: 0.253305, acc.: 58.59%] [G loss: 0.443126]\n",
      "epoch:13 step:12895 [D loss: 0.260839, acc.: 55.47%] [G loss: 0.433524]\n",
      "epoch:13 step:12896 [D loss: 0.212149, acc.: 67.19%] [G loss: 0.460436]\n",
      "epoch:13 step:12897 [D loss: 0.272309, acc.: 48.44%] [G loss: 0.391779]\n",
      "epoch:13 step:12898 [D loss: 0.216169, acc.: 67.97%] [G loss: 0.409096]\n",
      "epoch:13 step:12899 [D loss: 0.225496, acc.: 64.84%] [G loss: 0.461886]\n",
      "epoch:13 step:12900 [D loss: 0.195472, acc.: 71.88%] [G loss: 0.499897]\n",
      "epoch:13 step:12901 [D loss: 0.232092, acc.: 60.94%] [G loss: 0.478979]\n",
      "epoch:13 step:12902 [D loss: 0.241763, acc.: 62.50%] [G loss: 0.417863]\n",
      "epoch:13 step:12903 [D loss: 0.242639, acc.: 56.25%] [G loss: 0.407582]\n",
      "epoch:13 step:12904 [D loss: 0.235016, acc.: 60.94%] [G loss: 0.445234]\n",
      "epoch:13 step:12905 [D loss: 0.201708, acc.: 69.53%] [G loss: 0.485602]\n",
      "epoch:13 step:12906 [D loss: 0.204923, acc.: 65.62%] [G loss: 0.484912]\n",
      "epoch:13 step:12907 [D loss: 0.231745, acc.: 61.72%] [G loss: 0.445506]\n",
      "epoch:13 step:12908 [D loss: 0.249117, acc.: 60.94%] [G loss: 0.419400]\n",
      "epoch:13 step:12909 [D loss: 0.212974, acc.: 60.94%] [G loss: 0.403370]\n",
      "epoch:13 step:12910 [D loss: 0.236325, acc.: 60.94%] [G loss: 0.389188]\n",
      "epoch:13 step:12911 [D loss: 0.221382, acc.: 58.59%] [G loss: 0.499457]\n",
      "epoch:13 step:12912 [D loss: 0.216559, acc.: 62.50%] [G loss: 0.487680]\n",
      "epoch:13 step:12913 [D loss: 0.233930, acc.: 62.50%] [G loss: 0.429509]\n",
      "epoch:13 step:12914 [D loss: 0.213983, acc.: 62.50%] [G loss: 0.447603]\n",
      "epoch:13 step:12915 [D loss: 0.261047, acc.: 53.91%] [G loss: 0.431331]\n",
      "epoch:13 step:12916 [D loss: 0.238820, acc.: 56.25%] [G loss: 0.396357]\n",
      "epoch:13 step:12917 [D loss: 0.212334, acc.: 67.97%] [G loss: 0.431909]\n",
      "epoch:13 step:12918 [D loss: 0.217013, acc.: 64.84%] [G loss: 0.444965]\n",
      "epoch:13 step:12919 [D loss: 0.242525, acc.: 56.25%] [G loss: 0.391608]\n",
      "epoch:13 step:12920 [D loss: 0.252689, acc.: 53.91%] [G loss: 0.393506]\n",
      "epoch:13 step:12921 [D loss: 0.238367, acc.: 57.81%] [G loss: 0.436703]\n",
      "epoch:13 step:12922 [D loss: 0.254504, acc.: 53.12%] [G loss: 0.425411]\n",
      "epoch:13 step:12923 [D loss: 0.226905, acc.: 60.94%] [G loss: 0.453721]\n",
      "epoch:13 step:12924 [D loss: 0.202281, acc.: 62.50%] [G loss: 0.453874]\n",
      "epoch:13 step:12925 [D loss: 0.241805, acc.: 57.03%] [G loss: 0.436469]\n",
      "epoch:13 step:12926 [D loss: 0.221410, acc.: 69.53%] [G loss: 0.404107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12927 [D loss: 0.232573, acc.: 60.16%] [G loss: 0.449696]\n",
      "epoch:13 step:12928 [D loss: 0.207478, acc.: 65.62%] [G loss: 0.449720]\n",
      "epoch:13 step:12929 [D loss: 0.237911, acc.: 55.47%] [G loss: 0.444872]\n",
      "epoch:13 step:12930 [D loss: 0.240196, acc.: 61.72%] [G loss: 0.404690]\n",
      "epoch:13 step:12931 [D loss: 0.230363, acc.: 63.28%] [G loss: 0.406804]\n",
      "epoch:13 step:12932 [D loss: 0.232554, acc.: 58.59%] [G loss: 0.431187]\n",
      "epoch:13 step:12933 [D loss: 0.247805, acc.: 50.78%] [G loss: 0.435435]\n",
      "epoch:13 step:12934 [D loss: 0.229074, acc.: 63.28%] [G loss: 0.425836]\n",
      "epoch:13 step:12935 [D loss: 0.191218, acc.: 69.53%] [G loss: 0.415177]\n",
      "epoch:13 step:12936 [D loss: 0.224388, acc.: 67.97%] [G loss: 0.452996]\n",
      "epoch:13 step:12937 [D loss: 0.232416, acc.: 60.94%] [G loss: 0.440186]\n",
      "epoch:13 step:12938 [D loss: 0.234626, acc.: 54.69%] [G loss: 0.433046]\n",
      "epoch:13 step:12939 [D loss: 0.240522, acc.: 62.50%] [G loss: 0.434419]\n",
      "epoch:13 step:12940 [D loss: 0.264107, acc.: 53.12%] [G loss: 0.422531]\n",
      "epoch:13 step:12941 [D loss: 0.227656, acc.: 64.06%] [G loss: 0.419665]\n",
      "epoch:13 step:12942 [D loss: 0.242595, acc.: 57.81%] [G loss: 0.436881]\n",
      "epoch:13 step:12943 [D loss: 0.248858, acc.: 53.12%] [G loss: 0.386928]\n",
      "epoch:13 step:12944 [D loss: 0.223384, acc.: 58.59%] [G loss: 0.453704]\n",
      "epoch:13 step:12945 [D loss: 0.232922, acc.: 60.94%] [G loss: 0.399028]\n",
      "epoch:13 step:12946 [D loss: 0.249811, acc.: 57.81%] [G loss: 0.405834]\n",
      "epoch:13 step:12947 [D loss: 0.215311, acc.: 65.62%] [G loss: 0.410344]\n",
      "epoch:13 step:12948 [D loss: 0.208418, acc.: 71.09%] [G loss: 0.436270]\n",
      "epoch:13 step:12949 [D loss: 0.231844, acc.: 64.06%] [G loss: 0.496343]\n",
      "epoch:13 step:12950 [D loss: 0.203812, acc.: 72.66%] [G loss: 0.454129]\n",
      "epoch:13 step:12951 [D loss: 0.228568, acc.: 60.94%] [G loss: 0.434226]\n",
      "epoch:13 step:12952 [D loss: 0.228099, acc.: 57.03%] [G loss: 0.402862]\n",
      "epoch:13 step:12953 [D loss: 0.217403, acc.: 65.62%] [G loss: 0.444908]\n",
      "epoch:13 step:12954 [D loss: 0.227436, acc.: 64.06%] [G loss: 0.422520]\n",
      "epoch:13 step:12955 [D loss: 0.213731, acc.: 67.97%] [G loss: 0.442298]\n",
      "epoch:13 step:12956 [D loss: 0.220331, acc.: 60.94%] [G loss: 0.510338]\n",
      "epoch:13 step:12957 [D loss: 0.255130, acc.: 53.12%] [G loss: 0.438845]\n",
      "epoch:13 step:12958 [D loss: 0.227498, acc.: 59.38%] [G loss: 0.417011]\n",
      "epoch:13 step:12959 [D loss: 0.197390, acc.: 64.84%] [G loss: 0.476898]\n",
      "epoch:13 step:12960 [D loss: 0.205450, acc.: 72.66%] [G loss: 0.425633]\n",
      "epoch:13 step:12961 [D loss: 0.217164, acc.: 64.06%] [G loss: 0.403115]\n",
      "epoch:13 step:12962 [D loss: 0.197114, acc.: 67.19%] [G loss: 0.467900]\n",
      "epoch:13 step:12963 [D loss: 0.202241, acc.: 65.62%] [G loss: 0.481654]\n",
      "epoch:13 step:12964 [D loss: 0.274295, acc.: 52.34%] [G loss: 0.453984]\n",
      "epoch:13 step:12965 [D loss: 0.233977, acc.: 60.94%] [G loss: 0.465863]\n",
      "epoch:13 step:12966 [D loss: 0.207490, acc.: 66.41%] [G loss: 0.403105]\n",
      "epoch:13 step:12967 [D loss: 0.219293, acc.: 64.06%] [G loss: 0.450745]\n",
      "epoch:13 step:12968 [D loss: 0.230980, acc.: 57.81%] [G loss: 0.464046]\n",
      "epoch:13 step:12969 [D loss: 0.260397, acc.: 57.03%] [G loss: 0.416057]\n",
      "epoch:13 step:12970 [D loss: 0.249650, acc.: 51.56%] [G loss: 0.387185]\n",
      "epoch:13 step:12971 [D loss: 0.213901, acc.: 69.53%] [G loss: 0.437361]\n",
      "epoch:13 step:12972 [D loss: 0.298491, acc.: 48.44%] [G loss: 0.407683]\n",
      "epoch:13 step:12973 [D loss: 0.213740, acc.: 68.75%] [G loss: 0.428987]\n",
      "epoch:13 step:12974 [D loss: 0.224309, acc.: 61.72%] [G loss: 0.443428]\n",
      "epoch:13 step:12975 [D loss: 0.253546, acc.: 48.44%] [G loss: 0.401144]\n",
      "epoch:13 step:12976 [D loss: 0.237778, acc.: 60.16%] [G loss: 0.402551]\n",
      "epoch:13 step:12977 [D loss: 0.221638, acc.: 61.72%] [G loss: 0.438082]\n",
      "epoch:13 step:12978 [D loss: 0.232207, acc.: 61.72%] [G loss: 0.437708]\n",
      "epoch:13 step:12979 [D loss: 0.222108, acc.: 59.38%] [G loss: 0.450115]\n",
      "epoch:13 step:12980 [D loss: 0.226077, acc.: 60.94%] [G loss: 0.423610]\n",
      "epoch:13 step:12981 [D loss: 0.251510, acc.: 53.91%] [G loss: 0.486239]\n",
      "epoch:13 step:12982 [D loss: 0.228169, acc.: 56.25%] [G loss: 0.476665]\n",
      "epoch:13 step:12983 [D loss: 0.205203, acc.: 68.75%] [G loss: 0.453613]\n",
      "epoch:13 step:12984 [D loss: 0.210913, acc.: 67.97%] [G loss: 0.492490]\n",
      "epoch:13 step:12985 [D loss: 0.250814, acc.: 56.25%] [G loss: 0.416026]\n",
      "epoch:13 step:12986 [D loss: 0.229798, acc.: 64.84%] [G loss: 0.420931]\n",
      "epoch:13 step:12987 [D loss: 0.239845, acc.: 56.25%] [G loss: 0.406312]\n",
      "epoch:13 step:12988 [D loss: 0.210580, acc.: 65.62%] [G loss: 0.438448]\n",
      "epoch:13 step:12989 [D loss: 0.229427, acc.: 62.50%] [G loss: 0.414364]\n",
      "epoch:13 step:12990 [D loss: 0.221354, acc.: 60.16%] [G loss: 0.378259]\n",
      "epoch:13 step:12991 [D loss: 0.231019, acc.: 59.38%] [G loss: 0.426001]\n",
      "epoch:13 step:12992 [D loss: 0.232047, acc.: 58.59%] [G loss: 0.447863]\n",
      "epoch:13 step:12993 [D loss: 0.235987, acc.: 57.03%] [G loss: 0.437227]\n",
      "epoch:13 step:12994 [D loss: 0.221754, acc.: 64.06%] [G loss: 0.399115]\n",
      "epoch:13 step:12995 [D loss: 0.220984, acc.: 60.16%] [G loss: 0.432049]\n",
      "epoch:13 step:12996 [D loss: 0.191139, acc.: 75.78%] [G loss: 0.488394]\n",
      "epoch:13 step:12997 [D loss: 0.233711, acc.: 60.16%] [G loss: 0.493766]\n",
      "epoch:13 step:12998 [D loss: 0.253897, acc.: 57.81%] [G loss: 0.451226]\n",
      "epoch:13 step:12999 [D loss: 0.238130, acc.: 59.38%] [G loss: 0.447197]\n",
      "epoch:13 step:13000 [D loss: 0.216105, acc.: 67.97%] [G loss: 0.431721]\n",
      "##############\n",
      "[2.40955946 1.89928093 5.92886503 4.80128331 3.78250308 5.8269893\n",
      " 4.28548683 4.63578432 4.51426493 3.93406637]\n",
      "##########\n",
      "epoch:13 step:13001 [D loss: 0.275603, acc.: 50.00%] [G loss: 0.394405]\n",
      "epoch:13 step:13002 [D loss: 0.222870, acc.: 67.19%] [G loss: 0.429293]\n",
      "epoch:13 step:13003 [D loss: 0.205804, acc.: 66.41%] [G loss: 0.439768]\n",
      "epoch:13 step:13004 [D loss: 0.197748, acc.: 66.41%] [G loss: 0.472362]\n",
      "epoch:13 step:13005 [D loss: 0.224181, acc.: 60.94%] [G loss: 0.454685]\n",
      "epoch:13 step:13006 [D loss: 0.215236, acc.: 62.50%] [G loss: 0.442630]\n",
      "epoch:13 step:13007 [D loss: 0.237883, acc.: 61.72%] [G loss: 0.433123]\n",
      "epoch:13 step:13008 [D loss: 0.256435, acc.: 57.03%] [G loss: 0.429607]\n",
      "epoch:13 step:13009 [D loss: 0.260897, acc.: 52.34%] [G loss: 0.422311]\n",
      "epoch:13 step:13010 [D loss: 0.257498, acc.: 57.81%] [G loss: 0.417470]\n",
      "epoch:13 step:13011 [D loss: 0.220403, acc.: 66.41%] [G loss: 0.418232]\n",
      "epoch:13 step:13012 [D loss: 0.239190, acc.: 59.38%] [G loss: 0.431173]\n",
      "epoch:13 step:13013 [D loss: 0.216871, acc.: 60.94%] [G loss: 0.422571]\n",
      "epoch:13 step:13014 [D loss: 0.216474, acc.: 60.94%] [G loss: 0.480793]\n",
      "epoch:13 step:13015 [D loss: 0.235377, acc.: 57.03%] [G loss: 0.420135]\n",
      "epoch:13 step:13016 [D loss: 0.225235, acc.: 64.84%] [G loss: 0.431947]\n",
      "epoch:13 step:13017 [D loss: 0.226269, acc.: 64.06%] [G loss: 0.433727]\n",
      "epoch:13 step:13018 [D loss: 0.201910, acc.: 71.88%] [G loss: 0.466286]\n",
      "epoch:13 step:13019 [D loss: 0.210514, acc.: 67.97%] [G loss: 0.449931]\n",
      "epoch:13 step:13020 [D loss: 0.244443, acc.: 58.59%] [G loss: 0.397772]\n",
      "epoch:13 step:13021 [D loss: 0.252407, acc.: 52.34%] [G loss: 0.426133]\n",
      "epoch:13 step:13022 [D loss: 0.233927, acc.: 61.72%] [G loss: 0.434874]\n",
      "epoch:13 step:13023 [D loss: 0.181768, acc.: 72.66%] [G loss: 0.464182]\n",
      "epoch:13 step:13024 [D loss: 0.229136, acc.: 63.28%] [G loss: 0.472068]\n",
      "epoch:13 step:13025 [D loss: 0.229082, acc.: 64.84%] [G loss: 0.442733]\n",
      "epoch:13 step:13026 [D loss: 0.200819, acc.: 67.97%] [G loss: 0.469521]\n",
      "epoch:13 step:13027 [D loss: 0.256479, acc.: 46.88%] [G loss: 0.409219]\n",
      "epoch:13 step:13028 [D loss: 0.246769, acc.: 57.81%] [G loss: 0.423588]\n",
      "epoch:13 step:13029 [D loss: 0.220321, acc.: 66.41%] [G loss: 0.428282]\n",
      "epoch:13 step:13030 [D loss: 0.220688, acc.: 60.94%] [G loss: 0.403248]\n",
      "epoch:13 step:13031 [D loss: 0.265257, acc.: 49.22%] [G loss: 0.432997]\n",
      "epoch:13 step:13032 [D loss: 0.237650, acc.: 56.25%] [G loss: 0.450465]\n",
      "epoch:13 step:13033 [D loss: 0.218674, acc.: 64.84%] [G loss: 0.417057]\n",
      "epoch:13 step:13034 [D loss: 0.198055, acc.: 72.66%] [G loss: 0.459210]\n",
      "epoch:13 step:13035 [D loss: 0.221218, acc.: 69.53%] [G loss: 0.446060]\n",
      "epoch:13 step:13036 [D loss: 0.227304, acc.: 62.50%] [G loss: 0.425748]\n",
      "epoch:13 step:13037 [D loss: 0.258519, acc.: 51.56%] [G loss: 0.429598]\n",
      "epoch:13 step:13038 [D loss: 0.208718, acc.: 70.31%] [G loss: 0.463210]\n",
      "epoch:13 step:13039 [D loss: 0.285103, acc.: 48.44%] [G loss: 0.379612]\n",
      "epoch:13 step:13040 [D loss: 0.235453, acc.: 61.72%] [G loss: 0.465174]\n",
      "epoch:13 step:13041 [D loss: 0.190699, acc.: 71.09%] [G loss: 0.466501]\n",
      "epoch:13 step:13042 [D loss: 0.251860, acc.: 53.91%] [G loss: 0.430661]\n",
      "epoch:13 step:13043 [D loss: 0.239649, acc.: 57.81%] [G loss: 0.407936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13044 [D loss: 0.236587, acc.: 60.16%] [G loss: 0.401257]\n",
      "epoch:13 step:13045 [D loss: 0.225455, acc.: 60.94%] [G loss: 0.453323]\n",
      "epoch:13 step:13046 [D loss: 0.242421, acc.: 60.94%] [G loss: 0.411662]\n",
      "epoch:13 step:13047 [D loss: 0.236979, acc.: 64.06%] [G loss: 0.376372]\n",
      "epoch:13 step:13048 [D loss: 0.234592, acc.: 63.28%] [G loss: 0.375498]\n",
      "epoch:13 step:13049 [D loss: 0.208574, acc.: 64.84%] [G loss: 0.427276]\n",
      "epoch:13 step:13050 [D loss: 0.241893, acc.: 54.69%] [G loss: 0.453043]\n",
      "epoch:13 step:13051 [D loss: 0.202640, acc.: 69.53%] [G loss: 0.445911]\n",
      "epoch:13 step:13052 [D loss: 0.202342, acc.: 67.97%] [G loss: 0.470090]\n",
      "epoch:13 step:13053 [D loss: 0.237278, acc.: 59.38%] [G loss: 0.470868]\n",
      "epoch:13 step:13054 [D loss: 0.216400, acc.: 65.62%] [G loss: 0.430303]\n",
      "epoch:13 step:13055 [D loss: 0.250979, acc.: 57.81%] [G loss: 0.419059]\n",
      "epoch:13 step:13056 [D loss: 0.179130, acc.: 76.56%] [G loss: 0.439199]\n",
      "epoch:13 step:13057 [D loss: 0.226634, acc.: 63.28%] [G loss: 0.427945]\n",
      "epoch:13 step:13058 [D loss: 0.220289, acc.: 61.72%] [G loss: 0.430503]\n",
      "epoch:13 step:13059 [D loss: 0.230437, acc.: 65.62%] [G loss: 0.404355]\n",
      "epoch:13 step:13060 [D loss: 0.222178, acc.: 63.28%] [G loss: 0.437623]\n",
      "epoch:13 step:13061 [D loss: 0.246464, acc.: 56.25%] [G loss: 0.419437]\n",
      "epoch:13 step:13062 [D loss: 0.226743, acc.: 61.72%] [G loss: 0.435927]\n",
      "epoch:13 step:13063 [D loss: 0.216524, acc.: 63.28%] [G loss: 0.447824]\n",
      "epoch:13 step:13064 [D loss: 0.255918, acc.: 61.72%] [G loss: 0.417839]\n",
      "epoch:13 step:13065 [D loss: 0.214796, acc.: 69.53%] [G loss: 0.510087]\n",
      "epoch:13 step:13066 [D loss: 0.221405, acc.: 65.62%] [G loss: 0.473426]\n",
      "epoch:13 step:13067 [D loss: 0.206255, acc.: 65.62%] [G loss: 0.485760]\n",
      "epoch:13 step:13068 [D loss: 0.212667, acc.: 65.62%] [G loss: 0.465529]\n",
      "epoch:13 step:13069 [D loss: 0.220618, acc.: 60.16%] [G loss: 0.428131]\n",
      "epoch:13 step:13070 [D loss: 0.199131, acc.: 67.19%] [G loss: 0.438152]\n",
      "epoch:13 step:13071 [D loss: 0.196361, acc.: 69.53%] [G loss: 0.465920]\n",
      "epoch:13 step:13072 [D loss: 0.280722, acc.: 49.22%] [G loss: 0.393860]\n",
      "epoch:13 step:13073 [D loss: 0.246429, acc.: 49.22%] [G loss: 0.474914]\n",
      "epoch:13 step:13074 [D loss: 0.203771, acc.: 70.31%] [G loss: 0.462682]\n",
      "epoch:13 step:13075 [D loss: 0.237693, acc.: 60.16%] [G loss: 0.513140]\n",
      "epoch:13 step:13076 [D loss: 0.203319, acc.: 72.66%] [G loss: 0.487905]\n",
      "epoch:13 step:13077 [D loss: 0.252400, acc.: 61.72%] [G loss: 0.440256]\n",
      "epoch:13 step:13078 [D loss: 0.206433, acc.: 71.88%] [G loss: 0.470785]\n",
      "epoch:13 step:13079 [D loss: 0.204787, acc.: 67.97%] [G loss: 0.465837]\n",
      "epoch:13 step:13080 [D loss: 0.169991, acc.: 72.66%] [G loss: 0.450188]\n",
      "epoch:13 step:13081 [D loss: 0.206830, acc.: 66.41%] [G loss: 0.440747]\n",
      "epoch:13 step:13082 [D loss: 0.208369, acc.: 64.06%] [G loss: 0.441382]\n",
      "epoch:13 step:13083 [D loss: 0.256617, acc.: 54.69%] [G loss: 0.424171]\n",
      "epoch:13 step:13084 [D loss: 0.247267, acc.: 60.16%] [G loss: 0.407733]\n",
      "epoch:13 step:13085 [D loss: 0.222895, acc.: 65.62%] [G loss: 0.429590]\n",
      "epoch:13 step:13086 [D loss: 0.218762, acc.: 59.38%] [G loss: 0.502182]\n",
      "epoch:13 step:13087 [D loss: 0.196894, acc.: 69.53%] [G loss: 0.477994]\n",
      "epoch:13 step:13088 [D loss: 0.239887, acc.: 57.03%] [G loss: 0.436495]\n",
      "epoch:13 step:13089 [D loss: 0.194337, acc.: 73.44%] [G loss: 0.482113]\n",
      "epoch:13 step:13090 [D loss: 0.202412, acc.: 67.19%] [G loss: 0.453044]\n",
      "epoch:13 step:13091 [D loss: 0.198234, acc.: 72.66%] [G loss: 0.460022]\n",
      "epoch:13 step:13092 [D loss: 0.202800, acc.: 71.09%] [G loss: 0.489210]\n",
      "epoch:13 step:13093 [D loss: 0.212305, acc.: 65.62%] [G loss: 0.464566]\n",
      "epoch:13 step:13094 [D loss: 0.206915, acc.: 66.41%] [G loss: 0.511929]\n",
      "epoch:13 step:13095 [D loss: 0.227515, acc.: 58.59%] [G loss: 0.475656]\n",
      "epoch:13 step:13096 [D loss: 0.284932, acc.: 59.38%] [G loss: 0.410367]\n",
      "epoch:13 step:13097 [D loss: 0.204113, acc.: 64.84%] [G loss: 0.472986]\n",
      "epoch:13 step:13098 [D loss: 0.209695, acc.: 67.19%] [G loss: 0.485853]\n",
      "epoch:13 step:13099 [D loss: 0.199458, acc.: 70.31%] [G loss: 0.502427]\n",
      "epoch:13 step:13100 [D loss: 0.213194, acc.: 69.53%] [G loss: 0.521050]\n",
      "epoch:13 step:13101 [D loss: 0.312017, acc.: 48.44%] [G loss: 0.467140]\n",
      "epoch:13 step:13102 [D loss: 0.223879, acc.: 60.16%] [G loss: 0.451945]\n",
      "epoch:13 step:13103 [D loss: 0.270131, acc.: 52.34%] [G loss: 0.392335]\n",
      "epoch:13 step:13104 [D loss: 0.171925, acc.: 78.91%] [G loss: 0.456073]\n",
      "epoch:13 step:13105 [D loss: 0.198172, acc.: 79.69%] [G loss: 0.453436]\n",
      "epoch:13 step:13106 [D loss: 0.201359, acc.: 68.75%] [G loss: 0.471619]\n",
      "epoch:13 step:13107 [D loss: 0.199685, acc.: 74.22%] [G loss: 0.508232]\n",
      "epoch:13 step:13108 [D loss: 0.214902, acc.: 60.16%] [G loss: 0.553289]\n",
      "epoch:13 step:13109 [D loss: 0.276332, acc.: 57.81%] [G loss: 0.542044]\n",
      "epoch:13 step:13110 [D loss: 0.226278, acc.: 63.28%] [G loss: 0.615531]\n",
      "epoch:13 step:13111 [D loss: 0.219880, acc.: 68.75%] [G loss: 0.502204]\n",
      "epoch:13 step:13112 [D loss: 0.250740, acc.: 60.16%] [G loss: 0.405463]\n",
      "epoch:13 step:13113 [D loss: 0.242683, acc.: 57.81%] [G loss: 0.397295]\n",
      "epoch:13 step:13114 [D loss: 0.252264, acc.: 59.38%] [G loss: 0.432673]\n",
      "epoch:13 step:13115 [D loss: 0.202494, acc.: 72.66%] [G loss: 0.496738]\n",
      "epoch:13 step:13116 [D loss: 0.211413, acc.: 69.53%] [G loss: 0.497388]\n",
      "epoch:13 step:13117 [D loss: 0.158531, acc.: 75.78%] [G loss: 0.556051]\n",
      "epoch:13 step:13118 [D loss: 0.205627, acc.: 72.66%] [G loss: 0.576923]\n",
      "epoch:14 step:13119 [D loss: 0.237913, acc.: 62.50%] [G loss: 0.464041]\n",
      "epoch:14 step:13120 [D loss: 0.266781, acc.: 60.94%] [G loss: 0.406290]\n",
      "epoch:14 step:13121 [D loss: 0.250477, acc.: 58.59%] [G loss: 0.415232]\n",
      "epoch:14 step:13122 [D loss: 0.237020, acc.: 59.38%] [G loss: 0.442626]\n",
      "epoch:14 step:13123 [D loss: 0.216625, acc.: 60.94%] [G loss: 0.460043]\n",
      "epoch:14 step:13124 [D loss: 0.239096, acc.: 59.38%] [G loss: 0.436719]\n",
      "epoch:14 step:13125 [D loss: 0.197713, acc.: 71.88%] [G loss: 0.469612]\n",
      "epoch:14 step:13126 [D loss: 0.221826, acc.: 60.94%] [G loss: 0.445077]\n",
      "epoch:14 step:13127 [D loss: 0.230412, acc.: 65.62%] [G loss: 0.436743]\n",
      "epoch:14 step:13128 [D loss: 0.205645, acc.: 67.19%] [G loss: 0.451719]\n",
      "epoch:14 step:13129 [D loss: 0.199963, acc.: 65.62%] [G loss: 0.466837]\n",
      "epoch:14 step:13130 [D loss: 0.220395, acc.: 62.50%] [G loss: 0.481083]\n",
      "epoch:14 step:13131 [D loss: 0.211690, acc.: 68.75%] [G loss: 0.446337]\n",
      "epoch:14 step:13132 [D loss: 0.202679, acc.: 67.97%] [G loss: 0.464761]\n",
      "epoch:14 step:13133 [D loss: 0.203201, acc.: 67.97%] [G loss: 0.510820]\n",
      "epoch:14 step:13134 [D loss: 0.209506, acc.: 64.06%] [G loss: 0.497416]\n",
      "epoch:14 step:13135 [D loss: 0.254128, acc.: 54.69%] [G loss: 0.468659]\n",
      "epoch:14 step:13136 [D loss: 0.221524, acc.: 67.97%] [G loss: 0.496269]\n",
      "epoch:14 step:13137 [D loss: 0.234419, acc.: 58.59%] [G loss: 0.488473]\n",
      "epoch:14 step:13138 [D loss: 0.269324, acc.: 50.78%] [G loss: 0.434425]\n",
      "epoch:14 step:13139 [D loss: 0.229428, acc.: 60.94%] [G loss: 0.437332]\n",
      "epoch:14 step:13140 [D loss: 0.198677, acc.: 71.88%] [G loss: 0.495750]\n",
      "epoch:14 step:13141 [D loss: 0.222362, acc.: 65.62%] [G loss: 0.463590]\n",
      "epoch:14 step:13142 [D loss: 0.194327, acc.: 66.41%] [G loss: 0.464392]\n",
      "epoch:14 step:13143 [D loss: 0.223187, acc.: 67.19%] [G loss: 0.428227]\n",
      "epoch:14 step:13144 [D loss: 0.209413, acc.: 63.28%] [G loss: 0.458378]\n",
      "epoch:14 step:13145 [D loss: 0.234172, acc.: 60.94%] [G loss: 0.438456]\n",
      "epoch:14 step:13146 [D loss: 0.205890, acc.: 68.75%] [G loss: 0.426081]\n",
      "epoch:14 step:13147 [D loss: 0.226429, acc.: 63.28%] [G loss: 0.458324]\n",
      "epoch:14 step:13148 [D loss: 0.213060, acc.: 63.28%] [G loss: 0.469029]\n",
      "epoch:14 step:13149 [D loss: 0.230781, acc.: 60.16%] [G loss: 0.466089]\n",
      "epoch:14 step:13150 [D loss: 0.224310, acc.: 67.19%] [G loss: 0.432753]\n",
      "epoch:14 step:13151 [D loss: 0.236699, acc.: 60.16%] [G loss: 0.450412]\n",
      "epoch:14 step:13152 [D loss: 0.243505, acc.: 51.56%] [G loss: 0.407619]\n",
      "epoch:14 step:13153 [D loss: 0.217862, acc.: 63.28%] [G loss: 0.440248]\n",
      "epoch:14 step:13154 [D loss: 0.241682, acc.: 57.81%] [G loss: 0.465762]\n",
      "epoch:14 step:13155 [D loss: 0.249865, acc.: 55.47%] [G loss: 0.406880]\n",
      "epoch:14 step:13156 [D loss: 0.224296, acc.: 61.72%] [G loss: 0.419240]\n",
      "epoch:14 step:13157 [D loss: 0.239987, acc.: 60.16%] [G loss: 0.415350]\n",
      "epoch:14 step:13158 [D loss: 0.189240, acc.: 69.53%] [G loss: 0.472461]\n",
      "epoch:14 step:13159 [D loss: 0.248921, acc.: 57.81%] [G loss: 0.421850]\n",
      "epoch:14 step:13160 [D loss: 0.203149, acc.: 64.84%] [G loss: 0.448527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13161 [D loss: 0.213030, acc.: 70.31%] [G loss: 0.458807]\n",
      "epoch:14 step:13162 [D loss: 0.248061, acc.: 53.91%] [G loss: 0.422295]\n",
      "epoch:14 step:13163 [D loss: 0.217624, acc.: 64.06%] [G loss: 0.427907]\n",
      "epoch:14 step:13164 [D loss: 0.242685, acc.: 60.16%] [G loss: 0.420398]\n",
      "epoch:14 step:13165 [D loss: 0.236612, acc.: 62.50%] [G loss: 0.399780]\n",
      "epoch:14 step:13166 [D loss: 0.195822, acc.: 70.31%] [G loss: 0.459292]\n",
      "epoch:14 step:13167 [D loss: 0.214239, acc.: 63.28%] [G loss: 0.452546]\n",
      "epoch:14 step:13168 [D loss: 0.195178, acc.: 74.22%] [G loss: 0.456297]\n",
      "epoch:14 step:13169 [D loss: 0.263857, acc.: 53.91%] [G loss: 0.420948]\n",
      "epoch:14 step:13170 [D loss: 0.241915, acc.: 64.06%] [G loss: 0.472902]\n",
      "epoch:14 step:13171 [D loss: 0.228396, acc.: 60.16%] [G loss: 0.472475]\n",
      "epoch:14 step:13172 [D loss: 0.225471, acc.: 64.84%] [G loss: 0.491325]\n",
      "epoch:14 step:13173 [D loss: 0.215316, acc.: 60.94%] [G loss: 0.495019]\n",
      "epoch:14 step:13174 [D loss: 0.233112, acc.: 60.94%] [G loss: 0.422925]\n",
      "epoch:14 step:13175 [D loss: 0.231634, acc.: 64.84%] [G loss: 0.413988]\n",
      "epoch:14 step:13176 [D loss: 0.227038, acc.: 61.72%] [G loss: 0.456568]\n",
      "epoch:14 step:13177 [D loss: 0.197249, acc.: 71.88%] [G loss: 0.465646]\n",
      "epoch:14 step:13178 [D loss: 0.243931, acc.: 60.94%] [G loss: 0.447278]\n",
      "epoch:14 step:13179 [D loss: 0.254020, acc.: 55.47%] [G loss: 0.421250]\n",
      "epoch:14 step:13180 [D loss: 0.201083, acc.: 71.88%] [G loss: 0.449889]\n",
      "epoch:14 step:13181 [D loss: 0.216075, acc.: 67.19%] [G loss: 0.443038]\n",
      "epoch:14 step:13182 [D loss: 0.202051, acc.: 66.41%] [G loss: 0.446250]\n",
      "epoch:14 step:13183 [D loss: 0.214399, acc.: 60.16%] [G loss: 0.449140]\n",
      "epoch:14 step:13184 [D loss: 0.232924, acc.: 60.16%] [G loss: 0.394605]\n",
      "epoch:14 step:13185 [D loss: 0.212295, acc.: 66.41%] [G loss: 0.413391]\n",
      "epoch:14 step:13186 [D loss: 0.213680, acc.: 65.62%] [G loss: 0.441821]\n",
      "epoch:14 step:13187 [D loss: 0.183825, acc.: 73.44%] [G loss: 0.473915]\n",
      "epoch:14 step:13188 [D loss: 0.204105, acc.: 71.88%] [G loss: 0.445579]\n",
      "epoch:14 step:13189 [D loss: 0.268515, acc.: 49.22%] [G loss: 0.405027]\n",
      "epoch:14 step:13190 [D loss: 0.233270, acc.: 57.81%] [G loss: 0.412898]\n",
      "epoch:14 step:13191 [D loss: 0.235387, acc.: 64.84%] [G loss: 0.431606]\n",
      "epoch:14 step:13192 [D loss: 0.184011, acc.: 71.88%] [G loss: 0.444699]\n",
      "epoch:14 step:13193 [D loss: 0.225655, acc.: 60.94%] [G loss: 0.407971]\n",
      "epoch:14 step:13194 [D loss: 0.208335, acc.: 62.50%] [G loss: 0.469924]\n",
      "epoch:14 step:13195 [D loss: 0.201022, acc.: 68.75%] [G loss: 0.484104]\n",
      "epoch:14 step:13196 [D loss: 0.273030, acc.: 54.69%] [G loss: 0.465506]\n",
      "epoch:14 step:13197 [D loss: 0.235423, acc.: 56.25%] [G loss: 0.443897]\n",
      "epoch:14 step:13198 [D loss: 0.240627, acc.: 62.50%] [G loss: 0.435688]\n",
      "epoch:14 step:13199 [D loss: 0.227493, acc.: 61.72%] [G loss: 0.436513]\n",
      "epoch:14 step:13200 [D loss: 0.238775, acc.: 57.81%] [G loss: 0.446643]\n",
      "##############\n",
      "[2.57743913 1.68094575 6.05819818 4.77277476 3.7435514  5.72942167\n",
      " 4.2971119  4.91366143 4.30272337 3.82416725]\n",
      "##########\n",
      "epoch:14 step:13201 [D loss: 0.191848, acc.: 69.53%] [G loss: 0.487530]\n",
      "epoch:14 step:13202 [D loss: 0.245588, acc.: 57.81%] [G loss: 0.480509]\n",
      "epoch:14 step:13203 [D loss: 0.236879, acc.: 62.50%] [G loss: 0.471340]\n",
      "epoch:14 step:13204 [D loss: 0.240483, acc.: 64.06%] [G loss: 0.424310]\n",
      "epoch:14 step:13205 [D loss: 0.212490, acc.: 67.19%] [G loss: 0.408660]\n",
      "epoch:14 step:13206 [D loss: 0.191722, acc.: 72.66%] [G loss: 0.471436]\n",
      "epoch:14 step:13207 [D loss: 0.220356, acc.: 63.28%] [G loss: 0.422683]\n",
      "epoch:14 step:13208 [D loss: 0.210871, acc.: 67.97%] [G loss: 0.479197]\n",
      "epoch:14 step:13209 [D loss: 0.218687, acc.: 60.94%] [G loss: 0.400120]\n",
      "epoch:14 step:13210 [D loss: 0.220957, acc.: 62.50%] [G loss: 0.442774]\n",
      "epoch:14 step:13211 [D loss: 0.216023, acc.: 65.62%] [G loss: 0.466362]\n",
      "epoch:14 step:13212 [D loss: 0.206329, acc.: 67.97%] [G loss: 0.476634]\n",
      "epoch:14 step:13213 [D loss: 0.214372, acc.: 67.97%] [G loss: 0.454770]\n",
      "epoch:14 step:13214 [D loss: 0.202585, acc.: 68.75%] [G loss: 0.445981]\n",
      "epoch:14 step:13215 [D loss: 0.188714, acc.: 71.09%] [G loss: 0.475559]\n",
      "epoch:14 step:13216 [D loss: 0.242835, acc.: 54.69%] [G loss: 0.483419]\n",
      "epoch:14 step:13217 [D loss: 0.241006, acc.: 58.59%] [G loss: 0.454775]\n",
      "epoch:14 step:13218 [D loss: 0.212415, acc.: 70.31%] [G loss: 0.466874]\n",
      "epoch:14 step:13219 [D loss: 0.218262, acc.: 63.28%] [G loss: 0.457678]\n",
      "epoch:14 step:13220 [D loss: 0.245320, acc.: 56.25%] [G loss: 0.441077]\n",
      "epoch:14 step:13221 [D loss: 0.217229, acc.: 62.50%] [G loss: 0.427222]\n",
      "epoch:14 step:13222 [D loss: 0.220329, acc.: 60.94%] [G loss: 0.417377]\n",
      "epoch:14 step:13223 [D loss: 0.235134, acc.: 63.28%] [G loss: 0.415644]\n",
      "epoch:14 step:13224 [D loss: 0.216367, acc.: 69.53%] [G loss: 0.472032]\n",
      "epoch:14 step:13225 [D loss: 0.195902, acc.: 71.09%] [G loss: 0.507985]\n",
      "epoch:14 step:13226 [D loss: 0.290592, acc.: 50.00%] [G loss: 0.478638]\n",
      "epoch:14 step:13227 [D loss: 0.262497, acc.: 51.56%] [G loss: 0.481520]\n",
      "epoch:14 step:13228 [D loss: 0.248453, acc.: 57.81%] [G loss: 0.444788]\n",
      "epoch:14 step:13229 [D loss: 0.211265, acc.: 60.94%] [G loss: 0.429491]\n",
      "epoch:14 step:13230 [D loss: 0.196178, acc.: 73.44%] [G loss: 0.435636]\n",
      "epoch:14 step:13231 [D loss: 0.199210, acc.: 69.53%] [G loss: 0.457092]\n",
      "epoch:14 step:13232 [D loss: 0.194610, acc.: 71.88%] [G loss: 0.477720]\n",
      "epoch:14 step:13233 [D loss: 0.231227, acc.: 57.81%] [G loss: 0.530833]\n",
      "epoch:14 step:13234 [D loss: 0.215998, acc.: 66.41%] [G loss: 0.447776]\n",
      "epoch:14 step:13235 [D loss: 0.216133, acc.: 66.41%] [G loss: 0.450499]\n",
      "epoch:14 step:13236 [D loss: 0.222812, acc.: 66.41%] [G loss: 0.441150]\n",
      "epoch:14 step:13237 [D loss: 0.180098, acc.: 75.00%] [G loss: 0.528606]\n",
      "epoch:14 step:13238 [D loss: 0.251349, acc.: 61.72%] [G loss: 0.451408]\n",
      "epoch:14 step:13239 [D loss: 0.233052, acc.: 59.38%] [G loss: 0.441315]\n",
      "epoch:14 step:13240 [D loss: 0.196551, acc.: 71.88%] [G loss: 0.460872]\n",
      "epoch:14 step:13241 [D loss: 0.206049, acc.: 67.97%] [G loss: 0.456915]\n",
      "epoch:14 step:13242 [D loss: 0.235036, acc.: 60.16%] [G loss: 0.483401]\n",
      "epoch:14 step:13243 [D loss: 0.233515, acc.: 61.72%] [G loss: 0.435744]\n",
      "epoch:14 step:13244 [D loss: 0.198665, acc.: 70.31%] [G loss: 0.426426]\n",
      "epoch:14 step:13245 [D loss: 0.213705, acc.: 64.06%] [G loss: 0.450156]\n",
      "epoch:14 step:13246 [D loss: 0.232590, acc.: 57.81%] [G loss: 0.420956]\n",
      "epoch:14 step:13247 [D loss: 0.240123, acc.: 60.94%] [G loss: 0.398405]\n",
      "epoch:14 step:13248 [D loss: 0.220405, acc.: 65.62%] [G loss: 0.439581]\n",
      "epoch:14 step:13249 [D loss: 0.195558, acc.: 75.78%] [G loss: 0.500657]\n",
      "epoch:14 step:13250 [D loss: 0.244562, acc.: 58.59%] [G loss: 0.432512]\n",
      "epoch:14 step:13251 [D loss: 0.250302, acc.: 56.25%] [G loss: 0.457955]\n",
      "epoch:14 step:13252 [D loss: 0.210476, acc.: 66.41%] [G loss: 0.493553]\n",
      "epoch:14 step:13253 [D loss: 0.231441, acc.: 64.84%] [G loss: 0.435037]\n",
      "epoch:14 step:13254 [D loss: 0.230831, acc.: 65.62%] [G loss: 0.454789]\n",
      "epoch:14 step:13255 [D loss: 0.252298, acc.: 57.81%] [G loss: 0.430152]\n",
      "epoch:14 step:13256 [D loss: 0.224803, acc.: 62.50%] [G loss: 0.395946]\n",
      "epoch:14 step:13257 [D loss: 0.228513, acc.: 60.94%] [G loss: 0.429976]\n",
      "epoch:14 step:13258 [D loss: 0.236217, acc.: 57.81%] [G loss: 0.426862]\n",
      "epoch:14 step:13259 [D loss: 0.225009, acc.: 60.16%] [G loss: 0.402996]\n",
      "epoch:14 step:13260 [D loss: 0.231955, acc.: 62.50%] [G loss: 0.431752]\n",
      "epoch:14 step:13261 [D loss: 0.240694, acc.: 55.47%] [G loss: 0.448422]\n",
      "epoch:14 step:13262 [D loss: 0.204838, acc.: 66.41%] [G loss: 0.466618]\n",
      "epoch:14 step:13263 [D loss: 0.238432, acc.: 58.59%] [G loss: 0.459915]\n",
      "epoch:14 step:13264 [D loss: 0.232235, acc.: 57.81%] [G loss: 0.458640]\n",
      "epoch:14 step:13265 [D loss: 0.250038, acc.: 54.69%] [G loss: 0.423009]\n",
      "epoch:14 step:13266 [D loss: 0.260098, acc.: 50.78%] [G loss: 0.439004]\n",
      "epoch:14 step:13267 [D loss: 0.214804, acc.: 64.06%] [G loss: 0.448956]\n",
      "epoch:14 step:13268 [D loss: 0.241789, acc.: 53.91%] [G loss: 0.417018]\n",
      "epoch:14 step:13269 [D loss: 0.210574, acc.: 68.75%] [G loss: 0.458106]\n",
      "epoch:14 step:13270 [D loss: 0.214761, acc.: 65.62%] [G loss: 0.447441]\n",
      "epoch:14 step:13271 [D loss: 0.228708, acc.: 61.72%] [G loss: 0.446879]\n",
      "epoch:14 step:13272 [D loss: 0.196717, acc.: 70.31%] [G loss: 0.415764]\n",
      "epoch:14 step:13273 [D loss: 0.212904, acc.: 68.75%] [G loss: 0.435166]\n",
      "epoch:14 step:13274 [D loss: 0.211188, acc.: 64.84%] [G loss: 0.429756]\n",
      "epoch:14 step:13275 [D loss: 0.208491, acc.: 67.19%] [G loss: 0.449703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13276 [D loss: 0.225231, acc.: 62.50%] [G loss: 0.416851]\n",
      "epoch:14 step:13277 [D loss: 0.226121, acc.: 67.19%] [G loss: 0.409352]\n",
      "epoch:14 step:13278 [D loss: 0.272820, acc.: 56.25%] [G loss: 0.410605]\n",
      "epoch:14 step:13279 [D loss: 0.248285, acc.: 60.16%] [G loss: 0.442229]\n",
      "epoch:14 step:13280 [D loss: 0.234641, acc.: 58.59%] [G loss: 0.426353]\n",
      "epoch:14 step:13281 [D loss: 0.244478, acc.: 53.91%] [G loss: 0.410677]\n",
      "epoch:14 step:13282 [D loss: 0.210935, acc.: 66.41%] [G loss: 0.455364]\n",
      "epoch:14 step:13283 [D loss: 0.239170, acc.: 61.72%] [G loss: 0.435451]\n",
      "epoch:14 step:13284 [D loss: 0.219349, acc.: 62.50%] [G loss: 0.459244]\n",
      "epoch:14 step:13285 [D loss: 0.211415, acc.: 63.28%] [G loss: 0.425206]\n",
      "epoch:14 step:13286 [D loss: 0.230015, acc.: 60.94%] [G loss: 0.429465]\n",
      "epoch:14 step:13287 [D loss: 0.234171, acc.: 58.59%] [G loss: 0.454039]\n",
      "epoch:14 step:13288 [D loss: 0.238463, acc.: 57.81%] [G loss: 0.403751]\n",
      "epoch:14 step:13289 [D loss: 0.218844, acc.: 67.19%] [G loss: 0.491221]\n",
      "epoch:14 step:13290 [D loss: 0.217160, acc.: 67.19%] [G loss: 0.468073]\n",
      "epoch:14 step:13291 [D loss: 0.241065, acc.: 57.81%] [G loss: 0.422535]\n",
      "epoch:14 step:13292 [D loss: 0.248467, acc.: 59.38%] [G loss: 0.434902]\n",
      "epoch:14 step:13293 [D loss: 0.228351, acc.: 60.16%] [G loss: 0.407710]\n",
      "epoch:14 step:13294 [D loss: 0.224352, acc.: 59.38%] [G loss: 0.443367]\n",
      "epoch:14 step:13295 [D loss: 0.234218, acc.: 58.59%] [G loss: 0.422968]\n",
      "epoch:14 step:13296 [D loss: 0.238981, acc.: 57.81%] [G loss: 0.402723]\n",
      "epoch:14 step:13297 [D loss: 0.240607, acc.: 59.38%] [G loss: 0.432970]\n",
      "epoch:14 step:13298 [D loss: 0.241457, acc.: 55.47%] [G loss: 0.428285]\n",
      "epoch:14 step:13299 [D loss: 0.233537, acc.: 59.38%] [G loss: 0.416280]\n",
      "epoch:14 step:13300 [D loss: 0.219569, acc.: 66.41%] [G loss: 0.426566]\n",
      "epoch:14 step:13301 [D loss: 0.223270, acc.: 65.62%] [G loss: 0.432726]\n",
      "epoch:14 step:13302 [D loss: 0.220695, acc.: 64.84%] [G loss: 0.451629]\n",
      "epoch:14 step:13303 [D loss: 0.238512, acc.: 60.16%] [G loss: 0.461063]\n",
      "epoch:14 step:13304 [D loss: 0.276932, acc.: 46.88%] [G loss: 0.400560]\n",
      "epoch:14 step:13305 [D loss: 0.234079, acc.: 59.38%] [G loss: 0.441937]\n",
      "epoch:14 step:13306 [D loss: 0.244925, acc.: 59.38%] [G loss: 0.417807]\n",
      "epoch:14 step:13307 [D loss: 0.228203, acc.: 63.28%] [G loss: 0.420208]\n",
      "epoch:14 step:13308 [D loss: 0.208282, acc.: 71.09%] [G loss: 0.398713]\n",
      "epoch:14 step:13309 [D loss: 0.208378, acc.: 67.19%] [G loss: 0.443246]\n",
      "epoch:14 step:13310 [D loss: 0.230800, acc.: 56.25%] [G loss: 0.428861]\n",
      "epoch:14 step:13311 [D loss: 0.203726, acc.: 71.88%] [G loss: 0.454376]\n",
      "epoch:14 step:13312 [D loss: 0.206681, acc.: 68.75%] [G loss: 0.438031]\n",
      "epoch:14 step:13313 [D loss: 0.218710, acc.: 64.84%] [G loss: 0.447485]\n",
      "epoch:14 step:13314 [D loss: 0.229816, acc.: 59.38%] [G loss: 0.429887]\n",
      "epoch:14 step:13315 [D loss: 0.221000, acc.: 61.72%] [G loss: 0.452991]\n",
      "epoch:14 step:13316 [D loss: 0.179574, acc.: 73.44%] [G loss: 0.482560]\n",
      "epoch:14 step:13317 [D loss: 0.213649, acc.: 64.84%] [G loss: 0.482062]\n",
      "epoch:14 step:13318 [D loss: 0.241046, acc.: 61.72%] [G loss: 0.455129]\n",
      "epoch:14 step:13319 [D loss: 0.231860, acc.: 61.72%] [G loss: 0.399034]\n",
      "epoch:14 step:13320 [D loss: 0.210159, acc.: 65.62%] [G loss: 0.435704]\n",
      "epoch:14 step:13321 [D loss: 0.258959, acc.: 53.91%] [G loss: 0.414497]\n",
      "epoch:14 step:13322 [D loss: 0.230082, acc.: 60.16%] [G loss: 0.450989]\n",
      "epoch:14 step:13323 [D loss: 0.211334, acc.: 67.97%] [G loss: 0.464361]\n",
      "epoch:14 step:13324 [D loss: 0.223020, acc.: 67.19%] [G loss: 0.487018]\n",
      "epoch:14 step:13325 [D loss: 0.172441, acc.: 78.12%] [G loss: 0.458165]\n",
      "epoch:14 step:13326 [D loss: 0.199436, acc.: 72.66%] [G loss: 0.504433]\n",
      "epoch:14 step:13327 [D loss: 0.169372, acc.: 77.34%] [G loss: 0.496721]\n",
      "epoch:14 step:13328 [D loss: 0.263428, acc.: 50.78%] [G loss: 0.479664]\n",
      "epoch:14 step:13329 [D loss: 0.240952, acc.: 63.28%] [G loss: 0.415628]\n",
      "epoch:14 step:13330 [D loss: 0.263886, acc.: 57.81%] [G loss: 0.416596]\n",
      "epoch:14 step:13331 [D loss: 0.240907, acc.: 64.84%] [G loss: 0.428339]\n",
      "epoch:14 step:13332 [D loss: 0.262394, acc.: 49.22%] [G loss: 0.417419]\n",
      "epoch:14 step:13333 [D loss: 0.252764, acc.: 50.00%] [G loss: 0.407452]\n",
      "epoch:14 step:13334 [D loss: 0.207051, acc.: 62.50%] [G loss: 0.447272]\n",
      "epoch:14 step:13335 [D loss: 0.210583, acc.: 65.62%] [G loss: 0.434767]\n",
      "epoch:14 step:13336 [D loss: 0.200900, acc.: 67.19%] [G loss: 0.450361]\n",
      "epoch:14 step:13337 [D loss: 0.205340, acc.: 68.75%] [G loss: 0.471755]\n",
      "epoch:14 step:13338 [D loss: 0.268903, acc.: 55.47%] [G loss: 0.444080]\n",
      "epoch:14 step:13339 [D loss: 0.212181, acc.: 64.06%] [G loss: 0.467346]\n",
      "epoch:14 step:13340 [D loss: 0.201147, acc.: 70.31%] [G loss: 0.498422]\n",
      "epoch:14 step:13341 [D loss: 0.219783, acc.: 63.28%] [G loss: 0.482788]\n",
      "epoch:14 step:13342 [D loss: 0.241737, acc.: 57.81%] [G loss: 0.456300]\n",
      "epoch:14 step:13343 [D loss: 0.214668, acc.: 69.53%] [G loss: 0.433621]\n",
      "epoch:14 step:13344 [D loss: 0.256874, acc.: 60.16%] [G loss: 0.406988]\n",
      "epoch:14 step:13345 [D loss: 0.215871, acc.: 68.75%] [G loss: 0.447254]\n",
      "epoch:14 step:13346 [D loss: 0.256378, acc.: 55.47%] [G loss: 0.346883]\n",
      "epoch:14 step:13347 [D loss: 0.220700, acc.: 60.16%] [G loss: 0.448984]\n",
      "epoch:14 step:13348 [D loss: 0.211990, acc.: 66.41%] [G loss: 0.476473]\n",
      "epoch:14 step:13349 [D loss: 0.186629, acc.: 71.88%] [G loss: 0.483195]\n",
      "epoch:14 step:13350 [D loss: 0.173265, acc.: 72.66%] [G loss: 0.477392]\n",
      "epoch:14 step:13351 [D loss: 0.274885, acc.: 55.47%] [G loss: 0.464643]\n",
      "epoch:14 step:13352 [D loss: 0.259743, acc.: 53.12%] [G loss: 0.442347]\n",
      "epoch:14 step:13353 [D loss: 0.224187, acc.: 67.97%] [G loss: 0.426533]\n",
      "epoch:14 step:13354 [D loss: 0.195613, acc.: 69.53%] [G loss: 0.454119]\n",
      "epoch:14 step:13355 [D loss: 0.217776, acc.: 60.94%] [G loss: 0.451919]\n",
      "epoch:14 step:13356 [D loss: 0.206858, acc.: 70.31%] [G loss: 0.413577]\n",
      "epoch:14 step:13357 [D loss: 0.206279, acc.: 67.19%] [G loss: 0.476947]\n",
      "epoch:14 step:13358 [D loss: 0.229924, acc.: 60.94%] [G loss: 0.429595]\n",
      "epoch:14 step:13359 [D loss: 0.201954, acc.: 71.09%] [G loss: 0.463949]\n",
      "epoch:14 step:13360 [D loss: 0.205501, acc.: 71.09%] [G loss: 0.458295]\n",
      "epoch:14 step:13361 [D loss: 0.229230, acc.: 57.81%] [G loss: 0.452147]\n",
      "epoch:14 step:13362 [D loss: 0.211475, acc.: 66.41%] [G loss: 0.436931]\n",
      "epoch:14 step:13363 [D loss: 0.199829, acc.: 67.97%] [G loss: 0.443998]\n",
      "epoch:14 step:13364 [D loss: 0.216607, acc.: 64.84%] [G loss: 0.478199]\n",
      "epoch:14 step:13365 [D loss: 0.230283, acc.: 61.72%] [G loss: 0.451779]\n",
      "epoch:14 step:13366 [D loss: 0.189914, acc.: 70.31%] [G loss: 0.486684]\n",
      "epoch:14 step:13367 [D loss: 0.268660, acc.: 55.47%] [G loss: 0.410747]\n",
      "epoch:14 step:13368 [D loss: 0.256917, acc.: 56.25%] [G loss: 0.430409]\n",
      "epoch:14 step:13369 [D loss: 0.260534, acc.: 49.22%] [G loss: 0.428094]\n",
      "epoch:14 step:13370 [D loss: 0.234266, acc.: 60.94%] [G loss: 0.469385]\n",
      "epoch:14 step:13371 [D loss: 0.230245, acc.: 60.16%] [G loss: 0.482632]\n",
      "epoch:14 step:13372 [D loss: 0.252431, acc.: 53.91%] [G loss: 0.438791]\n",
      "epoch:14 step:13373 [D loss: 0.215117, acc.: 65.62%] [G loss: 0.449527]\n",
      "epoch:14 step:13374 [D loss: 0.234129, acc.: 61.72%] [G loss: 0.441514]\n",
      "epoch:14 step:13375 [D loss: 0.221470, acc.: 60.94%] [G loss: 0.434105]\n",
      "epoch:14 step:13376 [D loss: 0.229933, acc.: 60.16%] [G loss: 0.432237]\n",
      "epoch:14 step:13377 [D loss: 0.194843, acc.: 70.31%] [G loss: 0.465916]\n",
      "epoch:14 step:13378 [D loss: 0.224342, acc.: 63.28%] [G loss: 0.433190]\n",
      "epoch:14 step:13379 [D loss: 0.219108, acc.: 63.28%] [G loss: 0.406247]\n",
      "epoch:14 step:13380 [D loss: 0.224600, acc.: 62.50%] [G loss: 0.442377]\n",
      "epoch:14 step:13381 [D loss: 0.229678, acc.: 64.06%] [G loss: 0.452662]\n",
      "epoch:14 step:13382 [D loss: 0.211317, acc.: 71.09%] [G loss: 0.427573]\n",
      "epoch:14 step:13383 [D loss: 0.244252, acc.: 56.25%] [G loss: 0.432245]\n",
      "epoch:14 step:13384 [D loss: 0.258118, acc.: 54.69%] [G loss: 0.406546]\n",
      "epoch:14 step:13385 [D loss: 0.221424, acc.: 65.62%] [G loss: 0.429187]\n",
      "epoch:14 step:13386 [D loss: 0.238682, acc.: 56.25%] [G loss: 0.443489]\n",
      "epoch:14 step:13387 [D loss: 0.226480, acc.: 60.94%] [G loss: 0.419382]\n",
      "epoch:14 step:13388 [D loss: 0.215186, acc.: 65.62%] [G loss: 0.450798]\n",
      "epoch:14 step:13389 [D loss: 0.226079, acc.: 64.84%] [G loss: 0.426395]\n",
      "epoch:14 step:13390 [D loss: 0.217640, acc.: 62.50%] [G loss: 0.474893]\n",
      "epoch:14 step:13391 [D loss: 0.214849, acc.: 64.84%] [G loss: 0.436653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13392 [D loss: 0.217361, acc.: 61.72%] [G loss: 0.466912]\n",
      "epoch:14 step:13393 [D loss: 0.225744, acc.: 68.75%] [G loss: 0.473678]\n",
      "epoch:14 step:13394 [D loss: 0.200097, acc.: 70.31%] [G loss: 0.477896]\n",
      "epoch:14 step:13395 [D loss: 0.231604, acc.: 60.94%] [G loss: 0.450954]\n",
      "epoch:14 step:13396 [D loss: 0.250452, acc.: 55.47%] [G loss: 0.419970]\n",
      "epoch:14 step:13397 [D loss: 0.216185, acc.: 64.06%] [G loss: 0.442295]\n",
      "epoch:14 step:13398 [D loss: 0.219995, acc.: 67.97%] [G loss: 0.412811]\n",
      "epoch:14 step:13399 [D loss: 0.256991, acc.: 60.16%] [G loss: 0.398752]\n",
      "epoch:14 step:13400 [D loss: 0.226088, acc.: 64.84%] [G loss: 0.409211]\n",
      "##############\n",
      "[2.71532141 1.90532245 6.14416857 4.67474619 3.50407517 5.59770256\n",
      " 4.33383394 4.6260919  4.5539793  3.87493152]\n",
      "##########\n",
      "epoch:14 step:13401 [D loss: 0.209960, acc.: 68.75%] [G loss: 0.465959]\n",
      "epoch:14 step:13402 [D loss: 0.212231, acc.: 67.97%] [G loss: 0.423051]\n",
      "epoch:14 step:13403 [D loss: 0.208920, acc.: 65.62%] [G loss: 0.452419]\n",
      "epoch:14 step:13404 [D loss: 0.205478, acc.: 69.53%] [G loss: 0.463591]\n",
      "epoch:14 step:13405 [D loss: 0.223243, acc.: 64.06%] [G loss: 0.415651]\n",
      "epoch:14 step:13406 [D loss: 0.253975, acc.: 54.69%] [G loss: 0.420081]\n",
      "epoch:14 step:13407 [D loss: 0.200585, acc.: 73.44%] [G loss: 0.442149]\n",
      "epoch:14 step:13408 [D loss: 0.230359, acc.: 61.72%] [G loss: 0.452670]\n",
      "epoch:14 step:13409 [D loss: 0.235114, acc.: 63.28%] [G loss: 0.448248]\n",
      "epoch:14 step:13410 [D loss: 0.222489, acc.: 61.72%] [G loss: 0.432217]\n",
      "epoch:14 step:13411 [D loss: 0.243391, acc.: 62.50%] [G loss: 0.422596]\n",
      "epoch:14 step:13412 [D loss: 0.259712, acc.: 54.69%] [G loss: 0.433522]\n",
      "epoch:14 step:13413 [D loss: 0.217211, acc.: 64.84%] [G loss: 0.452717]\n",
      "epoch:14 step:13414 [D loss: 0.224902, acc.: 63.28%] [G loss: 0.422618]\n",
      "epoch:14 step:13415 [D loss: 0.215719, acc.: 65.62%] [G loss: 0.433250]\n",
      "epoch:14 step:13416 [D loss: 0.195833, acc.: 73.44%] [G loss: 0.477351]\n",
      "epoch:14 step:13417 [D loss: 0.190040, acc.: 72.66%] [G loss: 0.434972]\n",
      "epoch:14 step:13418 [D loss: 0.195364, acc.: 71.88%] [G loss: 0.430416]\n",
      "epoch:14 step:13419 [D loss: 0.234216, acc.: 63.28%] [G loss: 0.434438]\n",
      "epoch:14 step:13420 [D loss: 0.251793, acc.: 58.59%] [G loss: 0.401481]\n",
      "epoch:14 step:13421 [D loss: 0.203986, acc.: 69.53%] [G loss: 0.449845]\n",
      "epoch:14 step:13422 [D loss: 0.232263, acc.: 63.28%] [G loss: 0.432642]\n",
      "epoch:14 step:13423 [D loss: 0.219187, acc.: 60.94%] [G loss: 0.448239]\n",
      "epoch:14 step:13424 [D loss: 0.213606, acc.: 63.28%] [G loss: 0.469848]\n",
      "epoch:14 step:13425 [D loss: 0.197447, acc.: 68.75%] [G loss: 0.473675]\n",
      "epoch:14 step:13426 [D loss: 0.254110, acc.: 55.47%] [G loss: 0.417697]\n",
      "epoch:14 step:13427 [D loss: 0.198170, acc.: 67.19%] [G loss: 0.497364]\n",
      "epoch:14 step:13428 [D loss: 0.209435, acc.: 65.62%] [G loss: 0.458664]\n",
      "epoch:14 step:13429 [D loss: 0.220022, acc.: 64.84%] [G loss: 0.444029]\n",
      "epoch:14 step:13430 [D loss: 0.174638, acc.: 71.88%] [G loss: 0.469576]\n",
      "epoch:14 step:13431 [D loss: 0.169646, acc.: 78.91%] [G loss: 0.556997]\n",
      "epoch:14 step:13432 [D loss: 0.214308, acc.: 67.19%] [G loss: 0.495568]\n",
      "epoch:14 step:13433 [D loss: 0.198303, acc.: 71.09%] [G loss: 0.497901]\n",
      "epoch:14 step:13434 [D loss: 0.265173, acc.: 56.25%] [G loss: 0.466687]\n",
      "epoch:14 step:13435 [D loss: 0.233418, acc.: 57.81%] [G loss: 0.392267]\n",
      "epoch:14 step:13436 [D loss: 0.217192, acc.: 67.97%] [G loss: 0.430412]\n",
      "epoch:14 step:13437 [D loss: 0.225069, acc.: 60.94%] [G loss: 0.436717]\n",
      "epoch:14 step:13438 [D loss: 0.212192, acc.: 69.53%] [G loss: 0.455979]\n",
      "epoch:14 step:13439 [D loss: 0.195066, acc.: 70.31%] [G loss: 0.487244]\n",
      "epoch:14 step:13440 [D loss: 0.208251, acc.: 67.19%] [G loss: 0.455877]\n",
      "epoch:14 step:13441 [D loss: 0.261674, acc.: 51.56%] [G loss: 0.389539]\n",
      "epoch:14 step:13442 [D loss: 0.226630, acc.: 64.84%] [G loss: 0.412145]\n",
      "epoch:14 step:13443 [D loss: 0.249679, acc.: 57.03%] [G loss: 0.421038]\n",
      "epoch:14 step:13444 [D loss: 0.242320, acc.: 59.38%] [G loss: 0.440371]\n",
      "epoch:14 step:13445 [D loss: 0.225361, acc.: 60.16%] [G loss: 0.457824]\n",
      "epoch:14 step:13446 [D loss: 0.205605, acc.: 67.97%] [G loss: 0.469732]\n",
      "epoch:14 step:13447 [D loss: 0.238815, acc.: 55.47%] [G loss: 0.458589]\n",
      "epoch:14 step:13448 [D loss: 0.215691, acc.: 67.19%] [G loss: 0.425404]\n",
      "epoch:14 step:13449 [D loss: 0.232608, acc.: 57.03%] [G loss: 0.444398]\n",
      "epoch:14 step:13450 [D loss: 0.194514, acc.: 68.75%] [G loss: 0.458709]\n",
      "epoch:14 step:13451 [D loss: 0.220838, acc.: 66.41%] [G loss: 0.456995]\n",
      "epoch:14 step:13452 [D loss: 0.207994, acc.: 65.62%] [G loss: 0.506261]\n",
      "epoch:14 step:13453 [D loss: 0.226536, acc.: 60.94%] [G loss: 0.467520]\n",
      "epoch:14 step:13454 [D loss: 0.202542, acc.: 70.31%] [G loss: 0.463219]\n",
      "epoch:14 step:13455 [D loss: 0.224687, acc.: 60.16%] [G loss: 0.460512]\n",
      "epoch:14 step:13456 [D loss: 0.212799, acc.: 66.41%] [G loss: 0.458956]\n",
      "epoch:14 step:13457 [D loss: 0.205968, acc.: 68.75%] [G loss: 0.442422]\n",
      "epoch:14 step:13458 [D loss: 0.229908, acc.: 63.28%] [G loss: 0.430290]\n",
      "epoch:14 step:13459 [D loss: 0.288829, acc.: 54.69%] [G loss: 0.425670]\n",
      "epoch:14 step:13460 [D loss: 0.224507, acc.: 63.28%] [G loss: 0.447497]\n",
      "epoch:14 step:13461 [D loss: 0.193437, acc.: 70.31%] [G loss: 0.507700]\n",
      "epoch:14 step:13462 [D loss: 0.212015, acc.: 67.97%] [G loss: 0.518666]\n",
      "epoch:14 step:13463 [D loss: 0.229805, acc.: 61.72%] [G loss: 0.489127]\n",
      "epoch:14 step:13464 [D loss: 0.213522, acc.: 63.28%] [G loss: 0.463964]\n",
      "epoch:14 step:13465 [D loss: 0.208573, acc.: 63.28%] [G loss: 0.566662]\n",
      "epoch:14 step:13466 [D loss: 0.289729, acc.: 52.34%] [G loss: 0.458704]\n",
      "epoch:14 step:13467 [D loss: 0.273787, acc.: 49.22%] [G loss: 0.404868]\n",
      "epoch:14 step:13468 [D loss: 0.226141, acc.: 57.81%] [G loss: 0.448323]\n",
      "epoch:14 step:13469 [D loss: 0.247111, acc.: 52.34%] [G loss: 0.432740]\n",
      "epoch:14 step:13470 [D loss: 0.245180, acc.: 56.25%] [G loss: 0.449382]\n",
      "epoch:14 step:13471 [D loss: 0.201363, acc.: 67.19%] [G loss: 0.482981]\n",
      "epoch:14 step:13472 [D loss: 0.190860, acc.: 68.75%] [G loss: 0.479062]\n",
      "epoch:14 step:13473 [D loss: 0.252116, acc.: 57.81%] [G loss: 0.438251]\n",
      "epoch:14 step:13474 [D loss: 0.209900, acc.: 67.97%] [G loss: 0.467649]\n",
      "epoch:14 step:13475 [D loss: 0.198126, acc.: 67.97%] [G loss: 0.460924]\n",
      "epoch:14 step:13476 [D loss: 0.194210, acc.: 68.75%] [G loss: 0.464165]\n",
      "epoch:14 step:13477 [D loss: 0.209243, acc.: 69.53%] [G loss: 0.474166]\n",
      "epoch:14 step:13478 [D loss: 0.225574, acc.: 57.81%] [G loss: 0.455709]\n",
      "epoch:14 step:13479 [D loss: 0.218324, acc.: 68.75%] [G loss: 0.427956]\n",
      "epoch:14 step:13480 [D loss: 0.232436, acc.: 64.06%] [G loss: 0.414665]\n",
      "epoch:14 step:13481 [D loss: 0.226239, acc.: 65.62%] [G loss: 0.433253]\n",
      "epoch:14 step:13482 [D loss: 0.203994, acc.: 70.31%] [G loss: 0.458588]\n",
      "epoch:14 step:13483 [D loss: 0.222220, acc.: 60.16%] [G loss: 0.465931]\n",
      "epoch:14 step:13484 [D loss: 0.223969, acc.: 67.97%] [G loss: 0.471976]\n",
      "epoch:14 step:13485 [D loss: 0.224034, acc.: 67.97%] [G loss: 0.428602]\n",
      "epoch:14 step:13486 [D loss: 0.220407, acc.: 65.62%] [G loss: 0.420171]\n",
      "epoch:14 step:13487 [D loss: 0.241552, acc.: 61.72%] [G loss: 0.431743]\n",
      "epoch:14 step:13488 [D loss: 0.186120, acc.: 71.09%] [G loss: 0.503447]\n",
      "epoch:14 step:13489 [D loss: 0.177529, acc.: 69.53%] [G loss: 0.501014]\n",
      "epoch:14 step:13490 [D loss: 0.213219, acc.: 67.19%] [G loss: 0.489718]\n",
      "epoch:14 step:13491 [D loss: 0.268147, acc.: 50.78%] [G loss: 0.498400]\n",
      "epoch:14 step:13492 [D loss: 0.193967, acc.: 68.75%] [G loss: 0.436867]\n",
      "epoch:14 step:13493 [D loss: 0.216452, acc.: 64.84%] [G loss: 0.455288]\n",
      "epoch:14 step:13494 [D loss: 0.262640, acc.: 57.03%] [G loss: 0.422216]\n",
      "epoch:14 step:13495 [D loss: 0.250579, acc.: 56.25%] [G loss: 0.440286]\n",
      "epoch:14 step:13496 [D loss: 0.235629, acc.: 59.38%] [G loss: 0.444232]\n",
      "epoch:14 step:13497 [D loss: 0.230633, acc.: 64.06%] [G loss: 0.429658]\n",
      "epoch:14 step:13498 [D loss: 0.208927, acc.: 67.19%] [G loss: 0.480486]\n",
      "epoch:14 step:13499 [D loss: 0.211502, acc.: 65.62%] [G loss: 0.484070]\n",
      "epoch:14 step:13500 [D loss: 0.245582, acc.: 60.94%] [G loss: 0.481588]\n",
      "epoch:14 step:13501 [D loss: 0.206877, acc.: 65.62%] [G loss: 0.451893]\n",
      "epoch:14 step:13502 [D loss: 0.216437, acc.: 64.84%] [G loss: 0.447338]\n",
      "epoch:14 step:13503 [D loss: 0.198031, acc.: 64.84%] [G loss: 0.457167]\n",
      "epoch:14 step:13504 [D loss: 0.232942, acc.: 62.50%] [G loss: 0.446655]\n",
      "epoch:14 step:13505 [D loss: 0.234735, acc.: 59.38%] [G loss: 0.420700]\n",
      "epoch:14 step:13506 [D loss: 0.220389, acc.: 63.28%] [G loss: 0.404272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13507 [D loss: 0.237038, acc.: 57.81%] [G loss: 0.438265]\n",
      "epoch:14 step:13508 [D loss: 0.257717, acc.: 55.47%] [G loss: 0.387135]\n",
      "epoch:14 step:13509 [D loss: 0.231361, acc.: 62.50%] [G loss: 0.428242]\n",
      "epoch:14 step:13510 [D loss: 0.236112, acc.: 60.94%] [G loss: 0.454061]\n",
      "epoch:14 step:13511 [D loss: 0.216731, acc.: 63.28%] [G loss: 0.468774]\n",
      "epoch:14 step:13512 [D loss: 0.226451, acc.: 64.06%] [G loss: 0.450349]\n",
      "epoch:14 step:13513 [D loss: 0.208400, acc.: 62.50%] [G loss: 0.455470]\n",
      "epoch:14 step:13514 [D loss: 0.265009, acc.: 51.56%] [G loss: 0.442189]\n",
      "epoch:14 step:13515 [D loss: 0.225586, acc.: 61.72%] [G loss: 0.440986]\n",
      "epoch:14 step:13516 [D loss: 0.196640, acc.: 74.22%] [G loss: 0.480340]\n",
      "epoch:14 step:13517 [D loss: 0.186241, acc.: 75.00%] [G loss: 0.498675]\n",
      "epoch:14 step:13518 [D loss: 0.256039, acc.: 56.25%] [G loss: 0.449127]\n",
      "epoch:14 step:13519 [D loss: 0.221829, acc.: 60.16%] [G loss: 0.378901]\n",
      "epoch:14 step:13520 [D loss: 0.216094, acc.: 61.72%] [G loss: 0.432374]\n",
      "epoch:14 step:13521 [D loss: 0.226184, acc.: 61.72%] [G loss: 0.439988]\n",
      "epoch:14 step:13522 [D loss: 0.240870, acc.: 60.16%] [G loss: 0.461588]\n",
      "epoch:14 step:13523 [D loss: 0.200375, acc.: 68.75%] [G loss: 0.482059]\n",
      "epoch:14 step:13524 [D loss: 0.191012, acc.: 68.75%] [G loss: 0.542300]\n",
      "epoch:14 step:13525 [D loss: 0.233100, acc.: 58.59%] [G loss: 0.472294]\n",
      "epoch:14 step:13526 [D loss: 0.252094, acc.: 55.47%] [G loss: 0.436388]\n",
      "epoch:14 step:13527 [D loss: 0.196207, acc.: 71.09%] [G loss: 0.428374]\n",
      "epoch:14 step:13528 [D loss: 0.255269, acc.: 58.59%] [G loss: 0.373024]\n",
      "epoch:14 step:13529 [D loss: 0.226837, acc.: 65.62%] [G loss: 0.436019]\n",
      "epoch:14 step:13530 [D loss: 0.229290, acc.: 64.06%] [G loss: 0.453234]\n",
      "epoch:14 step:13531 [D loss: 0.234963, acc.: 67.97%] [G loss: 0.432864]\n",
      "epoch:14 step:13532 [D loss: 0.220358, acc.: 63.28%] [G loss: 0.425395]\n",
      "epoch:14 step:13533 [D loss: 0.215107, acc.: 64.84%] [G loss: 0.446007]\n",
      "epoch:14 step:13534 [D loss: 0.189701, acc.: 69.53%] [G loss: 0.472084]\n",
      "epoch:14 step:13535 [D loss: 0.241144, acc.: 56.25%] [G loss: 0.499788]\n",
      "epoch:14 step:13536 [D loss: 0.290244, acc.: 42.97%] [G loss: 0.387392]\n",
      "epoch:14 step:13537 [D loss: 0.236420, acc.: 60.16%] [G loss: 0.470576]\n",
      "epoch:14 step:13538 [D loss: 0.215222, acc.: 65.62%] [G loss: 0.493655]\n",
      "epoch:14 step:13539 [D loss: 0.274673, acc.: 52.34%] [G loss: 0.374180]\n",
      "epoch:14 step:13540 [D loss: 0.217096, acc.: 63.28%] [G loss: 0.435942]\n",
      "epoch:14 step:13541 [D loss: 0.221208, acc.: 71.88%] [G loss: 0.425026]\n",
      "epoch:14 step:13542 [D loss: 0.239907, acc.: 57.03%] [G loss: 0.433330]\n",
      "epoch:14 step:13543 [D loss: 0.221205, acc.: 62.50%] [G loss: 0.434504]\n",
      "epoch:14 step:13544 [D loss: 0.194651, acc.: 75.00%] [G loss: 0.438074]\n",
      "epoch:14 step:13545 [D loss: 0.208419, acc.: 65.62%] [G loss: 0.437429]\n",
      "epoch:14 step:13546 [D loss: 0.185101, acc.: 70.31%] [G loss: 0.469526]\n",
      "epoch:14 step:13547 [D loss: 0.216811, acc.: 69.53%] [G loss: 0.491669]\n",
      "epoch:14 step:13548 [D loss: 0.221653, acc.: 64.84%] [G loss: 0.522451]\n",
      "epoch:14 step:13549 [D loss: 0.222182, acc.: 67.19%] [G loss: 0.489601]\n",
      "epoch:14 step:13550 [D loss: 0.260080, acc.: 56.25%] [G loss: 0.421922]\n",
      "epoch:14 step:13551 [D loss: 0.210775, acc.: 64.06%] [G loss: 0.424489]\n",
      "epoch:14 step:13552 [D loss: 0.221399, acc.: 68.75%] [G loss: 0.474320]\n",
      "epoch:14 step:13553 [D loss: 0.208921, acc.: 67.19%] [G loss: 0.486176]\n",
      "epoch:14 step:13554 [D loss: 0.219591, acc.: 67.19%] [G loss: 0.462087]\n",
      "epoch:14 step:13555 [D loss: 0.251171, acc.: 60.16%] [G loss: 0.463948]\n",
      "epoch:14 step:13556 [D loss: 0.244133, acc.: 57.81%] [G loss: 0.412499]\n",
      "epoch:14 step:13557 [D loss: 0.212747, acc.: 68.75%] [G loss: 0.416125]\n",
      "epoch:14 step:13558 [D loss: 0.223611, acc.: 59.38%] [G loss: 0.464243]\n",
      "epoch:14 step:13559 [D loss: 0.210238, acc.: 68.75%] [G loss: 0.457750]\n",
      "epoch:14 step:13560 [D loss: 0.244641, acc.: 62.50%] [G loss: 0.449341]\n",
      "epoch:14 step:13561 [D loss: 0.241690, acc.: 58.59%] [G loss: 0.419518]\n",
      "epoch:14 step:13562 [D loss: 0.252846, acc.: 50.78%] [G loss: 0.386828]\n",
      "epoch:14 step:13563 [D loss: 0.226857, acc.: 62.50%] [G loss: 0.435184]\n",
      "epoch:14 step:13564 [D loss: 0.221722, acc.: 64.84%] [G loss: 0.470677]\n",
      "epoch:14 step:13565 [D loss: 0.214301, acc.: 68.75%] [G loss: 0.441627]\n",
      "epoch:14 step:13566 [D loss: 0.260261, acc.: 50.00%] [G loss: 0.430782]\n",
      "epoch:14 step:13567 [D loss: 0.228154, acc.: 65.62%] [G loss: 0.445391]\n",
      "epoch:14 step:13568 [D loss: 0.212654, acc.: 62.50%] [G loss: 0.436784]\n",
      "epoch:14 step:13569 [D loss: 0.196799, acc.: 71.09%] [G loss: 0.481446]\n",
      "epoch:14 step:13570 [D loss: 0.187128, acc.: 75.00%] [G loss: 0.486235]\n",
      "epoch:14 step:13571 [D loss: 0.196626, acc.: 72.66%] [G loss: 0.518140]\n",
      "epoch:14 step:13572 [D loss: 0.234297, acc.: 65.62%] [G loss: 0.471134]\n",
      "epoch:14 step:13573 [D loss: 0.244374, acc.: 57.81%] [G loss: 0.463497]\n",
      "epoch:14 step:13574 [D loss: 0.233597, acc.: 60.16%] [G loss: 0.449484]\n",
      "epoch:14 step:13575 [D loss: 0.224742, acc.: 60.94%] [G loss: 0.465787]\n",
      "epoch:14 step:13576 [D loss: 0.264400, acc.: 57.81%] [G loss: 0.433846]\n",
      "epoch:14 step:13577 [D loss: 0.260615, acc.: 53.91%] [G loss: 0.396928]\n",
      "epoch:14 step:13578 [D loss: 0.224267, acc.: 64.84%] [G loss: 0.464051]\n",
      "epoch:14 step:13579 [D loss: 0.266441, acc.: 53.91%] [G loss: 0.393141]\n",
      "epoch:14 step:13580 [D loss: 0.249536, acc.: 57.03%] [G loss: 0.433529]\n",
      "epoch:14 step:13581 [D loss: 0.239769, acc.: 61.72%] [G loss: 0.421681]\n",
      "epoch:14 step:13582 [D loss: 0.223218, acc.: 60.16%] [G loss: 0.409787]\n",
      "epoch:14 step:13583 [D loss: 0.222965, acc.: 60.16%] [G loss: 0.425642]\n",
      "epoch:14 step:13584 [D loss: 0.208220, acc.: 71.88%] [G loss: 0.449296]\n",
      "epoch:14 step:13585 [D loss: 0.220507, acc.: 64.06%] [G loss: 0.424656]\n",
      "epoch:14 step:13586 [D loss: 0.212397, acc.: 68.75%] [G loss: 0.392521]\n",
      "epoch:14 step:13587 [D loss: 0.211826, acc.: 67.19%] [G loss: 0.445853]\n",
      "epoch:14 step:13588 [D loss: 0.212556, acc.: 67.97%] [G loss: 0.486237]\n",
      "epoch:14 step:13589 [D loss: 0.187862, acc.: 72.66%] [G loss: 0.508960]\n",
      "epoch:14 step:13590 [D loss: 0.208628, acc.: 70.31%] [G loss: 0.493837]\n",
      "epoch:14 step:13591 [D loss: 0.253218, acc.: 55.47%] [G loss: 0.465686]\n",
      "epoch:14 step:13592 [D loss: 0.198431, acc.: 71.88%] [G loss: 0.463776]\n",
      "epoch:14 step:13593 [D loss: 0.187400, acc.: 78.12%] [G loss: 0.483476]\n",
      "epoch:14 step:13594 [D loss: 0.263311, acc.: 59.38%] [G loss: 0.431351]\n",
      "epoch:14 step:13595 [D loss: 0.234713, acc.: 60.94%] [G loss: 0.436617]\n",
      "epoch:14 step:13596 [D loss: 0.230391, acc.: 60.94%] [G loss: 0.417728]\n",
      "epoch:14 step:13597 [D loss: 0.242988, acc.: 60.94%] [G loss: 0.408044]\n",
      "epoch:14 step:13598 [D loss: 0.222013, acc.: 64.06%] [G loss: 0.419249]\n",
      "epoch:14 step:13599 [D loss: 0.192268, acc.: 75.78%] [G loss: 0.448983]\n",
      "epoch:14 step:13600 [D loss: 0.248620, acc.: 53.91%] [G loss: 0.444178]\n",
      "##############\n",
      "[2.87421477 1.85212301 6.24222356 4.85681066 3.68698513 5.53293315\n",
      " 4.41440118 4.58019149 4.49809721 3.75417348]\n",
      "##########\n",
      "epoch:14 step:13601 [D loss: 0.232299, acc.: 60.94%] [G loss: 0.380012]\n",
      "epoch:14 step:13602 [D loss: 0.194747, acc.: 70.31%] [G loss: 0.421955]\n",
      "epoch:14 step:13603 [D loss: 0.210555, acc.: 69.53%] [G loss: 0.475150]\n",
      "epoch:14 step:13604 [D loss: 0.229240, acc.: 59.38%] [G loss: 0.470171]\n",
      "epoch:14 step:13605 [D loss: 0.209857, acc.: 60.16%] [G loss: 0.436549]\n",
      "epoch:14 step:13606 [D loss: 0.176501, acc.: 74.22%] [G loss: 0.451842]\n",
      "epoch:14 step:13607 [D loss: 0.267173, acc.: 54.69%] [G loss: 0.416683]\n",
      "epoch:14 step:13608 [D loss: 0.243800, acc.: 58.59%] [G loss: 0.429999]\n",
      "epoch:14 step:13609 [D loss: 0.211469, acc.: 66.41%] [G loss: 0.427747]\n",
      "epoch:14 step:13610 [D loss: 0.232778, acc.: 60.16%] [G loss: 0.414963]\n",
      "epoch:14 step:13611 [D loss: 0.217904, acc.: 63.28%] [G loss: 0.470521]\n",
      "epoch:14 step:13612 [D loss: 0.232292, acc.: 63.28%] [G loss: 0.422479]\n",
      "epoch:14 step:13613 [D loss: 0.187021, acc.: 72.66%] [G loss: 0.516990]\n",
      "epoch:14 step:13614 [D loss: 0.197904, acc.: 71.88%] [G loss: 0.456970]\n",
      "epoch:14 step:13615 [D loss: 0.241989, acc.: 62.50%] [G loss: 0.436727]\n",
      "epoch:14 step:13616 [D loss: 0.231230, acc.: 61.72%] [G loss: 0.431487]\n",
      "epoch:14 step:13617 [D loss: 0.200333, acc.: 71.09%] [G loss: 0.441775]\n",
      "epoch:14 step:13618 [D loss: 0.259835, acc.: 52.34%] [G loss: 0.464405]\n",
      "epoch:14 step:13619 [D loss: 0.269622, acc.: 50.78%] [G loss: 0.413748]\n",
      "epoch:14 step:13620 [D loss: 0.219121, acc.: 64.84%] [G loss: 0.437772]\n",
      "epoch:14 step:13621 [D loss: 0.243901, acc.: 58.59%] [G loss: 0.432182]\n",
      "epoch:14 step:13622 [D loss: 0.182041, acc.: 75.00%] [G loss: 0.458494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13623 [D loss: 0.235999, acc.: 64.84%] [G loss: 0.448394]\n",
      "epoch:14 step:13624 [D loss: 0.253928, acc.: 56.25%] [G loss: 0.473237]\n",
      "epoch:14 step:13625 [D loss: 0.203918, acc.: 70.31%] [G loss: 0.474581]\n",
      "epoch:14 step:13626 [D loss: 0.210116, acc.: 65.62%] [G loss: 0.452642]\n",
      "epoch:14 step:13627 [D loss: 0.228522, acc.: 65.62%] [G loss: 0.466446]\n",
      "epoch:14 step:13628 [D loss: 0.244529, acc.: 61.72%] [G loss: 0.440443]\n",
      "epoch:14 step:13629 [D loss: 0.241987, acc.: 56.25%] [G loss: 0.426159]\n",
      "epoch:14 step:13630 [D loss: 0.219979, acc.: 63.28%] [G loss: 0.497055]\n",
      "epoch:14 step:13631 [D loss: 0.218148, acc.: 64.84%] [G loss: 0.428928]\n",
      "epoch:14 step:13632 [D loss: 0.221162, acc.: 64.06%] [G loss: 0.474178]\n",
      "epoch:14 step:13633 [D loss: 0.189710, acc.: 71.09%] [G loss: 0.479831]\n",
      "epoch:14 step:13634 [D loss: 0.188279, acc.: 74.22%] [G loss: 0.491424]\n",
      "epoch:14 step:13635 [D loss: 0.256549, acc.: 55.47%] [G loss: 0.425954]\n",
      "epoch:14 step:13636 [D loss: 0.244054, acc.: 60.16%] [G loss: 0.392798]\n",
      "epoch:14 step:13637 [D loss: 0.195617, acc.: 67.97%] [G loss: 0.447384]\n",
      "epoch:14 step:13638 [D loss: 0.213264, acc.: 63.28%] [G loss: 0.472316]\n",
      "epoch:14 step:13639 [D loss: 0.214250, acc.: 67.19%] [G loss: 0.444746]\n",
      "epoch:14 step:13640 [D loss: 0.213509, acc.: 66.41%] [G loss: 0.434420]\n",
      "epoch:14 step:13641 [D loss: 0.232630, acc.: 57.03%] [G loss: 0.421874]\n",
      "epoch:14 step:13642 [D loss: 0.217289, acc.: 63.28%] [G loss: 0.459496]\n",
      "epoch:14 step:13643 [D loss: 0.242160, acc.: 61.72%] [G loss: 0.428083]\n",
      "epoch:14 step:13644 [D loss: 0.194497, acc.: 71.88%] [G loss: 0.447217]\n",
      "epoch:14 step:13645 [D loss: 0.242496, acc.: 63.28%] [G loss: 0.426332]\n",
      "epoch:14 step:13646 [D loss: 0.264168, acc.: 53.91%] [G loss: 0.440718]\n",
      "epoch:14 step:13647 [D loss: 0.260830, acc.: 48.44%] [G loss: 0.397500]\n",
      "epoch:14 step:13648 [D loss: 0.205778, acc.: 68.75%] [G loss: 0.444927]\n",
      "epoch:14 step:13649 [D loss: 0.231617, acc.: 61.72%] [G loss: 0.423810]\n",
      "epoch:14 step:13650 [D loss: 0.233784, acc.: 57.03%] [G loss: 0.423730]\n",
      "epoch:14 step:13651 [D loss: 0.220802, acc.: 59.38%] [G loss: 0.453873]\n",
      "epoch:14 step:13652 [D loss: 0.191700, acc.: 69.53%] [G loss: 0.477219]\n",
      "epoch:14 step:13653 [D loss: 0.245262, acc.: 58.59%] [G loss: 0.444547]\n",
      "epoch:14 step:13654 [D loss: 0.234079, acc.: 61.72%] [G loss: 0.402991]\n",
      "epoch:14 step:13655 [D loss: 0.254092, acc.: 57.81%] [G loss: 0.422773]\n",
      "epoch:14 step:13656 [D loss: 0.230225, acc.: 59.38%] [G loss: 0.434073]\n",
      "epoch:14 step:13657 [D loss: 0.222230, acc.: 63.28%] [G loss: 0.437057]\n",
      "epoch:14 step:13658 [D loss: 0.219280, acc.: 66.41%] [G loss: 0.428324]\n",
      "epoch:14 step:13659 [D loss: 0.231173, acc.: 64.06%] [G loss: 0.384919]\n",
      "epoch:14 step:13660 [D loss: 0.272476, acc.: 52.34%] [G loss: 0.414769]\n",
      "epoch:14 step:13661 [D loss: 0.230174, acc.: 63.28%] [G loss: 0.415237]\n",
      "epoch:14 step:13662 [D loss: 0.247497, acc.: 56.25%] [G loss: 0.374648]\n",
      "epoch:14 step:13663 [D loss: 0.207652, acc.: 64.06%] [G loss: 0.405920]\n",
      "epoch:14 step:13664 [D loss: 0.203805, acc.: 64.84%] [G loss: 0.456248]\n",
      "epoch:14 step:13665 [D loss: 0.218700, acc.: 64.84%] [G loss: 0.418655]\n",
      "epoch:14 step:13666 [D loss: 0.218331, acc.: 66.41%] [G loss: 0.478154]\n",
      "epoch:14 step:13667 [D loss: 0.220954, acc.: 63.28%] [G loss: 0.470935]\n",
      "epoch:14 step:13668 [D loss: 0.213235, acc.: 61.72%] [G loss: 0.454944]\n",
      "epoch:14 step:13669 [D loss: 0.203849, acc.: 65.62%] [G loss: 0.451438]\n",
      "epoch:14 step:13670 [D loss: 0.188708, acc.: 72.66%] [G loss: 0.467887]\n",
      "epoch:14 step:13671 [D loss: 0.242405, acc.: 57.81%] [G loss: 0.430990]\n",
      "epoch:14 step:13672 [D loss: 0.209748, acc.: 64.06%] [G loss: 0.419644]\n",
      "epoch:14 step:13673 [D loss: 0.202277, acc.: 71.09%] [G loss: 0.421339]\n",
      "epoch:14 step:13674 [D loss: 0.210909, acc.: 69.53%] [G loss: 0.462283]\n",
      "epoch:14 step:13675 [D loss: 0.212583, acc.: 64.06%] [G loss: 0.477278]\n",
      "epoch:14 step:13676 [D loss: 0.220065, acc.: 64.84%] [G loss: 0.466012]\n",
      "epoch:14 step:13677 [D loss: 0.253367, acc.: 54.69%] [G loss: 0.420338]\n",
      "epoch:14 step:13678 [D loss: 0.236044, acc.: 59.38%] [G loss: 0.455821]\n",
      "epoch:14 step:13679 [D loss: 0.230409, acc.: 63.28%] [G loss: 0.440669]\n",
      "epoch:14 step:13680 [D loss: 0.224717, acc.: 63.28%] [G loss: 0.460838]\n",
      "epoch:14 step:13681 [D loss: 0.209039, acc.: 66.41%] [G loss: 0.464122]\n",
      "epoch:14 step:13682 [D loss: 0.177841, acc.: 78.12%] [G loss: 0.469301]\n",
      "epoch:14 step:13683 [D loss: 0.256881, acc.: 55.47%] [G loss: 0.481577]\n",
      "epoch:14 step:13684 [D loss: 0.236160, acc.: 57.81%] [G loss: 0.459166]\n",
      "epoch:14 step:13685 [D loss: 0.208861, acc.: 68.75%] [G loss: 0.498221]\n",
      "epoch:14 step:13686 [D loss: 0.199737, acc.: 71.09%] [G loss: 0.454838]\n",
      "epoch:14 step:13687 [D loss: 0.266556, acc.: 55.47%] [G loss: 0.432329]\n",
      "epoch:14 step:13688 [D loss: 0.211234, acc.: 68.75%] [G loss: 0.429736]\n",
      "epoch:14 step:13689 [D loss: 0.218135, acc.: 58.59%] [G loss: 0.384753]\n",
      "epoch:14 step:13690 [D loss: 0.219735, acc.: 62.50%] [G loss: 0.435636]\n",
      "epoch:14 step:13691 [D loss: 0.200364, acc.: 71.88%] [G loss: 0.457285]\n",
      "epoch:14 step:13692 [D loss: 0.185150, acc.: 71.09%] [G loss: 0.480350]\n",
      "epoch:14 step:13693 [D loss: 0.208186, acc.: 66.41%] [G loss: 0.478655]\n",
      "epoch:14 step:13694 [D loss: 0.222167, acc.: 62.50%] [G loss: 0.462720]\n",
      "epoch:14 step:13695 [D loss: 0.234504, acc.: 58.59%] [G loss: 0.449480]\n",
      "epoch:14 step:13696 [D loss: 0.218066, acc.: 64.06%] [G loss: 0.460095]\n",
      "epoch:14 step:13697 [D loss: 0.241859, acc.: 52.34%] [G loss: 0.409646]\n",
      "epoch:14 step:13698 [D loss: 0.214652, acc.: 66.41%] [G loss: 0.412322]\n",
      "epoch:14 step:13699 [D loss: 0.215085, acc.: 60.94%] [G loss: 0.449156]\n",
      "epoch:14 step:13700 [D loss: 0.204761, acc.: 68.75%] [G loss: 0.423269]\n",
      "epoch:14 step:13701 [D loss: 0.238771, acc.: 61.72%] [G loss: 0.466264]\n",
      "epoch:14 step:13702 [D loss: 0.244450, acc.: 57.03%] [G loss: 0.403856]\n",
      "epoch:14 step:13703 [D loss: 0.233427, acc.: 57.81%] [G loss: 0.431204]\n",
      "epoch:14 step:13704 [D loss: 0.251602, acc.: 56.25%] [G loss: 0.438351]\n",
      "epoch:14 step:13705 [D loss: 0.238554, acc.: 63.28%] [G loss: 0.441620]\n",
      "epoch:14 step:13706 [D loss: 0.222555, acc.: 64.06%] [G loss: 0.470877]\n",
      "epoch:14 step:13707 [D loss: 0.204476, acc.: 67.97%] [G loss: 0.471093]\n",
      "epoch:14 step:13708 [D loss: 0.260660, acc.: 59.38%] [G loss: 0.429991]\n",
      "epoch:14 step:13709 [D loss: 0.251423, acc.: 64.84%] [G loss: 0.440429]\n",
      "epoch:14 step:13710 [D loss: 0.186583, acc.: 74.22%] [G loss: 0.471805]\n",
      "epoch:14 step:13711 [D loss: 0.213284, acc.: 64.84%] [G loss: 0.480688]\n",
      "epoch:14 step:13712 [D loss: 0.233539, acc.: 57.03%] [G loss: 0.468458]\n",
      "epoch:14 step:13713 [D loss: 0.220413, acc.: 66.41%] [G loss: 0.404551]\n",
      "epoch:14 step:13714 [D loss: 0.226335, acc.: 65.62%] [G loss: 0.430897]\n",
      "epoch:14 step:13715 [D loss: 0.230326, acc.: 61.72%] [G loss: 0.429340]\n",
      "epoch:14 step:13716 [D loss: 0.219273, acc.: 68.75%] [G loss: 0.439885]\n",
      "epoch:14 step:13717 [D loss: 0.243076, acc.: 59.38%] [G loss: 0.448397]\n",
      "epoch:14 step:13718 [D loss: 0.245669, acc.: 57.03%] [G loss: 0.434413]\n",
      "epoch:14 step:13719 [D loss: 0.206877, acc.: 70.31%] [G loss: 0.456940]\n",
      "epoch:14 step:13720 [D loss: 0.237838, acc.: 64.06%] [G loss: 0.431998]\n",
      "epoch:14 step:13721 [D loss: 0.220626, acc.: 64.84%] [G loss: 0.390215]\n",
      "epoch:14 step:13722 [D loss: 0.220028, acc.: 61.72%] [G loss: 0.429572]\n",
      "epoch:14 step:13723 [D loss: 0.191885, acc.: 69.53%] [G loss: 0.435636]\n",
      "epoch:14 step:13724 [D loss: 0.244397, acc.: 54.69%] [G loss: 0.397637]\n",
      "epoch:14 step:13725 [D loss: 0.219906, acc.: 64.06%] [G loss: 0.417677]\n",
      "epoch:14 step:13726 [D loss: 0.207012, acc.: 67.19%] [G loss: 0.420193]\n",
      "epoch:14 step:13727 [D loss: 0.229639, acc.: 63.28%] [G loss: 0.423492]\n",
      "epoch:14 step:13728 [D loss: 0.234866, acc.: 66.41%] [G loss: 0.414722]\n",
      "epoch:14 step:13729 [D loss: 0.230146, acc.: 63.28%] [G loss: 0.382239]\n",
      "epoch:14 step:13730 [D loss: 0.256877, acc.: 58.59%] [G loss: 0.399289]\n",
      "epoch:14 step:13731 [D loss: 0.200321, acc.: 67.97%] [G loss: 0.468315]\n",
      "epoch:14 step:13732 [D loss: 0.242268, acc.: 53.12%] [G loss: 0.424167]\n",
      "epoch:14 step:13733 [D loss: 0.272052, acc.: 50.00%] [G loss: 0.391261]\n",
      "epoch:14 step:13734 [D loss: 0.227397, acc.: 69.53%] [G loss: 0.454662]\n",
      "epoch:14 step:13735 [D loss: 0.233673, acc.: 58.59%] [G loss: 0.472798]\n",
      "epoch:14 step:13736 [D loss: 0.234079, acc.: 60.94%] [G loss: 0.467099]\n",
      "epoch:14 step:13737 [D loss: 0.246520, acc.: 61.72%] [G loss: 0.427951]\n",
      "epoch:14 step:13738 [D loss: 0.227674, acc.: 65.62%] [G loss: 0.431610]\n",
      "epoch:14 step:13739 [D loss: 0.216397, acc.: 64.06%] [G loss: 0.444729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13740 [D loss: 0.242981, acc.: 64.84%] [G loss: 0.406150]\n",
      "epoch:14 step:13741 [D loss: 0.221707, acc.: 65.62%] [G loss: 0.447672]\n",
      "epoch:14 step:13742 [D loss: 0.169669, acc.: 75.78%] [G loss: 0.483601]\n",
      "epoch:14 step:13743 [D loss: 0.256739, acc.: 51.56%] [G loss: 0.414750]\n",
      "epoch:14 step:13744 [D loss: 0.232441, acc.: 57.81%] [G loss: 0.426778]\n",
      "epoch:14 step:13745 [D loss: 0.228048, acc.: 64.84%] [G loss: 0.428478]\n",
      "epoch:14 step:13746 [D loss: 0.261993, acc.: 50.00%] [G loss: 0.398538]\n",
      "epoch:14 step:13747 [D loss: 0.188297, acc.: 76.56%] [G loss: 0.449251]\n",
      "epoch:14 step:13748 [D loss: 0.211289, acc.: 67.97%] [G loss: 0.427895]\n",
      "epoch:14 step:13749 [D loss: 0.200265, acc.: 69.53%] [G loss: 0.442558]\n",
      "epoch:14 step:13750 [D loss: 0.215114, acc.: 64.84%] [G loss: 0.435878]\n",
      "epoch:14 step:13751 [D loss: 0.191901, acc.: 71.88%] [G loss: 0.438681]\n",
      "epoch:14 step:13752 [D loss: 0.215490, acc.: 63.28%] [G loss: 0.431681]\n",
      "epoch:14 step:13753 [D loss: 0.190490, acc.: 72.66%] [G loss: 0.455744]\n",
      "epoch:14 step:13754 [D loss: 0.249603, acc.: 55.47%] [G loss: 0.424945]\n",
      "epoch:14 step:13755 [D loss: 0.218090, acc.: 66.41%] [G loss: 0.486610]\n",
      "epoch:14 step:13756 [D loss: 0.205013, acc.: 68.75%] [G loss: 0.455337]\n",
      "epoch:14 step:13757 [D loss: 0.236078, acc.: 59.38%] [G loss: 0.432026]\n",
      "epoch:14 step:13758 [D loss: 0.235279, acc.: 59.38%] [G loss: 0.465713]\n",
      "epoch:14 step:13759 [D loss: 0.215472, acc.: 64.84%] [G loss: 0.480753]\n",
      "epoch:14 step:13760 [D loss: 0.190204, acc.: 71.09%] [G loss: 0.517508]\n",
      "epoch:14 step:13761 [D loss: 0.218564, acc.: 57.03%] [G loss: 0.472661]\n",
      "epoch:14 step:13762 [D loss: 0.220281, acc.: 61.72%] [G loss: 0.424326]\n",
      "epoch:14 step:13763 [D loss: 0.240827, acc.: 53.91%] [G loss: 0.429980]\n",
      "epoch:14 step:13764 [D loss: 0.215303, acc.: 66.41%] [G loss: 0.432425]\n",
      "epoch:14 step:13765 [D loss: 0.191887, acc.: 74.22%] [G loss: 0.464751]\n",
      "epoch:14 step:13766 [D loss: 0.177810, acc.: 72.66%] [G loss: 0.481846]\n",
      "epoch:14 step:13767 [D loss: 0.225431, acc.: 65.62%] [G loss: 0.490944]\n",
      "epoch:14 step:13768 [D loss: 0.201774, acc.: 71.09%] [G loss: 0.485598]\n",
      "epoch:14 step:13769 [D loss: 0.237181, acc.: 60.16%] [G loss: 0.461661]\n",
      "epoch:14 step:13770 [D loss: 0.224454, acc.: 61.72%] [G loss: 0.443238]\n",
      "epoch:14 step:13771 [D loss: 0.216820, acc.: 69.53%] [G loss: 0.455985]\n",
      "epoch:14 step:13772 [D loss: 0.222407, acc.: 60.94%] [G loss: 0.474431]\n",
      "epoch:14 step:13773 [D loss: 0.242348, acc.: 56.25%] [G loss: 0.436215]\n",
      "epoch:14 step:13774 [D loss: 0.237839, acc.: 51.56%] [G loss: 0.465630]\n",
      "epoch:14 step:13775 [D loss: 0.227574, acc.: 67.97%] [G loss: 0.421604]\n",
      "epoch:14 step:13776 [D loss: 0.224015, acc.: 64.84%] [G loss: 0.462463]\n",
      "epoch:14 step:13777 [D loss: 0.202415, acc.: 71.88%] [G loss: 0.460117]\n",
      "epoch:14 step:13778 [D loss: 0.199061, acc.: 67.97%] [G loss: 0.487310]\n",
      "epoch:14 step:13779 [D loss: 0.185663, acc.: 71.88%] [G loss: 0.481205]\n",
      "epoch:14 step:13780 [D loss: 0.243919, acc.: 58.59%] [G loss: 0.423591]\n",
      "epoch:14 step:13781 [D loss: 0.235204, acc.: 65.62%] [G loss: 0.498384]\n",
      "epoch:14 step:13782 [D loss: 0.264039, acc.: 52.34%] [G loss: 0.423136]\n",
      "epoch:14 step:13783 [D loss: 0.232276, acc.: 57.81%] [G loss: 0.444408]\n",
      "epoch:14 step:13784 [D loss: 0.189715, acc.: 75.00%] [G loss: 0.493528]\n",
      "epoch:14 step:13785 [D loss: 0.225575, acc.: 64.06%] [G loss: 0.493210]\n",
      "epoch:14 step:13786 [D loss: 0.219638, acc.: 64.06%] [G loss: 0.441927]\n",
      "epoch:14 step:13787 [D loss: 0.217336, acc.: 63.28%] [G loss: 0.435708]\n",
      "epoch:14 step:13788 [D loss: 0.231214, acc.: 60.94%] [G loss: 0.439415]\n",
      "epoch:14 step:13789 [D loss: 0.259007, acc.: 53.91%] [G loss: 0.396041]\n",
      "epoch:14 step:13790 [D loss: 0.207174, acc.: 69.53%] [G loss: 0.450475]\n",
      "epoch:14 step:13791 [D loss: 0.200337, acc.: 71.88%] [G loss: 0.463971]\n",
      "epoch:14 step:13792 [D loss: 0.220320, acc.: 67.97%] [G loss: 0.456919]\n",
      "epoch:14 step:13793 [D loss: 0.248391, acc.: 50.78%] [G loss: 0.432083]\n",
      "epoch:14 step:13794 [D loss: 0.224905, acc.: 64.06%] [G loss: 0.438010]\n",
      "epoch:14 step:13795 [D loss: 0.185437, acc.: 72.66%] [G loss: 0.436441]\n",
      "epoch:14 step:13796 [D loss: 0.236969, acc.: 60.94%] [G loss: 0.441578]\n",
      "epoch:14 step:13797 [D loss: 0.205239, acc.: 70.31%] [G loss: 0.423311]\n",
      "epoch:14 step:13798 [D loss: 0.214537, acc.: 67.97%] [G loss: 0.462664]\n",
      "epoch:14 step:13799 [D loss: 0.196101, acc.: 70.31%] [G loss: 0.490604]\n",
      "epoch:14 step:13800 [D loss: 0.245362, acc.: 57.81%] [G loss: 0.445401]\n",
      "##############\n",
      "[2.54348789 1.84229476 5.79224368 4.59107272 3.58621902 5.46480942\n",
      " 4.41719688 4.98591031 4.36060528 3.97211782]\n",
      "##########\n",
      "epoch:14 step:13801 [D loss: 0.228873, acc.: 59.38%] [G loss: 0.405158]\n",
      "epoch:14 step:13802 [D loss: 0.231806, acc.: 63.28%] [G loss: 0.439649]\n",
      "epoch:14 step:13803 [D loss: 0.212910, acc.: 68.75%] [G loss: 0.440118]\n",
      "epoch:14 step:13804 [D loss: 0.227696, acc.: 63.28%] [G loss: 0.432659]\n",
      "epoch:14 step:13805 [D loss: 0.224902, acc.: 64.06%] [G loss: 0.435334]\n",
      "epoch:14 step:13806 [D loss: 0.204811, acc.: 69.53%] [G loss: 0.436350]\n",
      "epoch:14 step:13807 [D loss: 0.218191, acc.: 64.06%] [G loss: 0.422204]\n",
      "epoch:14 step:13808 [D loss: 0.184494, acc.: 71.09%] [G loss: 0.501386]\n",
      "epoch:14 step:13809 [D loss: 0.201399, acc.: 67.97%] [G loss: 0.469036]\n",
      "epoch:14 step:13810 [D loss: 0.197164, acc.: 71.88%] [G loss: 0.486301]\n",
      "epoch:14 step:13811 [D loss: 0.199856, acc.: 67.97%] [G loss: 0.452558]\n",
      "epoch:14 step:13812 [D loss: 0.182797, acc.: 68.75%] [G loss: 0.504631]\n",
      "epoch:14 step:13813 [D loss: 0.220373, acc.: 64.84%] [G loss: 0.489786]\n",
      "epoch:14 step:13814 [D loss: 0.258403, acc.: 53.91%] [G loss: 0.422515]\n",
      "epoch:14 step:13815 [D loss: 0.229349, acc.: 64.06%] [G loss: 0.464224]\n",
      "epoch:14 step:13816 [D loss: 0.249108, acc.: 59.38%] [G loss: 0.440549]\n",
      "epoch:14 step:13817 [D loss: 0.200369, acc.: 71.09%] [G loss: 0.479953]\n",
      "epoch:14 step:13818 [D loss: 0.227169, acc.: 59.38%] [G loss: 0.491501]\n",
      "epoch:14 step:13819 [D loss: 0.210935, acc.: 70.31%] [G loss: 0.501310]\n",
      "epoch:14 step:13820 [D loss: 0.241020, acc.: 59.38%] [G loss: 0.451889]\n",
      "epoch:14 step:13821 [D loss: 0.261400, acc.: 57.81%] [G loss: 0.452468]\n",
      "epoch:14 step:13822 [D loss: 0.243757, acc.: 57.03%] [G loss: 0.467999]\n",
      "epoch:14 step:13823 [D loss: 0.216781, acc.: 67.97%] [G loss: 0.475047]\n",
      "epoch:14 step:13824 [D loss: 0.204877, acc.: 68.75%] [G loss: 0.441055]\n",
      "epoch:14 step:13825 [D loss: 0.219828, acc.: 64.06%] [G loss: 0.426475]\n",
      "epoch:14 step:13826 [D loss: 0.195996, acc.: 70.31%] [G loss: 0.425724]\n",
      "epoch:14 step:13827 [D loss: 0.173293, acc.: 75.00%] [G loss: 0.440611]\n",
      "epoch:14 step:13828 [D loss: 0.267845, acc.: 55.47%] [G loss: 0.397502]\n",
      "epoch:14 step:13829 [D loss: 0.220317, acc.: 63.28%] [G loss: 0.416230]\n",
      "epoch:14 step:13830 [D loss: 0.213894, acc.: 64.06%] [G loss: 0.466601]\n",
      "epoch:14 step:13831 [D loss: 0.218136, acc.: 65.62%] [G loss: 0.491971]\n",
      "epoch:14 step:13832 [D loss: 0.228472, acc.: 64.06%] [G loss: 0.472491]\n",
      "epoch:14 step:13833 [D loss: 0.244349, acc.: 58.59%] [G loss: 0.467995]\n",
      "epoch:14 step:13834 [D loss: 0.252323, acc.: 52.34%] [G loss: 0.433693]\n",
      "epoch:14 step:13835 [D loss: 0.238366, acc.: 63.28%] [G loss: 0.399709]\n",
      "epoch:14 step:13836 [D loss: 0.228905, acc.: 65.62%] [G loss: 0.423643]\n",
      "epoch:14 step:13837 [D loss: 0.224184, acc.: 60.16%] [G loss: 0.447248]\n",
      "epoch:14 step:13838 [D loss: 0.232149, acc.: 57.03%] [G loss: 0.479123]\n",
      "epoch:14 step:13839 [D loss: 0.231564, acc.: 60.16%] [G loss: 0.456553]\n",
      "epoch:14 step:13840 [D loss: 0.235472, acc.: 58.59%] [G loss: 0.429709]\n",
      "epoch:14 step:13841 [D loss: 0.207984, acc.: 63.28%] [G loss: 0.420469]\n",
      "epoch:14 step:13842 [D loss: 0.212290, acc.: 65.62%] [G loss: 0.459196]\n",
      "epoch:14 step:13843 [D loss: 0.206071, acc.: 64.06%] [G loss: 0.463770]\n",
      "epoch:14 step:13844 [D loss: 0.239363, acc.: 54.69%] [G loss: 0.440759]\n",
      "epoch:14 step:13845 [D loss: 0.219930, acc.: 59.38%] [G loss: 0.447646]\n",
      "epoch:14 step:13846 [D loss: 0.232275, acc.: 60.16%] [G loss: 0.424197]\n",
      "epoch:14 step:13847 [D loss: 0.220542, acc.: 64.84%] [G loss: 0.416422]\n",
      "epoch:14 step:13848 [D loss: 0.205493, acc.: 64.84%] [G loss: 0.446555]\n",
      "epoch:14 step:13849 [D loss: 0.217320, acc.: 62.50%] [G loss: 0.484419]\n",
      "epoch:14 step:13850 [D loss: 0.235399, acc.: 61.72%] [G loss: 0.448554]\n",
      "epoch:14 step:13851 [D loss: 0.215440, acc.: 69.53%] [G loss: 0.477465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13852 [D loss: 0.220399, acc.: 63.28%] [G loss: 0.462038]\n",
      "epoch:14 step:13853 [D loss: 0.236569, acc.: 57.81%] [G loss: 0.427463]\n",
      "epoch:14 step:13854 [D loss: 0.227740, acc.: 66.41%] [G loss: 0.406532]\n",
      "epoch:14 step:13855 [D loss: 0.214829, acc.: 67.19%] [G loss: 0.421747]\n",
      "epoch:14 step:13856 [D loss: 0.223868, acc.: 60.94%] [G loss: 0.476076]\n",
      "epoch:14 step:13857 [D loss: 0.245735, acc.: 62.50%] [G loss: 0.427701]\n",
      "epoch:14 step:13858 [D loss: 0.235379, acc.: 60.94%] [G loss: 0.439094]\n",
      "epoch:14 step:13859 [D loss: 0.268620, acc.: 54.69%] [G loss: 0.424517]\n",
      "epoch:14 step:13860 [D loss: 0.222093, acc.: 64.84%] [G loss: 0.434971]\n",
      "epoch:14 step:13861 [D loss: 0.206383, acc.: 68.75%] [G loss: 0.413377]\n",
      "epoch:14 step:13862 [D loss: 0.198405, acc.: 71.09%] [G loss: 0.444960]\n",
      "epoch:14 step:13863 [D loss: 0.227296, acc.: 59.38%] [G loss: 0.440601]\n",
      "epoch:14 step:13864 [D loss: 0.228721, acc.: 63.28%] [G loss: 0.436099]\n",
      "epoch:14 step:13865 [D loss: 0.209190, acc.: 67.19%] [G loss: 0.440104]\n",
      "epoch:14 step:13866 [D loss: 0.222235, acc.: 63.28%] [G loss: 0.454938]\n",
      "epoch:14 step:13867 [D loss: 0.243739, acc.: 61.72%] [G loss: 0.409862]\n",
      "epoch:14 step:13868 [D loss: 0.220609, acc.: 63.28%] [G loss: 0.420792]\n",
      "epoch:14 step:13869 [D loss: 0.214125, acc.: 66.41%] [G loss: 0.432173]\n",
      "epoch:14 step:13870 [D loss: 0.268301, acc.: 46.88%] [G loss: 0.409358]\n",
      "epoch:14 step:13871 [D loss: 0.228568, acc.: 64.06%] [G loss: 0.457884]\n",
      "epoch:14 step:13872 [D loss: 0.226401, acc.: 60.94%] [G loss: 0.418624]\n",
      "epoch:14 step:13873 [D loss: 0.225997, acc.: 61.72%] [G loss: 0.458923]\n",
      "epoch:14 step:13874 [D loss: 0.214187, acc.: 68.75%] [G loss: 0.459327]\n",
      "epoch:14 step:13875 [D loss: 0.231024, acc.: 57.03%] [G loss: 0.411862]\n",
      "epoch:14 step:13876 [D loss: 0.233874, acc.: 57.81%] [G loss: 0.388288]\n",
      "epoch:14 step:13877 [D loss: 0.247155, acc.: 55.47%] [G loss: 0.445142]\n",
      "epoch:14 step:13878 [D loss: 0.213909, acc.: 66.41%] [G loss: 0.433713]\n",
      "epoch:14 step:13879 [D loss: 0.207624, acc.: 67.97%] [G loss: 0.457622]\n",
      "epoch:14 step:13880 [D loss: 0.228233, acc.: 59.38%] [G loss: 0.428481]\n",
      "epoch:14 step:13881 [D loss: 0.218472, acc.: 67.97%] [G loss: 0.435253]\n",
      "epoch:14 step:13882 [D loss: 0.237510, acc.: 58.59%] [G loss: 0.436028]\n",
      "epoch:14 step:13883 [D loss: 0.254726, acc.: 60.16%] [G loss: 0.388409]\n",
      "epoch:14 step:13884 [D loss: 0.245416, acc.: 54.69%] [G loss: 0.445009]\n",
      "epoch:14 step:13885 [D loss: 0.202784, acc.: 69.53%] [G loss: 0.411724]\n",
      "epoch:14 step:13886 [D loss: 0.201081, acc.: 67.97%] [G loss: 0.488557]\n",
      "epoch:14 step:13887 [D loss: 0.216790, acc.: 63.28%] [G loss: 0.488774]\n",
      "epoch:14 step:13888 [D loss: 0.230722, acc.: 64.06%] [G loss: 0.461436]\n",
      "epoch:14 step:13889 [D loss: 0.212486, acc.: 66.41%] [G loss: 0.441548]\n",
      "epoch:14 step:13890 [D loss: 0.249803, acc.: 55.47%] [G loss: 0.440041]\n",
      "epoch:14 step:13891 [D loss: 0.217130, acc.: 67.19%] [G loss: 0.450867]\n",
      "epoch:14 step:13892 [D loss: 0.224773, acc.: 63.28%] [G loss: 0.449136]\n",
      "epoch:14 step:13893 [D loss: 0.199291, acc.: 71.09%] [G loss: 0.433813]\n",
      "epoch:14 step:13894 [D loss: 0.243323, acc.: 56.25%] [G loss: 0.452675]\n",
      "epoch:14 step:13895 [D loss: 0.225531, acc.: 63.28%] [G loss: 0.422029]\n",
      "epoch:14 step:13896 [D loss: 0.211857, acc.: 66.41%] [G loss: 0.439611]\n",
      "epoch:14 step:13897 [D loss: 0.226565, acc.: 61.72%] [G loss: 0.453011]\n",
      "epoch:14 step:13898 [D loss: 0.231944, acc.: 61.72%] [G loss: 0.443414]\n",
      "epoch:14 step:13899 [D loss: 0.192566, acc.: 67.19%] [G loss: 0.436585]\n",
      "epoch:14 step:13900 [D loss: 0.192374, acc.: 70.31%] [G loss: 0.483614]\n",
      "epoch:14 step:13901 [D loss: 0.253564, acc.: 54.69%] [G loss: 0.462433]\n",
      "epoch:14 step:13902 [D loss: 0.285289, acc.: 46.88%] [G loss: 0.405047]\n",
      "epoch:14 step:13903 [D loss: 0.222988, acc.: 60.94%] [G loss: 0.447013]\n",
      "epoch:14 step:13904 [D loss: 0.201491, acc.: 67.19%] [G loss: 0.463751]\n",
      "epoch:14 step:13905 [D loss: 0.259389, acc.: 55.47%] [G loss: 0.425378]\n",
      "epoch:14 step:13906 [D loss: 0.260715, acc.: 46.88%] [G loss: 0.416026]\n",
      "epoch:14 step:13907 [D loss: 0.234760, acc.: 59.38%] [G loss: 0.416026]\n",
      "epoch:14 step:13908 [D loss: 0.228414, acc.: 63.28%] [G loss: 0.438795]\n",
      "epoch:14 step:13909 [D loss: 0.268550, acc.: 53.91%] [G loss: 0.406899]\n",
      "epoch:14 step:13910 [D loss: 0.178455, acc.: 78.12%] [G loss: 0.443370]\n",
      "epoch:14 step:13911 [D loss: 0.215392, acc.: 60.94%] [G loss: 0.467512]\n",
      "epoch:14 step:13912 [D loss: 0.269711, acc.: 50.78%] [G loss: 0.467852]\n",
      "epoch:14 step:13913 [D loss: 0.252797, acc.: 57.81%] [G loss: 0.489589]\n",
      "epoch:14 step:13914 [D loss: 0.211462, acc.: 67.19%] [G loss: 0.501070]\n",
      "epoch:14 step:13915 [D loss: 0.253932, acc.: 57.81%] [G loss: 0.467227]\n",
      "epoch:14 step:13916 [D loss: 0.224512, acc.: 59.38%] [G loss: 0.437825]\n",
      "epoch:14 step:13917 [D loss: 0.240145, acc.: 60.16%] [G loss: 0.406690]\n",
      "epoch:14 step:13918 [D loss: 0.246178, acc.: 54.69%] [G loss: 0.444231]\n",
      "epoch:14 step:13919 [D loss: 0.225887, acc.: 67.97%] [G loss: 0.458774]\n",
      "epoch:14 step:13920 [D loss: 0.183851, acc.: 72.66%] [G loss: 0.461196]\n",
      "epoch:14 step:13921 [D loss: 0.225399, acc.: 60.94%] [G loss: 0.450534]\n",
      "epoch:14 step:13922 [D loss: 0.234842, acc.: 55.47%] [G loss: 0.433455]\n",
      "epoch:14 step:13923 [D loss: 0.232520, acc.: 64.06%] [G loss: 0.461989]\n",
      "epoch:14 step:13924 [D loss: 0.226963, acc.: 60.16%] [G loss: 0.433386]\n",
      "epoch:14 step:13925 [D loss: 0.228425, acc.: 64.84%] [G loss: 0.429424]\n",
      "epoch:14 step:13926 [D loss: 0.206197, acc.: 68.75%] [G loss: 0.462882]\n",
      "epoch:14 step:13927 [D loss: 0.229317, acc.: 57.03%] [G loss: 0.403157]\n",
      "epoch:14 step:13928 [D loss: 0.229889, acc.: 58.59%] [G loss: 0.419897]\n",
      "epoch:14 step:13929 [D loss: 0.231496, acc.: 60.94%] [G loss: 0.428617]\n",
      "epoch:14 step:13930 [D loss: 0.242930, acc.: 60.94%] [G loss: 0.425272]\n",
      "epoch:14 step:13931 [D loss: 0.236334, acc.: 61.72%] [G loss: 0.423298]\n",
      "epoch:14 step:13932 [D loss: 0.228022, acc.: 61.72%] [G loss: 0.418740]\n",
      "epoch:14 step:13933 [D loss: 0.224090, acc.: 67.19%] [G loss: 0.474466]\n",
      "epoch:14 step:13934 [D loss: 0.205812, acc.: 67.97%] [G loss: 0.470635]\n",
      "epoch:14 step:13935 [D loss: 0.268099, acc.: 59.38%] [G loss: 0.412535]\n",
      "epoch:14 step:13936 [D loss: 0.255076, acc.: 51.56%] [G loss: 0.421523]\n",
      "epoch:14 step:13937 [D loss: 0.235075, acc.: 58.59%] [G loss: 0.430124]\n",
      "epoch:14 step:13938 [D loss: 0.278730, acc.: 47.66%] [G loss: 0.399836]\n",
      "epoch:14 step:13939 [D loss: 0.230963, acc.: 60.94%] [G loss: 0.401255]\n",
      "epoch:14 step:13940 [D loss: 0.225638, acc.: 60.16%] [G loss: 0.413032]\n",
      "epoch:14 step:13941 [D loss: 0.207359, acc.: 69.53%] [G loss: 0.442469]\n",
      "epoch:14 step:13942 [D loss: 0.224237, acc.: 62.50%] [G loss: 0.443868]\n",
      "epoch:14 step:13943 [D loss: 0.224891, acc.: 64.84%] [G loss: 0.405829]\n",
      "epoch:14 step:13944 [D loss: 0.228878, acc.: 60.94%] [G loss: 0.470569]\n",
      "epoch:14 step:13945 [D loss: 0.256803, acc.: 57.03%] [G loss: 0.432373]\n",
      "epoch:14 step:13946 [D loss: 0.261721, acc.: 54.69%] [G loss: 0.434159]\n",
      "epoch:14 step:13947 [D loss: 0.222239, acc.: 65.62%] [G loss: 0.427667]\n",
      "epoch:14 step:13948 [D loss: 0.229600, acc.: 64.84%] [G loss: 0.411925]\n",
      "epoch:14 step:13949 [D loss: 0.246142, acc.: 59.38%] [G loss: 0.396987]\n",
      "epoch:14 step:13950 [D loss: 0.220897, acc.: 62.50%] [G loss: 0.409556]\n",
      "epoch:14 step:13951 [D loss: 0.197996, acc.: 71.09%] [G loss: 0.423986]\n",
      "epoch:14 step:13952 [D loss: 0.225364, acc.: 59.38%] [G loss: 0.401472]\n",
      "epoch:14 step:13953 [D loss: 0.221128, acc.: 64.06%] [G loss: 0.438491]\n",
      "epoch:14 step:13954 [D loss: 0.224027, acc.: 60.16%] [G loss: 0.390977]\n",
      "epoch:14 step:13955 [D loss: 0.217044, acc.: 67.97%] [G loss: 0.410569]\n",
      "epoch:14 step:13956 [D loss: 0.190111, acc.: 73.44%] [G loss: 0.461568]\n",
      "epoch:14 step:13957 [D loss: 0.207864, acc.: 64.06%] [G loss: 0.442915]\n",
      "epoch:14 step:13958 [D loss: 0.218788, acc.: 66.41%] [G loss: 0.427398]\n",
      "epoch:14 step:13959 [D loss: 0.214404, acc.: 65.62%] [G loss: 0.416868]\n",
      "epoch:14 step:13960 [D loss: 0.202750, acc.: 70.31%] [G loss: 0.408775]\n",
      "epoch:14 step:13961 [D loss: 0.219724, acc.: 64.06%] [G loss: 0.444194]\n",
      "epoch:14 step:13962 [D loss: 0.215422, acc.: 64.84%] [G loss: 0.449782]\n",
      "epoch:14 step:13963 [D loss: 0.215636, acc.: 63.28%] [G loss: 0.435763]\n",
      "epoch:14 step:13964 [D loss: 0.232132, acc.: 60.16%] [G loss: 0.409136]\n",
      "epoch:14 step:13965 [D loss: 0.245821, acc.: 55.47%] [G loss: 0.425653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13966 [D loss: 0.237344, acc.: 60.94%] [G loss: 0.404581]\n",
      "epoch:14 step:13967 [D loss: 0.206794, acc.: 69.53%] [G loss: 0.474646]\n",
      "epoch:14 step:13968 [D loss: 0.268629, acc.: 57.81%] [G loss: 0.433076]\n",
      "epoch:14 step:13969 [D loss: 0.222707, acc.: 59.38%] [G loss: 0.445209]\n",
      "epoch:14 step:13970 [D loss: 0.234053, acc.: 63.28%] [G loss: 0.416558]\n",
      "epoch:14 step:13971 [D loss: 0.220753, acc.: 60.16%] [G loss: 0.463500]\n",
      "epoch:14 step:13972 [D loss: 0.216674, acc.: 63.28%] [G loss: 0.461741]\n",
      "epoch:14 step:13973 [D loss: 0.250073, acc.: 54.69%] [G loss: 0.366023]\n",
      "epoch:14 step:13974 [D loss: 0.262417, acc.: 59.38%] [G loss: 0.388237]\n",
      "epoch:14 step:13975 [D loss: 0.228569, acc.: 59.38%] [G loss: 0.418531]\n",
      "epoch:14 step:13976 [D loss: 0.284766, acc.: 44.53%] [G loss: 0.437393]\n",
      "epoch:14 step:13977 [D loss: 0.227756, acc.: 64.06%] [G loss: 0.428678]\n",
      "epoch:14 step:13978 [D loss: 0.185785, acc.: 73.44%] [G loss: 0.498937]\n",
      "epoch:14 step:13979 [D loss: 0.253629, acc.: 58.59%] [G loss: 0.423735]\n",
      "epoch:14 step:13980 [D loss: 0.264540, acc.: 52.34%] [G loss: 0.396549]\n",
      "epoch:14 step:13981 [D loss: 0.229075, acc.: 59.38%] [G loss: 0.415611]\n",
      "epoch:14 step:13982 [D loss: 0.229226, acc.: 63.28%] [G loss: 0.430976]\n",
      "epoch:14 step:13983 [D loss: 0.254436, acc.: 54.69%] [G loss: 0.389259]\n",
      "epoch:14 step:13984 [D loss: 0.213237, acc.: 66.41%] [G loss: 0.408607]\n",
      "epoch:14 step:13985 [D loss: 0.248210, acc.: 57.03%] [G loss: 0.426368]\n",
      "epoch:14 step:13986 [D loss: 0.239737, acc.: 58.59%] [G loss: 0.398083]\n",
      "epoch:14 step:13987 [D loss: 0.216810, acc.: 67.97%] [G loss: 0.451378]\n",
      "epoch:14 step:13988 [D loss: 0.240443, acc.: 60.16%] [G loss: 0.412929]\n",
      "epoch:14 step:13989 [D loss: 0.219348, acc.: 61.72%] [G loss: 0.423200]\n",
      "epoch:14 step:13990 [D loss: 0.231831, acc.: 62.50%] [G loss: 0.424442]\n",
      "epoch:14 step:13991 [D loss: 0.226170, acc.: 62.50%] [G loss: 0.422699]\n",
      "epoch:14 step:13992 [D loss: 0.239605, acc.: 56.25%] [G loss: 0.424296]\n",
      "epoch:14 step:13993 [D loss: 0.201611, acc.: 64.06%] [G loss: 0.449880]\n",
      "epoch:14 step:13994 [D loss: 0.259650, acc.: 56.25%] [G loss: 0.422908]\n",
      "epoch:14 step:13995 [D loss: 0.222697, acc.: 67.19%] [G loss: 0.435405]\n",
      "epoch:14 step:13996 [D loss: 0.235754, acc.: 59.38%] [G loss: 0.402238]\n",
      "epoch:14 step:13997 [D loss: 0.234582, acc.: 60.94%] [G loss: 0.398468]\n",
      "epoch:14 step:13998 [D loss: 0.250429, acc.: 52.34%] [G loss: 0.371935]\n",
      "epoch:14 step:13999 [D loss: 0.230937, acc.: 60.94%] [G loss: 0.402370]\n",
      "epoch:14 step:14000 [D loss: 0.245781, acc.: 56.25%] [G loss: 0.397061]\n",
      "##############\n",
      "[2.52954802 1.94742834 5.88098729 4.82334694 3.50359148 5.56277609\n",
      " 4.28713525 4.50999221 4.40319287 3.92202778]\n",
      "##########\n",
      "epoch:14 step:14001 [D loss: 0.236033, acc.: 63.28%] [G loss: 0.405287]\n",
      "epoch:14 step:14002 [D loss: 0.200453, acc.: 67.19%] [G loss: 0.437260]\n",
      "epoch:14 step:14003 [D loss: 0.216813, acc.: 68.75%] [G loss: 0.453061]\n",
      "epoch:14 step:14004 [D loss: 0.214259, acc.: 70.31%] [G loss: 0.534507]\n",
      "epoch:14 step:14005 [D loss: 0.235157, acc.: 60.94%] [G loss: 0.476699]\n",
      "epoch:14 step:14006 [D loss: 0.220914, acc.: 65.62%] [G loss: 0.423691]\n",
      "epoch:14 step:14007 [D loss: 0.211109, acc.: 71.09%] [G loss: 0.462991]\n",
      "epoch:14 step:14008 [D loss: 0.185978, acc.: 75.00%] [G loss: 0.477454]\n",
      "epoch:14 step:14009 [D loss: 0.259711, acc.: 57.81%] [G loss: 0.409689]\n",
      "epoch:14 step:14010 [D loss: 0.250840, acc.: 60.94%] [G loss: 0.438016]\n",
      "epoch:14 step:14011 [D loss: 0.217888, acc.: 64.06%] [G loss: 0.418206]\n",
      "epoch:14 step:14012 [D loss: 0.220650, acc.: 62.50%] [G loss: 0.437764]\n",
      "epoch:14 step:14013 [D loss: 0.201184, acc.: 65.62%] [G loss: 0.467932]\n",
      "epoch:14 step:14014 [D loss: 0.238344, acc.: 62.50%] [G loss: 0.460693]\n",
      "epoch:14 step:14015 [D loss: 0.193205, acc.: 72.66%] [G loss: 0.460142]\n",
      "epoch:14 step:14016 [D loss: 0.244221, acc.: 66.41%] [G loss: 0.440574]\n",
      "epoch:14 step:14017 [D loss: 0.190355, acc.: 66.41%] [G loss: 0.479974]\n",
      "epoch:14 step:14018 [D loss: 0.213117, acc.: 68.75%] [G loss: 0.482588]\n",
      "epoch:14 step:14019 [D loss: 0.226715, acc.: 60.16%] [G loss: 0.468818]\n",
      "epoch:14 step:14020 [D loss: 0.251465, acc.: 57.81%] [G loss: 0.470891]\n",
      "epoch:14 step:14021 [D loss: 0.240638, acc.: 65.62%] [G loss: 0.456479]\n",
      "epoch:14 step:14022 [D loss: 0.236463, acc.: 61.72%] [G loss: 0.439317]\n",
      "epoch:14 step:14023 [D loss: 0.210443, acc.: 69.53%] [G loss: 0.502104]\n",
      "epoch:14 step:14024 [D loss: 0.219258, acc.: 70.31%] [G loss: 0.507702]\n",
      "epoch:14 step:14025 [D loss: 0.242008, acc.: 58.59%] [G loss: 0.409435]\n",
      "epoch:14 step:14026 [D loss: 0.204442, acc.: 65.62%] [G loss: 0.450311]\n",
      "epoch:14 step:14027 [D loss: 0.213089, acc.: 69.53%] [G loss: 0.461921]\n",
      "epoch:14 step:14028 [D loss: 0.216785, acc.: 62.50%] [G loss: 0.444033]\n",
      "epoch:14 step:14029 [D loss: 0.212352, acc.: 67.19%] [G loss: 0.446583]\n",
      "epoch:14 step:14030 [D loss: 0.197411, acc.: 67.19%] [G loss: 0.477348]\n",
      "epoch:14 step:14031 [D loss: 0.261832, acc.: 55.47%] [G loss: 0.444744]\n",
      "epoch:14 step:14032 [D loss: 0.226708, acc.: 63.28%] [G loss: 0.482856]\n",
      "epoch:14 step:14033 [D loss: 0.285049, acc.: 52.34%] [G loss: 0.417567]\n",
      "epoch:14 step:14034 [D loss: 0.227074, acc.: 60.16%] [G loss: 0.434917]\n",
      "epoch:14 step:14035 [D loss: 0.226956, acc.: 64.06%] [G loss: 0.439320]\n",
      "epoch:14 step:14036 [D loss: 0.197881, acc.: 73.44%] [G loss: 0.464309]\n",
      "epoch:14 step:14037 [D loss: 0.196602, acc.: 75.00%] [G loss: 0.491654]\n",
      "epoch:14 step:14038 [D loss: 0.295203, acc.: 50.78%] [G loss: 0.439794]\n",
      "epoch:14 step:14039 [D loss: 0.190964, acc.: 69.53%] [G loss: 0.464268]\n",
      "epoch:14 step:14040 [D loss: 0.241793, acc.: 59.38%] [G loss: 0.430050]\n",
      "epoch:14 step:14041 [D loss: 0.193276, acc.: 67.97%] [G loss: 0.462323]\n",
      "epoch:14 step:14042 [D loss: 0.176954, acc.: 77.34%] [G loss: 0.484811]\n",
      "epoch:14 step:14043 [D loss: 0.168497, acc.: 72.66%] [G loss: 0.492870]\n",
      "epoch:14 step:14044 [D loss: 0.200794, acc.: 78.12%] [G loss: 0.496444]\n",
      "epoch:14 step:14045 [D loss: 0.201474, acc.: 67.97%] [G loss: 0.520900]\n",
      "epoch:14 step:14046 [D loss: 0.295153, acc.: 56.25%] [G loss: 0.566138]\n",
      "epoch:14 step:14047 [D loss: 0.201897, acc.: 65.62%] [G loss: 0.580051]\n",
      "epoch:14 step:14048 [D loss: 0.199568, acc.: 71.09%] [G loss: 0.507007]\n",
      "epoch:14 step:14049 [D loss: 0.260482, acc.: 53.91%] [G loss: 0.453218]\n",
      "epoch:14 step:14050 [D loss: 0.265727, acc.: 58.59%] [G loss: 0.454902]\n",
      "epoch:14 step:14051 [D loss: 0.212460, acc.: 66.41%] [G loss: 0.460029]\n",
      "epoch:14 step:14052 [D loss: 0.185151, acc.: 76.56%] [G loss: 0.463607]\n",
      "epoch:14 step:14053 [D loss: 0.202151, acc.: 71.09%] [G loss: 0.499444]\n",
      "epoch:14 step:14054 [D loss: 0.182257, acc.: 72.66%] [G loss: 0.601819]\n",
      "epoch:14 step:14055 [D loss: 0.218588, acc.: 67.97%] [G loss: 0.533733]\n",
      "epoch:15 step:14056 [D loss: 0.261490, acc.: 60.94%] [G loss: 0.490287]\n",
      "epoch:15 step:14057 [D loss: 0.252804, acc.: 58.59%] [G loss: 0.488332]\n",
      "epoch:15 step:14058 [D loss: 0.233164, acc.: 63.28%] [G loss: 0.441321]\n",
      "epoch:15 step:14059 [D loss: 0.232701, acc.: 60.16%] [G loss: 0.440867]\n",
      "epoch:15 step:14060 [D loss: 0.219779, acc.: 60.94%] [G loss: 0.443819]\n",
      "epoch:15 step:14061 [D loss: 0.212487, acc.: 64.84%] [G loss: 0.489005]\n",
      "epoch:15 step:14062 [D loss: 0.230448, acc.: 61.72%] [G loss: 0.436380]\n",
      "epoch:15 step:14063 [D loss: 0.223070, acc.: 64.84%] [G loss: 0.418222]\n",
      "epoch:15 step:14064 [D loss: 0.191409, acc.: 69.53%] [G loss: 0.472961]\n",
      "epoch:15 step:14065 [D loss: 0.211303, acc.: 66.41%] [G loss: 0.449931]\n",
      "epoch:15 step:14066 [D loss: 0.238052, acc.: 61.72%] [G loss: 0.415519]\n",
      "epoch:15 step:14067 [D loss: 0.228159, acc.: 61.72%] [G loss: 0.438103]\n",
      "epoch:15 step:14068 [D loss: 0.215510, acc.: 65.62%] [G loss: 0.471896]\n",
      "epoch:15 step:14069 [D loss: 0.215758, acc.: 66.41%] [G loss: 0.492703]\n",
      "epoch:15 step:14070 [D loss: 0.190130, acc.: 71.88%] [G loss: 0.479837]\n",
      "epoch:15 step:14071 [D loss: 0.178979, acc.: 74.22%] [G loss: 0.515885]\n",
      "epoch:15 step:14072 [D loss: 0.252670, acc.: 56.25%] [G loss: 0.467519]\n",
      "epoch:15 step:14073 [D loss: 0.238336, acc.: 57.03%] [G loss: 0.449332]\n",
      "epoch:15 step:14074 [D loss: 0.268157, acc.: 53.91%] [G loss: 0.440442]\n",
      "epoch:15 step:14075 [D loss: 0.240635, acc.: 55.47%] [G loss: 0.465107]\n",
      "epoch:15 step:14076 [D loss: 0.225417, acc.: 61.72%] [G loss: 0.443484]\n",
      "epoch:15 step:14077 [D loss: 0.208271, acc.: 69.53%] [G loss: 0.490396]\n",
      "epoch:15 step:14078 [D loss: 0.251887, acc.: 54.69%] [G loss: 0.427599]\n",
      "epoch:15 step:14079 [D loss: 0.219201, acc.: 64.06%] [G loss: 0.423307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14080 [D loss: 0.210207, acc.: 64.06%] [G loss: 0.411997]\n",
      "epoch:15 step:14081 [D loss: 0.230930, acc.: 60.16%] [G loss: 0.436582]\n",
      "epoch:15 step:14082 [D loss: 0.244778, acc.: 53.12%] [G loss: 0.413865]\n",
      "epoch:15 step:14083 [D loss: 0.217064, acc.: 66.41%] [G loss: 0.477220]\n",
      "epoch:15 step:14084 [D loss: 0.216929, acc.: 63.28%] [G loss: 0.456666]\n",
      "epoch:15 step:14085 [D loss: 0.224535, acc.: 60.94%] [G loss: 0.460510]\n",
      "epoch:15 step:14086 [D loss: 0.233787, acc.: 53.91%] [G loss: 0.450029]\n",
      "epoch:15 step:14087 [D loss: 0.247272, acc.: 58.59%] [G loss: 0.482015]\n",
      "epoch:15 step:14088 [D loss: 0.210128, acc.: 71.09%] [G loss: 0.429228]\n",
      "epoch:15 step:14089 [D loss: 0.218306, acc.: 61.72%] [G loss: 0.455422]\n",
      "epoch:15 step:14090 [D loss: 0.239961, acc.: 60.94%] [G loss: 0.414530]\n",
      "epoch:15 step:14091 [D loss: 0.215410, acc.: 66.41%] [G loss: 0.426363]\n",
      "epoch:15 step:14092 [D loss: 0.234983, acc.: 64.06%] [G loss: 0.410799]\n",
      "epoch:15 step:14093 [D loss: 0.241028, acc.: 59.38%] [G loss: 0.405764]\n",
      "epoch:15 step:14094 [D loss: 0.220624, acc.: 66.41%] [G loss: 0.428287]\n",
      "epoch:15 step:14095 [D loss: 0.204570, acc.: 68.75%] [G loss: 0.450917]\n",
      "epoch:15 step:14096 [D loss: 0.232435, acc.: 58.59%] [G loss: 0.406546]\n",
      "epoch:15 step:14097 [D loss: 0.213415, acc.: 67.19%] [G loss: 0.476063]\n",
      "epoch:15 step:14098 [D loss: 0.195263, acc.: 70.31%] [G loss: 0.480935]\n",
      "epoch:15 step:14099 [D loss: 0.274517, acc.: 50.00%] [G loss: 0.395993]\n",
      "epoch:15 step:14100 [D loss: 0.222794, acc.: 64.84%] [G loss: 0.467865]\n",
      "epoch:15 step:14101 [D loss: 0.237054, acc.: 61.72%] [G loss: 0.419124]\n",
      "epoch:15 step:14102 [D loss: 0.238991, acc.: 65.62%] [G loss: 0.404176]\n",
      "epoch:15 step:14103 [D loss: 0.226934, acc.: 66.41%] [G loss: 0.422062]\n",
      "epoch:15 step:14104 [D loss: 0.224982, acc.: 60.16%] [G loss: 0.465742]\n",
      "epoch:15 step:14105 [D loss: 0.211351, acc.: 66.41%] [G loss: 0.493978]\n",
      "epoch:15 step:14106 [D loss: 0.235628, acc.: 54.69%] [G loss: 0.448310]\n",
      "epoch:15 step:14107 [D loss: 0.228515, acc.: 62.50%] [G loss: 0.405983]\n",
      "epoch:15 step:14108 [D loss: 0.215394, acc.: 67.97%] [G loss: 0.435045]\n",
      "epoch:15 step:14109 [D loss: 0.211242, acc.: 66.41%] [G loss: 0.438524]\n",
      "epoch:15 step:14110 [D loss: 0.217579, acc.: 65.62%] [G loss: 0.469093]\n",
      "epoch:15 step:14111 [D loss: 0.232193, acc.: 60.16%] [G loss: 0.448050]\n",
      "epoch:15 step:14112 [D loss: 0.234249, acc.: 60.16%] [G loss: 0.439556]\n",
      "epoch:15 step:14113 [D loss: 0.216460, acc.: 65.62%] [G loss: 0.463658]\n",
      "epoch:15 step:14114 [D loss: 0.223490, acc.: 64.84%] [G loss: 0.461559]\n",
      "epoch:15 step:14115 [D loss: 0.238053, acc.: 62.50%] [G loss: 0.422782]\n",
      "epoch:15 step:14116 [D loss: 0.215968, acc.: 66.41%] [G loss: 0.454365]\n",
      "epoch:15 step:14117 [D loss: 0.242225, acc.: 57.81%] [G loss: 0.364497]\n",
      "epoch:15 step:14118 [D loss: 0.206918, acc.: 67.19%] [G loss: 0.423650]\n",
      "epoch:15 step:14119 [D loss: 0.215755, acc.: 67.19%] [G loss: 0.428636]\n",
      "epoch:15 step:14120 [D loss: 0.241467, acc.: 53.91%] [G loss: 0.435673]\n",
      "epoch:15 step:14121 [D loss: 0.234048, acc.: 63.28%] [G loss: 0.446240]\n",
      "epoch:15 step:14122 [D loss: 0.219937, acc.: 64.06%] [G loss: 0.452343]\n",
      "epoch:15 step:14123 [D loss: 0.219559, acc.: 64.84%] [G loss: 0.436413]\n",
      "epoch:15 step:14124 [D loss: 0.192635, acc.: 71.09%] [G loss: 0.435699]\n",
      "epoch:15 step:14125 [D loss: 0.200766, acc.: 74.22%] [G loss: 0.456144]\n",
      "epoch:15 step:14126 [D loss: 0.256610, acc.: 51.56%] [G loss: 0.469974]\n",
      "epoch:15 step:14127 [D loss: 0.239989, acc.: 59.38%] [G loss: 0.424300]\n",
      "epoch:15 step:14128 [D loss: 0.231604, acc.: 59.38%] [G loss: 0.408399]\n",
      "epoch:15 step:14129 [D loss: 0.194780, acc.: 71.09%] [G loss: 0.434176]\n",
      "epoch:15 step:14130 [D loss: 0.214717, acc.: 63.28%] [G loss: 0.449173]\n",
      "epoch:15 step:14131 [D loss: 0.202185, acc.: 66.41%] [G loss: 0.465038]\n",
      "epoch:15 step:14132 [D loss: 0.166383, acc.: 78.91%] [G loss: 0.493476]\n",
      "epoch:15 step:14133 [D loss: 0.307956, acc.: 51.56%] [G loss: 0.408567]\n",
      "epoch:15 step:14134 [D loss: 0.235765, acc.: 54.69%] [G loss: 0.403646]\n",
      "epoch:15 step:14135 [D loss: 0.226032, acc.: 64.84%] [G loss: 0.372961]\n",
      "epoch:15 step:14136 [D loss: 0.232508, acc.: 61.72%] [G loss: 0.415893]\n",
      "epoch:15 step:14137 [D loss: 0.219249, acc.: 66.41%] [G loss: 0.433192]\n",
      "epoch:15 step:14138 [D loss: 0.210211, acc.: 65.62%] [G loss: 0.454184]\n",
      "epoch:15 step:14139 [D loss: 0.211604, acc.: 66.41%] [G loss: 0.458736]\n",
      "epoch:15 step:14140 [D loss: 0.220542, acc.: 64.84%] [G loss: 0.506545]\n",
      "epoch:15 step:14141 [D loss: 0.216332, acc.: 64.06%] [G loss: 0.422228]\n",
      "epoch:15 step:14142 [D loss: 0.244748, acc.: 58.59%] [G loss: 0.427914]\n",
      "epoch:15 step:14143 [D loss: 0.228609, acc.: 66.41%] [G loss: 0.419420]\n",
      "epoch:15 step:14144 [D loss: 0.215676, acc.: 67.97%] [G loss: 0.426075]\n",
      "epoch:15 step:14145 [D loss: 0.237776, acc.: 57.03%] [G loss: 0.456932]\n",
      "epoch:15 step:14146 [D loss: 0.200266, acc.: 67.97%] [G loss: 0.467592]\n",
      "epoch:15 step:14147 [D loss: 0.219368, acc.: 65.62%] [G loss: 0.459293]\n",
      "epoch:15 step:14148 [D loss: 0.187490, acc.: 70.31%] [G loss: 0.488863]\n",
      "epoch:15 step:14149 [D loss: 0.241808, acc.: 60.16%] [G loss: 0.477075]\n",
      "epoch:15 step:14150 [D loss: 0.198765, acc.: 68.75%] [G loss: 0.488519]\n",
      "epoch:15 step:14151 [D loss: 0.214523, acc.: 64.06%] [G loss: 0.467076]\n",
      "epoch:15 step:14152 [D loss: 0.205891, acc.: 67.19%] [G loss: 0.517971]\n",
      "epoch:15 step:14153 [D loss: 0.207375, acc.: 67.19%] [G loss: 0.494395]\n",
      "epoch:15 step:14154 [D loss: 0.218515, acc.: 65.62%] [G loss: 0.398504]\n",
      "epoch:15 step:14155 [D loss: 0.210571, acc.: 66.41%] [G loss: 0.468654]\n",
      "epoch:15 step:14156 [D loss: 0.209532, acc.: 71.09%] [G loss: 0.479615]\n",
      "epoch:15 step:14157 [D loss: 0.262914, acc.: 50.78%] [G loss: 0.428608]\n",
      "epoch:15 step:14158 [D loss: 0.222784, acc.: 62.50%] [G loss: 0.397014]\n",
      "epoch:15 step:14159 [D loss: 0.229906, acc.: 63.28%] [G loss: 0.395181]\n",
      "epoch:15 step:14160 [D loss: 0.259178, acc.: 53.12%] [G loss: 0.376009]\n",
      "epoch:15 step:14161 [D loss: 0.238321, acc.: 61.72%] [G loss: 0.400732]\n",
      "epoch:15 step:14162 [D loss: 0.212571, acc.: 62.50%] [G loss: 0.447237]\n",
      "epoch:15 step:14163 [D loss: 0.263325, acc.: 53.91%] [G loss: 0.473396]\n",
      "epoch:15 step:14164 [D loss: 0.249627, acc.: 57.03%] [G loss: 0.454658]\n",
      "epoch:15 step:14165 [D loss: 0.257262, acc.: 55.47%] [G loss: 0.408972]\n",
      "epoch:15 step:14166 [D loss: 0.209647, acc.: 69.53%] [G loss: 0.446735]\n",
      "epoch:15 step:14167 [D loss: 0.184810, acc.: 75.78%] [G loss: 0.498931]\n",
      "epoch:15 step:14168 [D loss: 0.228033, acc.: 63.28%] [G loss: 0.473560]\n",
      "epoch:15 step:14169 [D loss: 0.217446, acc.: 60.94%] [G loss: 0.461521]\n",
      "epoch:15 step:14170 [D loss: 0.217783, acc.: 63.28%] [G loss: 0.462667]\n",
      "epoch:15 step:14171 [D loss: 0.209662, acc.: 63.28%] [G loss: 0.457337]\n",
      "epoch:15 step:14172 [D loss: 0.221106, acc.: 61.72%] [G loss: 0.485884]\n",
      "epoch:15 step:14173 [D loss: 0.221046, acc.: 67.19%] [G loss: 0.465785]\n",
      "epoch:15 step:14174 [D loss: 0.190698, acc.: 72.66%] [G loss: 0.481272]\n",
      "epoch:15 step:14175 [D loss: 0.248649, acc.: 57.03%] [G loss: 0.475647]\n",
      "epoch:15 step:14176 [D loss: 0.233583, acc.: 58.59%] [G loss: 0.437154]\n",
      "epoch:15 step:14177 [D loss: 0.194041, acc.: 69.53%] [G loss: 0.460830]\n",
      "epoch:15 step:14178 [D loss: 0.225619, acc.: 60.16%] [G loss: 0.511105]\n",
      "epoch:15 step:14179 [D loss: 0.232421, acc.: 63.28%] [G loss: 0.508399]\n",
      "epoch:15 step:14180 [D loss: 0.229747, acc.: 62.50%] [G loss: 0.461341]\n",
      "epoch:15 step:14181 [D loss: 0.184432, acc.: 71.09%] [G loss: 0.488817]\n",
      "epoch:15 step:14182 [D loss: 0.235840, acc.: 54.69%] [G loss: 0.401664]\n",
      "epoch:15 step:14183 [D loss: 0.237065, acc.: 59.38%] [G loss: 0.451081]\n",
      "epoch:15 step:14184 [D loss: 0.236085, acc.: 57.03%] [G loss: 0.426020]\n",
      "epoch:15 step:14185 [D loss: 0.241027, acc.: 57.81%] [G loss: 0.401048]\n",
      "epoch:15 step:14186 [D loss: 0.220820, acc.: 63.28%] [G loss: 0.407096]\n",
      "epoch:15 step:14187 [D loss: 0.209335, acc.: 65.62%] [G loss: 0.436280]\n",
      "epoch:15 step:14188 [D loss: 0.225849, acc.: 61.72%] [G loss: 0.433062]\n",
      "epoch:15 step:14189 [D loss: 0.204787, acc.: 67.19%] [G loss: 0.474899]\n",
      "epoch:15 step:14190 [D loss: 0.210540, acc.: 67.97%] [G loss: 0.459572]\n",
      "epoch:15 step:14191 [D loss: 0.228241, acc.: 67.19%] [G loss: 0.449929]\n",
      "epoch:15 step:14192 [D loss: 0.269849, acc.: 53.12%] [G loss: 0.439599]\n",
      "epoch:15 step:14193 [D loss: 0.255510, acc.: 60.16%] [G loss: 0.401272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14194 [D loss: 0.230375, acc.: 62.50%] [G loss: 0.423325]\n",
      "epoch:15 step:14195 [D loss: 0.252024, acc.: 56.25%] [G loss: 0.430576]\n",
      "epoch:15 step:14196 [D loss: 0.214135, acc.: 68.75%] [G loss: 0.430112]\n",
      "epoch:15 step:14197 [D loss: 0.203545, acc.: 67.97%] [G loss: 0.441740]\n",
      "epoch:15 step:14198 [D loss: 0.239726, acc.: 60.94%] [G loss: 0.430033]\n",
      "epoch:15 step:14199 [D loss: 0.228523, acc.: 64.06%] [G loss: 0.450092]\n",
      "epoch:15 step:14200 [D loss: 0.228385, acc.: 62.50%] [G loss: 0.452165]\n",
      "##############\n",
      "[2.51492745 1.73410145 6.03388896 4.83699393 3.50645571 5.57228811\n",
      " 4.60290646 4.65745406 4.41456104 3.9863956 ]\n",
      "##########\n",
      "epoch:15 step:14201 [D loss: 0.222036, acc.: 60.16%] [G loss: 0.427345]\n",
      "epoch:15 step:14202 [D loss: 0.246745, acc.: 59.38%] [G loss: 0.440208]\n",
      "epoch:15 step:14203 [D loss: 0.261811, acc.: 56.25%] [G loss: 0.425927]\n",
      "epoch:15 step:14204 [D loss: 0.229598, acc.: 65.62%] [G loss: 0.410638]\n",
      "epoch:15 step:14205 [D loss: 0.220301, acc.: 63.28%] [G loss: 0.412107]\n",
      "epoch:15 step:14206 [D loss: 0.221379, acc.: 65.62%] [G loss: 0.444982]\n",
      "epoch:15 step:14207 [D loss: 0.232658, acc.: 59.38%] [G loss: 0.414450]\n",
      "epoch:15 step:14208 [D loss: 0.245069, acc.: 54.69%] [G loss: 0.412358]\n",
      "epoch:15 step:14209 [D loss: 0.217337, acc.: 62.50%] [G loss: 0.425820]\n",
      "epoch:15 step:14210 [D loss: 0.204369, acc.: 67.19%] [G loss: 0.438614]\n",
      "epoch:15 step:14211 [D loss: 0.216386, acc.: 67.19%] [G loss: 0.454155]\n",
      "epoch:15 step:14212 [D loss: 0.232495, acc.: 62.50%] [G loss: 0.419666]\n",
      "epoch:15 step:14213 [D loss: 0.225647, acc.: 67.19%] [G loss: 0.430985]\n",
      "epoch:15 step:14214 [D loss: 0.212176, acc.: 70.31%] [G loss: 0.449707]\n",
      "epoch:15 step:14215 [D loss: 0.243746, acc.: 58.59%] [G loss: 0.423092]\n",
      "epoch:15 step:14216 [D loss: 0.253059, acc.: 56.25%] [G loss: 0.422279]\n",
      "epoch:15 step:14217 [D loss: 0.194592, acc.: 71.09%] [G loss: 0.507785]\n",
      "epoch:15 step:14218 [D loss: 0.252873, acc.: 56.25%] [G loss: 0.444950]\n",
      "epoch:15 step:14219 [D loss: 0.209410, acc.: 70.31%] [G loss: 0.447731]\n",
      "epoch:15 step:14220 [D loss: 0.207870, acc.: 72.66%] [G loss: 0.422337]\n",
      "epoch:15 step:14221 [D loss: 0.213531, acc.: 62.50%] [G loss: 0.428786]\n",
      "epoch:15 step:14222 [D loss: 0.227180, acc.: 63.28%] [G loss: 0.439032]\n",
      "epoch:15 step:14223 [D loss: 0.215067, acc.: 64.84%] [G loss: 0.410163]\n",
      "epoch:15 step:14224 [D loss: 0.239103, acc.: 58.59%] [G loss: 0.428717]\n",
      "epoch:15 step:14225 [D loss: 0.255866, acc.: 49.22%] [G loss: 0.414530]\n",
      "epoch:15 step:14226 [D loss: 0.221404, acc.: 61.72%] [G loss: 0.431877]\n",
      "epoch:15 step:14227 [D loss: 0.239268, acc.: 57.81%] [G loss: 0.460422]\n",
      "epoch:15 step:14228 [D loss: 0.205484, acc.: 69.53%] [G loss: 0.433080]\n",
      "epoch:15 step:14229 [D loss: 0.250552, acc.: 53.91%] [G loss: 0.448682]\n",
      "epoch:15 step:14230 [D loss: 0.209795, acc.: 71.09%] [G loss: 0.455848]\n",
      "epoch:15 step:14231 [D loss: 0.212258, acc.: 66.41%] [G loss: 0.400455]\n",
      "epoch:15 step:14232 [D loss: 0.240278, acc.: 60.16%] [G loss: 0.417133]\n",
      "epoch:15 step:14233 [D loss: 0.231835, acc.: 60.16%] [G loss: 0.414632]\n",
      "epoch:15 step:14234 [D loss: 0.229559, acc.: 60.94%] [G loss: 0.400571]\n",
      "epoch:15 step:14235 [D loss: 0.254161, acc.: 50.78%] [G loss: 0.408485]\n",
      "epoch:15 step:14236 [D loss: 0.223957, acc.: 63.28%] [G loss: 0.416500]\n",
      "epoch:15 step:14237 [D loss: 0.250894, acc.: 59.38%] [G loss: 0.396250]\n",
      "epoch:15 step:14238 [D loss: 0.231547, acc.: 62.50%] [G loss: 0.438939]\n",
      "epoch:15 step:14239 [D loss: 0.215924, acc.: 67.19%] [G loss: 0.470296]\n",
      "epoch:15 step:14240 [D loss: 0.237737, acc.: 58.59%] [G loss: 0.409787]\n",
      "epoch:15 step:14241 [D loss: 0.256087, acc.: 60.16%] [G loss: 0.414643]\n",
      "epoch:15 step:14242 [D loss: 0.240496, acc.: 53.91%] [G loss: 0.427718]\n",
      "epoch:15 step:14243 [D loss: 0.224886, acc.: 60.16%] [G loss: 0.428076]\n",
      "epoch:15 step:14244 [D loss: 0.208897, acc.: 69.53%] [G loss: 0.431335]\n",
      "epoch:15 step:14245 [D loss: 0.229897, acc.: 59.38%] [G loss: 0.399242]\n",
      "epoch:15 step:14246 [D loss: 0.218113, acc.: 66.41%] [G loss: 0.420700]\n",
      "epoch:15 step:14247 [D loss: 0.213969, acc.: 64.06%] [G loss: 0.464881]\n",
      "epoch:15 step:14248 [D loss: 0.236808, acc.: 62.50%] [G loss: 0.398446]\n",
      "epoch:15 step:14249 [D loss: 0.197967, acc.: 71.09%] [G loss: 0.508220]\n",
      "epoch:15 step:14250 [D loss: 0.207651, acc.: 71.88%] [G loss: 0.483929]\n",
      "epoch:15 step:14251 [D loss: 0.239112, acc.: 62.50%] [G loss: 0.439785]\n",
      "epoch:15 step:14252 [D loss: 0.217721, acc.: 64.06%] [G loss: 0.443860]\n",
      "epoch:15 step:14253 [D loss: 0.193828, acc.: 73.44%] [G loss: 0.451605]\n",
      "epoch:15 step:14254 [D loss: 0.213849, acc.: 62.50%] [G loss: 0.467492]\n",
      "epoch:15 step:14255 [D loss: 0.263133, acc.: 52.34%] [G loss: 0.408473]\n",
      "epoch:15 step:14256 [D loss: 0.221637, acc.: 64.06%] [G loss: 0.447660]\n",
      "epoch:15 step:14257 [D loss: 0.228088, acc.: 62.50%] [G loss: 0.464729]\n",
      "epoch:15 step:14258 [D loss: 0.266940, acc.: 55.47%] [G loss: 0.426882]\n",
      "epoch:15 step:14259 [D loss: 0.211037, acc.: 68.75%] [G loss: 0.466593]\n",
      "epoch:15 step:14260 [D loss: 0.232630, acc.: 62.50%] [G loss: 0.416824]\n",
      "epoch:15 step:14261 [D loss: 0.192706, acc.: 70.31%] [G loss: 0.524807]\n",
      "epoch:15 step:14262 [D loss: 0.204687, acc.: 65.62%] [G loss: 0.467232]\n",
      "epoch:15 step:14263 [D loss: 0.174641, acc.: 71.09%] [G loss: 0.444754]\n",
      "epoch:15 step:14264 [D loss: 0.202740, acc.: 66.41%] [G loss: 0.459792]\n",
      "epoch:15 step:14265 [D loss: 0.237129, acc.: 60.94%] [G loss: 0.465360]\n",
      "epoch:15 step:14266 [D loss: 0.241153, acc.: 58.59%] [G loss: 0.395630]\n",
      "epoch:15 step:14267 [D loss: 0.232607, acc.: 60.16%] [G loss: 0.403318]\n",
      "epoch:15 step:14268 [D loss: 0.260605, acc.: 50.78%] [G loss: 0.383009]\n",
      "epoch:15 step:14269 [D loss: 0.229795, acc.: 56.25%] [G loss: 0.405937]\n",
      "epoch:15 step:14270 [D loss: 0.268536, acc.: 53.12%] [G loss: 0.401497]\n",
      "epoch:15 step:14271 [D loss: 0.212101, acc.: 64.84%] [G loss: 0.457053]\n",
      "epoch:15 step:14272 [D loss: 0.226030, acc.: 64.84%] [G loss: 0.416298]\n",
      "epoch:15 step:14273 [D loss: 0.204392, acc.: 72.66%] [G loss: 0.476886]\n",
      "epoch:15 step:14274 [D loss: 0.208136, acc.: 70.31%] [G loss: 0.463863]\n",
      "epoch:15 step:14275 [D loss: 0.282952, acc.: 50.00%] [G loss: 0.437586]\n",
      "epoch:15 step:14276 [D loss: 0.222909, acc.: 67.19%] [G loss: 0.442348]\n",
      "epoch:15 step:14277 [D loss: 0.217716, acc.: 67.19%] [G loss: 0.422041]\n",
      "epoch:15 step:14278 [D loss: 0.210564, acc.: 64.84%] [G loss: 0.505530]\n",
      "epoch:15 step:14279 [D loss: 0.232577, acc.: 62.50%] [G loss: 0.449691]\n",
      "epoch:15 step:14280 [D loss: 0.240572, acc.: 58.59%] [G loss: 0.424664]\n",
      "epoch:15 step:14281 [D loss: 0.221710, acc.: 61.72%] [G loss: 0.458553]\n",
      "epoch:15 step:14282 [D loss: 0.214719, acc.: 61.72%] [G loss: 0.441874]\n",
      "epoch:15 step:14283 [D loss: 0.249145, acc.: 56.25%] [G loss: 0.414729]\n",
      "epoch:15 step:14284 [D loss: 0.200824, acc.: 68.75%] [G loss: 0.431238]\n",
      "epoch:15 step:14285 [D loss: 0.197949, acc.: 68.75%] [G loss: 0.437659]\n",
      "epoch:15 step:14286 [D loss: 0.152901, acc.: 78.12%] [G loss: 0.512847]\n",
      "epoch:15 step:14287 [D loss: 0.175687, acc.: 75.78%] [G loss: 0.537446]\n",
      "epoch:15 step:14288 [D loss: 0.264394, acc.: 57.81%] [G loss: 0.482328]\n",
      "epoch:15 step:14289 [D loss: 0.260441, acc.: 54.69%] [G loss: 0.442436]\n",
      "epoch:15 step:14290 [D loss: 0.234790, acc.: 58.59%] [G loss: 0.418076]\n",
      "epoch:15 step:14291 [D loss: 0.192414, acc.: 71.09%] [G loss: 0.463607]\n",
      "epoch:15 step:14292 [D loss: 0.227531, acc.: 64.84%] [G loss: 0.413453]\n",
      "epoch:15 step:14293 [D loss: 0.229277, acc.: 62.50%] [G loss: 0.421344]\n",
      "epoch:15 step:14294 [D loss: 0.199641, acc.: 71.09%] [G loss: 0.495134]\n",
      "epoch:15 step:14295 [D loss: 0.220179, acc.: 61.72%] [G loss: 0.449254]\n",
      "epoch:15 step:14296 [D loss: 0.198373, acc.: 71.88%] [G loss: 0.450897]\n",
      "epoch:15 step:14297 [D loss: 0.196616, acc.: 74.22%] [G loss: 0.478602]\n",
      "epoch:15 step:14298 [D loss: 0.241560, acc.: 59.38%] [G loss: 0.421841]\n",
      "epoch:15 step:14299 [D loss: 0.203596, acc.: 70.31%] [G loss: 0.458518]\n",
      "epoch:15 step:14300 [D loss: 0.210952, acc.: 62.50%] [G loss: 0.456997]\n",
      "epoch:15 step:14301 [D loss: 0.207817, acc.: 64.84%] [G loss: 0.459354]\n",
      "epoch:15 step:14302 [D loss: 0.212315, acc.: 62.50%] [G loss: 0.417509]\n",
      "epoch:15 step:14303 [D loss: 0.209058, acc.: 67.97%] [G loss: 0.470564]\n",
      "epoch:15 step:14304 [D loss: 0.244307, acc.: 58.59%] [G loss: 0.471993]\n",
      "epoch:15 step:14305 [D loss: 0.260480, acc.: 54.69%] [G loss: 0.451893]\n",
      "epoch:15 step:14306 [D loss: 0.248552, acc.: 53.91%] [G loss: 0.409599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14307 [D loss: 0.236602, acc.: 62.50%] [G loss: 0.462617]\n",
      "epoch:15 step:14308 [D loss: 0.224818, acc.: 60.94%] [G loss: 0.422640]\n",
      "epoch:15 step:14309 [D loss: 0.230396, acc.: 59.38%] [G loss: 0.437247]\n",
      "epoch:15 step:14310 [D loss: 0.231526, acc.: 62.50%] [G loss: 0.427540]\n",
      "epoch:15 step:14311 [D loss: 0.229998, acc.: 64.06%] [G loss: 0.427138]\n",
      "epoch:15 step:14312 [D loss: 0.238223, acc.: 64.06%] [G loss: 0.430061]\n",
      "epoch:15 step:14313 [D loss: 0.203466, acc.: 67.19%] [G loss: 0.426860]\n",
      "epoch:15 step:14314 [D loss: 0.198066, acc.: 71.88%] [G loss: 0.447537]\n",
      "epoch:15 step:14315 [D loss: 0.221167, acc.: 60.94%] [G loss: 0.479113]\n",
      "epoch:15 step:14316 [D loss: 0.231891, acc.: 56.25%] [G loss: 0.425315]\n",
      "epoch:15 step:14317 [D loss: 0.212393, acc.: 61.72%] [G loss: 0.492793]\n",
      "epoch:15 step:14318 [D loss: 0.238621, acc.: 61.72%] [G loss: 0.441821]\n",
      "epoch:15 step:14319 [D loss: 0.235915, acc.: 60.16%] [G loss: 0.420425]\n",
      "epoch:15 step:14320 [D loss: 0.269805, acc.: 56.25%] [G loss: 0.412755]\n",
      "epoch:15 step:14321 [D loss: 0.239762, acc.: 60.16%] [G loss: 0.438835]\n",
      "epoch:15 step:14322 [D loss: 0.196426, acc.: 67.97%] [G loss: 0.453298]\n",
      "epoch:15 step:14323 [D loss: 0.219331, acc.: 63.28%] [G loss: 0.408598]\n",
      "epoch:15 step:14324 [D loss: 0.216348, acc.: 64.06%] [G loss: 0.437853]\n",
      "epoch:15 step:14325 [D loss: 0.206534, acc.: 65.62%] [G loss: 0.466443]\n",
      "epoch:15 step:14326 [D loss: 0.186487, acc.: 75.78%] [G loss: 0.480085]\n",
      "epoch:15 step:14327 [D loss: 0.233804, acc.: 59.38%] [G loss: 0.428446]\n",
      "epoch:15 step:14328 [D loss: 0.221292, acc.: 64.06%] [G loss: 0.421461]\n",
      "epoch:15 step:14329 [D loss: 0.197996, acc.: 71.09%] [G loss: 0.457536]\n",
      "epoch:15 step:14330 [D loss: 0.216684, acc.: 63.28%] [G loss: 0.467623]\n",
      "epoch:15 step:14331 [D loss: 0.214318, acc.: 69.53%] [G loss: 0.465499]\n",
      "epoch:15 step:14332 [D loss: 0.258634, acc.: 56.25%] [G loss: 0.402639]\n",
      "epoch:15 step:14333 [D loss: 0.239273, acc.: 53.91%] [G loss: 0.418546]\n",
      "epoch:15 step:14334 [D loss: 0.221597, acc.: 63.28%] [G loss: 0.430873]\n",
      "epoch:15 step:14335 [D loss: 0.197141, acc.: 68.75%] [G loss: 0.463754]\n",
      "epoch:15 step:14336 [D loss: 0.280942, acc.: 47.66%] [G loss: 0.414539]\n",
      "epoch:15 step:14337 [D loss: 0.253823, acc.: 53.12%] [G loss: 0.398498]\n",
      "epoch:15 step:14338 [D loss: 0.202707, acc.: 69.53%] [G loss: 0.443906]\n",
      "epoch:15 step:14339 [D loss: 0.225369, acc.: 63.28%] [G loss: 0.421180]\n",
      "epoch:15 step:14340 [D loss: 0.237902, acc.: 59.38%] [G loss: 0.444193]\n",
      "epoch:15 step:14341 [D loss: 0.183260, acc.: 75.00%] [G loss: 0.483771]\n",
      "epoch:15 step:14342 [D loss: 0.216199, acc.: 63.28%] [G loss: 0.443090]\n",
      "epoch:15 step:14343 [D loss: 0.208405, acc.: 67.19%] [G loss: 0.412535]\n",
      "epoch:15 step:14344 [D loss: 0.195269, acc.: 72.66%] [G loss: 0.424624]\n",
      "epoch:15 step:14345 [D loss: 0.224001, acc.: 59.38%] [G loss: 0.464752]\n",
      "epoch:15 step:14346 [D loss: 0.251659, acc.: 57.81%] [G loss: 0.422987]\n",
      "epoch:15 step:14347 [D loss: 0.230002, acc.: 57.81%] [G loss: 0.397599]\n",
      "epoch:15 step:14348 [D loss: 0.238183, acc.: 60.94%] [G loss: 0.427503]\n",
      "epoch:15 step:14349 [D loss: 0.246037, acc.: 53.12%] [G loss: 0.438336]\n",
      "epoch:15 step:14350 [D loss: 0.224422, acc.: 60.94%] [G loss: 0.417097]\n",
      "epoch:15 step:14351 [D loss: 0.199475, acc.: 68.75%] [G loss: 0.438575]\n",
      "epoch:15 step:14352 [D loss: 0.231282, acc.: 58.59%] [G loss: 0.393547]\n",
      "epoch:15 step:14353 [D loss: 0.208619, acc.: 68.75%] [G loss: 0.448255]\n",
      "epoch:15 step:14354 [D loss: 0.199127, acc.: 71.88%] [G loss: 0.489860]\n",
      "epoch:15 step:14355 [D loss: 0.238721, acc.: 59.38%] [G loss: 0.417335]\n",
      "epoch:15 step:14356 [D loss: 0.245325, acc.: 62.50%] [G loss: 0.410872]\n",
      "epoch:15 step:14357 [D loss: 0.221840, acc.: 64.06%] [G loss: 0.417428]\n",
      "epoch:15 step:14358 [D loss: 0.207753, acc.: 69.53%] [G loss: 0.471927]\n",
      "epoch:15 step:14359 [D loss: 0.228488, acc.: 64.84%] [G loss: 0.425418]\n",
      "epoch:15 step:14360 [D loss: 0.207858, acc.: 60.94%] [G loss: 0.447004]\n",
      "epoch:15 step:14361 [D loss: 0.212879, acc.: 61.72%] [G loss: 0.461788]\n",
      "epoch:15 step:14362 [D loss: 0.232741, acc.: 60.94%] [G loss: 0.474899]\n",
      "epoch:15 step:14363 [D loss: 0.235169, acc.: 59.38%] [G loss: 0.426592]\n",
      "epoch:15 step:14364 [D loss: 0.207914, acc.: 67.19%] [G loss: 0.476227]\n",
      "epoch:15 step:14365 [D loss: 0.239124, acc.: 57.03%] [G loss: 0.437724]\n",
      "epoch:15 step:14366 [D loss: 0.220099, acc.: 63.28%] [G loss: 0.412134]\n",
      "epoch:15 step:14367 [D loss: 0.160503, acc.: 77.34%] [G loss: 0.463221]\n",
      "epoch:15 step:14368 [D loss: 0.163030, acc.: 78.91%] [G loss: 0.540353]\n",
      "epoch:15 step:14369 [D loss: 0.186236, acc.: 70.31%] [G loss: 0.561567]\n",
      "epoch:15 step:14370 [D loss: 0.216109, acc.: 64.06%] [G loss: 0.476065]\n",
      "epoch:15 step:14371 [D loss: 0.273531, acc.: 52.34%] [G loss: 0.472907]\n",
      "epoch:15 step:14372 [D loss: 0.261532, acc.: 61.72%] [G loss: 0.372552]\n",
      "epoch:15 step:14373 [D loss: 0.238452, acc.: 56.25%] [G loss: 0.432912]\n",
      "epoch:15 step:14374 [D loss: 0.226466, acc.: 63.28%] [G loss: 0.430438]\n",
      "epoch:15 step:14375 [D loss: 0.204059, acc.: 69.53%] [G loss: 0.431494]\n",
      "epoch:15 step:14376 [D loss: 0.189764, acc.: 71.09%] [G loss: 0.473039]\n",
      "epoch:15 step:14377 [D loss: 0.213627, acc.: 63.28%] [G loss: 0.454887]\n",
      "epoch:15 step:14378 [D loss: 0.270042, acc.: 53.12%] [G loss: 0.451951]\n",
      "epoch:15 step:14379 [D loss: 0.246413, acc.: 57.03%] [G loss: 0.430015]\n",
      "epoch:15 step:14380 [D loss: 0.249246, acc.: 57.03%] [G loss: 0.423171]\n",
      "epoch:15 step:14381 [D loss: 0.210674, acc.: 65.62%] [G loss: 0.445017]\n",
      "epoch:15 step:14382 [D loss: 0.227714, acc.: 64.06%] [G loss: 0.428164]\n",
      "epoch:15 step:14383 [D loss: 0.213297, acc.: 63.28%] [G loss: 0.440249]\n",
      "epoch:15 step:14384 [D loss: 0.213802, acc.: 67.19%] [G loss: 0.458918]\n",
      "epoch:15 step:14385 [D loss: 0.200952, acc.: 66.41%] [G loss: 0.436311]\n",
      "epoch:15 step:14386 [D loss: 0.210498, acc.: 67.97%] [G loss: 0.441661]\n",
      "epoch:15 step:14387 [D loss: 0.203485, acc.: 67.19%] [G loss: 0.457747]\n",
      "epoch:15 step:14388 [D loss: 0.212907, acc.: 68.75%] [G loss: 0.449384]\n",
      "epoch:15 step:14389 [D loss: 0.212122, acc.: 67.97%] [G loss: 0.462722]\n",
      "epoch:15 step:14390 [D loss: 0.223485, acc.: 64.84%] [G loss: 0.451166]\n",
      "epoch:15 step:14391 [D loss: 0.230417, acc.: 64.06%] [G loss: 0.453191]\n",
      "epoch:15 step:14392 [D loss: 0.237089, acc.: 61.72%] [G loss: 0.493123]\n",
      "epoch:15 step:14393 [D loss: 0.211052, acc.: 67.19%] [G loss: 0.461289]\n",
      "epoch:15 step:14394 [D loss: 0.205734, acc.: 67.19%] [G loss: 0.465321]\n",
      "epoch:15 step:14395 [D loss: 0.219683, acc.: 63.28%] [G loss: 0.461005]\n",
      "epoch:15 step:14396 [D loss: 0.323376, acc.: 45.31%] [G loss: 0.397153]\n",
      "epoch:15 step:14397 [D loss: 0.249120, acc.: 57.03%] [G loss: 0.419216]\n",
      "epoch:15 step:14398 [D loss: 0.202574, acc.: 62.50%] [G loss: 0.447565]\n",
      "epoch:15 step:14399 [D loss: 0.207182, acc.: 70.31%] [G loss: 0.520275]\n",
      "epoch:15 step:14400 [D loss: 0.237372, acc.: 63.28%] [G loss: 0.447575]\n",
      "##############\n",
      "[2.49749883 1.66936303 5.90927233 4.83072804 3.66526178 5.26389784\n",
      " 4.46329282 4.92389941 4.34372988 3.97492674]\n",
      "##########\n",
      "epoch:15 step:14401 [D loss: 0.177981, acc.: 73.44%] [G loss: 0.515664]\n",
      "epoch:15 step:14402 [D loss: 0.196110, acc.: 71.88%] [G loss: 0.527019]\n",
      "epoch:15 step:14403 [D loss: 0.286236, acc.: 57.03%] [G loss: 0.454861]\n",
      "epoch:15 step:14404 [D loss: 0.275299, acc.: 50.78%] [G loss: 0.399663]\n",
      "epoch:15 step:14405 [D loss: 0.202634, acc.: 64.06%] [G loss: 0.468233]\n",
      "epoch:15 step:14406 [D loss: 0.233184, acc.: 60.16%] [G loss: 0.427385]\n",
      "epoch:15 step:14407 [D loss: 0.244542, acc.: 53.91%] [G loss: 0.410154]\n",
      "epoch:15 step:14408 [D loss: 0.204538, acc.: 66.41%] [G loss: 0.465779]\n",
      "epoch:15 step:14409 [D loss: 0.193013, acc.: 71.09%] [G loss: 0.459597]\n",
      "epoch:15 step:14410 [D loss: 0.240642, acc.: 63.28%] [G loss: 0.430687]\n",
      "epoch:15 step:14411 [D loss: 0.219694, acc.: 68.75%] [G loss: 0.422853]\n",
      "epoch:15 step:14412 [D loss: 0.200086, acc.: 69.53%] [G loss: 0.445952]\n",
      "epoch:15 step:14413 [D loss: 0.197753, acc.: 67.19%] [G loss: 0.457613]\n",
      "epoch:15 step:14414 [D loss: 0.210845, acc.: 68.75%] [G loss: 0.458310]\n",
      "epoch:15 step:14415 [D loss: 0.206507, acc.: 66.41%] [G loss: 0.453904]\n",
      "epoch:15 step:14416 [D loss: 0.199039, acc.: 69.53%] [G loss: 0.465560]\n",
      "epoch:15 step:14417 [D loss: 0.220519, acc.: 68.75%] [G loss: 0.442788]\n",
      "epoch:15 step:14418 [D loss: 0.219434, acc.: 63.28%] [G loss: 0.412997]\n",
      "epoch:15 step:14419 [D loss: 0.195208, acc.: 73.44%] [G loss: 0.434659]\n",
      "epoch:15 step:14420 [D loss: 0.218242, acc.: 64.06%] [G loss: 0.466933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14421 [D loss: 0.219082, acc.: 67.97%] [G loss: 0.453774]\n",
      "epoch:15 step:14422 [D loss: 0.235854, acc.: 54.69%] [G loss: 0.387366]\n",
      "epoch:15 step:14423 [D loss: 0.232630, acc.: 59.38%] [G loss: 0.433621]\n",
      "epoch:15 step:14424 [D loss: 0.209350, acc.: 64.84%] [G loss: 0.439880]\n",
      "epoch:15 step:14425 [D loss: 0.183586, acc.: 75.78%] [G loss: 0.461022]\n",
      "epoch:15 step:14426 [D loss: 0.207172, acc.: 68.75%] [G loss: 0.499241]\n",
      "epoch:15 step:14427 [D loss: 0.230167, acc.: 59.38%] [G loss: 0.469902]\n",
      "epoch:15 step:14428 [D loss: 0.260707, acc.: 56.25%] [G loss: 0.445998]\n",
      "epoch:15 step:14429 [D loss: 0.174224, acc.: 77.34%] [G loss: 0.453307]\n",
      "epoch:15 step:14430 [D loss: 0.242184, acc.: 53.91%] [G loss: 0.430891]\n",
      "epoch:15 step:14431 [D loss: 0.260323, acc.: 53.91%] [G loss: 0.433536]\n",
      "epoch:15 step:14432 [D loss: 0.261422, acc.: 53.12%] [G loss: 0.400804]\n",
      "epoch:15 step:14433 [D loss: 0.236755, acc.: 60.16%] [G loss: 0.421657]\n",
      "epoch:15 step:14434 [D loss: 0.221434, acc.: 64.84%] [G loss: 0.426313]\n",
      "epoch:15 step:14435 [D loss: 0.249572, acc.: 58.59%] [G loss: 0.442342]\n",
      "epoch:15 step:14436 [D loss: 0.217341, acc.: 61.72%] [G loss: 0.418417]\n",
      "epoch:15 step:14437 [D loss: 0.235125, acc.: 56.25%] [G loss: 0.443526]\n",
      "epoch:15 step:14438 [D loss: 0.215192, acc.: 69.53%] [G loss: 0.431211]\n",
      "epoch:15 step:14439 [D loss: 0.226832, acc.: 61.72%] [G loss: 0.431775]\n",
      "epoch:15 step:14440 [D loss: 0.188651, acc.: 68.75%] [G loss: 0.461775]\n",
      "epoch:15 step:14441 [D loss: 0.245810, acc.: 50.00%] [G loss: 0.443499]\n",
      "epoch:15 step:14442 [D loss: 0.218433, acc.: 64.84%] [G loss: 0.427747]\n",
      "epoch:15 step:14443 [D loss: 0.210553, acc.: 69.53%] [G loss: 0.416411]\n",
      "epoch:15 step:14444 [D loss: 0.222722, acc.: 64.84%] [G loss: 0.461433]\n",
      "epoch:15 step:14445 [D loss: 0.243693, acc.: 54.69%] [G loss: 0.422101]\n",
      "epoch:15 step:14446 [D loss: 0.215062, acc.: 67.19%] [G loss: 0.461095]\n",
      "epoch:15 step:14447 [D loss: 0.228278, acc.: 61.72%] [G loss: 0.427978]\n",
      "epoch:15 step:14448 [D loss: 0.224967, acc.: 64.06%] [G loss: 0.405902]\n",
      "epoch:15 step:14449 [D loss: 0.235219, acc.: 60.16%] [G loss: 0.416816]\n",
      "epoch:15 step:14450 [D loss: 0.202190, acc.: 67.97%] [G loss: 0.463176]\n",
      "epoch:15 step:14451 [D loss: 0.244388, acc.: 53.12%] [G loss: 0.425944]\n",
      "epoch:15 step:14452 [D loss: 0.233325, acc.: 64.06%] [G loss: 0.484662]\n",
      "epoch:15 step:14453 [D loss: 0.197631, acc.: 69.53%] [G loss: 0.476384]\n",
      "epoch:15 step:14454 [D loss: 0.184157, acc.: 71.88%] [G loss: 0.497226]\n",
      "epoch:15 step:14455 [D loss: 0.266753, acc.: 44.53%] [G loss: 0.405081]\n",
      "epoch:15 step:14456 [D loss: 0.260422, acc.: 53.91%] [G loss: 0.396738]\n",
      "epoch:15 step:14457 [D loss: 0.247635, acc.: 56.25%] [G loss: 0.389537]\n",
      "epoch:15 step:14458 [D loss: 0.245885, acc.: 60.16%] [G loss: 0.382841]\n",
      "epoch:15 step:14459 [D loss: 0.224017, acc.: 68.75%] [G loss: 0.417142]\n",
      "epoch:15 step:14460 [D loss: 0.209340, acc.: 69.53%] [G loss: 0.441063]\n",
      "epoch:15 step:14461 [D loss: 0.194713, acc.: 71.09%] [G loss: 0.494753]\n",
      "epoch:15 step:14462 [D loss: 0.256826, acc.: 53.91%] [G loss: 0.490597]\n",
      "epoch:15 step:14463 [D loss: 0.240820, acc.: 63.28%] [G loss: 0.482222]\n",
      "epoch:15 step:14464 [D loss: 0.236134, acc.: 54.69%] [G loss: 0.444512]\n",
      "epoch:15 step:14465 [D loss: 0.237383, acc.: 61.72%] [G loss: 0.431736]\n",
      "epoch:15 step:14466 [D loss: 0.236619, acc.: 60.16%] [G loss: 0.439949]\n",
      "epoch:15 step:14467 [D loss: 0.229448, acc.: 61.72%] [G loss: 0.427001]\n",
      "epoch:15 step:14468 [D loss: 0.247250, acc.: 57.03%] [G loss: 0.408775]\n",
      "epoch:15 step:14469 [D loss: 0.215085, acc.: 61.72%] [G loss: 0.400218]\n",
      "epoch:15 step:14470 [D loss: 0.222926, acc.: 64.84%] [G loss: 0.426508]\n",
      "epoch:15 step:14471 [D loss: 0.222673, acc.: 67.19%] [G loss: 0.481621]\n",
      "epoch:15 step:14472 [D loss: 0.237133, acc.: 58.59%] [G loss: 0.484687]\n",
      "epoch:15 step:14473 [D loss: 0.264437, acc.: 48.44%] [G loss: 0.377801]\n",
      "epoch:15 step:14474 [D loss: 0.260396, acc.: 57.03%] [G loss: 0.384333]\n",
      "epoch:15 step:14475 [D loss: 0.221171, acc.: 63.28%] [G loss: 0.430246]\n",
      "epoch:15 step:14476 [D loss: 0.247314, acc.: 60.16%] [G loss: 0.411015]\n",
      "epoch:15 step:14477 [D loss: 0.229328, acc.: 64.84%] [G loss: 0.414994]\n",
      "epoch:15 step:14478 [D loss: 0.222736, acc.: 66.41%] [G loss: 0.413721]\n",
      "epoch:15 step:14479 [D loss: 0.224381, acc.: 60.94%] [G loss: 0.409621]\n",
      "epoch:15 step:14480 [D loss: 0.207677, acc.: 62.50%] [G loss: 0.428101]\n",
      "epoch:15 step:14481 [D loss: 0.228112, acc.: 62.50%] [G loss: 0.441807]\n",
      "epoch:15 step:14482 [D loss: 0.196271, acc.: 72.66%] [G loss: 0.442899]\n",
      "epoch:15 step:14483 [D loss: 0.225068, acc.: 61.72%] [G loss: 0.419322]\n",
      "epoch:15 step:14484 [D loss: 0.205658, acc.: 72.66%] [G loss: 0.481293]\n",
      "epoch:15 step:14485 [D loss: 0.210819, acc.: 71.09%] [G loss: 0.496417]\n",
      "epoch:15 step:14486 [D loss: 0.243490, acc.: 60.16%] [G loss: 0.474119]\n",
      "epoch:15 step:14487 [D loss: 0.221191, acc.: 60.94%] [G loss: 0.477041]\n",
      "epoch:15 step:14488 [D loss: 0.211798, acc.: 62.50%] [G loss: 0.439963]\n",
      "epoch:15 step:14489 [D loss: 0.215824, acc.: 68.75%] [G loss: 0.437950]\n",
      "epoch:15 step:14490 [D loss: 0.193758, acc.: 70.31%] [G loss: 0.525022]\n",
      "epoch:15 step:14491 [D loss: 0.204289, acc.: 67.19%] [G loss: 0.489813]\n",
      "epoch:15 step:14492 [D loss: 0.266878, acc.: 56.25%] [G loss: 0.458485]\n",
      "epoch:15 step:14493 [D loss: 0.228296, acc.: 57.03%] [G loss: 0.444267]\n",
      "epoch:15 step:14494 [D loss: 0.227596, acc.: 66.41%] [G loss: 0.431653]\n",
      "epoch:15 step:14495 [D loss: 0.212206, acc.: 65.62%] [G loss: 0.444269]\n",
      "epoch:15 step:14496 [D loss: 0.213971, acc.: 64.84%] [G loss: 0.524204]\n",
      "epoch:15 step:14497 [D loss: 0.229166, acc.: 61.72%] [G loss: 0.432930]\n",
      "epoch:15 step:14498 [D loss: 0.233424, acc.: 61.72%] [G loss: 0.462158]\n",
      "epoch:15 step:14499 [D loss: 0.233282, acc.: 60.16%] [G loss: 0.459140]\n",
      "epoch:15 step:14500 [D loss: 0.229870, acc.: 64.84%] [G loss: 0.427255]\n",
      "epoch:15 step:14501 [D loss: 0.216788, acc.: 66.41%] [G loss: 0.475042]\n",
      "epoch:15 step:14502 [D loss: 0.206105, acc.: 73.44%] [G loss: 0.480233]\n",
      "epoch:15 step:14503 [D loss: 0.236777, acc.: 58.59%] [G loss: 0.417480]\n",
      "epoch:15 step:14504 [D loss: 0.226721, acc.: 64.06%] [G loss: 0.447038]\n",
      "epoch:15 step:14505 [D loss: 0.215844, acc.: 64.06%] [G loss: 0.449745]\n",
      "epoch:15 step:14506 [D loss: 0.201954, acc.: 68.75%] [G loss: 0.446898]\n",
      "epoch:15 step:14507 [D loss: 0.194914, acc.: 74.22%] [G loss: 0.481471]\n",
      "epoch:15 step:14508 [D loss: 0.201612, acc.: 67.19%] [G loss: 0.450647]\n",
      "epoch:15 step:14509 [D loss: 0.218073, acc.: 67.97%] [G loss: 0.450249]\n",
      "epoch:15 step:14510 [D loss: 0.228501, acc.: 62.50%] [G loss: 0.435125]\n",
      "epoch:15 step:14511 [D loss: 0.215524, acc.: 61.72%] [G loss: 0.476025]\n",
      "epoch:15 step:14512 [D loss: 0.220081, acc.: 63.28%] [G loss: 0.484121]\n",
      "epoch:15 step:14513 [D loss: 0.270193, acc.: 49.22%] [G loss: 0.429204]\n",
      "epoch:15 step:14514 [D loss: 0.245091, acc.: 60.94%] [G loss: 0.427390]\n",
      "epoch:15 step:14515 [D loss: 0.238582, acc.: 60.94%] [G loss: 0.438587]\n",
      "epoch:15 step:14516 [D loss: 0.245949, acc.: 59.38%] [G loss: 0.418978]\n",
      "epoch:15 step:14517 [D loss: 0.243790, acc.: 57.81%] [G loss: 0.401436]\n",
      "epoch:15 step:14518 [D loss: 0.224753, acc.: 64.06%] [G loss: 0.417228]\n",
      "epoch:15 step:14519 [D loss: 0.226020, acc.: 64.06%] [G loss: 0.406031]\n",
      "epoch:15 step:14520 [D loss: 0.251476, acc.: 53.91%] [G loss: 0.414442]\n",
      "epoch:15 step:14521 [D loss: 0.222911, acc.: 70.31%] [G loss: 0.430849]\n",
      "epoch:15 step:14522 [D loss: 0.228891, acc.: 60.94%] [G loss: 0.413576]\n",
      "epoch:15 step:14523 [D loss: 0.214346, acc.: 67.19%] [G loss: 0.487440]\n",
      "epoch:15 step:14524 [D loss: 0.197290, acc.: 71.88%] [G loss: 0.460076]\n",
      "epoch:15 step:14525 [D loss: 0.198536, acc.: 67.19%] [G loss: 0.474673]\n",
      "epoch:15 step:14526 [D loss: 0.204836, acc.: 67.19%] [G loss: 0.504286]\n",
      "epoch:15 step:14527 [D loss: 0.185697, acc.: 72.66%] [G loss: 0.505865]\n",
      "epoch:15 step:14528 [D loss: 0.243759, acc.: 58.59%] [G loss: 0.446286]\n",
      "epoch:15 step:14529 [D loss: 0.201346, acc.: 64.84%] [G loss: 0.439307]\n",
      "epoch:15 step:14530 [D loss: 0.221953, acc.: 64.84%] [G loss: 0.443712]\n",
      "epoch:15 step:14531 [D loss: 0.231287, acc.: 67.97%] [G loss: 0.441033]\n",
      "epoch:15 step:14532 [D loss: 0.286091, acc.: 47.66%] [G loss: 0.425727]\n",
      "epoch:15 step:14533 [D loss: 0.242583, acc.: 54.69%] [G loss: 0.389590]\n",
      "epoch:15 step:14534 [D loss: 0.217734, acc.: 65.62%] [G loss: 0.430216]\n",
      "epoch:15 step:14535 [D loss: 0.219869, acc.: 58.59%] [G loss: 0.430003]\n",
      "epoch:15 step:14536 [D loss: 0.183590, acc.: 74.22%] [G loss: 0.464773]\n",
      "epoch:15 step:14537 [D loss: 0.263677, acc.: 53.12%] [G loss: 0.388451]\n",
      "epoch:15 step:14538 [D loss: 0.212165, acc.: 67.19%] [G loss: 0.454709]\n",
      "epoch:15 step:14539 [D loss: 0.201168, acc.: 74.22%] [G loss: 0.467311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14540 [D loss: 0.226385, acc.: 62.50%] [G loss: 0.446145]\n",
      "epoch:15 step:14541 [D loss: 0.243538, acc.: 61.72%] [G loss: 0.447788]\n",
      "epoch:15 step:14542 [D loss: 0.228969, acc.: 59.38%] [G loss: 0.455646]\n",
      "epoch:15 step:14543 [D loss: 0.200760, acc.: 75.78%] [G loss: 0.477432]\n",
      "epoch:15 step:14544 [D loss: 0.287223, acc.: 50.78%] [G loss: 0.403719]\n",
      "epoch:15 step:14545 [D loss: 0.236772, acc.: 60.94%] [G loss: 0.418955]\n",
      "epoch:15 step:14546 [D loss: 0.234589, acc.: 57.81%] [G loss: 0.425608]\n",
      "epoch:15 step:14547 [D loss: 0.241364, acc.: 56.25%] [G loss: 0.456451]\n",
      "epoch:15 step:14548 [D loss: 0.223557, acc.: 63.28%] [G loss: 0.435213]\n",
      "epoch:15 step:14549 [D loss: 0.209398, acc.: 71.88%] [G loss: 0.454774]\n",
      "epoch:15 step:14550 [D loss: 0.201439, acc.: 71.09%] [G loss: 0.460636]\n",
      "epoch:15 step:14551 [D loss: 0.204752, acc.: 64.84%] [G loss: 0.488382]\n",
      "epoch:15 step:14552 [D loss: 0.218183, acc.: 63.28%] [G loss: 0.462245]\n",
      "epoch:15 step:14553 [D loss: 0.208069, acc.: 64.84%] [G loss: 0.490530]\n",
      "epoch:15 step:14554 [D loss: 0.192948, acc.: 73.44%] [G loss: 0.481628]\n",
      "epoch:15 step:14555 [D loss: 0.299382, acc.: 50.78%] [G loss: 0.420068]\n",
      "epoch:15 step:14556 [D loss: 0.307469, acc.: 42.19%] [G loss: 0.418111]\n",
      "epoch:15 step:14557 [D loss: 0.244748, acc.: 57.03%] [G loss: 0.426509]\n",
      "epoch:15 step:14558 [D loss: 0.231379, acc.: 63.28%] [G loss: 0.468536]\n",
      "epoch:15 step:14559 [D loss: 0.203668, acc.: 69.53%] [G loss: 0.516586]\n",
      "epoch:15 step:14560 [D loss: 0.197349, acc.: 75.00%] [G loss: 0.530085]\n",
      "epoch:15 step:14561 [D loss: 0.231240, acc.: 59.38%] [G loss: 0.439314]\n",
      "epoch:15 step:14562 [D loss: 0.237304, acc.: 61.72%] [G loss: 0.462366]\n",
      "epoch:15 step:14563 [D loss: 0.184753, acc.: 71.88%] [G loss: 0.499675]\n",
      "epoch:15 step:14564 [D loss: 0.255748, acc.: 57.03%] [G loss: 0.405442]\n",
      "epoch:15 step:14565 [D loss: 0.243014, acc.: 58.59%] [G loss: 0.399582]\n",
      "epoch:15 step:14566 [D loss: 0.252174, acc.: 57.81%] [G loss: 0.406105]\n",
      "epoch:15 step:14567 [D loss: 0.231141, acc.: 60.16%] [G loss: 0.456088]\n",
      "epoch:15 step:14568 [D loss: 0.206071, acc.: 62.50%] [G loss: 0.458933]\n",
      "epoch:15 step:14569 [D loss: 0.193521, acc.: 64.84%] [G loss: 0.433544]\n",
      "epoch:15 step:14570 [D loss: 0.214595, acc.: 67.97%] [G loss: 0.449583]\n",
      "epoch:15 step:14571 [D loss: 0.212042, acc.: 71.09%] [G loss: 0.465249]\n",
      "epoch:15 step:14572 [D loss: 0.268585, acc.: 54.69%] [G loss: 0.404494]\n",
      "epoch:15 step:14573 [D loss: 0.245011, acc.: 58.59%] [G loss: 0.450191]\n",
      "epoch:15 step:14574 [D loss: 0.205624, acc.: 65.62%] [G loss: 0.431797]\n",
      "epoch:15 step:14575 [D loss: 0.193804, acc.: 62.50%] [G loss: 0.429920]\n",
      "epoch:15 step:14576 [D loss: 0.203007, acc.: 66.41%] [G loss: 0.483896]\n",
      "epoch:15 step:14577 [D loss: 0.208431, acc.: 63.28%] [G loss: 0.443909]\n",
      "epoch:15 step:14578 [D loss: 0.203330, acc.: 65.62%] [G loss: 0.460022]\n",
      "epoch:15 step:14579 [D loss: 0.210842, acc.: 66.41%] [G loss: 0.437616]\n",
      "epoch:15 step:14580 [D loss: 0.222214, acc.: 64.84%] [G loss: 0.451672]\n",
      "epoch:15 step:14581 [D loss: 0.246573, acc.: 61.72%] [G loss: 0.420844]\n",
      "epoch:15 step:14582 [D loss: 0.221232, acc.: 62.50%] [G loss: 0.436341]\n",
      "epoch:15 step:14583 [D loss: 0.276522, acc.: 48.44%] [G loss: 0.409924]\n",
      "epoch:15 step:14584 [D loss: 0.249779, acc.: 60.16%] [G loss: 0.456153]\n",
      "epoch:15 step:14585 [D loss: 0.193444, acc.: 75.78%] [G loss: 0.489496]\n",
      "epoch:15 step:14586 [D loss: 0.238105, acc.: 61.72%] [G loss: 0.425974]\n",
      "epoch:15 step:14587 [D loss: 0.233670, acc.: 58.59%] [G loss: 0.422721]\n",
      "epoch:15 step:14588 [D loss: 0.223109, acc.: 63.28%] [G loss: 0.429346]\n",
      "epoch:15 step:14589 [D loss: 0.191399, acc.: 71.88%] [G loss: 0.418243]\n",
      "epoch:15 step:14590 [D loss: 0.234745, acc.: 58.59%] [G loss: 0.425295]\n",
      "epoch:15 step:14591 [D loss: 0.210311, acc.: 69.53%] [G loss: 0.451369]\n",
      "epoch:15 step:14592 [D loss: 0.244969, acc.: 61.72%] [G loss: 0.435074]\n",
      "epoch:15 step:14593 [D loss: 0.254067, acc.: 53.91%] [G loss: 0.428515]\n",
      "epoch:15 step:14594 [D loss: 0.228454, acc.: 63.28%] [G loss: 0.456876]\n",
      "epoch:15 step:14595 [D loss: 0.255689, acc.: 56.25%] [G loss: 0.422258]\n",
      "epoch:15 step:14596 [D loss: 0.215091, acc.: 64.84%] [G loss: 0.433637]\n",
      "epoch:15 step:14597 [D loss: 0.255081, acc.: 58.59%] [G loss: 0.415051]\n",
      "epoch:15 step:14598 [D loss: 0.234629, acc.: 60.16%] [G loss: 0.411326]\n",
      "epoch:15 step:14599 [D loss: 0.221798, acc.: 63.28%] [G loss: 0.457448]\n",
      "epoch:15 step:14600 [D loss: 0.230919, acc.: 58.59%] [G loss: 0.428783]\n",
      "##############\n",
      "[2.5300775  1.93405014 6.13022841 4.75584166 3.5146965  5.7093462\n",
      " 4.50892729 4.8575329  4.510041   3.86731379]\n",
      "##########\n",
      "epoch:15 step:14601 [D loss: 0.232554, acc.: 64.06%] [G loss: 0.449877]\n",
      "epoch:15 step:14602 [D loss: 0.247736, acc.: 59.38%] [G loss: 0.447199]\n",
      "epoch:15 step:14603 [D loss: 0.238910, acc.: 59.38%] [G loss: 0.445608]\n",
      "epoch:15 step:14604 [D loss: 0.218421, acc.: 64.06%] [G loss: 0.437236]\n",
      "epoch:15 step:14605 [D loss: 0.237910, acc.: 62.50%] [G loss: 0.424466]\n",
      "epoch:15 step:14606 [D loss: 0.205848, acc.: 70.31%] [G loss: 0.481069]\n",
      "epoch:15 step:14607 [D loss: 0.201277, acc.: 67.19%] [G loss: 0.461475]\n",
      "epoch:15 step:14608 [D loss: 0.230816, acc.: 61.72%] [G loss: 0.467113]\n",
      "epoch:15 step:14609 [D loss: 0.221455, acc.: 67.19%] [G loss: 0.426103]\n",
      "epoch:15 step:14610 [D loss: 0.203004, acc.: 71.09%] [G loss: 0.430100]\n",
      "epoch:15 step:14611 [D loss: 0.202000, acc.: 67.97%] [G loss: 0.416606]\n",
      "epoch:15 step:14612 [D loss: 0.204300, acc.: 68.75%] [G loss: 0.464349]\n",
      "epoch:15 step:14613 [D loss: 0.202977, acc.: 69.53%] [G loss: 0.488624]\n",
      "epoch:15 step:14614 [D loss: 0.259287, acc.: 51.56%] [G loss: 0.434312]\n",
      "epoch:15 step:14615 [D loss: 0.236644, acc.: 61.72%] [G loss: 0.465262]\n",
      "epoch:15 step:14616 [D loss: 0.222475, acc.: 64.06%] [G loss: 0.473719]\n",
      "epoch:15 step:14617 [D loss: 0.230899, acc.: 62.50%] [G loss: 0.445295]\n",
      "epoch:15 step:14618 [D loss: 0.201458, acc.: 67.19%] [G loss: 0.430467]\n",
      "epoch:15 step:14619 [D loss: 0.187042, acc.: 81.25%] [G loss: 0.463170]\n",
      "epoch:15 step:14620 [D loss: 0.245101, acc.: 57.81%] [G loss: 0.486934]\n",
      "epoch:15 step:14621 [D loss: 0.272037, acc.: 50.00%] [G loss: 0.470943]\n",
      "epoch:15 step:14622 [D loss: 0.217416, acc.: 62.50%] [G loss: 0.457462]\n",
      "epoch:15 step:14623 [D loss: 0.214964, acc.: 65.62%] [G loss: 0.450486]\n",
      "epoch:15 step:14624 [D loss: 0.261611, acc.: 58.59%] [G loss: 0.403779]\n",
      "epoch:15 step:14625 [D loss: 0.211539, acc.: 64.84%] [G loss: 0.396291]\n",
      "epoch:15 step:14626 [D loss: 0.205082, acc.: 66.41%] [G loss: 0.414814]\n",
      "epoch:15 step:14627 [D loss: 0.202287, acc.: 74.22%] [G loss: 0.446908]\n",
      "epoch:15 step:14628 [D loss: 0.229609, acc.: 61.72%] [G loss: 0.443831]\n",
      "epoch:15 step:14629 [D loss: 0.186681, acc.: 72.66%] [G loss: 0.474938]\n",
      "epoch:15 step:14630 [D loss: 0.209167, acc.: 71.09%] [G loss: 0.471666]\n",
      "epoch:15 step:14631 [D loss: 0.242644, acc.: 59.38%] [G loss: 0.431879]\n",
      "epoch:15 step:14632 [D loss: 0.218115, acc.: 61.72%] [G loss: 0.439358]\n",
      "epoch:15 step:14633 [D loss: 0.213990, acc.: 64.06%] [G loss: 0.451683]\n",
      "epoch:15 step:14634 [D loss: 0.213502, acc.: 66.41%] [G loss: 0.432359]\n",
      "epoch:15 step:14635 [D loss: 0.221633, acc.: 67.19%] [G loss: 0.452970]\n",
      "epoch:15 step:14636 [D loss: 0.218881, acc.: 66.41%] [G loss: 0.450616]\n",
      "epoch:15 step:14637 [D loss: 0.197624, acc.: 69.53%] [G loss: 0.457424]\n",
      "epoch:15 step:14638 [D loss: 0.217476, acc.: 67.19%] [G loss: 0.481584]\n",
      "epoch:15 step:14639 [D loss: 0.232639, acc.: 63.28%] [G loss: 0.445666]\n",
      "epoch:15 step:14640 [D loss: 0.242787, acc.: 58.59%] [G loss: 0.405148]\n",
      "epoch:15 step:14641 [D loss: 0.227169, acc.: 60.16%] [G loss: 0.437900]\n",
      "epoch:15 step:14642 [D loss: 0.247491, acc.: 62.50%] [G loss: 0.419030]\n",
      "epoch:15 step:14643 [D loss: 0.219038, acc.: 64.06%] [G loss: 0.437962]\n",
      "epoch:15 step:14644 [D loss: 0.212587, acc.: 66.41%] [G loss: 0.448587]\n",
      "epoch:15 step:14645 [D loss: 0.229807, acc.: 61.72%] [G loss: 0.433493]\n",
      "epoch:15 step:14646 [D loss: 0.226496, acc.: 64.84%] [G loss: 0.446427]\n",
      "epoch:15 step:14647 [D loss: 0.211158, acc.: 72.66%] [G loss: 0.428343]\n",
      "epoch:15 step:14648 [D loss: 0.204071, acc.: 64.84%] [G loss: 0.464670]\n",
      "epoch:15 step:14649 [D loss: 0.221096, acc.: 65.62%] [G loss: 0.432192]\n",
      "epoch:15 step:14650 [D loss: 0.199940, acc.: 66.41%] [G loss: 0.445312]\n",
      "epoch:15 step:14651 [D loss: 0.267911, acc.: 46.88%] [G loss: 0.407483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14652 [D loss: 0.235928, acc.: 60.94%] [G loss: 0.427803]\n",
      "epoch:15 step:14653 [D loss: 0.218780, acc.: 66.41%] [G loss: 0.463520]\n",
      "epoch:15 step:14654 [D loss: 0.228942, acc.: 62.50%] [G loss: 0.438697]\n",
      "epoch:15 step:14655 [D loss: 0.231559, acc.: 63.28%] [G loss: 0.419589]\n",
      "epoch:15 step:14656 [D loss: 0.227791, acc.: 60.94%] [G loss: 0.373427]\n",
      "epoch:15 step:14657 [D loss: 0.236362, acc.: 57.03%] [G loss: 0.421948]\n",
      "epoch:15 step:14658 [D loss: 0.218579, acc.: 62.50%] [G loss: 0.449841]\n",
      "epoch:15 step:14659 [D loss: 0.237230, acc.: 60.94%] [G loss: 0.417762]\n",
      "epoch:15 step:14660 [D loss: 0.204533, acc.: 72.66%] [G loss: 0.436912]\n",
      "epoch:15 step:14661 [D loss: 0.232168, acc.: 61.72%] [G loss: 0.418807]\n",
      "epoch:15 step:14662 [D loss: 0.206225, acc.: 65.62%] [G loss: 0.452138]\n",
      "epoch:15 step:14663 [D loss: 0.214099, acc.: 62.50%] [G loss: 0.424139]\n",
      "epoch:15 step:14664 [D loss: 0.221866, acc.: 64.84%] [G loss: 0.426961]\n",
      "epoch:15 step:14665 [D loss: 0.223631, acc.: 64.84%] [G loss: 0.401105]\n",
      "epoch:15 step:14666 [D loss: 0.210823, acc.: 68.75%] [G loss: 0.445834]\n",
      "epoch:15 step:14667 [D loss: 0.234987, acc.: 60.94%] [G loss: 0.449259]\n",
      "epoch:15 step:14668 [D loss: 0.206530, acc.: 67.19%] [G loss: 0.475865]\n",
      "epoch:15 step:14669 [D loss: 0.245713, acc.: 57.81%] [G loss: 0.422784]\n",
      "epoch:15 step:14670 [D loss: 0.250708, acc.: 54.69%] [G loss: 0.419725]\n",
      "epoch:15 step:14671 [D loss: 0.238114, acc.: 58.59%] [G loss: 0.413247]\n",
      "epoch:15 step:14672 [D loss: 0.217953, acc.: 65.62%] [G loss: 0.459180]\n",
      "epoch:15 step:14673 [D loss: 0.229970, acc.: 63.28%] [G loss: 0.463183]\n",
      "epoch:15 step:14674 [D loss: 0.253778, acc.: 53.91%] [G loss: 0.401464]\n",
      "epoch:15 step:14675 [D loss: 0.204165, acc.: 67.97%] [G loss: 0.421244]\n",
      "epoch:15 step:14676 [D loss: 0.237943, acc.: 58.59%] [G loss: 0.415093]\n",
      "epoch:15 step:14677 [D loss: 0.240412, acc.: 57.81%] [G loss: 0.416058]\n",
      "epoch:15 step:14678 [D loss: 0.212186, acc.: 63.28%] [G loss: 0.471049]\n",
      "epoch:15 step:14679 [D loss: 0.199979, acc.: 68.75%] [G loss: 0.460513]\n",
      "epoch:15 step:14680 [D loss: 0.235999, acc.: 57.81%] [G loss: 0.424696]\n",
      "epoch:15 step:14681 [D loss: 0.255026, acc.: 56.25%] [G loss: 0.409674]\n",
      "epoch:15 step:14682 [D loss: 0.216112, acc.: 66.41%] [G loss: 0.433672]\n",
      "epoch:15 step:14683 [D loss: 0.236800, acc.: 60.16%] [G loss: 0.449646]\n",
      "epoch:15 step:14684 [D loss: 0.217499, acc.: 67.19%] [G loss: 0.418062]\n",
      "epoch:15 step:14685 [D loss: 0.225070, acc.: 64.06%] [G loss: 0.468042]\n",
      "epoch:15 step:14686 [D loss: 0.187744, acc.: 72.66%] [G loss: 0.462877]\n",
      "epoch:15 step:14687 [D loss: 0.219012, acc.: 67.97%] [G loss: 0.452422]\n",
      "epoch:15 step:14688 [D loss: 0.198645, acc.: 71.88%] [G loss: 0.457032]\n",
      "epoch:15 step:14689 [D loss: 0.207469, acc.: 62.50%] [G loss: 0.440491]\n",
      "epoch:15 step:14690 [D loss: 0.215169, acc.: 64.06%] [G loss: 0.477957]\n",
      "epoch:15 step:14691 [D loss: 0.244014, acc.: 66.41%] [G loss: 0.464728]\n",
      "epoch:15 step:14692 [D loss: 0.226848, acc.: 64.06%] [G loss: 0.440786]\n",
      "epoch:15 step:14693 [D loss: 0.215919, acc.: 65.62%] [G loss: 0.422488]\n",
      "epoch:15 step:14694 [D loss: 0.239168, acc.: 56.25%] [G loss: 0.414878]\n",
      "epoch:15 step:14695 [D loss: 0.221478, acc.: 64.84%] [G loss: 0.491891]\n",
      "epoch:15 step:14696 [D loss: 0.189598, acc.: 72.66%] [G loss: 0.500434]\n",
      "epoch:15 step:14697 [D loss: 0.173076, acc.: 78.91%] [G loss: 0.548019]\n",
      "epoch:15 step:14698 [D loss: 0.235080, acc.: 59.38%] [G loss: 0.459781]\n",
      "epoch:15 step:14699 [D loss: 0.229803, acc.: 64.06%] [G loss: 0.446445]\n",
      "epoch:15 step:14700 [D loss: 0.238693, acc.: 53.91%] [G loss: 0.437597]\n",
      "epoch:15 step:14701 [D loss: 0.213365, acc.: 67.97%] [G loss: 0.435931]\n",
      "epoch:15 step:14702 [D loss: 0.200996, acc.: 68.75%] [G loss: 0.451157]\n",
      "epoch:15 step:14703 [D loss: 0.177590, acc.: 74.22%] [G loss: 0.517443]\n",
      "epoch:15 step:14704 [D loss: 0.220067, acc.: 63.28%] [G loss: 0.503848]\n",
      "epoch:15 step:14705 [D loss: 0.211988, acc.: 64.06%] [G loss: 0.476729]\n",
      "epoch:15 step:14706 [D loss: 0.212136, acc.: 63.28%] [G loss: 0.500346]\n",
      "epoch:15 step:14707 [D loss: 0.241721, acc.: 56.25%] [G loss: 0.429261]\n",
      "epoch:15 step:14708 [D loss: 0.220020, acc.: 68.75%] [G loss: 0.490517]\n",
      "epoch:15 step:14709 [D loss: 0.207904, acc.: 67.19%] [G loss: 0.468239]\n",
      "epoch:15 step:14710 [D loss: 0.234911, acc.: 58.59%] [G loss: 0.457777]\n",
      "epoch:15 step:14711 [D loss: 0.231691, acc.: 59.38%] [G loss: 0.446310]\n",
      "epoch:15 step:14712 [D loss: 0.219698, acc.: 66.41%] [G loss: 0.455395]\n",
      "epoch:15 step:14713 [D loss: 0.215891, acc.: 67.97%] [G loss: 0.467446]\n",
      "epoch:15 step:14714 [D loss: 0.206624, acc.: 64.06%] [G loss: 0.491124]\n",
      "epoch:15 step:14715 [D loss: 0.210823, acc.: 67.19%] [G loss: 0.446822]\n",
      "epoch:15 step:14716 [D loss: 0.196272, acc.: 72.66%] [G loss: 0.465614]\n",
      "epoch:15 step:14717 [D loss: 0.233312, acc.: 58.59%] [G loss: 0.441219]\n",
      "epoch:15 step:14718 [D loss: 0.233805, acc.: 64.06%] [G loss: 0.438290]\n",
      "epoch:15 step:14719 [D loss: 0.209793, acc.: 71.88%] [G loss: 0.447396]\n",
      "epoch:15 step:14720 [D loss: 0.230408, acc.: 60.94%] [G loss: 0.464438]\n",
      "epoch:15 step:14721 [D loss: 0.216693, acc.: 64.84%] [G loss: 0.436935]\n",
      "epoch:15 step:14722 [D loss: 0.241061, acc.: 68.75%] [G loss: 0.444887]\n",
      "epoch:15 step:14723 [D loss: 0.225418, acc.: 59.38%] [G loss: 0.467668]\n",
      "epoch:15 step:14724 [D loss: 0.213825, acc.: 65.62%] [G loss: 0.432722]\n",
      "epoch:15 step:14725 [D loss: 0.233580, acc.: 60.16%] [G loss: 0.413570]\n",
      "epoch:15 step:14726 [D loss: 0.222840, acc.: 66.41%] [G loss: 0.425424]\n",
      "epoch:15 step:14727 [D loss: 0.241908, acc.: 59.38%] [G loss: 0.413554]\n",
      "epoch:15 step:14728 [D loss: 0.231547, acc.: 60.16%] [G loss: 0.465243]\n",
      "epoch:15 step:14729 [D loss: 0.202197, acc.: 67.97%] [G loss: 0.498443]\n",
      "epoch:15 step:14730 [D loss: 0.236625, acc.: 64.06%] [G loss: 0.442150]\n",
      "epoch:15 step:14731 [D loss: 0.219970, acc.: 66.41%] [G loss: 0.417647]\n",
      "epoch:15 step:14732 [D loss: 0.215095, acc.: 65.62%] [G loss: 0.428493]\n",
      "epoch:15 step:14733 [D loss: 0.243858, acc.: 59.38%] [G loss: 0.432741]\n",
      "epoch:15 step:14734 [D loss: 0.236734, acc.: 63.28%] [G loss: 0.449549]\n",
      "epoch:15 step:14735 [D loss: 0.232559, acc.: 57.03%] [G loss: 0.439688]\n",
      "epoch:15 step:14736 [D loss: 0.216590, acc.: 62.50%] [G loss: 0.436296]\n",
      "epoch:15 step:14737 [D loss: 0.221520, acc.: 64.84%] [G loss: 0.415559]\n",
      "epoch:15 step:14738 [D loss: 0.236476, acc.: 56.25%] [G loss: 0.415954]\n",
      "epoch:15 step:14739 [D loss: 0.221156, acc.: 62.50%] [G loss: 0.424210]\n",
      "epoch:15 step:14740 [D loss: 0.227897, acc.: 61.72%] [G loss: 0.450020]\n",
      "epoch:15 step:14741 [D loss: 0.210071, acc.: 65.62%] [G loss: 0.451067]\n",
      "epoch:15 step:14742 [D loss: 0.235243, acc.: 60.94%] [G loss: 0.431700]\n",
      "epoch:15 step:14743 [D loss: 0.198548, acc.: 71.09%] [G loss: 0.467095]\n",
      "epoch:15 step:14744 [D loss: 0.196146, acc.: 73.44%] [G loss: 0.467779]\n",
      "epoch:15 step:14745 [D loss: 0.214483, acc.: 63.28%] [G loss: 0.430805]\n",
      "epoch:15 step:14746 [D loss: 0.196242, acc.: 69.53%] [G loss: 0.456809]\n",
      "epoch:15 step:14747 [D loss: 0.203469, acc.: 67.19%] [G loss: 0.439167]\n",
      "epoch:15 step:14748 [D loss: 0.200746, acc.: 64.06%] [G loss: 0.477933]\n",
      "epoch:15 step:14749 [D loss: 0.183409, acc.: 73.44%] [G loss: 0.456117]\n",
      "epoch:15 step:14750 [D loss: 0.214243, acc.: 67.19%] [G loss: 0.486997]\n",
      "epoch:15 step:14751 [D loss: 0.258507, acc.: 53.12%] [G loss: 0.448689]\n",
      "epoch:15 step:14752 [D loss: 0.227068, acc.: 63.28%] [G loss: 0.407231]\n",
      "epoch:15 step:14753 [D loss: 0.240294, acc.: 60.94%] [G loss: 0.451987]\n",
      "epoch:15 step:14754 [D loss: 0.214316, acc.: 64.84%] [G loss: 0.467629]\n",
      "epoch:15 step:14755 [D loss: 0.219798, acc.: 62.50%] [G loss: 0.490258]\n",
      "epoch:15 step:14756 [D loss: 0.208814, acc.: 67.19%] [G loss: 0.500764]\n",
      "epoch:15 step:14757 [D loss: 0.239078, acc.: 57.81%] [G loss: 0.467444]\n",
      "epoch:15 step:14758 [D loss: 0.243029, acc.: 61.72%] [G loss: 0.417207]\n",
      "epoch:15 step:14759 [D loss: 0.238579, acc.: 57.03%] [G loss: 0.412031]\n",
      "epoch:15 step:14760 [D loss: 0.226620, acc.: 64.84%] [G loss: 0.413870]\n",
      "epoch:15 step:14761 [D loss: 0.206724, acc.: 71.09%] [G loss: 0.446371]\n",
      "epoch:15 step:14762 [D loss: 0.231506, acc.: 62.50%] [G loss: 0.419640]\n",
      "epoch:15 step:14763 [D loss: 0.204689, acc.: 67.97%] [G loss: 0.457438]\n",
      "epoch:15 step:14764 [D loss: 0.191837, acc.: 71.09%] [G loss: 0.491467]\n",
      "epoch:15 step:14765 [D loss: 0.233408, acc.: 59.38%] [G loss: 0.460622]\n",
      "epoch:15 step:14766 [D loss: 0.231319, acc.: 61.72%] [G loss: 0.447100]\n",
      "epoch:15 step:14767 [D loss: 0.206780, acc.: 63.28%] [G loss: 0.440491]\n",
      "epoch:15 step:14768 [D loss: 0.208226, acc.: 69.53%] [G loss: 0.452746]\n",
      "epoch:15 step:14769 [D loss: 0.223830, acc.: 60.16%] [G loss: 0.444459]\n",
      "epoch:15 step:14770 [D loss: 0.217715, acc.: 60.16%] [G loss: 0.434859]\n",
      "epoch:15 step:14771 [D loss: 0.283799, acc.: 51.56%] [G loss: 0.378697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14772 [D loss: 0.220596, acc.: 64.84%] [G loss: 0.440146]\n",
      "epoch:15 step:14773 [D loss: 0.240475, acc.: 67.19%] [G loss: 0.414263]\n",
      "epoch:15 step:14774 [D loss: 0.210134, acc.: 62.50%] [G loss: 0.429252]\n",
      "epoch:15 step:14775 [D loss: 0.209799, acc.: 65.62%] [G loss: 0.493336]\n",
      "epoch:15 step:14776 [D loss: 0.222216, acc.: 67.19%] [G loss: 0.437134]\n",
      "epoch:15 step:14777 [D loss: 0.231386, acc.: 60.16%] [G loss: 0.438021]\n",
      "epoch:15 step:14778 [D loss: 0.248648, acc.: 60.16%] [G loss: 0.385733]\n",
      "epoch:15 step:14779 [D loss: 0.200008, acc.: 68.75%] [G loss: 0.423714]\n",
      "epoch:15 step:14780 [D loss: 0.211164, acc.: 71.09%] [G loss: 0.447230]\n",
      "epoch:15 step:14781 [D loss: 0.236652, acc.: 60.94%] [G loss: 0.453664]\n",
      "epoch:15 step:14782 [D loss: 0.255480, acc.: 55.47%] [G loss: 0.430579]\n",
      "epoch:15 step:14783 [D loss: 0.229673, acc.: 59.38%] [G loss: 0.437725]\n",
      "epoch:15 step:14784 [D loss: 0.221895, acc.: 64.06%] [G loss: 0.430950]\n",
      "epoch:15 step:14785 [D loss: 0.198438, acc.: 70.31%] [G loss: 0.450041]\n",
      "epoch:15 step:14786 [D loss: 0.226586, acc.: 60.16%] [G loss: 0.468625]\n",
      "epoch:15 step:14787 [D loss: 0.236436, acc.: 63.28%] [G loss: 0.477505]\n",
      "epoch:15 step:14788 [D loss: 0.202683, acc.: 72.66%] [G loss: 0.530628]\n",
      "epoch:15 step:14789 [D loss: 0.234423, acc.: 60.16%] [G loss: 0.481497]\n",
      "epoch:15 step:14790 [D loss: 0.252902, acc.: 53.91%] [G loss: 0.423818]\n",
      "epoch:15 step:14791 [D loss: 0.189341, acc.: 76.56%] [G loss: 0.468138]\n",
      "epoch:15 step:14792 [D loss: 0.226827, acc.: 64.84%] [G loss: 0.416067]\n",
      "epoch:15 step:14793 [D loss: 0.242038, acc.: 58.59%] [G loss: 0.456233]\n",
      "epoch:15 step:14794 [D loss: 0.265431, acc.: 53.91%] [G loss: 0.394492]\n",
      "epoch:15 step:14795 [D loss: 0.214668, acc.: 67.97%] [G loss: 0.423049]\n",
      "epoch:15 step:14796 [D loss: 0.262629, acc.: 55.47%] [G loss: 0.419188]\n",
      "epoch:15 step:14797 [D loss: 0.207674, acc.: 69.53%] [G loss: 0.471056]\n",
      "epoch:15 step:14798 [D loss: 0.218754, acc.: 65.62%] [G loss: 0.456958]\n",
      "epoch:15 step:14799 [D loss: 0.195679, acc.: 69.53%] [G loss: 0.412334]\n",
      "epoch:15 step:14800 [D loss: 0.250229, acc.: 51.56%] [G loss: 0.381786]\n",
      "##############\n",
      "[2.57941031 1.90697936 5.84235899 4.85373957 3.77384451 5.79336404\n",
      " 4.31666935 4.66034445 4.46307122 4.02112784]\n",
      "##########\n",
      "epoch:15 step:14801 [D loss: 0.226683, acc.: 64.84%] [G loss: 0.432670]\n",
      "epoch:15 step:14802 [D loss: 0.234492, acc.: 64.84%] [G loss: 0.462863]\n",
      "epoch:15 step:14803 [D loss: 0.236015, acc.: 60.16%] [G loss: 0.437710]\n",
      "epoch:15 step:14804 [D loss: 0.234408, acc.: 57.03%] [G loss: 0.410293]\n",
      "epoch:15 step:14805 [D loss: 0.191527, acc.: 70.31%] [G loss: 0.456815]\n",
      "epoch:15 step:14806 [D loss: 0.232830, acc.: 59.38%] [G loss: 0.447292]\n",
      "epoch:15 step:14807 [D loss: 0.233086, acc.: 62.50%] [G loss: 0.463762]\n",
      "epoch:15 step:14808 [D loss: 0.218316, acc.: 63.28%] [G loss: 0.463352]\n",
      "epoch:15 step:14809 [D loss: 0.203906, acc.: 69.53%] [G loss: 0.448421]\n",
      "epoch:15 step:14810 [D loss: 0.210202, acc.: 68.75%] [G loss: 0.466895]\n",
      "epoch:15 step:14811 [D loss: 0.234578, acc.: 60.94%] [G loss: 0.489996]\n",
      "epoch:15 step:14812 [D loss: 0.252486, acc.: 60.16%] [G loss: 0.395929]\n",
      "epoch:15 step:14813 [D loss: 0.258233, acc.: 64.84%] [G loss: 0.392834]\n",
      "epoch:15 step:14814 [D loss: 0.247964, acc.: 60.94%] [G loss: 0.466504]\n",
      "epoch:15 step:14815 [D loss: 0.228223, acc.: 59.38%] [G loss: 0.421524]\n",
      "epoch:15 step:14816 [D loss: 0.243045, acc.: 55.47%] [G loss: 0.427449]\n",
      "epoch:15 step:14817 [D loss: 0.221044, acc.: 62.50%] [G loss: 0.417344]\n",
      "epoch:15 step:14818 [D loss: 0.236589, acc.: 62.50%] [G loss: 0.433685]\n",
      "epoch:15 step:14819 [D loss: 0.203170, acc.: 64.06%] [G loss: 0.440138]\n",
      "epoch:15 step:14820 [D loss: 0.253530, acc.: 52.34%] [G loss: 0.402728]\n",
      "epoch:15 step:14821 [D loss: 0.239645, acc.: 62.50%] [G loss: 0.434237]\n",
      "epoch:15 step:14822 [D loss: 0.205708, acc.: 65.62%] [G loss: 0.465327]\n",
      "epoch:15 step:14823 [D loss: 0.263907, acc.: 53.91%] [G loss: 0.401539]\n",
      "epoch:15 step:14824 [D loss: 0.215885, acc.: 64.84%] [G loss: 0.471562]\n",
      "epoch:15 step:14825 [D loss: 0.217598, acc.: 61.72%] [G loss: 0.453022]\n",
      "epoch:15 step:14826 [D loss: 0.214237, acc.: 65.62%] [G loss: 0.432131]\n",
      "epoch:15 step:14827 [D loss: 0.237690, acc.: 56.25%] [G loss: 0.411382]\n",
      "epoch:15 step:14828 [D loss: 0.220821, acc.: 63.28%] [G loss: 0.466711]\n",
      "epoch:15 step:14829 [D loss: 0.201256, acc.: 67.19%] [G loss: 0.479130]\n",
      "epoch:15 step:14830 [D loss: 0.212318, acc.: 69.53%] [G loss: 0.491730]\n",
      "epoch:15 step:14831 [D loss: 0.224416, acc.: 62.50%] [G loss: 0.439188]\n",
      "epoch:15 step:14832 [D loss: 0.220097, acc.: 63.28%] [G loss: 0.415454]\n",
      "epoch:15 step:14833 [D loss: 0.232372, acc.: 59.38%] [G loss: 0.433478]\n",
      "epoch:15 step:14834 [D loss: 0.248519, acc.: 57.03%] [G loss: 0.415491]\n",
      "epoch:15 step:14835 [D loss: 0.208087, acc.: 61.72%] [G loss: 0.461671]\n",
      "epoch:15 step:14836 [D loss: 0.224652, acc.: 64.06%] [G loss: 0.499347]\n",
      "epoch:15 step:14837 [D loss: 0.177523, acc.: 72.66%] [G loss: 0.504666]\n",
      "epoch:15 step:14838 [D loss: 0.258299, acc.: 52.34%] [G loss: 0.469207]\n",
      "epoch:15 step:14839 [D loss: 0.274110, acc.: 48.44%] [G loss: 0.445602]\n",
      "epoch:15 step:14840 [D loss: 0.233350, acc.: 60.94%] [G loss: 0.433799]\n",
      "epoch:15 step:14841 [D loss: 0.212914, acc.: 60.94%] [G loss: 0.468725]\n",
      "epoch:15 step:14842 [D loss: 0.257640, acc.: 49.22%] [G loss: 0.429291]\n",
      "epoch:15 step:14843 [D loss: 0.250026, acc.: 57.03%] [G loss: 0.422543]\n",
      "epoch:15 step:14844 [D loss: 0.238315, acc.: 59.38%] [G loss: 0.397470]\n",
      "epoch:15 step:14845 [D loss: 0.216829, acc.: 69.53%] [G loss: 0.431754]\n",
      "epoch:15 step:14846 [D loss: 0.253328, acc.: 52.34%] [G loss: 0.408259]\n",
      "epoch:15 step:14847 [D loss: 0.201065, acc.: 71.09%] [G loss: 0.456470]\n",
      "epoch:15 step:14848 [D loss: 0.212347, acc.: 64.84%] [G loss: 0.482853]\n",
      "epoch:15 step:14849 [D loss: 0.260345, acc.: 51.56%] [G loss: 0.434278]\n",
      "epoch:15 step:14850 [D loss: 0.259730, acc.: 55.47%] [G loss: 0.438495]\n",
      "epoch:15 step:14851 [D loss: 0.203277, acc.: 71.88%] [G loss: 0.485327]\n",
      "epoch:15 step:14852 [D loss: 0.232322, acc.: 64.06%] [G loss: 0.414682]\n",
      "epoch:15 step:14853 [D loss: 0.230793, acc.: 62.50%] [G loss: 0.444531]\n",
      "epoch:15 step:14854 [D loss: 0.236499, acc.: 61.72%] [G loss: 0.416932]\n",
      "epoch:15 step:14855 [D loss: 0.281483, acc.: 50.78%] [G loss: 0.422662]\n",
      "epoch:15 step:14856 [D loss: 0.201776, acc.: 68.75%] [G loss: 0.479246]\n",
      "epoch:15 step:14857 [D loss: 0.206940, acc.: 67.97%] [G loss: 0.515533]\n",
      "epoch:15 step:14858 [D loss: 0.214010, acc.: 69.53%] [G loss: 0.464258]\n",
      "epoch:15 step:14859 [D loss: 0.269679, acc.: 53.12%] [G loss: 0.403930]\n",
      "epoch:15 step:14860 [D loss: 0.215347, acc.: 69.53%] [G loss: 0.447238]\n",
      "epoch:15 step:14861 [D loss: 0.227064, acc.: 62.50%] [G loss: 0.383579]\n",
      "epoch:15 step:14862 [D loss: 0.225149, acc.: 63.28%] [G loss: 0.411696]\n",
      "epoch:15 step:14863 [D loss: 0.230791, acc.: 60.16%] [G loss: 0.436950]\n",
      "epoch:15 step:14864 [D loss: 0.218061, acc.: 62.50%] [G loss: 0.420023]\n",
      "epoch:15 step:14865 [D loss: 0.236246, acc.: 60.94%] [G loss: 0.403208]\n",
      "epoch:15 step:14866 [D loss: 0.236271, acc.: 57.81%] [G loss: 0.441941]\n",
      "epoch:15 step:14867 [D loss: 0.242450, acc.: 57.03%] [G loss: 0.456626]\n",
      "epoch:15 step:14868 [D loss: 0.245519, acc.: 58.59%] [G loss: 0.413495]\n",
      "epoch:15 step:14869 [D loss: 0.236049, acc.: 57.03%] [G loss: 0.438202]\n",
      "epoch:15 step:14870 [D loss: 0.209731, acc.: 64.06%] [G loss: 0.476335]\n",
      "epoch:15 step:14871 [D loss: 0.212975, acc.: 60.94%] [G loss: 0.476636]\n",
      "epoch:15 step:14872 [D loss: 0.264286, acc.: 59.38%] [G loss: 0.437243]\n",
      "epoch:15 step:14873 [D loss: 0.243132, acc.: 56.25%] [G loss: 0.438555]\n",
      "epoch:15 step:14874 [D loss: 0.227605, acc.: 63.28%] [G loss: 0.448402]\n",
      "epoch:15 step:14875 [D loss: 0.274187, acc.: 49.22%] [G loss: 0.446933]\n",
      "epoch:15 step:14876 [D loss: 0.216523, acc.: 60.94%] [G loss: 0.403812]\n",
      "epoch:15 step:14877 [D loss: 0.225152, acc.: 63.28%] [G loss: 0.407835]\n",
      "epoch:15 step:14878 [D loss: 0.196709, acc.: 71.09%] [G loss: 0.438581]\n",
      "epoch:15 step:14879 [D loss: 0.242769, acc.: 59.38%] [G loss: 0.432547]\n",
      "epoch:15 step:14880 [D loss: 0.217891, acc.: 66.41%] [G loss: 0.475808]\n",
      "epoch:15 step:14881 [D loss: 0.226423, acc.: 57.03%] [G loss: 0.440380]\n",
      "epoch:15 step:14882 [D loss: 0.249510, acc.: 58.59%] [G loss: 0.413204]\n",
      "epoch:15 step:14883 [D loss: 0.270561, acc.: 46.88%] [G loss: 0.401763]\n",
      "epoch:15 step:14884 [D loss: 0.245699, acc.: 64.06%] [G loss: 0.437684]\n",
      "epoch:15 step:14885 [D loss: 0.209758, acc.: 67.19%] [G loss: 0.467768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14886 [D loss: 0.231904, acc.: 60.16%] [G loss: 0.435264]\n",
      "epoch:15 step:14887 [D loss: 0.223158, acc.: 59.38%] [G loss: 0.447792]\n",
      "epoch:15 step:14888 [D loss: 0.200008, acc.: 67.97%] [G loss: 0.425083]\n",
      "epoch:15 step:14889 [D loss: 0.247264, acc.: 55.47%] [G loss: 0.398058]\n",
      "epoch:15 step:14890 [D loss: 0.241042, acc.: 60.16%] [G loss: 0.395792]\n",
      "epoch:15 step:14891 [D loss: 0.238121, acc.: 59.38%] [G loss: 0.433475]\n",
      "epoch:15 step:14892 [D loss: 0.197201, acc.: 75.78%] [G loss: 0.441416]\n",
      "epoch:15 step:14893 [D loss: 0.200680, acc.: 68.75%] [G loss: 0.454651]\n",
      "epoch:15 step:14894 [D loss: 0.216722, acc.: 64.84%] [G loss: 0.414685]\n",
      "epoch:15 step:14895 [D loss: 0.217859, acc.: 64.06%] [G loss: 0.451195]\n",
      "epoch:15 step:14896 [D loss: 0.204618, acc.: 67.97%] [G loss: 0.410935]\n",
      "epoch:15 step:14897 [D loss: 0.183025, acc.: 75.78%] [G loss: 0.438774]\n",
      "epoch:15 step:14898 [D loss: 0.232258, acc.: 60.16%] [G loss: 0.440532]\n",
      "epoch:15 step:14899 [D loss: 0.217018, acc.: 66.41%] [G loss: 0.459395]\n",
      "epoch:15 step:14900 [D loss: 0.224703, acc.: 64.84%] [G loss: 0.449714]\n",
      "epoch:15 step:14901 [D loss: 0.247721, acc.: 53.12%] [G loss: 0.447358]\n",
      "epoch:15 step:14902 [D loss: 0.263506, acc.: 51.56%] [G loss: 0.432570]\n",
      "epoch:15 step:14903 [D loss: 0.243450, acc.: 52.34%] [G loss: 0.407099]\n",
      "epoch:15 step:14904 [D loss: 0.198025, acc.: 72.66%] [G loss: 0.492642]\n",
      "epoch:15 step:14905 [D loss: 0.237062, acc.: 61.72%] [G loss: 0.501782]\n",
      "epoch:15 step:14906 [D loss: 0.261548, acc.: 50.78%] [G loss: 0.432619]\n",
      "epoch:15 step:14907 [D loss: 0.209112, acc.: 64.84%] [G loss: 0.501488]\n",
      "epoch:15 step:14908 [D loss: 0.205759, acc.: 67.19%] [G loss: 0.449151]\n",
      "epoch:15 step:14909 [D loss: 0.210978, acc.: 65.62%] [G loss: 0.440986]\n",
      "epoch:15 step:14910 [D loss: 0.264308, acc.: 50.78%] [G loss: 0.431764]\n",
      "epoch:15 step:14911 [D loss: 0.227458, acc.: 63.28%] [G loss: 0.449886]\n",
      "epoch:15 step:14912 [D loss: 0.244458, acc.: 60.94%] [G loss: 0.370726]\n",
      "epoch:15 step:14913 [D loss: 0.270262, acc.: 48.44%] [G loss: 0.413740]\n",
      "epoch:15 step:14914 [D loss: 0.254189, acc.: 57.81%] [G loss: 0.433766]\n",
      "epoch:15 step:14915 [D loss: 0.171725, acc.: 76.56%] [G loss: 0.475470]\n",
      "epoch:15 step:14916 [D loss: 0.282886, acc.: 46.88%] [G loss: 0.376256]\n",
      "epoch:15 step:14917 [D loss: 0.245987, acc.: 54.69%] [G loss: 0.394499]\n",
      "epoch:15 step:14918 [D loss: 0.212605, acc.: 63.28%] [G loss: 0.409225]\n",
      "epoch:15 step:14919 [D loss: 0.242343, acc.: 58.59%] [G loss: 0.441854]\n",
      "epoch:15 step:14920 [D loss: 0.254885, acc.: 59.38%] [G loss: 0.381588]\n",
      "epoch:15 step:14921 [D loss: 0.221242, acc.: 65.62%] [G loss: 0.439086]\n",
      "epoch:15 step:14922 [D loss: 0.252292, acc.: 56.25%] [G loss: 0.389785]\n",
      "epoch:15 step:14923 [D loss: 0.242987, acc.: 57.81%] [G loss: 0.432408]\n",
      "epoch:15 step:14924 [D loss: 0.226616, acc.: 64.06%] [G loss: 0.444144]\n",
      "epoch:15 step:14925 [D loss: 0.232083, acc.: 59.38%] [G loss: 0.422892]\n",
      "epoch:15 step:14926 [D loss: 0.194645, acc.: 69.53%] [G loss: 0.507611]\n",
      "epoch:15 step:14927 [D loss: 0.251854, acc.: 54.69%] [G loss: 0.418688]\n",
      "epoch:15 step:14928 [D loss: 0.233466, acc.: 60.16%] [G loss: 0.421450]\n",
      "epoch:15 step:14929 [D loss: 0.226380, acc.: 56.25%] [G loss: 0.403687]\n",
      "epoch:15 step:14930 [D loss: 0.166623, acc.: 79.69%] [G loss: 0.454475]\n",
      "epoch:15 step:14931 [D loss: 0.236473, acc.: 57.03%] [G loss: 0.395797]\n",
      "epoch:15 step:14932 [D loss: 0.228209, acc.: 65.62%] [G loss: 0.423498]\n",
      "epoch:15 step:14933 [D loss: 0.240372, acc.: 57.03%] [G loss: 0.424105]\n",
      "epoch:15 step:14934 [D loss: 0.221411, acc.: 61.72%] [G loss: 0.434760]\n",
      "epoch:15 step:14935 [D loss: 0.251749, acc.: 53.12%] [G loss: 0.432466]\n",
      "epoch:15 step:14936 [D loss: 0.239043, acc.: 60.16%] [G loss: 0.400534]\n",
      "epoch:15 step:14937 [D loss: 0.215724, acc.: 67.19%] [G loss: 0.416498]\n",
      "epoch:15 step:14938 [D loss: 0.242580, acc.: 54.69%] [G loss: 0.414917]\n",
      "epoch:15 step:14939 [D loss: 0.206574, acc.: 71.09%] [G loss: 0.445541]\n",
      "epoch:15 step:14940 [D loss: 0.234609, acc.: 56.25%] [G loss: 0.428306]\n",
      "epoch:15 step:14941 [D loss: 0.213195, acc.: 64.84%] [G loss: 0.459742]\n",
      "epoch:15 step:14942 [D loss: 0.234728, acc.: 58.59%] [G loss: 0.491703]\n",
      "epoch:15 step:14943 [D loss: 0.229994, acc.: 57.81%] [G loss: 0.408186]\n",
      "epoch:15 step:14944 [D loss: 0.222504, acc.: 70.31%] [G loss: 0.448538]\n",
      "epoch:15 step:14945 [D loss: 0.189449, acc.: 73.44%] [G loss: 0.463036]\n",
      "epoch:15 step:14946 [D loss: 0.247254, acc.: 55.47%] [G loss: 0.383571]\n",
      "epoch:15 step:14947 [D loss: 0.258894, acc.: 54.69%] [G loss: 0.412193]\n",
      "epoch:15 step:14948 [D loss: 0.226094, acc.: 62.50%] [G loss: 0.399898]\n",
      "epoch:15 step:14949 [D loss: 0.218701, acc.: 63.28%] [G loss: 0.442569]\n",
      "epoch:15 step:14950 [D loss: 0.222392, acc.: 64.84%] [G loss: 0.440829]\n",
      "epoch:15 step:14951 [D loss: 0.197174, acc.: 71.88%] [G loss: 0.481623]\n",
      "epoch:15 step:14952 [D loss: 0.218775, acc.: 66.41%] [G loss: 0.424926]\n",
      "epoch:15 step:14953 [D loss: 0.208429, acc.: 65.62%] [G loss: 0.443376]\n",
      "epoch:15 step:14954 [D loss: 0.184737, acc.: 75.78%] [G loss: 0.440525]\n",
      "epoch:15 step:14955 [D loss: 0.215076, acc.: 65.62%] [G loss: 0.440054]\n",
      "epoch:15 step:14956 [D loss: 0.201730, acc.: 64.84%] [G loss: 0.473011]\n",
      "epoch:15 step:14957 [D loss: 0.245535, acc.: 58.59%] [G loss: 0.420946]\n",
      "epoch:15 step:14958 [D loss: 0.222030, acc.: 67.97%] [G loss: 0.448081]\n",
      "epoch:15 step:14959 [D loss: 0.216872, acc.: 64.06%] [G loss: 0.439393]\n",
      "epoch:15 step:14960 [D loss: 0.207912, acc.: 67.19%] [G loss: 0.479325]\n",
      "epoch:15 step:14961 [D loss: 0.191347, acc.: 71.09%] [G loss: 0.458114]\n",
      "epoch:15 step:14962 [D loss: 0.244124, acc.: 53.12%] [G loss: 0.440862]\n",
      "epoch:15 step:14963 [D loss: 0.220659, acc.: 63.28%] [G loss: 0.419394]\n",
      "epoch:15 step:14964 [D loss: 0.209108, acc.: 67.97%] [G loss: 0.471376]\n",
      "epoch:15 step:14965 [D loss: 0.215117, acc.: 62.50%] [G loss: 0.457495]\n",
      "epoch:15 step:14966 [D loss: 0.190734, acc.: 72.66%] [G loss: 0.457392]\n",
      "epoch:15 step:14967 [D loss: 0.189769, acc.: 72.66%] [G loss: 0.488363]\n",
      "epoch:15 step:14968 [D loss: 0.223961, acc.: 61.72%] [G loss: 0.434187]\n",
      "epoch:15 step:14969 [D loss: 0.232060, acc.: 62.50%] [G loss: 0.436714]\n",
      "epoch:15 step:14970 [D loss: 0.259178, acc.: 62.50%] [G loss: 0.398781]\n",
      "epoch:15 step:14971 [D loss: 0.218961, acc.: 67.97%] [G loss: 0.419473]\n",
      "epoch:15 step:14972 [D loss: 0.225385, acc.: 62.50%] [G loss: 0.442127]\n",
      "epoch:15 step:14973 [D loss: 0.202269, acc.: 71.09%] [G loss: 0.463653]\n",
      "epoch:15 step:14974 [D loss: 0.203702, acc.: 68.75%] [G loss: 0.536129]\n",
      "epoch:15 step:14975 [D loss: 0.307455, acc.: 46.09%] [G loss: 0.458031]\n",
      "epoch:15 step:14976 [D loss: 0.204518, acc.: 66.41%] [G loss: 0.486374]\n",
      "epoch:15 step:14977 [D loss: 0.237794, acc.: 59.38%] [G loss: 0.409186]\n",
      "epoch:15 step:14978 [D loss: 0.185246, acc.: 75.00%] [G loss: 0.469529]\n",
      "epoch:15 step:14979 [D loss: 0.196585, acc.: 70.31%] [G loss: 0.466973]\n",
      "epoch:15 step:14980 [D loss: 0.212534, acc.: 65.62%] [G loss: 0.476461]\n",
      "epoch:15 step:14981 [D loss: 0.182681, acc.: 75.00%] [G loss: 0.520468]\n",
      "epoch:15 step:14982 [D loss: 0.207636, acc.: 69.53%] [G loss: 0.534602]\n",
      "epoch:15 step:14983 [D loss: 0.292740, acc.: 57.81%] [G loss: 0.468483]\n",
      "epoch:15 step:14984 [D loss: 0.231797, acc.: 59.38%] [G loss: 0.520858]\n",
      "epoch:15 step:14985 [D loss: 0.189361, acc.: 71.09%] [G loss: 0.450145]\n",
      "epoch:15 step:14986 [D loss: 0.255324, acc.: 57.03%] [G loss: 0.471553]\n",
      "epoch:15 step:14987 [D loss: 0.268394, acc.: 56.25%] [G loss: 0.407661]\n",
      "epoch:15 step:14988 [D loss: 0.209353, acc.: 67.19%] [G loss: 0.465624]\n",
      "epoch:15 step:14989 [D loss: 0.203673, acc.: 72.66%] [G loss: 0.457738]\n",
      "epoch:15 step:14990 [D loss: 0.186936, acc.: 67.19%] [G loss: 0.512761]\n",
      "epoch:15 step:14991 [D loss: 0.184007, acc.: 71.88%] [G loss: 0.497327]\n",
      "epoch:15 step:14992 [D loss: 0.199654, acc.: 66.41%] [G loss: 0.556919]\n",
      "epoch:16 step:14993 [D loss: 0.248149, acc.: 61.72%] [G loss: 0.523043]\n",
      "epoch:16 step:14994 [D loss: 0.253233, acc.: 56.25%] [G loss: 0.442251]\n",
      "epoch:16 step:14995 [D loss: 0.253905, acc.: 60.94%] [G loss: 0.445897]\n",
      "epoch:16 step:14996 [D loss: 0.226297, acc.: 60.16%] [G loss: 0.495574]\n",
      "epoch:16 step:14997 [D loss: 0.235963, acc.: 56.25%] [G loss: 0.426599]\n",
      "epoch:16 step:14998 [D loss: 0.224826, acc.: 61.72%] [G loss: 0.433575]\n",
      "epoch:16 step:14999 [D loss: 0.211772, acc.: 65.62%] [G loss: 0.427004]\n",
      "epoch:16 step:15000 [D loss: 0.215977, acc.: 69.53%] [G loss: 0.448705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.43650445 1.98233165 6.07386481 4.61230071 3.51851713 5.40014122\n",
      " 4.4584047  4.83152936 4.57893055 3.84520835]\n",
      "##########\n",
      "epoch:16 step:15001 [D loss: 0.215713, acc.: 67.19%] [G loss: 0.458455]\n",
      "epoch:16 step:15002 [D loss: 0.216226, acc.: 64.84%] [G loss: 0.459049]\n",
      "epoch:16 step:15003 [D loss: 0.211675, acc.: 64.06%] [G loss: 0.452946]\n",
      "epoch:16 step:15004 [D loss: 0.223491, acc.: 64.06%] [G loss: 0.441119]\n",
      "epoch:16 step:15005 [D loss: 0.206677, acc.: 70.31%] [G loss: 0.448455]\n",
      "epoch:16 step:15006 [D loss: 0.223668, acc.: 64.84%] [G loss: 0.455700]\n",
      "epoch:16 step:15007 [D loss: 0.210618, acc.: 67.97%] [G loss: 0.476359]\n",
      "epoch:16 step:15008 [D loss: 0.206795, acc.: 64.84%] [G loss: 0.440010]\n",
      "epoch:16 step:15009 [D loss: 0.233496, acc.: 59.38%] [G loss: 0.384114]\n",
      "epoch:16 step:15010 [D loss: 0.223355, acc.: 64.06%] [G loss: 0.430329]\n",
      "epoch:16 step:15011 [D loss: 0.253047, acc.: 54.69%] [G loss: 0.414094]\n",
      "epoch:16 step:15012 [D loss: 0.253588, acc.: 52.34%] [G loss: 0.400927]\n",
      "epoch:16 step:15013 [D loss: 0.257075, acc.: 54.69%] [G loss: 0.431979]\n",
      "epoch:16 step:15014 [D loss: 0.201596, acc.: 70.31%] [G loss: 0.498579]\n",
      "epoch:16 step:15015 [D loss: 0.252954, acc.: 55.47%] [G loss: 0.420315]\n",
      "epoch:16 step:15016 [D loss: 0.211321, acc.: 68.75%] [G loss: 0.419770]\n",
      "epoch:16 step:15017 [D loss: 0.213388, acc.: 63.28%] [G loss: 0.417983]\n",
      "epoch:16 step:15018 [D loss: 0.235947, acc.: 57.81%] [G loss: 0.427938]\n",
      "epoch:16 step:15019 [D loss: 0.216551, acc.: 67.97%] [G loss: 0.465296]\n",
      "epoch:16 step:15020 [D loss: 0.232666, acc.: 57.03%] [G loss: 0.409841]\n",
      "epoch:16 step:15021 [D loss: 0.211489, acc.: 69.53%] [G loss: 0.419395]\n",
      "epoch:16 step:15022 [D loss: 0.224675, acc.: 63.28%] [G loss: 0.428416]\n",
      "epoch:16 step:15023 [D loss: 0.249771, acc.: 55.47%] [G loss: 0.388813]\n",
      "epoch:16 step:15024 [D loss: 0.216273, acc.: 63.28%] [G loss: 0.392786]\n",
      "epoch:16 step:15025 [D loss: 0.222766, acc.: 63.28%] [G loss: 0.467746]\n",
      "epoch:16 step:15026 [D loss: 0.257523, acc.: 53.12%] [G loss: 0.451089]\n",
      "epoch:16 step:15027 [D loss: 0.221432, acc.: 66.41%] [G loss: 0.412039]\n",
      "epoch:16 step:15028 [D loss: 0.217027, acc.: 63.28%] [G loss: 0.429130]\n",
      "epoch:16 step:15029 [D loss: 0.249106, acc.: 55.47%] [G loss: 0.410498]\n",
      "epoch:16 step:15030 [D loss: 0.259631, acc.: 52.34%] [G loss: 0.399959]\n",
      "epoch:16 step:15031 [D loss: 0.214143, acc.: 67.97%] [G loss: 0.412723]\n",
      "epoch:16 step:15032 [D loss: 0.206527, acc.: 71.88%] [G loss: 0.456745]\n",
      "epoch:16 step:15033 [D loss: 0.247694, acc.: 55.47%] [G loss: 0.398895]\n",
      "epoch:16 step:15034 [D loss: 0.217552, acc.: 61.72%] [G loss: 0.435946]\n",
      "epoch:16 step:15035 [D loss: 0.228188, acc.: 62.50%] [G loss: 0.415513]\n",
      "epoch:16 step:15036 [D loss: 0.250453, acc.: 55.47%] [G loss: 0.394510]\n",
      "epoch:16 step:15037 [D loss: 0.216743, acc.: 64.84%] [G loss: 0.452976]\n",
      "epoch:16 step:15038 [D loss: 0.224493, acc.: 64.84%] [G loss: 0.430225]\n",
      "epoch:16 step:15039 [D loss: 0.217273, acc.: 67.97%] [G loss: 0.434782]\n",
      "epoch:16 step:15040 [D loss: 0.179775, acc.: 75.00%] [G loss: 0.463801]\n",
      "epoch:16 step:15041 [D loss: 0.210030, acc.: 67.97%] [G loss: 0.413040]\n",
      "epoch:16 step:15042 [D loss: 0.214861, acc.: 69.53%] [G loss: 0.422822]\n",
      "epoch:16 step:15043 [D loss: 0.243850, acc.: 57.81%] [G loss: 0.398142]\n",
      "epoch:16 step:15044 [D loss: 0.233238, acc.: 60.94%] [G loss: 0.458866]\n",
      "epoch:16 step:15045 [D loss: 0.216229, acc.: 65.62%] [G loss: 0.440711]\n",
      "epoch:16 step:15046 [D loss: 0.224510, acc.: 64.06%] [G loss: 0.424168]\n",
      "epoch:16 step:15047 [D loss: 0.230297, acc.: 60.16%] [G loss: 0.458513]\n",
      "epoch:16 step:15048 [D loss: 0.207621, acc.: 65.62%] [G loss: 0.453087]\n",
      "epoch:16 step:15049 [D loss: 0.243056, acc.: 57.81%] [G loss: 0.435727]\n",
      "epoch:16 step:15050 [D loss: 0.235801, acc.: 58.59%] [G loss: 0.416537]\n",
      "epoch:16 step:15051 [D loss: 0.213369, acc.: 64.06%] [G loss: 0.464542]\n",
      "epoch:16 step:15052 [D loss: 0.232430, acc.: 60.16%] [G loss: 0.448818]\n",
      "epoch:16 step:15053 [D loss: 0.222168, acc.: 62.50%] [G loss: 0.438826]\n",
      "epoch:16 step:15054 [D loss: 0.219790, acc.: 64.84%] [G loss: 0.421010]\n",
      "epoch:16 step:15055 [D loss: 0.223127, acc.: 64.84%] [G loss: 0.424196]\n",
      "epoch:16 step:15056 [D loss: 0.229643, acc.: 60.16%] [G loss: 0.426597]\n",
      "epoch:16 step:15057 [D loss: 0.233189, acc.: 62.50%] [G loss: 0.432984]\n",
      "epoch:16 step:15058 [D loss: 0.218306, acc.: 60.16%] [G loss: 0.431723]\n",
      "epoch:16 step:15059 [D loss: 0.229193, acc.: 64.06%] [G loss: 0.401965]\n",
      "epoch:16 step:15060 [D loss: 0.240923, acc.: 59.38%] [G loss: 0.436370]\n",
      "epoch:16 step:15061 [D loss: 0.185804, acc.: 70.31%] [G loss: 0.439190]\n",
      "epoch:16 step:15062 [D loss: 0.214655, acc.: 64.84%] [G loss: 0.432332]\n",
      "epoch:16 step:15063 [D loss: 0.238669, acc.: 61.72%] [G loss: 0.419363]\n",
      "epoch:16 step:15064 [D loss: 0.242196, acc.: 59.38%] [G loss: 0.402345]\n",
      "epoch:16 step:15065 [D loss: 0.226624, acc.: 63.28%] [G loss: 0.435541]\n",
      "epoch:16 step:15066 [D loss: 0.212234, acc.: 67.19%] [G loss: 0.444444]\n",
      "epoch:16 step:15067 [D loss: 0.201238, acc.: 67.97%] [G loss: 0.462949]\n",
      "epoch:16 step:15068 [D loss: 0.192723, acc.: 71.09%] [G loss: 0.486612]\n",
      "epoch:16 step:15069 [D loss: 0.216082, acc.: 62.50%] [G loss: 0.471377]\n",
      "epoch:16 step:15070 [D loss: 0.275683, acc.: 53.91%] [G loss: 0.381356]\n",
      "epoch:16 step:15071 [D loss: 0.245782, acc.: 52.34%] [G loss: 0.431806]\n",
      "epoch:16 step:15072 [D loss: 0.205567, acc.: 71.88%] [G loss: 0.424277]\n",
      "epoch:16 step:15073 [D loss: 0.222026, acc.: 60.94%] [G loss: 0.453288]\n",
      "epoch:16 step:15074 [D loss: 0.211401, acc.: 64.06%] [G loss: 0.431355]\n",
      "epoch:16 step:15075 [D loss: 0.247020, acc.: 58.59%] [G loss: 0.426163]\n",
      "epoch:16 step:15076 [D loss: 0.204086, acc.: 70.31%] [G loss: 0.488217]\n",
      "epoch:16 step:15077 [D loss: 0.220764, acc.: 66.41%] [G loss: 0.470878]\n",
      "epoch:16 step:15078 [D loss: 0.231270, acc.: 60.16%] [G loss: 0.434586]\n",
      "epoch:16 step:15079 [D loss: 0.230318, acc.: 61.72%] [G loss: 0.419221]\n",
      "epoch:16 step:15080 [D loss: 0.208095, acc.: 67.19%] [G loss: 0.421564]\n",
      "epoch:16 step:15081 [D loss: 0.220684, acc.: 66.41%] [G loss: 0.423255]\n",
      "epoch:16 step:15082 [D loss: 0.238904, acc.: 61.72%] [G loss: 0.442667]\n",
      "epoch:16 step:15083 [D loss: 0.231728, acc.: 61.72%] [G loss: 0.463289]\n",
      "epoch:16 step:15084 [D loss: 0.195758, acc.: 77.34%] [G loss: 0.481752]\n",
      "epoch:16 step:15085 [D loss: 0.203716, acc.: 67.19%] [G loss: 0.444786]\n",
      "epoch:16 step:15086 [D loss: 0.228588, acc.: 62.50%] [G loss: 0.461072]\n",
      "epoch:16 step:15087 [D loss: 0.218024, acc.: 64.84%] [G loss: 0.480487]\n",
      "epoch:16 step:15088 [D loss: 0.215805, acc.: 64.06%] [G loss: 0.443178]\n",
      "epoch:16 step:15089 [D loss: 0.205874, acc.: 71.88%] [G loss: 0.448963]\n",
      "epoch:16 step:15090 [D loss: 0.206461, acc.: 64.84%] [G loss: 0.495784]\n",
      "epoch:16 step:15091 [D loss: 0.273535, acc.: 51.56%] [G loss: 0.435263]\n",
      "epoch:16 step:15092 [D loss: 0.199488, acc.: 68.75%] [G loss: 0.455120]\n",
      "epoch:16 step:15093 [D loss: 0.251183, acc.: 53.12%] [G loss: 0.426869]\n",
      "epoch:16 step:15094 [D loss: 0.253365, acc.: 57.81%] [G loss: 0.416502]\n",
      "epoch:16 step:15095 [D loss: 0.231810, acc.: 57.81%] [G loss: 0.389295]\n",
      "epoch:16 step:15096 [D loss: 0.232340, acc.: 60.16%] [G loss: 0.416415]\n",
      "epoch:16 step:15097 [D loss: 0.255136, acc.: 59.38%] [G loss: 0.404811]\n",
      "epoch:16 step:15098 [D loss: 0.197311, acc.: 71.09%] [G loss: 0.453317]\n",
      "epoch:16 step:15099 [D loss: 0.201735, acc.: 67.19%] [G loss: 0.459809]\n",
      "epoch:16 step:15100 [D loss: 0.265286, acc.: 56.25%] [G loss: 0.431083]\n",
      "epoch:16 step:15101 [D loss: 0.239730, acc.: 61.72%] [G loss: 0.419694]\n",
      "epoch:16 step:15102 [D loss: 0.238208, acc.: 60.94%] [G loss: 0.412921]\n",
      "epoch:16 step:15103 [D loss: 0.215516, acc.: 66.41%] [G loss: 0.442906]\n",
      "epoch:16 step:15104 [D loss: 0.204856, acc.: 67.19%] [G loss: 0.460134]\n",
      "epoch:16 step:15105 [D loss: 0.206260, acc.: 67.19%] [G loss: 0.462597]\n",
      "epoch:16 step:15106 [D loss: 0.222535, acc.: 62.50%] [G loss: 0.428801]\n",
      "epoch:16 step:15107 [D loss: 0.204196, acc.: 65.62%] [G loss: 0.477017]\n",
      "epoch:16 step:15108 [D loss: 0.213788, acc.: 65.62%] [G loss: 0.468399]\n",
      "epoch:16 step:15109 [D loss: 0.205111, acc.: 67.19%] [G loss: 0.479164]\n",
      "epoch:16 step:15110 [D loss: 0.236549, acc.: 64.06%] [G loss: 0.465578]\n",
      "epoch:16 step:15111 [D loss: 0.197303, acc.: 68.75%] [G loss: 0.568002]\n",
      "epoch:16 step:15112 [D loss: 0.259906, acc.: 58.59%] [G loss: 0.476179]\n",
      "epoch:16 step:15113 [D loss: 0.255018, acc.: 59.38%] [G loss: 0.423190]\n",
      "epoch:16 step:15114 [D loss: 0.213279, acc.: 67.97%] [G loss: 0.462476]\n",
      "epoch:16 step:15115 [D loss: 0.197591, acc.: 65.62%] [G loss: 0.479813]\n",
      "epoch:16 step:15116 [D loss: 0.235273, acc.: 62.50%] [G loss: 0.484449]\n",
      "epoch:16 step:15117 [D loss: 0.221286, acc.: 69.53%] [G loss: 0.457025]\n",
      "epoch:16 step:15118 [D loss: 0.205065, acc.: 67.97%] [G loss: 0.445307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15119 [D loss: 0.215453, acc.: 64.84%] [G loss: 0.446791]\n",
      "epoch:16 step:15120 [D loss: 0.223007, acc.: 61.72%] [G loss: 0.453559]\n",
      "epoch:16 step:15121 [D loss: 0.223661, acc.: 68.75%] [G loss: 0.434207]\n",
      "epoch:16 step:15122 [D loss: 0.194555, acc.: 71.09%] [G loss: 0.433077]\n",
      "epoch:16 step:15123 [D loss: 0.220204, acc.: 67.97%] [G loss: 0.448691]\n",
      "epoch:16 step:15124 [D loss: 0.220330, acc.: 67.19%] [G loss: 0.444180]\n",
      "epoch:16 step:15125 [D loss: 0.224449, acc.: 61.72%] [G loss: 0.433909]\n",
      "epoch:16 step:15126 [D loss: 0.220291, acc.: 64.06%] [G loss: 0.444090]\n",
      "epoch:16 step:15127 [D loss: 0.222518, acc.: 62.50%] [G loss: 0.426393]\n",
      "epoch:16 step:15128 [D loss: 0.224014, acc.: 64.84%] [G loss: 0.454452]\n",
      "epoch:16 step:15129 [D loss: 0.256119, acc.: 57.81%] [G loss: 0.395551]\n",
      "epoch:16 step:15130 [D loss: 0.245176, acc.: 62.50%] [G loss: 0.422789]\n",
      "epoch:16 step:15131 [D loss: 0.208792, acc.: 64.06%] [G loss: 0.441144]\n",
      "epoch:16 step:15132 [D loss: 0.224863, acc.: 58.59%] [G loss: 0.467116]\n",
      "epoch:16 step:15133 [D loss: 0.213118, acc.: 65.62%] [G loss: 0.412428]\n",
      "epoch:16 step:15134 [D loss: 0.223798, acc.: 60.94%] [G loss: 0.402887]\n",
      "epoch:16 step:15135 [D loss: 0.236254, acc.: 57.81%] [G loss: 0.407729]\n",
      "epoch:16 step:15136 [D loss: 0.211554, acc.: 64.06%] [G loss: 0.404543]\n",
      "epoch:16 step:15137 [D loss: 0.242755, acc.: 57.03%] [G loss: 0.429422]\n",
      "epoch:16 step:15138 [D loss: 0.215475, acc.: 62.50%] [G loss: 0.458848]\n",
      "epoch:16 step:15139 [D loss: 0.231901, acc.: 56.25%] [G loss: 0.416601]\n",
      "epoch:16 step:15140 [D loss: 0.260918, acc.: 51.56%] [G loss: 0.410920]\n",
      "epoch:16 step:15141 [D loss: 0.207472, acc.: 67.97%] [G loss: 0.425497]\n",
      "epoch:16 step:15142 [D loss: 0.238866, acc.: 57.03%] [G loss: 0.407656]\n",
      "epoch:16 step:15143 [D loss: 0.213781, acc.: 64.06%] [G loss: 0.435801]\n",
      "epoch:16 step:15144 [D loss: 0.237579, acc.: 64.06%] [G loss: 0.450153]\n",
      "epoch:16 step:15145 [D loss: 0.239090, acc.: 59.38%] [G loss: 0.415826]\n",
      "epoch:16 step:15146 [D loss: 0.215666, acc.: 64.84%] [G loss: 0.449336]\n",
      "epoch:16 step:15147 [D loss: 0.216390, acc.: 67.19%] [G loss: 0.419423]\n",
      "epoch:16 step:15148 [D loss: 0.214928, acc.: 66.41%] [G loss: 0.454845]\n",
      "epoch:16 step:15149 [D loss: 0.218696, acc.: 63.28%] [G loss: 0.453476]\n",
      "epoch:16 step:15150 [D loss: 0.230783, acc.: 58.59%] [G loss: 0.400386]\n",
      "epoch:16 step:15151 [D loss: 0.223572, acc.: 62.50%] [G loss: 0.454747]\n",
      "epoch:16 step:15152 [D loss: 0.263551, acc.: 50.78%] [G loss: 0.429575]\n",
      "epoch:16 step:15153 [D loss: 0.284573, acc.: 52.34%] [G loss: 0.463397]\n",
      "epoch:16 step:15154 [D loss: 0.226681, acc.: 64.84%] [G loss: 0.427053]\n",
      "epoch:16 step:15155 [D loss: 0.215709, acc.: 64.84%] [G loss: 0.472849]\n",
      "epoch:16 step:15156 [D loss: 0.230794, acc.: 57.81%] [G loss: 0.426327]\n",
      "epoch:16 step:15157 [D loss: 0.224077, acc.: 65.62%] [G loss: 0.471631]\n",
      "epoch:16 step:15158 [D loss: 0.230493, acc.: 62.50%] [G loss: 0.457464]\n",
      "epoch:16 step:15159 [D loss: 0.209124, acc.: 64.84%] [G loss: 0.464315]\n",
      "epoch:16 step:15160 [D loss: 0.208759, acc.: 68.75%] [G loss: 0.472641]\n",
      "epoch:16 step:15161 [D loss: 0.227664, acc.: 60.94%] [G loss: 0.446358]\n",
      "epoch:16 step:15162 [D loss: 0.251738, acc.: 53.91%] [G loss: 0.382636]\n",
      "epoch:16 step:15163 [D loss: 0.220929, acc.: 57.81%] [G loss: 0.431525]\n",
      "epoch:16 step:15164 [D loss: 0.214771, acc.: 65.62%] [G loss: 0.409007]\n",
      "epoch:16 step:15165 [D loss: 0.219207, acc.: 64.84%] [G loss: 0.401680]\n",
      "epoch:16 step:15166 [D loss: 0.228420, acc.: 60.16%] [G loss: 0.413576]\n",
      "epoch:16 step:15167 [D loss: 0.221801, acc.: 64.84%] [G loss: 0.408560]\n",
      "epoch:16 step:15168 [D loss: 0.250271, acc.: 58.59%] [G loss: 0.423601]\n",
      "epoch:16 step:15169 [D loss: 0.238982, acc.: 58.59%] [G loss: 0.421761]\n",
      "epoch:16 step:15170 [D loss: 0.232481, acc.: 57.03%] [G loss: 0.403455]\n",
      "epoch:16 step:15171 [D loss: 0.224115, acc.: 62.50%] [G loss: 0.415833]\n",
      "epoch:16 step:15172 [D loss: 0.215067, acc.: 64.84%] [G loss: 0.436797]\n",
      "epoch:16 step:15173 [D loss: 0.251235, acc.: 56.25%] [G loss: 0.399006]\n",
      "epoch:16 step:15174 [D loss: 0.226230, acc.: 63.28%] [G loss: 0.413069]\n",
      "epoch:16 step:15175 [D loss: 0.229954, acc.: 63.28%] [G loss: 0.393952]\n",
      "epoch:16 step:15176 [D loss: 0.204044, acc.: 67.19%] [G loss: 0.449064]\n",
      "epoch:16 step:15177 [D loss: 0.257340, acc.: 57.03%] [G loss: 0.423128]\n",
      "epoch:16 step:15178 [D loss: 0.234855, acc.: 64.84%] [G loss: 0.435744]\n",
      "epoch:16 step:15179 [D loss: 0.229992, acc.: 60.16%] [G loss: 0.421702]\n",
      "epoch:16 step:15180 [D loss: 0.246244, acc.: 55.47%] [G loss: 0.380380]\n",
      "epoch:16 step:15181 [D loss: 0.252152, acc.: 53.12%] [G loss: 0.431742]\n",
      "epoch:16 step:15182 [D loss: 0.238650, acc.: 58.59%] [G loss: 0.432198]\n",
      "epoch:16 step:15183 [D loss: 0.223475, acc.: 65.62%] [G loss: 0.428939]\n",
      "epoch:16 step:15184 [D loss: 0.207835, acc.: 69.53%] [G loss: 0.461177]\n",
      "epoch:16 step:15185 [D loss: 0.208497, acc.: 65.62%] [G loss: 0.441369]\n",
      "epoch:16 step:15186 [D loss: 0.202854, acc.: 69.53%] [G loss: 0.448153]\n",
      "epoch:16 step:15187 [D loss: 0.199955, acc.: 69.53%] [G loss: 0.470434]\n",
      "epoch:16 step:15188 [D loss: 0.211596, acc.: 69.53%] [G loss: 0.464522]\n",
      "epoch:16 step:15189 [D loss: 0.227565, acc.: 60.94%] [G loss: 0.453346]\n",
      "epoch:16 step:15190 [D loss: 0.208635, acc.: 69.53%] [G loss: 0.417070]\n",
      "epoch:16 step:15191 [D loss: 0.217797, acc.: 61.72%] [G loss: 0.478002]\n",
      "epoch:16 step:15192 [D loss: 0.252047, acc.: 55.47%] [G loss: 0.427751]\n",
      "epoch:16 step:15193 [D loss: 0.237979, acc.: 57.81%] [G loss: 0.404849]\n",
      "epoch:16 step:15194 [D loss: 0.209967, acc.: 67.19%] [G loss: 0.441174]\n",
      "epoch:16 step:15195 [D loss: 0.286240, acc.: 48.44%] [G loss: 0.393346]\n",
      "epoch:16 step:15196 [D loss: 0.200220, acc.: 70.31%] [G loss: 0.443000]\n",
      "epoch:16 step:15197 [D loss: 0.247036, acc.: 58.59%] [G loss: 0.466335]\n",
      "epoch:16 step:15198 [D loss: 0.212655, acc.: 69.53%] [G loss: 0.462494]\n",
      "epoch:16 step:15199 [D loss: 0.232014, acc.: 59.38%] [G loss: 0.426974]\n",
      "epoch:16 step:15200 [D loss: 0.172088, acc.: 76.56%] [G loss: 0.482115]\n",
      "##############\n",
      "[2.50552006 1.74695599 6.02858708 4.91525729 3.48774187 5.29230901\n",
      " 4.29659385 4.67754008 4.33389647 3.75318462]\n",
      "##########\n",
      "epoch:16 step:15201 [D loss: 0.228990, acc.: 64.06%] [G loss: 0.492889]\n",
      "epoch:16 step:15202 [D loss: 0.257812, acc.: 50.00%] [G loss: 0.462557]\n",
      "epoch:16 step:15203 [D loss: 0.247293, acc.: 60.94%] [G loss: 0.388815]\n",
      "epoch:16 step:15204 [D loss: 0.251447, acc.: 56.25%] [G loss: 0.397524]\n",
      "epoch:16 step:15205 [D loss: 0.247630, acc.: 55.47%] [G loss: 0.389528]\n",
      "epoch:16 step:15206 [D loss: 0.232058, acc.: 62.50%] [G loss: 0.443967]\n",
      "epoch:16 step:15207 [D loss: 0.268228, acc.: 52.34%] [G loss: 0.419637]\n",
      "epoch:16 step:15208 [D loss: 0.234981, acc.: 61.72%] [G loss: 0.466079]\n",
      "epoch:16 step:15209 [D loss: 0.228449, acc.: 64.84%] [G loss: 0.421688]\n",
      "epoch:16 step:15210 [D loss: 0.220763, acc.: 63.28%] [G loss: 0.401634]\n",
      "epoch:16 step:15211 [D loss: 0.200284, acc.: 69.53%] [G loss: 0.436287]\n",
      "epoch:16 step:15212 [D loss: 0.273758, acc.: 50.78%] [G loss: 0.440968]\n",
      "epoch:16 step:15213 [D loss: 0.229843, acc.: 62.50%] [G loss: 0.446852]\n",
      "epoch:16 step:15214 [D loss: 0.212652, acc.: 65.62%] [G loss: 0.427492]\n",
      "epoch:16 step:15215 [D loss: 0.205212, acc.: 63.28%] [G loss: 0.444975]\n",
      "epoch:16 step:15216 [D loss: 0.236113, acc.: 58.59%] [G loss: 0.397356]\n",
      "epoch:16 step:15217 [D loss: 0.221918, acc.: 60.16%] [G loss: 0.403960]\n",
      "epoch:16 step:15218 [D loss: 0.211409, acc.: 67.19%] [G loss: 0.442585]\n",
      "epoch:16 step:15219 [D loss: 0.216798, acc.: 70.31%] [G loss: 0.404335]\n",
      "epoch:16 step:15220 [D loss: 0.232545, acc.: 62.50%] [G loss: 0.404876]\n",
      "epoch:16 step:15221 [D loss: 0.201432, acc.: 71.09%] [G loss: 0.432370]\n",
      "epoch:16 step:15222 [D loss: 0.211286, acc.: 70.31%] [G loss: 0.464336]\n",
      "epoch:16 step:15223 [D loss: 0.181525, acc.: 77.34%] [G loss: 0.509760]\n",
      "epoch:16 step:15224 [D loss: 0.198706, acc.: 70.31%] [G loss: 0.507025]\n",
      "epoch:16 step:15225 [D loss: 0.237663, acc.: 61.72%] [G loss: 0.447541]\n",
      "epoch:16 step:15226 [D loss: 0.232985, acc.: 60.94%] [G loss: 0.426894]\n",
      "epoch:16 step:15227 [D loss: 0.196042, acc.: 68.75%] [G loss: 0.453069]\n",
      "epoch:16 step:15228 [D loss: 0.201380, acc.: 71.09%] [G loss: 0.413106]\n",
      "epoch:16 step:15229 [D loss: 0.218705, acc.: 64.06%] [G loss: 0.413438]\n",
      "epoch:16 step:15230 [D loss: 0.209979, acc.: 67.19%] [G loss: 0.422094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15231 [D loss: 0.224606, acc.: 60.16%] [G loss: 0.433831]\n",
      "epoch:16 step:15232 [D loss: 0.232280, acc.: 64.06%] [G loss: 0.420290]\n",
      "epoch:16 step:15233 [D loss: 0.197608, acc.: 72.66%] [G loss: 0.469584]\n",
      "epoch:16 step:15234 [D loss: 0.210092, acc.: 70.31%] [G loss: 0.493881]\n",
      "epoch:16 step:15235 [D loss: 0.238400, acc.: 60.16%] [G loss: 0.429285]\n",
      "epoch:16 step:15236 [D loss: 0.214238, acc.: 64.06%] [G loss: 0.445845]\n",
      "epoch:16 step:15237 [D loss: 0.207079, acc.: 69.53%] [G loss: 0.427038]\n",
      "epoch:16 step:15238 [D loss: 0.203797, acc.: 66.41%] [G loss: 0.478713]\n",
      "epoch:16 step:15239 [D loss: 0.208904, acc.: 60.16%] [G loss: 0.470289]\n",
      "epoch:16 step:15240 [D loss: 0.188635, acc.: 70.31%] [G loss: 0.497320]\n",
      "epoch:16 step:15241 [D loss: 0.250797, acc.: 60.16%] [G loss: 0.440570]\n",
      "epoch:16 step:15242 [D loss: 0.262952, acc.: 51.56%] [G loss: 0.458002]\n",
      "epoch:16 step:15243 [D loss: 0.255996, acc.: 53.91%] [G loss: 0.410566]\n",
      "epoch:16 step:15244 [D loss: 0.220739, acc.: 60.16%] [G loss: 0.457501]\n",
      "epoch:16 step:15245 [D loss: 0.225662, acc.: 64.06%] [G loss: 0.457800]\n",
      "epoch:16 step:15246 [D loss: 0.241440, acc.: 58.59%] [G loss: 0.445353]\n",
      "epoch:16 step:15247 [D loss: 0.224503, acc.: 60.16%] [G loss: 0.447768]\n",
      "epoch:16 step:15248 [D loss: 0.229725, acc.: 62.50%] [G loss: 0.404295]\n",
      "epoch:16 step:15249 [D loss: 0.249644, acc.: 55.47%] [G loss: 0.390862]\n",
      "epoch:16 step:15250 [D loss: 0.199977, acc.: 71.09%] [G loss: 0.418900]\n",
      "epoch:16 step:15251 [D loss: 0.200547, acc.: 69.53%] [G loss: 0.436473]\n",
      "epoch:16 step:15252 [D loss: 0.237327, acc.: 61.72%] [G loss: 0.402769]\n",
      "epoch:16 step:15253 [D loss: 0.199448, acc.: 71.88%] [G loss: 0.450446]\n",
      "epoch:16 step:15254 [D loss: 0.219556, acc.: 62.50%] [G loss: 0.455305]\n",
      "epoch:16 step:15255 [D loss: 0.238540, acc.: 57.81%] [G loss: 0.458189]\n",
      "epoch:16 step:15256 [D loss: 0.229539, acc.: 67.97%] [G loss: 0.427618]\n",
      "epoch:16 step:15257 [D loss: 0.238956, acc.: 57.03%] [G loss: 0.436780]\n",
      "epoch:16 step:15258 [D loss: 0.254947, acc.: 55.47%] [G loss: 0.395800]\n",
      "epoch:16 step:15259 [D loss: 0.208638, acc.: 70.31%] [G loss: 0.443141]\n",
      "epoch:16 step:15260 [D loss: 0.207631, acc.: 66.41%] [G loss: 0.419372]\n",
      "epoch:16 step:15261 [D loss: 0.208627, acc.: 75.78%] [G loss: 0.433634]\n",
      "epoch:16 step:15262 [D loss: 0.203826, acc.: 67.97%] [G loss: 0.459496]\n",
      "epoch:16 step:15263 [D loss: 0.192132, acc.: 68.75%] [G loss: 0.439893]\n",
      "epoch:16 step:15264 [D loss: 0.227436, acc.: 57.81%] [G loss: 0.487754]\n",
      "epoch:16 step:15265 [D loss: 0.197877, acc.: 68.75%] [G loss: 0.459857]\n",
      "epoch:16 step:15266 [D loss: 0.192028, acc.: 66.41%] [G loss: 0.463250]\n",
      "epoch:16 step:15267 [D loss: 0.221141, acc.: 61.72%] [G loss: 0.453552]\n",
      "epoch:16 step:15268 [D loss: 0.212293, acc.: 67.97%] [G loss: 0.466584]\n",
      "epoch:16 step:15269 [D loss: 0.253876, acc.: 53.91%] [G loss: 0.410954]\n",
      "epoch:16 step:15270 [D loss: 0.225845, acc.: 59.38%] [G loss: 0.449338]\n",
      "epoch:16 step:15271 [D loss: 0.247964, acc.: 53.12%] [G loss: 0.412582]\n",
      "epoch:16 step:15272 [D loss: 0.196736, acc.: 72.66%] [G loss: 0.442873]\n",
      "epoch:16 step:15273 [D loss: 0.250273, acc.: 57.81%] [G loss: 0.460192]\n",
      "epoch:16 step:15274 [D loss: 0.249379, acc.: 54.69%] [G loss: 0.381081]\n",
      "epoch:16 step:15275 [D loss: 0.227560, acc.: 62.50%] [G loss: 0.419578]\n",
      "epoch:16 step:15276 [D loss: 0.225200, acc.: 57.81%] [G loss: 0.426823]\n",
      "epoch:16 step:15277 [D loss: 0.224640, acc.: 60.16%] [G loss: 0.448656]\n",
      "epoch:16 step:15278 [D loss: 0.211453, acc.: 66.41%] [G loss: 0.465585]\n",
      "epoch:16 step:15279 [D loss: 0.240113, acc.: 59.38%] [G loss: 0.398003]\n",
      "epoch:16 step:15280 [D loss: 0.206958, acc.: 67.19%] [G loss: 0.458731]\n",
      "epoch:16 step:15281 [D loss: 0.214045, acc.: 67.19%] [G loss: 0.434012]\n",
      "epoch:16 step:15282 [D loss: 0.232867, acc.: 59.38%] [G loss: 0.423308]\n",
      "epoch:16 step:15283 [D loss: 0.255923, acc.: 60.16%] [G loss: 0.436061]\n",
      "epoch:16 step:15284 [D loss: 0.226077, acc.: 61.72%] [G loss: 0.468816]\n",
      "epoch:16 step:15285 [D loss: 0.234563, acc.: 62.50%] [G loss: 0.445701]\n",
      "epoch:16 step:15286 [D loss: 0.245826, acc.: 57.03%] [G loss: 0.415588]\n",
      "epoch:16 step:15287 [D loss: 0.229146, acc.: 61.72%] [G loss: 0.403002]\n",
      "epoch:16 step:15288 [D loss: 0.198292, acc.: 74.22%] [G loss: 0.452889]\n",
      "epoch:16 step:15289 [D loss: 0.214781, acc.: 67.19%] [G loss: 0.467175]\n",
      "epoch:16 step:15290 [D loss: 0.195468, acc.: 71.88%] [G loss: 0.436761]\n",
      "epoch:16 step:15291 [D loss: 0.215540, acc.: 66.41%] [G loss: 0.443279]\n",
      "epoch:16 step:15292 [D loss: 0.217010, acc.: 67.19%] [G loss: 0.432168]\n",
      "epoch:16 step:15293 [D loss: 0.238733, acc.: 60.94%] [G loss: 0.453578]\n",
      "epoch:16 step:15294 [D loss: 0.229135, acc.: 63.28%] [G loss: 0.445629]\n",
      "epoch:16 step:15295 [D loss: 0.232389, acc.: 60.16%] [G loss: 0.425731]\n",
      "epoch:16 step:15296 [D loss: 0.209121, acc.: 66.41%] [G loss: 0.459780]\n",
      "epoch:16 step:15297 [D loss: 0.205338, acc.: 65.62%] [G loss: 0.429471]\n",
      "epoch:16 step:15298 [D loss: 0.230352, acc.: 57.81%] [G loss: 0.455730]\n",
      "epoch:16 step:15299 [D loss: 0.211705, acc.: 65.62%] [G loss: 0.446824]\n",
      "epoch:16 step:15300 [D loss: 0.210289, acc.: 70.31%] [G loss: 0.449414]\n",
      "epoch:16 step:15301 [D loss: 0.237817, acc.: 58.59%] [G loss: 0.413330]\n",
      "epoch:16 step:15302 [D loss: 0.213733, acc.: 68.75%] [G loss: 0.441726]\n",
      "epoch:16 step:15303 [D loss: 0.231945, acc.: 60.94%] [G loss: 0.435559]\n",
      "epoch:16 step:15304 [D loss: 0.184937, acc.: 76.56%] [G loss: 0.493682]\n",
      "epoch:16 step:15305 [D loss: 0.174515, acc.: 72.66%] [G loss: 0.534778]\n",
      "epoch:16 step:15306 [D loss: 0.200189, acc.: 70.31%] [G loss: 0.522851]\n",
      "epoch:16 step:15307 [D loss: 0.195401, acc.: 72.66%] [G loss: 0.527541]\n",
      "epoch:16 step:15308 [D loss: 0.267254, acc.: 56.25%] [G loss: 0.445330]\n",
      "epoch:16 step:15309 [D loss: 0.242799, acc.: 60.16%] [G loss: 0.409432]\n",
      "epoch:16 step:15310 [D loss: 0.219615, acc.: 63.28%] [G loss: 0.413343]\n",
      "epoch:16 step:15311 [D loss: 0.232836, acc.: 60.16%] [G loss: 0.407948]\n",
      "epoch:16 step:15312 [D loss: 0.239597, acc.: 57.03%] [G loss: 0.423598]\n",
      "epoch:16 step:15313 [D loss: 0.204475, acc.: 66.41%] [G loss: 0.452435]\n",
      "epoch:16 step:15314 [D loss: 0.203824, acc.: 75.00%] [G loss: 0.464851]\n",
      "epoch:16 step:15315 [D loss: 0.271412, acc.: 52.34%] [G loss: 0.388836]\n",
      "epoch:16 step:15316 [D loss: 0.227131, acc.: 62.50%] [G loss: 0.402040]\n",
      "epoch:16 step:15317 [D loss: 0.229373, acc.: 62.50%] [G loss: 0.461582]\n",
      "epoch:16 step:15318 [D loss: 0.237209, acc.: 60.94%] [G loss: 0.408058]\n",
      "epoch:16 step:15319 [D loss: 0.254881, acc.: 61.72%] [G loss: 0.413844]\n",
      "epoch:16 step:15320 [D loss: 0.223141, acc.: 63.28%] [G loss: 0.457027]\n",
      "epoch:16 step:15321 [D loss: 0.212069, acc.: 68.75%] [G loss: 0.452166]\n",
      "epoch:16 step:15322 [D loss: 0.210466, acc.: 64.06%] [G loss: 0.416833]\n",
      "epoch:16 step:15323 [D loss: 0.212809, acc.: 60.94%] [G loss: 0.419451]\n",
      "epoch:16 step:15324 [D loss: 0.216461, acc.: 64.06%] [G loss: 0.453411]\n",
      "epoch:16 step:15325 [D loss: 0.210533, acc.: 69.53%] [G loss: 0.477918]\n",
      "epoch:16 step:15326 [D loss: 0.226255, acc.: 66.41%] [G loss: 0.467763]\n",
      "epoch:16 step:15327 [D loss: 0.200641, acc.: 67.97%] [G loss: 0.494082]\n",
      "epoch:16 step:15328 [D loss: 0.204578, acc.: 70.31%] [G loss: 0.462525]\n",
      "epoch:16 step:15329 [D loss: 0.244315, acc.: 57.03%] [G loss: 0.431387]\n",
      "epoch:16 step:15330 [D loss: 0.211439, acc.: 67.97%] [G loss: 0.410784]\n",
      "epoch:16 step:15331 [D loss: 0.222299, acc.: 69.53%] [G loss: 0.446806]\n",
      "epoch:16 step:15332 [D loss: 0.205890, acc.: 67.97%] [G loss: 0.443504]\n",
      "epoch:16 step:15333 [D loss: 0.287262, acc.: 51.56%] [G loss: 0.460752]\n",
      "epoch:16 step:15334 [D loss: 0.235623, acc.: 64.06%] [G loss: 0.476074]\n",
      "epoch:16 step:15335 [D loss: 0.225900, acc.: 64.84%] [G loss: 0.477673]\n",
      "epoch:16 step:15336 [D loss: 0.191352, acc.: 69.53%] [G loss: 0.494720]\n",
      "epoch:16 step:15337 [D loss: 0.222422, acc.: 64.06%] [G loss: 0.449345]\n",
      "epoch:16 step:15338 [D loss: 0.231587, acc.: 62.50%] [G loss: 0.451263]\n",
      "epoch:16 step:15339 [D loss: 0.169818, acc.: 74.22%] [G loss: 0.524506]\n",
      "epoch:16 step:15340 [D loss: 0.262702, acc.: 60.16%] [G loss: 0.465328]\n",
      "epoch:16 step:15341 [D loss: 0.258820, acc.: 56.25%] [G loss: 0.391715]\n",
      "epoch:16 step:15342 [D loss: 0.223240, acc.: 62.50%] [G loss: 0.410914]\n",
      "epoch:16 step:15343 [D loss: 0.223402, acc.: 60.94%] [G loss: 0.432770]\n",
      "epoch:16 step:15344 [D loss: 0.238024, acc.: 60.16%] [G loss: 0.431187]\n",
      "epoch:16 step:15345 [D loss: 0.214839, acc.: 66.41%] [G loss: 0.485528]\n",
      "epoch:16 step:15346 [D loss: 0.178079, acc.: 75.78%] [G loss: 0.474435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15347 [D loss: 0.223372, acc.: 63.28%] [G loss: 0.454393]\n",
      "epoch:16 step:15348 [D loss: 0.242877, acc.: 54.69%] [G loss: 0.425605]\n",
      "epoch:16 step:15349 [D loss: 0.195220, acc.: 69.53%] [G loss: 0.447791]\n",
      "epoch:16 step:15350 [D loss: 0.213599, acc.: 65.62%] [G loss: 0.447091]\n",
      "epoch:16 step:15351 [D loss: 0.198031, acc.: 68.75%] [G loss: 0.449575]\n",
      "epoch:16 step:15352 [D loss: 0.197764, acc.: 67.19%] [G loss: 0.503160]\n",
      "epoch:16 step:15353 [D loss: 0.208044, acc.: 67.19%] [G loss: 0.449933]\n",
      "epoch:16 step:15354 [D loss: 0.214692, acc.: 65.62%] [G loss: 0.428621]\n",
      "epoch:16 step:15355 [D loss: 0.227118, acc.: 61.72%] [G loss: 0.459328]\n",
      "epoch:16 step:15356 [D loss: 0.236411, acc.: 56.25%] [G loss: 0.437810]\n",
      "epoch:16 step:15357 [D loss: 0.237476, acc.: 60.16%] [G loss: 0.460439]\n",
      "epoch:16 step:15358 [D loss: 0.228385, acc.: 63.28%] [G loss: 0.439250]\n",
      "epoch:16 step:15359 [D loss: 0.212315, acc.: 67.19%] [G loss: 0.460923]\n",
      "epoch:16 step:15360 [D loss: 0.256672, acc.: 54.69%] [G loss: 0.422117]\n",
      "epoch:16 step:15361 [D loss: 0.251651, acc.: 53.12%] [G loss: 0.402879]\n",
      "epoch:16 step:15362 [D loss: 0.219630, acc.: 64.84%] [G loss: 0.436061]\n",
      "epoch:16 step:15363 [D loss: 0.202658, acc.: 66.41%] [G loss: 0.469446]\n",
      "epoch:16 step:15364 [D loss: 0.206160, acc.: 71.09%] [G loss: 0.459225]\n",
      "epoch:16 step:15365 [D loss: 0.225396, acc.: 67.19%] [G loss: 0.460522]\n",
      "epoch:16 step:15366 [D loss: 0.191948, acc.: 68.75%] [G loss: 0.479836]\n",
      "epoch:16 step:15367 [D loss: 0.258749, acc.: 50.78%] [G loss: 0.427336]\n",
      "epoch:16 step:15368 [D loss: 0.252544, acc.: 53.91%] [G loss: 0.440012]\n",
      "epoch:16 step:15369 [D loss: 0.255346, acc.: 53.12%] [G loss: 0.356244]\n",
      "epoch:16 step:15370 [D loss: 0.243343, acc.: 55.47%] [G loss: 0.383337]\n",
      "epoch:16 step:15371 [D loss: 0.224991, acc.: 68.75%] [G loss: 0.429605]\n",
      "epoch:16 step:15372 [D loss: 0.234157, acc.: 60.16%] [G loss: 0.439556]\n",
      "epoch:16 step:15373 [D loss: 0.196282, acc.: 71.09%] [G loss: 0.422685]\n",
      "epoch:16 step:15374 [D loss: 0.229598, acc.: 58.59%] [G loss: 0.381273]\n",
      "epoch:16 step:15375 [D loss: 0.232934, acc.: 60.94%] [G loss: 0.408605]\n",
      "epoch:16 step:15376 [D loss: 0.216217, acc.: 62.50%] [G loss: 0.414326]\n",
      "epoch:16 step:15377 [D loss: 0.194941, acc.: 67.19%] [G loss: 0.465571]\n",
      "epoch:16 step:15378 [D loss: 0.255684, acc.: 51.56%] [G loss: 0.423733]\n",
      "epoch:16 step:15379 [D loss: 0.232490, acc.: 59.38%] [G loss: 0.443273]\n",
      "epoch:16 step:15380 [D loss: 0.222633, acc.: 61.72%] [G loss: 0.454591]\n",
      "epoch:16 step:15381 [D loss: 0.240237, acc.: 57.81%] [G loss: 0.409227]\n",
      "epoch:16 step:15382 [D loss: 0.250037, acc.: 57.03%] [G loss: 0.411316]\n",
      "epoch:16 step:15383 [D loss: 0.232180, acc.: 60.16%] [G loss: 0.420965]\n",
      "epoch:16 step:15384 [D loss: 0.214338, acc.: 65.62%] [G loss: 0.413233]\n",
      "epoch:16 step:15385 [D loss: 0.216723, acc.: 65.62%] [G loss: 0.425155]\n",
      "epoch:16 step:15386 [D loss: 0.225463, acc.: 60.94%] [G loss: 0.466996]\n",
      "epoch:16 step:15387 [D loss: 0.209222, acc.: 67.97%] [G loss: 0.428792]\n",
      "epoch:16 step:15388 [D loss: 0.263428, acc.: 50.00%] [G loss: 0.411785]\n",
      "epoch:16 step:15389 [D loss: 0.233847, acc.: 65.62%] [G loss: 0.453764]\n",
      "epoch:16 step:15390 [D loss: 0.198403, acc.: 68.75%] [G loss: 0.433933]\n",
      "epoch:16 step:15391 [D loss: 0.209412, acc.: 68.75%] [G loss: 0.494606]\n",
      "epoch:16 step:15392 [D loss: 0.253893, acc.: 53.12%] [G loss: 0.406521]\n",
      "epoch:16 step:15393 [D loss: 0.235744, acc.: 54.69%] [G loss: 0.425709]\n",
      "epoch:16 step:15394 [D loss: 0.214430, acc.: 65.62%] [G loss: 0.392597]\n",
      "epoch:16 step:15395 [D loss: 0.232463, acc.: 59.38%] [G loss: 0.411627]\n",
      "epoch:16 step:15396 [D loss: 0.239284, acc.: 61.72%] [G loss: 0.429872]\n",
      "epoch:16 step:15397 [D loss: 0.209458, acc.: 68.75%] [G loss: 0.442513]\n",
      "epoch:16 step:15398 [D loss: 0.192038, acc.: 71.09%] [G loss: 0.498914]\n",
      "epoch:16 step:15399 [D loss: 0.220885, acc.: 67.97%] [G loss: 0.508683]\n",
      "epoch:16 step:15400 [D loss: 0.241051, acc.: 60.94%] [G loss: 0.410231]\n",
      "##############\n",
      "[2.56353323 1.99426332 6.15463288 4.87121314 3.67245659 5.80551041\n",
      " 4.44316377 4.85555064 4.30271411 4.08447991]\n",
      "##########\n",
      "epoch:16 step:15401 [D loss: 0.217407, acc.: 63.28%] [G loss: 0.411838]\n",
      "epoch:16 step:15402 [D loss: 0.242993, acc.: 56.25%] [G loss: 0.435279]\n",
      "epoch:16 step:15403 [D loss: 0.246435, acc.: 64.06%] [G loss: 0.443704]\n",
      "epoch:16 step:15404 [D loss: 0.212506, acc.: 59.38%] [G loss: 0.410793]\n",
      "epoch:16 step:15405 [D loss: 0.259774, acc.: 53.91%] [G loss: 0.442815]\n",
      "epoch:16 step:15406 [D loss: 0.215078, acc.: 68.75%] [G loss: 0.435258]\n",
      "epoch:16 step:15407 [D loss: 0.231151, acc.: 65.62%] [G loss: 0.473655]\n",
      "epoch:16 step:15408 [D loss: 0.191496, acc.: 71.09%] [G loss: 0.488019]\n",
      "epoch:16 step:15409 [D loss: 0.243053, acc.: 58.59%] [G loss: 0.451748]\n",
      "epoch:16 step:15410 [D loss: 0.253470, acc.: 53.12%] [G loss: 0.412345]\n",
      "epoch:16 step:15411 [D loss: 0.234355, acc.: 60.94%] [G loss: 0.434106]\n",
      "epoch:16 step:15412 [D loss: 0.217570, acc.: 64.84%] [G loss: 0.450645]\n",
      "epoch:16 step:15413 [D loss: 0.278533, acc.: 52.34%] [G loss: 0.366097]\n",
      "epoch:16 step:15414 [D loss: 0.236013, acc.: 55.47%] [G loss: 0.428752]\n",
      "epoch:16 step:15415 [D loss: 0.242371, acc.: 56.25%] [G loss: 0.430644]\n",
      "epoch:16 step:15416 [D loss: 0.239351, acc.: 56.25%] [G loss: 0.436847]\n",
      "epoch:16 step:15417 [D loss: 0.219891, acc.: 63.28%] [G loss: 0.406324]\n",
      "epoch:16 step:15418 [D loss: 0.226830, acc.: 60.94%] [G loss: 0.439523]\n",
      "epoch:16 step:15419 [D loss: 0.199885, acc.: 65.62%] [G loss: 0.426040]\n",
      "epoch:16 step:15420 [D loss: 0.202715, acc.: 66.41%] [G loss: 0.481886]\n",
      "epoch:16 step:15421 [D loss: 0.219040, acc.: 64.84%] [G loss: 0.447154]\n",
      "epoch:16 step:15422 [D loss: 0.197343, acc.: 70.31%] [G loss: 0.478603]\n",
      "epoch:16 step:15423 [D loss: 0.233960, acc.: 58.59%] [G loss: 0.460275]\n",
      "epoch:16 step:15424 [D loss: 0.226020, acc.: 60.94%] [G loss: 0.436357]\n",
      "epoch:16 step:15425 [D loss: 0.221665, acc.: 64.84%] [G loss: 0.456681]\n",
      "epoch:16 step:15426 [D loss: 0.215291, acc.: 66.41%] [G loss: 0.457309]\n",
      "epoch:16 step:15427 [D loss: 0.188235, acc.: 75.00%] [G loss: 0.461341]\n",
      "epoch:16 step:15428 [D loss: 0.203571, acc.: 64.06%] [G loss: 0.443165]\n",
      "epoch:16 step:15429 [D loss: 0.261068, acc.: 53.91%] [G loss: 0.419318]\n",
      "epoch:16 step:15430 [D loss: 0.236161, acc.: 57.03%] [G loss: 0.428590]\n",
      "epoch:16 step:15431 [D loss: 0.218467, acc.: 70.31%] [G loss: 0.443173]\n",
      "epoch:16 step:15432 [D loss: 0.213193, acc.: 62.50%] [G loss: 0.457254]\n",
      "epoch:16 step:15433 [D loss: 0.218821, acc.: 66.41%] [G loss: 0.452638]\n",
      "epoch:16 step:15434 [D loss: 0.240845, acc.: 53.12%] [G loss: 0.459507]\n",
      "epoch:16 step:15435 [D loss: 0.230719, acc.: 60.94%] [G loss: 0.431372]\n",
      "epoch:16 step:15436 [D loss: 0.234977, acc.: 61.72%] [G loss: 0.444421]\n",
      "epoch:16 step:15437 [D loss: 0.217158, acc.: 66.41%] [G loss: 0.480831]\n",
      "epoch:16 step:15438 [D loss: 0.238870, acc.: 60.16%] [G loss: 0.436106]\n",
      "epoch:16 step:15439 [D loss: 0.234356, acc.: 62.50%] [G loss: 0.413190]\n",
      "epoch:16 step:15440 [D loss: 0.256900, acc.: 54.69%] [G loss: 0.439431]\n",
      "epoch:16 step:15441 [D loss: 0.219727, acc.: 60.94%] [G loss: 0.423520]\n",
      "epoch:16 step:15442 [D loss: 0.196925, acc.: 69.53%] [G loss: 0.462168]\n",
      "epoch:16 step:15443 [D loss: 0.189741, acc.: 75.00%] [G loss: 0.445672]\n",
      "epoch:16 step:15444 [D loss: 0.222276, acc.: 60.94%] [G loss: 0.444963]\n",
      "epoch:16 step:15445 [D loss: 0.197791, acc.: 70.31%] [G loss: 0.453254]\n",
      "epoch:16 step:15446 [D loss: 0.224774, acc.: 60.94%] [G loss: 0.449423]\n",
      "epoch:16 step:15447 [D loss: 0.236679, acc.: 62.50%] [G loss: 0.452553]\n",
      "epoch:16 step:15448 [D loss: 0.245200, acc.: 56.25%] [G loss: 0.443605]\n",
      "epoch:16 step:15449 [D loss: 0.227742, acc.: 60.94%] [G loss: 0.476388]\n",
      "epoch:16 step:15450 [D loss: 0.269762, acc.: 53.91%] [G loss: 0.442824]\n",
      "epoch:16 step:15451 [D loss: 0.235708, acc.: 58.59%] [G loss: 0.417128]\n",
      "epoch:16 step:15452 [D loss: 0.239539, acc.: 63.28%] [G loss: 0.416313]\n",
      "epoch:16 step:15453 [D loss: 0.217054, acc.: 67.19%] [G loss: 0.418407]\n",
      "epoch:16 step:15454 [D loss: 0.289944, acc.: 49.22%] [G loss: 0.409506]\n",
      "epoch:16 step:15455 [D loss: 0.248556, acc.: 59.38%] [G loss: 0.441579]\n",
      "epoch:16 step:15456 [D loss: 0.212144, acc.: 61.72%] [G loss: 0.433929]\n",
      "epoch:16 step:15457 [D loss: 0.247387, acc.: 52.34%] [G loss: 0.406103]\n",
      "epoch:16 step:15458 [D loss: 0.211513, acc.: 67.97%] [G loss: 0.400564]\n",
      "epoch:16 step:15459 [D loss: 0.233574, acc.: 60.16%] [G loss: 0.420675]\n",
      "epoch:16 step:15460 [D loss: 0.201817, acc.: 74.22%] [G loss: 0.469845]\n",
      "epoch:16 step:15461 [D loss: 0.201115, acc.: 70.31%] [G loss: 0.462934]\n",
      "epoch:16 step:15462 [D loss: 0.213204, acc.: 63.28%] [G loss: 0.490554]\n",
      "epoch:16 step:15463 [D loss: 0.198243, acc.: 73.44%] [G loss: 0.465153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15464 [D loss: 0.203228, acc.: 66.41%] [G loss: 0.503484]\n",
      "epoch:16 step:15465 [D loss: 0.233392, acc.: 60.94%] [G loss: 0.485089]\n",
      "epoch:16 step:15466 [D loss: 0.205896, acc.: 66.41%] [G loss: 0.488324]\n",
      "epoch:16 step:15467 [D loss: 0.195907, acc.: 71.88%] [G loss: 0.484165]\n",
      "epoch:16 step:15468 [D loss: 0.245625, acc.: 60.94%] [G loss: 0.459360]\n",
      "epoch:16 step:15469 [D loss: 0.268870, acc.: 53.91%] [G loss: 0.429549]\n",
      "epoch:16 step:15470 [D loss: 0.243715, acc.: 60.16%] [G loss: 0.433042]\n",
      "epoch:16 step:15471 [D loss: 0.234328, acc.: 57.81%] [G loss: 0.412888]\n",
      "epoch:16 step:15472 [D loss: 0.227049, acc.: 62.50%] [G loss: 0.425680]\n",
      "epoch:16 step:15473 [D loss: 0.170932, acc.: 76.56%] [G loss: 0.447172]\n",
      "epoch:16 step:15474 [D loss: 0.272690, acc.: 50.00%] [G loss: 0.400055]\n",
      "epoch:16 step:15475 [D loss: 0.245072, acc.: 60.16%] [G loss: 0.413845]\n",
      "epoch:16 step:15476 [D loss: 0.204738, acc.: 65.62%] [G loss: 0.467862]\n",
      "epoch:16 step:15477 [D loss: 0.205600, acc.: 69.53%] [G loss: 0.472640]\n",
      "epoch:16 step:15478 [D loss: 0.257411, acc.: 51.56%] [G loss: 0.457604]\n",
      "epoch:16 step:15479 [D loss: 0.236437, acc.: 53.91%] [G loss: 0.428736]\n",
      "epoch:16 step:15480 [D loss: 0.201182, acc.: 72.66%] [G loss: 0.461966]\n",
      "epoch:16 step:15481 [D loss: 0.253286, acc.: 57.03%] [G loss: 0.432059]\n",
      "epoch:16 step:15482 [D loss: 0.228230, acc.: 60.94%] [G loss: 0.408838]\n",
      "epoch:16 step:15483 [D loss: 0.225542, acc.: 61.72%] [G loss: 0.447736]\n",
      "epoch:16 step:15484 [D loss: 0.219909, acc.: 67.19%] [G loss: 0.455775]\n",
      "epoch:16 step:15485 [D loss: 0.217838, acc.: 62.50%] [G loss: 0.430607]\n",
      "epoch:16 step:15486 [D loss: 0.218057, acc.: 63.28%] [G loss: 0.407552]\n",
      "epoch:16 step:15487 [D loss: 0.193894, acc.: 73.44%] [G loss: 0.467043]\n",
      "epoch:16 step:15488 [D loss: 0.207769, acc.: 70.31%] [G loss: 0.443568]\n",
      "epoch:16 step:15489 [D loss: 0.225063, acc.: 61.72%] [G loss: 0.439646]\n",
      "epoch:16 step:15490 [D loss: 0.226839, acc.: 61.72%] [G loss: 0.425946]\n",
      "epoch:16 step:15491 [D loss: 0.211452, acc.: 71.88%] [G loss: 0.487150]\n",
      "epoch:16 step:15492 [D loss: 0.278451, acc.: 54.69%] [G loss: 0.405706]\n",
      "epoch:16 step:15493 [D loss: 0.267918, acc.: 53.91%] [G loss: 0.443246]\n",
      "epoch:16 step:15494 [D loss: 0.225804, acc.: 57.03%] [G loss: 0.434388]\n",
      "epoch:16 step:15495 [D loss: 0.219200, acc.: 63.28%] [G loss: 0.447351]\n",
      "epoch:16 step:15496 [D loss: 0.198658, acc.: 69.53%] [G loss: 0.444270]\n",
      "epoch:16 step:15497 [D loss: 0.205185, acc.: 69.53%] [G loss: 0.483552]\n",
      "epoch:16 step:15498 [D loss: 0.235064, acc.: 60.94%] [G loss: 0.480165]\n",
      "epoch:16 step:15499 [D loss: 0.198765, acc.: 70.31%] [G loss: 0.449118]\n",
      "epoch:16 step:15500 [D loss: 0.232368, acc.: 62.50%] [G loss: 0.489621]\n",
      "epoch:16 step:15501 [D loss: 0.258090, acc.: 60.16%] [G loss: 0.462178]\n",
      "epoch:16 step:15502 [D loss: 0.214133, acc.: 65.62%] [G loss: 0.456212]\n",
      "epoch:16 step:15503 [D loss: 0.200366, acc.: 66.41%] [G loss: 0.446387]\n",
      "epoch:16 step:15504 [D loss: 0.209422, acc.: 67.19%] [G loss: 0.450123]\n",
      "epoch:16 step:15505 [D loss: 0.198625, acc.: 65.62%] [G loss: 0.486413]\n",
      "epoch:16 step:15506 [D loss: 0.201589, acc.: 68.75%] [G loss: 0.466993]\n",
      "epoch:16 step:15507 [D loss: 0.241802, acc.: 63.28%] [G loss: 0.452508]\n",
      "epoch:16 step:15508 [D loss: 0.212595, acc.: 66.41%] [G loss: 0.469183]\n",
      "epoch:16 step:15509 [D loss: 0.235344, acc.: 64.84%] [G loss: 0.426499]\n",
      "epoch:16 step:15510 [D loss: 0.238635, acc.: 61.72%] [G loss: 0.462480]\n",
      "epoch:16 step:15511 [D loss: 0.176452, acc.: 72.66%] [G loss: 0.471701]\n",
      "epoch:16 step:15512 [D loss: 0.216474, acc.: 64.06%] [G loss: 0.443308]\n",
      "epoch:16 step:15513 [D loss: 0.210205, acc.: 68.75%] [G loss: 0.475365]\n",
      "epoch:16 step:15514 [D loss: 0.200963, acc.: 68.75%] [G loss: 0.487795]\n",
      "epoch:16 step:15515 [D loss: 0.237186, acc.: 60.16%] [G loss: 0.453597]\n",
      "epoch:16 step:15516 [D loss: 0.221632, acc.: 68.75%] [G loss: 0.451735]\n",
      "epoch:16 step:15517 [D loss: 0.218726, acc.: 65.62%] [G loss: 0.484175]\n",
      "epoch:16 step:15518 [D loss: 0.234712, acc.: 57.81%] [G loss: 0.464765]\n",
      "epoch:16 step:15519 [D loss: 0.232717, acc.: 60.16%] [G loss: 0.426493]\n",
      "epoch:16 step:15520 [D loss: 0.271974, acc.: 53.12%] [G loss: 0.476968]\n",
      "epoch:16 step:15521 [D loss: 0.245018, acc.: 53.91%] [G loss: 0.448058]\n",
      "epoch:16 step:15522 [D loss: 0.213967, acc.: 67.97%] [G loss: 0.448165]\n",
      "epoch:16 step:15523 [D loss: 0.269134, acc.: 52.34%] [G loss: 0.387915]\n",
      "epoch:16 step:15524 [D loss: 0.200837, acc.: 71.09%] [G loss: 0.460554]\n",
      "epoch:16 step:15525 [D loss: 0.233388, acc.: 57.81%] [G loss: 0.407616]\n",
      "epoch:16 step:15526 [D loss: 0.178392, acc.: 75.00%] [G loss: 0.455117]\n",
      "epoch:16 step:15527 [D loss: 0.247147, acc.: 55.47%] [G loss: 0.415844]\n",
      "epoch:16 step:15528 [D loss: 0.215917, acc.: 67.19%] [G loss: 0.417772]\n",
      "epoch:16 step:15529 [D loss: 0.239845, acc.: 64.06%] [G loss: 0.420198]\n",
      "epoch:16 step:15530 [D loss: 0.240321, acc.: 57.81%] [G loss: 0.456101]\n",
      "epoch:16 step:15531 [D loss: 0.224498, acc.: 58.59%] [G loss: 0.450766]\n",
      "epoch:16 step:15532 [D loss: 0.223613, acc.: 64.84%] [G loss: 0.453153]\n",
      "epoch:16 step:15533 [D loss: 0.220996, acc.: 64.06%] [G loss: 0.443106]\n",
      "epoch:16 step:15534 [D loss: 0.293688, acc.: 45.31%] [G loss: 0.395960]\n",
      "epoch:16 step:15535 [D loss: 0.244073, acc.: 62.50%] [G loss: 0.388482]\n",
      "epoch:16 step:15536 [D loss: 0.249516, acc.: 54.69%] [G loss: 0.390895]\n",
      "epoch:16 step:15537 [D loss: 0.236974, acc.: 59.38%] [G loss: 0.443520]\n",
      "epoch:16 step:15538 [D loss: 0.221689, acc.: 62.50%] [G loss: 0.445224]\n",
      "epoch:16 step:15539 [D loss: 0.233099, acc.: 59.38%] [G loss: 0.422458]\n",
      "epoch:16 step:15540 [D loss: 0.223379, acc.: 63.28%] [G loss: 0.434329]\n",
      "epoch:16 step:15541 [D loss: 0.210816, acc.: 66.41%] [G loss: 0.437653]\n",
      "epoch:16 step:15542 [D loss: 0.214350, acc.: 60.94%] [G loss: 0.428961]\n",
      "epoch:16 step:15543 [D loss: 0.202128, acc.: 67.19%] [G loss: 0.475530]\n",
      "epoch:16 step:15544 [D loss: 0.214275, acc.: 66.41%] [G loss: 0.422685]\n",
      "epoch:16 step:15545 [D loss: 0.240773, acc.: 64.06%] [G loss: 0.453957]\n",
      "epoch:16 step:15546 [D loss: 0.210995, acc.: 66.41%] [G loss: 0.479405]\n",
      "epoch:16 step:15547 [D loss: 0.217849, acc.: 62.50%] [G loss: 0.447032]\n",
      "epoch:16 step:15548 [D loss: 0.187175, acc.: 75.00%] [G loss: 0.472771]\n",
      "epoch:16 step:15549 [D loss: 0.215482, acc.: 64.06%] [G loss: 0.488520]\n",
      "epoch:16 step:15550 [D loss: 0.213896, acc.: 64.84%] [G loss: 0.483513]\n",
      "epoch:16 step:15551 [D loss: 0.250111, acc.: 53.91%] [G loss: 0.423353]\n",
      "epoch:16 step:15552 [D loss: 0.245846, acc.: 59.38%] [G loss: 0.413421]\n",
      "epoch:16 step:15553 [D loss: 0.206745, acc.: 64.84%] [G loss: 0.473415]\n",
      "epoch:16 step:15554 [D loss: 0.243573, acc.: 56.25%] [G loss: 0.426669]\n",
      "epoch:16 step:15555 [D loss: 0.189463, acc.: 71.09%] [G loss: 0.435601]\n",
      "epoch:16 step:15556 [D loss: 0.179404, acc.: 75.00%] [G loss: 0.451885]\n",
      "epoch:16 step:15557 [D loss: 0.234678, acc.: 58.59%] [G loss: 0.502396]\n",
      "epoch:16 step:15558 [D loss: 0.245401, acc.: 57.81%] [G loss: 0.464578]\n",
      "epoch:16 step:15559 [D loss: 0.228717, acc.: 61.72%] [G loss: 0.464832]\n",
      "epoch:16 step:15560 [D loss: 0.222334, acc.: 64.84%] [G loss: 0.461568]\n",
      "epoch:16 step:15561 [D loss: 0.279637, acc.: 50.78%] [G loss: 0.414306]\n",
      "epoch:16 step:15562 [D loss: 0.215790, acc.: 61.72%] [G loss: 0.405942]\n",
      "epoch:16 step:15563 [D loss: 0.242801, acc.: 59.38%] [G loss: 0.405838]\n",
      "epoch:16 step:15564 [D loss: 0.219305, acc.: 65.62%] [G loss: 0.417223]\n",
      "epoch:16 step:15565 [D loss: 0.207426, acc.: 68.75%] [G loss: 0.463803]\n",
      "epoch:16 step:15566 [D loss: 0.197174, acc.: 69.53%] [G loss: 0.476358]\n",
      "epoch:16 step:15567 [D loss: 0.178993, acc.: 73.44%] [G loss: 0.491131]\n",
      "epoch:16 step:15568 [D loss: 0.241614, acc.: 59.38%] [G loss: 0.436028]\n",
      "epoch:16 step:15569 [D loss: 0.214784, acc.: 67.97%] [G loss: 0.409706]\n",
      "epoch:16 step:15570 [D loss: 0.219080, acc.: 60.16%] [G loss: 0.425178]\n",
      "epoch:16 step:15571 [D loss: 0.235537, acc.: 60.16%] [G loss: 0.393854]\n",
      "epoch:16 step:15572 [D loss: 0.227175, acc.: 64.84%] [G loss: 0.418229]\n",
      "epoch:16 step:15573 [D loss: 0.218753, acc.: 60.16%] [G loss: 0.406621]\n",
      "epoch:16 step:15574 [D loss: 0.193313, acc.: 65.62%] [G loss: 0.486238]\n",
      "epoch:16 step:15575 [D loss: 0.230563, acc.: 57.81%] [G loss: 0.490607]\n",
      "epoch:16 step:15576 [D loss: 0.210497, acc.: 65.62%] [G loss: 0.457188]\n",
      "epoch:16 step:15577 [D loss: 0.225552, acc.: 63.28%] [G loss: 0.436950]\n",
      "epoch:16 step:15578 [D loss: 0.232411, acc.: 60.94%] [G loss: 0.433562]\n",
      "epoch:16 step:15579 [D loss: 0.264498, acc.: 53.12%] [G loss: 0.367364]\n",
      "epoch:16 step:15580 [D loss: 0.220122, acc.: 68.75%] [G loss: 0.418252]\n",
      "epoch:16 step:15581 [D loss: 0.206612, acc.: 68.75%] [G loss: 0.452910]\n",
      "epoch:16 step:15582 [D loss: 0.246146, acc.: 54.69%] [G loss: 0.446849]\n",
      "epoch:16 step:15583 [D loss: 0.253437, acc.: 57.03%] [G loss: 0.414147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15584 [D loss: 0.192996, acc.: 74.22%] [G loss: 0.473336]\n",
      "epoch:16 step:15585 [D loss: 0.202831, acc.: 73.44%] [G loss: 0.465653]\n",
      "epoch:16 step:15586 [D loss: 0.240097, acc.: 60.94%] [G loss: 0.422739]\n",
      "epoch:16 step:15587 [D loss: 0.225651, acc.: 62.50%] [G loss: 0.420927]\n",
      "epoch:16 step:15588 [D loss: 0.242971, acc.: 56.25%] [G loss: 0.472053]\n",
      "epoch:16 step:15589 [D loss: 0.243383, acc.: 61.72%] [G loss: 0.440533]\n",
      "epoch:16 step:15590 [D loss: 0.216796, acc.: 64.06%] [G loss: 0.449976]\n",
      "epoch:16 step:15591 [D loss: 0.232265, acc.: 62.50%] [G loss: 0.421702]\n",
      "epoch:16 step:15592 [D loss: 0.244121, acc.: 61.72%] [G loss: 0.424578]\n",
      "epoch:16 step:15593 [D loss: 0.243863, acc.: 57.81%] [G loss: 0.413212]\n",
      "epoch:16 step:15594 [D loss: 0.240136, acc.: 56.25%] [G loss: 0.416505]\n",
      "epoch:16 step:15595 [D loss: 0.221575, acc.: 64.84%] [G loss: 0.442307]\n",
      "epoch:16 step:15596 [D loss: 0.205830, acc.: 65.62%] [G loss: 0.433209]\n",
      "epoch:16 step:15597 [D loss: 0.224384, acc.: 63.28%] [G loss: 0.408748]\n",
      "epoch:16 step:15598 [D loss: 0.231131, acc.: 57.03%] [G loss: 0.424321]\n",
      "epoch:16 step:15599 [D loss: 0.221493, acc.: 62.50%] [G loss: 0.444868]\n",
      "epoch:16 step:15600 [D loss: 0.214205, acc.: 66.41%] [G loss: 0.456438]\n",
      "##############\n",
      "[2.57299918 1.50623652 5.92957101 4.80977044 3.50892394 5.36984935\n",
      " 4.58742834 4.5459172  4.22721956 4.06223077]\n",
      "##########\n",
      "epoch:16 step:15601 [D loss: 0.241381, acc.: 57.03%] [G loss: 0.417541]\n",
      "epoch:16 step:15602 [D loss: 0.211668, acc.: 69.53%] [G loss: 0.457563]\n",
      "epoch:16 step:15603 [D loss: 0.229437, acc.: 60.94%] [G loss: 0.421746]\n",
      "epoch:16 step:15604 [D loss: 0.226233, acc.: 65.62%] [G loss: 0.408418]\n",
      "epoch:16 step:15605 [D loss: 0.200760, acc.: 71.09%] [G loss: 0.447728]\n",
      "epoch:16 step:15606 [D loss: 0.257162, acc.: 54.69%] [G loss: 0.400233]\n",
      "epoch:16 step:15607 [D loss: 0.248221, acc.: 54.69%] [G loss: 0.433914]\n",
      "epoch:16 step:15608 [D loss: 0.246092, acc.: 57.03%] [G loss: 0.420242]\n",
      "epoch:16 step:15609 [D loss: 0.213894, acc.: 60.94%] [G loss: 0.451921]\n",
      "epoch:16 step:15610 [D loss: 0.215746, acc.: 63.28%] [G loss: 0.442086]\n",
      "epoch:16 step:15611 [D loss: 0.228879, acc.: 60.16%] [G loss: 0.476186]\n",
      "epoch:16 step:15612 [D loss: 0.215434, acc.: 66.41%] [G loss: 0.445836]\n",
      "epoch:16 step:15613 [D loss: 0.259990, acc.: 51.56%] [G loss: 0.393673]\n",
      "epoch:16 step:15614 [D loss: 0.248336, acc.: 53.91%] [G loss: 0.427983]\n",
      "epoch:16 step:15615 [D loss: 0.205588, acc.: 67.97%] [G loss: 0.468030]\n",
      "epoch:16 step:15616 [D loss: 0.189479, acc.: 74.22%] [G loss: 0.462686]\n",
      "epoch:16 step:15617 [D loss: 0.250733, acc.: 49.22%] [G loss: 0.422402]\n",
      "epoch:16 step:15618 [D loss: 0.231827, acc.: 56.25%] [G loss: 0.455244]\n",
      "epoch:16 step:15619 [D loss: 0.214625, acc.: 65.62%] [G loss: 0.441175]\n",
      "epoch:16 step:15620 [D loss: 0.232609, acc.: 62.50%] [G loss: 0.420922]\n",
      "epoch:16 step:15621 [D loss: 0.187388, acc.: 75.00%] [G loss: 0.478785]\n",
      "epoch:16 step:15622 [D loss: 0.241873, acc.: 54.69%] [G loss: 0.439131]\n",
      "epoch:16 step:15623 [D loss: 0.187563, acc.: 71.09%] [G loss: 0.450755]\n",
      "epoch:16 step:15624 [D loss: 0.228579, acc.: 63.28%] [G loss: 0.491950]\n",
      "epoch:16 step:15625 [D loss: 0.222291, acc.: 61.72%] [G loss: 0.469070]\n",
      "epoch:16 step:15626 [D loss: 0.214650, acc.: 70.31%] [G loss: 0.469513]\n",
      "epoch:16 step:15627 [D loss: 0.206640, acc.: 60.94%] [G loss: 0.478687]\n",
      "epoch:16 step:15628 [D loss: 0.241025, acc.: 60.16%] [G loss: 0.429169]\n",
      "epoch:16 step:15629 [D loss: 0.231091, acc.: 62.50%] [G loss: 0.423171]\n",
      "epoch:16 step:15630 [D loss: 0.231164, acc.: 60.16%] [G loss: 0.425171]\n",
      "epoch:16 step:15631 [D loss: 0.251308, acc.: 52.34%] [G loss: 0.425633]\n",
      "epoch:16 step:15632 [D loss: 0.229485, acc.: 57.81%] [G loss: 0.447919]\n",
      "epoch:16 step:15633 [D loss: 0.202460, acc.: 64.06%] [G loss: 0.462035]\n",
      "epoch:16 step:15634 [D loss: 0.191542, acc.: 74.22%] [G loss: 0.529040]\n",
      "epoch:16 step:15635 [D loss: 0.229803, acc.: 59.38%] [G loss: 0.493372]\n",
      "epoch:16 step:15636 [D loss: 0.244680, acc.: 57.03%] [G loss: 0.424120]\n",
      "epoch:16 step:15637 [D loss: 0.237030, acc.: 62.50%] [G loss: 0.434947]\n",
      "epoch:16 step:15638 [D loss: 0.223871, acc.: 62.50%] [G loss: 0.411567]\n",
      "epoch:16 step:15639 [D loss: 0.195481, acc.: 71.09%] [G loss: 0.445658]\n",
      "epoch:16 step:15640 [D loss: 0.175596, acc.: 74.22%] [G loss: 0.480126]\n",
      "epoch:16 step:15641 [D loss: 0.208207, acc.: 69.53%] [G loss: 0.490156]\n",
      "epoch:16 step:15642 [D loss: 0.215561, acc.: 66.41%] [G loss: 0.483576]\n",
      "epoch:16 step:15643 [D loss: 0.225340, acc.: 64.84%] [G loss: 0.470872]\n",
      "epoch:16 step:15644 [D loss: 0.247047, acc.: 53.91%] [G loss: 0.407597]\n",
      "epoch:16 step:15645 [D loss: 0.225471, acc.: 61.72%] [G loss: 0.468463]\n",
      "epoch:16 step:15646 [D loss: 0.209248, acc.: 66.41%] [G loss: 0.486797]\n",
      "epoch:16 step:15647 [D loss: 0.250338, acc.: 53.12%] [G loss: 0.418689]\n",
      "epoch:16 step:15648 [D loss: 0.216167, acc.: 65.62%] [G loss: 0.437660]\n",
      "epoch:16 step:15649 [D loss: 0.245214, acc.: 58.59%] [G loss: 0.429185]\n",
      "epoch:16 step:15650 [D loss: 0.219641, acc.: 61.72%] [G loss: 0.426193]\n",
      "epoch:16 step:15651 [D loss: 0.207695, acc.: 66.41%] [G loss: 0.468376]\n",
      "epoch:16 step:15652 [D loss: 0.211939, acc.: 66.41%] [G loss: 0.431506]\n",
      "epoch:16 step:15653 [D loss: 0.215490, acc.: 65.62%] [G loss: 0.476380]\n",
      "epoch:16 step:15654 [D loss: 0.216162, acc.: 67.97%] [G loss: 0.470786]\n",
      "epoch:16 step:15655 [D loss: 0.253983, acc.: 55.47%] [G loss: 0.412232]\n",
      "epoch:16 step:15656 [D loss: 0.223137, acc.: 60.94%] [G loss: 0.462716]\n",
      "epoch:16 step:15657 [D loss: 0.227082, acc.: 62.50%] [G loss: 0.442086]\n",
      "epoch:16 step:15658 [D loss: 0.225471, acc.: 64.06%] [G loss: 0.467424]\n",
      "epoch:16 step:15659 [D loss: 0.241128, acc.: 65.62%] [G loss: 0.434374]\n",
      "epoch:16 step:15660 [D loss: 0.232684, acc.: 54.69%] [G loss: 0.421456]\n",
      "epoch:16 step:15661 [D loss: 0.220184, acc.: 64.84%] [G loss: 0.428944]\n",
      "epoch:16 step:15662 [D loss: 0.240980, acc.: 57.03%] [G loss: 0.448386]\n",
      "epoch:16 step:15663 [D loss: 0.196531, acc.: 72.66%] [G loss: 0.391393]\n",
      "epoch:16 step:15664 [D loss: 0.199062, acc.: 70.31%] [G loss: 0.458499]\n",
      "epoch:16 step:15665 [D loss: 0.239291, acc.: 62.50%] [G loss: 0.424391]\n",
      "epoch:16 step:15666 [D loss: 0.215357, acc.: 67.97%] [G loss: 0.464255]\n",
      "epoch:16 step:15667 [D loss: 0.231341, acc.: 65.62%] [G loss: 0.455882]\n",
      "epoch:16 step:15668 [D loss: 0.237775, acc.: 57.81%] [G loss: 0.388265]\n",
      "epoch:16 step:15669 [D loss: 0.196146, acc.: 71.88%] [G loss: 0.449145]\n",
      "epoch:16 step:15670 [D loss: 0.221283, acc.: 66.41%] [G loss: 0.444186]\n",
      "epoch:16 step:15671 [D loss: 0.199685, acc.: 71.09%] [G loss: 0.449056]\n",
      "epoch:16 step:15672 [D loss: 0.211047, acc.: 60.94%] [G loss: 0.422818]\n",
      "epoch:16 step:15673 [D loss: 0.187182, acc.: 73.44%] [G loss: 0.464717]\n",
      "epoch:16 step:15674 [D loss: 0.228534, acc.: 58.59%] [G loss: 0.428644]\n",
      "epoch:16 step:15675 [D loss: 0.216819, acc.: 59.38%] [G loss: 0.442382]\n",
      "epoch:16 step:15676 [D loss: 0.246776, acc.: 58.59%] [G loss: 0.415638]\n",
      "epoch:16 step:15677 [D loss: 0.225807, acc.: 63.28%] [G loss: 0.444481]\n",
      "epoch:16 step:15678 [D loss: 0.218980, acc.: 63.28%] [G loss: 0.450301]\n",
      "epoch:16 step:15679 [D loss: 0.231178, acc.: 60.94%] [G loss: 0.436191]\n",
      "epoch:16 step:15680 [D loss: 0.197213, acc.: 68.75%] [G loss: 0.451754]\n",
      "epoch:16 step:15681 [D loss: 0.213471, acc.: 68.75%] [G loss: 0.481745]\n",
      "epoch:16 step:15682 [D loss: 0.204669, acc.: 65.62%] [G loss: 0.482538]\n",
      "epoch:16 step:15683 [D loss: 0.201060, acc.: 67.19%] [G loss: 0.446284]\n",
      "epoch:16 step:15684 [D loss: 0.225579, acc.: 58.59%] [G loss: 0.457368]\n",
      "epoch:16 step:15685 [D loss: 0.222293, acc.: 66.41%] [G loss: 0.502968]\n",
      "epoch:16 step:15686 [D loss: 0.179980, acc.: 73.44%] [G loss: 0.516759]\n",
      "epoch:16 step:15687 [D loss: 0.199731, acc.: 71.09%] [G loss: 0.442023]\n",
      "epoch:16 step:15688 [D loss: 0.222679, acc.: 64.84%] [G loss: 0.439805]\n",
      "epoch:16 step:15689 [D loss: 0.215155, acc.: 62.50%] [G loss: 0.455506]\n",
      "epoch:16 step:15690 [D loss: 0.227589, acc.: 60.16%] [G loss: 0.444265]\n",
      "epoch:16 step:15691 [D loss: 0.222735, acc.: 61.72%] [G loss: 0.420700]\n",
      "epoch:16 step:15692 [D loss: 0.219957, acc.: 64.84%] [G loss: 0.492785]\n",
      "epoch:16 step:15693 [D loss: 0.218349, acc.: 67.19%] [G loss: 0.485549]\n",
      "epoch:16 step:15694 [D loss: 0.250041, acc.: 57.03%] [G loss: 0.448781]\n",
      "epoch:16 step:15695 [D loss: 0.238647, acc.: 53.91%] [G loss: 0.434114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15696 [D loss: 0.237355, acc.: 56.25%] [G loss: 0.428945]\n",
      "epoch:16 step:15697 [D loss: 0.232923, acc.: 59.38%] [G loss: 0.431133]\n",
      "epoch:16 step:15698 [D loss: 0.210207, acc.: 62.50%] [G loss: 0.409688]\n",
      "epoch:16 step:15699 [D loss: 0.221202, acc.: 59.38%] [G loss: 0.418317]\n",
      "epoch:16 step:15700 [D loss: 0.192545, acc.: 71.88%] [G loss: 0.486417]\n",
      "epoch:16 step:15701 [D loss: 0.216358, acc.: 67.97%] [G loss: 0.493336]\n",
      "epoch:16 step:15702 [D loss: 0.265425, acc.: 53.91%] [G loss: 0.450070]\n",
      "epoch:16 step:15703 [D loss: 0.216702, acc.: 61.72%] [G loss: 0.480206]\n",
      "epoch:16 step:15704 [D loss: 0.201259, acc.: 64.06%] [G loss: 0.476154]\n",
      "epoch:16 step:15705 [D loss: 0.240187, acc.: 57.81%] [G loss: 0.475122]\n",
      "epoch:16 step:15706 [D loss: 0.206803, acc.: 63.28%] [G loss: 0.437036]\n",
      "epoch:16 step:15707 [D loss: 0.235333, acc.: 57.81%] [G loss: 0.419926]\n",
      "epoch:16 step:15708 [D loss: 0.234415, acc.: 56.25%] [G loss: 0.408798]\n",
      "epoch:16 step:15709 [D loss: 0.225808, acc.: 67.97%] [G loss: 0.409372]\n",
      "epoch:16 step:15710 [D loss: 0.244438, acc.: 60.16%] [G loss: 0.455183]\n",
      "epoch:16 step:15711 [D loss: 0.200574, acc.: 67.97%] [G loss: 0.498934]\n",
      "epoch:16 step:15712 [D loss: 0.222310, acc.: 68.75%] [G loss: 0.481374]\n",
      "epoch:16 step:15713 [D loss: 0.226466, acc.: 58.59%] [G loss: 0.436155]\n",
      "epoch:16 step:15714 [D loss: 0.251265, acc.: 53.91%] [G loss: 0.367744]\n",
      "epoch:16 step:15715 [D loss: 0.198166, acc.: 71.09%] [G loss: 0.428404]\n",
      "epoch:16 step:15716 [D loss: 0.200728, acc.: 71.88%] [G loss: 0.468433]\n",
      "epoch:16 step:15717 [D loss: 0.228531, acc.: 63.28%] [G loss: 0.441651]\n",
      "epoch:16 step:15718 [D loss: 0.235929, acc.: 57.03%] [G loss: 0.419696]\n",
      "epoch:16 step:15719 [D loss: 0.232092, acc.: 59.38%] [G loss: 0.409905]\n",
      "epoch:16 step:15720 [D loss: 0.227355, acc.: 60.16%] [G loss: 0.435232]\n",
      "epoch:16 step:15721 [D loss: 0.216787, acc.: 58.59%] [G loss: 0.413542]\n",
      "epoch:16 step:15722 [D loss: 0.208010, acc.: 70.31%] [G loss: 0.484532]\n",
      "epoch:16 step:15723 [D loss: 0.219640, acc.: 65.62%] [G loss: 0.448538]\n",
      "epoch:16 step:15724 [D loss: 0.230854, acc.: 62.50%] [G loss: 0.426417]\n",
      "epoch:16 step:15725 [D loss: 0.229506, acc.: 64.06%] [G loss: 0.428141]\n",
      "epoch:16 step:15726 [D loss: 0.239975, acc.: 57.03%] [G loss: 0.411794]\n",
      "epoch:16 step:15727 [D loss: 0.227789, acc.: 61.72%] [G loss: 0.417625]\n",
      "epoch:16 step:15728 [D loss: 0.225198, acc.: 64.06%] [G loss: 0.443242]\n",
      "epoch:16 step:15729 [D loss: 0.205188, acc.: 71.09%] [G loss: 0.434029]\n",
      "epoch:16 step:15730 [D loss: 0.231845, acc.: 60.16%] [G loss: 0.439636]\n",
      "epoch:16 step:15731 [D loss: 0.235419, acc.: 61.72%] [G loss: 0.456151]\n",
      "epoch:16 step:15732 [D loss: 0.212792, acc.: 67.19%] [G loss: 0.442290]\n",
      "epoch:16 step:15733 [D loss: 0.249617, acc.: 63.28%] [G loss: 0.439695]\n",
      "epoch:16 step:15734 [D loss: 0.216819, acc.: 64.84%] [G loss: 0.446094]\n",
      "epoch:16 step:15735 [D loss: 0.221949, acc.: 66.41%] [G loss: 0.406012]\n",
      "epoch:16 step:15736 [D loss: 0.227774, acc.: 62.50%] [G loss: 0.428743]\n",
      "epoch:16 step:15737 [D loss: 0.238762, acc.: 57.81%] [G loss: 0.385657]\n",
      "epoch:16 step:15738 [D loss: 0.230226, acc.: 59.38%] [G loss: 0.416074]\n",
      "epoch:16 step:15739 [D loss: 0.222376, acc.: 65.62%] [G loss: 0.445759]\n",
      "epoch:16 step:15740 [D loss: 0.224894, acc.: 61.72%] [G loss: 0.418388]\n",
      "epoch:16 step:15741 [D loss: 0.228967, acc.: 61.72%] [G loss: 0.451415]\n",
      "epoch:16 step:15742 [D loss: 0.222231, acc.: 66.41%] [G loss: 0.428163]\n",
      "epoch:16 step:15743 [D loss: 0.228287, acc.: 59.38%] [G loss: 0.421338]\n",
      "epoch:16 step:15744 [D loss: 0.229383, acc.: 61.72%] [G loss: 0.415967]\n",
      "epoch:16 step:15745 [D loss: 0.233675, acc.: 64.84%] [G loss: 0.440230]\n",
      "epoch:16 step:15746 [D loss: 0.219811, acc.: 64.84%] [G loss: 0.421095]\n",
      "epoch:16 step:15747 [D loss: 0.223722, acc.: 67.97%] [G loss: 0.463867]\n",
      "epoch:16 step:15748 [D loss: 0.244346, acc.: 60.94%] [G loss: 0.463390]\n",
      "epoch:16 step:15749 [D loss: 0.245457, acc.: 58.59%] [G loss: 0.456350]\n",
      "epoch:16 step:15750 [D loss: 0.214293, acc.: 67.19%] [G loss: 0.482312]\n",
      "epoch:16 step:15751 [D loss: 0.267895, acc.: 52.34%] [G loss: 0.374272]\n",
      "epoch:16 step:15752 [D loss: 0.234701, acc.: 62.50%] [G loss: 0.441568]\n",
      "epoch:16 step:15753 [D loss: 0.233479, acc.: 60.94%] [G loss: 0.432017]\n",
      "epoch:16 step:15754 [D loss: 0.246201, acc.: 57.03%] [G loss: 0.410007]\n",
      "epoch:16 step:15755 [D loss: 0.242691, acc.: 60.94%] [G loss: 0.426202]\n",
      "epoch:16 step:15756 [D loss: 0.228390, acc.: 64.84%] [G loss: 0.431647]\n",
      "epoch:16 step:15757 [D loss: 0.240712, acc.: 57.03%] [G loss: 0.409142]\n",
      "epoch:16 step:15758 [D loss: 0.220589, acc.: 64.06%] [G loss: 0.448808]\n",
      "epoch:16 step:15759 [D loss: 0.216547, acc.: 61.72%] [G loss: 0.430613]\n",
      "epoch:16 step:15760 [D loss: 0.264998, acc.: 53.12%] [G loss: 0.434999]\n",
      "epoch:16 step:15761 [D loss: 0.212909, acc.: 66.41%] [G loss: 0.455739]\n",
      "epoch:16 step:15762 [D loss: 0.219162, acc.: 65.62%] [G loss: 0.458816]\n",
      "epoch:16 step:15763 [D loss: 0.219288, acc.: 61.72%] [G loss: 0.408486]\n",
      "epoch:16 step:15764 [D loss: 0.236055, acc.: 55.47%] [G loss: 0.432486]\n",
      "epoch:16 step:15765 [D loss: 0.221243, acc.: 65.62%] [G loss: 0.395260]\n",
      "epoch:16 step:15766 [D loss: 0.221669, acc.: 60.94%] [G loss: 0.459578]\n",
      "epoch:16 step:15767 [D loss: 0.213841, acc.: 62.50%] [G loss: 0.467926]\n",
      "epoch:16 step:15768 [D loss: 0.228355, acc.: 62.50%] [G loss: 0.427984]\n",
      "epoch:16 step:15769 [D loss: 0.230839, acc.: 63.28%] [G loss: 0.457446]\n",
      "epoch:16 step:15770 [D loss: 0.221080, acc.: 65.62%] [G loss: 0.461160]\n",
      "epoch:16 step:15771 [D loss: 0.236230, acc.: 64.06%] [G loss: 0.430764]\n",
      "epoch:16 step:15772 [D loss: 0.231298, acc.: 58.59%] [G loss: 0.396476]\n",
      "epoch:16 step:15773 [D loss: 0.189531, acc.: 70.31%] [G loss: 0.452465]\n",
      "epoch:16 step:15774 [D loss: 0.200602, acc.: 71.09%] [G loss: 0.480173]\n",
      "epoch:16 step:15775 [D loss: 0.247787, acc.: 58.59%] [G loss: 0.478623]\n",
      "epoch:16 step:15776 [D loss: 0.254356, acc.: 50.78%] [G loss: 0.427489]\n",
      "epoch:16 step:15777 [D loss: 0.209484, acc.: 67.97%] [G loss: 0.434251]\n",
      "epoch:16 step:15778 [D loss: 0.213329, acc.: 64.84%] [G loss: 0.435781]\n",
      "epoch:16 step:15779 [D loss: 0.251135, acc.: 53.91%] [G loss: 0.449837]\n",
      "epoch:16 step:15780 [D loss: 0.271761, acc.: 52.34%] [G loss: 0.399038]\n",
      "epoch:16 step:15781 [D loss: 0.220262, acc.: 63.28%] [G loss: 0.441410]\n",
      "epoch:16 step:15782 [D loss: 0.235254, acc.: 65.62%] [G loss: 0.438687]\n",
      "epoch:16 step:15783 [D loss: 0.270759, acc.: 50.00%] [G loss: 0.409637]\n",
      "epoch:16 step:15784 [D loss: 0.196419, acc.: 68.75%] [G loss: 0.399292]\n",
      "epoch:16 step:15785 [D loss: 0.213252, acc.: 63.28%] [G loss: 0.452790]\n",
      "epoch:16 step:15786 [D loss: 0.234633, acc.: 58.59%] [G loss: 0.493331]\n",
      "epoch:16 step:15787 [D loss: 0.225998, acc.: 65.62%] [G loss: 0.482372]\n",
      "epoch:16 step:15788 [D loss: 0.214050, acc.: 68.75%] [G loss: 0.454519]\n",
      "epoch:16 step:15789 [D loss: 0.258104, acc.: 54.69%] [G loss: 0.423680]\n",
      "epoch:16 step:15790 [D loss: 0.214183, acc.: 67.97%] [G loss: 0.419345]\n",
      "epoch:16 step:15791 [D loss: 0.216879, acc.: 64.84%] [G loss: 0.472464]\n",
      "epoch:16 step:15792 [D loss: 0.264625, acc.: 49.22%] [G loss: 0.389139]\n",
      "epoch:16 step:15793 [D loss: 0.211593, acc.: 69.53%] [G loss: 0.434139]\n",
      "epoch:16 step:15794 [D loss: 0.224350, acc.: 61.72%] [G loss: 0.437917]\n",
      "epoch:16 step:15795 [D loss: 0.208221, acc.: 68.75%] [G loss: 0.476729]\n",
      "epoch:16 step:15796 [D loss: 0.266773, acc.: 52.34%] [G loss: 0.418451]\n",
      "epoch:16 step:15797 [D loss: 0.218592, acc.: 65.62%] [G loss: 0.440365]\n",
      "epoch:16 step:15798 [D loss: 0.225103, acc.: 65.62%] [G loss: 0.407607]\n",
      "epoch:16 step:15799 [D loss: 0.218426, acc.: 63.28%] [G loss: 0.445908]\n",
      "epoch:16 step:15800 [D loss: 0.246672, acc.: 61.72%] [G loss: 0.427620]\n",
      "##############\n",
      "[2.45632132 1.58342672 5.96914235 4.70014    3.65100901 5.64323162\n",
      " 4.37594267 4.79323225 4.25095612 3.88965018]\n",
      "##########\n",
      "epoch:16 step:15801 [D loss: 0.228947, acc.: 60.94%] [G loss: 0.406966]\n",
      "epoch:16 step:15802 [D loss: 0.209209, acc.: 67.19%] [G loss: 0.428348]\n",
      "epoch:16 step:15803 [D loss: 0.231165, acc.: 64.06%] [G loss: 0.452908]\n",
      "epoch:16 step:15804 [D loss: 0.229602, acc.: 64.06%] [G loss: 0.401199]\n",
      "epoch:16 step:15805 [D loss: 0.213930, acc.: 66.41%] [G loss: 0.413772]\n",
      "epoch:16 step:15806 [D loss: 0.237674, acc.: 56.25%] [G loss: 0.427430]\n",
      "epoch:16 step:15807 [D loss: 0.199310, acc.: 65.62%] [G loss: 0.452226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15808 [D loss: 0.218850, acc.: 59.38%] [G loss: 0.475425]\n",
      "epoch:16 step:15809 [D loss: 0.242861, acc.: 59.38%] [G loss: 0.480566]\n",
      "epoch:16 step:15810 [D loss: 0.245427, acc.: 56.25%] [G loss: 0.382513]\n",
      "epoch:16 step:15811 [D loss: 0.228089, acc.: 64.06%] [G loss: 0.424982]\n",
      "epoch:16 step:15812 [D loss: 0.284468, acc.: 47.66%] [G loss: 0.426750]\n",
      "epoch:16 step:15813 [D loss: 0.216756, acc.: 66.41%] [G loss: 0.453452]\n",
      "epoch:16 step:15814 [D loss: 0.222529, acc.: 65.62%] [G loss: 0.425629]\n",
      "epoch:16 step:15815 [D loss: 0.203928, acc.: 71.09%] [G loss: 0.434577]\n",
      "epoch:16 step:15816 [D loss: 0.227201, acc.: 58.59%] [G loss: 0.441406]\n",
      "epoch:16 step:15817 [D loss: 0.221724, acc.: 61.72%] [G loss: 0.461155]\n",
      "epoch:16 step:15818 [D loss: 0.227172, acc.: 62.50%] [G loss: 0.459794]\n",
      "epoch:16 step:15819 [D loss: 0.232469, acc.: 57.81%] [G loss: 0.466683]\n",
      "epoch:16 step:15820 [D loss: 0.264746, acc.: 50.78%] [G loss: 0.432019]\n",
      "epoch:16 step:15821 [D loss: 0.250376, acc.: 53.91%] [G loss: 0.424913]\n",
      "epoch:16 step:15822 [D loss: 0.223650, acc.: 63.28%] [G loss: 0.397030]\n",
      "epoch:16 step:15823 [D loss: 0.230805, acc.: 64.06%] [G loss: 0.408708]\n",
      "epoch:16 step:15824 [D loss: 0.238428, acc.: 60.94%] [G loss: 0.460716]\n",
      "epoch:16 step:15825 [D loss: 0.204548, acc.: 67.97%] [G loss: 0.458115]\n",
      "epoch:16 step:15826 [D loss: 0.238702, acc.: 54.69%] [G loss: 0.422837]\n",
      "epoch:16 step:15827 [D loss: 0.216903, acc.: 67.19%] [G loss: 0.416773]\n",
      "epoch:16 step:15828 [D loss: 0.204860, acc.: 70.31%] [G loss: 0.396187]\n",
      "epoch:16 step:15829 [D loss: 0.216870, acc.: 65.62%] [G loss: 0.413133]\n",
      "epoch:16 step:15830 [D loss: 0.208927, acc.: 67.19%] [G loss: 0.439408]\n",
      "epoch:16 step:15831 [D loss: 0.204478, acc.: 65.62%] [G loss: 0.472244]\n",
      "epoch:16 step:15832 [D loss: 0.238878, acc.: 57.81%] [G loss: 0.418585]\n",
      "epoch:16 step:15833 [D loss: 0.230052, acc.: 64.84%] [G loss: 0.409621]\n",
      "epoch:16 step:15834 [D loss: 0.211016, acc.: 68.75%] [G loss: 0.427869]\n",
      "epoch:16 step:15835 [D loss: 0.213634, acc.: 66.41%] [G loss: 0.436879]\n",
      "epoch:16 step:15836 [D loss: 0.222995, acc.: 65.62%] [G loss: 0.447954]\n",
      "epoch:16 step:15837 [D loss: 0.229087, acc.: 62.50%] [G loss: 0.430577]\n",
      "epoch:16 step:15838 [D loss: 0.250042, acc.: 54.69%] [G loss: 0.440205]\n",
      "epoch:16 step:15839 [D loss: 0.258499, acc.: 51.56%] [G loss: 0.418683]\n",
      "epoch:16 step:15840 [D loss: 0.221046, acc.: 61.72%] [G loss: 0.400705]\n",
      "epoch:16 step:15841 [D loss: 0.203437, acc.: 66.41%] [G loss: 0.444533]\n",
      "epoch:16 step:15842 [D loss: 0.243064, acc.: 59.38%] [G loss: 0.417173]\n",
      "epoch:16 step:15843 [D loss: 0.242006, acc.: 56.25%] [G loss: 0.433778]\n",
      "epoch:16 step:15844 [D loss: 0.209417, acc.: 64.06%] [G loss: 0.445910]\n",
      "epoch:16 step:15845 [D loss: 0.223944, acc.: 65.62%] [G loss: 0.424758]\n",
      "epoch:16 step:15846 [D loss: 0.214874, acc.: 60.16%] [G loss: 0.472468]\n",
      "epoch:16 step:15847 [D loss: 0.242185, acc.: 57.03%] [G loss: 0.441821]\n",
      "epoch:16 step:15848 [D loss: 0.249534, acc.: 56.25%] [G loss: 0.433296]\n",
      "epoch:16 step:15849 [D loss: 0.204077, acc.: 71.09%] [G loss: 0.436773]\n",
      "epoch:16 step:15850 [D loss: 0.289379, acc.: 46.88%] [G loss: 0.441102]\n",
      "epoch:16 step:15851 [D loss: 0.209128, acc.: 70.31%] [G loss: 0.456722]\n",
      "epoch:16 step:15852 [D loss: 0.183405, acc.: 72.66%] [G loss: 0.458444]\n",
      "epoch:16 step:15853 [D loss: 0.232655, acc.: 64.06%] [G loss: 0.458341]\n",
      "epoch:16 step:15854 [D loss: 0.237071, acc.: 63.28%] [G loss: 0.361343]\n",
      "epoch:16 step:15855 [D loss: 0.213753, acc.: 64.06%] [G loss: 0.445593]\n",
      "epoch:16 step:15856 [D loss: 0.253300, acc.: 57.81%] [G loss: 0.401312]\n",
      "epoch:16 step:15857 [D loss: 0.240691, acc.: 60.16%] [G loss: 0.423356]\n",
      "epoch:16 step:15858 [D loss: 0.249753, acc.: 56.25%] [G loss: 0.417875]\n",
      "epoch:16 step:15859 [D loss: 0.231329, acc.: 57.81%] [G loss: 0.433748]\n",
      "epoch:16 step:15860 [D loss: 0.227612, acc.: 67.97%] [G loss: 0.427035]\n",
      "epoch:16 step:15861 [D loss: 0.245640, acc.: 54.69%] [G loss: 0.427210]\n",
      "epoch:16 step:15862 [D loss: 0.227698, acc.: 60.94%] [G loss: 0.423421]\n",
      "epoch:16 step:15863 [D loss: 0.200751, acc.: 69.53%] [G loss: 0.446818]\n",
      "epoch:16 step:15864 [D loss: 0.235246, acc.: 57.03%] [G loss: 0.405434]\n",
      "epoch:16 step:15865 [D loss: 0.235051, acc.: 59.38%] [G loss: 0.396623]\n",
      "epoch:16 step:15866 [D loss: 0.248471, acc.: 58.59%] [G loss: 0.412879]\n",
      "epoch:16 step:15867 [D loss: 0.192619, acc.: 71.09%] [G loss: 0.457910]\n",
      "epoch:16 step:15868 [D loss: 0.223086, acc.: 64.06%] [G loss: 0.433734]\n",
      "epoch:16 step:15869 [D loss: 0.219990, acc.: 60.16%] [G loss: 0.438455]\n",
      "epoch:16 step:15870 [D loss: 0.230804, acc.: 58.59%] [G loss: 0.423163]\n",
      "epoch:16 step:15871 [D loss: 0.253832, acc.: 57.03%] [G loss: 0.454927]\n",
      "epoch:16 step:15872 [D loss: 0.253187, acc.: 54.69%] [G loss: 0.451584]\n",
      "epoch:16 step:15873 [D loss: 0.233574, acc.: 62.50%] [G loss: 0.422182]\n",
      "epoch:16 step:15874 [D loss: 0.219398, acc.: 64.06%] [G loss: 0.420718]\n",
      "epoch:16 step:15875 [D loss: 0.229007, acc.: 66.41%] [G loss: 0.441940]\n",
      "epoch:16 step:15876 [D loss: 0.201800, acc.: 74.22%] [G loss: 0.449515]\n",
      "epoch:16 step:15877 [D loss: 0.201451, acc.: 70.31%] [G loss: 0.474303]\n",
      "epoch:16 step:15878 [D loss: 0.191909, acc.: 71.88%] [G loss: 0.483601]\n",
      "epoch:16 step:15879 [D loss: 0.217190, acc.: 67.97%] [G loss: 0.447723]\n",
      "epoch:16 step:15880 [D loss: 0.235364, acc.: 67.97%] [G loss: 0.406736]\n",
      "epoch:16 step:15881 [D loss: 0.207084, acc.: 74.22%] [G loss: 0.460203]\n",
      "epoch:16 step:15882 [D loss: 0.194169, acc.: 70.31%] [G loss: 0.469810]\n",
      "epoch:16 step:15883 [D loss: 0.257139, acc.: 52.34%] [G loss: 0.402787]\n",
      "epoch:16 step:15884 [D loss: 0.258292, acc.: 53.91%] [G loss: 0.430437]\n",
      "epoch:16 step:15885 [D loss: 0.211804, acc.: 68.75%] [G loss: 0.446730]\n",
      "epoch:16 step:15886 [D loss: 0.204008, acc.: 67.19%] [G loss: 0.459748]\n",
      "epoch:16 step:15887 [D loss: 0.213942, acc.: 67.19%] [G loss: 0.476984]\n",
      "epoch:16 step:15888 [D loss: 0.214582, acc.: 68.75%] [G loss: 0.478514]\n",
      "epoch:16 step:15889 [D loss: 0.198786, acc.: 70.31%] [G loss: 0.458267]\n",
      "epoch:16 step:15890 [D loss: 0.200559, acc.: 72.66%] [G loss: 0.477461]\n",
      "epoch:16 step:15891 [D loss: 0.183214, acc.: 71.88%] [G loss: 0.504995]\n",
      "epoch:16 step:15892 [D loss: 0.199258, acc.: 70.31%] [G loss: 0.463181]\n",
      "epoch:16 step:15893 [D loss: 0.231465, acc.: 61.72%] [G loss: 0.490890]\n",
      "epoch:16 step:15894 [D loss: 0.246192, acc.: 60.16%] [G loss: 0.448323]\n",
      "epoch:16 step:15895 [D loss: 0.218945, acc.: 66.41%] [G loss: 0.457076]\n",
      "epoch:16 step:15896 [D loss: 0.237026, acc.: 59.38%] [G loss: 0.414388]\n",
      "epoch:16 step:15897 [D loss: 0.187060, acc.: 74.22%] [G loss: 0.454009]\n",
      "epoch:16 step:15898 [D loss: 0.193746, acc.: 71.09%] [G loss: 0.503478]\n",
      "epoch:16 step:15899 [D loss: 0.238885, acc.: 57.03%] [G loss: 0.466078]\n",
      "epoch:16 step:15900 [D loss: 0.234985, acc.: 59.38%] [G loss: 0.449968]\n",
      "epoch:16 step:15901 [D loss: 0.214462, acc.: 64.84%] [G loss: 0.451918]\n",
      "epoch:16 step:15902 [D loss: 0.237930, acc.: 58.59%] [G loss: 0.484126]\n",
      "epoch:16 step:15903 [D loss: 0.213170, acc.: 67.97%] [G loss: 0.435817]\n",
      "epoch:16 step:15904 [D loss: 0.207637, acc.: 67.19%] [G loss: 0.506962]\n",
      "epoch:16 step:15905 [D loss: 0.219373, acc.: 61.72%] [G loss: 0.473329]\n",
      "epoch:16 step:15906 [D loss: 0.242351, acc.: 58.59%] [G loss: 0.439163]\n",
      "epoch:16 step:15907 [D loss: 0.297207, acc.: 49.22%] [G loss: 0.384008]\n",
      "epoch:16 step:15908 [D loss: 0.245105, acc.: 60.16%] [G loss: 0.425467]\n",
      "epoch:16 step:15909 [D loss: 0.213336, acc.: 68.75%] [G loss: 0.449967]\n",
      "epoch:16 step:15910 [D loss: 0.199581, acc.: 75.78%] [G loss: 0.443674]\n",
      "epoch:16 step:15911 [D loss: 0.197265, acc.: 69.53%] [G loss: 0.467398]\n",
      "epoch:16 step:15912 [D loss: 0.330981, acc.: 39.06%] [G loss: 0.419603]\n",
      "epoch:16 step:15913 [D loss: 0.211049, acc.: 65.62%] [G loss: 0.461733]\n",
      "epoch:16 step:15914 [D loss: 0.226156, acc.: 62.50%] [G loss: 0.418301]\n",
      "epoch:16 step:15915 [D loss: 0.221406, acc.: 66.41%] [G loss: 0.406653]\n",
      "epoch:16 step:15916 [D loss: 0.185170, acc.: 75.00%] [G loss: 0.456797]\n",
      "epoch:16 step:15917 [D loss: 0.187636, acc.: 72.66%] [G loss: 0.496077]\n",
      "epoch:16 step:15918 [D loss: 0.160607, acc.: 82.03%] [G loss: 0.520741]\n",
      "epoch:16 step:15919 [D loss: 0.235166, acc.: 63.28%] [G loss: 0.486179]\n",
      "epoch:16 step:15920 [D loss: 0.312697, acc.: 52.34%] [G loss: 0.539504]\n",
      "epoch:16 step:15921 [D loss: 0.265209, acc.: 56.25%] [G loss: 0.580026]\n",
      "epoch:16 step:15922 [D loss: 0.216324, acc.: 66.41%] [G loss: 0.510091]\n",
      "epoch:16 step:15923 [D loss: 0.249852, acc.: 52.34%] [G loss: 0.420671]\n",
      "epoch:16 step:15924 [D loss: 0.246965, acc.: 53.91%] [G loss: 0.426954]\n",
      "epoch:16 step:15925 [D loss: 0.220488, acc.: 67.97%] [G loss: 0.426050]\n",
      "epoch:16 step:15926 [D loss: 0.239567, acc.: 65.62%] [G loss: 0.433959]\n",
      "epoch:16 step:15927 [D loss: 0.204605, acc.: 69.53%] [G loss: 0.464421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15928 [D loss: 0.180275, acc.: 70.31%] [G loss: 0.545323]\n",
      "epoch:16 step:15929 [D loss: 0.206030, acc.: 71.09%] [G loss: 0.577240]\n",
      "epoch:17 step:15930 [D loss: 0.243550, acc.: 61.72%] [G loss: 0.507012]\n",
      "epoch:17 step:15931 [D loss: 0.271987, acc.: 56.25%] [G loss: 0.488587]\n",
      "epoch:17 step:15932 [D loss: 0.234075, acc.: 60.94%] [G loss: 0.462116]\n",
      "epoch:17 step:15933 [D loss: 0.239474, acc.: 57.81%] [G loss: 0.447905]\n",
      "epoch:17 step:15934 [D loss: 0.232816, acc.: 62.50%] [G loss: 0.436208]\n",
      "epoch:17 step:15935 [D loss: 0.209206, acc.: 65.62%] [G loss: 0.471819]\n",
      "epoch:17 step:15936 [D loss: 0.208245, acc.: 67.97%] [G loss: 0.477094]\n",
      "epoch:17 step:15937 [D loss: 0.217722, acc.: 64.06%] [G loss: 0.439688]\n",
      "epoch:17 step:15938 [D loss: 0.208869, acc.: 65.62%] [G loss: 0.494850]\n",
      "epoch:17 step:15939 [D loss: 0.232486, acc.: 59.38%] [G loss: 0.433656]\n",
      "epoch:17 step:15940 [D loss: 0.193326, acc.: 73.44%] [G loss: 0.445824]\n",
      "epoch:17 step:15941 [D loss: 0.236365, acc.: 60.16%] [G loss: 0.446758]\n",
      "epoch:17 step:15942 [D loss: 0.214494, acc.: 66.41%] [G loss: 0.464278]\n",
      "epoch:17 step:15943 [D loss: 0.215937, acc.: 60.94%] [G loss: 0.424973]\n",
      "epoch:17 step:15944 [D loss: 0.187637, acc.: 75.00%] [G loss: 0.470726]\n",
      "epoch:17 step:15945 [D loss: 0.200072, acc.: 70.31%] [G loss: 0.451653]\n",
      "epoch:17 step:15946 [D loss: 0.246182, acc.: 59.38%] [G loss: 0.445072]\n",
      "epoch:17 step:15947 [D loss: 0.249592, acc.: 51.56%] [G loss: 0.419188]\n",
      "epoch:17 step:15948 [D loss: 0.237981, acc.: 62.50%] [G loss: 0.453591]\n",
      "epoch:17 step:15949 [D loss: 0.264690, acc.: 51.56%] [G loss: 0.447645]\n",
      "epoch:17 step:15950 [D loss: 0.239856, acc.: 57.03%] [G loss: 0.454368]\n",
      "epoch:17 step:15951 [D loss: 0.207337, acc.: 67.97%] [G loss: 0.469373]\n",
      "epoch:17 step:15952 [D loss: 0.236938, acc.: 57.81%] [G loss: 0.424667]\n",
      "epoch:17 step:15953 [D loss: 0.232287, acc.: 61.72%] [G loss: 0.398943]\n",
      "epoch:17 step:15954 [D loss: 0.211152, acc.: 65.62%] [G loss: 0.432813]\n",
      "epoch:17 step:15955 [D loss: 0.225677, acc.: 59.38%] [G loss: 0.455623]\n",
      "epoch:17 step:15956 [D loss: 0.221832, acc.: 67.19%] [G loss: 0.449923]\n",
      "epoch:17 step:15957 [D loss: 0.220195, acc.: 64.84%] [G loss: 0.446762]\n",
      "epoch:17 step:15958 [D loss: 0.213412, acc.: 64.84%] [G loss: 0.448978]\n",
      "epoch:17 step:15959 [D loss: 0.219680, acc.: 63.28%] [G loss: 0.426571]\n",
      "epoch:17 step:15960 [D loss: 0.253169, acc.: 58.59%] [G loss: 0.421003]\n",
      "epoch:17 step:15961 [D loss: 0.248846, acc.: 55.47%] [G loss: 0.414301]\n",
      "epoch:17 step:15962 [D loss: 0.225734, acc.: 64.06%] [G loss: 0.429587]\n",
      "epoch:17 step:15963 [D loss: 0.225714, acc.: 66.41%] [G loss: 0.458202]\n",
      "epoch:17 step:15964 [D loss: 0.211837, acc.: 67.19%] [G loss: 0.472958]\n",
      "epoch:17 step:15965 [D loss: 0.211753, acc.: 65.62%] [G loss: 0.409985]\n",
      "epoch:17 step:15966 [D loss: 0.261547, acc.: 57.81%] [G loss: 0.429744]\n",
      "epoch:17 step:15967 [D loss: 0.233244, acc.: 57.81%] [G loss: 0.400272]\n",
      "epoch:17 step:15968 [D loss: 0.210505, acc.: 67.97%] [G loss: 0.439169]\n",
      "epoch:17 step:15969 [D loss: 0.212132, acc.: 63.28%] [G loss: 0.415697]\n",
      "epoch:17 step:15970 [D loss: 0.230286, acc.: 61.72%] [G loss: 0.454532]\n",
      "epoch:17 step:15971 [D loss: 0.210995, acc.: 71.09%] [G loss: 0.419782]\n",
      "epoch:17 step:15972 [D loss: 0.203899, acc.: 70.31%] [G loss: 0.455942]\n",
      "epoch:17 step:15973 [D loss: 0.240842, acc.: 60.16%] [G loss: 0.434707]\n",
      "epoch:17 step:15974 [D loss: 0.215186, acc.: 64.06%] [G loss: 0.485009]\n",
      "epoch:17 step:15975 [D loss: 0.225796, acc.: 59.38%] [G loss: 0.467843]\n",
      "epoch:17 step:15976 [D loss: 0.245412, acc.: 57.03%] [G loss: 0.376992]\n",
      "epoch:17 step:15977 [D loss: 0.207263, acc.: 71.09%] [G loss: 0.455816]\n",
      "epoch:17 step:15978 [D loss: 0.205921, acc.: 67.19%] [G loss: 0.444250]\n",
      "epoch:17 step:15979 [D loss: 0.202731, acc.: 71.09%] [G loss: 0.437225]\n",
      "epoch:17 step:15980 [D loss: 0.247284, acc.: 54.69%] [G loss: 0.400656]\n",
      "epoch:17 step:15981 [D loss: 0.239798, acc.: 63.28%] [G loss: 0.440916]\n",
      "epoch:17 step:15982 [D loss: 0.231155, acc.: 65.62%] [G loss: 0.435201]\n",
      "epoch:17 step:15983 [D loss: 0.232896, acc.: 60.16%] [G loss: 0.457040]\n",
      "epoch:17 step:15984 [D loss: 0.208959, acc.: 67.97%] [G loss: 0.436464]\n",
      "epoch:17 step:15985 [D loss: 0.222779, acc.: 63.28%] [G loss: 0.407995]\n",
      "epoch:17 step:15986 [D loss: 0.236381, acc.: 60.94%] [G loss: 0.420288]\n",
      "epoch:17 step:15987 [D loss: 0.222742, acc.: 66.41%] [G loss: 0.419263]\n",
      "epoch:17 step:15988 [D loss: 0.227914, acc.: 61.72%] [G loss: 0.455914]\n",
      "epoch:17 step:15989 [D loss: 0.223550, acc.: 64.84%] [G loss: 0.451716]\n",
      "epoch:17 step:15990 [D loss: 0.256120, acc.: 52.34%] [G loss: 0.412040]\n",
      "epoch:17 step:15991 [D loss: 0.235551, acc.: 57.81%] [G loss: 0.434974]\n",
      "epoch:17 step:15992 [D loss: 0.215121, acc.: 65.62%] [G loss: 0.431926]\n",
      "epoch:17 step:15993 [D loss: 0.212225, acc.: 72.66%] [G loss: 0.428978]\n",
      "epoch:17 step:15994 [D loss: 0.215998, acc.: 61.72%] [G loss: 0.453764]\n",
      "epoch:17 step:15995 [D loss: 0.224531, acc.: 61.72%] [G loss: 0.446590]\n",
      "epoch:17 step:15996 [D loss: 0.221357, acc.: 62.50%] [G loss: 0.441849]\n",
      "epoch:17 step:15997 [D loss: 0.238794, acc.: 59.38%] [G loss: 0.412860]\n",
      "epoch:17 step:15998 [D loss: 0.185236, acc.: 72.66%] [G loss: 0.435255]\n",
      "epoch:17 step:15999 [D loss: 0.197637, acc.: 70.31%] [G loss: 0.440939]\n",
      "epoch:17 step:16000 [D loss: 0.250128, acc.: 56.25%] [G loss: 0.404341]\n",
      "##############\n",
      "[2.55258286 1.82023568 5.89758454 4.65051036 3.55511653 5.46580612\n",
      " 4.3580665  4.7467857  4.11313848 4.06352484]\n",
      "##########\n",
      "epoch:17 step:16001 [D loss: 0.242408, acc.: 54.69%] [G loss: 0.407243]\n",
      "epoch:17 step:16002 [D loss: 0.244194, acc.: 53.91%] [G loss: 0.382552]\n",
      "epoch:17 step:16003 [D loss: 0.202460, acc.: 69.53%] [G loss: 0.446936]\n",
      "epoch:17 step:16004 [D loss: 0.214996, acc.: 63.28%] [G loss: 0.478449]\n",
      "epoch:17 step:16005 [D loss: 0.212196, acc.: 64.06%] [G loss: 0.431428]\n",
      "epoch:17 step:16006 [D loss: 0.181734, acc.: 68.75%] [G loss: 0.472775]\n",
      "epoch:17 step:16007 [D loss: 0.259965, acc.: 53.12%] [G loss: 0.403395]\n",
      "epoch:17 step:16008 [D loss: 0.251544, acc.: 57.81%] [G loss: 0.392876]\n",
      "epoch:17 step:16009 [D loss: 0.211151, acc.: 68.75%] [G loss: 0.438171]\n",
      "epoch:17 step:16010 [D loss: 0.229420, acc.: 61.72%] [G loss: 0.416294]\n",
      "epoch:17 step:16011 [D loss: 0.225379, acc.: 62.50%] [G loss: 0.414972]\n",
      "epoch:17 step:16012 [D loss: 0.212287, acc.: 66.41%] [G loss: 0.486004]\n",
      "epoch:17 step:16013 [D loss: 0.221917, acc.: 64.84%] [G loss: 0.442246]\n",
      "epoch:17 step:16014 [D loss: 0.239148, acc.: 60.16%] [G loss: 0.437921]\n",
      "epoch:17 step:16015 [D loss: 0.220994, acc.: 59.38%] [G loss: 0.428729]\n",
      "epoch:17 step:16016 [D loss: 0.211155, acc.: 67.19%] [G loss: 0.445101]\n",
      "epoch:17 step:16017 [D loss: 0.213684, acc.: 63.28%] [G loss: 0.455472]\n",
      "epoch:17 step:16018 [D loss: 0.218096, acc.: 61.72%] [G loss: 0.400583]\n",
      "epoch:17 step:16019 [D loss: 0.232354, acc.: 64.06%] [G loss: 0.440667]\n",
      "epoch:17 step:16020 [D loss: 0.218815, acc.: 64.06%] [G loss: 0.431426]\n",
      "epoch:17 step:16021 [D loss: 0.209357, acc.: 64.06%] [G loss: 0.414710]\n",
      "epoch:17 step:16022 [D loss: 0.195895, acc.: 71.88%] [G loss: 0.453619]\n",
      "epoch:17 step:16023 [D loss: 0.248274, acc.: 55.47%] [G loss: 0.460947]\n",
      "epoch:17 step:16024 [D loss: 0.223354, acc.: 60.16%] [G loss: 0.426183]\n",
      "epoch:17 step:16025 [D loss: 0.244839, acc.: 60.16%] [G loss: 0.449203]\n",
      "epoch:17 step:16026 [D loss: 0.215890, acc.: 69.53%] [G loss: 0.423555]\n",
      "epoch:17 step:16027 [D loss: 0.249337, acc.: 57.03%] [G loss: 0.438506]\n",
      "epoch:17 step:16028 [D loss: 0.225871, acc.: 63.28%] [G loss: 0.475167]\n",
      "epoch:17 step:16029 [D loss: 0.194308, acc.: 67.97%] [G loss: 0.440403]\n",
      "epoch:17 step:16030 [D loss: 0.236695, acc.: 58.59%] [G loss: 0.440638]\n",
      "epoch:17 step:16031 [D loss: 0.257603, acc.: 59.38%] [G loss: 0.426452]\n",
      "epoch:17 step:16032 [D loss: 0.225634, acc.: 64.06%] [G loss: 0.466850]\n",
      "epoch:17 step:16033 [D loss: 0.226643, acc.: 57.81%] [G loss: 0.443457]\n",
      "epoch:17 step:16034 [D loss: 0.226309, acc.: 67.97%] [G loss: 0.410492]\n",
      "epoch:17 step:16035 [D loss: 0.232157, acc.: 56.25%] [G loss: 0.423691]\n",
      "epoch:17 step:16036 [D loss: 0.191925, acc.: 74.22%] [G loss: 0.482277]\n",
      "epoch:17 step:16037 [D loss: 0.261410, acc.: 54.69%] [G loss: 0.454879]\n",
      "epoch:17 step:16038 [D loss: 0.236126, acc.: 60.94%] [G loss: 0.468693]\n",
      "epoch:17 step:16039 [D loss: 0.217787, acc.: 62.50%] [G loss: 0.415558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16040 [D loss: 0.224453, acc.: 61.72%] [G loss: 0.413326]\n",
      "epoch:17 step:16041 [D loss: 0.220289, acc.: 59.38%] [G loss: 0.469795]\n",
      "epoch:17 step:16042 [D loss: 0.207071, acc.: 65.62%] [G loss: 0.469266]\n",
      "epoch:17 step:16043 [D loss: 0.242154, acc.: 59.38%] [G loss: 0.437485]\n",
      "epoch:17 step:16044 [D loss: 0.199073, acc.: 70.31%] [G loss: 0.494028]\n",
      "epoch:17 step:16045 [D loss: 0.208430, acc.: 64.84%] [G loss: 0.462979]\n",
      "epoch:17 step:16046 [D loss: 0.185935, acc.: 75.78%] [G loss: 0.506840]\n",
      "epoch:17 step:16047 [D loss: 0.234192, acc.: 59.38%] [G loss: 0.425184]\n",
      "epoch:17 step:16048 [D loss: 0.164648, acc.: 76.56%] [G loss: 0.518999]\n",
      "epoch:17 step:16049 [D loss: 0.240184, acc.: 65.62%] [G loss: 0.486484]\n",
      "epoch:17 step:16050 [D loss: 0.225705, acc.: 60.94%] [G loss: 0.452176]\n",
      "epoch:17 step:16051 [D loss: 0.178826, acc.: 78.12%] [G loss: 0.445815]\n",
      "epoch:17 step:16052 [D loss: 0.192150, acc.: 71.09%] [G loss: 0.428081]\n",
      "epoch:17 step:16053 [D loss: 0.254034, acc.: 51.56%] [G loss: 0.455315]\n",
      "epoch:17 step:16054 [D loss: 0.254490, acc.: 55.47%] [G loss: 0.419025]\n",
      "epoch:17 step:16055 [D loss: 0.212198, acc.: 64.84%] [G loss: 0.450934]\n",
      "epoch:17 step:16056 [D loss: 0.229303, acc.: 62.50%] [G loss: 0.461002]\n",
      "epoch:17 step:16057 [D loss: 0.229710, acc.: 61.72%] [G loss: 0.383872]\n",
      "epoch:17 step:16058 [D loss: 0.236534, acc.: 60.16%] [G loss: 0.407986]\n",
      "epoch:17 step:16059 [D loss: 0.209291, acc.: 67.19%] [G loss: 0.456967]\n",
      "epoch:17 step:16060 [D loss: 0.204118, acc.: 64.06%] [G loss: 0.453252]\n",
      "epoch:17 step:16061 [D loss: 0.212619, acc.: 70.31%] [G loss: 0.428577]\n",
      "epoch:17 step:16062 [D loss: 0.237689, acc.: 60.16%] [G loss: 0.441089]\n",
      "epoch:17 step:16063 [D loss: 0.246963, acc.: 54.69%] [G loss: 0.414721]\n",
      "epoch:17 step:16064 [D loss: 0.208432, acc.: 71.09%] [G loss: 0.455969]\n",
      "epoch:17 step:16065 [D loss: 0.225183, acc.: 61.72%] [G loss: 0.424143]\n",
      "epoch:17 step:16066 [D loss: 0.292715, acc.: 47.66%] [G loss: 0.454394]\n",
      "epoch:17 step:16067 [D loss: 0.236253, acc.: 61.72%] [G loss: 0.421558]\n",
      "epoch:17 step:16068 [D loss: 0.242077, acc.: 59.38%] [G loss: 0.407980]\n",
      "epoch:17 step:16069 [D loss: 0.243435, acc.: 57.03%] [G loss: 0.403441]\n",
      "epoch:17 step:16070 [D loss: 0.220509, acc.: 61.72%] [G loss: 0.446198]\n",
      "epoch:17 step:16071 [D loss: 0.252761, acc.: 55.47%] [G loss: 0.398302]\n",
      "epoch:17 step:16072 [D loss: 0.226818, acc.: 59.38%] [G loss: 0.421137]\n",
      "epoch:17 step:16073 [D loss: 0.243588, acc.: 57.81%] [G loss: 0.436447]\n",
      "epoch:17 step:16074 [D loss: 0.229785, acc.: 60.94%] [G loss: 0.395067]\n",
      "epoch:17 step:16075 [D loss: 0.227827, acc.: 66.41%] [G loss: 0.391857]\n",
      "epoch:17 step:16076 [D loss: 0.239734, acc.: 61.72%] [G loss: 0.424381]\n",
      "epoch:17 step:16077 [D loss: 0.251087, acc.: 55.47%] [G loss: 0.423368]\n",
      "epoch:17 step:16078 [D loss: 0.209255, acc.: 64.84%] [G loss: 0.415166]\n",
      "epoch:17 step:16079 [D loss: 0.248249, acc.: 52.34%] [G loss: 0.425776]\n",
      "epoch:17 step:16080 [D loss: 0.216106, acc.: 64.06%] [G loss: 0.423151]\n",
      "epoch:17 step:16081 [D loss: 0.240182, acc.: 64.06%] [G loss: 0.423347]\n",
      "epoch:17 step:16082 [D loss: 0.241891, acc.: 56.25%] [G loss: 0.417300]\n",
      "epoch:17 step:16083 [D loss: 0.226914, acc.: 55.47%] [G loss: 0.406601]\n",
      "epoch:17 step:16084 [D loss: 0.217147, acc.: 63.28%] [G loss: 0.454358]\n",
      "epoch:17 step:16085 [D loss: 0.235608, acc.: 61.72%] [G loss: 0.410033]\n",
      "epoch:17 step:16086 [D loss: 0.228128, acc.: 58.59%] [G loss: 0.451308]\n",
      "epoch:17 step:16087 [D loss: 0.237232, acc.: 60.94%] [G loss: 0.417069]\n",
      "epoch:17 step:16088 [D loss: 0.215763, acc.: 65.62%] [G loss: 0.454371]\n",
      "epoch:17 step:16089 [D loss: 0.255938, acc.: 59.38%] [G loss: 0.433828]\n",
      "epoch:17 step:16090 [D loss: 0.255393, acc.: 57.81%] [G loss: 0.454920]\n",
      "epoch:17 step:16091 [D loss: 0.229560, acc.: 62.50%] [G loss: 0.423693]\n",
      "epoch:17 step:16092 [D loss: 0.212268, acc.: 64.06%] [G loss: 0.436476]\n",
      "epoch:17 step:16093 [D loss: 0.224887, acc.: 60.16%] [G loss: 0.432180]\n",
      "epoch:17 step:16094 [D loss: 0.211874, acc.: 69.53%] [G loss: 0.444353]\n",
      "epoch:17 step:16095 [D loss: 0.207142, acc.: 67.19%] [G loss: 0.429684]\n",
      "epoch:17 step:16096 [D loss: 0.242014, acc.: 56.25%] [G loss: 0.432934]\n",
      "epoch:17 step:16097 [D loss: 0.201114, acc.: 71.88%] [G loss: 0.442633]\n",
      "epoch:17 step:16098 [D loss: 0.238629, acc.: 60.94%] [G loss: 0.460002]\n",
      "epoch:17 step:16099 [D loss: 0.244981, acc.: 60.16%] [G loss: 0.427384]\n",
      "epoch:17 step:16100 [D loss: 0.211339, acc.: 68.75%] [G loss: 0.436105]\n",
      "epoch:17 step:16101 [D loss: 0.230473, acc.: 60.94%] [G loss: 0.414330]\n",
      "epoch:17 step:16102 [D loss: 0.220930, acc.: 65.62%] [G loss: 0.402695]\n",
      "epoch:17 step:16103 [D loss: 0.241611, acc.: 54.69%] [G loss: 0.419449]\n",
      "epoch:17 step:16104 [D loss: 0.206000, acc.: 64.84%] [G loss: 0.430933]\n",
      "epoch:17 step:16105 [D loss: 0.220643, acc.: 62.50%] [G loss: 0.420982]\n",
      "epoch:17 step:16106 [D loss: 0.228525, acc.: 64.06%] [G loss: 0.419294]\n",
      "epoch:17 step:16107 [D loss: 0.227416, acc.: 58.59%] [G loss: 0.376668]\n",
      "epoch:17 step:16108 [D loss: 0.244911, acc.: 59.38%] [G loss: 0.396613]\n",
      "epoch:17 step:16109 [D loss: 0.248771, acc.: 54.69%] [G loss: 0.418642]\n",
      "epoch:17 step:16110 [D loss: 0.231743, acc.: 66.41%] [G loss: 0.441483]\n",
      "epoch:17 step:16111 [D loss: 0.255914, acc.: 55.47%] [G loss: 0.455531]\n",
      "epoch:17 step:16112 [D loss: 0.266433, acc.: 53.91%] [G loss: 0.434755]\n",
      "epoch:17 step:16113 [D loss: 0.209833, acc.: 68.75%] [G loss: 0.469336]\n",
      "epoch:17 step:16114 [D loss: 0.246935, acc.: 56.25%] [G loss: 0.403473]\n",
      "epoch:17 step:16115 [D loss: 0.236166, acc.: 59.38%] [G loss: 0.409424]\n",
      "epoch:17 step:16116 [D loss: 0.257779, acc.: 53.12%] [G loss: 0.363874]\n",
      "epoch:17 step:16117 [D loss: 0.236088, acc.: 57.03%] [G loss: 0.410796]\n",
      "epoch:17 step:16118 [D loss: 0.242020, acc.: 61.72%] [G loss: 0.399048]\n",
      "epoch:17 step:16119 [D loss: 0.221691, acc.: 62.50%] [G loss: 0.440368]\n",
      "epoch:17 step:16120 [D loss: 0.211881, acc.: 64.84%] [G loss: 0.438365]\n",
      "epoch:17 step:16121 [D loss: 0.203950, acc.: 64.06%] [G loss: 0.458626]\n",
      "epoch:17 step:16122 [D loss: 0.225699, acc.: 62.50%] [G loss: 0.420942]\n",
      "epoch:17 step:16123 [D loss: 0.198614, acc.: 70.31%] [G loss: 0.448995]\n",
      "epoch:17 step:16124 [D loss: 0.197498, acc.: 73.44%] [G loss: 0.452384]\n",
      "epoch:17 step:16125 [D loss: 0.239539, acc.: 59.38%] [G loss: 0.436525]\n",
      "epoch:17 step:16126 [D loss: 0.240380, acc.: 58.59%] [G loss: 0.464874]\n",
      "epoch:17 step:16127 [D loss: 0.196236, acc.: 70.31%] [G loss: 0.473124]\n",
      "epoch:17 step:16128 [D loss: 0.219806, acc.: 62.50%] [G loss: 0.435758]\n",
      "epoch:17 step:16129 [D loss: 0.256809, acc.: 54.69%] [G loss: 0.420716]\n",
      "epoch:17 step:16130 [D loss: 0.233924, acc.: 62.50%] [G loss: 0.410633]\n",
      "epoch:17 step:16131 [D loss: 0.216993, acc.: 64.84%] [G loss: 0.450186]\n",
      "epoch:17 step:16132 [D loss: 0.284156, acc.: 50.78%] [G loss: 0.407197]\n",
      "epoch:17 step:16133 [D loss: 0.210834, acc.: 68.75%] [G loss: 0.428920]\n",
      "epoch:17 step:16134 [D loss: 0.228043, acc.: 63.28%] [G loss: 0.450866]\n",
      "epoch:17 step:16135 [D loss: 0.230634, acc.: 60.16%] [G loss: 0.459143]\n",
      "epoch:17 step:16136 [D loss: 0.213677, acc.: 66.41%] [G loss: 0.480753]\n",
      "epoch:17 step:16137 [D loss: 0.182291, acc.: 71.09%] [G loss: 0.488787]\n",
      "epoch:17 step:16138 [D loss: 0.180229, acc.: 72.66%] [G loss: 0.533861]\n",
      "epoch:17 step:16139 [D loss: 0.285428, acc.: 45.31%] [G loss: 0.417363]\n",
      "epoch:17 step:16140 [D loss: 0.269620, acc.: 50.78%] [G loss: 0.423894]\n",
      "epoch:17 step:16141 [D loss: 0.246555, acc.: 57.03%] [G loss: 0.440385]\n",
      "epoch:17 step:16142 [D loss: 0.231926, acc.: 54.69%] [G loss: 0.393389]\n",
      "epoch:17 step:16143 [D loss: 0.234685, acc.: 59.38%] [G loss: 0.429644]\n",
      "epoch:17 step:16144 [D loss: 0.247817, acc.: 60.16%] [G loss: 0.383134]\n",
      "epoch:17 step:16145 [D loss: 0.240856, acc.: 57.03%] [G loss: 0.408971]\n",
      "epoch:17 step:16146 [D loss: 0.221486, acc.: 64.06%] [G loss: 0.424445]\n",
      "epoch:17 step:16147 [D loss: 0.203592, acc.: 64.06%] [G loss: 0.463834]\n",
      "epoch:17 step:16148 [D loss: 0.209364, acc.: 66.41%] [G loss: 0.467081]\n",
      "epoch:17 step:16149 [D loss: 0.275471, acc.: 60.16%] [G loss: 0.439896]\n",
      "epoch:17 step:16150 [D loss: 0.214259, acc.: 63.28%] [G loss: 0.441636]\n",
      "epoch:17 step:16151 [D loss: 0.215894, acc.: 61.72%] [G loss: 0.459995]\n",
      "epoch:17 step:16152 [D loss: 0.229579, acc.: 65.62%] [G loss: 0.502826]\n",
      "epoch:17 step:16153 [D loss: 0.234928, acc.: 60.16%] [G loss: 0.425443]\n",
      "epoch:17 step:16154 [D loss: 0.228234, acc.: 64.06%] [G loss: 0.431085]\n",
      "epoch:17 step:16155 [D loss: 0.234243, acc.: 63.28%] [G loss: 0.403691]\n",
      "epoch:17 step:16156 [D loss: 0.230455, acc.: 60.94%] [G loss: 0.436567]\n",
      "epoch:17 step:16157 [D loss: 0.225000, acc.: 62.50%] [G loss: 0.426182]\n",
      "epoch:17 step:16158 [D loss: 0.201515, acc.: 69.53%] [G loss: 0.450485]\n",
      "epoch:17 step:16159 [D loss: 0.199404, acc.: 70.31%] [G loss: 0.461514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16160 [D loss: 0.188756, acc.: 72.66%] [G loss: 0.521351]\n",
      "epoch:17 step:16161 [D loss: 0.164353, acc.: 78.12%] [G loss: 0.523051]\n",
      "epoch:17 step:16162 [D loss: 0.268124, acc.: 52.34%] [G loss: 0.436434]\n",
      "epoch:17 step:16163 [D loss: 0.256495, acc.: 54.69%] [G loss: 0.443167]\n",
      "epoch:17 step:16164 [D loss: 0.221127, acc.: 64.06%] [G loss: 0.439816]\n",
      "epoch:17 step:16165 [D loss: 0.198794, acc.: 69.53%] [G loss: 0.427106]\n",
      "epoch:17 step:16166 [D loss: 0.224516, acc.: 61.72%] [G loss: 0.433023]\n",
      "epoch:17 step:16167 [D loss: 0.205175, acc.: 71.88%] [G loss: 0.466491]\n",
      "epoch:17 step:16168 [D loss: 0.238906, acc.: 62.50%] [G loss: 0.419663]\n",
      "epoch:17 step:16169 [D loss: 0.246139, acc.: 60.94%] [G loss: 0.431243]\n",
      "epoch:17 step:16170 [D loss: 0.200225, acc.: 72.66%] [G loss: 0.448544]\n",
      "epoch:17 step:16171 [D loss: 0.192514, acc.: 71.09%] [G loss: 0.474455]\n",
      "epoch:17 step:16172 [D loss: 0.235794, acc.: 56.25%] [G loss: 0.426878]\n",
      "epoch:17 step:16173 [D loss: 0.233169, acc.: 57.81%] [G loss: 0.438695]\n",
      "epoch:17 step:16174 [D loss: 0.203168, acc.: 67.97%] [G loss: 0.446258]\n",
      "epoch:17 step:16175 [D loss: 0.214181, acc.: 67.19%] [G loss: 0.466650]\n",
      "epoch:17 step:16176 [D loss: 0.244292, acc.: 59.38%] [G loss: 0.443829]\n",
      "epoch:17 step:16177 [D loss: 0.197224, acc.: 74.22%] [G loss: 0.454326]\n",
      "epoch:17 step:16178 [D loss: 0.231625, acc.: 57.81%] [G loss: 0.464530]\n",
      "epoch:17 step:16179 [D loss: 0.293837, acc.: 46.09%] [G loss: 0.431998]\n",
      "epoch:17 step:16180 [D loss: 0.260774, acc.: 50.00%] [G loss: 0.434278]\n",
      "epoch:17 step:16181 [D loss: 0.233690, acc.: 60.16%] [G loss: 0.436703]\n",
      "epoch:17 step:16182 [D loss: 0.226440, acc.: 59.38%] [G loss: 0.465741]\n",
      "epoch:17 step:16183 [D loss: 0.227450, acc.: 62.50%] [G loss: 0.409098]\n",
      "epoch:17 step:16184 [D loss: 0.214941, acc.: 68.75%] [G loss: 0.450291]\n",
      "epoch:17 step:16185 [D loss: 0.231283, acc.: 62.50%] [G loss: 0.401510]\n",
      "epoch:17 step:16186 [D loss: 0.222611, acc.: 62.50%] [G loss: 0.397567]\n",
      "epoch:17 step:16187 [D loss: 0.216310, acc.: 63.28%] [G loss: 0.405836]\n",
      "epoch:17 step:16188 [D loss: 0.195969, acc.: 70.31%] [G loss: 0.423320]\n",
      "epoch:17 step:16189 [D loss: 0.237261, acc.: 62.50%] [G loss: 0.433540]\n",
      "epoch:17 step:16190 [D loss: 0.223563, acc.: 60.94%] [G loss: 0.442546]\n",
      "epoch:17 step:16191 [D loss: 0.230769, acc.: 63.28%] [G loss: 0.436738]\n",
      "epoch:17 step:16192 [D loss: 0.242578, acc.: 62.50%] [G loss: 0.388361]\n",
      "epoch:17 step:16193 [D loss: 0.224966, acc.: 61.72%] [G loss: 0.418851]\n",
      "epoch:17 step:16194 [D loss: 0.248526, acc.: 58.59%] [G loss: 0.405644]\n",
      "epoch:17 step:16195 [D loss: 0.241626, acc.: 59.38%] [G loss: 0.405918]\n",
      "epoch:17 step:16196 [D loss: 0.211546, acc.: 64.06%] [G loss: 0.441001]\n",
      "epoch:17 step:16197 [D loss: 0.215436, acc.: 70.31%] [G loss: 0.458447]\n",
      "epoch:17 step:16198 [D loss: 0.208375, acc.: 67.19%] [G loss: 0.436643]\n",
      "epoch:17 step:16199 [D loss: 0.223332, acc.: 61.72%] [G loss: 0.435790]\n",
      "epoch:17 step:16200 [D loss: 0.202601, acc.: 66.41%] [G loss: 0.437999]\n",
      "##############\n",
      "[2.54244802 1.74298966 6.01210308 4.71891371 3.70321077 5.76004598\n",
      " 4.45684465 5.0029154  4.52287736 3.83768094]\n",
      "##########\n",
      "epoch:17 step:16201 [D loss: 0.219363, acc.: 60.94%] [G loss: 0.453005]\n",
      "epoch:17 step:16202 [D loss: 0.221273, acc.: 66.41%] [G loss: 0.425808]\n",
      "epoch:17 step:16203 [D loss: 0.190940, acc.: 75.00%] [G loss: 0.450838]\n",
      "epoch:17 step:16204 [D loss: 0.236889, acc.: 58.59%] [G loss: 0.426995]\n",
      "epoch:17 step:16205 [D loss: 0.197539, acc.: 73.44%] [G loss: 0.489180]\n",
      "epoch:17 step:16206 [D loss: 0.255146, acc.: 57.03%] [G loss: 0.418216]\n",
      "epoch:17 step:16207 [D loss: 0.248999, acc.: 53.91%] [G loss: 0.387866]\n",
      "epoch:17 step:16208 [D loss: 0.242269, acc.: 53.91%] [G loss: 0.430000]\n",
      "epoch:17 step:16209 [D loss: 0.211150, acc.: 65.62%] [G loss: 0.473573]\n",
      "epoch:17 step:16210 [D loss: 0.251561, acc.: 60.94%] [G loss: 0.412269]\n",
      "epoch:17 step:16211 [D loss: 0.233554, acc.: 54.69%] [G loss: 0.407389]\n",
      "epoch:17 step:16212 [D loss: 0.198199, acc.: 65.62%] [G loss: 0.452576]\n",
      "epoch:17 step:16213 [D loss: 0.226886, acc.: 62.50%] [G loss: 0.448256]\n",
      "epoch:17 step:16214 [D loss: 0.238662, acc.: 63.28%] [G loss: 0.404723]\n",
      "epoch:17 step:16215 [D loss: 0.208593, acc.: 71.09%] [G loss: 0.416213]\n",
      "epoch:17 step:16216 [D loss: 0.223292, acc.: 64.06%] [G loss: 0.430908]\n",
      "epoch:17 step:16217 [D loss: 0.193111, acc.: 75.78%] [G loss: 0.436115]\n",
      "epoch:17 step:16218 [D loss: 0.229976, acc.: 62.50%] [G loss: 0.437242]\n",
      "epoch:17 step:16219 [D loss: 0.224687, acc.: 62.50%] [G loss: 0.493698]\n",
      "epoch:17 step:16220 [D loss: 0.247051, acc.: 60.16%] [G loss: 0.416808]\n",
      "epoch:17 step:16221 [D loss: 0.218150, acc.: 67.97%] [G loss: 0.443838]\n",
      "epoch:17 step:16222 [D loss: 0.229059, acc.: 64.06%] [G loss: 0.390776]\n",
      "epoch:17 step:16223 [D loss: 0.222637, acc.: 56.25%] [G loss: 0.426794]\n",
      "epoch:17 step:16224 [D loss: 0.235536, acc.: 57.81%] [G loss: 0.436618]\n",
      "epoch:17 step:16225 [D loss: 0.197604, acc.: 72.66%] [G loss: 0.444825]\n",
      "epoch:17 step:16226 [D loss: 0.211056, acc.: 62.50%] [G loss: 0.448673]\n",
      "epoch:17 step:16227 [D loss: 0.191333, acc.: 75.78%] [G loss: 0.465271]\n",
      "epoch:17 step:16228 [D loss: 0.204401, acc.: 71.09%] [G loss: 0.454437]\n",
      "epoch:17 step:16229 [D loss: 0.197666, acc.: 69.53%] [G loss: 0.437536]\n",
      "epoch:17 step:16230 [D loss: 0.245082, acc.: 57.81%] [G loss: 0.422587]\n",
      "epoch:17 step:16231 [D loss: 0.228934, acc.: 56.25%] [G loss: 0.426302]\n",
      "epoch:17 step:16232 [D loss: 0.210240, acc.: 69.53%] [G loss: 0.461426]\n",
      "epoch:17 step:16233 [D loss: 0.222906, acc.: 62.50%] [G loss: 0.441405]\n",
      "epoch:17 step:16234 [D loss: 0.197694, acc.: 67.97%] [G loss: 0.442437]\n",
      "epoch:17 step:16235 [D loss: 0.230222, acc.: 57.81%] [G loss: 0.438156]\n",
      "epoch:17 step:16236 [D loss: 0.227763, acc.: 62.50%] [G loss: 0.450581]\n",
      "epoch:17 step:16237 [D loss: 0.228956, acc.: 61.72%] [G loss: 0.441523]\n",
      "epoch:17 step:16238 [D loss: 0.220277, acc.: 60.94%] [G loss: 0.442102]\n",
      "epoch:17 step:16239 [D loss: 0.221812, acc.: 68.75%] [G loss: 0.470590]\n",
      "epoch:17 step:16240 [D loss: 0.203259, acc.: 70.31%] [G loss: 0.473151]\n",
      "epoch:17 step:16241 [D loss: 0.183575, acc.: 72.66%] [G loss: 0.471625]\n",
      "epoch:17 step:16242 [D loss: 0.217850, acc.: 70.31%] [G loss: 0.478074]\n",
      "epoch:17 step:16243 [D loss: 0.208720, acc.: 64.06%] [G loss: 0.508312]\n",
      "epoch:17 step:16244 [D loss: 0.193511, acc.: 71.09%] [G loss: 0.441567]\n",
      "epoch:17 step:16245 [D loss: 0.270316, acc.: 57.81%] [G loss: 0.416354]\n",
      "epoch:17 step:16246 [D loss: 0.251167, acc.: 53.91%] [G loss: 0.398495]\n",
      "epoch:17 step:16247 [D loss: 0.222454, acc.: 63.28%] [G loss: 0.440748]\n",
      "epoch:17 step:16248 [D loss: 0.234519, acc.: 67.97%] [G loss: 0.437125]\n",
      "epoch:17 step:16249 [D loss: 0.208374, acc.: 65.62%] [G loss: 0.463798]\n",
      "epoch:17 step:16250 [D loss: 0.216019, acc.: 62.50%] [G loss: 0.425002]\n",
      "epoch:17 step:16251 [D loss: 0.215578, acc.: 67.19%] [G loss: 0.454985]\n",
      "epoch:17 step:16252 [D loss: 0.245234, acc.: 51.56%] [G loss: 0.446994]\n",
      "epoch:17 step:16253 [D loss: 0.210695, acc.: 73.44%] [G loss: 0.422984]\n",
      "epoch:17 step:16254 [D loss: 0.234069, acc.: 59.38%] [G loss: 0.425952]\n",
      "epoch:17 step:16255 [D loss: 0.231253, acc.: 60.16%] [G loss: 0.422328]\n",
      "epoch:17 step:16256 [D loss: 0.200610, acc.: 68.75%] [G loss: 0.475514]\n",
      "epoch:17 step:16257 [D loss: 0.218109, acc.: 64.06%] [G loss: 0.449842]\n",
      "epoch:17 step:16258 [D loss: 0.218870, acc.: 60.94%] [G loss: 0.444856]\n",
      "epoch:17 step:16259 [D loss: 0.215059, acc.: 62.50%] [G loss: 0.456266]\n",
      "epoch:17 step:16260 [D loss: 0.226986, acc.: 64.84%] [G loss: 0.428893]\n",
      "epoch:17 step:16261 [D loss: 0.219939, acc.: 64.06%] [G loss: 0.447118]\n",
      "epoch:17 step:16262 [D loss: 0.211503, acc.: 63.28%] [G loss: 0.437246]\n",
      "epoch:17 step:16263 [D loss: 0.208826, acc.: 71.88%] [G loss: 0.450641]\n",
      "epoch:17 step:16264 [D loss: 0.228276, acc.: 60.16%] [G loss: 0.449591]\n",
      "epoch:17 step:16265 [D loss: 0.199428, acc.: 69.53%] [G loss: 0.450704]\n",
      "epoch:17 step:16266 [D loss: 0.217515, acc.: 64.06%] [G loss: 0.486990]\n",
      "epoch:17 step:16267 [D loss: 0.243810, acc.: 59.38%] [G loss: 0.457204]\n",
      "epoch:17 step:16268 [D loss: 0.221069, acc.: 67.19%] [G loss: 0.450938]\n",
      "epoch:17 step:16269 [D loss: 0.175077, acc.: 78.12%] [G loss: 0.494885]\n",
      "epoch:17 step:16270 [D loss: 0.307505, acc.: 47.66%] [G loss: 0.445444]\n",
      "epoch:17 step:16271 [D loss: 0.250684, acc.: 55.47%] [G loss: 0.493012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16272 [D loss: 0.224239, acc.: 66.41%] [G loss: 0.491923]\n",
      "epoch:17 step:16273 [D loss: 0.203339, acc.: 72.66%] [G loss: 0.497857]\n",
      "epoch:17 step:16274 [D loss: 0.233899, acc.: 59.38%] [G loss: 0.453983]\n",
      "epoch:17 step:16275 [D loss: 0.202919, acc.: 68.75%] [G loss: 0.493713]\n",
      "epoch:17 step:16276 [D loss: 0.190268, acc.: 73.44%] [G loss: 0.534412]\n",
      "epoch:17 step:16277 [D loss: 0.247414, acc.: 63.28%] [G loss: 0.440147]\n",
      "epoch:17 step:16278 [D loss: 0.278930, acc.: 50.78%] [G loss: 0.366937]\n",
      "epoch:17 step:16279 [D loss: 0.202330, acc.: 69.53%] [G loss: 0.413452]\n",
      "epoch:17 step:16280 [D loss: 0.245176, acc.: 54.69%] [G loss: 0.414857]\n",
      "epoch:17 step:16281 [D loss: 0.237120, acc.: 55.47%] [G loss: 0.428574]\n",
      "epoch:17 step:16282 [D loss: 0.225562, acc.: 63.28%] [G loss: 0.448773]\n",
      "epoch:17 step:16283 [D loss: 0.211634, acc.: 67.97%] [G loss: 0.457037]\n",
      "epoch:17 step:16284 [D loss: 0.221392, acc.: 64.06%] [G loss: 0.485789]\n",
      "epoch:17 step:16285 [D loss: 0.233865, acc.: 60.16%] [G loss: 0.430079]\n",
      "epoch:17 step:16286 [D loss: 0.188769, acc.: 70.31%] [G loss: 0.430739]\n",
      "epoch:17 step:16287 [D loss: 0.208549, acc.: 66.41%] [G loss: 0.447488]\n",
      "epoch:17 step:16288 [D loss: 0.181550, acc.: 74.22%] [G loss: 0.447815]\n",
      "epoch:17 step:16289 [D loss: 0.209908, acc.: 64.84%] [G loss: 0.465006]\n",
      "epoch:17 step:16290 [D loss: 0.201070, acc.: 67.97%] [G loss: 0.462983]\n",
      "epoch:17 step:16291 [D loss: 0.245552, acc.: 57.81%] [G loss: 0.435006]\n",
      "epoch:17 step:16292 [D loss: 0.241577, acc.: 56.25%] [G loss: 0.407553]\n",
      "epoch:17 step:16293 [D loss: 0.195288, acc.: 73.44%] [G loss: 0.435334]\n",
      "epoch:17 step:16294 [D loss: 0.222179, acc.: 64.84%] [G loss: 0.436735]\n",
      "epoch:17 step:16295 [D loss: 0.231458, acc.: 61.72%] [G loss: 0.421787]\n",
      "epoch:17 step:16296 [D loss: 0.221071, acc.: 66.41%] [G loss: 0.452407]\n",
      "epoch:17 step:16297 [D loss: 0.239708, acc.: 55.47%] [G loss: 0.417962]\n",
      "epoch:17 step:16298 [D loss: 0.243059, acc.: 52.34%] [G loss: 0.433996]\n",
      "epoch:17 step:16299 [D loss: 0.213670, acc.: 68.75%] [G loss: 0.450764]\n",
      "epoch:17 step:16300 [D loss: 0.206920, acc.: 68.75%] [G loss: 0.446713]\n",
      "epoch:17 step:16301 [D loss: 0.209680, acc.: 66.41%] [G loss: 0.460981]\n",
      "epoch:17 step:16302 [D loss: 0.252056, acc.: 58.59%] [G loss: 0.425977]\n",
      "epoch:17 step:16303 [D loss: 0.201414, acc.: 71.88%] [G loss: 0.502013]\n",
      "epoch:17 step:16304 [D loss: 0.231457, acc.: 54.69%] [G loss: 0.430061]\n",
      "epoch:17 step:16305 [D loss: 0.265472, acc.: 52.34%] [G loss: 0.418629]\n",
      "epoch:17 step:16306 [D loss: 0.265624, acc.: 50.00%] [G loss: 0.378678]\n",
      "epoch:17 step:16307 [D loss: 0.230999, acc.: 63.28%] [G loss: 0.460182]\n",
      "epoch:17 step:16308 [D loss: 0.216390, acc.: 67.19%] [G loss: 0.439438]\n",
      "epoch:17 step:16309 [D loss: 0.233486, acc.: 61.72%] [G loss: 0.407865]\n",
      "epoch:17 step:16310 [D loss: 0.193212, acc.: 71.09%] [G loss: 0.465473]\n",
      "epoch:17 step:16311 [D loss: 0.225458, acc.: 54.69%] [G loss: 0.420383]\n",
      "epoch:17 step:16312 [D loss: 0.205437, acc.: 67.19%] [G loss: 0.441961]\n",
      "epoch:17 step:16313 [D loss: 0.220355, acc.: 66.41%] [G loss: 0.413732]\n",
      "epoch:17 step:16314 [D loss: 0.169962, acc.: 75.78%] [G loss: 0.451493]\n",
      "epoch:17 step:16315 [D loss: 0.219789, acc.: 66.41%] [G loss: 0.427453]\n",
      "epoch:17 step:16316 [D loss: 0.219580, acc.: 61.72%] [G loss: 0.434884]\n",
      "epoch:17 step:16317 [D loss: 0.228798, acc.: 61.72%] [G loss: 0.402847]\n",
      "epoch:17 step:16318 [D loss: 0.230712, acc.: 60.16%] [G loss: 0.430847]\n",
      "epoch:17 step:16319 [D loss: 0.234378, acc.: 60.16%] [G loss: 0.460730]\n",
      "epoch:17 step:16320 [D loss: 0.226096, acc.: 67.19%] [G loss: 0.431053]\n",
      "epoch:17 step:16321 [D loss: 0.238503, acc.: 61.72%] [G loss: 0.440027]\n",
      "epoch:17 step:16322 [D loss: 0.221825, acc.: 66.41%] [G loss: 0.463153]\n",
      "epoch:17 step:16323 [D loss: 0.234478, acc.: 65.62%] [G loss: 0.426589]\n",
      "epoch:17 step:16324 [D loss: 0.230892, acc.: 62.50%] [G loss: 0.452310]\n",
      "epoch:17 step:16325 [D loss: 0.245275, acc.: 54.69%] [G loss: 0.437205]\n",
      "epoch:17 step:16326 [D loss: 0.233714, acc.: 57.03%] [G loss: 0.432820]\n",
      "epoch:17 step:16327 [D loss: 0.203418, acc.: 70.31%] [G loss: 0.461334]\n",
      "epoch:17 step:16328 [D loss: 0.212214, acc.: 69.53%] [G loss: 0.489770]\n",
      "epoch:17 step:16329 [D loss: 0.256911, acc.: 51.56%] [G loss: 0.420018]\n",
      "epoch:17 step:16330 [D loss: 0.243000, acc.: 52.34%] [G loss: 0.402504]\n",
      "epoch:17 step:16331 [D loss: 0.214328, acc.: 67.19%] [G loss: 0.448967]\n",
      "epoch:17 step:16332 [D loss: 0.238656, acc.: 60.94%] [G loss: 0.426407]\n",
      "epoch:17 step:16333 [D loss: 0.231771, acc.: 64.84%] [G loss: 0.402173]\n",
      "epoch:17 step:16334 [D loss: 0.196260, acc.: 75.00%] [G loss: 0.426551]\n",
      "epoch:17 step:16335 [D loss: 0.183694, acc.: 74.22%] [G loss: 0.487303]\n",
      "epoch:17 step:16336 [D loss: 0.247579, acc.: 60.94%] [G loss: 0.465771]\n",
      "epoch:17 step:16337 [D loss: 0.243704, acc.: 58.59%] [G loss: 0.472167]\n",
      "epoch:17 step:16338 [D loss: 0.226821, acc.: 58.59%] [G loss: 0.439175]\n",
      "epoch:17 step:16339 [D loss: 0.248976, acc.: 58.59%] [G loss: 0.409834]\n",
      "epoch:17 step:16340 [D loss: 0.246942, acc.: 54.69%] [G loss: 0.408665]\n",
      "epoch:17 step:16341 [D loss: 0.210727, acc.: 68.75%] [G loss: 0.408326]\n",
      "epoch:17 step:16342 [D loss: 0.232944, acc.: 65.62%] [G loss: 0.409211]\n",
      "epoch:17 step:16343 [D loss: 0.202952, acc.: 69.53%] [G loss: 0.442441]\n",
      "epoch:17 step:16344 [D loss: 0.224668, acc.: 61.72%] [G loss: 0.453800]\n",
      "epoch:17 step:16345 [D loss: 0.212500, acc.: 68.75%] [G loss: 0.471376]\n",
      "epoch:17 step:16346 [D loss: 0.236351, acc.: 60.94%] [G loss: 0.455489]\n",
      "epoch:17 step:16347 [D loss: 0.254809, acc.: 57.03%] [G loss: 0.449031]\n",
      "epoch:17 step:16348 [D loss: 0.225341, acc.: 64.06%] [G loss: 0.450237]\n",
      "epoch:17 step:16349 [D loss: 0.238792, acc.: 61.72%] [G loss: 0.428791]\n",
      "epoch:17 step:16350 [D loss: 0.250711, acc.: 55.47%] [G loss: 0.446188]\n",
      "epoch:17 step:16351 [D loss: 0.235740, acc.: 57.03%] [G loss: 0.412209]\n",
      "epoch:17 step:16352 [D loss: 0.221155, acc.: 64.06%] [G loss: 0.430904]\n",
      "epoch:17 step:16353 [D loss: 0.254629, acc.: 47.66%] [G loss: 0.422409]\n",
      "epoch:17 step:16354 [D loss: 0.227440, acc.: 60.16%] [G loss: 0.414444]\n",
      "epoch:17 step:16355 [D loss: 0.216091, acc.: 63.28%] [G loss: 0.404392]\n",
      "epoch:17 step:16356 [D loss: 0.225704, acc.: 61.72%] [G loss: 0.406243]\n",
      "epoch:17 step:16357 [D loss: 0.223190, acc.: 63.28%] [G loss: 0.432557]\n",
      "epoch:17 step:16358 [D loss: 0.192312, acc.: 74.22%] [G loss: 0.467957]\n",
      "epoch:17 step:16359 [D loss: 0.215693, acc.: 64.84%] [G loss: 0.493045]\n",
      "epoch:17 step:16360 [D loss: 0.236638, acc.: 59.38%] [G loss: 0.444204]\n",
      "epoch:17 step:16361 [D loss: 0.269448, acc.: 55.47%] [G loss: 0.413424]\n",
      "epoch:17 step:16362 [D loss: 0.238166, acc.: 56.25%] [G loss: 0.452352]\n",
      "epoch:17 step:16363 [D loss: 0.199043, acc.: 69.53%] [G loss: 0.472729]\n",
      "epoch:17 step:16364 [D loss: 0.215989, acc.: 64.06%] [G loss: 0.440314]\n",
      "epoch:17 step:16365 [D loss: 0.214015, acc.: 65.62%] [G loss: 0.479476]\n",
      "epoch:17 step:16366 [D loss: 0.248475, acc.: 60.16%] [G loss: 0.471797]\n",
      "epoch:17 step:16367 [D loss: 0.228103, acc.: 58.59%] [G loss: 0.456694]\n",
      "epoch:17 step:16368 [D loss: 0.216320, acc.: 66.41%] [G loss: 0.464292]\n",
      "epoch:17 step:16369 [D loss: 0.203698, acc.: 67.97%] [G loss: 0.450165]\n",
      "epoch:17 step:16370 [D loss: 0.222769, acc.: 61.72%] [G loss: 0.473624]\n",
      "epoch:17 step:16371 [D loss: 0.216967, acc.: 61.72%] [G loss: 0.423339]\n",
      "epoch:17 step:16372 [D loss: 0.269383, acc.: 53.12%] [G loss: 0.392733]\n",
      "epoch:17 step:16373 [D loss: 0.252966, acc.: 57.03%] [G loss: 0.440759]\n",
      "epoch:17 step:16374 [D loss: 0.226363, acc.: 61.72%] [G loss: 0.471049]\n",
      "epoch:17 step:16375 [D loss: 0.219407, acc.: 68.75%] [G loss: 0.463745]\n",
      "epoch:17 step:16376 [D loss: 0.212258, acc.: 62.50%] [G loss: 0.431642]\n",
      "epoch:17 step:16377 [D loss: 0.242441, acc.: 61.72%] [G loss: 0.412643]\n",
      "epoch:17 step:16378 [D loss: 0.233013, acc.: 57.81%] [G loss: 0.427540]\n",
      "epoch:17 step:16379 [D loss: 0.242088, acc.: 60.16%] [G loss: 0.441642]\n",
      "epoch:17 step:16380 [D loss: 0.208960, acc.: 64.84%] [G loss: 0.418116]\n",
      "epoch:17 step:16381 [D loss: 0.205599, acc.: 67.19%] [G loss: 0.465393]\n",
      "epoch:17 step:16382 [D loss: 0.197641, acc.: 67.19%] [G loss: 0.475007]\n",
      "epoch:17 step:16383 [D loss: 0.204758, acc.: 70.31%] [G loss: 0.476358]\n",
      "epoch:17 step:16384 [D loss: 0.247436, acc.: 57.03%] [G loss: 0.424116]\n",
      "epoch:17 step:16385 [D loss: 0.220559, acc.: 64.84%] [G loss: 0.451490]\n",
      "epoch:17 step:16386 [D loss: 0.211657, acc.: 66.41%] [G loss: 0.466228]\n",
      "epoch:17 step:16387 [D loss: 0.271346, acc.: 46.88%] [G loss: 0.422426]\n",
      "epoch:17 step:16388 [D loss: 0.244876, acc.: 56.25%] [G loss: 0.385443]\n",
      "epoch:17 step:16389 [D loss: 0.200089, acc.: 69.53%] [G loss: 0.451839]\n",
      "epoch:17 step:16390 [D loss: 0.263893, acc.: 50.00%] [G loss: 0.419416]\n",
      "epoch:17 step:16391 [D loss: 0.240197, acc.: 53.12%] [G loss: 0.453027]\n",
      "epoch:17 step:16392 [D loss: 0.233974, acc.: 60.16%] [G loss: 0.427796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16393 [D loss: 0.226128, acc.: 59.38%] [G loss: 0.417582]\n",
      "epoch:17 step:16394 [D loss: 0.231061, acc.: 59.38%] [G loss: 0.420986]\n",
      "epoch:17 step:16395 [D loss: 0.214959, acc.: 65.62%] [G loss: 0.443526]\n",
      "epoch:17 step:16396 [D loss: 0.241472, acc.: 61.72%] [G loss: 0.440511]\n",
      "epoch:17 step:16397 [D loss: 0.223037, acc.: 65.62%] [G loss: 0.446490]\n",
      "epoch:17 step:16398 [D loss: 0.190701, acc.: 69.53%] [G loss: 0.492891]\n",
      "epoch:17 step:16399 [D loss: 0.220983, acc.: 62.50%] [G loss: 0.425255]\n",
      "epoch:17 step:16400 [D loss: 0.204403, acc.: 70.31%] [G loss: 0.488432]\n",
      "##############\n",
      "[2.72637648 2.00345516 5.90666475 4.71209178 3.55086627 5.64744649\n",
      " 4.54146023 4.44457856 4.53476508 4.0652046 ]\n",
      "##########\n",
      "epoch:17 step:16401 [D loss: 0.209835, acc.: 64.84%] [G loss: 0.450465]\n",
      "epoch:17 step:16402 [D loss: 0.262659, acc.: 52.34%] [G loss: 0.434128]\n",
      "epoch:17 step:16403 [D loss: 0.196211, acc.: 68.75%] [G loss: 0.456480]\n",
      "epoch:17 step:16404 [D loss: 0.217960, acc.: 67.97%] [G loss: 0.490724]\n",
      "epoch:17 step:16405 [D loss: 0.227628, acc.: 65.62%] [G loss: 0.480687]\n",
      "epoch:17 step:16406 [D loss: 0.272992, acc.: 52.34%] [G loss: 0.430755]\n",
      "epoch:17 step:16407 [D loss: 0.239893, acc.: 56.25%] [G loss: 0.391531]\n",
      "epoch:17 step:16408 [D loss: 0.234122, acc.: 61.72%] [G loss: 0.385560]\n",
      "epoch:17 step:16409 [D loss: 0.213244, acc.: 66.41%] [G loss: 0.422964]\n",
      "epoch:17 step:16410 [D loss: 0.187925, acc.: 78.12%] [G loss: 0.447145]\n",
      "epoch:17 step:16411 [D loss: 0.251635, acc.: 53.12%] [G loss: 0.446327]\n",
      "epoch:17 step:16412 [D loss: 0.221993, acc.: 57.81%] [G loss: 0.430129]\n",
      "epoch:17 step:16413 [D loss: 0.215838, acc.: 64.06%] [G loss: 0.439187]\n",
      "epoch:17 step:16414 [D loss: 0.203858, acc.: 71.88%] [G loss: 0.451797]\n",
      "epoch:17 step:16415 [D loss: 0.233885, acc.: 60.94%] [G loss: 0.423152]\n",
      "epoch:17 step:16416 [D loss: 0.229739, acc.: 62.50%] [G loss: 0.426142]\n",
      "epoch:17 step:16417 [D loss: 0.190490, acc.: 69.53%] [G loss: 0.425334]\n",
      "epoch:17 step:16418 [D loss: 0.249395, acc.: 57.03%] [G loss: 0.421815]\n",
      "epoch:17 step:16419 [D loss: 0.214600, acc.: 67.19%] [G loss: 0.442330]\n",
      "epoch:17 step:16420 [D loss: 0.225858, acc.: 66.41%] [G loss: 0.428838]\n",
      "epoch:17 step:16421 [D loss: 0.225221, acc.: 60.94%] [G loss: 0.427374]\n",
      "epoch:17 step:16422 [D loss: 0.204345, acc.: 70.31%] [G loss: 0.427691]\n",
      "epoch:17 step:16423 [D loss: 0.214695, acc.: 67.97%] [G loss: 0.461247]\n",
      "epoch:17 step:16424 [D loss: 0.202200, acc.: 66.41%] [G loss: 0.464157]\n",
      "epoch:17 step:16425 [D loss: 0.213740, acc.: 66.41%] [G loss: 0.463158]\n",
      "epoch:17 step:16426 [D loss: 0.215280, acc.: 69.53%] [G loss: 0.457856]\n",
      "epoch:17 step:16427 [D loss: 0.220578, acc.: 64.84%] [G loss: 0.443164]\n",
      "epoch:17 step:16428 [D loss: 0.184796, acc.: 75.78%] [G loss: 0.477495]\n",
      "epoch:17 step:16429 [D loss: 0.257247, acc.: 57.03%] [G loss: 0.449902]\n",
      "epoch:17 step:16430 [D loss: 0.283509, acc.: 50.78%] [G loss: 0.398759]\n",
      "epoch:17 step:16431 [D loss: 0.235431, acc.: 55.47%] [G loss: 0.431319]\n",
      "epoch:17 step:16432 [D loss: 0.229807, acc.: 59.38%] [G loss: 0.433962]\n",
      "epoch:17 step:16433 [D loss: 0.207945, acc.: 64.06%] [G loss: 0.423824]\n",
      "epoch:17 step:16434 [D loss: 0.208194, acc.: 67.19%] [G loss: 0.496500]\n",
      "epoch:17 step:16435 [D loss: 0.237281, acc.: 60.16%] [G loss: 0.496222]\n",
      "epoch:17 step:16436 [D loss: 0.200572, acc.: 67.19%] [G loss: 0.498539]\n",
      "epoch:17 step:16437 [D loss: 0.193668, acc.: 69.53%] [G loss: 0.461682]\n",
      "epoch:17 step:16438 [D loss: 0.250004, acc.: 53.12%] [G loss: 0.428183]\n",
      "epoch:17 step:16439 [D loss: 0.239325, acc.: 56.25%] [G loss: 0.388424]\n",
      "epoch:17 step:16440 [D loss: 0.238925, acc.: 58.59%] [G loss: 0.388438]\n",
      "epoch:17 step:16441 [D loss: 0.232613, acc.: 62.50%] [G loss: 0.428291]\n",
      "epoch:17 step:16442 [D loss: 0.218714, acc.: 63.28%] [G loss: 0.482150]\n",
      "epoch:17 step:16443 [D loss: 0.194496, acc.: 70.31%] [G loss: 0.466975]\n",
      "epoch:17 step:16444 [D loss: 0.217723, acc.: 67.97%] [G loss: 0.460333]\n",
      "epoch:17 step:16445 [D loss: 0.196859, acc.: 67.19%] [G loss: 0.500240]\n",
      "epoch:17 step:16446 [D loss: 0.250516, acc.: 60.16%] [G loss: 0.421744]\n",
      "epoch:17 step:16447 [D loss: 0.257646, acc.: 58.59%] [G loss: 0.421818]\n",
      "epoch:17 step:16448 [D loss: 0.212770, acc.: 64.06%] [G loss: 0.423226]\n",
      "epoch:17 step:16449 [D loss: 0.209963, acc.: 60.94%] [G loss: 0.447973]\n",
      "epoch:17 step:16450 [D loss: 0.216534, acc.: 63.28%] [G loss: 0.447794]\n",
      "epoch:17 step:16451 [D loss: 0.203304, acc.: 71.88%] [G loss: 0.434168]\n",
      "epoch:17 step:16452 [D loss: 0.233223, acc.: 60.16%] [G loss: 0.416628]\n",
      "epoch:17 step:16453 [D loss: 0.227468, acc.: 62.50%] [G loss: 0.454322]\n",
      "epoch:17 step:16454 [D loss: 0.204216, acc.: 67.19%] [G loss: 0.480925]\n",
      "epoch:17 step:16455 [D loss: 0.199640, acc.: 69.53%] [G loss: 0.494513]\n",
      "epoch:17 step:16456 [D loss: 0.214519, acc.: 65.62%] [G loss: 0.443851]\n",
      "epoch:17 step:16457 [D loss: 0.271050, acc.: 57.81%] [G loss: 0.416281]\n",
      "epoch:17 step:16458 [D loss: 0.245740, acc.: 57.03%] [G loss: 0.402337]\n",
      "epoch:17 step:16459 [D loss: 0.217603, acc.: 64.84%] [G loss: 0.439612]\n",
      "epoch:17 step:16460 [D loss: 0.261596, acc.: 54.69%] [G loss: 0.413040]\n",
      "epoch:17 step:16461 [D loss: 0.219095, acc.: 60.94%] [G loss: 0.425140]\n",
      "epoch:17 step:16462 [D loss: 0.236138, acc.: 55.47%] [G loss: 0.417681]\n",
      "epoch:17 step:16463 [D loss: 0.199926, acc.: 67.19%] [G loss: 0.432658]\n",
      "epoch:17 step:16464 [D loss: 0.247169, acc.: 56.25%] [G loss: 0.400056]\n",
      "epoch:17 step:16465 [D loss: 0.223336, acc.: 60.94%] [G loss: 0.389825]\n",
      "epoch:17 step:16466 [D loss: 0.244956, acc.: 60.16%] [G loss: 0.455316]\n",
      "epoch:17 step:16467 [D loss: 0.234914, acc.: 60.94%] [G loss: 0.454414]\n",
      "epoch:17 step:16468 [D loss: 0.218839, acc.: 59.38%] [G loss: 0.446626]\n",
      "epoch:17 step:16469 [D loss: 0.225794, acc.: 65.62%] [G loss: 0.467302]\n",
      "epoch:17 step:16470 [D loss: 0.228699, acc.: 62.50%] [G loss: 0.411050]\n",
      "epoch:17 step:16471 [D loss: 0.266376, acc.: 50.00%] [G loss: 0.344632]\n",
      "epoch:17 step:16472 [D loss: 0.233372, acc.: 62.50%] [G loss: 0.391443]\n",
      "epoch:17 step:16473 [D loss: 0.240692, acc.: 55.47%] [G loss: 0.407779]\n",
      "epoch:17 step:16474 [D loss: 0.222756, acc.: 59.38%] [G loss: 0.451611]\n",
      "epoch:17 step:16475 [D loss: 0.217262, acc.: 65.62%] [G loss: 0.442951]\n",
      "epoch:17 step:16476 [D loss: 0.248039, acc.: 57.81%] [G loss: 0.427508]\n",
      "epoch:17 step:16477 [D loss: 0.223110, acc.: 65.62%] [G loss: 0.453840]\n",
      "epoch:17 step:16478 [D loss: 0.195370, acc.: 69.53%] [G loss: 0.470131]\n",
      "epoch:17 step:16479 [D loss: 0.200334, acc.: 68.75%] [G loss: 0.449285]\n",
      "epoch:17 step:16480 [D loss: 0.218853, acc.: 66.41%] [G loss: 0.469937]\n",
      "epoch:17 step:16481 [D loss: 0.208489, acc.: 66.41%] [G loss: 0.474023]\n",
      "epoch:17 step:16482 [D loss: 0.237444, acc.: 64.84%] [G loss: 0.471034]\n",
      "epoch:17 step:16483 [D loss: 0.199668, acc.: 67.97%] [G loss: 0.443563]\n",
      "epoch:17 step:16484 [D loss: 0.226975, acc.: 63.28%] [G loss: 0.431707]\n",
      "epoch:17 step:16485 [D loss: 0.195129, acc.: 75.00%] [G loss: 0.438301]\n",
      "epoch:17 step:16486 [D loss: 0.195120, acc.: 69.53%] [G loss: 0.434458]\n",
      "epoch:17 step:16487 [D loss: 0.218601, acc.: 57.03%] [G loss: 0.455695]\n",
      "epoch:17 step:16488 [D loss: 0.250298, acc.: 57.03%] [G loss: 0.416071]\n",
      "epoch:17 step:16489 [D loss: 0.246060, acc.: 57.81%] [G loss: 0.404619]\n",
      "epoch:17 step:16490 [D loss: 0.222012, acc.: 60.94%] [G loss: 0.436373]\n",
      "epoch:17 step:16491 [D loss: 0.221108, acc.: 66.41%] [G loss: 0.423784]\n",
      "epoch:17 step:16492 [D loss: 0.201185, acc.: 71.09%] [G loss: 0.449019]\n",
      "epoch:17 step:16493 [D loss: 0.202394, acc.: 70.31%] [G loss: 0.537564]\n",
      "epoch:17 step:16494 [D loss: 0.246520, acc.: 60.16%] [G loss: 0.453042]\n",
      "epoch:17 step:16495 [D loss: 0.261263, acc.: 56.25%] [G loss: 0.484529]\n",
      "epoch:17 step:16496 [D loss: 0.207637, acc.: 68.75%] [G loss: 0.437753]\n",
      "epoch:17 step:16497 [D loss: 0.198136, acc.: 68.75%] [G loss: 0.454626]\n",
      "epoch:17 step:16498 [D loss: 0.287155, acc.: 48.44%] [G loss: 0.376909]\n",
      "epoch:17 step:16499 [D loss: 0.216250, acc.: 62.50%] [G loss: 0.415764]\n",
      "epoch:17 step:16500 [D loss: 0.226425, acc.: 57.81%] [G loss: 0.411438]\n",
      "epoch:17 step:16501 [D loss: 0.210114, acc.: 66.41%] [G loss: 0.418657]\n",
      "epoch:17 step:16502 [D loss: 0.229511, acc.: 59.38%] [G loss: 0.430234]\n",
      "epoch:17 step:16503 [D loss: 0.194368, acc.: 71.88%] [G loss: 0.459092]\n",
      "epoch:17 step:16504 [D loss: 0.199027, acc.: 71.09%] [G loss: 0.486699]\n",
      "epoch:17 step:16505 [D loss: 0.242658, acc.: 56.25%] [G loss: 0.424597]\n",
      "epoch:17 step:16506 [D loss: 0.239460, acc.: 59.38%] [G loss: 0.439947]\n",
      "epoch:17 step:16507 [D loss: 0.224660, acc.: 58.59%] [G loss: 0.434512]\n",
      "epoch:17 step:16508 [D loss: 0.229038, acc.: 59.38%] [G loss: 0.403702]\n",
      "epoch:17 step:16509 [D loss: 0.228092, acc.: 60.94%] [G loss: 0.437647]\n",
      "epoch:17 step:16510 [D loss: 0.228780, acc.: 61.72%] [G loss: 0.423315]\n",
      "epoch:17 step:16511 [D loss: 0.204634, acc.: 64.06%] [G loss: 0.467414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16512 [D loss: 0.229294, acc.: 60.94%] [G loss: 0.453652]\n",
      "epoch:17 step:16513 [D loss: 0.230195, acc.: 60.94%] [G loss: 0.460484]\n",
      "epoch:17 step:16514 [D loss: 0.233175, acc.: 58.59%] [G loss: 0.402238]\n",
      "epoch:17 step:16515 [D loss: 0.225011, acc.: 64.84%] [G loss: 0.383836]\n",
      "epoch:17 step:16516 [D loss: 0.265619, acc.: 52.34%] [G loss: 0.396904]\n",
      "epoch:17 step:16517 [D loss: 0.225969, acc.: 62.50%] [G loss: 0.396247]\n",
      "epoch:17 step:16518 [D loss: 0.218375, acc.: 62.50%] [G loss: 0.415053]\n",
      "epoch:17 step:16519 [D loss: 0.252929, acc.: 58.59%] [G loss: 0.424920]\n",
      "epoch:17 step:16520 [D loss: 0.234604, acc.: 62.50%] [G loss: 0.394595]\n",
      "epoch:17 step:16521 [D loss: 0.204300, acc.: 70.31%] [G loss: 0.433167]\n",
      "epoch:17 step:16522 [D loss: 0.205339, acc.: 72.66%] [G loss: 0.407016]\n",
      "epoch:17 step:16523 [D loss: 0.236985, acc.: 56.25%] [G loss: 0.421206]\n",
      "epoch:17 step:16524 [D loss: 0.232414, acc.: 62.50%] [G loss: 0.407496]\n",
      "epoch:17 step:16525 [D loss: 0.244272, acc.: 55.47%] [G loss: 0.455639]\n",
      "epoch:17 step:16526 [D loss: 0.223474, acc.: 64.06%] [G loss: 0.415401]\n",
      "epoch:17 step:16527 [D loss: 0.192558, acc.: 71.09%] [G loss: 0.465363]\n",
      "epoch:17 step:16528 [D loss: 0.244111, acc.: 55.47%] [G loss: 0.427421]\n",
      "epoch:17 step:16529 [D loss: 0.253537, acc.: 56.25%] [G loss: 0.423153]\n",
      "epoch:17 step:16530 [D loss: 0.239797, acc.: 55.47%] [G loss: 0.448087]\n",
      "epoch:17 step:16531 [D loss: 0.221473, acc.: 64.84%] [G loss: 0.420381]\n",
      "epoch:17 step:16532 [D loss: 0.221053, acc.: 57.81%] [G loss: 0.402565]\n",
      "epoch:17 step:16533 [D loss: 0.226261, acc.: 64.84%] [G loss: 0.445746]\n",
      "epoch:17 step:16534 [D loss: 0.200652, acc.: 69.53%] [G loss: 0.430873]\n",
      "epoch:17 step:16535 [D loss: 0.231408, acc.: 62.50%] [G loss: 0.428723]\n",
      "epoch:17 step:16536 [D loss: 0.220806, acc.: 63.28%] [G loss: 0.415069]\n",
      "epoch:17 step:16537 [D loss: 0.218727, acc.: 66.41%] [G loss: 0.412747]\n",
      "epoch:17 step:16538 [D loss: 0.217543, acc.: 61.72%] [G loss: 0.420404]\n",
      "epoch:17 step:16539 [D loss: 0.212141, acc.: 64.84%] [G loss: 0.420127]\n",
      "epoch:17 step:16540 [D loss: 0.246305, acc.: 54.69%] [G loss: 0.416663]\n",
      "epoch:17 step:16541 [D loss: 0.207993, acc.: 64.84%] [G loss: 0.465798]\n",
      "epoch:17 step:16542 [D loss: 0.227868, acc.: 60.16%] [G loss: 0.426623]\n",
      "epoch:17 step:16543 [D loss: 0.249945, acc.: 56.25%] [G loss: 0.455773]\n",
      "epoch:17 step:16544 [D loss: 0.248815, acc.: 59.38%] [G loss: 0.414972]\n",
      "epoch:17 step:16545 [D loss: 0.219107, acc.: 65.62%] [G loss: 0.437671]\n",
      "epoch:17 step:16546 [D loss: 0.240112, acc.: 58.59%] [G loss: 0.421740]\n",
      "epoch:17 step:16547 [D loss: 0.229754, acc.: 61.72%] [G loss: 0.484301]\n",
      "epoch:17 step:16548 [D loss: 0.241824, acc.: 60.16%] [G loss: 0.418146]\n",
      "epoch:17 step:16549 [D loss: 0.211062, acc.: 64.06%] [G loss: 0.448269]\n",
      "epoch:17 step:16550 [D loss: 0.257107, acc.: 55.47%] [G loss: 0.416654]\n",
      "epoch:17 step:16551 [D loss: 0.211737, acc.: 66.41%] [G loss: 0.443023]\n",
      "epoch:17 step:16552 [D loss: 0.201553, acc.: 70.31%] [G loss: 0.428980]\n",
      "epoch:17 step:16553 [D loss: 0.193287, acc.: 68.75%] [G loss: 0.470764]\n",
      "epoch:17 step:16554 [D loss: 0.232762, acc.: 59.38%] [G loss: 0.403268]\n",
      "epoch:17 step:16555 [D loss: 0.230049, acc.: 66.41%] [G loss: 0.428863]\n",
      "epoch:17 step:16556 [D loss: 0.221683, acc.: 63.28%] [G loss: 0.418732]\n",
      "epoch:17 step:16557 [D loss: 0.238506, acc.: 63.28%] [G loss: 0.421224]\n",
      "epoch:17 step:16558 [D loss: 0.216228, acc.: 68.75%] [G loss: 0.392443]\n",
      "epoch:17 step:16559 [D loss: 0.222638, acc.: 62.50%] [G loss: 0.438593]\n",
      "epoch:17 step:16560 [D loss: 0.200289, acc.: 70.31%] [G loss: 0.428907]\n",
      "epoch:17 step:16561 [D loss: 0.206659, acc.: 67.19%] [G loss: 0.441888]\n",
      "epoch:17 step:16562 [D loss: 0.229297, acc.: 64.06%] [G loss: 0.449182]\n",
      "epoch:17 step:16563 [D loss: 0.185155, acc.: 71.09%] [G loss: 0.456853]\n",
      "epoch:17 step:16564 [D loss: 0.208724, acc.: 65.62%] [G loss: 0.400723]\n",
      "epoch:17 step:16565 [D loss: 0.237248, acc.: 53.12%] [G loss: 0.424249]\n",
      "epoch:17 step:16566 [D loss: 0.217831, acc.: 62.50%] [G loss: 0.420362]\n",
      "epoch:17 step:16567 [D loss: 0.230931, acc.: 60.94%] [G loss: 0.419525]\n",
      "epoch:17 step:16568 [D loss: 0.239579, acc.: 57.03%] [G loss: 0.478579]\n",
      "epoch:17 step:16569 [D loss: 0.253161, acc.: 55.47%] [G loss: 0.436018]\n",
      "epoch:17 step:16570 [D loss: 0.204316, acc.: 64.84%] [G loss: 0.457911]\n",
      "epoch:17 step:16571 [D loss: 0.189372, acc.: 69.53%] [G loss: 0.508142]\n",
      "epoch:17 step:16572 [D loss: 0.246902, acc.: 56.25%] [G loss: 0.454852]\n",
      "epoch:17 step:16573 [D loss: 0.217040, acc.: 64.84%] [G loss: 0.437746]\n",
      "epoch:17 step:16574 [D loss: 0.243962, acc.: 55.47%] [G loss: 0.372254]\n",
      "epoch:17 step:16575 [D loss: 0.215884, acc.: 64.84%] [G loss: 0.414728]\n",
      "epoch:17 step:16576 [D loss: 0.203797, acc.: 68.75%] [G loss: 0.445180]\n",
      "epoch:17 step:16577 [D loss: 0.173105, acc.: 78.12%] [G loss: 0.480381]\n",
      "epoch:17 step:16578 [D loss: 0.205046, acc.: 66.41%] [G loss: 0.496344]\n",
      "epoch:17 step:16579 [D loss: 0.193314, acc.: 72.66%] [G loss: 0.443960]\n",
      "epoch:17 step:16580 [D loss: 0.221357, acc.: 63.28%] [G loss: 0.450202]\n",
      "epoch:17 step:16581 [D loss: 0.234141, acc.: 60.94%] [G loss: 0.425920]\n",
      "epoch:17 step:16582 [D loss: 0.209806, acc.: 66.41%] [G loss: 0.474798]\n",
      "epoch:17 step:16583 [D loss: 0.214658, acc.: 64.06%] [G loss: 0.450906]\n",
      "epoch:17 step:16584 [D loss: 0.246055, acc.: 50.00%] [G loss: 0.417427]\n",
      "epoch:17 step:16585 [D loss: 0.231331, acc.: 60.94%] [G loss: 0.454436]\n",
      "epoch:17 step:16586 [D loss: 0.239661, acc.: 54.69%] [G loss: 0.456409]\n",
      "epoch:17 step:16587 [D loss: 0.223548, acc.: 64.84%] [G loss: 0.443989]\n",
      "epoch:17 step:16588 [D loss: 0.211976, acc.: 61.72%] [G loss: 0.434461]\n",
      "epoch:17 step:16589 [D loss: 0.212729, acc.: 61.72%] [G loss: 0.409272]\n",
      "epoch:17 step:16590 [D loss: 0.211279, acc.: 66.41%] [G loss: 0.464679]\n",
      "epoch:17 step:16591 [D loss: 0.221105, acc.: 60.94%] [G loss: 0.423968]\n",
      "epoch:17 step:16592 [D loss: 0.237900, acc.: 57.81%] [G loss: 0.402913]\n",
      "epoch:17 step:16593 [D loss: 0.254551, acc.: 55.47%] [G loss: 0.406117]\n",
      "epoch:17 step:16594 [D loss: 0.229428, acc.: 58.59%] [G loss: 0.475805]\n",
      "epoch:17 step:16595 [D loss: 0.220323, acc.: 63.28%] [G loss: 0.444245]\n",
      "epoch:17 step:16596 [D loss: 0.259718, acc.: 56.25%] [G loss: 0.450255]\n",
      "epoch:17 step:16597 [D loss: 0.228009, acc.: 56.25%] [G loss: 0.440226]\n",
      "epoch:17 step:16598 [D loss: 0.219326, acc.: 60.94%] [G loss: 0.439909]\n",
      "epoch:17 step:16599 [D loss: 0.238401, acc.: 62.50%] [G loss: 0.434443]\n",
      "epoch:17 step:16600 [D loss: 0.235579, acc.: 60.94%] [G loss: 0.397213]\n",
      "##############\n",
      "[2.51168692 1.85978863 5.80794591 4.60365842 3.71568175 5.5705217\n",
      " 4.48321232 4.64701355 4.46494477 3.83216057]\n",
      "##########\n",
      "epoch:17 step:16601 [D loss: 0.218965, acc.: 65.62%] [G loss: 0.423359]\n",
      "epoch:17 step:16602 [D loss: 0.222431, acc.: 63.28%] [G loss: 0.456554]\n",
      "epoch:17 step:16603 [D loss: 0.205179, acc.: 68.75%] [G loss: 0.482035]\n",
      "epoch:17 step:16604 [D loss: 0.235524, acc.: 57.03%] [G loss: 0.456200]\n",
      "epoch:17 step:16605 [D loss: 0.230838, acc.: 65.62%] [G loss: 0.427972]\n",
      "epoch:17 step:16606 [D loss: 0.199304, acc.: 69.53%] [G loss: 0.413971]\n",
      "epoch:17 step:16607 [D loss: 0.214159, acc.: 67.19%] [G loss: 0.427065]\n",
      "epoch:17 step:16608 [D loss: 0.212291, acc.: 64.06%] [G loss: 0.427045]\n",
      "epoch:17 step:16609 [D loss: 0.212392, acc.: 64.06%] [G loss: 0.453970]\n",
      "epoch:17 step:16610 [D loss: 0.201330, acc.: 67.19%] [G loss: 0.446603]\n",
      "epoch:17 step:16611 [D loss: 0.238861, acc.: 57.03%] [G loss: 0.441772]\n",
      "epoch:17 step:16612 [D loss: 0.230944, acc.: 62.50%] [G loss: 0.438294]\n",
      "epoch:17 step:16613 [D loss: 0.235944, acc.: 57.81%] [G loss: 0.431155]\n",
      "epoch:17 step:16614 [D loss: 0.218376, acc.: 62.50%] [G loss: 0.415431]\n",
      "epoch:17 step:16615 [D loss: 0.204360, acc.: 64.06%] [G loss: 0.429455]\n",
      "epoch:17 step:16616 [D loss: 0.220538, acc.: 60.94%] [G loss: 0.442221]\n",
      "epoch:17 step:16617 [D loss: 0.224372, acc.: 66.41%] [G loss: 0.436562]\n",
      "epoch:17 step:16618 [D loss: 0.217798, acc.: 69.53%] [G loss: 0.430715]\n",
      "epoch:17 step:16619 [D loss: 0.214911, acc.: 66.41%] [G loss: 0.465510]\n",
      "epoch:17 step:16620 [D loss: 0.220721, acc.: 63.28%] [G loss: 0.466145]\n",
      "epoch:17 step:16621 [D loss: 0.229453, acc.: 61.72%] [G loss: 0.429547]\n",
      "epoch:17 step:16622 [D loss: 0.206332, acc.: 67.97%] [G loss: 0.439391]\n",
      "epoch:17 step:16623 [D loss: 0.192216, acc.: 69.53%] [G loss: 0.469238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16624 [D loss: 0.212423, acc.: 68.75%] [G loss: 0.479997]\n",
      "epoch:17 step:16625 [D loss: 0.266729, acc.: 48.44%] [G loss: 0.443530]\n",
      "epoch:17 step:16626 [D loss: 0.222790, acc.: 64.84%] [G loss: 0.452356]\n",
      "epoch:17 step:16627 [D loss: 0.221435, acc.: 64.84%] [G loss: 0.418367]\n",
      "epoch:17 step:16628 [D loss: 0.216179, acc.: 70.31%] [G loss: 0.445732]\n",
      "epoch:17 step:16629 [D loss: 0.235973, acc.: 60.94%] [G loss: 0.432851]\n",
      "epoch:17 step:16630 [D loss: 0.212407, acc.: 67.19%] [G loss: 0.470189]\n",
      "epoch:17 step:16631 [D loss: 0.237012, acc.: 60.16%] [G loss: 0.432992]\n",
      "epoch:17 step:16632 [D loss: 0.269660, acc.: 51.56%] [G loss: 0.412934]\n",
      "epoch:17 step:16633 [D loss: 0.239742, acc.: 53.91%] [G loss: 0.436293]\n",
      "epoch:17 step:16634 [D loss: 0.207999, acc.: 69.53%] [G loss: 0.438087]\n",
      "epoch:17 step:16635 [D loss: 0.220748, acc.: 58.59%] [G loss: 0.418088]\n",
      "epoch:17 step:16636 [D loss: 0.235921, acc.: 61.72%] [G loss: 0.412804]\n",
      "epoch:17 step:16637 [D loss: 0.197211, acc.: 67.19%] [G loss: 0.465960]\n",
      "epoch:17 step:16638 [D loss: 0.209345, acc.: 68.75%] [G loss: 0.436544]\n",
      "epoch:17 step:16639 [D loss: 0.280078, acc.: 46.88%] [G loss: 0.417616]\n",
      "epoch:17 step:16640 [D loss: 0.217841, acc.: 60.94%] [G loss: 0.426142]\n",
      "epoch:17 step:16641 [D loss: 0.213167, acc.: 64.84%] [G loss: 0.434081]\n",
      "epoch:17 step:16642 [D loss: 0.227839, acc.: 70.31%] [G loss: 0.399043]\n",
      "epoch:17 step:16643 [D loss: 0.246706, acc.: 56.25%] [G loss: 0.439038]\n",
      "epoch:17 step:16644 [D loss: 0.226133, acc.: 64.84%] [G loss: 0.447008]\n",
      "epoch:17 step:16645 [D loss: 0.268452, acc.: 51.56%] [G loss: 0.417078]\n",
      "epoch:17 step:16646 [D loss: 0.239572, acc.: 57.81%] [G loss: 0.427854]\n",
      "epoch:17 step:16647 [D loss: 0.211163, acc.: 71.09%] [G loss: 0.446984]\n",
      "epoch:17 step:16648 [D loss: 0.193151, acc.: 71.88%] [G loss: 0.444045]\n",
      "epoch:17 step:16649 [D loss: 0.230328, acc.: 62.50%] [G loss: 0.461966]\n",
      "epoch:17 step:16650 [D loss: 0.224137, acc.: 61.72%] [G loss: 0.452723]\n",
      "epoch:17 step:16651 [D loss: 0.271137, acc.: 53.12%] [G loss: 0.434378]\n",
      "epoch:17 step:16652 [D loss: 0.213975, acc.: 66.41%] [G loss: 0.434791]\n",
      "epoch:17 step:16653 [D loss: 0.208510, acc.: 68.75%] [G loss: 0.439977]\n",
      "epoch:17 step:16654 [D loss: 0.212871, acc.: 66.41%] [G loss: 0.421550]\n",
      "epoch:17 step:16655 [D loss: 0.218962, acc.: 61.72%] [G loss: 0.429277]\n",
      "epoch:17 step:16656 [D loss: 0.238345, acc.: 57.81%] [G loss: 0.395234]\n",
      "epoch:17 step:16657 [D loss: 0.216205, acc.: 70.31%] [G loss: 0.418745]\n",
      "epoch:17 step:16658 [D loss: 0.229121, acc.: 60.94%] [G loss: 0.388341]\n",
      "epoch:17 step:16659 [D loss: 0.225669, acc.: 64.06%] [G loss: 0.442856]\n",
      "epoch:17 step:16660 [D loss: 0.224066, acc.: 59.38%] [G loss: 0.441883]\n",
      "epoch:17 step:16661 [D loss: 0.230322, acc.: 67.97%] [G loss: 0.441466]\n",
      "epoch:17 step:16662 [D loss: 0.224655, acc.: 68.75%] [G loss: 0.453358]\n",
      "epoch:17 step:16663 [D loss: 0.254645, acc.: 53.12%] [G loss: 0.448716]\n",
      "epoch:17 step:16664 [D loss: 0.248460, acc.: 57.03%] [G loss: 0.411219]\n",
      "epoch:17 step:16665 [D loss: 0.194273, acc.: 74.22%] [G loss: 0.457114]\n",
      "epoch:17 step:16666 [D loss: 0.190450, acc.: 75.78%] [G loss: 0.411026]\n",
      "epoch:17 step:16667 [D loss: 0.268737, acc.: 50.00%] [G loss: 0.384091]\n",
      "epoch:17 step:16668 [D loss: 0.230283, acc.: 60.94%] [G loss: 0.428731]\n",
      "epoch:17 step:16669 [D loss: 0.252327, acc.: 56.25%] [G loss: 0.400999]\n",
      "epoch:17 step:16670 [D loss: 0.229438, acc.: 64.84%] [G loss: 0.421177]\n",
      "epoch:17 step:16671 [D loss: 0.233569, acc.: 57.03%] [G loss: 0.390159]\n",
      "epoch:17 step:16672 [D loss: 0.214931, acc.: 70.31%] [G loss: 0.416019]\n",
      "epoch:17 step:16673 [D loss: 0.225396, acc.: 66.41%] [G loss: 0.428112]\n",
      "epoch:17 step:16674 [D loss: 0.239245, acc.: 60.94%] [G loss: 0.428184]\n",
      "epoch:17 step:16675 [D loss: 0.228823, acc.: 60.16%] [G loss: 0.418034]\n",
      "epoch:17 step:16676 [D loss: 0.225527, acc.: 62.50%] [G loss: 0.398649]\n",
      "epoch:17 step:16677 [D loss: 0.233805, acc.: 64.84%] [G loss: 0.395979]\n",
      "epoch:17 step:16678 [D loss: 0.225299, acc.: 60.94%] [G loss: 0.432798]\n",
      "epoch:17 step:16679 [D loss: 0.203670, acc.: 65.62%] [G loss: 0.443570]\n",
      "epoch:17 step:16680 [D loss: 0.225710, acc.: 60.94%] [G loss: 0.412758]\n",
      "epoch:17 step:16681 [D loss: 0.253316, acc.: 58.59%] [G loss: 0.419303]\n",
      "epoch:17 step:16682 [D loss: 0.232380, acc.: 59.38%] [G loss: 0.457226]\n",
      "epoch:17 step:16683 [D loss: 0.232273, acc.: 61.72%] [G loss: 0.450915]\n",
      "epoch:17 step:16684 [D loss: 0.219937, acc.: 64.84%] [G loss: 0.501743]\n",
      "epoch:17 step:16685 [D loss: 0.231360, acc.: 60.16%] [G loss: 0.432367]\n",
      "epoch:17 step:16686 [D loss: 0.244539, acc.: 56.25%] [G loss: 0.386888]\n",
      "epoch:17 step:16687 [D loss: 0.231950, acc.: 65.62%] [G loss: 0.418965]\n",
      "epoch:17 step:16688 [D loss: 0.238182, acc.: 57.03%] [G loss: 0.409660]\n",
      "epoch:17 step:16689 [D loss: 0.222760, acc.: 64.06%] [G loss: 0.444250]\n",
      "epoch:17 step:16690 [D loss: 0.215767, acc.: 59.38%] [G loss: 0.407528]\n",
      "epoch:17 step:16691 [D loss: 0.246709, acc.: 55.47%] [G loss: 0.423711]\n",
      "epoch:17 step:16692 [D loss: 0.244027, acc.: 60.16%] [G loss: 0.397700]\n",
      "epoch:17 step:16693 [D loss: 0.214108, acc.: 64.84%] [G loss: 0.457957]\n",
      "epoch:17 step:16694 [D loss: 0.249276, acc.: 57.81%] [G loss: 0.376037]\n",
      "epoch:17 step:16695 [D loss: 0.229839, acc.: 63.28%] [G loss: 0.416404]\n",
      "epoch:17 step:16696 [D loss: 0.210401, acc.: 65.62%] [G loss: 0.439474]\n",
      "epoch:17 step:16697 [D loss: 0.235139, acc.: 57.03%] [G loss: 0.401469]\n",
      "epoch:17 step:16698 [D loss: 0.218437, acc.: 68.75%] [G loss: 0.468751]\n",
      "epoch:17 step:16699 [D loss: 0.225442, acc.: 63.28%] [G loss: 0.474752]\n",
      "epoch:17 step:16700 [D loss: 0.206137, acc.: 63.28%] [G loss: 0.446035]\n",
      "epoch:17 step:16701 [D loss: 0.241499, acc.: 54.69%] [G loss: 0.448666]\n",
      "epoch:17 step:16702 [D loss: 0.207662, acc.: 67.97%] [G loss: 0.463565]\n",
      "epoch:17 step:16703 [D loss: 0.205452, acc.: 64.06%] [G loss: 0.440914]\n",
      "epoch:17 step:16704 [D loss: 0.189629, acc.: 69.53%] [G loss: 0.465407]\n",
      "epoch:17 step:16705 [D loss: 0.247502, acc.: 58.59%] [G loss: 0.412302]\n",
      "epoch:17 step:16706 [D loss: 0.224098, acc.: 63.28%] [G loss: 0.403686]\n",
      "epoch:17 step:16707 [D loss: 0.208240, acc.: 61.72%] [G loss: 0.450196]\n",
      "epoch:17 step:16708 [D loss: 0.241369, acc.: 62.50%] [G loss: 0.382741]\n",
      "epoch:17 step:16709 [D loss: 0.216173, acc.: 64.06%] [G loss: 0.429393]\n",
      "epoch:17 step:16710 [D loss: 0.214577, acc.: 65.62%] [G loss: 0.506944]\n",
      "epoch:17 step:16711 [D loss: 0.203667, acc.: 67.97%] [G loss: 0.520682]\n",
      "epoch:17 step:16712 [D loss: 0.269780, acc.: 53.91%] [G loss: 0.423709]\n",
      "epoch:17 step:16713 [D loss: 0.267455, acc.: 49.22%] [G loss: 0.412737]\n",
      "epoch:17 step:16714 [D loss: 0.225073, acc.: 62.50%] [G loss: 0.416248]\n",
      "epoch:17 step:16715 [D loss: 0.229991, acc.: 55.47%] [G loss: 0.408685]\n",
      "epoch:17 step:16716 [D loss: 0.254690, acc.: 57.81%] [G loss: 0.386276]\n",
      "epoch:17 step:16717 [D loss: 0.246219, acc.: 53.12%] [G loss: 0.400254]\n",
      "epoch:17 step:16718 [D loss: 0.222146, acc.: 62.50%] [G loss: 0.394716]\n",
      "epoch:17 step:16719 [D loss: 0.219702, acc.: 67.97%] [G loss: 0.436256]\n",
      "epoch:17 step:16720 [D loss: 0.253007, acc.: 56.25%] [G loss: 0.426059]\n",
      "epoch:17 step:16721 [D loss: 0.201753, acc.: 75.78%] [G loss: 0.422834]\n",
      "epoch:17 step:16722 [D loss: 0.230587, acc.: 61.72%] [G loss: 0.426244]\n",
      "epoch:17 step:16723 [D loss: 0.256259, acc.: 53.91%] [G loss: 0.444158]\n",
      "epoch:17 step:16724 [D loss: 0.253862, acc.: 51.56%] [G loss: 0.410360]\n",
      "epoch:17 step:16725 [D loss: 0.220622, acc.: 63.28%] [G loss: 0.430965]\n",
      "epoch:17 step:16726 [D loss: 0.233945, acc.: 59.38%] [G loss: 0.438361]\n",
      "epoch:17 step:16727 [D loss: 0.210962, acc.: 65.62%] [G loss: 0.409150]\n",
      "epoch:17 step:16728 [D loss: 0.242067, acc.: 60.16%] [G loss: 0.381146]\n",
      "epoch:17 step:16729 [D loss: 0.248130, acc.: 57.81%] [G loss: 0.432186]\n",
      "epoch:17 step:16730 [D loss: 0.209568, acc.: 68.75%] [G loss: 0.412296]\n",
      "epoch:17 step:16731 [D loss: 0.229744, acc.: 67.19%] [G loss: 0.436563]\n",
      "epoch:17 step:16732 [D loss: 0.197728, acc.: 73.44%] [G loss: 0.476466]\n",
      "epoch:17 step:16733 [D loss: 0.220887, acc.: 59.38%] [G loss: 0.438011]\n",
      "epoch:17 step:16734 [D loss: 0.237615, acc.: 56.25%] [G loss: 0.391974]\n",
      "epoch:17 step:16735 [D loss: 0.221241, acc.: 65.62%] [G loss: 0.410699]\n",
      "epoch:17 step:16736 [D loss: 0.224520, acc.: 60.94%] [G loss: 0.450524]\n",
      "epoch:17 step:16737 [D loss: 0.243105, acc.: 59.38%] [G loss: 0.421957]\n",
      "epoch:17 step:16738 [D loss: 0.228866, acc.: 64.06%] [G loss: 0.441958]\n",
      "epoch:17 step:16739 [D loss: 0.228078, acc.: 64.06%] [G loss: 0.407503]\n",
      "epoch:17 step:16740 [D loss: 0.229378, acc.: 64.06%] [G loss: 0.439741]\n",
      "epoch:17 step:16741 [D loss: 0.245726, acc.: 61.72%] [G loss: 0.421968]\n",
      "epoch:17 step:16742 [D loss: 0.217412, acc.: 69.53%] [G loss: 0.429852]\n",
      "epoch:17 step:16743 [D loss: 0.227482, acc.: 62.50%] [G loss: 0.443888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16744 [D loss: 0.207840, acc.: 64.06%] [G loss: 0.439134]\n",
      "epoch:17 step:16745 [D loss: 0.208391, acc.: 63.28%] [G loss: 0.420535]\n",
      "epoch:17 step:16746 [D loss: 0.259180, acc.: 53.91%] [G loss: 0.426512]\n",
      "epoch:17 step:16747 [D loss: 0.237798, acc.: 62.50%] [G loss: 0.419085]\n",
      "epoch:17 step:16748 [D loss: 0.223667, acc.: 61.72%] [G loss: 0.446063]\n",
      "epoch:17 step:16749 [D loss: 0.279420, acc.: 50.00%] [G loss: 0.406209]\n",
      "epoch:17 step:16750 [D loss: 0.225035, acc.: 57.03%] [G loss: 0.422571]\n",
      "epoch:17 step:16751 [D loss: 0.206311, acc.: 67.97%] [G loss: 0.435274]\n",
      "epoch:17 step:16752 [D loss: 0.196284, acc.: 67.97%] [G loss: 0.409438]\n",
      "epoch:17 step:16753 [D loss: 0.236574, acc.: 60.94%] [G loss: 0.406427]\n",
      "epoch:17 step:16754 [D loss: 0.222790, acc.: 66.41%] [G loss: 0.407900]\n",
      "epoch:17 step:16755 [D loss: 0.219005, acc.: 65.62%] [G loss: 0.470621]\n",
      "epoch:17 step:16756 [D loss: 0.245246, acc.: 55.47%] [G loss: 0.423833]\n",
      "epoch:17 step:16757 [D loss: 0.257592, acc.: 54.69%] [G loss: 0.407083]\n",
      "epoch:17 step:16758 [D loss: 0.236205, acc.: 60.16%] [G loss: 0.423516]\n",
      "epoch:17 step:16759 [D loss: 0.254928, acc.: 58.59%] [G loss: 0.437092]\n",
      "epoch:17 step:16760 [D loss: 0.222858, acc.: 60.94%] [G loss: 0.418426]\n",
      "epoch:17 step:16761 [D loss: 0.215547, acc.: 67.97%] [G loss: 0.432779]\n",
      "epoch:17 step:16762 [D loss: 0.201252, acc.: 67.97%] [G loss: 0.455179]\n",
      "epoch:17 step:16763 [D loss: 0.236919, acc.: 58.59%] [G loss: 0.422161]\n",
      "epoch:17 step:16764 [D loss: 0.231977, acc.: 60.16%] [G loss: 0.402158]\n",
      "epoch:17 step:16765 [D loss: 0.208945, acc.: 64.84%] [G loss: 0.419413]\n",
      "epoch:17 step:16766 [D loss: 0.216248, acc.: 61.72%] [G loss: 0.396157]\n",
      "epoch:17 step:16767 [D loss: 0.206841, acc.: 69.53%] [G loss: 0.439774]\n",
      "epoch:17 step:16768 [D loss: 0.217623, acc.: 68.75%] [G loss: 0.449163]\n",
      "epoch:17 step:16769 [D loss: 0.228167, acc.: 64.06%] [G loss: 0.435823]\n",
      "epoch:17 step:16770 [D loss: 0.233572, acc.: 59.38%] [G loss: 0.450503]\n",
      "epoch:17 step:16771 [D loss: 0.173946, acc.: 75.78%] [G loss: 0.448507]\n",
      "epoch:17 step:16772 [D loss: 0.228041, acc.: 65.62%] [G loss: 0.456574]\n",
      "epoch:17 step:16773 [D loss: 0.234870, acc.: 56.25%] [G loss: 0.468088]\n",
      "epoch:17 step:16774 [D loss: 0.218221, acc.: 68.75%] [G loss: 0.448318]\n",
      "epoch:17 step:16775 [D loss: 0.244781, acc.: 53.91%] [G loss: 0.450187]\n",
      "epoch:17 step:16776 [D loss: 0.279942, acc.: 47.66%] [G loss: 0.351017]\n",
      "epoch:17 step:16777 [D loss: 0.233026, acc.: 58.59%] [G loss: 0.395302]\n",
      "epoch:17 step:16778 [D loss: 0.216793, acc.: 67.19%] [G loss: 0.420064]\n",
      "epoch:17 step:16779 [D loss: 0.264939, acc.: 53.12%] [G loss: 0.445943]\n",
      "epoch:17 step:16780 [D loss: 0.241735, acc.: 53.91%] [G loss: 0.466576]\n",
      "epoch:17 step:16781 [D loss: 0.239860, acc.: 55.47%] [G loss: 0.415657]\n",
      "epoch:17 step:16782 [D loss: 0.221381, acc.: 66.41%] [G loss: 0.446099]\n",
      "epoch:17 step:16783 [D loss: 0.231798, acc.: 57.81%] [G loss: 0.429092]\n",
      "epoch:17 step:16784 [D loss: 0.243095, acc.: 57.81%] [G loss: 0.428663]\n",
      "epoch:17 step:16785 [D loss: 0.219862, acc.: 62.50%] [G loss: 0.417156]\n",
      "epoch:17 step:16786 [D loss: 0.226611, acc.: 64.84%] [G loss: 0.421587]\n",
      "epoch:17 step:16787 [D loss: 0.272975, acc.: 47.66%] [G loss: 0.428957]\n",
      "epoch:17 step:16788 [D loss: 0.228576, acc.: 63.28%] [G loss: 0.407740]\n",
      "epoch:17 step:16789 [D loss: 0.183590, acc.: 75.00%] [G loss: 0.430177]\n",
      "epoch:17 step:16790 [D loss: 0.242904, acc.: 57.03%] [G loss: 0.422321]\n",
      "epoch:17 step:16791 [D loss: 0.252754, acc.: 52.34%] [G loss: 0.360326]\n",
      "epoch:17 step:16792 [D loss: 0.230652, acc.: 61.72%] [G loss: 0.403085]\n",
      "epoch:17 step:16793 [D loss: 0.223742, acc.: 61.72%] [G loss: 0.393838]\n",
      "epoch:17 step:16794 [D loss: 0.255801, acc.: 57.81%] [G loss: 0.410526]\n",
      "epoch:17 step:16795 [D loss: 0.227408, acc.: 60.94%] [G loss: 0.433028]\n",
      "epoch:17 step:16796 [D loss: 0.242282, acc.: 56.25%] [G loss: 0.435411]\n",
      "epoch:17 step:16797 [D loss: 0.225375, acc.: 60.94%] [G loss: 0.426074]\n",
      "epoch:17 step:16798 [D loss: 0.240884, acc.: 55.47%] [G loss: 0.411673]\n",
      "epoch:17 step:16799 [D loss: 0.223957, acc.: 64.84%] [G loss: 0.437783]\n",
      "epoch:17 step:16800 [D loss: 0.229516, acc.: 58.59%] [G loss: 0.406500]\n",
      "##############\n",
      "[2.4941109  1.87792093 5.92760621 4.61824943 3.52704829 5.32576993\n",
      " 4.23758172 4.76261739 4.51488657 3.83065499]\n",
      "##########\n",
      "epoch:17 step:16801 [D loss: 0.235767, acc.: 63.28%] [G loss: 0.424484]\n",
      "epoch:17 step:16802 [D loss: 0.211889, acc.: 66.41%] [G loss: 0.453982]\n",
      "epoch:17 step:16803 [D loss: 0.226365, acc.: 60.94%] [G loss: 0.414217]\n",
      "epoch:17 step:16804 [D loss: 0.185580, acc.: 76.56%] [G loss: 0.430545]\n",
      "epoch:17 step:16805 [D loss: 0.238024, acc.: 58.59%] [G loss: 0.436166]\n",
      "epoch:17 step:16806 [D loss: 0.245835, acc.: 58.59%] [G loss: 0.420148]\n",
      "epoch:17 step:16807 [D loss: 0.228437, acc.: 61.72%] [G loss: 0.401950]\n",
      "epoch:17 step:16808 [D loss: 0.216617, acc.: 63.28%] [G loss: 0.432432]\n",
      "epoch:17 step:16809 [D loss: 0.237748, acc.: 57.03%] [G loss: 0.376405]\n",
      "epoch:17 step:16810 [D loss: 0.266297, acc.: 45.31%] [G loss: 0.379544]\n",
      "epoch:17 step:16811 [D loss: 0.216333, acc.: 64.84%] [G loss: 0.439025]\n",
      "epoch:17 step:16812 [D loss: 0.236746, acc.: 58.59%] [G loss: 0.432433]\n",
      "epoch:17 step:16813 [D loss: 0.222069, acc.: 63.28%] [G loss: 0.452991]\n",
      "epoch:17 step:16814 [D loss: 0.208749, acc.: 68.75%] [G loss: 0.435459]\n",
      "epoch:17 step:16815 [D loss: 0.218737, acc.: 62.50%] [G loss: 0.488470]\n",
      "epoch:17 step:16816 [D loss: 0.211682, acc.: 66.41%] [G loss: 0.543261]\n",
      "epoch:17 step:16817 [D loss: 0.210928, acc.: 63.28%] [G loss: 0.480106]\n",
      "epoch:17 step:16818 [D loss: 0.225389, acc.: 67.97%] [G loss: 0.426822]\n",
      "epoch:17 step:16819 [D loss: 0.191519, acc.: 68.75%] [G loss: 0.486819]\n",
      "epoch:17 step:16820 [D loss: 0.277808, acc.: 48.44%] [G loss: 0.394812]\n",
      "epoch:17 step:16821 [D loss: 0.240329, acc.: 63.28%] [G loss: 0.422284]\n",
      "epoch:17 step:16822 [D loss: 0.205687, acc.: 68.75%] [G loss: 0.459376]\n",
      "epoch:17 step:16823 [D loss: 0.201072, acc.: 68.75%] [G loss: 0.474612]\n",
      "epoch:17 step:16824 [D loss: 0.185604, acc.: 71.09%] [G loss: 0.481630]\n",
      "epoch:17 step:16825 [D loss: 0.197310, acc.: 78.12%] [G loss: 0.427133]\n",
      "epoch:17 step:16826 [D loss: 0.221184, acc.: 63.28%] [G loss: 0.446780]\n",
      "epoch:17 step:16827 [D loss: 0.218030, acc.: 67.19%] [G loss: 0.444886]\n",
      "epoch:17 step:16828 [D loss: 0.191219, acc.: 67.97%] [G loss: 0.515857]\n",
      "epoch:17 step:16829 [D loss: 0.214893, acc.: 65.62%] [G loss: 0.483645]\n",
      "epoch:17 step:16830 [D loss: 0.208878, acc.: 66.41%] [G loss: 0.449175]\n",
      "epoch:17 step:16831 [D loss: 0.243001, acc.: 60.94%] [G loss: 0.458084]\n",
      "epoch:17 step:16832 [D loss: 0.217169, acc.: 71.88%] [G loss: 0.431199]\n",
      "epoch:17 step:16833 [D loss: 0.250722, acc.: 56.25%] [G loss: 0.428374]\n",
      "epoch:17 step:16834 [D loss: 0.226917, acc.: 58.59%] [G loss: 0.407835]\n",
      "epoch:17 step:16835 [D loss: 0.208888, acc.: 68.75%] [G loss: 0.511008]\n",
      "epoch:17 step:16836 [D loss: 0.242397, acc.: 57.81%] [G loss: 0.419271]\n",
      "epoch:17 step:16837 [D loss: 0.222773, acc.: 65.62%] [G loss: 0.438451]\n",
      "epoch:17 step:16838 [D loss: 0.204813, acc.: 63.28%] [G loss: 0.442693]\n",
      "epoch:17 step:16839 [D loss: 0.214002, acc.: 65.62%] [G loss: 0.435154]\n",
      "epoch:17 step:16840 [D loss: 0.207566, acc.: 73.44%] [G loss: 0.479257]\n",
      "epoch:17 step:16841 [D loss: 0.199888, acc.: 67.97%] [G loss: 0.509552]\n",
      "epoch:17 step:16842 [D loss: 0.221361, acc.: 64.06%] [G loss: 0.469969]\n",
      "epoch:17 step:16843 [D loss: 0.223267, acc.: 64.84%] [G loss: 0.443569]\n",
      "epoch:17 step:16844 [D loss: 0.276078, acc.: 53.91%] [G loss: 0.385300]\n",
      "epoch:17 step:16845 [D loss: 0.220853, acc.: 60.16%] [G loss: 0.424913]\n",
      "epoch:17 step:16846 [D loss: 0.227795, acc.: 64.84%] [G loss: 0.488737]\n",
      "epoch:17 step:16847 [D loss: 0.199714, acc.: 73.44%] [G loss: 0.460136]\n",
      "epoch:17 step:16848 [D loss: 0.184371, acc.: 71.88%] [G loss: 0.514030]\n",
      "epoch:17 step:16849 [D loss: 0.292970, acc.: 51.56%] [G loss: 0.460708]\n",
      "epoch:17 step:16850 [D loss: 0.213076, acc.: 64.06%] [G loss: 0.474269]\n",
      "epoch:17 step:16851 [D loss: 0.247116, acc.: 59.38%] [G loss: 0.414906]\n",
      "epoch:17 step:16852 [D loss: 0.187212, acc.: 73.44%] [G loss: 0.494155]\n",
      "epoch:17 step:16853 [D loss: 0.179556, acc.: 78.12%] [G loss: 0.479656]\n",
      "epoch:17 step:16854 [D loss: 0.214655, acc.: 68.75%] [G loss: 0.455981]\n",
      "epoch:17 step:16855 [D loss: 0.166216, acc.: 78.12%] [G loss: 0.510769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16856 [D loss: 0.196134, acc.: 71.88%] [G loss: 0.516027]\n",
      "epoch:17 step:16857 [D loss: 0.301635, acc.: 53.12%] [G loss: 0.501577]\n",
      "epoch:17 step:16858 [D loss: 0.239403, acc.: 62.50%] [G loss: 0.527054]\n",
      "epoch:17 step:16859 [D loss: 0.202027, acc.: 71.09%] [G loss: 0.523060]\n",
      "epoch:17 step:16860 [D loss: 0.236285, acc.: 63.28%] [G loss: 0.431802]\n",
      "epoch:17 step:16861 [D loss: 0.269103, acc.: 50.00%] [G loss: 0.373043]\n",
      "epoch:17 step:16862 [D loss: 0.228339, acc.: 63.28%] [G loss: 0.475996]\n",
      "epoch:17 step:16863 [D loss: 0.219171, acc.: 64.06%] [G loss: 0.507800]\n",
      "epoch:17 step:16864 [D loss: 0.227685, acc.: 59.38%] [G loss: 0.456908]\n",
      "epoch:17 step:16865 [D loss: 0.165544, acc.: 79.69%] [G loss: 0.451884]\n",
      "epoch:17 step:16866 [D loss: 0.189580, acc.: 71.88%] [G loss: 0.529060]\n",
      "epoch:18 step:16867 [D loss: 0.223766, acc.: 63.28%] [G loss: 0.493064]\n",
      "epoch:18 step:16868 [D loss: 0.278203, acc.: 53.91%] [G loss: 0.433579]\n",
      "epoch:18 step:16869 [D loss: 0.224086, acc.: 61.72%] [G loss: 0.481935]\n",
      "epoch:18 step:16870 [D loss: 0.254503, acc.: 59.38%] [G loss: 0.436494]\n",
      "epoch:18 step:16871 [D loss: 0.243687, acc.: 53.91%] [G loss: 0.455880]\n",
      "epoch:18 step:16872 [D loss: 0.244071, acc.: 55.47%] [G loss: 0.480989]\n",
      "epoch:18 step:16873 [D loss: 0.227421, acc.: 67.97%] [G loss: 0.442396]\n",
      "epoch:18 step:16874 [D loss: 0.223010, acc.: 63.28%] [G loss: 0.442973]\n",
      "epoch:18 step:16875 [D loss: 0.191667, acc.: 70.31%] [G loss: 0.445046]\n",
      "epoch:18 step:16876 [D loss: 0.223993, acc.: 63.28%] [G loss: 0.448196]\n",
      "epoch:18 step:16877 [D loss: 0.213850, acc.: 71.88%] [G loss: 0.476189]\n",
      "epoch:18 step:16878 [D loss: 0.219101, acc.: 64.84%] [G loss: 0.464228]\n",
      "epoch:18 step:16879 [D loss: 0.212161, acc.: 65.62%] [G loss: 0.486891]\n",
      "epoch:18 step:16880 [D loss: 0.208254, acc.: 66.41%] [G loss: 0.449389]\n",
      "epoch:18 step:16881 [D loss: 0.187252, acc.: 71.88%] [G loss: 0.479406]\n",
      "epoch:18 step:16882 [D loss: 0.203337, acc.: 70.31%] [G loss: 0.478135]\n",
      "epoch:18 step:16883 [D loss: 0.253742, acc.: 57.03%] [G loss: 0.456311]\n",
      "epoch:18 step:16884 [D loss: 0.215323, acc.: 62.50%] [G loss: 0.463294]\n",
      "epoch:18 step:16885 [D loss: 0.251031, acc.: 53.91%] [G loss: 0.424510]\n",
      "epoch:18 step:16886 [D loss: 0.239798, acc.: 59.38%] [G loss: 0.424453]\n",
      "epoch:18 step:16887 [D loss: 0.238849, acc.: 60.16%] [G loss: 0.434143]\n",
      "epoch:18 step:16888 [D loss: 0.207250, acc.: 67.19%] [G loss: 0.464174]\n",
      "epoch:18 step:16889 [D loss: 0.230546, acc.: 64.06%] [G loss: 0.460188]\n",
      "epoch:18 step:16890 [D loss: 0.198828, acc.: 72.66%] [G loss: 0.443636]\n",
      "epoch:18 step:16891 [D loss: 0.207013, acc.: 66.41%] [G loss: 0.457175]\n",
      "epoch:18 step:16892 [D loss: 0.262504, acc.: 51.56%] [G loss: 0.397821]\n",
      "epoch:18 step:16893 [D loss: 0.243184, acc.: 57.81%] [G loss: 0.367976]\n",
      "epoch:18 step:16894 [D loss: 0.224842, acc.: 62.50%] [G loss: 0.435731]\n",
      "epoch:18 step:16895 [D loss: 0.211677, acc.: 64.06%] [G loss: 0.430746]\n",
      "epoch:18 step:16896 [D loss: 0.218707, acc.: 67.19%] [G loss: 0.415211]\n",
      "epoch:18 step:16897 [D loss: 0.243006, acc.: 57.03%] [G loss: 0.466837]\n",
      "epoch:18 step:16898 [D loss: 0.235850, acc.: 60.94%] [G loss: 0.432809]\n",
      "epoch:18 step:16899 [D loss: 0.221461, acc.: 67.19%] [G loss: 0.414465]\n",
      "epoch:18 step:16900 [D loss: 0.262052, acc.: 53.91%] [G loss: 0.405316]\n",
      "epoch:18 step:16901 [D loss: 0.223100, acc.: 64.06%] [G loss: 0.428830]\n",
      "epoch:18 step:16902 [D loss: 0.224835, acc.: 62.50%] [G loss: 0.408068]\n",
      "epoch:18 step:16903 [D loss: 0.238719, acc.: 59.38%] [G loss: 0.419621]\n",
      "epoch:18 step:16904 [D loss: 0.241990, acc.: 60.16%] [G loss: 0.404718]\n",
      "epoch:18 step:16905 [D loss: 0.224937, acc.: 66.41%] [G loss: 0.390938]\n",
      "epoch:18 step:16906 [D loss: 0.214216, acc.: 67.97%] [G loss: 0.451553]\n",
      "epoch:18 step:16907 [D loss: 0.230060, acc.: 60.94%] [G loss: 0.458036]\n",
      "epoch:18 step:16908 [D loss: 0.218060, acc.: 67.97%] [G loss: 0.439069]\n",
      "epoch:18 step:16909 [D loss: 0.232749, acc.: 63.28%] [G loss: 0.436220]\n",
      "epoch:18 step:16910 [D loss: 0.247167, acc.: 53.91%] [G loss: 0.419824]\n",
      "epoch:18 step:16911 [D loss: 0.210879, acc.: 66.41%] [G loss: 0.461478]\n",
      "epoch:18 step:16912 [D loss: 0.252244, acc.: 52.34%] [G loss: 0.428519]\n",
      "epoch:18 step:16913 [D loss: 0.207917, acc.: 66.41%] [G loss: 0.443989]\n",
      "epoch:18 step:16914 [D loss: 0.215848, acc.: 71.09%] [G loss: 0.440653]\n",
      "epoch:18 step:16915 [D loss: 0.211027, acc.: 67.19%] [G loss: 0.435019]\n",
      "epoch:18 step:16916 [D loss: 0.202738, acc.: 71.88%] [G loss: 0.454536]\n",
      "epoch:18 step:16917 [D loss: 0.242295, acc.: 57.81%] [G loss: 0.461660]\n",
      "epoch:18 step:16918 [D loss: 0.230716, acc.: 62.50%] [G loss: 0.460025]\n",
      "epoch:18 step:16919 [D loss: 0.219085, acc.: 68.75%] [G loss: 0.449438]\n",
      "epoch:18 step:16920 [D loss: 0.232198, acc.: 62.50%] [G loss: 0.444083]\n",
      "epoch:18 step:16921 [D loss: 0.221332, acc.: 67.19%] [G loss: 0.450754]\n",
      "epoch:18 step:16922 [D loss: 0.244403, acc.: 57.81%] [G loss: 0.437541]\n",
      "epoch:18 step:16923 [D loss: 0.265124, acc.: 52.34%] [G loss: 0.431406]\n",
      "epoch:18 step:16924 [D loss: 0.232106, acc.: 57.03%] [G loss: 0.464562]\n",
      "epoch:18 step:16925 [D loss: 0.233160, acc.: 64.06%] [G loss: 0.468968]\n",
      "epoch:18 step:16926 [D loss: 0.224981, acc.: 59.38%] [G loss: 0.439341]\n",
      "epoch:18 step:16927 [D loss: 0.236017, acc.: 58.59%] [G loss: 0.422827]\n",
      "epoch:18 step:16928 [D loss: 0.201251, acc.: 72.66%] [G loss: 0.454214]\n",
      "epoch:18 step:16929 [D loss: 0.222366, acc.: 61.72%] [G loss: 0.395901]\n",
      "epoch:18 step:16930 [D loss: 0.223868, acc.: 63.28%] [G loss: 0.429207]\n",
      "epoch:18 step:16931 [D loss: 0.249422, acc.: 59.38%] [G loss: 0.399648]\n",
      "epoch:18 step:16932 [D loss: 0.225403, acc.: 57.81%] [G loss: 0.435199]\n",
      "epoch:18 step:16933 [D loss: 0.224533, acc.: 67.19%] [G loss: 0.427191]\n",
      "epoch:18 step:16934 [D loss: 0.236018, acc.: 59.38%] [G loss: 0.402632]\n",
      "epoch:18 step:16935 [D loss: 0.214099, acc.: 67.19%] [G loss: 0.399126]\n",
      "epoch:18 step:16936 [D loss: 0.206032, acc.: 73.44%] [G loss: 0.475128]\n",
      "epoch:18 step:16937 [D loss: 0.238875, acc.: 57.81%] [G loss: 0.446747]\n",
      "epoch:18 step:16938 [D loss: 0.252712, acc.: 53.12%] [G loss: 0.396067]\n",
      "epoch:18 step:16939 [D loss: 0.229820, acc.: 61.72%] [G loss: 0.406200]\n",
      "epoch:18 step:16940 [D loss: 0.209543, acc.: 67.19%] [G loss: 0.440133]\n",
      "epoch:18 step:16941 [D loss: 0.245262, acc.: 64.06%] [G loss: 0.440760]\n",
      "epoch:18 step:16942 [D loss: 0.209479, acc.: 69.53%] [G loss: 0.424807]\n",
      "epoch:18 step:16943 [D loss: 0.231207, acc.: 64.06%] [G loss: 0.421100]\n",
      "epoch:18 step:16944 [D loss: 0.255108, acc.: 53.12%] [G loss: 0.408496]\n",
      "epoch:18 step:16945 [D loss: 0.217975, acc.: 66.41%] [G loss: 0.412340]\n",
      "epoch:18 step:16946 [D loss: 0.231767, acc.: 65.62%] [G loss: 0.421541]\n",
      "epoch:18 step:16947 [D loss: 0.233786, acc.: 57.81%] [G loss: 0.384004]\n",
      "epoch:18 step:16948 [D loss: 0.210314, acc.: 69.53%] [G loss: 0.438483]\n",
      "epoch:18 step:16949 [D loss: 0.221692, acc.: 66.41%] [G loss: 0.431336]\n",
      "epoch:18 step:16950 [D loss: 0.223390, acc.: 67.97%] [G loss: 0.414346]\n",
      "epoch:18 step:16951 [D loss: 0.255989, acc.: 55.47%] [G loss: 0.404527]\n",
      "epoch:18 step:16952 [D loss: 0.214170, acc.: 67.97%] [G loss: 0.442260]\n",
      "epoch:18 step:16953 [D loss: 0.223263, acc.: 62.50%] [G loss: 0.447590]\n",
      "epoch:18 step:16954 [D loss: 0.214567, acc.: 67.97%] [G loss: 0.458880]\n",
      "epoch:18 step:16955 [D loss: 0.226921, acc.: 65.62%] [G loss: 0.437078]\n",
      "epoch:18 step:16956 [D loss: 0.213617, acc.: 64.06%] [G loss: 0.463486]\n",
      "epoch:18 step:16957 [D loss: 0.226598, acc.: 64.84%] [G loss: 0.414435]\n",
      "epoch:18 step:16958 [D loss: 0.205193, acc.: 68.75%] [G loss: 0.422482]\n",
      "epoch:18 step:16959 [D loss: 0.188745, acc.: 78.91%] [G loss: 0.452961]\n",
      "epoch:18 step:16960 [D loss: 0.229609, acc.: 64.06%] [G loss: 0.441299]\n",
      "epoch:18 step:16961 [D loss: 0.230838, acc.: 59.38%] [G loss: 0.440560]\n",
      "epoch:18 step:16962 [D loss: 0.211267, acc.: 62.50%] [G loss: 0.409774]\n",
      "epoch:18 step:16963 [D loss: 0.200989, acc.: 71.09%] [G loss: 0.494117]\n",
      "epoch:18 step:16964 [D loss: 0.252202, acc.: 57.03%] [G loss: 0.433825]\n",
      "epoch:18 step:16965 [D loss: 0.245569, acc.: 62.50%] [G loss: 0.423064]\n",
      "epoch:18 step:16966 [D loss: 0.187338, acc.: 75.78%] [G loss: 0.480579]\n",
      "epoch:18 step:16967 [D loss: 0.211685, acc.: 68.75%] [G loss: 0.424007]\n",
      "epoch:18 step:16968 [D loss: 0.227382, acc.: 59.38%] [G loss: 0.436324]\n",
      "epoch:18 step:16969 [D loss: 0.238294, acc.: 57.03%] [G loss: 0.395985]\n",
      "epoch:18 step:16970 [D loss: 0.254122, acc.: 52.34%] [G loss: 0.383035]\n",
      "epoch:18 step:16971 [D loss: 0.244713, acc.: 53.91%] [G loss: 0.429634]\n",
      "epoch:18 step:16972 [D loss: 0.212505, acc.: 66.41%] [G loss: 0.454411]\n",
      "epoch:18 step:16973 [D loss: 0.224209, acc.: 60.16%] [G loss: 0.447647]\n",
      "epoch:18 step:16974 [D loss: 0.268766, acc.: 52.34%] [G loss: 0.481343]\n",
      "epoch:18 step:16975 [D loss: 0.231830, acc.: 64.06%] [G loss: 0.448282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16976 [D loss: 0.229063, acc.: 61.72%] [G loss: 0.425250]\n",
      "epoch:18 step:16977 [D loss: 0.228635, acc.: 60.94%] [G loss: 0.403475]\n",
      "epoch:18 step:16978 [D loss: 0.193601, acc.: 72.66%] [G loss: 0.443328]\n",
      "epoch:18 step:16979 [D loss: 0.194988, acc.: 75.00%] [G loss: 0.452361]\n",
      "epoch:18 step:16980 [D loss: 0.215932, acc.: 64.84%] [G loss: 0.462312]\n",
      "epoch:18 step:16981 [D loss: 0.192237, acc.: 75.00%] [G loss: 0.441699]\n",
      "epoch:18 step:16982 [D loss: 0.211970, acc.: 64.84%] [G loss: 0.471261]\n",
      "epoch:18 step:16983 [D loss: 0.208304, acc.: 65.62%] [G loss: 0.484536]\n",
      "epoch:18 step:16984 [D loss: 0.194904, acc.: 70.31%] [G loss: 0.473790]\n",
      "epoch:18 step:16985 [D loss: 0.184723, acc.: 74.22%] [G loss: 0.467345]\n",
      "epoch:18 step:16986 [D loss: 0.247542, acc.: 57.03%] [G loss: 0.484915]\n",
      "epoch:18 step:16987 [D loss: 0.224588, acc.: 60.94%] [G loss: 0.451689]\n",
      "epoch:18 step:16988 [D loss: 0.217060, acc.: 66.41%] [G loss: 0.448452]\n",
      "epoch:18 step:16989 [D loss: 0.207282, acc.: 67.19%] [G loss: 0.454448]\n",
      "epoch:18 step:16990 [D loss: 0.231759, acc.: 58.59%] [G loss: 0.439718]\n",
      "epoch:18 step:16991 [D loss: 0.225281, acc.: 59.38%] [G loss: 0.455839]\n",
      "epoch:18 step:16992 [D loss: 0.203584, acc.: 67.97%] [G loss: 0.440598]\n",
      "epoch:18 step:16993 [D loss: 0.214772, acc.: 61.72%] [G loss: 0.447602]\n",
      "epoch:18 step:16994 [D loss: 0.229659, acc.: 64.06%] [G loss: 0.433281]\n",
      "epoch:18 step:16995 [D loss: 0.239419, acc.: 59.38%] [G loss: 0.387909]\n",
      "epoch:18 step:16996 [D loss: 0.216871, acc.: 69.53%] [G loss: 0.441399]\n",
      "epoch:18 step:16997 [D loss: 0.221285, acc.: 62.50%] [G loss: 0.460381]\n",
      "epoch:18 step:16998 [D loss: 0.222697, acc.: 62.50%] [G loss: 0.458345]\n",
      "epoch:18 step:16999 [D loss: 0.238714, acc.: 64.84%] [G loss: 0.473822]\n",
      "epoch:18 step:17000 [D loss: 0.204861, acc.: 68.75%] [G loss: 0.441203]\n",
      "##############\n",
      "[2.43848576 1.83198061 6.00742066 4.60205084 3.52652786 5.52504731\n",
      " 4.72701747 4.6394662  4.36394235 3.95248418]\n",
      "##########\n",
      "epoch:18 step:17001 [D loss: 0.242651, acc.: 61.72%] [G loss: 0.486158]\n",
      "epoch:18 step:17002 [D loss: 0.216371, acc.: 71.09%] [G loss: 0.488918]\n",
      "epoch:18 step:17003 [D loss: 0.275986, acc.: 58.59%] [G loss: 0.416787]\n",
      "epoch:18 step:17004 [D loss: 0.245902, acc.: 62.50%] [G loss: 0.421986]\n",
      "epoch:18 step:17005 [D loss: 0.226743, acc.: 60.94%] [G loss: 0.417157]\n",
      "epoch:18 step:17006 [D loss: 0.248239, acc.: 54.69%] [G loss: 0.413680]\n",
      "epoch:18 step:17007 [D loss: 0.243068, acc.: 59.38%] [G loss: 0.403620]\n",
      "epoch:18 step:17008 [D loss: 0.233193, acc.: 59.38%] [G loss: 0.409426]\n",
      "epoch:18 step:17009 [D loss: 0.232816, acc.: 60.94%] [G loss: 0.439593]\n",
      "epoch:18 step:17010 [D loss: 0.226430, acc.: 59.38%] [G loss: 0.421830]\n",
      "epoch:18 step:17011 [D loss: 0.253331, acc.: 52.34%] [G loss: 0.405126]\n",
      "epoch:18 step:17012 [D loss: 0.236229, acc.: 62.50%] [G loss: 0.441094]\n",
      "epoch:18 step:17013 [D loss: 0.253971, acc.: 59.38%] [G loss: 0.417316]\n",
      "epoch:18 step:17014 [D loss: 0.253625, acc.: 53.12%] [G loss: 0.417016]\n",
      "epoch:18 step:17015 [D loss: 0.222309, acc.: 64.06%] [G loss: 0.389937]\n",
      "epoch:18 step:17016 [D loss: 0.226390, acc.: 58.59%] [G loss: 0.428346]\n",
      "epoch:18 step:17017 [D loss: 0.216514, acc.: 64.06%] [G loss: 0.419395]\n",
      "epoch:18 step:17018 [D loss: 0.223313, acc.: 66.41%] [G loss: 0.448194]\n",
      "epoch:18 step:17019 [D loss: 0.253636, acc.: 53.12%] [G loss: 0.405643]\n",
      "epoch:18 step:17020 [D loss: 0.211574, acc.: 69.53%] [G loss: 0.418636]\n",
      "epoch:18 step:17021 [D loss: 0.222878, acc.: 64.06%] [G loss: 0.397505]\n",
      "epoch:18 step:17022 [D loss: 0.223595, acc.: 64.06%] [G loss: 0.455215]\n",
      "epoch:18 step:17023 [D loss: 0.238838, acc.: 57.81%] [G loss: 0.455014]\n",
      "epoch:18 step:17024 [D loss: 0.231571, acc.: 60.94%] [G loss: 0.437643]\n",
      "epoch:18 step:17025 [D loss: 0.229139, acc.: 66.41%] [G loss: 0.400471]\n",
      "epoch:18 step:17026 [D loss: 0.286689, acc.: 46.88%] [G loss: 0.418952]\n",
      "epoch:18 step:17027 [D loss: 0.252157, acc.: 58.59%] [G loss: 0.444673]\n",
      "epoch:18 step:17028 [D loss: 0.244953, acc.: 60.94%] [G loss: 0.415082]\n",
      "epoch:18 step:17029 [D loss: 0.234406, acc.: 60.94%] [G loss: 0.439979]\n",
      "epoch:18 step:17030 [D loss: 0.215752, acc.: 67.19%] [G loss: 0.426072]\n",
      "epoch:18 step:17031 [D loss: 0.216855, acc.: 62.50%] [G loss: 0.443344]\n",
      "epoch:18 step:17032 [D loss: 0.222186, acc.: 60.94%] [G loss: 0.399418]\n",
      "epoch:18 step:17033 [D loss: 0.207185, acc.: 66.41%] [G loss: 0.419457]\n",
      "epoch:18 step:17034 [D loss: 0.186142, acc.: 75.00%] [G loss: 0.435722]\n",
      "epoch:18 step:17035 [D loss: 0.242713, acc.: 63.28%] [G loss: 0.433547]\n",
      "epoch:18 step:17036 [D loss: 0.222148, acc.: 67.97%] [G loss: 0.421082]\n",
      "epoch:18 step:17037 [D loss: 0.223596, acc.: 61.72%] [G loss: 0.418139]\n",
      "epoch:18 step:17038 [D loss: 0.223892, acc.: 58.59%] [G loss: 0.389848]\n",
      "epoch:18 step:17039 [D loss: 0.218084, acc.: 65.62%] [G loss: 0.392630]\n",
      "epoch:18 step:17040 [D loss: 0.244395, acc.: 54.69%] [G loss: 0.422340]\n",
      "epoch:18 step:17041 [D loss: 0.226069, acc.: 63.28%] [G loss: 0.462190]\n",
      "epoch:18 step:17042 [D loss: 0.215556, acc.: 60.94%] [G loss: 0.477914]\n",
      "epoch:18 step:17043 [D loss: 0.257094, acc.: 53.91%] [G loss: 0.384998]\n",
      "epoch:18 step:17044 [D loss: 0.228065, acc.: 63.28%] [G loss: 0.412512]\n",
      "epoch:18 step:17045 [D loss: 0.235199, acc.: 56.25%] [G loss: 0.406591]\n",
      "epoch:18 step:17046 [D loss: 0.240611, acc.: 59.38%] [G loss: 0.426948]\n",
      "epoch:18 step:17047 [D loss: 0.234938, acc.: 62.50%] [G loss: 0.404232]\n",
      "epoch:18 step:17048 [D loss: 0.250102, acc.: 58.59%] [G loss: 0.416211]\n",
      "epoch:18 step:17049 [D loss: 0.256398, acc.: 54.69%] [G loss: 0.391701]\n",
      "epoch:18 step:17050 [D loss: 0.220366, acc.: 66.41%] [G loss: 0.438733]\n",
      "epoch:18 step:17051 [D loss: 0.244212, acc.: 58.59%] [G loss: 0.449860]\n",
      "epoch:18 step:17052 [D loss: 0.240334, acc.: 57.03%] [G loss: 0.397778]\n",
      "epoch:18 step:17053 [D loss: 0.226673, acc.: 61.72%] [G loss: 0.405751]\n",
      "epoch:18 step:17054 [D loss: 0.252014, acc.: 58.59%] [G loss: 0.386999]\n",
      "epoch:18 step:17055 [D loss: 0.224022, acc.: 64.84%] [G loss: 0.435415]\n",
      "epoch:18 step:17056 [D loss: 0.217361, acc.: 64.06%] [G loss: 0.425192]\n",
      "epoch:18 step:17057 [D loss: 0.226152, acc.: 65.62%] [G loss: 0.392089]\n",
      "epoch:18 step:17058 [D loss: 0.214660, acc.: 66.41%] [G loss: 0.433652]\n",
      "epoch:18 step:17059 [D loss: 0.239496, acc.: 56.25%] [G loss: 0.415057]\n",
      "epoch:18 step:17060 [D loss: 0.203326, acc.: 67.97%] [G loss: 0.461354]\n",
      "epoch:18 step:17061 [D loss: 0.200422, acc.: 71.88%] [G loss: 0.444459]\n",
      "epoch:18 step:17062 [D loss: 0.227523, acc.: 67.97%] [G loss: 0.444819]\n",
      "epoch:18 step:17063 [D loss: 0.204544, acc.: 68.75%] [G loss: 0.450654]\n",
      "epoch:18 step:17064 [D loss: 0.193175, acc.: 70.31%] [G loss: 0.438565]\n",
      "epoch:18 step:17065 [D loss: 0.235309, acc.: 57.81%] [G loss: 0.431582]\n",
      "epoch:18 step:17066 [D loss: 0.256524, acc.: 54.69%] [G loss: 0.400217]\n",
      "epoch:18 step:17067 [D loss: 0.244358, acc.: 58.59%] [G loss: 0.341496]\n",
      "epoch:18 step:17068 [D loss: 0.225552, acc.: 64.06%] [G loss: 0.436235]\n",
      "epoch:18 step:17069 [D loss: 0.286654, acc.: 43.75%] [G loss: 0.369398]\n",
      "epoch:18 step:17070 [D loss: 0.202035, acc.: 70.31%] [G loss: 0.448558]\n",
      "epoch:18 step:17071 [D loss: 0.236166, acc.: 60.94%] [G loss: 0.463863]\n",
      "epoch:18 step:17072 [D loss: 0.228836, acc.: 63.28%] [G loss: 0.429914]\n",
      "epoch:18 step:17073 [D loss: 0.207785, acc.: 66.41%] [G loss: 0.463293]\n",
      "epoch:18 step:17074 [D loss: 0.191048, acc.: 68.75%] [G loss: 0.469275]\n",
      "epoch:18 step:17075 [D loss: 0.190617, acc.: 68.75%] [G loss: 0.462170]\n",
      "epoch:18 step:17076 [D loss: 0.249497, acc.: 53.91%] [G loss: 0.424378]\n",
      "epoch:18 step:17077 [D loss: 0.253398, acc.: 54.69%] [G loss: 0.424713]\n",
      "epoch:18 step:17078 [D loss: 0.218171, acc.: 61.72%] [G loss: 0.445759]\n",
      "epoch:18 step:17079 [D loss: 0.232262, acc.: 57.03%] [G loss: 0.399801]\n",
      "epoch:18 step:17080 [D loss: 0.257953, acc.: 53.91%] [G loss: 0.394877]\n",
      "epoch:18 step:17081 [D loss: 0.233284, acc.: 61.72%] [G loss: 0.425902]\n",
      "epoch:18 step:17082 [D loss: 0.213984, acc.: 63.28%] [G loss: 0.437878]\n",
      "epoch:18 step:17083 [D loss: 0.227065, acc.: 60.94%] [G loss: 0.438596]\n",
      "epoch:18 step:17084 [D loss: 0.175888, acc.: 75.78%] [G loss: 0.469272]\n",
      "epoch:18 step:17085 [D loss: 0.188926, acc.: 75.00%] [G loss: 0.452407]\n",
      "epoch:18 step:17086 [D loss: 0.274357, acc.: 54.69%] [G loss: 0.422800]\n",
      "epoch:18 step:17087 [D loss: 0.203573, acc.: 69.53%] [G loss: 0.505883]\n",
      "epoch:18 step:17088 [D loss: 0.220639, acc.: 64.06%] [G loss: 0.482909]\n",
      "epoch:18 step:17089 [D loss: 0.214058, acc.: 67.97%] [G loss: 0.472855]\n",
      "epoch:18 step:17090 [D loss: 0.221591, acc.: 64.84%] [G loss: 0.474851]\n",
      "epoch:18 step:17091 [D loss: 0.233172, acc.: 60.16%] [G loss: 0.402084]\n",
      "epoch:18 step:17092 [D loss: 0.239880, acc.: 56.25%] [G loss: 0.416056]\n",
      "epoch:18 step:17093 [D loss: 0.211456, acc.: 70.31%] [G loss: 0.442155]\n",
      "epoch:18 step:17094 [D loss: 0.232809, acc.: 59.38%] [G loss: 0.415831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17095 [D loss: 0.212596, acc.: 68.75%] [G loss: 0.395871]\n",
      "epoch:18 step:17096 [D loss: 0.234442, acc.: 62.50%] [G loss: 0.423559]\n",
      "epoch:18 step:17097 [D loss: 0.176524, acc.: 75.78%] [G loss: 0.466848]\n",
      "epoch:18 step:17098 [D loss: 0.156616, acc.: 82.03%] [G loss: 0.467703]\n",
      "epoch:18 step:17099 [D loss: 0.231704, acc.: 62.50%] [G loss: 0.454260]\n",
      "epoch:18 step:17100 [D loss: 0.272092, acc.: 55.47%] [G loss: 0.418825]\n",
      "epoch:18 step:17101 [D loss: 0.228323, acc.: 58.59%] [G loss: 0.431130]\n",
      "epoch:18 step:17102 [D loss: 0.216767, acc.: 64.06%] [G loss: 0.435612]\n",
      "epoch:18 step:17103 [D loss: 0.241359, acc.: 60.94%] [G loss: 0.454285]\n",
      "epoch:18 step:17104 [D loss: 0.202744, acc.: 69.53%] [G loss: 0.452669]\n",
      "epoch:18 step:17105 [D loss: 0.214008, acc.: 62.50%] [G loss: 0.456028]\n",
      "epoch:18 step:17106 [D loss: 0.227205, acc.: 60.94%] [G loss: 0.422496]\n",
      "epoch:18 step:17107 [D loss: 0.217794, acc.: 60.94%] [G loss: 0.462989]\n",
      "epoch:18 step:17108 [D loss: 0.204551, acc.: 70.31%] [G loss: 0.479055]\n",
      "epoch:18 step:17109 [D loss: 0.220728, acc.: 67.19%] [G loss: 0.471817]\n",
      "epoch:18 step:17110 [D loss: 0.208940, acc.: 65.62%] [G loss: 0.451789]\n",
      "epoch:18 step:17111 [D loss: 0.223405, acc.: 61.72%] [G loss: 0.447547]\n",
      "epoch:18 step:17112 [D loss: 0.229993, acc.: 67.19%] [G loss: 0.451240]\n",
      "epoch:18 step:17113 [D loss: 0.225585, acc.: 65.62%] [G loss: 0.453795]\n",
      "epoch:18 step:17114 [D loss: 0.215271, acc.: 67.19%] [G loss: 0.438722]\n",
      "epoch:18 step:17115 [D loss: 0.223038, acc.: 64.06%] [G loss: 0.481411]\n",
      "epoch:18 step:17116 [D loss: 0.255781, acc.: 55.47%] [G loss: 0.400225]\n",
      "epoch:18 step:17117 [D loss: 0.263875, acc.: 52.34%] [G loss: 0.425177]\n",
      "epoch:18 step:17118 [D loss: 0.244955, acc.: 57.03%] [G loss: 0.439976]\n",
      "epoch:18 step:17119 [D loss: 0.236227, acc.: 58.59%] [G loss: 0.444897]\n",
      "epoch:18 step:17120 [D loss: 0.220258, acc.: 65.62%] [G loss: 0.463608]\n",
      "epoch:18 step:17121 [D loss: 0.207030, acc.: 67.97%] [G loss: 0.456717]\n",
      "epoch:18 step:17122 [D loss: 0.231007, acc.: 62.50%] [G loss: 0.414429]\n",
      "epoch:18 step:17123 [D loss: 0.216824, acc.: 66.41%] [G loss: 0.400169]\n",
      "epoch:18 step:17124 [D loss: 0.230922, acc.: 62.50%] [G loss: 0.448606]\n",
      "epoch:18 step:17125 [D loss: 0.196157, acc.: 71.09%] [G loss: 0.440815]\n",
      "epoch:18 step:17126 [D loss: 0.234602, acc.: 62.50%] [G loss: 0.413828]\n",
      "epoch:18 step:17127 [D loss: 0.241144, acc.: 57.03%] [G loss: 0.410518]\n",
      "epoch:18 step:17128 [D loss: 0.231283, acc.: 62.50%] [G loss: 0.446272]\n",
      "epoch:18 step:17129 [D loss: 0.244965, acc.: 59.38%] [G loss: 0.454439]\n",
      "epoch:18 step:17130 [D loss: 0.218161, acc.: 67.19%] [G loss: 0.430589]\n",
      "epoch:18 step:17131 [D loss: 0.233637, acc.: 59.38%] [G loss: 0.408406]\n",
      "epoch:18 step:17132 [D loss: 0.233966, acc.: 64.06%] [G loss: 0.408174]\n",
      "epoch:18 step:17133 [D loss: 0.210679, acc.: 63.28%] [G loss: 0.410290]\n",
      "epoch:18 step:17134 [D loss: 0.224268, acc.: 64.06%] [G loss: 0.398292]\n",
      "epoch:18 step:17135 [D loss: 0.207738, acc.: 68.75%] [G loss: 0.427799]\n",
      "epoch:18 step:17136 [D loss: 0.200320, acc.: 73.44%] [G loss: 0.423988]\n",
      "epoch:18 step:17137 [D loss: 0.213933, acc.: 64.06%] [G loss: 0.446643]\n",
      "epoch:18 step:17138 [D loss: 0.212959, acc.: 64.84%] [G loss: 0.431295]\n",
      "epoch:18 step:17139 [D loss: 0.223226, acc.: 58.59%] [G loss: 0.422961]\n",
      "epoch:18 step:17140 [D loss: 0.195341, acc.: 69.53%] [G loss: 0.428680]\n",
      "epoch:18 step:17141 [D loss: 0.219045, acc.: 63.28%] [G loss: 0.434114]\n",
      "epoch:18 step:17142 [D loss: 0.207782, acc.: 70.31%] [G loss: 0.466214]\n",
      "epoch:18 step:17143 [D loss: 0.264398, acc.: 50.78%] [G loss: 0.396297]\n",
      "epoch:18 step:17144 [D loss: 0.247865, acc.: 55.47%] [G loss: 0.440583]\n",
      "epoch:18 step:17145 [D loss: 0.243235, acc.: 57.03%] [G loss: 0.451875]\n",
      "epoch:18 step:17146 [D loss: 0.199989, acc.: 71.88%] [G loss: 0.465587]\n",
      "epoch:18 step:17147 [D loss: 0.265380, acc.: 53.12%] [G loss: 0.396627]\n",
      "epoch:18 step:17148 [D loss: 0.229847, acc.: 57.81%] [G loss: 0.413958]\n",
      "epoch:18 step:17149 [D loss: 0.222039, acc.: 60.16%] [G loss: 0.405035]\n",
      "epoch:18 step:17150 [D loss: 0.238680, acc.: 60.16%] [G loss: 0.397862]\n",
      "epoch:18 step:17151 [D loss: 0.222251, acc.: 64.84%] [G loss: 0.410105]\n",
      "epoch:18 step:17152 [D loss: 0.238554, acc.: 66.41%] [G loss: 0.429701]\n",
      "epoch:18 step:17153 [D loss: 0.230021, acc.: 60.94%] [G loss: 0.446090]\n",
      "epoch:18 step:17154 [D loss: 0.198735, acc.: 73.44%] [G loss: 0.441993]\n",
      "epoch:18 step:17155 [D loss: 0.198403, acc.: 68.75%] [G loss: 0.454210]\n",
      "epoch:18 step:17156 [D loss: 0.219743, acc.: 62.50%] [G loss: 0.397236]\n",
      "epoch:18 step:17157 [D loss: 0.249245, acc.: 53.91%] [G loss: 0.427475]\n",
      "epoch:18 step:17158 [D loss: 0.229170, acc.: 57.03%] [G loss: 0.423483]\n",
      "epoch:18 step:17159 [D loss: 0.218013, acc.: 64.06%] [G loss: 0.466427]\n",
      "epoch:18 step:17160 [D loss: 0.245040, acc.: 50.00%] [G loss: 0.403595]\n",
      "epoch:18 step:17161 [D loss: 0.209857, acc.: 67.97%] [G loss: 0.434003]\n",
      "epoch:18 step:17162 [D loss: 0.213330, acc.: 67.19%] [G loss: 0.433313]\n",
      "epoch:18 step:17163 [D loss: 0.240026, acc.: 60.94%] [G loss: 0.426410]\n",
      "epoch:18 step:17164 [D loss: 0.191138, acc.: 73.44%] [G loss: 0.459542]\n",
      "epoch:18 step:17165 [D loss: 0.205734, acc.: 69.53%] [G loss: 0.455501]\n",
      "epoch:18 step:17166 [D loss: 0.219712, acc.: 63.28%] [G loss: 0.414356]\n",
      "epoch:18 step:17167 [D loss: 0.222611, acc.: 62.50%] [G loss: 0.464413]\n",
      "epoch:18 step:17168 [D loss: 0.200662, acc.: 68.75%] [G loss: 0.441979]\n",
      "epoch:18 step:17169 [D loss: 0.230142, acc.: 61.72%] [G loss: 0.440949]\n",
      "epoch:18 step:17170 [D loss: 0.224459, acc.: 64.06%] [G loss: 0.432483]\n",
      "epoch:18 step:17171 [D loss: 0.209526, acc.: 67.19%] [G loss: 0.461945]\n",
      "epoch:18 step:17172 [D loss: 0.204756, acc.: 64.84%] [G loss: 0.489359]\n",
      "epoch:18 step:17173 [D loss: 0.234641, acc.: 62.50%] [G loss: 0.443479]\n",
      "epoch:18 step:17174 [D loss: 0.217850, acc.: 61.72%] [G loss: 0.441433]\n",
      "epoch:18 step:17175 [D loss: 0.212406, acc.: 66.41%] [G loss: 0.432425]\n",
      "epoch:18 step:17176 [D loss: 0.220847, acc.: 64.06%] [G loss: 0.415627]\n",
      "epoch:18 step:17177 [D loss: 0.219439, acc.: 64.84%] [G loss: 0.414320]\n",
      "epoch:18 step:17178 [D loss: 0.205301, acc.: 68.75%] [G loss: 0.478131]\n",
      "epoch:18 step:17179 [D loss: 0.186148, acc.: 72.66%] [G loss: 0.489848]\n",
      "epoch:18 step:17180 [D loss: 0.205367, acc.: 61.72%] [G loss: 0.514478]\n",
      "epoch:18 step:17181 [D loss: 0.179253, acc.: 74.22%] [G loss: 0.504221]\n",
      "epoch:18 step:17182 [D loss: 0.294343, acc.: 46.88%] [G loss: 0.445152]\n",
      "epoch:18 step:17183 [D loss: 0.231138, acc.: 58.59%] [G loss: 0.411900]\n",
      "epoch:18 step:17184 [D loss: 0.205328, acc.: 68.75%] [G loss: 0.410773]\n",
      "epoch:18 step:17185 [D loss: 0.230643, acc.: 60.94%] [G loss: 0.424003]\n",
      "epoch:18 step:17186 [D loss: 0.212333, acc.: 73.44%] [G loss: 0.451199]\n",
      "epoch:18 step:17187 [D loss: 0.193995, acc.: 74.22%] [G loss: 0.460182]\n",
      "epoch:18 step:17188 [D loss: 0.213419, acc.: 64.06%] [G loss: 0.461214]\n",
      "epoch:18 step:17189 [D loss: 0.245430, acc.: 54.69%] [G loss: 0.415695]\n",
      "epoch:18 step:17190 [D loss: 0.230396, acc.: 63.28%] [G loss: 0.398809]\n",
      "epoch:18 step:17191 [D loss: 0.222976, acc.: 64.84%] [G loss: 0.447184]\n",
      "epoch:18 step:17192 [D loss: 0.211058, acc.: 69.53%] [G loss: 0.413417]\n",
      "epoch:18 step:17193 [D loss: 0.243228, acc.: 57.81%] [G loss: 0.421545]\n",
      "epoch:18 step:17194 [D loss: 0.210785, acc.: 67.19%] [G loss: 0.446353]\n",
      "epoch:18 step:17195 [D loss: 0.220669, acc.: 64.84%] [G loss: 0.437441]\n",
      "epoch:18 step:17196 [D loss: 0.217567, acc.: 63.28%] [G loss: 0.413594]\n",
      "epoch:18 step:17197 [D loss: 0.216596, acc.: 67.97%] [G loss: 0.412193]\n",
      "epoch:18 step:17198 [D loss: 0.212534, acc.: 67.97%] [G loss: 0.423333]\n",
      "epoch:18 step:17199 [D loss: 0.211009, acc.: 69.53%] [G loss: 0.455203]\n",
      "epoch:18 step:17200 [D loss: 0.211608, acc.: 67.19%] [G loss: 0.438274]\n",
      "##############\n",
      "[2.55866381 1.68144298 5.95760214 4.97188867 3.72346282 5.53493127\n",
      " 4.45460468 5.00793791 4.53027469 4.00203231]\n",
      "##########\n",
      "epoch:18 step:17201 [D loss: 0.213208, acc.: 63.28%] [G loss: 0.430352]\n",
      "epoch:18 step:17202 [D loss: 0.194571, acc.: 71.88%] [G loss: 0.422888]\n",
      "epoch:18 step:17203 [D loss: 0.231524, acc.: 64.06%] [G loss: 0.466958]\n",
      "epoch:18 step:17204 [D loss: 0.192115, acc.: 70.31%] [G loss: 0.462178]\n",
      "epoch:18 step:17205 [D loss: 0.211424, acc.: 69.53%] [G loss: 0.478516]\n",
      "epoch:18 step:17206 [D loss: 0.202323, acc.: 71.88%] [G loss: 0.463202]\n",
      "epoch:18 step:17207 [D loss: 0.295237, acc.: 54.69%] [G loss: 0.407748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17208 [D loss: 0.268095, acc.: 56.25%] [G loss: 0.478438]\n",
      "epoch:18 step:17209 [D loss: 0.191187, acc.: 67.97%] [G loss: 0.485475]\n",
      "epoch:18 step:17210 [D loss: 0.190071, acc.: 73.44%] [G loss: 0.487668]\n",
      "epoch:18 step:17211 [D loss: 0.244418, acc.: 57.81%] [G loss: 0.413820]\n",
      "epoch:18 step:17212 [D loss: 0.206889, acc.: 64.06%] [G loss: 0.466609]\n",
      "epoch:18 step:17213 [D loss: 0.176510, acc.: 77.34%] [G loss: 0.522441]\n",
      "epoch:18 step:17214 [D loss: 0.284412, acc.: 54.69%] [G loss: 0.451786]\n",
      "epoch:18 step:17215 [D loss: 0.280141, acc.: 49.22%] [G loss: 0.400036]\n",
      "epoch:18 step:17216 [D loss: 0.229024, acc.: 62.50%] [G loss: 0.419815]\n",
      "epoch:18 step:17217 [D loss: 0.232569, acc.: 66.41%] [G loss: 0.462410]\n",
      "epoch:18 step:17218 [D loss: 0.236369, acc.: 55.47%] [G loss: 0.439671]\n",
      "epoch:18 step:17219 [D loss: 0.205654, acc.: 67.97%] [G loss: 0.440346]\n",
      "epoch:18 step:17220 [D loss: 0.177706, acc.: 71.88%] [G loss: 0.452771]\n",
      "epoch:18 step:17221 [D loss: 0.233587, acc.: 60.16%] [G loss: 0.458925]\n",
      "epoch:18 step:17222 [D loss: 0.215364, acc.: 64.06%] [G loss: 0.405398]\n",
      "epoch:18 step:17223 [D loss: 0.217219, acc.: 59.38%] [G loss: 0.418012]\n",
      "epoch:18 step:17224 [D loss: 0.214669, acc.: 66.41%] [G loss: 0.459027]\n",
      "epoch:18 step:17225 [D loss: 0.204441, acc.: 71.09%] [G loss: 0.470401]\n",
      "epoch:18 step:17226 [D loss: 0.203788, acc.: 64.84%] [G loss: 0.476353]\n",
      "epoch:18 step:17227 [D loss: 0.205904, acc.: 64.84%] [G loss: 0.472586]\n",
      "epoch:18 step:17228 [D loss: 0.235372, acc.: 61.72%] [G loss: 0.409453]\n",
      "epoch:18 step:17229 [D loss: 0.231799, acc.: 63.28%] [G loss: 0.382957]\n",
      "epoch:18 step:17230 [D loss: 0.213014, acc.: 64.84%] [G loss: 0.398695]\n",
      "epoch:18 step:17231 [D loss: 0.211006, acc.: 64.06%] [G loss: 0.429435]\n",
      "epoch:18 step:17232 [D loss: 0.230584, acc.: 62.50%] [G loss: 0.434663]\n",
      "epoch:18 step:17233 [D loss: 0.224195, acc.: 63.28%] [G loss: 0.410695]\n",
      "epoch:18 step:17234 [D loss: 0.234094, acc.: 60.94%] [G loss: 0.437666]\n",
      "epoch:18 step:17235 [D loss: 0.217713, acc.: 64.06%] [G loss: 0.423523]\n",
      "epoch:18 step:17236 [D loss: 0.201274, acc.: 69.53%] [G loss: 0.448875]\n",
      "epoch:18 step:17237 [D loss: 0.202803, acc.: 66.41%] [G loss: 0.442044]\n",
      "epoch:18 step:17238 [D loss: 0.205134, acc.: 67.97%] [G loss: 0.450094]\n",
      "epoch:18 step:17239 [D loss: 0.253877, acc.: 63.28%] [G loss: 0.440621]\n",
      "epoch:18 step:17240 [D loss: 0.195174, acc.: 63.28%] [G loss: 0.457136]\n",
      "epoch:18 step:17241 [D loss: 0.232808, acc.: 60.94%] [G loss: 0.427609]\n",
      "epoch:18 step:17242 [D loss: 0.253510, acc.: 53.12%] [G loss: 0.454194]\n",
      "epoch:18 step:17243 [D loss: 0.245470, acc.: 57.81%] [G loss: 0.435194]\n",
      "epoch:18 step:17244 [D loss: 0.229703, acc.: 64.06%] [G loss: 0.406088]\n",
      "epoch:18 step:17245 [D loss: 0.231305, acc.: 61.72%] [G loss: 0.428313]\n",
      "epoch:18 step:17246 [D loss: 0.217495, acc.: 65.62%] [G loss: 0.422507]\n",
      "epoch:18 step:17247 [D loss: 0.190061, acc.: 72.66%] [G loss: 0.430810]\n",
      "epoch:18 step:17248 [D loss: 0.216330, acc.: 63.28%] [G loss: 0.441235]\n",
      "epoch:18 step:17249 [D loss: 0.205862, acc.: 67.97%] [G loss: 0.442786]\n",
      "epoch:18 step:17250 [D loss: 0.212434, acc.: 65.62%] [G loss: 0.408392]\n",
      "epoch:18 step:17251 [D loss: 0.190502, acc.: 67.97%] [G loss: 0.451020]\n",
      "epoch:18 step:17252 [D loss: 0.225187, acc.: 59.38%] [G loss: 0.464825]\n",
      "epoch:18 step:17253 [D loss: 0.214726, acc.: 60.94%] [G loss: 0.478631]\n",
      "epoch:18 step:17254 [D loss: 0.214540, acc.: 67.97%] [G loss: 0.459322]\n",
      "epoch:18 step:17255 [D loss: 0.247390, acc.: 59.38%] [G loss: 0.406491]\n",
      "epoch:18 step:17256 [D loss: 0.252330, acc.: 50.78%] [G loss: 0.404294]\n",
      "epoch:18 step:17257 [D loss: 0.220714, acc.: 59.38%] [G loss: 0.404384]\n",
      "epoch:18 step:17258 [D loss: 0.219416, acc.: 64.06%] [G loss: 0.459184]\n",
      "epoch:18 step:17259 [D loss: 0.225819, acc.: 60.94%] [G loss: 0.413433]\n",
      "epoch:18 step:17260 [D loss: 0.227918, acc.: 61.72%] [G loss: 0.433999]\n",
      "epoch:18 step:17261 [D loss: 0.214643, acc.: 59.38%] [G loss: 0.457990]\n",
      "epoch:18 step:17262 [D loss: 0.228449, acc.: 63.28%] [G loss: 0.448612]\n",
      "epoch:18 step:17263 [D loss: 0.235118, acc.: 60.16%] [G loss: 0.473690]\n",
      "epoch:18 step:17264 [D loss: 0.210180, acc.: 67.19%] [G loss: 0.494737]\n",
      "epoch:18 step:17265 [D loss: 0.214388, acc.: 65.62%] [G loss: 0.468293]\n",
      "epoch:18 step:17266 [D loss: 0.277605, acc.: 42.97%] [G loss: 0.405309]\n",
      "epoch:18 step:17267 [D loss: 0.243761, acc.: 53.91%] [G loss: 0.387403]\n",
      "epoch:18 step:17268 [D loss: 0.214807, acc.: 69.53%] [G loss: 0.442153]\n",
      "epoch:18 step:17269 [D loss: 0.214221, acc.: 64.84%] [G loss: 0.403203]\n",
      "epoch:18 step:17270 [D loss: 0.246896, acc.: 58.59%] [G loss: 0.413160]\n",
      "epoch:18 step:17271 [D loss: 0.224018, acc.: 69.53%] [G loss: 0.457444]\n",
      "epoch:18 step:17272 [D loss: 0.182774, acc.: 72.66%] [G loss: 0.530912]\n",
      "epoch:18 step:17273 [D loss: 0.239547, acc.: 58.59%] [G loss: 0.495781]\n",
      "epoch:18 step:17274 [D loss: 0.255228, acc.: 56.25%] [G loss: 0.436132]\n",
      "epoch:18 step:17275 [D loss: 0.228566, acc.: 57.81%] [G loss: 0.496002]\n",
      "epoch:18 step:17276 [D loss: 0.258174, acc.: 56.25%] [G loss: 0.394211]\n",
      "epoch:18 step:17277 [D loss: 0.243384, acc.: 60.16%] [G loss: 0.405051]\n",
      "epoch:18 step:17278 [D loss: 0.245761, acc.: 53.91%] [G loss: 0.386076]\n",
      "epoch:18 step:17279 [D loss: 0.210397, acc.: 70.31%] [G loss: 0.445132]\n",
      "epoch:18 step:17280 [D loss: 0.196363, acc.: 69.53%] [G loss: 0.445335]\n",
      "epoch:18 step:17281 [D loss: 0.214153, acc.: 61.72%] [G loss: 0.448782]\n",
      "epoch:18 step:17282 [D loss: 0.205163, acc.: 67.97%] [G loss: 0.465378]\n",
      "epoch:18 step:17283 [D loss: 0.268821, acc.: 52.34%] [G loss: 0.415541]\n",
      "epoch:18 step:17284 [D loss: 0.265129, acc.: 53.91%] [G loss: 0.399956]\n",
      "epoch:18 step:17285 [D loss: 0.220855, acc.: 65.62%] [G loss: 0.442403]\n",
      "epoch:18 step:17286 [D loss: 0.233702, acc.: 55.47%] [G loss: 0.443708]\n",
      "epoch:18 step:17287 [D loss: 0.241859, acc.: 61.72%] [G loss: 0.414967]\n",
      "epoch:18 step:17288 [D loss: 0.241746, acc.: 53.12%] [G loss: 0.400734]\n",
      "epoch:18 step:17289 [D loss: 0.232688, acc.: 55.47%] [G loss: 0.436079]\n",
      "epoch:18 step:17290 [D loss: 0.223151, acc.: 57.03%] [G loss: 0.423127]\n",
      "epoch:18 step:17291 [D loss: 0.238662, acc.: 60.16%] [G loss: 0.375392]\n",
      "epoch:18 step:17292 [D loss: 0.221541, acc.: 64.06%] [G loss: 0.412941]\n",
      "epoch:18 step:17293 [D loss: 0.223993, acc.: 67.97%] [G loss: 0.435770]\n",
      "epoch:18 step:17294 [D loss: 0.200822, acc.: 71.09%] [G loss: 0.456645]\n",
      "epoch:18 step:17295 [D loss: 0.204161, acc.: 69.53%] [G loss: 0.416583]\n",
      "epoch:18 step:17296 [D loss: 0.198481, acc.: 71.09%] [G loss: 0.480418]\n",
      "epoch:18 step:17297 [D loss: 0.246799, acc.: 57.81%] [G loss: 0.414171]\n",
      "epoch:18 step:17298 [D loss: 0.248797, acc.: 55.47%] [G loss: 0.420316]\n",
      "epoch:18 step:17299 [D loss: 0.203242, acc.: 66.41%] [G loss: 0.432208]\n",
      "epoch:18 step:17300 [D loss: 0.232636, acc.: 63.28%] [G loss: 0.392763]\n",
      "epoch:18 step:17301 [D loss: 0.201626, acc.: 71.88%] [G loss: 0.469303]\n",
      "epoch:18 step:17302 [D loss: 0.208830, acc.: 71.09%] [G loss: 0.454902]\n",
      "epoch:18 step:17303 [D loss: 0.274765, acc.: 46.09%] [G loss: 0.457056]\n",
      "epoch:18 step:17304 [D loss: 0.238735, acc.: 56.25%] [G loss: 0.448547]\n",
      "epoch:18 step:17305 [D loss: 0.207574, acc.: 69.53%] [G loss: 0.452591]\n",
      "epoch:18 step:17306 [D loss: 0.218670, acc.: 62.50%] [G loss: 0.445944]\n",
      "epoch:18 step:17307 [D loss: 0.238377, acc.: 60.94%] [G loss: 0.435641]\n",
      "epoch:18 step:17308 [D loss: 0.231937, acc.: 61.72%] [G loss: 0.439171]\n",
      "epoch:18 step:17309 [D loss: 0.242738, acc.: 58.59%] [G loss: 0.381066]\n",
      "epoch:18 step:17310 [D loss: 0.224042, acc.: 64.06%] [G loss: 0.434712]\n",
      "epoch:18 step:17311 [D loss: 0.224915, acc.: 62.50%] [G loss: 0.438453]\n",
      "epoch:18 step:17312 [D loss: 0.198535, acc.: 65.62%] [G loss: 0.497222]\n",
      "epoch:18 step:17313 [D loss: 0.197323, acc.: 71.88%] [G loss: 0.468065]\n",
      "epoch:18 step:17314 [D loss: 0.267380, acc.: 53.12%] [G loss: 0.422178]\n",
      "epoch:18 step:17315 [D loss: 0.223736, acc.: 62.50%] [G loss: 0.433274]\n",
      "epoch:18 step:17316 [D loss: 0.202380, acc.: 68.75%] [G loss: 0.446780]\n",
      "epoch:18 step:17317 [D loss: 0.192361, acc.: 71.88%] [G loss: 0.440013]\n",
      "epoch:18 step:17318 [D loss: 0.207543, acc.: 64.06%] [G loss: 0.442860]\n",
      "epoch:18 step:17319 [D loss: 0.199876, acc.: 69.53%] [G loss: 0.437657]\n",
      "epoch:18 step:17320 [D loss: 0.215174, acc.: 64.06%] [G loss: 0.438629]\n",
      "epoch:18 step:17321 [D loss: 0.226012, acc.: 60.94%] [G loss: 0.424374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17322 [D loss: 0.200897, acc.: 69.53%] [G loss: 0.452324]\n",
      "epoch:18 step:17323 [D loss: 0.233801, acc.: 63.28%] [G loss: 0.462110]\n",
      "epoch:18 step:17324 [D loss: 0.267735, acc.: 56.25%] [G loss: 0.459130]\n",
      "epoch:18 step:17325 [D loss: 0.257131, acc.: 55.47%] [G loss: 0.420531]\n",
      "epoch:18 step:17326 [D loss: 0.201814, acc.: 64.84%] [G loss: 0.436820]\n",
      "epoch:18 step:17327 [D loss: 0.251388, acc.: 59.38%] [G loss: 0.412763]\n",
      "epoch:18 step:17328 [D loss: 0.246327, acc.: 58.59%] [G loss: 0.390511]\n",
      "epoch:18 step:17329 [D loss: 0.255699, acc.: 57.03%] [G loss: 0.377239]\n",
      "epoch:18 step:17330 [D loss: 0.224617, acc.: 61.72%] [G loss: 0.418649]\n",
      "epoch:18 step:17331 [D loss: 0.234959, acc.: 58.59%] [G loss: 0.380210]\n",
      "epoch:18 step:17332 [D loss: 0.235668, acc.: 60.16%] [G loss: 0.396041]\n",
      "epoch:18 step:17333 [D loss: 0.247426, acc.: 54.69%] [G loss: 0.400451]\n",
      "epoch:18 step:17334 [D loss: 0.222617, acc.: 66.41%] [G loss: 0.443096]\n",
      "epoch:18 step:17335 [D loss: 0.210708, acc.: 64.84%] [G loss: 0.444611]\n",
      "epoch:18 step:17336 [D loss: 0.205909, acc.: 66.41%] [G loss: 0.437570]\n",
      "epoch:18 step:17337 [D loss: 0.179171, acc.: 75.00%] [G loss: 0.478720]\n",
      "epoch:18 step:17338 [D loss: 0.229649, acc.: 65.62%] [G loss: 0.478811]\n",
      "epoch:18 step:17339 [D loss: 0.249673, acc.: 57.03%] [G loss: 0.446952]\n",
      "epoch:18 step:17340 [D loss: 0.222762, acc.: 61.72%] [G loss: 0.448260]\n",
      "epoch:18 step:17341 [D loss: 0.215470, acc.: 64.06%] [G loss: 0.441127]\n",
      "epoch:18 step:17342 [D loss: 0.243476, acc.: 64.06%] [G loss: 0.441556]\n",
      "epoch:18 step:17343 [D loss: 0.234644, acc.: 60.94%] [G loss: 0.425754]\n",
      "epoch:18 step:17344 [D loss: 0.222413, acc.: 54.69%] [G loss: 0.420656]\n",
      "epoch:18 step:17345 [D loss: 0.246548, acc.: 58.59%] [G loss: 0.386878]\n",
      "epoch:18 step:17346 [D loss: 0.219446, acc.: 68.75%] [G loss: 0.435502]\n",
      "epoch:18 step:17347 [D loss: 0.195672, acc.: 73.44%] [G loss: 0.451150]\n",
      "epoch:18 step:17348 [D loss: 0.251555, acc.: 59.38%] [G loss: 0.419423]\n",
      "epoch:18 step:17349 [D loss: 0.238678, acc.: 57.81%] [G loss: 0.419532]\n",
      "epoch:18 step:17350 [D loss: 0.222752, acc.: 64.06%] [G loss: 0.420258]\n",
      "epoch:18 step:17351 [D loss: 0.197779, acc.: 69.53%] [G loss: 0.470293]\n",
      "epoch:18 step:17352 [D loss: 0.236921, acc.: 60.94%] [G loss: 0.426074]\n",
      "epoch:18 step:17353 [D loss: 0.249255, acc.: 57.81%] [G loss: 0.443319]\n",
      "epoch:18 step:17354 [D loss: 0.209507, acc.: 62.50%] [G loss: 0.457718]\n",
      "epoch:18 step:17355 [D loss: 0.231617, acc.: 57.81%] [G loss: 0.471874]\n",
      "epoch:18 step:17356 [D loss: 0.231863, acc.: 63.28%] [G loss: 0.389625]\n",
      "epoch:18 step:17357 [D loss: 0.204940, acc.: 69.53%] [G loss: 0.436152]\n",
      "epoch:18 step:17358 [D loss: 0.220013, acc.: 66.41%] [G loss: 0.419437]\n",
      "epoch:18 step:17359 [D loss: 0.219554, acc.: 60.94%] [G loss: 0.374247]\n",
      "epoch:18 step:17360 [D loss: 0.211679, acc.: 71.09%] [G loss: 0.438342]\n",
      "epoch:18 step:17361 [D loss: 0.225668, acc.: 63.28%] [G loss: 0.447521]\n",
      "epoch:18 step:17362 [D loss: 0.225494, acc.: 66.41%] [G loss: 0.474126]\n",
      "epoch:18 step:17363 [D loss: 0.211768, acc.: 64.84%] [G loss: 0.462991]\n",
      "epoch:18 step:17364 [D loss: 0.225377, acc.: 64.06%] [G loss: 0.459742]\n",
      "epoch:18 step:17365 [D loss: 0.179002, acc.: 71.09%] [G loss: 0.440373]\n",
      "epoch:18 step:17366 [D loss: 0.244609, acc.: 59.38%] [G loss: 0.435259]\n",
      "epoch:18 step:17367 [D loss: 0.282425, acc.: 50.00%] [G loss: 0.401589]\n",
      "epoch:18 step:17368 [D loss: 0.242584, acc.: 57.81%] [G loss: 0.396796]\n",
      "epoch:18 step:17369 [D loss: 0.227668, acc.: 60.94%] [G loss: 0.399663]\n",
      "epoch:18 step:17370 [D loss: 0.201228, acc.: 68.75%] [G loss: 0.434926]\n",
      "epoch:18 step:17371 [D loss: 0.196434, acc.: 72.66%] [G loss: 0.428229]\n",
      "epoch:18 step:17372 [D loss: 0.233400, acc.: 57.81%] [G loss: 0.432656]\n",
      "epoch:18 step:17373 [D loss: 0.199086, acc.: 66.41%] [G loss: 0.484580]\n",
      "epoch:18 step:17374 [D loss: 0.205495, acc.: 64.84%] [G loss: 0.455836]\n",
      "epoch:18 step:17375 [D loss: 0.253030, acc.: 56.25%] [G loss: 0.442088]\n",
      "epoch:18 step:17376 [D loss: 0.232233, acc.: 58.59%] [G loss: 0.446763]\n",
      "epoch:18 step:17377 [D loss: 0.239060, acc.: 59.38%] [G loss: 0.416421]\n",
      "epoch:18 step:17378 [D loss: 0.212797, acc.: 61.72%] [G loss: 0.450813]\n",
      "epoch:18 step:17379 [D loss: 0.219951, acc.: 64.06%] [G loss: 0.437261]\n",
      "epoch:18 step:17380 [D loss: 0.203266, acc.: 67.19%] [G loss: 0.457507]\n",
      "epoch:18 step:17381 [D loss: 0.198297, acc.: 76.56%] [G loss: 0.472490]\n",
      "epoch:18 step:17382 [D loss: 0.193654, acc.: 69.53%] [G loss: 0.457996]\n",
      "epoch:18 step:17383 [D loss: 0.233954, acc.: 60.94%] [G loss: 0.428923]\n",
      "epoch:18 step:17384 [D loss: 0.261305, acc.: 53.91%] [G loss: 0.399327]\n",
      "epoch:18 step:17385 [D loss: 0.211524, acc.: 66.41%] [G loss: 0.432355]\n",
      "epoch:18 step:17386 [D loss: 0.198891, acc.: 67.19%] [G loss: 0.470606]\n",
      "epoch:18 step:17387 [D loss: 0.213201, acc.: 66.41%] [G loss: 0.464993]\n",
      "epoch:18 step:17388 [D loss: 0.192301, acc.: 70.31%] [G loss: 0.482449]\n",
      "epoch:18 step:17389 [D loss: 0.217942, acc.: 63.28%] [G loss: 0.414713]\n",
      "epoch:18 step:17390 [D loss: 0.230694, acc.: 59.38%] [G loss: 0.413628]\n",
      "epoch:18 step:17391 [D loss: 0.227218, acc.: 67.19%] [G loss: 0.456530]\n",
      "epoch:18 step:17392 [D loss: 0.209957, acc.: 66.41%] [G loss: 0.475730]\n",
      "epoch:18 step:17393 [D loss: 0.223012, acc.: 61.72%] [G loss: 0.461959]\n",
      "epoch:18 step:17394 [D loss: 0.270638, acc.: 48.44%] [G loss: 0.401472]\n",
      "epoch:18 step:17395 [D loss: 0.239712, acc.: 57.81%] [G loss: 0.404851]\n",
      "epoch:18 step:17396 [D loss: 0.215894, acc.: 65.62%] [G loss: 0.503536]\n",
      "epoch:18 step:17397 [D loss: 0.266456, acc.: 52.34%] [G loss: 0.414855]\n",
      "epoch:18 step:17398 [D loss: 0.227603, acc.: 61.72%] [G loss: 0.417443]\n",
      "epoch:18 step:17399 [D loss: 0.222914, acc.: 66.41%] [G loss: 0.438219]\n",
      "epoch:18 step:17400 [D loss: 0.218341, acc.: 64.06%] [G loss: 0.416969]\n",
      "##############\n",
      "[2.39409569 1.68408327 5.74109502 4.68198539 3.66642816 5.4830824\n",
      " 4.56757087 4.37187122 4.39458457 4.10481584]\n",
      "##########\n",
      "epoch:18 step:17401 [D loss: 0.240851, acc.: 63.28%] [G loss: 0.451191]\n",
      "epoch:18 step:17402 [D loss: 0.228302, acc.: 60.94%] [G loss: 0.453430]\n",
      "epoch:18 step:17403 [D loss: 0.239418, acc.: 60.94%] [G loss: 0.432519]\n",
      "epoch:18 step:17404 [D loss: 0.229911, acc.: 63.28%] [G loss: 0.428456]\n",
      "epoch:18 step:17405 [D loss: 0.249131, acc.: 52.34%] [G loss: 0.410230]\n",
      "epoch:18 step:17406 [D loss: 0.217719, acc.: 60.94%] [G loss: 0.436313]\n",
      "epoch:18 step:17407 [D loss: 0.232053, acc.: 65.62%] [G loss: 0.432382]\n",
      "epoch:18 step:17408 [D loss: 0.269343, acc.: 48.44%] [G loss: 0.400437]\n",
      "epoch:18 step:17409 [D loss: 0.227701, acc.: 63.28%] [G loss: 0.460625]\n",
      "epoch:18 step:17410 [D loss: 0.236096, acc.: 61.72%] [G loss: 0.420349]\n",
      "epoch:18 step:17411 [D loss: 0.235954, acc.: 56.25%] [G loss: 0.406466]\n",
      "epoch:18 step:17412 [D loss: 0.214402, acc.: 66.41%] [G loss: 0.444419]\n",
      "epoch:18 step:17413 [D loss: 0.233201, acc.: 63.28%] [G loss: 0.427401]\n",
      "epoch:18 step:17414 [D loss: 0.203038, acc.: 70.31%] [G loss: 0.436802]\n",
      "epoch:18 step:17415 [D loss: 0.199647, acc.: 65.62%] [G loss: 0.494864]\n",
      "epoch:18 step:17416 [D loss: 0.221517, acc.: 66.41%] [G loss: 0.515456]\n",
      "epoch:18 step:17417 [D loss: 0.195252, acc.: 71.88%] [G loss: 0.453307]\n",
      "epoch:18 step:17418 [D loss: 0.183147, acc.: 75.00%] [G loss: 0.498279]\n",
      "epoch:18 step:17419 [D loss: 0.234534, acc.: 59.38%] [G loss: 0.425864]\n",
      "epoch:18 step:17420 [D loss: 0.206617, acc.: 67.97%] [G loss: 0.455125]\n",
      "epoch:18 step:17421 [D loss: 0.206216, acc.: 68.75%] [G loss: 0.422417]\n",
      "epoch:18 step:17422 [D loss: 0.210028, acc.: 66.41%] [G loss: 0.444138]\n",
      "epoch:18 step:17423 [D loss: 0.216209, acc.: 63.28%] [G loss: 0.432410]\n",
      "epoch:18 step:17424 [D loss: 0.209371, acc.: 66.41%] [G loss: 0.461431]\n",
      "epoch:18 step:17425 [D loss: 0.239688, acc.: 56.25%] [G loss: 0.452121]\n",
      "epoch:18 step:17426 [D loss: 0.228403, acc.: 60.16%] [G loss: 0.415050]\n",
      "epoch:18 step:17427 [D loss: 0.223687, acc.: 57.81%] [G loss: 0.434803]\n",
      "epoch:18 step:17428 [D loss: 0.198090, acc.: 67.97%] [G loss: 0.420661]\n",
      "epoch:18 step:17429 [D loss: 0.221556, acc.: 67.19%] [G loss: 0.461165]\n",
      "epoch:18 step:17430 [D loss: 0.179799, acc.: 80.47%] [G loss: 0.541012]\n",
      "epoch:18 step:17431 [D loss: 0.227585, acc.: 67.97%] [G loss: 0.494636]\n",
      "epoch:18 step:17432 [D loss: 0.256792, acc.: 51.56%] [G loss: 0.461964]\n",
      "epoch:18 step:17433 [D loss: 0.215794, acc.: 63.28%] [G loss: 0.413556]\n",
      "epoch:18 step:17434 [D loss: 0.211604, acc.: 66.41%] [G loss: 0.449692]\n",
      "epoch:18 step:17435 [D loss: 0.238824, acc.: 59.38%] [G loss: 0.419490]\n",
      "epoch:18 step:17436 [D loss: 0.228136, acc.: 64.84%] [G loss: 0.454558]\n",
      "epoch:18 step:17437 [D loss: 0.200391, acc.: 72.66%] [G loss: 0.438686]\n",
      "epoch:18 step:17438 [D loss: 0.195340, acc.: 69.53%] [G loss: 0.420537]\n",
      "epoch:18 step:17439 [D loss: 0.221353, acc.: 61.72%] [G loss: 0.432150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17440 [D loss: 0.210006, acc.: 64.06%] [G loss: 0.426698]\n",
      "epoch:18 step:17441 [D loss: 0.198182, acc.: 67.97%] [G loss: 0.490527]\n",
      "epoch:18 step:17442 [D loss: 0.236916, acc.: 56.25%] [G loss: 0.468319]\n",
      "epoch:18 step:17443 [D loss: 0.236408, acc.: 61.72%] [G loss: 0.406373]\n",
      "epoch:18 step:17444 [D loss: 0.225798, acc.: 64.84%] [G loss: 0.433114]\n",
      "epoch:18 step:17445 [D loss: 0.249986, acc.: 53.12%] [G loss: 0.423419]\n",
      "epoch:18 step:17446 [D loss: 0.230785, acc.: 60.16%] [G loss: 0.442195]\n",
      "epoch:18 step:17447 [D loss: 0.223845, acc.: 60.16%] [G loss: 0.447083]\n",
      "epoch:18 step:17448 [D loss: 0.210218, acc.: 68.75%] [G loss: 0.460977]\n",
      "epoch:18 step:17449 [D loss: 0.221501, acc.: 63.28%] [G loss: 0.454082]\n",
      "epoch:18 step:17450 [D loss: 0.246081, acc.: 54.69%] [G loss: 0.445233]\n",
      "epoch:18 step:17451 [D loss: 0.234647, acc.: 66.41%] [G loss: 0.476542]\n",
      "epoch:18 step:17452 [D loss: 0.231728, acc.: 60.94%] [G loss: 0.415835]\n",
      "epoch:18 step:17453 [D loss: 0.233185, acc.: 57.03%] [G loss: 0.405907]\n",
      "epoch:18 step:17454 [D loss: 0.228993, acc.: 66.41%] [G loss: 0.409661]\n",
      "epoch:18 step:17455 [D loss: 0.193093, acc.: 67.97%] [G loss: 0.464240]\n",
      "epoch:18 step:17456 [D loss: 0.243390, acc.: 61.72%] [G loss: 0.445650]\n",
      "epoch:18 step:17457 [D loss: 0.253852, acc.: 66.41%] [G loss: 0.435965]\n",
      "epoch:18 step:17458 [D loss: 0.201716, acc.: 70.31%] [G loss: 0.441805]\n",
      "epoch:18 step:17459 [D loss: 0.209416, acc.: 64.06%] [G loss: 0.470276]\n",
      "epoch:18 step:17460 [D loss: 0.247560, acc.: 51.56%] [G loss: 0.449731]\n",
      "epoch:18 step:17461 [D loss: 0.220428, acc.: 65.62%] [G loss: 0.460359]\n",
      "epoch:18 step:17462 [D loss: 0.254988, acc.: 57.81%] [G loss: 0.426404]\n",
      "epoch:18 step:17463 [D loss: 0.238972, acc.: 60.94%] [G loss: 0.427125]\n",
      "epoch:18 step:17464 [D loss: 0.220619, acc.: 61.72%] [G loss: 0.444020]\n",
      "epoch:18 step:17465 [D loss: 0.233700, acc.: 60.16%] [G loss: 0.417884]\n",
      "epoch:18 step:17466 [D loss: 0.235140, acc.: 61.72%] [G loss: 0.403263]\n",
      "epoch:18 step:17467 [D loss: 0.238329, acc.: 60.16%] [G loss: 0.382707]\n",
      "epoch:18 step:17468 [D loss: 0.234035, acc.: 59.38%] [G loss: 0.407404]\n",
      "epoch:18 step:17469 [D loss: 0.215109, acc.: 64.06%] [G loss: 0.389378]\n",
      "epoch:18 step:17470 [D loss: 0.228829, acc.: 61.72%] [G loss: 0.425542]\n",
      "epoch:18 step:17471 [D loss: 0.210460, acc.: 64.06%] [G loss: 0.439679]\n",
      "epoch:18 step:17472 [D loss: 0.237805, acc.: 60.94%] [G loss: 0.438162]\n",
      "epoch:18 step:17473 [D loss: 0.214002, acc.: 63.28%] [G loss: 0.415971]\n",
      "epoch:18 step:17474 [D loss: 0.216151, acc.: 64.84%] [G loss: 0.422411]\n",
      "epoch:18 step:17475 [D loss: 0.227089, acc.: 61.72%] [G loss: 0.393738]\n",
      "epoch:18 step:17476 [D loss: 0.212750, acc.: 64.84%] [G loss: 0.408428]\n",
      "epoch:18 step:17477 [D loss: 0.222883, acc.: 64.84%] [G loss: 0.433269]\n",
      "epoch:18 step:17478 [D loss: 0.224343, acc.: 66.41%] [G loss: 0.416763]\n",
      "epoch:18 step:17479 [D loss: 0.200636, acc.: 71.09%] [G loss: 0.451405]\n",
      "epoch:18 step:17480 [D loss: 0.259056, acc.: 55.47%] [G loss: 0.383528]\n",
      "epoch:18 step:17481 [D loss: 0.259335, acc.: 59.38%] [G loss: 0.415064]\n",
      "epoch:18 step:17482 [D loss: 0.214813, acc.: 66.41%] [G loss: 0.449071]\n",
      "epoch:18 step:17483 [D loss: 0.240657, acc.: 57.81%] [G loss: 0.418981]\n",
      "epoch:18 step:17484 [D loss: 0.227031, acc.: 62.50%] [G loss: 0.421651]\n",
      "epoch:18 step:17485 [D loss: 0.250724, acc.: 53.91%] [G loss: 0.443587]\n",
      "epoch:18 step:17486 [D loss: 0.212278, acc.: 70.31%] [G loss: 0.454934]\n",
      "epoch:18 step:17487 [D loss: 0.265710, acc.: 47.66%] [G loss: 0.408575]\n",
      "epoch:18 step:17488 [D loss: 0.203680, acc.: 69.53%] [G loss: 0.478623]\n",
      "epoch:18 step:17489 [D loss: 0.216808, acc.: 64.84%] [G loss: 0.448062]\n",
      "epoch:18 step:17490 [D loss: 0.192916, acc.: 71.09%] [G loss: 0.466869]\n",
      "epoch:18 step:17491 [D loss: 0.215786, acc.: 66.41%] [G loss: 0.441812]\n",
      "epoch:18 step:17492 [D loss: 0.242687, acc.: 59.38%] [G loss: 0.416126]\n",
      "epoch:18 step:17493 [D loss: 0.198882, acc.: 71.88%] [G loss: 0.439175]\n",
      "epoch:18 step:17494 [D loss: 0.268894, acc.: 53.91%] [G loss: 0.435545]\n",
      "epoch:18 step:17495 [D loss: 0.211486, acc.: 67.97%] [G loss: 0.439010]\n",
      "epoch:18 step:17496 [D loss: 0.210315, acc.: 69.53%] [G loss: 0.448577]\n",
      "epoch:18 step:17497 [D loss: 0.183141, acc.: 68.75%] [G loss: 0.428504]\n",
      "epoch:18 step:17498 [D loss: 0.238346, acc.: 56.25%] [G loss: 0.423303]\n",
      "epoch:18 step:17499 [D loss: 0.219892, acc.: 65.62%] [G loss: 0.431988]\n",
      "epoch:18 step:17500 [D loss: 0.191627, acc.: 70.31%] [G loss: 0.461697]\n",
      "epoch:18 step:17501 [D loss: 0.224600, acc.: 63.28%] [G loss: 0.455434]\n",
      "epoch:18 step:17502 [D loss: 0.224553, acc.: 60.94%] [G loss: 0.442621]\n",
      "epoch:18 step:17503 [D loss: 0.230528, acc.: 59.38%] [G loss: 0.417540]\n",
      "epoch:18 step:17504 [D loss: 0.206305, acc.: 69.53%] [G loss: 0.430853]\n",
      "epoch:18 step:17505 [D loss: 0.218034, acc.: 63.28%] [G loss: 0.421860]\n",
      "epoch:18 step:17506 [D loss: 0.226374, acc.: 68.75%] [G loss: 0.398604]\n",
      "epoch:18 step:17507 [D loss: 0.211769, acc.: 63.28%] [G loss: 0.445022]\n",
      "epoch:18 step:17508 [D loss: 0.191028, acc.: 70.31%] [G loss: 0.482092]\n",
      "epoch:18 step:17509 [D loss: 0.216794, acc.: 64.06%] [G loss: 0.472173]\n",
      "epoch:18 step:17510 [D loss: 0.221542, acc.: 67.19%] [G loss: 0.415464]\n",
      "epoch:18 step:17511 [D loss: 0.234013, acc.: 60.94%] [G loss: 0.428942]\n",
      "epoch:18 step:17512 [D loss: 0.208870, acc.: 71.09%] [G loss: 0.421701]\n",
      "epoch:18 step:17513 [D loss: 0.205544, acc.: 67.19%] [G loss: 0.429183]\n",
      "epoch:18 step:17514 [D loss: 0.177767, acc.: 71.88%] [G loss: 0.484381]\n",
      "epoch:18 step:17515 [D loss: 0.215302, acc.: 64.84%] [G loss: 0.494414]\n",
      "epoch:18 step:17516 [D loss: 0.200928, acc.: 69.53%] [G loss: 0.449916]\n",
      "epoch:18 step:17517 [D loss: 0.233746, acc.: 62.50%] [G loss: 0.459198]\n",
      "epoch:18 step:17518 [D loss: 0.212585, acc.: 66.41%] [G loss: 0.454502]\n",
      "epoch:18 step:17519 [D loss: 0.228848, acc.: 58.59%] [G loss: 0.452846]\n",
      "epoch:18 step:17520 [D loss: 0.213569, acc.: 64.06%] [G loss: 0.464548]\n",
      "epoch:18 step:17521 [D loss: 0.264857, acc.: 59.38%] [G loss: 0.434634]\n",
      "epoch:18 step:17522 [D loss: 0.247304, acc.: 53.91%] [G loss: 0.408726]\n",
      "epoch:18 step:17523 [D loss: 0.252575, acc.: 59.38%] [G loss: 0.405413]\n",
      "epoch:18 step:17524 [D loss: 0.232218, acc.: 63.28%] [G loss: 0.433945]\n",
      "epoch:18 step:17525 [D loss: 0.215303, acc.: 62.50%] [G loss: 0.447907]\n",
      "epoch:18 step:17526 [D loss: 0.223379, acc.: 61.72%] [G loss: 0.415522]\n",
      "epoch:18 step:17527 [D loss: 0.205814, acc.: 67.97%] [G loss: 0.420550]\n",
      "epoch:18 step:17528 [D loss: 0.217517, acc.: 67.97%] [G loss: 0.458311]\n",
      "epoch:18 step:17529 [D loss: 0.269431, acc.: 47.66%] [G loss: 0.379961]\n",
      "epoch:18 step:17530 [D loss: 0.219656, acc.: 63.28%] [G loss: 0.427688]\n",
      "epoch:18 step:17531 [D loss: 0.216443, acc.: 60.94%] [G loss: 0.460865]\n",
      "epoch:18 step:17532 [D loss: 0.212649, acc.: 65.62%] [G loss: 0.415155]\n",
      "epoch:18 step:17533 [D loss: 0.251291, acc.: 65.62%] [G loss: 0.452739]\n",
      "epoch:18 step:17534 [D loss: 0.235466, acc.: 62.50%] [G loss: 0.435964]\n",
      "epoch:18 step:17535 [D loss: 0.217715, acc.: 64.06%] [G loss: 0.437393]\n",
      "epoch:18 step:17536 [D loss: 0.255363, acc.: 56.25%] [G loss: 0.419503]\n",
      "epoch:18 step:17537 [D loss: 0.236051, acc.: 57.03%] [G loss: 0.416682]\n",
      "epoch:18 step:17538 [D loss: 0.231381, acc.: 64.84%] [G loss: 0.424357]\n",
      "epoch:18 step:17539 [D loss: 0.214721, acc.: 62.50%] [G loss: 0.445452]\n",
      "epoch:18 step:17540 [D loss: 0.207920, acc.: 71.88%] [G loss: 0.457633]\n",
      "epoch:18 step:17541 [D loss: 0.250080, acc.: 60.94%] [G loss: 0.417703]\n",
      "epoch:18 step:17542 [D loss: 0.232643, acc.: 54.69%] [G loss: 0.393590]\n",
      "epoch:18 step:17543 [D loss: 0.206272, acc.: 64.84%] [G loss: 0.408125]\n",
      "epoch:18 step:17544 [D loss: 0.228453, acc.: 62.50%] [G loss: 0.401359]\n",
      "epoch:18 step:17545 [D loss: 0.226357, acc.: 64.06%] [G loss: 0.442977]\n",
      "epoch:18 step:17546 [D loss: 0.207428, acc.: 64.84%] [G loss: 0.425963]\n",
      "epoch:18 step:17547 [D loss: 0.204012, acc.: 68.75%] [G loss: 0.428206]\n",
      "epoch:18 step:17548 [D loss: 0.222173, acc.: 63.28%] [G loss: 0.425669]\n",
      "epoch:18 step:17549 [D loss: 0.226357, acc.: 60.16%] [G loss: 0.464453]\n",
      "epoch:18 step:17550 [D loss: 0.230190, acc.: 58.59%] [G loss: 0.445698]\n",
      "epoch:18 step:17551 [D loss: 0.225279, acc.: 64.84%] [G loss: 0.410366]\n",
      "epoch:18 step:17552 [D loss: 0.239665, acc.: 60.16%] [G loss: 0.411958]\n",
      "epoch:18 step:17553 [D loss: 0.250522, acc.: 55.47%] [G loss: 0.369095]\n",
      "epoch:18 step:17554 [D loss: 0.213174, acc.: 68.75%] [G loss: 0.421215]\n",
      "epoch:18 step:17555 [D loss: 0.219839, acc.: 65.62%] [G loss: 0.439377]\n",
      "epoch:18 step:17556 [D loss: 0.208394, acc.: 68.75%] [G loss: 0.486439]\n",
      "epoch:18 step:17557 [D loss: 0.209113, acc.: 65.62%] [G loss: 0.443562]\n",
      "epoch:18 step:17558 [D loss: 0.205261, acc.: 65.62%] [G loss: 0.468414]\n",
      "epoch:18 step:17559 [D loss: 0.208040, acc.: 71.09%] [G loss: 0.447190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17560 [D loss: 0.180762, acc.: 72.66%] [G loss: 0.512358]\n",
      "epoch:18 step:17561 [D loss: 0.202433, acc.: 66.41%] [G loss: 0.454211]\n",
      "epoch:18 step:17562 [D loss: 0.246185, acc.: 56.25%] [G loss: 0.419635]\n",
      "epoch:18 step:17563 [D loss: 0.210840, acc.: 65.62%] [G loss: 0.440534]\n",
      "epoch:18 step:17564 [D loss: 0.218234, acc.: 64.06%] [G loss: 0.466394]\n",
      "epoch:18 step:17565 [D loss: 0.195893, acc.: 71.88%] [G loss: 0.460725]\n",
      "epoch:18 step:17566 [D loss: 0.199831, acc.: 67.19%] [G loss: 0.479487]\n",
      "epoch:18 step:17567 [D loss: 0.211891, acc.: 69.53%] [G loss: 0.450952]\n",
      "epoch:18 step:17568 [D loss: 0.261751, acc.: 55.47%] [G loss: 0.432262]\n",
      "epoch:18 step:17569 [D loss: 0.255501, acc.: 52.34%] [G loss: 0.379609]\n",
      "epoch:18 step:17570 [D loss: 0.268562, acc.: 50.00%] [G loss: 0.378757]\n",
      "epoch:18 step:17571 [D loss: 0.223174, acc.: 64.06%] [G loss: 0.414193]\n",
      "epoch:18 step:17572 [D loss: 0.199123, acc.: 73.44%] [G loss: 0.464352]\n",
      "epoch:18 step:17573 [D loss: 0.242430, acc.: 54.69%] [G loss: 0.416662]\n",
      "epoch:18 step:17574 [D loss: 0.195772, acc.: 68.75%] [G loss: 0.486453]\n",
      "epoch:18 step:17575 [D loss: 0.176882, acc.: 77.34%] [G loss: 0.495736]\n",
      "epoch:18 step:17576 [D loss: 0.235678, acc.: 64.84%] [G loss: 0.453997]\n",
      "epoch:18 step:17577 [D loss: 0.231785, acc.: 63.28%] [G loss: 0.395529]\n",
      "epoch:18 step:17578 [D loss: 0.190733, acc.: 71.88%] [G loss: 0.472661]\n",
      "epoch:18 step:17579 [D loss: 0.245308, acc.: 57.81%] [G loss: 0.418441]\n",
      "epoch:18 step:17580 [D loss: 0.210413, acc.: 64.06%] [G loss: 0.471669]\n",
      "epoch:18 step:17581 [D loss: 0.242919, acc.: 60.94%] [G loss: 0.448045]\n",
      "epoch:18 step:17582 [D loss: 0.235164, acc.: 64.84%] [G loss: 0.386435]\n",
      "epoch:18 step:17583 [D loss: 0.242343, acc.: 61.72%] [G loss: 0.437983]\n",
      "epoch:18 step:17584 [D loss: 0.215420, acc.: 68.75%] [G loss: 0.452175]\n",
      "epoch:18 step:17585 [D loss: 0.208726, acc.: 68.75%] [G loss: 0.448634]\n",
      "epoch:18 step:17586 [D loss: 0.215924, acc.: 64.84%] [G loss: 0.476817]\n",
      "epoch:18 step:17587 [D loss: 0.233105, acc.: 64.06%] [G loss: 0.420593]\n",
      "epoch:18 step:17588 [D loss: 0.251531, acc.: 50.78%] [G loss: 0.430753]\n",
      "epoch:18 step:17589 [D loss: 0.231006, acc.: 57.81%] [G loss: 0.432925]\n",
      "epoch:18 step:17590 [D loss: 0.220309, acc.: 62.50%] [G loss: 0.446533]\n",
      "epoch:18 step:17591 [D loss: 0.215241, acc.: 70.31%] [G loss: 0.444049]\n",
      "epoch:18 step:17592 [D loss: 0.208221, acc.: 61.72%] [G loss: 0.431311]\n",
      "epoch:18 step:17593 [D loss: 0.242007, acc.: 59.38%] [G loss: 0.400015]\n",
      "epoch:18 step:17594 [D loss: 0.228215, acc.: 63.28%] [G loss: 0.394571]\n",
      "epoch:18 step:17595 [D loss: 0.217322, acc.: 67.19%] [G loss: 0.408571]\n",
      "epoch:18 step:17596 [D loss: 0.202733, acc.: 71.88%] [G loss: 0.457987]\n",
      "epoch:18 step:17597 [D loss: 0.233143, acc.: 58.59%] [G loss: 0.446910]\n",
      "epoch:18 step:17598 [D loss: 0.227431, acc.: 63.28%] [G loss: 0.458977]\n",
      "epoch:18 step:17599 [D loss: 0.210329, acc.: 66.41%] [G loss: 0.497488]\n",
      "epoch:18 step:17600 [D loss: 0.242411, acc.: 57.03%] [G loss: 0.400065]\n",
      "##############\n",
      "[2.5103016  1.7121642  6.01334545 4.71744541 3.64048848 5.43401417\n",
      " 4.51866908 4.49876524 4.37380144 4.00489841]\n",
      "##########\n",
      "epoch:18 step:17601 [D loss: 0.224849, acc.: 57.03%] [G loss: 0.426553]\n",
      "epoch:18 step:17602 [D loss: 0.215552, acc.: 68.75%] [G loss: 0.433504]\n",
      "epoch:18 step:17603 [D loss: 0.213145, acc.: 66.41%] [G loss: 0.449822]\n",
      "epoch:18 step:17604 [D loss: 0.233259, acc.: 53.91%] [G loss: 0.424676]\n",
      "epoch:18 step:17605 [D loss: 0.254002, acc.: 58.59%] [G loss: 0.404081]\n",
      "epoch:18 step:17606 [D loss: 0.241502, acc.: 58.59%] [G loss: 0.405667]\n",
      "epoch:18 step:17607 [D loss: 0.259726, acc.: 54.69%] [G loss: 0.392814]\n",
      "epoch:18 step:17608 [D loss: 0.206531, acc.: 67.97%] [G loss: 0.443352]\n",
      "epoch:18 step:17609 [D loss: 0.215649, acc.: 67.19%] [G loss: 0.395526]\n",
      "epoch:18 step:17610 [D loss: 0.226489, acc.: 61.72%] [G loss: 0.418735]\n",
      "epoch:18 step:17611 [D loss: 0.257088, acc.: 50.78%] [G loss: 0.431085]\n",
      "epoch:18 step:17612 [D loss: 0.214349, acc.: 64.84%] [G loss: 0.459047]\n",
      "epoch:18 step:17613 [D loss: 0.217796, acc.: 65.62%] [G loss: 0.421778]\n",
      "epoch:18 step:17614 [D loss: 0.221352, acc.: 60.94%] [G loss: 0.415715]\n",
      "epoch:18 step:17615 [D loss: 0.201494, acc.: 69.53%] [G loss: 0.423351]\n",
      "epoch:18 step:17616 [D loss: 0.214647, acc.: 63.28%] [G loss: 0.403975]\n",
      "epoch:18 step:17617 [D loss: 0.219878, acc.: 60.94%] [G loss: 0.433365]\n",
      "epoch:18 step:17618 [D loss: 0.239363, acc.: 57.81%] [G loss: 0.434162]\n",
      "epoch:18 step:17619 [D loss: 0.231310, acc.: 60.16%] [G loss: 0.457258]\n",
      "epoch:18 step:17620 [D loss: 0.205643, acc.: 65.62%] [G loss: 0.457745]\n",
      "epoch:18 step:17621 [D loss: 0.208716, acc.: 65.62%] [G loss: 0.431852]\n",
      "epoch:18 step:17622 [D loss: 0.223739, acc.: 64.06%] [G loss: 0.447980]\n",
      "epoch:18 step:17623 [D loss: 0.239887, acc.: 63.28%] [G loss: 0.387082]\n",
      "epoch:18 step:17624 [D loss: 0.221334, acc.: 64.06%] [G loss: 0.463678]\n",
      "epoch:18 step:17625 [D loss: 0.260865, acc.: 50.00%] [G loss: 0.433945]\n",
      "epoch:18 step:17626 [D loss: 0.232815, acc.: 61.72%] [G loss: 0.405804]\n",
      "epoch:18 step:17627 [D loss: 0.224938, acc.: 60.94%] [G loss: 0.414822]\n",
      "epoch:18 step:17628 [D loss: 0.208509, acc.: 59.38%] [G loss: 0.415178]\n",
      "epoch:18 step:17629 [D loss: 0.240530, acc.: 67.19%] [G loss: 0.397708]\n",
      "epoch:18 step:17630 [D loss: 0.215876, acc.: 64.84%] [G loss: 0.457378]\n",
      "epoch:18 step:17631 [D loss: 0.246698, acc.: 53.12%] [G loss: 0.388788]\n",
      "epoch:18 step:17632 [D loss: 0.218408, acc.: 67.97%] [G loss: 0.414010]\n",
      "epoch:18 step:17633 [D loss: 0.228812, acc.: 61.72%] [G loss: 0.455069]\n",
      "epoch:18 step:17634 [D loss: 0.251391, acc.: 60.16%] [G loss: 0.480279]\n",
      "epoch:18 step:17635 [D loss: 0.235956, acc.: 67.19%] [G loss: 0.464595]\n",
      "epoch:18 step:17636 [D loss: 0.234710, acc.: 63.28%] [G loss: 0.410973]\n",
      "epoch:18 step:17637 [D loss: 0.223961, acc.: 60.16%] [G loss: 0.479249]\n",
      "epoch:18 step:17638 [D loss: 0.253830, acc.: 49.22%] [G loss: 0.433320]\n",
      "epoch:18 step:17639 [D loss: 0.189237, acc.: 73.44%] [G loss: 0.460537]\n",
      "epoch:18 step:17640 [D loss: 0.213990, acc.: 65.62%] [G loss: 0.438976]\n",
      "epoch:18 step:17641 [D loss: 0.223869, acc.: 63.28%] [G loss: 0.436918]\n",
      "epoch:18 step:17642 [D loss: 0.234238, acc.: 63.28%] [G loss: 0.433355]\n",
      "epoch:18 step:17643 [D loss: 0.218973, acc.: 60.94%] [G loss: 0.422446]\n",
      "epoch:18 step:17644 [D loss: 0.228444, acc.: 67.19%] [G loss: 0.429862]\n",
      "epoch:18 step:17645 [D loss: 0.222284, acc.: 65.62%] [G loss: 0.436685]\n",
      "epoch:18 step:17646 [D loss: 0.208462, acc.: 64.06%] [G loss: 0.436618]\n",
      "epoch:18 step:17647 [D loss: 0.209353, acc.: 70.31%] [G loss: 0.473298]\n",
      "epoch:18 step:17648 [D loss: 0.206165, acc.: 67.97%] [G loss: 0.538550]\n",
      "epoch:18 step:17649 [D loss: 0.259695, acc.: 46.09%] [G loss: 0.465460]\n",
      "epoch:18 step:17650 [D loss: 0.276262, acc.: 48.44%] [G loss: 0.422707]\n",
      "epoch:18 step:17651 [D loss: 0.213712, acc.: 67.19%] [G loss: 0.438137]\n",
      "epoch:18 step:17652 [D loss: 0.216999, acc.: 67.19%] [G loss: 0.433604]\n",
      "epoch:18 step:17653 [D loss: 0.265815, acc.: 59.38%] [G loss: 0.426934]\n",
      "epoch:18 step:17654 [D loss: 0.261326, acc.: 52.34%] [G loss: 0.387175]\n",
      "epoch:18 step:17655 [D loss: 0.229595, acc.: 60.16%] [G loss: 0.409092]\n",
      "epoch:18 step:17656 [D loss: 0.228992, acc.: 64.84%] [G loss: 0.382660]\n",
      "epoch:18 step:17657 [D loss: 0.260989, acc.: 47.66%] [G loss: 0.376542]\n",
      "epoch:18 step:17658 [D loss: 0.204119, acc.: 71.09%] [G loss: 0.446892]\n",
      "epoch:18 step:17659 [D loss: 0.192392, acc.: 67.19%] [G loss: 0.413013]\n",
      "epoch:18 step:17660 [D loss: 0.268462, acc.: 46.88%] [G loss: 0.438984]\n",
      "epoch:18 step:17661 [D loss: 0.246646, acc.: 54.69%] [G loss: 0.419156]\n",
      "epoch:18 step:17662 [D loss: 0.237533, acc.: 63.28%] [G loss: 0.437992]\n",
      "epoch:18 step:17663 [D loss: 0.233484, acc.: 55.47%] [G loss: 0.445266]\n",
      "epoch:18 step:17664 [D loss: 0.223798, acc.: 60.16%] [G loss: 0.428562]\n",
      "epoch:18 step:17665 [D loss: 0.227696, acc.: 63.28%] [G loss: 0.406440]\n",
      "epoch:18 step:17666 [D loss: 0.258498, acc.: 56.25%] [G loss: 0.392000]\n",
      "epoch:18 step:17667 [D loss: 0.211497, acc.: 66.41%] [G loss: 0.427403]\n",
      "epoch:18 step:17668 [D loss: 0.205647, acc.: 65.62%] [G loss: 0.454700]\n",
      "epoch:18 step:17669 [D loss: 0.229247, acc.: 60.94%] [G loss: 0.482095]\n",
      "epoch:18 step:17670 [D loss: 0.230490, acc.: 60.16%] [G loss: 0.444115]\n",
      "epoch:18 step:17671 [D loss: 0.216825, acc.: 67.97%] [G loss: 0.439269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17672 [D loss: 0.216341, acc.: 64.06%] [G loss: 0.454187]\n",
      "epoch:18 step:17673 [D loss: 0.219826, acc.: 61.72%] [G loss: 0.420223]\n",
      "epoch:18 step:17674 [D loss: 0.247016, acc.: 60.94%] [G loss: 0.410043]\n",
      "epoch:18 step:17675 [D loss: 0.208476, acc.: 68.75%] [G loss: 0.442047]\n",
      "epoch:18 step:17676 [D loss: 0.209600, acc.: 68.75%] [G loss: 0.399300]\n",
      "epoch:18 step:17677 [D loss: 0.236596, acc.: 61.72%] [G loss: 0.437137]\n",
      "epoch:18 step:17678 [D loss: 0.235709, acc.: 58.59%] [G loss: 0.396530]\n",
      "epoch:18 step:17679 [D loss: 0.217260, acc.: 62.50%] [G loss: 0.427104]\n",
      "epoch:18 step:17680 [D loss: 0.240394, acc.: 61.72%] [G loss: 0.409657]\n",
      "epoch:18 step:17681 [D loss: 0.216609, acc.: 58.59%] [G loss: 0.426294]\n",
      "epoch:18 step:17682 [D loss: 0.207868, acc.: 66.41%] [G loss: 0.471957]\n",
      "epoch:18 step:17683 [D loss: 0.233936, acc.: 63.28%] [G loss: 0.448797]\n",
      "epoch:18 step:17684 [D loss: 0.247708, acc.: 58.59%] [G loss: 0.438757]\n",
      "epoch:18 step:17685 [D loss: 0.224800, acc.: 60.94%] [G loss: 0.475094]\n",
      "epoch:18 step:17686 [D loss: 0.299991, acc.: 46.09%] [G loss: 0.390681]\n",
      "epoch:18 step:17687 [D loss: 0.225323, acc.: 65.62%] [G loss: 0.403169]\n",
      "epoch:18 step:17688 [D loss: 0.228245, acc.: 59.38%] [G loss: 0.416552]\n",
      "epoch:18 step:17689 [D loss: 0.202584, acc.: 66.41%] [G loss: 0.459552]\n",
      "epoch:18 step:17690 [D loss: 0.226883, acc.: 67.19%] [G loss: 0.433631]\n",
      "epoch:18 step:17691 [D loss: 0.219270, acc.: 63.28%] [G loss: 0.404777]\n",
      "epoch:18 step:17692 [D loss: 0.220673, acc.: 62.50%] [G loss: 0.468386]\n",
      "epoch:18 step:17693 [D loss: 0.249611, acc.: 54.69%] [G loss: 0.411925]\n",
      "epoch:18 step:17694 [D loss: 0.257670, acc.: 52.34%] [G loss: 0.372753]\n",
      "epoch:18 step:17695 [D loss: 0.250938, acc.: 55.47%] [G loss: 0.406517]\n",
      "epoch:18 step:17696 [D loss: 0.241795, acc.: 56.25%] [G loss: 0.407292]\n",
      "epoch:18 step:17697 [D loss: 0.232408, acc.: 62.50%] [G loss: 0.431362]\n",
      "epoch:18 step:17698 [D loss: 0.198788, acc.: 73.44%] [G loss: 0.437834]\n",
      "epoch:18 step:17699 [D loss: 0.195638, acc.: 72.66%] [G loss: 0.426128]\n",
      "epoch:18 step:17700 [D loss: 0.223105, acc.: 63.28%] [G loss: 0.407156]\n",
      "epoch:18 step:17701 [D loss: 0.247390, acc.: 56.25%] [G loss: 0.390035]\n",
      "epoch:18 step:17702 [D loss: 0.223382, acc.: 63.28%] [G loss: 0.448669]\n",
      "epoch:18 step:17703 [D loss: 0.216057, acc.: 68.75%] [G loss: 0.414865]\n",
      "epoch:18 step:17704 [D loss: 0.214521, acc.: 59.38%] [G loss: 0.429878]\n",
      "epoch:18 step:17705 [D loss: 0.221943, acc.: 63.28%] [G loss: 0.435823]\n",
      "epoch:18 step:17706 [D loss: 0.229608, acc.: 64.06%] [G loss: 0.410880]\n",
      "epoch:18 step:17707 [D loss: 0.223518, acc.: 60.16%] [G loss: 0.414541]\n",
      "epoch:18 step:17708 [D loss: 0.194912, acc.: 72.66%] [G loss: 0.441901]\n",
      "epoch:18 step:17709 [D loss: 0.230902, acc.: 60.94%] [G loss: 0.420226]\n",
      "epoch:18 step:17710 [D loss: 0.232296, acc.: 60.94%] [G loss: 0.417840]\n",
      "epoch:18 step:17711 [D loss: 0.200423, acc.: 71.09%] [G loss: 0.417952]\n",
      "epoch:18 step:17712 [D loss: 0.236178, acc.: 55.47%] [G loss: 0.421277]\n",
      "epoch:18 step:17713 [D loss: 0.254841, acc.: 57.81%] [G loss: 0.364374]\n",
      "epoch:18 step:17714 [D loss: 0.247792, acc.: 52.34%] [G loss: 0.376399]\n",
      "epoch:18 step:17715 [D loss: 0.202543, acc.: 73.44%] [G loss: 0.433766]\n",
      "epoch:18 step:17716 [D loss: 0.266547, acc.: 52.34%] [G loss: 0.412207]\n",
      "epoch:18 step:17717 [D loss: 0.232169, acc.: 60.16%] [G loss: 0.452719]\n",
      "epoch:18 step:17718 [D loss: 0.227320, acc.: 59.38%] [G loss: 0.450968]\n",
      "epoch:18 step:17719 [D loss: 0.216130, acc.: 66.41%] [G loss: 0.442901]\n",
      "epoch:18 step:17720 [D loss: 0.232176, acc.: 60.16%] [G loss: 0.435135]\n",
      "epoch:18 step:17721 [D loss: 0.255583, acc.: 53.12%] [G loss: 0.412264]\n",
      "epoch:18 step:17722 [D loss: 0.233491, acc.: 63.28%] [G loss: 0.436514]\n",
      "epoch:18 step:17723 [D loss: 0.206551, acc.: 71.88%] [G loss: 0.484456]\n",
      "epoch:18 step:17724 [D loss: 0.267315, acc.: 53.91%] [G loss: 0.399633]\n",
      "epoch:18 step:17725 [D loss: 0.231232, acc.: 63.28%] [G loss: 0.423405]\n",
      "epoch:18 step:17726 [D loss: 0.212092, acc.: 70.31%] [G loss: 0.440123]\n",
      "epoch:18 step:17727 [D loss: 0.267781, acc.: 53.91%] [G loss: 0.441447]\n",
      "epoch:18 step:17728 [D loss: 0.261054, acc.: 53.12%] [G loss: 0.392430]\n",
      "epoch:18 step:17729 [D loss: 0.216874, acc.: 62.50%] [G loss: 0.403150]\n",
      "epoch:18 step:17730 [D loss: 0.224183, acc.: 64.84%] [G loss: 0.437718]\n",
      "epoch:18 step:17731 [D loss: 0.249609, acc.: 58.59%] [G loss: 0.410615]\n",
      "epoch:18 step:17732 [D loss: 0.222273, acc.: 63.28%] [G loss: 0.397390]\n",
      "epoch:18 step:17733 [D loss: 0.225880, acc.: 61.72%] [G loss: 0.413134]\n",
      "epoch:18 step:17734 [D loss: 0.217998, acc.: 65.62%] [G loss: 0.374954]\n",
      "epoch:18 step:17735 [D loss: 0.222780, acc.: 66.41%] [G loss: 0.445720]\n",
      "epoch:18 step:17736 [D loss: 0.230156, acc.: 58.59%] [G loss: 0.370234]\n",
      "epoch:18 step:17737 [D loss: 0.179419, acc.: 75.78%] [G loss: 0.446358]\n",
      "epoch:18 step:17738 [D loss: 0.226947, acc.: 67.97%] [G loss: 0.447761]\n",
      "epoch:18 step:17739 [D loss: 0.221543, acc.: 64.84%] [G loss: 0.400323]\n",
      "epoch:18 step:17740 [D loss: 0.257707, acc.: 54.69%] [G loss: 0.401200]\n",
      "epoch:18 step:17741 [D loss: 0.192833, acc.: 75.00%] [G loss: 0.469622]\n",
      "epoch:18 step:17742 [D loss: 0.225477, acc.: 65.62%] [G loss: 0.436199]\n",
      "epoch:18 step:17743 [D loss: 0.253130, acc.: 55.47%] [G loss: 0.440999]\n",
      "epoch:18 step:17744 [D loss: 0.216873, acc.: 67.97%] [G loss: 0.437737]\n",
      "epoch:18 step:17745 [D loss: 0.243762, acc.: 54.69%] [G loss: 0.430434]\n",
      "epoch:18 step:17746 [D loss: 0.232667, acc.: 57.03%] [G loss: 0.412972]\n",
      "epoch:18 step:17747 [D loss: 0.232343, acc.: 58.59%] [G loss: 0.436378]\n",
      "epoch:18 step:17748 [D loss: 0.226932, acc.: 64.06%] [G loss: 0.424075]\n",
      "epoch:18 step:17749 [D loss: 0.234228, acc.: 64.84%] [G loss: 0.423051]\n",
      "epoch:18 step:17750 [D loss: 0.220179, acc.: 67.19%] [G loss: 0.380814]\n",
      "epoch:18 step:17751 [D loss: 0.215369, acc.: 63.28%] [G loss: 0.470337]\n",
      "epoch:18 step:17752 [D loss: 0.203545, acc.: 65.62%] [G loss: 0.541197]\n",
      "epoch:18 step:17753 [D loss: 0.243716, acc.: 57.81%] [G loss: 0.454748]\n",
      "epoch:18 step:17754 [D loss: 0.221236, acc.: 64.84%] [G loss: 0.442576]\n",
      "epoch:18 step:17755 [D loss: 0.209983, acc.: 66.41%] [G loss: 0.435616]\n",
      "epoch:18 step:17756 [D loss: 0.195888, acc.: 69.53%] [G loss: 0.428938]\n",
      "epoch:18 step:17757 [D loss: 0.265218, acc.: 50.78%] [G loss: 0.399121]\n",
      "epoch:18 step:17758 [D loss: 0.249381, acc.: 56.25%] [G loss: 0.413143]\n",
      "epoch:18 step:17759 [D loss: 0.228621, acc.: 61.72%] [G loss: 0.431471]\n",
      "epoch:18 step:17760 [D loss: 0.197852, acc.: 68.75%] [G loss: 0.436515]\n",
      "epoch:18 step:17761 [D loss: 0.205374, acc.: 65.62%] [G loss: 0.480642]\n",
      "epoch:18 step:17762 [D loss: 0.210302, acc.: 66.41%] [G loss: 0.444543]\n",
      "epoch:18 step:17763 [D loss: 0.211292, acc.: 62.50%] [G loss: 0.446576]\n",
      "epoch:18 step:17764 [D loss: 0.196876, acc.: 66.41%] [G loss: 0.447230]\n",
      "epoch:18 step:17765 [D loss: 0.189972, acc.: 68.75%] [G loss: 0.471327]\n",
      "epoch:18 step:17766 [D loss: 0.234121, acc.: 62.50%] [G loss: 0.449015]\n",
      "epoch:18 step:17767 [D loss: 0.232904, acc.: 57.03%] [G loss: 0.453476]\n",
      "epoch:18 step:17768 [D loss: 0.238818, acc.: 60.94%] [G loss: 0.450552]\n",
      "epoch:18 step:17769 [D loss: 0.237868, acc.: 56.25%] [G loss: 0.408144]\n",
      "epoch:18 step:17770 [D loss: 0.216543, acc.: 67.19%] [G loss: 0.459927]\n",
      "epoch:18 step:17771 [D loss: 0.194399, acc.: 75.78%] [G loss: 0.460757]\n",
      "epoch:18 step:17772 [D loss: 0.214934, acc.: 62.50%] [G loss: 0.430684]\n",
      "epoch:18 step:17773 [D loss: 0.230666, acc.: 63.28%] [G loss: 0.409063]\n",
      "epoch:18 step:17774 [D loss: 0.199664, acc.: 68.75%] [G loss: 0.460922]\n",
      "epoch:18 step:17775 [D loss: 0.192476, acc.: 66.41%] [G loss: 0.465277]\n",
      "epoch:18 step:17776 [D loss: 0.211938, acc.: 63.28%] [G loss: 0.478221]\n",
      "epoch:18 step:17777 [D loss: 0.220655, acc.: 62.50%] [G loss: 0.499857]\n",
      "epoch:18 step:17778 [D loss: 0.237860, acc.: 60.94%] [G loss: 0.475981]\n",
      "epoch:18 step:17779 [D loss: 0.228361, acc.: 64.84%] [G loss: 0.459047]\n",
      "epoch:18 step:17780 [D loss: 0.222732, acc.: 60.94%] [G loss: 0.452494]\n",
      "epoch:18 step:17781 [D loss: 0.292372, acc.: 52.34%] [G loss: 0.434138]\n",
      "epoch:18 step:17782 [D loss: 0.218367, acc.: 65.62%] [G loss: 0.434709]\n",
      "epoch:18 step:17783 [D loss: 0.227350, acc.: 62.50%] [G loss: 0.442354]\n",
      "epoch:18 step:17784 [D loss: 0.199615, acc.: 75.78%] [G loss: 0.450034]\n",
      "epoch:18 step:17785 [D loss: 0.223160, acc.: 72.66%] [G loss: 0.472356]\n",
      "epoch:18 step:17786 [D loss: 0.293179, acc.: 46.88%] [G loss: 0.430301]\n",
      "epoch:18 step:17787 [D loss: 0.213816, acc.: 62.50%] [G loss: 0.455823]\n",
      "epoch:18 step:17788 [D loss: 0.222330, acc.: 64.06%] [G loss: 0.439778]\n",
      "epoch:18 step:17789 [D loss: 0.209459, acc.: 67.19%] [G loss: 0.428956]\n",
      "epoch:18 step:17790 [D loss: 0.186465, acc.: 72.66%] [G loss: 0.495304]\n",
      "epoch:18 step:17791 [D loss: 0.195596, acc.: 71.88%] [G loss: 0.497518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17792 [D loss: 0.180584, acc.: 71.88%] [G loss: 0.509047]\n",
      "epoch:18 step:17793 [D loss: 0.242195, acc.: 54.69%] [G loss: 0.491022]\n",
      "epoch:18 step:17794 [D loss: 0.318179, acc.: 52.34%] [G loss: 0.492493]\n",
      "epoch:18 step:17795 [D loss: 0.237372, acc.: 57.81%] [G loss: 0.513809]\n",
      "epoch:18 step:17796 [D loss: 0.208090, acc.: 63.28%] [G loss: 0.492896]\n",
      "epoch:18 step:17797 [D loss: 0.263155, acc.: 57.81%] [G loss: 0.453662]\n",
      "epoch:18 step:17798 [D loss: 0.245930, acc.: 55.47%] [G loss: 0.425143]\n",
      "epoch:18 step:17799 [D loss: 0.239229, acc.: 68.75%] [G loss: 0.434886]\n",
      "epoch:18 step:17800 [D loss: 0.207625, acc.: 67.19%] [G loss: 0.488835]\n",
      "##############\n",
      "[2.42012809 1.63210464 5.79616787 4.36803444 3.3280859  5.51707636\n",
      " 4.22084878 4.67388605 4.2126304  3.82309382]\n",
      "##########\n",
      "epoch:18 step:17801 [D loss: 0.198336, acc.: 71.88%] [G loss: 0.471828]\n",
      "epoch:18 step:17802 [D loss: 0.180996, acc.: 72.66%] [G loss: 0.520239]\n",
      "epoch:18 step:17803 [D loss: 0.179331, acc.: 74.22%] [G loss: 0.535581]\n",
      "epoch:19 step:17804 [D loss: 0.286611, acc.: 52.34%] [G loss: 0.476066]\n",
      "epoch:19 step:17805 [D loss: 0.270371, acc.: 55.47%] [G loss: 0.431873]\n",
      "epoch:19 step:17806 [D loss: 0.244796, acc.: 58.59%] [G loss: 0.460479]\n",
      "epoch:19 step:17807 [D loss: 0.224009, acc.: 60.94%] [G loss: 0.445020]\n",
      "epoch:19 step:17808 [D loss: 0.243250, acc.: 57.03%] [G loss: 0.403029]\n",
      "epoch:19 step:17809 [D loss: 0.224321, acc.: 65.62%] [G loss: 0.456075]\n",
      "epoch:19 step:17810 [D loss: 0.202841, acc.: 70.31%] [G loss: 0.439297]\n",
      "epoch:19 step:17811 [D loss: 0.218650, acc.: 61.72%] [G loss: 0.439490]\n",
      "epoch:19 step:17812 [D loss: 0.206398, acc.: 69.53%] [G loss: 0.446554]\n",
      "epoch:19 step:17813 [D loss: 0.229190, acc.: 60.16%] [G loss: 0.464879]\n",
      "epoch:19 step:17814 [D loss: 0.216001, acc.: 64.84%] [G loss: 0.438277]\n",
      "epoch:19 step:17815 [D loss: 0.211812, acc.: 64.84%] [G loss: 0.431186]\n",
      "epoch:19 step:17816 [D loss: 0.204559, acc.: 65.62%] [G loss: 0.400673]\n",
      "epoch:19 step:17817 [D loss: 0.188191, acc.: 73.44%] [G loss: 0.489902]\n",
      "epoch:19 step:17818 [D loss: 0.193311, acc.: 70.31%] [G loss: 0.450345]\n",
      "epoch:19 step:17819 [D loss: 0.209868, acc.: 67.19%] [G loss: 0.467167]\n",
      "epoch:19 step:17820 [D loss: 0.241245, acc.: 58.59%] [G loss: 0.438135]\n",
      "epoch:19 step:17821 [D loss: 0.227602, acc.: 65.62%] [G loss: 0.478581]\n",
      "epoch:19 step:17822 [D loss: 0.245953, acc.: 58.59%] [G loss: 0.425129]\n",
      "epoch:19 step:17823 [D loss: 0.251986, acc.: 54.69%] [G loss: 0.450597]\n",
      "epoch:19 step:17824 [D loss: 0.235450, acc.: 60.16%] [G loss: 0.408519]\n",
      "epoch:19 step:17825 [D loss: 0.225205, acc.: 61.72%] [G loss: 0.453946]\n",
      "epoch:19 step:17826 [D loss: 0.248854, acc.: 59.38%] [G loss: 0.405803]\n",
      "epoch:19 step:17827 [D loss: 0.238220, acc.: 63.28%] [G loss: 0.415899]\n",
      "epoch:19 step:17828 [D loss: 0.201925, acc.: 70.31%] [G loss: 0.423088]\n",
      "epoch:19 step:17829 [D loss: 0.228563, acc.: 59.38%] [G loss: 0.429409]\n",
      "epoch:19 step:17830 [D loss: 0.232311, acc.: 55.47%] [G loss: 0.406951]\n",
      "epoch:19 step:17831 [D loss: 0.199803, acc.: 70.31%] [G loss: 0.443226]\n",
      "epoch:19 step:17832 [D loss: 0.221099, acc.: 57.81%] [G loss: 0.435820]\n",
      "epoch:19 step:17833 [D loss: 0.198718, acc.: 70.31%] [G loss: 0.483245]\n",
      "epoch:19 step:17834 [D loss: 0.236995, acc.: 56.25%] [G loss: 0.413462]\n",
      "epoch:19 step:17835 [D loss: 0.206121, acc.: 67.97%] [G loss: 0.437993]\n",
      "epoch:19 step:17836 [D loss: 0.240072, acc.: 60.94%] [G loss: 0.424269]\n",
      "epoch:19 step:17837 [D loss: 0.250677, acc.: 50.78%] [G loss: 0.411917]\n",
      "epoch:19 step:17838 [D loss: 0.238614, acc.: 65.62%] [G loss: 0.420603]\n",
      "epoch:19 step:17839 [D loss: 0.217349, acc.: 68.75%] [G loss: 0.466360]\n",
      "epoch:19 step:17840 [D loss: 0.231150, acc.: 62.50%] [G loss: 0.447316]\n",
      "epoch:19 step:17841 [D loss: 0.245051, acc.: 57.03%] [G loss: 0.408155]\n",
      "epoch:19 step:17842 [D loss: 0.215280, acc.: 64.06%] [G loss: 0.414745]\n",
      "epoch:19 step:17843 [D loss: 0.210480, acc.: 64.84%] [G loss: 0.399234]\n",
      "epoch:19 step:17844 [D loss: 0.217349, acc.: 67.97%] [G loss: 0.395967]\n",
      "epoch:19 step:17845 [D loss: 0.217308, acc.: 60.94%] [G loss: 0.411433]\n",
      "epoch:19 step:17846 [D loss: 0.243588, acc.: 58.59%] [G loss: 0.452778]\n",
      "epoch:19 step:17847 [D loss: 0.240455, acc.: 57.03%] [G loss: 0.406935]\n",
      "epoch:19 step:17848 [D loss: 0.232684, acc.: 62.50%] [G loss: 0.400372]\n",
      "epoch:19 step:17849 [D loss: 0.248942, acc.: 60.94%] [G loss: 0.456463]\n",
      "epoch:19 step:17850 [D loss: 0.214896, acc.: 67.97%] [G loss: 0.414749]\n",
      "epoch:19 step:17851 [D loss: 0.214342, acc.: 69.53%] [G loss: 0.418622]\n",
      "epoch:19 step:17852 [D loss: 0.200333, acc.: 68.75%] [G loss: 0.421326]\n",
      "epoch:19 step:17853 [D loss: 0.202267, acc.: 67.97%] [G loss: 0.440102]\n",
      "epoch:19 step:17854 [D loss: 0.239393, acc.: 59.38%] [G loss: 0.414367]\n",
      "epoch:19 step:17855 [D loss: 0.223836, acc.: 63.28%] [G loss: 0.501408]\n",
      "epoch:19 step:17856 [D loss: 0.230279, acc.: 67.19%] [G loss: 0.442643]\n",
      "epoch:19 step:17857 [D loss: 0.223307, acc.: 67.97%] [G loss: 0.428592]\n",
      "epoch:19 step:17858 [D loss: 0.228250, acc.: 60.94%] [G loss: 0.438177]\n",
      "epoch:19 step:17859 [D loss: 0.222439, acc.: 64.84%] [G loss: 0.441441]\n",
      "epoch:19 step:17860 [D loss: 0.248002, acc.: 58.59%] [G loss: 0.421166]\n",
      "epoch:19 step:17861 [D loss: 0.231207, acc.: 57.03%] [G loss: 0.431045]\n",
      "epoch:19 step:17862 [D loss: 0.234172, acc.: 60.94%] [G loss: 0.403378]\n",
      "epoch:19 step:17863 [D loss: 0.226821, acc.: 60.94%] [G loss: 0.434075]\n",
      "epoch:19 step:17864 [D loss: 0.226269, acc.: 61.72%] [G loss: 0.444525]\n",
      "epoch:19 step:17865 [D loss: 0.252764, acc.: 60.94%] [G loss: 0.399466]\n",
      "epoch:19 step:17866 [D loss: 0.219529, acc.: 66.41%] [G loss: 0.443407]\n",
      "epoch:19 step:17867 [D loss: 0.227775, acc.: 59.38%] [G loss: 0.421811]\n",
      "epoch:19 step:17868 [D loss: 0.236203, acc.: 58.59%] [G loss: 0.409671]\n",
      "epoch:19 step:17869 [D loss: 0.206141, acc.: 71.09%] [G loss: 0.379230]\n",
      "epoch:19 step:17870 [D loss: 0.223039, acc.: 64.06%] [G loss: 0.431015]\n",
      "epoch:19 step:17871 [D loss: 0.218508, acc.: 69.53%] [G loss: 0.451245]\n",
      "epoch:19 step:17872 [D loss: 0.181629, acc.: 71.88%] [G loss: 0.447066]\n",
      "epoch:19 step:17873 [D loss: 0.220391, acc.: 65.62%] [G loss: 0.443897]\n",
      "epoch:19 step:17874 [D loss: 0.265641, acc.: 51.56%] [G loss: 0.438137]\n",
      "epoch:19 step:17875 [D loss: 0.235318, acc.: 59.38%] [G loss: 0.441888]\n",
      "epoch:19 step:17876 [D loss: 0.232974, acc.: 58.59%] [G loss: 0.423027]\n",
      "epoch:19 step:17877 [D loss: 0.214938, acc.: 64.84%] [G loss: 0.454240]\n",
      "epoch:19 step:17878 [D loss: 0.226127, acc.: 59.38%] [G loss: 0.435779]\n",
      "epoch:19 step:17879 [D loss: 0.201325, acc.: 67.97%] [G loss: 0.439264]\n",
      "epoch:19 step:17880 [D loss: 0.187330, acc.: 70.31%] [G loss: 0.472370]\n",
      "epoch:19 step:17881 [D loss: 0.251767, acc.: 58.59%] [G loss: 0.444121]\n",
      "epoch:19 step:17882 [D loss: 0.248441, acc.: 56.25%] [G loss: 0.398908]\n",
      "epoch:19 step:17883 [D loss: 0.210513, acc.: 67.97%] [G loss: 0.418835]\n",
      "epoch:19 step:17884 [D loss: 0.229103, acc.: 60.94%] [G loss: 0.425935]\n",
      "epoch:19 step:17885 [D loss: 0.197301, acc.: 68.75%] [G loss: 0.420303]\n",
      "epoch:19 step:17886 [D loss: 0.220064, acc.: 66.41%] [G loss: 0.461561]\n",
      "epoch:19 step:17887 [D loss: 0.201504, acc.: 69.53%] [G loss: 0.476762]\n",
      "epoch:19 step:17888 [D loss: 0.233694, acc.: 65.62%] [G loss: 0.425277]\n",
      "epoch:19 step:17889 [D loss: 0.235144, acc.: 63.28%] [G loss: 0.450932]\n",
      "epoch:19 step:17890 [D loss: 0.232662, acc.: 60.16%] [G loss: 0.425368]\n",
      "epoch:19 step:17891 [D loss: 0.227217, acc.: 61.72%] [G loss: 0.455173]\n",
      "epoch:19 step:17892 [D loss: 0.227915, acc.: 62.50%] [G loss: 0.418450]\n",
      "epoch:19 step:17893 [D loss: 0.227140, acc.: 63.28%] [G loss: 0.437638]\n",
      "epoch:19 step:17894 [D loss: 0.219167, acc.: 60.94%] [G loss: 0.410839]\n",
      "epoch:19 step:17895 [D loss: 0.214186, acc.: 63.28%] [G loss: 0.421238]\n",
      "epoch:19 step:17896 [D loss: 0.179914, acc.: 76.56%] [G loss: 0.483836]\n",
      "epoch:19 step:17897 [D loss: 0.237189, acc.: 57.03%] [G loss: 0.419017]\n",
      "epoch:19 step:17898 [D loss: 0.228433, acc.: 58.59%] [G loss: 0.465650]\n",
      "epoch:19 step:17899 [D loss: 0.211974, acc.: 67.19%] [G loss: 0.437245]\n",
      "epoch:19 step:17900 [D loss: 0.200755, acc.: 67.97%] [G loss: 0.460141]\n",
      "epoch:19 step:17901 [D loss: 0.225501, acc.: 61.72%] [G loss: 0.472479]\n",
      "epoch:19 step:17902 [D loss: 0.233759, acc.: 64.84%] [G loss: 0.471939]\n",
      "epoch:19 step:17903 [D loss: 0.193696, acc.: 68.75%] [G loss: 0.515030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17904 [D loss: 0.262388, acc.: 55.47%] [G loss: 0.409718]\n",
      "epoch:19 step:17905 [D loss: 0.267234, acc.: 54.69%] [G loss: 0.374907]\n",
      "epoch:19 step:17906 [D loss: 0.206429, acc.: 71.09%] [G loss: 0.433923]\n",
      "epoch:19 step:17907 [D loss: 0.236866, acc.: 57.03%] [G loss: 0.386085]\n",
      "epoch:19 step:17908 [D loss: 0.248541, acc.: 55.47%] [G loss: 0.380345]\n",
      "epoch:19 step:17909 [D loss: 0.209769, acc.: 69.53%] [G loss: 0.408207]\n",
      "epoch:19 step:17910 [D loss: 0.195759, acc.: 71.09%] [G loss: 0.481498]\n",
      "epoch:19 step:17911 [D loss: 0.266922, acc.: 55.47%] [G loss: 0.459676]\n",
      "epoch:19 step:17912 [D loss: 0.234352, acc.: 63.28%] [G loss: 0.440495]\n",
      "epoch:19 step:17913 [D loss: 0.238725, acc.: 57.81%] [G loss: 0.391374]\n",
      "epoch:19 step:17914 [D loss: 0.206891, acc.: 71.88%] [G loss: 0.431702]\n",
      "epoch:19 step:17915 [D loss: 0.208186, acc.: 67.19%] [G loss: 0.466248]\n",
      "epoch:19 step:17916 [D loss: 0.192000, acc.: 74.22%] [G loss: 0.446879]\n",
      "epoch:19 step:17917 [D loss: 0.220212, acc.: 68.75%] [G loss: 0.443769]\n",
      "epoch:19 step:17918 [D loss: 0.194478, acc.: 74.22%] [G loss: 0.488209]\n",
      "epoch:19 step:17919 [D loss: 0.207310, acc.: 67.19%] [G loss: 0.456174]\n",
      "epoch:19 step:17920 [D loss: 0.198432, acc.: 70.31%] [G loss: 0.511192]\n",
      "epoch:19 step:17921 [D loss: 0.209406, acc.: 69.53%] [G loss: 0.491310]\n",
      "epoch:19 step:17922 [D loss: 0.180661, acc.: 73.44%] [G loss: 0.545155]\n",
      "epoch:19 step:17923 [D loss: 0.254819, acc.: 59.38%] [G loss: 0.459482]\n",
      "epoch:19 step:17924 [D loss: 0.254222, acc.: 50.00%] [G loss: 0.437005]\n",
      "epoch:19 step:17925 [D loss: 0.194954, acc.: 72.66%] [G loss: 0.451781]\n",
      "epoch:19 step:17926 [D loss: 0.208553, acc.: 66.41%] [G loss: 0.451693]\n",
      "epoch:19 step:17927 [D loss: 0.206836, acc.: 67.97%] [G loss: 0.459553]\n",
      "epoch:19 step:17928 [D loss: 0.250395, acc.: 57.03%] [G loss: 0.402125]\n",
      "epoch:19 step:17929 [D loss: 0.196609, acc.: 71.88%] [G loss: 0.417025]\n",
      "epoch:19 step:17930 [D loss: 0.211899, acc.: 64.06%] [G loss: 0.441784]\n",
      "epoch:19 step:17931 [D loss: 0.230229, acc.: 58.59%] [G loss: 0.414665]\n",
      "epoch:19 step:17932 [D loss: 0.222330, acc.: 67.97%] [G loss: 0.434512]\n",
      "epoch:19 step:17933 [D loss: 0.204428, acc.: 71.09%] [G loss: 0.429155]\n",
      "epoch:19 step:17934 [D loss: 0.206560, acc.: 66.41%] [G loss: 0.450634]\n",
      "epoch:19 step:17935 [D loss: 0.219727, acc.: 63.28%] [G loss: 0.419196]\n",
      "epoch:19 step:17936 [D loss: 0.259332, acc.: 53.91%] [G loss: 0.472703]\n",
      "epoch:19 step:17937 [D loss: 0.212327, acc.: 63.28%] [G loss: 0.445079]\n",
      "epoch:19 step:17938 [D loss: 0.219953, acc.: 60.94%] [G loss: 0.457765]\n",
      "epoch:19 step:17939 [D loss: 0.224031, acc.: 62.50%] [G loss: 0.456032]\n",
      "epoch:19 step:17940 [D loss: 0.277946, acc.: 51.56%] [G loss: 0.422194]\n",
      "epoch:19 step:17941 [D loss: 0.245057, acc.: 60.16%] [G loss: 0.372442]\n",
      "epoch:19 step:17942 [D loss: 0.240275, acc.: 58.59%] [G loss: 0.379015]\n",
      "epoch:19 step:17943 [D loss: 0.227700, acc.: 59.38%] [G loss: 0.422576]\n",
      "epoch:19 step:17944 [D loss: 0.219571, acc.: 61.72%] [G loss: 0.425062]\n",
      "epoch:19 step:17945 [D loss: 0.240941, acc.: 54.69%] [G loss: 0.418762]\n",
      "epoch:19 step:17946 [D loss: 0.226077, acc.: 58.59%] [G loss: 0.427377]\n",
      "epoch:19 step:17947 [D loss: 0.199954, acc.: 78.12%] [G loss: 0.480744]\n",
      "epoch:19 step:17948 [D loss: 0.246778, acc.: 55.47%] [G loss: 0.407217]\n",
      "epoch:19 step:17949 [D loss: 0.233565, acc.: 60.94%] [G loss: 0.423678]\n",
      "epoch:19 step:17950 [D loss: 0.251722, acc.: 54.69%] [G loss: 0.404501]\n",
      "epoch:19 step:17951 [D loss: 0.235956, acc.: 55.47%] [G loss: 0.421382]\n",
      "epoch:19 step:17952 [D loss: 0.207949, acc.: 64.84%] [G loss: 0.452864]\n",
      "epoch:19 step:17953 [D loss: 0.228634, acc.: 55.47%] [G loss: 0.422708]\n",
      "epoch:19 step:17954 [D loss: 0.218754, acc.: 65.62%] [G loss: 0.435672]\n",
      "epoch:19 step:17955 [D loss: 0.216422, acc.: 65.62%] [G loss: 0.424553]\n",
      "epoch:19 step:17956 [D loss: 0.234846, acc.: 58.59%] [G loss: 0.445485]\n",
      "epoch:19 step:17957 [D loss: 0.215022, acc.: 63.28%] [G loss: 0.426542]\n",
      "epoch:19 step:17958 [D loss: 0.222040, acc.: 60.16%] [G loss: 0.442548]\n",
      "epoch:19 step:17959 [D loss: 0.225711, acc.: 64.84%] [G loss: 0.418406]\n",
      "epoch:19 step:17960 [D loss: 0.231986, acc.: 64.06%] [G loss: 0.426751]\n",
      "epoch:19 step:17961 [D loss: 0.216533, acc.: 62.50%] [G loss: 0.419983]\n",
      "epoch:19 step:17962 [D loss: 0.227213, acc.: 68.75%] [G loss: 0.427645]\n",
      "epoch:19 step:17963 [D loss: 0.285281, acc.: 53.12%] [G loss: 0.404991]\n",
      "epoch:19 step:17964 [D loss: 0.261737, acc.: 56.25%] [G loss: 0.413706]\n",
      "epoch:19 step:17965 [D loss: 0.248427, acc.: 53.91%] [G loss: 0.407504]\n",
      "epoch:19 step:17966 [D loss: 0.217329, acc.: 61.72%] [G loss: 0.434522]\n",
      "epoch:19 step:17967 [D loss: 0.230852, acc.: 61.72%] [G loss: 0.391565]\n",
      "epoch:19 step:17968 [D loss: 0.214623, acc.: 71.09%] [G loss: 0.415126]\n",
      "epoch:19 step:17969 [D loss: 0.216104, acc.: 62.50%] [G loss: 0.436519]\n",
      "epoch:19 step:17970 [D loss: 0.245234, acc.: 52.34%] [G loss: 0.414719]\n",
      "epoch:19 step:17971 [D loss: 0.201382, acc.: 69.53%] [G loss: 0.439619]\n",
      "epoch:19 step:17972 [D loss: 0.220966, acc.: 63.28%] [G loss: 0.474970]\n",
      "epoch:19 step:17973 [D loss: 0.266874, acc.: 50.78%] [G loss: 0.425655]\n",
      "epoch:19 step:17974 [D loss: 0.219480, acc.: 62.50%] [G loss: 0.434639]\n",
      "epoch:19 step:17975 [D loss: 0.236420, acc.: 60.16%] [G loss: 0.413313]\n",
      "epoch:19 step:17976 [D loss: 0.207932, acc.: 71.88%] [G loss: 0.425505]\n",
      "epoch:19 step:17977 [D loss: 0.249695, acc.: 57.03%] [G loss: 0.407574]\n",
      "epoch:19 step:17978 [D loss: 0.200055, acc.: 67.97%] [G loss: 0.445042]\n",
      "epoch:19 step:17979 [D loss: 0.219209, acc.: 59.38%] [G loss: 0.443148]\n",
      "epoch:19 step:17980 [D loss: 0.244319, acc.: 57.03%] [G loss: 0.422480]\n",
      "epoch:19 step:17981 [D loss: 0.247466, acc.: 57.03%] [G loss: 0.380610]\n",
      "epoch:19 step:17982 [D loss: 0.214930, acc.: 67.97%] [G loss: 0.418950]\n",
      "epoch:19 step:17983 [D loss: 0.242649, acc.: 57.81%] [G loss: 0.431228]\n",
      "epoch:19 step:17984 [D loss: 0.227265, acc.: 64.84%] [G loss: 0.424959]\n",
      "epoch:19 step:17985 [D loss: 0.244115, acc.: 57.81%] [G loss: 0.445790]\n",
      "epoch:19 step:17986 [D loss: 0.256685, acc.: 55.47%] [G loss: 0.425865]\n",
      "epoch:19 step:17987 [D loss: 0.228285, acc.: 63.28%] [G loss: 0.417520]\n",
      "epoch:19 step:17988 [D loss: 0.220402, acc.: 61.72%] [G loss: 0.434589]\n",
      "epoch:19 step:17989 [D loss: 0.246358, acc.: 56.25%] [G loss: 0.420114]\n",
      "epoch:19 step:17990 [D loss: 0.253542, acc.: 57.03%] [G loss: 0.405373]\n",
      "epoch:19 step:17991 [D loss: 0.233996, acc.: 63.28%] [G loss: 0.420766]\n",
      "epoch:19 step:17992 [D loss: 0.222234, acc.: 65.62%] [G loss: 0.421166]\n",
      "epoch:19 step:17993 [D loss: 0.228338, acc.: 61.72%] [G loss: 0.387127]\n",
      "epoch:19 step:17994 [D loss: 0.208675, acc.: 67.19%] [G loss: 0.412599]\n",
      "epoch:19 step:17995 [D loss: 0.198484, acc.: 69.53%] [G loss: 0.431515]\n",
      "epoch:19 step:17996 [D loss: 0.230740, acc.: 63.28%] [G loss: 0.399341]\n",
      "epoch:19 step:17997 [D loss: 0.207944, acc.: 68.75%] [G loss: 0.420821]\n",
      "epoch:19 step:17998 [D loss: 0.222622, acc.: 64.84%] [G loss: 0.466625]\n",
      "epoch:19 step:17999 [D loss: 0.225367, acc.: 63.28%] [G loss: 0.447075]\n",
      "epoch:19 step:18000 [D loss: 0.225122, acc.: 57.81%] [G loss: 0.426961]\n",
      "##############\n",
      "[2.47490363 1.67066717 6.03546411 4.74380652 3.39159516 5.74332011\n",
      " 4.26995659 4.61912744 4.37879765 4.01972592]\n",
      "##########\n",
      "epoch:19 step:18001 [D loss: 0.206902, acc.: 71.09%] [G loss: 0.431085]\n",
      "epoch:19 step:18002 [D loss: 0.215421, acc.: 62.50%] [G loss: 0.402794]\n",
      "epoch:19 step:18003 [D loss: 0.254515, acc.: 53.12%] [G loss: 0.399890]\n",
      "epoch:19 step:18004 [D loss: 0.238350, acc.: 61.72%] [G loss: 0.377194]\n",
      "epoch:19 step:18005 [D loss: 0.206910, acc.: 68.75%] [G loss: 0.441513]\n",
      "epoch:19 step:18006 [D loss: 0.286877, acc.: 45.31%] [G loss: 0.395727]\n",
      "epoch:19 step:18007 [D loss: 0.206350, acc.: 70.31%] [G loss: 0.443180]\n",
      "epoch:19 step:18008 [D loss: 0.226698, acc.: 62.50%] [G loss: 0.460425]\n",
      "epoch:19 step:18009 [D loss: 0.185245, acc.: 75.78%] [G loss: 0.487403]\n",
      "epoch:19 step:18010 [D loss: 0.222228, acc.: 61.72%] [G loss: 0.433048]\n",
      "epoch:19 step:18011 [D loss: 0.194902, acc.: 66.41%] [G loss: 0.498483]\n",
      "epoch:19 step:18012 [D loss: 0.197048, acc.: 67.97%] [G loss: 0.484191]\n",
      "epoch:19 step:18013 [D loss: 0.263856, acc.: 48.44%] [G loss: 0.408151]\n",
      "epoch:19 step:18014 [D loss: 0.250511, acc.: 56.25%] [G loss: 0.413102]\n",
      "epoch:19 step:18015 [D loss: 0.216440, acc.: 65.62%] [G loss: 0.394265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18016 [D loss: 0.244524, acc.: 55.47%] [G loss: 0.429267]\n",
      "epoch:19 step:18017 [D loss: 0.240380, acc.: 60.16%] [G loss: 0.437661]\n",
      "epoch:19 step:18018 [D loss: 0.250493, acc.: 58.59%] [G loss: 0.390654]\n",
      "epoch:19 step:18019 [D loss: 0.217186, acc.: 65.62%] [G loss: 0.417450]\n",
      "epoch:19 step:18020 [D loss: 0.222000, acc.: 66.41%] [G loss: 0.462349]\n",
      "epoch:19 step:18021 [D loss: 0.209269, acc.: 65.62%] [G loss: 0.405165]\n",
      "epoch:19 step:18022 [D loss: 0.187638, acc.: 73.44%] [G loss: 0.463123]\n",
      "epoch:19 step:18023 [D loss: 0.256029, acc.: 56.25%] [G loss: 0.453260]\n",
      "epoch:19 step:18024 [D loss: 0.217422, acc.: 64.84%] [G loss: 0.471333]\n",
      "epoch:19 step:18025 [D loss: 0.223972, acc.: 65.62%] [G loss: 0.436485]\n",
      "epoch:19 step:18026 [D loss: 0.205250, acc.: 72.66%] [G loss: 0.481253]\n",
      "epoch:19 step:18027 [D loss: 0.213820, acc.: 64.84%] [G loss: 0.467547]\n",
      "epoch:19 step:18028 [D loss: 0.271958, acc.: 57.81%] [G loss: 0.390773]\n",
      "epoch:19 step:18029 [D loss: 0.213507, acc.: 66.41%] [G loss: 0.478067]\n",
      "epoch:19 step:18030 [D loss: 0.230212, acc.: 63.28%] [G loss: 0.433019]\n",
      "epoch:19 step:18031 [D loss: 0.233899, acc.: 59.38%] [G loss: 0.431466]\n",
      "epoch:19 step:18032 [D loss: 0.207763, acc.: 61.72%] [G loss: 0.477101]\n",
      "epoch:19 step:18033 [D loss: 0.209006, acc.: 64.84%] [G loss: 0.445434]\n",
      "epoch:19 step:18034 [D loss: 0.182630, acc.: 73.44%] [G loss: 0.536863]\n",
      "epoch:19 step:18035 [D loss: 0.159425, acc.: 77.34%] [G loss: 0.518844]\n",
      "epoch:19 step:18036 [D loss: 0.268241, acc.: 56.25%] [G loss: 0.443590]\n",
      "epoch:19 step:18037 [D loss: 0.258879, acc.: 50.78%] [G loss: 0.433975]\n",
      "epoch:19 step:18038 [D loss: 0.221007, acc.: 63.28%] [G loss: 0.429462]\n",
      "epoch:19 step:18039 [D loss: 0.222828, acc.: 62.50%] [G loss: 0.417440]\n",
      "epoch:19 step:18040 [D loss: 0.220197, acc.: 64.06%] [G loss: 0.438782]\n",
      "epoch:19 step:18041 [D loss: 0.220899, acc.: 64.84%] [G loss: 0.445329]\n",
      "epoch:19 step:18042 [D loss: 0.213664, acc.: 60.94%] [G loss: 0.420181]\n",
      "epoch:19 step:18043 [D loss: 0.231516, acc.: 59.38%] [G loss: 0.437254]\n",
      "epoch:19 step:18044 [D loss: 0.207449, acc.: 69.53%] [G loss: 0.448840]\n",
      "epoch:19 step:18045 [D loss: 0.198337, acc.: 71.88%] [G loss: 0.492439]\n",
      "epoch:19 step:18046 [D loss: 0.234652, acc.: 57.03%] [G loss: 0.464526]\n",
      "epoch:19 step:18047 [D loss: 0.217351, acc.: 66.41%] [G loss: 0.476937]\n",
      "epoch:19 step:18048 [D loss: 0.211751, acc.: 69.53%] [G loss: 0.437868]\n",
      "epoch:19 step:18049 [D loss: 0.194452, acc.: 69.53%] [G loss: 0.446627]\n",
      "epoch:19 step:18050 [D loss: 0.222689, acc.: 57.03%] [G loss: 0.451382]\n",
      "epoch:19 step:18051 [D loss: 0.202308, acc.: 71.88%] [G loss: 0.466086]\n",
      "epoch:19 step:18052 [D loss: 0.269741, acc.: 54.69%] [G loss: 0.443632]\n",
      "epoch:19 step:18053 [D loss: 0.261960, acc.: 51.56%] [G loss: 0.427615]\n",
      "epoch:19 step:18054 [D loss: 0.262334, acc.: 50.78%] [G loss: 0.412415]\n",
      "epoch:19 step:18055 [D loss: 0.230155, acc.: 60.16%] [G loss: 0.465122]\n",
      "epoch:19 step:18056 [D loss: 0.252435, acc.: 60.94%] [G loss: 0.457842]\n",
      "epoch:19 step:18057 [D loss: 0.231446, acc.: 64.84%] [G loss: 0.462858]\n",
      "epoch:19 step:18058 [D loss: 0.244101, acc.: 57.03%] [G loss: 0.423684]\n",
      "epoch:19 step:18059 [D loss: 0.212766, acc.: 64.84%] [G loss: 0.415074]\n",
      "epoch:19 step:18060 [D loss: 0.228968, acc.: 60.94%] [G loss: 0.418860]\n",
      "epoch:19 step:18061 [D loss: 0.200369, acc.: 67.97%] [G loss: 0.416637]\n",
      "epoch:19 step:18062 [D loss: 0.209050, acc.: 67.19%] [G loss: 0.436965]\n",
      "epoch:19 step:18063 [D loss: 0.237434, acc.: 58.59%] [G loss: 0.445735]\n",
      "epoch:19 step:18064 [D loss: 0.210782, acc.: 64.84%] [G loss: 0.439800]\n",
      "epoch:19 step:18065 [D loss: 0.205895, acc.: 67.19%] [G loss: 0.490818]\n",
      "epoch:19 step:18066 [D loss: 0.251261, acc.: 57.03%] [G loss: 0.457125]\n",
      "epoch:19 step:18067 [D loss: 0.207632, acc.: 66.41%] [G loss: 0.478625]\n",
      "epoch:19 step:18068 [D loss: 0.215093, acc.: 65.62%] [G loss: 0.474940]\n",
      "epoch:19 step:18069 [D loss: 0.255616, acc.: 60.16%] [G loss: 0.372856]\n",
      "epoch:19 step:18070 [D loss: 0.228193, acc.: 64.06%] [G loss: 0.417548]\n",
      "epoch:19 step:18071 [D loss: 0.224221, acc.: 61.72%] [G loss: 0.434755]\n",
      "epoch:19 step:18072 [D loss: 0.220661, acc.: 64.06%] [G loss: 0.438926]\n",
      "epoch:19 step:18073 [D loss: 0.214747, acc.: 67.19%] [G loss: 0.443452]\n",
      "epoch:19 step:18074 [D loss: 0.184707, acc.: 73.44%] [G loss: 0.478772]\n",
      "epoch:19 step:18075 [D loss: 0.244851, acc.: 59.38%] [G loss: 0.444087]\n",
      "epoch:19 step:18076 [D loss: 0.229938, acc.: 61.72%] [G loss: 0.426697]\n",
      "epoch:19 step:18077 [D loss: 0.183146, acc.: 69.53%] [G loss: 0.447218]\n",
      "epoch:19 step:18078 [D loss: 0.218874, acc.: 64.84%] [G loss: 0.447425]\n",
      "epoch:19 step:18079 [D loss: 0.205290, acc.: 60.94%] [G loss: 0.454001]\n",
      "epoch:19 step:18080 [D loss: 0.233114, acc.: 58.59%] [G loss: 0.439327]\n",
      "epoch:19 step:18081 [D loss: 0.238094, acc.: 60.16%] [G loss: 0.414306]\n",
      "epoch:19 step:18082 [D loss: 0.236099, acc.: 60.94%] [G loss: 0.449650]\n",
      "epoch:19 step:18083 [D loss: 0.198618, acc.: 68.75%] [G loss: 0.461593]\n",
      "epoch:19 step:18084 [D loss: 0.259117, acc.: 61.72%] [G loss: 0.401795]\n",
      "epoch:19 step:18085 [D loss: 0.238486, acc.: 55.47%] [G loss: 0.435851]\n",
      "epoch:19 step:18086 [D loss: 0.228420, acc.: 60.16%] [G loss: 0.414152]\n",
      "epoch:19 step:18087 [D loss: 0.248492, acc.: 53.12%] [G loss: 0.399411]\n",
      "epoch:19 step:18088 [D loss: 0.239841, acc.: 61.72%] [G loss: 0.400356]\n",
      "epoch:19 step:18089 [D loss: 0.202452, acc.: 67.97%] [G loss: 0.435771]\n",
      "epoch:19 step:18090 [D loss: 0.240412, acc.: 54.69%] [G loss: 0.409192]\n",
      "epoch:19 step:18091 [D loss: 0.195833, acc.: 70.31%] [G loss: 0.460578]\n",
      "epoch:19 step:18092 [D loss: 0.209118, acc.: 68.75%] [G loss: 0.443165]\n",
      "epoch:19 step:18093 [D loss: 0.231839, acc.: 61.72%] [G loss: 0.446014]\n",
      "epoch:19 step:18094 [D loss: 0.267214, acc.: 57.03%] [G loss: 0.431589]\n",
      "epoch:19 step:18095 [D loss: 0.226831, acc.: 64.06%] [G loss: 0.438791]\n",
      "epoch:19 step:18096 [D loss: 0.221084, acc.: 64.84%] [G loss: 0.435507]\n",
      "epoch:19 step:18097 [D loss: 0.247839, acc.: 55.47%] [G loss: 0.448769]\n",
      "epoch:19 step:18098 [D loss: 0.230535, acc.: 57.81%] [G loss: 0.418347]\n",
      "epoch:19 step:18099 [D loss: 0.216066, acc.: 64.84%] [G loss: 0.413589]\n",
      "epoch:19 step:18100 [D loss: 0.237671, acc.: 63.28%] [G loss: 0.418404]\n",
      "epoch:19 step:18101 [D loss: 0.198664, acc.: 67.19%] [G loss: 0.483517]\n",
      "epoch:19 step:18102 [D loss: 0.186767, acc.: 71.88%] [G loss: 0.413309]\n",
      "epoch:19 step:18103 [D loss: 0.211358, acc.: 67.19%] [G loss: 0.478720]\n",
      "epoch:19 step:18104 [D loss: 0.259739, acc.: 55.47%] [G loss: 0.470396]\n",
      "epoch:19 step:18105 [D loss: 0.228228, acc.: 61.72%] [G loss: 0.426513]\n",
      "epoch:19 step:18106 [D loss: 0.244546, acc.: 63.28%] [G loss: 0.452024]\n",
      "epoch:19 step:18107 [D loss: 0.225854, acc.: 66.41%] [G loss: 0.420792]\n",
      "epoch:19 step:18108 [D loss: 0.220819, acc.: 63.28%] [G loss: 0.435811]\n",
      "epoch:19 step:18109 [D loss: 0.225936, acc.: 56.25%] [G loss: 0.403319]\n",
      "epoch:19 step:18110 [D loss: 0.250603, acc.: 53.12%] [G loss: 0.443164]\n",
      "epoch:19 step:18111 [D loss: 0.238183, acc.: 60.16%] [G loss: 0.429890]\n",
      "epoch:19 step:18112 [D loss: 0.197426, acc.: 68.75%] [G loss: 0.449618]\n",
      "epoch:19 step:18113 [D loss: 0.219398, acc.: 64.84%] [G loss: 0.403213]\n",
      "epoch:19 step:18114 [D loss: 0.192871, acc.: 71.09%] [G loss: 0.453744]\n",
      "epoch:19 step:18115 [D loss: 0.178123, acc.: 74.22%] [G loss: 0.484215]\n",
      "epoch:19 step:18116 [D loss: 0.176077, acc.: 74.22%] [G loss: 0.510061]\n",
      "epoch:19 step:18117 [D loss: 0.207033, acc.: 69.53%] [G loss: 0.487624]\n",
      "epoch:19 step:18118 [D loss: 0.182266, acc.: 73.44%] [G loss: 0.474346]\n",
      "epoch:19 step:18119 [D loss: 0.251180, acc.: 57.81%] [G loss: 0.416742]\n",
      "epoch:19 step:18120 [D loss: 0.244517, acc.: 53.91%] [G loss: 0.411002]\n",
      "epoch:19 step:18121 [D loss: 0.203299, acc.: 67.19%] [G loss: 0.418012]\n",
      "epoch:19 step:18122 [D loss: 0.220201, acc.: 64.06%] [G loss: 0.427049]\n",
      "epoch:19 step:18123 [D loss: 0.229292, acc.: 60.16%] [G loss: 0.388150]\n",
      "epoch:19 step:18124 [D loss: 0.215098, acc.: 65.62%] [G loss: 0.430753]\n",
      "epoch:19 step:18125 [D loss: 0.234885, acc.: 62.50%] [G loss: 0.428714]\n",
      "epoch:19 step:18126 [D loss: 0.241163, acc.: 57.81%] [G loss: 0.439348]\n",
      "epoch:19 step:18127 [D loss: 0.197371, acc.: 69.53%] [G loss: 0.427559]\n",
      "epoch:19 step:18128 [D loss: 0.228781, acc.: 61.72%] [G loss: 0.426840]\n",
      "epoch:19 step:18129 [D loss: 0.235304, acc.: 60.16%] [G loss: 0.394448]\n",
      "epoch:19 step:18130 [D loss: 0.232640, acc.: 58.59%] [G loss: 0.369011]\n",
      "epoch:19 step:18131 [D loss: 0.227840, acc.: 60.94%] [G loss: 0.438622]\n",
      "epoch:19 step:18132 [D loss: 0.222806, acc.: 60.16%] [G loss: 0.464433]\n",
      "epoch:19 step:18133 [D loss: 0.205356, acc.: 67.19%] [G loss: 0.451475]\n",
      "epoch:19 step:18134 [D loss: 0.217170, acc.: 65.62%] [G loss: 0.434909]\n",
      "epoch:19 step:18135 [D loss: 0.209826, acc.: 63.28%] [G loss: 0.462508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18136 [D loss: 0.233219, acc.: 60.16%] [G loss: 0.428549]\n",
      "epoch:19 step:18137 [D loss: 0.223020, acc.: 64.06%] [G loss: 0.453839]\n",
      "epoch:19 step:18138 [D loss: 0.229462, acc.: 56.25%] [G loss: 0.478151]\n",
      "epoch:19 step:18139 [D loss: 0.200330, acc.: 71.88%] [G loss: 0.506362]\n",
      "epoch:19 step:18140 [D loss: 0.222068, acc.: 62.50%] [G loss: 0.464017]\n",
      "epoch:19 step:18141 [D loss: 0.218417, acc.: 63.28%] [G loss: 0.426431]\n",
      "epoch:19 step:18142 [D loss: 0.216323, acc.: 65.62%] [G loss: 0.441373]\n",
      "epoch:19 step:18143 [D loss: 0.212128, acc.: 62.50%] [G loss: 0.476259]\n",
      "epoch:19 step:18144 [D loss: 0.278065, acc.: 53.91%] [G loss: 0.433335]\n",
      "epoch:19 step:18145 [D loss: 0.236536, acc.: 58.59%] [G loss: 0.421420]\n",
      "epoch:19 step:18146 [D loss: 0.214028, acc.: 70.31%] [G loss: 0.466006]\n",
      "epoch:19 step:18147 [D loss: 0.213220, acc.: 65.62%] [G loss: 0.458772]\n",
      "epoch:19 step:18148 [D loss: 0.229124, acc.: 60.16%] [G loss: 0.428134]\n",
      "epoch:19 step:18149 [D loss: 0.189635, acc.: 67.97%] [G loss: 0.488757]\n",
      "epoch:19 step:18150 [D loss: 0.168871, acc.: 75.78%] [G loss: 0.533018]\n",
      "epoch:19 step:18151 [D loss: 0.282607, acc.: 53.91%] [G loss: 0.451466]\n",
      "epoch:19 step:18152 [D loss: 0.289456, acc.: 48.44%] [G loss: 0.380775]\n",
      "epoch:19 step:18153 [D loss: 0.219113, acc.: 66.41%] [G loss: 0.424058]\n",
      "epoch:19 step:18154 [D loss: 0.233797, acc.: 63.28%] [G loss: 0.408996]\n",
      "epoch:19 step:18155 [D loss: 0.229617, acc.: 59.38%] [G loss: 0.392411]\n",
      "epoch:19 step:18156 [D loss: 0.209011, acc.: 64.84%] [G loss: 0.446519]\n",
      "epoch:19 step:18157 [D loss: 0.187776, acc.: 68.75%] [G loss: 0.460519]\n",
      "epoch:19 step:18158 [D loss: 0.253291, acc.: 57.81%] [G loss: 0.441795]\n",
      "epoch:19 step:18159 [D loss: 0.232320, acc.: 56.25%] [G loss: 0.452915]\n",
      "epoch:19 step:18160 [D loss: 0.210026, acc.: 64.06%] [G loss: 0.449687]\n",
      "epoch:19 step:18161 [D loss: 0.206890, acc.: 70.31%] [G loss: 0.446963]\n",
      "epoch:19 step:18162 [D loss: 0.204240, acc.: 72.66%] [G loss: 0.433489]\n",
      "epoch:19 step:18163 [D loss: 0.199635, acc.: 64.84%] [G loss: 0.450840]\n",
      "epoch:19 step:18164 [D loss: 0.185499, acc.: 71.88%] [G loss: 0.414256]\n",
      "epoch:19 step:18165 [D loss: 0.228008, acc.: 67.97%] [G loss: 0.408265]\n",
      "epoch:19 step:18166 [D loss: 0.215840, acc.: 67.97%] [G loss: 0.452519]\n",
      "epoch:19 step:18167 [D loss: 0.189348, acc.: 72.66%] [G loss: 0.452990]\n",
      "epoch:19 step:18168 [D loss: 0.231299, acc.: 64.84%] [G loss: 0.424071]\n",
      "epoch:19 step:18169 [D loss: 0.221297, acc.: 64.84%] [G loss: 0.428085]\n",
      "epoch:19 step:18170 [D loss: 0.201772, acc.: 69.53%] [G loss: 0.459004]\n",
      "epoch:19 step:18171 [D loss: 0.252033, acc.: 56.25%] [G loss: 0.415408]\n",
      "epoch:19 step:18172 [D loss: 0.267297, acc.: 51.56%] [G loss: 0.410967]\n",
      "epoch:19 step:18173 [D loss: 0.221990, acc.: 68.75%] [G loss: 0.433844]\n",
      "epoch:19 step:18174 [D loss: 0.203828, acc.: 63.28%] [G loss: 0.406753]\n",
      "epoch:19 step:18175 [D loss: 0.212356, acc.: 64.84%] [G loss: 0.473133]\n",
      "epoch:19 step:18176 [D loss: 0.231316, acc.: 60.16%] [G loss: 0.468830]\n",
      "epoch:19 step:18177 [D loss: 0.199071, acc.: 71.09%] [G loss: 0.427887]\n",
      "epoch:19 step:18178 [D loss: 0.232511, acc.: 60.94%] [G loss: 0.436398]\n",
      "epoch:19 step:18179 [D loss: 0.244310, acc.: 53.91%] [G loss: 0.461665]\n",
      "epoch:19 step:18180 [D loss: 0.248170, acc.: 58.59%] [G loss: 0.428499]\n",
      "epoch:19 step:18181 [D loss: 0.224535, acc.: 61.72%] [G loss: 0.426491]\n",
      "epoch:19 step:18182 [D loss: 0.229967, acc.: 58.59%] [G loss: 0.439532]\n",
      "epoch:19 step:18183 [D loss: 0.248986, acc.: 58.59%] [G loss: 0.401470]\n",
      "epoch:19 step:18184 [D loss: 0.205847, acc.: 64.84%] [G loss: 0.449278]\n",
      "epoch:19 step:18185 [D loss: 0.229630, acc.: 60.16%] [G loss: 0.425344]\n",
      "epoch:19 step:18186 [D loss: 0.225971, acc.: 65.62%] [G loss: 0.432168]\n",
      "epoch:19 step:18187 [D loss: 0.198519, acc.: 67.97%] [G loss: 0.455427]\n",
      "epoch:19 step:18188 [D loss: 0.180667, acc.: 75.00%] [G loss: 0.496005]\n",
      "epoch:19 step:18189 [D loss: 0.235138, acc.: 56.25%] [G loss: 0.420701]\n",
      "epoch:19 step:18190 [D loss: 0.234833, acc.: 60.16%] [G loss: 0.437741]\n",
      "epoch:19 step:18191 [D loss: 0.199430, acc.: 68.75%] [G loss: 0.458409]\n",
      "epoch:19 step:18192 [D loss: 0.232744, acc.: 63.28%] [G loss: 0.454875]\n",
      "epoch:19 step:18193 [D loss: 0.237659, acc.: 58.59%] [G loss: 0.425373]\n",
      "epoch:19 step:18194 [D loss: 0.219376, acc.: 65.62%] [G loss: 0.378229]\n",
      "epoch:19 step:18195 [D loss: 0.207470, acc.: 71.09%] [G loss: 0.438209]\n",
      "epoch:19 step:18196 [D loss: 0.230304, acc.: 61.72%] [G loss: 0.443983]\n",
      "epoch:19 step:18197 [D loss: 0.236169, acc.: 66.41%] [G loss: 0.439759]\n",
      "epoch:19 step:18198 [D loss: 0.202186, acc.: 68.75%] [G loss: 0.475751]\n",
      "epoch:19 step:18199 [D loss: 0.253414, acc.: 55.47%] [G loss: 0.446770]\n",
      "epoch:19 step:18200 [D loss: 0.231576, acc.: 63.28%] [G loss: 0.407097]\n",
      "##############\n",
      "[2.66193855 1.86205811 5.93868731 4.58692414 3.49604958 5.74066094\n",
      " 4.36942234 4.61696569 4.55301717 3.94018994]\n",
      "##########\n",
      "epoch:19 step:18201 [D loss: 0.190033, acc.: 71.88%] [G loss: 0.461318]\n",
      "epoch:19 step:18202 [D loss: 0.193631, acc.: 70.31%] [G loss: 0.497708]\n",
      "epoch:19 step:18203 [D loss: 0.244151, acc.: 57.03%] [G loss: 0.419508]\n",
      "epoch:19 step:18204 [D loss: 0.243446, acc.: 55.47%] [G loss: 0.394800]\n",
      "epoch:19 step:18205 [D loss: 0.257066, acc.: 62.50%] [G loss: 0.411145]\n",
      "epoch:19 step:18206 [D loss: 0.220741, acc.: 65.62%] [G loss: 0.463177]\n",
      "epoch:19 step:18207 [D loss: 0.243187, acc.: 52.34%] [G loss: 0.427512]\n",
      "epoch:19 step:18208 [D loss: 0.218169, acc.: 64.06%] [G loss: 0.438220]\n",
      "epoch:19 step:18209 [D loss: 0.205134, acc.: 68.75%] [G loss: 0.463009]\n",
      "epoch:19 step:18210 [D loss: 0.227073, acc.: 63.28%] [G loss: 0.497898]\n",
      "epoch:19 step:18211 [D loss: 0.237760, acc.: 63.28%] [G loss: 0.412501]\n",
      "epoch:19 step:18212 [D loss: 0.223691, acc.: 63.28%] [G loss: 0.417945]\n",
      "epoch:19 step:18213 [D loss: 0.237290, acc.: 62.50%] [G loss: 0.442296]\n",
      "epoch:19 step:18214 [D loss: 0.234590, acc.: 63.28%] [G loss: 0.426438]\n",
      "epoch:19 step:18215 [D loss: 0.240623, acc.: 65.62%] [G loss: 0.440984]\n",
      "epoch:19 step:18216 [D loss: 0.236056, acc.: 61.72%] [G loss: 0.447261]\n",
      "epoch:19 step:18217 [D loss: 0.215469, acc.: 69.53%] [G loss: 0.431192]\n",
      "epoch:19 step:18218 [D loss: 0.206455, acc.: 68.75%] [G loss: 0.470257]\n",
      "epoch:19 step:18219 [D loss: 0.201218, acc.: 71.09%] [G loss: 0.416369]\n",
      "epoch:19 step:18220 [D loss: 0.243429, acc.: 61.72%] [G loss: 0.410871]\n",
      "epoch:19 step:18221 [D loss: 0.258186, acc.: 51.56%] [G loss: 0.416695]\n",
      "epoch:19 step:18222 [D loss: 0.233779, acc.: 58.59%] [G loss: 0.433400]\n",
      "epoch:19 step:18223 [D loss: 0.238782, acc.: 60.16%] [G loss: 0.423666]\n",
      "epoch:19 step:18224 [D loss: 0.253520, acc.: 60.16%] [G loss: 0.422559]\n",
      "epoch:19 step:18225 [D loss: 0.223966, acc.: 61.72%] [G loss: 0.421131]\n",
      "epoch:19 step:18226 [D loss: 0.221223, acc.: 64.06%] [G loss: 0.395952]\n",
      "epoch:19 step:18227 [D loss: 0.247573, acc.: 54.69%] [G loss: 0.395509]\n",
      "epoch:19 step:18228 [D loss: 0.223483, acc.: 59.38%] [G loss: 0.417138]\n",
      "epoch:19 step:18229 [D loss: 0.218831, acc.: 66.41%] [G loss: 0.473232]\n",
      "epoch:19 step:18230 [D loss: 0.198045, acc.: 71.09%] [G loss: 0.434308]\n",
      "epoch:19 step:18231 [D loss: 0.220509, acc.: 59.38%] [G loss: 0.466769]\n",
      "epoch:19 step:18232 [D loss: 0.210366, acc.: 68.75%] [G loss: 0.453070]\n",
      "epoch:19 step:18233 [D loss: 0.209755, acc.: 66.41%] [G loss: 0.457997]\n",
      "epoch:19 step:18234 [D loss: 0.235515, acc.: 61.72%] [G loss: 0.416385]\n",
      "epoch:19 step:18235 [D loss: 0.242406, acc.: 56.25%] [G loss: 0.433021]\n",
      "epoch:19 step:18236 [D loss: 0.226069, acc.: 64.06%] [G loss: 0.429414]\n",
      "epoch:19 step:18237 [D loss: 0.209569, acc.: 68.75%] [G loss: 0.431390]\n",
      "epoch:19 step:18238 [D loss: 0.199520, acc.: 68.75%] [G loss: 0.406592]\n",
      "epoch:19 step:18239 [D loss: 0.213781, acc.: 71.09%] [G loss: 0.435923]\n",
      "epoch:19 step:18240 [D loss: 0.265263, acc.: 49.22%] [G loss: 0.456317]\n",
      "epoch:19 step:18241 [D loss: 0.211541, acc.: 64.84%] [G loss: 0.455673]\n",
      "epoch:19 step:18242 [D loss: 0.210857, acc.: 68.75%] [G loss: 0.435791]\n",
      "epoch:19 step:18243 [D loss: 0.203089, acc.: 69.53%] [G loss: 0.390622]\n",
      "epoch:19 step:18244 [D loss: 0.237930, acc.: 64.84%] [G loss: 0.407194]\n",
      "epoch:19 step:18245 [D loss: 0.237519, acc.: 62.50%] [G loss: 0.431578]\n",
      "epoch:19 step:18246 [D loss: 0.230064, acc.: 60.16%] [G loss: 0.431963]\n",
      "epoch:19 step:18247 [D loss: 0.220466, acc.: 66.41%] [G loss: 0.432206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18248 [D loss: 0.216760, acc.: 67.19%] [G loss: 0.457749]\n",
      "epoch:19 step:18249 [D loss: 0.225718, acc.: 64.84%] [G loss: 0.445471]\n",
      "epoch:19 step:18250 [D loss: 0.222490, acc.: 64.06%] [G loss: 0.441669]\n",
      "epoch:19 step:18251 [D loss: 0.224930, acc.: 64.06%] [G loss: 0.452716]\n",
      "epoch:19 step:18252 [D loss: 0.230356, acc.: 59.38%] [G loss: 0.438389]\n",
      "epoch:19 step:18253 [D loss: 0.223374, acc.: 61.72%] [G loss: 0.443895]\n",
      "epoch:19 step:18254 [D loss: 0.186557, acc.: 70.31%] [G loss: 0.429477]\n",
      "epoch:19 step:18255 [D loss: 0.215835, acc.: 62.50%] [G loss: 0.434069]\n",
      "epoch:19 step:18256 [D loss: 0.205995, acc.: 71.88%] [G loss: 0.452518]\n",
      "epoch:19 step:18257 [D loss: 0.221464, acc.: 64.84%] [G loss: 0.460165]\n",
      "epoch:19 step:18258 [D loss: 0.245745, acc.: 60.16%] [G loss: 0.471580]\n",
      "epoch:19 step:18259 [D loss: 0.249411, acc.: 57.03%] [G loss: 0.438489]\n",
      "epoch:19 step:18260 [D loss: 0.227593, acc.: 58.59%] [G loss: 0.428907]\n",
      "epoch:19 step:18261 [D loss: 0.290205, acc.: 53.12%] [G loss: 0.418241]\n",
      "epoch:19 step:18262 [D loss: 0.242010, acc.: 60.16%] [G loss: 0.420602]\n",
      "epoch:19 step:18263 [D loss: 0.251778, acc.: 57.03%] [G loss: 0.388172]\n",
      "epoch:19 step:18264 [D loss: 0.214254, acc.: 67.97%] [G loss: 0.419318]\n",
      "epoch:19 step:18265 [D loss: 0.241226, acc.: 54.69%] [G loss: 0.384526]\n",
      "epoch:19 step:18266 [D loss: 0.225787, acc.: 60.16%] [G loss: 0.404504]\n",
      "epoch:19 step:18267 [D loss: 0.220305, acc.: 60.16%] [G loss: 0.408874]\n",
      "epoch:19 step:18268 [D loss: 0.247336, acc.: 55.47%] [G loss: 0.452213]\n",
      "epoch:19 step:18269 [D loss: 0.214453, acc.: 65.62%] [G loss: 0.406235]\n",
      "epoch:19 step:18270 [D loss: 0.217505, acc.: 61.72%] [G loss: 0.433542]\n",
      "epoch:19 step:18271 [D loss: 0.222328, acc.: 64.84%] [G loss: 0.395688]\n",
      "epoch:19 step:18272 [D loss: 0.214291, acc.: 64.06%] [G loss: 0.427091]\n",
      "epoch:19 step:18273 [D loss: 0.219737, acc.: 64.06%] [G loss: 0.460085]\n",
      "epoch:19 step:18274 [D loss: 0.200144, acc.: 71.09%] [G loss: 0.483759]\n",
      "epoch:19 step:18275 [D loss: 0.209201, acc.: 63.28%] [G loss: 0.481684]\n",
      "epoch:19 step:18276 [D loss: 0.250475, acc.: 55.47%] [G loss: 0.459510]\n",
      "epoch:19 step:18277 [D loss: 0.211922, acc.: 70.31%] [G loss: 0.454100]\n",
      "epoch:19 step:18278 [D loss: 0.202281, acc.: 72.66%] [G loss: 0.453507]\n",
      "epoch:19 step:18279 [D loss: 0.232099, acc.: 66.41%] [G loss: 0.448239]\n",
      "epoch:19 step:18280 [D loss: 0.256688, acc.: 47.66%] [G loss: 0.416683]\n",
      "epoch:19 step:18281 [D loss: 0.235231, acc.: 61.72%] [G loss: 0.388708]\n",
      "epoch:19 step:18282 [D loss: 0.230080, acc.: 60.16%] [G loss: 0.427505]\n",
      "epoch:19 step:18283 [D loss: 0.236007, acc.: 57.81%] [G loss: 0.406486]\n",
      "epoch:19 step:18284 [D loss: 0.198895, acc.: 65.62%] [G loss: 0.452454]\n",
      "epoch:19 step:18285 [D loss: 0.260494, acc.: 53.91%] [G loss: 0.440671]\n",
      "epoch:19 step:18286 [D loss: 0.225271, acc.: 62.50%] [G loss: 0.429023]\n",
      "epoch:19 step:18287 [D loss: 0.180385, acc.: 71.09%] [G loss: 0.450092]\n",
      "epoch:19 step:18288 [D loss: 0.227589, acc.: 65.62%] [G loss: 0.433684]\n",
      "epoch:19 step:18289 [D loss: 0.261637, acc.: 54.69%] [G loss: 0.432451]\n",
      "epoch:19 step:18290 [D loss: 0.256283, acc.: 57.03%] [G loss: 0.408070]\n",
      "epoch:19 step:18291 [D loss: 0.185828, acc.: 74.22%] [G loss: 0.437569]\n",
      "epoch:19 step:18292 [D loss: 0.240339, acc.: 60.16%] [G loss: 0.403294]\n",
      "epoch:19 step:18293 [D loss: 0.237132, acc.: 58.59%] [G loss: 0.410927]\n",
      "epoch:19 step:18294 [D loss: 0.225178, acc.: 66.41%] [G loss: 0.477907]\n",
      "epoch:19 step:18295 [D loss: 0.219104, acc.: 65.62%] [G loss: 0.447089]\n",
      "epoch:19 step:18296 [D loss: 0.210629, acc.: 69.53%] [G loss: 0.412385]\n",
      "epoch:19 step:18297 [D loss: 0.218482, acc.: 67.97%] [G loss: 0.409963]\n",
      "epoch:19 step:18298 [D loss: 0.204118, acc.: 65.62%] [G loss: 0.470395]\n",
      "epoch:19 step:18299 [D loss: 0.212335, acc.: 68.75%] [G loss: 0.436591]\n",
      "epoch:19 step:18300 [D loss: 0.208202, acc.: 68.75%] [G loss: 0.433222]\n",
      "epoch:19 step:18301 [D loss: 0.213130, acc.: 62.50%] [G loss: 0.440829]\n",
      "epoch:19 step:18302 [D loss: 0.177821, acc.: 75.78%] [G loss: 0.478465]\n",
      "epoch:19 step:18303 [D loss: 0.236223, acc.: 57.81%] [G loss: 0.455081]\n",
      "epoch:19 step:18304 [D loss: 0.284691, acc.: 50.78%] [G loss: 0.428959]\n",
      "epoch:19 step:18305 [D loss: 0.235847, acc.: 57.03%] [G loss: 0.378761]\n",
      "epoch:19 step:18306 [D loss: 0.239882, acc.: 59.38%] [G loss: 0.401782]\n",
      "epoch:19 step:18307 [D loss: 0.213702, acc.: 67.97%] [G loss: 0.440794]\n",
      "epoch:19 step:18308 [D loss: 0.226327, acc.: 62.50%] [G loss: 0.447455]\n",
      "epoch:19 step:18309 [D loss: 0.226794, acc.: 62.50%] [G loss: 0.491885]\n",
      "epoch:19 step:18310 [D loss: 0.195167, acc.: 71.09%] [G loss: 0.480950]\n",
      "epoch:19 step:18311 [D loss: 0.190523, acc.: 68.75%] [G loss: 0.483892]\n",
      "epoch:19 step:18312 [D loss: 0.255540, acc.: 62.50%] [G loss: 0.417986]\n",
      "epoch:19 step:18313 [D loss: 0.226826, acc.: 69.53%] [G loss: 0.434040]\n",
      "epoch:19 step:18314 [D loss: 0.227277, acc.: 60.94%] [G loss: 0.440420]\n",
      "epoch:19 step:18315 [D loss: 0.215944, acc.: 60.94%] [G loss: 0.428477]\n",
      "epoch:19 step:18316 [D loss: 0.233583, acc.: 60.16%] [G loss: 0.384165]\n",
      "epoch:19 step:18317 [D loss: 0.218057, acc.: 63.28%] [G loss: 0.423942]\n",
      "epoch:19 step:18318 [D loss: 0.208180, acc.: 68.75%] [G loss: 0.459807]\n",
      "epoch:19 step:18319 [D loss: 0.206286, acc.: 71.09%] [G loss: 0.458905]\n",
      "epoch:19 step:18320 [D loss: 0.257436, acc.: 57.03%] [G loss: 0.446268]\n",
      "epoch:19 step:18321 [D loss: 0.228593, acc.: 60.16%] [G loss: 0.435792]\n",
      "epoch:19 step:18322 [D loss: 0.204629, acc.: 70.31%] [G loss: 0.400630]\n",
      "epoch:19 step:18323 [D loss: 0.213563, acc.: 64.84%] [G loss: 0.462345]\n",
      "epoch:19 step:18324 [D loss: 0.212714, acc.: 65.62%] [G loss: 0.411303]\n",
      "epoch:19 step:18325 [D loss: 0.220867, acc.: 63.28%] [G loss: 0.431671]\n",
      "epoch:19 step:18326 [D loss: 0.204465, acc.: 64.84%] [G loss: 0.437699]\n",
      "epoch:19 step:18327 [D loss: 0.255270, acc.: 52.34%] [G loss: 0.396518]\n",
      "epoch:19 step:18328 [D loss: 0.239898, acc.: 59.38%] [G loss: 0.465282]\n",
      "epoch:19 step:18329 [D loss: 0.220340, acc.: 64.84%] [G loss: 0.464556]\n",
      "epoch:19 step:18330 [D loss: 0.224689, acc.: 64.06%] [G loss: 0.444660]\n",
      "epoch:19 step:18331 [D loss: 0.307315, acc.: 43.75%] [G loss: 0.397241]\n",
      "epoch:19 step:18332 [D loss: 0.232394, acc.: 60.94%] [G loss: 0.427201]\n",
      "epoch:19 step:18333 [D loss: 0.230308, acc.: 60.16%] [G loss: 0.424105]\n",
      "epoch:19 step:18334 [D loss: 0.238596, acc.: 57.03%] [G loss: 0.429857]\n",
      "epoch:19 step:18335 [D loss: 0.233417, acc.: 61.72%] [G loss: 0.424699]\n",
      "epoch:19 step:18336 [D loss: 0.219724, acc.: 64.06%] [G loss: 0.446826]\n",
      "epoch:19 step:18337 [D loss: 0.192475, acc.: 71.88%] [G loss: 0.426064]\n",
      "epoch:19 step:18338 [D loss: 0.254151, acc.: 58.59%] [G loss: 0.389795]\n",
      "epoch:19 step:18339 [D loss: 0.212747, acc.: 64.06%] [G loss: 0.397534]\n",
      "epoch:19 step:18340 [D loss: 0.234947, acc.: 51.56%] [G loss: 0.444239]\n",
      "epoch:19 step:18341 [D loss: 0.256159, acc.: 57.03%] [G loss: 0.413716]\n",
      "epoch:19 step:18342 [D loss: 0.225785, acc.: 60.16%] [G loss: 0.424376]\n",
      "epoch:19 step:18343 [D loss: 0.232189, acc.: 63.28%] [G loss: 0.398157]\n",
      "epoch:19 step:18344 [D loss: 0.219279, acc.: 67.19%] [G loss: 0.408584]\n",
      "epoch:19 step:18345 [D loss: 0.283793, acc.: 46.88%] [G loss: 0.373383]\n",
      "epoch:19 step:18346 [D loss: 0.233702, acc.: 62.50%] [G loss: 0.408316]\n",
      "epoch:19 step:18347 [D loss: 0.231000, acc.: 62.50%] [G loss: 0.444719]\n",
      "epoch:19 step:18348 [D loss: 0.244936, acc.: 60.16%] [G loss: 0.417650]\n",
      "epoch:19 step:18349 [D loss: 0.210588, acc.: 71.88%] [G loss: 0.435389]\n",
      "epoch:19 step:18350 [D loss: 0.244193, acc.: 55.47%] [G loss: 0.415822]\n",
      "epoch:19 step:18351 [D loss: 0.212087, acc.: 65.62%] [G loss: 0.420721]\n",
      "epoch:19 step:18352 [D loss: 0.209464, acc.: 64.84%] [G loss: 0.484711]\n",
      "epoch:19 step:18353 [D loss: 0.212246, acc.: 64.06%] [G loss: 0.435926]\n",
      "epoch:19 step:18354 [D loss: 0.217835, acc.: 66.41%] [G loss: 0.466500]\n",
      "epoch:19 step:18355 [D loss: 0.228831, acc.: 61.72%] [G loss: 0.457577]\n",
      "epoch:19 step:18356 [D loss: 0.246036, acc.: 58.59%] [G loss: 0.414333]\n",
      "epoch:19 step:18357 [D loss: 0.205043, acc.: 64.06%] [G loss: 0.416149]\n",
      "epoch:19 step:18358 [D loss: 0.210657, acc.: 67.97%] [G loss: 0.443737]\n",
      "epoch:19 step:18359 [D loss: 0.214277, acc.: 66.41%] [G loss: 0.436559]\n",
      "epoch:19 step:18360 [D loss: 0.221567, acc.: 67.97%] [G loss: 0.469234]\n",
      "epoch:19 step:18361 [D loss: 0.236455, acc.: 62.50%] [G loss: 0.439711]\n",
      "epoch:19 step:18362 [D loss: 0.236446, acc.: 57.81%] [G loss: 0.463447]\n",
      "epoch:19 step:18363 [D loss: 0.247217, acc.: 57.81%] [G loss: 0.435123]\n",
      "epoch:19 step:18364 [D loss: 0.222357, acc.: 60.16%] [G loss: 0.446823]\n",
      "epoch:19 step:18365 [D loss: 0.238161, acc.: 53.12%] [G loss: 0.420263]\n",
      "epoch:19 step:18366 [D loss: 0.213582, acc.: 63.28%] [G loss: 0.445003]\n",
      "epoch:19 step:18367 [D loss: 0.194333, acc.: 71.88%] [G loss: 0.452938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18368 [D loss: 0.217848, acc.: 65.62%] [G loss: 0.458202]\n",
      "epoch:19 step:18369 [D loss: 0.265881, acc.: 55.47%] [G loss: 0.481865]\n",
      "epoch:19 step:18370 [D loss: 0.224314, acc.: 60.94%] [G loss: 0.487272]\n",
      "epoch:19 step:18371 [D loss: 0.203410, acc.: 73.44%] [G loss: 0.474269]\n",
      "epoch:19 step:18372 [D loss: 0.258429, acc.: 48.44%] [G loss: 0.408961]\n",
      "epoch:19 step:18373 [D loss: 0.183523, acc.: 75.00%] [G loss: 0.448452]\n",
      "epoch:19 step:18374 [D loss: 0.199941, acc.: 68.75%] [G loss: 0.409305]\n",
      "epoch:19 step:18375 [D loss: 0.195202, acc.: 69.53%] [G loss: 0.447217]\n",
      "epoch:19 step:18376 [D loss: 0.210133, acc.: 66.41%] [G loss: 0.409153]\n",
      "epoch:19 step:18377 [D loss: 0.173774, acc.: 71.09%] [G loss: 0.466250]\n",
      "epoch:19 step:18378 [D loss: 0.210873, acc.: 60.94%] [G loss: 0.477251]\n",
      "epoch:19 step:18379 [D loss: 0.238979, acc.: 64.06%] [G loss: 0.472214]\n",
      "epoch:19 step:18380 [D loss: 0.245215, acc.: 60.16%] [G loss: 0.436274]\n",
      "epoch:19 step:18381 [D loss: 0.218283, acc.: 64.84%] [G loss: 0.440804]\n",
      "epoch:19 step:18382 [D loss: 0.251564, acc.: 52.34%] [G loss: 0.418354]\n",
      "epoch:19 step:18383 [D loss: 0.224132, acc.: 63.28%] [G loss: 0.455760]\n",
      "epoch:19 step:18384 [D loss: 0.228050, acc.: 61.72%] [G loss: 0.420587]\n",
      "epoch:19 step:18385 [D loss: 0.197041, acc.: 67.97%] [G loss: 0.441883]\n",
      "epoch:19 step:18386 [D loss: 0.246091, acc.: 54.69%] [G loss: 0.429396]\n",
      "epoch:19 step:18387 [D loss: 0.217590, acc.: 61.72%] [G loss: 0.426083]\n",
      "epoch:19 step:18388 [D loss: 0.244786, acc.: 52.34%] [G loss: 0.403065]\n",
      "epoch:19 step:18389 [D loss: 0.257664, acc.: 48.44%] [G loss: 0.409662]\n",
      "epoch:19 step:18390 [D loss: 0.227707, acc.: 61.72%] [G loss: 0.436228]\n",
      "epoch:19 step:18391 [D loss: 0.225262, acc.: 66.41%] [G loss: 0.452644]\n",
      "epoch:19 step:18392 [D loss: 0.213195, acc.: 67.19%] [G loss: 0.417666]\n",
      "epoch:19 step:18393 [D loss: 0.226467, acc.: 64.84%] [G loss: 0.430832]\n",
      "epoch:19 step:18394 [D loss: 0.220858, acc.: 64.84%] [G loss: 0.435874]\n",
      "epoch:19 step:18395 [D loss: 0.218711, acc.: 71.88%] [G loss: 0.423563]\n",
      "epoch:19 step:18396 [D loss: 0.197603, acc.: 69.53%] [G loss: 0.459956]\n",
      "epoch:19 step:18397 [D loss: 0.241944, acc.: 58.59%] [G loss: 0.447681]\n",
      "epoch:19 step:18398 [D loss: 0.201675, acc.: 72.66%] [G loss: 0.425827]\n",
      "epoch:19 step:18399 [D loss: 0.248563, acc.: 60.16%] [G loss: 0.427417]\n",
      "epoch:19 step:18400 [D loss: 0.259074, acc.: 52.34%] [G loss: 0.408329]\n",
      "##############\n",
      "[2.4193288  1.44150443 5.92754021 4.85625639 3.60325249 5.6155405\n",
      " 4.38262916 4.61030981 4.59899118 3.86261558]\n",
      "##########\n",
      "epoch:19 step:18401 [D loss: 0.198815, acc.: 75.00%] [G loss: 0.433117]\n",
      "epoch:19 step:18402 [D loss: 0.219712, acc.: 68.75%] [G loss: 0.412880]\n",
      "epoch:19 step:18403 [D loss: 0.242011, acc.: 58.59%] [G loss: 0.410059]\n",
      "epoch:19 step:18404 [D loss: 0.238821, acc.: 56.25%] [G loss: 0.404065]\n",
      "epoch:19 step:18405 [D loss: 0.229328, acc.: 60.94%] [G loss: 0.415049]\n",
      "epoch:19 step:18406 [D loss: 0.210634, acc.: 63.28%] [G loss: 0.449488]\n",
      "epoch:19 step:18407 [D loss: 0.234876, acc.: 64.06%] [G loss: 0.405700]\n",
      "epoch:19 step:18408 [D loss: 0.217465, acc.: 71.09%] [G loss: 0.409991]\n",
      "epoch:19 step:18409 [D loss: 0.232179, acc.: 65.62%] [G loss: 0.415522]\n",
      "epoch:19 step:18410 [D loss: 0.215750, acc.: 61.72%] [G loss: 0.426284]\n",
      "epoch:19 step:18411 [D loss: 0.239068, acc.: 61.72%] [G loss: 0.412286]\n",
      "epoch:19 step:18412 [D loss: 0.214802, acc.: 62.50%] [G loss: 0.425015]\n",
      "epoch:19 step:18413 [D loss: 0.217508, acc.: 64.06%] [G loss: 0.421921]\n",
      "epoch:19 step:18414 [D loss: 0.235011, acc.: 64.06%] [G loss: 0.420886]\n",
      "epoch:19 step:18415 [D loss: 0.235953, acc.: 60.16%] [G loss: 0.419774]\n",
      "epoch:19 step:18416 [D loss: 0.219736, acc.: 67.97%] [G loss: 0.399908]\n",
      "epoch:19 step:18417 [D loss: 0.256966, acc.: 50.00%] [G loss: 0.428513]\n",
      "epoch:19 step:18418 [D loss: 0.241815, acc.: 56.25%] [G loss: 0.434660]\n",
      "epoch:19 step:18419 [D loss: 0.225223, acc.: 64.06%] [G loss: 0.399054]\n",
      "epoch:19 step:18420 [D loss: 0.225437, acc.: 65.62%] [G loss: 0.420104]\n",
      "epoch:19 step:18421 [D loss: 0.211309, acc.: 67.19%] [G loss: 0.464281]\n",
      "epoch:19 step:18422 [D loss: 0.217083, acc.: 67.97%] [G loss: 0.451576]\n",
      "epoch:19 step:18423 [D loss: 0.211521, acc.: 67.19%] [G loss: 0.442708]\n",
      "epoch:19 step:18424 [D loss: 0.230036, acc.: 57.03%] [G loss: 0.403213]\n",
      "epoch:19 step:18425 [D loss: 0.247011, acc.: 55.47%] [G loss: 0.396193]\n",
      "epoch:19 step:18426 [D loss: 0.211166, acc.: 70.31%] [G loss: 0.413478]\n",
      "epoch:19 step:18427 [D loss: 0.202621, acc.: 67.97%] [G loss: 0.437288]\n",
      "epoch:19 step:18428 [D loss: 0.237193, acc.: 61.72%] [G loss: 0.432251]\n",
      "epoch:19 step:18429 [D loss: 0.230520, acc.: 59.38%] [G loss: 0.405606]\n",
      "epoch:19 step:18430 [D loss: 0.207301, acc.: 68.75%] [G loss: 0.437619]\n",
      "epoch:19 step:18431 [D loss: 0.248038, acc.: 54.69%] [G loss: 0.408661]\n",
      "epoch:19 step:18432 [D loss: 0.202064, acc.: 71.09%] [G loss: 0.422156]\n",
      "epoch:19 step:18433 [D loss: 0.235924, acc.: 60.94%] [G loss: 0.481181]\n",
      "epoch:19 step:18434 [D loss: 0.201272, acc.: 66.41%] [G loss: 0.462102]\n",
      "epoch:19 step:18435 [D loss: 0.214051, acc.: 65.62%] [G loss: 0.417594]\n",
      "epoch:19 step:18436 [D loss: 0.200443, acc.: 74.22%] [G loss: 0.461662]\n",
      "epoch:19 step:18437 [D loss: 0.206146, acc.: 64.84%] [G loss: 0.457085]\n",
      "epoch:19 step:18438 [D loss: 0.212317, acc.: 66.41%] [G loss: 0.456722]\n",
      "epoch:19 step:18439 [D loss: 0.228140, acc.: 57.03%] [G loss: 0.445026]\n",
      "epoch:19 step:18440 [D loss: 0.229866, acc.: 64.06%] [G loss: 0.445520]\n",
      "epoch:19 step:18441 [D loss: 0.215837, acc.: 67.19%] [G loss: 0.422152]\n",
      "epoch:19 step:18442 [D loss: 0.217858, acc.: 60.16%] [G loss: 0.450024]\n",
      "epoch:19 step:18443 [D loss: 0.242806, acc.: 54.69%] [G loss: 0.448284]\n",
      "epoch:19 step:18444 [D loss: 0.217737, acc.: 61.72%] [G loss: 0.478424]\n",
      "epoch:19 step:18445 [D loss: 0.180265, acc.: 71.88%] [G loss: 0.479703]\n",
      "epoch:19 step:18446 [D loss: 0.218477, acc.: 61.72%] [G loss: 0.463675]\n",
      "epoch:19 step:18447 [D loss: 0.217129, acc.: 60.16%] [G loss: 0.439816]\n",
      "epoch:19 step:18448 [D loss: 0.233268, acc.: 56.25%] [G loss: 0.398327]\n",
      "epoch:19 step:18449 [D loss: 0.224037, acc.: 63.28%] [G loss: 0.392858]\n",
      "epoch:19 step:18450 [D loss: 0.217138, acc.: 66.41%] [G loss: 0.484855]\n",
      "epoch:19 step:18451 [D loss: 0.188235, acc.: 73.44%] [G loss: 0.534997]\n",
      "epoch:19 step:18452 [D loss: 0.226368, acc.: 57.81%] [G loss: 0.462972]\n",
      "epoch:19 step:18453 [D loss: 0.180421, acc.: 72.66%] [G loss: 0.493153]\n",
      "epoch:19 step:18454 [D loss: 0.248411, acc.: 53.91%] [G loss: 0.452623]\n",
      "epoch:19 step:18455 [D loss: 0.222446, acc.: 60.16%] [G loss: 0.475443]\n",
      "epoch:19 step:18456 [D loss: 0.217638, acc.: 65.62%] [G loss: 0.447841]\n",
      "epoch:19 step:18457 [D loss: 0.234235, acc.: 58.59%] [G loss: 0.423842]\n",
      "epoch:19 step:18458 [D loss: 0.233106, acc.: 61.72%] [G loss: 0.454168]\n",
      "epoch:19 step:18459 [D loss: 0.251585, acc.: 57.03%] [G loss: 0.440013]\n",
      "epoch:19 step:18460 [D loss: 0.209791, acc.: 67.97%] [G loss: 0.493074]\n",
      "epoch:19 step:18461 [D loss: 0.215657, acc.: 64.84%] [G loss: 0.429491]\n",
      "epoch:19 step:18462 [D loss: 0.229941, acc.: 57.03%] [G loss: 0.415793]\n",
      "epoch:19 step:18463 [D loss: 0.209282, acc.: 60.94%] [G loss: 0.441209]\n",
      "epoch:19 step:18464 [D loss: 0.210252, acc.: 68.75%] [G loss: 0.436138]\n",
      "epoch:19 step:18465 [D loss: 0.209168, acc.: 67.19%] [G loss: 0.438271]\n",
      "epoch:19 step:18466 [D loss: 0.224639, acc.: 65.62%] [G loss: 0.438981]\n",
      "epoch:19 step:18467 [D loss: 0.225287, acc.: 63.28%] [G loss: 0.427224]\n",
      "epoch:19 step:18468 [D loss: 0.239194, acc.: 60.94%] [G loss: 0.417575]\n",
      "epoch:19 step:18469 [D loss: 0.225078, acc.: 64.06%] [G loss: 0.437728]\n",
      "epoch:19 step:18470 [D loss: 0.255369, acc.: 56.25%] [G loss: 0.439680]\n",
      "epoch:19 step:18471 [D loss: 0.213842, acc.: 65.62%] [G loss: 0.430664]\n",
      "epoch:19 step:18472 [D loss: 0.205992, acc.: 67.97%] [G loss: 0.414645]\n",
      "epoch:19 step:18473 [D loss: 0.259803, acc.: 52.34%] [G loss: 0.402561]\n",
      "epoch:19 step:18474 [D loss: 0.227858, acc.: 62.50%] [G loss: 0.423638]\n",
      "epoch:19 step:18475 [D loss: 0.230524, acc.: 65.62%] [G loss: 0.417765]\n",
      "epoch:19 step:18476 [D loss: 0.229317, acc.: 62.50%] [G loss: 0.418108]\n",
      "epoch:19 step:18477 [D loss: 0.200116, acc.: 69.53%] [G loss: 0.470582]\n",
      "epoch:19 step:18478 [D loss: 0.228697, acc.: 53.91%] [G loss: 0.462345]\n",
      "epoch:19 step:18479 [D loss: 0.219060, acc.: 66.41%] [G loss: 0.414975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18480 [D loss: 0.193916, acc.: 71.09%] [G loss: 0.402320]\n",
      "epoch:19 step:18481 [D loss: 0.220592, acc.: 69.53%] [G loss: 0.438999]\n",
      "epoch:19 step:18482 [D loss: 0.208230, acc.: 71.09%] [G loss: 0.407606]\n",
      "epoch:19 step:18483 [D loss: 0.207220, acc.: 64.06%] [G loss: 0.413107]\n",
      "epoch:19 step:18484 [D loss: 0.201277, acc.: 73.44%] [G loss: 0.444077]\n",
      "epoch:19 step:18485 [D loss: 0.229969, acc.: 64.84%] [G loss: 0.418323]\n",
      "epoch:19 step:18486 [D loss: 0.248781, acc.: 58.59%] [G loss: 0.420008]\n",
      "epoch:19 step:18487 [D loss: 0.226701, acc.: 57.81%] [G loss: 0.419362]\n",
      "epoch:19 step:18488 [D loss: 0.226406, acc.: 64.06%] [G loss: 0.394680]\n",
      "epoch:19 step:18489 [D loss: 0.217150, acc.: 66.41%] [G loss: 0.421531]\n",
      "epoch:19 step:18490 [D loss: 0.230677, acc.: 60.94%] [G loss: 0.421962]\n",
      "epoch:19 step:18491 [D loss: 0.210739, acc.: 67.97%] [G loss: 0.414232]\n",
      "epoch:19 step:18492 [D loss: 0.222649, acc.: 65.62%] [G loss: 0.430757]\n",
      "epoch:19 step:18493 [D loss: 0.201787, acc.: 68.75%] [G loss: 0.439567]\n",
      "epoch:19 step:18494 [D loss: 0.188810, acc.: 70.31%] [G loss: 0.468432]\n",
      "epoch:19 step:18495 [D loss: 0.207090, acc.: 68.75%] [G loss: 0.468680]\n",
      "epoch:19 step:18496 [D loss: 0.229478, acc.: 64.06%] [G loss: 0.401954]\n",
      "epoch:19 step:18497 [D loss: 0.197803, acc.: 71.09%] [G loss: 0.513235]\n",
      "epoch:19 step:18498 [D loss: 0.225120, acc.: 63.28%] [G loss: 0.448332]\n",
      "epoch:19 step:18499 [D loss: 0.234611, acc.: 57.03%] [G loss: 0.444654]\n",
      "epoch:19 step:18500 [D loss: 0.233165, acc.: 60.94%] [G loss: 0.436275]\n",
      "epoch:19 step:18501 [D loss: 0.257030, acc.: 56.25%] [G loss: 0.437457]\n",
      "epoch:19 step:18502 [D loss: 0.208241, acc.: 69.53%] [G loss: 0.444872]\n",
      "epoch:19 step:18503 [D loss: 0.193765, acc.: 73.44%] [G loss: 0.498171]\n",
      "epoch:19 step:18504 [D loss: 0.206986, acc.: 66.41%] [G loss: 0.475529]\n",
      "epoch:19 step:18505 [D loss: 0.248271, acc.: 59.38%] [G loss: 0.455518]\n",
      "epoch:19 step:18506 [D loss: 0.271043, acc.: 56.25%] [G loss: 0.399436]\n",
      "epoch:19 step:18507 [D loss: 0.242647, acc.: 53.91%] [G loss: 0.422330]\n",
      "epoch:19 step:18508 [D loss: 0.224470, acc.: 68.75%] [G loss: 0.457257]\n",
      "epoch:19 step:18509 [D loss: 0.215119, acc.: 65.62%] [G loss: 0.450786]\n",
      "epoch:19 step:18510 [D loss: 0.234497, acc.: 61.72%] [G loss: 0.410415]\n",
      "epoch:19 step:18511 [D loss: 0.190825, acc.: 69.53%] [G loss: 0.440190]\n",
      "epoch:19 step:18512 [D loss: 0.209825, acc.: 67.97%] [G loss: 0.468976]\n",
      "epoch:19 step:18513 [D loss: 0.278081, acc.: 46.88%] [G loss: 0.441823]\n",
      "epoch:19 step:18514 [D loss: 0.219179, acc.: 67.97%] [G loss: 0.432838]\n",
      "epoch:19 step:18515 [D loss: 0.206038, acc.: 66.41%] [G loss: 0.435047]\n",
      "epoch:19 step:18516 [D loss: 0.225519, acc.: 59.38%] [G loss: 0.465526]\n",
      "epoch:19 step:18517 [D loss: 0.213475, acc.: 67.19%] [G loss: 0.451253]\n",
      "epoch:19 step:18518 [D loss: 0.240956, acc.: 54.69%] [G loss: 0.408006]\n",
      "epoch:19 step:18519 [D loss: 0.282778, acc.: 53.12%] [G loss: 0.390652]\n",
      "epoch:19 step:18520 [D loss: 0.228393, acc.: 66.41%] [G loss: 0.429560]\n",
      "epoch:19 step:18521 [D loss: 0.224740, acc.: 61.72%] [G loss: 0.480664]\n",
      "epoch:19 step:18522 [D loss: 0.215365, acc.: 64.06%] [G loss: 0.463718]\n",
      "epoch:19 step:18523 [D loss: 0.239285, acc.: 56.25%] [G loss: 0.428365]\n",
      "epoch:19 step:18524 [D loss: 0.210158, acc.: 61.72%] [G loss: 0.465133]\n",
      "epoch:19 step:18525 [D loss: 0.263863, acc.: 54.69%] [G loss: 0.410876]\n",
      "epoch:19 step:18526 [D loss: 0.213298, acc.: 67.19%] [G loss: 0.425760]\n",
      "epoch:19 step:18527 [D loss: 0.202821, acc.: 70.31%] [G loss: 0.420325]\n",
      "epoch:19 step:18528 [D loss: 0.215545, acc.: 67.97%] [G loss: 0.439892]\n",
      "epoch:19 step:18529 [D loss: 0.229417, acc.: 64.06%] [G loss: 0.417556]\n",
      "epoch:19 step:18530 [D loss: 0.236264, acc.: 60.94%] [G loss: 0.395444]\n",
      "epoch:19 step:18531 [D loss: 0.224401, acc.: 63.28%] [G loss: 0.410308]\n",
      "epoch:19 step:18532 [D loss: 0.225721, acc.: 63.28%] [G loss: 0.428611]\n",
      "epoch:19 step:18533 [D loss: 0.210501, acc.: 66.41%] [G loss: 0.426038]\n",
      "epoch:19 step:18534 [D loss: 0.204415, acc.: 67.97%] [G loss: 0.453346]\n",
      "epoch:19 step:18535 [D loss: 0.215649, acc.: 62.50%] [G loss: 0.427717]\n",
      "epoch:19 step:18536 [D loss: 0.210587, acc.: 72.66%] [G loss: 0.450502]\n",
      "epoch:19 step:18537 [D loss: 0.219498, acc.: 64.06%] [G loss: 0.455269]\n",
      "epoch:19 step:18538 [D loss: 0.229209, acc.: 60.16%] [G loss: 0.422700]\n",
      "epoch:19 step:18539 [D loss: 0.218929, acc.: 61.72%] [G loss: 0.404206]\n",
      "epoch:19 step:18540 [D loss: 0.212957, acc.: 64.84%] [G loss: 0.435234]\n",
      "epoch:19 step:18541 [D loss: 0.247090, acc.: 53.91%] [G loss: 0.416642]\n",
      "epoch:19 step:18542 [D loss: 0.241737, acc.: 53.91%] [G loss: 0.407602]\n",
      "epoch:19 step:18543 [D loss: 0.240092, acc.: 58.59%] [G loss: 0.425835]\n",
      "epoch:19 step:18544 [D loss: 0.242450, acc.: 57.03%] [G loss: 0.440731]\n",
      "epoch:19 step:18545 [D loss: 0.231813, acc.: 60.16%] [G loss: 0.426680]\n",
      "epoch:19 step:18546 [D loss: 0.203607, acc.: 70.31%] [G loss: 0.485638]\n",
      "epoch:19 step:18547 [D loss: 0.231262, acc.: 61.72%] [G loss: 0.430360]\n",
      "epoch:19 step:18548 [D loss: 0.253352, acc.: 56.25%] [G loss: 0.370845]\n",
      "epoch:19 step:18549 [D loss: 0.218973, acc.: 64.84%] [G loss: 0.434234]\n",
      "epoch:19 step:18550 [D loss: 0.201026, acc.: 69.53%] [G loss: 0.435356]\n",
      "epoch:19 step:18551 [D loss: 0.213174, acc.: 65.62%] [G loss: 0.398695]\n",
      "epoch:19 step:18552 [D loss: 0.229855, acc.: 60.16%] [G loss: 0.425119]\n",
      "epoch:19 step:18553 [D loss: 0.247405, acc.: 55.47%] [G loss: 0.408847]\n",
      "epoch:19 step:18554 [D loss: 0.217423, acc.: 61.72%] [G loss: 0.442822]\n",
      "epoch:19 step:18555 [D loss: 0.277500, acc.: 48.44%] [G loss: 0.430052]\n",
      "epoch:19 step:18556 [D loss: 0.216720, acc.: 67.19%] [G loss: 0.466242]\n",
      "epoch:19 step:18557 [D loss: 0.236673, acc.: 58.59%] [G loss: 0.417408]\n",
      "epoch:19 step:18558 [D loss: 0.214300, acc.: 66.41%] [G loss: 0.495454]\n",
      "epoch:19 step:18559 [D loss: 0.237355, acc.: 57.81%] [G loss: 0.462073]\n",
      "epoch:19 step:18560 [D loss: 0.237975, acc.: 59.38%] [G loss: 0.441526]\n",
      "epoch:19 step:18561 [D loss: 0.231938, acc.: 59.38%] [G loss: 0.444659]\n",
      "epoch:19 step:18562 [D loss: 0.251408, acc.: 55.47%] [G loss: 0.432757]\n",
      "epoch:19 step:18563 [D loss: 0.245699, acc.: 55.47%] [G loss: 0.407532]\n",
      "epoch:19 step:18564 [D loss: 0.250946, acc.: 55.47%] [G loss: 0.414310]\n",
      "epoch:19 step:18565 [D loss: 0.235525, acc.: 57.81%] [G loss: 0.418049]\n",
      "epoch:19 step:18566 [D loss: 0.238737, acc.: 57.03%] [G loss: 0.395428]\n",
      "epoch:19 step:18567 [D loss: 0.224255, acc.: 60.94%] [G loss: 0.447768]\n",
      "epoch:19 step:18568 [D loss: 0.261686, acc.: 53.12%] [G loss: 0.393186]\n",
      "epoch:19 step:18569 [D loss: 0.239857, acc.: 62.50%] [G loss: 0.444351]\n",
      "epoch:19 step:18570 [D loss: 0.208582, acc.: 67.97%] [G loss: 0.468887]\n",
      "epoch:19 step:18571 [D loss: 0.220248, acc.: 66.41%] [G loss: 0.431406]\n",
      "epoch:19 step:18572 [D loss: 0.228931, acc.: 60.16%] [G loss: 0.448841]\n",
      "epoch:19 step:18573 [D loss: 0.231696, acc.: 60.94%] [G loss: 0.458457]\n",
      "epoch:19 step:18574 [D loss: 0.214180, acc.: 68.75%] [G loss: 0.435788]\n",
      "epoch:19 step:18575 [D loss: 0.243913, acc.: 57.81%] [G loss: 0.399689]\n",
      "epoch:19 step:18576 [D loss: 0.233566, acc.: 63.28%] [G loss: 0.428504]\n",
      "epoch:19 step:18577 [D loss: 0.217418, acc.: 60.16%] [G loss: 0.420642]\n",
      "epoch:19 step:18578 [D loss: 0.224944, acc.: 59.38%] [G loss: 0.502653]\n",
      "epoch:19 step:18579 [D loss: 0.233499, acc.: 63.28%] [G loss: 0.457796]\n",
      "epoch:19 step:18580 [D loss: 0.221394, acc.: 63.28%] [G loss: 0.433649]\n",
      "epoch:19 step:18581 [D loss: 0.227703, acc.: 59.38%] [G loss: 0.435558]\n",
      "epoch:19 step:18582 [D loss: 0.240626, acc.: 64.06%] [G loss: 0.407406]\n",
      "epoch:19 step:18583 [D loss: 0.225541, acc.: 64.06%] [G loss: 0.426203]\n",
      "epoch:19 step:18584 [D loss: 0.196004, acc.: 69.53%] [G loss: 0.434522]\n",
      "epoch:19 step:18585 [D loss: 0.209277, acc.: 71.09%] [G loss: 0.467269]\n",
      "epoch:19 step:18586 [D loss: 0.232577, acc.: 58.59%] [G loss: 0.481892]\n",
      "epoch:19 step:18587 [D loss: 0.260486, acc.: 49.22%] [G loss: 0.415126]\n",
      "epoch:19 step:18588 [D loss: 0.208200, acc.: 73.44%] [G loss: 0.442389]\n",
      "epoch:19 step:18589 [D loss: 0.206362, acc.: 68.75%] [G loss: 0.464744]\n",
      "epoch:19 step:18590 [D loss: 0.248647, acc.: 55.47%] [G loss: 0.436557]\n",
      "epoch:19 step:18591 [D loss: 0.274109, acc.: 53.91%] [G loss: 0.377554]\n",
      "epoch:19 step:18592 [D loss: 0.236698, acc.: 59.38%] [G loss: 0.419210]\n",
      "epoch:19 step:18593 [D loss: 0.241407, acc.: 57.81%] [G loss: 0.404021]\n",
      "epoch:19 step:18594 [D loss: 0.262740, acc.: 53.91%] [G loss: 0.388127]\n",
      "epoch:19 step:18595 [D loss: 0.187955, acc.: 76.56%] [G loss: 0.439521]\n",
      "epoch:19 step:18596 [D loss: 0.210600, acc.: 70.31%] [G loss: 0.460713]\n",
      "epoch:19 step:18597 [D loss: 0.276037, acc.: 47.66%] [G loss: 0.384635]\n",
      "epoch:19 step:18598 [D loss: 0.241812, acc.: 57.03%] [G loss: 0.411734]\n",
      "epoch:19 step:18599 [D loss: 0.228568, acc.: 63.28%] [G loss: 0.422244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18600 [D loss: 0.243491, acc.: 55.47%] [G loss: 0.419277]\n",
      "##############\n",
      "[2.52217963 2.00090243 6.299742   4.66576819 3.59587628 5.54987108\n",
      " 4.50505304 4.64691829 4.45060669 3.87003331]\n",
      "##########\n",
      "epoch:19 step:18601 [D loss: 0.206170, acc.: 61.72%] [G loss: 0.442716]\n",
      "epoch:19 step:18602 [D loss: 0.236204, acc.: 60.16%] [G loss: 0.416605]\n",
      "epoch:19 step:18603 [D loss: 0.229349, acc.: 60.16%] [G loss: 0.466118]\n",
      "epoch:19 step:18604 [D loss: 0.201102, acc.: 66.41%] [G loss: 0.484556]\n",
      "epoch:19 step:18605 [D loss: 0.204374, acc.: 70.31%] [G loss: 0.482496]\n",
      "epoch:19 step:18606 [D loss: 0.199436, acc.: 71.09%] [G loss: 0.506699]\n",
      "epoch:19 step:18607 [D loss: 0.250064, acc.: 60.94%] [G loss: 0.415083]\n",
      "epoch:19 step:18608 [D loss: 0.229327, acc.: 62.50%] [G loss: 0.424835]\n",
      "epoch:19 step:18609 [D loss: 0.218972, acc.: 64.84%] [G loss: 0.400436]\n",
      "epoch:19 step:18610 [D loss: 0.218804, acc.: 63.28%] [G loss: 0.450278]\n",
      "epoch:19 step:18611 [D loss: 0.248396, acc.: 60.16%] [G loss: 0.401128]\n",
      "epoch:19 step:18612 [D loss: 0.235736, acc.: 60.16%] [G loss: 0.389287]\n",
      "epoch:19 step:18613 [D loss: 0.202452, acc.: 67.19%] [G loss: 0.417669]\n",
      "epoch:19 step:18614 [D loss: 0.236192, acc.: 64.84%] [G loss: 0.426504]\n",
      "epoch:19 step:18615 [D loss: 0.216790, acc.: 66.41%] [G loss: 0.463531]\n",
      "epoch:19 step:18616 [D loss: 0.213204, acc.: 68.75%] [G loss: 0.430982]\n",
      "epoch:19 step:18617 [D loss: 0.215175, acc.: 65.62%] [G loss: 0.462964]\n",
      "epoch:19 step:18618 [D loss: 0.203117, acc.: 64.84%] [G loss: 0.431085]\n",
      "epoch:19 step:18619 [D loss: 0.231362, acc.: 60.16%] [G loss: 0.462191]\n",
      "epoch:19 step:18620 [D loss: 0.256582, acc.: 53.91%] [G loss: 0.431883]\n",
      "epoch:19 step:18621 [D loss: 0.239568, acc.: 60.94%] [G loss: 0.433624]\n",
      "epoch:19 step:18622 [D loss: 0.211873, acc.: 65.62%] [G loss: 0.425961]\n",
      "epoch:19 step:18623 [D loss: 0.260034, acc.: 50.00%] [G loss: 0.410708]\n",
      "epoch:19 step:18624 [D loss: 0.221846, acc.: 60.16%] [G loss: 0.421032]\n",
      "epoch:19 step:18625 [D loss: 0.208782, acc.: 64.06%] [G loss: 0.428733]\n",
      "epoch:19 step:18626 [D loss: 0.198376, acc.: 71.09%] [G loss: 0.419234]\n",
      "epoch:19 step:18627 [D loss: 0.228074, acc.: 61.72%] [G loss: 0.425743]\n",
      "epoch:19 step:18628 [D loss: 0.252751, acc.: 56.25%] [G loss: 0.391022]\n",
      "epoch:19 step:18629 [D loss: 0.226440, acc.: 62.50%] [G loss: 0.417070]\n",
      "epoch:19 step:18630 [D loss: 0.236708, acc.: 58.59%] [G loss: 0.448989]\n",
      "epoch:19 step:18631 [D loss: 0.264183, acc.: 51.56%] [G loss: 0.420649]\n",
      "epoch:19 step:18632 [D loss: 0.240084, acc.: 57.03%] [G loss: 0.450439]\n",
      "epoch:19 step:18633 [D loss: 0.231079, acc.: 58.59%] [G loss: 0.395142]\n",
      "epoch:19 step:18634 [D loss: 0.220327, acc.: 67.19%] [G loss: 0.430840]\n",
      "epoch:19 step:18635 [D loss: 0.218091, acc.: 64.06%] [G loss: 0.418156]\n",
      "epoch:19 step:18636 [D loss: 0.196901, acc.: 69.53%] [G loss: 0.422620]\n",
      "epoch:19 step:18637 [D loss: 0.226898, acc.: 64.84%] [G loss: 0.419056]\n",
      "epoch:19 step:18638 [D loss: 0.226951, acc.: 66.41%] [G loss: 0.419119]\n",
      "epoch:19 step:18639 [D loss: 0.210237, acc.: 69.53%] [G loss: 0.405582]\n",
      "epoch:19 step:18640 [D loss: 0.210115, acc.: 69.53%] [G loss: 0.422245]\n",
      "epoch:19 step:18641 [D loss: 0.187638, acc.: 71.88%] [G loss: 0.441728]\n",
      "epoch:19 step:18642 [D loss: 0.225758, acc.: 64.84%] [G loss: 0.401749]\n",
      "epoch:19 step:18643 [D loss: 0.236807, acc.: 58.59%] [G loss: 0.454993]\n",
      "epoch:19 step:18644 [D loss: 0.223410, acc.: 67.19%] [G loss: 0.441807]\n",
      "epoch:19 step:18645 [D loss: 0.190966, acc.: 71.09%] [G loss: 0.420964]\n",
      "epoch:19 step:18646 [D loss: 0.222248, acc.: 69.53%] [G loss: 0.444910]\n",
      "epoch:19 step:18647 [D loss: 0.221440, acc.: 64.06%] [G loss: 0.427739]\n",
      "epoch:19 step:18648 [D loss: 0.215024, acc.: 64.06%] [G loss: 0.436890]\n",
      "epoch:19 step:18649 [D loss: 0.253276, acc.: 53.12%] [G loss: 0.406592]\n",
      "epoch:19 step:18650 [D loss: 0.253136, acc.: 59.38%] [G loss: 0.435903]\n",
      "epoch:19 step:18651 [D loss: 0.227417, acc.: 64.06%] [G loss: 0.435258]\n",
      "epoch:19 step:18652 [D loss: 0.188797, acc.: 75.78%] [G loss: 0.457201]\n",
      "epoch:19 step:18653 [D loss: 0.241722, acc.: 55.47%] [G loss: 0.429167]\n",
      "epoch:19 step:18654 [D loss: 0.243219, acc.: 60.16%] [G loss: 0.456984]\n",
      "epoch:19 step:18655 [D loss: 0.223357, acc.: 60.94%] [G loss: 0.490347]\n",
      "epoch:19 step:18656 [D loss: 0.207084, acc.: 67.97%] [G loss: 0.482449]\n",
      "epoch:19 step:18657 [D loss: 0.223498, acc.: 65.62%] [G loss: 0.424596]\n",
      "epoch:19 step:18658 [D loss: 0.251894, acc.: 53.91%] [G loss: 0.416629]\n",
      "epoch:19 step:18659 [D loss: 0.231289, acc.: 60.16%] [G loss: 0.421539]\n",
      "epoch:19 step:18660 [D loss: 0.243140, acc.: 57.81%] [G loss: 0.444196]\n",
      "epoch:19 step:18661 [D loss: 0.259094, acc.: 53.91%] [G loss: 0.397468]\n",
      "epoch:19 step:18662 [D loss: 0.242575, acc.: 62.50%] [G loss: 0.447446]\n",
      "epoch:19 step:18663 [D loss: 0.186414, acc.: 78.12%] [G loss: 0.464051]\n",
      "epoch:19 step:18664 [D loss: 0.253133, acc.: 55.47%] [G loss: 0.407432]\n",
      "epoch:19 step:18665 [D loss: 0.253524, acc.: 49.22%] [G loss: 0.383447]\n",
      "epoch:19 step:18666 [D loss: 0.228744, acc.: 63.28%] [G loss: 0.393795]\n",
      "epoch:19 step:18667 [D loss: 0.244564, acc.: 58.59%] [G loss: 0.405340]\n",
      "epoch:19 step:18668 [D loss: 0.248101, acc.: 53.91%] [G loss: 0.379287]\n",
      "epoch:19 step:18669 [D loss: 0.245234, acc.: 60.16%] [G loss: 0.385090]\n",
      "epoch:19 step:18670 [D loss: 0.245118, acc.: 53.91%] [G loss: 0.402645]\n",
      "epoch:19 step:18671 [D loss: 0.194351, acc.: 70.31%] [G loss: 0.478903]\n",
      "epoch:19 step:18672 [D loss: 0.241623, acc.: 60.16%] [G loss: 0.406023]\n",
      "epoch:19 step:18673 [D loss: 0.237139, acc.: 60.94%] [G loss: 0.441852]\n",
      "epoch:19 step:18674 [D loss: 0.205303, acc.: 70.31%] [G loss: 0.458569]\n",
      "epoch:19 step:18675 [D loss: 0.224968, acc.: 60.94%] [G loss: 0.475703]\n",
      "epoch:19 step:18676 [D loss: 0.227664, acc.: 64.84%] [G loss: 0.475082]\n",
      "epoch:19 step:18677 [D loss: 0.253369, acc.: 53.91%] [G loss: 0.432102]\n",
      "epoch:19 step:18678 [D loss: 0.205807, acc.: 64.06%] [G loss: 0.443488]\n",
      "epoch:19 step:18679 [D loss: 0.236791, acc.: 65.62%] [G loss: 0.438865]\n",
      "epoch:19 step:18680 [D loss: 0.239651, acc.: 57.03%] [G loss: 0.434315]\n",
      "epoch:19 step:18681 [D loss: 0.247599, acc.: 57.03%] [G loss: 0.374975]\n",
      "epoch:19 step:18682 [D loss: 0.229918, acc.: 62.50%] [G loss: 0.407371]\n",
      "epoch:19 step:18683 [D loss: 0.231641, acc.: 66.41%] [G loss: 0.419949]\n",
      "epoch:19 step:18684 [D loss: 0.213399, acc.: 63.28%] [G loss: 0.435434]\n",
      "epoch:19 step:18685 [D loss: 0.230386, acc.: 58.59%] [G loss: 0.419350]\n",
      "epoch:19 step:18686 [D loss: 0.227893, acc.: 61.72%] [G loss: 0.436826]\n",
      "epoch:19 step:18687 [D loss: 0.214180, acc.: 62.50%] [G loss: 0.432355]\n",
      "epoch:19 step:18688 [D loss: 0.238522, acc.: 60.16%] [G loss: 0.473272]\n",
      "epoch:19 step:18689 [D loss: 0.218657, acc.: 62.50%] [G loss: 0.467477]\n",
      "epoch:19 step:18690 [D loss: 0.232343, acc.: 61.72%] [G loss: 0.445240]\n",
      "epoch:19 step:18691 [D loss: 0.215454, acc.: 66.41%] [G loss: 0.421003]\n",
      "epoch:19 step:18692 [D loss: 0.201226, acc.: 66.41%] [G loss: 0.460405]\n",
      "epoch:19 step:18693 [D loss: 0.220144, acc.: 68.75%] [G loss: 0.418582]\n",
      "epoch:19 step:18694 [D loss: 0.268021, acc.: 49.22%] [G loss: 0.391021]\n",
      "epoch:19 step:18695 [D loss: 0.249626, acc.: 50.00%] [G loss: 0.408124]\n",
      "epoch:19 step:18696 [D loss: 0.219882, acc.: 62.50%] [G loss: 0.395877]\n",
      "epoch:19 step:18697 [D loss: 0.219909, acc.: 65.62%] [G loss: 0.436732]\n",
      "epoch:19 step:18698 [D loss: 0.208505, acc.: 67.97%] [G loss: 0.441430]\n",
      "epoch:19 step:18699 [D loss: 0.214170, acc.: 66.41%] [G loss: 0.474239]\n",
      "epoch:19 step:18700 [D loss: 0.221100, acc.: 64.06%] [G loss: 0.457434]\n",
      "epoch:19 step:18701 [D loss: 0.200150, acc.: 69.53%] [G loss: 0.485204]\n",
      "epoch:19 step:18702 [D loss: 0.184103, acc.: 71.88%] [G loss: 0.449292]\n",
      "epoch:19 step:18703 [D loss: 0.203082, acc.: 64.84%] [G loss: 0.466647]\n",
      "epoch:19 step:18704 [D loss: 0.237872, acc.: 58.59%] [G loss: 0.430297]\n",
      "epoch:19 step:18705 [D loss: 0.214872, acc.: 61.72%] [G loss: 0.446632]\n",
      "epoch:19 step:18706 [D loss: 0.216730, acc.: 67.19%] [G loss: 0.422839]\n",
      "epoch:19 step:18707 [D loss: 0.222038, acc.: 68.75%] [G loss: 0.423378]\n",
      "epoch:19 step:18708 [D loss: 0.208535, acc.: 65.62%] [G loss: 0.505226]\n",
      "epoch:19 step:18709 [D loss: 0.187743, acc.: 71.88%] [G loss: 0.519559]\n",
      "epoch:19 step:18710 [D loss: 0.263746, acc.: 53.91%] [G loss: 0.428605]\n",
      "epoch:19 step:18711 [D loss: 0.221471, acc.: 66.41%] [G loss: 0.435982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18712 [D loss: 0.201505, acc.: 66.41%] [G loss: 0.444424]\n",
      "epoch:19 step:18713 [D loss: 0.235297, acc.: 56.25%] [G loss: 0.466787]\n",
      "epoch:19 step:18714 [D loss: 0.186877, acc.: 76.56%] [G loss: 0.458683]\n",
      "epoch:19 step:18715 [D loss: 0.200289, acc.: 68.75%] [G loss: 0.475814]\n",
      "epoch:19 step:18716 [D loss: 0.226094, acc.: 64.84%] [G loss: 0.491691]\n",
      "epoch:19 step:18717 [D loss: 0.220678, acc.: 64.84%] [G loss: 0.438821]\n",
      "epoch:19 step:18718 [D loss: 0.272820, acc.: 55.47%] [G loss: 0.433515]\n",
      "epoch:19 step:18719 [D loss: 0.215949, acc.: 69.53%] [G loss: 0.439152]\n",
      "epoch:19 step:18720 [D loss: 0.213362, acc.: 64.06%] [G loss: 0.471828]\n",
      "epoch:19 step:18721 [D loss: 0.214202, acc.: 67.19%] [G loss: 0.466466]\n",
      "epoch:19 step:18722 [D loss: 0.174490, acc.: 78.91%] [G loss: 0.520813]\n",
      "epoch:19 step:18723 [D loss: 0.328733, acc.: 42.19%] [G loss: 0.422451]\n",
      "epoch:19 step:18724 [D loss: 0.224583, acc.: 60.94%] [G loss: 0.457070]\n",
      "epoch:19 step:18725 [D loss: 0.231480, acc.: 63.28%] [G loss: 0.414791]\n",
      "epoch:19 step:18726 [D loss: 0.186714, acc.: 74.22%] [G loss: 0.471472]\n",
      "epoch:19 step:18727 [D loss: 0.184453, acc.: 79.69%] [G loss: 0.489651]\n",
      "epoch:19 step:18728 [D loss: 0.170625, acc.: 82.03%] [G loss: 0.482240]\n",
      "epoch:19 step:18729 [D loss: 0.171687, acc.: 81.25%] [G loss: 0.503590]\n",
      "epoch:19 step:18730 [D loss: 0.198019, acc.: 64.06%] [G loss: 0.501740]\n",
      "epoch:19 step:18731 [D loss: 0.290241, acc.: 55.47%] [G loss: 0.485468]\n",
      "epoch:19 step:18732 [D loss: 0.253018, acc.: 58.59%] [G loss: 0.487726]\n",
      "epoch:19 step:18733 [D loss: 0.206784, acc.: 64.06%] [G loss: 0.425805]\n",
      "epoch:19 step:18734 [D loss: 0.274570, acc.: 54.69%] [G loss: 0.449030]\n",
      "epoch:19 step:18735 [D loss: 0.237486, acc.: 58.59%] [G loss: 0.458808]\n",
      "epoch:19 step:18736 [D loss: 0.217831, acc.: 66.41%] [G loss: 0.403248]\n",
      "epoch:19 step:18737 [D loss: 0.233486, acc.: 60.16%] [G loss: 0.443564]\n",
      "epoch:19 step:18738 [D loss: 0.198266, acc.: 71.09%] [G loss: 0.454086]\n",
      "epoch:19 step:18739 [D loss: 0.173267, acc.: 75.00%] [G loss: 0.538051]\n",
      "epoch:19 step:18740 [D loss: 0.189112, acc.: 70.31%] [G loss: 0.536381]\n",
      "epoch:20 step:18741 [D loss: 0.230783, acc.: 64.84%] [G loss: 0.520123]\n",
      "epoch:20 step:18742 [D loss: 0.270053, acc.: 60.94%] [G loss: 0.483034]\n",
      "epoch:20 step:18743 [D loss: 0.265783, acc.: 53.12%] [G loss: 0.475581]\n",
      "epoch:20 step:18744 [D loss: 0.223833, acc.: 61.72%] [G loss: 0.501664]\n",
      "epoch:20 step:18745 [D loss: 0.257803, acc.: 56.25%] [G loss: 0.401609]\n",
      "epoch:20 step:18746 [D loss: 0.208298, acc.: 67.19%] [G loss: 0.441465]\n",
      "epoch:20 step:18747 [D loss: 0.213895, acc.: 62.50%] [G loss: 0.428596]\n",
      "epoch:20 step:18748 [D loss: 0.206867, acc.: 66.41%] [G loss: 0.445361]\n",
      "epoch:20 step:18749 [D loss: 0.191426, acc.: 76.56%] [G loss: 0.417372]\n",
      "epoch:20 step:18750 [D loss: 0.203555, acc.: 68.75%] [G loss: 0.436615]\n",
      "epoch:20 step:18751 [D loss: 0.217442, acc.: 64.06%] [G loss: 0.430468]\n",
      "epoch:20 step:18752 [D loss: 0.206053, acc.: 69.53%] [G loss: 0.465426]\n",
      "epoch:20 step:18753 [D loss: 0.220296, acc.: 63.28%] [G loss: 0.428414]\n",
      "epoch:20 step:18754 [D loss: 0.205065, acc.: 67.97%] [G loss: 0.426221]\n",
      "epoch:20 step:18755 [D loss: 0.198025, acc.: 71.88%] [G loss: 0.490622]\n",
      "epoch:20 step:18756 [D loss: 0.224571, acc.: 64.84%] [G loss: 0.460959]\n",
      "epoch:20 step:18757 [D loss: 0.229954, acc.: 63.28%] [G loss: 0.446723]\n",
      "epoch:20 step:18758 [D loss: 0.226031, acc.: 62.50%] [G loss: 0.435588]\n",
      "epoch:20 step:18759 [D loss: 0.263507, acc.: 53.91%] [G loss: 0.445989]\n",
      "epoch:20 step:18760 [D loss: 0.250082, acc.: 56.25%] [G loss: 0.485472]\n",
      "epoch:20 step:18761 [D loss: 0.217351, acc.: 62.50%] [G loss: 0.443163]\n",
      "epoch:20 step:18762 [D loss: 0.213806, acc.: 63.28%] [G loss: 0.477658]\n",
      "epoch:20 step:18763 [D loss: 0.276191, acc.: 46.88%] [G loss: 0.409549]\n",
      "epoch:20 step:18764 [D loss: 0.212059, acc.: 67.19%] [G loss: 0.445803]\n",
      "epoch:20 step:18765 [D loss: 0.221874, acc.: 64.06%] [G loss: 0.443252]\n",
      "epoch:20 step:18766 [D loss: 0.214463, acc.: 63.28%] [G loss: 0.450250]\n",
      "epoch:20 step:18767 [D loss: 0.213789, acc.: 65.62%] [G loss: 0.473480]\n",
      "epoch:20 step:18768 [D loss: 0.224657, acc.: 63.28%] [G loss: 0.416891]\n",
      "epoch:20 step:18769 [D loss: 0.217671, acc.: 64.06%] [G loss: 0.435063]\n",
      "epoch:20 step:18770 [D loss: 0.222976, acc.: 65.62%] [G loss: 0.442995]\n",
      "epoch:20 step:18771 [D loss: 0.231888, acc.: 63.28%] [G loss: 0.433984]\n",
      "epoch:20 step:18772 [D loss: 0.209471, acc.: 70.31%] [G loss: 0.455778]\n",
      "epoch:20 step:18773 [D loss: 0.245409, acc.: 56.25%] [G loss: 0.447040]\n",
      "epoch:20 step:18774 [D loss: 0.247813, acc.: 61.72%] [G loss: 0.446605]\n",
      "epoch:20 step:18775 [D loss: 0.247101, acc.: 54.69%] [G loss: 0.416435]\n",
      "epoch:20 step:18776 [D loss: 0.227203, acc.: 63.28%] [G loss: 0.453386]\n",
      "epoch:20 step:18777 [D loss: 0.225140, acc.: 62.50%] [G loss: 0.437528]\n",
      "epoch:20 step:18778 [D loss: 0.254784, acc.: 53.12%] [G loss: 0.376871]\n",
      "epoch:20 step:18779 [D loss: 0.242940, acc.: 63.28%] [G loss: 0.409504]\n",
      "epoch:20 step:18780 [D loss: 0.215947, acc.: 62.50%] [G loss: 0.409655]\n",
      "epoch:20 step:18781 [D loss: 0.226397, acc.: 62.50%] [G loss: 0.459548]\n",
      "epoch:20 step:18782 [D loss: 0.218000, acc.: 63.28%] [G loss: 0.422359]\n",
      "epoch:20 step:18783 [D loss: 0.213632, acc.: 66.41%] [G loss: 0.408215]\n",
      "epoch:20 step:18784 [D loss: 0.221849, acc.: 61.72%] [G loss: 0.390826]\n",
      "epoch:20 step:18785 [D loss: 0.215069, acc.: 69.53%] [G loss: 0.432035]\n",
      "epoch:20 step:18786 [D loss: 0.239568, acc.: 57.03%] [G loss: 0.473381]\n",
      "epoch:20 step:18787 [D loss: 0.230148, acc.: 60.16%] [G loss: 0.424092]\n",
      "epoch:20 step:18788 [D loss: 0.215362, acc.: 60.16%] [G loss: 0.405593]\n",
      "epoch:20 step:18789 [D loss: 0.217587, acc.: 63.28%] [G loss: 0.433289]\n",
      "epoch:20 step:18790 [D loss: 0.222797, acc.: 70.31%] [G loss: 0.408238]\n",
      "epoch:20 step:18791 [D loss: 0.245130, acc.: 58.59%] [G loss: 0.413529]\n",
      "epoch:20 step:18792 [D loss: 0.210754, acc.: 66.41%] [G loss: 0.462177]\n",
      "epoch:20 step:18793 [D loss: 0.237987, acc.: 64.06%] [G loss: 0.480708]\n",
      "epoch:20 step:18794 [D loss: 0.214455, acc.: 65.62%] [G loss: 0.464473]\n",
      "epoch:20 step:18795 [D loss: 0.236801, acc.: 55.47%] [G loss: 0.479555]\n",
      "epoch:20 step:18796 [D loss: 0.216582, acc.: 66.41%] [G loss: 0.420434]\n",
      "epoch:20 step:18797 [D loss: 0.243137, acc.: 60.94%] [G loss: 0.436457]\n",
      "epoch:20 step:18798 [D loss: 0.208612, acc.: 64.06%] [G loss: 0.455428]\n",
      "epoch:20 step:18799 [D loss: 0.211920, acc.: 63.28%] [G loss: 0.409523]\n",
      "epoch:20 step:18800 [D loss: 0.251804, acc.: 61.72%] [G loss: 0.387896]\n",
      "##############\n",
      "[2.62473808 1.74857631 6.16673835 4.62869478 3.71909462 5.45227243\n",
      " 4.66925025 4.74879282 4.44715711 4.02022321]\n",
      "##########\n",
      "epoch:20 step:18801 [D loss: 0.248399, acc.: 53.12%] [G loss: 0.425124]\n",
      "epoch:20 step:18802 [D loss: 0.230307, acc.: 61.72%] [G loss: 0.428245]\n",
      "epoch:20 step:18803 [D loss: 0.228411, acc.: 59.38%] [G loss: 0.407832]\n",
      "epoch:20 step:18804 [D loss: 0.197497, acc.: 75.78%] [G loss: 0.443436]\n",
      "epoch:20 step:18805 [D loss: 0.228080, acc.: 63.28%] [G loss: 0.409241]\n",
      "epoch:20 step:18806 [D loss: 0.222852, acc.: 64.06%] [G loss: 0.394846]\n",
      "epoch:20 step:18807 [D loss: 0.207288, acc.: 67.19%] [G loss: 0.416539]\n",
      "epoch:20 step:18808 [D loss: 0.208435, acc.: 70.31%] [G loss: 0.453463]\n",
      "epoch:20 step:18809 [D loss: 0.199811, acc.: 71.88%] [G loss: 0.417641]\n",
      "epoch:20 step:18810 [D loss: 0.214993, acc.: 64.84%] [G loss: 0.477485]\n",
      "epoch:20 step:18811 [D loss: 0.246862, acc.: 62.50%] [G loss: 0.410719]\n",
      "epoch:20 step:18812 [D loss: 0.226036, acc.: 61.72%] [G loss: 0.404669]\n",
      "epoch:20 step:18813 [D loss: 0.238024, acc.: 56.25%] [G loss: 0.403454]\n",
      "epoch:20 step:18814 [D loss: 0.206724, acc.: 66.41%] [G loss: 0.425211]\n",
      "epoch:20 step:18815 [D loss: 0.231574, acc.: 60.16%] [G loss: 0.443337]\n",
      "epoch:20 step:18816 [D loss: 0.203889, acc.: 66.41%] [G loss: 0.442767]\n",
      "epoch:20 step:18817 [D loss: 0.190685, acc.: 73.44%] [G loss: 0.466268]\n",
      "epoch:20 step:18818 [D loss: 0.265483, acc.: 55.47%] [G loss: 0.395977]\n",
      "epoch:20 step:18819 [D loss: 0.244626, acc.: 55.47%] [G loss: 0.419954]\n",
      "epoch:20 step:18820 [D loss: 0.223209, acc.: 68.75%] [G loss: 0.420580]\n",
      "epoch:20 step:18821 [D loss: 0.236484, acc.: 62.50%] [G loss: 0.415469]\n",
      "epoch:20 step:18822 [D loss: 0.223520, acc.: 64.06%] [G loss: 0.439658]\n",
      "epoch:20 step:18823 [D loss: 0.216569, acc.: 61.72%] [G loss: 0.467702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18824 [D loss: 0.196110, acc.: 71.09%] [G loss: 0.484219]\n",
      "epoch:20 step:18825 [D loss: 0.227626, acc.: 60.16%] [G loss: 0.422494]\n",
      "epoch:20 step:18826 [D loss: 0.230669, acc.: 58.59%] [G loss: 0.445016]\n",
      "epoch:20 step:18827 [D loss: 0.226517, acc.: 63.28%] [G loss: 0.399704]\n",
      "epoch:20 step:18828 [D loss: 0.198873, acc.: 73.44%] [G loss: 0.489877]\n",
      "epoch:20 step:18829 [D loss: 0.202469, acc.: 73.44%] [G loss: 0.393805]\n",
      "epoch:20 step:18830 [D loss: 0.234538, acc.: 57.03%] [G loss: 0.430799]\n",
      "epoch:20 step:18831 [D loss: 0.222137, acc.: 62.50%] [G loss: 0.461939]\n",
      "epoch:20 step:18832 [D loss: 0.187916, acc.: 75.00%] [G loss: 0.479095]\n",
      "epoch:20 step:18833 [D loss: 0.195654, acc.: 67.97%] [G loss: 0.448939]\n",
      "epoch:20 step:18834 [D loss: 0.234194, acc.: 63.28%] [G loss: 0.404945]\n",
      "epoch:20 step:18835 [D loss: 0.213587, acc.: 66.41%] [G loss: 0.456907]\n",
      "epoch:20 step:18836 [D loss: 0.223012, acc.: 64.06%] [G loss: 0.431448]\n",
      "epoch:20 step:18837 [D loss: 0.193721, acc.: 69.53%] [G loss: 0.438603]\n",
      "epoch:20 step:18838 [D loss: 0.264799, acc.: 56.25%] [G loss: 0.438949]\n",
      "epoch:20 step:18839 [D loss: 0.244862, acc.: 59.38%] [G loss: 0.409386]\n",
      "epoch:20 step:18840 [D loss: 0.193440, acc.: 71.88%] [G loss: 0.448397]\n",
      "epoch:20 step:18841 [D loss: 0.225119, acc.: 60.16%] [G loss: 0.441643]\n",
      "epoch:20 step:18842 [D loss: 0.240296, acc.: 62.50%] [G loss: 0.417108]\n",
      "epoch:20 step:18843 [D loss: 0.233015, acc.: 54.69%] [G loss: 0.419102]\n",
      "epoch:20 step:18844 [D loss: 0.217451, acc.: 60.16%] [G loss: 0.411757]\n",
      "epoch:20 step:18845 [D loss: 0.233105, acc.: 64.06%] [G loss: 0.399610]\n",
      "epoch:20 step:18846 [D loss: 0.210213, acc.: 67.97%] [G loss: 0.484179]\n",
      "epoch:20 step:18847 [D loss: 0.199375, acc.: 67.19%] [G loss: 0.501333]\n",
      "epoch:20 step:18848 [D loss: 0.266956, acc.: 53.91%] [G loss: 0.453393]\n",
      "epoch:20 step:18849 [D loss: 0.267065, acc.: 50.78%] [G loss: 0.386293]\n",
      "epoch:20 step:18850 [D loss: 0.248683, acc.: 57.03%] [G loss: 0.418127]\n",
      "epoch:20 step:18851 [D loss: 0.214886, acc.: 65.62%] [G loss: 0.438924]\n",
      "epoch:20 step:18852 [D loss: 0.204053, acc.: 69.53%] [G loss: 0.448599]\n",
      "epoch:20 step:18853 [D loss: 0.188672, acc.: 75.00%] [G loss: 0.459410]\n",
      "epoch:20 step:18854 [D loss: 0.240003, acc.: 61.72%] [G loss: 0.440188]\n",
      "epoch:20 step:18855 [D loss: 0.212249, acc.: 69.53%] [G loss: 0.488620]\n",
      "epoch:20 step:18856 [D loss: 0.198044, acc.: 67.19%] [G loss: 0.497018]\n",
      "epoch:20 step:18857 [D loss: 0.186940, acc.: 75.00%] [G loss: 0.488610]\n",
      "epoch:20 step:18858 [D loss: 0.224354, acc.: 67.97%] [G loss: 0.420256]\n",
      "epoch:20 step:18859 [D loss: 0.185890, acc.: 71.88%] [G loss: 0.539835]\n",
      "epoch:20 step:18860 [D loss: 0.269760, acc.: 59.38%] [G loss: 0.473394]\n",
      "epoch:20 step:18861 [D loss: 0.224778, acc.: 64.06%] [G loss: 0.468246]\n",
      "epoch:20 step:18862 [D loss: 0.206026, acc.: 68.75%] [G loss: 0.429284]\n",
      "epoch:20 step:18863 [D loss: 0.185169, acc.: 71.88%] [G loss: 0.456775]\n",
      "epoch:20 step:18864 [D loss: 0.261182, acc.: 55.47%] [G loss: 0.440040]\n",
      "epoch:20 step:18865 [D loss: 0.241927, acc.: 59.38%] [G loss: 0.382054]\n",
      "epoch:20 step:18866 [D loss: 0.185734, acc.: 71.09%] [G loss: 0.467826]\n",
      "epoch:20 step:18867 [D loss: 0.199477, acc.: 65.62%] [G loss: 0.441865]\n",
      "epoch:20 step:18868 [D loss: 0.233814, acc.: 57.03%] [G loss: 0.398498]\n",
      "epoch:20 step:18869 [D loss: 0.214542, acc.: 63.28%] [G loss: 0.425264]\n",
      "epoch:20 step:18870 [D loss: 0.223401, acc.: 66.41%] [G loss: 0.429115]\n",
      "epoch:20 step:18871 [D loss: 0.218601, acc.: 65.62%] [G loss: 0.398756]\n",
      "epoch:20 step:18872 [D loss: 0.213924, acc.: 71.09%] [G loss: 0.425969]\n",
      "epoch:20 step:18873 [D loss: 0.251518, acc.: 55.47%] [G loss: 0.475923]\n",
      "epoch:20 step:18874 [D loss: 0.232822, acc.: 65.62%] [G loss: 0.432311]\n",
      "epoch:20 step:18875 [D loss: 0.224698, acc.: 64.84%] [G loss: 0.461456]\n",
      "epoch:20 step:18876 [D loss: 0.231496, acc.: 64.84%] [G loss: 0.470214]\n",
      "epoch:20 step:18877 [D loss: 0.268458, acc.: 50.78%] [G loss: 0.396386]\n",
      "epoch:20 step:18878 [D loss: 0.248246, acc.: 60.94%] [G loss: 0.394046]\n",
      "epoch:20 step:18879 [D loss: 0.260698, acc.: 50.78%] [G loss: 0.427123]\n",
      "epoch:20 step:18880 [D loss: 0.250992, acc.: 57.81%] [G loss: 0.464227]\n",
      "epoch:20 step:18881 [D loss: 0.227155, acc.: 60.16%] [G loss: 0.408236]\n",
      "epoch:20 step:18882 [D loss: 0.240737, acc.: 59.38%] [G loss: 0.408021]\n",
      "epoch:20 step:18883 [D loss: 0.231873, acc.: 57.81%] [G loss: 0.435025]\n",
      "epoch:20 step:18884 [D loss: 0.195850, acc.: 67.19%] [G loss: 0.451533]\n",
      "epoch:20 step:18885 [D loss: 0.214521, acc.: 67.19%] [G loss: 0.452665]\n",
      "epoch:20 step:18886 [D loss: 0.240891, acc.: 57.03%] [G loss: 0.414775]\n",
      "epoch:20 step:18887 [D loss: 0.246328, acc.: 56.25%] [G loss: 0.425658]\n",
      "epoch:20 step:18888 [D loss: 0.253651, acc.: 55.47%] [G loss: 0.434381]\n",
      "epoch:20 step:18889 [D loss: 0.233902, acc.: 61.72%] [G loss: 0.421213]\n",
      "epoch:20 step:18890 [D loss: 0.238013, acc.: 54.69%] [G loss: 0.409846]\n",
      "epoch:20 step:18891 [D loss: 0.210418, acc.: 66.41%] [G loss: 0.428546]\n",
      "epoch:20 step:18892 [D loss: 0.221276, acc.: 66.41%] [G loss: 0.443246]\n",
      "epoch:20 step:18893 [D loss: 0.245262, acc.: 59.38%] [G loss: 0.430750]\n",
      "epoch:20 step:18894 [D loss: 0.205426, acc.: 67.97%] [G loss: 0.449293]\n",
      "epoch:20 step:18895 [D loss: 0.219253, acc.: 64.84%] [G loss: 0.421977]\n",
      "epoch:20 step:18896 [D loss: 0.225633, acc.: 60.94%] [G loss: 0.425698]\n",
      "epoch:20 step:18897 [D loss: 0.216782, acc.: 59.38%] [G loss: 0.410622]\n",
      "epoch:20 step:18898 [D loss: 0.247628, acc.: 54.69%] [G loss: 0.398787]\n",
      "epoch:20 step:18899 [D loss: 0.215151, acc.: 64.84%] [G loss: 0.431655]\n",
      "epoch:20 step:18900 [D loss: 0.272587, acc.: 49.22%] [G loss: 0.457495]\n",
      "epoch:20 step:18901 [D loss: 0.234152, acc.: 57.81%] [G loss: 0.448015]\n",
      "epoch:20 step:18902 [D loss: 0.247668, acc.: 55.47%] [G loss: 0.427339]\n",
      "epoch:20 step:18903 [D loss: 0.229557, acc.: 55.47%] [G loss: 0.440249]\n",
      "epoch:20 step:18904 [D loss: 0.224497, acc.: 64.06%] [G loss: 0.475594]\n",
      "epoch:20 step:18905 [D loss: 0.206856, acc.: 68.75%] [G loss: 0.407170]\n",
      "epoch:20 step:18906 [D loss: 0.213534, acc.: 65.62%] [G loss: 0.422092]\n",
      "epoch:20 step:18907 [D loss: 0.222355, acc.: 58.59%] [G loss: 0.434957]\n",
      "epoch:20 step:18908 [D loss: 0.200624, acc.: 66.41%] [G loss: 0.467624]\n",
      "epoch:20 step:18909 [D loss: 0.232770, acc.: 58.59%] [G loss: 0.409193]\n",
      "epoch:20 step:18910 [D loss: 0.232050, acc.: 62.50%] [G loss: 0.431625]\n",
      "epoch:20 step:18911 [D loss: 0.242062, acc.: 55.47%] [G loss: 0.410578]\n",
      "epoch:20 step:18912 [D loss: 0.220542, acc.: 64.84%] [G loss: 0.407064]\n",
      "epoch:20 step:18913 [D loss: 0.218758, acc.: 65.62%] [G loss: 0.428953]\n",
      "epoch:20 step:18914 [D loss: 0.243880, acc.: 60.94%] [G loss: 0.401858]\n",
      "epoch:20 step:18915 [D loss: 0.220114, acc.: 61.72%] [G loss: 0.457876]\n",
      "epoch:20 step:18916 [D loss: 0.228884, acc.: 60.94%] [G loss: 0.417967]\n",
      "epoch:20 step:18917 [D loss: 0.219181, acc.: 62.50%] [G loss: 0.443160]\n",
      "epoch:20 step:18918 [D loss: 0.230172, acc.: 60.94%] [G loss: 0.393581]\n",
      "epoch:20 step:18919 [D loss: 0.240851, acc.: 57.03%] [G loss: 0.404826]\n",
      "epoch:20 step:18920 [D loss: 0.236156, acc.: 58.59%] [G loss: 0.441273]\n",
      "epoch:20 step:18921 [D loss: 0.246339, acc.: 57.81%] [G loss: 0.398115]\n",
      "epoch:20 step:18922 [D loss: 0.239045, acc.: 67.19%] [G loss: 0.399048]\n",
      "epoch:20 step:18923 [D loss: 0.251884, acc.: 58.59%] [G loss: 0.437970]\n",
      "epoch:20 step:18924 [D loss: 0.214696, acc.: 67.97%] [G loss: 0.450622]\n",
      "epoch:20 step:18925 [D loss: 0.221705, acc.: 64.06%] [G loss: 0.427154]\n",
      "epoch:20 step:18926 [D loss: 0.247177, acc.: 58.59%] [G loss: 0.415149]\n",
      "epoch:20 step:18927 [D loss: 0.238478, acc.: 64.06%] [G loss: 0.410281]\n",
      "epoch:20 step:18928 [D loss: 0.238773, acc.: 56.25%] [G loss: 0.408116]\n",
      "epoch:20 step:18929 [D loss: 0.228482, acc.: 58.59%] [G loss: 0.421998]\n",
      "epoch:20 step:18930 [D loss: 0.225554, acc.: 64.84%] [G loss: 0.410547]\n",
      "epoch:20 step:18931 [D loss: 0.216769, acc.: 65.62%] [G loss: 0.406136]\n",
      "epoch:20 step:18932 [D loss: 0.202639, acc.: 69.53%] [G loss: 0.411104]\n",
      "epoch:20 step:18933 [D loss: 0.224320, acc.: 65.62%] [G loss: 0.448663]\n",
      "epoch:20 step:18934 [D loss: 0.212283, acc.: 68.75%] [G loss: 0.446547]\n",
      "epoch:20 step:18935 [D loss: 0.223228, acc.: 65.62%] [G loss: 0.433101]\n",
      "epoch:20 step:18936 [D loss: 0.222770, acc.: 63.28%] [G loss: 0.420078]\n",
      "epoch:20 step:18937 [D loss: 0.196000, acc.: 67.97%] [G loss: 0.476685]\n",
      "epoch:20 step:18938 [D loss: 0.216913, acc.: 64.06%] [G loss: 0.433034]\n",
      "epoch:20 step:18939 [D loss: 0.214024, acc.: 61.72%] [G loss: 0.471025]\n",
      "epoch:20 step:18940 [D loss: 0.257915, acc.: 53.12%] [G loss: 0.420598]\n",
      "epoch:20 step:18941 [D loss: 0.235417, acc.: 60.16%] [G loss: 0.411006]\n",
      "epoch:20 step:18942 [D loss: 0.225326, acc.: 60.94%] [G loss: 0.453688]\n",
      "epoch:20 step:18943 [D loss: 0.286534, acc.: 50.78%] [G loss: 0.412124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18944 [D loss: 0.242650, acc.: 64.06%] [G loss: 0.425653]\n",
      "epoch:20 step:18945 [D loss: 0.219249, acc.: 67.97%] [G loss: 0.466397]\n",
      "epoch:20 step:18946 [D loss: 0.224636, acc.: 64.06%] [G loss: 0.418191]\n",
      "epoch:20 step:18947 [D loss: 0.219262, acc.: 64.06%] [G loss: 0.443646]\n",
      "epoch:20 step:18948 [D loss: 0.210783, acc.: 67.19%] [G loss: 0.471121]\n",
      "epoch:20 step:18949 [D loss: 0.203313, acc.: 71.09%] [G loss: 0.488038]\n",
      "epoch:20 step:18950 [D loss: 0.262012, acc.: 50.00%] [G loss: 0.478387]\n",
      "epoch:20 step:18951 [D loss: 0.250296, acc.: 58.59%] [G loss: 0.415510]\n",
      "epoch:20 step:18952 [D loss: 0.241673, acc.: 57.81%] [G loss: 0.383473]\n",
      "epoch:20 step:18953 [D loss: 0.230362, acc.: 61.72%] [G loss: 0.418616]\n",
      "epoch:20 step:18954 [D loss: 0.262687, acc.: 54.69%] [G loss: 0.392773]\n",
      "epoch:20 step:18955 [D loss: 0.246745, acc.: 56.25%] [G loss: 0.388874]\n",
      "epoch:20 step:18956 [D loss: 0.219084, acc.: 64.06%] [G loss: 0.383056]\n",
      "epoch:20 step:18957 [D loss: 0.258223, acc.: 53.12%] [G loss: 0.400846]\n",
      "epoch:20 step:18958 [D loss: 0.186419, acc.: 75.00%] [G loss: 0.464123]\n",
      "epoch:20 step:18959 [D loss: 0.210004, acc.: 63.28%] [G loss: 0.487877]\n",
      "epoch:20 step:18960 [D loss: 0.290095, acc.: 53.91%] [G loss: 0.455011]\n",
      "epoch:20 step:18961 [D loss: 0.212984, acc.: 63.28%] [G loss: 0.441177]\n",
      "epoch:20 step:18962 [D loss: 0.198499, acc.: 71.09%] [G loss: 0.441531]\n",
      "epoch:20 step:18963 [D loss: 0.200099, acc.: 71.09%] [G loss: 0.468640]\n",
      "epoch:20 step:18964 [D loss: 0.244895, acc.: 56.25%] [G loss: 0.421838]\n",
      "epoch:20 step:18965 [D loss: 0.243604, acc.: 57.81%] [G loss: 0.383984]\n",
      "epoch:20 step:18966 [D loss: 0.231227, acc.: 61.72%] [G loss: 0.419416]\n",
      "epoch:20 step:18967 [D loss: 0.211394, acc.: 68.75%] [G loss: 0.444635]\n",
      "epoch:20 step:18968 [D loss: 0.236360, acc.: 57.81%] [G loss: 0.438536]\n",
      "epoch:20 step:18969 [D loss: 0.218006, acc.: 63.28%] [G loss: 0.449831]\n",
      "epoch:20 step:18970 [D loss: 0.195886, acc.: 71.09%] [G loss: 0.448640]\n",
      "epoch:20 step:18971 [D loss: 0.175038, acc.: 78.91%] [G loss: 0.483697]\n",
      "epoch:20 step:18972 [D loss: 0.177154, acc.: 81.25%] [G loss: 0.524217]\n",
      "epoch:20 step:18973 [D loss: 0.250936, acc.: 51.56%] [G loss: 0.507259]\n",
      "epoch:20 step:18974 [D loss: 0.235332, acc.: 64.84%] [G loss: 0.434689]\n",
      "epoch:20 step:18975 [D loss: 0.229410, acc.: 59.38%] [G loss: 0.436090]\n",
      "epoch:20 step:18976 [D loss: 0.225644, acc.: 64.84%] [G loss: 0.424619]\n",
      "epoch:20 step:18977 [D loss: 0.233727, acc.: 60.94%] [G loss: 0.426820]\n",
      "epoch:20 step:18978 [D loss: 0.208028, acc.: 66.41%] [G loss: 0.448001]\n",
      "epoch:20 step:18979 [D loss: 0.209313, acc.: 64.84%] [G loss: 0.428402]\n",
      "epoch:20 step:18980 [D loss: 0.219122, acc.: 64.84%] [G loss: 0.396181]\n",
      "epoch:20 step:18981 [D loss: 0.188507, acc.: 74.22%] [G loss: 0.427074]\n",
      "epoch:20 step:18982 [D loss: 0.189686, acc.: 74.22%] [G loss: 0.444620]\n",
      "epoch:20 step:18983 [D loss: 0.216774, acc.: 64.06%] [G loss: 0.463324]\n",
      "epoch:20 step:18984 [D loss: 0.212817, acc.: 67.19%] [G loss: 0.424349]\n",
      "epoch:20 step:18985 [D loss: 0.194377, acc.: 68.75%] [G loss: 0.491323]\n",
      "epoch:20 step:18986 [D loss: 0.220360, acc.: 67.19%] [G loss: 0.459876]\n",
      "epoch:20 step:18987 [D loss: 0.213541, acc.: 64.06%] [G loss: 0.462377]\n",
      "epoch:20 step:18988 [D loss: 0.185750, acc.: 73.44%] [G loss: 0.469359]\n",
      "epoch:20 step:18989 [D loss: 0.266382, acc.: 53.12%] [G loss: 0.427127]\n",
      "epoch:20 step:18990 [D loss: 0.266182, acc.: 57.03%] [G loss: 0.428797]\n",
      "epoch:20 step:18991 [D loss: 0.229394, acc.: 60.16%] [G loss: 0.464197]\n",
      "epoch:20 step:18992 [D loss: 0.217087, acc.: 64.84%] [G loss: 0.442541]\n",
      "epoch:20 step:18993 [D loss: 0.247776, acc.: 57.81%] [G loss: 0.425190]\n",
      "epoch:20 step:18994 [D loss: 0.218102, acc.: 65.62%] [G loss: 0.435295]\n",
      "epoch:20 step:18995 [D loss: 0.244747, acc.: 53.91%] [G loss: 0.428467]\n",
      "epoch:20 step:18996 [D loss: 0.245623, acc.: 55.47%] [G loss: 0.396829]\n",
      "epoch:20 step:18997 [D loss: 0.229636, acc.: 60.94%] [G loss: 0.400808]\n",
      "epoch:20 step:18998 [D loss: 0.214295, acc.: 67.19%] [G loss: 0.417586]\n",
      "epoch:20 step:18999 [D loss: 0.206333, acc.: 62.50%] [G loss: 0.435671]\n",
      "epoch:20 step:19000 [D loss: 0.206697, acc.: 71.09%] [G loss: 0.440532]\n",
      "##############\n",
      "[2.38417104 1.81341471 6.20612147 4.79866016 3.51120478 5.58955554\n",
      " 4.44300304 4.73878419 4.28900528 3.80351593]\n",
      "##########\n",
      "epoch:20 step:19001 [D loss: 0.248545, acc.: 57.81%] [G loss: 0.425336]\n",
      "epoch:20 step:19002 [D loss: 0.203387, acc.: 66.41%] [G loss: 0.463490]\n",
      "epoch:20 step:19003 [D loss: 0.242394, acc.: 57.03%] [G loss: 0.460914]\n",
      "epoch:20 step:19004 [D loss: 0.214381, acc.: 64.84%] [G loss: 0.446788]\n",
      "epoch:20 step:19005 [D loss: 0.256961, acc.: 53.91%] [G loss: 0.426179]\n",
      "epoch:20 step:19006 [D loss: 0.244813, acc.: 57.03%] [G loss: 0.415035]\n",
      "epoch:20 step:19007 [D loss: 0.215764, acc.: 67.19%] [G loss: 0.432216]\n",
      "epoch:20 step:19008 [D loss: 0.222917, acc.: 62.50%] [G loss: 0.392281]\n",
      "epoch:20 step:19009 [D loss: 0.228950, acc.: 58.59%] [G loss: 0.437663]\n",
      "epoch:20 step:19010 [D loss: 0.215183, acc.: 64.06%] [G loss: 0.429413]\n",
      "epoch:20 step:19011 [D loss: 0.200986, acc.: 71.09%] [G loss: 0.459583]\n",
      "epoch:20 step:19012 [D loss: 0.227190, acc.: 67.19%] [G loss: 0.451007]\n",
      "epoch:20 step:19013 [D loss: 0.228466, acc.: 60.94%] [G loss: 0.468776]\n",
      "epoch:20 step:19014 [D loss: 0.186652, acc.: 69.53%] [G loss: 0.421925]\n",
      "epoch:20 step:19015 [D loss: 0.224873, acc.: 64.06%] [G loss: 0.446714]\n",
      "epoch:20 step:19016 [D loss: 0.185851, acc.: 75.78%] [G loss: 0.464730]\n",
      "epoch:20 step:19017 [D loss: 0.241904, acc.: 57.81%] [G loss: 0.400557]\n",
      "epoch:20 step:19018 [D loss: 0.239220, acc.: 57.81%] [G loss: 0.393502]\n",
      "epoch:20 step:19019 [D loss: 0.210552, acc.: 70.31%] [G loss: 0.438642]\n",
      "epoch:20 step:19020 [D loss: 0.201535, acc.: 65.62%] [G loss: 0.431785]\n",
      "epoch:20 step:19021 [D loss: 0.251324, acc.: 60.94%] [G loss: 0.460625]\n",
      "epoch:20 step:19022 [D loss: 0.231270, acc.: 63.28%] [G loss: 0.428194]\n",
      "epoch:20 step:19023 [D loss: 0.215439, acc.: 65.62%] [G loss: 0.433732]\n",
      "epoch:20 step:19024 [D loss: 0.230409, acc.: 62.50%] [G loss: 0.405080]\n",
      "epoch:20 step:19025 [D loss: 0.220152, acc.: 62.50%] [G loss: 0.414067]\n",
      "epoch:20 step:19026 [D loss: 0.223011, acc.: 65.62%] [G loss: 0.442593]\n",
      "epoch:20 step:19027 [D loss: 0.238216, acc.: 62.50%] [G loss: 0.412794]\n",
      "epoch:20 step:19028 [D loss: 0.212832, acc.: 65.62%] [G loss: 0.453603]\n",
      "epoch:20 step:19029 [D loss: 0.191793, acc.: 75.00%] [G loss: 0.426956]\n",
      "epoch:20 step:19030 [D loss: 0.205835, acc.: 67.97%] [G loss: 0.441725]\n",
      "epoch:20 step:19031 [D loss: 0.254246, acc.: 55.47%] [G loss: 0.417816]\n",
      "epoch:20 step:19032 [D loss: 0.216677, acc.: 64.06%] [G loss: 0.406506]\n",
      "epoch:20 step:19033 [D loss: 0.221739, acc.: 67.19%] [G loss: 0.401260]\n",
      "epoch:20 step:19034 [D loss: 0.244184, acc.: 50.00%] [G loss: 0.411045]\n",
      "epoch:20 step:19035 [D loss: 0.227349, acc.: 63.28%] [G loss: 0.388752]\n",
      "epoch:20 step:19036 [D loss: 0.215493, acc.: 64.06%] [G loss: 0.416052]\n",
      "epoch:20 step:19037 [D loss: 0.216776, acc.: 64.06%] [G loss: 0.424391]\n",
      "epoch:20 step:19038 [D loss: 0.202873, acc.: 74.22%] [G loss: 0.432129]\n",
      "epoch:20 step:19039 [D loss: 0.204153, acc.: 67.97%] [G loss: 0.452873]\n",
      "epoch:20 step:19040 [D loss: 0.214055, acc.: 63.28%] [G loss: 0.474013]\n",
      "epoch:20 step:19041 [D loss: 0.250269, acc.: 57.81%] [G loss: 0.434847]\n",
      "epoch:20 step:19042 [D loss: 0.223845, acc.: 64.06%] [G loss: 0.434621]\n",
      "epoch:20 step:19043 [D loss: 0.223149, acc.: 67.97%] [G loss: 0.446288]\n",
      "epoch:20 step:19044 [D loss: 0.209321, acc.: 71.09%] [G loss: 0.441175]\n",
      "epoch:20 step:19045 [D loss: 0.203436, acc.: 71.88%] [G loss: 0.446266]\n",
      "epoch:20 step:19046 [D loss: 0.225691, acc.: 59.38%] [G loss: 0.405358]\n",
      "epoch:20 step:19047 [D loss: 0.235954, acc.: 64.06%] [G loss: 0.448121]\n",
      "epoch:20 step:19048 [D loss: 0.260984, acc.: 54.69%] [G loss: 0.433218]\n",
      "epoch:20 step:19049 [D loss: 0.215457, acc.: 68.75%] [G loss: 0.475410]\n",
      "epoch:20 step:19050 [D loss: 0.227229, acc.: 58.59%] [G loss: 0.459715]\n",
      "epoch:20 step:19051 [D loss: 0.227280, acc.: 62.50%] [G loss: 0.407474]\n",
      "epoch:20 step:19052 [D loss: 0.178881, acc.: 74.22%] [G loss: 0.461570]\n",
      "epoch:20 step:19053 [D loss: 0.189606, acc.: 68.75%] [G loss: 0.477593]\n",
      "epoch:20 step:19054 [D loss: 0.173798, acc.: 71.88%] [G loss: 0.470495]\n",
      "epoch:20 step:19055 [D loss: 0.207058, acc.: 68.75%] [G loss: 0.504890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19056 [D loss: 0.288573, acc.: 52.34%] [G loss: 0.430891]\n",
      "epoch:20 step:19057 [D loss: 0.237154, acc.: 59.38%] [G loss: 0.418933]\n",
      "epoch:20 step:19058 [D loss: 0.233247, acc.: 55.47%] [G loss: 0.391657]\n",
      "epoch:20 step:19059 [D loss: 0.214145, acc.: 66.41%] [G loss: 0.444493]\n",
      "epoch:20 step:19060 [D loss: 0.249104, acc.: 54.69%] [G loss: 0.413507]\n",
      "epoch:20 step:19061 [D loss: 0.217894, acc.: 62.50%] [G loss: 0.436089]\n",
      "epoch:20 step:19062 [D loss: 0.229990, acc.: 60.16%] [G loss: 0.447904]\n",
      "epoch:20 step:19063 [D loss: 0.251354, acc.: 54.69%] [G loss: 0.440196]\n",
      "epoch:20 step:19064 [D loss: 0.217739, acc.: 67.19%] [G loss: 0.436015]\n",
      "epoch:20 step:19065 [D loss: 0.249768, acc.: 57.81%] [G loss: 0.456844]\n",
      "epoch:20 step:19066 [D loss: 0.233560, acc.: 61.72%] [G loss: 0.381666]\n",
      "epoch:20 step:19067 [D loss: 0.227106, acc.: 61.72%] [G loss: 0.426228]\n",
      "epoch:20 step:19068 [D loss: 0.225083, acc.: 60.94%] [G loss: 0.448870]\n",
      "epoch:20 step:19069 [D loss: 0.212041, acc.: 65.62%] [G loss: 0.442721]\n",
      "epoch:20 step:19070 [D loss: 0.232778, acc.: 57.81%] [G loss: 0.439111]\n",
      "epoch:20 step:19071 [D loss: 0.216733, acc.: 65.62%] [G loss: 0.421892]\n",
      "epoch:20 step:19072 [D loss: 0.219342, acc.: 68.75%] [G loss: 0.422207]\n",
      "epoch:20 step:19073 [D loss: 0.205094, acc.: 71.88%] [G loss: 0.445050]\n",
      "epoch:20 step:19074 [D loss: 0.211622, acc.: 64.84%] [G loss: 0.444265]\n",
      "epoch:20 step:19075 [D loss: 0.214999, acc.: 65.62%] [G loss: 0.447983]\n",
      "epoch:20 step:19076 [D loss: 0.209012, acc.: 67.19%] [G loss: 0.461965]\n",
      "epoch:20 step:19077 [D loss: 0.243373, acc.: 63.28%] [G loss: 0.457058]\n",
      "epoch:20 step:19078 [D loss: 0.232364, acc.: 59.38%] [G loss: 0.428849]\n",
      "epoch:20 step:19079 [D loss: 0.209805, acc.: 71.09%] [G loss: 0.439635]\n",
      "epoch:20 step:19080 [D loss: 0.196669, acc.: 67.19%] [G loss: 0.447180]\n",
      "epoch:20 step:19081 [D loss: 0.274872, acc.: 55.47%] [G loss: 0.400000]\n",
      "epoch:20 step:19082 [D loss: 0.241349, acc.: 61.72%] [G loss: 0.426200]\n",
      "epoch:20 step:19083 [D loss: 0.213014, acc.: 65.62%] [G loss: 0.414181]\n",
      "epoch:20 step:19084 [D loss: 0.222871, acc.: 64.06%] [G loss: 0.481188]\n",
      "epoch:20 step:19085 [D loss: 0.203162, acc.: 64.84%] [G loss: 0.485382]\n",
      "epoch:20 step:19086 [D loss: 0.191079, acc.: 70.31%] [G loss: 0.487853]\n",
      "epoch:20 step:19087 [D loss: 0.181169, acc.: 71.88%] [G loss: 0.515310]\n",
      "epoch:20 step:19088 [D loss: 0.310849, acc.: 50.00%] [G loss: 0.400927]\n",
      "epoch:20 step:19089 [D loss: 0.290552, acc.: 45.31%] [G loss: 0.408154]\n",
      "epoch:20 step:19090 [D loss: 0.236979, acc.: 58.59%] [G loss: 0.396978]\n",
      "epoch:20 step:19091 [D loss: 0.230775, acc.: 61.72%] [G loss: 0.427181]\n",
      "epoch:20 step:19092 [D loss: 0.214704, acc.: 65.62%] [G loss: 0.448010]\n",
      "epoch:20 step:19093 [D loss: 0.206485, acc.: 64.84%] [G loss: 0.467966]\n",
      "epoch:20 step:19094 [D loss: 0.170764, acc.: 75.00%] [G loss: 0.496499]\n",
      "epoch:20 step:19095 [D loss: 0.253579, acc.: 59.38%] [G loss: 0.444480]\n",
      "epoch:20 step:19096 [D loss: 0.215076, acc.: 60.94%] [G loss: 0.454323]\n",
      "epoch:20 step:19097 [D loss: 0.223203, acc.: 64.06%] [G loss: 0.397465]\n",
      "epoch:20 step:19098 [D loss: 0.206583, acc.: 63.28%] [G loss: 0.411125]\n",
      "epoch:20 step:19099 [D loss: 0.189710, acc.: 68.75%] [G loss: 0.449273]\n",
      "epoch:20 step:19100 [D loss: 0.198349, acc.: 67.19%] [G loss: 0.454829]\n",
      "epoch:20 step:19101 [D loss: 0.208411, acc.: 64.84%] [G loss: 0.449636]\n",
      "epoch:20 step:19102 [D loss: 0.223627, acc.: 61.72%] [G loss: 0.434624]\n",
      "epoch:20 step:19103 [D loss: 0.240571, acc.: 53.91%] [G loss: 0.373748]\n",
      "epoch:20 step:19104 [D loss: 0.204562, acc.: 64.84%] [G loss: 0.431940]\n",
      "epoch:20 step:19105 [D loss: 0.224899, acc.: 60.16%] [G loss: 0.444443]\n",
      "epoch:20 step:19106 [D loss: 0.252629, acc.: 56.25%] [G loss: 0.400602]\n",
      "epoch:20 step:19107 [D loss: 0.206885, acc.: 69.53%] [G loss: 0.458086]\n",
      "epoch:20 step:19108 [D loss: 0.239446, acc.: 57.03%] [G loss: 0.443632]\n",
      "epoch:20 step:19109 [D loss: 0.211754, acc.: 67.97%] [G loss: 0.394475]\n",
      "epoch:20 step:19110 [D loss: 0.201688, acc.: 70.31%] [G loss: 0.455523]\n",
      "epoch:20 step:19111 [D loss: 0.197448, acc.: 71.09%] [G loss: 0.451973]\n",
      "epoch:20 step:19112 [D loss: 0.208375, acc.: 67.19%] [G loss: 0.447179]\n",
      "epoch:20 step:19113 [D loss: 0.231321, acc.: 66.41%] [G loss: 0.417152]\n",
      "epoch:20 step:19114 [D loss: 0.213456, acc.: 65.62%] [G loss: 0.441165]\n",
      "epoch:20 step:19115 [D loss: 0.229980, acc.: 60.94%] [G loss: 0.470683]\n",
      "epoch:20 step:19116 [D loss: 0.236356, acc.: 63.28%] [G loss: 0.430367]\n",
      "epoch:20 step:19117 [D loss: 0.263302, acc.: 49.22%] [G loss: 0.393020]\n",
      "epoch:20 step:19118 [D loss: 0.200249, acc.: 69.53%] [G loss: 0.439532]\n",
      "epoch:20 step:19119 [D loss: 0.234565, acc.: 60.94%] [G loss: 0.438873]\n",
      "epoch:20 step:19120 [D loss: 0.281442, acc.: 47.66%] [G loss: 0.399441]\n",
      "epoch:20 step:19121 [D loss: 0.216363, acc.: 64.84%] [G loss: 0.428323]\n",
      "epoch:20 step:19122 [D loss: 0.231375, acc.: 60.94%] [G loss: 0.423405]\n",
      "epoch:20 step:19123 [D loss: 0.194160, acc.: 71.09%] [G loss: 0.452283]\n",
      "epoch:20 step:19124 [D loss: 0.206967, acc.: 64.06%] [G loss: 0.426247]\n",
      "epoch:20 step:19125 [D loss: 0.200336, acc.: 66.41%] [G loss: 0.448373]\n",
      "epoch:20 step:19126 [D loss: 0.228767, acc.: 60.94%] [G loss: 0.433547]\n",
      "epoch:20 step:19127 [D loss: 0.221101, acc.: 63.28%] [G loss: 0.448286]\n",
      "epoch:20 step:19128 [D loss: 0.189881, acc.: 71.88%] [G loss: 0.433251]\n",
      "epoch:20 step:19129 [D loss: 0.223084, acc.: 60.16%] [G loss: 0.413968]\n",
      "epoch:20 step:19130 [D loss: 0.264389, acc.: 52.34%] [G loss: 0.383478]\n",
      "epoch:20 step:19131 [D loss: 0.219735, acc.: 63.28%] [G loss: 0.416001]\n",
      "epoch:20 step:19132 [D loss: 0.226398, acc.: 60.94%] [G loss: 0.400256]\n",
      "epoch:20 step:19133 [D loss: 0.236358, acc.: 59.38%] [G loss: 0.411464]\n",
      "epoch:20 step:19134 [D loss: 0.226948, acc.: 67.97%] [G loss: 0.419567]\n",
      "epoch:20 step:19135 [D loss: 0.219679, acc.: 62.50%] [G loss: 0.385901]\n",
      "epoch:20 step:19136 [D loss: 0.240402, acc.: 57.81%] [G loss: 0.455730]\n",
      "epoch:20 step:19137 [D loss: 0.238721, acc.: 57.81%] [G loss: 0.450837]\n",
      "epoch:20 step:19138 [D loss: 0.211737, acc.: 60.94%] [G loss: 0.479730]\n",
      "epoch:20 step:19139 [D loss: 0.208505, acc.: 68.75%] [G loss: 0.456148]\n",
      "epoch:20 step:19140 [D loss: 0.250615, acc.: 53.91%] [G loss: 0.459732]\n",
      "epoch:20 step:19141 [D loss: 0.241724, acc.: 58.59%] [G loss: 0.386308]\n",
      "epoch:20 step:19142 [D loss: 0.207468, acc.: 69.53%] [G loss: 0.430237]\n",
      "epoch:20 step:19143 [D loss: 0.231782, acc.: 64.84%] [G loss: 0.412630]\n",
      "epoch:20 step:19144 [D loss: 0.221032, acc.: 66.41%] [G loss: 0.409432]\n",
      "epoch:20 step:19145 [D loss: 0.175869, acc.: 80.47%] [G loss: 0.446405]\n",
      "epoch:20 step:19146 [D loss: 0.188903, acc.: 73.44%] [G loss: 0.447625]\n",
      "epoch:20 step:19147 [D loss: 0.223640, acc.: 67.19%] [G loss: 0.456787]\n",
      "epoch:20 step:19148 [D loss: 0.218235, acc.: 64.06%] [G loss: 0.466932]\n",
      "epoch:20 step:19149 [D loss: 0.238656, acc.: 57.81%] [G loss: 0.429141]\n",
      "epoch:20 step:19150 [D loss: 0.259726, acc.: 53.91%] [G loss: 0.403134]\n",
      "epoch:20 step:19151 [D loss: 0.230858, acc.: 62.50%] [G loss: 0.412774]\n",
      "epoch:20 step:19152 [D loss: 0.228609, acc.: 62.50%] [G loss: 0.377660]\n",
      "epoch:20 step:19153 [D loss: 0.248724, acc.: 56.25%] [G loss: 0.398149]\n",
      "epoch:20 step:19154 [D loss: 0.216781, acc.: 59.38%] [G loss: 0.411057]\n",
      "epoch:20 step:19155 [D loss: 0.211483, acc.: 64.84%] [G loss: 0.457478]\n",
      "epoch:20 step:19156 [D loss: 0.190387, acc.: 73.44%] [G loss: 0.438943]\n",
      "epoch:20 step:19157 [D loss: 0.240166, acc.: 60.94%] [G loss: 0.431808]\n",
      "epoch:20 step:19158 [D loss: 0.229823, acc.: 61.72%] [G loss: 0.398793]\n",
      "epoch:20 step:19159 [D loss: 0.244480, acc.: 57.03%] [G loss: 0.436699]\n",
      "epoch:20 step:19160 [D loss: 0.244250, acc.: 58.59%] [G loss: 0.503179]\n",
      "epoch:20 step:19161 [D loss: 0.264497, acc.: 51.56%] [G loss: 0.428242]\n",
      "epoch:20 step:19162 [D loss: 0.242733, acc.: 52.34%] [G loss: 0.394395]\n",
      "epoch:20 step:19163 [D loss: 0.227957, acc.: 62.50%] [G loss: 0.421171]\n",
      "epoch:20 step:19164 [D loss: 0.255227, acc.: 55.47%] [G loss: 0.398159]\n",
      "epoch:20 step:19165 [D loss: 0.212715, acc.: 69.53%] [G loss: 0.379177]\n",
      "epoch:20 step:19166 [D loss: 0.235977, acc.: 59.38%] [G loss: 0.385283]\n",
      "epoch:20 step:19167 [D loss: 0.201664, acc.: 66.41%] [G loss: 0.410442]\n",
      "epoch:20 step:19168 [D loss: 0.207064, acc.: 66.41%] [G loss: 0.435793]\n",
      "epoch:20 step:19169 [D loss: 0.212316, acc.: 67.19%] [G loss: 0.474124]\n",
      "epoch:20 step:19170 [D loss: 0.210384, acc.: 66.41%] [G loss: 0.523844]\n",
      "epoch:20 step:19171 [D loss: 0.226094, acc.: 64.06%] [G loss: 0.468980]\n",
      "epoch:20 step:19172 [D loss: 0.233508, acc.: 63.28%] [G loss: 0.397926]\n",
      "epoch:20 step:19173 [D loss: 0.212397, acc.: 67.19%] [G loss: 0.403464]\n",
      "epoch:20 step:19174 [D loss: 0.210746, acc.: 71.09%] [G loss: 0.445446]\n",
      "epoch:20 step:19175 [D loss: 0.204756, acc.: 69.53%] [G loss: 0.477018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19176 [D loss: 0.206508, acc.: 72.66%] [G loss: 0.450535]\n",
      "epoch:20 step:19177 [D loss: 0.281706, acc.: 48.44%] [G loss: 0.439860]\n",
      "epoch:20 step:19178 [D loss: 0.217871, acc.: 62.50%] [G loss: 0.442488]\n",
      "epoch:20 step:19179 [D loss: 0.196194, acc.: 66.41%] [G loss: 0.434470]\n",
      "epoch:20 step:19180 [D loss: 0.218400, acc.: 62.50%] [G loss: 0.428578]\n",
      "epoch:20 step:19181 [D loss: 0.220442, acc.: 65.62%] [G loss: 0.446196]\n",
      "epoch:20 step:19182 [D loss: 0.227061, acc.: 64.06%] [G loss: 0.439311]\n",
      "epoch:20 step:19183 [D loss: 0.271644, acc.: 55.47%] [G loss: 0.432960]\n",
      "epoch:20 step:19184 [D loss: 0.233296, acc.: 58.59%] [G loss: 0.438949]\n",
      "epoch:20 step:19185 [D loss: 0.217921, acc.: 65.62%] [G loss: 0.449599]\n",
      "epoch:20 step:19186 [D loss: 0.221729, acc.: 60.94%] [G loss: 0.429383]\n",
      "epoch:20 step:19187 [D loss: 0.203121, acc.: 69.53%] [G loss: 0.467828]\n",
      "epoch:20 step:19188 [D loss: 0.258860, acc.: 50.78%] [G loss: 0.415980]\n",
      "epoch:20 step:19189 [D loss: 0.216501, acc.: 66.41%] [G loss: 0.440905]\n",
      "epoch:20 step:19190 [D loss: 0.210301, acc.: 65.62%] [G loss: 0.433181]\n",
      "epoch:20 step:19191 [D loss: 0.204271, acc.: 70.31%] [G loss: 0.447651]\n",
      "epoch:20 step:19192 [D loss: 0.196477, acc.: 69.53%] [G loss: 0.447064]\n",
      "epoch:20 step:19193 [D loss: 0.214467, acc.: 65.62%] [G loss: 0.476614]\n",
      "epoch:20 step:19194 [D loss: 0.211046, acc.: 65.62%] [G loss: 0.481672]\n",
      "epoch:20 step:19195 [D loss: 0.244367, acc.: 62.50%] [G loss: 0.415513]\n",
      "epoch:20 step:19196 [D loss: 0.218248, acc.: 62.50%] [G loss: 0.446841]\n",
      "epoch:20 step:19197 [D loss: 0.226050, acc.: 64.84%] [G loss: 0.417291]\n",
      "epoch:20 step:19198 [D loss: 0.267452, acc.: 56.25%] [G loss: 0.416928]\n",
      "epoch:20 step:19199 [D loss: 0.226983, acc.: 63.28%] [G loss: 0.418998]\n",
      "epoch:20 step:19200 [D loss: 0.252005, acc.: 57.81%] [G loss: 0.380965]\n",
      "##############\n",
      "[2.33766682 1.99987582 6.0676979  4.63700479 3.47063208 5.53907596\n",
      " 4.35725884 4.76836698 4.31945797 3.92773458]\n",
      "##########\n",
      "epoch:20 step:19201 [D loss: 0.246957, acc.: 58.59%] [G loss: 0.407326]\n",
      "epoch:20 step:19202 [D loss: 0.267776, acc.: 45.31%] [G loss: 0.389460]\n",
      "epoch:20 step:19203 [D loss: 0.246242, acc.: 56.25%] [G loss: 0.410720]\n",
      "epoch:20 step:19204 [D loss: 0.217357, acc.: 65.62%] [G loss: 0.447822]\n",
      "epoch:20 step:19205 [D loss: 0.230604, acc.: 57.03%] [G loss: 0.402682]\n",
      "epoch:20 step:19206 [D loss: 0.193144, acc.: 71.88%] [G loss: 0.409133]\n",
      "epoch:20 step:19207 [D loss: 0.218686, acc.: 67.97%] [G loss: 0.395230]\n",
      "epoch:20 step:19208 [D loss: 0.220399, acc.: 63.28%] [G loss: 0.447528]\n",
      "epoch:20 step:19209 [D loss: 0.205050, acc.: 70.31%] [G loss: 0.451658]\n",
      "epoch:20 step:19210 [D loss: 0.220102, acc.: 63.28%] [G loss: 0.468208]\n",
      "epoch:20 step:19211 [D loss: 0.178893, acc.: 76.56%] [G loss: 0.480674]\n",
      "epoch:20 step:19212 [D loss: 0.197055, acc.: 70.31%] [G loss: 0.480532]\n",
      "epoch:20 step:19213 [D loss: 0.252385, acc.: 53.91%] [G loss: 0.443551]\n",
      "epoch:20 step:19214 [D loss: 0.204148, acc.: 67.19%] [G loss: 0.465786]\n",
      "epoch:20 step:19215 [D loss: 0.200311, acc.: 70.31%] [G loss: 0.435152]\n",
      "epoch:20 step:19216 [D loss: 0.234618, acc.: 58.59%] [G loss: 0.467952]\n",
      "epoch:20 step:19217 [D loss: 0.255664, acc.: 50.78%] [G loss: 0.420899]\n",
      "epoch:20 step:19218 [D loss: 0.235336, acc.: 62.50%] [G loss: 0.417641]\n",
      "epoch:20 step:19219 [D loss: 0.220197, acc.: 64.84%] [G loss: 0.408729]\n",
      "epoch:20 step:19220 [D loss: 0.201859, acc.: 71.88%] [G loss: 0.424780]\n",
      "epoch:20 step:19221 [D loss: 0.180278, acc.: 77.34%] [G loss: 0.453467]\n",
      "epoch:20 step:19222 [D loss: 0.260560, acc.: 52.34%] [G loss: 0.407801]\n",
      "epoch:20 step:19223 [D loss: 0.224056, acc.: 65.62%] [G loss: 0.434712]\n",
      "epoch:20 step:19224 [D loss: 0.208424, acc.: 70.31%] [G loss: 0.431240]\n",
      "epoch:20 step:19225 [D loss: 0.199552, acc.: 71.88%] [G loss: 0.458963]\n",
      "epoch:20 step:19226 [D loss: 0.243147, acc.: 55.47%] [G loss: 0.421897]\n",
      "epoch:20 step:19227 [D loss: 0.232097, acc.: 60.16%] [G loss: 0.432454]\n",
      "epoch:20 step:19228 [D loss: 0.206956, acc.: 65.62%] [G loss: 0.425420]\n",
      "epoch:20 step:19229 [D loss: 0.239935, acc.: 59.38%] [G loss: 0.423053]\n",
      "epoch:20 step:19230 [D loss: 0.248197, acc.: 56.25%] [G loss: 0.416957]\n",
      "epoch:20 step:19231 [D loss: 0.229317, acc.: 68.75%] [G loss: 0.447328]\n",
      "epoch:20 step:19232 [D loss: 0.232325, acc.: 60.94%] [G loss: 0.435326]\n",
      "epoch:20 step:19233 [D loss: 0.229207, acc.: 60.94%] [G loss: 0.403518]\n",
      "epoch:20 step:19234 [D loss: 0.209094, acc.: 68.75%] [G loss: 0.465406]\n",
      "epoch:20 step:19235 [D loss: 0.203453, acc.: 67.19%] [G loss: 0.424172]\n",
      "epoch:20 step:19236 [D loss: 0.212517, acc.: 65.62%] [G loss: 0.450275]\n",
      "epoch:20 step:19237 [D loss: 0.214116, acc.: 67.19%] [G loss: 0.433018]\n",
      "epoch:20 step:19238 [D loss: 0.203377, acc.: 68.75%] [G loss: 0.444624]\n",
      "epoch:20 step:19239 [D loss: 0.182847, acc.: 76.56%] [G loss: 0.448836]\n",
      "epoch:20 step:19240 [D loss: 0.261730, acc.: 56.25%] [G loss: 0.435849]\n",
      "epoch:20 step:19241 [D loss: 0.291669, acc.: 50.78%] [G loss: 0.409445]\n",
      "epoch:20 step:19242 [D loss: 0.235398, acc.: 59.38%] [G loss: 0.427564]\n",
      "epoch:20 step:19243 [D loss: 0.223977, acc.: 60.94%] [G loss: 0.437942]\n",
      "epoch:20 step:19244 [D loss: 0.194547, acc.: 75.00%] [G loss: 0.426620]\n",
      "epoch:20 step:19245 [D loss: 0.203778, acc.: 64.06%] [G loss: 0.442499]\n",
      "epoch:20 step:19246 [D loss: 0.236658, acc.: 58.59%] [G loss: 0.488710]\n",
      "epoch:20 step:19247 [D loss: 0.194591, acc.: 69.53%] [G loss: 0.491367]\n",
      "epoch:20 step:19248 [D loss: 0.187571, acc.: 75.78%] [G loss: 0.487707]\n",
      "epoch:20 step:19249 [D loss: 0.256467, acc.: 55.47%] [G loss: 0.439187]\n",
      "epoch:20 step:19250 [D loss: 0.251630, acc.: 59.38%] [G loss: 0.423128]\n",
      "epoch:20 step:19251 [D loss: 0.212228, acc.: 66.41%] [G loss: 0.433691]\n",
      "epoch:20 step:19252 [D loss: 0.231403, acc.: 60.94%] [G loss: 0.422858]\n",
      "epoch:20 step:19253 [D loss: 0.215042, acc.: 65.62%] [G loss: 0.415384]\n",
      "epoch:20 step:19254 [D loss: 0.199260, acc.: 72.66%] [G loss: 0.431718]\n",
      "epoch:20 step:19255 [D loss: 0.205769, acc.: 69.53%] [G loss: 0.457475]\n",
      "epoch:20 step:19256 [D loss: 0.214555, acc.: 71.88%] [G loss: 0.424847]\n",
      "epoch:20 step:19257 [D loss: 0.242723, acc.: 61.72%] [G loss: 0.436693]\n",
      "epoch:20 step:19258 [D loss: 0.237762, acc.: 56.25%] [G loss: 0.414542]\n",
      "epoch:20 step:19259 [D loss: 0.187439, acc.: 73.44%] [G loss: 0.479640]\n",
      "epoch:20 step:19260 [D loss: 0.218880, acc.: 64.06%] [G loss: 0.434919]\n",
      "epoch:20 step:19261 [D loss: 0.211699, acc.: 65.62%] [G loss: 0.437384]\n",
      "epoch:20 step:19262 [D loss: 0.212856, acc.: 70.31%] [G loss: 0.435737]\n",
      "epoch:20 step:19263 [D loss: 0.217852, acc.: 61.72%] [G loss: 0.456782]\n",
      "epoch:20 step:19264 [D loss: 0.225173, acc.: 66.41%] [G loss: 0.445865]\n",
      "epoch:20 step:19265 [D loss: 0.222777, acc.: 61.72%] [G loss: 0.430458]\n",
      "epoch:20 step:19266 [D loss: 0.223130, acc.: 63.28%] [G loss: 0.399416]\n",
      "epoch:20 step:19267 [D loss: 0.223443, acc.: 60.94%] [G loss: 0.440336]\n",
      "epoch:20 step:19268 [D loss: 0.264107, acc.: 58.59%] [G loss: 0.419389]\n",
      "epoch:20 step:19269 [D loss: 0.265653, acc.: 53.12%] [G loss: 0.447702]\n",
      "epoch:20 step:19270 [D loss: 0.228100, acc.: 60.16%] [G loss: 0.438237]\n",
      "epoch:20 step:19271 [D loss: 0.258395, acc.: 53.12%] [G loss: 0.400390]\n",
      "epoch:20 step:19272 [D loss: 0.228574, acc.: 61.72%] [G loss: 0.413026]\n",
      "epoch:20 step:19273 [D loss: 0.224508, acc.: 64.06%] [G loss: 0.450427]\n",
      "epoch:20 step:19274 [D loss: 0.219922, acc.: 64.06%] [G loss: 0.473049]\n",
      "epoch:20 step:19275 [D loss: 0.242348, acc.: 59.38%] [G loss: 0.436058]\n",
      "epoch:20 step:19276 [D loss: 0.240222, acc.: 59.38%] [G loss: 0.369029]\n",
      "epoch:20 step:19277 [D loss: 0.230422, acc.: 63.28%] [G loss: 0.435012]\n",
      "epoch:20 step:19278 [D loss: 0.258374, acc.: 54.69%] [G loss: 0.434574]\n",
      "epoch:20 step:19279 [D loss: 0.209559, acc.: 69.53%] [G loss: 0.456016]\n",
      "epoch:20 step:19280 [D loss: 0.232426, acc.: 63.28%] [G loss: 0.430159]\n",
      "epoch:20 step:19281 [D loss: 0.228495, acc.: 60.94%] [G loss: 0.397626]\n",
      "epoch:20 step:19282 [D loss: 0.267976, acc.: 46.88%] [G loss: 0.413110]\n",
      "epoch:20 step:19283 [D loss: 0.239893, acc.: 58.59%] [G loss: 0.408090]\n",
      "epoch:20 step:19284 [D loss: 0.227801, acc.: 64.06%] [G loss: 0.387467]\n",
      "epoch:20 step:19285 [D loss: 0.221315, acc.: 64.06%] [G loss: 0.453825]\n",
      "epoch:20 step:19286 [D loss: 0.208933, acc.: 62.50%] [G loss: 0.420242]\n",
      "epoch:20 step:19287 [D loss: 0.224682, acc.: 63.28%] [G loss: 0.433671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19288 [D loss: 0.198327, acc.: 71.09%] [G loss: 0.424980]\n",
      "epoch:20 step:19289 [D loss: 0.214137, acc.: 63.28%] [G loss: 0.442172]\n",
      "epoch:20 step:19290 [D loss: 0.190960, acc.: 72.66%] [G loss: 0.453546]\n",
      "epoch:20 step:19291 [D loss: 0.209518, acc.: 70.31%] [G loss: 0.468877]\n",
      "epoch:20 step:19292 [D loss: 0.204000, acc.: 67.97%] [G loss: 0.455106]\n",
      "epoch:20 step:19293 [D loss: 0.256108, acc.: 57.81%] [G loss: 0.469470]\n",
      "epoch:20 step:19294 [D loss: 0.201511, acc.: 68.75%] [G loss: 0.455661]\n",
      "epoch:20 step:19295 [D loss: 0.220777, acc.: 64.06%] [G loss: 0.439560]\n",
      "epoch:20 step:19296 [D loss: 0.227343, acc.: 60.94%] [G loss: 0.453535]\n",
      "epoch:20 step:19297 [D loss: 0.197431, acc.: 70.31%] [G loss: 0.503881]\n",
      "epoch:20 step:19298 [D loss: 0.232774, acc.: 65.62%] [G loss: 0.415052]\n",
      "epoch:20 step:19299 [D loss: 0.262349, acc.: 57.03%] [G loss: 0.408902]\n",
      "epoch:20 step:19300 [D loss: 0.245788, acc.: 60.16%] [G loss: 0.415393]\n",
      "epoch:20 step:19301 [D loss: 0.214983, acc.: 63.28%] [G loss: 0.421980]\n",
      "epoch:20 step:19302 [D loss: 0.216021, acc.: 68.75%] [G loss: 0.451537]\n",
      "epoch:20 step:19303 [D loss: 0.191294, acc.: 71.09%] [G loss: 0.427858]\n",
      "epoch:20 step:19304 [D loss: 0.164395, acc.: 81.25%] [G loss: 0.491236]\n",
      "epoch:20 step:19305 [D loss: 0.241875, acc.: 60.16%] [G loss: 0.478813]\n",
      "epoch:20 step:19306 [D loss: 0.279782, acc.: 47.66%] [G loss: 0.482598]\n",
      "epoch:20 step:19307 [D loss: 0.191640, acc.: 75.78%] [G loss: 0.500445]\n",
      "epoch:20 step:19308 [D loss: 0.221465, acc.: 64.84%] [G loss: 0.493376]\n",
      "epoch:20 step:19309 [D loss: 0.266797, acc.: 52.34%] [G loss: 0.379428]\n",
      "epoch:20 step:19310 [D loss: 0.226840, acc.: 61.72%] [G loss: 0.426041]\n",
      "epoch:20 step:19311 [D loss: 0.226340, acc.: 60.16%] [G loss: 0.373703]\n",
      "epoch:20 step:19312 [D loss: 0.215146, acc.: 62.50%] [G loss: 0.425452]\n",
      "epoch:20 step:19313 [D loss: 0.221999, acc.: 64.84%] [G loss: 0.409794]\n",
      "epoch:20 step:19314 [D loss: 0.196912, acc.: 67.19%] [G loss: 0.471644]\n",
      "epoch:20 step:19315 [D loss: 0.199252, acc.: 68.75%] [G loss: 0.522396]\n",
      "epoch:20 step:19316 [D loss: 0.233229, acc.: 60.16%] [G loss: 0.452193]\n",
      "epoch:20 step:19317 [D loss: 0.214670, acc.: 64.06%] [G loss: 0.395034]\n",
      "epoch:20 step:19318 [D loss: 0.250923, acc.: 55.47%] [G loss: 0.372429]\n",
      "epoch:20 step:19319 [D loss: 0.217701, acc.: 62.50%] [G loss: 0.402350]\n",
      "epoch:20 step:19320 [D loss: 0.230482, acc.: 59.38%] [G loss: 0.432119]\n",
      "epoch:20 step:19321 [D loss: 0.206988, acc.: 69.53%] [G loss: 0.435335]\n",
      "epoch:20 step:19322 [D loss: 0.210806, acc.: 64.06%] [G loss: 0.439014]\n",
      "epoch:20 step:19323 [D loss: 0.231846, acc.: 61.72%] [G loss: 0.435862]\n",
      "epoch:20 step:19324 [D loss: 0.236712, acc.: 53.91%] [G loss: 0.434507]\n",
      "epoch:20 step:19325 [D loss: 0.224341, acc.: 57.81%] [G loss: 0.475529]\n",
      "epoch:20 step:19326 [D loss: 0.241417, acc.: 53.91%] [G loss: 0.418495]\n",
      "epoch:20 step:19327 [D loss: 0.223457, acc.: 62.50%] [G loss: 0.427079]\n",
      "epoch:20 step:19328 [D loss: 0.233372, acc.: 62.50%] [G loss: 0.403062]\n",
      "epoch:20 step:19329 [D loss: 0.201796, acc.: 74.22%] [G loss: 0.452510]\n",
      "epoch:20 step:19330 [D loss: 0.231667, acc.: 61.72%] [G loss: 0.471680]\n",
      "epoch:20 step:19331 [D loss: 0.220412, acc.: 63.28%] [G loss: 0.464288]\n",
      "epoch:20 step:19332 [D loss: 0.213908, acc.: 69.53%] [G loss: 0.424681]\n",
      "epoch:20 step:19333 [D loss: 0.207246, acc.: 67.97%] [G loss: 0.480291]\n",
      "epoch:20 step:19334 [D loss: 0.244112, acc.: 57.81%] [G loss: 0.420018]\n",
      "epoch:20 step:19335 [D loss: 0.204653, acc.: 71.09%] [G loss: 0.420185]\n",
      "epoch:20 step:19336 [D loss: 0.244553, acc.: 56.25%] [G loss: 0.413328]\n",
      "epoch:20 step:19337 [D loss: 0.230196, acc.: 62.50%] [G loss: 0.388319]\n",
      "epoch:20 step:19338 [D loss: 0.229441, acc.: 65.62%] [G loss: 0.433089]\n",
      "epoch:20 step:19339 [D loss: 0.222517, acc.: 63.28%] [G loss: 0.424065]\n",
      "epoch:20 step:19340 [D loss: 0.252462, acc.: 57.81%] [G loss: 0.409134]\n",
      "epoch:20 step:19341 [D loss: 0.250328, acc.: 53.12%] [G loss: 0.396839]\n",
      "epoch:20 step:19342 [D loss: 0.225963, acc.: 58.59%] [G loss: 0.370089]\n",
      "epoch:20 step:19343 [D loss: 0.233242, acc.: 60.16%] [G loss: 0.398942]\n",
      "epoch:20 step:19344 [D loss: 0.205110, acc.: 71.88%] [G loss: 0.439401]\n",
      "epoch:20 step:19345 [D loss: 0.210469, acc.: 67.19%] [G loss: 0.427286]\n",
      "epoch:20 step:19346 [D loss: 0.231045, acc.: 63.28%] [G loss: 0.423098]\n",
      "epoch:20 step:19347 [D loss: 0.205662, acc.: 71.09%] [G loss: 0.413265]\n",
      "epoch:20 step:19348 [D loss: 0.236703, acc.: 57.81%] [G loss: 0.407759]\n",
      "epoch:20 step:19349 [D loss: 0.220960, acc.: 58.59%] [G loss: 0.440180]\n",
      "epoch:20 step:19350 [D loss: 0.206321, acc.: 67.19%] [G loss: 0.411690]\n",
      "epoch:20 step:19351 [D loss: 0.215669, acc.: 71.09%] [G loss: 0.405787]\n",
      "epoch:20 step:19352 [D loss: 0.211297, acc.: 69.53%] [G loss: 0.438120]\n",
      "epoch:20 step:19353 [D loss: 0.196888, acc.: 72.66%] [G loss: 0.444055]\n",
      "epoch:20 step:19354 [D loss: 0.254678, acc.: 53.12%] [G loss: 0.404040]\n",
      "epoch:20 step:19355 [D loss: 0.250997, acc.: 58.59%] [G loss: 0.462684]\n",
      "epoch:20 step:19356 [D loss: 0.234962, acc.: 60.16%] [G loss: 0.389407]\n",
      "epoch:20 step:19357 [D loss: 0.227473, acc.: 59.38%] [G loss: 0.425360]\n",
      "epoch:20 step:19358 [D loss: 0.193885, acc.: 71.09%] [G loss: 0.458848]\n",
      "epoch:20 step:19359 [D loss: 0.250123, acc.: 53.12%] [G loss: 0.424835]\n",
      "epoch:20 step:19360 [D loss: 0.208093, acc.: 67.97%] [G loss: 0.474467]\n",
      "epoch:20 step:19361 [D loss: 0.247790, acc.: 52.34%] [G loss: 0.419360]\n",
      "epoch:20 step:19362 [D loss: 0.234833, acc.: 60.94%] [G loss: 0.430447]\n",
      "epoch:20 step:19363 [D loss: 0.218064, acc.: 65.62%] [G loss: 0.431150]\n",
      "epoch:20 step:19364 [D loss: 0.199993, acc.: 71.09%] [G loss: 0.419468]\n",
      "epoch:20 step:19365 [D loss: 0.216662, acc.: 61.72%] [G loss: 0.443069]\n",
      "epoch:20 step:19366 [D loss: 0.238903, acc.: 62.50%] [G loss: 0.432471]\n",
      "epoch:20 step:19367 [D loss: 0.215739, acc.: 68.75%] [G loss: 0.394482]\n",
      "epoch:20 step:19368 [D loss: 0.242360, acc.: 60.94%] [G loss: 0.429566]\n",
      "epoch:20 step:19369 [D loss: 0.205917, acc.: 71.09%] [G loss: 0.446893]\n",
      "epoch:20 step:19370 [D loss: 0.239189, acc.: 57.03%] [G loss: 0.416966]\n",
      "epoch:20 step:19371 [D loss: 0.209257, acc.: 64.84%] [G loss: 0.417789]\n",
      "epoch:20 step:19372 [D loss: 0.236589, acc.: 63.28%] [G loss: 0.434496]\n",
      "epoch:20 step:19373 [D loss: 0.205222, acc.: 71.09%] [G loss: 0.452524]\n",
      "epoch:20 step:19374 [D loss: 0.197472, acc.: 68.75%] [G loss: 0.459079]\n",
      "epoch:20 step:19375 [D loss: 0.203791, acc.: 66.41%] [G loss: 0.446952]\n",
      "epoch:20 step:19376 [D loss: 0.255161, acc.: 57.03%] [G loss: 0.403266]\n",
      "epoch:20 step:19377 [D loss: 0.223219, acc.: 63.28%] [G loss: 0.459661]\n",
      "epoch:20 step:19378 [D loss: 0.218950, acc.: 66.41%] [G loss: 0.428216]\n",
      "epoch:20 step:19379 [D loss: 0.234744, acc.: 60.16%] [G loss: 0.439018]\n",
      "epoch:20 step:19380 [D loss: 0.225592, acc.: 63.28%] [G loss: 0.428146]\n",
      "epoch:20 step:19381 [D loss: 0.211974, acc.: 61.72%] [G loss: 0.459337]\n",
      "epoch:20 step:19382 [D loss: 0.202567, acc.: 66.41%] [G loss: 0.521549]\n",
      "epoch:20 step:19383 [D loss: 0.231703, acc.: 62.50%] [G loss: 0.472898]\n",
      "epoch:20 step:19384 [D loss: 0.246698, acc.: 60.16%] [G loss: 0.422660]\n",
      "epoch:20 step:19385 [D loss: 0.230519, acc.: 61.72%] [G loss: 0.402691]\n",
      "epoch:20 step:19386 [D loss: 0.213253, acc.: 68.75%] [G loss: 0.430587]\n",
      "epoch:20 step:19387 [D loss: 0.199322, acc.: 70.31%] [G loss: 0.449427]\n",
      "epoch:20 step:19388 [D loss: 0.210525, acc.: 73.44%] [G loss: 0.452101]\n",
      "epoch:20 step:19389 [D loss: 0.225483, acc.: 63.28%] [G loss: 0.489938]\n",
      "epoch:20 step:19390 [D loss: 0.209599, acc.: 69.53%] [G loss: 0.485819]\n",
      "epoch:20 step:19391 [D loss: 0.230047, acc.: 58.59%] [G loss: 0.452385]\n",
      "epoch:20 step:19392 [D loss: 0.231404, acc.: 58.59%] [G loss: 0.432763]\n",
      "epoch:20 step:19393 [D loss: 0.224249, acc.: 63.28%] [G loss: 0.428691]\n",
      "epoch:20 step:19394 [D loss: 0.209363, acc.: 64.84%] [G loss: 0.456745]\n",
      "epoch:20 step:19395 [D loss: 0.245001, acc.: 57.03%] [G loss: 0.406548]\n",
      "epoch:20 step:19396 [D loss: 0.210605, acc.: 66.41%] [G loss: 0.431588]\n",
      "epoch:20 step:19397 [D loss: 0.222294, acc.: 66.41%] [G loss: 0.456351]\n",
      "epoch:20 step:19398 [D loss: 0.230370, acc.: 63.28%] [G loss: 0.452788]\n",
      "epoch:20 step:19399 [D loss: 0.214526, acc.: 69.53%] [G loss: 0.422026]\n",
      "epoch:20 step:19400 [D loss: 0.226225, acc.: 62.50%] [G loss: 0.459336]\n",
      "##############\n",
      "[2.63031927 1.84427787 6.09052485 4.80385954 3.86242789 5.59024068\n",
      " 4.58561983 5.03579155 4.59595519 4.14841199]\n",
      "##########\n",
      "epoch:20 step:19401 [D loss: 0.221530, acc.: 64.06%] [G loss: 0.417177]\n",
      "epoch:20 step:19402 [D loss: 0.229443, acc.: 62.50%] [G loss: 0.435741]\n",
      "epoch:20 step:19403 [D loss: 0.234864, acc.: 58.59%] [G loss: 0.444513]\n",
      "epoch:20 step:19404 [D loss: 0.210786, acc.: 64.84%] [G loss: 0.467217]\n",
      "epoch:20 step:19405 [D loss: 0.231458, acc.: 62.50%] [G loss: 0.461223]\n",
      "epoch:20 step:19406 [D loss: 0.199916, acc.: 68.75%] [G loss: 0.464631]\n",
      "epoch:20 step:19407 [D loss: 0.231954, acc.: 59.38%] [G loss: 0.454176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19408 [D loss: 0.243305, acc.: 55.47%] [G loss: 0.391794]\n",
      "epoch:20 step:19409 [D loss: 0.209618, acc.: 61.72%] [G loss: 0.416064]\n",
      "epoch:20 step:19410 [D loss: 0.248668, acc.: 53.12%] [G loss: 0.377963]\n",
      "epoch:20 step:19411 [D loss: 0.244895, acc.: 56.25%] [G loss: 0.378344]\n",
      "epoch:20 step:19412 [D loss: 0.225482, acc.: 62.50%] [G loss: 0.424327]\n",
      "epoch:20 step:19413 [D loss: 0.225038, acc.: 65.62%] [G loss: 0.444910]\n",
      "epoch:20 step:19414 [D loss: 0.220368, acc.: 66.41%] [G loss: 0.397756]\n",
      "epoch:20 step:19415 [D loss: 0.218287, acc.: 67.97%] [G loss: 0.453535]\n",
      "epoch:20 step:19416 [D loss: 0.223334, acc.: 63.28%] [G loss: 0.417344]\n",
      "epoch:20 step:19417 [D loss: 0.220733, acc.: 65.62%] [G loss: 0.429417]\n",
      "epoch:20 step:19418 [D loss: 0.228572, acc.: 60.94%] [G loss: 0.434822]\n",
      "epoch:20 step:19419 [D loss: 0.209232, acc.: 68.75%] [G loss: 0.417801]\n",
      "epoch:20 step:19420 [D loss: 0.231490, acc.: 63.28%] [G loss: 0.474654]\n",
      "epoch:20 step:19421 [D loss: 0.218051, acc.: 64.84%] [G loss: 0.441687]\n",
      "epoch:20 step:19422 [D loss: 0.238198, acc.: 58.59%] [G loss: 0.436471]\n",
      "epoch:20 step:19423 [D loss: 0.229871, acc.: 57.81%] [G loss: 0.426658]\n",
      "epoch:20 step:19424 [D loss: 0.244123, acc.: 59.38%] [G loss: 0.396870]\n",
      "epoch:20 step:19425 [D loss: 0.227635, acc.: 61.72%] [G loss: 0.470537]\n",
      "epoch:20 step:19426 [D loss: 0.205192, acc.: 70.31%] [G loss: 0.427489]\n",
      "epoch:20 step:19427 [D loss: 0.244153, acc.: 55.47%] [G loss: 0.397779]\n",
      "epoch:20 step:19428 [D loss: 0.194373, acc.: 71.09%] [G loss: 0.431833]\n",
      "epoch:20 step:19429 [D loss: 0.218068, acc.: 65.62%] [G loss: 0.428561]\n",
      "epoch:20 step:19430 [D loss: 0.183898, acc.: 74.22%] [G loss: 0.465805]\n",
      "epoch:20 step:19431 [D loss: 0.213672, acc.: 69.53%] [G loss: 0.472291]\n",
      "epoch:20 step:19432 [D loss: 0.228597, acc.: 61.72%] [G loss: 0.461887]\n",
      "epoch:20 step:19433 [D loss: 0.217237, acc.: 64.06%] [G loss: 0.462160]\n",
      "epoch:20 step:19434 [D loss: 0.206518, acc.: 66.41%] [G loss: 0.454303]\n",
      "epoch:20 step:19435 [D loss: 0.216450, acc.: 66.41%] [G loss: 0.468956]\n",
      "epoch:20 step:19436 [D loss: 0.247880, acc.: 54.69%] [G loss: 0.429290]\n",
      "epoch:20 step:19437 [D loss: 0.228125, acc.: 61.72%] [G loss: 0.412773]\n",
      "epoch:20 step:19438 [D loss: 0.236413, acc.: 63.28%] [G loss: 0.413208]\n",
      "epoch:20 step:19439 [D loss: 0.230373, acc.: 69.53%] [G loss: 0.414401]\n",
      "epoch:20 step:19440 [D loss: 0.215671, acc.: 66.41%] [G loss: 0.506285]\n",
      "epoch:20 step:19441 [D loss: 0.203309, acc.: 67.97%] [G loss: 0.509511]\n",
      "epoch:20 step:19442 [D loss: 0.236021, acc.: 60.94%] [G loss: 0.468464]\n",
      "epoch:20 step:19443 [D loss: 0.259115, acc.: 51.56%] [G loss: 0.404605]\n",
      "epoch:20 step:19444 [D loss: 0.267367, acc.: 51.56%] [G loss: 0.419666]\n",
      "epoch:20 step:19445 [D loss: 0.222451, acc.: 60.94%] [G loss: 0.420869]\n",
      "epoch:20 step:19446 [D loss: 0.199231, acc.: 69.53%] [G loss: 0.445719]\n",
      "epoch:20 step:19447 [D loss: 0.227152, acc.: 60.94%] [G loss: 0.454582]\n",
      "epoch:20 step:19448 [D loss: 0.191534, acc.: 71.88%] [G loss: 0.450306]\n",
      "epoch:20 step:19449 [D loss: 0.215094, acc.: 68.75%] [G loss: 0.435533]\n",
      "epoch:20 step:19450 [D loss: 0.256089, acc.: 52.34%] [G loss: 0.452703]\n",
      "epoch:20 step:19451 [D loss: 0.216840, acc.: 70.31%] [G loss: 0.456056]\n",
      "epoch:20 step:19452 [D loss: 0.213795, acc.: 64.06%] [G loss: 0.462290]\n",
      "epoch:20 step:19453 [D loss: 0.225052, acc.: 62.50%] [G loss: 0.433610]\n",
      "epoch:20 step:19454 [D loss: 0.242088, acc.: 57.03%] [G loss: 0.413266]\n",
      "epoch:20 step:19455 [D loss: 0.219874, acc.: 64.84%] [G loss: 0.420454]\n",
      "epoch:20 step:19456 [D loss: 0.252978, acc.: 58.59%] [G loss: 0.393788]\n",
      "epoch:20 step:19457 [D loss: 0.219606, acc.: 65.62%] [G loss: 0.438982]\n",
      "epoch:20 step:19458 [D loss: 0.243874, acc.: 57.81%] [G loss: 0.416166]\n",
      "epoch:20 step:19459 [D loss: 0.204104, acc.: 68.75%] [G loss: 0.458506]\n",
      "epoch:20 step:19460 [D loss: 0.200683, acc.: 71.09%] [G loss: 0.470938]\n",
      "epoch:20 step:19461 [D loss: 0.265790, acc.: 54.69%] [G loss: 0.423117]\n",
      "epoch:20 step:19462 [D loss: 0.246369, acc.: 60.16%] [G loss: 0.378221]\n",
      "epoch:20 step:19463 [D loss: 0.228308, acc.: 66.41%] [G loss: 0.456963]\n",
      "epoch:20 step:19464 [D loss: 0.205374, acc.: 69.53%] [G loss: 0.457267]\n",
      "epoch:20 step:19465 [D loss: 0.198976, acc.: 67.97%] [G loss: 0.468376]\n",
      "epoch:20 step:19466 [D loss: 0.235557, acc.: 58.59%] [G loss: 0.459752]\n",
      "epoch:20 step:19467 [D loss: 0.264423, acc.: 51.56%] [G loss: 0.425712]\n",
      "epoch:20 step:19468 [D loss: 0.218012, acc.: 64.84%] [G loss: 0.413152]\n",
      "epoch:20 step:19469 [D loss: 0.233054, acc.: 58.59%] [G loss: 0.394046]\n",
      "epoch:20 step:19470 [D loss: 0.206778, acc.: 61.72%] [G loss: 0.436272]\n",
      "epoch:20 step:19471 [D loss: 0.243249, acc.: 56.25%] [G loss: 0.444402]\n",
      "epoch:20 step:19472 [D loss: 0.214083, acc.: 68.75%] [G loss: 0.441763]\n",
      "epoch:20 step:19473 [D loss: 0.208044, acc.: 67.97%] [G loss: 0.428016]\n",
      "epoch:20 step:19474 [D loss: 0.224934, acc.: 64.84%] [G loss: 0.454056]\n",
      "epoch:20 step:19475 [D loss: 0.246876, acc.: 56.25%] [G loss: 0.397432]\n",
      "epoch:20 step:19476 [D loss: 0.239033, acc.: 60.16%] [G loss: 0.410555]\n",
      "epoch:20 step:19477 [D loss: 0.229636, acc.: 64.84%] [G loss: 0.423378]\n",
      "epoch:20 step:19478 [D loss: 0.234361, acc.: 54.69%] [G loss: 0.414741]\n",
      "epoch:20 step:19479 [D loss: 0.255185, acc.: 60.16%] [G loss: 0.424679]\n",
      "epoch:20 step:19480 [D loss: 0.245001, acc.: 57.03%] [G loss: 0.400048]\n",
      "epoch:20 step:19481 [D loss: 0.247003, acc.: 58.59%] [G loss: 0.409665]\n",
      "epoch:20 step:19482 [D loss: 0.233559, acc.: 57.81%] [G loss: 0.396336]\n",
      "epoch:20 step:19483 [D loss: 0.214775, acc.: 63.28%] [G loss: 0.432495]\n",
      "epoch:20 step:19484 [D loss: 0.205347, acc.: 70.31%] [G loss: 0.399379]\n",
      "epoch:20 step:19485 [D loss: 0.241941, acc.: 57.81%] [G loss: 0.424009]\n",
      "epoch:20 step:19486 [D loss: 0.239785, acc.: 61.72%] [G loss: 0.411240]\n",
      "epoch:20 step:19487 [D loss: 0.216147, acc.: 64.84%] [G loss: 0.435030]\n",
      "epoch:20 step:19488 [D loss: 0.215655, acc.: 64.84%] [G loss: 0.434003]\n",
      "epoch:20 step:19489 [D loss: 0.206171, acc.: 66.41%] [G loss: 0.454467]\n",
      "epoch:20 step:19490 [D loss: 0.177320, acc.: 74.22%] [G loss: 0.447372]\n",
      "epoch:20 step:19491 [D loss: 0.231351, acc.: 61.72%] [G loss: 0.443568]\n",
      "epoch:20 step:19492 [D loss: 0.257217, acc.: 58.59%] [G loss: 0.428651]\n",
      "epoch:20 step:19493 [D loss: 0.246802, acc.: 55.47%] [G loss: 0.412036]\n",
      "epoch:20 step:19494 [D loss: 0.218939, acc.: 64.06%] [G loss: 0.456744]\n",
      "epoch:20 step:19495 [D loss: 0.251951, acc.: 54.69%] [G loss: 0.404056]\n",
      "epoch:20 step:19496 [D loss: 0.226656, acc.: 63.28%] [G loss: 0.462382]\n",
      "epoch:20 step:19497 [D loss: 0.222114, acc.: 62.50%] [G loss: 0.410509]\n",
      "epoch:20 step:19498 [D loss: 0.223907, acc.: 63.28%] [G loss: 0.397030]\n",
      "epoch:20 step:19499 [D loss: 0.259020, acc.: 58.59%] [G loss: 0.391207]\n",
      "epoch:20 step:19500 [D loss: 0.211903, acc.: 58.59%] [G loss: 0.451468]\n",
      "epoch:20 step:19501 [D loss: 0.219686, acc.: 60.94%] [G loss: 0.406918]\n",
      "epoch:20 step:19502 [D loss: 0.242899, acc.: 57.03%] [G loss: 0.407984]\n",
      "epoch:20 step:19503 [D loss: 0.211905, acc.: 62.50%] [G loss: 0.428193]\n",
      "epoch:20 step:19504 [D loss: 0.240141, acc.: 60.94%] [G loss: 0.415513]\n",
      "epoch:20 step:19505 [D loss: 0.272152, acc.: 48.44%] [G loss: 0.381490]\n",
      "epoch:20 step:19506 [D loss: 0.247139, acc.: 54.69%] [G loss: 0.455073]\n",
      "epoch:20 step:19507 [D loss: 0.207417, acc.: 68.75%] [G loss: 0.456462]\n",
      "epoch:20 step:19508 [D loss: 0.215585, acc.: 67.97%] [G loss: 0.465108]\n",
      "epoch:20 step:19509 [D loss: 0.220251, acc.: 64.84%] [G loss: 0.496852]\n",
      "epoch:20 step:19510 [D loss: 0.200525, acc.: 68.75%] [G loss: 0.477608]\n",
      "epoch:20 step:19511 [D loss: 0.232858, acc.: 60.94%] [G loss: 0.404249]\n",
      "epoch:20 step:19512 [D loss: 0.234329, acc.: 58.59%] [G loss: 0.410893]\n",
      "epoch:20 step:19513 [D loss: 0.223573, acc.: 65.62%] [G loss: 0.396968]\n",
      "epoch:20 step:19514 [D loss: 0.200938, acc.: 67.19%] [G loss: 0.421435]\n",
      "epoch:20 step:19515 [D loss: 0.219206, acc.: 62.50%] [G loss: 0.470935]\n",
      "epoch:20 step:19516 [D loss: 0.228043, acc.: 57.81%] [G loss: 0.510456]\n",
      "epoch:20 step:19517 [D loss: 0.218388, acc.: 60.94%] [G loss: 0.422680]\n",
      "epoch:20 step:19518 [D loss: 0.221061, acc.: 60.94%] [G loss: 0.406387]\n",
      "epoch:20 step:19519 [D loss: 0.212063, acc.: 66.41%] [G loss: 0.438169]\n",
      "epoch:20 step:19520 [D loss: 0.225723, acc.: 64.06%] [G loss: 0.405887]\n",
      "epoch:20 step:19521 [D loss: 0.212335, acc.: 60.94%] [G loss: 0.440695]\n",
      "epoch:20 step:19522 [D loss: 0.225100, acc.: 64.06%] [G loss: 0.452605]\n",
      "epoch:20 step:19523 [D loss: 0.252862, acc.: 53.12%] [G loss: 0.485651]\n",
      "epoch:20 step:19524 [D loss: 0.268301, acc.: 48.44%] [G loss: 0.434082]\n",
      "epoch:20 step:19525 [D loss: 0.228358, acc.: 61.72%] [G loss: 0.404233]\n",
      "epoch:20 step:19526 [D loss: 0.206403, acc.: 65.62%] [G loss: 0.418741]\n",
      "epoch:20 step:19527 [D loss: 0.251065, acc.: 53.91%] [G loss: 0.399781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19528 [D loss: 0.256055, acc.: 54.69%] [G loss: 0.363424]\n",
      "epoch:20 step:19529 [D loss: 0.247467, acc.: 55.47%] [G loss: 0.425775]\n",
      "epoch:20 step:19530 [D loss: 0.211673, acc.: 64.06%] [G loss: 0.458259]\n",
      "epoch:20 step:19531 [D loss: 0.258511, acc.: 60.16%] [G loss: 0.408429]\n",
      "epoch:20 step:19532 [D loss: 0.207294, acc.: 64.84%] [G loss: 0.423123]\n",
      "epoch:20 step:19533 [D loss: 0.215224, acc.: 64.06%] [G loss: 0.439942]\n",
      "epoch:20 step:19534 [D loss: 0.263424, acc.: 51.56%] [G loss: 0.434850]\n",
      "epoch:20 step:19535 [D loss: 0.251233, acc.: 53.91%] [G loss: 0.415024]\n",
      "epoch:20 step:19536 [D loss: 0.228389, acc.: 62.50%] [G loss: 0.471719]\n",
      "epoch:20 step:19537 [D loss: 0.222616, acc.: 66.41%] [G loss: 0.431257]\n",
      "epoch:20 step:19538 [D loss: 0.229970, acc.: 64.06%] [G loss: 0.449336]\n",
      "epoch:20 step:19539 [D loss: 0.213857, acc.: 66.41%] [G loss: 0.432365]\n",
      "epoch:20 step:19540 [D loss: 0.238369, acc.: 54.69%] [G loss: 0.394048]\n",
      "epoch:20 step:19541 [D loss: 0.205844, acc.: 71.88%] [G loss: 0.438732]\n",
      "epoch:20 step:19542 [D loss: 0.211013, acc.: 71.09%] [G loss: 0.452937]\n",
      "epoch:20 step:19543 [D loss: 0.241842, acc.: 59.38%] [G loss: 0.414138]\n",
      "epoch:20 step:19544 [D loss: 0.240615, acc.: 58.59%] [G loss: 0.426489]\n",
      "epoch:20 step:19545 [D loss: 0.220656, acc.: 62.50%] [G loss: 0.448544]\n",
      "epoch:20 step:19546 [D loss: 0.218928, acc.: 64.06%] [G loss: 0.439026]\n",
      "epoch:20 step:19547 [D loss: 0.206768, acc.: 70.31%] [G loss: 0.436082]\n",
      "epoch:20 step:19548 [D loss: 0.256430, acc.: 53.91%] [G loss: 0.400566]\n",
      "epoch:20 step:19549 [D loss: 0.239577, acc.: 58.59%] [G loss: 0.376979]\n",
      "epoch:20 step:19550 [D loss: 0.228270, acc.: 64.06%] [G loss: 0.387676]\n",
      "epoch:20 step:19551 [D loss: 0.217028, acc.: 60.94%] [G loss: 0.420507]\n",
      "epoch:20 step:19552 [D loss: 0.246682, acc.: 53.12%] [G loss: 0.438825]\n",
      "epoch:20 step:19553 [D loss: 0.219294, acc.: 64.84%] [G loss: 0.411654]\n",
      "epoch:20 step:19554 [D loss: 0.219525, acc.: 61.72%] [G loss: 0.409643]\n",
      "epoch:20 step:19555 [D loss: 0.210452, acc.: 70.31%] [G loss: 0.470568]\n",
      "epoch:20 step:19556 [D loss: 0.212182, acc.: 60.16%] [G loss: 0.464452]\n",
      "epoch:20 step:19557 [D loss: 0.264997, acc.: 50.00%] [G loss: 0.432398]\n",
      "epoch:20 step:19558 [D loss: 0.236834, acc.: 59.38%] [G loss: 0.425952]\n",
      "epoch:20 step:19559 [D loss: 0.234819, acc.: 65.62%] [G loss: 0.436902]\n",
      "epoch:20 step:19560 [D loss: 0.269113, acc.: 50.78%] [G loss: 0.434645]\n",
      "epoch:20 step:19561 [D loss: 0.227892, acc.: 64.06%] [G loss: 0.409595]\n",
      "epoch:20 step:19562 [D loss: 0.217024, acc.: 64.06%] [G loss: 0.412680]\n",
      "epoch:20 step:19563 [D loss: 0.205922, acc.: 69.53%] [G loss: 0.425859]\n",
      "epoch:20 step:19564 [D loss: 0.226386, acc.: 60.94%] [G loss: 0.434138]\n",
      "epoch:20 step:19565 [D loss: 0.195167, acc.: 74.22%] [G loss: 0.424543]\n",
      "epoch:20 step:19566 [D loss: 0.236175, acc.: 53.12%] [G loss: 0.443850]\n",
      "epoch:20 step:19567 [D loss: 0.258068, acc.: 57.81%] [G loss: 0.448602]\n",
      "epoch:20 step:19568 [D loss: 0.246314, acc.: 56.25%] [G loss: 0.450784]\n",
      "epoch:20 step:19569 [D loss: 0.240320, acc.: 63.28%] [G loss: 0.452548]\n",
      "epoch:20 step:19570 [D loss: 0.232188, acc.: 60.94%] [G loss: 0.408052]\n",
      "epoch:20 step:19571 [D loss: 0.234702, acc.: 62.50%] [G loss: 0.398446]\n",
      "epoch:20 step:19572 [D loss: 0.199953, acc.: 71.88%] [G loss: 0.427401]\n",
      "epoch:20 step:19573 [D loss: 0.199308, acc.: 68.75%] [G loss: 0.450032]\n",
      "epoch:20 step:19574 [D loss: 0.247464, acc.: 57.81%] [G loss: 0.407835]\n",
      "epoch:20 step:19575 [D loss: 0.217419, acc.: 64.84%] [G loss: 0.434439]\n",
      "epoch:20 step:19576 [D loss: 0.231402, acc.: 57.81%] [G loss: 0.388755]\n",
      "epoch:20 step:19577 [D loss: 0.203955, acc.: 67.97%] [G loss: 0.420145]\n",
      "epoch:20 step:19578 [D loss: 0.225612, acc.: 60.94%] [G loss: 0.411634]\n",
      "epoch:20 step:19579 [D loss: 0.230770, acc.: 60.94%] [G loss: 0.403390]\n",
      "epoch:20 step:19580 [D loss: 0.214242, acc.: 69.53%] [G loss: 0.446080]\n",
      "epoch:20 step:19581 [D loss: 0.224382, acc.: 64.06%] [G loss: 0.430539]\n",
      "epoch:20 step:19582 [D loss: 0.201507, acc.: 69.53%] [G loss: 0.408374]\n",
      "epoch:20 step:19583 [D loss: 0.234222, acc.: 63.28%] [G loss: 0.399300]\n",
      "epoch:20 step:19584 [D loss: 0.233661, acc.: 59.38%] [G loss: 0.416353]\n",
      "epoch:20 step:19585 [D loss: 0.229685, acc.: 57.03%] [G loss: 0.446593]\n",
      "epoch:20 step:19586 [D loss: 0.216628, acc.: 61.72%] [G loss: 0.438742]\n",
      "epoch:20 step:19587 [D loss: 0.261504, acc.: 55.47%] [G loss: 0.419170]\n",
      "epoch:20 step:19588 [D loss: 0.229571, acc.: 60.94%] [G loss: 0.400302]\n",
      "epoch:20 step:19589 [D loss: 0.225086, acc.: 64.84%] [G loss: 0.398739]\n",
      "epoch:20 step:19590 [D loss: 0.242354, acc.: 54.69%] [G loss: 0.419367]\n",
      "epoch:20 step:19591 [D loss: 0.241411, acc.: 55.47%] [G loss: 0.454172]\n",
      "epoch:20 step:19592 [D loss: 0.236287, acc.: 61.72%] [G loss: 0.441938]\n",
      "epoch:20 step:19593 [D loss: 0.195537, acc.: 67.19%] [G loss: 0.482930]\n",
      "epoch:20 step:19594 [D loss: 0.245953, acc.: 57.03%] [G loss: 0.448442]\n",
      "epoch:20 step:19595 [D loss: 0.235577, acc.: 57.03%] [G loss: 0.418475]\n",
      "epoch:20 step:19596 [D loss: 0.243430, acc.: 53.91%] [G loss: 0.408590]\n",
      "epoch:20 step:19597 [D loss: 0.233245, acc.: 64.84%] [G loss: 0.425569]\n",
      "epoch:20 step:19598 [D loss: 0.269090, acc.: 46.09%] [G loss: 0.393104]\n",
      "epoch:20 step:19599 [D loss: 0.226068, acc.: 61.72%] [G loss: 0.444093]\n",
      "epoch:20 step:19600 [D loss: 0.191834, acc.: 73.44%] [G loss: 0.464372]\n",
      "##############\n",
      "[2.38715849 2.10163539 6.08355147 4.89294731 3.73621805 5.7592191\n",
      " 4.49562354 4.59403104 4.55470809 3.9457147 ]\n",
      "##########\n",
      "epoch:20 step:19601 [D loss: 0.265840, acc.: 54.69%] [G loss: 0.423798]\n",
      "epoch:20 step:19602 [D loss: 0.244303, acc.: 55.47%] [G loss: 0.399393]\n",
      "epoch:20 step:19603 [D loss: 0.207549, acc.: 65.62%] [G loss: 0.402179]\n",
      "epoch:20 step:19604 [D loss: 0.228864, acc.: 62.50%] [G loss: 0.411799]\n",
      "epoch:20 step:19605 [D loss: 0.246328, acc.: 59.38%] [G loss: 0.380250]\n",
      "epoch:20 step:19606 [D loss: 0.229766, acc.: 60.94%] [G loss: 0.403570]\n",
      "epoch:20 step:19607 [D loss: 0.236844, acc.: 55.47%] [G loss: 0.437140]\n",
      "epoch:20 step:19608 [D loss: 0.230556, acc.: 62.50%] [G loss: 0.446507]\n",
      "epoch:20 step:19609 [D loss: 0.239056, acc.: 59.38%] [G loss: 0.405215]\n",
      "epoch:20 step:19610 [D loss: 0.219040, acc.: 64.84%] [G loss: 0.426441]\n",
      "epoch:20 step:19611 [D loss: 0.207734, acc.: 66.41%] [G loss: 0.451258]\n",
      "epoch:20 step:19612 [D loss: 0.213199, acc.: 66.41%] [G loss: 0.407432]\n",
      "epoch:20 step:19613 [D loss: 0.219902, acc.: 68.75%] [G loss: 0.441050]\n",
      "epoch:20 step:19614 [D loss: 0.230375, acc.: 59.38%] [G loss: 0.393968]\n",
      "epoch:20 step:19615 [D loss: 0.217446, acc.: 67.97%] [G loss: 0.420538]\n",
      "epoch:20 step:19616 [D loss: 0.235838, acc.: 57.81%] [G loss: 0.424382]\n",
      "epoch:20 step:19617 [D loss: 0.229963, acc.: 66.41%] [G loss: 0.431618]\n",
      "epoch:20 step:19618 [D loss: 0.225831, acc.: 57.81%] [G loss: 0.416170]\n",
      "epoch:20 step:19619 [D loss: 0.209449, acc.: 69.53%] [G loss: 0.434791]\n",
      "epoch:20 step:19620 [D loss: 0.248013, acc.: 53.12%] [G loss: 0.395300]\n",
      "epoch:20 step:19621 [D loss: 0.232895, acc.: 55.47%] [G loss: 0.425774]\n",
      "epoch:20 step:19622 [D loss: 0.252083, acc.: 53.12%] [G loss: 0.428508]\n",
      "epoch:20 step:19623 [D loss: 0.243733, acc.: 62.50%] [G loss: 0.414517]\n",
      "epoch:20 step:19624 [D loss: 0.226079, acc.: 64.84%] [G loss: 0.444881]\n",
      "epoch:20 step:19625 [D loss: 0.222094, acc.: 68.75%] [G loss: 0.475322]\n",
      "epoch:20 step:19626 [D loss: 0.199718, acc.: 70.31%] [G loss: 0.484047]\n",
      "epoch:20 step:19627 [D loss: 0.229987, acc.: 63.28%] [G loss: 0.448627]\n",
      "epoch:20 step:19628 [D loss: 0.230694, acc.: 60.16%] [G loss: 0.405110]\n",
      "epoch:20 step:19629 [D loss: 0.191183, acc.: 70.31%] [G loss: 0.452896]\n",
      "epoch:20 step:19630 [D loss: 0.195638, acc.: 71.09%] [G loss: 0.434250]\n",
      "epoch:20 step:19631 [D loss: 0.244045, acc.: 56.25%] [G loss: 0.448067]\n",
      "epoch:20 step:19632 [D loss: 0.263882, acc.: 52.34%] [G loss: 0.407907]\n",
      "epoch:20 step:19633 [D loss: 0.205053, acc.: 67.97%] [G loss: 0.425096]\n",
      "epoch:20 step:19634 [D loss: 0.210637, acc.: 67.97%] [G loss: 0.474957]\n",
      "epoch:20 step:19635 [D loss: 0.235411, acc.: 56.25%] [G loss: 0.419932]\n",
      "epoch:20 step:19636 [D loss: 0.207393, acc.: 68.75%] [G loss: 0.466107]\n",
      "epoch:20 step:19637 [D loss: 0.216602, acc.: 65.62%] [G loss: 0.442173]\n",
      "epoch:20 step:19638 [D loss: 0.219277, acc.: 64.06%] [G loss: 0.437988]\n",
      "epoch:20 step:19639 [D loss: 0.201967, acc.: 67.97%] [G loss: 0.462040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19640 [D loss: 0.226618, acc.: 67.19%] [G loss: 0.437817]\n",
      "epoch:20 step:19641 [D loss: 0.208517, acc.: 70.31%] [G loss: 0.430942]\n",
      "epoch:20 step:19642 [D loss: 0.251247, acc.: 57.03%] [G loss: 0.403470]\n",
      "epoch:20 step:19643 [D loss: 0.215382, acc.: 66.41%] [G loss: 0.447847]\n",
      "epoch:20 step:19644 [D loss: 0.227825, acc.: 59.38%] [G loss: 0.408513]\n",
      "epoch:20 step:19645 [D loss: 0.199812, acc.: 70.31%] [G loss: 0.436247]\n",
      "epoch:20 step:19646 [D loss: 0.218881, acc.: 63.28%] [G loss: 0.432700]\n",
      "epoch:20 step:19647 [D loss: 0.233903, acc.: 61.72%] [G loss: 0.423736]\n",
      "epoch:20 step:19648 [D loss: 0.206190, acc.: 68.75%] [G loss: 0.452569]\n",
      "epoch:20 step:19649 [D loss: 0.195163, acc.: 64.84%] [G loss: 0.425815]\n",
      "epoch:20 step:19650 [D loss: 0.215576, acc.: 62.50%] [G loss: 0.429030]\n",
      "epoch:20 step:19651 [D loss: 0.227517, acc.: 64.84%] [G loss: 0.445661]\n",
      "epoch:20 step:19652 [D loss: 0.226627, acc.: 60.94%] [G loss: 0.442424]\n",
      "epoch:20 step:19653 [D loss: 0.230209, acc.: 64.06%] [G loss: 0.482608]\n",
      "epoch:20 step:19654 [D loss: 0.212894, acc.: 66.41%] [G loss: 0.457983]\n",
      "epoch:20 step:19655 [D loss: 0.287265, acc.: 56.25%] [G loss: 0.362417]\n",
      "epoch:20 step:19656 [D loss: 0.226233, acc.: 65.62%] [G loss: 0.401929]\n",
      "epoch:20 step:19657 [D loss: 0.237823, acc.: 59.38%] [G loss: 0.414705]\n",
      "epoch:20 step:19658 [D loss: 0.203917, acc.: 72.66%] [G loss: 0.433057]\n",
      "epoch:20 step:19659 [D loss: 0.199412, acc.: 72.66%] [G loss: 0.468171]\n",
      "epoch:20 step:19660 [D loss: 0.281599, acc.: 50.00%] [G loss: 0.443146]\n",
      "epoch:20 step:19661 [D loss: 0.214510, acc.: 63.28%] [G loss: 0.450697]\n",
      "epoch:20 step:19662 [D loss: 0.244938, acc.: 60.16%] [G loss: 0.372519]\n",
      "epoch:20 step:19663 [D loss: 0.209113, acc.: 68.75%] [G loss: 0.443775]\n",
      "epoch:20 step:19664 [D loss: 0.185540, acc.: 75.78%] [G loss: 0.457678]\n",
      "epoch:20 step:19665 [D loss: 0.191294, acc.: 79.69%] [G loss: 0.474639]\n",
      "epoch:20 step:19666 [D loss: 0.166692, acc.: 83.59%] [G loss: 0.462638]\n",
      "epoch:20 step:19667 [D loss: 0.238673, acc.: 60.94%] [G loss: 0.480234]\n",
      "epoch:20 step:19668 [D loss: 0.290170, acc.: 55.47%] [G loss: 0.456120]\n",
      "epoch:20 step:19669 [D loss: 0.264961, acc.: 57.03%] [G loss: 0.475394]\n",
      "epoch:20 step:19670 [D loss: 0.199906, acc.: 68.75%] [G loss: 0.520754]\n",
      "epoch:20 step:19671 [D loss: 0.251441, acc.: 60.16%] [G loss: 0.431905]\n",
      "epoch:20 step:19672 [D loss: 0.256442, acc.: 53.91%] [G loss: 0.436742]\n",
      "epoch:20 step:19673 [D loss: 0.205570, acc.: 74.22%] [G loss: 0.428072]\n",
      "epoch:20 step:19674 [D loss: 0.239398, acc.: 64.84%] [G loss: 0.430250]\n",
      "epoch:20 step:19675 [D loss: 0.195882, acc.: 66.41%] [G loss: 0.433592]\n",
      "epoch:20 step:19676 [D loss: 0.183299, acc.: 73.44%] [G loss: 0.539992]\n",
      "epoch:20 step:19677 [D loss: 0.194861, acc.: 72.66%] [G loss: 0.539167]\n",
      "epoch:21 step:19678 [D loss: 0.273662, acc.: 57.81%] [G loss: 0.492000]\n",
      "epoch:21 step:19679 [D loss: 0.262985, acc.: 60.94%] [G loss: 0.429137]\n",
      "epoch:21 step:19680 [D loss: 0.238282, acc.: 59.38%] [G loss: 0.395664]\n",
      "epoch:21 step:19681 [D loss: 0.232122, acc.: 57.81%] [G loss: 0.410431]\n",
      "epoch:21 step:19682 [D loss: 0.224456, acc.: 61.72%] [G loss: 0.438149]\n",
      "epoch:21 step:19683 [D loss: 0.232408, acc.: 53.91%] [G loss: 0.420102]\n",
      "epoch:21 step:19684 [D loss: 0.205439, acc.: 67.19%] [G loss: 0.455675]\n",
      "epoch:21 step:19685 [D loss: 0.205739, acc.: 74.22%] [G loss: 0.469894]\n",
      "epoch:21 step:19686 [D loss: 0.204298, acc.: 70.31%] [G loss: 0.466568]\n",
      "epoch:21 step:19687 [D loss: 0.229576, acc.: 60.16%] [G loss: 0.422865]\n",
      "epoch:21 step:19688 [D loss: 0.205426, acc.: 67.97%] [G loss: 0.430092]\n",
      "epoch:21 step:19689 [D loss: 0.226461, acc.: 62.50%] [G loss: 0.486098]\n",
      "epoch:21 step:19690 [D loss: 0.206997, acc.: 64.84%] [G loss: 0.422915]\n",
      "epoch:21 step:19691 [D loss: 0.217826, acc.: 67.97%] [G loss: 0.401543]\n",
      "epoch:21 step:19692 [D loss: 0.205539, acc.: 68.75%] [G loss: 0.441323]\n",
      "epoch:21 step:19693 [D loss: 0.194434, acc.: 68.75%] [G loss: 0.471529]\n",
      "epoch:21 step:19694 [D loss: 0.237241, acc.: 60.94%] [G loss: 0.459748]\n",
      "epoch:21 step:19695 [D loss: 0.263956, acc.: 59.38%] [G loss: 0.457063]\n",
      "epoch:21 step:19696 [D loss: 0.244188, acc.: 60.94%] [G loss: 0.454200]\n",
      "epoch:21 step:19697 [D loss: 0.273345, acc.: 49.22%] [G loss: 0.448157]\n",
      "epoch:21 step:19698 [D loss: 0.228420, acc.: 58.59%] [G loss: 0.437553]\n",
      "epoch:21 step:19699 [D loss: 0.198560, acc.: 68.75%] [G loss: 0.456897]\n",
      "epoch:21 step:19700 [D loss: 0.232100, acc.: 60.94%] [G loss: 0.390687]\n",
      "epoch:21 step:19701 [D loss: 0.222594, acc.: 64.84%] [G loss: 0.424457]\n",
      "epoch:21 step:19702 [D loss: 0.213154, acc.: 67.97%] [G loss: 0.401384]\n",
      "epoch:21 step:19703 [D loss: 0.233350, acc.: 60.94%] [G loss: 0.394364]\n",
      "epoch:21 step:19704 [D loss: 0.226204, acc.: 62.50%] [G loss: 0.427291]\n",
      "epoch:21 step:19705 [D loss: 0.222059, acc.: 60.16%] [G loss: 0.417739]\n",
      "epoch:21 step:19706 [D loss: 0.227125, acc.: 60.94%] [G loss: 0.406121]\n",
      "epoch:21 step:19707 [D loss: 0.243986, acc.: 54.69%] [G loss: 0.410660]\n",
      "epoch:21 step:19708 [D loss: 0.237483, acc.: 58.59%] [G loss: 0.427292]\n",
      "epoch:21 step:19709 [D loss: 0.244226, acc.: 53.12%] [G loss: 0.411756]\n",
      "epoch:21 step:19710 [D loss: 0.209453, acc.: 71.88%] [G loss: 0.434264]\n",
      "epoch:21 step:19711 [D loss: 0.244098, acc.: 57.81%] [G loss: 0.412638]\n",
      "epoch:21 step:19712 [D loss: 0.231482, acc.: 54.69%] [G loss: 0.419680]\n",
      "epoch:21 step:19713 [D loss: 0.201781, acc.: 65.62%] [G loss: 0.447348]\n",
      "epoch:21 step:19714 [D loss: 0.262372, acc.: 52.34%] [G loss: 0.378665]\n",
      "epoch:21 step:19715 [D loss: 0.248970, acc.: 53.91%] [G loss: 0.387594]\n",
      "epoch:21 step:19716 [D loss: 0.218624, acc.: 70.31%] [G loss: 0.426364]\n",
      "epoch:21 step:19717 [D loss: 0.207999, acc.: 68.75%] [G loss: 0.434077]\n",
      "epoch:21 step:19718 [D loss: 0.234673, acc.: 59.38%] [G loss: 0.400083]\n",
      "epoch:21 step:19719 [D loss: 0.207961, acc.: 64.84%] [G loss: 0.423232]\n",
      "epoch:21 step:19720 [D loss: 0.227535, acc.: 60.94%] [G loss: 0.419744]\n",
      "epoch:21 step:19721 [D loss: 0.252932, acc.: 56.25%] [G loss: 0.417827]\n",
      "epoch:21 step:19722 [D loss: 0.227949, acc.: 58.59%] [G loss: 0.428143]\n",
      "epoch:21 step:19723 [D loss: 0.213914, acc.: 59.38%] [G loss: 0.493268]\n",
      "epoch:21 step:19724 [D loss: 0.239064, acc.: 59.38%] [G loss: 0.373856]\n",
      "epoch:21 step:19725 [D loss: 0.215816, acc.: 61.72%] [G loss: 0.451463]\n",
      "epoch:21 step:19726 [D loss: 0.215449, acc.: 67.19%] [G loss: 0.438321]\n",
      "epoch:21 step:19727 [D loss: 0.198085, acc.: 67.97%] [G loss: 0.447254]\n",
      "epoch:21 step:19728 [D loss: 0.241962, acc.: 57.81%] [G loss: 0.446990]\n",
      "epoch:21 step:19729 [D loss: 0.219850, acc.: 69.53%] [G loss: 0.454100]\n",
      "epoch:21 step:19730 [D loss: 0.227136, acc.: 62.50%] [G loss: 0.439949]\n",
      "epoch:21 step:19731 [D loss: 0.213499, acc.: 64.06%] [G loss: 0.409682]\n",
      "epoch:21 step:19732 [D loss: 0.230227, acc.: 57.03%] [G loss: 0.460004]\n",
      "epoch:21 step:19733 [D loss: 0.212782, acc.: 68.75%] [G loss: 0.435776]\n",
      "epoch:21 step:19734 [D loss: 0.244992, acc.: 61.72%] [G loss: 0.434114]\n",
      "epoch:21 step:19735 [D loss: 0.225974, acc.: 62.50%] [G loss: 0.432953]\n",
      "epoch:21 step:19736 [D loss: 0.234682, acc.: 60.16%] [G loss: 0.431943]\n",
      "epoch:21 step:19737 [D loss: 0.229418, acc.: 61.72%] [G loss: 0.406599]\n",
      "epoch:21 step:19738 [D loss: 0.228149, acc.: 67.97%] [G loss: 0.430542]\n",
      "epoch:21 step:19739 [D loss: 0.229239, acc.: 65.62%] [G loss: 0.400413]\n",
      "epoch:21 step:19740 [D loss: 0.212838, acc.: 64.84%] [G loss: 0.414633]\n",
      "epoch:21 step:19741 [D loss: 0.214706, acc.: 62.50%] [G loss: 0.401764]\n",
      "epoch:21 step:19742 [D loss: 0.205025, acc.: 67.97%] [G loss: 0.446817]\n",
      "epoch:21 step:19743 [D loss: 0.218166, acc.: 63.28%] [G loss: 0.407422]\n",
      "epoch:21 step:19744 [D loss: 0.220991, acc.: 60.94%] [G loss: 0.409753]\n",
      "epoch:21 step:19745 [D loss: 0.225143, acc.: 62.50%] [G loss: 0.425913]\n",
      "epoch:21 step:19746 [D loss: 0.201039, acc.: 68.75%] [G loss: 0.392004]\n",
      "epoch:21 step:19747 [D loss: 0.194002, acc.: 73.44%] [G loss: 0.468135]\n",
      "epoch:21 step:19748 [D loss: 0.253076, acc.: 56.25%] [G loss: 0.423026]\n",
      "epoch:21 step:19749 [D loss: 0.225929, acc.: 63.28%] [G loss: 0.398921]\n",
      "epoch:21 step:19750 [D loss: 0.227328, acc.: 60.94%] [G loss: 0.414554]\n",
      "epoch:21 step:19751 [D loss: 0.203719, acc.: 71.88%] [G loss: 0.446343]\n",
      "epoch:21 step:19752 [D loss: 0.230281, acc.: 60.16%] [G loss: 0.410799]\n",
      "epoch:21 step:19753 [D loss: 0.192029, acc.: 67.97%] [G loss: 0.437579]\n",
      "epoch:21 step:19754 [D loss: 0.221756, acc.: 61.72%] [G loss: 0.497832]\n",
      "epoch:21 step:19755 [D loss: 0.280692, acc.: 47.66%] [G loss: 0.417840]\n",
      "epoch:21 step:19756 [D loss: 0.238372, acc.: 56.25%] [G loss: 0.392414]\n",
      "epoch:21 step:19757 [D loss: 0.228164, acc.: 62.50%] [G loss: 0.419930]\n",
      "epoch:21 step:19758 [D loss: 0.237915, acc.: 57.81%] [G loss: 0.382466]\n",
      "epoch:21 step:19759 [D loss: 0.221363, acc.: 63.28%] [G loss: 0.401962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19760 [D loss: 0.206489, acc.: 67.97%] [G loss: 0.453817]\n",
      "epoch:21 step:19761 [D loss: 0.211273, acc.: 64.84%] [G loss: 0.455660]\n",
      "epoch:21 step:19762 [D loss: 0.248228, acc.: 57.03%] [G loss: 0.427988]\n",
      "epoch:21 step:19763 [D loss: 0.233822, acc.: 57.03%] [G loss: 0.435437]\n",
      "epoch:21 step:19764 [D loss: 0.226827, acc.: 61.72%] [G loss: 0.414913]\n",
      "epoch:21 step:19765 [D loss: 0.201709, acc.: 70.31%] [G loss: 0.452481]\n",
      "epoch:21 step:19766 [D loss: 0.210464, acc.: 67.19%] [G loss: 0.410269]\n",
      "epoch:21 step:19767 [D loss: 0.219683, acc.: 66.41%] [G loss: 0.414385]\n",
      "epoch:21 step:19768 [D loss: 0.230852, acc.: 60.16%] [G loss: 0.429038]\n",
      "epoch:21 step:19769 [D loss: 0.198013, acc.: 67.19%] [G loss: 0.456145]\n",
      "epoch:21 step:19770 [D loss: 0.226240, acc.: 61.72%] [G loss: 0.411009]\n",
      "epoch:21 step:19771 [D loss: 0.236588, acc.: 60.94%] [G loss: 0.454940]\n",
      "epoch:21 step:19772 [D loss: 0.217798, acc.: 66.41%] [G loss: 0.473262]\n",
      "epoch:21 step:19773 [D loss: 0.209121, acc.: 71.88%] [G loss: 0.433546]\n",
      "epoch:21 step:19774 [D loss: 0.210197, acc.: 66.41%] [G loss: 0.544139]\n",
      "epoch:21 step:19775 [D loss: 0.215011, acc.: 64.06%] [G loss: 0.480722]\n",
      "epoch:21 step:19776 [D loss: 0.236507, acc.: 58.59%] [G loss: 0.451883]\n",
      "epoch:21 step:19777 [D loss: 0.207367, acc.: 69.53%] [G loss: 0.456116]\n",
      "epoch:21 step:19778 [D loss: 0.230262, acc.: 60.94%] [G loss: 0.386266]\n",
      "epoch:21 step:19779 [D loss: 0.237807, acc.: 58.59%] [G loss: 0.397645]\n",
      "epoch:21 step:19780 [D loss: 0.241230, acc.: 60.94%] [G loss: 0.404583]\n",
      "epoch:21 step:19781 [D loss: 0.214584, acc.: 68.75%] [G loss: 0.439357]\n",
      "epoch:21 step:19782 [D loss: 0.223029, acc.: 57.81%] [G loss: 0.396028]\n",
      "epoch:21 step:19783 [D loss: 0.235490, acc.: 59.38%] [G loss: 0.400294]\n",
      "epoch:21 step:19784 [D loss: 0.240276, acc.: 58.59%] [G loss: 0.465068]\n",
      "epoch:21 step:19785 [D loss: 0.248799, acc.: 57.81%] [G loss: 0.479622]\n",
      "epoch:21 step:19786 [D loss: 0.241408, acc.: 57.81%] [G loss: 0.448243]\n",
      "epoch:21 step:19787 [D loss: 0.237339, acc.: 57.81%] [G loss: 0.420866]\n",
      "epoch:21 step:19788 [D loss: 0.211766, acc.: 67.97%] [G loss: 0.409770]\n",
      "epoch:21 step:19789 [D loss: 0.202679, acc.: 67.19%] [G loss: 0.422640]\n",
      "epoch:21 step:19790 [D loss: 0.197824, acc.: 67.97%] [G loss: 0.443740]\n",
      "epoch:21 step:19791 [D loss: 0.218739, acc.: 65.62%] [G loss: 0.419093]\n",
      "epoch:21 step:19792 [D loss: 0.190834, acc.: 67.19%] [G loss: 0.467709]\n",
      "epoch:21 step:19793 [D loss: 0.213414, acc.: 64.06%] [G loss: 0.474132]\n",
      "epoch:21 step:19794 [D loss: 0.246473, acc.: 62.50%] [G loss: 0.480021]\n",
      "epoch:21 step:19795 [D loss: 0.217981, acc.: 64.84%] [G loss: 0.497523]\n",
      "epoch:21 step:19796 [D loss: 0.165636, acc.: 82.81%] [G loss: 0.479286]\n",
      "epoch:21 step:19797 [D loss: 0.238870, acc.: 54.69%] [G loss: 0.443967]\n",
      "epoch:21 step:19798 [D loss: 0.239793, acc.: 59.38%] [G loss: 0.399705]\n",
      "epoch:21 step:19799 [D loss: 0.197401, acc.: 71.09%] [G loss: 0.445141]\n",
      "epoch:21 step:19800 [D loss: 0.195896, acc.: 71.88%] [G loss: 0.457584]\n",
      "##############\n",
      "[2.521634   1.86839715 6.14940195 4.71263519 3.82816569 5.4289714\n",
      " 4.44514889 4.73914497 4.46487789 3.85582199]\n",
      "##########\n",
      "epoch:21 step:19801 [D loss: 0.221954, acc.: 62.50%] [G loss: 0.449808]\n",
      "epoch:21 step:19802 [D loss: 0.234735, acc.: 60.16%] [G loss: 0.450412]\n",
      "epoch:21 step:19803 [D loss: 0.198991, acc.: 66.41%] [G loss: 0.429138]\n",
      "epoch:21 step:19804 [D loss: 0.231963, acc.: 60.94%] [G loss: 0.426423]\n",
      "epoch:21 step:19805 [D loss: 0.229683, acc.: 64.84%] [G loss: 0.417868]\n",
      "epoch:21 step:19806 [D loss: 0.219555, acc.: 64.84%] [G loss: 0.396076]\n",
      "epoch:21 step:19807 [D loss: 0.217483, acc.: 61.72%] [G loss: 0.433655]\n",
      "epoch:21 step:19808 [D loss: 0.209497, acc.: 69.53%] [G loss: 0.436384]\n",
      "epoch:21 step:19809 [D loss: 0.234487, acc.: 60.16%] [G loss: 0.400847]\n",
      "epoch:21 step:19810 [D loss: 0.254104, acc.: 57.81%] [G loss: 0.401859]\n",
      "epoch:21 step:19811 [D loss: 0.223759, acc.: 64.84%] [G loss: 0.501746]\n",
      "epoch:21 step:19812 [D loss: 0.236347, acc.: 61.72%] [G loss: 0.419436]\n",
      "epoch:21 step:19813 [D loss: 0.209070, acc.: 64.06%] [G loss: 0.440883]\n",
      "epoch:21 step:19814 [D loss: 0.254261, acc.: 56.25%] [G loss: 0.435353]\n",
      "epoch:21 step:19815 [D loss: 0.260555, acc.: 53.12%] [G loss: 0.384257]\n",
      "epoch:21 step:19816 [D loss: 0.237591, acc.: 60.94%] [G loss: 0.434245]\n",
      "epoch:21 step:19817 [D loss: 0.221066, acc.: 60.94%] [G loss: 0.426864]\n",
      "epoch:21 step:19818 [D loss: 0.220796, acc.: 63.28%] [G loss: 0.419476]\n",
      "epoch:21 step:19819 [D loss: 0.235752, acc.: 64.06%] [G loss: 0.402876]\n",
      "epoch:21 step:19820 [D loss: 0.241011, acc.: 60.94%] [G loss: 0.407008]\n",
      "epoch:21 step:19821 [D loss: 0.224564, acc.: 64.84%] [G loss: 0.438111]\n",
      "epoch:21 step:19822 [D loss: 0.226265, acc.: 62.50%] [G loss: 0.458514]\n",
      "epoch:21 step:19823 [D loss: 0.220044, acc.: 60.94%] [G loss: 0.386045]\n",
      "epoch:21 step:19824 [D loss: 0.223502, acc.: 59.38%] [G loss: 0.436612]\n",
      "epoch:21 step:19825 [D loss: 0.241915, acc.: 63.28%] [G loss: 0.414530]\n",
      "epoch:21 step:19826 [D loss: 0.205015, acc.: 67.19%] [G loss: 0.426364]\n",
      "epoch:21 step:19827 [D loss: 0.213657, acc.: 60.94%] [G loss: 0.425758]\n",
      "epoch:21 step:19828 [D loss: 0.198543, acc.: 74.22%] [G loss: 0.465441]\n",
      "epoch:21 step:19829 [D loss: 0.214312, acc.: 60.94%] [G loss: 0.392575]\n",
      "epoch:21 step:19830 [D loss: 0.244536, acc.: 60.16%] [G loss: 0.435266]\n",
      "epoch:21 step:19831 [D loss: 0.224225, acc.: 64.84%] [G loss: 0.399200]\n",
      "epoch:21 step:19832 [D loss: 0.215543, acc.: 63.28%] [G loss: 0.432948]\n",
      "epoch:21 step:19833 [D loss: 0.211085, acc.: 65.62%] [G loss: 0.407890]\n",
      "epoch:21 step:19834 [D loss: 0.220631, acc.: 68.75%] [G loss: 0.416044]\n",
      "epoch:21 step:19835 [D loss: 0.223504, acc.: 64.84%] [G loss: 0.407550]\n",
      "epoch:21 step:19836 [D loss: 0.244233, acc.: 58.59%] [G loss: 0.392926]\n",
      "epoch:21 step:19837 [D loss: 0.238311, acc.: 57.81%] [G loss: 0.463935]\n",
      "epoch:21 step:19838 [D loss: 0.229111, acc.: 60.94%] [G loss: 0.478751]\n",
      "epoch:21 step:19839 [D loss: 0.225292, acc.: 64.06%] [G loss: 0.437723]\n",
      "epoch:21 step:19840 [D loss: 0.236263, acc.: 64.06%] [G loss: 0.386873]\n",
      "epoch:21 step:19841 [D loss: 0.233237, acc.: 60.16%] [G loss: 0.424528]\n",
      "epoch:21 step:19842 [D loss: 0.212453, acc.: 64.84%] [G loss: 0.437930]\n",
      "epoch:21 step:19843 [D loss: 0.211092, acc.: 65.62%] [G loss: 0.409816]\n",
      "epoch:21 step:19844 [D loss: 0.216120, acc.: 62.50%] [G loss: 0.422270]\n",
      "epoch:21 step:19845 [D loss: 0.203001, acc.: 70.31%] [G loss: 0.452762]\n",
      "epoch:21 step:19846 [D loss: 0.239972, acc.: 58.59%] [G loss: 0.431776]\n",
      "epoch:21 step:19847 [D loss: 0.240900, acc.: 63.28%] [G loss: 0.400015]\n",
      "epoch:21 step:19848 [D loss: 0.219913, acc.: 65.62%] [G loss: 0.429295]\n",
      "epoch:21 step:19849 [D loss: 0.230441, acc.: 64.84%] [G loss: 0.429119]\n",
      "epoch:21 step:19850 [D loss: 0.196169, acc.: 66.41%] [G loss: 0.438075]\n",
      "epoch:21 step:19851 [D loss: 0.251240, acc.: 58.59%] [G loss: 0.416977]\n",
      "epoch:21 step:19852 [D loss: 0.212505, acc.: 67.97%] [G loss: 0.433986]\n",
      "epoch:21 step:19853 [D loss: 0.238453, acc.: 61.72%] [G loss: 0.396215]\n",
      "epoch:21 step:19854 [D loss: 0.232192, acc.: 53.91%] [G loss: 0.434944]\n",
      "epoch:21 step:19855 [D loss: 0.220987, acc.: 60.94%] [G loss: 0.421326]\n",
      "epoch:21 step:19856 [D loss: 0.241435, acc.: 60.16%] [G loss: 0.422310]\n",
      "epoch:21 step:19857 [D loss: 0.227804, acc.: 64.06%] [G loss: 0.436348]\n",
      "epoch:21 step:19858 [D loss: 0.226109, acc.: 63.28%] [G loss: 0.432167]\n",
      "epoch:21 step:19859 [D loss: 0.232654, acc.: 60.94%] [G loss: 0.419410]\n",
      "epoch:21 step:19860 [D loss: 0.242103, acc.: 61.72%] [G loss: 0.402129]\n",
      "epoch:21 step:19861 [D loss: 0.232221, acc.: 62.50%] [G loss: 0.442761]\n",
      "epoch:21 step:19862 [D loss: 0.254834, acc.: 54.69%] [G loss: 0.405952]\n",
      "epoch:21 step:19863 [D loss: 0.224361, acc.: 60.94%] [G loss: 0.402843]\n",
      "epoch:21 step:19864 [D loss: 0.227837, acc.: 57.81%] [G loss: 0.414216]\n",
      "epoch:21 step:19865 [D loss: 0.237116, acc.: 60.94%] [G loss: 0.378776]\n",
      "epoch:21 step:19866 [D loss: 0.227266, acc.: 67.19%] [G loss: 0.397724]\n",
      "epoch:21 step:19867 [D loss: 0.215657, acc.: 65.62%] [G loss: 0.391764]\n",
      "epoch:21 step:19868 [D loss: 0.209109, acc.: 66.41%] [G loss: 0.419733]\n",
      "epoch:21 step:19869 [D loss: 0.228125, acc.: 59.38%] [G loss: 0.418931]\n",
      "epoch:21 step:19870 [D loss: 0.215519, acc.: 64.84%] [G loss: 0.400002]\n",
      "epoch:21 step:19871 [D loss: 0.205605, acc.: 70.31%] [G loss: 0.430671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19872 [D loss: 0.212634, acc.: 64.84%] [G loss: 0.455029]\n",
      "epoch:21 step:19873 [D loss: 0.203535, acc.: 69.53%] [G loss: 0.448522]\n",
      "epoch:21 step:19874 [D loss: 0.236521, acc.: 64.06%] [G loss: 0.415418]\n",
      "epoch:21 step:19875 [D loss: 0.198434, acc.: 71.09%] [G loss: 0.454227]\n",
      "epoch:21 step:19876 [D loss: 0.217343, acc.: 64.84%] [G loss: 0.455519]\n",
      "epoch:21 step:19877 [D loss: 0.250156, acc.: 54.69%] [G loss: 0.448345]\n",
      "epoch:21 step:19878 [D loss: 0.249056, acc.: 54.69%] [G loss: 0.412259]\n",
      "epoch:21 step:19879 [D loss: 0.224901, acc.: 66.41%] [G loss: 0.416674]\n",
      "epoch:21 step:19880 [D loss: 0.298731, acc.: 44.53%] [G loss: 0.413209]\n",
      "epoch:21 step:19881 [D loss: 0.201047, acc.: 64.06%] [G loss: 0.433792]\n",
      "epoch:21 step:19882 [D loss: 0.230602, acc.: 61.72%] [G loss: 0.429957]\n",
      "epoch:21 step:19883 [D loss: 0.221831, acc.: 65.62%] [G loss: 0.476801]\n",
      "epoch:21 step:19884 [D loss: 0.209858, acc.: 72.66%] [G loss: 0.445645]\n",
      "epoch:21 step:19885 [D loss: 0.199406, acc.: 67.19%] [G loss: 0.482119]\n",
      "epoch:21 step:19886 [D loss: 0.192523, acc.: 72.66%] [G loss: 0.497680]\n",
      "epoch:21 step:19887 [D loss: 0.261971, acc.: 50.78%] [G loss: 0.460559]\n",
      "epoch:21 step:19888 [D loss: 0.251591, acc.: 57.81%] [G loss: 0.429614]\n",
      "epoch:21 step:19889 [D loss: 0.239854, acc.: 58.59%] [G loss: 0.435525]\n",
      "epoch:21 step:19890 [D loss: 0.237634, acc.: 59.38%] [G loss: 0.407545]\n",
      "epoch:21 step:19891 [D loss: 0.240348, acc.: 58.59%] [G loss: 0.438641]\n",
      "epoch:21 step:19892 [D loss: 0.255074, acc.: 53.12%] [G loss: 0.407015]\n",
      "epoch:21 step:19893 [D loss: 0.204019, acc.: 68.75%] [G loss: 0.410998]\n",
      "epoch:21 step:19894 [D loss: 0.241816, acc.: 56.25%] [G loss: 0.436675]\n",
      "epoch:21 step:19895 [D loss: 0.194310, acc.: 72.66%] [G loss: 0.446995]\n",
      "epoch:21 step:19896 [D loss: 0.189641, acc.: 75.78%] [G loss: 0.479894]\n",
      "epoch:21 step:19897 [D loss: 0.268102, acc.: 59.38%] [G loss: 0.451927]\n",
      "epoch:21 step:19898 [D loss: 0.224336, acc.: 62.50%] [G loss: 0.460706]\n",
      "epoch:21 step:19899 [D loss: 0.226207, acc.: 58.59%] [G loss: 0.437394]\n",
      "epoch:21 step:19900 [D loss: 0.210231, acc.: 67.19%] [G loss: 0.459550]\n",
      "epoch:21 step:19901 [D loss: 0.250214, acc.: 55.47%] [G loss: 0.391470]\n",
      "epoch:21 step:19902 [D loss: 0.230705, acc.: 57.81%] [G loss: 0.420938]\n",
      "epoch:21 step:19903 [D loss: 0.218415, acc.: 61.72%] [G loss: 0.408173]\n",
      "epoch:21 step:19904 [D loss: 0.228813, acc.: 64.84%] [G loss: 0.418129]\n",
      "epoch:21 step:19905 [D loss: 0.232025, acc.: 62.50%] [G loss: 0.409627]\n",
      "epoch:21 step:19906 [D loss: 0.200619, acc.: 68.75%] [G loss: 0.421052]\n",
      "epoch:21 step:19907 [D loss: 0.185987, acc.: 75.00%] [G loss: 0.454842]\n",
      "epoch:21 step:19908 [D loss: 0.171658, acc.: 74.22%] [G loss: 0.511091]\n",
      "epoch:21 step:19909 [D loss: 0.182680, acc.: 70.31%] [G loss: 0.522322]\n",
      "epoch:21 step:19910 [D loss: 0.270577, acc.: 53.12%] [G loss: 0.446287]\n",
      "epoch:21 step:19911 [D loss: 0.243948, acc.: 57.03%] [G loss: 0.418397]\n",
      "epoch:21 step:19912 [D loss: 0.219525, acc.: 67.19%] [G loss: 0.440250]\n",
      "epoch:21 step:19913 [D loss: 0.219030, acc.: 59.38%] [G loss: 0.412610]\n",
      "epoch:21 step:19914 [D loss: 0.241000, acc.: 60.16%] [G loss: 0.425267]\n",
      "epoch:21 step:19915 [D loss: 0.208986, acc.: 68.75%] [G loss: 0.459576]\n",
      "epoch:21 step:19916 [D loss: 0.195413, acc.: 75.00%] [G loss: 0.457365]\n",
      "epoch:21 step:19917 [D loss: 0.225828, acc.: 68.75%] [G loss: 0.455351]\n",
      "epoch:21 step:19918 [D loss: 0.192651, acc.: 71.88%] [G loss: 0.456772]\n",
      "epoch:21 step:19919 [D loss: 0.191254, acc.: 75.78%] [G loss: 0.487277]\n",
      "epoch:21 step:19920 [D loss: 0.215943, acc.: 66.41%] [G loss: 0.437494]\n",
      "epoch:21 step:19921 [D loss: 0.218628, acc.: 67.97%] [G loss: 0.423478]\n",
      "epoch:21 step:19922 [D loss: 0.233858, acc.: 63.28%] [G loss: 0.450094]\n",
      "epoch:21 step:19923 [D loss: 0.224273, acc.: 67.97%] [G loss: 0.472595]\n",
      "epoch:21 step:19924 [D loss: 0.225652, acc.: 58.59%] [G loss: 0.453554]\n",
      "epoch:21 step:19925 [D loss: 0.201876, acc.: 73.44%] [G loss: 0.468199]\n",
      "epoch:21 step:19926 [D loss: 0.278003, acc.: 50.78%] [G loss: 0.451149]\n",
      "epoch:21 step:19927 [D loss: 0.259907, acc.: 51.56%] [G loss: 0.441289]\n",
      "epoch:21 step:19928 [D loss: 0.242922, acc.: 54.69%] [G loss: 0.392882]\n",
      "epoch:21 step:19929 [D loss: 0.231858, acc.: 61.72%] [G loss: 0.414458]\n",
      "epoch:21 step:19930 [D loss: 0.221906, acc.: 57.81%] [G loss: 0.415481]\n",
      "epoch:21 step:19931 [D loss: 0.235032, acc.: 60.94%] [G loss: 0.451741]\n",
      "epoch:21 step:19932 [D loss: 0.229633, acc.: 60.16%] [G loss: 0.394028]\n",
      "epoch:21 step:19933 [D loss: 0.222283, acc.: 64.84%] [G loss: 0.429491]\n",
      "epoch:21 step:19934 [D loss: 0.220966, acc.: 66.41%] [G loss: 0.428742]\n",
      "epoch:21 step:19935 [D loss: 0.223599, acc.: 66.41%] [G loss: 0.404239]\n",
      "epoch:21 step:19936 [D loss: 0.230652, acc.: 67.97%] [G loss: 0.449662]\n",
      "epoch:21 step:19937 [D loss: 0.216274, acc.: 60.16%] [G loss: 0.471869]\n",
      "epoch:21 step:19938 [D loss: 0.218924, acc.: 62.50%] [G loss: 0.463070]\n",
      "epoch:21 step:19939 [D loss: 0.205566, acc.: 69.53%] [G loss: 0.467077]\n",
      "epoch:21 step:19940 [D loss: 0.232079, acc.: 61.72%] [G loss: 0.442947]\n",
      "epoch:21 step:19941 [D loss: 0.225234, acc.: 70.31%] [G loss: 0.430263]\n",
      "epoch:21 step:19942 [D loss: 0.227784, acc.: 56.25%] [G loss: 0.404667]\n",
      "epoch:21 step:19943 [D loss: 0.229917, acc.: 57.81%] [G loss: 0.423393]\n",
      "epoch:21 step:19944 [D loss: 0.226435, acc.: 63.28%] [G loss: 0.408028]\n",
      "epoch:21 step:19945 [D loss: 0.221630, acc.: 60.94%] [G loss: 0.410844]\n",
      "epoch:21 step:19946 [D loss: 0.195528, acc.: 72.66%] [G loss: 0.491176]\n",
      "epoch:21 step:19947 [D loss: 0.220806, acc.: 64.06%] [G loss: 0.466150]\n",
      "epoch:21 step:19948 [D loss: 0.202119, acc.: 67.19%] [G loss: 0.433784]\n",
      "epoch:21 step:19949 [D loss: 0.241077, acc.: 59.38%] [G loss: 0.414826]\n",
      "epoch:21 step:19950 [D loss: 0.223833, acc.: 64.06%] [G loss: 0.449679]\n",
      "epoch:21 step:19951 [D loss: 0.202851, acc.: 67.19%] [G loss: 0.435863]\n",
      "epoch:21 step:19952 [D loss: 0.231878, acc.: 58.59%] [G loss: 0.445038]\n",
      "epoch:21 step:19953 [D loss: 0.217163, acc.: 63.28%] [G loss: 0.434827]\n",
      "epoch:21 step:19954 [D loss: 0.256819, acc.: 55.47%] [G loss: 0.435387]\n",
      "epoch:21 step:19955 [D loss: 0.229229, acc.: 58.59%] [G loss: 0.454974]\n",
      "epoch:21 step:19956 [D loss: 0.226598, acc.: 64.06%] [G loss: 0.424229]\n",
      "epoch:21 step:19957 [D loss: 0.206273, acc.: 68.75%] [G loss: 0.447469]\n",
      "epoch:21 step:19958 [D loss: 0.255362, acc.: 54.69%] [G loss: 0.413599]\n",
      "epoch:21 step:19959 [D loss: 0.238487, acc.: 59.38%] [G loss: 0.396682]\n",
      "epoch:21 step:19960 [D loss: 0.203133, acc.: 69.53%] [G loss: 0.456109]\n",
      "epoch:21 step:19961 [D loss: 0.256952, acc.: 53.91%] [G loss: 0.409182]\n",
      "epoch:21 step:19962 [D loss: 0.244477, acc.: 55.47%] [G loss: 0.415181]\n",
      "epoch:21 step:19963 [D loss: 0.203051, acc.: 71.09%] [G loss: 0.450640]\n",
      "epoch:21 step:19964 [D loss: 0.222629, acc.: 71.09%] [G loss: 0.399540]\n",
      "epoch:21 step:19965 [D loss: 0.201458, acc.: 73.44%] [G loss: 0.442168]\n",
      "epoch:21 step:19966 [D loss: 0.200227, acc.: 70.31%] [G loss: 0.430737]\n",
      "epoch:21 step:19967 [D loss: 0.252995, acc.: 53.12%] [G loss: 0.469589]\n",
      "epoch:21 step:19968 [D loss: 0.267517, acc.: 53.91%] [G loss: 0.444205]\n",
      "epoch:21 step:19969 [D loss: 0.223270, acc.: 60.94%] [G loss: 0.437356]\n",
      "epoch:21 step:19970 [D loss: 0.252468, acc.: 62.50%] [G loss: 0.415907]\n",
      "epoch:21 step:19971 [D loss: 0.256476, acc.: 50.78%] [G loss: 0.404493]\n",
      "epoch:21 step:19972 [D loss: 0.221475, acc.: 60.94%] [G loss: 0.428975]\n",
      "epoch:21 step:19973 [D loss: 0.206208, acc.: 68.75%] [G loss: 0.389630]\n",
      "epoch:21 step:19974 [D loss: 0.233326, acc.: 61.72%] [G loss: 0.406217]\n",
      "epoch:21 step:19975 [D loss: 0.192834, acc.: 72.66%] [G loss: 0.430879]\n",
      "epoch:21 step:19976 [D loss: 0.203268, acc.: 68.75%] [G loss: 0.450027]\n",
      "epoch:21 step:19977 [D loss: 0.218531, acc.: 66.41%] [G loss: 0.465628]\n",
      "epoch:21 step:19978 [D loss: 0.244969, acc.: 59.38%] [G loss: 0.410527]\n",
      "epoch:21 step:19979 [D loss: 0.217418, acc.: 62.50%] [G loss: 0.435993]\n",
      "epoch:21 step:19980 [D loss: 0.221638, acc.: 65.62%] [G loss: 0.425910]\n",
      "epoch:21 step:19981 [D loss: 0.220219, acc.: 62.50%] [G loss: 0.406271]\n",
      "epoch:21 step:19982 [D loss: 0.246143, acc.: 55.47%] [G loss: 0.422260]\n",
      "epoch:21 step:19983 [D loss: 0.224731, acc.: 58.59%] [G loss: 0.434842]\n",
      "epoch:21 step:19984 [D loss: 0.211051, acc.: 65.62%] [G loss: 0.443288]\n",
      "epoch:21 step:19985 [D loss: 0.225624, acc.: 60.94%] [G loss: 0.454363]\n",
      "epoch:21 step:19986 [D loss: 0.212848, acc.: 70.31%] [G loss: 0.441044]\n",
      "epoch:21 step:19987 [D loss: 0.224663, acc.: 59.38%] [G loss: 0.468288]\n",
      "epoch:21 step:19988 [D loss: 0.216908, acc.: 68.75%] [G loss: 0.433948]\n",
      "epoch:21 step:19989 [D loss: 0.194248, acc.: 67.97%] [G loss: 0.435857]\n",
      "epoch:21 step:19990 [D loss: 0.183772, acc.: 75.78%] [G loss: 0.512481]\n",
      "epoch:21 step:19991 [D loss: 0.188398, acc.: 67.97%] [G loss: 0.489388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19992 [D loss: 0.173548, acc.: 75.78%] [G loss: 0.516173]\n",
      "epoch:21 step:19993 [D loss: 0.286501, acc.: 46.88%] [G loss: 0.389998]\n",
      "epoch:21 step:19994 [D loss: 0.229593, acc.: 63.28%] [G loss: 0.401507]\n",
      "epoch:21 step:19995 [D loss: 0.216314, acc.: 67.19%] [G loss: 0.398942]\n",
      "epoch:21 step:19996 [D loss: 0.243864, acc.: 60.16%] [G loss: 0.408384]\n",
      "epoch:21 step:19997 [D loss: 0.224110, acc.: 61.72%] [G loss: 0.431064]\n",
      "epoch:21 step:19998 [D loss: 0.205123, acc.: 63.28%] [G loss: 0.414647]\n",
      "epoch:21 step:19999 [D loss: 0.211665, acc.: 68.75%] [G loss: 0.455620]\n",
      "epoch:21 step:20000 [D loss: 0.223339, acc.: 64.84%] [G loss: 0.448101]\n",
      "##############\n",
      "[2.66507791 1.89454594 6.08165702 4.58396831 3.67238011 5.49687866\n",
      " 4.56332833 4.90018884 4.60594775 3.93787339]\n",
      "##########\n",
      "epoch:21 step:20001 [D loss: 0.215366, acc.: 64.84%] [G loss: 0.429940]\n",
      "epoch:21 step:20002 [D loss: 0.212822, acc.: 63.28%] [G loss: 0.420546]\n",
      "epoch:21 step:20003 [D loss: 0.223981, acc.: 61.72%] [G loss: 0.408184]\n",
      "epoch:21 step:20004 [D loss: 0.210774, acc.: 67.97%] [G loss: 0.448394]\n",
      "epoch:21 step:20005 [D loss: 0.205809, acc.: 70.31%] [G loss: 0.416903]\n",
      "epoch:21 step:20006 [D loss: 0.231031, acc.: 60.94%] [G loss: 0.427528]\n",
      "epoch:21 step:20007 [D loss: 0.215942, acc.: 66.41%] [G loss: 0.434339]\n",
      "epoch:21 step:20008 [D loss: 0.200658, acc.: 71.09%] [G loss: 0.430427]\n",
      "epoch:21 step:20009 [D loss: 0.210544, acc.: 62.50%] [G loss: 0.447881]\n",
      "epoch:21 step:20010 [D loss: 0.202244, acc.: 75.00%] [G loss: 0.446725]\n",
      "epoch:21 step:20011 [D loss: 0.226335, acc.: 64.84%] [G loss: 0.418107]\n",
      "epoch:21 step:20012 [D loss: 0.202209, acc.: 68.75%] [G loss: 0.445132]\n",
      "epoch:21 step:20013 [D loss: 0.196461, acc.: 73.44%] [G loss: 0.476901]\n",
      "epoch:21 step:20014 [D loss: 0.217621, acc.: 62.50%] [G loss: 0.471933]\n",
      "epoch:21 step:20015 [D loss: 0.230150, acc.: 59.38%] [G loss: 0.413900]\n",
      "epoch:21 step:20016 [D loss: 0.196265, acc.: 75.78%] [G loss: 0.433636]\n",
      "epoch:21 step:20017 [D loss: 0.198013, acc.: 67.97%] [G loss: 0.460142]\n",
      "epoch:21 step:20018 [D loss: 0.277052, acc.: 53.12%] [G loss: 0.436950]\n",
      "epoch:21 step:20019 [D loss: 0.214631, acc.: 64.84%] [G loss: 0.463716]\n",
      "epoch:21 step:20020 [D loss: 0.203757, acc.: 68.75%] [G loss: 0.488507]\n",
      "epoch:21 step:20021 [D loss: 0.207073, acc.: 69.53%] [G loss: 0.483002]\n",
      "epoch:21 step:20022 [D loss: 0.222619, acc.: 62.50%] [G loss: 0.458552]\n",
      "epoch:21 step:20023 [D loss: 0.192923, acc.: 71.88%] [G loss: 0.472724]\n",
      "epoch:21 step:20024 [D loss: 0.178568, acc.: 74.22%] [G loss: 0.534211]\n",
      "epoch:21 step:20025 [D loss: 0.239694, acc.: 62.50%] [G loss: 0.428925]\n",
      "epoch:21 step:20026 [D loss: 0.259650, acc.: 56.25%] [G loss: 0.432912]\n",
      "epoch:21 step:20027 [D loss: 0.230727, acc.: 63.28%] [G loss: 0.384861]\n",
      "epoch:21 step:20028 [D loss: 0.223063, acc.: 62.50%] [G loss: 0.411307]\n",
      "epoch:21 step:20029 [D loss: 0.224294, acc.: 65.62%] [G loss: 0.424276]\n",
      "epoch:21 step:20030 [D loss: 0.210698, acc.: 72.66%] [G loss: 0.430136]\n",
      "epoch:21 step:20031 [D loss: 0.176480, acc.: 75.00%] [G loss: 0.443678]\n",
      "epoch:21 step:20032 [D loss: 0.239521, acc.: 60.94%] [G loss: 0.406462]\n",
      "epoch:21 step:20033 [D loss: 0.219071, acc.: 63.28%] [G loss: 0.443958]\n",
      "epoch:21 step:20034 [D loss: 0.211211, acc.: 69.53%] [G loss: 0.448007]\n",
      "epoch:21 step:20035 [D loss: 0.222774, acc.: 60.94%] [G loss: 0.482510]\n",
      "epoch:21 step:20036 [D loss: 0.206906, acc.: 71.09%] [G loss: 0.460095]\n",
      "epoch:21 step:20037 [D loss: 0.192100, acc.: 69.53%] [G loss: 0.466245]\n",
      "epoch:21 step:20038 [D loss: 0.206797, acc.: 68.75%] [G loss: 0.460495]\n",
      "epoch:21 step:20039 [D loss: 0.219475, acc.: 64.84%] [G loss: 0.412913]\n",
      "epoch:21 step:20040 [D loss: 0.236138, acc.: 59.38%] [G loss: 0.375119]\n",
      "epoch:21 step:20041 [D loss: 0.228426, acc.: 60.16%] [G loss: 0.409265]\n",
      "epoch:21 step:20042 [D loss: 0.202575, acc.: 67.19%] [G loss: 0.457790]\n",
      "epoch:21 step:20043 [D loss: 0.226442, acc.: 56.25%] [G loss: 0.437905]\n",
      "epoch:21 step:20044 [D loss: 0.212825, acc.: 64.84%] [G loss: 0.406552]\n",
      "epoch:21 step:20045 [D loss: 0.241863, acc.: 55.47%] [G loss: 0.458348]\n",
      "epoch:21 step:20046 [D loss: 0.227565, acc.: 64.06%] [G loss: 0.455890]\n",
      "epoch:21 step:20047 [D loss: 0.180215, acc.: 74.22%] [G loss: 0.490628]\n",
      "epoch:21 step:20048 [D loss: 0.187827, acc.: 76.56%] [G loss: 0.485949]\n",
      "epoch:21 step:20049 [D loss: 0.240090, acc.: 58.59%] [G loss: 0.426909]\n",
      "epoch:21 step:20050 [D loss: 0.234348, acc.: 60.94%] [G loss: 0.419540]\n",
      "epoch:21 step:20051 [D loss: 0.189481, acc.: 73.44%] [G loss: 0.455513]\n",
      "epoch:21 step:20052 [D loss: 0.245771, acc.: 56.25%] [G loss: 0.434719]\n",
      "epoch:21 step:20053 [D loss: 0.268716, acc.: 55.47%] [G loss: 0.407246]\n",
      "epoch:21 step:20054 [D loss: 0.236515, acc.: 60.16%] [G loss: 0.419473]\n",
      "epoch:21 step:20055 [D loss: 0.239466, acc.: 57.03%] [G loss: 0.383031]\n",
      "epoch:21 step:20056 [D loss: 0.229385, acc.: 67.19%] [G loss: 0.441596]\n",
      "epoch:21 step:20057 [D loss: 0.240258, acc.: 53.12%] [G loss: 0.421770]\n",
      "epoch:21 step:20058 [D loss: 0.183191, acc.: 71.88%] [G loss: 0.423640]\n",
      "epoch:21 step:20059 [D loss: 0.237638, acc.: 60.16%] [G loss: 0.433243]\n",
      "epoch:21 step:20060 [D loss: 0.237577, acc.: 56.25%] [G loss: 0.403547]\n",
      "epoch:21 step:20061 [D loss: 0.225413, acc.: 64.84%] [G loss: 0.438210]\n",
      "epoch:21 step:20062 [D loss: 0.194880, acc.: 69.53%] [G loss: 0.473704]\n",
      "epoch:21 step:20063 [D loss: 0.241190, acc.: 57.81%] [G loss: 0.431987]\n",
      "epoch:21 step:20064 [D loss: 0.205056, acc.: 68.75%] [G loss: 0.495281]\n",
      "epoch:21 step:20065 [D loss: 0.211850, acc.: 65.62%] [G loss: 0.411967]\n",
      "epoch:21 step:20066 [D loss: 0.219828, acc.: 62.50%] [G loss: 0.451831]\n",
      "epoch:21 step:20067 [D loss: 0.245941, acc.: 57.81%] [G loss: 0.441258]\n",
      "epoch:21 step:20068 [D loss: 0.216422, acc.: 65.62%] [G loss: 0.411512]\n",
      "epoch:21 step:20069 [D loss: 0.229641, acc.: 59.38%] [G loss: 0.437729]\n",
      "epoch:21 step:20070 [D loss: 0.231045, acc.: 57.81%] [G loss: 0.429096]\n",
      "epoch:21 step:20071 [D loss: 0.217745, acc.: 64.06%] [G loss: 0.434865]\n",
      "epoch:21 step:20072 [D loss: 0.216376, acc.: 61.72%] [G loss: 0.413546]\n",
      "epoch:21 step:20073 [D loss: 0.248105, acc.: 57.03%] [G loss: 0.400483]\n",
      "epoch:21 step:20074 [D loss: 0.233785, acc.: 60.16%] [G loss: 0.430485]\n",
      "epoch:21 step:20075 [D loss: 0.204968, acc.: 67.19%] [G loss: 0.430046]\n",
      "epoch:21 step:20076 [D loss: 0.181815, acc.: 71.09%] [G loss: 0.492336]\n",
      "epoch:21 step:20077 [D loss: 0.268510, acc.: 47.66%] [G loss: 0.410552]\n",
      "epoch:21 step:20078 [D loss: 0.255443, acc.: 51.56%] [G loss: 0.414099]\n",
      "epoch:21 step:20079 [D loss: 0.228115, acc.: 64.84%] [G loss: 0.454412]\n",
      "epoch:21 step:20080 [D loss: 0.227530, acc.: 61.72%] [G loss: 0.461802]\n",
      "epoch:21 step:20081 [D loss: 0.244703, acc.: 56.25%] [G loss: 0.382235]\n",
      "epoch:21 step:20082 [D loss: 0.223002, acc.: 64.84%] [G loss: 0.438245]\n",
      "epoch:21 step:20083 [D loss: 0.193218, acc.: 75.00%] [G loss: 0.486070]\n",
      "epoch:21 step:20084 [D loss: 0.221605, acc.: 59.38%] [G loss: 0.462426]\n",
      "epoch:21 step:20085 [D loss: 0.246104, acc.: 52.34%] [G loss: 0.427020]\n",
      "epoch:21 step:20086 [D loss: 0.258035, acc.: 57.81%] [G loss: 0.416566]\n",
      "epoch:21 step:20087 [D loss: 0.228858, acc.: 60.16%] [G loss: 0.454265]\n",
      "epoch:21 step:20088 [D loss: 0.240078, acc.: 57.03%] [G loss: 0.411481]\n",
      "epoch:21 step:20089 [D loss: 0.230826, acc.: 60.16%] [G loss: 0.400438]\n",
      "epoch:21 step:20090 [D loss: 0.214735, acc.: 65.62%] [G loss: 0.432362]\n",
      "epoch:21 step:20091 [D loss: 0.223518, acc.: 67.19%] [G loss: 0.432449]\n",
      "epoch:21 step:20092 [D loss: 0.213011, acc.: 68.75%] [G loss: 0.450570]\n",
      "epoch:21 step:20093 [D loss: 0.204568, acc.: 64.84%] [G loss: 0.495895]\n",
      "epoch:21 step:20094 [D loss: 0.235586, acc.: 57.81%] [G loss: 0.480571]\n",
      "epoch:21 step:20095 [D loss: 0.253562, acc.: 53.12%] [G loss: 0.419992]\n",
      "epoch:21 step:20096 [D loss: 0.234060, acc.: 61.72%] [G loss: 0.426364]\n",
      "epoch:21 step:20097 [D loss: 0.225280, acc.: 65.62%] [G loss: 0.425962]\n",
      "epoch:21 step:20098 [D loss: 0.277637, acc.: 50.00%] [G loss: 0.376234]\n",
      "epoch:21 step:20099 [D loss: 0.231839, acc.: 56.25%] [G loss: 0.417533]\n",
      "epoch:21 step:20100 [D loss: 0.197584, acc.: 74.22%] [G loss: 0.446077]\n",
      "epoch:21 step:20101 [D loss: 0.245573, acc.: 55.47%] [G loss: 0.391509]\n",
      "epoch:21 step:20102 [D loss: 0.202471, acc.: 71.09%] [G loss: 0.405100]\n",
      "epoch:21 step:20103 [D loss: 0.212414, acc.: 66.41%] [G loss: 0.407880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20104 [D loss: 0.215779, acc.: 63.28%] [G loss: 0.444550]\n",
      "epoch:21 step:20105 [D loss: 0.220324, acc.: 60.16%] [G loss: 0.442668]\n",
      "epoch:21 step:20106 [D loss: 0.187195, acc.: 73.44%] [G loss: 0.477471]\n",
      "epoch:21 step:20107 [D loss: 0.197742, acc.: 71.88%] [G loss: 0.455234]\n",
      "epoch:21 step:20108 [D loss: 0.230622, acc.: 58.59%] [G loss: 0.386040]\n",
      "epoch:21 step:20109 [D loss: 0.250096, acc.: 64.84%] [G loss: 0.405136]\n",
      "epoch:21 step:20110 [D loss: 0.228201, acc.: 66.41%] [G loss: 0.438596]\n",
      "epoch:21 step:20111 [D loss: 0.216778, acc.: 64.06%] [G loss: 0.431636]\n",
      "epoch:21 step:20112 [D loss: 0.216137, acc.: 67.97%] [G loss: 0.428731]\n",
      "epoch:21 step:20113 [D loss: 0.211731, acc.: 62.50%] [G loss: 0.501394]\n",
      "epoch:21 step:20114 [D loss: 0.260401, acc.: 53.91%] [G loss: 0.470413]\n",
      "epoch:21 step:20115 [D loss: 0.257539, acc.: 49.22%] [G loss: 0.434969]\n",
      "epoch:21 step:20116 [D loss: 0.238092, acc.: 63.28%] [G loss: 0.399362]\n",
      "epoch:21 step:20117 [D loss: 0.220769, acc.: 66.41%] [G loss: 0.433513]\n",
      "epoch:21 step:20118 [D loss: 0.229862, acc.: 59.38%] [G loss: 0.425517]\n",
      "epoch:21 step:20119 [D loss: 0.242937, acc.: 56.25%] [G loss: 0.425432]\n",
      "epoch:21 step:20120 [D loss: 0.244632, acc.: 53.12%] [G loss: 0.379500]\n",
      "epoch:21 step:20121 [D loss: 0.234286, acc.: 60.16%] [G loss: 0.421070]\n",
      "epoch:21 step:20122 [D loss: 0.198322, acc.: 73.44%] [G loss: 0.473004]\n",
      "epoch:21 step:20123 [D loss: 0.223200, acc.: 66.41%] [G loss: 0.442497]\n",
      "epoch:21 step:20124 [D loss: 0.192846, acc.: 67.97%] [G loss: 0.461864]\n",
      "epoch:21 step:20125 [D loss: 0.253875, acc.: 59.38%] [G loss: 0.441508]\n",
      "epoch:21 step:20126 [D loss: 0.237483, acc.: 60.16%] [G loss: 0.438517]\n",
      "epoch:21 step:20127 [D loss: 0.221074, acc.: 64.84%] [G loss: 0.454234]\n",
      "epoch:21 step:20128 [D loss: 0.202499, acc.: 71.09%] [G loss: 0.421175]\n",
      "epoch:21 step:20129 [D loss: 0.216957, acc.: 65.62%] [G loss: 0.425344]\n",
      "epoch:21 step:20130 [D loss: 0.207695, acc.: 70.31%] [G loss: 0.436782]\n",
      "epoch:21 step:20131 [D loss: 0.244544, acc.: 62.50%] [G loss: 0.436202]\n",
      "epoch:21 step:20132 [D loss: 0.215716, acc.: 64.84%] [G loss: 0.445296]\n",
      "epoch:21 step:20133 [D loss: 0.210138, acc.: 65.62%] [G loss: 0.461644]\n",
      "epoch:21 step:20134 [D loss: 0.228470, acc.: 63.28%] [G loss: 0.464186]\n",
      "epoch:21 step:20135 [D loss: 0.286111, acc.: 46.88%] [G loss: 0.408723]\n",
      "epoch:21 step:20136 [D loss: 0.248876, acc.: 56.25%] [G loss: 0.420029]\n",
      "epoch:21 step:20137 [D loss: 0.232277, acc.: 62.50%] [G loss: 0.397837]\n",
      "epoch:21 step:20138 [D loss: 0.249287, acc.: 60.16%] [G loss: 0.385579]\n",
      "epoch:21 step:20139 [D loss: 0.252414, acc.: 51.56%] [G loss: 0.417167]\n",
      "epoch:21 step:20140 [D loss: 0.226327, acc.: 63.28%] [G loss: 0.387801]\n",
      "epoch:21 step:20141 [D loss: 0.193806, acc.: 73.44%] [G loss: 0.406025]\n",
      "epoch:21 step:20142 [D loss: 0.231692, acc.: 59.38%] [G loss: 0.398019]\n",
      "epoch:21 step:20143 [D loss: 0.210536, acc.: 62.50%] [G loss: 0.398412]\n",
      "epoch:21 step:20144 [D loss: 0.226643, acc.: 62.50%] [G loss: 0.432725]\n",
      "epoch:21 step:20145 [D loss: 0.249897, acc.: 58.59%] [G loss: 0.397987]\n",
      "epoch:21 step:20146 [D loss: 0.195728, acc.: 68.75%] [G loss: 0.467738]\n",
      "epoch:21 step:20147 [D loss: 0.196196, acc.: 75.00%] [G loss: 0.447106]\n",
      "epoch:21 step:20148 [D loss: 0.195853, acc.: 67.97%] [G loss: 0.468658]\n",
      "epoch:21 step:20149 [D loss: 0.204183, acc.: 68.75%] [G loss: 0.491762]\n",
      "epoch:21 step:20150 [D loss: 0.251342, acc.: 53.91%] [G loss: 0.428494]\n",
      "epoch:21 step:20151 [D loss: 0.198511, acc.: 65.62%] [G loss: 0.461218]\n",
      "epoch:21 step:20152 [D loss: 0.203696, acc.: 68.75%] [G loss: 0.449662]\n",
      "epoch:21 step:20153 [D loss: 0.233757, acc.: 57.81%] [G loss: 0.429235]\n",
      "epoch:21 step:20154 [D loss: 0.250961, acc.: 52.34%] [G loss: 0.412239]\n",
      "epoch:21 step:20155 [D loss: 0.245060, acc.: 59.38%] [G loss: 0.399276]\n",
      "epoch:21 step:20156 [D loss: 0.215858, acc.: 63.28%] [G loss: 0.408345]\n",
      "epoch:21 step:20157 [D loss: 0.228634, acc.: 61.72%] [G loss: 0.388835]\n",
      "epoch:21 step:20158 [D loss: 0.199300, acc.: 74.22%] [G loss: 0.429106]\n",
      "epoch:21 step:20159 [D loss: 0.275738, acc.: 50.78%] [G loss: 0.387046]\n",
      "epoch:21 step:20160 [D loss: 0.228569, acc.: 55.47%] [G loss: 0.428519]\n",
      "epoch:21 step:20161 [D loss: 0.208252, acc.: 65.62%] [G loss: 0.452706]\n",
      "epoch:21 step:20162 [D loss: 0.205666, acc.: 67.19%] [G loss: 0.410750]\n",
      "epoch:21 step:20163 [D loss: 0.258893, acc.: 56.25%] [G loss: 0.417806]\n",
      "epoch:21 step:20164 [D loss: 0.229245, acc.: 59.38%] [G loss: 0.420576]\n",
      "epoch:21 step:20165 [D loss: 0.183934, acc.: 75.00%] [G loss: 0.462929]\n",
      "epoch:21 step:20166 [D loss: 0.276081, acc.: 42.19%] [G loss: 0.405093]\n",
      "epoch:21 step:20167 [D loss: 0.236501, acc.: 64.84%] [G loss: 0.411672]\n",
      "epoch:21 step:20168 [D loss: 0.221463, acc.: 60.94%] [G loss: 0.413345]\n",
      "epoch:21 step:20169 [D loss: 0.222688, acc.: 60.94%] [G loss: 0.423255]\n",
      "epoch:21 step:20170 [D loss: 0.226961, acc.: 62.50%] [G loss: 0.434869]\n",
      "epoch:21 step:20171 [D loss: 0.227174, acc.: 60.16%] [G loss: 0.432896]\n",
      "epoch:21 step:20172 [D loss: 0.211191, acc.: 67.97%] [G loss: 0.452028]\n",
      "epoch:21 step:20173 [D loss: 0.238486, acc.: 57.81%] [G loss: 0.426786]\n",
      "epoch:21 step:20174 [D loss: 0.223857, acc.: 63.28%] [G loss: 0.424013]\n",
      "epoch:21 step:20175 [D loss: 0.212530, acc.: 64.06%] [G loss: 0.409298]\n",
      "epoch:21 step:20176 [D loss: 0.197834, acc.: 71.88%] [G loss: 0.440753]\n",
      "epoch:21 step:20177 [D loss: 0.254303, acc.: 60.16%] [G loss: 0.436523]\n",
      "epoch:21 step:20178 [D loss: 0.294067, acc.: 46.88%] [G loss: 0.420492]\n",
      "epoch:21 step:20179 [D loss: 0.240481, acc.: 58.59%] [G loss: 0.424134]\n",
      "epoch:21 step:20180 [D loss: 0.235790, acc.: 64.06%] [G loss: 0.402818]\n",
      "epoch:21 step:20181 [D loss: 0.193912, acc.: 74.22%] [G loss: 0.425905]\n",
      "epoch:21 step:20182 [D loss: 0.194245, acc.: 67.97%] [G loss: 0.449368]\n",
      "epoch:21 step:20183 [D loss: 0.233882, acc.: 59.38%] [G loss: 0.433107]\n",
      "epoch:21 step:20184 [D loss: 0.197704, acc.: 71.09%] [G loss: 0.447834]\n",
      "epoch:21 step:20185 [D loss: 0.194010, acc.: 70.31%] [G loss: 0.484398]\n",
      "epoch:21 step:20186 [D loss: 0.240581, acc.: 62.50%] [G loss: 0.449900]\n",
      "epoch:21 step:20187 [D loss: 0.257445, acc.: 53.91%] [G loss: 0.418408]\n",
      "epoch:21 step:20188 [D loss: 0.228578, acc.: 60.94%] [G loss: 0.414824]\n",
      "epoch:21 step:20189 [D loss: 0.227790, acc.: 60.16%] [G loss: 0.455423]\n",
      "epoch:21 step:20190 [D loss: 0.198348, acc.: 71.88%] [G loss: 0.461941]\n",
      "epoch:21 step:20191 [D loss: 0.188781, acc.: 69.53%] [G loss: 0.474704]\n",
      "epoch:21 step:20192 [D loss: 0.200474, acc.: 67.19%] [G loss: 0.446211]\n",
      "epoch:21 step:20193 [D loss: 0.208119, acc.: 67.19%] [G loss: 0.450813]\n",
      "epoch:21 step:20194 [D loss: 0.230063, acc.: 62.50%] [G loss: 0.447912]\n",
      "epoch:21 step:20195 [D loss: 0.257996, acc.: 57.81%] [G loss: 0.442587]\n",
      "epoch:21 step:20196 [D loss: 0.234562, acc.: 62.50%] [G loss: 0.424611]\n",
      "epoch:21 step:20197 [D loss: 0.220819, acc.: 64.84%] [G loss: 0.435355]\n",
      "epoch:21 step:20198 [D loss: 0.229128, acc.: 60.16%] [G loss: 0.420699]\n",
      "epoch:21 step:20199 [D loss: 0.234994, acc.: 59.38%] [G loss: 0.389107]\n",
      "epoch:21 step:20200 [D loss: 0.207419, acc.: 65.62%] [G loss: 0.444271]\n",
      "##############\n",
      "[2.46271624 2.11572055 5.82546322 5.01876687 3.69974872 5.46765964\n",
      " 4.51611089 4.90370286 4.15716548 4.01419173]\n",
      "##########\n",
      "epoch:21 step:20201 [D loss: 0.240967, acc.: 57.81%] [G loss: 0.439771]\n",
      "epoch:21 step:20202 [D loss: 0.219383, acc.: 68.75%] [G loss: 0.463492]\n",
      "epoch:21 step:20203 [D loss: 0.226402, acc.: 63.28%] [G loss: 0.422761]\n",
      "epoch:21 step:20204 [D loss: 0.245659, acc.: 59.38%] [G loss: 0.409537]\n",
      "epoch:21 step:20205 [D loss: 0.259461, acc.: 52.34%] [G loss: 0.392436]\n",
      "epoch:21 step:20206 [D loss: 0.245203, acc.: 57.81%] [G loss: 0.457105]\n",
      "epoch:21 step:20207 [D loss: 0.211816, acc.: 66.41%] [G loss: 0.470788]\n",
      "epoch:21 step:20208 [D loss: 0.261625, acc.: 57.81%] [G loss: 0.394888]\n",
      "epoch:21 step:20209 [D loss: 0.226447, acc.: 59.38%] [G loss: 0.396745]\n",
      "epoch:21 step:20210 [D loss: 0.225268, acc.: 66.41%] [G loss: 0.413530]\n",
      "epoch:21 step:20211 [D loss: 0.200358, acc.: 68.75%] [G loss: 0.425277]\n",
      "epoch:21 step:20212 [D loss: 0.223698, acc.: 64.84%] [G loss: 0.439137]\n",
      "epoch:21 step:20213 [D loss: 0.210881, acc.: 65.62%] [G loss: 0.418686]\n",
      "epoch:21 step:20214 [D loss: 0.218360, acc.: 60.94%] [G loss: 0.461129]\n",
      "epoch:21 step:20215 [D loss: 0.251795, acc.: 53.91%] [G loss: 0.404477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20216 [D loss: 0.228702, acc.: 57.03%] [G loss: 0.408540]\n",
      "epoch:21 step:20217 [D loss: 0.236511, acc.: 63.28%] [G loss: 0.425974]\n",
      "epoch:21 step:20218 [D loss: 0.248502, acc.: 59.38%] [G loss: 0.402685]\n",
      "epoch:21 step:20219 [D loss: 0.258321, acc.: 57.81%] [G loss: 0.406827]\n",
      "epoch:21 step:20220 [D loss: 0.218004, acc.: 64.06%] [G loss: 0.446761]\n",
      "epoch:21 step:20221 [D loss: 0.223313, acc.: 62.50%] [G loss: 0.449823]\n",
      "epoch:21 step:20222 [D loss: 0.252169, acc.: 60.16%] [G loss: 0.426667]\n",
      "epoch:21 step:20223 [D loss: 0.243278, acc.: 58.59%] [G loss: 0.420923]\n",
      "epoch:21 step:20224 [D loss: 0.247743, acc.: 58.59%] [G loss: 0.425967]\n",
      "epoch:21 step:20225 [D loss: 0.213817, acc.: 68.75%] [G loss: 0.417194]\n",
      "epoch:21 step:20226 [D loss: 0.202466, acc.: 64.06%] [G loss: 0.463826]\n",
      "epoch:21 step:20227 [D loss: 0.211908, acc.: 67.19%] [G loss: 0.418090]\n",
      "epoch:21 step:20228 [D loss: 0.200945, acc.: 71.09%] [G loss: 0.446868]\n",
      "epoch:21 step:20229 [D loss: 0.211194, acc.: 66.41%] [G loss: 0.428639]\n",
      "epoch:21 step:20230 [D loss: 0.235306, acc.: 61.72%] [G loss: 0.436270]\n",
      "epoch:21 step:20231 [D loss: 0.216688, acc.: 67.97%] [G loss: 0.437095]\n",
      "epoch:21 step:20232 [D loss: 0.198361, acc.: 73.44%] [G loss: 0.465632]\n",
      "epoch:21 step:20233 [D loss: 0.227243, acc.: 67.19%] [G loss: 0.443716]\n",
      "epoch:21 step:20234 [D loss: 0.208417, acc.: 70.31%] [G loss: 0.472791]\n",
      "epoch:21 step:20235 [D loss: 0.207996, acc.: 66.41%] [G loss: 0.452695]\n",
      "epoch:21 step:20236 [D loss: 0.260516, acc.: 57.03%] [G loss: 0.421317]\n",
      "epoch:21 step:20237 [D loss: 0.227697, acc.: 57.03%] [G loss: 0.438734]\n",
      "epoch:21 step:20238 [D loss: 0.224110, acc.: 63.28%] [G loss: 0.428867]\n",
      "epoch:21 step:20239 [D loss: 0.238876, acc.: 58.59%] [G loss: 0.406034]\n",
      "epoch:21 step:20240 [D loss: 0.206675, acc.: 67.97%] [G loss: 0.431928]\n",
      "epoch:21 step:20241 [D loss: 0.181108, acc.: 75.78%] [G loss: 0.492807]\n",
      "epoch:21 step:20242 [D loss: 0.231674, acc.: 64.84%] [G loss: 0.474805]\n",
      "epoch:21 step:20243 [D loss: 0.260215, acc.: 56.25%] [G loss: 0.441528]\n",
      "epoch:21 step:20244 [D loss: 0.206416, acc.: 71.09%] [G loss: 0.431222]\n",
      "epoch:21 step:20245 [D loss: 0.209621, acc.: 66.41%] [G loss: 0.425726]\n",
      "epoch:21 step:20246 [D loss: 0.283900, acc.: 50.78%] [G loss: 0.350955]\n",
      "epoch:21 step:20247 [D loss: 0.216706, acc.: 66.41%] [G loss: 0.385460]\n",
      "epoch:21 step:20248 [D loss: 0.210725, acc.: 64.84%] [G loss: 0.412636]\n",
      "epoch:21 step:20249 [D loss: 0.198190, acc.: 69.53%] [G loss: 0.388646]\n",
      "epoch:21 step:20250 [D loss: 0.228906, acc.: 59.38%] [G loss: 0.459624]\n",
      "epoch:21 step:20251 [D loss: 0.166874, acc.: 75.78%] [G loss: 0.520282]\n",
      "epoch:21 step:20252 [D loss: 0.216913, acc.: 66.41%] [G loss: 0.478442]\n",
      "epoch:21 step:20253 [D loss: 0.247676, acc.: 57.81%] [G loss: 0.472013]\n",
      "epoch:21 step:20254 [D loss: 0.217310, acc.: 65.62%] [G loss: 0.442939]\n",
      "epoch:21 step:20255 [D loss: 0.221190, acc.: 67.97%] [G loss: 0.416704]\n",
      "epoch:21 step:20256 [D loss: 0.208917, acc.: 64.84%] [G loss: 0.444032]\n",
      "epoch:21 step:20257 [D loss: 0.236845, acc.: 56.25%] [G loss: 0.439013]\n",
      "epoch:21 step:20258 [D loss: 0.220206, acc.: 63.28%] [G loss: 0.400228]\n",
      "epoch:21 step:20259 [D loss: 0.202205, acc.: 64.84%] [G loss: 0.438268]\n",
      "epoch:21 step:20260 [D loss: 0.228063, acc.: 64.84%] [G loss: 0.416774]\n",
      "epoch:21 step:20261 [D loss: 0.223570, acc.: 62.50%] [G loss: 0.444021]\n",
      "epoch:21 step:20262 [D loss: 0.260746, acc.: 53.12%] [G loss: 0.420210]\n",
      "epoch:21 step:20263 [D loss: 0.244031, acc.: 56.25%] [G loss: 0.408362]\n",
      "epoch:21 step:20264 [D loss: 0.244691, acc.: 61.72%] [G loss: 0.388282]\n",
      "epoch:21 step:20265 [D loss: 0.237501, acc.: 59.38%] [G loss: 0.391779]\n",
      "epoch:21 step:20266 [D loss: 0.211049, acc.: 64.84%] [G loss: 0.422699]\n",
      "epoch:21 step:20267 [D loss: 0.253953, acc.: 55.47%] [G loss: 0.418772]\n",
      "epoch:21 step:20268 [D loss: 0.231325, acc.: 63.28%] [G loss: 0.464991]\n",
      "epoch:21 step:20269 [D loss: 0.209339, acc.: 70.31%] [G loss: 0.444950]\n",
      "epoch:21 step:20270 [D loss: 0.206347, acc.: 66.41%] [G loss: 0.441859]\n",
      "epoch:21 step:20271 [D loss: 0.245195, acc.: 57.03%] [G loss: 0.418893]\n",
      "epoch:21 step:20272 [D loss: 0.205897, acc.: 67.19%] [G loss: 0.402926]\n",
      "epoch:21 step:20273 [D loss: 0.226351, acc.: 63.28%] [G loss: 0.421244]\n",
      "epoch:21 step:20274 [D loss: 0.271485, acc.: 47.66%] [G loss: 0.406536]\n",
      "epoch:21 step:20275 [D loss: 0.205849, acc.: 64.84%] [G loss: 0.444890]\n",
      "epoch:21 step:20276 [D loss: 0.228064, acc.: 63.28%] [G loss: 0.454726]\n",
      "epoch:21 step:20277 [D loss: 0.245843, acc.: 60.16%] [G loss: 0.433224]\n",
      "epoch:21 step:20278 [D loss: 0.230176, acc.: 61.72%] [G loss: 0.443264]\n",
      "epoch:21 step:20279 [D loss: 0.230275, acc.: 65.62%] [G loss: 0.425629]\n",
      "epoch:21 step:20280 [D loss: 0.205702, acc.: 71.88%] [G loss: 0.415932]\n",
      "epoch:21 step:20281 [D loss: 0.214410, acc.: 67.97%] [G loss: 0.406758]\n",
      "epoch:21 step:20282 [D loss: 0.198399, acc.: 69.53%] [G loss: 0.480823]\n",
      "epoch:21 step:20283 [D loss: 0.231465, acc.: 56.25%] [G loss: 0.409888]\n",
      "epoch:21 step:20284 [D loss: 0.212555, acc.: 64.84%] [G loss: 0.458803]\n",
      "epoch:21 step:20285 [D loss: 0.208020, acc.: 65.62%] [G loss: 0.434575]\n",
      "epoch:21 step:20286 [D loss: 0.228972, acc.: 55.47%] [G loss: 0.426547]\n",
      "epoch:21 step:20287 [D loss: 0.212407, acc.: 65.62%] [G loss: 0.441411]\n",
      "epoch:21 step:20288 [D loss: 0.223611, acc.: 62.50%] [G loss: 0.425018]\n",
      "epoch:21 step:20289 [D loss: 0.231062, acc.: 58.59%] [G loss: 0.410654]\n",
      "epoch:21 step:20290 [D loss: 0.202289, acc.: 69.53%] [G loss: 0.449980]\n",
      "epoch:21 step:20291 [D loss: 0.267905, acc.: 49.22%] [G loss: 0.426616]\n",
      "epoch:21 step:20292 [D loss: 0.246573, acc.: 56.25%] [G loss: 0.426635]\n",
      "epoch:21 step:20293 [D loss: 0.238173, acc.: 60.16%] [G loss: 0.408830]\n",
      "epoch:21 step:20294 [D loss: 0.222051, acc.: 60.94%] [G loss: 0.427572]\n",
      "epoch:21 step:20295 [D loss: 0.244199, acc.: 57.81%] [G loss: 0.418003]\n",
      "epoch:21 step:20296 [D loss: 0.227080, acc.: 58.59%] [G loss: 0.403745]\n",
      "epoch:21 step:20297 [D loss: 0.212944, acc.: 64.06%] [G loss: 0.449347]\n",
      "epoch:21 step:20298 [D loss: 0.226374, acc.: 60.94%] [G loss: 0.449006]\n",
      "epoch:21 step:20299 [D loss: 0.207571, acc.: 65.62%] [G loss: 0.432178]\n",
      "epoch:21 step:20300 [D loss: 0.229449, acc.: 65.62%] [G loss: 0.439703]\n",
      "epoch:21 step:20301 [D loss: 0.198429, acc.: 71.09%] [G loss: 0.466365]\n",
      "epoch:21 step:20302 [D loss: 0.231120, acc.: 55.47%] [G loss: 0.423915]\n",
      "epoch:21 step:20303 [D loss: 0.234466, acc.: 52.34%] [G loss: 0.406498]\n",
      "epoch:21 step:20304 [D loss: 0.240043, acc.: 62.50%] [G loss: 0.418965]\n",
      "epoch:21 step:20305 [D loss: 0.229039, acc.: 61.72%] [G loss: 0.441078]\n",
      "epoch:21 step:20306 [D loss: 0.209412, acc.: 65.62%] [G loss: 0.450972]\n",
      "epoch:21 step:20307 [D loss: 0.217214, acc.: 66.41%] [G loss: 0.413704]\n",
      "epoch:21 step:20308 [D loss: 0.205150, acc.: 67.19%] [G loss: 0.434399]\n",
      "epoch:21 step:20309 [D loss: 0.193148, acc.: 65.62%] [G loss: 0.456623]\n",
      "epoch:21 step:20310 [D loss: 0.219849, acc.: 65.62%] [G loss: 0.436997]\n",
      "epoch:21 step:20311 [D loss: 0.190796, acc.: 71.09%] [G loss: 0.417722]\n",
      "epoch:21 step:20312 [D loss: 0.214044, acc.: 67.19%] [G loss: 0.466766]\n",
      "epoch:21 step:20313 [D loss: 0.230298, acc.: 57.03%] [G loss: 0.421395]\n",
      "epoch:21 step:20314 [D loss: 0.236116, acc.: 57.03%] [G loss: 0.426888]\n",
      "epoch:21 step:20315 [D loss: 0.206206, acc.: 70.31%] [G loss: 0.419328]\n",
      "epoch:21 step:20316 [D loss: 0.240741, acc.: 52.34%] [G loss: 0.413660]\n",
      "epoch:21 step:20317 [D loss: 0.206629, acc.: 66.41%] [G loss: 0.427153]\n",
      "epoch:21 step:20318 [D loss: 0.186444, acc.: 75.00%] [G loss: 0.456967]\n",
      "epoch:21 step:20319 [D loss: 0.204506, acc.: 72.66%] [G loss: 0.487632]\n",
      "epoch:21 step:20320 [D loss: 0.219789, acc.: 64.06%] [G loss: 0.455332]\n",
      "epoch:21 step:20321 [D loss: 0.243576, acc.: 50.00%] [G loss: 0.414033]\n",
      "epoch:21 step:20322 [D loss: 0.222682, acc.: 63.28%] [G loss: 0.402173]\n",
      "epoch:21 step:20323 [D loss: 0.221476, acc.: 66.41%] [G loss: 0.430809]\n",
      "epoch:21 step:20324 [D loss: 0.206888, acc.: 71.09%] [G loss: 0.452974]\n",
      "epoch:21 step:20325 [D loss: 0.176409, acc.: 71.09%] [G loss: 0.529838]\n",
      "epoch:21 step:20326 [D loss: 0.219598, acc.: 62.50%] [G loss: 0.476208]\n",
      "epoch:21 step:20327 [D loss: 0.216309, acc.: 67.19%] [G loss: 0.475240]\n",
      "epoch:21 step:20328 [D loss: 0.211151, acc.: 68.75%] [G loss: 0.437719]\n",
      "epoch:21 step:20329 [D loss: 0.222504, acc.: 66.41%] [G loss: 0.439023]\n",
      "epoch:21 step:20330 [D loss: 0.210595, acc.: 66.41%] [G loss: 0.466486]\n",
      "epoch:21 step:20331 [D loss: 0.223786, acc.: 63.28%] [G loss: 0.438149]\n",
      "epoch:21 step:20332 [D loss: 0.246795, acc.: 57.03%] [G loss: 0.429225]\n",
      "epoch:21 step:20333 [D loss: 0.238791, acc.: 62.50%] [G loss: 0.425527]\n",
      "epoch:21 step:20334 [D loss: 0.233173, acc.: 59.38%] [G loss: 0.425608]\n",
      "epoch:21 step:20335 [D loss: 0.227715, acc.: 67.97%] [G loss: 0.451654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20336 [D loss: 0.211226, acc.: 64.84%] [G loss: 0.459647]\n",
      "epoch:21 step:20337 [D loss: 0.227406, acc.: 63.28%] [G loss: 0.441083]\n",
      "epoch:21 step:20338 [D loss: 0.193990, acc.: 73.44%] [G loss: 0.497946]\n",
      "epoch:21 step:20339 [D loss: 0.216391, acc.: 61.72%] [G loss: 0.433437]\n",
      "epoch:21 step:20340 [D loss: 0.239635, acc.: 55.47%] [G loss: 0.392866]\n",
      "epoch:21 step:20341 [D loss: 0.239002, acc.: 55.47%] [G loss: 0.423617]\n",
      "epoch:21 step:20342 [D loss: 0.216011, acc.: 65.62%] [G loss: 0.454584]\n",
      "epoch:21 step:20343 [D loss: 0.231262, acc.: 61.72%] [G loss: 0.441939]\n",
      "epoch:21 step:20344 [D loss: 0.223543, acc.: 65.62%] [G loss: 0.434578]\n",
      "epoch:21 step:20345 [D loss: 0.220170, acc.: 64.06%] [G loss: 0.392575]\n",
      "epoch:21 step:20346 [D loss: 0.220611, acc.: 64.06%] [G loss: 0.422712]\n",
      "epoch:21 step:20347 [D loss: 0.252273, acc.: 56.25%] [G loss: 0.442981]\n",
      "epoch:21 step:20348 [D loss: 0.221177, acc.: 64.06%] [G loss: 0.430105]\n",
      "epoch:21 step:20349 [D loss: 0.224136, acc.: 62.50%] [G loss: 0.415596]\n",
      "epoch:21 step:20350 [D loss: 0.224769, acc.: 58.59%] [G loss: 0.415353]\n",
      "epoch:21 step:20351 [D loss: 0.227163, acc.: 61.72%] [G loss: 0.465517]\n",
      "epoch:21 step:20352 [D loss: 0.211922, acc.: 67.97%] [G loss: 0.435567]\n",
      "epoch:21 step:20353 [D loss: 0.225050, acc.: 64.06%] [G loss: 0.394908]\n",
      "epoch:21 step:20354 [D loss: 0.198781, acc.: 67.19%] [G loss: 0.460402]\n",
      "epoch:21 step:20355 [D loss: 0.210458, acc.: 65.62%] [G loss: 0.456537]\n",
      "epoch:21 step:20356 [D loss: 0.220856, acc.: 67.19%] [G loss: 0.423722]\n",
      "epoch:21 step:20357 [D loss: 0.209369, acc.: 60.16%] [G loss: 0.438311]\n",
      "epoch:21 step:20358 [D loss: 0.187708, acc.: 76.56%] [G loss: 0.430176]\n",
      "epoch:21 step:20359 [D loss: 0.235124, acc.: 61.72%] [G loss: 0.402272]\n",
      "epoch:21 step:20360 [D loss: 0.231338, acc.: 57.81%] [G loss: 0.413229]\n",
      "epoch:21 step:20361 [D loss: 0.221438, acc.: 63.28%] [G loss: 0.388218]\n",
      "epoch:21 step:20362 [D loss: 0.230434, acc.: 57.03%] [G loss: 0.414168]\n",
      "epoch:21 step:20363 [D loss: 0.199227, acc.: 68.75%] [G loss: 0.450344]\n",
      "epoch:21 step:20364 [D loss: 0.236101, acc.: 57.03%] [G loss: 0.396640]\n",
      "epoch:21 step:20365 [D loss: 0.209067, acc.: 63.28%] [G loss: 0.432659]\n",
      "epoch:21 step:20366 [D loss: 0.218822, acc.: 67.97%] [G loss: 0.425972]\n",
      "epoch:21 step:20367 [D loss: 0.191719, acc.: 69.53%] [G loss: 0.486399]\n",
      "epoch:21 step:20368 [D loss: 0.215090, acc.: 66.41%] [G loss: 0.426714]\n",
      "epoch:21 step:20369 [D loss: 0.208095, acc.: 66.41%] [G loss: 0.426435]\n",
      "epoch:21 step:20370 [D loss: 0.217546, acc.: 69.53%] [G loss: 0.452890]\n",
      "epoch:21 step:20371 [D loss: 0.198135, acc.: 67.19%] [G loss: 0.482805]\n",
      "epoch:21 step:20372 [D loss: 0.229595, acc.: 64.06%] [G loss: 0.450916]\n",
      "epoch:21 step:20373 [D loss: 0.257000, acc.: 51.56%] [G loss: 0.449655]\n",
      "epoch:21 step:20374 [D loss: 0.221871, acc.: 59.38%] [G loss: 0.426158]\n",
      "epoch:21 step:20375 [D loss: 0.230507, acc.: 60.94%] [G loss: 0.418233]\n",
      "epoch:21 step:20376 [D loss: 0.218685, acc.: 63.28%] [G loss: 0.448718]\n",
      "epoch:21 step:20377 [D loss: 0.193230, acc.: 67.97%] [G loss: 0.488346]\n",
      "epoch:21 step:20378 [D loss: 0.230748, acc.: 60.94%] [G loss: 0.430527]\n",
      "epoch:21 step:20379 [D loss: 0.233196, acc.: 62.50%] [G loss: 0.461516]\n",
      "epoch:21 step:20380 [D loss: 0.262696, acc.: 52.34%] [G loss: 0.444003]\n",
      "epoch:21 step:20381 [D loss: 0.234718, acc.: 57.03%] [G loss: 0.360086]\n",
      "epoch:21 step:20382 [D loss: 0.234640, acc.: 61.72%] [G loss: 0.406839]\n",
      "epoch:21 step:20383 [D loss: 0.204868, acc.: 68.75%] [G loss: 0.461534]\n",
      "epoch:21 step:20384 [D loss: 0.215811, acc.: 68.75%] [G loss: 0.451152]\n",
      "epoch:21 step:20385 [D loss: 0.208503, acc.: 68.75%] [G loss: 0.459014]\n",
      "epoch:21 step:20386 [D loss: 0.197697, acc.: 71.88%] [G loss: 0.460898]\n",
      "epoch:21 step:20387 [D loss: 0.250727, acc.: 51.56%] [G loss: 0.421630]\n",
      "epoch:21 step:20388 [D loss: 0.215682, acc.: 61.72%] [G loss: 0.410360]\n",
      "epoch:21 step:20389 [D loss: 0.214143, acc.: 67.19%] [G loss: 0.435569]\n",
      "epoch:21 step:20390 [D loss: 0.238429, acc.: 61.72%] [G loss: 0.438633]\n",
      "epoch:21 step:20391 [D loss: 0.216307, acc.: 60.94%] [G loss: 0.472333]\n",
      "epoch:21 step:20392 [D loss: 0.231910, acc.: 64.06%] [G loss: 0.452421]\n",
      "epoch:21 step:20393 [D loss: 0.230510, acc.: 61.72%] [G loss: 0.445663]\n",
      "epoch:21 step:20394 [D loss: 0.239926, acc.: 60.94%] [G loss: 0.384469]\n",
      "epoch:21 step:20395 [D loss: 0.232190, acc.: 62.50%] [G loss: 0.415459]\n",
      "epoch:21 step:20396 [D loss: 0.207371, acc.: 69.53%] [G loss: 0.494687]\n",
      "epoch:21 step:20397 [D loss: 0.223715, acc.: 55.47%] [G loss: 0.440532]\n",
      "epoch:21 step:20398 [D loss: 0.213963, acc.: 60.94%] [G loss: 0.404570]\n",
      "epoch:21 step:20399 [D loss: 0.233923, acc.: 61.72%] [G loss: 0.380361]\n",
      "epoch:21 step:20400 [D loss: 0.209850, acc.: 71.88%] [G loss: 0.431602]\n",
      "##############\n",
      "[2.62532333 1.94319146 6.35498146 4.8464202  3.64841532 5.82322095\n",
      " 4.78286213 4.6341248  4.59839754 4.07496017]\n",
      "##########\n",
      "epoch:21 step:20401 [D loss: 0.204062, acc.: 67.19%] [G loss: 0.428553]\n",
      "epoch:21 step:20402 [D loss: 0.196431, acc.: 73.44%] [G loss: 0.447202]\n",
      "epoch:21 step:20403 [D loss: 0.223475, acc.: 61.72%] [G loss: 0.429690]\n",
      "epoch:21 step:20404 [D loss: 0.231884, acc.: 63.28%] [G loss: 0.418508]\n",
      "epoch:21 step:20405 [D loss: 0.217355, acc.: 64.06%] [G loss: 0.421294]\n",
      "epoch:21 step:20406 [D loss: 0.236346, acc.: 58.59%] [G loss: 0.410667]\n",
      "epoch:21 step:20407 [D loss: 0.234137, acc.: 61.72%] [G loss: 0.434294]\n",
      "epoch:21 step:20408 [D loss: 0.220327, acc.: 65.62%] [G loss: 0.451770]\n",
      "epoch:21 step:20409 [D loss: 0.214901, acc.: 64.84%] [G loss: 0.410010]\n",
      "epoch:21 step:20410 [D loss: 0.206219, acc.: 67.97%] [G loss: 0.459788]\n",
      "epoch:21 step:20411 [D loss: 0.231450, acc.: 64.84%] [G loss: 0.433131]\n",
      "epoch:21 step:20412 [D loss: 0.221455, acc.: 59.38%] [G loss: 0.444225]\n",
      "epoch:21 step:20413 [D loss: 0.233178, acc.: 58.59%] [G loss: 0.410967]\n",
      "epoch:21 step:20414 [D loss: 0.216681, acc.: 64.84%] [G loss: 0.423635]\n",
      "epoch:21 step:20415 [D loss: 0.241737, acc.: 55.47%] [G loss: 0.395946]\n",
      "epoch:21 step:20416 [D loss: 0.244506, acc.: 60.16%] [G loss: 0.387800]\n",
      "epoch:21 step:20417 [D loss: 0.242248, acc.: 57.81%] [G loss: 0.405604]\n",
      "epoch:21 step:20418 [D loss: 0.250175, acc.: 51.56%] [G loss: 0.430205]\n",
      "epoch:21 step:20419 [D loss: 0.233313, acc.: 62.50%] [G loss: 0.412306]\n",
      "epoch:21 step:20420 [D loss: 0.220229, acc.: 61.72%] [G loss: 0.443242]\n",
      "epoch:21 step:20421 [D loss: 0.226189, acc.: 65.62%] [G loss: 0.434363]\n",
      "epoch:21 step:20422 [D loss: 0.225479, acc.: 62.50%] [G loss: 0.457504]\n",
      "epoch:21 step:20423 [D loss: 0.224937, acc.: 59.38%] [G loss: 0.404729]\n",
      "epoch:21 step:20424 [D loss: 0.218076, acc.: 62.50%] [G loss: 0.418929]\n",
      "epoch:21 step:20425 [D loss: 0.238435, acc.: 60.94%] [G loss: 0.423266]\n",
      "epoch:21 step:20426 [D loss: 0.252524, acc.: 57.03%] [G loss: 0.384793]\n",
      "epoch:21 step:20427 [D loss: 0.202953, acc.: 65.62%] [G loss: 0.469745]\n",
      "epoch:21 step:20428 [D loss: 0.215847, acc.: 63.28%] [G loss: 0.410320]\n",
      "epoch:21 step:20429 [D loss: 0.250593, acc.: 51.56%] [G loss: 0.433400]\n",
      "epoch:21 step:20430 [D loss: 0.232853, acc.: 61.72%] [G loss: 0.446865]\n",
      "epoch:21 step:20431 [D loss: 0.226380, acc.: 65.62%] [G loss: 0.484949]\n",
      "epoch:21 step:20432 [D loss: 0.227194, acc.: 65.62%] [G loss: 0.435445]\n",
      "epoch:21 step:20433 [D loss: 0.235721, acc.: 59.38%] [G loss: 0.438120]\n",
      "epoch:21 step:20434 [D loss: 0.245421, acc.: 60.16%] [G loss: 0.411925]\n",
      "epoch:21 step:20435 [D loss: 0.215924, acc.: 60.94%] [G loss: 0.416011]\n",
      "epoch:21 step:20436 [D loss: 0.236547, acc.: 59.38%] [G loss: 0.407016]\n",
      "epoch:21 step:20437 [D loss: 0.228666, acc.: 60.16%] [G loss: 0.424762]\n",
      "epoch:21 step:20438 [D loss: 0.205307, acc.: 71.09%] [G loss: 0.421172]\n",
      "epoch:21 step:20439 [D loss: 0.237113, acc.: 56.25%] [G loss: 0.394857]\n",
      "epoch:21 step:20440 [D loss: 0.234811, acc.: 57.03%] [G loss: 0.434265]\n",
      "epoch:21 step:20441 [D loss: 0.233525, acc.: 59.38%] [G loss: 0.468148]\n",
      "epoch:21 step:20442 [D loss: 0.279365, acc.: 51.56%] [G loss: 0.388902]\n",
      "epoch:21 step:20443 [D loss: 0.249136, acc.: 57.03%] [G loss: 0.384162]\n",
      "epoch:21 step:20444 [D loss: 0.206767, acc.: 67.97%] [G loss: 0.415277]\n",
      "epoch:21 step:20445 [D loss: 0.220509, acc.: 65.62%] [G loss: 0.449338]\n",
      "epoch:21 step:20446 [D loss: 0.220215, acc.: 63.28%] [G loss: 0.456155]\n",
      "epoch:21 step:20447 [D loss: 0.219027, acc.: 60.94%] [G loss: 0.421214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20448 [D loss: 0.213394, acc.: 66.41%] [G loss: 0.451116]\n",
      "epoch:21 step:20449 [D loss: 0.264995, acc.: 51.56%] [G loss: 0.394990]\n",
      "epoch:21 step:20450 [D loss: 0.238028, acc.: 66.41%] [G loss: 0.419475]\n",
      "epoch:21 step:20451 [D loss: 0.212994, acc.: 67.19%] [G loss: 0.446729]\n",
      "epoch:21 step:20452 [D loss: 0.222908, acc.: 64.84%] [G loss: 0.473896]\n",
      "epoch:21 step:20453 [D loss: 0.252746, acc.: 58.59%] [G loss: 0.436978]\n",
      "epoch:21 step:20454 [D loss: 0.211273, acc.: 67.19%] [G loss: 0.459300]\n",
      "epoch:21 step:20455 [D loss: 0.243732, acc.: 58.59%] [G loss: 0.441805]\n",
      "epoch:21 step:20456 [D loss: 0.205635, acc.: 71.09%] [G loss: 0.453419]\n",
      "epoch:21 step:20457 [D loss: 0.192920, acc.: 69.53%] [G loss: 0.446758]\n",
      "epoch:21 step:20458 [D loss: 0.197662, acc.: 68.75%] [G loss: 0.475623]\n",
      "epoch:21 step:20459 [D loss: 0.221389, acc.: 64.06%] [G loss: 0.484004]\n",
      "epoch:21 step:20460 [D loss: 0.256891, acc.: 51.56%] [G loss: 0.453993]\n",
      "epoch:21 step:20461 [D loss: 0.260517, acc.: 46.09%] [G loss: 0.449537]\n",
      "epoch:21 step:20462 [D loss: 0.218181, acc.: 67.19%] [G loss: 0.382325]\n",
      "epoch:21 step:20463 [D loss: 0.199233, acc.: 69.53%] [G loss: 0.446634]\n",
      "epoch:21 step:20464 [D loss: 0.269816, acc.: 50.00%] [G loss: 0.406279]\n",
      "epoch:21 step:20465 [D loss: 0.244347, acc.: 58.59%] [G loss: 0.403874]\n",
      "epoch:21 step:20466 [D loss: 0.232670, acc.: 60.94%] [G loss: 0.412275]\n",
      "epoch:21 step:20467 [D loss: 0.221709, acc.: 66.41%] [G loss: 0.443899]\n",
      "epoch:21 step:20468 [D loss: 0.256566, acc.: 53.12%] [G loss: 0.392764]\n",
      "epoch:21 step:20469 [D loss: 0.207900, acc.: 71.09%] [G loss: 0.425714]\n",
      "epoch:21 step:20470 [D loss: 0.209905, acc.: 60.16%] [G loss: 0.456559]\n",
      "epoch:21 step:20471 [D loss: 0.249793, acc.: 57.03%] [G loss: 0.416742]\n",
      "epoch:21 step:20472 [D loss: 0.284826, acc.: 42.97%] [G loss: 0.390293]\n",
      "epoch:21 step:20473 [D loss: 0.219091, acc.: 67.19%] [G loss: 0.451301]\n",
      "epoch:21 step:20474 [D loss: 0.222601, acc.: 60.94%] [G loss: 0.411722]\n",
      "epoch:21 step:20475 [D loss: 0.238027, acc.: 57.03%] [G loss: 0.374518]\n",
      "epoch:21 step:20476 [D loss: 0.238452, acc.: 54.69%] [G loss: 0.414219]\n",
      "epoch:21 step:20477 [D loss: 0.217733, acc.: 67.19%] [G loss: 0.452952]\n",
      "epoch:21 step:20478 [D loss: 0.195936, acc.: 70.31%] [G loss: 0.449327]\n",
      "epoch:21 step:20479 [D loss: 0.219700, acc.: 62.50%] [G loss: 0.497295]\n",
      "epoch:21 step:20480 [D loss: 0.211319, acc.: 62.50%] [G loss: 0.459895]\n",
      "epoch:21 step:20481 [D loss: 0.223282, acc.: 60.16%] [G loss: 0.443294]\n",
      "epoch:21 step:20482 [D loss: 0.229755, acc.: 62.50%] [G loss: 0.443391]\n",
      "epoch:21 step:20483 [D loss: 0.230354, acc.: 61.72%] [G loss: 0.404253]\n",
      "epoch:21 step:20484 [D loss: 0.233268, acc.: 61.72%] [G loss: 0.401770]\n",
      "epoch:21 step:20485 [D loss: 0.239765, acc.: 57.81%] [G loss: 0.388690]\n",
      "epoch:21 step:20486 [D loss: 0.250153, acc.: 56.25%] [G loss: 0.370252]\n",
      "epoch:21 step:20487 [D loss: 0.213638, acc.: 66.41%] [G loss: 0.432648]\n",
      "epoch:21 step:20488 [D loss: 0.231228, acc.: 58.59%] [G loss: 0.403351]\n",
      "epoch:21 step:20489 [D loss: 0.223292, acc.: 60.94%] [G loss: 0.441578]\n",
      "epoch:21 step:20490 [D loss: 0.218215, acc.: 71.88%] [G loss: 0.435492]\n",
      "epoch:21 step:20491 [D loss: 0.199343, acc.: 66.41%] [G loss: 0.417982]\n",
      "epoch:21 step:20492 [D loss: 0.198536, acc.: 73.44%] [G loss: 0.476486]\n",
      "epoch:21 step:20493 [D loss: 0.227132, acc.: 59.38%] [G loss: 0.428815]\n",
      "epoch:21 step:20494 [D loss: 0.242064, acc.: 55.47%] [G loss: 0.394832]\n",
      "epoch:21 step:20495 [D loss: 0.246100, acc.: 58.59%] [G loss: 0.398985]\n",
      "epoch:21 step:20496 [D loss: 0.220178, acc.: 66.41%] [G loss: 0.429046]\n",
      "epoch:21 step:20497 [D loss: 0.280744, acc.: 50.00%] [G loss: 0.428786]\n",
      "epoch:21 step:20498 [D loss: 0.238653, acc.: 57.03%] [G loss: 0.403014]\n",
      "epoch:21 step:20499 [D loss: 0.202159, acc.: 67.19%] [G loss: 0.434996]\n",
      "epoch:21 step:20500 [D loss: 0.216272, acc.: 67.19%] [G loss: 0.460536]\n",
      "epoch:21 step:20501 [D loss: 0.224859, acc.: 59.38%] [G loss: 0.422629]\n",
      "epoch:21 step:20502 [D loss: 0.208378, acc.: 69.53%] [G loss: 0.399708]\n",
      "epoch:21 step:20503 [D loss: 0.212660, acc.: 65.62%] [G loss: 0.468543]\n",
      "epoch:21 step:20504 [D loss: 0.267795, acc.: 52.34%] [G loss: 0.413585]\n",
      "epoch:21 step:20505 [D loss: 0.251387, acc.: 56.25%] [G loss: 0.394827]\n",
      "epoch:21 step:20506 [D loss: 0.231550, acc.: 59.38%] [G loss: 0.402614]\n",
      "epoch:21 step:20507 [D loss: 0.217841, acc.: 64.84%] [G loss: 0.430499]\n",
      "epoch:21 step:20508 [D loss: 0.222053, acc.: 62.50%] [G loss: 0.433034]\n",
      "epoch:21 step:20509 [D loss: 0.201086, acc.: 67.19%] [G loss: 0.425882]\n",
      "epoch:21 step:20510 [D loss: 0.207020, acc.: 64.84%] [G loss: 0.482841]\n",
      "epoch:21 step:20511 [D loss: 0.233920, acc.: 57.03%] [G loss: 0.443612]\n",
      "epoch:21 step:20512 [D loss: 0.206763, acc.: 68.75%] [G loss: 0.423682]\n",
      "epoch:21 step:20513 [D loss: 0.236264, acc.: 61.72%] [G loss: 0.395725]\n",
      "epoch:21 step:20514 [D loss: 0.215510, acc.: 62.50%] [G loss: 0.403672]\n",
      "epoch:21 step:20515 [D loss: 0.201242, acc.: 67.97%] [G loss: 0.448906]\n",
      "epoch:21 step:20516 [D loss: 0.215138, acc.: 66.41%] [G loss: 0.426877]\n",
      "epoch:21 step:20517 [D loss: 0.218638, acc.: 62.50%] [G loss: 0.417082]\n",
      "epoch:21 step:20518 [D loss: 0.224955, acc.: 65.62%] [G loss: 0.425882]\n",
      "epoch:21 step:20519 [D loss: 0.209548, acc.: 70.31%] [G loss: 0.422137]\n",
      "epoch:21 step:20520 [D loss: 0.208346, acc.: 70.31%] [G loss: 0.447693]\n",
      "epoch:21 step:20521 [D loss: 0.230243, acc.: 63.28%] [G loss: 0.438256]\n",
      "epoch:21 step:20522 [D loss: 0.202937, acc.: 69.53%] [G loss: 0.477614]\n",
      "epoch:21 step:20523 [D loss: 0.223661, acc.: 59.38%] [G loss: 0.457451]\n",
      "epoch:21 step:20524 [D loss: 0.257746, acc.: 60.16%] [G loss: 0.399928]\n",
      "epoch:21 step:20525 [D loss: 0.235667, acc.: 60.16%] [G loss: 0.406097]\n",
      "epoch:21 step:20526 [D loss: 0.214988, acc.: 64.84%] [G loss: 0.387170]\n",
      "epoch:21 step:20527 [D loss: 0.250996, acc.: 57.81%] [G loss: 0.412436]\n",
      "epoch:21 step:20528 [D loss: 0.240756, acc.: 55.47%] [G loss: 0.454053]\n",
      "epoch:21 step:20529 [D loss: 0.220336, acc.: 61.72%] [G loss: 0.429365]\n",
      "epoch:21 step:20530 [D loss: 0.194998, acc.: 71.88%] [G loss: 0.473091]\n",
      "epoch:21 step:20531 [D loss: 0.246068, acc.: 55.47%] [G loss: 0.410716]\n",
      "epoch:21 step:20532 [D loss: 0.237327, acc.: 59.38%] [G loss: 0.448204]\n",
      "epoch:21 step:20533 [D loss: 0.241511, acc.: 57.03%] [G loss: 0.453526]\n",
      "epoch:21 step:20534 [D loss: 0.216234, acc.: 63.28%] [G loss: 0.422117]\n",
      "epoch:21 step:20535 [D loss: 0.259449, acc.: 53.91%] [G loss: 0.386076]\n",
      "epoch:21 step:20536 [D loss: 0.238366, acc.: 55.47%] [G loss: 0.412562]\n",
      "epoch:21 step:20537 [D loss: 0.204473, acc.: 65.62%] [G loss: 0.463826]\n",
      "epoch:21 step:20538 [D loss: 0.248657, acc.: 54.69%] [G loss: 0.408848]\n",
      "epoch:21 step:20539 [D loss: 0.241361, acc.: 54.69%] [G loss: 0.415788]\n",
      "epoch:21 step:20540 [D loss: 0.223209, acc.: 63.28%] [G loss: 0.406055]\n",
      "epoch:21 step:20541 [D loss: 0.219158, acc.: 64.84%] [G loss: 0.412434]\n",
      "epoch:21 step:20542 [D loss: 0.242876, acc.: 58.59%] [G loss: 0.406521]\n",
      "epoch:21 step:20543 [D loss: 0.205143, acc.: 73.44%] [G loss: 0.407589]\n",
      "epoch:21 step:20544 [D loss: 0.237817, acc.: 57.81%] [G loss: 0.430172]\n",
      "epoch:21 step:20545 [D loss: 0.233870, acc.: 63.28%] [G loss: 0.373950]\n",
      "epoch:21 step:20546 [D loss: 0.240862, acc.: 60.94%] [G loss: 0.434009]\n",
      "epoch:21 step:20547 [D loss: 0.242580, acc.: 59.38%] [G loss: 0.400358]\n",
      "epoch:21 step:20548 [D loss: 0.208494, acc.: 70.31%] [G loss: 0.415612]\n",
      "epoch:21 step:20549 [D loss: 0.223404, acc.: 61.72%] [G loss: 0.423039]\n",
      "epoch:21 step:20550 [D loss: 0.220262, acc.: 62.50%] [G loss: 0.395983]\n",
      "epoch:21 step:20551 [D loss: 0.225793, acc.: 60.94%] [G loss: 0.422797]\n",
      "epoch:21 step:20552 [D loss: 0.196452, acc.: 66.41%] [G loss: 0.432962]\n",
      "epoch:21 step:20553 [D loss: 0.235285, acc.: 60.94%] [G loss: 0.400636]\n",
      "epoch:21 step:20554 [D loss: 0.264935, acc.: 46.09%] [G loss: 0.377914]\n",
      "epoch:21 step:20555 [D loss: 0.224740, acc.: 60.94%] [G loss: 0.412997]\n",
      "epoch:21 step:20556 [D loss: 0.236986, acc.: 57.81%] [G loss: 0.460621]\n",
      "epoch:21 step:20557 [D loss: 0.229309, acc.: 57.81%] [G loss: 0.462633]\n",
      "epoch:21 step:20558 [D loss: 0.249564, acc.: 52.34%] [G loss: 0.389999]\n",
      "epoch:21 step:20559 [D loss: 0.231895, acc.: 61.72%] [G loss: 0.448923]\n",
      "epoch:21 step:20560 [D loss: 0.225539, acc.: 64.84%] [G loss: 0.460322]\n",
      "epoch:21 step:20561 [D loss: 0.214054, acc.: 66.41%] [G loss: 0.468681]\n",
      "epoch:21 step:20562 [D loss: 0.205700, acc.: 71.09%] [G loss: 0.463116]\n",
      "epoch:21 step:20563 [D loss: 0.221157, acc.: 59.38%] [G loss: 0.452132]\n",
      "epoch:21 step:20564 [D loss: 0.217170, acc.: 67.19%] [G loss: 0.444948]\n",
      "epoch:21 step:20565 [D loss: 0.233786, acc.: 59.38%] [G loss: 0.440857]\n",
      "epoch:21 step:20566 [D loss: 0.217909, acc.: 61.72%] [G loss: 0.441608]\n",
      "epoch:21 step:20567 [D loss: 0.198320, acc.: 67.19%] [G loss: 0.440766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20568 [D loss: 0.278115, acc.: 50.78%] [G loss: 0.384359]\n",
      "epoch:21 step:20569 [D loss: 0.247643, acc.: 57.81%] [G loss: 0.407440]\n",
      "epoch:21 step:20570 [D loss: 0.205208, acc.: 71.09%] [G loss: 0.428183]\n",
      "epoch:21 step:20571 [D loss: 0.189054, acc.: 75.78%] [G loss: 0.456450]\n",
      "epoch:21 step:20572 [D loss: 0.216061, acc.: 63.28%] [G loss: 0.469933]\n",
      "epoch:21 step:20573 [D loss: 0.209452, acc.: 64.84%] [G loss: 0.465481]\n",
      "epoch:21 step:20574 [D loss: 0.212877, acc.: 62.50%] [G loss: 0.416254]\n",
      "epoch:21 step:20575 [D loss: 0.180156, acc.: 76.56%] [G loss: 0.479317]\n",
      "epoch:21 step:20576 [D loss: 0.201419, acc.: 62.50%] [G loss: 0.472373]\n",
      "epoch:21 step:20577 [D loss: 0.201662, acc.: 71.09%] [G loss: 0.429630]\n",
      "epoch:21 step:20578 [D loss: 0.219675, acc.: 64.84%] [G loss: 0.496260]\n",
      "epoch:21 step:20579 [D loss: 0.240382, acc.: 59.38%] [G loss: 0.459063]\n",
      "epoch:21 step:20580 [D loss: 0.226089, acc.: 60.16%] [G loss: 0.466086]\n",
      "epoch:21 step:20581 [D loss: 0.212436, acc.: 66.41%] [G loss: 0.420786]\n",
      "epoch:21 step:20582 [D loss: 0.203108, acc.: 67.19%] [G loss: 0.467075]\n",
      "epoch:21 step:20583 [D loss: 0.214239, acc.: 60.94%] [G loss: 0.462603]\n",
      "epoch:21 step:20584 [D loss: 0.217616, acc.: 65.62%] [G loss: 0.426024]\n",
      "epoch:21 step:20585 [D loss: 0.227197, acc.: 57.03%] [G loss: 0.435605]\n",
      "epoch:21 step:20586 [D loss: 0.202494, acc.: 65.62%] [G loss: 0.456539]\n",
      "epoch:21 step:20587 [D loss: 0.194209, acc.: 69.53%] [G loss: 0.455010]\n",
      "epoch:21 step:20588 [D loss: 0.195841, acc.: 69.53%] [G loss: 0.484755]\n",
      "epoch:21 step:20589 [D loss: 0.194834, acc.: 71.09%] [G loss: 0.469080]\n",
      "epoch:21 step:20590 [D loss: 0.238138, acc.: 62.50%] [G loss: 0.432239]\n",
      "epoch:21 step:20591 [D loss: 0.223085, acc.: 62.50%] [G loss: 0.437847]\n",
      "epoch:21 step:20592 [D loss: 0.288336, acc.: 50.78%] [G loss: 0.424922]\n",
      "epoch:21 step:20593 [D loss: 0.207646, acc.: 66.41%] [G loss: 0.395613]\n",
      "epoch:21 step:20594 [D loss: 0.211371, acc.: 67.97%] [G loss: 0.426311]\n",
      "epoch:21 step:20595 [D loss: 0.186218, acc.: 72.66%] [G loss: 0.434896]\n",
      "epoch:21 step:20596 [D loss: 0.208593, acc.: 67.19%] [G loss: 0.480038]\n",
      "epoch:21 step:20597 [D loss: 0.302132, acc.: 44.53%] [G loss: 0.481831]\n",
      "epoch:21 step:20598 [D loss: 0.219396, acc.: 61.72%] [G loss: 0.459546]\n",
      "epoch:21 step:20599 [D loss: 0.249323, acc.: 58.59%] [G loss: 0.455299]\n",
      "epoch:21 step:20600 [D loss: 0.202776, acc.: 68.75%] [G loss: 0.438829]\n",
      "##############\n",
      "[2.66408412 2.07931693 6.10738046 4.6720741  3.54089957 5.48138429\n",
      " 4.45396254 4.66779821 4.40512711 4.17953858]\n",
      "##########\n",
      "epoch:21 step:20601 [D loss: 0.219927, acc.: 67.19%] [G loss: 0.448089]\n",
      "epoch:21 step:20602 [D loss: 0.214945, acc.: 69.53%] [G loss: 0.463523]\n",
      "epoch:21 step:20603 [D loss: 0.184398, acc.: 75.78%] [G loss: 0.496993]\n",
      "epoch:21 step:20604 [D loss: 0.212707, acc.: 62.50%] [G loss: 0.508320]\n",
      "epoch:21 step:20605 [D loss: 0.315138, acc.: 54.69%] [G loss: 0.491209]\n",
      "epoch:21 step:20606 [D loss: 0.248295, acc.: 53.12%] [G loss: 0.540753]\n",
      "epoch:21 step:20607 [D loss: 0.238055, acc.: 69.53%] [G loss: 0.517038]\n",
      "epoch:21 step:20608 [D loss: 0.238104, acc.: 60.16%] [G loss: 0.396860]\n",
      "epoch:21 step:20609 [D loss: 0.261473, acc.: 58.59%] [G loss: 0.420538]\n",
      "epoch:21 step:20610 [D loss: 0.230900, acc.: 62.50%] [G loss: 0.416408]\n",
      "epoch:21 step:20611 [D loss: 0.197293, acc.: 74.22%] [G loss: 0.445290]\n",
      "epoch:21 step:20612 [D loss: 0.164569, acc.: 78.12%] [G loss: 0.493057]\n",
      "epoch:21 step:20613 [D loss: 0.199882, acc.: 70.31%] [G loss: 0.477411]\n",
      "epoch:21 step:20614 [D loss: 0.184900, acc.: 77.34%] [G loss: 0.545615]\n",
      "epoch:22 step:20615 [D loss: 0.267582, acc.: 54.69%] [G loss: 0.472446]\n",
      "epoch:22 step:20616 [D loss: 0.254862, acc.: 57.03%] [G loss: 0.455470]\n",
      "epoch:22 step:20617 [D loss: 0.253021, acc.: 58.59%] [G loss: 0.448796]\n",
      "epoch:22 step:20618 [D loss: 0.221346, acc.: 66.41%] [G loss: 0.432446]\n",
      "epoch:22 step:20619 [D loss: 0.225269, acc.: 61.72%] [G loss: 0.448828]\n",
      "epoch:22 step:20620 [D loss: 0.206805, acc.: 66.41%] [G loss: 0.416789]\n",
      "epoch:22 step:20621 [D loss: 0.198055, acc.: 71.88%] [G loss: 0.436863]\n",
      "epoch:22 step:20622 [D loss: 0.222994, acc.: 61.72%] [G loss: 0.409802]\n",
      "epoch:22 step:20623 [D loss: 0.188986, acc.: 73.44%] [G loss: 0.448512]\n",
      "epoch:22 step:20624 [D loss: 0.216433, acc.: 64.84%] [G loss: 0.428921]\n",
      "epoch:22 step:20625 [D loss: 0.191939, acc.: 74.22%] [G loss: 0.460347]\n",
      "epoch:22 step:20626 [D loss: 0.225905, acc.: 57.81%] [G loss: 0.480814]\n",
      "epoch:22 step:20627 [D loss: 0.205898, acc.: 65.62%] [G loss: 0.459718]\n",
      "epoch:22 step:20628 [D loss: 0.205413, acc.: 66.41%] [G loss: 0.443089]\n",
      "epoch:22 step:20629 [D loss: 0.199125, acc.: 70.31%] [G loss: 0.436267]\n",
      "epoch:22 step:20630 [D loss: 0.195769, acc.: 71.09%] [G loss: 0.432631]\n",
      "epoch:22 step:20631 [D loss: 0.262670, acc.: 56.25%] [G loss: 0.415254]\n",
      "epoch:22 step:20632 [D loss: 0.214463, acc.: 61.72%] [G loss: 0.420475]\n",
      "epoch:22 step:20633 [D loss: 0.232426, acc.: 56.25%] [G loss: 0.439290]\n",
      "epoch:22 step:20634 [D loss: 0.249331, acc.: 59.38%] [G loss: 0.446556]\n",
      "epoch:22 step:20635 [D loss: 0.202472, acc.: 71.09%] [G loss: 0.462469]\n",
      "epoch:22 step:20636 [D loss: 0.209934, acc.: 67.97%] [G loss: 0.491158]\n",
      "epoch:22 step:20637 [D loss: 0.241682, acc.: 58.59%] [G loss: 0.403054]\n",
      "epoch:22 step:20638 [D loss: 0.205740, acc.: 66.41%] [G loss: 0.421154]\n",
      "epoch:22 step:20639 [D loss: 0.215922, acc.: 66.41%] [G loss: 0.386440]\n",
      "epoch:22 step:20640 [D loss: 0.237365, acc.: 58.59%] [G loss: 0.457644]\n",
      "epoch:22 step:20641 [D loss: 0.220314, acc.: 65.62%] [G loss: 0.394656]\n",
      "epoch:22 step:20642 [D loss: 0.237833, acc.: 57.03%] [G loss: 0.400209]\n",
      "epoch:22 step:20643 [D loss: 0.233000, acc.: 60.94%] [G loss: 0.407075]\n",
      "epoch:22 step:20644 [D loss: 0.217861, acc.: 64.84%] [G loss: 0.456851]\n",
      "epoch:22 step:20645 [D loss: 0.251573, acc.: 49.22%] [G loss: 0.432643]\n",
      "epoch:22 step:20646 [D loss: 0.222765, acc.: 64.06%] [G loss: 0.431823]\n",
      "epoch:22 step:20647 [D loss: 0.247693, acc.: 59.38%] [G loss: 0.407424]\n",
      "epoch:22 step:20648 [D loss: 0.246742, acc.: 57.03%] [G loss: 0.409716]\n",
      "epoch:22 step:20649 [D loss: 0.235412, acc.: 56.25%] [G loss: 0.434073]\n",
      "epoch:22 step:20650 [D loss: 0.217691, acc.: 64.84%] [G loss: 0.441394]\n",
      "epoch:22 step:20651 [D loss: 0.245275, acc.: 57.03%] [G loss: 0.407028]\n",
      "epoch:22 step:20652 [D loss: 0.256624, acc.: 54.69%] [G loss: 0.424801]\n",
      "epoch:22 step:20653 [D loss: 0.220834, acc.: 62.50%] [G loss: 0.429643]\n",
      "epoch:22 step:20654 [D loss: 0.189873, acc.: 73.44%] [G loss: 0.432724]\n",
      "epoch:22 step:20655 [D loss: 0.230506, acc.: 60.16%] [G loss: 0.432362]\n",
      "epoch:22 step:20656 [D loss: 0.224389, acc.: 60.94%] [G loss: 0.427572]\n",
      "epoch:22 step:20657 [D loss: 0.222410, acc.: 66.41%] [G loss: 0.418646]\n",
      "epoch:22 step:20658 [D loss: 0.243877, acc.: 57.03%] [G loss: 0.406825]\n",
      "epoch:22 step:20659 [D loss: 0.211473, acc.: 64.84%] [G loss: 0.415423]\n",
      "epoch:22 step:20660 [D loss: 0.236130, acc.: 60.94%] [G loss: 0.430123]\n",
      "epoch:22 step:20661 [D loss: 0.232682, acc.: 64.06%] [G loss: 0.396439]\n",
      "epoch:22 step:20662 [D loss: 0.219927, acc.: 67.97%] [G loss: 0.426848]\n",
      "epoch:22 step:20663 [D loss: 0.213793, acc.: 63.28%] [G loss: 0.451253]\n",
      "epoch:22 step:20664 [D loss: 0.207369, acc.: 69.53%] [G loss: 0.431011]\n",
      "epoch:22 step:20665 [D loss: 0.234762, acc.: 57.03%] [G loss: 0.459046]\n",
      "epoch:22 step:20666 [D loss: 0.238318, acc.: 57.81%] [G loss: 0.402182]\n",
      "epoch:22 step:20667 [D loss: 0.210203, acc.: 65.62%] [G loss: 0.422380]\n",
      "epoch:22 step:20668 [D loss: 0.235340, acc.: 62.50%] [G loss: 0.459331]\n",
      "epoch:22 step:20669 [D loss: 0.229319, acc.: 60.16%] [G loss: 0.459041]\n",
      "epoch:22 step:20670 [D loss: 0.231215, acc.: 63.28%] [G loss: 0.454178]\n",
      "epoch:22 step:20671 [D loss: 0.217822, acc.: 64.06%] [G loss: 0.417739]\n",
      "epoch:22 step:20672 [D loss: 0.218584, acc.: 64.06%] [G loss: 0.417280]\n",
      "epoch:22 step:20673 [D loss: 0.229437, acc.: 62.50%] [G loss: 0.393964]\n",
      "epoch:22 step:20674 [D loss: 0.234051, acc.: 58.59%] [G loss: 0.400482]\n",
      "epoch:22 step:20675 [D loss: 0.248227, acc.: 54.69%] [G loss: 0.408773]\n",
      "epoch:22 step:20676 [D loss: 0.230833, acc.: 60.16%] [G loss: 0.418166]\n",
      "epoch:22 step:20677 [D loss: 0.216911, acc.: 60.94%] [G loss: 0.397137]\n",
      "epoch:22 step:20678 [D loss: 0.223997, acc.: 63.28%] [G loss: 0.428416]\n",
      "epoch:22 step:20679 [D loss: 0.224078, acc.: 63.28%] [G loss: 0.428163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20680 [D loss: 0.240490, acc.: 58.59%] [G loss: 0.388974]\n",
      "epoch:22 step:20681 [D loss: 0.209836, acc.: 67.97%] [G loss: 0.435531]\n",
      "epoch:22 step:20682 [D loss: 0.236634, acc.: 58.59%] [G loss: 0.420101]\n",
      "epoch:22 step:20683 [D loss: 0.230785, acc.: 59.38%] [G loss: 0.432708]\n",
      "epoch:22 step:20684 [D loss: 0.200488, acc.: 69.53%] [G loss: 0.474412]\n",
      "epoch:22 step:20685 [D loss: 0.254678, acc.: 55.47%] [G loss: 0.402123]\n",
      "epoch:22 step:20686 [D loss: 0.230249, acc.: 57.03%] [G loss: 0.427975]\n",
      "epoch:22 step:20687 [D loss: 0.202215, acc.: 67.19%] [G loss: 0.413560]\n",
      "epoch:22 step:20688 [D loss: 0.203854, acc.: 69.53%] [G loss: 0.403967]\n",
      "epoch:22 step:20689 [D loss: 0.206610, acc.: 67.19%] [G loss: 0.455975]\n",
      "epoch:22 step:20690 [D loss: 0.218959, acc.: 63.28%] [G loss: 0.452082]\n",
      "epoch:22 step:20691 [D loss: 0.216880, acc.: 65.62%] [G loss: 0.458966]\n",
      "epoch:22 step:20692 [D loss: 0.302107, acc.: 45.31%] [G loss: 0.417426]\n",
      "epoch:22 step:20693 [D loss: 0.265041, acc.: 50.78%] [G loss: 0.377772]\n",
      "epoch:22 step:20694 [D loss: 0.214390, acc.: 67.19%] [G loss: 0.419360]\n",
      "epoch:22 step:20695 [D loss: 0.225743, acc.: 65.62%] [G loss: 0.414694]\n",
      "epoch:22 step:20696 [D loss: 0.215926, acc.: 63.28%] [G loss: 0.414187]\n",
      "epoch:22 step:20697 [D loss: 0.218421, acc.: 65.62%] [G loss: 0.421461]\n",
      "epoch:22 step:20698 [D loss: 0.222103, acc.: 61.72%] [G loss: 0.405233]\n",
      "epoch:22 step:20699 [D loss: 0.228072, acc.: 63.28%] [G loss: 0.419487]\n",
      "epoch:22 step:20700 [D loss: 0.223472, acc.: 65.62%] [G loss: 0.436637]\n",
      "epoch:22 step:20701 [D loss: 0.234858, acc.: 61.72%] [G loss: 0.412933]\n",
      "epoch:22 step:20702 [D loss: 0.216956, acc.: 66.41%] [G loss: 0.423064]\n",
      "epoch:22 step:20703 [D loss: 0.212925, acc.: 67.97%] [G loss: 0.445521]\n",
      "epoch:22 step:20704 [D loss: 0.214172, acc.: 70.31%] [G loss: 0.444890]\n",
      "epoch:22 step:20705 [D loss: 0.227839, acc.: 65.62%] [G loss: 0.448847]\n",
      "epoch:22 step:20706 [D loss: 0.193820, acc.: 72.66%] [G loss: 0.444953]\n",
      "epoch:22 step:20707 [D loss: 0.188150, acc.: 70.31%] [G loss: 0.435286]\n",
      "epoch:22 step:20708 [D loss: 0.222186, acc.: 62.50%] [G loss: 0.484741]\n",
      "epoch:22 step:20709 [D loss: 0.208630, acc.: 64.06%] [G loss: 0.471570]\n",
      "epoch:22 step:20710 [D loss: 0.217870, acc.: 67.97%] [G loss: 0.423795]\n",
      "epoch:22 step:20711 [D loss: 0.214035, acc.: 64.84%] [G loss: 0.464168]\n",
      "epoch:22 step:20712 [D loss: 0.206541, acc.: 69.53%] [G loss: 0.516143]\n",
      "epoch:22 step:20713 [D loss: 0.227447, acc.: 60.94%] [G loss: 0.444955]\n",
      "epoch:22 step:20714 [D loss: 0.199952, acc.: 71.09%] [G loss: 0.439646]\n",
      "epoch:22 step:20715 [D loss: 0.219721, acc.: 63.28%] [G loss: 0.433382]\n",
      "epoch:22 step:20716 [D loss: 0.268027, acc.: 46.09%] [G loss: 0.361162]\n",
      "epoch:22 step:20717 [D loss: 0.222501, acc.: 65.62%] [G loss: 0.424118]\n",
      "epoch:22 step:20718 [D loss: 0.234120, acc.: 58.59%] [G loss: 0.442740]\n",
      "epoch:22 step:20719 [D loss: 0.241880, acc.: 64.06%] [G loss: 0.423290]\n",
      "epoch:22 step:20720 [D loss: 0.227801, acc.: 58.59%] [G loss: 0.389805]\n",
      "epoch:22 step:20721 [D loss: 0.194039, acc.: 71.09%] [G loss: 0.461440]\n",
      "epoch:22 step:20722 [D loss: 0.282638, acc.: 47.66%] [G loss: 0.477432]\n",
      "epoch:22 step:20723 [D loss: 0.227614, acc.: 70.31%] [G loss: 0.474288]\n",
      "epoch:22 step:20724 [D loss: 0.243454, acc.: 60.16%] [G loss: 0.449496]\n",
      "epoch:22 step:20725 [D loss: 0.244031, acc.: 57.03%] [G loss: 0.417188]\n",
      "epoch:22 step:20726 [D loss: 0.193540, acc.: 71.09%] [G loss: 0.442244]\n",
      "epoch:22 step:20727 [D loss: 0.202957, acc.: 67.19%] [G loss: 0.451719]\n",
      "epoch:22 step:20728 [D loss: 0.219937, acc.: 62.50%] [G loss: 0.455126]\n",
      "epoch:22 step:20729 [D loss: 0.206535, acc.: 64.84%] [G loss: 0.471043]\n",
      "epoch:22 step:20730 [D loss: 0.215218, acc.: 67.97%] [G loss: 0.467049]\n",
      "epoch:22 step:20731 [D loss: 0.219584, acc.: 64.84%] [G loss: 0.472171]\n",
      "epoch:22 step:20732 [D loss: 0.209836, acc.: 65.62%] [G loss: 0.455682]\n",
      "epoch:22 step:20733 [D loss: 0.168656, acc.: 78.12%] [G loss: 0.486451]\n",
      "epoch:22 step:20734 [D loss: 0.246679, acc.: 57.81%] [G loss: 0.457817]\n",
      "epoch:22 step:20735 [D loss: 0.228321, acc.: 64.06%] [G loss: 0.414404]\n",
      "epoch:22 step:20736 [D loss: 0.204463, acc.: 67.97%] [G loss: 0.430200]\n",
      "epoch:22 step:20737 [D loss: 0.193409, acc.: 71.09%] [G loss: 0.479432]\n",
      "epoch:22 step:20738 [D loss: 0.229082, acc.: 63.28%] [G loss: 0.460781]\n",
      "epoch:22 step:20739 [D loss: 0.226279, acc.: 64.84%] [G loss: 0.410949]\n",
      "epoch:22 step:20740 [D loss: 0.193056, acc.: 70.31%] [G loss: 0.440370]\n",
      "epoch:22 step:20741 [D loss: 0.228007, acc.: 64.84%] [G loss: 0.405965]\n",
      "epoch:22 step:20742 [D loss: 0.240661, acc.: 53.91%] [G loss: 0.401136]\n",
      "epoch:22 step:20743 [D loss: 0.225275, acc.: 64.84%] [G loss: 0.430632]\n",
      "epoch:22 step:20744 [D loss: 0.202134, acc.: 67.97%] [G loss: 0.425151]\n",
      "epoch:22 step:20745 [D loss: 0.214214, acc.: 69.53%] [G loss: 0.412219]\n",
      "epoch:22 step:20746 [D loss: 0.225050, acc.: 60.94%] [G loss: 0.402333]\n",
      "epoch:22 step:20747 [D loss: 0.262965, acc.: 54.69%] [G loss: 0.430876]\n",
      "epoch:22 step:20748 [D loss: 0.219873, acc.: 64.06%] [G loss: 0.422544]\n",
      "epoch:22 step:20749 [D loss: 0.223184, acc.: 62.50%] [G loss: 0.472379]\n",
      "epoch:22 step:20750 [D loss: 0.197892, acc.: 69.53%] [G loss: 0.477351]\n",
      "epoch:22 step:20751 [D loss: 0.247591, acc.: 54.69%] [G loss: 0.426045]\n",
      "epoch:22 step:20752 [D loss: 0.253306, acc.: 61.72%] [G loss: 0.415502]\n",
      "epoch:22 step:20753 [D loss: 0.241390, acc.: 56.25%] [G loss: 0.439196]\n",
      "epoch:22 step:20754 [D loss: 0.236117, acc.: 59.38%] [G loss: 0.426345]\n",
      "epoch:22 step:20755 [D loss: 0.244867, acc.: 61.72%] [G loss: 0.438683]\n",
      "epoch:22 step:20756 [D loss: 0.233839, acc.: 57.81%] [G loss: 0.412256]\n",
      "epoch:22 step:20757 [D loss: 0.232938, acc.: 60.94%] [G loss: 0.426285]\n",
      "epoch:22 step:20758 [D loss: 0.219292, acc.: 63.28%] [G loss: 0.370987]\n",
      "epoch:22 step:20759 [D loss: 0.239862, acc.: 58.59%] [G loss: 0.400063]\n",
      "epoch:22 step:20760 [D loss: 0.229290, acc.: 58.59%] [G loss: 0.430143]\n",
      "epoch:22 step:20761 [D loss: 0.240933, acc.: 59.38%] [G loss: 0.401618]\n",
      "epoch:22 step:20762 [D loss: 0.234999, acc.: 61.72%] [G loss: 0.441441]\n",
      "epoch:22 step:20763 [D loss: 0.216270, acc.: 66.41%] [G loss: 0.416330]\n",
      "epoch:22 step:20764 [D loss: 0.233357, acc.: 55.47%] [G loss: 0.415008]\n",
      "epoch:22 step:20765 [D loss: 0.241700, acc.: 57.81%] [G loss: 0.369229]\n",
      "epoch:22 step:20766 [D loss: 0.215035, acc.: 70.31%] [G loss: 0.428842]\n",
      "epoch:22 step:20767 [D loss: 0.231071, acc.: 61.72%] [G loss: 0.431480]\n",
      "epoch:22 step:20768 [D loss: 0.217219, acc.: 63.28%] [G loss: 0.420875]\n",
      "epoch:22 step:20769 [D loss: 0.213876, acc.: 63.28%] [G loss: 0.469753]\n",
      "epoch:22 step:20770 [D loss: 0.215866, acc.: 64.06%] [G loss: 0.446073]\n",
      "epoch:22 step:20771 [D loss: 0.239955, acc.: 53.91%] [G loss: 0.451941]\n",
      "epoch:22 step:20772 [D loss: 0.205786, acc.: 72.66%] [G loss: 0.423771]\n",
      "epoch:22 step:20773 [D loss: 0.203529, acc.: 70.31%] [G loss: 0.448463]\n",
      "epoch:22 step:20774 [D loss: 0.257777, acc.: 58.59%] [G loss: 0.407808]\n",
      "epoch:22 step:20775 [D loss: 0.227733, acc.: 61.72%] [G loss: 0.461207]\n",
      "epoch:22 step:20776 [D loss: 0.244292, acc.: 60.94%] [G loss: 0.395947]\n",
      "epoch:22 step:20777 [D loss: 0.206825, acc.: 64.84%] [G loss: 0.432900]\n",
      "epoch:22 step:20778 [D loss: 0.225833, acc.: 60.16%] [G loss: 0.440038]\n",
      "epoch:22 step:20779 [D loss: 0.216818, acc.: 68.75%] [G loss: 0.425689]\n",
      "epoch:22 step:20780 [D loss: 0.238598, acc.: 60.16%] [G loss: 0.359678]\n",
      "epoch:22 step:20781 [D loss: 0.212226, acc.: 64.84%] [G loss: 0.449402]\n",
      "epoch:22 step:20782 [D loss: 0.206345, acc.: 70.31%] [G loss: 0.416165]\n",
      "epoch:22 step:20783 [D loss: 0.230138, acc.: 59.38%] [G loss: 0.437880]\n",
      "epoch:22 step:20784 [D loss: 0.245451, acc.: 51.56%] [G loss: 0.383324]\n",
      "epoch:22 step:20785 [D loss: 0.242798, acc.: 54.69%] [G loss: 0.415818]\n",
      "epoch:22 step:20786 [D loss: 0.231316, acc.: 59.38%] [G loss: 0.414393]\n",
      "epoch:22 step:20787 [D loss: 0.218816, acc.: 68.75%] [G loss: 0.435110]\n",
      "epoch:22 step:20788 [D loss: 0.256970, acc.: 53.91%] [G loss: 0.405858]\n",
      "epoch:22 step:20789 [D loss: 0.229237, acc.: 58.59%] [G loss: 0.395476]\n",
      "epoch:22 step:20790 [D loss: 0.223303, acc.: 61.72%] [G loss: 0.436236]\n",
      "epoch:22 step:20791 [D loss: 0.219833, acc.: 64.06%] [G loss: 0.417849]\n",
      "epoch:22 step:20792 [D loss: 0.228234, acc.: 58.59%] [G loss: 0.408859]\n",
      "epoch:22 step:20793 [D loss: 0.248964, acc.: 54.69%] [G loss: 0.400244]\n",
      "epoch:22 step:20794 [D loss: 0.230891, acc.: 59.38%] [G loss: 0.439490]\n",
      "epoch:22 step:20795 [D loss: 0.226585, acc.: 60.94%] [G loss: 0.418382]\n",
      "epoch:22 step:20796 [D loss: 0.235426, acc.: 59.38%] [G loss: 0.469449]\n",
      "epoch:22 step:20797 [D loss: 0.249490, acc.: 54.69%] [G loss: 0.386456]\n",
      "epoch:22 step:20798 [D loss: 0.212405, acc.: 64.84%] [G loss: 0.427920]\n",
      "epoch:22 step:20799 [D loss: 0.231654, acc.: 64.06%] [G loss: 0.375520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20800 [D loss: 0.224478, acc.: 60.94%] [G loss: 0.394058]\n",
      "##############\n",
      "[2.46218739 1.66069439 6.08223387 4.73090469 3.62291682 5.53031435\n",
      " 4.68917263 4.90336557 4.35947074 4.24583654]\n",
      "##########\n",
      "epoch:22 step:20801 [D loss: 0.231821, acc.: 60.94%] [G loss: 0.407476]\n",
      "epoch:22 step:20802 [D loss: 0.246816, acc.: 55.47%] [G loss: 0.402240]\n",
      "epoch:22 step:20803 [D loss: 0.238469, acc.: 64.06%] [G loss: 0.420992]\n",
      "epoch:22 step:20804 [D loss: 0.219056, acc.: 63.28%] [G loss: 0.403232]\n",
      "epoch:22 step:20805 [D loss: 0.242053, acc.: 59.38%] [G loss: 0.430161]\n",
      "epoch:22 step:20806 [D loss: 0.232456, acc.: 61.72%] [G loss: 0.427083]\n",
      "epoch:22 step:20807 [D loss: 0.223114, acc.: 57.81%] [G loss: 0.430164]\n",
      "epoch:22 step:20808 [D loss: 0.213162, acc.: 64.84%] [G loss: 0.469254]\n",
      "epoch:22 step:20809 [D loss: 0.211231, acc.: 62.50%] [G loss: 0.436407]\n",
      "epoch:22 step:20810 [D loss: 0.227310, acc.: 62.50%] [G loss: 0.460629]\n",
      "epoch:22 step:20811 [D loss: 0.210132, acc.: 65.62%] [G loss: 0.420603]\n",
      "epoch:22 step:20812 [D loss: 0.191457, acc.: 72.66%] [G loss: 0.449027]\n",
      "epoch:22 step:20813 [D loss: 0.209459, acc.: 68.75%] [G loss: 0.455424]\n",
      "epoch:22 step:20814 [D loss: 0.243567, acc.: 56.25%] [G loss: 0.430376]\n",
      "epoch:22 step:20815 [D loss: 0.233936, acc.: 60.94%] [G loss: 0.402567]\n",
      "epoch:22 step:20816 [D loss: 0.216076, acc.: 64.06%] [G loss: 0.420982]\n",
      "epoch:22 step:20817 [D loss: 0.293414, acc.: 47.66%] [G loss: 0.447085]\n",
      "epoch:22 step:20818 [D loss: 0.198919, acc.: 66.41%] [G loss: 0.448273]\n",
      "epoch:22 step:20819 [D loss: 0.239484, acc.: 58.59%] [G loss: 0.496933]\n",
      "epoch:22 step:20820 [D loss: 0.226621, acc.: 65.62%] [G loss: 0.469521]\n",
      "epoch:22 step:20821 [D loss: 0.236307, acc.: 58.59%] [G loss: 0.407727]\n",
      "epoch:22 step:20822 [D loss: 0.176864, acc.: 73.44%] [G loss: 0.458907]\n",
      "epoch:22 step:20823 [D loss: 0.194925, acc.: 70.31%] [G loss: 0.458263]\n",
      "epoch:22 step:20824 [D loss: 0.261559, acc.: 51.56%] [G loss: 0.433204]\n",
      "epoch:22 step:20825 [D loss: 0.252138, acc.: 53.12%] [G loss: 0.404646]\n",
      "epoch:22 step:20826 [D loss: 0.257313, acc.: 55.47%] [G loss: 0.402680]\n",
      "epoch:22 step:20827 [D loss: 0.238964, acc.: 60.16%] [G loss: 0.414449]\n",
      "epoch:22 step:20828 [D loss: 0.245644, acc.: 56.25%] [G loss: 0.391520]\n",
      "epoch:22 step:20829 [D loss: 0.242155, acc.: 58.59%] [G loss: 0.394669]\n",
      "epoch:22 step:20830 [D loss: 0.231485, acc.: 65.62%] [G loss: 0.433878]\n",
      "epoch:22 step:20831 [D loss: 0.208130, acc.: 67.19%] [G loss: 0.452316]\n",
      "epoch:22 step:20832 [D loss: 0.195776, acc.: 71.88%] [G loss: 0.430715]\n",
      "epoch:22 step:20833 [D loss: 0.208161, acc.: 68.75%] [G loss: 0.432002]\n",
      "epoch:22 step:20834 [D loss: 0.256752, acc.: 60.16%] [G loss: 0.444688]\n",
      "epoch:22 step:20835 [D loss: 0.213516, acc.: 64.84%] [G loss: 0.468401]\n",
      "epoch:22 step:20836 [D loss: 0.220915, acc.: 61.72%] [G loss: 0.432330]\n",
      "epoch:22 step:20837 [D loss: 0.212314, acc.: 68.75%] [G loss: 0.520680]\n",
      "epoch:22 step:20838 [D loss: 0.243351, acc.: 60.16%] [G loss: 0.423332]\n",
      "epoch:22 step:20839 [D loss: 0.242472, acc.: 62.50%] [G loss: 0.397205]\n",
      "epoch:22 step:20840 [D loss: 0.241990, acc.: 56.25%] [G loss: 0.384065]\n",
      "epoch:22 step:20841 [D loss: 0.218075, acc.: 66.41%] [G loss: 0.395159]\n",
      "epoch:22 step:20842 [D loss: 0.242821, acc.: 57.03%] [G loss: 0.406529]\n",
      "epoch:22 step:20843 [D loss: 0.208301, acc.: 66.41%] [G loss: 0.418316]\n",
      "epoch:22 step:20844 [D loss: 0.200716, acc.: 67.19%] [G loss: 0.479034]\n",
      "epoch:22 step:20845 [D loss: 0.210078, acc.: 69.53%] [G loss: 0.469839]\n",
      "epoch:22 step:20846 [D loss: 0.166974, acc.: 78.91%] [G loss: 0.512330]\n",
      "epoch:22 step:20847 [D loss: 0.260497, acc.: 57.81%] [G loss: 0.412792]\n",
      "epoch:22 step:20848 [D loss: 0.243171, acc.: 59.38%] [G loss: 0.427949]\n",
      "epoch:22 step:20849 [D loss: 0.222635, acc.: 66.41%] [G loss: 0.433729]\n",
      "epoch:22 step:20850 [D loss: 0.232296, acc.: 62.50%] [G loss: 0.448527]\n",
      "epoch:22 step:20851 [D loss: 0.228720, acc.: 60.16%] [G loss: 0.435871]\n",
      "epoch:22 step:20852 [D loss: 0.195483, acc.: 71.09%] [G loss: 0.445141]\n",
      "epoch:22 step:20853 [D loss: 0.203915, acc.: 68.75%] [G loss: 0.431844]\n",
      "epoch:22 step:20854 [D loss: 0.221666, acc.: 66.41%] [G loss: 0.412234]\n",
      "epoch:22 step:20855 [D loss: 0.193081, acc.: 72.66%] [G loss: 0.460374]\n",
      "epoch:22 step:20856 [D loss: 0.171482, acc.: 78.12%] [G loss: 0.507168]\n",
      "epoch:22 step:20857 [D loss: 0.235853, acc.: 63.28%] [G loss: 0.456816]\n",
      "epoch:22 step:20858 [D loss: 0.235528, acc.: 59.38%] [G loss: 0.421706]\n",
      "epoch:22 step:20859 [D loss: 0.225781, acc.: 62.50%] [G loss: 0.445376]\n",
      "epoch:22 step:20860 [D loss: 0.220885, acc.: 63.28%] [G loss: 0.412873]\n",
      "epoch:22 step:20861 [D loss: 0.207036, acc.: 68.75%] [G loss: 0.466148]\n",
      "epoch:22 step:20862 [D loss: 0.206418, acc.: 64.84%] [G loss: 0.429066]\n",
      "epoch:22 step:20863 [D loss: 0.269213, acc.: 50.78%] [G loss: 0.427409]\n",
      "epoch:22 step:20864 [D loss: 0.288073, acc.: 49.22%] [G loss: 0.400475]\n",
      "epoch:22 step:20865 [D loss: 0.244273, acc.: 59.38%] [G loss: 0.471691]\n",
      "epoch:22 step:20866 [D loss: 0.238213, acc.: 60.16%] [G loss: 0.406396]\n",
      "epoch:22 step:20867 [D loss: 0.224838, acc.: 60.94%] [G loss: 0.408690]\n",
      "epoch:22 step:20868 [D loss: 0.243975, acc.: 60.94%] [G loss: 0.418817]\n",
      "epoch:22 step:20869 [D loss: 0.222900, acc.: 62.50%] [G loss: 0.440229]\n",
      "epoch:22 step:20870 [D loss: 0.221319, acc.: 62.50%] [G loss: 0.387025]\n",
      "epoch:22 step:20871 [D loss: 0.235970, acc.: 57.81%] [G loss: 0.412868]\n",
      "epoch:22 step:20872 [D loss: 0.216079, acc.: 67.19%] [G loss: 0.420139]\n",
      "epoch:22 step:20873 [D loss: 0.211198, acc.: 67.19%] [G loss: 0.392640]\n",
      "epoch:22 step:20874 [D loss: 0.229648, acc.: 57.81%] [G loss: 0.411800]\n",
      "epoch:22 step:20875 [D loss: 0.255853, acc.: 57.81%] [G loss: 0.403288]\n",
      "epoch:22 step:20876 [D loss: 0.215369, acc.: 60.16%] [G loss: 0.427720]\n",
      "epoch:22 step:20877 [D loss: 0.239146, acc.: 59.38%] [G loss: 0.448165]\n",
      "epoch:22 step:20878 [D loss: 0.231681, acc.: 63.28%] [G loss: 0.470875]\n",
      "epoch:22 step:20879 [D loss: 0.223605, acc.: 57.81%] [G loss: 0.401289]\n",
      "epoch:22 step:20880 [D loss: 0.239821, acc.: 58.59%] [G loss: 0.400496]\n",
      "epoch:22 step:20881 [D loss: 0.218900, acc.: 63.28%] [G loss: 0.407927]\n",
      "epoch:22 step:20882 [D loss: 0.232004, acc.: 57.03%] [G loss: 0.424232]\n",
      "epoch:22 step:20883 [D loss: 0.225653, acc.: 60.94%] [G loss: 0.418469]\n",
      "epoch:22 step:20884 [D loss: 0.223119, acc.: 63.28%] [G loss: 0.455008]\n",
      "epoch:22 step:20885 [D loss: 0.204615, acc.: 71.88%] [G loss: 0.457040]\n",
      "epoch:22 step:20886 [D loss: 0.236196, acc.: 57.03%] [G loss: 0.423691]\n",
      "epoch:22 step:20887 [D loss: 0.220924, acc.: 65.62%] [G loss: 0.427984]\n",
      "epoch:22 step:20888 [D loss: 0.190263, acc.: 73.44%] [G loss: 0.454165]\n",
      "epoch:22 step:20889 [D loss: 0.218900, acc.: 64.84%] [G loss: 0.420742]\n",
      "epoch:22 step:20890 [D loss: 0.217442, acc.: 66.41%] [G loss: 0.457622]\n",
      "epoch:22 step:20891 [D loss: 0.238966, acc.: 64.06%] [G loss: 0.420945]\n",
      "epoch:22 step:20892 [D loss: 0.257806, acc.: 53.91%] [G loss: 0.409583]\n",
      "epoch:22 step:20893 [D loss: 0.220004, acc.: 60.16%] [G loss: 0.442851]\n",
      "epoch:22 step:20894 [D loss: 0.199517, acc.: 69.53%] [G loss: 0.471179]\n",
      "epoch:22 step:20895 [D loss: 0.233713, acc.: 60.94%] [G loss: 0.427061]\n",
      "epoch:22 step:20896 [D loss: 0.248753, acc.: 53.12%] [G loss: 0.421780]\n",
      "epoch:22 step:20897 [D loss: 0.208615, acc.: 68.75%] [G loss: 0.430965]\n",
      "epoch:22 step:20898 [D loss: 0.221736, acc.: 58.59%] [G loss: 0.433351]\n",
      "epoch:22 step:20899 [D loss: 0.248183, acc.: 56.25%] [G loss: 0.427764]\n",
      "epoch:22 step:20900 [D loss: 0.202364, acc.: 68.75%] [G loss: 0.474530]\n",
      "epoch:22 step:20901 [D loss: 0.238958, acc.: 58.59%] [G loss: 0.437024]\n",
      "epoch:22 step:20902 [D loss: 0.203897, acc.: 67.97%] [G loss: 0.432340]\n",
      "epoch:22 step:20903 [D loss: 0.218265, acc.: 63.28%] [G loss: 0.424211]\n",
      "epoch:22 step:20904 [D loss: 0.222735, acc.: 67.97%] [G loss: 0.456887]\n",
      "epoch:22 step:20905 [D loss: 0.245473, acc.: 60.94%] [G loss: 0.450988]\n",
      "epoch:22 step:20906 [D loss: 0.233719, acc.: 60.94%] [G loss: 0.434980]\n",
      "epoch:22 step:20907 [D loss: 0.233663, acc.: 61.72%] [G loss: 0.449426]\n",
      "epoch:22 step:20908 [D loss: 0.248248, acc.: 53.12%] [G loss: 0.441013]\n",
      "epoch:22 step:20909 [D loss: 0.217828, acc.: 63.28%] [G loss: 0.441317]\n",
      "epoch:22 step:20910 [D loss: 0.211462, acc.: 63.28%] [G loss: 0.442994]\n",
      "epoch:22 step:20911 [D loss: 0.227562, acc.: 64.84%] [G loss: 0.421440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20912 [D loss: 0.187509, acc.: 72.66%] [G loss: 0.461915]\n",
      "epoch:22 step:20913 [D loss: 0.226707, acc.: 64.06%] [G loss: 0.402057]\n",
      "epoch:22 step:20914 [D loss: 0.218554, acc.: 65.62%] [G loss: 0.412481]\n",
      "epoch:22 step:20915 [D loss: 0.239759, acc.: 61.72%] [G loss: 0.387674]\n",
      "epoch:22 step:20916 [D loss: 0.208797, acc.: 64.84%] [G loss: 0.428892]\n",
      "epoch:22 step:20917 [D loss: 0.246083, acc.: 59.38%] [G loss: 0.448552]\n",
      "epoch:22 step:20918 [D loss: 0.232383, acc.: 64.06%] [G loss: 0.441701]\n",
      "epoch:22 step:20919 [D loss: 0.215732, acc.: 65.62%] [G loss: 0.402614]\n",
      "epoch:22 step:20920 [D loss: 0.223449, acc.: 61.72%] [G loss: 0.404521]\n",
      "epoch:22 step:20921 [D loss: 0.229267, acc.: 66.41%] [G loss: 0.429100]\n",
      "epoch:22 step:20922 [D loss: 0.217183, acc.: 68.75%] [G loss: 0.443903]\n",
      "epoch:22 step:20923 [D loss: 0.222577, acc.: 64.06%] [G loss: 0.418702]\n",
      "epoch:22 step:20924 [D loss: 0.226296, acc.: 60.94%] [G loss: 0.413403]\n",
      "epoch:22 step:20925 [D loss: 0.223287, acc.: 64.84%] [G loss: 0.438379]\n",
      "epoch:22 step:20926 [D loss: 0.164200, acc.: 82.03%] [G loss: 0.464150]\n",
      "epoch:22 step:20927 [D loss: 0.185743, acc.: 71.88%] [G loss: 0.517934]\n",
      "epoch:22 step:20928 [D loss: 0.178927, acc.: 73.44%] [G loss: 0.490141]\n",
      "epoch:22 step:20929 [D loss: 0.187906, acc.: 71.09%] [G loss: 0.505621]\n",
      "epoch:22 step:20930 [D loss: 0.271137, acc.: 56.25%] [G loss: 0.444695]\n",
      "epoch:22 step:20931 [D loss: 0.247227, acc.: 57.81%] [G loss: 0.429537]\n",
      "epoch:22 step:20932 [D loss: 0.218988, acc.: 65.62%] [G loss: 0.386088]\n",
      "epoch:22 step:20933 [D loss: 0.221189, acc.: 60.94%] [G loss: 0.399864]\n",
      "epoch:22 step:20934 [D loss: 0.240703, acc.: 57.03%] [G loss: 0.460896]\n",
      "epoch:22 step:20935 [D loss: 0.194512, acc.: 69.53%] [G loss: 0.401853]\n",
      "epoch:22 step:20936 [D loss: 0.217603, acc.: 64.84%] [G loss: 0.393081]\n",
      "epoch:22 step:20937 [D loss: 0.247090, acc.: 59.38%] [G loss: 0.427012]\n",
      "epoch:22 step:20938 [D loss: 0.222922, acc.: 63.28%] [G loss: 0.407478]\n",
      "epoch:22 step:20939 [D loss: 0.259893, acc.: 54.69%] [G loss: 0.421957]\n",
      "epoch:22 step:20940 [D loss: 0.209460, acc.: 67.19%] [G loss: 0.457416]\n",
      "epoch:22 step:20941 [D loss: 0.220420, acc.: 65.62%] [G loss: 0.421963]\n",
      "epoch:22 step:20942 [D loss: 0.200722, acc.: 64.84%] [G loss: 0.471403]\n",
      "epoch:22 step:20943 [D loss: 0.218955, acc.: 64.84%] [G loss: 0.445013]\n",
      "epoch:22 step:20944 [D loss: 0.232882, acc.: 60.16%] [G loss: 0.407502]\n",
      "epoch:22 step:20945 [D loss: 0.212485, acc.: 63.28%] [G loss: 0.425995]\n",
      "epoch:22 step:20946 [D loss: 0.208150, acc.: 65.62%] [G loss: 0.435254]\n",
      "epoch:22 step:20947 [D loss: 0.223242, acc.: 61.72%] [G loss: 0.416972]\n",
      "epoch:22 step:20948 [D loss: 0.220436, acc.: 61.72%] [G loss: 0.417914]\n",
      "epoch:22 step:20949 [D loss: 0.215514, acc.: 60.16%] [G loss: 0.467624]\n",
      "epoch:22 step:20950 [D loss: 0.201398, acc.: 67.19%] [G loss: 0.458535]\n",
      "epoch:22 step:20951 [D loss: 0.212119, acc.: 64.84%] [G loss: 0.480427]\n",
      "epoch:22 step:20952 [D loss: 0.229035, acc.: 65.62%] [G loss: 0.440667]\n",
      "epoch:22 step:20953 [D loss: 0.215291, acc.: 67.97%] [G loss: 0.434926]\n",
      "epoch:22 step:20954 [D loss: 0.228731, acc.: 61.72%] [G loss: 0.433675]\n",
      "epoch:22 step:20955 [D loss: 0.274030, acc.: 57.03%] [G loss: 0.459637]\n",
      "epoch:22 step:20956 [D loss: 0.208381, acc.: 65.62%] [G loss: 0.466326]\n",
      "epoch:22 step:20957 [D loss: 0.221555, acc.: 62.50%] [G loss: 0.434506]\n",
      "epoch:22 step:20958 [D loss: 0.203736, acc.: 67.97%] [G loss: 0.454318]\n",
      "epoch:22 step:20959 [D loss: 0.210222, acc.: 63.28%] [G loss: 0.426168]\n",
      "epoch:22 step:20960 [D loss: 0.176523, acc.: 74.22%] [G loss: 0.522965]\n",
      "epoch:22 step:20961 [D loss: 0.193434, acc.: 70.31%] [G loss: 0.500224]\n",
      "epoch:22 step:20962 [D loss: 0.268564, acc.: 53.91%] [G loss: 0.475061]\n",
      "epoch:22 step:20963 [D loss: 0.268572, acc.: 54.69%] [G loss: 0.449061]\n",
      "epoch:22 step:20964 [D loss: 0.221542, acc.: 64.06%] [G loss: 0.415059]\n",
      "epoch:22 step:20965 [D loss: 0.237078, acc.: 60.94%] [G loss: 0.413479]\n",
      "epoch:22 step:20966 [D loss: 0.245649, acc.: 57.81%] [G loss: 0.408382]\n",
      "epoch:22 step:20967 [D loss: 0.210607, acc.: 68.75%] [G loss: 0.426586]\n",
      "epoch:22 step:20968 [D loss: 0.169704, acc.: 74.22%] [G loss: 0.486678]\n",
      "epoch:22 step:20969 [D loss: 0.232112, acc.: 61.72%] [G loss: 0.462655]\n",
      "epoch:22 step:20970 [D loss: 0.235328, acc.: 55.47%] [G loss: 0.419564]\n",
      "epoch:22 step:20971 [D loss: 0.201855, acc.: 67.97%] [G loss: 0.422881]\n",
      "epoch:22 step:20972 [D loss: 0.210026, acc.: 67.97%] [G loss: 0.438757]\n",
      "epoch:22 step:20973 [D loss: 0.215209, acc.: 65.62%] [G loss: 0.444096]\n",
      "epoch:22 step:20974 [D loss: 0.198985, acc.: 68.75%] [G loss: 0.499420]\n",
      "epoch:22 step:20975 [D loss: 0.187431, acc.: 74.22%] [G loss: 0.485459]\n",
      "epoch:22 step:20976 [D loss: 0.224968, acc.: 62.50%] [G loss: 0.406982]\n",
      "epoch:22 step:20977 [D loss: 0.231332, acc.: 60.16%] [G loss: 0.410933]\n",
      "epoch:22 step:20978 [D loss: 0.220869, acc.: 60.16%] [G loss: 0.414237]\n",
      "epoch:22 step:20979 [D loss: 0.233498, acc.: 55.47%] [G loss: 0.378805]\n",
      "epoch:22 step:20980 [D loss: 0.239761, acc.: 53.12%] [G loss: 0.404012]\n",
      "epoch:22 step:20981 [D loss: 0.226021, acc.: 65.62%] [G loss: 0.407009]\n",
      "epoch:22 step:20982 [D loss: 0.247083, acc.: 51.56%] [G loss: 0.401364]\n",
      "epoch:22 step:20983 [D loss: 0.220958, acc.: 62.50%] [G loss: 0.422884]\n",
      "epoch:22 step:20984 [D loss: 0.192859, acc.: 71.09%] [G loss: 0.472214]\n",
      "epoch:22 step:20985 [D loss: 0.190121, acc.: 68.75%] [G loss: 0.492109]\n",
      "epoch:22 step:20986 [D loss: 0.214887, acc.: 65.62%] [G loss: 0.463775]\n",
      "epoch:22 step:20987 [D loss: 0.256802, acc.: 50.78%] [G loss: 0.404154]\n",
      "epoch:22 step:20988 [D loss: 0.199245, acc.: 68.75%] [G loss: 0.432776]\n",
      "epoch:22 step:20989 [D loss: 0.220258, acc.: 62.50%] [G loss: 0.445696]\n",
      "epoch:22 step:20990 [D loss: 0.240075, acc.: 60.16%] [G loss: 0.440431]\n",
      "epoch:22 step:20991 [D loss: 0.248546, acc.: 55.47%] [G loss: 0.429514]\n",
      "epoch:22 step:20992 [D loss: 0.224606, acc.: 65.62%] [G loss: 0.419477]\n",
      "epoch:22 step:20993 [D loss: 0.215602, acc.: 64.06%] [G loss: 0.430036]\n",
      "epoch:22 step:20994 [D loss: 0.264882, acc.: 54.69%] [G loss: 0.410101]\n",
      "epoch:22 step:20995 [D loss: 0.201961, acc.: 68.75%] [G loss: 0.438146]\n",
      "epoch:22 step:20996 [D loss: 0.214786, acc.: 65.62%] [G loss: 0.433714]\n",
      "epoch:22 step:20997 [D loss: 0.223130, acc.: 64.84%] [G loss: 0.392208]\n",
      "epoch:22 step:20998 [D loss: 0.203650, acc.: 67.19%] [G loss: 0.413543]\n",
      "epoch:22 step:20999 [D loss: 0.195161, acc.: 65.62%] [G loss: 0.454857]\n",
      "epoch:22 step:21000 [D loss: 0.235072, acc.: 60.94%] [G loss: 0.429043]\n",
      "##############\n",
      "[2.46091972 1.93576673 6.27005986 4.64825599 3.62204866 5.78727815\n",
      " 4.63036269 4.69793817 4.37692799 4.18776535]\n",
      "##########\n",
      "epoch:22 step:21001 [D loss: 0.218741, acc.: 66.41%] [G loss: 0.444718]\n",
      "epoch:22 step:21002 [D loss: 0.215105, acc.: 63.28%] [G loss: 0.463225]\n",
      "epoch:22 step:21003 [D loss: 0.247709, acc.: 57.03%] [G loss: 0.456676]\n",
      "epoch:22 step:21004 [D loss: 0.232171, acc.: 67.19%] [G loss: 0.484395]\n",
      "epoch:22 step:21005 [D loss: 0.222300, acc.: 60.94%] [G loss: 0.408143]\n",
      "epoch:22 step:21006 [D loss: 0.238316, acc.: 58.59%] [G loss: 0.393341]\n",
      "epoch:22 step:21007 [D loss: 0.229160, acc.: 60.94%] [G loss: 0.412013]\n",
      "epoch:22 step:21008 [D loss: 0.224549, acc.: 65.62%] [G loss: 0.410294]\n",
      "epoch:22 step:21009 [D loss: 0.223901, acc.: 63.28%] [G loss: 0.429857]\n",
      "epoch:22 step:21010 [D loss: 0.236862, acc.: 60.94%] [G loss: 0.442795]\n",
      "epoch:22 step:21011 [D loss: 0.234267, acc.: 63.28%] [G loss: 0.419214]\n",
      "epoch:22 step:21012 [D loss: 0.207710, acc.: 71.88%] [G loss: 0.446857]\n",
      "epoch:22 step:21013 [D loss: 0.213401, acc.: 63.28%] [G loss: 0.475097]\n",
      "epoch:22 step:21014 [D loss: 0.251830, acc.: 50.78%] [G loss: 0.385924]\n",
      "epoch:22 step:21015 [D loss: 0.233109, acc.: 63.28%] [G loss: 0.402749]\n",
      "epoch:22 step:21016 [D loss: 0.210489, acc.: 66.41%] [G loss: 0.390208]\n",
      "epoch:22 step:21017 [D loss: 0.216123, acc.: 62.50%] [G loss: 0.436042]\n",
      "epoch:22 step:21018 [D loss: 0.215745, acc.: 65.62%] [G loss: 0.434866]\n",
      "epoch:22 step:21019 [D loss: 0.194410, acc.: 70.31%] [G loss: 0.477444]\n",
      "epoch:22 step:21020 [D loss: 0.221322, acc.: 71.88%] [G loss: 0.508268]\n",
      "epoch:22 step:21021 [D loss: 0.227317, acc.: 64.06%] [G loss: 0.461864]\n",
      "epoch:22 step:21022 [D loss: 0.265952, acc.: 56.25%] [G loss: 0.428102]\n",
      "epoch:22 step:21023 [D loss: 0.225913, acc.: 59.38%] [G loss: 0.424822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21024 [D loss: 0.234443, acc.: 60.16%] [G loss: 0.405672]\n",
      "epoch:22 step:21025 [D loss: 0.253274, acc.: 59.38%] [G loss: 0.416709]\n",
      "epoch:22 step:21026 [D loss: 0.221202, acc.: 64.06%] [G loss: 0.406206]\n",
      "epoch:22 step:21027 [D loss: 0.246972, acc.: 59.38%] [G loss: 0.436482]\n",
      "epoch:22 step:21028 [D loss: 0.202312, acc.: 68.75%] [G loss: 0.471487]\n",
      "epoch:22 step:21029 [D loss: 0.214895, acc.: 66.41%] [G loss: 0.412457]\n",
      "epoch:22 step:21030 [D loss: 0.194339, acc.: 73.44%] [G loss: 0.480507]\n",
      "epoch:22 step:21031 [D loss: 0.234717, acc.: 60.94%] [G loss: 0.455268]\n",
      "epoch:22 step:21032 [D loss: 0.252987, acc.: 54.69%] [G loss: 0.446724]\n",
      "epoch:22 step:21033 [D loss: 0.247029, acc.: 64.06%] [G loss: 0.433858]\n",
      "epoch:22 step:21034 [D loss: 0.220952, acc.: 66.41%] [G loss: 0.439802]\n",
      "epoch:22 step:21035 [D loss: 0.238846, acc.: 59.38%] [G loss: 0.422229]\n",
      "epoch:22 step:21036 [D loss: 0.233761, acc.: 56.25%] [G loss: 0.425776]\n",
      "epoch:22 step:21037 [D loss: 0.219718, acc.: 72.66%] [G loss: 0.401111]\n",
      "epoch:22 step:21038 [D loss: 0.225980, acc.: 62.50%] [G loss: 0.380350]\n",
      "epoch:22 step:21039 [D loss: 0.228716, acc.: 66.41%] [G loss: 0.450237]\n",
      "epoch:22 step:21040 [D loss: 0.201673, acc.: 72.66%] [G loss: 0.454637]\n",
      "epoch:22 step:21041 [D loss: 0.198152, acc.: 74.22%] [G loss: 0.410128]\n",
      "epoch:22 step:21042 [D loss: 0.224749, acc.: 60.94%] [G loss: 0.421114]\n",
      "epoch:22 step:21043 [D loss: 0.213146, acc.: 64.84%] [G loss: 0.452883]\n",
      "epoch:22 step:21044 [D loss: 0.201337, acc.: 67.97%] [G loss: 0.481464]\n",
      "epoch:22 step:21045 [D loss: 0.222825, acc.: 65.62%] [G loss: 0.455860]\n",
      "epoch:22 step:21046 [D loss: 0.239081, acc.: 55.47%] [G loss: 0.387916]\n",
      "epoch:22 step:21047 [D loss: 0.233417, acc.: 60.16%] [G loss: 0.443425]\n",
      "epoch:22 step:21048 [D loss: 0.193787, acc.: 74.22%] [G loss: 0.464783]\n",
      "epoch:22 step:21049 [D loss: 0.213035, acc.: 69.53%] [G loss: 0.505214]\n",
      "epoch:22 step:21050 [D loss: 0.194827, acc.: 71.88%] [G loss: 0.535357]\n",
      "epoch:22 step:21051 [D loss: 0.264966, acc.: 53.91%] [G loss: 0.448415]\n",
      "epoch:22 step:21052 [D loss: 0.227124, acc.: 60.16%] [G loss: 0.449626]\n",
      "epoch:22 step:21053 [D loss: 0.208536, acc.: 63.28%] [G loss: 0.470639]\n",
      "epoch:22 step:21054 [D loss: 0.205562, acc.: 67.97%] [G loss: 0.509071]\n",
      "epoch:22 step:21055 [D loss: 0.241216, acc.: 59.38%] [G loss: 0.452754]\n",
      "epoch:22 step:21056 [D loss: 0.236282, acc.: 60.94%] [G loss: 0.443137]\n",
      "epoch:22 step:21057 [D loss: 0.241746, acc.: 60.94%] [G loss: 0.430867]\n",
      "epoch:22 step:21058 [D loss: 0.241941, acc.: 57.81%] [G loss: 0.420437]\n",
      "epoch:22 step:21059 [D loss: 0.228084, acc.: 60.16%] [G loss: 0.449852]\n",
      "epoch:22 step:21060 [D loss: 0.220333, acc.: 64.84%] [G loss: 0.471694]\n",
      "epoch:22 step:21061 [D loss: 0.212634, acc.: 64.06%] [G loss: 0.455737]\n",
      "epoch:22 step:21062 [D loss: 0.256648, acc.: 59.38%] [G loss: 0.444996]\n",
      "epoch:22 step:21063 [D loss: 0.222185, acc.: 64.84%] [G loss: 0.424560]\n",
      "epoch:22 step:21064 [D loss: 0.220979, acc.: 63.28%] [G loss: 0.414091]\n",
      "epoch:22 step:21065 [D loss: 0.207059, acc.: 69.53%] [G loss: 0.445704]\n",
      "epoch:22 step:21066 [D loss: 0.211998, acc.: 63.28%] [G loss: 0.449615]\n",
      "epoch:22 step:21067 [D loss: 0.207516, acc.: 67.19%] [G loss: 0.436848]\n",
      "epoch:22 step:21068 [D loss: 0.205586, acc.: 69.53%] [G loss: 0.484717]\n",
      "epoch:22 step:21069 [D loss: 0.244261, acc.: 59.38%] [G loss: 0.412204]\n",
      "epoch:22 step:21070 [D loss: 0.213371, acc.: 64.84%] [G loss: 0.460988]\n",
      "epoch:22 step:21071 [D loss: 0.210331, acc.: 70.31%] [G loss: 0.467800]\n",
      "epoch:22 step:21072 [D loss: 0.306210, acc.: 43.75%] [G loss: 0.391472]\n",
      "epoch:22 step:21073 [D loss: 0.251660, acc.: 51.56%] [G loss: 0.407558]\n",
      "epoch:22 step:21074 [D loss: 0.257235, acc.: 54.69%] [G loss: 0.393131]\n",
      "epoch:22 step:21075 [D loss: 0.224831, acc.: 59.38%] [G loss: 0.383903]\n",
      "epoch:22 step:21076 [D loss: 0.239101, acc.: 59.38%] [G loss: 0.402713]\n",
      "epoch:22 step:21077 [D loss: 0.236922, acc.: 56.25%] [G loss: 0.405453]\n",
      "epoch:22 step:21078 [D loss: 0.221814, acc.: 62.50%] [G loss: 0.407113]\n",
      "epoch:22 step:21079 [D loss: 0.238843, acc.: 58.59%] [G loss: 0.415795]\n",
      "epoch:22 step:21080 [D loss: 0.218729, acc.: 64.84%] [G loss: 0.425963]\n",
      "epoch:22 step:21081 [D loss: 0.217807, acc.: 64.84%] [G loss: 0.428302]\n",
      "epoch:22 step:21082 [D loss: 0.223794, acc.: 62.50%] [G loss: 0.451398]\n",
      "epoch:22 step:21083 [D loss: 0.221134, acc.: 63.28%] [G loss: 0.458654]\n",
      "epoch:22 step:21084 [D loss: 0.199066, acc.: 68.75%] [G loss: 0.468444]\n",
      "epoch:22 step:21085 [D loss: 0.203970, acc.: 73.44%] [G loss: 0.459771]\n",
      "epoch:22 step:21086 [D loss: 0.233314, acc.: 62.50%] [G loss: 0.486190]\n",
      "epoch:22 step:21087 [D loss: 0.255193, acc.: 53.12%] [G loss: 0.478625]\n",
      "epoch:22 step:21088 [D loss: 0.205632, acc.: 66.41%] [G loss: 0.488053]\n",
      "epoch:22 step:21089 [D loss: 0.213346, acc.: 65.62%] [G loss: 0.472231]\n",
      "epoch:22 step:21090 [D loss: 0.229869, acc.: 64.06%] [G loss: 0.430113]\n",
      "epoch:22 step:21091 [D loss: 0.255266, acc.: 57.81%] [G loss: 0.422088]\n",
      "epoch:22 step:21092 [D loss: 0.235928, acc.: 60.16%] [G loss: 0.388157]\n",
      "epoch:22 step:21093 [D loss: 0.204821, acc.: 67.19%] [G loss: 0.375479]\n",
      "epoch:22 step:21094 [D loss: 0.215173, acc.: 71.09%] [G loss: 0.406111]\n",
      "epoch:22 step:21095 [D loss: 0.204253, acc.: 69.53%] [G loss: 0.429462]\n",
      "epoch:22 step:21096 [D loss: 0.271552, acc.: 50.78%] [G loss: 0.433216]\n",
      "epoch:22 step:21097 [D loss: 0.220635, acc.: 63.28%] [G loss: 0.429526]\n",
      "epoch:22 step:21098 [D loss: 0.195704, acc.: 70.31%] [G loss: 0.440494]\n",
      "epoch:22 step:21099 [D loss: 0.200160, acc.: 69.53%] [G loss: 0.447497]\n",
      "epoch:22 step:21100 [D loss: 0.250998, acc.: 58.59%] [G loss: 0.433554]\n",
      "epoch:22 step:21101 [D loss: 0.242112, acc.: 56.25%] [G loss: 0.386765]\n",
      "epoch:22 step:21102 [D loss: 0.202432, acc.: 66.41%] [G loss: 0.431341]\n",
      "epoch:22 step:21103 [D loss: 0.244353, acc.: 58.59%] [G loss: 0.418125]\n",
      "epoch:22 step:21104 [D loss: 0.223236, acc.: 64.06%] [G loss: 0.433487]\n",
      "epoch:22 step:21105 [D loss: 0.207136, acc.: 69.53%] [G loss: 0.463040]\n",
      "epoch:22 step:21106 [D loss: 0.238671, acc.: 60.94%] [G loss: 0.398546]\n",
      "epoch:22 step:21107 [D loss: 0.226439, acc.: 57.81%] [G loss: 0.415257]\n",
      "epoch:22 step:21108 [D loss: 0.227310, acc.: 61.72%] [G loss: 0.408954]\n",
      "epoch:22 step:21109 [D loss: 0.193180, acc.: 70.31%] [G loss: 0.434005]\n",
      "epoch:22 step:21110 [D loss: 0.226896, acc.: 64.84%] [G loss: 0.472738]\n",
      "epoch:22 step:21111 [D loss: 0.214159, acc.: 61.72%] [G loss: 0.444091]\n",
      "epoch:22 step:21112 [D loss: 0.224269, acc.: 62.50%] [G loss: 0.438833]\n",
      "epoch:22 step:21113 [D loss: 0.182598, acc.: 75.00%] [G loss: 0.464704]\n",
      "epoch:22 step:21114 [D loss: 0.255743, acc.: 57.03%] [G loss: 0.424462]\n",
      "epoch:22 step:21115 [D loss: 0.280644, acc.: 51.56%] [G loss: 0.409281]\n",
      "epoch:22 step:21116 [D loss: 0.244761, acc.: 54.69%] [G loss: 0.408184]\n",
      "epoch:22 step:21117 [D loss: 0.229459, acc.: 62.50%] [G loss: 0.417680]\n",
      "epoch:22 step:21118 [D loss: 0.215618, acc.: 64.84%] [G loss: 0.455650]\n",
      "epoch:22 step:21119 [D loss: 0.198662, acc.: 71.09%] [G loss: 0.476925]\n",
      "epoch:22 step:21120 [D loss: 0.252339, acc.: 60.16%] [G loss: 0.482659]\n",
      "epoch:22 step:21121 [D loss: 0.197952, acc.: 71.09%] [G loss: 0.508600]\n",
      "epoch:22 step:21122 [D loss: 0.204549, acc.: 67.97%] [G loss: 0.459001]\n",
      "epoch:22 step:21123 [D loss: 0.237722, acc.: 57.03%] [G loss: 0.412694]\n",
      "epoch:22 step:21124 [D loss: 0.235887, acc.: 57.81%] [G loss: 0.403328]\n",
      "epoch:22 step:21125 [D loss: 0.239514, acc.: 59.38%] [G loss: 0.440896]\n",
      "epoch:22 step:21126 [D loss: 0.232462, acc.: 59.38%] [G loss: 0.416869]\n",
      "epoch:22 step:21127 [D loss: 0.204965, acc.: 65.62%] [G loss: 0.451881]\n",
      "epoch:22 step:21128 [D loss: 0.188885, acc.: 69.53%] [G loss: 0.482293]\n",
      "epoch:22 step:21129 [D loss: 0.224095, acc.: 66.41%] [G loss: 0.419904]\n",
      "epoch:22 step:21130 [D loss: 0.212138, acc.: 64.84%] [G loss: 0.445358]\n",
      "epoch:22 step:21131 [D loss: 0.241795, acc.: 59.38%] [G loss: 0.390477]\n",
      "epoch:22 step:21132 [D loss: 0.243593, acc.: 58.59%] [G loss: 0.439488]\n",
      "epoch:22 step:21133 [D loss: 0.211484, acc.: 69.53%] [G loss: 0.409332]\n",
      "epoch:22 step:21134 [D loss: 0.200116, acc.: 67.97%] [G loss: 0.434949]\n",
      "epoch:22 step:21135 [D loss: 0.203298, acc.: 67.97%] [G loss: 0.452613]\n",
      "epoch:22 step:21136 [D loss: 0.199822, acc.: 71.88%] [G loss: 0.476111]\n",
      "epoch:22 step:21137 [D loss: 0.213175, acc.: 66.41%] [G loss: 0.465553]\n",
      "epoch:22 step:21138 [D loss: 0.231655, acc.: 64.06%] [G loss: 0.437362]\n",
      "epoch:22 step:21139 [D loss: 0.231066, acc.: 67.19%] [G loss: 0.432756]\n",
      "epoch:22 step:21140 [D loss: 0.220507, acc.: 61.72%] [G loss: 0.433034]\n",
      "epoch:22 step:21141 [D loss: 0.227913, acc.: 56.25%] [G loss: 0.416993]\n",
      "epoch:22 step:21142 [D loss: 0.260357, acc.: 48.44%] [G loss: 0.420506]\n",
      "epoch:22 step:21143 [D loss: 0.256451, acc.: 53.12%] [G loss: 0.415340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21144 [D loss: 0.215429, acc.: 69.53%] [G loss: 0.473344]\n",
      "epoch:22 step:21145 [D loss: 0.225245, acc.: 63.28%] [G loss: 0.446606]\n",
      "epoch:22 step:21146 [D loss: 0.236348, acc.: 60.16%] [G loss: 0.380601]\n",
      "epoch:22 step:21147 [D loss: 0.224392, acc.: 57.03%] [G loss: 0.400662]\n",
      "epoch:22 step:21148 [D loss: 0.190484, acc.: 73.44%] [G loss: 0.432664]\n",
      "epoch:22 step:21149 [D loss: 0.258060, acc.: 58.59%] [G loss: 0.390790]\n",
      "epoch:22 step:21150 [D loss: 0.212541, acc.: 69.53%] [G loss: 0.414956]\n",
      "epoch:22 step:21151 [D loss: 0.219117, acc.: 66.41%] [G loss: 0.399995]\n",
      "epoch:22 step:21152 [D loss: 0.251174, acc.: 59.38%] [G loss: 0.392604]\n",
      "epoch:22 step:21153 [D loss: 0.206363, acc.: 65.62%] [G loss: 0.449013]\n",
      "epoch:22 step:21154 [D loss: 0.238159, acc.: 59.38%] [G loss: 0.433895]\n",
      "epoch:22 step:21155 [D loss: 0.228405, acc.: 64.84%] [G loss: 0.409987]\n",
      "epoch:22 step:21156 [D loss: 0.284960, acc.: 47.66%] [G loss: 0.381053]\n",
      "epoch:22 step:21157 [D loss: 0.237904, acc.: 62.50%] [G loss: 0.416222]\n",
      "epoch:22 step:21158 [D loss: 0.253381, acc.: 56.25%] [G loss: 0.420370]\n",
      "epoch:22 step:21159 [D loss: 0.223574, acc.: 63.28%] [G loss: 0.404412]\n",
      "epoch:22 step:21160 [D loss: 0.231314, acc.: 61.72%] [G loss: 0.441777]\n",
      "epoch:22 step:21161 [D loss: 0.212286, acc.: 71.09%] [G loss: 0.415511]\n",
      "epoch:22 step:21162 [D loss: 0.245107, acc.: 55.47%] [G loss: 0.408926]\n",
      "epoch:22 step:21163 [D loss: 0.195552, acc.: 71.88%] [G loss: 0.440983]\n",
      "epoch:22 step:21164 [D loss: 0.216363, acc.: 64.06%] [G loss: 0.451902]\n",
      "epoch:22 step:21165 [D loss: 0.206228, acc.: 74.22%] [G loss: 0.478013]\n",
      "epoch:22 step:21166 [D loss: 0.186118, acc.: 72.66%] [G loss: 0.473412]\n",
      "epoch:22 step:21167 [D loss: 0.233924, acc.: 61.72%] [G loss: 0.435816]\n",
      "epoch:22 step:21168 [D loss: 0.213293, acc.: 67.19%] [G loss: 0.432127]\n",
      "epoch:22 step:21169 [D loss: 0.205775, acc.: 70.31%] [G loss: 0.461663]\n",
      "epoch:22 step:21170 [D loss: 0.215213, acc.: 66.41%] [G loss: 0.418355]\n",
      "epoch:22 step:21171 [D loss: 0.185307, acc.: 75.00%] [G loss: 0.461496]\n",
      "epoch:22 step:21172 [D loss: 0.219589, acc.: 64.84%] [G loss: 0.442007]\n",
      "epoch:22 step:21173 [D loss: 0.267324, acc.: 53.12%] [G loss: 0.393939]\n",
      "epoch:22 step:21174 [D loss: 0.230547, acc.: 57.81%] [G loss: 0.444789]\n",
      "epoch:22 step:21175 [D loss: 0.218166, acc.: 67.19%] [G loss: 0.407851]\n",
      "epoch:22 step:21176 [D loss: 0.219578, acc.: 67.97%] [G loss: 0.430546]\n",
      "epoch:22 step:21177 [D loss: 0.210682, acc.: 62.50%] [G loss: 0.400392]\n",
      "epoch:22 step:21178 [D loss: 0.150897, acc.: 82.81%] [G loss: 0.474101]\n",
      "epoch:22 step:21179 [D loss: 0.233544, acc.: 59.38%] [G loss: 0.451406]\n",
      "epoch:22 step:21180 [D loss: 0.244753, acc.: 57.81%] [G loss: 0.437497]\n",
      "epoch:22 step:21181 [D loss: 0.219418, acc.: 60.94%] [G loss: 0.470738]\n",
      "epoch:22 step:21182 [D loss: 0.212618, acc.: 62.50%] [G loss: 0.460663]\n",
      "epoch:22 step:21183 [D loss: 0.252938, acc.: 55.47%] [G loss: 0.386342]\n",
      "epoch:22 step:21184 [D loss: 0.226411, acc.: 64.06%] [G loss: 0.348541]\n",
      "epoch:22 step:21185 [D loss: 0.220447, acc.: 64.06%] [G loss: 0.377295]\n",
      "epoch:22 step:21186 [D loss: 0.205877, acc.: 71.09%] [G loss: 0.427521]\n",
      "epoch:22 step:21187 [D loss: 0.234939, acc.: 59.38%] [G loss: 0.432806]\n",
      "epoch:22 step:21188 [D loss: 0.167837, acc.: 83.59%] [G loss: 0.522634]\n",
      "epoch:22 step:21189 [D loss: 0.199817, acc.: 73.44%] [G loss: 0.483924]\n",
      "epoch:22 step:21190 [D loss: 0.234574, acc.: 60.16%] [G loss: 0.430654]\n",
      "epoch:22 step:21191 [D loss: 0.252185, acc.: 54.69%] [G loss: 0.414759]\n",
      "epoch:22 step:21192 [D loss: 0.229167, acc.: 61.72%] [G loss: 0.395471]\n",
      "epoch:22 step:21193 [D loss: 0.227985, acc.: 60.16%] [G loss: 0.415813]\n",
      "epoch:22 step:21194 [D loss: 0.246909, acc.: 58.59%] [G loss: 0.417431]\n",
      "epoch:22 step:21195 [D loss: 0.210813, acc.: 64.84%] [G loss: 0.411245]\n",
      "epoch:22 step:21196 [D loss: 0.197901, acc.: 71.09%] [G loss: 0.446452]\n",
      "epoch:22 step:21197 [D loss: 0.244238, acc.: 60.16%] [G loss: 0.423554]\n",
      "epoch:22 step:21198 [D loss: 0.219326, acc.: 66.41%] [G loss: 0.448203]\n",
      "epoch:22 step:21199 [D loss: 0.230475, acc.: 58.59%] [G loss: 0.413349]\n",
      "epoch:22 step:21200 [D loss: 0.240522, acc.: 60.94%] [G loss: 0.400197]\n",
      "##############\n",
      "[2.49615187 1.61824064 5.88208093 4.85955488 3.65048815 5.73929396\n",
      " 4.33469811 4.63532946 4.41638351 3.92679362]\n",
      "##########\n",
      "epoch:22 step:21201 [D loss: 0.233277, acc.: 62.50%] [G loss: 0.436779]\n",
      "epoch:22 step:21202 [D loss: 0.246374, acc.: 58.59%] [G loss: 0.427903]\n",
      "epoch:22 step:21203 [D loss: 0.196652, acc.: 69.53%] [G loss: 0.500872]\n",
      "epoch:22 step:21204 [D loss: 0.236624, acc.: 58.59%] [G loss: 0.420189]\n",
      "epoch:22 step:21205 [D loss: 0.227712, acc.: 62.50%] [G loss: 0.447573]\n",
      "epoch:22 step:21206 [D loss: 0.204779, acc.: 69.53%] [G loss: 0.406287]\n",
      "epoch:22 step:21207 [D loss: 0.206174, acc.: 67.19%] [G loss: 0.432402]\n",
      "epoch:22 step:21208 [D loss: 0.225404, acc.: 63.28%] [G loss: 0.418869]\n",
      "epoch:22 step:21209 [D loss: 0.215833, acc.: 68.75%] [G loss: 0.432830]\n",
      "epoch:22 step:21210 [D loss: 0.248079, acc.: 55.47%] [G loss: 0.403974]\n",
      "epoch:22 step:21211 [D loss: 0.248982, acc.: 56.25%] [G loss: 0.412067]\n",
      "epoch:22 step:21212 [D loss: 0.203841, acc.: 71.88%] [G loss: 0.449003]\n",
      "epoch:22 step:21213 [D loss: 0.235920, acc.: 57.81%] [G loss: 0.432978]\n",
      "epoch:22 step:21214 [D loss: 0.269014, acc.: 47.66%] [G loss: 0.413277]\n",
      "epoch:22 step:21215 [D loss: 0.226745, acc.: 64.06%] [G loss: 0.424457]\n",
      "epoch:22 step:21216 [D loss: 0.241245, acc.: 57.03%] [G loss: 0.425501]\n",
      "epoch:22 step:21217 [D loss: 0.231711, acc.: 66.41%] [G loss: 0.405825]\n",
      "epoch:22 step:21218 [D loss: 0.223697, acc.: 65.62%] [G loss: 0.430954]\n",
      "epoch:22 step:21219 [D loss: 0.221681, acc.: 62.50%] [G loss: 0.459284]\n",
      "epoch:22 step:21220 [D loss: 0.227812, acc.: 60.16%] [G loss: 0.442901]\n",
      "epoch:22 step:21221 [D loss: 0.226888, acc.: 58.59%] [G loss: 0.416200]\n",
      "epoch:22 step:21222 [D loss: 0.226742, acc.: 64.06%] [G loss: 0.405266]\n",
      "epoch:22 step:21223 [D loss: 0.204568, acc.: 68.75%] [G loss: 0.431731]\n",
      "epoch:22 step:21224 [D loss: 0.230371, acc.: 59.38%] [G loss: 0.419875]\n",
      "epoch:22 step:21225 [D loss: 0.233319, acc.: 60.16%] [G loss: 0.395459]\n",
      "epoch:22 step:21226 [D loss: 0.226252, acc.: 60.16%] [G loss: 0.391264]\n",
      "epoch:22 step:21227 [D loss: 0.193207, acc.: 68.75%] [G loss: 0.423694]\n",
      "epoch:22 step:21228 [D loss: 0.240173, acc.: 55.47%] [G loss: 0.414043]\n",
      "epoch:22 step:21229 [D loss: 0.231669, acc.: 58.59%] [G loss: 0.376859]\n",
      "epoch:22 step:21230 [D loss: 0.223916, acc.: 63.28%] [G loss: 0.396074]\n",
      "epoch:22 step:21231 [D loss: 0.250082, acc.: 57.03%] [G loss: 0.394415]\n",
      "epoch:22 step:21232 [D loss: 0.236360, acc.: 61.72%] [G loss: 0.444332]\n",
      "epoch:22 step:21233 [D loss: 0.230000, acc.: 62.50%] [G loss: 0.435269]\n",
      "epoch:22 step:21234 [D loss: 0.190719, acc.: 67.19%] [G loss: 0.478492]\n",
      "epoch:22 step:21235 [D loss: 0.235048, acc.: 59.38%] [G loss: 0.415202]\n",
      "epoch:22 step:21236 [D loss: 0.216662, acc.: 63.28%] [G loss: 0.448800]\n",
      "epoch:22 step:21237 [D loss: 0.215840, acc.: 66.41%] [G loss: 0.408219]\n",
      "epoch:22 step:21238 [D loss: 0.195463, acc.: 66.41%] [G loss: 0.445778]\n",
      "epoch:22 step:21239 [D loss: 0.205615, acc.: 69.53%] [G loss: 0.499071]\n",
      "epoch:22 step:21240 [D loss: 0.232824, acc.: 63.28%] [G loss: 0.463439]\n",
      "epoch:22 step:21241 [D loss: 0.212259, acc.: 64.84%] [G loss: 0.438636]\n",
      "epoch:22 step:21242 [D loss: 0.255403, acc.: 58.59%] [G loss: 0.422326]\n",
      "epoch:22 step:21243 [D loss: 0.198704, acc.: 69.53%] [G loss: 0.421126]\n",
      "epoch:22 step:21244 [D loss: 0.229901, acc.: 61.72%] [G loss: 0.420948]\n",
      "epoch:22 step:21245 [D loss: 0.190959, acc.: 71.88%] [G loss: 0.450274]\n",
      "epoch:22 step:21246 [D loss: 0.211116, acc.: 61.72%] [G loss: 0.470632]\n",
      "epoch:22 step:21247 [D loss: 0.214350, acc.: 65.62%] [G loss: 0.440210]\n",
      "epoch:22 step:21248 [D loss: 0.202020, acc.: 67.97%] [G loss: 0.447908]\n",
      "epoch:22 step:21249 [D loss: 0.186764, acc.: 72.66%] [G loss: 0.430950]\n",
      "epoch:22 step:21250 [D loss: 0.231390, acc.: 60.94%] [G loss: 0.447052]\n",
      "epoch:22 step:21251 [D loss: 0.219139, acc.: 63.28%] [G loss: 0.445184]\n",
      "epoch:22 step:21252 [D loss: 0.220235, acc.: 60.94%] [G loss: 0.414290]\n",
      "epoch:22 step:21253 [D loss: 0.241168, acc.: 53.12%] [G loss: 0.392518]\n",
      "epoch:22 step:21254 [D loss: 0.235601, acc.: 60.16%] [G loss: 0.434484]\n",
      "epoch:22 step:21255 [D loss: 0.213262, acc.: 67.97%] [G loss: 0.453337]\n",
      "epoch:22 step:21256 [D loss: 0.186834, acc.: 74.22%] [G loss: 0.485395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21257 [D loss: 0.221872, acc.: 60.94%] [G loss: 0.453862]\n",
      "epoch:22 step:21258 [D loss: 0.250195, acc.: 50.00%] [G loss: 0.403976]\n",
      "epoch:22 step:21259 [D loss: 0.206215, acc.: 67.19%] [G loss: 0.419977]\n",
      "epoch:22 step:21260 [D loss: 0.237221, acc.: 60.94%] [G loss: 0.436543]\n",
      "epoch:22 step:21261 [D loss: 0.206645, acc.: 67.97%] [G loss: 0.460689]\n",
      "epoch:22 step:21262 [D loss: 0.193991, acc.: 65.62%] [G loss: 0.472113]\n",
      "epoch:22 step:21263 [D loss: 0.236059, acc.: 59.38%] [G loss: 0.481425]\n",
      "epoch:22 step:21264 [D loss: 0.188023, acc.: 74.22%] [G loss: 0.488635]\n",
      "epoch:22 step:21265 [D loss: 0.208600, acc.: 60.94%] [G loss: 0.457299]\n",
      "epoch:22 step:21266 [D loss: 0.244934, acc.: 63.28%] [G loss: 0.451668]\n",
      "epoch:22 step:21267 [D loss: 0.243616, acc.: 56.25%] [G loss: 0.429173]\n",
      "epoch:22 step:21268 [D loss: 0.215108, acc.: 62.50%] [G loss: 0.450205]\n",
      "epoch:22 step:21269 [D loss: 0.217204, acc.: 68.75%] [G loss: 0.454349]\n",
      "epoch:22 step:21270 [D loss: 0.226849, acc.: 66.41%] [G loss: 0.435846]\n",
      "epoch:22 step:21271 [D loss: 0.222459, acc.: 68.75%] [G loss: 0.406149]\n",
      "epoch:22 step:21272 [D loss: 0.224975, acc.: 65.62%] [G loss: 0.413016]\n",
      "epoch:22 step:21273 [D loss: 0.225696, acc.: 64.06%] [G loss: 0.450910]\n",
      "epoch:22 step:21274 [D loss: 0.217251, acc.: 66.41%] [G loss: 0.454735]\n",
      "epoch:22 step:21275 [D loss: 0.202230, acc.: 76.56%] [G loss: 0.457664]\n",
      "epoch:22 step:21276 [D loss: 0.237742, acc.: 59.38%] [G loss: 0.431997]\n",
      "epoch:22 step:21277 [D loss: 0.243409, acc.: 57.81%] [G loss: 0.423342]\n",
      "epoch:22 step:21278 [D loss: 0.216502, acc.: 68.75%] [G loss: 0.445973]\n",
      "epoch:22 step:21279 [D loss: 0.205298, acc.: 65.62%] [G loss: 0.442820]\n",
      "epoch:22 step:21280 [D loss: 0.211710, acc.: 63.28%] [G loss: 0.420107]\n",
      "epoch:22 step:21281 [D loss: 0.263872, acc.: 48.44%] [G loss: 0.391839]\n",
      "epoch:22 step:21282 [D loss: 0.231967, acc.: 60.16%] [G loss: 0.402247]\n",
      "epoch:22 step:21283 [D loss: 0.224322, acc.: 67.19%] [G loss: 0.437478]\n",
      "epoch:22 step:21284 [D loss: 0.224761, acc.: 59.38%] [G loss: 0.440826]\n",
      "epoch:22 step:21285 [D loss: 0.217662, acc.: 64.06%] [G loss: 0.394788]\n",
      "epoch:22 step:21286 [D loss: 0.224300, acc.: 67.19%] [G loss: 0.452546]\n",
      "epoch:22 step:21287 [D loss: 0.242640, acc.: 57.03%] [G loss: 0.407454]\n",
      "epoch:22 step:21288 [D loss: 0.195935, acc.: 74.22%] [G loss: 0.472391]\n",
      "epoch:22 step:21289 [D loss: 0.249242, acc.: 57.81%] [G loss: 0.406849]\n",
      "epoch:22 step:21290 [D loss: 0.214016, acc.: 66.41%] [G loss: 0.432335]\n",
      "epoch:22 step:21291 [D loss: 0.182928, acc.: 75.00%] [G loss: 0.458493]\n",
      "epoch:22 step:21292 [D loss: 0.244321, acc.: 60.16%] [G loss: 0.438242]\n",
      "epoch:22 step:21293 [D loss: 0.214361, acc.: 65.62%] [G loss: 0.431014]\n",
      "epoch:22 step:21294 [D loss: 0.214920, acc.: 64.06%] [G loss: 0.460168]\n",
      "epoch:22 step:21295 [D loss: 0.199075, acc.: 69.53%] [G loss: 0.465867]\n",
      "epoch:22 step:21296 [D loss: 0.235349, acc.: 57.81%] [G loss: 0.442769]\n",
      "epoch:22 step:21297 [D loss: 0.240849, acc.: 53.91%] [G loss: 0.419930]\n",
      "epoch:22 step:21298 [D loss: 0.233290, acc.: 58.59%] [G loss: 0.413186]\n",
      "epoch:22 step:21299 [D loss: 0.233349, acc.: 57.81%] [G loss: 0.417204]\n",
      "epoch:22 step:21300 [D loss: 0.212832, acc.: 66.41%] [G loss: 0.424365]\n",
      "epoch:22 step:21301 [D loss: 0.224107, acc.: 62.50%] [G loss: 0.426172]\n",
      "epoch:22 step:21302 [D loss: 0.201410, acc.: 65.62%] [G loss: 0.403254]\n",
      "epoch:22 step:21303 [D loss: 0.231497, acc.: 61.72%] [G loss: 0.438054]\n",
      "epoch:22 step:21304 [D loss: 0.198768, acc.: 69.53%] [G loss: 0.477603]\n",
      "epoch:22 step:21305 [D loss: 0.209822, acc.: 61.72%] [G loss: 0.442025]\n",
      "epoch:22 step:21306 [D loss: 0.205694, acc.: 68.75%] [G loss: 0.505672]\n",
      "epoch:22 step:21307 [D loss: 0.205686, acc.: 67.19%] [G loss: 0.496312]\n",
      "epoch:22 step:21308 [D loss: 0.197575, acc.: 71.09%] [G loss: 0.476097]\n",
      "epoch:22 step:21309 [D loss: 0.215427, acc.: 63.28%] [G loss: 0.449777]\n",
      "epoch:22 step:21310 [D loss: 0.252571, acc.: 52.34%] [G loss: 0.437413]\n",
      "epoch:22 step:21311 [D loss: 0.213174, acc.: 66.41%] [G loss: 0.422124]\n",
      "epoch:22 step:21312 [D loss: 0.229404, acc.: 65.62%] [G loss: 0.435621]\n",
      "epoch:22 step:21313 [D loss: 0.213600, acc.: 66.41%] [G loss: 0.453277]\n",
      "epoch:22 step:21314 [D loss: 0.193608, acc.: 67.97%] [G loss: 0.455978]\n",
      "epoch:22 step:21315 [D loss: 0.204055, acc.: 67.97%] [G loss: 0.453022]\n",
      "epoch:22 step:21316 [D loss: 0.232019, acc.: 64.84%] [G loss: 0.468487]\n",
      "epoch:22 step:21317 [D loss: 0.259540, acc.: 58.59%] [G loss: 0.392217]\n",
      "epoch:22 step:21318 [D loss: 0.257642, acc.: 57.81%] [G loss: 0.351754]\n",
      "epoch:22 step:21319 [D loss: 0.213534, acc.: 65.62%] [G loss: 0.419140]\n",
      "epoch:22 step:21320 [D loss: 0.214669, acc.: 67.97%] [G loss: 0.397694]\n",
      "epoch:22 step:21321 [D loss: 0.208895, acc.: 71.09%] [G loss: 0.435543]\n",
      "epoch:22 step:21322 [D loss: 0.222680, acc.: 65.62%] [G loss: 0.429598]\n",
      "epoch:22 step:21323 [D loss: 0.204229, acc.: 75.78%] [G loss: 0.457992]\n",
      "epoch:22 step:21324 [D loss: 0.259490, acc.: 55.47%] [G loss: 0.388615]\n",
      "epoch:22 step:21325 [D loss: 0.210801, acc.: 68.75%] [G loss: 0.424736]\n",
      "epoch:22 step:21326 [D loss: 0.198817, acc.: 73.44%] [G loss: 0.469108]\n",
      "epoch:22 step:21327 [D loss: 0.232259, acc.: 59.38%] [G loss: 0.421288]\n",
      "epoch:22 step:21328 [D loss: 0.243728, acc.: 56.25%] [G loss: 0.406590]\n",
      "epoch:22 step:21329 [D loss: 0.229595, acc.: 58.59%] [G loss: 0.426867]\n",
      "epoch:22 step:21330 [D loss: 0.258437, acc.: 53.12%] [G loss: 0.392814]\n",
      "epoch:22 step:21331 [D loss: 0.214473, acc.: 73.44%] [G loss: 0.412834]\n",
      "epoch:22 step:21332 [D loss: 0.213447, acc.: 64.84%] [G loss: 0.387725]\n",
      "epoch:22 step:21333 [D loss: 0.206863, acc.: 67.19%] [G loss: 0.446371]\n",
      "epoch:22 step:21334 [D loss: 0.233050, acc.: 60.16%] [G loss: 0.455518]\n",
      "epoch:22 step:21335 [D loss: 0.223119, acc.: 63.28%] [G loss: 0.447018]\n",
      "epoch:22 step:21336 [D loss: 0.230064, acc.: 59.38%] [G loss: 0.407382]\n",
      "epoch:22 step:21337 [D loss: 0.194527, acc.: 72.66%] [G loss: 0.436826]\n",
      "epoch:22 step:21338 [D loss: 0.187049, acc.: 70.31%] [G loss: 0.446901]\n",
      "epoch:22 step:21339 [D loss: 0.223690, acc.: 67.19%] [G loss: 0.459733]\n",
      "epoch:22 step:21340 [D loss: 0.230401, acc.: 66.41%] [G loss: 0.439270]\n",
      "epoch:22 step:21341 [D loss: 0.230588, acc.: 61.72%] [G loss: 0.386797]\n",
      "epoch:22 step:21342 [D loss: 0.230241, acc.: 60.94%] [G loss: 0.421200]\n",
      "epoch:22 step:21343 [D loss: 0.226859, acc.: 66.41%] [G loss: 0.419810]\n",
      "epoch:22 step:21344 [D loss: 0.231056, acc.: 60.16%] [G loss: 0.451850]\n",
      "epoch:22 step:21345 [D loss: 0.222613, acc.: 58.59%] [G loss: 0.477363]\n",
      "epoch:22 step:21346 [D loss: 0.229083, acc.: 63.28%] [G loss: 0.438744]\n",
      "epoch:22 step:21347 [D loss: 0.203823, acc.: 70.31%] [G loss: 0.464229]\n",
      "epoch:22 step:21348 [D loss: 0.259680, acc.: 60.94%] [G loss: 0.439085]\n",
      "epoch:22 step:21349 [D loss: 0.230480, acc.: 60.94%] [G loss: 0.424288]\n",
      "epoch:22 step:21350 [D loss: 0.233329, acc.: 58.59%] [G loss: 0.469502]\n",
      "epoch:22 step:21351 [D loss: 0.219444, acc.: 64.84%] [G loss: 0.422150]\n",
      "epoch:22 step:21352 [D loss: 0.229017, acc.: 62.50%] [G loss: 0.418136]\n",
      "epoch:22 step:21353 [D loss: 0.241364, acc.: 54.69%] [G loss: 0.422187]\n",
      "epoch:22 step:21354 [D loss: 0.229156, acc.: 59.38%] [G loss: 0.414882]\n",
      "epoch:22 step:21355 [D loss: 0.232805, acc.: 61.72%] [G loss: 0.396066]\n",
      "epoch:22 step:21356 [D loss: 0.227001, acc.: 60.94%] [G loss: 0.415778]\n",
      "epoch:22 step:21357 [D loss: 0.220741, acc.: 58.59%] [G loss: 0.438421]\n",
      "epoch:22 step:21358 [D loss: 0.224167, acc.: 66.41%] [G loss: 0.431882]\n",
      "epoch:22 step:21359 [D loss: 0.222869, acc.: 61.72%] [G loss: 0.439207]\n",
      "epoch:22 step:21360 [D loss: 0.219399, acc.: 65.62%] [G loss: 0.408910]\n",
      "epoch:22 step:21361 [D loss: 0.215259, acc.: 65.62%] [G loss: 0.466037]\n",
      "epoch:22 step:21362 [D loss: 0.221835, acc.: 62.50%] [G loss: 0.395846]\n",
      "epoch:22 step:21363 [D loss: 0.215158, acc.: 71.09%] [G loss: 0.430609]\n",
      "epoch:22 step:21364 [D loss: 0.202053, acc.: 68.75%] [G loss: 0.423796]\n",
      "epoch:22 step:21365 [D loss: 0.228369, acc.: 61.72%] [G loss: 0.428616]\n",
      "epoch:22 step:21366 [D loss: 0.212664, acc.: 67.97%] [G loss: 0.455191]\n",
      "epoch:22 step:21367 [D loss: 0.230248, acc.: 60.94%] [G loss: 0.449322]\n",
      "epoch:22 step:21368 [D loss: 0.225698, acc.: 65.62%] [G loss: 0.418741]\n",
      "epoch:22 step:21369 [D loss: 0.216290, acc.: 67.97%] [G loss: 0.518531]\n",
      "epoch:22 step:21370 [D loss: 0.232059, acc.: 64.06%] [G loss: 0.469110]\n",
      "epoch:22 step:21371 [D loss: 0.225281, acc.: 61.72%] [G loss: 0.418730]\n",
      "epoch:22 step:21372 [D loss: 0.243035, acc.: 57.03%] [G loss: 0.439411]\n",
      "epoch:22 step:21373 [D loss: 0.250334, acc.: 55.47%] [G loss: 0.418893]\n",
      "epoch:22 step:21374 [D loss: 0.236823, acc.: 57.81%] [G loss: 0.419526]\n",
      "epoch:22 step:21375 [D loss: 0.209024, acc.: 67.97%] [G loss: 0.442322]\n",
      "epoch:22 step:21376 [D loss: 0.269345, acc.: 49.22%] [G loss: 0.373600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21377 [D loss: 0.227481, acc.: 62.50%] [G loss: 0.408929]\n",
      "epoch:22 step:21378 [D loss: 0.253476, acc.: 53.91%] [G loss: 0.400119]\n",
      "epoch:22 step:21379 [D loss: 0.272384, acc.: 51.56%] [G loss: 0.413562]\n",
      "epoch:22 step:21380 [D loss: 0.236596, acc.: 59.38%] [G loss: 0.414585]\n",
      "epoch:22 step:21381 [D loss: 0.212296, acc.: 65.62%] [G loss: 0.449654]\n",
      "epoch:22 step:21382 [D loss: 0.224296, acc.: 66.41%] [G loss: 0.422102]\n",
      "epoch:22 step:21383 [D loss: 0.197390, acc.: 67.19%] [G loss: 0.451610]\n",
      "epoch:22 step:21384 [D loss: 0.218171, acc.: 64.84%] [G loss: 0.447269]\n",
      "epoch:22 step:21385 [D loss: 0.210120, acc.: 69.53%] [G loss: 0.437669]\n",
      "epoch:22 step:21386 [D loss: 0.232414, acc.: 58.59%] [G loss: 0.424437]\n",
      "epoch:22 step:21387 [D loss: 0.207474, acc.: 64.06%] [G loss: 0.406989]\n",
      "epoch:22 step:21388 [D loss: 0.195195, acc.: 64.06%] [G loss: 0.472995]\n",
      "epoch:22 step:21389 [D loss: 0.195405, acc.: 65.62%] [G loss: 0.478032]\n",
      "epoch:22 step:21390 [D loss: 0.241020, acc.: 64.06%] [G loss: 0.462405]\n",
      "epoch:22 step:21391 [D loss: 0.233308, acc.: 57.81%] [G loss: 0.470904]\n",
      "epoch:22 step:21392 [D loss: 0.234303, acc.: 58.59%] [G loss: 0.425302]\n",
      "epoch:22 step:21393 [D loss: 0.219816, acc.: 61.72%] [G loss: 0.442143]\n",
      "epoch:22 step:21394 [D loss: 0.198392, acc.: 68.75%] [G loss: 0.423138]\n",
      "epoch:22 step:21395 [D loss: 0.226306, acc.: 64.06%] [G loss: 0.454412]\n",
      "epoch:22 step:21396 [D loss: 0.198382, acc.: 68.75%] [G loss: 0.484518]\n",
      "epoch:22 step:21397 [D loss: 0.248307, acc.: 58.59%] [G loss: 0.474894]\n",
      "epoch:22 step:21398 [D loss: 0.261036, acc.: 47.66%] [G loss: 0.404693]\n",
      "epoch:22 step:21399 [D loss: 0.226829, acc.: 70.31%] [G loss: 0.395270]\n",
      "epoch:22 step:21400 [D loss: 0.186528, acc.: 75.78%] [G loss: 0.473871]\n",
      "##############\n",
      "[2.88689789 1.89808668 6.04783618 4.74940381 3.55105749 5.64681714\n",
      " 4.46853533 4.82667201 4.42214276 4.05293733]\n",
      "##########\n",
      "epoch:22 step:21401 [D loss: 0.265824, acc.: 49.22%] [G loss: 0.418008]\n",
      "epoch:22 step:21402 [D loss: 0.251147, acc.: 50.00%] [G loss: 0.384153]\n",
      "epoch:22 step:21403 [D loss: 0.219547, acc.: 66.41%] [G loss: 0.397266]\n",
      "epoch:22 step:21404 [D loss: 0.220307, acc.: 60.94%] [G loss: 0.399835]\n",
      "epoch:22 step:21405 [D loss: 0.250436, acc.: 54.69%] [G loss: 0.390798]\n",
      "epoch:22 step:21406 [D loss: 0.202311, acc.: 70.31%] [G loss: 0.426177]\n",
      "epoch:22 step:21407 [D loss: 0.196208, acc.: 67.97%] [G loss: 0.477921]\n",
      "epoch:22 step:21408 [D loss: 0.245042, acc.: 57.03%] [G loss: 0.426788]\n",
      "epoch:22 step:21409 [D loss: 0.268443, acc.: 50.00%] [G loss: 0.399508]\n",
      "epoch:22 step:21410 [D loss: 0.203776, acc.: 71.09%] [G loss: 0.401525]\n",
      "epoch:22 step:21411 [D loss: 0.221524, acc.: 63.28%] [G loss: 0.402406]\n",
      "epoch:22 step:21412 [D loss: 0.240066, acc.: 57.03%] [G loss: 0.426626]\n",
      "epoch:22 step:21413 [D loss: 0.229310, acc.: 60.94%] [G loss: 0.418373]\n",
      "epoch:22 step:21414 [D loss: 0.236772, acc.: 56.25%] [G loss: 0.424772]\n",
      "epoch:22 step:21415 [D loss: 0.203722, acc.: 64.84%] [G loss: 0.446187]\n",
      "epoch:22 step:21416 [D loss: 0.190982, acc.: 71.88%] [G loss: 0.452850]\n",
      "epoch:22 step:21417 [D loss: 0.204574, acc.: 71.09%] [G loss: 0.433876]\n",
      "epoch:22 step:21418 [D loss: 0.254243, acc.: 58.59%] [G loss: 0.404150]\n",
      "epoch:22 step:21419 [D loss: 0.234429, acc.: 54.69%] [G loss: 0.412016]\n",
      "epoch:22 step:21420 [D loss: 0.230903, acc.: 64.06%] [G loss: 0.436549]\n",
      "epoch:22 step:21421 [D loss: 0.227478, acc.: 60.94%] [G loss: 0.433459]\n",
      "epoch:22 step:21422 [D loss: 0.234355, acc.: 56.25%] [G loss: 0.418530]\n",
      "epoch:22 step:21423 [D loss: 0.241062, acc.: 54.69%] [G loss: 0.376857]\n",
      "epoch:22 step:21424 [D loss: 0.209805, acc.: 66.41%] [G loss: 0.412726]\n",
      "epoch:22 step:21425 [D loss: 0.221491, acc.: 60.94%] [G loss: 0.457779]\n",
      "epoch:22 step:21426 [D loss: 0.218227, acc.: 65.62%] [G loss: 0.404731]\n",
      "epoch:22 step:21427 [D loss: 0.226112, acc.: 64.84%] [G loss: 0.465694]\n",
      "epoch:22 step:21428 [D loss: 0.224811, acc.: 64.06%] [G loss: 0.410101]\n",
      "epoch:22 step:21429 [D loss: 0.198257, acc.: 69.53%] [G loss: 0.483777]\n",
      "epoch:22 step:21430 [D loss: 0.218201, acc.: 64.06%] [G loss: 0.432320]\n",
      "epoch:22 step:21431 [D loss: 0.231757, acc.: 63.28%] [G loss: 0.433618]\n",
      "epoch:22 step:21432 [D loss: 0.244336, acc.: 63.28%] [G loss: 0.451602]\n",
      "epoch:22 step:21433 [D loss: 0.227645, acc.: 59.38%] [G loss: 0.437792]\n",
      "epoch:22 step:21434 [D loss: 0.244585, acc.: 55.47%] [G loss: 0.428884]\n",
      "epoch:22 step:21435 [D loss: 0.229700, acc.: 59.38%] [G loss: 0.385614]\n",
      "epoch:22 step:21436 [D loss: 0.216416, acc.: 64.06%] [G loss: 0.397433]\n",
      "epoch:22 step:21437 [D loss: 0.189733, acc.: 72.66%] [G loss: 0.461499]\n",
      "epoch:22 step:21438 [D loss: 0.244440, acc.: 53.91%] [G loss: 0.428352]\n",
      "epoch:22 step:21439 [D loss: 0.219665, acc.: 64.06%] [G loss: 0.394210]\n",
      "epoch:22 step:21440 [D loss: 0.218492, acc.: 65.62%] [G loss: 0.446326]\n",
      "epoch:22 step:21441 [D loss: 0.242237, acc.: 57.03%] [G loss: 0.421699]\n",
      "epoch:22 step:21442 [D loss: 0.273836, acc.: 51.56%] [G loss: 0.433464]\n",
      "epoch:22 step:21443 [D loss: 0.229924, acc.: 59.38%] [G loss: 0.412245]\n",
      "epoch:22 step:21444 [D loss: 0.209090, acc.: 68.75%] [G loss: 0.446125]\n",
      "epoch:22 step:21445 [D loss: 0.227451, acc.: 64.84%] [G loss: 0.406024]\n",
      "epoch:22 step:21446 [D loss: 0.220542, acc.: 66.41%] [G loss: 0.461953]\n",
      "epoch:22 step:21447 [D loss: 0.205690, acc.: 66.41%] [G loss: 0.414074]\n",
      "epoch:22 step:21448 [D loss: 0.232018, acc.: 56.25%] [G loss: 0.423715]\n",
      "epoch:22 step:21449 [D loss: 0.234175, acc.: 61.72%] [G loss: 0.380876]\n",
      "epoch:22 step:21450 [D loss: 0.219752, acc.: 64.06%] [G loss: 0.443759]\n",
      "epoch:22 step:21451 [D loss: 0.241415, acc.: 61.72%] [G loss: 0.427571]\n",
      "epoch:22 step:21452 [D loss: 0.207515, acc.: 64.06%] [G loss: 0.444467]\n",
      "epoch:22 step:21453 [D loss: 0.236919, acc.: 60.16%] [G loss: 0.445775]\n",
      "epoch:22 step:21454 [D loss: 0.238140, acc.: 63.28%] [G loss: 0.426884]\n",
      "epoch:22 step:21455 [D loss: 0.225249, acc.: 63.28%] [G loss: 0.401110]\n",
      "epoch:22 step:21456 [D loss: 0.193257, acc.: 69.53%] [G loss: 0.420454]\n",
      "epoch:22 step:21457 [D loss: 0.240917, acc.: 57.03%] [G loss: 0.459915]\n",
      "epoch:22 step:21458 [D loss: 0.244792, acc.: 60.94%] [G loss: 0.424334]\n",
      "epoch:22 step:21459 [D loss: 0.207144, acc.: 66.41%] [G loss: 0.434597]\n",
      "epoch:22 step:21460 [D loss: 0.218640, acc.: 62.50%] [G loss: 0.451864]\n",
      "epoch:22 step:21461 [D loss: 0.271046, acc.: 53.91%] [G loss: 0.418499]\n",
      "epoch:22 step:21462 [D loss: 0.232827, acc.: 63.28%] [G loss: 0.398156]\n",
      "epoch:22 step:21463 [D loss: 0.211161, acc.: 67.97%] [G loss: 0.383320]\n",
      "epoch:22 step:21464 [D loss: 0.230808, acc.: 62.50%] [G loss: 0.410014]\n",
      "epoch:22 step:21465 [D loss: 0.229648, acc.: 62.50%] [G loss: 0.413164]\n",
      "epoch:22 step:21466 [D loss: 0.212710, acc.: 64.84%] [G loss: 0.416265]\n",
      "epoch:22 step:21467 [D loss: 0.221504, acc.: 61.72%] [G loss: 0.438836]\n",
      "epoch:22 step:21468 [D loss: 0.221746, acc.: 62.50%] [G loss: 0.450267]\n",
      "epoch:22 step:21469 [D loss: 0.239783, acc.: 56.25%] [G loss: 0.424257]\n",
      "epoch:22 step:21470 [D loss: 0.238617, acc.: 59.38%] [G loss: 0.358308]\n",
      "epoch:22 step:21471 [D loss: 0.216390, acc.: 67.19%] [G loss: 0.402659]\n",
      "epoch:22 step:21472 [D loss: 0.270767, acc.: 47.66%] [G loss: 0.446037]\n",
      "epoch:22 step:21473 [D loss: 0.226400, acc.: 61.72%] [G loss: 0.408965]\n",
      "epoch:22 step:21474 [D loss: 0.210641, acc.: 63.28%] [G loss: 0.406090]\n",
      "epoch:22 step:21475 [D loss: 0.256356, acc.: 53.91%] [G loss: 0.412646]\n",
      "epoch:22 step:21476 [D loss: 0.256895, acc.: 55.47%] [G loss: 0.377131]\n",
      "epoch:22 step:21477 [D loss: 0.215277, acc.: 59.38%] [G loss: 0.390112]\n",
      "epoch:22 step:21478 [D loss: 0.232082, acc.: 57.03%] [G loss: 0.394423]\n",
      "epoch:22 step:21479 [D loss: 0.244367, acc.: 60.16%] [G loss: 0.373563]\n",
      "epoch:22 step:21480 [D loss: 0.233645, acc.: 57.03%] [G loss: 0.388750]\n",
      "epoch:22 step:21481 [D loss: 0.251154, acc.: 53.12%] [G loss: 0.397353]\n",
      "epoch:22 step:21482 [D loss: 0.212411, acc.: 64.06%] [G loss: 0.460011]\n",
      "epoch:22 step:21483 [D loss: 0.229215, acc.: 60.94%] [G loss: 0.397469]\n",
      "epoch:22 step:21484 [D loss: 0.237155, acc.: 63.28%] [G loss: 0.433660]\n",
      "epoch:22 step:21485 [D loss: 0.206235, acc.: 68.75%] [G loss: 0.411390]\n",
      "epoch:22 step:21486 [D loss: 0.200738, acc.: 70.31%] [G loss: 0.457638]\n",
      "epoch:22 step:21487 [D loss: 0.216828, acc.: 66.41%] [G loss: 0.443973]\n",
      "epoch:22 step:21488 [D loss: 0.237530, acc.: 67.19%] [G loss: 0.400307]\n",
      "epoch:22 step:21489 [D loss: 0.193449, acc.: 68.75%] [G loss: 0.448559]\n",
      "epoch:22 step:21490 [D loss: 0.245596, acc.: 57.03%] [G loss: 0.432736]\n",
      "epoch:22 step:21491 [D loss: 0.247484, acc.: 57.81%] [G loss: 0.408231]\n",
      "epoch:22 step:21492 [D loss: 0.207750, acc.: 73.44%] [G loss: 0.431221]\n",
      "epoch:22 step:21493 [D loss: 0.236275, acc.: 60.16%] [G loss: 0.419038]\n",
      "epoch:22 step:21494 [D loss: 0.248719, acc.: 55.47%] [G loss: 0.426726]\n",
      "epoch:22 step:21495 [D loss: 0.258992, acc.: 46.09%] [G loss: 0.371094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21496 [D loss: 0.239522, acc.: 57.03%] [G loss: 0.421039]\n",
      "epoch:22 step:21497 [D loss: 0.225334, acc.: 62.50%] [G loss: 0.425126]\n",
      "epoch:22 step:21498 [D loss: 0.215009, acc.: 71.09%] [G loss: 0.453894]\n",
      "epoch:22 step:21499 [D loss: 0.208560, acc.: 67.97%] [G loss: 0.481678]\n",
      "epoch:22 step:21500 [D loss: 0.199162, acc.: 67.97%] [G loss: 0.463430]\n",
      "epoch:22 step:21501 [D loss: 0.233346, acc.: 61.72%] [G loss: 0.447871]\n",
      "epoch:22 step:21502 [D loss: 0.225692, acc.: 64.06%] [G loss: 0.381329]\n",
      "epoch:22 step:21503 [D loss: 0.208234, acc.: 67.97%] [G loss: 0.442394]\n",
      "epoch:22 step:21504 [D loss: 0.194891, acc.: 71.88%] [G loss: 0.464883]\n",
      "epoch:22 step:21505 [D loss: 0.257745, acc.: 54.69%] [G loss: 0.431277]\n",
      "epoch:22 step:21506 [D loss: 0.264845, acc.: 49.22%] [G loss: 0.376361]\n",
      "epoch:22 step:21507 [D loss: 0.217223, acc.: 66.41%] [G loss: 0.396232]\n",
      "epoch:22 step:21508 [D loss: 0.212007, acc.: 67.97%] [G loss: 0.454555]\n",
      "epoch:22 step:21509 [D loss: 0.209542, acc.: 69.53%] [G loss: 0.443534]\n",
      "epoch:22 step:21510 [D loss: 0.203333, acc.: 66.41%] [G loss: 0.463878]\n",
      "epoch:22 step:21511 [D loss: 0.202868, acc.: 69.53%] [G loss: 0.454695]\n",
      "epoch:22 step:21512 [D loss: 0.208849, acc.: 64.06%] [G loss: 0.441816]\n",
      "epoch:22 step:21513 [D loss: 0.199622, acc.: 71.88%] [G loss: 0.467963]\n",
      "epoch:22 step:21514 [D loss: 0.205558, acc.: 68.75%] [G loss: 0.481148]\n",
      "epoch:22 step:21515 [D loss: 0.233502, acc.: 61.72%] [G loss: 0.446520]\n",
      "epoch:22 step:21516 [D loss: 0.232452, acc.: 62.50%] [G loss: 0.441963]\n",
      "epoch:22 step:21517 [D loss: 0.222690, acc.: 57.81%] [G loss: 0.444949]\n",
      "epoch:22 step:21518 [D loss: 0.247455, acc.: 59.38%] [G loss: 0.430213]\n",
      "epoch:22 step:21519 [D loss: 0.179169, acc.: 75.78%] [G loss: 0.485669]\n",
      "epoch:22 step:21520 [D loss: 0.212390, acc.: 67.19%] [G loss: 0.445459]\n",
      "epoch:22 step:21521 [D loss: 0.234312, acc.: 60.16%] [G loss: 0.435495]\n",
      "epoch:22 step:21522 [D loss: 0.189508, acc.: 71.09%] [G loss: 0.467900]\n",
      "epoch:22 step:21523 [D loss: 0.216394, acc.: 61.72%] [G loss: 0.450814]\n",
      "epoch:22 step:21524 [D loss: 0.214067, acc.: 65.62%] [G loss: 0.427297]\n",
      "epoch:22 step:21525 [D loss: 0.214146, acc.: 66.41%] [G loss: 0.444426]\n",
      "epoch:22 step:21526 [D loss: 0.220109, acc.: 66.41%] [G loss: 0.438680]\n",
      "epoch:22 step:21527 [D loss: 0.244263, acc.: 62.50%] [G loss: 0.485165]\n",
      "epoch:22 step:21528 [D loss: 0.197894, acc.: 71.09%] [G loss: 0.476180]\n",
      "epoch:22 step:21529 [D loss: 0.270382, acc.: 57.03%] [G loss: 0.421022]\n",
      "epoch:22 step:21530 [D loss: 0.191047, acc.: 71.09%] [G loss: 0.416839]\n",
      "epoch:22 step:21531 [D loss: 0.265016, acc.: 56.25%] [G loss: 0.443100]\n",
      "epoch:22 step:21532 [D loss: 0.210740, acc.: 69.53%] [G loss: 0.460964]\n",
      "epoch:22 step:21533 [D loss: 0.184511, acc.: 72.66%] [G loss: 0.465872]\n",
      "epoch:22 step:21534 [D loss: 0.279667, acc.: 53.12%] [G loss: 0.444055]\n",
      "epoch:22 step:21535 [D loss: 0.203548, acc.: 69.53%] [G loss: 0.436217]\n",
      "epoch:22 step:21536 [D loss: 0.236985, acc.: 60.94%] [G loss: 0.423919]\n",
      "epoch:22 step:21537 [D loss: 0.203351, acc.: 67.19%] [G loss: 0.458723]\n",
      "epoch:22 step:21538 [D loss: 0.164359, acc.: 78.91%] [G loss: 0.515438]\n",
      "epoch:22 step:21539 [D loss: 0.198028, acc.: 71.88%] [G loss: 0.495294]\n",
      "epoch:22 step:21540 [D loss: 0.196019, acc.: 75.00%] [G loss: 0.578704]\n",
      "epoch:22 step:21541 [D loss: 0.197450, acc.: 68.75%] [G loss: 0.564257]\n",
      "epoch:22 step:21542 [D loss: 0.333020, acc.: 53.12%] [G loss: 0.580061]\n",
      "epoch:22 step:21543 [D loss: 0.232604, acc.: 62.50%] [G loss: 0.621661]\n",
      "epoch:22 step:21544 [D loss: 0.241661, acc.: 60.16%] [G loss: 0.416470]\n",
      "epoch:22 step:21545 [D loss: 0.293958, acc.: 57.03%] [G loss: 0.378611]\n",
      "epoch:22 step:21546 [D loss: 0.220740, acc.: 68.75%] [G loss: 0.422019]\n",
      "epoch:22 step:21547 [D loss: 0.250580, acc.: 62.50%] [G loss: 0.434479]\n",
      "epoch:22 step:21548 [D loss: 0.178690, acc.: 72.66%] [G loss: 0.458175]\n",
      "epoch:22 step:21549 [D loss: 0.220074, acc.: 66.41%] [G loss: 0.453299]\n",
      "epoch:22 step:21550 [D loss: 0.174131, acc.: 74.22%] [G loss: 0.486694]\n",
      "epoch:22 step:21551 [D loss: 0.193178, acc.: 71.09%] [G loss: 0.556248]\n",
      "epoch:23 step:21552 [D loss: 0.256092, acc.: 57.81%] [G loss: 0.471676]\n",
      "epoch:23 step:21553 [D loss: 0.233240, acc.: 60.94%] [G loss: 0.456898]\n",
      "epoch:23 step:21554 [D loss: 0.228644, acc.: 65.62%] [G loss: 0.450905]\n",
      "epoch:23 step:21555 [D loss: 0.218174, acc.: 67.19%] [G loss: 0.451978]\n",
      "epoch:23 step:21556 [D loss: 0.231586, acc.: 60.94%] [G loss: 0.447271]\n",
      "epoch:23 step:21557 [D loss: 0.198188, acc.: 67.19%] [G loss: 0.426483]\n",
      "epoch:23 step:21558 [D loss: 0.228654, acc.: 59.38%] [G loss: 0.452349]\n",
      "epoch:23 step:21559 [D loss: 0.226909, acc.: 64.06%] [G loss: 0.418315]\n",
      "epoch:23 step:21560 [D loss: 0.209983, acc.: 65.62%] [G loss: 0.400739]\n",
      "epoch:23 step:21561 [D loss: 0.210204, acc.: 63.28%] [G loss: 0.424012]\n",
      "epoch:23 step:21562 [D loss: 0.184476, acc.: 69.53%] [G loss: 0.439385]\n",
      "epoch:23 step:21563 [D loss: 0.236972, acc.: 63.28%] [G loss: 0.454914]\n",
      "epoch:23 step:21564 [D loss: 0.212758, acc.: 65.62%] [G loss: 0.452448]\n",
      "epoch:23 step:21565 [D loss: 0.197743, acc.: 72.66%] [G loss: 0.448392]\n",
      "epoch:23 step:21566 [D loss: 0.200665, acc.: 68.75%] [G loss: 0.448861]\n",
      "epoch:23 step:21567 [D loss: 0.193756, acc.: 68.75%] [G loss: 0.492753]\n",
      "epoch:23 step:21568 [D loss: 0.243421, acc.: 56.25%] [G loss: 0.423106]\n",
      "epoch:23 step:21569 [D loss: 0.207015, acc.: 63.28%] [G loss: 0.422535]\n",
      "epoch:23 step:21570 [D loss: 0.256085, acc.: 53.91%] [G loss: 0.422814]\n",
      "epoch:23 step:21571 [D loss: 0.294938, acc.: 47.66%] [G loss: 0.419246]\n",
      "epoch:23 step:21572 [D loss: 0.226663, acc.: 60.94%] [G loss: 0.459705]\n",
      "epoch:23 step:21573 [D loss: 0.185093, acc.: 71.09%] [G loss: 0.519089]\n",
      "epoch:23 step:21574 [D loss: 0.240230, acc.: 58.59%] [G loss: 0.393510]\n",
      "epoch:23 step:21575 [D loss: 0.223181, acc.: 65.62%] [G loss: 0.382754]\n",
      "epoch:23 step:21576 [D loss: 0.222235, acc.: 63.28%] [G loss: 0.434109]\n",
      "epoch:23 step:21577 [D loss: 0.233973, acc.: 61.72%] [G loss: 0.400537]\n",
      "epoch:23 step:21578 [D loss: 0.243157, acc.: 59.38%] [G loss: 0.428161]\n",
      "epoch:23 step:21579 [D loss: 0.217857, acc.: 60.94%] [G loss: 0.439051]\n",
      "epoch:23 step:21580 [D loss: 0.223559, acc.: 64.84%] [G loss: 0.415855]\n",
      "epoch:23 step:21581 [D loss: 0.220529, acc.: 59.38%] [G loss: 0.443163]\n",
      "epoch:23 step:21582 [D loss: 0.242146, acc.: 54.69%] [G loss: 0.426418]\n",
      "epoch:23 step:21583 [D loss: 0.229297, acc.: 65.62%] [G loss: 0.418818]\n",
      "epoch:23 step:21584 [D loss: 0.225834, acc.: 64.06%] [G loss: 0.432691]\n",
      "epoch:23 step:21585 [D loss: 0.263191, acc.: 54.69%] [G loss: 0.358524]\n",
      "epoch:23 step:21586 [D loss: 0.240828, acc.: 60.94%] [G loss: 0.408883]\n",
      "epoch:23 step:21587 [D loss: 0.232478, acc.: 58.59%] [G loss: 0.401460]\n",
      "epoch:23 step:21588 [D loss: 0.232471, acc.: 63.28%] [G loss: 0.425284]\n",
      "epoch:23 step:21589 [D loss: 0.260437, acc.: 53.91%] [G loss: 0.423456]\n",
      "epoch:23 step:21590 [D loss: 0.204607, acc.: 71.88%] [G loss: 0.453482]\n",
      "epoch:23 step:21591 [D loss: 0.183272, acc.: 69.53%] [G loss: 0.416714]\n",
      "epoch:23 step:21592 [D loss: 0.223665, acc.: 63.28%] [G loss: 0.455391]\n",
      "epoch:23 step:21593 [D loss: 0.205629, acc.: 70.31%] [G loss: 0.429602]\n",
      "epoch:23 step:21594 [D loss: 0.244270, acc.: 61.72%] [G loss: 0.415021]\n",
      "epoch:23 step:21595 [D loss: 0.224686, acc.: 64.06%] [G loss: 0.410712]\n",
      "epoch:23 step:21596 [D loss: 0.242210, acc.: 57.81%] [G loss: 0.416861]\n",
      "epoch:23 step:21597 [D loss: 0.264418, acc.: 52.34%] [G loss: 0.436733]\n",
      "epoch:23 step:21598 [D loss: 0.234359, acc.: 60.16%] [G loss: 0.462152]\n",
      "epoch:23 step:21599 [D loss: 0.209241, acc.: 68.75%] [G loss: 0.451375]\n",
      "epoch:23 step:21600 [D loss: 0.189864, acc.: 71.09%] [G loss: 0.446766]\n",
      "##############\n",
      "[2.57963884 1.87966976 6.02927154 4.75167237 3.62347232 5.83266243\n",
      " 4.67179833 4.52487385 4.43528007 3.91122455]\n",
      "##########\n",
      "epoch:23 step:21601 [D loss: 0.205978, acc.: 69.53%] [G loss: 0.447646]\n",
      "epoch:23 step:21602 [D loss: 0.250302, acc.: 57.81%] [G loss: 0.424784]\n",
      "epoch:23 step:21603 [D loss: 0.243279, acc.: 60.16%] [G loss: 0.424449]\n",
      "epoch:23 step:21604 [D loss: 0.219489, acc.: 69.53%] [G loss: 0.445872]\n",
      "epoch:23 step:21605 [D loss: 0.220067, acc.: 64.84%] [G loss: 0.459882]\n",
      "epoch:23 step:21606 [D loss: 0.218703, acc.: 61.72%] [G loss: 0.415688]\n",
      "epoch:23 step:21607 [D loss: 0.199279, acc.: 65.62%] [G loss: 0.427844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21608 [D loss: 0.253633, acc.: 57.81%] [G loss: 0.419215]\n",
      "epoch:23 step:21609 [D loss: 0.229640, acc.: 64.06%] [G loss: 0.418787]\n",
      "epoch:23 step:21610 [D loss: 0.190059, acc.: 72.66%] [G loss: 0.440437]\n",
      "epoch:23 step:21611 [D loss: 0.235155, acc.: 58.59%] [G loss: 0.392130]\n",
      "epoch:23 step:21612 [D loss: 0.245812, acc.: 61.72%] [G loss: 0.388435]\n",
      "epoch:23 step:21613 [D loss: 0.222566, acc.: 64.84%] [G loss: 0.399775]\n",
      "epoch:23 step:21614 [D loss: 0.205448, acc.: 62.50%] [G loss: 0.412545]\n",
      "epoch:23 step:21615 [D loss: 0.207780, acc.: 67.97%] [G loss: 0.417724]\n",
      "epoch:23 step:21616 [D loss: 0.244565, acc.: 60.16%] [G loss: 0.395452]\n",
      "epoch:23 step:21617 [D loss: 0.228239, acc.: 60.94%] [G loss: 0.434860]\n",
      "epoch:23 step:21618 [D loss: 0.212207, acc.: 61.72%] [G loss: 0.428416]\n",
      "epoch:23 step:21619 [D loss: 0.223876, acc.: 59.38%] [G loss: 0.427380]\n",
      "epoch:23 step:21620 [D loss: 0.197789, acc.: 75.78%] [G loss: 0.443751]\n",
      "epoch:23 step:21621 [D loss: 0.181539, acc.: 78.12%] [G loss: 0.477294]\n",
      "epoch:23 step:21622 [D loss: 0.242855, acc.: 60.16%] [G loss: 0.424125]\n",
      "epoch:23 step:21623 [D loss: 0.228408, acc.: 60.94%] [G loss: 0.409734]\n",
      "epoch:23 step:21624 [D loss: 0.211341, acc.: 64.06%] [G loss: 0.430115]\n",
      "epoch:23 step:21625 [D loss: 0.201783, acc.: 71.88%] [G loss: 0.448894]\n",
      "epoch:23 step:21626 [D loss: 0.235703, acc.: 63.28%] [G loss: 0.409881]\n",
      "epoch:23 step:21627 [D loss: 0.198706, acc.: 67.97%] [G loss: 0.466351]\n",
      "epoch:23 step:21628 [D loss: 0.222141, acc.: 64.84%] [G loss: 0.461686]\n",
      "epoch:23 step:21629 [D loss: 0.272005, acc.: 57.03%] [G loss: 0.443511]\n",
      "epoch:23 step:21630 [D loss: 0.245440, acc.: 54.69%] [G loss: 0.444323]\n",
      "epoch:23 step:21631 [D loss: 0.231765, acc.: 62.50%] [G loss: 0.422504]\n",
      "epoch:23 step:21632 [D loss: 0.219058, acc.: 64.06%] [G loss: 0.414472]\n",
      "epoch:23 step:21633 [D loss: 0.252156, acc.: 58.59%] [G loss: 0.412645]\n",
      "epoch:23 step:21634 [D loss: 0.214159, acc.: 67.19%] [G loss: 0.425658]\n",
      "epoch:23 step:21635 [D loss: 0.226442, acc.: 64.06%] [G loss: 0.441582]\n",
      "epoch:23 step:21636 [D loss: 0.202413, acc.: 70.31%] [G loss: 0.454585]\n",
      "epoch:23 step:21637 [D loss: 0.250048, acc.: 58.59%] [G loss: 0.425559]\n",
      "epoch:23 step:21638 [D loss: 0.234178, acc.: 59.38%] [G loss: 0.412578]\n",
      "epoch:23 step:21639 [D loss: 0.210808, acc.: 64.84%] [G loss: 0.419955]\n",
      "epoch:23 step:21640 [D loss: 0.200173, acc.: 67.97%] [G loss: 0.463936]\n",
      "epoch:23 step:21641 [D loss: 0.240253, acc.: 65.62%] [G loss: 0.417634]\n",
      "epoch:23 step:21642 [D loss: 0.231045, acc.: 60.16%] [G loss: 0.436454]\n",
      "epoch:23 step:21643 [D loss: 0.218335, acc.: 61.72%] [G loss: 0.442122]\n",
      "epoch:23 step:21644 [D loss: 0.208568, acc.: 66.41%] [G loss: 0.420842]\n",
      "epoch:23 step:21645 [D loss: 0.237581, acc.: 57.03%] [G loss: 0.434246]\n",
      "epoch:23 step:21646 [D loss: 0.237956, acc.: 60.16%] [G loss: 0.486177]\n",
      "epoch:23 step:21647 [D loss: 0.209096, acc.: 71.88%] [G loss: 0.437758]\n",
      "epoch:23 step:21648 [D loss: 0.205323, acc.: 70.31%] [G loss: 0.485084]\n",
      "epoch:23 step:21649 [D loss: 0.239306, acc.: 52.34%] [G loss: 0.469996]\n",
      "epoch:23 step:21650 [D loss: 0.262382, acc.: 59.38%] [G loss: 0.435246]\n",
      "epoch:23 step:21651 [D loss: 0.213773, acc.: 63.28%] [G loss: 0.422187]\n",
      "epoch:23 step:21652 [D loss: 0.242418, acc.: 55.47%] [G loss: 0.438017]\n",
      "epoch:23 step:21653 [D loss: 0.237611, acc.: 57.81%] [G loss: 0.405318]\n",
      "epoch:23 step:21654 [D loss: 0.217804, acc.: 61.72%] [G loss: 0.421185]\n",
      "epoch:23 step:21655 [D loss: 0.233669, acc.: 61.72%] [G loss: 0.415467]\n",
      "epoch:23 step:21656 [D loss: 0.206080, acc.: 70.31%] [G loss: 0.444641]\n",
      "epoch:23 step:21657 [D loss: 0.225137, acc.: 62.50%] [G loss: 0.431162]\n",
      "epoch:23 step:21658 [D loss: 0.211270, acc.: 62.50%] [G loss: 0.475797]\n",
      "epoch:23 step:21659 [D loss: 0.257464, acc.: 57.81%] [G loss: 0.456597]\n",
      "epoch:23 step:21660 [D loss: 0.247491, acc.: 60.16%] [G loss: 0.443442]\n",
      "epoch:23 step:21661 [D loss: 0.243678, acc.: 58.59%] [G loss: 0.443188]\n",
      "epoch:23 step:21662 [D loss: 0.214065, acc.: 65.62%] [G loss: 0.402046]\n",
      "epoch:23 step:21663 [D loss: 0.200343, acc.: 72.66%] [G loss: 0.432506]\n",
      "epoch:23 step:21664 [D loss: 0.206630, acc.: 66.41%] [G loss: 0.458396]\n",
      "epoch:23 step:21665 [D loss: 0.213806, acc.: 67.19%] [G loss: 0.421305]\n",
      "epoch:23 step:21666 [D loss: 0.195091, acc.: 65.62%] [G loss: 0.448241]\n",
      "epoch:23 step:21667 [D loss: 0.209859, acc.: 64.06%] [G loss: 0.467549]\n",
      "epoch:23 step:21668 [D loss: 0.211350, acc.: 69.53%] [G loss: 0.470783]\n",
      "epoch:23 step:21669 [D loss: 0.202057, acc.: 67.19%] [G loss: 0.457412]\n",
      "epoch:23 step:21670 [D loss: 0.193721, acc.: 71.88%] [G loss: 0.494204]\n",
      "epoch:23 step:21671 [D loss: 0.235772, acc.: 63.28%] [G loss: 0.486135]\n",
      "epoch:23 step:21672 [D loss: 0.232299, acc.: 64.06%] [G loss: 0.430963]\n",
      "epoch:23 step:21673 [D loss: 0.205098, acc.: 72.66%] [G loss: 0.470787]\n",
      "epoch:23 step:21674 [D loss: 0.212752, acc.: 65.62%] [G loss: 0.423267]\n",
      "epoch:23 step:21675 [D loss: 0.227441, acc.: 63.28%] [G loss: 0.483248]\n",
      "epoch:23 step:21676 [D loss: 0.242563, acc.: 60.16%] [G loss: 0.419515]\n",
      "epoch:23 step:21677 [D loss: 0.231339, acc.: 63.28%] [G loss: 0.439261]\n",
      "epoch:23 step:21678 [D loss: 0.226076, acc.: 64.84%] [G loss: 0.404566]\n",
      "epoch:23 step:21679 [D loss: 0.245483, acc.: 51.56%] [G loss: 0.383603]\n",
      "epoch:23 step:21680 [D loss: 0.219198, acc.: 63.28%] [G loss: 0.389933]\n",
      "epoch:23 step:21681 [D loss: 0.222427, acc.: 62.50%] [G loss: 0.392126]\n",
      "epoch:23 step:21682 [D loss: 0.198372, acc.: 64.06%] [G loss: 0.447261]\n",
      "epoch:23 step:21683 [D loss: 0.221009, acc.: 64.06%] [G loss: 0.402996]\n",
      "epoch:23 step:21684 [D loss: 0.240334, acc.: 57.03%] [G loss: 0.428049]\n",
      "epoch:23 step:21685 [D loss: 0.225992, acc.: 63.28%] [G loss: 0.435277]\n",
      "epoch:23 step:21686 [D loss: 0.221614, acc.: 60.94%] [G loss: 0.473197]\n",
      "epoch:23 step:21687 [D loss: 0.199205, acc.: 68.75%] [G loss: 0.458891]\n",
      "epoch:23 step:21688 [D loss: 0.259714, acc.: 57.81%] [G loss: 0.431144]\n",
      "epoch:23 step:21689 [D loss: 0.251107, acc.: 55.47%] [G loss: 0.402182]\n",
      "epoch:23 step:21690 [D loss: 0.238712, acc.: 64.06%] [G loss: 0.384203]\n",
      "epoch:23 step:21691 [D loss: 0.232899, acc.: 60.94%] [G loss: 0.413643]\n",
      "epoch:23 step:21692 [D loss: 0.234735, acc.: 63.28%] [G loss: 0.367431]\n",
      "epoch:23 step:21693 [D loss: 0.237630, acc.: 60.94%] [G loss: 0.424671]\n",
      "epoch:23 step:21694 [D loss: 0.232344, acc.: 57.81%] [G loss: 0.418325]\n",
      "epoch:23 step:21695 [D loss: 0.221374, acc.: 64.84%] [G loss: 0.400360]\n",
      "epoch:23 step:21696 [D loss: 0.203866, acc.: 71.09%] [G loss: 0.456916]\n",
      "epoch:23 step:21697 [D loss: 0.230259, acc.: 59.38%] [G loss: 0.390074]\n",
      "epoch:23 step:21698 [D loss: 0.228386, acc.: 62.50%] [G loss: 0.402024]\n",
      "epoch:23 step:21699 [D loss: 0.256062, acc.: 53.12%] [G loss: 0.396849]\n",
      "epoch:23 step:21700 [D loss: 0.206573, acc.: 71.88%] [G loss: 0.448384]\n",
      "epoch:23 step:21701 [D loss: 0.235565, acc.: 60.94%] [G loss: 0.421508]\n",
      "epoch:23 step:21702 [D loss: 0.223994, acc.: 61.72%] [G loss: 0.441687]\n",
      "epoch:23 step:21703 [D loss: 0.225096, acc.: 66.41%] [G loss: 0.401288]\n",
      "epoch:23 step:21704 [D loss: 0.271547, acc.: 49.22%] [G loss: 0.381015]\n",
      "epoch:23 step:21705 [D loss: 0.208968, acc.: 66.41%] [G loss: 0.404807]\n",
      "epoch:23 step:21706 [D loss: 0.223834, acc.: 58.59%] [G loss: 0.430135]\n",
      "epoch:23 step:21707 [D loss: 0.219511, acc.: 64.84%] [G loss: 0.431652]\n",
      "epoch:23 step:21708 [D loss: 0.222087, acc.: 61.72%] [G loss: 0.448599]\n",
      "epoch:23 step:21709 [D loss: 0.195146, acc.: 76.56%] [G loss: 0.463013]\n",
      "epoch:23 step:21710 [D loss: 0.226291, acc.: 64.84%] [G loss: 0.404215]\n",
      "epoch:23 step:21711 [D loss: 0.270246, acc.: 51.56%] [G loss: 0.443487]\n",
      "epoch:23 step:21712 [D loss: 0.269675, acc.: 46.09%] [G loss: 0.444970]\n",
      "epoch:23 step:21713 [D loss: 0.245841, acc.: 51.56%] [G loss: 0.419105]\n",
      "epoch:23 step:21714 [D loss: 0.231232, acc.: 64.06%] [G loss: 0.419961]\n",
      "epoch:23 step:21715 [D loss: 0.217860, acc.: 64.84%] [G loss: 0.425420]\n",
      "epoch:23 step:21716 [D loss: 0.215113, acc.: 67.19%] [G loss: 0.382097]\n",
      "epoch:23 step:21717 [D loss: 0.239560, acc.: 60.16%] [G loss: 0.414810]\n",
      "epoch:23 step:21718 [D loss: 0.219939, acc.: 60.94%] [G loss: 0.450714]\n",
      "epoch:23 step:21719 [D loss: 0.217457, acc.: 64.84%] [G loss: 0.443587]\n",
      "epoch:23 step:21720 [D loss: 0.229554, acc.: 59.38%] [G loss: 0.455347]\n",
      "epoch:23 step:21721 [D loss: 0.235212, acc.: 59.38%] [G loss: 0.404743]\n",
      "epoch:23 step:21722 [D loss: 0.249086, acc.: 57.81%] [G loss: 0.420323]\n",
      "epoch:23 step:21723 [D loss: 0.215849, acc.: 64.84%] [G loss: 0.419600]\n",
      "epoch:23 step:21724 [D loss: 0.245579, acc.: 62.50%] [G loss: 0.417500]\n",
      "epoch:23 step:21725 [D loss: 0.253448, acc.: 55.47%] [G loss: 0.430482]\n",
      "epoch:23 step:21726 [D loss: 0.222310, acc.: 60.94%] [G loss: 0.456809]\n",
      "epoch:23 step:21727 [D loss: 0.221178, acc.: 57.81%] [G loss: 0.451195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21728 [D loss: 0.234874, acc.: 60.16%] [G loss: 0.440485]\n",
      "epoch:23 step:21729 [D loss: 0.253934, acc.: 59.38%] [G loss: 0.416179]\n",
      "epoch:23 step:21730 [D loss: 0.217956, acc.: 66.41%] [G loss: 0.415214]\n",
      "epoch:23 step:21731 [D loss: 0.239257, acc.: 58.59%] [G loss: 0.362922]\n",
      "epoch:23 step:21732 [D loss: 0.233729, acc.: 64.84%] [G loss: 0.436279]\n",
      "epoch:23 step:21733 [D loss: 0.213574, acc.: 67.19%] [G loss: 0.428289]\n",
      "epoch:23 step:21734 [D loss: 0.236102, acc.: 59.38%] [G loss: 0.422377]\n",
      "epoch:23 step:21735 [D loss: 0.232911, acc.: 64.06%] [G loss: 0.434134]\n",
      "epoch:23 step:21736 [D loss: 0.211316, acc.: 67.19%] [G loss: 0.407118]\n",
      "epoch:23 step:21737 [D loss: 0.237091, acc.: 63.28%] [G loss: 0.397738]\n",
      "epoch:23 step:21738 [D loss: 0.217784, acc.: 61.72%] [G loss: 0.421066]\n",
      "epoch:23 step:21739 [D loss: 0.245198, acc.: 58.59%] [G loss: 0.386861]\n",
      "epoch:23 step:21740 [D loss: 0.238631, acc.: 60.16%] [G loss: 0.365495]\n",
      "epoch:23 step:21741 [D loss: 0.230476, acc.: 61.72%] [G loss: 0.387138]\n",
      "epoch:23 step:21742 [D loss: 0.197281, acc.: 65.62%] [G loss: 0.417832]\n",
      "epoch:23 step:21743 [D loss: 0.239444, acc.: 54.69%] [G loss: 0.421255]\n",
      "epoch:23 step:21744 [D loss: 0.215364, acc.: 65.62%] [G loss: 0.437360]\n",
      "epoch:23 step:21745 [D loss: 0.222628, acc.: 63.28%] [G loss: 0.429165]\n",
      "epoch:23 step:21746 [D loss: 0.225363, acc.: 67.97%] [G loss: 0.410716]\n",
      "epoch:23 step:21747 [D loss: 0.231961, acc.: 61.72%] [G loss: 0.426089]\n",
      "epoch:23 step:21748 [D loss: 0.217882, acc.: 65.62%] [G loss: 0.434490]\n",
      "epoch:23 step:21749 [D loss: 0.213928, acc.: 68.75%] [G loss: 0.426380]\n",
      "epoch:23 step:21750 [D loss: 0.217901, acc.: 61.72%] [G loss: 0.452134]\n",
      "epoch:23 step:21751 [D loss: 0.260087, acc.: 53.91%] [G loss: 0.394333]\n",
      "epoch:23 step:21752 [D loss: 0.248092, acc.: 56.25%] [G loss: 0.405698]\n",
      "epoch:23 step:21753 [D loss: 0.226609, acc.: 63.28%] [G loss: 0.394286]\n",
      "epoch:23 step:21754 [D loss: 0.286180, acc.: 47.66%] [G loss: 0.384369]\n",
      "epoch:23 step:21755 [D loss: 0.230560, acc.: 56.25%] [G loss: 0.401326]\n",
      "epoch:23 step:21756 [D loss: 0.230550, acc.: 59.38%] [G loss: 0.425321]\n",
      "epoch:23 step:21757 [D loss: 0.227936, acc.: 64.06%] [G loss: 0.422027]\n",
      "epoch:23 step:21758 [D loss: 0.213325, acc.: 66.41%] [G loss: 0.429057]\n",
      "epoch:23 step:21759 [D loss: 0.199068, acc.: 63.28%] [G loss: 0.430300]\n",
      "epoch:23 step:21760 [D loss: 0.177829, acc.: 75.78%] [G loss: 0.465981]\n",
      "epoch:23 step:21761 [D loss: 0.268368, acc.: 60.16%] [G loss: 0.451223]\n",
      "epoch:23 step:21762 [D loss: 0.255849, acc.: 58.59%] [G loss: 0.425288]\n",
      "epoch:23 step:21763 [D loss: 0.233548, acc.: 58.59%] [G loss: 0.383553]\n",
      "epoch:23 step:21764 [D loss: 0.267823, acc.: 53.12%] [G loss: 0.435626]\n",
      "epoch:23 step:21765 [D loss: 0.265230, acc.: 49.22%] [G loss: 0.415253]\n",
      "epoch:23 step:21766 [D loss: 0.248863, acc.: 52.34%] [G loss: 0.396322]\n",
      "epoch:23 step:21767 [D loss: 0.220735, acc.: 61.72%] [G loss: 0.386172]\n",
      "epoch:23 step:21768 [D loss: 0.216791, acc.: 62.50%] [G loss: 0.401145]\n",
      "epoch:23 step:21769 [D loss: 0.198107, acc.: 71.09%] [G loss: 0.422027]\n",
      "epoch:23 step:21770 [D loss: 0.179135, acc.: 77.34%] [G loss: 0.447184]\n",
      "epoch:23 step:21771 [D loss: 0.277060, acc.: 53.91%] [G loss: 0.436573]\n",
      "epoch:23 step:21772 [D loss: 0.217763, acc.: 65.62%] [G loss: 0.431210]\n",
      "epoch:23 step:21773 [D loss: 0.233288, acc.: 57.81%] [G loss: 0.409713]\n",
      "epoch:23 step:21774 [D loss: 0.202782, acc.: 72.66%] [G loss: 0.443847]\n",
      "epoch:23 step:21775 [D loss: 0.227279, acc.: 63.28%] [G loss: 0.428259]\n",
      "epoch:23 step:21776 [D loss: 0.217418, acc.: 66.41%] [G loss: 0.426577]\n",
      "epoch:23 step:21777 [D loss: 0.224078, acc.: 64.84%] [G loss: 0.393169]\n",
      "epoch:23 step:21778 [D loss: 0.224763, acc.: 70.31%] [G loss: 0.379551]\n",
      "epoch:23 step:21779 [D loss: 0.221284, acc.: 62.50%] [G loss: 0.434420]\n",
      "epoch:23 step:21780 [D loss: 0.213792, acc.: 68.75%] [G loss: 0.429297]\n",
      "epoch:23 step:21781 [D loss: 0.208137, acc.: 66.41%] [G loss: 0.441773]\n",
      "epoch:23 step:21782 [D loss: 0.177823, acc.: 77.34%] [G loss: 0.515141]\n",
      "epoch:23 step:21783 [D loss: 0.162087, acc.: 78.91%] [G loss: 0.498450]\n",
      "epoch:23 step:21784 [D loss: 0.242313, acc.: 61.72%] [G loss: 0.460632]\n",
      "epoch:23 step:21785 [D loss: 0.249613, acc.: 63.28%] [G loss: 0.413899]\n",
      "epoch:23 step:21786 [D loss: 0.223699, acc.: 60.16%] [G loss: 0.450420]\n",
      "epoch:23 step:21787 [D loss: 0.206683, acc.: 64.84%] [G loss: 0.448847]\n",
      "epoch:23 step:21788 [D loss: 0.212078, acc.: 65.62%] [G loss: 0.433030]\n",
      "epoch:23 step:21789 [D loss: 0.211919, acc.: 67.97%] [G loss: 0.429693]\n",
      "epoch:23 step:21790 [D loss: 0.224817, acc.: 61.72%] [G loss: 0.417847]\n",
      "epoch:23 step:21791 [D loss: 0.243493, acc.: 58.59%] [G loss: 0.422095]\n",
      "epoch:23 step:21792 [D loss: 0.218768, acc.: 67.19%] [G loss: 0.455740]\n",
      "epoch:23 step:21793 [D loss: 0.214793, acc.: 65.62%] [G loss: 0.468727]\n",
      "epoch:23 step:21794 [D loss: 0.229040, acc.: 60.94%] [G loss: 0.429518]\n",
      "epoch:23 step:21795 [D loss: 0.230037, acc.: 62.50%] [G loss: 0.441434]\n",
      "epoch:23 step:21796 [D loss: 0.193181, acc.: 71.88%] [G loss: 0.461541]\n",
      "epoch:23 step:21797 [D loss: 0.231441, acc.: 60.94%] [G loss: 0.399176]\n",
      "epoch:23 step:21798 [D loss: 0.239900, acc.: 55.47%] [G loss: 0.410964]\n",
      "epoch:23 step:21799 [D loss: 0.202390, acc.: 71.09%] [G loss: 0.428148]\n",
      "epoch:23 step:21800 [D loss: 0.250096, acc.: 52.34%] [G loss: 0.427842]\n",
      "##############\n",
      "[2.60843075 2.0737313  5.91879899 4.68428645 3.91379575 5.52613975\n",
      " 4.57153765 4.79412289 4.43112926 4.21300837]\n",
      "##########\n",
      "epoch:23 step:21801 [D loss: 0.272053, acc.: 51.56%] [G loss: 0.370796]\n",
      "epoch:23 step:21802 [D loss: 0.273946, acc.: 50.78%] [G loss: 0.368526]\n",
      "epoch:23 step:21803 [D loss: 0.232551, acc.: 62.50%] [G loss: 0.427230]\n",
      "epoch:23 step:21804 [D loss: 0.225747, acc.: 65.62%] [G loss: 0.428077]\n",
      "epoch:23 step:21805 [D loss: 0.228348, acc.: 64.06%] [G loss: 0.468600]\n",
      "epoch:23 step:21806 [D loss: 0.243946, acc.: 55.47%] [G loss: 0.445600]\n",
      "epoch:23 step:21807 [D loss: 0.213615, acc.: 60.94%] [G loss: 0.441282]\n",
      "epoch:23 step:21808 [D loss: 0.235890, acc.: 62.50%] [G loss: 0.386371]\n",
      "epoch:23 step:21809 [D loss: 0.218194, acc.: 64.06%] [G loss: 0.397534]\n",
      "epoch:23 step:21810 [D loss: 0.205209, acc.: 68.75%] [G loss: 0.423948]\n",
      "epoch:23 step:21811 [D loss: 0.235712, acc.: 61.72%] [G loss: 0.420509]\n",
      "epoch:23 step:21812 [D loss: 0.215829, acc.: 64.84%] [G loss: 0.422334]\n",
      "epoch:23 step:21813 [D loss: 0.209564, acc.: 65.62%] [G loss: 0.440368]\n",
      "epoch:23 step:21814 [D loss: 0.251468, acc.: 57.81%] [G loss: 0.460412]\n",
      "epoch:23 step:21815 [D loss: 0.202170, acc.: 71.09%] [G loss: 0.443266]\n",
      "epoch:23 step:21816 [D loss: 0.223008, acc.: 64.06%] [G loss: 0.412818]\n",
      "epoch:23 step:21817 [D loss: 0.256846, acc.: 59.38%] [G loss: 0.436253]\n",
      "epoch:23 step:21818 [D loss: 0.203500, acc.: 68.75%] [G loss: 0.469696]\n",
      "epoch:23 step:21819 [D loss: 0.227059, acc.: 60.16%] [G loss: 0.413375]\n",
      "epoch:23 step:21820 [D loss: 0.205255, acc.: 68.75%] [G loss: 0.479499]\n",
      "epoch:23 step:21821 [D loss: 0.213999, acc.: 71.09%] [G loss: 0.409176]\n",
      "epoch:23 step:21822 [D loss: 0.216583, acc.: 64.06%] [G loss: 0.443187]\n",
      "epoch:23 step:21823 [D loss: 0.224182, acc.: 60.94%] [G loss: 0.441617]\n",
      "epoch:23 step:21824 [D loss: 0.216933, acc.: 60.94%] [G loss: 0.410728]\n",
      "epoch:23 step:21825 [D loss: 0.195257, acc.: 69.53%] [G loss: 0.456358]\n",
      "epoch:23 step:21826 [D loss: 0.232717, acc.: 56.25%] [G loss: 0.468728]\n",
      "epoch:23 step:21827 [D loss: 0.199194, acc.: 69.53%] [G loss: 0.465722]\n",
      "epoch:23 step:21828 [D loss: 0.246161, acc.: 57.03%] [G loss: 0.430807]\n",
      "epoch:23 step:21829 [D loss: 0.230646, acc.: 61.72%] [G loss: 0.412241]\n",
      "epoch:23 step:21830 [D loss: 0.239933, acc.: 59.38%] [G loss: 0.409357]\n",
      "epoch:23 step:21831 [D loss: 0.193349, acc.: 68.75%] [G loss: 0.452314]\n",
      "epoch:23 step:21832 [D loss: 0.253403, acc.: 58.59%] [G loss: 0.446972]\n",
      "epoch:23 step:21833 [D loss: 0.236293, acc.: 58.59%] [G loss: 0.395951]\n",
      "epoch:23 step:21834 [D loss: 0.202370, acc.: 72.66%] [G loss: 0.421091]\n",
      "epoch:23 step:21835 [D loss: 0.210319, acc.: 63.28%] [G loss: 0.425878]\n",
      "epoch:23 step:21836 [D loss: 0.234280, acc.: 59.38%] [G loss: 0.405818]\n",
      "epoch:23 step:21837 [D loss: 0.224501, acc.: 67.19%] [G loss: 0.413156]\n",
      "epoch:23 step:21838 [D loss: 0.215027, acc.: 66.41%] [G loss: 0.440816]\n",
      "epoch:23 step:21839 [D loss: 0.221136, acc.: 62.50%] [G loss: 0.455253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21840 [D loss: 0.201834, acc.: 67.19%] [G loss: 0.471160]\n",
      "epoch:23 step:21841 [D loss: 0.209918, acc.: 63.28%] [G loss: 0.460359]\n",
      "epoch:23 step:21842 [D loss: 0.265297, acc.: 55.47%] [G loss: 0.396765]\n",
      "epoch:23 step:21843 [D loss: 0.219751, acc.: 69.53%] [G loss: 0.372226]\n",
      "epoch:23 step:21844 [D loss: 0.227642, acc.: 67.19%] [G loss: 0.389202]\n",
      "epoch:23 step:21845 [D loss: 0.260270, acc.: 51.56%] [G loss: 0.385826]\n",
      "epoch:23 step:21846 [D loss: 0.222662, acc.: 59.38%] [G loss: 0.455173]\n",
      "epoch:23 step:21847 [D loss: 0.193421, acc.: 72.66%] [G loss: 0.419036]\n",
      "epoch:23 step:21848 [D loss: 0.222232, acc.: 60.94%] [G loss: 0.446241]\n",
      "epoch:23 step:21849 [D loss: 0.204007, acc.: 67.19%] [G loss: 0.404800]\n",
      "epoch:23 step:21850 [D loss: 0.197068, acc.: 68.75%] [G loss: 0.465327]\n",
      "epoch:23 step:21851 [D loss: 0.224288, acc.: 61.72%] [G loss: 0.415776]\n",
      "epoch:23 step:21852 [D loss: 0.249939, acc.: 54.69%] [G loss: 0.443136]\n",
      "epoch:23 step:21853 [D loss: 0.228191, acc.: 62.50%] [G loss: 0.416859]\n",
      "epoch:23 step:21854 [D loss: 0.225192, acc.: 61.72%] [G loss: 0.469105]\n",
      "epoch:23 step:21855 [D loss: 0.207943, acc.: 69.53%] [G loss: 0.416890]\n",
      "epoch:23 step:21856 [D loss: 0.200550, acc.: 65.62%] [G loss: 0.426250]\n",
      "epoch:23 step:21857 [D loss: 0.251069, acc.: 51.56%] [G loss: 0.434941]\n",
      "epoch:23 step:21858 [D loss: 0.216595, acc.: 62.50%] [G loss: 0.458353]\n",
      "epoch:23 step:21859 [D loss: 0.221771, acc.: 62.50%] [G loss: 0.408135]\n",
      "epoch:23 step:21860 [D loss: 0.231978, acc.: 66.41%] [G loss: 0.419666]\n",
      "epoch:23 step:21861 [D loss: 0.222654, acc.: 60.94%] [G loss: 0.405800]\n",
      "epoch:23 step:21862 [D loss: 0.203926, acc.: 71.88%] [G loss: 0.447080]\n",
      "epoch:23 step:21863 [D loss: 0.197631, acc.: 70.31%] [G loss: 0.450401]\n",
      "epoch:23 step:21864 [D loss: 0.172942, acc.: 80.47%] [G loss: 0.524945]\n",
      "epoch:23 step:21865 [D loss: 0.205301, acc.: 70.31%] [G loss: 0.492278]\n",
      "epoch:23 step:21866 [D loss: 0.197100, acc.: 71.09%] [G loss: 0.484238]\n",
      "epoch:23 step:21867 [D loss: 0.274589, acc.: 49.22%] [G loss: 0.441387]\n",
      "epoch:23 step:21868 [D loss: 0.219338, acc.: 59.38%] [G loss: 0.433174]\n",
      "epoch:23 step:21869 [D loss: 0.226188, acc.: 64.06%] [G loss: 0.401954]\n",
      "epoch:23 step:21870 [D loss: 0.218773, acc.: 62.50%] [G loss: 0.374157]\n",
      "epoch:23 step:21871 [D loss: 0.220470, acc.: 64.06%] [G loss: 0.426027]\n",
      "epoch:23 step:21872 [D loss: 0.220549, acc.: 62.50%] [G loss: 0.391673]\n",
      "epoch:23 step:21873 [D loss: 0.234300, acc.: 61.72%] [G loss: 0.416562]\n",
      "epoch:23 step:21874 [D loss: 0.241754, acc.: 59.38%] [G loss: 0.427128]\n",
      "epoch:23 step:21875 [D loss: 0.214227, acc.: 68.75%] [G loss: 0.398930]\n",
      "epoch:23 step:21876 [D loss: 0.207349, acc.: 71.09%] [G loss: 0.425766]\n",
      "epoch:23 step:21877 [D loss: 0.221986, acc.: 64.84%] [G loss: 0.417114]\n",
      "epoch:23 step:21878 [D loss: 0.230907, acc.: 62.50%] [G loss: 0.434870]\n",
      "epoch:23 step:21879 [D loss: 0.214396, acc.: 67.19%] [G loss: 0.452875]\n",
      "epoch:23 step:21880 [D loss: 0.236277, acc.: 56.25%] [G loss: 0.445956]\n",
      "epoch:23 step:21881 [D loss: 0.219563, acc.: 65.62%] [G loss: 0.423063]\n",
      "epoch:23 step:21882 [D loss: 0.219831, acc.: 66.41%] [G loss: 0.444561]\n",
      "epoch:23 step:21883 [D loss: 0.223479, acc.: 62.50%] [G loss: 0.426144]\n",
      "epoch:23 step:21884 [D loss: 0.215994, acc.: 64.06%] [G loss: 0.449192]\n",
      "epoch:23 step:21885 [D loss: 0.212413, acc.: 67.97%] [G loss: 0.455735]\n",
      "epoch:23 step:21886 [D loss: 0.233382, acc.: 59.38%] [G loss: 0.439692]\n",
      "epoch:23 step:21887 [D loss: 0.218017, acc.: 65.62%] [G loss: 0.439792]\n",
      "epoch:23 step:21888 [D loss: 0.222093, acc.: 65.62%] [G loss: 0.441037]\n",
      "epoch:23 step:21889 [D loss: 0.236867, acc.: 59.38%] [G loss: 0.440242]\n",
      "epoch:23 step:21890 [D loss: 0.214677, acc.: 71.09%] [G loss: 0.426956]\n",
      "epoch:23 step:21891 [D loss: 0.218606, acc.: 62.50%] [G loss: 0.461576]\n",
      "epoch:23 step:21892 [D loss: 0.275652, acc.: 54.69%] [G loss: 0.434797]\n",
      "epoch:23 step:21893 [D loss: 0.238350, acc.: 53.91%] [G loss: 0.459979]\n",
      "epoch:23 step:21894 [D loss: 0.241089, acc.: 61.72%] [G loss: 0.481411]\n",
      "epoch:23 step:21895 [D loss: 0.227506, acc.: 67.19%] [G loss: 0.445070]\n",
      "epoch:23 step:21896 [D loss: 0.205954, acc.: 65.62%] [G loss: 0.450765]\n",
      "epoch:23 step:21897 [D loss: 0.174712, acc.: 78.91%] [G loss: 0.473703]\n",
      "epoch:23 step:21898 [D loss: 0.203663, acc.: 64.84%] [G loss: 0.489971]\n",
      "epoch:23 step:21899 [D loss: 0.236433, acc.: 67.19%] [G loss: 0.438858]\n",
      "epoch:23 step:21900 [D loss: 0.297859, acc.: 43.75%] [G loss: 0.390584]\n",
      "epoch:23 step:21901 [D loss: 0.207376, acc.: 68.75%] [G loss: 0.439268]\n",
      "epoch:23 step:21902 [D loss: 0.246225, acc.: 58.59%] [G loss: 0.405956]\n",
      "epoch:23 step:21903 [D loss: 0.221098, acc.: 59.38%] [G loss: 0.431330]\n",
      "epoch:23 step:21904 [D loss: 0.226244, acc.: 60.16%] [G loss: 0.419077]\n",
      "epoch:23 step:21905 [D loss: 0.167976, acc.: 75.00%] [G loss: 0.472507]\n",
      "epoch:23 step:21906 [D loss: 0.228939, acc.: 64.84%] [G loss: 0.432124]\n",
      "epoch:23 step:21907 [D loss: 0.230138, acc.: 64.06%] [G loss: 0.411203]\n",
      "epoch:23 step:21908 [D loss: 0.203864, acc.: 63.28%] [G loss: 0.444359]\n",
      "epoch:23 step:21909 [D loss: 0.231267, acc.: 58.59%] [G loss: 0.438026]\n",
      "epoch:23 step:21910 [D loss: 0.191065, acc.: 75.00%] [G loss: 0.471641]\n",
      "epoch:23 step:21911 [D loss: 0.208999, acc.: 66.41%] [G loss: 0.440434]\n",
      "epoch:23 step:21912 [D loss: 0.210656, acc.: 71.88%] [G loss: 0.445106]\n",
      "epoch:23 step:21913 [D loss: 0.252864, acc.: 53.12%] [G loss: 0.393670]\n",
      "epoch:23 step:21914 [D loss: 0.227552, acc.: 60.94%] [G loss: 0.398907]\n",
      "epoch:23 step:21915 [D loss: 0.224646, acc.: 64.06%] [G loss: 0.416278]\n",
      "epoch:23 step:21916 [D loss: 0.218317, acc.: 61.72%] [G loss: 0.407854]\n",
      "epoch:23 step:21917 [D loss: 0.218659, acc.: 69.53%] [G loss: 0.407509]\n",
      "epoch:23 step:21918 [D loss: 0.221154, acc.: 68.75%] [G loss: 0.473071]\n",
      "epoch:23 step:21919 [D loss: 0.248832, acc.: 57.03%] [G loss: 0.420005]\n",
      "epoch:23 step:21920 [D loss: 0.238504, acc.: 58.59%] [G loss: 0.396447]\n",
      "epoch:23 step:21921 [D loss: 0.208410, acc.: 67.19%] [G loss: 0.448816]\n",
      "epoch:23 step:21922 [D loss: 0.197198, acc.: 71.88%] [G loss: 0.435944]\n",
      "epoch:23 step:21923 [D loss: 0.229867, acc.: 60.16%] [G loss: 0.459695]\n",
      "epoch:23 step:21924 [D loss: 0.261755, acc.: 53.91%] [G loss: 0.399393]\n",
      "epoch:23 step:21925 [D loss: 0.216068, acc.: 71.88%] [G loss: 0.419134]\n",
      "epoch:23 step:21926 [D loss: 0.231803, acc.: 63.28%] [G loss: 0.421646]\n",
      "epoch:23 step:21927 [D loss: 0.246133, acc.: 55.47%] [G loss: 0.458084]\n",
      "epoch:23 step:21928 [D loss: 0.256738, acc.: 53.12%] [G loss: 0.388819]\n",
      "epoch:23 step:21929 [D loss: 0.225572, acc.: 64.06%] [G loss: 0.417265]\n",
      "epoch:23 step:21930 [D loss: 0.208872, acc.: 64.06%] [G loss: 0.454876]\n",
      "epoch:23 step:21931 [D loss: 0.243399, acc.: 54.69%] [G loss: 0.411598]\n",
      "epoch:23 step:21932 [D loss: 0.208211, acc.: 67.97%] [G loss: 0.454727]\n",
      "epoch:23 step:21933 [D loss: 0.207764, acc.: 71.88%] [G loss: 0.447473]\n",
      "epoch:23 step:21934 [D loss: 0.223735, acc.: 64.84%] [G loss: 0.386229]\n",
      "epoch:23 step:21935 [D loss: 0.206977, acc.: 66.41%] [G loss: 0.418113]\n",
      "epoch:23 step:21936 [D loss: 0.203394, acc.: 64.84%] [G loss: 0.415344]\n",
      "epoch:23 step:21937 [D loss: 0.256415, acc.: 53.12%] [G loss: 0.426391]\n",
      "epoch:23 step:21938 [D loss: 0.224124, acc.: 59.38%] [G loss: 0.461651]\n",
      "epoch:23 step:21939 [D loss: 0.200191, acc.: 64.06%] [G loss: 0.438216]\n",
      "epoch:23 step:21940 [D loss: 0.245650, acc.: 57.81%] [G loss: 0.394898]\n",
      "epoch:23 step:21941 [D loss: 0.251680, acc.: 60.16%] [G loss: 0.417182]\n",
      "epoch:23 step:21942 [D loss: 0.214507, acc.: 69.53%] [G loss: 0.415085]\n",
      "epoch:23 step:21943 [D loss: 0.226510, acc.: 60.94%] [G loss: 0.453326]\n",
      "epoch:23 step:21944 [D loss: 0.240578, acc.: 63.28%] [G loss: 0.389065]\n",
      "epoch:23 step:21945 [D loss: 0.220196, acc.: 65.62%] [G loss: 0.414654]\n",
      "epoch:23 step:21946 [D loss: 0.224847, acc.: 63.28%] [G loss: 0.408914]\n",
      "epoch:23 step:21947 [D loss: 0.233763, acc.: 54.69%] [G loss: 0.454488]\n",
      "epoch:23 step:21948 [D loss: 0.218307, acc.: 60.94%] [G loss: 0.466737]\n",
      "epoch:23 step:21949 [D loss: 0.203758, acc.: 68.75%] [G loss: 0.486340]\n",
      "epoch:23 step:21950 [D loss: 0.188063, acc.: 75.00%] [G loss: 0.492254]\n",
      "epoch:23 step:21951 [D loss: 0.266865, acc.: 53.12%] [G loss: 0.384124]\n",
      "epoch:23 step:21952 [D loss: 0.228109, acc.: 64.84%] [G loss: 0.404832]\n",
      "epoch:23 step:21953 [D loss: 0.256112, acc.: 59.38%] [G loss: 0.394166]\n",
      "epoch:23 step:21954 [D loss: 0.221326, acc.: 68.75%] [G loss: 0.424698]\n",
      "epoch:23 step:21955 [D loss: 0.228831, acc.: 66.41%] [G loss: 0.422106]\n",
      "epoch:23 step:21956 [D loss: 0.218270, acc.: 64.84%] [G loss: 0.461173]\n",
      "epoch:23 step:21957 [D loss: 0.196024, acc.: 69.53%] [G loss: 0.456803]\n",
      "epoch:23 step:21958 [D loss: 0.241366, acc.: 62.50%] [G loss: 0.441480]\n",
      "epoch:23 step:21959 [D loss: 0.251773, acc.: 56.25%] [G loss: 0.431633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21960 [D loss: 0.221848, acc.: 64.06%] [G loss: 0.451647]\n",
      "epoch:23 step:21961 [D loss: 0.254559, acc.: 59.38%] [G loss: 0.422641]\n",
      "epoch:23 step:21962 [D loss: 0.252112, acc.: 57.03%] [G loss: 0.412767]\n",
      "epoch:23 step:21963 [D loss: 0.228747, acc.: 60.16%] [G loss: 0.413790]\n",
      "epoch:23 step:21964 [D loss: 0.235928, acc.: 60.94%] [G loss: 0.444610]\n",
      "epoch:23 step:21965 [D loss: 0.233081, acc.: 61.72%] [G loss: 0.435209]\n",
      "epoch:23 step:21966 [D loss: 0.229509, acc.: 57.03%] [G loss: 0.425298]\n",
      "epoch:23 step:21967 [D loss: 0.185260, acc.: 76.56%] [G loss: 0.479038]\n",
      "epoch:23 step:21968 [D loss: 0.235674, acc.: 59.38%] [G loss: 0.411217]\n",
      "epoch:23 step:21969 [D loss: 0.259046, acc.: 56.25%] [G loss: 0.423866]\n",
      "epoch:23 step:21970 [D loss: 0.252520, acc.: 56.25%] [G loss: 0.406208]\n",
      "epoch:23 step:21971 [D loss: 0.240008, acc.: 60.16%] [G loss: 0.413479]\n",
      "epoch:23 step:21972 [D loss: 0.263419, acc.: 53.91%] [G loss: 0.421333]\n",
      "epoch:23 step:21973 [D loss: 0.236430, acc.: 60.94%] [G loss: 0.417568]\n",
      "epoch:23 step:21974 [D loss: 0.240092, acc.: 60.16%] [G loss: 0.405062]\n",
      "epoch:23 step:21975 [D loss: 0.233500, acc.: 58.59%] [G loss: 0.408563]\n",
      "epoch:23 step:21976 [D loss: 0.232381, acc.: 60.16%] [G loss: 0.414083]\n",
      "epoch:23 step:21977 [D loss: 0.216060, acc.: 64.84%] [G loss: 0.427955]\n",
      "epoch:23 step:21978 [D loss: 0.214018, acc.: 67.97%] [G loss: 0.417859]\n",
      "epoch:23 step:21979 [D loss: 0.218224, acc.: 61.72%] [G loss: 0.444729]\n",
      "epoch:23 step:21980 [D loss: 0.185479, acc.: 73.44%] [G loss: 0.445213]\n",
      "epoch:23 step:21981 [D loss: 0.214294, acc.: 69.53%] [G loss: 0.487660]\n",
      "epoch:23 step:21982 [D loss: 0.228939, acc.: 61.72%] [G loss: 0.460396]\n",
      "epoch:23 step:21983 [D loss: 0.225941, acc.: 64.06%] [G loss: 0.425347]\n",
      "epoch:23 step:21984 [D loss: 0.215205, acc.: 67.19%] [G loss: 0.413626]\n",
      "epoch:23 step:21985 [D loss: 0.181850, acc.: 76.56%] [G loss: 0.460667]\n",
      "epoch:23 step:21986 [D loss: 0.194669, acc.: 69.53%] [G loss: 0.448698]\n",
      "epoch:23 step:21987 [D loss: 0.203442, acc.: 65.62%] [G loss: 0.455577]\n",
      "epoch:23 step:21988 [D loss: 0.277628, acc.: 52.34%] [G loss: 0.483332]\n",
      "epoch:23 step:21989 [D loss: 0.225227, acc.: 61.72%] [G loss: 0.458408]\n",
      "epoch:23 step:21990 [D loss: 0.206883, acc.: 67.97%] [G loss: 0.458483]\n",
      "epoch:23 step:21991 [D loss: 0.218598, acc.: 63.28%] [G loss: 0.442953]\n",
      "epoch:23 step:21992 [D loss: 0.241046, acc.: 61.72%] [G loss: 0.423231]\n",
      "epoch:23 step:21993 [D loss: 0.235416, acc.: 60.16%] [G loss: 0.431917]\n",
      "epoch:23 step:21994 [D loss: 0.246107, acc.: 53.12%] [G loss: 0.403000]\n",
      "epoch:23 step:21995 [D loss: 0.239614, acc.: 61.72%] [G loss: 0.410812]\n",
      "epoch:23 step:21996 [D loss: 0.214872, acc.: 64.84%] [G loss: 0.468462]\n",
      "epoch:23 step:21997 [D loss: 0.231316, acc.: 66.41%] [G loss: 0.455855]\n",
      "epoch:23 step:21998 [D loss: 0.222724, acc.: 62.50%] [G loss: 0.435056]\n",
      "epoch:23 step:21999 [D loss: 0.234225, acc.: 58.59%] [G loss: 0.483769]\n",
      "epoch:23 step:22000 [D loss: 0.210720, acc.: 63.28%] [G loss: 0.448915]\n",
      "##############\n",
      "[2.64707366 2.12809379 6.24534614 4.67188072 3.72381146 5.47611968\n",
      " 4.39725315 4.64333051 4.25466965 3.8561592 ]\n",
      "##########\n",
      "epoch:23 step:22001 [D loss: 0.216870, acc.: 62.50%] [G loss: 0.404012]\n",
      "epoch:23 step:22002 [D loss: 0.208496, acc.: 68.75%] [G loss: 0.409290]\n",
      "epoch:23 step:22003 [D loss: 0.200003, acc.: 70.31%] [G loss: 0.451525]\n",
      "epoch:23 step:22004 [D loss: 0.231435, acc.: 60.94%] [G loss: 0.446417]\n",
      "epoch:23 step:22005 [D loss: 0.211950, acc.: 65.62%] [G loss: 0.502056]\n",
      "epoch:23 step:22006 [D loss: 0.230382, acc.: 57.03%] [G loss: 0.439759]\n",
      "epoch:23 step:22007 [D loss: 0.227735, acc.: 61.72%] [G loss: 0.440909]\n",
      "epoch:23 step:22008 [D loss: 0.225488, acc.: 66.41%] [G loss: 0.454921]\n",
      "epoch:23 step:22009 [D loss: 0.261467, acc.: 51.56%] [G loss: 0.451837]\n",
      "epoch:23 step:22010 [D loss: 0.245117, acc.: 60.94%] [G loss: 0.457410]\n",
      "epoch:23 step:22011 [D loss: 0.244812, acc.: 59.38%] [G loss: 0.417221]\n",
      "epoch:23 step:22012 [D loss: 0.218285, acc.: 66.41%] [G loss: 0.443354]\n",
      "epoch:23 step:22013 [D loss: 0.228748, acc.: 63.28%] [G loss: 0.407781]\n",
      "epoch:23 step:22014 [D loss: 0.232972, acc.: 60.94%] [G loss: 0.412208]\n",
      "epoch:23 step:22015 [D loss: 0.234124, acc.: 62.50%] [G loss: 0.423494]\n",
      "epoch:23 step:22016 [D loss: 0.231708, acc.: 62.50%] [G loss: 0.399211]\n",
      "epoch:23 step:22017 [D loss: 0.228312, acc.: 58.59%] [G loss: 0.410687]\n",
      "epoch:23 step:22018 [D loss: 0.214618, acc.: 61.72%] [G loss: 0.423603]\n",
      "epoch:23 step:22019 [D loss: 0.236006, acc.: 61.72%] [G loss: 0.424211]\n",
      "epoch:23 step:22020 [D loss: 0.208560, acc.: 65.62%] [G loss: 0.444793]\n",
      "epoch:23 step:22021 [D loss: 0.206270, acc.: 64.84%] [G loss: 0.455928]\n",
      "epoch:23 step:22022 [D loss: 0.182389, acc.: 75.00%] [G loss: 0.452839]\n",
      "epoch:23 step:22023 [D loss: 0.197152, acc.: 66.41%] [G loss: 0.470861]\n",
      "epoch:23 step:22024 [D loss: 0.245551, acc.: 60.94%] [G loss: 0.458087]\n",
      "epoch:23 step:22025 [D loss: 0.201973, acc.: 71.09%] [G loss: 0.464498]\n",
      "epoch:23 step:22026 [D loss: 0.222105, acc.: 63.28%] [G loss: 0.460602]\n",
      "epoch:23 step:22027 [D loss: 0.227832, acc.: 64.84%] [G loss: 0.470720]\n",
      "epoch:23 step:22028 [D loss: 0.258697, acc.: 48.44%] [G loss: 0.425972]\n",
      "epoch:23 step:22029 [D loss: 0.236159, acc.: 58.59%] [G loss: 0.390358]\n",
      "epoch:23 step:22030 [D loss: 0.235537, acc.: 63.28%] [G loss: 0.365691]\n",
      "epoch:23 step:22031 [D loss: 0.225750, acc.: 58.59%] [G loss: 0.394687]\n",
      "epoch:23 step:22032 [D loss: 0.164670, acc.: 80.47%] [G loss: 0.445133]\n",
      "epoch:23 step:22033 [D loss: 0.246713, acc.: 57.03%] [G loss: 0.438033]\n",
      "epoch:23 step:22034 [D loss: 0.225783, acc.: 59.38%] [G loss: 0.446992]\n",
      "epoch:23 step:22035 [D loss: 0.209759, acc.: 64.84%] [G loss: 0.490240]\n",
      "epoch:23 step:22036 [D loss: 0.216406, acc.: 67.97%] [G loss: 0.487725]\n",
      "epoch:23 step:22037 [D loss: 0.239927, acc.: 60.16%] [G loss: 0.469597]\n",
      "epoch:23 step:22038 [D loss: 0.237588, acc.: 57.03%] [G loss: 0.412069]\n",
      "epoch:23 step:22039 [D loss: 0.189322, acc.: 75.00%] [G loss: 0.432603]\n",
      "epoch:23 step:22040 [D loss: 0.251692, acc.: 54.69%] [G loss: 0.443178]\n",
      "epoch:23 step:22041 [D loss: 0.242590, acc.: 61.72%] [G loss: 0.428769]\n",
      "epoch:23 step:22042 [D loss: 0.228725, acc.: 64.84%] [G loss: 0.433136]\n",
      "epoch:23 step:22043 [D loss: 0.252991, acc.: 56.25%] [G loss: 0.424939]\n",
      "epoch:23 step:22044 [D loss: 0.211852, acc.: 63.28%] [G loss: 0.406278]\n",
      "epoch:23 step:22045 [D loss: 0.217014, acc.: 62.50%] [G loss: 0.460862]\n",
      "epoch:23 step:22046 [D loss: 0.210994, acc.: 67.97%] [G loss: 0.427792]\n",
      "epoch:23 step:22047 [D loss: 0.203052, acc.: 70.31%] [G loss: 0.480548]\n",
      "epoch:23 step:22048 [D loss: 0.212780, acc.: 65.62%] [G loss: 0.450111]\n",
      "epoch:23 step:22049 [D loss: 0.203263, acc.: 69.53%] [G loss: 0.461782]\n",
      "epoch:23 step:22050 [D loss: 0.193706, acc.: 73.44%] [G loss: 0.447447]\n",
      "epoch:23 step:22051 [D loss: 0.277339, acc.: 53.12%] [G loss: 0.429749]\n",
      "epoch:23 step:22052 [D loss: 0.270163, acc.: 53.12%] [G loss: 0.392965]\n",
      "epoch:23 step:22053 [D loss: 0.224616, acc.: 58.59%] [G loss: 0.425368]\n",
      "epoch:23 step:22054 [D loss: 0.231221, acc.: 60.16%] [G loss: 0.391235]\n",
      "epoch:23 step:22055 [D loss: 0.190795, acc.: 75.00%] [G loss: 0.416774]\n",
      "epoch:23 step:22056 [D loss: 0.213377, acc.: 64.06%] [G loss: 0.440712]\n",
      "epoch:23 step:22057 [D loss: 0.234709, acc.: 59.38%] [G loss: 0.438488]\n",
      "epoch:23 step:22058 [D loss: 0.200642, acc.: 65.62%] [G loss: 0.424169]\n",
      "epoch:23 step:22059 [D loss: 0.184341, acc.: 70.31%] [G loss: 0.488476]\n",
      "epoch:23 step:22060 [D loss: 0.226983, acc.: 64.06%] [G loss: 0.434621]\n",
      "epoch:23 step:22061 [D loss: 0.241311, acc.: 64.84%] [G loss: 0.419263]\n",
      "epoch:23 step:22062 [D loss: 0.233397, acc.: 61.72%] [G loss: 0.420329]\n",
      "epoch:23 step:22063 [D loss: 0.241612, acc.: 51.56%] [G loss: 0.434206]\n",
      "epoch:23 step:22064 [D loss: 0.215263, acc.: 61.72%] [G loss: 0.455006]\n",
      "epoch:23 step:22065 [D loss: 0.201839, acc.: 67.19%] [G loss: 0.435942]\n",
      "epoch:23 step:22066 [D loss: 0.203308, acc.: 70.31%] [G loss: 0.454931]\n",
      "epoch:23 step:22067 [D loss: 0.206490, acc.: 67.97%] [G loss: 0.466143]\n",
      "epoch:23 step:22068 [D loss: 0.209894, acc.: 67.19%] [G loss: 0.496941]\n",
      "epoch:23 step:22069 [D loss: 0.243852, acc.: 59.38%] [G loss: 0.411932]\n",
      "epoch:23 step:22070 [D loss: 0.199250, acc.: 68.75%] [G loss: 0.426119]\n",
      "epoch:23 step:22071 [D loss: 0.200442, acc.: 67.19%] [G loss: 0.454727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22072 [D loss: 0.220377, acc.: 66.41%] [G loss: 0.435861]\n",
      "epoch:23 step:22073 [D loss: 0.190778, acc.: 72.66%] [G loss: 0.460387]\n",
      "epoch:23 step:22074 [D loss: 0.199195, acc.: 63.28%] [G loss: 0.441180]\n",
      "epoch:23 step:22075 [D loss: 0.218085, acc.: 67.19%] [G loss: 0.421212]\n",
      "epoch:23 step:22076 [D loss: 0.227631, acc.: 60.16%] [G loss: 0.417154]\n",
      "epoch:23 step:22077 [D loss: 0.218585, acc.: 67.19%] [G loss: 0.415774]\n",
      "epoch:23 step:22078 [D loss: 0.239393, acc.: 57.03%] [G loss: 0.453206]\n",
      "epoch:23 step:22079 [D loss: 0.259082, acc.: 57.81%] [G loss: 0.449713]\n",
      "epoch:23 step:22080 [D loss: 0.249906, acc.: 57.03%] [G loss: 0.400658]\n",
      "epoch:23 step:22081 [D loss: 0.214091, acc.: 66.41%] [G loss: 0.462947]\n",
      "epoch:23 step:22082 [D loss: 0.255759, acc.: 58.59%] [G loss: 0.433692]\n",
      "epoch:23 step:22083 [D loss: 0.252855, acc.: 56.25%] [G loss: 0.394134]\n",
      "epoch:23 step:22084 [D loss: 0.230352, acc.: 60.16%] [G loss: 0.415828]\n",
      "epoch:23 step:22085 [D loss: 0.192034, acc.: 69.53%] [G loss: 0.523572]\n",
      "epoch:23 step:22086 [D loss: 0.249026, acc.: 55.47%] [G loss: 0.422653]\n",
      "epoch:23 step:22087 [D loss: 0.225184, acc.: 58.59%] [G loss: 0.415014]\n",
      "epoch:23 step:22088 [D loss: 0.230416, acc.: 59.38%] [G loss: 0.432682]\n",
      "epoch:23 step:22089 [D loss: 0.244946, acc.: 58.59%] [G loss: 0.403911]\n",
      "epoch:23 step:22090 [D loss: 0.194677, acc.: 68.75%] [G loss: 0.417249]\n",
      "epoch:23 step:22091 [D loss: 0.235270, acc.: 61.72%] [G loss: 0.416145]\n",
      "epoch:23 step:22092 [D loss: 0.229014, acc.: 61.72%] [G loss: 0.399728]\n",
      "epoch:23 step:22093 [D loss: 0.273836, acc.: 46.09%] [G loss: 0.410793]\n",
      "epoch:23 step:22094 [D loss: 0.240405, acc.: 58.59%] [G loss: 0.442369]\n",
      "epoch:23 step:22095 [D loss: 0.213562, acc.: 67.19%] [G loss: 0.441505]\n",
      "epoch:23 step:22096 [D loss: 0.227251, acc.: 59.38%] [G loss: 0.431935]\n",
      "epoch:23 step:22097 [D loss: 0.227634, acc.: 61.72%] [G loss: 0.454463]\n",
      "epoch:23 step:22098 [D loss: 0.229664, acc.: 65.62%] [G loss: 0.465098]\n",
      "epoch:23 step:22099 [D loss: 0.224848, acc.: 60.94%] [G loss: 0.446717]\n",
      "epoch:23 step:22100 [D loss: 0.218235, acc.: 64.84%] [G loss: 0.453355]\n",
      "epoch:23 step:22101 [D loss: 0.198960, acc.: 71.09%] [G loss: 0.468216]\n",
      "epoch:23 step:22102 [D loss: 0.200815, acc.: 70.31%] [G loss: 0.460072]\n",
      "epoch:23 step:22103 [D loss: 0.217885, acc.: 66.41%] [G loss: 0.415358]\n",
      "epoch:23 step:22104 [D loss: 0.225066, acc.: 62.50%] [G loss: 0.445831]\n",
      "epoch:23 step:22105 [D loss: 0.187391, acc.: 72.66%] [G loss: 0.439037]\n",
      "epoch:23 step:22106 [D loss: 0.209960, acc.: 69.53%] [G loss: 0.443984]\n",
      "epoch:23 step:22107 [D loss: 0.233515, acc.: 64.06%] [G loss: 0.466286]\n",
      "epoch:23 step:22108 [D loss: 0.213707, acc.: 63.28%] [G loss: 0.461266]\n",
      "epoch:23 step:22109 [D loss: 0.228622, acc.: 64.06%] [G loss: 0.429935]\n",
      "epoch:23 step:22110 [D loss: 0.252614, acc.: 56.25%] [G loss: 0.420234]\n",
      "epoch:23 step:22111 [D loss: 0.224414, acc.: 60.16%] [G loss: 0.416401]\n",
      "epoch:23 step:22112 [D loss: 0.227390, acc.: 64.06%] [G loss: 0.417026]\n",
      "epoch:23 step:22113 [D loss: 0.204550, acc.: 68.75%] [G loss: 0.413473]\n",
      "epoch:23 step:22114 [D loss: 0.188016, acc.: 74.22%] [G loss: 0.439504]\n",
      "epoch:23 step:22115 [D loss: 0.196558, acc.: 66.41%] [G loss: 0.451691]\n",
      "epoch:23 step:22116 [D loss: 0.229089, acc.: 62.50%] [G loss: 0.459049]\n",
      "epoch:23 step:22117 [D loss: 0.249812, acc.: 56.25%] [G loss: 0.477011]\n",
      "epoch:23 step:22118 [D loss: 0.219306, acc.: 67.19%] [G loss: 0.519478]\n",
      "epoch:23 step:22119 [D loss: 0.218177, acc.: 67.19%] [G loss: 0.476196]\n",
      "epoch:23 step:22120 [D loss: 0.266390, acc.: 53.91%] [G loss: 0.399199]\n",
      "epoch:23 step:22121 [D loss: 0.208442, acc.: 67.97%] [G loss: 0.364675]\n",
      "epoch:23 step:22122 [D loss: 0.208973, acc.: 66.41%] [G loss: 0.385568]\n",
      "epoch:23 step:22123 [D loss: 0.226435, acc.: 64.84%] [G loss: 0.398660]\n",
      "epoch:23 step:22124 [D loss: 0.230822, acc.: 60.16%] [G loss: 0.391665]\n",
      "epoch:23 step:22125 [D loss: 0.188444, acc.: 75.78%] [G loss: 0.476716]\n",
      "epoch:23 step:22126 [D loss: 0.189608, acc.: 73.44%] [G loss: 0.479022]\n",
      "epoch:23 step:22127 [D loss: 0.226730, acc.: 58.59%] [G loss: 0.446182]\n",
      "epoch:23 step:22128 [D loss: 0.212658, acc.: 65.62%] [G loss: 0.417343]\n",
      "epoch:23 step:22129 [D loss: 0.228895, acc.: 60.16%] [G loss: 0.484808]\n",
      "epoch:23 step:22130 [D loss: 0.228988, acc.: 56.25%] [G loss: 0.431522]\n",
      "epoch:23 step:22131 [D loss: 0.231232, acc.: 60.94%] [G loss: 0.430671]\n",
      "epoch:23 step:22132 [D loss: 0.215485, acc.: 69.53%] [G loss: 0.444776]\n",
      "epoch:23 step:22133 [D loss: 0.217093, acc.: 61.72%] [G loss: 0.438641]\n",
      "epoch:23 step:22134 [D loss: 0.234145, acc.: 57.81%] [G loss: 0.431702]\n",
      "epoch:23 step:22135 [D loss: 0.219130, acc.: 64.06%] [G loss: 0.408542]\n",
      "epoch:23 step:22136 [D loss: 0.215953, acc.: 62.50%] [G loss: 0.378740]\n",
      "epoch:23 step:22137 [D loss: 0.210806, acc.: 66.41%] [G loss: 0.394717]\n",
      "epoch:23 step:22138 [D loss: 0.240635, acc.: 58.59%] [G loss: 0.407388]\n",
      "epoch:23 step:22139 [D loss: 0.257677, acc.: 53.12%] [G loss: 0.401118]\n",
      "epoch:23 step:22140 [D loss: 0.205882, acc.: 64.84%] [G loss: 0.405195]\n",
      "epoch:23 step:22141 [D loss: 0.243024, acc.: 57.03%] [G loss: 0.411951]\n",
      "epoch:23 step:22142 [D loss: 0.223264, acc.: 63.28%] [G loss: 0.458058]\n",
      "epoch:23 step:22143 [D loss: 0.213772, acc.: 67.97%] [G loss: 0.452576]\n",
      "epoch:23 step:22144 [D loss: 0.218746, acc.: 61.72%] [G loss: 0.441288]\n",
      "epoch:23 step:22145 [D loss: 0.240001, acc.: 60.94%] [G loss: 0.430261]\n",
      "epoch:23 step:22146 [D loss: 0.216336, acc.: 65.62%] [G loss: 0.407731]\n",
      "epoch:23 step:22147 [D loss: 0.254197, acc.: 51.56%] [G loss: 0.412702]\n",
      "epoch:23 step:22148 [D loss: 0.238992, acc.: 64.06%] [G loss: 0.395283]\n",
      "epoch:23 step:22149 [D loss: 0.224755, acc.: 63.28%] [G loss: 0.422602]\n",
      "epoch:23 step:22150 [D loss: 0.222397, acc.: 64.06%] [G loss: 0.419476]\n",
      "epoch:23 step:22151 [D loss: 0.242773, acc.: 54.69%] [G loss: 0.401122]\n",
      "epoch:23 step:22152 [D loss: 0.221408, acc.: 66.41%] [G loss: 0.383068]\n",
      "epoch:23 step:22153 [D loss: 0.236321, acc.: 60.94%] [G loss: 0.399348]\n",
      "epoch:23 step:22154 [D loss: 0.219681, acc.: 60.94%] [G loss: 0.442546]\n",
      "epoch:23 step:22155 [D loss: 0.189734, acc.: 76.56%] [G loss: 0.440554]\n",
      "epoch:23 step:22156 [D loss: 0.219027, acc.: 60.16%] [G loss: 0.438557]\n",
      "epoch:23 step:22157 [D loss: 0.236772, acc.: 58.59%] [G loss: 0.446421]\n",
      "epoch:23 step:22158 [D loss: 0.212264, acc.: 62.50%] [G loss: 0.420820]\n",
      "epoch:23 step:22159 [D loss: 0.211428, acc.: 67.97%] [G loss: 0.457801]\n",
      "epoch:23 step:22160 [D loss: 0.232627, acc.: 56.25%] [G loss: 0.414921]\n",
      "epoch:23 step:22161 [D loss: 0.215815, acc.: 60.16%] [G loss: 0.398713]\n",
      "epoch:23 step:22162 [D loss: 0.230183, acc.: 58.59%] [G loss: 0.408023]\n",
      "epoch:23 step:22163 [D loss: 0.237791, acc.: 56.25%] [G loss: 0.422983]\n",
      "epoch:23 step:22164 [D loss: 0.215875, acc.: 66.41%] [G loss: 0.430395]\n",
      "epoch:23 step:22165 [D loss: 0.255087, acc.: 52.34%] [G loss: 0.398053]\n",
      "epoch:23 step:22166 [D loss: 0.237888, acc.: 62.50%] [G loss: 0.394089]\n",
      "epoch:23 step:22167 [D loss: 0.212670, acc.: 62.50%] [G loss: 0.410302]\n",
      "epoch:23 step:22168 [D loss: 0.210097, acc.: 65.62%] [G loss: 0.418969]\n",
      "epoch:23 step:22169 [D loss: 0.216706, acc.: 67.97%] [G loss: 0.405016]\n",
      "epoch:23 step:22170 [D loss: 0.226966, acc.: 63.28%] [G loss: 0.463935]\n",
      "epoch:23 step:22171 [D loss: 0.185226, acc.: 71.09%] [G loss: 0.447953]\n",
      "epoch:23 step:22172 [D loss: 0.232788, acc.: 59.38%] [G loss: 0.388097]\n",
      "epoch:23 step:22173 [D loss: 0.222419, acc.: 60.16%] [G loss: 0.368659]\n",
      "epoch:23 step:22174 [D loss: 0.226917, acc.: 58.59%] [G loss: 0.439800]\n",
      "epoch:23 step:22175 [D loss: 0.201048, acc.: 69.53%] [G loss: 0.465073]\n",
      "epoch:23 step:22176 [D loss: 0.223793, acc.: 58.59%] [G loss: 0.459555]\n",
      "epoch:23 step:22177 [D loss: 0.238988, acc.: 57.03%] [G loss: 0.451781]\n",
      "epoch:23 step:22178 [D loss: 0.208697, acc.: 68.75%] [G loss: 0.427919]\n",
      "epoch:23 step:22179 [D loss: 0.227517, acc.: 64.84%] [G loss: 0.425332]\n",
      "epoch:23 step:22180 [D loss: 0.226249, acc.: 60.94%] [G loss: 0.422561]\n",
      "epoch:23 step:22181 [D loss: 0.215467, acc.: 70.31%] [G loss: 0.454241]\n",
      "epoch:23 step:22182 [D loss: 0.205654, acc.: 66.41%] [G loss: 0.438137]\n",
      "epoch:23 step:22183 [D loss: 0.199827, acc.: 67.97%] [G loss: 0.465337]\n",
      "epoch:23 step:22184 [D loss: 0.222539, acc.: 62.50%] [G loss: 0.417460]\n",
      "epoch:23 step:22185 [D loss: 0.180104, acc.: 71.09%] [G loss: 0.432297]\n",
      "epoch:23 step:22186 [D loss: 0.215052, acc.: 70.31%] [G loss: 0.433515]\n",
      "epoch:23 step:22187 [D loss: 0.234757, acc.: 57.03%] [G loss: 0.450682]\n",
      "epoch:23 step:22188 [D loss: 0.228572, acc.: 61.72%] [G loss: 0.416390]\n",
      "epoch:23 step:22189 [D loss: 0.192450, acc.: 73.44%] [G loss: 0.450900]\n",
      "epoch:23 step:22190 [D loss: 0.229966, acc.: 59.38%] [G loss: 0.411634]\n",
      "epoch:23 step:22191 [D loss: 0.249928, acc.: 57.03%] [G loss: 0.413784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22192 [D loss: 0.202365, acc.: 65.62%] [G loss: 0.463246]\n",
      "epoch:23 step:22193 [D loss: 0.201080, acc.: 74.22%] [G loss: 0.464906]\n",
      "epoch:23 step:22194 [D loss: 0.225724, acc.: 60.16%] [G loss: 0.467368]\n",
      "epoch:23 step:22195 [D loss: 0.230776, acc.: 60.94%] [G loss: 0.438224]\n",
      "epoch:23 step:22196 [D loss: 0.220685, acc.: 60.16%] [G loss: 0.424181]\n",
      "epoch:23 step:22197 [D loss: 0.211758, acc.: 68.75%] [G loss: 0.438548]\n",
      "epoch:23 step:22198 [D loss: 0.211683, acc.: 68.75%] [G loss: 0.472819]\n",
      "epoch:23 step:22199 [D loss: 0.184886, acc.: 71.88%] [G loss: 0.516943]\n",
      "epoch:23 step:22200 [D loss: 0.234715, acc.: 64.84%] [G loss: 0.463786]\n",
      "##############\n",
      "[2.68870694 1.81222033 5.97049696 4.75966984 3.77169987 5.76973749\n",
      " 4.48168909 4.60249413 4.44925035 4.19719218]\n",
      "##########\n",
      "epoch:23 step:22201 [D loss: 0.202074, acc.: 68.75%] [G loss: 0.464979]\n",
      "epoch:23 step:22202 [D loss: 0.216430, acc.: 62.50%] [G loss: 0.454397]\n",
      "epoch:23 step:22203 [D loss: 0.230835, acc.: 64.06%] [G loss: 0.438172]\n",
      "epoch:23 step:22204 [D loss: 0.217436, acc.: 66.41%] [G loss: 0.439879]\n",
      "epoch:23 step:22205 [D loss: 0.230233, acc.: 60.16%] [G loss: 0.438310]\n",
      "epoch:23 step:22206 [D loss: 0.258873, acc.: 52.34%] [G loss: 0.454858]\n",
      "epoch:23 step:22207 [D loss: 0.231532, acc.: 60.16%] [G loss: 0.419596]\n",
      "epoch:23 step:22208 [D loss: 0.226787, acc.: 65.62%] [G loss: 0.442403]\n",
      "epoch:23 step:22209 [D loss: 0.227252, acc.: 57.03%] [G loss: 0.416581]\n",
      "epoch:23 step:22210 [D loss: 0.213676, acc.: 64.06%] [G loss: 0.430173]\n",
      "epoch:23 step:22211 [D loss: 0.210252, acc.: 67.19%] [G loss: 0.426138]\n",
      "epoch:23 step:22212 [D loss: 0.210425, acc.: 69.53%] [G loss: 0.407948]\n",
      "epoch:23 step:22213 [D loss: 0.222366, acc.: 62.50%] [G loss: 0.436842]\n",
      "epoch:23 step:22214 [D loss: 0.241729, acc.: 60.94%] [G loss: 0.405430]\n",
      "epoch:23 step:22215 [D loss: 0.215546, acc.: 67.97%] [G loss: 0.436375]\n",
      "epoch:23 step:22216 [D loss: 0.223339, acc.: 64.06%] [G loss: 0.415686]\n",
      "epoch:23 step:22217 [D loss: 0.224144, acc.: 66.41%] [G loss: 0.456193]\n",
      "epoch:23 step:22218 [D loss: 0.242659, acc.: 55.47%] [G loss: 0.412661]\n",
      "epoch:23 step:22219 [D loss: 0.241813, acc.: 60.16%] [G loss: 0.421496]\n",
      "epoch:23 step:22220 [D loss: 0.218178, acc.: 63.28%] [G loss: 0.399100]\n",
      "epoch:23 step:22221 [D loss: 0.236044, acc.: 64.84%] [G loss: 0.414319]\n",
      "epoch:23 step:22222 [D loss: 0.227411, acc.: 62.50%] [G loss: 0.433555]\n",
      "epoch:23 step:22223 [D loss: 0.226489, acc.: 64.06%] [G loss: 0.445281]\n",
      "epoch:23 step:22224 [D loss: 0.228069, acc.: 64.84%] [G loss: 0.409450]\n",
      "epoch:23 step:22225 [D loss: 0.205526, acc.: 68.75%] [G loss: 0.441529]\n",
      "epoch:23 step:22226 [D loss: 0.237171, acc.: 60.16%] [G loss: 0.431915]\n",
      "epoch:23 step:22227 [D loss: 0.234555, acc.: 60.94%] [G loss: 0.416130]\n",
      "epoch:23 step:22228 [D loss: 0.222829, acc.: 62.50%] [G loss: 0.436550]\n",
      "epoch:23 step:22229 [D loss: 0.237356, acc.: 55.47%] [G loss: 0.395593]\n",
      "epoch:23 step:22230 [D loss: 0.202823, acc.: 70.31%] [G loss: 0.458967]\n",
      "epoch:23 step:22231 [D loss: 0.194682, acc.: 67.19%] [G loss: 0.434446]\n",
      "epoch:23 step:22232 [D loss: 0.196063, acc.: 67.19%] [G loss: 0.501610]\n",
      "epoch:23 step:22233 [D loss: 0.242523, acc.: 50.78%] [G loss: 0.392237]\n",
      "epoch:23 step:22234 [D loss: 0.238755, acc.: 59.38%] [G loss: 0.446540]\n",
      "epoch:23 step:22235 [D loss: 0.228028, acc.: 63.28%] [G loss: 0.437109]\n",
      "epoch:23 step:22236 [D loss: 0.242615, acc.: 62.50%] [G loss: 0.399627]\n",
      "epoch:23 step:22237 [D loss: 0.207539, acc.: 69.53%] [G loss: 0.441495]\n",
      "epoch:23 step:22238 [D loss: 0.257267, acc.: 49.22%] [G loss: 0.394147]\n",
      "epoch:23 step:22239 [D loss: 0.212769, acc.: 65.62%] [G loss: 0.426692]\n",
      "epoch:23 step:22240 [D loss: 0.194338, acc.: 72.66%] [G loss: 0.466415]\n",
      "epoch:23 step:22241 [D loss: 0.187197, acc.: 71.88%] [G loss: 0.468050]\n",
      "epoch:23 step:22242 [D loss: 0.196371, acc.: 75.78%] [G loss: 0.477558]\n",
      "epoch:23 step:22243 [D loss: 0.203115, acc.: 67.97%] [G loss: 0.490777]\n",
      "epoch:23 step:22244 [D loss: 0.211096, acc.: 68.75%] [G loss: 0.476568]\n",
      "epoch:23 step:22245 [D loss: 0.201907, acc.: 71.88%] [G loss: 0.424200]\n",
      "epoch:23 step:22246 [D loss: 0.221927, acc.: 63.28%] [G loss: 0.414570]\n",
      "epoch:23 step:22247 [D loss: 0.228474, acc.: 62.50%] [G loss: 0.421838]\n",
      "epoch:23 step:22248 [D loss: 0.220998, acc.: 62.50%] [G loss: 0.415633]\n",
      "epoch:23 step:22249 [D loss: 0.239822, acc.: 57.81%] [G loss: 0.410499]\n",
      "epoch:23 step:22250 [D loss: 0.218736, acc.: 64.06%] [G loss: 0.414729]\n",
      "epoch:23 step:22251 [D loss: 0.209745, acc.: 65.62%] [G loss: 0.488478]\n",
      "epoch:23 step:22252 [D loss: 0.204185, acc.: 64.06%] [G loss: 0.489111]\n",
      "epoch:23 step:22253 [D loss: 0.243131, acc.: 60.94%] [G loss: 0.480435]\n",
      "epoch:23 step:22254 [D loss: 0.236215, acc.: 59.38%] [G loss: 0.414570]\n",
      "epoch:23 step:22255 [D loss: 0.244804, acc.: 57.81%] [G loss: 0.440486]\n",
      "epoch:23 step:22256 [D loss: 0.204277, acc.: 70.31%] [G loss: 0.453684]\n",
      "epoch:23 step:22257 [D loss: 0.215656, acc.: 66.41%] [G loss: 0.454318]\n",
      "epoch:23 step:22258 [D loss: 0.246437, acc.: 60.94%] [G loss: 0.456890]\n",
      "epoch:23 step:22259 [D loss: 0.202493, acc.: 65.62%] [G loss: 0.423977]\n",
      "epoch:23 step:22260 [D loss: 0.184420, acc.: 75.78%] [G loss: 0.436547]\n",
      "epoch:23 step:22261 [D loss: 0.263602, acc.: 46.88%] [G loss: 0.407978]\n",
      "epoch:23 step:22262 [D loss: 0.218681, acc.: 64.84%] [G loss: 0.409531]\n",
      "epoch:23 step:22263 [D loss: 0.211960, acc.: 63.28%] [G loss: 0.429043]\n",
      "epoch:23 step:22264 [D loss: 0.206089, acc.: 66.41%] [G loss: 0.469237]\n",
      "epoch:23 step:22265 [D loss: 0.223810, acc.: 57.03%] [G loss: 0.435757]\n",
      "epoch:23 step:22266 [D loss: 0.262642, acc.: 51.56%] [G loss: 0.393658]\n",
      "epoch:23 step:22267 [D loss: 0.255021, acc.: 51.56%] [G loss: 0.435855]\n",
      "epoch:23 step:22268 [D loss: 0.229408, acc.: 60.94%] [G loss: 0.432019]\n",
      "epoch:23 step:22269 [D loss: 0.194142, acc.: 73.44%] [G loss: 0.446671]\n",
      "epoch:23 step:22270 [D loss: 0.189051, acc.: 71.09%] [G loss: 0.470976]\n",
      "epoch:23 step:22271 [D loss: 0.199954, acc.: 71.88%] [G loss: 0.468569]\n",
      "epoch:23 step:22272 [D loss: 0.219490, acc.: 61.72%] [G loss: 0.435482]\n",
      "epoch:23 step:22273 [D loss: 0.236117, acc.: 59.38%] [G loss: 0.422956]\n",
      "epoch:23 step:22274 [D loss: 0.247274, acc.: 63.28%] [G loss: 0.426162]\n",
      "epoch:23 step:22275 [D loss: 0.218561, acc.: 66.41%] [G loss: 0.455819]\n",
      "epoch:23 step:22276 [D loss: 0.224360, acc.: 63.28%] [G loss: 0.448386]\n",
      "epoch:23 step:22277 [D loss: 0.232004, acc.: 64.06%] [G loss: 0.464927]\n",
      "epoch:23 step:22278 [D loss: 0.239750, acc.: 53.91%] [G loss: 0.436551]\n",
      "epoch:23 step:22279 [D loss: 0.235938, acc.: 60.94%] [G loss: 0.413586]\n",
      "epoch:23 step:22280 [D loss: 0.232448, acc.: 61.72%] [G loss: 0.399168]\n",
      "epoch:23 step:22281 [D loss: 0.227828, acc.: 57.81%] [G loss: 0.421340]\n",
      "epoch:23 step:22282 [D loss: 0.247653, acc.: 56.25%] [G loss: 0.425202]\n",
      "epoch:23 step:22283 [D loss: 0.229660, acc.: 60.94%] [G loss: 0.447812]\n",
      "epoch:23 step:22284 [D loss: 0.216596, acc.: 67.97%] [G loss: 0.437844]\n",
      "epoch:23 step:22285 [D loss: 0.222997, acc.: 63.28%] [G loss: 0.439148]\n",
      "epoch:23 step:22286 [D loss: 0.248489, acc.: 54.69%] [G loss: 0.443515]\n",
      "epoch:23 step:22287 [D loss: 0.202592, acc.: 69.53%] [G loss: 0.402477]\n",
      "epoch:23 step:22288 [D loss: 0.234570, acc.: 59.38%] [G loss: 0.377308]\n",
      "epoch:23 step:22289 [D loss: 0.229547, acc.: 60.94%] [G loss: 0.407202]\n",
      "epoch:23 step:22290 [D loss: 0.272047, acc.: 52.34%] [G loss: 0.385812]\n",
      "epoch:23 step:22291 [D loss: 0.233380, acc.: 61.72%] [G loss: 0.431575]\n",
      "epoch:23 step:22292 [D loss: 0.263575, acc.: 56.25%] [G loss: 0.408745]\n",
      "epoch:23 step:22293 [D loss: 0.223799, acc.: 59.38%] [G loss: 0.441990]\n",
      "epoch:23 step:22294 [D loss: 0.203959, acc.: 67.97%] [G loss: 0.440386]\n",
      "epoch:23 step:22295 [D loss: 0.220420, acc.: 60.94%] [G loss: 0.382904]\n",
      "epoch:23 step:22296 [D loss: 0.244267, acc.: 57.81%] [G loss: 0.423323]\n",
      "epoch:23 step:22297 [D loss: 0.233388, acc.: 60.16%] [G loss: 0.405388]\n",
      "epoch:23 step:22298 [D loss: 0.200188, acc.: 69.53%] [G loss: 0.436061]\n",
      "epoch:23 step:22299 [D loss: 0.219531, acc.: 65.62%] [G loss: 0.416767]\n",
      "epoch:23 step:22300 [D loss: 0.220532, acc.: 62.50%] [G loss: 0.412472]\n",
      "epoch:23 step:22301 [D loss: 0.197858, acc.: 71.88%] [G loss: 0.409455]\n",
      "epoch:23 step:22302 [D loss: 0.230231, acc.: 61.72%] [G loss: 0.429811]\n",
      "epoch:23 step:22303 [D loss: 0.235482, acc.: 59.38%] [G loss: 0.429772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22304 [D loss: 0.215259, acc.: 68.75%] [G loss: 0.454297]\n",
      "epoch:23 step:22305 [D loss: 0.223398, acc.: 60.94%] [G loss: 0.425043]\n",
      "epoch:23 step:22306 [D loss: 0.222959, acc.: 66.41%] [G loss: 0.413769]\n",
      "epoch:23 step:22307 [D loss: 0.234302, acc.: 59.38%] [G loss: 0.430531]\n",
      "epoch:23 step:22308 [D loss: 0.236106, acc.: 64.84%] [G loss: 0.442454]\n",
      "epoch:23 step:22309 [D loss: 0.226987, acc.: 64.06%] [G loss: 0.435962]\n",
      "epoch:23 step:22310 [D loss: 0.259609, acc.: 53.12%] [G loss: 0.436293]\n",
      "epoch:23 step:22311 [D loss: 0.233987, acc.: 62.50%] [G loss: 0.418621]\n",
      "epoch:23 step:22312 [D loss: 0.213410, acc.: 64.06%] [G loss: 0.442457]\n",
      "epoch:23 step:22313 [D loss: 0.258215, acc.: 53.91%] [G loss: 0.410094]\n",
      "epoch:23 step:22314 [D loss: 0.233274, acc.: 62.50%] [G loss: 0.439694]\n",
      "epoch:23 step:22315 [D loss: 0.241321, acc.: 60.16%] [G loss: 0.447886]\n",
      "epoch:23 step:22316 [D loss: 0.250827, acc.: 60.16%] [G loss: 0.411809]\n",
      "epoch:23 step:22317 [D loss: 0.218850, acc.: 66.41%] [G loss: 0.403592]\n",
      "epoch:23 step:22318 [D loss: 0.228186, acc.: 54.69%] [G loss: 0.454339]\n",
      "epoch:23 step:22319 [D loss: 0.236940, acc.: 57.81%] [G loss: 0.446961]\n",
      "epoch:23 step:22320 [D loss: 0.212348, acc.: 65.62%] [G loss: 0.445570]\n",
      "epoch:23 step:22321 [D loss: 0.215683, acc.: 66.41%] [G loss: 0.469849]\n",
      "epoch:23 step:22322 [D loss: 0.209671, acc.: 64.84%] [G loss: 0.464178]\n",
      "epoch:23 step:22323 [D loss: 0.261883, acc.: 52.34%] [G loss: 0.409388]\n",
      "epoch:23 step:22324 [D loss: 0.206973, acc.: 67.19%] [G loss: 0.445223]\n",
      "epoch:23 step:22325 [D loss: 0.224194, acc.: 63.28%] [G loss: 0.451786]\n",
      "epoch:23 step:22326 [D loss: 0.225024, acc.: 64.06%] [G loss: 0.446687]\n",
      "epoch:23 step:22327 [D loss: 0.218682, acc.: 63.28%] [G loss: 0.441559]\n",
      "epoch:23 step:22328 [D loss: 0.212629, acc.: 64.06%] [G loss: 0.426558]\n",
      "epoch:23 step:22329 [D loss: 0.212107, acc.: 62.50%] [G loss: 0.427329]\n",
      "epoch:23 step:22330 [D loss: 0.216582, acc.: 66.41%] [G loss: 0.426104]\n",
      "epoch:23 step:22331 [D loss: 0.213881, acc.: 67.97%] [G loss: 0.436240]\n",
      "epoch:23 step:22332 [D loss: 0.228737, acc.: 59.38%] [G loss: 0.406877]\n",
      "epoch:23 step:22333 [D loss: 0.188977, acc.: 68.75%] [G loss: 0.455055]\n",
      "epoch:23 step:22334 [D loss: 0.221334, acc.: 65.62%] [G loss: 0.442974]\n",
      "epoch:23 step:22335 [D loss: 0.273568, acc.: 49.22%] [G loss: 0.385352]\n",
      "epoch:23 step:22336 [D loss: 0.240119, acc.: 56.25%] [G loss: 0.386135]\n",
      "epoch:23 step:22337 [D loss: 0.201673, acc.: 70.31%] [G loss: 0.439724]\n",
      "epoch:23 step:22338 [D loss: 0.250421, acc.: 50.00%] [G loss: 0.434069]\n",
      "epoch:23 step:22339 [D loss: 0.240350, acc.: 60.94%] [G loss: 0.422822]\n",
      "epoch:23 step:22340 [D loss: 0.225245, acc.: 69.53%] [G loss: 0.429090]\n",
      "epoch:23 step:22341 [D loss: 0.231183, acc.: 64.06%] [G loss: 0.418951]\n",
      "epoch:23 step:22342 [D loss: 0.256734, acc.: 57.81%] [G loss: 0.410880]\n",
      "epoch:23 step:22343 [D loss: 0.206099, acc.: 65.62%] [G loss: 0.448702]\n",
      "epoch:23 step:22344 [D loss: 0.207018, acc.: 65.62%] [G loss: 0.414625]\n",
      "epoch:23 step:22345 [D loss: 0.250692, acc.: 57.03%] [G loss: 0.395465]\n",
      "epoch:23 step:22346 [D loss: 0.253404, acc.: 53.91%] [G loss: 0.461972]\n",
      "epoch:23 step:22347 [D loss: 0.205448, acc.: 67.19%] [G loss: 0.474399]\n",
      "epoch:23 step:22348 [D loss: 0.222782, acc.: 64.84%] [G loss: 0.436344]\n",
      "epoch:23 step:22349 [D loss: 0.258032, acc.: 58.59%] [G loss: 0.444200]\n",
      "epoch:23 step:22350 [D loss: 0.233589, acc.: 59.38%] [G loss: 0.441063]\n",
      "epoch:23 step:22351 [D loss: 0.256997, acc.: 50.78%] [G loss: 0.406788]\n",
      "epoch:23 step:22352 [D loss: 0.223765, acc.: 63.28%] [G loss: 0.414428]\n",
      "epoch:23 step:22353 [D loss: 0.217751, acc.: 64.84%] [G loss: 0.482376]\n",
      "epoch:23 step:22354 [D loss: 0.201673, acc.: 69.53%] [G loss: 0.503425]\n",
      "epoch:23 step:22355 [D loss: 0.246307, acc.: 57.03%] [G loss: 0.412994]\n",
      "epoch:23 step:22356 [D loss: 0.219684, acc.: 60.94%] [G loss: 0.412762]\n",
      "epoch:23 step:22357 [D loss: 0.232188, acc.: 59.38%] [G loss: 0.385009]\n",
      "epoch:23 step:22358 [D loss: 0.203908, acc.: 67.97%] [G loss: 0.416973]\n",
      "epoch:23 step:22359 [D loss: 0.231667, acc.: 60.16%] [G loss: 0.412039]\n",
      "epoch:23 step:22360 [D loss: 0.226048, acc.: 64.84%] [G loss: 0.409981]\n",
      "epoch:23 step:22361 [D loss: 0.220181, acc.: 62.50%] [G loss: 0.423406]\n",
      "epoch:23 step:22362 [D loss: 0.234141, acc.: 57.81%] [G loss: 0.405341]\n",
      "epoch:23 step:22363 [D loss: 0.228849, acc.: 63.28%] [G loss: 0.385842]\n",
      "epoch:23 step:22364 [D loss: 0.233662, acc.: 60.16%] [G loss: 0.412886]\n",
      "epoch:23 step:22365 [D loss: 0.213806, acc.: 62.50%] [G loss: 0.403940]\n",
      "epoch:23 step:22366 [D loss: 0.202792, acc.: 67.97%] [G loss: 0.451622]\n",
      "epoch:23 step:22367 [D loss: 0.226069, acc.: 60.94%] [G loss: 0.418819]\n",
      "epoch:23 step:22368 [D loss: 0.242475, acc.: 58.59%] [G loss: 0.430609]\n",
      "epoch:23 step:22369 [D loss: 0.218217, acc.: 65.62%] [G loss: 0.443073]\n",
      "epoch:23 step:22370 [D loss: 0.229244, acc.: 62.50%] [G loss: 0.422543]\n",
      "epoch:23 step:22371 [D loss: 0.265345, acc.: 55.47%] [G loss: 0.394501]\n",
      "epoch:23 step:22372 [D loss: 0.234368, acc.: 56.25%] [G loss: 0.388445]\n",
      "epoch:23 step:22373 [D loss: 0.208758, acc.: 64.84%] [G loss: 0.414404]\n",
      "epoch:23 step:22374 [D loss: 0.201838, acc.: 72.66%] [G loss: 0.411916]\n",
      "epoch:23 step:22375 [D loss: 0.225002, acc.: 63.28%] [G loss: 0.422722]\n",
      "epoch:23 step:22376 [D loss: 0.207605, acc.: 64.84%] [G loss: 0.424930]\n",
      "epoch:23 step:22377 [D loss: 0.202154, acc.: 66.41%] [G loss: 0.438537]\n",
      "epoch:23 step:22378 [D loss: 0.242828, acc.: 52.34%] [G loss: 0.416344]\n",
      "epoch:23 step:22379 [D loss: 0.273534, acc.: 49.22%] [G loss: 0.383920]\n",
      "epoch:23 step:22380 [D loss: 0.249202, acc.: 50.78%] [G loss: 0.388555]\n",
      "epoch:23 step:22381 [D loss: 0.229042, acc.: 60.94%] [G loss: 0.421679]\n",
      "epoch:23 step:22382 [D loss: 0.200361, acc.: 71.88%] [G loss: 0.452598]\n",
      "epoch:23 step:22383 [D loss: 0.224334, acc.: 66.41%] [G loss: 0.449922]\n",
      "epoch:23 step:22384 [D loss: 0.214339, acc.: 60.16%] [G loss: 0.407229]\n",
      "epoch:23 step:22385 [D loss: 0.265876, acc.: 50.00%] [G loss: 0.372065]\n",
      "epoch:23 step:22386 [D loss: 0.222871, acc.: 61.72%] [G loss: 0.446586]\n",
      "epoch:23 step:22387 [D loss: 0.209852, acc.: 66.41%] [G loss: 0.414178]\n",
      "epoch:23 step:22388 [D loss: 0.213726, acc.: 66.41%] [G loss: 0.398252]\n",
      "epoch:23 step:22389 [D loss: 0.201117, acc.: 68.75%] [G loss: 0.408820]\n",
      "epoch:23 step:22390 [D loss: 0.227655, acc.: 61.72%] [G loss: 0.426592]\n",
      "epoch:23 step:22391 [D loss: 0.230516, acc.: 62.50%] [G loss: 0.454045]\n",
      "epoch:23 step:22392 [D loss: 0.235495, acc.: 58.59%] [G loss: 0.442475]\n",
      "epoch:23 step:22393 [D loss: 0.195272, acc.: 71.09%] [G loss: 0.437170]\n",
      "epoch:23 step:22394 [D loss: 0.237795, acc.: 60.16%] [G loss: 0.445416]\n",
      "epoch:23 step:22395 [D loss: 0.231948, acc.: 57.81%] [G loss: 0.482774]\n",
      "epoch:23 step:22396 [D loss: 0.194238, acc.: 72.66%] [G loss: 0.457138]\n",
      "epoch:23 step:22397 [D loss: 0.236386, acc.: 54.69%] [G loss: 0.426626]\n",
      "epoch:23 step:22398 [D loss: 0.248089, acc.: 57.81%] [G loss: 0.388647]\n",
      "epoch:23 step:22399 [D loss: 0.246695, acc.: 59.38%] [G loss: 0.355604]\n",
      "epoch:23 step:22400 [D loss: 0.215658, acc.: 61.72%] [G loss: 0.411765]\n",
      "##############\n",
      "[2.67554829 1.87152557 6.2270581  4.73225383 3.55293461 5.42318535\n",
      " 4.55796781 4.67883772 4.29039489 3.83673816]\n",
      "##########\n",
      "epoch:23 step:22401 [D loss: 0.236830, acc.: 54.69%] [G loss: 0.412545]\n",
      "epoch:23 step:22402 [D loss: 0.246656, acc.: 52.34%] [G loss: 0.412775]\n",
      "epoch:23 step:22403 [D loss: 0.216581, acc.: 62.50%] [G loss: 0.435151]\n",
      "epoch:23 step:22404 [D loss: 0.214763, acc.: 66.41%] [G loss: 0.433003]\n",
      "epoch:23 step:22405 [D loss: 0.213109, acc.: 64.06%] [G loss: 0.434755]\n",
      "epoch:23 step:22406 [D loss: 0.217745, acc.: 65.62%] [G loss: 0.417877]\n",
      "epoch:23 step:22407 [D loss: 0.241822, acc.: 57.03%] [G loss: 0.399780]\n",
      "epoch:23 step:22408 [D loss: 0.214703, acc.: 67.19%] [G loss: 0.432627]\n",
      "epoch:23 step:22409 [D loss: 0.267045, acc.: 49.22%] [G loss: 0.432769]\n",
      "epoch:23 step:22410 [D loss: 0.224334, acc.: 64.06%] [G loss: 0.398631]\n",
      "epoch:23 step:22411 [D loss: 0.190465, acc.: 71.88%] [G loss: 0.432235]\n",
      "epoch:23 step:22412 [D loss: 0.246607, acc.: 58.59%] [G loss: 0.422182]\n",
      "epoch:23 step:22413 [D loss: 0.250542, acc.: 56.25%] [G loss: 0.383399]\n",
      "epoch:23 step:22414 [D loss: 0.209116, acc.: 68.75%] [G loss: 0.426086]\n",
      "epoch:23 step:22415 [D loss: 0.222930, acc.: 60.94%] [G loss: 0.425830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22416 [D loss: 0.231911, acc.: 62.50%] [G loss: 0.391877]\n",
      "epoch:23 step:22417 [D loss: 0.234288, acc.: 61.72%] [G loss: 0.365568]\n",
      "epoch:23 step:22418 [D loss: 0.248792, acc.: 54.69%] [G loss: 0.435377]\n",
      "epoch:23 step:22419 [D loss: 0.248246, acc.: 52.34%] [G loss: 0.411331]\n",
      "epoch:23 step:22420 [D loss: 0.239519, acc.: 59.38%] [G loss: 0.419972]\n",
      "epoch:23 step:22421 [D loss: 0.238295, acc.: 65.62%] [G loss: 0.448999]\n",
      "epoch:23 step:22422 [D loss: 0.238717, acc.: 60.16%] [G loss: 0.434302]\n",
      "epoch:23 step:22423 [D loss: 0.227824, acc.: 61.72%] [G loss: 0.423848]\n",
      "epoch:23 step:22424 [D loss: 0.241248, acc.: 60.16%] [G loss: 0.433691]\n",
      "epoch:23 step:22425 [D loss: 0.230592, acc.: 61.72%] [G loss: 0.405095]\n",
      "epoch:23 step:22426 [D loss: 0.186056, acc.: 76.56%] [G loss: 0.421881]\n",
      "epoch:23 step:22427 [D loss: 0.209113, acc.: 66.41%] [G loss: 0.439804]\n",
      "epoch:23 step:22428 [D loss: 0.254503, acc.: 56.25%] [G loss: 0.401031]\n",
      "epoch:23 step:22429 [D loss: 0.210204, acc.: 64.84%] [G loss: 0.429589]\n",
      "epoch:23 step:22430 [D loss: 0.234184, acc.: 59.38%] [G loss: 0.396696]\n",
      "epoch:23 step:22431 [D loss: 0.245053, acc.: 60.94%] [G loss: 0.385957]\n",
      "epoch:23 step:22432 [D loss: 0.223452, acc.: 63.28%] [G loss: 0.393781]\n",
      "epoch:23 step:22433 [D loss: 0.228617, acc.: 59.38%] [G loss: 0.381521]\n",
      "epoch:23 step:22434 [D loss: 0.227759, acc.: 62.50%] [G loss: 0.418276]\n",
      "epoch:23 step:22435 [D loss: 0.207161, acc.: 67.97%] [G loss: 0.417773]\n",
      "epoch:23 step:22436 [D loss: 0.209268, acc.: 69.53%] [G loss: 0.458746]\n",
      "epoch:23 step:22437 [D loss: 0.224738, acc.: 66.41%] [G loss: 0.430643]\n",
      "epoch:23 step:22438 [D loss: 0.232823, acc.: 62.50%] [G loss: 0.448157]\n",
      "epoch:23 step:22439 [D loss: 0.250202, acc.: 56.25%] [G loss: 0.454122]\n",
      "epoch:23 step:22440 [D loss: 0.215143, acc.: 66.41%] [G loss: 0.455432]\n",
      "epoch:23 step:22441 [D loss: 0.190250, acc.: 72.66%] [G loss: 0.436821]\n",
      "epoch:23 step:22442 [D loss: 0.265034, acc.: 57.03%] [G loss: 0.396751]\n",
      "epoch:23 step:22443 [D loss: 0.234631, acc.: 64.06%] [G loss: 0.430132]\n",
      "epoch:23 step:22444 [D loss: 0.231776, acc.: 60.16%] [G loss: 0.442764]\n",
      "epoch:23 step:22445 [D loss: 0.212201, acc.: 67.19%] [G loss: 0.411152]\n",
      "epoch:23 step:22446 [D loss: 0.205791, acc.: 69.53%] [G loss: 0.462528]\n",
      "epoch:23 step:22447 [D loss: 0.232024, acc.: 63.28%] [G loss: 0.441488]\n",
      "epoch:23 step:22448 [D loss: 0.201127, acc.: 70.31%] [G loss: 0.497908]\n",
      "epoch:23 step:22449 [D loss: 0.210810, acc.: 65.62%] [G loss: 0.471356]\n",
      "epoch:23 step:22450 [D loss: 0.189375, acc.: 73.44%] [G loss: 0.493776]\n",
      "epoch:23 step:22451 [D loss: 0.229560, acc.: 58.59%] [G loss: 0.450010]\n",
      "epoch:23 step:22452 [D loss: 0.227202, acc.: 67.19%] [G loss: 0.416190]\n",
      "epoch:23 step:22453 [D loss: 0.240630, acc.: 60.94%] [G loss: 0.420378]\n",
      "epoch:23 step:22454 [D loss: 0.235293, acc.: 61.72%] [G loss: 0.398922]\n",
      "epoch:23 step:22455 [D loss: 0.237179, acc.: 57.03%] [G loss: 0.418935]\n",
      "epoch:23 step:22456 [D loss: 0.203306, acc.: 71.09%] [G loss: 0.440695]\n",
      "epoch:23 step:22457 [D loss: 0.211532, acc.: 65.62%] [G loss: 0.463247]\n",
      "epoch:23 step:22458 [D loss: 0.221476, acc.: 60.16%] [G loss: 0.499047]\n",
      "epoch:23 step:22459 [D loss: 0.206645, acc.: 66.41%] [G loss: 0.440433]\n",
      "epoch:23 step:22460 [D loss: 0.200192, acc.: 68.75%] [G loss: 0.463223]\n",
      "epoch:23 step:22461 [D loss: 0.199629, acc.: 63.28%] [G loss: 0.462446]\n",
      "epoch:23 step:22462 [D loss: 0.212770, acc.: 64.84%] [G loss: 0.467748]\n",
      "epoch:23 step:22463 [D loss: 0.218358, acc.: 65.62%] [G loss: 0.454252]\n",
      "epoch:23 step:22464 [D loss: 0.208039, acc.: 66.41%] [G loss: 0.450142]\n",
      "epoch:23 step:22465 [D loss: 0.203318, acc.: 68.75%] [G loss: 0.486051]\n",
      "epoch:23 step:22466 [D loss: 0.276442, acc.: 56.25%] [G loss: 0.419199]\n",
      "epoch:23 step:22467 [D loss: 0.239413, acc.: 61.72%] [G loss: 0.438196]\n",
      "epoch:23 step:22468 [D loss: 0.241779, acc.: 57.03%] [G loss: 0.407205]\n",
      "epoch:23 step:22469 [D loss: 0.205847, acc.: 73.44%] [G loss: 0.438142]\n",
      "epoch:23 step:22470 [D loss: 0.180771, acc.: 75.00%] [G loss: 0.499716]\n",
      "epoch:23 step:22471 [D loss: 0.294784, acc.: 50.00%] [G loss: 0.422799]\n",
      "epoch:23 step:22472 [D loss: 0.208110, acc.: 64.84%] [G loss: 0.435347]\n",
      "epoch:23 step:22473 [D loss: 0.219862, acc.: 65.62%] [G loss: 0.510300]\n",
      "epoch:23 step:22474 [D loss: 0.200560, acc.: 62.50%] [G loss: 0.439029]\n",
      "epoch:23 step:22475 [D loss: 0.179418, acc.: 76.56%] [G loss: 0.462803]\n",
      "epoch:23 step:22476 [D loss: 0.193603, acc.: 70.31%] [G loss: 0.483877]\n",
      "epoch:23 step:22477 [D loss: 0.169177, acc.: 82.03%] [G loss: 0.552160]\n",
      "epoch:23 step:22478 [D loss: 0.214580, acc.: 65.62%] [G loss: 0.541766]\n",
      "epoch:23 step:22479 [D loss: 0.299396, acc.: 57.03%] [G loss: 0.530906]\n",
      "epoch:23 step:22480 [D loss: 0.219197, acc.: 70.31%] [G loss: 0.584301]\n",
      "epoch:23 step:22481 [D loss: 0.224268, acc.: 65.62%] [G loss: 0.547575]\n",
      "epoch:23 step:22482 [D loss: 0.242820, acc.: 52.34%] [G loss: 0.427058]\n",
      "epoch:23 step:22483 [D loss: 0.258026, acc.: 55.47%] [G loss: 0.405012]\n",
      "epoch:23 step:22484 [D loss: 0.221988, acc.: 70.31%] [G loss: 0.397393]\n",
      "epoch:23 step:22485 [D loss: 0.230061, acc.: 63.28%] [G loss: 0.441716]\n",
      "epoch:23 step:22486 [D loss: 0.197857, acc.: 71.09%] [G loss: 0.495098]\n",
      "epoch:23 step:22487 [D loss: 0.173344, acc.: 74.22%] [G loss: 0.532846]\n",
      "epoch:23 step:22488 [D loss: 0.205118, acc.: 69.53%] [G loss: 0.532877]\n",
      "epoch:24 step:22489 [D loss: 0.268819, acc.: 53.91%] [G loss: 0.435174]\n",
      "epoch:24 step:22490 [D loss: 0.256410, acc.: 60.94%] [G loss: 0.469919]\n",
      "epoch:24 step:22491 [D loss: 0.236338, acc.: 60.16%] [G loss: 0.500412]\n",
      "epoch:24 step:22492 [D loss: 0.214501, acc.: 61.72%] [G loss: 0.459522]\n",
      "epoch:24 step:22493 [D loss: 0.231945, acc.: 63.28%] [G loss: 0.444746]\n",
      "epoch:24 step:22494 [D loss: 0.208362, acc.: 72.66%] [G loss: 0.468813]\n",
      "epoch:24 step:22495 [D loss: 0.202483, acc.: 71.88%] [G loss: 0.433803]\n",
      "epoch:24 step:22496 [D loss: 0.201820, acc.: 70.31%] [G loss: 0.445165]\n",
      "epoch:24 step:22497 [D loss: 0.225051, acc.: 62.50%] [G loss: 0.420384]\n",
      "epoch:24 step:22498 [D loss: 0.223378, acc.: 57.81%] [G loss: 0.423569]\n",
      "epoch:24 step:22499 [D loss: 0.216218, acc.: 71.09%] [G loss: 0.430530]\n",
      "epoch:24 step:22500 [D loss: 0.233062, acc.: 62.50%] [G loss: 0.430469]\n",
      "epoch:24 step:22501 [D loss: 0.230292, acc.: 62.50%] [G loss: 0.390290]\n",
      "epoch:24 step:22502 [D loss: 0.198051, acc.: 71.88%] [G loss: 0.422311]\n",
      "epoch:24 step:22503 [D loss: 0.167488, acc.: 76.56%] [G loss: 0.479920]\n",
      "epoch:24 step:22504 [D loss: 0.193951, acc.: 69.53%] [G loss: 0.446983]\n",
      "epoch:24 step:22505 [D loss: 0.236291, acc.: 60.94%] [G loss: 0.453258]\n",
      "epoch:24 step:22506 [D loss: 0.220149, acc.: 64.84%] [G loss: 0.428466]\n",
      "epoch:24 step:22507 [D loss: 0.231445, acc.: 60.16%] [G loss: 0.457257]\n",
      "epoch:24 step:22508 [D loss: 0.259675, acc.: 53.12%] [G loss: 0.476378]\n",
      "epoch:24 step:22509 [D loss: 0.226604, acc.: 65.62%] [G loss: 0.461616]\n",
      "epoch:24 step:22510 [D loss: 0.207583, acc.: 64.84%] [G loss: 0.471602]\n",
      "epoch:24 step:22511 [D loss: 0.242086, acc.: 58.59%] [G loss: 0.409469]\n",
      "epoch:24 step:22512 [D loss: 0.212134, acc.: 71.09%] [G loss: 0.420577]\n",
      "epoch:24 step:22513 [D loss: 0.214445, acc.: 67.19%] [G loss: 0.447184]\n",
      "epoch:24 step:22514 [D loss: 0.243517, acc.: 59.38%] [G loss: 0.403088]\n",
      "epoch:24 step:22515 [D loss: 0.217648, acc.: 60.94%] [G loss: 0.442424]\n",
      "epoch:24 step:22516 [D loss: 0.229215, acc.: 61.72%] [G loss: 0.452424]\n",
      "epoch:24 step:22517 [D loss: 0.209051, acc.: 67.97%] [G loss: 0.432008]\n",
      "epoch:24 step:22518 [D loss: 0.224346, acc.: 60.16%] [G loss: 0.402485]\n",
      "epoch:24 step:22519 [D loss: 0.235598, acc.: 56.25%] [G loss: 0.445226]\n",
      "epoch:24 step:22520 [D loss: 0.225379, acc.: 61.72%] [G loss: 0.425048]\n",
      "epoch:24 step:22521 [D loss: 0.230136, acc.: 60.94%] [G loss: 0.401692]\n",
      "epoch:24 step:22522 [D loss: 0.249570, acc.: 57.81%] [G loss: 0.429704]\n",
      "epoch:24 step:22523 [D loss: 0.224479, acc.: 63.28%] [G loss: 0.436990]\n",
      "epoch:24 step:22524 [D loss: 0.224350, acc.: 64.06%] [G loss: 0.433397]\n",
      "epoch:24 step:22525 [D loss: 0.243659, acc.: 60.16%] [G loss: 0.435362]\n",
      "epoch:24 step:22526 [D loss: 0.227374, acc.: 65.62%] [G loss: 0.420306]\n",
      "epoch:24 step:22527 [D loss: 0.218220, acc.: 68.75%] [G loss: 0.418581]\n",
      "epoch:24 step:22528 [D loss: 0.186357, acc.: 74.22%] [G loss: 0.438066]\n",
      "epoch:24 step:22529 [D loss: 0.264115, acc.: 55.47%] [G loss: 0.389017]\n",
      "epoch:24 step:22530 [D loss: 0.199083, acc.: 75.00%] [G loss: 0.409358]\n",
      "epoch:24 step:22531 [D loss: 0.243661, acc.: 57.03%] [G loss: 0.400734]\n",
      "epoch:24 step:22532 [D loss: 0.250486, acc.: 59.38%] [G loss: 0.401807]\n",
      "epoch:24 step:22533 [D loss: 0.211365, acc.: 66.41%] [G loss: 0.428319]\n",
      "epoch:24 step:22534 [D loss: 0.214878, acc.: 64.84%] [G loss: 0.477882]\n",
      "epoch:24 step:22535 [D loss: 0.232144, acc.: 60.94%] [G loss: 0.390934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22536 [D loss: 0.219807, acc.: 60.94%] [G loss: 0.414391]\n",
      "epoch:24 step:22537 [D loss: 0.212593, acc.: 67.97%] [G loss: 0.418404]\n",
      "epoch:24 step:22538 [D loss: 0.209137, acc.: 69.53%] [G loss: 0.458563]\n",
      "epoch:24 step:22539 [D loss: 0.231375, acc.: 55.47%] [G loss: 0.428583]\n",
      "epoch:24 step:22540 [D loss: 0.240560, acc.: 60.16%] [G loss: 0.374118]\n",
      "epoch:24 step:22541 [D loss: 0.242708, acc.: 59.38%] [G loss: 0.448846]\n",
      "epoch:24 step:22542 [D loss: 0.212644, acc.: 71.88%] [G loss: 0.480213]\n",
      "epoch:24 step:22543 [D loss: 0.214523, acc.: 64.06%] [G loss: 0.476388]\n",
      "epoch:24 step:22544 [D loss: 0.229512, acc.: 58.59%] [G loss: 0.435626]\n",
      "epoch:24 step:22545 [D loss: 0.226004, acc.: 60.94%] [G loss: 0.423324]\n",
      "epoch:24 step:22546 [D loss: 0.234641, acc.: 61.72%] [G loss: 0.453343]\n",
      "epoch:24 step:22547 [D loss: 0.216896, acc.: 66.41%] [G loss: 0.404058]\n",
      "epoch:24 step:22548 [D loss: 0.230406, acc.: 60.16%] [G loss: 0.455948]\n",
      "epoch:24 step:22549 [D loss: 0.237536, acc.: 63.28%] [G loss: 0.438483]\n",
      "epoch:24 step:22550 [D loss: 0.220462, acc.: 62.50%] [G loss: 0.445829]\n",
      "epoch:24 step:22551 [D loss: 0.231573, acc.: 59.38%] [G loss: 0.392532]\n",
      "epoch:24 step:22552 [D loss: 0.221488, acc.: 64.06%] [G loss: 0.474780]\n",
      "epoch:24 step:22553 [D loss: 0.224374, acc.: 64.06%] [G loss: 0.447555]\n",
      "epoch:24 step:22554 [D loss: 0.222791, acc.: 64.84%] [G loss: 0.401844]\n",
      "epoch:24 step:22555 [D loss: 0.220353, acc.: 64.84%] [G loss: 0.399783]\n",
      "epoch:24 step:22556 [D loss: 0.230149, acc.: 64.06%] [G loss: 0.383869]\n",
      "epoch:24 step:22557 [D loss: 0.192521, acc.: 70.31%] [G loss: 0.428156]\n",
      "epoch:24 step:22558 [D loss: 0.213396, acc.: 62.50%] [G loss: 0.455109]\n",
      "epoch:24 step:22559 [D loss: 0.251790, acc.: 60.16%] [G loss: 0.453660]\n",
      "epoch:24 step:22560 [D loss: 0.228830, acc.: 63.28%] [G loss: 0.410795]\n",
      "epoch:24 step:22561 [D loss: 0.225264, acc.: 62.50%] [G loss: 0.409545]\n",
      "epoch:24 step:22562 [D loss: 0.207084, acc.: 65.62%] [G loss: 0.463625]\n",
      "epoch:24 step:22563 [D loss: 0.224262, acc.: 63.28%] [G loss: 0.440262]\n",
      "epoch:24 step:22564 [D loss: 0.208789, acc.: 67.19%] [G loss: 0.493284]\n",
      "epoch:24 step:22565 [D loss: 0.210087, acc.: 65.62%] [G loss: 0.448079]\n",
      "epoch:24 step:22566 [D loss: 0.261858, acc.: 53.12%] [G loss: 0.415512]\n",
      "epoch:24 step:22567 [D loss: 0.244616, acc.: 53.91%] [G loss: 0.417854]\n",
      "epoch:24 step:22568 [D loss: 0.221444, acc.: 71.09%] [G loss: 0.430798]\n",
      "epoch:24 step:22569 [D loss: 0.226982, acc.: 62.50%] [G loss: 0.423362]\n",
      "epoch:24 step:22570 [D loss: 0.227375, acc.: 57.03%] [G loss: 0.396475]\n",
      "epoch:24 step:22571 [D loss: 0.214530, acc.: 65.62%] [G loss: 0.428213]\n",
      "epoch:24 step:22572 [D loss: 0.220833, acc.: 60.16%] [G loss: 0.417893]\n",
      "epoch:24 step:22573 [D loss: 0.243322, acc.: 54.69%] [G loss: 0.416148]\n",
      "epoch:24 step:22574 [D loss: 0.229895, acc.: 61.72%] [G loss: 0.435702]\n",
      "epoch:24 step:22575 [D loss: 0.232097, acc.: 62.50%] [G loss: 0.406218]\n",
      "epoch:24 step:22576 [D loss: 0.198651, acc.: 67.97%] [G loss: 0.421830]\n",
      "epoch:24 step:22577 [D loss: 0.212394, acc.: 66.41%] [G loss: 0.428635]\n",
      "epoch:24 step:22578 [D loss: 0.235136, acc.: 57.03%] [G loss: 0.395269]\n",
      "epoch:24 step:22579 [D loss: 0.195345, acc.: 70.31%] [G loss: 0.435361]\n",
      "epoch:24 step:22580 [D loss: 0.207846, acc.: 67.19%] [G loss: 0.439473]\n",
      "epoch:24 step:22581 [D loss: 0.206602, acc.: 69.53%] [G loss: 0.442250]\n",
      "epoch:24 step:22582 [D loss: 0.223972, acc.: 61.72%] [G loss: 0.441287]\n",
      "epoch:24 step:22583 [D loss: 0.234243, acc.: 60.16%] [G loss: 0.452785]\n",
      "epoch:24 step:22584 [D loss: 0.213109, acc.: 63.28%] [G loss: 0.451115]\n",
      "epoch:24 step:22585 [D loss: 0.200290, acc.: 68.75%] [G loss: 0.465746]\n",
      "epoch:24 step:22586 [D loss: 0.231384, acc.: 62.50%] [G loss: 0.440276]\n",
      "epoch:24 step:22587 [D loss: 0.264131, acc.: 57.03%] [G loss: 0.444381]\n",
      "epoch:24 step:22588 [D loss: 0.194878, acc.: 70.31%] [G loss: 0.436840]\n",
      "epoch:24 step:22589 [D loss: 0.231852, acc.: 60.16%] [G loss: 0.458151]\n",
      "epoch:24 step:22590 [D loss: 0.249204, acc.: 53.12%] [G loss: 0.393269]\n",
      "epoch:24 step:22591 [D loss: 0.213267, acc.: 61.72%] [G loss: 0.414142]\n",
      "epoch:24 step:22592 [D loss: 0.233346, acc.: 60.16%] [G loss: 0.414319]\n",
      "epoch:24 step:22593 [D loss: 0.215773, acc.: 64.84%] [G loss: 0.462522]\n",
      "epoch:24 step:22594 [D loss: 0.211449, acc.: 67.97%] [G loss: 0.448695]\n",
      "epoch:24 step:22595 [D loss: 0.221945, acc.: 64.84%] [G loss: 0.438776]\n",
      "epoch:24 step:22596 [D loss: 0.261818, acc.: 57.03%] [G loss: 0.453998]\n",
      "epoch:24 step:22597 [D loss: 0.227814, acc.: 62.50%] [G loss: 0.438216]\n",
      "epoch:24 step:22598 [D loss: 0.236510, acc.: 60.94%] [G loss: 0.418918]\n",
      "epoch:24 step:22599 [D loss: 0.204567, acc.: 64.84%] [G loss: 0.403787]\n",
      "epoch:24 step:22600 [D loss: 0.192635, acc.: 72.66%] [G loss: 0.427242]\n",
      "##############\n",
      "[2.56736161 2.02272811 6.11825544 4.82170402 3.81579028 5.67905217\n",
      " 4.38447209 4.80222272 4.53729159 3.87769777]\n",
      "##########\n",
      "epoch:24 step:22601 [D loss: 0.208571, acc.: 66.41%] [G loss: 0.450998]\n",
      "epoch:24 step:22602 [D loss: 0.232558, acc.: 60.16%] [G loss: 0.432173]\n",
      "epoch:24 step:22603 [D loss: 0.190589, acc.: 67.97%] [G loss: 0.432248]\n",
      "epoch:24 step:22604 [D loss: 0.203197, acc.: 69.53%] [G loss: 0.451473]\n",
      "epoch:24 step:22605 [D loss: 0.203686, acc.: 72.66%] [G loss: 0.487094]\n",
      "epoch:24 step:22606 [D loss: 0.225380, acc.: 64.84%] [G loss: 0.514130]\n",
      "epoch:24 step:22607 [D loss: 0.190920, acc.: 68.75%] [G loss: 0.525624]\n",
      "epoch:24 step:22608 [D loss: 0.259356, acc.: 57.81%] [G loss: 0.440626]\n",
      "epoch:24 step:22609 [D loss: 0.233430, acc.: 60.16%] [G loss: 0.420220]\n",
      "epoch:24 step:22610 [D loss: 0.194495, acc.: 71.88%] [G loss: 0.452826]\n",
      "epoch:24 step:22611 [D loss: 0.218895, acc.: 62.50%] [G loss: 0.461587]\n",
      "epoch:24 step:22612 [D loss: 0.248056, acc.: 54.69%] [G loss: 0.459912]\n",
      "epoch:24 step:22613 [D loss: 0.233497, acc.: 61.72%] [G loss: 0.482683]\n",
      "epoch:24 step:22614 [D loss: 0.221911, acc.: 67.19%] [G loss: 0.474423]\n",
      "epoch:24 step:22615 [D loss: 0.227254, acc.: 62.50%] [G loss: 0.426205]\n",
      "epoch:24 step:22616 [D loss: 0.244174, acc.: 57.03%] [G loss: 0.398312]\n",
      "epoch:24 step:22617 [D loss: 0.220148, acc.: 71.88%] [G loss: 0.433688]\n",
      "epoch:24 step:22618 [D loss: 0.216807, acc.: 65.62%] [G loss: 0.437846]\n",
      "epoch:24 step:22619 [D loss: 0.222134, acc.: 63.28%] [G loss: 0.379624]\n",
      "epoch:24 step:22620 [D loss: 0.227193, acc.: 64.06%] [G loss: 0.385118]\n",
      "epoch:24 step:22621 [D loss: 0.254527, acc.: 54.69%] [G loss: 0.430579]\n",
      "epoch:24 step:22622 [D loss: 0.222037, acc.: 66.41%] [G loss: 0.414844]\n",
      "epoch:24 step:22623 [D loss: 0.222302, acc.: 69.53%] [G loss: 0.454039]\n",
      "epoch:24 step:22624 [D loss: 0.216472, acc.: 64.84%] [G loss: 0.436045]\n",
      "epoch:24 step:22625 [D loss: 0.231169, acc.: 65.62%] [G loss: 0.424709]\n",
      "epoch:24 step:22626 [D loss: 0.228801, acc.: 59.38%] [G loss: 0.398513]\n",
      "epoch:24 step:22627 [D loss: 0.215669, acc.: 64.06%] [G loss: 0.409688]\n",
      "epoch:24 step:22628 [D loss: 0.219768, acc.: 64.84%] [G loss: 0.446186]\n",
      "epoch:24 step:22629 [D loss: 0.241392, acc.: 55.47%] [G loss: 0.387192]\n",
      "epoch:24 step:22630 [D loss: 0.228727, acc.: 65.62%] [G loss: 0.411123]\n",
      "epoch:24 step:22631 [D loss: 0.251609, acc.: 51.56%] [G loss: 0.398355]\n",
      "epoch:24 step:22632 [D loss: 0.216942, acc.: 65.62%] [G loss: 0.416748]\n",
      "epoch:24 step:22633 [D loss: 0.220010, acc.: 65.62%] [G loss: 0.407516]\n",
      "epoch:24 step:22634 [D loss: 0.230775, acc.: 60.16%] [G loss: 0.376143]\n",
      "epoch:24 step:22635 [D loss: 0.233382, acc.: 59.38%] [G loss: 0.405562]\n",
      "epoch:24 step:22636 [D loss: 0.228591, acc.: 63.28%] [G loss: 0.435840]\n",
      "epoch:24 step:22637 [D loss: 0.210525, acc.: 62.50%] [G loss: 0.446297]\n",
      "epoch:24 step:22638 [D loss: 0.236442, acc.: 60.16%] [G loss: 0.387071]\n",
      "epoch:24 step:22639 [D loss: 0.205701, acc.: 67.97%] [G loss: 0.436941]\n",
      "epoch:24 step:22640 [D loss: 0.221933, acc.: 64.06%] [G loss: 0.427125]\n",
      "epoch:24 step:22641 [D loss: 0.245908, acc.: 57.03%] [G loss: 0.439516]\n",
      "epoch:24 step:22642 [D loss: 0.218648, acc.: 63.28%] [G loss: 0.437124]\n",
      "epoch:24 step:22643 [D loss: 0.215952, acc.: 64.84%] [G loss: 0.427477]\n",
      "epoch:24 step:22644 [D loss: 0.211955, acc.: 64.06%] [G loss: 0.426764]\n",
      "epoch:24 step:22645 [D loss: 0.204717, acc.: 68.75%] [G loss: 0.431645]\n",
      "epoch:24 step:22646 [D loss: 0.232282, acc.: 61.72%] [G loss: 0.452564]\n",
      "epoch:24 step:22647 [D loss: 0.217489, acc.: 63.28%] [G loss: 0.471101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22648 [D loss: 0.264599, acc.: 57.81%] [G loss: 0.418660]\n",
      "epoch:24 step:22649 [D loss: 0.255190, acc.: 54.69%] [G loss: 0.438966]\n",
      "epoch:24 step:22650 [D loss: 0.195449, acc.: 69.53%] [G loss: 0.449741]\n",
      "epoch:24 step:22651 [D loss: 0.247897, acc.: 53.12%] [G loss: 0.425744]\n",
      "epoch:24 step:22652 [D loss: 0.211743, acc.: 67.97%] [G loss: 0.439568]\n",
      "epoch:24 step:22653 [D loss: 0.217289, acc.: 61.72%] [G loss: 0.441825]\n",
      "epoch:24 step:22654 [D loss: 0.219778, acc.: 66.41%] [G loss: 0.406811]\n",
      "epoch:24 step:22655 [D loss: 0.210335, acc.: 64.84%] [G loss: 0.445980]\n",
      "epoch:24 step:22656 [D loss: 0.224034, acc.: 67.19%] [G loss: 0.422476]\n",
      "epoch:24 step:22657 [D loss: 0.229253, acc.: 61.72%] [G loss: 0.430258]\n",
      "epoch:24 step:22658 [D loss: 0.229255, acc.: 64.06%] [G loss: 0.442188]\n",
      "epoch:24 step:22659 [D loss: 0.223794, acc.: 65.62%] [G loss: 0.429682]\n",
      "epoch:24 step:22660 [D loss: 0.227789, acc.: 64.84%] [G loss: 0.416486]\n",
      "epoch:24 step:22661 [D loss: 0.200826, acc.: 71.09%] [G loss: 0.399820]\n",
      "epoch:24 step:22662 [D loss: 0.273254, acc.: 47.66%] [G loss: 0.380988]\n",
      "epoch:24 step:22663 [D loss: 0.206130, acc.: 67.19%] [G loss: 0.444632]\n",
      "epoch:24 step:22664 [D loss: 0.222583, acc.: 61.72%] [G loss: 0.422661]\n",
      "epoch:24 step:22665 [D loss: 0.243021, acc.: 56.25%] [G loss: 0.414088]\n",
      "epoch:24 step:22666 [D loss: 0.237288, acc.: 60.94%] [G loss: 0.402843]\n",
      "epoch:24 step:22667 [D loss: 0.221411, acc.: 60.16%] [G loss: 0.413420]\n",
      "epoch:24 step:22668 [D loss: 0.237693, acc.: 58.59%] [G loss: 0.404376]\n",
      "epoch:24 step:22669 [D loss: 0.223594, acc.: 64.84%] [G loss: 0.418999]\n",
      "epoch:24 step:22670 [D loss: 0.224847, acc.: 64.84%] [G loss: 0.437900]\n",
      "epoch:24 step:22671 [D loss: 0.201334, acc.: 73.44%] [G loss: 0.433974]\n",
      "epoch:24 step:22672 [D loss: 0.241200, acc.: 56.25%] [G loss: 0.404744]\n",
      "epoch:24 step:22673 [D loss: 0.220912, acc.: 65.62%] [G loss: 0.442857]\n",
      "epoch:24 step:22674 [D loss: 0.232476, acc.: 63.28%] [G loss: 0.439268]\n",
      "epoch:24 step:22675 [D loss: 0.242688, acc.: 57.03%] [G loss: 0.407286]\n",
      "epoch:24 step:22676 [D loss: 0.253508, acc.: 53.12%] [G loss: 0.383422]\n",
      "epoch:24 step:22677 [D loss: 0.257715, acc.: 56.25%] [G loss: 0.392639]\n",
      "epoch:24 step:22678 [D loss: 0.221522, acc.: 63.28%] [G loss: 0.406419]\n",
      "epoch:24 step:22679 [D loss: 0.228039, acc.: 64.06%] [G loss: 0.406830]\n",
      "epoch:24 step:22680 [D loss: 0.203840, acc.: 69.53%] [G loss: 0.420239]\n",
      "epoch:24 step:22681 [D loss: 0.221883, acc.: 60.94%] [G loss: 0.408522]\n",
      "epoch:24 step:22682 [D loss: 0.219683, acc.: 63.28%] [G loss: 0.447689]\n",
      "epoch:24 step:22683 [D loss: 0.217159, acc.: 62.50%] [G loss: 0.420961]\n",
      "epoch:24 step:22684 [D loss: 0.252571, acc.: 58.59%] [G loss: 0.414883]\n",
      "epoch:24 step:22685 [D loss: 0.222099, acc.: 67.97%] [G loss: 0.419635]\n",
      "epoch:24 step:22686 [D loss: 0.207070, acc.: 68.75%] [G loss: 0.438841]\n",
      "epoch:24 step:22687 [D loss: 0.225403, acc.: 62.50%] [G loss: 0.428651]\n",
      "epoch:24 step:22688 [D loss: 0.273729, acc.: 49.22%] [G loss: 0.411834]\n",
      "epoch:24 step:22689 [D loss: 0.214853, acc.: 70.31%] [G loss: 0.453250]\n",
      "epoch:24 step:22690 [D loss: 0.244239, acc.: 60.16%] [G loss: 0.410470]\n",
      "epoch:24 step:22691 [D loss: 0.261562, acc.: 54.69%] [G loss: 0.452179]\n",
      "epoch:24 step:22692 [D loss: 0.212214, acc.: 65.62%] [G loss: 0.411195]\n",
      "epoch:24 step:22693 [D loss: 0.249581, acc.: 60.94%] [G loss: 0.419768]\n",
      "epoch:24 step:22694 [D loss: 0.206060, acc.: 69.53%] [G loss: 0.421529]\n",
      "epoch:24 step:22695 [D loss: 0.225379, acc.: 63.28%] [G loss: 0.432036]\n",
      "epoch:24 step:22696 [D loss: 0.181807, acc.: 67.97%] [G loss: 0.454210]\n",
      "epoch:24 step:22697 [D loss: 0.179893, acc.: 72.66%] [G loss: 0.475871]\n",
      "epoch:24 step:22698 [D loss: 0.270938, acc.: 51.56%] [G loss: 0.416535]\n",
      "epoch:24 step:22699 [D loss: 0.268091, acc.: 53.91%] [G loss: 0.431076]\n",
      "epoch:24 step:22700 [D loss: 0.237569, acc.: 61.72%] [G loss: 0.410497]\n",
      "epoch:24 step:22701 [D loss: 0.227704, acc.: 58.59%] [G loss: 0.396858]\n",
      "epoch:24 step:22702 [D loss: 0.256708, acc.: 47.66%] [G loss: 0.370713]\n",
      "epoch:24 step:22703 [D loss: 0.248885, acc.: 53.91%] [G loss: 0.372065]\n",
      "epoch:24 step:22704 [D loss: 0.224209, acc.: 64.06%] [G loss: 0.415654]\n",
      "epoch:24 step:22705 [D loss: 0.224890, acc.: 58.59%] [G loss: 0.411737]\n",
      "epoch:24 step:22706 [D loss: 0.197061, acc.: 69.53%] [G loss: 0.422994]\n",
      "epoch:24 step:22707 [D loss: 0.183749, acc.: 75.00%] [G loss: 0.482933]\n",
      "epoch:24 step:22708 [D loss: 0.300169, acc.: 52.34%] [G loss: 0.398476]\n",
      "epoch:24 step:22709 [D loss: 0.240619, acc.: 61.72%] [G loss: 0.423368]\n",
      "epoch:24 step:22710 [D loss: 0.213949, acc.: 62.50%] [G loss: 0.493852]\n",
      "epoch:24 step:22711 [D loss: 0.202220, acc.: 70.31%] [G loss: 0.496116]\n",
      "epoch:24 step:22712 [D loss: 0.256799, acc.: 54.69%] [G loss: 0.399066]\n",
      "epoch:24 step:22713 [D loss: 0.230915, acc.: 60.16%] [G loss: 0.366657]\n",
      "epoch:24 step:22714 [D loss: 0.228195, acc.: 63.28%] [G loss: 0.384340]\n",
      "epoch:24 step:22715 [D loss: 0.212086, acc.: 67.97%] [G loss: 0.428939]\n",
      "epoch:24 step:22716 [D loss: 0.243305, acc.: 57.03%] [G loss: 0.399894]\n",
      "epoch:24 step:22717 [D loss: 0.199303, acc.: 71.09%] [G loss: 0.456000]\n",
      "epoch:24 step:22718 [D loss: 0.226878, acc.: 63.28%] [G loss: 0.393984]\n",
      "epoch:24 step:22719 [D loss: 0.185666, acc.: 72.66%] [G loss: 0.475298]\n",
      "epoch:24 step:22720 [D loss: 0.186398, acc.: 74.22%] [G loss: 0.552557]\n",
      "epoch:24 step:22721 [D loss: 0.267166, acc.: 51.56%] [G loss: 0.480231]\n",
      "epoch:24 step:22722 [D loss: 0.250032, acc.: 55.47%] [G loss: 0.419298]\n",
      "epoch:24 step:22723 [D loss: 0.238524, acc.: 64.06%] [G loss: 0.399429]\n",
      "epoch:24 step:22724 [D loss: 0.208097, acc.: 67.19%] [G loss: 0.445085]\n",
      "epoch:24 step:22725 [D loss: 0.218955, acc.: 63.28%] [G loss: 0.439270]\n",
      "epoch:24 step:22726 [D loss: 0.204696, acc.: 70.31%] [G loss: 0.426461]\n",
      "epoch:24 step:22727 [D loss: 0.221486, acc.: 61.72%] [G loss: 0.429020]\n",
      "epoch:24 step:22728 [D loss: 0.244697, acc.: 57.81%] [G loss: 0.406956]\n",
      "epoch:24 step:22729 [D loss: 0.206512, acc.: 69.53%] [G loss: 0.448847]\n",
      "epoch:24 step:22730 [D loss: 0.201405, acc.: 71.88%] [G loss: 0.481907]\n",
      "epoch:24 step:22731 [D loss: 0.212737, acc.: 65.62%] [G loss: 0.486264]\n",
      "epoch:24 step:22732 [D loss: 0.198254, acc.: 70.31%] [G loss: 0.443700]\n",
      "epoch:24 step:22733 [D loss: 0.215288, acc.: 67.97%] [G loss: 0.451395]\n",
      "epoch:24 step:22734 [D loss: 0.200194, acc.: 66.41%] [G loss: 0.457266]\n",
      "epoch:24 step:22735 [D loss: 0.239028, acc.: 61.72%] [G loss: 0.443524]\n",
      "epoch:24 step:22736 [D loss: 0.203677, acc.: 66.41%] [G loss: 0.438132]\n",
      "epoch:24 step:22737 [D loss: 0.249377, acc.: 56.25%] [G loss: 0.429885]\n",
      "epoch:24 step:22738 [D loss: 0.258281, acc.: 56.25%] [G loss: 0.417468]\n",
      "epoch:24 step:22739 [D loss: 0.268670, acc.: 53.12%] [G loss: 0.399405]\n",
      "epoch:24 step:22740 [D loss: 0.229939, acc.: 60.94%] [G loss: 0.398654]\n",
      "epoch:24 step:22741 [D loss: 0.241320, acc.: 61.72%] [G loss: 0.443804]\n",
      "epoch:24 step:22742 [D loss: 0.237806, acc.: 61.72%] [G loss: 0.438336]\n",
      "epoch:24 step:22743 [D loss: 0.224345, acc.: 63.28%] [G loss: 0.461534]\n",
      "epoch:24 step:22744 [D loss: 0.219064, acc.: 63.28%] [G loss: 0.437012]\n",
      "epoch:24 step:22745 [D loss: 0.237022, acc.: 55.47%] [G loss: 0.436320]\n",
      "epoch:24 step:22746 [D loss: 0.200064, acc.: 67.97%] [G loss: 0.400780]\n",
      "epoch:24 step:22747 [D loss: 0.215596, acc.: 67.19%] [G loss: 0.412182]\n",
      "epoch:24 step:22748 [D loss: 0.224585, acc.: 62.50%] [G loss: 0.421479]\n",
      "epoch:24 step:22749 [D loss: 0.224587, acc.: 61.72%] [G loss: 0.439985]\n",
      "epoch:24 step:22750 [D loss: 0.211381, acc.: 64.06%] [G loss: 0.438811]\n",
      "epoch:24 step:22751 [D loss: 0.253260, acc.: 57.03%] [G loss: 0.453610]\n",
      "epoch:24 step:22752 [D loss: 0.213882, acc.: 68.75%] [G loss: 0.446534]\n",
      "epoch:24 step:22753 [D loss: 0.248279, acc.: 58.59%] [G loss: 0.436264]\n",
      "epoch:24 step:22754 [D loss: 0.240816, acc.: 58.59%] [G loss: 0.394492]\n",
      "epoch:24 step:22755 [D loss: 0.205419, acc.: 68.75%] [G loss: 0.420902]\n",
      "epoch:24 step:22756 [D loss: 0.215376, acc.: 66.41%] [G loss: 0.397560]\n",
      "epoch:24 step:22757 [D loss: 0.216762, acc.: 65.62%] [G loss: 0.436934]\n",
      "epoch:24 step:22758 [D loss: 0.224619, acc.: 64.06%] [G loss: 0.435938]\n",
      "epoch:24 step:22759 [D loss: 0.192947, acc.: 76.56%] [G loss: 0.460642]\n",
      "epoch:24 step:22760 [D loss: 0.225427, acc.: 64.84%] [G loss: 0.480643]\n",
      "epoch:24 step:22761 [D loss: 0.208729, acc.: 67.97%] [G loss: 0.423545]\n",
      "epoch:24 step:22762 [D loss: 0.192756, acc.: 70.31%] [G loss: 0.476399]\n",
      "epoch:24 step:22763 [D loss: 0.205867, acc.: 64.84%] [G loss: 0.453065]\n",
      "epoch:24 step:22764 [D loss: 0.197044, acc.: 69.53%] [G loss: 0.431131]\n",
      "epoch:24 step:22765 [D loss: 0.243025, acc.: 61.72%] [G loss: 0.435564]\n",
      "epoch:24 step:22766 [D loss: 0.255055, acc.: 53.91%] [G loss: 0.383703]\n",
      "epoch:24 step:22767 [D loss: 0.235351, acc.: 62.50%] [G loss: 0.441144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22768 [D loss: 0.201012, acc.: 64.84%] [G loss: 0.424944]\n",
      "epoch:24 step:22769 [D loss: 0.244783, acc.: 54.69%] [G loss: 0.439933]\n",
      "epoch:24 step:22770 [D loss: 0.238736, acc.: 55.47%] [G loss: 0.427866]\n",
      "epoch:24 step:22771 [D loss: 0.204420, acc.: 69.53%] [G loss: 0.400115]\n",
      "epoch:24 step:22772 [D loss: 0.204928, acc.: 64.84%] [G loss: 0.461943]\n",
      "epoch:24 step:22773 [D loss: 0.212874, acc.: 62.50%] [G loss: 0.390691]\n",
      "epoch:24 step:22774 [D loss: 0.223705, acc.: 63.28%] [G loss: 0.411980]\n",
      "epoch:24 step:22775 [D loss: 0.242135, acc.: 51.56%] [G loss: 0.432388]\n",
      "epoch:24 step:22776 [D loss: 0.221564, acc.: 65.62%] [G loss: 0.426498]\n",
      "epoch:24 step:22777 [D loss: 0.212745, acc.: 67.97%] [G loss: 0.465705]\n",
      "epoch:24 step:22778 [D loss: 0.244622, acc.: 58.59%] [G loss: 0.446042]\n",
      "epoch:24 step:22779 [D loss: 0.247120, acc.: 55.47%] [G loss: 0.450728]\n",
      "epoch:24 step:22780 [D loss: 0.228715, acc.: 62.50%] [G loss: 0.420710]\n",
      "epoch:24 step:22781 [D loss: 0.210479, acc.: 69.53%] [G loss: 0.434420]\n",
      "epoch:24 step:22782 [D loss: 0.219483, acc.: 62.50%] [G loss: 0.422038]\n",
      "epoch:24 step:22783 [D loss: 0.245765, acc.: 58.59%] [G loss: 0.378016]\n",
      "epoch:24 step:22784 [D loss: 0.200145, acc.: 66.41%] [G loss: 0.432047]\n",
      "epoch:24 step:22785 [D loss: 0.227276, acc.: 58.59%] [G loss: 0.416584]\n",
      "epoch:24 step:22786 [D loss: 0.206298, acc.: 70.31%] [G loss: 0.431571]\n",
      "epoch:24 step:22787 [D loss: 0.204000, acc.: 67.97%] [G loss: 0.447788]\n",
      "epoch:24 step:22788 [D loss: 0.208954, acc.: 65.62%] [G loss: 0.445508]\n",
      "epoch:24 step:22789 [D loss: 0.278847, acc.: 52.34%] [G loss: 0.440259]\n",
      "epoch:24 step:22790 [D loss: 0.219806, acc.: 65.62%] [G loss: 0.455199]\n",
      "epoch:24 step:22791 [D loss: 0.231275, acc.: 63.28%] [G loss: 0.413179]\n",
      "epoch:24 step:22792 [D loss: 0.227112, acc.: 64.06%] [G loss: 0.408965]\n",
      "epoch:24 step:22793 [D loss: 0.216597, acc.: 62.50%] [G loss: 0.420018]\n",
      "epoch:24 step:22794 [D loss: 0.239350, acc.: 56.25%] [G loss: 0.443742]\n",
      "epoch:24 step:22795 [D loss: 0.214737, acc.: 66.41%] [G loss: 0.403612]\n",
      "epoch:24 step:22796 [D loss: 0.222885, acc.: 61.72%] [G loss: 0.405908]\n",
      "epoch:24 step:22797 [D loss: 0.200818, acc.: 69.53%] [G loss: 0.412700]\n",
      "epoch:24 step:22798 [D loss: 0.208400, acc.: 71.88%] [G loss: 0.431799]\n",
      "epoch:24 step:22799 [D loss: 0.215146, acc.: 67.97%] [G loss: 0.422189]\n",
      "epoch:24 step:22800 [D loss: 0.185904, acc.: 74.22%] [G loss: 0.482435]\n",
      "##############\n",
      "[2.48321757 1.84147533 6.02170691 4.70397167 3.44981934 5.53934162\n",
      " 4.39429735 5.00474787 4.31279449 3.97685241]\n",
      "##########\n",
      "epoch:24 step:22801 [D loss: 0.200604, acc.: 65.62%] [G loss: 0.487413]\n",
      "epoch:24 step:22802 [D loss: 0.202626, acc.: 71.88%] [G loss: 0.477430]\n",
      "epoch:24 step:22803 [D loss: 0.189710, acc.: 67.97%] [G loss: 0.495289]\n",
      "epoch:24 step:22804 [D loss: 0.255227, acc.: 55.47%] [G loss: 0.452961]\n",
      "epoch:24 step:22805 [D loss: 0.243943, acc.: 60.94%] [G loss: 0.406436]\n",
      "epoch:24 step:22806 [D loss: 0.211009, acc.: 70.31%] [G loss: 0.432626]\n",
      "epoch:24 step:22807 [D loss: 0.223981, acc.: 62.50%] [G loss: 0.425278]\n",
      "epoch:24 step:22808 [D loss: 0.207490, acc.: 71.09%] [G loss: 0.446590]\n",
      "epoch:24 step:22809 [D loss: 0.234594, acc.: 63.28%] [G loss: 0.419114]\n",
      "epoch:24 step:22810 [D loss: 0.240208, acc.: 61.72%] [G loss: 0.437808]\n",
      "epoch:24 step:22811 [D loss: 0.266019, acc.: 54.69%] [G loss: 0.399290]\n",
      "epoch:24 step:22812 [D loss: 0.216035, acc.: 63.28%] [G loss: 0.432674]\n",
      "epoch:24 step:22813 [D loss: 0.229008, acc.: 59.38%] [G loss: 0.413255]\n",
      "epoch:24 step:22814 [D loss: 0.238656, acc.: 61.72%] [G loss: 0.404928]\n",
      "epoch:24 step:22815 [D loss: 0.209910, acc.: 71.09%] [G loss: 0.414033]\n",
      "epoch:24 step:22816 [D loss: 0.225191, acc.: 61.72%] [G loss: 0.431604]\n",
      "epoch:24 step:22817 [D loss: 0.253833, acc.: 53.12%] [G loss: 0.433207]\n",
      "epoch:24 step:22818 [D loss: 0.205087, acc.: 71.09%] [G loss: 0.399787]\n",
      "epoch:24 step:22819 [D loss: 0.220221, acc.: 65.62%] [G loss: 0.428528]\n",
      "epoch:24 step:22820 [D loss: 0.187688, acc.: 74.22%] [G loss: 0.459261]\n",
      "epoch:24 step:22821 [D loss: 0.234769, acc.: 59.38%] [G loss: 0.383897]\n",
      "epoch:24 step:22822 [D loss: 0.216088, acc.: 60.94%] [G loss: 0.443430]\n",
      "epoch:24 step:22823 [D loss: 0.207141, acc.: 63.28%] [G loss: 0.437057]\n",
      "epoch:24 step:22824 [D loss: 0.202933, acc.: 69.53%] [G loss: 0.472810]\n",
      "epoch:24 step:22825 [D loss: 0.225793, acc.: 67.19%] [G loss: 0.458896]\n",
      "epoch:24 step:22826 [D loss: 0.198300, acc.: 67.97%] [G loss: 0.456067]\n",
      "epoch:24 step:22827 [D loss: 0.218776, acc.: 67.19%] [G loss: 0.443258]\n",
      "epoch:24 step:22828 [D loss: 0.225031, acc.: 63.28%] [G loss: 0.448886]\n",
      "epoch:24 step:22829 [D loss: 0.279495, acc.: 58.59%] [G loss: 0.444523]\n",
      "epoch:24 step:22830 [D loss: 0.240007, acc.: 57.81%] [G loss: 0.461206]\n",
      "epoch:24 step:22831 [D loss: 0.213259, acc.: 65.62%] [G loss: 0.438201]\n",
      "epoch:24 step:22832 [D loss: 0.219731, acc.: 64.06%] [G loss: 0.493547]\n",
      "epoch:24 step:22833 [D loss: 0.205117, acc.: 69.53%] [G loss: 0.463885]\n",
      "epoch:24 step:22834 [D loss: 0.195421, acc.: 69.53%] [G loss: 0.461500]\n",
      "epoch:24 step:22835 [D loss: 0.165182, acc.: 80.47%] [G loss: 0.531801]\n",
      "epoch:24 step:22836 [D loss: 0.274367, acc.: 56.25%] [G loss: 0.460996]\n",
      "epoch:24 step:22837 [D loss: 0.274233, acc.: 53.91%] [G loss: 0.435868]\n",
      "epoch:24 step:22838 [D loss: 0.208441, acc.: 67.19%] [G loss: 0.376672]\n",
      "epoch:24 step:22839 [D loss: 0.254635, acc.: 56.25%] [G loss: 0.376302]\n",
      "epoch:24 step:22840 [D loss: 0.216185, acc.: 66.41%] [G loss: 0.408292]\n",
      "epoch:24 step:22841 [D loss: 0.223617, acc.: 62.50%] [G loss: 0.428330]\n",
      "epoch:24 step:22842 [D loss: 0.194269, acc.: 73.44%] [G loss: 0.467358]\n",
      "epoch:24 step:22843 [D loss: 0.238653, acc.: 58.59%] [G loss: 0.416526]\n",
      "epoch:24 step:22844 [D loss: 0.204529, acc.: 67.19%] [G loss: 0.425760]\n",
      "epoch:24 step:22845 [D loss: 0.224053, acc.: 64.06%] [G loss: 0.411508]\n",
      "epoch:24 step:22846 [D loss: 0.200859, acc.: 68.75%] [G loss: 0.424471]\n",
      "epoch:24 step:22847 [D loss: 0.187962, acc.: 72.66%] [G loss: 0.457384]\n",
      "epoch:24 step:22848 [D loss: 0.220362, acc.: 66.41%] [G loss: 0.437460]\n",
      "epoch:24 step:22849 [D loss: 0.225376, acc.: 64.84%] [G loss: 0.468896]\n",
      "epoch:24 step:22850 [D loss: 0.252095, acc.: 54.69%] [G loss: 0.426198]\n",
      "epoch:24 step:22851 [D loss: 0.225345, acc.: 60.94%] [G loss: 0.412295]\n",
      "epoch:24 step:22852 [D loss: 0.228176, acc.: 55.47%] [G loss: 0.409396]\n",
      "epoch:24 step:22853 [D loss: 0.233325, acc.: 62.50%] [G loss: 0.413294]\n",
      "epoch:24 step:22854 [D loss: 0.225778, acc.: 61.72%] [G loss: 0.438351]\n",
      "epoch:24 step:22855 [D loss: 0.228754, acc.: 62.50%] [G loss: 0.437513]\n",
      "epoch:24 step:22856 [D loss: 0.225643, acc.: 60.16%] [G loss: 0.386957]\n",
      "epoch:24 step:22857 [D loss: 0.251882, acc.: 57.81%] [G loss: 0.412148]\n",
      "epoch:24 step:22858 [D loss: 0.195718, acc.: 72.66%] [G loss: 0.440197]\n",
      "epoch:24 step:22859 [D loss: 0.220362, acc.: 60.16%] [G loss: 0.443958]\n",
      "epoch:24 step:22860 [D loss: 0.241533, acc.: 57.03%] [G loss: 0.418220]\n",
      "epoch:24 step:22861 [D loss: 0.235824, acc.: 63.28%] [G loss: 0.427445]\n",
      "epoch:24 step:22862 [D loss: 0.199119, acc.: 71.88%] [G loss: 0.405899]\n",
      "epoch:24 step:22863 [D loss: 0.221576, acc.: 61.72%] [G loss: 0.426983]\n",
      "epoch:24 step:22864 [D loss: 0.248568, acc.: 54.69%] [G loss: 0.423369]\n",
      "epoch:24 step:22865 [D loss: 0.244878, acc.: 59.38%] [G loss: 0.456521]\n",
      "epoch:24 step:22866 [D loss: 0.232905, acc.: 67.97%] [G loss: 0.410470]\n",
      "epoch:24 step:22867 [D loss: 0.229469, acc.: 59.38%] [G loss: 0.428142]\n",
      "epoch:24 step:22868 [D loss: 0.241204, acc.: 60.16%] [G loss: 0.404319]\n",
      "epoch:24 step:22869 [D loss: 0.217199, acc.: 62.50%] [G loss: 0.403394]\n",
      "epoch:24 step:22870 [D loss: 0.227001, acc.: 61.72%] [G loss: 0.424195]\n",
      "epoch:24 step:22871 [D loss: 0.212608, acc.: 67.97%] [G loss: 0.415578]\n",
      "epoch:24 step:22872 [D loss: 0.213766, acc.: 65.62%] [G loss: 0.447363]\n",
      "epoch:24 step:22873 [D loss: 0.175471, acc.: 70.31%] [G loss: 0.468352]\n",
      "epoch:24 step:22874 [D loss: 0.248008, acc.: 52.34%] [G loss: 0.430103]\n",
      "epoch:24 step:22875 [D loss: 0.214000, acc.: 60.94%] [G loss: 0.421932]\n",
      "epoch:24 step:22876 [D loss: 0.200592, acc.: 71.09%] [G loss: 0.437346]\n",
      "epoch:24 step:22877 [D loss: 0.225621, acc.: 61.72%] [G loss: 0.423349]\n",
      "epoch:24 step:22878 [D loss: 0.249110, acc.: 59.38%] [G loss: 0.421799]\n",
      "epoch:24 step:22879 [D loss: 0.211446, acc.: 65.62%] [G loss: 0.428156]\n",
      "epoch:24 step:22880 [D loss: 0.225864, acc.: 67.97%] [G loss: 0.410726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22881 [D loss: 0.214954, acc.: 66.41%] [G loss: 0.438551]\n",
      "epoch:24 step:22882 [D loss: 0.228733, acc.: 64.84%] [G loss: 0.450560]\n",
      "epoch:24 step:22883 [D loss: 0.210645, acc.: 62.50%] [G loss: 0.435068]\n",
      "epoch:24 step:22884 [D loss: 0.247322, acc.: 56.25%] [G loss: 0.441622]\n",
      "epoch:24 step:22885 [D loss: 0.235971, acc.: 64.84%] [G loss: 0.422274]\n",
      "epoch:24 step:22886 [D loss: 0.215569, acc.: 60.16%] [G loss: 0.459722]\n",
      "epoch:24 step:22887 [D loss: 0.198374, acc.: 68.75%] [G loss: 0.448600]\n",
      "epoch:24 step:22888 [D loss: 0.228693, acc.: 57.03%] [G loss: 0.407606]\n",
      "epoch:24 step:22889 [D loss: 0.250114, acc.: 53.91%] [G loss: 0.405097]\n",
      "epoch:24 step:22890 [D loss: 0.227729, acc.: 68.75%] [G loss: 0.425748]\n",
      "epoch:24 step:22891 [D loss: 0.212564, acc.: 64.06%] [G loss: 0.440975]\n",
      "epoch:24 step:22892 [D loss: 0.240086, acc.: 58.59%] [G loss: 0.433501]\n",
      "epoch:24 step:22893 [D loss: 0.220941, acc.: 66.41%] [G loss: 0.464606]\n",
      "epoch:24 step:22894 [D loss: 0.179568, acc.: 70.31%] [G loss: 0.502118]\n",
      "epoch:24 step:22895 [D loss: 0.220817, acc.: 61.72%] [G loss: 0.469736]\n",
      "epoch:24 step:22896 [D loss: 0.261190, acc.: 52.34%] [G loss: 0.431549]\n",
      "epoch:24 step:22897 [D loss: 0.229950, acc.: 64.06%] [G loss: 0.429468]\n",
      "epoch:24 step:22898 [D loss: 0.235763, acc.: 57.81%] [G loss: 0.385006]\n",
      "epoch:24 step:22899 [D loss: 0.253981, acc.: 56.25%] [G loss: 0.410738]\n",
      "epoch:24 step:22900 [D loss: 0.228955, acc.: 60.16%] [G loss: 0.426566]\n",
      "epoch:24 step:22901 [D loss: 0.243219, acc.: 58.59%] [G loss: 0.403373]\n",
      "epoch:24 step:22902 [D loss: 0.223483, acc.: 60.16%] [G loss: 0.405946]\n",
      "epoch:24 step:22903 [D loss: 0.217253, acc.: 67.97%] [G loss: 0.429692]\n",
      "epoch:24 step:22904 [D loss: 0.206367, acc.: 71.09%] [G loss: 0.463689]\n",
      "epoch:24 step:22905 [D loss: 0.241846, acc.: 62.50%] [G loss: 0.425094]\n",
      "epoch:24 step:22906 [D loss: 0.235113, acc.: 57.81%] [G loss: 0.404724]\n",
      "epoch:24 step:22907 [D loss: 0.258877, acc.: 54.69%] [G loss: 0.396996]\n",
      "epoch:24 step:22908 [D loss: 0.242102, acc.: 60.16%] [G loss: 0.443005]\n",
      "epoch:24 step:22909 [D loss: 0.271563, acc.: 56.25%] [G loss: 0.410809]\n",
      "epoch:24 step:22910 [D loss: 0.239779, acc.: 58.59%] [G loss: 0.419998]\n",
      "epoch:24 step:22911 [D loss: 0.226648, acc.: 61.72%] [G loss: 0.407535]\n",
      "epoch:24 step:22912 [D loss: 0.265359, acc.: 50.00%] [G loss: 0.423365]\n",
      "epoch:24 step:22913 [D loss: 0.226247, acc.: 60.94%] [G loss: 0.452172]\n",
      "epoch:24 step:22914 [D loss: 0.236262, acc.: 61.72%] [G loss: 0.406009]\n",
      "epoch:24 step:22915 [D loss: 0.205778, acc.: 67.19%] [G loss: 0.456907]\n",
      "epoch:24 step:22916 [D loss: 0.221106, acc.: 63.28%] [G loss: 0.461379]\n",
      "epoch:24 step:22917 [D loss: 0.174851, acc.: 75.00%] [G loss: 0.513707]\n",
      "epoch:24 step:22918 [D loss: 0.186297, acc.: 75.78%] [G loss: 0.449127]\n",
      "epoch:24 step:22919 [D loss: 0.264482, acc.: 55.47%] [G loss: 0.436789]\n",
      "epoch:24 step:22920 [D loss: 0.245107, acc.: 57.03%] [G loss: 0.425409]\n",
      "epoch:24 step:22921 [D loss: 0.206391, acc.: 64.84%] [G loss: 0.480456]\n",
      "epoch:24 step:22922 [D loss: 0.203051, acc.: 71.88%] [G loss: 0.491060]\n",
      "epoch:24 step:22923 [D loss: 0.219085, acc.: 67.19%] [G loss: 0.465608]\n",
      "epoch:24 step:22924 [D loss: 0.203623, acc.: 67.97%] [G loss: 0.463100]\n",
      "epoch:24 step:22925 [D loss: 0.266262, acc.: 49.22%] [G loss: 0.455050]\n",
      "epoch:24 step:22926 [D loss: 0.239287, acc.: 56.25%] [G loss: 0.406466]\n",
      "epoch:24 step:22927 [D loss: 0.234530, acc.: 58.59%] [G loss: 0.459978]\n",
      "epoch:24 step:22928 [D loss: 0.214282, acc.: 64.84%] [G loss: 0.428180]\n",
      "epoch:24 step:22929 [D loss: 0.214024, acc.: 65.62%] [G loss: 0.453276]\n",
      "epoch:24 step:22930 [D loss: 0.240767, acc.: 57.81%] [G loss: 0.435137]\n",
      "epoch:24 step:22931 [D loss: 0.249188, acc.: 57.03%] [G loss: 0.439139]\n",
      "epoch:24 step:22932 [D loss: 0.222037, acc.: 58.59%] [G loss: 0.415708]\n",
      "epoch:24 step:22933 [D loss: 0.212729, acc.: 61.72%] [G loss: 0.463109]\n",
      "epoch:24 step:22934 [D loss: 0.234092, acc.: 60.16%] [G loss: 0.406008]\n",
      "epoch:24 step:22935 [D loss: 0.204939, acc.: 67.97%] [G loss: 0.430434]\n",
      "epoch:24 step:22936 [D loss: 0.240702, acc.: 59.38%] [G loss: 0.430677]\n",
      "epoch:24 step:22937 [D loss: 0.223337, acc.: 61.72%] [G loss: 0.412250]\n",
      "epoch:24 step:22938 [D loss: 0.205721, acc.: 62.50%] [G loss: 0.438794]\n",
      "epoch:24 step:22939 [D loss: 0.193810, acc.: 67.97%] [G loss: 0.421344]\n",
      "epoch:24 step:22940 [D loss: 0.203564, acc.: 68.75%] [G loss: 0.399225]\n",
      "epoch:24 step:22941 [D loss: 0.212198, acc.: 61.72%] [G loss: 0.409200]\n",
      "epoch:24 step:22942 [D loss: 0.200028, acc.: 71.09%] [G loss: 0.476077]\n",
      "epoch:24 step:22943 [D loss: 0.237968, acc.: 60.16%] [G loss: 0.429312]\n",
      "epoch:24 step:22944 [D loss: 0.225534, acc.: 59.38%] [G loss: 0.432930]\n",
      "epoch:24 step:22945 [D loss: 0.204820, acc.: 70.31%] [G loss: 0.446948]\n",
      "epoch:24 step:22946 [D loss: 0.265265, acc.: 54.69%] [G loss: 0.418973]\n",
      "epoch:24 step:22947 [D loss: 0.239113, acc.: 57.81%] [G loss: 0.442206]\n",
      "epoch:24 step:22948 [D loss: 0.247954, acc.: 57.81%] [G loss: 0.380873]\n",
      "epoch:24 step:22949 [D loss: 0.210232, acc.: 64.84%] [G loss: 0.453341]\n",
      "epoch:24 step:22950 [D loss: 0.262031, acc.: 50.78%] [G loss: 0.372433]\n",
      "epoch:24 step:22951 [D loss: 0.225843, acc.: 62.50%] [G loss: 0.382140]\n",
      "epoch:24 step:22952 [D loss: 0.238233, acc.: 58.59%] [G loss: 0.379752]\n",
      "epoch:24 step:22953 [D loss: 0.241387, acc.: 53.12%] [G loss: 0.386887]\n",
      "epoch:24 step:22954 [D loss: 0.220018, acc.: 64.84%] [G loss: 0.442161]\n",
      "epoch:24 step:22955 [D loss: 0.216926, acc.: 64.06%] [G loss: 0.442004]\n",
      "epoch:24 step:22956 [D loss: 0.225238, acc.: 64.84%] [G loss: 0.432312]\n",
      "epoch:24 step:22957 [D loss: 0.215722, acc.: 63.28%] [G loss: 0.427978]\n",
      "epoch:24 step:22958 [D loss: 0.207394, acc.: 65.62%] [G loss: 0.467375]\n",
      "epoch:24 step:22959 [D loss: 0.183328, acc.: 75.00%] [G loss: 0.451076]\n",
      "epoch:24 step:22960 [D loss: 0.205238, acc.: 67.97%] [G loss: 0.475076]\n",
      "epoch:24 step:22961 [D loss: 0.275308, acc.: 53.12%] [G loss: 0.417077]\n",
      "epoch:24 step:22962 [D loss: 0.205228, acc.: 67.19%] [G loss: 0.467490]\n",
      "epoch:24 step:22963 [D loss: 0.211510, acc.: 67.19%] [G loss: 0.487522]\n",
      "epoch:24 step:22964 [D loss: 0.225913, acc.: 64.06%] [G loss: 0.474572]\n",
      "epoch:24 step:22965 [D loss: 0.235005, acc.: 57.03%] [G loss: 0.451303]\n",
      "epoch:24 step:22966 [D loss: 0.236048, acc.: 59.38%] [G loss: 0.391140]\n",
      "epoch:24 step:22967 [D loss: 0.235809, acc.: 63.28%] [G loss: 0.415379]\n",
      "epoch:24 step:22968 [D loss: 0.199171, acc.: 71.88%] [G loss: 0.412453]\n",
      "epoch:24 step:22969 [D loss: 0.180423, acc.: 73.44%] [G loss: 0.456497]\n",
      "epoch:24 step:22970 [D loss: 0.245336, acc.: 52.34%] [G loss: 0.409547]\n",
      "epoch:24 step:22971 [D loss: 0.247689, acc.: 53.91%] [G loss: 0.404831]\n",
      "epoch:24 step:22972 [D loss: 0.199866, acc.: 72.66%] [G loss: 0.399541]\n",
      "epoch:24 step:22973 [D loss: 0.203270, acc.: 67.19%] [G loss: 0.472417]\n",
      "epoch:24 step:22974 [D loss: 0.235300, acc.: 61.72%] [G loss: 0.424187]\n",
      "epoch:24 step:22975 [D loss: 0.244241, acc.: 56.25%] [G loss: 0.402884]\n",
      "epoch:24 step:22976 [D loss: 0.199868, acc.: 68.75%] [G loss: 0.447822]\n",
      "epoch:24 step:22977 [D loss: 0.250261, acc.: 53.12%] [G loss: 0.418128]\n",
      "epoch:24 step:22978 [D loss: 0.223125, acc.: 65.62%] [G loss: 0.431782]\n",
      "epoch:24 step:22979 [D loss: 0.232189, acc.: 61.72%] [G loss: 0.433207]\n",
      "epoch:24 step:22980 [D loss: 0.239491, acc.: 59.38%] [G loss: 0.415822]\n",
      "epoch:24 step:22981 [D loss: 0.215672, acc.: 64.84%] [G loss: 0.402043]\n",
      "epoch:24 step:22982 [D loss: 0.235492, acc.: 61.72%] [G loss: 0.420816]\n",
      "epoch:24 step:22983 [D loss: 0.188066, acc.: 71.88%] [G loss: 0.447881]\n",
      "epoch:24 step:22984 [D loss: 0.218813, acc.: 65.62%] [G loss: 0.441961]\n",
      "epoch:24 step:22985 [D loss: 0.225431, acc.: 60.94%] [G loss: 0.434239]\n",
      "epoch:24 step:22986 [D loss: 0.200622, acc.: 66.41%] [G loss: 0.466709]\n",
      "epoch:24 step:22987 [D loss: 0.201461, acc.: 71.88%] [G loss: 0.434509]\n",
      "epoch:24 step:22988 [D loss: 0.276839, acc.: 52.34%] [G loss: 0.405184]\n",
      "epoch:24 step:22989 [D loss: 0.272609, acc.: 55.47%] [G loss: 0.430142]\n",
      "epoch:24 step:22990 [D loss: 0.214259, acc.: 65.62%] [G loss: 0.436977]\n",
      "epoch:24 step:22991 [D loss: 0.216920, acc.: 65.62%] [G loss: 0.421522]\n",
      "epoch:24 step:22992 [D loss: 0.207736, acc.: 67.19%] [G loss: 0.418518]\n",
      "epoch:24 step:22993 [D loss: 0.220487, acc.: 64.06%] [G loss: 0.437736]\n",
      "epoch:24 step:22994 [D loss: 0.209701, acc.: 64.84%] [G loss: 0.425158]\n",
      "epoch:24 step:22995 [D loss: 0.206841, acc.: 67.19%] [G loss: 0.442369]\n",
      "epoch:24 step:22996 [D loss: 0.188369, acc.: 69.53%] [G loss: 0.528288]\n",
      "epoch:24 step:22997 [D loss: 0.251338, acc.: 58.59%] [G loss: 0.429626]\n",
      "epoch:24 step:22998 [D loss: 0.245455, acc.: 61.72%] [G loss: 0.413061]\n",
      "epoch:24 step:22999 [D loss: 0.227094, acc.: 64.84%] [G loss: 0.393055]\n",
      "epoch:24 step:23000 [D loss: 0.224476, acc.: 64.84%] [G loss: 0.406928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.58367861 1.9457173  5.82427804 4.84859986 3.6527976  5.63808885\n",
      " 4.56477815 5.0215776  4.39939986 4.04346654]\n",
      "##########\n",
      "epoch:24 step:23001 [D loss: 0.222120, acc.: 64.06%] [G loss: 0.430839]\n",
      "epoch:24 step:23002 [D loss: 0.223294, acc.: 60.94%] [G loss: 0.456653]\n",
      "epoch:24 step:23003 [D loss: 0.197429, acc.: 69.53%] [G loss: 0.497536]\n",
      "epoch:24 step:23004 [D loss: 0.191924, acc.: 75.00%] [G loss: 0.492168]\n",
      "epoch:24 step:23005 [D loss: 0.264673, acc.: 52.34%] [G loss: 0.419724]\n",
      "epoch:24 step:23006 [D loss: 0.253160, acc.: 58.59%] [G loss: 0.406425]\n",
      "epoch:24 step:23007 [D loss: 0.190786, acc.: 70.31%] [G loss: 0.424393]\n",
      "epoch:24 step:23008 [D loss: 0.216614, acc.: 64.06%] [G loss: 0.452871]\n",
      "epoch:24 step:23009 [D loss: 0.203954, acc.: 67.97%] [G loss: 0.451915]\n",
      "epoch:24 step:23010 [D loss: 0.212573, acc.: 71.88%] [G loss: 0.463339]\n",
      "epoch:24 step:23011 [D loss: 0.208304, acc.: 67.19%] [G loss: 0.430771]\n",
      "epoch:24 step:23012 [D loss: 0.220145, acc.: 65.62%] [G loss: 0.400945]\n",
      "epoch:24 step:23013 [D loss: 0.224265, acc.: 67.19%] [G loss: 0.409433]\n",
      "epoch:24 step:23014 [D loss: 0.224900, acc.: 64.84%] [G loss: 0.429779]\n",
      "epoch:24 step:23015 [D loss: 0.262158, acc.: 53.91%] [G loss: 0.412119]\n",
      "epoch:24 step:23016 [D loss: 0.281769, acc.: 49.22%] [G loss: 0.428497]\n",
      "epoch:24 step:23017 [D loss: 0.237787, acc.: 57.03%] [G loss: 0.422329]\n",
      "epoch:24 step:23018 [D loss: 0.224606, acc.: 69.53%] [G loss: 0.421100]\n",
      "epoch:24 step:23019 [D loss: 0.234656, acc.: 59.38%] [G loss: 0.431896]\n",
      "epoch:24 step:23020 [D loss: 0.210743, acc.: 65.62%] [G loss: 0.447449]\n",
      "epoch:24 step:23021 [D loss: 0.234222, acc.: 62.50%] [G loss: 0.422745]\n",
      "epoch:24 step:23022 [D loss: 0.201578, acc.: 71.09%] [G loss: 0.423055]\n",
      "epoch:24 step:23023 [D loss: 0.239821, acc.: 57.81%] [G loss: 0.385637]\n",
      "epoch:24 step:23024 [D loss: 0.223829, acc.: 60.16%] [G loss: 0.410101]\n",
      "epoch:24 step:23025 [D loss: 0.215022, acc.: 65.62%] [G loss: 0.427872]\n",
      "epoch:24 step:23026 [D loss: 0.243724, acc.: 63.28%] [G loss: 0.418133]\n",
      "epoch:24 step:23027 [D loss: 0.190702, acc.: 71.09%] [G loss: 0.424160]\n",
      "epoch:24 step:23028 [D loss: 0.241947, acc.: 59.38%] [G loss: 0.430258]\n",
      "epoch:24 step:23029 [D loss: 0.225917, acc.: 60.94%] [G loss: 0.426156]\n",
      "epoch:24 step:23030 [D loss: 0.271086, acc.: 53.91%] [G loss: 0.390200]\n",
      "epoch:24 step:23031 [D loss: 0.255833, acc.: 53.91%] [G loss: 0.381151]\n",
      "epoch:24 step:23032 [D loss: 0.221732, acc.: 67.97%] [G loss: 0.416023]\n",
      "epoch:24 step:23033 [D loss: 0.226504, acc.: 60.16%] [G loss: 0.433367]\n",
      "epoch:24 step:23034 [D loss: 0.210420, acc.: 69.53%] [G loss: 0.403154]\n",
      "epoch:24 step:23035 [D loss: 0.223061, acc.: 64.06%] [G loss: 0.424887]\n",
      "epoch:24 step:23036 [D loss: 0.207310, acc.: 67.19%] [G loss: 0.425067]\n",
      "epoch:24 step:23037 [D loss: 0.192892, acc.: 67.97%] [G loss: 0.429362]\n",
      "epoch:24 step:23038 [D loss: 0.255793, acc.: 57.03%] [G loss: 0.452590]\n",
      "epoch:24 step:23039 [D loss: 0.216789, acc.: 67.19%] [G loss: 0.458503]\n",
      "epoch:24 step:23040 [D loss: 0.209603, acc.: 67.97%] [G loss: 0.507984]\n",
      "epoch:24 step:23041 [D loss: 0.235786, acc.: 57.03%] [G loss: 0.416003]\n",
      "epoch:24 step:23042 [D loss: 0.210052, acc.: 64.84%] [G loss: 0.459694]\n",
      "epoch:24 step:23043 [D loss: 0.208850, acc.: 71.09%] [G loss: 0.446040]\n",
      "epoch:24 step:23044 [D loss: 0.231901, acc.: 65.62%] [G loss: 0.468060]\n",
      "epoch:24 step:23045 [D loss: 0.206644, acc.: 66.41%] [G loss: 0.458138]\n",
      "epoch:24 step:23046 [D loss: 0.237211, acc.: 57.81%] [G loss: 0.428339]\n",
      "epoch:24 step:23047 [D loss: 0.249155, acc.: 51.56%] [G loss: 0.426916]\n",
      "epoch:24 step:23048 [D loss: 0.225040, acc.: 62.50%] [G loss: 0.412446]\n",
      "epoch:24 step:23049 [D loss: 0.204333, acc.: 66.41%] [G loss: 0.437400]\n",
      "epoch:24 step:23050 [D loss: 0.221540, acc.: 67.19%] [G loss: 0.425709]\n",
      "epoch:24 step:23051 [D loss: 0.210213, acc.: 67.19%] [G loss: 0.434278]\n",
      "epoch:24 step:23052 [D loss: 0.188346, acc.: 69.53%] [G loss: 0.429465]\n",
      "epoch:24 step:23053 [D loss: 0.233549, acc.: 64.06%] [G loss: 0.430176]\n",
      "epoch:24 step:23054 [D loss: 0.261883, acc.: 56.25%] [G loss: 0.446377]\n",
      "epoch:24 step:23055 [D loss: 0.232915, acc.: 60.16%] [G loss: 0.434649]\n",
      "epoch:24 step:23056 [D loss: 0.226938, acc.: 63.28%] [G loss: 0.490038]\n",
      "epoch:24 step:23057 [D loss: 0.258362, acc.: 58.59%] [G loss: 0.391794]\n",
      "epoch:24 step:23058 [D loss: 0.240858, acc.: 60.94%] [G loss: 0.371361]\n",
      "epoch:24 step:23059 [D loss: 0.221480, acc.: 63.28%] [G loss: 0.447123]\n",
      "epoch:24 step:23060 [D loss: 0.228966, acc.: 62.50%] [G loss: 0.403511]\n",
      "epoch:24 step:23061 [D loss: 0.201585, acc.: 70.31%] [G loss: 0.463560]\n",
      "epoch:24 step:23062 [D loss: 0.187684, acc.: 71.88%] [G loss: 0.473197]\n",
      "epoch:24 step:23063 [D loss: 0.176464, acc.: 73.44%] [G loss: 0.442920]\n",
      "epoch:24 step:23064 [D loss: 0.251449, acc.: 58.59%] [G loss: 0.408476]\n",
      "epoch:24 step:23065 [D loss: 0.239398, acc.: 61.72%] [G loss: 0.422148]\n",
      "epoch:24 step:23066 [D loss: 0.214198, acc.: 67.19%] [G loss: 0.431564]\n",
      "epoch:24 step:23067 [D loss: 0.242569, acc.: 58.59%] [G loss: 0.385906]\n",
      "epoch:24 step:23068 [D loss: 0.222042, acc.: 65.62%] [G loss: 0.419887]\n",
      "epoch:24 step:23069 [D loss: 0.200679, acc.: 68.75%] [G loss: 0.428098]\n",
      "epoch:24 step:23070 [D loss: 0.208302, acc.: 68.75%] [G loss: 0.432506]\n",
      "epoch:24 step:23071 [D loss: 0.251640, acc.: 57.03%] [G loss: 0.428348]\n",
      "epoch:24 step:23072 [D loss: 0.232671, acc.: 62.50%] [G loss: 0.433753]\n",
      "epoch:24 step:23073 [D loss: 0.229208, acc.: 60.94%] [G loss: 0.405287]\n",
      "epoch:24 step:23074 [D loss: 0.240127, acc.: 57.03%] [G loss: 0.420801]\n",
      "epoch:24 step:23075 [D loss: 0.229951, acc.: 63.28%] [G loss: 0.389468]\n",
      "epoch:24 step:23076 [D loss: 0.229910, acc.: 64.84%] [G loss: 0.430872]\n",
      "epoch:24 step:23077 [D loss: 0.198502, acc.: 67.97%] [G loss: 0.477755]\n",
      "epoch:24 step:23078 [D loss: 0.242727, acc.: 60.16%] [G loss: 0.443488]\n",
      "epoch:24 step:23079 [D loss: 0.230415, acc.: 62.50%] [G loss: 0.438888]\n",
      "epoch:24 step:23080 [D loss: 0.215871, acc.: 67.97%] [G loss: 0.451666]\n",
      "epoch:24 step:23081 [D loss: 0.191422, acc.: 75.00%] [G loss: 0.422351]\n",
      "epoch:24 step:23082 [D loss: 0.251201, acc.: 52.34%] [G loss: 0.388394]\n",
      "epoch:24 step:23083 [D loss: 0.204425, acc.: 68.75%] [G loss: 0.407813]\n",
      "epoch:24 step:23084 [D loss: 0.254950, acc.: 53.91%] [G loss: 0.429446]\n",
      "epoch:24 step:23085 [D loss: 0.244094, acc.: 60.94%] [G loss: 0.399947]\n",
      "epoch:24 step:23086 [D loss: 0.235537, acc.: 60.16%] [G loss: 0.416471]\n",
      "epoch:24 step:23087 [D loss: 0.221884, acc.: 63.28%] [G loss: 0.394744]\n",
      "epoch:24 step:23088 [D loss: 0.256610, acc.: 53.12%] [G loss: 0.392616]\n",
      "epoch:24 step:23089 [D loss: 0.231034, acc.: 51.56%] [G loss: 0.375914]\n",
      "epoch:24 step:23090 [D loss: 0.216585, acc.: 62.50%] [G loss: 0.427087]\n",
      "epoch:24 step:23091 [D loss: 0.224482, acc.: 64.84%] [G loss: 0.387820]\n",
      "epoch:24 step:23092 [D loss: 0.206321, acc.: 67.97%] [G loss: 0.454287]\n",
      "epoch:24 step:23093 [D loss: 0.199929, acc.: 67.97%] [G loss: 0.446668]\n",
      "epoch:24 step:23094 [D loss: 0.232570, acc.: 57.81%] [G loss: 0.377986]\n",
      "epoch:24 step:23095 [D loss: 0.222854, acc.: 64.06%] [G loss: 0.451199]\n",
      "epoch:24 step:23096 [D loss: 0.229048, acc.: 60.94%] [G loss: 0.416023]\n",
      "epoch:24 step:23097 [D loss: 0.224132, acc.: 64.84%] [G loss: 0.413841]\n",
      "epoch:24 step:23098 [D loss: 0.218134, acc.: 64.06%] [G loss: 0.426828]\n",
      "epoch:24 step:23099 [D loss: 0.226450, acc.: 61.72%] [G loss: 0.407499]\n",
      "epoch:24 step:23100 [D loss: 0.240887, acc.: 59.38%] [G loss: 0.387682]\n",
      "epoch:24 step:23101 [D loss: 0.224824, acc.: 64.06%] [G loss: 0.401978]\n",
      "epoch:24 step:23102 [D loss: 0.239907, acc.: 57.03%] [G loss: 0.413639]\n",
      "epoch:24 step:23103 [D loss: 0.237893, acc.: 62.50%] [G loss: 0.401593]\n",
      "epoch:24 step:23104 [D loss: 0.229132, acc.: 63.28%] [G loss: 0.407339]\n",
      "epoch:24 step:23105 [D loss: 0.240660, acc.: 59.38%] [G loss: 0.451708]\n",
      "epoch:24 step:23106 [D loss: 0.224942, acc.: 60.16%] [G loss: 0.426994]\n",
      "epoch:24 step:23107 [D loss: 0.222885, acc.: 62.50%] [G loss: 0.442074]\n",
      "epoch:24 step:23108 [D loss: 0.194134, acc.: 72.66%] [G loss: 0.429324]\n",
      "epoch:24 step:23109 [D loss: 0.232525, acc.: 60.16%] [G loss: 0.382960]\n",
      "epoch:24 step:23110 [D loss: 0.214619, acc.: 69.53%] [G loss: 0.442185]\n",
      "epoch:24 step:23111 [D loss: 0.251369, acc.: 60.16%] [G loss: 0.387811]\n",
      "epoch:24 step:23112 [D loss: 0.206262, acc.: 71.09%] [G loss: 0.463524]\n",
      "epoch:24 step:23113 [D loss: 0.231344, acc.: 57.03%] [G loss: 0.421278]\n",
      "epoch:24 step:23114 [D loss: 0.247053, acc.: 55.47%] [G loss: 0.414040]\n",
      "epoch:24 step:23115 [D loss: 0.225854, acc.: 64.84%] [G loss: 0.432137]\n",
      "epoch:24 step:23116 [D loss: 0.233730, acc.: 60.94%] [G loss: 0.430021]\n",
      "epoch:24 step:23117 [D loss: 0.202975, acc.: 67.97%] [G loss: 0.395787]\n",
      "epoch:24 step:23118 [D loss: 0.230435, acc.: 60.94%] [G loss: 0.387262]\n",
      "epoch:24 step:23119 [D loss: 0.207522, acc.: 71.09%] [G loss: 0.442995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23120 [D loss: 0.219471, acc.: 63.28%] [G loss: 0.412483]\n",
      "epoch:24 step:23121 [D loss: 0.218735, acc.: 63.28%] [G loss: 0.441701]\n",
      "epoch:24 step:23122 [D loss: 0.193235, acc.: 73.44%] [G loss: 0.456251]\n",
      "epoch:24 step:23123 [D loss: 0.203054, acc.: 67.19%] [G loss: 0.436203]\n",
      "epoch:24 step:23124 [D loss: 0.224864, acc.: 60.16%] [G loss: 0.426095]\n",
      "epoch:24 step:23125 [D loss: 0.214917, acc.: 64.06%] [G loss: 0.423090]\n",
      "epoch:24 step:23126 [D loss: 0.229969, acc.: 60.94%] [G loss: 0.443963]\n",
      "epoch:24 step:23127 [D loss: 0.229880, acc.: 61.72%] [G loss: 0.431696]\n",
      "epoch:24 step:23128 [D loss: 0.217220, acc.: 64.84%] [G loss: 0.479308]\n",
      "epoch:24 step:23129 [D loss: 0.194814, acc.: 69.53%] [G loss: 0.503467]\n",
      "epoch:24 step:23130 [D loss: 0.186536, acc.: 73.44%] [G loss: 0.496190]\n",
      "epoch:24 step:23131 [D loss: 0.239118, acc.: 59.38%] [G loss: 0.453430]\n",
      "epoch:24 step:23132 [D loss: 0.239372, acc.: 64.06%] [G loss: 0.459408]\n",
      "epoch:24 step:23133 [D loss: 0.243490, acc.: 54.69%] [G loss: 0.391471]\n",
      "epoch:24 step:23134 [D loss: 0.200789, acc.: 67.97%] [G loss: 0.430966]\n",
      "epoch:24 step:23135 [D loss: 0.202930, acc.: 70.31%] [G loss: 0.436290]\n",
      "epoch:24 step:23136 [D loss: 0.179728, acc.: 73.44%] [G loss: 0.454453]\n",
      "epoch:24 step:23137 [D loss: 0.206228, acc.: 69.53%] [G loss: 0.468269]\n",
      "epoch:24 step:23138 [D loss: 0.193523, acc.: 68.75%] [G loss: 0.476746]\n",
      "epoch:24 step:23139 [D loss: 0.221458, acc.: 65.62%] [G loss: 0.517863]\n",
      "epoch:24 step:23140 [D loss: 0.228724, acc.: 60.16%] [G loss: 0.443423]\n",
      "epoch:24 step:23141 [D loss: 0.211294, acc.: 66.41%] [G loss: 0.435328]\n",
      "epoch:24 step:23142 [D loss: 0.210606, acc.: 71.09%] [G loss: 0.461343]\n",
      "epoch:24 step:23143 [D loss: 0.258411, acc.: 57.03%] [G loss: 0.399051]\n",
      "epoch:24 step:23144 [D loss: 0.238172, acc.: 59.38%] [G loss: 0.430592]\n",
      "epoch:24 step:23145 [D loss: 0.247898, acc.: 61.72%] [G loss: 0.401756]\n",
      "epoch:24 step:23146 [D loss: 0.236428, acc.: 58.59%] [G loss: 0.377024]\n",
      "epoch:24 step:23147 [D loss: 0.216322, acc.: 64.84%] [G loss: 0.422403]\n",
      "epoch:24 step:23148 [D loss: 0.222340, acc.: 64.06%] [G loss: 0.470366]\n",
      "epoch:24 step:23149 [D loss: 0.201302, acc.: 72.66%] [G loss: 0.482900]\n",
      "epoch:24 step:23150 [D loss: 0.224426, acc.: 57.81%] [G loss: 0.447565]\n",
      "epoch:24 step:23151 [D loss: 0.251740, acc.: 53.91%] [G loss: 0.405434]\n",
      "epoch:24 step:23152 [D loss: 0.200501, acc.: 71.88%] [G loss: 0.438113]\n",
      "epoch:24 step:23153 [D loss: 0.260794, acc.: 54.69%] [G loss: 0.449978]\n",
      "epoch:24 step:23154 [D loss: 0.201547, acc.: 64.84%] [G loss: 0.441382]\n",
      "epoch:24 step:23155 [D loss: 0.241647, acc.: 61.72%] [G loss: 0.417192]\n",
      "epoch:24 step:23156 [D loss: 0.230045, acc.: 62.50%] [G loss: 0.422503]\n",
      "epoch:24 step:23157 [D loss: 0.221205, acc.: 66.41%] [G loss: 0.411107]\n",
      "epoch:24 step:23158 [D loss: 0.231826, acc.: 64.06%] [G loss: 0.444210]\n",
      "epoch:24 step:23159 [D loss: 0.239049, acc.: 57.03%] [G loss: 0.404479]\n",
      "epoch:24 step:23160 [D loss: 0.222512, acc.: 57.81%] [G loss: 0.420782]\n",
      "epoch:24 step:23161 [D loss: 0.220589, acc.: 63.28%] [G loss: 0.438344]\n",
      "epoch:24 step:23162 [D loss: 0.209112, acc.: 71.09%] [G loss: 0.425433]\n",
      "epoch:24 step:23163 [D loss: 0.244870, acc.: 60.16%] [G loss: 0.415641]\n",
      "epoch:24 step:23164 [D loss: 0.217708, acc.: 60.94%] [G loss: 0.412438]\n",
      "epoch:24 step:23165 [D loss: 0.215601, acc.: 70.31%] [G loss: 0.379707]\n",
      "epoch:24 step:23166 [D loss: 0.222065, acc.: 64.06%] [G loss: 0.437124]\n",
      "epoch:24 step:23167 [D loss: 0.204576, acc.: 69.53%] [G loss: 0.424429]\n",
      "epoch:24 step:23168 [D loss: 0.228772, acc.: 60.94%] [G loss: 0.386287]\n",
      "epoch:24 step:23169 [D loss: 0.198934, acc.: 73.44%] [G loss: 0.405545]\n",
      "epoch:24 step:23170 [D loss: 0.228206, acc.: 57.03%] [G loss: 0.403128]\n",
      "epoch:24 step:23171 [D loss: 0.237278, acc.: 58.59%] [G loss: 0.413057]\n",
      "epoch:24 step:23172 [D loss: 0.227109, acc.: 60.16%] [G loss: 0.422988]\n",
      "epoch:24 step:23173 [D loss: 0.211629, acc.: 66.41%] [G loss: 0.412395]\n",
      "epoch:24 step:23174 [D loss: 0.221555, acc.: 67.97%] [G loss: 0.394671]\n",
      "epoch:24 step:23175 [D loss: 0.230878, acc.: 62.50%] [G loss: 0.440303]\n",
      "epoch:24 step:23176 [D loss: 0.203413, acc.: 67.97%] [G loss: 0.423672]\n",
      "epoch:24 step:23177 [D loss: 0.234235, acc.: 60.94%] [G loss: 0.449972]\n",
      "epoch:24 step:23178 [D loss: 0.197258, acc.: 69.53%] [G loss: 0.437008]\n",
      "epoch:24 step:23179 [D loss: 0.223465, acc.: 64.06%] [G loss: 0.465808]\n",
      "epoch:24 step:23180 [D loss: 0.201039, acc.: 72.66%] [G loss: 0.472564]\n",
      "epoch:24 step:23181 [D loss: 0.213281, acc.: 61.72%] [G loss: 0.440138]\n",
      "epoch:24 step:23182 [D loss: 0.174835, acc.: 71.88%] [G loss: 0.441629]\n",
      "epoch:24 step:23183 [D loss: 0.205712, acc.: 64.06%] [G loss: 0.467872]\n",
      "epoch:24 step:23184 [D loss: 0.260167, acc.: 48.44%] [G loss: 0.423222]\n",
      "epoch:24 step:23185 [D loss: 0.225205, acc.: 61.72%] [G loss: 0.439280]\n",
      "epoch:24 step:23186 [D loss: 0.228969, acc.: 61.72%] [G loss: 0.422035]\n",
      "epoch:24 step:23187 [D loss: 0.220881, acc.: 64.84%] [G loss: 0.455513]\n",
      "epoch:24 step:23188 [D loss: 0.205976, acc.: 63.28%] [G loss: 0.424673]\n",
      "epoch:24 step:23189 [D loss: 0.201559, acc.: 67.97%] [G loss: 0.438770]\n",
      "epoch:24 step:23190 [D loss: 0.249334, acc.: 57.81%] [G loss: 0.458327]\n",
      "epoch:24 step:23191 [D loss: 0.255720, acc.: 52.34%] [G loss: 0.429834]\n",
      "epoch:24 step:23192 [D loss: 0.242124, acc.: 54.69%] [G loss: 0.410766]\n",
      "epoch:24 step:23193 [D loss: 0.222594, acc.: 67.19%] [G loss: 0.414719]\n",
      "epoch:24 step:23194 [D loss: 0.205977, acc.: 70.31%] [G loss: 0.442151]\n",
      "epoch:24 step:23195 [D loss: 0.208715, acc.: 65.62%] [G loss: 0.433771]\n",
      "epoch:24 step:23196 [D loss: 0.221934, acc.: 64.84%] [G loss: 0.414256]\n",
      "epoch:24 step:23197 [D loss: 0.201412, acc.: 65.62%] [G loss: 0.425161]\n",
      "epoch:24 step:23198 [D loss: 0.258092, acc.: 52.34%] [G loss: 0.406993]\n",
      "epoch:24 step:23199 [D loss: 0.235866, acc.: 60.94%] [G loss: 0.399002]\n",
      "epoch:24 step:23200 [D loss: 0.206313, acc.: 63.28%] [G loss: 0.464994]\n",
      "##############\n",
      "[2.53180478 2.09573087 5.86116909 4.59970505 3.65934069 5.58904018\n",
      " 4.62774866 4.86995705 4.4330579  3.92384484]\n",
      "##########\n",
      "epoch:24 step:23201 [D loss: 0.231789, acc.: 61.72%] [G loss: 0.435235]\n",
      "epoch:24 step:23202 [D loss: 0.234511, acc.: 58.59%] [G loss: 0.432852]\n",
      "epoch:24 step:23203 [D loss: 0.239584, acc.: 59.38%] [G loss: 0.410124]\n",
      "epoch:24 step:23204 [D loss: 0.225256, acc.: 63.28%] [G loss: 0.436945]\n",
      "epoch:24 step:23205 [D loss: 0.209615, acc.: 64.84%] [G loss: 0.436267]\n",
      "epoch:24 step:23206 [D loss: 0.222888, acc.: 62.50%] [G loss: 0.447426]\n",
      "epoch:24 step:23207 [D loss: 0.212115, acc.: 66.41%] [G loss: 0.438671]\n",
      "epoch:24 step:23208 [D loss: 0.222688, acc.: 60.94%] [G loss: 0.408739]\n",
      "epoch:24 step:23209 [D loss: 0.236810, acc.: 57.81%] [G loss: 0.418536]\n",
      "epoch:24 step:23210 [D loss: 0.262225, acc.: 52.34%] [G loss: 0.421097]\n",
      "epoch:24 step:23211 [D loss: 0.204497, acc.: 71.88%] [G loss: 0.449513]\n",
      "epoch:24 step:23212 [D loss: 0.220325, acc.: 63.28%] [G loss: 0.434276]\n",
      "epoch:24 step:23213 [D loss: 0.215391, acc.: 65.62%] [G loss: 0.434182]\n",
      "epoch:24 step:23214 [D loss: 0.242458, acc.: 56.25%] [G loss: 0.392912]\n",
      "epoch:24 step:23215 [D loss: 0.236617, acc.: 60.16%] [G loss: 0.416833]\n",
      "epoch:24 step:23216 [D loss: 0.225448, acc.: 61.72%] [G loss: 0.380505]\n",
      "epoch:24 step:23217 [D loss: 0.214725, acc.: 66.41%] [G loss: 0.423512]\n",
      "epoch:24 step:23218 [D loss: 0.217430, acc.: 59.38%] [G loss: 0.401536]\n",
      "epoch:24 step:23219 [D loss: 0.231499, acc.: 62.50%] [G loss: 0.428033]\n",
      "epoch:24 step:23220 [D loss: 0.209975, acc.: 60.16%] [G loss: 0.441591]\n",
      "epoch:24 step:23221 [D loss: 0.236573, acc.: 63.28%] [G loss: 0.426090]\n",
      "epoch:24 step:23222 [D loss: 0.262713, acc.: 52.34%] [G loss: 0.405776]\n",
      "epoch:24 step:23223 [D loss: 0.226403, acc.: 60.16%] [G loss: 0.417422]\n",
      "epoch:24 step:23224 [D loss: 0.237375, acc.: 57.81%] [G loss: 0.414968]\n",
      "epoch:24 step:23225 [D loss: 0.202999, acc.: 72.66%] [G loss: 0.417367]\n",
      "epoch:24 step:23226 [D loss: 0.244555, acc.: 53.91%] [G loss: 0.394328]\n",
      "epoch:24 step:23227 [D loss: 0.234901, acc.: 56.25%] [G loss: 0.387465]\n",
      "epoch:24 step:23228 [D loss: 0.224389, acc.: 60.16%] [G loss: 0.399936]\n",
      "epoch:24 step:23229 [D loss: 0.239730, acc.: 57.81%] [G loss: 0.394505]\n",
      "epoch:24 step:23230 [D loss: 0.213292, acc.: 65.62%] [G loss: 0.428685]\n",
      "epoch:24 step:23231 [D loss: 0.216850, acc.: 60.16%] [G loss: 0.452329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23232 [D loss: 0.234684, acc.: 62.50%] [G loss: 0.443508]\n",
      "epoch:24 step:23233 [D loss: 0.223943, acc.: 64.84%] [G loss: 0.445603]\n",
      "epoch:24 step:23234 [D loss: 0.221276, acc.: 63.28%] [G loss: 0.424057]\n",
      "epoch:24 step:23235 [D loss: 0.199223, acc.: 71.09%] [G loss: 0.422274]\n",
      "epoch:24 step:23236 [D loss: 0.237576, acc.: 60.16%] [G loss: 0.410539]\n",
      "epoch:24 step:23237 [D loss: 0.221683, acc.: 60.94%] [G loss: 0.416929]\n",
      "epoch:24 step:23238 [D loss: 0.207820, acc.: 64.06%] [G loss: 0.443647]\n",
      "epoch:24 step:23239 [D loss: 0.233655, acc.: 59.38%] [G loss: 0.447088]\n",
      "epoch:24 step:23240 [D loss: 0.255541, acc.: 51.56%] [G loss: 0.423980]\n",
      "epoch:24 step:23241 [D loss: 0.232167, acc.: 62.50%] [G loss: 0.422689]\n",
      "epoch:24 step:23242 [D loss: 0.216909, acc.: 60.16%] [G loss: 0.400132]\n",
      "epoch:24 step:23243 [D loss: 0.236953, acc.: 57.03%] [G loss: 0.462670]\n",
      "epoch:24 step:23244 [D loss: 0.238876, acc.: 57.81%] [G loss: 0.422660]\n",
      "epoch:24 step:23245 [D loss: 0.241806, acc.: 55.47%] [G loss: 0.402139]\n",
      "epoch:24 step:23246 [D loss: 0.215496, acc.: 66.41%] [G loss: 0.380815]\n",
      "epoch:24 step:23247 [D loss: 0.240178, acc.: 60.16%] [G loss: 0.444291]\n",
      "epoch:24 step:23248 [D loss: 0.224496, acc.: 63.28%] [G loss: 0.415550]\n",
      "epoch:24 step:23249 [D loss: 0.235438, acc.: 59.38%] [G loss: 0.399252]\n",
      "epoch:24 step:23250 [D loss: 0.238800, acc.: 59.38%] [G loss: 0.415556]\n",
      "epoch:24 step:23251 [D loss: 0.228860, acc.: 65.62%] [G loss: 0.429555]\n",
      "epoch:24 step:23252 [D loss: 0.246020, acc.: 57.81%] [G loss: 0.410922]\n",
      "epoch:24 step:23253 [D loss: 0.229868, acc.: 60.94%] [G loss: 0.409417]\n",
      "epoch:24 step:23254 [D loss: 0.248619, acc.: 60.94%] [G loss: 0.407487]\n",
      "epoch:24 step:23255 [D loss: 0.199918, acc.: 67.19%] [G loss: 0.409437]\n",
      "epoch:24 step:23256 [D loss: 0.232249, acc.: 60.94%] [G loss: 0.483860]\n",
      "epoch:24 step:23257 [D loss: 0.201533, acc.: 69.53%] [G loss: 0.443924]\n",
      "epoch:24 step:23258 [D loss: 0.217014, acc.: 64.84%] [G loss: 0.456718]\n",
      "epoch:24 step:23259 [D loss: 0.224465, acc.: 63.28%] [G loss: 0.460886]\n",
      "epoch:24 step:23260 [D loss: 0.226425, acc.: 59.38%] [G loss: 0.429513]\n",
      "epoch:24 step:23261 [D loss: 0.223633, acc.: 65.62%] [G loss: 0.455502]\n",
      "epoch:24 step:23262 [D loss: 0.208451, acc.: 67.19%] [G loss: 0.463587]\n",
      "epoch:24 step:23263 [D loss: 0.200887, acc.: 69.53%] [G loss: 0.481417]\n",
      "epoch:24 step:23264 [D loss: 0.241926, acc.: 55.47%] [G loss: 0.409631]\n",
      "epoch:24 step:23265 [D loss: 0.218547, acc.: 67.19%] [G loss: 0.456103]\n",
      "epoch:24 step:23266 [D loss: 0.234339, acc.: 57.03%] [G loss: 0.450943]\n",
      "epoch:24 step:23267 [D loss: 0.211995, acc.: 71.09%] [G loss: 0.427965]\n",
      "epoch:24 step:23268 [D loss: 0.219439, acc.: 63.28%] [G loss: 0.383223]\n",
      "epoch:24 step:23269 [D loss: 0.218556, acc.: 64.06%] [G loss: 0.431187]\n",
      "epoch:24 step:23270 [D loss: 0.198273, acc.: 70.31%] [G loss: 0.480425]\n",
      "epoch:24 step:23271 [D loss: 0.250778, acc.: 55.47%] [G loss: 0.476099]\n",
      "epoch:24 step:23272 [D loss: 0.242616, acc.: 54.69%] [G loss: 0.430411]\n",
      "epoch:24 step:23273 [D loss: 0.205998, acc.: 69.53%] [G loss: 0.424258]\n",
      "epoch:24 step:23274 [D loss: 0.197038, acc.: 71.09%] [G loss: 0.442714]\n",
      "epoch:24 step:23275 [D loss: 0.261882, acc.: 51.56%] [G loss: 0.412343]\n",
      "epoch:24 step:23276 [D loss: 0.219195, acc.: 64.06%] [G loss: 0.408322]\n",
      "epoch:24 step:23277 [D loss: 0.215767, acc.: 67.19%] [G loss: 0.392230]\n",
      "epoch:24 step:23278 [D loss: 0.215464, acc.: 60.94%] [G loss: 0.429184]\n",
      "epoch:24 step:23279 [D loss: 0.254271, acc.: 57.81%] [G loss: 0.409726]\n",
      "epoch:24 step:23280 [D loss: 0.180005, acc.: 76.56%] [G loss: 0.451336]\n",
      "epoch:24 step:23281 [D loss: 0.199512, acc.: 73.44%] [G loss: 0.456304]\n",
      "epoch:24 step:23282 [D loss: 0.266552, acc.: 50.00%] [G loss: 0.404928]\n",
      "epoch:24 step:23283 [D loss: 0.247354, acc.: 54.69%] [G loss: 0.435131]\n",
      "epoch:24 step:23284 [D loss: 0.211073, acc.: 67.97%] [G loss: 0.453007]\n",
      "epoch:24 step:23285 [D loss: 0.219110, acc.: 66.41%] [G loss: 0.428946]\n",
      "epoch:24 step:23286 [D loss: 0.234246, acc.: 62.50%] [G loss: 0.425472]\n",
      "epoch:24 step:23287 [D loss: 0.249790, acc.: 58.59%] [G loss: 0.423626]\n",
      "epoch:24 step:23288 [D loss: 0.245456, acc.: 54.69%] [G loss: 0.428570]\n",
      "epoch:24 step:23289 [D loss: 0.217067, acc.: 63.28%] [G loss: 0.440604]\n",
      "epoch:24 step:23290 [D loss: 0.197014, acc.: 71.88%] [G loss: 0.492420]\n",
      "epoch:24 step:23291 [D loss: 0.240837, acc.: 58.59%] [G loss: 0.435969]\n",
      "epoch:24 step:23292 [D loss: 0.214896, acc.: 67.19%] [G loss: 0.435464]\n",
      "epoch:24 step:23293 [D loss: 0.218846, acc.: 65.62%] [G loss: 0.443272]\n",
      "epoch:24 step:23294 [D loss: 0.243337, acc.: 60.16%] [G loss: 0.393832]\n",
      "epoch:24 step:23295 [D loss: 0.221680, acc.: 66.41%] [G loss: 0.418035]\n",
      "epoch:24 step:23296 [D loss: 0.237801, acc.: 60.16%] [G loss: 0.413937]\n",
      "epoch:24 step:23297 [D loss: 0.237799, acc.: 60.94%] [G loss: 0.429587]\n",
      "epoch:24 step:23298 [D loss: 0.226178, acc.: 66.41%] [G loss: 0.404403]\n",
      "epoch:24 step:23299 [D loss: 0.214351, acc.: 64.84%] [G loss: 0.417769]\n",
      "epoch:24 step:23300 [D loss: 0.248470, acc.: 53.12%] [G loss: 0.395895]\n",
      "epoch:24 step:23301 [D loss: 0.198226, acc.: 71.88%] [G loss: 0.408508]\n",
      "epoch:24 step:23302 [D loss: 0.216955, acc.: 66.41%] [G loss: 0.408517]\n",
      "epoch:24 step:23303 [D loss: 0.175833, acc.: 75.00%] [G loss: 0.440774]\n",
      "epoch:24 step:23304 [D loss: 0.201799, acc.: 64.84%] [G loss: 0.431539]\n",
      "epoch:24 step:23305 [D loss: 0.245500, acc.: 60.94%] [G loss: 0.441161]\n",
      "epoch:24 step:23306 [D loss: 0.260181, acc.: 53.12%] [G loss: 0.431238]\n",
      "epoch:24 step:23307 [D loss: 0.201421, acc.: 65.62%] [G loss: 0.458839]\n",
      "epoch:24 step:23308 [D loss: 0.273865, acc.: 54.69%] [G loss: 0.417035]\n",
      "epoch:24 step:23309 [D loss: 0.228244, acc.: 58.59%] [G loss: 0.424811]\n",
      "epoch:24 step:23310 [D loss: 0.223001, acc.: 61.72%] [G loss: 0.417927]\n",
      "epoch:24 step:23311 [D loss: 0.191827, acc.: 70.31%] [G loss: 0.424635]\n",
      "epoch:24 step:23312 [D loss: 0.240577, acc.: 57.81%] [G loss: 0.423115]\n",
      "epoch:24 step:23313 [D loss: 0.229761, acc.: 59.38%] [G loss: 0.434488]\n",
      "epoch:24 step:23314 [D loss: 0.224076, acc.: 62.50%] [G loss: 0.426857]\n",
      "epoch:24 step:23315 [D loss: 0.237184, acc.: 59.38%] [G loss: 0.427302]\n",
      "epoch:24 step:23316 [D loss: 0.245873, acc.: 57.81%] [G loss: 0.403579]\n",
      "epoch:24 step:23317 [D loss: 0.229748, acc.: 63.28%] [G loss: 0.388630]\n",
      "epoch:24 step:23318 [D loss: 0.207629, acc.: 65.62%] [G loss: 0.450624]\n",
      "epoch:24 step:23319 [D loss: 0.237777, acc.: 57.81%] [G loss: 0.417102]\n",
      "epoch:24 step:23320 [D loss: 0.198504, acc.: 69.53%] [G loss: 0.440913]\n",
      "epoch:24 step:23321 [D loss: 0.199478, acc.: 64.84%] [G loss: 0.409967]\n",
      "epoch:24 step:23322 [D loss: 0.259382, acc.: 58.59%] [G loss: 0.393122]\n",
      "epoch:24 step:23323 [D loss: 0.223314, acc.: 61.72%] [G loss: 0.423409]\n",
      "epoch:24 step:23324 [D loss: 0.219379, acc.: 58.59%] [G loss: 0.420255]\n",
      "epoch:24 step:23325 [D loss: 0.192836, acc.: 67.97%] [G loss: 0.443183]\n",
      "epoch:24 step:23326 [D loss: 0.216549, acc.: 67.19%] [G loss: 0.381337]\n",
      "epoch:24 step:23327 [D loss: 0.215882, acc.: 65.62%] [G loss: 0.436586]\n",
      "epoch:24 step:23328 [D loss: 0.237048, acc.: 60.16%] [G loss: 0.440747]\n",
      "epoch:24 step:23329 [D loss: 0.215503, acc.: 64.84%] [G loss: 0.425556]\n",
      "epoch:24 step:23330 [D loss: 0.208441, acc.: 61.72%] [G loss: 0.441259]\n",
      "epoch:24 step:23331 [D loss: 0.215875, acc.: 64.06%] [G loss: 0.432609]\n",
      "epoch:24 step:23332 [D loss: 0.229422, acc.: 58.59%] [G loss: 0.446509]\n",
      "epoch:24 step:23333 [D loss: 0.217378, acc.: 67.19%] [G loss: 0.471658]\n",
      "epoch:24 step:23334 [D loss: 0.206820, acc.: 67.19%] [G loss: 0.425027]\n",
      "epoch:24 step:23335 [D loss: 0.263310, acc.: 52.34%] [G loss: 0.409296]\n",
      "epoch:24 step:23336 [D loss: 0.231095, acc.: 61.72%] [G loss: 0.375405]\n",
      "epoch:24 step:23337 [D loss: 0.196391, acc.: 73.44%] [G loss: 0.420679]\n",
      "epoch:24 step:23338 [D loss: 0.234098, acc.: 59.38%] [G loss: 0.421872]\n",
      "epoch:24 step:23339 [D loss: 0.226618, acc.: 59.38%] [G loss: 0.425512]\n",
      "epoch:24 step:23340 [D loss: 0.211597, acc.: 61.72%] [G loss: 0.440246]\n",
      "epoch:24 step:23341 [D loss: 0.227009, acc.: 61.72%] [G loss: 0.460023]\n",
      "epoch:24 step:23342 [D loss: 0.223254, acc.: 64.84%] [G loss: 0.460893]\n",
      "epoch:24 step:23343 [D loss: 0.229380, acc.: 65.62%] [G loss: 0.471745]\n",
      "epoch:24 step:23344 [D loss: 0.236737, acc.: 60.94%] [G loss: 0.458978]\n",
      "epoch:24 step:23345 [D loss: 0.231245, acc.: 60.94%] [G loss: 0.404380]\n",
      "epoch:24 step:23346 [D loss: 0.279067, acc.: 47.66%] [G loss: 0.406801]\n",
      "epoch:24 step:23347 [D loss: 0.242300, acc.: 59.38%] [G loss: 0.402756]\n",
      "epoch:24 step:23348 [D loss: 0.213419, acc.: 64.06%] [G loss: 0.425491]\n",
      "epoch:24 step:23349 [D loss: 0.240052, acc.: 60.16%] [G loss: 0.405933]\n",
      "epoch:24 step:23350 [D loss: 0.227937, acc.: 60.94%] [G loss: 0.379605]\n",
      "epoch:24 step:23351 [D loss: 0.205823, acc.: 63.28%] [G loss: 0.396481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23352 [D loss: 0.222550, acc.: 60.16%] [G loss: 0.362065]\n",
      "epoch:24 step:23353 [D loss: 0.254904, acc.: 54.69%] [G loss: 0.345900]\n",
      "epoch:24 step:23354 [D loss: 0.240671, acc.: 57.03%] [G loss: 0.384646]\n",
      "epoch:24 step:23355 [D loss: 0.240671, acc.: 57.03%] [G loss: 0.417239]\n",
      "epoch:24 step:23356 [D loss: 0.220944, acc.: 66.41%] [G loss: 0.404702]\n",
      "epoch:24 step:23357 [D loss: 0.264317, acc.: 50.78%] [G loss: 0.400431]\n",
      "epoch:24 step:23358 [D loss: 0.228400, acc.: 64.06%] [G loss: 0.457859]\n",
      "epoch:24 step:23359 [D loss: 0.205004, acc.: 68.75%] [G loss: 0.430390]\n",
      "epoch:24 step:23360 [D loss: 0.227901, acc.: 61.72%] [G loss: 0.441388]\n",
      "epoch:24 step:23361 [D loss: 0.232370, acc.: 63.28%] [G loss: 0.420488]\n",
      "epoch:24 step:23362 [D loss: 0.227955, acc.: 65.62%] [G loss: 0.420295]\n",
      "epoch:24 step:23363 [D loss: 0.200018, acc.: 72.66%] [G loss: 0.413403]\n",
      "epoch:24 step:23364 [D loss: 0.237877, acc.: 62.50%] [G loss: 0.398771]\n",
      "epoch:24 step:23365 [D loss: 0.244071, acc.: 60.94%] [G loss: 0.407433]\n",
      "epoch:24 step:23366 [D loss: 0.219876, acc.: 67.19%] [G loss: 0.421060]\n",
      "epoch:24 step:23367 [D loss: 0.232277, acc.: 64.06%] [G loss: 0.431294]\n",
      "epoch:24 step:23368 [D loss: 0.241463, acc.: 59.38%] [G loss: 0.434249]\n",
      "epoch:24 step:23369 [D loss: 0.245262, acc.: 54.69%] [G loss: 0.375584]\n",
      "epoch:24 step:23370 [D loss: 0.248034, acc.: 60.16%] [G loss: 0.383299]\n",
      "epoch:24 step:23371 [D loss: 0.246894, acc.: 54.69%] [G loss: 0.392764]\n",
      "epoch:24 step:23372 [D loss: 0.217439, acc.: 61.72%] [G loss: 0.416285]\n",
      "epoch:24 step:23373 [D loss: 0.215454, acc.: 63.28%] [G loss: 0.469391]\n",
      "epoch:24 step:23374 [D loss: 0.190358, acc.: 75.78%] [G loss: 0.507330]\n",
      "epoch:24 step:23375 [D loss: 0.234427, acc.: 64.84%] [G loss: 0.456327]\n",
      "epoch:24 step:23376 [D loss: 0.219888, acc.: 61.72%] [G loss: 0.445997]\n",
      "epoch:24 step:23377 [D loss: 0.201685, acc.: 72.66%] [G loss: 0.457926]\n",
      "epoch:24 step:23378 [D loss: 0.195634, acc.: 74.22%] [G loss: 0.482257]\n",
      "epoch:24 step:23379 [D loss: 0.263234, acc.: 54.69%] [G loss: 0.428427]\n",
      "epoch:24 step:23380 [D loss: 0.260751, acc.: 53.91%] [G loss: 0.416794]\n",
      "epoch:24 step:23381 [D loss: 0.204869, acc.: 64.84%] [G loss: 0.400189]\n",
      "epoch:24 step:23382 [D loss: 0.207630, acc.: 66.41%] [G loss: 0.423755]\n",
      "epoch:24 step:23383 [D loss: 0.218573, acc.: 64.06%] [G loss: 0.445088]\n",
      "epoch:24 step:23384 [D loss: 0.207094, acc.: 67.19%] [G loss: 0.458304]\n",
      "epoch:24 step:23385 [D loss: 0.200298, acc.: 70.31%] [G loss: 0.473886]\n",
      "epoch:24 step:23386 [D loss: 0.233571, acc.: 62.50%] [G loss: 0.409363]\n",
      "epoch:24 step:23387 [D loss: 0.219589, acc.: 59.38%] [G loss: 0.456033]\n",
      "epoch:24 step:23388 [D loss: 0.224478, acc.: 65.62%] [G loss: 0.454000]\n",
      "epoch:24 step:23389 [D loss: 0.219157, acc.: 60.94%] [G loss: 0.437839]\n",
      "epoch:24 step:23390 [D loss: 0.258056, acc.: 57.03%] [G loss: 0.390442]\n",
      "epoch:24 step:23391 [D loss: 0.232478, acc.: 60.94%] [G loss: 0.425686]\n",
      "epoch:24 step:23392 [D loss: 0.224501, acc.: 60.16%] [G loss: 0.386978]\n",
      "epoch:24 step:23393 [D loss: 0.190942, acc.: 70.31%] [G loss: 0.428190]\n",
      "epoch:24 step:23394 [D loss: 0.218601, acc.: 60.94%] [G loss: 0.430879]\n",
      "epoch:24 step:23395 [D loss: 0.220912, acc.: 64.06%] [G loss: 0.437219]\n",
      "epoch:24 step:23396 [D loss: 0.190233, acc.: 75.78%] [G loss: 0.448282]\n",
      "epoch:24 step:23397 [D loss: 0.191860, acc.: 71.09%] [G loss: 0.457754]\n",
      "epoch:24 step:23398 [D loss: 0.239275, acc.: 60.94%] [G loss: 0.412243]\n",
      "epoch:24 step:23399 [D loss: 0.221143, acc.: 64.84%] [G loss: 0.417454]\n",
      "epoch:24 step:23400 [D loss: 0.214110, acc.: 62.50%] [G loss: 0.437934]\n",
      "##############\n",
      "[2.53548968 2.00898458 5.86536069 4.79451427 3.74427788 5.73786333\n",
      " 4.50406543 4.80249364 4.55316486 4.10242459]\n",
      "##########\n",
      "epoch:24 step:23401 [D loss: 0.215753, acc.: 68.75%] [G loss: 0.473461]\n",
      "epoch:24 step:23402 [D loss: 0.215264, acc.: 65.62%] [G loss: 0.497776]\n",
      "epoch:24 step:23403 [D loss: 0.271810, acc.: 54.69%] [G loss: 0.400318]\n",
      "epoch:24 step:23404 [D loss: 0.208673, acc.: 64.84%] [G loss: 0.391316]\n",
      "epoch:24 step:23405 [D loss: 0.228237, acc.: 65.62%] [G loss: 0.434993]\n",
      "epoch:24 step:23406 [D loss: 0.202103, acc.: 69.53%] [G loss: 0.436024]\n",
      "epoch:24 step:23407 [D loss: 0.211871, acc.: 67.97%] [G loss: 0.436892]\n",
      "epoch:24 step:23408 [D loss: 0.295128, acc.: 44.53%] [G loss: 0.443074]\n",
      "epoch:24 step:23409 [D loss: 0.207937, acc.: 69.53%] [G loss: 0.468394]\n",
      "epoch:24 step:23410 [D loss: 0.248734, acc.: 55.47%] [G loss: 0.400995]\n",
      "epoch:24 step:23411 [D loss: 0.169367, acc.: 81.25%] [G loss: 0.439101]\n",
      "epoch:24 step:23412 [D loss: 0.187869, acc.: 75.78%] [G loss: 0.401786]\n",
      "epoch:24 step:23413 [D loss: 0.189917, acc.: 72.66%] [G loss: 0.471498]\n",
      "epoch:24 step:23414 [D loss: 0.181239, acc.: 78.12%] [G loss: 0.510103]\n",
      "epoch:24 step:23415 [D loss: 0.250029, acc.: 57.81%] [G loss: 0.482745]\n",
      "epoch:24 step:23416 [D loss: 0.314478, acc.: 50.78%] [G loss: 0.485814]\n",
      "epoch:24 step:23417 [D loss: 0.248899, acc.: 53.12%] [G loss: 0.550030]\n",
      "epoch:24 step:23418 [D loss: 0.194682, acc.: 69.53%] [G loss: 0.506416]\n",
      "epoch:24 step:23419 [D loss: 0.245201, acc.: 59.38%] [G loss: 0.444469]\n",
      "epoch:24 step:23420 [D loss: 0.264707, acc.: 61.72%] [G loss: 0.414766]\n",
      "epoch:24 step:23421 [D loss: 0.219073, acc.: 64.84%] [G loss: 0.404058]\n",
      "epoch:24 step:23422 [D loss: 0.224131, acc.: 65.62%] [G loss: 0.411210]\n",
      "epoch:24 step:23423 [D loss: 0.205082, acc.: 74.22%] [G loss: 0.476836]\n",
      "epoch:24 step:23424 [D loss: 0.202025, acc.: 68.75%] [G loss: 0.469635]\n",
      "epoch:24 step:23425 [D loss: 0.189146, acc.: 72.66%] [G loss: 0.565224]\n",
      "epoch:25 step:23426 [D loss: 0.237937, acc.: 64.06%] [G loss: 0.456975]\n",
      "epoch:25 step:23427 [D loss: 0.281041, acc.: 53.12%] [G loss: 0.441376]\n",
      "epoch:25 step:23428 [D loss: 0.241225, acc.: 60.94%] [G loss: 0.414722]\n",
      "epoch:25 step:23429 [D loss: 0.224697, acc.: 65.62%] [G loss: 0.422105]\n",
      "epoch:25 step:23430 [D loss: 0.221678, acc.: 68.75%] [G loss: 0.443777]\n",
      "epoch:25 step:23431 [D loss: 0.233250, acc.: 61.72%] [G loss: 0.424512]\n",
      "epoch:25 step:23432 [D loss: 0.200747, acc.: 69.53%] [G loss: 0.438106]\n",
      "epoch:25 step:23433 [D loss: 0.203118, acc.: 71.09%] [G loss: 0.409607]\n",
      "epoch:25 step:23434 [D loss: 0.202788, acc.: 69.53%] [G loss: 0.425512]\n",
      "epoch:25 step:23435 [D loss: 0.203423, acc.: 67.19%] [G loss: 0.450372]\n",
      "epoch:25 step:23436 [D loss: 0.203622, acc.: 69.53%] [G loss: 0.448393]\n",
      "epoch:25 step:23437 [D loss: 0.219004, acc.: 67.97%] [G loss: 0.423128]\n",
      "epoch:25 step:23438 [D loss: 0.229668, acc.: 57.03%] [G loss: 0.441323]\n",
      "epoch:25 step:23439 [D loss: 0.186459, acc.: 75.00%] [G loss: 0.476213]\n",
      "epoch:25 step:23440 [D loss: 0.207377, acc.: 69.53%] [G loss: 0.476596]\n",
      "epoch:25 step:23441 [D loss: 0.210752, acc.: 67.19%] [G loss: 0.448125]\n",
      "epoch:25 step:23442 [D loss: 0.239666, acc.: 61.72%] [G loss: 0.464322]\n",
      "epoch:25 step:23443 [D loss: 0.216677, acc.: 62.50%] [G loss: 0.446958]\n",
      "epoch:25 step:23444 [D loss: 0.251077, acc.: 59.38%] [G loss: 0.454546]\n",
      "epoch:25 step:23445 [D loss: 0.287509, acc.: 45.31%] [G loss: 0.403427]\n",
      "epoch:25 step:23446 [D loss: 0.213801, acc.: 64.06%] [G loss: 0.507096]\n",
      "epoch:25 step:23447 [D loss: 0.207755, acc.: 65.62%] [G loss: 0.447079]\n",
      "epoch:25 step:23448 [D loss: 0.241142, acc.: 57.81%] [G loss: 0.436433]\n",
      "epoch:25 step:23449 [D loss: 0.192526, acc.: 68.75%] [G loss: 0.447607]\n",
      "epoch:25 step:23450 [D loss: 0.230889, acc.: 59.38%] [G loss: 0.427073]\n",
      "epoch:25 step:23451 [D loss: 0.233297, acc.: 57.03%] [G loss: 0.421388]\n",
      "epoch:25 step:23452 [D loss: 0.213599, acc.: 67.97%] [G loss: 0.424822]\n",
      "epoch:25 step:23453 [D loss: 0.218783, acc.: 66.41%] [G loss: 0.457802]\n",
      "epoch:25 step:23454 [D loss: 0.211365, acc.: 69.53%] [G loss: 0.428771]\n",
      "epoch:25 step:23455 [D loss: 0.231530, acc.: 52.34%] [G loss: 0.444918]\n",
      "epoch:25 step:23456 [D loss: 0.238068, acc.: 61.72%] [G loss: 0.454824]\n",
      "epoch:25 step:23457 [D loss: 0.227938, acc.: 62.50%] [G loss: 0.433866]\n",
      "epoch:25 step:23458 [D loss: 0.231287, acc.: 63.28%] [G loss: 0.409619]\n",
      "epoch:25 step:23459 [D loss: 0.230822, acc.: 64.84%] [G loss: 0.405976]\n",
      "epoch:25 step:23460 [D loss: 0.227219, acc.: 63.28%] [G loss: 0.416466]\n",
      "epoch:25 step:23461 [D loss: 0.222512, acc.: 64.06%] [G loss: 0.399341]\n",
      "epoch:25 step:23462 [D loss: 0.246810, acc.: 53.91%] [G loss: 0.377981]\n",
      "epoch:25 step:23463 [D loss: 0.235777, acc.: 60.94%] [G loss: 0.415225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23464 [D loss: 0.205617, acc.: 70.31%] [G loss: 0.436210]\n",
      "epoch:25 step:23465 [D loss: 0.201254, acc.: 71.88%] [G loss: 0.429114]\n",
      "epoch:25 step:23466 [D loss: 0.247719, acc.: 55.47%] [G loss: 0.425061]\n",
      "epoch:25 step:23467 [D loss: 0.231311, acc.: 59.38%] [G loss: 0.401271]\n",
      "epoch:25 step:23468 [D loss: 0.230123, acc.: 60.16%] [G loss: 0.409318]\n",
      "epoch:25 step:23469 [D loss: 0.250471, acc.: 51.56%] [G loss: 0.402149]\n",
      "epoch:25 step:23470 [D loss: 0.227128, acc.: 63.28%] [G loss: 0.421772]\n",
      "epoch:25 step:23471 [D loss: 0.223616, acc.: 65.62%] [G loss: 0.486965]\n",
      "epoch:25 step:23472 [D loss: 0.240409, acc.: 57.81%] [G loss: 0.381822]\n",
      "epoch:25 step:23473 [D loss: 0.207779, acc.: 66.41%] [G loss: 0.416950]\n",
      "epoch:25 step:23474 [D loss: 0.241584, acc.: 58.59%] [G loss: 0.412820]\n",
      "epoch:25 step:23475 [D loss: 0.212248, acc.: 63.28%] [G loss: 0.469239]\n",
      "epoch:25 step:23476 [D loss: 0.241896, acc.: 57.81%] [G loss: 0.438113]\n",
      "epoch:25 step:23477 [D loss: 0.221920, acc.: 68.75%] [G loss: 0.427541]\n",
      "epoch:25 step:23478 [D loss: 0.224391, acc.: 62.50%] [G loss: 0.418939]\n",
      "epoch:25 step:23479 [D loss: 0.237413, acc.: 57.81%] [G loss: 0.457002]\n",
      "epoch:25 step:23480 [D loss: 0.199945, acc.: 69.53%] [G loss: 0.467558]\n",
      "epoch:25 step:23481 [D loss: 0.235592, acc.: 62.50%] [G loss: 0.440607]\n",
      "epoch:25 step:23482 [D loss: 0.233464, acc.: 62.50%] [G loss: 0.427988]\n",
      "epoch:25 step:23483 [D loss: 0.248158, acc.: 57.03%] [G loss: 0.378458]\n",
      "epoch:25 step:23484 [D loss: 0.209264, acc.: 63.28%] [G loss: 0.430389]\n",
      "epoch:25 step:23485 [D loss: 0.235734, acc.: 60.94%] [G loss: 0.435433]\n",
      "epoch:25 step:23486 [D loss: 0.233092, acc.: 57.03%] [G loss: 0.403847]\n",
      "epoch:25 step:23487 [D loss: 0.237977, acc.: 62.50%] [G loss: 0.409837]\n",
      "epoch:25 step:23488 [D loss: 0.241152, acc.: 61.72%] [G loss: 0.398299]\n",
      "epoch:25 step:23489 [D loss: 0.211689, acc.: 65.62%] [G loss: 0.436862]\n",
      "epoch:25 step:23490 [D loss: 0.211011, acc.: 66.41%] [G loss: 0.433751]\n",
      "epoch:25 step:23491 [D loss: 0.220095, acc.: 64.06%] [G loss: 0.451938]\n",
      "epoch:25 step:23492 [D loss: 0.210465, acc.: 71.09%] [G loss: 0.436859]\n",
      "epoch:25 step:23493 [D loss: 0.235245, acc.: 57.81%] [G loss: 0.392508]\n",
      "epoch:25 step:23494 [D loss: 0.191558, acc.: 71.09%] [G loss: 0.425055]\n",
      "epoch:25 step:23495 [D loss: 0.191978, acc.: 73.44%] [G loss: 0.467410]\n",
      "epoch:25 step:23496 [D loss: 0.245018, acc.: 54.69%] [G loss: 0.390675]\n",
      "epoch:25 step:23497 [D loss: 0.233284, acc.: 59.38%] [G loss: 0.391720]\n",
      "epoch:25 step:23498 [D loss: 0.221842, acc.: 61.72%] [G loss: 0.385038]\n",
      "epoch:25 step:23499 [D loss: 0.201025, acc.: 66.41%] [G loss: 0.431525]\n",
      "epoch:25 step:23500 [D loss: 0.210094, acc.: 67.19%] [G loss: 0.450666]\n",
      "epoch:25 step:23501 [D loss: 0.199955, acc.: 66.41%] [G loss: 0.483425]\n",
      "epoch:25 step:23502 [D loss: 0.197339, acc.: 70.31%] [G loss: 0.460450]\n",
      "epoch:25 step:23503 [D loss: 0.256846, acc.: 57.03%] [G loss: 0.409251]\n",
      "epoch:25 step:23504 [D loss: 0.255687, acc.: 54.69%] [G loss: 0.442126]\n",
      "epoch:25 step:23505 [D loss: 0.231978, acc.: 60.16%] [G loss: 0.390054]\n",
      "epoch:25 step:23506 [D loss: 0.247701, acc.: 56.25%] [G loss: 0.412565]\n",
      "epoch:25 step:23507 [D loss: 0.219341, acc.: 64.06%] [G loss: 0.422147]\n",
      "epoch:25 step:23508 [D loss: 0.202480, acc.: 69.53%] [G loss: 0.425598]\n",
      "epoch:25 step:23509 [D loss: 0.207546, acc.: 62.50%] [G loss: 0.414540]\n",
      "epoch:25 step:23510 [D loss: 0.222525, acc.: 60.94%] [G loss: 0.439785]\n",
      "epoch:25 step:23511 [D loss: 0.241281, acc.: 57.81%] [G loss: 0.403758]\n",
      "epoch:25 step:23512 [D loss: 0.239208, acc.: 57.03%] [G loss: 0.420260]\n",
      "epoch:25 step:23513 [D loss: 0.209636, acc.: 68.75%] [G loss: 0.435051]\n",
      "epoch:25 step:23514 [D loss: 0.226052, acc.: 63.28%] [G loss: 0.420094]\n",
      "epoch:25 step:23515 [D loss: 0.211875, acc.: 64.84%] [G loss: 0.403778]\n",
      "epoch:25 step:23516 [D loss: 0.231793, acc.: 60.16%] [G loss: 0.440853]\n",
      "epoch:25 step:23517 [D loss: 0.210200, acc.: 61.72%] [G loss: 0.431169]\n",
      "epoch:25 step:23518 [D loss: 0.187618, acc.: 69.53%] [G loss: 0.472112]\n",
      "epoch:25 step:23519 [D loss: 0.235817, acc.: 64.84%] [G loss: 0.426782]\n",
      "epoch:25 step:23520 [D loss: 0.255910, acc.: 57.81%] [G loss: 0.411393]\n",
      "epoch:25 step:23521 [D loss: 0.223508, acc.: 60.94%] [G loss: 0.441552]\n",
      "epoch:25 step:23522 [D loss: 0.219820, acc.: 61.72%] [G loss: 0.527449]\n",
      "epoch:25 step:23523 [D loss: 0.252851, acc.: 58.59%] [G loss: 0.434803]\n",
      "epoch:25 step:23524 [D loss: 0.258115, acc.: 53.91%] [G loss: 0.395868]\n",
      "epoch:25 step:23525 [D loss: 0.218994, acc.: 62.50%] [G loss: 0.435521]\n",
      "epoch:25 step:23526 [D loss: 0.248357, acc.: 59.38%] [G loss: 0.421872]\n",
      "epoch:25 step:23527 [D loss: 0.228464, acc.: 57.81%] [G loss: 0.421889]\n",
      "epoch:25 step:23528 [D loss: 0.228102, acc.: 60.94%] [G loss: 0.400544]\n",
      "epoch:25 step:23529 [D loss: 0.214474, acc.: 60.94%] [G loss: 0.431339]\n",
      "epoch:25 step:23530 [D loss: 0.247135, acc.: 53.91%] [G loss: 0.356357]\n",
      "epoch:25 step:23531 [D loss: 0.194637, acc.: 71.88%] [G loss: 0.434181]\n",
      "epoch:25 step:23532 [D loss: 0.200801, acc.: 66.41%] [G loss: 0.469467]\n",
      "epoch:25 step:23533 [D loss: 0.266888, acc.: 51.56%] [G loss: 0.432988]\n",
      "epoch:25 step:23534 [D loss: 0.260045, acc.: 54.69%] [G loss: 0.419632]\n",
      "epoch:25 step:23535 [D loss: 0.247158, acc.: 53.91%] [G loss: 0.402186]\n",
      "epoch:25 step:23536 [D loss: 0.228017, acc.: 64.06%] [G loss: 0.379977]\n",
      "epoch:25 step:23537 [D loss: 0.197805, acc.: 71.09%] [G loss: 0.414890]\n",
      "epoch:25 step:23538 [D loss: 0.213862, acc.: 63.28%] [G loss: 0.430142]\n",
      "epoch:25 step:23539 [D loss: 0.217595, acc.: 68.75%] [G loss: 0.454330]\n",
      "epoch:25 step:23540 [D loss: 0.188293, acc.: 69.53%] [G loss: 0.466300]\n",
      "epoch:25 step:23541 [D loss: 0.223946, acc.: 63.28%] [G loss: 0.471587]\n",
      "epoch:25 step:23542 [D loss: 0.194821, acc.: 73.44%] [G loss: 0.474770]\n",
      "epoch:25 step:23543 [D loss: 0.214367, acc.: 66.41%] [G loss: 0.454230]\n",
      "epoch:25 step:23544 [D loss: 0.179938, acc.: 70.31%] [G loss: 0.529136]\n",
      "epoch:25 step:23545 [D loss: 0.247753, acc.: 60.94%] [G loss: 0.488270]\n",
      "epoch:25 step:23546 [D loss: 0.225455, acc.: 63.28%] [G loss: 0.440180]\n",
      "epoch:25 step:23547 [D loss: 0.197214, acc.: 71.09%] [G loss: 0.459098]\n",
      "epoch:25 step:23548 [D loss: 0.220755, acc.: 60.94%] [G loss: 0.460964]\n",
      "epoch:25 step:23549 [D loss: 0.248264, acc.: 47.66%] [G loss: 0.451338]\n",
      "epoch:25 step:23550 [D loss: 0.232486, acc.: 61.72%] [G loss: 0.452722]\n",
      "epoch:25 step:23551 [D loss: 0.196526, acc.: 71.88%] [G loss: 0.467536]\n",
      "epoch:25 step:23552 [D loss: 0.217126, acc.: 65.62%] [G loss: 0.425985]\n",
      "epoch:25 step:23553 [D loss: 0.235053, acc.: 60.16%] [G loss: 0.394779]\n",
      "epoch:25 step:23554 [D loss: 0.218354, acc.: 64.84%] [G loss: 0.406961]\n",
      "epoch:25 step:23555 [D loss: 0.217729, acc.: 63.28%] [G loss: 0.410682]\n",
      "epoch:25 step:23556 [D loss: 0.219895, acc.: 64.06%] [G loss: 0.422941]\n",
      "epoch:25 step:23557 [D loss: 0.221541, acc.: 61.72%] [G loss: 0.425787]\n",
      "epoch:25 step:23558 [D loss: 0.228297, acc.: 65.62%] [G loss: 0.430416]\n",
      "epoch:25 step:23559 [D loss: 0.227312, acc.: 63.28%] [G loss: 0.411496]\n",
      "epoch:25 step:23560 [D loss: 0.215364, acc.: 67.19%] [G loss: 0.440143]\n",
      "epoch:25 step:23561 [D loss: 0.238540, acc.: 66.41%] [G loss: 0.475977]\n",
      "epoch:25 step:23562 [D loss: 0.235174, acc.: 58.59%] [G loss: 0.439975]\n",
      "epoch:25 step:23563 [D loss: 0.254866, acc.: 54.69%] [G loss: 0.376509]\n",
      "epoch:25 step:23564 [D loss: 0.210755, acc.: 64.84%] [G loss: 0.432329]\n",
      "epoch:25 step:23565 [D loss: 0.230085, acc.: 64.84%] [G loss: 0.387791]\n",
      "epoch:25 step:23566 [D loss: 0.244952, acc.: 59.38%] [G loss: 0.404060]\n",
      "epoch:25 step:23567 [D loss: 0.260158, acc.: 49.22%] [G loss: 0.373422]\n",
      "epoch:25 step:23568 [D loss: 0.241767, acc.: 56.25%] [G loss: 0.418928]\n",
      "epoch:25 step:23569 [D loss: 0.234220, acc.: 64.84%] [G loss: 0.426277]\n",
      "epoch:25 step:23570 [D loss: 0.218039, acc.: 65.62%] [G loss: 0.431111]\n",
      "epoch:25 step:23571 [D loss: 0.239060, acc.: 60.16%] [G loss: 0.467893]\n",
      "epoch:25 step:23572 [D loss: 0.251838, acc.: 59.38%] [G loss: 0.375832]\n",
      "epoch:25 step:23573 [D loss: 0.243525, acc.: 57.81%] [G loss: 0.384143]\n",
      "epoch:25 step:23574 [D loss: 0.225135, acc.: 67.19%] [G loss: 0.445884]\n",
      "epoch:25 step:23575 [D loss: 0.244740, acc.: 55.47%] [G loss: 0.430781]\n",
      "epoch:25 step:23576 [D loss: 0.200682, acc.: 72.66%] [G loss: 0.422609]\n",
      "epoch:25 step:23577 [D loss: 0.235000, acc.: 64.84%] [G loss: 0.446855]\n",
      "epoch:25 step:23578 [D loss: 0.230513, acc.: 60.94%] [G loss: 0.463521]\n",
      "epoch:25 step:23579 [D loss: 0.223350, acc.: 61.72%] [G loss: 0.437890]\n",
      "epoch:25 step:23580 [D loss: 0.223027, acc.: 58.59%] [G loss: 0.437692]\n",
      "epoch:25 step:23581 [D loss: 0.230988, acc.: 59.38%] [G loss: 0.391632]\n",
      "epoch:25 step:23582 [D loss: 0.219403, acc.: 60.94%] [G loss: 0.439327]\n",
      "epoch:25 step:23583 [D loss: 0.228346, acc.: 64.06%] [G loss: 0.423967]\n",
      "epoch:25 step:23584 [D loss: 0.230082, acc.: 53.12%] [G loss: 0.433454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23585 [D loss: 0.257637, acc.: 56.25%] [G loss: 0.407294]\n",
      "epoch:25 step:23586 [D loss: 0.243032, acc.: 53.91%] [G loss: 0.424761]\n",
      "epoch:25 step:23587 [D loss: 0.252023, acc.: 59.38%] [G loss: 0.430656]\n",
      "epoch:25 step:23588 [D loss: 0.246209, acc.: 53.12%] [G loss: 0.411992]\n",
      "epoch:25 step:23589 [D loss: 0.212209, acc.: 66.41%] [G loss: 0.464395]\n",
      "epoch:25 step:23590 [D loss: 0.223779, acc.: 64.84%] [G loss: 0.428886]\n",
      "epoch:25 step:23591 [D loss: 0.210151, acc.: 69.53%] [G loss: 0.442955]\n",
      "epoch:25 step:23592 [D loss: 0.215765, acc.: 64.06%] [G loss: 0.437003]\n",
      "epoch:25 step:23593 [D loss: 0.199805, acc.: 72.66%] [G loss: 0.457642]\n",
      "epoch:25 step:23594 [D loss: 0.221565, acc.: 61.72%] [G loss: 0.456189]\n",
      "epoch:25 step:23595 [D loss: 0.260267, acc.: 56.25%] [G loss: 0.378210]\n",
      "epoch:25 step:23596 [D loss: 0.243236, acc.: 57.03%] [G loss: 0.405751]\n",
      "epoch:25 step:23597 [D loss: 0.231227, acc.: 60.94%] [G loss: 0.438465]\n",
      "epoch:25 step:23598 [D loss: 0.209747, acc.: 68.75%] [G loss: 0.451121]\n",
      "epoch:25 step:23599 [D loss: 0.249424, acc.: 52.34%] [G loss: 0.396975]\n",
      "epoch:25 step:23600 [D loss: 0.225607, acc.: 62.50%] [G loss: 0.397598]\n",
      "##############\n",
      "[2.62090649 1.89420708 6.11056487 4.59513624 3.81694662 5.55391639\n",
      " 4.43405474 4.75652295 4.38083187 3.81076053]\n",
      "##########\n",
      "epoch:25 step:23601 [D loss: 0.233139, acc.: 57.81%] [G loss: 0.432422]\n",
      "epoch:25 step:23602 [D loss: 0.227970, acc.: 58.59%] [G loss: 0.407087]\n",
      "epoch:25 step:23603 [D loss: 0.238624, acc.: 57.81%] [G loss: 0.412182]\n",
      "epoch:25 step:23604 [D loss: 0.217654, acc.: 65.62%] [G loss: 0.438682]\n",
      "epoch:25 step:23605 [D loss: 0.229156, acc.: 62.50%] [G loss: 0.443139]\n",
      "epoch:25 step:23606 [D loss: 0.212312, acc.: 61.72%] [G loss: 0.479879]\n",
      "epoch:25 step:23607 [D loss: 0.229458, acc.: 65.62%] [G loss: 0.445762]\n",
      "epoch:25 step:23608 [D loss: 0.240240, acc.: 60.16%] [G loss: 0.426373]\n",
      "epoch:25 step:23609 [D loss: 0.231650, acc.: 56.25%] [G loss: 0.424815]\n",
      "epoch:25 step:23610 [D loss: 0.264567, acc.: 57.03%] [G loss: 0.424038]\n",
      "epoch:25 step:23611 [D loss: 0.229742, acc.: 62.50%] [G loss: 0.427936]\n",
      "epoch:25 step:23612 [D loss: 0.244278, acc.: 58.59%] [G loss: 0.386706]\n",
      "epoch:25 step:23613 [D loss: 0.249427, acc.: 59.38%] [G loss: 0.407022]\n",
      "epoch:25 step:23614 [D loss: 0.228974, acc.: 58.59%] [G loss: 0.424764]\n",
      "epoch:25 step:23615 [D loss: 0.234125, acc.: 64.06%] [G loss: 0.418101]\n",
      "epoch:25 step:23616 [D loss: 0.230808, acc.: 62.50%] [G loss: 0.422472]\n",
      "epoch:25 step:23617 [D loss: 0.250288, acc.: 55.47%] [G loss: 0.413670]\n",
      "epoch:25 step:23618 [D loss: 0.231830, acc.: 60.16%] [G loss: 0.423829]\n",
      "epoch:25 step:23619 [D loss: 0.202743, acc.: 67.19%] [G loss: 0.455281]\n",
      "epoch:25 step:23620 [D loss: 0.213940, acc.: 61.72%] [G loss: 0.431930]\n",
      "epoch:25 step:23621 [D loss: 0.245248, acc.: 61.72%] [G loss: 0.425864]\n",
      "epoch:25 step:23622 [D loss: 0.226222, acc.: 61.72%] [G loss: 0.419574]\n",
      "epoch:25 step:23623 [D loss: 0.189311, acc.: 76.56%] [G loss: 0.415302]\n",
      "epoch:25 step:23624 [D loss: 0.214501, acc.: 66.41%] [G loss: 0.451189]\n",
      "epoch:25 step:23625 [D loss: 0.246861, acc.: 57.03%] [G loss: 0.410140]\n",
      "epoch:25 step:23626 [D loss: 0.228685, acc.: 60.16%] [G loss: 0.380599]\n",
      "epoch:25 step:23627 [D loss: 0.217009, acc.: 59.38%] [G loss: 0.394178]\n",
      "epoch:25 step:23628 [D loss: 0.270753, acc.: 48.44%] [G loss: 0.412369]\n",
      "epoch:25 step:23629 [D loss: 0.208167, acc.: 68.75%] [G loss: 0.453354]\n",
      "epoch:25 step:23630 [D loss: 0.225252, acc.: 61.72%] [G loss: 0.466832]\n",
      "epoch:25 step:23631 [D loss: 0.251291, acc.: 59.38%] [G loss: 0.471822]\n",
      "epoch:25 step:23632 [D loss: 0.217360, acc.: 64.84%] [G loss: 0.473643]\n",
      "epoch:25 step:23633 [D loss: 0.178577, acc.: 73.44%] [G loss: 0.490120]\n",
      "epoch:25 step:23634 [D loss: 0.187936, acc.: 74.22%] [G loss: 0.481288]\n",
      "epoch:25 step:23635 [D loss: 0.269979, acc.: 54.69%] [G loss: 0.432732]\n",
      "epoch:25 step:23636 [D loss: 0.243500, acc.: 62.50%] [G loss: 0.405325]\n",
      "epoch:25 step:23637 [D loss: 0.225352, acc.: 62.50%] [G loss: 0.382369]\n",
      "epoch:25 step:23638 [D loss: 0.240054, acc.: 56.25%] [G loss: 0.398923]\n",
      "epoch:25 step:23639 [D loss: 0.261680, acc.: 51.56%] [G loss: 0.351818]\n",
      "epoch:25 step:23640 [D loss: 0.234730, acc.: 54.69%] [G loss: 0.410309]\n",
      "epoch:25 step:23641 [D loss: 0.208519, acc.: 67.97%] [G loss: 0.425856]\n",
      "epoch:25 step:23642 [D loss: 0.236534, acc.: 64.84%] [G loss: 0.405009]\n",
      "epoch:25 step:23643 [D loss: 0.195113, acc.: 67.97%] [G loss: 0.441799]\n",
      "epoch:25 step:23644 [D loss: 0.191831, acc.: 74.22%] [G loss: 0.475219]\n",
      "epoch:25 step:23645 [D loss: 0.286783, acc.: 55.47%] [G loss: 0.449605]\n",
      "epoch:25 step:23646 [D loss: 0.210179, acc.: 65.62%] [G loss: 0.464085]\n",
      "epoch:25 step:23647 [D loss: 0.199472, acc.: 67.19%] [G loss: 0.446088]\n",
      "epoch:25 step:23648 [D loss: 0.220554, acc.: 62.50%] [G loss: 0.485080]\n",
      "epoch:25 step:23649 [D loss: 0.244967, acc.: 58.59%] [G loss: 0.418683]\n",
      "epoch:25 step:23650 [D loss: 0.253407, acc.: 57.03%] [G loss: 0.408547]\n",
      "epoch:25 step:23651 [D loss: 0.242991, acc.: 64.06%] [G loss: 0.421059]\n",
      "epoch:25 step:23652 [D loss: 0.207670, acc.: 67.97%] [G loss: 0.431488]\n",
      "epoch:25 step:23653 [D loss: 0.220206, acc.: 66.41%] [G loss: 0.420236]\n",
      "epoch:25 step:23654 [D loss: 0.221673, acc.: 67.97%] [G loss: 0.433279]\n",
      "epoch:25 step:23655 [D loss: 0.202992, acc.: 66.41%] [G loss: 0.420973]\n",
      "epoch:25 step:23656 [D loss: 0.166629, acc.: 82.03%] [G loss: 0.478919]\n",
      "epoch:25 step:23657 [D loss: 0.169224, acc.: 74.22%] [G loss: 0.491654]\n",
      "epoch:25 step:23658 [D loss: 0.232444, acc.: 61.72%] [G loss: 0.467659]\n",
      "epoch:25 step:23659 [D loss: 0.252390, acc.: 57.81%] [G loss: 0.406131]\n",
      "epoch:25 step:23660 [D loss: 0.229793, acc.: 61.72%] [G loss: 0.426775]\n",
      "epoch:25 step:23661 [D loss: 0.219585, acc.: 56.25%] [G loss: 0.434152]\n",
      "epoch:25 step:23662 [D loss: 0.201420, acc.: 67.97%] [G loss: 0.455072]\n",
      "epoch:25 step:23663 [D loss: 0.201895, acc.: 69.53%] [G loss: 0.435722]\n",
      "epoch:25 step:23664 [D loss: 0.199081, acc.: 70.31%] [G loss: 0.422890]\n",
      "epoch:25 step:23665 [D loss: 0.246946, acc.: 58.59%] [G loss: 0.411416]\n",
      "epoch:25 step:23666 [D loss: 0.212881, acc.: 63.28%] [G loss: 0.423262]\n",
      "epoch:25 step:23667 [D loss: 0.195033, acc.: 73.44%] [G loss: 0.460760]\n",
      "epoch:25 step:23668 [D loss: 0.219622, acc.: 62.50%] [G loss: 0.463089]\n",
      "epoch:25 step:23669 [D loss: 0.207701, acc.: 66.41%] [G loss: 0.400428]\n",
      "epoch:25 step:23670 [D loss: 0.219488, acc.: 59.38%] [G loss: 0.429893]\n",
      "epoch:25 step:23671 [D loss: 0.246243, acc.: 54.69%] [G loss: 0.466456]\n",
      "epoch:25 step:23672 [D loss: 0.232347, acc.: 59.38%] [G loss: 0.464865]\n",
      "epoch:25 step:23673 [D loss: 0.217125, acc.: 67.97%] [G loss: 0.461341]\n",
      "epoch:25 step:23674 [D loss: 0.255818, acc.: 55.47%] [G loss: 0.464193]\n",
      "epoch:25 step:23675 [D loss: 0.251235, acc.: 55.47%] [G loss: 0.403044]\n",
      "epoch:25 step:23676 [D loss: 0.255559, acc.: 48.44%] [G loss: 0.464495]\n",
      "epoch:25 step:23677 [D loss: 0.237315, acc.: 56.25%] [G loss: 0.421602]\n",
      "epoch:25 step:23678 [D loss: 0.234917, acc.: 58.59%] [G loss: 0.430436]\n",
      "epoch:25 step:23679 [D loss: 0.236684, acc.: 61.72%] [G loss: 0.404389]\n",
      "epoch:25 step:23680 [D loss: 0.234381, acc.: 56.25%] [G loss: 0.428449]\n",
      "epoch:25 step:23681 [D loss: 0.226529, acc.: 60.94%] [G loss: 0.431955]\n",
      "epoch:25 step:23682 [D loss: 0.247906, acc.: 55.47%] [G loss: 0.401112]\n",
      "epoch:25 step:23683 [D loss: 0.206439, acc.: 67.97%] [G loss: 0.463582]\n",
      "epoch:25 step:23684 [D loss: 0.223912, acc.: 63.28%] [G loss: 0.437840]\n",
      "epoch:25 step:23685 [D loss: 0.226705, acc.: 59.38%] [G loss: 0.434410]\n",
      "epoch:25 step:23686 [D loss: 0.221425, acc.: 64.84%] [G loss: 0.448315]\n",
      "epoch:25 step:23687 [D loss: 0.206384, acc.: 64.06%] [G loss: 0.440232]\n",
      "epoch:25 step:23688 [D loss: 0.238269, acc.: 59.38%] [G loss: 0.409392]\n",
      "epoch:25 step:23689 [D loss: 0.222174, acc.: 66.41%] [G loss: 0.433569]\n",
      "epoch:25 step:23690 [D loss: 0.216569, acc.: 63.28%] [G loss: 0.435400]\n",
      "epoch:25 step:23691 [D loss: 0.251364, acc.: 59.38%] [G loss: 0.424831]\n",
      "epoch:25 step:23692 [D loss: 0.232937, acc.: 57.81%] [G loss: 0.416410]\n",
      "epoch:25 step:23693 [D loss: 0.225877, acc.: 60.16%] [G loss: 0.419779]\n",
      "epoch:25 step:23694 [D loss: 0.208760, acc.: 71.88%] [G loss: 0.442498]\n",
      "epoch:25 step:23695 [D loss: 0.224426, acc.: 64.06%] [G loss: 0.414071]\n",
      "epoch:25 step:23696 [D loss: 0.214522, acc.: 67.97%] [G loss: 0.428963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23697 [D loss: 0.214362, acc.: 67.97%] [G loss: 0.434744]\n",
      "epoch:25 step:23698 [D loss: 0.223479, acc.: 55.47%] [G loss: 0.407941]\n",
      "epoch:25 step:23699 [D loss: 0.209130, acc.: 67.19%] [G loss: 0.454346]\n",
      "epoch:25 step:23700 [D loss: 0.215870, acc.: 65.62%] [G loss: 0.423577]\n",
      "epoch:25 step:23701 [D loss: 0.207543, acc.: 66.41%] [G loss: 0.479887]\n",
      "epoch:25 step:23702 [D loss: 0.243155, acc.: 54.69%] [G loss: 0.451971]\n",
      "epoch:25 step:23703 [D loss: 0.246483, acc.: 56.25%] [G loss: 0.385530]\n",
      "epoch:25 step:23704 [D loss: 0.236161, acc.: 57.03%] [G loss: 0.429729]\n",
      "epoch:25 step:23705 [D loss: 0.204956, acc.: 70.31%] [G loss: 0.456714]\n",
      "epoch:25 step:23706 [D loss: 0.246088, acc.: 55.47%] [G loss: 0.462162]\n",
      "epoch:25 step:23707 [D loss: 0.225883, acc.: 61.72%] [G loss: 0.456576]\n",
      "epoch:25 step:23708 [D loss: 0.217607, acc.: 64.84%] [G loss: 0.461700]\n",
      "epoch:25 step:23709 [D loss: 0.229876, acc.: 59.38%] [G loss: 0.415445]\n",
      "epoch:25 step:23710 [D loss: 0.233044, acc.: 59.38%] [G loss: 0.428460]\n",
      "epoch:25 step:23711 [D loss: 0.205697, acc.: 68.75%] [G loss: 0.418568]\n",
      "epoch:25 step:23712 [D loss: 0.227477, acc.: 60.94%] [G loss: 0.402999]\n",
      "epoch:25 step:23713 [D loss: 0.217262, acc.: 63.28%] [G loss: 0.427499]\n",
      "epoch:25 step:23714 [D loss: 0.208048, acc.: 67.19%] [G loss: 0.457290]\n",
      "epoch:25 step:23715 [D loss: 0.224752, acc.: 62.50%] [G loss: 0.463407]\n",
      "epoch:25 step:23716 [D loss: 0.286692, acc.: 46.88%] [G loss: 0.412885]\n",
      "epoch:25 step:23717 [D loss: 0.215053, acc.: 64.06%] [G loss: 0.422425]\n",
      "epoch:25 step:23718 [D loss: 0.202660, acc.: 73.44%] [G loss: 0.464104]\n",
      "epoch:25 step:23719 [D loss: 0.245742, acc.: 55.47%] [G loss: 0.395874]\n",
      "epoch:25 step:23720 [D loss: 0.221037, acc.: 63.28%] [G loss: 0.434664]\n",
      "epoch:25 step:23721 [D loss: 0.201385, acc.: 67.97%] [G loss: 0.426309]\n",
      "epoch:25 step:23722 [D loss: 0.230554, acc.: 62.50%] [G loss: 0.419629]\n",
      "epoch:25 step:23723 [D loss: 0.200357, acc.: 73.44%] [G loss: 0.461750]\n",
      "epoch:25 step:23724 [D loss: 0.200486, acc.: 71.88%] [G loss: 0.462165]\n",
      "epoch:25 step:23725 [D loss: 0.181685, acc.: 75.78%] [G loss: 0.478635]\n",
      "epoch:25 step:23726 [D loss: 0.249525, acc.: 51.56%] [G loss: 0.460761]\n",
      "epoch:25 step:23727 [D loss: 0.230749, acc.: 61.72%] [G loss: 0.396190]\n",
      "epoch:25 step:23728 [D loss: 0.233797, acc.: 65.62%] [G loss: 0.430000]\n",
      "epoch:25 step:23729 [D loss: 0.234884, acc.: 57.03%] [G loss: 0.434141]\n",
      "epoch:25 step:23730 [D loss: 0.224184, acc.: 59.38%] [G loss: 0.479833]\n",
      "epoch:25 step:23731 [D loss: 0.233411, acc.: 60.16%] [G loss: 0.404629]\n",
      "epoch:25 step:23732 [D loss: 0.224419, acc.: 61.72%] [G loss: 0.419143]\n",
      "epoch:25 step:23733 [D loss: 0.237021, acc.: 58.59%] [G loss: 0.422916]\n",
      "epoch:25 step:23734 [D loss: 0.203437, acc.: 71.88%] [G loss: 0.456363]\n",
      "epoch:25 step:23735 [D loss: 0.198887, acc.: 69.53%] [G loss: 0.436919]\n",
      "epoch:25 step:23736 [D loss: 0.191236, acc.: 73.44%] [G loss: 0.441261]\n",
      "epoch:25 step:23737 [D loss: 0.214743, acc.: 62.50%] [G loss: 0.478373]\n",
      "epoch:25 step:23738 [D loss: 0.176750, acc.: 72.66%] [G loss: 0.512876]\n",
      "epoch:25 step:23739 [D loss: 0.184312, acc.: 73.44%] [G loss: 0.481588]\n",
      "epoch:25 step:23740 [D loss: 0.180433, acc.: 73.44%] [G loss: 0.505825]\n",
      "epoch:25 step:23741 [D loss: 0.269088, acc.: 53.91%] [G loss: 0.408256]\n",
      "epoch:25 step:23742 [D loss: 0.238485, acc.: 57.81%] [G loss: 0.428550]\n",
      "epoch:25 step:23743 [D loss: 0.209356, acc.: 60.16%] [G loss: 0.424844]\n",
      "epoch:25 step:23744 [D loss: 0.207399, acc.: 67.97%] [G loss: 0.397687]\n",
      "epoch:25 step:23745 [D loss: 0.221415, acc.: 64.06%] [G loss: 0.420971]\n",
      "epoch:25 step:23746 [D loss: 0.198436, acc.: 69.53%] [G loss: 0.454454]\n",
      "epoch:25 step:23747 [D loss: 0.209363, acc.: 67.19%] [G loss: 0.433156]\n",
      "epoch:25 step:23748 [D loss: 0.255224, acc.: 55.47%] [G loss: 0.404045]\n",
      "epoch:25 step:23749 [D loss: 0.235294, acc.: 64.06%] [G loss: 0.396922]\n",
      "epoch:25 step:23750 [D loss: 0.216957, acc.: 62.50%] [G loss: 0.431273]\n",
      "epoch:25 step:23751 [D loss: 0.196944, acc.: 69.53%] [G loss: 0.464276]\n",
      "epoch:25 step:23752 [D loss: 0.205585, acc.: 67.97%] [G loss: 0.446636]\n",
      "epoch:25 step:23753 [D loss: 0.202355, acc.: 64.06%] [G loss: 0.433781]\n",
      "epoch:25 step:23754 [D loss: 0.217120, acc.: 64.06%] [G loss: 0.416646]\n",
      "epoch:25 step:23755 [D loss: 0.217131, acc.: 64.06%] [G loss: 0.454510]\n",
      "epoch:25 step:23756 [D loss: 0.230066, acc.: 62.50%] [G loss: 0.398435]\n",
      "epoch:25 step:23757 [D loss: 0.215011, acc.: 65.62%] [G loss: 0.422633]\n",
      "epoch:25 step:23758 [D loss: 0.226456, acc.: 64.84%] [G loss: 0.431220]\n",
      "epoch:25 step:23759 [D loss: 0.203934, acc.: 66.41%] [G loss: 0.443674]\n",
      "epoch:25 step:23760 [D loss: 0.195312, acc.: 70.31%] [G loss: 0.440376]\n",
      "epoch:25 step:23761 [D loss: 0.188809, acc.: 71.88%] [G loss: 0.477927]\n",
      "epoch:25 step:23762 [D loss: 0.229756, acc.: 60.16%] [G loss: 0.457483]\n",
      "epoch:25 step:23763 [D loss: 0.214029, acc.: 66.41%] [G loss: 0.406105]\n",
      "epoch:25 step:23764 [D loss: 0.213649, acc.: 67.19%] [G loss: 0.409513]\n",
      "epoch:25 step:23765 [D loss: 0.197442, acc.: 71.09%] [G loss: 0.395755]\n",
      "epoch:25 step:23766 [D loss: 0.283927, acc.: 50.78%] [G loss: 0.427860]\n",
      "epoch:25 step:23767 [D loss: 0.237746, acc.: 59.38%] [G loss: 0.468105]\n",
      "epoch:25 step:23768 [D loss: 0.208770, acc.: 63.28%] [G loss: 0.468013]\n",
      "epoch:25 step:23769 [D loss: 0.210552, acc.: 67.97%] [G loss: 0.478687]\n",
      "epoch:25 step:23770 [D loss: 0.244279, acc.: 59.38%] [G loss: 0.447094]\n",
      "epoch:25 step:23771 [D loss: 0.198683, acc.: 66.41%] [G loss: 0.458745]\n",
      "epoch:25 step:23772 [D loss: 0.192606, acc.: 72.66%] [G loss: 0.546114]\n",
      "epoch:25 step:23773 [D loss: 0.262915, acc.: 58.59%] [G loss: 0.453005]\n",
      "epoch:25 step:23774 [D loss: 0.284029, acc.: 50.78%] [G loss: 0.396440]\n",
      "epoch:25 step:23775 [D loss: 0.215396, acc.: 62.50%] [G loss: 0.391299]\n",
      "epoch:25 step:23776 [D loss: 0.248267, acc.: 52.34%] [G loss: 0.388962]\n",
      "epoch:25 step:23777 [D loss: 0.216875, acc.: 65.62%] [G loss: 0.401805]\n",
      "epoch:25 step:23778 [D loss: 0.208101, acc.: 69.53%] [G loss: 0.389741]\n",
      "epoch:25 step:23779 [D loss: 0.187372, acc.: 75.00%] [G loss: 0.455256]\n",
      "epoch:25 step:23780 [D loss: 0.231219, acc.: 62.50%] [G loss: 0.414468]\n",
      "epoch:25 step:23781 [D loss: 0.235024, acc.: 59.38%] [G loss: 0.452724]\n",
      "epoch:25 step:23782 [D loss: 0.230493, acc.: 57.03%] [G loss: 0.392091]\n",
      "epoch:25 step:23783 [D loss: 0.222563, acc.: 61.72%] [G loss: 0.420650]\n",
      "epoch:25 step:23784 [D loss: 0.188819, acc.: 72.66%] [G loss: 0.467667]\n",
      "epoch:25 step:23785 [D loss: 0.202434, acc.: 66.41%] [G loss: 0.438520]\n",
      "epoch:25 step:23786 [D loss: 0.203007, acc.: 69.53%] [G loss: 0.452052]\n",
      "epoch:25 step:23787 [D loss: 0.241515, acc.: 59.38%] [G loss: 0.427843]\n",
      "epoch:25 step:23788 [D loss: 0.233199, acc.: 57.81%] [G loss: 0.437152]\n",
      "epoch:25 step:23789 [D loss: 0.221556, acc.: 61.72%] [G loss: 0.401471]\n",
      "epoch:25 step:23790 [D loss: 0.228607, acc.: 63.28%] [G loss: 0.456493]\n",
      "epoch:25 step:23791 [D loss: 0.231712, acc.: 57.81%] [G loss: 0.466712]\n",
      "epoch:25 step:23792 [D loss: 0.202180, acc.: 67.19%] [G loss: 0.453837]\n",
      "epoch:25 step:23793 [D loss: 0.247341, acc.: 57.03%] [G loss: 0.408442]\n",
      "epoch:25 step:23794 [D loss: 0.213532, acc.: 70.31%] [G loss: 0.444440]\n",
      "epoch:25 step:23795 [D loss: 0.206756, acc.: 71.09%] [G loss: 0.460788]\n",
      "epoch:25 step:23796 [D loss: 0.187833, acc.: 70.31%] [G loss: 0.481845]\n",
      "epoch:25 step:23797 [D loss: 0.200721, acc.: 67.19%] [G loss: 0.502169]\n",
      "epoch:25 step:23798 [D loss: 0.244914, acc.: 59.38%] [G loss: 0.443462]\n",
      "epoch:25 step:23799 [D loss: 0.193270, acc.: 71.09%] [G loss: 0.422110]\n",
      "epoch:25 step:23800 [D loss: 0.231598, acc.: 64.06%] [G loss: 0.437878]\n",
      "##############\n",
      "[2.65503074 1.93997092 5.91190078 4.96112321 3.67220346 5.7005915\n",
      " 4.50593397 4.80875943 4.52993573 3.97343242]\n",
      "##########\n",
      "epoch:25 step:23801 [D loss: 0.262668, acc.: 54.69%] [G loss: 0.419129]\n",
      "epoch:25 step:23802 [D loss: 0.243259, acc.: 62.50%] [G loss: 0.431858]\n",
      "epoch:25 step:23803 [D loss: 0.227168, acc.: 59.38%] [G loss: 0.419056]\n",
      "epoch:25 step:23804 [D loss: 0.237383, acc.: 61.72%] [G loss: 0.439432]\n",
      "epoch:25 step:23805 [D loss: 0.227370, acc.: 61.72%] [G loss: 0.432846]\n",
      "epoch:25 step:23806 [D loss: 0.217588, acc.: 62.50%] [G loss: 0.434065]\n",
      "epoch:25 step:23807 [D loss: 0.246397, acc.: 56.25%] [G loss: 0.407388]\n",
      "epoch:25 step:23808 [D loss: 0.205318, acc.: 64.84%] [G loss: 0.455323]\n",
      "epoch:25 step:23809 [D loss: 0.194862, acc.: 71.09%] [G loss: 0.424582]\n",
      "epoch:25 step:23810 [D loss: 0.189835, acc.: 71.09%] [G loss: 0.445496]\n",
      "epoch:25 step:23811 [D loss: 0.257938, acc.: 53.91%] [G loss: 0.430648]\n",
      "epoch:25 step:23812 [D loss: 0.218989, acc.: 61.72%] [G loss: 0.458739]\n",
      "epoch:25 step:23813 [D loss: 0.216954, acc.: 60.16%] [G loss: 0.440950]\n",
      "epoch:25 step:23814 [D loss: 0.224355, acc.: 60.94%] [G loss: 0.431550]\n",
      "epoch:25 step:23815 [D loss: 0.236103, acc.: 60.94%] [G loss: 0.417258]\n",
      "epoch:25 step:23816 [D loss: 0.242501, acc.: 61.72%] [G loss: 0.392129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23817 [D loss: 0.200129, acc.: 67.19%] [G loss: 0.418104]\n",
      "epoch:25 step:23818 [D loss: 0.224588, acc.: 57.03%] [G loss: 0.446875]\n",
      "epoch:25 step:23819 [D loss: 0.231109, acc.: 64.06%] [G loss: 0.472395]\n",
      "epoch:25 step:23820 [D loss: 0.205139, acc.: 69.53%] [G loss: 0.437183]\n",
      "epoch:25 step:23821 [D loss: 0.258044, acc.: 52.34%] [G loss: 0.422218]\n",
      "epoch:25 step:23822 [D loss: 0.223006, acc.: 63.28%] [G loss: 0.442221]\n",
      "epoch:25 step:23823 [D loss: 0.209162, acc.: 72.66%] [G loss: 0.407115]\n",
      "epoch:25 step:23824 [D loss: 0.195694, acc.: 73.44%] [G loss: 0.477260]\n",
      "epoch:25 step:23825 [D loss: 0.257072, acc.: 54.69%] [G loss: 0.401839]\n",
      "epoch:25 step:23826 [D loss: 0.216765, acc.: 59.38%] [G loss: 0.407669]\n",
      "epoch:25 step:23827 [D loss: 0.242472, acc.: 58.59%] [G loss: 0.391555]\n",
      "epoch:25 step:23828 [D loss: 0.232095, acc.: 62.50%] [G loss: 0.396131]\n",
      "epoch:25 step:23829 [D loss: 0.242486, acc.: 55.47%] [G loss: 0.379689]\n",
      "epoch:25 step:23830 [D loss: 0.197814, acc.: 71.09%] [G loss: 0.414190]\n",
      "epoch:25 step:23831 [D loss: 0.189181, acc.: 74.22%] [G loss: 0.518157]\n",
      "epoch:25 step:23832 [D loss: 0.240737, acc.: 57.81%] [G loss: 0.447134]\n",
      "epoch:25 step:23833 [D loss: 0.258570, acc.: 55.47%] [G loss: 0.442679]\n",
      "epoch:25 step:23834 [D loss: 0.253763, acc.: 51.56%] [G loss: 0.396871]\n",
      "epoch:25 step:23835 [D loss: 0.221709, acc.: 65.62%] [G loss: 0.419483]\n",
      "epoch:25 step:23836 [D loss: 0.230949, acc.: 58.59%] [G loss: 0.403614]\n",
      "epoch:25 step:23837 [D loss: 0.233439, acc.: 60.94%] [G loss: 0.400654]\n",
      "epoch:25 step:23838 [D loss: 0.224768, acc.: 67.19%] [G loss: 0.418410]\n",
      "epoch:25 step:23839 [D loss: 0.199341, acc.: 73.44%] [G loss: 0.415734]\n",
      "epoch:25 step:23840 [D loss: 0.229321, acc.: 63.28%] [G loss: 0.461462]\n",
      "epoch:25 step:23841 [D loss: 0.202450, acc.: 70.31%] [G loss: 0.488926]\n",
      "epoch:25 step:23842 [D loss: 0.223169, acc.: 63.28%] [G loss: 0.460938]\n",
      "epoch:25 step:23843 [D loss: 0.236264, acc.: 55.47%] [G loss: 0.439231]\n",
      "epoch:25 step:23844 [D loss: 0.238285, acc.: 59.38%] [G loss: 0.381119]\n",
      "epoch:25 step:23845 [D loss: 0.240838, acc.: 65.62%] [G loss: 0.430904]\n",
      "epoch:25 step:23846 [D loss: 0.245870, acc.: 61.72%] [G loss: 0.416491]\n",
      "epoch:25 step:23847 [D loss: 0.228018, acc.: 58.59%] [G loss: 0.406246]\n",
      "epoch:25 step:23848 [D loss: 0.242640, acc.: 59.38%] [G loss: 0.399198]\n",
      "epoch:25 step:23849 [D loss: 0.231827, acc.: 57.03%] [G loss: 0.452946]\n",
      "epoch:25 step:23850 [D loss: 0.210773, acc.: 59.38%] [G loss: 0.443440]\n",
      "epoch:25 step:23851 [D loss: 0.211107, acc.: 61.72%] [G loss: 0.419345]\n",
      "epoch:25 step:23852 [D loss: 0.253065, acc.: 62.50%] [G loss: 0.426833]\n",
      "epoch:25 step:23853 [D loss: 0.204860, acc.: 65.62%] [G loss: 0.452524]\n",
      "epoch:25 step:23854 [D loss: 0.186715, acc.: 74.22%] [G loss: 0.435029]\n",
      "epoch:25 step:23855 [D loss: 0.211996, acc.: 67.19%] [G loss: 0.475252]\n",
      "epoch:25 step:23856 [D loss: 0.230915, acc.: 57.03%] [G loss: 0.442110]\n",
      "epoch:25 step:23857 [D loss: 0.239613, acc.: 56.25%] [G loss: 0.418413]\n",
      "epoch:25 step:23858 [D loss: 0.232099, acc.: 60.94%] [G loss: 0.414796]\n",
      "epoch:25 step:23859 [D loss: 0.200641, acc.: 70.31%] [G loss: 0.437967]\n",
      "epoch:25 step:23860 [D loss: 0.211352, acc.: 65.62%] [G loss: 0.452194]\n",
      "epoch:25 step:23861 [D loss: 0.191549, acc.: 71.88%] [G loss: 0.452067]\n",
      "epoch:25 step:23862 [D loss: 0.270044, acc.: 46.09%] [G loss: 0.425882]\n",
      "epoch:25 step:23863 [D loss: 0.231656, acc.: 57.03%] [G loss: 0.412555]\n",
      "epoch:25 step:23864 [D loss: 0.232270, acc.: 62.50%] [G loss: 0.429663]\n",
      "epoch:25 step:23865 [D loss: 0.232820, acc.: 62.50%] [G loss: 0.426427]\n",
      "epoch:25 step:23866 [D loss: 0.217959, acc.: 64.06%] [G loss: 0.463290]\n",
      "epoch:25 step:23867 [D loss: 0.235699, acc.: 60.94%] [G loss: 0.423399]\n",
      "epoch:25 step:23868 [D loss: 0.236516, acc.: 62.50%] [G loss: 0.441567]\n",
      "epoch:25 step:23869 [D loss: 0.217042, acc.: 65.62%] [G loss: 0.439726]\n",
      "epoch:25 step:23870 [D loss: 0.222472, acc.: 61.72%] [G loss: 0.444753]\n",
      "epoch:25 step:23871 [D loss: 0.241123, acc.: 53.91%] [G loss: 0.433486]\n",
      "epoch:25 step:23872 [D loss: 0.204368, acc.: 67.19%] [G loss: 0.435630]\n",
      "epoch:25 step:23873 [D loss: 0.248975, acc.: 56.25%] [G loss: 0.395608]\n",
      "epoch:25 step:23874 [D loss: 0.236511, acc.: 60.94%] [G loss: 0.410511]\n",
      "epoch:25 step:23875 [D loss: 0.226989, acc.: 61.72%] [G loss: 0.425992]\n",
      "epoch:25 step:23876 [D loss: 0.195370, acc.: 74.22%] [G loss: 0.464073]\n",
      "epoch:25 step:23877 [D loss: 0.210409, acc.: 62.50%] [G loss: 0.464215]\n",
      "epoch:25 step:23878 [D loss: 0.201288, acc.: 64.84%] [G loss: 0.453239]\n",
      "epoch:25 step:23879 [D loss: 0.203831, acc.: 68.75%] [G loss: 0.451162]\n",
      "epoch:25 step:23880 [D loss: 0.221166, acc.: 62.50%] [G loss: 0.470499]\n",
      "epoch:25 step:23881 [D loss: 0.213076, acc.: 65.62%] [G loss: 0.417526]\n",
      "epoch:25 step:23882 [D loss: 0.214363, acc.: 65.62%] [G loss: 0.501815]\n",
      "epoch:25 step:23883 [D loss: 0.287956, acc.: 51.56%] [G loss: 0.394646]\n",
      "epoch:25 step:23884 [D loss: 0.225238, acc.: 64.84%] [G loss: 0.431583]\n",
      "epoch:25 step:23885 [D loss: 0.249781, acc.: 61.72%] [G loss: 0.421150]\n",
      "epoch:25 step:23886 [D loss: 0.213989, acc.: 60.94%] [G loss: 0.444633]\n",
      "epoch:25 step:23887 [D loss: 0.222847, acc.: 62.50%] [G loss: 0.415823]\n",
      "epoch:25 step:23888 [D loss: 0.242831, acc.: 57.03%] [G loss: 0.391398]\n",
      "epoch:25 step:23889 [D loss: 0.218395, acc.: 64.06%] [G loss: 0.442729]\n",
      "epoch:25 step:23890 [D loss: 0.215779, acc.: 66.41%] [G loss: 0.405138]\n",
      "epoch:25 step:23891 [D loss: 0.227361, acc.: 58.59%] [G loss: 0.406081]\n",
      "epoch:25 step:23892 [D loss: 0.238416, acc.: 60.16%] [G loss: 0.411418]\n",
      "epoch:25 step:23893 [D loss: 0.236229, acc.: 60.16%] [G loss: 0.402982]\n",
      "epoch:25 step:23894 [D loss: 0.194314, acc.: 68.75%] [G loss: 0.444300]\n",
      "epoch:25 step:23895 [D loss: 0.205508, acc.: 69.53%] [G loss: 0.409765]\n",
      "epoch:25 step:23896 [D loss: 0.215530, acc.: 63.28%] [G loss: 0.483461]\n",
      "epoch:25 step:23897 [D loss: 0.194792, acc.: 68.75%] [G loss: 0.475642]\n",
      "epoch:25 step:23898 [D loss: 0.233834, acc.: 57.81%] [G loss: 0.468929]\n",
      "epoch:25 step:23899 [D loss: 0.189867, acc.: 71.09%] [G loss: 0.486539]\n",
      "epoch:25 step:23900 [D loss: 0.184143, acc.: 72.66%] [G loss: 0.460390]\n",
      "epoch:25 step:23901 [D loss: 0.232544, acc.: 64.06%] [G loss: 0.454219]\n",
      "epoch:25 step:23902 [D loss: 0.258827, acc.: 54.69%] [G loss: 0.439676]\n",
      "epoch:25 step:23903 [D loss: 0.256566, acc.: 55.47%] [G loss: 0.394966]\n",
      "epoch:25 step:23904 [D loss: 0.221317, acc.: 64.84%] [G loss: 0.397096]\n",
      "epoch:25 step:23905 [D loss: 0.212527, acc.: 64.84%] [G loss: 0.409527]\n",
      "epoch:25 step:23906 [D loss: 0.180831, acc.: 70.31%] [G loss: 0.474294]\n",
      "epoch:25 step:23907 [D loss: 0.262131, acc.: 52.34%] [G loss: 0.457093]\n",
      "epoch:25 step:23908 [D loss: 0.225696, acc.: 61.72%] [G loss: 0.419505]\n",
      "epoch:25 step:23909 [D loss: 0.193837, acc.: 78.12%] [G loss: 0.470970]\n",
      "epoch:25 step:23910 [D loss: 0.205690, acc.: 71.88%] [G loss: 0.442029]\n",
      "epoch:25 step:23911 [D loss: 0.267393, acc.: 48.44%] [G loss: 0.421241]\n",
      "epoch:25 step:23912 [D loss: 0.224878, acc.: 61.72%] [G loss: 0.442072]\n",
      "epoch:25 step:23913 [D loss: 0.179043, acc.: 78.91%] [G loss: 0.452237]\n",
      "epoch:25 step:23914 [D loss: 0.206092, acc.: 68.75%] [G loss: 0.430443]\n",
      "epoch:25 step:23915 [D loss: 0.244624, acc.: 63.28%] [G loss: 0.420911]\n",
      "epoch:25 step:23916 [D loss: 0.217561, acc.: 62.50%] [G loss: 0.424150]\n",
      "epoch:25 step:23917 [D loss: 0.235489, acc.: 65.62%] [G loss: 0.420777]\n",
      "epoch:25 step:23918 [D loss: 0.194901, acc.: 73.44%] [G loss: 0.431479]\n",
      "epoch:25 step:23919 [D loss: 0.218082, acc.: 67.19%] [G loss: 0.423403]\n",
      "epoch:25 step:23920 [D loss: 0.202986, acc.: 71.88%] [G loss: 0.461659]\n",
      "epoch:25 step:23921 [D loss: 0.218649, acc.: 65.62%] [G loss: 0.459118]\n",
      "epoch:25 step:23922 [D loss: 0.212521, acc.: 65.62%] [G loss: 0.437583]\n",
      "epoch:25 step:23923 [D loss: 0.213373, acc.: 67.97%] [G loss: 0.431655]\n",
      "epoch:25 step:23924 [D loss: 0.198149, acc.: 72.66%] [G loss: 0.470364]\n",
      "epoch:25 step:23925 [D loss: 0.257737, acc.: 57.03%] [G loss: 0.448039]\n",
      "epoch:25 step:23926 [D loss: 0.284391, acc.: 53.91%] [G loss: 0.428442]\n",
      "epoch:25 step:23927 [D loss: 0.253169, acc.: 55.47%] [G loss: 0.380405]\n",
      "epoch:25 step:23928 [D loss: 0.215958, acc.: 64.84%] [G loss: 0.436765]\n",
      "epoch:25 step:23929 [D loss: 0.209729, acc.: 67.19%] [G loss: 0.448803]\n",
      "epoch:25 step:23930 [D loss: 0.225695, acc.: 60.94%] [G loss: 0.458901]\n",
      "epoch:25 step:23931 [D loss: 0.238338, acc.: 62.50%] [G loss: 0.442506]\n",
      "epoch:25 step:23932 [D loss: 0.206006, acc.: 70.31%] [G loss: 0.478659]\n",
      "epoch:25 step:23933 [D loss: 0.198562, acc.: 67.97%] [G loss: 0.464086]\n",
      "epoch:25 step:23934 [D loss: 0.237727, acc.: 61.72%] [G loss: 0.419181]\n",
      "epoch:25 step:23935 [D loss: 0.245025, acc.: 61.72%] [G loss: 0.396222]\n",
      "epoch:25 step:23936 [D loss: 0.248322, acc.: 56.25%] [G loss: 0.388852]\n",
      "epoch:25 step:23937 [D loss: 0.211529, acc.: 69.53%] [G loss: 0.468147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23938 [D loss: 0.220774, acc.: 61.72%] [G loss: 0.427755]\n",
      "epoch:25 step:23939 [D loss: 0.210770, acc.: 63.28%] [G loss: 0.451201]\n",
      "epoch:25 step:23940 [D loss: 0.200084, acc.: 68.75%] [G loss: 0.514002]\n",
      "epoch:25 step:23941 [D loss: 0.222258, acc.: 64.06%] [G loss: 0.461891]\n",
      "epoch:25 step:23942 [D loss: 0.271880, acc.: 56.25%] [G loss: 0.438418]\n",
      "epoch:25 step:23943 [D loss: 0.249562, acc.: 56.25%] [G loss: 0.414612]\n",
      "epoch:25 step:23944 [D loss: 0.187354, acc.: 66.41%] [G loss: 0.449566]\n",
      "epoch:25 step:23945 [D loss: 0.224914, acc.: 61.72%] [G loss: 0.417019]\n",
      "epoch:25 step:23946 [D loss: 0.202902, acc.: 70.31%] [G loss: 0.465261]\n",
      "epoch:25 step:23947 [D loss: 0.220829, acc.: 66.41%] [G loss: 0.450985]\n",
      "epoch:25 step:23948 [D loss: 0.197596, acc.: 72.66%] [G loss: 0.467994]\n",
      "epoch:25 step:23949 [D loss: 0.255791, acc.: 59.38%] [G loss: 0.437853]\n",
      "epoch:25 step:23950 [D loss: 0.200786, acc.: 69.53%] [G loss: 0.478946]\n",
      "epoch:25 step:23951 [D loss: 0.212640, acc.: 64.84%] [G loss: 0.450385]\n",
      "epoch:25 step:23952 [D loss: 0.226503, acc.: 61.72%] [G loss: 0.401184]\n",
      "epoch:25 step:23953 [D loss: 0.263427, acc.: 55.47%] [G loss: 0.413078]\n",
      "epoch:25 step:23954 [D loss: 0.253702, acc.: 58.59%] [G loss: 0.383317]\n",
      "epoch:25 step:23955 [D loss: 0.240549, acc.: 60.94%] [G loss: 0.471159]\n",
      "epoch:25 step:23956 [D loss: 0.233145, acc.: 67.19%] [G loss: 0.406895]\n",
      "epoch:25 step:23957 [D loss: 0.229034, acc.: 66.41%] [G loss: 0.420513]\n",
      "epoch:25 step:23958 [D loss: 0.224940, acc.: 64.06%] [G loss: 0.416336]\n",
      "epoch:25 step:23959 [D loss: 0.195975, acc.: 68.75%] [G loss: 0.454464]\n",
      "epoch:25 step:23960 [D loss: 0.258316, acc.: 50.78%] [G loss: 0.417027]\n",
      "epoch:25 step:23961 [D loss: 0.242330, acc.: 56.25%] [G loss: 0.403987]\n",
      "epoch:25 step:23962 [D loss: 0.239651, acc.: 58.59%] [G loss: 0.452298]\n",
      "epoch:25 step:23963 [D loss: 0.252172, acc.: 56.25%] [G loss: 0.417964]\n",
      "epoch:25 step:23964 [D loss: 0.217368, acc.: 60.94%] [G loss: 0.424705]\n",
      "epoch:25 step:23965 [D loss: 0.233992, acc.: 62.50%] [G loss: 0.404070]\n",
      "epoch:25 step:23966 [D loss: 0.229831, acc.: 65.62%] [G loss: 0.379298]\n",
      "epoch:25 step:23967 [D loss: 0.256917, acc.: 53.91%] [G loss: 0.395718]\n",
      "epoch:25 step:23968 [D loss: 0.231516, acc.: 62.50%] [G loss: 0.414011]\n",
      "epoch:25 step:23969 [D loss: 0.240164, acc.: 61.72%] [G loss: 0.391689]\n",
      "epoch:25 step:23970 [D loss: 0.215720, acc.: 67.97%] [G loss: 0.401826]\n",
      "epoch:25 step:23971 [D loss: 0.234313, acc.: 62.50%] [G loss: 0.397291]\n",
      "epoch:25 step:23972 [D loss: 0.239103, acc.: 62.50%] [G loss: 0.458402]\n",
      "epoch:25 step:23973 [D loss: 0.206292, acc.: 64.84%] [G loss: 0.436082]\n",
      "epoch:25 step:23974 [D loss: 0.234371, acc.: 59.38%] [G loss: 0.440055]\n",
      "epoch:25 step:23975 [D loss: 0.207764, acc.: 67.97%] [G loss: 0.459354]\n",
      "epoch:25 step:23976 [D loss: 0.208755, acc.: 69.53%] [G loss: 0.439999]\n",
      "epoch:25 step:23977 [D loss: 0.206327, acc.: 67.97%] [G loss: 0.456482]\n",
      "epoch:25 step:23978 [D loss: 0.246201, acc.: 57.03%] [G loss: 0.433625]\n",
      "epoch:25 step:23979 [D loss: 0.202175, acc.: 69.53%] [G loss: 0.460959]\n",
      "epoch:25 step:23980 [D loss: 0.212583, acc.: 65.62%] [G loss: 0.431511]\n",
      "epoch:25 step:23981 [D loss: 0.212309, acc.: 66.41%] [G loss: 0.429306]\n",
      "epoch:25 step:23982 [D loss: 0.205923, acc.: 64.84%] [G loss: 0.479545]\n",
      "epoch:25 step:23983 [D loss: 0.184573, acc.: 72.66%] [G loss: 0.430046]\n",
      "epoch:25 step:23984 [D loss: 0.256955, acc.: 51.56%] [G loss: 0.386746]\n",
      "epoch:25 step:23985 [D loss: 0.238037, acc.: 57.81%] [G loss: 0.400494]\n",
      "epoch:25 step:23986 [D loss: 0.202305, acc.: 68.75%] [G loss: 0.480645]\n",
      "epoch:25 step:23987 [D loss: 0.221524, acc.: 63.28%] [G loss: 0.406724]\n",
      "epoch:25 step:23988 [D loss: 0.214654, acc.: 66.41%] [G loss: 0.424567]\n",
      "epoch:25 step:23989 [D loss: 0.172644, acc.: 77.34%] [G loss: 0.458657]\n",
      "epoch:25 step:23990 [D loss: 0.237494, acc.: 59.38%] [G loss: 0.486024]\n",
      "epoch:25 step:23991 [D loss: 0.250193, acc.: 59.38%] [G loss: 0.477608]\n",
      "epoch:25 step:23992 [D loss: 0.210914, acc.: 70.31%] [G loss: 0.470485]\n",
      "epoch:25 step:23993 [D loss: 0.210129, acc.: 68.75%] [G loss: 0.459827]\n",
      "epoch:25 step:23994 [D loss: 0.256921, acc.: 56.25%] [G loss: 0.402252]\n",
      "epoch:25 step:23995 [D loss: 0.211549, acc.: 70.31%] [G loss: 0.430290]\n",
      "epoch:25 step:23996 [D loss: 0.238814, acc.: 53.91%] [G loss: 0.360128]\n",
      "epoch:25 step:23997 [D loss: 0.205863, acc.: 71.09%] [G loss: 0.402947]\n",
      "epoch:25 step:23998 [D loss: 0.224802, acc.: 64.06%] [G loss: 0.418827]\n",
      "epoch:25 step:23999 [D loss: 0.188239, acc.: 74.22%] [G loss: 0.447340]\n",
      "epoch:25 step:24000 [D loss: 0.217980, acc.: 66.41%] [G loss: 0.432857]\n",
      "##############\n",
      "[2.70541243 1.82412504 5.8319964  4.65160255 3.57267944 5.56149858\n",
      " 4.5341017  4.69312799 4.64985741 3.96418848]\n",
      "##########\n",
      "epoch:25 step:24001 [D loss: 0.228322, acc.: 67.97%] [G loss: 0.424987]\n",
      "epoch:25 step:24002 [D loss: 0.223626, acc.: 61.72%] [G loss: 0.434876]\n",
      "epoch:25 step:24003 [D loss: 0.217397, acc.: 64.84%] [G loss: 0.382303]\n",
      "epoch:25 step:24004 [D loss: 0.250851, acc.: 54.69%] [G loss: 0.425565]\n",
      "epoch:25 step:24005 [D loss: 0.212054, acc.: 62.50%] [G loss: 0.420856]\n",
      "epoch:25 step:24006 [D loss: 0.221372, acc.: 62.50%] [G loss: 0.383742]\n",
      "epoch:25 step:24007 [D loss: 0.203761, acc.: 71.09%] [G loss: 0.450899]\n",
      "epoch:25 step:24008 [D loss: 0.231140, acc.: 64.06%] [G loss: 0.426457]\n",
      "epoch:25 step:24009 [D loss: 0.215923, acc.: 62.50%] [G loss: 0.405658]\n",
      "epoch:25 step:24010 [D loss: 0.228861, acc.: 61.72%] [G loss: 0.429109]\n",
      "epoch:25 step:24011 [D loss: 0.230925, acc.: 61.72%] [G loss: 0.432499]\n",
      "epoch:25 step:24012 [D loss: 0.226053, acc.: 60.16%] [G loss: 0.426350]\n",
      "epoch:25 step:24013 [D loss: 0.224171, acc.: 61.72%] [G loss: 0.423878]\n",
      "epoch:25 step:24014 [D loss: 0.217589, acc.: 63.28%] [G loss: 0.427242]\n",
      "epoch:25 step:24015 [D loss: 0.233410, acc.: 57.81%] [G loss: 0.426195]\n",
      "epoch:25 step:24016 [D loss: 0.245043, acc.: 56.25%] [G loss: 0.428425]\n",
      "epoch:25 step:24017 [D loss: 0.211146, acc.: 69.53%] [G loss: 0.468524]\n",
      "epoch:25 step:24018 [D loss: 0.217840, acc.: 61.72%] [G loss: 0.442875]\n",
      "epoch:25 step:24019 [D loss: 0.257091, acc.: 57.03%] [G loss: 0.422225]\n",
      "epoch:25 step:24020 [D loss: 0.208441, acc.: 60.16%] [G loss: 0.441929]\n",
      "epoch:25 step:24021 [D loss: 0.234891, acc.: 55.47%] [G loss: 0.397642]\n",
      "epoch:25 step:24022 [D loss: 0.222034, acc.: 62.50%] [G loss: 0.428117]\n",
      "epoch:25 step:24023 [D loss: 0.228046, acc.: 61.72%] [G loss: 0.421997]\n",
      "epoch:25 step:24024 [D loss: 0.230223, acc.: 57.81%] [G loss: 0.428235]\n",
      "epoch:25 step:24025 [D loss: 0.242422, acc.: 58.59%] [G loss: 0.412877]\n",
      "epoch:25 step:24026 [D loss: 0.221022, acc.: 64.06%] [G loss: 0.426911]\n",
      "epoch:25 step:24027 [D loss: 0.228399, acc.: 61.72%] [G loss: 0.426646]\n",
      "epoch:25 step:24028 [D loss: 0.241269, acc.: 58.59%] [G loss: 0.402470]\n",
      "epoch:25 step:24029 [D loss: 0.238567, acc.: 62.50%] [G loss: 0.433012]\n",
      "epoch:25 step:24030 [D loss: 0.220875, acc.: 63.28%] [G loss: 0.450917]\n",
      "epoch:25 step:24031 [D loss: 0.235730, acc.: 58.59%] [G loss: 0.410549]\n",
      "epoch:25 step:24032 [D loss: 0.205112, acc.: 66.41%] [G loss: 0.430673]\n",
      "epoch:25 step:24033 [D loss: 0.232468, acc.: 65.62%] [G loss: 0.393376]\n",
      "epoch:25 step:24034 [D loss: 0.207966, acc.: 66.41%] [G loss: 0.390237]\n",
      "epoch:25 step:24035 [D loss: 0.230613, acc.: 60.94%] [G loss: 0.395109]\n",
      "epoch:25 step:24036 [D loss: 0.227896, acc.: 63.28%] [G loss: 0.409136]\n",
      "epoch:25 step:24037 [D loss: 0.227740, acc.: 61.72%] [G loss: 0.421755]\n",
      "epoch:25 step:24038 [D loss: 0.212116, acc.: 69.53%] [G loss: 0.415956]\n",
      "epoch:25 step:24039 [D loss: 0.238993, acc.: 55.47%] [G loss: 0.449406]\n",
      "epoch:25 step:24040 [D loss: 0.253763, acc.: 55.47%] [G loss: 0.398822]\n",
      "epoch:25 step:24041 [D loss: 0.252329, acc.: 56.25%] [G loss: 0.382223]\n",
      "epoch:25 step:24042 [D loss: 0.215564, acc.: 67.19%] [G loss: 0.447379]\n",
      "epoch:25 step:24043 [D loss: 0.230003, acc.: 65.62%] [G loss: 0.380135]\n",
      "epoch:25 step:24044 [D loss: 0.223601, acc.: 64.84%] [G loss: 0.428760]\n",
      "epoch:25 step:24045 [D loss: 0.189389, acc.: 72.66%] [G loss: 0.485340]\n",
      "epoch:25 step:24046 [D loss: 0.235917, acc.: 57.81%] [G loss: 0.411056]\n",
      "epoch:25 step:24047 [D loss: 0.237310, acc.: 60.16%] [G loss: 0.398321]\n",
      "epoch:25 step:24048 [D loss: 0.219574, acc.: 63.28%] [G loss: 0.428145]\n",
      "epoch:25 step:24049 [D loss: 0.174371, acc.: 75.00%] [G loss: 0.428874]\n",
      "epoch:25 step:24050 [D loss: 0.235640, acc.: 58.59%] [G loss: 0.412075]\n",
      "epoch:25 step:24051 [D loss: 0.235731, acc.: 60.94%] [G loss: 0.467411]\n",
      "epoch:25 step:24052 [D loss: 0.212416, acc.: 68.75%] [G loss: 0.441581]\n",
      "epoch:25 step:24053 [D loss: 0.252817, acc.: 61.72%] [G loss: 0.395527]\n",
      "epoch:25 step:24054 [D loss: 0.200438, acc.: 70.31%] [G loss: 0.427924]\n",
      "epoch:25 step:24055 [D loss: 0.239071, acc.: 61.72%] [G loss: 0.431181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24056 [D loss: 0.219274, acc.: 67.97%] [G loss: 0.456901]\n",
      "epoch:25 step:24057 [D loss: 0.203500, acc.: 67.19%] [G loss: 0.485136]\n",
      "epoch:25 step:24058 [D loss: 0.222814, acc.: 64.06%] [G loss: 0.475985]\n",
      "epoch:25 step:24059 [D loss: 0.166699, acc.: 81.25%] [G loss: 0.473047]\n",
      "epoch:25 step:24060 [D loss: 0.202627, acc.: 66.41%] [G loss: 0.430893]\n",
      "epoch:25 step:24061 [D loss: 0.239141, acc.: 57.81%] [G loss: 0.397376]\n",
      "epoch:25 step:24062 [D loss: 0.232112, acc.: 63.28%] [G loss: 0.427227]\n",
      "epoch:25 step:24063 [D loss: 0.226878, acc.: 62.50%] [G loss: 0.410236]\n",
      "epoch:25 step:24064 [D loss: 0.217179, acc.: 67.19%] [G loss: 0.404778]\n",
      "epoch:25 step:24065 [D loss: 0.227777, acc.: 58.59%] [G loss: 0.435518]\n",
      "epoch:25 step:24066 [D loss: 0.202127, acc.: 67.19%] [G loss: 0.451429]\n",
      "epoch:25 step:24067 [D loss: 0.189126, acc.: 76.56%] [G loss: 0.500588]\n",
      "epoch:25 step:24068 [D loss: 0.222001, acc.: 63.28%] [G loss: 0.457761]\n",
      "epoch:25 step:24069 [D loss: 0.245110, acc.: 60.16%] [G loss: 0.428383]\n",
      "epoch:25 step:24070 [D loss: 0.254280, acc.: 48.44%] [G loss: 0.395310]\n",
      "epoch:25 step:24071 [D loss: 0.200713, acc.: 65.62%] [G loss: 0.451591]\n",
      "epoch:25 step:24072 [D loss: 0.207389, acc.: 75.00%] [G loss: 0.476258]\n",
      "epoch:25 step:24073 [D loss: 0.188598, acc.: 73.44%] [G loss: 0.495021]\n",
      "epoch:25 step:24074 [D loss: 0.214855, acc.: 66.41%] [G loss: 0.488238]\n",
      "epoch:25 step:24075 [D loss: 0.210557, acc.: 69.53%] [G loss: 0.498504]\n",
      "epoch:25 step:24076 [D loss: 0.209483, acc.: 70.31%] [G loss: 0.427499]\n",
      "epoch:25 step:24077 [D loss: 0.233251, acc.: 57.81%] [G loss: 0.457706]\n",
      "epoch:25 step:24078 [D loss: 0.220805, acc.: 61.72%] [G loss: 0.441347]\n",
      "epoch:25 step:24079 [D loss: 0.215494, acc.: 63.28%] [G loss: 0.475973]\n",
      "epoch:25 step:24080 [D loss: 0.236167, acc.: 62.50%] [G loss: 0.401831]\n",
      "epoch:25 step:24081 [D loss: 0.218878, acc.: 65.62%] [G loss: 0.422412]\n",
      "epoch:25 step:24082 [D loss: 0.231921, acc.: 64.06%] [G loss: 0.429161]\n",
      "epoch:25 step:24083 [D loss: 0.209541, acc.: 71.09%] [G loss: 0.443143]\n",
      "epoch:25 step:24084 [D loss: 0.219925, acc.: 67.97%] [G loss: 0.415168]\n",
      "epoch:25 step:24085 [D loss: 0.207007, acc.: 66.41%] [G loss: 0.422162]\n",
      "epoch:25 step:24086 [D loss: 0.227482, acc.: 64.84%] [G loss: 0.420303]\n",
      "epoch:25 step:24087 [D loss: 0.206268, acc.: 67.19%] [G loss: 0.468562]\n",
      "epoch:25 step:24088 [D loss: 0.242602, acc.: 59.38%] [G loss: 0.433567]\n",
      "epoch:25 step:24089 [D loss: 0.241180, acc.: 57.03%] [G loss: 0.431253]\n",
      "epoch:25 step:24090 [D loss: 0.229922, acc.: 58.59%] [G loss: 0.449173]\n",
      "epoch:25 step:24091 [D loss: 0.213260, acc.: 65.62%] [G loss: 0.453663]\n",
      "epoch:25 step:24092 [D loss: 0.256538, acc.: 53.12%] [G loss: 0.401279]\n",
      "epoch:25 step:24093 [D loss: 0.224522, acc.: 60.16%] [G loss: 0.424658]\n",
      "epoch:25 step:24094 [D loss: 0.224803, acc.: 62.50%] [G loss: 0.419526]\n",
      "epoch:25 step:24095 [D loss: 0.222349, acc.: 62.50%] [G loss: 0.407954]\n",
      "epoch:25 step:24096 [D loss: 0.258542, acc.: 54.69%] [G loss: 0.422324]\n",
      "epoch:25 step:24097 [D loss: 0.241037, acc.: 55.47%] [G loss: 0.427148]\n",
      "epoch:25 step:24098 [D loss: 0.234207, acc.: 59.38%] [G loss: 0.407984]\n",
      "epoch:25 step:24099 [D loss: 0.218752, acc.: 64.84%] [G loss: 0.434010]\n",
      "epoch:25 step:24100 [D loss: 0.242999, acc.: 62.50%] [G loss: 0.424614]\n",
      "epoch:25 step:24101 [D loss: 0.223318, acc.: 57.81%] [G loss: 0.432616]\n",
      "epoch:25 step:24102 [D loss: 0.206693, acc.: 68.75%] [G loss: 0.447172]\n",
      "epoch:25 step:24103 [D loss: 0.216271, acc.: 64.06%] [G loss: 0.405497]\n",
      "epoch:25 step:24104 [D loss: 0.230937, acc.: 58.59%] [G loss: 0.395878]\n",
      "epoch:25 step:24105 [D loss: 0.239791, acc.: 53.12%] [G loss: 0.434143]\n",
      "epoch:25 step:24106 [D loss: 0.197965, acc.: 73.44%] [G loss: 0.477014]\n",
      "epoch:25 step:24107 [D loss: 0.230775, acc.: 59.38%] [G loss: 0.429441]\n",
      "epoch:25 step:24108 [D loss: 0.236312, acc.: 60.16%] [G loss: 0.395692]\n",
      "epoch:25 step:24109 [D loss: 0.225983, acc.: 60.94%] [G loss: 0.421634]\n",
      "epoch:25 step:24110 [D loss: 0.214722, acc.: 63.28%] [G loss: 0.402154]\n",
      "epoch:25 step:24111 [D loss: 0.218197, acc.: 66.41%] [G loss: 0.422449]\n",
      "epoch:25 step:24112 [D loss: 0.241041, acc.: 58.59%] [G loss: 0.451907]\n",
      "epoch:25 step:24113 [D loss: 0.187544, acc.: 65.62%] [G loss: 0.451148]\n",
      "epoch:25 step:24114 [D loss: 0.201806, acc.: 71.88%] [G loss: 0.439369]\n",
      "epoch:25 step:24115 [D loss: 0.209313, acc.: 68.75%] [G loss: 0.449441]\n",
      "epoch:25 step:24116 [D loss: 0.187434, acc.: 73.44%] [G loss: 0.506097]\n",
      "epoch:25 step:24117 [D loss: 0.206332, acc.: 67.97%] [G loss: 0.492925]\n",
      "epoch:25 step:24118 [D loss: 0.199291, acc.: 67.97%] [G loss: 0.456600]\n",
      "epoch:25 step:24119 [D loss: 0.191491, acc.: 68.75%] [G loss: 0.480836]\n",
      "epoch:25 step:24120 [D loss: 0.217908, acc.: 67.19%] [G loss: 0.438881]\n",
      "epoch:25 step:24121 [D loss: 0.248435, acc.: 54.69%] [G loss: 0.413902]\n",
      "epoch:25 step:24122 [D loss: 0.218742, acc.: 67.19%] [G loss: 0.437995]\n",
      "epoch:25 step:24123 [D loss: 0.241952, acc.: 64.06%] [G loss: 0.423970]\n",
      "epoch:25 step:24124 [D loss: 0.202219, acc.: 68.75%] [G loss: 0.425374]\n",
      "epoch:25 step:24125 [D loss: 0.207742, acc.: 63.28%] [G loss: 0.455584]\n",
      "epoch:25 step:24126 [D loss: 0.214009, acc.: 62.50%] [G loss: 0.443416]\n",
      "epoch:25 step:24127 [D loss: 0.240055, acc.: 54.69%] [G loss: 0.469582]\n",
      "epoch:25 step:24128 [D loss: 0.227383, acc.: 61.72%] [G loss: 0.436009]\n",
      "epoch:25 step:24129 [D loss: 0.269902, acc.: 53.91%] [G loss: 0.419899]\n",
      "epoch:25 step:24130 [D loss: 0.213327, acc.: 65.62%] [G loss: 0.397403]\n",
      "epoch:25 step:24131 [D loss: 0.201851, acc.: 67.19%] [G loss: 0.440916]\n",
      "epoch:25 step:24132 [D loss: 0.218425, acc.: 64.84%] [G loss: 0.448770]\n",
      "epoch:25 step:24133 [D loss: 0.211538, acc.: 68.75%] [G loss: 0.453095]\n",
      "epoch:25 step:24134 [D loss: 0.169077, acc.: 78.91%] [G loss: 0.470351]\n",
      "epoch:25 step:24135 [D loss: 0.262782, acc.: 54.69%] [G loss: 0.450622]\n",
      "epoch:25 step:24136 [D loss: 0.238995, acc.: 61.72%] [G loss: 0.368190]\n",
      "epoch:25 step:24137 [D loss: 0.230091, acc.: 63.28%] [G loss: 0.434467]\n",
      "epoch:25 step:24138 [D loss: 0.230964, acc.: 56.25%] [G loss: 0.432070]\n",
      "epoch:25 step:24139 [D loss: 0.246712, acc.: 57.81%] [G loss: 0.460216]\n",
      "epoch:25 step:24140 [D loss: 0.239049, acc.: 59.38%] [G loss: 0.402252]\n",
      "epoch:25 step:24141 [D loss: 0.255234, acc.: 55.47%] [G loss: 0.383401]\n",
      "epoch:25 step:24142 [D loss: 0.219204, acc.: 64.06%] [G loss: 0.408034]\n",
      "epoch:25 step:24143 [D loss: 0.232332, acc.: 64.84%] [G loss: 0.381717]\n",
      "epoch:25 step:24144 [D loss: 0.204260, acc.: 72.66%] [G loss: 0.471155]\n",
      "epoch:25 step:24145 [D loss: 0.223232, acc.: 62.50%] [G loss: 0.436478]\n",
      "epoch:25 step:24146 [D loss: 0.222236, acc.: 64.06%] [G loss: 0.420242]\n",
      "epoch:25 step:24147 [D loss: 0.206718, acc.: 67.19%] [G loss: 0.418694]\n",
      "epoch:25 step:24148 [D loss: 0.224280, acc.: 62.50%] [G loss: 0.431239]\n",
      "epoch:25 step:24149 [D loss: 0.235617, acc.: 55.47%] [G loss: 0.427111]\n",
      "epoch:25 step:24150 [D loss: 0.198465, acc.: 71.09%] [G loss: 0.441782]\n",
      "epoch:25 step:24151 [D loss: 0.215293, acc.: 60.94%] [G loss: 0.406144]\n",
      "epoch:25 step:24152 [D loss: 0.242318, acc.: 56.25%] [G loss: 0.430168]\n",
      "epoch:25 step:24153 [D loss: 0.227767, acc.: 62.50%] [G loss: 0.399479]\n",
      "epoch:25 step:24154 [D loss: 0.236442, acc.: 57.81%] [G loss: 0.409606]\n",
      "epoch:25 step:24155 [D loss: 0.204819, acc.: 67.19%] [G loss: 0.446817]\n",
      "epoch:25 step:24156 [D loss: 0.245841, acc.: 57.03%] [G loss: 0.444297]\n",
      "epoch:25 step:24157 [D loss: 0.221218, acc.: 61.72%] [G loss: 0.423699]\n",
      "epoch:25 step:24158 [D loss: 0.198184, acc.: 71.88%] [G loss: 0.471266]\n",
      "epoch:25 step:24159 [D loss: 0.254509, acc.: 50.00%] [G loss: 0.401959]\n",
      "epoch:25 step:24160 [D loss: 0.229497, acc.: 61.72%] [G loss: 0.417202]\n",
      "epoch:25 step:24161 [D loss: 0.213114, acc.: 60.94%] [G loss: 0.412695]\n",
      "epoch:25 step:24162 [D loss: 0.230445, acc.: 59.38%] [G loss: 0.448927]\n",
      "epoch:25 step:24163 [D loss: 0.224390, acc.: 61.72%] [G loss: 0.404088]\n",
      "epoch:25 step:24164 [D loss: 0.239584, acc.: 60.16%] [G loss: 0.380758]\n",
      "epoch:25 step:24165 [D loss: 0.249713, acc.: 57.81%] [G loss: 0.369559]\n",
      "epoch:25 step:24166 [D loss: 0.246610, acc.: 54.69%] [G loss: 0.370895]\n",
      "epoch:25 step:24167 [D loss: 0.224930, acc.: 63.28%] [G loss: 0.379982]\n",
      "epoch:25 step:24168 [D loss: 0.229026, acc.: 63.28%] [G loss: 0.431675]\n",
      "epoch:25 step:24169 [D loss: 0.221407, acc.: 62.50%] [G loss: 0.416956]\n",
      "epoch:25 step:24170 [D loss: 0.228528, acc.: 60.94%] [G loss: 0.417727]\n",
      "epoch:25 step:24171 [D loss: 0.217033, acc.: 62.50%] [G loss: 0.434087]\n",
      "epoch:25 step:24172 [D loss: 0.228515, acc.: 59.38%] [G loss: 0.412610]\n",
      "epoch:25 step:24173 [D loss: 0.230614, acc.: 62.50%] [G loss: 0.396012]\n",
      "epoch:25 step:24174 [D loss: 0.225015, acc.: 60.16%] [G loss: 0.415050]\n",
      "epoch:25 step:24175 [D loss: 0.226201, acc.: 62.50%] [G loss: 0.411542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24176 [D loss: 0.233285, acc.: 57.81%] [G loss: 0.401668]\n",
      "epoch:25 step:24177 [D loss: 0.253368, acc.: 55.47%] [G loss: 0.429078]\n",
      "epoch:25 step:24178 [D loss: 0.260156, acc.: 61.72%] [G loss: 0.496365]\n",
      "epoch:25 step:24179 [D loss: 0.226431, acc.: 64.06%] [G loss: 0.453168]\n",
      "epoch:25 step:24180 [D loss: 0.215970, acc.: 68.75%] [G loss: 0.470511]\n",
      "epoch:25 step:24181 [D loss: 0.219013, acc.: 65.62%] [G loss: 0.440052]\n",
      "epoch:25 step:24182 [D loss: 0.239978, acc.: 59.38%] [G loss: 0.421039]\n",
      "epoch:25 step:24183 [D loss: 0.214577, acc.: 64.84%] [G loss: 0.429654]\n",
      "epoch:25 step:24184 [D loss: 0.234807, acc.: 57.81%] [G loss: 0.398850]\n",
      "epoch:25 step:24185 [D loss: 0.262013, acc.: 55.47%] [G loss: 0.387824]\n",
      "epoch:25 step:24186 [D loss: 0.242170, acc.: 57.81%] [G loss: 0.394559]\n",
      "epoch:25 step:24187 [D loss: 0.209247, acc.: 63.28%] [G loss: 0.413983]\n",
      "epoch:25 step:24188 [D loss: 0.228496, acc.: 64.84%] [G loss: 0.408896]\n",
      "epoch:25 step:24189 [D loss: 0.238513, acc.: 57.81%] [G loss: 0.425085]\n",
      "epoch:25 step:24190 [D loss: 0.247541, acc.: 53.12%] [G loss: 0.426606]\n",
      "epoch:25 step:24191 [D loss: 0.244027, acc.: 60.16%] [G loss: 0.405032]\n",
      "epoch:25 step:24192 [D loss: 0.222064, acc.: 60.16%] [G loss: 0.421554]\n",
      "epoch:25 step:24193 [D loss: 0.224339, acc.: 65.62%] [G loss: 0.442156]\n",
      "epoch:25 step:24194 [D loss: 0.193000, acc.: 70.31%] [G loss: 0.423617]\n",
      "epoch:25 step:24195 [D loss: 0.229437, acc.: 62.50%] [G loss: 0.422377]\n",
      "epoch:25 step:24196 [D loss: 0.232114, acc.: 58.59%] [G loss: 0.422659]\n",
      "epoch:25 step:24197 [D loss: 0.274662, acc.: 49.22%] [G loss: 0.431855]\n",
      "epoch:25 step:24198 [D loss: 0.221055, acc.: 60.94%] [G loss: 0.430014]\n",
      "epoch:25 step:24199 [D loss: 0.212578, acc.: 63.28%] [G loss: 0.427776]\n",
      "epoch:25 step:24200 [D loss: 0.211017, acc.: 66.41%] [G loss: 0.454486]\n",
      "##############\n",
      "[2.59713393 1.92274164 6.15933411 4.78538641 3.59702288 5.72459619\n",
      " 4.57942714 4.81457054 4.66110509 4.05166922]\n",
      "##########\n",
      "epoch:25 step:24201 [D loss: 0.206631, acc.: 67.97%] [G loss: 0.463533]\n",
      "epoch:25 step:24202 [D loss: 0.238596, acc.: 66.41%] [G loss: 0.414324]\n",
      "epoch:25 step:24203 [D loss: 0.225733, acc.: 58.59%] [G loss: 0.445246]\n",
      "epoch:25 step:24204 [D loss: 0.241091, acc.: 63.28%] [G loss: 0.441486]\n",
      "epoch:25 step:24205 [D loss: 0.214958, acc.: 64.06%] [G loss: 0.429971]\n",
      "epoch:25 step:24206 [D loss: 0.212910, acc.: 67.19%] [G loss: 0.417866]\n",
      "epoch:25 step:24207 [D loss: 0.209431, acc.: 67.19%] [G loss: 0.456929]\n",
      "epoch:25 step:24208 [D loss: 0.224035, acc.: 57.03%] [G loss: 0.432390]\n",
      "epoch:25 step:24209 [D loss: 0.233711, acc.: 57.03%] [G loss: 0.406082]\n",
      "epoch:25 step:24210 [D loss: 0.232796, acc.: 64.06%] [G loss: 0.402746]\n",
      "epoch:25 step:24211 [D loss: 0.206557, acc.: 68.75%] [G loss: 0.467519]\n",
      "epoch:25 step:24212 [D loss: 0.277148, acc.: 49.22%] [G loss: 0.394144]\n",
      "epoch:25 step:24213 [D loss: 0.240365, acc.: 60.16%] [G loss: 0.397654]\n",
      "epoch:25 step:24214 [D loss: 0.227396, acc.: 61.72%] [G loss: 0.440794]\n",
      "epoch:25 step:24215 [D loss: 0.199141, acc.: 71.88%] [G loss: 0.474428]\n",
      "epoch:25 step:24216 [D loss: 0.272538, acc.: 50.78%] [G loss: 0.375650]\n",
      "epoch:25 step:24217 [D loss: 0.193785, acc.: 72.66%] [G loss: 0.476920]\n",
      "epoch:25 step:24218 [D loss: 0.205478, acc.: 67.19%] [G loss: 0.475424]\n",
      "epoch:25 step:24219 [D loss: 0.242010, acc.: 56.25%] [G loss: 0.464617]\n",
      "epoch:25 step:24220 [D loss: 0.243653, acc.: 54.69%] [G loss: 0.415821]\n",
      "epoch:25 step:24221 [D loss: 0.226134, acc.: 57.81%] [G loss: 0.406141]\n",
      "epoch:25 step:24222 [D loss: 0.255830, acc.: 52.34%] [G loss: 0.440429]\n",
      "epoch:25 step:24223 [D loss: 0.219262, acc.: 66.41%] [G loss: 0.408204]\n",
      "epoch:25 step:24224 [D loss: 0.208811, acc.: 67.97%] [G loss: 0.427758]\n",
      "epoch:25 step:24225 [D loss: 0.239642, acc.: 56.25%] [G loss: 0.378490]\n",
      "epoch:25 step:24226 [D loss: 0.210707, acc.: 64.84%] [G loss: 0.428286]\n",
      "epoch:25 step:24227 [D loss: 0.212634, acc.: 67.19%] [G loss: 0.476554]\n",
      "epoch:25 step:24228 [D loss: 0.215291, acc.: 67.97%] [G loss: 0.510922]\n",
      "epoch:25 step:24229 [D loss: 0.246372, acc.: 56.25%] [G loss: 0.407631]\n",
      "epoch:25 step:24230 [D loss: 0.223438, acc.: 61.72%] [G loss: 0.386716]\n",
      "epoch:25 step:24231 [D loss: 0.207234, acc.: 67.97%] [G loss: 0.416508]\n",
      "epoch:25 step:24232 [D loss: 0.223713, acc.: 62.50%] [G loss: 0.408225]\n",
      "epoch:25 step:24233 [D loss: 0.234808, acc.: 57.81%] [G loss: 0.396249]\n",
      "epoch:25 step:24234 [D loss: 0.260656, acc.: 50.78%] [G loss: 0.373651]\n",
      "epoch:25 step:24235 [D loss: 0.212773, acc.: 69.53%] [G loss: 0.397071]\n",
      "epoch:25 step:24236 [D loss: 0.237473, acc.: 57.81%] [G loss: 0.434401]\n",
      "epoch:25 step:24237 [D loss: 0.223748, acc.: 66.41%] [G loss: 0.435220]\n",
      "epoch:25 step:24238 [D loss: 0.226694, acc.: 62.50%] [G loss: 0.438980]\n",
      "epoch:25 step:24239 [D loss: 0.196610, acc.: 67.97%] [G loss: 0.453577]\n",
      "epoch:25 step:24240 [D loss: 0.193576, acc.: 70.31%] [G loss: 0.492051]\n",
      "epoch:25 step:24241 [D loss: 0.257117, acc.: 55.47%] [G loss: 0.424542]\n",
      "epoch:25 step:24242 [D loss: 0.238359, acc.: 61.72%] [G loss: 0.451593]\n",
      "epoch:25 step:24243 [D loss: 0.238631, acc.: 65.62%] [G loss: 0.443028]\n",
      "epoch:25 step:24244 [D loss: 0.222391, acc.: 61.72%] [G loss: 0.447464]\n",
      "epoch:25 step:24245 [D loss: 0.276507, acc.: 53.91%] [G loss: 0.449110]\n",
      "epoch:25 step:24246 [D loss: 0.219781, acc.: 64.06%] [G loss: 0.434646]\n",
      "epoch:25 step:24247 [D loss: 0.207653, acc.: 68.75%] [G loss: 0.451903]\n",
      "epoch:25 step:24248 [D loss: 0.201204, acc.: 74.22%] [G loss: 0.445349]\n",
      "epoch:25 step:24249 [D loss: 0.229837, acc.: 60.94%] [G loss: 0.442160]\n",
      "epoch:25 step:24250 [D loss: 0.204136, acc.: 73.44%] [G loss: 0.452583]\n",
      "epoch:25 step:24251 [D loss: 0.216005, acc.: 63.28%] [G loss: 0.457005]\n",
      "epoch:25 step:24252 [D loss: 0.258490, acc.: 49.22%] [G loss: 0.400190]\n",
      "epoch:25 step:24253 [D loss: 0.239695, acc.: 54.69%] [G loss: 0.421439]\n",
      "epoch:25 step:24254 [D loss: 0.230753, acc.: 64.06%] [G loss: 0.407398]\n",
      "epoch:25 step:24255 [D loss: 0.223051, acc.: 64.84%] [G loss: 0.423260]\n",
      "epoch:25 step:24256 [D loss: 0.230520, acc.: 58.59%] [G loss: 0.404508]\n",
      "epoch:25 step:24257 [D loss: 0.222441, acc.: 65.62%] [G loss: 0.438303]\n",
      "epoch:25 step:24258 [D loss: 0.204164, acc.: 67.19%] [G loss: 0.434835]\n",
      "epoch:25 step:24259 [D loss: 0.229028, acc.: 62.50%] [G loss: 0.404218]\n",
      "epoch:25 step:24260 [D loss: 0.200361, acc.: 71.09%] [G loss: 0.428762]\n",
      "epoch:25 step:24261 [D loss: 0.219155, acc.: 63.28%] [G loss: 0.390504]\n",
      "epoch:25 step:24262 [D loss: 0.209001, acc.: 67.19%] [G loss: 0.413849]\n",
      "epoch:25 step:24263 [D loss: 0.223935, acc.: 62.50%] [G loss: 0.422285]\n",
      "epoch:25 step:24264 [D loss: 0.220708, acc.: 61.72%] [G loss: 0.384772]\n",
      "epoch:25 step:24265 [D loss: 0.220314, acc.: 66.41%] [G loss: 0.440343]\n",
      "epoch:25 step:24266 [D loss: 0.228913, acc.: 66.41%] [G loss: 0.440380]\n",
      "epoch:25 step:24267 [D loss: 0.204899, acc.: 71.09%] [G loss: 0.421373]\n",
      "epoch:25 step:24268 [D loss: 0.226057, acc.: 67.19%] [G loss: 0.430647]\n",
      "epoch:25 step:24269 [D loss: 0.252615, acc.: 53.91%] [G loss: 0.417291]\n",
      "epoch:25 step:24270 [D loss: 0.208776, acc.: 68.75%] [G loss: 0.435853]\n",
      "epoch:25 step:24271 [D loss: 0.237663, acc.: 57.03%] [G loss: 0.414311]\n",
      "epoch:25 step:24272 [D loss: 0.270232, acc.: 50.00%] [G loss: 0.399531]\n",
      "epoch:25 step:24273 [D loss: 0.214041, acc.: 69.53%] [G loss: 0.392896]\n",
      "epoch:25 step:24274 [D loss: 0.219500, acc.: 58.59%] [G loss: 0.379615]\n",
      "epoch:25 step:24275 [D loss: 0.241274, acc.: 58.59%] [G loss: 0.406486]\n",
      "epoch:25 step:24276 [D loss: 0.235727, acc.: 57.03%] [G loss: 0.436956]\n",
      "epoch:25 step:24277 [D loss: 0.225756, acc.: 57.81%] [G loss: 0.426073]\n",
      "epoch:25 step:24278 [D loss: 0.216731, acc.: 63.28%] [G loss: 0.439410]\n",
      "epoch:25 step:24279 [D loss: 0.219115, acc.: 64.84%] [G loss: 0.408026]\n",
      "epoch:25 step:24280 [D loss: 0.217807, acc.: 67.97%] [G loss: 0.415003]\n",
      "epoch:25 step:24281 [D loss: 0.238086, acc.: 60.16%] [G loss: 0.415080]\n",
      "epoch:25 step:24282 [D loss: 0.233932, acc.: 58.59%] [G loss: 0.406466]\n",
      "epoch:25 step:24283 [D loss: 0.263018, acc.: 52.34%] [G loss: 0.407055]\n",
      "epoch:25 step:24284 [D loss: 0.229127, acc.: 59.38%] [G loss: 0.410566]\n",
      "epoch:25 step:24285 [D loss: 0.200591, acc.: 65.62%] [G loss: 0.438533]\n",
      "epoch:25 step:24286 [D loss: 0.250676, acc.: 50.78%] [G loss: 0.400447]\n",
      "epoch:25 step:24287 [D loss: 0.230992, acc.: 61.72%] [G loss: 0.402232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24288 [D loss: 0.238365, acc.: 60.16%] [G loss: 0.394877]\n",
      "epoch:25 step:24289 [D loss: 0.221054, acc.: 59.38%] [G loss: 0.415971]\n",
      "epoch:25 step:24290 [D loss: 0.247255, acc.: 59.38%] [G loss: 0.401617]\n",
      "epoch:25 step:24291 [D loss: 0.225239, acc.: 62.50%] [G loss: 0.430307]\n",
      "epoch:25 step:24292 [D loss: 0.239648, acc.: 53.12%] [G loss: 0.383787]\n",
      "epoch:25 step:24293 [D loss: 0.229480, acc.: 59.38%] [G loss: 0.399012]\n",
      "epoch:25 step:24294 [D loss: 0.224202, acc.: 61.72%] [G loss: 0.428378]\n",
      "epoch:25 step:24295 [D loss: 0.229073, acc.: 58.59%] [G loss: 0.416397]\n",
      "epoch:25 step:24296 [D loss: 0.206727, acc.: 66.41%] [G loss: 0.420720]\n",
      "epoch:25 step:24297 [D loss: 0.231591, acc.: 62.50%] [G loss: 0.417030]\n",
      "epoch:25 step:24298 [D loss: 0.219758, acc.: 59.38%] [G loss: 0.426282]\n",
      "epoch:25 step:24299 [D loss: 0.235963, acc.: 63.28%] [G loss: 0.438568]\n",
      "epoch:25 step:24300 [D loss: 0.217683, acc.: 68.75%] [G loss: 0.393310]\n",
      "epoch:25 step:24301 [D loss: 0.209120, acc.: 68.75%] [G loss: 0.417535]\n",
      "epoch:25 step:24302 [D loss: 0.246385, acc.: 55.47%] [G loss: 0.419122]\n",
      "epoch:25 step:24303 [D loss: 0.230046, acc.: 64.06%] [G loss: 0.391199]\n",
      "epoch:25 step:24304 [D loss: 0.211901, acc.: 66.41%] [G loss: 0.430589]\n",
      "epoch:25 step:24305 [D loss: 0.252111, acc.: 57.03%] [G loss: 0.415818]\n",
      "epoch:25 step:24306 [D loss: 0.236805, acc.: 60.16%] [G loss: 0.367114]\n",
      "epoch:25 step:24307 [D loss: 0.226105, acc.: 66.41%] [G loss: 0.405698]\n",
      "epoch:25 step:24308 [D loss: 0.231362, acc.: 60.94%] [G loss: 0.395877]\n",
      "epoch:25 step:24309 [D loss: 0.199634, acc.: 71.88%] [G loss: 0.450734]\n",
      "epoch:25 step:24310 [D loss: 0.200723, acc.: 71.09%] [G loss: 0.496831]\n",
      "epoch:25 step:24311 [D loss: 0.189451, acc.: 70.31%] [G loss: 0.493919]\n",
      "epoch:25 step:24312 [D loss: 0.251236, acc.: 59.38%] [G loss: 0.463769]\n",
      "epoch:25 step:24313 [D loss: 0.226396, acc.: 64.06%] [G loss: 0.451548]\n",
      "epoch:25 step:24314 [D loss: 0.213787, acc.: 67.19%] [G loss: 0.405671]\n",
      "epoch:25 step:24315 [D loss: 0.192027, acc.: 73.44%] [G loss: 0.430010]\n",
      "epoch:25 step:24316 [D loss: 0.260803, acc.: 46.88%] [G loss: 0.433419]\n",
      "epoch:25 step:24317 [D loss: 0.269289, acc.: 54.69%] [G loss: 0.382401]\n",
      "epoch:25 step:24318 [D loss: 0.214642, acc.: 65.62%] [G loss: 0.447488]\n",
      "epoch:25 step:24319 [D loss: 0.211331, acc.: 67.19%] [G loss: 0.435004]\n",
      "epoch:25 step:24320 [D loss: 0.225837, acc.: 66.41%] [G loss: 0.445349]\n",
      "epoch:25 step:24321 [D loss: 0.217552, acc.: 67.97%] [G loss: 0.447945]\n",
      "epoch:25 step:24322 [D loss: 0.202296, acc.: 67.19%] [G loss: 0.451484]\n",
      "epoch:25 step:24323 [D loss: 0.219272, acc.: 62.50%] [G loss: 0.472292]\n",
      "epoch:25 step:24324 [D loss: 0.184896, acc.: 73.44%] [G loss: 0.499690]\n",
      "epoch:25 step:24325 [D loss: 0.252916, acc.: 60.16%] [G loss: 0.427211]\n",
      "epoch:25 step:24326 [D loss: 0.203112, acc.: 69.53%] [G loss: 0.456261]\n",
      "epoch:25 step:24327 [D loss: 0.239791, acc.: 63.28%] [G loss: 0.433289]\n",
      "epoch:25 step:24328 [D loss: 0.221555, acc.: 66.41%] [G loss: 0.410755]\n",
      "epoch:25 step:24329 [D loss: 0.245917, acc.: 58.59%] [G loss: 0.422347]\n",
      "epoch:25 step:24330 [D loss: 0.199087, acc.: 73.44%] [G loss: 0.429283]\n",
      "epoch:25 step:24331 [D loss: 0.229079, acc.: 64.06%] [G loss: 0.427053]\n",
      "epoch:25 step:24332 [D loss: 0.233611, acc.: 60.16%] [G loss: 0.469709]\n",
      "epoch:25 step:24333 [D loss: 0.220167, acc.: 69.53%] [G loss: 0.413734]\n",
      "epoch:25 step:24334 [D loss: 0.198890, acc.: 72.66%] [G loss: 0.444741]\n",
      "epoch:25 step:24335 [D loss: 0.211387, acc.: 68.75%] [G loss: 0.445878]\n",
      "epoch:25 step:24336 [D loss: 0.203770, acc.: 71.09%] [G loss: 0.449797]\n",
      "epoch:25 step:24337 [D loss: 0.206443, acc.: 67.97%] [G loss: 0.450236]\n",
      "epoch:25 step:24338 [D loss: 0.229879, acc.: 63.28%] [G loss: 0.446793]\n",
      "epoch:25 step:24339 [D loss: 0.193223, acc.: 65.62%] [G loss: 0.459949]\n",
      "epoch:25 step:24340 [D loss: 0.290946, acc.: 49.22%] [G loss: 0.357603]\n",
      "epoch:25 step:24341 [D loss: 0.233820, acc.: 57.81%] [G loss: 0.378770]\n",
      "epoch:25 step:24342 [D loss: 0.237664, acc.: 60.94%] [G loss: 0.442581]\n",
      "epoch:25 step:24343 [D loss: 0.203683, acc.: 67.97%] [G loss: 0.465037]\n",
      "epoch:25 step:24344 [D loss: 0.182208, acc.: 76.56%] [G loss: 0.502536]\n",
      "epoch:25 step:24345 [D loss: 0.316579, acc.: 43.75%] [G loss: 0.412732]\n",
      "epoch:25 step:24346 [D loss: 0.196465, acc.: 66.41%] [G loss: 0.439217]\n",
      "epoch:25 step:24347 [D loss: 0.243669, acc.: 55.47%] [G loss: 0.427445]\n",
      "epoch:25 step:24348 [D loss: 0.174066, acc.: 78.12%] [G loss: 0.435710]\n",
      "epoch:25 step:24349 [D loss: 0.189426, acc.: 73.44%] [G loss: 0.496335]\n",
      "epoch:25 step:24350 [D loss: 0.168207, acc.: 80.47%] [G loss: 0.547036]\n",
      "epoch:25 step:24351 [D loss: 0.197031, acc.: 75.00%] [G loss: 0.483856]\n",
      "epoch:25 step:24352 [D loss: 0.219335, acc.: 66.41%] [G loss: 0.496358]\n",
      "epoch:25 step:24353 [D loss: 0.274257, acc.: 54.69%] [G loss: 0.545675]\n",
      "epoch:25 step:24354 [D loss: 0.247548, acc.: 59.38%] [G loss: 0.613935]\n",
      "epoch:25 step:24355 [D loss: 0.202100, acc.: 71.09%] [G loss: 0.488023]\n",
      "epoch:25 step:24356 [D loss: 0.251110, acc.: 56.25%] [G loss: 0.402635]\n",
      "epoch:25 step:24357 [D loss: 0.270777, acc.: 57.03%] [G loss: 0.397497]\n",
      "epoch:25 step:24358 [D loss: 0.215586, acc.: 64.84%] [G loss: 0.490542]\n",
      "epoch:25 step:24359 [D loss: 0.206123, acc.: 67.19%] [G loss: 0.448929]\n",
      "epoch:25 step:24360 [D loss: 0.189725, acc.: 71.88%] [G loss: 0.480413]\n",
      "epoch:25 step:24361 [D loss: 0.175252, acc.: 71.88%] [G loss: 0.520541]\n",
      "epoch:25 step:24362 [D loss: 0.210610, acc.: 71.09%] [G loss: 0.517693]\n",
      "epoch:26 step:24363 [D loss: 0.245707, acc.: 60.16%] [G loss: 0.441099]\n",
      "epoch:26 step:24364 [D loss: 0.243752, acc.: 57.03%] [G loss: 0.459459]\n",
      "epoch:26 step:24365 [D loss: 0.237055, acc.: 60.94%] [G loss: 0.412750]\n",
      "epoch:26 step:24366 [D loss: 0.223422, acc.: 70.31%] [G loss: 0.426821]\n",
      "epoch:26 step:24367 [D loss: 0.233195, acc.: 57.81%] [G loss: 0.432095]\n",
      "epoch:26 step:24368 [D loss: 0.221667, acc.: 67.19%] [G loss: 0.454557]\n",
      "epoch:26 step:24369 [D loss: 0.197652, acc.: 74.22%] [G loss: 0.455174]\n",
      "epoch:26 step:24370 [D loss: 0.210694, acc.: 64.84%] [G loss: 0.434436]\n",
      "epoch:26 step:24371 [D loss: 0.193780, acc.: 72.66%] [G loss: 0.493261]\n",
      "epoch:26 step:24372 [D loss: 0.214487, acc.: 67.97%] [G loss: 0.443444]\n",
      "epoch:26 step:24373 [D loss: 0.217584, acc.: 65.62%] [G loss: 0.440641]\n",
      "epoch:26 step:24374 [D loss: 0.220098, acc.: 65.62%] [G loss: 0.448452]\n",
      "epoch:26 step:24375 [D loss: 0.217331, acc.: 67.19%] [G loss: 0.448114]\n",
      "epoch:26 step:24376 [D loss: 0.193723, acc.: 71.88%] [G loss: 0.444480]\n",
      "epoch:26 step:24377 [D loss: 0.187734, acc.: 75.78%] [G loss: 0.466259]\n",
      "epoch:26 step:24378 [D loss: 0.202635, acc.: 66.41%] [G loss: 0.425068]\n",
      "epoch:26 step:24379 [D loss: 0.232135, acc.: 57.81%] [G loss: 0.445984]\n",
      "epoch:26 step:24380 [D loss: 0.212130, acc.: 67.19%] [G loss: 0.461884]\n",
      "epoch:26 step:24381 [D loss: 0.255635, acc.: 57.81%] [G loss: 0.438589]\n",
      "epoch:26 step:24382 [D loss: 0.270532, acc.: 52.34%] [G loss: 0.472875]\n",
      "epoch:26 step:24383 [D loss: 0.228574, acc.: 60.16%] [G loss: 0.487232]\n",
      "epoch:26 step:24384 [D loss: 0.210095, acc.: 66.41%] [G loss: 0.487662]\n",
      "epoch:26 step:24385 [D loss: 0.264298, acc.: 57.03%] [G loss: 0.380088]\n",
      "epoch:26 step:24386 [D loss: 0.196592, acc.: 67.97%] [G loss: 0.438059]\n",
      "epoch:26 step:24387 [D loss: 0.220200, acc.: 60.94%] [G loss: 0.437943]\n",
      "epoch:26 step:24388 [D loss: 0.216914, acc.: 64.84%] [G loss: 0.422922]\n",
      "epoch:26 step:24389 [D loss: 0.212799, acc.: 67.19%] [G loss: 0.439427]\n",
      "epoch:26 step:24390 [D loss: 0.224499, acc.: 65.62%] [G loss: 0.434129]\n",
      "epoch:26 step:24391 [D loss: 0.198780, acc.: 65.62%] [G loss: 0.483483]\n",
      "epoch:26 step:24392 [D loss: 0.231444, acc.: 64.06%] [G loss: 0.444819]\n",
      "epoch:26 step:24393 [D loss: 0.226782, acc.: 64.84%] [G loss: 0.453895]\n",
      "epoch:26 step:24394 [D loss: 0.226386, acc.: 57.81%] [G loss: 0.446869]\n",
      "epoch:26 step:24395 [D loss: 0.233626, acc.: 57.81%] [G loss: 0.410980]\n",
      "epoch:26 step:24396 [D loss: 0.234569, acc.: 57.81%] [G loss: 0.402366]\n",
      "epoch:26 step:24397 [D loss: 0.239182, acc.: 60.16%] [G loss: 0.433423]\n",
      "epoch:26 step:24398 [D loss: 0.227357, acc.: 57.03%] [G loss: 0.388468]\n",
      "epoch:26 step:24399 [D loss: 0.225279, acc.: 59.38%] [G loss: 0.430954]\n",
      "epoch:26 step:24400 [D loss: 0.244266, acc.: 57.03%] [G loss: 0.394251]\n",
      "##############\n",
      "[2.8070803  2.07140464 6.03793418 4.83814451 3.54423859 5.73458488\n",
      " 4.6596409  4.73121451 4.66720562 4.11486495]\n",
      "##########\n",
      "epoch:26 step:24401 [D loss: 0.212814, acc.: 67.19%] [G loss: 0.396741]\n",
      "epoch:26 step:24402 [D loss: 0.198170, acc.: 67.97%] [G loss: 0.436183]\n",
      "epoch:26 step:24403 [D loss: 0.238090, acc.: 59.38%] [G loss: 0.429451]\n",
      "epoch:26 step:24404 [D loss: 0.214161, acc.: 69.53%] [G loss: 0.398777]\n",
      "epoch:26 step:24405 [D loss: 0.244525, acc.: 54.69%] [G loss: 0.402707]\n",
      "epoch:26 step:24406 [D loss: 0.238503, acc.: 57.03%] [G loss: 0.411757]\n",
      "epoch:26 step:24407 [D loss: 0.221845, acc.: 65.62%] [G loss: 0.437921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24408 [D loss: 0.244614, acc.: 59.38%] [G loss: 0.439134]\n",
      "epoch:26 step:24409 [D loss: 0.218350, acc.: 67.97%] [G loss: 0.404094]\n",
      "epoch:26 step:24410 [D loss: 0.216680, acc.: 60.94%] [G loss: 0.388584]\n",
      "epoch:26 step:24411 [D loss: 0.190429, acc.: 75.78%] [G loss: 0.483142]\n",
      "epoch:26 step:24412 [D loss: 0.203170, acc.: 67.19%] [G loss: 0.451654]\n",
      "epoch:26 step:24413 [D loss: 0.217418, acc.: 66.41%] [G loss: 0.426076]\n",
      "epoch:26 step:24414 [D loss: 0.243934, acc.: 61.72%] [G loss: 0.418814]\n",
      "epoch:26 step:24415 [D loss: 0.213937, acc.: 67.19%] [G loss: 0.439957]\n",
      "epoch:26 step:24416 [D loss: 0.205572, acc.: 67.19%] [G loss: 0.443604]\n",
      "epoch:26 step:24417 [D loss: 0.218640, acc.: 67.19%] [G loss: 0.411456]\n",
      "epoch:26 step:24418 [D loss: 0.232069, acc.: 60.16%] [G loss: 0.418940]\n",
      "epoch:26 step:24419 [D loss: 0.235175, acc.: 64.84%] [G loss: 0.425497]\n",
      "epoch:26 step:24420 [D loss: 0.228774, acc.: 60.16%] [G loss: 0.432715]\n",
      "epoch:26 step:24421 [D loss: 0.223370, acc.: 60.94%] [G loss: 0.425058]\n",
      "epoch:26 step:24422 [D loss: 0.235038, acc.: 66.41%] [G loss: 0.423858]\n",
      "epoch:26 step:24423 [D loss: 0.224681, acc.: 64.06%] [G loss: 0.385476]\n",
      "epoch:26 step:24424 [D loss: 0.253214, acc.: 57.03%] [G loss: 0.403596]\n",
      "epoch:26 step:24425 [D loss: 0.214684, acc.: 64.06%] [G loss: 0.422274]\n",
      "epoch:26 step:24426 [D loss: 0.231990, acc.: 58.59%] [G loss: 0.409013]\n",
      "epoch:26 step:24427 [D loss: 0.226810, acc.: 60.16%] [G loss: 0.456628]\n",
      "epoch:26 step:24428 [D loss: 0.218394, acc.: 67.97%] [G loss: 0.428970]\n",
      "epoch:26 step:24429 [D loss: 0.222905, acc.: 63.28%] [G loss: 0.385220]\n",
      "epoch:26 step:24430 [D loss: 0.231223, acc.: 55.47%] [G loss: 0.429472]\n",
      "epoch:26 step:24431 [D loss: 0.188186, acc.: 75.00%] [G loss: 0.431563]\n",
      "epoch:26 step:24432 [D loss: 0.203397, acc.: 71.09%] [G loss: 0.450921]\n",
      "epoch:26 step:24433 [D loss: 0.244690, acc.: 61.72%] [G loss: 0.421808]\n",
      "epoch:26 step:24434 [D loss: 0.245395, acc.: 56.25%] [G loss: 0.413520]\n",
      "epoch:26 step:24435 [D loss: 0.235403, acc.: 63.28%] [G loss: 0.396922]\n",
      "epoch:26 step:24436 [D loss: 0.205071, acc.: 68.75%] [G loss: 0.456412]\n",
      "epoch:26 step:24437 [D loss: 0.210163, acc.: 68.75%] [G loss: 0.450603]\n",
      "epoch:26 step:24438 [D loss: 0.213735, acc.: 64.84%] [G loss: 0.432746]\n",
      "epoch:26 step:24439 [D loss: 0.210928, acc.: 66.41%] [G loss: 0.440331]\n",
      "epoch:26 step:24440 [D loss: 0.259777, acc.: 52.34%] [G loss: 0.458146]\n",
      "epoch:26 step:24441 [D loss: 0.232446, acc.: 61.72%] [G loss: 0.441053]\n",
      "epoch:26 step:24442 [D loss: 0.247551, acc.: 53.91%] [G loss: 0.368253]\n",
      "epoch:26 step:24443 [D loss: 0.218554, acc.: 60.94%] [G loss: 0.406017]\n",
      "epoch:26 step:24444 [D loss: 0.232018, acc.: 60.16%] [G loss: 0.409169]\n",
      "epoch:26 step:24445 [D loss: 0.209170, acc.: 67.97%] [G loss: 0.457887]\n",
      "epoch:26 step:24446 [D loss: 0.219108, acc.: 62.50%] [G loss: 0.463884]\n",
      "epoch:26 step:24447 [D loss: 0.245625, acc.: 54.69%] [G loss: 0.455728]\n",
      "epoch:26 step:24448 [D loss: 0.228258, acc.: 67.19%] [G loss: 0.411940]\n",
      "epoch:26 step:24449 [D loss: 0.210963, acc.: 70.31%] [G loss: 0.426511]\n",
      "epoch:26 step:24450 [D loss: 0.219897, acc.: 66.41%] [G loss: 0.390908]\n",
      "epoch:26 step:24451 [D loss: 0.200760, acc.: 74.22%] [G loss: 0.434609]\n",
      "epoch:26 step:24452 [D loss: 0.210019, acc.: 65.62%] [G loss: 0.418236]\n",
      "epoch:26 step:24453 [D loss: 0.251896, acc.: 56.25%] [G loss: 0.406719]\n",
      "epoch:26 step:24454 [D loss: 0.219395, acc.: 65.62%] [G loss: 0.436954]\n",
      "epoch:26 step:24455 [D loss: 0.214136, acc.: 64.84%] [G loss: 0.457968]\n",
      "epoch:26 step:24456 [D loss: 0.207380, acc.: 65.62%] [G loss: 0.504105]\n",
      "epoch:26 step:24457 [D loss: 0.223428, acc.: 64.84%] [G loss: 0.427860]\n",
      "epoch:26 step:24458 [D loss: 0.219298, acc.: 62.50%] [G loss: 0.412533]\n",
      "epoch:26 step:24459 [D loss: 0.194575, acc.: 71.88%] [G loss: 0.405856]\n",
      "epoch:26 step:24460 [D loss: 0.213234, acc.: 65.62%] [G loss: 0.490672]\n",
      "epoch:26 step:24461 [D loss: 0.244821, acc.: 57.03%] [G loss: 0.430504]\n",
      "epoch:26 step:24462 [D loss: 0.219215, acc.: 65.62%] [G loss: 0.463606]\n",
      "epoch:26 step:24463 [D loss: 0.220182, acc.: 60.94%] [G loss: 0.430107]\n",
      "epoch:26 step:24464 [D loss: 0.245149, acc.: 57.81%] [G loss: 0.371191]\n",
      "epoch:26 step:24465 [D loss: 0.211849, acc.: 69.53%] [G loss: 0.428835]\n",
      "epoch:26 step:24466 [D loss: 0.234015, acc.: 62.50%] [G loss: 0.392193]\n",
      "epoch:26 step:24467 [D loss: 0.244578, acc.: 61.72%] [G loss: 0.428230]\n",
      "epoch:26 step:24468 [D loss: 0.210124, acc.: 67.97%] [G loss: 0.434852]\n",
      "epoch:26 step:24469 [D loss: 0.216171, acc.: 65.62%] [G loss: 0.434167]\n",
      "epoch:26 step:24470 [D loss: 0.275988, acc.: 50.78%] [G loss: 0.435935]\n",
      "epoch:26 step:24471 [D loss: 0.231887, acc.: 60.94%] [G loss: 0.500667]\n",
      "epoch:26 step:24472 [D loss: 0.244950, acc.: 54.69%] [G loss: 0.404189]\n",
      "epoch:26 step:24473 [D loss: 0.223447, acc.: 60.16%] [G loss: 0.424199]\n",
      "epoch:26 step:24474 [D loss: 0.211382, acc.: 59.38%] [G loss: 0.405563]\n",
      "epoch:26 step:24475 [D loss: 0.189282, acc.: 69.53%] [G loss: 0.452413]\n",
      "epoch:26 step:24476 [D loss: 0.225245, acc.: 62.50%] [G loss: 0.444251]\n",
      "epoch:26 step:24477 [D loss: 0.179009, acc.: 71.88%] [G loss: 0.471336]\n",
      "epoch:26 step:24478 [D loss: 0.196262, acc.: 67.97%] [G loss: 0.521230]\n",
      "epoch:26 step:24479 [D loss: 0.201723, acc.: 73.44%] [G loss: 0.487944]\n",
      "epoch:26 step:24480 [D loss: 0.207071, acc.: 68.75%] [G loss: 0.493314]\n",
      "epoch:26 step:24481 [D loss: 0.173449, acc.: 74.22%] [G loss: 0.499942]\n",
      "epoch:26 step:24482 [D loss: 0.247060, acc.: 57.81%] [G loss: 0.469803]\n",
      "epoch:26 step:24483 [D loss: 0.245374, acc.: 55.47%] [G loss: 0.422558]\n",
      "epoch:26 step:24484 [D loss: 0.220509, acc.: 65.62%] [G loss: 0.451730]\n",
      "epoch:26 step:24485 [D loss: 0.198733, acc.: 68.75%] [G loss: 0.437314]\n",
      "epoch:26 step:24486 [D loss: 0.247029, acc.: 57.03%] [G loss: 0.441571]\n",
      "epoch:26 step:24487 [D loss: 0.262920, acc.: 50.00%] [G loss: 0.406992]\n",
      "epoch:26 step:24488 [D loss: 0.187493, acc.: 79.69%] [G loss: 0.445546]\n",
      "epoch:26 step:24489 [D loss: 0.203424, acc.: 70.31%] [G loss: 0.453859]\n",
      "epoch:26 step:24490 [D loss: 0.250333, acc.: 56.25%] [G loss: 0.383408]\n",
      "epoch:26 step:24491 [D loss: 0.202924, acc.: 69.53%] [G loss: 0.436337]\n",
      "epoch:26 step:24492 [D loss: 0.201080, acc.: 65.62%] [G loss: 0.405902]\n",
      "epoch:26 step:24493 [D loss: 0.215165, acc.: 67.97%] [G loss: 0.434229]\n",
      "epoch:26 step:24494 [D loss: 0.212386, acc.: 64.06%] [G loss: 0.388476]\n",
      "epoch:26 step:24495 [D loss: 0.252178, acc.: 57.81%] [G loss: 0.424259]\n",
      "epoch:26 step:24496 [D loss: 0.234374, acc.: 60.16%] [G loss: 0.421640]\n",
      "epoch:26 step:24497 [D loss: 0.230539, acc.: 62.50%] [G loss: 0.425168]\n",
      "epoch:26 step:24498 [D loss: 0.199872, acc.: 72.66%] [G loss: 0.493324]\n",
      "epoch:26 step:24499 [D loss: 0.242572, acc.: 62.50%] [G loss: 0.428050]\n",
      "epoch:26 step:24500 [D loss: 0.262799, acc.: 50.00%] [G loss: 0.404789]\n",
      "epoch:26 step:24501 [D loss: 0.233920, acc.: 66.41%] [G loss: 0.402029]\n",
      "epoch:26 step:24502 [D loss: 0.247875, acc.: 55.47%] [G loss: 0.404359]\n",
      "epoch:26 step:24503 [D loss: 0.238440, acc.: 60.94%] [G loss: 0.381576]\n",
      "epoch:26 step:24504 [D loss: 0.230894, acc.: 63.28%] [G loss: 0.413038]\n",
      "epoch:26 step:24505 [D loss: 0.234644, acc.: 64.06%] [G loss: 0.387674]\n",
      "epoch:26 step:24506 [D loss: 0.227131, acc.: 63.28%] [G loss: 0.400963]\n",
      "epoch:26 step:24507 [D loss: 0.207815, acc.: 63.28%] [G loss: 0.427417]\n",
      "epoch:26 step:24508 [D loss: 0.256379, acc.: 52.34%] [G loss: 0.405381]\n",
      "epoch:26 step:24509 [D loss: 0.222340, acc.: 62.50%] [G loss: 0.433694]\n",
      "epoch:26 step:24510 [D loss: 0.229609, acc.: 56.25%] [G loss: 0.405925]\n",
      "epoch:26 step:24511 [D loss: 0.229010, acc.: 61.72%] [G loss: 0.416747]\n",
      "epoch:26 step:24512 [D loss: 0.243334, acc.: 57.03%] [G loss: 0.412683]\n",
      "epoch:26 step:24513 [D loss: 0.200023, acc.: 72.66%] [G loss: 0.429498]\n",
      "epoch:26 step:24514 [D loss: 0.222965, acc.: 65.62%] [G loss: 0.411960]\n",
      "epoch:26 step:24515 [D loss: 0.246041, acc.: 55.47%] [G loss: 0.387130]\n",
      "epoch:26 step:24516 [D loss: 0.229811, acc.: 57.81%] [G loss: 0.436874]\n",
      "epoch:26 step:24517 [D loss: 0.219301, acc.: 64.84%] [G loss: 0.400684]\n",
      "epoch:26 step:24518 [D loss: 0.196199, acc.: 75.78%] [G loss: 0.424947]\n",
      "epoch:26 step:24519 [D loss: 0.236681, acc.: 57.03%] [G loss: 0.385384]\n",
      "epoch:26 step:24520 [D loss: 0.211606, acc.: 70.31%] [G loss: 0.443368]\n",
      "epoch:26 step:24521 [D loss: 0.200744, acc.: 73.44%] [G loss: 0.430923]\n",
      "epoch:26 step:24522 [D loss: 0.265198, acc.: 51.56%] [G loss: 0.437823]\n",
      "epoch:26 step:24523 [D loss: 0.225276, acc.: 63.28%] [G loss: 0.465146]\n",
      "epoch:26 step:24524 [D loss: 0.272573, acc.: 52.34%] [G loss: 0.412596]\n",
      "epoch:26 step:24525 [D loss: 0.233841, acc.: 59.38%] [G loss: 0.452317]\n",
      "epoch:26 step:24526 [D loss: 0.235101, acc.: 61.72%] [G loss: 0.475134]\n",
      "epoch:26 step:24527 [D loss: 0.218185, acc.: 65.62%] [G loss: 0.431646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24528 [D loss: 0.231567, acc.: 63.28%] [G loss: 0.394939]\n",
      "epoch:26 step:24529 [D loss: 0.218453, acc.: 64.06%] [G loss: 0.443948]\n",
      "epoch:26 step:24530 [D loss: 0.208806, acc.: 63.28%] [G loss: 0.438305]\n",
      "epoch:26 step:24531 [D loss: 0.237266, acc.: 60.16%] [G loss: 0.431590]\n",
      "epoch:26 step:24532 [D loss: 0.224018, acc.: 63.28%] [G loss: 0.427314]\n",
      "epoch:26 step:24533 [D loss: 0.226419, acc.: 63.28%] [G loss: 0.402524]\n",
      "epoch:26 step:24534 [D loss: 0.215536, acc.: 64.84%] [G loss: 0.417948]\n",
      "epoch:26 step:24535 [D loss: 0.206521, acc.: 65.62%] [G loss: 0.410416]\n",
      "epoch:26 step:24536 [D loss: 0.235786, acc.: 60.94%] [G loss: 0.410132]\n",
      "epoch:26 step:24537 [D loss: 0.214112, acc.: 65.62%] [G loss: 0.417899]\n",
      "epoch:26 step:24538 [D loss: 0.198095, acc.: 65.62%] [G loss: 0.426301]\n",
      "epoch:26 step:24539 [D loss: 0.222250, acc.: 60.16%] [G loss: 0.419253]\n",
      "epoch:26 step:24540 [D loss: 0.236970, acc.: 57.81%] [G loss: 0.425125]\n",
      "epoch:26 step:24541 [D loss: 0.230243, acc.: 59.38%] [G loss: 0.432598]\n",
      "epoch:26 step:24542 [D loss: 0.240766, acc.: 60.16%] [G loss: 0.438842]\n",
      "epoch:26 step:24543 [D loss: 0.225934, acc.: 64.84%] [G loss: 0.445855]\n",
      "epoch:26 step:24544 [D loss: 0.243028, acc.: 60.16%] [G loss: 0.464080]\n",
      "epoch:26 step:24545 [D loss: 0.222194, acc.: 68.75%] [G loss: 0.446149]\n",
      "epoch:26 step:24546 [D loss: 0.240785, acc.: 57.81%] [G loss: 0.425086]\n",
      "epoch:26 step:24547 [D loss: 0.237694, acc.: 59.38%] [G loss: 0.405393]\n",
      "epoch:26 step:24548 [D loss: 0.253166, acc.: 60.94%] [G loss: 0.397288]\n",
      "epoch:26 step:24549 [D loss: 0.230073, acc.: 63.28%] [G loss: 0.412204]\n",
      "epoch:26 step:24550 [D loss: 0.232977, acc.: 62.50%] [G loss: 0.425990]\n",
      "epoch:26 step:24551 [D loss: 0.243681, acc.: 61.72%] [G loss: 0.402341]\n",
      "epoch:26 step:24552 [D loss: 0.230570, acc.: 62.50%] [G loss: 0.425036]\n",
      "epoch:26 step:24553 [D loss: 0.209990, acc.: 64.84%] [G loss: 0.388894]\n",
      "epoch:26 step:24554 [D loss: 0.207855, acc.: 68.75%] [G loss: 0.406328]\n",
      "epoch:26 step:24555 [D loss: 0.231676, acc.: 59.38%] [G loss: 0.398999]\n",
      "epoch:26 step:24556 [D loss: 0.207593, acc.: 67.19%] [G loss: 0.387172]\n",
      "epoch:26 step:24557 [D loss: 0.203227, acc.: 64.84%] [G loss: 0.426027]\n",
      "epoch:26 step:24558 [D loss: 0.230912, acc.: 59.38%] [G loss: 0.430471]\n",
      "epoch:26 step:24559 [D loss: 0.210726, acc.: 70.31%] [G loss: 0.425999]\n",
      "epoch:26 step:24560 [D loss: 0.199071, acc.: 71.88%] [G loss: 0.426389]\n",
      "epoch:26 step:24561 [D loss: 0.241450, acc.: 56.25%] [G loss: 0.398655]\n",
      "epoch:26 step:24562 [D loss: 0.255509, acc.: 50.78%] [G loss: 0.404209]\n",
      "epoch:26 step:24563 [D loss: 0.216544, acc.: 61.72%] [G loss: 0.435037]\n",
      "epoch:26 step:24564 [D loss: 0.238220, acc.: 62.50%] [G loss: 0.397149]\n",
      "epoch:26 step:24565 [D loss: 0.277766, acc.: 50.78%] [G loss: 0.391408]\n",
      "epoch:26 step:24566 [D loss: 0.212542, acc.: 65.62%] [G loss: 0.488204]\n",
      "epoch:26 step:24567 [D loss: 0.231224, acc.: 60.94%] [G loss: 0.456888]\n",
      "epoch:26 step:24568 [D loss: 0.231335, acc.: 64.06%] [G loss: 0.477352]\n",
      "epoch:26 step:24569 [D loss: 0.228080, acc.: 61.72%] [G loss: 0.425664]\n",
      "epoch:26 step:24570 [D loss: 0.182457, acc.: 69.53%] [G loss: 0.469931]\n",
      "epoch:26 step:24571 [D loss: 0.178255, acc.: 75.78%] [G loss: 0.494065]\n",
      "epoch:26 step:24572 [D loss: 0.247454, acc.: 54.69%] [G loss: 0.423082]\n",
      "epoch:26 step:24573 [D loss: 0.243937, acc.: 56.25%] [G loss: 0.408930]\n",
      "epoch:26 step:24574 [D loss: 0.243817, acc.: 59.38%] [G loss: 0.397707]\n",
      "epoch:26 step:24575 [D loss: 0.221801, acc.: 60.94%] [G loss: 0.427712]\n",
      "epoch:26 step:24576 [D loss: 0.257552, acc.: 50.00%] [G loss: 0.376107]\n",
      "epoch:26 step:24577 [D loss: 0.244285, acc.: 53.12%] [G loss: 0.395729]\n",
      "epoch:26 step:24578 [D loss: 0.235368, acc.: 57.81%] [G loss: 0.420557]\n",
      "epoch:26 step:24579 [D loss: 0.218731, acc.: 60.94%] [G loss: 0.417792]\n",
      "epoch:26 step:24580 [D loss: 0.182055, acc.: 74.22%] [G loss: 0.442626]\n",
      "epoch:26 step:24581 [D loss: 0.194341, acc.: 71.09%] [G loss: 0.462843]\n",
      "epoch:26 step:24582 [D loss: 0.267308, acc.: 56.25%] [G loss: 0.404470]\n",
      "epoch:26 step:24583 [D loss: 0.221465, acc.: 62.50%] [G loss: 0.391881]\n",
      "epoch:26 step:24584 [D loss: 0.216994, acc.: 68.75%] [G loss: 0.433372]\n",
      "epoch:26 step:24585 [D loss: 0.200873, acc.: 71.88%] [G loss: 0.489769]\n",
      "epoch:26 step:24586 [D loss: 0.229091, acc.: 64.06%] [G loss: 0.408563]\n",
      "epoch:26 step:24587 [D loss: 0.210679, acc.: 68.75%] [G loss: 0.396169]\n",
      "epoch:26 step:24588 [D loss: 0.242603, acc.: 61.72%] [G loss: 0.379548]\n",
      "epoch:26 step:24589 [D loss: 0.201266, acc.: 70.31%] [G loss: 0.411263]\n",
      "epoch:26 step:24590 [D loss: 0.243067, acc.: 57.81%] [G loss: 0.387119]\n",
      "epoch:26 step:24591 [D loss: 0.193867, acc.: 71.88%] [G loss: 0.420501]\n",
      "epoch:26 step:24592 [D loss: 0.186710, acc.: 75.78%] [G loss: 0.462991]\n",
      "epoch:26 step:24593 [D loss: 0.170365, acc.: 75.00%] [G loss: 0.576960]\n",
      "epoch:26 step:24594 [D loss: 0.180160, acc.: 76.56%] [G loss: 0.513044]\n",
      "epoch:26 step:24595 [D loss: 0.269154, acc.: 56.25%] [G loss: 0.448784]\n",
      "epoch:26 step:24596 [D loss: 0.262731, acc.: 51.56%] [G loss: 0.426842]\n",
      "epoch:26 step:24597 [D loss: 0.211660, acc.: 67.19%] [G loss: 0.453225]\n",
      "epoch:26 step:24598 [D loss: 0.213041, acc.: 65.62%] [G loss: 0.428640]\n",
      "epoch:26 step:24599 [D loss: 0.217164, acc.: 64.06%] [G loss: 0.445927]\n",
      "epoch:26 step:24600 [D loss: 0.214770, acc.: 63.28%] [G loss: 0.416940]\n",
      "##############\n",
      "[2.63675288 1.80323197 6.28172072 4.5279907  3.69789481 5.50060664\n",
      " 4.43413093 4.75465018 4.37150011 3.99451429]\n",
      "##########\n",
      "epoch:26 step:24601 [D loss: 0.235735, acc.: 59.38%] [G loss: 0.474350]\n",
      "epoch:26 step:24602 [D loss: 0.235055, acc.: 63.28%] [G loss: 0.438260]\n",
      "epoch:26 step:24603 [D loss: 0.220447, acc.: 65.62%] [G loss: 0.439760]\n",
      "epoch:26 step:24604 [D loss: 0.208559, acc.: 69.53%] [G loss: 0.443027]\n",
      "epoch:26 step:24605 [D loss: 0.196926, acc.: 74.22%] [G loss: 0.431236]\n",
      "epoch:26 step:24606 [D loss: 0.229224, acc.: 64.84%] [G loss: 0.464750]\n",
      "epoch:26 step:24607 [D loss: 0.222434, acc.: 67.19%] [G loss: 0.443910]\n",
      "epoch:26 step:24608 [D loss: 0.241588, acc.: 55.47%] [G loss: 0.434936]\n",
      "epoch:26 step:24609 [D loss: 0.218635, acc.: 64.84%] [G loss: 0.508732]\n",
      "epoch:26 step:24610 [D loss: 0.204202, acc.: 64.84%] [G loss: 0.476656]\n",
      "epoch:26 step:24611 [D loss: 0.239876, acc.: 61.72%] [G loss: 0.434422]\n",
      "epoch:26 step:24612 [D loss: 0.269540, acc.: 53.12%] [G loss: 0.384779]\n",
      "epoch:26 step:24613 [D loss: 0.256097, acc.: 52.34%] [G loss: 0.424005]\n",
      "epoch:26 step:24614 [D loss: 0.206873, acc.: 66.41%] [G loss: 0.439939]\n",
      "epoch:26 step:24615 [D loss: 0.228548, acc.: 59.38%] [G loss: 0.407414]\n",
      "epoch:26 step:24616 [D loss: 0.232532, acc.: 61.72%] [G loss: 0.415409]\n",
      "epoch:26 step:24617 [D loss: 0.221147, acc.: 68.75%] [G loss: 0.408169]\n",
      "epoch:26 step:24618 [D loss: 0.233000, acc.: 64.06%] [G loss: 0.430679]\n",
      "epoch:26 step:24619 [D loss: 0.242662, acc.: 56.25%] [G loss: 0.403482]\n",
      "epoch:26 step:24620 [D loss: 0.197967, acc.: 67.19%] [G loss: 0.405521]\n",
      "epoch:26 step:24621 [D loss: 0.208644, acc.: 70.31%] [G loss: 0.435949]\n",
      "epoch:26 step:24622 [D loss: 0.226767, acc.: 64.84%] [G loss: 0.388477]\n",
      "epoch:26 step:24623 [D loss: 0.192606, acc.: 74.22%] [G loss: 0.475848]\n",
      "epoch:26 step:24624 [D loss: 0.225276, acc.: 61.72%] [G loss: 0.452631]\n",
      "epoch:26 step:24625 [D loss: 0.283599, acc.: 49.22%] [G loss: 0.397753]\n",
      "epoch:26 step:24626 [D loss: 0.183901, acc.: 75.00%] [G loss: 0.469264]\n",
      "epoch:26 step:24627 [D loss: 0.253739, acc.: 53.91%] [G loss: 0.403660]\n",
      "epoch:26 step:24628 [D loss: 0.255020, acc.: 57.81%] [G loss: 0.415847]\n",
      "epoch:26 step:24629 [D loss: 0.210792, acc.: 64.84%] [G loss: 0.443760]\n",
      "epoch:26 step:24630 [D loss: 0.227481, acc.: 61.72%] [G loss: 0.408168]\n",
      "epoch:26 step:24631 [D loss: 0.193845, acc.: 72.66%] [G loss: 0.412758]\n",
      "epoch:26 step:24632 [D loss: 0.238847, acc.: 60.16%] [G loss: 0.403485]\n",
      "epoch:26 step:24633 [D loss: 0.204635, acc.: 64.06%] [G loss: 0.410599]\n",
      "epoch:26 step:24634 [D loss: 0.230999, acc.: 67.97%] [G loss: 0.428220]\n",
      "epoch:26 step:24635 [D loss: 0.216618, acc.: 62.50%] [G loss: 0.432221]\n",
      "epoch:26 step:24636 [D loss: 0.206760, acc.: 66.41%] [G loss: 0.433852]\n",
      "epoch:26 step:24637 [D loss: 0.229389, acc.: 65.62%] [G loss: 0.428002]\n",
      "epoch:26 step:24638 [D loss: 0.202555, acc.: 67.19%] [G loss: 0.448541]\n",
      "epoch:26 step:24639 [D loss: 0.226653, acc.: 61.72%] [G loss: 0.421864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24640 [D loss: 0.255116, acc.: 54.69%] [G loss: 0.427732]\n",
      "epoch:26 step:24641 [D loss: 0.221055, acc.: 66.41%] [G loss: 0.425627]\n",
      "epoch:26 step:24642 [D loss: 0.223096, acc.: 64.84%] [G loss: 0.389651]\n",
      "epoch:26 step:24643 [D loss: 0.260879, acc.: 57.81%] [G loss: 0.432550]\n",
      "epoch:26 step:24644 [D loss: 0.238653, acc.: 60.16%] [G loss: 0.421657]\n",
      "epoch:26 step:24645 [D loss: 0.206216, acc.: 64.84%] [G loss: 0.438744]\n",
      "epoch:26 step:24646 [D loss: 0.215212, acc.: 61.72%] [G loss: 0.461598]\n",
      "epoch:26 step:24647 [D loss: 0.238131, acc.: 60.16%] [G loss: 0.424026]\n",
      "epoch:26 step:24648 [D loss: 0.201959, acc.: 72.66%] [G loss: 0.426767]\n",
      "epoch:26 step:24649 [D loss: 0.262728, acc.: 50.78%] [G loss: 0.405884]\n",
      "epoch:26 step:24650 [D loss: 0.196493, acc.: 76.56%] [G loss: 0.497931]\n",
      "epoch:26 step:24651 [D loss: 0.211778, acc.: 67.19%] [G loss: 0.483911]\n",
      "epoch:26 step:24652 [D loss: 0.234939, acc.: 64.06%] [G loss: 0.477267]\n",
      "epoch:26 step:24653 [D loss: 0.256638, acc.: 55.47%] [G loss: 0.416671]\n",
      "epoch:26 step:24654 [D loss: 0.212712, acc.: 67.19%] [G loss: 0.393851]\n",
      "epoch:26 step:24655 [D loss: 0.197739, acc.: 71.09%] [G loss: 0.411930]\n",
      "epoch:26 step:24656 [D loss: 0.238773, acc.: 58.59%] [G loss: 0.393483]\n",
      "epoch:26 step:24657 [D loss: 0.218601, acc.: 64.84%] [G loss: 0.441233]\n",
      "epoch:26 step:24658 [D loss: 0.215583, acc.: 64.84%] [G loss: 0.413334]\n",
      "epoch:26 step:24659 [D loss: 0.232364, acc.: 61.72%] [G loss: 0.416744]\n",
      "epoch:26 step:24660 [D loss: 0.183424, acc.: 76.56%] [G loss: 0.403374]\n",
      "epoch:26 step:24661 [D loss: 0.217108, acc.: 67.19%] [G loss: 0.395977]\n",
      "epoch:26 step:24662 [D loss: 0.214446, acc.: 65.62%] [G loss: 0.457038]\n",
      "epoch:26 step:24663 [D loss: 0.251595, acc.: 55.47%] [G loss: 0.421512]\n",
      "epoch:26 step:24664 [D loss: 0.224641, acc.: 60.16%] [G loss: 0.426055]\n",
      "epoch:26 step:24665 [D loss: 0.227627, acc.: 63.28%] [G loss: 0.409445]\n",
      "epoch:26 step:24666 [D loss: 0.207932, acc.: 66.41%] [G loss: 0.436172]\n",
      "epoch:26 step:24667 [D loss: 0.224173, acc.: 61.72%] [G loss: 0.438715]\n",
      "epoch:26 step:24668 [D loss: 0.217827, acc.: 64.06%] [G loss: 0.442692]\n",
      "epoch:26 step:24669 [D loss: 0.192353, acc.: 74.22%] [G loss: 0.499105]\n",
      "epoch:26 step:24670 [D loss: 0.231637, acc.: 59.38%] [G loss: 0.437175]\n",
      "epoch:26 step:24671 [D loss: 0.216121, acc.: 66.41%] [G loss: 0.442193]\n",
      "epoch:26 step:24672 [D loss: 0.230096, acc.: 58.59%] [G loss: 0.429514]\n",
      "epoch:26 step:24673 [D loss: 0.184666, acc.: 78.12%] [G loss: 0.424952]\n",
      "epoch:26 step:24674 [D loss: 0.203439, acc.: 67.97%] [G loss: 0.441287]\n",
      "epoch:26 step:24675 [D loss: 0.198765, acc.: 72.66%] [G loss: 0.486054]\n",
      "epoch:26 step:24676 [D loss: 0.193307, acc.: 70.31%] [G loss: 0.518204]\n",
      "epoch:26 step:24677 [D loss: 0.193370, acc.: 71.88%] [G loss: 0.510208]\n",
      "epoch:26 step:24678 [D loss: 0.296276, acc.: 47.66%] [G loss: 0.418135]\n",
      "epoch:26 step:24679 [D loss: 0.249564, acc.: 53.91%] [G loss: 0.399526]\n",
      "epoch:26 step:24680 [D loss: 0.201678, acc.: 71.88%] [G loss: 0.428881]\n",
      "epoch:26 step:24681 [D loss: 0.240159, acc.: 57.03%] [G loss: 0.389898]\n",
      "epoch:26 step:24682 [D loss: 0.215299, acc.: 73.44%] [G loss: 0.421122]\n",
      "epoch:26 step:24683 [D loss: 0.184805, acc.: 75.00%] [G loss: 0.473049]\n",
      "epoch:26 step:24684 [D loss: 0.222032, acc.: 68.75%] [G loss: 0.457037]\n",
      "epoch:26 step:24685 [D loss: 0.242890, acc.: 58.59%] [G loss: 0.463354]\n",
      "epoch:26 step:24686 [D loss: 0.222457, acc.: 67.19%] [G loss: 0.418081]\n",
      "epoch:26 step:24687 [D loss: 0.230780, acc.: 58.59%] [G loss: 0.434440]\n",
      "epoch:26 step:24688 [D loss: 0.229986, acc.: 62.50%] [G loss: 0.400574]\n",
      "epoch:26 step:24689 [D loss: 0.221107, acc.: 61.72%] [G loss: 0.399805]\n",
      "epoch:26 step:24690 [D loss: 0.205998, acc.: 67.19%] [G loss: 0.423725]\n",
      "epoch:26 step:24691 [D loss: 0.228936, acc.: 60.94%] [G loss: 0.447217]\n",
      "epoch:26 step:24692 [D loss: 0.229849, acc.: 61.72%] [G loss: 0.438141]\n",
      "epoch:26 step:24693 [D loss: 0.223005, acc.: 60.94%] [G loss: 0.421561]\n",
      "epoch:26 step:24694 [D loss: 0.211840, acc.: 61.72%] [G loss: 0.446338]\n",
      "epoch:26 step:24695 [D loss: 0.238334, acc.: 61.72%] [G loss: 0.415516]\n",
      "epoch:26 step:24696 [D loss: 0.220620, acc.: 64.06%] [G loss: 0.422017]\n",
      "epoch:26 step:24697 [D loss: 0.197379, acc.: 68.75%] [G loss: 0.469691]\n",
      "epoch:26 step:24698 [D loss: 0.207615, acc.: 65.62%] [G loss: 0.465914]\n",
      "epoch:26 step:24699 [D loss: 0.221774, acc.: 64.06%] [G loss: 0.461868]\n",
      "epoch:26 step:24700 [D loss: 0.214944, acc.: 67.19%] [G loss: 0.438428]\n",
      "epoch:26 step:24701 [D loss: 0.208045, acc.: 67.19%] [G loss: 0.414252]\n",
      "epoch:26 step:24702 [D loss: 0.201637, acc.: 67.19%] [G loss: 0.452056]\n",
      "epoch:26 step:24703 [D loss: 0.271316, acc.: 52.34%] [G loss: 0.456773]\n",
      "epoch:26 step:24704 [D loss: 0.209818, acc.: 60.94%] [G loss: 0.497616]\n",
      "epoch:26 step:24705 [D loss: 0.224596, acc.: 61.72%] [G loss: 0.444661]\n",
      "epoch:26 step:24706 [D loss: 0.220332, acc.: 67.19%] [G loss: 0.473650]\n",
      "epoch:26 step:24707 [D loss: 0.232853, acc.: 57.81%] [G loss: 0.401817]\n",
      "epoch:26 step:24708 [D loss: 0.196008, acc.: 70.31%] [G loss: 0.453715]\n",
      "epoch:26 step:24709 [D loss: 0.170586, acc.: 75.00%] [G loss: 0.512726]\n",
      "epoch:26 step:24710 [D loss: 0.250897, acc.: 63.28%] [G loss: 0.431601]\n",
      "epoch:26 step:24711 [D loss: 0.299335, acc.: 45.31%] [G loss: 0.470527]\n",
      "epoch:26 step:24712 [D loss: 0.220485, acc.: 64.84%] [G loss: 0.401584]\n",
      "epoch:26 step:24713 [D loss: 0.239607, acc.: 57.81%] [G loss: 0.430014]\n",
      "epoch:26 step:24714 [D loss: 0.232442, acc.: 57.81%] [G loss: 0.430918]\n",
      "epoch:26 step:24715 [D loss: 0.212007, acc.: 69.53%] [G loss: 0.455379]\n",
      "epoch:26 step:24716 [D loss: 0.213548, acc.: 67.97%] [G loss: 0.441416]\n",
      "epoch:26 step:24717 [D loss: 0.240967, acc.: 58.59%] [G loss: 0.454055]\n",
      "epoch:26 step:24718 [D loss: 0.263708, acc.: 54.69%] [G loss: 0.416920]\n",
      "epoch:26 step:24719 [D loss: 0.215864, acc.: 60.16%] [G loss: 0.403595]\n",
      "epoch:26 step:24720 [D loss: 0.213927, acc.: 67.19%] [G loss: 0.421816]\n",
      "epoch:26 step:24721 [D loss: 0.188822, acc.: 78.12%] [G loss: 0.430368]\n",
      "epoch:26 step:24722 [D loss: 0.220946, acc.: 61.72%] [G loss: 0.386963]\n",
      "epoch:26 step:24723 [D loss: 0.198301, acc.: 67.97%] [G loss: 0.425586]\n",
      "epoch:26 step:24724 [D loss: 0.250579, acc.: 56.25%] [G loss: 0.420203]\n",
      "epoch:26 step:24725 [D loss: 0.243176, acc.: 58.59%] [G loss: 0.401095]\n",
      "epoch:26 step:24726 [D loss: 0.211901, acc.: 65.62%] [G loss: 0.452495]\n",
      "epoch:26 step:24727 [D loss: 0.221830, acc.: 62.50%] [G loss: 0.448539]\n",
      "epoch:26 step:24728 [D loss: 0.215647, acc.: 60.94%] [G loss: 0.446040]\n",
      "epoch:26 step:24729 [D loss: 0.224440, acc.: 58.59%] [G loss: 0.439934]\n",
      "epoch:26 step:24730 [D loss: 0.237914, acc.: 57.81%] [G loss: 0.429522]\n",
      "epoch:26 step:24731 [D loss: 0.230290, acc.: 57.81%] [G loss: 0.412817]\n",
      "epoch:26 step:24732 [D loss: 0.203366, acc.: 68.75%] [G loss: 0.428047]\n",
      "epoch:26 step:24733 [D loss: 0.180754, acc.: 71.09%] [G loss: 0.439056]\n",
      "epoch:26 step:24734 [D loss: 0.209294, acc.: 71.09%] [G loss: 0.467721]\n",
      "epoch:26 step:24735 [D loss: 0.235229, acc.: 61.72%] [G loss: 0.372478]\n",
      "epoch:26 step:24736 [D loss: 0.204819, acc.: 71.09%] [G loss: 0.440644]\n",
      "epoch:26 step:24737 [D loss: 0.225484, acc.: 60.94%] [G loss: 0.438389]\n",
      "epoch:26 step:24738 [D loss: 0.242143, acc.: 59.38%] [G loss: 0.399742]\n",
      "epoch:26 step:24739 [D loss: 0.249481, acc.: 55.47%] [G loss: 0.437878]\n",
      "epoch:26 step:24740 [D loss: 0.253521, acc.: 50.00%] [G loss: 0.418401]\n",
      "epoch:26 step:24741 [D loss: 0.227808, acc.: 60.94%] [G loss: 0.433064]\n",
      "epoch:26 step:24742 [D loss: 0.222360, acc.: 63.28%] [G loss: 0.410850]\n",
      "epoch:26 step:24743 [D loss: 0.212880, acc.: 62.50%] [G loss: 0.433149]\n",
      "epoch:26 step:24744 [D loss: 0.239502, acc.: 60.16%] [G loss: 0.391738]\n",
      "epoch:26 step:24745 [D loss: 0.217871, acc.: 64.06%] [G loss: 0.446406]\n",
      "epoch:26 step:24746 [D loss: 0.224931, acc.: 64.06%] [G loss: 0.449465]\n",
      "epoch:26 step:24747 [D loss: 0.193677, acc.: 71.88%] [G loss: 0.439156]\n",
      "epoch:26 step:24748 [D loss: 0.223546, acc.: 62.50%] [G loss: 0.452531]\n",
      "epoch:26 step:24749 [D loss: 0.239818, acc.: 57.03%] [G loss: 0.417443]\n",
      "epoch:26 step:24750 [D loss: 0.197862, acc.: 67.19%] [G loss: 0.420700]\n",
      "epoch:26 step:24751 [D loss: 0.213009, acc.: 64.84%] [G loss: 0.388110]\n",
      "epoch:26 step:24752 [D loss: 0.234529, acc.: 57.81%] [G loss: 0.405835]\n",
      "epoch:26 step:24753 [D loss: 0.204677, acc.: 69.53%] [G loss: 0.444513]\n",
      "epoch:26 step:24754 [D loss: 0.233609, acc.: 63.28%] [G loss: 0.430887]\n",
      "epoch:26 step:24755 [D loss: 0.216562, acc.: 63.28%] [G loss: 0.445558]\n",
      "epoch:26 step:24756 [D loss: 0.213647, acc.: 66.41%] [G loss: 0.455408]\n",
      "epoch:26 step:24757 [D loss: 0.221268, acc.: 58.59%] [G loss: 0.428393]\n",
      "epoch:26 step:24758 [D loss: 0.235008, acc.: 59.38%] [G loss: 0.413049]\n",
      "epoch:26 step:24759 [D loss: 0.212102, acc.: 68.75%] [G loss: 0.458375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24760 [D loss: 0.210264, acc.: 67.19%] [G loss: 0.455741]\n",
      "epoch:26 step:24761 [D loss: 0.200163, acc.: 66.41%] [G loss: 0.482639]\n",
      "epoch:26 step:24762 [D loss: 0.281640, acc.: 42.97%] [G loss: 0.400357]\n",
      "epoch:26 step:24763 [D loss: 0.222683, acc.: 61.72%] [G loss: 0.370151]\n",
      "epoch:26 step:24764 [D loss: 0.219155, acc.: 66.41%] [G loss: 0.411248]\n",
      "epoch:26 step:24765 [D loss: 0.214502, acc.: 65.62%] [G loss: 0.436824]\n",
      "epoch:26 step:24766 [D loss: 0.232342, acc.: 65.62%] [G loss: 0.445213]\n",
      "epoch:26 step:24767 [D loss: 0.208815, acc.: 63.28%] [G loss: 0.460395]\n",
      "epoch:26 step:24768 [D loss: 0.205541, acc.: 68.75%] [G loss: 0.460930]\n",
      "epoch:26 step:24769 [D loss: 0.218625, acc.: 63.28%] [G loss: 0.455810]\n",
      "epoch:26 step:24770 [D loss: 0.275254, acc.: 50.00%] [G loss: 0.430402]\n",
      "epoch:26 step:24771 [D loss: 0.210117, acc.: 66.41%] [G loss: 0.491397]\n",
      "epoch:26 step:24772 [D loss: 0.255570, acc.: 55.47%] [G loss: 0.395533]\n",
      "epoch:26 step:24773 [D loss: 0.227961, acc.: 58.59%] [G loss: 0.402990]\n",
      "epoch:26 step:24774 [D loss: 0.242673, acc.: 57.81%] [G loss: 0.427087]\n",
      "epoch:26 step:24775 [D loss: 0.220394, acc.: 64.84%] [G loss: 0.426687]\n",
      "epoch:26 step:24776 [D loss: 0.210775, acc.: 63.28%] [G loss: 0.420665]\n",
      "epoch:26 step:24777 [D loss: 0.238956, acc.: 54.69%] [G loss: 0.426393]\n",
      "epoch:26 step:24778 [D loss: 0.204666, acc.: 67.19%] [G loss: 0.501782]\n",
      "epoch:26 step:24779 [D loss: 0.229027, acc.: 71.09%] [G loss: 0.456295]\n",
      "epoch:26 step:24780 [D loss: 0.237469, acc.: 60.94%] [G loss: 0.436416]\n",
      "epoch:26 step:24781 [D loss: 0.260108, acc.: 57.03%] [G loss: 0.396159]\n",
      "epoch:26 step:24782 [D loss: 0.221755, acc.: 62.50%] [G loss: 0.437248]\n",
      "epoch:26 step:24783 [D loss: 0.268930, acc.: 51.56%] [G loss: 0.425025]\n",
      "epoch:26 step:24784 [D loss: 0.251571, acc.: 53.91%] [G loss: 0.378775]\n",
      "epoch:26 step:24785 [D loss: 0.227892, acc.: 64.06%] [G loss: 0.443843]\n",
      "epoch:26 step:24786 [D loss: 0.236611, acc.: 57.03%] [G loss: 0.437444]\n",
      "epoch:26 step:24787 [D loss: 0.232997, acc.: 62.50%] [G loss: 0.433356]\n",
      "epoch:26 step:24788 [D loss: 0.199533, acc.: 73.44%] [G loss: 0.424847]\n",
      "epoch:26 step:24789 [D loss: 0.233452, acc.: 60.94%] [G loss: 0.424136]\n",
      "epoch:26 step:24790 [D loss: 0.209335, acc.: 64.06%] [G loss: 0.425968]\n",
      "epoch:26 step:24791 [D loss: 0.216011, acc.: 66.41%] [G loss: 0.424841]\n",
      "epoch:26 step:24792 [D loss: 0.187610, acc.: 73.44%] [G loss: 0.460854]\n",
      "epoch:26 step:24793 [D loss: 0.240879, acc.: 63.28%] [G loss: 0.450978]\n",
      "epoch:26 step:24794 [D loss: 0.243887, acc.: 57.03%] [G loss: 0.452015]\n",
      "epoch:26 step:24795 [D loss: 0.230380, acc.: 56.25%] [G loss: 0.413122]\n",
      "epoch:26 step:24796 [D loss: 0.203243, acc.: 70.31%] [G loss: 0.383440]\n",
      "epoch:26 step:24797 [D loss: 0.204329, acc.: 64.84%] [G loss: 0.448469]\n",
      "epoch:26 step:24798 [D loss: 0.185724, acc.: 74.22%] [G loss: 0.452314]\n",
      "epoch:26 step:24799 [D loss: 0.277280, acc.: 48.44%] [G loss: 0.458686]\n",
      "epoch:26 step:24800 [D loss: 0.243890, acc.: 53.12%] [G loss: 0.406133]\n",
      "##############\n",
      "[2.83565789 1.78727854 6.19293968 4.87072006 3.89359974 5.56933136\n",
      " 4.66918447 4.75212927 4.40177015 4.0960542 ]\n",
      "##########\n",
      "epoch:26 step:24801 [D loss: 0.235011, acc.: 57.81%] [G loss: 0.414577]\n",
      "epoch:26 step:24802 [D loss: 0.196619, acc.: 70.31%] [G loss: 0.479157]\n",
      "epoch:26 step:24803 [D loss: 0.206096, acc.: 70.31%] [G loss: 0.440675]\n",
      "epoch:26 step:24804 [D loss: 0.227579, acc.: 59.38%] [G loss: 0.478000]\n",
      "epoch:26 step:24805 [D loss: 0.237442, acc.: 54.69%] [G loss: 0.421436]\n",
      "epoch:26 step:24806 [D loss: 0.221116, acc.: 64.06%] [G loss: 0.414715]\n",
      "epoch:26 step:24807 [D loss: 0.215466, acc.: 68.75%] [G loss: 0.435573]\n",
      "epoch:26 step:24808 [D loss: 0.215230, acc.: 66.41%] [G loss: 0.439575]\n",
      "epoch:26 step:24809 [D loss: 0.222880, acc.: 64.06%] [G loss: 0.428213]\n",
      "epoch:26 step:24810 [D loss: 0.247852, acc.: 53.91%] [G loss: 0.388234]\n",
      "epoch:26 step:24811 [D loss: 0.217045, acc.: 59.38%] [G loss: 0.510284]\n",
      "epoch:26 step:24812 [D loss: 0.196145, acc.: 71.09%] [G loss: 0.419737]\n",
      "epoch:26 step:24813 [D loss: 0.193100, acc.: 71.88%] [G loss: 0.461867]\n",
      "epoch:26 step:24814 [D loss: 0.232603, acc.: 66.41%] [G loss: 0.463476]\n",
      "epoch:26 step:24815 [D loss: 0.198295, acc.: 70.31%] [G loss: 0.480880]\n",
      "epoch:26 step:24816 [D loss: 0.206726, acc.: 69.53%] [G loss: 0.463462]\n",
      "epoch:26 step:24817 [D loss: 0.246305, acc.: 57.81%] [G loss: 0.390924]\n",
      "epoch:26 step:24818 [D loss: 0.210857, acc.: 66.41%] [G loss: 0.421113]\n",
      "epoch:26 step:24819 [D loss: 0.209623, acc.: 64.06%] [G loss: 0.490990]\n",
      "epoch:26 step:24820 [D loss: 0.274087, acc.: 42.97%] [G loss: 0.390135]\n",
      "epoch:26 step:24821 [D loss: 0.223689, acc.: 63.28%] [G loss: 0.413146]\n",
      "epoch:26 step:24822 [D loss: 0.234888, acc.: 60.16%] [G loss: 0.409644]\n",
      "epoch:26 step:24823 [D loss: 0.231124, acc.: 64.06%] [G loss: 0.415181]\n",
      "epoch:26 step:24824 [D loss: 0.240110, acc.: 56.25%] [G loss: 0.405938]\n",
      "epoch:26 step:24825 [D loss: 0.242451, acc.: 53.12%] [G loss: 0.409049]\n",
      "epoch:26 step:24826 [D loss: 0.220166, acc.: 61.72%] [G loss: 0.402640]\n",
      "epoch:26 step:24827 [D loss: 0.234821, acc.: 59.38%] [G loss: 0.404325]\n",
      "epoch:26 step:24828 [D loss: 0.213336, acc.: 67.19%] [G loss: 0.411280]\n",
      "epoch:26 step:24829 [D loss: 0.202088, acc.: 64.06%] [G loss: 0.469002]\n",
      "epoch:26 step:24830 [D loss: 0.231472, acc.: 63.28%] [G loss: 0.433162]\n",
      "epoch:26 step:24831 [D loss: 0.201719, acc.: 67.19%] [G loss: 0.443407]\n",
      "epoch:26 step:24832 [D loss: 0.224782, acc.: 64.06%] [G loss: 0.451575]\n",
      "epoch:26 step:24833 [D loss: 0.188939, acc.: 74.22%] [G loss: 0.451019]\n",
      "epoch:26 step:24834 [D loss: 0.198475, acc.: 67.97%] [G loss: 0.466254]\n",
      "epoch:26 step:24835 [D loss: 0.264595, acc.: 48.44%] [G loss: 0.394986]\n",
      "epoch:26 step:24836 [D loss: 0.206336, acc.: 64.84%] [G loss: 0.420286]\n",
      "epoch:26 step:24837 [D loss: 0.238725, acc.: 58.59%] [G loss: 0.418169]\n",
      "epoch:26 step:24838 [D loss: 0.217675, acc.: 63.28%] [G loss: 0.458272]\n",
      "epoch:26 step:24839 [D loss: 0.259572, acc.: 44.53%] [G loss: 0.411652]\n",
      "epoch:26 step:24840 [D loss: 0.256091, acc.: 54.69%] [G loss: 0.396151]\n",
      "epoch:26 step:24841 [D loss: 0.223247, acc.: 64.06%] [G loss: 0.401594]\n",
      "epoch:26 step:24842 [D loss: 0.206484, acc.: 70.31%] [G loss: 0.433867]\n",
      "epoch:26 step:24843 [D loss: 0.180362, acc.: 72.66%] [G loss: 0.494058]\n",
      "epoch:26 step:24844 [D loss: 0.261137, acc.: 55.47%] [G loss: 0.409538]\n",
      "epoch:26 step:24845 [D loss: 0.216666, acc.: 64.06%] [G loss: 0.429806]\n",
      "epoch:26 step:24846 [D loss: 0.207666, acc.: 66.41%] [G loss: 0.419233]\n",
      "epoch:26 step:24847 [D loss: 0.229428, acc.: 64.06%] [G loss: 0.449865]\n",
      "epoch:26 step:24848 [D loss: 0.237838, acc.: 57.81%] [G loss: 0.452847]\n",
      "epoch:26 step:24849 [D loss: 0.239569, acc.: 57.81%] [G loss: 0.442431]\n",
      "epoch:26 step:24850 [D loss: 0.208831, acc.: 66.41%] [G loss: 0.415721]\n",
      "epoch:26 step:24851 [D loss: 0.225527, acc.: 61.72%] [G loss: 0.414001]\n",
      "epoch:26 step:24852 [D loss: 0.236841, acc.: 58.59%] [G loss: 0.392727]\n",
      "epoch:26 step:24853 [D loss: 0.230001, acc.: 58.59%] [G loss: 0.400358]\n",
      "epoch:26 step:24854 [D loss: 0.230213, acc.: 61.72%] [G loss: 0.416444]\n",
      "epoch:26 step:24855 [D loss: 0.216426, acc.: 62.50%] [G loss: 0.440186]\n",
      "epoch:26 step:24856 [D loss: 0.198147, acc.: 71.09%] [G loss: 0.452108]\n",
      "epoch:26 step:24857 [D loss: 0.211197, acc.: 67.19%] [G loss: 0.455172]\n",
      "epoch:26 step:24858 [D loss: 0.214522, acc.: 64.84%] [G loss: 0.446176]\n",
      "epoch:26 step:24859 [D loss: 0.243669, acc.: 52.34%] [G loss: 0.419371]\n",
      "epoch:26 step:24860 [D loss: 0.216023, acc.: 61.72%] [G loss: 0.445636]\n",
      "epoch:26 step:24861 [D loss: 0.187148, acc.: 75.00%] [G loss: 0.467664]\n",
      "epoch:26 step:24862 [D loss: 0.241531, acc.: 65.62%] [G loss: 0.444784]\n",
      "epoch:26 step:24863 [D loss: 0.294087, acc.: 50.00%] [G loss: 0.420865]\n",
      "epoch:26 step:24864 [D loss: 0.234051, acc.: 60.16%] [G loss: 0.411530]\n",
      "epoch:26 step:24865 [D loss: 0.240029, acc.: 60.16%] [G loss: 0.417718]\n",
      "epoch:26 step:24866 [D loss: 0.217024, acc.: 62.50%] [G loss: 0.433554]\n",
      "epoch:26 step:24867 [D loss: 0.215396, acc.: 67.97%] [G loss: 0.451958]\n",
      "epoch:26 step:24868 [D loss: 0.214376, acc.: 60.16%] [G loss: 0.433281]\n",
      "epoch:26 step:24869 [D loss: 0.196708, acc.: 67.97%] [G loss: 0.540912]\n",
      "epoch:26 step:24870 [D loss: 0.188527, acc.: 69.53%] [G loss: 0.468971]\n",
      "epoch:26 step:24871 [D loss: 0.262870, acc.: 56.25%] [G loss: 0.457110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24872 [D loss: 0.222591, acc.: 65.62%] [G loss: 0.426253]\n",
      "epoch:26 step:24873 [D loss: 0.237767, acc.: 57.03%] [G loss: 0.397310]\n",
      "epoch:26 step:24874 [D loss: 0.208433, acc.: 65.62%] [G loss: 0.423296]\n",
      "epoch:26 step:24875 [D loss: 0.191852, acc.: 75.00%] [G loss: 0.461841]\n",
      "epoch:26 step:24876 [D loss: 0.218282, acc.: 68.75%] [G loss: 0.447834]\n",
      "epoch:26 step:24877 [D loss: 0.202698, acc.: 70.31%] [G loss: 0.442445]\n",
      "epoch:26 step:24878 [D loss: 0.197002, acc.: 71.88%] [G loss: 0.488804]\n",
      "epoch:26 step:24879 [D loss: 0.245914, acc.: 57.81%] [G loss: 0.379826]\n",
      "epoch:26 step:24880 [D loss: 0.238182, acc.: 61.72%] [G loss: 0.436856]\n",
      "epoch:26 step:24881 [D loss: 0.207339, acc.: 70.31%] [G loss: 0.402942]\n",
      "epoch:26 step:24882 [D loss: 0.217995, acc.: 63.28%] [G loss: 0.431164]\n",
      "epoch:26 step:24883 [D loss: 0.215368, acc.: 66.41%] [G loss: 0.432671]\n",
      "epoch:26 step:24884 [D loss: 0.201745, acc.: 69.53%] [G loss: 0.464859]\n",
      "epoch:26 step:24885 [D loss: 0.202964, acc.: 65.62%] [G loss: 0.436735]\n",
      "epoch:26 step:24886 [D loss: 0.243829, acc.: 59.38%] [G loss: 0.398320]\n",
      "epoch:26 step:24887 [D loss: 0.222447, acc.: 64.06%] [G loss: 0.385074]\n",
      "epoch:26 step:24888 [D loss: 0.224988, acc.: 65.62%] [G loss: 0.427522]\n",
      "epoch:26 step:24889 [D loss: 0.231754, acc.: 60.16%] [G loss: 0.453469]\n",
      "epoch:26 step:24890 [D loss: 0.251534, acc.: 56.25%] [G loss: 0.467411]\n",
      "epoch:26 step:24891 [D loss: 0.269762, acc.: 46.09%] [G loss: 0.418397]\n",
      "epoch:26 step:24892 [D loss: 0.230926, acc.: 60.16%] [G loss: 0.449928]\n",
      "epoch:26 step:24893 [D loss: 0.246662, acc.: 61.72%] [G loss: 0.413336]\n",
      "epoch:26 step:24894 [D loss: 0.247898, acc.: 57.03%] [G loss: 0.381688]\n",
      "epoch:26 step:24895 [D loss: 0.224921, acc.: 60.16%] [G loss: 0.418305]\n",
      "epoch:26 step:24896 [D loss: 0.209739, acc.: 64.06%] [G loss: 0.472457]\n",
      "epoch:26 step:24897 [D loss: 0.247066, acc.: 53.91%] [G loss: 0.418261]\n",
      "epoch:26 step:24898 [D loss: 0.224182, acc.: 61.72%] [G loss: 0.429929]\n",
      "epoch:26 step:24899 [D loss: 0.224078, acc.: 67.97%] [G loss: 0.404930]\n",
      "epoch:26 step:24900 [D loss: 0.237482, acc.: 60.16%] [G loss: 0.403773]\n",
      "epoch:26 step:24901 [D loss: 0.205309, acc.: 67.97%] [G loss: 0.440926]\n",
      "epoch:26 step:24902 [D loss: 0.238230, acc.: 51.56%] [G loss: 0.397328]\n",
      "epoch:26 step:24903 [D loss: 0.230702, acc.: 59.38%] [G loss: 0.390918]\n",
      "epoch:26 step:24904 [D loss: 0.253830, acc.: 56.25%] [G loss: 0.405182]\n",
      "epoch:26 step:24905 [D loss: 0.229421, acc.: 57.81%] [G loss: 0.442825]\n",
      "epoch:26 step:24906 [D loss: 0.241717, acc.: 59.38%] [G loss: 0.416732]\n",
      "epoch:26 step:24907 [D loss: 0.211640, acc.: 67.97%] [G loss: 0.422533]\n",
      "epoch:26 step:24908 [D loss: 0.201983, acc.: 73.44%] [G loss: 0.439215]\n",
      "epoch:26 step:24909 [D loss: 0.210964, acc.: 69.53%] [G loss: 0.427592]\n",
      "epoch:26 step:24910 [D loss: 0.209985, acc.: 64.84%] [G loss: 0.424128]\n",
      "epoch:26 step:24911 [D loss: 0.201897, acc.: 66.41%] [G loss: 0.445221]\n",
      "epoch:26 step:24912 [D loss: 0.217411, acc.: 64.84%] [G loss: 0.423048]\n",
      "epoch:26 step:24913 [D loss: 0.198701, acc.: 68.75%] [G loss: 0.438817]\n",
      "epoch:26 step:24914 [D loss: 0.207758, acc.: 65.62%] [G loss: 0.418440]\n",
      "epoch:26 step:24915 [D loss: 0.253595, acc.: 55.47%] [G loss: 0.404410]\n",
      "epoch:26 step:24916 [D loss: 0.218456, acc.: 64.06%] [G loss: 0.387448]\n",
      "epoch:26 step:24917 [D loss: 0.210307, acc.: 67.19%] [G loss: 0.465828]\n",
      "epoch:26 step:24918 [D loss: 0.224308, acc.: 65.62%] [G loss: 0.423398]\n",
      "epoch:26 step:24919 [D loss: 0.185089, acc.: 73.44%] [G loss: 0.456115]\n",
      "epoch:26 step:24920 [D loss: 0.212456, acc.: 65.62%] [G loss: 0.428910]\n",
      "epoch:26 step:24921 [D loss: 0.248681, acc.: 50.78%] [G loss: 0.405150]\n",
      "epoch:26 step:24922 [D loss: 0.226248, acc.: 63.28%] [G loss: 0.427654]\n",
      "epoch:26 step:24923 [D loss: 0.208090, acc.: 73.44%] [G loss: 0.480335]\n",
      "epoch:26 step:24924 [D loss: 0.226902, acc.: 63.28%] [G loss: 0.386039]\n",
      "epoch:26 step:24925 [D loss: 0.195458, acc.: 74.22%] [G loss: 0.434341]\n",
      "epoch:26 step:24926 [D loss: 0.179084, acc.: 71.09%] [G loss: 0.453363]\n",
      "epoch:26 step:24927 [D loss: 0.254535, acc.: 56.25%] [G loss: 0.432146]\n",
      "epoch:26 step:24928 [D loss: 0.248564, acc.: 56.25%] [G loss: 0.441290]\n",
      "epoch:26 step:24929 [D loss: 0.207535, acc.: 71.88%] [G loss: 0.441414]\n",
      "epoch:26 step:24930 [D loss: 0.215671, acc.: 72.66%] [G loss: 0.390768]\n",
      "epoch:26 step:24931 [D loss: 0.251994, acc.: 56.25%] [G loss: 0.395204]\n",
      "epoch:26 step:24932 [D loss: 0.218403, acc.: 64.84%] [G loss: 0.387824]\n",
      "epoch:26 step:24933 [D loss: 0.232187, acc.: 57.03%] [G loss: 0.383960]\n",
      "epoch:26 step:24934 [D loss: 0.210793, acc.: 65.62%] [G loss: 0.410600]\n",
      "epoch:26 step:24935 [D loss: 0.211641, acc.: 65.62%] [G loss: 0.428825]\n",
      "epoch:26 step:24936 [D loss: 0.183609, acc.: 75.78%] [G loss: 0.446289]\n",
      "epoch:26 step:24937 [D loss: 0.208705, acc.: 67.19%] [G loss: 0.473184]\n",
      "epoch:26 step:24938 [D loss: 0.240675, acc.: 58.59%] [G loss: 0.417086]\n",
      "epoch:26 step:24939 [D loss: 0.224906, acc.: 65.62%] [G loss: 0.448642]\n",
      "epoch:26 step:24940 [D loss: 0.237945, acc.: 53.91%] [G loss: 0.424829]\n",
      "epoch:26 step:24941 [D loss: 0.246374, acc.: 54.69%] [G loss: 0.432940]\n",
      "epoch:26 step:24942 [D loss: 0.233157, acc.: 57.81%] [G loss: 0.402144]\n",
      "epoch:26 step:24943 [D loss: 0.213165, acc.: 66.41%] [G loss: 0.393194]\n",
      "epoch:26 step:24944 [D loss: 0.208943, acc.: 69.53%] [G loss: 0.427072]\n",
      "epoch:26 step:24945 [D loss: 0.222252, acc.: 64.06%] [G loss: 0.455159]\n",
      "epoch:26 step:24946 [D loss: 0.229919, acc.: 67.19%] [G loss: 0.402134]\n",
      "epoch:26 step:24947 [D loss: 0.255763, acc.: 57.03%] [G loss: 0.413559]\n",
      "epoch:26 step:24948 [D loss: 0.234176, acc.: 60.16%] [G loss: 0.417337]\n",
      "epoch:26 step:24949 [D loss: 0.245292, acc.: 54.69%] [G loss: 0.404461]\n",
      "epoch:26 step:24950 [D loss: 0.229502, acc.: 57.81%] [G loss: 0.383690]\n",
      "epoch:26 step:24951 [D loss: 0.199055, acc.: 70.31%] [G loss: 0.418763]\n",
      "epoch:26 step:24952 [D loss: 0.241785, acc.: 57.81%] [G loss: 0.440604]\n",
      "epoch:26 step:24953 [D loss: 0.226825, acc.: 67.97%] [G loss: 0.434939]\n",
      "epoch:26 step:24954 [D loss: 0.201217, acc.: 70.31%] [G loss: 0.452563]\n",
      "epoch:26 step:24955 [D loss: 0.202671, acc.: 71.88%] [G loss: 0.445076]\n",
      "epoch:26 step:24956 [D loss: 0.222983, acc.: 59.38%] [G loss: 0.427425]\n",
      "epoch:26 step:24957 [D loss: 0.214569, acc.: 67.97%] [G loss: 0.388173]\n",
      "epoch:26 step:24958 [D loss: 0.237718, acc.: 64.06%] [G loss: 0.402110]\n",
      "epoch:26 step:24959 [D loss: 0.238356, acc.: 60.94%] [G loss: 0.414087]\n",
      "epoch:26 step:24960 [D loss: 0.216815, acc.: 64.06%] [G loss: 0.451114]\n",
      "epoch:26 step:24961 [D loss: 0.236117, acc.: 59.38%] [G loss: 0.432674]\n",
      "epoch:26 step:24962 [D loss: 0.243670, acc.: 62.50%] [G loss: 0.442741]\n",
      "epoch:26 step:24963 [D loss: 0.221221, acc.: 64.84%] [G loss: 0.429514]\n",
      "epoch:26 step:24964 [D loss: 0.234505, acc.: 61.72%] [G loss: 0.436362]\n",
      "epoch:26 step:24965 [D loss: 0.226004, acc.: 64.84%] [G loss: 0.406851]\n",
      "epoch:26 step:24966 [D loss: 0.195002, acc.: 68.75%] [G loss: 0.453135]\n",
      "epoch:26 step:24967 [D loss: 0.208830, acc.: 68.75%] [G loss: 0.406734]\n",
      "epoch:26 step:24968 [D loss: 0.244544, acc.: 57.81%] [G loss: 0.420809]\n",
      "epoch:26 step:24969 [D loss: 0.211093, acc.: 64.06%] [G loss: 0.461596]\n",
      "epoch:26 step:24970 [D loss: 0.219109, acc.: 65.62%] [G loss: 0.422943]\n",
      "epoch:26 step:24971 [D loss: 0.229361, acc.: 63.28%] [G loss: 0.424579]\n",
      "epoch:26 step:24972 [D loss: 0.232338, acc.: 60.16%] [G loss: 0.425297]\n",
      "epoch:26 step:24973 [D loss: 0.229318, acc.: 61.72%] [G loss: 0.454502]\n",
      "epoch:26 step:24974 [D loss: 0.207988, acc.: 64.84%] [G loss: 0.453969]\n",
      "epoch:26 step:24975 [D loss: 0.213426, acc.: 62.50%] [G loss: 0.419298]\n",
      "epoch:26 step:24976 [D loss: 0.252763, acc.: 53.91%] [G loss: 0.423625]\n",
      "epoch:26 step:24977 [D loss: 0.242269, acc.: 57.03%] [G loss: 0.393384]\n",
      "epoch:26 step:24978 [D loss: 0.221903, acc.: 62.50%] [G loss: 0.384125]\n",
      "epoch:26 step:24979 [D loss: 0.223392, acc.: 64.06%] [G loss: 0.405254]\n",
      "epoch:26 step:24980 [D loss: 0.228371, acc.: 58.59%] [G loss: 0.414410]\n",
      "epoch:26 step:24981 [D loss: 0.231880, acc.: 59.38%] [G loss: 0.425969]\n",
      "epoch:26 step:24982 [D loss: 0.201231, acc.: 69.53%] [G loss: 0.467270]\n",
      "epoch:26 step:24983 [D loss: 0.235382, acc.: 58.59%] [G loss: 0.409498]\n",
      "epoch:26 step:24984 [D loss: 0.230313, acc.: 60.16%] [G loss: 0.393869]\n",
      "epoch:26 step:24985 [D loss: 0.225294, acc.: 62.50%] [G loss: 0.388708]\n",
      "epoch:26 step:24986 [D loss: 0.194318, acc.: 71.88%] [G loss: 0.459342]\n",
      "epoch:26 step:24987 [D loss: 0.211799, acc.: 66.41%] [G loss: 0.457269]\n",
      "epoch:26 step:24988 [D loss: 0.251493, acc.: 60.16%] [G loss: 0.407029]\n",
      "epoch:26 step:24989 [D loss: 0.213815, acc.: 66.41%] [G loss: 0.446522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24990 [D loss: 0.229226, acc.: 63.28%] [G loss: 0.445659]\n",
      "epoch:26 step:24991 [D loss: 0.210868, acc.: 70.31%] [G loss: 0.411630]\n",
      "epoch:26 step:24992 [D loss: 0.202868, acc.: 71.09%] [G loss: 0.423433]\n",
      "epoch:26 step:24993 [D loss: 0.217725, acc.: 67.19%] [G loss: 0.404986]\n",
      "epoch:26 step:24994 [D loss: 0.214032, acc.: 62.50%] [G loss: 0.444678]\n",
      "epoch:26 step:24995 [D loss: 0.205795, acc.: 67.97%] [G loss: 0.465862]\n",
      "epoch:26 step:24996 [D loss: 0.203583, acc.: 70.31%] [G loss: 0.424078]\n",
      "epoch:26 step:24997 [D loss: 0.207129, acc.: 70.31%] [G loss: 0.478501]\n",
      "epoch:26 step:24998 [D loss: 0.229859, acc.: 57.81%] [G loss: 0.421574]\n",
      "epoch:26 step:24999 [D loss: 0.221971, acc.: 63.28%] [G loss: 0.426250]\n",
      "epoch:26 step:25000 [D loss: 0.198506, acc.: 67.97%] [G loss: 0.445805]\n",
      "##############\n",
      "[2.51452464 2.01498866 5.9292148  4.83970448 3.7125703  5.46523002\n",
      " 4.5045818  4.74911789 4.53492692 4.2028455 ]\n",
      "##########\n",
      "epoch:26 step:25001 [D loss: 0.232407, acc.: 59.38%] [G loss: 0.421744]\n",
      "epoch:26 step:25002 [D loss: 0.240811, acc.: 57.03%] [G loss: 0.441556]\n",
      "epoch:26 step:25003 [D loss: 0.195056, acc.: 70.31%] [G loss: 0.493850]\n",
      "epoch:26 step:25004 [D loss: 0.179619, acc.: 77.34%] [G loss: 0.477707]\n",
      "epoch:26 step:25005 [D loss: 0.218345, acc.: 64.06%] [G loss: 0.430835]\n",
      "epoch:26 step:25006 [D loss: 0.239143, acc.: 58.59%] [G loss: 0.435422]\n",
      "epoch:26 step:25007 [D loss: 0.204870, acc.: 69.53%] [G loss: 0.433674]\n",
      "epoch:26 step:25008 [D loss: 0.199428, acc.: 72.66%] [G loss: 0.426651]\n",
      "epoch:26 step:25009 [D loss: 0.209071, acc.: 69.53%] [G loss: 0.461255]\n",
      "epoch:26 step:25010 [D loss: 0.177359, acc.: 75.00%] [G loss: 0.501582]\n",
      "epoch:26 step:25011 [D loss: 0.216161, acc.: 66.41%] [G loss: 0.439878]\n",
      "epoch:26 step:25012 [D loss: 0.197422, acc.: 67.19%] [G loss: 0.493461]\n",
      "epoch:26 step:25013 [D loss: 0.231464, acc.: 60.16%] [G loss: 0.452080]\n",
      "epoch:26 step:25014 [D loss: 0.230269, acc.: 65.62%] [G loss: 0.438923]\n",
      "epoch:26 step:25015 [D loss: 0.206934, acc.: 64.06%] [G loss: 0.476971]\n",
      "epoch:26 step:25016 [D loss: 0.238167, acc.: 52.34%] [G loss: 0.453078]\n",
      "epoch:26 step:25017 [D loss: 0.240832, acc.: 59.38%] [G loss: 0.426097]\n",
      "epoch:26 step:25018 [D loss: 0.238860, acc.: 53.12%] [G loss: 0.425528]\n",
      "epoch:26 step:25019 [D loss: 0.228765, acc.: 59.38%] [G loss: 0.431554]\n",
      "epoch:26 step:25020 [D loss: 0.241422, acc.: 55.47%] [G loss: 0.426385]\n",
      "epoch:26 step:25021 [D loss: 0.214102, acc.: 66.41%] [G loss: 0.422371]\n",
      "epoch:26 step:25022 [D loss: 0.211226, acc.: 64.84%] [G loss: 0.402113]\n",
      "epoch:26 step:25023 [D loss: 0.205303, acc.: 68.75%] [G loss: 0.440138]\n",
      "epoch:26 step:25024 [D loss: 0.239748, acc.: 58.59%] [G loss: 0.409110]\n",
      "epoch:26 step:25025 [D loss: 0.246790, acc.: 57.81%] [G loss: 0.423900]\n",
      "epoch:26 step:25026 [D loss: 0.214584, acc.: 64.84%] [G loss: 0.441850]\n",
      "epoch:26 step:25027 [D loss: 0.234918, acc.: 60.94%] [G loss: 0.429172]\n",
      "epoch:26 step:25028 [D loss: 0.177723, acc.: 72.66%] [G loss: 0.461165]\n",
      "epoch:26 step:25029 [D loss: 0.258612, acc.: 55.47%] [G loss: 0.367541]\n",
      "epoch:26 step:25030 [D loss: 0.225396, acc.: 62.50%] [G loss: 0.414219]\n",
      "epoch:26 step:25031 [D loss: 0.224535, acc.: 64.06%] [G loss: 0.404422]\n",
      "epoch:26 step:25032 [D loss: 0.236025, acc.: 60.94%] [G loss: 0.394290]\n",
      "epoch:26 step:25033 [D loss: 0.236385, acc.: 56.25%] [G loss: 0.403786]\n",
      "epoch:26 step:25034 [D loss: 0.231631, acc.: 59.38%] [G loss: 0.407397]\n",
      "epoch:26 step:25035 [D loss: 0.226881, acc.: 60.16%] [G loss: 0.416403]\n",
      "epoch:26 step:25036 [D loss: 0.221517, acc.: 67.19%] [G loss: 0.466192]\n",
      "epoch:26 step:25037 [D loss: 0.253201, acc.: 54.69%] [G loss: 0.443610]\n",
      "epoch:26 step:25038 [D loss: 0.228381, acc.: 60.94%] [G loss: 0.406539]\n",
      "epoch:26 step:25039 [D loss: 0.201762, acc.: 67.97%] [G loss: 0.438538]\n",
      "epoch:26 step:25040 [D loss: 0.224989, acc.: 62.50%] [G loss: 0.421002]\n",
      "epoch:26 step:25041 [D loss: 0.210667, acc.: 71.09%] [G loss: 0.412419]\n",
      "epoch:26 step:25042 [D loss: 0.219888, acc.: 60.16%] [G loss: 0.420633]\n",
      "epoch:26 step:25043 [D loss: 0.210365, acc.: 68.75%] [G loss: 0.435318]\n",
      "epoch:26 step:25044 [D loss: 0.236566, acc.: 60.16%] [G loss: 0.468192]\n",
      "epoch:26 step:25045 [D loss: 0.248180, acc.: 53.91%] [G loss: 0.369028]\n",
      "epoch:26 step:25046 [D loss: 0.239017, acc.: 58.59%] [G loss: 0.379891]\n",
      "epoch:26 step:25047 [D loss: 0.218278, acc.: 63.28%] [G loss: 0.444332]\n",
      "epoch:26 step:25048 [D loss: 0.220855, acc.: 63.28%] [G loss: 0.451641]\n",
      "epoch:26 step:25049 [D loss: 0.224226, acc.: 58.59%] [G loss: 0.434581]\n",
      "epoch:26 step:25050 [D loss: 0.196551, acc.: 67.19%] [G loss: 0.441048]\n",
      "epoch:26 step:25051 [D loss: 0.231476, acc.: 58.59%] [G loss: 0.432361]\n",
      "epoch:26 step:25052 [D loss: 0.208989, acc.: 63.28%] [G loss: 0.442841]\n",
      "epoch:26 step:25053 [D loss: 0.232462, acc.: 63.28%] [G loss: 0.417016]\n",
      "epoch:26 step:25054 [D loss: 0.213604, acc.: 68.75%] [G loss: 0.485869]\n",
      "epoch:26 step:25055 [D loss: 0.191336, acc.: 68.75%] [G loss: 0.484902]\n",
      "epoch:26 step:25056 [D loss: 0.211557, acc.: 65.62%] [G loss: 0.448824]\n",
      "epoch:26 step:25057 [D loss: 0.191803, acc.: 71.88%] [G loss: 0.482000]\n",
      "epoch:26 step:25058 [D loss: 0.245298, acc.: 53.12%] [G loss: 0.454598]\n",
      "epoch:26 step:25059 [D loss: 0.219966, acc.: 63.28%] [G loss: 0.423037]\n",
      "epoch:26 step:25060 [D loss: 0.219012, acc.: 67.19%] [G loss: 0.417534]\n",
      "epoch:26 step:25061 [D loss: 0.207152, acc.: 65.62%] [G loss: 0.429894]\n",
      "epoch:26 step:25062 [D loss: 0.208516, acc.: 64.84%] [G loss: 0.455059]\n",
      "epoch:26 step:25063 [D loss: 0.212118, acc.: 66.41%] [G loss: 0.496450]\n",
      "epoch:26 step:25064 [D loss: 0.255820, acc.: 54.69%] [G loss: 0.451323]\n",
      "epoch:26 step:25065 [D loss: 0.240685, acc.: 60.16%] [G loss: 0.426109]\n",
      "epoch:26 step:25066 [D loss: 0.277013, acc.: 50.00%] [G loss: 0.389422]\n",
      "epoch:26 step:25067 [D loss: 0.215592, acc.: 64.84%] [G loss: 0.407811]\n",
      "epoch:26 step:25068 [D loss: 0.223119, acc.: 60.94%] [G loss: 0.437108]\n",
      "epoch:26 step:25069 [D loss: 0.242274, acc.: 59.38%] [G loss: 0.391098]\n",
      "epoch:26 step:25070 [D loss: 0.200489, acc.: 71.88%] [G loss: 0.415350]\n",
      "epoch:26 step:25071 [D loss: 0.202746, acc.: 68.75%] [G loss: 0.425678]\n",
      "epoch:26 step:25072 [D loss: 0.221461, acc.: 66.41%] [G loss: 0.463320]\n",
      "epoch:26 step:25073 [D loss: 0.217971, acc.: 59.38%] [G loss: 0.432428]\n",
      "epoch:26 step:25074 [D loss: 0.199417, acc.: 72.66%] [G loss: 0.477119]\n",
      "epoch:26 step:25075 [D loss: 0.224160, acc.: 60.94%] [G loss: 0.420896]\n",
      "epoch:26 step:25076 [D loss: 0.240264, acc.: 59.38%] [G loss: 0.435939]\n",
      "epoch:26 step:25077 [D loss: 0.230647, acc.: 56.25%] [G loss: 0.421352]\n",
      "epoch:26 step:25078 [D loss: 0.252666, acc.: 54.69%] [G loss: 0.411334]\n",
      "epoch:26 step:25079 [D loss: 0.228407, acc.: 64.84%] [G loss: 0.452859]\n",
      "epoch:26 step:25080 [D loss: 0.223820, acc.: 62.50%] [G loss: 0.421268]\n",
      "epoch:26 step:25081 [D loss: 0.215299, acc.: 63.28%] [G loss: 0.459541]\n",
      "epoch:26 step:25082 [D loss: 0.215984, acc.: 61.72%] [G loss: 0.438023]\n",
      "epoch:26 step:25083 [D loss: 0.262526, acc.: 53.91%] [G loss: 0.425967]\n",
      "epoch:26 step:25084 [D loss: 0.248091, acc.: 53.12%] [G loss: 0.400079]\n",
      "epoch:26 step:25085 [D loss: 0.226609, acc.: 58.59%] [G loss: 0.403547]\n",
      "epoch:26 step:25086 [D loss: 0.201728, acc.: 66.41%] [G loss: 0.439519]\n",
      "epoch:26 step:25087 [D loss: 0.203318, acc.: 67.97%] [G loss: 0.472063]\n",
      "epoch:26 step:25088 [D loss: 0.215455, acc.: 63.28%] [G loss: 0.444296]\n",
      "epoch:26 step:25089 [D loss: 0.237434, acc.: 60.94%] [G loss: 0.410496]\n",
      "epoch:26 step:25090 [D loss: 0.245557, acc.: 51.56%] [G loss: 0.404168]\n",
      "epoch:26 step:25091 [D loss: 0.213087, acc.: 68.75%] [G loss: 0.403469]\n",
      "epoch:26 step:25092 [D loss: 0.202396, acc.: 64.84%] [G loss: 0.441556]\n",
      "epoch:26 step:25093 [D loss: 0.228359, acc.: 65.62%] [G loss: 0.416108]\n",
      "epoch:26 step:25094 [D loss: 0.231836, acc.: 63.28%] [G loss: 0.432669]\n",
      "epoch:26 step:25095 [D loss: 0.208435, acc.: 63.28%] [G loss: 0.465898]\n",
      "epoch:26 step:25096 [D loss: 0.253226, acc.: 52.34%] [G loss: 0.449669]\n",
      "epoch:26 step:25097 [D loss: 0.243930, acc.: 61.72%] [G loss: 0.429337]\n",
      "epoch:26 step:25098 [D loss: 0.208404, acc.: 67.97%] [G loss: 0.420756]\n",
      "epoch:26 step:25099 [D loss: 0.213627, acc.: 66.41%] [G loss: 0.397992]\n",
      "epoch:26 step:25100 [D loss: 0.222728, acc.: 57.03%] [G loss: 0.374959]\n",
      "epoch:26 step:25101 [D loss: 0.235551, acc.: 61.72%] [G loss: 0.435922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25102 [D loss: 0.237477, acc.: 60.94%] [G loss: 0.409833]\n",
      "epoch:26 step:25103 [D loss: 0.235887, acc.: 55.47%] [G loss: 0.413377]\n",
      "epoch:26 step:25104 [D loss: 0.206739, acc.: 67.19%] [G loss: 0.418677]\n",
      "epoch:26 step:25105 [D loss: 0.219008, acc.: 67.19%] [G loss: 0.407080]\n",
      "epoch:26 step:25106 [D loss: 0.201592, acc.: 67.19%] [G loss: 0.377752]\n",
      "epoch:26 step:25107 [D loss: 0.252551, acc.: 52.34%] [G loss: 0.403494]\n",
      "epoch:26 step:25108 [D loss: 0.210343, acc.: 63.28%] [G loss: 0.416832]\n",
      "epoch:26 step:25109 [D loss: 0.223945, acc.: 67.19%] [G loss: 0.379218]\n",
      "epoch:26 step:25110 [D loss: 0.223020, acc.: 59.38%] [G loss: 0.456790]\n",
      "epoch:26 step:25111 [D loss: 0.208153, acc.: 68.75%] [G loss: 0.410322]\n",
      "epoch:26 step:25112 [D loss: 0.221916, acc.: 60.94%] [G loss: 0.400612]\n",
      "epoch:26 step:25113 [D loss: 0.222943, acc.: 62.50%] [G loss: 0.414041]\n",
      "epoch:26 step:25114 [D loss: 0.252026, acc.: 50.00%] [G loss: 0.395255]\n",
      "epoch:26 step:25115 [D loss: 0.231563, acc.: 61.72%] [G loss: 0.452856]\n",
      "epoch:26 step:25116 [D loss: 0.207520, acc.: 67.19%] [G loss: 0.495241]\n",
      "epoch:26 step:25117 [D loss: 0.229361, acc.: 61.72%] [G loss: 0.447436]\n",
      "epoch:26 step:25118 [D loss: 0.218488, acc.: 65.62%] [G loss: 0.432396]\n",
      "epoch:26 step:25119 [D loss: 0.234352, acc.: 64.06%] [G loss: 0.438209]\n",
      "epoch:26 step:25120 [D loss: 0.242450, acc.: 62.50%] [G loss: 0.404238]\n",
      "epoch:26 step:25121 [D loss: 0.233608, acc.: 60.94%] [G loss: 0.396498]\n",
      "epoch:26 step:25122 [D loss: 0.216926, acc.: 64.06%] [G loss: 0.391927]\n",
      "epoch:26 step:25123 [D loss: 0.224575, acc.: 57.03%] [G loss: 0.405469]\n",
      "epoch:26 step:25124 [D loss: 0.243910, acc.: 58.59%] [G loss: 0.399682]\n",
      "epoch:26 step:25125 [D loss: 0.234813, acc.: 65.62%] [G loss: 0.399077]\n",
      "epoch:26 step:25126 [D loss: 0.240328, acc.: 57.03%] [G loss: 0.410086]\n",
      "epoch:26 step:25127 [D loss: 0.236285, acc.: 58.59%] [G loss: 0.390479]\n",
      "epoch:26 step:25128 [D loss: 0.244899, acc.: 60.94%] [G loss: 0.403895]\n",
      "epoch:26 step:25129 [D loss: 0.205302, acc.: 66.41%] [G loss: 0.425771]\n",
      "epoch:26 step:25130 [D loss: 0.222539, acc.: 66.41%] [G loss: 0.438264]\n",
      "epoch:26 step:25131 [D loss: 0.206555, acc.: 68.75%] [G loss: 0.423547]\n",
      "epoch:26 step:25132 [D loss: 0.224932, acc.: 63.28%] [G loss: 0.428334]\n",
      "epoch:26 step:25133 [D loss: 0.201164, acc.: 71.09%] [G loss: 0.432003]\n",
      "epoch:26 step:25134 [D loss: 0.251846, acc.: 57.81%] [G loss: 0.446679]\n",
      "epoch:26 step:25135 [D loss: 0.233161, acc.: 61.72%] [G loss: 0.440911]\n",
      "epoch:26 step:25136 [D loss: 0.208204, acc.: 67.97%] [G loss: 0.466173]\n",
      "epoch:26 step:25137 [D loss: 0.200872, acc.: 67.97%] [G loss: 0.468024]\n",
      "epoch:26 step:25138 [D loss: 0.219450, acc.: 60.94%] [G loss: 0.457905]\n",
      "epoch:26 step:25139 [D loss: 0.232764, acc.: 63.28%] [G loss: 0.449233]\n",
      "epoch:26 step:25140 [D loss: 0.227459, acc.: 62.50%] [G loss: 0.420612]\n",
      "epoch:26 step:25141 [D loss: 0.245366, acc.: 57.03%] [G loss: 0.369725]\n",
      "epoch:26 step:25142 [D loss: 0.213183, acc.: 66.41%] [G loss: 0.428057]\n",
      "epoch:26 step:25143 [D loss: 0.217355, acc.: 60.94%] [G loss: 0.419306]\n",
      "epoch:26 step:25144 [D loss: 0.196839, acc.: 66.41%] [G loss: 0.490604]\n",
      "epoch:26 step:25145 [D loss: 0.245955, acc.: 55.47%] [G loss: 0.462428]\n",
      "epoch:26 step:25146 [D loss: 0.290896, acc.: 43.75%] [G loss: 0.409905]\n",
      "epoch:26 step:25147 [D loss: 0.247094, acc.: 54.69%] [G loss: 0.400997]\n",
      "epoch:26 step:25148 [D loss: 0.201203, acc.: 67.19%] [G loss: 0.444376]\n",
      "epoch:26 step:25149 [D loss: 0.248313, acc.: 48.44%] [G loss: 0.408238]\n",
      "epoch:26 step:25150 [D loss: 0.251392, acc.: 59.38%] [G loss: 0.386547]\n",
      "epoch:26 step:25151 [D loss: 0.230485, acc.: 64.84%] [G loss: 0.399101]\n",
      "epoch:26 step:25152 [D loss: 0.222284, acc.: 62.50%] [G loss: 0.392211]\n",
      "epoch:26 step:25153 [D loss: 0.239892, acc.: 58.59%] [G loss: 0.353019]\n",
      "epoch:26 step:25154 [D loss: 0.211062, acc.: 67.19%] [G loss: 0.414578]\n",
      "epoch:26 step:25155 [D loss: 0.207958, acc.: 65.62%] [G loss: 0.426652]\n",
      "epoch:26 step:25156 [D loss: 0.256398, acc.: 50.78%] [G loss: 0.461624]\n",
      "epoch:26 step:25157 [D loss: 0.236294, acc.: 54.69%] [G loss: 0.484739]\n",
      "epoch:26 step:25158 [D loss: 0.214796, acc.: 64.84%] [G loss: 0.467093]\n",
      "epoch:26 step:25159 [D loss: 0.253533, acc.: 52.34%] [G loss: 0.383152]\n",
      "epoch:26 step:25160 [D loss: 0.217741, acc.: 64.84%] [G loss: 0.469078]\n",
      "epoch:26 step:25161 [D loss: 0.242954, acc.: 57.81%] [G loss: 0.400262]\n",
      "epoch:26 step:25162 [D loss: 0.229575, acc.: 60.94%] [G loss: 0.416583]\n",
      "epoch:26 step:25163 [D loss: 0.217936, acc.: 66.41%] [G loss: 0.442891]\n",
      "epoch:26 step:25164 [D loss: 0.201547, acc.: 68.75%] [G loss: 0.484455]\n",
      "epoch:26 step:25165 [D loss: 0.208608, acc.: 66.41%] [G loss: 0.468436]\n",
      "epoch:26 step:25166 [D loss: 0.248438, acc.: 57.03%] [G loss: 0.392830]\n",
      "epoch:26 step:25167 [D loss: 0.217370, acc.: 62.50%] [G loss: 0.408285]\n",
      "epoch:26 step:25168 [D loss: 0.224264, acc.: 63.28%] [G loss: 0.439660]\n",
      "epoch:26 step:25169 [D loss: 0.221191, acc.: 64.84%] [G loss: 0.441377]\n",
      "epoch:26 step:25170 [D loss: 0.236733, acc.: 58.59%] [G loss: 0.420672]\n",
      "epoch:26 step:25171 [D loss: 0.223284, acc.: 62.50%] [G loss: 0.400954]\n",
      "epoch:26 step:25172 [D loss: 0.236843, acc.: 58.59%] [G loss: 0.402799]\n",
      "epoch:26 step:25173 [D loss: 0.222294, acc.: 64.06%] [G loss: 0.445983]\n",
      "epoch:26 step:25174 [D loss: 0.237153, acc.: 62.50%] [G loss: 0.434980]\n",
      "epoch:26 step:25175 [D loss: 0.230038, acc.: 62.50%] [G loss: 0.383324]\n",
      "epoch:26 step:25176 [D loss: 0.231134, acc.: 60.94%] [G loss: 0.433145]\n",
      "epoch:26 step:25177 [D loss: 0.208532, acc.: 64.06%] [G loss: 0.452897]\n",
      "epoch:26 step:25178 [D loss: 0.231125, acc.: 60.94%] [G loss: 0.456374]\n",
      "epoch:26 step:25179 [D loss: 0.238235, acc.: 66.41%] [G loss: 0.438642]\n",
      "epoch:26 step:25180 [D loss: 0.231327, acc.: 64.84%] [G loss: 0.400931]\n",
      "epoch:26 step:25181 [D loss: 0.238970, acc.: 60.16%] [G loss: 0.382858]\n",
      "epoch:26 step:25182 [D loss: 0.275756, acc.: 49.22%] [G loss: 0.380093]\n",
      "epoch:26 step:25183 [D loss: 0.246045, acc.: 55.47%] [G loss: 0.395563]\n",
      "epoch:26 step:25184 [D loss: 0.208086, acc.: 64.84%] [G loss: 0.395272]\n",
      "epoch:26 step:25185 [D loss: 0.202698, acc.: 67.19%] [G loss: 0.431177]\n",
      "epoch:26 step:25186 [D loss: 0.231505, acc.: 60.16%] [G loss: 0.432344]\n",
      "epoch:26 step:25187 [D loss: 0.198530, acc.: 73.44%] [G loss: 0.428307]\n",
      "epoch:26 step:25188 [D loss: 0.200354, acc.: 67.97%] [G loss: 0.423072]\n",
      "epoch:26 step:25189 [D loss: 0.248824, acc.: 56.25%] [G loss: 0.425852]\n",
      "epoch:26 step:25190 [D loss: 0.256210, acc.: 53.91%] [G loss: 0.420797]\n",
      "epoch:26 step:25191 [D loss: 0.243517, acc.: 57.03%] [G loss: 0.406304]\n",
      "epoch:26 step:25192 [D loss: 0.240588, acc.: 63.28%] [G loss: 0.438767]\n",
      "epoch:26 step:25193 [D loss: 0.232392, acc.: 62.50%] [G loss: 0.398963]\n",
      "epoch:26 step:25194 [D loss: 0.237596, acc.: 64.06%] [G loss: 0.428810]\n",
      "epoch:26 step:25195 [D loss: 0.217185, acc.: 64.06%] [G loss: 0.408821]\n",
      "epoch:26 step:25196 [D loss: 0.220716, acc.: 63.28%] [G loss: 0.415048]\n",
      "epoch:26 step:25197 [D loss: 0.251716, acc.: 57.81%] [G loss: 0.423705]\n",
      "epoch:26 step:25198 [D loss: 0.225362, acc.: 63.28%] [G loss: 0.379930]\n",
      "epoch:26 step:25199 [D loss: 0.217068, acc.: 61.72%] [G loss: 0.431042]\n",
      "epoch:26 step:25200 [D loss: 0.196416, acc.: 71.88%] [G loss: 0.432668]\n",
      "##############\n",
      "[2.41843263 2.08347002 6.14984257 4.86455576 3.55351344 5.44183913\n",
      " 4.51376211 4.68706111 4.45239593 4.16443539]\n",
      "##########\n",
      "epoch:26 step:25201 [D loss: 0.207688, acc.: 68.75%] [G loss: 0.443624]\n",
      "epoch:26 step:25202 [D loss: 0.228528, acc.: 65.62%] [G loss: 0.433953]\n",
      "epoch:26 step:25203 [D loss: 0.225680, acc.: 60.16%] [G loss: 0.390749]\n",
      "epoch:26 step:25204 [D loss: 0.185097, acc.: 77.34%] [G loss: 0.451432]\n",
      "epoch:26 step:25205 [D loss: 0.215544, acc.: 65.62%] [G loss: 0.446970]\n",
      "epoch:26 step:25206 [D loss: 0.222145, acc.: 65.62%] [G loss: 0.402045]\n",
      "epoch:26 step:25207 [D loss: 0.213752, acc.: 63.28%] [G loss: 0.404551]\n",
      "epoch:26 step:25208 [D loss: 0.222464, acc.: 56.25%] [G loss: 0.425824]\n",
      "epoch:26 step:25209 [D loss: 0.238888, acc.: 64.06%] [G loss: 0.397889]\n",
      "epoch:26 step:25210 [D loss: 0.233747, acc.: 64.06%] [G loss: 0.383863]\n",
      "epoch:26 step:25211 [D loss: 0.193291, acc.: 73.44%] [G loss: 0.434824]\n",
      "epoch:26 step:25212 [D loss: 0.258095, acc.: 54.69%] [G loss: 0.384631]\n",
      "epoch:26 step:25213 [D loss: 0.220492, acc.: 60.16%] [G loss: 0.452762]\n",
      "epoch:26 step:25214 [D loss: 0.207713, acc.: 64.06%] [G loss: 0.424692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25215 [D loss: 0.210252, acc.: 66.41%] [G loss: 0.452959]\n",
      "epoch:26 step:25216 [D loss: 0.227761, acc.: 61.72%] [G loss: 0.417369]\n",
      "epoch:26 step:25217 [D loss: 0.213524, acc.: 66.41%] [G loss: 0.456259]\n",
      "epoch:26 step:25218 [D loss: 0.246477, acc.: 56.25%] [G loss: 0.442135]\n",
      "epoch:26 step:25219 [D loss: 0.218445, acc.: 67.19%] [G loss: 0.468942]\n",
      "epoch:26 step:25220 [D loss: 0.263702, acc.: 53.91%] [G loss: 0.388970]\n",
      "epoch:26 step:25221 [D loss: 0.253532, acc.: 54.69%] [G loss: 0.383999]\n",
      "epoch:26 step:25222 [D loss: 0.211337, acc.: 70.31%] [G loss: 0.406105]\n",
      "epoch:26 step:25223 [D loss: 0.241853, acc.: 53.91%] [G loss: 0.396878]\n",
      "epoch:26 step:25224 [D loss: 0.227134, acc.: 60.16%] [G loss: 0.404584]\n",
      "epoch:26 step:25225 [D loss: 0.212985, acc.: 68.75%] [G loss: 0.426870]\n",
      "epoch:26 step:25226 [D loss: 0.241828, acc.: 54.69%] [G loss: 0.410043]\n",
      "epoch:26 step:25227 [D loss: 0.253014, acc.: 54.69%] [G loss: 0.374553]\n",
      "epoch:26 step:25228 [D loss: 0.233989, acc.: 59.38%] [G loss: 0.372826]\n",
      "epoch:26 step:25229 [D loss: 0.261633, acc.: 48.44%] [G loss: 0.385442]\n",
      "epoch:26 step:25230 [D loss: 0.232282, acc.: 58.59%] [G loss: 0.392339]\n",
      "epoch:26 step:25231 [D loss: 0.239912, acc.: 53.91%] [G loss: 0.399452]\n",
      "epoch:26 step:25232 [D loss: 0.256301, acc.: 52.34%] [G loss: 0.414858]\n",
      "epoch:26 step:25233 [D loss: 0.205795, acc.: 64.84%] [G loss: 0.403119]\n",
      "epoch:26 step:25234 [D loss: 0.226122, acc.: 62.50%] [G loss: 0.446872]\n",
      "epoch:26 step:25235 [D loss: 0.227844, acc.: 67.19%] [G loss: 0.450211]\n",
      "epoch:26 step:25236 [D loss: 0.270185, acc.: 53.12%] [G loss: 0.409954]\n",
      "epoch:26 step:25237 [D loss: 0.184042, acc.: 75.78%] [G loss: 0.425090]\n",
      "epoch:26 step:25238 [D loss: 0.235317, acc.: 62.50%] [G loss: 0.435073]\n",
      "epoch:26 step:25239 [D loss: 0.251908, acc.: 54.69%] [G loss: 0.414972]\n",
      "epoch:26 step:25240 [D loss: 0.229581, acc.: 57.81%] [G loss: 0.425124]\n",
      "epoch:26 step:25241 [D loss: 0.227957, acc.: 57.81%] [G loss: 0.412504]\n",
      "epoch:26 step:25242 [D loss: 0.236049, acc.: 61.72%] [G loss: 0.433362]\n",
      "epoch:26 step:25243 [D loss: 0.246407, acc.: 56.25%] [G loss: 0.419745]\n",
      "epoch:26 step:25244 [D loss: 0.217510, acc.: 65.62%] [G loss: 0.406376]\n",
      "epoch:26 step:25245 [D loss: 0.239635, acc.: 59.38%] [G loss: 0.416540]\n",
      "epoch:26 step:25246 [D loss: 0.215885, acc.: 67.97%] [G loss: 0.459340]\n",
      "epoch:26 step:25247 [D loss: 0.224021, acc.: 61.72%] [G loss: 0.471794]\n",
      "epoch:26 step:25248 [D loss: 0.209668, acc.: 68.75%] [G loss: 0.469393]\n",
      "epoch:26 step:25249 [D loss: 0.239218, acc.: 60.16%] [G loss: 0.432170]\n",
      "epoch:26 step:25250 [D loss: 0.236376, acc.: 60.94%] [G loss: 0.407942]\n",
      "epoch:26 step:25251 [D loss: 0.221110, acc.: 61.72%] [G loss: 0.418789]\n",
      "epoch:26 step:25252 [D loss: 0.212109, acc.: 64.84%] [G loss: 0.435273]\n",
      "epoch:26 step:25253 [D loss: 0.236795, acc.: 62.50%] [G loss: 0.411839]\n",
      "epoch:26 step:25254 [D loss: 0.258885, acc.: 53.91%] [G loss: 0.411728]\n",
      "epoch:26 step:25255 [D loss: 0.200438, acc.: 68.75%] [G loss: 0.429809]\n",
      "epoch:26 step:25256 [D loss: 0.211316, acc.: 69.53%] [G loss: 0.423709]\n",
      "epoch:26 step:25257 [D loss: 0.215312, acc.: 61.72%] [G loss: 0.452508]\n",
      "epoch:26 step:25258 [D loss: 0.212665, acc.: 64.84%] [G loss: 0.465363]\n",
      "epoch:26 step:25259 [D loss: 0.198803, acc.: 73.44%] [G loss: 0.455311]\n",
      "epoch:26 step:25260 [D loss: 0.215158, acc.: 70.31%] [G loss: 0.431290]\n",
      "epoch:26 step:25261 [D loss: 0.175486, acc.: 79.69%] [G loss: 0.474696]\n",
      "epoch:26 step:25262 [D loss: 0.235391, acc.: 59.38%] [G loss: 0.450127]\n",
      "epoch:26 step:25263 [D loss: 0.206490, acc.: 67.19%] [G loss: 0.475995]\n",
      "epoch:26 step:25264 [D loss: 0.231960, acc.: 63.28%] [G loss: 0.435224]\n",
      "epoch:26 step:25265 [D loss: 0.230498, acc.: 65.62%] [G loss: 0.410970]\n",
      "epoch:26 step:25266 [D loss: 0.229516, acc.: 61.72%] [G loss: 0.439714]\n",
      "epoch:26 step:25267 [D loss: 0.209265, acc.: 65.62%] [G loss: 0.429771]\n",
      "epoch:26 step:25268 [D loss: 0.194074, acc.: 67.97%] [G loss: 0.478083]\n",
      "epoch:26 step:25269 [D loss: 0.244358, acc.: 57.81%] [G loss: 0.454712]\n",
      "epoch:26 step:25270 [D loss: 0.216379, acc.: 70.31%] [G loss: 0.448015]\n",
      "epoch:26 step:25271 [D loss: 0.196844, acc.: 73.44%] [G loss: 0.427333]\n",
      "epoch:26 step:25272 [D loss: 0.200905, acc.: 67.19%] [G loss: 0.482835]\n",
      "epoch:26 step:25273 [D loss: 0.214127, acc.: 64.06%] [G loss: 0.426947]\n",
      "epoch:26 step:25274 [D loss: 0.219210, acc.: 67.19%] [G loss: 0.460100]\n",
      "epoch:26 step:25275 [D loss: 0.210728, acc.: 66.41%] [G loss: 0.472319]\n",
      "epoch:26 step:25276 [D loss: 0.215313, acc.: 64.84%] [G loss: 0.483937]\n",
      "epoch:26 step:25277 [D loss: 0.288469, acc.: 51.56%] [G loss: 0.398083]\n",
      "epoch:26 step:25278 [D loss: 0.241543, acc.: 59.38%] [G loss: 0.409632]\n",
      "epoch:26 step:25279 [D loss: 0.240735, acc.: 59.38%] [G loss: 0.417280]\n",
      "epoch:26 step:25280 [D loss: 0.184314, acc.: 73.44%] [G loss: 0.443054]\n",
      "epoch:26 step:25281 [D loss: 0.201013, acc.: 71.88%] [G loss: 0.437775]\n",
      "epoch:26 step:25282 [D loss: 0.262305, acc.: 54.69%] [G loss: 0.440164]\n",
      "epoch:26 step:25283 [D loss: 0.198648, acc.: 69.53%] [G loss: 0.435895]\n",
      "epoch:26 step:25284 [D loss: 0.247578, acc.: 57.81%] [G loss: 0.400994]\n",
      "epoch:26 step:25285 [D loss: 0.208241, acc.: 67.97%] [G loss: 0.413743]\n",
      "epoch:26 step:25286 [D loss: 0.196630, acc.: 67.19%] [G loss: 0.476017]\n",
      "epoch:26 step:25287 [D loss: 0.191240, acc.: 70.31%] [G loss: 0.493082]\n",
      "epoch:26 step:25288 [D loss: 0.202267, acc.: 72.66%] [G loss: 0.501518]\n",
      "epoch:26 step:25289 [D loss: 0.223935, acc.: 64.84%] [G loss: 0.518803]\n",
      "epoch:26 step:25290 [D loss: 0.296697, acc.: 50.78%] [G loss: 0.553766]\n",
      "epoch:26 step:25291 [D loss: 0.246729, acc.: 60.16%] [G loss: 0.629345]\n",
      "epoch:26 step:25292 [D loss: 0.218450, acc.: 67.19%] [G loss: 0.536490]\n",
      "epoch:26 step:25293 [D loss: 0.252768, acc.: 61.72%] [G loss: 0.429114]\n",
      "epoch:26 step:25294 [D loss: 0.252185, acc.: 57.03%] [G loss: 0.441633]\n",
      "epoch:26 step:25295 [D loss: 0.217407, acc.: 67.97%] [G loss: 0.428407]\n",
      "epoch:26 step:25296 [D loss: 0.214833, acc.: 71.09%] [G loss: 0.453290]\n",
      "epoch:26 step:25297 [D loss: 0.221785, acc.: 67.19%] [G loss: 0.471012]\n",
      "epoch:26 step:25298 [D loss: 0.167340, acc.: 81.25%] [G loss: 0.513019]\n",
      "epoch:26 step:25299 [D loss: 0.174325, acc.: 72.66%] [G loss: 0.588299]\n",
      "epoch:27 step:25300 [D loss: 0.238734, acc.: 60.94%] [G loss: 0.512494]\n",
      "epoch:27 step:25301 [D loss: 0.264671, acc.: 57.81%] [G loss: 0.472705]\n",
      "epoch:27 step:25302 [D loss: 0.238949, acc.: 56.25%] [G loss: 0.449783]\n",
      "epoch:27 step:25303 [D loss: 0.260721, acc.: 57.03%] [G loss: 0.459205]\n",
      "epoch:27 step:25304 [D loss: 0.233626, acc.: 61.72%] [G loss: 0.465226]\n",
      "epoch:27 step:25305 [D loss: 0.219255, acc.: 65.62%] [G loss: 0.443938]\n",
      "epoch:27 step:25306 [D loss: 0.209883, acc.: 65.62%] [G loss: 0.471325]\n",
      "epoch:27 step:25307 [D loss: 0.208778, acc.: 67.97%] [G loss: 0.434935]\n",
      "epoch:27 step:25308 [D loss: 0.201265, acc.: 66.41%] [G loss: 0.447441]\n",
      "epoch:27 step:25309 [D loss: 0.209299, acc.: 68.75%] [G loss: 0.449177]\n",
      "epoch:27 step:25310 [D loss: 0.208414, acc.: 70.31%] [G loss: 0.432579]\n",
      "epoch:27 step:25311 [D loss: 0.232087, acc.: 60.94%] [G loss: 0.442716]\n",
      "epoch:27 step:25312 [D loss: 0.201369, acc.: 69.53%] [G loss: 0.399939]\n",
      "epoch:27 step:25313 [D loss: 0.194615, acc.: 71.09%] [G loss: 0.378322]\n",
      "epoch:27 step:25314 [D loss: 0.197302, acc.: 67.97%] [G loss: 0.450207]\n",
      "epoch:27 step:25315 [D loss: 0.200183, acc.: 69.53%] [G loss: 0.469648]\n",
      "epoch:27 step:25316 [D loss: 0.226649, acc.: 60.94%] [G loss: 0.453057]\n",
      "epoch:27 step:25317 [D loss: 0.211151, acc.: 63.28%] [G loss: 0.510934]\n",
      "epoch:27 step:25318 [D loss: 0.246398, acc.: 56.25%] [G loss: 0.462970]\n",
      "epoch:27 step:25319 [D loss: 0.285926, acc.: 52.34%] [G loss: 0.428763]\n",
      "epoch:27 step:25320 [D loss: 0.228307, acc.: 57.81%] [G loss: 0.497083]\n",
      "epoch:27 step:25321 [D loss: 0.214177, acc.: 65.62%] [G loss: 0.473135]\n",
      "epoch:27 step:25322 [D loss: 0.216008, acc.: 64.06%] [G loss: 0.426540]\n",
      "epoch:27 step:25323 [D loss: 0.224811, acc.: 59.38%] [G loss: 0.387287]\n",
      "epoch:27 step:25324 [D loss: 0.230798, acc.: 60.94%] [G loss: 0.388058]\n",
      "epoch:27 step:25325 [D loss: 0.234094, acc.: 58.59%] [G loss: 0.389732]\n",
      "epoch:27 step:25326 [D loss: 0.225258, acc.: 62.50%] [G loss: 0.408512]\n",
      "epoch:27 step:25327 [D loss: 0.235108, acc.: 61.72%] [G loss: 0.417231]\n",
      "epoch:27 step:25328 [D loss: 0.221926, acc.: 64.06%] [G loss: 0.450391]\n",
      "epoch:27 step:25329 [D loss: 0.235894, acc.: 60.16%] [G loss: 0.443606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25330 [D loss: 0.249083, acc.: 53.91%] [G loss: 0.378185]\n",
      "epoch:27 step:25331 [D loss: 0.235554, acc.: 59.38%] [G loss: 0.378848]\n",
      "epoch:27 step:25332 [D loss: 0.226156, acc.: 60.16%] [G loss: 0.408893]\n",
      "epoch:27 step:25333 [D loss: 0.244142, acc.: 60.16%] [G loss: 0.369501]\n",
      "epoch:27 step:25334 [D loss: 0.224942, acc.: 60.16%] [G loss: 0.404128]\n",
      "epoch:27 step:25335 [D loss: 0.230896, acc.: 67.19%] [G loss: 0.406275]\n",
      "epoch:27 step:25336 [D loss: 0.232077, acc.: 58.59%] [G loss: 0.432797]\n",
      "epoch:27 step:25337 [D loss: 0.238217, acc.: 60.16%] [G loss: 0.399576]\n",
      "epoch:27 step:25338 [D loss: 0.215134, acc.: 70.31%] [G loss: 0.401008]\n",
      "epoch:27 step:25339 [D loss: 0.190057, acc.: 72.66%] [G loss: 0.442787]\n",
      "epoch:27 step:25340 [D loss: 0.257675, acc.: 53.91%] [G loss: 0.420493]\n",
      "epoch:27 step:25341 [D loss: 0.201277, acc.: 69.53%] [G loss: 0.435617]\n",
      "epoch:27 step:25342 [D loss: 0.219748, acc.: 64.84%] [G loss: 0.391545]\n",
      "epoch:27 step:25343 [D loss: 0.248084, acc.: 57.03%] [G loss: 0.419495]\n",
      "epoch:27 step:25344 [D loss: 0.216214, acc.: 64.06%] [G loss: 0.451272]\n",
      "epoch:27 step:25345 [D loss: 0.231051, acc.: 64.84%] [G loss: 0.447301]\n",
      "epoch:27 step:25346 [D loss: 0.251864, acc.: 54.69%] [G loss: 0.389106]\n",
      "epoch:27 step:25347 [D loss: 0.207834, acc.: 64.06%] [G loss: 0.398280]\n",
      "epoch:27 step:25348 [D loss: 0.202700, acc.: 67.97%] [G loss: 0.415308]\n",
      "epoch:27 step:25349 [D loss: 0.196333, acc.: 71.09%] [G loss: 0.415194]\n",
      "epoch:27 step:25350 [D loss: 0.252800, acc.: 58.59%] [G loss: 0.423145]\n",
      "epoch:27 step:25351 [D loss: 0.204346, acc.: 71.88%] [G loss: 0.457360]\n",
      "epoch:27 step:25352 [D loss: 0.215808, acc.: 67.97%] [G loss: 0.426491]\n",
      "epoch:27 step:25353 [D loss: 0.219040, acc.: 63.28%] [G loss: 0.437427]\n",
      "epoch:27 step:25354 [D loss: 0.212905, acc.: 64.84%] [G loss: 0.439962]\n",
      "epoch:27 step:25355 [D loss: 0.231407, acc.: 60.94%] [G loss: 0.430384]\n",
      "epoch:27 step:25356 [D loss: 0.224203, acc.: 69.53%] [G loss: 0.497024]\n",
      "epoch:27 step:25357 [D loss: 0.212803, acc.: 66.41%] [G loss: 0.449795]\n",
      "epoch:27 step:25358 [D loss: 0.231104, acc.: 57.81%] [G loss: 0.394277]\n",
      "epoch:27 step:25359 [D loss: 0.233107, acc.: 61.72%] [G loss: 0.428247]\n",
      "epoch:27 step:25360 [D loss: 0.263496, acc.: 49.22%] [G loss: 0.368580]\n",
      "epoch:27 step:25361 [D loss: 0.213051, acc.: 68.75%] [G loss: 0.444058]\n",
      "epoch:27 step:25362 [D loss: 0.208292, acc.: 65.62%] [G loss: 0.422643]\n",
      "epoch:27 step:25363 [D loss: 0.220199, acc.: 65.62%] [G loss: 0.439094]\n",
      "epoch:27 step:25364 [D loss: 0.230144, acc.: 60.16%] [G loss: 0.448296]\n",
      "epoch:27 step:25365 [D loss: 0.215770, acc.: 63.28%] [G loss: 0.437454]\n",
      "epoch:27 step:25366 [D loss: 0.198854, acc.: 67.97%] [G loss: 0.437708]\n",
      "epoch:27 step:25367 [D loss: 0.235632, acc.: 57.81%] [G loss: 0.433055]\n",
      "epoch:27 step:25368 [D loss: 0.195005, acc.: 73.44%] [G loss: 0.414984]\n",
      "epoch:27 step:25369 [D loss: 0.199636, acc.: 71.09%] [G loss: 0.489664]\n",
      "epoch:27 step:25370 [D loss: 0.238404, acc.: 59.38%] [G loss: 0.448478]\n",
      "epoch:27 step:25371 [D loss: 0.223109, acc.: 59.38%] [G loss: 0.435492]\n",
      "epoch:27 step:25372 [D loss: 0.228862, acc.: 59.38%] [G loss: 0.408822]\n",
      "epoch:27 step:25373 [D loss: 0.203158, acc.: 69.53%] [G loss: 0.429647]\n",
      "epoch:27 step:25374 [D loss: 0.237341, acc.: 59.38%] [G loss: 0.461583]\n",
      "epoch:27 step:25375 [D loss: 0.167433, acc.: 77.34%] [G loss: 0.500734]\n",
      "epoch:27 step:25376 [D loss: 0.215079, acc.: 61.72%] [G loss: 0.476475]\n",
      "epoch:27 step:25377 [D loss: 0.272077, acc.: 51.56%] [G loss: 0.420764]\n",
      "epoch:27 step:25378 [D loss: 0.233700, acc.: 57.81%] [G loss: 0.370549]\n",
      "epoch:27 step:25379 [D loss: 0.247872, acc.: 57.81%] [G loss: 0.381976]\n",
      "epoch:27 step:25380 [D loss: 0.233691, acc.: 59.38%] [G loss: 0.422099]\n",
      "epoch:27 step:25381 [D loss: 0.200669, acc.: 68.75%] [G loss: 0.393555]\n",
      "epoch:27 step:25382 [D loss: 0.229042, acc.: 61.72%] [G loss: 0.371656]\n",
      "epoch:27 step:25383 [D loss: 0.214882, acc.: 63.28%] [G loss: 0.424456]\n",
      "epoch:27 step:25384 [D loss: 0.225718, acc.: 61.72%] [G loss: 0.426560]\n",
      "epoch:27 step:25385 [D loss: 0.252680, acc.: 59.38%] [G loss: 0.396640]\n",
      "epoch:27 step:25386 [D loss: 0.224669, acc.: 60.16%] [G loss: 0.418818]\n",
      "epoch:27 step:25387 [D loss: 0.207401, acc.: 65.62%] [G loss: 0.444225]\n",
      "epoch:27 step:25388 [D loss: 0.212651, acc.: 66.41%] [G loss: 0.432396]\n",
      "epoch:27 step:25389 [D loss: 0.215458, acc.: 61.72%] [G loss: 0.419486]\n",
      "epoch:27 step:25390 [D loss: 0.228490, acc.: 61.72%] [G loss: 0.401224]\n",
      "epoch:27 step:25391 [D loss: 0.203950, acc.: 69.53%] [G loss: 0.469751]\n",
      "epoch:27 step:25392 [D loss: 0.182060, acc.: 74.22%] [G loss: 0.460391]\n",
      "epoch:27 step:25393 [D loss: 0.222861, acc.: 64.84%] [G loss: 0.433126]\n",
      "epoch:27 step:25394 [D loss: 0.204194, acc.: 63.28%] [G loss: 0.449845]\n",
      "epoch:27 step:25395 [D loss: 0.230562, acc.: 60.94%] [G loss: 0.414332]\n",
      "epoch:27 step:25396 [D loss: 0.201466, acc.: 68.75%] [G loss: 0.454630]\n",
      "epoch:27 step:25397 [D loss: 0.231934, acc.: 59.38%] [G loss: 0.456305]\n",
      "epoch:27 step:25398 [D loss: 0.254137, acc.: 56.25%] [G loss: 0.456918]\n",
      "epoch:27 step:25399 [D loss: 0.210611, acc.: 71.09%] [G loss: 0.417385]\n",
      "epoch:27 step:25400 [D loss: 0.224244, acc.: 65.62%] [G loss: 0.418431]\n",
      "##############\n",
      "[2.68219977 1.80667602 6.06976465 4.79306072 3.7502026  5.59126115\n",
      " 4.33960387 4.90078163 4.43234583 4.09160687]\n",
      "##########\n",
      "epoch:27 step:25401 [D loss: 0.242164, acc.: 59.38%] [G loss: 0.407293]\n",
      "epoch:27 step:25402 [D loss: 0.224106, acc.: 62.50%] [G loss: 0.402540]\n",
      "epoch:27 step:25403 [D loss: 0.239546, acc.: 57.03%] [G loss: 0.401270]\n",
      "epoch:27 step:25404 [D loss: 0.245747, acc.: 61.72%] [G loss: 0.374627]\n",
      "epoch:27 step:25405 [D loss: 0.223785, acc.: 60.94%] [G loss: 0.427891]\n",
      "epoch:27 step:25406 [D loss: 0.225234, acc.: 64.06%] [G loss: 0.429305]\n",
      "epoch:27 step:25407 [D loss: 0.234715, acc.: 62.50%] [G loss: 0.518916]\n",
      "epoch:27 step:25408 [D loss: 0.257298, acc.: 54.69%] [G loss: 0.466249]\n",
      "epoch:27 step:25409 [D loss: 0.242940, acc.: 55.47%] [G loss: 0.430051]\n",
      "epoch:27 step:25410 [D loss: 0.208372, acc.: 70.31%] [G loss: 0.410229]\n",
      "epoch:27 step:25411 [D loss: 0.194100, acc.: 75.78%] [G loss: 0.443541]\n",
      "epoch:27 step:25412 [D loss: 0.202791, acc.: 66.41%] [G loss: 0.467259]\n",
      "epoch:27 step:25413 [D loss: 0.218392, acc.: 67.97%] [G loss: 0.485319]\n",
      "epoch:27 step:25414 [D loss: 0.193040, acc.: 67.97%] [G loss: 0.470997]\n",
      "epoch:27 step:25415 [D loss: 0.205155, acc.: 63.28%] [G loss: 0.465840]\n",
      "epoch:27 step:25416 [D loss: 0.179372, acc.: 74.22%] [G loss: 0.451038]\n",
      "epoch:27 step:25417 [D loss: 0.210433, acc.: 64.84%] [G loss: 0.478506]\n",
      "epoch:27 step:25418 [D loss: 0.168711, acc.: 77.34%] [G loss: 0.496621]\n",
      "epoch:27 step:25419 [D loss: 0.218781, acc.: 67.97%] [G loss: 0.491022]\n",
      "epoch:27 step:25420 [D loss: 0.241976, acc.: 56.25%] [G loss: 0.422547]\n",
      "epoch:27 step:25421 [D loss: 0.196915, acc.: 72.66%] [G loss: 0.479375]\n",
      "epoch:27 step:25422 [D loss: 0.227289, acc.: 57.81%] [G loss: 0.454187]\n",
      "epoch:27 step:25423 [D loss: 0.231179, acc.: 62.50%] [G loss: 0.497133]\n",
      "epoch:27 step:25424 [D loss: 0.231270, acc.: 60.94%] [G loss: 0.431327]\n",
      "epoch:27 step:25425 [D loss: 0.223402, acc.: 64.84%] [G loss: 0.400153]\n",
      "epoch:27 step:25426 [D loss: 0.210519, acc.: 68.75%] [G loss: 0.420342]\n",
      "epoch:27 step:25427 [D loss: 0.226967, acc.: 59.38%] [G loss: 0.418195]\n",
      "epoch:27 step:25428 [D loss: 0.236362, acc.: 59.38%] [G loss: 0.389508]\n",
      "epoch:27 step:25429 [D loss: 0.215680, acc.: 61.72%] [G loss: 0.413244]\n",
      "epoch:27 step:25430 [D loss: 0.220217, acc.: 64.06%] [G loss: 0.421839]\n",
      "epoch:27 step:25431 [D loss: 0.220785, acc.: 70.31%] [G loss: 0.454100]\n",
      "epoch:27 step:25432 [D loss: 0.243267, acc.: 55.47%] [G loss: 0.453472]\n",
      "epoch:27 step:25433 [D loss: 0.238126, acc.: 58.59%] [G loss: 0.454704]\n",
      "epoch:27 step:25434 [D loss: 0.196048, acc.: 70.31%] [G loss: 0.466190]\n",
      "epoch:27 step:25435 [D loss: 0.228049, acc.: 63.28%] [G loss: 0.453477]\n",
      "epoch:27 step:25436 [D loss: 0.238840, acc.: 54.69%] [G loss: 0.440907]\n",
      "epoch:27 step:25437 [D loss: 0.256046, acc.: 50.00%] [G loss: 0.380051]\n",
      "epoch:27 step:25438 [D loss: 0.244172, acc.: 61.72%] [G loss: 0.375555]\n",
      "epoch:27 step:25439 [D loss: 0.257748, acc.: 53.91%] [G loss: 0.412857]\n",
      "epoch:27 step:25440 [D loss: 0.223068, acc.: 61.72%] [G loss: 0.433419]\n",
      "epoch:27 step:25441 [D loss: 0.232842, acc.: 55.47%] [G loss: 0.390455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25442 [D loss: 0.241235, acc.: 52.34%] [G loss: 0.443266]\n",
      "epoch:27 step:25443 [D loss: 0.218405, acc.: 62.50%] [G loss: 0.435351]\n",
      "epoch:27 step:25444 [D loss: 0.202896, acc.: 67.97%] [G loss: 0.433822]\n",
      "epoch:27 step:25445 [D loss: 0.239406, acc.: 59.38%] [G loss: 0.447867]\n",
      "epoch:27 step:25446 [D loss: 0.246011, acc.: 51.56%] [G loss: 0.416717]\n",
      "epoch:27 step:25447 [D loss: 0.248981, acc.: 61.72%] [G loss: 0.436384]\n",
      "epoch:27 step:25448 [D loss: 0.231305, acc.: 60.16%] [G loss: 0.441297]\n",
      "epoch:27 step:25449 [D loss: 0.245446, acc.: 61.72%] [G loss: 0.405952]\n",
      "epoch:27 step:25450 [D loss: 0.192725, acc.: 74.22%] [G loss: 0.437248]\n",
      "epoch:27 step:25451 [D loss: 0.227712, acc.: 63.28%] [G loss: 0.434221]\n",
      "epoch:27 step:25452 [D loss: 0.228668, acc.: 58.59%] [G loss: 0.422643]\n",
      "epoch:27 step:25453 [D loss: 0.225893, acc.: 65.62%] [G loss: 0.388130]\n",
      "epoch:27 step:25454 [D loss: 0.253846, acc.: 56.25%] [G loss: 0.423589]\n",
      "epoch:27 step:25455 [D loss: 0.213092, acc.: 64.06%] [G loss: 0.410550]\n",
      "epoch:27 step:25456 [D loss: 0.222601, acc.: 57.03%] [G loss: 0.447740]\n",
      "epoch:27 step:25457 [D loss: 0.225165, acc.: 67.97%] [G loss: 0.488193]\n",
      "epoch:27 step:25458 [D loss: 0.214191, acc.: 67.19%] [G loss: 0.400531]\n",
      "epoch:27 step:25459 [D loss: 0.266858, acc.: 53.91%] [G loss: 0.421663]\n",
      "epoch:27 step:25460 [D loss: 0.223872, acc.: 60.16%] [G loss: 0.445187]\n",
      "epoch:27 step:25461 [D loss: 0.222750, acc.: 63.28%] [G loss: 0.404421]\n",
      "epoch:27 step:25462 [D loss: 0.219237, acc.: 60.94%] [G loss: 0.434741]\n",
      "epoch:27 step:25463 [D loss: 0.216388, acc.: 66.41%] [G loss: 0.454269]\n",
      "epoch:27 step:25464 [D loss: 0.232743, acc.: 62.50%] [G loss: 0.424754]\n",
      "epoch:27 step:25465 [D loss: 0.216099, acc.: 68.75%] [G loss: 0.446145]\n",
      "epoch:27 step:25466 [D loss: 0.232722, acc.: 64.06%] [G loss: 0.397671]\n",
      "epoch:27 step:25467 [D loss: 0.204092, acc.: 71.09%] [G loss: 0.431085]\n",
      "epoch:27 step:25468 [D loss: 0.231741, acc.: 64.06%] [G loss: 0.451956]\n",
      "epoch:27 step:25469 [D loss: 0.242738, acc.: 57.03%] [G loss: 0.415590]\n",
      "epoch:27 step:25470 [D loss: 0.238472, acc.: 60.94%] [G loss: 0.412597]\n",
      "epoch:27 step:25471 [D loss: 0.217987, acc.: 64.84%] [G loss: 0.425762]\n",
      "epoch:27 step:25472 [D loss: 0.228983, acc.: 59.38%] [G loss: 0.386569]\n",
      "epoch:27 step:25473 [D loss: 0.241005, acc.: 58.59%] [G loss: 0.439729]\n",
      "epoch:27 step:25474 [D loss: 0.221038, acc.: 63.28%] [G loss: 0.443737]\n",
      "epoch:27 step:25475 [D loss: 0.215606, acc.: 63.28%] [G loss: 0.409851]\n",
      "epoch:27 step:25476 [D loss: 0.222418, acc.: 60.16%] [G loss: 0.420849]\n",
      "epoch:27 step:25477 [D loss: 0.229460, acc.: 62.50%] [G loss: 0.413293]\n",
      "epoch:27 step:25478 [D loss: 0.222910, acc.: 64.06%] [G loss: 0.401341]\n",
      "epoch:27 step:25479 [D loss: 0.230612, acc.: 56.25%] [G loss: 0.437602]\n",
      "epoch:27 step:25480 [D loss: 0.222675, acc.: 62.50%] [G loss: 0.448765]\n",
      "epoch:27 step:25481 [D loss: 0.230444, acc.: 64.84%] [G loss: 0.438362]\n",
      "epoch:27 step:25482 [D loss: 0.242606, acc.: 61.72%] [G loss: 0.447181]\n",
      "epoch:27 step:25483 [D loss: 0.219775, acc.: 68.75%] [G loss: 0.422055]\n",
      "epoch:27 step:25484 [D loss: 0.228753, acc.: 63.28%] [G loss: 0.442894]\n",
      "epoch:27 step:25485 [D loss: 0.210220, acc.: 67.97%] [G loss: 0.415975]\n",
      "epoch:27 step:25486 [D loss: 0.215615, acc.: 60.16%] [G loss: 0.456496]\n",
      "epoch:27 step:25487 [D loss: 0.241461, acc.: 59.38%] [G loss: 0.406602]\n",
      "epoch:27 step:25488 [D loss: 0.238282, acc.: 59.38%] [G loss: 0.414808]\n",
      "epoch:27 step:25489 [D loss: 0.219138, acc.: 66.41%] [G loss: 0.408366]\n",
      "epoch:27 step:25490 [D loss: 0.234239, acc.: 60.94%] [G loss: 0.392981]\n",
      "epoch:27 step:25491 [D loss: 0.209920, acc.: 69.53%] [G loss: 0.461883]\n",
      "epoch:27 step:25492 [D loss: 0.230782, acc.: 62.50%] [G loss: 0.455556]\n",
      "epoch:27 step:25493 [D loss: 0.224422, acc.: 60.94%] [G loss: 0.415362]\n",
      "epoch:27 step:25494 [D loss: 0.208960, acc.: 64.84%] [G loss: 0.394968]\n",
      "epoch:27 step:25495 [D loss: 0.226733, acc.: 60.94%] [G loss: 0.449836]\n",
      "epoch:27 step:25496 [D loss: 0.218897, acc.: 69.53%] [G loss: 0.434783]\n",
      "epoch:27 step:25497 [D loss: 0.198911, acc.: 71.09%] [G loss: 0.439713]\n",
      "epoch:27 step:25498 [D loss: 0.225271, acc.: 64.06%] [G loss: 0.462430]\n",
      "epoch:27 step:25499 [D loss: 0.256518, acc.: 54.69%] [G loss: 0.435269]\n",
      "epoch:27 step:25500 [D loss: 0.227588, acc.: 65.62%] [G loss: 0.439038]\n",
      "epoch:27 step:25501 [D loss: 0.208382, acc.: 68.75%] [G loss: 0.437906]\n",
      "epoch:27 step:25502 [D loss: 0.255231, acc.: 55.47%] [G loss: 0.459696]\n",
      "epoch:27 step:25503 [D loss: 0.203984, acc.: 69.53%] [G loss: 0.431048]\n",
      "epoch:27 step:25504 [D loss: 0.239343, acc.: 60.16%] [G loss: 0.400757]\n",
      "epoch:27 step:25505 [D loss: 0.213139, acc.: 67.97%] [G loss: 0.473667]\n",
      "epoch:27 step:25506 [D loss: 0.219060, acc.: 65.62%] [G loss: 0.430681]\n",
      "epoch:27 step:25507 [D loss: 0.191174, acc.: 74.22%] [G loss: 0.426266]\n",
      "epoch:27 step:25508 [D loss: 0.180388, acc.: 78.12%] [G loss: 0.472123]\n",
      "epoch:27 step:25509 [D loss: 0.280377, acc.: 47.66%] [G loss: 0.427651]\n",
      "epoch:27 step:25510 [D loss: 0.231332, acc.: 60.94%] [G loss: 0.432149]\n",
      "epoch:27 step:25511 [D loss: 0.234444, acc.: 61.72%] [G loss: 0.408191]\n",
      "epoch:27 step:25512 [D loss: 0.252703, acc.: 56.25%] [G loss: 0.398197]\n",
      "epoch:27 step:25513 [D loss: 0.247873, acc.: 54.69%] [G loss: 0.415480]\n",
      "epoch:27 step:25514 [D loss: 0.234696, acc.: 64.84%] [G loss: 0.402301]\n",
      "epoch:27 step:25515 [D loss: 0.205176, acc.: 68.75%] [G loss: 0.429312]\n",
      "epoch:27 step:25516 [D loss: 0.232227, acc.: 58.59%] [G loss: 0.416591]\n",
      "epoch:27 step:25517 [D loss: 0.193031, acc.: 72.66%] [G loss: 0.440486]\n",
      "epoch:27 step:25518 [D loss: 0.196820, acc.: 71.09%] [G loss: 0.481571]\n",
      "epoch:27 step:25519 [D loss: 0.248942, acc.: 55.47%] [G loss: 0.454926]\n",
      "epoch:27 step:25520 [D loss: 0.210334, acc.: 64.84%] [G loss: 0.469985]\n",
      "epoch:27 step:25521 [D loss: 0.234185, acc.: 61.72%] [G loss: 0.419535]\n",
      "epoch:27 step:25522 [D loss: 0.208850, acc.: 63.28%] [G loss: 0.475027]\n",
      "epoch:27 step:25523 [D loss: 0.231982, acc.: 59.38%] [G loss: 0.431315]\n",
      "epoch:27 step:25524 [D loss: 0.224433, acc.: 61.72%] [G loss: 0.404774]\n",
      "epoch:27 step:25525 [D loss: 0.239902, acc.: 55.47%] [G loss: 0.393358]\n",
      "epoch:27 step:25526 [D loss: 0.215481, acc.: 69.53%] [G loss: 0.423328]\n",
      "epoch:27 step:25527 [D loss: 0.224243, acc.: 60.16%] [G loss: 0.404840]\n",
      "epoch:27 step:25528 [D loss: 0.208892, acc.: 64.84%] [G loss: 0.444919]\n",
      "epoch:27 step:25529 [D loss: 0.192785, acc.: 71.88%] [G loss: 0.414911]\n",
      "epoch:27 step:25530 [D loss: 0.183120, acc.: 73.44%] [G loss: 0.467565]\n",
      "epoch:27 step:25531 [D loss: 0.151738, acc.: 80.47%] [G loss: 0.533239]\n",
      "epoch:27 step:25532 [D loss: 0.223248, acc.: 65.62%] [G loss: 0.468520]\n",
      "epoch:27 step:25533 [D loss: 0.261334, acc.: 53.91%] [G loss: 0.416792]\n",
      "epoch:27 step:25534 [D loss: 0.231155, acc.: 61.72%] [G loss: 0.427393]\n",
      "epoch:27 step:25535 [D loss: 0.213312, acc.: 68.75%] [G loss: 0.464672]\n",
      "epoch:27 step:25536 [D loss: 0.194251, acc.: 71.09%] [G loss: 0.488730]\n",
      "epoch:27 step:25537 [D loss: 0.211023, acc.: 67.97%] [G loss: 0.470669]\n",
      "epoch:27 step:25538 [D loss: 0.211570, acc.: 62.50%] [G loss: 0.460124]\n",
      "epoch:27 step:25539 [D loss: 0.218867, acc.: 67.97%] [G loss: 0.425900]\n",
      "epoch:27 step:25540 [D loss: 0.199576, acc.: 70.31%] [G loss: 0.445881]\n",
      "epoch:27 step:25541 [D loss: 0.219719, acc.: 67.19%] [G loss: 0.442169]\n",
      "epoch:27 step:25542 [D loss: 0.224449, acc.: 62.50%] [G loss: 0.448519]\n",
      "epoch:27 step:25543 [D loss: 0.205778, acc.: 67.97%] [G loss: 0.439092]\n",
      "epoch:27 step:25544 [D loss: 0.205350, acc.: 70.31%] [G loss: 0.423396]\n",
      "epoch:27 step:25545 [D loss: 0.212012, acc.: 72.66%] [G loss: 0.457808]\n",
      "epoch:27 step:25546 [D loss: 0.229860, acc.: 59.38%] [G loss: 0.477548]\n",
      "epoch:27 step:25547 [D loss: 0.188928, acc.: 71.88%] [G loss: 0.462153]\n",
      "epoch:27 step:25548 [D loss: 0.239650, acc.: 60.16%] [G loss: 0.444474]\n",
      "epoch:27 step:25549 [D loss: 0.267765, acc.: 49.22%] [G loss: 0.424300]\n",
      "epoch:27 step:25550 [D loss: 0.241290, acc.: 55.47%] [G loss: 0.419757]\n",
      "epoch:27 step:25551 [D loss: 0.245355, acc.: 59.38%] [G loss: 0.399102]\n",
      "epoch:27 step:25552 [D loss: 0.217467, acc.: 67.97%] [G loss: 0.428387]\n",
      "epoch:27 step:25553 [D loss: 0.228047, acc.: 60.94%] [G loss: 0.412223]\n",
      "epoch:27 step:25554 [D loss: 0.211433, acc.: 67.97%] [G loss: 0.473575]\n",
      "epoch:27 step:25555 [D loss: 0.208421, acc.: 70.31%] [G loss: 0.396044]\n",
      "epoch:27 step:25556 [D loss: 0.234820, acc.: 60.16%] [G loss: 0.414062]\n",
      "epoch:27 step:25557 [D loss: 0.232040, acc.: 58.59%] [G loss: 0.435768]\n",
      "epoch:27 step:25558 [D loss: 0.211566, acc.: 67.19%] [G loss: 0.485882]\n",
      "epoch:27 step:25559 [D loss: 0.223379, acc.: 63.28%] [G loss: 0.429074]\n",
      "epoch:27 step:25560 [D loss: 0.219333, acc.: 60.94%] [G loss: 0.433959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25561 [D loss: 0.195113, acc.: 67.19%] [G loss: 0.474226]\n",
      "epoch:27 step:25562 [D loss: 0.243940, acc.: 60.16%] [G loss: 0.427220]\n",
      "epoch:27 step:25563 [D loss: 0.207152, acc.: 71.09%] [G loss: 0.473520]\n",
      "epoch:27 step:25564 [D loss: 0.227622, acc.: 66.41%] [G loss: 0.459998]\n",
      "epoch:27 step:25565 [D loss: 0.238340, acc.: 58.59%] [G loss: 0.424886]\n",
      "epoch:27 step:25566 [D loss: 0.218674, acc.: 65.62%] [G loss: 0.409307]\n",
      "epoch:27 step:25567 [D loss: 0.240633, acc.: 59.38%] [G loss: 0.413497]\n",
      "epoch:27 step:25568 [D loss: 0.216315, acc.: 62.50%] [G loss: 0.432936]\n",
      "epoch:27 step:25569 [D loss: 0.227067, acc.: 59.38%] [G loss: 0.427385]\n",
      "epoch:27 step:25570 [D loss: 0.183742, acc.: 74.22%] [G loss: 0.422038]\n",
      "epoch:27 step:25571 [D loss: 0.242439, acc.: 59.38%] [G loss: 0.388533]\n",
      "epoch:27 step:25572 [D loss: 0.215371, acc.: 64.84%] [G loss: 0.440263]\n",
      "epoch:27 step:25573 [D loss: 0.199825, acc.: 71.09%] [G loss: 0.423482]\n",
      "epoch:27 step:25574 [D loss: 0.218245, acc.: 63.28%] [G loss: 0.422119]\n",
      "epoch:27 step:25575 [D loss: 0.181969, acc.: 71.09%] [G loss: 0.488379]\n",
      "epoch:27 step:25576 [D loss: 0.240027, acc.: 59.38%] [G loss: 0.436012]\n",
      "epoch:27 step:25577 [D loss: 0.243979, acc.: 60.16%] [G loss: 0.433654]\n",
      "epoch:27 step:25578 [D loss: 0.226575, acc.: 63.28%] [G loss: 0.467031]\n",
      "epoch:27 step:25579 [D loss: 0.227550, acc.: 61.72%] [G loss: 0.460134]\n",
      "epoch:27 step:25580 [D loss: 0.256861, acc.: 53.91%] [G loss: 0.399672]\n",
      "epoch:27 step:25581 [D loss: 0.253577, acc.: 54.69%] [G loss: 0.422507]\n",
      "epoch:27 step:25582 [D loss: 0.202213, acc.: 65.62%] [G loss: 0.440893]\n",
      "epoch:27 step:25583 [D loss: 0.217866, acc.: 59.38%] [G loss: 0.408908]\n",
      "epoch:27 step:25584 [D loss: 0.235811, acc.: 60.16%] [G loss: 0.424740]\n",
      "epoch:27 step:25585 [D loss: 0.209824, acc.: 68.75%] [G loss: 0.419091]\n",
      "epoch:27 step:25586 [D loss: 0.233484, acc.: 58.59%] [G loss: 0.407582]\n",
      "epoch:27 step:25587 [D loss: 0.211581, acc.: 65.62%] [G loss: 0.456375]\n",
      "epoch:27 step:25588 [D loss: 0.202430, acc.: 67.97%] [G loss: 0.464186]\n",
      "epoch:27 step:25589 [D loss: 0.242118, acc.: 57.03%] [G loss: 0.461992]\n",
      "epoch:27 step:25590 [D loss: 0.255854, acc.: 56.25%] [G loss: 0.383097]\n",
      "epoch:27 step:25591 [D loss: 0.231425, acc.: 58.59%] [G loss: 0.425635]\n",
      "epoch:27 step:25592 [D loss: 0.236016, acc.: 60.16%] [G loss: 0.411852]\n",
      "epoch:27 step:25593 [D loss: 0.244874, acc.: 53.12%] [G loss: 0.385834]\n",
      "epoch:27 step:25594 [D loss: 0.258851, acc.: 55.47%] [G loss: 0.375814]\n",
      "epoch:27 step:25595 [D loss: 0.215438, acc.: 60.16%] [G loss: 0.383351]\n",
      "epoch:27 step:25596 [D loss: 0.219377, acc.: 62.50%] [G loss: 0.401251]\n",
      "epoch:27 step:25597 [D loss: 0.185271, acc.: 79.69%] [G loss: 0.437419]\n",
      "epoch:27 step:25598 [D loss: 0.200661, acc.: 68.75%] [G loss: 0.455290]\n",
      "epoch:27 step:25599 [D loss: 0.202769, acc.: 68.75%] [G loss: 0.450724]\n",
      "epoch:27 step:25600 [D loss: 0.238794, acc.: 60.16%] [G loss: 0.460153]\n",
      "##############\n",
      "[2.67078824 2.04561468 6.29705194 4.77236527 3.50713017 5.58999547\n",
      " 4.58023694 4.95757856 4.47110361 4.26653998]\n",
      "##########\n",
      "epoch:27 step:25601 [D loss: 0.211904, acc.: 66.41%] [G loss: 0.418382]\n",
      "epoch:27 step:25602 [D loss: 0.220260, acc.: 64.84%] [G loss: 0.417087]\n",
      "epoch:27 step:25603 [D loss: 0.218142, acc.: 66.41%] [G loss: 0.419181]\n",
      "epoch:27 step:25604 [D loss: 0.215314, acc.: 64.84%] [G loss: 0.463729]\n",
      "epoch:27 step:25605 [D loss: 0.201205, acc.: 64.84%] [G loss: 0.450520]\n",
      "epoch:27 step:25606 [D loss: 0.192483, acc.: 74.22%] [G loss: 0.443830]\n",
      "epoch:27 step:25607 [D loss: 0.230995, acc.: 56.25%] [G loss: 0.412472]\n",
      "epoch:27 step:25608 [D loss: 0.216094, acc.: 65.62%] [G loss: 0.384458]\n",
      "epoch:27 step:25609 [D loss: 0.214584, acc.: 64.06%] [G loss: 0.437814]\n",
      "epoch:27 step:25610 [D loss: 0.212102, acc.: 69.53%] [G loss: 0.410579]\n",
      "epoch:27 step:25611 [D loss: 0.209562, acc.: 67.97%] [G loss: 0.437436]\n",
      "epoch:27 step:25612 [D loss: 0.169109, acc.: 82.81%] [G loss: 0.476045]\n",
      "epoch:27 step:25613 [D loss: 0.209522, acc.: 67.19%] [G loss: 0.495067]\n",
      "epoch:27 step:25614 [D loss: 0.185621, acc.: 71.88%] [G loss: 0.505160]\n",
      "epoch:27 step:25615 [D loss: 0.262170, acc.: 56.25%] [G loss: 0.432400]\n",
      "epoch:27 step:25616 [D loss: 0.236771, acc.: 60.16%] [G loss: 0.402103]\n",
      "epoch:27 step:25617 [D loss: 0.219991, acc.: 66.41%] [G loss: 0.405573]\n",
      "epoch:27 step:25618 [D loss: 0.229587, acc.: 58.59%] [G loss: 0.433351]\n",
      "epoch:27 step:25619 [D loss: 0.229867, acc.: 64.84%] [G loss: 0.416313]\n",
      "epoch:27 step:25620 [D loss: 0.215766, acc.: 67.97%] [G loss: 0.406342]\n",
      "epoch:27 step:25621 [D loss: 0.212180, acc.: 69.53%] [G loss: 0.437648]\n",
      "epoch:27 step:25622 [D loss: 0.240259, acc.: 56.25%] [G loss: 0.417269]\n",
      "epoch:27 step:25623 [D loss: 0.235592, acc.: 59.38%] [G loss: 0.424180]\n",
      "epoch:27 step:25624 [D loss: 0.207230, acc.: 68.75%] [G loss: 0.437541]\n",
      "epoch:27 step:25625 [D loss: 0.234591, acc.: 54.69%] [G loss: 0.402757]\n",
      "epoch:27 step:25626 [D loss: 0.230398, acc.: 60.16%] [G loss: 0.414463]\n",
      "epoch:27 step:25627 [D loss: 0.207180, acc.: 64.84%] [G loss: 0.427753]\n",
      "epoch:27 step:25628 [D loss: 0.216958, acc.: 60.16%] [G loss: 0.414307]\n",
      "epoch:27 step:25629 [D loss: 0.218763, acc.: 68.75%] [G loss: 0.414540]\n",
      "epoch:27 step:25630 [D loss: 0.195163, acc.: 70.31%] [G loss: 0.443086]\n",
      "epoch:27 step:25631 [D loss: 0.216552, acc.: 64.06%] [G loss: 0.394662]\n",
      "epoch:27 step:25632 [D loss: 0.200230, acc.: 72.66%] [G loss: 0.446098]\n",
      "epoch:27 step:25633 [D loss: 0.240195, acc.: 57.03%] [G loss: 0.433011]\n",
      "epoch:27 step:25634 [D loss: 0.233596, acc.: 58.59%] [G loss: 0.450122]\n",
      "epoch:27 step:25635 [D loss: 0.198976, acc.: 69.53%] [G loss: 0.448637]\n",
      "epoch:27 step:25636 [D loss: 0.240078, acc.: 57.03%] [G loss: 0.450451]\n",
      "epoch:27 step:25637 [D loss: 0.210150, acc.: 67.19%] [G loss: 0.453960]\n",
      "epoch:27 step:25638 [D loss: 0.217400, acc.: 66.41%] [G loss: 0.441450]\n",
      "epoch:27 step:25639 [D loss: 0.210633, acc.: 66.41%] [G loss: 0.441389]\n",
      "epoch:27 step:25640 [D loss: 0.287699, acc.: 46.88%] [G loss: 0.412909]\n",
      "epoch:27 step:25641 [D loss: 0.223495, acc.: 62.50%] [G loss: 0.463834]\n",
      "epoch:27 step:25642 [D loss: 0.241652, acc.: 60.16%] [G loss: 0.419023]\n",
      "epoch:27 step:25643 [D loss: 0.221473, acc.: 62.50%] [G loss: 0.457325]\n",
      "epoch:27 step:25644 [D loss: 0.216505, acc.: 67.97%] [G loss: 0.435600]\n",
      "epoch:27 step:25645 [D loss: 0.203249, acc.: 66.41%] [G loss: 0.444897]\n",
      "epoch:27 step:25646 [D loss: 0.175488, acc.: 75.78%] [G loss: 0.524108]\n",
      "epoch:27 step:25647 [D loss: 0.246435, acc.: 63.28%] [G loss: 0.449027]\n",
      "epoch:27 step:25648 [D loss: 0.304601, acc.: 50.78%] [G loss: 0.408943]\n",
      "epoch:27 step:25649 [D loss: 0.203938, acc.: 69.53%] [G loss: 0.406828]\n",
      "epoch:27 step:25650 [D loss: 0.230125, acc.: 59.38%] [G loss: 0.464008]\n",
      "epoch:27 step:25651 [D loss: 0.221836, acc.: 60.16%] [G loss: 0.446974]\n",
      "epoch:27 step:25652 [D loss: 0.226535, acc.: 60.94%] [G loss: 0.405994]\n",
      "epoch:27 step:25653 [D loss: 0.186926, acc.: 72.66%] [G loss: 0.500627]\n",
      "epoch:27 step:25654 [D loss: 0.258144, acc.: 57.81%] [G loss: 0.421771]\n",
      "epoch:27 step:25655 [D loss: 0.224628, acc.: 60.94%] [G loss: 0.403961]\n",
      "epoch:27 step:25656 [D loss: 0.208005, acc.: 65.62%] [G loss: 0.410169]\n",
      "epoch:27 step:25657 [D loss: 0.223622, acc.: 63.28%] [G loss: 0.477110]\n",
      "epoch:27 step:25658 [D loss: 0.215826, acc.: 60.94%] [G loss: 0.458971]\n",
      "epoch:27 step:25659 [D loss: 0.200051, acc.: 68.75%] [G loss: 0.458848]\n",
      "epoch:27 step:25660 [D loss: 0.202734, acc.: 67.19%] [G loss: 0.461122]\n",
      "epoch:27 step:25661 [D loss: 0.235528, acc.: 61.72%] [G loss: 0.448076]\n",
      "epoch:27 step:25662 [D loss: 0.210231, acc.: 64.06%] [G loss: 0.439710]\n",
      "epoch:27 step:25663 [D loss: 0.204579, acc.: 64.84%] [G loss: 0.408646]\n",
      "epoch:27 step:25664 [D loss: 0.219936, acc.: 65.62%] [G loss: 0.414626]\n",
      "epoch:27 step:25665 [D loss: 0.236828, acc.: 55.47%] [G loss: 0.404745]\n",
      "epoch:27 step:25666 [D loss: 0.255775, acc.: 52.34%] [G loss: 0.386396]\n",
      "epoch:27 step:25667 [D loss: 0.250048, acc.: 53.91%] [G loss: 0.426544]\n",
      "epoch:27 step:25668 [D loss: 0.212311, acc.: 66.41%] [G loss: 0.432526]\n",
      "epoch:27 step:25669 [D loss: 0.197858, acc.: 72.66%] [G loss: 0.438515]\n",
      "epoch:27 step:25670 [D loss: 0.217712, acc.: 68.75%] [G loss: 0.421655]\n",
      "epoch:27 step:25671 [D loss: 0.203944, acc.: 67.97%] [G loss: 0.473975]\n",
      "epoch:27 step:25672 [D loss: 0.231271, acc.: 63.28%] [G loss: 0.439207]\n",
      "epoch:27 step:25673 [D loss: 0.181361, acc.: 78.91%] [G loss: 0.442559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25674 [D loss: 0.218830, acc.: 65.62%] [G loss: 0.440314]\n",
      "epoch:27 step:25675 [D loss: 0.268046, acc.: 50.78%] [G loss: 0.395572]\n",
      "epoch:27 step:25676 [D loss: 0.213257, acc.: 61.72%] [G loss: 0.440988]\n",
      "epoch:27 step:25677 [D loss: 0.233391, acc.: 60.94%] [G loss: 0.437180]\n",
      "epoch:27 step:25678 [D loss: 0.240059, acc.: 54.69%] [G loss: 0.440636]\n",
      "epoch:27 step:25679 [D loss: 0.246766, acc.: 51.56%] [G loss: 0.432766]\n",
      "epoch:27 step:25680 [D loss: 0.218014, acc.: 63.28%] [G loss: 0.399451]\n",
      "epoch:27 step:25681 [D loss: 0.228458, acc.: 63.28%] [G loss: 0.415012]\n",
      "epoch:27 step:25682 [D loss: 0.216589, acc.: 64.84%] [G loss: 0.405468]\n",
      "epoch:27 step:25683 [D loss: 0.187393, acc.: 71.88%] [G loss: 0.439870]\n",
      "epoch:27 step:25684 [D loss: 0.177594, acc.: 73.44%] [G loss: 0.432221]\n",
      "epoch:27 step:25685 [D loss: 0.232106, acc.: 57.81%] [G loss: 0.426262]\n",
      "epoch:27 step:25686 [D loss: 0.202988, acc.: 67.19%] [G loss: 0.478503]\n",
      "epoch:27 step:25687 [D loss: 0.221806, acc.: 67.97%] [G loss: 0.437070]\n",
      "epoch:27 step:25688 [D loss: 0.231047, acc.: 62.50%] [G loss: 0.425446]\n",
      "epoch:27 step:25689 [D loss: 0.252546, acc.: 53.12%] [G loss: 0.377633]\n",
      "epoch:27 step:25690 [D loss: 0.209194, acc.: 61.72%] [G loss: 0.443588]\n",
      "epoch:27 step:25691 [D loss: 0.224175, acc.: 61.72%] [G loss: 0.428413]\n",
      "epoch:27 step:25692 [D loss: 0.226903, acc.: 59.38%] [G loss: 0.403710]\n",
      "epoch:27 step:25693 [D loss: 0.228575, acc.: 64.06%] [G loss: 0.437783]\n",
      "epoch:27 step:25694 [D loss: 0.207210, acc.: 67.97%] [G loss: 0.421271]\n",
      "epoch:27 step:25695 [D loss: 0.232336, acc.: 57.81%] [G loss: 0.445114]\n",
      "epoch:27 step:25696 [D loss: 0.247703, acc.: 55.47%] [G loss: 0.454766]\n",
      "epoch:27 step:25697 [D loss: 0.216605, acc.: 71.09%] [G loss: 0.443588]\n",
      "epoch:27 step:25698 [D loss: 0.197832, acc.: 64.06%] [G loss: 0.466784]\n",
      "epoch:27 step:25699 [D loss: 0.263283, acc.: 52.34%] [G loss: 0.435799]\n",
      "epoch:27 step:25700 [D loss: 0.267426, acc.: 47.66%] [G loss: 0.414107]\n",
      "epoch:27 step:25701 [D loss: 0.207140, acc.: 71.88%] [G loss: 0.445459]\n",
      "epoch:27 step:25702 [D loss: 0.267649, acc.: 53.12%] [G loss: 0.402795]\n",
      "epoch:27 step:25703 [D loss: 0.213691, acc.: 65.62%] [G loss: 0.413852]\n",
      "epoch:27 step:25704 [D loss: 0.224365, acc.: 63.28%] [G loss: 0.425481]\n",
      "epoch:27 step:25705 [D loss: 0.186247, acc.: 74.22%] [G loss: 0.449374]\n",
      "epoch:27 step:25706 [D loss: 0.234722, acc.: 62.50%] [G loss: 0.437822]\n",
      "epoch:27 step:25707 [D loss: 0.245550, acc.: 57.81%] [G loss: 0.434731]\n",
      "epoch:27 step:25708 [D loss: 0.214901, acc.: 66.41%] [G loss: 0.440230]\n",
      "epoch:27 step:25709 [D loss: 0.257301, acc.: 53.91%] [G loss: 0.427447]\n",
      "epoch:27 step:25710 [D loss: 0.237881, acc.: 56.25%] [G loss: 0.398891]\n",
      "epoch:27 step:25711 [D loss: 0.222970, acc.: 57.81%] [G loss: 0.433014]\n",
      "epoch:27 step:25712 [D loss: 0.237577, acc.: 62.50%] [G loss: 0.413825]\n",
      "epoch:27 step:25713 [D loss: 0.210401, acc.: 64.06%] [G loss: 0.450330]\n",
      "epoch:27 step:25714 [D loss: 0.238836, acc.: 58.59%] [G loss: 0.461140]\n",
      "epoch:27 step:25715 [D loss: 0.195022, acc.: 68.75%] [G loss: 0.494479]\n",
      "epoch:27 step:25716 [D loss: 0.227890, acc.: 64.06%] [G loss: 0.459579]\n",
      "epoch:27 step:25717 [D loss: 0.259636, acc.: 56.25%] [G loss: 0.427769]\n",
      "epoch:27 step:25718 [D loss: 0.250197, acc.: 54.69%] [G loss: 0.442051]\n",
      "epoch:27 step:25719 [D loss: 0.229669, acc.: 62.50%] [G loss: 0.473926]\n",
      "epoch:27 step:25720 [D loss: 0.290353, acc.: 46.88%] [G loss: 0.392734]\n",
      "epoch:27 step:25721 [D loss: 0.221925, acc.: 65.62%] [G loss: 0.422237]\n",
      "epoch:27 step:25722 [D loss: 0.214419, acc.: 61.72%] [G loss: 0.419188]\n",
      "epoch:27 step:25723 [D loss: 0.243599, acc.: 55.47%] [G loss: 0.372526]\n",
      "epoch:27 step:25724 [D loss: 0.224533, acc.: 63.28%] [G loss: 0.365643]\n",
      "epoch:27 step:25725 [D loss: 0.212313, acc.: 67.97%] [G loss: 0.380656]\n",
      "epoch:27 step:25726 [D loss: 0.205821, acc.: 67.97%] [G loss: 0.452230]\n",
      "epoch:27 step:25727 [D loss: 0.201059, acc.: 69.53%] [G loss: 0.437114]\n",
      "epoch:27 step:25728 [D loss: 0.189706, acc.: 71.09%] [G loss: 0.440011]\n",
      "epoch:27 step:25729 [D loss: 0.198331, acc.: 71.09%] [G loss: 0.450743]\n",
      "epoch:27 step:25730 [D loss: 0.237690, acc.: 60.94%] [G loss: 0.441216]\n",
      "epoch:27 step:25731 [D loss: 0.244764, acc.: 57.03%] [G loss: 0.438328]\n",
      "epoch:27 step:25732 [D loss: 0.210839, acc.: 65.62%] [G loss: 0.407035]\n",
      "epoch:27 step:25733 [D loss: 0.219637, acc.: 63.28%] [G loss: 0.461578]\n",
      "epoch:27 step:25734 [D loss: 0.199669, acc.: 73.44%] [G loss: 0.446449]\n",
      "epoch:27 step:25735 [D loss: 0.192524, acc.: 77.34%] [G loss: 0.474177]\n",
      "epoch:27 step:25736 [D loss: 0.262138, acc.: 55.47%] [G loss: 0.422465]\n",
      "epoch:27 step:25737 [D loss: 0.230291, acc.: 59.38%] [G loss: 0.432924]\n",
      "epoch:27 step:25738 [D loss: 0.219367, acc.: 61.72%] [G loss: 0.408357]\n",
      "epoch:27 step:25739 [D loss: 0.206463, acc.: 67.19%] [G loss: 0.442860]\n",
      "epoch:27 step:25740 [D loss: 0.245873, acc.: 54.69%] [G loss: 0.428529]\n",
      "epoch:27 step:25741 [D loss: 0.217048, acc.: 64.06%] [G loss: 0.472222]\n",
      "epoch:27 step:25742 [D loss: 0.238991, acc.: 65.62%] [G loss: 0.424824]\n",
      "epoch:27 step:25743 [D loss: 0.224264, acc.: 63.28%] [G loss: 0.453044]\n",
      "epoch:27 step:25744 [D loss: 0.229892, acc.: 62.50%] [G loss: 0.483473]\n",
      "epoch:27 step:25745 [D loss: 0.223325, acc.: 61.72%] [G loss: 0.455655]\n",
      "epoch:27 step:25746 [D loss: 0.197746, acc.: 72.66%] [G loss: 0.454557]\n",
      "epoch:27 step:25747 [D loss: 0.242918, acc.: 50.78%] [G loss: 0.470240]\n",
      "epoch:27 step:25748 [D loss: 0.233546, acc.: 58.59%] [G loss: 0.399251]\n",
      "epoch:27 step:25749 [D loss: 0.212231, acc.: 66.41%] [G loss: 0.460852]\n",
      "epoch:27 step:25750 [D loss: 0.187188, acc.: 72.66%] [G loss: 0.455901]\n",
      "epoch:27 step:25751 [D loss: 0.195312, acc.: 71.88%] [G loss: 0.481583]\n",
      "epoch:27 step:25752 [D loss: 0.219593, acc.: 61.72%] [G loss: 0.398500]\n",
      "epoch:27 step:25753 [D loss: 0.205430, acc.: 69.53%] [G loss: 0.450294]\n",
      "epoch:27 step:25754 [D loss: 0.226018, acc.: 66.41%] [G loss: 0.428859]\n",
      "epoch:27 step:25755 [D loss: 0.217873, acc.: 66.41%] [G loss: 0.409054]\n",
      "epoch:27 step:25756 [D loss: 0.211269, acc.: 64.84%] [G loss: 0.436224]\n",
      "epoch:27 step:25757 [D loss: 0.273185, acc.: 49.22%] [G loss: 0.427840]\n",
      "epoch:27 step:25758 [D loss: 0.217287, acc.: 60.94%] [G loss: 0.469967]\n",
      "epoch:27 step:25759 [D loss: 0.260529, acc.: 53.91%] [G loss: 0.384168]\n",
      "epoch:27 step:25760 [D loss: 0.220250, acc.: 61.72%] [G loss: 0.443696]\n",
      "epoch:27 step:25761 [D loss: 0.223959, acc.: 60.16%] [G loss: 0.415714]\n",
      "epoch:27 step:25762 [D loss: 0.226032, acc.: 61.72%] [G loss: 0.403651]\n",
      "epoch:27 step:25763 [D loss: 0.226821, acc.: 58.59%] [G loss: 0.418891]\n",
      "epoch:27 step:25764 [D loss: 0.218855, acc.: 69.53%] [G loss: 0.439758]\n",
      "epoch:27 step:25765 [D loss: 0.231080, acc.: 65.62%] [G loss: 0.398124]\n",
      "epoch:27 step:25766 [D loss: 0.202675, acc.: 74.22%] [G loss: 0.439590]\n",
      "epoch:27 step:25767 [D loss: 0.250989, acc.: 61.72%] [G loss: 0.400525]\n",
      "epoch:27 step:25768 [D loss: 0.225524, acc.: 62.50%] [G loss: 0.456905]\n",
      "epoch:27 step:25769 [D loss: 0.228445, acc.: 61.72%] [G loss: 0.450131]\n",
      "epoch:27 step:25770 [D loss: 0.169049, acc.: 78.91%] [G loss: 0.502007]\n",
      "epoch:27 step:25771 [D loss: 0.194415, acc.: 69.53%] [G loss: 0.439505]\n",
      "epoch:27 step:25772 [D loss: 0.257177, acc.: 52.34%] [G loss: 0.460892]\n",
      "epoch:27 step:25773 [D loss: 0.218688, acc.: 59.38%] [G loss: 0.459748]\n",
      "epoch:27 step:25774 [D loss: 0.197667, acc.: 69.53%] [G loss: 0.471592]\n",
      "epoch:27 step:25775 [D loss: 0.246694, acc.: 58.59%] [G loss: 0.413165]\n",
      "epoch:27 step:25776 [D loss: 0.245083, acc.: 56.25%] [G loss: 0.432749]\n",
      "epoch:27 step:25777 [D loss: 0.233075, acc.: 57.81%] [G loss: 0.388181]\n",
      "epoch:27 step:25778 [D loss: 0.240986, acc.: 59.38%] [G loss: 0.395978]\n",
      "epoch:27 step:25779 [D loss: 0.203971, acc.: 71.88%] [G loss: 0.453613]\n",
      "epoch:27 step:25780 [D loss: 0.180125, acc.: 75.78%] [G loss: 0.467315]\n",
      "epoch:27 step:25781 [D loss: 0.251555, acc.: 55.47%] [G loss: 0.411951]\n",
      "epoch:27 step:25782 [D loss: 0.218092, acc.: 63.28%] [G loss: 0.395492]\n",
      "epoch:27 step:25783 [D loss: 0.192248, acc.: 71.09%] [G loss: 0.421605]\n",
      "epoch:27 step:25784 [D loss: 0.192452, acc.: 71.88%] [G loss: 0.468112]\n",
      "epoch:27 step:25785 [D loss: 0.231010, acc.: 60.16%] [G loss: 0.423339]\n",
      "epoch:27 step:25786 [D loss: 0.242344, acc.: 56.25%] [G loss: 0.379773]\n",
      "epoch:27 step:25787 [D loss: 0.197381, acc.: 73.44%] [G loss: 0.446110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25788 [D loss: 0.250628, acc.: 54.69%] [G loss: 0.428227]\n",
      "epoch:27 step:25789 [D loss: 0.242435, acc.: 62.50%] [G loss: 0.417306]\n",
      "epoch:27 step:25790 [D loss: 0.223447, acc.: 61.72%] [G loss: 0.428540]\n",
      "epoch:27 step:25791 [D loss: 0.218451, acc.: 67.97%] [G loss: 0.429977]\n",
      "epoch:27 step:25792 [D loss: 0.217809, acc.: 64.84%] [G loss: 0.423443]\n",
      "epoch:27 step:25793 [D loss: 0.227426, acc.: 58.59%] [G loss: 0.423348]\n",
      "epoch:27 step:25794 [D loss: 0.199518, acc.: 67.19%] [G loss: 0.460381]\n",
      "epoch:27 step:25795 [D loss: 0.218442, acc.: 63.28%] [G loss: 0.447275]\n",
      "epoch:27 step:25796 [D loss: 0.212594, acc.: 64.84%] [G loss: 0.457481]\n",
      "epoch:27 step:25797 [D loss: 0.215694, acc.: 69.53%] [G loss: 0.442493]\n",
      "epoch:27 step:25798 [D loss: 0.170917, acc.: 78.91%] [G loss: 0.466695]\n",
      "epoch:27 step:25799 [D loss: 0.239294, acc.: 63.28%] [G loss: 0.438284]\n",
      "epoch:27 step:25800 [D loss: 0.265182, acc.: 54.69%] [G loss: 0.407014]\n",
      "##############\n",
      "[2.82545103 2.01097777 5.88126935 5.02424214 3.59961092 5.62196289\n",
      " 4.35494615 4.65295378 4.52151392 4.11624266]\n",
      "##########\n",
      "epoch:27 step:25801 [D loss: 0.225149, acc.: 64.84%] [G loss: 0.419098]\n",
      "epoch:27 step:25802 [D loss: 0.221967, acc.: 61.72%] [G loss: 0.395896]\n",
      "epoch:27 step:25803 [D loss: 0.198393, acc.: 72.66%] [G loss: 0.379059]\n",
      "epoch:27 step:25804 [D loss: 0.212803, acc.: 65.62%] [G loss: 0.447438]\n",
      "epoch:27 step:25805 [D loss: 0.208443, acc.: 68.75%] [G loss: 0.431631]\n",
      "epoch:27 step:25806 [D loss: 0.225986, acc.: 63.28%] [G loss: 0.463411]\n",
      "epoch:27 step:25807 [D loss: 0.203914, acc.: 70.31%] [G loss: 0.474144]\n",
      "epoch:27 step:25808 [D loss: 0.257469, acc.: 57.81%] [G loss: 0.418938]\n",
      "epoch:27 step:25809 [D loss: 0.228557, acc.: 64.06%] [G loss: 0.441581]\n",
      "epoch:27 step:25810 [D loss: 0.230375, acc.: 61.72%] [G loss: 0.444366]\n",
      "epoch:27 step:25811 [D loss: 0.226488, acc.: 62.50%] [G loss: 0.428628]\n",
      "epoch:27 step:25812 [D loss: 0.212121, acc.: 64.06%] [G loss: 0.424063]\n",
      "epoch:27 step:25813 [D loss: 0.201428, acc.: 71.09%] [G loss: 0.410812]\n",
      "epoch:27 step:25814 [D loss: 0.228416, acc.: 67.97%] [G loss: 0.483242]\n",
      "epoch:27 step:25815 [D loss: 0.199848, acc.: 72.66%] [G loss: 0.469269]\n",
      "epoch:27 step:25816 [D loss: 0.242784, acc.: 62.50%] [G loss: 0.447347]\n",
      "epoch:27 step:25817 [D loss: 0.260429, acc.: 50.00%] [G loss: 0.407313]\n",
      "epoch:27 step:25818 [D loss: 0.198834, acc.: 68.75%] [G loss: 0.442456]\n",
      "epoch:27 step:25819 [D loss: 0.201504, acc.: 66.41%] [G loss: 0.458333]\n",
      "epoch:27 step:25820 [D loss: 0.207073, acc.: 69.53%] [G loss: 0.420704]\n",
      "epoch:27 step:25821 [D loss: 0.195219, acc.: 70.31%] [G loss: 0.485927]\n",
      "epoch:27 step:25822 [D loss: 0.203333, acc.: 67.19%] [G loss: 0.492651]\n",
      "epoch:27 step:25823 [D loss: 0.240512, acc.: 58.59%] [G loss: 0.436121]\n",
      "epoch:27 step:25824 [D loss: 0.228696, acc.: 64.06%] [G loss: 0.436890]\n",
      "epoch:27 step:25825 [D loss: 0.224627, acc.: 59.38%] [G loss: 0.493686]\n",
      "epoch:27 step:25826 [D loss: 0.223945, acc.: 64.06%] [G loss: 0.447773]\n",
      "epoch:27 step:25827 [D loss: 0.285888, acc.: 44.53%] [G loss: 0.424299]\n",
      "epoch:27 step:25828 [D loss: 0.220455, acc.: 65.62%] [G loss: 0.460329]\n",
      "epoch:27 step:25829 [D loss: 0.214607, acc.: 67.19%] [G loss: 0.419299]\n",
      "epoch:27 step:25830 [D loss: 0.242445, acc.: 62.50%] [G loss: 0.409210]\n",
      "epoch:27 step:25831 [D loss: 0.215716, acc.: 65.62%] [G loss: 0.379080]\n",
      "epoch:27 step:25832 [D loss: 0.231607, acc.: 64.84%] [G loss: 0.421806]\n",
      "epoch:27 step:25833 [D loss: 0.213970, acc.: 58.59%] [G loss: 0.430060]\n",
      "epoch:27 step:25834 [D loss: 0.247254, acc.: 56.25%] [G loss: 0.438374]\n",
      "epoch:27 step:25835 [D loss: 0.214845, acc.: 64.06%] [G loss: 0.409102]\n",
      "epoch:27 step:25836 [D loss: 0.245905, acc.: 53.12%] [G loss: 0.435679]\n",
      "epoch:27 step:25837 [D loss: 0.239132, acc.: 57.03%] [G loss: 0.393163]\n",
      "epoch:27 step:25838 [D loss: 0.235422, acc.: 57.81%] [G loss: 0.426709]\n",
      "epoch:27 step:25839 [D loss: 0.216392, acc.: 67.97%] [G loss: 0.412986]\n",
      "epoch:27 step:25840 [D loss: 0.233750, acc.: 54.69%] [G loss: 0.401874]\n",
      "epoch:27 step:25841 [D loss: 0.260499, acc.: 54.69%] [G loss: 0.393549]\n",
      "epoch:27 step:25842 [D loss: 0.229433, acc.: 62.50%] [G loss: 0.382758]\n",
      "epoch:27 step:25843 [D loss: 0.232838, acc.: 57.81%] [G loss: 0.403969]\n",
      "epoch:27 step:25844 [D loss: 0.203001, acc.: 67.97%] [G loss: 0.411845]\n",
      "epoch:27 step:25845 [D loss: 0.223750, acc.: 61.72%] [G loss: 0.447355]\n",
      "epoch:27 step:25846 [D loss: 0.226006, acc.: 61.72%] [G loss: 0.402652]\n",
      "epoch:27 step:25847 [D loss: 0.219138, acc.: 62.50%] [G loss: 0.407096]\n",
      "epoch:27 step:25848 [D loss: 0.190617, acc.: 75.00%] [G loss: 0.450747]\n",
      "epoch:27 step:25849 [D loss: 0.184690, acc.: 74.22%] [G loss: 0.480657]\n",
      "epoch:27 step:25850 [D loss: 0.209148, acc.: 66.41%] [G loss: 0.447330]\n",
      "epoch:27 step:25851 [D loss: 0.202219, acc.: 71.88%] [G loss: 0.462938]\n",
      "epoch:27 step:25852 [D loss: 0.244297, acc.: 56.25%] [G loss: 0.422720]\n",
      "epoch:27 step:25853 [D loss: 0.228045, acc.: 60.94%] [G loss: 0.458229]\n",
      "epoch:27 step:25854 [D loss: 0.194999, acc.: 74.22%] [G loss: 0.461941]\n",
      "epoch:27 step:25855 [D loss: 0.213947, acc.: 63.28%] [G loss: 0.448262]\n",
      "epoch:27 step:25856 [D loss: 0.221405, acc.: 61.72%] [G loss: 0.463678]\n",
      "epoch:27 step:25857 [D loss: 0.212664, acc.: 61.72%] [G loss: 0.429827]\n",
      "epoch:27 step:25858 [D loss: 0.279040, acc.: 53.12%] [G loss: 0.432685]\n",
      "epoch:27 step:25859 [D loss: 0.234995, acc.: 60.94%] [G loss: 0.425663]\n",
      "epoch:27 step:25860 [D loss: 0.226238, acc.: 62.50%] [G loss: 0.456184]\n",
      "epoch:27 step:25861 [D loss: 0.225578, acc.: 62.50%] [G loss: 0.390967]\n",
      "epoch:27 step:25862 [D loss: 0.207509, acc.: 64.84%] [G loss: 0.417196]\n",
      "epoch:27 step:25863 [D loss: 0.189037, acc.: 75.78%] [G loss: 0.492794]\n",
      "epoch:27 step:25864 [D loss: 0.222808, acc.: 64.06%] [G loss: 0.452551]\n",
      "epoch:27 step:25865 [D loss: 0.256993, acc.: 55.47%] [G loss: 0.444683]\n",
      "epoch:27 step:25866 [D loss: 0.226201, acc.: 63.28%] [G loss: 0.473856]\n",
      "epoch:27 step:25867 [D loss: 0.215187, acc.: 67.97%] [G loss: 0.435570]\n",
      "epoch:27 step:25868 [D loss: 0.247847, acc.: 63.28%] [G loss: 0.421175]\n",
      "epoch:27 step:25869 [D loss: 0.208424, acc.: 65.62%] [G loss: 0.422775]\n",
      "epoch:27 step:25870 [D loss: 0.237500, acc.: 59.38%] [G loss: 0.417229]\n",
      "epoch:27 step:25871 [D loss: 0.239936, acc.: 60.16%] [G loss: 0.421010]\n",
      "epoch:27 step:25872 [D loss: 0.230569, acc.: 60.16%] [G loss: 0.425508]\n",
      "epoch:27 step:25873 [D loss: 0.201469, acc.: 70.31%] [G loss: 0.452245]\n",
      "epoch:27 step:25874 [D loss: 0.217250, acc.: 66.41%] [G loss: 0.482745]\n",
      "epoch:27 step:25875 [D loss: 0.233740, acc.: 59.38%] [G loss: 0.450915]\n",
      "epoch:27 step:25876 [D loss: 0.207752, acc.: 69.53%] [G loss: 0.435758]\n",
      "epoch:27 step:25877 [D loss: 0.222544, acc.: 62.50%] [G loss: 0.438485]\n",
      "epoch:27 step:25878 [D loss: 0.230360, acc.: 63.28%] [G loss: 0.454593]\n",
      "epoch:27 step:25879 [D loss: 0.234171, acc.: 60.94%] [G loss: 0.441116]\n",
      "epoch:27 step:25880 [D loss: 0.221047, acc.: 64.06%] [G loss: 0.400692]\n",
      "epoch:27 step:25881 [D loss: 0.186996, acc.: 68.75%] [G loss: 0.487951]\n",
      "epoch:27 step:25882 [D loss: 0.245800, acc.: 58.59%] [G loss: 0.449103]\n",
      "epoch:27 step:25883 [D loss: 0.237011, acc.: 57.81%] [G loss: 0.415974]\n",
      "epoch:27 step:25884 [D loss: 0.215446, acc.: 67.19%] [G loss: 0.426037]\n",
      "epoch:27 step:25885 [D loss: 0.250252, acc.: 56.25%] [G loss: 0.434627]\n",
      "epoch:27 step:25886 [D loss: 0.260422, acc.: 53.91%] [G loss: 0.416984]\n",
      "epoch:27 step:25887 [D loss: 0.236659, acc.: 59.38%] [G loss: 0.421450]\n",
      "epoch:27 step:25888 [D loss: 0.223099, acc.: 60.94%] [G loss: 0.395523]\n",
      "epoch:27 step:25889 [D loss: 0.224914, acc.: 61.72%] [G loss: 0.467385]\n",
      "epoch:27 step:25890 [D loss: 0.220027, acc.: 64.84%] [G loss: 0.427025]\n",
      "epoch:27 step:25891 [D loss: 0.210062, acc.: 71.09%] [G loss: 0.404944]\n",
      "epoch:27 step:25892 [D loss: 0.196865, acc.: 68.75%] [G loss: 0.462023]\n",
      "epoch:27 step:25893 [D loss: 0.249180, acc.: 53.91%] [G loss: 0.391722]\n",
      "epoch:27 step:25894 [D loss: 0.223861, acc.: 62.50%] [G loss: 0.393677]\n",
      "epoch:27 step:25895 [D loss: 0.230018, acc.: 56.25%] [G loss: 0.464136]\n",
      "epoch:27 step:25896 [D loss: 0.241046, acc.: 53.12%] [G loss: 0.440304]\n",
      "epoch:27 step:25897 [D loss: 0.235053, acc.: 59.38%] [G loss: 0.413028]\n",
      "epoch:27 step:25898 [D loss: 0.208759, acc.: 66.41%] [G loss: 0.434641]\n",
      "epoch:27 step:25899 [D loss: 0.233752, acc.: 63.28%] [G loss: 0.398577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25900 [D loss: 0.228591, acc.: 64.06%] [G loss: 0.432143]\n",
      "epoch:27 step:25901 [D loss: 0.241243, acc.: 59.38%] [G loss: 0.421223]\n",
      "epoch:27 step:25902 [D loss: 0.226379, acc.: 64.06%] [G loss: 0.430477]\n",
      "epoch:27 step:25903 [D loss: 0.215041, acc.: 64.84%] [G loss: 0.428032]\n",
      "epoch:27 step:25904 [D loss: 0.192478, acc.: 71.09%] [G loss: 0.450616]\n",
      "epoch:27 step:25905 [D loss: 0.232980, acc.: 60.16%] [G loss: 0.447033]\n",
      "epoch:27 step:25906 [D loss: 0.223444, acc.: 61.72%] [G loss: 0.403413]\n",
      "epoch:27 step:25907 [D loss: 0.206466, acc.: 64.84%] [G loss: 0.460205]\n",
      "epoch:27 step:25908 [D loss: 0.198989, acc.: 71.88%] [G loss: 0.467097]\n",
      "epoch:27 step:25909 [D loss: 0.245638, acc.: 53.91%] [G loss: 0.421459]\n",
      "epoch:27 step:25910 [D loss: 0.223167, acc.: 60.94%] [G loss: 0.409645]\n",
      "epoch:27 step:25911 [D loss: 0.256790, acc.: 60.16%] [G loss: 0.368069]\n",
      "epoch:27 step:25912 [D loss: 0.223553, acc.: 64.06%] [G loss: 0.425295]\n",
      "epoch:27 step:25913 [D loss: 0.254182, acc.: 54.69%] [G loss: 0.408858]\n",
      "epoch:27 step:25914 [D loss: 0.249708, acc.: 57.03%] [G loss: 0.412433]\n",
      "epoch:27 step:25915 [D loss: 0.208243, acc.: 67.97%] [G loss: 0.435108]\n",
      "epoch:27 step:25916 [D loss: 0.224245, acc.: 67.97%] [G loss: 0.412307]\n",
      "epoch:27 step:25917 [D loss: 0.220717, acc.: 70.31%] [G loss: 0.396273]\n",
      "epoch:27 step:25918 [D loss: 0.236066, acc.: 59.38%] [G loss: 0.389308]\n",
      "epoch:27 step:25919 [D loss: 0.200888, acc.: 70.31%] [G loss: 0.451640]\n",
      "epoch:27 step:25920 [D loss: 0.239050, acc.: 58.59%] [G loss: 0.424541]\n",
      "epoch:27 step:25921 [D loss: 0.220296, acc.: 63.28%] [G loss: 0.474889]\n",
      "epoch:27 step:25922 [D loss: 0.222588, acc.: 63.28%] [G loss: 0.443010]\n",
      "epoch:27 step:25923 [D loss: 0.205801, acc.: 63.28%] [G loss: 0.430211]\n",
      "epoch:27 step:25924 [D loss: 0.235406, acc.: 59.38%] [G loss: 0.420566]\n",
      "epoch:27 step:25925 [D loss: 0.233243, acc.: 60.94%] [G loss: 0.430435]\n",
      "epoch:27 step:25926 [D loss: 0.213110, acc.: 64.84%] [G loss: 0.438807]\n",
      "epoch:27 step:25927 [D loss: 0.224150, acc.: 66.41%] [G loss: 0.429826]\n",
      "epoch:27 step:25928 [D loss: 0.221546, acc.: 66.41%] [G loss: 0.396535]\n",
      "epoch:27 step:25929 [D loss: 0.248510, acc.: 53.12%] [G loss: 0.410373]\n",
      "epoch:27 step:25930 [D loss: 0.200190, acc.: 74.22%] [G loss: 0.449045]\n",
      "epoch:27 step:25931 [D loss: 0.201036, acc.: 70.31%] [G loss: 0.473650]\n",
      "epoch:27 step:25932 [D loss: 0.215376, acc.: 64.84%] [G loss: 0.453869]\n",
      "epoch:27 step:25933 [D loss: 0.206187, acc.: 67.19%] [G loss: 0.417078]\n",
      "epoch:27 step:25934 [D loss: 0.229461, acc.: 64.06%] [G loss: 0.471803]\n",
      "epoch:27 step:25935 [D loss: 0.228458, acc.: 58.59%] [G loss: 0.413219]\n",
      "epoch:27 step:25936 [D loss: 0.206333, acc.: 69.53%] [G loss: 0.474773]\n",
      "epoch:27 step:25937 [D loss: 0.220654, acc.: 63.28%] [G loss: 0.456111]\n",
      "epoch:27 step:25938 [D loss: 0.230341, acc.: 58.59%] [G loss: 0.453984]\n",
      "epoch:27 step:25939 [D loss: 0.222622, acc.: 55.47%] [G loss: 0.419836]\n",
      "epoch:27 step:25940 [D loss: 0.188566, acc.: 75.00%] [G loss: 0.513950]\n",
      "epoch:27 step:25941 [D loss: 0.192462, acc.: 76.56%] [G loss: 0.496668]\n",
      "epoch:27 step:25942 [D loss: 0.239970, acc.: 57.03%] [G loss: 0.489302]\n",
      "epoch:27 step:25943 [D loss: 0.239416, acc.: 56.25%] [G loss: 0.435204]\n",
      "epoch:27 step:25944 [D loss: 0.235881, acc.: 57.03%] [G loss: 0.409376]\n",
      "epoch:27 step:25945 [D loss: 0.215055, acc.: 65.62%] [G loss: 0.458232]\n",
      "epoch:27 step:25946 [D loss: 0.202378, acc.: 70.31%] [G loss: 0.427420]\n",
      "epoch:27 step:25947 [D loss: 0.180910, acc.: 74.22%] [G loss: 0.485134]\n",
      "epoch:27 step:25948 [D loss: 0.216840, acc.: 65.62%] [G loss: 0.479165]\n",
      "epoch:27 step:25949 [D loss: 0.215907, acc.: 67.97%] [G loss: 0.469489]\n",
      "epoch:27 step:25950 [D loss: 0.224119, acc.: 63.28%] [G loss: 0.465389]\n",
      "epoch:27 step:25951 [D loss: 0.230844, acc.: 59.38%] [G loss: 0.433926]\n",
      "epoch:27 step:25952 [D loss: 0.209667, acc.: 71.09%] [G loss: 0.473341]\n",
      "epoch:27 step:25953 [D loss: 0.198505, acc.: 67.97%] [G loss: 0.453529]\n",
      "epoch:27 step:25954 [D loss: 0.232190, acc.: 60.94%] [G loss: 0.425806]\n",
      "epoch:27 step:25955 [D loss: 0.228212, acc.: 62.50%] [G loss: 0.446482]\n",
      "epoch:27 step:25956 [D loss: 0.221730, acc.: 65.62%] [G loss: 0.420948]\n",
      "epoch:27 step:25957 [D loss: 0.244918, acc.: 59.38%] [G loss: 0.430784]\n",
      "epoch:27 step:25958 [D loss: 0.216259, acc.: 66.41%] [G loss: 0.443743]\n",
      "epoch:27 step:25959 [D loss: 0.223528, acc.: 60.94%] [G loss: 0.417068]\n",
      "epoch:27 step:25960 [D loss: 0.199451, acc.: 67.97%] [G loss: 0.433546]\n",
      "epoch:27 step:25961 [D loss: 0.242279, acc.: 62.50%] [G loss: 0.451298]\n",
      "epoch:27 step:25962 [D loss: 0.273044, acc.: 44.53%] [G loss: 0.402587]\n",
      "epoch:27 step:25963 [D loss: 0.214950, acc.: 65.62%] [G loss: 0.450351]\n",
      "epoch:27 step:25964 [D loss: 0.205150, acc.: 69.53%] [G loss: 0.457182]\n",
      "epoch:27 step:25965 [D loss: 0.203143, acc.: 67.19%] [G loss: 0.434765]\n",
      "epoch:27 step:25966 [D loss: 0.242968, acc.: 57.03%] [G loss: 0.413546]\n",
      "epoch:27 step:25967 [D loss: 0.229665, acc.: 63.28%] [G loss: 0.409336]\n",
      "epoch:27 step:25968 [D loss: 0.228476, acc.: 62.50%] [G loss: 0.386623]\n",
      "epoch:27 step:25969 [D loss: 0.236176, acc.: 59.38%] [G loss: 0.408064]\n",
      "epoch:27 step:25970 [D loss: 0.230605, acc.: 62.50%] [G loss: 0.425421]\n",
      "epoch:27 step:25971 [D loss: 0.235691, acc.: 58.59%] [G loss: 0.415412]\n",
      "epoch:27 step:25972 [D loss: 0.229646, acc.: 60.94%] [G loss: 0.411085]\n",
      "epoch:27 step:25973 [D loss: 0.205254, acc.: 67.19%] [G loss: 0.400182]\n",
      "epoch:27 step:25974 [D loss: 0.243464, acc.: 57.81%] [G loss: 0.438116]\n",
      "epoch:27 step:25975 [D loss: 0.227788, acc.: 60.94%] [G loss: 0.430974]\n",
      "epoch:27 step:25976 [D loss: 0.204779, acc.: 69.53%] [G loss: 0.416051]\n",
      "epoch:27 step:25977 [D loss: 0.218947, acc.: 61.72%] [G loss: 0.419877]\n",
      "epoch:27 step:25978 [D loss: 0.215805, acc.: 67.97%] [G loss: 0.431684]\n",
      "epoch:27 step:25979 [D loss: 0.215658, acc.: 67.19%] [G loss: 0.426265]\n",
      "epoch:27 step:25980 [D loss: 0.205451, acc.: 71.88%] [G loss: 0.419396]\n",
      "epoch:27 step:25981 [D loss: 0.246094, acc.: 55.47%] [G loss: 0.423728]\n",
      "epoch:27 step:25982 [D loss: 0.222121, acc.: 64.06%] [G loss: 0.420820]\n",
      "epoch:27 step:25983 [D loss: 0.231149, acc.: 56.25%] [G loss: 0.386159]\n",
      "epoch:27 step:25984 [D loss: 0.212191, acc.: 69.53%] [G loss: 0.433574]\n",
      "epoch:27 step:25985 [D loss: 0.233556, acc.: 59.38%] [G loss: 0.414436]\n",
      "epoch:27 step:25986 [D loss: 0.236529, acc.: 56.25%] [G loss: 0.439798]\n",
      "epoch:27 step:25987 [D loss: 0.217575, acc.: 59.38%] [G loss: 0.416130]\n",
      "epoch:27 step:25988 [D loss: 0.207077, acc.: 70.31%] [G loss: 0.479563]\n",
      "epoch:27 step:25989 [D loss: 0.191327, acc.: 68.75%] [G loss: 0.445689]\n",
      "epoch:27 step:25990 [D loss: 0.216273, acc.: 64.84%] [G loss: 0.415729]\n",
      "epoch:27 step:25991 [D loss: 0.223912, acc.: 65.62%] [G loss: 0.457777]\n",
      "epoch:27 step:25992 [D loss: 0.185620, acc.: 77.34%] [G loss: 0.462937]\n",
      "epoch:27 step:25993 [D loss: 0.187892, acc.: 73.44%] [G loss: 0.444904]\n",
      "epoch:27 step:25994 [D loss: 0.212134, acc.: 70.31%] [G loss: 0.469246]\n",
      "epoch:27 step:25995 [D loss: 0.249623, acc.: 55.47%] [G loss: 0.405888]\n",
      "epoch:27 step:25996 [D loss: 0.227315, acc.: 60.94%] [G loss: 0.441539]\n",
      "epoch:27 step:25997 [D loss: 0.245655, acc.: 54.69%] [G loss: 0.449177]\n",
      "epoch:27 step:25998 [D loss: 0.215070, acc.: 67.19%] [G loss: 0.448020]\n",
      "epoch:27 step:25999 [D loss: 0.199506, acc.: 65.62%] [G loss: 0.478193]\n",
      "epoch:27 step:26000 [D loss: 0.208556, acc.: 64.06%] [G loss: 0.506169]\n",
      "##############\n",
      "[2.62423485 1.61425676 5.90093215 4.58695672 3.59993641 5.56052777\n",
      " 4.57195597 4.7909176  4.30374006 4.31050734]\n",
      "##########\n",
      "epoch:27 step:26001 [D loss: 0.246752, acc.: 62.50%] [G loss: 0.485971]\n",
      "epoch:27 step:26002 [D loss: 0.249555, acc.: 57.81%] [G loss: 0.441825]\n",
      "epoch:27 step:26003 [D loss: 0.265699, acc.: 47.66%] [G loss: 0.434249]\n",
      "epoch:27 step:26004 [D loss: 0.212815, acc.: 67.97%] [G loss: 0.448537]\n",
      "epoch:27 step:26005 [D loss: 0.223195, acc.: 64.06%] [G loss: 0.414641]\n",
      "epoch:27 step:26006 [D loss: 0.235371, acc.: 60.94%] [G loss: 0.421044]\n",
      "epoch:27 step:26007 [D loss: 0.230333, acc.: 65.62%] [G loss: 0.416262]\n",
      "epoch:27 step:26008 [D loss: 0.196779, acc.: 70.31%] [G loss: 0.457282]\n",
      "epoch:27 step:26009 [D loss: 0.260891, acc.: 54.69%] [G loss: 0.433840]\n",
      "epoch:27 step:26010 [D loss: 0.215731, acc.: 65.62%] [G loss: 0.455473]\n",
      "epoch:27 step:26011 [D loss: 0.238055, acc.: 57.81%] [G loss: 0.455299]\n",
      "epoch:27 step:26012 [D loss: 0.214037, acc.: 66.41%] [G loss: 0.485513]\n",
      "epoch:27 step:26013 [D loss: 0.205838, acc.: 64.84%] [G loss: 0.444947]\n",
      "epoch:27 step:26014 [D loss: 0.228812, acc.: 60.94%] [G loss: 0.447955]\n",
      "epoch:27 step:26015 [D loss: 0.242866, acc.: 54.69%] [G loss: 0.400644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26016 [D loss: 0.222849, acc.: 60.16%] [G loss: 0.419060]\n",
      "epoch:27 step:26017 [D loss: 0.213079, acc.: 62.50%] [G loss: 0.387324]\n",
      "epoch:27 step:26018 [D loss: 0.206156, acc.: 71.09%] [G loss: 0.424703]\n",
      "epoch:27 step:26019 [D loss: 0.204343, acc.: 64.84%] [G loss: 0.460700]\n",
      "epoch:27 step:26020 [D loss: 0.237365, acc.: 59.38%] [G loss: 0.421540]\n",
      "epoch:27 step:26021 [D loss: 0.252026, acc.: 47.66%] [G loss: 0.429388]\n",
      "epoch:27 step:26022 [D loss: 0.218333, acc.: 65.62%] [G loss: 0.438252]\n",
      "epoch:27 step:26023 [D loss: 0.207296, acc.: 66.41%] [G loss: 0.421926]\n",
      "epoch:27 step:26024 [D loss: 0.204431, acc.: 68.75%] [G loss: 0.444850]\n",
      "epoch:27 step:26025 [D loss: 0.207912, acc.: 68.75%] [G loss: 0.455710]\n",
      "epoch:27 step:26026 [D loss: 0.228950, acc.: 60.16%] [G loss: 0.439201]\n",
      "epoch:27 step:26027 [D loss: 0.251860, acc.: 53.91%] [G loss: 0.400053]\n",
      "epoch:27 step:26028 [D loss: 0.206632, acc.: 68.75%] [G loss: 0.428989]\n",
      "epoch:27 step:26029 [D loss: 0.216678, acc.: 62.50%] [G loss: 0.423574]\n",
      "epoch:27 step:26030 [D loss: 0.245485, acc.: 57.03%] [G loss: 0.425431]\n",
      "epoch:27 step:26031 [D loss: 0.217065, acc.: 66.41%] [G loss: 0.466181]\n",
      "epoch:27 step:26032 [D loss: 0.209542, acc.: 64.06%] [G loss: 0.465710]\n",
      "epoch:27 step:26033 [D loss: 0.252637, acc.: 56.25%] [G loss: 0.420797]\n",
      "epoch:27 step:26034 [D loss: 0.221765, acc.: 61.72%] [G loss: 0.416285]\n",
      "epoch:27 step:26035 [D loss: 0.198813, acc.: 72.66%] [G loss: 0.441488]\n",
      "epoch:27 step:26036 [D loss: 0.219626, acc.: 60.16%] [G loss: 0.415138]\n",
      "epoch:27 step:26037 [D loss: 0.225533, acc.: 62.50%] [G loss: 0.412294]\n",
      "epoch:27 step:26038 [D loss: 0.226077, acc.: 59.38%] [G loss: 0.437456]\n",
      "epoch:27 step:26039 [D loss: 0.233848, acc.: 60.94%] [G loss: 0.384082]\n",
      "epoch:27 step:26040 [D loss: 0.226290, acc.: 60.16%] [G loss: 0.432796]\n",
      "epoch:27 step:26041 [D loss: 0.211684, acc.: 60.94%] [G loss: 0.461081]\n",
      "epoch:27 step:26042 [D loss: 0.213259, acc.: 64.84%] [G loss: 0.468811]\n",
      "epoch:27 step:26043 [D loss: 0.222184, acc.: 63.28%] [G loss: 0.404242]\n",
      "epoch:27 step:26044 [D loss: 0.240963, acc.: 56.25%] [G loss: 0.422708]\n",
      "epoch:27 step:26045 [D loss: 0.248792, acc.: 56.25%] [G loss: 0.384907]\n",
      "epoch:27 step:26046 [D loss: 0.197933, acc.: 71.88%] [G loss: 0.422810]\n",
      "epoch:27 step:26047 [D loss: 0.228079, acc.: 64.06%] [G loss: 0.407980]\n",
      "epoch:27 step:26048 [D loss: 0.218832, acc.: 67.19%] [G loss: 0.422794]\n",
      "epoch:27 step:26049 [D loss: 0.221710, acc.: 64.84%] [G loss: 0.399940]\n",
      "epoch:27 step:26050 [D loss: 0.229406, acc.: 59.38%] [G loss: 0.431273]\n",
      "epoch:27 step:26051 [D loss: 0.241563, acc.: 59.38%] [G loss: 0.379542]\n",
      "epoch:27 step:26052 [D loss: 0.226055, acc.: 60.16%] [G loss: 0.417580]\n",
      "epoch:27 step:26053 [D loss: 0.235288, acc.: 60.16%] [G loss: 0.419200]\n",
      "epoch:27 step:26054 [D loss: 0.222501, acc.: 63.28%] [G loss: 0.465634]\n",
      "epoch:27 step:26055 [D loss: 0.235978, acc.: 57.03%] [G loss: 0.428324]\n",
      "epoch:27 step:26056 [D loss: 0.236419, acc.: 57.81%] [G loss: 0.429011]\n",
      "epoch:27 step:26057 [D loss: 0.240077, acc.: 60.94%] [G loss: 0.375651]\n",
      "epoch:27 step:26058 [D loss: 0.234448, acc.: 60.16%] [G loss: 0.424650]\n",
      "epoch:27 step:26059 [D loss: 0.239393, acc.: 57.81%] [G loss: 0.422266]\n",
      "epoch:27 step:26060 [D loss: 0.227872, acc.: 62.50%] [G loss: 0.444585]\n",
      "epoch:27 step:26061 [D loss: 0.233931, acc.: 61.72%] [G loss: 0.428257]\n",
      "epoch:27 step:26062 [D loss: 0.240943, acc.: 63.28%] [G loss: 0.407852]\n",
      "epoch:27 step:26063 [D loss: 0.219703, acc.: 66.41%] [G loss: 0.439986]\n",
      "epoch:27 step:26064 [D loss: 0.242306, acc.: 61.72%] [G loss: 0.401882]\n",
      "epoch:27 step:26065 [D loss: 0.226500, acc.: 61.72%] [G loss: 0.459318]\n",
      "epoch:27 step:26066 [D loss: 0.215820, acc.: 62.50%] [G loss: 0.444831]\n",
      "epoch:27 step:26067 [D loss: 0.244400, acc.: 62.50%] [G loss: 0.440046]\n",
      "epoch:27 step:26068 [D loss: 0.211802, acc.: 68.75%] [G loss: 0.472841]\n",
      "epoch:27 step:26069 [D loss: 0.216176, acc.: 62.50%] [G loss: 0.422272]\n",
      "epoch:27 step:26070 [D loss: 0.227030, acc.: 63.28%] [G loss: 0.422601]\n",
      "epoch:27 step:26071 [D loss: 0.234746, acc.: 58.59%] [G loss: 0.403808]\n",
      "epoch:27 step:26072 [D loss: 0.221617, acc.: 60.94%] [G loss: 0.415589]\n",
      "epoch:27 step:26073 [D loss: 0.213819, acc.: 64.06%] [G loss: 0.473896]\n",
      "epoch:27 step:26074 [D loss: 0.204723, acc.: 66.41%] [G loss: 0.484066]\n",
      "epoch:27 step:26075 [D loss: 0.226320, acc.: 61.72%] [G loss: 0.425595]\n",
      "epoch:27 step:26076 [D loss: 0.203019, acc.: 67.97%] [G loss: 0.454986]\n",
      "epoch:27 step:26077 [D loss: 0.227148, acc.: 60.94%] [G loss: 0.467808]\n",
      "epoch:27 step:26078 [D loss: 0.245157, acc.: 63.28%] [G loss: 0.434005]\n",
      "epoch:27 step:26079 [D loss: 0.205278, acc.: 62.50%] [G loss: 0.446282]\n",
      "epoch:27 step:26080 [D loss: 0.207448, acc.: 64.06%] [G loss: 0.457235]\n",
      "epoch:27 step:26081 [D loss: 0.193812, acc.: 73.44%] [G loss: 0.497799]\n",
      "epoch:27 step:26082 [D loss: 0.241698, acc.: 59.38%] [G loss: 0.471913]\n",
      "epoch:27 step:26083 [D loss: 0.232486, acc.: 57.03%] [G loss: 0.420824]\n",
      "epoch:27 step:26084 [D loss: 0.235835, acc.: 62.50%] [G loss: 0.422381]\n",
      "epoch:27 step:26085 [D loss: 0.192419, acc.: 68.75%] [G loss: 0.438023]\n",
      "epoch:27 step:26086 [D loss: 0.257182, acc.: 51.56%] [G loss: 0.443996]\n",
      "epoch:27 step:26087 [D loss: 0.245770, acc.: 61.72%] [G loss: 0.418406]\n",
      "epoch:27 step:26088 [D loss: 0.240711, acc.: 63.28%] [G loss: 0.395434]\n",
      "epoch:27 step:26089 [D loss: 0.228336, acc.: 61.72%] [G loss: 0.388944]\n",
      "epoch:27 step:26090 [D loss: 0.218224, acc.: 64.84%] [G loss: 0.399915]\n",
      "epoch:27 step:26091 [D loss: 0.203970, acc.: 71.09%] [G loss: 0.434450]\n",
      "epoch:27 step:26092 [D loss: 0.211443, acc.: 64.84%] [G loss: 0.438002]\n",
      "epoch:27 step:26093 [D loss: 0.241832, acc.: 58.59%] [G loss: 0.457474]\n",
      "epoch:27 step:26094 [D loss: 0.257476, acc.: 53.91%] [G loss: 0.409964]\n",
      "epoch:27 step:26095 [D loss: 0.216651, acc.: 67.19%] [G loss: 0.457570]\n",
      "epoch:27 step:26096 [D loss: 0.222370, acc.: 60.94%] [G loss: 0.458760]\n",
      "epoch:27 step:26097 [D loss: 0.220911, acc.: 62.50%] [G loss: 0.404112]\n",
      "epoch:27 step:26098 [D loss: 0.228669, acc.: 60.94%] [G loss: 0.438173]\n",
      "epoch:27 step:26099 [D loss: 0.247512, acc.: 54.69%] [G loss: 0.376550]\n",
      "epoch:27 step:26100 [D loss: 0.209186, acc.: 67.19%] [G loss: 0.427348]\n",
      "epoch:27 step:26101 [D loss: 0.216096, acc.: 64.84%] [G loss: 0.451914]\n",
      "epoch:27 step:26102 [D loss: 0.235595, acc.: 66.41%] [G loss: 0.463346]\n",
      "epoch:27 step:26103 [D loss: 0.236915, acc.: 58.59%] [G loss: 0.398517]\n",
      "epoch:27 step:26104 [D loss: 0.221157, acc.: 64.06%] [G loss: 0.441577]\n",
      "epoch:27 step:26105 [D loss: 0.225575, acc.: 63.28%] [G loss: 0.391138]\n",
      "epoch:27 step:26106 [D loss: 0.221814, acc.: 64.84%] [G loss: 0.392743]\n",
      "epoch:27 step:26107 [D loss: 0.248590, acc.: 58.59%] [G loss: 0.373420]\n",
      "epoch:27 step:26108 [D loss: 0.219523, acc.: 62.50%] [G loss: 0.419636]\n",
      "epoch:27 step:26109 [D loss: 0.232032, acc.: 55.47%] [G loss: 0.412006]\n",
      "epoch:27 step:26110 [D loss: 0.212114, acc.: 66.41%] [G loss: 0.439637]\n",
      "epoch:27 step:26111 [D loss: 0.235880, acc.: 58.59%] [G loss: 0.456575]\n",
      "epoch:27 step:26112 [D loss: 0.194458, acc.: 71.88%] [G loss: 0.455867]\n",
      "epoch:27 step:26113 [D loss: 0.204218, acc.: 64.84%] [G loss: 0.425875]\n",
      "epoch:27 step:26114 [D loss: 0.192095, acc.: 64.84%] [G loss: 0.457395]\n",
      "epoch:27 step:26115 [D loss: 0.216878, acc.: 64.06%] [G loss: 0.425679]\n",
      "epoch:27 step:26116 [D loss: 0.255432, acc.: 59.38%] [G loss: 0.409206]\n",
      "epoch:27 step:26117 [D loss: 0.255685, acc.: 57.03%] [G loss: 0.415817]\n",
      "epoch:27 step:26118 [D loss: 0.250440, acc.: 57.03%] [G loss: 0.412572]\n",
      "epoch:27 step:26119 [D loss: 0.274924, acc.: 48.44%] [G loss: 0.414743]\n",
      "epoch:27 step:26120 [D loss: 0.240577, acc.: 53.12%] [G loss: 0.403606]\n",
      "epoch:27 step:26121 [D loss: 0.222349, acc.: 63.28%] [G loss: 0.410076]\n",
      "epoch:27 step:26122 [D loss: 0.208944, acc.: 67.19%] [G loss: 0.444646]\n",
      "epoch:27 step:26123 [D loss: 0.242328, acc.: 59.38%] [G loss: 0.419116]\n",
      "epoch:27 step:26124 [D loss: 0.209900, acc.: 67.97%] [G loss: 0.416751]\n",
      "epoch:27 step:26125 [D loss: 0.219668, acc.: 64.06%] [G loss: 0.438674]\n",
      "epoch:27 step:26126 [D loss: 0.281679, acc.: 51.56%] [G loss: 0.402150]\n",
      "epoch:27 step:26127 [D loss: 0.264758, acc.: 55.47%] [G loss: 0.415441]\n",
      "epoch:27 step:26128 [D loss: 0.242790, acc.: 58.59%] [G loss: 0.460029]\n",
      "epoch:27 step:26129 [D loss: 0.233224, acc.: 58.59%] [G loss: 0.410542]\n",
      "epoch:27 step:26130 [D loss: 0.233436, acc.: 60.16%] [G loss: 0.413552]\n",
      "epoch:27 step:26131 [D loss: 0.206413, acc.: 68.75%] [G loss: 0.371628]\n",
      "epoch:27 step:26132 [D loss: 0.198203, acc.: 71.09%] [G loss: 0.439624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26133 [D loss: 0.240914, acc.: 53.91%] [G loss: 0.420763]\n",
      "epoch:27 step:26134 [D loss: 0.241974, acc.: 56.25%] [G loss: 0.419929]\n",
      "epoch:27 step:26135 [D loss: 0.230083, acc.: 64.06%] [G loss: 0.431660]\n",
      "epoch:27 step:26136 [D loss: 0.197817, acc.: 68.75%] [G loss: 0.413510]\n",
      "epoch:27 step:26137 [D loss: 0.204304, acc.: 71.88%] [G loss: 0.427729]\n",
      "epoch:27 step:26138 [D loss: 0.208887, acc.: 66.41%] [G loss: 0.449723]\n",
      "epoch:27 step:26139 [D loss: 0.217387, acc.: 68.75%] [G loss: 0.467313]\n",
      "epoch:27 step:26140 [D loss: 0.225097, acc.: 60.16%] [G loss: 0.433727]\n",
      "epoch:27 step:26141 [D loss: 0.199288, acc.: 71.88%] [G loss: 0.414075]\n",
      "epoch:27 step:26142 [D loss: 0.241019, acc.: 57.81%] [G loss: 0.418538]\n",
      "epoch:27 step:26143 [D loss: 0.215557, acc.: 65.62%] [G loss: 0.454630]\n",
      "epoch:27 step:26144 [D loss: 0.183875, acc.: 75.00%] [G loss: 0.462269]\n",
      "epoch:27 step:26145 [D loss: 0.227009, acc.: 53.91%] [G loss: 0.399862]\n",
      "epoch:27 step:26146 [D loss: 0.233698, acc.: 60.16%] [G loss: 0.417112]\n",
      "epoch:27 step:26147 [D loss: 0.218543, acc.: 64.84%] [G loss: 0.396884]\n",
      "epoch:27 step:26148 [D loss: 0.214273, acc.: 70.31%] [G loss: 0.399073]\n",
      "epoch:27 step:26149 [D loss: 0.221962, acc.: 64.84%] [G loss: 0.448708]\n",
      "epoch:27 step:26150 [D loss: 0.231973, acc.: 57.81%] [G loss: 0.401303]\n",
      "epoch:27 step:26151 [D loss: 0.237513, acc.: 55.47%] [G loss: 0.408487]\n",
      "epoch:27 step:26152 [D loss: 0.212619, acc.: 65.62%] [G loss: 0.438492]\n",
      "epoch:27 step:26153 [D loss: 0.221904, acc.: 65.62%] [G loss: 0.438672]\n",
      "epoch:27 step:26154 [D loss: 0.221319, acc.: 64.84%] [G loss: 0.450373]\n",
      "epoch:27 step:26155 [D loss: 0.222338, acc.: 63.28%] [G loss: 0.417415]\n",
      "epoch:27 step:26156 [D loss: 0.212566, acc.: 67.97%] [G loss: 0.400265]\n",
      "epoch:27 step:26157 [D loss: 0.283342, acc.: 49.22%] [G loss: 0.396707]\n",
      "epoch:27 step:26158 [D loss: 0.253378, acc.: 52.34%] [G loss: 0.402876]\n",
      "epoch:27 step:26159 [D loss: 0.199079, acc.: 71.88%] [G loss: 0.434406]\n",
      "epoch:27 step:26160 [D loss: 0.245651, acc.: 56.25%] [G loss: 0.432330]\n",
      "epoch:27 step:26161 [D loss: 0.223739, acc.: 62.50%] [G loss: 0.435369]\n",
      "epoch:27 step:26162 [D loss: 0.228528, acc.: 63.28%] [G loss: 0.377439]\n",
      "epoch:27 step:26163 [D loss: 0.237832, acc.: 59.38%] [G loss: 0.375130]\n",
      "epoch:27 step:26164 [D loss: 0.251370, acc.: 53.12%] [G loss: 0.366264]\n",
      "epoch:27 step:26165 [D loss: 0.220573, acc.: 59.38%] [G loss: 0.423867]\n",
      "epoch:27 step:26166 [D loss: 0.234144, acc.: 59.38%] [G loss: 0.380207]\n",
      "epoch:27 step:26167 [D loss: 0.230601, acc.: 61.72%] [G loss: 0.393127]\n",
      "epoch:27 step:26168 [D loss: 0.232816, acc.: 57.03%] [G loss: 0.425484]\n",
      "epoch:27 step:26169 [D loss: 0.220518, acc.: 63.28%] [G loss: 0.416466]\n",
      "epoch:27 step:26170 [D loss: 0.204777, acc.: 64.06%] [G loss: 0.429430]\n",
      "epoch:27 step:26171 [D loss: 0.242845, acc.: 56.25%] [G loss: 0.398532]\n",
      "epoch:27 step:26172 [D loss: 0.223360, acc.: 65.62%] [G loss: 0.424718]\n",
      "epoch:27 step:26173 [D loss: 0.225416, acc.: 60.94%] [G loss: 0.424644]\n",
      "epoch:27 step:26174 [D loss: 0.218590, acc.: 67.97%] [G loss: 0.408407]\n",
      "epoch:27 step:26175 [D loss: 0.240616, acc.: 60.94%] [G loss: 0.400051]\n",
      "epoch:27 step:26176 [D loss: 0.233420, acc.: 60.94%] [G loss: 0.384400]\n",
      "epoch:27 step:26177 [D loss: 0.240039, acc.: 57.81%] [G loss: 0.398799]\n",
      "epoch:27 step:26178 [D loss: 0.228184, acc.: 61.72%] [G loss: 0.421623]\n",
      "epoch:27 step:26179 [D loss: 0.252237, acc.: 49.22%] [G loss: 0.387240]\n",
      "epoch:27 step:26180 [D loss: 0.225975, acc.: 67.97%] [G loss: 0.417066]\n",
      "epoch:27 step:26181 [D loss: 0.217051, acc.: 64.84%] [G loss: 0.432988]\n",
      "epoch:27 step:26182 [D loss: 0.229118, acc.: 59.38%] [G loss: 0.441298]\n",
      "epoch:27 step:26183 [D loss: 0.216087, acc.: 60.94%] [G loss: 0.430052]\n",
      "epoch:27 step:26184 [D loss: 0.211566, acc.: 68.75%] [G loss: 0.428581]\n",
      "epoch:27 step:26185 [D loss: 0.202326, acc.: 71.09%] [G loss: 0.459514]\n",
      "epoch:27 step:26186 [D loss: 0.240575, acc.: 64.06%] [G loss: 0.445124]\n",
      "epoch:27 step:26187 [D loss: 0.226850, acc.: 57.81%] [G loss: 0.469090]\n",
      "epoch:27 step:26188 [D loss: 0.219272, acc.: 59.38%] [G loss: 0.433209]\n",
      "epoch:27 step:26189 [D loss: 0.222383, acc.: 67.19%] [G loss: 0.414206]\n",
      "epoch:27 step:26190 [D loss: 0.252651, acc.: 57.81%] [G loss: 0.412283]\n",
      "epoch:27 step:26191 [D loss: 0.243066, acc.: 57.81%] [G loss: 0.410085]\n",
      "epoch:27 step:26192 [D loss: 0.206253, acc.: 66.41%] [G loss: 0.407754]\n",
      "epoch:27 step:26193 [D loss: 0.197884, acc.: 74.22%] [G loss: 0.443181]\n",
      "epoch:27 step:26194 [D loss: 0.222938, acc.: 62.50%] [G loss: 0.410236]\n",
      "epoch:27 step:26195 [D loss: 0.217321, acc.: 64.06%] [G loss: 0.474075]\n",
      "epoch:27 step:26196 [D loss: 0.201638, acc.: 68.75%] [G loss: 0.438301]\n",
      "epoch:27 step:26197 [D loss: 0.183127, acc.: 78.91%] [G loss: 0.479373]\n",
      "epoch:27 step:26198 [D loss: 0.184825, acc.: 77.34%] [G loss: 0.478956]\n",
      "epoch:27 step:26199 [D loss: 0.204092, acc.: 67.97%] [G loss: 0.465445]\n",
      "epoch:27 step:26200 [D loss: 0.201466, acc.: 68.75%] [G loss: 0.481851]\n",
      "##############\n",
      "[2.71081631 1.9266683  6.01852114 4.62903205 3.75979364 5.85572683\n",
      " 4.72774819 4.81308005 4.5041812  4.163058  ]\n",
      "##########\n",
      "epoch:27 step:26201 [D loss: 0.235466, acc.: 60.16%] [G loss: 0.428198]\n",
      "epoch:27 step:26202 [D loss: 0.210492, acc.: 69.53%] [G loss: 0.408458]\n",
      "epoch:27 step:26203 [D loss: 0.210170, acc.: 68.75%] [G loss: 0.397622]\n",
      "epoch:27 step:26204 [D loss: 0.223415, acc.: 67.97%] [G loss: 0.429175]\n",
      "epoch:27 step:26205 [D loss: 0.222056, acc.: 63.28%] [G loss: 0.446997]\n",
      "epoch:27 step:26206 [D loss: 0.227099, acc.: 61.72%] [G loss: 0.448977]\n",
      "epoch:27 step:26207 [D loss: 0.197359, acc.: 69.53%] [G loss: 0.449422]\n",
      "epoch:27 step:26208 [D loss: 0.194946, acc.: 74.22%] [G loss: 0.449662]\n",
      "epoch:27 step:26209 [D loss: 0.214148, acc.: 64.84%] [G loss: 0.470224]\n",
      "epoch:27 step:26210 [D loss: 0.220877, acc.: 60.16%] [G loss: 0.441763]\n",
      "epoch:27 step:26211 [D loss: 0.217852, acc.: 68.75%] [G loss: 0.435393]\n",
      "epoch:27 step:26212 [D loss: 0.217864, acc.: 61.72%] [G loss: 0.426205]\n",
      "epoch:27 step:26213 [D loss: 0.231301, acc.: 60.16%] [G loss: 0.454038]\n",
      "epoch:27 step:26214 [D loss: 0.270713, acc.: 55.47%] [G loss: 0.401644]\n",
      "epoch:27 step:26215 [D loss: 0.202792, acc.: 71.88%] [G loss: 0.433719]\n",
      "epoch:27 step:26216 [D loss: 0.225251, acc.: 60.94%] [G loss: 0.416685]\n",
      "epoch:27 step:26217 [D loss: 0.165569, acc.: 77.34%] [G loss: 0.475925]\n",
      "epoch:27 step:26218 [D loss: 0.193377, acc.: 71.09%] [G loss: 0.469481]\n",
      "epoch:27 step:26219 [D loss: 0.274433, acc.: 49.22%] [G loss: 0.458518]\n",
      "epoch:27 step:26220 [D loss: 0.202700, acc.: 67.19%] [G loss: 0.481017]\n",
      "epoch:27 step:26221 [D loss: 0.242344, acc.: 58.59%] [G loss: 0.409374]\n",
      "epoch:27 step:26222 [D loss: 0.209749, acc.: 64.06%] [G loss: 0.432905]\n",
      "epoch:27 step:26223 [D loss: 0.158734, acc.: 82.03%] [G loss: 0.456790]\n",
      "epoch:27 step:26224 [D loss: 0.190731, acc.: 73.44%] [G loss: 0.446301]\n",
      "epoch:27 step:26225 [D loss: 0.202130, acc.: 75.78%] [G loss: 0.495704]\n",
      "epoch:27 step:26226 [D loss: 0.202950, acc.: 62.50%] [G loss: 0.528127]\n",
      "epoch:27 step:26227 [D loss: 0.311857, acc.: 54.69%] [G loss: 0.555313]\n",
      "epoch:27 step:26228 [D loss: 0.244562, acc.: 61.72%] [G loss: 0.530557]\n",
      "epoch:27 step:26229 [D loss: 0.231072, acc.: 71.09%] [G loss: 0.539259]\n",
      "epoch:27 step:26230 [D loss: 0.257418, acc.: 57.03%] [G loss: 0.399473]\n",
      "epoch:27 step:26231 [D loss: 0.297175, acc.: 46.88%] [G loss: 0.405105]\n",
      "epoch:27 step:26232 [D loss: 0.266592, acc.: 58.59%] [G loss: 0.403301]\n",
      "epoch:27 step:26233 [D loss: 0.229841, acc.: 61.72%] [G loss: 0.437016]\n",
      "epoch:27 step:26234 [D loss: 0.201946, acc.: 68.75%] [G loss: 0.447763]\n",
      "epoch:27 step:26235 [D loss: 0.184126, acc.: 71.09%] [G loss: 0.507166]\n",
      "epoch:27 step:26236 [D loss: 0.195028, acc.: 69.53%] [G loss: 0.542127]\n",
      "epoch:28 step:26237 [D loss: 0.260452, acc.: 56.25%] [G loss: 0.498972]\n",
      "epoch:28 step:26238 [D loss: 0.248048, acc.: 57.81%] [G loss: 0.455085]\n",
      "epoch:28 step:26239 [D loss: 0.246831, acc.: 59.38%] [G loss: 0.455126]\n",
      "epoch:28 step:26240 [D loss: 0.217744, acc.: 62.50%] [G loss: 0.489886]\n",
      "epoch:28 step:26241 [D loss: 0.208462, acc.: 67.97%] [G loss: 0.441509]\n",
      "epoch:28 step:26242 [D loss: 0.225100, acc.: 65.62%] [G loss: 0.455166]\n",
      "epoch:28 step:26243 [D loss: 0.205084, acc.: 66.41%] [G loss: 0.465624]\n",
      "epoch:28 step:26244 [D loss: 0.222120, acc.: 65.62%] [G loss: 0.427960]\n",
      "epoch:28 step:26245 [D loss: 0.196262, acc.: 69.53%] [G loss: 0.458830]\n",
      "epoch:28 step:26246 [D loss: 0.216285, acc.: 68.75%] [G loss: 0.401483]\n",
      "epoch:28 step:26247 [D loss: 0.198165, acc.: 72.66%] [G loss: 0.439225]\n",
      "epoch:28 step:26248 [D loss: 0.245796, acc.: 63.28%] [G loss: 0.428362]\n",
      "epoch:28 step:26249 [D loss: 0.202407, acc.: 70.31%] [G loss: 0.426640]\n",
      "epoch:28 step:26250 [D loss: 0.214225, acc.: 67.19%] [G loss: 0.412462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26251 [D loss: 0.200991, acc.: 70.31%] [G loss: 0.448747]\n",
      "epoch:28 step:26252 [D loss: 0.188060, acc.: 70.31%] [G loss: 0.463066]\n",
      "epoch:28 step:26253 [D loss: 0.229146, acc.: 57.81%] [G loss: 0.441695]\n",
      "epoch:28 step:26254 [D loss: 0.213734, acc.: 67.97%] [G loss: 0.425695]\n",
      "epoch:28 step:26255 [D loss: 0.266007, acc.: 52.34%] [G loss: 0.461434]\n",
      "epoch:28 step:26256 [D loss: 0.263411, acc.: 58.59%] [G loss: 0.470348]\n",
      "epoch:28 step:26257 [D loss: 0.214745, acc.: 64.06%] [G loss: 0.519022]\n",
      "epoch:28 step:26258 [D loss: 0.202988, acc.: 67.97%] [G loss: 0.484655]\n",
      "epoch:28 step:26259 [D loss: 0.275116, acc.: 50.78%] [G loss: 0.406968]\n",
      "epoch:28 step:26260 [D loss: 0.217561, acc.: 68.75%] [G loss: 0.419655]\n",
      "epoch:28 step:26261 [D loss: 0.231359, acc.: 60.94%] [G loss: 0.422175]\n",
      "epoch:28 step:26262 [D loss: 0.238171, acc.: 57.81%] [G loss: 0.418133]\n",
      "epoch:28 step:26263 [D loss: 0.211788, acc.: 62.50%] [G loss: 0.404226]\n",
      "epoch:28 step:26264 [D loss: 0.216069, acc.: 67.19%] [G loss: 0.416506]\n",
      "epoch:28 step:26265 [D loss: 0.228639, acc.: 64.84%] [G loss: 0.425449]\n",
      "epoch:28 step:26266 [D loss: 0.230763, acc.: 60.16%] [G loss: 0.402868]\n",
      "epoch:28 step:26267 [D loss: 0.248133, acc.: 58.59%] [G loss: 0.433366]\n",
      "epoch:28 step:26268 [D loss: 0.233891, acc.: 58.59%] [G loss: 0.440260]\n",
      "epoch:28 step:26269 [D loss: 0.233463, acc.: 65.62%] [G loss: 0.414202]\n",
      "epoch:28 step:26270 [D loss: 0.270620, acc.: 48.44%] [G loss: 0.401604]\n",
      "epoch:28 step:26271 [D loss: 0.224150, acc.: 60.16%] [G loss: 0.440190]\n",
      "epoch:28 step:26272 [D loss: 0.210946, acc.: 63.28%] [G loss: 0.453630]\n",
      "epoch:28 step:26273 [D loss: 0.253251, acc.: 55.47%] [G loss: 0.404245]\n",
      "epoch:28 step:26274 [D loss: 0.246343, acc.: 56.25%] [G loss: 0.390550]\n",
      "epoch:28 step:26275 [D loss: 0.228036, acc.: 66.41%] [G loss: 0.374049]\n",
      "epoch:28 step:26276 [D loss: 0.205362, acc.: 69.53%] [G loss: 0.434990]\n",
      "epoch:28 step:26277 [D loss: 0.235424, acc.: 59.38%] [G loss: 0.443165]\n",
      "epoch:28 step:26278 [D loss: 0.207852, acc.: 66.41%] [G loss: 0.438043]\n",
      "epoch:28 step:26279 [D loss: 0.241002, acc.: 61.72%] [G loss: 0.402255]\n",
      "epoch:28 step:26280 [D loss: 0.240774, acc.: 57.03%] [G loss: 0.423426]\n",
      "epoch:28 step:26281 [D loss: 0.232442, acc.: 62.50%] [G loss: 0.407996]\n",
      "epoch:28 step:26282 [D loss: 0.232464, acc.: 62.50%] [G loss: 0.417198]\n",
      "epoch:28 step:26283 [D loss: 0.224738, acc.: 64.06%] [G loss: 0.424921]\n",
      "epoch:28 step:26284 [D loss: 0.206602, acc.: 67.97%] [G loss: 0.432266]\n",
      "epoch:28 step:26285 [D loss: 0.218430, acc.: 66.41%] [G loss: 0.434229]\n",
      "epoch:28 step:26286 [D loss: 0.211483, acc.: 63.28%] [G loss: 0.437526]\n",
      "epoch:28 step:26287 [D loss: 0.240373, acc.: 62.50%] [G loss: 0.428381]\n",
      "epoch:28 step:26288 [D loss: 0.222338, acc.: 64.84%] [G loss: 0.437245]\n",
      "epoch:28 step:26289 [D loss: 0.209117, acc.: 61.72%] [G loss: 0.448828]\n",
      "epoch:28 step:26290 [D loss: 0.212106, acc.: 63.28%] [G loss: 0.452579]\n",
      "epoch:28 step:26291 [D loss: 0.221023, acc.: 60.16%] [G loss: 0.445819]\n",
      "epoch:28 step:26292 [D loss: 0.229244, acc.: 57.81%] [G loss: 0.443768]\n",
      "epoch:28 step:26293 [D loss: 0.236239, acc.: 57.03%] [G loss: 0.424535]\n",
      "epoch:28 step:26294 [D loss: 0.235768, acc.: 60.16%] [G loss: 0.450576]\n",
      "epoch:28 step:26295 [D loss: 0.198047, acc.: 68.75%] [G loss: 0.464263]\n",
      "epoch:28 step:26296 [D loss: 0.261461, acc.: 55.47%] [G loss: 0.390554]\n",
      "epoch:28 step:26297 [D loss: 0.238823, acc.: 55.47%] [G loss: 0.387664]\n",
      "epoch:28 step:26298 [D loss: 0.239220, acc.: 62.50%] [G loss: 0.380094]\n",
      "epoch:28 step:26299 [D loss: 0.221651, acc.: 63.28%] [G loss: 0.422347]\n",
      "epoch:28 step:26300 [D loss: 0.210504, acc.: 66.41%] [G loss: 0.435449]\n",
      "epoch:28 step:26301 [D loss: 0.226103, acc.: 58.59%] [G loss: 0.410034]\n",
      "epoch:28 step:26302 [D loss: 0.243685, acc.: 58.59%] [G loss: 0.376490]\n",
      "epoch:28 step:26303 [D loss: 0.235983, acc.: 62.50%] [G loss: 0.412441]\n",
      "epoch:28 step:26304 [D loss: 0.202744, acc.: 71.88%] [G loss: 0.460631]\n",
      "epoch:28 step:26305 [D loss: 0.179458, acc.: 76.56%] [G loss: 0.432484]\n",
      "epoch:28 step:26306 [D loss: 0.216516, acc.: 64.06%] [G loss: 0.413216]\n",
      "epoch:28 step:26307 [D loss: 0.239623, acc.: 59.38%] [G loss: 0.402953]\n",
      "epoch:28 step:26308 [D loss: 0.215968, acc.: 64.84%] [G loss: 0.438829]\n",
      "epoch:28 step:26309 [D loss: 0.226699, acc.: 59.38%] [G loss: 0.412732]\n",
      "epoch:28 step:26310 [D loss: 0.212525, acc.: 65.62%] [G loss: 0.420266]\n",
      "epoch:28 step:26311 [D loss: 0.211007, acc.: 64.84%] [G loss: 0.445706]\n",
      "epoch:28 step:26312 [D loss: 0.195984, acc.: 72.66%] [G loss: 0.455733]\n",
      "epoch:28 step:26313 [D loss: 0.214796, acc.: 60.16%] [G loss: 0.493918]\n",
      "epoch:28 step:26314 [D loss: 0.250938, acc.: 53.91%] [G loss: 0.415260]\n",
      "epoch:28 step:26315 [D loss: 0.227556, acc.: 62.50%] [G loss: 0.435935]\n",
      "epoch:28 step:26316 [D loss: 0.223078, acc.: 63.28%] [G loss: 0.396613]\n",
      "epoch:28 step:26317 [D loss: 0.215753, acc.: 65.62%] [G loss: 0.394505]\n",
      "epoch:28 step:26318 [D loss: 0.217139, acc.: 66.41%] [G loss: 0.429169]\n",
      "epoch:28 step:26319 [D loss: 0.221074, acc.: 65.62%] [G loss: 0.410188]\n",
      "epoch:28 step:26320 [D loss: 0.218814, acc.: 62.50%] [G loss: 0.460481]\n",
      "epoch:28 step:26321 [D loss: 0.233107, acc.: 57.03%] [G loss: 0.481480]\n",
      "epoch:28 step:26322 [D loss: 0.235308, acc.: 59.38%] [G loss: 0.461565]\n",
      "epoch:28 step:26323 [D loss: 0.226508, acc.: 58.59%] [G loss: 0.405625]\n",
      "epoch:28 step:26324 [D loss: 0.199771, acc.: 69.53%] [G loss: 0.406444]\n",
      "epoch:28 step:26325 [D loss: 0.206329, acc.: 73.44%] [G loss: 0.445584]\n",
      "epoch:28 step:26326 [D loss: 0.194520, acc.: 70.31%] [G loss: 0.417106]\n",
      "epoch:28 step:26327 [D loss: 0.205524, acc.: 67.97%] [G loss: 0.444971]\n",
      "epoch:28 step:26328 [D loss: 0.209694, acc.: 67.19%] [G loss: 0.445864]\n",
      "epoch:28 step:26329 [D loss: 0.205922, acc.: 64.06%] [G loss: 0.501069]\n",
      "epoch:28 step:26330 [D loss: 0.209120, acc.: 64.06%] [G loss: 0.476937]\n",
      "epoch:28 step:26331 [D loss: 0.239958, acc.: 58.59%] [G loss: 0.392904]\n",
      "epoch:28 step:26332 [D loss: 0.236309, acc.: 60.94%] [G loss: 0.393754]\n",
      "epoch:28 step:26333 [D loss: 0.208407, acc.: 64.84%] [G loss: 0.471822]\n",
      "epoch:28 step:26334 [D loss: 0.217819, acc.: 64.84%] [G loss: 0.464502]\n",
      "epoch:28 step:26335 [D loss: 0.242948, acc.: 58.59%] [G loss: 0.432692]\n",
      "epoch:28 step:26336 [D loss: 0.180412, acc.: 80.47%] [G loss: 0.460490]\n",
      "epoch:28 step:26337 [D loss: 0.238090, acc.: 57.03%] [G loss: 0.395158]\n",
      "epoch:28 step:26338 [D loss: 0.260185, acc.: 53.91%] [G loss: 0.412456]\n",
      "epoch:28 step:26339 [D loss: 0.227712, acc.: 62.50%] [G loss: 0.411372]\n",
      "epoch:28 step:26340 [D loss: 0.232568, acc.: 60.94%] [G loss: 0.419046]\n",
      "epoch:28 step:26341 [D loss: 0.224373, acc.: 59.38%] [G loss: 0.437051]\n",
      "epoch:28 step:26342 [D loss: 0.193605, acc.: 75.78%] [G loss: 0.429099]\n",
      "epoch:28 step:26343 [D loss: 0.210123, acc.: 71.09%] [G loss: 0.452440]\n",
      "epoch:28 step:26344 [D loss: 0.240337, acc.: 63.28%] [G loss: 0.464042]\n",
      "epoch:28 step:26345 [D loss: 0.252883, acc.: 58.59%] [G loss: 0.470305]\n",
      "epoch:28 step:26346 [D loss: 0.237104, acc.: 57.03%] [G loss: 0.421987]\n",
      "epoch:28 step:26347 [D loss: 0.203372, acc.: 69.53%] [G loss: 0.422364]\n",
      "epoch:28 step:26348 [D loss: 0.212593, acc.: 62.50%] [G loss: 0.407545]\n",
      "epoch:28 step:26349 [D loss: 0.193234, acc.: 73.44%] [G loss: 0.468513]\n",
      "epoch:28 step:26350 [D loss: 0.191198, acc.: 76.56%] [G loss: 0.461421]\n",
      "epoch:28 step:26351 [D loss: 0.191207, acc.: 68.75%] [G loss: 0.471609]\n",
      "epoch:28 step:26352 [D loss: 0.220716, acc.: 61.72%] [G loss: 0.468459]\n",
      "epoch:28 step:26353 [D loss: 0.200822, acc.: 72.66%] [G loss: 0.427087]\n",
      "epoch:28 step:26354 [D loss: 0.218426, acc.: 67.19%] [G loss: 0.480924]\n",
      "epoch:28 step:26355 [D loss: 0.195205, acc.: 67.19%] [G loss: 0.518247]\n",
      "epoch:28 step:26356 [D loss: 0.231090, acc.: 64.06%] [G loss: 0.496737]\n",
      "epoch:28 step:26357 [D loss: 0.241771, acc.: 57.81%] [G loss: 0.443808]\n",
      "epoch:28 step:26358 [D loss: 0.185486, acc.: 75.00%] [G loss: 0.438660]\n",
      "epoch:28 step:26359 [D loss: 0.216233, acc.: 62.50%] [G loss: 0.469652]\n",
      "epoch:28 step:26360 [D loss: 0.259211, acc.: 57.81%] [G loss: 0.473039]\n",
      "epoch:28 step:26361 [D loss: 0.233069, acc.: 66.41%] [G loss: 0.407584]\n",
      "epoch:28 step:26362 [D loss: 0.217127, acc.: 69.53%] [G loss: 0.420072]\n",
      "epoch:28 step:26363 [D loss: 0.206031, acc.: 67.19%] [G loss: 0.442244]\n",
      "epoch:28 step:26364 [D loss: 0.239838, acc.: 57.03%] [G loss: 0.407687]\n",
      "epoch:28 step:26365 [D loss: 0.225436, acc.: 68.75%] [G loss: 0.407365]\n",
      "epoch:28 step:26366 [D loss: 0.209179, acc.: 65.62%] [G loss: 0.397576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26367 [D loss: 0.203102, acc.: 67.97%] [G loss: 0.423736]\n",
      "epoch:28 step:26368 [D loss: 0.205825, acc.: 66.41%] [G loss: 0.406239]\n",
      "epoch:28 step:26369 [D loss: 0.255431, acc.: 53.91%] [G loss: 0.394586]\n",
      "epoch:28 step:26370 [D loss: 0.252263, acc.: 51.56%] [G loss: 0.443562]\n",
      "epoch:28 step:26371 [D loss: 0.220059, acc.: 60.94%] [G loss: 0.470036]\n",
      "epoch:28 step:26372 [D loss: 0.197207, acc.: 76.56%] [G loss: 0.472453]\n",
      "epoch:28 step:26373 [D loss: 0.236268, acc.: 60.16%] [G loss: 0.417478]\n",
      "epoch:28 step:26374 [D loss: 0.261554, acc.: 53.91%] [G loss: 0.381423]\n",
      "epoch:28 step:26375 [D loss: 0.209756, acc.: 63.28%] [G loss: 0.405178]\n",
      "epoch:28 step:26376 [D loss: 0.239890, acc.: 58.59%] [G loss: 0.404040]\n",
      "epoch:28 step:26377 [D loss: 0.242123, acc.: 57.03%] [G loss: 0.429084]\n",
      "epoch:28 step:26378 [D loss: 0.237456, acc.: 60.94%] [G loss: 0.406531]\n",
      "epoch:28 step:26379 [D loss: 0.246729, acc.: 57.03%] [G loss: 0.407622]\n",
      "epoch:28 step:26380 [D loss: 0.219296, acc.: 67.19%] [G loss: 0.435823]\n",
      "epoch:28 step:26381 [D loss: 0.200924, acc.: 71.09%] [G loss: 0.436337]\n",
      "epoch:28 step:26382 [D loss: 0.253173, acc.: 59.38%] [G loss: 0.423881]\n",
      "epoch:28 step:26383 [D loss: 0.240459, acc.: 53.91%] [G loss: 0.444776]\n",
      "epoch:28 step:26384 [D loss: 0.235830, acc.: 62.50%] [G loss: 0.424336]\n",
      "epoch:28 step:26385 [D loss: 0.213409, acc.: 69.53%] [G loss: 0.408581]\n",
      "epoch:28 step:26386 [D loss: 0.235654, acc.: 57.03%] [G loss: 0.428791]\n",
      "epoch:28 step:26387 [D loss: 0.204725, acc.: 70.31%] [G loss: 0.421854]\n",
      "epoch:28 step:26388 [D loss: 0.233887, acc.: 59.38%] [G loss: 0.411993]\n",
      "epoch:28 step:26389 [D loss: 0.241617, acc.: 50.78%] [G loss: 0.386411]\n",
      "epoch:28 step:26390 [D loss: 0.234268, acc.: 59.38%] [G loss: 0.404207]\n",
      "epoch:28 step:26391 [D loss: 0.240253, acc.: 57.03%] [G loss: 0.389674]\n",
      "epoch:28 step:26392 [D loss: 0.232094, acc.: 57.03%] [G loss: 0.396180]\n",
      "epoch:28 step:26393 [D loss: 0.212367, acc.: 68.75%] [G loss: 0.458914]\n",
      "epoch:28 step:26394 [D loss: 0.204336, acc.: 70.31%] [G loss: 0.405542]\n",
      "epoch:28 step:26395 [D loss: 0.223653, acc.: 64.06%] [G loss: 0.413730]\n",
      "epoch:28 step:26396 [D loss: 0.261638, acc.: 58.59%] [G loss: 0.412667]\n",
      "epoch:28 step:26397 [D loss: 0.251223, acc.: 54.69%] [G loss: 0.472982]\n",
      "epoch:28 step:26398 [D loss: 0.251266, acc.: 57.03%] [G loss: 0.441344]\n",
      "epoch:28 step:26399 [D loss: 0.221565, acc.: 61.72%] [G loss: 0.422416]\n",
      "epoch:28 step:26400 [D loss: 0.237579, acc.: 63.28%] [G loss: 0.426814]\n",
      "##############\n",
      "[2.59322189 2.06539821 6.10039355 4.61239552 3.75964742 5.52685787\n",
      " 4.56404766 4.83831417 4.26438678 4.08689566]\n",
      "##########\n",
      "epoch:28 step:26401 [D loss: 0.217265, acc.: 65.62%] [G loss: 0.468307]\n",
      "epoch:28 step:26402 [D loss: 0.228192, acc.: 60.94%] [G loss: 0.439246]\n",
      "epoch:28 step:26403 [D loss: 0.210793, acc.: 64.84%] [G loss: 0.425548]\n",
      "epoch:28 step:26404 [D loss: 0.223171, acc.: 63.28%] [G loss: 0.439172]\n",
      "epoch:28 step:26405 [D loss: 0.238965, acc.: 57.81%] [G loss: 0.439473]\n",
      "epoch:28 step:26406 [D loss: 0.250921, acc.: 57.03%] [G loss: 0.395596]\n",
      "epoch:28 step:26407 [D loss: 0.226215, acc.: 61.72%] [G loss: 0.394154]\n",
      "epoch:28 step:26408 [D loss: 0.228975, acc.: 59.38%] [G loss: 0.411156]\n",
      "epoch:28 step:26409 [D loss: 0.196844, acc.: 69.53%] [G loss: 0.425715]\n",
      "epoch:28 step:26410 [D loss: 0.255171, acc.: 56.25%] [G loss: 0.378780]\n",
      "epoch:28 step:26411 [D loss: 0.233360, acc.: 63.28%] [G loss: 0.427922]\n",
      "epoch:28 step:26412 [D loss: 0.222580, acc.: 60.16%] [G loss: 0.427667]\n",
      "epoch:28 step:26413 [D loss: 0.223527, acc.: 57.81%] [G loss: 0.421859]\n",
      "epoch:28 step:26414 [D loss: 0.229318, acc.: 63.28%] [G loss: 0.404490]\n",
      "epoch:28 step:26415 [D loss: 0.259179, acc.: 50.78%] [G loss: 0.408159]\n",
      "epoch:28 step:26416 [D loss: 0.242002, acc.: 57.03%] [G loss: 0.418377]\n",
      "epoch:28 step:26417 [D loss: 0.239181, acc.: 61.72%] [G loss: 0.421741]\n",
      "epoch:28 step:26418 [D loss: 0.225964, acc.: 60.94%] [G loss: 0.417448]\n",
      "epoch:28 step:26419 [D loss: 0.223512, acc.: 61.72%] [G loss: 0.417462]\n",
      "epoch:28 step:26420 [D loss: 0.215167, acc.: 65.62%] [G loss: 0.389219]\n",
      "epoch:28 step:26421 [D loss: 0.212186, acc.: 64.06%] [G loss: 0.431282]\n",
      "epoch:28 step:26422 [D loss: 0.227417, acc.: 61.72%] [G loss: 0.436648]\n",
      "epoch:28 step:26423 [D loss: 0.216709, acc.: 63.28%] [G loss: 0.437696]\n",
      "epoch:28 step:26424 [D loss: 0.231370, acc.: 60.16%] [G loss: 0.409977]\n",
      "epoch:28 step:26425 [D loss: 0.228218, acc.: 65.62%] [G loss: 0.460542]\n",
      "epoch:28 step:26426 [D loss: 0.235895, acc.: 60.16%] [G loss: 0.395806]\n",
      "epoch:28 step:26427 [D loss: 0.239237, acc.: 59.38%] [G loss: 0.409431]\n",
      "epoch:28 step:26428 [D loss: 0.218901, acc.: 64.06%] [G loss: 0.428372]\n",
      "epoch:28 step:26429 [D loss: 0.216512, acc.: 67.19%] [G loss: 0.423587]\n",
      "epoch:28 step:26430 [D loss: 0.208271, acc.: 67.19%] [G loss: 0.460197]\n",
      "epoch:28 step:26431 [D loss: 0.207013, acc.: 69.53%] [G loss: 0.420625]\n",
      "epoch:28 step:26432 [D loss: 0.218449, acc.: 65.62%] [G loss: 0.413278]\n",
      "epoch:28 step:26433 [D loss: 0.233458, acc.: 56.25%] [G loss: 0.408806]\n",
      "epoch:28 step:26434 [D loss: 0.199074, acc.: 69.53%] [G loss: 0.472391]\n",
      "epoch:28 step:26435 [D loss: 0.207295, acc.: 64.84%] [G loss: 0.460521]\n",
      "epoch:28 step:26436 [D loss: 0.243990, acc.: 54.69%] [G loss: 0.430673]\n",
      "epoch:28 step:26437 [D loss: 0.243040, acc.: 55.47%] [G loss: 0.406903]\n",
      "epoch:28 step:26438 [D loss: 0.211976, acc.: 65.62%] [G loss: 0.440212]\n",
      "epoch:28 step:26439 [D loss: 0.259299, acc.: 55.47%] [G loss: 0.426904]\n",
      "epoch:28 step:26440 [D loss: 0.219333, acc.: 66.41%] [G loss: 0.451577]\n",
      "epoch:28 step:26441 [D loss: 0.215326, acc.: 69.53%] [G loss: 0.483087]\n",
      "epoch:28 step:26442 [D loss: 0.242937, acc.: 61.72%] [G loss: 0.483092]\n",
      "epoch:28 step:26443 [D loss: 0.221573, acc.: 65.62%] [G loss: 0.465903]\n",
      "epoch:28 step:26444 [D loss: 0.191264, acc.: 70.31%] [G loss: 0.454069]\n",
      "epoch:28 step:26445 [D loss: 0.166161, acc.: 77.34%] [G loss: 0.490720]\n",
      "epoch:28 step:26446 [D loss: 0.269805, acc.: 50.78%] [G loss: 0.445369]\n",
      "epoch:28 step:26447 [D loss: 0.260659, acc.: 51.56%] [G loss: 0.402383]\n",
      "epoch:28 step:26448 [D loss: 0.247833, acc.: 58.59%] [G loss: 0.445394]\n",
      "epoch:28 step:26449 [D loss: 0.238070, acc.: 59.38%] [G loss: 0.430869]\n",
      "epoch:28 step:26450 [D loss: 0.259385, acc.: 48.44%] [G loss: 0.401695]\n",
      "epoch:28 step:26451 [D loss: 0.251978, acc.: 57.03%] [G loss: 0.354287]\n",
      "epoch:28 step:26452 [D loss: 0.184406, acc.: 72.66%] [G loss: 0.437111]\n",
      "epoch:28 step:26453 [D loss: 0.246884, acc.: 56.25%] [G loss: 0.430340]\n",
      "epoch:28 step:26454 [D loss: 0.213426, acc.: 67.19%] [G loss: 0.438976]\n",
      "epoch:28 step:26455 [D loss: 0.183378, acc.: 73.44%] [G loss: 0.466345]\n",
      "epoch:28 step:26456 [D loss: 0.272304, acc.: 52.34%] [G loss: 0.446401]\n",
      "epoch:28 step:26457 [D loss: 0.205294, acc.: 64.84%] [G loss: 0.442769]\n",
      "epoch:28 step:26458 [D loss: 0.201775, acc.: 67.19%] [G loss: 0.464834]\n",
      "epoch:28 step:26459 [D loss: 0.229236, acc.: 64.84%] [G loss: 0.460087]\n",
      "epoch:28 step:26460 [D loss: 0.227499, acc.: 56.25%] [G loss: 0.415226]\n",
      "epoch:28 step:26461 [D loss: 0.207812, acc.: 71.09%] [G loss: 0.465031]\n",
      "epoch:28 step:26462 [D loss: 0.239148, acc.: 60.16%] [G loss: 0.418539]\n",
      "epoch:28 step:26463 [D loss: 0.223828, acc.: 66.41%] [G loss: 0.386371]\n",
      "epoch:28 step:26464 [D loss: 0.232534, acc.: 63.28%] [G loss: 0.420517]\n",
      "epoch:28 step:26465 [D loss: 0.205406, acc.: 68.75%] [G loss: 0.442171]\n",
      "epoch:28 step:26466 [D loss: 0.213356, acc.: 67.97%] [G loss: 0.428034]\n",
      "epoch:28 step:26467 [D loss: 0.178304, acc.: 76.56%] [G loss: 0.467301]\n",
      "epoch:28 step:26468 [D loss: 0.157507, acc.: 78.91%] [G loss: 0.511476]\n",
      "epoch:28 step:26469 [D loss: 0.251971, acc.: 60.16%] [G loss: 0.433447]\n",
      "epoch:28 step:26470 [D loss: 0.242708, acc.: 55.47%] [G loss: 0.380665]\n",
      "epoch:28 step:26471 [D loss: 0.221855, acc.: 63.28%] [G loss: 0.429608]\n",
      "epoch:28 step:26472 [D loss: 0.210767, acc.: 66.41%] [G loss: 0.432949]\n",
      "epoch:28 step:26473 [D loss: 0.222405, acc.: 67.19%] [G loss: 0.443648]\n",
      "epoch:28 step:26474 [D loss: 0.205690, acc.: 68.75%] [G loss: 0.454393]\n",
      "epoch:28 step:26475 [D loss: 0.216963, acc.: 64.84%] [G loss: 0.428113]\n",
      "epoch:28 step:26476 [D loss: 0.233728, acc.: 60.16%] [G loss: 0.409778]\n",
      "epoch:28 step:26477 [D loss: 0.196032, acc.: 67.97%] [G loss: 0.433124]\n",
      "epoch:28 step:26478 [D loss: 0.201490, acc.: 68.75%] [G loss: 0.453842]\n",
      "epoch:28 step:26479 [D loss: 0.247845, acc.: 57.81%] [G loss: 0.454997]\n",
      "epoch:28 step:26480 [D loss: 0.222427, acc.: 62.50%] [G loss: 0.429596]\n",
      "epoch:28 step:26481 [D loss: 0.218129, acc.: 64.06%] [G loss: 0.452899]\n",
      "epoch:28 step:26482 [D loss: 0.221245, acc.: 63.28%] [G loss: 0.440754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26483 [D loss: 0.232690, acc.: 56.25%] [G loss: 0.418894]\n",
      "epoch:28 step:26484 [D loss: 0.190980, acc.: 72.66%] [G loss: 0.498308]\n",
      "epoch:28 step:26485 [D loss: 0.280658, acc.: 53.12%] [G loss: 0.424358]\n",
      "epoch:28 step:26486 [D loss: 0.254000, acc.: 53.12%] [G loss: 0.431578]\n",
      "epoch:28 step:26487 [D loss: 0.267305, acc.: 55.47%] [G loss: 0.398510]\n",
      "epoch:28 step:26488 [D loss: 0.231105, acc.: 61.72%] [G loss: 0.460648]\n",
      "epoch:28 step:26489 [D loss: 0.220143, acc.: 61.72%] [G loss: 0.436660]\n",
      "epoch:28 step:26490 [D loss: 0.245016, acc.: 57.81%] [G loss: 0.438121]\n",
      "epoch:28 step:26491 [D loss: 0.240203, acc.: 57.81%] [G loss: 0.432122]\n",
      "epoch:28 step:26492 [D loss: 0.224267, acc.: 60.16%] [G loss: 0.420760]\n",
      "epoch:28 step:26493 [D loss: 0.250390, acc.: 57.81%] [G loss: 0.391602]\n",
      "epoch:28 step:26494 [D loss: 0.203590, acc.: 69.53%] [G loss: 0.422154]\n",
      "epoch:28 step:26495 [D loss: 0.235909, acc.: 58.59%] [G loss: 0.442168]\n",
      "epoch:28 step:26496 [D loss: 0.213740, acc.: 67.97%] [G loss: 0.455639]\n",
      "epoch:28 step:26497 [D loss: 0.221435, acc.: 59.38%] [G loss: 0.469383]\n",
      "epoch:28 step:26498 [D loss: 0.199054, acc.: 63.28%] [G loss: 0.451471]\n",
      "epoch:28 step:26499 [D loss: 0.227741, acc.: 62.50%] [G loss: 0.456577]\n",
      "epoch:28 step:26500 [D loss: 0.226807, acc.: 65.62%] [G loss: 0.424425]\n",
      "epoch:28 step:26501 [D loss: 0.231992, acc.: 62.50%] [G loss: 0.423409]\n",
      "epoch:28 step:26502 [D loss: 0.234641, acc.: 57.81%] [G loss: 0.411493]\n",
      "epoch:28 step:26503 [D loss: 0.228941, acc.: 60.94%] [G loss: 0.407559]\n",
      "epoch:28 step:26504 [D loss: 0.209880, acc.: 66.41%] [G loss: 0.454162]\n",
      "epoch:28 step:26505 [D loss: 0.208803, acc.: 70.31%] [G loss: 0.421380]\n",
      "epoch:28 step:26506 [D loss: 0.219149, acc.: 63.28%] [G loss: 0.439760]\n",
      "epoch:28 step:26507 [D loss: 0.192944, acc.: 68.75%] [G loss: 0.462556]\n",
      "epoch:28 step:26508 [D loss: 0.246476, acc.: 60.16%] [G loss: 0.433736]\n",
      "epoch:28 step:26509 [D loss: 0.223041, acc.: 67.97%] [G loss: 0.443460]\n",
      "epoch:28 step:26510 [D loss: 0.187790, acc.: 73.44%] [G loss: 0.431509]\n",
      "epoch:28 step:26511 [D loss: 0.212427, acc.: 64.84%] [G loss: 0.417937]\n",
      "epoch:28 step:26512 [D loss: 0.200327, acc.: 68.75%] [G loss: 0.445598]\n",
      "epoch:28 step:26513 [D loss: 0.237450, acc.: 57.81%] [G loss: 0.462150]\n",
      "epoch:28 step:26514 [D loss: 0.226526, acc.: 62.50%] [G loss: 0.434913]\n",
      "epoch:28 step:26515 [D loss: 0.230938, acc.: 60.94%] [G loss: 0.391750]\n",
      "epoch:28 step:26516 [D loss: 0.199431, acc.: 67.97%] [G loss: 0.458699]\n",
      "epoch:28 step:26517 [D loss: 0.271370, acc.: 50.78%] [G loss: 0.430968]\n",
      "epoch:28 step:26518 [D loss: 0.247901, acc.: 53.91%] [G loss: 0.425972]\n",
      "epoch:28 step:26519 [D loss: 0.205213, acc.: 72.66%] [G loss: 0.438863]\n",
      "epoch:28 step:26520 [D loss: 0.218007, acc.: 62.50%] [G loss: 0.402392]\n",
      "epoch:28 step:26521 [D loss: 0.236763, acc.: 62.50%] [G loss: 0.424796]\n",
      "epoch:28 step:26522 [D loss: 0.226900, acc.: 60.16%] [G loss: 0.438566]\n",
      "epoch:28 step:26523 [D loss: 0.228704, acc.: 59.38%] [G loss: 0.414261]\n",
      "epoch:28 step:26524 [D loss: 0.233996, acc.: 62.50%] [G loss: 0.413822]\n",
      "epoch:28 step:26525 [D loss: 0.197931, acc.: 69.53%] [G loss: 0.485448]\n",
      "epoch:28 step:26526 [D loss: 0.228508, acc.: 58.59%] [G loss: 0.481240]\n",
      "epoch:28 step:26527 [D loss: 0.251764, acc.: 58.59%] [G loss: 0.434676]\n",
      "epoch:28 step:26528 [D loss: 0.199377, acc.: 69.53%] [G loss: 0.428081]\n",
      "epoch:28 step:26529 [D loss: 0.204987, acc.: 69.53%] [G loss: 0.472680]\n",
      "epoch:28 step:26530 [D loss: 0.248900, acc.: 57.81%] [G loss: 0.411419]\n",
      "epoch:28 step:26531 [D loss: 0.231002, acc.: 59.38%] [G loss: 0.374187]\n",
      "epoch:28 step:26532 [D loss: 0.192037, acc.: 67.19%] [G loss: 0.412325]\n",
      "epoch:28 step:26533 [D loss: 0.244328, acc.: 54.69%] [G loss: 0.391630]\n",
      "epoch:28 step:26534 [D loss: 0.191717, acc.: 80.47%] [G loss: 0.417131]\n",
      "epoch:28 step:26535 [D loss: 0.197790, acc.: 71.09%] [G loss: 0.427029]\n",
      "epoch:28 step:26536 [D loss: 0.198537, acc.: 68.75%] [G loss: 0.403730]\n",
      "epoch:28 step:26537 [D loss: 0.237820, acc.: 62.50%] [G loss: 0.403536]\n",
      "epoch:28 step:26538 [D loss: 0.204911, acc.: 68.75%] [G loss: 0.417868]\n",
      "epoch:28 step:26539 [D loss: 0.231886, acc.: 63.28%] [G loss: 0.407595]\n",
      "epoch:28 step:26540 [D loss: 0.211287, acc.: 67.19%] [G loss: 0.387145]\n",
      "epoch:28 step:26541 [D loss: 0.218697, acc.: 60.16%] [G loss: 0.393708]\n",
      "epoch:28 step:26542 [D loss: 0.236882, acc.: 59.38%] [G loss: 0.386605]\n",
      "epoch:28 step:26543 [D loss: 0.239416, acc.: 61.72%] [G loss: 0.426540]\n",
      "epoch:28 step:26544 [D loss: 0.247338, acc.: 50.78%] [G loss: 0.409708]\n",
      "epoch:28 step:26545 [D loss: 0.199314, acc.: 69.53%] [G loss: 0.424480]\n",
      "epoch:28 step:26546 [D loss: 0.214611, acc.: 63.28%] [G loss: 0.427766]\n",
      "epoch:28 step:26547 [D loss: 0.211006, acc.: 71.09%] [G loss: 0.415255]\n",
      "epoch:28 step:26548 [D loss: 0.222396, acc.: 69.53%] [G loss: 0.396865]\n",
      "epoch:28 step:26549 [D loss: 0.195569, acc.: 70.31%] [G loss: 0.483359]\n",
      "epoch:28 step:26550 [D loss: 0.174913, acc.: 75.00%] [G loss: 0.466583]\n",
      "epoch:28 step:26551 [D loss: 0.206519, acc.: 64.84%] [G loss: 0.436700]\n",
      "epoch:28 step:26552 [D loss: 0.285660, acc.: 48.44%] [G loss: 0.418553]\n",
      "epoch:28 step:26553 [D loss: 0.222406, acc.: 59.38%] [G loss: 0.449002]\n",
      "epoch:28 step:26554 [D loss: 0.224997, acc.: 67.19%] [G loss: 0.417940]\n",
      "epoch:28 step:26555 [D loss: 0.223003, acc.: 62.50%] [G loss: 0.406110]\n",
      "epoch:28 step:26556 [D loss: 0.228269, acc.: 65.62%] [G loss: 0.422716]\n",
      "epoch:28 step:26557 [D loss: 0.215160, acc.: 63.28%] [G loss: 0.427817]\n",
      "epoch:28 step:26558 [D loss: 0.219054, acc.: 67.19%] [G loss: 0.450027]\n",
      "epoch:28 step:26559 [D loss: 0.229743, acc.: 60.16%] [G loss: 0.445130]\n",
      "epoch:28 step:26560 [D loss: 0.229176, acc.: 70.31%] [G loss: 0.425524]\n",
      "epoch:28 step:26561 [D loss: 0.242363, acc.: 60.16%] [G loss: 0.395735]\n",
      "epoch:28 step:26562 [D loss: 0.202543, acc.: 69.53%] [G loss: 0.452463]\n",
      "epoch:28 step:26563 [D loss: 0.237059, acc.: 56.25%] [G loss: 0.397237]\n",
      "epoch:28 step:26564 [D loss: 0.217223, acc.: 64.84%] [G loss: 0.436942]\n",
      "epoch:28 step:26565 [D loss: 0.222158, acc.: 60.16%] [G loss: 0.402348]\n",
      "epoch:28 step:26566 [D loss: 0.216515, acc.: 64.06%] [G loss: 0.429439]\n",
      "epoch:28 step:26567 [D loss: 0.220865, acc.: 63.28%] [G loss: 0.427394]\n",
      "epoch:28 step:26568 [D loss: 0.220475, acc.: 63.28%] [G loss: 0.408904]\n",
      "epoch:28 step:26569 [D loss: 0.201828, acc.: 64.84%] [G loss: 0.448692]\n",
      "epoch:28 step:26570 [D loss: 0.220713, acc.: 61.72%] [G loss: 0.421124]\n",
      "epoch:28 step:26571 [D loss: 0.206887, acc.: 67.19%] [G loss: 0.430911]\n",
      "epoch:28 step:26572 [D loss: 0.205510, acc.: 67.97%] [G loss: 0.465689]\n",
      "epoch:28 step:26573 [D loss: 0.227074, acc.: 59.38%] [G loss: 0.413275]\n",
      "epoch:28 step:26574 [D loss: 0.187815, acc.: 75.78%] [G loss: 0.456749]\n",
      "epoch:28 step:26575 [D loss: 0.215741, acc.: 62.50%] [G loss: 0.463693]\n",
      "epoch:28 step:26576 [D loss: 0.205805, acc.: 68.75%] [G loss: 0.481022]\n",
      "epoch:28 step:26577 [D loss: 0.271123, acc.: 57.03%] [G loss: 0.432545]\n",
      "epoch:28 step:26578 [D loss: 0.236617, acc.: 59.38%] [G loss: 0.435857]\n",
      "epoch:28 step:26579 [D loss: 0.207403, acc.: 67.97%] [G loss: 0.441607]\n",
      "epoch:28 step:26580 [D loss: 0.216689, acc.: 65.62%] [G loss: 0.447343]\n",
      "epoch:28 step:26581 [D loss: 0.229215, acc.: 59.38%] [G loss: 0.445987]\n",
      "epoch:28 step:26582 [D loss: 0.184195, acc.: 67.97%] [G loss: 0.491728]\n",
      "epoch:28 step:26583 [D loss: 0.155787, acc.: 76.56%] [G loss: 0.531765]\n",
      "epoch:28 step:26584 [D loss: 0.265499, acc.: 57.03%] [G loss: 0.412215]\n",
      "epoch:28 step:26585 [D loss: 0.276877, acc.: 50.00%] [G loss: 0.393679]\n",
      "epoch:28 step:26586 [D loss: 0.239480, acc.: 57.81%] [G loss: 0.375762]\n",
      "epoch:28 step:26587 [D loss: 0.251012, acc.: 59.38%] [G loss: 0.404650]\n",
      "epoch:28 step:26588 [D loss: 0.227450, acc.: 63.28%] [G loss: 0.454140]\n",
      "epoch:28 step:26589 [D loss: 0.207046, acc.: 71.88%] [G loss: 0.460866]\n",
      "epoch:28 step:26590 [D loss: 0.198601, acc.: 64.84%] [G loss: 0.420677]\n",
      "epoch:28 step:26591 [D loss: 0.232455, acc.: 60.16%] [G loss: 0.436652]\n",
      "epoch:28 step:26592 [D loss: 0.234645, acc.: 59.38%] [G loss: 0.418737]\n",
      "epoch:28 step:26593 [D loss: 0.205448, acc.: 68.75%] [G loss: 0.419931]\n",
      "epoch:28 step:26594 [D loss: 0.186274, acc.: 74.22%] [G loss: 0.454901]\n",
      "epoch:28 step:26595 [D loss: 0.202463, acc.: 67.97%] [G loss: 0.387498]\n",
      "epoch:28 step:26596 [D loss: 0.211982, acc.: 71.09%] [G loss: 0.425376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26597 [D loss: 0.176002, acc.: 74.22%] [G loss: 0.450518]\n",
      "epoch:28 step:26598 [D loss: 0.250840, acc.: 54.69%] [G loss: 0.401511]\n",
      "epoch:28 step:26599 [D loss: 0.211482, acc.: 65.62%] [G loss: 0.412088]\n",
      "epoch:28 step:26600 [D loss: 0.201462, acc.: 64.84%] [G loss: 0.451826]\n",
      "##############\n",
      "[2.83464646 2.01170425 5.77972222 4.86108467 3.62384286 5.64318308\n",
      " 4.4201245  4.85481754 4.4879468  4.14347916]\n",
      "##########\n",
      "epoch:28 step:26601 [D loss: 0.228251, acc.: 60.94%] [G loss: 0.407605]\n",
      "epoch:28 step:26602 [D loss: 0.258483, acc.: 53.91%] [G loss: 0.414070]\n",
      "epoch:28 step:26603 [D loss: 0.213490, acc.: 67.19%] [G loss: 0.410836]\n",
      "epoch:28 step:26604 [D loss: 0.261434, acc.: 53.91%] [G loss: 0.391221]\n",
      "epoch:28 step:26605 [D loss: 0.209761, acc.: 65.62%] [G loss: 0.433973]\n",
      "epoch:28 step:26606 [D loss: 0.206341, acc.: 66.41%] [G loss: 0.427058]\n",
      "epoch:28 step:26607 [D loss: 0.191587, acc.: 68.75%] [G loss: 0.468795]\n",
      "epoch:28 step:26608 [D loss: 0.232988, acc.: 63.28%] [G loss: 0.421988]\n",
      "epoch:28 step:26609 [D loss: 0.224390, acc.: 65.62%] [G loss: 0.470402]\n",
      "epoch:28 step:26610 [D loss: 0.186716, acc.: 75.00%] [G loss: 0.420856]\n",
      "epoch:28 step:26611 [D loss: 0.261536, acc.: 57.81%] [G loss: 0.439639]\n",
      "epoch:28 step:26612 [D loss: 0.239180, acc.: 60.16%] [G loss: 0.446458]\n",
      "epoch:28 step:26613 [D loss: 0.259686, acc.: 56.25%] [G loss: 0.402174]\n",
      "epoch:28 step:26614 [D loss: 0.239111, acc.: 60.94%] [G loss: 0.414592]\n",
      "epoch:28 step:26615 [D loss: 0.232140, acc.: 56.25%] [G loss: 0.449911]\n",
      "epoch:28 step:26616 [D loss: 0.224352, acc.: 61.72%] [G loss: 0.463730]\n",
      "epoch:28 step:26617 [D loss: 0.253645, acc.: 53.91%] [G loss: 0.389254]\n",
      "epoch:28 step:26618 [D loss: 0.233683, acc.: 60.16%] [G loss: 0.388643]\n",
      "epoch:28 step:26619 [D loss: 0.193852, acc.: 75.78%] [G loss: 0.445545]\n",
      "epoch:28 step:26620 [D loss: 0.213301, acc.: 67.97%] [G loss: 0.422814]\n",
      "epoch:28 step:26621 [D loss: 0.201191, acc.: 72.66%] [G loss: 0.459388]\n",
      "epoch:28 step:26622 [D loss: 0.245912, acc.: 60.16%] [G loss: 0.459030]\n",
      "epoch:28 step:26623 [D loss: 0.205952, acc.: 66.41%] [G loss: 0.426110]\n",
      "epoch:28 step:26624 [D loss: 0.221994, acc.: 63.28%] [G loss: 0.402567]\n",
      "epoch:28 step:26625 [D loss: 0.231489, acc.: 58.59%] [G loss: 0.399198]\n",
      "epoch:28 step:26626 [D loss: 0.248033, acc.: 57.03%] [G loss: 0.426900]\n",
      "epoch:28 step:26627 [D loss: 0.227302, acc.: 67.19%] [G loss: 0.402305]\n",
      "epoch:28 step:26628 [D loss: 0.223163, acc.: 68.75%] [G loss: 0.391384]\n",
      "epoch:28 step:26629 [D loss: 0.229500, acc.: 63.28%] [G loss: 0.391975]\n",
      "epoch:28 step:26630 [D loss: 0.227620, acc.: 60.94%] [G loss: 0.435049]\n",
      "epoch:28 step:26631 [D loss: 0.217320, acc.: 65.62%] [G loss: 0.438027]\n",
      "epoch:28 step:26632 [D loss: 0.242264, acc.: 58.59%] [G loss: 0.447788]\n",
      "epoch:28 step:26633 [D loss: 0.223598, acc.: 60.16%] [G loss: 0.412876]\n",
      "epoch:28 step:26634 [D loss: 0.209792, acc.: 66.41%] [G loss: 0.440107]\n",
      "epoch:28 step:26635 [D loss: 0.195881, acc.: 69.53%] [G loss: 0.451551]\n",
      "epoch:28 step:26636 [D loss: 0.235355, acc.: 56.25%] [G loss: 0.417881]\n",
      "epoch:28 step:26637 [D loss: 0.222046, acc.: 65.62%] [G loss: 0.405319]\n",
      "epoch:28 step:26638 [D loss: 0.193177, acc.: 72.66%] [G loss: 0.409152]\n",
      "epoch:28 step:26639 [D loss: 0.239433, acc.: 56.25%] [G loss: 0.403923]\n",
      "epoch:28 step:26640 [D loss: 0.236123, acc.: 58.59%] [G loss: 0.420062]\n",
      "epoch:28 step:26641 [D loss: 0.210584, acc.: 70.31%] [G loss: 0.421393]\n",
      "epoch:28 step:26642 [D loss: 0.200795, acc.: 67.97%] [G loss: 0.456227]\n",
      "epoch:28 step:26643 [D loss: 0.227950, acc.: 60.94%] [G loss: 0.457120]\n",
      "epoch:28 step:26644 [D loss: 0.253081, acc.: 54.69%] [G loss: 0.449619]\n",
      "epoch:28 step:26645 [D loss: 0.226450, acc.: 64.84%] [G loss: 0.399775]\n",
      "epoch:28 step:26646 [D loss: 0.253810, acc.: 51.56%] [G loss: 0.394004]\n",
      "epoch:28 step:26647 [D loss: 0.253734, acc.: 60.16%] [G loss: 0.380861]\n",
      "epoch:28 step:26648 [D loss: 0.247441, acc.: 56.25%] [G loss: 0.390089]\n",
      "epoch:28 step:26649 [D loss: 0.231970, acc.: 63.28%] [G loss: 0.414920]\n",
      "epoch:28 step:26650 [D loss: 0.222289, acc.: 67.19%] [G loss: 0.455486]\n",
      "epoch:28 step:26651 [D loss: 0.232958, acc.: 61.72%] [G loss: 0.429311]\n",
      "epoch:28 step:26652 [D loss: 0.191704, acc.: 75.00%] [G loss: 0.506124]\n",
      "epoch:28 step:26653 [D loss: 0.217335, acc.: 70.31%] [G loss: 0.480711]\n",
      "epoch:28 step:26654 [D loss: 0.242452, acc.: 53.12%] [G loss: 0.425135]\n",
      "epoch:28 step:26655 [D loss: 0.246661, acc.: 61.72%] [G loss: 0.439746]\n",
      "epoch:28 step:26656 [D loss: 0.230553, acc.: 61.72%] [G loss: 0.440855]\n",
      "epoch:28 step:26657 [D loss: 0.266242, acc.: 53.12%] [G loss: 0.410426]\n",
      "epoch:28 step:26658 [D loss: 0.238278, acc.: 59.38%] [G loss: 0.397101]\n",
      "epoch:28 step:26659 [D loss: 0.229349, acc.: 62.50%] [G loss: 0.377794]\n",
      "epoch:28 step:26660 [D loss: 0.228719, acc.: 64.06%] [G loss: 0.402276]\n",
      "epoch:28 step:26661 [D loss: 0.213945, acc.: 63.28%] [G loss: 0.416993]\n",
      "epoch:28 step:26662 [D loss: 0.231276, acc.: 57.03%] [G loss: 0.427794]\n",
      "epoch:28 step:26663 [D loss: 0.192317, acc.: 69.53%] [G loss: 0.462484]\n",
      "epoch:28 step:26664 [D loss: 0.229702, acc.: 60.16%] [G loss: 0.426338]\n",
      "epoch:28 step:26665 [D loss: 0.216238, acc.: 69.53%] [G loss: 0.421412]\n",
      "epoch:28 step:26666 [D loss: 0.192155, acc.: 73.44%] [G loss: 0.445475]\n",
      "epoch:28 step:26667 [D loss: 0.232527, acc.: 62.50%] [G loss: 0.457324]\n",
      "epoch:28 step:26668 [D loss: 0.224779, acc.: 62.50%] [G loss: 0.440431]\n",
      "epoch:28 step:26669 [D loss: 0.212499, acc.: 65.62%] [G loss: 0.425615]\n",
      "epoch:28 step:26670 [D loss: 0.197199, acc.: 70.31%] [G loss: 0.440299]\n",
      "epoch:28 step:26671 [D loss: 0.208119, acc.: 68.75%] [G loss: 0.449215]\n",
      "epoch:28 step:26672 [D loss: 0.188644, acc.: 70.31%] [G loss: 0.477394]\n",
      "epoch:28 step:26673 [D loss: 0.281386, acc.: 50.00%] [G loss: 0.463159]\n",
      "epoch:28 step:26674 [D loss: 0.238152, acc.: 60.16%] [G loss: 0.467057]\n",
      "epoch:28 step:26675 [D loss: 0.238620, acc.: 62.50%] [G loss: 0.474090]\n",
      "epoch:28 step:26676 [D loss: 0.222869, acc.: 65.62%] [G loss: 0.487884]\n",
      "epoch:28 step:26677 [D loss: 0.208914, acc.: 68.75%] [G loss: 0.434534]\n",
      "epoch:28 step:26678 [D loss: 0.227860, acc.: 65.62%] [G loss: 0.414129]\n",
      "epoch:28 step:26679 [D loss: 0.237952, acc.: 56.25%] [G loss: 0.413393]\n",
      "epoch:28 step:26680 [D loss: 0.236908, acc.: 51.56%] [G loss: 0.430212]\n",
      "epoch:28 step:26681 [D loss: 0.222445, acc.: 65.62%] [G loss: 0.455565]\n",
      "epoch:28 step:26682 [D loss: 0.228540, acc.: 60.94%] [G loss: 0.420704]\n",
      "epoch:28 step:26683 [D loss: 0.192467, acc.: 66.41%] [G loss: 0.492831]\n",
      "epoch:28 step:26684 [D loss: 0.258077, acc.: 53.12%] [G loss: 0.442110]\n",
      "epoch:28 step:26685 [D loss: 0.238489, acc.: 58.59%] [G loss: 0.418633]\n",
      "epoch:28 step:26686 [D loss: 0.218863, acc.: 63.28%] [G loss: 0.420580]\n",
      "epoch:28 step:26687 [D loss: 0.194499, acc.: 67.97%] [G loss: 0.417968]\n",
      "epoch:28 step:26688 [D loss: 0.204505, acc.: 69.53%] [G loss: 0.412901]\n",
      "epoch:28 step:26689 [D loss: 0.199748, acc.: 67.97%] [G loss: 0.443901]\n",
      "epoch:28 step:26690 [D loss: 0.215053, acc.: 68.75%] [G loss: 0.432779]\n",
      "epoch:28 step:26691 [D loss: 0.230558, acc.: 63.28%] [G loss: 0.458456]\n",
      "epoch:28 step:26692 [D loss: 0.251580, acc.: 52.34%] [G loss: 0.448977]\n",
      "epoch:28 step:26693 [D loss: 0.217736, acc.: 68.75%] [G loss: 0.461373]\n",
      "epoch:28 step:26694 [D loss: 0.283111, acc.: 55.47%] [G loss: 0.392388]\n",
      "epoch:28 step:26695 [D loss: 0.216876, acc.: 65.62%] [G loss: 0.449056]\n",
      "epoch:28 step:26696 [D loss: 0.228270, acc.: 66.41%] [G loss: 0.419994]\n",
      "epoch:28 step:26697 [D loss: 0.234021, acc.: 64.06%] [G loss: 0.425177]\n",
      "epoch:28 step:26698 [D loss: 0.232571, acc.: 65.62%] [G loss: 0.454929]\n",
      "epoch:28 step:26699 [D loss: 0.221538, acc.: 64.06%] [G loss: 0.435804]\n",
      "epoch:28 step:26700 [D loss: 0.198973, acc.: 67.19%] [G loss: 0.413587]\n",
      "epoch:28 step:26701 [D loss: 0.224525, acc.: 64.84%] [G loss: 0.410435]\n",
      "epoch:28 step:26702 [D loss: 0.229454, acc.: 61.72%] [G loss: 0.426614]\n",
      "epoch:28 step:26703 [D loss: 0.227510, acc.: 56.25%] [G loss: 0.436632]\n",
      "epoch:28 step:26704 [D loss: 0.213362, acc.: 64.84%] [G loss: 0.437325]\n",
      "epoch:28 step:26705 [D loss: 0.200274, acc.: 69.53%] [G loss: 0.447951]\n",
      "epoch:28 step:26706 [D loss: 0.220265, acc.: 57.81%] [G loss: 0.441248]\n",
      "epoch:28 step:26707 [D loss: 0.186733, acc.: 71.09%] [G loss: 0.464345]\n",
      "epoch:28 step:26708 [D loss: 0.182384, acc.: 73.44%] [G loss: 0.463344]\n",
      "epoch:28 step:26709 [D loss: 0.263557, acc.: 55.47%] [G loss: 0.444065]\n",
      "epoch:28 step:26710 [D loss: 0.221049, acc.: 61.72%] [G loss: 0.407653]\n",
      "epoch:28 step:26711 [D loss: 0.209328, acc.: 64.84%] [G loss: 0.457059]\n",
      "epoch:28 step:26712 [D loss: 0.229082, acc.: 63.28%] [G loss: 0.444368]\n",
      "epoch:28 step:26713 [D loss: 0.233683, acc.: 64.06%] [G loss: 0.420021]\n",
      "epoch:28 step:26714 [D loss: 0.231519, acc.: 60.94%] [G loss: 0.386988]\n",
      "epoch:28 step:26715 [D loss: 0.213073, acc.: 65.62%] [G loss: 0.393328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26716 [D loss: 0.206501, acc.: 69.53%] [G loss: 0.403595]\n",
      "epoch:28 step:26717 [D loss: 0.189720, acc.: 76.56%] [G loss: 0.451148]\n",
      "epoch:28 step:26718 [D loss: 0.252324, acc.: 54.69%] [G loss: 0.444265]\n",
      "epoch:28 step:26719 [D loss: 0.220832, acc.: 62.50%] [G loss: 0.415172]\n",
      "epoch:28 step:26720 [D loss: 0.202749, acc.: 71.88%] [G loss: 0.467425]\n",
      "epoch:28 step:26721 [D loss: 0.209791, acc.: 71.88%] [G loss: 0.460448]\n",
      "epoch:28 step:26722 [D loss: 0.236270, acc.: 58.59%] [G loss: 0.424352]\n",
      "epoch:28 step:26723 [D loss: 0.250181, acc.: 53.12%] [G loss: 0.430675]\n",
      "epoch:28 step:26724 [D loss: 0.183135, acc.: 71.09%] [G loss: 0.416572]\n",
      "epoch:28 step:26725 [D loss: 0.236468, acc.: 62.50%] [G loss: 0.456658]\n",
      "epoch:28 step:26726 [D loss: 0.219542, acc.: 64.84%] [G loss: 0.422162]\n",
      "epoch:28 step:26727 [D loss: 0.216832, acc.: 64.06%] [G loss: 0.413805]\n",
      "epoch:28 step:26728 [D loss: 0.218217, acc.: 67.97%] [G loss: 0.401231]\n",
      "epoch:28 step:26729 [D loss: 0.215665, acc.: 61.72%] [G loss: 0.425471]\n",
      "epoch:28 step:26730 [D loss: 0.211638, acc.: 69.53%] [G loss: 0.459133]\n",
      "epoch:28 step:26731 [D loss: 0.197745, acc.: 70.31%] [G loss: 0.469276]\n",
      "epoch:28 step:26732 [D loss: 0.229443, acc.: 65.62%] [G loss: 0.454115]\n",
      "epoch:28 step:26733 [D loss: 0.248286, acc.: 53.12%] [G loss: 0.410953]\n",
      "epoch:28 step:26734 [D loss: 0.188155, acc.: 70.31%] [G loss: 0.453803]\n",
      "epoch:28 step:26735 [D loss: 0.185797, acc.: 72.66%] [G loss: 0.471478]\n",
      "epoch:28 step:26736 [D loss: 0.258566, acc.: 53.12%] [G loss: 0.436546]\n",
      "epoch:28 step:26737 [D loss: 0.264034, acc.: 56.25%] [G loss: 0.433093]\n",
      "epoch:28 step:26738 [D loss: 0.238928, acc.: 57.81%] [G loss: 0.371949]\n",
      "epoch:28 step:26739 [D loss: 0.245263, acc.: 53.12%] [G loss: 0.400264]\n",
      "epoch:28 step:26740 [D loss: 0.214012, acc.: 65.62%] [G loss: 0.406520]\n",
      "epoch:28 step:26741 [D loss: 0.210839, acc.: 67.97%] [G loss: 0.394050]\n",
      "epoch:28 step:26742 [D loss: 0.205047, acc.: 71.09%] [G loss: 0.519747]\n",
      "epoch:28 step:26743 [D loss: 0.207083, acc.: 64.84%] [G loss: 0.462497]\n",
      "epoch:28 step:26744 [D loss: 0.200613, acc.: 71.09%] [G loss: 0.527872]\n",
      "epoch:28 step:26745 [D loss: 0.246962, acc.: 57.03%] [G loss: 0.454302]\n",
      "epoch:28 step:26746 [D loss: 0.230718, acc.: 64.84%] [G loss: 0.409851]\n",
      "epoch:28 step:26747 [D loss: 0.226842, acc.: 63.28%] [G loss: 0.403271]\n",
      "epoch:28 step:26748 [D loss: 0.205603, acc.: 70.31%] [G loss: 0.425109]\n",
      "epoch:28 step:26749 [D loss: 0.201798, acc.: 70.31%] [G loss: 0.460449]\n",
      "epoch:28 step:26750 [D loss: 0.205146, acc.: 67.97%] [G loss: 0.425447]\n",
      "epoch:28 step:26751 [D loss: 0.198206, acc.: 71.09%] [G loss: 0.440214]\n",
      "epoch:28 step:26752 [D loss: 0.209617, acc.: 67.19%] [G loss: 0.429265]\n",
      "epoch:28 step:26753 [D loss: 0.247700, acc.: 53.12%] [G loss: 0.405335]\n",
      "epoch:28 step:26754 [D loss: 0.252807, acc.: 55.47%] [G loss: 0.401313]\n",
      "epoch:28 step:26755 [D loss: 0.204561, acc.: 67.97%] [G loss: 0.434530]\n",
      "epoch:28 step:26756 [D loss: 0.209389, acc.: 65.62%] [G loss: 0.427073]\n",
      "epoch:28 step:26757 [D loss: 0.204487, acc.: 65.62%] [G loss: 0.411088]\n",
      "epoch:28 step:26758 [D loss: 0.219402, acc.: 64.06%] [G loss: 0.431358]\n",
      "epoch:28 step:26759 [D loss: 0.177732, acc.: 74.22%] [G loss: 0.488520]\n",
      "epoch:28 step:26760 [D loss: 0.237878, acc.: 60.94%] [G loss: 0.401103]\n",
      "epoch:28 step:26761 [D loss: 0.213987, acc.: 70.31%] [G loss: 0.472132]\n",
      "epoch:28 step:26762 [D loss: 0.227213, acc.: 61.72%] [G loss: 0.438269]\n",
      "epoch:28 step:26763 [D loss: 0.246731, acc.: 60.94%] [G loss: 0.449975]\n",
      "epoch:28 step:26764 [D loss: 0.258380, acc.: 55.47%] [G loss: 0.449254]\n",
      "epoch:28 step:26765 [D loss: 0.257124, acc.: 55.47%] [G loss: 0.426404]\n",
      "epoch:28 step:26766 [D loss: 0.222061, acc.: 63.28%] [G loss: 0.417719]\n",
      "epoch:28 step:26767 [D loss: 0.259402, acc.: 56.25%] [G loss: 0.405778]\n",
      "epoch:28 step:26768 [D loss: 0.252036, acc.: 56.25%] [G loss: 0.402411]\n",
      "epoch:28 step:26769 [D loss: 0.228951, acc.: 58.59%] [G loss: 0.428105]\n",
      "epoch:28 step:26770 [D loss: 0.201117, acc.: 70.31%] [G loss: 0.403625]\n",
      "epoch:28 step:26771 [D loss: 0.234070, acc.: 58.59%] [G loss: 0.387919]\n",
      "epoch:28 step:26772 [D loss: 0.222056, acc.: 61.72%] [G loss: 0.424285]\n",
      "epoch:28 step:26773 [D loss: 0.250325, acc.: 57.81%] [G loss: 0.422937]\n",
      "epoch:28 step:26774 [D loss: 0.235577, acc.: 63.28%] [G loss: 0.417237]\n",
      "epoch:28 step:26775 [D loss: 0.213690, acc.: 64.06%] [G loss: 0.426050]\n",
      "epoch:28 step:26776 [D loss: 0.215295, acc.: 63.28%] [G loss: 0.402457]\n",
      "epoch:28 step:26777 [D loss: 0.208786, acc.: 67.19%] [G loss: 0.428382]\n",
      "epoch:28 step:26778 [D loss: 0.282235, acc.: 52.34%] [G loss: 0.404254]\n",
      "epoch:28 step:26779 [D loss: 0.231912, acc.: 57.81%] [G loss: 0.413374]\n",
      "epoch:28 step:26780 [D loss: 0.236812, acc.: 59.38%] [G loss: 0.400072]\n",
      "epoch:28 step:26781 [D loss: 0.230600, acc.: 60.16%] [G loss: 0.424646]\n",
      "epoch:28 step:26782 [D loss: 0.210876, acc.: 68.75%] [G loss: 0.403254]\n",
      "epoch:28 step:26783 [D loss: 0.210831, acc.: 68.75%] [G loss: 0.448880]\n",
      "epoch:28 step:26784 [D loss: 0.228291, acc.: 64.06%] [G loss: 0.415052]\n",
      "epoch:28 step:26785 [D loss: 0.188621, acc.: 71.88%] [G loss: 0.453687]\n",
      "epoch:28 step:26786 [D loss: 0.187848, acc.: 71.09%] [G loss: 0.468224]\n",
      "epoch:28 step:26787 [D loss: 0.182406, acc.: 76.56%] [G loss: 0.495928]\n",
      "epoch:28 step:26788 [D loss: 0.223233, acc.: 67.19%] [G loss: 0.456397]\n",
      "epoch:28 step:26789 [D loss: 0.222869, acc.: 66.41%] [G loss: 0.438603]\n",
      "epoch:28 step:26790 [D loss: 0.227849, acc.: 58.59%] [G loss: 0.417056]\n",
      "epoch:28 step:26791 [D loss: 0.200020, acc.: 74.22%] [G loss: 0.444149]\n",
      "epoch:28 step:26792 [D loss: 0.216273, acc.: 65.62%] [G loss: 0.454299]\n",
      "epoch:28 step:26793 [D loss: 0.199426, acc.: 70.31%] [G loss: 0.453885]\n",
      "epoch:28 step:26794 [D loss: 0.214861, acc.: 60.94%] [G loss: 0.421061]\n",
      "epoch:28 step:26795 [D loss: 0.247765, acc.: 55.47%] [G loss: 0.439708]\n",
      "epoch:28 step:26796 [D loss: 0.225623, acc.: 56.25%] [G loss: 0.420236]\n",
      "epoch:28 step:26797 [D loss: 0.218869, acc.: 66.41%] [G loss: 0.455523]\n",
      "epoch:28 step:26798 [D loss: 0.205526, acc.: 70.31%] [G loss: 0.421106]\n",
      "epoch:28 step:26799 [D loss: 0.205412, acc.: 66.41%] [G loss: 0.431092]\n",
      "epoch:28 step:26800 [D loss: 0.182788, acc.: 76.56%] [G loss: 0.413145]\n",
      "##############\n",
      "[2.60184772 2.01208143 6.00969038 4.56096072 3.70562792 5.47354272\n",
      " 4.27792299 4.86040826 4.49249812 4.09325657]\n",
      "##########\n",
      "epoch:28 step:26801 [D loss: 0.229666, acc.: 66.41%] [G loss: 0.457976]\n",
      "epoch:28 step:26802 [D loss: 0.238874, acc.: 57.81%] [G loss: 0.452237]\n",
      "epoch:28 step:26803 [D loss: 0.217444, acc.: 62.50%] [G loss: 0.435646]\n",
      "epoch:28 step:26804 [D loss: 0.215355, acc.: 67.19%] [G loss: 0.435934]\n",
      "epoch:28 step:26805 [D loss: 0.254101, acc.: 56.25%] [G loss: 0.400639]\n",
      "epoch:28 step:26806 [D loss: 0.228023, acc.: 59.38%] [G loss: 0.391309]\n",
      "epoch:28 step:26807 [D loss: 0.224535, acc.: 62.50%] [G loss: 0.419684]\n",
      "epoch:28 step:26808 [D loss: 0.208488, acc.: 61.72%] [G loss: 0.443412]\n",
      "epoch:28 step:26809 [D loss: 0.225018, acc.: 59.38%] [G loss: 0.463417]\n",
      "epoch:28 step:26810 [D loss: 0.190427, acc.: 71.88%] [G loss: 0.493964]\n",
      "epoch:28 step:26811 [D loss: 0.212101, acc.: 66.41%] [G loss: 0.488374]\n",
      "epoch:28 step:26812 [D loss: 0.234758, acc.: 59.38%] [G loss: 0.429645]\n",
      "epoch:28 step:26813 [D loss: 0.231289, acc.: 62.50%] [G loss: 0.421748]\n",
      "epoch:28 step:26814 [D loss: 0.239191, acc.: 60.16%] [G loss: 0.412189]\n",
      "epoch:28 step:26815 [D loss: 0.227768, acc.: 54.69%] [G loss: 0.425774]\n",
      "epoch:28 step:26816 [D loss: 0.247857, acc.: 56.25%] [G loss: 0.428608]\n",
      "epoch:28 step:26817 [D loss: 0.210773, acc.: 67.19%] [G loss: 0.443043]\n",
      "epoch:28 step:26818 [D loss: 0.208171, acc.: 68.75%] [G loss: 0.441349]\n",
      "epoch:28 step:26819 [D loss: 0.242557, acc.: 59.38%] [G loss: 0.424434]\n",
      "epoch:28 step:26820 [D loss: 0.215211, acc.: 62.50%] [G loss: 0.450606]\n",
      "epoch:28 step:26821 [D loss: 0.226263, acc.: 60.94%] [G loss: 0.420233]\n",
      "epoch:28 step:26822 [D loss: 0.224079, acc.: 65.62%] [G loss: 0.408358]\n",
      "epoch:28 step:26823 [D loss: 0.230387, acc.: 61.72%] [G loss: 0.417415]\n",
      "epoch:28 step:26824 [D loss: 0.231324, acc.: 61.72%] [G loss: 0.443107]\n",
      "epoch:28 step:26825 [D loss: 0.203866, acc.: 67.97%] [G loss: 0.483124]\n",
      "epoch:28 step:26826 [D loss: 0.241215, acc.: 57.03%] [G loss: 0.450299]\n",
      "epoch:28 step:26827 [D loss: 0.241505, acc.: 57.81%] [G loss: 0.427564]\n",
      "epoch:28 step:26828 [D loss: 0.206620, acc.: 67.97%] [G loss: 0.431054]\n",
      "epoch:28 step:26829 [D loss: 0.217269, acc.: 62.50%] [G loss: 0.405202]\n",
      "epoch:28 step:26830 [D loss: 0.224033, acc.: 62.50%] [G loss: 0.430989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26831 [D loss: 0.209522, acc.: 67.19%] [G loss: 0.407595]\n",
      "epoch:28 step:26832 [D loss: 0.223697, acc.: 58.59%] [G loss: 0.429559]\n",
      "epoch:28 step:26833 [D loss: 0.245602, acc.: 57.03%] [G loss: 0.436613]\n",
      "epoch:28 step:26834 [D loss: 0.209613, acc.: 72.66%] [G loss: 0.420777]\n",
      "epoch:28 step:26835 [D loss: 0.205287, acc.: 68.75%] [G loss: 0.429748]\n",
      "epoch:28 step:26836 [D loss: 0.241044, acc.: 61.72%] [G loss: 0.400980]\n",
      "epoch:28 step:26837 [D loss: 0.234412, acc.: 61.72%] [G loss: 0.420473]\n",
      "epoch:28 step:26838 [D loss: 0.216991, acc.: 67.19%] [G loss: 0.449222]\n",
      "epoch:28 step:26839 [D loss: 0.233365, acc.: 60.94%] [G loss: 0.405707]\n",
      "epoch:28 step:26840 [D loss: 0.221218, acc.: 60.16%] [G loss: 0.409404]\n",
      "epoch:28 step:26841 [D loss: 0.208989, acc.: 64.06%] [G loss: 0.423278]\n",
      "epoch:28 step:26842 [D loss: 0.204737, acc.: 64.06%] [G loss: 0.405020]\n",
      "epoch:28 step:26843 [D loss: 0.217022, acc.: 67.19%] [G loss: 0.450790]\n",
      "epoch:28 step:26844 [D loss: 0.219374, acc.: 62.50%] [G loss: 0.436814]\n",
      "epoch:28 step:26845 [D loss: 0.200084, acc.: 68.75%] [G loss: 0.427095]\n",
      "epoch:28 step:26846 [D loss: 0.218859, acc.: 64.06%] [G loss: 0.420279]\n",
      "epoch:28 step:26847 [D loss: 0.229384, acc.: 62.50%] [G loss: 0.407703]\n",
      "epoch:28 step:26848 [D loss: 0.230190, acc.: 60.94%] [G loss: 0.388753]\n",
      "epoch:28 step:26849 [D loss: 0.197461, acc.: 72.66%] [G loss: 0.430912]\n",
      "epoch:28 step:26850 [D loss: 0.258929, acc.: 57.03%] [G loss: 0.393125]\n",
      "epoch:28 step:26851 [D loss: 0.219781, acc.: 61.72%] [G loss: 0.427397]\n",
      "epoch:28 step:26852 [D loss: 0.209298, acc.: 71.09%] [G loss: 0.413001]\n",
      "epoch:28 step:26853 [D loss: 0.223289, acc.: 65.62%] [G loss: 0.436885]\n",
      "epoch:28 step:26854 [D loss: 0.224751, acc.: 60.16%] [G loss: 0.384397]\n",
      "epoch:28 step:26855 [D loss: 0.221441, acc.: 65.62%] [G loss: 0.444572]\n",
      "epoch:28 step:26856 [D loss: 0.190352, acc.: 72.66%] [G loss: 0.449923]\n",
      "epoch:28 step:26857 [D loss: 0.217352, acc.: 63.28%] [G loss: 0.450043]\n",
      "epoch:28 step:26858 [D loss: 0.226288, acc.: 59.38%] [G loss: 0.399592]\n",
      "epoch:28 step:26859 [D loss: 0.202117, acc.: 70.31%] [G loss: 0.425499]\n",
      "epoch:28 step:26860 [D loss: 0.189392, acc.: 66.41%] [G loss: 0.474786]\n",
      "epoch:28 step:26861 [D loss: 0.222069, acc.: 64.06%] [G loss: 0.457282]\n",
      "epoch:28 step:26862 [D loss: 0.231304, acc.: 60.94%] [G loss: 0.438273]\n",
      "epoch:28 step:26863 [D loss: 0.212438, acc.: 69.53%] [G loss: 0.443938]\n",
      "epoch:28 step:26864 [D loss: 0.220946, acc.: 65.62%] [G loss: 0.426692]\n",
      "epoch:28 step:26865 [D loss: 0.215448, acc.: 69.53%] [G loss: 0.420932]\n",
      "epoch:28 step:26866 [D loss: 0.222744, acc.: 62.50%] [G loss: 0.438867]\n",
      "epoch:28 step:26867 [D loss: 0.202060, acc.: 71.09%] [G loss: 0.452660]\n",
      "epoch:28 step:26868 [D loss: 0.203575, acc.: 71.09%] [G loss: 0.465679]\n",
      "epoch:28 step:26869 [D loss: 0.218996, acc.: 65.62%] [G loss: 0.420074]\n",
      "epoch:28 step:26870 [D loss: 0.202765, acc.: 73.44%] [G loss: 0.445587]\n",
      "epoch:28 step:26871 [D loss: 0.219670, acc.: 63.28%] [G loss: 0.431632]\n",
      "epoch:28 step:26872 [D loss: 0.225753, acc.: 62.50%] [G loss: 0.434559]\n",
      "epoch:28 step:26873 [D loss: 0.204684, acc.: 71.09%] [G loss: 0.474817]\n",
      "epoch:28 step:26874 [D loss: 0.220053, acc.: 64.06%] [G loss: 0.429580]\n",
      "epoch:28 step:26875 [D loss: 0.214626, acc.: 64.06%] [G loss: 0.462128]\n",
      "epoch:28 step:26876 [D loss: 0.229277, acc.: 57.81%] [G loss: 0.440756]\n",
      "epoch:28 step:26877 [D loss: 0.214554, acc.: 70.31%] [G loss: 0.509153]\n",
      "epoch:28 step:26878 [D loss: 0.182814, acc.: 71.09%] [G loss: 0.483331]\n",
      "epoch:28 step:26879 [D loss: 0.221907, acc.: 62.50%] [G loss: 0.472123]\n",
      "epoch:28 step:26880 [D loss: 0.241134, acc.: 57.81%] [G loss: 0.434907]\n",
      "epoch:28 step:26881 [D loss: 0.224375, acc.: 61.72%] [G loss: 0.412879]\n",
      "epoch:28 step:26882 [D loss: 0.199617, acc.: 73.44%] [G loss: 0.433870]\n",
      "epoch:28 step:26883 [D loss: 0.212048, acc.: 69.53%] [G loss: 0.464669]\n",
      "epoch:28 step:26884 [D loss: 0.174990, acc.: 76.56%] [G loss: 0.502276]\n",
      "epoch:28 step:26885 [D loss: 0.234491, acc.: 60.94%] [G loss: 0.481740]\n",
      "epoch:28 step:26886 [D loss: 0.215182, acc.: 67.97%] [G loss: 0.455512]\n",
      "epoch:28 step:26887 [D loss: 0.236003, acc.: 60.16%] [G loss: 0.465356]\n",
      "epoch:28 step:26888 [D loss: 0.231860, acc.: 60.16%] [G loss: 0.471714]\n",
      "epoch:28 step:26889 [D loss: 0.220140, acc.: 67.19%] [G loss: 0.448387]\n",
      "epoch:28 step:26890 [D loss: 0.219903, acc.: 63.28%] [G loss: 0.449877]\n",
      "epoch:28 step:26891 [D loss: 0.242216, acc.: 60.16%] [G loss: 0.462845]\n",
      "epoch:28 step:26892 [D loss: 0.242071, acc.: 59.38%] [G loss: 0.401815]\n",
      "epoch:28 step:26893 [D loss: 0.247560, acc.: 56.25%] [G loss: 0.392514]\n",
      "epoch:28 step:26894 [D loss: 0.219109, acc.: 63.28%] [G loss: 0.456489]\n",
      "epoch:28 step:26895 [D loss: 0.229624, acc.: 62.50%] [G loss: 0.428123]\n",
      "epoch:28 step:26896 [D loss: 0.227446, acc.: 60.16%] [G loss: 0.465730]\n",
      "epoch:28 step:26897 [D loss: 0.202657, acc.: 67.97%] [G loss: 0.476422]\n",
      "epoch:28 step:26898 [D loss: 0.241942, acc.: 63.28%] [G loss: 0.421296]\n",
      "epoch:28 step:26899 [D loss: 0.231879, acc.: 61.72%] [G loss: 0.436042]\n",
      "epoch:28 step:26900 [D loss: 0.227885, acc.: 62.50%] [G loss: 0.437788]\n",
      "epoch:28 step:26901 [D loss: 0.220558, acc.: 61.72%] [G loss: 0.403407]\n",
      "epoch:28 step:26902 [D loss: 0.215943, acc.: 63.28%] [G loss: 0.433069]\n",
      "epoch:28 step:26903 [D loss: 0.252516, acc.: 53.91%] [G loss: 0.428327]\n",
      "epoch:28 step:26904 [D loss: 0.234731, acc.: 60.16%] [G loss: 0.420186]\n",
      "epoch:28 step:26905 [D loss: 0.206067, acc.: 66.41%] [G loss: 0.418975]\n",
      "epoch:28 step:26906 [D loss: 0.235663, acc.: 58.59%] [G loss: 0.377231]\n",
      "epoch:28 step:26907 [D loss: 0.252401, acc.: 50.00%] [G loss: 0.392945]\n",
      "epoch:28 step:26908 [D loss: 0.235775, acc.: 65.62%] [G loss: 0.452591]\n",
      "epoch:28 step:26909 [D loss: 0.220985, acc.: 66.41%] [G loss: 0.471415]\n",
      "epoch:28 step:26910 [D loss: 0.222053, acc.: 64.06%] [G loss: 0.450548]\n",
      "epoch:28 step:26911 [D loss: 0.262863, acc.: 53.12%] [G loss: 0.434481]\n",
      "epoch:28 step:26912 [D loss: 0.232300, acc.: 55.47%] [G loss: 0.440557]\n",
      "epoch:28 step:26913 [D loss: 0.196800, acc.: 71.09%] [G loss: 0.466777]\n",
      "epoch:28 step:26914 [D loss: 0.239768, acc.: 59.38%] [G loss: 0.414755]\n",
      "epoch:28 step:26915 [D loss: 0.206704, acc.: 68.75%] [G loss: 0.467893]\n",
      "epoch:28 step:26916 [D loss: 0.235308, acc.: 61.72%] [G loss: 0.438513]\n",
      "epoch:28 step:26917 [D loss: 0.200395, acc.: 69.53%] [G loss: 0.416081]\n",
      "epoch:28 step:26918 [D loss: 0.227453, acc.: 57.81%] [G loss: 0.432858]\n",
      "epoch:28 step:26919 [D loss: 0.221384, acc.: 60.16%] [G loss: 0.412087]\n",
      "epoch:28 step:26920 [D loss: 0.239797, acc.: 56.25%] [G loss: 0.414744]\n",
      "epoch:28 step:26921 [D loss: 0.211663, acc.: 64.84%] [G loss: 0.399800]\n",
      "epoch:28 step:26922 [D loss: 0.213760, acc.: 64.84%] [G loss: 0.409429]\n",
      "epoch:28 step:26923 [D loss: 0.223358, acc.: 61.72%] [G loss: 0.435800]\n",
      "epoch:28 step:26924 [D loss: 0.200944, acc.: 68.75%] [G loss: 0.440531]\n",
      "epoch:28 step:26925 [D loss: 0.211317, acc.: 72.66%] [G loss: 0.420845]\n",
      "epoch:28 step:26926 [D loss: 0.189173, acc.: 67.97%] [G loss: 0.461701]\n",
      "epoch:28 step:26927 [D loss: 0.215588, acc.: 65.62%] [G loss: 0.433867]\n",
      "epoch:28 step:26928 [D loss: 0.212251, acc.: 64.84%] [G loss: 0.475074]\n",
      "epoch:28 step:26929 [D loss: 0.194662, acc.: 67.19%] [G loss: 0.513300]\n",
      "epoch:28 step:26930 [D loss: 0.181866, acc.: 75.78%] [G loss: 0.447000]\n",
      "epoch:28 step:26931 [D loss: 0.203493, acc.: 67.97%] [G loss: 0.463620]\n",
      "epoch:28 step:26932 [D loss: 0.226579, acc.: 60.16%] [G loss: 0.409995]\n",
      "epoch:28 step:26933 [D loss: 0.259263, acc.: 57.81%] [G loss: 0.398834]\n",
      "epoch:28 step:26934 [D loss: 0.232874, acc.: 62.50%] [G loss: 0.415760]\n",
      "epoch:28 step:26935 [D loss: 0.209628, acc.: 66.41%] [G loss: 0.427658]\n",
      "epoch:28 step:26936 [D loss: 0.193283, acc.: 71.09%] [G loss: 0.443754]\n",
      "epoch:28 step:26937 [D loss: 0.212709, acc.: 67.19%] [G loss: 0.439279]\n",
      "epoch:28 step:26938 [D loss: 0.258942, acc.: 53.91%] [G loss: 0.423364]\n",
      "epoch:28 step:26939 [D loss: 0.236448, acc.: 58.59%] [G loss: 0.435974]\n",
      "epoch:28 step:26940 [D loss: 0.252466, acc.: 55.47%] [G loss: 0.427309]\n",
      "epoch:28 step:26941 [D loss: 0.222840, acc.: 62.50%] [G loss: 0.429453]\n",
      "epoch:28 step:26942 [D loss: 0.208486, acc.: 65.62%] [G loss: 0.456518]\n",
      "epoch:28 step:26943 [D loss: 0.209996, acc.: 66.41%] [G loss: 0.448374]\n",
      "epoch:28 step:26944 [D loss: 0.200292, acc.: 75.00%] [G loss: 0.459688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26945 [D loss: 0.194342, acc.: 69.53%] [G loss: 0.457763]\n",
      "epoch:28 step:26946 [D loss: 0.271300, acc.: 50.00%] [G loss: 0.405789]\n",
      "epoch:28 step:26947 [D loss: 0.243704, acc.: 54.69%] [G loss: 0.416407]\n",
      "epoch:28 step:26948 [D loss: 0.214751, acc.: 63.28%] [G loss: 0.457620]\n",
      "epoch:28 step:26949 [D loss: 0.230012, acc.: 59.38%] [G loss: 0.480080]\n",
      "epoch:28 step:26950 [D loss: 0.208011, acc.: 64.84%] [G loss: 0.431218]\n",
      "epoch:28 step:26951 [D loss: 0.221339, acc.: 62.50%] [G loss: 0.423744]\n",
      "epoch:28 step:26952 [D loss: 0.262797, acc.: 51.56%] [G loss: 0.358510]\n",
      "epoch:28 step:26953 [D loss: 0.219281, acc.: 62.50%] [G loss: 0.430741]\n",
      "epoch:28 step:26954 [D loss: 0.224492, acc.: 66.41%] [G loss: 0.405145]\n",
      "epoch:28 step:26955 [D loss: 0.212550, acc.: 63.28%] [G loss: 0.436969]\n",
      "epoch:28 step:26956 [D loss: 0.199184, acc.: 67.97%] [G loss: 0.484807]\n",
      "epoch:28 step:26957 [D loss: 0.230303, acc.: 62.50%] [G loss: 0.438209]\n",
      "epoch:28 step:26958 [D loss: 0.236625, acc.: 61.72%] [G loss: 0.410561]\n",
      "epoch:28 step:26959 [D loss: 0.237654, acc.: 60.16%] [G loss: 0.410420]\n",
      "epoch:28 step:26960 [D loss: 0.218936, acc.: 60.94%] [G loss: 0.464751]\n",
      "epoch:28 step:26961 [D loss: 0.203105, acc.: 69.53%] [G loss: 0.449531]\n",
      "epoch:28 step:26962 [D loss: 0.204947, acc.: 68.75%] [G loss: 0.469942]\n",
      "epoch:28 step:26963 [D loss: 0.225371, acc.: 65.62%] [G loss: 0.416368]\n",
      "epoch:28 step:26964 [D loss: 0.232092, acc.: 67.19%] [G loss: 0.396154]\n",
      "epoch:28 step:26965 [D loss: 0.211186, acc.: 69.53%] [G loss: 0.444307]\n",
      "epoch:28 step:26966 [D loss: 0.202861, acc.: 66.41%] [G loss: 0.443551]\n",
      "epoch:28 step:26967 [D loss: 0.236295, acc.: 54.69%] [G loss: 0.466736]\n",
      "epoch:28 step:26968 [D loss: 0.211902, acc.: 64.84%] [G loss: 0.445107]\n",
      "epoch:28 step:26969 [D loss: 0.182769, acc.: 71.88%] [G loss: 0.469514]\n",
      "epoch:28 step:26970 [D loss: 0.245760, acc.: 60.16%] [G loss: 0.424255]\n",
      "epoch:28 step:26971 [D loss: 0.221440, acc.: 63.28%] [G loss: 0.433225]\n",
      "epoch:28 step:26972 [D loss: 0.207389, acc.: 67.97%] [G loss: 0.454237]\n",
      "epoch:28 step:26973 [D loss: 0.227899, acc.: 64.06%] [G loss: 0.408605]\n",
      "epoch:28 step:26974 [D loss: 0.234370, acc.: 62.50%] [G loss: 0.403956]\n",
      "epoch:28 step:26975 [D loss: 0.246801, acc.: 62.50%] [G loss: 0.418103]\n",
      "epoch:28 step:26976 [D loss: 0.244245, acc.: 56.25%] [G loss: 0.410206]\n",
      "epoch:28 step:26977 [D loss: 0.234424, acc.: 58.59%] [G loss: 0.401047]\n",
      "epoch:28 step:26978 [D loss: 0.216904, acc.: 62.50%] [G loss: 0.415993]\n",
      "epoch:28 step:26979 [D loss: 0.225197, acc.: 60.16%] [G loss: 0.454687]\n",
      "epoch:28 step:26980 [D loss: 0.198455, acc.: 71.88%] [G loss: 0.444322]\n",
      "epoch:28 step:26981 [D loss: 0.251375, acc.: 54.69%] [G loss: 0.412461]\n",
      "epoch:28 step:26982 [D loss: 0.212719, acc.: 64.06%] [G loss: 0.411367]\n",
      "epoch:28 step:26983 [D loss: 0.210688, acc.: 65.62%] [G loss: 0.414235]\n",
      "epoch:28 step:26984 [D loss: 0.238273, acc.: 57.03%] [G loss: 0.433441]\n",
      "epoch:28 step:26985 [D loss: 0.207182, acc.: 65.62%] [G loss: 0.423592]\n",
      "epoch:28 step:26986 [D loss: 0.198182, acc.: 68.75%] [G loss: 0.440875]\n",
      "epoch:28 step:26987 [D loss: 0.226683, acc.: 66.41%] [G loss: 0.407962]\n",
      "epoch:28 step:26988 [D loss: 0.253911, acc.: 53.91%] [G loss: 0.420662]\n",
      "epoch:28 step:26989 [D loss: 0.240845, acc.: 55.47%] [G loss: 0.444118]\n",
      "epoch:28 step:26990 [D loss: 0.224597, acc.: 61.72%] [G loss: 0.484227]\n",
      "epoch:28 step:26991 [D loss: 0.231437, acc.: 62.50%] [G loss: 0.507422]\n",
      "epoch:28 step:26992 [D loss: 0.231931, acc.: 57.81%] [G loss: 0.459056]\n",
      "epoch:28 step:26993 [D loss: 0.227061, acc.: 67.97%] [G loss: 0.446376]\n",
      "epoch:28 step:26994 [D loss: 0.218445, acc.: 64.06%] [G loss: 0.410800]\n",
      "epoch:28 step:26995 [D loss: 0.258886, acc.: 55.47%] [G loss: 0.420857]\n",
      "epoch:28 step:26996 [D loss: 0.241311, acc.: 52.34%] [G loss: 0.422291]\n",
      "epoch:28 step:26997 [D loss: 0.218817, acc.: 59.38%] [G loss: 0.443317]\n",
      "epoch:28 step:26998 [D loss: 0.256093, acc.: 53.12%] [G loss: 0.423417]\n",
      "epoch:28 step:26999 [D loss: 0.221823, acc.: 68.75%] [G loss: 0.393953]\n",
      "epoch:28 step:27000 [D loss: 0.240903, acc.: 54.69%] [G loss: 0.397566]\n",
      "##############\n",
      "[2.68718774 2.04482801 6.06241359 4.90244147 3.6552396  5.44847771\n",
      " 4.46824131 4.77495935 4.71530311 4.02003097]\n",
      "##########\n",
      "epoch:28 step:27001 [D loss: 0.268018, acc.: 51.56%] [G loss: 0.394268]\n",
      "epoch:28 step:27002 [D loss: 0.225642, acc.: 58.59%] [G loss: 0.434540]\n",
      "epoch:28 step:27003 [D loss: 0.237061, acc.: 64.06%] [G loss: 0.430065]\n",
      "epoch:28 step:27004 [D loss: 0.224260, acc.: 67.19%] [G loss: 0.436733]\n",
      "epoch:28 step:27005 [D loss: 0.215546, acc.: 64.84%] [G loss: 0.426813]\n",
      "epoch:28 step:27006 [D loss: 0.231873, acc.: 63.28%] [G loss: 0.459213]\n",
      "epoch:28 step:27007 [D loss: 0.218668, acc.: 66.41%] [G loss: 0.433606]\n",
      "epoch:28 step:27008 [D loss: 0.247809, acc.: 53.91%] [G loss: 0.433361]\n",
      "epoch:28 step:27009 [D loss: 0.204972, acc.: 70.31%] [G loss: 0.423056]\n",
      "epoch:28 step:27010 [D loss: 0.188273, acc.: 77.34%] [G loss: 0.470040]\n",
      "epoch:28 step:27011 [D loss: 0.204579, acc.: 67.97%] [G loss: 0.476259]\n",
      "epoch:28 step:27012 [D loss: 0.247147, acc.: 61.72%] [G loss: 0.463043]\n",
      "epoch:28 step:27013 [D loss: 0.209496, acc.: 65.62%] [G loss: 0.492402]\n",
      "epoch:28 step:27014 [D loss: 0.222887, acc.: 64.84%] [G loss: 0.470112]\n",
      "epoch:28 step:27015 [D loss: 0.220000, acc.: 65.62%] [G loss: 0.415651]\n",
      "epoch:28 step:27016 [D loss: 0.244056, acc.: 60.16%] [G loss: 0.376819]\n",
      "epoch:28 step:27017 [D loss: 0.210765, acc.: 66.41%] [G loss: 0.465833]\n",
      "epoch:28 step:27018 [D loss: 0.205241, acc.: 70.31%] [G loss: 0.462572]\n",
      "epoch:28 step:27019 [D loss: 0.248379, acc.: 56.25%] [G loss: 0.450613]\n",
      "epoch:28 step:27020 [D loss: 0.255176, acc.: 51.56%] [G loss: 0.434673]\n",
      "epoch:28 step:27021 [D loss: 0.218291, acc.: 71.09%] [G loss: 0.432898]\n",
      "epoch:28 step:27022 [D loss: 0.196868, acc.: 68.75%] [G loss: 0.439021]\n",
      "epoch:28 step:27023 [D loss: 0.264509, acc.: 52.34%] [G loss: 0.387255]\n",
      "epoch:28 step:27024 [D loss: 0.250556, acc.: 57.81%] [G loss: 0.426382]\n",
      "epoch:28 step:27025 [D loss: 0.228404, acc.: 56.25%] [G loss: 0.440409]\n",
      "epoch:28 step:27026 [D loss: 0.218949, acc.: 64.84%] [G loss: 0.440272]\n",
      "epoch:28 step:27027 [D loss: 0.240982, acc.: 58.59%] [G loss: 0.434822]\n",
      "epoch:28 step:27028 [D loss: 0.193057, acc.: 71.88%] [G loss: 0.429447]\n",
      "epoch:28 step:27029 [D loss: 0.202471, acc.: 67.97%] [G loss: 0.437633]\n",
      "epoch:28 step:27030 [D loss: 0.264188, acc.: 50.00%] [G loss: 0.440373]\n",
      "epoch:28 step:27031 [D loss: 0.246199, acc.: 56.25%] [G loss: 0.402080]\n",
      "epoch:28 step:27032 [D loss: 0.218165, acc.: 61.72%] [G loss: 0.416496]\n",
      "epoch:28 step:27033 [D loss: 0.254361, acc.: 52.34%] [G loss: 0.423142]\n",
      "epoch:28 step:27034 [D loss: 0.213657, acc.: 62.50%] [G loss: 0.429691]\n",
      "epoch:28 step:27035 [D loss: 0.219515, acc.: 64.84%] [G loss: 0.451029]\n",
      "epoch:28 step:27036 [D loss: 0.271614, acc.: 51.56%] [G loss: 0.402711]\n",
      "epoch:28 step:27037 [D loss: 0.222326, acc.: 65.62%] [G loss: 0.413373]\n",
      "epoch:28 step:27038 [D loss: 0.219489, acc.: 65.62%] [G loss: 0.441314]\n",
      "epoch:28 step:27039 [D loss: 0.219789, acc.: 66.41%] [G loss: 0.466250]\n",
      "epoch:28 step:27040 [D loss: 0.257605, acc.: 53.12%] [G loss: 0.410270]\n",
      "epoch:28 step:27041 [D loss: 0.218035, acc.: 60.94%] [G loss: 0.424557]\n",
      "epoch:28 step:27042 [D loss: 0.209595, acc.: 71.09%] [G loss: 0.424447]\n",
      "epoch:28 step:27043 [D loss: 0.228099, acc.: 60.16%] [G loss: 0.410914]\n",
      "epoch:28 step:27044 [D loss: 0.222992, acc.: 61.72%] [G loss: 0.441056]\n",
      "epoch:28 step:27045 [D loss: 0.240942, acc.: 55.47%] [G loss: 0.410016]\n",
      "epoch:28 step:27046 [D loss: 0.226790, acc.: 64.06%] [G loss: 0.419460]\n",
      "epoch:28 step:27047 [D loss: 0.218717, acc.: 61.72%] [G loss: 0.417896]\n",
      "epoch:28 step:27048 [D loss: 0.235160, acc.: 57.03%] [G loss: 0.458111]\n",
      "epoch:28 step:27049 [D loss: 0.216901, acc.: 63.28%] [G loss: 0.429235]\n",
      "epoch:28 step:27050 [D loss: 0.201269, acc.: 64.84%] [G loss: 0.476544]\n",
      "epoch:28 step:27051 [D loss: 0.216289, acc.: 63.28%] [G loss: 0.425035]\n",
      "epoch:28 step:27052 [D loss: 0.218013, acc.: 60.16%] [G loss: 0.437850]\n",
      "epoch:28 step:27053 [D loss: 0.225195, acc.: 61.72%] [G loss: 0.463033]\n",
      "epoch:28 step:27054 [D loss: 0.260911, acc.: 57.03%] [G loss: 0.379275]\n",
      "epoch:28 step:27055 [D loss: 0.212889, acc.: 70.31%] [G loss: 0.395684]\n",
      "epoch:28 step:27056 [D loss: 0.262189, acc.: 57.03%] [G loss: 0.413881]\n",
      "epoch:28 step:27057 [D loss: 0.235561, acc.: 58.59%] [G loss: 0.399280]\n",
      "epoch:28 step:27058 [D loss: 0.199504, acc.: 68.75%] [G loss: 0.406978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27059 [D loss: 0.226393, acc.: 59.38%] [G loss: 0.390054]\n",
      "epoch:28 step:27060 [D loss: 0.226000, acc.: 63.28%] [G loss: 0.397338]\n",
      "epoch:28 step:27061 [D loss: 0.216561, acc.: 67.19%] [G loss: 0.401828]\n",
      "epoch:28 step:27062 [D loss: 0.206539, acc.: 67.97%] [G loss: 0.428612]\n",
      "epoch:28 step:27063 [D loss: 0.256041, acc.: 53.12%] [G loss: 0.408284]\n",
      "epoch:28 step:27064 [D loss: 0.262506, acc.: 54.69%] [G loss: 0.419354]\n",
      "epoch:28 step:27065 [D loss: 0.233998, acc.: 60.16%] [G loss: 0.417949]\n",
      "epoch:28 step:27066 [D loss: 0.242517, acc.: 56.25%] [G loss: 0.448666]\n",
      "epoch:28 step:27067 [D loss: 0.218145, acc.: 67.19%] [G loss: 0.414545]\n",
      "epoch:28 step:27068 [D loss: 0.202966, acc.: 67.19%] [G loss: 0.452979]\n",
      "epoch:28 step:27069 [D loss: 0.225983, acc.: 67.97%] [G loss: 0.435100]\n",
      "epoch:28 step:27070 [D loss: 0.242183, acc.: 59.38%] [G loss: 0.417060]\n",
      "epoch:28 step:27071 [D loss: 0.226746, acc.: 64.06%] [G loss: 0.387405]\n",
      "epoch:28 step:27072 [D loss: 0.227773, acc.: 67.19%] [G loss: 0.447889]\n",
      "epoch:28 step:27073 [D loss: 0.200105, acc.: 67.97%] [G loss: 0.433663]\n",
      "epoch:28 step:27074 [D loss: 0.201732, acc.: 68.75%] [G loss: 0.424175]\n",
      "epoch:28 step:27075 [D loss: 0.199745, acc.: 70.31%] [G loss: 0.414420]\n",
      "epoch:28 step:27076 [D loss: 0.209698, acc.: 74.22%] [G loss: 0.426212]\n",
      "epoch:28 step:27077 [D loss: 0.211949, acc.: 71.09%] [G loss: 0.418548]\n",
      "epoch:28 step:27078 [D loss: 0.205114, acc.: 70.31%] [G loss: 0.417967]\n",
      "epoch:28 step:27079 [D loss: 0.230035, acc.: 62.50%] [G loss: 0.378943]\n",
      "epoch:28 step:27080 [D loss: 0.232087, acc.: 59.38%] [G loss: 0.399808]\n",
      "epoch:28 step:27081 [D loss: 0.208790, acc.: 67.19%] [G loss: 0.424960]\n",
      "epoch:28 step:27082 [D loss: 0.225488, acc.: 63.28%] [G loss: 0.450228]\n",
      "epoch:28 step:27083 [D loss: 0.244376, acc.: 50.00%] [G loss: 0.402967]\n",
      "epoch:28 step:27084 [D loss: 0.215526, acc.: 64.06%] [G loss: 0.402963]\n",
      "epoch:28 step:27085 [D loss: 0.204037, acc.: 66.41%] [G loss: 0.392854]\n",
      "epoch:28 step:27086 [D loss: 0.256608, acc.: 53.91%] [G loss: 0.383602]\n",
      "epoch:28 step:27087 [D loss: 0.224994, acc.: 60.94%] [G loss: 0.432936]\n",
      "epoch:28 step:27088 [D loss: 0.214653, acc.: 62.50%] [G loss: 0.472202]\n",
      "epoch:28 step:27089 [D loss: 0.209073, acc.: 64.84%] [G loss: 0.462102]\n",
      "epoch:28 step:27090 [D loss: 0.244597, acc.: 55.47%] [G loss: 0.448802]\n",
      "epoch:28 step:27091 [D loss: 0.217662, acc.: 66.41%] [G loss: 0.415770]\n",
      "epoch:28 step:27092 [D loss: 0.239317, acc.: 57.03%] [G loss: 0.404889]\n",
      "epoch:28 step:27093 [D loss: 0.190601, acc.: 69.53%] [G loss: 0.465226]\n",
      "epoch:28 step:27094 [D loss: 0.269742, acc.: 54.69%] [G loss: 0.399416]\n",
      "epoch:28 step:27095 [D loss: 0.240136, acc.: 55.47%] [G loss: 0.400617]\n",
      "epoch:28 step:27096 [D loss: 0.189739, acc.: 76.56%] [G loss: 0.427323]\n",
      "epoch:28 step:27097 [D loss: 0.251808, acc.: 56.25%] [G loss: 0.382985]\n",
      "epoch:28 step:27098 [D loss: 0.241169, acc.: 57.03%] [G loss: 0.362484]\n",
      "epoch:28 step:27099 [D loss: 0.222836, acc.: 57.03%] [G loss: 0.412339]\n",
      "epoch:28 step:27100 [D loss: 0.240260, acc.: 57.03%] [G loss: 0.404510]\n",
      "epoch:28 step:27101 [D loss: 0.256838, acc.: 56.25%] [G loss: 0.390241]\n",
      "epoch:28 step:27102 [D loss: 0.252025, acc.: 57.81%] [G loss: 0.394741]\n",
      "epoch:28 step:27103 [D loss: 0.238315, acc.: 54.69%] [G loss: 0.450912]\n",
      "epoch:28 step:27104 [D loss: 0.233472, acc.: 61.72%] [G loss: 0.402922]\n",
      "epoch:28 step:27105 [D loss: 0.252357, acc.: 55.47%] [G loss: 0.401342]\n",
      "epoch:28 step:27106 [D loss: 0.224876, acc.: 60.94%] [G loss: 0.397378]\n",
      "epoch:28 step:27107 [D loss: 0.222689, acc.: 59.38%] [G loss: 0.417990]\n",
      "epoch:28 step:27108 [D loss: 0.240081, acc.: 59.38%] [G loss: 0.418903]\n",
      "epoch:28 step:27109 [D loss: 0.241085, acc.: 54.69%] [G loss: 0.418059]\n",
      "epoch:28 step:27110 [D loss: 0.235629, acc.: 60.94%] [G loss: 0.416959]\n",
      "epoch:28 step:27111 [D loss: 0.183240, acc.: 75.78%] [G loss: 0.439093]\n",
      "epoch:28 step:27112 [D loss: 0.257346, acc.: 53.12%] [G loss: 0.408764]\n",
      "epoch:28 step:27113 [D loss: 0.239136, acc.: 60.16%] [G loss: 0.421068]\n",
      "epoch:28 step:27114 [D loss: 0.210143, acc.: 63.28%] [G loss: 0.438630]\n",
      "epoch:28 step:27115 [D loss: 0.228975, acc.: 61.72%] [G loss: 0.397931]\n",
      "epoch:28 step:27116 [D loss: 0.261708, acc.: 50.78%] [G loss: 0.400829]\n",
      "epoch:28 step:27117 [D loss: 0.256150, acc.: 57.03%] [G loss: 0.417364]\n",
      "epoch:28 step:27118 [D loss: 0.223813, acc.: 61.72%] [G loss: 0.406941]\n",
      "epoch:28 step:27119 [D loss: 0.226211, acc.: 61.72%] [G loss: 0.415092]\n",
      "epoch:28 step:27120 [D loss: 0.203204, acc.: 68.75%] [G loss: 0.448158]\n",
      "epoch:28 step:27121 [D loss: 0.212429, acc.: 66.41%] [G loss: 0.479102]\n",
      "epoch:28 step:27122 [D loss: 0.199953, acc.: 72.66%] [G loss: 0.506144]\n",
      "epoch:28 step:27123 [D loss: 0.239550, acc.: 62.50%] [G loss: 0.477503]\n",
      "epoch:28 step:27124 [D loss: 0.227921, acc.: 66.41%] [G loss: 0.422481]\n",
      "epoch:28 step:27125 [D loss: 0.181202, acc.: 75.00%] [G loss: 0.453270]\n",
      "epoch:28 step:27126 [D loss: 0.186700, acc.: 63.28%] [G loss: 0.443811]\n",
      "epoch:28 step:27127 [D loss: 0.290502, acc.: 47.66%] [G loss: 0.354449]\n",
      "epoch:28 step:27128 [D loss: 0.238086, acc.: 57.81%] [G loss: 0.413798]\n",
      "epoch:28 step:27129 [D loss: 0.213484, acc.: 67.97%] [G loss: 0.415012]\n",
      "epoch:28 step:27130 [D loss: 0.205330, acc.: 71.09%] [G loss: 0.426363]\n",
      "epoch:28 step:27131 [D loss: 0.223022, acc.: 60.94%] [G loss: 0.459675]\n",
      "epoch:28 step:27132 [D loss: 0.211209, acc.: 67.97%] [G loss: 0.428381]\n",
      "epoch:28 step:27133 [D loss: 0.208221, acc.: 66.41%] [G loss: 0.479257]\n",
      "epoch:28 step:27134 [D loss: 0.207151, acc.: 65.62%] [G loss: 0.438818]\n",
      "epoch:28 step:27135 [D loss: 0.205246, acc.: 67.97%] [G loss: 0.445734]\n",
      "epoch:28 step:27136 [D loss: 0.205145, acc.: 64.84%] [G loss: 0.479066]\n",
      "epoch:28 step:27137 [D loss: 0.217020, acc.: 64.84%] [G loss: 0.488029]\n",
      "epoch:28 step:27138 [D loss: 0.213922, acc.: 64.84%] [G loss: 0.461217]\n",
      "epoch:28 step:27139 [D loss: 0.216972, acc.: 66.41%] [G loss: 0.434683]\n",
      "epoch:28 step:27140 [D loss: 0.233637, acc.: 62.50%] [G loss: 0.375448]\n",
      "epoch:28 step:27141 [D loss: 0.192432, acc.: 72.66%] [G loss: 0.455448]\n",
      "epoch:28 step:27142 [D loss: 0.213798, acc.: 64.84%] [G loss: 0.444896]\n",
      "epoch:28 step:27143 [D loss: 0.237676, acc.: 61.72%] [G loss: 0.449670]\n",
      "epoch:28 step:27144 [D loss: 0.208937, acc.: 66.41%] [G loss: 0.421580]\n",
      "epoch:28 step:27145 [D loss: 0.215229, acc.: 63.28%] [G loss: 0.444303]\n",
      "epoch:28 step:27146 [D loss: 0.204451, acc.: 63.28%] [G loss: 0.466273]\n",
      "epoch:28 step:27147 [D loss: 0.204107, acc.: 68.75%] [G loss: 0.481920]\n",
      "epoch:28 step:27148 [D loss: 0.227446, acc.: 64.06%] [G loss: 0.438785]\n",
      "epoch:28 step:27149 [D loss: 0.224824, acc.: 59.38%] [G loss: 0.456071]\n",
      "epoch:28 step:27150 [D loss: 0.225921, acc.: 60.94%] [G loss: 0.449431]\n",
      "epoch:28 step:27151 [D loss: 0.264519, acc.: 60.94%] [G loss: 0.398353]\n",
      "epoch:28 step:27152 [D loss: 0.220766, acc.: 59.38%] [G loss: 0.449560]\n",
      "epoch:28 step:27153 [D loss: 0.208529, acc.: 65.62%] [G loss: 0.455898]\n",
      "epoch:28 step:27154 [D loss: 0.211521, acc.: 66.41%] [G loss: 0.480809]\n",
      "epoch:28 step:27155 [D loss: 0.198686, acc.: 70.31%] [G loss: 0.451204]\n",
      "epoch:28 step:27156 [D loss: 0.299877, acc.: 44.53%] [G loss: 0.382649]\n",
      "epoch:28 step:27157 [D loss: 0.203680, acc.: 67.19%] [G loss: 0.463024]\n",
      "epoch:28 step:27158 [D loss: 0.230113, acc.: 58.59%] [G loss: 0.453608]\n",
      "epoch:28 step:27159 [D loss: 0.204670, acc.: 70.31%] [G loss: 0.399316]\n",
      "epoch:28 step:27160 [D loss: 0.190670, acc.: 77.34%] [G loss: 0.451077]\n",
      "epoch:28 step:27161 [D loss: 0.179177, acc.: 76.56%] [G loss: 0.476808]\n",
      "epoch:28 step:27162 [D loss: 0.193559, acc.: 73.44%] [G loss: 0.543955]\n",
      "epoch:28 step:27163 [D loss: 0.252303, acc.: 57.03%] [G loss: 0.466501]\n",
      "epoch:28 step:27164 [D loss: 0.277166, acc.: 53.12%] [G loss: 0.505559]\n",
      "epoch:28 step:27165 [D loss: 0.235576, acc.: 60.94%] [G loss: 0.588375]\n",
      "epoch:28 step:27166 [D loss: 0.174399, acc.: 75.00%] [G loss: 0.546614]\n",
      "epoch:28 step:27167 [D loss: 0.227570, acc.: 60.94%] [G loss: 0.424423]\n",
      "epoch:28 step:27168 [D loss: 0.251329, acc.: 55.47%] [G loss: 0.429838]\n",
      "epoch:28 step:27169 [D loss: 0.230245, acc.: 63.28%] [G loss: 0.426326]\n",
      "epoch:28 step:27170 [D loss: 0.235062, acc.: 60.94%] [G loss: 0.399279]\n",
      "epoch:28 step:27171 [D loss: 0.215615, acc.: 61.72%] [G loss: 0.436256]\n",
      "epoch:28 step:27172 [D loss: 0.178392, acc.: 75.78%] [G loss: 0.454626]\n",
      "epoch:28 step:27173 [D loss: 0.198331, acc.: 75.00%] [G loss: 0.526507]\n",
      "epoch:29 step:27174 [D loss: 0.239556, acc.: 58.59%] [G loss: 0.487409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27175 [D loss: 0.254940, acc.: 57.81%] [G loss: 0.443898]\n",
      "epoch:29 step:27176 [D loss: 0.227341, acc.: 59.38%] [G loss: 0.439271]\n",
      "epoch:29 step:27177 [D loss: 0.234121, acc.: 62.50%] [G loss: 0.441753]\n",
      "epoch:29 step:27178 [D loss: 0.222647, acc.: 66.41%] [G loss: 0.446887]\n",
      "epoch:29 step:27179 [D loss: 0.202100, acc.: 71.88%] [G loss: 0.451966]\n",
      "epoch:29 step:27180 [D loss: 0.204816, acc.: 67.19%] [G loss: 0.420945]\n",
      "epoch:29 step:27181 [D loss: 0.205076, acc.: 68.75%] [G loss: 0.448359]\n",
      "epoch:29 step:27182 [D loss: 0.195625, acc.: 70.31%] [G loss: 0.479312]\n",
      "epoch:29 step:27183 [D loss: 0.206655, acc.: 67.97%] [G loss: 0.426142]\n",
      "epoch:29 step:27184 [D loss: 0.242671, acc.: 60.94%] [G loss: 0.423679]\n",
      "epoch:29 step:27185 [D loss: 0.229092, acc.: 61.72%] [G loss: 0.446728]\n",
      "epoch:29 step:27186 [D loss: 0.212070, acc.: 64.84%] [G loss: 0.455439]\n",
      "epoch:29 step:27187 [D loss: 0.208106, acc.: 67.19%] [G loss: 0.422160]\n",
      "epoch:29 step:27188 [D loss: 0.182579, acc.: 72.66%] [G loss: 0.445976]\n",
      "epoch:29 step:27189 [D loss: 0.198029, acc.: 67.97%] [G loss: 0.442944]\n",
      "epoch:29 step:27190 [D loss: 0.251546, acc.: 52.34%] [G loss: 0.447830]\n",
      "epoch:29 step:27191 [D loss: 0.238737, acc.: 59.38%] [G loss: 0.468904]\n",
      "epoch:29 step:27192 [D loss: 0.251661, acc.: 54.69%] [G loss: 0.455567]\n",
      "epoch:29 step:27193 [D loss: 0.268437, acc.: 57.03%] [G loss: 0.433548]\n",
      "epoch:29 step:27194 [D loss: 0.243544, acc.: 57.81%] [G loss: 0.404294]\n",
      "epoch:29 step:27195 [D loss: 0.207877, acc.: 67.19%] [G loss: 0.463016]\n",
      "epoch:29 step:27196 [D loss: 0.239176, acc.: 57.81%] [G loss: 0.419949]\n",
      "epoch:29 step:27197 [D loss: 0.223276, acc.: 59.38%] [G loss: 0.426197]\n",
      "epoch:29 step:27198 [D loss: 0.216717, acc.: 65.62%] [G loss: 0.429257]\n",
      "epoch:29 step:27199 [D loss: 0.223085, acc.: 64.06%] [G loss: 0.458992]\n",
      "epoch:29 step:27200 [D loss: 0.224047, acc.: 58.59%] [G loss: 0.402683]\n",
      "##############\n",
      "[2.63415697 1.74591582 6.04933806 4.72804817 3.43422756 5.43520662\n",
      " 4.4441197  4.78442101 4.2639101  4.15930347]\n",
      "##########\n",
      "epoch:29 step:27201 [D loss: 0.227264, acc.: 60.94%] [G loss: 0.385223]\n",
      "epoch:29 step:27202 [D loss: 0.231211, acc.: 63.28%] [G loss: 0.424082]\n",
      "epoch:29 step:27203 [D loss: 0.238664, acc.: 59.38%] [G loss: 0.429930]\n",
      "epoch:29 step:27204 [D loss: 0.224683, acc.: 64.84%] [G loss: 0.459487]\n",
      "epoch:29 step:27205 [D loss: 0.210815, acc.: 64.06%] [G loss: 0.488694]\n",
      "epoch:29 step:27206 [D loss: 0.247783, acc.: 54.69%] [G loss: 0.423584]\n",
      "epoch:29 step:27207 [D loss: 0.233504, acc.: 60.16%] [G loss: 0.378060]\n",
      "epoch:29 step:27208 [D loss: 0.214594, acc.: 62.50%] [G loss: 0.410889]\n",
      "epoch:29 step:27209 [D loss: 0.230473, acc.: 60.94%] [G loss: 0.428473]\n",
      "epoch:29 step:27210 [D loss: 0.237775, acc.: 61.72%] [G loss: 0.420893]\n",
      "epoch:29 step:27211 [D loss: 0.233636, acc.: 61.72%] [G loss: 0.410230]\n",
      "epoch:29 step:27212 [D loss: 0.214633, acc.: 61.72%] [G loss: 0.422470]\n",
      "epoch:29 step:27213 [D loss: 0.198300, acc.: 72.66%] [G loss: 0.430134]\n",
      "epoch:29 step:27214 [D loss: 0.245097, acc.: 57.03%] [G loss: 0.426487]\n",
      "epoch:29 step:27215 [D loss: 0.223240, acc.: 62.50%] [G loss: 0.432404]\n",
      "epoch:29 step:27216 [D loss: 0.229824, acc.: 64.06%] [G loss: 0.410330]\n",
      "epoch:29 step:27217 [D loss: 0.215309, acc.: 66.41%] [G loss: 0.443251]\n",
      "epoch:29 step:27218 [D loss: 0.214400, acc.: 67.97%] [G loss: 0.439660]\n",
      "epoch:29 step:27219 [D loss: 0.222480, acc.: 60.16%] [G loss: 0.510621]\n",
      "epoch:29 step:27220 [D loss: 0.228286, acc.: 61.72%] [G loss: 0.375653]\n",
      "epoch:29 step:27221 [D loss: 0.211877, acc.: 68.75%] [G loss: 0.387445]\n",
      "epoch:29 step:27222 [D loss: 0.211378, acc.: 64.84%] [G loss: 0.433908]\n",
      "epoch:29 step:27223 [D loss: 0.225955, acc.: 64.06%] [G loss: 0.422538]\n",
      "epoch:29 step:27224 [D loss: 0.236267, acc.: 57.03%] [G loss: 0.439044]\n",
      "epoch:29 step:27225 [D loss: 0.230067, acc.: 64.84%] [G loss: 0.423423]\n",
      "epoch:29 step:27226 [D loss: 0.232756, acc.: 64.06%] [G loss: 0.434540]\n",
      "epoch:29 step:27227 [D loss: 0.203044, acc.: 72.66%] [G loss: 0.440758]\n",
      "epoch:29 step:27228 [D loss: 0.208321, acc.: 67.97%] [G loss: 0.432270]\n",
      "epoch:29 step:27229 [D loss: 0.218808, acc.: 66.41%] [G loss: 0.416552]\n",
      "epoch:29 step:27230 [D loss: 0.216162, acc.: 63.28%] [G loss: 0.418812]\n",
      "epoch:29 step:27231 [D loss: 0.236570, acc.: 61.72%] [G loss: 0.436022]\n",
      "epoch:29 step:27232 [D loss: 0.225905, acc.: 60.94%] [G loss: 0.458860]\n",
      "epoch:29 step:27233 [D loss: 0.225296, acc.: 63.28%] [G loss: 0.449142]\n",
      "epoch:29 step:27234 [D loss: 0.245458, acc.: 54.69%] [G loss: 0.398664]\n",
      "epoch:29 step:27235 [D loss: 0.246067, acc.: 58.59%] [G loss: 0.387858]\n",
      "epoch:29 step:27236 [D loss: 0.208663, acc.: 69.53%] [G loss: 0.430970]\n",
      "epoch:29 step:27237 [D loss: 0.230885, acc.: 60.94%] [G loss: 0.397229]\n",
      "epoch:29 step:27238 [D loss: 0.225554, acc.: 56.25%] [G loss: 0.427264]\n",
      "epoch:29 step:27239 [D loss: 0.218892, acc.: 64.06%] [G loss: 0.422995]\n",
      "epoch:29 step:27240 [D loss: 0.215188, acc.: 64.06%] [G loss: 0.409837]\n",
      "epoch:29 step:27241 [D loss: 0.219134, acc.: 63.28%] [G loss: 0.412727]\n",
      "epoch:29 step:27242 [D loss: 0.197080, acc.: 69.53%] [G loss: 0.459052]\n",
      "epoch:29 step:27243 [D loss: 0.188515, acc.: 71.88%] [G loss: 0.443716]\n",
      "epoch:29 step:27244 [D loss: 0.230545, acc.: 66.41%] [G loss: 0.460827]\n",
      "epoch:29 step:27245 [D loss: 0.235536, acc.: 57.81%] [G loss: 0.408224]\n",
      "epoch:29 step:27246 [D loss: 0.236504, acc.: 57.03%] [G loss: 0.416373]\n",
      "epoch:29 step:27247 [D loss: 0.209032, acc.: 70.31%] [G loss: 0.425700]\n",
      "epoch:29 step:27248 [D loss: 0.229636, acc.: 62.50%] [G loss: 0.451281]\n",
      "epoch:29 step:27249 [D loss: 0.192767, acc.: 75.78%] [G loss: 0.431916]\n",
      "epoch:29 step:27250 [D loss: 0.234262, acc.: 61.72%] [G loss: 0.458182]\n",
      "epoch:29 step:27251 [D loss: 0.264560, acc.: 52.34%] [G loss: 0.415995]\n",
      "epoch:29 step:27252 [D loss: 0.273741, acc.: 53.12%] [G loss: 0.339394]\n",
      "epoch:29 step:27253 [D loss: 0.212084, acc.: 59.38%] [G loss: 0.391074]\n",
      "epoch:29 step:27254 [D loss: 0.239870, acc.: 57.81%] [G loss: 0.395727]\n",
      "epoch:29 step:27255 [D loss: 0.228600, acc.: 60.16%] [G loss: 0.391365]\n",
      "epoch:29 step:27256 [D loss: 0.218467, acc.: 66.41%] [G loss: 0.418178]\n",
      "epoch:29 step:27257 [D loss: 0.216171, acc.: 64.06%] [G loss: 0.419804]\n",
      "epoch:29 step:27258 [D loss: 0.231906, acc.: 61.72%] [G loss: 0.453251]\n",
      "epoch:29 step:27259 [D loss: 0.219091, acc.: 69.53%] [G loss: 0.439232]\n",
      "epoch:29 step:27260 [D loss: 0.234720, acc.: 64.84%] [G loss: 0.409521]\n",
      "epoch:29 step:27261 [D loss: 0.197590, acc.: 67.97%] [G loss: 0.431812]\n",
      "epoch:29 step:27262 [D loss: 0.212420, acc.: 67.19%] [G loss: 0.427554]\n",
      "epoch:29 step:27263 [D loss: 0.226329, acc.: 65.62%] [G loss: 0.435775]\n",
      "epoch:29 step:27264 [D loss: 0.229061, acc.: 57.81%] [G loss: 0.449833]\n",
      "epoch:29 step:27265 [D loss: 0.214292, acc.: 64.84%] [G loss: 0.444684]\n",
      "epoch:29 step:27266 [D loss: 0.199348, acc.: 70.31%] [G loss: 0.423419]\n",
      "epoch:29 step:27267 [D loss: 0.240397, acc.: 61.72%] [G loss: 0.477701]\n",
      "epoch:29 step:27268 [D loss: 0.245706, acc.: 63.28%] [G loss: 0.433820]\n",
      "epoch:29 step:27269 [D loss: 0.241031, acc.: 60.16%] [G loss: 0.432050]\n",
      "epoch:29 step:27270 [D loss: 0.181231, acc.: 74.22%] [G loss: 0.494395]\n",
      "epoch:29 step:27271 [D loss: 0.224538, acc.: 64.84%] [G loss: 0.461870]\n",
      "epoch:29 step:27272 [D loss: 0.230598, acc.: 62.50%] [G loss: 0.494658]\n",
      "epoch:29 step:27273 [D loss: 0.208832, acc.: 68.75%] [G loss: 0.445848]\n",
      "epoch:29 step:27274 [D loss: 0.243742, acc.: 55.47%] [G loss: 0.439440]\n",
      "epoch:29 step:27275 [D loss: 0.238911, acc.: 56.25%] [G loss: 0.375026]\n",
      "epoch:29 step:27276 [D loss: 0.232871, acc.: 64.06%] [G loss: 0.382008]\n",
      "epoch:29 step:27277 [D loss: 0.221474, acc.: 66.41%] [G loss: 0.445555]\n",
      "epoch:29 step:27278 [D loss: 0.248905, acc.: 53.91%] [G loss: 0.387351]\n",
      "epoch:29 step:27279 [D loss: 0.196452, acc.: 71.09%] [G loss: 0.447951]\n",
      "epoch:29 step:27280 [D loss: 0.199007, acc.: 71.88%] [G loss: 0.447295]\n",
      "epoch:29 step:27281 [D loss: 0.264442, acc.: 50.78%] [G loss: 0.461115]\n",
      "epoch:29 step:27282 [D loss: 0.229796, acc.: 60.94%] [G loss: 0.480065]\n",
      "epoch:29 step:27283 [D loss: 0.220701, acc.: 62.50%] [G loss: 0.461777]\n",
      "epoch:29 step:27284 [D loss: 0.209237, acc.: 69.53%] [G loss: 0.449185]\n",
      "epoch:29 step:27285 [D loss: 0.198488, acc.: 69.53%] [G loss: 0.412974]\n",
      "epoch:29 step:27286 [D loss: 0.213662, acc.: 61.72%] [G loss: 0.432547]\n",
      "epoch:29 step:27287 [D loss: 0.191393, acc.: 74.22%] [G loss: 0.479233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27288 [D loss: 0.190942, acc.: 68.75%] [G loss: 0.477589]\n",
      "epoch:29 step:27289 [D loss: 0.215166, acc.: 60.94%] [G loss: 0.480095]\n",
      "epoch:29 step:27290 [D loss: 0.192623, acc.: 72.66%] [G loss: 0.457164]\n",
      "epoch:29 step:27291 [D loss: 0.229379, acc.: 61.72%] [G loss: 0.492141]\n",
      "epoch:29 step:27292 [D loss: 0.162057, acc.: 77.34%] [G loss: 0.513580]\n",
      "epoch:29 step:27293 [D loss: 0.229053, acc.: 62.50%] [G loss: 0.477216]\n",
      "epoch:29 step:27294 [D loss: 0.233349, acc.: 60.94%] [G loss: 0.446026]\n",
      "epoch:29 step:27295 [D loss: 0.199940, acc.: 71.88%] [G loss: 0.467911]\n",
      "epoch:29 step:27296 [D loss: 0.217988, acc.: 62.50%] [G loss: 0.450998]\n",
      "epoch:29 step:27297 [D loss: 0.255348, acc.: 55.47%] [G loss: 0.423410]\n",
      "epoch:29 step:27298 [D loss: 0.239974, acc.: 57.81%] [G loss: 0.430575]\n",
      "epoch:29 step:27299 [D loss: 0.198557, acc.: 67.97%] [G loss: 0.422853]\n",
      "epoch:29 step:27300 [D loss: 0.240442, acc.: 60.16%] [G loss: 0.413756]\n",
      "epoch:29 step:27301 [D loss: 0.229416, acc.: 63.28%] [G loss: 0.396078]\n",
      "epoch:29 step:27302 [D loss: 0.238696, acc.: 57.81%] [G loss: 0.400411]\n",
      "epoch:29 step:27303 [D loss: 0.215454, acc.: 63.28%] [G loss: 0.434694]\n",
      "epoch:29 step:27304 [D loss: 0.233369, acc.: 57.81%] [G loss: 0.393444]\n",
      "epoch:29 step:27305 [D loss: 0.215418, acc.: 64.84%] [G loss: 0.447036]\n",
      "epoch:29 step:27306 [D loss: 0.226201, acc.: 61.72%] [G loss: 0.437943]\n",
      "epoch:29 step:27307 [D loss: 0.219224, acc.: 64.06%] [G loss: 0.459339]\n",
      "epoch:29 step:27308 [D loss: 0.214028, acc.: 63.28%] [G loss: 0.426691]\n",
      "epoch:29 step:27309 [D loss: 0.199780, acc.: 71.09%] [G loss: 0.468296]\n",
      "epoch:29 step:27310 [D loss: 0.238430, acc.: 64.06%] [G loss: 0.399224]\n",
      "epoch:29 step:27311 [D loss: 0.242761, acc.: 57.81%] [G loss: 0.386055]\n",
      "epoch:29 step:27312 [D loss: 0.232702, acc.: 58.59%] [G loss: 0.374886]\n",
      "epoch:29 step:27313 [D loss: 0.242130, acc.: 55.47%] [G loss: 0.418320]\n",
      "epoch:29 step:27314 [D loss: 0.231447, acc.: 62.50%] [G loss: 0.440905]\n",
      "epoch:29 step:27315 [D loss: 0.234233, acc.: 57.81%] [G loss: 0.452298]\n",
      "epoch:29 step:27316 [D loss: 0.220106, acc.: 60.16%] [G loss: 0.427481]\n",
      "epoch:29 step:27317 [D loss: 0.238829, acc.: 64.84%] [G loss: 0.411255]\n",
      "epoch:29 step:27318 [D loss: 0.230107, acc.: 64.06%] [G loss: 0.426806]\n",
      "epoch:29 step:27319 [D loss: 0.229993, acc.: 63.28%] [G loss: 0.408980]\n",
      "epoch:29 step:27320 [D loss: 0.251812, acc.: 60.16%] [G loss: 0.409683]\n",
      "epoch:29 step:27321 [D loss: 0.225014, acc.: 62.50%] [G loss: 0.409090]\n",
      "epoch:29 step:27322 [D loss: 0.220412, acc.: 65.62%] [G loss: 0.404341]\n",
      "epoch:29 step:27323 [D loss: 0.230101, acc.: 57.81%] [G loss: 0.403631]\n",
      "epoch:29 step:27324 [D loss: 0.209103, acc.: 67.97%] [G loss: 0.429137]\n",
      "epoch:29 step:27325 [D loss: 0.236106, acc.: 58.59%] [G loss: 0.427164]\n",
      "epoch:29 step:27326 [D loss: 0.213132, acc.: 67.19%] [G loss: 0.468972]\n",
      "epoch:29 step:27327 [D loss: 0.217968, acc.: 65.62%] [G loss: 0.380006]\n",
      "epoch:29 step:27328 [D loss: 0.224630, acc.: 63.28%] [G loss: 0.397999]\n",
      "epoch:29 step:27329 [D loss: 0.216788, acc.: 63.28%] [G loss: 0.430104]\n",
      "epoch:29 step:27330 [D loss: 0.228236, acc.: 64.06%] [G loss: 0.428471]\n",
      "epoch:29 step:27331 [D loss: 0.223927, acc.: 61.72%] [G loss: 0.409793]\n",
      "epoch:29 step:27332 [D loss: 0.218444, acc.: 67.19%] [G loss: 0.429323]\n",
      "epoch:29 step:27333 [D loss: 0.250698, acc.: 57.81%] [G loss: 0.433111]\n",
      "epoch:29 step:27334 [D loss: 0.238130, acc.: 63.28%] [G loss: 0.449417]\n",
      "epoch:29 step:27335 [D loss: 0.265328, acc.: 53.91%] [G loss: 0.414087]\n",
      "epoch:29 step:27336 [D loss: 0.217210, acc.: 60.94%] [G loss: 0.407217]\n",
      "epoch:29 step:27337 [D loss: 0.217805, acc.: 68.75%] [G loss: 0.467978]\n",
      "epoch:29 step:27338 [D loss: 0.227874, acc.: 58.59%] [G loss: 0.463103]\n",
      "epoch:29 step:27339 [D loss: 0.227496, acc.: 61.72%] [G loss: 0.436956]\n",
      "epoch:29 step:27340 [D loss: 0.238689, acc.: 56.25%] [G loss: 0.430604]\n",
      "epoch:29 step:27341 [D loss: 0.188087, acc.: 70.31%] [G loss: 0.446007]\n",
      "epoch:29 step:27342 [D loss: 0.242507, acc.: 61.72%] [G loss: 0.451547]\n",
      "epoch:29 step:27343 [D loss: 0.241998, acc.: 53.91%] [G loss: 0.411602]\n",
      "epoch:29 step:27344 [D loss: 0.226935, acc.: 61.72%] [G loss: 0.406982]\n",
      "epoch:29 step:27345 [D loss: 0.220142, acc.: 64.84%] [G loss: 0.427498]\n",
      "epoch:29 step:27346 [D loss: 0.205996, acc.: 71.09%] [G loss: 0.421110]\n",
      "epoch:29 step:27347 [D loss: 0.222227, acc.: 64.84%] [G loss: 0.409404]\n",
      "epoch:29 step:27348 [D loss: 0.209355, acc.: 66.41%] [G loss: 0.388025]\n",
      "epoch:29 step:27349 [D loss: 0.222153, acc.: 64.06%] [G loss: 0.398109]\n",
      "epoch:29 step:27350 [D loss: 0.243658, acc.: 57.81%] [G loss: 0.390941]\n",
      "epoch:29 step:27351 [D loss: 0.226671, acc.: 65.62%] [G loss: 0.411819]\n",
      "epoch:29 step:27352 [D loss: 0.236757, acc.: 59.38%] [G loss: 0.410157]\n",
      "epoch:29 step:27353 [D loss: 0.233319, acc.: 61.72%] [G loss: 0.465162]\n",
      "epoch:29 step:27354 [D loss: 0.237824, acc.: 56.25%] [G loss: 0.394769]\n",
      "epoch:29 step:27355 [D loss: 0.234916, acc.: 58.59%] [G loss: 0.415154]\n",
      "epoch:29 step:27356 [D loss: 0.222401, acc.: 64.06%] [G loss: 0.452326]\n",
      "epoch:29 step:27357 [D loss: 0.249388, acc.: 56.25%] [G loss: 0.410971]\n",
      "epoch:29 step:27358 [D loss: 0.235112, acc.: 61.72%] [G loss: 0.374703]\n",
      "epoch:29 step:27359 [D loss: 0.228141, acc.: 61.72%] [G loss: 0.397233]\n",
      "epoch:29 step:27360 [D loss: 0.210249, acc.: 65.62%] [G loss: 0.456608]\n",
      "epoch:29 step:27361 [D loss: 0.244068, acc.: 57.81%] [G loss: 0.417467]\n",
      "epoch:29 step:27362 [D loss: 0.232831, acc.: 53.91%] [G loss: 0.391001]\n",
      "epoch:29 step:27363 [D loss: 0.234394, acc.: 60.16%] [G loss: 0.378277]\n",
      "epoch:29 step:27364 [D loss: 0.209791, acc.: 68.75%] [G loss: 0.427879]\n",
      "epoch:29 step:27365 [D loss: 0.225896, acc.: 59.38%] [G loss: 0.407189]\n",
      "epoch:29 step:27366 [D loss: 0.226267, acc.: 62.50%] [G loss: 0.430347]\n",
      "epoch:29 step:27367 [D loss: 0.211846, acc.: 69.53%] [G loss: 0.423241]\n",
      "epoch:29 step:27368 [D loss: 0.215540, acc.: 59.38%] [G loss: 0.423033]\n",
      "epoch:29 step:27369 [D loss: 0.212501, acc.: 65.62%] [G loss: 0.449417]\n",
      "epoch:29 step:27370 [D loss: 0.206535, acc.: 64.06%] [G loss: 0.467690]\n",
      "epoch:29 step:27371 [D loss: 0.209001, acc.: 69.53%] [G loss: 0.414715]\n",
      "epoch:29 step:27372 [D loss: 0.215768, acc.: 64.06%] [G loss: 0.416514]\n",
      "epoch:29 step:27373 [D loss: 0.260226, acc.: 54.69%] [G loss: 0.412142]\n",
      "epoch:29 step:27374 [D loss: 0.232145, acc.: 59.38%] [G loss: 0.408269]\n",
      "epoch:29 step:27375 [D loss: 0.227494, acc.: 60.16%] [G loss: 0.402570]\n",
      "epoch:29 step:27376 [D loss: 0.267288, acc.: 54.69%] [G loss: 0.417694]\n",
      "epoch:29 step:27377 [D loss: 0.217104, acc.: 70.31%] [G loss: 0.418891]\n",
      "epoch:29 step:27378 [D loss: 0.216834, acc.: 65.62%] [G loss: 0.461774]\n",
      "epoch:29 step:27379 [D loss: 0.218213, acc.: 66.41%] [G loss: 0.462015]\n",
      "epoch:29 step:27380 [D loss: 0.217546, acc.: 63.28%] [G loss: 0.454024]\n",
      "epoch:29 step:27381 [D loss: 0.166911, acc.: 75.00%] [G loss: 0.520541]\n",
      "epoch:29 step:27382 [D loss: 0.207218, acc.: 69.53%] [G loss: 0.483224]\n",
      "epoch:29 step:27383 [D loss: 0.265086, acc.: 53.12%] [G loss: 0.403336]\n",
      "epoch:29 step:27384 [D loss: 0.246656, acc.: 57.81%] [G loss: 0.411420]\n",
      "epoch:29 step:27385 [D loss: 0.247890, acc.: 56.25%] [G loss: 0.390647]\n",
      "epoch:29 step:27386 [D loss: 0.223081, acc.: 61.72%] [G loss: 0.429333]\n",
      "epoch:29 step:27387 [D loss: 0.238803, acc.: 60.94%] [G loss: 0.432995]\n",
      "epoch:29 step:27388 [D loss: 0.251172, acc.: 57.03%] [G loss: 0.372506]\n",
      "epoch:29 step:27389 [D loss: 0.221418, acc.: 68.75%] [G loss: 0.416724]\n",
      "epoch:29 step:27390 [D loss: 0.232109, acc.: 61.72%] [G loss: 0.413360]\n",
      "epoch:29 step:27391 [D loss: 0.200089, acc.: 74.22%] [G loss: 0.439218]\n",
      "epoch:29 step:27392 [D loss: 0.177615, acc.: 82.03%] [G loss: 0.460027]\n",
      "epoch:29 step:27393 [D loss: 0.266099, acc.: 49.22%] [G loss: 0.420788]\n",
      "epoch:29 step:27394 [D loss: 0.201330, acc.: 64.84%] [G loss: 0.441824]\n",
      "epoch:29 step:27395 [D loss: 0.244852, acc.: 63.28%] [G loss: 0.444088]\n",
      "epoch:29 step:27396 [D loss: 0.201055, acc.: 71.88%] [G loss: 0.503192]\n",
      "epoch:29 step:27397 [D loss: 0.245408, acc.: 57.81%] [G loss: 0.411103]\n",
      "epoch:29 step:27398 [D loss: 0.227179, acc.: 61.72%] [G loss: 0.401115]\n",
      "epoch:29 step:27399 [D loss: 0.235357, acc.: 58.59%] [G loss: 0.399976]\n",
      "epoch:29 step:27400 [D loss: 0.207812, acc.: 67.97%] [G loss: 0.399388]\n",
      "##############\n",
      "[2.66968217 1.84177932 6.1195337  4.7380112  3.38474173 5.79525787\n",
      " 4.68404034 4.87621715 4.43104592 4.18904439]\n",
      "##########\n",
      "epoch:29 step:27401 [D loss: 0.241815, acc.: 57.81%] [G loss: 0.403324]\n",
      "epoch:29 step:27402 [D loss: 0.196217, acc.: 67.97%] [G loss: 0.386076]\n",
      "epoch:29 step:27403 [D loss: 0.210859, acc.: 68.75%] [G loss: 0.424427]\n",
      "epoch:29 step:27404 [D loss: 0.188290, acc.: 70.31%] [G loss: 0.465425]\n",
      "epoch:29 step:27405 [D loss: 0.170179, acc.: 76.56%] [G loss: 0.524869]\n",
      "epoch:29 step:27406 [D loss: 0.243267, acc.: 62.50%] [G loss: 0.436627]\n",
      "epoch:29 step:27407 [D loss: 0.265467, acc.: 50.00%] [G loss: 0.423191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27408 [D loss: 0.239790, acc.: 57.03%] [G loss: 0.424319]\n",
      "epoch:29 step:27409 [D loss: 0.211445, acc.: 67.19%] [G loss: 0.429547]\n",
      "epoch:29 step:27410 [D loss: 0.204529, acc.: 69.53%] [G loss: 0.406859]\n",
      "epoch:29 step:27411 [D loss: 0.212351, acc.: 67.97%] [G loss: 0.409922]\n",
      "epoch:29 step:27412 [D loss: 0.228824, acc.: 60.94%] [G loss: 0.409432]\n",
      "epoch:29 step:27413 [D loss: 0.244134, acc.: 60.16%] [G loss: 0.431297]\n",
      "epoch:29 step:27414 [D loss: 0.195722, acc.: 70.31%] [G loss: 0.454762]\n",
      "epoch:29 step:27415 [D loss: 0.217762, acc.: 62.50%] [G loss: 0.455844]\n",
      "epoch:29 step:27416 [D loss: 0.236906, acc.: 57.03%] [G loss: 0.419882]\n",
      "epoch:29 step:27417 [D loss: 0.203776, acc.: 69.53%] [G loss: 0.453563]\n",
      "epoch:29 step:27418 [D loss: 0.217859, acc.: 64.84%] [G loss: 0.441740]\n",
      "epoch:29 step:27419 [D loss: 0.207727, acc.: 65.62%] [G loss: 0.413522]\n",
      "epoch:29 step:27420 [D loss: 0.236534, acc.: 60.94%] [G loss: 0.440333]\n",
      "epoch:29 step:27421 [D loss: 0.201997, acc.: 67.19%] [G loss: 0.465732]\n",
      "epoch:29 step:27422 [D loss: 0.240091, acc.: 58.59%] [G loss: 0.430945]\n",
      "epoch:29 step:27423 [D loss: 0.266341, acc.: 53.91%] [G loss: 0.406646]\n",
      "epoch:29 step:27424 [D loss: 0.264523, acc.: 50.78%] [G loss: 0.399133]\n",
      "epoch:29 step:27425 [D loss: 0.227177, acc.: 60.94%] [G loss: 0.382110]\n",
      "epoch:29 step:27426 [D loss: 0.237291, acc.: 60.16%] [G loss: 0.408719]\n",
      "epoch:29 step:27427 [D loss: 0.222885, acc.: 68.75%] [G loss: 0.407360]\n",
      "epoch:29 step:27428 [D loss: 0.209564, acc.: 67.19%] [G loss: 0.396122]\n",
      "epoch:29 step:27429 [D loss: 0.207102, acc.: 70.31%] [G loss: 0.444903]\n",
      "epoch:29 step:27430 [D loss: 0.239397, acc.: 60.16%] [G loss: 0.410654]\n",
      "epoch:29 step:27431 [D loss: 0.214375, acc.: 64.06%] [G loss: 0.412901]\n",
      "epoch:29 step:27432 [D loss: 0.213324, acc.: 64.84%] [G loss: 0.457005]\n",
      "epoch:29 step:27433 [D loss: 0.218906, acc.: 62.50%] [G loss: 0.460013]\n",
      "epoch:29 step:27434 [D loss: 0.213033, acc.: 64.06%] [G loss: 0.501354]\n",
      "epoch:29 step:27435 [D loss: 0.225111, acc.: 62.50%] [G loss: 0.417080]\n",
      "epoch:29 step:27436 [D loss: 0.234562, acc.: 58.59%] [G loss: 0.445545]\n",
      "epoch:29 step:27437 [D loss: 0.207676, acc.: 67.19%] [G loss: 0.407075]\n",
      "epoch:29 step:27438 [D loss: 0.233538, acc.: 54.69%] [G loss: 0.450730]\n",
      "epoch:29 step:27439 [D loss: 0.229821, acc.: 60.16%] [G loss: 0.398065]\n",
      "epoch:29 step:27440 [D loss: 0.224428, acc.: 62.50%] [G loss: 0.385584]\n",
      "epoch:29 step:27441 [D loss: 0.216286, acc.: 61.72%] [G loss: 0.416394]\n",
      "epoch:29 step:27442 [D loss: 0.212604, acc.: 69.53%] [G loss: 0.420268]\n",
      "epoch:29 step:27443 [D loss: 0.216049, acc.: 70.31%] [G loss: 0.420499]\n",
      "epoch:29 step:27444 [D loss: 0.190214, acc.: 75.00%] [G loss: 0.438042]\n",
      "epoch:29 step:27445 [D loss: 0.213374, acc.: 66.41%] [G loss: 0.430102]\n",
      "epoch:29 step:27446 [D loss: 0.194755, acc.: 71.09%] [G loss: 0.425820]\n",
      "epoch:29 step:27447 [D loss: 0.173994, acc.: 74.22%] [G loss: 0.430428]\n",
      "epoch:29 step:27448 [D loss: 0.241178, acc.: 60.94%] [G loss: 0.440202]\n",
      "epoch:29 step:27449 [D loss: 0.210318, acc.: 66.41%] [G loss: 0.431715]\n",
      "epoch:29 step:27450 [D loss: 0.240869, acc.: 58.59%] [G loss: 0.438633]\n",
      "epoch:29 step:27451 [D loss: 0.265521, acc.: 46.88%] [G loss: 0.437184]\n",
      "epoch:29 step:27452 [D loss: 0.219592, acc.: 63.28%] [G loss: 0.424752]\n",
      "epoch:29 step:27453 [D loss: 0.173726, acc.: 75.78%] [G loss: 0.456144]\n",
      "epoch:29 step:27454 [D loss: 0.255725, acc.: 56.25%] [G loss: 0.413075]\n",
      "epoch:29 step:27455 [D loss: 0.240897, acc.: 62.50%] [G loss: 0.396635]\n",
      "epoch:29 step:27456 [D loss: 0.212845, acc.: 65.62%] [G loss: 0.437968]\n",
      "epoch:29 step:27457 [D loss: 0.223930, acc.: 57.81%] [G loss: 0.431589]\n",
      "epoch:29 step:27458 [D loss: 0.240587, acc.: 58.59%] [G loss: 0.390989]\n",
      "epoch:29 step:27459 [D loss: 0.210991, acc.: 64.84%] [G loss: 0.410822]\n",
      "epoch:29 step:27460 [D loss: 0.243953, acc.: 54.69%] [G loss: 0.411630]\n",
      "epoch:29 step:27461 [D loss: 0.238968, acc.: 60.94%] [G loss: 0.419550]\n",
      "epoch:29 step:27462 [D loss: 0.206419, acc.: 65.62%] [G loss: 0.466615]\n",
      "epoch:29 step:27463 [D loss: 0.205519, acc.: 70.31%] [G loss: 0.432729]\n",
      "epoch:29 step:27464 [D loss: 0.261242, acc.: 53.91%] [G loss: 0.406286]\n",
      "epoch:29 step:27465 [D loss: 0.196423, acc.: 71.09%] [G loss: 0.451526]\n",
      "epoch:29 step:27466 [D loss: 0.220023, acc.: 63.28%] [G loss: 0.446043]\n",
      "epoch:29 step:27467 [D loss: 0.241207, acc.: 56.25%] [G loss: 0.421156]\n",
      "epoch:29 step:27468 [D loss: 0.236902, acc.: 61.72%] [G loss: 0.421916]\n",
      "epoch:29 step:27469 [D loss: 0.209229, acc.: 67.19%] [G loss: 0.408624]\n",
      "epoch:29 step:27470 [D loss: 0.235590, acc.: 58.59%] [G loss: 0.400552]\n",
      "epoch:29 step:27471 [D loss: 0.209837, acc.: 67.97%] [G loss: 0.430587]\n",
      "epoch:29 step:27472 [D loss: 0.205983, acc.: 68.75%] [G loss: 0.431404]\n",
      "epoch:29 step:27473 [D loss: 0.195494, acc.: 72.66%] [G loss: 0.445641]\n",
      "epoch:29 step:27474 [D loss: 0.246030, acc.: 55.47%] [G loss: 0.416545]\n",
      "epoch:29 step:27475 [D loss: 0.236423, acc.: 53.91%] [G loss: 0.397395]\n",
      "epoch:29 step:27476 [D loss: 0.235064, acc.: 63.28%] [G loss: 0.403359]\n",
      "epoch:29 step:27477 [D loss: 0.212459, acc.: 67.97%] [G loss: 0.459054]\n",
      "epoch:29 step:27478 [D loss: 0.219395, acc.: 60.94%] [G loss: 0.413745]\n",
      "epoch:29 step:27479 [D loss: 0.212886, acc.: 66.41%] [G loss: 0.455533]\n",
      "epoch:29 step:27480 [D loss: 0.235073, acc.: 59.38%] [G loss: 0.417675]\n",
      "epoch:29 step:27481 [D loss: 0.231026, acc.: 60.16%] [G loss: 0.382088]\n",
      "epoch:29 step:27482 [D loss: 0.214969, acc.: 67.19%] [G loss: 0.428957]\n",
      "epoch:29 step:27483 [D loss: 0.196054, acc.: 68.75%] [G loss: 0.430462]\n",
      "epoch:29 step:27484 [D loss: 0.196458, acc.: 74.22%] [G loss: 0.434154]\n",
      "epoch:29 step:27485 [D loss: 0.190592, acc.: 71.88%] [G loss: 0.457320]\n",
      "epoch:29 step:27486 [D loss: 0.188241, acc.: 71.88%] [G loss: 0.457210]\n",
      "epoch:29 step:27487 [D loss: 0.209297, acc.: 67.97%] [G loss: 0.448339]\n",
      "epoch:29 step:27488 [D loss: 0.187779, acc.: 71.88%] [G loss: 0.480536]\n",
      "epoch:29 step:27489 [D loss: 0.276685, acc.: 50.00%] [G loss: 0.427699]\n",
      "epoch:29 step:27490 [D loss: 0.243200, acc.: 58.59%] [G loss: 0.433095]\n",
      "epoch:29 step:27491 [D loss: 0.200674, acc.: 66.41%] [G loss: 0.421698]\n",
      "epoch:29 step:27492 [D loss: 0.198532, acc.: 71.09%] [G loss: 0.443643]\n",
      "epoch:29 step:27493 [D loss: 0.224455, acc.: 64.06%] [G loss: 0.423245]\n",
      "epoch:29 step:27494 [D loss: 0.196340, acc.: 70.31%] [G loss: 0.441533]\n",
      "epoch:29 step:27495 [D loss: 0.212302, acc.: 67.19%] [G loss: 0.410430]\n",
      "epoch:29 step:27496 [D loss: 0.243694, acc.: 55.47%] [G loss: 0.415884]\n",
      "epoch:29 step:27497 [D loss: 0.230006, acc.: 64.84%] [G loss: 0.396554]\n",
      "epoch:29 step:27498 [D loss: 0.226980, acc.: 62.50%] [G loss: 0.414474]\n",
      "epoch:29 step:27499 [D loss: 0.228624, acc.: 59.38%] [G loss: 0.412467]\n",
      "epoch:29 step:27500 [D loss: 0.225034, acc.: 64.06%] [G loss: 0.418852]\n",
      "epoch:29 step:27501 [D loss: 0.192818, acc.: 71.88%] [G loss: 0.470397]\n",
      "epoch:29 step:27502 [D loss: 0.228490, acc.: 61.72%] [G loss: 0.386291]\n",
      "epoch:29 step:27503 [D loss: 0.223083, acc.: 66.41%] [G loss: 0.444459]\n",
      "epoch:29 step:27504 [D loss: 0.224608, acc.: 60.16%] [G loss: 0.423881]\n",
      "epoch:29 step:27505 [D loss: 0.213523, acc.: 64.06%] [G loss: 0.440525]\n",
      "epoch:29 step:27506 [D loss: 0.190989, acc.: 71.09%] [G loss: 0.467009]\n",
      "epoch:29 step:27507 [D loss: 0.218509, acc.: 60.94%] [G loss: 0.408098]\n",
      "epoch:29 step:27508 [D loss: 0.203577, acc.: 67.19%] [G loss: 0.465098]\n",
      "epoch:29 step:27509 [D loss: 0.205001, acc.: 67.19%] [G loss: 0.451345]\n",
      "epoch:29 step:27510 [D loss: 0.218237, acc.: 65.62%] [G loss: 0.430315]\n",
      "epoch:29 step:27511 [D loss: 0.218612, acc.: 67.97%] [G loss: 0.393289]\n",
      "epoch:29 step:27512 [D loss: 0.213829, acc.: 69.53%] [G loss: 0.432238]\n",
      "epoch:29 step:27513 [D loss: 0.203287, acc.: 73.44%] [G loss: 0.443926]\n",
      "epoch:29 step:27514 [D loss: 0.308827, acc.: 42.97%] [G loss: 0.416985]\n",
      "epoch:29 step:27515 [D loss: 0.240087, acc.: 55.47%] [G loss: 0.465634]\n",
      "epoch:29 step:27516 [D loss: 0.216481, acc.: 59.38%] [G loss: 0.445320]\n",
      "epoch:29 step:27517 [D loss: 0.202917, acc.: 67.19%] [G loss: 0.472078]\n",
      "epoch:29 step:27518 [D loss: 0.232598, acc.: 60.16%] [G loss: 0.459474]\n",
      "epoch:29 step:27519 [D loss: 0.208720, acc.: 64.06%] [G loss: 0.474085]\n",
      "epoch:29 step:27520 [D loss: 0.180994, acc.: 72.66%] [G loss: 0.562492]\n",
      "epoch:29 step:27521 [D loss: 0.244470, acc.: 60.16%] [G loss: 0.478176]\n",
      "epoch:29 step:27522 [D loss: 0.283179, acc.: 51.56%] [G loss: 0.452498]\n",
      "epoch:29 step:27523 [D loss: 0.207706, acc.: 67.19%] [G loss: 0.417560]\n",
      "epoch:29 step:27524 [D loss: 0.241914, acc.: 57.81%] [G loss: 0.416932]\n",
      "epoch:29 step:27525 [D loss: 0.222042, acc.: 64.84%] [G loss: 0.447267]\n",
      "epoch:29 step:27526 [D loss: 0.200184, acc.: 68.75%] [G loss: 0.467255]\n",
      "epoch:29 step:27527 [D loss: 0.196709, acc.: 71.09%] [G loss: 0.457060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27528 [D loss: 0.237852, acc.: 60.16%] [G loss: 0.449920]\n",
      "epoch:29 step:27529 [D loss: 0.242243, acc.: 61.72%] [G loss: 0.392747]\n",
      "epoch:29 step:27530 [D loss: 0.217137, acc.: 64.06%] [G loss: 0.435536]\n",
      "epoch:29 step:27531 [D loss: 0.208556, acc.: 71.09%] [G loss: 0.434965]\n",
      "epoch:29 step:27532 [D loss: 0.242326, acc.: 63.28%] [G loss: 0.468798]\n",
      "epoch:29 step:27533 [D loss: 0.202224, acc.: 70.31%] [G loss: 0.459454]\n",
      "epoch:29 step:27534 [D loss: 0.212241, acc.: 66.41%] [G loss: 0.430112]\n",
      "epoch:29 step:27535 [D loss: 0.254714, acc.: 56.25%] [G loss: 0.404308]\n",
      "epoch:29 step:27536 [D loss: 0.213743, acc.: 63.28%] [G loss: 0.410079]\n",
      "epoch:29 step:27537 [D loss: 0.224704, acc.: 58.59%] [G loss: 0.407877]\n",
      "epoch:29 step:27538 [D loss: 0.219805, acc.: 66.41%] [G loss: 0.410524]\n",
      "epoch:29 step:27539 [D loss: 0.227867, acc.: 58.59%] [G loss: 0.433500]\n",
      "epoch:29 step:27540 [D loss: 0.226066, acc.: 60.94%] [G loss: 0.415511]\n",
      "epoch:29 step:27541 [D loss: 0.263825, acc.: 55.47%] [G loss: 0.430803]\n",
      "epoch:29 step:27542 [D loss: 0.208442, acc.: 65.62%] [G loss: 0.424422]\n",
      "epoch:29 step:27543 [D loss: 0.191830, acc.: 75.00%] [G loss: 0.421789]\n",
      "epoch:29 step:27544 [D loss: 0.188623, acc.: 68.75%] [G loss: 0.425032]\n",
      "epoch:29 step:27545 [D loss: 0.232017, acc.: 65.62%] [G loss: 0.453956]\n",
      "epoch:29 step:27546 [D loss: 0.228127, acc.: 60.94%] [G loss: 0.443514]\n",
      "epoch:29 step:27547 [D loss: 0.213636, acc.: 68.75%] [G loss: 0.480717]\n",
      "epoch:29 step:27548 [D loss: 0.216644, acc.: 64.84%] [G loss: 0.449680]\n",
      "epoch:29 step:27549 [D loss: 0.263762, acc.: 57.03%] [G loss: 0.427596]\n",
      "epoch:29 step:27550 [D loss: 0.245790, acc.: 55.47%] [G loss: 0.428092]\n",
      "epoch:29 step:27551 [D loss: 0.235987, acc.: 57.81%] [G loss: 0.434898]\n",
      "epoch:29 step:27552 [D loss: 0.238864, acc.: 60.16%] [G loss: 0.425302]\n",
      "epoch:29 step:27553 [D loss: 0.243634, acc.: 60.94%] [G loss: 0.426906]\n",
      "epoch:29 step:27554 [D loss: 0.223105, acc.: 59.38%] [G loss: 0.409829]\n",
      "epoch:29 step:27555 [D loss: 0.225802, acc.: 61.72%] [G loss: 0.415320]\n",
      "epoch:29 step:27556 [D loss: 0.220587, acc.: 67.19%] [G loss: 0.390081]\n",
      "epoch:29 step:27557 [D loss: 0.214867, acc.: 62.50%] [G loss: 0.413471]\n",
      "epoch:29 step:27558 [D loss: 0.185201, acc.: 68.75%] [G loss: 0.452413]\n",
      "epoch:29 step:27559 [D loss: 0.244065, acc.: 58.59%] [G loss: 0.426665]\n",
      "epoch:29 step:27560 [D loss: 0.222421, acc.: 64.84%] [G loss: 0.422774]\n",
      "epoch:29 step:27561 [D loss: 0.213860, acc.: 65.62%] [G loss: 0.452736]\n",
      "epoch:29 step:27562 [D loss: 0.222144, acc.: 62.50%] [G loss: 0.495033]\n",
      "epoch:29 step:27563 [D loss: 0.232175, acc.: 57.03%] [G loss: 0.434389]\n",
      "epoch:29 step:27564 [D loss: 0.203833, acc.: 67.19%] [G loss: 0.436931]\n",
      "epoch:29 step:27565 [D loss: 0.211845, acc.: 63.28%] [G loss: 0.401885]\n",
      "epoch:29 step:27566 [D loss: 0.202952, acc.: 66.41%] [G loss: 0.435936]\n",
      "epoch:29 step:27567 [D loss: 0.240055, acc.: 54.69%] [G loss: 0.384919]\n",
      "epoch:29 step:27568 [D loss: 0.229035, acc.: 59.38%] [G loss: 0.425925]\n",
      "epoch:29 step:27569 [D loss: 0.230606, acc.: 61.72%] [G loss: 0.428821]\n",
      "epoch:29 step:27570 [D loss: 0.230531, acc.: 60.94%] [G loss: 0.441910]\n",
      "epoch:29 step:27571 [D loss: 0.204026, acc.: 71.88%] [G loss: 0.479164]\n",
      "epoch:29 step:27572 [D loss: 0.203555, acc.: 64.84%] [G loss: 0.445021]\n",
      "epoch:29 step:27573 [D loss: 0.248076, acc.: 62.50%] [G loss: 0.441391]\n",
      "epoch:29 step:27574 [D loss: 0.226296, acc.: 61.72%] [G loss: 0.447435]\n",
      "epoch:29 step:27575 [D loss: 0.218336, acc.: 67.19%] [G loss: 0.378561]\n",
      "epoch:29 step:27576 [D loss: 0.234114, acc.: 59.38%] [G loss: 0.422174]\n",
      "epoch:29 step:27577 [D loss: 0.222657, acc.: 60.94%] [G loss: 0.425819]\n",
      "epoch:29 step:27578 [D loss: 0.214804, acc.: 65.62%] [G loss: 0.415570]\n",
      "epoch:29 step:27579 [D loss: 0.191728, acc.: 75.00%] [G loss: 0.470911]\n",
      "epoch:29 step:27580 [D loss: 0.224772, acc.: 64.84%] [G loss: 0.458536]\n",
      "epoch:29 step:27581 [D loss: 0.224288, acc.: 64.06%] [G loss: 0.476469]\n",
      "epoch:29 step:27582 [D loss: 0.213956, acc.: 66.41%] [G loss: 0.413192]\n",
      "epoch:29 step:27583 [D loss: 0.243144, acc.: 60.94%] [G loss: 0.408004]\n",
      "epoch:29 step:27584 [D loss: 0.255675, acc.: 57.03%] [G loss: 0.428013]\n",
      "epoch:29 step:27585 [D loss: 0.240993, acc.: 61.72%] [G loss: 0.411348]\n",
      "epoch:29 step:27586 [D loss: 0.230654, acc.: 60.94%] [G loss: 0.442526]\n",
      "epoch:29 step:27587 [D loss: 0.210758, acc.: 67.19%] [G loss: 0.443838]\n",
      "epoch:29 step:27588 [D loss: 0.219906, acc.: 60.94%] [G loss: 0.447262]\n",
      "epoch:29 step:27589 [D loss: 0.201449, acc.: 69.53%] [G loss: 0.464920]\n",
      "epoch:29 step:27590 [D loss: 0.223856, acc.: 67.19%] [G loss: 0.488271]\n",
      "epoch:29 step:27591 [D loss: 0.275000, acc.: 45.31%] [G loss: 0.442663]\n",
      "epoch:29 step:27592 [D loss: 0.219958, acc.: 67.97%] [G loss: 0.471740]\n",
      "epoch:29 step:27593 [D loss: 0.225802, acc.: 61.72%] [G loss: 0.391268]\n",
      "epoch:29 step:27594 [D loss: 0.241452, acc.: 58.59%] [G loss: 0.406940]\n",
      "epoch:29 step:27595 [D loss: 0.273719, acc.: 50.00%] [G loss: 0.365651]\n",
      "epoch:29 step:27596 [D loss: 0.228330, acc.: 62.50%] [G loss: 0.411888]\n",
      "epoch:29 step:27597 [D loss: 0.215817, acc.: 61.72%] [G loss: 0.450024]\n",
      "epoch:29 step:27598 [D loss: 0.242435, acc.: 57.81%] [G loss: 0.410943]\n",
      "epoch:29 step:27599 [D loss: 0.222236, acc.: 60.16%] [G loss: 0.418725]\n",
      "epoch:29 step:27600 [D loss: 0.207329, acc.: 71.88%] [G loss: 0.421111]\n",
      "##############\n",
      "[2.81919016 1.9930673  6.10454183 4.83222226 3.57019066 5.60961263\n",
      " 4.5668139  4.77613218 4.625139   3.96564564]\n",
      "##########\n",
      "epoch:29 step:27601 [D loss: 0.210072, acc.: 64.06%] [G loss: 0.439790]\n",
      "epoch:29 step:27602 [D loss: 0.186353, acc.: 71.88%] [G loss: 0.465995]\n",
      "epoch:29 step:27603 [D loss: 0.187339, acc.: 69.53%] [G loss: 0.474959]\n",
      "epoch:29 step:27604 [D loss: 0.228660, acc.: 60.16%] [G loss: 0.415142]\n",
      "epoch:29 step:27605 [D loss: 0.251715, acc.: 59.38%] [G loss: 0.464253]\n",
      "epoch:29 step:27606 [D loss: 0.229975, acc.: 63.28%] [G loss: 0.421215]\n",
      "epoch:29 step:27607 [D loss: 0.214018, acc.: 66.41%] [G loss: 0.430208]\n",
      "epoch:29 step:27608 [D loss: 0.198848, acc.: 69.53%] [G loss: 0.448983]\n",
      "epoch:29 step:27609 [D loss: 0.199209, acc.: 70.31%] [G loss: 0.441405]\n",
      "epoch:29 step:27610 [D loss: 0.279698, acc.: 46.88%] [G loss: 0.418030]\n",
      "epoch:29 step:27611 [D loss: 0.232403, acc.: 54.69%] [G loss: 0.394915]\n",
      "epoch:29 step:27612 [D loss: 0.233505, acc.: 60.16%] [G loss: 0.455189]\n",
      "epoch:29 step:27613 [D loss: 0.215555, acc.: 67.19%] [G loss: 0.443211]\n",
      "epoch:29 step:27614 [D loss: 0.223411, acc.: 61.72%] [G loss: 0.432056]\n",
      "epoch:29 step:27615 [D loss: 0.249951, acc.: 56.25%] [G loss: 0.397563]\n",
      "epoch:29 step:27616 [D loss: 0.244866, acc.: 61.72%] [G loss: 0.384190]\n",
      "epoch:29 step:27617 [D loss: 0.220246, acc.: 65.62%] [G loss: 0.406591]\n",
      "epoch:29 step:27618 [D loss: 0.205178, acc.: 62.50%] [G loss: 0.450063]\n",
      "epoch:29 step:27619 [D loss: 0.250989, acc.: 60.94%] [G loss: 0.423177]\n",
      "epoch:29 step:27620 [D loss: 0.203065, acc.: 68.75%] [G loss: 0.455176]\n",
      "epoch:29 step:27621 [D loss: 0.263611, acc.: 53.12%] [G loss: 0.395910]\n",
      "epoch:29 step:27622 [D loss: 0.257336, acc.: 53.12%] [G loss: 0.417306]\n",
      "epoch:29 step:27623 [D loss: 0.220656, acc.: 64.84%] [G loss: 0.423915]\n",
      "epoch:29 step:27624 [D loss: 0.197650, acc.: 70.31%] [G loss: 0.409054]\n",
      "epoch:29 step:27625 [D loss: 0.194859, acc.: 72.66%] [G loss: 0.414641]\n",
      "epoch:29 step:27626 [D loss: 0.217736, acc.: 65.62%] [G loss: 0.451199]\n",
      "epoch:29 step:27627 [D loss: 0.205050, acc.: 69.53%] [G loss: 0.489204]\n",
      "epoch:29 step:27628 [D loss: 0.241221, acc.: 57.81%] [G loss: 0.477106]\n",
      "epoch:29 step:27629 [D loss: 0.205162, acc.: 67.19%] [G loss: 0.469186]\n",
      "epoch:29 step:27630 [D loss: 0.215855, acc.: 63.28%] [G loss: 0.480214]\n",
      "epoch:29 step:27631 [D loss: 0.274374, acc.: 46.09%] [G loss: 0.414378]\n",
      "epoch:29 step:27632 [D loss: 0.217726, acc.: 67.19%] [G loss: 0.430061]\n",
      "epoch:29 step:27633 [D loss: 0.250738, acc.: 55.47%] [G loss: 0.414023]\n",
      "epoch:29 step:27634 [D loss: 0.217074, acc.: 67.97%] [G loss: 0.411655]\n",
      "epoch:29 step:27635 [D loss: 0.231398, acc.: 58.59%] [G loss: 0.407974]\n",
      "epoch:29 step:27636 [D loss: 0.245629, acc.: 53.91%] [G loss: 0.365804]\n",
      "epoch:29 step:27637 [D loss: 0.219358, acc.: 64.84%] [G loss: 0.429887]\n",
      "epoch:29 step:27638 [D loss: 0.210925, acc.: 65.62%] [G loss: 0.427649]\n",
      "epoch:29 step:27639 [D loss: 0.219393, acc.: 66.41%] [G loss: 0.402250]\n",
      "epoch:29 step:27640 [D loss: 0.222206, acc.: 64.06%] [G loss: 0.443231]\n",
      "epoch:29 step:27641 [D loss: 0.245977, acc.: 57.81%] [G loss: 0.402542]\n",
      "epoch:29 step:27642 [D loss: 0.183578, acc.: 75.00%] [G loss: 0.491070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27643 [D loss: 0.210232, acc.: 67.19%] [G loss: 0.472424]\n",
      "epoch:29 step:27644 [D loss: 0.170776, acc.: 81.25%] [G loss: 0.448410]\n",
      "epoch:29 step:27645 [D loss: 0.199372, acc.: 73.44%] [G loss: 0.484862]\n",
      "epoch:29 step:27646 [D loss: 0.254242, acc.: 53.91%] [G loss: 0.433094]\n",
      "epoch:29 step:27647 [D loss: 0.212570, acc.: 63.28%] [G loss: 0.437844]\n",
      "epoch:29 step:27648 [D loss: 0.192773, acc.: 70.31%] [G loss: 0.477293]\n",
      "epoch:29 step:27649 [D loss: 0.202560, acc.: 71.88%] [G loss: 0.465533]\n",
      "epoch:29 step:27650 [D loss: 0.261301, acc.: 50.78%] [G loss: 0.441765]\n",
      "epoch:29 step:27651 [D loss: 0.220393, acc.: 64.06%] [G loss: 0.392441]\n",
      "epoch:29 step:27652 [D loss: 0.227459, acc.: 64.06%] [G loss: 0.403384]\n",
      "epoch:29 step:27653 [D loss: 0.218577, acc.: 63.28%] [G loss: 0.402375]\n",
      "epoch:29 step:27654 [D loss: 0.195364, acc.: 66.41%] [G loss: 0.421063]\n",
      "epoch:29 step:27655 [D loss: 0.253209, acc.: 53.91%] [G loss: 0.432049]\n",
      "epoch:29 step:27656 [D loss: 0.217143, acc.: 63.28%] [G loss: 0.420961]\n",
      "epoch:29 step:27657 [D loss: 0.199425, acc.: 75.00%] [G loss: 0.449910]\n",
      "epoch:29 step:27658 [D loss: 0.219835, acc.: 71.09%] [G loss: 0.457150]\n",
      "epoch:29 step:27659 [D loss: 0.220595, acc.: 63.28%] [G loss: 0.473375]\n",
      "epoch:29 step:27660 [D loss: 0.241315, acc.: 57.81%] [G loss: 0.441625]\n",
      "epoch:29 step:27661 [D loss: 0.192200, acc.: 69.53%] [G loss: 0.428939]\n",
      "epoch:29 step:27662 [D loss: 0.241068, acc.: 59.38%] [G loss: 0.439249]\n",
      "epoch:29 step:27663 [D loss: 0.220621, acc.: 64.84%] [G loss: 0.467911]\n",
      "epoch:29 step:27664 [D loss: 0.223700, acc.: 64.06%] [G loss: 0.428623]\n",
      "epoch:29 step:27665 [D loss: 0.242527, acc.: 53.91%] [G loss: 0.399857]\n",
      "epoch:29 step:27666 [D loss: 0.194828, acc.: 68.75%] [G loss: 0.428222]\n",
      "epoch:29 step:27667 [D loss: 0.211901, acc.: 65.62%] [G loss: 0.418907]\n",
      "epoch:29 step:27668 [D loss: 0.195117, acc.: 67.97%] [G loss: 0.478665]\n",
      "epoch:29 step:27669 [D loss: 0.217086, acc.: 67.97%] [G loss: 0.444476]\n",
      "epoch:29 step:27670 [D loss: 0.221471, acc.: 60.16%] [G loss: 0.411316]\n",
      "epoch:29 step:27671 [D loss: 0.213991, acc.: 65.62%] [G loss: 0.454211]\n",
      "epoch:29 step:27672 [D loss: 0.184469, acc.: 75.78%] [G loss: 0.429639]\n",
      "epoch:29 step:27673 [D loss: 0.240986, acc.: 60.16%] [G loss: 0.464498]\n",
      "epoch:29 step:27674 [D loss: 0.255730, acc.: 50.78%] [G loss: 0.404969]\n",
      "epoch:29 step:27675 [D loss: 0.239521, acc.: 60.16%] [G loss: 0.407949]\n",
      "epoch:29 step:27676 [D loss: 0.225145, acc.: 63.28%] [G loss: 0.416291]\n",
      "epoch:29 step:27677 [D loss: 0.205537, acc.: 71.88%] [G loss: 0.417486]\n",
      "epoch:29 step:27678 [D loss: 0.223581, acc.: 64.84%] [G loss: 0.418710]\n",
      "epoch:29 step:27679 [D loss: 0.210810, acc.: 64.06%] [G loss: 0.445330]\n",
      "epoch:29 step:27680 [D loss: 0.211615, acc.: 66.41%] [G loss: 0.460132]\n",
      "epoch:29 step:27681 [D loss: 0.198053, acc.: 71.09%] [G loss: 0.495529]\n",
      "epoch:29 step:27682 [D loss: 0.241090, acc.: 55.47%] [G loss: 0.433032]\n",
      "epoch:29 step:27683 [D loss: 0.231605, acc.: 65.62%] [G loss: 0.401639]\n",
      "epoch:29 step:27684 [D loss: 0.248838, acc.: 55.47%] [G loss: 0.407726]\n",
      "epoch:29 step:27685 [D loss: 0.201977, acc.: 72.66%] [G loss: 0.458044]\n",
      "epoch:29 step:27686 [D loss: 0.200046, acc.: 69.53%] [G loss: 0.399859]\n",
      "epoch:29 step:27687 [D loss: 0.213023, acc.: 67.19%] [G loss: 0.420846]\n",
      "epoch:29 step:27688 [D loss: 0.220392, acc.: 67.97%] [G loss: 0.410867]\n",
      "epoch:29 step:27689 [D loss: 0.201868, acc.: 71.09%] [G loss: 0.464165]\n",
      "epoch:29 step:27690 [D loss: 0.218870, acc.: 61.72%] [G loss: 0.446548]\n",
      "epoch:29 step:27691 [D loss: 0.241662, acc.: 60.94%] [G loss: 0.376957]\n",
      "epoch:29 step:27692 [D loss: 0.195038, acc.: 69.53%] [G loss: 0.411443]\n",
      "epoch:29 step:27693 [D loss: 0.214359, acc.: 60.94%] [G loss: 0.404807]\n",
      "epoch:29 step:27694 [D loss: 0.212981, acc.: 67.19%] [G loss: 0.468938]\n",
      "epoch:29 step:27695 [D loss: 0.212593, acc.: 62.50%] [G loss: 0.452506]\n",
      "epoch:29 step:27696 [D loss: 0.196828, acc.: 69.53%] [G loss: 0.443309]\n",
      "epoch:29 step:27697 [D loss: 0.241500, acc.: 65.62%] [G loss: 0.440572]\n",
      "epoch:29 step:27698 [D loss: 0.204573, acc.: 66.41%] [G loss: 0.446778]\n",
      "epoch:29 step:27699 [D loss: 0.206662, acc.: 71.88%] [G loss: 0.446185]\n",
      "epoch:29 step:27700 [D loss: 0.225632, acc.: 60.16%] [G loss: 0.421361]\n",
      "epoch:29 step:27701 [D loss: 0.264337, acc.: 57.03%] [G loss: 0.407996]\n",
      "epoch:29 step:27702 [D loss: 0.231184, acc.: 55.47%] [G loss: 0.476491]\n",
      "epoch:29 step:27703 [D loss: 0.226387, acc.: 60.16%] [G loss: 0.468313]\n",
      "epoch:29 step:27704 [D loss: 0.232466, acc.: 64.06%] [G loss: 0.404298]\n",
      "epoch:29 step:27705 [D loss: 0.248952, acc.: 55.47%] [G loss: 0.385116]\n",
      "epoch:29 step:27706 [D loss: 0.238296, acc.: 60.16%] [G loss: 0.396953]\n",
      "epoch:29 step:27707 [D loss: 0.192464, acc.: 69.53%] [G loss: 0.436706]\n",
      "epoch:29 step:27708 [D loss: 0.219616, acc.: 60.94%] [G loss: 0.439914]\n",
      "epoch:29 step:27709 [D loss: 0.224939, acc.: 61.72%] [G loss: 0.379601]\n",
      "epoch:29 step:27710 [D loss: 0.245305, acc.: 54.69%] [G loss: 0.461978]\n",
      "epoch:29 step:27711 [D loss: 0.252266, acc.: 60.16%] [G loss: 0.440003]\n",
      "epoch:29 step:27712 [D loss: 0.194150, acc.: 70.31%] [G loss: 0.463386]\n",
      "epoch:29 step:27713 [D loss: 0.219802, acc.: 61.72%] [G loss: 0.445966]\n",
      "epoch:29 step:27714 [D loss: 0.233264, acc.: 57.03%] [G loss: 0.418839]\n",
      "epoch:29 step:27715 [D loss: 0.268344, acc.: 54.69%] [G loss: 0.406628]\n",
      "epoch:29 step:27716 [D loss: 0.247278, acc.: 60.94%] [G loss: 0.396535]\n",
      "epoch:29 step:27717 [D loss: 0.234808, acc.: 63.28%] [G loss: 0.394685]\n",
      "epoch:29 step:27718 [D loss: 0.217074, acc.: 65.62%] [G loss: 0.419780]\n",
      "epoch:29 step:27719 [D loss: 0.223220, acc.: 57.03%] [G loss: 0.433753]\n",
      "epoch:29 step:27720 [D loss: 0.222876, acc.: 61.72%] [G loss: 0.418106]\n",
      "epoch:29 step:27721 [D loss: 0.196034, acc.: 68.75%] [G loss: 0.446543]\n",
      "epoch:29 step:27722 [D loss: 0.204200, acc.: 67.19%] [G loss: 0.450420]\n",
      "epoch:29 step:27723 [D loss: 0.200049, acc.: 71.09%] [G loss: 0.413969]\n",
      "epoch:29 step:27724 [D loss: 0.224490, acc.: 61.72%] [G loss: 0.438266]\n",
      "epoch:29 step:27725 [D loss: 0.191465, acc.: 73.44%] [G loss: 0.474813]\n",
      "epoch:29 step:27726 [D loss: 0.233782, acc.: 60.94%] [G loss: 0.418210]\n",
      "epoch:29 step:27727 [D loss: 0.219718, acc.: 63.28%] [G loss: 0.435392]\n",
      "epoch:29 step:27728 [D loss: 0.222401, acc.: 62.50%] [G loss: 0.438682]\n",
      "epoch:29 step:27729 [D loss: 0.240527, acc.: 60.16%] [G loss: 0.452791]\n",
      "epoch:29 step:27730 [D loss: 0.204326, acc.: 66.41%] [G loss: 0.472790]\n",
      "epoch:29 step:27731 [D loss: 0.234634, acc.: 60.94%] [G loss: 0.400222]\n",
      "epoch:29 step:27732 [D loss: 0.252446, acc.: 53.12%] [G loss: 0.440948]\n",
      "epoch:29 step:27733 [D loss: 0.223422, acc.: 58.59%] [G loss: 0.423606]\n",
      "epoch:29 step:27734 [D loss: 0.232589, acc.: 62.50%] [G loss: 0.449107]\n",
      "epoch:29 step:27735 [D loss: 0.214519, acc.: 64.84%] [G loss: 0.418395]\n",
      "epoch:29 step:27736 [D loss: 0.206627, acc.: 72.66%] [G loss: 0.450443]\n",
      "epoch:29 step:27737 [D loss: 0.180412, acc.: 70.31%] [G loss: 0.481690]\n",
      "epoch:29 step:27738 [D loss: 0.236664, acc.: 57.81%] [G loss: 0.469340]\n",
      "epoch:29 step:27739 [D loss: 0.271032, acc.: 51.56%] [G loss: 0.430626]\n",
      "epoch:29 step:27740 [D loss: 0.204599, acc.: 64.84%] [G loss: 0.440672]\n",
      "epoch:29 step:27741 [D loss: 0.204405, acc.: 68.75%] [G loss: 0.461055]\n",
      "epoch:29 step:27742 [D loss: 0.265558, acc.: 51.56%] [G loss: 0.380911]\n",
      "epoch:29 step:27743 [D loss: 0.243652, acc.: 60.16%] [G loss: 0.380125]\n",
      "epoch:29 step:27744 [D loss: 0.215057, acc.: 62.50%] [G loss: 0.408437]\n",
      "epoch:29 step:27745 [D loss: 0.212221, acc.: 64.84%] [G loss: 0.446171]\n",
      "epoch:29 step:27746 [D loss: 0.222026, acc.: 67.19%] [G loss: 0.457715]\n",
      "epoch:29 step:27747 [D loss: 0.196122, acc.: 75.00%] [G loss: 0.435615]\n",
      "epoch:29 step:27748 [D loss: 0.207765, acc.: 66.41%] [G loss: 0.455740]\n",
      "epoch:29 step:27749 [D loss: 0.232099, acc.: 59.38%] [G loss: 0.459322]\n",
      "epoch:29 step:27750 [D loss: 0.243808, acc.: 61.72%] [G loss: 0.440665]\n",
      "epoch:29 step:27751 [D loss: 0.220834, acc.: 59.38%] [G loss: 0.435277]\n",
      "epoch:29 step:27752 [D loss: 0.229933, acc.: 59.38%] [G loss: 0.413156]\n",
      "epoch:29 step:27753 [D loss: 0.245805, acc.: 60.16%] [G loss: 0.405107]\n",
      "epoch:29 step:27754 [D loss: 0.245463, acc.: 58.59%] [G loss: 0.408160]\n",
      "epoch:29 step:27755 [D loss: 0.214542, acc.: 64.84%] [G loss: 0.458509]\n",
      "epoch:29 step:27756 [D loss: 0.253361, acc.: 53.12%] [G loss: 0.421331]\n",
      "epoch:29 step:27757 [D loss: 0.228682, acc.: 64.84%] [G loss: 0.437627]\n",
      "epoch:29 step:27758 [D loss: 0.251711, acc.: 52.34%] [G loss: 0.417427]\n",
      "epoch:29 step:27759 [D loss: 0.232369, acc.: 58.59%] [G loss: 0.436141]\n",
      "epoch:29 step:27760 [D loss: 0.228267, acc.: 58.59%] [G loss: 0.418089]\n",
      "epoch:29 step:27761 [D loss: 0.237086, acc.: 53.12%] [G loss: 0.427237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27762 [D loss: 0.200282, acc.: 71.09%] [G loss: 0.452099]\n",
      "epoch:29 step:27763 [D loss: 0.226123, acc.: 61.72%] [G loss: 0.442227]\n",
      "epoch:29 step:27764 [D loss: 0.247029, acc.: 53.12%] [G loss: 0.405875]\n",
      "epoch:29 step:27765 [D loss: 0.213278, acc.: 67.19%] [G loss: 0.435649]\n",
      "epoch:29 step:27766 [D loss: 0.204452, acc.: 65.62%] [G loss: 0.461890]\n",
      "epoch:29 step:27767 [D loss: 0.244517, acc.: 56.25%] [G loss: 0.426987]\n",
      "epoch:29 step:27768 [D loss: 0.215568, acc.: 68.75%] [G loss: 0.413683]\n",
      "epoch:29 step:27769 [D loss: 0.245555, acc.: 55.47%] [G loss: 0.411128]\n",
      "epoch:29 step:27770 [D loss: 0.238628, acc.: 60.16%] [G loss: 0.470301]\n",
      "epoch:29 step:27771 [D loss: 0.209529, acc.: 62.50%] [G loss: 0.472275]\n",
      "epoch:29 step:27772 [D loss: 0.222388, acc.: 60.94%] [G loss: 0.431730]\n",
      "epoch:29 step:27773 [D loss: 0.258214, acc.: 53.91%] [G loss: 0.405506]\n",
      "epoch:29 step:27774 [D loss: 0.231946, acc.: 61.72%] [G loss: 0.407675]\n",
      "epoch:29 step:27775 [D loss: 0.241783, acc.: 57.81%] [G loss: 0.388258]\n",
      "epoch:29 step:27776 [D loss: 0.213908, acc.: 66.41%] [G loss: 0.409667]\n",
      "epoch:29 step:27777 [D loss: 0.203634, acc.: 66.41%] [G loss: 0.424082]\n",
      "epoch:29 step:27778 [D loss: 0.213860, acc.: 64.84%] [G loss: 0.455233]\n",
      "epoch:29 step:27779 [D loss: 0.223939, acc.: 62.50%] [G loss: 0.437871]\n",
      "epoch:29 step:27780 [D loss: 0.209087, acc.: 64.06%] [G loss: 0.404019]\n",
      "epoch:29 step:27781 [D loss: 0.227251, acc.: 64.84%] [G loss: 0.431035]\n",
      "epoch:29 step:27782 [D loss: 0.229562, acc.: 57.81%] [G loss: 0.379246]\n",
      "epoch:29 step:27783 [D loss: 0.224482, acc.: 58.59%] [G loss: 0.446771]\n",
      "epoch:29 step:27784 [D loss: 0.214566, acc.: 64.06%] [G loss: 0.436981]\n",
      "epoch:29 step:27785 [D loss: 0.205764, acc.: 65.62%] [G loss: 0.440132]\n",
      "epoch:29 step:27786 [D loss: 0.217214, acc.: 68.75%] [G loss: 0.395602]\n",
      "epoch:29 step:27787 [D loss: 0.230132, acc.: 61.72%] [G loss: 0.435909]\n",
      "epoch:29 step:27788 [D loss: 0.249038, acc.: 55.47%] [G loss: 0.398638]\n",
      "epoch:29 step:27789 [D loss: 0.234658, acc.: 60.94%] [G loss: 0.380223]\n",
      "epoch:29 step:27790 [D loss: 0.217063, acc.: 64.84%] [G loss: 0.393916]\n",
      "epoch:29 step:27791 [D loss: 0.229124, acc.: 63.28%] [G loss: 0.431237]\n",
      "epoch:29 step:27792 [D loss: 0.237831, acc.: 62.50%] [G loss: 0.409184]\n",
      "epoch:29 step:27793 [D loss: 0.191200, acc.: 72.66%] [G loss: 0.495926]\n",
      "epoch:29 step:27794 [D loss: 0.245036, acc.: 57.03%] [G loss: 0.408801]\n",
      "epoch:29 step:27795 [D loss: 0.206389, acc.: 67.19%] [G loss: 0.461069]\n",
      "epoch:29 step:27796 [D loss: 0.224399, acc.: 63.28%] [G loss: 0.394438]\n",
      "epoch:29 step:27797 [D loss: 0.184628, acc.: 72.66%] [G loss: 0.425422]\n",
      "epoch:29 step:27798 [D loss: 0.218781, acc.: 63.28%] [G loss: 0.436113]\n",
      "epoch:29 step:27799 [D loss: 0.218612, acc.: 63.28%] [G loss: 0.436690]\n",
      "epoch:29 step:27800 [D loss: 0.215973, acc.: 66.41%] [G loss: 0.462481]\n",
      "##############\n",
      "[2.67723498 2.13238193 5.89739704 4.7567121  3.45955748 5.65251051\n",
      " 4.54838019 4.76264469 4.6097049  3.89057341]\n",
      "##########\n",
      "epoch:29 step:27801 [D loss: 0.235837, acc.: 62.50%] [G loss: 0.435931]\n",
      "epoch:29 step:27802 [D loss: 0.216981, acc.: 67.97%] [G loss: 0.423733]\n",
      "epoch:29 step:27803 [D loss: 0.248284, acc.: 57.03%] [G loss: 0.434392]\n",
      "epoch:29 step:27804 [D loss: 0.184728, acc.: 75.78%] [G loss: 0.440166]\n",
      "epoch:29 step:27805 [D loss: 0.211638, acc.: 66.41%] [G loss: 0.416153]\n",
      "epoch:29 step:27806 [D loss: 0.237919, acc.: 54.69%] [G loss: 0.425285]\n",
      "epoch:29 step:27807 [D loss: 0.187850, acc.: 74.22%] [G loss: 0.475879]\n",
      "epoch:29 step:27808 [D loss: 0.203935, acc.: 67.97%] [G loss: 0.462455]\n",
      "epoch:29 step:27809 [D loss: 0.243015, acc.: 57.03%] [G loss: 0.423539]\n",
      "epoch:29 step:27810 [D loss: 0.215657, acc.: 70.31%] [G loss: 0.460757]\n",
      "epoch:29 step:27811 [D loss: 0.215354, acc.: 63.28%] [G loss: 0.433767]\n",
      "epoch:29 step:27812 [D loss: 0.232140, acc.: 59.38%] [G loss: 0.420006]\n",
      "epoch:29 step:27813 [D loss: 0.227603, acc.: 61.72%] [G loss: 0.449505]\n",
      "epoch:29 step:27814 [D loss: 0.203228, acc.: 70.31%] [G loss: 0.475258]\n",
      "epoch:29 step:27815 [D loss: 0.196758, acc.: 69.53%] [G loss: 0.491199]\n",
      "epoch:29 step:27816 [D loss: 0.230429, acc.: 63.28%] [G loss: 0.438750]\n",
      "epoch:29 step:27817 [D loss: 0.237000, acc.: 64.06%] [G loss: 0.426339]\n",
      "epoch:29 step:27818 [D loss: 0.224939, acc.: 59.38%] [G loss: 0.377450]\n",
      "epoch:29 step:27819 [D loss: 0.205575, acc.: 65.62%] [G loss: 0.410496]\n",
      "epoch:29 step:27820 [D loss: 0.197991, acc.: 71.09%] [G loss: 0.422976]\n",
      "epoch:29 step:27821 [D loss: 0.200945, acc.: 70.31%] [G loss: 0.462559]\n",
      "epoch:29 step:27822 [D loss: 0.207951, acc.: 65.62%] [G loss: 0.490099]\n",
      "epoch:29 step:27823 [D loss: 0.177006, acc.: 77.34%] [G loss: 0.501808]\n",
      "epoch:29 step:27824 [D loss: 0.219147, acc.: 61.72%] [G loss: 0.422017]\n",
      "epoch:29 step:27825 [D loss: 0.260573, acc.: 60.16%] [G loss: 0.415856]\n",
      "epoch:29 step:27826 [D loss: 0.209627, acc.: 58.59%] [G loss: 0.470394]\n",
      "epoch:29 step:27827 [D loss: 0.209286, acc.: 66.41%] [G loss: 0.455552]\n",
      "epoch:29 step:27828 [D loss: 0.227240, acc.: 62.50%] [G loss: 0.447316]\n",
      "epoch:29 step:27829 [D loss: 0.209423, acc.: 64.06%] [G loss: 0.469666]\n",
      "epoch:29 step:27830 [D loss: 0.221596, acc.: 64.06%] [G loss: 0.392662]\n",
      "epoch:29 step:27831 [D loss: 0.229083, acc.: 59.38%] [G loss: 0.411482]\n",
      "epoch:29 step:27832 [D loss: 0.207832, acc.: 62.50%] [G loss: 0.455170]\n",
      "epoch:29 step:27833 [D loss: 0.226360, acc.: 58.59%] [G loss: 0.436936]\n",
      "epoch:29 step:27834 [D loss: 0.213964, acc.: 68.75%] [G loss: 0.423427]\n",
      "epoch:29 step:27835 [D loss: 0.237635, acc.: 57.81%] [G loss: 0.439810]\n",
      "epoch:29 step:27836 [D loss: 0.249123, acc.: 52.34%] [G loss: 0.397942]\n",
      "epoch:29 step:27837 [D loss: 0.233782, acc.: 61.72%] [G loss: 0.419052]\n",
      "epoch:29 step:27838 [D loss: 0.206350, acc.: 66.41%] [G loss: 0.432452]\n",
      "epoch:29 step:27839 [D loss: 0.194911, acc.: 68.75%] [G loss: 0.493915]\n",
      "epoch:29 step:27840 [D loss: 0.229955, acc.: 56.25%] [G loss: 0.452477]\n",
      "epoch:29 step:27841 [D loss: 0.229047, acc.: 64.84%] [G loss: 0.427650]\n",
      "epoch:29 step:27842 [D loss: 0.217164, acc.: 65.62%] [G loss: 0.402921]\n",
      "epoch:29 step:27843 [D loss: 0.229154, acc.: 62.50%] [G loss: 0.412406]\n",
      "epoch:29 step:27844 [D loss: 0.225546, acc.: 62.50%] [G loss: 0.400132]\n",
      "epoch:29 step:27845 [D loss: 0.213125, acc.: 65.62%] [G loss: 0.454452]\n",
      "epoch:29 step:27846 [D loss: 0.238138, acc.: 57.03%] [G loss: 0.428219]\n",
      "epoch:29 step:27847 [D loss: 0.198201, acc.: 69.53%] [G loss: 0.488741]\n",
      "epoch:29 step:27848 [D loss: 0.245806, acc.: 59.38%] [G loss: 0.431834]\n",
      "epoch:29 step:27849 [D loss: 0.224276, acc.: 62.50%] [G loss: 0.406386]\n",
      "epoch:29 step:27850 [D loss: 0.224023, acc.: 58.59%] [G loss: 0.412024]\n",
      "epoch:29 step:27851 [D loss: 0.238115, acc.: 64.06%] [G loss: 0.413065]\n",
      "epoch:29 step:27852 [D loss: 0.200543, acc.: 72.66%] [G loss: 0.443686]\n",
      "epoch:29 step:27853 [D loss: 0.211716, acc.: 62.50%] [G loss: 0.445634]\n",
      "epoch:29 step:27854 [D loss: 0.216490, acc.: 59.38%] [G loss: 0.419508]\n",
      "epoch:29 step:27855 [D loss: 0.231209, acc.: 57.03%] [G loss: 0.416507]\n",
      "epoch:29 step:27856 [D loss: 0.214619, acc.: 62.50%] [G loss: 0.460632]\n",
      "epoch:29 step:27857 [D loss: 0.241296, acc.: 57.03%] [G loss: 0.439359]\n",
      "epoch:29 step:27858 [D loss: 0.214075, acc.: 62.50%] [G loss: 0.399743]\n",
      "epoch:29 step:27859 [D loss: 0.209463, acc.: 67.97%] [G loss: 0.390749]\n",
      "epoch:29 step:27860 [D loss: 0.225612, acc.: 62.50%] [G loss: 0.446024]\n",
      "epoch:29 step:27861 [D loss: 0.204741, acc.: 66.41%] [G loss: 0.445194]\n",
      "epoch:29 step:27862 [D loss: 0.198920, acc.: 70.31%] [G loss: 0.439857]\n",
      "epoch:29 step:27863 [D loss: 0.196583, acc.: 67.97%] [G loss: 0.430198]\n",
      "epoch:29 step:27864 [D loss: 0.181256, acc.: 78.91%] [G loss: 0.477693]\n",
      "epoch:29 step:27865 [D loss: 0.219256, acc.: 67.97%] [G loss: 0.476683]\n",
      "epoch:29 step:27866 [D loss: 0.198358, acc.: 72.66%] [G loss: 0.467417]\n",
      "epoch:29 step:27867 [D loss: 0.202893, acc.: 68.75%] [G loss: 0.443897]\n",
      "epoch:29 step:27868 [D loss: 0.192009, acc.: 70.31%] [G loss: 0.455621]\n",
      "epoch:29 step:27869 [D loss: 0.258485, acc.: 52.34%] [G loss: 0.410302]\n",
      "epoch:29 step:27870 [D loss: 0.235693, acc.: 60.16%] [G loss: 0.412967]\n",
      "epoch:29 step:27871 [D loss: 0.232140, acc.: 59.38%] [G loss: 0.419170]\n",
      "epoch:29 step:27872 [D loss: 0.213198, acc.: 66.41%] [G loss: 0.416539]\n",
      "epoch:29 step:27873 [D loss: 0.198199, acc.: 65.62%] [G loss: 0.441435]\n",
      "epoch:29 step:27874 [D loss: 0.225428, acc.: 56.25%] [G loss: 0.503484]\n",
      "epoch:29 step:27875 [D loss: 0.225025, acc.: 64.84%] [G loss: 0.507942]\n",
      "epoch:29 step:27876 [D loss: 0.269797, acc.: 48.44%] [G loss: 0.441520]\n",
      "epoch:29 step:27877 [D loss: 0.238490, acc.: 59.38%] [G loss: 0.439395]\n",
      "epoch:29 step:27878 [D loss: 0.232610, acc.: 61.72%] [G loss: 0.407554]\n",
      "epoch:29 step:27879 [D loss: 0.222910, acc.: 60.16%] [G loss: 0.470074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27880 [D loss: 0.213436, acc.: 67.97%] [G loss: 0.457862]\n",
      "epoch:29 step:27881 [D loss: 0.206147, acc.: 68.75%] [G loss: 0.411217]\n",
      "epoch:29 step:27882 [D loss: 0.205468, acc.: 67.19%] [G loss: 0.453180]\n",
      "epoch:29 step:27883 [D loss: 0.236371, acc.: 57.03%] [G loss: 0.425575]\n",
      "epoch:29 step:27884 [D loss: 0.233211, acc.: 58.59%] [G loss: 0.410277]\n",
      "epoch:29 step:27885 [D loss: 0.227259, acc.: 58.59%] [G loss: 0.440865]\n",
      "epoch:29 step:27886 [D loss: 0.232372, acc.: 58.59%] [G loss: 0.420118]\n",
      "epoch:29 step:27887 [D loss: 0.226290, acc.: 64.84%] [G loss: 0.429865]\n",
      "epoch:29 step:27888 [D loss: 0.209102, acc.: 65.62%] [G loss: 0.396773]\n",
      "epoch:29 step:27889 [D loss: 0.232374, acc.: 57.03%] [G loss: 0.415032]\n",
      "epoch:29 step:27890 [D loss: 0.216475, acc.: 67.19%] [G loss: 0.408778]\n",
      "epoch:29 step:27891 [D loss: 0.237293, acc.: 57.81%] [G loss: 0.432451]\n",
      "epoch:29 step:27892 [D loss: 0.193101, acc.: 70.31%] [G loss: 0.452507]\n",
      "epoch:29 step:27893 [D loss: 0.223751, acc.: 60.16%] [G loss: 0.471945]\n",
      "epoch:29 step:27894 [D loss: 0.229771, acc.: 64.06%] [G loss: 0.471949]\n",
      "epoch:29 step:27895 [D loss: 0.247971, acc.: 57.81%] [G loss: 0.421271]\n",
      "epoch:29 step:27896 [D loss: 0.225196, acc.: 61.72%] [G loss: 0.441629]\n",
      "epoch:29 step:27897 [D loss: 0.212868, acc.: 64.84%] [G loss: 0.420908]\n",
      "epoch:29 step:27898 [D loss: 0.215465, acc.: 62.50%] [G loss: 0.398504]\n",
      "epoch:29 step:27899 [D loss: 0.218297, acc.: 64.06%] [G loss: 0.413337]\n",
      "epoch:29 step:27900 [D loss: 0.225080, acc.: 63.28%] [G loss: 0.445639]\n",
      "epoch:29 step:27901 [D loss: 0.223058, acc.: 62.50%] [G loss: 0.436149]\n",
      "epoch:29 step:27902 [D loss: 0.221879, acc.: 57.81%] [G loss: 0.422579]\n",
      "epoch:29 step:27903 [D loss: 0.217355, acc.: 60.94%] [G loss: 0.447294]\n",
      "epoch:29 step:27904 [D loss: 0.236974, acc.: 57.81%] [G loss: 0.450430]\n",
      "epoch:29 step:27905 [D loss: 0.219232, acc.: 60.94%] [G loss: 0.432996]\n",
      "epoch:29 step:27906 [D loss: 0.187611, acc.: 71.88%] [G loss: 0.455035]\n",
      "epoch:29 step:27907 [D loss: 0.260344, acc.: 54.69%] [G loss: 0.419789]\n",
      "epoch:29 step:27908 [D loss: 0.249104, acc.: 56.25%] [G loss: 0.418929]\n",
      "epoch:29 step:27909 [D loss: 0.209749, acc.: 65.62%] [G loss: 0.444336]\n",
      "epoch:29 step:27910 [D loss: 0.231313, acc.: 60.16%] [G loss: 0.400169]\n",
      "epoch:29 step:27911 [D loss: 0.225230, acc.: 65.62%] [G loss: 0.444425]\n",
      "epoch:29 step:27912 [D loss: 0.244081, acc.: 59.38%] [G loss: 0.395265]\n",
      "epoch:29 step:27913 [D loss: 0.226512, acc.: 63.28%] [G loss: 0.401146]\n",
      "epoch:29 step:27914 [D loss: 0.238626, acc.: 63.28%] [G loss: 0.387397]\n",
      "epoch:29 step:27915 [D loss: 0.237929, acc.: 55.47%] [G loss: 0.404549]\n",
      "epoch:29 step:27916 [D loss: 0.207699, acc.: 65.62%] [G loss: 0.403566]\n",
      "epoch:29 step:27917 [D loss: 0.211193, acc.: 64.84%] [G loss: 0.446528]\n",
      "epoch:29 step:27918 [D loss: 0.241343, acc.: 55.47%] [G loss: 0.423755]\n",
      "epoch:29 step:27919 [D loss: 0.222737, acc.: 64.06%] [G loss: 0.426085]\n",
      "epoch:29 step:27920 [D loss: 0.234086, acc.: 59.38%] [G loss: 0.441561]\n",
      "epoch:29 step:27921 [D loss: 0.226524, acc.: 58.59%] [G loss: 0.411999]\n",
      "epoch:29 step:27922 [D loss: 0.207560, acc.: 64.84%] [G loss: 0.448101]\n",
      "epoch:29 step:27923 [D loss: 0.209590, acc.: 64.06%] [G loss: 0.395347]\n",
      "epoch:29 step:27924 [D loss: 0.200360, acc.: 67.19%] [G loss: 0.440114]\n",
      "epoch:29 step:27925 [D loss: 0.249135, acc.: 53.91%] [G loss: 0.405683]\n",
      "epoch:29 step:27926 [D loss: 0.230924, acc.: 64.06%] [G loss: 0.439803]\n",
      "epoch:29 step:27927 [D loss: 0.228622, acc.: 62.50%] [G loss: 0.444890]\n",
      "epoch:29 step:27928 [D loss: 0.210363, acc.: 68.75%] [G loss: 0.467390]\n",
      "epoch:29 step:27929 [D loss: 0.234624, acc.: 58.59%] [G loss: 0.455453]\n",
      "epoch:29 step:27930 [D loss: 0.219307, acc.: 64.06%] [G loss: 0.445767]\n",
      "epoch:29 step:27931 [D loss: 0.235249, acc.: 59.38%] [G loss: 0.437131]\n",
      "epoch:29 step:27932 [D loss: 0.234638, acc.: 62.50%] [G loss: 0.432470]\n",
      "epoch:29 step:27933 [D loss: 0.236217, acc.: 55.47%] [G loss: 0.395213]\n",
      "epoch:29 step:27934 [D loss: 0.235482, acc.: 62.50%] [G loss: 0.363991]\n",
      "epoch:29 step:27935 [D loss: 0.233629, acc.: 62.50%] [G loss: 0.422879]\n",
      "epoch:29 step:27936 [D loss: 0.216516, acc.: 63.28%] [G loss: 0.424217]\n",
      "epoch:29 step:27937 [D loss: 0.240325, acc.: 55.47%] [G loss: 0.442787]\n",
      "epoch:29 step:27938 [D loss: 0.232897, acc.: 60.94%] [G loss: 0.407878]\n",
      "epoch:29 step:27939 [D loss: 0.221293, acc.: 65.62%] [G loss: 0.402865]\n",
      "epoch:29 step:27940 [D loss: 0.184578, acc.: 73.44%] [G loss: 0.440844]\n",
      "epoch:29 step:27941 [D loss: 0.221417, acc.: 67.97%] [G loss: 0.402711]\n",
      "epoch:29 step:27942 [D loss: 0.211793, acc.: 66.41%] [G loss: 0.462645]\n",
      "epoch:29 step:27943 [D loss: 0.218953, acc.: 64.84%] [G loss: 0.436090]\n",
      "epoch:29 step:27944 [D loss: 0.203657, acc.: 69.53%] [G loss: 0.454841]\n",
      "epoch:29 step:27945 [D loss: 0.239365, acc.: 58.59%] [G loss: 0.429889]\n",
      "epoch:29 step:27946 [D loss: 0.230312, acc.: 65.62%] [G loss: 0.438551]\n",
      "epoch:29 step:27947 [D loss: 0.205768, acc.: 72.66%] [G loss: 0.440235]\n",
      "epoch:29 step:27948 [D loss: 0.223724, acc.: 68.75%] [G loss: 0.450283]\n",
      "epoch:29 step:27949 [D loss: 0.229112, acc.: 58.59%] [G loss: 0.429995]\n",
      "epoch:29 step:27950 [D loss: 0.216328, acc.: 64.06%] [G loss: 0.444639]\n",
      "epoch:29 step:27951 [D loss: 0.232029, acc.: 63.28%] [G loss: 0.435313]\n",
      "epoch:29 step:27952 [D loss: 0.225383, acc.: 67.19%] [G loss: 0.442816]\n",
      "epoch:29 step:27953 [D loss: 0.207818, acc.: 66.41%] [G loss: 0.445411]\n",
      "epoch:29 step:27954 [D loss: 0.221354, acc.: 64.84%] [G loss: 0.480544]\n",
      "epoch:29 step:27955 [D loss: 0.179083, acc.: 75.00%] [G loss: 0.463277]\n",
      "epoch:29 step:27956 [D loss: 0.237987, acc.: 56.25%] [G loss: 0.472182]\n",
      "epoch:29 step:27957 [D loss: 0.255332, acc.: 53.12%] [G loss: 0.429787]\n",
      "epoch:29 step:27958 [D loss: 0.222703, acc.: 66.41%] [G loss: 0.418489]\n",
      "epoch:29 step:27959 [D loss: 0.206865, acc.: 66.41%] [G loss: 0.412469]\n",
      "epoch:29 step:27960 [D loss: 0.251616, acc.: 50.00%] [G loss: 0.419578]\n",
      "epoch:29 step:27961 [D loss: 0.225249, acc.: 63.28%] [G loss: 0.404455]\n",
      "epoch:29 step:27962 [D loss: 0.222585, acc.: 62.50%] [G loss: 0.399046]\n",
      "epoch:29 step:27963 [D loss: 0.223362, acc.: 67.97%] [G loss: 0.445357]\n",
      "epoch:29 step:27964 [D loss: 0.260666, acc.: 55.47%] [G loss: 0.412823]\n",
      "epoch:29 step:27965 [D loss: 0.195445, acc.: 71.88%] [G loss: 0.448206]\n",
      "epoch:29 step:27966 [D loss: 0.210322, acc.: 60.94%] [G loss: 0.473654]\n",
      "epoch:29 step:27967 [D loss: 0.246471, acc.: 53.12%] [G loss: 0.475521]\n",
      "epoch:29 step:27968 [D loss: 0.252626, acc.: 57.03%] [G loss: 0.430090]\n",
      "epoch:29 step:27969 [D loss: 0.212232, acc.: 67.19%] [G loss: 0.428585]\n",
      "epoch:29 step:27970 [D loss: 0.256676, acc.: 47.66%] [G loss: 0.420648]\n",
      "epoch:29 step:27971 [D loss: 0.221582, acc.: 63.28%] [G loss: 0.392542]\n",
      "epoch:29 step:27972 [D loss: 0.228444, acc.: 60.94%] [G loss: 0.383003]\n",
      "epoch:29 step:27973 [D loss: 0.271584, acc.: 48.44%] [G loss: 0.405278]\n",
      "epoch:29 step:27974 [D loss: 0.234398, acc.: 60.94%] [G loss: 0.415092]\n",
      "epoch:29 step:27975 [D loss: 0.206815, acc.: 65.62%] [G loss: 0.396301]\n",
      "epoch:29 step:27976 [D loss: 0.216858, acc.: 68.75%] [G loss: 0.467039]\n",
      "epoch:29 step:27977 [D loss: 0.255370, acc.: 50.78%] [G loss: 0.436725]\n",
      "epoch:29 step:27978 [D loss: 0.245979, acc.: 58.59%] [G loss: 0.402892]\n",
      "epoch:29 step:27979 [D loss: 0.220429, acc.: 61.72%] [G loss: 0.389014]\n",
      "epoch:29 step:27980 [D loss: 0.209071, acc.: 64.84%] [G loss: 0.431314]\n",
      "epoch:29 step:27981 [D loss: 0.244292, acc.: 53.91%] [G loss: 0.395735]\n",
      "epoch:29 step:27982 [D loss: 0.258494, acc.: 55.47%] [G loss: 0.424460]\n",
      "epoch:29 step:27983 [D loss: 0.230125, acc.: 60.94%] [G loss: 0.431485]\n",
      "epoch:29 step:27984 [D loss: 0.210874, acc.: 65.62%] [G loss: 0.398193]\n",
      "epoch:29 step:27985 [D loss: 0.253029, acc.: 53.12%] [G loss: 0.403227]\n",
      "epoch:29 step:27986 [D loss: 0.217449, acc.: 65.62%] [G loss: 0.438073]\n",
      "epoch:29 step:27987 [D loss: 0.208258, acc.: 67.19%] [G loss: 0.426029]\n",
      "epoch:29 step:27988 [D loss: 0.209181, acc.: 62.50%] [G loss: 0.404591]\n",
      "epoch:29 step:27989 [D loss: 0.220764, acc.: 66.41%] [G loss: 0.429171]\n",
      "epoch:29 step:27990 [D loss: 0.251219, acc.: 53.12%] [G loss: 0.431527]\n",
      "epoch:29 step:27991 [D loss: 0.220423, acc.: 67.19%] [G loss: 0.433835]\n",
      "epoch:29 step:27992 [D loss: 0.216447, acc.: 64.06%] [G loss: 0.438575]\n",
      "epoch:29 step:27993 [D loss: 0.263950, acc.: 55.47%] [G loss: 0.374166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27994 [D loss: 0.245679, acc.: 58.59%] [G loss: 0.394205]\n",
      "epoch:29 step:27995 [D loss: 0.221985, acc.: 62.50%] [G loss: 0.437782]\n",
      "epoch:29 step:27996 [D loss: 0.198274, acc.: 73.44%] [G loss: 0.429000]\n",
      "epoch:29 step:27997 [D loss: 0.225694, acc.: 62.50%] [G loss: 0.403978]\n",
      "epoch:29 step:27998 [D loss: 0.219771, acc.: 64.84%] [G loss: 0.398571]\n",
      "epoch:29 step:27999 [D loss: 0.225406, acc.: 65.62%] [G loss: 0.401325]\n",
      "epoch:29 step:28000 [D loss: 0.238862, acc.: 56.25%] [G loss: 0.418704]\n",
      "##############\n",
      "[2.65194832 1.92995078 6.12647604 4.81225193 3.60127842 5.4784765\n",
      " 4.50242794 5.13158729 4.46582561 3.95251836]\n",
      "##########\n",
      "epoch:29 step:28001 [D loss: 0.230660, acc.: 62.50%] [G loss: 0.477472]\n",
      "epoch:29 step:28002 [D loss: 0.235174, acc.: 59.38%] [G loss: 0.429572]\n",
      "epoch:29 step:28003 [D loss: 0.221488, acc.: 58.59%] [G loss: 0.413282]\n",
      "epoch:29 step:28004 [D loss: 0.240506, acc.: 59.38%] [G loss: 0.383790]\n",
      "epoch:29 step:28005 [D loss: 0.204363, acc.: 67.97%] [G loss: 0.409042]\n",
      "epoch:29 step:28006 [D loss: 0.194597, acc.: 66.41%] [G loss: 0.419103]\n",
      "epoch:29 step:28007 [D loss: 0.242033, acc.: 60.16%] [G loss: 0.412308]\n",
      "epoch:29 step:28008 [D loss: 0.242494, acc.: 60.94%] [G loss: 0.423571]\n",
      "epoch:29 step:28009 [D loss: 0.211692, acc.: 67.97%] [G loss: 0.443103]\n",
      "epoch:29 step:28010 [D loss: 0.203795, acc.: 66.41%] [G loss: 0.450969]\n",
      "epoch:29 step:28011 [D loss: 0.217340, acc.: 68.75%] [G loss: 0.416148]\n",
      "epoch:29 step:28012 [D loss: 0.209906, acc.: 68.75%] [G loss: 0.405173]\n",
      "epoch:29 step:28013 [D loss: 0.231475, acc.: 64.84%] [G loss: 0.426919]\n",
      "epoch:29 step:28014 [D loss: 0.200290, acc.: 66.41%] [G loss: 0.438694]\n",
      "epoch:29 step:28015 [D loss: 0.187371, acc.: 74.22%] [G loss: 0.432580]\n",
      "epoch:29 step:28016 [D loss: 0.248566, acc.: 57.03%] [G loss: 0.413521]\n",
      "epoch:29 step:28017 [D loss: 0.220918, acc.: 65.62%] [G loss: 0.437490]\n",
      "epoch:29 step:28018 [D loss: 0.214354, acc.: 60.16%] [G loss: 0.427431]\n",
      "epoch:29 step:28019 [D loss: 0.239757, acc.: 60.16%] [G loss: 0.424878]\n",
      "epoch:29 step:28020 [D loss: 0.209202, acc.: 60.94%] [G loss: 0.434479]\n",
      "epoch:29 step:28021 [D loss: 0.250897, acc.: 59.38%] [G loss: 0.385801]\n",
      "epoch:29 step:28022 [D loss: 0.213974, acc.: 68.75%] [G loss: 0.392377]\n",
      "epoch:29 step:28023 [D loss: 0.227603, acc.: 62.50%] [G loss: 0.402971]\n",
      "epoch:29 step:28024 [D loss: 0.226122, acc.: 62.50%] [G loss: 0.436168]\n",
      "epoch:29 step:28025 [D loss: 0.210422, acc.: 68.75%] [G loss: 0.402034]\n",
      "epoch:29 step:28026 [D loss: 0.219115, acc.: 67.19%] [G loss: 0.454988]\n",
      "epoch:29 step:28027 [D loss: 0.208278, acc.: 65.62%] [G loss: 0.460180]\n",
      "epoch:29 step:28028 [D loss: 0.243893, acc.: 60.94%] [G loss: 0.424188]\n",
      "epoch:29 step:28029 [D loss: 0.240522, acc.: 55.47%] [G loss: 0.401109]\n",
      "epoch:29 step:28030 [D loss: 0.205572, acc.: 71.09%] [G loss: 0.432017]\n",
      "epoch:29 step:28031 [D loss: 0.292337, acc.: 43.75%] [G loss: 0.355996]\n",
      "epoch:29 step:28032 [D loss: 0.236648, acc.: 59.38%] [G loss: 0.401976]\n",
      "epoch:29 step:28033 [D loss: 0.211520, acc.: 64.84%] [G loss: 0.482329]\n",
      "epoch:29 step:28034 [D loss: 0.254339, acc.: 55.47%] [G loss: 0.399844]\n",
      "epoch:29 step:28035 [D loss: 0.250118, acc.: 54.69%] [G loss: 0.390842]\n",
      "epoch:29 step:28036 [D loss: 0.200018, acc.: 70.31%] [G loss: 0.398670]\n",
      "epoch:29 step:28037 [D loss: 0.223410, acc.: 61.72%] [G loss: 0.396873]\n",
      "epoch:29 step:28038 [D loss: 0.240788, acc.: 58.59%] [G loss: 0.393366]\n",
      "epoch:29 step:28039 [D loss: 0.241731, acc.: 56.25%] [G loss: 0.375398]\n",
      "epoch:29 step:28040 [D loss: 0.250487, acc.: 60.16%] [G loss: 0.396208]\n",
      "epoch:29 step:28041 [D loss: 0.224239, acc.: 60.16%] [G loss: 0.391767]\n",
      "epoch:29 step:28042 [D loss: 0.247081, acc.: 57.81%] [G loss: 0.407879]\n",
      "epoch:29 step:28043 [D loss: 0.229511, acc.: 58.59%] [G loss: 0.418277]\n",
      "epoch:29 step:28044 [D loss: 0.198887, acc.: 67.97%] [G loss: 0.440870]\n",
      "epoch:29 step:28045 [D loss: 0.254603, acc.: 57.03%] [G loss: 0.418060]\n",
      "epoch:29 step:28046 [D loss: 0.224279, acc.: 65.62%] [G loss: 0.433349]\n",
      "epoch:29 step:28047 [D loss: 0.208904, acc.: 66.41%] [G loss: 0.429686]\n",
      "epoch:29 step:28048 [D loss: 0.192487, acc.: 73.44%] [G loss: 0.413663]\n",
      "epoch:29 step:28049 [D loss: 0.233492, acc.: 62.50%] [G loss: 0.374614]\n",
      "epoch:29 step:28050 [D loss: 0.238548, acc.: 58.59%] [G loss: 0.430343]\n",
      "epoch:29 step:28051 [D loss: 0.222533, acc.: 64.06%] [G loss: 0.401215]\n",
      "epoch:29 step:28052 [D loss: 0.238188, acc.: 59.38%] [G loss: 0.367182]\n",
      "epoch:29 step:28053 [D loss: 0.244689, acc.: 60.94%] [G loss: 0.410178]\n",
      "epoch:29 step:28054 [D loss: 0.220711, acc.: 65.62%] [G loss: 0.443494]\n",
      "epoch:29 step:28055 [D loss: 0.249025, acc.: 57.81%] [G loss: 0.449121]\n",
      "epoch:29 step:28056 [D loss: 0.228632, acc.: 66.41%] [G loss: 0.386510]\n",
      "epoch:29 step:28057 [D loss: 0.224004, acc.: 65.62%] [G loss: 0.433241]\n",
      "epoch:29 step:28058 [D loss: 0.197962, acc.: 71.09%] [G loss: 0.469926]\n",
      "epoch:29 step:28059 [D loss: 0.202516, acc.: 66.41%] [G loss: 0.509173]\n",
      "epoch:29 step:28060 [D loss: 0.232133, acc.: 61.72%] [G loss: 0.457613]\n",
      "epoch:29 step:28061 [D loss: 0.246480, acc.: 60.16%] [G loss: 0.416838]\n",
      "epoch:29 step:28062 [D loss: 0.244574, acc.: 62.50%] [G loss: 0.440183]\n",
      "epoch:29 step:28063 [D loss: 0.190846, acc.: 71.88%] [G loss: 0.476045]\n",
      "epoch:29 step:28064 [D loss: 0.291783, acc.: 48.44%] [G loss: 0.388523]\n",
      "epoch:29 step:28065 [D loss: 0.234810, acc.: 62.50%] [G loss: 0.442399]\n",
      "epoch:29 step:28066 [D loss: 0.208430, acc.: 63.28%] [G loss: 0.409972]\n",
      "epoch:29 step:28067 [D loss: 0.191816, acc.: 71.88%] [G loss: 0.451511]\n",
      "epoch:29 step:28068 [D loss: 0.206268, acc.: 65.62%] [G loss: 0.447399]\n",
      "epoch:29 step:28069 [D loss: 0.200107, acc.: 69.53%] [G loss: 0.391926]\n",
      "epoch:29 step:28070 [D loss: 0.211322, acc.: 69.53%] [G loss: 0.480672]\n",
      "epoch:29 step:28071 [D loss: 0.213024, acc.: 67.97%] [G loss: 0.432881]\n",
      "epoch:29 step:28072 [D loss: 0.182863, acc.: 73.44%] [G loss: 0.468808]\n",
      "epoch:29 step:28073 [D loss: 0.221958, acc.: 64.84%] [G loss: 0.472383]\n",
      "epoch:29 step:28074 [D loss: 0.219388, acc.: 64.84%] [G loss: 0.471355]\n",
      "epoch:29 step:28075 [D loss: 0.228072, acc.: 62.50%] [G loss: 0.440169]\n",
      "epoch:29 step:28076 [D loss: 0.235934, acc.: 65.62%] [G loss: 0.435335]\n",
      "epoch:29 step:28077 [D loss: 0.231992, acc.: 60.94%] [G loss: 0.427978]\n",
      "epoch:29 step:28078 [D loss: 0.199249, acc.: 69.53%] [G loss: 0.443047]\n",
      "epoch:29 step:28079 [D loss: 0.227420, acc.: 57.03%] [G loss: 0.448619]\n",
      "epoch:29 step:28080 [D loss: 0.222966, acc.: 63.28%] [G loss: 0.426800]\n",
      "epoch:29 step:28081 [D loss: 0.209320, acc.: 66.41%] [G loss: 0.421154]\n",
      "epoch:29 step:28082 [D loss: 0.192442, acc.: 70.31%] [G loss: 0.415038]\n",
      "epoch:29 step:28083 [D loss: 0.218583, acc.: 63.28%] [G loss: 0.472951]\n",
      "epoch:29 step:28084 [D loss: 0.222775, acc.: 61.72%] [G loss: 0.446067]\n",
      "epoch:29 step:28085 [D loss: 0.201546, acc.: 74.22%] [G loss: 0.475778]\n",
      "epoch:29 step:28086 [D loss: 0.225755, acc.: 64.84%] [G loss: 0.442690]\n",
      "epoch:29 step:28087 [D loss: 0.221852, acc.: 60.94%] [G loss: 0.443412]\n",
      "epoch:29 step:28088 [D loss: 0.298396, acc.: 53.12%] [G loss: 0.387440]\n",
      "epoch:29 step:28089 [D loss: 0.226380, acc.: 59.38%] [G loss: 0.395162]\n",
      "epoch:29 step:28090 [D loss: 0.226054, acc.: 64.06%] [G loss: 0.432379]\n",
      "epoch:29 step:28091 [D loss: 0.203608, acc.: 68.75%] [G loss: 0.441449]\n",
      "epoch:29 step:28092 [D loss: 0.205968, acc.: 67.97%] [G loss: 0.487980]\n",
      "epoch:29 step:28093 [D loss: 0.286093, acc.: 45.31%] [G loss: 0.449293]\n",
      "epoch:29 step:28094 [D loss: 0.228485, acc.: 65.62%] [G loss: 0.426205]\n",
      "epoch:29 step:28095 [D loss: 0.220285, acc.: 64.06%] [G loss: 0.421233]\n",
      "epoch:29 step:28096 [D loss: 0.212620, acc.: 69.53%] [G loss: 0.433784]\n",
      "epoch:29 step:28097 [D loss: 0.174514, acc.: 75.00%] [G loss: 0.491604]\n",
      "epoch:29 step:28098 [D loss: 0.194306, acc.: 71.88%] [G loss: 0.479586]\n",
      "epoch:29 step:28099 [D loss: 0.197697, acc.: 74.22%] [G loss: 0.505917]\n",
      "epoch:29 step:28100 [D loss: 0.191579, acc.: 72.66%] [G loss: 0.497163]\n",
      "epoch:29 step:28101 [D loss: 0.287434, acc.: 54.69%] [G loss: 0.503615]\n",
      "epoch:29 step:28102 [D loss: 0.218766, acc.: 65.62%] [G loss: 0.541081]\n",
      "epoch:29 step:28103 [D loss: 0.186381, acc.: 72.66%] [G loss: 0.580255]\n",
      "epoch:29 step:28104 [D loss: 0.245179, acc.: 60.94%] [G loss: 0.441056]\n",
      "epoch:29 step:28105 [D loss: 0.283610, acc.: 57.03%] [G loss: 0.395958]\n",
      "epoch:29 step:28106 [D loss: 0.222042, acc.: 65.62%] [G loss: 0.436494]\n",
      "epoch:29 step:28107 [D loss: 0.212368, acc.: 65.62%] [G loss: 0.416261]\n",
      "epoch:29 step:28108 [D loss: 0.182053, acc.: 74.22%] [G loss: 0.468431]\n",
      "epoch:29 step:28109 [D loss: 0.196918, acc.: 71.88%] [G loss: 0.505468]\n",
      "epoch:29 step:28110 [D loss: 0.164839, acc.: 74.22%] [G loss: 0.585457]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isdir('saved_models_mnist_{}'.format('lsgan')):\n",
    "    os.mkdir('saved_models_mnist_{}'.format('lsgan'))\n",
    "f = open('saved_models_mnist_{}/log_collapse1.txt'.format('lsgan'), mode='w')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LSGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        # (!!!) Optimize w.r.t. MSE loss instead of crossentropy\n",
    "        self.combined.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # (!!!) No softmax\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        # Rescale -1 to 1\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, global_step,d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.sample_images(global_step, X_test,y_test, sampleSize)\n",
    "    def sample_images(self, epoch,x_test,y_test,sample_num):\n",
    "        r, c = sample_num//10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = LSGAN()\n",
    "    gan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
