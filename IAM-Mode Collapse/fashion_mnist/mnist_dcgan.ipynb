{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import util\n",
    "import tensorflow as tf\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "def EuclideanDistances(A, B):\n",
    "\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(),ED.shape[0]*ED.shape[1])\n",
    "def cal_distance_image_real(images,labels):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "def cal_distance_image_fake(images):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    labels = tf.Session().run(tf.argmax(y_logits, 1))\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D loss: 1.145454, acc.: 28.12%] [G loss: 0.970436]\n",
      "epoch:0 step:2 [D loss: 0.385451, acc.: 85.16%] [G loss: 1.744552]\n",
      "epoch:0 step:3 [D loss: 0.256372, acc.: 96.09%] [G loss: 1.961175]\n",
      "epoch:0 step:4 [D loss: 0.165176, acc.: 99.22%] [G loss: 1.796844]\n",
      "epoch:0 step:5 [D loss: 0.117719, acc.: 97.66%] [G loss: 1.354308]\n",
      "epoch:0 step:6 [D loss: 0.080510, acc.: 100.00%] [G loss: 1.150870]\n",
      "epoch:0 step:7 [D loss: 0.074582, acc.: 100.00%] [G loss: 1.146539]\n",
      "epoch:0 step:8 [D loss: 0.101671, acc.: 98.44%] [G loss: 1.371565]\n",
      "epoch:0 step:9 [D loss: 0.079582, acc.: 99.22%] [G loss: 1.747888]\n",
      "epoch:0 step:10 [D loss: 0.193920, acc.: 94.53%] [G loss: 2.035152]\n",
      "epoch:0 step:11 [D loss: 0.302437, acc.: 85.94%] [G loss: 3.538689]\n",
      "epoch:0 step:12 [D loss: 0.426587, acc.: 78.12%] [G loss: 5.252704]\n",
      "epoch:0 step:13 [D loss: 0.860469, acc.: 56.25%] [G loss: 5.176997]\n",
      "epoch:0 step:14 [D loss: 0.782995, acc.: 59.38%] [G loss: 4.087924]\n",
      "epoch:0 step:15 [D loss: 0.518344, acc.: 76.56%] [G loss: 2.711158]\n",
      "epoch:0 step:16 [D loss: 0.595389, acc.: 71.09%] [G loss: 1.693179]\n",
      "epoch:0 step:17 [D loss: 0.653290, acc.: 69.53%] [G loss: 1.931507]\n",
      "epoch:0 step:18 [D loss: 0.795313, acc.: 65.62%] [G loss: 2.123946]\n",
      "epoch:0 step:19 [D loss: 0.799247, acc.: 62.50%] [G loss: 2.131631]\n",
      "epoch:0 step:20 [D loss: 0.392205, acc.: 85.16%] [G loss: 1.381494]\n",
      "epoch:0 step:21 [D loss: 0.182135, acc.: 93.75%] [G loss: 0.783754]\n",
      "epoch:0 step:22 [D loss: 0.134660, acc.: 96.09%] [G loss: 0.344563]\n",
      "epoch:0 step:23 [D loss: 0.074616, acc.: 98.44%] [G loss: 0.375368]\n",
      "epoch:0 step:24 [D loss: 0.145751, acc.: 93.75%] [G loss: 0.416781]\n",
      "epoch:0 step:25 [D loss: 0.086697, acc.: 96.88%] [G loss: 0.330386]\n",
      "epoch:0 step:26 [D loss: 0.261387, acc.: 89.06%] [G loss: 0.430192]\n",
      "epoch:0 step:27 [D loss: 0.186852, acc.: 91.41%] [G loss: 0.719193]\n",
      "epoch:0 step:28 [D loss: 0.227143, acc.: 90.62%] [G loss: 1.146088]\n",
      "epoch:0 step:29 [D loss: 0.274159, acc.: 89.06%] [G loss: 1.291530]\n",
      "epoch:0 step:30 [D loss: 0.516422, acc.: 78.12%] [G loss: 1.743555]\n",
      "epoch:0 step:31 [D loss: 0.946751, acc.: 52.34%] [G loss: 2.662613]\n",
      "epoch:0 step:32 [D loss: 1.828714, acc.: 17.97%] [G loss: 2.451383]\n",
      "epoch:0 step:33 [D loss: 1.221844, acc.: 38.28%] [G loss: 1.798758]\n",
      "epoch:0 step:34 [D loss: 0.850478, acc.: 63.28%] [G loss: 1.305343]\n",
      "epoch:0 step:35 [D loss: 0.732546, acc.: 64.06%] [G loss: 1.125249]\n",
      "epoch:0 step:36 [D loss: 0.450117, acc.: 82.03%] [G loss: 1.105680]\n",
      "epoch:0 step:37 [D loss: 1.022085, acc.: 48.44%] [G loss: 1.412232]\n",
      "epoch:0 step:38 [D loss: 1.199890, acc.: 38.28%] [G loss: 2.076221]\n",
      "epoch:0 step:39 [D loss: 0.954022, acc.: 41.41%] [G loss: 1.752969]\n",
      "epoch:0 step:40 [D loss: 0.891019, acc.: 52.34%] [G loss: 1.535095]\n",
      "epoch:0 step:41 [D loss: 0.757466, acc.: 57.81%] [G loss: 1.532212]\n",
      "epoch:0 step:42 [D loss: 0.860661, acc.: 47.66%] [G loss: 1.589195]\n",
      "epoch:0 step:43 [D loss: 0.759014, acc.: 64.84%] [G loss: 1.809172]\n",
      "epoch:0 step:44 [D loss: 0.674252, acc.: 67.19%] [G loss: 1.828609]\n",
      "epoch:0 step:45 [D loss: 0.821020, acc.: 53.91%] [G loss: 1.532856]\n",
      "epoch:0 step:46 [D loss: 0.653958, acc.: 60.94%] [G loss: 1.794730]\n",
      "epoch:0 step:47 [D loss: 0.849424, acc.: 51.56%] [G loss: 1.725819]\n",
      "epoch:0 step:48 [D loss: 0.680910, acc.: 59.38%] [G loss: 1.819498]\n",
      "epoch:0 step:49 [D loss: 0.653624, acc.: 66.41%] [G loss: 1.659112]\n",
      "epoch:0 step:50 [D loss: 0.697077, acc.: 61.72%] [G loss: 1.545651]\n",
      "epoch:0 step:51 [D loss: 0.677213, acc.: 62.50%] [G loss: 1.748489]\n",
      "epoch:0 step:52 [D loss: 0.671651, acc.: 65.62%] [G loss: 1.956273]\n",
      "epoch:0 step:53 [D loss: 0.754810, acc.: 61.72%] [G loss: 1.898693]\n",
      "epoch:0 step:54 [D loss: 0.783187, acc.: 55.47%] [G loss: 1.653587]\n",
      "epoch:0 step:55 [D loss: 0.752350, acc.: 56.25%] [G loss: 1.882218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:56 [D loss: 0.822589, acc.: 59.38%] [G loss: 1.455574]\n",
      "epoch:0 step:57 [D loss: 0.686524, acc.: 60.16%] [G loss: 1.465844]\n",
      "epoch:0 step:58 [D loss: 0.790365, acc.: 57.81%] [G loss: 1.865590]\n",
      "epoch:0 step:59 [D loss: 0.727056, acc.: 58.59%] [G loss: 1.680666]\n",
      "epoch:0 step:60 [D loss: 0.668991, acc.: 65.62%] [G loss: 1.679888]\n",
      "epoch:0 step:61 [D loss: 0.495998, acc.: 77.34%] [G loss: 1.597771]\n",
      "epoch:0 step:62 [D loss: 0.523173, acc.: 76.56%] [G loss: 1.553009]\n",
      "epoch:0 step:63 [D loss: 0.486684, acc.: 76.56%] [G loss: 1.273860]\n",
      "epoch:0 step:64 [D loss: 0.418755, acc.: 82.03%] [G loss: 0.918257]\n",
      "epoch:0 step:65 [D loss: 0.491727, acc.: 77.34%] [G loss: 0.978193]\n",
      "epoch:0 step:66 [D loss: 0.413580, acc.: 83.59%] [G loss: 0.968596]\n",
      "epoch:0 step:67 [D loss: 0.533194, acc.: 72.66%] [G loss: 0.794507]\n",
      "epoch:0 step:68 [D loss: 0.821108, acc.: 57.81%] [G loss: 1.206363]\n",
      "epoch:0 step:69 [D loss: 0.931978, acc.: 52.34%] [G loss: 1.400116]\n",
      "epoch:0 step:70 [D loss: 0.755480, acc.: 55.47%] [G loss: 1.697675]\n",
      "epoch:0 step:71 [D loss: 0.631466, acc.: 65.62%] [G loss: 1.571023]\n",
      "epoch:0 step:72 [D loss: 0.528495, acc.: 74.22%] [G loss: 1.538061]\n",
      "epoch:0 step:73 [D loss: 0.676076, acc.: 67.19%] [G loss: 1.837761]\n",
      "epoch:0 step:74 [D loss: 0.854847, acc.: 55.47%] [G loss: 1.718430]\n",
      "epoch:0 step:75 [D loss: 0.776110, acc.: 56.25%] [G loss: 2.014989]\n",
      "epoch:0 step:76 [D loss: 0.873656, acc.: 50.00%] [G loss: 1.856804]\n",
      "epoch:0 step:77 [D loss: 0.644897, acc.: 61.72%] [G loss: 1.663976]\n",
      "epoch:0 step:78 [D loss: 0.625612, acc.: 65.62%] [G loss: 1.657672]\n",
      "epoch:0 step:79 [D loss: 0.618020, acc.: 65.62%] [G loss: 1.565553]\n",
      "epoch:0 step:80 [D loss: 0.631347, acc.: 65.62%] [G loss: 1.482951]\n",
      "epoch:0 step:81 [D loss: 0.948772, acc.: 53.91%] [G loss: 1.886075]\n",
      "epoch:0 step:82 [D loss: 0.698672, acc.: 60.16%] [G loss: 1.927100]\n",
      "epoch:0 step:83 [D loss: 0.840539, acc.: 52.34%] [G loss: 1.383776]\n",
      "epoch:0 step:84 [D loss: 0.603827, acc.: 68.75%] [G loss: 1.433587]\n",
      "epoch:0 step:85 [D loss: 0.565711, acc.: 70.31%] [G loss: 1.155391]\n",
      "epoch:0 step:86 [D loss: 0.619798, acc.: 66.41%] [G loss: 1.360080]\n",
      "epoch:0 step:87 [D loss: 0.749057, acc.: 62.50%] [G loss: 1.325987]\n",
      "epoch:0 step:88 [D loss: 0.788850, acc.: 57.81%] [G loss: 1.231503]\n",
      "epoch:0 step:89 [D loss: 1.017426, acc.: 42.19%] [G loss: 1.244526]\n",
      "epoch:0 step:90 [D loss: 0.765971, acc.: 61.72%] [G loss: 1.546383]\n",
      "epoch:0 step:91 [D loss: 0.898818, acc.: 45.31%] [G loss: 1.581636]\n",
      "epoch:0 step:92 [D loss: 0.699543, acc.: 60.94%] [G loss: 1.724122]\n",
      "epoch:0 step:93 [D loss: 0.702469, acc.: 58.59%] [G loss: 1.608257]\n",
      "epoch:0 step:94 [D loss: 0.746786, acc.: 57.81%] [G loss: 1.712093]\n",
      "epoch:0 step:95 [D loss: 0.770733, acc.: 55.47%] [G loss: 1.673833]\n",
      "epoch:0 step:96 [D loss: 0.764621, acc.: 60.94%] [G loss: 1.620872]\n",
      "epoch:0 step:97 [D loss: 0.713776, acc.: 62.50%] [G loss: 1.531726]\n",
      "epoch:0 step:98 [D loss: 0.762890, acc.: 58.59%] [G loss: 1.621449]\n",
      "epoch:0 step:99 [D loss: 0.704636, acc.: 60.16%] [G loss: 1.546235]\n",
      "epoch:0 step:100 [D loss: 0.785337, acc.: 57.03%] [G loss: 1.407486]\n",
      "epoch:0 step:101 [D loss: 0.860858, acc.: 50.00%] [G loss: 1.342692]\n",
      "epoch:0 step:102 [D loss: 0.926320, acc.: 45.31%] [G loss: 1.569052]\n",
      "epoch:0 step:103 [D loss: 0.698262, acc.: 63.28%] [G loss: 1.373469]\n",
      "epoch:0 step:104 [D loss: 0.712978, acc.: 57.81%] [G loss: 1.395536]\n",
      "epoch:0 step:105 [D loss: 0.692077, acc.: 66.41%] [G loss: 1.496663]\n",
      "epoch:0 step:106 [D loss: 0.599635, acc.: 65.62%] [G loss: 1.432950]\n",
      "epoch:0 step:107 [D loss: 0.704634, acc.: 60.94%] [G loss: 1.274011]\n",
      "epoch:0 step:108 [D loss: 0.847041, acc.: 53.12%] [G loss: 1.460811]\n",
      "epoch:0 step:109 [D loss: 0.650252, acc.: 62.50%] [G loss: 1.430222]\n",
      "epoch:0 step:110 [D loss: 0.747672, acc.: 57.03%] [G loss: 1.243195]\n",
      "epoch:0 step:111 [D loss: 0.679952, acc.: 62.50%] [G loss: 1.527314]\n",
      "epoch:0 step:112 [D loss: 0.857790, acc.: 48.44%] [G loss: 1.285668]\n",
      "epoch:0 step:113 [D loss: 0.656258, acc.: 65.62%] [G loss: 1.753746]\n",
      "epoch:0 step:114 [D loss: 0.592350, acc.: 68.75%] [G loss: 1.382068]\n",
      "epoch:0 step:115 [D loss: 0.777717, acc.: 57.03%] [G loss: 1.285854]\n",
      "epoch:0 step:116 [D loss: 0.737457, acc.: 58.59%] [G loss: 1.497977]\n",
      "epoch:0 step:117 [D loss: 0.742367, acc.: 53.91%] [G loss: 1.801174]\n",
      "epoch:0 step:118 [D loss: 0.858445, acc.: 53.12%] [G loss: 1.448493]\n",
      "epoch:0 step:119 [D loss: 0.740916, acc.: 56.25%] [G loss: 1.207779]\n",
      "epoch:0 step:120 [D loss: 0.792003, acc.: 53.91%] [G loss: 1.420788]\n",
      "epoch:0 step:121 [D loss: 0.762265, acc.: 57.81%] [G loss: 1.289717]\n",
      "epoch:0 step:122 [D loss: 0.697742, acc.: 56.25%] [G loss: 1.406323]\n",
      "epoch:0 step:123 [D loss: 0.591053, acc.: 72.66%] [G loss: 1.710222]\n",
      "epoch:0 step:124 [D loss: 0.640822, acc.: 69.53%] [G loss: 1.543294]\n",
      "epoch:0 step:125 [D loss: 0.666100, acc.: 62.50%] [G loss: 1.191068]\n",
      "epoch:0 step:126 [D loss: 0.629676, acc.: 67.19%] [G loss: 1.258460]\n",
      "epoch:0 step:127 [D loss: 0.614775, acc.: 68.75%] [G loss: 1.628142]\n",
      "epoch:0 step:128 [D loss: 0.689662, acc.: 60.16%] [G loss: 1.552811]\n",
      "epoch:0 step:129 [D loss: 0.821586, acc.: 57.81%] [G loss: 1.457332]\n",
      "epoch:0 step:130 [D loss: 0.691341, acc.: 60.94%] [G loss: 1.533037]\n",
      "epoch:0 step:131 [D loss: 0.716039, acc.: 62.50%] [G loss: 1.699286]\n",
      "epoch:0 step:132 [D loss: 0.876234, acc.: 46.09%] [G loss: 1.475780]\n",
      "epoch:0 step:133 [D loss: 0.762782, acc.: 57.03%] [G loss: 1.482293]\n",
      "epoch:0 step:134 [D loss: 0.645203, acc.: 65.62%] [G loss: 1.546607]\n",
      "epoch:0 step:135 [D loss: 0.764007, acc.: 54.69%] [G loss: 1.229648]\n",
      "epoch:0 step:136 [D loss: 0.709938, acc.: 59.38%] [G loss: 1.386932]\n",
      "epoch:0 step:137 [D loss: 0.481399, acc.: 80.47%] [G loss: 1.650093]\n",
      "epoch:0 step:138 [D loss: 0.748247, acc.: 59.38%] [G loss: 1.359539]\n",
      "epoch:0 step:139 [D loss: 0.762184, acc.: 57.81%] [G loss: 1.317970]\n",
      "epoch:0 step:140 [D loss: 0.711180, acc.: 59.38%] [G loss: 1.479116]\n",
      "epoch:0 step:141 [D loss: 0.617195, acc.: 67.97%] [G loss: 1.527345]\n",
      "epoch:0 step:142 [D loss: 0.761326, acc.: 55.47%] [G loss: 1.512150]\n",
      "epoch:0 step:143 [D loss: 0.706131, acc.: 60.16%] [G loss: 1.536931]\n",
      "epoch:0 step:144 [D loss: 0.684161, acc.: 60.16%] [G loss: 1.532521]\n",
      "epoch:0 step:145 [D loss: 0.688954, acc.: 69.53%] [G loss: 1.680124]\n",
      "epoch:0 step:146 [D loss: 0.718357, acc.: 64.84%] [G loss: 1.484438]\n",
      "epoch:0 step:147 [D loss: 0.780526, acc.: 57.81%] [G loss: 1.225309]\n",
      "epoch:0 step:148 [D loss: 0.752731, acc.: 56.25%] [G loss: 1.291605]\n",
      "epoch:0 step:149 [D loss: 0.746070, acc.: 57.81%] [G loss: 1.252805]\n",
      "epoch:0 step:150 [D loss: 0.714901, acc.: 58.59%] [G loss: 1.386049]\n",
      "epoch:0 step:151 [D loss: 0.794811, acc.: 55.47%] [G loss: 1.551615]\n",
      "epoch:0 step:152 [D loss: 0.593153, acc.: 64.84%] [G loss: 1.440987]\n",
      "epoch:0 step:153 [D loss: 0.696400, acc.: 58.59%] [G loss: 1.331377]\n",
      "epoch:0 step:154 [D loss: 0.642575, acc.: 65.62%] [G loss: 1.259862]\n",
      "epoch:0 step:155 [D loss: 0.688248, acc.: 63.28%] [G loss: 1.598529]\n",
      "epoch:0 step:156 [D loss: 0.680407, acc.: 64.84%] [G loss: 1.481275]\n",
      "epoch:0 step:157 [D loss: 0.696434, acc.: 62.50%] [G loss: 1.358685]\n",
      "epoch:0 step:158 [D loss: 0.646087, acc.: 69.53%] [G loss: 1.434686]\n",
      "epoch:0 step:159 [D loss: 0.598034, acc.: 70.31%] [G loss: 1.315316]\n",
      "epoch:0 step:160 [D loss: 0.649550, acc.: 64.06%] [G loss: 1.328310]\n",
      "epoch:0 step:161 [D loss: 0.596523, acc.: 66.41%] [G loss: 1.541965]\n",
      "epoch:0 step:162 [D loss: 0.718107, acc.: 57.81%] [G loss: 1.266446]\n",
      "epoch:0 step:163 [D loss: 0.731765, acc.: 55.47%] [G loss: 1.371295]\n",
      "epoch:0 step:164 [D loss: 0.671614, acc.: 64.06%] [G loss: 1.445198]\n",
      "epoch:0 step:165 [D loss: 0.542475, acc.: 70.31%] [G loss: 1.442360]\n",
      "epoch:0 step:166 [D loss: 0.694618, acc.: 59.38%] [G loss: 1.284225]\n",
      "epoch:0 step:167 [D loss: 0.666224, acc.: 60.94%] [G loss: 1.482828]\n",
      "epoch:0 step:168 [D loss: 0.731359, acc.: 60.16%] [G loss: 1.378025]\n",
      "epoch:0 step:169 [D loss: 0.692602, acc.: 57.81%] [G loss: 1.260806]\n",
      "epoch:0 step:170 [D loss: 0.521346, acc.: 74.22%] [G loss: 1.691834]\n",
      "epoch:0 step:171 [D loss: 0.779842, acc.: 55.47%] [G loss: 1.511276]\n",
      "epoch:0 step:172 [D loss: 0.799887, acc.: 53.91%] [G loss: 1.706370]\n",
      "epoch:0 step:173 [D loss: 0.718207, acc.: 53.91%] [G loss: 1.453164]\n",
      "epoch:0 step:174 [D loss: 0.689917, acc.: 64.84%] [G loss: 1.466362]\n",
      "epoch:0 step:175 [D loss: 0.679075, acc.: 62.50%] [G loss: 1.320845]\n",
      "epoch:0 step:176 [D loss: 0.677055, acc.: 58.59%] [G loss: 1.383088]\n",
      "epoch:0 step:177 [D loss: 0.609854, acc.: 63.28%] [G loss: 1.319489]\n",
      "epoch:0 step:178 [D loss: 0.617833, acc.: 65.62%] [G loss: 1.370700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:179 [D loss: 0.669058, acc.: 65.62%] [G loss: 1.486704]\n",
      "epoch:0 step:180 [D loss: 0.644264, acc.: 67.97%] [G loss: 1.508768]\n",
      "epoch:0 step:181 [D loss: 0.728246, acc.: 59.38%] [G loss: 1.317614]\n",
      "epoch:0 step:182 [D loss: 0.491400, acc.: 80.47%] [G loss: 1.409816]\n",
      "epoch:0 step:183 [D loss: 0.594720, acc.: 70.31%] [G loss: 1.129401]\n",
      "epoch:0 step:184 [D loss: 0.528518, acc.: 71.88%] [G loss: 1.474337]\n",
      "epoch:0 step:185 [D loss: 0.650714, acc.: 60.16%] [G loss: 1.353460]\n",
      "epoch:0 step:186 [D loss: 0.673439, acc.: 60.94%] [G loss: 1.305401]\n",
      "epoch:0 step:187 [D loss: 0.534003, acc.: 73.44%] [G loss: 1.560924]\n",
      "epoch:0 step:188 [D loss: 0.635142, acc.: 67.19%] [G loss: 1.525559]\n",
      "epoch:0 step:189 [D loss: 0.599141, acc.: 72.66%] [G loss: 1.361362]\n",
      "epoch:0 step:190 [D loss: 0.497914, acc.: 77.34%] [G loss: 1.597524]\n",
      "epoch:0 step:191 [D loss: 0.582320, acc.: 67.97%] [G loss: 1.761942]\n",
      "epoch:0 step:192 [D loss: 0.642452, acc.: 61.72%] [G loss: 1.387023]\n",
      "epoch:0 step:193 [D loss: 0.673547, acc.: 57.03%] [G loss: 1.596242]\n",
      "epoch:0 step:194 [D loss: 0.506496, acc.: 77.34%] [G loss: 1.568677]\n",
      "epoch:0 step:195 [D loss: 0.571784, acc.: 74.22%] [G loss: 1.473115]\n",
      "epoch:0 step:196 [D loss: 0.547674, acc.: 70.31%] [G loss: 1.647104]\n",
      "epoch:0 step:197 [D loss: 0.654942, acc.: 68.75%] [G loss: 1.261353]\n",
      "epoch:0 step:198 [D loss: 0.664001, acc.: 65.62%] [G loss: 1.315708]\n",
      "epoch:0 step:199 [D loss: 0.666682, acc.: 63.28%] [G loss: 1.769493]\n",
      "epoch:0 step:200 [D loss: 0.493340, acc.: 77.34%] [G loss: 2.005569]\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "##############\n",
      "[2.87826752 2.98091557 2.75425291 4.03969689 1.96004813 6.88649814\n",
      " 2.50271149 3.86422841 4.01740129 5.56971149]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.557911, acc.: 71.09%] [G loss: 1.634822]\n",
      "epoch:0 step:202 [D loss: 0.761247, acc.: 60.16%] [G loss: 1.346001]\n",
      "epoch:0 step:203 [D loss: 0.481782, acc.: 74.22%] [G loss: 1.741007]\n",
      "epoch:0 step:204 [D loss: 0.561080, acc.: 75.78%] [G loss: 1.472909]\n",
      "epoch:0 step:205 [D loss: 0.474219, acc.: 72.66%] [G loss: 1.431535]\n",
      "epoch:0 step:206 [D loss: 0.498872, acc.: 78.91%] [G loss: 1.440520]\n",
      "epoch:0 step:207 [D loss: 0.567955, acc.: 65.62%] [G loss: 1.678550]\n",
      "epoch:0 step:208 [D loss: 0.530869, acc.: 69.53%] [G loss: 1.733441]\n",
      "epoch:0 step:209 [D loss: 0.544816, acc.: 71.88%] [G loss: 1.708170]\n",
      "epoch:0 step:210 [D loss: 0.588721, acc.: 65.62%] [G loss: 1.607292]\n",
      "epoch:0 step:211 [D loss: 0.523637, acc.: 71.88%] [G loss: 1.757370]\n",
      "epoch:0 step:212 [D loss: 0.525940, acc.: 71.09%] [G loss: 1.756101]\n",
      "epoch:0 step:213 [D loss: 0.482111, acc.: 75.00%] [G loss: 1.839943]\n",
      "epoch:0 step:214 [D loss: 0.489379, acc.: 75.78%] [G loss: 1.380967]\n",
      "epoch:0 step:215 [D loss: 0.548077, acc.: 67.97%] [G loss: 1.444592]\n",
      "epoch:0 step:216 [D loss: 0.713836, acc.: 55.47%] [G loss: 1.493501]\n",
      "epoch:0 step:217 [D loss: 0.695051, acc.: 60.16%] [G loss: 1.661012]\n",
      "epoch:0 step:218 [D loss: 0.673071, acc.: 67.19%] [G loss: 1.525393]\n",
      "epoch:0 step:219 [D loss: 0.740510, acc.: 57.81%] [G loss: 1.612121]\n",
      "epoch:0 step:220 [D loss: 0.619353, acc.: 67.97%] [G loss: 1.524623]\n",
      "epoch:0 step:221 [D loss: 0.491717, acc.: 77.34%] [G loss: 1.560939]\n",
      "epoch:0 step:222 [D loss: 0.588474, acc.: 70.31%] [G loss: 1.675754]\n",
      "epoch:0 step:223 [D loss: 0.672121, acc.: 67.19%] [G loss: 1.516028]\n",
      "epoch:0 step:224 [D loss: 0.661449, acc.: 65.62%] [G loss: 1.407938]\n",
      "epoch:0 step:225 [D loss: 0.552592, acc.: 71.88%] [G loss: 1.561038]\n",
      "epoch:0 step:226 [D loss: 0.491802, acc.: 76.56%] [G loss: 1.754457]\n",
      "epoch:0 step:227 [D loss: 0.478570, acc.: 77.34%] [G loss: 1.506184]\n",
      "epoch:0 step:228 [D loss: 0.708320, acc.: 60.94%] [G loss: 1.435665]\n",
      "epoch:0 step:229 [D loss: 0.519449, acc.: 78.12%] [G loss: 1.553231]\n",
      "epoch:0 step:230 [D loss: 0.520149, acc.: 73.44%] [G loss: 1.532113]\n",
      "epoch:0 step:231 [D loss: 0.589813, acc.: 68.75%] [G loss: 1.581171]\n",
      "epoch:0 step:232 [D loss: 0.475749, acc.: 78.12%] [G loss: 1.772790]\n",
      "epoch:0 step:233 [D loss: 0.650076, acc.: 60.16%] [G loss: 1.644039]\n",
      "epoch:0 step:234 [D loss: 0.587224, acc.: 68.75%] [G loss: 1.166427]\n",
      "epoch:0 step:235 [D loss: 0.556499, acc.: 67.97%] [G loss: 1.220912]\n",
      "epoch:0 step:236 [D loss: 0.666483, acc.: 60.94%] [G loss: 1.354851]\n",
      "epoch:0 step:237 [D loss: 0.827281, acc.: 53.91%] [G loss: 1.359371]\n",
      "epoch:0 step:238 [D loss: 0.579674, acc.: 71.88%] [G loss: 1.654562]\n",
      "epoch:0 step:239 [D loss: 0.565293, acc.: 64.84%] [G loss: 1.548179]\n",
      "epoch:0 step:240 [D loss: 0.612148, acc.: 67.19%] [G loss: 1.571001]\n",
      "epoch:0 step:241 [D loss: 0.609236, acc.: 69.53%] [G loss: 1.549996]\n",
      "epoch:0 step:242 [D loss: 0.719298, acc.: 56.25%] [G loss: 1.554239]\n",
      "epoch:0 step:243 [D loss: 0.505035, acc.: 74.22%] [G loss: 1.776536]\n",
      "epoch:0 step:244 [D loss: 0.582566, acc.: 67.97%] [G loss: 1.533475]\n",
      "epoch:0 step:245 [D loss: 0.552982, acc.: 72.66%] [G loss: 1.572800]\n",
      "epoch:0 step:246 [D loss: 0.674511, acc.: 65.62%] [G loss: 1.484014]\n",
      "epoch:0 step:247 [D loss: 0.593929, acc.: 70.31%] [G loss: 1.730051]\n",
      "epoch:0 step:248 [D loss: 0.667574, acc.: 63.28%] [G loss: 1.864998]\n",
      "epoch:0 step:249 [D loss: 0.567708, acc.: 69.53%] [G loss: 1.703040]\n",
      "epoch:0 step:250 [D loss: 0.569158, acc.: 75.00%] [G loss: 1.711727]\n",
      "epoch:0 step:251 [D loss: 0.578311, acc.: 70.31%] [G loss: 1.703040]\n",
      "epoch:0 step:252 [D loss: 0.407447, acc.: 81.25%] [G loss: 1.829665]\n",
      "epoch:0 step:253 [D loss: 0.642400, acc.: 64.84%] [G loss: 1.433021]\n",
      "epoch:0 step:254 [D loss: 0.535376, acc.: 76.56%] [G loss: 1.355798]\n",
      "epoch:0 step:255 [D loss: 0.478794, acc.: 80.47%] [G loss: 1.488578]\n",
      "epoch:0 step:256 [D loss: 0.498380, acc.: 78.91%] [G loss: 1.227356]\n",
      "epoch:0 step:257 [D loss: 0.545079, acc.: 69.53%] [G loss: 1.613792]\n",
      "epoch:0 step:258 [D loss: 0.710789, acc.: 59.38%] [G loss: 1.307899]\n",
      "epoch:0 step:259 [D loss: 0.815472, acc.: 53.91%] [G loss: 1.221188]\n",
      "epoch:0 step:260 [D loss: 0.619702, acc.: 65.62%] [G loss: 1.605878]\n",
      "epoch:0 step:261 [D loss: 0.658277, acc.: 63.28%] [G loss: 1.790571]\n",
      "epoch:0 step:262 [D loss: 0.711265, acc.: 60.16%] [G loss: 1.633999]\n",
      "epoch:0 step:263 [D loss: 0.618548, acc.: 66.41%] [G loss: 1.602453]\n",
      "epoch:0 step:264 [D loss: 0.623521, acc.: 61.72%] [G loss: 1.720232]\n",
      "epoch:0 step:265 [D loss: 0.580519, acc.: 67.97%] [G loss: 1.547858]\n",
      "epoch:0 step:266 [D loss: 0.683563, acc.: 60.94%] [G loss: 1.470574]\n",
      "epoch:0 step:267 [D loss: 0.479041, acc.: 77.34%] [G loss: 1.690169]\n",
      "epoch:0 step:268 [D loss: 0.572037, acc.: 71.09%] [G loss: 1.578051]\n",
      "epoch:0 step:269 [D loss: 0.728224, acc.: 59.38%] [G loss: 1.498834]\n",
      "epoch:0 step:270 [D loss: 0.620076, acc.: 68.75%] [G loss: 1.574370]\n",
      "epoch:0 step:271 [D loss: 0.615440, acc.: 65.62%] [G loss: 1.462028]\n",
      "epoch:0 step:272 [D loss: 0.698984, acc.: 60.16%] [G loss: 1.587268]\n",
      "epoch:0 step:273 [D loss: 0.625083, acc.: 64.84%] [G loss: 1.746584]\n",
      "epoch:0 step:274 [D loss: 0.687539, acc.: 58.59%] [G loss: 1.378950]\n",
      "epoch:0 step:275 [D loss: 0.590821, acc.: 68.75%] [G loss: 1.340657]\n",
      "epoch:0 step:276 [D loss: 0.696940, acc.: 60.94%] [G loss: 1.444958]\n",
      "epoch:0 step:277 [D loss: 0.714162, acc.: 57.03%] [G loss: 1.550069]\n",
      "epoch:0 step:278 [D loss: 0.639768, acc.: 60.16%] [G loss: 1.427941]\n",
      "epoch:0 step:279 [D loss: 0.573856, acc.: 71.09%] [G loss: 1.439357]\n",
      "epoch:0 step:280 [D loss: 0.828480, acc.: 46.88%] [G loss: 1.243795]\n",
      "epoch:0 step:281 [D loss: 0.633773, acc.: 64.84%] [G loss: 1.433869]\n",
      "epoch:0 step:282 [D loss: 0.554496, acc.: 73.44%] [G loss: 1.548391]\n",
      "epoch:0 step:283 [D loss: 0.535788, acc.: 72.66%] [G loss: 1.502712]\n",
      "epoch:0 step:284 [D loss: 0.609838, acc.: 59.38%] [G loss: 1.312681]\n",
      "epoch:0 step:285 [D loss: 0.705697, acc.: 59.38%] [G loss: 1.485838]\n",
      "epoch:0 step:286 [D loss: 0.690668, acc.: 64.06%] [G loss: 1.517882]\n",
      "epoch:0 step:287 [D loss: 0.687659, acc.: 60.94%] [G loss: 1.428427]\n",
      "epoch:0 step:288 [D loss: 0.585114, acc.: 69.53%] [G loss: 1.576082]\n",
      "epoch:0 step:289 [D loss: 0.435405, acc.: 78.91%] [G loss: 1.728897]\n",
      "epoch:0 step:290 [D loss: 0.667290, acc.: 62.50%] [G loss: 1.695818]\n",
      "epoch:0 step:291 [D loss: 0.609891, acc.: 67.97%] [G loss: 1.372387]\n",
      "epoch:0 step:292 [D loss: 0.658609, acc.: 64.06%] [G loss: 1.454980]\n",
      "epoch:0 step:293 [D loss: 0.648249, acc.: 60.16%] [G loss: 1.661774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:294 [D loss: 0.535270, acc.: 76.56%] [G loss: 1.508343]\n",
      "epoch:0 step:295 [D loss: 0.641596, acc.: 62.50%] [G loss: 1.615929]\n",
      "epoch:0 step:296 [D loss: 0.624925, acc.: 60.94%] [G loss: 1.641127]\n",
      "epoch:0 step:297 [D loss: 0.807712, acc.: 57.03%] [G loss: 1.451208]\n",
      "epoch:0 step:298 [D loss: 0.584637, acc.: 67.19%] [G loss: 1.619772]\n",
      "epoch:0 step:299 [D loss: 0.572973, acc.: 72.66%] [G loss: 1.373768]\n",
      "epoch:0 step:300 [D loss: 0.653287, acc.: 67.19%] [G loss: 1.563024]\n",
      "epoch:0 step:301 [D loss: 0.660906, acc.: 63.28%] [G loss: 1.573438]\n",
      "epoch:0 step:302 [D loss: 0.634319, acc.: 64.84%] [G loss: 1.511027]\n",
      "epoch:0 step:303 [D loss: 0.518172, acc.: 76.56%] [G loss: 1.878928]\n",
      "epoch:0 step:304 [D loss: 0.521594, acc.: 73.44%] [G loss: 1.541513]\n",
      "epoch:0 step:305 [D loss: 0.568094, acc.: 73.44%] [G loss: 1.567520]\n",
      "epoch:0 step:306 [D loss: 0.613756, acc.: 73.44%] [G loss: 1.465927]\n",
      "epoch:0 step:307 [D loss: 0.493211, acc.: 76.56%] [G loss: 1.589814]\n",
      "epoch:0 step:308 [D loss: 0.666883, acc.: 65.62%] [G loss: 1.379773]\n",
      "epoch:0 step:309 [D loss: 0.723132, acc.: 57.81%] [G loss: 1.343320]\n",
      "epoch:0 step:310 [D loss: 0.632114, acc.: 64.84%] [G loss: 1.459962]\n",
      "epoch:0 step:311 [D loss: 0.630344, acc.: 61.72%] [G loss: 1.404518]\n",
      "epoch:0 step:312 [D loss: 0.662436, acc.: 64.84%] [G loss: 1.255922]\n",
      "epoch:0 step:313 [D loss: 0.640102, acc.: 67.97%] [G loss: 1.274217]\n",
      "epoch:0 step:314 [D loss: 0.626475, acc.: 66.41%] [G loss: 1.739376]\n",
      "epoch:0 step:315 [D loss: 0.650922, acc.: 62.50%] [G loss: 1.370423]\n",
      "epoch:0 step:316 [D loss: 0.500453, acc.: 73.44%] [G loss: 1.504074]\n",
      "epoch:0 step:317 [D loss: 0.617050, acc.: 66.41%] [G loss: 1.676528]\n",
      "epoch:0 step:318 [D loss: 0.654266, acc.: 61.72%] [G loss: 1.660448]\n",
      "epoch:0 step:319 [D loss: 0.653218, acc.: 65.62%] [G loss: 1.567862]\n",
      "epoch:0 step:320 [D loss: 0.516419, acc.: 73.44%] [G loss: 1.578626]\n",
      "epoch:0 step:321 [D loss: 0.419490, acc.: 84.38%] [G loss: 1.768744]\n",
      "epoch:0 step:322 [D loss: 0.617631, acc.: 66.41%] [G loss: 1.503416]\n",
      "epoch:0 step:323 [D loss: 0.625435, acc.: 67.19%] [G loss: 1.537621]\n",
      "epoch:0 step:324 [D loss: 0.560111, acc.: 71.88%] [G loss: 1.457523]\n",
      "epoch:0 step:325 [D loss: 0.403204, acc.: 85.94%] [G loss: 1.713250]\n",
      "epoch:0 step:326 [D loss: 0.519855, acc.: 69.53%] [G loss: 1.556654]\n",
      "epoch:0 step:327 [D loss: 0.529614, acc.: 73.44%] [G loss: 1.508674]\n",
      "epoch:0 step:328 [D loss: 0.677345, acc.: 65.62%] [G loss: 1.497518]\n",
      "epoch:0 step:329 [D loss: 0.623517, acc.: 67.97%] [G loss: 1.518917]\n",
      "epoch:0 step:330 [D loss: 0.547320, acc.: 71.88%] [G loss: 1.409758]\n",
      "epoch:0 step:331 [D loss: 0.741361, acc.: 53.12%] [G loss: 1.470116]\n",
      "epoch:0 step:332 [D loss: 0.611421, acc.: 67.97%] [G loss: 1.720744]\n",
      "epoch:0 step:333 [D loss: 0.736864, acc.: 53.91%] [G loss: 1.480179]\n",
      "epoch:0 step:334 [D loss: 0.601485, acc.: 69.53%] [G loss: 1.568104]\n",
      "epoch:0 step:335 [D loss: 0.535902, acc.: 70.31%] [G loss: 1.739959]\n",
      "epoch:0 step:336 [D loss: 0.650351, acc.: 64.06%] [G loss: 1.566590]\n",
      "epoch:0 step:337 [D loss: 0.633676, acc.: 63.28%] [G loss: 1.595163]\n",
      "epoch:0 step:338 [D loss: 0.622148, acc.: 64.84%] [G loss: 1.811677]\n",
      "epoch:0 step:339 [D loss: 0.543145, acc.: 71.88%] [G loss: 1.717755]\n",
      "epoch:0 step:340 [D loss: 0.510049, acc.: 74.22%] [G loss: 1.776572]\n",
      "epoch:0 step:341 [D loss: 0.560503, acc.: 68.75%] [G loss: 1.632952]\n",
      "epoch:0 step:342 [D loss: 0.433278, acc.: 84.38%] [G loss: 1.618216]\n",
      "epoch:0 step:343 [D loss: 0.602273, acc.: 65.62%] [G loss: 1.910098]\n",
      "epoch:0 step:344 [D loss: 0.502263, acc.: 75.00%] [G loss: 1.633601]\n",
      "epoch:0 step:345 [D loss: 0.595414, acc.: 66.41%] [G loss: 1.542559]\n",
      "epoch:0 step:346 [D loss: 0.496658, acc.: 75.00%] [G loss: 1.582360]\n",
      "epoch:0 step:347 [D loss: 0.448255, acc.: 79.69%] [G loss: 1.765999]\n",
      "epoch:0 step:348 [D loss: 0.515245, acc.: 77.34%] [G loss: 1.674266]\n",
      "epoch:0 step:349 [D loss: 0.476573, acc.: 78.91%] [G loss: 1.789836]\n",
      "epoch:0 step:350 [D loss: 0.561967, acc.: 67.97%] [G loss: 1.569514]\n",
      "epoch:0 step:351 [D loss: 0.586550, acc.: 68.75%] [G loss: 1.580709]\n",
      "epoch:0 step:352 [D loss: 0.615064, acc.: 64.84%] [G loss: 1.628370]\n",
      "epoch:0 step:353 [D loss: 0.597815, acc.: 68.75%] [G loss: 1.629210]\n",
      "epoch:0 step:354 [D loss: 0.577393, acc.: 64.06%] [G loss: 1.795849]\n",
      "epoch:0 step:355 [D loss: 0.475392, acc.: 78.91%] [G loss: 1.646508]\n",
      "epoch:0 step:356 [D loss: 0.624699, acc.: 67.19%] [G loss: 1.544709]\n",
      "epoch:0 step:357 [D loss: 0.485359, acc.: 78.91%] [G loss: 1.769288]\n",
      "epoch:0 step:358 [D loss: 0.612937, acc.: 67.19%] [G loss: 1.614194]\n",
      "epoch:0 step:359 [D loss: 0.526200, acc.: 68.75%] [G loss: 1.648722]\n",
      "epoch:0 step:360 [D loss: 0.547053, acc.: 72.66%] [G loss: 1.688026]\n",
      "epoch:0 step:361 [D loss: 0.692131, acc.: 57.81%] [G loss: 1.704852]\n",
      "epoch:0 step:362 [D loss: 0.469959, acc.: 77.34%] [G loss: 1.541995]\n",
      "epoch:0 step:363 [D loss: 0.544973, acc.: 69.53%] [G loss: 1.598599]\n",
      "epoch:0 step:364 [D loss: 0.597541, acc.: 71.09%] [G loss: 1.748703]\n",
      "epoch:0 step:365 [D loss: 0.477866, acc.: 76.56%] [G loss: 1.661960]\n",
      "epoch:0 step:366 [D loss: 0.486869, acc.: 75.78%] [G loss: 1.915507]\n",
      "epoch:0 step:367 [D loss: 0.794307, acc.: 58.59%] [G loss: 1.664258]\n",
      "epoch:0 step:368 [D loss: 0.529846, acc.: 75.78%] [G loss: 1.919681]\n",
      "epoch:0 step:369 [D loss: 0.603658, acc.: 64.84%] [G loss: 1.568280]\n",
      "epoch:0 step:370 [D loss: 0.566125, acc.: 71.09%] [G loss: 1.649712]\n",
      "epoch:0 step:371 [D loss: 0.398937, acc.: 82.03%] [G loss: 1.629585]\n",
      "epoch:0 step:372 [D loss: 0.503570, acc.: 72.66%] [G loss: 1.539290]\n",
      "epoch:0 step:373 [D loss: 0.543009, acc.: 71.88%] [G loss: 1.680257]\n",
      "epoch:0 step:374 [D loss: 0.459480, acc.: 79.69%] [G loss: 1.861757]\n",
      "epoch:0 step:375 [D loss: 0.408554, acc.: 85.94%] [G loss: 1.710176]\n",
      "epoch:0 step:376 [D loss: 0.523850, acc.: 71.88%] [G loss: 1.698562]\n",
      "epoch:0 step:377 [D loss: 0.588129, acc.: 70.31%] [G loss: 1.683879]\n",
      "epoch:0 step:378 [D loss: 0.569054, acc.: 67.19%] [G loss: 1.924049]\n",
      "epoch:0 step:379 [D loss: 0.426958, acc.: 82.03%] [G loss: 1.982945]\n",
      "epoch:0 step:380 [D loss: 0.586378, acc.: 68.75%] [G loss: 1.758258]\n",
      "epoch:0 step:381 [D loss: 0.699446, acc.: 61.72%] [G loss: 1.590093]\n",
      "epoch:0 step:382 [D loss: 0.519420, acc.: 73.44%] [G loss: 1.658872]\n",
      "epoch:0 step:383 [D loss: 0.560881, acc.: 72.66%] [G loss: 1.798714]\n",
      "epoch:0 step:384 [D loss: 0.546017, acc.: 71.09%] [G loss: 1.749734]\n",
      "epoch:0 step:385 [D loss: 0.554591, acc.: 75.00%] [G loss: 1.725760]\n",
      "epoch:0 step:386 [D loss: 0.717350, acc.: 63.28%] [G loss: 1.456586]\n",
      "epoch:0 step:387 [D loss: 0.663782, acc.: 64.06%] [G loss: 1.673402]\n",
      "epoch:0 step:388 [D loss: 0.608107, acc.: 66.41%] [G loss: 1.750476]\n",
      "epoch:0 step:389 [D loss: 0.713565, acc.: 57.81%] [G loss: 1.482095]\n",
      "epoch:0 step:390 [D loss: 0.460656, acc.: 81.25%] [G loss: 1.512878]\n",
      "epoch:0 step:391 [D loss: 0.633063, acc.: 63.28%] [G loss: 1.751366]\n",
      "epoch:0 step:392 [D loss: 0.662388, acc.: 64.84%] [G loss: 1.936887]\n",
      "epoch:0 step:393 [D loss: 0.567739, acc.: 67.97%] [G loss: 1.597284]\n",
      "epoch:0 step:394 [D loss: 0.532189, acc.: 72.66%] [G loss: 1.810684]\n",
      "epoch:0 step:395 [D loss: 0.503199, acc.: 72.66%] [G loss: 1.750787]\n",
      "epoch:0 step:396 [D loss: 0.505309, acc.: 76.56%] [G loss: 1.343551]\n",
      "epoch:0 step:397 [D loss: 0.501253, acc.: 78.12%] [G loss: 1.812160]\n",
      "epoch:0 step:398 [D loss: 0.535079, acc.: 71.88%] [G loss: 1.753043]\n",
      "epoch:0 step:399 [D loss: 0.455739, acc.: 75.78%] [G loss: 1.685822]\n",
      "epoch:0 step:400 [D loss: 0.353207, acc.: 84.38%] [G loss: 1.786654]\n",
      "##############\n",
      "[2.79872458 2.33947771 1.26042624 2.92722691 0.98937571 6.19081997\n",
      " 1.99875537 2.76173011 4.02987894 4.24011208]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.384711, acc.: 85.94%] [G loss: 1.687823]\n",
      "epoch:0 step:402 [D loss: 0.478419, acc.: 81.25%] [G loss: 1.437583]\n",
      "epoch:0 step:403 [D loss: 0.586492, acc.: 67.19%] [G loss: 1.802384]\n",
      "epoch:0 step:404 [D loss: 0.407521, acc.: 85.94%] [G loss: 1.921746]\n",
      "epoch:0 step:405 [D loss: 0.559649, acc.: 71.09%] [G loss: 1.397664]\n",
      "epoch:0 step:406 [D loss: 0.470710, acc.: 78.91%] [G loss: 1.560480]\n",
      "epoch:0 step:407 [D loss: 0.793159, acc.: 54.69%] [G loss: 1.315099]\n",
      "epoch:0 step:408 [D loss: 0.578227, acc.: 68.75%] [G loss: 1.533841]\n",
      "epoch:0 step:409 [D loss: 0.576706, acc.: 71.09%] [G loss: 1.546317]\n",
      "epoch:0 step:410 [D loss: 0.659734, acc.: 61.72%] [G loss: 1.434658]\n",
      "epoch:0 step:411 [D loss: 0.578822, acc.: 67.97%] [G loss: 1.379250]\n",
      "epoch:0 step:412 [D loss: 0.351484, acc.: 86.72%] [G loss: 1.738982]\n",
      "epoch:0 step:413 [D loss: 0.658210, acc.: 66.41%] [G loss: 1.469741]\n",
      "epoch:0 step:414 [D loss: 0.472422, acc.: 75.00%] [G loss: 1.683665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:415 [D loss: 0.613664, acc.: 64.84%] [G loss: 1.544177]\n",
      "epoch:0 step:416 [D loss: 0.616228, acc.: 67.19%] [G loss: 1.663039]\n",
      "epoch:0 step:417 [D loss: 0.658633, acc.: 68.75%] [G loss: 2.102263]\n",
      "epoch:0 step:418 [D loss: 0.723777, acc.: 59.38%] [G loss: 2.008166]\n",
      "epoch:0 step:419 [D loss: 0.436602, acc.: 77.34%] [G loss: 2.113604]\n",
      "epoch:0 step:420 [D loss: 0.631610, acc.: 62.50%] [G loss: 1.481114]\n",
      "epoch:0 step:421 [D loss: 0.647165, acc.: 64.06%] [G loss: 1.650931]\n",
      "epoch:0 step:422 [D loss: 0.487219, acc.: 74.22%] [G loss: 1.782959]\n",
      "epoch:0 step:423 [D loss: 0.605120, acc.: 66.41%] [G loss: 1.642191]\n",
      "epoch:0 step:424 [D loss: 0.606437, acc.: 67.19%] [G loss: 1.762268]\n",
      "epoch:0 step:425 [D loss: 0.592624, acc.: 70.31%] [G loss: 1.964741]\n",
      "epoch:0 step:426 [D loss: 0.568159, acc.: 75.78%] [G loss: 1.825477]\n",
      "epoch:0 step:427 [D loss: 0.531305, acc.: 72.66%] [G loss: 1.817023]\n",
      "epoch:0 step:428 [D loss: 0.557036, acc.: 71.88%] [G loss: 1.573107]\n",
      "epoch:0 step:429 [D loss: 0.699704, acc.: 60.94%] [G loss: 1.566462]\n",
      "epoch:0 step:430 [D loss: 0.464088, acc.: 80.47%] [G loss: 1.585326]\n",
      "epoch:0 step:431 [D loss: 0.651769, acc.: 64.84%] [G loss: 1.381039]\n",
      "epoch:0 step:432 [D loss: 0.441637, acc.: 77.34%] [G loss: 1.703892]\n",
      "epoch:0 step:433 [D loss: 0.465795, acc.: 78.12%] [G loss: 1.768297]\n",
      "epoch:0 step:434 [D loss: 0.517054, acc.: 71.88%] [G loss: 1.333215]\n",
      "epoch:0 step:435 [D loss: 0.504584, acc.: 75.78%] [G loss: 1.421820]\n",
      "epoch:0 step:436 [D loss: 0.515716, acc.: 71.09%] [G loss: 1.703116]\n",
      "epoch:0 step:437 [D loss: 0.740027, acc.: 57.03%] [G loss: 1.386208]\n",
      "epoch:0 step:438 [D loss: 0.747616, acc.: 57.03%] [G loss: 1.386350]\n",
      "epoch:0 step:439 [D loss: 0.696803, acc.: 60.16%] [G loss: 1.494639]\n",
      "epoch:0 step:440 [D loss: 0.615270, acc.: 66.41%] [G loss: 1.736716]\n",
      "epoch:0 step:441 [D loss: 0.587054, acc.: 68.75%] [G loss: 1.778877]\n",
      "epoch:0 step:442 [D loss: 0.594269, acc.: 67.97%] [G loss: 1.739164]\n",
      "epoch:0 step:443 [D loss: 0.545915, acc.: 67.19%] [G loss: 1.931016]\n",
      "epoch:0 step:444 [D loss: 0.608234, acc.: 63.28%] [G loss: 1.861967]\n",
      "epoch:0 step:445 [D loss: 0.563437, acc.: 72.66%] [G loss: 1.618820]\n",
      "epoch:0 step:446 [D loss: 0.703652, acc.: 61.72%] [G loss: 1.405251]\n",
      "epoch:0 step:447 [D loss: 0.445650, acc.: 78.91%] [G loss: 1.871186]\n",
      "epoch:0 step:448 [D loss: 0.503496, acc.: 78.12%] [G loss: 1.576723]\n",
      "epoch:0 step:449 [D loss: 0.403318, acc.: 84.38%] [G loss: 2.138812]\n",
      "epoch:0 step:450 [D loss: 0.440147, acc.: 80.47%] [G loss: 1.921959]\n",
      "epoch:0 step:451 [D loss: 0.516449, acc.: 74.22%] [G loss: 1.785963]\n",
      "epoch:0 step:452 [D loss: 0.509171, acc.: 77.34%] [G loss: 2.046094]\n",
      "epoch:0 step:453 [D loss: 0.493983, acc.: 82.03%] [G loss: 2.042898]\n",
      "epoch:0 step:454 [D loss: 0.484574, acc.: 78.91%] [G loss: 2.024394]\n",
      "epoch:0 step:455 [D loss: 0.458917, acc.: 78.91%] [G loss: 2.019890]\n",
      "epoch:0 step:456 [D loss: 0.601254, acc.: 67.19%] [G loss: 1.711271]\n",
      "epoch:0 step:457 [D loss: 0.555643, acc.: 69.53%] [G loss: 1.856300]\n",
      "epoch:0 step:458 [D loss: 0.520340, acc.: 75.00%] [G loss: 1.954894]\n",
      "epoch:0 step:459 [D loss: 0.653330, acc.: 61.72%] [G loss: 1.653806]\n",
      "epoch:0 step:460 [D loss: 0.517932, acc.: 76.56%] [G loss: 1.795199]\n",
      "epoch:0 step:461 [D loss: 0.626986, acc.: 64.06%] [G loss: 1.374215]\n",
      "epoch:0 step:462 [D loss: 0.564844, acc.: 73.44%] [G loss: 1.523347]\n",
      "epoch:0 step:463 [D loss: 0.552504, acc.: 71.09%] [G loss: 1.822150]\n",
      "epoch:0 step:464 [D loss: 0.529426, acc.: 71.09%] [G loss: 1.792276]\n",
      "epoch:0 step:465 [D loss: 0.570088, acc.: 69.53%] [G loss: 1.496380]\n",
      "epoch:0 step:466 [D loss: 0.484819, acc.: 77.34%] [G loss: 1.773913]\n",
      "epoch:0 step:467 [D loss: 0.622548, acc.: 66.41%] [G loss: 1.330817]\n",
      "epoch:0 step:468 [D loss: 0.566020, acc.: 69.53%] [G loss: 1.757878]\n",
      "epoch:0 step:469 [D loss: 0.552471, acc.: 68.75%] [G loss: 1.819385]\n",
      "epoch:0 step:470 [D loss: 0.613398, acc.: 67.19%] [G loss: 1.734812]\n",
      "epoch:0 step:471 [D loss: 0.682357, acc.: 59.38%] [G loss: 1.672414]\n",
      "epoch:0 step:472 [D loss: 0.491654, acc.: 75.78%] [G loss: 2.075154]\n",
      "epoch:0 step:473 [D loss: 0.606315, acc.: 68.75%] [G loss: 2.077330]\n",
      "epoch:0 step:474 [D loss: 0.764714, acc.: 60.16%] [G loss: 1.442891]\n",
      "epoch:0 step:475 [D loss: 0.453214, acc.: 75.78%] [G loss: 1.635974]\n",
      "epoch:0 step:476 [D loss: 0.518933, acc.: 73.44%] [G loss: 1.829363]\n",
      "epoch:0 step:477 [D loss: 0.643276, acc.: 65.62%] [G loss: 1.369398]\n",
      "epoch:0 step:478 [D loss: 0.413244, acc.: 82.03%] [G loss: 1.512618]\n",
      "epoch:0 step:479 [D loss: 0.515242, acc.: 78.12%] [G loss: 1.497188]\n",
      "epoch:0 step:480 [D loss: 0.552978, acc.: 71.09%] [G loss: 1.480689]\n",
      "epoch:0 step:481 [D loss: 0.490324, acc.: 78.12%] [G loss: 1.818568]\n",
      "epoch:0 step:482 [D loss: 0.581133, acc.: 68.75%] [G loss: 1.634288]\n",
      "epoch:0 step:483 [D loss: 0.602048, acc.: 70.31%] [G loss: 1.700334]\n",
      "epoch:0 step:484 [D loss: 0.533542, acc.: 73.44%] [G loss: 1.630774]\n",
      "epoch:0 step:485 [D loss: 0.605073, acc.: 67.97%] [G loss: 1.594300]\n",
      "epoch:0 step:486 [D loss: 0.667073, acc.: 60.16%] [G loss: 1.624102]\n",
      "epoch:0 step:487 [D loss: 0.560737, acc.: 71.88%] [G loss: 1.496761]\n",
      "epoch:0 step:488 [D loss: 0.564363, acc.: 64.84%] [G loss: 1.635338]\n",
      "epoch:0 step:489 [D loss: 0.651884, acc.: 57.81%] [G loss: 1.564703]\n",
      "epoch:0 step:490 [D loss: 0.542225, acc.: 75.00%] [G loss: 1.570039]\n",
      "epoch:0 step:491 [D loss: 0.511116, acc.: 75.00%] [G loss: 1.626526]\n",
      "epoch:0 step:492 [D loss: 0.571678, acc.: 72.66%] [G loss: 1.670977]\n",
      "epoch:0 step:493 [D loss: 0.484025, acc.: 76.56%] [G loss: 1.654062]\n",
      "epoch:0 step:494 [D loss: 0.537916, acc.: 77.34%] [G loss: 1.818730]\n",
      "epoch:0 step:495 [D loss: 0.537971, acc.: 70.31%] [G loss: 1.592303]\n",
      "epoch:0 step:496 [D loss: 0.545588, acc.: 75.00%] [G loss: 1.555603]\n",
      "epoch:0 step:497 [D loss: 0.572421, acc.: 71.09%] [G loss: 1.418364]\n",
      "epoch:0 step:498 [D loss: 0.508238, acc.: 74.22%] [G loss: 1.685495]\n",
      "epoch:0 step:499 [D loss: 0.688356, acc.: 60.16%] [G loss: 1.314478]\n",
      "epoch:0 step:500 [D loss: 0.558176, acc.: 68.75%] [G loss: 1.345750]\n",
      "epoch:0 step:501 [D loss: 0.712919, acc.: 61.72%] [G loss: 1.250269]\n",
      "epoch:0 step:502 [D loss: 0.604097, acc.: 67.19%] [G loss: 1.852229]\n",
      "epoch:0 step:503 [D loss: 0.626064, acc.: 67.97%] [G loss: 1.620242]\n",
      "epoch:0 step:504 [D loss: 0.577019, acc.: 70.31%] [G loss: 1.615981]\n",
      "epoch:0 step:505 [D loss: 0.622109, acc.: 69.53%] [G loss: 1.825653]\n",
      "epoch:0 step:506 [D loss: 0.618543, acc.: 64.84%] [G loss: 1.416907]\n",
      "epoch:0 step:507 [D loss: 0.576006, acc.: 72.66%] [G loss: 1.694508]\n",
      "epoch:0 step:508 [D loss: 0.528818, acc.: 73.44%] [G loss: 1.649302]\n",
      "epoch:0 step:509 [D loss: 0.530385, acc.: 71.09%] [G loss: 1.580162]\n",
      "epoch:0 step:510 [D loss: 0.570437, acc.: 71.88%] [G loss: 1.529930]\n",
      "epoch:0 step:511 [D loss: 0.686750, acc.: 63.28%] [G loss: 1.394767]\n",
      "epoch:0 step:512 [D loss: 0.581091, acc.: 70.31%] [G loss: 1.277864]\n",
      "epoch:0 step:513 [D loss: 0.440919, acc.: 78.91%] [G loss: 1.471359]\n",
      "epoch:0 step:514 [D loss: 0.534502, acc.: 71.09%] [G loss: 1.243722]\n",
      "epoch:0 step:515 [D loss: 0.424092, acc.: 82.81%] [G loss: 1.255941]\n",
      "epoch:0 step:516 [D loss: 0.475015, acc.: 79.69%] [G loss: 0.932430]\n",
      "epoch:0 step:517 [D loss: 0.628957, acc.: 67.19%] [G loss: 0.990774]\n",
      "epoch:0 step:518 [D loss: 0.434043, acc.: 77.34%] [G loss: 1.648707]\n",
      "epoch:0 step:519 [D loss: 0.423205, acc.: 82.03%] [G loss: 2.206378]\n",
      "epoch:0 step:520 [D loss: 0.440872, acc.: 83.59%] [G loss: 2.270375]\n",
      "epoch:0 step:521 [D loss: 0.399210, acc.: 85.16%] [G loss: 2.467356]\n",
      "epoch:0 step:522 [D loss: 0.275262, acc.: 91.41%] [G loss: 2.629693]\n",
      "epoch:0 step:523 [D loss: 0.416209, acc.: 84.38%] [G loss: 2.516283]\n",
      "epoch:0 step:524 [D loss: 0.440784, acc.: 74.22%] [G loss: 2.951079]\n",
      "epoch:0 step:525 [D loss: 0.797106, acc.: 55.47%] [G loss: 2.585350]\n",
      "epoch:0 step:526 [D loss: 0.748968, acc.: 59.38%] [G loss: 1.889971]\n",
      "epoch:0 step:527 [D loss: 0.833688, acc.: 53.12%] [G loss: 1.527513]\n",
      "epoch:0 step:528 [D loss: 0.621827, acc.: 62.50%] [G loss: 1.635522]\n",
      "epoch:0 step:529 [D loss: 0.630984, acc.: 63.28%] [G loss: 1.492892]\n",
      "epoch:0 step:530 [D loss: 0.590353, acc.: 67.97%] [G loss: 1.732360]\n",
      "epoch:0 step:531 [D loss: 0.608796, acc.: 70.31%] [G loss: 1.848751]\n",
      "epoch:0 step:532 [D loss: 0.571744, acc.: 67.19%] [G loss: 1.565828]\n",
      "epoch:0 step:533 [D loss: 0.627304, acc.: 68.75%] [G loss: 1.707948]\n",
      "epoch:0 step:534 [D loss: 0.476817, acc.: 78.12%] [G loss: 1.921852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:535 [D loss: 0.506811, acc.: 77.34%] [G loss: 1.681282]\n",
      "epoch:0 step:536 [D loss: 0.697803, acc.: 64.06%] [G loss: 1.358629]\n",
      "epoch:0 step:537 [D loss: 0.474201, acc.: 79.69%] [G loss: 1.854874]\n",
      "epoch:0 step:538 [D loss: 0.609897, acc.: 68.75%] [G loss: 1.612036]\n",
      "epoch:0 step:539 [D loss: 0.552634, acc.: 67.19%] [G loss: 1.614786]\n",
      "epoch:0 step:540 [D loss: 0.736431, acc.: 58.59%] [G loss: 1.796402]\n",
      "epoch:0 step:541 [D loss: 0.486510, acc.: 78.91%] [G loss: 1.833338]\n",
      "epoch:0 step:542 [D loss: 0.502825, acc.: 75.00%] [G loss: 1.562839]\n",
      "epoch:0 step:543 [D loss: 0.582429, acc.: 64.84%] [G loss: 1.624701]\n",
      "epoch:0 step:544 [D loss: 0.374364, acc.: 80.47%] [G loss: 1.729262]\n",
      "epoch:0 step:545 [D loss: 0.486057, acc.: 80.47%] [G loss: 1.418789]\n",
      "epoch:0 step:546 [D loss: 0.361908, acc.: 86.72%] [G loss: 1.306469]\n",
      "epoch:0 step:547 [D loss: 0.569249, acc.: 71.88%] [G loss: 1.438251]\n",
      "epoch:0 step:548 [D loss: 0.454257, acc.: 80.47%] [G loss: 1.126897]\n",
      "epoch:0 step:549 [D loss: 0.546296, acc.: 72.66%] [G loss: 1.176096]\n",
      "epoch:0 step:550 [D loss: 0.562469, acc.: 72.66%] [G loss: 1.080777]\n",
      "epoch:0 step:551 [D loss: 0.546356, acc.: 70.31%] [G loss: 1.044097]\n",
      "epoch:0 step:552 [D loss: 0.582904, acc.: 68.75%] [G loss: 0.827770]\n",
      "epoch:0 step:553 [D loss: 0.634596, acc.: 64.84%] [G loss: 1.107083]\n",
      "epoch:0 step:554 [D loss: 0.487369, acc.: 76.56%] [G loss: 1.145421]\n",
      "epoch:0 step:555 [D loss: 0.765966, acc.: 53.91%] [G loss: 1.228142]\n",
      "epoch:0 step:556 [D loss: 0.552551, acc.: 72.66%] [G loss: 1.057069]\n",
      "epoch:0 step:557 [D loss: 0.649871, acc.: 66.41%] [G loss: 1.501273]\n",
      "epoch:0 step:558 [D loss: 0.757434, acc.: 55.47%] [G loss: 1.294370]\n",
      "epoch:0 step:559 [D loss: 0.634903, acc.: 68.75%] [G loss: 1.329068]\n",
      "epoch:0 step:560 [D loss: 0.657783, acc.: 61.72%] [G loss: 1.550085]\n",
      "epoch:0 step:561 [D loss: 0.673812, acc.: 57.03%] [G loss: 1.460028]\n",
      "epoch:0 step:562 [D loss: 0.817344, acc.: 47.66%] [G loss: 1.507398]\n",
      "epoch:0 step:563 [D loss: 0.621644, acc.: 69.53%] [G loss: 1.497662]\n",
      "epoch:0 step:564 [D loss: 0.611186, acc.: 69.53%] [G loss: 1.691315]\n",
      "epoch:0 step:565 [D loss: 0.511149, acc.: 77.34%] [G loss: 1.819697]\n",
      "epoch:0 step:566 [D loss: 0.536113, acc.: 70.31%] [G loss: 1.627815]\n",
      "epoch:0 step:567 [D loss: 0.651639, acc.: 62.50%] [G loss: 1.330827]\n",
      "epoch:0 step:568 [D loss: 0.701756, acc.: 60.16%] [G loss: 1.317343]\n",
      "epoch:0 step:569 [D loss: 0.626260, acc.: 65.62%] [G loss: 1.362050]\n",
      "epoch:0 step:570 [D loss: 0.506567, acc.: 75.78%] [G loss: 1.656505]\n",
      "epoch:0 step:571 [D loss: 0.615026, acc.: 67.97%] [G loss: 1.410281]\n",
      "epoch:0 step:572 [D loss: 0.584298, acc.: 72.66%] [G loss: 1.568408]\n",
      "epoch:0 step:573 [D loss: 0.767124, acc.: 54.69%] [G loss: 1.407259]\n",
      "epoch:0 step:574 [D loss: 0.646828, acc.: 63.28%] [G loss: 1.591959]\n",
      "epoch:0 step:575 [D loss: 0.660474, acc.: 56.25%] [G loss: 1.326899]\n",
      "epoch:0 step:576 [D loss: 0.567908, acc.: 71.09%] [G loss: 1.676106]\n",
      "epoch:0 step:577 [D loss: 0.466473, acc.: 78.12%] [G loss: 1.603387]\n",
      "epoch:0 step:578 [D loss: 0.732886, acc.: 57.81%] [G loss: 1.526380]\n",
      "epoch:0 step:579 [D loss: 0.493830, acc.: 76.56%] [G loss: 1.591588]\n",
      "epoch:0 step:580 [D loss: 0.792095, acc.: 53.12%] [G loss: 1.113127]\n",
      "epoch:0 step:581 [D loss: 0.636934, acc.: 67.97%] [G loss: 1.343957]\n",
      "epoch:0 step:582 [D loss: 0.668643, acc.: 64.84%] [G loss: 1.614427]\n",
      "epoch:0 step:583 [D loss: 0.690251, acc.: 65.62%] [G loss: 1.569023]\n",
      "epoch:0 step:584 [D loss: 0.596266, acc.: 71.88%] [G loss: 1.425157]\n",
      "epoch:0 step:585 [D loss: 0.600866, acc.: 69.53%] [G loss: 1.533050]\n",
      "epoch:0 step:586 [D loss: 0.712676, acc.: 58.59%] [G loss: 1.703185]\n",
      "epoch:0 step:587 [D loss: 0.599131, acc.: 68.75%] [G loss: 1.722326]\n",
      "epoch:0 step:588 [D loss: 0.552102, acc.: 72.66%] [G loss: 1.860801]\n",
      "epoch:0 step:589 [D loss: 0.534012, acc.: 76.56%] [G loss: 1.487715]\n",
      "epoch:0 step:590 [D loss: 0.578508, acc.: 67.97%] [G loss: 1.579848]\n",
      "epoch:0 step:591 [D loss: 0.521519, acc.: 75.00%] [G loss: 1.767192]\n",
      "epoch:0 step:592 [D loss: 0.672770, acc.: 60.94%] [G loss: 1.367930]\n",
      "epoch:0 step:593 [D loss: 0.795255, acc.: 50.78%] [G loss: 1.539262]\n",
      "epoch:0 step:594 [D loss: 0.516084, acc.: 76.56%] [G loss: 1.560622]\n",
      "epoch:0 step:595 [D loss: 0.686209, acc.: 60.94%] [G loss: 1.298202]\n",
      "epoch:0 step:596 [D loss: 0.499757, acc.: 74.22%] [G loss: 1.856583]\n",
      "epoch:0 step:597 [D loss: 0.516377, acc.: 73.44%] [G loss: 1.610073]\n",
      "epoch:0 step:598 [D loss: 0.646517, acc.: 67.19%] [G loss: 1.308568]\n",
      "epoch:0 step:599 [D loss: 0.675609, acc.: 64.84%] [G loss: 1.213233]\n",
      "epoch:0 step:600 [D loss: 0.465856, acc.: 75.00%] [G loss: 1.655393]\n",
      "##############\n",
      "[2.84352056 2.57925571 1.49413204 3.30133604 1.24400353 7.5598824\n",
      " 2.28082442 3.23760667 4.04726839 3.93002345]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.531327, acc.: 71.88%] [G loss: 1.413183]\n",
      "epoch:0 step:602 [D loss: 0.603815, acc.: 67.19%] [G loss: 1.433657]\n",
      "epoch:0 step:603 [D loss: 0.617877, acc.: 69.53%] [G loss: 1.369635]\n",
      "epoch:0 step:604 [D loss: 0.814824, acc.: 56.25%] [G loss: 1.272889]\n",
      "epoch:0 step:605 [D loss: 0.660608, acc.: 60.94%] [G loss: 1.379695]\n",
      "epoch:0 step:606 [D loss: 0.579602, acc.: 65.62%] [G loss: 1.392660]\n",
      "epoch:0 step:607 [D loss: 0.777077, acc.: 50.78%] [G loss: 1.317371]\n",
      "epoch:0 step:608 [D loss: 0.627044, acc.: 60.94%] [G loss: 1.564607]\n",
      "epoch:0 step:609 [D loss: 0.648570, acc.: 57.03%] [G loss: 1.547366]\n",
      "epoch:0 step:610 [D loss: 0.692460, acc.: 58.59%] [G loss: 1.225544]\n",
      "epoch:0 step:611 [D loss: 0.613520, acc.: 68.75%] [G loss: 1.436310]\n",
      "epoch:0 step:612 [D loss: 0.611797, acc.: 67.19%] [G loss: 1.441928]\n",
      "epoch:0 step:613 [D loss: 0.553651, acc.: 73.44%] [G loss: 1.355088]\n",
      "epoch:0 step:614 [D loss: 0.505758, acc.: 71.09%] [G loss: 1.427763]\n",
      "epoch:0 step:615 [D loss: 0.497484, acc.: 75.00%] [G loss: 1.369405]\n",
      "epoch:0 step:616 [D loss: 0.631936, acc.: 64.06%] [G loss: 1.418192]\n",
      "epoch:0 step:617 [D loss: 0.726217, acc.: 59.38%] [G loss: 1.574649]\n",
      "epoch:0 step:618 [D loss: 0.582235, acc.: 75.00%] [G loss: 1.707429]\n",
      "epoch:0 step:619 [D loss: 0.628247, acc.: 64.84%] [G loss: 1.460188]\n",
      "epoch:0 step:620 [D loss: 0.654221, acc.: 60.94%] [G loss: 1.307086]\n",
      "epoch:0 step:621 [D loss: 0.646365, acc.: 67.97%] [G loss: 1.738508]\n",
      "epoch:0 step:622 [D loss: 0.454180, acc.: 84.38%] [G loss: 1.600665]\n",
      "epoch:0 step:623 [D loss: 0.649572, acc.: 61.72%] [G loss: 1.361634]\n",
      "epoch:0 step:624 [D loss: 0.597564, acc.: 64.06%] [G loss: 1.496129]\n",
      "epoch:0 step:625 [D loss: 0.588762, acc.: 67.19%] [G loss: 1.373850]\n",
      "epoch:0 step:626 [D loss: 0.582876, acc.: 68.75%] [G loss: 1.491953]\n",
      "epoch:0 step:627 [D loss: 0.587160, acc.: 69.53%] [G loss: 1.310143]\n",
      "epoch:0 step:628 [D loss: 0.533026, acc.: 74.22%] [G loss: 1.526232]\n",
      "epoch:0 step:629 [D loss: 0.426050, acc.: 81.25%] [G loss: 1.381512]\n",
      "epoch:0 step:630 [D loss: 0.735800, acc.: 57.03%] [G loss: 1.243338]\n",
      "epoch:0 step:631 [D loss: 0.597147, acc.: 67.97%] [G loss: 1.238244]\n",
      "epoch:0 step:632 [D loss: 0.603929, acc.: 64.06%] [G loss: 1.251897]\n",
      "epoch:0 step:633 [D loss: 0.426840, acc.: 79.69%] [G loss: 1.397467]\n",
      "epoch:0 step:634 [D loss: 0.588587, acc.: 64.84%] [G loss: 1.223783]\n",
      "epoch:0 step:635 [D loss: 0.701526, acc.: 57.81%] [G loss: 1.345159]\n",
      "epoch:0 step:636 [D loss: 0.546384, acc.: 67.97%] [G loss: 1.526610]\n",
      "epoch:0 step:637 [D loss: 0.744281, acc.: 59.38%] [G loss: 1.504447]\n",
      "epoch:0 step:638 [D loss: 0.648059, acc.: 64.06%] [G loss: 1.369543]\n",
      "epoch:0 step:639 [D loss: 0.711173, acc.: 57.03%] [G loss: 1.447551]\n",
      "epoch:0 step:640 [D loss: 0.637123, acc.: 69.53%] [G loss: 1.651707]\n",
      "epoch:0 step:641 [D loss: 0.712537, acc.: 53.12%] [G loss: 1.261750]\n",
      "epoch:0 step:642 [D loss: 0.499469, acc.: 72.66%] [G loss: 1.594482]\n",
      "epoch:0 step:643 [D loss: 0.595686, acc.: 73.44%] [G loss: 1.730478]\n",
      "epoch:0 step:644 [D loss: 0.730289, acc.: 57.03%] [G loss: 1.653299]\n",
      "epoch:0 step:645 [D loss: 0.689334, acc.: 61.72%] [G loss: 1.399926]\n",
      "epoch:0 step:646 [D loss: 0.664882, acc.: 62.50%] [G loss: 1.613620]\n",
      "epoch:0 step:647 [D loss: 0.704566, acc.: 60.16%] [G loss: 1.259459]\n",
      "epoch:0 step:648 [D loss: 0.640475, acc.: 64.06%] [G loss: 1.337668]\n",
      "epoch:0 step:649 [D loss: 0.757500, acc.: 59.38%] [G loss: 1.328824]\n",
      "epoch:0 step:650 [D loss: 0.635327, acc.: 60.94%] [G loss: 1.361033]\n",
      "epoch:0 step:651 [D loss: 0.623277, acc.: 64.84%] [G loss: 1.585579]\n",
      "epoch:0 step:652 [D loss: 0.681351, acc.: 57.81%] [G loss: 1.361469]\n",
      "epoch:0 step:653 [D loss: 0.575521, acc.: 71.88%] [G loss: 1.398095]\n",
      "epoch:0 step:654 [D loss: 0.603543, acc.: 64.84%] [G loss: 1.411299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:655 [D loss: 0.693521, acc.: 53.91%] [G loss: 1.311626]\n",
      "epoch:0 step:656 [D loss: 0.702325, acc.: 57.81%] [G loss: 1.496184]\n",
      "epoch:0 step:657 [D loss: 0.657368, acc.: 64.84%] [G loss: 1.268547]\n",
      "epoch:0 step:658 [D loss: 0.579068, acc.: 70.31%] [G loss: 1.502429]\n",
      "epoch:0 step:659 [D loss: 0.621040, acc.: 65.62%] [G loss: 1.657943]\n",
      "epoch:0 step:660 [D loss: 0.558305, acc.: 73.44%] [G loss: 1.384505]\n",
      "epoch:0 step:661 [D loss: 0.723339, acc.: 54.69%] [G loss: 1.360783]\n",
      "epoch:0 step:662 [D loss: 0.650979, acc.: 65.62%] [G loss: 1.253853]\n",
      "epoch:0 step:663 [D loss: 0.659710, acc.: 64.84%] [G loss: 1.261169]\n",
      "epoch:0 step:664 [D loss: 0.738724, acc.: 51.56%] [G loss: 1.332422]\n",
      "epoch:0 step:665 [D loss: 0.783541, acc.: 54.69%] [G loss: 1.303951]\n",
      "epoch:0 step:666 [D loss: 0.545781, acc.: 71.09%] [G loss: 1.463467]\n",
      "epoch:0 step:667 [D loss: 0.524020, acc.: 75.78%] [G loss: 1.367714]\n",
      "epoch:0 step:668 [D loss: 0.868368, acc.: 52.34%] [G loss: 1.242100]\n",
      "epoch:0 step:669 [D loss: 0.586324, acc.: 69.53%] [G loss: 1.368897]\n",
      "epoch:0 step:670 [D loss: 0.638801, acc.: 67.19%] [G loss: 1.598193]\n",
      "epoch:0 step:671 [D loss: 0.606760, acc.: 68.75%] [G loss: 1.597391]\n",
      "epoch:0 step:672 [D loss: 0.733060, acc.: 59.38%] [G loss: 1.414566]\n",
      "epoch:0 step:673 [D loss: 0.595675, acc.: 71.88%] [G loss: 1.259640]\n",
      "epoch:0 step:674 [D loss: 0.851657, acc.: 48.44%] [G loss: 1.151748]\n",
      "epoch:0 step:675 [D loss: 0.692762, acc.: 61.72%] [G loss: 1.469856]\n",
      "epoch:0 step:676 [D loss: 0.743130, acc.: 56.25%] [G loss: 1.257340]\n",
      "epoch:0 step:677 [D loss: 0.626505, acc.: 65.62%] [G loss: 1.436280]\n",
      "epoch:0 step:678 [D loss: 0.625410, acc.: 64.84%] [G loss: 1.325742]\n",
      "epoch:0 step:679 [D loss: 0.716045, acc.: 60.16%] [G loss: 1.373711]\n",
      "epoch:0 step:680 [D loss: 0.593351, acc.: 70.31%] [G loss: 1.275154]\n",
      "epoch:0 step:681 [D loss: 0.527910, acc.: 75.78%] [G loss: 1.452474]\n",
      "epoch:0 step:682 [D loss: 0.681288, acc.: 64.06%] [G loss: 1.170400]\n",
      "epoch:0 step:683 [D loss: 0.584097, acc.: 67.97%] [G loss: 1.416981]\n",
      "epoch:0 step:684 [D loss: 0.539823, acc.: 73.44%] [G loss: 1.524073]\n",
      "epoch:0 step:685 [D loss: 0.570963, acc.: 69.53%] [G loss: 1.432193]\n",
      "epoch:0 step:686 [D loss: 0.574956, acc.: 69.53%] [G loss: 1.402597]\n",
      "epoch:0 step:687 [D loss: 0.632266, acc.: 62.50%] [G loss: 1.330294]\n",
      "epoch:0 step:688 [D loss: 0.647603, acc.: 65.62%] [G loss: 1.510493]\n",
      "epoch:0 step:689 [D loss: 0.772335, acc.: 50.78%] [G loss: 1.183725]\n",
      "epoch:0 step:690 [D loss: 0.619690, acc.: 67.97%] [G loss: 1.635129]\n",
      "epoch:0 step:691 [D loss: 0.684260, acc.: 59.38%] [G loss: 1.416965]\n",
      "epoch:0 step:692 [D loss: 0.657620, acc.: 63.28%] [G loss: 1.365946]\n",
      "epoch:0 step:693 [D loss: 0.589553, acc.: 69.53%] [G loss: 1.573765]\n",
      "epoch:0 step:694 [D loss: 0.732427, acc.: 57.03%] [G loss: 1.388811]\n",
      "epoch:0 step:695 [D loss: 0.596389, acc.: 68.75%] [G loss: 1.390204]\n",
      "epoch:0 step:696 [D loss: 0.760636, acc.: 55.47%] [G loss: 1.315025]\n",
      "epoch:0 step:697 [D loss: 0.593122, acc.: 66.41%] [G loss: 1.353276]\n",
      "epoch:0 step:698 [D loss: 0.600901, acc.: 72.66%] [G loss: 1.499603]\n",
      "epoch:0 step:699 [D loss: 0.686577, acc.: 60.94%] [G loss: 1.180819]\n",
      "epoch:0 step:700 [D loss: 0.599615, acc.: 64.84%] [G loss: 1.417141]\n",
      "epoch:0 step:701 [D loss: 0.509468, acc.: 75.00%] [G loss: 1.433325]\n",
      "epoch:0 step:702 [D loss: 0.654874, acc.: 61.72%] [G loss: 1.353321]\n",
      "epoch:0 step:703 [D loss: 0.658152, acc.: 62.50%] [G loss: 1.395113]\n",
      "epoch:0 step:704 [D loss: 0.771715, acc.: 55.47%] [G loss: 1.276111]\n",
      "epoch:0 step:705 [D loss: 0.723462, acc.: 57.81%] [G loss: 1.265701]\n",
      "epoch:0 step:706 [D loss: 0.681318, acc.: 54.69%] [G loss: 1.297828]\n",
      "epoch:0 step:707 [D loss: 0.580635, acc.: 66.41%] [G loss: 1.493535]\n",
      "epoch:0 step:708 [D loss: 0.626574, acc.: 68.75%] [G loss: 1.377607]\n",
      "epoch:0 step:709 [D loss: 0.727833, acc.: 59.38%] [G loss: 1.275782]\n",
      "epoch:0 step:710 [D loss: 0.698538, acc.: 53.91%] [G loss: 1.400501]\n",
      "epoch:0 step:711 [D loss: 0.633596, acc.: 65.62%] [G loss: 1.399659]\n",
      "epoch:0 step:712 [D loss: 0.599127, acc.: 64.06%] [G loss: 1.752985]\n",
      "epoch:0 step:713 [D loss: 0.695119, acc.: 56.25%] [G loss: 1.184665]\n",
      "epoch:0 step:714 [D loss: 0.751020, acc.: 59.38%] [G loss: 1.211604]\n",
      "epoch:0 step:715 [D loss: 0.676391, acc.: 65.62%] [G loss: 1.352694]\n",
      "epoch:0 step:716 [D loss: 0.667513, acc.: 61.72%] [G loss: 1.251459]\n",
      "epoch:0 step:717 [D loss: 0.554426, acc.: 69.53%] [G loss: 1.609450]\n",
      "epoch:0 step:718 [D loss: 0.538246, acc.: 72.66%] [G loss: 1.446435]\n",
      "epoch:0 step:719 [D loss: 0.573798, acc.: 71.09%] [G loss: 1.326916]\n",
      "epoch:0 step:720 [D loss: 0.676572, acc.: 57.03%] [G loss: 1.464484]\n",
      "epoch:0 step:721 [D loss: 0.630855, acc.: 62.50%] [G loss: 1.343840]\n",
      "epoch:0 step:722 [D loss: 0.610341, acc.: 65.62%] [G loss: 1.461463]\n",
      "epoch:0 step:723 [D loss: 0.692565, acc.: 62.50%] [G loss: 1.252633]\n",
      "epoch:0 step:724 [D loss: 0.687249, acc.: 59.38%] [G loss: 1.175465]\n",
      "epoch:0 step:725 [D loss: 0.563841, acc.: 71.88%] [G loss: 1.265103]\n",
      "epoch:0 step:726 [D loss: 0.567100, acc.: 71.88%] [G loss: 1.204761]\n",
      "epoch:0 step:727 [D loss: 0.600384, acc.: 72.66%] [G loss: 1.418258]\n",
      "epoch:0 step:728 [D loss: 0.739746, acc.: 53.91%] [G loss: 1.111501]\n",
      "epoch:0 step:729 [D loss: 0.565187, acc.: 67.19%] [G loss: 1.222426]\n",
      "epoch:0 step:730 [D loss: 0.588683, acc.: 71.88%] [G loss: 1.276795]\n",
      "epoch:0 step:731 [D loss: 0.705912, acc.: 57.03%] [G loss: 1.136810]\n",
      "epoch:0 step:732 [D loss: 0.630632, acc.: 67.19%] [G loss: 1.071089]\n",
      "epoch:0 step:733 [D loss: 0.657747, acc.: 59.38%] [G loss: 1.396250]\n",
      "epoch:0 step:734 [D loss: 0.476020, acc.: 81.25%] [G loss: 1.473696]\n",
      "epoch:0 step:735 [D loss: 0.509161, acc.: 76.56%] [G loss: 1.335316]\n",
      "epoch:0 step:736 [D loss: 0.712933, acc.: 57.03%] [G loss: 1.280455]\n",
      "epoch:0 step:737 [D loss: 0.659455, acc.: 58.59%] [G loss: 1.420280]\n",
      "epoch:0 step:738 [D loss: 0.622086, acc.: 67.97%] [G loss: 1.181746]\n",
      "epoch:0 step:739 [D loss: 0.786713, acc.: 53.12%] [G loss: 1.081248]\n",
      "epoch:0 step:740 [D loss: 0.514313, acc.: 76.56%] [G loss: 1.218316]\n",
      "epoch:0 step:741 [D loss: 0.582209, acc.: 68.75%] [G loss: 1.429325]\n",
      "epoch:0 step:742 [D loss: 0.685443, acc.: 60.94%] [G loss: 1.370355]\n",
      "epoch:0 step:743 [D loss: 0.710499, acc.: 53.12%] [G loss: 1.161219]\n",
      "epoch:0 step:744 [D loss: 0.662089, acc.: 60.94%] [G loss: 1.307450]\n",
      "epoch:0 step:745 [D loss: 0.582519, acc.: 69.53%] [G loss: 1.318088]\n",
      "epoch:0 step:746 [D loss: 0.621010, acc.: 64.84%] [G loss: 1.446760]\n",
      "epoch:0 step:747 [D loss: 0.782434, acc.: 53.91%] [G loss: 1.247748]\n",
      "epoch:0 step:748 [D loss: 0.640599, acc.: 63.28%] [G loss: 1.381951]\n",
      "epoch:0 step:749 [D loss: 0.714529, acc.: 60.16%] [G loss: 1.309916]\n",
      "epoch:0 step:750 [D loss: 0.695131, acc.: 58.59%] [G loss: 1.323152]\n",
      "epoch:0 step:751 [D loss: 0.679581, acc.: 62.50%] [G loss: 1.549572]\n",
      "epoch:0 step:752 [D loss: 0.605886, acc.: 70.31%] [G loss: 1.566734]\n",
      "epoch:0 step:753 [D loss: 0.594853, acc.: 69.53%] [G loss: 1.500953]\n",
      "epoch:0 step:754 [D loss: 0.750213, acc.: 57.03%] [G loss: 1.143654]\n",
      "epoch:0 step:755 [D loss: 0.578937, acc.: 72.66%] [G loss: 1.394031]\n",
      "epoch:0 step:756 [D loss: 0.724591, acc.: 59.38%] [G loss: 1.224261]\n",
      "epoch:0 step:757 [D loss: 0.679050, acc.: 60.16%] [G loss: 1.373128]\n",
      "epoch:0 step:758 [D loss: 0.619522, acc.: 69.53%] [G loss: 1.291748]\n",
      "epoch:0 step:759 [D loss: 0.618684, acc.: 67.97%] [G loss: 1.443719]\n",
      "epoch:0 step:760 [D loss: 0.674922, acc.: 63.28%] [G loss: 1.224700]\n",
      "epoch:0 step:761 [D loss: 0.582019, acc.: 66.41%] [G loss: 1.214195]\n",
      "epoch:0 step:762 [D loss: 0.604157, acc.: 61.72%] [G loss: 1.427060]\n",
      "epoch:0 step:763 [D loss: 0.586718, acc.: 71.09%] [G loss: 1.417522]\n",
      "epoch:0 step:764 [D loss: 0.716334, acc.: 55.47%] [G loss: 1.388354]\n",
      "epoch:0 step:765 [D loss: 0.600507, acc.: 66.41%] [G loss: 1.579487]\n",
      "epoch:0 step:766 [D loss: 0.685233, acc.: 60.16%] [G loss: 1.463509]\n",
      "epoch:0 step:767 [D loss: 0.615620, acc.: 64.84%] [G loss: 1.513521]\n",
      "epoch:0 step:768 [D loss: 0.612903, acc.: 64.84%] [G loss: 1.447840]\n",
      "epoch:0 step:769 [D loss: 0.596272, acc.: 68.75%] [G loss: 1.178136]\n",
      "epoch:0 step:770 [D loss: 0.547448, acc.: 73.44%] [G loss: 1.416429]\n",
      "epoch:0 step:771 [D loss: 0.628006, acc.: 60.94%] [G loss: 1.071065]\n",
      "epoch:0 step:772 [D loss: 0.559326, acc.: 69.53%] [G loss: 1.466766]\n",
      "epoch:0 step:773 [D loss: 0.668686, acc.: 61.72%] [G loss: 1.257456]\n",
      "epoch:0 step:774 [D loss: 0.551195, acc.: 69.53%] [G loss: 1.346619]\n",
      "epoch:0 step:775 [D loss: 0.639255, acc.: 64.06%] [G loss: 1.257062]\n",
      "epoch:0 step:776 [D loss: 0.623364, acc.: 63.28%] [G loss: 1.242604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:777 [D loss: 0.720902, acc.: 55.47%] [G loss: 1.269032]\n",
      "epoch:0 step:778 [D loss: 0.556035, acc.: 69.53%] [G loss: 1.467018]\n",
      "epoch:0 step:779 [D loss: 0.578100, acc.: 71.09%] [G loss: 1.378049]\n",
      "epoch:0 step:780 [D loss: 0.705912, acc.: 60.94%] [G loss: 1.123511]\n",
      "epoch:0 step:781 [D loss: 0.645080, acc.: 61.72%] [G loss: 1.413385]\n",
      "epoch:0 step:782 [D loss: 0.562650, acc.: 69.53%] [G loss: 1.178392]\n",
      "epoch:0 step:783 [D loss: 0.725153, acc.: 53.12%] [G loss: 1.145836]\n",
      "epoch:0 step:784 [D loss: 0.644551, acc.: 63.28%] [G loss: 1.304637]\n",
      "epoch:0 step:785 [D loss: 0.776463, acc.: 51.56%] [G loss: 1.227802]\n",
      "epoch:0 step:786 [D loss: 0.758388, acc.: 53.91%] [G loss: 1.082309]\n",
      "epoch:0 step:787 [D loss: 0.633784, acc.: 62.50%] [G loss: 1.250617]\n",
      "epoch:0 step:788 [D loss: 0.699577, acc.: 59.38%] [G loss: 1.436475]\n",
      "epoch:0 step:789 [D loss: 0.638098, acc.: 61.72%] [G loss: 1.297849]\n",
      "epoch:0 step:790 [D loss: 0.490794, acc.: 75.78%] [G loss: 1.570971]\n",
      "epoch:0 step:791 [D loss: 0.649676, acc.: 62.50%] [G loss: 1.321589]\n",
      "epoch:0 step:792 [D loss: 0.577554, acc.: 66.41%] [G loss: 1.295549]\n",
      "epoch:0 step:793 [D loss: 0.672914, acc.: 58.59%] [G loss: 1.132935]\n",
      "epoch:0 step:794 [D loss: 0.577993, acc.: 70.31%] [G loss: 1.417966]\n",
      "epoch:0 step:795 [D loss: 0.604942, acc.: 66.41%] [G loss: 1.436683]\n",
      "epoch:0 step:796 [D loss: 0.624560, acc.: 67.19%] [G loss: 1.280601]\n",
      "epoch:0 step:797 [D loss: 0.699460, acc.: 58.59%] [G loss: 1.420062]\n",
      "epoch:0 step:798 [D loss: 0.685155, acc.: 59.38%] [G loss: 1.381404]\n",
      "epoch:0 step:799 [D loss: 0.534571, acc.: 75.00%] [G loss: 1.343855]\n",
      "epoch:0 step:800 [D loss: 0.694065, acc.: 61.72%] [G loss: 1.130864]\n",
      "##############\n",
      "[2.57119813 2.052602   1.75444829 3.78835166 0.95875896 7.20448345\n",
      " 2.19613388 3.10993743 3.91215218 5.10587101]\n",
      "##########\n",
      "epoch:0 step:801 [D loss: 0.662731, acc.: 63.28%] [G loss: 1.119398]\n",
      "epoch:0 step:802 [D loss: 0.679349, acc.: 60.94%] [G loss: 1.362800]\n",
      "epoch:0 step:803 [D loss: 0.612883, acc.: 63.28%] [G loss: 1.359929]\n",
      "epoch:0 step:804 [D loss: 0.640608, acc.: 65.62%] [G loss: 1.194117]\n",
      "epoch:0 step:805 [D loss: 0.674321, acc.: 64.06%] [G loss: 1.276963]\n",
      "epoch:0 step:806 [D loss: 0.616069, acc.: 64.84%] [G loss: 1.427650]\n",
      "epoch:0 step:807 [D loss: 0.638488, acc.: 60.94%] [G loss: 1.494881]\n",
      "epoch:0 step:808 [D loss: 0.697287, acc.: 65.62%] [G loss: 1.274535]\n",
      "epoch:0 step:809 [D loss: 0.641850, acc.: 70.31%] [G loss: 1.347243]\n",
      "epoch:0 step:810 [D loss: 0.532882, acc.: 76.56%] [G loss: 1.102086]\n",
      "epoch:0 step:811 [D loss: 0.637951, acc.: 67.19%] [G loss: 1.380665]\n",
      "epoch:0 step:812 [D loss: 0.679939, acc.: 60.16%] [G loss: 1.411386]\n",
      "epoch:0 step:813 [D loss: 0.606872, acc.: 70.31%] [G loss: 1.291631]\n",
      "epoch:0 step:814 [D loss: 0.633219, acc.: 62.50%] [G loss: 1.329261]\n",
      "epoch:0 step:815 [D loss: 0.695177, acc.: 54.69%] [G loss: 1.332323]\n",
      "epoch:0 step:816 [D loss: 0.612256, acc.: 63.28%] [G loss: 1.498758]\n",
      "epoch:0 step:817 [D loss: 0.638455, acc.: 65.62%] [G loss: 1.259308]\n",
      "epoch:0 step:818 [D loss: 0.711798, acc.: 53.91%] [G loss: 1.399276]\n",
      "epoch:0 step:819 [D loss: 0.678272, acc.: 59.38%] [G loss: 1.109687]\n",
      "epoch:0 step:820 [D loss: 0.631977, acc.: 64.84%] [G loss: 1.480395]\n",
      "epoch:0 step:821 [D loss: 0.632003, acc.: 63.28%] [G loss: 1.271346]\n",
      "epoch:0 step:822 [D loss: 0.715207, acc.: 52.34%] [G loss: 1.400053]\n",
      "epoch:0 step:823 [D loss: 0.670538, acc.: 57.03%] [G loss: 1.198351]\n",
      "epoch:0 step:824 [D loss: 0.701595, acc.: 61.72%] [G loss: 1.162632]\n",
      "epoch:0 step:825 [D loss: 0.642525, acc.: 58.59%] [G loss: 1.485276]\n",
      "epoch:0 step:826 [D loss: 0.577936, acc.: 70.31%] [G loss: 1.331901]\n",
      "epoch:0 step:827 [D loss: 0.638371, acc.: 64.84%] [G loss: 1.163201]\n",
      "epoch:0 step:828 [D loss: 0.752912, acc.: 55.47%] [G loss: 1.217048]\n",
      "epoch:0 step:829 [D loss: 0.600964, acc.: 66.41%] [G loss: 1.443409]\n",
      "epoch:0 step:830 [D loss: 0.637791, acc.: 67.97%] [G loss: 1.271488]\n",
      "epoch:0 step:831 [D loss: 0.689775, acc.: 62.50%] [G loss: 1.294464]\n",
      "epoch:0 step:832 [D loss: 0.590928, acc.: 74.22%] [G loss: 1.387228]\n",
      "epoch:0 step:833 [D loss: 0.707045, acc.: 62.50%] [G loss: 1.177516]\n",
      "epoch:0 step:834 [D loss: 0.708267, acc.: 54.69%] [G loss: 1.307573]\n",
      "epoch:0 step:835 [D loss: 0.530719, acc.: 75.78%] [G loss: 1.065991]\n",
      "epoch:0 step:836 [D loss: 0.586856, acc.: 66.41%] [G loss: 1.267616]\n",
      "epoch:0 step:837 [D loss: 0.623145, acc.: 67.97%] [G loss: 1.289846]\n",
      "epoch:0 step:838 [D loss: 0.785768, acc.: 54.69%] [G loss: 1.178417]\n",
      "epoch:0 step:839 [D loss: 0.669507, acc.: 61.72%] [G loss: 1.291158]\n",
      "epoch:0 step:840 [D loss: 0.668297, acc.: 63.28%] [G loss: 1.334692]\n",
      "epoch:0 step:841 [D loss: 0.583385, acc.: 71.88%] [G loss: 1.360921]\n",
      "epoch:0 step:842 [D loss: 0.575787, acc.: 68.75%] [G loss: 1.141373]\n",
      "epoch:0 step:843 [D loss: 0.617250, acc.: 62.50%] [G loss: 1.327548]\n",
      "epoch:0 step:844 [D loss: 0.707256, acc.: 57.03%] [G loss: 1.300441]\n",
      "epoch:0 step:845 [D loss: 0.626568, acc.: 63.28%] [G loss: 1.220585]\n",
      "epoch:0 step:846 [D loss: 0.659832, acc.: 62.50%] [G loss: 1.404215]\n",
      "epoch:0 step:847 [D loss: 0.592560, acc.: 70.31%] [G loss: 1.299590]\n",
      "epoch:0 step:848 [D loss: 0.668138, acc.: 64.06%] [G loss: 1.147287]\n",
      "epoch:0 step:849 [D loss: 0.591980, acc.: 67.19%] [G loss: 1.340167]\n",
      "epoch:0 step:850 [D loss: 0.636762, acc.: 64.06%] [G loss: 1.293599]\n",
      "epoch:0 step:851 [D loss: 0.586254, acc.: 70.31%] [G loss: 1.338631]\n",
      "epoch:0 step:852 [D loss: 0.582469, acc.: 67.97%] [G loss: 1.384636]\n",
      "epoch:0 step:853 [D loss: 0.638037, acc.: 68.75%] [G loss: 1.528439]\n",
      "epoch:0 step:854 [D loss: 0.596772, acc.: 70.31%] [G loss: 1.161081]\n",
      "epoch:0 step:855 [D loss: 0.741035, acc.: 55.47%] [G loss: 1.116129]\n",
      "epoch:0 step:856 [D loss: 0.568559, acc.: 69.53%] [G loss: 1.308079]\n",
      "epoch:0 step:857 [D loss: 0.651376, acc.: 64.06%] [G loss: 1.536503]\n",
      "epoch:0 step:858 [D loss: 0.739529, acc.: 53.91%] [G loss: 1.375846]\n",
      "epoch:0 step:859 [D loss: 0.542762, acc.: 72.66%] [G loss: 1.388533]\n",
      "epoch:0 step:860 [D loss: 0.600270, acc.: 64.84%] [G loss: 1.465451]\n",
      "epoch:0 step:861 [D loss: 0.643513, acc.: 61.72%] [G loss: 1.270632]\n",
      "epoch:0 step:862 [D loss: 0.715210, acc.: 58.59%] [G loss: 1.060651]\n",
      "epoch:0 step:863 [D loss: 0.587695, acc.: 69.53%] [G loss: 1.159127]\n",
      "epoch:0 step:864 [D loss: 0.659073, acc.: 64.84%] [G loss: 1.261958]\n",
      "epoch:0 step:865 [D loss: 0.547869, acc.: 73.44%] [G loss: 1.473881]\n",
      "epoch:0 step:866 [D loss: 0.679493, acc.: 60.94%] [G loss: 1.312238]\n",
      "epoch:0 step:867 [D loss: 0.602964, acc.: 66.41%] [G loss: 1.281990]\n",
      "epoch:0 step:868 [D loss: 0.742554, acc.: 57.03%] [G loss: 1.091524]\n",
      "epoch:0 step:869 [D loss: 0.623946, acc.: 62.50%] [G loss: 1.177886]\n",
      "epoch:0 step:870 [D loss: 0.712199, acc.: 57.03%] [G loss: 1.275725]\n",
      "epoch:0 step:871 [D loss: 0.669784, acc.: 60.94%] [G loss: 1.270630]\n",
      "epoch:0 step:872 [D loss: 0.663943, acc.: 66.41%] [G loss: 1.014494]\n",
      "epoch:0 step:873 [D loss: 0.670659, acc.: 63.28%] [G loss: 1.097952]\n",
      "epoch:0 step:874 [D loss: 0.610890, acc.: 64.84%] [G loss: 1.137987]\n",
      "epoch:0 step:875 [D loss: 0.677711, acc.: 58.59%] [G loss: 1.331262]\n",
      "epoch:0 step:876 [D loss: 0.615407, acc.: 66.41%] [G loss: 1.597713]\n",
      "epoch:0 step:877 [D loss: 0.613146, acc.: 61.72%] [G loss: 1.523311]\n",
      "epoch:0 step:878 [D loss: 0.709296, acc.: 55.47%] [G loss: 1.256979]\n",
      "epoch:0 step:879 [D loss: 0.564776, acc.: 71.88%] [G loss: 1.234735]\n",
      "epoch:0 step:880 [D loss: 0.551011, acc.: 69.53%] [G loss: 1.270701]\n",
      "epoch:0 step:881 [D loss: 0.725757, acc.: 59.38%] [G loss: 1.147508]\n",
      "epoch:0 step:882 [D loss: 0.631368, acc.: 63.28%] [G loss: 1.295263]\n",
      "epoch:0 step:883 [D loss: 0.658310, acc.: 61.72%] [G loss: 1.122017]\n",
      "epoch:0 step:884 [D loss: 0.703450, acc.: 62.50%] [G loss: 1.212358]\n",
      "epoch:0 step:885 [D loss: 0.554029, acc.: 71.88%] [G loss: 1.398263]\n",
      "epoch:0 step:886 [D loss: 0.721681, acc.: 60.94%] [G loss: 1.237633]\n",
      "epoch:0 step:887 [D loss: 0.635551, acc.: 64.84%] [G loss: 1.234848]\n",
      "epoch:0 step:888 [D loss: 0.683943, acc.: 59.38%] [G loss: 1.369830]\n",
      "epoch:0 step:889 [D loss: 0.554886, acc.: 70.31%] [G loss: 1.191979]\n",
      "epoch:0 step:890 [D loss: 0.641735, acc.: 64.84%] [G loss: 1.253306]\n",
      "epoch:0 step:891 [D loss: 0.685537, acc.: 59.38%] [G loss: 1.089138]\n",
      "epoch:0 step:892 [D loss: 0.694099, acc.: 57.81%] [G loss: 1.145792]\n",
      "epoch:0 step:893 [D loss: 0.740971, acc.: 55.47%] [G loss: 1.106666]\n",
      "epoch:0 step:894 [D loss: 0.682476, acc.: 61.72%] [G loss: 1.197324]\n",
      "epoch:0 step:895 [D loss: 0.600833, acc.: 67.97%] [G loss: 1.217293]\n",
      "epoch:0 step:896 [D loss: 0.594615, acc.: 72.66%] [G loss: 1.364304]\n",
      "epoch:0 step:897 [D loss: 0.675240, acc.: 60.94%] [G loss: 1.224066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:898 [D loss: 0.583252, acc.: 67.19%] [G loss: 1.276740]\n",
      "epoch:0 step:899 [D loss: 0.742567, acc.: 56.25%] [G loss: 1.269971]\n",
      "epoch:0 step:900 [D loss: 0.665257, acc.: 60.16%] [G loss: 1.170449]\n",
      "epoch:0 step:901 [D loss: 0.654517, acc.: 58.59%] [G loss: 1.050726]\n",
      "epoch:0 step:902 [D loss: 0.658435, acc.: 61.72%] [G loss: 1.210215]\n",
      "epoch:0 step:903 [D loss: 0.524784, acc.: 75.00%] [G loss: 1.368029]\n",
      "epoch:0 step:904 [D loss: 0.540085, acc.: 75.00%] [G loss: 1.222963]\n",
      "epoch:0 step:905 [D loss: 0.503989, acc.: 73.44%] [G loss: 1.297124]\n",
      "epoch:0 step:906 [D loss: 0.594683, acc.: 65.62%] [G loss: 1.102031]\n",
      "epoch:0 step:907 [D loss: 0.759918, acc.: 53.91%] [G loss: 1.118221]\n",
      "epoch:0 step:908 [D loss: 0.653155, acc.: 60.16%] [G loss: 1.249916]\n",
      "epoch:0 step:909 [D loss: 0.707096, acc.: 54.69%] [G loss: 1.210769]\n",
      "epoch:0 step:910 [D loss: 0.607828, acc.: 67.97%] [G loss: 1.170110]\n",
      "epoch:0 step:911 [D loss: 0.661873, acc.: 65.62%] [G loss: 1.062445]\n",
      "epoch:0 step:912 [D loss: 0.531382, acc.: 73.44%] [G loss: 1.323972]\n",
      "epoch:0 step:913 [D loss: 0.710931, acc.: 58.59%] [G loss: 1.115923]\n",
      "epoch:0 step:914 [D loss: 0.613916, acc.: 64.06%] [G loss: 1.311277]\n",
      "epoch:0 step:915 [D loss: 0.699216, acc.: 66.41%] [G loss: 1.244833]\n",
      "epoch:0 step:916 [D loss: 0.571851, acc.: 70.31%] [G loss: 1.151900]\n",
      "epoch:0 step:917 [D loss: 0.669367, acc.: 60.94%] [G loss: 1.192811]\n",
      "epoch:0 step:918 [D loss: 0.600777, acc.: 67.19%] [G loss: 1.297477]\n",
      "epoch:0 step:919 [D loss: 0.608869, acc.: 67.19%] [G loss: 1.362595]\n",
      "epoch:0 step:920 [D loss: 0.598306, acc.: 67.97%] [G loss: 1.540331]\n",
      "epoch:0 step:921 [D loss: 0.496201, acc.: 76.56%] [G loss: 1.098527]\n",
      "epoch:0 step:922 [D loss: 0.583353, acc.: 69.53%] [G loss: 1.154159]\n",
      "epoch:0 step:923 [D loss: 0.576818, acc.: 71.09%] [G loss: 1.214748]\n",
      "epoch:0 step:924 [D loss: 0.717857, acc.: 52.34%] [G loss: 1.355564]\n",
      "epoch:0 step:925 [D loss: 0.605205, acc.: 64.06%] [G loss: 1.588458]\n",
      "epoch:0 step:926 [D loss: 0.590745, acc.: 71.09%] [G loss: 1.186518]\n",
      "epoch:0 step:927 [D loss: 0.628533, acc.: 64.84%] [G loss: 1.227328]\n",
      "epoch:0 step:928 [D loss: 0.651593, acc.: 65.62%] [G loss: 1.265748]\n",
      "epoch:0 step:929 [D loss: 0.564084, acc.: 71.09%] [G loss: 1.447766]\n",
      "epoch:0 step:930 [D loss: 0.584284, acc.: 68.75%] [G loss: 1.292776]\n",
      "epoch:0 step:931 [D loss: 0.632748, acc.: 64.84%] [G loss: 1.270691]\n",
      "epoch:0 step:932 [D loss: 0.665088, acc.: 57.81%] [G loss: 1.154887]\n",
      "epoch:0 step:933 [D loss: 0.662103, acc.: 57.81%] [G loss: 1.219847]\n",
      "epoch:0 step:934 [D loss: 0.600960, acc.: 72.66%] [G loss: 1.283649]\n",
      "epoch:0 step:935 [D loss: 0.730418, acc.: 55.47%] [G loss: 1.331645]\n",
      "epoch:0 step:936 [D loss: 0.578755, acc.: 69.53%] [G loss: 1.147956]\n",
      "epoch:0 step:937 [D loss: 0.668706, acc.: 64.84%] [G loss: 1.199223]\n",
      "epoch:1 step:938 [D loss: 0.631362, acc.: 62.50%] [G loss: 1.326842]\n",
      "epoch:1 step:939 [D loss: 0.624493, acc.: 64.84%] [G loss: 1.487017]\n",
      "epoch:1 step:940 [D loss: 0.569724, acc.: 69.53%] [G loss: 1.468021]\n",
      "epoch:1 step:941 [D loss: 0.641405, acc.: 62.50%] [G loss: 1.390271]\n",
      "epoch:1 step:942 [D loss: 0.615966, acc.: 62.50%] [G loss: 1.191278]\n",
      "epoch:1 step:943 [D loss: 0.669354, acc.: 62.50%] [G loss: 1.308014]\n",
      "epoch:1 step:944 [D loss: 0.576974, acc.: 68.75%] [G loss: 1.212987]\n",
      "epoch:1 step:945 [D loss: 0.639077, acc.: 59.38%] [G loss: 1.363509]\n",
      "epoch:1 step:946 [D loss: 0.673306, acc.: 60.16%] [G loss: 1.167446]\n",
      "epoch:1 step:947 [D loss: 0.604972, acc.: 68.75%] [G loss: 1.261762]\n",
      "epoch:1 step:948 [D loss: 0.633647, acc.: 62.50%] [G loss: 1.362639]\n",
      "epoch:1 step:949 [D loss: 0.542137, acc.: 75.78%] [G loss: 1.389796]\n",
      "epoch:1 step:950 [D loss: 0.629189, acc.: 67.97%] [G loss: 1.381747]\n",
      "epoch:1 step:951 [D loss: 0.605424, acc.: 67.97%] [G loss: 1.282536]\n",
      "epoch:1 step:952 [D loss: 0.686181, acc.: 55.47%] [G loss: 1.210620]\n",
      "epoch:1 step:953 [D loss: 0.646774, acc.: 65.62%] [G loss: 1.215714]\n",
      "epoch:1 step:954 [D loss: 0.561569, acc.: 67.19%] [G loss: 1.297317]\n",
      "epoch:1 step:955 [D loss: 0.638728, acc.: 64.84%] [G loss: 1.294439]\n",
      "epoch:1 step:956 [D loss: 0.685691, acc.: 64.84%] [G loss: 1.277313]\n",
      "epoch:1 step:957 [D loss: 0.692353, acc.: 63.28%] [G loss: 1.280504]\n",
      "epoch:1 step:958 [D loss: 0.750203, acc.: 53.12%] [G loss: 1.247506]\n",
      "epoch:1 step:959 [D loss: 0.622956, acc.: 69.53%] [G loss: 1.339728]\n",
      "epoch:1 step:960 [D loss: 0.665313, acc.: 61.72%] [G loss: 1.198325]\n",
      "epoch:1 step:961 [D loss: 0.636430, acc.: 65.62%] [G loss: 1.169246]\n",
      "epoch:1 step:962 [D loss: 0.539269, acc.: 70.31%] [G loss: 1.500335]\n",
      "epoch:1 step:963 [D loss: 0.649204, acc.: 63.28%] [G loss: 1.274520]\n",
      "epoch:1 step:964 [D loss: 0.568835, acc.: 69.53%] [G loss: 1.362631]\n",
      "epoch:1 step:965 [D loss: 0.649462, acc.: 62.50%] [G loss: 1.312275]\n",
      "epoch:1 step:966 [D loss: 0.643286, acc.: 67.19%] [G loss: 1.229735]\n",
      "epoch:1 step:967 [D loss: 0.619645, acc.: 64.06%] [G loss: 1.313036]\n",
      "epoch:1 step:968 [D loss: 0.765034, acc.: 47.66%] [G loss: 1.158845]\n",
      "epoch:1 step:969 [D loss: 0.613986, acc.: 70.31%] [G loss: 1.247667]\n",
      "epoch:1 step:970 [D loss: 0.693186, acc.: 59.38%] [G loss: 1.107792]\n",
      "epoch:1 step:971 [D loss: 0.682635, acc.: 60.94%] [G loss: 1.201802]\n",
      "epoch:1 step:972 [D loss: 0.570770, acc.: 69.53%] [G loss: 1.264355]\n",
      "epoch:1 step:973 [D loss: 0.572850, acc.: 69.53%] [G loss: 1.251476]\n",
      "epoch:1 step:974 [D loss: 0.561082, acc.: 67.19%] [G loss: 1.535515]\n",
      "epoch:1 step:975 [D loss: 0.769311, acc.: 49.22%] [G loss: 1.146269]\n",
      "epoch:1 step:976 [D loss: 0.663632, acc.: 61.72%] [G loss: 1.425182]\n",
      "epoch:1 step:977 [D loss: 0.628520, acc.: 67.19%] [G loss: 1.419182]\n",
      "epoch:1 step:978 [D loss: 0.643604, acc.: 66.41%] [G loss: 1.253450]\n",
      "epoch:1 step:979 [D loss: 0.640337, acc.: 60.16%] [G loss: 1.363449]\n",
      "epoch:1 step:980 [D loss: 0.683189, acc.: 56.25%] [G loss: 1.074747]\n",
      "epoch:1 step:981 [D loss: 0.664690, acc.: 60.94%] [G loss: 1.297758]\n",
      "epoch:1 step:982 [D loss: 0.667923, acc.: 63.28%] [G loss: 1.189218]\n",
      "epoch:1 step:983 [D loss: 0.642060, acc.: 67.19%] [G loss: 1.243850]\n",
      "epoch:1 step:984 [D loss: 0.670906, acc.: 64.06%] [G loss: 1.256388]\n",
      "epoch:1 step:985 [D loss: 0.681446, acc.: 57.81%] [G loss: 1.282827]\n",
      "epoch:1 step:986 [D loss: 0.619424, acc.: 65.62%] [G loss: 1.565795]\n",
      "epoch:1 step:987 [D loss: 0.538999, acc.: 71.88%] [G loss: 1.233926]\n",
      "epoch:1 step:988 [D loss: 0.704564, acc.: 57.03%] [G loss: 1.318484]\n",
      "epoch:1 step:989 [D loss: 0.666930, acc.: 63.28%] [G loss: 1.311634]\n",
      "epoch:1 step:990 [D loss: 0.641374, acc.: 66.41%] [G loss: 1.507225]\n",
      "epoch:1 step:991 [D loss: 0.557116, acc.: 74.22%] [G loss: 1.363258]\n",
      "epoch:1 step:992 [D loss: 0.658159, acc.: 66.41%] [G loss: 1.289038]\n",
      "epoch:1 step:993 [D loss: 0.607487, acc.: 63.28%] [G loss: 1.316079]\n",
      "epoch:1 step:994 [D loss: 0.686551, acc.: 57.81%] [G loss: 1.283298]\n",
      "epoch:1 step:995 [D loss: 0.570392, acc.: 69.53%] [G loss: 1.414173]\n",
      "epoch:1 step:996 [D loss: 0.606352, acc.: 65.62%] [G loss: 1.380171]\n",
      "epoch:1 step:997 [D loss: 0.628547, acc.: 61.72%] [G loss: 1.391243]\n",
      "epoch:1 step:998 [D loss: 0.659042, acc.: 62.50%] [G loss: 1.377534]\n",
      "epoch:1 step:999 [D loss: 0.623803, acc.: 65.62%] [G loss: 1.244849]\n",
      "epoch:1 step:1000 [D loss: 0.589702, acc.: 64.84%] [G loss: 1.220432]\n",
      "##############\n",
      "[2.44264479 2.10400085 1.4195185  3.13626636 0.97861364 6.6081615\n",
      " 2.08172557 3.02387763 3.60798835 7.14868929]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.706840, acc.: 59.38%] [G loss: 1.231876]\n",
      "epoch:1 step:1002 [D loss: 0.545101, acc.: 71.88%] [G loss: 1.315999]\n",
      "epoch:1 step:1003 [D loss: 0.540675, acc.: 75.00%] [G loss: 1.241294]\n",
      "epoch:1 step:1004 [D loss: 0.596937, acc.: 67.97%] [G loss: 1.341604]\n",
      "epoch:1 step:1005 [D loss: 0.556542, acc.: 68.75%] [G loss: 1.250885]\n",
      "epoch:1 step:1006 [D loss: 0.648781, acc.: 60.94%] [G loss: 1.373386]\n",
      "epoch:1 step:1007 [D loss: 0.669419, acc.: 64.84%] [G loss: 1.217523]\n",
      "epoch:1 step:1008 [D loss: 0.633318, acc.: 67.19%] [G loss: 1.332303]\n",
      "epoch:1 step:1009 [D loss: 0.569558, acc.: 71.09%] [G loss: 1.287661]\n",
      "epoch:1 step:1010 [D loss: 0.541060, acc.: 74.22%] [G loss: 1.308335]\n",
      "epoch:1 step:1011 [D loss: 0.654775, acc.: 62.50%] [G loss: 1.424979]\n",
      "epoch:1 step:1012 [D loss: 0.560110, acc.: 72.66%] [G loss: 1.242474]\n",
      "epoch:1 step:1013 [D loss: 0.529470, acc.: 71.09%] [G loss: 1.365056]\n",
      "epoch:1 step:1014 [D loss: 0.617074, acc.: 64.84%] [G loss: 1.417184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1015 [D loss: 0.600382, acc.: 68.75%] [G loss: 1.396426]\n",
      "epoch:1 step:1016 [D loss: 0.539945, acc.: 73.44%] [G loss: 1.290864]\n",
      "epoch:1 step:1017 [D loss: 0.636049, acc.: 63.28%] [G loss: 1.185540]\n",
      "epoch:1 step:1018 [D loss: 0.609537, acc.: 65.62%] [G loss: 1.436528]\n",
      "epoch:1 step:1019 [D loss: 0.602764, acc.: 67.19%] [G loss: 1.255741]\n",
      "epoch:1 step:1020 [D loss: 0.667939, acc.: 56.25%] [G loss: 1.105050]\n",
      "epoch:1 step:1021 [D loss: 0.543318, acc.: 71.09%] [G loss: 1.280650]\n",
      "epoch:1 step:1022 [D loss: 0.527829, acc.: 76.56%] [G loss: 1.202863]\n",
      "epoch:1 step:1023 [D loss: 0.568622, acc.: 73.44%] [G loss: 1.130941]\n",
      "epoch:1 step:1024 [D loss: 0.624048, acc.: 65.62%] [G loss: 1.323653]\n",
      "epoch:1 step:1025 [D loss: 0.613232, acc.: 64.84%] [G loss: 1.341090]\n",
      "epoch:1 step:1026 [D loss: 0.721294, acc.: 60.16%] [G loss: 1.018168]\n",
      "epoch:1 step:1027 [D loss: 0.684251, acc.: 58.59%] [G loss: 1.314146]\n",
      "epoch:1 step:1028 [D loss: 0.747969, acc.: 51.56%] [G loss: 1.344135]\n",
      "epoch:1 step:1029 [D loss: 0.592225, acc.: 65.62%] [G loss: 1.291969]\n",
      "epoch:1 step:1030 [D loss: 0.688869, acc.: 60.16%] [G loss: 1.288681]\n",
      "epoch:1 step:1031 [D loss: 0.614394, acc.: 70.31%] [G loss: 1.217421]\n",
      "epoch:1 step:1032 [D loss: 0.702151, acc.: 56.25%] [G loss: 0.964555]\n",
      "epoch:1 step:1033 [D loss: 0.489833, acc.: 72.66%] [G loss: 1.374471]\n",
      "epoch:1 step:1034 [D loss: 0.596178, acc.: 67.19%] [G loss: 1.133350]\n",
      "epoch:1 step:1035 [D loss: 0.572695, acc.: 68.75%] [G loss: 1.146154]\n",
      "epoch:1 step:1036 [D loss: 0.608151, acc.: 64.06%] [G loss: 1.269550]\n",
      "epoch:1 step:1037 [D loss: 0.585957, acc.: 69.53%] [G loss: 1.437819]\n",
      "epoch:1 step:1038 [D loss: 0.585146, acc.: 69.53%] [G loss: 1.280553]\n",
      "epoch:1 step:1039 [D loss: 0.717744, acc.: 59.38%] [G loss: 1.234040]\n",
      "epoch:1 step:1040 [D loss: 0.519058, acc.: 72.66%] [G loss: 1.421103]\n",
      "epoch:1 step:1041 [D loss: 0.586620, acc.: 68.75%] [G loss: 1.164194]\n",
      "epoch:1 step:1042 [D loss: 0.607270, acc.: 67.97%] [G loss: 1.425254]\n",
      "epoch:1 step:1043 [D loss: 0.672643, acc.: 64.84%] [G loss: 1.341448]\n",
      "epoch:1 step:1044 [D loss: 0.568656, acc.: 72.66%] [G loss: 1.575123]\n",
      "epoch:1 step:1045 [D loss: 0.616747, acc.: 60.16%] [G loss: 1.235579]\n",
      "epoch:1 step:1046 [D loss: 0.545154, acc.: 74.22%] [G loss: 1.483603]\n",
      "epoch:1 step:1047 [D loss: 0.602968, acc.: 68.75%] [G loss: 1.196966]\n",
      "epoch:1 step:1048 [D loss: 0.678121, acc.: 60.94%] [G loss: 1.277763]\n",
      "epoch:1 step:1049 [D loss: 0.540291, acc.: 68.75%] [G loss: 1.410163]\n",
      "epoch:1 step:1050 [D loss: 0.594500, acc.: 71.88%] [G loss: 1.268450]\n",
      "epoch:1 step:1051 [D loss: 0.677215, acc.: 60.94%] [G loss: 1.199396]\n",
      "epoch:1 step:1052 [D loss: 0.648836, acc.: 64.84%] [G loss: 1.203757]\n",
      "epoch:1 step:1053 [D loss: 0.694069, acc.: 60.16%] [G loss: 1.246449]\n",
      "epoch:1 step:1054 [D loss: 0.702582, acc.: 57.03%] [G loss: 1.223418]\n",
      "epoch:1 step:1055 [D loss: 0.580338, acc.: 68.75%] [G loss: 1.358654]\n",
      "epoch:1 step:1056 [D loss: 0.662956, acc.: 64.84%] [G loss: 1.120147]\n",
      "epoch:1 step:1057 [D loss: 0.579993, acc.: 67.19%] [G loss: 1.266013]\n",
      "epoch:1 step:1058 [D loss: 0.770190, acc.: 50.00%] [G loss: 1.025940]\n",
      "epoch:1 step:1059 [D loss: 0.549903, acc.: 73.44%] [G loss: 1.447894]\n",
      "epoch:1 step:1060 [D loss: 0.664286, acc.: 64.06%] [G loss: 1.114749]\n",
      "epoch:1 step:1061 [D loss: 0.735436, acc.: 53.12%] [G loss: 1.237774]\n",
      "epoch:1 step:1062 [D loss: 0.552456, acc.: 71.09%] [G loss: 1.553736]\n",
      "epoch:1 step:1063 [D loss: 0.644385, acc.: 67.19%] [G loss: 1.184081]\n",
      "epoch:1 step:1064 [D loss: 0.676619, acc.: 66.41%] [G loss: 1.262079]\n",
      "epoch:1 step:1065 [D loss: 0.627643, acc.: 64.84%] [G loss: 1.303045]\n",
      "epoch:1 step:1066 [D loss: 0.547174, acc.: 73.44%] [G loss: 1.331421]\n",
      "epoch:1 step:1067 [D loss: 0.649968, acc.: 59.38%] [G loss: 1.206541]\n",
      "epoch:1 step:1068 [D loss: 0.635549, acc.: 64.06%] [G loss: 1.133072]\n",
      "epoch:1 step:1069 [D loss: 0.670519, acc.: 60.16%] [G loss: 1.163833]\n",
      "epoch:1 step:1070 [D loss: 0.593853, acc.: 68.75%] [G loss: 1.180010]\n",
      "epoch:1 step:1071 [D loss: 0.571036, acc.: 69.53%] [G loss: 1.331316]\n",
      "epoch:1 step:1072 [D loss: 0.539910, acc.: 75.78%] [G loss: 1.426716]\n",
      "epoch:1 step:1073 [D loss: 0.635888, acc.: 61.72%] [G loss: 1.219202]\n",
      "epoch:1 step:1074 [D loss: 0.687792, acc.: 61.72%] [G loss: 1.207176]\n",
      "epoch:1 step:1075 [D loss: 0.561341, acc.: 70.31%] [G loss: 1.141498]\n",
      "epoch:1 step:1076 [D loss: 0.653924, acc.: 63.28%] [G loss: 1.264813]\n",
      "epoch:1 step:1077 [D loss: 0.546670, acc.: 73.44%] [G loss: 1.348496]\n",
      "epoch:1 step:1078 [D loss: 0.641078, acc.: 64.06%] [G loss: 1.262024]\n",
      "epoch:1 step:1079 [D loss: 0.655389, acc.: 59.38%] [G loss: 1.186725]\n",
      "epoch:1 step:1080 [D loss: 0.644999, acc.: 63.28%] [G loss: 1.324746]\n",
      "epoch:1 step:1081 [D loss: 0.601376, acc.: 68.75%] [G loss: 1.279438]\n",
      "epoch:1 step:1082 [D loss: 0.678301, acc.: 65.62%] [G loss: 1.268552]\n",
      "epoch:1 step:1083 [D loss: 0.524050, acc.: 75.78%] [G loss: 1.394541]\n",
      "epoch:1 step:1084 [D loss: 0.660835, acc.: 60.94%] [G loss: 1.243993]\n",
      "epoch:1 step:1085 [D loss: 0.644154, acc.: 60.94%] [G loss: 1.348863]\n",
      "epoch:1 step:1086 [D loss: 0.648708, acc.: 60.16%] [G loss: 1.119559]\n",
      "epoch:1 step:1087 [D loss: 0.636011, acc.: 66.41%] [G loss: 1.155842]\n",
      "epoch:1 step:1088 [D loss: 0.639234, acc.: 65.62%] [G loss: 1.433125]\n",
      "epoch:1 step:1089 [D loss: 0.547103, acc.: 75.78%] [G loss: 1.522341]\n",
      "epoch:1 step:1090 [D loss: 0.628414, acc.: 65.62%] [G loss: 1.207211]\n",
      "epoch:1 step:1091 [D loss: 0.632939, acc.: 62.50%] [G loss: 1.257710]\n",
      "epoch:1 step:1092 [D loss: 0.629461, acc.: 64.84%] [G loss: 1.225693]\n",
      "epoch:1 step:1093 [D loss: 0.594414, acc.: 70.31%] [G loss: 1.070936]\n",
      "epoch:1 step:1094 [D loss: 0.674491, acc.: 57.81%] [G loss: 1.191988]\n",
      "epoch:1 step:1095 [D loss: 0.673341, acc.: 59.38%] [G loss: 1.149211]\n",
      "epoch:1 step:1096 [D loss: 0.522141, acc.: 72.66%] [G loss: 1.202385]\n",
      "epoch:1 step:1097 [D loss: 0.703257, acc.: 60.16%] [G loss: 1.085616]\n",
      "epoch:1 step:1098 [D loss: 0.504774, acc.: 79.69%] [G loss: 1.568706]\n",
      "epoch:1 step:1099 [D loss: 0.614492, acc.: 65.62%] [G loss: 1.423137]\n",
      "epoch:1 step:1100 [D loss: 0.667199, acc.: 62.50%] [G loss: 1.370434]\n",
      "epoch:1 step:1101 [D loss: 0.565414, acc.: 71.88%] [G loss: 1.337707]\n",
      "epoch:1 step:1102 [D loss: 0.687560, acc.: 63.28%] [G loss: 1.210199]\n",
      "epoch:1 step:1103 [D loss: 0.631312, acc.: 61.72%] [G loss: 1.084451]\n",
      "epoch:1 step:1104 [D loss: 0.603191, acc.: 68.75%] [G loss: 1.364372]\n",
      "epoch:1 step:1105 [D loss: 0.581565, acc.: 64.84%] [G loss: 1.257543]\n",
      "epoch:1 step:1106 [D loss: 0.724223, acc.: 53.91%] [G loss: 1.203988]\n",
      "epoch:1 step:1107 [D loss: 0.778211, acc.: 53.91%] [G loss: 1.143279]\n",
      "epoch:1 step:1108 [D loss: 0.510725, acc.: 77.34%] [G loss: 1.244695]\n",
      "epoch:1 step:1109 [D loss: 0.641976, acc.: 68.75%] [G loss: 1.165243]\n",
      "epoch:1 step:1110 [D loss: 0.635271, acc.: 64.84%] [G loss: 1.296108]\n",
      "epoch:1 step:1111 [D loss: 0.604717, acc.: 67.97%] [G loss: 1.402506]\n",
      "epoch:1 step:1112 [D loss: 0.610367, acc.: 68.75%] [G loss: 1.271875]\n",
      "epoch:1 step:1113 [D loss: 0.628820, acc.: 67.97%] [G loss: 1.313061]\n",
      "epoch:1 step:1114 [D loss: 0.590228, acc.: 68.75%] [G loss: 1.237909]\n",
      "epoch:1 step:1115 [D loss: 0.642486, acc.: 64.06%] [G loss: 1.423230]\n",
      "epoch:1 step:1116 [D loss: 0.607239, acc.: 68.75%] [G loss: 1.202991]\n",
      "epoch:1 step:1117 [D loss: 0.671216, acc.: 63.28%] [G loss: 1.384964]\n",
      "epoch:1 step:1118 [D loss: 0.711801, acc.: 57.03%] [G loss: 1.326631]\n",
      "epoch:1 step:1119 [D loss: 0.572234, acc.: 70.31%] [G loss: 1.357492]\n",
      "epoch:1 step:1120 [D loss: 0.838459, acc.: 42.97%] [G loss: 1.178298]\n",
      "epoch:1 step:1121 [D loss: 0.654921, acc.: 60.94%] [G loss: 1.190673]\n",
      "epoch:1 step:1122 [D loss: 0.698024, acc.: 62.50%] [G loss: 1.204768]\n",
      "epoch:1 step:1123 [D loss: 0.630517, acc.: 65.62%] [G loss: 1.267241]\n",
      "epoch:1 step:1124 [D loss: 0.494861, acc.: 79.69%] [G loss: 1.529656]\n",
      "epoch:1 step:1125 [D loss: 0.682681, acc.: 59.38%] [G loss: 1.240619]\n",
      "epoch:1 step:1126 [D loss: 0.605346, acc.: 64.06%] [G loss: 1.130031]\n",
      "epoch:1 step:1127 [D loss: 0.650542, acc.: 59.38%] [G loss: 1.269246]\n",
      "epoch:1 step:1128 [D loss: 0.745410, acc.: 54.69%] [G loss: 1.126506]\n",
      "epoch:1 step:1129 [D loss: 0.635517, acc.: 63.28%] [G loss: 1.178128]\n",
      "epoch:1 step:1130 [D loss: 0.650057, acc.: 63.28%] [G loss: 1.190103]\n",
      "epoch:1 step:1131 [D loss: 0.569775, acc.: 72.66%] [G loss: 1.327335]\n",
      "epoch:1 step:1132 [D loss: 0.736074, acc.: 53.12%] [G loss: 1.098861]\n",
      "epoch:1 step:1133 [D loss: 0.605814, acc.: 64.84%] [G loss: 1.486024]\n",
      "epoch:1 step:1134 [D loss: 0.577993, acc.: 68.75%] [G loss: 1.474457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1135 [D loss: 0.680238, acc.: 61.72%] [G loss: 1.169506]\n",
      "epoch:1 step:1136 [D loss: 0.707695, acc.: 57.03%] [G loss: 1.168716]\n",
      "epoch:1 step:1137 [D loss: 0.626197, acc.: 62.50%] [G loss: 1.256207]\n",
      "epoch:1 step:1138 [D loss: 0.613222, acc.: 65.62%] [G loss: 1.306801]\n",
      "epoch:1 step:1139 [D loss: 0.611669, acc.: 62.50%] [G loss: 1.224960]\n",
      "epoch:1 step:1140 [D loss: 0.630230, acc.: 67.97%] [G loss: 1.206703]\n",
      "epoch:1 step:1141 [D loss: 0.665435, acc.: 60.16%] [G loss: 1.393888]\n",
      "epoch:1 step:1142 [D loss: 0.674490, acc.: 55.47%] [G loss: 1.134190]\n",
      "epoch:1 step:1143 [D loss: 0.696153, acc.: 55.47%] [G loss: 1.197698]\n",
      "epoch:1 step:1144 [D loss: 0.540369, acc.: 72.66%] [G loss: 1.411219]\n",
      "epoch:1 step:1145 [D loss: 0.651766, acc.: 65.62%] [G loss: 1.047257]\n",
      "epoch:1 step:1146 [D loss: 0.660185, acc.: 64.84%] [G loss: 1.188942]\n",
      "epoch:1 step:1147 [D loss: 0.657153, acc.: 65.62%] [G loss: 1.101751]\n",
      "epoch:1 step:1148 [D loss: 0.659487, acc.: 60.94%] [G loss: 1.323077]\n",
      "epoch:1 step:1149 [D loss: 0.555229, acc.: 68.75%] [G loss: 1.334064]\n",
      "epoch:1 step:1150 [D loss: 0.647436, acc.: 62.50%] [G loss: 1.444381]\n",
      "epoch:1 step:1151 [D loss: 0.821036, acc.: 52.34%] [G loss: 1.167301]\n",
      "epoch:1 step:1152 [D loss: 0.754440, acc.: 50.78%] [G loss: 0.972063]\n",
      "epoch:1 step:1153 [D loss: 0.641096, acc.: 60.94%] [G loss: 1.049551]\n",
      "epoch:1 step:1154 [D loss: 0.739782, acc.: 52.34%] [G loss: 1.152604]\n",
      "epoch:1 step:1155 [D loss: 0.682277, acc.: 60.16%] [G loss: 1.174483]\n",
      "epoch:1 step:1156 [D loss: 0.551331, acc.: 71.09%] [G loss: 1.148963]\n",
      "epoch:1 step:1157 [D loss: 0.592345, acc.: 65.62%] [G loss: 1.081881]\n",
      "epoch:1 step:1158 [D loss: 0.647092, acc.: 72.66%] [G loss: 1.183297]\n",
      "epoch:1 step:1159 [D loss: 0.663178, acc.: 65.62%] [G loss: 1.328365]\n",
      "epoch:1 step:1160 [D loss: 0.682101, acc.: 55.47%] [G loss: 1.259388]\n",
      "epoch:1 step:1161 [D loss: 0.622646, acc.: 67.97%] [G loss: 1.401163]\n",
      "epoch:1 step:1162 [D loss: 0.586080, acc.: 64.84%] [G loss: 1.225603]\n",
      "epoch:1 step:1163 [D loss: 0.622926, acc.: 62.50%] [G loss: 1.129080]\n",
      "epoch:1 step:1164 [D loss: 0.607908, acc.: 68.75%] [G loss: 1.047634]\n",
      "epoch:1 step:1165 [D loss: 0.617647, acc.: 67.97%] [G loss: 1.393535]\n",
      "epoch:1 step:1166 [D loss: 0.662435, acc.: 60.94%] [G loss: 1.420072]\n",
      "epoch:1 step:1167 [D loss: 0.635150, acc.: 59.38%] [G loss: 1.332571]\n",
      "epoch:1 step:1168 [D loss: 0.729909, acc.: 60.94%] [G loss: 1.219170]\n",
      "epoch:1 step:1169 [D loss: 0.655294, acc.: 64.84%] [G loss: 1.261539]\n",
      "epoch:1 step:1170 [D loss: 0.659328, acc.: 65.62%] [G loss: 1.143323]\n",
      "epoch:1 step:1171 [D loss: 0.650813, acc.: 62.50%] [G loss: 1.023021]\n",
      "epoch:1 step:1172 [D loss: 0.639481, acc.: 64.84%] [G loss: 1.145100]\n",
      "epoch:1 step:1173 [D loss: 0.535397, acc.: 72.66%] [G loss: 1.359869]\n",
      "epoch:1 step:1174 [D loss: 0.595675, acc.: 66.41%] [G loss: 1.120643]\n",
      "epoch:1 step:1175 [D loss: 0.664457, acc.: 59.38%] [G loss: 1.191649]\n",
      "epoch:1 step:1176 [D loss: 0.572229, acc.: 67.19%] [G loss: 1.427094]\n",
      "epoch:1 step:1177 [D loss: 0.676844, acc.: 62.50%] [G loss: 1.294841]\n",
      "epoch:1 step:1178 [D loss: 0.680874, acc.: 58.59%] [G loss: 1.152135]\n",
      "epoch:1 step:1179 [D loss: 0.720676, acc.: 56.25%] [G loss: 1.194480]\n",
      "epoch:1 step:1180 [D loss: 0.649090, acc.: 64.06%] [G loss: 1.211360]\n",
      "epoch:1 step:1181 [D loss: 0.694783, acc.: 60.16%] [G loss: 1.152292]\n",
      "epoch:1 step:1182 [D loss: 0.657681, acc.: 60.94%] [G loss: 1.052509]\n",
      "epoch:1 step:1183 [D loss: 0.606225, acc.: 70.31%] [G loss: 1.268541]\n",
      "epoch:1 step:1184 [D loss: 0.614548, acc.: 62.50%] [G loss: 1.312825]\n",
      "epoch:1 step:1185 [D loss: 0.682703, acc.: 57.81%] [G loss: 1.204425]\n",
      "epoch:1 step:1186 [D loss: 0.624536, acc.: 60.94%] [G loss: 1.157219]\n",
      "epoch:1 step:1187 [D loss: 0.557307, acc.: 68.75%] [G loss: 1.154445]\n",
      "epoch:1 step:1188 [D loss: 0.648782, acc.: 64.84%] [G loss: 1.233780]\n",
      "epoch:1 step:1189 [D loss: 0.613280, acc.: 68.75%] [G loss: 0.993945]\n",
      "epoch:1 step:1190 [D loss: 0.595251, acc.: 71.09%] [G loss: 1.355827]\n",
      "epoch:1 step:1191 [D loss: 0.650555, acc.: 60.94%] [G loss: 1.116187]\n",
      "epoch:1 step:1192 [D loss: 0.568849, acc.: 71.88%] [G loss: 1.396006]\n",
      "epoch:1 step:1193 [D loss: 0.617335, acc.: 64.84%] [G loss: 1.362026]\n",
      "epoch:1 step:1194 [D loss: 0.700704, acc.: 58.59%] [G loss: 1.235019]\n",
      "epoch:1 step:1195 [D loss: 0.629543, acc.: 67.19%] [G loss: 1.177993]\n",
      "epoch:1 step:1196 [D loss: 0.631994, acc.: 61.72%] [G loss: 1.365524]\n",
      "epoch:1 step:1197 [D loss: 0.742062, acc.: 59.38%] [G loss: 1.101542]\n",
      "epoch:1 step:1198 [D loss: 0.577529, acc.: 69.53%] [G loss: 1.327287]\n",
      "epoch:1 step:1199 [D loss: 0.734171, acc.: 57.81%] [G loss: 1.101609]\n",
      "epoch:1 step:1200 [D loss: 0.715854, acc.: 57.03%] [G loss: 1.280428]\n",
      "##############\n",
      "[2.46031578 1.83725387 1.49017175 2.90418239 0.86038388 6.34256065\n",
      " 2.15109009 3.13503739 3.43976343 5.03764236]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.656610, acc.: 60.94%] [G loss: 1.231311]\n",
      "epoch:1 step:1202 [D loss: 0.658211, acc.: 59.38%] [G loss: 1.265188]\n",
      "epoch:1 step:1203 [D loss: 0.574172, acc.: 66.41%] [G loss: 1.320518]\n",
      "epoch:1 step:1204 [D loss: 0.596688, acc.: 65.62%] [G loss: 1.209444]\n",
      "epoch:1 step:1205 [D loss: 0.574073, acc.: 69.53%] [G loss: 1.329630]\n",
      "epoch:1 step:1206 [D loss: 0.598847, acc.: 71.09%] [G loss: 1.275516]\n",
      "epoch:1 step:1207 [D loss: 0.654639, acc.: 60.94%] [G loss: 1.223308]\n",
      "epoch:1 step:1208 [D loss: 0.568246, acc.: 66.41%] [G loss: 1.262788]\n",
      "epoch:1 step:1209 [D loss: 0.598831, acc.: 67.97%] [G loss: 1.331054]\n",
      "epoch:1 step:1210 [D loss: 0.710231, acc.: 57.81%] [G loss: 1.236774]\n",
      "epoch:1 step:1211 [D loss: 0.740604, acc.: 53.91%] [G loss: 1.125678]\n",
      "epoch:1 step:1212 [D loss: 0.657402, acc.: 62.50%] [G loss: 1.173877]\n",
      "epoch:1 step:1213 [D loss: 0.697258, acc.: 60.94%] [G loss: 0.940884]\n",
      "epoch:1 step:1214 [D loss: 0.722967, acc.: 54.69%] [G loss: 1.014556]\n",
      "epoch:1 step:1215 [D loss: 0.679505, acc.: 61.72%] [G loss: 1.229833]\n",
      "epoch:1 step:1216 [D loss: 0.693521, acc.: 60.16%] [G loss: 1.182502]\n",
      "epoch:1 step:1217 [D loss: 0.691840, acc.: 56.25%] [G loss: 1.265650]\n",
      "epoch:1 step:1218 [D loss: 0.598159, acc.: 66.41%] [G loss: 1.247737]\n",
      "epoch:1 step:1219 [D loss: 0.644893, acc.: 65.62%] [G loss: 1.148735]\n",
      "epoch:1 step:1220 [D loss: 0.637822, acc.: 66.41%] [G loss: 1.179721]\n",
      "epoch:1 step:1221 [D loss: 0.608575, acc.: 68.75%] [G loss: 1.326312]\n",
      "epoch:1 step:1222 [D loss: 0.714724, acc.: 57.03%] [G loss: 1.191264]\n",
      "epoch:1 step:1223 [D loss: 0.661367, acc.: 57.81%] [G loss: 1.100115]\n",
      "epoch:1 step:1224 [D loss: 0.685464, acc.: 58.59%] [G loss: 1.080003]\n",
      "epoch:1 step:1225 [D loss: 0.687909, acc.: 61.72%] [G loss: 1.138618]\n",
      "epoch:1 step:1226 [D loss: 0.594637, acc.: 69.53%] [G loss: 1.369612]\n",
      "epoch:1 step:1227 [D loss: 0.625536, acc.: 62.50%] [G loss: 1.254074]\n",
      "epoch:1 step:1228 [D loss: 0.689151, acc.: 60.16%] [G loss: 1.307277]\n",
      "epoch:1 step:1229 [D loss: 0.654650, acc.: 60.94%] [G loss: 1.284370]\n",
      "epoch:1 step:1230 [D loss: 0.639832, acc.: 68.75%] [G loss: 1.230900]\n",
      "epoch:1 step:1231 [D loss: 0.658783, acc.: 64.84%] [G loss: 1.241534]\n",
      "epoch:1 step:1232 [D loss: 0.617057, acc.: 66.41%] [G loss: 1.194697]\n",
      "epoch:1 step:1233 [D loss: 0.615688, acc.: 66.41%] [G loss: 1.226279]\n",
      "epoch:1 step:1234 [D loss: 0.642737, acc.: 64.06%] [G loss: 1.291004]\n",
      "epoch:1 step:1235 [D loss: 0.672619, acc.: 59.38%] [G loss: 1.181833]\n",
      "epoch:1 step:1236 [D loss: 0.625243, acc.: 67.97%] [G loss: 1.229659]\n",
      "epoch:1 step:1237 [D loss: 0.680720, acc.: 63.28%] [G loss: 1.032961]\n",
      "epoch:1 step:1238 [D loss: 0.743379, acc.: 58.59%] [G loss: 1.080310]\n",
      "epoch:1 step:1239 [D loss: 0.690425, acc.: 59.38%] [G loss: 1.158871]\n",
      "epoch:1 step:1240 [D loss: 0.672954, acc.: 63.28%] [G loss: 1.085293]\n",
      "epoch:1 step:1241 [D loss: 0.612180, acc.: 66.41%] [G loss: 1.176889]\n",
      "epoch:1 step:1242 [D loss: 0.572979, acc.: 66.41%] [G loss: 1.378652]\n",
      "epoch:1 step:1243 [D loss: 0.687268, acc.: 57.03%] [G loss: 1.133849]\n",
      "epoch:1 step:1244 [D loss: 0.767959, acc.: 55.47%] [G loss: 1.053601]\n",
      "epoch:1 step:1245 [D loss: 0.616457, acc.: 66.41%] [G loss: 1.309326]\n",
      "epoch:1 step:1246 [D loss: 0.605352, acc.: 64.06%] [G loss: 1.283063]\n",
      "epoch:1 step:1247 [D loss: 0.717690, acc.: 55.47%] [G loss: 1.250020]\n",
      "epoch:1 step:1248 [D loss: 0.705769, acc.: 60.94%] [G loss: 1.027679]\n",
      "epoch:1 step:1249 [D loss: 0.674965, acc.: 60.94%] [G loss: 1.217784]\n",
      "epoch:1 step:1250 [D loss: 0.658489, acc.: 64.06%] [G loss: 1.123205]\n",
      "epoch:1 step:1251 [D loss: 0.613627, acc.: 60.94%] [G loss: 1.020798]\n",
      "epoch:1 step:1252 [D loss: 0.604550, acc.: 62.50%] [G loss: 1.115257]\n",
      "epoch:1 step:1253 [D loss: 0.610138, acc.: 65.62%] [G loss: 1.189890]\n",
      "epoch:1 step:1254 [D loss: 0.734484, acc.: 52.34%] [G loss: 1.154515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1255 [D loss: 0.570816, acc.: 72.66%] [G loss: 1.168387]\n",
      "epoch:1 step:1256 [D loss: 0.665671, acc.: 55.47%] [G loss: 1.203239]\n",
      "epoch:1 step:1257 [D loss: 0.661675, acc.: 58.59%] [G loss: 1.159132]\n",
      "epoch:1 step:1258 [D loss: 0.586378, acc.: 67.19%] [G loss: 1.304680]\n",
      "epoch:1 step:1259 [D loss: 0.645105, acc.: 61.72%] [G loss: 1.277318]\n",
      "epoch:1 step:1260 [D loss: 0.721768, acc.: 57.81%] [G loss: 1.139727]\n",
      "epoch:1 step:1261 [D loss: 0.679783, acc.: 60.16%] [G loss: 1.066509]\n",
      "epoch:1 step:1262 [D loss: 0.604814, acc.: 70.31%] [G loss: 1.118783]\n",
      "epoch:1 step:1263 [D loss: 0.628905, acc.: 67.19%] [G loss: 1.171380]\n",
      "epoch:1 step:1264 [D loss: 0.636720, acc.: 64.84%] [G loss: 1.259111]\n",
      "epoch:1 step:1265 [D loss: 0.628609, acc.: 65.62%] [G loss: 1.150169]\n",
      "epoch:1 step:1266 [D loss: 0.661399, acc.: 59.38%] [G loss: 1.294269]\n",
      "epoch:1 step:1267 [D loss: 0.646397, acc.: 63.28%] [G loss: 1.322869]\n",
      "epoch:1 step:1268 [D loss: 0.660044, acc.: 64.06%] [G loss: 1.053383]\n",
      "epoch:1 step:1269 [D loss: 0.611338, acc.: 61.72%] [G loss: 1.257805]\n",
      "epoch:1 step:1270 [D loss: 0.542662, acc.: 69.53%] [G loss: 1.442600]\n",
      "epoch:1 step:1271 [D loss: 0.707608, acc.: 63.28%] [G loss: 1.291740]\n",
      "epoch:1 step:1272 [D loss: 0.562448, acc.: 69.53%] [G loss: 1.205754]\n",
      "epoch:1 step:1273 [D loss: 0.660118, acc.: 59.38%] [G loss: 1.289860]\n",
      "epoch:1 step:1274 [D loss: 0.620333, acc.: 62.50%] [G loss: 1.172329]\n",
      "epoch:1 step:1275 [D loss: 0.572357, acc.: 71.88%] [G loss: 1.161344]\n",
      "epoch:1 step:1276 [D loss: 0.671049, acc.: 66.41%] [G loss: 1.189197]\n",
      "epoch:1 step:1277 [D loss: 0.502271, acc.: 78.91%] [G loss: 1.251192]\n",
      "epoch:1 step:1278 [D loss: 0.637531, acc.: 57.03%] [G loss: 1.066822]\n",
      "epoch:1 step:1279 [D loss: 0.682626, acc.: 60.16%] [G loss: 1.240865]\n",
      "epoch:1 step:1280 [D loss: 0.644809, acc.: 60.94%] [G loss: 1.188798]\n",
      "epoch:1 step:1281 [D loss: 0.649790, acc.: 63.28%] [G loss: 1.312959]\n",
      "epoch:1 step:1282 [D loss: 0.605868, acc.: 68.75%] [G loss: 1.297264]\n",
      "epoch:1 step:1283 [D loss: 0.641302, acc.: 63.28%] [G loss: 1.260328]\n",
      "epoch:1 step:1284 [D loss: 0.662876, acc.: 59.38%] [G loss: 1.066480]\n",
      "epoch:1 step:1285 [D loss: 0.625300, acc.: 63.28%] [G loss: 1.201744]\n",
      "epoch:1 step:1286 [D loss: 0.716699, acc.: 54.69%] [G loss: 1.308742]\n",
      "epoch:1 step:1287 [D loss: 0.651056, acc.: 65.62%] [G loss: 1.277959]\n",
      "epoch:1 step:1288 [D loss: 0.669744, acc.: 61.72%] [G loss: 1.220855]\n",
      "epoch:1 step:1289 [D loss: 0.736656, acc.: 49.22%] [G loss: 1.066573]\n",
      "epoch:1 step:1290 [D loss: 0.642262, acc.: 61.72%] [G loss: 1.214570]\n",
      "epoch:1 step:1291 [D loss: 0.596945, acc.: 67.97%] [G loss: 1.299072]\n",
      "epoch:1 step:1292 [D loss: 0.692712, acc.: 61.72%] [G loss: 1.091535]\n",
      "epoch:1 step:1293 [D loss: 0.613383, acc.: 61.72%] [G loss: 1.139222]\n",
      "epoch:1 step:1294 [D loss: 0.603913, acc.: 70.31%] [G loss: 1.150841]\n",
      "epoch:1 step:1295 [D loss: 0.548483, acc.: 70.31%] [G loss: 1.270929]\n",
      "epoch:1 step:1296 [D loss: 0.690254, acc.: 60.94%] [G loss: 1.273908]\n",
      "epoch:1 step:1297 [D loss: 0.653184, acc.: 63.28%] [G loss: 1.280898]\n",
      "epoch:1 step:1298 [D loss: 0.630955, acc.: 62.50%] [G loss: 1.073238]\n",
      "epoch:1 step:1299 [D loss: 0.725915, acc.: 60.16%] [G loss: 1.096273]\n",
      "epoch:1 step:1300 [D loss: 0.638749, acc.: 64.06%] [G loss: 1.180785]\n",
      "epoch:1 step:1301 [D loss: 0.616645, acc.: 64.84%] [G loss: 1.324814]\n",
      "epoch:1 step:1302 [D loss: 0.656887, acc.: 58.59%] [G loss: 1.143508]\n",
      "epoch:1 step:1303 [D loss: 0.652880, acc.: 64.84%] [G loss: 1.134348]\n",
      "epoch:1 step:1304 [D loss: 0.635386, acc.: 64.06%] [G loss: 1.194327]\n",
      "epoch:1 step:1305 [D loss: 0.647386, acc.: 65.62%] [G loss: 1.173169]\n",
      "epoch:1 step:1306 [D loss: 0.677484, acc.: 57.81%] [G loss: 1.307875]\n",
      "epoch:1 step:1307 [D loss: 0.667865, acc.: 55.47%] [G loss: 1.112572]\n",
      "epoch:1 step:1308 [D loss: 0.642157, acc.: 59.38%] [G loss: 1.086029]\n",
      "epoch:1 step:1309 [D loss: 0.619533, acc.: 60.16%] [G loss: 1.222507]\n",
      "epoch:1 step:1310 [D loss: 0.486912, acc.: 79.69%] [G loss: 1.238412]\n",
      "epoch:1 step:1311 [D loss: 0.623062, acc.: 62.50%] [G loss: 1.193478]\n",
      "epoch:1 step:1312 [D loss: 0.755350, acc.: 57.03%] [G loss: 1.113829]\n",
      "epoch:1 step:1313 [D loss: 0.646474, acc.: 63.28%] [G loss: 1.089292]\n",
      "epoch:1 step:1314 [D loss: 0.580842, acc.: 68.75%] [G loss: 1.204879]\n",
      "epoch:1 step:1315 [D loss: 0.712649, acc.: 57.03%] [G loss: 1.139297]\n",
      "epoch:1 step:1316 [D loss: 0.630621, acc.: 65.62%] [G loss: 1.280089]\n",
      "epoch:1 step:1317 [D loss: 0.655107, acc.: 63.28%] [G loss: 1.225391]\n",
      "epoch:1 step:1318 [D loss: 0.690327, acc.: 57.03%] [G loss: 1.154966]\n",
      "epoch:1 step:1319 [D loss: 0.586697, acc.: 62.50%] [G loss: 1.200318]\n",
      "epoch:1 step:1320 [D loss: 0.649861, acc.: 64.84%] [G loss: 1.169530]\n",
      "epoch:1 step:1321 [D loss: 0.445735, acc.: 82.81%] [G loss: 1.380106]\n",
      "epoch:1 step:1322 [D loss: 0.595430, acc.: 67.19%] [G loss: 1.266781]\n",
      "epoch:1 step:1323 [D loss: 0.712399, acc.: 57.81%] [G loss: 1.240731]\n",
      "epoch:1 step:1324 [D loss: 0.664575, acc.: 64.84%] [G loss: 1.149107]\n",
      "epoch:1 step:1325 [D loss: 0.650581, acc.: 64.84%] [G loss: 1.117418]\n",
      "epoch:1 step:1326 [D loss: 0.589131, acc.: 65.62%] [G loss: 1.387223]\n",
      "epoch:1 step:1327 [D loss: 0.680153, acc.: 62.50%] [G loss: 1.290337]\n",
      "epoch:1 step:1328 [D loss: 0.643504, acc.: 62.50%] [G loss: 1.168767]\n",
      "epoch:1 step:1329 [D loss: 0.666432, acc.: 62.50%] [G loss: 1.241971]\n",
      "epoch:1 step:1330 [D loss: 0.716350, acc.: 53.91%] [G loss: 1.133483]\n",
      "epoch:1 step:1331 [D loss: 0.637457, acc.: 67.19%] [G loss: 1.220613]\n",
      "epoch:1 step:1332 [D loss: 0.648223, acc.: 65.62%] [G loss: 1.298042]\n",
      "epoch:1 step:1333 [D loss: 0.642441, acc.: 62.50%] [G loss: 1.186796]\n",
      "epoch:1 step:1334 [D loss: 0.632437, acc.: 64.06%] [G loss: 1.223340]\n",
      "epoch:1 step:1335 [D loss: 0.660294, acc.: 63.28%] [G loss: 1.200575]\n",
      "epoch:1 step:1336 [D loss: 0.575842, acc.: 70.31%] [G loss: 1.198901]\n",
      "epoch:1 step:1337 [D loss: 0.550605, acc.: 77.34%] [G loss: 1.129443]\n",
      "epoch:1 step:1338 [D loss: 0.608360, acc.: 73.44%] [G loss: 1.455087]\n",
      "epoch:1 step:1339 [D loss: 0.649446, acc.: 59.38%] [G loss: 1.362401]\n",
      "epoch:1 step:1340 [D loss: 0.821969, acc.: 50.78%] [G loss: 1.070014]\n",
      "epoch:1 step:1341 [D loss: 0.684248, acc.: 60.94%] [G loss: 0.930135]\n",
      "epoch:1 step:1342 [D loss: 0.638939, acc.: 64.06%] [G loss: 1.094811]\n",
      "epoch:1 step:1343 [D loss: 0.614174, acc.: 63.28%] [G loss: 1.173262]\n",
      "epoch:1 step:1344 [D loss: 0.592424, acc.: 66.41%] [G loss: 1.391101]\n",
      "epoch:1 step:1345 [D loss: 0.629884, acc.: 62.50%] [G loss: 1.234640]\n",
      "epoch:1 step:1346 [D loss: 0.739495, acc.: 56.25%] [G loss: 0.973622]\n",
      "epoch:1 step:1347 [D loss: 0.670909, acc.: 62.50%] [G loss: 1.314343]\n",
      "epoch:1 step:1348 [D loss: 0.661680, acc.: 61.72%] [G loss: 1.334722]\n",
      "epoch:1 step:1349 [D loss: 0.546398, acc.: 76.56%] [G loss: 1.411380]\n",
      "epoch:1 step:1350 [D loss: 0.650815, acc.: 60.16%] [G loss: 1.498987]\n",
      "epoch:1 step:1351 [D loss: 0.587878, acc.: 69.53%] [G loss: 1.293693]\n",
      "epoch:1 step:1352 [D loss: 0.774553, acc.: 50.00%] [G loss: 0.998880]\n",
      "epoch:1 step:1353 [D loss: 0.751308, acc.: 57.81%] [G loss: 1.056820]\n",
      "epoch:1 step:1354 [D loss: 0.689162, acc.: 58.59%] [G loss: 1.062518]\n",
      "epoch:1 step:1355 [D loss: 0.688491, acc.: 60.94%] [G loss: 1.044764]\n",
      "epoch:1 step:1356 [D loss: 0.660363, acc.: 63.28%] [G loss: 1.225056]\n",
      "epoch:1 step:1357 [D loss: 0.581997, acc.: 67.19%] [G loss: 1.196855]\n",
      "epoch:1 step:1358 [D loss: 0.597877, acc.: 71.88%] [G loss: 1.091529]\n",
      "epoch:1 step:1359 [D loss: 0.642693, acc.: 66.41%] [G loss: 1.141080]\n",
      "epoch:1 step:1360 [D loss: 0.623981, acc.: 61.72%] [G loss: 1.215301]\n",
      "epoch:1 step:1361 [D loss: 0.635498, acc.: 64.06%] [G loss: 0.976740]\n",
      "epoch:1 step:1362 [D loss: 0.625267, acc.: 64.84%] [G loss: 1.194701]\n",
      "epoch:1 step:1363 [D loss: 0.536332, acc.: 73.44%] [G loss: 1.167053]\n",
      "epoch:1 step:1364 [D loss: 0.604866, acc.: 68.75%] [G loss: 1.259898]\n",
      "epoch:1 step:1365 [D loss: 0.661729, acc.: 60.16%] [G loss: 0.952914]\n",
      "epoch:1 step:1366 [D loss: 0.745991, acc.: 57.03%] [G loss: 1.159780]\n",
      "epoch:1 step:1367 [D loss: 0.585806, acc.: 68.75%] [G loss: 1.251533]\n",
      "epoch:1 step:1368 [D loss: 0.681508, acc.: 56.25%] [G loss: 1.242781]\n",
      "epoch:1 step:1369 [D loss: 0.631531, acc.: 69.53%] [G loss: 1.291513]\n",
      "epoch:1 step:1370 [D loss: 0.632320, acc.: 67.97%] [G loss: 1.121174]\n",
      "epoch:1 step:1371 [D loss: 0.692010, acc.: 60.16%] [G loss: 1.266066]\n",
      "epoch:1 step:1372 [D loss: 0.557792, acc.: 73.44%] [G loss: 1.391060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1373 [D loss: 0.685729, acc.: 62.50%] [G loss: 1.328375]\n",
      "epoch:1 step:1374 [D loss: 0.616101, acc.: 66.41%] [G loss: 1.219980]\n",
      "epoch:1 step:1375 [D loss: 0.767206, acc.: 50.00%] [G loss: 1.028485]\n",
      "epoch:1 step:1376 [D loss: 0.521840, acc.: 74.22%] [G loss: 0.954819]\n",
      "epoch:1 step:1377 [D loss: 0.659280, acc.: 66.41%] [G loss: 1.153931]\n",
      "epoch:1 step:1378 [D loss: 0.588327, acc.: 71.09%] [G loss: 1.202501]\n",
      "epoch:1 step:1379 [D loss: 0.696907, acc.: 60.94%] [G loss: 1.072950]\n",
      "epoch:1 step:1380 [D loss: 0.657734, acc.: 60.94%] [G loss: 1.096060]\n",
      "epoch:1 step:1381 [D loss: 0.704627, acc.: 60.94%] [G loss: 1.086076]\n",
      "epoch:1 step:1382 [D loss: 0.650715, acc.: 62.50%] [G loss: 1.156903]\n",
      "epoch:1 step:1383 [D loss: 0.639934, acc.: 66.41%] [G loss: 1.080635]\n",
      "epoch:1 step:1384 [D loss: 0.675108, acc.: 58.59%] [G loss: 1.142223]\n",
      "epoch:1 step:1385 [D loss: 0.716224, acc.: 55.47%] [G loss: 1.162304]\n",
      "epoch:1 step:1386 [D loss: 0.581473, acc.: 71.09%] [G loss: 1.248944]\n",
      "epoch:1 step:1387 [D loss: 0.708466, acc.: 54.69%] [G loss: 1.135773]\n",
      "epoch:1 step:1388 [D loss: 0.660880, acc.: 65.62%] [G loss: 1.213470]\n",
      "epoch:1 step:1389 [D loss: 0.701174, acc.: 56.25%] [G loss: 1.225178]\n",
      "epoch:1 step:1390 [D loss: 0.592558, acc.: 64.06%] [G loss: 1.308741]\n",
      "epoch:1 step:1391 [D loss: 0.659505, acc.: 62.50%] [G loss: 1.097474]\n",
      "epoch:1 step:1392 [D loss: 0.608180, acc.: 67.19%] [G loss: 1.324552]\n",
      "epoch:1 step:1393 [D loss: 0.621559, acc.: 62.50%] [G loss: 1.271753]\n",
      "epoch:1 step:1394 [D loss: 0.686051, acc.: 58.59%] [G loss: 1.125474]\n",
      "epoch:1 step:1395 [D loss: 0.683068, acc.: 58.59%] [G loss: 1.041778]\n",
      "epoch:1 step:1396 [D loss: 0.627431, acc.: 61.72%] [G loss: 1.237591]\n",
      "epoch:1 step:1397 [D loss: 0.643474, acc.: 62.50%] [G loss: 1.236054]\n",
      "epoch:1 step:1398 [D loss: 0.770426, acc.: 53.91%] [G loss: 1.037269]\n",
      "epoch:1 step:1399 [D loss: 0.606932, acc.: 70.31%] [G loss: 1.030602]\n",
      "epoch:1 step:1400 [D loss: 0.652950, acc.: 64.84%] [G loss: 1.106409]\n",
      "##############\n",
      "[2.5871536  2.0007173  2.03396649 2.74424635 1.05876649 6.74892065\n",
      " 2.21649548 3.33471787 3.78907664 4.87210395]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.774639, acc.: 49.22%] [G loss: 1.212265]\n",
      "epoch:1 step:1402 [D loss: 0.620443, acc.: 68.75%] [G loss: 1.324012]\n",
      "epoch:1 step:1403 [D loss: 0.595851, acc.: 64.84%] [G loss: 1.202271]\n",
      "epoch:1 step:1404 [D loss: 0.594004, acc.: 71.88%] [G loss: 1.094411]\n",
      "epoch:1 step:1405 [D loss: 0.565027, acc.: 71.88%] [G loss: 1.186448]\n",
      "epoch:1 step:1406 [D loss: 0.660224, acc.: 61.72%] [G loss: 1.237072]\n",
      "epoch:1 step:1407 [D loss: 0.635265, acc.: 65.62%] [G loss: 1.239374]\n",
      "epoch:1 step:1408 [D loss: 0.656240, acc.: 64.06%] [G loss: 1.060902]\n",
      "epoch:1 step:1409 [D loss: 0.620791, acc.: 63.28%] [G loss: 1.107106]\n",
      "epoch:1 step:1410 [D loss: 0.636292, acc.: 62.50%] [G loss: 1.221301]\n",
      "epoch:1 step:1411 [D loss: 0.682422, acc.: 54.69%] [G loss: 1.136522]\n",
      "epoch:1 step:1412 [D loss: 0.540915, acc.: 72.66%] [G loss: 1.041773]\n",
      "epoch:1 step:1413 [D loss: 0.686624, acc.: 56.25%] [G loss: 1.294312]\n",
      "epoch:1 step:1414 [D loss: 0.686945, acc.: 56.25%] [G loss: 1.200359]\n",
      "epoch:1 step:1415 [D loss: 0.680400, acc.: 62.50%] [G loss: 1.368402]\n",
      "epoch:1 step:1416 [D loss: 0.719278, acc.: 60.94%] [G loss: 0.945387]\n",
      "epoch:1 step:1417 [D loss: 0.596798, acc.: 67.19%] [G loss: 1.116305]\n",
      "epoch:1 step:1418 [D loss: 0.707803, acc.: 54.69%] [G loss: 0.985228]\n",
      "epoch:1 step:1419 [D loss: 0.677481, acc.: 58.59%] [G loss: 1.119731]\n",
      "epoch:1 step:1420 [D loss: 0.645820, acc.: 62.50%] [G loss: 1.276770]\n",
      "epoch:1 step:1421 [D loss: 0.761228, acc.: 51.56%] [G loss: 1.239604]\n",
      "epoch:1 step:1422 [D loss: 0.735440, acc.: 55.47%] [G loss: 1.277635]\n",
      "epoch:1 step:1423 [D loss: 0.713319, acc.: 58.59%] [G loss: 1.094378]\n",
      "epoch:1 step:1424 [D loss: 0.628011, acc.: 62.50%] [G loss: 1.162209]\n",
      "epoch:1 step:1425 [D loss: 0.649585, acc.: 63.28%] [G loss: 1.153072]\n",
      "epoch:1 step:1426 [D loss: 0.691691, acc.: 59.38%] [G loss: 1.212760]\n",
      "epoch:1 step:1427 [D loss: 0.678242, acc.: 62.50%] [G loss: 1.127905]\n",
      "epoch:1 step:1428 [D loss: 0.546419, acc.: 73.44%] [G loss: 1.241117]\n",
      "epoch:1 step:1429 [D loss: 0.658217, acc.: 60.94%] [G loss: 1.322887]\n",
      "epoch:1 step:1430 [D loss: 0.632163, acc.: 60.16%] [G loss: 1.226212]\n",
      "epoch:1 step:1431 [D loss: 0.670767, acc.: 61.72%] [G loss: 1.211221]\n",
      "epoch:1 step:1432 [D loss: 0.665777, acc.: 60.94%] [G loss: 1.198153]\n",
      "epoch:1 step:1433 [D loss: 0.628507, acc.: 64.84%] [G loss: 1.170719]\n",
      "epoch:1 step:1434 [D loss: 0.602614, acc.: 63.28%] [G loss: 1.133948]\n",
      "epoch:1 step:1435 [D loss: 0.654210, acc.: 61.72%] [G loss: 1.127196]\n",
      "epoch:1 step:1436 [D loss: 0.549717, acc.: 71.88%] [G loss: 1.393372]\n",
      "epoch:1 step:1437 [D loss: 0.597153, acc.: 65.62%] [G loss: 1.027548]\n",
      "epoch:1 step:1438 [D loss: 0.739910, acc.: 54.69%] [G loss: 1.116898]\n",
      "epoch:1 step:1439 [D loss: 0.613893, acc.: 69.53%] [G loss: 1.181276]\n",
      "epoch:1 step:1440 [D loss: 0.670468, acc.: 60.94%] [G loss: 1.201685]\n",
      "epoch:1 step:1441 [D loss: 0.628099, acc.: 64.06%] [G loss: 1.321212]\n",
      "epoch:1 step:1442 [D loss: 0.585203, acc.: 64.84%] [G loss: 1.244086]\n",
      "epoch:1 step:1443 [D loss: 0.705595, acc.: 57.03%] [G loss: 1.175463]\n",
      "epoch:1 step:1444 [D loss: 0.616803, acc.: 62.50%] [G loss: 1.232116]\n",
      "epoch:1 step:1445 [D loss: 0.554974, acc.: 72.66%] [G loss: 1.112957]\n",
      "epoch:1 step:1446 [D loss: 0.712624, acc.: 57.81%] [G loss: 1.219943]\n",
      "epoch:1 step:1447 [D loss: 0.591146, acc.: 68.75%] [G loss: 1.281065]\n",
      "epoch:1 step:1448 [D loss: 0.595610, acc.: 71.88%] [G loss: 1.176920]\n",
      "epoch:1 step:1449 [D loss: 0.675891, acc.: 60.94%] [G loss: 0.994273]\n",
      "epoch:1 step:1450 [D loss: 0.663236, acc.: 59.38%] [G loss: 1.211741]\n",
      "epoch:1 step:1451 [D loss: 0.688837, acc.: 60.94%] [G loss: 1.122679]\n",
      "epoch:1 step:1452 [D loss: 0.571962, acc.: 69.53%] [G loss: 1.272055]\n",
      "epoch:1 step:1453 [D loss: 0.636096, acc.: 63.28%] [G loss: 1.249748]\n",
      "epoch:1 step:1454 [D loss: 0.546854, acc.: 67.19%] [G loss: 1.172035]\n",
      "epoch:1 step:1455 [D loss: 0.670082, acc.: 59.38%] [G loss: 1.059745]\n",
      "epoch:1 step:1456 [D loss: 0.591068, acc.: 69.53%] [G loss: 1.098623]\n",
      "epoch:1 step:1457 [D loss: 0.659209, acc.: 62.50%] [G loss: 1.101619]\n",
      "epoch:1 step:1458 [D loss: 0.683207, acc.: 64.06%] [G loss: 1.018719]\n",
      "epoch:1 step:1459 [D loss: 0.554943, acc.: 70.31%] [G loss: 1.160857]\n",
      "epoch:1 step:1460 [D loss: 0.588704, acc.: 69.53%] [G loss: 1.216712]\n",
      "epoch:1 step:1461 [D loss: 0.652773, acc.: 63.28%] [G loss: 1.256755]\n",
      "epoch:1 step:1462 [D loss: 0.682514, acc.: 60.16%] [G loss: 1.206755]\n",
      "epoch:1 step:1463 [D loss: 0.749981, acc.: 55.47%] [G loss: 1.150006]\n",
      "epoch:1 step:1464 [D loss: 0.745850, acc.: 52.34%] [G loss: 1.011723]\n",
      "epoch:1 step:1465 [D loss: 0.702905, acc.: 54.69%] [G loss: 1.094452]\n",
      "epoch:1 step:1466 [D loss: 0.571185, acc.: 69.53%] [G loss: 1.224076]\n",
      "epoch:1 step:1467 [D loss: 0.550693, acc.: 73.44%] [G loss: 1.100703]\n",
      "epoch:1 step:1468 [D loss: 0.571029, acc.: 67.97%] [G loss: 1.120790]\n",
      "epoch:1 step:1469 [D loss: 0.696972, acc.: 60.16%] [G loss: 1.110298]\n",
      "epoch:1 step:1470 [D loss: 0.760156, acc.: 52.34%] [G loss: 1.288698]\n",
      "epoch:1 step:1471 [D loss: 0.671547, acc.: 67.19%] [G loss: 0.993549]\n",
      "epoch:1 step:1472 [D loss: 0.611549, acc.: 64.84%] [G loss: 1.268559]\n",
      "epoch:1 step:1473 [D loss: 0.645885, acc.: 61.72%] [G loss: 1.192758]\n",
      "epoch:1 step:1474 [D loss: 0.617143, acc.: 67.97%] [G loss: 1.373547]\n",
      "epoch:1 step:1475 [D loss: 0.769085, acc.: 48.44%] [G loss: 1.129317]\n",
      "epoch:1 step:1476 [D loss: 0.620842, acc.: 64.84%] [G loss: 1.114464]\n",
      "epoch:1 step:1477 [D loss: 0.721267, acc.: 60.16%] [G loss: 1.223766]\n",
      "epoch:1 step:1478 [D loss: 0.656717, acc.: 63.28%] [G loss: 1.144757]\n",
      "epoch:1 step:1479 [D loss: 0.631745, acc.: 61.72%] [G loss: 1.122527]\n",
      "epoch:1 step:1480 [D loss: 0.700669, acc.: 62.50%] [G loss: 1.206355]\n",
      "epoch:1 step:1481 [D loss: 0.622641, acc.: 64.84%] [G loss: 0.973998]\n",
      "epoch:1 step:1482 [D loss: 0.643813, acc.: 63.28%] [G loss: 1.130019]\n",
      "epoch:1 step:1483 [D loss: 0.619429, acc.: 62.50%] [G loss: 1.152424]\n",
      "epoch:1 step:1484 [D loss: 0.713640, acc.: 53.12%] [G loss: 1.132891]\n",
      "epoch:1 step:1485 [D loss: 0.648868, acc.: 65.62%] [G loss: 1.061718]\n",
      "epoch:1 step:1486 [D loss: 0.646556, acc.: 60.94%] [G loss: 1.142233]\n",
      "epoch:1 step:1487 [D loss: 0.623991, acc.: 62.50%] [G loss: 1.549103]\n",
      "epoch:1 step:1488 [D loss: 0.684827, acc.: 56.25%] [G loss: 1.200095]\n",
      "epoch:1 step:1489 [D loss: 0.693616, acc.: 54.69%] [G loss: 1.214505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1490 [D loss: 0.739061, acc.: 53.12%] [G loss: 1.082760]\n",
      "epoch:1 step:1491 [D loss: 0.552809, acc.: 73.44%] [G loss: 1.235221]\n",
      "epoch:1 step:1492 [D loss: 0.694002, acc.: 56.25%] [G loss: 1.193898]\n",
      "epoch:1 step:1493 [D loss: 0.598288, acc.: 63.28%] [G loss: 1.131653]\n",
      "epoch:1 step:1494 [D loss: 0.684687, acc.: 66.41%] [G loss: 0.983214]\n",
      "epoch:1 step:1495 [D loss: 0.739247, acc.: 57.03%] [G loss: 1.210672]\n",
      "epoch:1 step:1496 [D loss: 0.601782, acc.: 71.09%] [G loss: 1.222754]\n",
      "epoch:1 step:1497 [D loss: 0.624309, acc.: 64.06%] [G loss: 1.221430]\n",
      "epoch:1 step:1498 [D loss: 0.527417, acc.: 72.66%] [G loss: 1.290119]\n",
      "epoch:1 step:1499 [D loss: 0.633952, acc.: 70.31%] [G loss: 1.063282]\n",
      "epoch:1 step:1500 [D loss: 0.604235, acc.: 66.41%] [G loss: 1.125758]\n",
      "epoch:1 step:1501 [D loss: 0.568462, acc.: 72.66%] [G loss: 1.243032]\n",
      "epoch:1 step:1502 [D loss: 0.598487, acc.: 69.53%] [G loss: 1.326131]\n",
      "epoch:1 step:1503 [D loss: 0.765961, acc.: 52.34%] [G loss: 0.942343]\n",
      "epoch:1 step:1504 [D loss: 0.687686, acc.: 65.62%] [G loss: 1.173524]\n",
      "epoch:1 step:1505 [D loss: 0.688110, acc.: 65.62%] [G loss: 1.127933]\n",
      "epoch:1 step:1506 [D loss: 0.672765, acc.: 65.62%] [G loss: 1.132161]\n",
      "epoch:1 step:1507 [D loss: 0.681703, acc.: 62.50%] [G loss: 1.134064]\n",
      "epoch:1 step:1508 [D loss: 0.597686, acc.: 67.19%] [G loss: 1.120457]\n",
      "epoch:1 step:1509 [D loss: 0.595155, acc.: 68.75%] [G loss: 1.202880]\n",
      "epoch:1 step:1510 [D loss: 0.699594, acc.: 57.03%] [G loss: 1.098338]\n",
      "epoch:1 step:1511 [D loss: 0.637353, acc.: 64.06%] [G loss: 1.082861]\n",
      "epoch:1 step:1512 [D loss: 0.655258, acc.: 63.28%] [G loss: 1.012159]\n",
      "epoch:1 step:1513 [D loss: 0.623335, acc.: 67.19%] [G loss: 0.920735]\n",
      "epoch:1 step:1514 [D loss: 0.704586, acc.: 61.72%] [G loss: 1.099990]\n",
      "epoch:1 step:1515 [D loss: 0.652240, acc.: 61.72%] [G loss: 1.193846]\n",
      "epoch:1 step:1516 [D loss: 0.767734, acc.: 53.91%] [G loss: 1.027262]\n",
      "epoch:1 step:1517 [D loss: 0.786800, acc.: 49.22%] [G loss: 1.047361]\n",
      "epoch:1 step:1518 [D loss: 0.845640, acc.: 47.66%] [G loss: 1.189010]\n",
      "epoch:1 step:1519 [D loss: 0.613539, acc.: 65.62%] [G loss: 1.218823]\n",
      "epoch:1 step:1520 [D loss: 0.690103, acc.: 60.94%] [G loss: 0.933884]\n",
      "epoch:1 step:1521 [D loss: 0.709445, acc.: 55.47%] [G loss: 1.314982]\n",
      "epoch:1 step:1522 [D loss: 0.565004, acc.: 71.88%] [G loss: 1.307220]\n",
      "epoch:1 step:1523 [D loss: 0.590392, acc.: 72.66%] [G loss: 1.034342]\n",
      "epoch:1 step:1524 [D loss: 0.670859, acc.: 58.59%] [G loss: 1.026180]\n",
      "epoch:1 step:1525 [D loss: 0.630077, acc.: 64.06%] [G loss: 1.145551]\n",
      "epoch:1 step:1526 [D loss: 0.590354, acc.: 69.53%] [G loss: 1.293111]\n",
      "epoch:1 step:1527 [D loss: 0.635720, acc.: 60.94%] [G loss: 1.301378]\n",
      "epoch:1 step:1528 [D loss: 0.740498, acc.: 48.44%] [G loss: 0.990621]\n",
      "epoch:1 step:1529 [D loss: 0.584907, acc.: 66.41%] [G loss: 1.343655]\n",
      "epoch:1 step:1530 [D loss: 0.748634, acc.: 53.91%] [G loss: 1.130739]\n",
      "epoch:1 step:1531 [D loss: 0.579166, acc.: 67.19%] [G loss: 1.180086]\n",
      "epoch:1 step:1532 [D loss: 0.747429, acc.: 55.47%] [G loss: 1.027766]\n",
      "epoch:1 step:1533 [D loss: 0.736213, acc.: 56.25%] [G loss: 1.006136]\n",
      "epoch:1 step:1534 [D loss: 0.847913, acc.: 40.62%] [G loss: 1.062496]\n",
      "epoch:1 step:1535 [D loss: 0.644456, acc.: 64.06%] [G loss: 1.269791]\n",
      "epoch:1 step:1536 [D loss: 0.750379, acc.: 53.12%] [G loss: 1.030881]\n",
      "epoch:1 step:1537 [D loss: 0.657230, acc.: 62.50%] [G loss: 1.192173]\n",
      "epoch:1 step:1538 [D loss: 0.645821, acc.: 59.38%] [G loss: 1.213890]\n",
      "epoch:1 step:1539 [D loss: 0.586418, acc.: 74.22%] [G loss: 1.118386]\n",
      "epoch:1 step:1540 [D loss: 0.633367, acc.: 64.84%] [G loss: 1.160228]\n",
      "epoch:1 step:1541 [D loss: 0.682076, acc.: 55.47%] [G loss: 1.222718]\n",
      "epoch:1 step:1542 [D loss: 0.553716, acc.: 72.66%] [G loss: 1.284086]\n",
      "epoch:1 step:1543 [D loss: 0.768191, acc.: 50.78%] [G loss: 0.973298]\n",
      "epoch:1 step:1544 [D loss: 0.680019, acc.: 60.94%] [G loss: 1.166847]\n",
      "epoch:1 step:1545 [D loss: 0.688235, acc.: 57.03%] [G loss: 1.196495]\n",
      "epoch:1 step:1546 [D loss: 0.587683, acc.: 72.66%] [G loss: 1.323858]\n",
      "epoch:1 step:1547 [D loss: 0.639987, acc.: 63.28%] [G loss: 1.166291]\n",
      "epoch:1 step:1548 [D loss: 0.664940, acc.: 61.72%] [G loss: 1.163833]\n",
      "epoch:1 step:1549 [D loss: 0.681389, acc.: 60.94%] [G loss: 0.997841]\n",
      "epoch:1 step:1550 [D loss: 0.595149, acc.: 72.66%] [G loss: 1.025946]\n",
      "epoch:1 step:1551 [D loss: 0.675173, acc.: 58.59%] [G loss: 1.240102]\n",
      "epoch:1 step:1552 [D loss: 0.612093, acc.: 67.19%] [G loss: 1.175600]\n",
      "epoch:1 step:1553 [D loss: 0.587675, acc.: 67.97%] [G loss: 1.216671]\n",
      "epoch:1 step:1554 [D loss: 0.754373, acc.: 51.56%] [G loss: 0.920510]\n",
      "epoch:1 step:1555 [D loss: 0.688795, acc.: 59.38%] [G loss: 1.049960]\n",
      "epoch:1 step:1556 [D loss: 0.734415, acc.: 53.91%] [G loss: 0.997503]\n",
      "epoch:1 step:1557 [D loss: 0.588291, acc.: 68.75%] [G loss: 1.205234]\n",
      "epoch:1 step:1558 [D loss: 0.749613, acc.: 52.34%] [G loss: 1.201941]\n",
      "epoch:1 step:1559 [D loss: 0.677841, acc.: 59.38%] [G loss: 1.141987]\n",
      "epoch:1 step:1560 [D loss: 0.588508, acc.: 71.09%] [G loss: 1.274916]\n",
      "epoch:1 step:1561 [D loss: 0.563891, acc.: 69.53%] [G loss: 1.334106]\n",
      "epoch:1 step:1562 [D loss: 0.638231, acc.: 65.62%] [G loss: 1.249733]\n",
      "epoch:1 step:1563 [D loss: 0.705664, acc.: 56.25%] [G loss: 1.161750]\n",
      "epoch:1 step:1564 [D loss: 0.674822, acc.: 56.25%] [G loss: 1.158791]\n",
      "epoch:1 step:1565 [D loss: 0.737891, acc.: 53.91%] [G loss: 1.162670]\n",
      "epoch:1 step:1566 [D loss: 0.653878, acc.: 60.94%] [G loss: 1.170446]\n",
      "epoch:1 step:1567 [D loss: 0.621923, acc.: 61.72%] [G loss: 1.213801]\n",
      "epoch:1 step:1568 [D loss: 0.586192, acc.: 70.31%] [G loss: 1.049367]\n",
      "epoch:1 step:1569 [D loss: 0.698720, acc.: 57.81%] [G loss: 1.274116]\n",
      "epoch:1 step:1570 [D loss: 0.569773, acc.: 70.31%] [G loss: 1.209557]\n",
      "epoch:1 step:1571 [D loss: 0.614983, acc.: 64.06%] [G loss: 1.132314]\n",
      "epoch:1 step:1572 [D loss: 0.718390, acc.: 57.81%] [G loss: 1.032430]\n",
      "epoch:1 step:1573 [D loss: 0.659187, acc.: 67.19%] [G loss: 1.366174]\n",
      "epoch:1 step:1574 [D loss: 0.672289, acc.: 63.28%] [G loss: 1.119498]\n",
      "epoch:1 step:1575 [D loss: 0.671740, acc.: 59.38%] [G loss: 1.084445]\n",
      "epoch:1 step:1576 [D loss: 0.641494, acc.: 63.28%] [G loss: 1.113273]\n",
      "epoch:1 step:1577 [D loss: 0.682212, acc.: 64.06%] [G loss: 1.188498]\n",
      "epoch:1 step:1578 [D loss: 0.747448, acc.: 50.78%] [G loss: 1.320302]\n",
      "epoch:1 step:1579 [D loss: 0.593718, acc.: 67.19%] [G loss: 1.180966]\n",
      "epoch:1 step:1580 [D loss: 0.728120, acc.: 60.16%] [G loss: 0.953294]\n",
      "epoch:1 step:1581 [D loss: 0.555721, acc.: 71.88%] [G loss: 1.263741]\n",
      "epoch:1 step:1582 [D loss: 0.650417, acc.: 59.38%] [G loss: 1.233974]\n",
      "epoch:1 step:1583 [D loss: 0.713227, acc.: 60.16%] [G loss: 1.251994]\n",
      "epoch:1 step:1584 [D loss: 0.616973, acc.: 63.28%] [G loss: 1.143763]\n",
      "epoch:1 step:1585 [D loss: 0.675048, acc.: 57.81%] [G loss: 1.069461]\n",
      "epoch:1 step:1586 [D loss: 0.664306, acc.: 59.38%] [G loss: 1.161144]\n",
      "epoch:1 step:1587 [D loss: 0.682353, acc.: 60.94%] [G loss: 1.137751]\n",
      "epoch:1 step:1588 [D loss: 0.685512, acc.: 57.03%] [G loss: 1.216131]\n",
      "epoch:1 step:1589 [D loss: 0.651740, acc.: 62.50%] [G loss: 1.053894]\n",
      "epoch:1 step:1590 [D loss: 0.650297, acc.: 65.62%] [G loss: 1.130792]\n",
      "epoch:1 step:1591 [D loss: 0.641861, acc.: 61.72%] [G loss: 1.186275]\n",
      "epoch:1 step:1592 [D loss: 0.730886, acc.: 53.91%] [G loss: 0.920866]\n",
      "epoch:1 step:1593 [D loss: 0.582000, acc.: 67.19%] [G loss: 0.950508]\n",
      "epoch:1 step:1594 [D loss: 0.606049, acc.: 63.28%] [G loss: 1.131378]\n",
      "epoch:1 step:1595 [D loss: 0.678681, acc.: 55.47%] [G loss: 1.160538]\n",
      "epoch:1 step:1596 [D loss: 0.675993, acc.: 60.16%] [G loss: 1.104580]\n",
      "epoch:1 step:1597 [D loss: 0.647907, acc.: 63.28%] [G loss: 1.064924]\n",
      "epoch:1 step:1598 [D loss: 0.707370, acc.: 57.81%] [G loss: 1.044018]\n",
      "epoch:1 step:1599 [D loss: 0.709943, acc.: 57.81%] [G loss: 1.116789]\n",
      "epoch:1 step:1600 [D loss: 0.618056, acc.: 64.84%] [G loss: 1.091819]\n",
      "##############\n",
      "[2.84329614 2.03433179 1.92116177 2.96216555 1.1840024  5.36455011\n",
      " 2.24212313 3.04579306 3.65246666 4.3704979 ]\n",
      "##########\n",
      "epoch:1 step:1601 [D loss: 0.684952, acc.: 54.69%] [G loss: 1.050782]\n",
      "epoch:1 step:1602 [D loss: 0.702722, acc.: 57.03%] [G loss: 1.099540]\n",
      "epoch:1 step:1603 [D loss: 0.585575, acc.: 71.88%] [G loss: 1.096942]\n",
      "epoch:1 step:1604 [D loss: 0.660147, acc.: 60.94%] [G loss: 1.178521]\n",
      "epoch:1 step:1605 [D loss: 0.728873, acc.: 51.56%] [G loss: 1.136176]\n",
      "epoch:1 step:1606 [D loss: 0.699733, acc.: 60.94%] [G loss: 1.026513]\n",
      "epoch:1 step:1607 [D loss: 0.611035, acc.: 64.84%] [G loss: 1.100421]\n",
      "epoch:1 step:1608 [D loss: 0.492505, acc.: 78.91%] [G loss: 1.329623]\n",
      "epoch:1 step:1609 [D loss: 0.624096, acc.: 63.28%] [G loss: 1.363262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1610 [D loss: 0.726941, acc.: 53.12%] [G loss: 0.955221]\n",
      "epoch:1 step:1611 [D loss: 0.766553, acc.: 56.25%] [G loss: 1.068545]\n",
      "epoch:1 step:1612 [D loss: 0.747437, acc.: 56.25%] [G loss: 1.193033]\n",
      "epoch:1 step:1613 [D loss: 0.660881, acc.: 62.50%] [G loss: 1.194977]\n",
      "epoch:1 step:1614 [D loss: 0.687077, acc.: 55.47%] [G loss: 1.238147]\n",
      "epoch:1 step:1615 [D loss: 0.680086, acc.: 60.94%] [G loss: 1.057073]\n",
      "epoch:1 step:1616 [D loss: 0.624948, acc.: 64.06%] [G loss: 1.138043]\n",
      "epoch:1 step:1617 [D loss: 0.646003, acc.: 62.50%] [G loss: 1.070812]\n",
      "epoch:1 step:1618 [D loss: 0.655591, acc.: 61.72%] [G loss: 1.088670]\n",
      "epoch:1 step:1619 [D loss: 0.551476, acc.: 70.31%] [G loss: 1.339092]\n",
      "epoch:1 step:1620 [D loss: 0.646308, acc.: 60.16%] [G loss: 1.125482]\n",
      "epoch:1 step:1621 [D loss: 0.567632, acc.: 71.09%] [G loss: 1.169361]\n",
      "epoch:1 step:1622 [D loss: 0.669814, acc.: 63.28%] [G loss: 1.081840]\n",
      "epoch:1 step:1623 [D loss: 0.700640, acc.: 60.94%] [G loss: 1.301861]\n",
      "epoch:1 step:1624 [D loss: 0.620358, acc.: 63.28%] [G loss: 1.044552]\n",
      "epoch:1 step:1625 [D loss: 0.645193, acc.: 65.62%] [G loss: 1.353654]\n",
      "epoch:1 step:1626 [D loss: 0.655597, acc.: 64.84%] [G loss: 1.363970]\n",
      "epoch:1 step:1627 [D loss: 0.696935, acc.: 55.47%] [G loss: 1.226686]\n",
      "epoch:1 step:1628 [D loss: 0.679329, acc.: 61.72%] [G loss: 1.217697]\n",
      "epoch:1 step:1629 [D loss: 0.667859, acc.: 63.28%] [G loss: 1.129481]\n",
      "epoch:1 step:1630 [D loss: 0.706163, acc.: 58.59%] [G loss: 1.156245]\n",
      "epoch:1 step:1631 [D loss: 0.611027, acc.: 68.75%] [G loss: 0.991848]\n",
      "epoch:1 step:1632 [D loss: 0.637797, acc.: 62.50%] [G loss: 1.048303]\n",
      "epoch:1 step:1633 [D loss: 0.606572, acc.: 67.19%] [G loss: 1.151270]\n",
      "epoch:1 step:1634 [D loss: 0.785011, acc.: 46.88%] [G loss: 0.956568]\n",
      "epoch:1 step:1635 [D loss: 0.596877, acc.: 64.06%] [G loss: 1.445405]\n",
      "epoch:1 step:1636 [D loss: 0.661037, acc.: 61.72%] [G loss: 1.164420]\n",
      "epoch:1 step:1637 [D loss: 0.621864, acc.: 64.06%] [G loss: 1.149533]\n",
      "epoch:1 step:1638 [D loss: 0.570287, acc.: 69.53%] [G loss: 1.298423]\n",
      "epoch:1 step:1639 [D loss: 0.593691, acc.: 71.09%] [G loss: 1.151260]\n",
      "epoch:1 step:1640 [D loss: 0.633477, acc.: 64.84%] [G loss: 1.184243]\n",
      "epoch:1 step:1641 [D loss: 0.695514, acc.: 57.03%] [G loss: 1.149784]\n",
      "epoch:1 step:1642 [D loss: 0.607474, acc.: 71.09%] [G loss: 1.111760]\n",
      "epoch:1 step:1643 [D loss: 0.696803, acc.: 64.06%] [G loss: 1.095412]\n",
      "epoch:1 step:1644 [D loss: 0.665326, acc.: 60.16%] [G loss: 1.113165]\n",
      "epoch:1 step:1645 [D loss: 0.618358, acc.: 64.84%] [G loss: 1.255823]\n",
      "epoch:1 step:1646 [D loss: 0.831179, acc.: 50.78%] [G loss: 1.087594]\n",
      "epoch:1 step:1647 [D loss: 0.603751, acc.: 67.19%] [G loss: 1.254043]\n",
      "epoch:1 step:1648 [D loss: 0.599437, acc.: 73.44%] [G loss: 1.102911]\n",
      "epoch:1 step:1649 [D loss: 0.605822, acc.: 67.97%] [G loss: 1.245511]\n",
      "epoch:1 step:1650 [D loss: 0.716087, acc.: 56.25%] [G loss: 1.209464]\n",
      "epoch:1 step:1651 [D loss: 0.744655, acc.: 55.47%] [G loss: 1.042971]\n",
      "epoch:1 step:1652 [D loss: 0.707333, acc.: 60.16%] [G loss: 1.226961]\n",
      "epoch:1 step:1653 [D loss: 0.656775, acc.: 61.72%] [G loss: 1.115041]\n",
      "epoch:1 step:1654 [D loss: 0.632356, acc.: 62.50%] [G loss: 1.270664]\n",
      "epoch:1 step:1655 [D loss: 0.600758, acc.: 69.53%] [G loss: 1.124111]\n",
      "epoch:1 step:1656 [D loss: 0.673062, acc.: 56.25%] [G loss: 1.109036]\n",
      "epoch:1 step:1657 [D loss: 0.655809, acc.: 63.28%] [G loss: 1.184375]\n",
      "epoch:1 step:1658 [D loss: 0.678472, acc.: 56.25%] [G loss: 1.159818]\n",
      "epoch:1 step:1659 [D loss: 0.645723, acc.: 64.06%] [G loss: 1.120474]\n",
      "epoch:1 step:1660 [D loss: 0.597068, acc.: 68.75%] [G loss: 1.254895]\n",
      "epoch:1 step:1661 [D loss: 0.665651, acc.: 60.94%] [G loss: 1.133441]\n",
      "epoch:1 step:1662 [D loss: 0.602773, acc.: 70.31%] [G loss: 1.291858]\n",
      "epoch:1 step:1663 [D loss: 0.678945, acc.: 62.50%] [G loss: 1.066058]\n",
      "epoch:1 step:1664 [D loss: 0.660845, acc.: 62.50%] [G loss: 1.072375]\n",
      "epoch:1 step:1665 [D loss: 0.738538, acc.: 50.00%] [G loss: 0.926611]\n",
      "epoch:1 step:1666 [D loss: 0.651846, acc.: 64.06%] [G loss: 1.014609]\n",
      "epoch:1 step:1667 [D loss: 0.675036, acc.: 61.72%] [G loss: 1.066692]\n",
      "epoch:1 step:1668 [D loss: 0.678683, acc.: 62.50%] [G loss: 1.097384]\n",
      "epoch:1 step:1669 [D loss: 0.666338, acc.: 59.38%] [G loss: 1.002842]\n",
      "epoch:1 step:1670 [D loss: 0.664078, acc.: 60.16%] [G loss: 1.169225]\n",
      "epoch:1 step:1671 [D loss: 0.637283, acc.: 64.06%] [G loss: 1.088473]\n",
      "epoch:1 step:1672 [D loss: 0.508713, acc.: 73.44%] [G loss: 1.298620]\n",
      "epoch:1 step:1673 [D loss: 0.698504, acc.: 53.91%] [G loss: 1.155072]\n",
      "epoch:1 step:1674 [D loss: 0.686780, acc.: 57.81%] [G loss: 1.126291]\n",
      "epoch:1 step:1675 [D loss: 0.715956, acc.: 57.81%] [G loss: 1.111322]\n",
      "epoch:1 step:1676 [D loss: 0.750210, acc.: 54.69%] [G loss: 0.960379]\n",
      "epoch:1 step:1677 [D loss: 0.570630, acc.: 71.09%] [G loss: 1.309741]\n",
      "epoch:1 step:1678 [D loss: 0.636141, acc.: 60.94%] [G loss: 1.188895]\n",
      "epoch:1 step:1679 [D loss: 0.687024, acc.: 61.72%] [G loss: 1.023999]\n",
      "epoch:1 step:1680 [D loss: 0.730300, acc.: 54.69%] [G loss: 1.046234]\n",
      "epoch:1 step:1681 [D loss: 0.670164, acc.: 67.19%] [G loss: 1.158873]\n",
      "epoch:1 step:1682 [D loss: 0.654065, acc.: 58.59%] [G loss: 0.942484]\n",
      "epoch:1 step:1683 [D loss: 0.630089, acc.: 62.50%] [G loss: 1.021674]\n",
      "epoch:1 step:1684 [D loss: 0.645911, acc.: 64.06%] [G loss: 1.085086]\n",
      "epoch:1 step:1685 [D loss: 0.647106, acc.: 63.28%] [G loss: 1.096986]\n",
      "epoch:1 step:1686 [D loss: 0.654719, acc.: 60.94%] [G loss: 1.155264]\n",
      "epoch:1 step:1687 [D loss: 0.721332, acc.: 53.91%] [G loss: 1.232599]\n",
      "epoch:1 step:1688 [D loss: 0.674271, acc.: 58.59%] [G loss: 1.262487]\n",
      "epoch:1 step:1689 [D loss: 0.682900, acc.: 61.72%] [G loss: 1.081980]\n",
      "epoch:1 step:1690 [D loss: 0.601879, acc.: 66.41%] [G loss: 1.191730]\n",
      "epoch:1 step:1691 [D loss: 0.710587, acc.: 53.91%] [G loss: 1.219432]\n",
      "epoch:1 step:1692 [D loss: 0.693407, acc.: 57.03%] [G loss: 1.016389]\n",
      "epoch:1 step:1693 [D loss: 0.677197, acc.: 62.50%] [G loss: 1.167460]\n",
      "epoch:1 step:1694 [D loss: 0.603756, acc.: 66.41%] [G loss: 1.214315]\n",
      "epoch:1 step:1695 [D loss: 0.652174, acc.: 64.06%] [G loss: 1.194839]\n",
      "epoch:1 step:1696 [D loss: 0.531257, acc.: 75.78%] [G loss: 1.202128]\n",
      "epoch:1 step:1697 [D loss: 0.598625, acc.: 67.19%] [G loss: 1.182679]\n",
      "epoch:1 step:1698 [D loss: 0.646204, acc.: 61.72%] [G loss: 1.110980]\n",
      "epoch:1 step:1699 [D loss: 0.716320, acc.: 51.56%] [G loss: 1.229988]\n",
      "epoch:1 step:1700 [D loss: 0.593125, acc.: 69.53%] [G loss: 1.327645]\n",
      "epoch:1 step:1701 [D loss: 0.727586, acc.: 55.47%] [G loss: 0.961564]\n",
      "epoch:1 step:1702 [D loss: 0.679869, acc.: 55.47%] [G loss: 1.151302]\n",
      "epoch:1 step:1703 [D loss: 0.643974, acc.: 67.19%] [G loss: 1.187600]\n",
      "epoch:1 step:1704 [D loss: 0.624142, acc.: 64.06%] [G loss: 1.283915]\n",
      "epoch:1 step:1705 [D loss: 0.705020, acc.: 59.38%] [G loss: 1.028210]\n",
      "epoch:1 step:1706 [D loss: 0.651547, acc.: 63.28%] [G loss: 1.090135]\n",
      "epoch:1 step:1707 [D loss: 0.628015, acc.: 64.06%] [G loss: 0.997391]\n",
      "epoch:1 step:1708 [D loss: 0.652869, acc.: 56.25%] [G loss: 1.118903]\n",
      "epoch:1 step:1709 [D loss: 0.632452, acc.: 62.50%] [G loss: 1.067317]\n",
      "epoch:1 step:1710 [D loss: 0.685497, acc.: 62.50%] [G loss: 1.025219]\n",
      "epoch:1 step:1711 [D loss: 0.615984, acc.: 67.19%] [G loss: 1.195712]\n",
      "epoch:1 step:1712 [D loss: 0.664532, acc.: 59.38%] [G loss: 1.006051]\n",
      "epoch:1 step:1713 [D loss: 0.598276, acc.: 67.19%] [G loss: 1.311399]\n",
      "epoch:1 step:1714 [D loss: 0.806398, acc.: 48.44%] [G loss: 1.035261]\n",
      "epoch:1 step:1715 [D loss: 0.717957, acc.: 55.47%] [G loss: 1.047345]\n",
      "epoch:1 step:1716 [D loss: 0.609330, acc.: 63.28%] [G loss: 1.189036]\n",
      "epoch:1 step:1717 [D loss: 0.705299, acc.: 54.69%] [G loss: 1.184350]\n",
      "epoch:1 step:1718 [D loss: 0.570703, acc.: 70.31%] [G loss: 1.210084]\n",
      "epoch:1 step:1719 [D loss: 0.585556, acc.: 72.66%] [G loss: 1.093089]\n",
      "epoch:1 step:1720 [D loss: 0.640807, acc.: 65.62%] [G loss: 1.167409]\n",
      "epoch:1 step:1721 [D loss: 0.721031, acc.: 54.69%] [G loss: 1.075153]\n",
      "epoch:1 step:1722 [D loss: 0.686376, acc.: 56.25%] [G loss: 1.017866]\n",
      "epoch:1 step:1723 [D loss: 0.698289, acc.: 57.81%] [G loss: 1.186763]\n",
      "epoch:1 step:1724 [D loss: 0.610435, acc.: 67.19%] [G loss: 1.207280]\n",
      "epoch:1 step:1725 [D loss: 0.587920, acc.: 70.31%] [G loss: 1.224540]\n",
      "epoch:1 step:1726 [D loss: 0.639334, acc.: 65.62%] [G loss: 1.097636]\n",
      "epoch:1 step:1727 [D loss: 0.629081, acc.: 67.97%] [G loss: 1.227046]\n",
      "epoch:1 step:1728 [D loss: 0.602935, acc.: 68.75%] [G loss: 1.232513]\n",
      "epoch:1 step:1729 [D loss: 0.591417, acc.: 68.75%] [G loss: 1.360795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1730 [D loss: 0.699243, acc.: 52.34%] [G loss: 1.095358]\n",
      "epoch:1 step:1731 [D loss: 0.675944, acc.: 61.72%] [G loss: 1.112922]\n",
      "epoch:1 step:1732 [D loss: 0.617462, acc.: 71.88%] [G loss: 1.261947]\n",
      "epoch:1 step:1733 [D loss: 0.688091, acc.: 60.16%] [G loss: 1.113071]\n",
      "epoch:1 step:1734 [D loss: 0.740717, acc.: 57.03%] [G loss: 1.212701]\n",
      "epoch:1 step:1735 [D loss: 0.602454, acc.: 66.41%] [G loss: 1.283894]\n",
      "epoch:1 step:1736 [D loss: 0.598742, acc.: 67.19%] [G loss: 1.226860]\n",
      "epoch:1 step:1737 [D loss: 0.732231, acc.: 55.47%] [G loss: 1.005176]\n",
      "epoch:1 step:1738 [D loss: 0.706177, acc.: 57.81%] [G loss: 1.169449]\n",
      "epoch:1 step:1739 [D loss: 0.548557, acc.: 71.09%] [G loss: 1.288610]\n",
      "epoch:1 step:1740 [D loss: 0.735463, acc.: 57.03%] [G loss: 1.097970]\n",
      "epoch:1 step:1741 [D loss: 0.693835, acc.: 60.16%] [G loss: 1.087196]\n",
      "epoch:1 step:1742 [D loss: 0.741380, acc.: 51.56%] [G loss: 0.946067]\n",
      "epoch:1 step:1743 [D loss: 0.625538, acc.: 64.84%] [G loss: 1.275761]\n",
      "epoch:1 step:1744 [D loss: 0.576779, acc.: 67.19%] [G loss: 1.106964]\n",
      "epoch:1 step:1745 [D loss: 0.636911, acc.: 65.62%] [G loss: 1.118214]\n",
      "epoch:1 step:1746 [D loss: 0.696656, acc.: 59.38%] [G loss: 1.234573]\n",
      "epoch:1 step:1747 [D loss: 0.682453, acc.: 59.38%] [G loss: 1.114626]\n",
      "epoch:1 step:1748 [D loss: 0.591510, acc.: 64.84%] [G loss: 1.100846]\n",
      "epoch:1 step:1749 [D loss: 0.658162, acc.: 60.16%] [G loss: 1.046340]\n",
      "epoch:1 step:1750 [D loss: 0.622329, acc.: 64.06%] [G loss: 1.168893]\n",
      "epoch:1 step:1751 [D loss: 0.675391, acc.: 59.38%] [G loss: 1.139653]\n",
      "epoch:1 step:1752 [D loss: 0.621123, acc.: 61.72%] [G loss: 1.001805]\n",
      "epoch:1 step:1753 [D loss: 0.613853, acc.: 67.19%] [G loss: 1.026296]\n",
      "epoch:1 step:1754 [D loss: 0.580125, acc.: 69.53%] [G loss: 1.038053]\n",
      "epoch:1 step:1755 [D loss: 0.745567, acc.: 52.34%] [G loss: 1.127015]\n",
      "epoch:1 step:1756 [D loss: 0.609174, acc.: 70.31%] [G loss: 0.994023]\n",
      "epoch:1 step:1757 [D loss: 0.617071, acc.: 64.06%] [G loss: 1.130525]\n",
      "epoch:1 step:1758 [D loss: 0.583038, acc.: 69.53%] [G loss: 1.169576]\n",
      "epoch:1 step:1759 [D loss: 0.573319, acc.: 69.53%] [G loss: 1.083117]\n",
      "epoch:1 step:1760 [D loss: 0.714567, acc.: 53.91%] [G loss: 1.131897]\n",
      "epoch:1 step:1761 [D loss: 0.731045, acc.: 53.12%] [G loss: 1.147781]\n",
      "epoch:1 step:1762 [D loss: 0.602812, acc.: 67.19%] [G loss: 1.039260]\n",
      "epoch:1 step:1763 [D loss: 0.613770, acc.: 67.97%] [G loss: 0.957310]\n",
      "epoch:1 step:1764 [D loss: 0.742056, acc.: 57.81%] [G loss: 1.071548]\n",
      "epoch:1 step:1765 [D loss: 0.727212, acc.: 53.12%] [G loss: 1.099557]\n",
      "epoch:1 step:1766 [D loss: 0.668315, acc.: 58.59%] [G loss: 1.011855]\n",
      "epoch:1 step:1767 [D loss: 0.685055, acc.: 57.03%] [G loss: 1.091182]\n",
      "epoch:1 step:1768 [D loss: 0.611769, acc.: 62.50%] [G loss: 1.023851]\n",
      "epoch:1 step:1769 [D loss: 0.675461, acc.: 56.25%] [G loss: 1.114233]\n",
      "epoch:1 step:1770 [D loss: 0.678871, acc.: 61.72%] [G loss: 1.147723]\n",
      "epoch:1 step:1771 [D loss: 0.626038, acc.: 65.62%] [G loss: 0.974629]\n",
      "epoch:1 step:1772 [D loss: 0.733376, acc.: 56.25%] [G loss: 1.025795]\n",
      "epoch:1 step:1773 [D loss: 0.616688, acc.: 64.84%] [G loss: 1.110928]\n",
      "epoch:1 step:1774 [D loss: 0.649484, acc.: 64.84%] [G loss: 1.170501]\n",
      "epoch:1 step:1775 [D loss: 0.664086, acc.: 59.38%] [G loss: 1.089775]\n",
      "epoch:1 step:1776 [D loss: 0.648328, acc.: 59.38%] [G loss: 1.108758]\n",
      "epoch:1 step:1777 [D loss: 0.558892, acc.: 71.09%] [G loss: 1.225580]\n",
      "epoch:1 step:1778 [D loss: 0.557263, acc.: 70.31%] [G loss: 1.075688]\n",
      "epoch:1 step:1779 [D loss: 0.650844, acc.: 58.59%] [G loss: 1.031078]\n",
      "epoch:1 step:1780 [D loss: 0.679283, acc.: 57.03%] [G loss: 1.155492]\n",
      "epoch:1 step:1781 [D loss: 0.700277, acc.: 54.69%] [G loss: 1.030851]\n",
      "epoch:1 step:1782 [D loss: 0.711671, acc.: 57.81%] [G loss: 1.165252]\n",
      "epoch:1 step:1783 [D loss: 0.693828, acc.: 61.72%] [G loss: 1.134840]\n",
      "epoch:1 step:1784 [D loss: 0.663674, acc.: 61.72%] [G loss: 1.182281]\n",
      "epoch:1 step:1785 [D loss: 0.651400, acc.: 64.84%] [G loss: 1.219569]\n",
      "epoch:1 step:1786 [D loss: 0.604944, acc.: 68.75%] [G loss: 1.167112]\n",
      "epoch:1 step:1787 [D loss: 0.730445, acc.: 52.34%] [G loss: 1.266649]\n",
      "epoch:1 step:1788 [D loss: 0.692732, acc.: 58.59%] [G loss: 1.146880]\n",
      "epoch:1 step:1789 [D loss: 0.653433, acc.: 62.50%] [G loss: 1.261193]\n",
      "epoch:1 step:1790 [D loss: 0.629453, acc.: 64.84%] [G loss: 1.141321]\n",
      "epoch:1 step:1791 [D loss: 0.693241, acc.: 56.25%] [G loss: 1.120908]\n",
      "epoch:1 step:1792 [D loss: 0.756004, acc.: 48.44%] [G loss: 0.901210]\n",
      "epoch:1 step:1793 [D loss: 0.721740, acc.: 58.59%] [G loss: 1.240861]\n",
      "epoch:1 step:1794 [D loss: 0.670302, acc.: 59.38%] [G loss: 1.181744]\n",
      "epoch:1 step:1795 [D loss: 0.624552, acc.: 65.62%] [G loss: 1.165981]\n",
      "epoch:1 step:1796 [D loss: 0.614078, acc.: 64.84%] [G loss: 0.998820]\n",
      "epoch:1 step:1797 [D loss: 0.561829, acc.: 73.44%] [G loss: 1.188624]\n",
      "epoch:1 step:1798 [D loss: 0.644704, acc.: 67.19%] [G loss: 1.138481]\n",
      "epoch:1 step:1799 [D loss: 0.662260, acc.: 61.72%] [G loss: 1.102743]\n",
      "epoch:1 step:1800 [D loss: 0.757967, acc.: 53.91%] [G loss: 1.129027]\n",
      "##############\n",
      "[2.77517156 2.04078884 1.94328352 2.68832266 0.96655545 6.24059571\n",
      " 2.32709814 3.00066428 3.69913637 5.05308367]\n",
      "##########\n",
      "epoch:1 step:1801 [D loss: 0.652230, acc.: 64.84%] [G loss: 1.100472]\n",
      "epoch:1 step:1802 [D loss: 0.618321, acc.: 63.28%] [G loss: 1.142336]\n",
      "epoch:1 step:1803 [D loss: 0.552285, acc.: 74.22%] [G loss: 1.284352]\n",
      "epoch:1 step:1804 [D loss: 0.629802, acc.: 65.62%] [G loss: 1.055174]\n",
      "epoch:1 step:1805 [D loss: 0.640639, acc.: 63.28%] [G loss: 1.194346]\n",
      "epoch:1 step:1806 [D loss: 0.598431, acc.: 71.09%] [G loss: 1.231540]\n",
      "epoch:1 step:1807 [D loss: 0.661950, acc.: 62.50%] [G loss: 1.033329]\n",
      "epoch:1 step:1808 [D loss: 0.589663, acc.: 67.97%] [G loss: 1.200169]\n",
      "epoch:1 step:1809 [D loss: 0.668472, acc.: 63.28%] [G loss: 1.110225]\n",
      "epoch:1 step:1810 [D loss: 0.657656, acc.: 64.06%] [G loss: 1.083321]\n",
      "epoch:1 step:1811 [D loss: 0.777213, acc.: 46.88%] [G loss: 1.169317]\n",
      "epoch:1 step:1812 [D loss: 0.675084, acc.: 61.72%] [G loss: 1.118173]\n",
      "epoch:1 step:1813 [D loss: 0.633392, acc.: 60.94%] [G loss: 1.258018]\n",
      "epoch:1 step:1814 [D loss: 0.636888, acc.: 67.97%] [G loss: 1.103795]\n",
      "epoch:1 step:1815 [D loss: 0.681632, acc.: 57.03%] [G loss: 1.140952]\n",
      "epoch:1 step:1816 [D loss: 0.737884, acc.: 50.00%] [G loss: 1.008144]\n",
      "epoch:1 step:1817 [D loss: 0.634967, acc.: 64.84%] [G loss: 1.129783]\n",
      "epoch:1 step:1818 [D loss: 0.680001, acc.: 58.59%] [G loss: 1.186000]\n",
      "epoch:1 step:1819 [D loss: 0.633814, acc.: 60.16%] [G loss: 1.141055]\n",
      "epoch:1 step:1820 [D loss: 0.682180, acc.: 59.38%] [G loss: 1.153238]\n",
      "epoch:1 step:1821 [D loss: 0.636403, acc.: 63.28%] [G loss: 1.086358]\n",
      "epoch:1 step:1822 [D loss: 0.614254, acc.: 66.41%] [G loss: 1.099243]\n",
      "epoch:1 step:1823 [D loss: 0.653863, acc.: 64.06%] [G loss: 1.047394]\n",
      "epoch:1 step:1824 [D loss: 0.621104, acc.: 61.72%] [G loss: 1.223855]\n",
      "epoch:1 step:1825 [D loss: 0.663401, acc.: 57.03%] [G loss: 1.143830]\n",
      "epoch:1 step:1826 [D loss: 0.698466, acc.: 52.34%] [G loss: 1.203921]\n",
      "epoch:1 step:1827 [D loss: 0.690842, acc.: 57.03%] [G loss: 1.292591]\n",
      "epoch:1 step:1828 [D loss: 0.672528, acc.: 60.94%] [G loss: 1.153797]\n",
      "epoch:1 step:1829 [D loss: 0.689641, acc.: 61.72%] [G loss: 1.118647]\n",
      "epoch:1 step:1830 [D loss: 0.673606, acc.: 57.03%] [G loss: 0.988431]\n",
      "epoch:1 step:1831 [D loss: 0.629379, acc.: 64.06%] [G loss: 1.270436]\n",
      "epoch:1 step:1832 [D loss: 0.595868, acc.: 68.75%] [G loss: 1.206527]\n",
      "epoch:1 step:1833 [D loss: 0.578616, acc.: 73.44%] [G loss: 1.149949]\n",
      "epoch:1 step:1834 [D loss: 0.680112, acc.: 61.72%] [G loss: 1.066087]\n",
      "epoch:1 step:1835 [D loss: 0.636406, acc.: 64.06%] [G loss: 1.103956]\n",
      "epoch:1 step:1836 [D loss: 0.717278, acc.: 57.03%] [G loss: 1.220935]\n",
      "epoch:1 step:1837 [D loss: 0.573694, acc.: 71.88%] [G loss: 1.138441]\n",
      "epoch:1 step:1838 [D loss: 0.698353, acc.: 57.81%] [G loss: 1.065123]\n",
      "epoch:1 step:1839 [D loss: 0.697328, acc.: 58.59%] [G loss: 0.929990]\n",
      "epoch:1 step:1840 [D loss: 0.624400, acc.: 66.41%] [G loss: 1.151910]\n",
      "epoch:1 step:1841 [D loss: 0.556156, acc.: 71.09%] [G loss: 1.270603]\n",
      "epoch:1 step:1842 [D loss: 0.600311, acc.: 66.41%] [G loss: 1.287804]\n",
      "epoch:1 step:1843 [D loss: 0.700773, acc.: 57.81%] [G loss: 1.183318]\n",
      "epoch:1 step:1844 [D loss: 0.660219, acc.: 58.59%] [G loss: 1.209098]\n",
      "epoch:1 step:1845 [D loss: 0.616814, acc.: 65.62%] [G loss: 1.143944]\n",
      "epoch:1 step:1846 [D loss: 0.692772, acc.: 56.25%] [G loss: 1.285681]\n",
      "epoch:1 step:1847 [D loss: 0.664543, acc.: 59.38%] [G loss: 1.111938]\n",
      "epoch:1 step:1848 [D loss: 0.674821, acc.: 60.16%] [G loss: 1.066195]\n",
      "epoch:1 step:1849 [D loss: 0.664736, acc.: 56.25%] [G loss: 1.232370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1850 [D loss: 0.650836, acc.: 59.38%] [G loss: 1.075201]\n",
      "epoch:1 step:1851 [D loss: 0.627376, acc.: 58.59%] [G loss: 1.075001]\n",
      "epoch:1 step:1852 [D loss: 0.552326, acc.: 73.44%] [G loss: 1.177701]\n",
      "epoch:1 step:1853 [D loss: 0.618326, acc.: 65.62%] [G loss: 1.064447]\n",
      "epoch:1 step:1854 [D loss: 0.596719, acc.: 70.31%] [G loss: 0.961899]\n",
      "epoch:1 step:1855 [D loss: 0.629961, acc.: 60.16%] [G loss: 0.982627]\n",
      "epoch:1 step:1856 [D loss: 0.651886, acc.: 63.28%] [G loss: 1.113816]\n",
      "epoch:1 step:1857 [D loss: 0.643207, acc.: 62.50%] [G loss: 1.165180]\n",
      "epoch:1 step:1858 [D loss: 0.665553, acc.: 60.16%] [G loss: 1.183297]\n",
      "epoch:1 step:1859 [D loss: 0.583011, acc.: 67.19%] [G loss: 1.174230]\n",
      "epoch:1 step:1860 [D loss: 0.627475, acc.: 64.84%] [G loss: 1.139952]\n",
      "epoch:1 step:1861 [D loss: 0.641326, acc.: 64.84%] [G loss: 1.092169]\n",
      "epoch:1 step:1862 [D loss: 0.720494, acc.: 53.12%] [G loss: 0.992293]\n",
      "epoch:1 step:1863 [D loss: 0.670421, acc.: 54.69%] [G loss: 1.039569]\n",
      "epoch:1 step:1864 [D loss: 0.666966, acc.: 58.59%] [G loss: 0.939984]\n",
      "epoch:1 step:1865 [D loss: 0.637210, acc.: 60.16%] [G loss: 1.159487]\n",
      "epoch:1 step:1866 [D loss: 0.650184, acc.: 60.94%] [G loss: 1.072878]\n",
      "epoch:1 step:1867 [D loss: 0.567165, acc.: 67.97%] [G loss: 1.330339]\n",
      "epoch:1 step:1868 [D loss: 0.716795, acc.: 57.03%] [G loss: 1.068877]\n",
      "epoch:1 step:1869 [D loss: 0.619812, acc.: 66.41%] [G loss: 1.080778]\n",
      "epoch:1 step:1870 [D loss: 0.716232, acc.: 53.91%] [G loss: 0.868213]\n",
      "epoch:1 step:1871 [D loss: 0.551164, acc.: 74.22%] [G loss: 1.271090]\n",
      "epoch:1 step:1872 [D loss: 0.650761, acc.: 62.50%] [G loss: 1.078884]\n",
      "epoch:1 step:1873 [D loss: 0.605659, acc.: 68.75%] [G loss: 1.106667]\n",
      "epoch:1 step:1874 [D loss: 0.705454, acc.: 55.47%] [G loss: 1.060915]\n",
      "epoch:2 step:1875 [D loss: 0.615737, acc.: 64.84%] [G loss: 1.096817]\n",
      "epoch:2 step:1876 [D loss: 0.685366, acc.: 60.94%] [G loss: 1.045903]\n",
      "epoch:2 step:1877 [D loss: 0.685394, acc.: 59.38%] [G loss: 1.068007]\n",
      "epoch:2 step:1878 [D loss: 0.715825, acc.: 61.72%] [G loss: 1.096174]\n",
      "epoch:2 step:1879 [D loss: 0.706790, acc.: 53.12%] [G loss: 1.127717]\n",
      "epoch:2 step:1880 [D loss: 0.715636, acc.: 53.91%] [G loss: 1.103887]\n",
      "epoch:2 step:1881 [D loss: 0.662153, acc.: 59.38%] [G loss: 1.028491]\n",
      "epoch:2 step:1882 [D loss: 0.616211, acc.: 64.84%] [G loss: 1.025285]\n",
      "epoch:2 step:1883 [D loss: 0.618399, acc.: 65.62%] [G loss: 1.139914]\n",
      "epoch:2 step:1884 [D loss: 0.693225, acc.: 55.47%] [G loss: 1.003388]\n",
      "epoch:2 step:1885 [D loss: 0.574553, acc.: 67.97%] [G loss: 1.236642]\n",
      "epoch:2 step:1886 [D loss: 0.659520, acc.: 60.94%] [G loss: 1.179208]\n",
      "epoch:2 step:1887 [D loss: 0.695557, acc.: 58.59%] [G loss: 1.083249]\n",
      "epoch:2 step:1888 [D loss: 0.708524, acc.: 54.69%] [G loss: 1.036724]\n",
      "epoch:2 step:1889 [D loss: 0.684471, acc.: 61.72%] [G loss: 0.974678]\n",
      "epoch:2 step:1890 [D loss: 0.671616, acc.: 55.47%] [G loss: 1.070762]\n",
      "epoch:2 step:1891 [D loss: 0.620708, acc.: 64.84%] [G loss: 1.384890]\n",
      "epoch:2 step:1892 [D loss: 0.646662, acc.: 62.50%] [G loss: 1.189197]\n",
      "epoch:2 step:1893 [D loss: 0.625528, acc.: 64.06%] [G loss: 1.038950]\n",
      "epoch:2 step:1894 [D loss: 0.653580, acc.: 67.19%] [G loss: 1.082282]\n",
      "epoch:2 step:1895 [D loss: 0.605895, acc.: 68.75%] [G loss: 1.252982]\n",
      "epoch:2 step:1896 [D loss: 0.638319, acc.: 62.50%] [G loss: 1.211538]\n",
      "epoch:2 step:1897 [D loss: 0.629484, acc.: 61.72%] [G loss: 1.145375]\n",
      "epoch:2 step:1898 [D loss: 0.619601, acc.: 64.06%] [G loss: 1.208981]\n",
      "epoch:2 step:1899 [D loss: 0.581340, acc.: 75.00%] [G loss: 1.153689]\n",
      "epoch:2 step:1900 [D loss: 0.710574, acc.: 53.12%] [G loss: 1.087175]\n",
      "epoch:2 step:1901 [D loss: 0.606260, acc.: 70.31%] [G loss: 1.032760]\n",
      "epoch:2 step:1902 [D loss: 0.631579, acc.: 64.84%] [G loss: 1.153743]\n",
      "epoch:2 step:1903 [D loss: 0.694143, acc.: 57.03%] [G loss: 1.189280]\n",
      "epoch:2 step:1904 [D loss: 0.598087, acc.: 69.53%] [G loss: 1.209307]\n",
      "epoch:2 step:1905 [D loss: 0.698949, acc.: 55.47%] [G loss: 0.913665]\n",
      "epoch:2 step:1906 [D loss: 0.620464, acc.: 67.97%] [G loss: 1.049898]\n",
      "epoch:2 step:1907 [D loss: 0.672433, acc.: 56.25%] [G loss: 0.999890]\n",
      "epoch:2 step:1908 [D loss: 0.605256, acc.: 71.09%] [G loss: 1.115253]\n",
      "epoch:2 step:1909 [D loss: 0.547145, acc.: 72.66%] [G loss: 1.206171]\n",
      "epoch:2 step:1910 [D loss: 0.558898, acc.: 70.31%] [G loss: 1.095646]\n",
      "epoch:2 step:1911 [D loss: 0.585165, acc.: 70.31%] [G loss: 1.156895]\n",
      "epoch:2 step:1912 [D loss: 0.714431, acc.: 57.81%] [G loss: 1.032846]\n",
      "epoch:2 step:1913 [D loss: 0.649896, acc.: 65.62%] [G loss: 1.009643]\n",
      "epoch:2 step:1914 [D loss: 0.684801, acc.: 64.84%] [G loss: 1.004388]\n",
      "epoch:2 step:1915 [D loss: 0.543621, acc.: 70.31%] [G loss: 1.138274]\n",
      "epoch:2 step:1916 [D loss: 0.664121, acc.: 58.59%] [G loss: 1.034136]\n",
      "epoch:2 step:1917 [D loss: 0.702698, acc.: 53.91%] [G loss: 0.984478]\n",
      "epoch:2 step:1918 [D loss: 0.587084, acc.: 69.53%] [G loss: 1.021826]\n",
      "epoch:2 step:1919 [D loss: 0.695433, acc.: 60.16%] [G loss: 1.028496]\n",
      "epoch:2 step:1920 [D loss: 0.692476, acc.: 57.81%] [G loss: 1.085085]\n",
      "epoch:2 step:1921 [D loss: 0.756471, acc.: 50.78%] [G loss: 0.989842]\n",
      "epoch:2 step:1922 [D loss: 0.615324, acc.: 68.75%] [G loss: 1.318088]\n",
      "epoch:2 step:1923 [D loss: 0.660589, acc.: 59.38%] [G loss: 1.166280]\n",
      "epoch:2 step:1924 [D loss: 0.573718, acc.: 66.41%] [G loss: 1.207656]\n",
      "epoch:2 step:1925 [D loss: 0.644188, acc.: 64.06%] [G loss: 0.972587]\n",
      "epoch:2 step:1926 [D loss: 0.616581, acc.: 67.97%] [G loss: 1.111673]\n",
      "epoch:2 step:1927 [D loss: 0.619888, acc.: 62.50%] [G loss: 1.264145]\n",
      "epoch:2 step:1928 [D loss: 0.559591, acc.: 73.44%] [G loss: 1.158932]\n",
      "epoch:2 step:1929 [D loss: 0.674991, acc.: 57.81%] [G loss: 1.036927]\n",
      "epoch:2 step:1930 [D loss: 0.750465, acc.: 50.78%] [G loss: 0.957408]\n",
      "epoch:2 step:1931 [D loss: 0.688828, acc.: 57.81%] [G loss: 1.162296]\n",
      "epoch:2 step:1932 [D loss: 0.624249, acc.: 67.97%] [G loss: 0.995891]\n",
      "epoch:2 step:1933 [D loss: 0.681852, acc.: 59.38%] [G loss: 1.251868]\n",
      "epoch:2 step:1934 [D loss: 0.518378, acc.: 76.56%] [G loss: 1.112657]\n",
      "epoch:2 step:1935 [D loss: 0.652901, acc.: 64.84%] [G loss: 1.166772]\n",
      "epoch:2 step:1936 [D loss: 0.638090, acc.: 64.84%] [G loss: 1.020538]\n",
      "epoch:2 step:1937 [D loss: 0.622923, acc.: 66.41%] [G loss: 1.009152]\n",
      "epoch:2 step:1938 [D loss: 0.587235, acc.: 68.75%] [G loss: 1.262285]\n",
      "epoch:2 step:1939 [D loss: 0.755205, acc.: 56.25%] [G loss: 0.917305]\n",
      "epoch:2 step:1940 [D loss: 0.596539, acc.: 71.09%] [G loss: 1.115635]\n",
      "epoch:2 step:1941 [D loss: 0.610939, acc.: 64.84%] [G loss: 0.988491]\n",
      "epoch:2 step:1942 [D loss: 0.632457, acc.: 63.28%] [G loss: 1.180336]\n",
      "epoch:2 step:1943 [D loss: 0.688249, acc.: 63.28%] [G loss: 1.092256]\n",
      "epoch:2 step:1944 [D loss: 0.701965, acc.: 59.38%] [G loss: 1.092734]\n",
      "epoch:2 step:1945 [D loss: 0.639550, acc.: 69.53%] [G loss: 1.232953]\n",
      "epoch:2 step:1946 [D loss: 0.633002, acc.: 60.94%] [G loss: 1.162459]\n",
      "epoch:2 step:1947 [D loss: 0.595655, acc.: 64.84%] [G loss: 1.104957]\n",
      "epoch:2 step:1948 [D loss: 0.581315, acc.: 65.62%] [G loss: 1.174946]\n",
      "epoch:2 step:1949 [D loss: 0.650103, acc.: 63.28%] [G loss: 1.117712]\n",
      "epoch:2 step:1950 [D loss: 0.537426, acc.: 75.00%] [G loss: 1.111047]\n",
      "epoch:2 step:1951 [D loss: 0.604744, acc.: 68.75%] [G loss: 1.201835]\n",
      "epoch:2 step:1952 [D loss: 0.736341, acc.: 53.12%] [G loss: 1.100893]\n",
      "epoch:2 step:1953 [D loss: 0.707185, acc.: 57.03%] [G loss: 1.221711]\n",
      "epoch:2 step:1954 [D loss: 0.676552, acc.: 60.16%] [G loss: 0.986414]\n",
      "epoch:2 step:1955 [D loss: 0.671399, acc.: 62.50%] [G loss: 1.158228]\n",
      "epoch:2 step:1956 [D loss: 0.566934, acc.: 68.75%] [G loss: 1.096261]\n",
      "epoch:2 step:1957 [D loss: 0.696725, acc.: 56.25%] [G loss: 1.247713]\n",
      "epoch:2 step:1958 [D loss: 0.650863, acc.: 59.38%] [G loss: 1.054321]\n",
      "epoch:2 step:1959 [D loss: 0.636297, acc.: 63.28%] [G loss: 1.127696]\n",
      "epoch:2 step:1960 [D loss: 0.583433, acc.: 71.88%] [G loss: 1.147025]\n",
      "epoch:2 step:1961 [D loss: 0.585638, acc.: 64.84%] [G loss: 1.300153]\n",
      "epoch:2 step:1962 [D loss: 0.695833, acc.: 56.25%] [G loss: 1.147948]\n",
      "epoch:2 step:1963 [D loss: 0.637196, acc.: 61.72%] [G loss: 1.307303]\n",
      "epoch:2 step:1964 [D loss: 0.643754, acc.: 64.84%] [G loss: 1.111797]\n",
      "epoch:2 step:1965 [D loss: 0.673895, acc.: 58.59%] [G loss: 1.190738]\n",
      "epoch:2 step:1966 [D loss: 0.630165, acc.: 65.62%] [G loss: 1.089605]\n",
      "epoch:2 step:1967 [D loss: 0.647819, acc.: 64.06%] [G loss: 1.082451]\n",
      "epoch:2 step:1968 [D loss: 0.612564, acc.: 65.62%] [G loss: 1.211198]\n",
      "epoch:2 step:1969 [D loss: 0.650900, acc.: 63.28%] [G loss: 0.997832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1970 [D loss: 0.666918, acc.: 58.59%] [G loss: 1.043558]\n",
      "epoch:2 step:1971 [D loss: 0.718423, acc.: 61.72%] [G loss: 1.100546]\n",
      "epoch:2 step:1972 [D loss: 0.656998, acc.: 57.81%] [G loss: 1.020830]\n",
      "epoch:2 step:1973 [D loss: 0.589808, acc.: 67.97%] [G loss: 1.263351]\n",
      "epoch:2 step:1974 [D loss: 0.734545, acc.: 52.34%] [G loss: 1.099612]\n",
      "epoch:2 step:1975 [D loss: 0.720503, acc.: 54.69%] [G loss: 0.967660]\n",
      "epoch:2 step:1976 [D loss: 0.680744, acc.: 58.59%] [G loss: 1.172066]\n",
      "epoch:2 step:1977 [D loss: 0.711200, acc.: 53.12%] [G loss: 0.956008]\n",
      "epoch:2 step:1978 [D loss: 0.621770, acc.: 69.53%] [G loss: 1.254778]\n",
      "epoch:2 step:1979 [D loss: 0.533399, acc.: 73.44%] [G loss: 1.090810]\n",
      "epoch:2 step:1980 [D loss: 0.637259, acc.: 62.50%] [G loss: 1.150061]\n",
      "epoch:2 step:1981 [D loss: 0.609637, acc.: 64.84%] [G loss: 1.227941]\n",
      "epoch:2 step:1982 [D loss: 0.569177, acc.: 76.56%] [G loss: 1.117966]\n",
      "epoch:2 step:1983 [D loss: 0.617288, acc.: 71.88%] [G loss: 1.172759]\n",
      "epoch:2 step:1984 [D loss: 0.628915, acc.: 66.41%] [G loss: 1.156601]\n",
      "epoch:2 step:1985 [D loss: 0.750353, acc.: 54.69%] [G loss: 1.129158]\n",
      "epoch:2 step:1986 [D loss: 0.653225, acc.: 59.38%] [G loss: 1.056372]\n",
      "epoch:2 step:1987 [D loss: 0.681589, acc.: 64.06%] [G loss: 1.145121]\n",
      "epoch:2 step:1988 [D loss: 0.722354, acc.: 56.25%] [G loss: 0.919103]\n",
      "epoch:2 step:1989 [D loss: 0.685403, acc.: 57.03%] [G loss: 1.194188]\n",
      "epoch:2 step:1990 [D loss: 0.627167, acc.: 64.06%] [G loss: 1.159894]\n",
      "epoch:2 step:1991 [D loss: 0.718374, acc.: 55.47%] [G loss: 1.156621]\n",
      "epoch:2 step:1992 [D loss: 0.655822, acc.: 61.72%] [G loss: 1.039343]\n",
      "epoch:2 step:1993 [D loss: 0.696440, acc.: 53.12%] [G loss: 1.221537]\n",
      "epoch:2 step:1994 [D loss: 0.662860, acc.: 63.28%] [G loss: 1.038320]\n",
      "epoch:2 step:1995 [D loss: 0.648301, acc.: 67.19%] [G loss: 1.141993]\n",
      "epoch:2 step:1996 [D loss: 0.640940, acc.: 68.75%] [G loss: 0.964770]\n",
      "epoch:2 step:1997 [D loss: 0.678368, acc.: 55.47%] [G loss: 0.991481]\n",
      "epoch:2 step:1998 [D loss: 0.584832, acc.: 66.41%] [G loss: 1.107292]\n",
      "epoch:2 step:1999 [D loss: 0.646867, acc.: 63.28%] [G loss: 1.246968]\n",
      "epoch:2 step:2000 [D loss: 0.634103, acc.: 65.62%] [G loss: 1.142639]\n",
      "##############\n",
      "[2.76560212 2.02633527 2.13611969 2.88966275 1.0792202  6.45096966\n",
      " 2.29336181 3.14174656 3.83544742 5.07435276]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.607350, acc.: 66.41%] [G loss: 1.032711]\n",
      "epoch:2 step:2002 [D loss: 0.699375, acc.: 55.47%] [G loss: 0.984977]\n",
      "epoch:2 step:2003 [D loss: 0.708707, acc.: 56.25%] [G loss: 1.321869]\n",
      "epoch:2 step:2004 [D loss: 0.701298, acc.: 61.72%] [G loss: 1.134658]\n",
      "epoch:2 step:2005 [D loss: 0.695881, acc.: 60.94%] [G loss: 1.191924]\n",
      "epoch:2 step:2006 [D loss: 0.709108, acc.: 57.03%] [G loss: 1.035408]\n",
      "epoch:2 step:2007 [D loss: 0.627904, acc.: 68.75%] [G loss: 1.087638]\n",
      "epoch:2 step:2008 [D loss: 0.670049, acc.: 57.81%] [G loss: 1.103286]\n",
      "epoch:2 step:2009 [D loss: 0.582428, acc.: 72.66%] [G loss: 1.022176]\n",
      "epoch:2 step:2010 [D loss: 0.611438, acc.: 63.28%] [G loss: 1.216897]\n",
      "epoch:2 step:2011 [D loss: 0.557059, acc.: 71.88%] [G loss: 1.263034]\n",
      "epoch:2 step:2012 [D loss: 0.629478, acc.: 67.97%] [G loss: 1.191894]\n",
      "epoch:2 step:2013 [D loss: 0.629449, acc.: 64.84%] [G loss: 1.179912]\n",
      "epoch:2 step:2014 [D loss: 0.615616, acc.: 64.06%] [G loss: 1.111513]\n",
      "epoch:2 step:2015 [D loss: 0.769482, acc.: 50.00%] [G loss: 0.964899]\n",
      "epoch:2 step:2016 [D loss: 0.608047, acc.: 67.19%] [G loss: 1.135431]\n",
      "epoch:2 step:2017 [D loss: 0.628377, acc.: 64.84%] [G loss: 1.076173]\n",
      "epoch:2 step:2018 [D loss: 0.623607, acc.: 64.84%] [G loss: 1.294790]\n",
      "epoch:2 step:2019 [D loss: 0.680935, acc.: 57.03%] [G loss: 1.046076]\n",
      "epoch:2 step:2020 [D loss: 0.644918, acc.: 64.06%] [G loss: 1.081803]\n",
      "epoch:2 step:2021 [D loss: 0.609765, acc.: 67.97%] [G loss: 1.272463]\n",
      "epoch:2 step:2022 [D loss: 0.627484, acc.: 67.19%] [G loss: 1.154313]\n",
      "epoch:2 step:2023 [D loss: 0.688503, acc.: 53.12%] [G loss: 1.267816]\n",
      "epoch:2 step:2024 [D loss: 0.637229, acc.: 64.84%] [G loss: 1.230032]\n",
      "epoch:2 step:2025 [D loss: 0.754037, acc.: 51.56%] [G loss: 1.002428]\n",
      "epoch:2 step:2026 [D loss: 0.541015, acc.: 75.00%] [G loss: 1.099130]\n",
      "epoch:2 step:2027 [D loss: 0.634108, acc.: 64.84%] [G loss: 1.108922]\n",
      "epoch:2 step:2028 [D loss: 0.708639, acc.: 55.47%] [G loss: 0.979271]\n",
      "epoch:2 step:2029 [D loss: 0.698093, acc.: 55.47%] [G loss: 1.071964]\n",
      "epoch:2 step:2030 [D loss: 0.546275, acc.: 73.44%] [G loss: 1.158750]\n",
      "epoch:2 step:2031 [D loss: 0.670095, acc.: 58.59%] [G loss: 0.997673]\n",
      "epoch:2 step:2032 [D loss: 0.671136, acc.: 59.38%] [G loss: 1.209844]\n",
      "epoch:2 step:2033 [D loss: 0.736478, acc.: 53.91%] [G loss: 1.053120]\n",
      "epoch:2 step:2034 [D loss: 0.676179, acc.: 59.38%] [G loss: 1.042780]\n",
      "epoch:2 step:2035 [D loss: 0.663720, acc.: 64.06%] [G loss: 1.231745]\n",
      "epoch:2 step:2036 [D loss: 0.545206, acc.: 73.44%] [G loss: 1.341943]\n",
      "epoch:2 step:2037 [D loss: 0.651690, acc.: 65.62%] [G loss: 1.238208]\n",
      "epoch:2 step:2038 [D loss: 0.643415, acc.: 59.38%] [G loss: 1.070112]\n",
      "epoch:2 step:2039 [D loss: 0.652616, acc.: 61.72%] [G loss: 1.096739]\n",
      "epoch:2 step:2040 [D loss: 0.631100, acc.: 61.72%] [G loss: 1.220480]\n",
      "epoch:2 step:2041 [D loss: 0.608212, acc.: 69.53%] [G loss: 1.195732]\n",
      "epoch:2 step:2042 [D loss: 0.625886, acc.: 64.06%] [G loss: 1.151237]\n",
      "epoch:2 step:2043 [D loss: 0.608835, acc.: 64.84%] [G loss: 1.293036]\n",
      "epoch:2 step:2044 [D loss: 0.743852, acc.: 56.25%] [G loss: 1.162675]\n",
      "epoch:2 step:2045 [D loss: 0.629070, acc.: 66.41%] [G loss: 1.001536]\n",
      "epoch:2 step:2046 [D loss: 0.499874, acc.: 79.69%] [G loss: 1.119752]\n",
      "epoch:2 step:2047 [D loss: 0.634561, acc.: 60.94%] [G loss: 1.032179]\n",
      "epoch:2 step:2048 [D loss: 0.584359, acc.: 64.84%] [G loss: 0.917477]\n",
      "epoch:2 step:2049 [D loss: 0.596628, acc.: 64.84%] [G loss: 1.281629]\n",
      "epoch:2 step:2050 [D loss: 0.646024, acc.: 64.06%] [G loss: 1.128956]\n",
      "epoch:2 step:2051 [D loss: 0.683817, acc.: 57.81%] [G loss: 1.032268]\n",
      "epoch:2 step:2052 [D loss: 0.704565, acc.: 60.94%] [G loss: 1.063827]\n",
      "epoch:2 step:2053 [D loss: 0.667132, acc.: 60.94%] [G loss: 1.019317]\n",
      "epoch:2 step:2054 [D loss: 0.651111, acc.: 58.59%] [G loss: 1.192307]\n",
      "epoch:2 step:2055 [D loss: 0.689101, acc.: 61.72%] [G loss: 1.059945]\n",
      "epoch:2 step:2056 [D loss: 0.650119, acc.: 60.94%] [G loss: 1.013869]\n",
      "epoch:2 step:2057 [D loss: 0.624020, acc.: 64.06%] [G loss: 1.216578]\n",
      "epoch:2 step:2058 [D loss: 0.704781, acc.: 55.47%] [G loss: 1.094297]\n",
      "epoch:2 step:2059 [D loss: 0.788853, acc.: 45.31%] [G loss: 0.984276]\n",
      "epoch:2 step:2060 [D loss: 0.589105, acc.: 70.31%] [G loss: 1.110784]\n",
      "epoch:2 step:2061 [D loss: 0.562124, acc.: 73.44%] [G loss: 1.290414]\n",
      "epoch:2 step:2062 [D loss: 0.577601, acc.: 67.97%] [G loss: 1.148389]\n",
      "epoch:2 step:2063 [D loss: 0.723913, acc.: 48.44%] [G loss: 1.108848]\n",
      "epoch:2 step:2064 [D loss: 0.601125, acc.: 67.19%] [G loss: 1.174928]\n",
      "epoch:2 step:2065 [D loss: 0.575195, acc.: 71.88%] [G loss: 1.140696]\n",
      "epoch:2 step:2066 [D loss: 0.711339, acc.: 64.06%] [G loss: 1.231156]\n",
      "epoch:2 step:2067 [D loss: 0.725986, acc.: 54.69%] [G loss: 1.073065]\n",
      "epoch:2 step:2068 [D loss: 0.637022, acc.: 62.50%] [G loss: 0.987728]\n",
      "epoch:2 step:2069 [D loss: 0.759393, acc.: 53.12%] [G loss: 1.060944]\n",
      "epoch:2 step:2070 [D loss: 0.614802, acc.: 63.28%] [G loss: 1.229334]\n",
      "epoch:2 step:2071 [D loss: 0.581258, acc.: 67.97%] [G loss: 1.354302]\n",
      "epoch:2 step:2072 [D loss: 0.632544, acc.: 62.50%] [G loss: 1.077356]\n",
      "epoch:2 step:2073 [D loss: 0.676166, acc.: 61.72%] [G loss: 0.921287]\n",
      "epoch:2 step:2074 [D loss: 0.674916, acc.: 62.50%] [G loss: 1.028548]\n",
      "epoch:2 step:2075 [D loss: 0.545933, acc.: 75.78%] [G loss: 1.286478]\n",
      "epoch:2 step:2076 [D loss: 0.616377, acc.: 62.50%] [G loss: 1.124083]\n",
      "epoch:2 step:2077 [D loss: 0.670869, acc.: 64.06%] [G loss: 0.917188]\n",
      "epoch:2 step:2078 [D loss: 0.679009, acc.: 64.06%] [G loss: 1.053719]\n",
      "epoch:2 step:2079 [D loss: 0.652381, acc.: 62.50%] [G loss: 1.231547]\n",
      "epoch:2 step:2080 [D loss: 0.629335, acc.: 60.16%] [G loss: 1.179553]\n",
      "epoch:2 step:2081 [D loss: 0.640883, acc.: 64.06%] [G loss: 1.125053]\n",
      "epoch:2 step:2082 [D loss: 0.589735, acc.: 64.84%] [G loss: 1.134167]\n",
      "epoch:2 step:2083 [D loss: 0.612230, acc.: 64.06%] [G loss: 1.157454]\n",
      "epoch:2 step:2084 [D loss: 0.625800, acc.: 67.97%] [G loss: 1.141611]\n",
      "epoch:2 step:2085 [D loss: 0.707522, acc.: 54.69%] [G loss: 1.003053]\n",
      "epoch:2 step:2086 [D loss: 0.587233, acc.: 69.53%] [G loss: 1.151709]\n",
      "epoch:2 step:2087 [D loss: 0.639766, acc.: 62.50%] [G loss: 1.048535]\n",
      "epoch:2 step:2088 [D loss: 0.652249, acc.: 62.50%] [G loss: 1.086223]\n",
      "epoch:2 step:2089 [D loss: 0.657012, acc.: 58.59%] [G loss: 1.151036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2090 [D loss: 0.646799, acc.: 64.84%] [G loss: 0.997956]\n",
      "epoch:2 step:2091 [D loss: 0.616658, acc.: 63.28%] [G loss: 1.045327]\n",
      "epoch:2 step:2092 [D loss: 0.677632, acc.: 59.38%] [G loss: 0.999005]\n",
      "epoch:2 step:2093 [D loss: 0.729532, acc.: 57.81%] [G loss: 1.154135]\n",
      "epoch:2 step:2094 [D loss: 0.737828, acc.: 53.91%] [G loss: 1.057849]\n",
      "epoch:2 step:2095 [D loss: 0.667071, acc.: 61.72%] [G loss: 1.201798]\n",
      "epoch:2 step:2096 [D loss: 0.662268, acc.: 60.16%] [G loss: 1.198794]\n",
      "epoch:2 step:2097 [D loss: 0.651673, acc.: 64.84%] [G loss: 1.265590]\n",
      "epoch:2 step:2098 [D loss: 0.690038, acc.: 58.59%] [G loss: 1.130481]\n",
      "epoch:2 step:2099 [D loss: 0.616042, acc.: 65.62%] [G loss: 1.217119]\n",
      "epoch:2 step:2100 [D loss: 0.645029, acc.: 62.50%] [G loss: 1.116195]\n",
      "epoch:2 step:2101 [D loss: 0.674232, acc.: 61.72%] [G loss: 1.064362]\n",
      "epoch:2 step:2102 [D loss: 0.625560, acc.: 62.50%] [G loss: 1.034335]\n",
      "epoch:2 step:2103 [D loss: 0.674207, acc.: 60.94%] [G loss: 1.203143]\n",
      "epoch:2 step:2104 [D loss: 0.718430, acc.: 56.25%] [G loss: 1.137483]\n",
      "epoch:2 step:2105 [D loss: 0.654578, acc.: 61.72%] [G loss: 1.068050]\n",
      "epoch:2 step:2106 [D loss: 0.589768, acc.: 68.75%] [G loss: 1.155104]\n",
      "epoch:2 step:2107 [D loss: 0.627038, acc.: 66.41%] [G loss: 1.034124]\n",
      "epoch:2 step:2108 [D loss: 0.733688, acc.: 55.47%] [G loss: 1.011162]\n",
      "epoch:2 step:2109 [D loss: 0.617566, acc.: 65.62%] [G loss: 1.094389]\n",
      "epoch:2 step:2110 [D loss: 0.599866, acc.: 68.75%] [G loss: 1.387189]\n",
      "epoch:2 step:2111 [D loss: 0.593644, acc.: 69.53%] [G loss: 1.274994]\n",
      "epoch:2 step:2112 [D loss: 0.673401, acc.: 55.47%] [G loss: 1.042294]\n",
      "epoch:2 step:2113 [D loss: 0.613630, acc.: 65.62%] [G loss: 1.182118]\n",
      "epoch:2 step:2114 [D loss: 0.640091, acc.: 65.62%] [G loss: 1.224406]\n",
      "epoch:2 step:2115 [D loss: 0.640982, acc.: 62.50%] [G loss: 0.911085]\n",
      "epoch:2 step:2116 [D loss: 0.614240, acc.: 69.53%] [G loss: 1.055278]\n",
      "epoch:2 step:2117 [D loss: 0.584501, acc.: 71.09%] [G loss: 1.159105]\n",
      "epoch:2 step:2118 [D loss: 0.705694, acc.: 60.16%] [G loss: 1.105226]\n",
      "epoch:2 step:2119 [D loss: 0.688462, acc.: 61.72%] [G loss: 1.012455]\n",
      "epoch:2 step:2120 [D loss: 0.619721, acc.: 65.62%] [G loss: 1.025743]\n",
      "epoch:2 step:2121 [D loss: 0.641451, acc.: 64.84%] [G loss: 1.141389]\n",
      "epoch:2 step:2122 [D loss: 0.629304, acc.: 63.28%] [G loss: 0.999845]\n",
      "epoch:2 step:2123 [D loss: 0.683975, acc.: 61.72%] [G loss: 1.135992]\n",
      "epoch:2 step:2124 [D loss: 0.656121, acc.: 62.50%] [G loss: 1.041111]\n",
      "epoch:2 step:2125 [D loss: 0.684542, acc.: 57.03%] [G loss: 0.946437]\n",
      "epoch:2 step:2126 [D loss: 0.667836, acc.: 60.16%] [G loss: 1.038964]\n",
      "epoch:2 step:2127 [D loss: 0.570088, acc.: 69.53%] [G loss: 1.037839]\n",
      "epoch:2 step:2128 [D loss: 0.658058, acc.: 62.50%] [G loss: 1.290453]\n",
      "epoch:2 step:2129 [D loss: 0.614532, acc.: 64.06%] [G loss: 1.296905]\n",
      "epoch:2 step:2130 [D loss: 0.644763, acc.: 64.06%] [G loss: 1.234191]\n",
      "epoch:2 step:2131 [D loss: 0.643778, acc.: 61.72%] [G loss: 1.080139]\n",
      "epoch:2 step:2132 [D loss: 0.682384, acc.: 57.03%] [G loss: 1.212136]\n",
      "epoch:2 step:2133 [D loss: 0.684887, acc.: 60.94%] [G loss: 1.231705]\n",
      "epoch:2 step:2134 [D loss: 0.670945, acc.: 57.03%] [G loss: 1.134019]\n",
      "epoch:2 step:2135 [D loss: 0.643209, acc.: 64.84%] [G loss: 1.149359]\n",
      "epoch:2 step:2136 [D loss: 0.683148, acc.: 57.81%] [G loss: 0.956095]\n",
      "epoch:2 step:2137 [D loss: 0.589129, acc.: 68.75%] [G loss: 1.207317]\n",
      "epoch:2 step:2138 [D loss: 0.617396, acc.: 67.97%] [G loss: 1.294016]\n",
      "epoch:2 step:2139 [D loss: 0.696449, acc.: 58.59%] [G loss: 1.172175]\n",
      "epoch:2 step:2140 [D loss: 0.619545, acc.: 67.97%] [G loss: 1.126881]\n",
      "epoch:2 step:2141 [D loss: 0.551581, acc.: 71.88%] [G loss: 1.287637]\n",
      "epoch:2 step:2142 [D loss: 0.555783, acc.: 71.09%] [G loss: 1.072121]\n",
      "epoch:2 step:2143 [D loss: 0.590891, acc.: 70.31%] [G loss: 1.341507]\n",
      "epoch:2 step:2144 [D loss: 0.540054, acc.: 73.44%] [G loss: 1.089029]\n",
      "epoch:2 step:2145 [D loss: 0.550787, acc.: 75.00%] [G loss: 1.064996]\n",
      "epoch:2 step:2146 [D loss: 0.654692, acc.: 58.59%] [G loss: 1.150986]\n",
      "epoch:2 step:2147 [D loss: 0.615296, acc.: 68.75%] [G loss: 1.185411]\n",
      "epoch:2 step:2148 [D loss: 0.718457, acc.: 58.59%] [G loss: 1.078940]\n",
      "epoch:2 step:2149 [D loss: 0.765037, acc.: 50.00%] [G loss: 0.986745]\n",
      "epoch:2 step:2150 [D loss: 0.653842, acc.: 60.94%] [G loss: 1.154331]\n",
      "epoch:2 step:2151 [D loss: 0.607031, acc.: 64.84%] [G loss: 1.019058]\n",
      "epoch:2 step:2152 [D loss: 0.608100, acc.: 73.44%] [G loss: 0.946785]\n",
      "epoch:2 step:2153 [D loss: 0.658239, acc.: 59.38%] [G loss: 0.912140]\n",
      "epoch:2 step:2154 [D loss: 0.714938, acc.: 53.91%] [G loss: 1.034120]\n",
      "epoch:2 step:2155 [D loss: 0.553582, acc.: 72.66%] [G loss: 1.108541]\n",
      "epoch:2 step:2156 [D loss: 0.642186, acc.: 60.16%] [G loss: 0.996739]\n",
      "epoch:2 step:2157 [D loss: 0.635018, acc.: 63.28%] [G loss: 1.115245]\n",
      "epoch:2 step:2158 [D loss: 0.610801, acc.: 64.84%] [G loss: 1.102436]\n",
      "epoch:2 step:2159 [D loss: 0.723932, acc.: 54.69%] [G loss: 1.022686]\n",
      "epoch:2 step:2160 [D loss: 0.650032, acc.: 66.41%] [G loss: 1.024046]\n",
      "epoch:2 step:2161 [D loss: 0.699029, acc.: 50.78%] [G loss: 1.176415]\n",
      "epoch:2 step:2162 [D loss: 0.689201, acc.: 59.38%] [G loss: 0.942303]\n",
      "epoch:2 step:2163 [D loss: 0.710894, acc.: 52.34%] [G loss: 1.009537]\n",
      "epoch:2 step:2164 [D loss: 0.615462, acc.: 68.75%] [G loss: 1.039230]\n",
      "epoch:2 step:2165 [D loss: 0.639248, acc.: 63.28%] [G loss: 1.169882]\n",
      "epoch:2 step:2166 [D loss: 0.647257, acc.: 58.59%] [G loss: 1.225834]\n",
      "epoch:2 step:2167 [D loss: 0.643195, acc.: 63.28%] [G loss: 1.108926]\n",
      "epoch:2 step:2168 [D loss: 0.604164, acc.: 68.75%] [G loss: 1.241245]\n",
      "epoch:2 step:2169 [D loss: 0.656655, acc.: 63.28%] [G loss: 1.220952]\n",
      "epoch:2 step:2170 [D loss: 0.603824, acc.: 64.84%] [G loss: 1.163262]\n",
      "epoch:2 step:2171 [D loss: 0.572520, acc.: 71.88%] [G loss: 1.008809]\n",
      "epoch:2 step:2172 [D loss: 0.561377, acc.: 71.88%] [G loss: 1.076924]\n",
      "epoch:2 step:2173 [D loss: 0.696909, acc.: 60.16%] [G loss: 1.034196]\n",
      "epoch:2 step:2174 [D loss: 0.663787, acc.: 67.97%] [G loss: 1.088327]\n",
      "epoch:2 step:2175 [D loss: 0.712495, acc.: 55.47%] [G loss: 1.159364]\n",
      "epoch:2 step:2176 [D loss: 0.736312, acc.: 50.78%] [G loss: 1.004546]\n",
      "epoch:2 step:2177 [D loss: 0.623779, acc.: 65.62%] [G loss: 1.140757]\n",
      "epoch:2 step:2178 [D loss: 0.696121, acc.: 57.03%] [G loss: 1.017287]\n",
      "epoch:2 step:2179 [D loss: 0.709959, acc.: 57.81%] [G loss: 1.006591]\n",
      "epoch:2 step:2180 [D loss: 0.643016, acc.: 59.38%] [G loss: 1.116230]\n",
      "epoch:2 step:2181 [D loss: 0.709025, acc.: 57.81%] [G loss: 1.019798]\n",
      "epoch:2 step:2182 [D loss: 0.627572, acc.: 66.41%] [G loss: 1.093911]\n",
      "epoch:2 step:2183 [D loss: 0.586784, acc.: 73.44%] [G loss: 1.086667]\n",
      "epoch:2 step:2184 [D loss: 0.648754, acc.: 67.19%] [G loss: 1.086122]\n",
      "epoch:2 step:2185 [D loss: 0.623311, acc.: 65.62%] [G loss: 1.295922]\n",
      "epoch:2 step:2186 [D loss: 0.733224, acc.: 56.25%] [G loss: 1.089181]\n",
      "epoch:2 step:2187 [D loss: 0.618732, acc.: 64.06%] [G loss: 1.117847]\n",
      "epoch:2 step:2188 [D loss: 0.632762, acc.: 64.06%] [G loss: 1.142964]\n",
      "epoch:2 step:2189 [D loss: 0.598561, acc.: 70.31%] [G loss: 1.035639]\n",
      "epoch:2 step:2190 [D loss: 0.675221, acc.: 60.16%] [G loss: 1.170197]\n",
      "epoch:2 step:2191 [D loss: 0.693638, acc.: 53.91%] [G loss: 1.019493]\n",
      "epoch:2 step:2192 [D loss: 0.676600, acc.: 53.91%] [G loss: 1.130778]\n",
      "epoch:2 step:2193 [D loss: 0.609150, acc.: 63.28%] [G loss: 1.323503]\n",
      "epoch:2 step:2194 [D loss: 0.695761, acc.: 55.47%] [G loss: 1.068264]\n",
      "epoch:2 step:2195 [D loss: 0.660323, acc.: 58.59%] [G loss: 1.033728]\n",
      "epoch:2 step:2196 [D loss: 0.627427, acc.: 64.06%] [G loss: 1.177292]\n",
      "epoch:2 step:2197 [D loss: 0.709831, acc.: 57.81%] [G loss: 0.943218]\n",
      "epoch:2 step:2198 [D loss: 0.685639, acc.: 55.47%] [G loss: 1.032939]\n",
      "epoch:2 step:2199 [D loss: 0.698616, acc.: 57.03%] [G loss: 1.180550]\n",
      "epoch:2 step:2200 [D loss: 0.690292, acc.: 55.47%] [G loss: 1.162267]\n",
      "##############\n",
      "[2.6104803  1.81427222 2.00032478 3.00683433 0.91187301 5.75302789\n",
      " 1.90586355 2.75392682 3.7060134  5.23879005]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.528457, acc.: 74.22%] [G loss: 1.521886]\n",
      "epoch:2 step:2202 [D loss: 0.709988, acc.: 58.59%] [G loss: 1.183519]\n",
      "epoch:2 step:2203 [D loss: 0.737791, acc.: 53.91%] [G loss: 1.116260]\n",
      "epoch:2 step:2204 [D loss: 0.672166, acc.: 57.03%] [G loss: 1.014494]\n",
      "epoch:2 step:2205 [D loss: 0.723753, acc.: 59.38%] [G loss: 1.236683]\n",
      "epoch:2 step:2206 [D loss: 0.599883, acc.: 67.97%] [G loss: 1.272148]\n",
      "epoch:2 step:2207 [D loss: 0.625697, acc.: 62.50%] [G loss: 1.227458]\n",
      "epoch:2 step:2208 [D loss: 0.560763, acc.: 67.97%] [G loss: 1.339691]\n",
      "epoch:2 step:2209 [D loss: 0.644454, acc.: 64.84%] [G loss: 1.035543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2210 [D loss: 0.621725, acc.: 66.41%] [G loss: 0.991899]\n",
      "epoch:2 step:2211 [D loss: 0.686427, acc.: 62.50%] [G loss: 1.139114]\n",
      "epoch:2 step:2212 [D loss: 0.635816, acc.: 60.94%] [G loss: 1.161359]\n",
      "epoch:2 step:2213 [D loss: 0.683976, acc.: 60.16%] [G loss: 1.183069]\n",
      "epoch:2 step:2214 [D loss: 0.646145, acc.: 61.72%] [G loss: 1.220836]\n",
      "epoch:2 step:2215 [D loss: 0.627813, acc.: 62.50%] [G loss: 1.217286]\n",
      "epoch:2 step:2216 [D loss: 0.650015, acc.: 64.06%] [G loss: 1.130229]\n",
      "epoch:2 step:2217 [D loss: 0.631846, acc.: 60.94%] [G loss: 1.061781]\n",
      "epoch:2 step:2218 [D loss: 0.620321, acc.: 60.94%] [G loss: 1.093146]\n",
      "epoch:2 step:2219 [D loss: 0.704000, acc.: 54.69%] [G loss: 1.124620]\n",
      "epoch:2 step:2220 [D loss: 0.671029, acc.: 60.94%] [G loss: 1.084493]\n",
      "epoch:2 step:2221 [D loss: 0.586997, acc.: 67.19%] [G loss: 1.132576]\n",
      "epoch:2 step:2222 [D loss: 0.670214, acc.: 67.19%] [G loss: 1.110908]\n",
      "epoch:2 step:2223 [D loss: 0.629432, acc.: 60.94%] [G loss: 1.089381]\n",
      "epoch:2 step:2224 [D loss: 0.638385, acc.: 60.16%] [G loss: 1.109466]\n",
      "epoch:2 step:2225 [D loss: 0.619052, acc.: 67.19%] [G loss: 1.052343]\n",
      "epoch:2 step:2226 [D loss: 0.738183, acc.: 52.34%] [G loss: 1.113385]\n",
      "epoch:2 step:2227 [D loss: 0.620243, acc.: 63.28%] [G loss: 0.991724]\n",
      "epoch:2 step:2228 [D loss: 0.678923, acc.: 60.94%] [G loss: 1.042957]\n",
      "epoch:2 step:2229 [D loss: 0.575362, acc.: 69.53%] [G loss: 1.135030]\n",
      "epoch:2 step:2230 [D loss: 0.578711, acc.: 70.31%] [G loss: 0.923316]\n",
      "epoch:2 step:2231 [D loss: 0.583286, acc.: 66.41%] [G loss: 1.243753]\n",
      "epoch:2 step:2232 [D loss: 0.607306, acc.: 62.50%] [G loss: 1.133907]\n",
      "epoch:2 step:2233 [D loss: 0.659152, acc.: 60.16%] [G loss: 1.143951]\n",
      "epoch:2 step:2234 [D loss: 0.625882, acc.: 67.19%] [G loss: 1.170175]\n",
      "epoch:2 step:2235 [D loss: 0.652439, acc.: 64.84%] [G loss: 1.100947]\n",
      "epoch:2 step:2236 [D loss: 0.608541, acc.: 64.84%] [G loss: 1.147184]\n",
      "epoch:2 step:2237 [D loss: 0.667862, acc.: 56.25%] [G loss: 1.002338]\n",
      "epoch:2 step:2238 [D loss: 0.615517, acc.: 67.97%] [G loss: 1.223604]\n",
      "epoch:2 step:2239 [D loss: 0.612116, acc.: 67.19%] [G loss: 1.052920]\n",
      "epoch:2 step:2240 [D loss: 0.653950, acc.: 64.06%] [G loss: 1.005779]\n",
      "epoch:2 step:2241 [D loss: 0.681940, acc.: 65.62%] [G loss: 1.215388]\n",
      "epoch:2 step:2242 [D loss: 0.675543, acc.: 64.06%] [G loss: 0.925118]\n",
      "epoch:2 step:2243 [D loss: 0.712533, acc.: 53.12%] [G loss: 1.143924]\n",
      "epoch:2 step:2244 [D loss: 0.655769, acc.: 62.50%] [G loss: 1.062190]\n",
      "epoch:2 step:2245 [D loss: 0.681048, acc.: 58.59%] [G loss: 0.961937]\n",
      "epoch:2 step:2246 [D loss: 0.681538, acc.: 57.81%] [G loss: 0.940720]\n",
      "epoch:2 step:2247 [D loss: 0.573488, acc.: 71.09%] [G loss: 1.147353]\n",
      "epoch:2 step:2248 [D loss: 0.628530, acc.: 66.41%] [G loss: 1.023035]\n",
      "epoch:2 step:2249 [D loss: 0.656363, acc.: 63.28%] [G loss: 1.148408]\n",
      "epoch:2 step:2250 [D loss: 0.646955, acc.: 65.62%] [G loss: 1.000388]\n",
      "epoch:2 step:2251 [D loss: 0.700730, acc.: 55.47%] [G loss: 0.845535]\n",
      "epoch:2 step:2252 [D loss: 0.627550, acc.: 60.94%] [G loss: 1.212626]\n",
      "epoch:2 step:2253 [D loss: 0.592340, acc.: 68.75%] [G loss: 1.128973]\n",
      "epoch:2 step:2254 [D loss: 0.591590, acc.: 71.88%] [G loss: 1.098808]\n",
      "epoch:2 step:2255 [D loss: 0.652751, acc.: 60.94%] [G loss: 1.148564]\n",
      "epoch:2 step:2256 [D loss: 0.727256, acc.: 60.16%] [G loss: 1.039995]\n",
      "epoch:2 step:2257 [D loss: 0.622906, acc.: 64.84%] [G loss: 1.020347]\n",
      "epoch:2 step:2258 [D loss: 0.710152, acc.: 55.47%] [G loss: 1.137219]\n",
      "epoch:2 step:2259 [D loss: 0.647361, acc.: 66.41%] [G loss: 1.158272]\n",
      "epoch:2 step:2260 [D loss: 0.575822, acc.: 68.75%] [G loss: 1.058091]\n",
      "epoch:2 step:2261 [D loss: 0.715197, acc.: 51.56%] [G loss: 0.905429]\n",
      "epoch:2 step:2262 [D loss: 0.660779, acc.: 56.25%] [G loss: 1.011077]\n",
      "epoch:2 step:2263 [D loss: 0.648801, acc.: 59.38%] [G loss: 1.117300]\n",
      "epoch:2 step:2264 [D loss: 0.656020, acc.: 62.50%] [G loss: 1.231082]\n",
      "epoch:2 step:2265 [D loss: 0.652978, acc.: 58.59%] [G loss: 1.142953]\n",
      "epoch:2 step:2266 [D loss: 0.554888, acc.: 74.22%] [G loss: 1.255427]\n",
      "epoch:2 step:2267 [D loss: 0.680964, acc.: 59.38%] [G loss: 1.063643]\n",
      "epoch:2 step:2268 [D loss: 0.703035, acc.: 57.03%] [G loss: 1.041748]\n",
      "epoch:2 step:2269 [D loss: 0.625973, acc.: 69.53%] [G loss: 1.027092]\n",
      "epoch:2 step:2270 [D loss: 0.668196, acc.: 62.50%] [G loss: 1.006256]\n",
      "epoch:2 step:2271 [D loss: 0.715725, acc.: 56.25%] [G loss: 1.208931]\n",
      "epoch:2 step:2272 [D loss: 0.706879, acc.: 55.47%] [G loss: 0.997687]\n",
      "epoch:2 step:2273 [D loss: 0.571482, acc.: 73.44%] [G loss: 1.119568]\n",
      "epoch:2 step:2274 [D loss: 0.627397, acc.: 67.19%] [G loss: 1.205037]\n",
      "epoch:2 step:2275 [D loss: 0.622706, acc.: 71.09%] [G loss: 1.054522]\n",
      "epoch:2 step:2276 [D loss: 0.606254, acc.: 64.06%] [G loss: 1.177994]\n",
      "epoch:2 step:2277 [D loss: 0.636207, acc.: 62.50%] [G loss: 1.233698]\n",
      "epoch:2 step:2278 [D loss: 0.622191, acc.: 63.28%] [G loss: 1.051191]\n",
      "epoch:2 step:2279 [D loss: 0.605516, acc.: 63.28%] [G loss: 1.089402]\n",
      "epoch:2 step:2280 [D loss: 0.559723, acc.: 69.53%] [G loss: 1.040111]\n",
      "epoch:2 step:2281 [D loss: 0.606379, acc.: 64.06%] [G loss: 1.250438]\n",
      "epoch:2 step:2282 [D loss: 0.581261, acc.: 63.28%] [G loss: 1.201574]\n",
      "epoch:2 step:2283 [D loss: 0.708737, acc.: 57.81%] [G loss: 0.986542]\n",
      "epoch:2 step:2284 [D loss: 0.737069, acc.: 53.91%] [G loss: 0.998845]\n",
      "epoch:2 step:2285 [D loss: 0.625078, acc.: 63.28%] [G loss: 1.139332]\n",
      "epoch:2 step:2286 [D loss: 0.581151, acc.: 71.09%] [G loss: 1.016501]\n",
      "epoch:2 step:2287 [D loss: 0.640781, acc.: 58.59%] [G loss: 1.143496]\n",
      "epoch:2 step:2288 [D loss: 0.628642, acc.: 64.06%] [G loss: 1.227188]\n",
      "epoch:2 step:2289 [D loss: 0.676973, acc.: 57.81%] [G loss: 1.235845]\n",
      "epoch:2 step:2290 [D loss: 0.637332, acc.: 64.06%] [G loss: 1.022691]\n",
      "epoch:2 step:2291 [D loss: 0.659332, acc.: 64.84%] [G loss: 1.056705]\n",
      "epoch:2 step:2292 [D loss: 0.621424, acc.: 68.75%] [G loss: 1.114364]\n",
      "epoch:2 step:2293 [D loss: 0.658228, acc.: 59.38%] [G loss: 0.978399]\n",
      "epoch:2 step:2294 [D loss: 0.671631, acc.: 60.94%] [G loss: 1.158635]\n",
      "epoch:2 step:2295 [D loss: 0.695605, acc.: 54.69%] [G loss: 1.207795]\n",
      "epoch:2 step:2296 [D loss: 0.623347, acc.: 65.62%] [G loss: 1.239288]\n",
      "epoch:2 step:2297 [D loss: 0.646222, acc.: 67.19%] [G loss: 1.193494]\n",
      "epoch:2 step:2298 [D loss: 0.675333, acc.: 62.50%] [G loss: 1.125333]\n",
      "epoch:2 step:2299 [D loss: 0.605207, acc.: 68.75%] [G loss: 1.332613]\n",
      "epoch:2 step:2300 [D loss: 0.636108, acc.: 67.19%] [G loss: 1.021572]\n",
      "epoch:2 step:2301 [D loss: 0.635957, acc.: 61.72%] [G loss: 1.210382]\n",
      "epoch:2 step:2302 [D loss: 0.739286, acc.: 53.91%] [G loss: 1.015053]\n",
      "epoch:2 step:2303 [D loss: 0.640953, acc.: 64.06%] [G loss: 0.932893]\n",
      "epoch:2 step:2304 [D loss: 0.724159, acc.: 59.38%] [G loss: 1.104613]\n",
      "epoch:2 step:2305 [D loss: 0.579822, acc.: 71.09%] [G loss: 1.237189]\n",
      "epoch:2 step:2306 [D loss: 0.606483, acc.: 65.62%] [G loss: 1.169843]\n",
      "epoch:2 step:2307 [D loss: 0.652873, acc.: 58.59%] [G loss: 1.052509]\n",
      "epoch:2 step:2308 [D loss: 0.648897, acc.: 60.94%] [G loss: 1.153432]\n",
      "epoch:2 step:2309 [D loss: 0.607560, acc.: 62.50%] [G loss: 1.064306]\n",
      "epoch:2 step:2310 [D loss: 0.651002, acc.: 64.84%] [G loss: 1.056190]\n",
      "epoch:2 step:2311 [D loss: 0.723288, acc.: 54.69%] [G loss: 1.036412]\n",
      "epoch:2 step:2312 [D loss: 0.731234, acc.: 57.03%] [G loss: 0.994592]\n",
      "epoch:2 step:2313 [D loss: 0.578270, acc.: 67.19%] [G loss: 1.153538]\n",
      "epoch:2 step:2314 [D loss: 0.598378, acc.: 66.41%] [G loss: 1.331259]\n",
      "epoch:2 step:2315 [D loss: 0.593684, acc.: 68.75%] [G loss: 1.127537]\n",
      "epoch:2 step:2316 [D loss: 0.680316, acc.: 57.03%] [G loss: 1.178829]\n",
      "epoch:2 step:2317 [D loss: 0.708506, acc.: 60.16%] [G loss: 0.997432]\n",
      "epoch:2 step:2318 [D loss: 0.650545, acc.: 61.72%] [G loss: 1.144081]\n",
      "epoch:2 step:2319 [D loss: 0.620567, acc.: 66.41%] [G loss: 1.090423]\n",
      "epoch:2 step:2320 [D loss: 0.645711, acc.: 61.72%] [G loss: 1.105346]\n",
      "epoch:2 step:2321 [D loss: 0.622971, acc.: 65.62%] [G loss: 1.053818]\n",
      "epoch:2 step:2322 [D loss: 0.631223, acc.: 61.72%] [G loss: 1.148232]\n",
      "epoch:2 step:2323 [D loss: 0.667694, acc.: 60.16%] [G loss: 1.009277]\n",
      "epoch:2 step:2324 [D loss: 0.601782, acc.: 69.53%] [G loss: 1.105647]\n",
      "epoch:2 step:2325 [D loss: 0.571015, acc.: 67.97%] [G loss: 1.186081]\n",
      "epoch:2 step:2326 [D loss: 0.749027, acc.: 51.56%] [G loss: 1.174449]\n",
      "epoch:2 step:2327 [D loss: 0.684842, acc.: 57.03%] [G loss: 1.042668]\n",
      "epoch:2 step:2328 [D loss: 0.636479, acc.: 60.16%] [G loss: 1.043751]\n",
      "epoch:2 step:2329 [D loss: 0.567345, acc.: 71.88%] [G loss: 1.310618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2330 [D loss: 0.622025, acc.: 67.19%] [G loss: 1.163051]\n",
      "epoch:2 step:2331 [D loss: 0.620824, acc.: 61.72%] [G loss: 1.287759]\n",
      "epoch:2 step:2332 [D loss: 0.575735, acc.: 73.44%] [G loss: 1.195331]\n",
      "epoch:2 step:2333 [D loss: 0.637629, acc.: 64.84%] [G loss: 1.073956]\n",
      "epoch:2 step:2334 [D loss: 0.583718, acc.: 67.19%] [G loss: 1.096119]\n",
      "epoch:2 step:2335 [D loss: 0.669877, acc.: 56.25%] [G loss: 1.163714]\n",
      "epoch:2 step:2336 [D loss: 0.687390, acc.: 55.47%] [G loss: 1.016942]\n",
      "epoch:2 step:2337 [D loss: 0.717410, acc.: 53.12%] [G loss: 0.964619]\n",
      "epoch:2 step:2338 [D loss: 0.631742, acc.: 67.97%] [G loss: 1.208423]\n",
      "epoch:2 step:2339 [D loss: 0.593487, acc.: 67.97%] [G loss: 1.110894]\n",
      "epoch:2 step:2340 [D loss: 0.690051, acc.: 60.16%] [G loss: 1.105396]\n",
      "epoch:2 step:2341 [D loss: 0.566909, acc.: 63.28%] [G loss: 1.151128]\n",
      "epoch:2 step:2342 [D loss: 0.604711, acc.: 67.19%] [G loss: 1.165318]\n",
      "epoch:2 step:2343 [D loss: 0.689300, acc.: 57.81%] [G loss: 1.082989]\n",
      "epoch:2 step:2344 [D loss: 0.680524, acc.: 56.25%] [G loss: 0.987896]\n",
      "epoch:2 step:2345 [D loss: 0.638055, acc.: 60.94%] [G loss: 1.214812]\n",
      "epoch:2 step:2346 [D loss: 0.673347, acc.: 60.16%] [G loss: 1.135757]\n",
      "epoch:2 step:2347 [D loss: 0.617344, acc.: 64.06%] [G loss: 1.042621]\n",
      "epoch:2 step:2348 [D loss: 0.669508, acc.: 57.03%] [G loss: 1.009176]\n",
      "epoch:2 step:2349 [D loss: 0.583136, acc.: 68.75%] [G loss: 1.246960]\n",
      "epoch:2 step:2350 [D loss: 0.723502, acc.: 53.91%] [G loss: 1.063640]\n",
      "epoch:2 step:2351 [D loss: 0.745341, acc.: 50.78%] [G loss: 1.029432]\n",
      "epoch:2 step:2352 [D loss: 0.683596, acc.: 57.03%] [G loss: 1.096117]\n",
      "epoch:2 step:2353 [D loss: 0.676254, acc.: 55.47%] [G loss: 1.245753]\n",
      "epoch:2 step:2354 [D loss: 0.663411, acc.: 56.25%] [G loss: 1.019676]\n",
      "epoch:2 step:2355 [D loss: 0.729785, acc.: 50.78%] [G loss: 1.069540]\n",
      "epoch:2 step:2356 [D loss: 0.547911, acc.: 72.66%] [G loss: 1.161469]\n",
      "epoch:2 step:2357 [D loss: 0.648746, acc.: 58.59%] [G loss: 0.991036]\n",
      "epoch:2 step:2358 [D loss: 0.663825, acc.: 59.38%] [G loss: 0.916804]\n",
      "epoch:2 step:2359 [D loss: 0.648964, acc.: 63.28%] [G loss: 1.101549]\n",
      "epoch:2 step:2360 [D loss: 0.620595, acc.: 64.84%] [G loss: 1.309717]\n",
      "epoch:2 step:2361 [D loss: 0.681524, acc.: 57.81%] [G loss: 1.069811]\n",
      "epoch:2 step:2362 [D loss: 0.560739, acc.: 72.66%] [G loss: 1.102031]\n",
      "epoch:2 step:2363 [D loss: 0.614820, acc.: 66.41%] [G loss: 1.216995]\n",
      "epoch:2 step:2364 [D loss: 0.684765, acc.: 60.16%] [G loss: 0.962097]\n",
      "epoch:2 step:2365 [D loss: 0.600307, acc.: 67.97%] [G loss: 1.213866]\n",
      "epoch:2 step:2366 [D loss: 0.646796, acc.: 59.38%] [G loss: 1.193460]\n",
      "epoch:2 step:2367 [D loss: 0.699383, acc.: 61.72%] [G loss: 1.003052]\n",
      "epoch:2 step:2368 [D loss: 0.681143, acc.: 57.03%] [G loss: 0.976645]\n",
      "epoch:2 step:2369 [D loss: 0.667744, acc.: 59.38%] [G loss: 1.215910]\n",
      "epoch:2 step:2370 [D loss: 0.607449, acc.: 67.97%] [G loss: 1.018776]\n",
      "epoch:2 step:2371 [D loss: 0.601399, acc.: 65.62%] [G loss: 1.116354]\n",
      "epoch:2 step:2372 [D loss: 0.539373, acc.: 73.44%] [G loss: 1.073161]\n",
      "epoch:2 step:2373 [D loss: 0.622949, acc.: 66.41%] [G loss: 1.055072]\n",
      "epoch:2 step:2374 [D loss: 0.565944, acc.: 71.88%] [G loss: 1.066376]\n",
      "epoch:2 step:2375 [D loss: 0.625836, acc.: 66.41%] [G loss: 1.053900]\n",
      "epoch:2 step:2376 [D loss: 0.592089, acc.: 69.53%] [G loss: 1.093686]\n",
      "epoch:2 step:2377 [D loss: 0.583063, acc.: 72.66%] [G loss: 1.103288]\n",
      "epoch:2 step:2378 [D loss: 0.612462, acc.: 63.28%] [G loss: 1.052601]\n",
      "epoch:2 step:2379 [D loss: 0.638091, acc.: 66.41%] [G loss: 1.061750]\n",
      "epoch:2 step:2380 [D loss: 0.735520, acc.: 56.25%] [G loss: 1.143676]\n",
      "epoch:2 step:2381 [D loss: 0.598709, acc.: 65.62%] [G loss: 1.121749]\n",
      "epoch:2 step:2382 [D loss: 0.601607, acc.: 67.97%] [G loss: 1.007687]\n",
      "epoch:2 step:2383 [D loss: 0.606587, acc.: 67.97%] [G loss: 1.177208]\n",
      "epoch:2 step:2384 [D loss: 0.627100, acc.: 64.06%] [G loss: 1.011194]\n",
      "epoch:2 step:2385 [D loss: 0.624797, acc.: 67.19%] [G loss: 1.080545]\n",
      "epoch:2 step:2386 [D loss: 0.678448, acc.: 57.81%] [G loss: 1.006408]\n",
      "epoch:2 step:2387 [D loss: 0.607483, acc.: 69.53%] [G loss: 1.062591]\n",
      "epoch:2 step:2388 [D loss: 0.582158, acc.: 67.97%] [G loss: 1.027126]\n",
      "epoch:2 step:2389 [D loss: 0.562451, acc.: 75.78%] [G loss: 1.283867]\n",
      "epoch:2 step:2390 [D loss: 0.614084, acc.: 67.19%] [G loss: 1.129602]\n",
      "epoch:2 step:2391 [D loss: 0.590745, acc.: 68.75%] [G loss: 1.148040]\n",
      "epoch:2 step:2392 [D loss: 0.495113, acc.: 77.34%] [G loss: 1.203749]\n",
      "epoch:2 step:2393 [D loss: 0.539932, acc.: 71.09%] [G loss: 1.265225]\n",
      "epoch:2 step:2394 [D loss: 0.617394, acc.: 62.50%] [G loss: 1.022637]\n",
      "epoch:2 step:2395 [D loss: 0.616071, acc.: 64.84%] [G loss: 1.219044]\n",
      "epoch:2 step:2396 [D loss: 0.688018, acc.: 60.94%] [G loss: 0.973963]\n",
      "epoch:2 step:2397 [D loss: 0.582418, acc.: 68.75%] [G loss: 1.141289]\n",
      "epoch:2 step:2398 [D loss: 0.592753, acc.: 69.53%] [G loss: 1.183540]\n",
      "epoch:2 step:2399 [D loss: 0.687543, acc.: 57.03%] [G loss: 1.003921]\n",
      "epoch:2 step:2400 [D loss: 0.674655, acc.: 58.59%] [G loss: 1.345401]\n",
      "##############\n",
      "[2.7654791  1.92385533 1.77947086 2.98586283 0.9749794  5.98593431\n",
      " 2.07550906 2.63722603 3.38861459 4.60092589]\n",
      "##########\n",
      "epoch:2 step:2401 [D loss: 0.709346, acc.: 57.81%] [G loss: 1.243222]\n",
      "epoch:2 step:2402 [D loss: 0.661440, acc.: 61.72%] [G loss: 1.161807]\n",
      "epoch:2 step:2403 [D loss: 0.566081, acc.: 66.41%] [G loss: 1.137264]\n",
      "epoch:2 step:2404 [D loss: 0.623285, acc.: 66.41%] [G loss: 0.943221]\n",
      "epoch:2 step:2405 [D loss: 0.644793, acc.: 62.50%] [G loss: 1.209712]\n",
      "epoch:2 step:2406 [D loss: 0.582196, acc.: 69.53%] [G loss: 1.123739]\n",
      "epoch:2 step:2407 [D loss: 0.643699, acc.: 63.28%] [G loss: 1.220731]\n",
      "epoch:2 step:2408 [D loss: 0.620003, acc.: 67.19%] [G loss: 1.080463]\n",
      "epoch:2 step:2409 [D loss: 0.633345, acc.: 65.62%] [G loss: 1.033187]\n",
      "epoch:2 step:2410 [D loss: 0.649493, acc.: 64.84%] [G loss: 1.026204]\n",
      "epoch:2 step:2411 [D loss: 0.694728, acc.: 60.94%] [G loss: 1.011864]\n",
      "epoch:2 step:2412 [D loss: 0.603768, acc.: 64.84%] [G loss: 0.944520]\n",
      "epoch:2 step:2413 [D loss: 0.607482, acc.: 64.84%] [G loss: 1.211644]\n",
      "epoch:2 step:2414 [D loss: 0.642224, acc.: 60.94%] [G loss: 1.183546]\n",
      "epoch:2 step:2415 [D loss: 0.713255, acc.: 56.25%] [G loss: 1.081579]\n",
      "epoch:2 step:2416 [D loss: 0.682716, acc.: 64.84%] [G loss: 1.098724]\n",
      "epoch:2 step:2417 [D loss: 0.784164, acc.: 47.66%] [G loss: 0.918563]\n",
      "epoch:2 step:2418 [D loss: 0.709276, acc.: 59.38%] [G loss: 0.995382]\n",
      "epoch:2 step:2419 [D loss: 0.644380, acc.: 60.94%] [G loss: 1.099330]\n",
      "epoch:2 step:2420 [D loss: 0.573770, acc.: 71.88%] [G loss: 1.279271]\n",
      "epoch:2 step:2421 [D loss: 0.681847, acc.: 60.94%] [G loss: 1.200828]\n",
      "epoch:2 step:2422 [D loss: 0.677609, acc.: 61.72%] [G loss: 1.052091]\n",
      "epoch:2 step:2423 [D loss: 0.547976, acc.: 75.78%] [G loss: 1.206087]\n",
      "epoch:2 step:2424 [D loss: 0.589133, acc.: 65.62%] [G loss: 1.133300]\n",
      "epoch:2 step:2425 [D loss: 0.701790, acc.: 56.25%] [G loss: 1.067876]\n",
      "epoch:2 step:2426 [D loss: 0.560004, acc.: 67.19%] [G loss: 1.099681]\n",
      "epoch:2 step:2427 [D loss: 0.597014, acc.: 66.41%] [G loss: 1.152428]\n",
      "epoch:2 step:2428 [D loss: 0.627104, acc.: 62.50%] [G loss: 1.239430]\n",
      "epoch:2 step:2429 [D loss: 0.551425, acc.: 69.53%] [G loss: 1.167947]\n",
      "epoch:2 step:2430 [D loss: 0.593389, acc.: 72.66%] [G loss: 0.989349]\n",
      "epoch:2 step:2431 [D loss: 0.628349, acc.: 63.28%] [G loss: 1.073698]\n",
      "epoch:2 step:2432 [D loss: 0.641293, acc.: 61.72%] [G loss: 1.039498]\n",
      "epoch:2 step:2433 [D loss: 0.606677, acc.: 66.41%] [G loss: 1.346281]\n",
      "epoch:2 step:2434 [D loss: 0.619451, acc.: 64.84%] [G loss: 1.141712]\n",
      "epoch:2 step:2435 [D loss: 0.635379, acc.: 62.50%] [G loss: 1.037143]\n",
      "epoch:2 step:2436 [D loss: 0.655871, acc.: 62.50%] [G loss: 0.997988]\n",
      "epoch:2 step:2437 [D loss: 0.571105, acc.: 70.31%] [G loss: 1.007898]\n",
      "epoch:2 step:2438 [D loss: 0.511885, acc.: 78.91%] [G loss: 1.088564]\n",
      "epoch:2 step:2439 [D loss: 0.595301, acc.: 69.53%] [G loss: 1.097842]\n",
      "epoch:2 step:2440 [D loss: 0.549589, acc.: 75.00%] [G loss: 1.093601]\n",
      "epoch:2 step:2441 [D loss: 0.611662, acc.: 67.97%] [G loss: 1.035405]\n",
      "epoch:2 step:2442 [D loss: 0.718230, acc.: 57.03%] [G loss: 0.961673]\n",
      "epoch:2 step:2443 [D loss: 0.572811, acc.: 73.44%] [G loss: 0.963747]\n",
      "epoch:2 step:2444 [D loss: 0.649643, acc.: 61.72%] [G loss: 1.146905]\n",
      "epoch:2 step:2445 [D loss: 0.617110, acc.: 64.84%] [G loss: 1.332722]\n",
      "epoch:2 step:2446 [D loss: 0.572979, acc.: 69.53%] [G loss: 1.196728]\n",
      "epoch:2 step:2447 [D loss: 0.609227, acc.: 65.62%] [G loss: 1.108023]\n",
      "epoch:2 step:2448 [D loss: 0.611486, acc.: 65.62%] [G loss: 0.966353]\n",
      "epoch:2 step:2449 [D loss: 0.645941, acc.: 61.72%] [G loss: 0.901251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2450 [D loss: 0.690511, acc.: 63.28%] [G loss: 1.172091]\n",
      "epoch:2 step:2451 [D loss: 0.684578, acc.: 60.94%] [G loss: 1.129268]\n",
      "epoch:2 step:2452 [D loss: 0.608591, acc.: 64.06%] [G loss: 1.208103]\n",
      "epoch:2 step:2453 [D loss: 0.629363, acc.: 66.41%] [G loss: 1.218878]\n",
      "epoch:2 step:2454 [D loss: 0.693691, acc.: 56.25%] [G loss: 1.073159]\n",
      "epoch:2 step:2455 [D loss: 0.627760, acc.: 65.62%] [G loss: 1.125604]\n",
      "epoch:2 step:2456 [D loss: 0.665050, acc.: 57.03%] [G loss: 1.085771]\n",
      "epoch:2 step:2457 [D loss: 0.593593, acc.: 73.44%] [G loss: 1.149592]\n",
      "epoch:2 step:2458 [D loss: 0.672467, acc.: 60.94%] [G loss: 1.107755]\n",
      "epoch:2 step:2459 [D loss: 0.642683, acc.: 62.50%] [G loss: 1.026796]\n",
      "epoch:2 step:2460 [D loss: 0.646527, acc.: 64.06%] [G loss: 1.085242]\n",
      "epoch:2 step:2461 [D loss: 0.648325, acc.: 63.28%] [G loss: 1.146541]\n",
      "epoch:2 step:2462 [D loss: 0.649666, acc.: 61.72%] [G loss: 1.001218]\n",
      "epoch:2 step:2463 [D loss: 0.573441, acc.: 64.84%] [G loss: 1.120268]\n",
      "epoch:2 step:2464 [D loss: 0.650273, acc.: 60.94%] [G loss: 1.055359]\n",
      "epoch:2 step:2465 [D loss: 0.650830, acc.: 64.84%] [G loss: 1.089672]\n",
      "epoch:2 step:2466 [D loss: 0.610590, acc.: 66.41%] [G loss: 1.248765]\n",
      "epoch:2 step:2467 [D loss: 0.727301, acc.: 53.91%] [G loss: 0.984510]\n",
      "epoch:2 step:2468 [D loss: 0.620874, acc.: 60.94%] [G loss: 1.040509]\n",
      "epoch:2 step:2469 [D loss: 0.709694, acc.: 54.69%] [G loss: 1.152686]\n",
      "epoch:2 step:2470 [D loss: 0.645047, acc.: 65.62%] [G loss: 1.219856]\n",
      "epoch:2 step:2471 [D loss: 0.668613, acc.: 53.12%] [G loss: 1.072654]\n",
      "epoch:2 step:2472 [D loss: 0.582110, acc.: 71.88%] [G loss: 1.187022]\n",
      "epoch:2 step:2473 [D loss: 0.545081, acc.: 77.34%] [G loss: 1.089918]\n",
      "epoch:2 step:2474 [D loss: 0.696675, acc.: 56.25%] [G loss: 1.110713]\n",
      "epoch:2 step:2475 [D loss: 0.621418, acc.: 66.41%] [G loss: 1.155241]\n",
      "epoch:2 step:2476 [D loss: 0.631396, acc.: 64.84%] [G loss: 1.092959]\n",
      "epoch:2 step:2477 [D loss: 0.605169, acc.: 66.41%] [G loss: 1.189400]\n",
      "epoch:2 step:2478 [D loss: 0.767521, acc.: 47.66%] [G loss: 1.067149]\n",
      "epoch:2 step:2479 [D loss: 0.716427, acc.: 54.69%] [G loss: 1.117696]\n",
      "epoch:2 step:2480 [D loss: 0.573647, acc.: 67.97%] [G loss: 1.285736]\n",
      "epoch:2 step:2481 [D loss: 0.562344, acc.: 69.53%] [G loss: 1.205096]\n",
      "epoch:2 step:2482 [D loss: 0.635432, acc.: 62.50%] [G loss: 1.115152]\n",
      "epoch:2 step:2483 [D loss: 0.581744, acc.: 70.31%] [G loss: 1.118401]\n",
      "epoch:2 step:2484 [D loss: 0.656083, acc.: 57.81%] [G loss: 1.299549]\n",
      "epoch:2 step:2485 [D loss: 0.584365, acc.: 71.88%] [G loss: 1.233563]\n",
      "epoch:2 step:2486 [D loss: 0.658496, acc.: 59.38%] [G loss: 1.047014]\n",
      "epoch:2 step:2487 [D loss: 0.578921, acc.: 72.66%] [G loss: 1.068307]\n",
      "epoch:2 step:2488 [D loss: 0.652186, acc.: 65.62%] [G loss: 1.251917]\n",
      "epoch:2 step:2489 [D loss: 0.575733, acc.: 70.31%] [G loss: 1.068554]\n",
      "epoch:2 step:2490 [D loss: 0.632264, acc.: 69.53%] [G loss: 1.077941]\n",
      "epoch:2 step:2491 [D loss: 0.627834, acc.: 65.62%] [G loss: 1.108493]\n",
      "epoch:2 step:2492 [D loss: 0.637181, acc.: 63.28%] [G loss: 1.189720]\n",
      "epoch:2 step:2493 [D loss: 0.579320, acc.: 70.31%] [G loss: 1.176256]\n",
      "epoch:2 step:2494 [D loss: 0.686611, acc.: 57.03%] [G loss: 0.932309]\n",
      "epoch:2 step:2495 [D loss: 0.648830, acc.: 61.72%] [G loss: 1.151469]\n",
      "epoch:2 step:2496 [D loss: 0.613271, acc.: 72.66%] [G loss: 1.117991]\n",
      "epoch:2 step:2497 [D loss: 0.606343, acc.: 67.97%] [G loss: 1.298350]\n",
      "epoch:2 step:2498 [D loss: 0.717761, acc.: 53.91%] [G loss: 1.153088]\n",
      "epoch:2 step:2499 [D loss: 0.559004, acc.: 74.22%] [G loss: 1.168057]\n",
      "epoch:2 step:2500 [D loss: 0.601289, acc.: 67.19%] [G loss: 1.088979]\n",
      "epoch:2 step:2501 [D loss: 0.653809, acc.: 66.41%] [G loss: 1.031383]\n",
      "epoch:2 step:2502 [D loss: 0.713612, acc.: 56.25%] [G loss: 1.051915]\n",
      "epoch:2 step:2503 [D loss: 0.598510, acc.: 69.53%] [G loss: 1.065399]\n",
      "epoch:2 step:2504 [D loss: 0.710983, acc.: 55.47%] [G loss: 1.010861]\n",
      "epoch:2 step:2505 [D loss: 0.510557, acc.: 75.00%] [G loss: 1.159486]\n",
      "epoch:2 step:2506 [D loss: 0.619373, acc.: 67.97%] [G loss: 1.188478]\n",
      "epoch:2 step:2507 [D loss: 0.599569, acc.: 64.84%] [G loss: 1.159958]\n",
      "epoch:2 step:2508 [D loss: 0.639189, acc.: 63.28%] [G loss: 1.162606]\n",
      "epoch:2 step:2509 [D loss: 0.612994, acc.: 64.84%] [G loss: 1.293538]\n",
      "epoch:2 step:2510 [D loss: 0.632067, acc.: 64.06%] [G loss: 1.004497]\n",
      "epoch:2 step:2511 [D loss: 0.596328, acc.: 68.75%] [G loss: 1.315569]\n",
      "epoch:2 step:2512 [D loss: 0.706580, acc.: 57.81%] [G loss: 1.154503]\n",
      "epoch:2 step:2513 [D loss: 0.636538, acc.: 66.41%] [G loss: 1.187297]\n",
      "epoch:2 step:2514 [D loss: 0.639385, acc.: 64.84%] [G loss: 1.123243]\n",
      "epoch:2 step:2515 [D loss: 0.606846, acc.: 67.19%] [G loss: 0.899334]\n",
      "epoch:2 step:2516 [D loss: 0.690275, acc.: 58.59%] [G loss: 1.092101]\n",
      "epoch:2 step:2517 [D loss: 0.654011, acc.: 67.19%] [G loss: 1.047672]\n",
      "epoch:2 step:2518 [D loss: 0.633999, acc.: 63.28%] [G loss: 1.009503]\n",
      "epoch:2 step:2519 [D loss: 0.628778, acc.: 63.28%] [G loss: 1.052946]\n",
      "epoch:2 step:2520 [D loss: 0.627367, acc.: 64.84%] [G loss: 1.112664]\n",
      "epoch:2 step:2521 [D loss: 0.614035, acc.: 67.19%] [G loss: 1.219513]\n",
      "epoch:2 step:2522 [D loss: 0.583478, acc.: 68.75%] [G loss: 1.034544]\n",
      "epoch:2 step:2523 [D loss: 0.652236, acc.: 65.62%] [G loss: 1.095147]\n",
      "epoch:2 step:2524 [D loss: 0.588867, acc.: 71.09%] [G loss: 0.952473]\n",
      "epoch:2 step:2525 [D loss: 0.713301, acc.: 55.47%] [G loss: 0.956715]\n",
      "epoch:2 step:2526 [D loss: 0.657044, acc.: 63.28%] [G loss: 0.954517]\n",
      "epoch:2 step:2527 [D loss: 0.624242, acc.: 64.06%] [G loss: 1.040924]\n",
      "epoch:2 step:2528 [D loss: 0.603949, acc.: 66.41%] [G loss: 1.075148]\n",
      "epoch:2 step:2529 [D loss: 0.606746, acc.: 65.62%] [G loss: 1.200274]\n",
      "epoch:2 step:2530 [D loss: 0.662125, acc.: 60.94%] [G loss: 0.938315]\n",
      "epoch:2 step:2531 [D loss: 0.631954, acc.: 65.62%] [G loss: 1.081507]\n",
      "epoch:2 step:2532 [D loss: 0.706158, acc.: 56.25%] [G loss: 1.075431]\n",
      "epoch:2 step:2533 [D loss: 0.651267, acc.: 59.38%] [G loss: 0.943692]\n",
      "epoch:2 step:2534 [D loss: 0.563351, acc.: 70.31%] [G loss: 1.203485]\n",
      "epoch:2 step:2535 [D loss: 0.576846, acc.: 73.44%] [G loss: 1.168674]\n",
      "epoch:2 step:2536 [D loss: 0.620681, acc.: 60.16%] [G loss: 1.322824]\n",
      "epoch:2 step:2537 [D loss: 0.642743, acc.: 64.06%] [G loss: 1.143738]\n",
      "epoch:2 step:2538 [D loss: 0.576417, acc.: 73.44%] [G loss: 1.216746]\n",
      "epoch:2 step:2539 [D loss: 0.666348, acc.: 63.28%] [G loss: 1.195408]\n",
      "epoch:2 step:2540 [D loss: 0.626276, acc.: 67.19%] [G loss: 1.146680]\n",
      "epoch:2 step:2541 [D loss: 0.643989, acc.: 65.62%] [G loss: 1.047792]\n",
      "epoch:2 step:2542 [D loss: 0.696726, acc.: 55.47%] [G loss: 1.173938]\n",
      "epoch:2 step:2543 [D loss: 0.623102, acc.: 67.19%] [G loss: 1.129113]\n",
      "epoch:2 step:2544 [D loss: 0.649943, acc.: 67.19%] [G loss: 1.116740]\n",
      "epoch:2 step:2545 [D loss: 0.674557, acc.: 60.94%] [G loss: 0.987156]\n",
      "epoch:2 step:2546 [D loss: 0.653731, acc.: 60.94%] [G loss: 1.268315]\n",
      "epoch:2 step:2547 [D loss: 0.615350, acc.: 64.84%] [G loss: 1.103060]\n",
      "epoch:2 step:2548 [D loss: 0.633289, acc.: 64.84%] [G loss: 0.988551]\n",
      "epoch:2 step:2549 [D loss: 0.661462, acc.: 62.50%] [G loss: 1.068176]\n",
      "epoch:2 step:2550 [D loss: 0.629288, acc.: 65.62%] [G loss: 0.986930]\n",
      "epoch:2 step:2551 [D loss: 0.577334, acc.: 70.31%] [G loss: 1.169091]\n",
      "epoch:2 step:2552 [D loss: 0.621545, acc.: 65.62%] [G loss: 1.131840]\n",
      "epoch:2 step:2553 [D loss: 0.586804, acc.: 66.41%] [G loss: 0.903328]\n",
      "epoch:2 step:2554 [D loss: 0.662194, acc.: 57.81%] [G loss: 1.066616]\n",
      "epoch:2 step:2555 [D loss: 0.692410, acc.: 54.69%] [G loss: 1.006807]\n",
      "epoch:2 step:2556 [D loss: 0.641588, acc.: 60.94%] [G loss: 1.206083]\n",
      "epoch:2 step:2557 [D loss: 0.781190, acc.: 53.12%] [G loss: 1.121236]\n",
      "epoch:2 step:2558 [D loss: 0.540735, acc.: 75.00%] [G loss: 1.115818]\n",
      "epoch:2 step:2559 [D loss: 0.643219, acc.: 62.50%] [G loss: 0.988039]\n",
      "epoch:2 step:2560 [D loss: 0.589309, acc.: 67.97%] [G loss: 1.165032]\n",
      "epoch:2 step:2561 [D loss: 0.670642, acc.: 67.19%] [G loss: 1.008464]\n",
      "epoch:2 step:2562 [D loss: 0.652965, acc.: 64.06%] [G loss: 1.020043]\n",
      "epoch:2 step:2563 [D loss: 0.729839, acc.: 57.81%] [G loss: 0.972858]\n",
      "epoch:2 step:2564 [D loss: 0.570510, acc.: 69.53%] [G loss: 1.385894]\n",
      "epoch:2 step:2565 [D loss: 0.585493, acc.: 67.97%] [G loss: 1.394798]\n",
      "epoch:2 step:2566 [D loss: 0.620416, acc.: 65.62%] [G loss: 1.017797]\n",
      "epoch:2 step:2567 [D loss: 0.720286, acc.: 56.25%] [G loss: 0.911397]\n",
      "epoch:2 step:2568 [D loss: 0.611147, acc.: 65.62%] [G loss: 1.085730]\n",
      "epoch:2 step:2569 [D loss: 0.578794, acc.: 69.53%] [G loss: 1.258781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2570 [D loss: 0.566224, acc.: 71.88%] [G loss: 1.177779]\n",
      "epoch:2 step:2571 [D loss: 0.654866, acc.: 63.28%] [G loss: 1.272593]\n",
      "epoch:2 step:2572 [D loss: 0.590126, acc.: 68.75%] [G loss: 1.143388]\n",
      "epoch:2 step:2573 [D loss: 0.589637, acc.: 71.09%] [G loss: 1.040621]\n",
      "epoch:2 step:2574 [D loss: 0.551113, acc.: 73.44%] [G loss: 1.259044]\n",
      "epoch:2 step:2575 [D loss: 0.552247, acc.: 71.88%] [G loss: 1.242470]\n",
      "epoch:2 step:2576 [D loss: 0.556080, acc.: 71.88%] [G loss: 1.132637]\n",
      "epoch:2 step:2577 [D loss: 0.794694, acc.: 47.66%] [G loss: 1.072503]\n",
      "epoch:2 step:2578 [D loss: 0.617832, acc.: 68.75%] [G loss: 1.190387]\n",
      "epoch:2 step:2579 [D loss: 0.620830, acc.: 60.16%] [G loss: 1.225468]\n",
      "epoch:2 step:2580 [D loss: 0.675561, acc.: 61.72%] [G loss: 0.927693]\n",
      "epoch:2 step:2581 [D loss: 0.694605, acc.: 60.16%] [G loss: 1.132913]\n",
      "epoch:2 step:2582 [D loss: 0.658036, acc.: 60.94%] [G loss: 1.137032]\n",
      "epoch:2 step:2583 [D loss: 0.620896, acc.: 62.50%] [G loss: 1.175084]\n",
      "epoch:2 step:2584 [D loss: 0.670506, acc.: 57.81%] [G loss: 1.004728]\n",
      "epoch:2 step:2585 [D loss: 0.652106, acc.: 60.16%] [G loss: 1.019271]\n",
      "epoch:2 step:2586 [D loss: 0.655303, acc.: 65.62%] [G loss: 1.093382]\n",
      "epoch:2 step:2587 [D loss: 0.665432, acc.: 57.03%] [G loss: 1.295322]\n",
      "epoch:2 step:2588 [D loss: 0.793087, acc.: 46.88%] [G loss: 1.051514]\n",
      "epoch:2 step:2589 [D loss: 0.711149, acc.: 55.47%] [G loss: 0.877088]\n",
      "epoch:2 step:2590 [D loss: 0.611286, acc.: 66.41%] [G loss: 1.225659]\n",
      "epoch:2 step:2591 [D loss: 0.687924, acc.: 61.72%] [G loss: 1.255013]\n",
      "epoch:2 step:2592 [D loss: 0.630821, acc.: 67.19%] [G loss: 1.167081]\n",
      "epoch:2 step:2593 [D loss: 0.642879, acc.: 60.16%] [G loss: 1.027836]\n",
      "epoch:2 step:2594 [D loss: 0.677014, acc.: 56.25%] [G loss: 1.156352]\n",
      "epoch:2 step:2595 [D loss: 0.640417, acc.: 60.94%] [G loss: 0.993114]\n",
      "epoch:2 step:2596 [D loss: 0.662505, acc.: 59.38%] [G loss: 1.085051]\n",
      "epoch:2 step:2597 [D loss: 0.648916, acc.: 62.50%] [G loss: 1.127223]\n",
      "epoch:2 step:2598 [D loss: 0.602441, acc.: 65.62%] [G loss: 1.204818]\n",
      "epoch:2 step:2599 [D loss: 0.572271, acc.: 69.53%] [G loss: 1.162435]\n",
      "epoch:2 step:2600 [D loss: 0.556853, acc.: 69.53%] [G loss: 1.052137]\n",
      "##############\n",
      "[2.696843   1.86064599 1.76419527 2.83738831 0.62568982 6.47643258\n",
      " 1.98568329 2.94518871 3.71388522 4.15707399]\n",
      "##########\n",
      "epoch:2 step:2601 [D loss: 0.618621, acc.: 68.75%] [G loss: 1.159061]\n",
      "epoch:2 step:2602 [D loss: 0.613034, acc.: 69.53%] [G loss: 1.134020]\n",
      "epoch:2 step:2603 [D loss: 0.705654, acc.: 54.69%] [G loss: 1.069942]\n",
      "epoch:2 step:2604 [D loss: 0.657251, acc.: 63.28%] [G loss: 1.096618]\n",
      "epoch:2 step:2605 [D loss: 0.633895, acc.: 63.28%] [G loss: 1.170674]\n",
      "epoch:2 step:2606 [D loss: 0.635048, acc.: 65.62%] [G loss: 1.285213]\n",
      "epoch:2 step:2607 [D loss: 0.663841, acc.: 58.59%] [G loss: 1.175368]\n",
      "epoch:2 step:2608 [D loss: 0.615703, acc.: 66.41%] [G loss: 1.187631]\n",
      "epoch:2 step:2609 [D loss: 0.664362, acc.: 63.28%] [G loss: 1.148698]\n",
      "epoch:2 step:2610 [D loss: 0.630927, acc.: 64.84%] [G loss: 0.926648]\n",
      "epoch:2 step:2611 [D loss: 0.752347, acc.: 50.00%] [G loss: 1.057549]\n",
      "epoch:2 step:2612 [D loss: 0.595258, acc.: 66.41%] [G loss: 1.048074]\n",
      "epoch:2 step:2613 [D loss: 0.647197, acc.: 64.84%] [G loss: 1.157722]\n",
      "epoch:2 step:2614 [D loss: 0.728572, acc.: 54.69%] [G loss: 1.035750]\n",
      "epoch:2 step:2615 [D loss: 0.623695, acc.: 65.62%] [G loss: 1.047528]\n",
      "epoch:2 step:2616 [D loss: 0.648346, acc.: 64.84%] [G loss: 0.920215]\n",
      "epoch:2 step:2617 [D loss: 0.745808, acc.: 53.91%] [G loss: 1.176970]\n",
      "epoch:2 step:2618 [D loss: 0.702321, acc.: 55.47%] [G loss: 1.209023]\n",
      "epoch:2 step:2619 [D loss: 0.485713, acc.: 78.91%] [G loss: 1.351178]\n",
      "epoch:2 step:2620 [D loss: 0.550067, acc.: 72.66%] [G loss: 1.380243]\n",
      "epoch:2 step:2621 [D loss: 0.662984, acc.: 58.59%] [G loss: 1.024877]\n",
      "epoch:2 step:2622 [D loss: 0.589770, acc.: 69.53%] [G loss: 1.187133]\n",
      "epoch:2 step:2623 [D loss: 0.605442, acc.: 69.53%] [G loss: 1.194971]\n",
      "epoch:2 step:2624 [D loss: 0.653878, acc.: 63.28%] [G loss: 1.139160]\n",
      "epoch:2 step:2625 [D loss: 0.526983, acc.: 76.56%] [G loss: 1.096040]\n",
      "epoch:2 step:2626 [D loss: 0.651833, acc.: 63.28%] [G loss: 1.036411]\n",
      "epoch:2 step:2627 [D loss: 0.602231, acc.: 70.31%] [G loss: 1.189812]\n",
      "epoch:2 step:2628 [D loss: 0.736847, acc.: 54.69%] [G loss: 1.025552]\n",
      "epoch:2 step:2629 [D loss: 0.677435, acc.: 61.72%] [G loss: 1.070362]\n",
      "epoch:2 step:2630 [D loss: 0.731055, acc.: 56.25%] [G loss: 1.079963]\n",
      "epoch:2 step:2631 [D loss: 0.658424, acc.: 60.94%] [G loss: 1.248410]\n",
      "epoch:2 step:2632 [D loss: 0.644088, acc.: 64.84%] [G loss: 1.280144]\n",
      "epoch:2 step:2633 [D loss: 0.579912, acc.: 67.19%] [G loss: 1.043994]\n",
      "epoch:2 step:2634 [D loss: 0.589990, acc.: 67.19%] [G loss: 1.166452]\n",
      "epoch:2 step:2635 [D loss: 0.708579, acc.: 57.03%] [G loss: 1.024656]\n",
      "epoch:2 step:2636 [D loss: 0.599556, acc.: 69.53%] [G loss: 1.101199]\n",
      "epoch:2 step:2637 [D loss: 0.644700, acc.: 64.84%] [G loss: 1.081066]\n",
      "epoch:2 step:2638 [D loss: 0.565193, acc.: 71.88%] [G loss: 1.260413]\n",
      "epoch:2 step:2639 [D loss: 0.695303, acc.: 59.38%] [G loss: 0.977859]\n",
      "epoch:2 step:2640 [D loss: 0.619302, acc.: 67.97%] [G loss: 1.259056]\n",
      "epoch:2 step:2641 [D loss: 0.626657, acc.: 67.97%] [G loss: 1.210515]\n",
      "epoch:2 step:2642 [D loss: 0.711646, acc.: 53.91%] [G loss: 1.063151]\n",
      "epoch:2 step:2643 [D loss: 0.613868, acc.: 67.97%] [G loss: 1.074888]\n",
      "epoch:2 step:2644 [D loss: 0.628317, acc.: 62.50%] [G loss: 1.173193]\n",
      "epoch:2 step:2645 [D loss: 0.630184, acc.: 62.50%] [G loss: 1.103439]\n",
      "epoch:2 step:2646 [D loss: 0.604987, acc.: 63.28%] [G loss: 1.080892]\n",
      "epoch:2 step:2647 [D loss: 0.596146, acc.: 68.75%] [G loss: 1.065455]\n",
      "epoch:2 step:2648 [D loss: 0.596884, acc.: 69.53%] [G loss: 1.233368]\n",
      "epoch:2 step:2649 [D loss: 0.619170, acc.: 63.28%] [G loss: 1.147864]\n",
      "epoch:2 step:2650 [D loss: 0.618179, acc.: 68.75%] [G loss: 1.034785]\n",
      "epoch:2 step:2651 [D loss: 0.654477, acc.: 62.50%] [G loss: 1.134245]\n",
      "epoch:2 step:2652 [D loss: 0.579922, acc.: 71.09%] [G loss: 1.059989]\n",
      "epoch:2 step:2653 [D loss: 0.672988, acc.: 64.06%] [G loss: 0.995955]\n",
      "epoch:2 step:2654 [D loss: 0.775032, acc.: 52.34%] [G loss: 0.946288]\n",
      "epoch:2 step:2655 [D loss: 0.663600, acc.: 57.03%] [G loss: 1.051692]\n",
      "epoch:2 step:2656 [D loss: 0.548321, acc.: 68.75%] [G loss: 1.225742]\n",
      "epoch:2 step:2657 [D loss: 0.615194, acc.: 64.06%] [G loss: 1.086292]\n",
      "epoch:2 step:2658 [D loss: 0.682023, acc.: 61.72%] [G loss: 1.088206]\n",
      "epoch:2 step:2659 [D loss: 0.646576, acc.: 60.94%] [G loss: 0.967695]\n",
      "epoch:2 step:2660 [D loss: 0.666538, acc.: 61.72%] [G loss: 1.238753]\n",
      "epoch:2 step:2661 [D loss: 0.623685, acc.: 64.84%] [G loss: 1.266393]\n",
      "epoch:2 step:2662 [D loss: 0.605488, acc.: 75.00%] [G loss: 1.190509]\n",
      "epoch:2 step:2663 [D loss: 0.658851, acc.: 63.28%] [G loss: 1.097639]\n",
      "epoch:2 step:2664 [D loss: 0.616661, acc.: 64.84%] [G loss: 1.087103]\n",
      "epoch:2 step:2665 [D loss: 0.701636, acc.: 60.94%] [G loss: 1.060984]\n",
      "epoch:2 step:2666 [D loss: 0.609550, acc.: 67.97%] [G loss: 1.173007]\n",
      "epoch:2 step:2667 [D loss: 0.715015, acc.: 55.47%] [G loss: 1.046664]\n",
      "epoch:2 step:2668 [D loss: 0.600239, acc.: 69.53%] [G loss: 1.114675]\n",
      "epoch:2 step:2669 [D loss: 0.617257, acc.: 65.62%] [G loss: 1.249659]\n",
      "epoch:2 step:2670 [D loss: 0.647510, acc.: 61.72%] [G loss: 1.210033]\n",
      "epoch:2 step:2671 [D loss: 0.689501, acc.: 59.38%] [G loss: 1.240862]\n",
      "epoch:2 step:2672 [D loss: 0.668585, acc.: 62.50%] [G loss: 1.149124]\n",
      "epoch:2 step:2673 [D loss: 0.678135, acc.: 61.72%] [G loss: 1.185267]\n",
      "epoch:2 step:2674 [D loss: 0.643163, acc.: 63.28%] [G loss: 1.175449]\n",
      "epoch:2 step:2675 [D loss: 0.644737, acc.: 60.94%] [G loss: 1.238171]\n",
      "epoch:2 step:2676 [D loss: 0.640381, acc.: 64.84%] [G loss: 1.089214]\n",
      "epoch:2 step:2677 [D loss: 0.708224, acc.: 59.38%] [G loss: 0.985888]\n",
      "epoch:2 step:2678 [D loss: 0.668923, acc.: 63.28%] [G loss: 1.242057]\n",
      "epoch:2 step:2679 [D loss: 0.781640, acc.: 48.44%] [G loss: 0.936760]\n",
      "epoch:2 step:2680 [D loss: 0.688288, acc.: 59.38%] [G loss: 1.037753]\n",
      "epoch:2 step:2681 [D loss: 0.631952, acc.: 64.84%] [G loss: 1.111957]\n",
      "epoch:2 step:2682 [D loss: 0.665360, acc.: 62.50%] [G loss: 1.059056]\n",
      "epoch:2 step:2683 [D loss: 0.626774, acc.: 64.06%] [G loss: 0.934345]\n",
      "epoch:2 step:2684 [D loss: 0.609775, acc.: 64.84%] [G loss: 1.160917]\n",
      "epoch:2 step:2685 [D loss: 0.600622, acc.: 74.22%] [G loss: 1.009066]\n",
      "epoch:2 step:2686 [D loss: 0.635818, acc.: 65.62%] [G loss: 1.178011]\n",
      "epoch:2 step:2687 [D loss: 0.554910, acc.: 71.88%] [G loss: 1.204915]\n",
      "epoch:2 step:2688 [D loss: 0.701314, acc.: 51.56%] [G loss: 1.092658]\n",
      "epoch:2 step:2689 [D loss: 0.700505, acc.: 58.59%] [G loss: 1.053643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2690 [D loss: 0.595744, acc.: 70.31%] [G loss: 1.111063]\n",
      "epoch:2 step:2691 [D loss: 0.666248, acc.: 57.81%] [G loss: 1.030376]\n",
      "epoch:2 step:2692 [D loss: 0.694036, acc.: 57.81%] [G loss: 0.985638]\n",
      "epoch:2 step:2693 [D loss: 0.572722, acc.: 70.31%] [G loss: 1.095375]\n",
      "epoch:2 step:2694 [D loss: 0.602840, acc.: 61.72%] [G loss: 0.874365]\n",
      "epoch:2 step:2695 [D loss: 0.687506, acc.: 57.81%] [G loss: 1.021172]\n",
      "epoch:2 step:2696 [D loss: 0.572602, acc.: 74.22%] [G loss: 1.208527]\n",
      "epoch:2 step:2697 [D loss: 0.757367, acc.: 50.78%] [G loss: 1.004732]\n",
      "epoch:2 step:2698 [D loss: 0.656989, acc.: 61.72%] [G loss: 1.130537]\n",
      "epoch:2 step:2699 [D loss: 0.588590, acc.: 71.88%] [G loss: 1.006674]\n",
      "epoch:2 step:2700 [D loss: 0.580500, acc.: 71.88%] [G loss: 1.208515]\n",
      "epoch:2 step:2701 [D loss: 0.636174, acc.: 64.84%] [G loss: 1.195088]\n",
      "epoch:2 step:2702 [D loss: 0.696044, acc.: 58.59%] [G loss: 1.019190]\n",
      "epoch:2 step:2703 [D loss: 0.635090, acc.: 67.97%] [G loss: 1.080391]\n",
      "epoch:2 step:2704 [D loss: 0.668292, acc.: 60.94%] [G loss: 1.038248]\n",
      "epoch:2 step:2705 [D loss: 0.661590, acc.: 62.50%] [G loss: 1.208640]\n",
      "epoch:2 step:2706 [D loss: 0.573473, acc.: 67.97%] [G loss: 1.181727]\n",
      "epoch:2 step:2707 [D loss: 0.649216, acc.: 61.72%] [G loss: 1.203603]\n",
      "epoch:2 step:2708 [D loss: 0.735428, acc.: 53.91%] [G loss: 1.067945]\n",
      "epoch:2 step:2709 [D loss: 0.539566, acc.: 75.00%] [G loss: 1.246028]\n",
      "epoch:2 step:2710 [D loss: 0.643552, acc.: 64.06%] [G loss: 1.111123]\n",
      "epoch:2 step:2711 [D loss: 0.577667, acc.: 70.31%] [G loss: 1.159665]\n",
      "epoch:2 step:2712 [D loss: 0.680814, acc.: 60.16%] [G loss: 1.107220]\n",
      "epoch:2 step:2713 [D loss: 0.652338, acc.: 60.94%] [G loss: 1.067655]\n",
      "epoch:2 step:2714 [D loss: 0.635257, acc.: 65.62%] [G loss: 1.240199]\n",
      "epoch:2 step:2715 [D loss: 0.666474, acc.: 60.16%] [G loss: 1.036900]\n",
      "epoch:2 step:2716 [D loss: 0.691974, acc.: 53.12%] [G loss: 1.196895]\n",
      "epoch:2 step:2717 [D loss: 0.647183, acc.: 64.06%] [G loss: 1.139234]\n",
      "epoch:2 step:2718 [D loss: 0.651620, acc.: 60.94%] [G loss: 1.081563]\n",
      "epoch:2 step:2719 [D loss: 0.589560, acc.: 70.31%] [G loss: 1.131486]\n",
      "epoch:2 step:2720 [D loss: 0.633034, acc.: 64.84%] [G loss: 1.044956]\n",
      "epoch:2 step:2721 [D loss: 0.639894, acc.: 63.28%] [G loss: 0.967026]\n",
      "epoch:2 step:2722 [D loss: 0.615945, acc.: 67.19%] [G loss: 1.123995]\n",
      "epoch:2 step:2723 [D loss: 0.638383, acc.: 60.94%] [G loss: 1.055294]\n",
      "epoch:2 step:2724 [D loss: 0.582854, acc.: 71.88%] [G loss: 1.039515]\n",
      "epoch:2 step:2725 [D loss: 0.629758, acc.: 63.28%] [G loss: 1.117260]\n",
      "epoch:2 step:2726 [D loss: 0.542994, acc.: 73.44%] [G loss: 1.220071]\n",
      "epoch:2 step:2727 [D loss: 0.611498, acc.: 65.62%] [G loss: 1.099430]\n",
      "epoch:2 step:2728 [D loss: 0.664166, acc.: 59.38%] [G loss: 1.212058]\n",
      "epoch:2 step:2729 [D loss: 0.649566, acc.: 61.72%] [G loss: 1.161296]\n",
      "epoch:2 step:2730 [D loss: 0.547685, acc.: 74.22%] [G loss: 1.206368]\n",
      "epoch:2 step:2731 [D loss: 0.747823, acc.: 50.78%] [G loss: 1.020299]\n",
      "epoch:2 step:2732 [D loss: 0.614007, acc.: 66.41%] [G loss: 1.156132]\n",
      "epoch:2 step:2733 [D loss: 0.744617, acc.: 54.69%] [G loss: 1.099760]\n",
      "epoch:2 step:2734 [D loss: 0.633139, acc.: 64.84%] [G loss: 1.238073]\n",
      "epoch:2 step:2735 [D loss: 0.619123, acc.: 64.84%] [G loss: 1.305122]\n",
      "epoch:2 step:2736 [D loss: 0.679596, acc.: 58.59%] [G loss: 1.137595]\n",
      "epoch:2 step:2737 [D loss: 0.584153, acc.: 65.62%] [G loss: 1.168398]\n",
      "epoch:2 step:2738 [D loss: 0.696795, acc.: 57.03%] [G loss: 1.096948]\n",
      "epoch:2 step:2739 [D loss: 0.663337, acc.: 63.28%] [G loss: 1.218959]\n",
      "epoch:2 step:2740 [D loss: 0.506221, acc.: 76.56%] [G loss: 1.135782]\n",
      "epoch:2 step:2741 [D loss: 0.609926, acc.: 66.41%] [G loss: 1.111800]\n",
      "epoch:2 step:2742 [D loss: 0.545339, acc.: 76.56%] [G loss: 1.227568]\n",
      "epoch:2 step:2743 [D loss: 0.529420, acc.: 78.91%] [G loss: 1.246037]\n",
      "epoch:2 step:2744 [D loss: 0.658597, acc.: 60.16%] [G loss: 1.093571]\n",
      "epoch:2 step:2745 [D loss: 0.617261, acc.: 64.06%] [G loss: 1.108553]\n",
      "epoch:2 step:2746 [D loss: 0.667008, acc.: 60.16%] [G loss: 1.331983]\n",
      "epoch:2 step:2747 [D loss: 0.620472, acc.: 61.72%] [G loss: 1.011997]\n",
      "epoch:2 step:2748 [D loss: 0.691486, acc.: 60.94%] [G loss: 1.104361]\n",
      "epoch:2 step:2749 [D loss: 0.528173, acc.: 75.00%] [G loss: 1.283358]\n",
      "epoch:2 step:2750 [D loss: 0.656335, acc.: 60.16%] [G loss: 0.978480]\n",
      "epoch:2 step:2751 [D loss: 0.583009, acc.: 72.66%] [G loss: 1.048321]\n",
      "epoch:2 step:2752 [D loss: 0.721091, acc.: 56.25%] [G loss: 1.036456]\n",
      "epoch:2 step:2753 [D loss: 0.686323, acc.: 57.81%] [G loss: 1.218408]\n",
      "epoch:2 step:2754 [D loss: 0.509267, acc.: 75.00%] [G loss: 1.280784]\n",
      "epoch:2 step:2755 [D loss: 0.732834, acc.: 46.88%] [G loss: 1.218411]\n",
      "epoch:2 step:2756 [D loss: 0.576743, acc.: 74.22%] [G loss: 1.059294]\n",
      "epoch:2 step:2757 [D loss: 0.640786, acc.: 66.41%] [G loss: 1.065678]\n",
      "epoch:2 step:2758 [D loss: 0.581589, acc.: 70.31%] [G loss: 1.266581]\n",
      "epoch:2 step:2759 [D loss: 0.624220, acc.: 67.19%] [G loss: 1.211160]\n",
      "epoch:2 step:2760 [D loss: 0.532652, acc.: 75.00%] [G loss: 1.086755]\n",
      "epoch:2 step:2761 [D loss: 0.608893, acc.: 67.97%] [G loss: 1.132856]\n",
      "epoch:2 step:2762 [D loss: 0.628138, acc.: 63.28%] [G loss: 1.122951]\n",
      "epoch:2 step:2763 [D loss: 0.548286, acc.: 77.34%] [G loss: 1.128024]\n",
      "epoch:2 step:2764 [D loss: 0.620490, acc.: 58.59%] [G loss: 1.173944]\n",
      "epoch:2 step:2765 [D loss: 0.714948, acc.: 54.69%] [G loss: 0.958585]\n",
      "epoch:2 step:2766 [D loss: 0.640138, acc.: 60.94%] [G loss: 0.955631]\n",
      "epoch:2 step:2767 [D loss: 0.592722, acc.: 71.88%] [G loss: 1.030420]\n",
      "epoch:2 step:2768 [D loss: 0.705989, acc.: 54.69%] [G loss: 1.119562]\n",
      "epoch:2 step:2769 [D loss: 0.544224, acc.: 72.66%] [G loss: 1.166573]\n",
      "epoch:2 step:2770 [D loss: 0.626117, acc.: 57.81%] [G loss: 1.222284]\n",
      "epoch:2 step:2771 [D loss: 0.589306, acc.: 72.66%] [G loss: 1.239340]\n",
      "epoch:2 step:2772 [D loss: 0.578432, acc.: 71.88%] [G loss: 1.167791]\n",
      "epoch:2 step:2773 [D loss: 0.669202, acc.: 62.50%] [G loss: 1.241163]\n",
      "epoch:2 step:2774 [D loss: 0.742736, acc.: 55.47%] [G loss: 0.987538]\n",
      "epoch:2 step:2775 [D loss: 0.707708, acc.: 55.47%] [G loss: 1.181063]\n",
      "epoch:2 step:2776 [D loss: 0.550496, acc.: 75.00%] [G loss: 1.233324]\n",
      "epoch:2 step:2777 [D loss: 0.610706, acc.: 64.84%] [G loss: 1.130067]\n",
      "epoch:2 step:2778 [D loss: 0.575203, acc.: 67.97%] [G loss: 1.232309]\n",
      "epoch:2 step:2779 [D loss: 0.606419, acc.: 66.41%] [G loss: 1.023401]\n",
      "epoch:2 step:2780 [D loss: 0.674046, acc.: 60.16%] [G loss: 0.993804]\n",
      "epoch:2 step:2781 [D loss: 0.720711, acc.: 57.81%] [G loss: 1.172040]\n",
      "epoch:2 step:2782 [D loss: 0.702146, acc.: 54.69%] [G loss: 1.270017]\n",
      "epoch:2 step:2783 [D loss: 0.687263, acc.: 59.38%] [G loss: 1.125679]\n",
      "epoch:2 step:2784 [D loss: 0.644929, acc.: 61.72%] [G loss: 1.001553]\n",
      "epoch:2 step:2785 [D loss: 0.618987, acc.: 66.41%] [G loss: 1.015108]\n",
      "epoch:2 step:2786 [D loss: 0.706014, acc.: 56.25%] [G loss: 0.997880]\n",
      "epoch:2 step:2787 [D loss: 0.695142, acc.: 57.81%] [G loss: 1.038427]\n",
      "epoch:2 step:2788 [D loss: 0.568602, acc.: 67.97%] [G loss: 1.127617]\n",
      "epoch:2 step:2789 [D loss: 0.678148, acc.: 58.59%] [G loss: 1.101881]\n",
      "epoch:2 step:2790 [D loss: 0.623728, acc.: 71.09%] [G loss: 1.071362]\n",
      "epoch:2 step:2791 [D loss: 0.660354, acc.: 58.59%] [G loss: 1.069318]\n",
      "epoch:2 step:2792 [D loss: 0.602566, acc.: 64.84%] [G loss: 1.068701]\n",
      "epoch:2 step:2793 [D loss: 0.620495, acc.: 60.94%] [G loss: 0.849383]\n",
      "epoch:2 step:2794 [D loss: 0.675709, acc.: 64.84%] [G loss: 1.137866]\n",
      "epoch:2 step:2795 [D loss: 0.701213, acc.: 50.78%] [G loss: 1.131076]\n",
      "epoch:2 step:2796 [D loss: 0.544102, acc.: 75.78%] [G loss: 1.171497]\n",
      "epoch:2 step:2797 [D loss: 0.595634, acc.: 71.09%] [G loss: 1.195191]\n",
      "epoch:2 step:2798 [D loss: 0.681738, acc.: 61.72%] [G loss: 1.065631]\n",
      "epoch:2 step:2799 [D loss: 0.702994, acc.: 55.47%] [G loss: 1.127841]\n",
      "epoch:2 step:2800 [D loss: 0.510031, acc.: 78.91%] [G loss: 1.079697]\n",
      "##############\n",
      "[2.70644844 1.94983271 1.87479894 2.9389519  1.01192503 6.02048577\n",
      " 2.19821396 3.12060886 3.91395111 5.73368444]\n",
      "##########\n",
      "epoch:2 step:2801 [D loss: 0.590577, acc.: 65.62%] [G loss: 0.980859]\n",
      "epoch:2 step:2802 [D loss: 0.644165, acc.: 63.28%] [G loss: 0.974656]\n",
      "epoch:2 step:2803 [D loss: 0.714887, acc.: 59.38%] [G loss: 1.184174]\n",
      "epoch:2 step:2804 [D loss: 0.553220, acc.: 65.62%] [G loss: 1.193465]\n",
      "epoch:2 step:2805 [D loss: 0.697077, acc.: 56.25%] [G loss: 1.138987]\n",
      "epoch:2 step:2806 [D loss: 0.676496, acc.: 55.47%] [G loss: 0.964506]\n",
      "epoch:2 step:2807 [D loss: 0.668822, acc.: 63.28%] [G loss: 1.091168]\n",
      "epoch:2 step:2808 [D loss: 0.527264, acc.: 78.91%] [G loss: 1.082023]\n",
      "epoch:2 step:2809 [D loss: 0.563320, acc.: 74.22%] [G loss: 1.119205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2810 [D loss: 0.560770, acc.: 71.09%] [G loss: 1.069485]\n",
      "epoch:2 step:2811 [D loss: 0.722001, acc.: 54.69%] [G loss: 1.052279]\n",
      "epoch:3 step:2812 [D loss: 0.588474, acc.: 69.53%] [G loss: 1.358946]\n",
      "epoch:3 step:2813 [D loss: 0.647469, acc.: 60.94%] [G loss: 1.147334]\n",
      "epoch:3 step:2814 [D loss: 0.517020, acc.: 78.91%] [G loss: 1.191668]\n",
      "epoch:3 step:2815 [D loss: 0.642568, acc.: 60.94%] [G loss: 1.208980]\n",
      "epoch:3 step:2816 [D loss: 0.707488, acc.: 59.38%] [G loss: 1.251684]\n",
      "epoch:3 step:2817 [D loss: 0.662109, acc.: 59.38%] [G loss: 1.004575]\n",
      "epoch:3 step:2818 [D loss: 0.610661, acc.: 71.09%] [G loss: 1.015672]\n",
      "epoch:3 step:2819 [D loss: 0.591531, acc.: 63.28%] [G loss: 1.017709]\n",
      "epoch:3 step:2820 [D loss: 0.566142, acc.: 68.75%] [G loss: 1.038295]\n",
      "epoch:3 step:2821 [D loss: 0.636413, acc.: 62.50%] [G loss: 0.964081]\n",
      "epoch:3 step:2822 [D loss: 0.590983, acc.: 67.97%] [G loss: 1.102830]\n",
      "epoch:3 step:2823 [D loss: 0.626331, acc.: 70.31%] [G loss: 1.187009]\n",
      "epoch:3 step:2824 [D loss: 0.612495, acc.: 64.06%] [G loss: 1.193570]\n",
      "epoch:3 step:2825 [D loss: 0.604739, acc.: 63.28%] [G loss: 1.194878]\n",
      "epoch:3 step:2826 [D loss: 0.567434, acc.: 71.88%] [G loss: 1.122141]\n",
      "epoch:3 step:2827 [D loss: 0.584354, acc.: 72.66%] [G loss: 1.204338]\n",
      "epoch:3 step:2828 [D loss: 0.608852, acc.: 67.97%] [G loss: 1.113214]\n",
      "epoch:3 step:2829 [D loss: 0.555396, acc.: 73.44%] [G loss: 1.204700]\n",
      "epoch:3 step:2830 [D loss: 0.607600, acc.: 67.19%] [G loss: 1.144097]\n",
      "epoch:3 step:2831 [D loss: 0.649593, acc.: 61.72%] [G loss: 1.088050]\n",
      "epoch:3 step:2832 [D loss: 0.632063, acc.: 67.19%] [G loss: 1.026174]\n",
      "epoch:3 step:2833 [D loss: 0.614576, acc.: 62.50%] [G loss: 1.248350]\n",
      "epoch:3 step:2834 [D loss: 0.602525, acc.: 71.09%] [G loss: 1.048491]\n",
      "epoch:3 step:2835 [D loss: 0.566749, acc.: 75.00%] [G loss: 1.133906]\n",
      "epoch:3 step:2836 [D loss: 0.618966, acc.: 67.19%] [G loss: 0.976583]\n",
      "epoch:3 step:2837 [D loss: 0.732667, acc.: 52.34%] [G loss: 1.035513]\n",
      "epoch:3 step:2838 [D loss: 0.622334, acc.: 66.41%] [G loss: 1.068057]\n",
      "epoch:3 step:2839 [D loss: 0.594009, acc.: 70.31%] [G loss: 1.067993]\n",
      "epoch:3 step:2840 [D loss: 0.686816, acc.: 60.16%] [G loss: 0.954868]\n",
      "epoch:3 step:2841 [D loss: 0.594183, acc.: 67.97%] [G loss: 1.091675]\n",
      "epoch:3 step:2842 [D loss: 0.750714, acc.: 50.78%] [G loss: 1.054229]\n",
      "epoch:3 step:2843 [D loss: 0.559231, acc.: 73.44%] [G loss: 1.077837]\n",
      "epoch:3 step:2844 [D loss: 0.702702, acc.: 60.16%] [G loss: 1.197893]\n",
      "epoch:3 step:2845 [D loss: 0.612475, acc.: 64.06%] [G loss: 1.000978]\n",
      "epoch:3 step:2846 [D loss: 0.617442, acc.: 67.97%] [G loss: 1.151716]\n",
      "epoch:3 step:2847 [D loss: 0.559617, acc.: 74.22%] [G loss: 1.110096]\n",
      "epoch:3 step:2848 [D loss: 0.582313, acc.: 68.75%] [G loss: 1.213153]\n",
      "epoch:3 step:2849 [D loss: 0.606360, acc.: 67.97%] [G loss: 0.999385]\n",
      "epoch:3 step:2850 [D loss: 0.616002, acc.: 64.84%] [G loss: 1.054648]\n",
      "epoch:3 step:2851 [D loss: 0.663453, acc.: 63.28%] [G loss: 1.259308]\n",
      "epoch:3 step:2852 [D loss: 0.627521, acc.: 66.41%] [G loss: 1.217776]\n",
      "epoch:3 step:2853 [D loss: 0.698282, acc.: 55.47%] [G loss: 1.197623]\n",
      "epoch:3 step:2854 [D loss: 0.621684, acc.: 67.19%] [G loss: 1.229516]\n",
      "epoch:3 step:2855 [D loss: 0.667849, acc.: 58.59%] [G loss: 1.192898]\n",
      "epoch:3 step:2856 [D loss: 0.663658, acc.: 57.03%] [G loss: 1.042849]\n",
      "epoch:3 step:2857 [D loss: 0.547740, acc.: 71.09%] [G loss: 1.170192]\n",
      "epoch:3 step:2858 [D loss: 0.706727, acc.: 54.69%] [G loss: 1.043302]\n",
      "epoch:3 step:2859 [D loss: 0.525765, acc.: 72.66%] [G loss: 1.139586]\n",
      "epoch:3 step:2860 [D loss: 0.620409, acc.: 63.28%] [G loss: 1.239186]\n",
      "epoch:3 step:2861 [D loss: 0.548720, acc.: 70.31%] [G loss: 1.192225]\n",
      "epoch:3 step:2862 [D loss: 0.645108, acc.: 61.72%] [G loss: 1.029238]\n",
      "epoch:3 step:2863 [D loss: 0.634391, acc.: 64.84%] [G loss: 1.129897]\n",
      "epoch:3 step:2864 [D loss: 0.583117, acc.: 70.31%] [G loss: 1.172915]\n",
      "epoch:3 step:2865 [D loss: 0.568477, acc.: 69.53%] [G loss: 1.206740]\n",
      "epoch:3 step:2866 [D loss: 0.614380, acc.: 66.41%] [G loss: 1.175134]\n",
      "epoch:3 step:2867 [D loss: 0.708227, acc.: 54.69%] [G loss: 1.021710]\n",
      "epoch:3 step:2868 [D loss: 0.694179, acc.: 56.25%] [G loss: 1.429468]\n",
      "epoch:3 step:2869 [D loss: 0.572935, acc.: 69.53%] [G loss: 1.192847]\n",
      "epoch:3 step:2870 [D loss: 0.640830, acc.: 63.28%] [G loss: 1.115158]\n",
      "epoch:3 step:2871 [D loss: 0.580207, acc.: 66.41%] [G loss: 1.150493]\n",
      "epoch:3 step:2872 [D loss: 0.587345, acc.: 71.88%] [G loss: 1.274899]\n",
      "epoch:3 step:2873 [D loss: 0.749012, acc.: 50.78%] [G loss: 1.077395]\n",
      "epoch:3 step:2874 [D loss: 0.656982, acc.: 62.50%] [G loss: 1.119115]\n",
      "epoch:3 step:2875 [D loss: 0.641420, acc.: 64.84%] [G loss: 1.153460]\n",
      "epoch:3 step:2876 [D loss: 0.654492, acc.: 62.50%] [G loss: 1.024197]\n",
      "epoch:3 step:2877 [D loss: 0.662137, acc.: 60.94%] [G loss: 0.945883]\n",
      "epoch:3 step:2878 [D loss: 0.576663, acc.: 67.97%] [G loss: 1.025387]\n",
      "epoch:3 step:2879 [D loss: 0.577120, acc.: 73.44%] [G loss: 1.133292]\n",
      "epoch:3 step:2880 [D loss: 0.590284, acc.: 69.53%] [G loss: 1.145093]\n",
      "epoch:3 step:2881 [D loss: 0.625089, acc.: 63.28%] [G loss: 1.048506]\n",
      "epoch:3 step:2882 [D loss: 0.692751, acc.: 59.38%] [G loss: 0.968525]\n",
      "epoch:3 step:2883 [D loss: 0.555999, acc.: 70.31%] [G loss: 1.151601]\n",
      "epoch:3 step:2884 [D loss: 0.604041, acc.: 64.84%] [G loss: 1.312314]\n",
      "epoch:3 step:2885 [D loss: 0.650716, acc.: 63.28%] [G loss: 1.084841]\n",
      "epoch:3 step:2886 [D loss: 0.674766, acc.: 59.38%] [G loss: 1.083385]\n",
      "epoch:3 step:2887 [D loss: 0.582014, acc.: 68.75%] [G loss: 1.262184]\n",
      "epoch:3 step:2888 [D loss: 0.700245, acc.: 62.50%] [G loss: 1.099577]\n",
      "epoch:3 step:2889 [D loss: 0.671923, acc.: 57.03%] [G loss: 1.142256]\n",
      "epoch:3 step:2890 [D loss: 0.697254, acc.: 60.16%] [G loss: 1.275482]\n",
      "epoch:3 step:2891 [D loss: 0.612822, acc.: 64.84%] [G loss: 1.344643]\n",
      "epoch:3 step:2892 [D loss: 0.667285, acc.: 60.16%] [G loss: 1.104965]\n",
      "epoch:3 step:2893 [D loss: 0.597637, acc.: 71.09%] [G loss: 1.063519]\n",
      "epoch:3 step:2894 [D loss: 0.570571, acc.: 70.31%] [G loss: 1.174251]\n",
      "epoch:3 step:2895 [D loss: 0.682992, acc.: 58.59%] [G loss: 1.063098]\n",
      "epoch:3 step:2896 [D loss: 0.618997, acc.: 61.72%] [G loss: 1.049096]\n",
      "epoch:3 step:2897 [D loss: 0.668817, acc.: 64.06%] [G loss: 1.008209]\n",
      "epoch:3 step:2898 [D loss: 0.595095, acc.: 66.41%] [G loss: 1.074174]\n",
      "epoch:3 step:2899 [D loss: 0.577552, acc.: 73.44%] [G loss: 1.117256]\n",
      "epoch:3 step:2900 [D loss: 0.623315, acc.: 62.50%] [G loss: 1.327810]\n",
      "epoch:3 step:2901 [D loss: 0.648232, acc.: 60.94%] [G loss: 1.166009]\n",
      "epoch:3 step:2902 [D loss: 0.548630, acc.: 70.31%] [G loss: 1.169011]\n",
      "epoch:3 step:2903 [D loss: 0.582663, acc.: 64.84%] [G loss: 1.209404]\n",
      "epoch:3 step:2904 [D loss: 0.600616, acc.: 71.88%] [G loss: 1.116345]\n",
      "epoch:3 step:2905 [D loss: 0.669686, acc.: 68.75%] [G loss: 1.023210]\n",
      "epoch:3 step:2906 [D loss: 0.640325, acc.: 64.06%] [G loss: 1.194408]\n",
      "epoch:3 step:2907 [D loss: 0.579923, acc.: 68.75%] [G loss: 1.104669]\n",
      "epoch:3 step:2908 [D loss: 0.669089, acc.: 62.50%] [G loss: 1.024837]\n",
      "epoch:3 step:2909 [D loss: 0.626249, acc.: 62.50%] [G loss: 0.878715]\n",
      "epoch:3 step:2910 [D loss: 0.662845, acc.: 58.59%] [G loss: 1.060323]\n",
      "epoch:3 step:2911 [D loss: 0.614403, acc.: 69.53%] [G loss: 1.222480]\n",
      "epoch:3 step:2912 [D loss: 0.608070, acc.: 65.62%] [G loss: 1.179886]\n",
      "epoch:3 step:2913 [D loss: 0.737915, acc.: 46.88%] [G loss: 1.263831]\n",
      "epoch:3 step:2914 [D loss: 0.550062, acc.: 75.78%] [G loss: 1.341783]\n",
      "epoch:3 step:2915 [D loss: 0.601250, acc.: 71.09%] [G loss: 1.131879]\n",
      "epoch:3 step:2916 [D loss: 0.520047, acc.: 77.34%] [G loss: 1.134300]\n",
      "epoch:3 step:2917 [D loss: 0.590773, acc.: 70.31%] [G loss: 1.149653]\n",
      "epoch:3 step:2918 [D loss: 0.621568, acc.: 63.28%] [G loss: 1.169384]\n",
      "epoch:3 step:2919 [D loss: 0.621779, acc.: 67.19%] [G loss: 1.076594]\n",
      "epoch:3 step:2920 [D loss: 0.702023, acc.: 57.03%] [G loss: 1.309811]\n",
      "epoch:3 step:2921 [D loss: 0.667370, acc.: 64.84%] [G loss: 1.053247]\n",
      "epoch:3 step:2922 [D loss: 0.653063, acc.: 60.94%] [G loss: 1.057502]\n",
      "epoch:3 step:2923 [D loss: 0.637359, acc.: 62.50%] [G loss: 1.221316]\n",
      "epoch:3 step:2924 [D loss: 0.720998, acc.: 57.81%] [G loss: 1.078357]\n",
      "epoch:3 step:2925 [D loss: 0.674292, acc.: 62.50%] [G loss: 1.234515]\n",
      "epoch:3 step:2926 [D loss: 0.716573, acc.: 50.78%] [G loss: 0.808844]\n",
      "epoch:3 step:2927 [D loss: 0.622972, acc.: 65.62%] [G loss: 1.073022]\n",
      "epoch:3 step:2928 [D loss: 0.739433, acc.: 54.69%] [G loss: 1.108052]\n",
      "epoch:3 step:2929 [D loss: 0.656405, acc.: 62.50%] [G loss: 1.175596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2930 [D loss: 0.650492, acc.: 67.19%] [G loss: 1.179422]\n",
      "epoch:3 step:2931 [D loss: 0.680446, acc.: 59.38%] [G loss: 0.984262]\n",
      "epoch:3 step:2932 [D loss: 0.701591, acc.: 57.81%] [G loss: 1.055468]\n",
      "epoch:3 step:2933 [D loss: 0.641327, acc.: 62.50%] [G loss: 1.019728]\n",
      "epoch:3 step:2934 [D loss: 0.591259, acc.: 69.53%] [G loss: 1.079643]\n",
      "epoch:3 step:2935 [D loss: 0.643909, acc.: 57.81%] [G loss: 1.122211]\n",
      "epoch:3 step:2936 [D loss: 0.643732, acc.: 62.50%] [G loss: 1.071911]\n",
      "epoch:3 step:2937 [D loss: 0.598236, acc.: 68.75%] [G loss: 1.271833]\n",
      "epoch:3 step:2938 [D loss: 0.656860, acc.: 67.19%] [G loss: 1.122255]\n",
      "epoch:3 step:2939 [D loss: 0.638979, acc.: 64.84%] [G loss: 1.086347]\n",
      "epoch:3 step:2940 [D loss: 0.652827, acc.: 62.50%] [G loss: 1.002699]\n",
      "epoch:3 step:2941 [D loss: 0.625420, acc.: 67.19%] [G loss: 1.065865]\n",
      "epoch:3 step:2942 [D loss: 0.662729, acc.: 61.72%] [G loss: 1.131431]\n",
      "epoch:3 step:2943 [D loss: 0.760746, acc.: 43.75%] [G loss: 1.112880]\n",
      "epoch:3 step:2944 [D loss: 0.636882, acc.: 65.62%] [G loss: 1.146983]\n",
      "epoch:3 step:2945 [D loss: 0.693500, acc.: 59.38%] [G loss: 0.972445]\n",
      "epoch:3 step:2946 [D loss: 0.650806, acc.: 57.81%] [G loss: 1.034238]\n",
      "epoch:3 step:2947 [D loss: 0.603371, acc.: 65.62%] [G loss: 1.128372]\n",
      "epoch:3 step:2948 [D loss: 0.692842, acc.: 58.59%] [G loss: 1.031773]\n",
      "epoch:3 step:2949 [D loss: 0.574428, acc.: 67.19%] [G loss: 1.133896]\n",
      "epoch:3 step:2950 [D loss: 0.651611, acc.: 59.38%] [G loss: 1.125084]\n",
      "epoch:3 step:2951 [D loss: 0.737703, acc.: 50.78%] [G loss: 0.938774]\n",
      "epoch:3 step:2952 [D loss: 0.663832, acc.: 62.50%] [G loss: 1.102348]\n",
      "epoch:3 step:2953 [D loss: 0.546335, acc.: 73.44%] [G loss: 1.113830]\n",
      "epoch:3 step:2954 [D loss: 0.711468, acc.: 55.47%] [G loss: 1.179839]\n",
      "epoch:3 step:2955 [D loss: 0.584256, acc.: 70.31%] [G loss: 1.305987]\n",
      "epoch:3 step:2956 [D loss: 0.650597, acc.: 62.50%] [G loss: 1.097033]\n",
      "epoch:3 step:2957 [D loss: 0.716943, acc.: 55.47%] [G loss: 1.050855]\n",
      "epoch:3 step:2958 [D loss: 0.709409, acc.: 53.91%] [G loss: 0.970986]\n",
      "epoch:3 step:2959 [D loss: 0.633612, acc.: 64.06%] [G loss: 1.178945]\n",
      "epoch:3 step:2960 [D loss: 0.668678, acc.: 56.25%] [G loss: 1.073429]\n",
      "epoch:3 step:2961 [D loss: 0.737099, acc.: 52.34%] [G loss: 1.042161]\n",
      "epoch:3 step:2962 [D loss: 0.626140, acc.: 65.62%] [G loss: 1.123599]\n",
      "epoch:3 step:2963 [D loss: 0.647951, acc.: 57.81%] [G loss: 1.079705]\n",
      "epoch:3 step:2964 [D loss: 0.649298, acc.: 64.06%] [G loss: 1.023046]\n",
      "epoch:3 step:2965 [D loss: 0.643240, acc.: 65.62%] [G loss: 1.176469]\n",
      "epoch:3 step:2966 [D loss: 0.637995, acc.: 66.41%] [G loss: 1.014875]\n",
      "epoch:3 step:2967 [D loss: 0.538663, acc.: 78.91%] [G loss: 1.136618]\n",
      "epoch:3 step:2968 [D loss: 0.706086, acc.: 53.91%] [G loss: 1.028642]\n",
      "epoch:3 step:2969 [D loss: 0.650804, acc.: 64.06%] [G loss: 1.075592]\n",
      "epoch:3 step:2970 [D loss: 0.632440, acc.: 57.81%] [G loss: 1.064381]\n",
      "epoch:3 step:2971 [D loss: 0.593857, acc.: 67.97%] [G loss: 1.145177]\n",
      "epoch:3 step:2972 [D loss: 0.568258, acc.: 71.09%] [G loss: 1.213138]\n",
      "epoch:3 step:2973 [D loss: 0.614779, acc.: 66.41%] [G loss: 1.200960]\n",
      "epoch:3 step:2974 [D loss: 0.681481, acc.: 60.94%] [G loss: 0.893372]\n",
      "epoch:3 step:2975 [D loss: 0.589101, acc.: 70.31%] [G loss: 1.198428]\n",
      "epoch:3 step:2976 [D loss: 0.663066, acc.: 61.72%] [G loss: 1.085024]\n",
      "epoch:3 step:2977 [D loss: 0.552022, acc.: 71.09%] [G loss: 1.100334]\n",
      "epoch:3 step:2978 [D loss: 0.635605, acc.: 62.50%] [G loss: 1.109083]\n",
      "epoch:3 step:2979 [D loss: 0.643952, acc.: 63.28%] [G loss: 1.141852]\n",
      "epoch:3 step:2980 [D loss: 0.607516, acc.: 68.75%] [G loss: 1.230495]\n",
      "epoch:3 step:2981 [D loss: 0.634668, acc.: 67.19%] [G loss: 0.965769]\n",
      "epoch:3 step:2982 [D loss: 0.604184, acc.: 68.75%] [G loss: 0.986230]\n",
      "epoch:3 step:2983 [D loss: 0.614786, acc.: 64.06%] [G loss: 1.216249]\n",
      "epoch:3 step:2984 [D loss: 0.610065, acc.: 67.97%] [G loss: 1.219766]\n",
      "epoch:3 step:2985 [D loss: 0.665953, acc.: 61.72%] [G loss: 1.090055]\n",
      "epoch:3 step:2986 [D loss: 0.617541, acc.: 63.28%] [G loss: 1.131684]\n",
      "epoch:3 step:2987 [D loss: 0.682944, acc.: 56.25%] [G loss: 1.015821]\n",
      "epoch:3 step:2988 [D loss: 0.656006, acc.: 57.81%] [G loss: 1.070706]\n",
      "epoch:3 step:2989 [D loss: 0.688691, acc.: 58.59%] [G loss: 1.285227]\n",
      "epoch:3 step:2990 [D loss: 0.588113, acc.: 65.62%] [G loss: 1.195744]\n",
      "epoch:3 step:2991 [D loss: 0.724659, acc.: 53.91%] [G loss: 1.018334]\n",
      "epoch:3 step:2992 [D loss: 0.688417, acc.: 58.59%] [G loss: 1.244966]\n",
      "epoch:3 step:2993 [D loss: 0.575096, acc.: 71.88%] [G loss: 1.166197]\n",
      "epoch:3 step:2994 [D loss: 0.640140, acc.: 64.06%] [G loss: 1.022326]\n",
      "epoch:3 step:2995 [D loss: 0.622678, acc.: 67.97%] [G loss: 1.140095]\n",
      "epoch:3 step:2996 [D loss: 0.635960, acc.: 62.50%] [G loss: 1.174412]\n",
      "epoch:3 step:2997 [D loss: 0.646637, acc.: 64.06%] [G loss: 0.984588]\n",
      "epoch:3 step:2998 [D loss: 0.564390, acc.: 72.66%] [G loss: 1.147488]\n",
      "epoch:3 step:2999 [D loss: 0.712708, acc.: 55.47%] [G loss: 0.972521]\n",
      "epoch:3 step:3000 [D loss: 0.572926, acc.: 69.53%] [G loss: 1.167244]\n",
      "##############\n",
      "[2.7604442  2.33566789 2.02684456 2.91872497 1.10992569 6.49788318\n",
      " 1.97671485 3.17114167 3.85669367 4.14700716]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.509182, acc.: 80.47%] [G loss: 1.178181]\n",
      "epoch:3 step:3002 [D loss: 0.599150, acc.: 65.62%] [G loss: 1.049106]\n",
      "epoch:3 step:3003 [D loss: 0.654599, acc.: 63.28%] [G loss: 1.284447]\n",
      "epoch:3 step:3004 [D loss: 0.731788, acc.: 50.00%] [G loss: 0.961957]\n",
      "epoch:3 step:3005 [D loss: 0.609285, acc.: 65.62%] [G loss: 1.096412]\n",
      "epoch:3 step:3006 [D loss: 0.685186, acc.: 57.81%] [G loss: 1.033114]\n",
      "epoch:3 step:3007 [D loss: 0.644857, acc.: 65.62%] [G loss: 1.048090]\n",
      "epoch:3 step:3008 [D loss: 0.600909, acc.: 71.09%] [G loss: 1.227778]\n",
      "epoch:3 step:3009 [D loss: 0.631222, acc.: 66.41%] [G loss: 0.987866]\n",
      "epoch:3 step:3010 [D loss: 0.549666, acc.: 70.31%] [G loss: 1.185727]\n",
      "epoch:3 step:3011 [D loss: 0.577538, acc.: 69.53%] [G loss: 1.330580]\n",
      "epoch:3 step:3012 [D loss: 0.614680, acc.: 67.97%] [G loss: 1.289359]\n",
      "epoch:3 step:3013 [D loss: 0.633105, acc.: 66.41%] [G loss: 1.002862]\n",
      "epoch:3 step:3014 [D loss: 0.568509, acc.: 73.44%] [G loss: 1.138499]\n",
      "epoch:3 step:3015 [D loss: 0.601573, acc.: 70.31%] [G loss: 1.002901]\n",
      "epoch:3 step:3016 [D loss: 0.581808, acc.: 70.31%] [G loss: 1.093671]\n",
      "epoch:3 step:3017 [D loss: 0.633182, acc.: 64.84%] [G loss: 1.091629]\n",
      "epoch:3 step:3018 [D loss: 0.689367, acc.: 55.47%] [G loss: 1.035545]\n",
      "epoch:3 step:3019 [D loss: 0.607652, acc.: 64.84%] [G loss: 0.996315]\n",
      "epoch:3 step:3020 [D loss: 0.621254, acc.: 65.62%] [G loss: 0.949393]\n",
      "epoch:3 step:3021 [D loss: 0.659865, acc.: 59.38%] [G loss: 1.096711]\n",
      "epoch:3 step:3022 [D loss: 0.716567, acc.: 57.81%] [G loss: 1.138761]\n",
      "epoch:3 step:3023 [D loss: 0.629122, acc.: 68.75%] [G loss: 1.096542]\n",
      "epoch:3 step:3024 [D loss: 0.741052, acc.: 55.47%] [G loss: 0.948981]\n",
      "epoch:3 step:3025 [D loss: 0.738749, acc.: 53.91%] [G loss: 1.163389]\n",
      "epoch:3 step:3026 [D loss: 0.665640, acc.: 59.38%] [G loss: 0.989996]\n",
      "epoch:3 step:3027 [D loss: 0.637058, acc.: 64.06%] [G loss: 1.185142]\n",
      "epoch:3 step:3028 [D loss: 0.672284, acc.: 60.94%] [G loss: 1.011518]\n",
      "epoch:3 step:3029 [D loss: 0.672043, acc.: 60.94%] [G loss: 1.157458]\n",
      "epoch:3 step:3030 [D loss: 0.673113, acc.: 63.28%] [G loss: 0.968187]\n",
      "epoch:3 step:3031 [D loss: 0.607967, acc.: 66.41%] [G loss: 1.161712]\n",
      "epoch:3 step:3032 [D loss: 0.658565, acc.: 63.28%] [G loss: 1.099975]\n",
      "epoch:3 step:3033 [D loss: 0.653863, acc.: 64.84%] [G loss: 1.049026]\n",
      "epoch:3 step:3034 [D loss: 0.648265, acc.: 61.72%] [G loss: 1.059966]\n",
      "epoch:3 step:3035 [D loss: 0.699060, acc.: 56.25%] [G loss: 1.141781]\n",
      "epoch:3 step:3036 [D loss: 0.543305, acc.: 71.88%] [G loss: 1.098223]\n",
      "epoch:3 step:3037 [D loss: 0.581774, acc.: 67.97%] [G loss: 1.291320]\n",
      "epoch:3 step:3038 [D loss: 0.689851, acc.: 55.47%] [G loss: 1.126551]\n",
      "epoch:3 step:3039 [D loss: 0.632795, acc.: 64.84%] [G loss: 1.088518]\n",
      "epoch:3 step:3040 [D loss: 0.625438, acc.: 64.06%] [G loss: 1.004445]\n",
      "epoch:3 step:3041 [D loss: 0.593899, acc.: 68.75%] [G loss: 1.176066]\n",
      "epoch:3 step:3042 [D loss: 0.717004, acc.: 53.12%] [G loss: 1.016384]\n",
      "epoch:3 step:3043 [D loss: 0.678351, acc.: 62.50%] [G loss: 1.041881]\n",
      "epoch:3 step:3044 [D loss: 0.724603, acc.: 55.47%] [G loss: 1.227762]\n",
      "epoch:3 step:3045 [D loss: 0.680970, acc.: 57.03%] [G loss: 0.963448]\n",
      "epoch:3 step:3046 [D loss: 0.671552, acc.: 56.25%] [G loss: 1.207752]\n",
      "epoch:3 step:3047 [D loss: 0.587348, acc.: 75.00%] [G loss: 1.021403]\n",
      "epoch:3 step:3048 [D loss: 0.621505, acc.: 64.84%] [G loss: 1.247227]\n",
      "epoch:3 step:3049 [D loss: 0.677394, acc.: 57.81%] [G loss: 1.070073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3050 [D loss: 0.639718, acc.: 65.62%] [G loss: 0.991631]\n",
      "epoch:3 step:3051 [D loss: 0.704572, acc.: 56.25%] [G loss: 0.962481]\n",
      "epoch:3 step:3052 [D loss: 0.638624, acc.: 60.16%] [G loss: 1.336059]\n",
      "epoch:3 step:3053 [D loss: 0.600932, acc.: 69.53%] [G loss: 1.071324]\n",
      "epoch:3 step:3054 [D loss: 0.662504, acc.: 61.72%] [G loss: 0.986170]\n",
      "epoch:3 step:3055 [D loss: 0.712841, acc.: 56.25%] [G loss: 0.999464]\n",
      "epoch:3 step:3056 [D loss: 0.659879, acc.: 58.59%] [G loss: 1.082296]\n",
      "epoch:3 step:3057 [D loss: 0.590516, acc.: 69.53%] [G loss: 1.084248]\n",
      "epoch:3 step:3058 [D loss: 0.625840, acc.: 64.06%] [G loss: 1.085859]\n",
      "epoch:3 step:3059 [D loss: 0.570343, acc.: 74.22%] [G loss: 1.108239]\n",
      "epoch:3 step:3060 [D loss: 0.619883, acc.: 66.41%] [G loss: 1.061298]\n",
      "epoch:3 step:3061 [D loss: 0.589043, acc.: 70.31%] [G loss: 1.118203]\n",
      "epoch:3 step:3062 [D loss: 0.589255, acc.: 68.75%] [G loss: 1.127201]\n",
      "epoch:3 step:3063 [D loss: 0.697760, acc.: 58.59%] [G loss: 1.010053]\n",
      "epoch:3 step:3064 [D loss: 0.599349, acc.: 64.84%] [G loss: 1.185358]\n",
      "epoch:3 step:3065 [D loss: 0.645207, acc.: 67.97%] [G loss: 1.073071]\n",
      "epoch:3 step:3066 [D loss: 0.692266, acc.: 60.16%] [G loss: 0.957614]\n",
      "epoch:3 step:3067 [D loss: 0.612062, acc.: 67.97%] [G loss: 0.962400]\n",
      "epoch:3 step:3068 [D loss: 0.586953, acc.: 66.41%] [G loss: 1.117938]\n",
      "epoch:3 step:3069 [D loss: 0.562140, acc.: 71.88%] [G loss: 1.109109]\n",
      "epoch:3 step:3070 [D loss: 0.674716, acc.: 58.59%] [G loss: 0.967992]\n",
      "epoch:3 step:3071 [D loss: 0.652240, acc.: 63.28%] [G loss: 1.218585]\n",
      "epoch:3 step:3072 [D loss: 0.644150, acc.: 64.06%] [G loss: 1.091951]\n",
      "epoch:3 step:3073 [D loss: 0.725068, acc.: 53.12%] [G loss: 1.042325]\n",
      "epoch:3 step:3074 [D loss: 0.723026, acc.: 55.47%] [G loss: 0.999061]\n",
      "epoch:3 step:3075 [D loss: 0.654799, acc.: 60.94%] [G loss: 0.876408]\n",
      "epoch:3 step:3076 [D loss: 0.574196, acc.: 64.84%] [G loss: 1.264902]\n",
      "epoch:3 step:3077 [D loss: 0.729603, acc.: 50.78%] [G loss: 1.041576]\n",
      "epoch:3 step:3078 [D loss: 0.549536, acc.: 69.53%] [G loss: 1.165117]\n",
      "epoch:3 step:3079 [D loss: 0.608770, acc.: 65.62%] [G loss: 1.161445]\n",
      "epoch:3 step:3080 [D loss: 0.680181, acc.: 60.94%] [G loss: 1.157311]\n",
      "epoch:3 step:3081 [D loss: 0.556658, acc.: 68.75%] [G loss: 1.280420]\n",
      "epoch:3 step:3082 [D loss: 0.532262, acc.: 75.00%] [G loss: 1.193368]\n",
      "epoch:3 step:3083 [D loss: 0.675635, acc.: 55.47%] [G loss: 1.266176]\n",
      "epoch:3 step:3084 [D loss: 0.641059, acc.: 62.50%] [G loss: 1.115741]\n",
      "epoch:3 step:3085 [D loss: 0.731389, acc.: 51.56%] [G loss: 1.019987]\n",
      "epoch:3 step:3086 [D loss: 0.627792, acc.: 69.53%] [G loss: 0.994438]\n",
      "epoch:3 step:3087 [D loss: 0.557521, acc.: 72.66%] [G loss: 1.024557]\n",
      "epoch:3 step:3088 [D loss: 0.664097, acc.: 58.59%] [G loss: 0.932929]\n",
      "epoch:3 step:3089 [D loss: 0.524409, acc.: 75.78%] [G loss: 1.113278]\n",
      "epoch:3 step:3090 [D loss: 0.608877, acc.: 64.84%] [G loss: 1.172322]\n",
      "epoch:3 step:3091 [D loss: 0.660564, acc.: 61.72%] [G loss: 1.179358]\n",
      "epoch:3 step:3092 [D loss: 0.533243, acc.: 75.78%] [G loss: 1.044769]\n",
      "epoch:3 step:3093 [D loss: 0.564641, acc.: 72.66%] [G loss: 1.159322]\n",
      "epoch:3 step:3094 [D loss: 0.572470, acc.: 71.09%] [G loss: 1.161168]\n",
      "epoch:3 step:3095 [D loss: 0.480528, acc.: 83.59%] [G loss: 1.290940]\n",
      "epoch:3 step:3096 [D loss: 0.669500, acc.: 60.16%] [G loss: 1.112200]\n",
      "epoch:3 step:3097 [D loss: 0.627135, acc.: 64.06%] [G loss: 1.170713]\n",
      "epoch:3 step:3098 [D loss: 0.684165, acc.: 61.72%] [G loss: 0.940892]\n",
      "epoch:3 step:3099 [D loss: 0.683566, acc.: 59.38%] [G loss: 0.912893]\n",
      "epoch:3 step:3100 [D loss: 0.701275, acc.: 53.12%] [G loss: 1.063171]\n",
      "epoch:3 step:3101 [D loss: 0.640988, acc.: 65.62%] [G loss: 1.098440]\n",
      "epoch:3 step:3102 [D loss: 0.605764, acc.: 70.31%] [G loss: 1.149363]\n",
      "epoch:3 step:3103 [D loss: 0.628464, acc.: 62.50%] [G loss: 1.094038]\n",
      "epoch:3 step:3104 [D loss: 0.672221, acc.: 57.03%] [G loss: 1.067335]\n",
      "epoch:3 step:3105 [D loss: 0.670113, acc.: 57.81%] [G loss: 0.977209]\n",
      "epoch:3 step:3106 [D loss: 0.618187, acc.: 67.19%] [G loss: 1.167549]\n",
      "epoch:3 step:3107 [D loss: 0.655220, acc.: 61.72%] [G loss: 1.005877]\n",
      "epoch:3 step:3108 [D loss: 0.677220, acc.: 57.81%] [G loss: 1.113622]\n",
      "epoch:3 step:3109 [D loss: 0.590579, acc.: 64.84%] [G loss: 1.179241]\n",
      "epoch:3 step:3110 [D loss: 0.711378, acc.: 52.34%] [G loss: 1.024045]\n",
      "epoch:3 step:3111 [D loss: 0.639282, acc.: 63.28%] [G loss: 1.189591]\n",
      "epoch:3 step:3112 [D loss: 0.623240, acc.: 66.41%] [G loss: 1.217847]\n",
      "epoch:3 step:3113 [D loss: 0.658612, acc.: 60.16%] [G loss: 1.241382]\n",
      "epoch:3 step:3114 [D loss: 0.622709, acc.: 66.41%] [G loss: 1.057871]\n",
      "epoch:3 step:3115 [D loss: 0.566613, acc.: 67.19%] [G loss: 1.121676]\n",
      "epoch:3 step:3116 [D loss: 0.740994, acc.: 54.69%] [G loss: 1.038538]\n",
      "epoch:3 step:3117 [D loss: 0.633277, acc.: 62.50%] [G loss: 1.234297]\n",
      "epoch:3 step:3118 [D loss: 0.646673, acc.: 62.50%] [G loss: 1.040433]\n",
      "epoch:3 step:3119 [D loss: 0.693878, acc.: 53.91%] [G loss: 1.009095]\n",
      "epoch:3 step:3120 [D loss: 0.641315, acc.: 63.28%] [G loss: 1.028653]\n",
      "epoch:3 step:3121 [D loss: 0.727747, acc.: 56.25%] [G loss: 1.075315]\n",
      "epoch:3 step:3122 [D loss: 0.623595, acc.: 62.50%] [G loss: 1.220945]\n",
      "epoch:3 step:3123 [D loss: 0.692840, acc.: 63.28%] [G loss: 1.145249]\n",
      "epoch:3 step:3124 [D loss: 0.549784, acc.: 71.09%] [G loss: 1.048503]\n",
      "epoch:3 step:3125 [D loss: 0.596778, acc.: 67.19%] [G loss: 1.139995]\n",
      "epoch:3 step:3126 [D loss: 0.588261, acc.: 70.31%] [G loss: 1.172390]\n",
      "epoch:3 step:3127 [D loss: 0.722175, acc.: 56.25%] [G loss: 0.980463]\n",
      "epoch:3 step:3128 [D loss: 0.643270, acc.: 63.28%] [G loss: 0.998720]\n",
      "epoch:3 step:3129 [D loss: 0.568790, acc.: 71.09%] [G loss: 1.056257]\n",
      "epoch:3 step:3130 [D loss: 0.680719, acc.: 57.81%] [G loss: 1.105220]\n",
      "epoch:3 step:3131 [D loss: 0.597998, acc.: 65.62%] [G loss: 1.193468]\n",
      "epoch:3 step:3132 [D loss: 0.661207, acc.: 62.50%] [G loss: 1.014144]\n",
      "epoch:3 step:3133 [D loss: 0.740801, acc.: 48.44%] [G loss: 1.005976]\n",
      "epoch:3 step:3134 [D loss: 0.684507, acc.: 59.38%] [G loss: 1.108287]\n",
      "epoch:3 step:3135 [D loss: 0.599527, acc.: 67.19%] [G loss: 1.078215]\n",
      "epoch:3 step:3136 [D loss: 0.540328, acc.: 71.09%] [G loss: 1.004629]\n",
      "epoch:3 step:3137 [D loss: 0.659251, acc.: 60.16%] [G loss: 1.012398]\n",
      "epoch:3 step:3138 [D loss: 0.565012, acc.: 71.88%] [G loss: 1.101545]\n",
      "epoch:3 step:3139 [D loss: 0.554451, acc.: 73.44%] [G loss: 1.143960]\n",
      "epoch:3 step:3140 [D loss: 0.810647, acc.: 42.97%] [G loss: 0.946005]\n",
      "epoch:3 step:3141 [D loss: 0.661741, acc.: 64.84%] [G loss: 1.155689]\n",
      "epoch:3 step:3142 [D loss: 0.613214, acc.: 67.97%] [G loss: 0.971334]\n",
      "epoch:3 step:3143 [D loss: 0.556131, acc.: 73.44%] [G loss: 1.153494]\n",
      "epoch:3 step:3144 [D loss: 0.663505, acc.: 61.72%] [G loss: 1.078213]\n",
      "epoch:3 step:3145 [D loss: 0.685745, acc.: 60.94%] [G loss: 1.137875]\n",
      "epoch:3 step:3146 [D loss: 0.620728, acc.: 66.41%] [G loss: 1.174442]\n",
      "epoch:3 step:3147 [D loss: 0.616266, acc.: 67.19%] [G loss: 1.109748]\n",
      "epoch:3 step:3148 [D loss: 0.645139, acc.: 64.06%] [G loss: 1.158130]\n",
      "epoch:3 step:3149 [D loss: 0.638018, acc.: 62.50%] [G loss: 0.958276]\n",
      "epoch:3 step:3150 [D loss: 0.654504, acc.: 60.16%] [G loss: 1.038442]\n",
      "epoch:3 step:3151 [D loss: 0.597848, acc.: 67.19%] [G loss: 1.237053]\n",
      "epoch:3 step:3152 [D loss: 0.649823, acc.: 62.50%] [G loss: 1.120269]\n",
      "epoch:3 step:3153 [D loss: 0.597903, acc.: 67.97%] [G loss: 1.149813]\n",
      "epoch:3 step:3154 [D loss: 0.617747, acc.: 67.97%] [G loss: 1.116490]\n",
      "epoch:3 step:3155 [D loss: 0.597593, acc.: 67.97%] [G loss: 1.136636]\n",
      "epoch:3 step:3156 [D loss: 0.620541, acc.: 64.84%] [G loss: 1.104281]\n",
      "epoch:3 step:3157 [D loss: 0.688863, acc.: 56.25%] [G loss: 0.995128]\n",
      "epoch:3 step:3158 [D loss: 0.605716, acc.: 68.75%] [G loss: 0.989535]\n",
      "epoch:3 step:3159 [D loss: 0.584238, acc.: 73.44%] [G loss: 1.051665]\n",
      "epoch:3 step:3160 [D loss: 0.636402, acc.: 63.28%] [G loss: 1.125971]\n",
      "epoch:3 step:3161 [D loss: 0.690052, acc.: 58.59%] [G loss: 1.083445]\n",
      "epoch:3 step:3162 [D loss: 0.622974, acc.: 65.62%] [G loss: 0.943660]\n",
      "epoch:3 step:3163 [D loss: 0.678816, acc.: 57.81%] [G loss: 0.905466]\n",
      "epoch:3 step:3164 [D loss: 0.577726, acc.: 71.88%] [G loss: 1.194829]\n",
      "epoch:3 step:3165 [D loss: 0.588057, acc.: 67.19%] [G loss: 1.112847]\n",
      "epoch:3 step:3166 [D loss: 0.725908, acc.: 57.03%] [G loss: 1.016961]\n",
      "epoch:3 step:3167 [D loss: 0.598783, acc.: 65.62%] [G loss: 1.146831]\n",
      "epoch:3 step:3168 [D loss: 0.703293, acc.: 50.78%] [G loss: 1.197656]\n",
      "epoch:3 step:3169 [D loss: 0.642629, acc.: 61.72%] [G loss: 1.123164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3170 [D loss: 0.600299, acc.: 68.75%] [G loss: 1.201927]\n",
      "epoch:3 step:3171 [D loss: 0.682803, acc.: 60.94%] [G loss: 1.046160]\n",
      "epoch:3 step:3172 [D loss: 0.688716, acc.: 62.50%] [G loss: 1.114331]\n",
      "epoch:3 step:3173 [D loss: 0.588237, acc.: 67.97%] [G loss: 1.201801]\n",
      "epoch:3 step:3174 [D loss: 0.702914, acc.: 57.03%] [G loss: 1.106726]\n",
      "epoch:3 step:3175 [D loss: 0.591962, acc.: 69.53%] [G loss: 1.153380]\n",
      "epoch:3 step:3176 [D loss: 0.631416, acc.: 61.72%] [G loss: 1.154357]\n",
      "epoch:3 step:3177 [D loss: 0.560369, acc.: 71.88%] [G loss: 1.321749]\n",
      "epoch:3 step:3178 [D loss: 0.663364, acc.: 60.16%] [G loss: 1.220502]\n",
      "epoch:3 step:3179 [D loss: 0.605649, acc.: 67.97%] [G loss: 0.984189]\n",
      "epoch:3 step:3180 [D loss: 0.680047, acc.: 57.81%] [G loss: 1.150460]\n",
      "epoch:3 step:3181 [D loss: 0.623687, acc.: 64.84%] [G loss: 1.079778]\n",
      "epoch:3 step:3182 [D loss: 0.618634, acc.: 70.31%] [G loss: 1.032437]\n",
      "epoch:3 step:3183 [D loss: 0.588413, acc.: 71.88%] [G loss: 1.099531]\n",
      "epoch:3 step:3184 [D loss: 0.662377, acc.: 60.94%] [G loss: 1.018348]\n",
      "epoch:3 step:3185 [D loss: 0.694278, acc.: 60.16%] [G loss: 1.114641]\n",
      "epoch:3 step:3186 [D loss: 0.686876, acc.: 60.94%] [G loss: 1.058418]\n",
      "epoch:3 step:3187 [D loss: 0.643790, acc.: 61.72%] [G loss: 0.905294]\n",
      "epoch:3 step:3188 [D loss: 0.690846, acc.: 57.81%] [G loss: 0.929732]\n",
      "epoch:3 step:3189 [D loss: 0.596864, acc.: 67.19%] [G loss: 1.031006]\n",
      "epoch:3 step:3190 [D loss: 0.700414, acc.: 56.25%] [G loss: 0.961067]\n",
      "epoch:3 step:3191 [D loss: 0.649237, acc.: 60.94%] [G loss: 1.063363]\n",
      "epoch:3 step:3192 [D loss: 0.602233, acc.: 67.19%] [G loss: 1.113548]\n",
      "epoch:3 step:3193 [D loss: 0.682656, acc.: 60.94%] [G loss: 1.077181]\n",
      "epoch:3 step:3194 [D loss: 0.597557, acc.: 64.84%] [G loss: 1.127372]\n",
      "epoch:3 step:3195 [D loss: 0.555717, acc.: 74.22%] [G loss: 1.129740]\n",
      "epoch:3 step:3196 [D loss: 0.657525, acc.: 64.06%] [G loss: 0.959253]\n",
      "epoch:3 step:3197 [D loss: 0.685974, acc.: 59.38%] [G loss: 1.047441]\n",
      "epoch:3 step:3198 [D loss: 0.564653, acc.: 68.75%] [G loss: 1.179036]\n",
      "epoch:3 step:3199 [D loss: 0.683573, acc.: 59.38%] [G loss: 1.075320]\n",
      "epoch:3 step:3200 [D loss: 0.643818, acc.: 63.28%] [G loss: 1.027323]\n",
      "##############\n",
      "[2.58958515 2.08871231 2.01273571 3.0350688  1.01554083 6.97455978\n",
      " 2.04233113 2.83859416 3.77642466 5.86819545]\n",
      "##########\n",
      "epoch:3 step:3201 [D loss: 0.603811, acc.: 66.41%] [G loss: 1.079809]\n",
      "epoch:3 step:3202 [D loss: 0.643036, acc.: 64.84%] [G loss: 1.189138]\n",
      "epoch:3 step:3203 [D loss: 0.590732, acc.: 69.53%] [G loss: 0.909746]\n",
      "epoch:3 step:3204 [D loss: 0.620233, acc.: 67.97%] [G loss: 1.177773]\n",
      "epoch:3 step:3205 [D loss: 0.630760, acc.: 61.72%] [G loss: 1.006149]\n",
      "epoch:3 step:3206 [D loss: 0.521582, acc.: 78.12%] [G loss: 1.075653]\n",
      "epoch:3 step:3207 [D loss: 0.644253, acc.: 64.06%] [G loss: 1.080672]\n",
      "epoch:3 step:3208 [D loss: 0.613804, acc.: 65.62%] [G loss: 1.087601]\n",
      "epoch:3 step:3209 [D loss: 0.569605, acc.: 67.97%] [G loss: 1.130199]\n",
      "epoch:3 step:3210 [D loss: 0.594139, acc.: 69.53%] [G loss: 1.142278]\n",
      "epoch:3 step:3211 [D loss: 0.581999, acc.: 70.31%] [G loss: 0.963511]\n",
      "epoch:3 step:3212 [D loss: 0.609075, acc.: 68.75%] [G loss: 1.112519]\n",
      "epoch:3 step:3213 [D loss: 0.586927, acc.: 71.88%] [G loss: 1.193583]\n",
      "epoch:3 step:3214 [D loss: 0.630266, acc.: 60.94%] [G loss: 1.238961]\n",
      "epoch:3 step:3215 [D loss: 0.631489, acc.: 67.19%] [G loss: 1.130775]\n",
      "epoch:3 step:3216 [D loss: 0.578014, acc.: 68.75%] [G loss: 1.269778]\n",
      "epoch:3 step:3217 [D loss: 0.626560, acc.: 61.72%] [G loss: 1.052527]\n",
      "epoch:3 step:3218 [D loss: 0.652083, acc.: 62.50%] [G loss: 1.022563]\n",
      "epoch:3 step:3219 [D loss: 0.613673, acc.: 62.50%] [G loss: 1.144545]\n",
      "epoch:3 step:3220 [D loss: 0.624248, acc.: 66.41%] [G loss: 0.995199]\n",
      "epoch:3 step:3221 [D loss: 0.664032, acc.: 59.38%] [G loss: 1.214404]\n",
      "epoch:3 step:3222 [D loss: 0.704102, acc.: 55.47%] [G loss: 1.068492]\n",
      "epoch:3 step:3223 [D loss: 0.577151, acc.: 71.09%] [G loss: 1.122197]\n",
      "epoch:3 step:3224 [D loss: 0.544232, acc.: 74.22%] [G loss: 0.965926]\n",
      "epoch:3 step:3225 [D loss: 0.673266, acc.: 57.03%] [G loss: 1.046212]\n",
      "epoch:3 step:3226 [D loss: 0.613707, acc.: 67.19%] [G loss: 1.091685]\n",
      "epoch:3 step:3227 [D loss: 0.615755, acc.: 65.62%] [G loss: 1.204869]\n",
      "epoch:3 step:3228 [D loss: 0.651377, acc.: 60.94%] [G loss: 0.965281]\n",
      "epoch:3 step:3229 [D loss: 0.557924, acc.: 72.66%] [G loss: 1.082381]\n",
      "epoch:3 step:3230 [D loss: 0.547488, acc.: 71.88%] [G loss: 1.127986]\n",
      "epoch:3 step:3231 [D loss: 0.590164, acc.: 68.75%] [G loss: 1.232253]\n",
      "epoch:3 step:3232 [D loss: 0.621977, acc.: 67.97%] [G loss: 1.138431]\n",
      "epoch:3 step:3233 [D loss: 0.727891, acc.: 55.47%] [G loss: 1.105101]\n",
      "epoch:3 step:3234 [D loss: 0.564276, acc.: 67.19%] [G loss: 1.199731]\n",
      "epoch:3 step:3235 [D loss: 0.562535, acc.: 71.88%] [G loss: 1.244290]\n",
      "epoch:3 step:3236 [D loss: 0.602386, acc.: 62.50%] [G loss: 1.182843]\n",
      "epoch:3 step:3237 [D loss: 0.662937, acc.: 55.47%] [G loss: 0.958240]\n",
      "epoch:3 step:3238 [D loss: 0.606836, acc.: 64.06%] [G loss: 1.067638]\n",
      "epoch:3 step:3239 [D loss: 0.803608, acc.: 48.44%] [G loss: 1.103242]\n",
      "epoch:3 step:3240 [D loss: 0.684750, acc.: 55.47%] [G loss: 0.974385]\n",
      "epoch:3 step:3241 [D loss: 0.714641, acc.: 58.59%] [G loss: 1.221632]\n",
      "epoch:3 step:3242 [D loss: 0.634162, acc.: 65.62%] [G loss: 1.085978]\n",
      "epoch:3 step:3243 [D loss: 0.593867, acc.: 66.41%] [G loss: 0.941346]\n",
      "epoch:3 step:3244 [D loss: 0.620708, acc.: 63.28%] [G loss: 1.169434]\n",
      "epoch:3 step:3245 [D loss: 0.565918, acc.: 72.66%] [G loss: 1.337154]\n",
      "epoch:3 step:3246 [D loss: 0.625983, acc.: 64.06%] [G loss: 1.230772]\n",
      "epoch:3 step:3247 [D loss: 0.585450, acc.: 69.53%] [G loss: 1.105237]\n",
      "epoch:3 step:3248 [D loss: 0.681098, acc.: 61.72%] [G loss: 0.985352]\n",
      "epoch:3 step:3249 [D loss: 0.658940, acc.: 60.16%] [G loss: 1.097615]\n",
      "epoch:3 step:3250 [D loss: 0.541279, acc.: 77.34%] [G loss: 1.021121]\n",
      "epoch:3 step:3251 [D loss: 0.663058, acc.: 58.59%] [G loss: 1.105180]\n",
      "epoch:3 step:3252 [D loss: 0.617373, acc.: 67.97%] [G loss: 1.225650]\n",
      "epoch:3 step:3253 [D loss: 0.655445, acc.: 67.19%] [G loss: 1.086356]\n",
      "epoch:3 step:3254 [D loss: 0.581183, acc.: 69.53%] [G loss: 1.089499]\n",
      "epoch:3 step:3255 [D loss: 0.623441, acc.: 61.72%] [G loss: 1.084853]\n",
      "epoch:3 step:3256 [D loss: 0.590367, acc.: 64.84%] [G loss: 1.099314]\n",
      "epoch:3 step:3257 [D loss: 0.745897, acc.: 53.91%] [G loss: 1.054116]\n",
      "epoch:3 step:3258 [D loss: 0.588157, acc.: 67.97%] [G loss: 1.103486]\n",
      "epoch:3 step:3259 [D loss: 0.712845, acc.: 59.38%] [G loss: 1.198048]\n",
      "epoch:3 step:3260 [D loss: 0.590693, acc.: 67.97%] [G loss: 1.013075]\n",
      "epoch:3 step:3261 [D loss: 0.620575, acc.: 67.97%] [G loss: 1.208056]\n",
      "epoch:3 step:3262 [D loss: 0.626132, acc.: 60.94%] [G loss: 1.081733]\n",
      "epoch:3 step:3263 [D loss: 0.710332, acc.: 53.12%] [G loss: 1.085910]\n",
      "epoch:3 step:3264 [D loss: 0.651985, acc.: 63.28%] [G loss: 1.066611]\n",
      "epoch:3 step:3265 [D loss: 0.636849, acc.: 65.62%] [G loss: 1.154540]\n",
      "epoch:3 step:3266 [D loss: 0.532540, acc.: 73.44%] [G loss: 1.292904]\n",
      "epoch:3 step:3267 [D loss: 0.683045, acc.: 58.59%] [G loss: 1.048970]\n",
      "epoch:3 step:3268 [D loss: 0.637759, acc.: 63.28%] [G loss: 1.184089]\n",
      "epoch:3 step:3269 [D loss: 0.687364, acc.: 54.69%] [G loss: 1.208173]\n",
      "epoch:3 step:3270 [D loss: 0.675533, acc.: 60.94%] [G loss: 1.160600]\n",
      "epoch:3 step:3271 [D loss: 0.739523, acc.: 54.69%] [G loss: 1.158566]\n",
      "epoch:3 step:3272 [D loss: 0.697254, acc.: 60.16%] [G loss: 1.220634]\n",
      "epoch:3 step:3273 [D loss: 0.625176, acc.: 68.75%] [G loss: 1.103496]\n",
      "epoch:3 step:3274 [D loss: 0.679711, acc.: 52.34%] [G loss: 1.064988]\n",
      "epoch:3 step:3275 [D loss: 0.608880, acc.: 71.09%] [G loss: 0.978287]\n",
      "epoch:3 step:3276 [D loss: 0.632534, acc.: 61.72%] [G loss: 1.015719]\n",
      "epoch:3 step:3277 [D loss: 0.585246, acc.: 68.75%] [G loss: 1.079342]\n",
      "epoch:3 step:3278 [D loss: 0.688252, acc.: 56.25%] [G loss: 1.014432]\n",
      "epoch:3 step:3279 [D loss: 0.623930, acc.: 64.06%] [G loss: 1.222701]\n",
      "epoch:3 step:3280 [D loss: 0.636595, acc.: 67.19%] [G loss: 1.199503]\n",
      "epoch:3 step:3281 [D loss: 0.690255, acc.: 55.47%] [G loss: 1.269115]\n",
      "epoch:3 step:3282 [D loss: 0.627260, acc.: 59.38%] [G loss: 0.990716]\n",
      "epoch:3 step:3283 [D loss: 0.618638, acc.: 62.50%] [G loss: 1.113295]\n",
      "epoch:3 step:3284 [D loss: 0.614985, acc.: 67.19%] [G loss: 1.075647]\n",
      "epoch:3 step:3285 [D loss: 0.673200, acc.: 59.38%] [G loss: 1.092320]\n",
      "epoch:3 step:3286 [D loss: 0.518051, acc.: 74.22%] [G loss: 1.279065]\n",
      "epoch:3 step:3287 [D loss: 0.723212, acc.: 54.69%] [G loss: 0.995296]\n",
      "epoch:3 step:3288 [D loss: 0.678620, acc.: 55.47%] [G loss: 1.024516]\n",
      "epoch:3 step:3289 [D loss: 0.566932, acc.: 72.66%] [G loss: 1.095999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3290 [D loss: 0.674864, acc.: 64.06%] [G loss: 1.139441]\n",
      "epoch:3 step:3291 [D loss: 0.666752, acc.: 60.16%] [G loss: 1.215470]\n",
      "epoch:3 step:3292 [D loss: 0.622107, acc.: 64.06%] [G loss: 1.075422]\n",
      "epoch:3 step:3293 [D loss: 0.637842, acc.: 64.06%] [G loss: 1.210254]\n",
      "epoch:3 step:3294 [D loss: 0.615673, acc.: 68.75%] [G loss: 1.081076]\n",
      "epoch:3 step:3295 [D loss: 0.624958, acc.: 67.19%] [G loss: 0.977441]\n",
      "epoch:3 step:3296 [D loss: 0.728356, acc.: 55.47%] [G loss: 1.128602]\n",
      "epoch:3 step:3297 [D loss: 0.572338, acc.: 75.78%] [G loss: 1.167865]\n",
      "epoch:3 step:3298 [D loss: 0.569280, acc.: 69.53%] [G loss: 1.099038]\n",
      "epoch:3 step:3299 [D loss: 0.617686, acc.: 65.62%] [G loss: 1.048835]\n",
      "epoch:3 step:3300 [D loss: 0.690909, acc.: 59.38%] [G loss: 1.020155]\n",
      "epoch:3 step:3301 [D loss: 0.629485, acc.: 64.84%] [G loss: 1.003702]\n",
      "epoch:3 step:3302 [D loss: 0.602763, acc.: 68.75%] [G loss: 1.109961]\n",
      "epoch:3 step:3303 [D loss: 0.665144, acc.: 57.81%] [G loss: 1.056672]\n",
      "epoch:3 step:3304 [D loss: 0.645820, acc.: 60.94%] [G loss: 1.017054]\n",
      "epoch:3 step:3305 [D loss: 0.598097, acc.: 65.62%] [G loss: 1.178013]\n",
      "epoch:3 step:3306 [D loss: 0.674280, acc.: 57.81%] [G loss: 0.939520]\n",
      "epoch:3 step:3307 [D loss: 0.630466, acc.: 65.62%] [G loss: 1.143792]\n",
      "epoch:3 step:3308 [D loss: 0.628080, acc.: 67.97%] [G loss: 1.099055]\n",
      "epoch:3 step:3309 [D loss: 0.526965, acc.: 73.44%] [G loss: 1.252996]\n",
      "epoch:3 step:3310 [D loss: 0.662711, acc.: 61.72%] [G loss: 1.055238]\n",
      "epoch:3 step:3311 [D loss: 0.698222, acc.: 58.59%] [G loss: 1.236369]\n",
      "epoch:3 step:3312 [D loss: 0.629570, acc.: 62.50%] [G loss: 1.110931]\n",
      "epoch:3 step:3313 [D loss: 0.624761, acc.: 63.28%] [G loss: 1.097363]\n",
      "epoch:3 step:3314 [D loss: 0.571195, acc.: 67.97%] [G loss: 1.074692]\n",
      "epoch:3 step:3315 [D loss: 0.634206, acc.: 65.62%] [G loss: 1.154997]\n",
      "epoch:3 step:3316 [D loss: 0.698275, acc.: 55.47%] [G loss: 0.935594]\n",
      "epoch:3 step:3317 [D loss: 0.644539, acc.: 60.16%] [G loss: 0.999746]\n",
      "epoch:3 step:3318 [D loss: 0.667255, acc.: 53.91%] [G loss: 1.157120]\n",
      "epoch:3 step:3319 [D loss: 0.663523, acc.: 59.38%] [G loss: 1.095606]\n",
      "epoch:3 step:3320 [D loss: 0.625272, acc.: 60.16%] [G loss: 1.193930]\n",
      "epoch:3 step:3321 [D loss: 0.714784, acc.: 56.25%] [G loss: 0.982706]\n",
      "epoch:3 step:3322 [D loss: 0.606898, acc.: 69.53%] [G loss: 1.141760]\n",
      "epoch:3 step:3323 [D loss: 0.604150, acc.: 64.84%] [G loss: 0.998589]\n",
      "epoch:3 step:3324 [D loss: 0.655574, acc.: 62.50%] [G loss: 1.074440]\n",
      "epoch:3 step:3325 [D loss: 0.699719, acc.: 57.81%] [G loss: 1.027747]\n",
      "epoch:3 step:3326 [D loss: 0.565968, acc.: 70.31%] [G loss: 1.176043]\n",
      "epoch:3 step:3327 [D loss: 0.615170, acc.: 64.06%] [G loss: 1.121263]\n",
      "epoch:3 step:3328 [D loss: 0.637790, acc.: 56.25%] [G loss: 1.074534]\n",
      "epoch:3 step:3329 [D loss: 0.580414, acc.: 64.06%] [G loss: 1.202801]\n",
      "epoch:3 step:3330 [D loss: 0.534154, acc.: 73.44%] [G loss: 1.193236]\n",
      "epoch:3 step:3331 [D loss: 0.706411, acc.: 56.25%] [G loss: 1.109653]\n",
      "epoch:3 step:3332 [D loss: 0.712827, acc.: 57.81%] [G loss: 1.007140]\n",
      "epoch:3 step:3333 [D loss: 0.606721, acc.: 67.97%] [G loss: 1.177072]\n",
      "epoch:3 step:3334 [D loss: 0.615718, acc.: 63.28%] [G loss: 1.013467]\n",
      "epoch:3 step:3335 [D loss: 0.604051, acc.: 65.62%] [G loss: 1.246986]\n",
      "epoch:3 step:3336 [D loss: 0.690441, acc.: 57.81%] [G loss: 1.155572]\n",
      "epoch:3 step:3337 [D loss: 0.567720, acc.: 71.09%] [G loss: 1.013011]\n",
      "epoch:3 step:3338 [D loss: 0.641222, acc.: 63.28%] [G loss: 1.059610]\n",
      "epoch:3 step:3339 [D loss: 0.705389, acc.: 53.12%] [G loss: 0.962132]\n",
      "epoch:3 step:3340 [D loss: 0.625762, acc.: 65.62%] [G loss: 1.193053]\n",
      "epoch:3 step:3341 [D loss: 0.595629, acc.: 70.31%] [G loss: 1.077618]\n",
      "epoch:3 step:3342 [D loss: 0.659543, acc.: 60.94%] [G loss: 1.157116]\n",
      "epoch:3 step:3343 [D loss: 0.682936, acc.: 53.12%] [G loss: 0.833951]\n",
      "epoch:3 step:3344 [D loss: 0.631213, acc.: 67.97%] [G loss: 1.191430]\n",
      "epoch:3 step:3345 [D loss: 0.671066, acc.: 64.84%] [G loss: 1.057028]\n",
      "epoch:3 step:3346 [D loss: 0.663862, acc.: 60.94%] [G loss: 1.033333]\n",
      "epoch:3 step:3347 [D loss: 0.730374, acc.: 51.56%] [G loss: 1.026445]\n",
      "epoch:3 step:3348 [D loss: 0.686861, acc.: 60.94%] [G loss: 1.121750]\n",
      "epoch:3 step:3349 [D loss: 0.664689, acc.: 57.81%] [G loss: 1.082695]\n",
      "epoch:3 step:3350 [D loss: 0.614162, acc.: 68.75%] [G loss: 1.104889]\n",
      "epoch:3 step:3351 [D loss: 0.649835, acc.: 58.59%] [G loss: 1.205836]\n",
      "epoch:3 step:3352 [D loss: 0.601864, acc.: 65.62%] [G loss: 1.098334]\n",
      "epoch:3 step:3353 [D loss: 0.708204, acc.: 51.56%] [G loss: 1.311329]\n",
      "epoch:3 step:3354 [D loss: 0.649627, acc.: 60.94%] [G loss: 1.449193]\n",
      "epoch:3 step:3355 [D loss: 0.640842, acc.: 66.41%] [G loss: 1.146029]\n",
      "epoch:3 step:3356 [D loss: 0.621690, acc.: 64.84%] [G loss: 1.238531]\n",
      "epoch:3 step:3357 [D loss: 0.611910, acc.: 66.41%] [G loss: 1.053848]\n",
      "epoch:3 step:3358 [D loss: 0.615453, acc.: 69.53%] [G loss: 1.259354]\n",
      "epoch:3 step:3359 [D loss: 0.657181, acc.: 63.28%] [G loss: 1.098326]\n",
      "epoch:3 step:3360 [D loss: 0.591270, acc.: 67.19%] [G loss: 1.067365]\n",
      "epoch:3 step:3361 [D loss: 0.586485, acc.: 67.97%] [G loss: 1.201721]\n",
      "epoch:3 step:3362 [D loss: 0.673296, acc.: 60.16%] [G loss: 1.163194]\n",
      "epoch:3 step:3363 [D loss: 0.645649, acc.: 61.72%] [G loss: 1.157337]\n",
      "epoch:3 step:3364 [D loss: 0.622074, acc.: 66.41%] [G loss: 1.208375]\n",
      "epoch:3 step:3365 [D loss: 0.689213, acc.: 60.16%] [G loss: 1.112055]\n",
      "epoch:3 step:3366 [D loss: 0.657973, acc.: 57.81%] [G loss: 1.227893]\n",
      "epoch:3 step:3367 [D loss: 0.618969, acc.: 67.97%] [G loss: 1.123636]\n",
      "epoch:3 step:3368 [D loss: 0.648926, acc.: 63.28%] [G loss: 1.313575]\n",
      "epoch:3 step:3369 [D loss: 0.674948, acc.: 63.28%] [G loss: 1.247840]\n",
      "epoch:3 step:3370 [D loss: 0.614756, acc.: 69.53%] [G loss: 1.206357]\n",
      "epoch:3 step:3371 [D loss: 0.613326, acc.: 62.50%] [G loss: 1.118587]\n",
      "epoch:3 step:3372 [D loss: 0.612786, acc.: 62.50%] [G loss: 1.159270]\n",
      "epoch:3 step:3373 [D loss: 0.628998, acc.: 61.72%] [G loss: 1.066557]\n",
      "epoch:3 step:3374 [D loss: 0.631061, acc.: 67.97%] [G loss: 0.954599]\n",
      "epoch:3 step:3375 [D loss: 0.549493, acc.: 71.09%] [G loss: 1.246417]\n",
      "epoch:3 step:3376 [D loss: 0.518451, acc.: 72.66%] [G loss: 1.111907]\n",
      "epoch:3 step:3377 [D loss: 0.677116, acc.: 63.28%] [G loss: 0.986572]\n",
      "epoch:3 step:3378 [D loss: 0.543886, acc.: 72.66%] [G loss: 1.089456]\n",
      "epoch:3 step:3379 [D loss: 0.714226, acc.: 56.25%] [G loss: 1.034494]\n",
      "epoch:3 step:3380 [D loss: 0.577175, acc.: 67.97%] [G loss: 1.184545]\n",
      "epoch:3 step:3381 [D loss: 0.531566, acc.: 76.56%] [G loss: 1.162613]\n",
      "epoch:3 step:3382 [D loss: 0.569987, acc.: 71.09%] [G loss: 1.163493]\n",
      "epoch:3 step:3383 [D loss: 0.626413, acc.: 68.75%] [G loss: 1.107150]\n",
      "epoch:3 step:3384 [D loss: 0.641853, acc.: 64.84%] [G loss: 1.293764]\n",
      "epoch:3 step:3385 [D loss: 0.688331, acc.: 56.25%] [G loss: 1.245126]\n",
      "epoch:3 step:3386 [D loss: 0.705677, acc.: 55.47%] [G loss: 1.121368]\n",
      "epoch:3 step:3387 [D loss: 0.532074, acc.: 69.53%] [G loss: 1.201323]\n",
      "epoch:3 step:3388 [D loss: 0.659124, acc.: 64.84%] [G loss: 1.071069]\n",
      "epoch:3 step:3389 [D loss: 0.657091, acc.: 57.03%] [G loss: 1.145156]\n",
      "epoch:3 step:3390 [D loss: 0.579692, acc.: 75.78%] [G loss: 1.126737]\n",
      "epoch:3 step:3391 [D loss: 0.657632, acc.: 61.72%] [G loss: 1.118863]\n",
      "epoch:3 step:3392 [D loss: 0.659355, acc.: 60.94%] [G loss: 1.169273]\n",
      "epoch:3 step:3393 [D loss: 0.762232, acc.: 51.56%] [G loss: 1.071666]\n",
      "epoch:3 step:3394 [D loss: 0.652146, acc.: 63.28%] [G loss: 1.027233]\n",
      "epoch:3 step:3395 [D loss: 0.667688, acc.: 62.50%] [G loss: 1.149215]\n",
      "epoch:3 step:3396 [D loss: 0.636003, acc.: 60.94%] [G loss: 1.055298]\n",
      "epoch:3 step:3397 [D loss: 0.645237, acc.: 60.94%] [G loss: 1.122154]\n",
      "epoch:3 step:3398 [D loss: 0.695154, acc.: 57.03%] [G loss: 1.168815]\n",
      "epoch:3 step:3399 [D loss: 0.701635, acc.: 56.25%] [G loss: 1.017590]\n",
      "epoch:3 step:3400 [D loss: 0.662428, acc.: 57.03%] [G loss: 1.121386]\n",
      "##############\n",
      "[2.50586978 2.0299557  1.70199927 2.67516276 0.67170395 5.63312264\n",
      " 2.08512226 3.08705567 3.67283999 5.06851824]\n",
      "##########\n",
      "epoch:3 step:3401 [D loss: 0.617715, acc.: 67.19%] [G loss: 1.231399]\n",
      "epoch:3 step:3402 [D loss: 0.661686, acc.: 60.16%] [G loss: 1.127676]\n",
      "epoch:3 step:3403 [D loss: 0.629903, acc.: 64.84%] [G loss: 1.101269]\n",
      "epoch:3 step:3404 [D loss: 0.576656, acc.: 73.44%] [G loss: 1.131614]\n",
      "epoch:3 step:3405 [D loss: 0.619241, acc.: 63.28%] [G loss: 1.148048]\n",
      "epoch:3 step:3406 [D loss: 0.650043, acc.: 63.28%] [G loss: 1.080523]\n",
      "epoch:3 step:3407 [D loss: 0.628801, acc.: 64.84%] [G loss: 1.201215]\n",
      "epoch:3 step:3408 [D loss: 0.658378, acc.: 62.50%] [G loss: 0.970256]\n",
      "epoch:3 step:3409 [D loss: 0.684710, acc.: 57.03%] [G loss: 1.062892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3410 [D loss: 0.579390, acc.: 69.53%] [G loss: 1.026797]\n",
      "epoch:3 step:3411 [D loss: 0.590481, acc.: 66.41%] [G loss: 1.218140]\n",
      "epoch:3 step:3412 [D loss: 0.603018, acc.: 66.41%] [G loss: 1.230169]\n",
      "epoch:3 step:3413 [D loss: 0.602659, acc.: 70.31%] [G loss: 1.045455]\n",
      "epoch:3 step:3414 [D loss: 0.636359, acc.: 60.94%] [G loss: 1.046248]\n",
      "epoch:3 step:3415 [D loss: 0.638831, acc.: 63.28%] [G loss: 1.023316]\n",
      "epoch:3 step:3416 [D loss: 0.719826, acc.: 54.69%] [G loss: 1.016729]\n",
      "epoch:3 step:3417 [D loss: 0.650604, acc.: 60.94%] [G loss: 1.103243]\n",
      "epoch:3 step:3418 [D loss: 0.684136, acc.: 63.28%] [G loss: 0.998408]\n",
      "epoch:3 step:3419 [D loss: 0.646354, acc.: 53.91%] [G loss: 1.081264]\n",
      "epoch:3 step:3420 [D loss: 0.573012, acc.: 71.09%] [G loss: 1.301553]\n",
      "epoch:3 step:3421 [D loss: 0.715128, acc.: 53.91%] [G loss: 1.037926]\n",
      "epoch:3 step:3422 [D loss: 0.589781, acc.: 72.66%] [G loss: 1.259464]\n",
      "epoch:3 step:3423 [D loss: 0.658899, acc.: 64.06%] [G loss: 1.123640]\n",
      "epoch:3 step:3424 [D loss: 0.586610, acc.: 66.41%] [G loss: 1.112481]\n",
      "epoch:3 step:3425 [D loss: 0.703673, acc.: 56.25%] [G loss: 1.099385]\n",
      "epoch:3 step:3426 [D loss: 0.605681, acc.: 64.84%] [G loss: 0.999228]\n",
      "epoch:3 step:3427 [D loss: 0.646964, acc.: 60.16%] [G loss: 1.090813]\n",
      "epoch:3 step:3428 [D loss: 0.646373, acc.: 57.03%] [G loss: 1.281978]\n",
      "epoch:3 step:3429 [D loss: 0.656949, acc.: 59.38%] [G loss: 1.236253]\n",
      "epoch:3 step:3430 [D loss: 0.618348, acc.: 66.41%] [G loss: 1.121234]\n",
      "epoch:3 step:3431 [D loss: 0.613485, acc.: 63.28%] [G loss: 1.279497]\n",
      "epoch:3 step:3432 [D loss: 0.645826, acc.: 60.94%] [G loss: 0.980612]\n",
      "epoch:3 step:3433 [D loss: 0.674210, acc.: 54.69%] [G loss: 1.121125]\n",
      "epoch:3 step:3434 [D loss: 0.609982, acc.: 65.62%] [G loss: 1.309107]\n",
      "epoch:3 step:3435 [D loss: 0.682433, acc.: 58.59%] [G loss: 1.186348]\n",
      "epoch:3 step:3436 [D loss: 0.601512, acc.: 71.88%] [G loss: 1.091238]\n",
      "epoch:3 step:3437 [D loss: 0.668394, acc.: 63.28%] [G loss: 1.110841]\n",
      "epoch:3 step:3438 [D loss: 0.661896, acc.: 58.59%] [G loss: 1.052275]\n",
      "epoch:3 step:3439 [D loss: 0.754622, acc.: 50.78%] [G loss: 1.175414]\n",
      "epoch:3 step:3440 [D loss: 0.694107, acc.: 54.69%] [G loss: 1.053642]\n",
      "epoch:3 step:3441 [D loss: 0.653598, acc.: 60.16%] [G loss: 1.296936]\n",
      "epoch:3 step:3442 [D loss: 0.619222, acc.: 61.72%] [G loss: 0.967483]\n",
      "epoch:3 step:3443 [D loss: 0.635709, acc.: 67.19%] [G loss: 1.064848]\n",
      "epoch:3 step:3444 [D loss: 0.612351, acc.: 67.97%] [G loss: 1.207926]\n",
      "epoch:3 step:3445 [D loss: 0.593314, acc.: 64.84%] [G loss: 1.266762]\n",
      "epoch:3 step:3446 [D loss: 0.659543, acc.: 58.59%] [G loss: 1.219164]\n",
      "epoch:3 step:3447 [D loss: 0.612220, acc.: 65.62%] [G loss: 0.978378]\n",
      "epoch:3 step:3448 [D loss: 0.695605, acc.: 54.69%] [G loss: 0.922069]\n",
      "epoch:3 step:3449 [D loss: 0.635779, acc.: 64.84%] [G loss: 1.025071]\n",
      "epoch:3 step:3450 [D loss: 0.704600, acc.: 54.69%] [G loss: 1.091859]\n",
      "epoch:3 step:3451 [D loss: 0.602449, acc.: 64.84%] [G loss: 1.213194]\n",
      "epoch:3 step:3452 [D loss: 0.678573, acc.: 64.06%] [G loss: 1.032291]\n",
      "epoch:3 step:3453 [D loss: 0.636307, acc.: 65.62%] [G loss: 1.205824]\n",
      "epoch:3 step:3454 [D loss: 0.684933, acc.: 57.03%] [G loss: 1.034111]\n",
      "epoch:3 step:3455 [D loss: 0.603327, acc.: 69.53%] [G loss: 1.039850]\n",
      "epoch:3 step:3456 [D loss: 0.566391, acc.: 74.22%] [G loss: 1.242606]\n",
      "epoch:3 step:3457 [D loss: 0.643486, acc.: 63.28%] [G loss: 1.227046]\n",
      "epoch:3 step:3458 [D loss: 0.592113, acc.: 64.84%] [G loss: 1.091842]\n",
      "epoch:3 step:3459 [D loss: 0.613587, acc.: 67.97%] [G loss: 1.131971]\n",
      "epoch:3 step:3460 [D loss: 0.691504, acc.: 54.69%] [G loss: 1.199143]\n",
      "epoch:3 step:3461 [D loss: 0.586267, acc.: 73.44%] [G loss: 1.084854]\n",
      "epoch:3 step:3462 [D loss: 0.572926, acc.: 67.97%] [G loss: 1.090549]\n",
      "epoch:3 step:3463 [D loss: 0.668052, acc.: 56.25%] [G loss: 1.100097]\n",
      "epoch:3 step:3464 [D loss: 0.545228, acc.: 73.44%] [G loss: 1.171740]\n",
      "epoch:3 step:3465 [D loss: 0.675214, acc.: 57.03%] [G loss: 1.095896]\n",
      "epoch:3 step:3466 [D loss: 0.712176, acc.: 57.03%] [G loss: 1.062177]\n",
      "epoch:3 step:3467 [D loss: 0.620429, acc.: 65.62%] [G loss: 1.183358]\n",
      "epoch:3 step:3468 [D loss: 0.650522, acc.: 60.94%] [G loss: 1.020347]\n",
      "epoch:3 step:3469 [D loss: 0.645014, acc.: 63.28%] [G loss: 1.008234]\n",
      "epoch:3 step:3470 [D loss: 0.645569, acc.: 64.84%] [G loss: 1.064142]\n",
      "epoch:3 step:3471 [D loss: 0.569709, acc.: 75.78%] [G loss: 1.066905]\n",
      "epoch:3 step:3472 [D loss: 0.581926, acc.: 71.88%] [G loss: 1.120758]\n",
      "epoch:3 step:3473 [D loss: 0.684183, acc.: 59.38%] [G loss: 1.080924]\n",
      "epoch:3 step:3474 [D loss: 0.633114, acc.: 64.06%] [G loss: 1.221977]\n",
      "epoch:3 step:3475 [D loss: 0.589073, acc.: 64.84%] [G loss: 1.063257]\n",
      "epoch:3 step:3476 [D loss: 0.611198, acc.: 67.19%] [G loss: 1.159565]\n",
      "epoch:3 step:3477 [D loss: 0.567819, acc.: 71.88%] [G loss: 1.078208]\n",
      "epoch:3 step:3478 [D loss: 0.662977, acc.: 63.28%] [G loss: 1.014201]\n",
      "epoch:3 step:3479 [D loss: 0.742747, acc.: 53.91%] [G loss: 1.070162]\n",
      "epoch:3 step:3480 [D loss: 0.605204, acc.: 66.41%] [G loss: 1.123132]\n",
      "epoch:3 step:3481 [D loss: 0.625554, acc.: 62.50%] [G loss: 0.879288]\n",
      "epoch:3 step:3482 [D loss: 0.668092, acc.: 60.94%] [G loss: 1.003099]\n",
      "epoch:3 step:3483 [D loss: 0.624810, acc.: 65.62%] [G loss: 1.240296]\n",
      "epoch:3 step:3484 [D loss: 0.693034, acc.: 59.38%] [G loss: 1.079517]\n",
      "epoch:3 step:3485 [D loss: 0.694917, acc.: 57.81%] [G loss: 1.119631]\n",
      "epoch:3 step:3486 [D loss: 0.671468, acc.: 55.47%] [G loss: 1.090223]\n",
      "epoch:3 step:3487 [D loss: 0.588583, acc.: 68.75%] [G loss: 1.117553]\n",
      "epoch:3 step:3488 [D loss: 0.629667, acc.: 62.50%] [G loss: 1.349221]\n",
      "epoch:3 step:3489 [D loss: 0.639542, acc.: 66.41%] [G loss: 1.126796]\n",
      "epoch:3 step:3490 [D loss: 0.671492, acc.: 60.16%] [G loss: 0.982665]\n",
      "epoch:3 step:3491 [D loss: 0.679677, acc.: 57.03%] [G loss: 1.065512]\n",
      "epoch:3 step:3492 [D loss: 0.786357, acc.: 50.00%] [G loss: 1.062295]\n",
      "epoch:3 step:3493 [D loss: 0.612496, acc.: 67.19%] [G loss: 1.137072]\n",
      "epoch:3 step:3494 [D loss: 0.624195, acc.: 61.72%] [G loss: 1.284364]\n",
      "epoch:3 step:3495 [D loss: 0.651121, acc.: 60.16%] [G loss: 1.117715]\n",
      "epoch:3 step:3496 [D loss: 0.677053, acc.: 59.38%] [G loss: 1.019235]\n",
      "epoch:3 step:3497 [D loss: 0.703743, acc.: 56.25%] [G loss: 0.928992]\n",
      "epoch:3 step:3498 [D loss: 0.602014, acc.: 72.66%] [G loss: 1.153260]\n",
      "epoch:3 step:3499 [D loss: 0.549473, acc.: 70.31%] [G loss: 1.075191]\n",
      "epoch:3 step:3500 [D loss: 0.613555, acc.: 65.62%] [G loss: 1.035811]\n",
      "epoch:3 step:3501 [D loss: 0.661843, acc.: 60.94%] [G loss: 0.948394]\n",
      "epoch:3 step:3502 [D loss: 0.600873, acc.: 72.66%] [G loss: 0.987406]\n",
      "epoch:3 step:3503 [D loss: 0.644753, acc.: 61.72%] [G loss: 0.981955]\n",
      "epoch:3 step:3504 [D loss: 0.625266, acc.: 60.94%] [G loss: 1.059493]\n",
      "epoch:3 step:3505 [D loss: 0.673846, acc.: 60.16%] [G loss: 1.057111]\n",
      "epoch:3 step:3506 [D loss: 0.644349, acc.: 60.94%] [G loss: 1.050120]\n",
      "epoch:3 step:3507 [D loss: 0.590595, acc.: 70.31%] [G loss: 1.242949]\n",
      "epoch:3 step:3508 [D loss: 0.584241, acc.: 73.44%] [G loss: 1.089920]\n",
      "epoch:3 step:3509 [D loss: 0.602462, acc.: 67.97%] [G loss: 1.079240]\n",
      "epoch:3 step:3510 [D loss: 0.701306, acc.: 57.03%] [G loss: 0.947840]\n",
      "epoch:3 step:3511 [D loss: 0.690634, acc.: 51.56%] [G loss: 0.976869]\n",
      "epoch:3 step:3512 [D loss: 0.585962, acc.: 67.97%] [G loss: 1.253412]\n",
      "epoch:3 step:3513 [D loss: 0.648997, acc.: 61.72%] [G loss: 1.111216]\n",
      "epoch:3 step:3514 [D loss: 0.581227, acc.: 62.50%] [G loss: 1.089192]\n",
      "epoch:3 step:3515 [D loss: 0.675740, acc.: 61.72%] [G loss: 1.009211]\n",
      "epoch:3 step:3516 [D loss: 0.688833, acc.: 53.91%] [G loss: 1.006006]\n",
      "epoch:3 step:3517 [D loss: 0.654757, acc.: 69.53%] [G loss: 0.963969]\n",
      "epoch:3 step:3518 [D loss: 0.653797, acc.: 65.62%] [G loss: 1.146830]\n",
      "epoch:3 step:3519 [D loss: 0.657615, acc.: 63.28%] [G loss: 1.201205]\n",
      "epoch:3 step:3520 [D loss: 0.761024, acc.: 54.69%] [G loss: 0.910911]\n",
      "epoch:3 step:3521 [D loss: 0.627202, acc.: 65.62%] [G loss: 1.032488]\n",
      "epoch:3 step:3522 [D loss: 0.625082, acc.: 65.62%] [G loss: 1.189463]\n",
      "epoch:3 step:3523 [D loss: 0.592491, acc.: 68.75%] [G loss: 1.154547]\n",
      "epoch:3 step:3524 [D loss: 0.535955, acc.: 69.53%] [G loss: 1.151655]\n",
      "epoch:3 step:3525 [D loss: 0.644995, acc.: 63.28%] [G loss: 1.161507]\n",
      "epoch:3 step:3526 [D loss: 0.655059, acc.: 59.38%] [G loss: 1.090673]\n",
      "epoch:3 step:3527 [D loss: 0.578502, acc.: 71.09%] [G loss: 1.139887]\n",
      "epoch:3 step:3528 [D loss: 0.672313, acc.: 59.38%] [G loss: 1.157287]\n",
      "epoch:3 step:3529 [D loss: 0.697284, acc.: 60.94%] [G loss: 1.148610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3530 [D loss: 0.682885, acc.: 62.50%] [G loss: 1.091176]\n",
      "epoch:3 step:3531 [D loss: 0.802103, acc.: 42.19%] [G loss: 1.025266]\n",
      "epoch:3 step:3532 [D loss: 0.651827, acc.: 59.38%] [G loss: 0.996059]\n",
      "epoch:3 step:3533 [D loss: 0.678135, acc.: 61.72%] [G loss: 1.069525]\n",
      "epoch:3 step:3534 [D loss: 0.641779, acc.: 64.06%] [G loss: 0.953951]\n",
      "epoch:3 step:3535 [D loss: 0.575415, acc.: 73.44%] [G loss: 1.034311]\n",
      "epoch:3 step:3536 [D loss: 0.606344, acc.: 71.09%] [G loss: 1.163292]\n",
      "epoch:3 step:3537 [D loss: 0.693140, acc.: 57.03%] [G loss: 1.109862]\n",
      "epoch:3 step:3538 [D loss: 0.624713, acc.: 61.72%] [G loss: 1.067726]\n",
      "epoch:3 step:3539 [D loss: 0.597345, acc.: 69.53%] [G loss: 1.080923]\n",
      "epoch:3 step:3540 [D loss: 0.629441, acc.: 65.62%] [G loss: 1.260967]\n",
      "epoch:3 step:3541 [D loss: 0.633073, acc.: 61.72%] [G loss: 0.988540]\n",
      "epoch:3 step:3542 [D loss: 0.662354, acc.: 60.94%] [G loss: 0.989483]\n",
      "epoch:3 step:3543 [D loss: 0.582117, acc.: 73.44%] [G loss: 1.054175]\n",
      "epoch:3 step:3544 [D loss: 0.687775, acc.: 57.81%] [G loss: 1.012799]\n",
      "epoch:3 step:3545 [D loss: 0.568458, acc.: 71.88%] [G loss: 1.205198]\n",
      "epoch:3 step:3546 [D loss: 0.708479, acc.: 53.91%] [G loss: 0.967624]\n",
      "epoch:3 step:3547 [D loss: 0.615785, acc.: 61.72%] [G loss: 1.044733]\n",
      "epoch:3 step:3548 [D loss: 0.638659, acc.: 64.06%] [G loss: 1.059457]\n",
      "epoch:3 step:3549 [D loss: 0.643956, acc.: 63.28%] [G loss: 1.179254]\n",
      "epoch:3 step:3550 [D loss: 0.692363, acc.: 63.28%] [G loss: 1.038711]\n",
      "epoch:3 step:3551 [D loss: 0.650404, acc.: 64.84%] [G loss: 1.171705]\n",
      "epoch:3 step:3552 [D loss: 0.721739, acc.: 58.59%] [G loss: 0.922434]\n",
      "epoch:3 step:3553 [D loss: 0.728589, acc.: 52.34%] [G loss: 1.230258]\n",
      "epoch:3 step:3554 [D loss: 0.704685, acc.: 64.06%] [G loss: 1.067391]\n",
      "epoch:3 step:3555 [D loss: 0.595516, acc.: 70.31%] [G loss: 1.197737]\n",
      "epoch:3 step:3556 [D loss: 0.653322, acc.: 57.03%] [G loss: 1.359390]\n",
      "epoch:3 step:3557 [D loss: 0.534746, acc.: 71.88%] [G loss: 1.248955]\n",
      "epoch:3 step:3558 [D loss: 0.736888, acc.: 51.56%] [G loss: 0.970381]\n",
      "epoch:3 step:3559 [D loss: 0.652579, acc.: 59.38%] [G loss: 1.182057]\n",
      "epoch:3 step:3560 [D loss: 0.603287, acc.: 64.06%] [G loss: 1.193161]\n",
      "epoch:3 step:3561 [D loss: 0.636232, acc.: 66.41%] [G loss: 1.169903]\n",
      "epoch:3 step:3562 [D loss: 0.665720, acc.: 62.50%] [G loss: 0.951748]\n",
      "epoch:3 step:3563 [D loss: 0.611713, acc.: 67.97%] [G loss: 1.236390]\n",
      "epoch:3 step:3564 [D loss: 0.643186, acc.: 68.75%] [G loss: 0.924844]\n",
      "epoch:3 step:3565 [D loss: 0.705879, acc.: 56.25%] [G loss: 1.087069]\n",
      "epoch:3 step:3566 [D loss: 0.685194, acc.: 56.25%] [G loss: 1.165479]\n",
      "epoch:3 step:3567 [D loss: 0.617112, acc.: 64.84%] [G loss: 1.239932]\n",
      "epoch:3 step:3568 [D loss: 0.662981, acc.: 64.84%] [G loss: 1.094149]\n",
      "epoch:3 step:3569 [D loss: 0.651645, acc.: 63.28%] [G loss: 1.155224]\n",
      "epoch:3 step:3570 [D loss: 0.564101, acc.: 73.44%] [G loss: 1.065811]\n",
      "epoch:3 step:3571 [D loss: 0.721712, acc.: 56.25%] [G loss: 1.161678]\n",
      "epoch:3 step:3572 [D loss: 0.701489, acc.: 49.22%] [G loss: 1.001525]\n",
      "epoch:3 step:3573 [D loss: 0.682560, acc.: 58.59%] [G loss: 1.157425]\n",
      "epoch:3 step:3574 [D loss: 0.695974, acc.: 54.69%] [G loss: 1.240176]\n",
      "epoch:3 step:3575 [D loss: 0.551880, acc.: 75.78%] [G loss: 1.200653]\n",
      "epoch:3 step:3576 [D loss: 0.641772, acc.: 67.19%] [G loss: 1.180152]\n",
      "epoch:3 step:3577 [D loss: 0.651101, acc.: 60.94%] [G loss: 1.122436]\n",
      "epoch:3 step:3578 [D loss: 0.730659, acc.: 54.69%] [G loss: 1.224028]\n",
      "epoch:3 step:3579 [D loss: 0.638014, acc.: 60.16%] [G loss: 1.041395]\n",
      "epoch:3 step:3580 [D loss: 0.656098, acc.: 63.28%] [G loss: 1.127604]\n",
      "epoch:3 step:3581 [D loss: 0.623811, acc.: 61.72%] [G loss: 1.289740]\n",
      "epoch:3 step:3582 [D loss: 0.687574, acc.: 55.47%] [G loss: 1.041858]\n",
      "epoch:3 step:3583 [D loss: 0.625494, acc.: 66.41%] [G loss: 1.197183]\n",
      "epoch:3 step:3584 [D loss: 0.557411, acc.: 73.44%] [G loss: 1.197772]\n",
      "epoch:3 step:3585 [D loss: 0.605749, acc.: 65.62%] [G loss: 1.028239]\n",
      "epoch:3 step:3586 [D loss: 0.558423, acc.: 75.78%] [G loss: 1.107769]\n",
      "epoch:3 step:3587 [D loss: 0.578244, acc.: 67.97%] [G loss: 1.073800]\n",
      "epoch:3 step:3588 [D loss: 0.597790, acc.: 69.53%] [G loss: 0.968280]\n",
      "epoch:3 step:3589 [D loss: 0.642148, acc.: 60.16%] [G loss: 1.145022]\n",
      "epoch:3 step:3590 [D loss: 0.613789, acc.: 65.62%] [G loss: 0.981849]\n",
      "epoch:3 step:3591 [D loss: 0.736863, acc.: 55.47%] [G loss: 1.068844]\n",
      "epoch:3 step:3592 [D loss: 0.645944, acc.: 64.06%] [G loss: 1.057344]\n",
      "epoch:3 step:3593 [D loss: 0.598373, acc.: 61.72%] [G loss: 1.189528]\n",
      "epoch:3 step:3594 [D loss: 0.638452, acc.: 67.97%] [G loss: 1.167169]\n",
      "epoch:3 step:3595 [D loss: 0.679569, acc.: 58.59%] [G loss: 1.008137]\n",
      "epoch:3 step:3596 [D loss: 0.727348, acc.: 55.47%] [G loss: 0.869273]\n",
      "epoch:3 step:3597 [D loss: 0.607478, acc.: 66.41%] [G loss: 1.070451]\n",
      "epoch:3 step:3598 [D loss: 0.740183, acc.: 51.56%] [G loss: 0.971728]\n",
      "epoch:3 step:3599 [D loss: 0.590658, acc.: 70.31%] [G loss: 1.134528]\n",
      "epoch:3 step:3600 [D loss: 0.660827, acc.: 60.94%] [G loss: 1.023084]\n",
      "##############\n",
      "[2.76608917 2.0259876  1.93450559 2.99793972 0.71993201 6.37753135\n",
      " 1.94659457 2.84302029 3.71408878 6.49931921]\n",
      "##########\n",
      "epoch:3 step:3601 [D loss: 0.552096, acc.: 72.66%] [G loss: 1.041715]\n",
      "epoch:3 step:3602 [D loss: 0.571672, acc.: 69.53%] [G loss: 1.102754]\n",
      "epoch:3 step:3603 [D loss: 0.605835, acc.: 64.84%] [G loss: 0.935924]\n",
      "epoch:3 step:3604 [D loss: 0.593625, acc.: 68.75%] [G loss: 1.009763]\n",
      "epoch:3 step:3605 [D loss: 0.502485, acc.: 77.34%] [G loss: 1.235023]\n",
      "epoch:3 step:3606 [D loss: 0.538805, acc.: 76.56%] [G loss: 1.173378]\n",
      "epoch:3 step:3607 [D loss: 0.641734, acc.: 64.84%] [G loss: 0.967724]\n",
      "epoch:3 step:3608 [D loss: 0.680567, acc.: 60.16%] [G loss: 1.255555]\n",
      "epoch:3 step:3609 [D loss: 0.642866, acc.: 60.94%] [G loss: 1.184814]\n",
      "epoch:3 step:3610 [D loss: 0.600498, acc.: 65.62%] [G loss: 1.185085]\n",
      "epoch:3 step:3611 [D loss: 0.717807, acc.: 57.03%] [G loss: 1.224607]\n",
      "epoch:3 step:3612 [D loss: 0.612222, acc.: 69.53%] [G loss: 1.073311]\n",
      "epoch:3 step:3613 [D loss: 0.683747, acc.: 58.59%] [G loss: 1.123021]\n",
      "epoch:3 step:3614 [D loss: 0.693586, acc.: 57.81%] [G loss: 1.098331]\n",
      "epoch:3 step:3615 [D loss: 0.725310, acc.: 51.56%] [G loss: 1.080215]\n",
      "epoch:3 step:3616 [D loss: 0.638037, acc.: 62.50%] [G loss: 1.056683]\n",
      "epoch:3 step:3617 [D loss: 0.677339, acc.: 60.94%] [G loss: 0.986825]\n",
      "epoch:3 step:3618 [D loss: 0.654733, acc.: 61.72%] [G loss: 1.097223]\n",
      "epoch:3 step:3619 [D loss: 0.589399, acc.: 67.97%] [G loss: 1.047912]\n",
      "epoch:3 step:3620 [D loss: 0.597619, acc.: 64.84%] [G loss: 1.113265]\n",
      "epoch:3 step:3621 [D loss: 0.616986, acc.: 65.62%] [G loss: 1.053029]\n",
      "epoch:3 step:3622 [D loss: 0.655343, acc.: 57.81%] [G loss: 0.993740]\n",
      "epoch:3 step:3623 [D loss: 0.653329, acc.: 63.28%] [G loss: 1.149035]\n",
      "epoch:3 step:3624 [D loss: 0.618861, acc.: 67.97%] [G loss: 1.193945]\n",
      "epoch:3 step:3625 [D loss: 0.658292, acc.: 57.81%] [G loss: 0.880367]\n",
      "epoch:3 step:3626 [D loss: 0.597870, acc.: 66.41%] [G loss: 1.115890]\n",
      "epoch:3 step:3627 [D loss: 0.492603, acc.: 75.78%] [G loss: 1.151156]\n",
      "epoch:3 step:3628 [D loss: 0.627187, acc.: 66.41%] [G loss: 1.139781]\n",
      "epoch:3 step:3629 [D loss: 0.679255, acc.: 60.16%] [G loss: 1.090704]\n",
      "epoch:3 step:3630 [D loss: 0.687747, acc.: 56.25%] [G loss: 0.937455]\n",
      "epoch:3 step:3631 [D loss: 0.713555, acc.: 52.34%] [G loss: 1.041892]\n",
      "epoch:3 step:3632 [D loss: 0.631565, acc.: 63.28%] [G loss: 0.984500]\n",
      "epoch:3 step:3633 [D loss: 0.529743, acc.: 72.66%] [G loss: 1.119542]\n",
      "epoch:3 step:3634 [D loss: 0.706485, acc.: 57.03%] [G loss: 0.912086]\n",
      "epoch:3 step:3635 [D loss: 0.623514, acc.: 67.97%] [G loss: 0.944043]\n",
      "epoch:3 step:3636 [D loss: 0.661812, acc.: 60.16%] [G loss: 1.120284]\n",
      "epoch:3 step:3637 [D loss: 0.596024, acc.: 72.66%] [G loss: 1.240260]\n",
      "epoch:3 step:3638 [D loss: 0.627001, acc.: 61.72%] [G loss: 1.157394]\n",
      "epoch:3 step:3639 [D loss: 0.683508, acc.: 61.72%] [G loss: 1.043247]\n",
      "epoch:3 step:3640 [D loss: 0.651477, acc.: 62.50%] [G loss: 1.001938]\n",
      "epoch:3 step:3641 [D loss: 0.738384, acc.: 54.69%] [G loss: 0.890531]\n",
      "epoch:3 step:3642 [D loss: 0.676500, acc.: 60.16%] [G loss: 1.116631]\n",
      "epoch:3 step:3643 [D loss: 0.569012, acc.: 69.53%] [G loss: 1.074004]\n",
      "epoch:3 step:3644 [D loss: 0.669650, acc.: 56.25%] [G loss: 1.119452]\n",
      "epoch:3 step:3645 [D loss: 0.684798, acc.: 60.94%] [G loss: 1.168617]\n",
      "epoch:3 step:3646 [D loss: 0.512325, acc.: 80.47%] [G loss: 1.078929]\n",
      "epoch:3 step:3647 [D loss: 0.611503, acc.: 67.19%] [G loss: 1.020056]\n",
      "epoch:3 step:3648 [D loss: 0.656319, acc.: 60.16%] [G loss: 0.970463]\n",
      "epoch:3 step:3649 [D loss: 0.702578, acc.: 53.91%] [G loss: 0.969175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3650 [D loss: 0.682553, acc.: 59.38%] [G loss: 1.208957]\n",
      "epoch:3 step:3651 [D loss: 0.562190, acc.: 67.97%] [G loss: 0.993328]\n",
      "epoch:3 step:3652 [D loss: 0.581308, acc.: 69.53%] [G loss: 1.011795]\n",
      "epoch:3 step:3653 [D loss: 0.623489, acc.: 70.31%] [G loss: 1.045973]\n",
      "epoch:3 step:3654 [D loss: 0.630270, acc.: 63.28%] [G loss: 1.081453]\n",
      "epoch:3 step:3655 [D loss: 0.658467, acc.: 59.38%] [G loss: 1.120883]\n",
      "epoch:3 step:3656 [D loss: 0.687460, acc.: 62.50%] [G loss: 1.091865]\n",
      "epoch:3 step:3657 [D loss: 0.633268, acc.: 64.06%] [G loss: 1.101298]\n",
      "epoch:3 step:3658 [D loss: 0.616455, acc.: 60.94%] [G loss: 1.088084]\n",
      "epoch:3 step:3659 [D loss: 0.690918, acc.: 59.38%] [G loss: 1.129286]\n",
      "epoch:3 step:3660 [D loss: 0.740143, acc.: 50.78%] [G loss: 0.997881]\n",
      "epoch:3 step:3661 [D loss: 0.626976, acc.: 64.06%] [G loss: 1.181475]\n",
      "epoch:3 step:3662 [D loss: 0.698580, acc.: 59.38%] [G loss: 1.062603]\n",
      "epoch:3 step:3663 [D loss: 0.594927, acc.: 72.66%] [G loss: 1.041763]\n",
      "epoch:3 step:3664 [D loss: 0.575719, acc.: 67.97%] [G loss: 1.061611]\n",
      "epoch:3 step:3665 [D loss: 0.678509, acc.: 59.38%] [G loss: 0.957393]\n",
      "epoch:3 step:3666 [D loss: 0.597756, acc.: 67.97%] [G loss: 1.220603]\n",
      "epoch:3 step:3667 [D loss: 0.586802, acc.: 73.44%] [G loss: 1.216233]\n",
      "epoch:3 step:3668 [D loss: 0.732088, acc.: 50.00%] [G loss: 0.937268]\n",
      "epoch:3 step:3669 [D loss: 0.551601, acc.: 72.66%] [G loss: 1.239503]\n",
      "epoch:3 step:3670 [D loss: 0.611693, acc.: 71.88%] [G loss: 0.994754]\n",
      "epoch:3 step:3671 [D loss: 0.549604, acc.: 73.44%] [G loss: 0.960225]\n",
      "epoch:3 step:3672 [D loss: 0.658895, acc.: 64.84%] [G loss: 1.031576]\n",
      "epoch:3 step:3673 [D loss: 0.586422, acc.: 69.53%] [G loss: 1.110732]\n",
      "epoch:3 step:3674 [D loss: 0.610267, acc.: 66.41%] [G loss: 1.193237]\n",
      "epoch:3 step:3675 [D loss: 0.632508, acc.: 64.84%] [G loss: 1.102310]\n",
      "epoch:3 step:3676 [D loss: 0.634379, acc.: 62.50%] [G loss: 1.154248]\n",
      "epoch:3 step:3677 [D loss: 0.671925, acc.: 55.47%] [G loss: 1.044744]\n",
      "epoch:3 step:3678 [D loss: 0.615413, acc.: 64.84%] [G loss: 1.111159]\n",
      "epoch:3 step:3679 [D loss: 0.608748, acc.: 62.50%] [G loss: 1.089489]\n",
      "epoch:3 step:3680 [D loss: 0.673715, acc.: 59.38%] [G loss: 1.045169]\n",
      "epoch:3 step:3681 [D loss: 0.599864, acc.: 68.75%] [G loss: 1.178611]\n",
      "epoch:3 step:3682 [D loss: 0.626804, acc.: 62.50%] [G loss: 1.110143]\n",
      "epoch:3 step:3683 [D loss: 0.722119, acc.: 57.81%] [G loss: 1.114966]\n",
      "epoch:3 step:3684 [D loss: 0.631687, acc.: 66.41%] [G loss: 1.161905]\n",
      "epoch:3 step:3685 [D loss: 0.644804, acc.: 60.94%] [G loss: 1.277577]\n",
      "epoch:3 step:3686 [D loss: 0.578930, acc.: 71.88%] [G loss: 1.305010]\n",
      "epoch:3 step:3687 [D loss: 0.642867, acc.: 62.50%] [G loss: 1.117974]\n",
      "epoch:3 step:3688 [D loss: 0.640005, acc.: 62.50%] [G loss: 1.205675]\n",
      "epoch:3 step:3689 [D loss: 0.679015, acc.: 59.38%] [G loss: 1.171197]\n",
      "epoch:3 step:3690 [D loss: 0.739357, acc.: 54.69%] [G loss: 1.005140]\n",
      "epoch:3 step:3691 [D loss: 0.602051, acc.: 67.97%] [G loss: 1.018876]\n",
      "epoch:3 step:3692 [D loss: 0.663017, acc.: 61.72%] [G loss: 1.184580]\n",
      "epoch:3 step:3693 [D loss: 0.581885, acc.: 75.00%] [G loss: 1.255368]\n",
      "epoch:3 step:3694 [D loss: 0.657833, acc.: 60.16%] [G loss: 1.033993]\n",
      "epoch:3 step:3695 [D loss: 0.685676, acc.: 60.16%] [G loss: 1.115161]\n",
      "epoch:3 step:3696 [D loss: 0.680924, acc.: 56.25%] [G loss: 0.989244]\n",
      "epoch:3 step:3697 [D loss: 0.590917, acc.: 74.22%] [G loss: 1.242335]\n",
      "epoch:3 step:3698 [D loss: 0.668585, acc.: 62.50%] [G loss: 0.915621]\n",
      "epoch:3 step:3699 [D loss: 0.657450, acc.: 59.38%] [G loss: 1.277639]\n",
      "epoch:3 step:3700 [D loss: 0.607848, acc.: 64.84%] [G loss: 1.202822]\n",
      "epoch:3 step:3701 [D loss: 0.695287, acc.: 57.03%] [G loss: 1.055355]\n",
      "epoch:3 step:3702 [D loss: 0.712107, acc.: 57.03%] [G loss: 1.119208]\n",
      "epoch:3 step:3703 [D loss: 0.659401, acc.: 57.03%] [G loss: 1.050792]\n",
      "epoch:3 step:3704 [D loss: 0.636239, acc.: 63.28%] [G loss: 0.911789]\n",
      "epoch:3 step:3705 [D loss: 0.669798, acc.: 57.81%] [G loss: 0.861966]\n",
      "epoch:3 step:3706 [D loss: 0.546553, acc.: 73.44%] [G loss: 1.312009]\n",
      "epoch:3 step:3707 [D loss: 0.635660, acc.: 64.84%] [G loss: 1.099990]\n",
      "epoch:3 step:3708 [D loss: 0.632587, acc.: 64.84%] [G loss: 1.027844]\n",
      "epoch:3 step:3709 [D loss: 0.627114, acc.: 58.59%] [G loss: 0.989619]\n",
      "epoch:3 step:3710 [D loss: 0.548043, acc.: 72.66%] [G loss: 1.161541]\n",
      "epoch:3 step:3711 [D loss: 0.691257, acc.: 54.69%] [G loss: 1.072684]\n",
      "epoch:3 step:3712 [D loss: 0.630205, acc.: 64.84%] [G loss: 1.173757]\n",
      "epoch:3 step:3713 [D loss: 0.642098, acc.: 57.03%] [G loss: 0.998520]\n",
      "epoch:3 step:3714 [D loss: 0.579902, acc.: 71.88%] [G loss: 1.110560]\n",
      "epoch:3 step:3715 [D loss: 0.598486, acc.: 65.62%] [G loss: 1.193952]\n",
      "epoch:3 step:3716 [D loss: 0.575979, acc.: 66.41%] [G loss: 1.081295]\n",
      "epoch:3 step:3717 [D loss: 0.653271, acc.: 56.25%] [G loss: 1.126858]\n",
      "epoch:3 step:3718 [D loss: 0.610960, acc.: 68.75%] [G loss: 1.113785]\n",
      "epoch:3 step:3719 [D loss: 0.656503, acc.: 61.72%] [G loss: 0.906176]\n",
      "epoch:3 step:3720 [D loss: 0.657604, acc.: 60.16%] [G loss: 1.005990]\n",
      "epoch:3 step:3721 [D loss: 0.588170, acc.: 69.53%] [G loss: 1.028830]\n",
      "epoch:3 step:3722 [D loss: 0.618183, acc.: 67.19%] [G loss: 1.034531]\n",
      "epoch:3 step:3723 [D loss: 0.642334, acc.: 64.84%] [G loss: 1.244829]\n",
      "epoch:3 step:3724 [D loss: 0.792525, acc.: 46.09%] [G loss: 0.947748]\n",
      "epoch:3 step:3725 [D loss: 0.723963, acc.: 50.00%] [G loss: 1.085752]\n",
      "epoch:3 step:3726 [D loss: 0.703177, acc.: 56.25%] [G loss: 1.155887]\n",
      "epoch:3 step:3727 [D loss: 0.670141, acc.: 61.72%] [G loss: 1.090105]\n",
      "epoch:3 step:3728 [D loss: 0.669373, acc.: 58.59%] [G loss: 1.236441]\n",
      "epoch:3 step:3729 [D loss: 0.725787, acc.: 52.34%] [G loss: 0.981152]\n",
      "epoch:3 step:3730 [D loss: 0.632361, acc.: 64.84%] [G loss: 1.267293]\n",
      "epoch:3 step:3731 [D loss: 0.540278, acc.: 74.22%] [G loss: 1.215326]\n",
      "epoch:3 step:3732 [D loss: 0.737460, acc.: 57.03%] [G loss: 1.028376]\n",
      "epoch:3 step:3733 [D loss: 0.608027, acc.: 68.75%] [G loss: 1.115522]\n",
      "epoch:3 step:3734 [D loss: 0.671894, acc.: 57.03%] [G loss: 1.164267]\n",
      "epoch:3 step:3735 [D loss: 0.585486, acc.: 70.31%] [G loss: 1.188981]\n",
      "epoch:3 step:3736 [D loss: 0.592852, acc.: 68.75%] [G loss: 1.151958]\n",
      "epoch:3 step:3737 [D loss: 0.547147, acc.: 71.09%] [G loss: 1.141567]\n",
      "epoch:3 step:3738 [D loss: 0.702595, acc.: 54.69%] [G loss: 1.171886]\n",
      "epoch:3 step:3739 [D loss: 0.592425, acc.: 74.22%] [G loss: 1.126199]\n",
      "epoch:3 step:3740 [D loss: 0.546330, acc.: 78.12%] [G loss: 1.134071]\n",
      "epoch:3 step:3741 [D loss: 0.636744, acc.: 63.28%] [G loss: 1.101484]\n",
      "epoch:3 step:3742 [D loss: 0.622408, acc.: 67.97%] [G loss: 1.130327]\n",
      "epoch:3 step:3743 [D loss: 0.598629, acc.: 64.84%] [G loss: 1.135859]\n",
      "epoch:3 step:3744 [D loss: 0.714594, acc.: 57.03%] [G loss: 1.135847]\n",
      "epoch:3 step:3745 [D loss: 0.584207, acc.: 66.41%] [G loss: 0.981632]\n",
      "epoch:3 step:3746 [D loss: 0.701142, acc.: 53.12%] [G loss: 1.030995]\n",
      "epoch:3 step:3747 [D loss: 0.617910, acc.: 63.28%] [G loss: 1.241739]\n",
      "epoch:3 step:3748 [D loss: 0.643289, acc.: 64.84%] [G loss: 1.291802]\n",
      "epoch:4 step:3749 [D loss: 0.632356, acc.: 63.28%] [G loss: 0.889296]\n",
      "epoch:4 step:3750 [D loss: 0.697224, acc.: 55.47%] [G loss: 1.034549]\n",
      "epoch:4 step:3751 [D loss: 0.647589, acc.: 65.62%] [G loss: 1.019891]\n",
      "epoch:4 step:3752 [D loss: 0.698406, acc.: 59.38%] [G loss: 1.060219]\n",
      "epoch:4 step:3753 [D loss: 0.630795, acc.: 63.28%] [G loss: 0.940316]\n",
      "epoch:4 step:3754 [D loss: 0.815493, acc.: 42.19%] [G loss: 0.953759]\n",
      "epoch:4 step:3755 [D loss: 0.726486, acc.: 55.47%] [G loss: 1.016324]\n",
      "epoch:4 step:3756 [D loss: 0.664902, acc.: 63.28%] [G loss: 1.102025]\n",
      "epoch:4 step:3757 [D loss: 0.706777, acc.: 53.91%] [G loss: 1.157801]\n",
      "epoch:4 step:3758 [D loss: 0.645038, acc.: 60.94%] [G loss: 1.068420]\n",
      "epoch:4 step:3759 [D loss: 0.554878, acc.: 71.88%] [G loss: 1.169381]\n",
      "epoch:4 step:3760 [D loss: 0.565335, acc.: 71.09%] [G loss: 1.160990]\n",
      "epoch:4 step:3761 [D loss: 0.620867, acc.: 68.75%] [G loss: 1.139716]\n",
      "epoch:4 step:3762 [D loss: 0.619707, acc.: 64.84%] [G loss: 1.103637]\n",
      "epoch:4 step:3763 [D loss: 0.595951, acc.: 65.62%] [G loss: 1.198394]\n",
      "epoch:4 step:3764 [D loss: 0.620388, acc.: 66.41%] [G loss: 1.166261]\n",
      "epoch:4 step:3765 [D loss: 0.623438, acc.: 64.84%] [G loss: 1.109125]\n",
      "epoch:4 step:3766 [D loss: 0.713243, acc.: 57.03%] [G loss: 1.010225]\n",
      "epoch:4 step:3767 [D loss: 0.670675, acc.: 61.72%] [G loss: 0.962620]\n",
      "epoch:4 step:3768 [D loss: 0.614273, acc.: 67.19%] [G loss: 1.216289]\n",
      "epoch:4 step:3769 [D loss: 0.603089, acc.: 68.75%] [G loss: 1.005526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3770 [D loss: 0.534685, acc.: 72.66%] [G loss: 1.114088]\n",
      "epoch:4 step:3771 [D loss: 0.624631, acc.: 68.75%] [G loss: 0.987328]\n",
      "epoch:4 step:3772 [D loss: 0.593495, acc.: 70.31%] [G loss: 1.082944]\n",
      "epoch:4 step:3773 [D loss: 0.637357, acc.: 67.19%] [G loss: 1.099377]\n",
      "epoch:4 step:3774 [D loss: 0.689380, acc.: 50.78%] [G loss: 1.171859]\n",
      "epoch:4 step:3775 [D loss: 0.611091, acc.: 64.06%] [G loss: 1.222187]\n",
      "epoch:4 step:3776 [D loss: 0.658474, acc.: 57.81%] [G loss: 1.029002]\n",
      "epoch:4 step:3777 [D loss: 0.678977, acc.: 60.94%] [G loss: 0.972913]\n",
      "epoch:4 step:3778 [D loss: 0.588277, acc.: 70.31%] [G loss: 1.300067]\n",
      "epoch:4 step:3779 [D loss: 0.578404, acc.: 75.78%] [G loss: 1.204424]\n",
      "epoch:4 step:3780 [D loss: 0.689816, acc.: 61.72%] [G loss: 0.901671]\n",
      "epoch:4 step:3781 [D loss: 0.593498, acc.: 67.19%] [G loss: 1.169958]\n",
      "epoch:4 step:3782 [D loss: 0.653480, acc.: 62.50%] [G loss: 1.029878]\n",
      "epoch:4 step:3783 [D loss: 0.645282, acc.: 63.28%] [G loss: 1.028550]\n",
      "epoch:4 step:3784 [D loss: 0.576570, acc.: 67.19%] [G loss: 1.234845]\n",
      "epoch:4 step:3785 [D loss: 0.593568, acc.: 66.41%] [G loss: 1.218605]\n",
      "epoch:4 step:3786 [D loss: 0.727615, acc.: 57.81%] [G loss: 1.190389]\n",
      "epoch:4 step:3787 [D loss: 0.650932, acc.: 60.16%] [G loss: 0.839440]\n",
      "epoch:4 step:3788 [D loss: 0.712508, acc.: 57.81%] [G loss: 1.159873]\n",
      "epoch:4 step:3789 [D loss: 0.577275, acc.: 72.66%] [G loss: 1.192141]\n",
      "epoch:4 step:3790 [D loss: 0.591903, acc.: 67.19%] [G loss: 1.033506]\n",
      "epoch:4 step:3791 [D loss: 0.533888, acc.: 75.78%] [G loss: 1.284998]\n",
      "epoch:4 step:3792 [D loss: 0.584028, acc.: 71.09%] [G loss: 0.963026]\n",
      "epoch:4 step:3793 [D loss: 0.599466, acc.: 64.84%] [G loss: 1.022753]\n",
      "epoch:4 step:3794 [D loss: 0.782564, acc.: 49.22%] [G loss: 0.991758]\n",
      "epoch:4 step:3795 [D loss: 0.660504, acc.: 56.25%] [G loss: 1.038442]\n",
      "epoch:4 step:3796 [D loss: 0.631327, acc.: 62.50%] [G loss: 1.126048]\n",
      "epoch:4 step:3797 [D loss: 0.664073, acc.: 60.94%] [G loss: 0.965202]\n",
      "epoch:4 step:3798 [D loss: 0.676623, acc.: 60.16%] [G loss: 1.092902]\n",
      "epoch:4 step:3799 [D loss: 0.628797, acc.: 65.62%] [G loss: 1.081781]\n",
      "epoch:4 step:3800 [D loss: 0.616874, acc.: 65.62%] [G loss: 1.065621]\n",
      "##############\n",
      "[2.72960235 2.05403203 2.12982166 3.17292972 1.02011118 6.33087878\n",
      " 2.24393808 3.05889033 3.89438274 8.14868929]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.740769, acc.: 49.22%] [G loss: 0.990289]\n",
      "epoch:4 step:3802 [D loss: 0.504511, acc.: 79.69%] [G loss: 1.306986]\n",
      "epoch:4 step:3803 [D loss: 0.742261, acc.: 46.88%] [G loss: 1.116380]\n",
      "epoch:4 step:3804 [D loss: 0.629965, acc.: 64.06%] [G loss: 1.155355]\n",
      "epoch:4 step:3805 [D loss: 0.607593, acc.: 66.41%] [G loss: 1.002187]\n",
      "epoch:4 step:3806 [D loss: 0.589349, acc.: 74.22%] [G loss: 1.196929]\n",
      "epoch:4 step:3807 [D loss: 0.547511, acc.: 73.44%] [G loss: 1.084199]\n",
      "epoch:4 step:3808 [D loss: 0.602728, acc.: 64.06%] [G loss: 0.904880]\n",
      "epoch:4 step:3809 [D loss: 0.609188, acc.: 67.97%] [G loss: 1.141626]\n",
      "epoch:4 step:3810 [D loss: 0.608620, acc.: 68.75%] [G loss: 1.188912]\n",
      "epoch:4 step:3811 [D loss: 0.689274, acc.: 59.38%] [G loss: 1.167478]\n",
      "epoch:4 step:3812 [D loss: 0.656207, acc.: 60.16%] [G loss: 1.028181]\n",
      "epoch:4 step:3813 [D loss: 0.654627, acc.: 60.16%] [G loss: 1.068226]\n",
      "epoch:4 step:3814 [D loss: 0.739481, acc.: 57.03%] [G loss: 1.034477]\n",
      "epoch:4 step:3815 [D loss: 0.705057, acc.: 50.78%] [G loss: 0.973940]\n",
      "epoch:4 step:3816 [D loss: 0.603411, acc.: 69.53%] [G loss: 1.288570]\n",
      "epoch:4 step:3817 [D loss: 0.586046, acc.: 71.09%] [G loss: 1.156691]\n",
      "epoch:4 step:3818 [D loss: 0.606579, acc.: 66.41%] [G loss: 1.101579]\n",
      "epoch:4 step:3819 [D loss: 0.691992, acc.: 62.50%] [G loss: 1.025116]\n",
      "epoch:4 step:3820 [D loss: 0.580868, acc.: 69.53%] [G loss: 1.014617]\n",
      "epoch:4 step:3821 [D loss: 0.627227, acc.: 59.38%] [G loss: 0.966016]\n",
      "epoch:4 step:3822 [D loss: 0.691618, acc.: 55.47%] [G loss: 1.144511]\n",
      "epoch:4 step:3823 [D loss: 0.585155, acc.: 67.97%] [G loss: 1.191906]\n",
      "epoch:4 step:3824 [D loss: 0.755615, acc.: 53.91%] [G loss: 1.029480]\n",
      "epoch:4 step:3825 [D loss: 0.611104, acc.: 65.62%] [G loss: 1.108928]\n",
      "epoch:4 step:3826 [D loss: 0.678020, acc.: 57.03%] [G loss: 1.046086]\n",
      "epoch:4 step:3827 [D loss: 0.741988, acc.: 47.66%] [G loss: 0.845074]\n",
      "epoch:4 step:3828 [D loss: 0.597606, acc.: 69.53%] [G loss: 1.098540]\n",
      "epoch:4 step:3829 [D loss: 0.655129, acc.: 61.72%] [G loss: 1.179513]\n",
      "epoch:4 step:3830 [D loss: 0.652624, acc.: 59.38%] [G loss: 1.063967]\n",
      "epoch:4 step:3831 [D loss: 0.603359, acc.: 69.53%] [G loss: 1.185121]\n",
      "epoch:4 step:3832 [D loss: 0.633907, acc.: 67.19%] [G loss: 1.182429]\n",
      "epoch:4 step:3833 [D loss: 0.534738, acc.: 78.12%] [G loss: 1.059344]\n",
      "epoch:4 step:3834 [D loss: 0.710393, acc.: 53.91%] [G loss: 1.087836]\n",
      "epoch:4 step:3835 [D loss: 0.664464, acc.: 61.72%] [G loss: 1.010596]\n",
      "epoch:4 step:3836 [D loss: 0.663250, acc.: 56.25%] [G loss: 1.038430]\n",
      "epoch:4 step:3837 [D loss: 0.720813, acc.: 56.25%] [G loss: 1.082200]\n",
      "epoch:4 step:3838 [D loss: 0.565608, acc.: 75.78%] [G loss: 1.129092]\n",
      "epoch:4 step:3839 [D loss: 0.610876, acc.: 60.94%] [G loss: 1.138907]\n",
      "epoch:4 step:3840 [D loss: 0.617118, acc.: 63.28%] [G loss: 1.056177]\n",
      "epoch:4 step:3841 [D loss: 0.691026, acc.: 52.34%] [G loss: 1.069349]\n",
      "epoch:4 step:3842 [D loss: 0.657536, acc.: 60.16%] [G loss: 1.218569]\n",
      "epoch:4 step:3843 [D loss: 0.658717, acc.: 62.50%] [G loss: 1.053526]\n",
      "epoch:4 step:3844 [D loss: 0.578734, acc.: 67.19%] [G loss: 1.099624]\n",
      "epoch:4 step:3845 [D loss: 0.607060, acc.: 65.62%] [G loss: 1.173434]\n",
      "epoch:4 step:3846 [D loss: 0.625099, acc.: 57.81%] [G loss: 1.222957]\n",
      "epoch:4 step:3847 [D loss: 0.619131, acc.: 62.50%] [G loss: 1.192028]\n",
      "epoch:4 step:3848 [D loss: 0.619392, acc.: 67.19%] [G loss: 1.092998]\n",
      "epoch:4 step:3849 [D loss: 0.642901, acc.: 62.50%] [G loss: 0.888079]\n",
      "epoch:4 step:3850 [D loss: 0.725404, acc.: 50.78%] [G loss: 1.074771]\n",
      "epoch:4 step:3851 [D loss: 0.582669, acc.: 67.19%] [G loss: 0.966237]\n",
      "epoch:4 step:3852 [D loss: 0.639020, acc.: 61.72%] [G loss: 1.080673]\n",
      "epoch:4 step:3853 [D loss: 0.607247, acc.: 66.41%] [G loss: 1.245002]\n",
      "epoch:4 step:3854 [D loss: 0.626896, acc.: 65.62%] [G loss: 1.285033]\n",
      "epoch:4 step:3855 [D loss: 0.567325, acc.: 71.09%] [G loss: 1.156538]\n",
      "epoch:4 step:3856 [D loss: 0.604466, acc.: 64.84%] [G loss: 1.072616]\n",
      "epoch:4 step:3857 [D loss: 0.613624, acc.: 67.19%] [G loss: 1.093607]\n",
      "epoch:4 step:3858 [D loss: 0.614622, acc.: 69.53%] [G loss: 1.089219]\n",
      "epoch:4 step:3859 [D loss: 0.627406, acc.: 66.41%] [G loss: 1.073079]\n",
      "epoch:4 step:3860 [D loss: 0.708930, acc.: 57.03%] [G loss: 0.963102]\n",
      "epoch:4 step:3861 [D loss: 0.637941, acc.: 66.41%] [G loss: 0.977599]\n",
      "epoch:4 step:3862 [D loss: 0.616554, acc.: 64.06%] [G loss: 1.007631]\n",
      "epoch:4 step:3863 [D loss: 0.645676, acc.: 63.28%] [G loss: 1.265029]\n",
      "epoch:4 step:3864 [D loss: 0.725361, acc.: 55.47%] [G loss: 0.933134]\n",
      "epoch:4 step:3865 [D loss: 0.727639, acc.: 50.78%] [G loss: 0.991989]\n",
      "epoch:4 step:3866 [D loss: 0.589606, acc.: 67.97%] [G loss: 1.215372]\n",
      "epoch:4 step:3867 [D loss: 0.730017, acc.: 53.12%] [G loss: 1.042531]\n",
      "epoch:4 step:3868 [D loss: 0.705994, acc.: 57.03%] [G loss: 1.044904]\n",
      "epoch:4 step:3869 [D loss: 0.730774, acc.: 50.78%] [G loss: 1.016087]\n",
      "epoch:4 step:3870 [D loss: 0.691454, acc.: 58.59%] [G loss: 0.952882]\n",
      "epoch:4 step:3871 [D loss: 0.603560, acc.: 71.88%] [G loss: 1.006407]\n",
      "epoch:4 step:3872 [D loss: 0.745501, acc.: 52.34%] [G loss: 1.002191]\n",
      "epoch:4 step:3873 [D loss: 0.595768, acc.: 65.62%] [G loss: 1.102399]\n",
      "epoch:4 step:3874 [D loss: 0.744329, acc.: 55.47%] [G loss: 0.951907]\n",
      "epoch:4 step:3875 [D loss: 0.626761, acc.: 64.06%] [G loss: 1.089635]\n",
      "epoch:4 step:3876 [D loss: 0.652571, acc.: 60.94%] [G loss: 1.064583]\n",
      "epoch:4 step:3877 [D loss: 0.620100, acc.: 62.50%] [G loss: 0.945524]\n",
      "epoch:4 step:3878 [D loss: 0.562119, acc.: 70.31%] [G loss: 1.270829]\n",
      "epoch:4 step:3879 [D loss: 0.675755, acc.: 60.94%] [G loss: 1.140254]\n",
      "epoch:4 step:3880 [D loss: 0.726243, acc.: 52.34%] [G loss: 1.019055]\n",
      "epoch:4 step:3881 [D loss: 0.615385, acc.: 64.06%] [G loss: 1.181203]\n",
      "epoch:4 step:3882 [D loss: 0.667727, acc.: 57.81%] [G loss: 1.075140]\n",
      "epoch:4 step:3883 [D loss: 0.664897, acc.: 57.81%] [G loss: 1.260816]\n",
      "epoch:4 step:3884 [D loss: 0.672357, acc.: 60.16%] [G loss: 1.056830]\n",
      "epoch:4 step:3885 [D loss: 0.555663, acc.: 72.66%] [G loss: 0.991366]\n",
      "epoch:4 step:3886 [D loss: 0.573792, acc.: 69.53%] [G loss: 1.089702]\n",
      "epoch:4 step:3887 [D loss: 0.628056, acc.: 58.59%] [G loss: 0.997439]\n",
      "epoch:4 step:3888 [D loss: 0.684230, acc.: 54.69%] [G loss: 1.113656]\n",
      "epoch:4 step:3889 [D loss: 0.560254, acc.: 71.09%] [G loss: 1.137914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3890 [D loss: 0.576911, acc.: 71.88%] [G loss: 1.136755]\n",
      "epoch:4 step:3891 [D loss: 0.674013, acc.: 55.47%] [G loss: 1.086807]\n",
      "epoch:4 step:3892 [D loss: 0.681986, acc.: 59.38%] [G loss: 1.115730]\n",
      "epoch:4 step:3893 [D loss: 0.565973, acc.: 75.78%] [G loss: 1.198346]\n",
      "epoch:4 step:3894 [D loss: 0.638554, acc.: 64.84%] [G loss: 1.049283]\n",
      "epoch:4 step:3895 [D loss: 0.623776, acc.: 62.50%] [G loss: 1.006018]\n",
      "epoch:4 step:3896 [D loss: 0.751227, acc.: 50.78%] [G loss: 0.916225]\n",
      "epoch:4 step:3897 [D loss: 0.750716, acc.: 48.44%] [G loss: 1.147475]\n",
      "epoch:4 step:3898 [D loss: 0.551082, acc.: 74.22%] [G loss: 1.202757]\n",
      "epoch:4 step:3899 [D loss: 0.682600, acc.: 57.03%] [G loss: 1.059694]\n",
      "epoch:4 step:3900 [D loss: 0.617181, acc.: 68.75%] [G loss: 1.128274]\n",
      "epoch:4 step:3901 [D loss: 0.588881, acc.: 68.75%] [G loss: 1.099580]\n",
      "epoch:4 step:3902 [D loss: 0.678557, acc.: 57.03%] [G loss: 1.129330]\n",
      "epoch:4 step:3903 [D loss: 0.641265, acc.: 59.38%] [G loss: 1.151482]\n",
      "epoch:4 step:3904 [D loss: 0.638762, acc.: 66.41%] [G loss: 1.015578]\n",
      "epoch:4 step:3905 [D loss: 0.692776, acc.: 57.03%] [G loss: 1.088590]\n",
      "epoch:4 step:3906 [D loss: 0.598416, acc.: 67.19%] [G loss: 1.142112]\n",
      "epoch:4 step:3907 [D loss: 0.626327, acc.: 68.75%] [G loss: 1.153584]\n",
      "epoch:4 step:3908 [D loss: 0.652143, acc.: 64.06%] [G loss: 0.943050]\n",
      "epoch:4 step:3909 [D loss: 0.601080, acc.: 64.06%] [G loss: 1.289611]\n",
      "epoch:4 step:3910 [D loss: 0.725911, acc.: 53.91%] [G loss: 1.109782]\n",
      "epoch:4 step:3911 [D loss: 0.625595, acc.: 65.62%] [G loss: 1.249504]\n",
      "epoch:4 step:3912 [D loss: 0.698270, acc.: 53.91%] [G loss: 0.984729]\n",
      "epoch:4 step:3913 [D loss: 0.620593, acc.: 64.06%] [G loss: 1.267434]\n",
      "epoch:4 step:3914 [D loss: 0.712681, acc.: 56.25%] [G loss: 1.036030]\n",
      "epoch:4 step:3915 [D loss: 0.672830, acc.: 60.16%] [G loss: 1.112429]\n",
      "epoch:4 step:3916 [D loss: 0.648559, acc.: 60.16%] [G loss: 1.071608]\n",
      "epoch:4 step:3917 [D loss: 0.652411, acc.: 63.28%] [G loss: 1.125425]\n",
      "epoch:4 step:3918 [D loss: 0.616713, acc.: 65.62%] [G loss: 1.120814]\n",
      "epoch:4 step:3919 [D loss: 0.678522, acc.: 61.72%] [G loss: 1.041546]\n",
      "epoch:4 step:3920 [D loss: 0.640884, acc.: 60.94%] [G loss: 0.969899]\n",
      "epoch:4 step:3921 [D loss: 0.600029, acc.: 70.31%] [G loss: 1.210912]\n",
      "epoch:4 step:3922 [D loss: 0.698250, acc.: 54.69%] [G loss: 0.894436]\n",
      "epoch:4 step:3923 [D loss: 0.598538, acc.: 64.06%] [G loss: 0.998146]\n",
      "epoch:4 step:3924 [D loss: 0.746225, acc.: 46.88%] [G loss: 1.049999]\n",
      "epoch:4 step:3925 [D loss: 0.609329, acc.: 65.62%] [G loss: 1.247406]\n",
      "epoch:4 step:3926 [D loss: 0.776819, acc.: 50.78%] [G loss: 1.069114]\n",
      "epoch:4 step:3927 [D loss: 0.711520, acc.: 56.25%] [G loss: 0.937872]\n",
      "epoch:4 step:3928 [D loss: 0.663889, acc.: 58.59%] [G loss: 0.892240]\n",
      "epoch:4 step:3929 [D loss: 0.680691, acc.: 61.72%] [G loss: 1.042345]\n",
      "epoch:4 step:3930 [D loss: 0.545224, acc.: 75.00%] [G loss: 1.180593]\n",
      "epoch:4 step:3931 [D loss: 0.645609, acc.: 60.94%] [G loss: 1.021921]\n",
      "epoch:4 step:3932 [D loss: 0.601565, acc.: 67.19%] [G loss: 1.066686]\n",
      "epoch:4 step:3933 [D loss: 0.636491, acc.: 63.28%] [G loss: 1.001922]\n",
      "epoch:4 step:3934 [D loss: 0.579604, acc.: 74.22%] [G loss: 1.112473]\n",
      "epoch:4 step:3935 [D loss: 0.576109, acc.: 66.41%] [G loss: 1.067845]\n",
      "epoch:4 step:3936 [D loss: 0.649908, acc.: 62.50%] [G loss: 1.147628]\n",
      "epoch:4 step:3937 [D loss: 0.651919, acc.: 60.94%] [G loss: 1.064904]\n",
      "epoch:4 step:3938 [D loss: 0.633404, acc.: 62.50%] [G loss: 0.981668]\n",
      "epoch:4 step:3939 [D loss: 0.607968, acc.: 63.28%] [G loss: 1.074941]\n",
      "epoch:4 step:3940 [D loss: 0.727403, acc.: 54.69%] [G loss: 1.038303]\n",
      "epoch:4 step:3941 [D loss: 0.696215, acc.: 55.47%] [G loss: 1.163749]\n",
      "epoch:4 step:3942 [D loss: 0.663906, acc.: 63.28%] [G loss: 0.971210]\n",
      "epoch:4 step:3943 [D loss: 0.622111, acc.: 67.19%] [G loss: 1.095835]\n",
      "epoch:4 step:3944 [D loss: 0.609074, acc.: 64.84%] [G loss: 0.929157]\n",
      "epoch:4 step:3945 [D loss: 0.681279, acc.: 58.59%] [G loss: 1.002162]\n",
      "epoch:4 step:3946 [D loss: 0.582331, acc.: 64.06%] [G loss: 1.176269]\n",
      "epoch:4 step:3947 [D loss: 0.496804, acc.: 79.69%] [G loss: 1.159594]\n",
      "epoch:4 step:3948 [D loss: 0.669593, acc.: 59.38%] [G loss: 0.985803]\n",
      "epoch:4 step:3949 [D loss: 0.598742, acc.: 68.75%] [G loss: 0.893274]\n",
      "epoch:4 step:3950 [D loss: 0.575453, acc.: 67.19%] [G loss: 1.107083]\n",
      "epoch:4 step:3951 [D loss: 0.704241, acc.: 57.03%] [G loss: 1.245560]\n",
      "epoch:4 step:3952 [D loss: 0.636843, acc.: 62.50%] [G loss: 0.990941]\n",
      "epoch:4 step:3953 [D loss: 0.653173, acc.: 64.06%] [G loss: 1.092735]\n",
      "epoch:4 step:3954 [D loss: 0.614417, acc.: 67.97%] [G loss: 1.169698]\n",
      "epoch:4 step:3955 [D loss: 0.672730, acc.: 58.59%] [G loss: 0.994798]\n",
      "epoch:4 step:3956 [D loss: 0.578438, acc.: 69.53%] [G loss: 0.964863]\n",
      "epoch:4 step:3957 [D loss: 0.688329, acc.: 57.03%] [G loss: 1.043144]\n",
      "epoch:4 step:3958 [D loss: 0.626869, acc.: 66.41%] [G loss: 1.032890]\n",
      "epoch:4 step:3959 [D loss: 0.678475, acc.: 61.72%] [G loss: 1.140938]\n",
      "epoch:4 step:3960 [D loss: 0.612530, acc.: 66.41%] [G loss: 1.143417]\n",
      "epoch:4 step:3961 [D loss: 0.609946, acc.: 64.84%] [G loss: 1.185035]\n",
      "epoch:4 step:3962 [D loss: 0.718468, acc.: 57.81%] [G loss: 0.919700]\n",
      "epoch:4 step:3963 [D loss: 0.625783, acc.: 60.94%] [G loss: 1.159144]\n",
      "epoch:4 step:3964 [D loss: 0.738476, acc.: 56.25%] [G loss: 1.033678]\n",
      "epoch:4 step:3965 [D loss: 0.580684, acc.: 68.75%] [G loss: 0.970765]\n",
      "epoch:4 step:3966 [D loss: 0.634299, acc.: 65.62%] [G loss: 1.047855]\n",
      "epoch:4 step:3967 [D loss: 0.700656, acc.: 60.16%] [G loss: 0.869102]\n",
      "epoch:4 step:3968 [D loss: 0.718397, acc.: 52.34%] [G loss: 0.961608]\n",
      "epoch:4 step:3969 [D loss: 0.678966, acc.: 59.38%] [G loss: 1.104234]\n",
      "epoch:4 step:3970 [D loss: 0.663634, acc.: 59.38%] [G loss: 1.132887]\n",
      "epoch:4 step:3971 [D loss: 0.612939, acc.: 65.62%] [G loss: 1.195539]\n",
      "epoch:4 step:3972 [D loss: 0.605564, acc.: 68.75%] [G loss: 0.913688]\n",
      "epoch:4 step:3973 [D loss: 0.730148, acc.: 53.12%] [G loss: 0.894495]\n",
      "epoch:4 step:3974 [D loss: 0.688268, acc.: 55.47%] [G loss: 1.117870]\n",
      "epoch:4 step:3975 [D loss: 0.596738, acc.: 68.75%] [G loss: 0.900629]\n",
      "epoch:4 step:3976 [D loss: 0.687164, acc.: 57.03%] [G loss: 1.183358]\n",
      "epoch:4 step:3977 [D loss: 0.615393, acc.: 69.53%] [G loss: 1.182806]\n",
      "epoch:4 step:3978 [D loss: 0.562650, acc.: 67.97%] [G loss: 1.046879]\n",
      "epoch:4 step:3979 [D loss: 0.616152, acc.: 67.97%] [G loss: 1.015572]\n",
      "epoch:4 step:3980 [D loss: 0.613464, acc.: 68.75%] [G loss: 1.078628]\n",
      "epoch:4 step:3981 [D loss: 0.611467, acc.: 64.06%] [G loss: 1.028738]\n",
      "epoch:4 step:3982 [D loss: 0.663886, acc.: 62.50%] [G loss: 0.945889]\n",
      "epoch:4 step:3983 [D loss: 0.642203, acc.: 65.62%] [G loss: 0.987921]\n",
      "epoch:4 step:3984 [D loss: 0.574113, acc.: 71.09%] [G loss: 1.265064]\n",
      "epoch:4 step:3985 [D loss: 0.601573, acc.: 64.06%] [G loss: 0.931956]\n",
      "epoch:4 step:3986 [D loss: 0.613781, acc.: 65.62%] [G loss: 1.108426]\n",
      "epoch:4 step:3987 [D loss: 0.638322, acc.: 64.06%] [G loss: 0.972015]\n",
      "epoch:4 step:3988 [D loss: 0.742142, acc.: 53.12%] [G loss: 0.832920]\n",
      "epoch:4 step:3989 [D loss: 0.709862, acc.: 57.81%] [G loss: 1.156581]\n",
      "epoch:4 step:3990 [D loss: 0.797316, acc.: 45.31%] [G loss: 1.017808]\n",
      "epoch:4 step:3991 [D loss: 0.658270, acc.: 56.25%] [G loss: 0.938102]\n",
      "epoch:4 step:3992 [D loss: 0.639382, acc.: 63.28%] [G loss: 1.074016]\n",
      "epoch:4 step:3993 [D loss: 0.644330, acc.: 64.84%] [G loss: 1.213845]\n",
      "epoch:4 step:3994 [D loss: 0.657677, acc.: 63.28%] [G loss: 0.976000]\n",
      "epoch:4 step:3995 [D loss: 0.660675, acc.: 61.72%] [G loss: 1.078293]\n",
      "epoch:4 step:3996 [D loss: 0.631349, acc.: 63.28%] [G loss: 1.047539]\n",
      "epoch:4 step:3997 [D loss: 0.703229, acc.: 57.81%] [G loss: 0.975330]\n",
      "epoch:4 step:3998 [D loss: 0.616887, acc.: 59.38%] [G loss: 1.222284]\n",
      "epoch:4 step:3999 [D loss: 0.650099, acc.: 60.94%] [G loss: 1.015722]\n",
      "epoch:4 step:4000 [D loss: 0.624713, acc.: 65.62%] [G loss: 1.101889]\n",
      "##############\n",
      "[2.72665183 2.02655619 2.13247722 2.90940191 1.06317005 6.94472859\n",
      " 2.35343059 3.33156591 3.86395858 5.00327727]\n",
      "##########\n",
      "epoch:4 step:4001 [D loss: 0.718540, acc.: 57.03%] [G loss: 0.984530]\n",
      "epoch:4 step:4002 [D loss: 0.699000, acc.: 57.81%] [G loss: 0.971916]\n",
      "epoch:4 step:4003 [D loss: 0.629268, acc.: 63.28%] [G loss: 1.177383]\n",
      "epoch:4 step:4004 [D loss: 0.594491, acc.: 64.06%] [G loss: 1.227563]\n",
      "epoch:4 step:4005 [D loss: 0.663035, acc.: 57.81%] [G loss: 0.984513]\n",
      "epoch:4 step:4006 [D loss: 0.634450, acc.: 62.50%] [G loss: 1.230452]\n",
      "epoch:4 step:4007 [D loss: 0.751376, acc.: 50.78%] [G loss: 1.089251]\n",
      "epoch:4 step:4008 [D loss: 0.647208, acc.: 64.84%] [G loss: 1.214209]\n",
      "epoch:4 step:4009 [D loss: 0.610810, acc.: 69.53%] [G loss: 1.025141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4010 [D loss: 0.679748, acc.: 60.16%] [G loss: 1.134593]\n",
      "epoch:4 step:4011 [D loss: 0.733987, acc.: 50.78%] [G loss: 1.070491]\n",
      "epoch:4 step:4012 [D loss: 0.736299, acc.: 54.69%] [G loss: 0.967378]\n",
      "epoch:4 step:4013 [D loss: 0.614397, acc.: 66.41%] [G loss: 1.079765]\n",
      "epoch:4 step:4014 [D loss: 0.672683, acc.: 58.59%] [G loss: 1.118594]\n",
      "epoch:4 step:4015 [D loss: 0.657681, acc.: 62.50%] [G loss: 0.880996]\n",
      "epoch:4 step:4016 [D loss: 0.561627, acc.: 74.22%] [G loss: 1.198810]\n",
      "epoch:4 step:4017 [D loss: 0.601101, acc.: 66.41%] [G loss: 1.096239]\n",
      "epoch:4 step:4018 [D loss: 0.646210, acc.: 61.72%] [G loss: 1.157305]\n",
      "epoch:4 step:4019 [D loss: 0.656379, acc.: 64.84%] [G loss: 0.955361]\n",
      "epoch:4 step:4020 [D loss: 0.691743, acc.: 53.91%] [G loss: 1.036409]\n",
      "epoch:4 step:4021 [D loss: 0.631737, acc.: 64.84%] [G loss: 1.100936]\n",
      "epoch:4 step:4022 [D loss: 0.605721, acc.: 67.19%] [G loss: 1.023029]\n",
      "epoch:4 step:4023 [D loss: 0.667174, acc.: 64.06%] [G loss: 0.986537]\n",
      "epoch:4 step:4024 [D loss: 0.683082, acc.: 57.03%] [G loss: 1.145189]\n",
      "epoch:4 step:4025 [D loss: 0.577844, acc.: 67.19%] [G loss: 1.068425]\n",
      "epoch:4 step:4026 [D loss: 0.585421, acc.: 68.75%] [G loss: 1.196630]\n",
      "epoch:4 step:4027 [D loss: 0.730262, acc.: 54.69%] [G loss: 1.177976]\n",
      "epoch:4 step:4028 [D loss: 0.661568, acc.: 58.59%] [G loss: 1.037082]\n",
      "epoch:4 step:4029 [D loss: 0.644817, acc.: 60.94%] [G loss: 0.909848]\n",
      "epoch:4 step:4030 [D loss: 0.648261, acc.: 60.94%] [G loss: 0.997143]\n",
      "epoch:4 step:4031 [D loss: 0.655327, acc.: 64.06%] [G loss: 1.151703]\n",
      "epoch:4 step:4032 [D loss: 0.671688, acc.: 63.28%] [G loss: 1.154795]\n",
      "epoch:4 step:4033 [D loss: 0.613690, acc.: 67.19%] [G loss: 1.132529]\n",
      "epoch:4 step:4034 [D loss: 0.681900, acc.: 60.16%] [G loss: 0.900411]\n",
      "epoch:4 step:4035 [D loss: 0.620044, acc.: 62.50%] [G loss: 0.971056]\n",
      "epoch:4 step:4036 [D loss: 0.658422, acc.: 60.16%] [G loss: 0.946258]\n",
      "epoch:4 step:4037 [D loss: 0.690934, acc.: 53.91%] [G loss: 1.147902]\n",
      "epoch:4 step:4038 [D loss: 0.647166, acc.: 60.94%] [G loss: 1.040035]\n",
      "epoch:4 step:4039 [D loss: 0.666083, acc.: 60.16%] [G loss: 1.015694]\n",
      "epoch:4 step:4040 [D loss: 0.678928, acc.: 64.06%] [G loss: 1.043470]\n",
      "epoch:4 step:4041 [D loss: 0.605131, acc.: 64.84%] [G loss: 0.954871]\n",
      "epoch:4 step:4042 [D loss: 0.616432, acc.: 64.06%] [G loss: 0.937711]\n",
      "epoch:4 step:4043 [D loss: 0.688508, acc.: 57.03%] [G loss: 1.089213]\n",
      "epoch:4 step:4044 [D loss: 0.611367, acc.: 65.62%] [G loss: 1.087269]\n",
      "epoch:4 step:4045 [D loss: 0.602049, acc.: 66.41%] [G loss: 1.091389]\n",
      "epoch:4 step:4046 [D loss: 0.646111, acc.: 62.50%] [G loss: 1.173169]\n",
      "epoch:4 step:4047 [D loss: 0.662188, acc.: 60.94%] [G loss: 1.169822]\n",
      "epoch:4 step:4048 [D loss: 0.616862, acc.: 65.62%] [G loss: 1.155116]\n",
      "epoch:4 step:4049 [D loss: 0.681761, acc.: 63.28%] [G loss: 1.172365]\n",
      "epoch:4 step:4050 [D loss: 0.642264, acc.: 65.62%] [G loss: 0.943922]\n",
      "epoch:4 step:4051 [D loss: 0.713758, acc.: 53.91%] [G loss: 1.106673]\n",
      "epoch:4 step:4052 [D loss: 0.671758, acc.: 61.72%] [G loss: 1.182046]\n",
      "epoch:4 step:4053 [D loss: 0.743778, acc.: 52.34%] [G loss: 1.273359]\n",
      "epoch:4 step:4054 [D loss: 0.645461, acc.: 64.06%] [G loss: 1.248210]\n",
      "epoch:4 step:4055 [D loss: 0.688834, acc.: 57.81%] [G loss: 1.066453]\n",
      "epoch:4 step:4056 [D loss: 0.752343, acc.: 46.88%] [G loss: 1.177611]\n",
      "epoch:4 step:4057 [D loss: 0.548320, acc.: 72.66%] [G loss: 1.065306]\n",
      "epoch:4 step:4058 [D loss: 0.615208, acc.: 66.41%] [G loss: 1.056511]\n",
      "epoch:4 step:4059 [D loss: 0.639244, acc.: 65.62%] [G loss: 1.062351]\n",
      "epoch:4 step:4060 [D loss: 0.723526, acc.: 57.81%] [G loss: 1.034813]\n",
      "epoch:4 step:4061 [D loss: 0.681013, acc.: 54.69%] [G loss: 1.101679]\n",
      "epoch:4 step:4062 [D loss: 0.549135, acc.: 75.78%] [G loss: 1.048207]\n",
      "epoch:4 step:4063 [D loss: 0.691905, acc.: 58.59%] [G loss: 1.008260]\n",
      "epoch:4 step:4064 [D loss: 0.621843, acc.: 63.28%] [G loss: 0.978049]\n",
      "epoch:4 step:4065 [D loss: 0.668406, acc.: 60.94%] [G loss: 1.105241]\n",
      "epoch:4 step:4066 [D loss: 0.684446, acc.: 60.94%] [G loss: 1.044540]\n",
      "epoch:4 step:4067 [D loss: 0.633691, acc.: 64.84%] [G loss: 0.965504]\n",
      "epoch:4 step:4068 [D loss: 0.679586, acc.: 57.03%] [G loss: 1.056003]\n",
      "epoch:4 step:4069 [D loss: 0.739738, acc.: 47.66%] [G loss: 0.951804]\n",
      "epoch:4 step:4070 [D loss: 0.620008, acc.: 65.62%] [G loss: 1.025619]\n",
      "epoch:4 step:4071 [D loss: 0.674249, acc.: 61.72%] [G loss: 1.024633]\n",
      "epoch:4 step:4072 [D loss: 0.616230, acc.: 62.50%] [G loss: 0.955716]\n",
      "epoch:4 step:4073 [D loss: 0.520740, acc.: 75.78%] [G loss: 1.155572]\n",
      "epoch:4 step:4074 [D loss: 0.650939, acc.: 61.72%] [G loss: 1.119686]\n",
      "epoch:4 step:4075 [D loss: 0.620277, acc.: 63.28%] [G loss: 0.926926]\n",
      "epoch:4 step:4076 [D loss: 0.485102, acc.: 85.16%] [G loss: 1.208720]\n",
      "epoch:4 step:4077 [D loss: 0.757484, acc.: 56.25%] [G loss: 1.178074]\n",
      "epoch:4 step:4078 [D loss: 0.731125, acc.: 53.12%] [G loss: 0.854340]\n",
      "epoch:4 step:4079 [D loss: 0.659354, acc.: 64.84%] [G loss: 1.008647]\n",
      "epoch:4 step:4080 [D loss: 0.589289, acc.: 67.97%] [G loss: 0.994882]\n",
      "epoch:4 step:4081 [D loss: 0.704063, acc.: 58.59%] [G loss: 0.987429]\n",
      "epoch:4 step:4082 [D loss: 0.616297, acc.: 64.84%] [G loss: 0.952916]\n",
      "epoch:4 step:4083 [D loss: 0.619017, acc.: 64.84%] [G loss: 1.115760]\n",
      "epoch:4 step:4084 [D loss: 0.608729, acc.: 71.09%] [G loss: 1.142138]\n",
      "epoch:4 step:4085 [D loss: 0.594876, acc.: 68.75%] [G loss: 1.222177]\n",
      "epoch:4 step:4086 [D loss: 0.578797, acc.: 68.75%] [G loss: 1.167480]\n",
      "epoch:4 step:4087 [D loss: 0.702718, acc.: 58.59%] [G loss: 1.007888]\n",
      "epoch:4 step:4088 [D loss: 0.662762, acc.: 54.69%] [G loss: 0.983492]\n",
      "epoch:4 step:4089 [D loss: 0.682540, acc.: 53.12%] [G loss: 0.937747]\n",
      "epoch:4 step:4090 [D loss: 0.714605, acc.: 53.12%] [G loss: 0.878917]\n",
      "epoch:4 step:4091 [D loss: 0.620632, acc.: 62.50%] [G loss: 1.218321]\n",
      "epoch:4 step:4092 [D loss: 0.685090, acc.: 59.38%] [G loss: 1.003804]\n",
      "epoch:4 step:4093 [D loss: 0.713222, acc.: 54.69%] [G loss: 0.911784]\n",
      "epoch:4 step:4094 [D loss: 0.646619, acc.: 65.62%] [G loss: 0.947164]\n",
      "epoch:4 step:4095 [D loss: 0.657475, acc.: 58.59%] [G loss: 0.915283]\n",
      "epoch:4 step:4096 [D loss: 0.706874, acc.: 57.81%] [G loss: 1.014746]\n",
      "epoch:4 step:4097 [D loss: 0.601106, acc.: 64.06%] [G loss: 0.960946]\n",
      "epoch:4 step:4098 [D loss: 0.592686, acc.: 66.41%] [G loss: 1.112088]\n",
      "epoch:4 step:4099 [D loss: 0.688522, acc.: 57.03%] [G loss: 1.060905]\n",
      "epoch:4 step:4100 [D loss: 0.604843, acc.: 68.75%] [G loss: 1.062980]\n",
      "epoch:4 step:4101 [D loss: 0.656373, acc.: 56.25%] [G loss: 0.914319]\n",
      "epoch:4 step:4102 [D loss: 0.619815, acc.: 63.28%] [G loss: 1.067109]\n",
      "epoch:4 step:4103 [D loss: 0.584706, acc.: 72.66%] [G loss: 1.021822]\n",
      "epoch:4 step:4104 [D loss: 0.667932, acc.: 60.16%] [G loss: 1.363320]\n",
      "epoch:4 step:4105 [D loss: 0.668150, acc.: 63.28%] [G loss: 1.111819]\n",
      "epoch:4 step:4106 [D loss: 0.669876, acc.: 59.38%] [G loss: 1.076095]\n",
      "epoch:4 step:4107 [D loss: 0.603316, acc.: 65.62%] [G loss: 1.150995]\n",
      "epoch:4 step:4108 [D loss: 0.704193, acc.: 57.81%] [G loss: 0.950578]\n",
      "epoch:4 step:4109 [D loss: 0.558449, acc.: 72.66%] [G loss: 1.170056]\n",
      "epoch:4 step:4110 [D loss: 0.704206, acc.: 60.16%] [G loss: 1.073408]\n",
      "epoch:4 step:4111 [D loss: 0.666596, acc.: 60.16%] [G loss: 0.986409]\n",
      "epoch:4 step:4112 [D loss: 0.695316, acc.: 56.25%] [G loss: 1.100440]\n",
      "epoch:4 step:4113 [D loss: 0.584611, acc.: 70.31%] [G loss: 1.248570]\n",
      "epoch:4 step:4114 [D loss: 0.623911, acc.: 64.84%] [G loss: 0.936477]\n",
      "epoch:4 step:4115 [D loss: 0.709861, acc.: 57.03%] [G loss: 1.124097]\n",
      "epoch:4 step:4116 [D loss: 0.622041, acc.: 65.62%] [G loss: 1.030115]\n",
      "epoch:4 step:4117 [D loss: 0.576244, acc.: 73.44%] [G loss: 1.127202]\n",
      "epoch:4 step:4118 [D loss: 0.595286, acc.: 67.97%] [G loss: 0.937748]\n",
      "epoch:4 step:4119 [D loss: 0.660559, acc.: 57.81%] [G loss: 1.189478]\n",
      "epoch:4 step:4120 [D loss: 0.653018, acc.: 59.38%] [G loss: 0.985689]\n",
      "epoch:4 step:4121 [D loss: 0.587381, acc.: 70.31%] [G loss: 1.288466]\n",
      "epoch:4 step:4122 [D loss: 0.711755, acc.: 50.78%] [G loss: 1.170704]\n",
      "epoch:4 step:4123 [D loss: 0.822526, acc.: 47.66%] [G loss: 0.920597]\n",
      "epoch:4 step:4124 [D loss: 0.719897, acc.: 53.12%] [G loss: 0.927599]\n",
      "epoch:4 step:4125 [D loss: 0.673059, acc.: 57.03%] [G loss: 0.973199]\n",
      "epoch:4 step:4126 [D loss: 0.629221, acc.: 67.97%] [G loss: 1.105196]\n",
      "epoch:4 step:4127 [D loss: 0.691240, acc.: 55.47%] [G loss: 1.117795]\n",
      "epoch:4 step:4128 [D loss: 0.675300, acc.: 58.59%] [G loss: 1.100025]\n",
      "epoch:4 step:4129 [D loss: 0.648071, acc.: 60.16%] [G loss: 0.977166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4130 [D loss: 0.564726, acc.: 74.22%] [G loss: 1.191858]\n",
      "epoch:4 step:4131 [D loss: 0.616877, acc.: 68.75%] [G loss: 1.036927]\n",
      "epoch:4 step:4132 [D loss: 0.637251, acc.: 67.97%] [G loss: 1.006715]\n",
      "epoch:4 step:4133 [D loss: 0.634332, acc.: 63.28%] [G loss: 1.206225]\n",
      "epoch:4 step:4134 [D loss: 0.632441, acc.: 64.06%] [G loss: 1.150183]\n",
      "epoch:4 step:4135 [D loss: 0.639581, acc.: 65.62%] [G loss: 1.180005]\n",
      "epoch:4 step:4136 [D loss: 0.696044, acc.: 55.47%] [G loss: 1.098654]\n",
      "epoch:4 step:4137 [D loss: 0.627017, acc.: 67.19%] [G loss: 0.955298]\n",
      "epoch:4 step:4138 [D loss: 0.590379, acc.: 67.19%] [G loss: 1.005786]\n",
      "epoch:4 step:4139 [D loss: 0.621904, acc.: 67.97%] [G loss: 0.990046]\n",
      "epoch:4 step:4140 [D loss: 0.625243, acc.: 67.19%] [G loss: 0.908712]\n",
      "epoch:4 step:4141 [D loss: 0.692115, acc.: 60.94%] [G loss: 1.010833]\n",
      "epoch:4 step:4142 [D loss: 0.616006, acc.: 63.28%] [G loss: 1.198549]\n",
      "epoch:4 step:4143 [D loss: 0.587473, acc.: 67.19%] [G loss: 0.999792]\n",
      "epoch:4 step:4144 [D loss: 0.579008, acc.: 71.09%] [G loss: 1.232109]\n",
      "epoch:4 step:4145 [D loss: 0.748533, acc.: 52.34%] [G loss: 0.916773]\n",
      "epoch:4 step:4146 [D loss: 0.602275, acc.: 64.84%] [G loss: 1.109448]\n",
      "epoch:4 step:4147 [D loss: 0.592466, acc.: 70.31%] [G loss: 1.063653]\n",
      "epoch:4 step:4148 [D loss: 0.617666, acc.: 64.84%] [G loss: 1.033123]\n",
      "epoch:4 step:4149 [D loss: 0.775774, acc.: 47.66%] [G loss: 1.123483]\n",
      "epoch:4 step:4150 [D loss: 0.608546, acc.: 65.62%] [G loss: 1.046081]\n",
      "epoch:4 step:4151 [D loss: 0.639147, acc.: 63.28%] [G loss: 1.072046]\n",
      "epoch:4 step:4152 [D loss: 0.696624, acc.: 56.25%] [G loss: 0.979396]\n",
      "epoch:4 step:4153 [D loss: 0.587948, acc.: 74.22%] [G loss: 1.019558]\n",
      "epoch:4 step:4154 [D loss: 0.563129, acc.: 69.53%] [G loss: 1.055576]\n",
      "epoch:4 step:4155 [D loss: 0.636300, acc.: 67.97%] [G loss: 1.288879]\n",
      "epoch:4 step:4156 [D loss: 0.649933, acc.: 61.72%] [G loss: 1.094744]\n",
      "epoch:4 step:4157 [D loss: 0.678222, acc.: 60.94%] [G loss: 1.062444]\n",
      "epoch:4 step:4158 [D loss: 0.699122, acc.: 53.91%] [G loss: 1.023309]\n",
      "epoch:4 step:4159 [D loss: 0.621636, acc.: 65.62%] [G loss: 0.950071]\n",
      "epoch:4 step:4160 [D loss: 0.717985, acc.: 55.47%] [G loss: 1.019769]\n",
      "epoch:4 step:4161 [D loss: 0.736522, acc.: 54.69%] [G loss: 0.967287]\n",
      "epoch:4 step:4162 [D loss: 0.558438, acc.: 74.22%] [G loss: 1.113247]\n",
      "epoch:4 step:4163 [D loss: 0.553277, acc.: 69.53%] [G loss: 1.250184]\n",
      "epoch:4 step:4164 [D loss: 0.714527, acc.: 52.34%] [G loss: 0.899108]\n",
      "epoch:4 step:4165 [D loss: 0.598841, acc.: 67.19%] [G loss: 1.113166]\n",
      "epoch:4 step:4166 [D loss: 0.652493, acc.: 57.03%] [G loss: 1.165725]\n",
      "epoch:4 step:4167 [D loss: 0.683240, acc.: 56.25%] [G loss: 1.027116]\n",
      "epoch:4 step:4168 [D loss: 0.591675, acc.: 65.62%] [G loss: 1.102085]\n",
      "epoch:4 step:4169 [D loss: 0.608719, acc.: 65.62%] [G loss: 1.106829]\n",
      "epoch:4 step:4170 [D loss: 0.633685, acc.: 64.06%] [G loss: 1.091934]\n",
      "epoch:4 step:4171 [D loss: 0.613189, acc.: 65.62%] [G loss: 1.062471]\n",
      "epoch:4 step:4172 [D loss: 0.577650, acc.: 71.09%] [G loss: 1.096219]\n",
      "epoch:4 step:4173 [D loss: 0.653316, acc.: 64.06%] [G loss: 0.979699]\n",
      "epoch:4 step:4174 [D loss: 0.700347, acc.: 57.03%] [G loss: 1.145247]\n",
      "epoch:4 step:4175 [D loss: 0.660138, acc.: 55.47%] [G loss: 1.083669]\n",
      "epoch:4 step:4176 [D loss: 0.578529, acc.: 71.09%] [G loss: 0.994544]\n",
      "epoch:4 step:4177 [D loss: 0.648751, acc.: 60.16%] [G loss: 1.086516]\n",
      "epoch:4 step:4178 [D loss: 0.736847, acc.: 50.00%] [G loss: 1.034032]\n",
      "epoch:4 step:4179 [D loss: 0.610000, acc.: 64.84%] [G loss: 1.075181]\n",
      "epoch:4 step:4180 [D loss: 0.649656, acc.: 60.94%] [G loss: 0.986089]\n",
      "epoch:4 step:4181 [D loss: 0.623608, acc.: 64.84%] [G loss: 1.161066]\n",
      "epoch:4 step:4182 [D loss: 0.710905, acc.: 57.03%] [G loss: 1.047763]\n",
      "epoch:4 step:4183 [D loss: 0.607778, acc.: 67.19%] [G loss: 1.094253]\n",
      "epoch:4 step:4184 [D loss: 0.615463, acc.: 65.62%] [G loss: 0.993389]\n",
      "epoch:4 step:4185 [D loss: 0.614827, acc.: 67.19%] [G loss: 0.947665]\n",
      "epoch:4 step:4186 [D loss: 0.714221, acc.: 56.25%] [G loss: 0.980954]\n",
      "epoch:4 step:4187 [D loss: 0.591961, acc.: 71.88%] [G loss: 0.949849]\n",
      "epoch:4 step:4188 [D loss: 0.568872, acc.: 70.31%] [G loss: 1.164778]\n",
      "epoch:4 step:4189 [D loss: 0.634351, acc.: 66.41%] [G loss: 1.150830]\n",
      "epoch:4 step:4190 [D loss: 0.713054, acc.: 55.47%] [G loss: 1.108891]\n",
      "epoch:4 step:4191 [D loss: 0.605130, acc.: 67.19%] [G loss: 0.917578]\n",
      "epoch:4 step:4192 [D loss: 0.683911, acc.: 61.72%] [G loss: 1.043490]\n",
      "epoch:4 step:4193 [D loss: 0.510129, acc.: 78.91%] [G loss: 0.932520]\n",
      "epoch:4 step:4194 [D loss: 0.660042, acc.: 61.72%] [G loss: 1.042623]\n",
      "epoch:4 step:4195 [D loss: 0.621297, acc.: 66.41%] [G loss: 1.109263]\n",
      "epoch:4 step:4196 [D loss: 0.587225, acc.: 69.53%] [G loss: 1.187815]\n",
      "epoch:4 step:4197 [D loss: 0.573077, acc.: 71.09%] [G loss: 0.995756]\n",
      "epoch:4 step:4198 [D loss: 0.576179, acc.: 70.31%] [G loss: 1.055860]\n",
      "epoch:4 step:4199 [D loss: 0.596025, acc.: 71.88%] [G loss: 1.187004]\n",
      "epoch:4 step:4200 [D loss: 0.596764, acc.: 70.31%] [G loss: 0.988140]\n",
      "##############\n",
      "[2.68281042 1.88769631 1.91780716 3.08383108 0.814556   7.14124345\n",
      " 1.80845322 3.11683238 3.83107002 5.43713805]\n",
      "##########\n",
      "epoch:4 step:4201 [D loss: 0.576975, acc.: 67.19%] [G loss: 1.157239]\n",
      "epoch:4 step:4202 [D loss: 0.678104, acc.: 53.12%] [G loss: 1.033817]\n",
      "epoch:4 step:4203 [D loss: 0.658422, acc.: 56.25%] [G loss: 0.982007]\n",
      "epoch:4 step:4204 [D loss: 0.660898, acc.: 60.94%] [G loss: 0.984984]\n",
      "epoch:4 step:4205 [D loss: 0.717184, acc.: 50.78%] [G loss: 1.048051]\n",
      "epoch:4 step:4206 [D loss: 0.630665, acc.: 64.06%] [G loss: 0.979818]\n",
      "epoch:4 step:4207 [D loss: 0.675470, acc.: 64.84%] [G loss: 0.869459]\n",
      "epoch:4 step:4208 [D loss: 0.595695, acc.: 67.19%] [G loss: 1.195516]\n",
      "epoch:4 step:4209 [D loss: 0.770229, acc.: 47.66%] [G loss: 1.034234]\n",
      "epoch:4 step:4210 [D loss: 0.638188, acc.: 64.06%] [G loss: 1.090229]\n",
      "epoch:4 step:4211 [D loss: 0.670864, acc.: 61.72%] [G loss: 1.072601]\n",
      "epoch:4 step:4212 [D loss: 0.534356, acc.: 75.00%] [G loss: 1.160339]\n",
      "epoch:4 step:4213 [D loss: 0.614917, acc.: 69.53%] [G loss: 1.044882]\n",
      "epoch:4 step:4214 [D loss: 0.652151, acc.: 57.81%] [G loss: 1.158838]\n",
      "epoch:4 step:4215 [D loss: 0.622655, acc.: 63.28%] [G loss: 1.018089]\n",
      "epoch:4 step:4216 [D loss: 0.634507, acc.: 57.81%] [G loss: 1.046598]\n",
      "epoch:4 step:4217 [D loss: 0.740616, acc.: 52.34%] [G loss: 1.036978]\n",
      "epoch:4 step:4218 [D loss: 0.715859, acc.: 57.81%] [G loss: 0.933158]\n",
      "epoch:4 step:4219 [D loss: 0.568226, acc.: 70.31%] [G loss: 1.087447]\n",
      "epoch:4 step:4220 [D loss: 0.634190, acc.: 62.50%] [G loss: 1.112854]\n",
      "epoch:4 step:4221 [D loss: 0.630225, acc.: 62.50%] [G loss: 1.061786]\n",
      "epoch:4 step:4222 [D loss: 0.561302, acc.: 71.88%] [G loss: 1.139478]\n",
      "epoch:4 step:4223 [D loss: 0.541509, acc.: 71.09%] [G loss: 1.031154]\n",
      "epoch:4 step:4224 [D loss: 0.550885, acc.: 74.22%] [G loss: 1.342166]\n",
      "epoch:4 step:4225 [D loss: 0.670994, acc.: 62.50%] [G loss: 1.130551]\n",
      "epoch:4 step:4226 [D loss: 0.686611, acc.: 57.03%] [G loss: 0.985274]\n",
      "epoch:4 step:4227 [D loss: 0.620592, acc.: 64.84%] [G loss: 1.116143]\n",
      "epoch:4 step:4228 [D loss: 0.589237, acc.: 67.97%] [G loss: 1.120209]\n",
      "epoch:4 step:4229 [D loss: 0.678543, acc.: 60.16%] [G loss: 0.982747]\n",
      "epoch:4 step:4230 [D loss: 0.672214, acc.: 58.59%] [G loss: 1.077442]\n",
      "epoch:4 step:4231 [D loss: 0.692625, acc.: 57.03%] [G loss: 0.952890]\n",
      "epoch:4 step:4232 [D loss: 0.620323, acc.: 67.97%] [G loss: 1.114973]\n",
      "epoch:4 step:4233 [D loss: 0.640227, acc.: 63.28%] [G loss: 1.021767]\n",
      "epoch:4 step:4234 [D loss: 0.619992, acc.: 67.19%] [G loss: 1.077718]\n",
      "epoch:4 step:4235 [D loss: 0.603069, acc.: 67.19%] [G loss: 1.238193]\n",
      "epoch:4 step:4236 [D loss: 0.597127, acc.: 66.41%] [G loss: 1.114014]\n",
      "epoch:4 step:4237 [D loss: 0.668249, acc.: 61.72%] [G loss: 0.929478]\n",
      "epoch:4 step:4238 [D loss: 0.643851, acc.: 62.50%] [G loss: 1.116250]\n",
      "epoch:4 step:4239 [D loss: 0.544146, acc.: 75.78%] [G loss: 1.017022]\n",
      "epoch:4 step:4240 [D loss: 0.826699, acc.: 39.84%] [G loss: 0.866307]\n",
      "epoch:4 step:4241 [D loss: 0.656375, acc.: 59.38%] [G loss: 1.138269]\n",
      "epoch:4 step:4242 [D loss: 0.517951, acc.: 77.34%] [G loss: 1.060026]\n",
      "epoch:4 step:4243 [D loss: 0.619952, acc.: 66.41%] [G loss: 1.054322]\n",
      "epoch:4 step:4244 [D loss: 0.684511, acc.: 56.25%] [G loss: 0.927893]\n",
      "epoch:4 step:4245 [D loss: 0.669262, acc.: 60.16%] [G loss: 1.154022]\n",
      "epoch:4 step:4246 [D loss: 0.637712, acc.: 63.28%] [G loss: 0.970463]\n",
      "epoch:4 step:4247 [D loss: 0.637769, acc.: 62.50%] [G loss: 1.077974]\n",
      "epoch:4 step:4248 [D loss: 0.687033, acc.: 58.59%] [G loss: 1.106807]\n",
      "epoch:4 step:4249 [D loss: 0.689675, acc.: 55.47%] [G loss: 1.027885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4250 [D loss: 0.560439, acc.: 73.44%] [G loss: 1.182027]\n",
      "epoch:4 step:4251 [D loss: 0.580704, acc.: 68.75%] [G loss: 1.152832]\n",
      "epoch:4 step:4252 [D loss: 0.780097, acc.: 46.09%] [G loss: 1.054037]\n",
      "epoch:4 step:4253 [D loss: 0.618742, acc.: 63.28%] [G loss: 1.079256]\n",
      "epoch:4 step:4254 [D loss: 0.651105, acc.: 57.81%] [G loss: 1.126857]\n",
      "epoch:4 step:4255 [D loss: 0.609455, acc.: 68.75%] [G loss: 1.030745]\n",
      "epoch:4 step:4256 [D loss: 0.669860, acc.: 56.25%] [G loss: 1.203044]\n",
      "epoch:4 step:4257 [D loss: 0.668493, acc.: 56.25%] [G loss: 1.163489]\n",
      "epoch:4 step:4258 [D loss: 0.567365, acc.: 71.88%] [G loss: 1.097779]\n",
      "epoch:4 step:4259 [D loss: 0.624902, acc.: 66.41%] [G loss: 1.109873]\n",
      "epoch:4 step:4260 [D loss: 0.653069, acc.: 60.94%] [G loss: 0.908060]\n",
      "epoch:4 step:4261 [D loss: 0.698761, acc.: 57.03%] [G loss: 1.088794]\n",
      "epoch:4 step:4262 [D loss: 0.621827, acc.: 65.62%] [G loss: 1.065861]\n",
      "epoch:4 step:4263 [D loss: 0.663019, acc.: 64.06%] [G loss: 1.134999]\n",
      "epoch:4 step:4264 [D loss: 0.653151, acc.: 61.72%] [G loss: 1.140113]\n",
      "epoch:4 step:4265 [D loss: 0.626635, acc.: 67.19%] [G loss: 1.148545]\n",
      "epoch:4 step:4266 [D loss: 0.756163, acc.: 48.44%] [G loss: 0.945519]\n",
      "epoch:4 step:4267 [D loss: 0.570445, acc.: 73.44%] [G loss: 1.110863]\n",
      "epoch:4 step:4268 [D loss: 0.569965, acc.: 70.31%] [G loss: 1.039397]\n",
      "epoch:4 step:4269 [D loss: 0.664249, acc.: 54.69%] [G loss: 0.940853]\n",
      "epoch:4 step:4270 [D loss: 0.646744, acc.: 64.06%] [G loss: 1.092280]\n",
      "epoch:4 step:4271 [D loss: 0.645480, acc.: 64.84%] [G loss: 1.193185]\n",
      "epoch:4 step:4272 [D loss: 0.602109, acc.: 70.31%] [G loss: 1.020460]\n",
      "epoch:4 step:4273 [D loss: 0.593974, acc.: 68.75%] [G loss: 1.136610]\n",
      "epoch:4 step:4274 [D loss: 0.604622, acc.: 65.62%] [G loss: 1.061163]\n",
      "epoch:4 step:4275 [D loss: 0.684345, acc.: 58.59%] [G loss: 0.989990]\n",
      "epoch:4 step:4276 [D loss: 0.744148, acc.: 55.47%] [G loss: 0.978666]\n",
      "epoch:4 step:4277 [D loss: 0.620313, acc.: 65.62%] [G loss: 0.876974]\n",
      "epoch:4 step:4278 [D loss: 0.630138, acc.: 64.06%] [G loss: 1.108384]\n",
      "epoch:4 step:4279 [D loss: 0.594273, acc.: 67.19%] [G loss: 1.033873]\n",
      "epoch:4 step:4280 [D loss: 0.599075, acc.: 65.62%] [G loss: 1.148817]\n",
      "epoch:4 step:4281 [D loss: 0.619379, acc.: 67.19%] [G loss: 0.979864]\n",
      "epoch:4 step:4282 [D loss: 0.613594, acc.: 65.62%] [G loss: 1.068667]\n",
      "epoch:4 step:4283 [D loss: 0.623638, acc.: 65.62%] [G loss: 1.141651]\n",
      "epoch:4 step:4284 [D loss: 0.618398, acc.: 63.28%] [G loss: 1.124011]\n",
      "epoch:4 step:4285 [D loss: 0.649838, acc.: 60.94%] [G loss: 0.951611]\n",
      "epoch:4 step:4286 [D loss: 0.637577, acc.: 66.41%] [G loss: 1.108908]\n",
      "epoch:4 step:4287 [D loss: 0.577430, acc.: 64.84%] [G loss: 1.025112]\n",
      "epoch:4 step:4288 [D loss: 0.628645, acc.: 59.38%] [G loss: 1.258125]\n",
      "epoch:4 step:4289 [D loss: 0.588522, acc.: 71.88%] [G loss: 1.271581]\n",
      "epoch:4 step:4290 [D loss: 0.705452, acc.: 53.12%] [G loss: 1.186056]\n",
      "epoch:4 step:4291 [D loss: 0.553952, acc.: 73.44%] [G loss: 1.163301]\n",
      "epoch:4 step:4292 [D loss: 0.615031, acc.: 68.75%] [G loss: 0.904532]\n",
      "epoch:4 step:4293 [D loss: 0.608246, acc.: 67.19%] [G loss: 1.216868]\n",
      "epoch:4 step:4294 [D loss: 0.591530, acc.: 67.97%] [G loss: 1.223497]\n",
      "epoch:4 step:4295 [D loss: 0.613797, acc.: 71.09%] [G loss: 1.044028]\n",
      "epoch:4 step:4296 [D loss: 0.724854, acc.: 57.03%] [G loss: 1.092857]\n",
      "epoch:4 step:4297 [D loss: 0.623668, acc.: 62.50%] [G loss: 1.199296]\n",
      "epoch:4 step:4298 [D loss: 0.555634, acc.: 69.53%] [G loss: 1.022524]\n",
      "epoch:4 step:4299 [D loss: 0.632657, acc.: 64.06%] [G loss: 1.144193]\n",
      "epoch:4 step:4300 [D loss: 0.682561, acc.: 54.69%] [G loss: 1.034085]\n",
      "epoch:4 step:4301 [D loss: 0.688143, acc.: 60.16%] [G loss: 0.919258]\n",
      "epoch:4 step:4302 [D loss: 0.595590, acc.: 67.19%] [G loss: 1.260360]\n",
      "epoch:4 step:4303 [D loss: 0.614803, acc.: 58.59%] [G loss: 1.117757]\n",
      "epoch:4 step:4304 [D loss: 0.679836, acc.: 54.69%] [G loss: 1.029479]\n",
      "epoch:4 step:4305 [D loss: 0.656381, acc.: 64.84%] [G loss: 1.121447]\n",
      "epoch:4 step:4306 [D loss: 0.600588, acc.: 65.62%] [G loss: 1.226753]\n",
      "epoch:4 step:4307 [D loss: 0.539380, acc.: 74.22%] [G loss: 1.236628]\n",
      "epoch:4 step:4308 [D loss: 0.600503, acc.: 65.62%] [G loss: 1.045256]\n",
      "epoch:4 step:4309 [D loss: 0.726913, acc.: 56.25%] [G loss: 1.038378]\n",
      "epoch:4 step:4310 [D loss: 0.576997, acc.: 72.66%] [G loss: 1.225270]\n",
      "epoch:4 step:4311 [D loss: 0.611818, acc.: 64.84%] [G loss: 1.038542]\n",
      "epoch:4 step:4312 [D loss: 0.644728, acc.: 62.50%] [G loss: 1.110473]\n",
      "epoch:4 step:4313 [D loss: 0.703284, acc.: 53.91%] [G loss: 1.119143]\n",
      "epoch:4 step:4314 [D loss: 0.626392, acc.: 61.72%] [G loss: 1.261153]\n",
      "epoch:4 step:4315 [D loss: 0.697065, acc.: 60.16%] [G loss: 1.176278]\n",
      "epoch:4 step:4316 [D loss: 0.649928, acc.: 63.28%] [G loss: 1.082820]\n",
      "epoch:4 step:4317 [D loss: 0.558444, acc.: 77.34%] [G loss: 1.159818]\n",
      "epoch:4 step:4318 [D loss: 0.685613, acc.: 59.38%] [G loss: 0.914737]\n",
      "epoch:4 step:4319 [D loss: 0.505863, acc.: 80.47%] [G loss: 1.058462]\n",
      "epoch:4 step:4320 [D loss: 0.495070, acc.: 78.91%] [G loss: 1.237740]\n",
      "epoch:4 step:4321 [D loss: 0.724576, acc.: 54.69%] [G loss: 0.824774]\n",
      "epoch:4 step:4322 [D loss: 0.620288, acc.: 64.06%] [G loss: 1.191747]\n",
      "epoch:4 step:4323 [D loss: 0.701108, acc.: 51.56%] [G loss: 0.913786]\n",
      "epoch:4 step:4324 [D loss: 0.583474, acc.: 69.53%] [G loss: 0.991736]\n",
      "epoch:4 step:4325 [D loss: 0.605917, acc.: 64.06%] [G loss: 1.123493]\n",
      "epoch:4 step:4326 [D loss: 0.672813, acc.: 60.94%] [G loss: 0.967104]\n",
      "epoch:4 step:4327 [D loss: 0.599982, acc.: 66.41%] [G loss: 1.107787]\n",
      "epoch:4 step:4328 [D loss: 0.560334, acc.: 71.88%] [G loss: 1.014940]\n",
      "epoch:4 step:4329 [D loss: 0.647149, acc.: 65.62%] [G loss: 1.209491]\n",
      "epoch:4 step:4330 [D loss: 0.624302, acc.: 64.84%] [G loss: 1.107226]\n",
      "epoch:4 step:4331 [D loss: 0.640494, acc.: 66.41%] [G loss: 1.055525]\n",
      "epoch:4 step:4332 [D loss: 0.589198, acc.: 68.75%] [G loss: 0.919321]\n",
      "epoch:4 step:4333 [D loss: 0.618156, acc.: 67.97%] [G loss: 0.976129]\n",
      "epoch:4 step:4334 [D loss: 0.611117, acc.: 65.62%] [G loss: 1.121520]\n",
      "epoch:4 step:4335 [D loss: 0.650410, acc.: 63.28%] [G loss: 0.879920]\n",
      "epoch:4 step:4336 [D loss: 0.652997, acc.: 63.28%] [G loss: 1.017051]\n",
      "epoch:4 step:4337 [D loss: 0.530253, acc.: 74.22%] [G loss: 1.305747]\n",
      "epoch:4 step:4338 [D loss: 0.739861, acc.: 56.25%] [G loss: 1.084591]\n",
      "epoch:4 step:4339 [D loss: 0.560575, acc.: 75.00%] [G loss: 1.130570]\n",
      "epoch:4 step:4340 [D loss: 0.648105, acc.: 66.41%] [G loss: 1.082728]\n",
      "epoch:4 step:4341 [D loss: 0.786764, acc.: 48.44%] [G loss: 0.794736]\n",
      "epoch:4 step:4342 [D loss: 0.556641, acc.: 69.53%] [G loss: 1.041960]\n",
      "epoch:4 step:4343 [D loss: 0.724117, acc.: 50.00%] [G loss: 1.010212]\n",
      "epoch:4 step:4344 [D loss: 0.645219, acc.: 60.94%] [G loss: 1.033897]\n",
      "epoch:4 step:4345 [D loss: 0.741255, acc.: 52.34%] [G loss: 0.922737]\n",
      "epoch:4 step:4346 [D loss: 0.727269, acc.: 51.56%] [G loss: 1.064054]\n",
      "epoch:4 step:4347 [D loss: 0.639237, acc.: 66.41%] [G loss: 1.136800]\n",
      "epoch:4 step:4348 [D loss: 0.570797, acc.: 75.00%] [G loss: 0.940755]\n",
      "epoch:4 step:4349 [D loss: 0.648556, acc.: 62.50%] [G loss: 1.194332]\n",
      "epoch:4 step:4350 [D loss: 0.672027, acc.: 58.59%] [G loss: 1.143952]\n",
      "epoch:4 step:4351 [D loss: 0.606540, acc.: 65.62%] [G loss: 0.968822]\n",
      "epoch:4 step:4352 [D loss: 0.711045, acc.: 53.12%] [G loss: 0.971208]\n",
      "epoch:4 step:4353 [D loss: 0.724140, acc.: 53.12%] [G loss: 0.824935]\n",
      "epoch:4 step:4354 [D loss: 0.667491, acc.: 59.38%] [G loss: 1.019675]\n",
      "epoch:4 step:4355 [D loss: 0.694336, acc.: 55.47%] [G loss: 0.982422]\n",
      "epoch:4 step:4356 [D loss: 0.589857, acc.: 69.53%] [G loss: 1.183636]\n",
      "epoch:4 step:4357 [D loss: 0.624441, acc.: 65.62%] [G loss: 1.042348]\n",
      "epoch:4 step:4358 [D loss: 0.698260, acc.: 59.38%] [G loss: 1.009509]\n",
      "epoch:4 step:4359 [D loss: 0.615525, acc.: 60.94%] [G loss: 0.923774]\n",
      "epoch:4 step:4360 [D loss: 0.690032, acc.: 58.59%] [G loss: 1.075372]\n",
      "epoch:4 step:4361 [D loss: 0.613206, acc.: 64.84%] [G loss: 0.911200]\n",
      "epoch:4 step:4362 [D loss: 0.704068, acc.: 56.25%] [G loss: 0.853427]\n",
      "epoch:4 step:4363 [D loss: 0.651459, acc.: 67.97%] [G loss: 0.984742]\n",
      "epoch:4 step:4364 [D loss: 0.605484, acc.: 68.75%] [G loss: 1.101302]\n",
      "epoch:4 step:4365 [D loss: 0.613817, acc.: 63.28%] [G loss: 1.163496]\n",
      "epoch:4 step:4366 [D loss: 0.627250, acc.: 67.19%] [G loss: 1.080903]\n",
      "epoch:4 step:4367 [D loss: 0.691900, acc.: 56.25%] [G loss: 1.093756]\n",
      "epoch:4 step:4368 [D loss: 0.644237, acc.: 64.84%] [G loss: 1.012107]\n",
      "epoch:4 step:4369 [D loss: 0.638182, acc.: 63.28%] [G loss: 1.015400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4370 [D loss: 0.685301, acc.: 56.25%] [G loss: 1.053999]\n",
      "epoch:4 step:4371 [D loss: 0.534086, acc.: 78.91%] [G loss: 1.171418]\n",
      "epoch:4 step:4372 [D loss: 0.673898, acc.: 55.47%] [G loss: 1.102712]\n",
      "epoch:4 step:4373 [D loss: 0.658875, acc.: 57.81%] [G loss: 1.013893]\n",
      "epoch:4 step:4374 [D loss: 0.666328, acc.: 60.94%] [G loss: 1.134364]\n",
      "epoch:4 step:4375 [D loss: 0.534809, acc.: 72.66%] [G loss: 1.251584]\n",
      "epoch:4 step:4376 [D loss: 0.703683, acc.: 55.47%] [G loss: 1.014211]\n",
      "epoch:4 step:4377 [D loss: 0.620797, acc.: 61.72%] [G loss: 0.988769]\n",
      "epoch:4 step:4378 [D loss: 0.667084, acc.: 56.25%] [G loss: 0.859721]\n",
      "epoch:4 step:4379 [D loss: 0.595974, acc.: 67.19%] [G loss: 0.985098]\n",
      "epoch:4 step:4380 [D loss: 0.625926, acc.: 68.75%] [G loss: 1.307196]\n",
      "epoch:4 step:4381 [D loss: 0.592473, acc.: 64.06%] [G loss: 1.158189]\n",
      "epoch:4 step:4382 [D loss: 0.568224, acc.: 70.31%] [G loss: 1.259560]\n",
      "epoch:4 step:4383 [D loss: 0.625244, acc.: 66.41%] [G loss: 0.967990]\n",
      "epoch:4 step:4384 [D loss: 0.640980, acc.: 60.94%] [G loss: 1.092352]\n",
      "epoch:4 step:4385 [D loss: 0.603811, acc.: 67.97%] [G loss: 1.191140]\n",
      "epoch:4 step:4386 [D loss: 0.653065, acc.: 64.06%] [G loss: 1.043914]\n",
      "epoch:4 step:4387 [D loss: 0.654788, acc.: 62.50%] [G loss: 0.990051]\n",
      "epoch:4 step:4388 [D loss: 0.640138, acc.: 60.16%] [G loss: 1.138305]\n",
      "epoch:4 step:4389 [D loss: 0.695107, acc.: 57.81%] [G loss: 1.020886]\n",
      "epoch:4 step:4390 [D loss: 0.678361, acc.: 58.59%] [G loss: 1.013183]\n",
      "epoch:4 step:4391 [D loss: 0.664718, acc.: 61.72%] [G loss: 1.026029]\n",
      "epoch:4 step:4392 [D loss: 0.725424, acc.: 52.34%] [G loss: 1.143172]\n",
      "epoch:4 step:4393 [D loss: 0.633040, acc.: 63.28%] [G loss: 1.010972]\n",
      "epoch:4 step:4394 [D loss: 0.652455, acc.: 64.06%] [G loss: 1.156476]\n",
      "epoch:4 step:4395 [D loss: 0.617507, acc.: 68.75%] [G loss: 1.198410]\n",
      "epoch:4 step:4396 [D loss: 0.682672, acc.: 57.81%] [G loss: 1.029256]\n",
      "epoch:4 step:4397 [D loss: 0.589234, acc.: 67.97%] [G loss: 1.118287]\n",
      "epoch:4 step:4398 [D loss: 0.614636, acc.: 67.97%] [G loss: 1.106758]\n",
      "epoch:4 step:4399 [D loss: 0.604709, acc.: 65.62%] [G loss: 1.068159]\n",
      "epoch:4 step:4400 [D loss: 0.593889, acc.: 69.53%] [G loss: 0.938473]\n",
      "##############\n",
      "[2.64125368 2.0184879  2.04676623 2.89245442 0.92672643 6.66510676\n",
      " 2.11570891 2.78386245 3.91659552 6.09865333]\n",
      "##########\n",
      "epoch:4 step:4401 [D loss: 0.592059, acc.: 69.53%] [G loss: 1.034965]\n",
      "epoch:4 step:4402 [D loss: 0.660826, acc.: 58.59%] [G loss: 0.979356]\n",
      "epoch:4 step:4403 [D loss: 0.716326, acc.: 53.91%] [G loss: 1.101326]\n",
      "epoch:4 step:4404 [D loss: 0.627300, acc.: 69.53%] [G loss: 1.108685]\n",
      "epoch:4 step:4405 [D loss: 0.736634, acc.: 53.91%] [G loss: 1.098593]\n",
      "epoch:4 step:4406 [D loss: 0.624527, acc.: 65.62%] [G loss: 0.988565]\n",
      "epoch:4 step:4407 [D loss: 0.601064, acc.: 67.19%] [G loss: 1.106449]\n",
      "epoch:4 step:4408 [D loss: 0.609409, acc.: 64.84%] [G loss: 1.126519]\n",
      "epoch:4 step:4409 [D loss: 0.672274, acc.: 60.16%] [G loss: 1.018127]\n",
      "epoch:4 step:4410 [D loss: 0.625784, acc.: 60.94%] [G loss: 1.037080]\n",
      "epoch:4 step:4411 [D loss: 0.658419, acc.: 61.72%] [G loss: 0.987729]\n",
      "epoch:4 step:4412 [D loss: 0.642615, acc.: 58.59%] [G loss: 1.073118]\n",
      "epoch:4 step:4413 [D loss: 0.674443, acc.: 55.47%] [G loss: 0.923741]\n",
      "epoch:4 step:4414 [D loss: 0.584053, acc.: 75.00%] [G loss: 1.079726]\n",
      "epoch:4 step:4415 [D loss: 0.692107, acc.: 59.38%] [G loss: 0.998229]\n",
      "epoch:4 step:4416 [D loss: 0.699897, acc.: 59.38%] [G loss: 0.888523]\n",
      "epoch:4 step:4417 [D loss: 0.543894, acc.: 71.88%] [G loss: 1.034181]\n",
      "epoch:4 step:4418 [D loss: 0.667152, acc.: 59.38%] [G loss: 0.992600]\n",
      "epoch:4 step:4419 [D loss: 0.727273, acc.: 53.91%] [G loss: 1.042876]\n",
      "epoch:4 step:4420 [D loss: 0.617737, acc.: 64.06%] [G loss: 1.196390]\n",
      "epoch:4 step:4421 [D loss: 0.713283, acc.: 59.38%] [G loss: 1.032134]\n",
      "epoch:4 step:4422 [D loss: 0.711844, acc.: 53.91%] [G loss: 1.015048]\n",
      "epoch:4 step:4423 [D loss: 0.545105, acc.: 70.31%] [G loss: 1.218840]\n",
      "epoch:4 step:4424 [D loss: 0.626344, acc.: 65.62%] [G loss: 1.054551]\n",
      "epoch:4 step:4425 [D loss: 0.675993, acc.: 64.06%] [G loss: 1.108243]\n",
      "epoch:4 step:4426 [D loss: 0.580651, acc.: 69.53%] [G loss: 1.223083]\n",
      "epoch:4 step:4427 [D loss: 0.611474, acc.: 64.84%] [G loss: 1.102922]\n",
      "epoch:4 step:4428 [D loss: 0.554345, acc.: 75.78%] [G loss: 1.066875]\n",
      "epoch:4 step:4429 [D loss: 0.552076, acc.: 71.88%] [G loss: 1.118479]\n",
      "epoch:4 step:4430 [D loss: 0.591902, acc.: 66.41%] [G loss: 1.095731]\n",
      "epoch:4 step:4431 [D loss: 0.757557, acc.: 49.22%] [G loss: 1.051053]\n",
      "epoch:4 step:4432 [D loss: 0.670530, acc.: 59.38%] [G loss: 0.957470]\n",
      "epoch:4 step:4433 [D loss: 0.671043, acc.: 58.59%] [G loss: 0.985639]\n",
      "epoch:4 step:4434 [D loss: 0.608072, acc.: 64.84%] [G loss: 1.120446]\n",
      "epoch:4 step:4435 [D loss: 0.664001, acc.: 57.81%] [G loss: 0.978371]\n",
      "epoch:4 step:4436 [D loss: 0.619271, acc.: 71.09%] [G loss: 1.154366]\n",
      "epoch:4 step:4437 [D loss: 0.565460, acc.: 68.75%] [G loss: 1.154981]\n",
      "epoch:4 step:4438 [D loss: 0.618117, acc.: 64.06%] [G loss: 1.077242]\n",
      "epoch:4 step:4439 [D loss: 0.723785, acc.: 53.12%] [G loss: 0.960036]\n",
      "epoch:4 step:4440 [D loss: 0.705125, acc.: 57.81%] [G loss: 0.962721]\n",
      "epoch:4 step:4441 [D loss: 0.671029, acc.: 60.94%] [G loss: 1.137116]\n",
      "epoch:4 step:4442 [D loss: 0.698563, acc.: 58.59%] [G loss: 0.982880]\n",
      "epoch:4 step:4443 [D loss: 0.669581, acc.: 57.81%] [G loss: 1.080847]\n",
      "epoch:4 step:4444 [D loss: 0.641249, acc.: 64.84%] [G loss: 1.232608]\n",
      "epoch:4 step:4445 [D loss: 0.573788, acc.: 69.53%] [G loss: 1.149801]\n",
      "epoch:4 step:4446 [D loss: 0.617227, acc.: 64.06%] [G loss: 1.262060]\n",
      "epoch:4 step:4447 [D loss: 0.608069, acc.: 71.09%] [G loss: 1.095029]\n",
      "epoch:4 step:4448 [D loss: 0.591673, acc.: 68.75%] [G loss: 1.137533]\n",
      "epoch:4 step:4449 [D loss: 0.680280, acc.: 58.59%] [G loss: 1.050518]\n",
      "epoch:4 step:4450 [D loss: 0.650507, acc.: 60.16%] [G loss: 1.207759]\n",
      "epoch:4 step:4451 [D loss: 0.616418, acc.: 67.97%] [G loss: 1.064964]\n",
      "epoch:4 step:4452 [D loss: 0.712382, acc.: 56.25%] [G loss: 1.022623]\n",
      "epoch:4 step:4453 [D loss: 0.589608, acc.: 67.19%] [G loss: 1.088024]\n",
      "epoch:4 step:4454 [D loss: 0.620450, acc.: 67.19%] [G loss: 1.153153]\n",
      "epoch:4 step:4455 [D loss: 0.597153, acc.: 65.62%] [G loss: 1.068889]\n",
      "epoch:4 step:4456 [D loss: 0.667169, acc.: 58.59%] [G loss: 1.073792]\n",
      "epoch:4 step:4457 [D loss: 0.714389, acc.: 53.91%] [G loss: 1.077646]\n",
      "epoch:4 step:4458 [D loss: 0.672151, acc.: 57.81%] [G loss: 1.004656]\n",
      "epoch:4 step:4459 [D loss: 0.572967, acc.: 76.56%] [G loss: 1.061338]\n",
      "epoch:4 step:4460 [D loss: 0.581048, acc.: 67.97%] [G loss: 1.022983]\n",
      "epoch:4 step:4461 [D loss: 0.587649, acc.: 72.66%] [G loss: 1.165186]\n",
      "epoch:4 step:4462 [D loss: 0.650705, acc.: 60.94%] [G loss: 1.103153]\n",
      "epoch:4 step:4463 [D loss: 0.733980, acc.: 52.34%] [G loss: 1.164597]\n",
      "epoch:4 step:4464 [D loss: 0.544875, acc.: 76.56%] [G loss: 1.165560]\n",
      "epoch:4 step:4465 [D loss: 0.603561, acc.: 69.53%] [G loss: 1.191697]\n",
      "epoch:4 step:4466 [D loss: 0.586113, acc.: 73.44%] [G loss: 1.010477]\n",
      "epoch:4 step:4467 [D loss: 0.714832, acc.: 58.59%] [G loss: 1.027680]\n",
      "epoch:4 step:4468 [D loss: 0.684292, acc.: 56.25%] [G loss: 1.195707]\n",
      "epoch:4 step:4469 [D loss: 0.730066, acc.: 55.47%] [G loss: 1.095650]\n",
      "epoch:4 step:4470 [D loss: 0.581982, acc.: 69.53%] [G loss: 1.119266]\n",
      "epoch:4 step:4471 [D loss: 0.623837, acc.: 61.72%] [G loss: 1.003806]\n",
      "epoch:4 step:4472 [D loss: 0.614524, acc.: 64.06%] [G loss: 1.060468]\n",
      "epoch:4 step:4473 [D loss: 0.623943, acc.: 63.28%] [G loss: 1.098416]\n",
      "epoch:4 step:4474 [D loss: 0.686919, acc.: 55.47%] [G loss: 1.225217]\n",
      "epoch:4 step:4475 [D loss: 0.661485, acc.: 63.28%] [G loss: 1.163200]\n",
      "epoch:4 step:4476 [D loss: 0.581481, acc.: 65.62%] [G loss: 1.210666]\n",
      "epoch:4 step:4477 [D loss: 0.710541, acc.: 49.22%] [G loss: 1.104957]\n",
      "epoch:4 step:4478 [D loss: 0.648365, acc.: 61.72%] [G loss: 1.167541]\n",
      "epoch:4 step:4479 [D loss: 0.714257, acc.: 60.16%] [G loss: 0.971571]\n",
      "epoch:4 step:4480 [D loss: 0.580907, acc.: 69.53%] [G loss: 1.097101]\n",
      "epoch:4 step:4481 [D loss: 0.699557, acc.: 57.03%] [G loss: 1.058666]\n",
      "epoch:4 step:4482 [D loss: 0.593690, acc.: 67.19%] [G loss: 1.050488]\n",
      "epoch:4 step:4483 [D loss: 0.654212, acc.: 60.16%] [G loss: 1.117172]\n",
      "epoch:4 step:4484 [D loss: 0.636328, acc.: 61.72%] [G loss: 1.095960]\n",
      "epoch:4 step:4485 [D loss: 0.675014, acc.: 57.81%] [G loss: 1.260915]\n",
      "epoch:4 step:4486 [D loss: 0.733224, acc.: 53.12%] [G loss: 0.868876]\n",
      "epoch:4 step:4487 [D loss: 0.648926, acc.: 60.94%] [G loss: 0.944593]\n",
      "epoch:4 step:4488 [D loss: 0.640580, acc.: 57.81%] [G loss: 1.136479]\n",
      "epoch:4 step:4489 [D loss: 0.658619, acc.: 61.72%] [G loss: 1.035628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4490 [D loss: 0.630940, acc.: 63.28%] [G loss: 1.113268]\n",
      "epoch:4 step:4491 [D loss: 0.707752, acc.: 53.12%] [G loss: 1.118797]\n",
      "epoch:4 step:4492 [D loss: 0.636481, acc.: 64.06%] [G loss: 0.983257]\n",
      "epoch:4 step:4493 [D loss: 0.623535, acc.: 67.97%] [G loss: 1.000429]\n",
      "epoch:4 step:4494 [D loss: 0.574188, acc.: 73.44%] [G loss: 1.056532]\n",
      "epoch:4 step:4495 [D loss: 0.602109, acc.: 68.75%] [G loss: 1.043898]\n",
      "epoch:4 step:4496 [D loss: 0.610965, acc.: 68.75%] [G loss: 1.164893]\n",
      "epoch:4 step:4497 [D loss: 0.652800, acc.: 67.19%] [G loss: 1.085197]\n",
      "epoch:4 step:4498 [D loss: 0.725652, acc.: 54.69%] [G loss: 1.059357]\n",
      "epoch:4 step:4499 [D loss: 0.597663, acc.: 66.41%] [G loss: 1.039585]\n",
      "epoch:4 step:4500 [D loss: 0.621560, acc.: 67.97%] [G loss: 1.101851]\n",
      "epoch:4 step:4501 [D loss: 0.634433, acc.: 65.62%] [G loss: 1.047659]\n",
      "epoch:4 step:4502 [D loss: 0.622863, acc.: 64.06%] [G loss: 1.086875]\n",
      "epoch:4 step:4503 [D loss: 0.574430, acc.: 74.22%] [G loss: 1.004936]\n",
      "epoch:4 step:4504 [D loss: 0.675507, acc.: 57.81%] [G loss: 1.095384]\n",
      "epoch:4 step:4505 [D loss: 0.696159, acc.: 59.38%] [G loss: 1.001775]\n",
      "epoch:4 step:4506 [D loss: 0.616992, acc.: 67.19%] [G loss: 1.307272]\n",
      "epoch:4 step:4507 [D loss: 0.643786, acc.: 60.16%] [G loss: 1.099849]\n",
      "epoch:4 step:4508 [D loss: 0.642126, acc.: 63.28%] [G loss: 0.953871]\n",
      "epoch:4 step:4509 [D loss: 0.693680, acc.: 57.81%] [G loss: 0.948294]\n",
      "epoch:4 step:4510 [D loss: 0.672462, acc.: 60.16%] [G loss: 1.022227]\n",
      "epoch:4 step:4511 [D loss: 0.685831, acc.: 59.38%] [G loss: 1.087235]\n",
      "epoch:4 step:4512 [D loss: 0.578844, acc.: 64.84%] [G loss: 1.237350]\n",
      "epoch:4 step:4513 [D loss: 0.641642, acc.: 63.28%] [G loss: 1.073163]\n",
      "epoch:4 step:4514 [D loss: 0.564875, acc.: 72.66%] [G loss: 1.146763]\n",
      "epoch:4 step:4515 [D loss: 0.638090, acc.: 64.84%] [G loss: 1.067982]\n",
      "epoch:4 step:4516 [D loss: 0.582286, acc.: 68.75%] [G loss: 1.099070]\n",
      "epoch:4 step:4517 [D loss: 0.595559, acc.: 67.97%] [G loss: 1.232244]\n",
      "epoch:4 step:4518 [D loss: 0.559592, acc.: 75.00%] [G loss: 1.171721]\n",
      "epoch:4 step:4519 [D loss: 0.711451, acc.: 49.22%] [G loss: 0.966550]\n",
      "epoch:4 step:4520 [D loss: 0.689871, acc.: 59.38%] [G loss: 0.979290]\n",
      "epoch:4 step:4521 [D loss: 0.647044, acc.: 63.28%] [G loss: 1.134602]\n",
      "epoch:4 step:4522 [D loss: 0.590797, acc.: 68.75%] [G loss: 1.029311]\n",
      "epoch:4 step:4523 [D loss: 0.594193, acc.: 67.97%] [G loss: 1.177777]\n",
      "epoch:4 step:4524 [D loss: 0.597309, acc.: 70.31%] [G loss: 1.118349]\n",
      "epoch:4 step:4525 [D loss: 0.670225, acc.: 57.03%] [G loss: 1.189517]\n",
      "epoch:4 step:4526 [D loss: 0.643558, acc.: 59.38%] [G loss: 1.012365]\n",
      "epoch:4 step:4527 [D loss: 0.566752, acc.: 71.88%] [G loss: 1.132626]\n",
      "epoch:4 step:4528 [D loss: 0.730286, acc.: 53.12%] [G loss: 1.027368]\n",
      "epoch:4 step:4529 [D loss: 0.645222, acc.: 62.50%] [G loss: 1.069483]\n",
      "epoch:4 step:4530 [D loss: 0.638189, acc.: 63.28%] [G loss: 1.110304]\n",
      "epoch:4 step:4531 [D loss: 0.553412, acc.: 70.31%] [G loss: 1.055475]\n",
      "epoch:4 step:4532 [D loss: 0.620418, acc.: 71.88%] [G loss: 1.225404]\n",
      "epoch:4 step:4533 [D loss: 0.724457, acc.: 53.91%] [G loss: 1.133710]\n",
      "epoch:4 step:4534 [D loss: 0.645289, acc.: 60.94%] [G loss: 0.943264]\n",
      "epoch:4 step:4535 [D loss: 0.590576, acc.: 70.31%] [G loss: 1.257172]\n",
      "epoch:4 step:4536 [D loss: 0.631275, acc.: 66.41%] [G loss: 1.157065]\n",
      "epoch:4 step:4537 [D loss: 0.628169, acc.: 63.28%] [G loss: 0.933523]\n",
      "epoch:4 step:4538 [D loss: 0.585034, acc.: 71.09%] [G loss: 1.086616]\n",
      "epoch:4 step:4539 [D loss: 0.751549, acc.: 48.44%] [G loss: 1.037010]\n",
      "epoch:4 step:4540 [D loss: 0.704975, acc.: 60.94%] [G loss: 1.000044]\n",
      "epoch:4 step:4541 [D loss: 0.657720, acc.: 57.03%] [G loss: 1.070820]\n",
      "epoch:4 step:4542 [D loss: 0.603273, acc.: 71.09%] [G loss: 1.043173]\n",
      "epoch:4 step:4543 [D loss: 0.638827, acc.: 60.94%] [G loss: 0.885335]\n",
      "epoch:4 step:4544 [D loss: 0.573165, acc.: 68.75%] [G loss: 1.077896]\n",
      "epoch:4 step:4545 [D loss: 0.669803, acc.: 58.59%] [G loss: 0.998603]\n",
      "epoch:4 step:4546 [D loss: 0.687184, acc.: 59.38%] [G loss: 1.023159]\n",
      "epoch:4 step:4547 [D loss: 0.618900, acc.: 63.28%] [G loss: 1.044194]\n",
      "epoch:4 step:4548 [D loss: 0.619667, acc.: 67.19%] [G loss: 1.114726]\n",
      "epoch:4 step:4549 [D loss: 0.705610, acc.: 58.59%] [G loss: 1.093789]\n",
      "epoch:4 step:4550 [D loss: 0.618288, acc.: 67.19%] [G loss: 1.211479]\n",
      "epoch:4 step:4551 [D loss: 0.751715, acc.: 53.12%] [G loss: 0.883259]\n",
      "epoch:4 step:4552 [D loss: 0.678020, acc.: 60.94%] [G loss: 1.039671]\n",
      "epoch:4 step:4553 [D loss: 0.664726, acc.: 56.25%] [G loss: 0.955125]\n",
      "epoch:4 step:4554 [D loss: 0.775574, acc.: 49.22%] [G loss: 0.973751]\n",
      "epoch:4 step:4555 [D loss: 0.651646, acc.: 61.72%] [G loss: 1.033697]\n",
      "epoch:4 step:4556 [D loss: 0.618559, acc.: 66.41%] [G loss: 1.066455]\n",
      "epoch:4 step:4557 [D loss: 0.714870, acc.: 54.69%] [G loss: 1.077124]\n",
      "epoch:4 step:4558 [D loss: 0.573305, acc.: 71.09%] [G loss: 1.240436]\n",
      "epoch:4 step:4559 [D loss: 0.750597, acc.: 53.12%] [G loss: 0.923348]\n",
      "epoch:4 step:4560 [D loss: 0.639566, acc.: 61.72%] [G loss: 1.007736]\n",
      "epoch:4 step:4561 [D loss: 0.642650, acc.: 67.19%] [G loss: 0.941504]\n",
      "epoch:4 step:4562 [D loss: 0.584720, acc.: 70.31%] [G loss: 1.117598]\n",
      "epoch:4 step:4563 [D loss: 0.582935, acc.: 70.31%] [G loss: 1.166479]\n",
      "epoch:4 step:4564 [D loss: 0.631764, acc.: 66.41%] [G loss: 1.020518]\n",
      "epoch:4 step:4565 [D loss: 0.652104, acc.: 61.72%] [G loss: 1.035764]\n",
      "epoch:4 step:4566 [D loss: 0.632597, acc.: 60.16%] [G loss: 0.953930]\n",
      "epoch:4 step:4567 [D loss: 0.579284, acc.: 72.66%] [G loss: 1.031953]\n",
      "epoch:4 step:4568 [D loss: 0.614751, acc.: 64.06%] [G loss: 1.123891]\n",
      "epoch:4 step:4569 [D loss: 0.591199, acc.: 69.53%] [G loss: 1.065242]\n",
      "epoch:4 step:4570 [D loss: 0.576980, acc.: 67.97%] [G loss: 0.937565]\n",
      "epoch:4 step:4571 [D loss: 0.645304, acc.: 67.97%] [G loss: 1.054127]\n",
      "epoch:4 step:4572 [D loss: 0.615453, acc.: 67.19%] [G loss: 1.137137]\n",
      "epoch:4 step:4573 [D loss: 0.563320, acc.: 70.31%] [G loss: 1.083212]\n",
      "epoch:4 step:4574 [D loss: 0.675110, acc.: 58.59%] [G loss: 0.989529]\n",
      "epoch:4 step:4575 [D loss: 0.610850, acc.: 65.62%] [G loss: 0.991921]\n",
      "epoch:4 step:4576 [D loss: 0.667885, acc.: 58.59%] [G loss: 1.098460]\n",
      "epoch:4 step:4577 [D loss: 0.627042, acc.: 67.19%] [G loss: 1.114504]\n",
      "epoch:4 step:4578 [D loss: 0.680131, acc.: 60.16%] [G loss: 1.088459]\n",
      "epoch:4 step:4579 [D loss: 0.625240, acc.: 64.06%] [G loss: 1.140787]\n",
      "epoch:4 step:4580 [D loss: 0.559136, acc.: 72.66%] [G loss: 1.074368]\n",
      "epoch:4 step:4581 [D loss: 0.663103, acc.: 66.41%] [G loss: 1.204398]\n",
      "epoch:4 step:4582 [D loss: 0.640341, acc.: 62.50%] [G loss: 1.030818]\n",
      "epoch:4 step:4583 [D loss: 0.615683, acc.: 64.84%] [G loss: 1.299111]\n",
      "epoch:4 step:4584 [D loss: 0.710214, acc.: 54.69%] [G loss: 1.117466]\n",
      "epoch:4 step:4585 [D loss: 0.700238, acc.: 57.03%] [G loss: 1.181564]\n",
      "epoch:4 step:4586 [D loss: 0.640482, acc.: 61.72%] [G loss: 1.238024]\n",
      "epoch:4 step:4587 [D loss: 0.734298, acc.: 50.78%] [G loss: 0.999534]\n",
      "epoch:4 step:4588 [D loss: 0.618705, acc.: 65.62%] [G loss: 0.943640]\n",
      "epoch:4 step:4589 [D loss: 0.518358, acc.: 75.78%] [G loss: 1.133618]\n",
      "epoch:4 step:4590 [D loss: 0.685623, acc.: 63.28%] [G loss: 1.177260]\n",
      "epoch:4 step:4591 [D loss: 0.670938, acc.: 54.69%] [G loss: 1.052713]\n",
      "epoch:4 step:4592 [D loss: 0.723539, acc.: 50.78%] [G loss: 1.272398]\n",
      "epoch:4 step:4593 [D loss: 0.633683, acc.: 67.19%] [G loss: 1.186119]\n",
      "epoch:4 step:4594 [D loss: 0.652299, acc.: 62.50%] [G loss: 1.122351]\n",
      "epoch:4 step:4595 [D loss: 0.648893, acc.: 61.72%] [G loss: 1.147820]\n",
      "epoch:4 step:4596 [D loss: 0.686079, acc.: 60.94%] [G loss: 1.214068]\n",
      "epoch:4 step:4597 [D loss: 0.613880, acc.: 68.75%] [G loss: 1.307380]\n",
      "epoch:4 step:4598 [D loss: 0.637641, acc.: 65.62%] [G loss: 1.055499]\n",
      "epoch:4 step:4599 [D loss: 0.714092, acc.: 57.81%] [G loss: 0.957733]\n",
      "epoch:4 step:4600 [D loss: 0.526718, acc.: 74.22%] [G loss: 1.070751]\n",
      "##############\n",
      "[2.57715701 2.00930918 1.90925654 2.8022857  0.74577025 6.78606129\n",
      " 2.03714479 2.8448717  3.76889224 5.56921698]\n",
      "##########\n",
      "epoch:4 step:4601 [D loss: 0.596085, acc.: 67.97%] [G loss: 1.217748]\n",
      "epoch:4 step:4602 [D loss: 0.596532, acc.: 67.19%] [G loss: 1.204485]\n",
      "epoch:4 step:4603 [D loss: 0.714924, acc.: 53.91%] [G loss: 0.929429]\n",
      "epoch:4 step:4604 [D loss: 0.570858, acc.: 71.09%] [G loss: 1.083426]\n",
      "epoch:4 step:4605 [D loss: 0.665325, acc.: 58.59%] [G loss: 1.200305]\n",
      "epoch:4 step:4606 [D loss: 0.692245, acc.: 53.12%] [G loss: 1.181999]\n",
      "epoch:4 step:4607 [D loss: 0.594539, acc.: 67.97%] [G loss: 0.920598]\n",
      "epoch:4 step:4608 [D loss: 0.617338, acc.: 60.16%] [G loss: 1.043796]\n",
      "epoch:4 step:4609 [D loss: 0.607451, acc.: 66.41%] [G loss: 1.021982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4610 [D loss: 0.698210, acc.: 61.72%] [G loss: 1.086996]\n",
      "epoch:4 step:4611 [D loss: 0.600878, acc.: 65.62%] [G loss: 1.143887]\n",
      "epoch:4 step:4612 [D loss: 0.652864, acc.: 60.94%] [G loss: 1.104943]\n",
      "epoch:4 step:4613 [D loss: 0.671641, acc.: 59.38%] [G loss: 1.197178]\n",
      "epoch:4 step:4614 [D loss: 0.590434, acc.: 67.19%] [G loss: 1.246325]\n",
      "epoch:4 step:4615 [D loss: 0.612433, acc.: 67.19%] [G loss: 1.092141]\n",
      "epoch:4 step:4616 [D loss: 0.699251, acc.: 51.56%] [G loss: 0.947216]\n",
      "epoch:4 step:4617 [D loss: 0.624985, acc.: 64.06%] [G loss: 1.134106]\n",
      "epoch:4 step:4618 [D loss: 0.569032, acc.: 73.44%] [G loss: 1.151443]\n",
      "epoch:4 step:4619 [D loss: 0.612162, acc.: 66.41%] [G loss: 1.130550]\n",
      "epoch:4 step:4620 [D loss: 0.764461, acc.: 53.12%] [G loss: 0.979069]\n",
      "epoch:4 step:4621 [D loss: 0.602623, acc.: 64.84%] [G loss: 1.109375]\n",
      "epoch:4 step:4622 [D loss: 0.609252, acc.: 65.62%] [G loss: 1.174137]\n",
      "epoch:4 step:4623 [D loss: 0.580125, acc.: 67.97%] [G loss: 1.199607]\n",
      "epoch:4 step:4624 [D loss: 0.767567, acc.: 48.44%] [G loss: 0.941918]\n",
      "epoch:4 step:4625 [D loss: 0.654179, acc.: 60.94%] [G loss: 1.043135]\n",
      "epoch:4 step:4626 [D loss: 0.677458, acc.: 54.69%] [G loss: 1.217373]\n",
      "epoch:4 step:4627 [D loss: 0.651823, acc.: 62.50%] [G loss: 1.102852]\n",
      "epoch:4 step:4628 [D loss: 0.578455, acc.: 71.88%] [G loss: 1.128650]\n",
      "epoch:4 step:4629 [D loss: 0.705878, acc.: 53.12%] [G loss: 0.972568]\n",
      "epoch:4 step:4630 [D loss: 0.461142, acc.: 79.69%] [G loss: 1.202968]\n",
      "epoch:4 step:4631 [D loss: 0.579255, acc.: 69.53%] [G loss: 1.084265]\n",
      "epoch:4 step:4632 [D loss: 0.598123, acc.: 62.50%] [G loss: 1.230415]\n",
      "epoch:4 step:4633 [D loss: 0.550410, acc.: 75.00%] [G loss: 1.133636]\n",
      "epoch:4 step:4634 [D loss: 0.615429, acc.: 62.50%] [G loss: 1.034353]\n",
      "epoch:4 step:4635 [D loss: 0.604633, acc.: 65.62%] [G loss: 1.041092]\n",
      "epoch:4 step:4636 [D loss: 0.690357, acc.: 56.25%] [G loss: 1.070281]\n",
      "epoch:4 step:4637 [D loss: 0.633595, acc.: 69.53%] [G loss: 1.063322]\n",
      "epoch:4 step:4638 [D loss: 0.725353, acc.: 53.91%] [G loss: 1.055852]\n",
      "epoch:4 step:4639 [D loss: 0.731511, acc.: 56.25%] [G loss: 0.921974]\n",
      "epoch:4 step:4640 [D loss: 0.642552, acc.: 60.94%] [G loss: 0.942709]\n",
      "epoch:4 step:4641 [D loss: 0.658881, acc.: 64.06%] [G loss: 1.163760]\n",
      "epoch:4 step:4642 [D loss: 0.725062, acc.: 55.47%] [G loss: 1.103981]\n",
      "epoch:4 step:4643 [D loss: 0.579376, acc.: 66.41%] [G loss: 1.177796]\n",
      "epoch:4 step:4644 [D loss: 0.674233, acc.: 57.81%] [G loss: 0.998329]\n",
      "epoch:4 step:4645 [D loss: 0.695669, acc.: 57.81%] [G loss: 1.075572]\n",
      "epoch:4 step:4646 [D loss: 0.637378, acc.: 60.94%] [G loss: 1.116303]\n",
      "epoch:4 step:4647 [D loss: 0.652260, acc.: 61.72%] [G loss: 0.972729]\n",
      "epoch:4 step:4648 [D loss: 0.567541, acc.: 71.88%] [G loss: 1.157319]\n",
      "epoch:4 step:4649 [D loss: 0.593680, acc.: 73.44%] [G loss: 0.945633]\n",
      "epoch:4 step:4650 [D loss: 0.591295, acc.: 67.97%] [G loss: 1.032457]\n",
      "epoch:4 step:4651 [D loss: 0.549026, acc.: 70.31%] [G loss: 1.050660]\n",
      "epoch:4 step:4652 [D loss: 0.585565, acc.: 64.84%] [G loss: 1.275174]\n",
      "epoch:4 step:4653 [D loss: 0.643485, acc.: 63.28%] [G loss: 1.000280]\n",
      "epoch:4 step:4654 [D loss: 0.561700, acc.: 71.09%] [G loss: 0.990141]\n",
      "epoch:4 step:4655 [D loss: 0.616911, acc.: 67.97%] [G loss: 1.143594]\n",
      "epoch:4 step:4656 [D loss: 0.732720, acc.: 54.69%] [G loss: 1.047654]\n",
      "epoch:4 step:4657 [D loss: 0.674199, acc.: 60.94%] [G loss: 1.131959]\n",
      "epoch:4 step:4658 [D loss: 0.609655, acc.: 65.62%] [G loss: 0.986269]\n",
      "epoch:4 step:4659 [D loss: 0.657959, acc.: 57.03%] [G loss: 1.026918]\n",
      "epoch:4 step:4660 [D loss: 0.664902, acc.: 58.59%] [G loss: 1.071966]\n",
      "epoch:4 step:4661 [D loss: 0.833915, acc.: 40.62%] [G loss: 0.795473]\n",
      "epoch:4 step:4662 [D loss: 0.632148, acc.: 61.72%] [G loss: 1.162020]\n",
      "epoch:4 step:4663 [D loss: 0.669701, acc.: 58.59%] [G loss: 0.981293]\n",
      "epoch:4 step:4664 [D loss: 0.622168, acc.: 61.72%] [G loss: 1.285258]\n",
      "epoch:4 step:4665 [D loss: 0.664277, acc.: 57.81%] [G loss: 1.059821]\n",
      "epoch:4 step:4666 [D loss: 0.594241, acc.: 70.31%] [G loss: 1.046559]\n",
      "epoch:4 step:4667 [D loss: 0.580207, acc.: 67.19%] [G loss: 1.154186]\n",
      "epoch:4 step:4668 [D loss: 0.671236, acc.: 57.03%] [G loss: 1.066830]\n",
      "epoch:4 step:4669 [D loss: 0.639373, acc.: 61.72%] [G loss: 1.149698]\n",
      "epoch:4 step:4670 [D loss: 0.615636, acc.: 67.97%] [G loss: 1.015346]\n",
      "epoch:4 step:4671 [D loss: 0.670501, acc.: 60.16%] [G loss: 1.082167]\n",
      "epoch:4 step:4672 [D loss: 0.670471, acc.: 60.16%] [G loss: 1.142216]\n",
      "epoch:4 step:4673 [D loss: 0.675129, acc.: 58.59%] [G loss: 1.021734]\n",
      "epoch:4 step:4674 [D loss: 0.583564, acc.: 71.88%] [G loss: 1.129911]\n",
      "epoch:4 step:4675 [D loss: 0.663660, acc.: 58.59%] [G loss: 1.184987]\n",
      "epoch:4 step:4676 [D loss: 0.563107, acc.: 70.31%] [G loss: 1.132824]\n",
      "epoch:4 step:4677 [D loss: 0.683596, acc.: 54.69%] [G loss: 0.995839]\n",
      "epoch:4 step:4678 [D loss: 0.562269, acc.: 67.97%] [G loss: 1.025659]\n",
      "epoch:4 step:4679 [D loss: 0.561296, acc.: 67.97%] [G loss: 1.097956]\n",
      "epoch:4 step:4680 [D loss: 0.678752, acc.: 58.59%] [G loss: 0.941743]\n",
      "epoch:4 step:4681 [D loss: 0.774706, acc.: 45.31%] [G loss: 0.883958]\n",
      "epoch:4 step:4682 [D loss: 0.709296, acc.: 54.69%] [G loss: 0.706256]\n",
      "epoch:4 step:4683 [D loss: 0.662956, acc.: 57.81%] [G loss: 1.095629]\n",
      "epoch:4 step:4684 [D loss: 0.586142, acc.: 67.97%] [G loss: 0.933963]\n",
      "epoch:4 step:4685 [D loss: 0.678439, acc.: 62.50%] [G loss: 1.100454]\n",
      "epoch:5 step:4686 [D loss: 0.681418, acc.: 57.03%] [G loss: 0.984285]\n",
      "epoch:5 step:4687 [D loss: 0.662334, acc.: 64.84%] [G loss: 0.947921]\n",
      "epoch:5 step:4688 [D loss: 0.615896, acc.: 64.84%] [G loss: 0.956995]\n",
      "epoch:5 step:4689 [D loss: 0.646097, acc.: 67.97%] [G loss: 1.094322]\n",
      "epoch:5 step:4690 [D loss: 0.627236, acc.: 60.94%] [G loss: 1.095423]\n",
      "epoch:5 step:4691 [D loss: 0.705841, acc.: 52.34%] [G loss: 1.063328]\n",
      "epoch:5 step:4692 [D loss: 0.683408, acc.: 54.69%] [G loss: 0.993408]\n",
      "epoch:5 step:4693 [D loss: 0.626426, acc.: 64.06%] [G loss: 0.966789]\n",
      "epoch:5 step:4694 [D loss: 0.630700, acc.: 65.62%] [G loss: 1.133532]\n",
      "epoch:5 step:4695 [D loss: 0.642029, acc.: 64.84%] [G loss: 1.103338]\n",
      "epoch:5 step:4696 [D loss: 0.573576, acc.: 67.97%] [G loss: 1.115587]\n",
      "epoch:5 step:4697 [D loss: 0.508534, acc.: 76.56%] [G loss: 1.299032]\n",
      "epoch:5 step:4698 [D loss: 0.554534, acc.: 74.22%] [G loss: 1.207628]\n",
      "epoch:5 step:4699 [D loss: 0.692613, acc.: 60.16%] [G loss: 0.948244]\n",
      "epoch:5 step:4700 [D loss: 0.612950, acc.: 67.97%] [G loss: 1.217296]\n",
      "epoch:5 step:4701 [D loss: 0.622082, acc.: 64.84%] [G loss: 1.100768]\n",
      "epoch:5 step:4702 [D loss: 0.581986, acc.: 72.66%] [G loss: 1.126806]\n",
      "epoch:5 step:4703 [D loss: 0.610446, acc.: 65.62%] [G loss: 1.187828]\n",
      "epoch:5 step:4704 [D loss: 0.698386, acc.: 53.12%] [G loss: 1.112331]\n",
      "epoch:5 step:4705 [D loss: 0.594230, acc.: 68.75%] [G loss: 1.102558]\n",
      "epoch:5 step:4706 [D loss: 0.669437, acc.: 62.50%] [G loss: 0.962910]\n",
      "epoch:5 step:4707 [D loss: 0.602550, acc.: 72.66%] [G loss: 1.044966]\n",
      "epoch:5 step:4708 [D loss: 0.754150, acc.: 50.78%] [G loss: 1.233353]\n",
      "epoch:5 step:4709 [D loss: 0.642371, acc.: 63.28%] [G loss: 1.047559]\n",
      "epoch:5 step:4710 [D loss: 0.595237, acc.: 65.62%] [G loss: 1.168459]\n",
      "epoch:5 step:4711 [D loss: 0.677256, acc.: 60.16%] [G loss: 1.001654]\n",
      "epoch:5 step:4712 [D loss: 0.613223, acc.: 68.75%] [G loss: 1.079539]\n",
      "epoch:5 step:4713 [D loss: 0.643624, acc.: 64.84%] [G loss: 1.141014]\n",
      "epoch:5 step:4714 [D loss: 0.727455, acc.: 60.16%] [G loss: 0.821531]\n",
      "epoch:5 step:4715 [D loss: 0.617054, acc.: 61.72%] [G loss: 0.983881]\n",
      "epoch:5 step:4716 [D loss: 0.715579, acc.: 53.12%] [G loss: 1.169774]\n",
      "epoch:5 step:4717 [D loss: 0.639407, acc.: 67.97%] [G loss: 1.091024]\n",
      "epoch:5 step:4718 [D loss: 0.616542, acc.: 66.41%] [G loss: 1.040747]\n",
      "epoch:5 step:4719 [D loss: 0.609027, acc.: 65.62%] [G loss: 1.140575]\n",
      "epoch:5 step:4720 [D loss: 0.617093, acc.: 64.84%] [G loss: 1.113407]\n",
      "epoch:5 step:4721 [D loss: 0.673695, acc.: 57.81%] [G loss: 1.110096]\n",
      "epoch:5 step:4722 [D loss: 0.545992, acc.: 75.78%] [G loss: 1.184196]\n",
      "epoch:5 step:4723 [D loss: 0.682209, acc.: 61.72%] [G loss: 1.053856]\n",
      "epoch:5 step:4724 [D loss: 0.622397, acc.: 67.19%] [G loss: 1.030810]\n",
      "epoch:5 step:4725 [D loss: 0.632518, acc.: 57.81%] [G loss: 0.957915]\n",
      "epoch:5 step:4726 [D loss: 0.618818, acc.: 66.41%] [G loss: 1.072413]\n",
      "epoch:5 step:4727 [D loss: 0.635381, acc.: 65.62%] [G loss: 1.207416]\n",
      "epoch:5 step:4728 [D loss: 0.709798, acc.: 54.69%] [G loss: 1.084097]\n",
      "epoch:5 step:4729 [D loss: 0.670584, acc.: 64.84%] [G loss: 0.984918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4730 [D loss: 0.590506, acc.: 67.19%] [G loss: 1.226341]\n",
      "epoch:5 step:4731 [D loss: 0.697465, acc.: 56.25%] [G loss: 1.074783]\n",
      "epoch:5 step:4732 [D loss: 0.632864, acc.: 60.16%] [G loss: 1.029398]\n",
      "epoch:5 step:4733 [D loss: 0.689248, acc.: 60.16%] [G loss: 1.035682]\n",
      "epoch:5 step:4734 [D loss: 0.641397, acc.: 64.06%] [G loss: 0.944214]\n",
      "epoch:5 step:4735 [D loss: 0.588368, acc.: 66.41%] [G loss: 1.157664]\n",
      "epoch:5 step:4736 [D loss: 0.735484, acc.: 56.25%] [G loss: 0.980259]\n",
      "epoch:5 step:4737 [D loss: 0.670386, acc.: 60.94%] [G loss: 1.096889]\n",
      "epoch:5 step:4738 [D loss: 0.639370, acc.: 62.50%] [G loss: 1.131525]\n",
      "epoch:5 step:4739 [D loss: 0.556849, acc.: 67.97%] [G loss: 1.255496]\n",
      "epoch:5 step:4740 [D loss: 0.640024, acc.: 58.59%] [G loss: 1.158570]\n",
      "epoch:5 step:4741 [D loss: 0.725539, acc.: 52.34%] [G loss: 1.164729]\n",
      "epoch:5 step:4742 [D loss: 0.780606, acc.: 52.34%] [G loss: 0.922108]\n",
      "epoch:5 step:4743 [D loss: 0.620175, acc.: 64.06%] [G loss: 1.149879]\n",
      "epoch:5 step:4744 [D loss: 0.579512, acc.: 71.88%] [G loss: 1.092173]\n",
      "epoch:5 step:4745 [D loss: 0.549830, acc.: 71.88%] [G loss: 1.191575]\n",
      "epoch:5 step:4746 [D loss: 0.720517, acc.: 50.78%] [G loss: 1.000498]\n",
      "epoch:5 step:4747 [D loss: 0.551260, acc.: 72.66%] [G loss: 1.134584]\n",
      "epoch:5 step:4748 [D loss: 0.581278, acc.: 70.31%] [G loss: 1.018562]\n",
      "epoch:5 step:4749 [D loss: 0.637765, acc.: 66.41%] [G loss: 1.091656]\n",
      "epoch:5 step:4750 [D loss: 0.585879, acc.: 69.53%] [G loss: 1.027581]\n",
      "epoch:5 step:4751 [D loss: 0.804107, acc.: 45.31%] [G loss: 0.790367]\n",
      "epoch:5 step:4752 [D loss: 0.540418, acc.: 75.00%] [G loss: 1.119629]\n",
      "epoch:5 step:4753 [D loss: 0.617868, acc.: 67.19%] [G loss: 1.146065]\n",
      "epoch:5 step:4754 [D loss: 0.607572, acc.: 62.50%] [G loss: 0.979420]\n",
      "epoch:5 step:4755 [D loss: 0.686464, acc.: 56.25%] [G loss: 1.107154]\n",
      "epoch:5 step:4756 [D loss: 0.678648, acc.: 53.91%] [G loss: 0.968833]\n",
      "epoch:5 step:4757 [D loss: 0.719612, acc.: 54.69%] [G loss: 0.942050]\n",
      "epoch:5 step:4758 [D loss: 0.644925, acc.: 60.16%] [G loss: 1.141331]\n",
      "epoch:5 step:4759 [D loss: 0.566688, acc.: 70.31%] [G loss: 1.161653]\n",
      "epoch:5 step:4760 [D loss: 0.582395, acc.: 71.88%] [G loss: 1.120707]\n",
      "epoch:5 step:4761 [D loss: 0.710145, acc.: 54.69%] [G loss: 1.039322]\n",
      "epoch:5 step:4762 [D loss: 0.592295, acc.: 68.75%] [G loss: 1.210808]\n",
      "epoch:5 step:4763 [D loss: 0.719195, acc.: 50.00%] [G loss: 1.064601]\n",
      "epoch:5 step:4764 [D loss: 0.676992, acc.: 61.72%] [G loss: 0.926092]\n",
      "epoch:5 step:4765 [D loss: 0.603313, acc.: 73.44%] [G loss: 1.017453]\n",
      "epoch:5 step:4766 [D loss: 0.639826, acc.: 63.28%] [G loss: 1.082233]\n",
      "epoch:5 step:4767 [D loss: 0.612835, acc.: 65.62%] [G loss: 1.106160]\n",
      "epoch:5 step:4768 [D loss: 0.818339, acc.: 45.31%] [G loss: 0.944860]\n",
      "epoch:5 step:4769 [D loss: 0.611538, acc.: 67.97%] [G loss: 1.209288]\n",
      "epoch:5 step:4770 [D loss: 0.569566, acc.: 71.09%] [G loss: 1.218199]\n",
      "epoch:5 step:4771 [D loss: 0.690325, acc.: 57.81%] [G loss: 1.039682]\n",
      "epoch:5 step:4772 [D loss: 0.681425, acc.: 57.03%] [G loss: 1.012763]\n",
      "epoch:5 step:4773 [D loss: 0.692228, acc.: 61.72%] [G loss: 1.035549]\n",
      "epoch:5 step:4774 [D loss: 0.615590, acc.: 71.09%] [G loss: 1.179607]\n",
      "epoch:5 step:4775 [D loss: 0.622515, acc.: 65.62%] [G loss: 0.940627]\n",
      "epoch:5 step:4776 [D loss: 0.664750, acc.: 57.03%] [G loss: 1.110069]\n",
      "epoch:5 step:4777 [D loss: 0.694327, acc.: 56.25%] [G loss: 0.892718]\n",
      "epoch:5 step:4778 [D loss: 0.584380, acc.: 68.75%] [G loss: 1.183144]\n",
      "epoch:5 step:4779 [D loss: 0.616786, acc.: 69.53%] [G loss: 1.131866]\n",
      "epoch:5 step:4780 [D loss: 0.651156, acc.: 65.62%] [G loss: 0.994634]\n",
      "epoch:5 step:4781 [D loss: 0.667084, acc.: 59.38%] [G loss: 1.101885]\n",
      "epoch:5 step:4782 [D loss: 0.606462, acc.: 68.75%] [G loss: 1.013427]\n",
      "epoch:5 step:4783 [D loss: 0.630433, acc.: 65.62%] [G loss: 1.102799]\n",
      "epoch:5 step:4784 [D loss: 0.642912, acc.: 63.28%] [G loss: 1.117362]\n",
      "epoch:5 step:4785 [D loss: 0.688118, acc.: 55.47%] [G loss: 0.998858]\n",
      "epoch:5 step:4786 [D loss: 0.579764, acc.: 69.53%] [G loss: 1.145130]\n",
      "epoch:5 step:4787 [D loss: 0.597867, acc.: 64.06%] [G loss: 1.159466]\n",
      "epoch:5 step:4788 [D loss: 0.614918, acc.: 63.28%] [G loss: 1.016503]\n",
      "epoch:5 step:4789 [D loss: 0.609815, acc.: 69.53%] [G loss: 0.961951]\n",
      "epoch:5 step:4790 [D loss: 0.498675, acc.: 83.59%] [G loss: 1.056845]\n",
      "epoch:5 step:4791 [D loss: 0.550835, acc.: 72.66%] [G loss: 1.121820]\n",
      "epoch:5 step:4792 [D loss: 0.662701, acc.: 63.28%] [G loss: 1.054221]\n",
      "epoch:5 step:4793 [D loss: 0.534085, acc.: 73.44%] [G loss: 1.114708]\n",
      "epoch:5 step:4794 [D loss: 0.574732, acc.: 67.19%] [G loss: 1.130046]\n",
      "epoch:5 step:4795 [D loss: 0.689182, acc.: 57.81%] [G loss: 0.929956]\n",
      "epoch:5 step:4796 [D loss: 0.719790, acc.: 53.91%] [G loss: 1.116005]\n",
      "epoch:5 step:4797 [D loss: 0.665178, acc.: 58.59%] [G loss: 1.042680]\n",
      "epoch:5 step:4798 [D loss: 0.550446, acc.: 75.00%] [G loss: 1.003113]\n",
      "epoch:5 step:4799 [D loss: 0.637479, acc.: 63.28%] [G loss: 1.003455]\n",
      "epoch:5 step:4800 [D loss: 0.703503, acc.: 51.56%] [G loss: 1.225713]\n",
      "##############\n",
      "[2.68430452 2.02350875 2.00930582 2.90084201 0.97320856 6.00026417\n",
      " 2.11449519 3.15631968 3.99588823 6.47427346]\n",
      "##########\n",
      "epoch:5 step:4801 [D loss: 0.693589, acc.: 57.03%] [G loss: 1.006460]\n",
      "epoch:5 step:4802 [D loss: 0.616033, acc.: 63.28%] [G loss: 1.129197]\n",
      "epoch:5 step:4803 [D loss: 0.618705, acc.: 64.84%] [G loss: 1.109072]\n",
      "epoch:5 step:4804 [D loss: 0.750114, acc.: 55.47%] [G loss: 1.054108]\n",
      "epoch:5 step:4805 [D loss: 0.703727, acc.: 53.91%] [G loss: 1.048140]\n",
      "epoch:5 step:4806 [D loss: 0.739812, acc.: 53.12%] [G loss: 0.885581]\n",
      "epoch:5 step:4807 [D loss: 0.651030, acc.: 57.81%] [G loss: 1.030682]\n",
      "epoch:5 step:4808 [D loss: 0.600225, acc.: 66.41%] [G loss: 1.127919]\n",
      "epoch:5 step:4809 [D loss: 0.615776, acc.: 62.50%] [G loss: 1.067403]\n",
      "epoch:5 step:4810 [D loss: 0.647589, acc.: 60.94%] [G loss: 1.078790]\n",
      "epoch:5 step:4811 [D loss: 0.640411, acc.: 64.06%] [G loss: 1.035479]\n",
      "epoch:5 step:4812 [D loss: 0.588302, acc.: 71.09%] [G loss: 1.187145]\n",
      "epoch:5 step:4813 [D loss: 0.705363, acc.: 60.16%] [G loss: 1.028024]\n",
      "epoch:5 step:4814 [D loss: 0.626542, acc.: 67.19%] [G loss: 1.006787]\n",
      "epoch:5 step:4815 [D loss: 0.674382, acc.: 57.81%] [G loss: 1.050537]\n",
      "epoch:5 step:4816 [D loss: 0.596233, acc.: 71.09%] [G loss: 1.297237]\n",
      "epoch:5 step:4817 [D loss: 0.726537, acc.: 52.34%] [G loss: 0.998472]\n",
      "epoch:5 step:4818 [D loss: 0.607678, acc.: 66.41%] [G loss: 0.956933]\n",
      "epoch:5 step:4819 [D loss: 0.612788, acc.: 70.31%] [G loss: 1.121053]\n",
      "epoch:5 step:4820 [D loss: 0.593003, acc.: 69.53%] [G loss: 1.219666]\n",
      "epoch:5 step:4821 [D loss: 0.666537, acc.: 60.16%] [G loss: 1.059961]\n",
      "epoch:5 step:4822 [D loss: 0.585627, acc.: 71.88%] [G loss: 1.034380]\n",
      "epoch:5 step:4823 [D loss: 0.673580, acc.: 58.59%] [G loss: 0.988282]\n",
      "epoch:5 step:4824 [D loss: 0.658551, acc.: 62.50%] [G loss: 1.002493]\n",
      "epoch:5 step:4825 [D loss: 0.643461, acc.: 64.84%] [G loss: 1.075019]\n",
      "epoch:5 step:4826 [D loss: 0.814522, acc.: 40.62%] [G loss: 0.951261]\n",
      "epoch:5 step:4827 [D loss: 0.590195, acc.: 69.53%] [G loss: 1.071001]\n",
      "epoch:5 step:4828 [D loss: 0.622145, acc.: 71.09%] [G loss: 1.138217]\n",
      "epoch:5 step:4829 [D loss: 0.679685, acc.: 57.81%] [G loss: 1.212336]\n",
      "epoch:5 step:4830 [D loss: 0.594207, acc.: 66.41%] [G loss: 1.277832]\n",
      "epoch:5 step:4831 [D loss: 0.612581, acc.: 64.06%] [G loss: 0.979552]\n",
      "epoch:5 step:4832 [D loss: 0.509857, acc.: 78.12%] [G loss: 1.115166]\n",
      "epoch:5 step:4833 [D loss: 0.728122, acc.: 53.12%] [G loss: 1.015223]\n",
      "epoch:5 step:4834 [D loss: 0.676941, acc.: 57.81%] [G loss: 1.063661]\n",
      "epoch:5 step:4835 [D loss: 0.570883, acc.: 70.31%] [G loss: 1.106038]\n",
      "epoch:5 step:4836 [D loss: 0.721640, acc.: 58.59%] [G loss: 0.898447]\n",
      "epoch:5 step:4837 [D loss: 0.594459, acc.: 69.53%] [G loss: 1.099947]\n",
      "epoch:5 step:4838 [D loss: 0.692661, acc.: 59.38%] [G loss: 0.964490]\n",
      "epoch:5 step:4839 [D loss: 0.647347, acc.: 63.28%] [G loss: 1.091852]\n",
      "epoch:5 step:4840 [D loss: 0.562638, acc.: 71.88%] [G loss: 0.982325]\n",
      "epoch:5 step:4841 [D loss: 0.604446, acc.: 67.19%] [G loss: 1.104717]\n",
      "epoch:5 step:4842 [D loss: 0.638906, acc.: 64.06%] [G loss: 1.092440]\n",
      "epoch:5 step:4843 [D loss: 0.646744, acc.: 62.50%] [G loss: 1.135916]\n",
      "epoch:5 step:4844 [D loss: 0.611500, acc.: 67.19%] [G loss: 1.020001]\n",
      "epoch:5 step:4845 [D loss: 0.639593, acc.: 59.38%] [G loss: 1.173002]\n",
      "epoch:5 step:4846 [D loss: 0.584104, acc.: 68.75%] [G loss: 0.952747]\n",
      "epoch:5 step:4847 [D loss: 0.618762, acc.: 68.75%] [G loss: 1.181044]\n",
      "epoch:5 step:4848 [D loss: 0.630548, acc.: 66.41%] [G loss: 1.199106]\n",
      "epoch:5 step:4849 [D loss: 0.655982, acc.: 61.72%] [G loss: 1.204610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4850 [D loss: 0.723881, acc.: 53.12%] [G loss: 0.940029]\n",
      "epoch:5 step:4851 [D loss: 0.649081, acc.: 58.59%] [G loss: 1.158645]\n",
      "epoch:5 step:4852 [D loss: 0.556999, acc.: 71.88%] [G loss: 1.118666]\n",
      "epoch:5 step:4853 [D loss: 0.631917, acc.: 58.59%] [G loss: 1.110366]\n",
      "epoch:5 step:4854 [D loss: 0.592717, acc.: 65.62%] [G loss: 1.243609]\n",
      "epoch:5 step:4855 [D loss: 0.625957, acc.: 61.72%] [G loss: 1.128952]\n",
      "epoch:5 step:4856 [D loss: 0.634306, acc.: 62.50%] [G loss: 1.134450]\n",
      "epoch:5 step:4857 [D loss: 0.579268, acc.: 71.09%] [G loss: 1.024826]\n",
      "epoch:5 step:4858 [D loss: 0.688260, acc.: 59.38%] [G loss: 1.126563]\n",
      "epoch:5 step:4859 [D loss: 0.642428, acc.: 62.50%] [G loss: 0.933680]\n",
      "epoch:5 step:4860 [D loss: 0.515921, acc.: 76.56%] [G loss: 1.325090]\n",
      "epoch:5 step:4861 [D loss: 0.581015, acc.: 67.97%] [G loss: 1.175366]\n",
      "epoch:5 step:4862 [D loss: 0.585342, acc.: 71.09%] [G loss: 1.076018]\n",
      "epoch:5 step:4863 [D loss: 0.787267, acc.: 46.09%] [G loss: 0.899116]\n",
      "epoch:5 step:4864 [D loss: 0.696649, acc.: 57.81%] [G loss: 1.040433]\n",
      "epoch:5 step:4865 [D loss: 0.640697, acc.: 64.06%] [G loss: 1.120033]\n",
      "epoch:5 step:4866 [D loss: 0.657522, acc.: 63.28%] [G loss: 1.120434]\n",
      "epoch:5 step:4867 [D loss: 0.576504, acc.: 70.31%] [G loss: 1.044653]\n",
      "epoch:5 step:4868 [D loss: 0.639417, acc.: 65.62%] [G loss: 0.987738]\n",
      "epoch:5 step:4869 [D loss: 0.697281, acc.: 60.94%] [G loss: 1.030920]\n",
      "epoch:5 step:4870 [D loss: 0.646780, acc.: 60.16%] [G loss: 1.112778]\n",
      "epoch:5 step:4871 [D loss: 0.618793, acc.: 64.84%] [G loss: 0.961836]\n",
      "epoch:5 step:4872 [D loss: 0.619977, acc.: 64.84%] [G loss: 0.971953]\n",
      "epoch:5 step:4873 [D loss: 0.668984, acc.: 54.69%] [G loss: 1.092419]\n",
      "epoch:5 step:4874 [D loss: 0.742288, acc.: 56.25%] [G loss: 1.050679]\n",
      "epoch:5 step:4875 [D loss: 0.755279, acc.: 53.12%] [G loss: 1.032865]\n",
      "epoch:5 step:4876 [D loss: 0.560307, acc.: 77.34%] [G loss: 1.156339]\n",
      "epoch:5 step:4877 [D loss: 0.596916, acc.: 67.19%] [G loss: 1.176178]\n",
      "epoch:5 step:4878 [D loss: 0.678253, acc.: 62.50%] [G loss: 1.171965]\n",
      "epoch:5 step:4879 [D loss: 0.618787, acc.: 60.94%] [G loss: 1.084435]\n",
      "epoch:5 step:4880 [D loss: 0.712039, acc.: 56.25%] [G loss: 1.094238]\n",
      "epoch:5 step:4881 [D loss: 0.693926, acc.: 55.47%] [G loss: 1.033870]\n",
      "epoch:5 step:4882 [D loss: 0.633462, acc.: 66.41%] [G loss: 1.097535]\n",
      "epoch:5 step:4883 [D loss: 0.615284, acc.: 65.62%] [G loss: 1.061767]\n",
      "epoch:5 step:4884 [D loss: 0.589929, acc.: 67.19%] [G loss: 0.997064]\n",
      "epoch:5 step:4885 [D loss: 0.618950, acc.: 74.22%] [G loss: 1.018654]\n",
      "epoch:5 step:4886 [D loss: 0.611802, acc.: 67.19%] [G loss: 0.967305]\n",
      "epoch:5 step:4887 [D loss: 0.575640, acc.: 71.09%] [G loss: 1.071807]\n",
      "epoch:5 step:4888 [D loss: 0.664209, acc.: 60.94%] [G loss: 0.945889]\n",
      "epoch:5 step:4889 [D loss: 0.647008, acc.: 61.72%] [G loss: 1.052573]\n",
      "epoch:5 step:4890 [D loss: 0.573319, acc.: 69.53%] [G loss: 1.016410]\n",
      "epoch:5 step:4891 [D loss: 0.758985, acc.: 44.53%] [G loss: 0.889780]\n",
      "epoch:5 step:4892 [D loss: 0.744135, acc.: 48.44%] [G loss: 0.894098]\n",
      "epoch:5 step:4893 [D loss: 0.612005, acc.: 63.28%] [G loss: 1.041825]\n",
      "epoch:5 step:4894 [D loss: 0.623636, acc.: 67.19%] [G loss: 1.019960]\n",
      "epoch:5 step:4895 [D loss: 0.651843, acc.: 63.28%] [G loss: 1.281628]\n",
      "epoch:5 step:4896 [D loss: 0.533371, acc.: 75.78%] [G loss: 0.999647]\n",
      "epoch:5 step:4897 [D loss: 0.678809, acc.: 55.47%] [G loss: 1.055926]\n",
      "epoch:5 step:4898 [D loss: 0.653691, acc.: 57.81%] [G loss: 1.177826]\n",
      "epoch:5 step:4899 [D loss: 0.682559, acc.: 57.03%] [G loss: 1.270919]\n",
      "epoch:5 step:4900 [D loss: 0.662177, acc.: 61.72%] [G loss: 1.242304]\n",
      "epoch:5 step:4901 [D loss: 0.587106, acc.: 68.75%] [G loss: 1.328675]\n",
      "epoch:5 step:4902 [D loss: 0.646693, acc.: 60.94%] [G loss: 0.943980]\n",
      "epoch:5 step:4903 [D loss: 0.661493, acc.: 58.59%] [G loss: 0.969009]\n",
      "epoch:5 step:4904 [D loss: 0.705258, acc.: 56.25%] [G loss: 1.178230]\n",
      "epoch:5 step:4905 [D loss: 0.695030, acc.: 60.94%] [G loss: 0.991745]\n",
      "epoch:5 step:4906 [D loss: 0.574222, acc.: 70.31%] [G loss: 1.111366]\n",
      "epoch:5 step:4907 [D loss: 0.742708, acc.: 58.59%] [G loss: 0.869349]\n",
      "epoch:5 step:4908 [D loss: 0.495667, acc.: 78.91%] [G loss: 1.102566]\n",
      "epoch:5 step:4909 [D loss: 0.772543, acc.: 50.00%] [G loss: 0.989161]\n",
      "epoch:5 step:4910 [D loss: 0.566882, acc.: 71.88%] [G loss: 1.130977]\n",
      "epoch:5 step:4911 [D loss: 0.653508, acc.: 61.72%] [G loss: 1.006241]\n",
      "epoch:5 step:4912 [D loss: 0.664875, acc.: 58.59%] [G loss: 1.123531]\n",
      "epoch:5 step:4913 [D loss: 0.571517, acc.: 68.75%] [G loss: 1.263767]\n",
      "epoch:5 step:4914 [D loss: 0.624615, acc.: 64.84%] [G loss: 1.017133]\n",
      "epoch:5 step:4915 [D loss: 0.661279, acc.: 59.38%] [G loss: 1.160526]\n",
      "epoch:5 step:4916 [D loss: 0.663334, acc.: 60.94%] [G loss: 1.081752]\n",
      "epoch:5 step:4917 [D loss: 0.690102, acc.: 60.94%] [G loss: 1.167001]\n",
      "epoch:5 step:4918 [D loss: 0.654992, acc.: 60.16%] [G loss: 1.099213]\n",
      "epoch:5 step:4919 [D loss: 0.609961, acc.: 72.66%] [G loss: 1.114845]\n",
      "epoch:5 step:4920 [D loss: 0.626847, acc.: 65.62%] [G loss: 1.042025]\n",
      "epoch:5 step:4921 [D loss: 0.626865, acc.: 69.53%] [G loss: 1.040992]\n",
      "epoch:5 step:4922 [D loss: 0.631828, acc.: 64.84%] [G loss: 1.030423]\n",
      "epoch:5 step:4923 [D loss: 0.684349, acc.: 59.38%] [G loss: 0.974923]\n",
      "epoch:5 step:4924 [D loss: 0.633619, acc.: 60.94%] [G loss: 0.983884]\n",
      "epoch:5 step:4925 [D loss: 0.584646, acc.: 76.56%] [G loss: 1.021746]\n",
      "epoch:5 step:4926 [D loss: 0.680625, acc.: 60.94%] [G loss: 1.128283]\n",
      "epoch:5 step:4927 [D loss: 0.656342, acc.: 58.59%] [G loss: 1.115309]\n",
      "epoch:5 step:4928 [D loss: 0.662718, acc.: 57.81%] [G loss: 0.995775]\n",
      "epoch:5 step:4929 [D loss: 0.675361, acc.: 60.94%] [G loss: 1.001843]\n",
      "epoch:5 step:4930 [D loss: 0.641492, acc.: 64.06%] [G loss: 1.016451]\n",
      "epoch:5 step:4931 [D loss: 0.652481, acc.: 58.59%] [G loss: 1.107379]\n",
      "epoch:5 step:4932 [D loss: 0.586107, acc.: 71.09%] [G loss: 1.066583]\n",
      "epoch:5 step:4933 [D loss: 0.695165, acc.: 58.59%] [G loss: 1.170886]\n",
      "epoch:5 step:4934 [D loss: 0.723179, acc.: 53.91%] [G loss: 1.084855]\n",
      "epoch:5 step:4935 [D loss: 0.621396, acc.: 65.62%] [G loss: 1.236890]\n",
      "epoch:5 step:4936 [D loss: 0.653934, acc.: 57.81%] [G loss: 1.015568]\n",
      "epoch:5 step:4937 [D loss: 0.680326, acc.: 57.81%] [G loss: 0.973583]\n",
      "epoch:5 step:4938 [D loss: 0.637197, acc.: 66.41%] [G loss: 1.118577]\n",
      "epoch:5 step:4939 [D loss: 0.526276, acc.: 75.78%] [G loss: 0.983180]\n",
      "epoch:5 step:4940 [D loss: 0.702380, acc.: 60.16%] [G loss: 0.942008]\n",
      "epoch:5 step:4941 [D loss: 0.625847, acc.: 67.19%] [G loss: 1.034187]\n",
      "epoch:5 step:4942 [D loss: 0.630663, acc.: 69.53%] [G loss: 1.041692]\n",
      "epoch:5 step:4943 [D loss: 0.626682, acc.: 67.19%] [G loss: 1.172944]\n",
      "epoch:5 step:4944 [D loss: 0.664562, acc.: 57.81%] [G loss: 0.914722]\n",
      "epoch:5 step:4945 [D loss: 0.601440, acc.: 70.31%] [G loss: 1.035976]\n",
      "epoch:5 step:4946 [D loss: 0.563244, acc.: 75.00%] [G loss: 1.093541]\n",
      "epoch:5 step:4947 [D loss: 0.763053, acc.: 50.78%] [G loss: 1.220418]\n",
      "epoch:5 step:4948 [D loss: 0.658020, acc.: 61.72%] [G loss: 1.174181]\n",
      "epoch:5 step:4949 [D loss: 0.570543, acc.: 71.88%] [G loss: 1.152836]\n",
      "epoch:5 step:4950 [D loss: 0.621027, acc.: 64.84%] [G loss: 0.935923]\n",
      "epoch:5 step:4951 [D loss: 0.653366, acc.: 66.41%] [G loss: 0.973562]\n",
      "epoch:5 step:4952 [D loss: 0.739618, acc.: 53.91%] [G loss: 1.046359]\n",
      "epoch:5 step:4953 [D loss: 0.550303, acc.: 71.09%] [G loss: 1.280137]\n",
      "epoch:5 step:4954 [D loss: 0.572502, acc.: 69.53%] [G loss: 1.138496]\n",
      "epoch:5 step:4955 [D loss: 0.583342, acc.: 69.53%] [G loss: 1.122370]\n",
      "epoch:5 step:4956 [D loss: 0.675874, acc.: 59.38%] [G loss: 1.105511]\n",
      "epoch:5 step:4957 [D loss: 0.604013, acc.: 68.75%] [G loss: 1.116082]\n",
      "epoch:5 step:4958 [D loss: 0.677037, acc.: 60.16%] [G loss: 1.051742]\n",
      "epoch:5 step:4959 [D loss: 0.693589, acc.: 60.94%] [G loss: 0.976569]\n",
      "epoch:5 step:4960 [D loss: 0.688367, acc.: 58.59%] [G loss: 0.954335]\n",
      "epoch:5 step:4961 [D loss: 0.746242, acc.: 50.78%] [G loss: 0.978385]\n",
      "epoch:5 step:4962 [D loss: 0.551188, acc.: 75.00%] [G loss: 1.072338]\n",
      "epoch:5 step:4963 [D loss: 0.618338, acc.: 69.53%] [G loss: 1.087382]\n",
      "epoch:5 step:4964 [D loss: 0.698845, acc.: 57.03%] [G loss: 1.008161]\n",
      "epoch:5 step:4965 [D loss: 0.631303, acc.: 64.84%] [G loss: 1.153419]\n",
      "epoch:5 step:4966 [D loss: 0.565700, acc.: 69.53%] [G loss: 1.295059]\n",
      "epoch:5 step:4967 [D loss: 0.628766, acc.: 68.75%] [G loss: 0.969569]\n",
      "epoch:5 step:4968 [D loss: 0.559014, acc.: 71.88%] [G loss: 1.080689]\n",
      "epoch:5 step:4969 [D loss: 0.659044, acc.: 55.47%] [G loss: 1.000139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4970 [D loss: 0.582980, acc.: 75.00%] [G loss: 1.152788]\n",
      "epoch:5 step:4971 [D loss: 0.683544, acc.: 60.94%] [G loss: 1.063358]\n",
      "epoch:5 step:4972 [D loss: 0.594198, acc.: 68.75%] [G loss: 1.055463]\n",
      "epoch:5 step:4973 [D loss: 0.619456, acc.: 65.62%] [G loss: 1.194344]\n",
      "epoch:5 step:4974 [D loss: 0.658969, acc.: 58.59%] [G loss: 0.993608]\n",
      "epoch:5 step:4975 [D loss: 0.645095, acc.: 60.94%] [G loss: 0.971349]\n",
      "epoch:5 step:4976 [D loss: 0.599838, acc.: 62.50%] [G loss: 1.073670]\n",
      "epoch:5 step:4977 [D loss: 0.652635, acc.: 60.94%] [G loss: 1.200491]\n",
      "epoch:5 step:4978 [D loss: 0.670626, acc.: 59.38%] [G loss: 1.104388]\n",
      "epoch:5 step:4979 [D loss: 0.579260, acc.: 71.88%] [G loss: 1.133764]\n",
      "epoch:5 step:4980 [D loss: 0.575752, acc.: 74.22%] [G loss: 1.309091]\n",
      "epoch:5 step:4981 [D loss: 0.606818, acc.: 66.41%] [G loss: 1.105718]\n",
      "epoch:5 step:4982 [D loss: 0.673805, acc.: 61.72%] [G loss: 1.185959]\n",
      "epoch:5 step:4983 [D loss: 0.658646, acc.: 60.16%] [G loss: 0.925405]\n",
      "epoch:5 step:4984 [D loss: 0.609956, acc.: 67.97%] [G loss: 1.077668]\n",
      "epoch:5 step:4985 [D loss: 0.620241, acc.: 65.62%] [G loss: 0.934827]\n",
      "epoch:5 step:4986 [D loss: 0.686372, acc.: 57.03%] [G loss: 1.075714]\n",
      "epoch:5 step:4987 [D loss: 0.564083, acc.: 74.22%] [G loss: 1.144334]\n",
      "epoch:5 step:4988 [D loss: 0.644946, acc.: 60.16%] [G loss: 1.029962]\n",
      "epoch:5 step:4989 [D loss: 0.617036, acc.: 70.31%] [G loss: 1.086122]\n",
      "epoch:5 step:4990 [D loss: 0.519774, acc.: 78.12%] [G loss: 1.168656]\n",
      "epoch:5 step:4991 [D loss: 0.630790, acc.: 64.84%] [G loss: 0.986115]\n",
      "epoch:5 step:4992 [D loss: 0.622665, acc.: 66.41%] [G loss: 1.064519]\n",
      "epoch:5 step:4993 [D loss: 0.608507, acc.: 66.41%] [G loss: 1.160312]\n",
      "epoch:5 step:4994 [D loss: 0.530682, acc.: 75.78%] [G loss: 0.940436]\n",
      "epoch:5 step:4995 [D loss: 0.649978, acc.: 63.28%] [G loss: 1.129488]\n",
      "epoch:5 step:4996 [D loss: 0.568078, acc.: 67.97%] [G loss: 1.075349]\n",
      "epoch:5 step:4997 [D loss: 0.704444, acc.: 54.69%] [G loss: 0.959287]\n",
      "epoch:5 step:4998 [D loss: 0.655938, acc.: 68.75%] [G loss: 1.033953]\n",
      "epoch:5 step:4999 [D loss: 0.647883, acc.: 64.06%] [G loss: 1.011137]\n",
      "epoch:5 step:5000 [D loss: 0.663585, acc.: 58.59%] [G loss: 0.896050]\n",
      "##############\n",
      "[2.50935536 2.00192642 1.72816151 2.96003804 0.92403419 6.34531042\n",
      " 1.89620882 2.93232338 3.76253517 5.23996736]\n",
      "##########\n",
      "epoch:5 step:5001 [D loss: 0.658721, acc.: 60.94%] [G loss: 1.017818]\n",
      "epoch:5 step:5002 [D loss: 0.629277, acc.: 67.19%] [G loss: 1.003998]\n",
      "epoch:5 step:5003 [D loss: 0.742733, acc.: 51.56%] [G loss: 1.039337]\n",
      "epoch:5 step:5004 [D loss: 0.676670, acc.: 60.16%] [G loss: 1.199975]\n",
      "epoch:5 step:5005 [D loss: 0.691289, acc.: 59.38%] [G loss: 1.201302]\n",
      "epoch:5 step:5006 [D loss: 0.736317, acc.: 57.03%] [G loss: 1.069667]\n",
      "epoch:5 step:5007 [D loss: 0.774176, acc.: 54.69%] [G loss: 0.901065]\n",
      "epoch:5 step:5008 [D loss: 0.629149, acc.: 60.16%] [G loss: 1.009913]\n",
      "epoch:5 step:5009 [D loss: 0.644825, acc.: 60.16%] [G loss: 0.997156]\n",
      "epoch:5 step:5010 [D loss: 0.589190, acc.: 67.97%] [G loss: 1.273079]\n",
      "epoch:5 step:5011 [D loss: 0.680739, acc.: 57.81%] [G loss: 1.314651]\n",
      "epoch:5 step:5012 [D loss: 0.572454, acc.: 73.44%] [G loss: 1.158292]\n",
      "epoch:5 step:5013 [D loss: 0.552073, acc.: 71.88%] [G loss: 1.189474]\n",
      "epoch:5 step:5014 [D loss: 0.673417, acc.: 64.84%] [G loss: 1.153666]\n",
      "epoch:5 step:5015 [D loss: 0.712916, acc.: 55.47%] [G loss: 0.956207]\n",
      "epoch:5 step:5016 [D loss: 0.685769, acc.: 59.38%] [G loss: 1.172398]\n",
      "epoch:5 step:5017 [D loss: 0.632719, acc.: 63.28%] [G loss: 1.191399]\n",
      "epoch:5 step:5018 [D loss: 0.722033, acc.: 57.03%] [G loss: 1.172656]\n",
      "epoch:5 step:5019 [D loss: 0.586689, acc.: 67.19%] [G loss: 1.035102]\n",
      "epoch:5 step:5020 [D loss: 0.591912, acc.: 73.44%] [G loss: 1.076864]\n",
      "epoch:5 step:5021 [D loss: 0.653343, acc.: 61.72%] [G loss: 1.138266]\n",
      "epoch:5 step:5022 [D loss: 0.751165, acc.: 52.34%] [G loss: 0.882130]\n",
      "epoch:5 step:5023 [D loss: 0.551644, acc.: 71.09%] [G loss: 1.129558]\n",
      "epoch:5 step:5024 [D loss: 0.665153, acc.: 59.38%] [G loss: 1.066259]\n",
      "epoch:5 step:5025 [D loss: 0.535612, acc.: 74.22%] [G loss: 1.125219]\n",
      "epoch:5 step:5026 [D loss: 0.643585, acc.: 65.62%] [G loss: 1.047359]\n",
      "epoch:5 step:5027 [D loss: 0.600138, acc.: 70.31%] [G loss: 1.074740]\n",
      "epoch:5 step:5028 [D loss: 0.634568, acc.: 63.28%] [G loss: 1.050113]\n",
      "epoch:5 step:5029 [D loss: 0.642979, acc.: 65.62%] [G loss: 1.129343]\n",
      "epoch:5 step:5030 [D loss: 0.665488, acc.: 57.03%] [G loss: 1.037460]\n",
      "epoch:5 step:5031 [D loss: 0.635367, acc.: 61.72%] [G loss: 1.110729]\n",
      "epoch:5 step:5032 [D loss: 0.620129, acc.: 67.19%] [G loss: 0.886929]\n",
      "epoch:5 step:5033 [D loss: 0.804104, acc.: 41.41%] [G loss: 0.897448]\n",
      "epoch:5 step:5034 [D loss: 0.684629, acc.: 55.47%] [G loss: 1.054114]\n",
      "epoch:5 step:5035 [D loss: 0.653607, acc.: 60.16%] [G loss: 0.939977]\n",
      "epoch:5 step:5036 [D loss: 0.654580, acc.: 56.25%] [G loss: 1.118108]\n",
      "epoch:5 step:5037 [D loss: 0.773808, acc.: 47.66%] [G loss: 0.920802]\n",
      "epoch:5 step:5038 [D loss: 0.669728, acc.: 59.38%] [G loss: 1.078343]\n",
      "epoch:5 step:5039 [D loss: 0.601579, acc.: 67.19%] [G loss: 1.224675]\n",
      "epoch:5 step:5040 [D loss: 0.572119, acc.: 75.00%] [G loss: 1.065122]\n",
      "epoch:5 step:5041 [D loss: 0.622869, acc.: 64.06%] [G loss: 1.113467]\n",
      "epoch:5 step:5042 [D loss: 0.758229, acc.: 46.88%] [G loss: 0.966637]\n",
      "epoch:5 step:5043 [D loss: 0.572098, acc.: 68.75%] [G loss: 0.966852]\n",
      "epoch:5 step:5044 [D loss: 0.657315, acc.: 62.50%] [G loss: 1.102840]\n",
      "epoch:5 step:5045 [D loss: 0.643739, acc.: 62.50%] [G loss: 1.047656]\n",
      "epoch:5 step:5046 [D loss: 0.606469, acc.: 66.41%] [G loss: 1.199447]\n",
      "epoch:5 step:5047 [D loss: 0.562985, acc.: 72.66%] [G loss: 1.055262]\n",
      "epoch:5 step:5048 [D loss: 0.658876, acc.: 56.25%] [G loss: 0.975517]\n",
      "epoch:5 step:5049 [D loss: 0.623588, acc.: 60.16%] [G loss: 0.919015]\n",
      "epoch:5 step:5050 [D loss: 0.577747, acc.: 75.00%] [G loss: 1.172120]\n",
      "epoch:5 step:5051 [D loss: 0.595391, acc.: 67.97%] [G loss: 1.100896]\n",
      "epoch:5 step:5052 [D loss: 0.666099, acc.: 58.59%] [G loss: 1.082603]\n",
      "epoch:5 step:5053 [D loss: 0.565417, acc.: 71.09%] [G loss: 1.192757]\n",
      "epoch:5 step:5054 [D loss: 0.737457, acc.: 53.91%] [G loss: 1.113196]\n",
      "epoch:5 step:5055 [D loss: 0.745264, acc.: 53.91%] [G loss: 1.119734]\n",
      "epoch:5 step:5056 [D loss: 0.596067, acc.: 71.09%] [G loss: 1.098269]\n",
      "epoch:5 step:5057 [D loss: 0.692779, acc.: 57.81%] [G loss: 1.053181]\n",
      "epoch:5 step:5058 [D loss: 0.686612, acc.: 58.59%] [G loss: 0.995766]\n",
      "epoch:5 step:5059 [D loss: 0.636239, acc.: 67.19%] [G loss: 1.058230]\n",
      "epoch:5 step:5060 [D loss: 0.743684, acc.: 52.34%] [G loss: 1.149603]\n",
      "epoch:5 step:5061 [D loss: 0.670627, acc.: 64.06%] [G loss: 1.039893]\n",
      "epoch:5 step:5062 [D loss: 0.624924, acc.: 61.72%] [G loss: 0.832273]\n",
      "epoch:5 step:5063 [D loss: 0.649813, acc.: 62.50%] [G loss: 0.958495]\n",
      "epoch:5 step:5064 [D loss: 0.672902, acc.: 57.81%] [G loss: 1.006396]\n",
      "epoch:5 step:5065 [D loss: 0.638332, acc.: 65.62%] [G loss: 1.074851]\n",
      "epoch:5 step:5066 [D loss: 0.720080, acc.: 53.12%] [G loss: 0.856243]\n",
      "epoch:5 step:5067 [D loss: 0.634875, acc.: 61.72%] [G loss: 0.934782]\n",
      "epoch:5 step:5068 [D loss: 0.583438, acc.: 67.19%] [G loss: 1.026769]\n",
      "epoch:5 step:5069 [D loss: 0.576051, acc.: 73.44%] [G loss: 1.156364]\n",
      "epoch:5 step:5070 [D loss: 0.577187, acc.: 70.31%] [G loss: 1.172513]\n",
      "epoch:5 step:5071 [D loss: 0.651721, acc.: 64.06%] [G loss: 1.021332]\n",
      "epoch:5 step:5072 [D loss: 0.618393, acc.: 66.41%] [G loss: 1.150254]\n",
      "epoch:5 step:5073 [D loss: 0.659322, acc.: 61.72%] [G loss: 0.986819]\n",
      "epoch:5 step:5074 [D loss: 0.601355, acc.: 66.41%] [G loss: 1.004777]\n",
      "epoch:5 step:5075 [D loss: 0.641949, acc.: 61.72%] [G loss: 1.113557]\n",
      "epoch:5 step:5076 [D loss: 0.584363, acc.: 68.75%] [G loss: 1.027866]\n",
      "epoch:5 step:5077 [D loss: 0.642761, acc.: 61.72%] [G loss: 0.847773]\n",
      "epoch:5 step:5078 [D loss: 0.665538, acc.: 55.47%] [G loss: 1.045249]\n",
      "epoch:5 step:5079 [D loss: 0.549563, acc.: 73.44%] [G loss: 1.105998]\n",
      "epoch:5 step:5080 [D loss: 0.557705, acc.: 76.56%] [G loss: 1.221733]\n",
      "epoch:5 step:5081 [D loss: 0.650969, acc.: 59.38%] [G loss: 1.065903]\n",
      "epoch:5 step:5082 [D loss: 0.725692, acc.: 57.03%] [G loss: 0.908542]\n",
      "epoch:5 step:5083 [D loss: 0.663859, acc.: 61.72%] [G loss: 1.044191]\n",
      "epoch:5 step:5084 [D loss: 0.569212, acc.: 72.66%] [G loss: 1.075837]\n",
      "epoch:5 step:5085 [D loss: 0.602132, acc.: 67.97%] [G loss: 1.153128]\n",
      "epoch:5 step:5086 [D loss: 0.637656, acc.: 67.97%] [G loss: 1.179640]\n",
      "epoch:5 step:5087 [D loss: 0.576097, acc.: 69.53%] [G loss: 1.332767]\n",
      "epoch:5 step:5088 [D loss: 0.640253, acc.: 62.50%] [G loss: 1.199753]\n",
      "epoch:5 step:5089 [D loss: 0.634844, acc.: 60.94%] [G loss: 0.999789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5090 [D loss: 0.665280, acc.: 60.94%] [G loss: 0.891238]\n",
      "epoch:5 step:5091 [D loss: 0.560812, acc.: 71.09%] [G loss: 1.058970]\n",
      "epoch:5 step:5092 [D loss: 0.639910, acc.: 66.41%] [G loss: 1.145077]\n",
      "epoch:5 step:5093 [D loss: 0.677540, acc.: 55.47%] [G loss: 1.074308]\n",
      "epoch:5 step:5094 [D loss: 0.543234, acc.: 77.34%] [G loss: 1.064128]\n",
      "epoch:5 step:5095 [D loss: 0.717306, acc.: 53.12%] [G loss: 0.999736]\n",
      "epoch:5 step:5096 [D loss: 0.655009, acc.: 60.94%] [G loss: 0.913368]\n",
      "epoch:5 step:5097 [D loss: 0.641949, acc.: 63.28%] [G loss: 1.231688]\n",
      "epoch:5 step:5098 [D loss: 0.629113, acc.: 64.06%] [G loss: 1.063103]\n",
      "epoch:5 step:5099 [D loss: 0.651032, acc.: 60.94%] [G loss: 1.118999]\n",
      "epoch:5 step:5100 [D loss: 0.729856, acc.: 49.22%] [G loss: 0.884248]\n",
      "epoch:5 step:5101 [D loss: 0.568225, acc.: 71.09%] [G loss: 1.242356]\n",
      "epoch:5 step:5102 [D loss: 0.633275, acc.: 62.50%] [G loss: 0.979837]\n",
      "epoch:5 step:5103 [D loss: 0.719759, acc.: 51.56%] [G loss: 1.132023]\n",
      "epoch:5 step:5104 [D loss: 0.638707, acc.: 60.94%] [G loss: 0.883762]\n",
      "epoch:5 step:5105 [D loss: 0.585924, acc.: 69.53%] [G loss: 1.093743]\n",
      "epoch:5 step:5106 [D loss: 0.691151, acc.: 60.16%] [G loss: 0.947842]\n",
      "epoch:5 step:5107 [D loss: 0.602592, acc.: 68.75%] [G loss: 1.094653]\n",
      "epoch:5 step:5108 [D loss: 0.682947, acc.: 58.59%] [G loss: 1.097137]\n",
      "epoch:5 step:5109 [D loss: 0.528421, acc.: 74.22%] [G loss: 1.101261]\n",
      "epoch:5 step:5110 [D loss: 0.639539, acc.: 67.19%] [G loss: 1.058513]\n",
      "epoch:5 step:5111 [D loss: 0.693337, acc.: 61.72%] [G loss: 0.918684]\n",
      "epoch:5 step:5112 [D loss: 0.704815, acc.: 53.12%] [G loss: 1.079116]\n",
      "epoch:5 step:5113 [D loss: 0.690104, acc.: 54.69%] [G loss: 1.041610]\n",
      "epoch:5 step:5114 [D loss: 0.650210, acc.: 59.38%] [G loss: 0.970118]\n",
      "epoch:5 step:5115 [D loss: 0.680368, acc.: 64.06%] [G loss: 1.054182]\n",
      "epoch:5 step:5116 [D loss: 0.584334, acc.: 68.75%] [G loss: 1.194246]\n",
      "epoch:5 step:5117 [D loss: 0.608622, acc.: 67.19%] [G loss: 1.103670]\n",
      "epoch:5 step:5118 [D loss: 0.641796, acc.: 64.84%] [G loss: 1.155123]\n",
      "epoch:5 step:5119 [D loss: 0.726171, acc.: 50.78%] [G loss: 0.902849]\n",
      "epoch:5 step:5120 [D loss: 0.639997, acc.: 67.19%] [G loss: 1.127464]\n",
      "epoch:5 step:5121 [D loss: 0.685291, acc.: 60.16%] [G loss: 0.958792]\n",
      "epoch:5 step:5122 [D loss: 0.663078, acc.: 55.47%] [G loss: 0.949706]\n",
      "epoch:5 step:5123 [D loss: 0.720722, acc.: 54.69%] [G loss: 1.264675]\n",
      "epoch:5 step:5124 [D loss: 0.555775, acc.: 74.22%] [G loss: 1.120107]\n",
      "epoch:5 step:5125 [D loss: 0.731951, acc.: 56.25%] [G loss: 1.071013]\n",
      "epoch:5 step:5126 [D loss: 0.696000, acc.: 59.38%] [G loss: 1.060491]\n",
      "epoch:5 step:5127 [D loss: 0.582338, acc.: 71.09%] [G loss: 1.243904]\n",
      "epoch:5 step:5128 [D loss: 0.753353, acc.: 50.00%] [G loss: 1.052553]\n",
      "epoch:5 step:5129 [D loss: 0.691403, acc.: 57.03%] [G loss: 1.000021]\n",
      "epoch:5 step:5130 [D loss: 0.509571, acc.: 73.44%] [G loss: 1.108320]\n",
      "epoch:5 step:5131 [D loss: 0.645518, acc.: 63.28%] [G loss: 1.054648]\n",
      "epoch:5 step:5132 [D loss: 0.619307, acc.: 62.50%] [G loss: 1.145023]\n",
      "epoch:5 step:5133 [D loss: 0.685772, acc.: 58.59%] [G loss: 0.980887]\n",
      "epoch:5 step:5134 [D loss: 0.653778, acc.: 56.25%] [G loss: 0.898721]\n",
      "epoch:5 step:5135 [D loss: 0.596171, acc.: 65.62%] [G loss: 1.129157]\n",
      "epoch:5 step:5136 [D loss: 0.605015, acc.: 69.53%] [G loss: 1.188767]\n",
      "epoch:5 step:5137 [D loss: 0.625437, acc.: 66.41%] [G loss: 1.005579]\n",
      "epoch:5 step:5138 [D loss: 0.646816, acc.: 64.06%] [G loss: 1.046506]\n",
      "epoch:5 step:5139 [D loss: 0.617568, acc.: 64.06%] [G loss: 1.133661]\n",
      "epoch:5 step:5140 [D loss: 0.725475, acc.: 54.69%] [G loss: 1.195267]\n",
      "epoch:5 step:5141 [D loss: 0.712594, acc.: 52.34%] [G loss: 0.990042]\n",
      "epoch:5 step:5142 [D loss: 0.693250, acc.: 52.34%] [G loss: 0.959241]\n",
      "epoch:5 step:5143 [D loss: 0.620726, acc.: 64.06%] [G loss: 1.135361]\n",
      "epoch:5 step:5144 [D loss: 0.557538, acc.: 68.75%] [G loss: 1.050732]\n",
      "epoch:5 step:5145 [D loss: 0.667697, acc.: 57.81%] [G loss: 1.040553]\n",
      "epoch:5 step:5146 [D loss: 0.696383, acc.: 56.25%] [G loss: 0.918401]\n",
      "epoch:5 step:5147 [D loss: 0.723832, acc.: 53.12%] [G loss: 0.801270]\n",
      "epoch:5 step:5148 [D loss: 0.696834, acc.: 58.59%] [G loss: 1.066051]\n",
      "epoch:5 step:5149 [D loss: 0.697746, acc.: 57.81%] [G loss: 1.072113]\n",
      "epoch:5 step:5150 [D loss: 0.597169, acc.: 68.75%] [G loss: 1.131877]\n",
      "epoch:5 step:5151 [D loss: 0.624570, acc.: 64.84%] [G loss: 1.031615]\n",
      "epoch:5 step:5152 [D loss: 0.698174, acc.: 54.69%] [G loss: 1.022753]\n",
      "epoch:5 step:5153 [D loss: 0.702658, acc.: 51.56%] [G loss: 1.024876]\n",
      "epoch:5 step:5154 [D loss: 0.604105, acc.: 67.19%] [G loss: 1.152929]\n",
      "epoch:5 step:5155 [D loss: 0.710522, acc.: 52.34%] [G loss: 0.906957]\n",
      "epoch:5 step:5156 [D loss: 0.657015, acc.: 66.41%] [G loss: 1.052388]\n",
      "epoch:5 step:5157 [D loss: 0.599793, acc.: 69.53%] [G loss: 1.029777]\n",
      "epoch:5 step:5158 [D loss: 0.613051, acc.: 64.84%] [G loss: 1.075813]\n",
      "epoch:5 step:5159 [D loss: 0.573884, acc.: 72.66%] [G loss: 1.064250]\n",
      "epoch:5 step:5160 [D loss: 0.505229, acc.: 81.25%] [G loss: 1.114008]\n",
      "epoch:5 step:5161 [D loss: 0.676641, acc.: 60.94%] [G loss: 1.014249]\n",
      "epoch:5 step:5162 [D loss: 0.670245, acc.: 64.06%] [G loss: 1.182448]\n",
      "epoch:5 step:5163 [D loss: 0.660992, acc.: 61.72%] [G loss: 1.030527]\n",
      "epoch:5 step:5164 [D loss: 0.614070, acc.: 65.62%] [G loss: 1.107124]\n",
      "epoch:5 step:5165 [D loss: 0.569646, acc.: 71.88%] [G loss: 1.134349]\n",
      "epoch:5 step:5166 [D loss: 0.653529, acc.: 61.72%] [G loss: 1.145622]\n",
      "epoch:5 step:5167 [D loss: 0.609702, acc.: 67.97%] [G loss: 1.082565]\n",
      "epoch:5 step:5168 [D loss: 0.620013, acc.: 67.97%] [G loss: 1.143284]\n",
      "epoch:5 step:5169 [D loss: 0.554851, acc.: 75.00%] [G loss: 1.038518]\n",
      "epoch:5 step:5170 [D loss: 0.676367, acc.: 58.59%] [G loss: 1.126203]\n",
      "epoch:5 step:5171 [D loss: 0.653760, acc.: 59.38%] [G loss: 1.132380]\n",
      "epoch:5 step:5172 [D loss: 0.616320, acc.: 65.62%] [G loss: 0.947737]\n",
      "epoch:5 step:5173 [D loss: 0.544751, acc.: 73.44%] [G loss: 1.196486]\n",
      "epoch:5 step:5174 [D loss: 0.596001, acc.: 68.75%] [G loss: 0.963614]\n",
      "epoch:5 step:5175 [D loss: 0.679709, acc.: 55.47%] [G loss: 0.940503]\n",
      "epoch:5 step:5176 [D loss: 0.571744, acc.: 69.53%] [G loss: 1.143917]\n",
      "epoch:5 step:5177 [D loss: 0.626190, acc.: 68.75%] [G loss: 1.045241]\n",
      "epoch:5 step:5178 [D loss: 0.678784, acc.: 53.12%] [G loss: 1.112393]\n",
      "epoch:5 step:5179 [D loss: 0.612325, acc.: 68.75%] [G loss: 0.999396]\n",
      "epoch:5 step:5180 [D loss: 0.721194, acc.: 51.56%] [G loss: 1.093521]\n",
      "epoch:5 step:5181 [D loss: 0.584920, acc.: 73.44%] [G loss: 1.075316]\n",
      "epoch:5 step:5182 [D loss: 0.666004, acc.: 62.50%] [G loss: 1.128431]\n",
      "epoch:5 step:5183 [D loss: 0.641962, acc.: 60.16%] [G loss: 1.442729]\n",
      "epoch:5 step:5184 [D loss: 0.591950, acc.: 70.31%] [G loss: 1.101564]\n",
      "epoch:5 step:5185 [D loss: 0.698128, acc.: 55.47%] [G loss: 1.065946]\n",
      "epoch:5 step:5186 [D loss: 0.620337, acc.: 67.97%] [G loss: 1.044860]\n",
      "epoch:5 step:5187 [D loss: 0.660385, acc.: 60.16%] [G loss: 0.941090]\n",
      "epoch:5 step:5188 [D loss: 0.584994, acc.: 68.75%] [G loss: 1.086218]\n",
      "epoch:5 step:5189 [D loss: 0.635933, acc.: 60.94%] [G loss: 1.156879]\n",
      "epoch:5 step:5190 [D loss: 0.650879, acc.: 59.38%] [G loss: 1.104464]\n",
      "epoch:5 step:5191 [D loss: 0.666086, acc.: 60.16%] [G loss: 0.852472]\n",
      "epoch:5 step:5192 [D loss: 0.616067, acc.: 66.41%] [G loss: 1.207078]\n",
      "epoch:5 step:5193 [D loss: 0.654551, acc.: 57.03%] [G loss: 0.998196]\n",
      "epoch:5 step:5194 [D loss: 0.677672, acc.: 58.59%] [G loss: 1.008866]\n",
      "epoch:5 step:5195 [D loss: 0.642119, acc.: 61.72%] [G loss: 1.135395]\n",
      "epoch:5 step:5196 [D loss: 0.604400, acc.: 67.97%] [G loss: 1.080903]\n",
      "epoch:5 step:5197 [D loss: 0.612713, acc.: 65.62%] [G loss: 1.177619]\n",
      "epoch:5 step:5198 [D loss: 0.582401, acc.: 66.41%] [G loss: 1.139238]\n",
      "epoch:5 step:5199 [D loss: 0.641068, acc.: 67.19%] [G loss: 1.124167]\n",
      "epoch:5 step:5200 [D loss: 0.594608, acc.: 65.62%] [G loss: 1.102793]\n",
      "##############\n",
      "[2.52716634 1.93752702 1.84958958 2.97261559 0.58350352 6.40458042\n",
      " 1.84329151 3.06169945 3.69910408 5.81561218]\n",
      "##########\n",
      "epoch:5 step:5201 [D loss: 0.673538, acc.: 60.94%] [G loss: 1.035664]\n",
      "epoch:5 step:5202 [D loss: 0.631053, acc.: 64.06%] [G loss: 1.219626]\n",
      "epoch:5 step:5203 [D loss: 0.657993, acc.: 58.59%] [G loss: 1.082583]\n",
      "epoch:5 step:5204 [D loss: 0.621844, acc.: 62.50%] [G loss: 1.039874]\n",
      "epoch:5 step:5205 [D loss: 0.674755, acc.: 58.59%] [G loss: 1.066178]\n",
      "epoch:5 step:5206 [D loss: 0.591648, acc.: 68.75%] [G loss: 1.200524]\n",
      "epoch:5 step:5207 [D loss: 0.660836, acc.: 62.50%] [G loss: 1.063581]\n",
      "epoch:5 step:5208 [D loss: 0.611257, acc.: 67.97%] [G loss: 0.877839]\n",
      "epoch:5 step:5209 [D loss: 0.559135, acc.: 71.09%] [G loss: 1.086939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5210 [D loss: 0.621300, acc.: 65.62%] [G loss: 1.020453]\n",
      "epoch:5 step:5211 [D loss: 0.698926, acc.: 57.81%] [G loss: 0.984605]\n",
      "epoch:5 step:5212 [D loss: 0.679272, acc.: 60.94%] [G loss: 0.957651]\n",
      "epoch:5 step:5213 [D loss: 0.598144, acc.: 69.53%] [G loss: 1.093940]\n",
      "epoch:5 step:5214 [D loss: 0.575506, acc.: 70.31%] [G loss: 1.133325]\n",
      "epoch:5 step:5215 [D loss: 0.616170, acc.: 67.19%] [G loss: 1.061010]\n",
      "epoch:5 step:5216 [D loss: 0.639601, acc.: 63.28%] [G loss: 1.019826]\n",
      "epoch:5 step:5217 [D loss: 0.700044, acc.: 57.81%] [G loss: 0.903089]\n",
      "epoch:5 step:5218 [D loss: 0.709673, acc.: 51.56%] [G loss: 0.973300]\n",
      "epoch:5 step:5219 [D loss: 0.643714, acc.: 63.28%] [G loss: 1.047539]\n",
      "epoch:5 step:5220 [D loss: 0.682966, acc.: 52.34%] [G loss: 1.094504]\n",
      "epoch:5 step:5221 [D loss: 0.632831, acc.: 65.62%] [G loss: 1.142934]\n",
      "epoch:5 step:5222 [D loss: 0.746255, acc.: 53.91%] [G loss: 0.850511]\n",
      "epoch:5 step:5223 [D loss: 0.719912, acc.: 57.81%] [G loss: 1.091844]\n",
      "epoch:5 step:5224 [D loss: 0.612780, acc.: 62.50%] [G loss: 1.223403]\n",
      "epoch:5 step:5225 [D loss: 0.668048, acc.: 58.59%] [G loss: 1.211277]\n",
      "epoch:5 step:5226 [D loss: 0.670084, acc.: 61.72%] [G loss: 1.147552]\n",
      "epoch:5 step:5227 [D loss: 0.640607, acc.: 61.72%] [G loss: 1.054649]\n",
      "epoch:5 step:5228 [D loss: 0.653050, acc.: 60.94%] [G loss: 1.176173]\n",
      "epoch:5 step:5229 [D loss: 0.691363, acc.: 58.59%] [G loss: 1.025748]\n",
      "epoch:5 step:5230 [D loss: 0.612305, acc.: 67.19%] [G loss: 0.999096]\n",
      "epoch:5 step:5231 [D loss: 0.568181, acc.: 70.31%] [G loss: 1.002045]\n",
      "epoch:5 step:5232 [D loss: 0.631056, acc.: 67.97%] [G loss: 1.028931]\n",
      "epoch:5 step:5233 [D loss: 0.622521, acc.: 63.28%] [G loss: 1.129753]\n",
      "epoch:5 step:5234 [D loss: 0.646076, acc.: 64.06%] [G loss: 0.976246]\n",
      "epoch:5 step:5235 [D loss: 0.626564, acc.: 63.28%] [G loss: 1.066705]\n",
      "epoch:5 step:5236 [D loss: 0.588207, acc.: 71.88%] [G loss: 1.290875]\n",
      "epoch:5 step:5237 [D loss: 0.664161, acc.: 63.28%] [G loss: 1.021171]\n",
      "epoch:5 step:5238 [D loss: 0.682974, acc.: 59.38%] [G loss: 1.092723]\n",
      "epoch:5 step:5239 [D loss: 0.628950, acc.: 66.41%] [G loss: 1.089810]\n",
      "epoch:5 step:5240 [D loss: 0.593499, acc.: 71.09%] [G loss: 1.366519]\n",
      "epoch:5 step:5241 [D loss: 0.676684, acc.: 56.25%] [G loss: 1.035110]\n",
      "epoch:5 step:5242 [D loss: 0.644595, acc.: 60.94%] [G loss: 1.074022]\n",
      "epoch:5 step:5243 [D loss: 0.591632, acc.: 67.19%] [G loss: 0.969979]\n",
      "epoch:5 step:5244 [D loss: 0.569435, acc.: 69.53%] [G loss: 1.129078]\n",
      "epoch:5 step:5245 [D loss: 0.637144, acc.: 60.94%] [G loss: 1.227594]\n",
      "epoch:5 step:5246 [D loss: 0.680272, acc.: 58.59%] [G loss: 1.009756]\n",
      "epoch:5 step:5247 [D loss: 0.661315, acc.: 60.16%] [G loss: 1.053088]\n",
      "epoch:5 step:5248 [D loss: 0.635880, acc.: 62.50%] [G loss: 1.108755]\n",
      "epoch:5 step:5249 [D loss: 0.513709, acc.: 75.78%] [G loss: 1.226107]\n",
      "epoch:5 step:5250 [D loss: 0.656139, acc.: 60.16%] [G loss: 0.912873]\n",
      "epoch:5 step:5251 [D loss: 0.739655, acc.: 55.47%] [G loss: 1.149315]\n",
      "epoch:5 step:5252 [D loss: 0.673889, acc.: 62.50%] [G loss: 1.049559]\n",
      "epoch:5 step:5253 [D loss: 0.613107, acc.: 63.28%] [G loss: 1.016426]\n",
      "epoch:5 step:5254 [D loss: 0.672828, acc.: 60.94%] [G loss: 1.010964]\n",
      "epoch:5 step:5255 [D loss: 0.628184, acc.: 64.84%] [G loss: 1.013143]\n",
      "epoch:5 step:5256 [D loss: 0.637569, acc.: 64.06%] [G loss: 1.248381]\n",
      "epoch:5 step:5257 [D loss: 0.589009, acc.: 66.41%] [G loss: 1.018605]\n",
      "epoch:5 step:5258 [D loss: 0.739531, acc.: 52.34%] [G loss: 1.251636]\n",
      "epoch:5 step:5259 [D loss: 0.658387, acc.: 60.16%] [G loss: 1.053553]\n",
      "epoch:5 step:5260 [D loss: 0.680332, acc.: 60.16%] [G loss: 1.024212]\n",
      "epoch:5 step:5261 [D loss: 0.581206, acc.: 71.09%] [G loss: 1.112257]\n",
      "epoch:5 step:5262 [D loss: 0.596246, acc.: 68.75%] [G loss: 1.082726]\n",
      "epoch:5 step:5263 [D loss: 0.673320, acc.: 58.59%] [G loss: 1.050677]\n",
      "epoch:5 step:5264 [D loss: 0.591228, acc.: 69.53%] [G loss: 1.129137]\n",
      "epoch:5 step:5265 [D loss: 0.628204, acc.: 64.84%] [G loss: 1.056881]\n",
      "epoch:5 step:5266 [D loss: 0.720285, acc.: 53.91%] [G loss: 0.986565]\n",
      "epoch:5 step:5267 [D loss: 0.663110, acc.: 61.72%] [G loss: 0.993403]\n",
      "epoch:5 step:5268 [D loss: 0.581160, acc.: 72.66%] [G loss: 1.124153]\n",
      "epoch:5 step:5269 [D loss: 0.668782, acc.: 59.38%] [G loss: 1.025910]\n",
      "epoch:5 step:5270 [D loss: 0.587856, acc.: 68.75%] [G loss: 1.140364]\n",
      "epoch:5 step:5271 [D loss: 0.657085, acc.: 61.72%] [G loss: 1.093143]\n",
      "epoch:5 step:5272 [D loss: 0.640484, acc.: 63.28%] [G loss: 0.968871]\n",
      "epoch:5 step:5273 [D loss: 0.614824, acc.: 65.62%] [G loss: 1.036177]\n",
      "epoch:5 step:5274 [D loss: 0.667424, acc.: 57.03%] [G loss: 1.129133]\n",
      "epoch:5 step:5275 [D loss: 0.646938, acc.: 64.06%] [G loss: 1.149397]\n",
      "epoch:5 step:5276 [D loss: 0.679098, acc.: 55.47%] [G loss: 1.005445]\n",
      "epoch:5 step:5277 [D loss: 0.625796, acc.: 64.06%] [G loss: 1.034800]\n",
      "epoch:5 step:5278 [D loss: 0.760210, acc.: 52.34%] [G loss: 1.044749]\n",
      "epoch:5 step:5279 [D loss: 0.639470, acc.: 67.19%] [G loss: 0.925538]\n",
      "epoch:5 step:5280 [D loss: 0.607504, acc.: 69.53%] [G loss: 1.046528]\n",
      "epoch:5 step:5281 [D loss: 0.700006, acc.: 56.25%] [G loss: 0.960256]\n",
      "epoch:5 step:5282 [D loss: 0.592897, acc.: 74.22%] [G loss: 1.021245]\n",
      "epoch:5 step:5283 [D loss: 0.769308, acc.: 47.66%] [G loss: 0.874853]\n",
      "epoch:5 step:5284 [D loss: 0.635858, acc.: 61.72%] [G loss: 1.102048]\n",
      "epoch:5 step:5285 [D loss: 0.605279, acc.: 63.28%] [G loss: 1.033057]\n",
      "epoch:5 step:5286 [D loss: 0.632481, acc.: 62.50%] [G loss: 1.250640]\n",
      "epoch:5 step:5287 [D loss: 0.569836, acc.: 72.66%] [G loss: 1.106596]\n",
      "epoch:5 step:5288 [D loss: 0.643664, acc.: 60.94%] [G loss: 1.138455]\n",
      "epoch:5 step:5289 [D loss: 0.657700, acc.: 52.34%] [G loss: 0.969513]\n",
      "epoch:5 step:5290 [D loss: 0.626495, acc.: 67.97%] [G loss: 1.001294]\n",
      "epoch:5 step:5291 [D loss: 0.726257, acc.: 53.91%] [G loss: 1.108464]\n",
      "epoch:5 step:5292 [D loss: 0.564635, acc.: 70.31%] [G loss: 1.088710]\n",
      "epoch:5 step:5293 [D loss: 0.490437, acc.: 75.78%] [G loss: 1.170547]\n",
      "epoch:5 step:5294 [D loss: 0.506627, acc.: 78.12%] [G loss: 1.206259]\n",
      "epoch:5 step:5295 [D loss: 0.609713, acc.: 67.19%] [G loss: 1.118770]\n",
      "epoch:5 step:5296 [D loss: 0.611225, acc.: 63.28%] [G loss: 0.855879]\n",
      "epoch:5 step:5297 [D loss: 0.560684, acc.: 73.44%] [G loss: 1.220775]\n",
      "epoch:5 step:5298 [D loss: 0.614747, acc.: 66.41%] [G loss: 1.243526]\n",
      "epoch:5 step:5299 [D loss: 0.626446, acc.: 64.06%] [G loss: 1.191261]\n",
      "epoch:5 step:5300 [D loss: 0.579198, acc.: 75.00%] [G loss: 1.223538]\n",
      "epoch:5 step:5301 [D loss: 0.578455, acc.: 70.31%] [G loss: 1.201798]\n",
      "epoch:5 step:5302 [D loss: 0.706866, acc.: 53.12%] [G loss: 1.110077]\n",
      "epoch:5 step:5303 [D loss: 0.679496, acc.: 60.16%] [G loss: 1.077214]\n",
      "epoch:5 step:5304 [D loss: 0.613436, acc.: 66.41%] [G loss: 1.338686]\n",
      "epoch:5 step:5305 [D loss: 0.688591, acc.: 60.16%] [G loss: 0.934503]\n",
      "epoch:5 step:5306 [D loss: 0.669600, acc.: 60.16%] [G loss: 0.960929]\n",
      "epoch:5 step:5307 [D loss: 0.616411, acc.: 64.84%] [G loss: 1.063222]\n",
      "epoch:5 step:5308 [D loss: 0.631791, acc.: 64.84%] [G loss: 0.989004]\n",
      "epoch:5 step:5309 [D loss: 0.614616, acc.: 64.84%] [G loss: 1.030169]\n",
      "epoch:5 step:5310 [D loss: 0.586742, acc.: 68.75%] [G loss: 1.306460]\n",
      "epoch:5 step:5311 [D loss: 0.647487, acc.: 64.06%] [G loss: 0.981775]\n",
      "epoch:5 step:5312 [D loss: 0.562253, acc.: 70.31%] [G loss: 1.002622]\n",
      "epoch:5 step:5313 [D loss: 0.674810, acc.: 55.47%] [G loss: 1.002813]\n",
      "epoch:5 step:5314 [D loss: 0.582395, acc.: 68.75%] [G loss: 1.173835]\n",
      "epoch:5 step:5315 [D loss: 0.686419, acc.: 60.94%] [G loss: 0.954740]\n",
      "epoch:5 step:5316 [D loss: 0.611467, acc.: 68.75%] [G loss: 1.041644]\n",
      "epoch:5 step:5317 [D loss: 0.679683, acc.: 64.06%] [G loss: 1.090189]\n",
      "epoch:5 step:5318 [D loss: 0.620365, acc.: 66.41%] [G loss: 1.075405]\n",
      "epoch:5 step:5319 [D loss: 0.628313, acc.: 65.62%] [G loss: 0.886679]\n",
      "epoch:5 step:5320 [D loss: 0.661971, acc.: 64.06%] [G loss: 0.985962]\n",
      "epoch:5 step:5321 [D loss: 0.600467, acc.: 71.88%] [G loss: 1.066339]\n",
      "epoch:5 step:5322 [D loss: 0.666288, acc.: 56.25%] [G loss: 0.908919]\n",
      "epoch:5 step:5323 [D loss: 0.762351, acc.: 50.78%] [G loss: 0.935893]\n",
      "epoch:5 step:5324 [D loss: 0.644148, acc.: 61.72%] [G loss: 1.095280]\n",
      "epoch:5 step:5325 [D loss: 0.603495, acc.: 68.75%] [G loss: 1.040416]\n",
      "epoch:5 step:5326 [D loss: 0.653278, acc.: 59.38%] [G loss: 0.903210]\n",
      "epoch:5 step:5327 [D loss: 0.559106, acc.: 70.31%] [G loss: 1.106168]\n",
      "epoch:5 step:5328 [D loss: 0.603057, acc.: 64.84%] [G loss: 1.032823]\n",
      "epoch:5 step:5329 [D loss: 0.625105, acc.: 64.06%] [G loss: 1.076869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5330 [D loss: 0.618594, acc.: 64.84%] [G loss: 1.053555]\n",
      "epoch:5 step:5331 [D loss: 0.590880, acc.: 68.75%] [G loss: 0.985641]\n",
      "epoch:5 step:5332 [D loss: 0.700364, acc.: 55.47%] [G loss: 1.084712]\n",
      "epoch:5 step:5333 [D loss: 0.571774, acc.: 71.88%] [G loss: 1.073809]\n",
      "epoch:5 step:5334 [D loss: 0.670020, acc.: 57.03%] [G loss: 1.051207]\n",
      "epoch:5 step:5335 [D loss: 0.582752, acc.: 75.00%] [G loss: 1.062244]\n",
      "epoch:5 step:5336 [D loss: 0.667772, acc.: 60.16%] [G loss: 1.118362]\n",
      "epoch:5 step:5337 [D loss: 0.609153, acc.: 68.75%] [G loss: 1.228385]\n",
      "epoch:5 step:5338 [D loss: 0.597525, acc.: 69.53%] [G loss: 1.167300]\n",
      "epoch:5 step:5339 [D loss: 0.653072, acc.: 58.59%] [G loss: 1.013411]\n",
      "epoch:5 step:5340 [D loss: 0.642936, acc.: 64.06%] [G loss: 1.238579]\n",
      "epoch:5 step:5341 [D loss: 0.574860, acc.: 74.22%] [G loss: 1.162719]\n",
      "epoch:5 step:5342 [D loss: 0.591157, acc.: 69.53%] [G loss: 1.136246]\n",
      "epoch:5 step:5343 [D loss: 0.661899, acc.: 61.72%] [G loss: 1.092172]\n",
      "epoch:5 step:5344 [D loss: 0.619593, acc.: 64.84%] [G loss: 1.002643]\n",
      "epoch:5 step:5345 [D loss: 0.621285, acc.: 61.72%] [G loss: 1.019859]\n",
      "epoch:5 step:5346 [D loss: 0.627107, acc.: 64.06%] [G loss: 0.959800]\n",
      "epoch:5 step:5347 [D loss: 0.698291, acc.: 55.47%] [G loss: 1.166676]\n",
      "epoch:5 step:5348 [D loss: 0.611507, acc.: 68.75%] [G loss: 1.099786]\n",
      "epoch:5 step:5349 [D loss: 0.592942, acc.: 65.62%] [G loss: 1.134955]\n",
      "epoch:5 step:5350 [D loss: 0.684203, acc.: 62.50%] [G loss: 1.037165]\n",
      "epoch:5 step:5351 [D loss: 0.579274, acc.: 73.44%] [G loss: 0.979715]\n",
      "epoch:5 step:5352 [D loss: 0.682230, acc.: 64.06%] [G loss: 0.906673]\n",
      "epoch:5 step:5353 [D loss: 0.712327, acc.: 57.03%] [G loss: 1.022538]\n",
      "epoch:5 step:5354 [D loss: 0.641821, acc.: 65.62%] [G loss: 0.890064]\n",
      "epoch:5 step:5355 [D loss: 0.522321, acc.: 78.12%] [G loss: 1.316918]\n",
      "epoch:5 step:5356 [D loss: 0.624085, acc.: 67.97%] [G loss: 1.160645]\n",
      "epoch:5 step:5357 [D loss: 0.619245, acc.: 67.97%] [G loss: 1.014415]\n",
      "epoch:5 step:5358 [D loss: 0.758929, acc.: 52.34%] [G loss: 0.978641]\n",
      "epoch:5 step:5359 [D loss: 0.660645, acc.: 64.84%] [G loss: 0.956609]\n",
      "epoch:5 step:5360 [D loss: 0.635676, acc.: 64.84%] [G loss: 1.147038]\n",
      "epoch:5 step:5361 [D loss: 0.665281, acc.: 64.06%] [G loss: 1.181966]\n",
      "epoch:5 step:5362 [D loss: 0.641988, acc.: 63.28%] [G loss: 1.182052]\n",
      "epoch:5 step:5363 [D loss: 0.617938, acc.: 64.06%] [G loss: 1.039283]\n",
      "epoch:5 step:5364 [D loss: 0.631905, acc.: 61.72%] [G loss: 1.147825]\n",
      "epoch:5 step:5365 [D loss: 0.601766, acc.: 67.19%] [G loss: 1.227363]\n",
      "epoch:5 step:5366 [D loss: 0.620164, acc.: 64.84%] [G loss: 1.146662]\n",
      "epoch:5 step:5367 [D loss: 0.619254, acc.: 60.94%] [G loss: 1.099701]\n",
      "epoch:5 step:5368 [D loss: 0.590699, acc.: 66.41%] [G loss: 1.143892]\n",
      "epoch:5 step:5369 [D loss: 0.645510, acc.: 57.03%] [G loss: 1.036780]\n",
      "epoch:5 step:5370 [D loss: 0.560905, acc.: 73.44%] [G loss: 0.951795]\n",
      "epoch:5 step:5371 [D loss: 0.597405, acc.: 70.31%] [G loss: 1.159833]\n",
      "epoch:5 step:5372 [D loss: 0.610644, acc.: 64.84%] [G loss: 1.193969]\n",
      "epoch:5 step:5373 [D loss: 0.655503, acc.: 64.06%] [G loss: 1.107203]\n",
      "epoch:5 step:5374 [D loss: 0.701027, acc.: 58.59%] [G loss: 1.129694]\n",
      "epoch:5 step:5375 [D loss: 0.602280, acc.: 71.88%] [G loss: 1.016510]\n",
      "epoch:5 step:5376 [D loss: 0.652410, acc.: 60.16%] [G loss: 1.115698]\n",
      "epoch:5 step:5377 [D loss: 0.655139, acc.: 60.16%] [G loss: 0.989343]\n",
      "epoch:5 step:5378 [D loss: 0.669182, acc.: 56.25%] [G loss: 1.146625]\n",
      "epoch:5 step:5379 [D loss: 0.721945, acc.: 52.34%] [G loss: 1.064515]\n",
      "epoch:5 step:5380 [D loss: 0.653554, acc.: 59.38%] [G loss: 1.091582]\n",
      "epoch:5 step:5381 [D loss: 0.608688, acc.: 68.75%] [G loss: 0.954804]\n",
      "epoch:5 step:5382 [D loss: 0.529762, acc.: 78.12%] [G loss: 1.161006]\n",
      "epoch:5 step:5383 [D loss: 0.702641, acc.: 56.25%] [G loss: 0.919511]\n",
      "epoch:5 step:5384 [D loss: 0.624198, acc.: 66.41%] [G loss: 1.048486]\n",
      "epoch:5 step:5385 [D loss: 0.616300, acc.: 59.38%] [G loss: 1.381662]\n",
      "epoch:5 step:5386 [D loss: 0.644934, acc.: 59.38%] [G loss: 1.152467]\n",
      "epoch:5 step:5387 [D loss: 0.577972, acc.: 71.09%] [G loss: 1.235513]\n",
      "epoch:5 step:5388 [D loss: 0.657069, acc.: 60.16%] [G loss: 1.104861]\n",
      "epoch:5 step:5389 [D loss: 0.650398, acc.: 63.28%] [G loss: 1.205258]\n",
      "epoch:5 step:5390 [D loss: 0.604501, acc.: 64.84%] [G loss: 1.131743]\n",
      "epoch:5 step:5391 [D loss: 0.615012, acc.: 69.53%] [G loss: 1.242300]\n",
      "epoch:5 step:5392 [D loss: 0.654849, acc.: 60.16%] [G loss: 1.086852]\n",
      "epoch:5 step:5393 [D loss: 0.611338, acc.: 64.06%] [G loss: 1.089803]\n",
      "epoch:5 step:5394 [D loss: 0.623260, acc.: 64.06%] [G loss: 1.211272]\n",
      "epoch:5 step:5395 [D loss: 0.623013, acc.: 66.41%] [G loss: 0.881105]\n",
      "epoch:5 step:5396 [D loss: 0.591203, acc.: 71.09%] [G loss: 0.965506]\n",
      "epoch:5 step:5397 [D loss: 0.561736, acc.: 70.31%] [G loss: 1.183760]\n",
      "epoch:5 step:5398 [D loss: 0.657070, acc.: 60.94%] [G loss: 0.928783]\n",
      "epoch:5 step:5399 [D loss: 0.701512, acc.: 56.25%] [G loss: 1.126708]\n",
      "epoch:5 step:5400 [D loss: 0.736429, acc.: 48.44%] [G loss: 1.214243]\n",
      "##############\n",
      "[2.6448915  2.01111224 2.05202133 3.11857611 0.86484481 6.08220291\n",
      " 1.90790812 3.05125657 3.80079048 5.1412572 ]\n",
      "##########\n",
      "epoch:5 step:5401 [D loss: 0.538434, acc.: 74.22%] [G loss: 1.101883]\n",
      "epoch:5 step:5402 [D loss: 0.629112, acc.: 65.62%] [G loss: 0.996008]\n",
      "epoch:5 step:5403 [D loss: 0.564229, acc.: 68.75%] [G loss: 1.002406]\n",
      "epoch:5 step:5404 [D loss: 0.628629, acc.: 70.31%] [G loss: 1.030345]\n",
      "epoch:5 step:5405 [D loss: 0.585947, acc.: 69.53%] [G loss: 1.163977]\n",
      "epoch:5 step:5406 [D loss: 0.585428, acc.: 67.19%] [G loss: 1.082664]\n",
      "epoch:5 step:5407 [D loss: 0.696516, acc.: 54.69%] [G loss: 0.907545]\n",
      "epoch:5 step:5408 [D loss: 0.650702, acc.: 67.19%] [G loss: 0.935566]\n",
      "epoch:5 step:5409 [D loss: 0.608924, acc.: 65.62%] [G loss: 1.048981]\n",
      "epoch:5 step:5410 [D loss: 0.602794, acc.: 68.75%] [G loss: 1.117467]\n",
      "epoch:5 step:5411 [D loss: 0.613206, acc.: 68.75%] [G loss: 1.119775]\n",
      "epoch:5 step:5412 [D loss: 0.573638, acc.: 66.41%] [G loss: 1.180790]\n",
      "epoch:5 step:5413 [D loss: 0.675752, acc.: 57.03%] [G loss: 0.875479]\n",
      "epoch:5 step:5414 [D loss: 0.669193, acc.: 57.81%] [G loss: 1.000322]\n",
      "epoch:5 step:5415 [D loss: 0.725682, acc.: 52.34%] [G loss: 1.148932]\n",
      "epoch:5 step:5416 [D loss: 0.603935, acc.: 71.09%] [G loss: 1.368805]\n",
      "epoch:5 step:5417 [D loss: 0.736335, acc.: 50.00%] [G loss: 0.941474]\n",
      "epoch:5 step:5418 [D loss: 0.644005, acc.: 62.50%] [G loss: 0.887040]\n",
      "epoch:5 step:5419 [D loss: 0.593115, acc.: 70.31%] [G loss: 1.074067]\n",
      "epoch:5 step:5420 [D loss: 0.711238, acc.: 49.22%] [G loss: 0.968300]\n",
      "epoch:5 step:5421 [D loss: 0.725922, acc.: 55.47%] [G loss: 0.955289]\n",
      "epoch:5 step:5422 [D loss: 0.724725, acc.: 53.91%] [G loss: 1.203344]\n",
      "epoch:5 step:5423 [D loss: 0.614373, acc.: 66.41%] [G loss: 1.266442]\n",
      "epoch:5 step:5424 [D loss: 0.611563, acc.: 65.62%] [G loss: 1.033574]\n",
      "epoch:5 step:5425 [D loss: 0.582681, acc.: 71.88%] [G loss: 1.097520]\n",
      "epoch:5 step:5426 [D loss: 0.611427, acc.: 70.31%] [G loss: 1.013653]\n",
      "epoch:5 step:5427 [D loss: 0.577716, acc.: 69.53%] [G loss: 1.133397]\n",
      "epoch:5 step:5428 [D loss: 0.692643, acc.: 57.81%] [G loss: 1.041901]\n",
      "epoch:5 step:5429 [D loss: 0.642564, acc.: 64.06%] [G loss: 0.901575]\n",
      "epoch:5 step:5430 [D loss: 0.644203, acc.: 63.28%] [G loss: 1.174121]\n",
      "epoch:5 step:5431 [D loss: 0.608675, acc.: 64.06%] [G loss: 1.148601]\n",
      "epoch:5 step:5432 [D loss: 0.608280, acc.: 67.97%] [G loss: 1.069383]\n",
      "epoch:5 step:5433 [D loss: 0.540853, acc.: 76.56%] [G loss: 1.119052]\n",
      "epoch:5 step:5434 [D loss: 0.628378, acc.: 63.28%] [G loss: 1.125987]\n",
      "epoch:5 step:5435 [D loss: 0.723449, acc.: 55.47%] [G loss: 0.892544]\n",
      "epoch:5 step:5436 [D loss: 0.552041, acc.: 75.00%] [G loss: 1.255493]\n",
      "epoch:5 step:5437 [D loss: 0.614973, acc.: 60.16%] [G loss: 1.054112]\n",
      "epoch:5 step:5438 [D loss: 0.615241, acc.: 64.06%] [G loss: 1.101320]\n",
      "epoch:5 step:5439 [D loss: 0.628806, acc.: 60.94%] [G loss: 1.070079]\n",
      "epoch:5 step:5440 [D loss: 0.670588, acc.: 57.03%] [G loss: 1.015704]\n",
      "epoch:5 step:5441 [D loss: 0.580438, acc.: 67.97%] [G loss: 0.998780]\n",
      "epoch:5 step:5442 [D loss: 0.543547, acc.: 77.34%] [G loss: 1.054743]\n",
      "epoch:5 step:5443 [D loss: 0.585401, acc.: 72.66%] [G loss: 1.157271]\n",
      "epoch:5 step:5444 [D loss: 0.562681, acc.: 71.09%] [G loss: 1.126552]\n",
      "epoch:5 step:5445 [D loss: 0.667266, acc.: 58.59%] [G loss: 0.990042]\n",
      "epoch:5 step:5446 [D loss: 0.669684, acc.: 62.50%] [G loss: 1.070766]\n",
      "epoch:5 step:5447 [D loss: 0.602624, acc.: 69.53%] [G loss: 1.209624]\n",
      "epoch:5 step:5448 [D loss: 0.589409, acc.: 69.53%] [G loss: 1.267149]\n",
      "epoch:5 step:5449 [D loss: 0.682065, acc.: 58.59%] [G loss: 1.076845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5450 [D loss: 0.648797, acc.: 62.50%] [G loss: 1.089534]\n",
      "epoch:5 step:5451 [D loss: 0.645071, acc.: 61.72%] [G loss: 1.225480]\n",
      "epoch:5 step:5452 [D loss: 0.664785, acc.: 64.06%] [G loss: 1.232461]\n",
      "epoch:5 step:5453 [D loss: 0.666996, acc.: 60.94%] [G loss: 1.126781]\n",
      "epoch:5 step:5454 [D loss: 0.693779, acc.: 53.91%] [G loss: 0.927506]\n",
      "epoch:5 step:5455 [D loss: 0.739227, acc.: 50.00%] [G loss: 1.028650]\n",
      "epoch:5 step:5456 [D loss: 0.606692, acc.: 68.75%] [G loss: 0.980697]\n",
      "epoch:5 step:5457 [D loss: 0.609988, acc.: 65.62%] [G loss: 0.848038]\n",
      "epoch:5 step:5458 [D loss: 0.676246, acc.: 57.81%] [G loss: 0.979380]\n",
      "epoch:5 step:5459 [D loss: 0.575746, acc.: 70.31%] [G loss: 1.075780]\n",
      "epoch:5 step:5460 [D loss: 0.637834, acc.: 60.16%] [G loss: 1.192646]\n",
      "epoch:5 step:5461 [D loss: 0.564280, acc.: 70.31%] [G loss: 1.262734]\n",
      "epoch:5 step:5462 [D loss: 0.604128, acc.: 68.75%] [G loss: 1.087054]\n",
      "epoch:5 step:5463 [D loss: 0.586328, acc.: 66.41%] [G loss: 1.098948]\n",
      "epoch:5 step:5464 [D loss: 0.618204, acc.: 69.53%] [G loss: 1.188852]\n",
      "epoch:5 step:5465 [D loss: 0.760112, acc.: 50.78%] [G loss: 0.994826]\n",
      "epoch:5 step:5466 [D loss: 0.669791, acc.: 61.72%] [G loss: 0.920556]\n",
      "epoch:5 step:5467 [D loss: 0.607863, acc.: 67.19%] [G loss: 1.157133]\n",
      "epoch:5 step:5468 [D loss: 0.623958, acc.: 64.06%] [G loss: 1.057796]\n",
      "epoch:5 step:5469 [D loss: 0.684957, acc.: 60.94%] [G loss: 0.995597]\n",
      "epoch:5 step:5470 [D loss: 0.600185, acc.: 71.88%] [G loss: 1.050454]\n",
      "epoch:5 step:5471 [D loss: 0.688098, acc.: 60.16%] [G loss: 1.132924]\n",
      "epoch:5 step:5472 [D loss: 0.546805, acc.: 77.34%] [G loss: 1.204477]\n",
      "epoch:5 step:5473 [D loss: 0.610961, acc.: 66.41%] [G loss: 1.218918]\n",
      "epoch:5 step:5474 [D loss: 0.586239, acc.: 70.31%] [G loss: 1.043085]\n",
      "epoch:5 step:5475 [D loss: 0.607725, acc.: 64.84%] [G loss: 0.988752]\n",
      "epoch:5 step:5476 [D loss: 0.559747, acc.: 72.66%] [G loss: 1.120040]\n",
      "epoch:5 step:5477 [D loss: 0.572548, acc.: 70.31%] [G loss: 1.098024]\n",
      "epoch:5 step:5478 [D loss: 0.688768, acc.: 58.59%] [G loss: 1.105533]\n",
      "epoch:5 step:5479 [D loss: 0.596288, acc.: 70.31%] [G loss: 1.041066]\n",
      "epoch:5 step:5480 [D loss: 0.656807, acc.: 64.84%] [G loss: 1.157574]\n",
      "epoch:5 step:5481 [D loss: 0.618586, acc.: 67.19%] [G loss: 1.020765]\n",
      "epoch:5 step:5482 [D loss: 0.680070, acc.: 59.38%] [G loss: 1.058137]\n",
      "epoch:5 step:5483 [D loss: 0.693800, acc.: 57.81%] [G loss: 1.060813]\n",
      "epoch:5 step:5484 [D loss: 0.586440, acc.: 72.66%] [G loss: 1.026916]\n",
      "epoch:5 step:5485 [D loss: 0.669428, acc.: 60.16%] [G loss: 0.997635]\n",
      "epoch:5 step:5486 [D loss: 0.602005, acc.: 70.31%] [G loss: 1.178452]\n",
      "epoch:5 step:5487 [D loss: 0.686788, acc.: 63.28%] [G loss: 1.102745]\n",
      "epoch:5 step:5488 [D loss: 0.623565, acc.: 64.84%] [G loss: 1.008479]\n",
      "epoch:5 step:5489 [D loss: 0.562781, acc.: 67.19%] [G loss: 1.196105]\n",
      "epoch:5 step:5490 [D loss: 0.766144, acc.: 48.44%] [G loss: 1.111117]\n",
      "epoch:5 step:5491 [D loss: 0.678940, acc.: 60.16%] [G loss: 1.076298]\n",
      "epoch:5 step:5492 [D loss: 0.606427, acc.: 64.84%] [G loss: 1.053463]\n",
      "epoch:5 step:5493 [D loss: 0.524243, acc.: 75.00%] [G loss: 1.247964]\n",
      "epoch:5 step:5494 [D loss: 0.558924, acc.: 67.97%] [G loss: 1.255763]\n",
      "epoch:5 step:5495 [D loss: 0.549146, acc.: 71.88%] [G loss: 1.107702]\n",
      "epoch:5 step:5496 [D loss: 0.580881, acc.: 67.19%] [G loss: 0.967533]\n",
      "epoch:5 step:5497 [D loss: 0.693361, acc.: 51.56%] [G loss: 1.123298]\n",
      "epoch:5 step:5498 [D loss: 0.546904, acc.: 73.44%] [G loss: 1.171743]\n",
      "epoch:5 step:5499 [D loss: 0.563421, acc.: 72.66%] [G loss: 1.158733]\n",
      "epoch:5 step:5500 [D loss: 0.684617, acc.: 56.25%] [G loss: 1.032088]\n",
      "epoch:5 step:5501 [D loss: 0.595713, acc.: 69.53%] [G loss: 1.089207]\n",
      "epoch:5 step:5502 [D loss: 0.646462, acc.: 56.25%] [G loss: 1.073714]\n",
      "epoch:5 step:5503 [D loss: 0.613816, acc.: 67.19%] [G loss: 1.046797]\n",
      "epoch:5 step:5504 [D loss: 0.508641, acc.: 76.56%] [G loss: 1.057080]\n",
      "epoch:5 step:5505 [D loss: 0.687021, acc.: 60.16%] [G loss: 0.875993]\n",
      "epoch:5 step:5506 [D loss: 0.615153, acc.: 64.84%] [G loss: 1.094239]\n",
      "epoch:5 step:5507 [D loss: 0.609600, acc.: 71.09%] [G loss: 1.101111]\n",
      "epoch:5 step:5508 [D loss: 0.821980, acc.: 46.09%] [G loss: 1.060462]\n",
      "epoch:5 step:5509 [D loss: 0.779291, acc.: 46.09%] [G loss: 0.981245]\n",
      "epoch:5 step:5510 [D loss: 0.616079, acc.: 67.19%] [G loss: 1.138751]\n",
      "epoch:5 step:5511 [D loss: 0.566346, acc.: 74.22%] [G loss: 1.131126]\n",
      "epoch:5 step:5512 [D loss: 0.624324, acc.: 63.28%] [G loss: 0.920278]\n",
      "epoch:5 step:5513 [D loss: 0.681562, acc.: 57.81%] [G loss: 0.931988]\n",
      "epoch:5 step:5514 [D loss: 0.578739, acc.: 72.66%] [G loss: 1.334044]\n",
      "epoch:5 step:5515 [D loss: 0.600078, acc.: 67.97%] [G loss: 1.116097]\n",
      "epoch:5 step:5516 [D loss: 0.670541, acc.: 54.69%] [G loss: 1.073474]\n",
      "epoch:5 step:5517 [D loss: 0.642410, acc.: 67.97%] [G loss: 0.948050]\n",
      "epoch:5 step:5518 [D loss: 0.618564, acc.: 61.72%] [G loss: 1.228439]\n",
      "epoch:5 step:5519 [D loss: 0.595649, acc.: 69.53%] [G loss: 1.247125]\n",
      "epoch:5 step:5520 [D loss: 0.764729, acc.: 50.78%] [G loss: 1.009629]\n",
      "epoch:5 step:5521 [D loss: 0.682053, acc.: 61.72%] [G loss: 0.992053]\n",
      "epoch:5 step:5522 [D loss: 0.738569, acc.: 56.25%] [G loss: 0.979479]\n",
      "epoch:5 step:5523 [D loss: 0.575253, acc.: 71.88%] [G loss: 1.262118]\n",
      "epoch:5 step:5524 [D loss: 0.695029, acc.: 63.28%] [G loss: 0.927213]\n",
      "epoch:5 step:5525 [D loss: 0.489490, acc.: 80.47%] [G loss: 1.012343]\n",
      "epoch:5 step:5526 [D loss: 0.533837, acc.: 75.78%] [G loss: 0.964880]\n",
      "epoch:5 step:5527 [D loss: 0.687140, acc.: 58.59%] [G loss: 0.993054]\n",
      "epoch:5 step:5528 [D loss: 0.651980, acc.: 62.50%] [G loss: 0.913915]\n",
      "epoch:5 step:5529 [D loss: 0.760169, acc.: 48.44%] [G loss: 0.950013]\n",
      "epoch:5 step:5530 [D loss: 0.622776, acc.: 66.41%] [G loss: 1.004313]\n",
      "epoch:5 step:5531 [D loss: 0.636972, acc.: 60.16%] [G loss: 1.185621]\n",
      "epoch:5 step:5532 [D loss: 0.609064, acc.: 69.53%] [G loss: 1.179277]\n",
      "epoch:5 step:5533 [D loss: 0.603971, acc.: 64.84%] [G loss: 1.055806]\n",
      "epoch:5 step:5534 [D loss: 0.549798, acc.: 75.00%] [G loss: 1.346713]\n",
      "epoch:5 step:5535 [D loss: 0.580141, acc.: 68.75%] [G loss: 0.981080]\n",
      "epoch:5 step:5536 [D loss: 0.639746, acc.: 61.72%] [G loss: 1.009325]\n",
      "epoch:5 step:5537 [D loss: 0.622126, acc.: 64.84%] [G loss: 1.045536]\n",
      "epoch:5 step:5538 [D loss: 0.500474, acc.: 77.34%] [G loss: 1.329994]\n",
      "epoch:5 step:5539 [D loss: 0.634526, acc.: 63.28%] [G loss: 1.280270]\n",
      "epoch:5 step:5540 [D loss: 0.690138, acc.: 53.91%] [G loss: 0.937328]\n",
      "epoch:5 step:5541 [D loss: 0.636674, acc.: 64.06%] [G loss: 0.966632]\n",
      "epoch:5 step:5542 [D loss: 0.680440, acc.: 56.25%] [G loss: 0.978303]\n",
      "epoch:5 step:5543 [D loss: 0.590200, acc.: 67.97%] [G loss: 0.987833]\n",
      "epoch:5 step:5544 [D loss: 0.656722, acc.: 61.72%] [G loss: 1.060638]\n",
      "epoch:5 step:5545 [D loss: 0.754718, acc.: 45.31%] [G loss: 1.053598]\n",
      "epoch:5 step:5546 [D loss: 0.594104, acc.: 68.75%] [G loss: 1.296384]\n",
      "epoch:5 step:5547 [D loss: 0.774756, acc.: 51.56%] [G loss: 1.259576]\n",
      "epoch:5 step:5548 [D loss: 0.651663, acc.: 61.72%] [G loss: 1.225115]\n",
      "epoch:5 step:5549 [D loss: 0.693874, acc.: 54.69%] [G loss: 1.069273]\n",
      "epoch:5 step:5550 [D loss: 0.577254, acc.: 67.97%] [G loss: 1.200005]\n",
      "epoch:5 step:5551 [D loss: 0.676539, acc.: 57.81%] [G loss: 0.927695]\n",
      "epoch:5 step:5552 [D loss: 0.617670, acc.: 65.62%] [G loss: 1.144529]\n",
      "epoch:5 step:5553 [D loss: 0.564352, acc.: 72.66%] [G loss: 1.248478]\n",
      "epoch:5 step:5554 [D loss: 0.571581, acc.: 67.97%] [G loss: 1.174433]\n",
      "epoch:5 step:5555 [D loss: 0.676702, acc.: 61.72%] [G loss: 1.165863]\n",
      "epoch:5 step:5556 [D loss: 0.680894, acc.: 59.38%] [G loss: 1.077033]\n",
      "epoch:5 step:5557 [D loss: 0.713056, acc.: 58.59%] [G loss: 1.031891]\n",
      "epoch:5 step:5558 [D loss: 0.685282, acc.: 57.81%] [G loss: 0.933171]\n",
      "epoch:5 step:5559 [D loss: 0.598913, acc.: 66.41%] [G loss: 1.195047]\n",
      "epoch:5 step:5560 [D loss: 0.613372, acc.: 61.72%] [G loss: 1.194187]\n",
      "epoch:5 step:5561 [D loss: 0.697840, acc.: 58.59%] [G loss: 1.047439]\n",
      "epoch:5 step:5562 [D loss: 0.527726, acc.: 75.78%] [G loss: 1.116091]\n",
      "epoch:5 step:5563 [D loss: 0.708917, acc.: 56.25%] [G loss: 1.104178]\n",
      "epoch:5 step:5564 [D loss: 0.666195, acc.: 58.59%] [G loss: 1.022635]\n",
      "epoch:5 step:5565 [D loss: 0.615038, acc.: 64.84%] [G loss: 1.126270]\n",
      "epoch:5 step:5566 [D loss: 0.566777, acc.: 72.66%] [G loss: 1.134997]\n",
      "epoch:5 step:5567 [D loss: 0.598303, acc.: 66.41%] [G loss: 1.073496]\n",
      "epoch:5 step:5568 [D loss: 0.696056, acc.: 59.38%] [G loss: 0.960152]\n",
      "epoch:5 step:5569 [D loss: 0.609709, acc.: 65.62%] [G loss: 1.026359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5570 [D loss: 0.599731, acc.: 70.31%] [G loss: 1.097887]\n",
      "epoch:5 step:5571 [D loss: 0.693598, acc.: 60.16%] [G loss: 0.995418]\n",
      "epoch:5 step:5572 [D loss: 0.622922, acc.: 62.50%] [G loss: 1.136656]\n",
      "epoch:5 step:5573 [D loss: 0.650627, acc.: 62.50%] [G loss: 1.017048]\n",
      "epoch:5 step:5574 [D loss: 0.602424, acc.: 67.97%] [G loss: 1.231724]\n",
      "epoch:5 step:5575 [D loss: 0.643985, acc.: 62.50%] [G loss: 1.100999]\n",
      "epoch:5 step:5576 [D loss: 0.770023, acc.: 45.31%] [G loss: 0.996860]\n",
      "epoch:5 step:5577 [D loss: 0.544170, acc.: 72.66%] [G loss: 1.138328]\n",
      "epoch:5 step:5578 [D loss: 0.656053, acc.: 64.84%] [G loss: 1.026464]\n",
      "epoch:5 step:5579 [D loss: 0.650753, acc.: 61.72%] [G loss: 1.167193]\n",
      "epoch:5 step:5580 [D loss: 0.571430, acc.: 73.44%] [G loss: 1.117622]\n",
      "epoch:5 step:5581 [D loss: 0.625236, acc.: 66.41%] [G loss: 0.999179]\n",
      "epoch:5 step:5582 [D loss: 0.595747, acc.: 72.66%] [G loss: 1.130971]\n",
      "epoch:5 step:5583 [D loss: 0.635880, acc.: 59.38%] [G loss: 1.140574]\n",
      "epoch:5 step:5584 [D loss: 0.670786, acc.: 54.69%] [G loss: 1.180253]\n",
      "epoch:5 step:5585 [D loss: 0.654045, acc.: 62.50%] [G loss: 0.955736]\n",
      "epoch:5 step:5586 [D loss: 0.621435, acc.: 69.53%] [G loss: 1.167842]\n",
      "epoch:5 step:5587 [D loss: 0.566543, acc.: 75.78%] [G loss: 1.019257]\n",
      "epoch:5 step:5588 [D loss: 0.584459, acc.: 75.00%] [G loss: 0.961916]\n",
      "epoch:5 step:5589 [D loss: 0.627722, acc.: 59.38%] [G loss: 1.194915]\n",
      "epoch:5 step:5590 [D loss: 0.572029, acc.: 67.97%] [G loss: 1.217893]\n",
      "epoch:5 step:5591 [D loss: 0.627788, acc.: 65.62%] [G loss: 1.057548]\n",
      "epoch:5 step:5592 [D loss: 0.744011, acc.: 53.91%] [G loss: 0.945210]\n",
      "epoch:5 step:5593 [D loss: 0.703075, acc.: 53.91%] [G loss: 0.942787]\n",
      "epoch:5 step:5594 [D loss: 0.734644, acc.: 52.34%] [G loss: 1.205406]\n",
      "epoch:5 step:5595 [D loss: 0.659938, acc.: 64.06%] [G loss: 1.206362]\n",
      "epoch:5 step:5596 [D loss: 0.656869, acc.: 56.25%] [G loss: 1.207350]\n",
      "epoch:5 step:5597 [D loss: 0.644580, acc.: 60.94%] [G loss: 1.033087]\n",
      "epoch:5 step:5598 [D loss: 0.649827, acc.: 60.94%] [G loss: 1.099338]\n",
      "epoch:5 step:5599 [D loss: 0.677939, acc.: 59.38%] [G loss: 1.013934]\n",
      "epoch:5 step:5600 [D loss: 0.558044, acc.: 68.75%] [G loss: 1.021179]\n",
      "##############\n",
      "[2.70044758 2.10361111 2.13462031 2.90005226 1.03767739 6.49823344\n",
      " 1.96349613 3.14068641 4.01310682 5.55588808]\n",
      "##########\n",
      "epoch:5 step:5601 [D loss: 0.664370, acc.: 64.06%] [G loss: 1.095238]\n",
      "epoch:5 step:5602 [D loss: 0.595383, acc.: 67.19%] [G loss: 1.109630]\n",
      "epoch:5 step:5603 [D loss: 0.641227, acc.: 59.38%] [G loss: 1.072020]\n",
      "epoch:5 step:5604 [D loss: 0.547371, acc.: 77.34%] [G loss: 1.306983]\n",
      "epoch:5 step:5605 [D loss: 0.679406, acc.: 63.28%] [G loss: 1.204139]\n",
      "epoch:5 step:5606 [D loss: 0.661909, acc.: 61.72%] [G loss: 1.030506]\n",
      "epoch:5 step:5607 [D loss: 0.586497, acc.: 67.97%] [G loss: 1.007164]\n",
      "epoch:5 step:5608 [D loss: 0.683127, acc.: 60.94%] [G loss: 1.038140]\n",
      "epoch:5 step:5609 [D loss: 0.696306, acc.: 54.69%] [G loss: 0.995450]\n",
      "epoch:5 step:5610 [D loss: 0.575013, acc.: 71.88%] [G loss: 1.206025]\n",
      "epoch:5 step:5611 [D loss: 0.625837, acc.: 67.97%] [G loss: 1.071592]\n",
      "epoch:5 step:5612 [D loss: 0.569883, acc.: 70.31%] [G loss: 0.991497]\n",
      "epoch:5 step:5613 [D loss: 0.572140, acc.: 68.75%] [G loss: 1.071907]\n",
      "epoch:5 step:5614 [D loss: 0.594587, acc.: 67.19%] [G loss: 1.096383]\n",
      "epoch:5 step:5615 [D loss: 0.637151, acc.: 64.84%] [G loss: 1.007045]\n",
      "epoch:5 step:5616 [D loss: 0.605247, acc.: 67.19%] [G loss: 1.053838]\n",
      "epoch:5 step:5617 [D loss: 0.715914, acc.: 60.16%] [G loss: 1.006712]\n",
      "epoch:5 step:5618 [D loss: 0.724696, acc.: 48.44%] [G loss: 1.069157]\n",
      "epoch:5 step:5619 [D loss: 0.729830, acc.: 52.34%] [G loss: 1.083475]\n",
      "epoch:5 step:5620 [D loss: 0.557698, acc.: 71.09%] [G loss: 1.247423]\n",
      "epoch:5 step:5621 [D loss: 0.579632, acc.: 71.88%] [G loss: 1.055719]\n",
      "epoch:5 step:5622 [D loss: 0.655754, acc.: 65.62%] [G loss: 1.310956]\n",
      "epoch:6 step:5623 [D loss: 0.697720, acc.: 56.25%] [G loss: 1.064083]\n",
      "epoch:6 step:5624 [D loss: 0.650403, acc.: 61.72%] [G loss: 1.101614]\n",
      "epoch:6 step:5625 [D loss: 0.691505, acc.: 60.94%] [G loss: 0.916845]\n",
      "epoch:6 step:5626 [D loss: 0.622436, acc.: 64.06%] [G loss: 1.087826]\n",
      "epoch:6 step:5627 [D loss: 0.597368, acc.: 70.31%] [G loss: 0.968640]\n",
      "epoch:6 step:5628 [D loss: 0.735127, acc.: 52.34%] [G loss: 1.049627]\n",
      "epoch:6 step:5629 [D loss: 0.818891, acc.: 48.44%] [G loss: 0.990925]\n",
      "epoch:6 step:5630 [D loss: 0.624622, acc.: 63.28%] [G loss: 1.253040]\n",
      "epoch:6 step:5631 [D loss: 0.611830, acc.: 60.16%] [G loss: 1.148609]\n",
      "epoch:6 step:5632 [D loss: 0.598347, acc.: 70.31%] [G loss: 1.385483]\n",
      "epoch:6 step:5633 [D loss: 0.523620, acc.: 74.22%] [G loss: 1.174377]\n",
      "epoch:6 step:5634 [D loss: 0.557522, acc.: 73.44%] [G loss: 1.214779]\n",
      "epoch:6 step:5635 [D loss: 0.564899, acc.: 75.00%] [G loss: 1.160982]\n",
      "epoch:6 step:5636 [D loss: 0.614863, acc.: 66.41%] [G loss: 0.948456]\n",
      "epoch:6 step:5637 [D loss: 0.548103, acc.: 74.22%] [G loss: 1.132255]\n",
      "epoch:6 step:5638 [D loss: 0.622855, acc.: 67.19%] [G loss: 0.919533]\n",
      "epoch:6 step:5639 [D loss: 0.627886, acc.: 60.94%] [G loss: 1.134349]\n",
      "epoch:6 step:5640 [D loss: 0.619232, acc.: 60.94%] [G loss: 1.199038]\n",
      "epoch:6 step:5641 [D loss: 0.716498, acc.: 56.25%] [G loss: 1.022623]\n",
      "epoch:6 step:5642 [D loss: 0.640694, acc.: 56.25%] [G loss: 0.955985]\n",
      "epoch:6 step:5643 [D loss: 0.632498, acc.: 60.94%] [G loss: 1.067314]\n",
      "epoch:6 step:5644 [D loss: 0.561330, acc.: 70.31%] [G loss: 1.085014]\n",
      "epoch:6 step:5645 [D loss: 0.692713, acc.: 60.94%] [G loss: 1.009269]\n",
      "epoch:6 step:5646 [D loss: 0.513465, acc.: 77.34%] [G loss: 0.960055]\n",
      "epoch:6 step:5647 [D loss: 0.577423, acc.: 71.09%] [G loss: 1.225911]\n",
      "epoch:6 step:5648 [D loss: 0.797901, acc.: 46.88%] [G loss: 0.976092]\n",
      "epoch:6 step:5649 [D loss: 0.687563, acc.: 64.84%] [G loss: 0.957828]\n",
      "epoch:6 step:5650 [D loss: 0.566926, acc.: 71.88%] [G loss: 1.011675]\n",
      "epoch:6 step:5651 [D loss: 0.657584, acc.: 60.16%] [G loss: 1.130731]\n",
      "epoch:6 step:5652 [D loss: 0.682885, acc.: 61.72%] [G loss: 0.914726]\n",
      "epoch:6 step:5653 [D loss: 0.628455, acc.: 66.41%] [G loss: 1.064510]\n",
      "epoch:6 step:5654 [D loss: 0.657501, acc.: 60.94%] [G loss: 1.025856]\n",
      "epoch:6 step:5655 [D loss: 0.598511, acc.: 67.97%] [G loss: 1.194768]\n",
      "epoch:6 step:5656 [D loss: 0.645130, acc.: 56.25%] [G loss: 1.031844]\n",
      "epoch:6 step:5657 [D loss: 0.508245, acc.: 78.91%] [G loss: 1.094572]\n",
      "epoch:6 step:5658 [D loss: 0.609640, acc.: 64.06%] [G loss: 1.175042]\n",
      "epoch:6 step:5659 [D loss: 0.543758, acc.: 74.22%] [G loss: 1.106378]\n",
      "epoch:6 step:5660 [D loss: 0.578196, acc.: 70.31%] [G loss: 1.265788]\n",
      "epoch:6 step:5661 [D loss: 0.697124, acc.: 57.81%] [G loss: 1.116106]\n",
      "epoch:6 step:5662 [D loss: 0.643503, acc.: 66.41%] [G loss: 1.195917]\n",
      "epoch:6 step:5663 [D loss: 0.603314, acc.: 66.41%] [G loss: 1.123830]\n",
      "epoch:6 step:5664 [D loss: 0.616024, acc.: 66.41%] [G loss: 1.136991]\n",
      "epoch:6 step:5665 [D loss: 0.588836, acc.: 69.53%] [G loss: 1.284643]\n",
      "epoch:6 step:5666 [D loss: 0.727205, acc.: 52.34%] [G loss: 1.125773]\n",
      "epoch:6 step:5667 [D loss: 0.605825, acc.: 64.06%] [G loss: 0.954186]\n",
      "epoch:6 step:5668 [D loss: 0.656763, acc.: 62.50%] [G loss: 1.065575]\n",
      "epoch:6 step:5669 [D loss: 0.675900, acc.: 62.50%] [G loss: 1.094063]\n",
      "epoch:6 step:5670 [D loss: 0.678316, acc.: 58.59%] [G loss: 1.032191]\n",
      "epoch:6 step:5671 [D loss: 0.603728, acc.: 67.19%] [G loss: 1.069441]\n",
      "epoch:6 step:5672 [D loss: 0.630684, acc.: 60.94%] [G loss: 1.053511]\n",
      "epoch:6 step:5673 [D loss: 0.578328, acc.: 71.88%] [G loss: 1.163944]\n",
      "epoch:6 step:5674 [D loss: 0.629845, acc.: 61.72%] [G loss: 1.026956]\n",
      "epoch:6 step:5675 [D loss: 0.631816, acc.: 60.16%] [G loss: 1.105504]\n",
      "epoch:6 step:5676 [D loss: 0.519404, acc.: 77.34%] [G loss: 1.260696]\n",
      "epoch:6 step:5677 [D loss: 0.623024, acc.: 64.84%] [G loss: 1.196972]\n",
      "epoch:6 step:5678 [D loss: 0.627791, acc.: 64.06%] [G loss: 1.253341]\n",
      "epoch:6 step:5679 [D loss: 0.721166, acc.: 53.12%] [G loss: 0.980744]\n",
      "epoch:6 step:5680 [D loss: 0.540329, acc.: 70.31%] [G loss: 1.262576]\n",
      "epoch:6 step:5681 [D loss: 0.595500, acc.: 70.31%] [G loss: 0.955942]\n",
      "epoch:6 step:5682 [D loss: 0.602524, acc.: 65.62%] [G loss: 1.143366]\n",
      "epoch:6 step:5683 [D loss: 0.638359, acc.: 65.62%] [G loss: 0.969845]\n",
      "epoch:6 step:5684 [D loss: 0.706498, acc.: 57.81%] [G loss: 1.163920]\n",
      "epoch:6 step:5685 [D loss: 0.610006, acc.: 69.53%] [G loss: 1.167087]\n",
      "epoch:6 step:5686 [D loss: 0.645771, acc.: 67.19%] [G loss: 0.973484]\n",
      "epoch:6 step:5687 [D loss: 0.693713, acc.: 60.94%] [G loss: 0.901737]\n",
      "epoch:6 step:5688 [D loss: 0.638196, acc.: 60.94%] [G loss: 1.084978]\n",
      "epoch:6 step:5689 [D loss: 0.629887, acc.: 66.41%] [G loss: 1.218410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5690 [D loss: 0.558640, acc.: 71.09%] [G loss: 1.021416]\n",
      "epoch:6 step:5691 [D loss: 0.627010, acc.: 66.41%] [G loss: 1.086239]\n",
      "epoch:6 step:5692 [D loss: 0.600429, acc.: 64.06%] [G loss: 1.032648]\n",
      "epoch:6 step:5693 [D loss: 0.658115, acc.: 66.41%] [G loss: 0.911682]\n",
      "epoch:6 step:5694 [D loss: 0.638674, acc.: 67.19%] [G loss: 1.022998]\n",
      "epoch:6 step:5695 [D loss: 0.620015, acc.: 67.97%] [G loss: 1.118521]\n",
      "epoch:6 step:5696 [D loss: 0.645173, acc.: 62.50%] [G loss: 1.145128]\n",
      "epoch:6 step:5697 [D loss: 0.626063, acc.: 64.06%] [G loss: 1.037946]\n",
      "epoch:6 step:5698 [D loss: 0.674122, acc.: 57.81%] [G loss: 0.971071]\n",
      "epoch:6 step:5699 [D loss: 0.654698, acc.: 60.94%] [G loss: 1.005235]\n",
      "epoch:6 step:5700 [D loss: 0.644541, acc.: 68.75%] [G loss: 0.855833]\n",
      "epoch:6 step:5701 [D loss: 0.623240, acc.: 67.19%] [G loss: 1.136878]\n",
      "epoch:6 step:5702 [D loss: 0.612069, acc.: 64.06%] [G loss: 0.974313]\n",
      "epoch:6 step:5703 [D loss: 0.622601, acc.: 64.06%] [G loss: 1.093447]\n",
      "epoch:6 step:5704 [D loss: 0.665117, acc.: 58.59%] [G loss: 0.985206]\n",
      "epoch:6 step:5705 [D loss: 0.715296, acc.: 59.38%] [G loss: 0.962293]\n",
      "epoch:6 step:5706 [D loss: 0.666308, acc.: 61.72%] [G loss: 0.801819]\n",
      "epoch:6 step:5707 [D loss: 0.611325, acc.: 65.62%] [G loss: 1.045962]\n",
      "epoch:6 step:5708 [D loss: 0.606627, acc.: 68.75%] [G loss: 1.133319]\n",
      "epoch:6 step:5709 [D loss: 0.594922, acc.: 69.53%] [G loss: 1.023122]\n",
      "epoch:6 step:5710 [D loss: 0.550163, acc.: 72.66%] [G loss: 1.259254]\n",
      "epoch:6 step:5711 [D loss: 0.603911, acc.: 65.62%] [G loss: 1.077292]\n",
      "epoch:6 step:5712 [D loss: 0.607725, acc.: 67.97%] [G loss: 1.116996]\n",
      "epoch:6 step:5713 [D loss: 0.528349, acc.: 75.00%] [G loss: 1.198107]\n",
      "epoch:6 step:5714 [D loss: 0.644272, acc.: 62.50%] [G loss: 1.279575]\n",
      "epoch:6 step:5715 [D loss: 0.690800, acc.: 55.47%] [G loss: 1.075270]\n",
      "epoch:6 step:5716 [D loss: 0.559782, acc.: 74.22%] [G loss: 1.162422]\n",
      "epoch:6 step:5717 [D loss: 0.737794, acc.: 50.00%] [G loss: 1.137640]\n",
      "epoch:6 step:5718 [D loss: 0.682497, acc.: 53.91%] [G loss: 1.024577]\n",
      "epoch:6 step:5719 [D loss: 0.672807, acc.: 62.50%] [G loss: 1.235505]\n",
      "epoch:6 step:5720 [D loss: 0.645728, acc.: 63.28%] [G loss: 0.999475]\n",
      "epoch:6 step:5721 [D loss: 0.728398, acc.: 50.78%] [G loss: 0.993870]\n",
      "epoch:6 step:5722 [D loss: 0.620675, acc.: 60.16%] [G loss: 1.120696]\n",
      "epoch:6 step:5723 [D loss: 0.594749, acc.: 71.88%] [G loss: 0.943641]\n",
      "epoch:6 step:5724 [D loss: 0.728929, acc.: 50.78%] [G loss: 1.090900]\n",
      "epoch:6 step:5725 [D loss: 0.673533, acc.: 57.81%] [G loss: 0.963631]\n",
      "epoch:6 step:5726 [D loss: 0.697508, acc.: 62.50%] [G loss: 1.090780]\n",
      "epoch:6 step:5727 [D loss: 0.627585, acc.: 67.97%] [G loss: 1.081592]\n",
      "epoch:6 step:5728 [D loss: 0.679030, acc.: 54.69%] [G loss: 1.189208]\n",
      "epoch:6 step:5729 [D loss: 0.631617, acc.: 63.28%] [G loss: 1.137327]\n",
      "epoch:6 step:5730 [D loss: 0.589469, acc.: 67.19%] [G loss: 1.251405]\n",
      "epoch:6 step:5731 [D loss: 0.704448, acc.: 54.69%] [G loss: 1.152970]\n",
      "epoch:6 step:5732 [D loss: 0.569213, acc.: 72.66%] [G loss: 1.198426]\n",
      "epoch:6 step:5733 [D loss: 0.647865, acc.: 61.72%] [G loss: 0.938620]\n",
      "epoch:6 step:5734 [D loss: 0.604950, acc.: 65.62%] [G loss: 1.183683]\n",
      "epoch:6 step:5735 [D loss: 0.699988, acc.: 59.38%] [G loss: 0.964463]\n",
      "epoch:6 step:5736 [D loss: 0.595383, acc.: 66.41%] [G loss: 1.003112]\n",
      "epoch:6 step:5737 [D loss: 0.650068, acc.: 67.19%] [G loss: 1.124364]\n",
      "epoch:6 step:5738 [D loss: 0.647825, acc.: 64.06%] [G loss: 1.040347]\n",
      "epoch:6 step:5739 [D loss: 0.594149, acc.: 67.97%] [G loss: 1.133004]\n",
      "epoch:6 step:5740 [D loss: 0.634962, acc.: 63.28%] [G loss: 1.136913]\n",
      "epoch:6 step:5741 [D loss: 0.671500, acc.: 59.38%] [G loss: 1.091166]\n",
      "epoch:6 step:5742 [D loss: 0.657114, acc.: 63.28%] [G loss: 1.052345]\n",
      "epoch:6 step:5743 [D loss: 0.586702, acc.: 63.28%] [G loss: 0.986503]\n",
      "epoch:6 step:5744 [D loss: 0.636397, acc.: 59.38%] [G loss: 0.952416]\n",
      "epoch:6 step:5745 [D loss: 0.632688, acc.: 60.94%] [G loss: 1.041745]\n",
      "epoch:6 step:5746 [D loss: 0.645990, acc.: 62.50%] [G loss: 1.110175]\n",
      "epoch:6 step:5747 [D loss: 0.496102, acc.: 79.69%] [G loss: 1.123195]\n",
      "epoch:6 step:5748 [D loss: 0.600393, acc.: 67.97%] [G loss: 1.222273]\n",
      "epoch:6 step:5749 [D loss: 0.665541, acc.: 63.28%] [G loss: 1.046018]\n",
      "epoch:6 step:5750 [D loss: 0.635086, acc.: 67.97%] [G loss: 1.059255]\n",
      "epoch:6 step:5751 [D loss: 0.646002, acc.: 60.16%] [G loss: 1.112736]\n",
      "epoch:6 step:5752 [D loss: 0.654390, acc.: 64.06%] [G loss: 1.088080]\n",
      "epoch:6 step:5753 [D loss: 0.645178, acc.: 59.38%] [G loss: 1.034386]\n",
      "epoch:6 step:5754 [D loss: 0.672197, acc.: 60.94%] [G loss: 1.009757]\n",
      "epoch:6 step:5755 [D loss: 0.619585, acc.: 66.41%] [G loss: 1.250166]\n",
      "epoch:6 step:5756 [D loss: 0.769502, acc.: 52.34%] [G loss: 1.092976]\n",
      "epoch:6 step:5757 [D loss: 0.650271, acc.: 58.59%] [G loss: 1.339868]\n",
      "epoch:6 step:5758 [D loss: 0.652099, acc.: 64.06%] [G loss: 1.274621]\n",
      "epoch:6 step:5759 [D loss: 0.654930, acc.: 62.50%] [G loss: 1.032994]\n",
      "epoch:6 step:5760 [D loss: 0.605142, acc.: 70.31%] [G loss: 1.058952]\n",
      "epoch:6 step:5761 [D loss: 0.701918, acc.: 58.59%] [G loss: 1.061494]\n",
      "epoch:6 step:5762 [D loss: 0.738104, acc.: 52.34%] [G loss: 1.029900]\n",
      "epoch:6 step:5763 [D loss: 0.620173, acc.: 67.97%] [G loss: 1.112755]\n",
      "epoch:6 step:5764 [D loss: 0.528226, acc.: 77.34%] [G loss: 1.285339]\n",
      "epoch:6 step:5765 [D loss: 0.734387, acc.: 55.47%] [G loss: 1.104571]\n",
      "epoch:6 step:5766 [D loss: 0.637347, acc.: 59.38%] [G loss: 0.984137]\n",
      "epoch:6 step:5767 [D loss: 0.572300, acc.: 69.53%] [G loss: 1.037924]\n",
      "epoch:6 step:5768 [D loss: 0.580234, acc.: 68.75%] [G loss: 1.074419]\n",
      "epoch:6 step:5769 [D loss: 0.600100, acc.: 64.84%] [G loss: 1.255705]\n",
      "epoch:6 step:5770 [D loss: 0.631985, acc.: 62.50%] [G loss: 1.058171]\n",
      "epoch:6 step:5771 [D loss: 0.589241, acc.: 68.75%] [G loss: 1.117027]\n",
      "epoch:6 step:5772 [D loss: 0.606559, acc.: 64.84%] [G loss: 1.097291]\n",
      "epoch:6 step:5773 [D loss: 0.576052, acc.: 70.31%] [G loss: 1.174361]\n",
      "epoch:6 step:5774 [D loss: 0.620098, acc.: 66.41%] [G loss: 1.016963]\n",
      "epoch:6 step:5775 [D loss: 0.549848, acc.: 67.97%] [G loss: 1.123257]\n",
      "epoch:6 step:5776 [D loss: 0.595498, acc.: 66.41%] [G loss: 0.970844]\n",
      "epoch:6 step:5777 [D loss: 0.639631, acc.: 61.72%] [G loss: 1.038339]\n",
      "epoch:6 step:5778 [D loss: 0.616695, acc.: 67.97%] [G loss: 1.074681]\n",
      "epoch:6 step:5779 [D loss: 0.643229, acc.: 64.84%] [G loss: 1.066921]\n",
      "epoch:6 step:5780 [D loss: 0.654046, acc.: 57.81%] [G loss: 1.009492]\n",
      "epoch:6 step:5781 [D loss: 0.586243, acc.: 72.66%] [G loss: 1.087546]\n",
      "epoch:6 step:5782 [D loss: 0.633074, acc.: 65.62%] [G loss: 1.115237]\n",
      "epoch:6 step:5783 [D loss: 0.648497, acc.: 66.41%] [G loss: 1.022279]\n",
      "epoch:6 step:5784 [D loss: 0.712576, acc.: 57.03%] [G loss: 1.077953]\n",
      "epoch:6 step:5785 [D loss: 0.686758, acc.: 59.38%] [G loss: 1.006823]\n",
      "epoch:6 step:5786 [D loss: 0.583012, acc.: 67.19%] [G loss: 1.078019]\n",
      "epoch:6 step:5787 [D loss: 0.659215, acc.: 59.38%] [G loss: 1.046803]\n",
      "epoch:6 step:5788 [D loss: 0.643449, acc.: 65.62%] [G loss: 1.366697]\n",
      "epoch:6 step:5789 [D loss: 0.593308, acc.: 70.31%] [G loss: 1.134836]\n",
      "epoch:6 step:5790 [D loss: 0.613512, acc.: 62.50%] [G loss: 1.018817]\n",
      "epoch:6 step:5791 [D loss: 0.674250, acc.: 57.81%] [G loss: 0.996891]\n",
      "epoch:6 step:5792 [D loss: 0.621568, acc.: 66.41%] [G loss: 1.089260]\n",
      "epoch:6 step:5793 [D loss: 0.594830, acc.: 67.97%] [G loss: 1.027348]\n",
      "epoch:6 step:5794 [D loss: 0.539264, acc.: 72.66%] [G loss: 1.122869]\n",
      "epoch:6 step:5795 [D loss: 0.729473, acc.: 53.91%] [G loss: 0.946483]\n",
      "epoch:6 step:5796 [D loss: 0.590836, acc.: 68.75%] [G loss: 1.221826]\n",
      "epoch:6 step:5797 [D loss: 0.543957, acc.: 72.66%] [G loss: 1.165517]\n",
      "epoch:6 step:5798 [D loss: 0.674480, acc.: 59.38%] [G loss: 1.030797]\n",
      "epoch:6 step:5799 [D loss: 0.566565, acc.: 76.56%] [G loss: 1.067060]\n",
      "epoch:6 step:5800 [D loss: 0.685790, acc.: 54.69%] [G loss: 1.007884]\n",
      "##############\n",
      "[2.66673249 2.07773674 1.80645561 2.81744338 1.15532438 6.70442161\n",
      " 1.94361752 2.81210977 3.78632544 7.14799875]\n",
      "##########\n",
      "epoch:6 step:5801 [D loss: 0.684415, acc.: 56.25%] [G loss: 1.177955]\n",
      "epoch:6 step:5802 [D loss: 0.638685, acc.: 65.62%] [G loss: 1.039597]\n",
      "epoch:6 step:5803 [D loss: 0.657040, acc.: 60.16%] [G loss: 0.883533]\n",
      "epoch:6 step:5804 [D loss: 0.615342, acc.: 64.84%] [G loss: 0.939324]\n",
      "epoch:6 step:5805 [D loss: 0.680301, acc.: 57.03%] [G loss: 1.212234]\n",
      "epoch:6 step:5806 [D loss: 0.557471, acc.: 73.44%] [G loss: 1.245933]\n",
      "epoch:6 step:5807 [D loss: 0.604001, acc.: 67.19%] [G loss: 1.228092]\n",
      "epoch:6 step:5808 [D loss: 0.529028, acc.: 78.12%] [G loss: 1.395096]\n",
      "epoch:6 step:5809 [D loss: 0.567223, acc.: 73.44%] [G loss: 0.988458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5810 [D loss: 0.687211, acc.: 58.59%] [G loss: 1.084627]\n",
      "epoch:6 step:5811 [D loss: 0.637513, acc.: 65.62%] [G loss: 1.119630]\n",
      "epoch:6 step:5812 [D loss: 0.750546, acc.: 48.44%] [G loss: 0.958830]\n",
      "epoch:6 step:5813 [D loss: 0.653393, acc.: 61.72%] [G loss: 0.981362]\n",
      "epoch:6 step:5814 [D loss: 0.641034, acc.: 66.41%] [G loss: 1.167201]\n",
      "epoch:6 step:5815 [D loss: 0.658247, acc.: 61.72%] [G loss: 0.966165]\n",
      "epoch:6 step:5816 [D loss: 0.727456, acc.: 58.59%] [G loss: 1.162399]\n",
      "epoch:6 step:5817 [D loss: 0.580097, acc.: 65.62%] [G loss: 1.005620]\n",
      "epoch:6 step:5818 [D loss: 0.557296, acc.: 68.75%] [G loss: 1.108259]\n",
      "epoch:6 step:5819 [D loss: 0.618235, acc.: 64.84%] [G loss: 1.192167]\n",
      "epoch:6 step:5820 [D loss: 0.597485, acc.: 67.19%] [G loss: 1.153315]\n",
      "epoch:6 step:5821 [D loss: 0.586599, acc.: 69.53%] [G loss: 1.107209]\n",
      "epoch:6 step:5822 [D loss: 0.482473, acc.: 80.47%] [G loss: 1.103214]\n",
      "epoch:6 step:5823 [D loss: 0.668179, acc.: 62.50%] [G loss: 1.100833]\n",
      "epoch:6 step:5824 [D loss: 0.539422, acc.: 76.56%] [G loss: 1.228145]\n",
      "epoch:6 step:5825 [D loss: 0.611276, acc.: 67.97%] [G loss: 1.165053]\n",
      "epoch:6 step:5826 [D loss: 0.637453, acc.: 60.94%] [G loss: 1.018959]\n",
      "epoch:6 step:5827 [D loss: 0.616408, acc.: 67.97%] [G loss: 1.073552]\n",
      "epoch:6 step:5828 [D loss: 0.653838, acc.: 63.28%] [G loss: 1.072516]\n",
      "epoch:6 step:5829 [D loss: 0.639695, acc.: 59.38%] [G loss: 1.064385]\n",
      "epoch:6 step:5830 [D loss: 0.584799, acc.: 64.84%] [G loss: 1.004404]\n",
      "epoch:6 step:5831 [D loss: 0.712136, acc.: 58.59%] [G loss: 1.114305]\n",
      "epoch:6 step:5832 [D loss: 0.677652, acc.: 60.94%] [G loss: 1.096367]\n",
      "epoch:6 step:5833 [D loss: 0.536786, acc.: 75.78%] [G loss: 1.290825]\n",
      "epoch:6 step:5834 [D loss: 0.602676, acc.: 68.75%] [G loss: 1.153306]\n",
      "epoch:6 step:5835 [D loss: 0.706304, acc.: 57.81%] [G loss: 0.889806]\n",
      "epoch:6 step:5836 [D loss: 0.644266, acc.: 67.97%] [G loss: 1.179492]\n",
      "epoch:6 step:5837 [D loss: 0.629510, acc.: 65.62%] [G loss: 0.862061]\n",
      "epoch:6 step:5838 [D loss: 0.726461, acc.: 55.47%] [G loss: 1.081147]\n",
      "epoch:6 step:5839 [D loss: 0.688974, acc.: 52.34%] [G loss: 0.962958]\n",
      "epoch:6 step:5840 [D loss: 0.561512, acc.: 69.53%] [G loss: 1.100208]\n",
      "epoch:6 step:5841 [D loss: 0.667499, acc.: 66.41%] [G loss: 0.887181]\n",
      "epoch:6 step:5842 [D loss: 0.594541, acc.: 68.75%] [G loss: 0.964805]\n",
      "epoch:6 step:5843 [D loss: 0.568938, acc.: 70.31%] [G loss: 1.060085]\n",
      "epoch:6 step:5844 [D loss: 0.722177, acc.: 56.25%] [G loss: 1.092019]\n",
      "epoch:6 step:5845 [D loss: 0.572680, acc.: 69.53%] [G loss: 1.111788]\n",
      "epoch:6 step:5846 [D loss: 0.620071, acc.: 64.06%] [G loss: 1.220824]\n",
      "epoch:6 step:5847 [D loss: 0.648577, acc.: 62.50%] [G loss: 1.241948]\n",
      "epoch:6 step:5848 [D loss: 0.744035, acc.: 48.44%] [G loss: 1.230712]\n",
      "epoch:6 step:5849 [D loss: 0.599401, acc.: 65.62%] [G loss: 1.003431]\n",
      "epoch:6 step:5850 [D loss: 0.633316, acc.: 66.41%] [G loss: 1.103262]\n",
      "epoch:6 step:5851 [D loss: 0.631201, acc.: 63.28%] [G loss: 1.100401]\n",
      "epoch:6 step:5852 [D loss: 0.685619, acc.: 54.69%] [G loss: 0.994930]\n",
      "epoch:6 step:5853 [D loss: 0.670323, acc.: 62.50%] [G loss: 1.099754]\n",
      "epoch:6 step:5854 [D loss: 0.656887, acc.: 62.50%] [G loss: 1.030774]\n",
      "epoch:6 step:5855 [D loss: 0.604953, acc.: 68.75%] [G loss: 1.149333]\n",
      "epoch:6 step:5856 [D loss: 0.741260, acc.: 46.09%] [G loss: 0.959208]\n",
      "epoch:6 step:5857 [D loss: 0.614070, acc.: 65.62%] [G loss: 1.046814]\n",
      "epoch:6 step:5858 [D loss: 0.594006, acc.: 67.97%] [G loss: 1.026320]\n",
      "epoch:6 step:5859 [D loss: 0.687030, acc.: 58.59%] [G loss: 0.994727]\n",
      "epoch:6 step:5860 [D loss: 0.641293, acc.: 62.50%] [G loss: 0.885020]\n",
      "epoch:6 step:5861 [D loss: 0.656271, acc.: 59.38%] [G loss: 1.060518]\n",
      "epoch:6 step:5862 [D loss: 0.552230, acc.: 72.66%] [G loss: 1.235643]\n",
      "epoch:6 step:5863 [D loss: 0.622269, acc.: 68.75%] [G loss: 1.143153]\n",
      "epoch:6 step:5864 [D loss: 0.684988, acc.: 59.38%] [G loss: 1.064608]\n",
      "epoch:6 step:5865 [D loss: 0.620205, acc.: 66.41%] [G loss: 1.052140]\n",
      "epoch:6 step:5866 [D loss: 0.628867, acc.: 67.97%] [G loss: 0.925892]\n",
      "epoch:6 step:5867 [D loss: 0.613903, acc.: 61.72%] [G loss: 1.092982]\n",
      "epoch:6 step:5868 [D loss: 0.625081, acc.: 60.94%] [G loss: 0.968414]\n",
      "epoch:6 step:5869 [D loss: 0.591141, acc.: 68.75%] [G loss: 1.194132]\n",
      "epoch:6 step:5870 [D loss: 0.581599, acc.: 67.97%] [G loss: 1.117249]\n",
      "epoch:6 step:5871 [D loss: 0.618146, acc.: 69.53%] [G loss: 1.081245]\n",
      "epoch:6 step:5872 [D loss: 0.643153, acc.: 60.94%] [G loss: 1.045782]\n",
      "epoch:6 step:5873 [D loss: 0.641396, acc.: 57.03%] [G loss: 1.216269]\n",
      "epoch:6 step:5874 [D loss: 0.571883, acc.: 70.31%] [G loss: 1.186427]\n",
      "epoch:6 step:5875 [D loss: 0.658315, acc.: 61.72%] [G loss: 1.124434]\n",
      "epoch:6 step:5876 [D loss: 0.674594, acc.: 61.72%] [G loss: 1.044617]\n",
      "epoch:6 step:5877 [D loss: 0.675003, acc.: 59.38%] [G loss: 0.977889]\n",
      "epoch:6 step:5878 [D loss: 0.597271, acc.: 69.53%] [G loss: 1.156165]\n",
      "epoch:6 step:5879 [D loss: 0.544001, acc.: 78.12%] [G loss: 1.160891]\n",
      "epoch:6 step:5880 [D loss: 0.565434, acc.: 69.53%] [G loss: 1.242588]\n",
      "epoch:6 step:5881 [D loss: 0.722130, acc.: 58.59%] [G loss: 0.967465]\n",
      "epoch:6 step:5882 [D loss: 0.627895, acc.: 64.06%] [G loss: 0.935910]\n",
      "epoch:6 step:5883 [D loss: 0.523326, acc.: 72.66%] [G loss: 1.252181]\n",
      "epoch:6 step:5884 [D loss: 0.683991, acc.: 64.84%] [G loss: 1.054914]\n",
      "epoch:6 step:5885 [D loss: 0.671682, acc.: 57.03%] [G loss: 1.031638]\n",
      "epoch:6 step:5886 [D loss: 0.623458, acc.: 62.50%] [G loss: 1.050941]\n",
      "epoch:6 step:5887 [D loss: 0.557190, acc.: 69.53%] [G loss: 1.103422]\n",
      "epoch:6 step:5888 [D loss: 0.553951, acc.: 71.09%] [G loss: 1.065726]\n",
      "epoch:6 step:5889 [D loss: 0.691852, acc.: 64.84%] [G loss: 1.077922]\n",
      "epoch:6 step:5890 [D loss: 0.602058, acc.: 67.19%] [G loss: 1.310182]\n",
      "epoch:6 step:5891 [D loss: 0.584482, acc.: 65.62%] [G loss: 1.172724]\n",
      "epoch:6 step:5892 [D loss: 0.634439, acc.: 65.62%] [G loss: 1.113919]\n",
      "epoch:6 step:5893 [D loss: 0.691888, acc.: 53.12%] [G loss: 0.936236]\n",
      "epoch:6 step:5894 [D loss: 0.593496, acc.: 68.75%] [G loss: 1.334976]\n",
      "epoch:6 step:5895 [D loss: 0.685988, acc.: 60.16%] [G loss: 1.113450]\n",
      "epoch:6 step:5896 [D loss: 0.649111, acc.: 57.03%] [G loss: 1.113988]\n",
      "epoch:6 step:5897 [D loss: 0.805191, acc.: 42.19%] [G loss: 0.934561]\n",
      "epoch:6 step:5898 [D loss: 0.711101, acc.: 57.81%] [G loss: 1.059990]\n",
      "epoch:6 step:5899 [D loss: 0.623901, acc.: 64.84%] [G loss: 1.038661]\n",
      "epoch:6 step:5900 [D loss: 0.638798, acc.: 60.16%] [G loss: 1.232377]\n",
      "epoch:6 step:5901 [D loss: 0.610249, acc.: 63.28%] [G loss: 1.129822]\n",
      "epoch:6 step:5902 [D loss: 0.649299, acc.: 62.50%] [G loss: 1.049408]\n",
      "epoch:6 step:5903 [D loss: 0.622333, acc.: 66.41%] [G loss: 1.082930]\n",
      "epoch:6 step:5904 [D loss: 0.607595, acc.: 66.41%] [G loss: 1.190002]\n",
      "epoch:6 step:5905 [D loss: 0.603513, acc.: 69.53%] [G loss: 1.349135]\n",
      "epoch:6 step:5906 [D loss: 0.647538, acc.: 60.16%] [G loss: 1.096682]\n",
      "epoch:6 step:5907 [D loss: 0.619400, acc.: 73.44%] [G loss: 1.208678]\n",
      "epoch:6 step:5908 [D loss: 0.669407, acc.: 57.81%] [G loss: 1.055668]\n",
      "epoch:6 step:5909 [D loss: 0.649410, acc.: 61.72%] [G loss: 1.061017]\n",
      "epoch:6 step:5910 [D loss: 0.694270, acc.: 55.47%] [G loss: 1.112240]\n",
      "epoch:6 step:5911 [D loss: 0.601450, acc.: 67.97%] [G loss: 1.128213]\n",
      "epoch:6 step:5912 [D loss: 0.518741, acc.: 75.78%] [G loss: 1.240511]\n",
      "epoch:6 step:5913 [D loss: 0.586044, acc.: 68.75%] [G loss: 1.130332]\n",
      "epoch:6 step:5914 [D loss: 0.690208, acc.: 57.03%] [G loss: 0.999711]\n",
      "epoch:6 step:5915 [D loss: 0.661233, acc.: 60.16%] [G loss: 0.976997]\n",
      "epoch:6 step:5916 [D loss: 0.601631, acc.: 65.62%] [G loss: 0.993274]\n",
      "epoch:6 step:5917 [D loss: 0.623227, acc.: 71.09%] [G loss: 1.072855]\n",
      "epoch:6 step:5918 [D loss: 0.630508, acc.: 65.62%] [G loss: 1.078948]\n",
      "epoch:6 step:5919 [D loss: 0.692320, acc.: 54.69%] [G loss: 1.168000]\n",
      "epoch:6 step:5920 [D loss: 0.622554, acc.: 60.16%] [G loss: 1.207827]\n",
      "epoch:6 step:5921 [D loss: 0.655095, acc.: 61.72%] [G loss: 0.949757]\n",
      "epoch:6 step:5922 [D loss: 0.592239, acc.: 67.97%] [G loss: 0.993537]\n",
      "epoch:6 step:5923 [D loss: 0.666632, acc.: 62.50%] [G loss: 1.091318]\n",
      "epoch:6 step:5924 [D loss: 0.630654, acc.: 67.19%] [G loss: 0.945304]\n",
      "epoch:6 step:5925 [D loss: 0.638914, acc.: 60.94%] [G loss: 1.278491]\n",
      "epoch:6 step:5926 [D loss: 0.644660, acc.: 64.06%] [G loss: 0.865407]\n",
      "epoch:6 step:5927 [D loss: 0.653823, acc.: 60.94%] [G loss: 1.088891]\n",
      "epoch:6 step:5928 [D loss: 0.578508, acc.: 67.97%] [G loss: 1.144171]\n",
      "epoch:6 step:5929 [D loss: 0.687842, acc.: 59.38%] [G loss: 1.100971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5930 [D loss: 0.631351, acc.: 64.06%] [G loss: 1.131547]\n",
      "epoch:6 step:5931 [D loss: 0.637923, acc.: 60.94%] [G loss: 1.104558]\n",
      "epoch:6 step:5932 [D loss: 0.672343, acc.: 57.81%] [G loss: 0.878649]\n",
      "epoch:6 step:5933 [D loss: 0.556141, acc.: 70.31%] [G loss: 1.093801]\n",
      "epoch:6 step:5934 [D loss: 0.634853, acc.: 60.16%] [G loss: 1.125032]\n",
      "epoch:6 step:5935 [D loss: 0.611079, acc.: 70.31%] [G loss: 1.022732]\n",
      "epoch:6 step:5936 [D loss: 0.590396, acc.: 73.44%] [G loss: 1.010432]\n",
      "epoch:6 step:5937 [D loss: 0.598210, acc.: 68.75%] [G loss: 1.061155]\n",
      "epoch:6 step:5938 [D loss: 0.741287, acc.: 50.78%] [G loss: 0.896765]\n",
      "epoch:6 step:5939 [D loss: 0.577923, acc.: 69.53%] [G loss: 1.224661]\n",
      "epoch:6 step:5940 [D loss: 0.647087, acc.: 59.38%] [G loss: 1.028431]\n",
      "epoch:6 step:5941 [D loss: 0.582678, acc.: 71.88%] [G loss: 0.929928]\n",
      "epoch:6 step:5942 [D loss: 0.601862, acc.: 66.41%] [G loss: 1.016257]\n",
      "epoch:6 step:5943 [D loss: 0.617505, acc.: 68.75%] [G loss: 1.130891]\n",
      "epoch:6 step:5944 [D loss: 0.508360, acc.: 78.12%] [G loss: 0.912624]\n",
      "epoch:6 step:5945 [D loss: 0.619580, acc.: 62.50%] [G loss: 1.019214]\n",
      "epoch:6 step:5946 [D loss: 0.627229, acc.: 57.81%] [G loss: 1.059606]\n",
      "epoch:6 step:5947 [D loss: 0.565873, acc.: 71.09%] [G loss: 1.020901]\n",
      "epoch:6 step:5948 [D loss: 0.679686, acc.: 60.94%] [G loss: 1.037136]\n",
      "epoch:6 step:5949 [D loss: 0.648079, acc.: 63.28%] [G loss: 1.197545]\n",
      "epoch:6 step:5950 [D loss: 0.526004, acc.: 78.12%] [G loss: 1.177461]\n",
      "epoch:6 step:5951 [D loss: 0.645709, acc.: 66.41%] [G loss: 1.182949]\n",
      "epoch:6 step:5952 [D loss: 0.719135, acc.: 53.91%] [G loss: 0.991062]\n",
      "epoch:6 step:5953 [D loss: 0.537473, acc.: 78.12%] [G loss: 1.084640]\n",
      "epoch:6 step:5954 [D loss: 0.599225, acc.: 71.88%] [G loss: 1.158933]\n",
      "epoch:6 step:5955 [D loss: 0.686047, acc.: 55.47%] [G loss: 1.032098]\n",
      "epoch:6 step:5956 [D loss: 0.578107, acc.: 74.22%] [G loss: 1.232968]\n",
      "epoch:6 step:5957 [D loss: 0.584429, acc.: 71.88%] [G loss: 1.258879]\n",
      "epoch:6 step:5958 [D loss: 0.544064, acc.: 72.66%] [G loss: 1.044188]\n",
      "epoch:6 step:5959 [D loss: 0.657694, acc.: 62.50%] [G loss: 1.200048]\n",
      "epoch:6 step:5960 [D loss: 0.582079, acc.: 71.09%] [G loss: 1.079032]\n",
      "epoch:6 step:5961 [D loss: 0.670659, acc.: 60.16%] [G loss: 1.119915]\n",
      "epoch:6 step:5962 [D loss: 0.606957, acc.: 67.19%] [G loss: 1.002244]\n",
      "epoch:6 step:5963 [D loss: 0.661454, acc.: 57.81%] [G loss: 1.119151]\n",
      "epoch:6 step:5964 [D loss: 0.752844, acc.: 50.78%] [G loss: 1.027283]\n",
      "epoch:6 step:5965 [D loss: 0.637307, acc.: 64.84%] [G loss: 1.086748]\n",
      "epoch:6 step:5966 [D loss: 0.695020, acc.: 57.03%] [G loss: 1.078324]\n",
      "epoch:6 step:5967 [D loss: 0.591436, acc.: 71.88%] [G loss: 1.189094]\n",
      "epoch:6 step:5968 [D loss: 0.670244, acc.: 64.06%] [G loss: 1.063124]\n",
      "epoch:6 step:5969 [D loss: 0.664992, acc.: 61.72%] [G loss: 0.938532]\n",
      "epoch:6 step:5970 [D loss: 0.697476, acc.: 57.81%] [G loss: 1.017128]\n",
      "epoch:6 step:5971 [D loss: 0.615996, acc.: 64.84%] [G loss: 0.915363]\n",
      "epoch:6 step:5972 [D loss: 0.600040, acc.: 68.75%] [G loss: 1.119900]\n",
      "epoch:6 step:5973 [D loss: 0.692552, acc.: 61.72%] [G loss: 1.150479]\n",
      "epoch:6 step:5974 [D loss: 0.697879, acc.: 55.47%] [G loss: 0.912831]\n",
      "epoch:6 step:5975 [D loss: 0.703835, acc.: 57.03%] [G loss: 1.156104]\n",
      "epoch:6 step:5976 [D loss: 0.701691, acc.: 57.03%] [G loss: 1.042401]\n",
      "epoch:6 step:5977 [D loss: 0.614823, acc.: 64.84%] [G loss: 1.151252]\n",
      "epoch:6 step:5978 [D loss: 0.635426, acc.: 64.06%] [G loss: 1.244355]\n",
      "epoch:6 step:5979 [D loss: 0.588311, acc.: 67.19%] [G loss: 1.149538]\n",
      "epoch:6 step:5980 [D loss: 0.679467, acc.: 59.38%] [G loss: 1.204644]\n",
      "epoch:6 step:5981 [D loss: 0.692294, acc.: 60.16%] [G loss: 0.854189]\n",
      "epoch:6 step:5982 [D loss: 0.591671, acc.: 66.41%] [G loss: 1.029498]\n",
      "epoch:6 step:5983 [D loss: 0.633002, acc.: 60.94%] [G loss: 1.032246]\n",
      "epoch:6 step:5984 [D loss: 0.680597, acc.: 57.81%] [G loss: 1.113284]\n",
      "epoch:6 step:5985 [D loss: 0.715891, acc.: 51.56%] [G loss: 1.034853]\n",
      "epoch:6 step:5986 [D loss: 0.691693, acc.: 57.81%] [G loss: 1.003535]\n",
      "epoch:6 step:5987 [D loss: 0.568056, acc.: 69.53%] [G loss: 1.082122]\n",
      "epoch:6 step:5988 [D loss: 0.568925, acc.: 71.09%] [G loss: 1.271674]\n",
      "epoch:6 step:5989 [D loss: 0.683522, acc.: 55.47%] [G loss: 1.348940]\n",
      "epoch:6 step:5990 [D loss: 0.605956, acc.: 67.97%] [G loss: 1.059047]\n",
      "epoch:6 step:5991 [D loss: 0.668407, acc.: 63.28%] [G loss: 1.172502]\n",
      "epoch:6 step:5992 [D loss: 0.631156, acc.: 60.94%] [G loss: 0.956929]\n",
      "epoch:6 step:5993 [D loss: 0.589318, acc.: 69.53%] [G loss: 1.041957]\n",
      "epoch:6 step:5994 [D loss: 0.634851, acc.: 64.06%] [G loss: 1.025770]\n",
      "epoch:6 step:5995 [D loss: 0.623853, acc.: 61.72%] [G loss: 1.097320]\n",
      "epoch:6 step:5996 [D loss: 0.649418, acc.: 59.38%] [G loss: 1.133531]\n",
      "epoch:6 step:5997 [D loss: 0.735149, acc.: 54.69%] [G loss: 1.030001]\n",
      "epoch:6 step:5998 [D loss: 0.603951, acc.: 61.72%] [G loss: 0.902429]\n",
      "epoch:6 step:5999 [D loss: 0.726043, acc.: 56.25%] [G loss: 0.995305]\n",
      "epoch:6 step:6000 [D loss: 0.653082, acc.: 62.50%] [G loss: 1.134285]\n",
      "##############\n",
      "[2.64028492 2.09133801 1.92151483 2.71080608 0.804224   6.02847636\n",
      " 2.00034009 2.9621849  3.7694515  6.17820181]\n",
      "##########\n",
      "epoch:6 step:6001 [D loss: 0.614449, acc.: 64.06%] [G loss: 1.231022]\n",
      "epoch:6 step:6002 [D loss: 0.678528, acc.: 58.59%] [G loss: 1.124048]\n",
      "epoch:6 step:6003 [D loss: 0.650587, acc.: 65.62%] [G loss: 0.969204]\n",
      "epoch:6 step:6004 [D loss: 0.653686, acc.: 64.06%] [G loss: 1.039347]\n",
      "epoch:6 step:6005 [D loss: 0.566125, acc.: 69.53%] [G loss: 1.166150]\n",
      "epoch:6 step:6006 [D loss: 0.686667, acc.: 60.16%] [G loss: 1.173945]\n",
      "epoch:6 step:6007 [D loss: 0.714766, acc.: 50.00%] [G loss: 1.087381]\n",
      "epoch:6 step:6008 [D loss: 0.590796, acc.: 65.62%] [G loss: 1.101747]\n",
      "epoch:6 step:6009 [D loss: 0.663740, acc.: 67.97%] [G loss: 1.042928]\n",
      "epoch:6 step:6010 [D loss: 0.646910, acc.: 59.38%] [G loss: 1.108550]\n",
      "epoch:6 step:6011 [D loss: 0.600091, acc.: 69.53%] [G loss: 1.052638]\n",
      "epoch:6 step:6012 [D loss: 0.627769, acc.: 59.38%] [G loss: 0.958285]\n",
      "epoch:6 step:6013 [D loss: 0.608212, acc.: 67.19%] [G loss: 1.156227]\n",
      "epoch:6 step:6014 [D loss: 0.656861, acc.: 62.50%] [G loss: 1.112556]\n",
      "epoch:6 step:6015 [D loss: 0.656791, acc.: 64.06%] [G loss: 1.229610]\n",
      "epoch:6 step:6016 [D loss: 0.579989, acc.: 73.44%] [G loss: 1.233320]\n",
      "epoch:6 step:6017 [D loss: 0.567444, acc.: 70.31%] [G loss: 1.217194]\n",
      "epoch:6 step:6018 [D loss: 0.583293, acc.: 68.75%] [G loss: 1.007605]\n",
      "epoch:6 step:6019 [D loss: 0.622083, acc.: 64.84%] [G loss: 1.056602]\n",
      "epoch:6 step:6020 [D loss: 0.650717, acc.: 62.50%] [G loss: 0.989365]\n",
      "epoch:6 step:6021 [D loss: 0.519305, acc.: 75.00%] [G loss: 1.180387]\n",
      "epoch:6 step:6022 [D loss: 0.674189, acc.: 60.94%] [G loss: 0.957353]\n",
      "epoch:6 step:6023 [D loss: 0.647233, acc.: 61.72%] [G loss: 1.015901]\n",
      "epoch:6 step:6024 [D loss: 0.624818, acc.: 63.28%] [G loss: 1.135382]\n",
      "epoch:6 step:6025 [D loss: 0.636984, acc.: 60.94%] [G loss: 1.106538]\n",
      "epoch:6 step:6026 [D loss: 0.724392, acc.: 52.34%] [G loss: 0.970884]\n",
      "epoch:6 step:6027 [D loss: 0.557674, acc.: 69.53%] [G loss: 0.955418]\n",
      "epoch:6 step:6028 [D loss: 0.702700, acc.: 59.38%] [G loss: 0.958402]\n",
      "epoch:6 step:6029 [D loss: 0.667350, acc.: 60.94%] [G loss: 1.271981]\n",
      "epoch:6 step:6030 [D loss: 0.612331, acc.: 62.50%] [G loss: 1.296811]\n",
      "epoch:6 step:6031 [D loss: 0.647641, acc.: 64.06%] [G loss: 1.101656]\n",
      "epoch:6 step:6032 [D loss: 0.745014, acc.: 50.78%] [G loss: 1.066252]\n",
      "epoch:6 step:6033 [D loss: 0.719511, acc.: 55.47%] [G loss: 1.099534]\n",
      "epoch:6 step:6034 [D loss: 0.667943, acc.: 58.59%] [G loss: 1.051669]\n",
      "epoch:6 step:6035 [D loss: 0.597123, acc.: 68.75%] [G loss: 1.054257]\n",
      "epoch:6 step:6036 [D loss: 0.605077, acc.: 67.19%] [G loss: 1.037559]\n",
      "epoch:6 step:6037 [D loss: 0.582512, acc.: 67.97%] [G loss: 1.285271]\n",
      "epoch:6 step:6038 [D loss: 0.577226, acc.: 68.75%] [G loss: 0.893469]\n",
      "epoch:6 step:6039 [D loss: 0.600437, acc.: 62.50%] [G loss: 1.019491]\n",
      "epoch:6 step:6040 [D loss: 0.629601, acc.: 63.28%] [G loss: 1.014262]\n",
      "epoch:6 step:6041 [D loss: 0.552804, acc.: 71.88%] [G loss: 1.105125]\n",
      "epoch:6 step:6042 [D loss: 0.684043, acc.: 55.47%] [G loss: 1.033864]\n",
      "epoch:6 step:6043 [D loss: 0.616689, acc.: 63.28%] [G loss: 1.038069]\n",
      "epoch:6 step:6044 [D loss: 0.689635, acc.: 62.50%] [G loss: 0.926485]\n",
      "epoch:6 step:6045 [D loss: 0.547238, acc.: 71.88%] [G loss: 1.101585]\n",
      "epoch:6 step:6046 [D loss: 0.579118, acc.: 68.75%] [G loss: 1.028482]\n",
      "epoch:6 step:6047 [D loss: 0.613916, acc.: 63.28%] [G loss: 1.184991]\n",
      "epoch:6 step:6048 [D loss: 0.663233, acc.: 60.94%] [G loss: 1.223833]\n",
      "epoch:6 step:6049 [D loss: 0.541542, acc.: 74.22%] [G loss: 1.027810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6050 [D loss: 0.672174, acc.: 60.94%] [G loss: 1.242341]\n",
      "epoch:6 step:6051 [D loss: 0.554219, acc.: 71.09%] [G loss: 1.062543]\n",
      "epoch:6 step:6052 [D loss: 0.660150, acc.: 64.84%] [G loss: 1.142752]\n",
      "epoch:6 step:6053 [D loss: 0.719046, acc.: 57.81%] [G loss: 1.216199]\n",
      "epoch:6 step:6054 [D loss: 0.664891, acc.: 59.38%] [G loss: 1.121365]\n",
      "epoch:6 step:6055 [D loss: 0.591747, acc.: 66.41%] [G loss: 1.165590]\n",
      "epoch:6 step:6056 [D loss: 0.684782, acc.: 59.38%] [G loss: 1.076984]\n",
      "epoch:6 step:6057 [D loss: 0.586657, acc.: 71.09%] [G loss: 0.963967]\n",
      "epoch:6 step:6058 [D loss: 0.643533, acc.: 58.59%] [G loss: 1.065876]\n",
      "epoch:6 step:6059 [D loss: 0.636019, acc.: 61.72%] [G loss: 1.049262]\n",
      "epoch:6 step:6060 [D loss: 0.678290, acc.: 59.38%] [G loss: 0.988761]\n",
      "epoch:6 step:6061 [D loss: 0.628939, acc.: 67.97%] [G loss: 1.140146]\n",
      "epoch:6 step:6062 [D loss: 0.499130, acc.: 80.47%] [G loss: 1.072296]\n",
      "epoch:6 step:6063 [D loss: 0.610668, acc.: 67.97%] [G loss: 1.071356]\n",
      "epoch:6 step:6064 [D loss: 0.706432, acc.: 55.47%] [G loss: 1.114554]\n",
      "epoch:6 step:6065 [D loss: 0.553935, acc.: 66.41%] [G loss: 1.157311]\n",
      "epoch:6 step:6066 [D loss: 0.595165, acc.: 70.31%] [G loss: 0.981964]\n",
      "epoch:6 step:6067 [D loss: 0.567305, acc.: 74.22%] [G loss: 1.032276]\n",
      "epoch:6 step:6068 [D loss: 0.694674, acc.: 58.59%] [G loss: 1.013294]\n",
      "epoch:6 step:6069 [D loss: 0.654513, acc.: 55.47%] [G loss: 1.131730]\n",
      "epoch:6 step:6070 [D loss: 0.696748, acc.: 55.47%] [G loss: 1.043641]\n",
      "epoch:6 step:6071 [D loss: 0.618253, acc.: 64.06%] [G loss: 1.030097]\n",
      "epoch:6 step:6072 [D loss: 0.598327, acc.: 64.84%] [G loss: 1.149946]\n",
      "epoch:6 step:6073 [D loss: 0.553121, acc.: 71.09%] [G loss: 1.100298]\n",
      "epoch:6 step:6074 [D loss: 0.549851, acc.: 71.88%] [G loss: 1.147405]\n",
      "epoch:6 step:6075 [D loss: 0.632000, acc.: 67.97%] [G loss: 1.128140]\n",
      "epoch:6 step:6076 [D loss: 0.687488, acc.: 55.47%] [G loss: 0.975765]\n",
      "epoch:6 step:6077 [D loss: 0.578031, acc.: 67.97%] [G loss: 1.146932]\n",
      "epoch:6 step:6078 [D loss: 0.734835, acc.: 57.03%] [G loss: 1.038556]\n",
      "epoch:6 step:6079 [D loss: 0.661411, acc.: 59.38%] [G loss: 1.027201]\n",
      "epoch:6 step:6080 [D loss: 0.639524, acc.: 61.72%] [G loss: 0.886363]\n",
      "epoch:6 step:6081 [D loss: 0.669701, acc.: 61.72%] [G loss: 0.932698]\n",
      "epoch:6 step:6082 [D loss: 0.558621, acc.: 71.88%] [G loss: 1.183054]\n",
      "epoch:6 step:6083 [D loss: 0.742082, acc.: 46.09%] [G loss: 1.135032]\n",
      "epoch:6 step:6084 [D loss: 0.625943, acc.: 64.06%] [G loss: 1.078504]\n",
      "epoch:6 step:6085 [D loss: 0.582421, acc.: 71.09%] [G loss: 0.913844]\n",
      "epoch:6 step:6086 [D loss: 0.591830, acc.: 66.41%] [G loss: 0.869190]\n",
      "epoch:6 step:6087 [D loss: 0.653211, acc.: 61.72%] [G loss: 1.129049]\n",
      "epoch:6 step:6088 [D loss: 0.762861, acc.: 49.22%] [G loss: 0.842990]\n",
      "epoch:6 step:6089 [D loss: 0.491638, acc.: 83.59%] [G loss: 1.420361]\n",
      "epoch:6 step:6090 [D loss: 0.579494, acc.: 69.53%] [G loss: 1.150853]\n",
      "epoch:6 step:6091 [D loss: 0.568692, acc.: 73.44%] [G loss: 1.208157]\n",
      "epoch:6 step:6092 [D loss: 0.781214, acc.: 49.22%] [G loss: 0.993732]\n",
      "epoch:6 step:6093 [D loss: 0.640781, acc.: 63.28%] [G loss: 0.965676]\n",
      "epoch:6 step:6094 [D loss: 0.546147, acc.: 75.78%] [G loss: 1.065272]\n",
      "epoch:6 step:6095 [D loss: 0.564781, acc.: 71.09%] [G loss: 0.984086]\n",
      "epoch:6 step:6096 [D loss: 0.576316, acc.: 73.44%] [G loss: 1.073585]\n",
      "epoch:6 step:6097 [D loss: 0.578905, acc.: 71.09%] [G loss: 0.997258]\n",
      "epoch:6 step:6098 [D loss: 0.534027, acc.: 71.88%] [G loss: 1.175969]\n",
      "epoch:6 step:6099 [D loss: 0.716678, acc.: 55.47%] [G loss: 0.970299]\n",
      "epoch:6 step:6100 [D loss: 0.576870, acc.: 71.88%] [G loss: 1.217163]\n",
      "epoch:6 step:6101 [D loss: 0.691243, acc.: 60.16%] [G loss: 0.957080]\n",
      "epoch:6 step:6102 [D loss: 0.629832, acc.: 61.72%] [G loss: 1.022338]\n",
      "epoch:6 step:6103 [D loss: 0.717454, acc.: 53.91%] [G loss: 1.050188]\n",
      "epoch:6 step:6104 [D loss: 0.541658, acc.: 75.00%] [G loss: 1.151573]\n",
      "epoch:6 step:6105 [D loss: 0.698312, acc.: 52.34%] [G loss: 1.052025]\n",
      "epoch:6 step:6106 [D loss: 0.477366, acc.: 78.91%] [G loss: 1.233955]\n",
      "epoch:6 step:6107 [D loss: 0.639367, acc.: 60.94%] [G loss: 1.186979]\n",
      "epoch:6 step:6108 [D loss: 0.524210, acc.: 78.12%] [G loss: 1.027691]\n",
      "epoch:6 step:6109 [D loss: 0.595118, acc.: 67.97%] [G loss: 1.193747]\n",
      "epoch:6 step:6110 [D loss: 0.523823, acc.: 72.66%] [G loss: 0.910992]\n",
      "epoch:6 step:6111 [D loss: 0.628908, acc.: 64.84%] [G loss: 1.099513]\n",
      "epoch:6 step:6112 [D loss: 0.653656, acc.: 64.06%] [G loss: 0.951761]\n",
      "epoch:6 step:6113 [D loss: 0.538478, acc.: 78.12%] [G loss: 0.992296]\n",
      "epoch:6 step:6114 [D loss: 0.608749, acc.: 64.06%] [G loss: 0.964883]\n",
      "epoch:6 step:6115 [D loss: 0.678681, acc.: 54.69%] [G loss: 0.970722]\n",
      "epoch:6 step:6116 [D loss: 0.582130, acc.: 70.31%] [G loss: 1.119726]\n",
      "epoch:6 step:6117 [D loss: 0.645262, acc.: 64.06%] [G loss: 1.114902]\n",
      "epoch:6 step:6118 [D loss: 0.563773, acc.: 76.56%] [G loss: 1.133065]\n",
      "epoch:6 step:6119 [D loss: 0.657141, acc.: 62.50%] [G loss: 1.071362]\n",
      "epoch:6 step:6120 [D loss: 0.625406, acc.: 64.84%] [G loss: 1.139070]\n",
      "epoch:6 step:6121 [D loss: 0.611832, acc.: 69.53%] [G loss: 1.080450]\n",
      "epoch:6 step:6122 [D loss: 0.533994, acc.: 73.44%] [G loss: 1.062556]\n",
      "epoch:6 step:6123 [D loss: 0.532915, acc.: 76.56%] [G loss: 0.911055]\n",
      "epoch:6 step:6124 [D loss: 0.589382, acc.: 71.09%] [G loss: 1.068275]\n",
      "epoch:6 step:6125 [D loss: 0.561374, acc.: 71.88%] [G loss: 1.107221]\n",
      "epoch:6 step:6126 [D loss: 0.579005, acc.: 71.09%] [G loss: 1.097121]\n",
      "epoch:6 step:6127 [D loss: 0.527177, acc.: 78.12%] [G loss: 1.256037]\n",
      "epoch:6 step:6128 [D loss: 0.681040, acc.: 65.62%] [G loss: 0.938225]\n",
      "epoch:6 step:6129 [D loss: 0.651469, acc.: 60.94%] [G loss: 1.146121]\n",
      "epoch:6 step:6130 [D loss: 0.587240, acc.: 67.19%] [G loss: 0.982337]\n",
      "epoch:6 step:6131 [D loss: 0.749282, acc.: 50.78%] [G loss: 0.811056]\n",
      "epoch:6 step:6132 [D loss: 0.573417, acc.: 69.53%] [G loss: 1.064713]\n",
      "epoch:6 step:6133 [D loss: 0.646332, acc.: 60.94%] [G loss: 0.999872]\n",
      "epoch:6 step:6134 [D loss: 0.569488, acc.: 75.78%] [G loss: 1.086757]\n",
      "epoch:6 step:6135 [D loss: 0.582174, acc.: 69.53%] [G loss: 1.102716]\n",
      "epoch:6 step:6136 [D loss: 0.659186, acc.: 59.38%] [G loss: 1.298846]\n",
      "epoch:6 step:6137 [D loss: 0.566213, acc.: 75.00%] [G loss: 1.295298]\n",
      "epoch:6 step:6138 [D loss: 0.626138, acc.: 65.62%] [G loss: 1.360179]\n",
      "epoch:6 step:6139 [D loss: 0.731034, acc.: 55.47%] [G loss: 1.130909]\n",
      "epoch:6 step:6140 [D loss: 0.622521, acc.: 67.19%] [G loss: 1.044739]\n",
      "epoch:6 step:6141 [D loss: 0.606116, acc.: 66.41%] [G loss: 1.204756]\n",
      "epoch:6 step:6142 [D loss: 0.745683, acc.: 46.09%] [G loss: 1.052422]\n",
      "epoch:6 step:6143 [D loss: 0.725240, acc.: 49.22%] [G loss: 0.990470]\n",
      "epoch:6 step:6144 [D loss: 0.603045, acc.: 65.62%] [G loss: 1.026842]\n",
      "epoch:6 step:6145 [D loss: 0.535552, acc.: 77.34%] [G loss: 1.044109]\n",
      "epoch:6 step:6146 [D loss: 0.533719, acc.: 78.91%] [G loss: 1.088297]\n",
      "epoch:6 step:6147 [D loss: 0.627828, acc.: 64.84%] [G loss: 1.049022]\n",
      "epoch:6 step:6148 [D loss: 0.776144, acc.: 48.44%] [G loss: 1.022935]\n",
      "epoch:6 step:6149 [D loss: 0.666616, acc.: 57.81%] [G loss: 1.131952]\n",
      "epoch:6 step:6150 [D loss: 0.681514, acc.: 63.28%] [G loss: 1.088791]\n",
      "epoch:6 step:6151 [D loss: 0.546327, acc.: 75.00%] [G loss: 1.031841]\n",
      "epoch:6 step:6152 [D loss: 0.763463, acc.: 50.00%] [G loss: 1.032994]\n",
      "epoch:6 step:6153 [D loss: 0.618809, acc.: 64.06%] [G loss: 1.042090]\n",
      "epoch:6 step:6154 [D loss: 0.697826, acc.: 53.12%] [G loss: 1.038128]\n",
      "epoch:6 step:6155 [D loss: 0.565350, acc.: 70.31%] [G loss: 1.249688]\n",
      "epoch:6 step:6156 [D loss: 0.629459, acc.: 58.59%] [G loss: 1.084294]\n",
      "epoch:6 step:6157 [D loss: 0.651790, acc.: 62.50%] [G loss: 0.916316]\n",
      "epoch:6 step:6158 [D loss: 0.662629, acc.: 57.81%] [G loss: 1.012513]\n",
      "epoch:6 step:6159 [D loss: 0.778724, acc.: 51.56%] [G loss: 1.062591]\n",
      "epoch:6 step:6160 [D loss: 0.663771, acc.: 57.03%] [G loss: 1.122859]\n",
      "epoch:6 step:6161 [D loss: 0.652005, acc.: 60.16%] [G loss: 1.147254]\n",
      "epoch:6 step:6162 [D loss: 0.587783, acc.: 69.53%] [G loss: 1.045788]\n",
      "epoch:6 step:6163 [D loss: 0.614880, acc.: 67.19%] [G loss: 1.189104]\n",
      "epoch:6 step:6164 [D loss: 0.543631, acc.: 72.66%] [G loss: 1.223912]\n",
      "epoch:6 step:6165 [D loss: 0.611997, acc.: 67.19%] [G loss: 1.170740]\n",
      "epoch:6 step:6166 [D loss: 0.592333, acc.: 69.53%] [G loss: 1.172511]\n",
      "epoch:6 step:6167 [D loss: 0.641182, acc.: 69.53%] [G loss: 0.985685]\n",
      "epoch:6 step:6168 [D loss: 0.582644, acc.: 67.19%] [G loss: 1.043225]\n",
      "epoch:6 step:6169 [D loss: 0.651908, acc.: 61.72%] [G loss: 1.056098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6170 [D loss: 0.667169, acc.: 59.38%] [G loss: 0.899495]\n",
      "epoch:6 step:6171 [D loss: 0.661085, acc.: 58.59%] [G loss: 1.076820]\n",
      "epoch:6 step:6172 [D loss: 0.552119, acc.: 75.00%] [G loss: 1.155867]\n",
      "epoch:6 step:6173 [D loss: 0.698833, acc.: 59.38%] [G loss: 1.093640]\n",
      "epoch:6 step:6174 [D loss: 0.648261, acc.: 60.94%] [G loss: 1.036886]\n",
      "epoch:6 step:6175 [D loss: 0.601542, acc.: 66.41%] [G loss: 1.188391]\n",
      "epoch:6 step:6176 [D loss: 0.583177, acc.: 70.31%] [G loss: 1.291617]\n",
      "epoch:6 step:6177 [D loss: 0.601702, acc.: 65.62%] [G loss: 1.037530]\n",
      "epoch:6 step:6178 [D loss: 0.708653, acc.: 55.47%] [G loss: 0.986117]\n",
      "epoch:6 step:6179 [D loss: 0.645220, acc.: 63.28%] [G loss: 1.089262]\n",
      "epoch:6 step:6180 [D loss: 0.653589, acc.: 61.72%] [G loss: 1.020953]\n",
      "epoch:6 step:6181 [D loss: 0.607705, acc.: 66.41%] [G loss: 1.120874]\n",
      "epoch:6 step:6182 [D loss: 0.584427, acc.: 68.75%] [G loss: 1.123572]\n",
      "epoch:6 step:6183 [D loss: 0.699702, acc.: 53.91%] [G loss: 1.000562]\n",
      "epoch:6 step:6184 [D loss: 0.647934, acc.: 57.03%] [G loss: 1.021792]\n",
      "epoch:6 step:6185 [D loss: 0.590228, acc.: 66.41%] [G loss: 1.211372]\n",
      "epoch:6 step:6186 [D loss: 0.604112, acc.: 67.19%] [G loss: 1.279354]\n",
      "epoch:6 step:6187 [D loss: 0.619432, acc.: 61.72%] [G loss: 1.211064]\n",
      "epoch:6 step:6188 [D loss: 0.632889, acc.: 64.06%] [G loss: 1.105846]\n",
      "epoch:6 step:6189 [D loss: 0.644329, acc.: 63.28%] [G loss: 1.052591]\n",
      "epoch:6 step:6190 [D loss: 0.655088, acc.: 64.06%] [G loss: 0.938703]\n",
      "epoch:6 step:6191 [D loss: 0.654972, acc.: 62.50%] [G loss: 0.938267]\n",
      "epoch:6 step:6192 [D loss: 0.660020, acc.: 60.16%] [G loss: 0.972105]\n",
      "epoch:6 step:6193 [D loss: 0.602002, acc.: 68.75%] [G loss: 0.989074]\n",
      "epoch:6 step:6194 [D loss: 0.723144, acc.: 55.47%] [G loss: 1.079638]\n",
      "epoch:6 step:6195 [D loss: 0.662273, acc.: 59.38%] [G loss: 1.325403]\n",
      "epoch:6 step:6196 [D loss: 0.718316, acc.: 55.47%] [G loss: 1.081554]\n",
      "epoch:6 step:6197 [D loss: 0.631829, acc.: 65.62%] [G loss: 1.217535]\n",
      "epoch:6 step:6198 [D loss: 0.587940, acc.: 71.09%] [G loss: 1.116912]\n",
      "epoch:6 step:6199 [D loss: 0.611513, acc.: 68.75%] [G loss: 1.172831]\n",
      "epoch:6 step:6200 [D loss: 0.641430, acc.: 63.28%] [G loss: 1.180722]\n",
      "##############\n",
      "[2.53954999 2.1289159  1.87934002 3.2339392  1.06184336 6.22090424\n",
      " 1.95309982 2.65504491 3.88114078 7.14868929]\n",
      "##########\n",
      "epoch:6 step:6201 [D loss: 0.697631, acc.: 62.50%] [G loss: 1.073532]\n",
      "epoch:6 step:6202 [D loss: 0.637510, acc.: 62.50%] [G loss: 1.024613]\n",
      "epoch:6 step:6203 [D loss: 0.685787, acc.: 61.72%] [G loss: 1.033930]\n",
      "epoch:6 step:6204 [D loss: 0.594012, acc.: 67.19%] [G loss: 1.149649]\n",
      "epoch:6 step:6205 [D loss: 0.590858, acc.: 67.97%] [G loss: 1.291772]\n",
      "epoch:6 step:6206 [D loss: 0.663524, acc.: 63.28%] [G loss: 1.016707]\n",
      "epoch:6 step:6207 [D loss: 0.537363, acc.: 74.22%] [G loss: 1.031392]\n",
      "epoch:6 step:6208 [D loss: 0.645484, acc.: 63.28%] [G loss: 1.159436]\n",
      "epoch:6 step:6209 [D loss: 0.569367, acc.: 67.19%] [G loss: 1.123519]\n",
      "epoch:6 step:6210 [D loss: 0.751946, acc.: 51.56%] [G loss: 0.856866]\n",
      "epoch:6 step:6211 [D loss: 0.659410, acc.: 59.38%] [G loss: 0.966471]\n",
      "epoch:6 step:6212 [D loss: 0.619181, acc.: 65.62%] [G loss: 1.181232]\n",
      "epoch:6 step:6213 [D loss: 0.609565, acc.: 68.75%] [G loss: 1.087152]\n",
      "epoch:6 step:6214 [D loss: 0.731759, acc.: 53.91%] [G loss: 0.922138]\n",
      "epoch:6 step:6215 [D loss: 0.745549, acc.: 47.66%] [G loss: 0.945011]\n",
      "epoch:6 step:6216 [D loss: 0.596033, acc.: 68.75%] [G loss: 1.187938]\n",
      "epoch:6 step:6217 [D loss: 0.706452, acc.: 58.59%] [G loss: 0.984231]\n",
      "epoch:6 step:6218 [D loss: 0.611704, acc.: 64.84%] [G loss: 1.030334]\n",
      "epoch:6 step:6219 [D loss: 0.617293, acc.: 66.41%] [G loss: 1.247983]\n",
      "epoch:6 step:6220 [D loss: 0.619566, acc.: 65.62%] [G loss: 1.129818]\n",
      "epoch:6 step:6221 [D loss: 0.569792, acc.: 71.09%] [G loss: 1.225999]\n",
      "epoch:6 step:6222 [D loss: 0.579441, acc.: 70.31%] [G loss: 1.027739]\n",
      "epoch:6 step:6223 [D loss: 0.627468, acc.: 66.41%] [G loss: 1.300702]\n",
      "epoch:6 step:6224 [D loss: 0.546405, acc.: 79.69%] [G loss: 1.059145]\n",
      "epoch:6 step:6225 [D loss: 0.690495, acc.: 57.03%] [G loss: 1.017357]\n",
      "epoch:6 step:6226 [D loss: 0.548292, acc.: 76.56%] [G loss: 1.109727]\n",
      "epoch:6 step:6227 [D loss: 0.592622, acc.: 67.97%] [G loss: 1.200552]\n",
      "epoch:6 step:6228 [D loss: 0.611730, acc.: 67.19%] [G loss: 1.084974]\n",
      "epoch:6 step:6229 [D loss: 0.566095, acc.: 71.88%] [G loss: 1.072940]\n",
      "epoch:6 step:6230 [D loss: 0.631130, acc.: 66.41%] [G loss: 1.138109]\n",
      "epoch:6 step:6231 [D loss: 0.615097, acc.: 69.53%] [G loss: 1.255568]\n",
      "epoch:6 step:6232 [D loss: 0.667725, acc.: 65.62%] [G loss: 1.056923]\n",
      "epoch:6 step:6233 [D loss: 0.628178, acc.: 60.16%] [G loss: 0.991987]\n",
      "epoch:6 step:6234 [D loss: 0.593772, acc.: 67.97%] [G loss: 0.970440]\n",
      "epoch:6 step:6235 [D loss: 0.640205, acc.: 64.06%] [G loss: 0.972628]\n",
      "epoch:6 step:6236 [D loss: 0.631623, acc.: 64.06%] [G loss: 1.049094]\n",
      "epoch:6 step:6237 [D loss: 0.566047, acc.: 74.22%] [G loss: 1.107533]\n",
      "epoch:6 step:6238 [D loss: 0.554644, acc.: 72.66%] [G loss: 1.171906]\n",
      "epoch:6 step:6239 [D loss: 0.668908, acc.: 58.59%] [G loss: 1.085060]\n",
      "epoch:6 step:6240 [D loss: 0.646560, acc.: 61.72%] [G loss: 0.870303]\n",
      "epoch:6 step:6241 [D loss: 0.629437, acc.: 65.62%] [G loss: 1.156822]\n",
      "epoch:6 step:6242 [D loss: 0.726301, acc.: 53.12%] [G loss: 1.099156]\n",
      "epoch:6 step:6243 [D loss: 0.655554, acc.: 65.62%] [G loss: 0.989217]\n",
      "epoch:6 step:6244 [D loss: 0.650943, acc.: 59.38%] [G loss: 0.991320]\n",
      "epoch:6 step:6245 [D loss: 0.614315, acc.: 65.62%] [G loss: 1.148798]\n",
      "epoch:6 step:6246 [D loss: 0.628769, acc.: 64.84%] [G loss: 1.117349]\n",
      "epoch:6 step:6247 [D loss: 0.628211, acc.: 63.28%] [G loss: 1.114268]\n",
      "epoch:6 step:6248 [D loss: 0.577677, acc.: 69.53%] [G loss: 1.274934]\n",
      "epoch:6 step:6249 [D loss: 0.591163, acc.: 66.41%] [G loss: 1.005241]\n",
      "epoch:6 step:6250 [D loss: 0.800823, acc.: 48.44%] [G loss: 1.044633]\n",
      "epoch:6 step:6251 [D loss: 0.710515, acc.: 53.12%] [G loss: 0.851250]\n",
      "epoch:6 step:6252 [D loss: 0.692742, acc.: 53.91%] [G loss: 1.211619]\n",
      "epoch:6 step:6253 [D loss: 0.562349, acc.: 65.62%] [G loss: 1.246768]\n",
      "epoch:6 step:6254 [D loss: 0.543561, acc.: 72.66%] [G loss: 1.137931]\n",
      "epoch:6 step:6255 [D loss: 0.556337, acc.: 71.88%] [G loss: 1.010041]\n",
      "epoch:6 step:6256 [D loss: 0.574544, acc.: 73.44%] [G loss: 1.205291]\n",
      "epoch:6 step:6257 [D loss: 0.619368, acc.: 64.84%] [G loss: 1.033183]\n",
      "epoch:6 step:6258 [D loss: 0.702786, acc.: 57.03%] [G loss: 1.146279]\n",
      "epoch:6 step:6259 [D loss: 0.629124, acc.: 64.06%] [G loss: 1.085833]\n",
      "epoch:6 step:6260 [D loss: 0.577955, acc.: 74.22%] [G loss: 1.025759]\n",
      "epoch:6 step:6261 [D loss: 0.683698, acc.: 58.59%] [G loss: 1.138417]\n",
      "epoch:6 step:6262 [D loss: 0.648961, acc.: 65.62%] [G loss: 1.039631]\n",
      "epoch:6 step:6263 [D loss: 0.689432, acc.: 56.25%] [G loss: 0.947981]\n",
      "epoch:6 step:6264 [D loss: 0.729813, acc.: 54.69%] [G loss: 0.883616]\n",
      "epoch:6 step:6265 [D loss: 0.640128, acc.: 63.28%] [G loss: 0.926662]\n",
      "epoch:6 step:6266 [D loss: 0.576404, acc.: 65.62%] [G loss: 1.312746]\n",
      "epoch:6 step:6267 [D loss: 0.633548, acc.: 60.94%] [G loss: 1.120855]\n",
      "epoch:6 step:6268 [D loss: 0.650901, acc.: 60.94%] [G loss: 1.062729]\n",
      "epoch:6 step:6269 [D loss: 0.708764, acc.: 57.03%] [G loss: 0.918583]\n",
      "epoch:6 step:6270 [D loss: 0.560593, acc.: 71.09%] [G loss: 1.341246]\n",
      "epoch:6 step:6271 [D loss: 0.578148, acc.: 69.53%] [G loss: 0.997650]\n",
      "epoch:6 step:6272 [D loss: 0.661922, acc.: 57.03%] [G loss: 0.936998]\n",
      "epoch:6 step:6273 [D loss: 0.472359, acc.: 85.16%] [G loss: 1.149341]\n",
      "epoch:6 step:6274 [D loss: 0.654520, acc.: 60.16%] [G loss: 0.987879]\n",
      "epoch:6 step:6275 [D loss: 0.714948, acc.: 56.25%] [G loss: 1.089199]\n",
      "epoch:6 step:6276 [D loss: 0.564642, acc.: 73.44%] [G loss: 1.033825]\n",
      "epoch:6 step:6277 [D loss: 0.595236, acc.: 68.75%] [G loss: 1.065997]\n",
      "epoch:6 step:6278 [D loss: 0.596407, acc.: 70.31%] [G loss: 1.048295]\n",
      "epoch:6 step:6279 [D loss: 0.767477, acc.: 47.66%] [G loss: 0.933863]\n",
      "epoch:6 step:6280 [D loss: 0.636724, acc.: 65.62%] [G loss: 1.076413]\n",
      "epoch:6 step:6281 [D loss: 0.654839, acc.: 64.06%] [G loss: 1.099329]\n",
      "epoch:6 step:6282 [D loss: 0.687541, acc.: 60.16%] [G loss: 1.075039]\n",
      "epoch:6 step:6283 [D loss: 0.655434, acc.: 58.59%] [G loss: 1.170953]\n",
      "epoch:6 step:6284 [D loss: 0.634279, acc.: 68.75%] [G loss: 1.084948]\n",
      "epoch:6 step:6285 [D loss: 0.663101, acc.: 61.72%] [G loss: 1.144269]\n",
      "epoch:6 step:6286 [D loss: 0.605590, acc.: 63.28%] [G loss: 1.105837]\n",
      "epoch:6 step:6287 [D loss: 0.670122, acc.: 57.81%] [G loss: 1.186615]\n",
      "epoch:6 step:6288 [D loss: 0.538913, acc.: 72.66%] [G loss: 1.097872]\n",
      "epoch:6 step:6289 [D loss: 0.634792, acc.: 64.84%] [G loss: 1.145336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6290 [D loss: 0.640430, acc.: 60.16%] [G loss: 1.173405]\n",
      "epoch:6 step:6291 [D loss: 0.665408, acc.: 61.72%] [G loss: 1.107148]\n",
      "epoch:6 step:6292 [D loss: 0.544418, acc.: 71.09%] [G loss: 1.110654]\n",
      "epoch:6 step:6293 [D loss: 0.569591, acc.: 69.53%] [G loss: 1.297499]\n",
      "epoch:6 step:6294 [D loss: 0.580070, acc.: 69.53%] [G loss: 1.066245]\n",
      "epoch:6 step:6295 [D loss: 0.710416, acc.: 58.59%] [G loss: 1.064679]\n",
      "epoch:6 step:6296 [D loss: 0.589733, acc.: 70.31%] [G loss: 1.230285]\n",
      "epoch:6 step:6297 [D loss: 0.626080, acc.: 65.62%] [G loss: 1.220883]\n",
      "epoch:6 step:6298 [D loss: 0.623746, acc.: 64.84%] [G loss: 1.121928]\n",
      "epoch:6 step:6299 [D loss: 0.671180, acc.: 64.84%] [G loss: 1.216276]\n",
      "epoch:6 step:6300 [D loss: 0.640357, acc.: 67.97%] [G loss: 1.251034]\n",
      "epoch:6 step:6301 [D loss: 0.734312, acc.: 53.12%] [G loss: 0.949891]\n",
      "epoch:6 step:6302 [D loss: 0.623353, acc.: 64.84%] [G loss: 1.123404]\n",
      "epoch:6 step:6303 [D loss: 0.637912, acc.: 64.06%] [G loss: 1.108551]\n",
      "epoch:6 step:6304 [D loss: 0.635367, acc.: 65.62%] [G loss: 1.043068]\n",
      "epoch:6 step:6305 [D loss: 0.615578, acc.: 67.97%] [G loss: 1.215900]\n",
      "epoch:6 step:6306 [D loss: 0.516218, acc.: 76.56%] [G loss: 1.198340]\n",
      "epoch:6 step:6307 [D loss: 0.668935, acc.: 61.72%] [G loss: 1.087268]\n",
      "epoch:6 step:6308 [D loss: 0.553021, acc.: 74.22%] [G loss: 1.382255]\n",
      "epoch:6 step:6309 [D loss: 0.624260, acc.: 66.41%] [G loss: 1.070626]\n",
      "epoch:6 step:6310 [D loss: 0.622870, acc.: 70.31%] [G loss: 1.048080]\n",
      "epoch:6 step:6311 [D loss: 0.754482, acc.: 51.56%] [G loss: 0.998828]\n",
      "epoch:6 step:6312 [D loss: 0.664454, acc.: 60.94%] [G loss: 1.121038]\n",
      "epoch:6 step:6313 [D loss: 0.690365, acc.: 60.94%] [G loss: 0.955186]\n",
      "epoch:6 step:6314 [D loss: 0.648050, acc.: 60.94%] [G loss: 1.021565]\n",
      "epoch:6 step:6315 [D loss: 0.766840, acc.: 50.00%] [G loss: 0.971828]\n",
      "epoch:6 step:6316 [D loss: 0.617274, acc.: 67.97%] [G loss: 1.211737]\n",
      "epoch:6 step:6317 [D loss: 0.597724, acc.: 62.50%] [G loss: 1.047696]\n",
      "epoch:6 step:6318 [D loss: 0.642630, acc.: 61.72%] [G loss: 0.988613]\n",
      "epoch:6 step:6319 [D loss: 0.654058, acc.: 61.72%] [G loss: 1.213813]\n",
      "epoch:6 step:6320 [D loss: 0.710457, acc.: 53.91%] [G loss: 1.207869]\n",
      "epoch:6 step:6321 [D loss: 0.586459, acc.: 66.41%] [G loss: 1.114637]\n",
      "epoch:6 step:6322 [D loss: 0.564720, acc.: 74.22%] [G loss: 1.129135]\n",
      "epoch:6 step:6323 [D loss: 0.622121, acc.: 66.41%] [G loss: 1.122065]\n",
      "epoch:6 step:6324 [D loss: 0.625481, acc.: 65.62%] [G loss: 1.003222]\n",
      "epoch:6 step:6325 [D loss: 0.632152, acc.: 61.72%] [G loss: 1.179060]\n",
      "epoch:6 step:6326 [D loss: 0.710007, acc.: 53.91%] [G loss: 1.078564]\n",
      "epoch:6 step:6327 [D loss: 0.631585, acc.: 63.28%] [G loss: 1.170499]\n",
      "epoch:6 step:6328 [D loss: 0.720051, acc.: 57.03%] [G loss: 1.092042]\n",
      "epoch:6 step:6329 [D loss: 0.555835, acc.: 71.09%] [G loss: 0.975180]\n",
      "epoch:6 step:6330 [D loss: 0.553282, acc.: 71.88%] [G loss: 0.974014]\n",
      "epoch:6 step:6331 [D loss: 0.624258, acc.: 67.19%] [G loss: 0.952913]\n",
      "epoch:6 step:6332 [D loss: 0.574742, acc.: 67.97%] [G loss: 1.056166]\n",
      "epoch:6 step:6333 [D loss: 0.562970, acc.: 75.00%] [G loss: 1.015504]\n",
      "epoch:6 step:6334 [D loss: 0.679724, acc.: 60.16%] [G loss: 1.024228]\n",
      "epoch:6 step:6335 [D loss: 0.575095, acc.: 71.09%] [G loss: 1.065484]\n",
      "epoch:6 step:6336 [D loss: 0.603401, acc.: 67.19%] [G loss: 1.095386]\n",
      "epoch:6 step:6337 [D loss: 0.589845, acc.: 63.28%] [G loss: 1.214102]\n",
      "epoch:6 step:6338 [D loss: 0.639967, acc.: 62.50%] [G loss: 0.997099]\n",
      "epoch:6 step:6339 [D loss: 0.637738, acc.: 64.06%] [G loss: 1.240121]\n",
      "epoch:6 step:6340 [D loss: 0.531698, acc.: 71.09%] [G loss: 1.226793]\n",
      "epoch:6 step:6341 [D loss: 0.619824, acc.: 63.28%] [G loss: 1.219126]\n",
      "epoch:6 step:6342 [D loss: 0.700849, acc.: 56.25%] [G loss: 1.097043]\n",
      "epoch:6 step:6343 [D loss: 0.719502, acc.: 53.91%] [G loss: 1.152248]\n",
      "epoch:6 step:6344 [D loss: 0.681991, acc.: 60.16%] [G loss: 1.044204]\n",
      "epoch:6 step:6345 [D loss: 0.608737, acc.: 64.06%] [G loss: 0.980714]\n",
      "epoch:6 step:6346 [D loss: 0.570991, acc.: 67.19%] [G loss: 1.022564]\n",
      "epoch:6 step:6347 [D loss: 0.728949, acc.: 50.00%] [G loss: 0.937492]\n",
      "epoch:6 step:6348 [D loss: 0.641737, acc.: 59.38%] [G loss: 1.050363]\n",
      "epoch:6 step:6349 [D loss: 0.626467, acc.: 70.31%] [G loss: 1.001255]\n",
      "epoch:6 step:6350 [D loss: 0.590432, acc.: 70.31%] [G loss: 1.232613]\n",
      "epoch:6 step:6351 [D loss: 0.673887, acc.: 55.47%] [G loss: 1.044296]\n",
      "epoch:6 step:6352 [D loss: 0.707796, acc.: 56.25%] [G loss: 1.071650]\n",
      "epoch:6 step:6353 [D loss: 0.615500, acc.: 63.28%] [G loss: 0.903005]\n",
      "epoch:6 step:6354 [D loss: 0.526079, acc.: 76.56%] [G loss: 1.095970]\n",
      "epoch:6 step:6355 [D loss: 0.715623, acc.: 53.91%] [G loss: 1.051952]\n",
      "epoch:6 step:6356 [D loss: 0.564024, acc.: 69.53%] [G loss: 1.118046]\n",
      "epoch:6 step:6357 [D loss: 0.582709, acc.: 72.66%] [G loss: 1.103797]\n",
      "epoch:6 step:6358 [D loss: 0.570098, acc.: 72.66%] [G loss: 1.109281]\n",
      "epoch:6 step:6359 [D loss: 0.662636, acc.: 59.38%] [G loss: 1.073671]\n",
      "epoch:6 step:6360 [D loss: 0.598809, acc.: 73.44%] [G loss: 0.907822]\n",
      "epoch:6 step:6361 [D loss: 0.620553, acc.: 66.41%] [G loss: 0.979608]\n",
      "epoch:6 step:6362 [D loss: 0.601194, acc.: 66.41%] [G loss: 1.103656]\n",
      "epoch:6 step:6363 [D loss: 0.651002, acc.: 60.94%] [G loss: 1.014323]\n",
      "epoch:6 step:6364 [D loss: 0.589064, acc.: 71.88%] [G loss: 1.069774]\n",
      "epoch:6 step:6365 [D loss: 0.658656, acc.: 61.72%] [G loss: 1.147028]\n",
      "epoch:6 step:6366 [D loss: 0.553631, acc.: 71.88%] [G loss: 1.055251]\n",
      "epoch:6 step:6367 [D loss: 0.565205, acc.: 71.88%] [G loss: 1.162456]\n",
      "epoch:6 step:6368 [D loss: 0.547674, acc.: 72.66%] [G loss: 1.237879]\n",
      "epoch:6 step:6369 [D loss: 0.614811, acc.: 68.75%] [G loss: 0.988086]\n",
      "epoch:6 step:6370 [D loss: 0.573856, acc.: 65.62%] [G loss: 1.150240]\n",
      "epoch:6 step:6371 [D loss: 0.691186, acc.: 59.38%] [G loss: 1.065061]\n",
      "epoch:6 step:6372 [D loss: 0.681010, acc.: 59.38%] [G loss: 1.046218]\n",
      "epoch:6 step:6373 [D loss: 0.596965, acc.: 68.75%] [G loss: 1.135298]\n",
      "epoch:6 step:6374 [D loss: 0.645878, acc.: 64.06%] [G loss: 1.071366]\n",
      "epoch:6 step:6375 [D loss: 0.629533, acc.: 61.72%] [G loss: 1.042816]\n",
      "epoch:6 step:6376 [D loss: 0.649233, acc.: 61.72%] [G loss: 1.019862]\n",
      "epoch:6 step:6377 [D loss: 0.534262, acc.: 74.22%] [G loss: 1.095534]\n",
      "epoch:6 step:6378 [D loss: 0.649356, acc.: 67.19%] [G loss: 0.954665]\n",
      "epoch:6 step:6379 [D loss: 0.553585, acc.: 69.53%] [G loss: 1.119280]\n",
      "epoch:6 step:6380 [D loss: 0.649900, acc.: 60.94%] [G loss: 1.139219]\n",
      "epoch:6 step:6381 [D loss: 0.550457, acc.: 77.34%] [G loss: 1.166960]\n",
      "epoch:6 step:6382 [D loss: 0.618639, acc.: 66.41%] [G loss: 1.072396]\n",
      "epoch:6 step:6383 [D loss: 0.670391, acc.: 63.28%] [G loss: 1.168484]\n",
      "epoch:6 step:6384 [D loss: 0.710218, acc.: 54.69%] [G loss: 0.909314]\n",
      "epoch:6 step:6385 [D loss: 0.566387, acc.: 72.66%] [G loss: 1.015027]\n",
      "epoch:6 step:6386 [D loss: 0.650879, acc.: 65.62%] [G loss: 1.024354]\n",
      "epoch:6 step:6387 [D loss: 0.579465, acc.: 73.44%] [G loss: 1.073737]\n",
      "epoch:6 step:6388 [D loss: 0.538329, acc.: 76.56%] [G loss: 1.151917]\n",
      "epoch:6 step:6389 [D loss: 0.647222, acc.: 60.16%] [G loss: 1.131590]\n",
      "epoch:6 step:6390 [D loss: 0.617275, acc.: 66.41%] [G loss: 1.094857]\n",
      "epoch:6 step:6391 [D loss: 0.593234, acc.: 72.66%] [G loss: 1.013501]\n",
      "epoch:6 step:6392 [D loss: 0.569770, acc.: 75.00%] [G loss: 1.153506]\n",
      "epoch:6 step:6393 [D loss: 0.761416, acc.: 51.56%] [G loss: 0.963089]\n",
      "epoch:6 step:6394 [D loss: 0.616212, acc.: 64.06%] [G loss: 1.076932]\n",
      "epoch:6 step:6395 [D loss: 0.615160, acc.: 62.50%] [G loss: 1.111837]\n",
      "epoch:6 step:6396 [D loss: 0.617585, acc.: 64.06%] [G loss: 0.901617]\n",
      "epoch:6 step:6397 [D loss: 0.673493, acc.: 57.81%] [G loss: 1.134685]\n",
      "epoch:6 step:6398 [D loss: 0.647411, acc.: 61.72%] [G loss: 0.993312]\n",
      "epoch:6 step:6399 [D loss: 0.676074, acc.: 58.59%] [G loss: 0.982438]\n",
      "epoch:6 step:6400 [D loss: 0.655066, acc.: 61.72%] [G loss: 1.180519]\n",
      "##############\n",
      "[2.69008666 2.2042963  1.93788972 2.96258011 0.99294218 6.52373856\n",
      " 2.08550105 3.1466103  3.87820124 5.93374111]\n",
      "##########\n",
      "epoch:6 step:6401 [D loss: 0.702845, acc.: 59.38%] [G loss: 1.069388]\n",
      "epoch:6 step:6402 [D loss: 0.640740, acc.: 60.16%] [G loss: 1.145066]\n",
      "epoch:6 step:6403 [D loss: 0.677694, acc.: 62.50%] [G loss: 1.013239]\n",
      "epoch:6 step:6404 [D loss: 0.637697, acc.: 64.84%] [G loss: 1.039556]\n",
      "epoch:6 step:6405 [D loss: 0.573074, acc.: 71.88%] [G loss: 1.029651]\n",
      "epoch:6 step:6406 [D loss: 0.660583, acc.: 61.72%] [G loss: 0.973038]\n",
      "epoch:6 step:6407 [D loss: 0.602411, acc.: 66.41%] [G loss: 1.133798]\n",
      "epoch:6 step:6408 [D loss: 0.703548, acc.: 53.12%] [G loss: 1.156018]\n",
      "epoch:6 step:6409 [D loss: 0.554861, acc.: 69.53%] [G loss: 1.145034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6410 [D loss: 0.719464, acc.: 57.03%] [G loss: 0.971834]\n",
      "epoch:6 step:6411 [D loss: 0.639995, acc.: 62.50%] [G loss: 1.190873]\n",
      "epoch:6 step:6412 [D loss: 0.535539, acc.: 74.22%] [G loss: 1.116239]\n",
      "epoch:6 step:6413 [D loss: 0.666542, acc.: 58.59%] [G loss: 1.202149]\n",
      "epoch:6 step:6414 [D loss: 0.686678, acc.: 57.03%] [G loss: 1.007275]\n",
      "epoch:6 step:6415 [D loss: 0.661586, acc.: 60.16%] [G loss: 1.005943]\n",
      "epoch:6 step:6416 [D loss: 0.615269, acc.: 65.62%] [G loss: 1.176263]\n",
      "epoch:6 step:6417 [D loss: 0.691860, acc.: 57.03%] [G loss: 1.111558]\n",
      "epoch:6 step:6418 [D loss: 0.581320, acc.: 71.88%] [G loss: 1.053967]\n",
      "epoch:6 step:6419 [D loss: 0.654007, acc.: 63.28%] [G loss: 1.029759]\n",
      "epoch:6 step:6420 [D loss: 0.656609, acc.: 64.84%] [G loss: 0.983803]\n",
      "epoch:6 step:6421 [D loss: 0.540355, acc.: 76.56%] [G loss: 1.240584]\n",
      "epoch:6 step:6422 [D loss: 0.678135, acc.: 57.81%] [G loss: 0.961069]\n",
      "epoch:6 step:6423 [D loss: 0.579934, acc.: 67.19%] [G loss: 1.046856]\n",
      "epoch:6 step:6424 [D loss: 0.642661, acc.: 61.72%] [G loss: 1.284980]\n",
      "epoch:6 step:6425 [D loss: 0.630659, acc.: 65.62%] [G loss: 1.148249]\n",
      "epoch:6 step:6426 [D loss: 0.691527, acc.: 56.25%] [G loss: 1.188359]\n",
      "epoch:6 step:6427 [D loss: 0.665677, acc.: 57.03%] [G loss: 1.072204]\n",
      "epoch:6 step:6428 [D loss: 0.618435, acc.: 64.06%] [G loss: 0.974472]\n",
      "epoch:6 step:6429 [D loss: 0.591219, acc.: 67.97%] [G loss: 1.194698]\n",
      "epoch:6 step:6430 [D loss: 0.623072, acc.: 66.41%] [G loss: 0.847753]\n",
      "epoch:6 step:6431 [D loss: 0.614134, acc.: 60.16%] [G loss: 1.239339]\n",
      "epoch:6 step:6432 [D loss: 0.580229, acc.: 70.31%] [G loss: 1.183880]\n",
      "epoch:6 step:6433 [D loss: 0.560236, acc.: 63.28%] [G loss: 1.141040]\n",
      "epoch:6 step:6434 [D loss: 0.652645, acc.: 59.38%] [G loss: 1.083337]\n",
      "epoch:6 step:6435 [D loss: 0.590754, acc.: 66.41%] [G loss: 1.215405]\n",
      "epoch:6 step:6436 [D loss: 0.657376, acc.: 67.19%] [G loss: 0.933522]\n",
      "epoch:6 step:6437 [D loss: 0.591219, acc.: 70.31%] [G loss: 1.042126]\n",
      "epoch:6 step:6438 [D loss: 0.582311, acc.: 64.84%] [G loss: 1.144517]\n",
      "epoch:6 step:6439 [D loss: 0.595458, acc.: 67.97%] [G loss: 1.261853]\n",
      "epoch:6 step:6440 [D loss: 0.571712, acc.: 66.41%] [G loss: 1.350555]\n",
      "epoch:6 step:6441 [D loss: 0.616291, acc.: 70.31%] [G loss: 1.004219]\n",
      "epoch:6 step:6442 [D loss: 0.576499, acc.: 71.88%] [G loss: 1.175606]\n",
      "epoch:6 step:6443 [D loss: 0.688982, acc.: 54.69%] [G loss: 1.122440]\n",
      "epoch:6 step:6444 [D loss: 0.666275, acc.: 59.38%] [G loss: 1.071461]\n",
      "epoch:6 step:6445 [D loss: 0.755925, acc.: 44.53%] [G loss: 1.062498]\n",
      "epoch:6 step:6446 [D loss: 0.624061, acc.: 65.62%] [G loss: 1.036164]\n",
      "epoch:6 step:6447 [D loss: 0.505819, acc.: 79.69%] [G loss: 1.113018]\n",
      "epoch:6 step:6448 [D loss: 0.550621, acc.: 74.22%] [G loss: 1.194226]\n",
      "epoch:6 step:6449 [D loss: 0.585700, acc.: 67.19%] [G loss: 0.941648]\n",
      "epoch:6 step:6450 [D loss: 0.594312, acc.: 61.72%] [G loss: 0.978012]\n",
      "epoch:6 step:6451 [D loss: 0.636564, acc.: 64.84%] [G loss: 1.128909]\n",
      "epoch:6 step:6452 [D loss: 0.647088, acc.: 58.59%] [G loss: 0.916982]\n",
      "epoch:6 step:6453 [D loss: 0.667671, acc.: 61.72%] [G loss: 1.008913]\n",
      "epoch:6 step:6454 [D loss: 0.631079, acc.: 69.53%] [G loss: 0.912844]\n",
      "epoch:6 step:6455 [D loss: 0.696037, acc.: 55.47%] [G loss: 1.069682]\n",
      "epoch:6 step:6456 [D loss: 0.674708, acc.: 56.25%] [G loss: 1.103989]\n",
      "epoch:6 step:6457 [D loss: 0.579218, acc.: 71.09%] [G loss: 1.116129]\n",
      "epoch:6 step:6458 [D loss: 0.667575, acc.: 63.28%] [G loss: 1.148456]\n",
      "epoch:6 step:6459 [D loss: 0.694624, acc.: 61.72%] [G loss: 1.008226]\n",
      "epoch:6 step:6460 [D loss: 0.636950, acc.: 63.28%] [G loss: 0.957588]\n",
      "epoch:6 step:6461 [D loss: 0.718029, acc.: 54.69%] [G loss: 1.000546]\n",
      "epoch:6 step:6462 [D loss: 0.587007, acc.: 67.19%] [G loss: 1.142091]\n",
      "epoch:6 step:6463 [D loss: 0.599246, acc.: 59.38%] [G loss: 1.054851]\n",
      "epoch:6 step:6464 [D loss: 0.614107, acc.: 65.62%] [G loss: 1.169314]\n",
      "epoch:6 step:6465 [D loss: 0.639722, acc.: 64.84%] [G loss: 1.148713]\n",
      "epoch:6 step:6466 [D loss: 0.647211, acc.: 65.62%] [G loss: 1.179437]\n",
      "epoch:6 step:6467 [D loss: 0.656988, acc.: 57.81%] [G loss: 1.139091]\n",
      "epoch:6 step:6468 [D loss: 0.595413, acc.: 66.41%] [G loss: 1.057459]\n",
      "epoch:6 step:6469 [D loss: 0.645415, acc.: 60.16%] [G loss: 1.202216]\n",
      "epoch:6 step:6470 [D loss: 0.605105, acc.: 71.88%] [G loss: 1.019673]\n",
      "epoch:6 step:6471 [D loss: 0.519685, acc.: 75.78%] [G loss: 1.070859]\n",
      "epoch:6 step:6472 [D loss: 0.565728, acc.: 71.09%] [G loss: 1.368599]\n",
      "epoch:6 step:6473 [D loss: 0.593105, acc.: 69.53%] [G loss: 1.184145]\n",
      "epoch:6 step:6474 [D loss: 0.567021, acc.: 71.09%] [G loss: 1.115473]\n",
      "epoch:6 step:6475 [D loss: 0.562133, acc.: 72.66%] [G loss: 1.147413]\n",
      "epoch:6 step:6476 [D loss: 0.643897, acc.: 59.38%] [G loss: 1.056890]\n",
      "epoch:6 step:6477 [D loss: 0.677246, acc.: 54.69%] [G loss: 1.186661]\n",
      "epoch:6 step:6478 [D loss: 0.537535, acc.: 75.78%] [G loss: 1.060361]\n",
      "epoch:6 step:6479 [D loss: 0.690992, acc.: 55.47%] [G loss: 1.064875]\n",
      "epoch:6 step:6480 [D loss: 0.598650, acc.: 68.75%] [G loss: 1.223310]\n",
      "epoch:6 step:6481 [D loss: 0.678925, acc.: 59.38%] [G loss: 1.063891]\n",
      "epoch:6 step:6482 [D loss: 0.645010, acc.: 58.59%] [G loss: 0.994154]\n",
      "epoch:6 step:6483 [D loss: 0.642732, acc.: 62.50%] [G loss: 1.018781]\n",
      "epoch:6 step:6484 [D loss: 0.720823, acc.: 53.91%] [G loss: 1.080166]\n",
      "epoch:6 step:6485 [D loss: 0.599775, acc.: 71.09%] [G loss: 1.110608]\n",
      "epoch:6 step:6486 [D loss: 0.584870, acc.: 66.41%] [G loss: 1.200198]\n",
      "epoch:6 step:6487 [D loss: 0.621875, acc.: 67.19%] [G loss: 1.046120]\n",
      "epoch:6 step:6488 [D loss: 0.562756, acc.: 69.53%] [G loss: 1.266740]\n",
      "epoch:6 step:6489 [D loss: 0.698488, acc.: 58.59%] [G loss: 1.108225]\n",
      "epoch:6 step:6490 [D loss: 0.466531, acc.: 85.94%] [G loss: 1.234016]\n",
      "epoch:6 step:6491 [D loss: 0.604433, acc.: 64.06%] [G loss: 1.237717]\n",
      "epoch:6 step:6492 [D loss: 0.558342, acc.: 75.78%] [G loss: 1.075516]\n",
      "epoch:6 step:6493 [D loss: 0.625710, acc.: 63.28%] [G loss: 1.193582]\n",
      "epoch:6 step:6494 [D loss: 0.746751, acc.: 48.44%] [G loss: 1.038578]\n",
      "epoch:6 step:6495 [D loss: 0.603325, acc.: 65.62%] [G loss: 1.073246]\n",
      "epoch:6 step:6496 [D loss: 0.617645, acc.: 63.28%] [G loss: 1.108026]\n",
      "epoch:6 step:6497 [D loss: 0.598122, acc.: 71.88%] [G loss: 1.201456]\n",
      "epoch:6 step:6498 [D loss: 0.642031, acc.: 63.28%] [G loss: 1.141751]\n",
      "epoch:6 step:6499 [D loss: 0.608711, acc.: 64.06%] [G loss: 1.192006]\n",
      "epoch:6 step:6500 [D loss: 0.590834, acc.: 67.97%] [G loss: 1.127407]\n",
      "epoch:6 step:6501 [D loss: 0.692263, acc.: 56.25%] [G loss: 1.218130]\n",
      "epoch:6 step:6502 [D loss: 0.540816, acc.: 73.44%] [G loss: 1.024225]\n",
      "epoch:6 step:6503 [D loss: 0.595326, acc.: 68.75%] [G loss: 1.105742]\n",
      "epoch:6 step:6504 [D loss: 0.565460, acc.: 70.31%] [G loss: 1.256043]\n",
      "epoch:6 step:6505 [D loss: 0.631195, acc.: 60.16%] [G loss: 1.054107]\n",
      "epoch:6 step:6506 [D loss: 0.664677, acc.: 64.06%] [G loss: 1.035435]\n",
      "epoch:6 step:6507 [D loss: 0.594393, acc.: 71.09%] [G loss: 0.956102]\n",
      "epoch:6 step:6508 [D loss: 0.614338, acc.: 63.28%] [G loss: 1.046750]\n",
      "epoch:6 step:6509 [D loss: 0.699670, acc.: 54.69%] [G loss: 0.911068]\n",
      "epoch:6 step:6510 [D loss: 0.653833, acc.: 62.50%] [G loss: 0.994168]\n",
      "epoch:6 step:6511 [D loss: 0.652155, acc.: 60.16%] [G loss: 1.061953]\n",
      "epoch:6 step:6512 [D loss: 0.640711, acc.: 63.28%] [G loss: 1.045171]\n",
      "epoch:6 step:6513 [D loss: 0.684073, acc.: 58.59%] [G loss: 1.069191]\n",
      "epoch:6 step:6514 [D loss: 0.695225, acc.: 57.81%] [G loss: 1.105907]\n",
      "epoch:6 step:6515 [D loss: 0.542422, acc.: 74.22%] [G loss: 1.189623]\n",
      "epoch:6 step:6516 [D loss: 0.600928, acc.: 65.62%] [G loss: 1.086187]\n",
      "epoch:6 step:6517 [D loss: 0.608369, acc.: 69.53%] [G loss: 1.282192]\n",
      "epoch:6 step:6518 [D loss: 0.670720, acc.: 60.94%] [G loss: 1.229221]\n",
      "epoch:6 step:6519 [D loss: 0.645299, acc.: 64.06%] [G loss: 1.098009]\n",
      "epoch:6 step:6520 [D loss: 0.557787, acc.: 70.31%] [G loss: 1.187991]\n",
      "epoch:6 step:6521 [D loss: 0.605026, acc.: 68.75%] [G loss: 1.212343]\n",
      "epoch:6 step:6522 [D loss: 0.618492, acc.: 66.41%] [G loss: 1.181821]\n",
      "epoch:6 step:6523 [D loss: 0.692126, acc.: 53.91%] [G loss: 1.101936]\n",
      "epoch:6 step:6524 [D loss: 0.691908, acc.: 55.47%] [G loss: 1.001504]\n",
      "epoch:6 step:6525 [D loss: 0.563083, acc.: 75.00%] [G loss: 1.316025]\n",
      "epoch:6 step:6526 [D loss: 0.657551, acc.: 62.50%] [G loss: 1.307916]\n",
      "epoch:6 step:6527 [D loss: 0.618653, acc.: 64.06%] [G loss: 1.115761]\n",
      "epoch:6 step:6528 [D loss: 0.653405, acc.: 63.28%] [G loss: 1.148810]\n",
      "epoch:6 step:6529 [D loss: 0.669349, acc.: 59.38%] [G loss: 1.040077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6530 [D loss: 0.641115, acc.: 64.06%] [G loss: 1.161819]\n",
      "epoch:6 step:6531 [D loss: 0.701048, acc.: 56.25%] [G loss: 0.893866]\n",
      "epoch:6 step:6532 [D loss: 0.647070, acc.: 62.50%] [G loss: 1.075791]\n",
      "epoch:6 step:6533 [D loss: 0.563190, acc.: 67.97%] [G loss: 0.978971]\n",
      "epoch:6 step:6534 [D loss: 0.608408, acc.: 67.19%] [G loss: 0.847410]\n",
      "epoch:6 step:6535 [D loss: 0.761431, acc.: 52.34%] [G loss: 0.916310]\n",
      "epoch:6 step:6536 [D loss: 0.594794, acc.: 69.53%] [G loss: 1.101966]\n",
      "epoch:6 step:6537 [D loss: 0.604058, acc.: 69.53%] [G loss: 1.030788]\n",
      "epoch:6 step:6538 [D loss: 0.580213, acc.: 71.09%] [G loss: 1.079488]\n",
      "epoch:6 step:6539 [D loss: 0.557456, acc.: 75.00%] [G loss: 1.024722]\n",
      "epoch:6 step:6540 [D loss: 0.561655, acc.: 74.22%] [G loss: 1.061165]\n",
      "epoch:6 step:6541 [D loss: 0.595468, acc.: 67.19%] [G loss: 1.172043]\n",
      "epoch:6 step:6542 [D loss: 0.599599, acc.: 66.41%] [G loss: 1.272908]\n",
      "epoch:6 step:6543 [D loss: 0.727836, acc.: 56.25%] [G loss: 0.988793]\n",
      "epoch:6 step:6544 [D loss: 0.561340, acc.: 70.31%] [G loss: 0.963028]\n",
      "epoch:6 step:6545 [D loss: 0.676390, acc.: 59.38%] [G loss: 1.030616]\n",
      "epoch:6 step:6546 [D loss: 0.707605, acc.: 62.50%] [G loss: 0.965586]\n",
      "epoch:6 step:6547 [D loss: 0.718968, acc.: 56.25%] [G loss: 1.084342]\n",
      "epoch:6 step:6548 [D loss: 0.560908, acc.: 72.66%] [G loss: 1.106666]\n",
      "epoch:6 step:6549 [D loss: 0.643975, acc.: 59.38%] [G loss: 1.180392]\n",
      "epoch:6 step:6550 [D loss: 0.596285, acc.: 67.97%] [G loss: 0.922192]\n",
      "epoch:6 step:6551 [D loss: 0.690789, acc.: 53.91%] [G loss: 1.005080]\n",
      "epoch:6 step:6552 [D loss: 0.605798, acc.: 70.31%] [G loss: 1.183113]\n",
      "epoch:6 step:6553 [D loss: 0.644275, acc.: 60.94%] [G loss: 1.104684]\n",
      "epoch:6 step:6554 [D loss: 0.532330, acc.: 75.78%] [G loss: 0.981881]\n",
      "epoch:6 step:6555 [D loss: 0.641510, acc.: 61.72%] [G loss: 1.183697]\n",
      "epoch:6 step:6556 [D loss: 0.617675, acc.: 63.28%] [G loss: 1.079386]\n",
      "epoch:6 step:6557 [D loss: 0.586162, acc.: 70.31%] [G loss: 1.099023]\n",
      "epoch:6 step:6558 [D loss: 0.576477, acc.: 70.31%] [G loss: 1.350115]\n",
      "epoch:6 step:6559 [D loss: 0.613598, acc.: 64.06%] [G loss: 1.045941]\n",
      "epoch:7 step:6560 [D loss: 0.638019, acc.: 62.50%] [G loss: 1.077704]\n",
      "epoch:7 step:6561 [D loss: 0.632077, acc.: 65.62%] [G loss: 0.995747]\n",
      "epoch:7 step:6562 [D loss: 0.622029, acc.: 67.97%] [G loss: 1.233658]\n",
      "epoch:7 step:6563 [D loss: 0.521200, acc.: 78.91%] [G loss: 1.073976]\n",
      "epoch:7 step:6564 [D loss: 0.596799, acc.: 71.88%] [G loss: 1.093145]\n",
      "epoch:7 step:6565 [D loss: 0.745196, acc.: 51.56%] [G loss: 0.959456]\n",
      "epoch:7 step:6566 [D loss: 0.773307, acc.: 52.34%] [G loss: 0.990915]\n",
      "epoch:7 step:6567 [D loss: 0.634927, acc.: 64.84%] [G loss: 0.969325]\n",
      "epoch:7 step:6568 [D loss: 0.588001, acc.: 68.75%] [G loss: 1.017873]\n",
      "epoch:7 step:6569 [D loss: 0.589947, acc.: 65.62%] [G loss: 1.104084]\n",
      "epoch:7 step:6570 [D loss: 0.517992, acc.: 75.00%] [G loss: 1.221817]\n",
      "epoch:7 step:6571 [D loss: 0.565788, acc.: 72.66%] [G loss: 1.283371]\n",
      "epoch:7 step:6572 [D loss: 0.578354, acc.: 70.31%] [G loss: 1.055683]\n",
      "epoch:7 step:6573 [D loss: 0.591994, acc.: 71.88%] [G loss: 1.257797]\n",
      "epoch:7 step:6574 [D loss: 0.630686, acc.: 67.19%] [G loss: 0.984081]\n",
      "epoch:7 step:6575 [D loss: 0.594583, acc.: 71.88%] [G loss: 1.205332]\n",
      "epoch:7 step:6576 [D loss: 0.633830, acc.: 62.50%] [G loss: 1.025347]\n",
      "epoch:7 step:6577 [D loss: 0.631493, acc.: 66.41%] [G loss: 1.051189]\n",
      "epoch:7 step:6578 [D loss: 0.613067, acc.: 66.41%] [G loss: 0.949328]\n",
      "epoch:7 step:6579 [D loss: 0.652618, acc.: 55.47%] [G loss: 1.117307]\n",
      "epoch:7 step:6580 [D loss: 0.602762, acc.: 68.75%] [G loss: 1.113812]\n",
      "epoch:7 step:6581 [D loss: 0.557557, acc.: 73.44%] [G loss: 1.123926]\n",
      "epoch:7 step:6582 [D loss: 0.655437, acc.: 60.94%] [G loss: 0.979974]\n",
      "epoch:7 step:6583 [D loss: 0.587706, acc.: 67.97%] [G loss: 1.135862]\n",
      "epoch:7 step:6584 [D loss: 0.618924, acc.: 64.84%] [G loss: 1.177470]\n",
      "epoch:7 step:6585 [D loss: 0.640029, acc.: 62.50%] [G loss: 1.107267]\n",
      "epoch:7 step:6586 [D loss: 0.607165, acc.: 71.09%] [G loss: 1.167280]\n",
      "epoch:7 step:6587 [D loss: 0.554533, acc.: 67.19%] [G loss: 0.917163]\n",
      "epoch:7 step:6588 [D loss: 0.730620, acc.: 53.12%] [G loss: 1.286228]\n",
      "epoch:7 step:6589 [D loss: 0.652185, acc.: 61.72%] [G loss: 1.114577]\n",
      "epoch:7 step:6590 [D loss: 0.646430, acc.: 61.72%] [G loss: 1.213991]\n",
      "epoch:7 step:6591 [D loss: 0.541403, acc.: 71.88%] [G loss: 1.115724]\n",
      "epoch:7 step:6592 [D loss: 0.609059, acc.: 69.53%] [G loss: 1.076283]\n",
      "epoch:7 step:6593 [D loss: 0.717050, acc.: 53.12%] [G loss: 1.149274]\n",
      "epoch:7 step:6594 [D loss: 0.616237, acc.: 66.41%] [G loss: 0.890177]\n",
      "epoch:7 step:6595 [D loss: 0.581971, acc.: 71.88%] [G loss: 1.083177]\n",
      "epoch:7 step:6596 [D loss: 0.548455, acc.: 72.66%] [G loss: 1.132326]\n",
      "epoch:7 step:6597 [D loss: 0.704821, acc.: 52.34%] [G loss: 1.128955]\n",
      "epoch:7 step:6598 [D loss: 0.609667, acc.: 64.84%] [G loss: 1.061539]\n",
      "epoch:7 step:6599 [D loss: 0.613124, acc.: 64.84%] [G loss: 1.183818]\n",
      "epoch:7 step:6600 [D loss: 0.553030, acc.: 75.00%] [G loss: 1.244232]\n",
      "##############\n",
      "[2.6769336  2.35904208 2.02963894 3.01791288 1.24770526 6.33703507\n",
      " 2.218213   3.01974712 3.90578559 7.14868929]\n",
      "##########\n",
      "epoch:7 step:6601 [D loss: 0.549508, acc.: 73.44%] [G loss: 1.175638]\n",
      "epoch:7 step:6602 [D loss: 0.553498, acc.: 78.12%] [G loss: 1.152242]\n",
      "epoch:7 step:6603 [D loss: 0.658645, acc.: 58.59%] [G loss: 1.023156]\n",
      "epoch:7 step:6604 [D loss: 0.602652, acc.: 67.97%] [G loss: 0.974433]\n",
      "epoch:7 step:6605 [D loss: 0.628456, acc.: 64.06%] [G loss: 1.188788]\n",
      "epoch:7 step:6606 [D loss: 0.627110, acc.: 65.62%] [G loss: 1.018178]\n",
      "epoch:7 step:6607 [D loss: 0.659780, acc.: 59.38%] [G loss: 1.037142]\n",
      "epoch:7 step:6608 [D loss: 0.582561, acc.: 72.66%] [G loss: 1.131064]\n",
      "epoch:7 step:6609 [D loss: 0.586358, acc.: 70.31%] [G loss: 1.030753]\n",
      "epoch:7 step:6610 [D loss: 0.632939, acc.: 61.72%] [G loss: 1.191224]\n",
      "epoch:7 step:6611 [D loss: 0.616705, acc.: 64.84%] [G loss: 1.045170]\n",
      "epoch:7 step:6612 [D loss: 0.591266, acc.: 67.97%] [G loss: 1.220702]\n",
      "epoch:7 step:6613 [D loss: 0.584207, acc.: 68.75%] [G loss: 1.186650]\n",
      "epoch:7 step:6614 [D loss: 0.658419, acc.: 60.94%] [G loss: 1.002730]\n",
      "epoch:7 step:6615 [D loss: 0.551115, acc.: 71.88%] [G loss: 1.088768]\n",
      "epoch:7 step:6616 [D loss: 0.677803, acc.: 60.16%] [G loss: 1.098940]\n",
      "epoch:7 step:6617 [D loss: 0.580190, acc.: 71.88%] [G loss: 1.020559]\n",
      "epoch:7 step:6618 [D loss: 0.541879, acc.: 71.88%] [G loss: 1.127679]\n",
      "epoch:7 step:6619 [D loss: 0.549855, acc.: 74.22%] [G loss: 1.086777]\n",
      "epoch:7 step:6620 [D loss: 0.615350, acc.: 67.97%] [G loss: 1.198183]\n",
      "epoch:7 step:6621 [D loss: 0.616184, acc.: 63.28%] [G loss: 1.162945]\n",
      "epoch:7 step:6622 [D loss: 0.552254, acc.: 69.53%] [G loss: 1.087581]\n",
      "epoch:7 step:6623 [D loss: 0.570610, acc.: 67.19%] [G loss: 1.060104]\n",
      "epoch:7 step:6624 [D loss: 0.595186, acc.: 66.41%] [G loss: 1.275499]\n",
      "epoch:7 step:6625 [D loss: 0.706912, acc.: 60.16%] [G loss: 1.004912]\n",
      "epoch:7 step:6626 [D loss: 0.632130, acc.: 64.06%] [G loss: 1.091487]\n",
      "epoch:7 step:6627 [D loss: 0.637854, acc.: 63.28%] [G loss: 1.397207]\n",
      "epoch:7 step:6628 [D loss: 0.604764, acc.: 64.84%] [G loss: 1.121871]\n",
      "epoch:7 step:6629 [D loss: 0.724719, acc.: 56.25%] [G loss: 1.180769]\n",
      "epoch:7 step:6630 [D loss: 0.647198, acc.: 64.84%] [G loss: 1.280781]\n",
      "epoch:7 step:6631 [D loss: 0.631037, acc.: 64.84%] [G loss: 1.019955]\n",
      "epoch:7 step:6632 [D loss: 0.562907, acc.: 67.19%] [G loss: 1.244423]\n",
      "epoch:7 step:6633 [D loss: 0.645434, acc.: 60.94%] [G loss: 1.084149]\n",
      "epoch:7 step:6634 [D loss: 0.548790, acc.: 71.88%] [G loss: 1.253611]\n",
      "epoch:7 step:6635 [D loss: 0.550962, acc.: 75.78%] [G loss: 1.092982]\n",
      "epoch:7 step:6636 [D loss: 0.730573, acc.: 52.34%] [G loss: 0.980221]\n",
      "epoch:7 step:6637 [D loss: 0.594116, acc.: 64.84%] [G loss: 1.092694]\n",
      "epoch:7 step:6638 [D loss: 0.667933, acc.: 61.72%] [G loss: 0.918157]\n",
      "epoch:7 step:6639 [D loss: 0.640626, acc.: 63.28%] [G loss: 1.101678]\n",
      "epoch:7 step:6640 [D loss: 0.667018, acc.: 60.16%] [G loss: 0.914803]\n",
      "epoch:7 step:6641 [D loss: 0.627875, acc.: 63.28%] [G loss: 1.085539]\n",
      "epoch:7 step:6642 [D loss: 0.686357, acc.: 61.72%] [G loss: 1.106543]\n",
      "epoch:7 step:6643 [D loss: 0.692096, acc.: 56.25%] [G loss: 0.988738]\n",
      "epoch:7 step:6644 [D loss: 0.559152, acc.: 75.00%] [G loss: 1.195637]\n",
      "epoch:7 step:6645 [D loss: 0.756830, acc.: 52.34%] [G loss: 1.201581]\n",
      "epoch:7 step:6646 [D loss: 0.631246, acc.: 65.62%] [G loss: 1.162476]\n",
      "epoch:7 step:6647 [D loss: 0.578877, acc.: 71.09%] [G loss: 1.020132]\n",
      "epoch:7 step:6648 [D loss: 0.557611, acc.: 73.44%] [G loss: 1.210231]\n",
      "epoch:7 step:6649 [D loss: 0.524350, acc.: 72.66%] [G loss: 1.150702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6650 [D loss: 0.590516, acc.: 68.75%] [G loss: 1.199552]\n",
      "epoch:7 step:6651 [D loss: 0.647592, acc.: 64.06%] [G loss: 1.025026]\n",
      "epoch:7 step:6652 [D loss: 0.582814, acc.: 67.19%] [G loss: 1.029043]\n",
      "epoch:7 step:6653 [D loss: 0.643632, acc.: 64.06%] [G loss: 1.222689]\n",
      "epoch:7 step:6654 [D loss: 0.587986, acc.: 67.97%] [G loss: 1.055677]\n",
      "epoch:7 step:6655 [D loss: 0.594423, acc.: 69.53%] [G loss: 1.121493]\n",
      "epoch:7 step:6656 [D loss: 0.680229, acc.: 58.59%] [G loss: 1.227426]\n",
      "epoch:7 step:6657 [D loss: 0.591367, acc.: 68.75%] [G loss: 1.032243]\n",
      "epoch:7 step:6658 [D loss: 0.717680, acc.: 57.03%] [G loss: 1.113687]\n",
      "epoch:7 step:6659 [D loss: 0.689118, acc.: 61.72%] [G loss: 1.127188]\n",
      "epoch:7 step:6660 [D loss: 0.603819, acc.: 65.62%] [G loss: 1.162617]\n",
      "epoch:7 step:6661 [D loss: 0.658178, acc.: 60.94%] [G loss: 1.179253]\n",
      "epoch:7 step:6662 [D loss: 0.646168, acc.: 61.72%] [G loss: 1.015526]\n",
      "epoch:7 step:6663 [D loss: 0.612100, acc.: 64.84%] [G loss: 1.131245]\n",
      "epoch:7 step:6664 [D loss: 0.624030, acc.: 64.84%] [G loss: 1.110142]\n",
      "epoch:7 step:6665 [D loss: 0.693007, acc.: 59.38%] [G loss: 0.978737]\n",
      "epoch:7 step:6666 [D loss: 0.471774, acc.: 82.03%] [G loss: 1.288295]\n",
      "epoch:7 step:6667 [D loss: 0.598809, acc.: 67.97%] [G loss: 1.229700]\n",
      "epoch:7 step:6668 [D loss: 0.559942, acc.: 71.09%] [G loss: 1.196290]\n",
      "epoch:7 step:6669 [D loss: 0.684996, acc.: 63.28%] [G loss: 1.137497]\n",
      "epoch:7 step:6670 [D loss: 0.629950, acc.: 63.28%] [G loss: 1.217548]\n",
      "epoch:7 step:6671 [D loss: 0.629818, acc.: 60.16%] [G loss: 1.129748]\n",
      "epoch:7 step:6672 [D loss: 0.580886, acc.: 67.97%] [G loss: 1.274744]\n",
      "epoch:7 step:6673 [D loss: 0.536837, acc.: 76.56%] [G loss: 1.233143]\n",
      "epoch:7 step:6674 [D loss: 0.619642, acc.: 65.62%] [G loss: 1.016953]\n",
      "epoch:7 step:6675 [D loss: 0.574375, acc.: 64.84%] [G loss: 1.182528]\n",
      "epoch:7 step:6676 [D loss: 0.587318, acc.: 69.53%] [G loss: 1.175990]\n",
      "epoch:7 step:6677 [D loss: 0.647251, acc.: 63.28%] [G loss: 1.103260]\n",
      "epoch:7 step:6678 [D loss: 0.610272, acc.: 67.97%] [G loss: 1.083271]\n",
      "epoch:7 step:6679 [D loss: 0.625789, acc.: 64.06%] [G loss: 1.053072]\n",
      "epoch:7 step:6680 [D loss: 0.731627, acc.: 61.72%] [G loss: 1.086766]\n",
      "epoch:7 step:6681 [D loss: 0.708527, acc.: 52.34%] [G loss: 1.055913]\n",
      "epoch:7 step:6682 [D loss: 0.621114, acc.: 62.50%] [G loss: 1.286948]\n",
      "epoch:7 step:6683 [D loss: 0.646975, acc.: 61.72%] [G loss: 1.057589]\n",
      "epoch:7 step:6684 [D loss: 0.625709, acc.: 67.19%] [G loss: 1.065717]\n",
      "epoch:7 step:6685 [D loss: 0.619879, acc.: 61.72%] [G loss: 1.164572]\n",
      "epoch:7 step:6686 [D loss: 0.689563, acc.: 60.94%] [G loss: 1.124723]\n",
      "epoch:7 step:6687 [D loss: 0.620912, acc.: 71.88%] [G loss: 1.260621]\n",
      "epoch:7 step:6688 [D loss: 0.673494, acc.: 58.59%] [G loss: 1.133313]\n",
      "epoch:7 step:6689 [D loss: 0.617699, acc.: 65.62%] [G loss: 1.179495]\n",
      "epoch:7 step:6690 [D loss: 0.697266, acc.: 57.03%] [G loss: 0.996580]\n",
      "epoch:7 step:6691 [D loss: 0.575860, acc.: 64.06%] [G loss: 1.243493]\n",
      "epoch:7 step:6692 [D loss: 0.583171, acc.: 71.09%] [G loss: 1.040590]\n",
      "epoch:7 step:6693 [D loss: 0.576960, acc.: 67.97%] [G loss: 1.406897]\n",
      "epoch:7 step:6694 [D loss: 0.592943, acc.: 64.84%] [G loss: 1.022359]\n",
      "epoch:7 step:6695 [D loss: 0.720382, acc.: 57.03%] [G loss: 1.050214]\n",
      "epoch:7 step:6696 [D loss: 0.609152, acc.: 65.62%] [G loss: 0.975829]\n",
      "epoch:7 step:6697 [D loss: 0.608469, acc.: 66.41%] [G loss: 0.818602]\n",
      "epoch:7 step:6698 [D loss: 0.624476, acc.: 69.53%] [G loss: 1.130414]\n",
      "epoch:7 step:6699 [D loss: 0.703310, acc.: 57.81%] [G loss: 1.114404]\n",
      "epoch:7 step:6700 [D loss: 0.628993, acc.: 62.50%] [G loss: 1.231299]\n",
      "epoch:7 step:6701 [D loss: 0.560482, acc.: 73.44%] [G loss: 0.932736]\n",
      "epoch:7 step:6702 [D loss: 0.663976, acc.: 64.06%] [G loss: 1.030389]\n",
      "epoch:7 step:6703 [D loss: 0.764675, acc.: 47.66%] [G loss: 1.087629]\n",
      "epoch:7 step:6704 [D loss: 0.618299, acc.: 64.06%] [G loss: 1.197518]\n",
      "epoch:7 step:6705 [D loss: 0.596456, acc.: 66.41%] [G loss: 1.057901]\n",
      "epoch:7 step:6706 [D loss: 0.699534, acc.: 60.16%] [G loss: 0.920619]\n",
      "epoch:7 step:6707 [D loss: 0.604210, acc.: 66.41%] [G loss: 1.200618]\n",
      "epoch:7 step:6708 [D loss: 0.707024, acc.: 52.34%] [G loss: 0.991370]\n",
      "epoch:7 step:6709 [D loss: 0.590248, acc.: 67.97%] [G loss: 1.055288]\n",
      "epoch:7 step:6710 [D loss: 0.527770, acc.: 78.12%] [G loss: 1.211580]\n",
      "epoch:7 step:6711 [D loss: 0.681247, acc.: 60.16%] [G loss: 1.047541]\n",
      "epoch:7 step:6712 [D loss: 0.570048, acc.: 69.53%] [G loss: 1.088773]\n",
      "epoch:7 step:6713 [D loss: 0.660367, acc.: 62.50%] [G loss: 1.078181]\n",
      "epoch:7 step:6714 [D loss: 0.589007, acc.: 65.62%] [G loss: 1.044864]\n",
      "epoch:7 step:6715 [D loss: 0.544900, acc.: 75.00%] [G loss: 1.260226]\n",
      "epoch:7 step:6716 [D loss: 0.611912, acc.: 62.50%] [G loss: 1.238169]\n",
      "epoch:7 step:6717 [D loss: 0.600182, acc.: 65.62%] [G loss: 1.105397]\n",
      "epoch:7 step:6718 [D loss: 0.553601, acc.: 75.00%] [G loss: 1.231029]\n",
      "epoch:7 step:6719 [D loss: 0.560330, acc.: 72.66%] [G loss: 1.221254]\n",
      "epoch:7 step:6720 [D loss: 0.582815, acc.: 64.84%] [G loss: 1.121879]\n",
      "epoch:7 step:6721 [D loss: 0.645812, acc.: 59.38%] [G loss: 1.199709]\n",
      "epoch:7 step:6722 [D loss: 0.610152, acc.: 67.19%] [G loss: 1.118668]\n",
      "epoch:7 step:6723 [D loss: 0.651564, acc.: 65.62%] [G loss: 0.906159]\n",
      "epoch:7 step:6724 [D loss: 0.600263, acc.: 68.75%] [G loss: 1.118477]\n",
      "epoch:7 step:6725 [D loss: 0.718042, acc.: 56.25%] [G loss: 0.942654]\n",
      "epoch:7 step:6726 [D loss: 0.610809, acc.: 64.84%] [G loss: 1.068393]\n",
      "epoch:7 step:6727 [D loss: 0.652721, acc.: 58.59%] [G loss: 1.128513]\n",
      "epoch:7 step:6728 [D loss: 0.525112, acc.: 78.12%] [G loss: 1.191606]\n",
      "epoch:7 step:6729 [D loss: 0.657038, acc.: 63.28%] [G loss: 0.927673]\n",
      "epoch:7 step:6730 [D loss: 0.684663, acc.: 60.16%] [G loss: 0.973665]\n",
      "epoch:7 step:6731 [D loss: 0.603225, acc.: 67.97%] [G loss: 1.067819]\n",
      "epoch:7 step:6732 [D loss: 0.558206, acc.: 71.88%] [G loss: 1.401805]\n",
      "epoch:7 step:6733 [D loss: 0.632352, acc.: 64.06%] [G loss: 0.947333]\n",
      "epoch:7 step:6734 [D loss: 0.644573, acc.: 59.38%] [G loss: 1.226333]\n",
      "epoch:7 step:6735 [D loss: 0.632392, acc.: 62.50%] [G loss: 1.201714]\n",
      "epoch:7 step:6736 [D loss: 0.686696, acc.: 60.94%] [G loss: 0.868959]\n",
      "epoch:7 step:6737 [D loss: 0.637533, acc.: 67.19%] [G loss: 1.027611]\n",
      "epoch:7 step:6738 [D loss: 0.579946, acc.: 67.97%] [G loss: 1.039641]\n",
      "epoch:7 step:6739 [D loss: 0.616078, acc.: 65.62%] [G loss: 1.048087]\n",
      "epoch:7 step:6740 [D loss: 0.778987, acc.: 50.78%] [G loss: 1.033215]\n",
      "epoch:7 step:6741 [D loss: 0.699517, acc.: 50.00%] [G loss: 1.153118]\n",
      "epoch:7 step:6742 [D loss: 0.586529, acc.: 64.84%] [G loss: 1.233324]\n",
      "epoch:7 step:6743 [D loss: 0.565535, acc.: 74.22%] [G loss: 1.082713]\n",
      "epoch:7 step:6744 [D loss: 0.694675, acc.: 56.25%] [G loss: 1.138238]\n",
      "epoch:7 step:6745 [D loss: 0.629351, acc.: 67.19%] [G loss: 1.082051]\n",
      "epoch:7 step:6746 [D loss: 0.617485, acc.: 66.41%] [G loss: 1.217259]\n",
      "epoch:7 step:6747 [D loss: 0.681260, acc.: 57.81%] [G loss: 1.233714]\n",
      "epoch:7 step:6748 [D loss: 0.682221, acc.: 60.16%] [G loss: 1.233578]\n",
      "epoch:7 step:6749 [D loss: 0.706045, acc.: 56.25%] [G loss: 1.106995]\n",
      "epoch:7 step:6750 [D loss: 0.531986, acc.: 74.22%] [G loss: 1.291844]\n",
      "epoch:7 step:6751 [D loss: 0.673069, acc.: 59.38%] [G loss: 1.148981]\n",
      "epoch:7 step:6752 [D loss: 0.700095, acc.: 52.34%] [G loss: 1.047397]\n",
      "epoch:7 step:6753 [D loss: 0.562861, acc.: 69.53%] [G loss: 1.237299]\n",
      "epoch:7 step:6754 [D loss: 0.604163, acc.: 70.31%] [G loss: 1.229877]\n",
      "epoch:7 step:6755 [D loss: 0.562503, acc.: 72.66%] [G loss: 0.985736]\n",
      "epoch:7 step:6756 [D loss: 0.554113, acc.: 76.56%] [G loss: 1.211296]\n",
      "epoch:7 step:6757 [D loss: 0.594963, acc.: 70.31%] [G loss: 1.033875]\n",
      "epoch:7 step:6758 [D loss: 0.557332, acc.: 74.22%] [G loss: 1.015712]\n",
      "epoch:7 step:6759 [D loss: 0.585406, acc.: 65.62%] [G loss: 0.942849]\n",
      "epoch:7 step:6760 [D loss: 0.435517, acc.: 87.50%] [G loss: 1.177039]\n",
      "epoch:7 step:6761 [D loss: 0.623476, acc.: 63.28%] [G loss: 1.090931]\n",
      "epoch:7 step:6762 [D loss: 0.575786, acc.: 66.41%] [G loss: 1.116996]\n",
      "epoch:7 step:6763 [D loss: 0.595432, acc.: 66.41%] [G loss: 0.976741]\n",
      "epoch:7 step:6764 [D loss: 0.578965, acc.: 71.88%] [G loss: 1.170573]\n",
      "epoch:7 step:6765 [D loss: 0.711017, acc.: 54.69%] [G loss: 1.137483]\n",
      "epoch:7 step:6766 [D loss: 0.594954, acc.: 68.75%] [G loss: 1.235750]\n",
      "epoch:7 step:6767 [D loss: 0.598677, acc.: 65.62%] [G loss: 1.247017]\n",
      "epoch:7 step:6768 [D loss: 0.715676, acc.: 59.38%] [G loss: 1.022461]\n",
      "epoch:7 step:6769 [D loss: 0.636330, acc.: 60.94%] [G loss: 1.028283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6770 [D loss: 0.639540, acc.: 63.28%] [G loss: 1.208275]\n",
      "epoch:7 step:6771 [D loss: 0.498931, acc.: 75.78%] [G loss: 1.030889]\n",
      "epoch:7 step:6772 [D loss: 0.618027, acc.: 64.84%] [G loss: 0.951522]\n",
      "epoch:7 step:6773 [D loss: 0.637489, acc.: 64.06%] [G loss: 1.053233]\n",
      "epoch:7 step:6774 [D loss: 0.628437, acc.: 67.19%] [G loss: 1.147188]\n",
      "epoch:7 step:6775 [D loss: 0.634027, acc.: 64.06%] [G loss: 1.059353]\n",
      "epoch:7 step:6776 [D loss: 0.488761, acc.: 78.12%] [G loss: 1.134295]\n",
      "epoch:7 step:6777 [D loss: 0.587503, acc.: 69.53%] [G loss: 1.201570]\n",
      "epoch:7 step:6778 [D loss: 0.667529, acc.: 60.16%] [G loss: 0.997119]\n",
      "epoch:7 step:6779 [D loss: 0.640781, acc.: 63.28%] [G loss: 1.122254]\n",
      "epoch:7 step:6780 [D loss: 0.604495, acc.: 70.31%] [G loss: 1.003778]\n",
      "epoch:7 step:6781 [D loss: 0.668036, acc.: 63.28%] [G loss: 1.003677]\n",
      "epoch:7 step:6782 [D loss: 0.598564, acc.: 67.19%] [G loss: 0.936890]\n",
      "epoch:7 step:6783 [D loss: 0.617558, acc.: 66.41%] [G loss: 1.048527]\n",
      "epoch:7 step:6784 [D loss: 0.531702, acc.: 75.78%] [G loss: 1.215332]\n",
      "epoch:7 step:6785 [D loss: 0.557903, acc.: 73.44%] [G loss: 1.026324]\n",
      "epoch:7 step:6786 [D loss: 0.645866, acc.: 60.94%] [G loss: 1.169741]\n",
      "epoch:7 step:6787 [D loss: 0.616511, acc.: 65.62%] [G loss: 1.225348]\n",
      "epoch:7 step:6788 [D loss: 0.637791, acc.: 63.28%] [G loss: 1.165568]\n",
      "epoch:7 step:6789 [D loss: 0.650089, acc.: 61.72%] [G loss: 1.099218]\n",
      "epoch:7 step:6790 [D loss: 0.624653, acc.: 67.97%] [G loss: 1.155628]\n",
      "epoch:7 step:6791 [D loss: 0.655423, acc.: 67.19%] [G loss: 1.096795]\n",
      "epoch:7 step:6792 [D loss: 0.564779, acc.: 71.88%] [G loss: 0.978546]\n",
      "epoch:7 step:6793 [D loss: 0.700443, acc.: 56.25%] [G loss: 1.178514]\n",
      "epoch:7 step:6794 [D loss: 0.629427, acc.: 63.28%] [G loss: 1.111240]\n",
      "epoch:7 step:6795 [D loss: 0.590598, acc.: 67.19%] [G loss: 1.091844]\n",
      "epoch:7 step:6796 [D loss: 0.574406, acc.: 71.88%] [G loss: 1.189366]\n",
      "epoch:7 step:6797 [D loss: 0.642929, acc.: 65.62%] [G loss: 0.984731]\n",
      "epoch:7 step:6798 [D loss: 0.647156, acc.: 64.84%] [G loss: 1.046477]\n",
      "epoch:7 step:6799 [D loss: 0.629379, acc.: 64.84%] [G loss: 1.124423]\n",
      "epoch:7 step:6800 [D loss: 0.645149, acc.: 64.84%] [G loss: 1.152667]\n",
      "##############\n",
      "[2.63469899 2.01074759 1.84653905 2.83251047 0.73183287 6.14347118\n",
      " 1.94348954 3.18699348 3.65335227 4.98575212]\n",
      "##########\n",
      "epoch:7 step:6801 [D loss: 0.558692, acc.: 77.34%] [G loss: 1.029063]\n",
      "epoch:7 step:6802 [D loss: 0.660293, acc.: 60.94%] [G loss: 1.108261]\n",
      "epoch:7 step:6803 [D loss: 0.607823, acc.: 67.97%] [G loss: 1.049973]\n",
      "epoch:7 step:6804 [D loss: 0.613052, acc.: 63.28%] [G loss: 1.155834]\n",
      "epoch:7 step:6805 [D loss: 0.642384, acc.: 61.72%] [G loss: 1.137837]\n",
      "epoch:7 step:6806 [D loss: 0.622212, acc.: 66.41%] [G loss: 1.214609]\n",
      "epoch:7 step:6807 [D loss: 0.555296, acc.: 67.97%] [G loss: 1.163001]\n",
      "epoch:7 step:6808 [D loss: 0.597640, acc.: 69.53%] [G loss: 1.173004]\n",
      "epoch:7 step:6809 [D loss: 0.674523, acc.: 62.50%] [G loss: 1.308283]\n",
      "epoch:7 step:6810 [D loss: 0.656736, acc.: 58.59%] [G loss: 1.079198]\n",
      "epoch:7 step:6811 [D loss: 0.618880, acc.: 67.19%] [G loss: 0.961999]\n",
      "epoch:7 step:6812 [D loss: 0.555330, acc.: 75.00%] [G loss: 1.157768]\n",
      "epoch:7 step:6813 [D loss: 0.703942, acc.: 53.12%] [G loss: 0.895284]\n",
      "epoch:7 step:6814 [D loss: 0.647537, acc.: 64.84%] [G loss: 1.084012]\n",
      "epoch:7 step:6815 [D loss: 0.603090, acc.: 64.06%] [G loss: 1.043629]\n",
      "epoch:7 step:6816 [D loss: 0.633302, acc.: 67.97%] [G loss: 1.068098]\n",
      "epoch:7 step:6817 [D loss: 0.675083, acc.: 62.50%] [G loss: 1.082870]\n",
      "epoch:7 step:6818 [D loss: 0.568709, acc.: 69.53%] [G loss: 1.091293]\n",
      "epoch:7 step:6819 [D loss: 0.583452, acc.: 67.19%] [G loss: 1.245635]\n",
      "epoch:7 step:6820 [D loss: 0.537526, acc.: 74.22%] [G loss: 1.303123]\n",
      "epoch:7 step:6821 [D loss: 0.587950, acc.: 66.41%] [G loss: 1.106584]\n",
      "epoch:7 step:6822 [D loss: 0.647853, acc.: 64.06%] [G loss: 1.131604]\n",
      "epoch:7 step:6823 [D loss: 0.537453, acc.: 70.31%] [G loss: 1.148297]\n",
      "epoch:7 step:6824 [D loss: 0.695754, acc.: 53.12%] [G loss: 0.854394]\n",
      "epoch:7 step:6825 [D loss: 0.657336, acc.: 57.81%] [G loss: 1.044576]\n",
      "epoch:7 step:6826 [D loss: 0.657579, acc.: 60.16%] [G loss: 1.074396]\n",
      "epoch:7 step:6827 [D loss: 0.612356, acc.: 66.41%] [G loss: 1.110667]\n",
      "epoch:7 step:6828 [D loss: 0.590041, acc.: 68.75%] [G loss: 1.046156]\n",
      "epoch:7 step:6829 [D loss: 0.537905, acc.: 72.66%] [G loss: 1.231746]\n",
      "epoch:7 step:6830 [D loss: 0.558996, acc.: 67.97%] [G loss: 1.186420]\n",
      "epoch:7 step:6831 [D loss: 0.540714, acc.: 75.78%] [G loss: 1.372226]\n",
      "epoch:7 step:6832 [D loss: 0.606771, acc.: 70.31%] [G loss: 1.084853]\n",
      "epoch:7 step:6833 [D loss: 0.678040, acc.: 60.94%] [G loss: 1.060108]\n",
      "epoch:7 step:6834 [D loss: 0.715486, acc.: 58.59%] [G loss: 1.025341]\n",
      "epoch:7 step:6835 [D loss: 0.687304, acc.: 60.16%] [G loss: 1.083497]\n",
      "epoch:7 step:6836 [D loss: 0.515270, acc.: 74.22%] [G loss: 1.231209]\n",
      "epoch:7 step:6837 [D loss: 0.657949, acc.: 62.50%] [G loss: 1.025094]\n",
      "epoch:7 step:6838 [D loss: 0.637272, acc.: 65.62%] [G loss: 1.208994]\n",
      "epoch:7 step:6839 [D loss: 0.610478, acc.: 65.62%] [G loss: 1.302899]\n",
      "epoch:7 step:6840 [D loss: 0.591084, acc.: 65.62%] [G loss: 1.174993]\n",
      "epoch:7 step:6841 [D loss: 0.548038, acc.: 74.22%] [G loss: 1.179676]\n",
      "epoch:7 step:6842 [D loss: 0.609926, acc.: 67.97%] [G loss: 1.111178]\n",
      "epoch:7 step:6843 [D loss: 0.587612, acc.: 71.88%] [G loss: 1.238147]\n",
      "epoch:7 step:6844 [D loss: 0.645517, acc.: 61.72%] [G loss: 1.061673]\n",
      "epoch:7 step:6845 [D loss: 0.616555, acc.: 64.06%] [G loss: 1.123543]\n",
      "epoch:7 step:6846 [D loss: 0.698325, acc.: 55.47%] [G loss: 0.984973]\n",
      "epoch:7 step:6847 [D loss: 0.706597, acc.: 51.56%] [G loss: 1.106960]\n",
      "epoch:7 step:6848 [D loss: 0.631796, acc.: 61.72%] [G loss: 1.115696]\n",
      "epoch:7 step:6849 [D loss: 0.549741, acc.: 72.66%] [G loss: 1.152889]\n",
      "epoch:7 step:6850 [D loss: 0.596107, acc.: 66.41%] [G loss: 1.096385]\n",
      "epoch:7 step:6851 [D loss: 0.626447, acc.: 65.62%] [G loss: 1.086817]\n",
      "epoch:7 step:6852 [D loss: 0.639629, acc.: 66.41%] [G loss: 1.170589]\n",
      "epoch:7 step:6853 [D loss: 0.678715, acc.: 60.94%] [G loss: 1.080212]\n",
      "epoch:7 step:6854 [D loss: 0.698523, acc.: 53.12%] [G loss: 1.151702]\n",
      "epoch:7 step:6855 [D loss: 0.574643, acc.: 71.09%] [G loss: 1.061657]\n",
      "epoch:7 step:6856 [D loss: 0.652182, acc.: 60.94%] [G loss: 1.182707]\n",
      "epoch:7 step:6857 [D loss: 0.626810, acc.: 65.62%] [G loss: 1.155896]\n",
      "epoch:7 step:6858 [D loss: 0.702791, acc.: 58.59%] [G loss: 1.196517]\n",
      "epoch:7 step:6859 [D loss: 0.591779, acc.: 65.62%] [G loss: 0.965450]\n",
      "epoch:7 step:6860 [D loss: 0.658783, acc.: 60.16%] [G loss: 1.198457]\n",
      "epoch:7 step:6861 [D loss: 0.541397, acc.: 71.09%] [G loss: 1.179505]\n",
      "epoch:7 step:6862 [D loss: 0.696098, acc.: 57.03%] [G loss: 1.174362]\n",
      "epoch:7 step:6863 [D loss: 0.594132, acc.: 67.97%] [G loss: 1.151047]\n",
      "epoch:7 step:6864 [D loss: 0.609783, acc.: 70.31%] [G loss: 0.900549]\n",
      "epoch:7 step:6865 [D loss: 0.614471, acc.: 66.41%] [G loss: 0.890354]\n",
      "epoch:7 step:6866 [D loss: 0.622403, acc.: 64.06%] [G loss: 1.109429]\n",
      "epoch:7 step:6867 [D loss: 0.641610, acc.: 62.50%] [G loss: 1.135794]\n",
      "epoch:7 step:6868 [D loss: 0.530021, acc.: 76.56%] [G loss: 1.163779]\n",
      "epoch:7 step:6869 [D loss: 0.747068, acc.: 53.12%] [G loss: 1.061927]\n",
      "epoch:7 step:6870 [D loss: 0.613106, acc.: 66.41%] [G loss: 1.078954]\n",
      "epoch:7 step:6871 [D loss: 0.688582, acc.: 61.72%] [G loss: 1.036406]\n",
      "epoch:7 step:6872 [D loss: 0.680295, acc.: 61.72%] [G loss: 1.048427]\n",
      "epoch:7 step:6873 [D loss: 0.545298, acc.: 75.78%] [G loss: 1.109159]\n",
      "epoch:7 step:6874 [D loss: 0.624076, acc.: 65.62%] [G loss: 1.125871]\n",
      "epoch:7 step:6875 [D loss: 0.602716, acc.: 64.06%] [G loss: 1.117458]\n",
      "epoch:7 step:6876 [D loss: 0.552008, acc.: 71.09%] [G loss: 1.034165]\n",
      "epoch:7 step:6877 [D loss: 0.568751, acc.: 73.44%] [G loss: 0.954695]\n",
      "epoch:7 step:6878 [D loss: 0.600355, acc.: 65.62%] [G loss: 0.951665]\n",
      "epoch:7 step:6879 [D loss: 0.613663, acc.: 67.19%] [G loss: 1.246378]\n",
      "epoch:7 step:6880 [D loss: 0.693433, acc.: 60.16%] [G loss: 0.970064]\n",
      "epoch:7 step:6881 [D loss: 0.593215, acc.: 67.97%] [G loss: 1.224230]\n",
      "epoch:7 step:6882 [D loss: 0.624829, acc.: 64.06%] [G loss: 0.992622]\n",
      "epoch:7 step:6883 [D loss: 0.586290, acc.: 69.53%] [G loss: 1.055916]\n",
      "epoch:7 step:6884 [D loss: 0.548063, acc.: 70.31%] [G loss: 1.058661]\n",
      "epoch:7 step:6885 [D loss: 0.604744, acc.: 68.75%] [G loss: 1.069020]\n",
      "epoch:7 step:6886 [D loss: 0.594749, acc.: 67.97%] [G loss: 1.316020]\n",
      "epoch:7 step:6887 [D loss: 0.521042, acc.: 76.56%] [G loss: 1.158738]\n",
      "epoch:7 step:6888 [D loss: 0.635463, acc.: 60.94%] [G loss: 1.213730]\n",
      "epoch:7 step:6889 [D loss: 0.707633, acc.: 56.25%] [G loss: 1.070973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6890 [D loss: 0.650398, acc.: 60.94%] [G loss: 1.007476]\n",
      "epoch:7 step:6891 [D loss: 0.518482, acc.: 77.34%] [G loss: 1.045604]\n",
      "epoch:7 step:6892 [D loss: 0.595670, acc.: 66.41%] [G loss: 1.170480]\n",
      "epoch:7 step:6893 [D loss: 0.593682, acc.: 68.75%] [G loss: 1.128582]\n",
      "epoch:7 step:6894 [D loss: 0.590530, acc.: 67.19%] [G loss: 1.109111]\n",
      "epoch:7 step:6895 [D loss: 0.511099, acc.: 78.91%] [G loss: 1.079269]\n",
      "epoch:7 step:6896 [D loss: 0.595675, acc.: 65.62%] [G loss: 1.162389]\n",
      "epoch:7 step:6897 [D loss: 0.592485, acc.: 63.28%] [G loss: 1.055314]\n",
      "epoch:7 step:6898 [D loss: 0.612148, acc.: 65.62%] [G loss: 1.073728]\n",
      "epoch:7 step:6899 [D loss: 0.596392, acc.: 66.41%] [G loss: 1.148878]\n",
      "epoch:7 step:6900 [D loss: 0.682728, acc.: 57.81%] [G loss: 1.136409]\n",
      "epoch:7 step:6901 [D loss: 0.679268, acc.: 59.38%] [G loss: 1.078341]\n",
      "epoch:7 step:6902 [D loss: 0.625304, acc.: 62.50%] [G loss: 1.121903]\n",
      "epoch:7 step:6903 [D loss: 0.624699, acc.: 65.62%] [G loss: 0.978747]\n",
      "epoch:7 step:6904 [D loss: 0.624248, acc.: 66.41%] [G loss: 1.080168]\n",
      "epoch:7 step:6905 [D loss: 0.734402, acc.: 50.00%] [G loss: 1.102773]\n",
      "epoch:7 step:6906 [D loss: 0.665389, acc.: 64.84%] [G loss: 0.898719]\n",
      "epoch:7 step:6907 [D loss: 0.644808, acc.: 64.84%] [G loss: 1.072236]\n",
      "epoch:7 step:6908 [D loss: 0.589608, acc.: 71.09%] [G loss: 1.177617]\n",
      "epoch:7 step:6909 [D loss: 0.640766, acc.: 68.75%] [G loss: 0.917253]\n",
      "epoch:7 step:6910 [D loss: 0.593857, acc.: 70.31%] [G loss: 1.134236]\n",
      "epoch:7 step:6911 [D loss: 0.711861, acc.: 52.34%] [G loss: 1.005940]\n",
      "epoch:7 step:6912 [D loss: 0.534110, acc.: 75.78%] [G loss: 1.044511]\n",
      "epoch:7 step:6913 [D loss: 0.661234, acc.: 58.59%] [G loss: 1.061110]\n",
      "epoch:7 step:6914 [D loss: 0.611619, acc.: 68.75%] [G loss: 1.072308]\n",
      "epoch:7 step:6915 [D loss: 0.633601, acc.: 64.84%] [G loss: 1.064555]\n",
      "epoch:7 step:6916 [D loss: 0.584940, acc.: 71.09%] [G loss: 1.008538]\n",
      "epoch:7 step:6917 [D loss: 0.680835, acc.: 56.25%] [G loss: 0.914462]\n",
      "epoch:7 step:6918 [D loss: 0.594200, acc.: 67.97%] [G loss: 1.148371]\n",
      "epoch:7 step:6919 [D loss: 0.642305, acc.: 62.50%] [G loss: 1.122355]\n",
      "epoch:7 step:6920 [D loss: 0.692943, acc.: 60.94%] [G loss: 0.953097]\n",
      "epoch:7 step:6921 [D loss: 0.673837, acc.: 63.28%] [G loss: 0.948748]\n",
      "epoch:7 step:6922 [D loss: 0.733065, acc.: 54.69%] [G loss: 1.008078]\n",
      "epoch:7 step:6923 [D loss: 0.619624, acc.: 64.06%] [G loss: 1.061450]\n",
      "epoch:7 step:6924 [D loss: 0.649189, acc.: 58.59%] [G loss: 1.119947]\n",
      "epoch:7 step:6925 [D loss: 0.558433, acc.: 69.53%] [G loss: 1.180274]\n",
      "epoch:7 step:6926 [D loss: 0.601514, acc.: 66.41%] [G loss: 1.258944]\n",
      "epoch:7 step:6927 [D loss: 0.578430, acc.: 69.53%] [G loss: 1.054037]\n",
      "epoch:7 step:6928 [D loss: 0.586398, acc.: 68.75%] [G loss: 1.130633]\n",
      "epoch:7 step:6929 [D loss: 0.667324, acc.: 65.62%] [G loss: 1.016077]\n",
      "epoch:7 step:6930 [D loss: 0.670549, acc.: 64.06%] [G loss: 0.845395]\n",
      "epoch:7 step:6931 [D loss: 0.520890, acc.: 73.44%] [G loss: 0.952343]\n",
      "epoch:7 step:6932 [D loss: 0.551170, acc.: 75.00%] [G loss: 1.139464]\n",
      "epoch:7 step:6933 [D loss: 0.603639, acc.: 71.09%] [G loss: 0.890524]\n",
      "epoch:7 step:6934 [D loss: 0.612413, acc.: 69.53%] [G loss: 0.859424]\n",
      "epoch:7 step:6935 [D loss: 0.675691, acc.: 55.47%] [G loss: 0.813611]\n",
      "epoch:7 step:6936 [D loss: 0.603200, acc.: 64.06%] [G loss: 1.164438]\n",
      "epoch:7 step:6937 [D loss: 0.587788, acc.: 65.62%] [G loss: 1.110827]\n",
      "epoch:7 step:6938 [D loss: 0.534058, acc.: 75.00%] [G loss: 1.255288]\n",
      "epoch:7 step:6939 [D loss: 0.551921, acc.: 71.09%] [G loss: 1.156449]\n",
      "epoch:7 step:6940 [D loss: 0.631617, acc.: 59.38%] [G loss: 1.030686]\n",
      "epoch:7 step:6941 [D loss: 0.583704, acc.: 70.31%] [G loss: 0.965500]\n",
      "epoch:7 step:6942 [D loss: 0.680276, acc.: 59.38%] [G loss: 0.997062]\n",
      "epoch:7 step:6943 [D loss: 0.652199, acc.: 63.28%] [G loss: 1.110980]\n",
      "epoch:7 step:6944 [D loss: 0.592647, acc.: 70.31%] [G loss: 1.199313]\n",
      "epoch:7 step:6945 [D loss: 0.680485, acc.: 58.59%] [G loss: 1.165568]\n",
      "epoch:7 step:6946 [D loss: 0.677119, acc.: 57.03%] [G loss: 1.183582]\n",
      "epoch:7 step:6947 [D loss: 0.731724, acc.: 57.81%] [G loss: 1.058761]\n",
      "epoch:7 step:6948 [D loss: 0.583347, acc.: 69.53%] [G loss: 1.115140]\n",
      "epoch:7 step:6949 [D loss: 0.501585, acc.: 76.56%] [G loss: 1.349745]\n",
      "epoch:7 step:6950 [D loss: 0.639955, acc.: 65.62%] [G loss: 1.041039]\n",
      "epoch:7 step:6951 [D loss: 0.639045, acc.: 61.72%] [G loss: 0.945203]\n",
      "epoch:7 step:6952 [D loss: 0.545748, acc.: 71.09%] [G loss: 1.185329]\n",
      "epoch:7 step:6953 [D loss: 0.688876, acc.: 58.59%] [G loss: 1.128403]\n",
      "epoch:7 step:6954 [D loss: 0.512294, acc.: 75.00%] [G loss: 1.238962]\n",
      "epoch:7 step:6955 [D loss: 0.666796, acc.: 58.59%] [G loss: 1.112910]\n",
      "epoch:7 step:6956 [D loss: 0.579666, acc.: 70.31%] [G loss: 1.089536]\n",
      "epoch:7 step:6957 [D loss: 0.622566, acc.: 59.38%] [G loss: 1.158146]\n",
      "epoch:7 step:6958 [D loss: 0.557806, acc.: 69.53%] [G loss: 1.116617]\n",
      "epoch:7 step:6959 [D loss: 0.712152, acc.: 54.69%] [G loss: 0.993378]\n",
      "epoch:7 step:6960 [D loss: 0.594402, acc.: 70.31%] [G loss: 1.193479]\n",
      "epoch:7 step:6961 [D loss: 0.696245, acc.: 59.38%] [G loss: 1.183476]\n",
      "epoch:7 step:6962 [D loss: 0.634137, acc.: 64.84%] [G loss: 1.186374]\n",
      "epoch:7 step:6963 [D loss: 0.669577, acc.: 61.72%] [G loss: 1.100240]\n",
      "epoch:7 step:6964 [D loss: 0.648639, acc.: 58.59%] [G loss: 1.071605]\n",
      "epoch:7 step:6965 [D loss: 0.534907, acc.: 75.00%] [G loss: 1.272605]\n",
      "epoch:7 step:6966 [D loss: 0.753698, acc.: 53.12%] [G loss: 1.052781]\n",
      "epoch:7 step:6967 [D loss: 0.598961, acc.: 71.09%] [G loss: 0.960830]\n",
      "epoch:7 step:6968 [D loss: 0.727954, acc.: 53.12%] [G loss: 0.878666]\n",
      "epoch:7 step:6969 [D loss: 0.651118, acc.: 63.28%] [G loss: 1.137648]\n",
      "epoch:7 step:6970 [D loss: 0.585246, acc.: 62.50%] [G loss: 1.041768]\n",
      "epoch:7 step:6971 [D loss: 0.662700, acc.: 59.38%] [G loss: 1.197230]\n",
      "epoch:7 step:6972 [D loss: 0.590250, acc.: 71.88%] [G loss: 1.089169]\n",
      "epoch:7 step:6973 [D loss: 0.669402, acc.: 62.50%] [G loss: 1.042925]\n",
      "epoch:7 step:6974 [D loss: 0.540237, acc.: 76.56%] [G loss: 1.084009]\n",
      "epoch:7 step:6975 [D loss: 0.664914, acc.: 60.94%] [G loss: 1.017220]\n",
      "epoch:7 step:6976 [D loss: 0.651569, acc.: 62.50%] [G loss: 1.057809]\n",
      "epoch:7 step:6977 [D loss: 0.618440, acc.: 62.50%] [G loss: 1.097381]\n",
      "epoch:7 step:6978 [D loss: 0.596037, acc.: 69.53%] [G loss: 1.369750]\n",
      "epoch:7 step:6979 [D loss: 0.486873, acc.: 82.03%] [G loss: 1.191676]\n",
      "epoch:7 step:6980 [D loss: 0.641154, acc.: 64.84%] [G loss: 1.169576]\n",
      "epoch:7 step:6981 [D loss: 0.646629, acc.: 61.72%] [G loss: 1.127504]\n",
      "epoch:7 step:6982 [D loss: 0.620204, acc.: 64.06%] [G loss: 1.040857]\n",
      "epoch:7 step:6983 [D loss: 0.585488, acc.: 72.66%] [G loss: 1.085319]\n",
      "epoch:7 step:6984 [D loss: 0.564370, acc.: 73.44%] [G loss: 1.133090]\n",
      "epoch:7 step:6985 [D loss: 0.578945, acc.: 71.88%] [G loss: 1.105281]\n",
      "epoch:7 step:6986 [D loss: 0.694419, acc.: 57.81%] [G loss: 0.919739]\n",
      "epoch:7 step:6987 [D loss: 0.656909, acc.: 64.84%] [G loss: 0.994447]\n",
      "epoch:7 step:6988 [D loss: 0.596429, acc.: 67.97%] [G loss: 1.202651]\n",
      "epoch:7 step:6989 [D loss: 0.683888, acc.: 59.38%] [G loss: 1.003565]\n",
      "epoch:7 step:6990 [D loss: 0.655016, acc.: 63.28%] [G loss: 1.139470]\n",
      "epoch:7 step:6991 [D loss: 0.513184, acc.: 78.91%] [G loss: 1.128978]\n",
      "epoch:7 step:6992 [D loss: 0.567721, acc.: 70.31%] [G loss: 1.178369]\n",
      "epoch:7 step:6993 [D loss: 0.685611, acc.: 58.59%] [G loss: 1.089542]\n",
      "epoch:7 step:6994 [D loss: 0.605032, acc.: 68.75%] [G loss: 1.109529]\n",
      "epoch:7 step:6995 [D loss: 0.607186, acc.: 67.19%] [G loss: 1.172067]\n",
      "epoch:7 step:6996 [D loss: 0.731689, acc.: 51.56%] [G loss: 1.018821]\n",
      "epoch:7 step:6997 [D loss: 0.662224, acc.: 58.59%] [G loss: 1.001848]\n",
      "epoch:7 step:6998 [D loss: 0.626310, acc.: 65.62%] [G loss: 1.299951]\n",
      "epoch:7 step:6999 [D loss: 0.576817, acc.: 68.75%] [G loss: 1.097139]\n",
      "epoch:7 step:7000 [D loss: 0.568145, acc.: 67.97%] [G loss: 1.242920]\n",
      "##############\n",
      "[2.71066984 2.12042824 1.83570933 2.61604768 0.80029261 5.39661682\n",
      " 1.81417414 2.73354094 3.81393942 7.14868929]\n",
      "##########\n",
      "epoch:7 step:7001 [D loss: 0.522517, acc.: 78.12%] [G loss: 1.339768]\n",
      "epoch:7 step:7002 [D loss: 0.641290, acc.: 64.84%] [G loss: 0.960815]\n",
      "epoch:7 step:7003 [D loss: 0.538355, acc.: 72.66%] [G loss: 1.132239]\n",
      "epoch:7 step:7004 [D loss: 0.459569, acc.: 82.03%] [G loss: 1.215473]\n",
      "epoch:7 step:7005 [D loss: 0.611118, acc.: 67.97%] [G loss: 1.183721]\n",
      "epoch:7 step:7006 [D loss: 0.520239, acc.: 75.00%] [G loss: 1.206212]\n",
      "epoch:7 step:7007 [D loss: 0.621310, acc.: 67.97%] [G loss: 1.084075]\n",
      "epoch:7 step:7008 [D loss: 0.629490, acc.: 64.06%] [G loss: 1.078618]\n",
      "epoch:7 step:7009 [D loss: 0.644206, acc.: 64.84%] [G loss: 1.318760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7010 [D loss: 0.632541, acc.: 66.41%] [G loss: 1.007556]\n",
      "epoch:7 step:7011 [D loss: 0.685580, acc.: 61.72%] [G loss: 1.055622]\n",
      "epoch:7 step:7012 [D loss: 0.517466, acc.: 80.47%] [G loss: 1.259597]\n",
      "epoch:7 step:7013 [D loss: 0.658495, acc.: 62.50%] [G loss: 1.013180]\n",
      "epoch:7 step:7014 [D loss: 0.626902, acc.: 63.28%] [G loss: 0.966526]\n",
      "epoch:7 step:7015 [D loss: 0.677009, acc.: 56.25%] [G loss: 1.082480]\n",
      "epoch:7 step:7016 [D loss: 0.632860, acc.: 63.28%] [G loss: 1.036851]\n",
      "epoch:7 step:7017 [D loss: 0.564160, acc.: 72.66%] [G loss: 1.101029]\n",
      "epoch:7 step:7018 [D loss: 0.619903, acc.: 63.28%] [G loss: 1.071684]\n",
      "epoch:7 step:7019 [D loss: 0.573095, acc.: 70.31%] [G loss: 1.001113]\n",
      "epoch:7 step:7020 [D loss: 0.639217, acc.: 64.84%] [G loss: 1.106738]\n",
      "epoch:7 step:7021 [D loss: 0.612296, acc.: 67.97%] [G loss: 0.975745]\n",
      "epoch:7 step:7022 [D loss: 0.640428, acc.: 63.28%] [G loss: 0.939076]\n",
      "epoch:7 step:7023 [D loss: 0.544651, acc.: 75.00%] [G loss: 1.078215]\n",
      "epoch:7 step:7024 [D loss: 0.673365, acc.: 59.38%] [G loss: 0.989626]\n",
      "epoch:7 step:7025 [D loss: 0.597496, acc.: 68.75%] [G loss: 1.089982]\n",
      "epoch:7 step:7026 [D loss: 0.554676, acc.: 71.88%] [G loss: 1.192139]\n",
      "epoch:7 step:7027 [D loss: 0.669622, acc.: 64.84%] [G loss: 1.075581]\n",
      "epoch:7 step:7028 [D loss: 0.684229, acc.: 64.06%] [G loss: 1.083609]\n",
      "epoch:7 step:7029 [D loss: 0.744519, acc.: 52.34%] [G loss: 0.958729]\n",
      "epoch:7 step:7030 [D loss: 0.646495, acc.: 64.84%] [G loss: 0.987150]\n",
      "epoch:7 step:7031 [D loss: 0.578873, acc.: 68.75%] [G loss: 1.248046]\n",
      "epoch:7 step:7032 [D loss: 0.635676, acc.: 60.16%] [G loss: 1.137546]\n",
      "epoch:7 step:7033 [D loss: 0.738474, acc.: 57.03%] [G loss: 0.963108]\n",
      "epoch:7 step:7034 [D loss: 0.557806, acc.: 69.53%] [G loss: 1.088721]\n",
      "epoch:7 step:7035 [D loss: 0.647238, acc.: 57.81%] [G loss: 1.170397]\n",
      "epoch:7 step:7036 [D loss: 0.670854, acc.: 60.16%] [G loss: 0.890430]\n",
      "epoch:7 step:7037 [D loss: 0.593226, acc.: 69.53%] [G loss: 1.198371]\n",
      "epoch:7 step:7038 [D loss: 0.573297, acc.: 75.00%] [G loss: 1.085487]\n",
      "epoch:7 step:7039 [D loss: 0.648915, acc.: 62.50%] [G loss: 1.097539]\n",
      "epoch:7 step:7040 [D loss: 0.583241, acc.: 67.97%] [G loss: 1.079496]\n",
      "epoch:7 step:7041 [D loss: 0.602350, acc.: 64.06%] [G loss: 1.064080]\n",
      "epoch:7 step:7042 [D loss: 0.688057, acc.: 57.03%] [G loss: 1.128892]\n",
      "epoch:7 step:7043 [D loss: 0.551966, acc.: 71.88%] [G loss: 1.107475]\n",
      "epoch:7 step:7044 [D loss: 0.660550, acc.: 63.28%] [G loss: 1.158292]\n",
      "epoch:7 step:7045 [D loss: 0.574478, acc.: 68.75%] [G loss: 1.066306]\n",
      "epoch:7 step:7046 [D loss: 0.671044, acc.: 60.94%] [G loss: 1.221690]\n",
      "epoch:7 step:7047 [D loss: 0.566992, acc.: 67.19%] [G loss: 1.070055]\n",
      "epoch:7 step:7048 [D loss: 0.685856, acc.: 54.69%] [G loss: 1.222327]\n",
      "epoch:7 step:7049 [D loss: 0.574682, acc.: 70.31%] [G loss: 1.428787]\n",
      "epoch:7 step:7050 [D loss: 0.603939, acc.: 68.75%] [G loss: 1.069647]\n",
      "epoch:7 step:7051 [D loss: 0.615211, acc.: 64.84%] [G loss: 1.309845]\n",
      "epoch:7 step:7052 [D loss: 0.587409, acc.: 73.44%] [G loss: 0.981071]\n",
      "epoch:7 step:7053 [D loss: 0.655068, acc.: 61.72%] [G loss: 1.172545]\n",
      "epoch:7 step:7054 [D loss: 0.711837, acc.: 55.47%] [G loss: 1.039263]\n",
      "epoch:7 step:7055 [D loss: 0.565924, acc.: 74.22%] [G loss: 1.133922]\n",
      "epoch:7 step:7056 [D loss: 0.622863, acc.: 61.72%] [G loss: 1.100444]\n",
      "epoch:7 step:7057 [D loss: 0.609796, acc.: 64.06%] [G loss: 0.967189]\n",
      "epoch:7 step:7058 [D loss: 0.647813, acc.: 60.94%] [G loss: 1.091369]\n",
      "epoch:7 step:7059 [D loss: 0.600523, acc.: 70.31%] [G loss: 1.089618]\n",
      "epoch:7 step:7060 [D loss: 0.668242, acc.: 58.59%] [G loss: 0.944000]\n",
      "epoch:7 step:7061 [D loss: 0.571420, acc.: 72.66%] [G loss: 1.135299]\n",
      "epoch:7 step:7062 [D loss: 0.603545, acc.: 70.31%] [G loss: 1.014158]\n",
      "epoch:7 step:7063 [D loss: 0.769236, acc.: 54.69%] [G loss: 1.020195]\n",
      "epoch:7 step:7064 [D loss: 0.635309, acc.: 67.19%] [G loss: 1.193749]\n",
      "epoch:7 step:7065 [D loss: 0.652336, acc.: 60.16%] [G loss: 1.169966]\n",
      "epoch:7 step:7066 [D loss: 0.704889, acc.: 52.34%] [G loss: 1.003098]\n",
      "epoch:7 step:7067 [D loss: 0.723343, acc.: 55.47%] [G loss: 1.039637]\n",
      "epoch:7 step:7068 [D loss: 0.500837, acc.: 82.03%] [G loss: 1.306792]\n",
      "epoch:7 step:7069 [D loss: 0.546574, acc.: 75.00%] [G loss: 1.138869]\n",
      "epoch:7 step:7070 [D loss: 0.681049, acc.: 61.72%] [G loss: 0.948259]\n",
      "epoch:7 step:7071 [D loss: 0.656776, acc.: 61.72%] [G loss: 0.967856]\n",
      "epoch:7 step:7072 [D loss: 0.626972, acc.: 63.28%] [G loss: 0.863135]\n",
      "epoch:7 step:7073 [D loss: 0.536212, acc.: 76.56%] [G loss: 1.111770]\n",
      "epoch:7 step:7074 [D loss: 0.646514, acc.: 60.16%] [G loss: 1.082348]\n",
      "epoch:7 step:7075 [D loss: 0.588343, acc.: 70.31%] [G loss: 1.300545]\n",
      "epoch:7 step:7076 [D loss: 0.646208, acc.: 66.41%] [G loss: 1.104629]\n",
      "epoch:7 step:7077 [D loss: 0.635368, acc.: 71.09%] [G loss: 0.871773]\n",
      "epoch:7 step:7078 [D loss: 0.692487, acc.: 58.59%] [G loss: 0.952039]\n",
      "epoch:7 step:7079 [D loss: 0.608059, acc.: 64.06%] [G loss: 1.071017]\n",
      "epoch:7 step:7080 [D loss: 0.519195, acc.: 75.78%] [G loss: 1.148480]\n",
      "epoch:7 step:7081 [D loss: 0.560925, acc.: 71.09%] [G loss: 1.102151]\n",
      "epoch:7 step:7082 [D loss: 0.581797, acc.: 67.97%] [G loss: 0.973604]\n",
      "epoch:7 step:7083 [D loss: 0.600162, acc.: 67.97%] [G loss: 1.158066]\n",
      "epoch:7 step:7084 [D loss: 0.497596, acc.: 75.00%] [G loss: 1.158946]\n",
      "epoch:7 step:7085 [D loss: 0.575159, acc.: 68.75%] [G loss: 1.198233]\n",
      "epoch:7 step:7086 [D loss: 0.556356, acc.: 71.88%] [G loss: 1.172959]\n",
      "epoch:7 step:7087 [D loss: 0.620104, acc.: 66.41%] [G loss: 1.136670]\n",
      "epoch:7 step:7088 [D loss: 0.590942, acc.: 70.31%] [G loss: 1.119502]\n",
      "epoch:7 step:7089 [D loss: 0.627584, acc.: 60.94%] [G loss: 1.156303]\n",
      "epoch:7 step:7090 [D loss: 0.540895, acc.: 72.66%] [G loss: 1.256116]\n",
      "epoch:7 step:7091 [D loss: 0.667191, acc.: 61.72%] [G loss: 1.125125]\n",
      "epoch:7 step:7092 [D loss: 0.595060, acc.: 67.97%] [G loss: 1.254781]\n",
      "epoch:7 step:7093 [D loss: 0.615764, acc.: 69.53%] [G loss: 1.090038]\n",
      "epoch:7 step:7094 [D loss: 0.639023, acc.: 67.19%] [G loss: 1.085799]\n",
      "epoch:7 step:7095 [D loss: 0.595588, acc.: 66.41%] [G loss: 1.065441]\n",
      "epoch:7 step:7096 [D loss: 0.683088, acc.: 52.34%] [G loss: 1.147224]\n",
      "epoch:7 step:7097 [D loss: 0.613652, acc.: 66.41%] [G loss: 1.198473]\n",
      "epoch:7 step:7098 [D loss: 0.507770, acc.: 82.81%] [G loss: 1.236897]\n",
      "epoch:7 step:7099 [D loss: 0.582599, acc.: 67.19%] [G loss: 1.102154]\n",
      "epoch:7 step:7100 [D loss: 0.564403, acc.: 71.88%] [G loss: 1.129437]\n",
      "epoch:7 step:7101 [D loss: 0.559610, acc.: 75.78%] [G loss: 1.154274]\n",
      "epoch:7 step:7102 [D loss: 0.598394, acc.: 65.62%] [G loss: 0.969523]\n",
      "epoch:7 step:7103 [D loss: 0.651086, acc.: 64.06%] [G loss: 0.820009]\n",
      "epoch:7 step:7104 [D loss: 0.575724, acc.: 71.09%] [G loss: 1.279562]\n",
      "epoch:7 step:7105 [D loss: 0.564032, acc.: 71.88%] [G loss: 1.107345]\n",
      "epoch:7 step:7106 [D loss: 0.565989, acc.: 71.09%] [G loss: 1.024926]\n",
      "epoch:7 step:7107 [D loss: 0.675709, acc.: 58.59%] [G loss: 1.081043]\n",
      "epoch:7 step:7108 [D loss: 0.666266, acc.: 61.72%] [G loss: 0.975364]\n",
      "epoch:7 step:7109 [D loss: 0.630340, acc.: 68.75%] [G loss: 1.082281]\n",
      "epoch:7 step:7110 [D loss: 0.544799, acc.: 70.31%] [G loss: 1.178743]\n",
      "epoch:7 step:7111 [D loss: 0.681721, acc.: 61.72%] [G loss: 1.242369]\n",
      "epoch:7 step:7112 [D loss: 0.641161, acc.: 62.50%] [G loss: 0.973349]\n",
      "epoch:7 step:7113 [D loss: 0.622936, acc.: 63.28%] [G loss: 1.004755]\n",
      "epoch:7 step:7114 [D loss: 0.549181, acc.: 74.22%] [G loss: 1.127373]\n",
      "epoch:7 step:7115 [D loss: 0.666729, acc.: 61.72%] [G loss: 1.093423]\n",
      "epoch:7 step:7116 [D loss: 0.631069, acc.: 64.06%] [G loss: 1.300622]\n",
      "epoch:7 step:7117 [D loss: 0.704408, acc.: 57.81%] [G loss: 1.181901]\n",
      "epoch:7 step:7118 [D loss: 0.639508, acc.: 64.84%] [G loss: 1.216919]\n",
      "epoch:7 step:7119 [D loss: 0.510610, acc.: 77.34%] [G loss: 1.263256]\n",
      "epoch:7 step:7120 [D loss: 0.735452, acc.: 53.91%] [G loss: 1.066448]\n",
      "epoch:7 step:7121 [D loss: 0.532435, acc.: 76.56%] [G loss: 1.411724]\n",
      "epoch:7 step:7122 [D loss: 0.562695, acc.: 72.66%] [G loss: 1.104318]\n",
      "epoch:7 step:7123 [D loss: 0.561376, acc.: 71.09%] [G loss: 1.197073]\n",
      "epoch:7 step:7124 [D loss: 0.589751, acc.: 72.66%] [G loss: 1.132039]\n",
      "epoch:7 step:7125 [D loss: 0.554059, acc.: 75.78%] [G loss: 1.326710]\n",
      "epoch:7 step:7126 [D loss: 0.644975, acc.: 64.06%] [G loss: 1.040931]\n",
      "epoch:7 step:7127 [D loss: 0.735934, acc.: 53.12%] [G loss: 1.090386]\n",
      "epoch:7 step:7128 [D loss: 0.591543, acc.: 66.41%] [G loss: 1.166065]\n",
      "epoch:7 step:7129 [D loss: 0.586158, acc.: 63.28%] [G loss: 1.057262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7130 [D loss: 0.686163, acc.: 60.16%] [G loss: 0.997484]\n",
      "epoch:7 step:7131 [D loss: 0.509721, acc.: 75.78%] [G loss: 1.105436]\n",
      "epoch:7 step:7132 [D loss: 0.700082, acc.: 56.25%] [G loss: 1.141604]\n",
      "epoch:7 step:7133 [D loss: 0.681132, acc.: 55.47%] [G loss: 1.184115]\n",
      "epoch:7 step:7134 [D loss: 0.787197, acc.: 49.22%] [G loss: 1.106751]\n",
      "epoch:7 step:7135 [D loss: 0.605543, acc.: 67.97%] [G loss: 1.056368]\n",
      "epoch:7 step:7136 [D loss: 0.559791, acc.: 75.78%] [G loss: 1.155810]\n",
      "epoch:7 step:7137 [D loss: 0.671970, acc.: 53.12%] [G loss: 0.898220]\n",
      "epoch:7 step:7138 [D loss: 0.650763, acc.: 62.50%] [G loss: 1.072075]\n",
      "epoch:7 step:7139 [D loss: 0.554534, acc.: 69.53%] [G loss: 1.288524]\n",
      "epoch:7 step:7140 [D loss: 0.716693, acc.: 55.47%] [G loss: 1.177860]\n",
      "epoch:7 step:7141 [D loss: 0.704398, acc.: 51.56%] [G loss: 0.987046]\n",
      "epoch:7 step:7142 [D loss: 0.642742, acc.: 60.16%] [G loss: 0.937665]\n",
      "epoch:7 step:7143 [D loss: 0.629250, acc.: 67.97%] [G loss: 1.089187]\n",
      "epoch:7 step:7144 [D loss: 0.625333, acc.: 62.50%] [G loss: 1.023411]\n",
      "epoch:7 step:7145 [D loss: 0.682367, acc.: 59.38%] [G loss: 1.044424]\n",
      "epoch:7 step:7146 [D loss: 0.595769, acc.: 68.75%] [G loss: 1.035956]\n",
      "epoch:7 step:7147 [D loss: 0.600768, acc.: 67.19%] [G loss: 1.105737]\n",
      "epoch:7 step:7148 [D loss: 0.641675, acc.: 60.94%] [G loss: 1.091192]\n",
      "epoch:7 step:7149 [D loss: 0.681100, acc.: 58.59%] [G loss: 1.174621]\n",
      "epoch:7 step:7150 [D loss: 0.570769, acc.: 73.44%] [G loss: 1.134513]\n",
      "epoch:7 step:7151 [D loss: 0.572363, acc.: 68.75%] [G loss: 1.032870]\n",
      "epoch:7 step:7152 [D loss: 0.707792, acc.: 52.34%] [G loss: 1.008583]\n",
      "epoch:7 step:7153 [D loss: 0.568801, acc.: 71.09%] [G loss: 1.180506]\n",
      "epoch:7 step:7154 [D loss: 0.593735, acc.: 68.75%] [G loss: 1.090097]\n",
      "epoch:7 step:7155 [D loss: 0.584712, acc.: 66.41%] [G loss: 1.008830]\n",
      "epoch:7 step:7156 [D loss: 0.553999, acc.: 78.12%] [G loss: 1.177448]\n",
      "epoch:7 step:7157 [D loss: 0.608079, acc.: 65.62%] [G loss: 1.075766]\n",
      "epoch:7 step:7158 [D loss: 0.535867, acc.: 73.44%] [G loss: 0.960639]\n",
      "epoch:7 step:7159 [D loss: 0.501132, acc.: 78.91%] [G loss: 0.956011]\n",
      "epoch:7 step:7160 [D loss: 0.641418, acc.: 61.72%] [G loss: 0.967910]\n",
      "epoch:7 step:7161 [D loss: 0.570596, acc.: 67.97%] [G loss: 1.001980]\n",
      "epoch:7 step:7162 [D loss: 0.608101, acc.: 69.53%] [G loss: 1.161826]\n",
      "epoch:7 step:7163 [D loss: 0.578890, acc.: 70.31%] [G loss: 1.197495]\n",
      "epoch:7 step:7164 [D loss: 0.560629, acc.: 70.31%] [G loss: 1.049525]\n",
      "epoch:7 step:7165 [D loss: 0.633800, acc.: 60.16%] [G loss: 1.047524]\n",
      "epoch:7 step:7166 [D loss: 0.674094, acc.: 57.03%] [G loss: 1.163612]\n",
      "epoch:7 step:7167 [D loss: 0.528427, acc.: 77.34%] [G loss: 1.139070]\n",
      "epoch:7 step:7168 [D loss: 0.569186, acc.: 70.31%] [G loss: 1.192305]\n",
      "epoch:7 step:7169 [D loss: 0.618703, acc.: 63.28%] [G loss: 1.181618]\n",
      "epoch:7 step:7170 [D loss: 0.617924, acc.: 64.84%] [G loss: 1.184745]\n",
      "epoch:7 step:7171 [D loss: 0.705567, acc.: 54.69%] [G loss: 0.966227]\n",
      "epoch:7 step:7172 [D loss: 0.508452, acc.: 81.25%] [G loss: 1.059388]\n",
      "epoch:7 step:7173 [D loss: 0.636131, acc.: 67.19%] [G loss: 1.156396]\n",
      "epoch:7 step:7174 [D loss: 0.571396, acc.: 66.41%] [G loss: 1.276011]\n",
      "epoch:7 step:7175 [D loss: 0.551383, acc.: 74.22%] [G loss: 1.317251]\n",
      "epoch:7 step:7176 [D loss: 0.600051, acc.: 66.41%] [G loss: 1.379364]\n",
      "epoch:7 step:7177 [D loss: 0.543332, acc.: 74.22%] [G loss: 1.281234]\n",
      "epoch:7 step:7178 [D loss: 0.531856, acc.: 72.66%] [G loss: 1.183453]\n",
      "epoch:7 step:7179 [D loss: 0.608530, acc.: 64.84%] [G loss: 1.213268]\n",
      "epoch:7 step:7180 [D loss: 0.628280, acc.: 66.41%] [G loss: 1.092350]\n",
      "epoch:7 step:7181 [D loss: 0.704813, acc.: 56.25%] [G loss: 1.036230]\n",
      "epoch:7 step:7182 [D loss: 0.599751, acc.: 71.09%] [G loss: 1.183274]\n",
      "epoch:7 step:7183 [D loss: 0.621154, acc.: 64.06%] [G loss: 1.130477]\n",
      "epoch:7 step:7184 [D loss: 0.612520, acc.: 64.84%] [G loss: 1.056784]\n",
      "epoch:7 step:7185 [D loss: 0.534318, acc.: 75.78%] [G loss: 1.288419]\n",
      "epoch:7 step:7186 [D loss: 0.546306, acc.: 75.78%] [G loss: 1.036992]\n",
      "epoch:7 step:7187 [D loss: 0.583151, acc.: 70.31%] [G loss: 1.206722]\n",
      "epoch:7 step:7188 [D loss: 0.636248, acc.: 60.94%] [G loss: 1.079696]\n",
      "epoch:7 step:7189 [D loss: 0.658359, acc.: 63.28%] [G loss: 1.087261]\n",
      "epoch:7 step:7190 [D loss: 0.542900, acc.: 74.22%] [G loss: 1.177040]\n",
      "epoch:7 step:7191 [D loss: 0.578378, acc.: 66.41%] [G loss: 0.983339]\n",
      "epoch:7 step:7192 [D loss: 0.581792, acc.: 70.31%] [G loss: 1.069265]\n",
      "epoch:7 step:7193 [D loss: 0.534537, acc.: 80.47%] [G loss: 1.227901]\n",
      "epoch:7 step:7194 [D loss: 0.648432, acc.: 60.16%] [G loss: 1.135221]\n",
      "epoch:7 step:7195 [D loss: 0.666965, acc.: 60.94%] [G loss: 1.012107]\n",
      "epoch:7 step:7196 [D loss: 0.639553, acc.: 64.84%] [G loss: 0.893482]\n",
      "epoch:7 step:7197 [D loss: 0.638375, acc.: 64.84%] [G loss: 1.000338]\n",
      "epoch:7 step:7198 [D loss: 0.647185, acc.: 66.41%] [G loss: 1.088304]\n",
      "epoch:7 step:7199 [D loss: 0.609873, acc.: 64.06%] [G loss: 1.208894]\n",
      "epoch:7 step:7200 [D loss: 0.651636, acc.: 63.28%] [G loss: 1.041454]\n",
      "##############\n",
      "[2.72952055 2.07921316 1.91159629 2.58459175 0.92609885 5.78623474\n",
      " 2.073433   2.73400971 3.84335225 4.30603241]\n",
      "##########\n",
      "epoch:7 step:7201 [D loss: 0.612171, acc.: 67.97%] [G loss: 1.121821]\n",
      "epoch:7 step:7202 [D loss: 0.681507, acc.: 60.94%] [G loss: 1.121614]\n",
      "epoch:7 step:7203 [D loss: 0.683199, acc.: 57.81%] [G loss: 0.996155]\n",
      "epoch:7 step:7204 [D loss: 0.580707, acc.: 69.53%] [G loss: 1.196076]\n",
      "epoch:7 step:7205 [D loss: 0.518068, acc.: 75.00%] [G loss: 1.220070]\n",
      "epoch:7 step:7206 [D loss: 0.727299, acc.: 53.91%] [G loss: 1.137877]\n",
      "epoch:7 step:7207 [D loss: 0.598486, acc.: 67.19%] [G loss: 1.061697]\n",
      "epoch:7 step:7208 [D loss: 0.624402, acc.: 68.75%] [G loss: 1.180023]\n",
      "epoch:7 step:7209 [D loss: 0.618018, acc.: 64.84%] [G loss: 1.272170]\n",
      "epoch:7 step:7210 [D loss: 0.542362, acc.: 71.88%] [G loss: 1.127711]\n",
      "epoch:7 step:7211 [D loss: 0.621121, acc.: 61.72%] [G loss: 1.125066]\n",
      "epoch:7 step:7212 [D loss: 0.580785, acc.: 70.31%] [G loss: 1.164397]\n",
      "epoch:7 step:7213 [D loss: 0.573758, acc.: 71.88%] [G loss: 1.202310]\n",
      "epoch:7 step:7214 [D loss: 0.655073, acc.: 60.16%] [G loss: 0.950682]\n",
      "epoch:7 step:7215 [D loss: 0.507478, acc.: 79.69%] [G loss: 1.190824]\n",
      "epoch:7 step:7216 [D loss: 0.631689, acc.: 63.28%] [G loss: 1.052463]\n",
      "epoch:7 step:7217 [D loss: 0.639812, acc.: 60.94%] [G loss: 0.848874]\n",
      "epoch:7 step:7218 [D loss: 0.624786, acc.: 66.41%] [G loss: 1.094064]\n",
      "epoch:7 step:7219 [D loss: 0.663263, acc.: 64.84%] [G loss: 1.144017]\n",
      "epoch:7 step:7220 [D loss: 0.759833, acc.: 50.00%] [G loss: 1.124605]\n",
      "epoch:7 step:7221 [D loss: 0.533073, acc.: 71.88%] [G loss: 1.178154]\n",
      "epoch:7 step:7222 [D loss: 0.708056, acc.: 51.56%] [G loss: 0.879802]\n",
      "epoch:7 step:7223 [D loss: 0.679899, acc.: 60.16%] [G loss: 1.147187]\n",
      "epoch:7 step:7224 [D loss: 0.586177, acc.: 67.97%] [G loss: 1.123421]\n",
      "epoch:7 step:7225 [D loss: 0.541581, acc.: 76.56%] [G loss: 1.188081]\n",
      "epoch:7 step:7226 [D loss: 0.546425, acc.: 75.00%] [G loss: 1.168841]\n",
      "epoch:7 step:7227 [D loss: 0.615099, acc.: 67.97%] [G loss: 0.985607]\n",
      "epoch:7 step:7228 [D loss: 0.569490, acc.: 71.88%] [G loss: 1.206059]\n",
      "epoch:7 step:7229 [D loss: 0.609699, acc.: 61.72%] [G loss: 0.982593]\n",
      "epoch:7 step:7230 [D loss: 0.532746, acc.: 74.22%] [G loss: 1.165261]\n",
      "epoch:7 step:7231 [D loss: 0.622664, acc.: 65.62%] [G loss: 1.334896]\n",
      "epoch:7 step:7232 [D loss: 0.695705, acc.: 60.16%] [G loss: 1.101511]\n",
      "epoch:7 step:7233 [D loss: 0.576981, acc.: 67.97%] [G loss: 1.050445]\n",
      "epoch:7 step:7234 [D loss: 0.549957, acc.: 73.44%] [G loss: 1.283844]\n",
      "epoch:7 step:7235 [D loss: 0.787810, acc.: 50.78%] [G loss: 0.870811]\n",
      "epoch:7 step:7236 [D loss: 0.724418, acc.: 58.59%] [G loss: 1.181282]\n",
      "epoch:7 step:7237 [D loss: 0.676910, acc.: 59.38%] [G loss: 1.092007]\n",
      "epoch:7 step:7238 [D loss: 0.732191, acc.: 48.44%] [G loss: 0.954588]\n",
      "epoch:7 step:7239 [D loss: 0.577398, acc.: 72.66%] [G loss: 1.209345]\n",
      "epoch:7 step:7240 [D loss: 0.713198, acc.: 57.03%] [G loss: 1.141127]\n",
      "epoch:7 step:7241 [D loss: 0.529763, acc.: 76.56%] [G loss: 1.034285]\n",
      "epoch:7 step:7242 [D loss: 0.714680, acc.: 56.25%] [G loss: 1.030598]\n",
      "epoch:7 step:7243 [D loss: 0.533255, acc.: 75.78%] [G loss: 1.157147]\n",
      "epoch:7 step:7244 [D loss: 0.537690, acc.: 69.53%] [G loss: 1.276167]\n",
      "epoch:7 step:7245 [D loss: 0.622159, acc.: 62.50%] [G loss: 0.994175]\n",
      "epoch:7 step:7246 [D loss: 0.600014, acc.: 73.44%] [G loss: 1.247782]\n",
      "epoch:7 step:7247 [D loss: 0.579591, acc.: 74.22%] [G loss: 1.079749]\n",
      "epoch:7 step:7248 [D loss: 0.699643, acc.: 57.03%] [G loss: 0.997145]\n",
      "epoch:7 step:7249 [D loss: 0.645837, acc.: 60.16%] [G loss: 1.100467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7250 [D loss: 0.717486, acc.: 56.25%] [G loss: 0.966496]\n",
      "epoch:7 step:7251 [D loss: 0.558156, acc.: 68.75%] [G loss: 1.092473]\n",
      "epoch:7 step:7252 [D loss: 0.578826, acc.: 67.97%] [G loss: 1.350147]\n",
      "epoch:7 step:7253 [D loss: 0.689513, acc.: 53.91%] [G loss: 0.998396]\n",
      "epoch:7 step:7254 [D loss: 0.531083, acc.: 71.88%] [G loss: 1.078165]\n",
      "epoch:7 step:7255 [D loss: 0.557375, acc.: 73.44%] [G loss: 1.044130]\n",
      "epoch:7 step:7256 [D loss: 0.648592, acc.: 63.28%] [G loss: 0.999943]\n",
      "epoch:7 step:7257 [D loss: 0.547922, acc.: 75.00%] [G loss: 1.137193]\n",
      "epoch:7 step:7258 [D loss: 0.552598, acc.: 75.78%] [G loss: 1.253170]\n",
      "epoch:7 step:7259 [D loss: 0.669375, acc.: 61.72%] [G loss: 0.919226]\n",
      "epoch:7 step:7260 [D loss: 0.509322, acc.: 76.56%] [G loss: 1.209365]\n",
      "epoch:7 step:7261 [D loss: 0.474850, acc.: 85.16%] [G loss: 1.119072]\n",
      "epoch:7 step:7262 [D loss: 0.658228, acc.: 59.38%] [G loss: 1.082527]\n",
      "epoch:7 step:7263 [D loss: 0.625843, acc.: 60.94%] [G loss: 1.245155]\n",
      "epoch:7 step:7264 [D loss: 0.615763, acc.: 64.06%] [G loss: 1.179243]\n",
      "epoch:7 step:7265 [D loss: 0.658061, acc.: 56.25%] [G loss: 0.998747]\n",
      "epoch:7 step:7266 [D loss: 0.553748, acc.: 71.09%] [G loss: 1.260070]\n",
      "epoch:7 step:7267 [D loss: 0.674727, acc.: 53.91%] [G loss: 1.006411]\n",
      "epoch:7 step:7268 [D loss: 0.660941, acc.: 66.41%] [G loss: 1.172691]\n",
      "epoch:7 step:7269 [D loss: 0.578521, acc.: 68.75%] [G loss: 1.089605]\n",
      "epoch:7 step:7270 [D loss: 0.623081, acc.: 64.84%] [G loss: 1.041343]\n",
      "epoch:7 step:7271 [D loss: 0.587202, acc.: 71.88%] [G loss: 1.075393]\n",
      "epoch:7 step:7272 [D loss: 0.628983, acc.: 61.72%] [G loss: 1.082548]\n",
      "epoch:7 step:7273 [D loss: 0.650155, acc.: 57.81%] [G loss: 1.170182]\n",
      "epoch:7 step:7274 [D loss: 0.572649, acc.: 73.44%] [G loss: 1.283024]\n",
      "epoch:7 step:7275 [D loss: 0.658774, acc.: 60.94%] [G loss: 1.061410]\n",
      "epoch:7 step:7276 [D loss: 0.609758, acc.: 66.41%] [G loss: 1.188695]\n",
      "epoch:7 step:7277 [D loss: 0.560449, acc.: 73.44%] [G loss: 1.128255]\n",
      "epoch:7 step:7278 [D loss: 0.586833, acc.: 67.97%] [G loss: 1.324993]\n",
      "epoch:7 step:7279 [D loss: 0.667460, acc.: 64.84%] [G loss: 0.926160]\n",
      "epoch:7 step:7280 [D loss: 0.625716, acc.: 62.50%] [G loss: 1.082862]\n",
      "epoch:7 step:7281 [D loss: 0.645175, acc.: 64.06%] [G loss: 1.110680]\n",
      "epoch:7 step:7282 [D loss: 0.584980, acc.: 65.62%] [G loss: 1.255732]\n",
      "epoch:7 step:7283 [D loss: 0.617164, acc.: 69.53%] [G loss: 1.277070]\n",
      "epoch:7 step:7284 [D loss: 0.653757, acc.: 60.94%] [G loss: 1.102901]\n",
      "epoch:7 step:7285 [D loss: 0.636845, acc.: 66.41%] [G loss: 1.187066]\n",
      "epoch:7 step:7286 [D loss: 0.578934, acc.: 69.53%] [G loss: 1.279085]\n",
      "epoch:7 step:7287 [D loss: 0.622033, acc.: 65.62%] [G loss: 1.145246]\n",
      "epoch:7 step:7288 [D loss: 0.549736, acc.: 76.56%] [G loss: 1.103306]\n",
      "epoch:7 step:7289 [D loss: 0.728931, acc.: 50.00%] [G loss: 1.029982]\n",
      "epoch:7 step:7290 [D loss: 0.716229, acc.: 47.66%] [G loss: 0.892319]\n",
      "epoch:7 step:7291 [D loss: 0.559519, acc.: 75.00%] [G loss: 0.992605]\n",
      "epoch:7 step:7292 [D loss: 0.570532, acc.: 68.75%] [G loss: 0.943338]\n",
      "epoch:7 step:7293 [D loss: 0.578647, acc.: 73.44%] [G loss: 1.167320]\n",
      "epoch:7 step:7294 [D loss: 0.650165, acc.: 65.62%] [G loss: 1.082640]\n",
      "epoch:7 step:7295 [D loss: 0.549402, acc.: 70.31%] [G loss: 1.054834]\n",
      "epoch:7 step:7296 [D loss: 0.630211, acc.: 62.50%] [G loss: 1.016574]\n",
      "epoch:7 step:7297 [D loss: 0.659438, acc.: 59.38%] [G loss: 0.977557]\n",
      "epoch:7 step:7298 [D loss: 0.624238, acc.: 68.75%] [G loss: 0.968557]\n",
      "epoch:7 step:7299 [D loss: 0.601502, acc.: 70.31%] [G loss: 1.231391]\n",
      "epoch:7 step:7300 [D loss: 0.659512, acc.: 53.91%] [G loss: 1.082458]\n",
      "epoch:7 step:7301 [D loss: 0.548680, acc.: 75.78%] [G loss: 1.174925]\n",
      "epoch:7 step:7302 [D loss: 0.743762, acc.: 50.78%] [G loss: 1.185493]\n",
      "epoch:7 step:7303 [D loss: 0.586749, acc.: 67.97%] [G loss: 1.118299]\n",
      "epoch:7 step:7304 [D loss: 0.636546, acc.: 63.28%] [G loss: 1.038764]\n",
      "epoch:7 step:7305 [D loss: 0.549642, acc.: 72.66%] [G loss: 1.229218]\n",
      "epoch:7 step:7306 [D loss: 0.582077, acc.: 67.97%] [G loss: 1.098963]\n",
      "epoch:7 step:7307 [D loss: 0.556600, acc.: 71.09%] [G loss: 1.140619]\n",
      "epoch:7 step:7308 [D loss: 0.686412, acc.: 62.50%] [G loss: 1.181729]\n",
      "epoch:7 step:7309 [D loss: 0.564290, acc.: 71.88%] [G loss: 1.128643]\n",
      "epoch:7 step:7310 [D loss: 0.553769, acc.: 72.66%] [G loss: 1.161661]\n",
      "epoch:7 step:7311 [D loss: 0.668805, acc.: 57.81%] [G loss: 1.259787]\n",
      "epoch:7 step:7312 [D loss: 0.603716, acc.: 68.75%] [G loss: 1.157930]\n",
      "epoch:7 step:7313 [D loss: 0.720968, acc.: 53.12%] [G loss: 1.110303]\n",
      "epoch:7 step:7314 [D loss: 0.629093, acc.: 64.84%] [G loss: 1.192036]\n",
      "epoch:7 step:7315 [D loss: 0.705703, acc.: 52.34%] [G loss: 1.153977]\n",
      "epoch:7 step:7316 [D loss: 0.510808, acc.: 78.91%] [G loss: 1.360445]\n",
      "epoch:7 step:7317 [D loss: 0.558820, acc.: 74.22%] [G loss: 1.210109]\n",
      "epoch:7 step:7318 [D loss: 0.651278, acc.: 64.06%] [G loss: 1.010880]\n",
      "epoch:7 step:7319 [D loss: 0.617276, acc.: 64.84%] [G loss: 1.176373]\n",
      "epoch:7 step:7320 [D loss: 0.675890, acc.: 57.81%] [G loss: 1.078387]\n",
      "epoch:7 step:7321 [D loss: 0.584486, acc.: 70.31%] [G loss: 1.202543]\n",
      "epoch:7 step:7322 [D loss: 0.596125, acc.: 68.75%] [G loss: 1.235321]\n",
      "epoch:7 step:7323 [D loss: 0.503113, acc.: 73.44%] [G loss: 1.221010]\n",
      "epoch:7 step:7324 [D loss: 0.670008, acc.: 59.38%] [G loss: 1.040908]\n",
      "epoch:7 step:7325 [D loss: 0.588594, acc.: 71.09%] [G loss: 1.194200]\n",
      "epoch:7 step:7326 [D loss: 0.645150, acc.: 63.28%] [G loss: 1.034586]\n",
      "epoch:7 step:7327 [D loss: 0.630885, acc.: 66.41%] [G loss: 1.107680]\n",
      "epoch:7 step:7328 [D loss: 0.534915, acc.: 72.66%] [G loss: 1.230929]\n",
      "epoch:7 step:7329 [D loss: 0.638205, acc.: 64.06%] [G loss: 1.169198]\n",
      "epoch:7 step:7330 [D loss: 0.598859, acc.: 67.97%] [G loss: 1.153233]\n",
      "epoch:7 step:7331 [D loss: 0.666829, acc.: 58.59%] [G loss: 1.229473]\n",
      "epoch:7 step:7332 [D loss: 0.599868, acc.: 67.19%] [G loss: 1.250788]\n",
      "epoch:7 step:7333 [D loss: 0.631766, acc.: 62.50%] [G loss: 1.168737]\n",
      "epoch:7 step:7334 [D loss: 0.530228, acc.: 77.34%] [G loss: 1.116843]\n",
      "epoch:7 step:7335 [D loss: 0.585033, acc.: 69.53%] [G loss: 1.249531]\n",
      "epoch:7 step:7336 [D loss: 0.554513, acc.: 68.75%] [G loss: 1.105224]\n",
      "epoch:7 step:7337 [D loss: 0.606840, acc.: 67.97%] [G loss: 1.040812]\n",
      "epoch:7 step:7338 [D loss: 0.504294, acc.: 75.78%] [G loss: 1.118665]\n",
      "epoch:7 step:7339 [D loss: 0.594009, acc.: 67.19%] [G loss: 0.970626]\n",
      "epoch:7 step:7340 [D loss: 0.589123, acc.: 66.41%] [G loss: 1.009890]\n",
      "epoch:7 step:7341 [D loss: 0.648268, acc.: 67.97%] [G loss: 0.907183]\n",
      "epoch:7 step:7342 [D loss: 0.571417, acc.: 67.19%] [G loss: 1.095302]\n",
      "epoch:7 step:7343 [D loss: 0.612348, acc.: 67.97%] [G loss: 1.141761]\n",
      "epoch:7 step:7344 [D loss: 0.695052, acc.: 54.69%] [G loss: 0.971177]\n",
      "epoch:7 step:7345 [D loss: 0.630439, acc.: 63.28%] [G loss: 1.283215]\n",
      "epoch:7 step:7346 [D loss: 0.579207, acc.: 67.97%] [G loss: 1.020407]\n",
      "epoch:7 step:7347 [D loss: 0.603249, acc.: 69.53%] [G loss: 1.152709]\n",
      "epoch:7 step:7348 [D loss: 0.610326, acc.: 63.28%] [G loss: 1.151697]\n",
      "epoch:7 step:7349 [D loss: 0.622433, acc.: 65.62%] [G loss: 0.997732]\n",
      "epoch:7 step:7350 [D loss: 0.631542, acc.: 62.50%] [G loss: 1.242688]\n",
      "epoch:7 step:7351 [D loss: 0.627275, acc.: 60.16%] [G loss: 1.291118]\n",
      "epoch:7 step:7352 [D loss: 0.581904, acc.: 73.44%] [G loss: 1.206784]\n",
      "epoch:7 step:7353 [D loss: 0.554855, acc.: 74.22%] [G loss: 1.290283]\n",
      "epoch:7 step:7354 [D loss: 0.627101, acc.: 62.50%] [G loss: 1.089609]\n",
      "epoch:7 step:7355 [D loss: 0.573929, acc.: 68.75%] [G loss: 1.064293]\n",
      "epoch:7 step:7356 [D loss: 0.707458, acc.: 53.91%] [G loss: 1.146999]\n",
      "epoch:7 step:7357 [D loss: 0.652436, acc.: 67.19%] [G loss: 0.997854]\n",
      "epoch:7 step:7358 [D loss: 0.607779, acc.: 70.31%] [G loss: 1.146315]\n",
      "epoch:7 step:7359 [D loss: 0.670740, acc.: 58.59%] [G loss: 0.889084]\n",
      "epoch:7 step:7360 [D loss: 0.547763, acc.: 71.88%] [G loss: 1.045149]\n",
      "epoch:7 step:7361 [D loss: 0.679312, acc.: 62.50%] [G loss: 0.939739]\n",
      "epoch:7 step:7362 [D loss: 0.538826, acc.: 77.34%] [G loss: 1.246077]\n",
      "epoch:7 step:7363 [D loss: 0.659172, acc.: 60.94%] [G loss: 1.179307]\n",
      "epoch:7 step:7364 [D loss: 0.657052, acc.: 60.16%] [G loss: 1.132979]\n",
      "epoch:7 step:7365 [D loss: 0.566041, acc.: 73.44%] [G loss: 0.981297]\n",
      "epoch:7 step:7366 [D loss: 0.555948, acc.: 71.09%] [G loss: 1.221541]\n",
      "epoch:7 step:7367 [D loss: 0.496551, acc.: 84.38%] [G loss: 1.176184]\n",
      "epoch:7 step:7368 [D loss: 0.621317, acc.: 64.84%] [G loss: 1.020848]\n",
      "epoch:7 step:7369 [D loss: 0.579864, acc.: 71.09%] [G loss: 1.038730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7370 [D loss: 0.618488, acc.: 67.19%] [G loss: 1.097810]\n",
      "epoch:7 step:7371 [D loss: 0.598034, acc.: 68.75%] [G loss: 1.221869]\n",
      "epoch:7 step:7372 [D loss: 0.608524, acc.: 64.06%] [G loss: 1.288282]\n",
      "epoch:7 step:7373 [D loss: 0.645580, acc.: 63.28%] [G loss: 1.235018]\n",
      "epoch:7 step:7374 [D loss: 0.586437, acc.: 71.09%] [G loss: 1.330786]\n",
      "epoch:7 step:7375 [D loss: 0.552977, acc.: 75.00%] [G loss: 1.273471]\n",
      "epoch:7 step:7376 [D loss: 0.640207, acc.: 64.06%] [G loss: 0.956471]\n",
      "epoch:7 step:7377 [D loss: 0.615558, acc.: 67.97%] [G loss: 1.196624]\n",
      "epoch:7 step:7378 [D loss: 0.549549, acc.: 70.31%] [G loss: 1.211340]\n",
      "epoch:7 step:7379 [D loss: 0.657496, acc.: 62.50%] [G loss: 1.223079]\n",
      "epoch:7 step:7380 [D loss: 0.700531, acc.: 55.47%] [G loss: 1.138471]\n",
      "epoch:7 step:7381 [D loss: 0.609496, acc.: 67.19%] [G loss: 1.112312]\n",
      "epoch:7 step:7382 [D loss: 0.646393, acc.: 60.94%] [G loss: 1.188611]\n",
      "epoch:7 step:7383 [D loss: 0.728705, acc.: 50.00%] [G loss: 1.108817]\n",
      "epoch:7 step:7384 [D loss: 0.647905, acc.: 61.72%] [G loss: 1.144315]\n",
      "epoch:7 step:7385 [D loss: 0.569461, acc.: 75.00%] [G loss: 1.083241]\n",
      "epoch:7 step:7386 [D loss: 0.625597, acc.: 67.97%] [G loss: 1.181214]\n",
      "epoch:7 step:7387 [D loss: 0.778814, acc.: 49.22%] [G loss: 1.035504]\n",
      "epoch:7 step:7388 [D loss: 0.584865, acc.: 70.31%] [G loss: 1.267587]\n",
      "epoch:7 step:7389 [D loss: 0.741987, acc.: 53.12%] [G loss: 1.068121]\n",
      "epoch:7 step:7390 [D loss: 0.700970, acc.: 54.69%] [G loss: 1.163782]\n",
      "epoch:7 step:7391 [D loss: 0.521008, acc.: 78.91%] [G loss: 1.263780]\n",
      "epoch:7 step:7392 [D loss: 0.740904, acc.: 53.12%] [G loss: 0.997182]\n",
      "epoch:7 step:7393 [D loss: 0.658221, acc.: 64.06%] [G loss: 1.310668]\n",
      "epoch:7 step:7394 [D loss: 0.657146, acc.: 62.50%] [G loss: 1.219962]\n",
      "epoch:7 step:7395 [D loss: 0.603197, acc.: 67.97%] [G loss: 1.204665]\n",
      "epoch:7 step:7396 [D loss: 0.715713, acc.: 51.56%] [G loss: 1.041204]\n",
      "epoch:7 step:7397 [D loss: 0.638106, acc.: 60.94%] [G loss: 1.150153]\n",
      "epoch:7 step:7398 [D loss: 0.647083, acc.: 62.50%] [G loss: 1.075063]\n",
      "epoch:7 step:7399 [D loss: 0.635882, acc.: 65.62%] [G loss: 1.089189]\n",
      "epoch:7 step:7400 [D loss: 0.577305, acc.: 71.09%] [G loss: 1.281172]\n",
      "##############\n",
      "[2.6985577  2.07401492 1.76309851 2.40554409 0.68786091 5.79482545\n",
      " 2.06786754 2.53383858 3.92719746 8.14868929]\n",
      "##########\n",
      "epoch:7 step:7401 [D loss: 0.677669, acc.: 58.59%] [G loss: 1.263837]\n",
      "epoch:7 step:7402 [D loss: 0.549555, acc.: 74.22%] [G loss: 1.177521]\n",
      "epoch:7 step:7403 [D loss: 0.715974, acc.: 54.69%] [G loss: 1.132391]\n",
      "epoch:7 step:7404 [D loss: 0.702646, acc.: 53.91%] [G loss: 1.258392]\n",
      "epoch:7 step:7405 [D loss: 0.606699, acc.: 67.19%] [G loss: 0.984444]\n",
      "epoch:7 step:7406 [D loss: 0.546224, acc.: 72.66%] [G loss: 0.995722]\n",
      "epoch:7 step:7407 [D loss: 0.657858, acc.: 54.69%] [G loss: 1.075039]\n",
      "epoch:7 step:7408 [D loss: 0.606405, acc.: 66.41%] [G loss: 1.213169]\n",
      "epoch:7 step:7409 [D loss: 0.584605, acc.: 75.00%] [G loss: 1.161502]\n",
      "epoch:7 step:7410 [D loss: 0.619035, acc.: 64.06%] [G loss: 1.267558]\n",
      "epoch:7 step:7411 [D loss: 0.532153, acc.: 75.78%] [G loss: 1.240564]\n",
      "epoch:7 step:7412 [D loss: 0.538817, acc.: 73.44%] [G loss: 1.295863]\n",
      "epoch:7 step:7413 [D loss: 0.570653, acc.: 73.44%] [G loss: 1.322884]\n",
      "epoch:7 step:7414 [D loss: 0.685763, acc.: 54.69%] [G loss: 0.988865]\n",
      "epoch:7 step:7415 [D loss: 0.543772, acc.: 73.44%] [G loss: 1.105859]\n",
      "epoch:7 step:7416 [D loss: 0.603620, acc.: 66.41%] [G loss: 1.092379]\n",
      "epoch:7 step:7417 [D loss: 0.561982, acc.: 73.44%] [G loss: 1.092425]\n",
      "epoch:7 step:7418 [D loss: 0.524406, acc.: 75.78%] [G loss: 1.257835]\n",
      "epoch:7 step:7419 [D loss: 0.623997, acc.: 70.31%] [G loss: 0.918563]\n",
      "epoch:7 step:7420 [D loss: 0.556515, acc.: 68.75%] [G loss: 1.198029]\n",
      "epoch:7 step:7421 [D loss: 0.636004, acc.: 64.06%] [G loss: 0.914663]\n",
      "epoch:7 step:7422 [D loss: 0.571492, acc.: 69.53%] [G loss: 1.075544]\n",
      "epoch:7 step:7423 [D loss: 0.761035, acc.: 47.66%] [G loss: 1.069220]\n",
      "epoch:7 step:7424 [D loss: 0.677815, acc.: 65.62%] [G loss: 1.165476]\n",
      "epoch:7 step:7425 [D loss: 0.554228, acc.: 75.78%] [G loss: 1.052029]\n",
      "epoch:7 step:7426 [D loss: 0.582006, acc.: 68.75%] [G loss: 1.194825]\n",
      "epoch:7 step:7427 [D loss: 0.548513, acc.: 69.53%] [G loss: 1.108455]\n",
      "epoch:7 step:7428 [D loss: 0.527815, acc.: 75.00%] [G loss: 1.418169]\n",
      "epoch:7 step:7429 [D loss: 0.623686, acc.: 60.94%] [G loss: 1.157021]\n",
      "epoch:7 step:7430 [D loss: 0.643416, acc.: 59.38%] [G loss: 1.033827]\n",
      "epoch:7 step:7431 [D loss: 0.738503, acc.: 49.22%] [G loss: 1.027849]\n",
      "epoch:7 step:7432 [D loss: 0.537611, acc.: 74.22%] [G loss: 1.219836]\n",
      "epoch:7 step:7433 [D loss: 0.590198, acc.: 64.84%] [G loss: 1.109892]\n",
      "epoch:7 step:7434 [D loss: 0.507268, acc.: 76.56%] [G loss: 1.131123]\n",
      "epoch:7 step:7435 [D loss: 0.678990, acc.: 62.50%] [G loss: 1.024925]\n",
      "epoch:7 step:7436 [D loss: 0.648022, acc.: 60.16%] [G loss: 0.928114]\n",
      "epoch:7 step:7437 [D loss: 0.623635, acc.: 64.84%] [G loss: 0.975805]\n",
      "epoch:7 step:7438 [D loss: 0.673356, acc.: 57.03%] [G loss: 1.121917]\n",
      "epoch:7 step:7439 [D loss: 0.683965, acc.: 57.81%] [G loss: 1.028626]\n",
      "epoch:7 step:7440 [D loss: 0.654668, acc.: 63.28%] [G loss: 1.071451]\n",
      "epoch:7 step:7441 [D loss: 0.551379, acc.: 71.88%] [G loss: 1.103682]\n",
      "epoch:7 step:7442 [D loss: 0.587451, acc.: 67.19%] [G loss: 1.133943]\n",
      "epoch:7 step:7443 [D loss: 0.616446, acc.: 69.53%] [G loss: 1.193830]\n",
      "epoch:7 step:7444 [D loss: 0.554930, acc.: 74.22%] [G loss: 1.179785]\n",
      "epoch:7 step:7445 [D loss: 0.625884, acc.: 66.41%] [G loss: 1.045380]\n",
      "epoch:7 step:7446 [D loss: 0.600119, acc.: 72.66%] [G loss: 1.117541]\n",
      "epoch:7 step:7447 [D loss: 0.623693, acc.: 60.16%] [G loss: 0.939083]\n",
      "epoch:7 step:7448 [D loss: 0.616486, acc.: 67.97%] [G loss: 1.119754]\n",
      "epoch:7 step:7449 [D loss: 0.573638, acc.: 65.62%] [G loss: 1.276399]\n",
      "epoch:7 step:7450 [D loss: 0.734206, acc.: 53.12%] [G loss: 1.081646]\n",
      "epoch:7 step:7451 [D loss: 0.535803, acc.: 77.34%] [G loss: 1.264437]\n",
      "epoch:7 step:7452 [D loss: 0.591390, acc.: 67.19%] [G loss: 1.171345]\n",
      "epoch:7 step:7453 [D loss: 0.665894, acc.: 60.94%] [G loss: 1.094703]\n",
      "epoch:7 step:7454 [D loss: 0.524224, acc.: 71.09%] [G loss: 1.225617]\n",
      "epoch:7 step:7455 [D loss: 0.600093, acc.: 69.53%] [G loss: 1.078368]\n",
      "epoch:7 step:7456 [D loss: 0.537644, acc.: 75.00%] [G loss: 1.223957]\n",
      "epoch:7 step:7457 [D loss: 0.443249, acc.: 88.28%] [G loss: 1.257966]\n",
      "epoch:7 step:7458 [D loss: 0.588069, acc.: 67.97%] [G loss: 1.235009]\n",
      "epoch:7 step:7459 [D loss: 0.657246, acc.: 60.94%] [G loss: 1.154561]\n",
      "epoch:7 step:7460 [D loss: 0.584415, acc.: 72.66%] [G loss: 1.113855]\n",
      "epoch:7 step:7461 [D loss: 0.562252, acc.: 74.22%] [G loss: 1.190176]\n",
      "epoch:7 step:7462 [D loss: 0.591394, acc.: 69.53%] [G loss: 1.024215]\n",
      "epoch:7 step:7463 [D loss: 0.578431, acc.: 72.66%] [G loss: 1.111180]\n",
      "epoch:7 step:7464 [D loss: 0.634760, acc.: 61.72%] [G loss: 1.045338]\n",
      "epoch:7 step:7465 [D loss: 0.607276, acc.: 69.53%] [G loss: 1.115556]\n",
      "epoch:7 step:7466 [D loss: 0.570852, acc.: 69.53%] [G loss: 1.101233]\n",
      "epoch:7 step:7467 [D loss: 0.769317, acc.: 51.56%] [G loss: 1.066076]\n",
      "epoch:7 step:7468 [D loss: 0.590830, acc.: 70.31%] [G loss: 1.349889]\n",
      "epoch:7 step:7469 [D loss: 0.670956, acc.: 66.41%] [G loss: 0.890373]\n",
      "epoch:7 step:7470 [D loss: 0.703589, acc.: 54.69%] [G loss: 0.854326]\n",
      "epoch:7 step:7471 [D loss: 0.639146, acc.: 62.50%] [G loss: 1.279505]\n",
      "epoch:7 step:7472 [D loss: 0.664259, acc.: 59.38%] [G loss: 0.979713]\n",
      "epoch:7 step:7473 [D loss: 0.532793, acc.: 74.22%] [G loss: 1.320700]\n",
      "epoch:7 step:7474 [D loss: 0.590195, acc.: 67.19%] [G loss: 1.118973]\n",
      "epoch:7 step:7475 [D loss: 0.602757, acc.: 67.97%] [G loss: 1.054498]\n",
      "epoch:7 step:7476 [D loss: 0.615768, acc.: 64.06%] [G loss: 1.158316]\n",
      "epoch:7 step:7477 [D loss: 0.477761, acc.: 80.47%] [G loss: 1.212887]\n",
      "epoch:7 step:7478 [D loss: 0.558523, acc.: 73.44%] [G loss: 1.102201]\n",
      "epoch:7 step:7479 [D loss: 0.562172, acc.: 68.75%] [G loss: 1.172382]\n",
      "epoch:7 step:7480 [D loss: 0.652526, acc.: 66.41%] [G loss: 0.999910]\n",
      "epoch:7 step:7481 [D loss: 0.515431, acc.: 76.56%] [G loss: 1.170025]\n",
      "epoch:7 step:7482 [D loss: 0.581820, acc.: 69.53%] [G loss: 1.082262]\n",
      "epoch:7 step:7483 [D loss: 0.609281, acc.: 64.84%] [G loss: 1.147478]\n",
      "epoch:7 step:7484 [D loss: 0.569945, acc.: 68.75%] [G loss: 1.244076]\n",
      "epoch:7 step:7485 [D loss: 0.574214, acc.: 70.31%] [G loss: 0.938896]\n",
      "epoch:7 step:7486 [D loss: 0.663503, acc.: 65.62%] [G loss: 1.184296]\n",
      "epoch:7 step:7487 [D loss: 0.652282, acc.: 61.72%] [G loss: 1.126616]\n",
      "epoch:7 step:7488 [D loss: 0.628371, acc.: 63.28%] [G loss: 1.047183]\n",
      "epoch:7 step:7489 [D loss: 0.695514, acc.: 61.72%] [G loss: 1.125385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7490 [D loss: 0.626445, acc.: 67.19%] [G loss: 1.005286]\n",
      "epoch:7 step:7491 [D loss: 0.570229, acc.: 71.88%] [G loss: 1.209330]\n",
      "epoch:7 step:7492 [D loss: 0.629849, acc.: 64.84%] [G loss: 1.020324]\n",
      "epoch:7 step:7493 [D loss: 0.648454, acc.: 64.06%] [G loss: 1.108862]\n",
      "epoch:7 step:7494 [D loss: 0.607328, acc.: 62.50%] [G loss: 1.215578]\n",
      "epoch:7 step:7495 [D loss: 0.605257, acc.: 67.19%] [G loss: 1.200632]\n",
      "epoch:7 step:7496 [D loss: 0.626673, acc.: 64.84%] [G loss: 1.198539]\n",
      "epoch:8 step:7497 [D loss: 0.615226, acc.: 64.84%] [G loss: 1.001568]\n",
      "epoch:8 step:7498 [D loss: 0.715558, acc.: 54.69%] [G loss: 1.104747]\n",
      "epoch:8 step:7499 [D loss: 0.687844, acc.: 60.16%] [G loss: 1.086892]\n",
      "epoch:8 step:7500 [D loss: 0.606809, acc.: 68.75%] [G loss: 1.066950]\n",
      "epoch:8 step:7501 [D loss: 0.631130, acc.: 63.28%] [G loss: 1.137142]\n",
      "epoch:8 step:7502 [D loss: 0.748336, acc.: 55.47%] [G loss: 1.128427]\n",
      "epoch:8 step:7503 [D loss: 0.744060, acc.: 53.91%] [G loss: 1.256624]\n",
      "epoch:8 step:7504 [D loss: 0.626109, acc.: 67.97%] [G loss: 1.179364]\n",
      "epoch:8 step:7505 [D loss: 0.582810, acc.: 67.19%] [G loss: 1.022894]\n",
      "epoch:8 step:7506 [D loss: 0.603108, acc.: 69.53%] [G loss: 1.135299]\n",
      "epoch:8 step:7507 [D loss: 0.521151, acc.: 74.22%] [G loss: 1.279773]\n",
      "epoch:8 step:7508 [D loss: 0.628384, acc.: 62.50%] [G loss: 1.082828]\n",
      "epoch:8 step:7509 [D loss: 0.644583, acc.: 64.06%] [G loss: 0.988303]\n",
      "epoch:8 step:7510 [D loss: 0.672605, acc.: 58.59%] [G loss: 1.143254]\n",
      "epoch:8 step:7511 [D loss: 0.596892, acc.: 68.75%] [G loss: 1.169250]\n",
      "epoch:8 step:7512 [D loss: 0.603499, acc.: 65.62%] [G loss: 0.952468]\n",
      "epoch:8 step:7513 [D loss: 0.526833, acc.: 76.56%] [G loss: 1.116721]\n",
      "epoch:8 step:7514 [D loss: 0.518139, acc.: 76.56%] [G loss: 1.194478]\n",
      "epoch:8 step:7515 [D loss: 0.717796, acc.: 52.34%] [G loss: 1.015909]\n",
      "epoch:8 step:7516 [D loss: 0.627773, acc.: 72.66%] [G loss: 0.992492]\n",
      "epoch:8 step:7517 [D loss: 0.665084, acc.: 60.16%] [G loss: 1.052341]\n",
      "epoch:8 step:7518 [D loss: 0.529418, acc.: 78.12%] [G loss: 1.154124]\n",
      "epoch:8 step:7519 [D loss: 0.660076, acc.: 57.81%] [G loss: 1.160131]\n",
      "epoch:8 step:7520 [D loss: 0.531613, acc.: 73.44%] [G loss: 1.158021]\n",
      "epoch:8 step:7521 [D loss: 0.658596, acc.: 63.28%] [G loss: 1.193993]\n",
      "epoch:8 step:7522 [D loss: 0.628344, acc.: 61.72%] [G loss: 1.092278]\n",
      "epoch:8 step:7523 [D loss: 0.577698, acc.: 69.53%] [G loss: 1.143321]\n",
      "epoch:8 step:7524 [D loss: 0.543052, acc.: 78.12%] [G loss: 1.165854]\n",
      "epoch:8 step:7525 [D loss: 0.651986, acc.: 59.38%] [G loss: 1.038074]\n",
      "epoch:8 step:7526 [D loss: 0.633944, acc.: 66.41%] [G loss: 1.053731]\n",
      "epoch:8 step:7527 [D loss: 0.608380, acc.: 64.06%] [G loss: 1.007858]\n",
      "epoch:8 step:7528 [D loss: 0.652177, acc.: 64.06%] [G loss: 0.887405]\n",
      "epoch:8 step:7529 [D loss: 0.540957, acc.: 71.88%] [G loss: 1.171083]\n",
      "epoch:8 step:7530 [D loss: 0.673829, acc.: 58.59%] [G loss: 1.173459]\n",
      "epoch:8 step:7531 [D loss: 0.667039, acc.: 60.94%] [G loss: 1.289829]\n",
      "epoch:8 step:7532 [D loss: 0.532727, acc.: 73.44%] [G loss: 1.270147]\n",
      "epoch:8 step:7533 [D loss: 0.671696, acc.: 59.38%] [G loss: 1.036107]\n",
      "epoch:8 step:7534 [D loss: 0.675505, acc.: 57.03%] [G loss: 1.268570]\n",
      "epoch:8 step:7535 [D loss: 0.629990, acc.: 64.84%] [G loss: 1.042056]\n",
      "epoch:8 step:7536 [D loss: 0.659643, acc.: 62.50%] [G loss: 1.260741]\n",
      "epoch:8 step:7537 [D loss: 0.642035, acc.: 62.50%] [G loss: 1.226373]\n",
      "epoch:8 step:7538 [D loss: 0.608957, acc.: 67.19%] [G loss: 1.320981]\n",
      "epoch:8 step:7539 [D loss: 0.564180, acc.: 70.31%] [G loss: 1.351692]\n",
      "epoch:8 step:7540 [D loss: 0.694960, acc.: 53.12%] [G loss: 1.028514]\n",
      "epoch:8 step:7541 [D loss: 0.598469, acc.: 66.41%] [G loss: 1.182369]\n",
      "epoch:8 step:7542 [D loss: 0.610786, acc.: 66.41%] [G loss: 1.095386]\n",
      "epoch:8 step:7543 [D loss: 0.620932, acc.: 71.09%] [G loss: 1.079698]\n",
      "epoch:8 step:7544 [D loss: 0.691278, acc.: 57.81%] [G loss: 1.072981]\n",
      "epoch:8 step:7545 [D loss: 0.578776, acc.: 71.09%] [G loss: 1.023890]\n",
      "epoch:8 step:7546 [D loss: 0.537933, acc.: 77.34%] [G loss: 1.034441]\n",
      "epoch:8 step:7547 [D loss: 0.639125, acc.: 68.75%] [G loss: 0.972661]\n",
      "epoch:8 step:7548 [D loss: 0.593458, acc.: 68.75%] [G loss: 1.023792]\n",
      "epoch:8 step:7549 [D loss: 0.655711, acc.: 63.28%] [G loss: 1.046026]\n",
      "epoch:8 step:7550 [D loss: 0.518563, acc.: 80.47%] [G loss: 1.291354]\n",
      "epoch:8 step:7551 [D loss: 0.673438, acc.: 60.94%] [G loss: 1.094916]\n",
      "epoch:8 step:7552 [D loss: 0.676224, acc.: 60.16%] [G loss: 1.010355]\n",
      "epoch:8 step:7553 [D loss: 0.696908, acc.: 64.06%] [G loss: 1.132095]\n",
      "epoch:8 step:7554 [D loss: 0.574979, acc.: 75.78%] [G loss: 1.044778]\n",
      "epoch:8 step:7555 [D loss: 0.517876, acc.: 75.78%] [G loss: 1.293198]\n",
      "epoch:8 step:7556 [D loss: 0.611145, acc.: 64.84%] [G loss: 1.266079]\n",
      "epoch:8 step:7557 [D loss: 0.722998, acc.: 54.69%] [G loss: 1.067995]\n",
      "epoch:8 step:7558 [D loss: 0.595458, acc.: 70.31%] [G loss: 1.094481]\n",
      "epoch:8 step:7559 [D loss: 0.552530, acc.: 73.44%] [G loss: 1.111526]\n",
      "epoch:8 step:7560 [D loss: 0.704788, acc.: 54.69%] [G loss: 1.061897]\n",
      "epoch:8 step:7561 [D loss: 0.606295, acc.: 69.53%] [G loss: 1.146261]\n",
      "epoch:8 step:7562 [D loss: 0.659616, acc.: 63.28%] [G loss: 1.053304]\n",
      "epoch:8 step:7563 [D loss: 0.677639, acc.: 55.47%] [G loss: 1.140580]\n",
      "epoch:8 step:7564 [D loss: 0.689547, acc.: 57.03%] [G loss: 1.117265]\n",
      "epoch:8 step:7565 [D loss: 0.549960, acc.: 74.22%] [G loss: 1.167467]\n",
      "epoch:8 step:7566 [D loss: 0.655523, acc.: 61.72%] [G loss: 1.092654]\n",
      "epoch:8 step:7567 [D loss: 0.642686, acc.: 64.06%] [G loss: 1.139920]\n",
      "epoch:8 step:7568 [D loss: 0.557541, acc.: 70.31%] [G loss: 1.219654]\n",
      "epoch:8 step:7569 [D loss: 0.667349, acc.: 60.94%] [G loss: 1.096081]\n",
      "epoch:8 step:7570 [D loss: 0.560056, acc.: 69.53%] [G loss: 1.419812]\n",
      "epoch:8 step:7571 [D loss: 0.617439, acc.: 61.72%] [G loss: 1.067056]\n",
      "epoch:8 step:7572 [D loss: 0.521095, acc.: 71.09%] [G loss: 1.076855]\n",
      "epoch:8 step:7573 [D loss: 0.567313, acc.: 71.88%] [G loss: 1.175546]\n",
      "epoch:8 step:7574 [D loss: 0.748368, acc.: 54.69%] [G loss: 0.948895]\n",
      "epoch:8 step:7575 [D loss: 0.585941, acc.: 72.66%] [G loss: 1.240938]\n",
      "epoch:8 step:7576 [D loss: 0.631962, acc.: 62.50%] [G loss: 1.186953]\n",
      "epoch:8 step:7577 [D loss: 0.570434, acc.: 69.53%] [G loss: 1.052929]\n",
      "epoch:8 step:7578 [D loss: 0.538786, acc.: 75.78%] [G loss: 1.392706]\n",
      "epoch:8 step:7579 [D loss: 0.755759, acc.: 55.47%] [G loss: 1.036354]\n",
      "epoch:8 step:7580 [D loss: 0.583980, acc.: 68.75%] [G loss: 1.127158]\n",
      "epoch:8 step:7581 [D loss: 0.553955, acc.: 72.66%] [G loss: 1.101981]\n",
      "epoch:8 step:7582 [D loss: 0.698520, acc.: 53.91%] [G loss: 1.136156]\n",
      "epoch:8 step:7583 [D loss: 0.645267, acc.: 65.62%] [G loss: 0.947366]\n",
      "epoch:8 step:7584 [D loss: 0.592027, acc.: 68.75%] [G loss: 0.968383]\n",
      "epoch:8 step:7585 [D loss: 0.749937, acc.: 47.66%] [G loss: 1.129344]\n",
      "epoch:8 step:7586 [D loss: 0.549754, acc.: 72.66%] [G loss: 1.118950]\n",
      "epoch:8 step:7587 [D loss: 0.606665, acc.: 66.41%] [G loss: 1.027798]\n",
      "epoch:8 step:7588 [D loss: 0.628787, acc.: 61.72%] [G loss: 0.909944]\n",
      "epoch:8 step:7589 [D loss: 0.608026, acc.: 67.97%] [G loss: 1.284629]\n",
      "epoch:8 step:7590 [D loss: 0.636238, acc.: 60.94%] [G loss: 1.370403]\n",
      "epoch:8 step:7591 [D loss: 0.683089, acc.: 60.16%] [G loss: 0.972653]\n",
      "epoch:8 step:7592 [D loss: 0.594286, acc.: 68.75%] [G loss: 1.139360]\n",
      "epoch:8 step:7593 [D loss: 0.675031, acc.: 65.62%] [G loss: 1.016480]\n",
      "epoch:8 step:7594 [D loss: 0.617399, acc.: 65.62%] [G loss: 1.093317]\n",
      "epoch:8 step:7595 [D loss: 0.561607, acc.: 77.34%] [G loss: 1.029629]\n",
      "epoch:8 step:7596 [D loss: 0.561233, acc.: 71.88%] [G loss: 1.015188]\n",
      "epoch:8 step:7597 [D loss: 0.529066, acc.: 71.88%] [G loss: 1.085157]\n",
      "epoch:8 step:7598 [D loss: 0.629040, acc.: 65.62%] [G loss: 1.123676]\n",
      "epoch:8 step:7599 [D loss: 0.622382, acc.: 65.62%] [G loss: 0.973431]\n",
      "epoch:8 step:7600 [D loss: 0.742352, acc.: 49.22%] [G loss: 1.021290]\n",
      "##############\n",
      "[2.6647553  2.19718317 2.22030191 3.03830203 1.00168249 6.77597808\n",
      " 2.20300081 3.36376959 3.96672709 6.05585637]\n",
      "##########\n",
      "epoch:8 step:7601 [D loss: 0.576257, acc.: 67.97%] [G loss: 1.047122]\n",
      "epoch:8 step:7602 [D loss: 0.625830, acc.: 67.19%] [G loss: 1.256182]\n",
      "epoch:8 step:7603 [D loss: 0.583847, acc.: 66.41%] [G loss: 1.267759]\n",
      "epoch:8 step:7604 [D loss: 0.704580, acc.: 53.12%] [G loss: 1.172908]\n",
      "epoch:8 step:7605 [D loss: 0.542402, acc.: 73.44%] [G loss: 1.198524]\n",
      "epoch:8 step:7606 [D loss: 0.584976, acc.: 67.97%] [G loss: 1.264897]\n",
      "epoch:8 step:7607 [D loss: 0.637008, acc.: 62.50%] [G loss: 1.187488]\n",
      "epoch:8 step:7608 [D loss: 0.607554, acc.: 66.41%] [G loss: 1.241666]\n",
      "epoch:8 step:7609 [D loss: 0.573753, acc.: 67.19%] [G loss: 1.115466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7610 [D loss: 0.577825, acc.: 70.31%] [G loss: 1.032386]\n",
      "epoch:8 step:7611 [D loss: 0.635736, acc.: 62.50%] [G loss: 1.229680]\n",
      "epoch:8 step:7612 [D loss: 0.705583, acc.: 56.25%] [G loss: 1.134990]\n",
      "epoch:8 step:7613 [D loss: 0.654853, acc.: 64.06%] [G loss: 0.946367]\n",
      "epoch:8 step:7614 [D loss: 0.574553, acc.: 71.88%] [G loss: 1.039876]\n",
      "epoch:8 step:7615 [D loss: 0.571908, acc.: 71.88%] [G loss: 1.051952]\n",
      "epoch:8 step:7616 [D loss: 0.663605, acc.: 64.84%] [G loss: 1.092043]\n",
      "epoch:8 step:7617 [D loss: 0.662114, acc.: 61.72%] [G loss: 0.997600]\n",
      "epoch:8 step:7618 [D loss: 0.579154, acc.: 69.53%] [G loss: 1.130193]\n",
      "epoch:8 step:7619 [D loss: 0.531731, acc.: 77.34%] [G loss: 1.359993]\n",
      "epoch:8 step:7620 [D loss: 0.627640, acc.: 63.28%] [G loss: 1.223072]\n",
      "epoch:8 step:7621 [D loss: 0.513677, acc.: 73.44%] [G loss: 1.220692]\n",
      "epoch:8 step:7622 [D loss: 0.634959, acc.: 65.62%] [G loss: 1.155386]\n",
      "epoch:8 step:7623 [D loss: 0.669409, acc.: 63.28%] [G loss: 1.080092]\n",
      "epoch:8 step:7624 [D loss: 0.624021, acc.: 66.41%] [G loss: 1.176883]\n",
      "epoch:8 step:7625 [D loss: 0.737700, acc.: 48.44%] [G loss: 0.951596]\n",
      "epoch:8 step:7626 [D loss: 0.529258, acc.: 71.09%] [G loss: 1.180299]\n",
      "epoch:8 step:7627 [D loss: 0.645820, acc.: 64.06%] [G loss: 1.089209]\n",
      "epoch:8 step:7628 [D loss: 0.522687, acc.: 73.44%] [G loss: 1.211018]\n",
      "epoch:8 step:7629 [D loss: 0.683582, acc.: 54.69%] [G loss: 1.110195]\n",
      "epoch:8 step:7630 [D loss: 0.652955, acc.: 55.47%] [G loss: 0.922847]\n",
      "epoch:8 step:7631 [D loss: 0.588394, acc.: 68.75%] [G loss: 1.249977]\n",
      "epoch:8 step:7632 [D loss: 0.704772, acc.: 58.59%] [G loss: 1.095420]\n",
      "epoch:8 step:7633 [D loss: 0.658297, acc.: 57.81%] [G loss: 0.991103]\n",
      "epoch:8 step:7634 [D loss: 0.601624, acc.: 63.28%] [G loss: 1.147946]\n",
      "epoch:8 step:7635 [D loss: 0.653204, acc.: 58.59%] [G loss: 1.213511]\n",
      "epoch:8 step:7636 [D loss: 0.650724, acc.: 64.06%] [G loss: 1.024855]\n",
      "epoch:8 step:7637 [D loss: 0.756424, acc.: 50.78%] [G loss: 1.165607]\n",
      "epoch:8 step:7638 [D loss: 0.533883, acc.: 74.22%] [G loss: 1.262097]\n",
      "epoch:8 step:7639 [D loss: 0.661854, acc.: 62.50%] [G loss: 1.117433]\n",
      "epoch:8 step:7640 [D loss: 0.684744, acc.: 56.25%] [G loss: 1.033343]\n",
      "epoch:8 step:7641 [D loss: 0.618980, acc.: 62.50%] [G loss: 1.113691]\n",
      "epoch:8 step:7642 [D loss: 0.613499, acc.: 68.75%] [G loss: 1.057890]\n",
      "epoch:8 step:7643 [D loss: 0.576056, acc.: 70.31%] [G loss: 1.004343]\n",
      "epoch:8 step:7644 [D loss: 0.675451, acc.: 60.16%] [G loss: 1.058108]\n",
      "epoch:8 step:7645 [D loss: 0.506633, acc.: 81.25%] [G loss: 1.397910]\n",
      "epoch:8 step:7646 [D loss: 0.517079, acc.: 77.34%] [G loss: 1.138929]\n",
      "epoch:8 step:7647 [D loss: 0.572350, acc.: 72.66%] [G loss: 1.064544]\n",
      "epoch:8 step:7648 [D loss: 0.600076, acc.: 68.75%] [G loss: 0.989971]\n",
      "epoch:8 step:7649 [D loss: 0.556613, acc.: 67.97%] [G loss: 1.007801]\n",
      "epoch:8 step:7650 [D loss: 0.571209, acc.: 71.88%] [G loss: 1.184557]\n",
      "epoch:8 step:7651 [D loss: 0.662380, acc.: 64.84%] [G loss: 1.169698]\n",
      "epoch:8 step:7652 [D loss: 0.642499, acc.: 68.75%] [G loss: 1.267942]\n",
      "epoch:8 step:7653 [D loss: 0.634820, acc.: 60.94%] [G loss: 1.129898]\n",
      "epoch:8 step:7654 [D loss: 0.515894, acc.: 78.91%] [G loss: 1.314160]\n",
      "epoch:8 step:7655 [D loss: 0.541875, acc.: 73.44%] [G loss: 0.940096]\n",
      "epoch:8 step:7656 [D loss: 0.676757, acc.: 62.50%] [G loss: 1.228088]\n",
      "epoch:8 step:7657 [D loss: 0.502903, acc.: 79.69%] [G loss: 1.049379]\n",
      "epoch:8 step:7658 [D loss: 0.644080, acc.: 65.62%] [G loss: 1.015255]\n",
      "epoch:8 step:7659 [D loss: 0.591357, acc.: 68.75%] [G loss: 1.131080]\n",
      "epoch:8 step:7660 [D loss: 0.625654, acc.: 67.97%] [G loss: 1.164660]\n",
      "epoch:8 step:7661 [D loss: 0.718894, acc.: 52.34%] [G loss: 0.942629]\n",
      "epoch:8 step:7662 [D loss: 0.600687, acc.: 74.22%] [G loss: 1.311793]\n",
      "epoch:8 step:7663 [D loss: 0.638089, acc.: 61.72%] [G loss: 1.128561]\n",
      "epoch:8 step:7664 [D loss: 0.694957, acc.: 60.94%] [G loss: 1.093538]\n",
      "epoch:8 step:7665 [D loss: 0.583763, acc.: 67.97%] [G loss: 1.304500]\n",
      "epoch:8 step:7666 [D loss: 0.559364, acc.: 71.09%] [G loss: 1.293862]\n",
      "epoch:8 step:7667 [D loss: 0.638922, acc.: 66.41%] [G loss: 1.128511]\n",
      "epoch:8 step:7668 [D loss: 0.605175, acc.: 66.41%] [G loss: 1.037928]\n",
      "epoch:8 step:7669 [D loss: 0.631032, acc.: 67.97%] [G loss: 1.187080]\n",
      "epoch:8 step:7670 [D loss: 0.639960, acc.: 65.62%] [G loss: 1.076536]\n",
      "epoch:8 step:7671 [D loss: 0.635480, acc.: 62.50%] [G loss: 1.209163]\n",
      "epoch:8 step:7672 [D loss: 0.578133, acc.: 67.19%] [G loss: 1.054301]\n",
      "epoch:8 step:7673 [D loss: 0.654323, acc.: 58.59%] [G loss: 1.071281]\n",
      "epoch:8 step:7674 [D loss: 0.575324, acc.: 74.22%] [G loss: 1.316548]\n",
      "epoch:8 step:7675 [D loss: 0.711807, acc.: 57.81%] [G loss: 1.127049]\n",
      "epoch:8 step:7676 [D loss: 0.617508, acc.: 65.62%] [G loss: 1.128219]\n",
      "epoch:8 step:7677 [D loss: 0.636814, acc.: 62.50%] [G loss: 1.096224]\n",
      "epoch:8 step:7678 [D loss: 0.579985, acc.: 68.75%] [G loss: 1.224045]\n",
      "epoch:8 step:7679 [D loss: 0.598975, acc.: 67.19%] [G loss: 1.220777]\n",
      "epoch:8 step:7680 [D loss: 0.628343, acc.: 63.28%] [G loss: 1.008657]\n",
      "epoch:8 step:7681 [D loss: 0.623178, acc.: 64.06%] [G loss: 1.187403]\n",
      "epoch:8 step:7682 [D loss: 0.549131, acc.: 76.56%] [G loss: 1.023731]\n",
      "epoch:8 step:7683 [D loss: 0.485660, acc.: 80.47%] [G loss: 1.359026]\n",
      "epoch:8 step:7684 [D loss: 0.670539, acc.: 60.16%] [G loss: 0.984374]\n",
      "epoch:8 step:7685 [D loss: 0.560477, acc.: 75.00%] [G loss: 1.406709]\n",
      "epoch:8 step:7686 [D loss: 0.693922, acc.: 63.28%] [G loss: 1.025537]\n",
      "epoch:8 step:7687 [D loss: 0.606420, acc.: 66.41%] [G loss: 0.991997]\n",
      "epoch:8 step:7688 [D loss: 0.673802, acc.: 57.81%] [G loss: 0.911323]\n",
      "epoch:8 step:7689 [D loss: 0.725753, acc.: 57.81%] [G loss: 0.990906]\n",
      "epoch:8 step:7690 [D loss: 0.570073, acc.: 68.75%] [G loss: 0.917995]\n",
      "epoch:8 step:7691 [D loss: 0.640230, acc.: 70.31%] [G loss: 1.218485]\n",
      "epoch:8 step:7692 [D loss: 0.590921, acc.: 67.97%] [G loss: 1.072116]\n",
      "epoch:8 step:7693 [D loss: 0.658448, acc.: 64.06%] [G loss: 1.013177]\n",
      "epoch:8 step:7694 [D loss: 0.627216, acc.: 63.28%] [G loss: 1.182686]\n",
      "epoch:8 step:7695 [D loss: 0.628995, acc.: 64.06%] [G loss: 0.989967]\n",
      "epoch:8 step:7696 [D loss: 0.551080, acc.: 75.00%] [G loss: 1.180865]\n",
      "epoch:8 step:7697 [D loss: 0.459294, acc.: 84.38%] [G loss: 1.182649]\n",
      "epoch:8 step:7698 [D loss: 0.580549, acc.: 68.75%] [G loss: 1.342360]\n",
      "epoch:8 step:7699 [D loss: 0.643561, acc.: 62.50%] [G loss: 1.129364]\n",
      "epoch:8 step:7700 [D loss: 0.673551, acc.: 57.03%] [G loss: 1.159279]\n",
      "epoch:8 step:7701 [D loss: 0.635673, acc.: 64.06%] [G loss: 1.217169]\n",
      "epoch:8 step:7702 [D loss: 0.604008, acc.: 67.97%] [G loss: 1.059822]\n",
      "epoch:8 step:7703 [D loss: 0.631977, acc.: 64.84%] [G loss: 1.063928]\n",
      "epoch:8 step:7704 [D loss: 0.477843, acc.: 80.47%] [G loss: 1.210339]\n",
      "epoch:8 step:7705 [D loss: 0.641017, acc.: 63.28%] [G loss: 1.196117]\n",
      "epoch:8 step:7706 [D loss: 0.533428, acc.: 73.44%] [G loss: 1.274877]\n",
      "epoch:8 step:7707 [D loss: 0.611425, acc.: 69.53%] [G loss: 1.066006]\n",
      "epoch:8 step:7708 [D loss: 0.563030, acc.: 72.66%] [G loss: 1.107014]\n",
      "epoch:8 step:7709 [D loss: 0.592467, acc.: 69.53%] [G loss: 0.986606]\n",
      "epoch:8 step:7710 [D loss: 0.620767, acc.: 62.50%] [G loss: 0.919464]\n",
      "epoch:8 step:7711 [D loss: 0.666000, acc.: 63.28%] [G loss: 0.950241]\n",
      "epoch:8 step:7712 [D loss: 0.606953, acc.: 70.31%] [G loss: 1.049747]\n",
      "epoch:8 step:7713 [D loss: 0.536313, acc.: 75.78%] [G loss: 0.994856]\n",
      "epoch:8 step:7714 [D loss: 0.629670, acc.: 58.59%] [G loss: 0.937846]\n",
      "epoch:8 step:7715 [D loss: 0.565297, acc.: 71.88%] [G loss: 1.386261]\n",
      "epoch:8 step:7716 [D loss: 0.636166, acc.: 64.06%] [G loss: 0.968265]\n",
      "epoch:8 step:7717 [D loss: 0.619943, acc.: 67.19%] [G loss: 1.166062]\n",
      "epoch:8 step:7718 [D loss: 0.559343, acc.: 73.44%] [G loss: 1.196170]\n",
      "epoch:8 step:7719 [D loss: 0.628983, acc.: 67.19%] [G loss: 1.033082]\n",
      "epoch:8 step:7720 [D loss: 0.576323, acc.: 70.31%] [G loss: 1.284032]\n",
      "epoch:8 step:7721 [D loss: 0.650998, acc.: 63.28%] [G loss: 1.195069]\n",
      "epoch:8 step:7722 [D loss: 0.686399, acc.: 58.59%] [G loss: 1.204362]\n",
      "epoch:8 step:7723 [D loss: 0.548412, acc.: 71.09%] [G loss: 1.315876]\n",
      "epoch:8 step:7724 [D loss: 0.554633, acc.: 71.88%] [G loss: 1.335716]\n",
      "epoch:8 step:7725 [D loss: 0.617265, acc.: 67.19%] [G loss: 1.029550]\n",
      "epoch:8 step:7726 [D loss: 0.550048, acc.: 71.88%] [G loss: 1.202030]\n",
      "epoch:8 step:7727 [D loss: 0.660069, acc.: 62.50%] [G loss: 1.056445]\n",
      "epoch:8 step:7728 [D loss: 0.704757, acc.: 54.69%] [G loss: 0.913885]\n",
      "epoch:8 step:7729 [D loss: 0.580449, acc.: 71.88%] [G loss: 1.051805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7730 [D loss: 0.554572, acc.: 76.56%] [G loss: 1.278627]\n",
      "epoch:8 step:7731 [D loss: 0.542137, acc.: 73.44%] [G loss: 1.324458]\n",
      "epoch:8 step:7732 [D loss: 0.619866, acc.: 64.84%] [G loss: 0.882138]\n",
      "epoch:8 step:7733 [D loss: 0.693703, acc.: 57.81%] [G loss: 1.222016]\n",
      "epoch:8 step:7734 [D loss: 0.617930, acc.: 67.19%] [G loss: 1.049391]\n",
      "epoch:8 step:7735 [D loss: 0.668607, acc.: 61.72%] [G loss: 1.062165]\n",
      "epoch:8 step:7736 [D loss: 0.661552, acc.: 60.16%] [G loss: 1.013561]\n",
      "epoch:8 step:7737 [D loss: 0.595114, acc.: 67.19%] [G loss: 1.178213]\n",
      "epoch:8 step:7738 [D loss: 0.697668, acc.: 57.03%] [G loss: 1.124659]\n",
      "epoch:8 step:7739 [D loss: 0.624011, acc.: 64.06%] [G loss: 1.074556]\n",
      "epoch:8 step:7740 [D loss: 0.627994, acc.: 68.75%] [G loss: 0.964521]\n",
      "epoch:8 step:7741 [D loss: 0.643801, acc.: 62.50%] [G loss: 1.116991]\n",
      "epoch:8 step:7742 [D loss: 0.638737, acc.: 61.72%] [G loss: 0.950677]\n",
      "epoch:8 step:7743 [D loss: 0.651471, acc.: 60.16%] [G loss: 1.014091]\n",
      "epoch:8 step:7744 [D loss: 0.604291, acc.: 67.19%] [G loss: 1.010873]\n",
      "epoch:8 step:7745 [D loss: 0.547106, acc.: 71.88%] [G loss: 1.199163]\n",
      "epoch:8 step:7746 [D loss: 0.568693, acc.: 71.88%] [G loss: 1.106972]\n",
      "epoch:8 step:7747 [D loss: 0.650881, acc.: 60.94%] [G loss: 1.034933]\n",
      "epoch:8 step:7748 [D loss: 0.505759, acc.: 79.69%] [G loss: 1.221547]\n",
      "epoch:8 step:7749 [D loss: 0.590092, acc.: 66.41%] [G loss: 1.283726]\n",
      "epoch:8 step:7750 [D loss: 0.677535, acc.: 62.50%] [G loss: 1.105342]\n",
      "epoch:8 step:7751 [D loss: 0.599394, acc.: 64.84%] [G loss: 1.149050]\n",
      "epoch:8 step:7752 [D loss: 0.637029, acc.: 58.59%] [G loss: 1.105261]\n",
      "epoch:8 step:7753 [D loss: 0.529079, acc.: 78.12%] [G loss: 1.113911]\n",
      "epoch:8 step:7754 [D loss: 0.588042, acc.: 71.88%] [G loss: 1.297397]\n",
      "epoch:8 step:7755 [D loss: 0.611377, acc.: 64.84%] [G loss: 1.085022]\n",
      "epoch:8 step:7756 [D loss: 0.494663, acc.: 79.69%] [G loss: 1.175149]\n",
      "epoch:8 step:7757 [D loss: 0.627953, acc.: 60.16%] [G loss: 1.032106]\n",
      "epoch:8 step:7758 [D loss: 0.654602, acc.: 57.81%] [G loss: 1.199779]\n",
      "epoch:8 step:7759 [D loss: 0.696511, acc.: 60.94%] [G loss: 1.195264]\n",
      "epoch:8 step:7760 [D loss: 0.531810, acc.: 76.56%] [G loss: 1.299603]\n",
      "epoch:8 step:7761 [D loss: 0.580360, acc.: 73.44%] [G loss: 1.385813]\n",
      "epoch:8 step:7762 [D loss: 0.564266, acc.: 71.09%] [G loss: 1.045582]\n",
      "epoch:8 step:7763 [D loss: 0.541620, acc.: 72.66%] [G loss: 1.325038]\n",
      "epoch:8 step:7764 [D loss: 0.536850, acc.: 78.91%] [G loss: 1.240512]\n",
      "epoch:8 step:7765 [D loss: 0.629600, acc.: 65.62%] [G loss: 1.075391]\n",
      "epoch:8 step:7766 [D loss: 0.574350, acc.: 67.19%] [G loss: 1.130726]\n",
      "epoch:8 step:7767 [D loss: 0.547006, acc.: 72.66%] [G loss: 1.287314]\n",
      "epoch:8 step:7768 [D loss: 0.596278, acc.: 67.97%] [G loss: 1.113447]\n",
      "epoch:8 step:7769 [D loss: 0.655165, acc.: 64.06%] [G loss: 1.089138]\n",
      "epoch:8 step:7770 [D loss: 0.623719, acc.: 65.62%] [G loss: 1.264220]\n",
      "epoch:8 step:7771 [D loss: 0.728993, acc.: 48.44%] [G loss: 0.950628]\n",
      "epoch:8 step:7772 [D loss: 0.740675, acc.: 50.00%] [G loss: 1.145028]\n",
      "epoch:8 step:7773 [D loss: 0.622310, acc.: 63.28%] [G loss: 1.066292]\n",
      "epoch:8 step:7774 [D loss: 0.544218, acc.: 74.22%] [G loss: 1.357910]\n",
      "epoch:8 step:7775 [D loss: 0.684651, acc.: 55.47%] [G loss: 1.054733]\n",
      "epoch:8 step:7776 [D loss: 0.666702, acc.: 57.03%] [G loss: 1.178634]\n",
      "epoch:8 step:7777 [D loss: 0.577469, acc.: 68.75%] [G loss: 1.087989]\n",
      "epoch:8 step:7778 [D loss: 0.545349, acc.: 69.53%] [G loss: 1.256791]\n",
      "epoch:8 step:7779 [D loss: 0.567801, acc.: 71.09%] [G loss: 1.083257]\n",
      "epoch:8 step:7780 [D loss: 0.587394, acc.: 69.53%] [G loss: 1.121667]\n",
      "epoch:8 step:7781 [D loss: 0.672496, acc.: 62.50%] [G loss: 1.061679]\n",
      "epoch:8 step:7782 [D loss: 0.558556, acc.: 74.22%] [G loss: 1.318999]\n",
      "epoch:8 step:7783 [D loss: 0.618458, acc.: 66.41%] [G loss: 1.006775]\n",
      "epoch:8 step:7784 [D loss: 0.599881, acc.: 68.75%] [G loss: 1.295916]\n",
      "epoch:8 step:7785 [D loss: 0.588049, acc.: 67.19%] [G loss: 0.992150]\n",
      "epoch:8 step:7786 [D loss: 0.588405, acc.: 72.66%] [G loss: 1.063183]\n",
      "epoch:8 step:7787 [D loss: 0.612016, acc.: 62.50%] [G loss: 1.331261]\n",
      "epoch:8 step:7788 [D loss: 0.693986, acc.: 53.91%] [G loss: 1.099682]\n",
      "epoch:8 step:7789 [D loss: 0.664728, acc.: 62.50%] [G loss: 1.069199]\n",
      "epoch:8 step:7790 [D loss: 0.667034, acc.: 57.03%] [G loss: 0.989015]\n",
      "epoch:8 step:7791 [D loss: 0.584558, acc.: 68.75%] [G loss: 1.191718]\n",
      "epoch:8 step:7792 [D loss: 0.584281, acc.: 66.41%] [G loss: 1.079654]\n",
      "epoch:8 step:7793 [D loss: 0.609862, acc.: 67.97%] [G loss: 1.081885]\n",
      "epoch:8 step:7794 [D loss: 0.576056, acc.: 65.62%] [G loss: 1.031291]\n",
      "epoch:8 step:7795 [D loss: 0.662851, acc.: 62.50%] [G loss: 0.961287]\n",
      "epoch:8 step:7796 [D loss: 0.538151, acc.: 72.66%] [G loss: 0.967082]\n",
      "epoch:8 step:7797 [D loss: 0.647272, acc.: 69.53%] [G loss: 1.311840]\n",
      "epoch:8 step:7798 [D loss: 0.741945, acc.: 54.69%] [G loss: 1.059279]\n",
      "epoch:8 step:7799 [D loss: 0.679489, acc.: 63.28%] [G loss: 1.074397]\n",
      "epoch:8 step:7800 [D loss: 0.610662, acc.: 66.41%] [G loss: 1.147871]\n",
      "##############\n",
      "[2.69584073 2.20141135 2.14965331 3.0828701  1.18492373 7.1655146\n",
      " 2.1965143  2.85421578 3.96252749 5.98638208]\n",
      "##########\n",
      "epoch:8 step:7801 [D loss: 0.704452, acc.: 60.16%] [G loss: 1.102332]\n",
      "epoch:8 step:7802 [D loss: 0.643015, acc.: 65.62%] [G loss: 1.407837]\n",
      "epoch:8 step:7803 [D loss: 0.710009, acc.: 58.59%] [G loss: 0.935870]\n",
      "epoch:8 step:7804 [D loss: 0.581069, acc.: 69.53%] [G loss: 1.075350]\n",
      "epoch:8 step:7805 [D loss: 0.549114, acc.: 74.22%] [G loss: 1.208829]\n",
      "epoch:8 step:7806 [D loss: 0.646313, acc.: 60.94%] [G loss: 1.060212]\n",
      "epoch:8 step:7807 [D loss: 0.539450, acc.: 71.88%] [G loss: 1.312185]\n",
      "epoch:8 step:7808 [D loss: 0.706741, acc.: 57.81%] [G loss: 1.244677]\n",
      "epoch:8 step:7809 [D loss: 0.632796, acc.: 64.06%] [G loss: 0.954657]\n",
      "epoch:8 step:7810 [D loss: 0.579166, acc.: 66.41%] [G loss: 1.303753]\n",
      "epoch:8 step:7811 [D loss: 0.578416, acc.: 71.88%] [G loss: 1.444381]\n",
      "epoch:8 step:7812 [D loss: 0.748749, acc.: 52.34%] [G loss: 1.075274]\n",
      "epoch:8 step:7813 [D loss: 0.514535, acc.: 79.69%] [G loss: 1.156791]\n",
      "epoch:8 step:7814 [D loss: 0.755526, acc.: 50.00%] [G loss: 1.130097]\n",
      "epoch:8 step:7815 [D loss: 0.622331, acc.: 64.06%] [G loss: 1.244861]\n",
      "epoch:8 step:7816 [D loss: 0.534505, acc.: 74.22%] [G loss: 1.188964]\n",
      "epoch:8 step:7817 [D loss: 0.600458, acc.: 67.19%] [G loss: 1.384875]\n",
      "epoch:8 step:7818 [D loss: 0.594452, acc.: 71.09%] [G loss: 1.040771]\n",
      "epoch:8 step:7819 [D loss: 0.729566, acc.: 51.56%] [G loss: 0.917798]\n",
      "epoch:8 step:7820 [D loss: 0.560234, acc.: 75.00%] [G loss: 1.108463]\n",
      "epoch:8 step:7821 [D loss: 0.662876, acc.: 59.38%] [G loss: 1.193105]\n",
      "epoch:8 step:7822 [D loss: 0.582155, acc.: 73.44%] [G loss: 1.091262]\n",
      "epoch:8 step:7823 [D loss: 0.537539, acc.: 75.78%] [G loss: 1.333822]\n",
      "epoch:8 step:7824 [D loss: 0.559305, acc.: 71.88%] [G loss: 1.053855]\n",
      "epoch:8 step:7825 [D loss: 0.680231, acc.: 58.59%] [G loss: 1.138100]\n",
      "epoch:8 step:7826 [D loss: 0.564295, acc.: 73.44%] [G loss: 1.049731]\n",
      "epoch:8 step:7827 [D loss: 0.554753, acc.: 68.75%] [G loss: 1.235260]\n",
      "epoch:8 step:7828 [D loss: 0.657600, acc.: 64.06%] [G loss: 1.020278]\n",
      "epoch:8 step:7829 [D loss: 0.706914, acc.: 55.47%] [G loss: 1.094342]\n",
      "epoch:8 step:7830 [D loss: 0.547100, acc.: 71.09%] [G loss: 1.156600]\n",
      "epoch:8 step:7831 [D loss: 0.650954, acc.: 64.06%] [G loss: 1.220163]\n",
      "epoch:8 step:7832 [D loss: 0.535058, acc.: 75.00%] [G loss: 1.510004]\n",
      "epoch:8 step:7833 [D loss: 0.690405, acc.: 60.94%] [G loss: 1.218957]\n",
      "epoch:8 step:7834 [D loss: 0.596697, acc.: 67.97%] [G loss: 1.254086]\n",
      "epoch:8 step:7835 [D loss: 0.628686, acc.: 64.84%] [G loss: 1.026236]\n",
      "epoch:8 step:7836 [D loss: 0.614259, acc.: 64.84%] [G loss: 1.269196]\n",
      "epoch:8 step:7837 [D loss: 0.626570, acc.: 62.50%] [G loss: 1.214617]\n",
      "epoch:8 step:7838 [D loss: 0.747272, acc.: 50.78%] [G loss: 1.026937]\n",
      "epoch:8 step:7839 [D loss: 0.569598, acc.: 71.88%] [G loss: 1.092574]\n",
      "epoch:8 step:7840 [D loss: 0.501137, acc.: 78.12%] [G loss: 1.244418]\n",
      "epoch:8 step:7841 [D loss: 0.638746, acc.: 65.62%] [G loss: 1.168919]\n",
      "epoch:8 step:7842 [D loss: 0.699635, acc.: 57.81%] [G loss: 1.177096]\n",
      "epoch:8 step:7843 [D loss: 0.579174, acc.: 73.44%] [G loss: 0.902681]\n",
      "epoch:8 step:7844 [D loss: 0.686947, acc.: 53.12%] [G loss: 1.115496]\n",
      "epoch:8 step:7845 [D loss: 0.596696, acc.: 68.75%] [G loss: 1.015162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7846 [D loss: 0.684146, acc.: 57.81%] [G loss: 1.205175]\n",
      "epoch:8 step:7847 [D loss: 0.607942, acc.: 67.97%] [G loss: 1.247206]\n",
      "epoch:8 step:7848 [D loss: 0.689833, acc.: 53.12%] [G loss: 1.113141]\n",
      "epoch:8 step:7849 [D loss: 0.637950, acc.: 64.06%] [G loss: 1.050338]\n",
      "epoch:8 step:7850 [D loss: 0.612435, acc.: 70.31%] [G loss: 1.098326]\n",
      "epoch:8 step:7851 [D loss: 0.596209, acc.: 70.31%] [G loss: 1.155631]\n",
      "epoch:8 step:7852 [D loss: 0.592519, acc.: 71.88%] [G loss: 1.001312]\n",
      "epoch:8 step:7853 [D loss: 0.588162, acc.: 71.88%] [G loss: 1.069177]\n",
      "epoch:8 step:7854 [D loss: 0.580250, acc.: 64.84%] [G loss: 1.139053]\n",
      "epoch:8 step:7855 [D loss: 0.674622, acc.: 59.38%] [G loss: 1.117739]\n",
      "epoch:8 step:7856 [D loss: 0.606359, acc.: 64.06%] [G loss: 1.022848]\n",
      "epoch:8 step:7857 [D loss: 0.537486, acc.: 73.44%] [G loss: 1.428331]\n",
      "epoch:8 step:7858 [D loss: 0.533160, acc.: 78.91%] [G loss: 1.078198]\n",
      "epoch:8 step:7859 [D loss: 0.584663, acc.: 71.09%] [G loss: 1.228819]\n",
      "epoch:8 step:7860 [D loss: 0.628675, acc.: 63.28%] [G loss: 1.147177]\n",
      "epoch:8 step:7861 [D loss: 0.582351, acc.: 67.19%] [G loss: 1.226917]\n",
      "epoch:8 step:7862 [D loss: 0.515829, acc.: 72.66%] [G loss: 1.261259]\n",
      "epoch:8 step:7863 [D loss: 0.699276, acc.: 55.47%] [G loss: 1.223145]\n",
      "epoch:8 step:7864 [D loss: 0.555671, acc.: 71.88%] [G loss: 1.132570]\n",
      "epoch:8 step:7865 [D loss: 0.648074, acc.: 60.94%] [G loss: 1.050992]\n",
      "epoch:8 step:7866 [D loss: 0.606963, acc.: 70.31%] [G loss: 1.185993]\n",
      "epoch:8 step:7867 [D loss: 0.678907, acc.: 61.72%] [G loss: 1.247885]\n",
      "epoch:8 step:7868 [D loss: 0.659506, acc.: 59.38%] [G loss: 1.163784]\n",
      "epoch:8 step:7869 [D loss: 0.675889, acc.: 62.50%] [G loss: 1.123738]\n",
      "epoch:8 step:7870 [D loss: 0.649029, acc.: 57.03%] [G loss: 1.119633]\n",
      "epoch:8 step:7871 [D loss: 0.620981, acc.: 62.50%] [G loss: 1.081037]\n",
      "epoch:8 step:7872 [D loss: 0.700397, acc.: 57.81%] [G loss: 0.988234]\n",
      "epoch:8 step:7873 [D loss: 0.559011, acc.: 75.00%] [G loss: 0.845322]\n",
      "epoch:8 step:7874 [D loss: 0.629947, acc.: 71.09%] [G loss: 1.119927]\n",
      "epoch:8 step:7875 [D loss: 0.596919, acc.: 67.97%] [G loss: 0.970515]\n",
      "epoch:8 step:7876 [D loss: 0.690900, acc.: 58.59%] [G loss: 1.055593]\n",
      "epoch:8 step:7877 [D loss: 0.653890, acc.: 57.03%] [G loss: 1.084374]\n",
      "epoch:8 step:7878 [D loss: 0.603802, acc.: 69.53%] [G loss: 1.267584]\n",
      "epoch:8 step:7879 [D loss: 0.627408, acc.: 63.28%] [G loss: 1.046999]\n",
      "epoch:8 step:7880 [D loss: 0.546575, acc.: 72.66%] [G loss: 1.233726]\n",
      "epoch:8 step:7881 [D loss: 0.583225, acc.: 71.09%] [G loss: 1.202038]\n",
      "epoch:8 step:7882 [D loss: 0.616856, acc.: 69.53%] [G loss: 1.104386]\n",
      "epoch:8 step:7883 [D loss: 0.697181, acc.: 55.47%] [G loss: 1.117117]\n",
      "epoch:8 step:7884 [D loss: 0.569513, acc.: 70.31%] [G loss: 1.298360]\n",
      "epoch:8 step:7885 [D loss: 0.733142, acc.: 52.34%] [G loss: 1.150418]\n",
      "epoch:8 step:7886 [D loss: 0.492020, acc.: 83.59%] [G loss: 1.295347]\n",
      "epoch:8 step:7887 [D loss: 0.594585, acc.: 70.31%] [G loss: 1.133545]\n",
      "epoch:8 step:7888 [D loss: 0.540559, acc.: 70.31%] [G loss: 1.182103]\n",
      "epoch:8 step:7889 [D loss: 0.524304, acc.: 76.56%] [G loss: 1.257384]\n",
      "epoch:8 step:7890 [D loss: 0.554154, acc.: 76.56%] [G loss: 1.285054]\n",
      "epoch:8 step:7891 [D loss: 0.609457, acc.: 64.06%] [G loss: 1.014740]\n",
      "epoch:8 step:7892 [D loss: 0.623133, acc.: 64.84%] [G loss: 1.281876]\n",
      "epoch:8 step:7893 [D loss: 0.746174, acc.: 55.47%] [G loss: 0.993634]\n",
      "epoch:8 step:7894 [D loss: 0.635574, acc.: 63.28%] [G loss: 1.175400]\n",
      "epoch:8 step:7895 [D loss: 0.623798, acc.: 69.53%] [G loss: 1.139483]\n",
      "epoch:8 step:7896 [D loss: 0.583762, acc.: 69.53%] [G loss: 1.210878]\n",
      "epoch:8 step:7897 [D loss: 0.570851, acc.: 67.19%] [G loss: 1.271015]\n",
      "epoch:8 step:7898 [D loss: 0.567262, acc.: 67.97%] [G loss: 1.145809]\n",
      "epoch:8 step:7899 [D loss: 0.650691, acc.: 56.25%] [G loss: 0.953950]\n",
      "epoch:8 step:7900 [D loss: 0.624145, acc.: 61.72%] [G loss: 1.108914]\n",
      "epoch:8 step:7901 [D loss: 0.620315, acc.: 63.28%] [G loss: 1.179562]\n",
      "epoch:8 step:7902 [D loss: 0.539489, acc.: 75.00%] [G loss: 1.210783]\n",
      "epoch:8 step:7903 [D loss: 0.690356, acc.: 55.47%] [G loss: 1.158401]\n",
      "epoch:8 step:7904 [D loss: 0.620746, acc.: 66.41%] [G loss: 1.040322]\n",
      "epoch:8 step:7905 [D loss: 0.793350, acc.: 48.44%] [G loss: 1.031709]\n",
      "epoch:8 step:7906 [D loss: 0.590757, acc.: 71.88%] [G loss: 1.039943]\n",
      "epoch:8 step:7907 [D loss: 0.591344, acc.: 66.41%] [G loss: 0.769825]\n",
      "epoch:8 step:7908 [D loss: 0.654849, acc.: 60.94%] [G loss: 1.139668]\n",
      "epoch:8 step:7909 [D loss: 0.689228, acc.: 59.38%] [G loss: 1.200333]\n",
      "epoch:8 step:7910 [D loss: 0.535827, acc.: 75.00%] [G loss: 1.195675]\n",
      "epoch:8 step:7911 [D loss: 0.470907, acc.: 83.59%] [G loss: 1.328861]\n",
      "epoch:8 step:7912 [D loss: 0.552602, acc.: 75.00%] [G loss: 1.277958]\n",
      "epoch:8 step:7913 [D loss: 0.648336, acc.: 60.16%] [G loss: 1.313883]\n",
      "epoch:8 step:7914 [D loss: 0.644088, acc.: 65.62%] [G loss: 1.144056]\n",
      "epoch:8 step:7915 [D loss: 0.606863, acc.: 73.44%] [G loss: 1.104461]\n",
      "epoch:8 step:7916 [D loss: 0.528667, acc.: 77.34%] [G loss: 1.238313]\n",
      "epoch:8 step:7917 [D loss: 0.574304, acc.: 65.62%] [G loss: 1.128294]\n",
      "epoch:8 step:7918 [D loss: 0.633247, acc.: 64.84%] [G loss: 0.986610]\n",
      "epoch:8 step:7919 [D loss: 0.572284, acc.: 71.88%] [G loss: 1.039078]\n",
      "epoch:8 step:7920 [D loss: 0.684549, acc.: 57.81%] [G loss: 0.964564]\n",
      "epoch:8 step:7921 [D loss: 0.571689, acc.: 72.66%] [G loss: 1.134784]\n",
      "epoch:8 step:7922 [D loss: 0.709668, acc.: 57.81%] [G loss: 1.132125]\n",
      "epoch:8 step:7923 [D loss: 0.550371, acc.: 75.78%] [G loss: 1.028735]\n",
      "epoch:8 step:7924 [D loss: 0.653769, acc.: 59.38%] [G loss: 0.990736]\n",
      "epoch:8 step:7925 [D loss: 0.633412, acc.: 67.19%] [G loss: 0.972963]\n",
      "epoch:8 step:7926 [D loss: 0.609443, acc.: 67.19%] [G loss: 1.081266]\n",
      "epoch:8 step:7927 [D loss: 0.585491, acc.: 70.31%] [G loss: 1.146224]\n",
      "epoch:8 step:7928 [D loss: 0.584625, acc.: 70.31%] [G loss: 1.339029]\n",
      "epoch:8 step:7929 [D loss: 0.638196, acc.: 64.06%] [G loss: 1.224520]\n",
      "epoch:8 step:7930 [D loss: 0.629931, acc.: 60.16%] [G loss: 1.149516]\n",
      "epoch:8 step:7931 [D loss: 0.608792, acc.: 67.97%] [G loss: 1.087169]\n",
      "epoch:8 step:7932 [D loss: 0.492529, acc.: 78.12%] [G loss: 1.054176]\n",
      "epoch:8 step:7933 [D loss: 0.629280, acc.: 64.06%] [G loss: 1.105659]\n",
      "epoch:8 step:7934 [D loss: 0.575145, acc.: 69.53%] [G loss: 1.144933]\n",
      "epoch:8 step:7935 [D loss: 0.545295, acc.: 71.88%] [G loss: 1.251984]\n",
      "epoch:8 step:7936 [D loss: 0.628771, acc.: 61.72%] [G loss: 1.066166]\n",
      "epoch:8 step:7937 [D loss: 0.537436, acc.: 74.22%] [G loss: 1.364038]\n",
      "epoch:8 step:7938 [D loss: 0.600639, acc.: 64.84%] [G loss: 1.086993]\n",
      "epoch:8 step:7939 [D loss: 0.590330, acc.: 70.31%] [G loss: 1.162302]\n",
      "epoch:8 step:7940 [D loss: 0.617359, acc.: 65.62%] [G loss: 1.175610]\n",
      "epoch:8 step:7941 [D loss: 0.581384, acc.: 65.62%] [G loss: 1.091294]\n",
      "epoch:8 step:7942 [D loss: 0.685411, acc.: 57.81%] [G loss: 1.214463]\n",
      "epoch:8 step:7943 [D loss: 0.548258, acc.: 75.00%] [G loss: 1.095389]\n",
      "epoch:8 step:7944 [D loss: 0.703558, acc.: 56.25%] [G loss: 1.036520]\n",
      "epoch:8 step:7945 [D loss: 0.540509, acc.: 72.66%] [G loss: 1.265480]\n",
      "epoch:8 step:7946 [D loss: 0.594156, acc.: 66.41%] [G loss: 1.550848]\n",
      "epoch:8 step:7947 [D loss: 0.550533, acc.: 72.66%] [G loss: 0.860134]\n",
      "epoch:8 step:7948 [D loss: 0.557507, acc.: 71.09%] [G loss: 1.152695]\n",
      "epoch:8 step:7949 [D loss: 0.546020, acc.: 74.22%] [G loss: 1.284617]\n",
      "epoch:8 step:7950 [D loss: 0.568862, acc.: 73.44%] [G loss: 1.357497]\n",
      "epoch:8 step:7951 [D loss: 0.600163, acc.: 69.53%] [G loss: 1.054875]\n",
      "epoch:8 step:7952 [D loss: 0.681554, acc.: 57.81%] [G loss: 1.012508]\n",
      "epoch:8 step:7953 [D loss: 0.637142, acc.: 60.16%] [G loss: 1.136533]\n",
      "epoch:8 step:7954 [D loss: 0.649428, acc.: 60.16%] [G loss: 0.911052]\n",
      "epoch:8 step:7955 [D loss: 0.609674, acc.: 71.09%] [G loss: 1.258817]\n",
      "epoch:8 step:7956 [D loss: 0.703685, acc.: 53.12%] [G loss: 1.075041]\n",
      "epoch:8 step:7957 [D loss: 0.666342, acc.: 59.38%] [G loss: 1.251705]\n",
      "epoch:8 step:7958 [D loss: 0.705612, acc.: 57.81%] [G loss: 0.971560]\n",
      "epoch:8 step:7959 [D loss: 0.619982, acc.: 65.62%] [G loss: 1.153269]\n",
      "epoch:8 step:7960 [D loss: 0.574689, acc.: 64.84%] [G loss: 1.186901]\n",
      "epoch:8 step:7961 [D loss: 0.622000, acc.: 62.50%] [G loss: 0.976265]\n",
      "epoch:8 step:7962 [D loss: 0.612439, acc.: 66.41%] [G loss: 1.116115]\n",
      "epoch:8 step:7963 [D loss: 0.615614, acc.: 60.94%] [G loss: 1.013090]\n",
      "epoch:8 step:7964 [D loss: 0.587836, acc.: 64.84%] [G loss: 1.140826]\n",
      "epoch:8 step:7965 [D loss: 0.563187, acc.: 72.66%] [G loss: 1.266298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7966 [D loss: 0.743127, acc.: 53.91%] [G loss: 1.023843]\n",
      "epoch:8 step:7967 [D loss: 0.700734, acc.: 63.28%] [G loss: 1.134042]\n",
      "epoch:8 step:7968 [D loss: 0.514022, acc.: 81.25%] [G loss: 1.196255]\n",
      "epoch:8 step:7969 [D loss: 0.551495, acc.: 73.44%] [G loss: 1.172942]\n",
      "epoch:8 step:7970 [D loss: 0.660278, acc.: 59.38%] [G loss: 1.040530]\n",
      "epoch:8 step:7971 [D loss: 0.565585, acc.: 74.22%] [G loss: 1.192043]\n",
      "epoch:8 step:7972 [D loss: 0.594521, acc.: 69.53%] [G loss: 1.133669]\n",
      "epoch:8 step:7973 [D loss: 0.722671, acc.: 56.25%] [G loss: 1.078714]\n",
      "epoch:8 step:7974 [D loss: 0.530802, acc.: 75.00%] [G loss: 1.248311]\n",
      "epoch:8 step:7975 [D loss: 0.600425, acc.: 68.75%] [G loss: 1.167048]\n",
      "epoch:8 step:7976 [D loss: 0.540919, acc.: 73.44%] [G loss: 1.178623]\n",
      "epoch:8 step:7977 [D loss: 0.607012, acc.: 70.31%] [G loss: 0.992856]\n",
      "epoch:8 step:7978 [D loss: 0.598210, acc.: 64.84%] [G loss: 1.186594]\n",
      "epoch:8 step:7979 [D loss: 0.753639, acc.: 53.91%] [G loss: 0.951544]\n",
      "epoch:8 step:7980 [D loss: 0.552094, acc.: 72.66%] [G loss: 1.156118]\n",
      "epoch:8 step:7981 [D loss: 0.604035, acc.: 70.31%] [G loss: 1.252679]\n",
      "epoch:8 step:7982 [D loss: 0.673358, acc.: 58.59%] [G loss: 0.970310]\n",
      "epoch:8 step:7983 [D loss: 0.555374, acc.: 70.31%] [G loss: 1.312256]\n",
      "epoch:8 step:7984 [D loss: 0.619271, acc.: 64.06%] [G loss: 0.970834]\n",
      "epoch:8 step:7985 [D loss: 0.561594, acc.: 72.66%] [G loss: 1.235934]\n",
      "epoch:8 step:7986 [D loss: 0.584428, acc.: 68.75%] [G loss: 1.302304]\n",
      "epoch:8 step:7987 [D loss: 0.586670, acc.: 68.75%] [G loss: 1.125127]\n",
      "epoch:8 step:7988 [D loss: 0.542282, acc.: 69.53%] [G loss: 1.192168]\n",
      "epoch:8 step:7989 [D loss: 0.603147, acc.: 67.19%] [G loss: 1.070096]\n",
      "epoch:8 step:7990 [D loss: 0.615580, acc.: 60.94%] [G loss: 1.135188]\n",
      "epoch:8 step:7991 [D loss: 0.623902, acc.: 67.97%] [G loss: 1.165593]\n",
      "epoch:8 step:7992 [D loss: 0.548688, acc.: 73.44%] [G loss: 1.137564]\n",
      "epoch:8 step:7993 [D loss: 0.560224, acc.: 69.53%] [G loss: 1.165138]\n",
      "epoch:8 step:7994 [D loss: 0.573802, acc.: 70.31%] [G loss: 0.958932]\n",
      "epoch:8 step:7995 [D loss: 0.652963, acc.: 64.84%] [G loss: 1.154865]\n",
      "epoch:8 step:7996 [D loss: 0.638972, acc.: 64.84%] [G loss: 1.096594]\n",
      "epoch:8 step:7997 [D loss: 0.548379, acc.: 75.00%] [G loss: 1.233019]\n",
      "epoch:8 step:7998 [D loss: 0.568885, acc.: 72.66%] [G loss: 1.343508]\n",
      "epoch:8 step:7999 [D loss: 0.518635, acc.: 77.34%] [G loss: 1.354990]\n",
      "epoch:8 step:8000 [D loss: 0.718449, acc.: 61.72%] [G loss: 1.045752]\n",
      "##############\n",
      "[2.78418395 2.09674153 2.06893691 2.81511659 0.79626348 6.17117258\n",
      " 1.86421532 2.7172001  3.80093048 7.14868929]\n",
      "##########\n",
      "epoch:8 step:8001 [D loss: 0.577408, acc.: 71.88%] [G loss: 1.412582]\n",
      "epoch:8 step:8002 [D loss: 0.654273, acc.: 60.16%] [G loss: 1.095597]\n",
      "epoch:8 step:8003 [D loss: 0.707371, acc.: 57.03%] [G loss: 1.125738]\n",
      "epoch:8 step:8004 [D loss: 0.547535, acc.: 75.78%] [G loss: 1.252372]\n",
      "epoch:8 step:8005 [D loss: 0.614168, acc.: 67.19%] [G loss: 1.160309]\n",
      "epoch:8 step:8006 [D loss: 0.625891, acc.: 61.72%] [G loss: 1.212488]\n",
      "epoch:8 step:8007 [D loss: 0.541174, acc.: 78.12%] [G loss: 1.027841]\n",
      "epoch:8 step:8008 [D loss: 0.719121, acc.: 55.47%] [G loss: 1.170841]\n",
      "epoch:8 step:8009 [D loss: 0.609055, acc.: 63.28%] [G loss: 1.061101]\n",
      "epoch:8 step:8010 [D loss: 0.618629, acc.: 65.62%] [G loss: 1.085883]\n",
      "epoch:8 step:8011 [D loss: 0.572597, acc.: 72.66%] [G loss: 1.219780]\n",
      "epoch:8 step:8012 [D loss: 0.579947, acc.: 71.09%] [G loss: 1.409140]\n",
      "epoch:8 step:8013 [D loss: 0.606119, acc.: 62.50%] [G loss: 1.357324]\n",
      "epoch:8 step:8014 [D loss: 0.625307, acc.: 61.72%] [G loss: 1.058448]\n",
      "epoch:8 step:8015 [D loss: 0.571556, acc.: 70.31%] [G loss: 1.255973]\n",
      "epoch:8 step:8016 [D loss: 0.623278, acc.: 70.31%] [G loss: 0.960633]\n",
      "epoch:8 step:8017 [D loss: 0.574325, acc.: 74.22%] [G loss: 1.184875]\n",
      "epoch:8 step:8018 [D loss: 0.544356, acc.: 74.22%] [G loss: 1.290577]\n",
      "epoch:8 step:8019 [D loss: 0.627611, acc.: 66.41%] [G loss: 1.240145]\n",
      "epoch:8 step:8020 [D loss: 0.544805, acc.: 73.44%] [G loss: 1.268671]\n",
      "epoch:8 step:8021 [D loss: 0.661204, acc.: 60.94%] [G loss: 0.949303]\n",
      "epoch:8 step:8022 [D loss: 0.660030, acc.: 60.16%] [G loss: 1.059901]\n",
      "epoch:8 step:8023 [D loss: 0.701141, acc.: 55.47%] [G loss: 1.184670]\n",
      "epoch:8 step:8024 [D loss: 0.530584, acc.: 73.44%] [G loss: 1.414754]\n",
      "epoch:8 step:8025 [D loss: 0.596251, acc.: 64.84%] [G loss: 0.937830]\n",
      "epoch:8 step:8026 [D loss: 0.586060, acc.: 64.84%] [G loss: 0.984969]\n",
      "epoch:8 step:8027 [D loss: 0.543930, acc.: 77.34%] [G loss: 0.969408]\n",
      "epoch:8 step:8028 [D loss: 0.783390, acc.: 46.09%] [G loss: 0.992570]\n",
      "epoch:8 step:8029 [D loss: 0.528114, acc.: 76.56%] [G loss: 1.300389]\n",
      "epoch:8 step:8030 [D loss: 0.605473, acc.: 68.75%] [G loss: 1.268371]\n",
      "epoch:8 step:8031 [D loss: 0.650660, acc.: 59.38%] [G loss: 1.192230]\n",
      "epoch:8 step:8032 [D loss: 0.556930, acc.: 75.00%] [G loss: 1.289102]\n",
      "epoch:8 step:8033 [D loss: 0.614443, acc.: 64.84%] [G loss: 1.117272]\n",
      "epoch:8 step:8034 [D loss: 0.638675, acc.: 65.62%] [G loss: 1.113109]\n",
      "epoch:8 step:8035 [D loss: 0.509696, acc.: 77.34%] [G loss: 1.261800]\n",
      "epoch:8 step:8036 [D loss: 0.548321, acc.: 75.00%] [G loss: 1.309687]\n",
      "epoch:8 step:8037 [D loss: 0.632330, acc.: 64.84%] [G loss: 0.984190]\n",
      "epoch:8 step:8038 [D loss: 0.595525, acc.: 73.44%] [G loss: 1.194077]\n",
      "epoch:8 step:8039 [D loss: 0.566716, acc.: 74.22%] [G loss: 1.081791]\n",
      "epoch:8 step:8040 [D loss: 0.660014, acc.: 62.50%] [G loss: 1.069216]\n",
      "epoch:8 step:8041 [D loss: 0.637753, acc.: 63.28%] [G loss: 1.091434]\n",
      "epoch:8 step:8042 [D loss: 0.584823, acc.: 72.66%] [G loss: 1.132467]\n",
      "epoch:8 step:8043 [D loss: 0.618247, acc.: 67.97%] [G loss: 0.997966]\n",
      "epoch:8 step:8044 [D loss: 0.633457, acc.: 67.97%] [G loss: 0.939780]\n",
      "epoch:8 step:8045 [D loss: 0.525527, acc.: 72.66%] [G loss: 1.189293]\n",
      "epoch:8 step:8046 [D loss: 0.478811, acc.: 79.69%] [G loss: 0.992561]\n",
      "epoch:8 step:8047 [D loss: 0.615295, acc.: 64.06%] [G loss: 1.141272]\n",
      "epoch:8 step:8048 [D loss: 0.468862, acc.: 78.91%] [G loss: 1.300499]\n",
      "epoch:8 step:8049 [D loss: 0.617618, acc.: 70.31%] [G loss: 1.122216]\n",
      "epoch:8 step:8050 [D loss: 0.586095, acc.: 67.97%] [G loss: 1.056458]\n",
      "epoch:8 step:8051 [D loss: 0.543656, acc.: 73.44%] [G loss: 1.384311]\n",
      "epoch:8 step:8052 [D loss: 0.607964, acc.: 70.31%] [G loss: 1.118243]\n",
      "epoch:8 step:8053 [D loss: 0.602960, acc.: 68.75%] [G loss: 1.357489]\n",
      "epoch:8 step:8054 [D loss: 0.620187, acc.: 66.41%] [G loss: 1.081610]\n",
      "epoch:8 step:8055 [D loss: 0.549117, acc.: 73.44%] [G loss: 1.116269]\n",
      "epoch:8 step:8056 [D loss: 0.583459, acc.: 71.88%] [G loss: 1.135486]\n",
      "epoch:8 step:8057 [D loss: 0.607056, acc.: 65.62%] [G loss: 1.316108]\n",
      "epoch:8 step:8058 [D loss: 0.656229, acc.: 57.81%] [G loss: 1.056305]\n",
      "epoch:8 step:8059 [D loss: 0.627137, acc.: 64.06%] [G loss: 1.149672]\n",
      "epoch:8 step:8060 [D loss: 0.474703, acc.: 82.03%] [G loss: 1.200877]\n",
      "epoch:8 step:8061 [D loss: 0.683370, acc.: 56.25%] [G loss: 1.022464]\n",
      "epoch:8 step:8062 [D loss: 0.641628, acc.: 65.62%] [G loss: 1.219746]\n",
      "epoch:8 step:8063 [D loss: 0.595857, acc.: 66.41%] [G loss: 1.261479]\n",
      "epoch:8 step:8064 [D loss: 0.640536, acc.: 67.19%] [G loss: 1.166986]\n",
      "epoch:8 step:8065 [D loss: 0.597225, acc.: 72.66%] [G loss: 0.943276]\n",
      "epoch:8 step:8066 [D loss: 0.571157, acc.: 75.00%] [G loss: 1.205562]\n",
      "epoch:8 step:8067 [D loss: 0.598090, acc.: 64.84%] [G loss: 0.985447]\n",
      "epoch:8 step:8068 [D loss: 0.600026, acc.: 67.97%] [G loss: 1.174872]\n",
      "epoch:8 step:8069 [D loss: 0.758769, acc.: 49.22%] [G loss: 1.010306]\n",
      "epoch:8 step:8070 [D loss: 0.751308, acc.: 45.31%] [G loss: 1.061388]\n",
      "epoch:8 step:8071 [D loss: 0.608461, acc.: 64.84%] [G loss: 1.002346]\n",
      "epoch:8 step:8072 [D loss: 0.665796, acc.: 57.03%] [G loss: 1.109251]\n",
      "epoch:8 step:8073 [D loss: 0.625426, acc.: 63.28%] [G loss: 1.252623]\n",
      "epoch:8 step:8074 [D loss: 0.662733, acc.: 62.50%] [G loss: 1.158924]\n",
      "epoch:8 step:8075 [D loss: 0.592184, acc.: 69.53%] [G loss: 1.015377]\n",
      "epoch:8 step:8076 [D loss: 0.622702, acc.: 67.19%] [G loss: 1.224709]\n",
      "epoch:8 step:8077 [D loss: 0.516908, acc.: 79.69%] [G loss: 1.239508]\n",
      "epoch:8 step:8078 [D loss: 0.615281, acc.: 66.41%] [G loss: 1.004396]\n",
      "epoch:8 step:8079 [D loss: 0.607628, acc.: 64.84%] [G loss: 1.033096]\n",
      "epoch:8 step:8080 [D loss: 0.586794, acc.: 67.97%] [G loss: 1.305473]\n",
      "epoch:8 step:8081 [D loss: 0.639622, acc.: 62.50%] [G loss: 1.128441]\n",
      "epoch:8 step:8082 [D loss: 0.594504, acc.: 67.19%] [G loss: 1.078152]\n",
      "epoch:8 step:8083 [D loss: 0.556515, acc.: 69.53%] [G loss: 1.171391]\n",
      "epoch:8 step:8084 [D loss: 0.649575, acc.: 64.84%] [G loss: 1.068822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8085 [D loss: 0.585729, acc.: 71.09%] [G loss: 1.351353]\n",
      "epoch:8 step:8086 [D loss: 0.618639, acc.: 69.53%] [G loss: 1.314815]\n",
      "epoch:8 step:8087 [D loss: 0.550837, acc.: 72.66%] [G loss: 1.110032]\n",
      "epoch:8 step:8088 [D loss: 0.644157, acc.: 66.41%] [G loss: 0.996614]\n",
      "epoch:8 step:8089 [D loss: 0.664479, acc.: 58.59%] [G loss: 1.337347]\n",
      "epoch:8 step:8090 [D loss: 0.583337, acc.: 71.88%] [G loss: 1.291339]\n",
      "epoch:8 step:8091 [D loss: 0.547487, acc.: 71.09%] [G loss: 1.091466]\n",
      "epoch:8 step:8092 [D loss: 0.555428, acc.: 74.22%] [G loss: 1.246038]\n",
      "epoch:8 step:8093 [D loss: 0.654448, acc.: 57.81%] [G loss: 1.000223]\n",
      "epoch:8 step:8094 [D loss: 0.587078, acc.: 67.19%] [G loss: 1.342760]\n",
      "epoch:8 step:8095 [D loss: 0.508609, acc.: 78.91%] [G loss: 1.219343]\n",
      "epoch:8 step:8096 [D loss: 0.557640, acc.: 71.09%] [G loss: 1.111902]\n",
      "epoch:8 step:8097 [D loss: 0.584151, acc.: 67.97%] [G loss: 1.437698]\n",
      "epoch:8 step:8098 [D loss: 0.521225, acc.: 78.91%] [G loss: 1.386183]\n",
      "epoch:8 step:8099 [D loss: 0.567787, acc.: 67.19%] [G loss: 1.263967]\n",
      "epoch:8 step:8100 [D loss: 0.670130, acc.: 57.81%] [G loss: 1.158194]\n",
      "epoch:8 step:8101 [D loss: 0.669183, acc.: 64.84%] [G loss: 1.102280]\n",
      "epoch:8 step:8102 [D loss: 0.581845, acc.: 71.88%] [G loss: 1.006485]\n",
      "epoch:8 step:8103 [D loss: 0.542383, acc.: 74.22%] [G loss: 1.216761]\n",
      "epoch:8 step:8104 [D loss: 0.688422, acc.: 57.03%] [G loss: 1.058404]\n",
      "epoch:8 step:8105 [D loss: 0.650747, acc.: 62.50%] [G loss: 1.078973]\n",
      "epoch:8 step:8106 [D loss: 0.584091, acc.: 74.22%] [G loss: 1.134943]\n",
      "epoch:8 step:8107 [D loss: 0.640214, acc.: 64.06%] [G loss: 1.159978]\n",
      "epoch:8 step:8108 [D loss: 0.610810, acc.: 62.50%] [G loss: 1.086315]\n",
      "epoch:8 step:8109 [D loss: 0.582469, acc.: 69.53%] [G loss: 1.066577]\n",
      "epoch:8 step:8110 [D loss: 0.591385, acc.: 65.62%] [G loss: 1.239095]\n",
      "epoch:8 step:8111 [D loss: 0.669359, acc.: 63.28%] [G loss: 1.112232]\n",
      "epoch:8 step:8112 [D loss: 0.560782, acc.: 70.31%] [G loss: 1.116664]\n",
      "epoch:8 step:8113 [D loss: 0.565027, acc.: 66.41%] [G loss: 1.085824]\n",
      "epoch:8 step:8114 [D loss: 0.564348, acc.: 73.44%] [G loss: 1.515662]\n",
      "epoch:8 step:8115 [D loss: 0.699757, acc.: 54.69%] [G loss: 1.220626]\n",
      "epoch:8 step:8116 [D loss: 0.703981, acc.: 59.38%] [G loss: 1.212229]\n",
      "epoch:8 step:8117 [D loss: 0.758471, acc.: 46.09%] [G loss: 0.946521]\n",
      "epoch:8 step:8118 [D loss: 0.689072, acc.: 57.81%] [G loss: 1.132341]\n",
      "epoch:8 step:8119 [D loss: 0.544763, acc.: 76.56%] [G loss: 1.192028]\n",
      "epoch:8 step:8120 [D loss: 0.572698, acc.: 69.53%] [G loss: 1.183592]\n",
      "epoch:8 step:8121 [D loss: 0.611342, acc.: 68.75%] [G loss: 1.102424]\n",
      "epoch:8 step:8122 [D loss: 0.638819, acc.: 67.19%] [G loss: 1.156679]\n",
      "epoch:8 step:8123 [D loss: 0.523989, acc.: 75.00%] [G loss: 1.226431]\n",
      "epoch:8 step:8124 [D loss: 0.714252, acc.: 53.91%] [G loss: 1.052732]\n",
      "epoch:8 step:8125 [D loss: 0.504840, acc.: 75.78%] [G loss: 1.194739]\n",
      "epoch:8 step:8126 [D loss: 0.656429, acc.: 60.16%] [G loss: 0.993817]\n",
      "epoch:8 step:8127 [D loss: 0.573658, acc.: 71.88%] [G loss: 1.049174]\n",
      "epoch:8 step:8128 [D loss: 0.600070, acc.: 71.09%] [G loss: 1.103268]\n",
      "epoch:8 step:8129 [D loss: 0.608652, acc.: 66.41%] [G loss: 1.183504]\n",
      "epoch:8 step:8130 [D loss: 0.525204, acc.: 78.91%] [G loss: 1.291506]\n",
      "epoch:8 step:8131 [D loss: 0.591531, acc.: 66.41%] [G loss: 1.214857]\n",
      "epoch:8 step:8132 [D loss: 0.542389, acc.: 70.31%] [G loss: 1.126783]\n",
      "epoch:8 step:8133 [D loss: 0.611567, acc.: 65.62%] [G loss: 1.118042]\n",
      "epoch:8 step:8134 [D loss: 0.555625, acc.: 69.53%] [G loss: 1.309644]\n",
      "epoch:8 step:8135 [D loss: 0.769440, acc.: 52.34%] [G loss: 1.083725]\n",
      "epoch:8 step:8136 [D loss: 0.606540, acc.: 68.75%] [G loss: 1.247116]\n",
      "epoch:8 step:8137 [D loss: 0.681355, acc.: 61.72%] [G loss: 0.953689]\n",
      "epoch:8 step:8138 [D loss: 0.619780, acc.: 63.28%] [G loss: 0.992581]\n",
      "epoch:8 step:8139 [D loss: 0.807199, acc.: 48.44%] [G loss: 1.072291]\n",
      "epoch:8 step:8140 [D loss: 0.595790, acc.: 65.62%] [G loss: 0.922463]\n",
      "epoch:8 step:8141 [D loss: 0.552971, acc.: 73.44%] [G loss: 1.251188]\n",
      "epoch:8 step:8142 [D loss: 0.696500, acc.: 61.72%] [G loss: 1.055625]\n",
      "epoch:8 step:8143 [D loss: 0.633982, acc.: 61.72%] [G loss: 1.287085]\n",
      "epoch:8 step:8144 [D loss: 0.546861, acc.: 73.44%] [G loss: 0.958297]\n",
      "epoch:8 step:8145 [D loss: 0.569854, acc.: 69.53%] [G loss: 1.286804]\n",
      "epoch:8 step:8146 [D loss: 0.545545, acc.: 76.56%] [G loss: 1.292927]\n",
      "epoch:8 step:8147 [D loss: 0.565640, acc.: 75.00%] [G loss: 1.253745]\n",
      "epoch:8 step:8148 [D loss: 0.594311, acc.: 66.41%] [G loss: 1.105650]\n",
      "epoch:8 step:8149 [D loss: 0.500107, acc.: 79.69%] [G loss: 1.207711]\n",
      "epoch:8 step:8150 [D loss: 0.617533, acc.: 67.19%] [G loss: 1.036517]\n",
      "epoch:8 step:8151 [D loss: 0.688280, acc.: 57.03%] [G loss: 1.092002]\n",
      "epoch:8 step:8152 [D loss: 0.528513, acc.: 80.47%] [G loss: 0.979990]\n",
      "epoch:8 step:8153 [D loss: 0.647103, acc.: 60.94%] [G loss: 1.045191]\n",
      "epoch:8 step:8154 [D loss: 0.614464, acc.: 64.84%] [G loss: 1.218662]\n",
      "epoch:8 step:8155 [D loss: 0.622751, acc.: 67.97%] [G loss: 1.125908]\n",
      "epoch:8 step:8156 [D loss: 0.608157, acc.: 64.84%] [G loss: 0.896971]\n",
      "epoch:8 step:8157 [D loss: 0.565591, acc.: 71.09%] [G loss: 1.136002]\n",
      "epoch:8 step:8158 [D loss: 0.614329, acc.: 67.19%] [G loss: 1.023714]\n",
      "epoch:8 step:8159 [D loss: 0.690128, acc.: 57.81%] [G loss: 1.079359]\n",
      "epoch:8 step:8160 [D loss: 0.660699, acc.: 60.94%] [G loss: 0.999748]\n",
      "epoch:8 step:8161 [D loss: 0.595976, acc.: 72.66%] [G loss: 1.170813]\n",
      "epoch:8 step:8162 [D loss: 0.606824, acc.: 67.19%] [G loss: 1.008451]\n",
      "epoch:8 step:8163 [D loss: 0.554211, acc.: 70.31%] [G loss: 1.196933]\n",
      "epoch:8 step:8164 [D loss: 0.728745, acc.: 54.69%] [G loss: 1.127319]\n",
      "epoch:8 step:8165 [D loss: 0.589100, acc.: 66.41%] [G loss: 1.354916]\n",
      "epoch:8 step:8166 [D loss: 0.593997, acc.: 71.09%] [G loss: 1.165850]\n",
      "epoch:8 step:8167 [D loss: 0.563210, acc.: 70.31%] [G loss: 1.215102]\n",
      "epoch:8 step:8168 [D loss: 0.670782, acc.: 63.28%] [G loss: 1.023622]\n",
      "epoch:8 step:8169 [D loss: 0.639180, acc.: 67.97%] [G loss: 0.980676]\n",
      "epoch:8 step:8170 [D loss: 0.613009, acc.: 66.41%] [G loss: 1.080051]\n",
      "epoch:8 step:8171 [D loss: 0.630976, acc.: 64.84%] [G loss: 1.229709]\n",
      "epoch:8 step:8172 [D loss: 0.546979, acc.: 74.22%] [G loss: 1.196127]\n",
      "epoch:8 step:8173 [D loss: 0.576349, acc.: 71.09%] [G loss: 1.307043]\n",
      "epoch:8 step:8174 [D loss: 0.647466, acc.: 62.50%] [G loss: 1.251705]\n",
      "epoch:8 step:8175 [D loss: 0.598822, acc.: 71.09%] [G loss: 1.147520]\n",
      "epoch:8 step:8176 [D loss: 0.635884, acc.: 63.28%] [G loss: 1.100311]\n",
      "epoch:8 step:8177 [D loss: 0.631354, acc.: 65.62%] [G loss: 1.252017]\n",
      "epoch:8 step:8178 [D loss: 0.513105, acc.: 76.56%] [G loss: 0.994080]\n",
      "epoch:8 step:8179 [D loss: 0.700140, acc.: 59.38%] [G loss: 0.991708]\n",
      "epoch:8 step:8180 [D loss: 0.516232, acc.: 75.78%] [G loss: 1.136690]\n",
      "epoch:8 step:8181 [D loss: 0.537889, acc.: 70.31%] [G loss: 1.082886]\n",
      "epoch:8 step:8182 [D loss: 0.606732, acc.: 68.75%] [G loss: 0.923194]\n",
      "epoch:8 step:8183 [D loss: 0.643780, acc.: 62.50%] [G loss: 1.240544]\n",
      "epoch:8 step:8184 [D loss: 0.696050, acc.: 59.38%] [G loss: 1.244753]\n",
      "epoch:8 step:8185 [D loss: 0.697360, acc.: 59.38%] [G loss: 1.346798]\n",
      "epoch:8 step:8186 [D loss: 0.597178, acc.: 72.66%] [G loss: 1.023911]\n",
      "epoch:8 step:8187 [D loss: 0.598601, acc.: 67.19%] [G loss: 1.164907]\n",
      "epoch:8 step:8188 [D loss: 0.616664, acc.: 70.31%] [G loss: 1.127315]\n",
      "epoch:8 step:8189 [D loss: 0.676948, acc.: 64.06%] [G loss: 0.979604]\n",
      "epoch:8 step:8190 [D loss: 0.635131, acc.: 63.28%] [G loss: 1.166994]\n",
      "epoch:8 step:8191 [D loss: 0.647385, acc.: 68.75%] [G loss: 1.094580]\n",
      "epoch:8 step:8192 [D loss: 0.628859, acc.: 62.50%] [G loss: 1.150094]\n",
      "epoch:8 step:8193 [D loss: 0.604331, acc.: 68.75%] [G loss: 1.073783]\n",
      "epoch:8 step:8194 [D loss: 0.584074, acc.: 72.66%] [G loss: 1.202449]\n",
      "epoch:8 step:8195 [D loss: 0.635924, acc.: 64.84%] [G loss: 1.184590]\n",
      "epoch:8 step:8196 [D loss: 0.610123, acc.: 67.19%] [G loss: 0.963591]\n",
      "epoch:8 step:8197 [D loss: 0.485893, acc.: 81.25%] [G loss: 1.204411]\n",
      "epoch:8 step:8198 [D loss: 0.515324, acc.: 79.69%] [G loss: 1.037805]\n",
      "epoch:8 step:8199 [D loss: 0.592417, acc.: 73.44%] [G loss: 1.064420]\n",
      "epoch:8 step:8200 [D loss: 0.534919, acc.: 71.09%] [G loss: 1.157959]\n",
      "##############\n",
      "[2.79236221 2.32799801 1.96769527 3.04752179 1.14646298 6.09477066\n",
      " 2.25419173 3.26702405 3.96494346 6.31518992]\n",
      "##########\n",
      "epoch:8 step:8201 [D loss: 0.611243, acc.: 67.19%] [G loss: 1.095090]\n",
      "epoch:8 step:8202 [D loss: 0.671542, acc.: 57.81%] [G loss: 1.202644]\n",
      "epoch:8 step:8203 [D loss: 0.739859, acc.: 51.56%] [G loss: 1.123386]\n",
      "epoch:8 step:8204 [D loss: 0.598045, acc.: 69.53%] [G loss: 0.963620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8205 [D loss: 0.759624, acc.: 57.81%] [G loss: 1.029458]\n",
      "epoch:8 step:8206 [D loss: 0.618818, acc.: 70.31%] [G loss: 1.303725]\n",
      "epoch:8 step:8207 [D loss: 0.590519, acc.: 67.97%] [G loss: 1.253449]\n",
      "epoch:8 step:8208 [D loss: 0.647783, acc.: 62.50%] [G loss: 1.010260]\n",
      "epoch:8 step:8209 [D loss: 0.610900, acc.: 69.53%] [G loss: 1.064803]\n",
      "epoch:8 step:8210 [D loss: 0.702635, acc.: 57.03%] [G loss: 1.082884]\n",
      "epoch:8 step:8211 [D loss: 0.673143, acc.: 60.94%] [G loss: 0.908196]\n",
      "epoch:8 step:8212 [D loss: 0.623564, acc.: 62.50%] [G loss: 0.945539]\n",
      "epoch:8 step:8213 [D loss: 0.631027, acc.: 59.38%] [G loss: 1.048706]\n",
      "epoch:8 step:8214 [D loss: 0.544631, acc.: 71.88%] [G loss: 1.178290]\n",
      "epoch:8 step:8215 [D loss: 0.671197, acc.: 56.25%] [G loss: 1.199042]\n",
      "epoch:8 step:8216 [D loss: 0.621483, acc.: 63.28%] [G loss: 1.078478]\n",
      "epoch:8 step:8217 [D loss: 0.699271, acc.: 57.81%] [G loss: 1.028887]\n",
      "epoch:8 step:8218 [D loss: 0.669075, acc.: 61.72%] [G loss: 1.337736]\n",
      "epoch:8 step:8219 [D loss: 0.616021, acc.: 70.31%] [G loss: 0.976167]\n",
      "epoch:8 step:8220 [D loss: 0.653239, acc.: 65.62%] [G loss: 1.267416]\n",
      "epoch:8 step:8221 [D loss: 0.609769, acc.: 65.62%] [G loss: 1.331343]\n",
      "epoch:8 step:8222 [D loss: 0.656880, acc.: 60.16%] [G loss: 1.117252]\n",
      "epoch:8 step:8223 [D loss: 0.524668, acc.: 76.56%] [G loss: 0.929076]\n",
      "epoch:8 step:8224 [D loss: 0.634567, acc.: 61.72%] [G loss: 1.170297]\n",
      "epoch:8 step:8225 [D loss: 0.696538, acc.: 60.94%] [G loss: 1.101959]\n",
      "epoch:8 step:8226 [D loss: 0.694828, acc.: 54.69%] [G loss: 1.095021]\n",
      "epoch:8 step:8227 [D loss: 0.666255, acc.: 60.16%] [G loss: 1.057613]\n",
      "epoch:8 step:8228 [D loss: 0.553108, acc.: 73.44%] [G loss: 1.202289]\n",
      "epoch:8 step:8229 [D loss: 0.695366, acc.: 56.25%] [G loss: 1.101612]\n",
      "epoch:8 step:8230 [D loss: 0.525274, acc.: 78.12%] [G loss: 1.248109]\n",
      "epoch:8 step:8231 [D loss: 0.670740, acc.: 57.03%] [G loss: 1.187938]\n",
      "epoch:8 step:8232 [D loss: 0.581176, acc.: 70.31%] [G loss: 1.326141]\n",
      "epoch:8 step:8233 [D loss: 0.692484, acc.: 57.81%] [G loss: 0.912195]\n",
      "epoch:8 step:8234 [D loss: 0.778740, acc.: 48.44%] [G loss: 1.064641]\n",
      "epoch:8 step:8235 [D loss: 0.573318, acc.: 70.31%] [G loss: 1.038210]\n",
      "epoch:8 step:8236 [D loss: 0.726237, acc.: 56.25%] [G loss: 0.874420]\n",
      "epoch:8 step:8237 [D loss: 0.625327, acc.: 66.41%] [G loss: 1.031274]\n",
      "epoch:8 step:8238 [D loss: 0.561494, acc.: 68.75%] [G loss: 1.148406]\n",
      "epoch:8 step:8239 [D loss: 0.691798, acc.: 58.59%] [G loss: 1.155884]\n",
      "epoch:8 step:8240 [D loss: 0.589959, acc.: 66.41%] [G loss: 1.055178]\n",
      "epoch:8 step:8241 [D loss: 0.608448, acc.: 64.84%] [G loss: 0.905695]\n",
      "epoch:8 step:8242 [D loss: 0.506358, acc.: 77.34%] [G loss: 1.304182]\n",
      "epoch:8 step:8243 [D loss: 0.705449, acc.: 56.25%] [G loss: 0.979431]\n",
      "epoch:8 step:8244 [D loss: 0.612981, acc.: 60.16%] [G loss: 1.066743]\n",
      "epoch:8 step:8245 [D loss: 0.677156, acc.: 57.03%] [G loss: 1.102466]\n",
      "epoch:8 step:8246 [D loss: 0.604328, acc.: 67.97%] [G loss: 1.115128]\n",
      "epoch:8 step:8247 [D loss: 0.558120, acc.: 74.22%] [G loss: 0.980672]\n",
      "epoch:8 step:8248 [D loss: 0.610232, acc.: 64.84%] [G loss: 1.224780]\n",
      "epoch:8 step:8249 [D loss: 0.538114, acc.: 74.22%] [G loss: 1.100955]\n",
      "epoch:8 step:8250 [D loss: 0.652339, acc.: 62.50%] [G loss: 1.201446]\n",
      "epoch:8 step:8251 [D loss: 0.603069, acc.: 63.28%] [G loss: 1.147679]\n",
      "epoch:8 step:8252 [D loss: 0.647959, acc.: 60.16%] [G loss: 0.946073]\n",
      "epoch:8 step:8253 [D loss: 0.556424, acc.: 74.22%] [G loss: 1.233437]\n",
      "epoch:8 step:8254 [D loss: 0.674104, acc.: 53.91%] [G loss: 1.160100]\n",
      "epoch:8 step:8255 [D loss: 0.607915, acc.: 66.41%] [G loss: 1.175782]\n",
      "epoch:8 step:8256 [D loss: 0.684736, acc.: 58.59%] [G loss: 0.942997]\n",
      "epoch:8 step:8257 [D loss: 0.725873, acc.: 49.22%] [G loss: 0.875677]\n",
      "epoch:8 step:8258 [D loss: 0.652247, acc.: 61.72%] [G loss: 1.165479]\n",
      "epoch:8 step:8259 [D loss: 0.632131, acc.: 67.19%] [G loss: 1.305231]\n",
      "epoch:8 step:8260 [D loss: 0.644701, acc.: 59.38%] [G loss: 0.994302]\n",
      "epoch:8 step:8261 [D loss: 0.660583, acc.: 62.50%] [G loss: 1.250882]\n",
      "epoch:8 step:8262 [D loss: 0.499546, acc.: 77.34%] [G loss: 1.368756]\n",
      "epoch:8 step:8263 [D loss: 0.617401, acc.: 61.72%] [G loss: 1.003606]\n",
      "epoch:8 step:8264 [D loss: 0.578500, acc.: 67.97%] [G loss: 1.159608]\n",
      "epoch:8 step:8265 [D loss: 0.577604, acc.: 67.97%] [G loss: 1.221228]\n",
      "epoch:8 step:8266 [D loss: 0.675585, acc.: 56.25%] [G loss: 1.074769]\n",
      "epoch:8 step:8267 [D loss: 0.665280, acc.: 62.50%] [G loss: 1.144224]\n",
      "epoch:8 step:8268 [D loss: 0.699052, acc.: 61.72%] [G loss: 0.941837]\n",
      "epoch:8 step:8269 [D loss: 0.625796, acc.: 62.50%] [G loss: 1.403010]\n",
      "epoch:8 step:8270 [D loss: 0.603291, acc.: 66.41%] [G loss: 1.142745]\n",
      "epoch:8 step:8271 [D loss: 0.682688, acc.: 58.59%] [G loss: 1.289841]\n",
      "epoch:8 step:8272 [D loss: 0.632468, acc.: 64.06%] [G loss: 1.372593]\n",
      "epoch:8 step:8273 [D loss: 0.538570, acc.: 75.78%] [G loss: 1.055785]\n",
      "epoch:8 step:8274 [D loss: 0.640229, acc.: 64.06%] [G loss: 1.187291]\n",
      "epoch:8 step:8275 [D loss: 0.603776, acc.: 63.28%] [G loss: 1.067676]\n",
      "epoch:8 step:8276 [D loss: 0.615255, acc.: 65.62%] [G loss: 1.171015]\n",
      "epoch:8 step:8277 [D loss: 0.583293, acc.: 73.44%] [G loss: 1.227681]\n",
      "epoch:8 step:8278 [D loss: 0.639266, acc.: 64.06%] [G loss: 0.786789]\n",
      "epoch:8 step:8279 [D loss: 0.538552, acc.: 67.97%] [G loss: 1.331103]\n",
      "epoch:8 step:8280 [D loss: 0.603482, acc.: 65.62%] [G loss: 1.351466]\n",
      "epoch:8 step:8281 [D loss: 0.587730, acc.: 67.97%] [G loss: 1.145349]\n",
      "epoch:8 step:8282 [D loss: 0.735577, acc.: 49.22%] [G loss: 1.063039]\n",
      "epoch:8 step:8283 [D loss: 0.614055, acc.: 64.06%] [G loss: 1.198832]\n",
      "epoch:8 step:8284 [D loss: 0.619582, acc.: 66.41%] [G loss: 1.028095]\n",
      "epoch:8 step:8285 [D loss: 0.617001, acc.: 66.41%] [G loss: 1.175925]\n",
      "epoch:8 step:8286 [D loss: 0.569711, acc.: 72.66%] [G loss: 1.054066]\n",
      "epoch:8 step:8287 [D loss: 0.606587, acc.: 70.31%] [G loss: 1.227972]\n",
      "epoch:8 step:8288 [D loss: 0.745441, acc.: 53.91%] [G loss: 1.019496]\n",
      "epoch:8 step:8289 [D loss: 0.626378, acc.: 65.62%] [G loss: 1.127879]\n",
      "epoch:8 step:8290 [D loss: 0.563825, acc.: 69.53%] [G loss: 0.972799]\n",
      "epoch:8 step:8291 [D loss: 0.622728, acc.: 67.19%] [G loss: 1.164276]\n",
      "epoch:8 step:8292 [D loss: 0.777833, acc.: 46.88%] [G loss: 1.058133]\n",
      "epoch:8 step:8293 [D loss: 0.540620, acc.: 71.09%] [G loss: 1.254984]\n",
      "epoch:8 step:8294 [D loss: 0.686306, acc.: 60.16%] [G loss: 1.057361]\n",
      "epoch:8 step:8295 [D loss: 0.586562, acc.: 71.09%] [G loss: 1.165564]\n",
      "epoch:8 step:8296 [D loss: 0.585242, acc.: 69.53%] [G loss: 1.266473]\n",
      "epoch:8 step:8297 [D loss: 0.610323, acc.: 67.97%] [G loss: 1.075767]\n",
      "epoch:8 step:8298 [D loss: 0.564866, acc.: 74.22%] [G loss: 1.140942]\n",
      "epoch:8 step:8299 [D loss: 0.624674, acc.: 67.97%] [G loss: 1.097566]\n",
      "epoch:8 step:8300 [D loss: 0.652112, acc.: 65.62%] [G loss: 1.140348]\n",
      "epoch:8 step:8301 [D loss: 0.687969, acc.: 58.59%] [G loss: 1.263752]\n",
      "epoch:8 step:8302 [D loss: 0.601698, acc.: 68.75%] [G loss: 0.906877]\n",
      "epoch:8 step:8303 [D loss: 0.496002, acc.: 79.69%] [G loss: 1.262169]\n",
      "epoch:8 step:8304 [D loss: 0.558551, acc.: 70.31%] [G loss: 1.009271]\n",
      "epoch:8 step:8305 [D loss: 0.538969, acc.: 73.44%] [G loss: 1.146693]\n",
      "epoch:8 step:8306 [D loss: 0.552290, acc.: 74.22%] [G loss: 1.414153]\n",
      "epoch:8 step:8307 [D loss: 0.555719, acc.: 69.53%] [G loss: 1.138197]\n",
      "epoch:8 step:8308 [D loss: 0.597931, acc.: 65.62%] [G loss: 1.157484]\n",
      "epoch:8 step:8309 [D loss: 0.511143, acc.: 80.47%] [G loss: 1.365557]\n",
      "epoch:8 step:8310 [D loss: 0.549421, acc.: 77.34%] [G loss: 1.079813]\n",
      "epoch:8 step:8311 [D loss: 0.580551, acc.: 70.31%] [G loss: 0.953880]\n",
      "epoch:8 step:8312 [D loss: 0.498847, acc.: 79.69%] [G loss: 1.325552]\n",
      "epoch:8 step:8313 [D loss: 0.566041, acc.: 71.88%] [G loss: 1.144345]\n",
      "epoch:8 step:8314 [D loss: 0.543929, acc.: 75.00%] [G loss: 1.035752]\n",
      "epoch:8 step:8315 [D loss: 0.664433, acc.: 63.28%] [G loss: 1.111650]\n",
      "epoch:8 step:8316 [D loss: 0.629722, acc.: 59.38%] [G loss: 1.249106]\n",
      "epoch:8 step:8317 [D loss: 0.629928, acc.: 67.97%] [G loss: 1.031526]\n",
      "epoch:8 step:8318 [D loss: 0.629463, acc.: 64.06%] [G loss: 1.302572]\n",
      "epoch:8 step:8319 [D loss: 0.733937, acc.: 55.47%] [G loss: 0.963826]\n",
      "epoch:8 step:8320 [D loss: 0.536240, acc.: 77.34%] [G loss: 1.109719]\n",
      "epoch:8 step:8321 [D loss: 0.500297, acc.: 75.78%] [G loss: 1.161897]\n",
      "epoch:8 step:8322 [D loss: 0.620125, acc.: 60.94%] [G loss: 1.001356]\n",
      "epoch:8 step:8323 [D loss: 0.611424, acc.: 65.62%] [G loss: 0.964817]\n",
      "epoch:8 step:8324 [D loss: 0.633986, acc.: 66.41%] [G loss: 1.179736]\n",
      "epoch:8 step:8325 [D loss: 0.644712, acc.: 66.41%] [G loss: 0.976272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8326 [D loss: 0.721030, acc.: 51.56%] [G loss: 1.137145]\n",
      "epoch:8 step:8327 [D loss: 0.567718, acc.: 71.88%] [G loss: 1.009622]\n",
      "epoch:8 step:8328 [D loss: 0.567913, acc.: 67.97%] [G loss: 1.046462]\n",
      "epoch:8 step:8329 [D loss: 0.588909, acc.: 71.09%] [G loss: 1.139143]\n",
      "epoch:8 step:8330 [D loss: 0.667910, acc.: 60.94%] [G loss: 0.929406]\n",
      "epoch:8 step:8331 [D loss: 0.645362, acc.: 62.50%] [G loss: 1.033454]\n",
      "epoch:8 step:8332 [D loss: 0.604120, acc.: 67.97%] [G loss: 1.100481]\n",
      "epoch:8 step:8333 [D loss: 0.648176, acc.: 60.16%] [G loss: 1.143111]\n",
      "epoch:8 step:8334 [D loss: 0.584910, acc.: 70.31%] [G loss: 1.041998]\n",
      "epoch:8 step:8335 [D loss: 0.724043, acc.: 54.69%] [G loss: 1.033155]\n",
      "epoch:8 step:8336 [D loss: 0.656765, acc.: 63.28%] [G loss: 0.978506]\n",
      "epoch:8 step:8337 [D loss: 0.534884, acc.: 76.56%] [G loss: 1.170638]\n",
      "epoch:8 step:8338 [D loss: 0.584242, acc.: 69.53%] [G loss: 1.265758]\n",
      "epoch:8 step:8339 [D loss: 0.628508, acc.: 62.50%] [G loss: 1.048067]\n",
      "epoch:8 step:8340 [D loss: 0.647678, acc.: 61.72%] [G loss: 1.007040]\n",
      "epoch:8 step:8341 [D loss: 0.649062, acc.: 62.50%] [G loss: 1.142555]\n",
      "epoch:8 step:8342 [D loss: 0.650652, acc.: 57.03%] [G loss: 1.008671]\n",
      "epoch:8 step:8343 [D loss: 0.633828, acc.: 63.28%] [G loss: 0.995354]\n",
      "epoch:8 step:8344 [D loss: 0.570937, acc.: 69.53%] [G loss: 1.269081]\n",
      "epoch:8 step:8345 [D loss: 0.694964, acc.: 56.25%] [G loss: 1.063300]\n",
      "epoch:8 step:8346 [D loss: 0.515668, acc.: 73.44%] [G loss: 1.180209]\n",
      "epoch:8 step:8347 [D loss: 0.618484, acc.: 69.53%] [G loss: 1.134696]\n",
      "epoch:8 step:8348 [D loss: 0.508065, acc.: 78.12%] [G loss: 1.180675]\n",
      "epoch:8 step:8349 [D loss: 0.601593, acc.: 65.62%] [G loss: 1.145935]\n",
      "epoch:8 step:8350 [D loss: 0.542034, acc.: 70.31%] [G loss: 1.275469]\n",
      "epoch:8 step:8351 [D loss: 0.686556, acc.: 62.50%] [G loss: 0.976793]\n",
      "epoch:8 step:8352 [D loss: 0.547234, acc.: 71.09%] [G loss: 1.175625]\n",
      "epoch:8 step:8353 [D loss: 0.619147, acc.: 64.84%] [G loss: 1.136839]\n",
      "epoch:8 step:8354 [D loss: 0.591924, acc.: 67.97%] [G loss: 1.241001]\n",
      "epoch:8 step:8355 [D loss: 0.604736, acc.: 67.19%] [G loss: 1.237257]\n",
      "epoch:8 step:8356 [D loss: 0.670215, acc.: 57.81%] [G loss: 1.020555]\n",
      "epoch:8 step:8357 [D loss: 0.670954, acc.: 61.72%] [G loss: 1.094202]\n",
      "epoch:8 step:8358 [D loss: 0.698662, acc.: 58.59%] [G loss: 1.154353]\n",
      "epoch:8 step:8359 [D loss: 0.601034, acc.: 71.09%] [G loss: 1.037249]\n",
      "epoch:8 step:8360 [D loss: 0.666091, acc.: 62.50%] [G loss: 1.146008]\n",
      "epoch:8 step:8361 [D loss: 0.575234, acc.: 67.97%] [G loss: 1.184491]\n",
      "epoch:8 step:8362 [D loss: 0.543238, acc.: 72.66%] [G loss: 1.150119]\n",
      "epoch:8 step:8363 [D loss: 0.645948, acc.: 67.19%] [G loss: 1.147207]\n",
      "epoch:8 step:8364 [D loss: 0.500086, acc.: 79.69%] [G loss: 1.283263]\n",
      "epoch:8 step:8365 [D loss: 0.625977, acc.: 63.28%] [G loss: 1.224592]\n",
      "epoch:8 step:8366 [D loss: 0.577419, acc.: 67.19%] [G loss: 1.225264]\n",
      "epoch:8 step:8367 [D loss: 0.554146, acc.: 74.22%] [G loss: 1.227619]\n",
      "epoch:8 step:8368 [D loss: 0.621557, acc.: 64.84%] [G loss: 1.188986]\n",
      "epoch:8 step:8369 [D loss: 0.535947, acc.: 76.56%] [G loss: 1.065647]\n",
      "epoch:8 step:8370 [D loss: 0.599052, acc.: 70.31%] [G loss: 1.292715]\n",
      "epoch:8 step:8371 [D loss: 0.579699, acc.: 69.53%] [G loss: 1.223894]\n",
      "epoch:8 step:8372 [D loss: 0.702268, acc.: 58.59%] [G loss: 1.062099]\n",
      "epoch:8 step:8373 [D loss: 0.655303, acc.: 60.94%] [G loss: 1.038720]\n",
      "epoch:8 step:8374 [D loss: 0.627295, acc.: 64.06%] [G loss: 1.392664]\n",
      "epoch:8 step:8375 [D loss: 0.651493, acc.: 59.38%] [G loss: 1.250203]\n",
      "epoch:8 step:8376 [D loss: 0.686341, acc.: 54.69%] [G loss: 1.178670]\n",
      "epoch:8 step:8377 [D loss: 0.597156, acc.: 64.84%] [G loss: 0.965850]\n",
      "epoch:8 step:8378 [D loss: 0.509600, acc.: 76.56%] [G loss: 1.280190]\n",
      "epoch:8 step:8379 [D loss: 0.690832, acc.: 56.25%] [G loss: 0.976868]\n",
      "epoch:8 step:8380 [D loss: 0.551618, acc.: 74.22%] [G loss: 1.345525]\n",
      "epoch:8 step:8381 [D loss: 0.627294, acc.: 67.97%] [G loss: 1.097105]\n",
      "epoch:8 step:8382 [D loss: 0.601475, acc.: 64.06%] [G loss: 1.260033]\n",
      "epoch:8 step:8383 [D loss: 0.594857, acc.: 67.97%] [G loss: 1.068491]\n",
      "epoch:8 step:8384 [D loss: 0.624484, acc.: 64.84%] [G loss: 1.345956]\n",
      "epoch:8 step:8385 [D loss: 0.572844, acc.: 75.78%] [G loss: 1.077338]\n",
      "epoch:8 step:8386 [D loss: 0.635266, acc.: 57.81%] [G loss: 1.028445]\n",
      "epoch:8 step:8387 [D loss: 0.644463, acc.: 67.19%] [G loss: 1.148514]\n",
      "epoch:8 step:8388 [D loss: 0.507963, acc.: 75.78%] [G loss: 1.142144]\n",
      "epoch:8 step:8389 [D loss: 0.494703, acc.: 77.34%] [G loss: 1.084020]\n",
      "epoch:8 step:8390 [D loss: 0.693905, acc.: 58.59%] [G loss: 0.901163]\n",
      "epoch:8 step:8391 [D loss: 0.542594, acc.: 75.00%] [G loss: 1.538064]\n",
      "epoch:8 step:8392 [D loss: 0.614504, acc.: 67.97%] [G loss: 1.341899]\n",
      "epoch:8 step:8393 [D loss: 0.578941, acc.: 71.09%] [G loss: 1.091319]\n",
      "epoch:8 step:8394 [D loss: 0.663938, acc.: 60.94%] [G loss: 0.965749]\n",
      "epoch:8 step:8395 [D loss: 0.537670, acc.: 73.44%] [G loss: 1.371753]\n",
      "epoch:8 step:8396 [D loss: 0.731494, acc.: 51.56%] [G loss: 1.030699]\n",
      "epoch:8 step:8397 [D loss: 0.628984, acc.: 68.75%] [G loss: 1.193535]\n",
      "epoch:8 step:8398 [D loss: 0.554905, acc.: 72.66%] [G loss: 1.147756]\n",
      "epoch:8 step:8399 [D loss: 0.554832, acc.: 71.88%] [G loss: 1.321809]\n",
      "epoch:8 step:8400 [D loss: 0.604076, acc.: 71.88%] [G loss: 1.088037]\n",
      "##############\n",
      "[2.63568572 1.78664673 1.6133551  2.5413577  0.71617029 5.33756858\n",
      " 1.86647991 2.62988711 3.76448266 6.344848  ]\n",
      "##########\n",
      "epoch:8 step:8401 [D loss: 0.588244, acc.: 67.97%] [G loss: 1.174273]\n",
      "epoch:8 step:8402 [D loss: 0.612478, acc.: 63.28%] [G loss: 1.037933]\n",
      "epoch:8 step:8403 [D loss: 0.637852, acc.: 65.62%] [G loss: 1.043759]\n",
      "epoch:8 step:8404 [D loss: 0.699009, acc.: 52.34%] [G loss: 1.085382]\n",
      "epoch:8 step:8405 [D loss: 0.536008, acc.: 73.44%] [G loss: 1.098655]\n",
      "epoch:8 step:8406 [D loss: 0.548787, acc.: 73.44%] [G loss: 1.294778]\n",
      "epoch:8 step:8407 [D loss: 0.524144, acc.: 78.12%] [G loss: 1.184275]\n",
      "epoch:8 step:8408 [D loss: 0.638650, acc.: 63.28%] [G loss: 1.173394]\n",
      "epoch:8 step:8409 [D loss: 0.756862, acc.: 46.88%] [G loss: 1.010559]\n",
      "epoch:8 step:8410 [D loss: 0.663188, acc.: 57.81%] [G loss: 1.292216]\n",
      "epoch:8 step:8411 [D loss: 0.641657, acc.: 60.94%] [G loss: 1.170318]\n",
      "epoch:8 step:8412 [D loss: 0.572691, acc.: 71.09%] [G loss: 1.250298]\n",
      "epoch:8 step:8413 [D loss: 0.570184, acc.: 71.09%] [G loss: 1.105863]\n",
      "epoch:8 step:8414 [D loss: 0.570916, acc.: 72.66%] [G loss: 1.134037]\n",
      "epoch:8 step:8415 [D loss: 0.531512, acc.: 74.22%] [G loss: 1.154432]\n",
      "epoch:8 step:8416 [D loss: 0.611138, acc.: 65.62%] [G loss: 1.184546]\n",
      "epoch:8 step:8417 [D loss: 0.550468, acc.: 75.78%] [G loss: 1.014744]\n",
      "epoch:8 step:8418 [D loss: 0.582767, acc.: 66.41%] [G loss: 1.195892]\n",
      "epoch:8 step:8419 [D loss: 0.593537, acc.: 67.97%] [G loss: 1.057597]\n",
      "epoch:8 step:8420 [D loss: 0.571345, acc.: 64.84%] [G loss: 1.132269]\n",
      "epoch:8 step:8421 [D loss: 0.540595, acc.: 74.22%] [G loss: 1.166091]\n",
      "epoch:8 step:8422 [D loss: 0.592655, acc.: 71.09%] [G loss: 1.125679]\n",
      "epoch:8 step:8423 [D loss: 0.507160, acc.: 74.22%] [G loss: 1.051299]\n",
      "epoch:8 step:8424 [D loss: 0.580868, acc.: 66.41%] [G loss: 1.082033]\n",
      "epoch:8 step:8425 [D loss: 0.586438, acc.: 67.97%] [G loss: 1.043221]\n",
      "epoch:8 step:8426 [D loss: 0.605531, acc.: 67.97%] [G loss: 1.014530]\n",
      "epoch:8 step:8427 [D loss: 0.605750, acc.: 67.19%] [G loss: 1.132504]\n",
      "epoch:8 step:8428 [D loss: 0.670204, acc.: 59.38%] [G loss: 1.145218]\n",
      "epoch:8 step:8429 [D loss: 0.730913, acc.: 54.69%] [G loss: 0.976681]\n",
      "epoch:8 step:8430 [D loss: 0.573300, acc.: 70.31%] [G loss: 0.954267]\n",
      "epoch:8 step:8431 [D loss: 0.563526, acc.: 69.53%] [G loss: 1.258463]\n",
      "epoch:8 step:8432 [D loss: 0.600230, acc.: 66.41%] [G loss: 1.032489]\n",
      "epoch:8 step:8433 [D loss: 0.639113, acc.: 62.50%] [G loss: 1.060474]\n",
      "epoch:9 step:8434 [D loss: 0.730898, acc.: 51.56%] [G loss: 0.976133]\n",
      "epoch:9 step:8435 [D loss: 0.624902, acc.: 66.41%] [G loss: 1.205264]\n",
      "epoch:9 step:8436 [D loss: 0.677681, acc.: 58.59%] [G loss: 0.901573]\n",
      "epoch:9 step:8437 [D loss: 0.526894, acc.: 75.00%] [G loss: 1.208927]\n",
      "epoch:9 step:8438 [D loss: 0.627485, acc.: 64.84%] [G loss: 1.032411]\n",
      "epoch:9 step:8439 [D loss: 0.690300, acc.: 57.81%] [G loss: 1.148895]\n",
      "epoch:9 step:8440 [D loss: 0.635356, acc.: 64.06%] [G loss: 1.131491]\n",
      "epoch:9 step:8441 [D loss: 0.522532, acc.: 75.00%] [G loss: 1.052988]\n",
      "epoch:9 step:8442 [D loss: 0.532762, acc.: 74.22%] [G loss: 1.274443]\n",
      "epoch:9 step:8443 [D loss: 0.597592, acc.: 69.53%] [G loss: 1.109235]\n",
      "epoch:9 step:8444 [D loss: 0.559498, acc.: 76.56%] [G loss: 0.953696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8445 [D loss: 0.515617, acc.: 77.34%] [G loss: 1.421188]\n",
      "epoch:9 step:8446 [D loss: 0.621579, acc.: 65.62%] [G loss: 1.214473]\n",
      "epoch:9 step:8447 [D loss: 0.683251, acc.: 55.47%] [G loss: 1.136310]\n",
      "epoch:9 step:8448 [D loss: 0.547348, acc.: 72.66%] [G loss: 1.078333]\n",
      "epoch:9 step:8449 [D loss: 0.540043, acc.: 71.88%] [G loss: 1.250862]\n",
      "epoch:9 step:8450 [D loss: 0.623946, acc.: 67.19%] [G loss: 1.091321]\n",
      "epoch:9 step:8451 [D loss: 0.663276, acc.: 60.16%] [G loss: 1.141403]\n",
      "epoch:9 step:8452 [D loss: 0.684615, acc.: 54.69%] [G loss: 1.050158]\n",
      "epoch:9 step:8453 [D loss: 0.601471, acc.: 67.19%] [G loss: 1.359223]\n",
      "epoch:9 step:8454 [D loss: 0.642794, acc.: 61.72%] [G loss: 0.976878]\n",
      "epoch:9 step:8455 [D loss: 0.564412, acc.: 71.88%] [G loss: 1.070831]\n",
      "epoch:9 step:8456 [D loss: 0.662517, acc.: 60.94%] [G loss: 1.157336]\n",
      "epoch:9 step:8457 [D loss: 0.550408, acc.: 76.56%] [G loss: 1.364120]\n",
      "epoch:9 step:8458 [D loss: 0.607127, acc.: 66.41%] [G loss: 1.375757]\n",
      "epoch:9 step:8459 [D loss: 0.626154, acc.: 66.41%] [G loss: 1.310323]\n",
      "epoch:9 step:8460 [D loss: 0.629926, acc.: 65.62%] [G loss: 1.185511]\n",
      "epoch:9 step:8461 [D loss: 0.532849, acc.: 74.22%] [G loss: 1.160795]\n",
      "epoch:9 step:8462 [D loss: 0.648380, acc.: 57.81%] [G loss: 1.194441]\n",
      "epoch:9 step:8463 [D loss: 0.646646, acc.: 60.94%] [G loss: 1.127855]\n",
      "epoch:9 step:8464 [D loss: 0.664370, acc.: 56.25%] [G loss: 1.075127]\n",
      "epoch:9 step:8465 [D loss: 0.597676, acc.: 71.09%] [G loss: 1.292503]\n",
      "epoch:9 step:8466 [D loss: 0.704224, acc.: 54.69%] [G loss: 1.101571]\n",
      "epoch:9 step:8467 [D loss: 0.608928, acc.: 67.97%] [G loss: 1.215278]\n",
      "epoch:9 step:8468 [D loss: 0.646657, acc.: 62.50%] [G loss: 1.048066]\n",
      "epoch:9 step:8469 [D loss: 0.596533, acc.: 69.53%] [G loss: 1.159328]\n",
      "epoch:9 step:8470 [D loss: 0.615090, acc.: 69.53%] [G loss: 1.262958]\n",
      "epoch:9 step:8471 [D loss: 0.592959, acc.: 71.88%] [G loss: 1.340288]\n",
      "epoch:9 step:8472 [D loss: 0.616181, acc.: 64.06%] [G loss: 1.169766]\n",
      "epoch:9 step:8473 [D loss: 0.640300, acc.: 59.38%] [G loss: 1.125648]\n",
      "epoch:9 step:8474 [D loss: 0.590387, acc.: 71.88%] [G loss: 1.173540]\n",
      "epoch:9 step:8475 [D loss: 0.603184, acc.: 67.97%] [G loss: 1.132684]\n",
      "epoch:9 step:8476 [D loss: 0.594278, acc.: 70.31%] [G loss: 1.331773]\n",
      "epoch:9 step:8477 [D loss: 0.618986, acc.: 63.28%] [G loss: 1.237131]\n",
      "epoch:9 step:8478 [D loss: 0.604191, acc.: 67.97%] [G loss: 1.206640]\n",
      "epoch:9 step:8479 [D loss: 0.599363, acc.: 64.84%] [G loss: 1.184481]\n",
      "epoch:9 step:8480 [D loss: 0.529191, acc.: 75.78%] [G loss: 1.384944]\n",
      "epoch:9 step:8481 [D loss: 0.664607, acc.: 60.16%] [G loss: 0.937819]\n",
      "epoch:9 step:8482 [D loss: 0.579581, acc.: 70.31%] [G loss: 1.052382]\n",
      "epoch:9 step:8483 [D loss: 0.519634, acc.: 77.34%] [G loss: 0.986689]\n",
      "epoch:9 step:8484 [D loss: 0.557214, acc.: 72.66%] [G loss: 1.044634]\n",
      "epoch:9 step:8485 [D loss: 0.501410, acc.: 78.91%] [G loss: 1.184383]\n",
      "epoch:9 step:8486 [D loss: 0.650031, acc.: 63.28%] [G loss: 0.964226]\n",
      "epoch:9 step:8487 [D loss: 0.611306, acc.: 65.62%] [G loss: 1.125009]\n",
      "epoch:9 step:8488 [D loss: 0.524234, acc.: 72.66%] [G loss: 1.214277]\n",
      "epoch:9 step:8489 [D loss: 0.611228, acc.: 67.19%] [G loss: 1.297150]\n",
      "epoch:9 step:8490 [D loss: 0.610806, acc.: 67.19%] [G loss: 1.274911]\n",
      "epoch:9 step:8491 [D loss: 0.562319, acc.: 66.41%] [G loss: 1.305215]\n",
      "epoch:9 step:8492 [D loss: 0.506181, acc.: 78.91%] [G loss: 0.994525]\n",
      "epoch:9 step:8493 [D loss: 0.566157, acc.: 74.22%] [G loss: 0.916346]\n",
      "epoch:9 step:8494 [D loss: 0.536129, acc.: 75.78%] [G loss: 1.168902]\n",
      "epoch:9 step:8495 [D loss: 0.544273, acc.: 71.88%] [G loss: 1.225778]\n",
      "epoch:9 step:8496 [D loss: 0.626305, acc.: 64.06%] [G loss: 1.041386]\n",
      "epoch:9 step:8497 [D loss: 0.612254, acc.: 69.53%] [G loss: 1.289418]\n",
      "epoch:9 step:8498 [D loss: 0.453718, acc.: 84.38%] [G loss: 1.377593]\n",
      "epoch:9 step:8499 [D loss: 0.530986, acc.: 75.00%] [G loss: 1.221710]\n",
      "epoch:9 step:8500 [D loss: 0.538908, acc.: 78.12%] [G loss: 1.146269]\n",
      "epoch:9 step:8501 [D loss: 0.550544, acc.: 75.00%] [G loss: 1.213946]\n",
      "epoch:9 step:8502 [D loss: 0.613218, acc.: 73.44%] [G loss: 1.136919]\n",
      "epoch:9 step:8503 [D loss: 0.647894, acc.: 57.81%] [G loss: 1.161801]\n",
      "epoch:9 step:8504 [D loss: 0.563154, acc.: 70.31%] [G loss: 1.036974]\n",
      "epoch:9 step:8505 [D loss: 0.581520, acc.: 67.19%] [G loss: 1.054726]\n",
      "epoch:9 step:8506 [D loss: 0.641060, acc.: 62.50%] [G loss: 1.121581]\n",
      "epoch:9 step:8507 [D loss: 0.621638, acc.: 65.62%] [G loss: 1.132905]\n",
      "epoch:9 step:8508 [D loss: 0.618408, acc.: 64.06%] [G loss: 1.256240]\n",
      "epoch:9 step:8509 [D loss: 0.619138, acc.: 71.09%] [G loss: 1.208602]\n",
      "epoch:9 step:8510 [D loss: 0.584514, acc.: 69.53%] [G loss: 0.788248]\n",
      "epoch:9 step:8511 [D loss: 0.608476, acc.: 66.41%] [G loss: 1.171515]\n",
      "epoch:9 step:8512 [D loss: 0.569178, acc.: 67.19%] [G loss: 1.145862]\n",
      "epoch:9 step:8513 [D loss: 0.650072, acc.: 60.94%] [G loss: 1.078202]\n",
      "epoch:9 step:8514 [D loss: 0.602549, acc.: 70.31%] [G loss: 1.195720]\n",
      "epoch:9 step:8515 [D loss: 0.622754, acc.: 66.41%] [G loss: 1.492385]\n",
      "epoch:9 step:8516 [D loss: 0.851081, acc.: 40.62%] [G loss: 0.891133]\n",
      "epoch:9 step:8517 [D loss: 0.676391, acc.: 62.50%] [G loss: 1.033928]\n",
      "epoch:9 step:8518 [D loss: 0.549896, acc.: 78.91%] [G loss: 1.278170]\n",
      "epoch:9 step:8519 [D loss: 0.651435, acc.: 64.84%] [G loss: 1.176213]\n",
      "epoch:9 step:8520 [D loss: 0.562688, acc.: 71.88%] [G loss: 1.162248]\n",
      "epoch:9 step:8521 [D loss: 0.608997, acc.: 65.62%] [G loss: 1.011913]\n",
      "epoch:9 step:8522 [D loss: 0.615342, acc.: 64.84%] [G loss: 1.113708]\n",
      "epoch:9 step:8523 [D loss: 0.588848, acc.: 66.41%] [G loss: 1.251095]\n",
      "epoch:9 step:8524 [D loss: 0.663204, acc.: 59.38%] [G loss: 1.243603]\n",
      "epoch:9 step:8525 [D loss: 0.513746, acc.: 74.22%] [G loss: 1.330292]\n",
      "epoch:9 step:8526 [D loss: 0.597090, acc.: 64.84%] [G loss: 1.194466]\n",
      "epoch:9 step:8527 [D loss: 0.492583, acc.: 77.34%] [G loss: 1.270750]\n",
      "epoch:9 step:8528 [D loss: 0.587573, acc.: 68.75%] [G loss: 1.259300]\n",
      "epoch:9 step:8529 [D loss: 0.655619, acc.: 60.94%] [G loss: 1.334133]\n",
      "epoch:9 step:8530 [D loss: 0.640825, acc.: 66.41%] [G loss: 1.118398]\n",
      "epoch:9 step:8531 [D loss: 0.504505, acc.: 76.56%] [G loss: 1.112168]\n",
      "epoch:9 step:8532 [D loss: 0.624295, acc.: 67.19%] [G loss: 1.345295]\n",
      "epoch:9 step:8533 [D loss: 0.734046, acc.: 53.91%] [G loss: 0.897495]\n",
      "epoch:9 step:8534 [D loss: 0.642341, acc.: 67.19%] [G loss: 0.984669]\n",
      "epoch:9 step:8535 [D loss: 0.728053, acc.: 58.59%] [G loss: 1.190752]\n",
      "epoch:9 step:8536 [D loss: 0.544185, acc.: 74.22%] [G loss: 1.110298]\n",
      "epoch:9 step:8537 [D loss: 0.600080, acc.: 64.84%] [G loss: 1.167814]\n",
      "epoch:9 step:8538 [D loss: 0.560710, acc.: 71.09%] [G loss: 1.310909]\n",
      "epoch:9 step:8539 [D loss: 0.614014, acc.: 64.84%] [G loss: 1.142519]\n",
      "epoch:9 step:8540 [D loss: 0.545419, acc.: 71.88%] [G loss: 1.079580]\n",
      "epoch:9 step:8541 [D loss: 0.521766, acc.: 71.88%] [G loss: 1.185954]\n",
      "epoch:9 step:8542 [D loss: 0.491026, acc.: 77.34%] [G loss: 1.101931]\n",
      "epoch:9 step:8543 [D loss: 0.717333, acc.: 55.47%] [G loss: 1.166707]\n",
      "epoch:9 step:8544 [D loss: 0.635072, acc.: 65.62%] [G loss: 1.073421]\n",
      "epoch:9 step:8545 [D loss: 0.678491, acc.: 57.03%] [G loss: 1.128131]\n",
      "epoch:9 step:8546 [D loss: 0.558397, acc.: 75.78%] [G loss: 1.149072]\n",
      "epoch:9 step:8547 [D loss: 0.562930, acc.: 68.75%] [G loss: 1.255597]\n",
      "epoch:9 step:8548 [D loss: 0.751786, acc.: 48.44%] [G loss: 1.218003]\n",
      "epoch:9 step:8549 [D loss: 0.679981, acc.: 53.91%] [G loss: 1.281128]\n",
      "epoch:9 step:8550 [D loss: 0.664272, acc.: 59.38%] [G loss: 1.090337]\n",
      "epoch:9 step:8551 [D loss: 0.588936, acc.: 73.44%] [G loss: 1.210938]\n",
      "epoch:9 step:8552 [D loss: 0.613473, acc.: 68.75%] [G loss: 1.216537]\n",
      "epoch:9 step:8553 [D loss: 0.570836, acc.: 70.31%] [G loss: 1.325700]\n",
      "epoch:9 step:8554 [D loss: 0.667803, acc.: 66.41%] [G loss: 1.294811]\n",
      "epoch:9 step:8555 [D loss: 0.648363, acc.: 58.59%] [G loss: 1.198257]\n",
      "epoch:9 step:8556 [D loss: 0.701414, acc.: 53.91%] [G loss: 1.330473]\n",
      "epoch:9 step:8557 [D loss: 0.560923, acc.: 70.31%] [G loss: 1.216611]\n",
      "epoch:9 step:8558 [D loss: 0.592155, acc.: 66.41%] [G loss: 1.136682]\n",
      "epoch:9 step:8559 [D loss: 0.711990, acc.: 58.59%] [G loss: 0.836672]\n",
      "epoch:9 step:8560 [D loss: 0.564434, acc.: 71.88%] [G loss: 1.015433]\n",
      "epoch:9 step:8561 [D loss: 0.607849, acc.: 64.84%] [G loss: 1.327176]\n",
      "epoch:9 step:8562 [D loss: 0.533502, acc.: 75.00%] [G loss: 1.178485]\n",
      "epoch:9 step:8563 [D loss: 0.664269, acc.: 54.69%] [G loss: 1.215477]\n",
      "epoch:9 step:8564 [D loss: 0.606158, acc.: 68.75%] [G loss: 1.345731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8565 [D loss: 0.627849, acc.: 60.16%] [G loss: 1.347007]\n",
      "epoch:9 step:8566 [D loss: 0.693727, acc.: 57.81%] [G loss: 0.863209]\n",
      "epoch:9 step:8567 [D loss: 0.589609, acc.: 69.53%] [G loss: 1.084727]\n",
      "epoch:9 step:8568 [D loss: 0.639085, acc.: 64.06%] [G loss: 1.351930]\n",
      "epoch:9 step:8569 [D loss: 0.731274, acc.: 53.91%] [G loss: 1.042151]\n",
      "epoch:9 step:8570 [D loss: 0.631242, acc.: 62.50%] [G loss: 1.124240]\n",
      "epoch:9 step:8571 [D loss: 0.610137, acc.: 64.84%] [G loss: 1.233663]\n",
      "epoch:9 step:8572 [D loss: 0.599072, acc.: 64.84%] [G loss: 1.072169]\n",
      "epoch:9 step:8573 [D loss: 0.653160, acc.: 64.84%] [G loss: 1.062022]\n",
      "epoch:9 step:8574 [D loss: 0.784360, acc.: 50.78%] [G loss: 1.009931]\n",
      "epoch:9 step:8575 [D loss: 0.573545, acc.: 71.88%] [G loss: 1.214834]\n",
      "epoch:9 step:8576 [D loss: 0.654959, acc.: 60.16%] [G loss: 1.201114]\n",
      "epoch:9 step:8577 [D loss: 0.616620, acc.: 67.97%] [G loss: 1.118224]\n",
      "epoch:9 step:8578 [D loss: 0.654166, acc.: 64.06%] [G loss: 0.974942]\n",
      "epoch:9 step:8579 [D loss: 0.649684, acc.: 64.84%] [G loss: 1.131343]\n",
      "epoch:9 step:8580 [D loss: 0.633981, acc.: 60.94%] [G loss: 0.935644]\n",
      "epoch:9 step:8581 [D loss: 0.585719, acc.: 70.31%] [G loss: 1.011286]\n",
      "epoch:9 step:8582 [D loss: 0.626563, acc.: 66.41%] [G loss: 1.233423]\n",
      "epoch:9 step:8583 [D loss: 0.567257, acc.: 69.53%] [G loss: 1.277814]\n",
      "epoch:9 step:8584 [D loss: 0.596564, acc.: 65.62%] [G loss: 1.122335]\n",
      "epoch:9 step:8585 [D loss: 0.637085, acc.: 64.06%] [G loss: 1.153203]\n",
      "epoch:9 step:8586 [D loss: 0.562278, acc.: 73.44%] [G loss: 1.253285]\n",
      "epoch:9 step:8587 [D loss: 0.547509, acc.: 71.88%] [G loss: 1.262841]\n",
      "epoch:9 step:8588 [D loss: 0.635793, acc.: 64.84%] [G loss: 0.959717]\n",
      "epoch:9 step:8589 [D loss: 0.614563, acc.: 66.41%] [G loss: 1.139832]\n",
      "epoch:9 step:8590 [D loss: 0.669243, acc.: 63.28%] [G loss: 1.242644]\n",
      "epoch:9 step:8591 [D loss: 0.606004, acc.: 67.19%] [G loss: 1.059738]\n",
      "epoch:9 step:8592 [D loss: 0.543946, acc.: 73.44%] [G loss: 1.092113]\n",
      "epoch:9 step:8593 [D loss: 0.537252, acc.: 72.66%] [G loss: 1.154996]\n",
      "epoch:9 step:8594 [D loss: 0.595869, acc.: 69.53%] [G loss: 0.974834]\n",
      "epoch:9 step:8595 [D loss: 0.655959, acc.: 60.94%] [G loss: 0.996537]\n",
      "epoch:9 step:8596 [D loss: 0.562682, acc.: 73.44%] [G loss: 1.193512]\n",
      "epoch:9 step:8597 [D loss: 0.614849, acc.: 64.84%] [G loss: 1.050153]\n",
      "epoch:9 step:8598 [D loss: 0.581163, acc.: 71.09%] [G loss: 1.085476]\n",
      "epoch:9 step:8599 [D loss: 0.573092, acc.: 67.97%] [G loss: 1.186602]\n",
      "epoch:9 step:8600 [D loss: 0.534802, acc.: 74.22%] [G loss: 1.175836]\n",
      "##############\n",
      "[2.72971416 2.14782387 1.82569607 2.63061437 0.86255859 5.99281994\n",
      " 1.98491858 2.82614584 3.96934807 4.32337911]\n",
      "##########\n",
      "epoch:9 step:8601 [D loss: 0.665156, acc.: 64.06%] [G loss: 1.078078]\n",
      "epoch:9 step:8602 [D loss: 0.589948, acc.: 69.53%] [G loss: 1.090104]\n",
      "epoch:9 step:8603 [D loss: 0.623579, acc.: 64.84%] [G loss: 1.107906]\n",
      "epoch:9 step:8604 [D loss: 0.678750, acc.: 62.50%] [G loss: 1.053494]\n",
      "epoch:9 step:8605 [D loss: 0.541202, acc.: 76.56%] [G loss: 1.058442]\n",
      "epoch:9 step:8606 [D loss: 0.649952, acc.: 66.41%] [G loss: 0.916163]\n",
      "epoch:9 step:8607 [D loss: 0.592412, acc.: 67.97%] [G loss: 1.266706]\n",
      "epoch:9 step:8608 [D loss: 0.624336, acc.: 62.50%] [G loss: 1.072909]\n",
      "epoch:9 step:8609 [D loss: 0.626916, acc.: 61.72%] [G loss: 1.246111]\n",
      "epoch:9 step:8610 [D loss: 0.675707, acc.: 63.28%] [G loss: 0.868060]\n",
      "epoch:9 step:8611 [D loss: 0.581920, acc.: 72.66%] [G loss: 1.109474]\n",
      "epoch:9 step:8612 [D loss: 0.602555, acc.: 66.41%] [G loss: 1.114484]\n",
      "epoch:9 step:8613 [D loss: 0.590484, acc.: 70.31%] [G loss: 1.229188]\n",
      "epoch:9 step:8614 [D loss: 0.604782, acc.: 60.94%] [G loss: 1.360425]\n",
      "epoch:9 step:8615 [D loss: 0.589739, acc.: 69.53%] [G loss: 1.308625]\n",
      "epoch:9 step:8616 [D loss: 0.503234, acc.: 78.12%] [G loss: 1.165295]\n",
      "epoch:9 step:8617 [D loss: 0.589143, acc.: 68.75%] [G loss: 1.243619]\n",
      "epoch:9 step:8618 [D loss: 0.692014, acc.: 57.81%] [G loss: 1.032590]\n",
      "epoch:9 step:8619 [D loss: 0.583312, acc.: 69.53%] [G loss: 1.014416]\n",
      "epoch:9 step:8620 [D loss: 0.467699, acc.: 83.59%] [G loss: 1.255273]\n",
      "epoch:9 step:8621 [D loss: 0.593023, acc.: 67.19%] [G loss: 1.038221]\n",
      "epoch:9 step:8622 [D loss: 0.514675, acc.: 78.91%] [G loss: 1.342337]\n",
      "epoch:9 step:8623 [D loss: 0.630792, acc.: 67.97%] [G loss: 0.990764]\n",
      "epoch:9 step:8624 [D loss: 0.594773, acc.: 71.09%] [G loss: 1.118909]\n",
      "epoch:9 step:8625 [D loss: 0.745674, acc.: 56.25%] [G loss: 1.209184]\n",
      "epoch:9 step:8626 [D loss: 0.580052, acc.: 70.31%] [G loss: 1.236557]\n",
      "epoch:9 step:8627 [D loss: 0.785031, acc.: 46.88%] [G loss: 1.028058]\n",
      "epoch:9 step:8628 [D loss: 0.639279, acc.: 64.06%] [G loss: 1.041717]\n",
      "epoch:9 step:8629 [D loss: 0.596874, acc.: 72.66%] [G loss: 1.104087]\n",
      "epoch:9 step:8630 [D loss: 0.567897, acc.: 64.06%] [G loss: 1.089318]\n",
      "epoch:9 step:8631 [D loss: 0.495879, acc.: 79.69%] [G loss: 1.182651]\n",
      "epoch:9 step:8632 [D loss: 0.563386, acc.: 70.31%] [G loss: 1.132242]\n",
      "epoch:9 step:8633 [D loss: 0.598172, acc.: 71.88%] [G loss: 1.044898]\n",
      "epoch:9 step:8634 [D loss: 0.582682, acc.: 68.75%] [G loss: 1.222202]\n",
      "epoch:9 step:8635 [D loss: 0.581184, acc.: 67.19%] [G loss: 1.287399]\n",
      "epoch:9 step:8636 [D loss: 0.559713, acc.: 74.22%] [G loss: 1.197494]\n",
      "epoch:9 step:8637 [D loss: 0.566825, acc.: 69.53%] [G loss: 1.082699]\n",
      "epoch:9 step:8638 [D loss: 0.711260, acc.: 55.47%] [G loss: 0.987016]\n",
      "epoch:9 step:8639 [D loss: 0.579295, acc.: 68.75%] [G loss: 1.191557]\n",
      "epoch:9 step:8640 [D loss: 0.565998, acc.: 74.22%] [G loss: 1.355383]\n",
      "epoch:9 step:8641 [D loss: 0.572381, acc.: 70.31%] [G loss: 1.206895]\n",
      "epoch:9 step:8642 [D loss: 0.644816, acc.: 67.19%] [G loss: 1.219443]\n",
      "epoch:9 step:8643 [D loss: 0.661025, acc.: 59.38%] [G loss: 0.968066]\n",
      "epoch:9 step:8644 [D loss: 0.660564, acc.: 65.62%] [G loss: 1.215358]\n",
      "epoch:9 step:8645 [D loss: 0.654250, acc.: 64.06%] [G loss: 1.180226]\n",
      "epoch:9 step:8646 [D loss: 0.594532, acc.: 69.53%] [G loss: 1.037839]\n",
      "epoch:9 step:8647 [D loss: 0.642554, acc.: 64.06%] [G loss: 1.136658]\n",
      "epoch:9 step:8648 [D loss: 0.561700, acc.: 72.66%] [G loss: 1.160730]\n",
      "epoch:9 step:8649 [D loss: 0.579872, acc.: 66.41%] [G loss: 1.102097]\n",
      "epoch:9 step:8650 [D loss: 0.590201, acc.: 67.19%] [G loss: 1.127121]\n",
      "epoch:9 step:8651 [D loss: 0.665209, acc.: 54.69%] [G loss: 1.149858]\n",
      "epoch:9 step:8652 [D loss: 0.761072, acc.: 56.25%] [G loss: 1.208321]\n",
      "epoch:9 step:8653 [D loss: 0.650423, acc.: 61.72%] [G loss: 1.159543]\n",
      "epoch:9 step:8654 [D loss: 0.675883, acc.: 59.38%] [G loss: 1.071312]\n",
      "epoch:9 step:8655 [D loss: 0.639672, acc.: 64.06%] [G loss: 1.087990]\n",
      "epoch:9 step:8656 [D loss: 0.591325, acc.: 69.53%] [G loss: 1.060785]\n",
      "epoch:9 step:8657 [D loss: 0.608868, acc.: 67.97%] [G loss: 1.212985]\n",
      "epoch:9 step:8658 [D loss: 0.595961, acc.: 65.62%] [G loss: 1.102108]\n",
      "epoch:9 step:8659 [D loss: 0.698925, acc.: 55.47%] [G loss: 0.990887]\n",
      "epoch:9 step:8660 [D loss: 0.489508, acc.: 78.91%] [G loss: 1.012806]\n",
      "epoch:9 step:8661 [D loss: 0.525529, acc.: 75.00%] [G loss: 1.242458]\n",
      "epoch:9 step:8662 [D loss: 0.579272, acc.: 68.75%] [G loss: 1.134275]\n",
      "epoch:9 step:8663 [D loss: 0.608885, acc.: 67.97%] [G loss: 0.928807]\n",
      "epoch:9 step:8664 [D loss: 0.438844, acc.: 85.16%] [G loss: 1.423640]\n",
      "epoch:9 step:8665 [D loss: 0.694100, acc.: 55.47%] [G loss: 1.236446]\n",
      "epoch:9 step:8666 [D loss: 0.575360, acc.: 72.66%] [G loss: 1.056972]\n",
      "epoch:9 step:8667 [D loss: 0.605709, acc.: 70.31%] [G loss: 1.234029]\n",
      "epoch:9 step:8668 [D loss: 0.559292, acc.: 72.66%] [G loss: 1.352505]\n",
      "epoch:9 step:8669 [D loss: 0.568074, acc.: 71.88%] [G loss: 1.110087]\n",
      "epoch:9 step:8670 [D loss: 0.632240, acc.: 66.41%] [G loss: 1.008776]\n",
      "epoch:9 step:8671 [D loss: 0.632491, acc.: 64.06%] [G loss: 1.251351]\n",
      "epoch:9 step:8672 [D loss: 0.663545, acc.: 59.38%] [G loss: 1.054846]\n",
      "epoch:9 step:8673 [D loss: 0.795023, acc.: 52.34%] [G loss: 1.027986]\n",
      "epoch:9 step:8674 [D loss: 0.663899, acc.: 61.72%] [G loss: 1.223021]\n",
      "epoch:9 step:8675 [D loss: 0.703084, acc.: 60.94%] [G loss: 1.353175]\n",
      "epoch:9 step:8676 [D loss: 0.714393, acc.: 53.91%] [G loss: 1.139941]\n",
      "epoch:9 step:8677 [D loss: 0.633481, acc.: 63.28%] [G loss: 1.055709]\n",
      "epoch:9 step:8678 [D loss: 0.605669, acc.: 66.41%] [G loss: 1.179307]\n",
      "epoch:9 step:8679 [D loss: 0.515921, acc.: 81.25%] [G loss: 1.303643]\n",
      "epoch:9 step:8680 [D loss: 0.602232, acc.: 73.44%] [G loss: 1.083474]\n",
      "epoch:9 step:8681 [D loss: 0.527015, acc.: 73.44%] [G loss: 1.082583]\n",
      "epoch:9 step:8682 [D loss: 0.632324, acc.: 66.41%] [G loss: 1.117359]\n",
      "epoch:9 step:8683 [D loss: 0.611885, acc.: 67.19%] [G loss: 1.202201]\n",
      "epoch:9 step:8684 [D loss: 0.654161, acc.: 61.72%] [G loss: 1.222684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8685 [D loss: 0.596918, acc.: 73.44%] [G loss: 0.994320]\n",
      "epoch:9 step:8686 [D loss: 0.626579, acc.: 64.84%] [G loss: 1.123084]\n",
      "epoch:9 step:8687 [D loss: 0.646902, acc.: 61.72%] [G loss: 1.022743]\n",
      "epoch:9 step:8688 [D loss: 0.471218, acc.: 82.81%] [G loss: 1.107005]\n",
      "epoch:9 step:8689 [D loss: 0.573258, acc.: 72.66%] [G loss: 1.295273]\n",
      "epoch:9 step:8690 [D loss: 0.482126, acc.: 82.03%] [G loss: 1.359294]\n",
      "epoch:9 step:8691 [D loss: 0.622092, acc.: 67.97%] [G loss: 1.158661]\n",
      "epoch:9 step:8692 [D loss: 0.569792, acc.: 68.75%] [G loss: 1.065025]\n",
      "epoch:9 step:8693 [D loss: 0.638775, acc.: 61.72%] [G loss: 1.074132]\n",
      "epoch:9 step:8694 [D loss: 0.519187, acc.: 77.34%] [G loss: 1.257176]\n",
      "epoch:9 step:8695 [D loss: 0.677980, acc.: 57.03%] [G loss: 1.094438]\n",
      "epoch:9 step:8696 [D loss: 0.621256, acc.: 65.62%] [G loss: 1.010066]\n",
      "epoch:9 step:8697 [D loss: 0.727941, acc.: 56.25%] [G loss: 0.992431]\n",
      "epoch:9 step:8698 [D loss: 0.571331, acc.: 71.88%] [G loss: 1.095496]\n",
      "epoch:9 step:8699 [D loss: 0.582049, acc.: 74.22%] [G loss: 1.167252]\n",
      "epoch:9 step:8700 [D loss: 0.622173, acc.: 67.97%] [G loss: 1.150907]\n",
      "epoch:9 step:8701 [D loss: 0.543677, acc.: 71.88%] [G loss: 1.334477]\n",
      "epoch:9 step:8702 [D loss: 0.624147, acc.: 64.84%] [G loss: 1.237658]\n",
      "epoch:9 step:8703 [D loss: 0.620463, acc.: 65.62%] [G loss: 1.098542]\n",
      "epoch:9 step:8704 [D loss: 0.593020, acc.: 68.75%] [G loss: 1.270661]\n",
      "epoch:9 step:8705 [D loss: 0.587626, acc.: 74.22%] [G loss: 1.229498]\n",
      "epoch:9 step:8706 [D loss: 0.528395, acc.: 73.44%] [G loss: 1.170393]\n",
      "epoch:9 step:8707 [D loss: 0.640153, acc.: 66.41%] [G loss: 1.049179]\n",
      "epoch:9 step:8708 [D loss: 0.748883, acc.: 53.12%] [G loss: 1.009750]\n",
      "epoch:9 step:8709 [D loss: 0.680448, acc.: 59.38%] [G loss: 1.216519]\n",
      "epoch:9 step:8710 [D loss: 0.520873, acc.: 78.12%] [G loss: 1.307777]\n",
      "epoch:9 step:8711 [D loss: 0.640821, acc.: 55.47%] [G loss: 1.040941]\n",
      "epoch:9 step:8712 [D loss: 0.620775, acc.: 65.62%] [G loss: 1.012258]\n",
      "epoch:9 step:8713 [D loss: 0.609227, acc.: 64.06%] [G loss: 1.013588]\n",
      "epoch:9 step:8714 [D loss: 0.722926, acc.: 53.12%] [G loss: 1.114819]\n",
      "epoch:9 step:8715 [D loss: 0.639660, acc.: 62.50%] [G loss: 1.226784]\n",
      "epoch:9 step:8716 [D loss: 0.590826, acc.: 67.97%] [G loss: 1.144274]\n",
      "epoch:9 step:8717 [D loss: 0.672072, acc.: 58.59%] [G loss: 1.025541]\n",
      "epoch:9 step:8718 [D loss: 0.556317, acc.: 76.56%] [G loss: 1.219392]\n",
      "epoch:9 step:8719 [D loss: 0.611957, acc.: 65.62%] [G loss: 1.024962]\n",
      "epoch:9 step:8720 [D loss: 0.563279, acc.: 71.88%] [G loss: 1.278667]\n",
      "epoch:9 step:8721 [D loss: 0.607029, acc.: 66.41%] [G loss: 1.158526]\n",
      "epoch:9 step:8722 [D loss: 0.542020, acc.: 70.31%] [G loss: 1.138547]\n",
      "epoch:9 step:8723 [D loss: 0.543450, acc.: 75.00%] [G loss: 1.136789]\n",
      "epoch:9 step:8724 [D loss: 0.546021, acc.: 75.78%] [G loss: 1.298429]\n",
      "epoch:9 step:8725 [D loss: 0.657443, acc.: 63.28%] [G loss: 1.194030]\n",
      "epoch:9 step:8726 [D loss: 0.583414, acc.: 67.97%] [G loss: 1.105288]\n",
      "epoch:9 step:8727 [D loss: 0.609214, acc.: 64.06%] [G loss: 1.012902]\n",
      "epoch:9 step:8728 [D loss: 0.656864, acc.: 62.50%] [G loss: 1.076430]\n",
      "epoch:9 step:8729 [D loss: 0.475634, acc.: 77.34%] [G loss: 1.439779]\n",
      "epoch:9 step:8730 [D loss: 0.547324, acc.: 73.44%] [G loss: 1.390898]\n",
      "epoch:9 step:8731 [D loss: 0.734969, acc.: 53.12%] [G loss: 0.974476]\n",
      "epoch:9 step:8732 [D loss: 0.565220, acc.: 69.53%] [G loss: 1.157073]\n",
      "epoch:9 step:8733 [D loss: 0.616418, acc.: 65.62%] [G loss: 1.126922]\n",
      "epoch:9 step:8734 [D loss: 0.626505, acc.: 61.72%] [G loss: 0.953428]\n",
      "epoch:9 step:8735 [D loss: 0.569489, acc.: 72.66%] [G loss: 1.193452]\n",
      "epoch:9 step:8736 [D loss: 0.575318, acc.: 67.97%] [G loss: 1.203434]\n",
      "epoch:9 step:8737 [D loss: 0.639737, acc.: 60.16%] [G loss: 0.960691]\n",
      "epoch:9 step:8738 [D loss: 0.661337, acc.: 60.16%] [G loss: 1.202460]\n",
      "epoch:9 step:8739 [D loss: 0.704367, acc.: 54.69%] [G loss: 1.046984]\n",
      "epoch:9 step:8740 [D loss: 0.655364, acc.: 57.03%] [G loss: 0.969208]\n",
      "epoch:9 step:8741 [D loss: 0.783519, acc.: 44.53%] [G loss: 0.844105]\n",
      "epoch:9 step:8742 [D loss: 0.604720, acc.: 69.53%] [G loss: 1.050564]\n",
      "epoch:9 step:8743 [D loss: 0.602190, acc.: 67.19%] [G loss: 1.154344]\n",
      "epoch:9 step:8744 [D loss: 0.713070, acc.: 53.91%] [G loss: 1.179687]\n",
      "epoch:9 step:8745 [D loss: 0.667416, acc.: 61.72%] [G loss: 1.061927]\n",
      "epoch:9 step:8746 [D loss: 0.613385, acc.: 64.84%] [G loss: 1.005085]\n",
      "epoch:9 step:8747 [D loss: 0.619762, acc.: 64.84%] [G loss: 1.119636]\n",
      "epoch:9 step:8748 [D loss: 0.557266, acc.: 72.66%] [G loss: 1.290683]\n",
      "epoch:9 step:8749 [D loss: 0.610999, acc.: 61.72%] [G loss: 1.204968]\n",
      "epoch:9 step:8750 [D loss: 0.588822, acc.: 64.84%] [G loss: 1.118654]\n",
      "epoch:9 step:8751 [D loss: 0.596295, acc.: 62.50%] [G loss: 0.875294]\n",
      "epoch:9 step:8752 [D loss: 0.641032, acc.: 65.62%] [G loss: 1.017329]\n",
      "epoch:9 step:8753 [D loss: 0.619078, acc.: 60.16%] [G loss: 1.074142]\n",
      "epoch:9 step:8754 [D loss: 0.583656, acc.: 67.97%] [G loss: 1.050065]\n",
      "epoch:9 step:8755 [D loss: 0.728214, acc.: 51.56%] [G loss: 0.927758]\n",
      "epoch:9 step:8756 [D loss: 0.564700, acc.: 70.31%] [G loss: 1.008530]\n",
      "epoch:9 step:8757 [D loss: 0.592364, acc.: 70.31%] [G loss: 1.149809]\n",
      "epoch:9 step:8758 [D loss: 0.588453, acc.: 72.66%] [G loss: 1.290091]\n",
      "epoch:9 step:8759 [D loss: 0.606487, acc.: 66.41%] [G loss: 1.332669]\n",
      "epoch:9 step:8760 [D loss: 0.638713, acc.: 65.62%] [G loss: 1.143641]\n",
      "epoch:9 step:8761 [D loss: 0.693790, acc.: 57.81%] [G loss: 1.237551]\n",
      "epoch:9 step:8762 [D loss: 0.564533, acc.: 69.53%] [G loss: 1.171200]\n",
      "epoch:9 step:8763 [D loss: 0.634322, acc.: 66.41%] [G loss: 1.384685]\n",
      "epoch:9 step:8764 [D loss: 0.623007, acc.: 60.94%] [G loss: 1.319697]\n",
      "epoch:9 step:8765 [D loss: 0.592781, acc.: 67.97%] [G loss: 1.141450]\n",
      "epoch:9 step:8766 [D loss: 0.652890, acc.: 62.50%] [G loss: 1.227777]\n",
      "epoch:9 step:8767 [D loss: 0.602987, acc.: 69.53%] [G loss: 1.036396]\n",
      "epoch:9 step:8768 [D loss: 0.597339, acc.: 67.97%] [G loss: 0.996645]\n",
      "epoch:9 step:8769 [D loss: 0.432956, acc.: 86.72%] [G loss: 1.178921]\n",
      "epoch:9 step:8770 [D loss: 0.669480, acc.: 63.28%] [G loss: 1.162490]\n",
      "epoch:9 step:8771 [D loss: 0.639968, acc.: 63.28%] [G loss: 1.038981]\n",
      "epoch:9 step:8772 [D loss: 0.701891, acc.: 57.03%] [G loss: 1.064125]\n",
      "epoch:9 step:8773 [D loss: 0.576309, acc.: 75.78%] [G loss: 1.237066]\n",
      "epoch:9 step:8774 [D loss: 0.495878, acc.: 78.91%] [G loss: 1.465015]\n",
      "epoch:9 step:8775 [D loss: 0.567163, acc.: 72.66%] [G loss: 1.003704]\n",
      "epoch:9 step:8776 [D loss: 0.500764, acc.: 75.00%] [G loss: 1.271322]\n",
      "epoch:9 step:8777 [D loss: 0.587583, acc.: 67.19%] [G loss: 1.133534]\n",
      "epoch:9 step:8778 [D loss: 0.498433, acc.: 74.22%] [G loss: 1.328330]\n",
      "epoch:9 step:8779 [D loss: 0.624407, acc.: 66.41%] [G loss: 1.151966]\n",
      "epoch:9 step:8780 [D loss: 0.638658, acc.: 64.84%] [G loss: 1.157351]\n",
      "epoch:9 step:8781 [D loss: 0.650829, acc.: 62.50%] [G loss: 1.418784]\n",
      "epoch:9 step:8782 [D loss: 0.530984, acc.: 73.44%] [G loss: 0.966764]\n",
      "epoch:9 step:8783 [D loss: 0.662302, acc.: 58.59%] [G loss: 1.178855]\n",
      "epoch:9 step:8784 [D loss: 0.662580, acc.: 61.72%] [G loss: 0.978209]\n",
      "epoch:9 step:8785 [D loss: 0.714381, acc.: 55.47%] [G loss: 1.226205]\n",
      "epoch:9 step:8786 [D loss: 0.552430, acc.: 70.31%] [G loss: 1.438682]\n",
      "epoch:9 step:8787 [D loss: 0.585183, acc.: 64.06%] [G loss: 1.263374]\n",
      "epoch:9 step:8788 [D loss: 0.595627, acc.: 70.31%] [G loss: 0.998156]\n",
      "epoch:9 step:8789 [D loss: 0.609127, acc.: 67.19%] [G loss: 1.047505]\n",
      "epoch:9 step:8790 [D loss: 0.570001, acc.: 74.22%] [G loss: 1.192015]\n",
      "epoch:9 step:8791 [D loss: 0.566649, acc.: 71.09%] [G loss: 1.062035]\n",
      "epoch:9 step:8792 [D loss: 0.687071, acc.: 59.38%] [G loss: 0.994946]\n",
      "epoch:9 step:8793 [D loss: 0.585113, acc.: 67.97%] [G loss: 1.097285]\n",
      "epoch:9 step:8794 [D loss: 0.489410, acc.: 80.47%] [G loss: 1.356935]\n",
      "epoch:9 step:8795 [D loss: 0.698070, acc.: 57.03%] [G loss: 0.985482]\n",
      "epoch:9 step:8796 [D loss: 0.554508, acc.: 71.09%] [G loss: 1.120421]\n",
      "epoch:9 step:8797 [D loss: 0.578956, acc.: 70.31%] [G loss: 1.138252]\n",
      "epoch:9 step:8798 [D loss: 0.646483, acc.: 57.81%] [G loss: 1.282820]\n",
      "epoch:9 step:8799 [D loss: 0.536662, acc.: 71.09%] [G loss: 1.381543]\n",
      "epoch:9 step:8800 [D loss: 0.631939, acc.: 64.06%] [G loss: 1.247462]\n",
      "##############\n",
      "[2.73993074 2.1903731  1.90530435 3.1372507  1.04893984 6.15072422\n",
      " 2.19227618 2.9025246  3.74629531 7.14868929]\n",
      "##########\n",
      "epoch:9 step:8801 [D loss: 0.589359, acc.: 67.97%] [G loss: 0.871406]\n",
      "epoch:9 step:8802 [D loss: 0.636872, acc.: 63.28%] [G loss: 1.142044]\n",
      "epoch:9 step:8803 [D loss: 0.710526, acc.: 59.38%] [G loss: 1.068092]\n",
      "epoch:9 step:8804 [D loss: 0.512395, acc.: 75.00%] [G loss: 1.031781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8805 [D loss: 0.609512, acc.: 67.19%] [G loss: 1.041761]\n",
      "epoch:9 step:8806 [D loss: 0.667829, acc.: 64.84%] [G loss: 1.090040]\n",
      "epoch:9 step:8807 [D loss: 0.720073, acc.: 60.16%] [G loss: 0.899773]\n",
      "epoch:9 step:8808 [D loss: 0.676136, acc.: 55.47%] [G loss: 1.243533]\n",
      "epoch:9 step:8809 [D loss: 0.524789, acc.: 76.56%] [G loss: 1.203149]\n",
      "epoch:9 step:8810 [D loss: 0.520983, acc.: 78.12%] [G loss: 1.334213]\n",
      "epoch:9 step:8811 [D loss: 0.640518, acc.: 68.75%] [G loss: 0.959186]\n",
      "epoch:9 step:8812 [D loss: 0.600387, acc.: 64.06%] [G loss: 0.914587]\n",
      "epoch:9 step:8813 [D loss: 0.564826, acc.: 77.34%] [G loss: 1.156337]\n",
      "epoch:9 step:8814 [D loss: 0.575039, acc.: 70.31%] [G loss: 0.981885]\n",
      "epoch:9 step:8815 [D loss: 0.700044, acc.: 58.59%] [G loss: 1.155588]\n",
      "epoch:9 step:8816 [D loss: 0.538246, acc.: 76.56%] [G loss: 1.217287]\n",
      "epoch:9 step:8817 [D loss: 0.758931, acc.: 48.44%] [G loss: 1.105508]\n",
      "epoch:9 step:8818 [D loss: 0.627847, acc.: 64.84%] [G loss: 1.054075]\n",
      "epoch:9 step:8819 [D loss: 0.609842, acc.: 69.53%] [G loss: 1.188379]\n",
      "epoch:9 step:8820 [D loss: 0.630485, acc.: 69.53%] [G loss: 1.383923]\n",
      "epoch:9 step:8821 [D loss: 0.786041, acc.: 53.12%] [G loss: 1.104832]\n",
      "epoch:9 step:8822 [D loss: 0.669709, acc.: 58.59%] [G loss: 0.997493]\n",
      "epoch:9 step:8823 [D loss: 0.616041, acc.: 64.06%] [G loss: 1.156038]\n",
      "epoch:9 step:8824 [D loss: 0.764533, acc.: 51.56%] [G loss: 1.145532]\n",
      "epoch:9 step:8825 [D loss: 0.602327, acc.: 64.06%] [G loss: 1.133378]\n",
      "epoch:9 step:8826 [D loss: 0.654981, acc.: 59.38%] [G loss: 1.198718]\n",
      "epoch:9 step:8827 [D loss: 0.557667, acc.: 70.31%] [G loss: 1.327130]\n",
      "epoch:9 step:8828 [D loss: 0.601202, acc.: 65.62%] [G loss: 1.315845]\n",
      "epoch:9 step:8829 [D loss: 0.658416, acc.: 63.28%] [G loss: 1.268339]\n",
      "epoch:9 step:8830 [D loss: 0.589737, acc.: 72.66%] [G loss: 1.136763]\n",
      "epoch:9 step:8831 [D loss: 0.688278, acc.: 61.72%] [G loss: 1.074111]\n",
      "epoch:9 step:8832 [D loss: 0.529573, acc.: 75.78%] [G loss: 1.100259]\n",
      "epoch:9 step:8833 [D loss: 0.547455, acc.: 74.22%] [G loss: 1.118960]\n",
      "epoch:9 step:8834 [D loss: 0.513032, acc.: 74.22%] [G loss: 1.330411]\n",
      "epoch:9 step:8835 [D loss: 0.589929, acc.: 69.53%] [G loss: 1.185732]\n",
      "epoch:9 step:8836 [D loss: 0.730726, acc.: 57.03%] [G loss: 1.180439]\n",
      "epoch:9 step:8837 [D loss: 0.619129, acc.: 65.62%] [G loss: 1.092710]\n",
      "epoch:9 step:8838 [D loss: 0.603058, acc.: 67.97%] [G loss: 1.070506]\n",
      "epoch:9 step:8839 [D loss: 0.601818, acc.: 67.19%] [G loss: 1.073930]\n",
      "epoch:9 step:8840 [D loss: 0.616154, acc.: 67.19%] [G loss: 1.226028]\n",
      "epoch:9 step:8841 [D loss: 0.596046, acc.: 68.75%] [G loss: 1.380576]\n",
      "epoch:9 step:8842 [D loss: 0.664972, acc.: 62.50%] [G loss: 1.098401]\n",
      "epoch:9 step:8843 [D loss: 0.627236, acc.: 64.06%] [G loss: 1.065290]\n",
      "epoch:9 step:8844 [D loss: 0.643727, acc.: 62.50%] [G loss: 1.247988]\n",
      "epoch:9 step:8845 [D loss: 0.729114, acc.: 48.44%] [G loss: 1.137263]\n",
      "epoch:9 step:8846 [D loss: 0.591777, acc.: 73.44%] [G loss: 1.271897]\n",
      "epoch:9 step:8847 [D loss: 0.614006, acc.: 70.31%] [G loss: 1.090900]\n",
      "epoch:9 step:8848 [D loss: 0.603494, acc.: 64.84%] [G loss: 1.195834]\n",
      "epoch:9 step:8849 [D loss: 0.610819, acc.: 66.41%] [G loss: 1.023393]\n",
      "epoch:9 step:8850 [D loss: 0.695348, acc.: 60.94%] [G loss: 1.054913]\n",
      "epoch:9 step:8851 [D loss: 0.580352, acc.: 65.62%] [G loss: 1.171218]\n",
      "epoch:9 step:8852 [D loss: 0.553487, acc.: 72.66%] [G loss: 1.258554]\n",
      "epoch:9 step:8853 [D loss: 0.524803, acc.: 76.56%] [G loss: 1.261446]\n",
      "epoch:9 step:8854 [D loss: 0.753490, acc.: 51.56%] [G loss: 0.782010]\n",
      "epoch:9 step:8855 [D loss: 0.554914, acc.: 75.00%] [G loss: 1.052514]\n",
      "epoch:9 step:8856 [D loss: 0.559740, acc.: 69.53%] [G loss: 1.143074]\n",
      "epoch:9 step:8857 [D loss: 0.686832, acc.: 57.03%] [G loss: 1.347175]\n",
      "epoch:9 step:8858 [D loss: 0.669912, acc.: 53.12%] [G loss: 1.165079]\n",
      "epoch:9 step:8859 [D loss: 0.772742, acc.: 42.97%] [G loss: 1.107637]\n",
      "epoch:9 step:8860 [D loss: 0.579001, acc.: 65.62%] [G loss: 1.125041]\n",
      "epoch:9 step:8861 [D loss: 0.575353, acc.: 72.66%] [G loss: 1.189337]\n",
      "epoch:9 step:8862 [D loss: 0.747926, acc.: 52.34%] [G loss: 1.052292]\n",
      "epoch:9 step:8863 [D loss: 0.637987, acc.: 60.94%] [G loss: 0.995702]\n",
      "epoch:9 step:8864 [D loss: 0.626221, acc.: 62.50%] [G loss: 0.993873]\n",
      "epoch:9 step:8865 [D loss: 0.626164, acc.: 67.97%] [G loss: 1.141103]\n",
      "epoch:9 step:8866 [D loss: 0.598480, acc.: 66.41%] [G loss: 1.174835]\n",
      "epoch:9 step:8867 [D loss: 0.603328, acc.: 63.28%] [G loss: 1.345303]\n",
      "epoch:9 step:8868 [D loss: 0.520914, acc.: 74.22%] [G loss: 1.073698]\n",
      "epoch:9 step:8869 [D loss: 0.502682, acc.: 79.69%] [G loss: 1.322649]\n",
      "epoch:9 step:8870 [D loss: 0.610739, acc.: 66.41%] [G loss: 1.086579]\n",
      "epoch:9 step:8871 [D loss: 0.650648, acc.: 60.94%] [G loss: 1.352875]\n",
      "epoch:9 step:8872 [D loss: 0.541811, acc.: 75.00%] [G loss: 1.035285]\n",
      "epoch:9 step:8873 [D loss: 0.575805, acc.: 71.09%] [G loss: 1.267363]\n",
      "epoch:9 step:8874 [D loss: 0.560574, acc.: 69.53%] [G loss: 1.240541]\n",
      "epoch:9 step:8875 [D loss: 0.755646, acc.: 47.66%] [G loss: 1.048574]\n",
      "epoch:9 step:8876 [D loss: 0.488024, acc.: 74.22%] [G loss: 1.248285]\n",
      "epoch:9 step:8877 [D loss: 0.578102, acc.: 66.41%] [G loss: 1.305179]\n",
      "epoch:9 step:8878 [D loss: 0.480071, acc.: 83.59%] [G loss: 1.256222]\n",
      "epoch:9 step:8879 [D loss: 0.761054, acc.: 45.31%] [G loss: 1.054225]\n",
      "epoch:9 step:8880 [D loss: 0.640358, acc.: 60.94%] [G loss: 1.249849]\n",
      "epoch:9 step:8881 [D loss: 0.604954, acc.: 72.66%] [G loss: 1.139905]\n",
      "epoch:9 step:8882 [D loss: 0.539094, acc.: 78.12%] [G loss: 1.333365]\n",
      "epoch:9 step:8883 [D loss: 0.666178, acc.: 64.84%] [G loss: 1.166231]\n",
      "epoch:9 step:8884 [D loss: 0.770013, acc.: 48.44%] [G loss: 0.990431]\n",
      "epoch:9 step:8885 [D loss: 0.614272, acc.: 66.41%] [G loss: 1.107007]\n",
      "epoch:9 step:8886 [D loss: 0.531803, acc.: 75.78%] [G loss: 1.158159]\n",
      "epoch:9 step:8887 [D loss: 0.608727, acc.: 64.84%] [G loss: 1.132362]\n",
      "epoch:9 step:8888 [D loss: 0.577799, acc.: 72.66%] [G loss: 1.316112]\n",
      "epoch:9 step:8889 [D loss: 0.698752, acc.: 57.03%] [G loss: 1.068608]\n",
      "epoch:9 step:8890 [D loss: 0.653594, acc.: 64.06%] [G loss: 1.262255]\n",
      "epoch:9 step:8891 [D loss: 0.641139, acc.: 67.19%] [G loss: 1.307095]\n",
      "epoch:9 step:8892 [D loss: 0.682886, acc.: 61.72%] [G loss: 1.074464]\n",
      "epoch:9 step:8893 [D loss: 0.562125, acc.: 75.00%] [G loss: 1.355343]\n",
      "epoch:9 step:8894 [D loss: 0.735852, acc.: 51.56%] [G loss: 1.254513]\n",
      "epoch:9 step:8895 [D loss: 0.608274, acc.: 62.50%] [G loss: 1.400107]\n",
      "epoch:9 step:8896 [D loss: 0.659623, acc.: 65.62%] [G loss: 0.937996]\n",
      "epoch:9 step:8897 [D loss: 0.576602, acc.: 73.44%] [G loss: 0.950121]\n",
      "epoch:9 step:8898 [D loss: 0.588192, acc.: 69.53%] [G loss: 1.051164]\n",
      "epoch:9 step:8899 [D loss: 0.547515, acc.: 73.44%] [G loss: 0.956960]\n",
      "epoch:9 step:8900 [D loss: 0.499466, acc.: 78.91%] [G loss: 1.304339]\n",
      "epoch:9 step:8901 [D loss: 0.498666, acc.: 78.91%] [G loss: 1.288860]\n",
      "epoch:9 step:8902 [D loss: 0.644602, acc.: 62.50%] [G loss: 0.975679]\n",
      "epoch:9 step:8903 [D loss: 0.715458, acc.: 55.47%] [G loss: 0.877363]\n",
      "epoch:9 step:8904 [D loss: 0.636674, acc.: 64.06%] [G loss: 0.939321]\n",
      "epoch:9 step:8905 [D loss: 0.645402, acc.: 64.06%] [G loss: 0.971847]\n",
      "epoch:9 step:8906 [D loss: 0.573494, acc.: 75.78%] [G loss: 1.073924]\n",
      "epoch:9 step:8907 [D loss: 0.570848, acc.: 73.44%] [G loss: 1.228746]\n",
      "epoch:9 step:8908 [D loss: 0.598217, acc.: 68.75%] [G loss: 1.134820]\n",
      "epoch:9 step:8909 [D loss: 0.510010, acc.: 78.91%] [G loss: 1.423792]\n",
      "epoch:9 step:8910 [D loss: 0.636903, acc.: 61.72%] [G loss: 1.195418]\n",
      "epoch:9 step:8911 [D loss: 0.657109, acc.: 62.50%] [G loss: 0.860122]\n",
      "epoch:9 step:8912 [D loss: 0.562747, acc.: 69.53%] [G loss: 1.275013]\n",
      "epoch:9 step:8913 [D loss: 0.508186, acc.: 77.34%] [G loss: 1.128893]\n",
      "epoch:9 step:8914 [D loss: 0.538561, acc.: 77.34%] [G loss: 1.295142]\n",
      "epoch:9 step:8915 [D loss: 0.575878, acc.: 65.62%] [G loss: 1.000691]\n",
      "epoch:9 step:8916 [D loss: 0.699195, acc.: 54.69%] [G loss: 0.935373]\n",
      "epoch:9 step:8917 [D loss: 0.492612, acc.: 79.69%] [G loss: 1.069174]\n",
      "epoch:9 step:8918 [D loss: 0.637824, acc.: 62.50%] [G loss: 1.263660]\n",
      "epoch:9 step:8919 [D loss: 0.619207, acc.: 64.84%] [G loss: 1.140434]\n",
      "epoch:9 step:8920 [D loss: 0.518412, acc.: 78.12%] [G loss: 1.146122]\n",
      "epoch:9 step:8921 [D loss: 0.466573, acc.: 82.81%] [G loss: 1.191587]\n",
      "epoch:9 step:8922 [D loss: 0.621378, acc.: 63.28%] [G loss: 1.086229]\n",
      "epoch:9 step:8923 [D loss: 0.672936, acc.: 56.25%] [G loss: 1.089475]\n",
      "epoch:9 step:8924 [D loss: 0.603012, acc.: 67.97%] [G loss: 1.299168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8925 [D loss: 0.605043, acc.: 64.06%] [G loss: 1.105507]\n",
      "epoch:9 step:8926 [D loss: 0.659826, acc.: 60.16%] [G loss: 1.234598]\n",
      "epoch:9 step:8927 [D loss: 0.564279, acc.: 68.75%] [G loss: 1.111944]\n",
      "epoch:9 step:8928 [D loss: 0.669081, acc.: 61.72%] [G loss: 1.059779]\n",
      "epoch:9 step:8929 [D loss: 0.542504, acc.: 73.44%] [G loss: 1.222936]\n",
      "epoch:9 step:8930 [D loss: 0.590659, acc.: 67.19%] [G loss: 1.143605]\n",
      "epoch:9 step:8931 [D loss: 0.576142, acc.: 71.88%] [G loss: 1.206725]\n",
      "epoch:9 step:8932 [D loss: 0.706183, acc.: 59.38%] [G loss: 1.213408]\n",
      "epoch:9 step:8933 [D loss: 0.610415, acc.: 64.84%] [G loss: 1.170067]\n",
      "epoch:9 step:8934 [D loss: 0.681519, acc.: 56.25%] [G loss: 0.930729]\n",
      "epoch:9 step:8935 [D loss: 0.575158, acc.: 67.97%] [G loss: 1.244038]\n",
      "epoch:9 step:8936 [D loss: 0.691687, acc.: 60.16%] [G loss: 0.920644]\n",
      "epoch:9 step:8937 [D loss: 0.589445, acc.: 65.62%] [G loss: 1.264591]\n",
      "epoch:9 step:8938 [D loss: 0.586337, acc.: 66.41%] [G loss: 1.123896]\n",
      "epoch:9 step:8939 [D loss: 0.541416, acc.: 73.44%] [G loss: 1.174476]\n",
      "epoch:9 step:8940 [D loss: 0.690754, acc.: 53.12%] [G loss: 1.220517]\n",
      "epoch:9 step:8941 [D loss: 0.677215, acc.: 60.94%] [G loss: 1.302240]\n",
      "epoch:9 step:8942 [D loss: 0.557640, acc.: 69.53%] [G loss: 1.258168]\n",
      "epoch:9 step:8943 [D loss: 0.567974, acc.: 71.88%] [G loss: 1.244394]\n",
      "epoch:9 step:8944 [D loss: 0.586761, acc.: 67.19%] [G loss: 1.073202]\n",
      "epoch:9 step:8945 [D loss: 0.543572, acc.: 72.66%] [G loss: 1.535401]\n",
      "epoch:9 step:8946 [D loss: 0.548601, acc.: 80.47%] [G loss: 1.099257]\n",
      "epoch:9 step:8947 [D loss: 0.633196, acc.: 61.72%] [G loss: 1.153576]\n",
      "epoch:9 step:8948 [D loss: 0.689453, acc.: 60.16%] [G loss: 0.922728]\n",
      "epoch:9 step:8949 [D loss: 0.630536, acc.: 64.06%] [G loss: 1.213514]\n",
      "epoch:9 step:8950 [D loss: 0.636881, acc.: 64.06%] [G loss: 1.118318]\n",
      "epoch:9 step:8951 [D loss: 0.690829, acc.: 62.50%] [G loss: 1.001876]\n",
      "epoch:9 step:8952 [D loss: 0.651307, acc.: 64.06%] [G loss: 1.050273]\n",
      "epoch:9 step:8953 [D loss: 0.511529, acc.: 77.34%] [G loss: 1.554626]\n",
      "epoch:9 step:8954 [D loss: 0.665916, acc.: 66.41%] [G loss: 0.988989]\n",
      "epoch:9 step:8955 [D loss: 0.633521, acc.: 65.62%] [G loss: 1.266670]\n",
      "epoch:9 step:8956 [D loss: 0.616131, acc.: 64.06%] [G loss: 0.943109]\n",
      "epoch:9 step:8957 [D loss: 0.665045, acc.: 57.81%] [G loss: 1.264025]\n",
      "epoch:9 step:8958 [D loss: 0.570713, acc.: 73.44%] [G loss: 1.283179]\n",
      "epoch:9 step:8959 [D loss: 0.565969, acc.: 69.53%] [G loss: 1.179357]\n",
      "epoch:9 step:8960 [D loss: 0.612118, acc.: 63.28%] [G loss: 0.995962]\n",
      "epoch:9 step:8961 [D loss: 0.663527, acc.: 54.69%] [G loss: 0.926767]\n",
      "epoch:9 step:8962 [D loss: 0.516518, acc.: 75.00%] [G loss: 1.137813]\n",
      "epoch:9 step:8963 [D loss: 0.611137, acc.: 63.28%] [G loss: 1.423260]\n",
      "epoch:9 step:8964 [D loss: 0.592676, acc.: 64.84%] [G loss: 1.009788]\n",
      "epoch:9 step:8965 [D loss: 0.632836, acc.: 64.84%] [G loss: 1.125942]\n",
      "epoch:9 step:8966 [D loss: 0.601739, acc.: 67.97%] [G loss: 1.110102]\n",
      "epoch:9 step:8967 [D loss: 0.675975, acc.: 56.25%] [G loss: 1.271763]\n",
      "epoch:9 step:8968 [D loss: 0.652161, acc.: 62.50%] [G loss: 1.115871]\n",
      "epoch:9 step:8969 [D loss: 0.618071, acc.: 68.75%] [G loss: 1.291364]\n",
      "epoch:9 step:8970 [D loss: 0.670527, acc.: 57.81%] [G loss: 0.928903]\n",
      "epoch:9 step:8971 [D loss: 0.571469, acc.: 71.88%] [G loss: 1.087459]\n",
      "epoch:9 step:8972 [D loss: 0.587818, acc.: 67.97%] [G loss: 1.231237]\n",
      "epoch:9 step:8973 [D loss: 0.544669, acc.: 78.12%] [G loss: 1.131386]\n",
      "epoch:9 step:8974 [D loss: 0.595745, acc.: 67.19%] [G loss: 1.098326]\n",
      "epoch:9 step:8975 [D loss: 0.642213, acc.: 63.28%] [G loss: 1.194181]\n",
      "epoch:9 step:8976 [D loss: 0.553401, acc.: 71.88%] [G loss: 1.260579]\n",
      "epoch:9 step:8977 [D loss: 0.548925, acc.: 75.00%] [G loss: 1.292695]\n",
      "epoch:9 step:8978 [D loss: 0.588563, acc.: 69.53%] [G loss: 1.166462]\n",
      "epoch:9 step:8979 [D loss: 0.588574, acc.: 71.09%] [G loss: 1.088934]\n",
      "epoch:9 step:8980 [D loss: 0.634413, acc.: 64.84%] [G loss: 1.156197]\n",
      "epoch:9 step:8981 [D loss: 0.591993, acc.: 73.44%] [G loss: 1.161720]\n",
      "epoch:9 step:8982 [D loss: 0.634917, acc.: 61.72%] [G loss: 1.130849]\n",
      "epoch:9 step:8983 [D loss: 0.534345, acc.: 70.31%] [G loss: 1.335896]\n",
      "epoch:9 step:8984 [D loss: 0.532334, acc.: 75.78%] [G loss: 1.385783]\n",
      "epoch:9 step:8985 [D loss: 0.592713, acc.: 67.97%] [G loss: 1.530996]\n",
      "epoch:9 step:8986 [D loss: 0.712604, acc.: 57.81%] [G loss: 1.035332]\n",
      "epoch:9 step:8987 [D loss: 0.692265, acc.: 58.59%] [G loss: 1.123125]\n",
      "epoch:9 step:8988 [D loss: 0.559395, acc.: 73.44%] [G loss: 1.147789]\n",
      "epoch:9 step:8989 [D loss: 0.588902, acc.: 68.75%] [G loss: 1.281512]\n",
      "epoch:9 step:8990 [D loss: 0.561388, acc.: 70.31%] [G loss: 1.109657]\n",
      "epoch:9 step:8991 [D loss: 0.590148, acc.: 67.97%] [G loss: 1.154917]\n",
      "epoch:9 step:8992 [D loss: 0.577570, acc.: 71.88%] [G loss: 1.170341]\n",
      "epoch:9 step:8993 [D loss: 0.611792, acc.: 70.31%] [G loss: 1.173462]\n",
      "epoch:9 step:8994 [D loss: 0.585137, acc.: 69.53%] [G loss: 1.273760]\n",
      "epoch:9 step:8995 [D loss: 0.535031, acc.: 73.44%] [G loss: 1.240395]\n",
      "epoch:9 step:8996 [D loss: 0.513905, acc.: 76.56%] [G loss: 1.120989]\n",
      "epoch:9 step:8997 [D loss: 0.557734, acc.: 73.44%] [G loss: 1.041330]\n",
      "epoch:9 step:8998 [D loss: 0.480374, acc.: 78.91%] [G loss: 1.143895]\n",
      "epoch:9 step:8999 [D loss: 0.665007, acc.: 58.59%] [G loss: 1.231638]\n",
      "epoch:9 step:9000 [D loss: 0.576966, acc.: 69.53%] [G loss: 1.174988]\n",
      "##############\n",
      "[2.79520488 2.38567118 2.03026908 2.92131126 1.31733056 6.0838182\n",
      " 2.05790526 2.7772173  3.91386715 8.14868929]\n",
      "##########\n",
      "epoch:9 step:9001 [D loss: 0.744137, acc.: 53.12%] [G loss: 1.055228]\n",
      "epoch:9 step:9002 [D loss: 0.703711, acc.: 56.25%] [G loss: 1.009689]\n",
      "epoch:9 step:9003 [D loss: 0.642832, acc.: 64.84%] [G loss: 1.112985]\n",
      "epoch:9 step:9004 [D loss: 0.575158, acc.: 66.41%] [G loss: 1.543536]\n",
      "epoch:9 step:9005 [D loss: 0.485295, acc.: 81.25%] [G loss: 1.286615]\n",
      "epoch:9 step:9006 [D loss: 0.727020, acc.: 57.81%] [G loss: 1.102946]\n",
      "epoch:9 step:9007 [D loss: 0.663938, acc.: 60.16%] [G loss: 1.161520]\n",
      "epoch:9 step:9008 [D loss: 0.563168, acc.: 71.09%] [G loss: 1.125902]\n",
      "epoch:9 step:9009 [D loss: 0.551465, acc.: 75.78%] [G loss: 1.244480]\n",
      "epoch:9 step:9010 [D loss: 0.631270, acc.: 60.16%] [G loss: 1.172651]\n",
      "epoch:9 step:9011 [D loss: 0.748658, acc.: 53.91%] [G loss: 1.110752]\n",
      "epoch:9 step:9012 [D loss: 0.649351, acc.: 63.28%] [G loss: 1.189629]\n",
      "epoch:9 step:9013 [D loss: 0.582323, acc.: 69.53%] [G loss: 1.114649]\n",
      "epoch:9 step:9014 [D loss: 0.532036, acc.: 78.12%] [G loss: 1.174458]\n",
      "epoch:9 step:9015 [D loss: 0.694895, acc.: 60.94%] [G loss: 0.979855]\n",
      "epoch:9 step:9016 [D loss: 0.537052, acc.: 71.88%] [G loss: 1.095667]\n",
      "epoch:9 step:9017 [D loss: 0.510743, acc.: 74.22%] [G loss: 1.257949]\n",
      "epoch:9 step:9018 [D loss: 0.522551, acc.: 76.56%] [G loss: 1.346800]\n",
      "epoch:9 step:9019 [D loss: 0.658088, acc.: 61.72%] [G loss: 1.104711]\n",
      "epoch:9 step:9020 [D loss: 0.626331, acc.: 64.06%] [G loss: 1.078418]\n",
      "epoch:9 step:9021 [D loss: 0.628633, acc.: 65.62%] [G loss: 1.051878]\n",
      "epoch:9 step:9022 [D loss: 0.597211, acc.: 68.75%] [G loss: 1.428085]\n",
      "epoch:9 step:9023 [D loss: 0.479098, acc.: 81.25%] [G loss: 1.376973]\n",
      "epoch:9 step:9024 [D loss: 0.505883, acc.: 77.34%] [G loss: 1.205954]\n",
      "epoch:9 step:9025 [D loss: 0.566791, acc.: 71.88%] [G loss: 1.236414]\n",
      "epoch:9 step:9026 [D loss: 0.676197, acc.: 63.28%] [G loss: 1.209938]\n",
      "epoch:9 step:9027 [D loss: 0.530780, acc.: 77.34%] [G loss: 1.333742]\n",
      "epoch:9 step:9028 [D loss: 0.647970, acc.: 52.34%] [G loss: 1.056864]\n",
      "epoch:9 step:9029 [D loss: 0.542199, acc.: 75.00%] [G loss: 0.968020]\n",
      "epoch:9 step:9030 [D loss: 0.512271, acc.: 73.44%] [G loss: 1.226610]\n",
      "epoch:9 step:9031 [D loss: 0.630563, acc.: 67.19%] [G loss: 1.022837]\n",
      "epoch:9 step:9032 [D loss: 0.471554, acc.: 76.56%] [G loss: 1.419550]\n",
      "epoch:9 step:9033 [D loss: 0.661488, acc.: 60.16%] [G loss: 1.007898]\n",
      "epoch:9 step:9034 [D loss: 0.643690, acc.: 62.50%] [G loss: 1.129946]\n",
      "epoch:9 step:9035 [D loss: 0.528868, acc.: 73.44%] [G loss: 1.051560]\n",
      "epoch:9 step:9036 [D loss: 0.594596, acc.: 69.53%] [G loss: 1.196672]\n",
      "epoch:9 step:9037 [D loss: 0.597434, acc.: 65.62%] [G loss: 1.125560]\n",
      "epoch:9 step:9038 [D loss: 0.485155, acc.: 82.03%] [G loss: 1.110311]\n",
      "epoch:9 step:9039 [D loss: 0.670876, acc.: 60.94%] [G loss: 1.002888]\n",
      "epoch:9 step:9040 [D loss: 0.547970, acc.: 72.66%] [G loss: 0.983382]\n",
      "epoch:9 step:9041 [D loss: 0.621028, acc.: 67.97%] [G loss: 0.934093]\n",
      "epoch:9 step:9042 [D loss: 0.585164, acc.: 73.44%] [G loss: 1.270225]\n",
      "epoch:9 step:9043 [D loss: 0.508678, acc.: 78.12%] [G loss: 1.205984]\n",
      "epoch:9 step:9044 [D loss: 0.633506, acc.: 61.72%] [G loss: 1.298797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9045 [D loss: 0.609887, acc.: 67.19%] [G loss: 0.968316]\n",
      "epoch:9 step:9046 [D loss: 0.489728, acc.: 80.47%] [G loss: 1.164985]\n",
      "epoch:9 step:9047 [D loss: 0.585503, acc.: 68.75%] [G loss: 1.121743]\n",
      "epoch:9 step:9048 [D loss: 0.549481, acc.: 73.44%] [G loss: 0.983928]\n",
      "epoch:9 step:9049 [D loss: 0.507158, acc.: 78.12%] [G loss: 1.389903]\n",
      "epoch:9 step:9050 [D loss: 0.585690, acc.: 73.44%] [G loss: 1.180305]\n",
      "epoch:9 step:9051 [D loss: 0.659437, acc.: 60.16%] [G loss: 1.343848]\n",
      "epoch:9 step:9052 [D loss: 0.707698, acc.: 55.47%] [G loss: 1.277640]\n",
      "epoch:9 step:9053 [D loss: 0.539672, acc.: 71.09%] [G loss: 1.207729]\n",
      "epoch:9 step:9054 [D loss: 0.626717, acc.: 64.84%] [G loss: 1.197002]\n",
      "epoch:9 step:9055 [D loss: 0.590601, acc.: 69.53%] [G loss: 1.229954]\n",
      "epoch:9 step:9056 [D loss: 0.563464, acc.: 68.75%] [G loss: 1.178330]\n",
      "epoch:9 step:9057 [D loss: 0.645691, acc.: 61.72%] [G loss: 1.217583]\n",
      "epoch:9 step:9058 [D loss: 0.495900, acc.: 78.12%] [G loss: 1.235576]\n",
      "epoch:9 step:9059 [D loss: 0.506518, acc.: 75.00%] [G loss: 1.220771]\n",
      "epoch:9 step:9060 [D loss: 0.584271, acc.: 70.31%] [G loss: 1.228550]\n",
      "epoch:9 step:9061 [D loss: 0.705789, acc.: 60.16%] [G loss: 1.015477]\n",
      "epoch:9 step:9062 [D loss: 0.622354, acc.: 64.84%] [G loss: 1.316501]\n",
      "epoch:9 step:9063 [D loss: 0.684596, acc.: 60.94%] [G loss: 1.216861]\n",
      "epoch:9 step:9064 [D loss: 0.513101, acc.: 79.69%] [G loss: 1.342522]\n",
      "epoch:9 step:9065 [D loss: 0.690170, acc.: 63.28%] [G loss: 1.096278]\n",
      "epoch:9 step:9066 [D loss: 0.517375, acc.: 74.22%] [G loss: 1.419368]\n",
      "epoch:9 step:9067 [D loss: 0.556114, acc.: 70.31%] [G loss: 1.341019]\n",
      "epoch:9 step:9068 [D loss: 0.634658, acc.: 64.06%] [G loss: 1.275125]\n",
      "epoch:9 step:9069 [D loss: 0.727976, acc.: 55.47%] [G loss: 1.131971]\n",
      "epoch:9 step:9070 [D loss: 0.607576, acc.: 64.84%] [G loss: 1.119310]\n",
      "epoch:9 step:9071 [D loss: 0.635567, acc.: 63.28%] [G loss: 1.184062]\n",
      "epoch:9 step:9072 [D loss: 0.610943, acc.: 70.31%] [G loss: 1.210229]\n",
      "epoch:9 step:9073 [D loss: 0.624304, acc.: 61.72%] [G loss: 1.054794]\n",
      "epoch:9 step:9074 [D loss: 0.598174, acc.: 68.75%] [G loss: 0.851286]\n",
      "epoch:9 step:9075 [D loss: 0.584430, acc.: 72.66%] [G loss: 1.155410]\n",
      "epoch:9 step:9076 [D loss: 0.644378, acc.: 60.16%] [G loss: 1.196187]\n",
      "epoch:9 step:9077 [D loss: 0.567675, acc.: 70.31%] [G loss: 1.087937]\n",
      "epoch:9 step:9078 [D loss: 0.534554, acc.: 77.34%] [G loss: 1.224332]\n",
      "epoch:9 step:9079 [D loss: 0.552849, acc.: 75.00%] [G loss: 0.956204]\n",
      "epoch:9 step:9080 [D loss: 0.658167, acc.: 61.72%] [G loss: 0.911515]\n",
      "epoch:9 step:9081 [D loss: 0.534036, acc.: 74.22%] [G loss: 1.252797]\n",
      "epoch:9 step:9082 [D loss: 0.570936, acc.: 73.44%] [G loss: 1.153718]\n",
      "epoch:9 step:9083 [D loss: 0.541867, acc.: 78.12%] [G loss: 0.974695]\n",
      "epoch:9 step:9084 [D loss: 0.560348, acc.: 71.09%] [G loss: 1.351879]\n",
      "epoch:9 step:9085 [D loss: 0.562813, acc.: 67.19%] [G loss: 1.343039]\n",
      "epoch:9 step:9086 [D loss: 0.620474, acc.: 63.28%] [G loss: 1.169724]\n",
      "epoch:9 step:9087 [D loss: 0.656836, acc.: 59.38%] [G loss: 1.154991]\n",
      "epoch:9 step:9088 [D loss: 0.646628, acc.: 63.28%] [G loss: 1.268369]\n",
      "epoch:9 step:9089 [D loss: 0.495588, acc.: 79.69%] [G loss: 1.039119]\n",
      "epoch:9 step:9090 [D loss: 0.741147, acc.: 57.03%] [G loss: 1.185592]\n",
      "epoch:9 step:9091 [D loss: 0.643167, acc.: 64.84%] [G loss: 1.183563]\n",
      "epoch:9 step:9092 [D loss: 0.638771, acc.: 63.28%] [G loss: 1.239016]\n",
      "epoch:9 step:9093 [D loss: 0.601386, acc.: 64.84%] [G loss: 1.144632]\n",
      "epoch:9 step:9094 [D loss: 0.653767, acc.: 63.28%] [G loss: 1.310257]\n",
      "epoch:9 step:9095 [D loss: 0.641267, acc.: 63.28%] [G loss: 1.254381]\n",
      "epoch:9 step:9096 [D loss: 0.710583, acc.: 57.81%] [G loss: 0.897297]\n",
      "epoch:9 step:9097 [D loss: 0.685189, acc.: 59.38%] [G loss: 1.008479]\n",
      "epoch:9 step:9098 [D loss: 0.582387, acc.: 72.66%] [G loss: 1.127498]\n",
      "epoch:9 step:9099 [D loss: 0.607405, acc.: 70.31%] [G loss: 1.178671]\n",
      "epoch:9 step:9100 [D loss: 0.638790, acc.: 61.72%] [G loss: 1.139343]\n",
      "epoch:9 step:9101 [D loss: 0.686014, acc.: 56.25%] [G loss: 0.991533]\n",
      "epoch:9 step:9102 [D loss: 0.572512, acc.: 70.31%] [G loss: 1.215599]\n",
      "epoch:9 step:9103 [D loss: 0.578358, acc.: 71.09%] [G loss: 1.006803]\n",
      "epoch:9 step:9104 [D loss: 0.503034, acc.: 78.91%] [G loss: 1.183962]\n",
      "epoch:9 step:9105 [D loss: 0.552148, acc.: 75.78%] [G loss: 1.152562]\n",
      "epoch:9 step:9106 [D loss: 0.682227, acc.: 62.50%] [G loss: 1.052361]\n",
      "epoch:9 step:9107 [D loss: 0.531796, acc.: 79.69%] [G loss: 1.260129]\n",
      "epoch:9 step:9108 [D loss: 0.588697, acc.: 71.09%] [G loss: 1.160616]\n",
      "epoch:9 step:9109 [D loss: 0.648024, acc.: 64.84%] [G loss: 1.084059]\n",
      "epoch:9 step:9110 [D loss: 0.571898, acc.: 68.75%] [G loss: 1.275285]\n",
      "epoch:9 step:9111 [D loss: 0.670113, acc.: 64.06%] [G loss: 1.215513]\n",
      "epoch:9 step:9112 [D loss: 0.638509, acc.: 64.06%] [G loss: 1.174466]\n",
      "epoch:9 step:9113 [D loss: 0.578873, acc.: 69.53%] [G loss: 1.175455]\n",
      "epoch:9 step:9114 [D loss: 0.588597, acc.: 67.97%] [G loss: 1.050858]\n",
      "epoch:9 step:9115 [D loss: 0.538186, acc.: 78.12%] [G loss: 1.235011]\n",
      "epoch:9 step:9116 [D loss: 0.707941, acc.: 56.25%] [G loss: 1.335570]\n",
      "epoch:9 step:9117 [D loss: 0.609139, acc.: 67.19%] [G loss: 1.201381]\n",
      "epoch:9 step:9118 [D loss: 0.508357, acc.: 75.78%] [G loss: 1.373057]\n",
      "epoch:9 step:9119 [D loss: 0.616963, acc.: 64.06%] [G loss: 1.168042]\n",
      "epoch:9 step:9120 [D loss: 0.632590, acc.: 64.06%] [G loss: 1.179583]\n",
      "epoch:9 step:9121 [D loss: 0.670815, acc.: 60.16%] [G loss: 1.077668]\n",
      "epoch:9 step:9122 [D loss: 0.587435, acc.: 66.41%] [G loss: 1.191522]\n",
      "epoch:9 step:9123 [D loss: 0.607644, acc.: 62.50%] [G loss: 1.253000]\n",
      "epoch:9 step:9124 [D loss: 0.677412, acc.: 59.38%] [G loss: 1.243243]\n",
      "epoch:9 step:9125 [D loss: 0.622976, acc.: 63.28%] [G loss: 1.231501]\n",
      "epoch:9 step:9126 [D loss: 0.771118, acc.: 49.22%] [G loss: 1.035587]\n",
      "epoch:9 step:9127 [D loss: 0.626277, acc.: 65.62%] [G loss: 1.364926]\n",
      "epoch:9 step:9128 [D loss: 0.572626, acc.: 71.88%] [G loss: 1.310456]\n",
      "epoch:9 step:9129 [D loss: 0.555453, acc.: 76.56%] [G loss: 1.250722]\n",
      "epoch:9 step:9130 [D loss: 0.514729, acc.: 78.12%] [G loss: 1.121182]\n",
      "epoch:9 step:9131 [D loss: 0.645598, acc.: 66.41%] [G loss: 1.092104]\n",
      "epoch:9 step:9132 [D loss: 0.677799, acc.: 57.81%] [G loss: 1.157469]\n",
      "epoch:9 step:9133 [D loss: 0.488909, acc.: 75.00%] [G loss: 1.251793]\n",
      "epoch:9 step:9134 [D loss: 0.611236, acc.: 66.41%] [G loss: 1.383702]\n",
      "epoch:9 step:9135 [D loss: 0.541530, acc.: 71.09%] [G loss: 1.201813]\n",
      "epoch:9 step:9136 [D loss: 0.609284, acc.: 68.75%] [G loss: 1.144229]\n",
      "epoch:9 step:9137 [D loss: 0.659182, acc.: 64.06%] [G loss: 1.128468]\n",
      "epoch:9 step:9138 [D loss: 0.575092, acc.: 76.56%] [G loss: 1.121833]\n",
      "epoch:9 step:9139 [D loss: 0.639736, acc.: 59.38%] [G loss: 1.131552]\n",
      "epoch:9 step:9140 [D loss: 0.580915, acc.: 70.31%] [G loss: 1.081738]\n",
      "epoch:9 step:9141 [D loss: 0.540487, acc.: 70.31%] [G loss: 1.070886]\n",
      "epoch:9 step:9142 [D loss: 0.662814, acc.: 66.41%] [G loss: 0.934457]\n",
      "epoch:9 step:9143 [D loss: 0.571353, acc.: 69.53%] [G loss: 1.079663]\n",
      "epoch:9 step:9144 [D loss: 0.541698, acc.: 71.09%] [G loss: 1.437528]\n",
      "epoch:9 step:9145 [D loss: 0.461110, acc.: 81.25%] [G loss: 1.526989]\n",
      "epoch:9 step:9146 [D loss: 0.525976, acc.: 75.00%] [G loss: 1.301124]\n",
      "epoch:9 step:9147 [D loss: 0.621199, acc.: 65.62%] [G loss: 1.121646]\n",
      "epoch:9 step:9148 [D loss: 0.701321, acc.: 53.91%] [G loss: 1.047520]\n",
      "epoch:9 step:9149 [D loss: 0.645179, acc.: 65.62%] [G loss: 1.031615]\n",
      "epoch:9 step:9150 [D loss: 0.450967, acc.: 83.59%] [G loss: 1.169938]\n",
      "epoch:9 step:9151 [D loss: 0.606315, acc.: 73.44%] [G loss: 0.912411]\n",
      "epoch:9 step:9152 [D loss: 0.645380, acc.: 64.06%] [G loss: 1.146100]\n",
      "epoch:9 step:9153 [D loss: 0.539061, acc.: 72.66%] [G loss: 1.227471]\n",
      "epoch:9 step:9154 [D loss: 0.623239, acc.: 64.06%] [G loss: 1.136840]\n",
      "epoch:9 step:9155 [D loss: 0.675397, acc.: 60.16%] [G loss: 1.409319]\n",
      "epoch:9 step:9156 [D loss: 0.499808, acc.: 78.91%] [G loss: 1.279627]\n",
      "epoch:9 step:9157 [D loss: 0.604826, acc.: 67.97%] [G loss: 1.119556]\n",
      "epoch:9 step:9158 [D loss: 0.644070, acc.: 59.38%] [G loss: 1.050653]\n",
      "epoch:9 step:9159 [D loss: 0.587540, acc.: 70.31%] [G loss: 1.055218]\n",
      "epoch:9 step:9160 [D loss: 0.504723, acc.: 78.12%] [G loss: 1.247859]\n",
      "epoch:9 step:9161 [D loss: 0.547735, acc.: 73.44%] [G loss: 1.116401]\n",
      "epoch:9 step:9162 [D loss: 0.537059, acc.: 72.66%] [G loss: 1.290188]\n",
      "epoch:9 step:9163 [D loss: 0.584745, acc.: 71.09%] [G loss: 1.354217]\n",
      "epoch:9 step:9164 [D loss: 0.659740, acc.: 61.72%] [G loss: 1.113215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9165 [D loss: 0.606458, acc.: 63.28%] [G loss: 1.005727]\n",
      "epoch:9 step:9166 [D loss: 0.583157, acc.: 71.88%] [G loss: 1.104800]\n",
      "epoch:9 step:9167 [D loss: 0.633739, acc.: 64.84%] [G loss: 1.141435]\n",
      "epoch:9 step:9168 [D loss: 0.677400, acc.: 58.59%] [G loss: 1.600592]\n",
      "epoch:9 step:9169 [D loss: 0.539289, acc.: 74.22%] [G loss: 1.188523]\n",
      "epoch:9 step:9170 [D loss: 0.750629, acc.: 53.91%] [G loss: 1.114267]\n",
      "epoch:9 step:9171 [D loss: 0.618130, acc.: 71.09%] [G loss: 1.040661]\n",
      "epoch:9 step:9172 [D loss: 0.566033, acc.: 74.22%] [G loss: 1.006840]\n",
      "epoch:9 step:9173 [D loss: 0.609197, acc.: 70.31%] [G loss: 1.323876]\n",
      "epoch:9 step:9174 [D loss: 0.749750, acc.: 56.25%] [G loss: 1.253505]\n",
      "epoch:9 step:9175 [D loss: 0.485868, acc.: 79.69%] [G loss: 1.339356]\n",
      "epoch:9 step:9176 [D loss: 0.671560, acc.: 60.94%] [G loss: 1.258483]\n",
      "epoch:9 step:9177 [D loss: 0.545915, acc.: 71.88%] [G loss: 1.489701]\n",
      "epoch:9 step:9178 [D loss: 0.682752, acc.: 60.16%] [G loss: 1.113016]\n",
      "epoch:9 step:9179 [D loss: 0.543658, acc.: 73.44%] [G loss: 1.438561]\n",
      "epoch:9 step:9180 [D loss: 0.558672, acc.: 75.00%] [G loss: 1.155058]\n",
      "epoch:9 step:9181 [D loss: 0.448723, acc.: 82.81%] [G loss: 1.164080]\n",
      "epoch:9 step:9182 [D loss: 0.714534, acc.: 55.47%] [G loss: 1.186012]\n",
      "epoch:9 step:9183 [D loss: 0.569029, acc.: 72.66%] [G loss: 1.247698]\n",
      "epoch:9 step:9184 [D loss: 0.714466, acc.: 57.81%] [G loss: 1.049700]\n",
      "epoch:9 step:9185 [D loss: 0.607257, acc.: 67.19%] [G loss: 1.291010]\n",
      "epoch:9 step:9186 [D loss: 0.594653, acc.: 64.84%] [G loss: 1.262904]\n",
      "epoch:9 step:9187 [D loss: 0.722578, acc.: 50.78%] [G loss: 1.077079]\n",
      "epoch:9 step:9188 [D loss: 0.596074, acc.: 65.62%] [G loss: 1.233253]\n",
      "epoch:9 step:9189 [D loss: 0.642906, acc.: 65.62%] [G loss: 1.235705]\n",
      "epoch:9 step:9190 [D loss: 0.635124, acc.: 60.94%] [G loss: 1.097549]\n",
      "epoch:9 step:9191 [D loss: 0.582728, acc.: 64.84%] [G loss: 1.052288]\n",
      "epoch:9 step:9192 [D loss: 0.659187, acc.: 64.84%] [G loss: 1.011673]\n",
      "epoch:9 step:9193 [D loss: 0.546726, acc.: 74.22%] [G loss: 1.036294]\n",
      "epoch:9 step:9194 [D loss: 0.593846, acc.: 66.41%] [G loss: 1.269830]\n",
      "epoch:9 step:9195 [D loss: 0.737246, acc.: 56.25%] [G loss: 0.834879]\n",
      "epoch:9 step:9196 [D loss: 0.512462, acc.: 76.56%] [G loss: 1.204675]\n",
      "epoch:9 step:9197 [D loss: 0.471587, acc.: 82.81%] [G loss: 1.166387]\n",
      "epoch:9 step:9198 [D loss: 0.583339, acc.: 69.53%] [G loss: 1.311004]\n",
      "epoch:9 step:9199 [D loss: 0.516011, acc.: 78.12%] [G loss: 1.098874]\n",
      "epoch:9 step:9200 [D loss: 0.576168, acc.: 71.88%] [G loss: 1.263106]\n",
      "##############\n",
      "[2.58157923 1.98424647 2.00682331 2.62429108 0.90845736 5.34328156\n",
      " 2.43145775 2.68214675 3.81809495 7.14771273]\n",
      "##########\n",
      "epoch:9 step:9201 [D loss: 0.665934, acc.: 60.94%] [G loss: 1.250105]\n",
      "epoch:9 step:9202 [D loss: 0.621699, acc.: 66.41%] [G loss: 1.111182]\n",
      "epoch:9 step:9203 [D loss: 0.713833, acc.: 50.78%] [G loss: 1.209785]\n",
      "epoch:9 step:9204 [D loss: 0.592290, acc.: 66.41%] [G loss: 1.125296]\n",
      "epoch:9 step:9205 [D loss: 0.707683, acc.: 58.59%] [G loss: 1.055091]\n",
      "epoch:9 step:9206 [D loss: 0.638842, acc.: 64.84%] [G loss: 1.412586]\n",
      "epoch:9 step:9207 [D loss: 0.565570, acc.: 67.97%] [G loss: 1.319623]\n",
      "epoch:9 step:9208 [D loss: 0.653956, acc.: 65.62%] [G loss: 0.920923]\n",
      "epoch:9 step:9209 [D loss: 0.548811, acc.: 73.44%] [G loss: 1.091820]\n",
      "epoch:9 step:9210 [D loss: 0.574906, acc.: 67.19%] [G loss: 1.259064]\n",
      "epoch:9 step:9211 [D loss: 0.548601, acc.: 71.88%] [G loss: 1.329968]\n",
      "epoch:9 step:9212 [D loss: 0.524345, acc.: 77.34%] [G loss: 1.587506]\n",
      "epoch:9 step:9213 [D loss: 0.678426, acc.: 57.81%] [G loss: 1.047325]\n",
      "epoch:9 step:9214 [D loss: 0.621037, acc.: 67.19%] [G loss: 1.220277]\n",
      "epoch:9 step:9215 [D loss: 0.490026, acc.: 78.12%] [G loss: 0.988603]\n",
      "epoch:9 step:9216 [D loss: 0.592168, acc.: 71.88%] [G loss: 1.022079]\n",
      "epoch:9 step:9217 [D loss: 0.657060, acc.: 65.62%] [G loss: 1.147639]\n",
      "epoch:9 step:9218 [D loss: 0.602427, acc.: 68.75%] [G loss: 1.086738]\n",
      "epoch:9 step:9219 [D loss: 0.666420, acc.: 61.72%] [G loss: 1.204203]\n",
      "epoch:9 step:9220 [D loss: 0.533545, acc.: 72.66%] [G loss: 1.282524]\n",
      "epoch:9 step:9221 [D loss: 0.550973, acc.: 66.41%] [G loss: 1.265331]\n",
      "epoch:9 step:9222 [D loss: 0.659573, acc.: 54.69%] [G loss: 1.104329]\n",
      "epoch:9 step:9223 [D loss: 0.712681, acc.: 56.25%] [G loss: 1.075582]\n",
      "epoch:9 step:9224 [D loss: 0.612599, acc.: 61.72%] [G loss: 1.262982]\n",
      "epoch:9 step:9225 [D loss: 0.703509, acc.: 54.69%] [G loss: 1.129224]\n",
      "epoch:9 step:9226 [D loss: 0.578155, acc.: 70.31%] [G loss: 1.136502]\n",
      "epoch:9 step:9227 [D loss: 0.614075, acc.: 67.19%] [G loss: 1.048010]\n",
      "epoch:9 step:9228 [D loss: 0.605290, acc.: 65.62%] [G loss: 1.141101]\n",
      "epoch:9 step:9229 [D loss: 0.622388, acc.: 71.88%] [G loss: 1.238687]\n",
      "epoch:9 step:9230 [D loss: 0.648880, acc.: 59.38%] [G loss: 1.198825]\n",
      "epoch:9 step:9231 [D loss: 0.674121, acc.: 64.84%] [G loss: 0.982440]\n",
      "epoch:9 step:9232 [D loss: 0.562976, acc.: 73.44%] [G loss: 0.938950]\n",
      "epoch:9 step:9233 [D loss: 0.603409, acc.: 62.50%] [G loss: 1.293143]\n",
      "epoch:9 step:9234 [D loss: 0.631391, acc.: 64.06%] [G loss: 1.380926]\n",
      "epoch:9 step:9235 [D loss: 0.730171, acc.: 55.47%] [G loss: 1.176283]\n",
      "epoch:9 step:9236 [D loss: 0.594564, acc.: 71.88%] [G loss: 0.953229]\n",
      "epoch:9 step:9237 [D loss: 0.572862, acc.: 72.66%] [G loss: 1.150414]\n",
      "epoch:9 step:9238 [D loss: 0.685155, acc.: 61.72%] [G loss: 1.302923]\n",
      "epoch:9 step:9239 [D loss: 0.630867, acc.: 62.50%] [G loss: 1.116446]\n",
      "epoch:9 step:9240 [D loss: 0.592938, acc.: 71.88%] [G loss: 1.349355]\n",
      "epoch:9 step:9241 [D loss: 0.509874, acc.: 75.78%] [G loss: 1.164241]\n",
      "epoch:9 step:9242 [D loss: 0.575081, acc.: 71.88%] [G loss: 1.298811]\n",
      "epoch:9 step:9243 [D loss: 0.533087, acc.: 75.78%] [G loss: 1.260071]\n",
      "epoch:9 step:9244 [D loss: 0.623133, acc.: 57.81%] [G loss: 1.127104]\n",
      "epoch:9 step:9245 [D loss: 0.771015, acc.: 51.56%] [G loss: 1.078840]\n",
      "epoch:9 step:9246 [D loss: 0.579579, acc.: 68.75%] [G loss: 1.261413]\n",
      "epoch:9 step:9247 [D loss: 0.545783, acc.: 74.22%] [G loss: 1.020712]\n",
      "epoch:9 step:9248 [D loss: 0.717217, acc.: 56.25%] [G loss: 1.090853]\n",
      "epoch:9 step:9249 [D loss: 0.502683, acc.: 79.69%] [G loss: 1.198618]\n",
      "epoch:9 step:9250 [D loss: 0.625147, acc.: 65.62%] [G loss: 1.237145]\n",
      "epoch:9 step:9251 [D loss: 0.606515, acc.: 60.94%] [G loss: 1.373853]\n",
      "epoch:9 step:9252 [D loss: 0.578999, acc.: 77.34%] [G loss: 1.134007]\n",
      "epoch:9 step:9253 [D loss: 0.566322, acc.: 66.41%] [G loss: 1.390877]\n",
      "epoch:9 step:9254 [D loss: 0.599432, acc.: 67.19%] [G loss: 1.152309]\n",
      "epoch:9 step:9255 [D loss: 0.621252, acc.: 61.72%] [G loss: 1.106793]\n",
      "epoch:9 step:9256 [D loss: 0.640853, acc.: 60.16%] [G loss: 1.187745]\n",
      "epoch:9 step:9257 [D loss: 0.666717, acc.: 60.16%] [G loss: 1.017408]\n",
      "epoch:9 step:9258 [D loss: 0.650143, acc.: 58.59%] [G loss: 1.178488]\n",
      "epoch:9 step:9259 [D loss: 0.549295, acc.: 74.22%] [G loss: 1.383555]\n",
      "epoch:9 step:9260 [D loss: 0.535237, acc.: 72.66%] [G loss: 1.377505]\n",
      "epoch:9 step:9261 [D loss: 0.598922, acc.: 66.41%] [G loss: 1.283750]\n",
      "epoch:9 step:9262 [D loss: 0.598930, acc.: 71.88%] [G loss: 1.248847]\n",
      "epoch:9 step:9263 [D loss: 0.627168, acc.: 63.28%] [G loss: 1.196531]\n",
      "epoch:9 step:9264 [D loss: 0.673466, acc.: 53.91%] [G loss: 1.097077]\n",
      "epoch:9 step:9265 [D loss: 0.689731, acc.: 58.59%] [G loss: 1.007870]\n",
      "epoch:9 step:9266 [D loss: 0.548728, acc.: 75.78%] [G loss: 1.424856]\n",
      "epoch:9 step:9267 [D loss: 0.605228, acc.: 66.41%] [G loss: 1.149439]\n",
      "epoch:9 step:9268 [D loss: 0.652491, acc.: 62.50%] [G loss: 1.058854]\n",
      "epoch:9 step:9269 [D loss: 0.681398, acc.: 57.03%] [G loss: 1.234813]\n",
      "epoch:9 step:9270 [D loss: 0.634068, acc.: 63.28%] [G loss: 1.251706]\n",
      "epoch:9 step:9271 [D loss: 0.572915, acc.: 69.53%] [G loss: 1.118709]\n",
      "epoch:9 step:9272 [D loss: 0.701362, acc.: 60.94%] [G loss: 1.281038]\n",
      "epoch:9 step:9273 [D loss: 0.549351, acc.: 72.66%] [G loss: 1.255583]\n",
      "epoch:9 step:9274 [D loss: 0.679911, acc.: 57.81%] [G loss: 1.300410]\n",
      "epoch:9 step:9275 [D loss: 0.612633, acc.: 65.62%] [G loss: 1.195298]\n",
      "epoch:9 step:9276 [D loss: 0.640646, acc.: 60.16%] [G loss: 1.017800]\n",
      "epoch:9 step:9277 [D loss: 0.663965, acc.: 62.50%] [G loss: 1.101953]\n",
      "epoch:9 step:9278 [D loss: 0.501957, acc.: 78.91%] [G loss: 1.115249]\n",
      "epoch:9 step:9279 [D loss: 0.691549, acc.: 53.91%] [G loss: 1.267776]\n",
      "epoch:9 step:9280 [D loss: 0.516481, acc.: 76.56%] [G loss: 1.173313]\n",
      "epoch:9 step:9281 [D loss: 0.640170, acc.: 61.72%] [G loss: 1.325966]\n",
      "epoch:9 step:9282 [D loss: 0.570850, acc.: 68.75%] [G loss: 1.146788]\n",
      "epoch:9 step:9283 [D loss: 0.625738, acc.: 66.41%] [G loss: 1.216953]\n",
      "epoch:9 step:9284 [D loss: 0.693000, acc.: 56.25%] [G loss: 1.239901]\n",
      "epoch:9 step:9285 [D loss: 0.477473, acc.: 81.25%] [G loss: 1.321030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9286 [D loss: 0.531655, acc.: 76.56%] [G loss: 1.395817]\n",
      "epoch:9 step:9287 [D loss: 0.591583, acc.: 64.06%] [G loss: 1.202921]\n",
      "epoch:9 step:9288 [D loss: 0.673947, acc.: 62.50%] [G loss: 1.029220]\n",
      "epoch:9 step:9289 [D loss: 0.652461, acc.: 64.84%] [G loss: 1.211703]\n",
      "epoch:9 step:9290 [D loss: 0.556476, acc.: 70.31%] [G loss: 1.325383]\n",
      "epoch:9 step:9291 [D loss: 0.637775, acc.: 67.19%] [G loss: 1.098151]\n",
      "epoch:9 step:9292 [D loss: 0.644598, acc.: 63.28%] [G loss: 1.039236]\n",
      "epoch:9 step:9293 [D loss: 0.594414, acc.: 67.97%] [G loss: 1.066941]\n",
      "epoch:9 step:9294 [D loss: 0.536866, acc.: 75.00%] [G loss: 1.312241]\n",
      "epoch:9 step:9295 [D loss: 0.667206, acc.: 59.38%] [G loss: 1.076756]\n",
      "epoch:9 step:9296 [D loss: 0.548097, acc.: 75.00%] [G loss: 0.972741]\n",
      "epoch:9 step:9297 [D loss: 0.643157, acc.: 61.72%] [G loss: 1.169578]\n",
      "epoch:9 step:9298 [D loss: 0.651418, acc.: 66.41%] [G loss: 1.108636]\n",
      "epoch:9 step:9299 [D loss: 0.688447, acc.: 55.47%] [G loss: 0.961463]\n",
      "epoch:9 step:9300 [D loss: 0.623026, acc.: 69.53%] [G loss: 1.179092]\n",
      "epoch:9 step:9301 [D loss: 0.578939, acc.: 74.22%] [G loss: 1.116119]\n",
      "epoch:9 step:9302 [D loss: 0.616651, acc.: 62.50%] [G loss: 1.175083]\n",
      "epoch:9 step:9303 [D loss: 0.548421, acc.: 73.44%] [G loss: 1.099193]\n",
      "epoch:9 step:9304 [D loss: 0.801189, acc.: 45.31%] [G loss: 0.984079]\n",
      "epoch:9 step:9305 [D loss: 0.602822, acc.: 67.97%] [G loss: 1.348317]\n",
      "epoch:9 step:9306 [D loss: 0.514706, acc.: 80.47%] [G loss: 1.137918]\n",
      "epoch:9 step:9307 [D loss: 0.600905, acc.: 65.62%] [G loss: 1.160429]\n",
      "epoch:9 step:9308 [D loss: 0.552415, acc.: 71.09%] [G loss: 1.154581]\n",
      "epoch:9 step:9309 [D loss: 0.798474, acc.: 43.75%] [G loss: 1.142510]\n",
      "epoch:9 step:9310 [D loss: 0.609290, acc.: 66.41%] [G loss: 1.198363]\n",
      "epoch:9 step:9311 [D loss: 0.654833, acc.: 60.16%] [G loss: 1.175936]\n",
      "epoch:9 step:9312 [D loss: 0.683912, acc.: 60.94%] [G loss: 1.280406]\n",
      "epoch:9 step:9313 [D loss: 0.596150, acc.: 64.84%] [G loss: 1.099683]\n",
      "epoch:9 step:9314 [D loss: 0.690135, acc.: 64.06%] [G loss: 1.035091]\n",
      "epoch:9 step:9315 [D loss: 0.592384, acc.: 69.53%] [G loss: 1.175550]\n",
      "epoch:9 step:9316 [D loss: 0.621476, acc.: 62.50%] [G loss: 1.283580]\n",
      "epoch:9 step:9317 [D loss: 0.584706, acc.: 64.84%] [G loss: 1.312913]\n",
      "epoch:9 step:9318 [D loss: 0.614035, acc.: 66.41%] [G loss: 1.275915]\n",
      "epoch:9 step:9319 [D loss: 0.550106, acc.: 76.56%] [G loss: 1.161245]\n",
      "epoch:9 step:9320 [D loss: 0.639960, acc.: 64.06%] [G loss: 1.234618]\n",
      "epoch:9 step:9321 [D loss: 0.688921, acc.: 54.69%] [G loss: 1.171067]\n",
      "epoch:9 step:9322 [D loss: 0.624669, acc.: 64.06%] [G loss: 1.200262]\n",
      "epoch:9 step:9323 [D loss: 0.585632, acc.: 67.97%] [G loss: 1.008219]\n",
      "epoch:9 step:9324 [D loss: 0.698273, acc.: 53.91%] [G loss: 1.227566]\n",
      "epoch:9 step:9325 [D loss: 0.619336, acc.: 67.19%] [G loss: 1.148127]\n",
      "epoch:9 step:9326 [D loss: 0.637183, acc.: 64.84%] [G loss: 1.268486]\n",
      "epoch:9 step:9327 [D loss: 0.689474, acc.: 61.72%] [G loss: 1.194361]\n",
      "epoch:9 step:9328 [D loss: 0.642938, acc.: 60.16%] [G loss: 1.093176]\n",
      "epoch:9 step:9329 [D loss: 0.665623, acc.: 57.03%] [G loss: 0.946396]\n",
      "epoch:9 step:9330 [D loss: 0.635959, acc.: 60.94%] [G loss: 1.306921]\n",
      "epoch:9 step:9331 [D loss: 0.567125, acc.: 74.22%] [G loss: 1.243815]\n",
      "epoch:9 step:9332 [D loss: 0.610339, acc.: 64.84%] [G loss: 1.053433]\n",
      "epoch:9 step:9333 [D loss: 0.619381, acc.: 64.84%] [G loss: 1.234668]\n",
      "epoch:9 step:9334 [D loss: 0.659978, acc.: 60.16%] [G loss: 1.253623]\n",
      "epoch:9 step:9335 [D loss: 0.517658, acc.: 76.56%] [G loss: 1.249634]\n",
      "epoch:9 step:9336 [D loss: 0.577959, acc.: 72.66%] [G loss: 1.149676]\n",
      "epoch:9 step:9337 [D loss: 0.624973, acc.: 70.31%] [G loss: 1.258175]\n",
      "epoch:9 step:9338 [D loss: 0.611964, acc.: 70.31%] [G loss: 1.105478]\n",
      "epoch:9 step:9339 [D loss: 0.596405, acc.: 66.41%] [G loss: 1.288603]\n",
      "epoch:9 step:9340 [D loss: 0.631245, acc.: 65.62%] [G loss: 1.207891]\n",
      "epoch:9 step:9341 [D loss: 0.613208, acc.: 63.28%] [G loss: 1.123089]\n",
      "epoch:9 step:9342 [D loss: 0.644381, acc.: 64.84%] [G loss: 0.898298]\n",
      "epoch:9 step:9343 [D loss: 0.514183, acc.: 73.44%] [G loss: 1.165744]\n",
      "epoch:9 step:9344 [D loss: 0.686940, acc.: 62.50%] [G loss: 1.336325]\n",
      "epoch:9 step:9345 [D loss: 0.739698, acc.: 51.56%] [G loss: 1.166160]\n",
      "epoch:9 step:9346 [D loss: 0.643778, acc.: 67.19%] [G loss: 1.295711]\n",
      "epoch:9 step:9347 [D loss: 0.641717, acc.: 64.06%] [G loss: 0.951271]\n",
      "epoch:9 step:9348 [D loss: 0.591002, acc.: 63.28%] [G loss: 1.073054]\n",
      "epoch:9 step:9349 [D loss: 0.589097, acc.: 70.31%] [G loss: 1.172308]\n",
      "epoch:9 step:9350 [D loss: 0.682519, acc.: 62.50%] [G loss: 1.044369]\n",
      "epoch:9 step:9351 [D loss: 0.634600, acc.: 63.28%] [G loss: 1.163532]\n",
      "epoch:9 step:9352 [D loss: 0.502294, acc.: 80.47%] [G loss: 1.308280]\n",
      "epoch:9 step:9353 [D loss: 0.571925, acc.: 70.31%] [G loss: 1.225785]\n",
      "epoch:9 step:9354 [D loss: 0.588537, acc.: 71.88%] [G loss: 1.251839]\n",
      "epoch:9 step:9355 [D loss: 0.545347, acc.: 75.78%] [G loss: 1.141430]\n",
      "epoch:9 step:9356 [D loss: 0.673033, acc.: 59.38%] [G loss: 0.973270]\n",
      "epoch:9 step:9357 [D loss: 0.582883, acc.: 69.53%] [G loss: 1.200166]\n",
      "epoch:9 step:9358 [D loss: 0.555131, acc.: 68.75%] [G loss: 1.452911]\n",
      "epoch:9 step:9359 [D loss: 0.559927, acc.: 73.44%] [G loss: 1.396752]\n",
      "epoch:9 step:9360 [D loss: 0.705166, acc.: 50.78%] [G loss: 1.004536]\n",
      "epoch:9 step:9361 [D loss: 0.526397, acc.: 77.34%] [G loss: 1.251274]\n",
      "epoch:9 step:9362 [D loss: 0.644588, acc.: 64.84%] [G loss: 1.146003]\n",
      "epoch:9 step:9363 [D loss: 0.606142, acc.: 67.97%] [G loss: 1.217729]\n",
      "epoch:9 step:9364 [D loss: 0.621714, acc.: 66.41%] [G loss: 1.040088]\n",
      "epoch:9 step:9365 [D loss: 0.643141, acc.: 67.19%] [G loss: 1.307277]\n",
      "epoch:9 step:9366 [D loss: 0.754361, acc.: 50.78%] [G loss: 1.048565]\n",
      "epoch:9 step:9367 [D loss: 0.530240, acc.: 75.00%] [G loss: 1.142145]\n",
      "epoch:9 step:9368 [D loss: 0.592197, acc.: 64.06%] [G loss: 1.207342]\n",
      "epoch:9 step:9369 [D loss: 0.450747, acc.: 87.50%] [G loss: 1.278098]\n",
      "epoch:9 step:9370 [D loss: 0.552930, acc.: 75.78%] [G loss: 1.240614]\n",
      "epoch:10 step:9371 [D loss: 0.591878, acc.: 70.31%] [G loss: 0.996804]\n",
      "epoch:10 step:9372 [D loss: 0.654354, acc.: 62.50%] [G loss: 0.966114]\n",
      "epoch:10 step:9373 [D loss: 0.620518, acc.: 64.06%] [G loss: 1.027640]\n",
      "epoch:10 step:9374 [D loss: 0.564772, acc.: 70.31%] [G loss: 1.184560]\n",
      "epoch:10 step:9375 [D loss: 0.634750, acc.: 66.41%] [G loss: 0.976968]\n",
      "epoch:10 step:9376 [D loss: 0.617123, acc.: 68.75%] [G loss: 1.055269]\n",
      "epoch:10 step:9377 [D loss: 0.634859, acc.: 66.41%] [G loss: 1.061457]\n",
      "epoch:10 step:9378 [D loss: 0.577632, acc.: 69.53%] [G loss: 0.996238]\n",
      "epoch:10 step:9379 [D loss: 0.520925, acc.: 70.31%] [G loss: 1.503724]\n",
      "epoch:10 step:9380 [D loss: 0.596130, acc.: 71.88%] [G loss: 1.299054]\n",
      "epoch:10 step:9381 [D loss: 0.543730, acc.: 72.66%] [G loss: 1.260034]\n",
      "epoch:10 step:9382 [D loss: 0.600480, acc.: 62.50%] [G loss: 1.087842]\n",
      "epoch:10 step:9383 [D loss: 0.553112, acc.: 73.44%] [G loss: 1.291248]\n",
      "epoch:10 step:9384 [D loss: 0.680236, acc.: 60.16%] [G loss: 1.100965]\n",
      "epoch:10 step:9385 [D loss: 0.557302, acc.: 73.44%] [G loss: 1.025231]\n",
      "epoch:10 step:9386 [D loss: 0.584521, acc.: 74.22%] [G loss: 1.214481]\n",
      "epoch:10 step:9387 [D loss: 0.557611, acc.: 71.09%] [G loss: 1.057602]\n",
      "epoch:10 step:9388 [D loss: 0.630994, acc.: 62.50%] [G loss: 1.189428]\n",
      "epoch:10 step:9389 [D loss: 0.724198, acc.: 55.47%] [G loss: 1.097959]\n",
      "epoch:10 step:9390 [D loss: 0.572533, acc.: 73.44%] [G loss: 1.306036]\n",
      "epoch:10 step:9391 [D loss: 0.703176, acc.: 53.12%] [G loss: 1.060755]\n",
      "epoch:10 step:9392 [D loss: 0.636914, acc.: 66.41%] [G loss: 1.164754]\n",
      "epoch:10 step:9393 [D loss: 0.600543, acc.: 67.97%] [G loss: 1.116329]\n",
      "epoch:10 step:9394 [D loss: 0.466551, acc.: 79.69%] [G loss: 1.273060]\n",
      "epoch:10 step:9395 [D loss: 0.630877, acc.: 64.84%] [G loss: 1.084818]\n",
      "epoch:10 step:9396 [D loss: 0.690330, acc.: 57.03%] [G loss: 1.181468]\n",
      "epoch:10 step:9397 [D loss: 0.538964, acc.: 74.22%] [G loss: 1.288723]\n",
      "epoch:10 step:9398 [D loss: 0.544234, acc.: 75.00%] [G loss: 1.245436]\n",
      "epoch:10 step:9399 [D loss: 0.643564, acc.: 60.16%] [G loss: 1.013876]\n",
      "epoch:10 step:9400 [D loss: 0.664330, acc.: 61.72%] [G loss: 0.979954]\n",
      "##############\n",
      "[2.75372811 2.1642329  1.98657449 2.99154213 0.99315648 6.62037011\n",
      " 2.32119704 2.86742615 3.83922044 7.14868929]\n",
      "##########\n",
      "epoch:10 step:9401 [D loss: 0.708457, acc.: 52.34%] [G loss: 1.082477]\n",
      "epoch:10 step:9402 [D loss: 0.589696, acc.: 67.19%] [G loss: 1.263289]\n",
      "epoch:10 step:9403 [D loss: 0.710808, acc.: 55.47%] [G loss: 1.041201]\n",
      "epoch:10 step:9404 [D loss: 0.678224, acc.: 60.16%] [G loss: 1.321022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9405 [D loss: 0.699722, acc.: 55.47%] [G loss: 1.108920]\n",
      "epoch:10 step:9406 [D loss: 0.480083, acc.: 83.59%] [G loss: 1.232594]\n",
      "epoch:10 step:9407 [D loss: 0.529671, acc.: 75.00%] [G loss: 1.359886]\n",
      "epoch:10 step:9408 [D loss: 0.702201, acc.: 58.59%] [G loss: 1.148926]\n",
      "epoch:10 step:9409 [D loss: 0.725959, acc.: 55.47%] [G loss: 1.199289]\n",
      "epoch:10 step:9410 [D loss: 0.588939, acc.: 66.41%] [G loss: 1.118298]\n",
      "epoch:10 step:9411 [D loss: 0.581377, acc.: 67.19%] [G loss: 1.059047]\n",
      "epoch:10 step:9412 [D loss: 0.569564, acc.: 71.09%] [G loss: 1.234740]\n",
      "epoch:10 step:9413 [D loss: 0.560215, acc.: 71.88%] [G loss: 1.215067]\n",
      "epoch:10 step:9414 [D loss: 0.677604, acc.: 63.28%] [G loss: 0.958939]\n",
      "epoch:10 step:9415 [D loss: 0.672325, acc.: 61.72%] [G loss: 1.104063]\n",
      "epoch:10 step:9416 [D loss: 0.742330, acc.: 50.78%] [G loss: 0.927804]\n",
      "epoch:10 step:9417 [D loss: 0.669095, acc.: 57.81%] [G loss: 1.045941]\n",
      "epoch:10 step:9418 [D loss: 0.589562, acc.: 67.97%] [G loss: 1.200458]\n",
      "epoch:10 step:9419 [D loss: 0.583637, acc.: 67.97%] [G loss: 1.310170]\n",
      "epoch:10 step:9420 [D loss: 0.537413, acc.: 72.66%] [G loss: 1.601483]\n",
      "epoch:10 step:9421 [D loss: 0.581261, acc.: 73.44%] [G loss: 1.059719]\n",
      "epoch:10 step:9422 [D loss: 0.646157, acc.: 64.06%] [G loss: 1.391973]\n",
      "epoch:10 step:9423 [D loss: 0.577755, acc.: 74.22%] [G loss: 1.163779]\n",
      "epoch:10 step:9424 [D loss: 0.612718, acc.: 69.53%] [G loss: 1.100347]\n",
      "epoch:10 step:9425 [D loss: 0.634716, acc.: 65.62%] [G loss: 1.177228]\n",
      "epoch:10 step:9426 [D loss: 0.566583, acc.: 73.44%] [G loss: 1.354012]\n",
      "epoch:10 step:9427 [D loss: 0.615538, acc.: 64.84%] [G loss: 1.199869]\n",
      "epoch:10 step:9428 [D loss: 0.572307, acc.: 73.44%] [G loss: 1.260573]\n",
      "epoch:10 step:9429 [D loss: 0.592817, acc.: 65.62%] [G loss: 1.061582]\n",
      "epoch:10 step:9430 [D loss: 0.545443, acc.: 75.78%] [G loss: 0.986567]\n",
      "epoch:10 step:9431 [D loss: 0.599957, acc.: 71.09%] [G loss: 1.278041]\n",
      "epoch:10 step:9432 [D loss: 0.505848, acc.: 82.81%] [G loss: 1.047613]\n",
      "epoch:10 step:9433 [D loss: 0.601096, acc.: 67.19%] [G loss: 1.115642]\n",
      "epoch:10 step:9434 [D loss: 0.713114, acc.: 55.47%] [G loss: 1.223304]\n",
      "epoch:10 step:9435 [D loss: 0.557128, acc.: 71.88%] [G loss: 1.128120]\n",
      "epoch:10 step:9436 [D loss: 0.748384, acc.: 54.69%] [G loss: 1.108208]\n",
      "epoch:10 step:9437 [D loss: 0.654409, acc.: 64.06%] [G loss: 1.211651]\n",
      "epoch:10 step:9438 [D loss: 0.606705, acc.: 67.97%] [G loss: 1.321388]\n",
      "epoch:10 step:9439 [D loss: 0.656409, acc.: 64.06%] [G loss: 1.215549]\n",
      "epoch:10 step:9440 [D loss: 0.629003, acc.: 64.84%] [G loss: 1.062025]\n",
      "epoch:10 step:9441 [D loss: 0.834941, acc.: 41.41%] [G loss: 0.941805]\n",
      "epoch:10 step:9442 [D loss: 0.647797, acc.: 60.16%] [G loss: 1.273435]\n",
      "epoch:10 step:9443 [D loss: 0.572519, acc.: 69.53%] [G loss: 0.986632]\n",
      "epoch:10 step:9444 [D loss: 0.542579, acc.: 70.31%] [G loss: 1.375400]\n",
      "epoch:10 step:9445 [D loss: 0.638172, acc.: 64.84%] [G loss: 1.249536]\n",
      "epoch:10 step:9446 [D loss: 0.656751, acc.: 63.28%] [G loss: 1.215484]\n",
      "epoch:10 step:9447 [D loss: 0.632297, acc.: 64.84%] [G loss: 1.267997]\n",
      "epoch:10 step:9448 [D loss: 0.639428, acc.: 67.19%] [G loss: 1.139490]\n",
      "epoch:10 step:9449 [D loss: 0.531067, acc.: 77.34%] [G loss: 1.230452]\n",
      "epoch:10 step:9450 [D loss: 0.604350, acc.: 67.19%] [G loss: 1.187921]\n",
      "epoch:10 step:9451 [D loss: 0.702081, acc.: 56.25%] [G loss: 0.932123]\n",
      "epoch:10 step:9452 [D loss: 0.539123, acc.: 74.22%] [G loss: 1.421644]\n",
      "epoch:10 step:9453 [D loss: 0.730879, acc.: 55.47%] [G loss: 1.077859]\n",
      "epoch:10 step:9454 [D loss: 0.613772, acc.: 64.06%] [G loss: 1.025088]\n",
      "epoch:10 step:9455 [D loss: 0.543372, acc.: 75.00%] [G loss: 1.324397]\n",
      "epoch:10 step:9456 [D loss: 0.666168, acc.: 61.72%] [G loss: 1.063969]\n",
      "epoch:10 step:9457 [D loss: 0.654670, acc.: 60.94%] [G loss: 1.097640]\n",
      "epoch:10 step:9458 [D loss: 0.585997, acc.: 65.62%] [G loss: 1.327148]\n",
      "epoch:10 step:9459 [D loss: 0.555238, acc.: 71.09%] [G loss: 1.135608]\n",
      "epoch:10 step:9460 [D loss: 0.603708, acc.: 67.19%] [G loss: 1.087400]\n",
      "epoch:10 step:9461 [D loss: 0.508163, acc.: 74.22%] [G loss: 1.315710]\n",
      "epoch:10 step:9462 [D loss: 0.625997, acc.: 62.50%] [G loss: 1.033509]\n",
      "epoch:10 step:9463 [D loss: 0.609681, acc.: 66.41%] [G loss: 1.005005]\n",
      "epoch:10 step:9464 [D loss: 0.538463, acc.: 76.56%] [G loss: 1.228104]\n",
      "epoch:10 step:9465 [D loss: 0.539243, acc.: 75.78%] [G loss: 1.144347]\n",
      "epoch:10 step:9466 [D loss: 0.565044, acc.: 72.66%] [G loss: 1.241951]\n",
      "epoch:10 step:9467 [D loss: 0.710586, acc.: 56.25%] [G loss: 1.070871]\n",
      "epoch:10 step:9468 [D loss: 0.614683, acc.: 67.97%] [G loss: 1.141547]\n",
      "epoch:10 step:9469 [D loss: 0.707821, acc.: 57.03%] [G loss: 1.002514]\n",
      "epoch:10 step:9470 [D loss: 0.568952, acc.: 68.75%] [G loss: 1.010390]\n",
      "epoch:10 step:9471 [D loss: 0.633282, acc.: 63.28%] [G loss: 0.952210]\n",
      "epoch:10 step:9472 [D loss: 0.587697, acc.: 71.09%] [G loss: 1.200183]\n",
      "epoch:10 step:9473 [D loss: 0.639138, acc.: 64.06%] [G loss: 1.068049]\n",
      "epoch:10 step:9474 [D loss: 0.610596, acc.: 69.53%] [G loss: 1.073310]\n",
      "epoch:10 step:9475 [D loss: 0.591787, acc.: 71.88%] [G loss: 1.261796]\n",
      "epoch:10 step:9476 [D loss: 0.696857, acc.: 57.81%] [G loss: 1.187849]\n",
      "epoch:10 step:9477 [D loss: 0.580952, acc.: 67.19%] [G loss: 1.310120]\n",
      "epoch:10 step:9478 [D loss: 0.633490, acc.: 66.41%] [G loss: 1.261600]\n",
      "epoch:10 step:9479 [D loss: 0.554495, acc.: 73.44%] [G loss: 1.223047]\n",
      "epoch:10 step:9480 [D loss: 0.657290, acc.: 60.94%] [G loss: 1.222532]\n",
      "epoch:10 step:9481 [D loss: 0.815632, acc.: 42.19%] [G loss: 0.897900]\n",
      "epoch:10 step:9482 [D loss: 0.491254, acc.: 78.91%] [G loss: 1.508734]\n",
      "epoch:10 step:9483 [D loss: 0.622976, acc.: 66.41%] [G loss: 1.034523]\n",
      "epoch:10 step:9484 [D loss: 0.518624, acc.: 77.34%] [G loss: 1.127251]\n",
      "epoch:10 step:9485 [D loss: 0.594017, acc.: 67.97%] [G loss: 1.055925]\n",
      "epoch:10 step:9486 [D loss: 0.621656, acc.: 64.84%] [G loss: 1.182258]\n",
      "epoch:10 step:9487 [D loss: 0.630158, acc.: 65.62%] [G loss: 1.136598]\n",
      "epoch:10 step:9488 [D loss: 0.601038, acc.: 70.31%] [G loss: 1.176572]\n",
      "epoch:10 step:9489 [D loss: 0.590528, acc.: 69.53%] [G loss: 1.065742]\n",
      "epoch:10 step:9490 [D loss: 0.765668, acc.: 46.09%] [G loss: 0.969479]\n",
      "epoch:10 step:9491 [D loss: 0.627712, acc.: 64.06%] [G loss: 1.112553]\n",
      "epoch:10 step:9492 [D loss: 0.609784, acc.: 63.28%] [G loss: 0.983072]\n",
      "epoch:10 step:9493 [D loss: 0.612394, acc.: 67.19%] [G loss: 1.084622]\n",
      "epoch:10 step:9494 [D loss: 0.552961, acc.: 71.09%] [G loss: 1.198452]\n",
      "epoch:10 step:9495 [D loss: 0.586757, acc.: 69.53%] [G loss: 1.052747]\n",
      "epoch:10 step:9496 [D loss: 0.572670, acc.: 74.22%] [G loss: 1.257960]\n",
      "epoch:10 step:9497 [D loss: 0.660502, acc.: 62.50%] [G loss: 1.190950]\n",
      "epoch:10 step:9498 [D loss: 0.653533, acc.: 61.72%] [G loss: 1.115890]\n",
      "epoch:10 step:9499 [D loss: 0.486526, acc.: 77.34%] [G loss: 1.286676]\n",
      "epoch:10 step:9500 [D loss: 0.594009, acc.: 66.41%] [G loss: 1.170426]\n",
      "epoch:10 step:9501 [D loss: 0.613685, acc.: 67.19%] [G loss: 1.168685]\n",
      "epoch:10 step:9502 [D loss: 0.720722, acc.: 62.50%] [G loss: 1.228034]\n",
      "epoch:10 step:9503 [D loss: 0.659735, acc.: 59.38%] [G loss: 1.045224]\n",
      "epoch:10 step:9504 [D loss: 0.612845, acc.: 65.62%] [G loss: 1.276897]\n",
      "epoch:10 step:9505 [D loss: 0.631120, acc.: 64.84%] [G loss: 1.049011]\n",
      "epoch:10 step:9506 [D loss: 0.707544, acc.: 57.81%] [G loss: 1.176095]\n",
      "epoch:10 step:9507 [D loss: 0.644309, acc.: 65.62%] [G loss: 1.021110]\n",
      "epoch:10 step:9508 [D loss: 0.620445, acc.: 64.06%] [G loss: 1.143436]\n",
      "epoch:10 step:9509 [D loss: 0.613596, acc.: 68.75%] [G loss: 1.071432]\n",
      "epoch:10 step:9510 [D loss: 0.587514, acc.: 71.09%] [G loss: 1.169676]\n",
      "epoch:10 step:9511 [D loss: 0.625975, acc.: 64.06%] [G loss: 1.134212]\n",
      "epoch:10 step:9512 [D loss: 0.616165, acc.: 73.44%] [G loss: 1.178488]\n",
      "epoch:10 step:9513 [D loss: 0.615757, acc.: 64.06%] [G loss: 1.118807]\n",
      "epoch:10 step:9514 [D loss: 0.708829, acc.: 59.38%] [G loss: 1.372511]\n",
      "epoch:10 step:9515 [D loss: 0.626179, acc.: 64.06%] [G loss: 1.237518]\n",
      "epoch:10 step:9516 [D loss: 0.485452, acc.: 80.47%] [G loss: 1.411423]\n",
      "epoch:10 step:9517 [D loss: 0.671131, acc.: 60.16%] [G loss: 1.100373]\n",
      "epoch:10 step:9518 [D loss: 0.515769, acc.: 78.12%] [G loss: 1.207850]\n",
      "epoch:10 step:9519 [D loss: 0.700071, acc.: 59.38%] [G loss: 1.027881]\n",
      "epoch:10 step:9520 [D loss: 0.648524, acc.: 61.72%] [G loss: 1.369306]\n",
      "epoch:10 step:9521 [D loss: 0.589901, acc.: 67.19%] [G loss: 1.183179]\n",
      "epoch:10 step:9522 [D loss: 0.540417, acc.: 73.44%] [G loss: 1.457970]\n",
      "epoch:10 step:9523 [D loss: 0.528352, acc.: 78.12%] [G loss: 1.312244]\n",
      "epoch:10 step:9524 [D loss: 0.624334, acc.: 63.28%] [G loss: 1.181266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9525 [D loss: 0.546710, acc.: 75.00%] [G loss: 1.022141]\n",
      "epoch:10 step:9526 [D loss: 0.654541, acc.: 63.28%] [G loss: 0.908004]\n",
      "epoch:10 step:9527 [D loss: 0.650694, acc.: 61.72%] [G loss: 1.207076]\n",
      "epoch:10 step:9528 [D loss: 0.613338, acc.: 67.19%] [G loss: 1.198134]\n",
      "epoch:10 step:9529 [D loss: 0.520876, acc.: 78.91%] [G loss: 1.318200]\n",
      "epoch:10 step:9530 [D loss: 0.563464, acc.: 69.53%] [G loss: 1.120468]\n",
      "epoch:10 step:9531 [D loss: 0.583190, acc.: 64.06%] [G loss: 1.124015]\n",
      "epoch:10 step:9532 [D loss: 0.580494, acc.: 71.88%] [G loss: 1.208903]\n",
      "epoch:10 step:9533 [D loss: 0.702416, acc.: 56.25%] [G loss: 0.964230]\n",
      "epoch:10 step:9534 [D loss: 0.628872, acc.: 67.19%] [G loss: 1.319599]\n",
      "epoch:10 step:9535 [D loss: 0.607846, acc.: 67.19%] [G loss: 1.252902]\n",
      "epoch:10 step:9536 [D loss: 0.653640, acc.: 61.72%] [G loss: 1.064122]\n",
      "epoch:10 step:9537 [D loss: 0.708241, acc.: 59.38%] [G loss: 1.006030]\n",
      "epoch:10 step:9538 [D loss: 0.531878, acc.: 72.66%] [G loss: 1.426275]\n",
      "epoch:10 step:9539 [D loss: 0.626316, acc.: 66.41%] [G loss: 1.190265]\n",
      "epoch:10 step:9540 [D loss: 0.654282, acc.: 57.81%] [G loss: 1.129114]\n",
      "epoch:10 step:9541 [D loss: 0.654112, acc.: 62.50%] [G loss: 1.113523]\n",
      "epoch:10 step:9542 [D loss: 0.555380, acc.: 67.19%] [G loss: 1.501156]\n",
      "epoch:10 step:9543 [D loss: 0.637443, acc.: 60.16%] [G loss: 1.470639]\n",
      "epoch:10 step:9544 [D loss: 0.571425, acc.: 70.31%] [G loss: 1.112044]\n",
      "epoch:10 step:9545 [D loss: 0.543787, acc.: 77.34%] [G loss: 1.358850]\n",
      "epoch:10 step:9546 [D loss: 0.675774, acc.: 62.50%] [G loss: 1.045062]\n",
      "epoch:10 step:9547 [D loss: 0.630809, acc.: 63.28%] [G loss: 0.903579]\n",
      "epoch:10 step:9548 [D loss: 0.615892, acc.: 66.41%] [G loss: 1.243674]\n",
      "epoch:10 step:9549 [D loss: 0.658575, acc.: 60.94%] [G loss: 1.010774]\n",
      "epoch:10 step:9550 [D loss: 0.550235, acc.: 72.66%] [G loss: 1.285249]\n",
      "epoch:10 step:9551 [D loss: 0.577839, acc.: 68.75%] [G loss: 1.205218]\n",
      "epoch:10 step:9552 [D loss: 0.539879, acc.: 74.22%] [G loss: 1.384368]\n",
      "epoch:10 step:9553 [D loss: 0.529298, acc.: 72.66%] [G loss: 1.247027]\n",
      "epoch:10 step:9554 [D loss: 0.654594, acc.: 60.94%] [G loss: 1.128776]\n",
      "epoch:10 step:9555 [D loss: 0.611875, acc.: 61.72%] [G loss: 1.247673]\n",
      "epoch:10 step:9556 [D loss: 0.612066, acc.: 61.72%] [G loss: 1.286196]\n",
      "epoch:10 step:9557 [D loss: 0.405707, acc.: 85.94%] [G loss: 1.463688]\n",
      "epoch:10 step:9558 [D loss: 0.510666, acc.: 79.69%] [G loss: 1.185684]\n",
      "epoch:10 step:9559 [D loss: 0.490427, acc.: 83.59%] [G loss: 1.234915]\n",
      "epoch:10 step:9560 [D loss: 0.604122, acc.: 64.84%] [G loss: 1.235300]\n",
      "epoch:10 step:9561 [D loss: 0.484899, acc.: 81.25%] [G loss: 1.147466]\n",
      "epoch:10 step:9562 [D loss: 0.571296, acc.: 73.44%] [G loss: 1.180447]\n",
      "epoch:10 step:9563 [D loss: 0.678218, acc.: 64.84%] [G loss: 1.094257]\n",
      "epoch:10 step:9564 [D loss: 0.666672, acc.: 61.72%] [G loss: 1.160380]\n",
      "epoch:10 step:9565 [D loss: 0.646213, acc.: 64.06%] [G loss: 1.288699]\n",
      "epoch:10 step:9566 [D loss: 0.555167, acc.: 72.66%] [G loss: 1.131227]\n",
      "epoch:10 step:9567 [D loss: 0.504914, acc.: 75.00%] [G loss: 1.217535]\n",
      "epoch:10 step:9568 [D loss: 0.536940, acc.: 79.69%] [G loss: 1.040284]\n",
      "epoch:10 step:9569 [D loss: 0.607220, acc.: 68.75%] [G loss: 1.143286]\n",
      "epoch:10 step:9570 [D loss: 0.557790, acc.: 70.31%] [G loss: 1.157911]\n",
      "epoch:10 step:9571 [D loss: 0.585955, acc.: 67.97%] [G loss: 1.119378]\n",
      "epoch:10 step:9572 [D loss: 0.428081, acc.: 86.72%] [G loss: 1.295737]\n",
      "epoch:10 step:9573 [D loss: 0.525162, acc.: 77.34%] [G loss: 1.219649]\n",
      "epoch:10 step:9574 [D loss: 0.584866, acc.: 67.19%] [G loss: 1.171915]\n",
      "epoch:10 step:9575 [D loss: 0.573064, acc.: 70.31%] [G loss: 1.146911]\n",
      "epoch:10 step:9576 [D loss: 0.601671, acc.: 69.53%] [G loss: 1.120165]\n",
      "epoch:10 step:9577 [D loss: 0.630064, acc.: 66.41%] [G loss: 1.188260]\n",
      "epoch:10 step:9578 [D loss: 0.578785, acc.: 71.09%] [G loss: 1.189542]\n",
      "epoch:10 step:9579 [D loss: 0.549978, acc.: 72.66%] [G loss: 1.367637]\n",
      "epoch:10 step:9580 [D loss: 0.645181, acc.: 64.06%] [G loss: 1.191567]\n",
      "epoch:10 step:9581 [D loss: 0.442378, acc.: 84.38%] [G loss: 1.405382]\n",
      "epoch:10 step:9582 [D loss: 0.637729, acc.: 64.06%] [G loss: 1.434662]\n",
      "epoch:10 step:9583 [D loss: 0.600193, acc.: 63.28%] [G loss: 1.126501]\n",
      "epoch:10 step:9584 [D loss: 0.648745, acc.: 61.72%] [G loss: 1.084630]\n",
      "epoch:10 step:9585 [D loss: 0.717211, acc.: 55.47%] [G loss: 1.116291]\n",
      "epoch:10 step:9586 [D loss: 0.650652, acc.: 62.50%] [G loss: 1.094038]\n",
      "epoch:10 step:9587 [D loss: 0.611564, acc.: 67.97%] [G loss: 0.907179]\n",
      "epoch:10 step:9588 [D loss: 0.639717, acc.: 64.84%] [G loss: 1.010400]\n",
      "epoch:10 step:9589 [D loss: 0.660390, acc.: 67.97%] [G loss: 1.263668]\n",
      "epoch:10 step:9590 [D loss: 0.597107, acc.: 66.41%] [G loss: 1.076770]\n",
      "epoch:10 step:9591 [D loss: 0.706696, acc.: 58.59%] [G loss: 0.993160]\n",
      "epoch:10 step:9592 [D loss: 0.585159, acc.: 67.19%] [G loss: 1.349469]\n",
      "epoch:10 step:9593 [D loss: 0.525958, acc.: 76.56%] [G loss: 1.063415]\n",
      "epoch:10 step:9594 [D loss: 0.657767, acc.: 66.41%] [G loss: 0.997547]\n",
      "epoch:10 step:9595 [D loss: 0.522120, acc.: 78.12%] [G loss: 1.280774]\n",
      "epoch:10 step:9596 [D loss: 0.621200, acc.: 66.41%] [G loss: 1.308584]\n",
      "epoch:10 step:9597 [D loss: 0.632138, acc.: 62.50%] [G loss: 1.043132]\n",
      "epoch:10 step:9598 [D loss: 0.639899, acc.: 62.50%] [G loss: 1.071175]\n",
      "epoch:10 step:9599 [D loss: 0.599994, acc.: 68.75%] [G loss: 1.200798]\n",
      "epoch:10 step:9600 [D loss: 0.561157, acc.: 71.88%] [G loss: 1.285063]\n",
      "##############\n",
      "[2.79568623 2.38374737 2.00822848 2.91530834 1.13815313 6.16086719\n",
      " 2.11810418 2.78753197 3.85749283 5.23192193]\n",
      "##########\n",
      "epoch:10 step:9601 [D loss: 0.643171, acc.: 57.03%] [G loss: 1.214583]\n",
      "epoch:10 step:9602 [D loss: 0.613726, acc.: 72.66%] [G loss: 1.180447]\n",
      "epoch:10 step:9603 [D loss: 0.516074, acc.: 74.22%] [G loss: 1.277996]\n",
      "epoch:10 step:9604 [D loss: 0.636798, acc.: 64.06%] [G loss: 1.203553]\n",
      "epoch:10 step:9605 [D loss: 0.618804, acc.: 63.28%] [G loss: 1.112505]\n",
      "epoch:10 step:9606 [D loss: 0.568810, acc.: 72.66%] [G loss: 1.303553]\n",
      "epoch:10 step:9607 [D loss: 0.647254, acc.: 57.81%] [G loss: 1.280896]\n",
      "epoch:10 step:9608 [D loss: 0.557853, acc.: 71.88%] [G loss: 1.186303]\n",
      "epoch:10 step:9609 [D loss: 0.559403, acc.: 73.44%] [G loss: 1.318984]\n",
      "epoch:10 step:9610 [D loss: 0.610278, acc.: 65.62%] [G loss: 1.182607]\n",
      "epoch:10 step:9611 [D loss: 0.555688, acc.: 67.97%] [G loss: 1.298492]\n",
      "epoch:10 step:9612 [D loss: 0.696023, acc.: 54.69%] [G loss: 1.184664]\n",
      "epoch:10 step:9613 [D loss: 0.734375, acc.: 53.12%] [G loss: 0.924978]\n",
      "epoch:10 step:9614 [D loss: 0.466328, acc.: 80.47%] [G loss: 1.183896]\n",
      "epoch:10 step:9615 [D loss: 0.668964, acc.: 61.72%] [G loss: 1.048227]\n",
      "epoch:10 step:9616 [D loss: 0.680178, acc.: 57.81%] [G loss: 1.102523]\n",
      "epoch:10 step:9617 [D loss: 0.586976, acc.: 67.19%] [G loss: 1.368750]\n",
      "epoch:10 step:9618 [D loss: 0.644524, acc.: 65.62%] [G loss: 1.186007]\n",
      "epoch:10 step:9619 [D loss: 0.531560, acc.: 80.47%] [G loss: 0.976927]\n",
      "epoch:10 step:9620 [D loss: 0.586165, acc.: 70.31%] [G loss: 1.348208]\n",
      "epoch:10 step:9621 [D loss: 0.560272, acc.: 74.22%] [G loss: 1.013371]\n",
      "epoch:10 step:9622 [D loss: 0.508496, acc.: 77.34%] [G loss: 1.250499]\n",
      "epoch:10 step:9623 [D loss: 0.533834, acc.: 70.31%] [G loss: 1.059005]\n",
      "epoch:10 step:9624 [D loss: 0.594266, acc.: 68.75%] [G loss: 1.093708]\n",
      "epoch:10 step:9625 [D loss: 0.572693, acc.: 71.88%] [G loss: 1.267673]\n",
      "epoch:10 step:9626 [D loss: 0.661898, acc.: 64.84%] [G loss: 1.051883]\n",
      "epoch:10 step:9627 [D loss: 0.521739, acc.: 75.78%] [G loss: 1.274558]\n",
      "epoch:10 step:9628 [D loss: 0.540331, acc.: 72.66%] [G loss: 1.363443]\n",
      "epoch:10 step:9629 [D loss: 0.646992, acc.: 63.28%] [G loss: 1.154794]\n",
      "epoch:10 step:9630 [D loss: 0.538293, acc.: 71.88%] [G loss: 1.284551]\n",
      "epoch:10 step:9631 [D loss: 0.511856, acc.: 75.78%] [G loss: 1.075291]\n",
      "epoch:10 step:9632 [D loss: 0.736743, acc.: 53.91%] [G loss: 1.089017]\n",
      "epoch:10 step:9633 [D loss: 0.729461, acc.: 52.34%] [G loss: 1.189049]\n",
      "epoch:10 step:9634 [D loss: 0.543680, acc.: 71.88%] [G loss: 1.194718]\n",
      "epoch:10 step:9635 [D loss: 0.481329, acc.: 78.12%] [G loss: 1.407156]\n",
      "epoch:10 step:9636 [D loss: 0.628114, acc.: 62.50%] [G loss: 1.075222]\n",
      "epoch:10 step:9637 [D loss: 0.460399, acc.: 80.47%] [G loss: 1.240539]\n",
      "epoch:10 step:9638 [D loss: 0.486473, acc.: 78.12%] [G loss: 1.297691]\n",
      "epoch:10 step:9639 [D loss: 0.599645, acc.: 65.62%] [G loss: 1.116801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9640 [D loss: 0.602059, acc.: 67.97%] [G loss: 1.257565]\n",
      "epoch:10 step:9641 [D loss: 0.596704, acc.: 63.28%] [G loss: 1.212789]\n",
      "epoch:10 step:9642 [D loss: 0.706357, acc.: 60.94%] [G loss: 0.907858]\n",
      "epoch:10 step:9643 [D loss: 0.544068, acc.: 75.00%] [G loss: 1.240049]\n",
      "epoch:10 step:9644 [D loss: 0.586703, acc.: 74.22%] [G loss: 1.140490]\n",
      "epoch:10 step:9645 [D loss: 0.618886, acc.: 70.31%] [G loss: 1.219922]\n",
      "epoch:10 step:9646 [D loss: 0.624944, acc.: 59.38%] [G loss: 1.059605]\n",
      "epoch:10 step:9647 [D loss: 0.653671, acc.: 63.28%] [G loss: 1.083217]\n",
      "epoch:10 step:9648 [D loss: 0.588235, acc.: 63.28%] [G loss: 1.185963]\n",
      "epoch:10 step:9649 [D loss: 0.595967, acc.: 68.75%] [G loss: 1.365873]\n",
      "epoch:10 step:9650 [D loss: 0.594542, acc.: 71.09%] [G loss: 1.053671]\n",
      "epoch:10 step:9651 [D loss: 0.586879, acc.: 67.19%] [G loss: 1.046134]\n",
      "epoch:10 step:9652 [D loss: 0.548100, acc.: 76.56%] [G loss: 1.191675]\n",
      "epoch:10 step:9653 [D loss: 0.641677, acc.: 64.06%] [G loss: 1.288619]\n",
      "epoch:10 step:9654 [D loss: 0.599164, acc.: 69.53%] [G loss: 1.053280]\n",
      "epoch:10 step:9655 [D loss: 0.534374, acc.: 72.66%] [G loss: 1.035935]\n",
      "epoch:10 step:9656 [D loss: 0.473971, acc.: 82.03%] [G loss: 1.291524]\n",
      "epoch:10 step:9657 [D loss: 0.562988, acc.: 71.88%] [G loss: 1.229079]\n",
      "epoch:10 step:9658 [D loss: 0.591913, acc.: 64.84%] [G loss: 1.163796]\n",
      "epoch:10 step:9659 [D loss: 0.666993, acc.: 57.81%] [G loss: 1.122561]\n",
      "epoch:10 step:9660 [D loss: 0.533415, acc.: 69.53%] [G loss: 1.338156]\n",
      "epoch:10 step:9661 [D loss: 0.539926, acc.: 75.78%] [G loss: 1.176209]\n",
      "epoch:10 step:9662 [D loss: 0.644400, acc.: 61.72%] [G loss: 1.213437]\n",
      "epoch:10 step:9663 [D loss: 0.596863, acc.: 67.19%] [G loss: 1.365317]\n",
      "epoch:10 step:9664 [D loss: 0.648900, acc.: 58.59%] [G loss: 1.133231]\n",
      "epoch:10 step:9665 [D loss: 0.723465, acc.: 52.34%] [G loss: 1.071579]\n",
      "epoch:10 step:9666 [D loss: 0.540172, acc.: 74.22%] [G loss: 1.172876]\n",
      "epoch:10 step:9667 [D loss: 0.739700, acc.: 57.81%] [G loss: 1.037052]\n",
      "epoch:10 step:9668 [D loss: 0.711915, acc.: 51.56%] [G loss: 1.100149]\n",
      "epoch:10 step:9669 [D loss: 0.716599, acc.: 52.34%] [G loss: 1.215078]\n",
      "epoch:10 step:9670 [D loss: 0.612367, acc.: 62.50%] [G loss: 1.016735]\n",
      "epoch:10 step:9671 [D loss: 0.591782, acc.: 69.53%] [G loss: 0.848591]\n",
      "epoch:10 step:9672 [D loss: 0.632026, acc.: 57.81%] [G loss: 1.104868]\n",
      "epoch:10 step:9673 [D loss: 0.642333, acc.: 61.72%] [G loss: 1.275775]\n",
      "epoch:10 step:9674 [D loss: 0.603909, acc.: 74.22%] [G loss: 1.147321]\n",
      "epoch:10 step:9675 [D loss: 0.573795, acc.: 70.31%] [G loss: 0.978955]\n",
      "epoch:10 step:9676 [D loss: 0.548403, acc.: 71.09%] [G loss: 1.045125]\n",
      "epoch:10 step:9677 [D loss: 0.695960, acc.: 54.69%] [G loss: 1.099808]\n",
      "epoch:10 step:9678 [D loss: 0.674873, acc.: 64.84%] [G loss: 1.130985]\n",
      "epoch:10 step:9679 [D loss: 0.477089, acc.: 78.12%] [G loss: 1.345753]\n",
      "epoch:10 step:9680 [D loss: 0.627260, acc.: 67.19%] [G loss: 1.219527]\n",
      "epoch:10 step:9681 [D loss: 0.575033, acc.: 76.56%] [G loss: 1.291759]\n",
      "epoch:10 step:9682 [D loss: 0.780806, acc.: 54.69%] [G loss: 1.027713]\n",
      "epoch:10 step:9683 [D loss: 0.538038, acc.: 71.09%] [G loss: 1.429058]\n",
      "epoch:10 step:9684 [D loss: 0.479016, acc.: 78.91%] [G loss: 1.293055]\n",
      "epoch:10 step:9685 [D loss: 0.598579, acc.: 68.75%] [G loss: 1.087198]\n",
      "epoch:10 step:9686 [D loss: 0.703002, acc.: 57.81%] [G loss: 1.246679]\n",
      "epoch:10 step:9687 [D loss: 0.489869, acc.: 81.25%] [G loss: 1.020269]\n",
      "epoch:10 step:9688 [D loss: 0.663670, acc.: 62.50%] [G loss: 0.984150]\n",
      "epoch:10 step:9689 [D loss: 0.551166, acc.: 73.44%] [G loss: 1.150988]\n",
      "epoch:10 step:9690 [D loss: 0.637770, acc.: 61.72%] [G loss: 0.946603]\n",
      "epoch:10 step:9691 [D loss: 0.631786, acc.: 62.50%] [G loss: 1.403962]\n",
      "epoch:10 step:9692 [D loss: 0.611352, acc.: 64.84%] [G loss: 1.127649]\n",
      "epoch:10 step:9693 [D loss: 0.625251, acc.: 66.41%] [G loss: 1.115565]\n",
      "epoch:10 step:9694 [D loss: 0.613479, acc.: 67.19%] [G loss: 1.118750]\n",
      "epoch:10 step:9695 [D loss: 0.547195, acc.: 71.09%] [G loss: 1.085405]\n",
      "epoch:10 step:9696 [D loss: 0.600186, acc.: 67.19%] [G loss: 1.208338]\n",
      "epoch:10 step:9697 [D loss: 0.542939, acc.: 75.00%] [G loss: 1.444200]\n",
      "epoch:10 step:9698 [D loss: 0.465081, acc.: 80.47%] [G loss: 1.354528]\n",
      "epoch:10 step:9699 [D loss: 0.592923, acc.: 67.19%] [G loss: 1.013386]\n",
      "epoch:10 step:9700 [D loss: 0.516308, acc.: 75.78%] [G loss: 1.209434]\n",
      "epoch:10 step:9701 [D loss: 0.714076, acc.: 57.81%] [G loss: 1.180114]\n",
      "epoch:10 step:9702 [D loss: 0.565929, acc.: 67.19%] [G loss: 1.300483]\n",
      "epoch:10 step:9703 [D loss: 0.650235, acc.: 61.72%] [G loss: 1.061133]\n",
      "epoch:10 step:9704 [D loss: 0.498406, acc.: 78.91%] [G loss: 1.301740]\n",
      "epoch:10 step:9705 [D loss: 0.554167, acc.: 71.09%] [G loss: 1.143073]\n",
      "epoch:10 step:9706 [D loss: 0.639158, acc.: 63.28%] [G loss: 0.973881]\n",
      "epoch:10 step:9707 [D loss: 0.647704, acc.: 60.94%] [G loss: 1.058418]\n",
      "epoch:10 step:9708 [D loss: 0.554083, acc.: 75.00%] [G loss: 1.338738]\n",
      "epoch:10 step:9709 [D loss: 0.490418, acc.: 77.34%] [G loss: 1.204221]\n",
      "epoch:10 step:9710 [D loss: 0.571219, acc.: 71.88%] [G loss: 0.978992]\n",
      "epoch:10 step:9711 [D loss: 0.671719, acc.: 60.94%] [G loss: 1.307725]\n",
      "epoch:10 step:9712 [D loss: 0.670006, acc.: 66.41%] [G loss: 1.306896]\n",
      "epoch:10 step:9713 [D loss: 0.590615, acc.: 70.31%] [G loss: 1.009820]\n",
      "epoch:10 step:9714 [D loss: 0.586552, acc.: 69.53%] [G loss: 1.230848]\n",
      "epoch:10 step:9715 [D loss: 0.567578, acc.: 67.97%] [G loss: 1.062280]\n",
      "epoch:10 step:9716 [D loss: 0.670454, acc.: 63.28%] [G loss: 1.010782]\n",
      "epoch:10 step:9717 [D loss: 0.633461, acc.: 67.97%] [G loss: 1.156124]\n",
      "epoch:10 step:9718 [D loss: 0.640601, acc.: 64.06%] [G loss: 0.960764]\n",
      "epoch:10 step:9719 [D loss: 0.625649, acc.: 67.19%] [G loss: 0.971187]\n",
      "epoch:10 step:9720 [D loss: 0.551321, acc.: 72.66%] [G loss: 1.242495]\n",
      "epoch:10 step:9721 [D loss: 0.646910, acc.: 66.41%] [G loss: 1.226780]\n",
      "epoch:10 step:9722 [D loss: 0.690380, acc.: 57.81%] [G loss: 1.012228]\n",
      "epoch:10 step:9723 [D loss: 0.704871, acc.: 61.72%] [G loss: 0.876722]\n",
      "epoch:10 step:9724 [D loss: 0.670562, acc.: 58.59%] [G loss: 1.175081]\n",
      "epoch:10 step:9725 [D loss: 0.616467, acc.: 64.84%] [G loss: 1.253226]\n",
      "epoch:10 step:9726 [D loss: 0.568972, acc.: 73.44%] [G loss: 1.161829]\n",
      "epoch:10 step:9727 [D loss: 0.636878, acc.: 67.19%] [G loss: 1.111740]\n",
      "epoch:10 step:9728 [D loss: 0.554244, acc.: 75.00%] [G loss: 1.134893]\n",
      "epoch:10 step:9729 [D loss: 0.611392, acc.: 63.28%] [G loss: 1.112595]\n",
      "epoch:10 step:9730 [D loss: 0.503599, acc.: 75.78%] [G loss: 1.139966]\n",
      "epoch:10 step:9731 [D loss: 0.656532, acc.: 55.47%] [G loss: 0.952152]\n",
      "epoch:10 step:9732 [D loss: 0.538435, acc.: 75.78%] [G loss: 1.105064]\n",
      "epoch:10 step:9733 [D loss: 0.577631, acc.: 71.09%] [G loss: 1.072719]\n",
      "epoch:10 step:9734 [D loss: 0.575887, acc.: 67.19%] [G loss: 1.234719]\n",
      "epoch:10 step:9735 [D loss: 0.506777, acc.: 76.56%] [G loss: 1.192677]\n",
      "epoch:10 step:9736 [D loss: 0.543983, acc.: 71.88%] [G loss: 1.202882]\n",
      "epoch:10 step:9737 [D loss: 0.606810, acc.: 65.62%] [G loss: 1.142670]\n",
      "epoch:10 step:9738 [D loss: 0.666499, acc.: 55.47%] [G loss: 1.045476]\n",
      "epoch:10 step:9739 [D loss: 0.688441, acc.: 54.69%] [G loss: 1.058647]\n",
      "epoch:10 step:9740 [D loss: 0.604558, acc.: 69.53%] [G loss: 1.115818]\n",
      "epoch:10 step:9741 [D loss: 0.580038, acc.: 70.31%] [G loss: 1.036428]\n",
      "epoch:10 step:9742 [D loss: 0.608402, acc.: 71.09%] [G loss: 1.279152]\n",
      "epoch:10 step:9743 [D loss: 0.664975, acc.: 60.16%] [G loss: 1.128587]\n",
      "epoch:10 step:9744 [D loss: 0.722138, acc.: 54.69%] [G loss: 1.144106]\n",
      "epoch:10 step:9745 [D loss: 0.627996, acc.: 67.97%] [G loss: 1.074964]\n",
      "epoch:10 step:9746 [D loss: 0.639279, acc.: 61.72%] [G loss: 1.239863]\n",
      "epoch:10 step:9747 [D loss: 0.621741, acc.: 62.50%] [G loss: 1.159864]\n",
      "epoch:10 step:9748 [D loss: 0.584226, acc.: 69.53%] [G loss: 1.086730]\n",
      "epoch:10 step:9749 [D loss: 0.537049, acc.: 74.22%] [G loss: 1.261988]\n",
      "epoch:10 step:9750 [D loss: 0.621544, acc.: 67.97%] [G loss: 1.383107]\n",
      "epoch:10 step:9751 [D loss: 0.572181, acc.: 67.97%] [G loss: 1.182480]\n",
      "epoch:10 step:9752 [D loss: 0.566454, acc.: 71.09%] [G loss: 1.140307]\n",
      "epoch:10 step:9753 [D loss: 0.589261, acc.: 67.19%] [G loss: 1.272335]\n",
      "epoch:10 step:9754 [D loss: 0.678215, acc.: 60.94%] [G loss: 1.173219]\n",
      "epoch:10 step:9755 [D loss: 0.598789, acc.: 65.62%] [G loss: 0.984041]\n",
      "epoch:10 step:9756 [D loss: 0.690163, acc.: 59.38%] [G loss: 1.207550]\n",
      "epoch:10 step:9757 [D loss: 0.585140, acc.: 70.31%] [G loss: 1.402372]\n",
      "epoch:10 step:9758 [D loss: 0.629833, acc.: 61.72%] [G loss: 1.280486]\n",
      "epoch:10 step:9759 [D loss: 0.596871, acc.: 65.62%] [G loss: 1.393857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9760 [D loss: 0.561587, acc.: 70.31%] [G loss: 1.337547]\n",
      "epoch:10 step:9761 [D loss: 0.711562, acc.: 51.56%] [G loss: 1.026001]\n",
      "epoch:10 step:9762 [D loss: 0.636113, acc.: 66.41%] [G loss: 1.248384]\n",
      "epoch:10 step:9763 [D loss: 0.666053, acc.: 60.94%] [G loss: 0.960469]\n",
      "epoch:10 step:9764 [D loss: 0.655929, acc.: 62.50%] [G loss: 1.088046]\n",
      "epoch:10 step:9765 [D loss: 0.625334, acc.: 67.19%] [G loss: 1.247421]\n",
      "epoch:10 step:9766 [D loss: 0.573800, acc.: 71.09%] [G loss: 1.208244]\n",
      "epoch:10 step:9767 [D loss: 0.608789, acc.: 64.84%] [G loss: 1.242174]\n",
      "epoch:10 step:9768 [D loss: 0.589985, acc.: 68.75%] [G loss: 1.096558]\n",
      "epoch:10 step:9769 [D loss: 0.552053, acc.: 75.78%] [G loss: 1.239114]\n",
      "epoch:10 step:9770 [D loss: 0.595471, acc.: 67.19%] [G loss: 1.128187]\n",
      "epoch:10 step:9771 [D loss: 0.561071, acc.: 73.44%] [G loss: 1.171105]\n",
      "epoch:10 step:9772 [D loss: 0.503819, acc.: 75.00%] [G loss: 1.115752]\n",
      "epoch:10 step:9773 [D loss: 0.523616, acc.: 78.12%] [G loss: 1.468161]\n",
      "epoch:10 step:9774 [D loss: 0.650440, acc.: 63.28%] [G loss: 0.996777]\n",
      "epoch:10 step:9775 [D loss: 0.544518, acc.: 73.44%] [G loss: 1.130168]\n",
      "epoch:10 step:9776 [D loss: 0.483379, acc.: 79.69%] [G loss: 1.142611]\n",
      "epoch:10 step:9777 [D loss: 0.634877, acc.: 65.62%] [G loss: 1.270423]\n",
      "epoch:10 step:9778 [D loss: 0.600901, acc.: 67.19%] [G loss: 1.202550]\n",
      "epoch:10 step:9779 [D loss: 0.578749, acc.: 71.88%] [G loss: 1.304536]\n",
      "epoch:10 step:9780 [D loss: 0.523055, acc.: 71.09%] [G loss: 1.167498]\n",
      "epoch:10 step:9781 [D loss: 0.670627, acc.: 63.28%] [G loss: 1.298160]\n",
      "epoch:10 step:9782 [D loss: 0.629002, acc.: 65.62%] [G loss: 1.093554]\n",
      "epoch:10 step:9783 [D loss: 0.649898, acc.: 58.59%] [G loss: 1.140868]\n",
      "epoch:10 step:9784 [D loss: 0.621276, acc.: 64.84%] [G loss: 1.238099]\n",
      "epoch:10 step:9785 [D loss: 0.669380, acc.: 61.72%] [G loss: 1.117961]\n",
      "epoch:10 step:9786 [D loss: 0.583009, acc.: 66.41%] [G loss: 1.022618]\n",
      "epoch:10 step:9787 [D loss: 0.594784, acc.: 65.62%] [G loss: 1.166015]\n",
      "epoch:10 step:9788 [D loss: 0.678083, acc.: 58.59%] [G loss: 1.081052]\n",
      "epoch:10 step:9789 [D loss: 0.511875, acc.: 75.00%] [G loss: 1.237601]\n",
      "epoch:10 step:9790 [D loss: 0.497770, acc.: 81.25%] [G loss: 1.405920]\n",
      "epoch:10 step:9791 [D loss: 0.736890, acc.: 56.25%] [G loss: 1.050983]\n",
      "epoch:10 step:9792 [D loss: 0.611849, acc.: 66.41%] [G loss: 1.185144]\n",
      "epoch:10 step:9793 [D loss: 0.636174, acc.: 61.72%] [G loss: 1.118559]\n",
      "epoch:10 step:9794 [D loss: 0.563105, acc.: 67.19%] [G loss: 1.338035]\n",
      "epoch:10 step:9795 [D loss: 0.587502, acc.: 67.97%] [G loss: 1.093559]\n",
      "epoch:10 step:9796 [D loss: 0.614663, acc.: 62.50%] [G loss: 1.139450]\n",
      "epoch:10 step:9797 [D loss: 0.718850, acc.: 56.25%] [G loss: 1.109529]\n",
      "epoch:10 step:9798 [D loss: 0.594208, acc.: 64.84%] [G loss: 1.424970]\n",
      "epoch:10 step:9799 [D loss: 0.602110, acc.: 64.06%] [G loss: 1.154513]\n",
      "epoch:10 step:9800 [D loss: 0.598885, acc.: 71.09%] [G loss: 1.240686]\n",
      "##############\n",
      "[2.69706759 2.08865117 1.87230827 2.91472552 1.08023194 5.83330217\n",
      " 2.27038242 2.95535011 4.04641364 8.14868929]\n",
      "##########\n",
      "epoch:10 step:9801 [D loss: 0.471102, acc.: 78.91%] [G loss: 1.230740]\n",
      "epoch:10 step:9802 [D loss: 0.552992, acc.: 71.09%] [G loss: 1.156726]\n",
      "epoch:10 step:9803 [D loss: 0.643835, acc.: 66.41%] [G loss: 1.176197]\n",
      "epoch:10 step:9804 [D loss: 0.694983, acc.: 54.69%] [G loss: 1.046775]\n",
      "epoch:10 step:9805 [D loss: 0.580138, acc.: 70.31%] [G loss: 1.241988]\n",
      "epoch:10 step:9806 [D loss: 0.595903, acc.: 67.19%] [G loss: 0.980893]\n",
      "epoch:10 step:9807 [D loss: 0.777369, acc.: 57.81%] [G loss: 1.194649]\n",
      "epoch:10 step:9808 [D loss: 0.641105, acc.: 60.94%] [G loss: 1.028595]\n",
      "epoch:10 step:9809 [D loss: 0.609861, acc.: 62.50%] [G loss: 1.383906]\n",
      "epoch:10 step:9810 [D loss: 0.588354, acc.: 70.31%] [G loss: 0.977908]\n",
      "epoch:10 step:9811 [D loss: 0.535597, acc.: 75.78%] [G loss: 1.295607]\n",
      "epoch:10 step:9812 [D loss: 0.604647, acc.: 64.06%] [G loss: 1.143947]\n",
      "epoch:10 step:9813 [D loss: 0.575429, acc.: 73.44%] [G loss: 1.362577]\n",
      "epoch:10 step:9814 [D loss: 0.562732, acc.: 73.44%] [G loss: 1.017145]\n",
      "epoch:10 step:9815 [D loss: 0.530431, acc.: 72.66%] [G loss: 1.101726]\n",
      "epoch:10 step:9816 [D loss: 0.704979, acc.: 58.59%] [G loss: 1.088169]\n",
      "epoch:10 step:9817 [D loss: 0.491518, acc.: 80.47%] [G loss: 1.493852]\n",
      "epoch:10 step:9818 [D loss: 0.579094, acc.: 65.62%] [G loss: 1.433547]\n",
      "epoch:10 step:9819 [D loss: 0.558152, acc.: 69.53%] [G loss: 1.330586]\n",
      "epoch:10 step:9820 [D loss: 0.652032, acc.: 62.50%] [G loss: 1.130317]\n",
      "epoch:10 step:9821 [D loss: 0.569086, acc.: 68.75%] [G loss: 1.004830]\n",
      "epoch:10 step:9822 [D loss: 0.584787, acc.: 71.88%] [G loss: 1.085097]\n",
      "epoch:10 step:9823 [D loss: 0.501575, acc.: 75.78%] [G loss: 1.037350]\n",
      "epoch:10 step:9824 [D loss: 0.659365, acc.: 64.06%] [G loss: 1.176789]\n",
      "epoch:10 step:9825 [D loss: 0.730612, acc.: 54.69%] [G loss: 1.184311]\n",
      "epoch:10 step:9826 [D loss: 0.809655, acc.: 48.44%] [G loss: 0.937308]\n",
      "epoch:10 step:9827 [D loss: 0.605141, acc.: 63.28%] [G loss: 1.034483]\n",
      "epoch:10 step:9828 [D loss: 0.506046, acc.: 76.56%] [G loss: 1.295797]\n",
      "epoch:10 step:9829 [D loss: 0.604171, acc.: 70.31%] [G loss: 1.216396]\n",
      "epoch:10 step:9830 [D loss: 0.591656, acc.: 68.75%] [G loss: 1.271168]\n",
      "epoch:10 step:9831 [D loss: 0.526232, acc.: 73.44%] [G loss: 1.413123]\n",
      "epoch:10 step:9832 [D loss: 0.644675, acc.: 65.62%] [G loss: 1.119414]\n",
      "epoch:10 step:9833 [D loss: 0.618441, acc.: 60.94%] [G loss: 1.388676]\n",
      "epoch:10 step:9834 [D loss: 0.635759, acc.: 67.97%] [G loss: 1.049956]\n",
      "epoch:10 step:9835 [D loss: 0.617907, acc.: 64.84%] [G loss: 1.018083]\n",
      "epoch:10 step:9836 [D loss: 0.631997, acc.: 61.72%] [G loss: 1.258299]\n",
      "epoch:10 step:9837 [D loss: 0.456143, acc.: 81.25%] [G loss: 1.537693]\n",
      "epoch:10 step:9838 [D loss: 0.504077, acc.: 78.12%] [G loss: 1.223472]\n",
      "epoch:10 step:9839 [D loss: 0.626798, acc.: 62.50%] [G loss: 1.068387]\n",
      "epoch:10 step:9840 [D loss: 0.569254, acc.: 72.66%] [G loss: 1.215798]\n",
      "epoch:10 step:9841 [D loss: 0.596627, acc.: 71.09%] [G loss: 1.018458]\n",
      "epoch:10 step:9842 [D loss: 0.577461, acc.: 68.75%] [G loss: 1.164570]\n",
      "epoch:10 step:9843 [D loss: 0.598973, acc.: 65.62%] [G loss: 1.435485]\n",
      "epoch:10 step:9844 [D loss: 0.751610, acc.: 53.12%] [G loss: 1.136152]\n",
      "epoch:10 step:9845 [D loss: 0.615263, acc.: 64.84%] [G loss: 1.126009]\n",
      "epoch:10 step:9846 [D loss: 0.715352, acc.: 58.59%] [G loss: 1.085523]\n",
      "epoch:10 step:9847 [D loss: 0.624164, acc.: 70.31%] [G loss: 1.279592]\n",
      "epoch:10 step:9848 [D loss: 0.626049, acc.: 67.97%] [G loss: 1.102452]\n",
      "epoch:10 step:9849 [D loss: 0.496862, acc.: 80.47%] [G loss: 1.172103]\n",
      "epoch:10 step:9850 [D loss: 0.529191, acc.: 75.00%] [G loss: 1.145488]\n",
      "epoch:10 step:9851 [D loss: 0.647909, acc.: 64.84%] [G loss: 1.134049]\n",
      "epoch:10 step:9852 [D loss: 0.629466, acc.: 63.28%] [G loss: 1.076104]\n",
      "epoch:10 step:9853 [D loss: 0.586727, acc.: 65.62%] [G loss: 1.207817]\n",
      "epoch:10 step:9854 [D loss: 0.563030, acc.: 74.22%] [G loss: 1.381491]\n",
      "epoch:10 step:9855 [D loss: 0.557046, acc.: 75.78%] [G loss: 1.419299]\n",
      "epoch:10 step:9856 [D loss: 0.504621, acc.: 76.56%] [G loss: 1.257335]\n",
      "epoch:10 step:9857 [D loss: 0.611686, acc.: 63.28%] [G loss: 1.084191]\n",
      "epoch:10 step:9858 [D loss: 0.529590, acc.: 75.00%] [G loss: 1.209431]\n",
      "epoch:10 step:9859 [D loss: 0.592852, acc.: 67.97%] [G loss: 1.220899]\n",
      "epoch:10 step:9860 [D loss: 0.543769, acc.: 75.78%] [G loss: 1.061266]\n",
      "epoch:10 step:9861 [D loss: 0.601904, acc.: 69.53%] [G loss: 1.004639]\n",
      "epoch:10 step:9862 [D loss: 0.646221, acc.: 59.38%] [G loss: 1.187518]\n",
      "epoch:10 step:9863 [D loss: 0.781167, acc.: 50.78%] [G loss: 1.225579]\n",
      "epoch:10 step:9864 [D loss: 0.572808, acc.: 71.88%] [G loss: 1.456403]\n",
      "epoch:10 step:9865 [D loss: 0.570572, acc.: 66.41%] [G loss: 1.355457]\n",
      "epoch:10 step:9866 [D loss: 0.521412, acc.: 70.31%] [G loss: 1.382941]\n",
      "epoch:10 step:9867 [D loss: 0.667039, acc.: 64.06%] [G loss: 1.096621]\n",
      "epoch:10 step:9868 [D loss: 0.642674, acc.: 61.72%] [G loss: 1.143573]\n",
      "epoch:10 step:9869 [D loss: 0.570494, acc.: 68.75%] [G loss: 0.981262]\n",
      "epoch:10 step:9870 [D loss: 0.573162, acc.: 72.66%] [G loss: 1.128178]\n",
      "epoch:10 step:9871 [D loss: 0.577373, acc.: 65.62%] [G loss: 1.004694]\n",
      "epoch:10 step:9872 [D loss: 0.564304, acc.: 77.34%] [G loss: 1.278031]\n",
      "epoch:10 step:9873 [D loss: 0.577380, acc.: 73.44%] [G loss: 1.012839]\n",
      "epoch:10 step:9874 [D loss: 0.653208, acc.: 62.50%] [G loss: 0.945700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9875 [D loss: 0.565209, acc.: 67.97%] [G loss: 1.236918]\n",
      "epoch:10 step:9876 [D loss: 0.639465, acc.: 64.84%] [G loss: 1.322735]\n",
      "epoch:10 step:9877 [D loss: 0.755216, acc.: 50.78%] [G loss: 1.119057]\n",
      "epoch:10 step:9878 [D loss: 0.692917, acc.: 60.16%] [G loss: 1.147718]\n",
      "epoch:10 step:9879 [D loss: 0.545591, acc.: 68.75%] [G loss: 1.357311]\n",
      "epoch:10 step:9880 [D loss: 0.544468, acc.: 72.66%] [G loss: 1.248455]\n",
      "epoch:10 step:9881 [D loss: 0.541373, acc.: 78.12%] [G loss: 1.580875]\n",
      "epoch:10 step:9882 [D loss: 0.589933, acc.: 71.88%] [G loss: 1.169978]\n",
      "epoch:10 step:9883 [D loss: 0.534010, acc.: 75.78%] [G loss: 1.403236]\n",
      "epoch:10 step:9884 [D loss: 0.626474, acc.: 59.38%] [G loss: 0.919746]\n",
      "epoch:10 step:9885 [D loss: 0.623461, acc.: 68.75%] [G loss: 1.059763]\n",
      "epoch:10 step:9886 [D loss: 0.587395, acc.: 70.31%] [G loss: 1.230440]\n",
      "epoch:10 step:9887 [D loss: 0.687877, acc.: 59.38%] [G loss: 1.025524]\n",
      "epoch:10 step:9888 [D loss: 0.542548, acc.: 74.22%] [G loss: 1.301705]\n",
      "epoch:10 step:9889 [D loss: 0.620634, acc.: 67.19%] [G loss: 1.208153]\n",
      "epoch:10 step:9890 [D loss: 0.520522, acc.: 76.56%] [G loss: 0.943589]\n",
      "epoch:10 step:9891 [D loss: 0.600804, acc.: 66.41%] [G loss: 1.225436]\n",
      "epoch:10 step:9892 [D loss: 0.628885, acc.: 66.41%] [G loss: 0.947539]\n",
      "epoch:10 step:9893 [D loss: 0.537967, acc.: 71.09%] [G loss: 1.284461]\n",
      "epoch:10 step:9894 [D loss: 0.632398, acc.: 64.84%] [G loss: 1.342580]\n",
      "epoch:10 step:9895 [D loss: 0.572653, acc.: 65.62%] [G loss: 1.273376]\n",
      "epoch:10 step:9896 [D loss: 0.570228, acc.: 71.09%] [G loss: 1.163443]\n",
      "epoch:10 step:9897 [D loss: 0.596006, acc.: 71.88%] [G loss: 1.128044]\n",
      "epoch:10 step:9898 [D loss: 0.576046, acc.: 73.44%] [G loss: 1.356941]\n",
      "epoch:10 step:9899 [D loss: 0.557544, acc.: 73.44%] [G loss: 1.317268]\n",
      "epoch:10 step:9900 [D loss: 0.532551, acc.: 76.56%] [G loss: 1.270701]\n",
      "epoch:10 step:9901 [D loss: 0.649464, acc.: 58.59%] [G loss: 1.046193]\n",
      "epoch:10 step:9902 [D loss: 0.619943, acc.: 60.94%] [G loss: 1.019963]\n",
      "epoch:10 step:9903 [D loss: 0.680365, acc.: 58.59%] [G loss: 0.944690]\n",
      "epoch:10 step:9904 [D loss: 0.633679, acc.: 62.50%] [G loss: 1.323370]\n",
      "epoch:10 step:9905 [D loss: 0.647992, acc.: 64.06%] [G loss: 1.140632]\n",
      "epoch:10 step:9906 [D loss: 0.584812, acc.: 67.19%] [G loss: 1.254871]\n",
      "epoch:10 step:9907 [D loss: 0.589076, acc.: 69.53%] [G loss: 1.155572]\n",
      "epoch:10 step:9908 [D loss: 0.650294, acc.: 57.81%] [G loss: 1.431042]\n",
      "epoch:10 step:9909 [D loss: 0.565704, acc.: 67.97%] [G loss: 1.142147]\n",
      "epoch:10 step:9910 [D loss: 0.515641, acc.: 79.69%] [G loss: 1.142600]\n",
      "epoch:10 step:9911 [D loss: 0.567742, acc.: 71.88%] [G loss: 1.274134]\n",
      "epoch:10 step:9912 [D loss: 0.551982, acc.: 69.53%] [G loss: 1.124498]\n",
      "epoch:10 step:9913 [D loss: 0.560036, acc.: 74.22%] [G loss: 1.161362]\n",
      "epoch:10 step:9914 [D loss: 0.480373, acc.: 78.12%] [G loss: 1.357807]\n",
      "epoch:10 step:9915 [D loss: 0.578153, acc.: 70.31%] [G loss: 1.373198]\n",
      "epoch:10 step:9916 [D loss: 0.547210, acc.: 75.78%] [G loss: 0.965985]\n",
      "epoch:10 step:9917 [D loss: 0.624953, acc.: 62.50%] [G loss: 1.286437]\n",
      "epoch:10 step:9918 [D loss: 0.650917, acc.: 64.06%] [G loss: 1.135564]\n",
      "epoch:10 step:9919 [D loss: 0.543438, acc.: 71.09%] [G loss: 1.231324]\n",
      "epoch:10 step:9920 [D loss: 0.524119, acc.: 74.22%] [G loss: 1.037072]\n",
      "epoch:10 step:9921 [D loss: 0.557040, acc.: 67.97%] [G loss: 1.250514]\n",
      "epoch:10 step:9922 [D loss: 0.674616, acc.: 57.81%] [G loss: 1.087220]\n",
      "epoch:10 step:9923 [D loss: 0.562606, acc.: 73.44%] [G loss: 1.129733]\n",
      "epoch:10 step:9924 [D loss: 0.594627, acc.: 69.53%] [G loss: 1.145795]\n",
      "epoch:10 step:9925 [D loss: 0.519210, acc.: 76.56%] [G loss: 1.145013]\n",
      "epoch:10 step:9926 [D loss: 0.657335, acc.: 64.84%] [G loss: 1.064036]\n",
      "epoch:10 step:9927 [D loss: 0.670297, acc.: 62.50%] [G loss: 1.084744]\n",
      "epoch:10 step:9928 [D loss: 0.587844, acc.: 69.53%] [G loss: 1.095340]\n",
      "epoch:10 step:9929 [D loss: 0.557142, acc.: 74.22%] [G loss: 1.282640]\n",
      "epoch:10 step:9930 [D loss: 0.596391, acc.: 67.19%] [G loss: 1.139252]\n",
      "epoch:10 step:9931 [D loss: 0.648249, acc.: 60.94%] [G loss: 1.061466]\n",
      "epoch:10 step:9932 [D loss: 0.627543, acc.: 68.75%] [G loss: 1.270815]\n",
      "epoch:10 step:9933 [D loss: 0.529374, acc.: 77.34%] [G loss: 1.296211]\n",
      "epoch:10 step:9934 [D loss: 0.549840, acc.: 73.44%] [G loss: 1.136762]\n",
      "epoch:10 step:9935 [D loss: 0.525585, acc.: 76.56%] [G loss: 1.165144]\n",
      "epoch:10 step:9936 [D loss: 0.620546, acc.: 64.84%] [G loss: 1.289326]\n",
      "epoch:10 step:9937 [D loss: 0.466812, acc.: 81.25%] [G loss: 1.408516]\n",
      "epoch:10 step:9938 [D loss: 0.657355, acc.: 64.06%] [G loss: 0.941121]\n",
      "epoch:10 step:9939 [D loss: 0.609582, acc.: 67.19%] [G loss: 1.122214]\n",
      "epoch:10 step:9940 [D loss: 0.691078, acc.: 54.69%] [G loss: 0.995868]\n",
      "epoch:10 step:9941 [D loss: 0.524795, acc.: 75.00%] [G loss: 1.197445]\n",
      "epoch:10 step:9942 [D loss: 0.643934, acc.: 59.38%] [G loss: 1.218730]\n",
      "epoch:10 step:9943 [D loss: 0.624561, acc.: 68.75%] [G loss: 1.222340]\n",
      "epoch:10 step:9944 [D loss: 0.562499, acc.: 67.19%] [G loss: 1.247080]\n",
      "epoch:10 step:9945 [D loss: 0.656169, acc.: 68.75%] [G loss: 1.249520]\n",
      "epoch:10 step:9946 [D loss: 0.504351, acc.: 77.34%] [G loss: 1.254341]\n",
      "epoch:10 step:9947 [D loss: 0.667328, acc.: 59.38%] [G loss: 1.152764]\n",
      "epoch:10 step:9948 [D loss: 0.618116, acc.: 71.09%] [G loss: 1.217629]\n",
      "epoch:10 step:9949 [D loss: 0.499429, acc.: 77.34%] [G loss: 1.297807]\n",
      "epoch:10 step:9950 [D loss: 0.515505, acc.: 74.22%] [G loss: 1.124214]\n",
      "epoch:10 step:9951 [D loss: 0.592328, acc.: 71.09%] [G loss: 1.214400]\n",
      "epoch:10 step:9952 [D loss: 0.658007, acc.: 64.06%] [G loss: 0.970194]\n",
      "epoch:10 step:9953 [D loss: 0.521203, acc.: 74.22%] [G loss: 1.424706]\n",
      "epoch:10 step:9954 [D loss: 0.717688, acc.: 52.34%] [G loss: 1.063447]\n",
      "epoch:10 step:9955 [D loss: 0.559624, acc.: 69.53%] [G loss: 1.254346]\n",
      "epoch:10 step:9956 [D loss: 0.646826, acc.: 64.84%] [G loss: 1.195567]\n",
      "epoch:10 step:9957 [D loss: 0.511586, acc.: 75.78%] [G loss: 1.502395]\n",
      "epoch:10 step:9958 [D loss: 0.576501, acc.: 74.22%] [G loss: 1.053031]\n",
      "epoch:10 step:9959 [D loss: 0.559057, acc.: 70.31%] [G loss: 1.245470]\n",
      "epoch:10 step:9960 [D loss: 0.578081, acc.: 71.09%] [G loss: 1.393849]\n",
      "epoch:10 step:9961 [D loss: 0.542811, acc.: 76.56%] [G loss: 1.566285]\n",
      "epoch:10 step:9962 [D loss: 0.558649, acc.: 68.75%] [G loss: 1.208608]\n",
      "epoch:10 step:9963 [D loss: 0.735485, acc.: 53.12%] [G loss: 1.291888]\n",
      "epoch:10 step:9964 [D loss: 0.570679, acc.: 71.09%] [G loss: 1.116735]\n",
      "epoch:10 step:9965 [D loss: 0.576813, acc.: 68.75%] [G loss: 1.239158]\n",
      "epoch:10 step:9966 [D loss: 0.572525, acc.: 71.88%] [G loss: 0.998866]\n",
      "epoch:10 step:9967 [D loss: 0.616483, acc.: 67.19%] [G loss: 1.169404]\n",
      "epoch:10 step:9968 [D loss: 0.640470, acc.: 66.41%] [G loss: 1.078323]\n",
      "epoch:10 step:9969 [D loss: 0.539093, acc.: 79.69%] [G loss: 1.321409]\n",
      "epoch:10 step:9970 [D loss: 0.574704, acc.: 70.31%] [G loss: 1.201800]\n",
      "epoch:10 step:9971 [D loss: 0.533653, acc.: 76.56%] [G loss: 1.256012]\n",
      "epoch:10 step:9972 [D loss: 0.521219, acc.: 76.56%] [G loss: 1.294021]\n",
      "epoch:10 step:9973 [D loss: 0.567677, acc.: 70.31%] [G loss: 1.110105]\n",
      "epoch:10 step:9974 [D loss: 0.552943, acc.: 72.66%] [G loss: 1.217550]\n",
      "epoch:10 step:9975 [D loss: 0.648212, acc.: 64.06%] [G loss: 1.210237]\n",
      "epoch:10 step:9976 [D loss: 0.564341, acc.: 70.31%] [G loss: 1.210971]\n",
      "epoch:10 step:9977 [D loss: 0.566865, acc.: 67.97%] [G loss: 1.171152]\n",
      "epoch:10 step:9978 [D loss: 0.574336, acc.: 69.53%] [G loss: 1.375900]\n",
      "epoch:10 step:9979 [D loss: 0.584770, acc.: 70.31%] [G loss: 1.167478]\n",
      "epoch:10 step:9980 [D loss: 0.576741, acc.: 68.75%] [G loss: 1.265223]\n",
      "epoch:10 step:9981 [D loss: 0.661232, acc.: 58.59%] [G loss: 1.139805]\n",
      "epoch:10 step:9982 [D loss: 0.584406, acc.: 67.97%] [G loss: 1.090538]\n",
      "epoch:10 step:9983 [D loss: 0.620778, acc.: 64.06%] [G loss: 1.253035]\n",
      "epoch:10 step:9984 [D loss: 0.734834, acc.: 55.47%] [G loss: 1.158137]\n",
      "epoch:10 step:9985 [D loss: 0.708105, acc.: 58.59%] [G loss: 1.141426]\n",
      "epoch:10 step:9986 [D loss: 0.516906, acc.: 70.31%] [G loss: 1.174405]\n",
      "epoch:10 step:9987 [D loss: 0.669676, acc.: 61.72%] [G loss: 1.133507]\n",
      "epoch:10 step:9988 [D loss: 0.563044, acc.: 71.09%] [G loss: 1.204565]\n",
      "epoch:10 step:9989 [D loss: 0.603742, acc.: 67.97%] [G loss: 1.341810]\n",
      "epoch:10 step:9990 [D loss: 0.781910, acc.: 47.66%] [G loss: 0.942756]\n",
      "epoch:10 step:9991 [D loss: 0.565639, acc.: 70.31%] [G loss: 1.565897]\n",
      "epoch:10 step:9992 [D loss: 0.635793, acc.: 67.19%] [G loss: 1.065437]\n",
      "epoch:10 step:9993 [D loss: 0.498164, acc.: 78.91%] [G loss: 1.353968]\n",
      "epoch:10 step:9994 [D loss: 0.555676, acc.: 70.31%] [G loss: 1.273000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9995 [D loss: 0.559891, acc.: 71.09%] [G loss: 1.097142]\n",
      "epoch:10 step:9996 [D loss: 0.594086, acc.: 69.53%] [G loss: 1.149427]\n",
      "epoch:10 step:9997 [D loss: 0.679859, acc.: 56.25%] [G loss: 1.005129]\n",
      "epoch:10 step:9998 [D loss: 0.540459, acc.: 72.66%] [G loss: 1.123890]\n",
      "epoch:10 step:9999 [D loss: 0.642811, acc.: 64.06%] [G loss: 1.163841]\n",
      "epoch:10 step:10000 [D loss: 0.675183, acc.: 55.47%] [G loss: 1.152301]\n",
      "##############\n",
      "[2.92442948 2.16134972 1.99050097 2.99133577 1.03662283 6.16546312\n",
      " 2.15094108 2.86608379 4.05800227 7.14868929]\n",
      "##########\n",
      "epoch:10 step:10001 [D loss: 0.567184, acc.: 67.97%] [G loss: 1.200588]\n",
      "epoch:10 step:10002 [D loss: 0.666954, acc.: 64.84%] [G loss: 1.136677]\n",
      "epoch:10 step:10003 [D loss: 0.535562, acc.: 78.12%] [G loss: 1.201629]\n",
      "epoch:10 step:10004 [D loss: 0.589214, acc.: 70.31%] [G loss: 1.263000]\n",
      "epoch:10 step:10005 [D loss: 0.613041, acc.: 64.06%] [G loss: 1.195477]\n",
      "epoch:10 step:10006 [D loss: 0.660273, acc.: 60.16%] [G loss: 1.388770]\n",
      "epoch:10 step:10007 [D loss: 0.550410, acc.: 68.75%] [G loss: 1.264868]\n",
      "epoch:10 step:10008 [D loss: 0.632376, acc.: 66.41%] [G loss: 1.100862]\n",
      "epoch:10 step:10009 [D loss: 0.684308, acc.: 62.50%] [G loss: 0.993382]\n",
      "epoch:10 step:10010 [D loss: 0.526921, acc.: 71.88%] [G loss: 1.212852]\n",
      "epoch:10 step:10011 [D loss: 0.537882, acc.: 74.22%] [G loss: 1.131137]\n",
      "epoch:10 step:10012 [D loss: 0.617884, acc.: 62.50%] [G loss: 1.231305]\n",
      "epoch:10 step:10013 [D loss: 0.777328, acc.: 50.00%] [G loss: 1.177407]\n",
      "epoch:10 step:10014 [D loss: 0.629722, acc.: 66.41%] [G loss: 1.243764]\n",
      "epoch:10 step:10015 [D loss: 0.622138, acc.: 68.75%] [G loss: 1.168873]\n",
      "epoch:10 step:10016 [D loss: 0.577907, acc.: 70.31%] [G loss: 1.227083]\n",
      "epoch:10 step:10017 [D loss: 0.618176, acc.: 64.84%] [G loss: 1.175701]\n",
      "epoch:10 step:10018 [D loss: 0.679096, acc.: 56.25%] [G loss: 1.314634]\n",
      "epoch:10 step:10019 [D loss: 0.561997, acc.: 72.66%] [G loss: 1.347249]\n",
      "epoch:10 step:10020 [D loss: 0.704508, acc.: 56.25%] [G loss: 1.164470]\n",
      "epoch:10 step:10021 [D loss: 0.550326, acc.: 68.75%] [G loss: 1.095343]\n",
      "epoch:10 step:10022 [D loss: 0.529287, acc.: 73.44%] [G loss: 1.277099]\n",
      "epoch:10 step:10023 [D loss: 0.562916, acc.: 74.22%] [G loss: 1.340419]\n",
      "epoch:10 step:10024 [D loss: 0.607652, acc.: 67.97%] [G loss: 1.173378]\n",
      "epoch:10 step:10025 [D loss: 0.663346, acc.: 60.94%] [G loss: 1.220345]\n",
      "epoch:10 step:10026 [D loss: 0.514159, acc.: 76.56%] [G loss: 1.209313]\n",
      "epoch:10 step:10027 [D loss: 0.546841, acc.: 72.66%] [G loss: 1.298377]\n",
      "epoch:10 step:10028 [D loss: 0.734061, acc.: 60.16%] [G loss: 1.029097]\n",
      "epoch:10 step:10029 [D loss: 0.602996, acc.: 64.84%] [G loss: 1.043914]\n",
      "epoch:10 step:10030 [D loss: 0.586665, acc.: 66.41%] [G loss: 1.267812]\n",
      "epoch:10 step:10031 [D loss: 0.718283, acc.: 53.91%] [G loss: 0.980304]\n",
      "epoch:10 step:10032 [D loss: 0.739415, acc.: 54.69%] [G loss: 1.182413]\n",
      "epoch:10 step:10033 [D loss: 0.668756, acc.: 66.41%] [G loss: 1.211698]\n",
      "epoch:10 step:10034 [D loss: 0.559320, acc.: 74.22%] [G loss: 1.249416]\n",
      "epoch:10 step:10035 [D loss: 0.609497, acc.: 64.06%] [G loss: 1.322288]\n",
      "epoch:10 step:10036 [D loss: 0.580932, acc.: 73.44%] [G loss: 0.917310]\n",
      "epoch:10 step:10037 [D loss: 0.663254, acc.: 59.38%] [G loss: 1.273799]\n",
      "epoch:10 step:10038 [D loss: 0.593433, acc.: 71.09%] [G loss: 1.065378]\n",
      "epoch:10 step:10039 [D loss: 0.487244, acc.: 78.91%] [G loss: 1.256394]\n",
      "epoch:10 step:10040 [D loss: 0.498146, acc.: 75.00%] [G loss: 1.268457]\n",
      "epoch:10 step:10041 [D loss: 0.703549, acc.: 57.03%] [G loss: 1.003871]\n",
      "epoch:10 step:10042 [D loss: 0.677007, acc.: 57.81%] [G loss: 1.106951]\n",
      "epoch:10 step:10043 [D loss: 0.688426, acc.: 59.38%] [G loss: 1.056104]\n",
      "epoch:10 step:10044 [D loss: 0.596156, acc.: 64.84%] [G loss: 1.128778]\n",
      "epoch:10 step:10045 [D loss: 0.596065, acc.: 67.97%] [G loss: 1.129319]\n",
      "epoch:10 step:10046 [D loss: 0.496358, acc.: 76.56%] [G loss: 1.376580]\n",
      "epoch:10 step:10047 [D loss: 0.510147, acc.: 75.00%] [G loss: 1.319068]\n",
      "epoch:10 step:10048 [D loss: 0.606104, acc.: 67.97%] [G loss: 1.315508]\n",
      "epoch:10 step:10049 [D loss: 0.580177, acc.: 70.31%] [G loss: 1.306729]\n",
      "epoch:10 step:10050 [D loss: 0.618545, acc.: 65.62%] [G loss: 1.159885]\n",
      "epoch:10 step:10051 [D loss: 0.533144, acc.: 77.34%] [G loss: 1.190736]\n",
      "epoch:10 step:10052 [D loss: 0.514376, acc.: 75.78%] [G loss: 1.328832]\n",
      "epoch:10 step:10053 [D loss: 0.617123, acc.: 62.50%] [G loss: 1.092719]\n",
      "epoch:10 step:10054 [D loss: 0.517572, acc.: 77.34%] [G loss: 1.124292]\n",
      "epoch:10 step:10055 [D loss: 0.607152, acc.: 64.06%] [G loss: 1.117295]\n",
      "epoch:10 step:10056 [D loss: 0.491726, acc.: 76.56%] [G loss: 1.428895]\n",
      "epoch:10 step:10057 [D loss: 0.539292, acc.: 76.56%] [G loss: 1.475111]\n",
      "epoch:10 step:10058 [D loss: 0.697527, acc.: 60.16%] [G loss: 1.116377]\n",
      "epoch:10 step:10059 [D loss: 0.536279, acc.: 71.09%] [G loss: 1.244366]\n",
      "epoch:10 step:10060 [D loss: 0.690126, acc.: 53.91%] [G loss: 1.077326]\n",
      "epoch:10 step:10061 [D loss: 0.473566, acc.: 78.91%] [G loss: 1.348239]\n",
      "epoch:10 step:10062 [D loss: 0.644693, acc.: 60.94%] [G loss: 1.012620]\n",
      "epoch:10 step:10063 [D loss: 0.642931, acc.: 62.50%] [G loss: 0.899869]\n",
      "epoch:10 step:10064 [D loss: 0.607595, acc.: 64.06%] [G loss: 1.277711]\n",
      "epoch:10 step:10065 [D loss: 0.493336, acc.: 76.56%] [G loss: 1.579150]\n",
      "epoch:10 step:10066 [D loss: 0.584089, acc.: 66.41%] [G loss: 1.076747]\n",
      "epoch:10 step:10067 [D loss: 0.491203, acc.: 81.25%] [G loss: 1.084552]\n",
      "epoch:10 step:10068 [D loss: 0.608255, acc.: 67.97%] [G loss: 1.104829]\n",
      "epoch:10 step:10069 [D loss: 0.611680, acc.: 65.62%] [G loss: 1.120158]\n",
      "epoch:10 step:10070 [D loss: 0.523271, acc.: 74.22%] [G loss: 1.242226]\n",
      "epoch:10 step:10071 [D loss: 0.519813, acc.: 76.56%] [G loss: 1.178680]\n",
      "epoch:10 step:10072 [D loss: 0.559839, acc.: 75.00%] [G loss: 1.422809]\n",
      "epoch:10 step:10073 [D loss: 0.738250, acc.: 51.56%] [G loss: 1.084164]\n",
      "epoch:10 step:10074 [D loss: 0.627891, acc.: 60.94%] [G loss: 1.117921]\n",
      "epoch:10 step:10075 [D loss: 0.454377, acc.: 80.47%] [G loss: 1.252764]\n",
      "epoch:10 step:10076 [D loss: 0.617236, acc.: 66.41%] [G loss: 1.178747]\n",
      "epoch:10 step:10077 [D loss: 0.710361, acc.: 54.69%] [G loss: 1.231395]\n",
      "epoch:10 step:10078 [D loss: 0.552686, acc.: 77.34%] [G loss: 1.237431]\n",
      "epoch:10 step:10079 [D loss: 0.708546, acc.: 57.03%] [G loss: 1.126607]\n",
      "epoch:10 step:10080 [D loss: 0.544546, acc.: 71.88%] [G loss: 1.446355]\n",
      "epoch:10 step:10081 [D loss: 0.542978, acc.: 73.44%] [G loss: 1.374428]\n",
      "epoch:10 step:10082 [D loss: 0.641797, acc.: 64.84%] [G loss: 1.081816]\n",
      "epoch:10 step:10083 [D loss: 0.610870, acc.: 67.97%] [G loss: 1.204957]\n",
      "epoch:10 step:10084 [D loss: 0.500059, acc.: 74.22%] [G loss: 1.313265]\n",
      "epoch:10 step:10085 [D loss: 0.650192, acc.: 57.03%] [G loss: 1.116980]\n",
      "epoch:10 step:10086 [D loss: 0.597451, acc.: 69.53%] [G loss: 1.176772]\n",
      "epoch:10 step:10087 [D loss: 0.709959, acc.: 52.34%] [G loss: 0.989361]\n",
      "epoch:10 step:10088 [D loss: 0.523443, acc.: 72.66%] [G loss: 1.138564]\n",
      "epoch:10 step:10089 [D loss: 0.501231, acc.: 76.56%] [G loss: 1.521439]\n",
      "epoch:10 step:10090 [D loss: 0.758790, acc.: 50.78%] [G loss: 1.160535]\n",
      "epoch:10 step:10091 [D loss: 0.552002, acc.: 74.22%] [G loss: 1.148950]\n",
      "epoch:10 step:10092 [D loss: 0.563702, acc.: 70.31%] [G loss: 1.441151]\n",
      "epoch:10 step:10093 [D loss: 0.628638, acc.: 62.50%] [G loss: 1.213557]\n",
      "epoch:10 step:10094 [D loss: 0.517002, acc.: 76.56%] [G loss: 1.140849]\n",
      "epoch:10 step:10095 [D loss: 0.544838, acc.: 71.88%] [G loss: 1.279382]\n",
      "epoch:10 step:10096 [D loss: 0.693148, acc.: 58.59%] [G loss: 1.039871]\n",
      "epoch:10 step:10097 [D loss: 0.694012, acc.: 60.16%] [G loss: 1.099022]\n",
      "epoch:10 step:10098 [D loss: 0.564926, acc.: 74.22%] [G loss: 1.285393]\n",
      "epoch:10 step:10099 [D loss: 0.569031, acc.: 69.53%] [G loss: 1.371534]\n",
      "epoch:10 step:10100 [D loss: 0.645590, acc.: 65.62%] [G loss: 1.196608]\n",
      "epoch:10 step:10101 [D loss: 0.751889, acc.: 52.34%] [G loss: 0.959480]\n",
      "epoch:10 step:10102 [D loss: 0.607359, acc.: 66.41%] [G loss: 1.084087]\n",
      "epoch:10 step:10103 [D loss: 0.622138, acc.: 65.62%] [G loss: 1.238146]\n",
      "epoch:10 step:10104 [D loss: 0.495151, acc.: 81.25%] [G loss: 1.487096]\n",
      "epoch:10 step:10105 [D loss: 0.524337, acc.: 73.44%] [G loss: 1.260222]\n",
      "epoch:10 step:10106 [D loss: 0.648571, acc.: 68.75%] [G loss: 1.113196]\n",
      "epoch:10 step:10107 [D loss: 0.635709, acc.: 64.84%] [G loss: 1.474528]\n",
      "epoch:10 step:10108 [D loss: 0.698680, acc.: 56.25%] [G loss: 1.394110]\n",
      "epoch:10 step:10109 [D loss: 0.600637, acc.: 64.06%] [G loss: 1.111898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10110 [D loss: 0.686740, acc.: 57.81%] [G loss: 1.057652]\n",
      "epoch:10 step:10111 [D loss: 0.582616, acc.: 70.31%] [G loss: 1.109156]\n",
      "epoch:10 step:10112 [D loss: 0.606594, acc.: 63.28%] [G loss: 1.172880]\n",
      "epoch:10 step:10113 [D loss: 0.639557, acc.: 58.59%] [G loss: 1.269258]\n",
      "epoch:10 step:10114 [D loss: 0.564940, acc.: 72.66%] [G loss: 0.986338]\n",
      "epoch:10 step:10115 [D loss: 0.591585, acc.: 73.44%] [G loss: 1.229780]\n",
      "epoch:10 step:10116 [D loss: 0.504960, acc.: 78.91%] [G loss: 1.241847]\n",
      "epoch:10 step:10117 [D loss: 0.564984, acc.: 71.09%] [G loss: 1.272996]\n",
      "epoch:10 step:10118 [D loss: 0.596119, acc.: 67.19%] [G loss: 1.017215]\n",
      "epoch:10 step:10119 [D loss: 0.573274, acc.: 68.75%] [G loss: 1.154108]\n",
      "epoch:10 step:10120 [D loss: 0.596447, acc.: 71.88%] [G loss: 1.189003]\n",
      "epoch:10 step:10121 [D loss: 0.526972, acc.: 76.56%] [G loss: 1.411593]\n",
      "epoch:10 step:10122 [D loss: 0.629869, acc.: 67.97%] [G loss: 1.164794]\n",
      "epoch:10 step:10123 [D loss: 0.613383, acc.: 68.75%] [G loss: 1.071490]\n",
      "epoch:10 step:10124 [D loss: 0.637092, acc.: 66.41%] [G loss: 1.216194]\n",
      "epoch:10 step:10125 [D loss: 0.596469, acc.: 66.41%] [G loss: 1.181374]\n",
      "epoch:10 step:10126 [D loss: 0.657864, acc.: 64.06%] [G loss: 1.276496]\n",
      "epoch:10 step:10127 [D loss: 0.569792, acc.: 70.31%] [G loss: 1.268724]\n",
      "epoch:10 step:10128 [D loss: 0.607830, acc.: 67.19%] [G loss: 1.182966]\n",
      "epoch:10 step:10129 [D loss: 0.615408, acc.: 66.41%] [G loss: 1.216079]\n",
      "epoch:10 step:10130 [D loss: 0.576519, acc.: 75.78%] [G loss: 1.140004]\n",
      "epoch:10 step:10131 [D loss: 0.709024, acc.: 57.03%] [G loss: 1.048688]\n",
      "epoch:10 step:10132 [D loss: 0.651437, acc.: 61.72%] [G loss: 0.979413]\n",
      "epoch:10 step:10133 [D loss: 0.574475, acc.: 67.97%] [G loss: 1.358565]\n",
      "epoch:10 step:10134 [D loss: 0.621119, acc.: 62.50%] [G loss: 1.252577]\n",
      "epoch:10 step:10135 [D loss: 0.473987, acc.: 83.59%] [G loss: 1.360527]\n",
      "epoch:10 step:10136 [D loss: 0.648124, acc.: 68.75%] [G loss: 1.372240]\n",
      "epoch:10 step:10137 [D loss: 0.629040, acc.: 64.84%] [G loss: 1.150927]\n",
      "epoch:10 step:10138 [D loss: 0.536026, acc.: 81.25%] [G loss: 0.976402]\n",
      "epoch:10 step:10139 [D loss: 0.514425, acc.: 76.56%] [G loss: 1.121848]\n",
      "epoch:10 step:10140 [D loss: 0.547281, acc.: 71.09%] [G loss: 1.180831]\n",
      "epoch:10 step:10141 [D loss: 0.521120, acc.: 72.66%] [G loss: 1.227310]\n",
      "epoch:10 step:10142 [D loss: 0.509689, acc.: 71.09%] [G loss: 1.179996]\n",
      "epoch:10 step:10143 [D loss: 0.556913, acc.: 69.53%] [G loss: 1.091224]\n",
      "epoch:10 step:10144 [D loss: 0.560973, acc.: 69.53%] [G loss: 1.133314]\n",
      "epoch:10 step:10145 [D loss: 0.636359, acc.: 61.72%] [G loss: 1.085325]\n",
      "epoch:10 step:10146 [D loss: 0.604535, acc.: 65.62%] [G loss: 1.145244]\n",
      "epoch:10 step:10147 [D loss: 0.604331, acc.: 66.41%] [G loss: 1.242857]\n",
      "epoch:10 step:10148 [D loss: 0.627967, acc.: 68.75%] [G loss: 1.137070]\n",
      "epoch:10 step:10149 [D loss: 0.596143, acc.: 63.28%] [G loss: 1.094917]\n",
      "epoch:10 step:10150 [D loss: 0.646167, acc.: 57.81%] [G loss: 1.120595]\n",
      "epoch:10 step:10151 [D loss: 0.665329, acc.: 59.38%] [G loss: 0.928885]\n",
      "epoch:10 step:10152 [D loss: 0.534532, acc.: 73.44%] [G loss: 1.184432]\n",
      "epoch:10 step:10153 [D loss: 0.628618, acc.: 65.62%] [G loss: 1.121610]\n",
      "epoch:10 step:10154 [D loss: 0.668792, acc.: 60.16%] [G loss: 1.198603]\n",
      "epoch:10 step:10155 [D loss: 0.615526, acc.: 65.62%] [G loss: 1.062242]\n",
      "epoch:10 step:10156 [D loss: 0.653444, acc.: 60.94%] [G loss: 1.023503]\n",
      "epoch:10 step:10157 [D loss: 0.424517, acc.: 81.25%] [G loss: 1.323409]\n",
      "epoch:10 step:10158 [D loss: 0.606960, acc.: 63.28%] [G loss: 1.167023]\n",
      "epoch:10 step:10159 [D loss: 0.645976, acc.: 60.94%] [G loss: 0.887600]\n",
      "epoch:10 step:10160 [D loss: 0.675862, acc.: 61.72%] [G loss: 1.215030]\n",
      "epoch:10 step:10161 [D loss: 0.617384, acc.: 64.84%] [G loss: 1.310156]\n",
      "epoch:10 step:10162 [D loss: 0.636633, acc.: 61.72%] [G loss: 1.253451]\n",
      "epoch:10 step:10163 [D loss: 0.771969, acc.: 49.22%] [G loss: 1.003255]\n",
      "epoch:10 step:10164 [D loss: 0.596537, acc.: 65.62%] [G loss: 1.067890]\n",
      "epoch:10 step:10165 [D loss: 0.578582, acc.: 67.97%] [G loss: 1.395319]\n",
      "epoch:10 step:10166 [D loss: 0.599513, acc.: 68.75%] [G loss: 1.211332]\n",
      "epoch:10 step:10167 [D loss: 0.538666, acc.: 71.09%] [G loss: 1.301764]\n",
      "epoch:10 step:10168 [D loss: 0.611461, acc.: 67.97%] [G loss: 1.113856]\n",
      "epoch:10 step:10169 [D loss: 0.566343, acc.: 74.22%] [G loss: 1.259962]\n",
      "epoch:10 step:10170 [D loss: 0.606545, acc.: 67.19%] [G loss: 1.263633]\n",
      "epoch:10 step:10171 [D loss: 0.709258, acc.: 57.03%] [G loss: 1.256510]\n",
      "epoch:10 step:10172 [D loss: 0.589475, acc.: 69.53%] [G loss: 1.321229]\n",
      "epoch:10 step:10173 [D loss: 0.641357, acc.: 67.19%] [G loss: 1.120529]\n",
      "epoch:10 step:10174 [D loss: 0.663299, acc.: 57.81%] [G loss: 1.058938]\n",
      "epoch:10 step:10175 [D loss: 0.499372, acc.: 81.25%] [G loss: 1.358799]\n",
      "epoch:10 step:10176 [D loss: 0.564488, acc.: 69.53%] [G loss: 1.003850]\n",
      "epoch:10 step:10177 [D loss: 0.588764, acc.: 67.97%] [G loss: 1.121165]\n",
      "epoch:10 step:10178 [D loss: 0.575697, acc.: 71.88%] [G loss: 1.209698]\n",
      "epoch:10 step:10179 [D loss: 0.521726, acc.: 75.78%] [G loss: 1.532069]\n",
      "epoch:10 step:10180 [D loss: 0.625541, acc.: 68.75%] [G loss: 1.445491]\n",
      "epoch:10 step:10181 [D loss: 0.645127, acc.: 63.28%] [G loss: 1.394198]\n",
      "epoch:10 step:10182 [D loss: 0.637862, acc.: 61.72%] [G loss: 1.168808]\n",
      "epoch:10 step:10183 [D loss: 0.685357, acc.: 63.28%] [G loss: 1.137859]\n",
      "epoch:10 step:10184 [D loss: 0.582289, acc.: 70.31%] [G loss: 1.229894]\n",
      "epoch:10 step:10185 [D loss: 0.541749, acc.: 75.00%] [G loss: 1.216732]\n",
      "epoch:10 step:10186 [D loss: 0.473653, acc.: 79.69%] [G loss: 1.226340]\n",
      "epoch:10 step:10187 [D loss: 0.630310, acc.: 61.72%] [G loss: 1.086261]\n",
      "epoch:10 step:10188 [D loss: 0.596361, acc.: 67.97%] [G loss: 1.017758]\n",
      "epoch:10 step:10189 [D loss: 0.602809, acc.: 67.97%] [G loss: 1.305755]\n",
      "epoch:10 step:10190 [D loss: 0.513739, acc.: 79.69%] [G loss: 1.240069]\n",
      "epoch:10 step:10191 [D loss: 0.528851, acc.: 72.66%] [G loss: 1.177941]\n",
      "epoch:10 step:10192 [D loss: 0.671833, acc.: 54.69%] [G loss: 0.905432]\n",
      "epoch:10 step:10193 [D loss: 0.656861, acc.: 62.50%] [G loss: 0.942327]\n",
      "epoch:10 step:10194 [D loss: 0.775772, acc.: 46.09%] [G loss: 1.086473]\n",
      "epoch:10 step:10195 [D loss: 0.552309, acc.: 73.44%] [G loss: 1.227615]\n",
      "epoch:10 step:10196 [D loss: 0.672406, acc.: 65.62%] [G loss: 1.042515]\n",
      "epoch:10 step:10197 [D loss: 0.619039, acc.: 60.16%] [G loss: 1.160932]\n",
      "epoch:10 step:10198 [D loss: 0.661020, acc.: 58.59%] [G loss: 1.023647]\n",
      "epoch:10 step:10199 [D loss: 0.546790, acc.: 75.78%] [G loss: 1.312079]\n",
      "epoch:10 step:10200 [D loss: 0.703694, acc.: 57.03%] [G loss: 1.332105]\n",
      "##############\n",
      "[2.59415246 2.16388648 1.59254013 2.93403816 0.96635985 5.27603829\n",
      " 2.03572137 2.65483155 3.783118   5.76726279]\n",
      "##########\n",
      "epoch:10 step:10201 [D loss: 0.585129, acc.: 67.97%] [G loss: 1.058406]\n",
      "epoch:10 step:10202 [D loss: 0.609533, acc.: 67.19%] [G loss: 1.070931]\n",
      "epoch:10 step:10203 [D loss: 0.538273, acc.: 79.69%] [G loss: 0.963307]\n",
      "epoch:10 step:10204 [D loss: 0.702644, acc.: 64.84%] [G loss: 1.146596]\n",
      "epoch:10 step:10205 [D loss: 0.551383, acc.: 73.44%] [G loss: 1.197772]\n",
      "epoch:10 step:10206 [D loss: 0.721588, acc.: 52.34%] [G loss: 1.008815]\n",
      "epoch:10 step:10207 [D loss: 0.687078, acc.: 57.81%] [G loss: 0.900623]\n",
      "epoch:10 step:10208 [D loss: 0.611178, acc.: 67.97%] [G loss: 0.943881]\n",
      "epoch:10 step:10209 [D loss: 0.581639, acc.: 71.88%] [G loss: 1.085207]\n",
      "epoch:10 step:10210 [D loss: 0.488691, acc.: 78.12%] [G loss: 1.320480]\n",
      "epoch:10 step:10211 [D loss: 0.656450, acc.: 60.94%] [G loss: 1.023165]\n",
      "epoch:10 step:10212 [D loss: 0.615763, acc.: 66.41%] [G loss: 1.262831]\n",
      "epoch:10 step:10213 [D loss: 0.591470, acc.: 68.75%] [G loss: 1.414599]\n",
      "epoch:10 step:10214 [D loss: 0.633752, acc.: 65.62%] [G loss: 1.253972]\n",
      "epoch:10 step:10215 [D loss: 0.641474, acc.: 60.16%] [G loss: 1.191604]\n",
      "epoch:10 step:10216 [D loss: 0.707119, acc.: 56.25%] [G loss: 1.032741]\n",
      "epoch:10 step:10217 [D loss: 0.530962, acc.: 73.44%] [G loss: 1.064493]\n",
      "epoch:10 step:10218 [D loss: 0.592573, acc.: 63.28%] [G loss: 0.873119]\n",
      "epoch:10 step:10219 [D loss: 0.676554, acc.: 61.72%] [G loss: 1.202754]\n",
      "epoch:10 step:10220 [D loss: 0.518530, acc.: 72.66%] [G loss: 1.119655]\n",
      "epoch:10 step:10221 [D loss: 0.560440, acc.: 71.09%] [G loss: 1.266414]\n",
      "epoch:10 step:10222 [D loss: 0.545000, acc.: 75.00%] [G loss: 1.027509]\n",
      "epoch:10 step:10223 [D loss: 0.652395, acc.: 65.62%] [G loss: 1.128131]\n",
      "epoch:10 step:10224 [D loss: 0.585821, acc.: 73.44%] [G loss: 1.386821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10225 [D loss: 0.690794, acc.: 61.72%] [G loss: 1.171796]\n",
      "epoch:10 step:10226 [D loss: 0.549383, acc.: 70.31%] [G loss: 1.471890]\n",
      "epoch:10 step:10227 [D loss: 0.637455, acc.: 64.84%] [G loss: 1.307706]\n",
      "epoch:10 step:10228 [D loss: 0.576989, acc.: 65.62%] [G loss: 1.191633]\n",
      "epoch:10 step:10229 [D loss: 0.542993, acc.: 72.66%] [G loss: 1.294006]\n",
      "epoch:10 step:10230 [D loss: 0.732495, acc.: 53.12%] [G loss: 1.017092]\n",
      "epoch:10 step:10231 [D loss: 0.537802, acc.: 71.88%] [G loss: 1.285628]\n",
      "epoch:10 step:10232 [D loss: 0.589293, acc.: 67.19%] [G loss: 1.280720]\n",
      "epoch:10 step:10233 [D loss: 0.595590, acc.: 65.62%] [G loss: 1.283657]\n",
      "epoch:10 step:10234 [D loss: 0.624537, acc.: 64.84%] [G loss: 1.050077]\n",
      "epoch:10 step:10235 [D loss: 0.534983, acc.: 74.22%] [G loss: 1.127032]\n",
      "epoch:10 step:10236 [D loss: 0.590779, acc.: 64.06%] [G loss: 1.178561]\n",
      "epoch:10 step:10237 [D loss: 0.633636, acc.: 63.28%] [G loss: 1.199035]\n",
      "epoch:10 step:10238 [D loss: 0.490989, acc.: 76.56%] [G loss: 1.287831]\n",
      "epoch:10 step:10239 [D loss: 0.504119, acc.: 76.56%] [G loss: 1.202419]\n",
      "epoch:10 step:10240 [D loss: 0.510591, acc.: 76.56%] [G loss: 1.240273]\n",
      "epoch:10 step:10241 [D loss: 0.494425, acc.: 75.78%] [G loss: 1.424002]\n",
      "epoch:10 step:10242 [D loss: 0.621976, acc.: 66.41%] [G loss: 1.081349]\n",
      "epoch:10 step:10243 [D loss: 0.560023, acc.: 70.31%] [G loss: 1.018012]\n",
      "epoch:10 step:10244 [D loss: 0.621087, acc.: 67.19%] [G loss: 1.317296]\n",
      "epoch:10 step:10245 [D loss: 0.594778, acc.: 68.75%] [G loss: 1.429291]\n",
      "epoch:10 step:10246 [D loss: 0.645063, acc.: 66.41%] [G loss: 1.270566]\n",
      "epoch:10 step:10247 [D loss: 0.617102, acc.: 64.84%] [G loss: 1.073087]\n",
      "epoch:10 step:10248 [D loss: 0.620246, acc.: 64.06%] [G loss: 1.180853]\n",
      "epoch:10 step:10249 [D loss: 0.638773, acc.: 62.50%] [G loss: 1.133550]\n",
      "epoch:10 step:10250 [D loss: 0.595357, acc.: 68.75%] [G loss: 1.102398]\n",
      "epoch:10 step:10251 [D loss: 0.594948, acc.: 69.53%] [G loss: 0.957004]\n",
      "epoch:10 step:10252 [D loss: 0.542378, acc.: 74.22%] [G loss: 1.045105]\n",
      "epoch:10 step:10253 [D loss: 0.523476, acc.: 72.66%] [G loss: 1.256048]\n",
      "epoch:10 step:10254 [D loss: 0.639899, acc.: 62.50%] [G loss: 1.177304]\n",
      "epoch:10 step:10255 [D loss: 0.517621, acc.: 75.78%] [G loss: 1.094531]\n",
      "epoch:10 step:10256 [D loss: 0.534034, acc.: 75.00%] [G loss: 1.031732]\n",
      "epoch:10 step:10257 [D loss: 0.628786, acc.: 66.41%] [G loss: 1.263128]\n",
      "epoch:10 step:10258 [D loss: 0.623560, acc.: 64.84%] [G loss: 1.180822]\n",
      "epoch:10 step:10259 [D loss: 0.616930, acc.: 67.97%] [G loss: 1.203267]\n",
      "epoch:10 step:10260 [D loss: 0.660054, acc.: 66.41%] [G loss: 1.105799]\n",
      "epoch:10 step:10261 [D loss: 0.695496, acc.: 59.38%] [G loss: 1.162391]\n",
      "epoch:10 step:10262 [D loss: 0.567434, acc.: 71.09%] [G loss: 1.172713]\n",
      "epoch:10 step:10263 [D loss: 0.520482, acc.: 79.69%] [G loss: 1.120035]\n",
      "epoch:10 step:10264 [D loss: 0.641789, acc.: 67.97%] [G loss: 1.193596]\n",
      "epoch:10 step:10265 [D loss: 0.593974, acc.: 67.97%] [G loss: 1.162819]\n",
      "epoch:10 step:10266 [D loss: 0.467069, acc.: 81.25%] [G loss: 1.336789]\n",
      "epoch:10 step:10267 [D loss: 0.616387, acc.: 62.50%] [G loss: 1.213176]\n",
      "epoch:10 step:10268 [D loss: 0.614433, acc.: 65.62%] [G loss: 0.841263]\n",
      "epoch:10 step:10269 [D loss: 0.657430, acc.: 61.72%] [G loss: 1.377888]\n",
      "epoch:10 step:10270 [D loss: 0.724276, acc.: 52.34%] [G loss: 0.862030]\n",
      "epoch:10 step:10271 [D loss: 0.594075, acc.: 66.41%] [G loss: 1.117544]\n",
      "epoch:10 step:10272 [D loss: 0.596355, acc.: 66.41%] [G loss: 1.245390]\n",
      "epoch:10 step:10273 [D loss: 0.533269, acc.: 75.00%] [G loss: 1.429755]\n",
      "epoch:10 step:10274 [D loss: 0.566800, acc.: 65.62%] [G loss: 1.187863]\n",
      "epoch:10 step:10275 [D loss: 0.590319, acc.: 67.19%] [G loss: 1.210974]\n",
      "epoch:10 step:10276 [D loss: 0.582770, acc.: 67.19%] [G loss: 1.329396]\n",
      "epoch:10 step:10277 [D loss: 0.549669, acc.: 77.34%] [G loss: 1.236548]\n",
      "epoch:10 step:10278 [D loss: 0.591354, acc.: 71.09%] [G loss: 1.296525]\n",
      "epoch:10 step:10279 [D loss: 0.619617, acc.: 65.62%] [G loss: 1.180305]\n",
      "epoch:10 step:10280 [D loss: 0.550893, acc.: 71.09%] [G loss: 1.107900]\n",
      "epoch:10 step:10281 [D loss: 0.525608, acc.: 76.56%] [G loss: 1.329157]\n",
      "epoch:10 step:10282 [D loss: 0.584847, acc.: 70.31%] [G loss: 1.208591]\n",
      "epoch:10 step:10283 [D loss: 0.729906, acc.: 51.56%] [G loss: 1.005977]\n",
      "epoch:10 step:10284 [D loss: 0.559633, acc.: 73.44%] [G loss: 1.228769]\n",
      "epoch:10 step:10285 [D loss: 0.569244, acc.: 68.75%] [G loss: 1.107067]\n",
      "epoch:10 step:10286 [D loss: 0.631128, acc.: 65.62%] [G loss: 0.910796]\n",
      "epoch:10 step:10287 [D loss: 0.515807, acc.: 80.47%] [G loss: 1.272364]\n",
      "epoch:10 step:10288 [D loss: 0.651330, acc.: 61.72%] [G loss: 1.125648]\n",
      "epoch:10 step:10289 [D loss: 0.567847, acc.: 65.62%] [G loss: 1.225726]\n",
      "epoch:10 step:10290 [D loss: 0.674983, acc.: 62.50%] [G loss: 1.130610]\n",
      "epoch:10 step:10291 [D loss: 0.603977, acc.: 64.84%] [G loss: 1.181799]\n",
      "epoch:10 step:10292 [D loss: 0.553695, acc.: 72.66%] [G loss: 1.119042]\n",
      "epoch:10 step:10293 [D loss: 0.591713, acc.: 71.88%] [G loss: 1.051163]\n",
      "epoch:10 step:10294 [D loss: 0.638258, acc.: 64.06%] [G loss: 1.181138]\n",
      "epoch:10 step:10295 [D loss: 0.603688, acc.: 66.41%] [G loss: 1.413531]\n",
      "epoch:10 step:10296 [D loss: 0.606647, acc.: 68.75%] [G loss: 1.136318]\n",
      "epoch:10 step:10297 [D loss: 0.727366, acc.: 59.38%] [G loss: 1.084223]\n",
      "epoch:10 step:10298 [D loss: 0.391344, acc.: 87.50%] [G loss: 1.307346]\n",
      "epoch:10 step:10299 [D loss: 0.558604, acc.: 74.22%] [G loss: 1.061580]\n",
      "epoch:10 step:10300 [D loss: 0.585578, acc.: 69.53%] [G loss: 1.001576]\n",
      "epoch:10 step:10301 [D loss: 0.559875, acc.: 67.97%] [G loss: 1.264000]\n",
      "epoch:10 step:10302 [D loss: 0.619231, acc.: 63.28%] [G loss: 1.213127]\n",
      "epoch:10 step:10303 [D loss: 0.672051, acc.: 58.59%] [G loss: 0.908069]\n",
      "epoch:10 step:10304 [D loss: 0.556352, acc.: 71.09%] [G loss: 1.057508]\n",
      "epoch:10 step:10305 [D loss: 0.616873, acc.: 68.75%] [G loss: 1.256013]\n",
      "epoch:10 step:10306 [D loss: 0.581532, acc.: 68.75%] [G loss: 1.336258]\n",
      "epoch:10 step:10307 [D loss: 0.633708, acc.: 58.59%] [G loss: 1.038240]\n",
      "epoch:11 step:10308 [D loss: 0.480313, acc.: 79.69%] [G loss: 1.198857]\n",
      "epoch:11 step:10309 [D loss: 0.739943, acc.: 53.91%] [G loss: 1.161693]\n",
      "epoch:11 step:10310 [D loss: 0.696877, acc.: 57.03%] [G loss: 1.116068]\n",
      "epoch:11 step:10311 [D loss: 0.586754, acc.: 79.69%] [G loss: 1.149559]\n",
      "epoch:11 step:10312 [D loss: 0.652971, acc.: 61.72%] [G loss: 1.070016]\n",
      "epoch:11 step:10313 [D loss: 0.629972, acc.: 64.84%] [G loss: 1.340613]\n",
      "epoch:11 step:10314 [D loss: 0.648420, acc.: 64.84%] [G loss: 1.054659]\n",
      "epoch:11 step:10315 [D loss: 0.588436, acc.: 68.75%] [G loss: 1.222155]\n",
      "epoch:11 step:10316 [D loss: 0.587592, acc.: 67.97%] [G loss: 1.261177]\n",
      "epoch:11 step:10317 [D loss: 0.458973, acc.: 81.25%] [G loss: 1.385175]\n",
      "epoch:11 step:10318 [D loss: 0.628570, acc.: 67.19%] [G loss: 1.170312]\n",
      "epoch:11 step:10319 [D loss: 0.628282, acc.: 60.16%] [G loss: 1.059761]\n",
      "epoch:11 step:10320 [D loss: 0.608382, acc.: 65.62%] [G loss: 1.330852]\n",
      "epoch:11 step:10321 [D loss: 0.746959, acc.: 51.56%] [G loss: 1.176888]\n",
      "epoch:11 step:10322 [D loss: 0.537429, acc.: 75.78%] [G loss: 1.335388]\n",
      "epoch:11 step:10323 [D loss: 0.583695, acc.: 68.75%] [G loss: 1.330264]\n",
      "epoch:11 step:10324 [D loss: 0.574368, acc.: 71.88%] [G loss: 1.202353]\n",
      "epoch:11 step:10325 [D loss: 0.657645, acc.: 60.94%] [G loss: 1.116307]\n",
      "epoch:11 step:10326 [D loss: 0.648512, acc.: 64.06%] [G loss: 1.038625]\n",
      "epoch:11 step:10327 [D loss: 0.538343, acc.: 75.00%] [G loss: 1.157429]\n",
      "epoch:11 step:10328 [D loss: 0.547360, acc.: 73.44%] [G loss: 1.155041]\n",
      "epoch:11 step:10329 [D loss: 0.574876, acc.: 69.53%] [G loss: 1.165875]\n",
      "epoch:11 step:10330 [D loss: 0.689793, acc.: 57.03%] [G loss: 1.520746]\n",
      "epoch:11 step:10331 [D loss: 0.549736, acc.: 72.66%] [G loss: 1.387814]\n",
      "epoch:11 step:10332 [D loss: 0.562328, acc.: 68.75%] [G loss: 1.362135]\n",
      "epoch:11 step:10333 [D loss: 0.629038, acc.: 64.06%] [G loss: 1.070661]\n",
      "epoch:11 step:10334 [D loss: 0.626935, acc.: 63.28%] [G loss: 0.941453]\n",
      "epoch:11 step:10335 [D loss: 0.608452, acc.: 66.41%] [G loss: 1.141775]\n",
      "epoch:11 step:10336 [D loss: 0.683356, acc.: 64.84%] [G loss: 1.127930]\n",
      "epoch:11 step:10337 [D loss: 0.656935, acc.: 61.72%] [G loss: 1.177675]\n",
      "epoch:11 step:10338 [D loss: 0.577269, acc.: 71.09%] [G loss: 1.300422]\n",
      "epoch:11 step:10339 [D loss: 0.550494, acc.: 71.88%] [G loss: 1.077864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10340 [D loss: 0.588296, acc.: 68.75%] [G loss: 1.046968]\n",
      "epoch:11 step:10341 [D loss: 0.561713, acc.: 75.00%] [G loss: 1.223868]\n",
      "epoch:11 step:10342 [D loss: 0.551298, acc.: 71.88%] [G loss: 1.063630]\n",
      "epoch:11 step:10343 [D loss: 0.568942, acc.: 68.75%] [G loss: 1.070669]\n",
      "epoch:11 step:10344 [D loss: 0.500907, acc.: 78.91%] [G loss: 1.305902]\n",
      "epoch:11 step:10345 [D loss: 0.601055, acc.: 64.06%] [G loss: 1.110700]\n",
      "epoch:11 step:10346 [D loss: 0.660518, acc.: 57.81%] [G loss: 1.339245]\n",
      "epoch:11 step:10347 [D loss: 0.570955, acc.: 75.00%] [G loss: 0.992875]\n",
      "epoch:11 step:10348 [D loss: 0.594476, acc.: 68.75%] [G loss: 0.950609]\n",
      "epoch:11 step:10349 [D loss: 0.497009, acc.: 75.78%] [G loss: 1.459505]\n",
      "epoch:11 step:10350 [D loss: 0.566908, acc.: 71.88%] [G loss: 0.889136]\n",
      "epoch:11 step:10351 [D loss: 0.676697, acc.: 57.03%] [G loss: 1.153177]\n",
      "epoch:11 step:10352 [D loss: 0.574161, acc.: 72.66%] [G loss: 1.220930]\n",
      "epoch:11 step:10353 [D loss: 0.823344, acc.: 44.53%] [G loss: 1.029433]\n",
      "epoch:11 step:10354 [D loss: 0.543162, acc.: 74.22%] [G loss: 1.327806]\n",
      "epoch:11 step:10355 [D loss: 0.650800, acc.: 60.94%] [G loss: 1.270134]\n",
      "epoch:11 step:10356 [D loss: 0.573174, acc.: 71.88%] [G loss: 1.449552]\n",
      "epoch:11 step:10357 [D loss: 0.567659, acc.: 69.53%] [G loss: 1.397195]\n",
      "epoch:11 step:10358 [D loss: 0.636443, acc.: 65.62%] [G loss: 1.154014]\n",
      "epoch:11 step:10359 [D loss: 0.679795, acc.: 56.25%] [G loss: 1.062374]\n",
      "epoch:11 step:10360 [D loss: 0.515581, acc.: 73.44%] [G loss: 1.438660]\n",
      "epoch:11 step:10361 [D loss: 0.635606, acc.: 64.06%] [G loss: 1.302486]\n",
      "epoch:11 step:10362 [D loss: 0.688115, acc.: 54.69%] [G loss: 1.345431]\n",
      "epoch:11 step:10363 [D loss: 0.686835, acc.: 53.91%] [G loss: 1.242067]\n",
      "epoch:11 step:10364 [D loss: 0.662907, acc.: 64.06%] [G loss: 1.128404]\n",
      "epoch:11 step:10365 [D loss: 0.603657, acc.: 71.09%] [G loss: 0.987135]\n",
      "epoch:11 step:10366 [D loss: 0.619818, acc.: 63.28%] [G loss: 1.066702]\n",
      "epoch:11 step:10367 [D loss: 0.644777, acc.: 67.97%] [G loss: 1.460558]\n",
      "epoch:11 step:10368 [D loss: 0.720141, acc.: 57.03%] [G loss: 1.389768]\n",
      "epoch:11 step:10369 [D loss: 0.552941, acc.: 73.44%] [G loss: 1.071288]\n",
      "epoch:11 step:10370 [D loss: 0.582927, acc.: 70.31%] [G loss: 1.236359]\n",
      "epoch:11 step:10371 [D loss: 0.780554, acc.: 57.03%] [G loss: 0.983388]\n",
      "epoch:11 step:10372 [D loss: 0.539992, acc.: 70.31%] [G loss: 1.144504]\n",
      "epoch:11 step:10373 [D loss: 0.659347, acc.: 68.75%] [G loss: 1.448947]\n",
      "epoch:11 step:10374 [D loss: 0.602281, acc.: 71.09%] [G loss: 1.257652]\n",
      "epoch:11 step:10375 [D loss: 0.716613, acc.: 52.34%] [G loss: 1.166777]\n",
      "epoch:11 step:10376 [D loss: 0.474757, acc.: 83.59%] [G loss: 1.260937]\n",
      "epoch:11 step:10377 [D loss: 0.571413, acc.: 72.66%] [G loss: 1.233112]\n",
      "epoch:11 step:10378 [D loss: 0.667512, acc.: 61.72%] [G loss: 1.089128]\n",
      "epoch:11 step:10379 [D loss: 0.495816, acc.: 74.22%] [G loss: 1.099726]\n",
      "epoch:11 step:10380 [D loss: 0.595657, acc.: 67.19%] [G loss: 1.234688]\n",
      "epoch:11 step:10381 [D loss: 0.500801, acc.: 77.34%] [G loss: 1.310226]\n",
      "epoch:11 step:10382 [D loss: 0.501602, acc.: 76.56%] [G loss: 1.270261]\n",
      "epoch:11 step:10383 [D loss: 0.532150, acc.: 76.56%] [G loss: 1.098057]\n",
      "epoch:11 step:10384 [D loss: 0.689537, acc.: 58.59%] [G loss: 1.244618]\n",
      "epoch:11 step:10385 [D loss: 0.616131, acc.: 67.19%] [G loss: 1.130743]\n",
      "epoch:11 step:10386 [D loss: 0.639877, acc.: 57.81%] [G loss: 1.311887]\n",
      "epoch:11 step:10387 [D loss: 0.627936, acc.: 67.19%] [G loss: 1.145572]\n",
      "epoch:11 step:10388 [D loss: 0.684743, acc.: 58.59%] [G loss: 1.224849]\n",
      "epoch:11 step:10389 [D loss: 0.592569, acc.: 66.41%] [G loss: 1.185740]\n",
      "epoch:11 step:10390 [D loss: 0.582462, acc.: 67.97%] [G loss: 1.134126]\n",
      "epoch:11 step:10391 [D loss: 0.564299, acc.: 71.09%] [G loss: 1.200099]\n",
      "epoch:11 step:10392 [D loss: 0.542892, acc.: 73.44%] [G loss: 0.981589]\n",
      "epoch:11 step:10393 [D loss: 0.620627, acc.: 67.19%] [G loss: 1.061939]\n",
      "epoch:11 step:10394 [D loss: 0.546975, acc.: 70.31%] [G loss: 1.085822]\n",
      "epoch:11 step:10395 [D loss: 0.629225, acc.: 64.06%] [G loss: 1.124555]\n",
      "epoch:11 step:10396 [D loss: 0.749257, acc.: 53.91%] [G loss: 1.013865]\n",
      "epoch:11 step:10397 [D loss: 0.465822, acc.: 78.91%] [G loss: 1.071880]\n",
      "epoch:11 step:10398 [D loss: 0.598403, acc.: 66.41%] [G loss: 1.104222]\n",
      "epoch:11 step:10399 [D loss: 0.678028, acc.: 58.59%] [G loss: 1.351908]\n",
      "epoch:11 step:10400 [D loss: 0.536983, acc.: 75.00%] [G loss: 1.162958]\n",
      "##############\n",
      "[2.64277683 1.99509723 1.97589898 2.83344427 1.00312564 6.06256499\n",
      " 2.2699783  2.56592421 3.8094128  4.29044296]\n",
      "##########\n",
      "epoch:11 step:10401 [D loss: 0.610019, acc.: 70.31%] [G loss: 1.157530]\n",
      "epoch:11 step:10402 [D loss: 0.580999, acc.: 72.66%] [G loss: 1.396470]\n",
      "epoch:11 step:10403 [D loss: 0.780163, acc.: 46.09%] [G loss: 1.063848]\n",
      "epoch:11 step:10404 [D loss: 0.607754, acc.: 64.06%] [G loss: 1.217503]\n",
      "epoch:11 step:10405 [D loss: 0.507636, acc.: 79.69%] [G loss: 1.272137]\n",
      "epoch:11 step:10406 [D loss: 0.642167, acc.: 59.38%] [G loss: 1.305774]\n",
      "epoch:11 step:10407 [D loss: 0.543839, acc.: 72.66%] [G loss: 1.248435]\n",
      "epoch:11 step:10408 [D loss: 0.457554, acc.: 81.25%] [G loss: 1.158901]\n",
      "epoch:11 step:10409 [D loss: 0.652589, acc.: 61.72%] [G loss: 1.152950]\n",
      "epoch:11 step:10410 [D loss: 0.613436, acc.: 69.53%] [G loss: 1.150576]\n",
      "epoch:11 step:10411 [D loss: 0.558012, acc.: 73.44%] [G loss: 1.294008]\n",
      "epoch:11 step:10412 [D loss: 0.615952, acc.: 65.62%] [G loss: 1.080748]\n",
      "epoch:11 step:10413 [D loss: 0.620456, acc.: 65.62%] [G loss: 1.194518]\n",
      "epoch:11 step:10414 [D loss: 0.541068, acc.: 73.44%] [G loss: 1.099320]\n",
      "epoch:11 step:10415 [D loss: 0.619623, acc.: 66.41%] [G loss: 1.085883]\n",
      "epoch:11 step:10416 [D loss: 0.537043, acc.: 73.44%] [G loss: 1.285065]\n",
      "epoch:11 step:10417 [D loss: 0.655316, acc.: 59.38%] [G loss: 1.256914]\n",
      "epoch:11 step:10418 [D loss: 0.635742, acc.: 60.16%] [G loss: 1.274435]\n",
      "epoch:11 step:10419 [D loss: 0.538502, acc.: 72.66%] [G loss: 1.153220]\n",
      "epoch:11 step:10420 [D loss: 0.675737, acc.: 56.25%] [G loss: 1.010118]\n",
      "epoch:11 step:10421 [D loss: 0.583634, acc.: 67.19%] [G loss: 1.218428]\n",
      "epoch:11 step:10422 [D loss: 0.610959, acc.: 61.72%] [G loss: 1.286648]\n",
      "epoch:11 step:10423 [D loss: 0.668591, acc.: 61.72%] [G loss: 1.083294]\n",
      "epoch:11 step:10424 [D loss: 0.628697, acc.: 64.06%] [G loss: 0.928017]\n",
      "epoch:11 step:10425 [D loss: 0.586022, acc.: 68.75%] [G loss: 1.157784]\n",
      "epoch:11 step:10426 [D loss: 0.612569, acc.: 65.62%] [G loss: 1.134883]\n",
      "epoch:11 step:10427 [D loss: 0.702477, acc.: 59.38%] [G loss: 1.171438]\n",
      "epoch:11 step:10428 [D loss: 0.571162, acc.: 71.88%] [G loss: 1.277026]\n",
      "epoch:11 step:10429 [D loss: 0.604709, acc.: 64.84%] [G loss: 1.080076]\n",
      "epoch:11 step:10430 [D loss: 0.590368, acc.: 64.84%] [G loss: 1.279752]\n",
      "epoch:11 step:10431 [D loss: 0.539398, acc.: 75.00%] [G loss: 0.947787]\n",
      "epoch:11 step:10432 [D loss: 0.563593, acc.: 72.66%] [G loss: 0.956831]\n",
      "epoch:11 step:10433 [D loss: 0.589100, acc.: 71.09%] [G loss: 1.179523]\n",
      "epoch:11 step:10434 [D loss: 0.595844, acc.: 66.41%] [G loss: 1.173500]\n",
      "epoch:11 step:10435 [D loss: 0.593420, acc.: 71.09%] [G loss: 1.091476]\n",
      "epoch:11 step:10436 [D loss: 0.605543, acc.: 67.97%] [G loss: 0.941853]\n",
      "epoch:11 step:10437 [D loss: 0.628156, acc.: 67.19%] [G loss: 1.210560]\n",
      "epoch:11 step:10438 [D loss: 0.631337, acc.: 66.41%] [G loss: 1.277013]\n",
      "epoch:11 step:10439 [D loss: 0.572737, acc.: 71.88%] [G loss: 1.088972]\n",
      "epoch:11 step:10440 [D loss: 0.557385, acc.: 68.75%] [G loss: 0.868568]\n",
      "epoch:11 step:10441 [D loss: 0.646203, acc.: 62.50%] [G loss: 1.323463]\n",
      "epoch:11 step:10442 [D loss: 0.554580, acc.: 72.66%] [G loss: 1.239320]\n",
      "epoch:11 step:10443 [D loss: 0.707875, acc.: 58.59%] [G loss: 1.127693]\n",
      "epoch:11 step:10444 [D loss: 0.599367, acc.: 69.53%] [G loss: 1.031604]\n",
      "epoch:11 step:10445 [D loss: 0.677669, acc.: 57.81%] [G loss: 1.107913]\n",
      "epoch:11 step:10446 [D loss: 0.629891, acc.: 65.62%] [G loss: 1.144022]\n",
      "epoch:11 step:10447 [D loss: 0.647224, acc.: 64.06%] [G loss: 1.119533]\n",
      "epoch:11 step:10448 [D loss: 0.581884, acc.: 64.06%] [G loss: 1.262700]\n",
      "epoch:11 step:10449 [D loss: 0.634597, acc.: 61.72%] [G loss: 1.218626]\n",
      "epoch:11 step:10450 [D loss: 0.711594, acc.: 50.78%] [G loss: 0.902319]\n",
      "epoch:11 step:10451 [D loss: 0.619553, acc.: 64.84%] [G loss: 1.116711]\n",
      "epoch:11 step:10452 [D loss: 0.492456, acc.: 79.69%] [G loss: 1.381738]\n",
      "epoch:11 step:10453 [D loss: 0.530566, acc.: 71.88%] [G loss: 1.114838]\n",
      "epoch:11 step:10454 [D loss: 0.662468, acc.: 58.59%] [G loss: 1.114511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10455 [D loss: 0.630492, acc.: 66.41%] [G loss: 1.248897]\n",
      "epoch:11 step:10456 [D loss: 0.635877, acc.: 60.16%] [G loss: 1.198434]\n",
      "epoch:11 step:10457 [D loss: 0.625008, acc.: 64.84%] [G loss: 1.086018]\n",
      "epoch:11 step:10458 [D loss: 0.583856, acc.: 65.62%] [G loss: 1.230219]\n",
      "epoch:11 step:10459 [D loss: 0.568813, acc.: 69.53%] [G loss: 1.243913]\n",
      "epoch:11 step:10460 [D loss: 0.650731, acc.: 64.84%] [G loss: 1.361292]\n",
      "epoch:11 step:10461 [D loss: 0.680912, acc.: 57.03%] [G loss: 1.280727]\n",
      "epoch:11 step:10462 [D loss: 0.589458, acc.: 68.75%] [G loss: 1.088103]\n",
      "epoch:11 step:10463 [D loss: 0.464959, acc.: 81.25%] [G loss: 1.344179]\n",
      "epoch:11 step:10464 [D loss: 0.708709, acc.: 59.38%] [G loss: 1.036794]\n",
      "epoch:11 step:10465 [D loss: 0.612538, acc.: 65.62%] [G loss: 1.308652]\n",
      "epoch:11 step:10466 [D loss: 0.443171, acc.: 82.03%] [G loss: 1.236545]\n",
      "epoch:11 step:10467 [D loss: 0.585063, acc.: 67.97%] [G loss: 1.299914]\n",
      "epoch:11 step:10468 [D loss: 0.615545, acc.: 60.94%] [G loss: 1.192023]\n",
      "epoch:11 step:10469 [D loss: 0.613709, acc.: 67.19%] [G loss: 1.135158]\n",
      "epoch:11 step:10470 [D loss: 0.472379, acc.: 80.47%] [G loss: 1.445010]\n",
      "epoch:11 step:10471 [D loss: 0.551844, acc.: 69.53%] [G loss: 1.266097]\n",
      "epoch:11 step:10472 [D loss: 0.719615, acc.: 52.34%] [G loss: 1.016995]\n",
      "epoch:11 step:10473 [D loss: 0.630325, acc.: 66.41%] [G loss: 1.081225]\n",
      "epoch:11 step:10474 [D loss: 0.549052, acc.: 72.66%] [G loss: 1.269314]\n",
      "epoch:11 step:10475 [D loss: 0.592649, acc.: 66.41%] [G loss: 1.502873]\n",
      "epoch:11 step:10476 [D loss: 0.625474, acc.: 64.06%] [G loss: 1.182775]\n",
      "epoch:11 step:10477 [D loss: 0.564053, acc.: 71.09%] [G loss: 1.075652]\n",
      "epoch:11 step:10478 [D loss: 0.585193, acc.: 68.75%] [G loss: 1.354870]\n",
      "epoch:11 step:10479 [D loss: 0.571313, acc.: 72.66%] [G loss: 1.267810]\n",
      "epoch:11 step:10480 [D loss: 0.632421, acc.: 65.62%] [G loss: 1.054765]\n",
      "epoch:11 step:10481 [D loss: 0.569303, acc.: 70.31%] [G loss: 1.161325]\n",
      "epoch:11 step:10482 [D loss: 0.553863, acc.: 70.31%] [G loss: 1.319380]\n",
      "epoch:11 step:10483 [D loss: 0.699447, acc.: 53.91%] [G loss: 1.004194]\n",
      "epoch:11 step:10484 [D loss: 0.710924, acc.: 52.34%] [G loss: 1.043052]\n",
      "epoch:11 step:10485 [D loss: 0.669824, acc.: 53.91%] [G loss: 1.229077]\n",
      "epoch:11 step:10486 [D loss: 0.608662, acc.: 64.84%] [G loss: 1.341523]\n",
      "epoch:11 step:10487 [D loss: 0.535691, acc.: 70.31%] [G loss: 1.174716]\n",
      "epoch:11 step:10488 [D loss: 0.549494, acc.: 70.31%] [G loss: 1.090248]\n",
      "epoch:11 step:10489 [D loss: 0.613772, acc.: 70.31%] [G loss: 1.178336]\n",
      "epoch:11 step:10490 [D loss: 0.499000, acc.: 80.47%] [G loss: 1.371994]\n",
      "epoch:11 step:10491 [D loss: 0.546503, acc.: 73.44%] [G loss: 1.430064]\n",
      "epoch:11 step:10492 [D loss: 0.542917, acc.: 75.00%] [G loss: 1.177210]\n",
      "epoch:11 step:10493 [D loss: 0.580854, acc.: 69.53%] [G loss: 1.341296]\n",
      "epoch:11 step:10494 [D loss: 0.485628, acc.: 79.69%] [G loss: 1.294291]\n",
      "epoch:11 step:10495 [D loss: 0.638666, acc.: 67.97%] [G loss: 1.029566]\n",
      "epoch:11 step:10496 [D loss: 0.590515, acc.: 72.66%] [G loss: 1.217753]\n",
      "epoch:11 step:10497 [D loss: 0.536483, acc.: 72.66%] [G loss: 1.351247]\n",
      "epoch:11 step:10498 [D loss: 0.591747, acc.: 68.75%] [G loss: 1.184563]\n",
      "epoch:11 step:10499 [D loss: 0.605988, acc.: 70.31%] [G loss: 1.068399]\n",
      "epoch:11 step:10500 [D loss: 0.593532, acc.: 65.62%] [G loss: 1.390697]\n",
      "epoch:11 step:10501 [D loss: 0.656637, acc.: 57.81%] [G loss: 1.221589]\n",
      "epoch:11 step:10502 [D loss: 0.538547, acc.: 71.09%] [G loss: 1.310014]\n",
      "epoch:11 step:10503 [D loss: 0.580259, acc.: 69.53%] [G loss: 1.172628]\n",
      "epoch:11 step:10504 [D loss: 0.538725, acc.: 73.44%] [G loss: 1.280459]\n",
      "epoch:11 step:10505 [D loss: 0.635467, acc.: 64.84%] [G loss: 1.121306]\n",
      "epoch:11 step:10506 [D loss: 0.566816, acc.: 67.97%] [G loss: 1.321887]\n",
      "epoch:11 step:10507 [D loss: 0.499887, acc.: 72.66%] [G loss: 1.349176]\n",
      "epoch:11 step:10508 [D loss: 0.468338, acc.: 82.03%] [G loss: 1.460807]\n",
      "epoch:11 step:10509 [D loss: 0.458704, acc.: 82.81%] [G loss: 1.490550]\n",
      "epoch:11 step:10510 [D loss: 0.655547, acc.: 65.62%] [G loss: 1.162948]\n",
      "epoch:11 step:10511 [D loss: 0.472359, acc.: 81.25%] [G loss: 1.343040]\n",
      "epoch:11 step:10512 [D loss: 0.610863, acc.: 65.62%] [G loss: 1.017155]\n",
      "epoch:11 step:10513 [D loss: 0.503245, acc.: 78.12%] [G loss: 1.237021]\n",
      "epoch:11 step:10514 [D loss: 0.525994, acc.: 75.00%] [G loss: 1.324417]\n",
      "epoch:11 step:10515 [D loss: 0.562947, acc.: 71.88%] [G loss: 1.044506]\n",
      "epoch:11 step:10516 [D loss: 0.630093, acc.: 66.41%] [G loss: 1.019789]\n",
      "epoch:11 step:10517 [D loss: 0.625915, acc.: 62.50%] [G loss: 1.380060]\n",
      "epoch:11 step:10518 [D loss: 0.652134, acc.: 61.72%] [G loss: 1.006061]\n",
      "epoch:11 step:10519 [D loss: 0.575531, acc.: 73.44%] [G loss: 1.163880]\n",
      "epoch:11 step:10520 [D loss: 0.660920, acc.: 62.50%] [G loss: 1.317115]\n",
      "epoch:11 step:10521 [D loss: 0.728785, acc.: 54.69%] [G loss: 1.331133]\n",
      "epoch:11 step:10522 [D loss: 0.671382, acc.: 55.47%] [G loss: 1.312248]\n",
      "epoch:11 step:10523 [D loss: 0.573224, acc.: 67.97%] [G loss: 1.271360]\n",
      "epoch:11 step:10524 [D loss: 0.683488, acc.: 57.81%] [G loss: 1.304973]\n",
      "epoch:11 step:10525 [D loss: 0.471048, acc.: 81.25%] [G loss: 1.489967]\n",
      "epoch:11 step:10526 [D loss: 0.641624, acc.: 59.38%] [G loss: 1.235345]\n",
      "epoch:11 step:10527 [D loss: 0.668326, acc.: 56.25%] [G loss: 1.407852]\n",
      "epoch:11 step:10528 [D loss: 0.592925, acc.: 69.53%] [G loss: 1.358129]\n",
      "epoch:11 step:10529 [D loss: 0.674910, acc.: 57.03%] [G loss: 1.369111]\n",
      "epoch:11 step:10530 [D loss: 0.588538, acc.: 67.19%] [G loss: 1.086975]\n",
      "epoch:11 step:10531 [D loss: 0.755152, acc.: 55.47%] [G loss: 1.092640]\n",
      "epoch:11 step:10532 [D loss: 0.586284, acc.: 71.88%] [G loss: 1.313604]\n",
      "epoch:11 step:10533 [D loss: 0.709192, acc.: 56.25%] [G loss: 1.212674]\n",
      "epoch:11 step:10534 [D loss: 0.552122, acc.: 75.00%] [G loss: 1.491950]\n",
      "epoch:11 step:10535 [D loss: 0.703803, acc.: 60.94%] [G loss: 1.335660]\n",
      "epoch:11 step:10536 [D loss: 0.478029, acc.: 78.91%] [G loss: 1.409872]\n",
      "epoch:11 step:10537 [D loss: 0.634363, acc.: 64.06%] [G loss: 1.113928]\n",
      "epoch:11 step:10538 [D loss: 0.536981, acc.: 77.34%] [G loss: 1.293049]\n",
      "epoch:11 step:10539 [D loss: 0.640165, acc.: 67.19%] [G loss: 1.219644]\n",
      "epoch:11 step:10540 [D loss: 0.576818, acc.: 71.09%] [G loss: 1.285675]\n",
      "epoch:11 step:10541 [D loss: 0.562059, acc.: 72.66%] [G loss: 1.229352]\n",
      "epoch:11 step:10542 [D loss: 0.522089, acc.: 75.00%] [G loss: 1.212537]\n",
      "epoch:11 step:10543 [D loss: 0.555100, acc.: 73.44%] [G loss: 1.170366]\n",
      "epoch:11 step:10544 [D loss: 0.767667, acc.: 53.12%] [G loss: 0.927156]\n",
      "epoch:11 step:10545 [D loss: 0.583672, acc.: 71.88%] [G loss: 1.112970]\n",
      "epoch:11 step:10546 [D loss: 0.554988, acc.: 68.75%] [G loss: 1.352892]\n",
      "epoch:11 step:10547 [D loss: 0.570399, acc.: 69.53%] [G loss: 1.185123]\n",
      "epoch:11 step:10548 [D loss: 0.562806, acc.: 73.44%] [G loss: 1.333200]\n",
      "epoch:11 step:10549 [D loss: 0.714593, acc.: 57.03%] [G loss: 1.100993]\n",
      "epoch:11 step:10550 [D loss: 0.591743, acc.: 61.72%] [G loss: 1.225387]\n",
      "epoch:11 step:10551 [D loss: 0.527117, acc.: 77.34%] [G loss: 1.128138]\n",
      "epoch:11 step:10552 [D loss: 0.613598, acc.: 60.94%] [G loss: 1.264930]\n",
      "epoch:11 step:10553 [D loss: 0.640253, acc.: 64.06%] [G loss: 1.341432]\n",
      "epoch:11 step:10554 [D loss: 0.601622, acc.: 67.19%] [G loss: 1.149408]\n",
      "epoch:11 step:10555 [D loss: 0.605864, acc.: 64.84%] [G loss: 1.310407]\n",
      "epoch:11 step:10556 [D loss: 0.616814, acc.: 64.84%] [G loss: 1.314658]\n",
      "epoch:11 step:10557 [D loss: 0.524102, acc.: 75.00%] [G loss: 1.235078]\n",
      "epoch:11 step:10558 [D loss: 0.484672, acc.: 76.56%] [G loss: 1.491537]\n",
      "epoch:11 step:10559 [D loss: 0.483593, acc.: 82.81%] [G loss: 1.259390]\n",
      "epoch:11 step:10560 [D loss: 0.568897, acc.: 75.00%] [G loss: 0.970492]\n",
      "epoch:11 step:10561 [D loss: 0.607120, acc.: 61.72%] [G loss: 1.258254]\n",
      "epoch:11 step:10562 [D loss: 0.548292, acc.: 75.00%] [G loss: 1.056903]\n",
      "epoch:11 step:10563 [D loss: 0.594932, acc.: 67.19%] [G loss: 1.178466]\n",
      "epoch:11 step:10564 [D loss: 0.500518, acc.: 80.47%] [G loss: 1.206099]\n",
      "epoch:11 step:10565 [D loss: 0.589146, acc.: 68.75%] [G loss: 1.208426]\n",
      "epoch:11 step:10566 [D loss: 0.588274, acc.: 68.75%] [G loss: 1.303987]\n",
      "epoch:11 step:10567 [D loss: 0.648542, acc.: 62.50%] [G loss: 1.110639]\n",
      "epoch:11 step:10568 [D loss: 0.546821, acc.: 73.44%] [G loss: 1.489400]\n",
      "epoch:11 step:10569 [D loss: 0.782661, acc.: 46.88%] [G loss: 1.228349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10570 [D loss: 0.715583, acc.: 55.47%] [G loss: 0.928998]\n",
      "epoch:11 step:10571 [D loss: 0.490602, acc.: 78.12%] [G loss: 1.202404]\n",
      "epoch:11 step:10572 [D loss: 0.537972, acc.: 75.00%] [G loss: 1.386500]\n",
      "epoch:11 step:10573 [D loss: 0.640758, acc.: 62.50%] [G loss: 1.180814]\n",
      "epoch:11 step:10574 [D loss: 0.612311, acc.: 62.50%] [G loss: 1.062085]\n",
      "epoch:11 step:10575 [D loss: 0.690031, acc.: 55.47%] [G loss: 1.211803]\n",
      "epoch:11 step:10576 [D loss: 0.459304, acc.: 80.47%] [G loss: 1.291094]\n",
      "epoch:11 step:10577 [D loss: 0.607831, acc.: 64.84%] [G loss: 1.080136]\n",
      "epoch:11 step:10578 [D loss: 0.601164, acc.: 64.06%] [G loss: 1.171043]\n",
      "epoch:11 step:10579 [D loss: 0.707949, acc.: 55.47%] [G loss: 1.263530]\n",
      "epoch:11 step:10580 [D loss: 0.629105, acc.: 64.84%] [G loss: 1.063851]\n",
      "epoch:11 step:10581 [D loss: 0.612445, acc.: 64.06%] [G loss: 1.081675]\n",
      "epoch:11 step:10582 [D loss: 0.608044, acc.: 67.97%] [G loss: 1.312543]\n",
      "epoch:11 step:10583 [D loss: 0.674565, acc.: 63.28%] [G loss: 1.346816]\n",
      "epoch:11 step:10584 [D loss: 0.643667, acc.: 67.19%] [G loss: 1.024693]\n",
      "epoch:11 step:10585 [D loss: 0.562750, acc.: 72.66%] [G loss: 1.090630]\n",
      "epoch:11 step:10586 [D loss: 0.658694, acc.: 62.50%] [G loss: 0.989467]\n",
      "epoch:11 step:10587 [D loss: 0.712495, acc.: 55.47%] [G loss: 1.226146]\n",
      "epoch:11 step:10588 [D loss: 0.542166, acc.: 71.09%] [G loss: 1.339355]\n",
      "epoch:11 step:10589 [D loss: 0.637752, acc.: 64.06%] [G loss: 0.805773]\n",
      "epoch:11 step:10590 [D loss: 0.519257, acc.: 78.12%] [G loss: 1.142429]\n",
      "epoch:11 step:10591 [D loss: 0.601403, acc.: 61.72%] [G loss: 1.255784]\n",
      "epoch:11 step:10592 [D loss: 0.531913, acc.: 77.34%] [G loss: 1.102105]\n",
      "epoch:11 step:10593 [D loss: 0.504397, acc.: 75.00%] [G loss: 1.276123]\n",
      "epoch:11 step:10594 [D loss: 0.619682, acc.: 68.75%] [G loss: 1.178536]\n",
      "epoch:11 step:10595 [D loss: 0.610179, acc.: 64.06%] [G loss: 1.184183]\n",
      "epoch:11 step:10596 [D loss: 0.539744, acc.: 72.66%] [G loss: 1.104861]\n",
      "epoch:11 step:10597 [D loss: 0.576324, acc.: 71.09%] [G loss: 1.073062]\n",
      "epoch:11 step:10598 [D loss: 0.556216, acc.: 71.09%] [G loss: 1.172901]\n",
      "epoch:11 step:10599 [D loss: 0.545777, acc.: 70.31%] [G loss: 1.208700]\n",
      "epoch:11 step:10600 [D loss: 0.777586, acc.: 53.91%] [G loss: 1.300585]\n",
      "##############\n",
      "[2.57408189 2.14781647 1.46117822 2.51678222 0.92497895 5.21682378\n",
      " 1.97373927 2.47577224 3.84493201 5.9414555 ]\n",
      "##########\n",
      "epoch:11 step:10601 [D loss: 0.617116, acc.: 65.62%] [G loss: 1.149865]\n",
      "epoch:11 step:10602 [D loss: 0.648324, acc.: 60.94%] [G loss: 1.264470]\n",
      "epoch:11 step:10603 [D loss: 0.607258, acc.: 67.97%] [G loss: 1.182241]\n",
      "epoch:11 step:10604 [D loss: 0.670689, acc.: 61.72%] [G loss: 1.080015]\n",
      "epoch:11 step:10605 [D loss: 0.637224, acc.: 69.53%] [G loss: 1.283332]\n",
      "epoch:11 step:10606 [D loss: 0.676757, acc.: 54.69%] [G loss: 0.988574]\n",
      "epoch:11 step:10607 [D loss: 0.681677, acc.: 60.16%] [G loss: 1.038041]\n",
      "epoch:11 step:10608 [D loss: 0.795858, acc.: 50.00%] [G loss: 1.169486]\n",
      "epoch:11 step:10609 [D loss: 0.650214, acc.: 62.50%] [G loss: 1.017976]\n",
      "epoch:11 step:10610 [D loss: 0.647974, acc.: 66.41%] [G loss: 1.069213]\n",
      "epoch:11 step:10611 [D loss: 0.580920, acc.: 69.53%] [G loss: 1.021559]\n",
      "epoch:11 step:10612 [D loss: 0.661874, acc.: 57.03%] [G loss: 1.229097]\n",
      "epoch:11 step:10613 [D loss: 0.608743, acc.: 69.53%] [G loss: 1.112986]\n",
      "epoch:11 step:10614 [D loss: 0.641077, acc.: 64.84%] [G loss: 0.969050]\n",
      "epoch:11 step:10615 [D loss: 0.647686, acc.: 60.94%] [G loss: 1.152305]\n",
      "epoch:11 step:10616 [D loss: 0.475135, acc.: 78.12%] [G loss: 1.224391]\n",
      "epoch:11 step:10617 [D loss: 0.545842, acc.: 74.22%] [G loss: 1.258328]\n",
      "epoch:11 step:10618 [D loss: 0.560431, acc.: 78.12%] [G loss: 1.261544]\n",
      "epoch:11 step:10619 [D loss: 0.662398, acc.: 64.06%] [G loss: 1.204628]\n",
      "epoch:11 step:10620 [D loss: 0.507757, acc.: 82.03%] [G loss: 1.308278]\n",
      "epoch:11 step:10621 [D loss: 0.586899, acc.: 70.31%] [G loss: 1.307832]\n",
      "epoch:11 step:10622 [D loss: 0.470620, acc.: 79.69%] [G loss: 1.240627]\n",
      "epoch:11 step:10623 [D loss: 0.592292, acc.: 64.84%] [G loss: 1.216135]\n",
      "epoch:11 step:10624 [D loss: 0.504370, acc.: 79.69%] [G loss: 1.171231]\n",
      "epoch:11 step:10625 [D loss: 0.721721, acc.: 51.56%] [G loss: 1.169298]\n",
      "epoch:11 step:10626 [D loss: 0.706294, acc.: 57.03%] [G loss: 0.964504]\n",
      "epoch:11 step:10627 [D loss: 0.572108, acc.: 66.41%] [G loss: 1.289732]\n",
      "epoch:11 step:10628 [D loss: 0.665599, acc.: 64.06%] [G loss: 0.947492]\n",
      "epoch:11 step:10629 [D loss: 0.694241, acc.: 56.25%] [G loss: 0.976848]\n",
      "epoch:11 step:10630 [D loss: 0.617564, acc.: 70.31%] [G loss: 1.046074]\n",
      "epoch:11 step:10631 [D loss: 0.531873, acc.: 77.34%] [G loss: 1.216503]\n",
      "epoch:11 step:10632 [D loss: 0.549316, acc.: 70.31%] [G loss: 1.140254]\n",
      "epoch:11 step:10633 [D loss: 0.553087, acc.: 74.22%] [G loss: 1.075771]\n",
      "epoch:11 step:10634 [D loss: 0.572177, acc.: 67.97%] [G loss: 1.102833]\n",
      "epoch:11 step:10635 [D loss: 0.494146, acc.: 79.69%] [G loss: 1.444089]\n",
      "epoch:11 step:10636 [D loss: 0.504058, acc.: 78.12%] [G loss: 1.354188]\n",
      "epoch:11 step:10637 [D loss: 0.611038, acc.: 61.72%] [G loss: 1.159014]\n",
      "epoch:11 step:10638 [D loss: 0.537575, acc.: 71.09%] [G loss: 1.163360]\n",
      "epoch:11 step:10639 [D loss: 0.542761, acc.: 71.88%] [G loss: 1.164485]\n",
      "epoch:11 step:10640 [D loss: 0.610392, acc.: 64.06%] [G loss: 1.138193]\n",
      "epoch:11 step:10641 [D loss: 0.692710, acc.: 57.81%] [G loss: 1.015487]\n",
      "epoch:11 step:10642 [D loss: 0.526546, acc.: 75.00%] [G loss: 1.358484]\n",
      "epoch:11 step:10643 [D loss: 0.473102, acc.: 82.81%] [G loss: 1.349701]\n",
      "epoch:11 step:10644 [D loss: 0.661779, acc.: 61.72%] [G loss: 1.370639]\n",
      "epoch:11 step:10645 [D loss: 0.490528, acc.: 81.25%] [G loss: 1.081069]\n",
      "epoch:11 step:10646 [D loss: 0.578687, acc.: 68.75%] [G loss: 1.228385]\n",
      "epoch:11 step:10647 [D loss: 0.616120, acc.: 63.28%] [G loss: 1.171677]\n",
      "epoch:11 step:10648 [D loss: 0.626207, acc.: 63.28%] [G loss: 1.162285]\n",
      "epoch:11 step:10649 [D loss: 0.664831, acc.: 56.25%] [G loss: 1.210947]\n",
      "epoch:11 step:10650 [D loss: 0.662187, acc.: 60.94%] [G loss: 1.123534]\n",
      "epoch:11 step:10651 [D loss: 0.480147, acc.: 82.81%] [G loss: 1.235049]\n",
      "epoch:11 step:10652 [D loss: 0.545976, acc.: 72.66%] [G loss: 1.045276]\n",
      "epoch:11 step:10653 [D loss: 0.591644, acc.: 67.19%] [G loss: 1.038717]\n",
      "epoch:11 step:10654 [D loss: 0.601982, acc.: 64.06%] [G loss: 1.090954]\n",
      "epoch:11 step:10655 [D loss: 0.639889, acc.: 62.50%] [G loss: 0.997113]\n",
      "epoch:11 step:10656 [D loss: 0.571227, acc.: 67.97%] [G loss: 1.059947]\n",
      "epoch:11 step:10657 [D loss: 0.741725, acc.: 50.00%] [G loss: 1.074563]\n",
      "epoch:11 step:10658 [D loss: 0.581329, acc.: 67.19%] [G loss: 1.063204]\n",
      "epoch:11 step:10659 [D loss: 0.648635, acc.: 60.16%] [G loss: 1.193798]\n",
      "epoch:11 step:10660 [D loss: 0.521973, acc.: 78.91%] [G loss: 1.361894]\n",
      "epoch:11 step:10661 [D loss: 0.678249, acc.: 58.59%] [G loss: 0.986757]\n",
      "epoch:11 step:10662 [D loss: 0.668240, acc.: 60.94%] [G loss: 1.013188]\n",
      "epoch:11 step:10663 [D loss: 0.567036, acc.: 69.53%] [G loss: 1.571384]\n",
      "epoch:11 step:10664 [D loss: 0.707503, acc.: 54.69%] [G loss: 1.036463]\n",
      "epoch:11 step:10665 [D loss: 0.596078, acc.: 64.06%] [G loss: 1.190349]\n",
      "epoch:11 step:10666 [D loss: 0.694181, acc.: 55.47%] [G loss: 1.166968]\n",
      "epoch:11 step:10667 [D loss: 0.482102, acc.: 81.25%] [G loss: 1.291327]\n",
      "epoch:11 step:10668 [D loss: 0.644534, acc.: 63.28%] [G loss: 1.007258]\n",
      "epoch:11 step:10669 [D loss: 0.495625, acc.: 78.91%] [G loss: 1.274060]\n",
      "epoch:11 step:10670 [D loss: 0.492620, acc.: 78.12%] [G loss: 1.295339]\n",
      "epoch:11 step:10671 [D loss: 0.637072, acc.: 64.06%] [G loss: 1.243538]\n",
      "epoch:11 step:10672 [D loss: 0.544910, acc.: 73.44%] [G loss: 1.313292]\n",
      "epoch:11 step:10673 [D loss: 0.536115, acc.: 74.22%] [G loss: 1.484008]\n",
      "epoch:11 step:10674 [D loss: 0.673031, acc.: 58.59%] [G loss: 1.060540]\n",
      "epoch:11 step:10675 [D loss: 0.539851, acc.: 73.44%] [G loss: 1.299412]\n",
      "epoch:11 step:10676 [D loss: 0.658973, acc.: 60.94%] [G loss: 1.390583]\n",
      "epoch:11 step:10677 [D loss: 0.565118, acc.: 70.31%] [G loss: 1.042466]\n",
      "epoch:11 step:10678 [D loss: 0.548471, acc.: 73.44%] [G loss: 1.144403]\n",
      "epoch:11 step:10679 [D loss: 0.584728, acc.: 69.53%] [G loss: 1.164385]\n",
      "epoch:11 step:10680 [D loss: 0.613886, acc.: 67.97%] [G loss: 1.264683]\n",
      "epoch:11 step:10681 [D loss: 0.683300, acc.: 59.38%] [G loss: 1.081733]\n",
      "epoch:11 step:10682 [D loss: 0.763116, acc.: 54.69%] [G loss: 1.184043]\n",
      "epoch:11 step:10683 [D loss: 0.722829, acc.: 61.72%] [G loss: 0.970904]\n",
      "epoch:11 step:10684 [D loss: 0.435474, acc.: 84.38%] [G loss: 1.079891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10685 [D loss: 0.624904, acc.: 60.94%] [G loss: 1.064421]\n",
      "epoch:11 step:10686 [D loss: 0.499202, acc.: 78.91%] [G loss: 1.316298]\n",
      "epoch:11 step:10687 [D loss: 0.673436, acc.: 57.81%] [G loss: 1.222538]\n",
      "epoch:11 step:10688 [D loss: 0.672413, acc.: 58.59%] [G loss: 1.385754]\n",
      "epoch:11 step:10689 [D loss: 0.602395, acc.: 69.53%] [G loss: 1.044614]\n",
      "epoch:11 step:10690 [D loss: 0.574991, acc.: 73.44%] [G loss: 1.259665]\n",
      "epoch:11 step:10691 [D loss: 0.650220, acc.: 60.16%] [G loss: 1.226235]\n",
      "epoch:11 step:10692 [D loss: 0.628638, acc.: 68.75%] [G loss: 0.974876]\n",
      "epoch:11 step:10693 [D loss: 0.666876, acc.: 62.50%] [G loss: 1.093929]\n",
      "epoch:11 step:10694 [D loss: 0.622763, acc.: 64.06%] [G loss: 1.221182]\n",
      "epoch:11 step:10695 [D loss: 0.633403, acc.: 60.94%] [G loss: 1.266238]\n",
      "epoch:11 step:10696 [D loss: 0.745567, acc.: 53.12%] [G loss: 0.950340]\n",
      "epoch:11 step:10697 [D loss: 0.592301, acc.: 65.62%] [G loss: 1.279353]\n",
      "epoch:11 step:10698 [D loss: 0.643797, acc.: 61.72%] [G loss: 1.161950]\n",
      "epoch:11 step:10699 [D loss: 0.682444, acc.: 60.16%] [G loss: 1.261890]\n",
      "epoch:11 step:10700 [D loss: 0.660917, acc.: 59.38%] [G loss: 1.238354]\n",
      "epoch:11 step:10701 [D loss: 0.636452, acc.: 67.97%] [G loss: 1.057287]\n",
      "epoch:11 step:10702 [D loss: 0.593816, acc.: 65.62%] [G loss: 1.186916]\n",
      "epoch:11 step:10703 [D loss: 0.576463, acc.: 70.31%] [G loss: 1.236638]\n",
      "epoch:11 step:10704 [D loss: 0.538537, acc.: 78.12%] [G loss: 1.111974]\n",
      "epoch:11 step:10705 [D loss: 0.587498, acc.: 70.31%] [G loss: 1.122672]\n",
      "epoch:11 step:10706 [D loss: 0.609908, acc.: 68.75%] [G loss: 1.124593]\n",
      "epoch:11 step:10707 [D loss: 0.567460, acc.: 69.53%] [G loss: 1.282158]\n",
      "epoch:11 step:10708 [D loss: 0.567034, acc.: 67.97%] [G loss: 1.137988]\n",
      "epoch:11 step:10709 [D loss: 0.589811, acc.: 69.53%] [G loss: 1.325517]\n",
      "epoch:11 step:10710 [D loss: 0.669281, acc.: 61.72%] [G loss: 0.987703]\n",
      "epoch:11 step:10711 [D loss: 0.562549, acc.: 74.22%] [G loss: 1.181711]\n",
      "epoch:11 step:10712 [D loss: 0.538725, acc.: 70.31%] [G loss: 1.242833]\n",
      "epoch:11 step:10713 [D loss: 0.715125, acc.: 57.81%] [G loss: 1.143704]\n",
      "epoch:11 step:10714 [D loss: 0.654129, acc.: 64.06%] [G loss: 1.472275]\n",
      "epoch:11 step:10715 [D loss: 0.489703, acc.: 75.00%] [G loss: 1.413338]\n",
      "epoch:11 step:10716 [D loss: 0.668631, acc.: 59.38%] [G loss: 1.192453]\n",
      "epoch:11 step:10717 [D loss: 0.528291, acc.: 72.66%] [G loss: 1.196998]\n",
      "epoch:11 step:10718 [D loss: 0.632511, acc.: 59.38%] [G loss: 1.143130]\n",
      "epoch:11 step:10719 [D loss: 0.600046, acc.: 67.97%] [G loss: 1.107502]\n",
      "epoch:11 step:10720 [D loss: 0.666566, acc.: 62.50%] [G loss: 1.180643]\n",
      "epoch:11 step:10721 [D loss: 0.703215, acc.: 57.03%] [G loss: 1.001570]\n",
      "epoch:11 step:10722 [D loss: 0.592764, acc.: 67.97%] [G loss: 1.060931]\n",
      "epoch:11 step:10723 [D loss: 0.630782, acc.: 62.50%] [G loss: 0.983500]\n",
      "epoch:11 step:10724 [D loss: 0.595206, acc.: 65.62%] [G loss: 1.057843]\n",
      "epoch:11 step:10725 [D loss: 0.615648, acc.: 64.06%] [G loss: 1.158728]\n",
      "epoch:11 step:10726 [D loss: 0.595149, acc.: 68.75%] [G loss: 1.342239]\n",
      "epoch:11 step:10727 [D loss: 0.527770, acc.: 76.56%] [G loss: 1.233569]\n",
      "epoch:11 step:10728 [D loss: 0.592460, acc.: 67.19%] [G loss: 1.056823]\n",
      "epoch:11 step:10729 [D loss: 0.538411, acc.: 71.09%] [G loss: 1.365422]\n",
      "epoch:11 step:10730 [D loss: 0.648957, acc.: 60.16%] [G loss: 1.096194]\n",
      "epoch:11 step:10731 [D loss: 0.630400, acc.: 61.72%] [G loss: 1.218250]\n",
      "epoch:11 step:10732 [D loss: 0.523025, acc.: 75.00%] [G loss: 0.835385]\n",
      "epoch:11 step:10733 [D loss: 0.704070, acc.: 56.25%] [G loss: 1.111225]\n",
      "epoch:11 step:10734 [D loss: 0.703842, acc.: 58.59%] [G loss: 1.109014]\n",
      "epoch:11 step:10735 [D loss: 0.619511, acc.: 63.28%] [G loss: 1.310353]\n",
      "epoch:11 step:10736 [D loss: 0.511882, acc.: 77.34%] [G loss: 1.213763]\n",
      "epoch:11 step:10737 [D loss: 0.487040, acc.: 78.12%] [G loss: 1.337829]\n",
      "epoch:11 step:10738 [D loss: 0.670557, acc.: 60.16%] [G loss: 1.295861]\n",
      "epoch:11 step:10739 [D loss: 0.605954, acc.: 68.75%] [G loss: 1.092591]\n",
      "epoch:11 step:10740 [D loss: 0.592428, acc.: 70.31%] [G loss: 1.181658]\n",
      "epoch:11 step:10741 [D loss: 0.628063, acc.: 64.06%] [G loss: 1.095759]\n",
      "epoch:11 step:10742 [D loss: 0.591218, acc.: 68.75%] [G loss: 1.217374]\n",
      "epoch:11 step:10743 [D loss: 0.548032, acc.: 73.44%] [G loss: 1.168678]\n",
      "epoch:11 step:10744 [D loss: 0.523971, acc.: 75.78%] [G loss: 1.224316]\n",
      "epoch:11 step:10745 [D loss: 0.570446, acc.: 69.53%] [G loss: 1.294133]\n",
      "epoch:11 step:10746 [D loss: 0.549891, acc.: 69.53%] [G loss: 1.025806]\n",
      "epoch:11 step:10747 [D loss: 0.524057, acc.: 74.22%] [G loss: 1.119615]\n",
      "epoch:11 step:10748 [D loss: 0.534466, acc.: 78.91%] [G loss: 1.326523]\n",
      "epoch:11 step:10749 [D loss: 0.506160, acc.: 76.56%] [G loss: 1.190702]\n",
      "epoch:11 step:10750 [D loss: 0.499301, acc.: 76.56%] [G loss: 1.256758]\n",
      "epoch:11 step:10751 [D loss: 0.522299, acc.: 72.66%] [G loss: 1.223000]\n",
      "epoch:11 step:10752 [D loss: 0.538880, acc.: 75.78%] [G loss: 1.260494]\n",
      "epoch:11 step:10753 [D loss: 0.597009, acc.: 65.62%] [G loss: 1.153076]\n",
      "epoch:11 step:10754 [D loss: 0.515042, acc.: 80.47%] [G loss: 1.208290]\n",
      "epoch:11 step:10755 [D loss: 0.627046, acc.: 64.84%] [G loss: 1.280472]\n",
      "epoch:11 step:10756 [D loss: 0.511213, acc.: 79.69%] [G loss: 1.399035]\n",
      "epoch:11 step:10757 [D loss: 0.653047, acc.: 65.62%] [G loss: 0.929723]\n",
      "epoch:11 step:10758 [D loss: 0.547037, acc.: 74.22%] [G loss: 1.005759]\n",
      "epoch:11 step:10759 [D loss: 0.610137, acc.: 68.75%] [G loss: 1.091026]\n",
      "epoch:11 step:10760 [D loss: 0.534118, acc.: 75.78%] [G loss: 1.322376]\n",
      "epoch:11 step:10761 [D loss: 0.560162, acc.: 71.09%] [G loss: 0.971266]\n",
      "epoch:11 step:10762 [D loss: 0.633351, acc.: 63.28%] [G loss: 1.252249]\n",
      "epoch:11 step:10763 [D loss: 0.596939, acc.: 71.09%] [G loss: 1.118334]\n",
      "epoch:11 step:10764 [D loss: 0.643052, acc.: 62.50%] [G loss: 1.293468]\n",
      "epoch:11 step:10765 [D loss: 0.595435, acc.: 66.41%] [G loss: 1.291591]\n",
      "epoch:11 step:10766 [D loss: 0.505582, acc.: 78.91%] [G loss: 1.267657]\n",
      "epoch:11 step:10767 [D loss: 0.715507, acc.: 57.81%] [G loss: 1.125650]\n",
      "epoch:11 step:10768 [D loss: 0.636193, acc.: 60.94%] [G loss: 1.395907]\n",
      "epoch:11 step:10769 [D loss: 0.664568, acc.: 60.94%] [G loss: 1.223447]\n",
      "epoch:11 step:10770 [D loss: 0.645308, acc.: 65.62%] [G loss: 1.202190]\n",
      "epoch:11 step:10771 [D loss: 0.561909, acc.: 72.66%] [G loss: 0.972976]\n",
      "epoch:11 step:10772 [D loss: 0.659736, acc.: 57.03%] [G loss: 0.976185]\n",
      "epoch:11 step:10773 [D loss: 0.593332, acc.: 64.84%] [G loss: 1.372494]\n",
      "epoch:11 step:10774 [D loss: 0.488131, acc.: 81.25%] [G loss: 1.322515]\n",
      "epoch:11 step:10775 [D loss: 0.561322, acc.: 75.00%] [G loss: 1.169635]\n",
      "epoch:11 step:10776 [D loss: 0.565375, acc.: 69.53%] [G loss: 1.322121]\n",
      "epoch:11 step:10777 [D loss: 0.746143, acc.: 50.00%] [G loss: 0.955628]\n",
      "epoch:11 step:10778 [D loss: 0.644618, acc.: 64.06%] [G loss: 1.108876]\n",
      "epoch:11 step:10779 [D loss: 0.584781, acc.: 71.88%] [G loss: 1.206807]\n",
      "epoch:11 step:10780 [D loss: 0.546147, acc.: 75.78%] [G loss: 1.197216]\n",
      "epoch:11 step:10781 [D loss: 0.552362, acc.: 70.31%] [G loss: 1.123087]\n",
      "epoch:11 step:10782 [D loss: 0.662411, acc.: 62.50%] [G loss: 1.126085]\n",
      "epoch:11 step:10783 [D loss: 0.619081, acc.: 62.50%] [G loss: 1.462103]\n",
      "epoch:11 step:10784 [D loss: 0.579021, acc.: 64.06%] [G loss: 1.176892]\n",
      "epoch:11 step:10785 [D loss: 0.727052, acc.: 54.69%] [G loss: 1.065905]\n",
      "epoch:11 step:10786 [D loss: 0.508424, acc.: 81.25%] [G loss: 1.280046]\n",
      "epoch:11 step:10787 [D loss: 0.625008, acc.: 64.06%] [G loss: 1.001702]\n",
      "epoch:11 step:10788 [D loss: 0.648283, acc.: 67.19%] [G loss: 1.201882]\n",
      "epoch:11 step:10789 [D loss: 0.606688, acc.: 64.06%] [G loss: 1.307332]\n",
      "epoch:11 step:10790 [D loss: 0.611313, acc.: 70.31%] [G loss: 1.136132]\n",
      "epoch:11 step:10791 [D loss: 0.474772, acc.: 80.47%] [G loss: 1.395878]\n",
      "epoch:11 step:10792 [D loss: 0.621415, acc.: 64.06%] [G loss: 1.135759]\n",
      "epoch:11 step:10793 [D loss: 0.480678, acc.: 84.38%] [G loss: 1.136801]\n",
      "epoch:11 step:10794 [D loss: 0.643893, acc.: 64.84%] [G loss: 1.092084]\n",
      "epoch:11 step:10795 [D loss: 0.523756, acc.: 73.44%] [G loss: 1.091775]\n",
      "epoch:11 step:10796 [D loss: 0.575446, acc.: 70.31%] [G loss: 1.401363]\n",
      "epoch:11 step:10797 [D loss: 0.558150, acc.: 71.88%] [G loss: 1.323575]\n",
      "epoch:11 step:10798 [D loss: 0.554293, acc.: 71.09%] [G loss: 1.382242]\n",
      "epoch:11 step:10799 [D loss: 0.663998, acc.: 65.62%] [G loss: 1.110717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10800 [D loss: 0.627629, acc.: 66.41%] [G loss: 1.321931]\n",
      "##############\n",
      "[2.88717662 2.09140566 2.13482854 2.89811469 1.05291839 6.19319541\n",
      " 2.1357659  3.09079128 4.0085494  7.14868929]\n",
      "##########\n",
      "epoch:11 step:10801 [D loss: 0.630916, acc.: 65.62%] [G loss: 1.365989]\n",
      "epoch:11 step:10802 [D loss: 0.634088, acc.: 67.19%] [G loss: 0.873824]\n",
      "epoch:11 step:10803 [D loss: 0.649846, acc.: 64.84%] [G loss: 1.094702]\n",
      "epoch:11 step:10804 [D loss: 0.675598, acc.: 54.69%] [G loss: 1.205527]\n",
      "epoch:11 step:10805 [D loss: 0.595758, acc.: 65.62%] [G loss: 1.230321]\n",
      "epoch:11 step:10806 [D loss: 0.598983, acc.: 72.66%] [G loss: 1.081851]\n",
      "epoch:11 step:10807 [D loss: 0.615959, acc.: 65.62%] [G loss: 1.135681]\n",
      "epoch:11 step:10808 [D loss: 0.501852, acc.: 77.34%] [G loss: 1.217844]\n",
      "epoch:11 step:10809 [D loss: 0.660707, acc.: 60.94%] [G loss: 1.016033]\n",
      "epoch:11 step:10810 [D loss: 0.534226, acc.: 76.56%] [G loss: 1.217483]\n",
      "epoch:11 step:10811 [D loss: 0.534811, acc.: 71.88%] [G loss: 1.398988]\n",
      "epoch:11 step:10812 [D loss: 0.718677, acc.: 60.94%] [G loss: 1.102813]\n",
      "epoch:11 step:10813 [D loss: 0.531882, acc.: 72.66%] [G loss: 1.176821]\n",
      "epoch:11 step:10814 [D loss: 0.593134, acc.: 66.41%] [G loss: 1.147437]\n",
      "epoch:11 step:10815 [D loss: 0.615466, acc.: 71.09%] [G loss: 1.158429]\n",
      "epoch:11 step:10816 [D loss: 0.597144, acc.: 69.53%] [G loss: 1.036628]\n",
      "epoch:11 step:10817 [D loss: 0.607950, acc.: 67.19%] [G loss: 1.029525]\n",
      "epoch:11 step:10818 [D loss: 0.514445, acc.: 78.91%] [G loss: 0.936400]\n",
      "epoch:11 step:10819 [D loss: 0.511940, acc.: 80.47%] [G loss: 1.399295]\n",
      "epoch:11 step:10820 [D loss: 0.597815, acc.: 69.53%] [G loss: 1.281190]\n",
      "epoch:11 step:10821 [D loss: 0.544149, acc.: 74.22%] [G loss: 1.129787]\n",
      "epoch:11 step:10822 [D loss: 0.694828, acc.: 54.69%] [G loss: 1.216982]\n",
      "epoch:11 step:10823 [D loss: 0.610260, acc.: 69.53%] [G loss: 1.542248]\n",
      "epoch:11 step:10824 [D loss: 0.534891, acc.: 71.88%] [G loss: 1.286933]\n",
      "epoch:11 step:10825 [D loss: 0.673513, acc.: 56.25%] [G loss: 1.081620]\n",
      "epoch:11 step:10826 [D loss: 0.680125, acc.: 58.59%] [G loss: 1.139010]\n",
      "epoch:11 step:10827 [D loss: 0.679221, acc.: 57.03%] [G loss: 0.957500]\n",
      "epoch:11 step:10828 [D loss: 0.759043, acc.: 51.56%] [G loss: 1.054977]\n",
      "epoch:11 step:10829 [D loss: 0.612737, acc.: 66.41%] [G loss: 0.965436]\n",
      "epoch:11 step:10830 [D loss: 0.508804, acc.: 77.34%] [G loss: 1.399486]\n",
      "epoch:11 step:10831 [D loss: 0.639490, acc.: 63.28%] [G loss: 1.145805]\n",
      "epoch:11 step:10832 [D loss: 0.649286, acc.: 63.28%] [G loss: 1.188757]\n",
      "epoch:11 step:10833 [D loss: 0.722789, acc.: 53.12%] [G loss: 1.160365]\n",
      "epoch:11 step:10834 [D loss: 0.611467, acc.: 64.84%] [G loss: 1.133214]\n",
      "epoch:11 step:10835 [D loss: 0.620565, acc.: 70.31%] [G loss: 1.282471]\n",
      "epoch:11 step:10836 [D loss: 0.567601, acc.: 69.53%] [G loss: 1.281303]\n",
      "epoch:11 step:10837 [D loss: 0.663146, acc.: 65.62%] [G loss: 1.238612]\n",
      "epoch:11 step:10838 [D loss: 0.545149, acc.: 70.31%] [G loss: 1.279480]\n",
      "epoch:11 step:10839 [D loss: 0.719762, acc.: 57.03%] [G loss: 0.965391]\n",
      "epoch:11 step:10840 [D loss: 0.712155, acc.: 56.25%] [G loss: 1.077093]\n",
      "epoch:11 step:10841 [D loss: 0.688501, acc.: 60.94%] [G loss: 0.988535]\n",
      "epoch:11 step:10842 [D loss: 0.507193, acc.: 78.12%] [G loss: 1.229953]\n",
      "epoch:11 step:10843 [D loss: 0.543573, acc.: 72.66%] [G loss: 1.142667]\n",
      "epoch:11 step:10844 [D loss: 0.608392, acc.: 65.62%] [G loss: 1.170260]\n",
      "epoch:11 step:10845 [D loss: 0.570318, acc.: 69.53%] [G loss: 1.188955]\n",
      "epoch:11 step:10846 [D loss: 0.675371, acc.: 63.28%] [G loss: 1.108553]\n",
      "epoch:11 step:10847 [D loss: 0.641904, acc.: 60.16%] [G loss: 1.222750]\n",
      "epoch:11 step:10848 [D loss: 0.569206, acc.: 71.88%] [G loss: 1.061541]\n",
      "epoch:11 step:10849 [D loss: 0.521291, acc.: 75.78%] [G loss: 1.129848]\n",
      "epoch:11 step:10850 [D loss: 0.627288, acc.: 60.94%] [G loss: 1.223002]\n",
      "epoch:11 step:10851 [D loss: 0.567477, acc.: 71.88%] [G loss: 1.312341]\n",
      "epoch:11 step:10852 [D loss: 0.554539, acc.: 73.44%] [G loss: 1.180986]\n",
      "epoch:11 step:10853 [D loss: 0.540362, acc.: 74.22%] [G loss: 1.066336]\n",
      "epoch:11 step:10854 [D loss: 0.666960, acc.: 63.28%] [G loss: 1.043093]\n",
      "epoch:11 step:10855 [D loss: 0.694230, acc.: 55.47%] [G loss: 1.001708]\n",
      "epoch:11 step:10856 [D loss: 0.594378, acc.: 71.88%] [G loss: 1.147372]\n",
      "epoch:11 step:10857 [D loss: 0.582564, acc.: 68.75%] [G loss: 0.991448]\n",
      "epoch:11 step:10858 [D loss: 0.659940, acc.: 62.50%] [G loss: 1.282570]\n",
      "epoch:11 step:10859 [D loss: 0.628424, acc.: 62.50%] [G loss: 0.940989]\n",
      "epoch:11 step:10860 [D loss: 0.747526, acc.: 57.03%] [G loss: 1.114589]\n",
      "epoch:11 step:10861 [D loss: 0.519523, acc.: 77.34%] [G loss: 1.008062]\n",
      "epoch:11 step:10862 [D loss: 0.577577, acc.: 74.22%] [G loss: 1.149301]\n",
      "epoch:11 step:10863 [D loss: 0.510699, acc.: 79.69%] [G loss: 1.113681]\n",
      "epoch:11 step:10864 [D loss: 0.643567, acc.: 60.16%] [G loss: 0.991905]\n",
      "epoch:11 step:10865 [D loss: 0.590353, acc.: 69.53%] [G loss: 1.281708]\n",
      "epoch:11 step:10866 [D loss: 0.468175, acc.: 80.47%] [G loss: 1.262995]\n",
      "epoch:11 step:10867 [D loss: 0.506987, acc.: 76.56%] [G loss: 1.303337]\n",
      "epoch:11 step:10868 [D loss: 0.543472, acc.: 71.88%] [G loss: 1.148716]\n",
      "epoch:11 step:10869 [D loss: 0.605738, acc.: 66.41%] [G loss: 1.069839]\n",
      "epoch:11 step:10870 [D loss: 0.558981, acc.: 73.44%] [G loss: 1.178522]\n",
      "epoch:11 step:10871 [D loss: 0.519269, acc.: 75.00%] [G loss: 1.291960]\n",
      "epoch:11 step:10872 [D loss: 0.618062, acc.: 64.06%] [G loss: 1.286807]\n",
      "epoch:11 step:10873 [D loss: 0.508717, acc.: 75.00%] [G loss: 1.282354]\n",
      "epoch:11 step:10874 [D loss: 0.573412, acc.: 69.53%] [G loss: 1.261196]\n",
      "epoch:11 step:10875 [D loss: 0.602256, acc.: 70.31%] [G loss: 1.232762]\n",
      "epoch:11 step:10876 [D loss: 0.528455, acc.: 76.56%] [G loss: 1.227724]\n",
      "epoch:11 step:10877 [D loss: 0.533604, acc.: 72.66%] [G loss: 1.187340]\n",
      "epoch:11 step:10878 [D loss: 0.551991, acc.: 71.88%] [G loss: 1.088309]\n",
      "epoch:11 step:10879 [D loss: 0.588443, acc.: 68.75%] [G loss: 0.957683]\n",
      "epoch:11 step:10880 [D loss: 0.620150, acc.: 66.41%] [G loss: 1.444803]\n",
      "epoch:11 step:10881 [D loss: 0.687525, acc.: 61.72%] [G loss: 1.314063]\n",
      "epoch:11 step:10882 [D loss: 0.612523, acc.: 61.72%] [G loss: 1.147929]\n",
      "epoch:11 step:10883 [D loss: 0.533293, acc.: 72.66%] [G loss: 1.130299]\n",
      "epoch:11 step:10884 [D loss: 0.601725, acc.: 66.41%] [G loss: 1.247558]\n",
      "epoch:11 step:10885 [D loss: 0.595806, acc.: 67.19%] [G loss: 1.148375]\n",
      "epoch:11 step:10886 [D loss: 0.638106, acc.: 67.19%] [G loss: 1.143763]\n",
      "epoch:11 step:10887 [D loss: 0.631692, acc.: 67.97%] [G loss: 1.235002]\n",
      "epoch:11 step:10888 [D loss: 0.610696, acc.: 66.41%] [G loss: 1.114965]\n",
      "epoch:11 step:10889 [D loss: 0.491704, acc.: 82.81%] [G loss: 1.226243]\n",
      "epoch:11 step:10890 [D loss: 0.554872, acc.: 69.53%] [G loss: 1.250612]\n",
      "epoch:11 step:10891 [D loss: 0.629717, acc.: 67.19%] [G loss: 1.314017]\n",
      "epoch:11 step:10892 [D loss: 0.693051, acc.: 61.72%] [G loss: 1.149688]\n",
      "epoch:11 step:10893 [D loss: 0.515818, acc.: 75.00%] [G loss: 1.216318]\n",
      "epoch:11 step:10894 [D loss: 0.488905, acc.: 80.47%] [G loss: 1.257483]\n",
      "epoch:11 step:10895 [D loss: 0.663113, acc.: 60.94%] [G loss: 1.060626]\n",
      "epoch:11 step:10896 [D loss: 0.605031, acc.: 66.41%] [G loss: 0.990909]\n",
      "epoch:11 step:10897 [D loss: 0.571321, acc.: 71.88%] [G loss: 1.257213]\n",
      "epoch:11 step:10898 [D loss: 0.620839, acc.: 72.66%] [G loss: 1.238456]\n",
      "epoch:11 step:10899 [D loss: 0.626859, acc.: 64.84%] [G loss: 1.111082]\n",
      "epoch:11 step:10900 [D loss: 0.657951, acc.: 64.84%] [G loss: 1.054129]\n",
      "epoch:11 step:10901 [D loss: 0.511514, acc.: 75.78%] [G loss: 1.299824]\n",
      "epoch:11 step:10902 [D loss: 0.657875, acc.: 63.28%] [G loss: 1.303095]\n",
      "epoch:11 step:10903 [D loss: 0.551396, acc.: 71.88%] [G loss: 1.151082]\n",
      "epoch:11 step:10904 [D loss: 0.609544, acc.: 68.75%] [G loss: 1.331614]\n",
      "epoch:11 step:10905 [D loss: 0.563651, acc.: 75.00%] [G loss: 1.364822]\n",
      "epoch:11 step:10906 [D loss: 0.560585, acc.: 66.41%] [G loss: 1.324908]\n",
      "epoch:11 step:10907 [D loss: 0.536739, acc.: 76.56%] [G loss: 1.252135]\n",
      "epoch:11 step:10908 [D loss: 0.633887, acc.: 61.72%] [G loss: 1.220873]\n",
      "epoch:11 step:10909 [D loss: 0.562967, acc.: 71.09%] [G loss: 1.103825]\n",
      "epoch:11 step:10910 [D loss: 0.666733, acc.: 63.28%] [G loss: 0.988395]\n",
      "epoch:11 step:10911 [D loss: 0.585057, acc.: 71.88%] [G loss: 1.266536]\n",
      "epoch:11 step:10912 [D loss: 0.618869, acc.: 67.97%] [G loss: 0.998906]\n",
      "epoch:11 step:10913 [D loss: 0.635087, acc.: 64.84%] [G loss: 1.425129]\n",
      "epoch:11 step:10914 [D loss: 0.583434, acc.: 71.09%] [G loss: 1.183512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10915 [D loss: 0.617871, acc.: 65.62%] [G loss: 1.038737]\n",
      "epoch:11 step:10916 [D loss: 0.529869, acc.: 74.22%] [G loss: 1.306954]\n",
      "epoch:11 step:10917 [D loss: 0.580038, acc.: 69.53%] [G loss: 1.073656]\n",
      "epoch:11 step:10918 [D loss: 0.621700, acc.: 64.84%] [G loss: 1.305602]\n",
      "epoch:11 step:10919 [D loss: 0.544447, acc.: 72.66%] [G loss: 1.250030]\n",
      "epoch:11 step:10920 [D loss: 0.701439, acc.: 59.38%] [G loss: 1.168380]\n",
      "epoch:11 step:10921 [D loss: 0.608457, acc.: 64.84%] [G loss: 1.179429]\n",
      "epoch:11 step:10922 [D loss: 0.675434, acc.: 62.50%] [G loss: 1.077149]\n",
      "epoch:11 step:10923 [D loss: 0.650622, acc.: 61.72%] [G loss: 1.213804]\n",
      "epoch:11 step:10924 [D loss: 0.730529, acc.: 50.78%] [G loss: 1.402104]\n",
      "epoch:11 step:10925 [D loss: 0.479618, acc.: 82.81%] [G loss: 1.337932]\n",
      "epoch:11 step:10926 [D loss: 0.693875, acc.: 62.50%] [G loss: 1.126384]\n",
      "epoch:11 step:10927 [D loss: 0.709025, acc.: 55.47%] [G loss: 1.164103]\n",
      "epoch:11 step:10928 [D loss: 0.727529, acc.: 55.47%] [G loss: 1.154839]\n",
      "epoch:11 step:10929 [D loss: 0.672943, acc.: 60.94%] [G loss: 1.189322]\n",
      "epoch:11 step:10930 [D loss: 0.595257, acc.: 70.31%] [G loss: 1.198990]\n",
      "epoch:11 step:10931 [D loss: 0.631657, acc.: 66.41%] [G loss: 1.089973]\n",
      "epoch:11 step:10932 [D loss: 0.455950, acc.: 82.81%] [G loss: 1.113908]\n",
      "epoch:11 step:10933 [D loss: 0.651172, acc.: 60.16%] [G loss: 1.054934]\n",
      "epoch:11 step:10934 [D loss: 0.479341, acc.: 82.03%] [G loss: 1.031934]\n",
      "epoch:11 step:10935 [D loss: 0.546317, acc.: 75.78%] [G loss: 1.396819]\n",
      "epoch:11 step:10936 [D loss: 0.642019, acc.: 64.06%] [G loss: 1.152699]\n",
      "epoch:11 step:10937 [D loss: 0.626755, acc.: 64.06%] [G loss: 0.987560]\n",
      "epoch:11 step:10938 [D loss: 0.583654, acc.: 68.75%] [G loss: 1.185281]\n",
      "epoch:11 step:10939 [D loss: 0.485990, acc.: 75.78%] [G loss: 1.392094]\n",
      "epoch:11 step:10940 [D loss: 0.608156, acc.: 68.75%] [G loss: 1.056470]\n",
      "epoch:11 step:10941 [D loss: 0.601514, acc.: 71.09%] [G loss: 1.372002]\n",
      "epoch:11 step:10942 [D loss: 0.673570, acc.: 56.25%] [G loss: 1.120462]\n",
      "epoch:11 step:10943 [D loss: 0.580867, acc.: 71.88%] [G loss: 1.078726]\n",
      "epoch:11 step:10944 [D loss: 0.711563, acc.: 60.16%] [G loss: 1.106651]\n",
      "epoch:11 step:10945 [D loss: 0.693322, acc.: 58.59%] [G loss: 1.027115]\n",
      "epoch:11 step:10946 [D loss: 0.674803, acc.: 57.03%] [G loss: 1.080875]\n",
      "epoch:11 step:10947 [D loss: 0.577924, acc.: 67.97%] [G loss: 1.263291]\n",
      "epoch:11 step:10948 [D loss: 0.625325, acc.: 69.53%] [G loss: 1.340801]\n",
      "epoch:11 step:10949 [D loss: 0.550940, acc.: 71.88%] [G loss: 1.083963]\n",
      "epoch:11 step:10950 [D loss: 0.670064, acc.: 60.16%] [G loss: 1.127642]\n",
      "epoch:11 step:10951 [D loss: 0.575808, acc.: 69.53%] [G loss: 0.971721]\n",
      "epoch:11 step:10952 [D loss: 0.560973, acc.: 73.44%] [G loss: 1.226861]\n",
      "epoch:11 step:10953 [D loss: 0.542071, acc.: 75.00%] [G loss: 1.178576]\n",
      "epoch:11 step:10954 [D loss: 0.524444, acc.: 78.12%] [G loss: 1.124274]\n",
      "epoch:11 step:10955 [D loss: 0.615637, acc.: 64.06%] [G loss: 1.234477]\n",
      "epoch:11 step:10956 [D loss: 0.601407, acc.: 64.06%] [G loss: 1.354734]\n",
      "epoch:11 step:10957 [D loss: 0.529561, acc.: 78.12%] [G loss: 1.330888]\n",
      "epoch:11 step:10958 [D loss: 0.668564, acc.: 61.72%] [G loss: 1.123247]\n",
      "epoch:11 step:10959 [D loss: 0.590248, acc.: 69.53%] [G loss: 1.339785]\n",
      "epoch:11 step:10960 [D loss: 0.652899, acc.: 65.62%] [G loss: 1.285021]\n",
      "epoch:11 step:10961 [D loss: 0.454092, acc.: 86.72%] [G loss: 1.569653]\n",
      "epoch:11 step:10962 [D loss: 0.723925, acc.: 55.47%] [G loss: 1.157330]\n",
      "epoch:11 step:10963 [D loss: 0.533958, acc.: 71.88%] [G loss: 1.276407]\n",
      "epoch:11 step:10964 [D loss: 0.746953, acc.: 54.69%] [G loss: 1.072411]\n",
      "epoch:11 step:10965 [D loss: 0.654533, acc.: 58.59%] [G loss: 1.219974]\n",
      "epoch:11 step:10966 [D loss: 0.614629, acc.: 64.06%] [G loss: 1.089169]\n",
      "epoch:11 step:10967 [D loss: 0.566583, acc.: 72.66%] [G loss: 1.114313]\n",
      "epoch:11 step:10968 [D loss: 0.535209, acc.: 77.34%] [G loss: 1.219999]\n",
      "epoch:11 step:10969 [D loss: 0.740540, acc.: 53.91%] [G loss: 1.096312]\n",
      "epoch:11 step:10970 [D loss: 0.661132, acc.: 62.50%] [G loss: 1.260976]\n",
      "epoch:11 step:10971 [D loss: 0.524905, acc.: 76.56%] [G loss: 1.319562]\n",
      "epoch:11 step:10972 [D loss: 0.605861, acc.: 69.53%] [G loss: 1.372628]\n",
      "epoch:11 step:10973 [D loss: 0.522233, acc.: 75.00%] [G loss: 1.415385]\n",
      "epoch:11 step:10974 [D loss: 0.646563, acc.: 63.28%] [G loss: 1.259508]\n",
      "epoch:11 step:10975 [D loss: 0.685834, acc.: 58.59%] [G loss: 0.999508]\n",
      "epoch:11 step:10976 [D loss: 0.583741, acc.: 70.31%] [G loss: 1.344337]\n",
      "epoch:11 step:10977 [D loss: 0.572797, acc.: 70.31%] [G loss: 1.061638]\n",
      "epoch:11 step:10978 [D loss: 0.610901, acc.: 71.09%] [G loss: 1.293378]\n",
      "epoch:11 step:10979 [D loss: 0.518661, acc.: 77.34%] [G loss: 1.185650]\n",
      "epoch:11 step:10980 [D loss: 0.617125, acc.: 60.94%] [G loss: 1.164587]\n",
      "epoch:11 step:10981 [D loss: 0.455524, acc.: 81.25%] [G loss: 1.388146]\n",
      "epoch:11 step:10982 [D loss: 0.500034, acc.: 76.56%] [G loss: 1.303734]\n",
      "epoch:11 step:10983 [D loss: 0.591454, acc.: 67.19%] [G loss: 1.225857]\n",
      "epoch:11 step:10984 [D loss: 0.641357, acc.: 53.12%] [G loss: 1.070605]\n",
      "epoch:11 step:10985 [D loss: 0.611403, acc.: 68.75%] [G loss: 1.130343]\n",
      "epoch:11 step:10986 [D loss: 0.603871, acc.: 66.41%] [G loss: 1.209278]\n",
      "epoch:11 step:10987 [D loss: 0.714533, acc.: 54.69%] [G loss: 1.236535]\n",
      "epoch:11 step:10988 [D loss: 0.716512, acc.: 56.25%] [G loss: 1.081466]\n",
      "epoch:11 step:10989 [D loss: 0.607635, acc.: 66.41%] [G loss: 1.313554]\n",
      "epoch:11 step:10990 [D loss: 0.588841, acc.: 70.31%] [G loss: 1.271401]\n",
      "epoch:11 step:10991 [D loss: 0.605128, acc.: 66.41%] [G loss: 1.097781]\n",
      "epoch:11 step:10992 [D loss: 0.625804, acc.: 67.19%] [G loss: 1.098091]\n",
      "epoch:11 step:10993 [D loss: 0.623488, acc.: 67.19%] [G loss: 1.143219]\n",
      "epoch:11 step:10994 [D loss: 0.563995, acc.: 72.66%] [G loss: 1.202771]\n",
      "epoch:11 step:10995 [D loss: 0.566540, acc.: 73.44%] [G loss: 1.310666]\n",
      "epoch:11 step:10996 [D loss: 0.621422, acc.: 67.19%] [G loss: 1.150065]\n",
      "epoch:11 step:10997 [D loss: 0.532937, acc.: 72.66%] [G loss: 1.352554]\n",
      "epoch:11 step:10998 [D loss: 0.560554, acc.: 70.31%] [G loss: 1.034493]\n",
      "epoch:11 step:10999 [D loss: 0.615319, acc.: 65.62%] [G loss: 1.202013]\n",
      "epoch:11 step:11000 [D loss: 0.642561, acc.: 64.06%] [G loss: 1.245818]\n",
      "##############\n",
      "[2.73118809 2.08944847 1.84897301 3.10776247 0.90935239 5.7230532\n",
      " 2.25580392 3.13075452 4.07907751 7.14868929]\n",
      "##########\n",
      "epoch:11 step:11001 [D loss: 0.682413, acc.: 55.47%] [G loss: 0.942198]\n",
      "epoch:11 step:11002 [D loss: 0.598283, acc.: 65.62%] [G loss: 1.299441]\n",
      "epoch:11 step:11003 [D loss: 0.585801, acc.: 67.97%] [G loss: 1.041721]\n",
      "epoch:11 step:11004 [D loss: 0.498950, acc.: 75.00%] [G loss: 1.245173]\n",
      "epoch:11 step:11005 [D loss: 0.594846, acc.: 67.19%] [G loss: 1.080397]\n",
      "epoch:11 step:11006 [D loss: 0.591818, acc.: 66.41%] [G loss: 1.143831]\n",
      "epoch:11 step:11007 [D loss: 0.619663, acc.: 69.53%] [G loss: 1.242949]\n",
      "epoch:11 step:11008 [D loss: 0.510675, acc.: 78.12%] [G loss: 1.309920]\n",
      "epoch:11 step:11009 [D loss: 0.595960, acc.: 65.62%] [G loss: 1.075549]\n",
      "epoch:11 step:11010 [D loss: 0.685383, acc.: 61.72%] [G loss: 1.079588]\n",
      "epoch:11 step:11011 [D loss: 0.702998, acc.: 56.25%] [G loss: 1.006550]\n",
      "epoch:11 step:11012 [D loss: 0.575013, acc.: 66.41%] [G loss: 1.107335]\n",
      "epoch:11 step:11013 [D loss: 0.662454, acc.: 57.81%] [G loss: 1.086569]\n",
      "epoch:11 step:11014 [D loss: 0.534430, acc.: 76.56%] [G loss: 1.338277]\n",
      "epoch:11 step:11015 [D loss: 0.462241, acc.: 80.47%] [G loss: 1.034450]\n",
      "epoch:11 step:11016 [D loss: 0.677820, acc.: 64.84%] [G loss: 0.963480]\n",
      "epoch:11 step:11017 [D loss: 0.583820, acc.: 71.88%] [G loss: 1.083047]\n",
      "epoch:11 step:11018 [D loss: 0.460072, acc.: 84.38%] [G loss: 1.183425]\n",
      "epoch:11 step:11019 [D loss: 0.631912, acc.: 65.62%] [G loss: 1.266759]\n",
      "epoch:11 step:11020 [D loss: 0.521671, acc.: 74.22%] [G loss: 1.202760]\n",
      "epoch:11 step:11021 [D loss: 0.596901, acc.: 66.41%] [G loss: 1.333506]\n",
      "epoch:11 step:11022 [D loss: 0.655194, acc.: 57.81%] [G loss: 1.109487]\n",
      "epoch:11 step:11023 [D loss: 0.590644, acc.: 68.75%] [G loss: 1.243935]\n",
      "epoch:11 step:11024 [D loss: 0.572595, acc.: 71.09%] [G loss: 1.281402]\n",
      "epoch:11 step:11025 [D loss: 0.552065, acc.: 71.88%] [G loss: 1.170277]\n",
      "epoch:11 step:11026 [D loss: 0.662565, acc.: 62.50%] [G loss: 1.003595]\n",
      "epoch:11 step:11027 [D loss: 0.667806, acc.: 60.94%] [G loss: 1.114773]\n",
      "epoch:11 step:11028 [D loss: 0.561808, acc.: 73.44%] [G loss: 1.380655]\n",
      "epoch:11 step:11029 [D loss: 0.622546, acc.: 64.06%] [G loss: 1.468354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11030 [D loss: 0.605826, acc.: 67.97%] [G loss: 1.185946]\n",
      "epoch:11 step:11031 [D loss: 0.604226, acc.: 64.06%] [G loss: 1.098479]\n",
      "epoch:11 step:11032 [D loss: 0.463171, acc.: 81.25%] [G loss: 1.356007]\n",
      "epoch:11 step:11033 [D loss: 0.715119, acc.: 57.03%] [G loss: 1.052452]\n",
      "epoch:11 step:11034 [D loss: 0.588318, acc.: 69.53%] [G loss: 1.113788]\n",
      "epoch:11 step:11035 [D loss: 0.684244, acc.: 58.59%] [G loss: 1.306973]\n",
      "epoch:11 step:11036 [D loss: 0.589308, acc.: 65.62%] [G loss: 1.141576]\n",
      "epoch:11 step:11037 [D loss: 0.593092, acc.: 71.88%] [G loss: 1.298983]\n",
      "epoch:11 step:11038 [D loss: 0.734791, acc.: 53.91%] [G loss: 0.880289]\n",
      "epoch:11 step:11039 [D loss: 0.651724, acc.: 64.84%] [G loss: 1.272303]\n",
      "epoch:11 step:11040 [D loss: 0.669184, acc.: 59.38%] [G loss: 0.994218]\n",
      "epoch:11 step:11041 [D loss: 0.514743, acc.: 79.69%] [G loss: 0.913373]\n",
      "epoch:11 step:11042 [D loss: 0.596992, acc.: 71.09%] [G loss: 0.893128]\n",
      "epoch:11 step:11043 [D loss: 0.577600, acc.: 71.09%] [G loss: 1.207203]\n",
      "epoch:11 step:11044 [D loss: 0.532023, acc.: 74.22%] [G loss: 1.267931]\n",
      "epoch:11 step:11045 [D loss: 0.677110, acc.: 61.72%] [G loss: 1.046094]\n",
      "epoch:11 step:11046 [D loss: 0.543520, acc.: 72.66%] [G loss: 1.206267]\n",
      "epoch:11 step:11047 [D loss: 0.597198, acc.: 68.75%] [G loss: 1.265580]\n",
      "epoch:11 step:11048 [D loss: 0.605259, acc.: 71.88%] [G loss: 1.302038]\n",
      "epoch:11 step:11049 [D loss: 0.569642, acc.: 71.09%] [G loss: 1.411467]\n",
      "epoch:11 step:11050 [D loss: 0.727873, acc.: 53.12%] [G loss: 1.234212]\n",
      "epoch:11 step:11051 [D loss: 0.592144, acc.: 69.53%] [G loss: 1.153052]\n",
      "epoch:11 step:11052 [D loss: 0.572219, acc.: 70.31%] [G loss: 1.347438]\n",
      "epoch:11 step:11053 [D loss: 0.599273, acc.: 69.53%] [G loss: 1.125393]\n",
      "epoch:11 step:11054 [D loss: 0.624135, acc.: 64.06%] [G loss: 1.054889]\n",
      "epoch:11 step:11055 [D loss: 0.577972, acc.: 72.66%] [G loss: 1.215865]\n",
      "epoch:11 step:11056 [D loss: 0.627246, acc.: 66.41%] [G loss: 1.147555]\n",
      "epoch:11 step:11057 [D loss: 0.631613, acc.: 63.28%] [G loss: 1.358508]\n",
      "epoch:11 step:11058 [D loss: 0.606932, acc.: 68.75%] [G loss: 1.159164]\n",
      "epoch:11 step:11059 [D loss: 0.555353, acc.: 72.66%] [G loss: 1.410173]\n",
      "epoch:11 step:11060 [D loss: 0.628302, acc.: 65.62%] [G loss: 0.982127]\n",
      "epoch:11 step:11061 [D loss: 0.546178, acc.: 71.09%] [G loss: 1.055782]\n",
      "epoch:11 step:11062 [D loss: 0.632002, acc.: 64.84%] [G loss: 1.053062]\n",
      "epoch:11 step:11063 [D loss: 0.599583, acc.: 64.06%] [G loss: 1.246883]\n",
      "epoch:11 step:11064 [D loss: 0.591838, acc.: 68.75%] [G loss: 1.202189]\n",
      "epoch:11 step:11065 [D loss: 0.608113, acc.: 70.31%] [G loss: 1.150221]\n",
      "epoch:11 step:11066 [D loss: 0.551193, acc.: 72.66%] [G loss: 1.310710]\n",
      "epoch:11 step:11067 [D loss: 0.551519, acc.: 72.66%] [G loss: 1.311167]\n",
      "epoch:11 step:11068 [D loss: 0.682134, acc.: 57.03%] [G loss: 1.118445]\n",
      "epoch:11 step:11069 [D loss: 0.680880, acc.: 64.06%] [G loss: 1.302206]\n",
      "epoch:11 step:11070 [D loss: 0.613795, acc.: 67.97%] [G loss: 1.209470]\n",
      "epoch:11 step:11071 [D loss: 0.496868, acc.: 74.22%] [G loss: 1.387518]\n",
      "epoch:11 step:11072 [D loss: 0.640628, acc.: 64.06%] [G loss: 1.236004]\n",
      "epoch:11 step:11073 [D loss: 0.509269, acc.: 76.56%] [G loss: 1.272316]\n",
      "epoch:11 step:11074 [D loss: 0.631606, acc.: 64.06%] [G loss: 1.044144]\n",
      "epoch:11 step:11075 [D loss: 0.605670, acc.: 62.50%] [G loss: 1.141834]\n",
      "epoch:11 step:11076 [D loss: 0.501628, acc.: 75.00%] [G loss: 1.269490]\n",
      "epoch:11 step:11077 [D loss: 0.588944, acc.: 71.88%] [G loss: 1.260462]\n",
      "epoch:11 step:11078 [D loss: 0.658875, acc.: 61.72%] [G loss: 1.241608]\n",
      "epoch:11 step:11079 [D loss: 0.632692, acc.: 63.28%] [G loss: 1.263542]\n",
      "epoch:11 step:11080 [D loss: 0.663719, acc.: 57.03%] [G loss: 1.091902]\n",
      "epoch:11 step:11081 [D loss: 0.678676, acc.: 56.25%] [G loss: 1.257267]\n",
      "epoch:11 step:11082 [D loss: 0.558882, acc.: 71.09%] [G loss: 1.390994]\n",
      "epoch:11 step:11083 [D loss: 0.549133, acc.: 68.75%] [G loss: 1.304088]\n",
      "epoch:11 step:11084 [D loss: 0.721207, acc.: 60.16%] [G loss: 1.254434]\n",
      "epoch:11 step:11085 [D loss: 0.732152, acc.: 49.22%] [G loss: 1.086552]\n",
      "epoch:11 step:11086 [D loss: 0.562491, acc.: 69.53%] [G loss: 1.006765]\n",
      "epoch:11 step:11087 [D loss: 0.720821, acc.: 58.59%] [G loss: 1.090777]\n",
      "epoch:11 step:11088 [D loss: 0.709142, acc.: 62.50%] [G loss: 1.223979]\n",
      "epoch:11 step:11089 [D loss: 0.533204, acc.: 77.34%] [G loss: 1.206491]\n",
      "epoch:11 step:11090 [D loss: 0.560191, acc.: 67.19%] [G loss: 1.115487]\n",
      "epoch:11 step:11091 [D loss: 0.664379, acc.: 60.16%] [G loss: 1.449352]\n",
      "epoch:11 step:11092 [D loss: 0.669351, acc.: 63.28%] [G loss: 1.029506]\n",
      "epoch:11 step:11093 [D loss: 0.563004, acc.: 72.66%] [G loss: 1.232100]\n",
      "epoch:11 step:11094 [D loss: 0.636302, acc.: 63.28%] [G loss: 1.169181]\n",
      "epoch:11 step:11095 [D loss: 0.622122, acc.: 71.09%] [G loss: 1.117678]\n",
      "epoch:11 step:11096 [D loss: 0.631273, acc.: 65.62%] [G loss: 0.984807]\n",
      "epoch:11 step:11097 [D loss: 0.493538, acc.: 82.03%] [G loss: 1.128821]\n",
      "epoch:11 step:11098 [D loss: 0.490390, acc.: 78.91%] [G loss: 1.280251]\n",
      "epoch:11 step:11099 [D loss: 0.524857, acc.: 75.00%] [G loss: 1.306324]\n",
      "epoch:11 step:11100 [D loss: 0.602022, acc.: 69.53%] [G loss: 0.941440]\n",
      "epoch:11 step:11101 [D loss: 0.555484, acc.: 76.56%] [G loss: 1.095544]\n",
      "epoch:11 step:11102 [D loss: 0.656832, acc.: 63.28%] [G loss: 1.062704]\n",
      "epoch:11 step:11103 [D loss: 0.603660, acc.: 64.84%] [G loss: 1.192266]\n",
      "epoch:11 step:11104 [D loss: 0.644624, acc.: 62.50%] [G loss: 1.205801]\n",
      "epoch:11 step:11105 [D loss: 0.577795, acc.: 75.78%] [G loss: 1.272127]\n",
      "epoch:11 step:11106 [D loss: 0.607408, acc.: 63.28%] [G loss: 1.116286]\n",
      "epoch:11 step:11107 [D loss: 0.602243, acc.: 64.84%] [G loss: 1.101446]\n",
      "epoch:11 step:11108 [D loss: 0.619433, acc.: 67.19%] [G loss: 1.087315]\n",
      "epoch:11 step:11109 [D loss: 0.667493, acc.: 60.94%] [G loss: 0.901767]\n",
      "epoch:11 step:11110 [D loss: 0.585926, acc.: 71.09%] [G loss: 1.039569]\n",
      "epoch:11 step:11111 [D loss: 0.555572, acc.: 75.78%] [G loss: 0.873278]\n",
      "epoch:11 step:11112 [D loss: 0.680194, acc.: 55.47%] [G loss: 1.107615]\n",
      "epoch:11 step:11113 [D loss: 0.587212, acc.: 69.53%] [G loss: 1.315120]\n",
      "epoch:11 step:11114 [D loss: 0.645995, acc.: 62.50%] [G loss: 1.198009]\n",
      "epoch:11 step:11115 [D loss: 0.580507, acc.: 67.19%] [G loss: 1.205671]\n",
      "epoch:11 step:11116 [D loss: 0.579796, acc.: 68.75%] [G loss: 1.156143]\n",
      "epoch:11 step:11117 [D loss: 0.635374, acc.: 66.41%] [G loss: 1.215951]\n",
      "epoch:11 step:11118 [D loss: 0.686999, acc.: 56.25%] [G loss: 0.955779]\n",
      "epoch:11 step:11119 [D loss: 0.518301, acc.: 73.44%] [G loss: 1.193443]\n",
      "epoch:11 step:11120 [D loss: 0.474973, acc.: 82.81%] [G loss: 1.007290]\n",
      "epoch:11 step:11121 [D loss: 0.540218, acc.: 75.00%] [G loss: 1.041440]\n",
      "epoch:11 step:11122 [D loss: 0.623068, acc.: 64.84%] [G loss: 1.039431]\n",
      "epoch:11 step:11123 [D loss: 0.518987, acc.: 75.78%] [G loss: 1.137747]\n",
      "epoch:11 step:11124 [D loss: 0.547744, acc.: 73.44%] [G loss: 1.120435]\n",
      "epoch:11 step:11125 [D loss: 0.601864, acc.: 64.84%] [G loss: 0.978697]\n",
      "epoch:11 step:11126 [D loss: 0.665481, acc.: 57.03%] [G loss: 1.149445]\n",
      "epoch:11 step:11127 [D loss: 0.551424, acc.: 71.88%] [G loss: 1.078651]\n",
      "epoch:11 step:11128 [D loss: 0.653727, acc.: 61.72%] [G loss: 1.171510]\n",
      "epoch:11 step:11129 [D loss: 0.638485, acc.: 64.84%] [G loss: 1.312821]\n",
      "epoch:11 step:11130 [D loss: 0.558182, acc.: 69.53%] [G loss: 1.407219]\n",
      "epoch:11 step:11131 [D loss: 0.574970, acc.: 70.31%] [G loss: 1.460905]\n",
      "epoch:11 step:11132 [D loss: 0.563660, acc.: 70.31%] [G loss: 1.056092]\n",
      "epoch:11 step:11133 [D loss: 0.547545, acc.: 74.22%] [G loss: 1.002096]\n",
      "epoch:11 step:11134 [D loss: 0.607374, acc.: 60.16%] [G loss: 1.219128]\n",
      "epoch:11 step:11135 [D loss: 0.701133, acc.: 56.25%] [G loss: 1.054959]\n",
      "epoch:11 step:11136 [D loss: 0.567324, acc.: 72.66%] [G loss: 1.004396]\n",
      "epoch:11 step:11137 [D loss: 0.669511, acc.: 62.50%] [G loss: 1.515724]\n",
      "epoch:11 step:11138 [D loss: 0.615837, acc.: 66.41%] [G loss: 1.345470]\n",
      "epoch:11 step:11139 [D loss: 0.762405, acc.: 49.22%] [G loss: 1.073871]\n",
      "epoch:11 step:11140 [D loss: 0.675763, acc.: 60.94%] [G loss: 1.284405]\n",
      "epoch:11 step:11141 [D loss: 0.612676, acc.: 64.84%] [G loss: 1.206214]\n",
      "epoch:11 step:11142 [D loss: 0.618478, acc.: 65.62%] [G loss: 1.160997]\n",
      "epoch:11 step:11143 [D loss: 0.633737, acc.: 60.16%] [G loss: 1.252250]\n",
      "epoch:11 step:11144 [D loss: 0.632689, acc.: 60.94%] [G loss: 1.078662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11145 [D loss: 0.541711, acc.: 72.66%] [G loss: 1.219008]\n",
      "epoch:11 step:11146 [D loss: 0.646020, acc.: 61.72%] [G loss: 0.968618]\n",
      "epoch:11 step:11147 [D loss: 0.521328, acc.: 77.34%] [G loss: 0.924739]\n",
      "epoch:11 step:11148 [D loss: 0.558781, acc.: 76.56%] [G loss: 1.092272]\n",
      "epoch:11 step:11149 [D loss: 0.627241, acc.: 63.28%] [G loss: 1.112702]\n",
      "epoch:11 step:11150 [D loss: 0.594782, acc.: 67.97%] [G loss: 1.442633]\n",
      "epoch:11 step:11151 [D loss: 0.554632, acc.: 68.75%] [G loss: 1.372403]\n",
      "epoch:11 step:11152 [D loss: 0.640476, acc.: 59.38%] [G loss: 1.037152]\n",
      "epoch:11 step:11153 [D loss: 0.705401, acc.: 58.59%] [G loss: 1.121621]\n",
      "epoch:11 step:11154 [D loss: 0.616064, acc.: 64.84%] [G loss: 1.186244]\n",
      "epoch:11 step:11155 [D loss: 0.525391, acc.: 75.00%] [G loss: 1.084186]\n",
      "epoch:11 step:11156 [D loss: 0.734413, acc.: 52.34%] [G loss: 0.960007]\n",
      "epoch:11 step:11157 [D loss: 0.537744, acc.: 70.31%] [G loss: 1.214934]\n",
      "epoch:11 step:11158 [D loss: 0.710496, acc.: 56.25%] [G loss: 1.016043]\n",
      "epoch:11 step:11159 [D loss: 0.505585, acc.: 74.22%] [G loss: 1.232819]\n",
      "epoch:11 step:11160 [D loss: 0.480701, acc.: 78.91%] [G loss: 1.010686]\n",
      "epoch:11 step:11161 [D loss: 0.644364, acc.: 63.28%] [G loss: 1.217761]\n",
      "epoch:11 step:11162 [D loss: 0.615069, acc.: 68.75%] [G loss: 1.149502]\n",
      "epoch:11 step:11163 [D loss: 0.554640, acc.: 73.44%] [G loss: 1.207423]\n",
      "epoch:11 step:11164 [D loss: 0.659602, acc.: 62.50%] [G loss: 1.193584]\n",
      "epoch:11 step:11165 [D loss: 0.551472, acc.: 71.88%] [G loss: 1.590807]\n",
      "epoch:11 step:11166 [D loss: 0.603700, acc.: 64.06%] [G loss: 1.139152]\n",
      "epoch:11 step:11167 [D loss: 0.704196, acc.: 53.12%] [G loss: 1.071262]\n",
      "epoch:11 step:11168 [D loss: 0.618752, acc.: 64.06%] [G loss: 1.349877]\n",
      "epoch:11 step:11169 [D loss: 0.734417, acc.: 57.03%] [G loss: 1.084060]\n",
      "epoch:11 step:11170 [D loss: 0.606165, acc.: 65.62%] [G loss: 1.395099]\n",
      "epoch:11 step:11171 [D loss: 0.628671, acc.: 66.41%] [G loss: 1.258205]\n",
      "epoch:11 step:11172 [D loss: 0.530058, acc.: 75.00%] [G loss: 1.187477]\n",
      "epoch:11 step:11173 [D loss: 0.488295, acc.: 81.25%] [G loss: 1.540323]\n",
      "epoch:11 step:11174 [D loss: 0.692668, acc.: 57.03%] [G loss: 1.055768]\n",
      "epoch:11 step:11175 [D loss: 0.495906, acc.: 74.22%] [G loss: 1.431334]\n",
      "epoch:11 step:11176 [D loss: 0.540733, acc.: 73.44%] [G loss: 1.249278]\n",
      "epoch:11 step:11177 [D loss: 0.589704, acc.: 71.09%] [G loss: 1.415866]\n",
      "epoch:11 step:11178 [D loss: 0.590793, acc.: 65.62%] [G loss: 1.219207]\n",
      "epoch:11 step:11179 [D loss: 0.628256, acc.: 68.75%] [G loss: 1.311462]\n",
      "epoch:11 step:11180 [D loss: 0.497267, acc.: 79.69%] [G loss: 1.471869]\n",
      "epoch:11 step:11181 [D loss: 0.537372, acc.: 75.78%] [G loss: 1.310435]\n",
      "epoch:11 step:11182 [D loss: 0.725110, acc.: 57.03%] [G loss: 1.220413]\n",
      "epoch:11 step:11183 [D loss: 0.650912, acc.: 62.50%] [G loss: 1.200356]\n",
      "epoch:11 step:11184 [D loss: 0.563734, acc.: 73.44%] [G loss: 1.287513]\n",
      "epoch:11 step:11185 [D loss: 0.602123, acc.: 71.88%] [G loss: 1.280756]\n",
      "epoch:11 step:11186 [D loss: 0.621708, acc.: 67.19%] [G loss: 1.248499]\n",
      "epoch:11 step:11187 [D loss: 0.605578, acc.: 66.41%] [G loss: 1.057052]\n",
      "epoch:11 step:11188 [D loss: 0.686103, acc.: 58.59%] [G loss: 1.327222]\n",
      "epoch:11 step:11189 [D loss: 0.541496, acc.: 75.00%] [G loss: 1.204163]\n",
      "epoch:11 step:11190 [D loss: 0.490357, acc.: 75.78%] [G loss: 1.322484]\n",
      "epoch:11 step:11191 [D loss: 0.597678, acc.: 65.62%] [G loss: 1.338992]\n",
      "epoch:11 step:11192 [D loss: 0.598068, acc.: 66.41%] [G loss: 1.010492]\n",
      "epoch:11 step:11193 [D loss: 0.546311, acc.: 71.88%] [G loss: 1.064923]\n",
      "epoch:11 step:11194 [D loss: 0.550213, acc.: 74.22%] [G loss: 1.272217]\n",
      "epoch:11 step:11195 [D loss: 0.489399, acc.: 75.78%] [G loss: 1.021181]\n",
      "epoch:11 step:11196 [D loss: 0.548369, acc.: 71.09%] [G loss: 1.221126]\n",
      "epoch:11 step:11197 [D loss: 0.707523, acc.: 53.12%] [G loss: 1.018781]\n",
      "epoch:11 step:11198 [D loss: 0.629488, acc.: 67.19%] [G loss: 1.055255]\n",
      "epoch:11 step:11199 [D loss: 0.536385, acc.: 76.56%] [G loss: 1.138200]\n",
      "epoch:11 step:11200 [D loss: 0.577236, acc.: 71.88%] [G loss: 1.172327]\n",
      "##############\n",
      "[2.67843369 2.17566506 1.97650868 2.78201603 1.10421262 6.47418327\n",
      " 2.25808605 2.92127865 3.91837821 4.59964945]\n",
      "##########\n",
      "epoch:11 step:11201 [D loss: 0.682706, acc.: 63.28%] [G loss: 0.847134]\n",
      "epoch:11 step:11202 [D loss: 0.617600, acc.: 66.41%] [G loss: 1.227869]\n",
      "epoch:11 step:11203 [D loss: 0.624099, acc.: 68.75%] [G loss: 1.163114]\n",
      "epoch:11 step:11204 [D loss: 0.611045, acc.: 72.66%] [G loss: 1.173827]\n",
      "epoch:11 step:11205 [D loss: 0.623042, acc.: 64.84%] [G loss: 1.164425]\n",
      "epoch:11 step:11206 [D loss: 0.526373, acc.: 71.88%] [G loss: 1.100903]\n",
      "epoch:11 step:11207 [D loss: 0.627980, acc.: 67.97%] [G loss: 1.442311]\n",
      "epoch:11 step:11208 [D loss: 0.679556, acc.: 57.03%] [G loss: 1.253300]\n",
      "epoch:11 step:11209 [D loss: 0.625348, acc.: 64.06%] [G loss: 1.153904]\n",
      "epoch:11 step:11210 [D loss: 0.489019, acc.: 79.69%] [G loss: 1.400509]\n",
      "epoch:11 step:11211 [D loss: 0.605434, acc.: 63.28%] [G loss: 1.336238]\n",
      "epoch:11 step:11212 [D loss: 0.603104, acc.: 64.84%] [G loss: 1.143343]\n",
      "epoch:11 step:11213 [D loss: 0.719088, acc.: 57.03%] [G loss: 1.167657]\n",
      "epoch:11 step:11214 [D loss: 0.582293, acc.: 69.53%] [G loss: 1.263016]\n",
      "epoch:11 step:11215 [D loss: 0.788012, acc.: 45.31%] [G loss: 1.033266]\n",
      "epoch:11 step:11216 [D loss: 0.636481, acc.: 63.28%] [G loss: 1.018913]\n",
      "epoch:11 step:11217 [D loss: 0.597736, acc.: 64.84%] [G loss: 1.228004]\n",
      "epoch:11 step:11218 [D loss: 0.472598, acc.: 79.69%] [G loss: 1.403895]\n",
      "epoch:11 step:11219 [D loss: 0.724719, acc.: 52.34%] [G loss: 1.341270]\n",
      "epoch:11 step:11220 [D loss: 0.759938, acc.: 53.91%] [G loss: 1.021834]\n",
      "epoch:11 step:11221 [D loss: 0.627385, acc.: 63.28%] [G loss: 0.996277]\n",
      "epoch:11 step:11222 [D loss: 0.650218, acc.: 58.59%] [G loss: 1.051067]\n",
      "epoch:11 step:11223 [D loss: 0.563075, acc.: 71.88%] [G loss: 1.216087]\n",
      "epoch:11 step:11224 [D loss: 0.528698, acc.: 75.00%] [G loss: 1.059498]\n",
      "epoch:11 step:11225 [D loss: 0.580964, acc.: 67.97%] [G loss: 1.106519]\n",
      "epoch:11 step:11226 [D loss: 0.522866, acc.: 75.78%] [G loss: 1.316976]\n",
      "epoch:11 step:11227 [D loss: 0.715157, acc.: 51.56%] [G loss: 1.063427]\n",
      "epoch:11 step:11228 [D loss: 0.714457, acc.: 55.47%] [G loss: 1.328948]\n",
      "epoch:11 step:11229 [D loss: 0.553366, acc.: 74.22%] [G loss: 1.262042]\n",
      "epoch:11 step:11230 [D loss: 0.699314, acc.: 53.12%] [G loss: 1.110426]\n",
      "epoch:11 step:11231 [D loss: 0.515989, acc.: 74.22%] [G loss: 1.272482]\n",
      "epoch:11 step:11232 [D loss: 0.625279, acc.: 64.06%] [G loss: 1.310261]\n",
      "epoch:11 step:11233 [D loss: 0.648247, acc.: 63.28%] [G loss: 1.299738]\n",
      "epoch:11 step:11234 [D loss: 0.666414, acc.: 61.72%] [G loss: 1.382134]\n",
      "epoch:11 step:11235 [D loss: 0.511748, acc.: 71.88%] [G loss: 1.393914]\n",
      "epoch:11 step:11236 [D loss: 0.600907, acc.: 67.97%] [G loss: 1.241613]\n",
      "epoch:11 step:11237 [D loss: 0.506950, acc.: 77.34%] [G loss: 1.243223]\n",
      "epoch:11 step:11238 [D loss: 0.693032, acc.: 59.38%] [G loss: 1.060276]\n",
      "epoch:11 step:11239 [D loss: 0.505222, acc.: 73.44%] [G loss: 1.169155]\n",
      "epoch:11 step:11240 [D loss: 0.758134, acc.: 52.34%] [G loss: 1.232222]\n",
      "epoch:11 step:11241 [D loss: 0.590792, acc.: 66.41%] [G loss: 1.329960]\n",
      "epoch:11 step:11242 [D loss: 0.631915, acc.: 64.84%] [G loss: 1.246520]\n",
      "epoch:11 step:11243 [D loss: 0.549577, acc.: 70.31%] [G loss: 1.195390]\n",
      "epoch:11 step:11244 [D loss: 0.668621, acc.: 57.81%] [G loss: 1.227552]\n",
      "epoch:12 step:11245 [D loss: 0.608630, acc.: 66.41%] [G loss: 1.268756]\n",
      "epoch:12 step:11246 [D loss: 0.604511, acc.: 62.50%] [G loss: 1.273605]\n",
      "epoch:12 step:11247 [D loss: 0.586906, acc.: 67.97%] [G loss: 1.187162]\n",
      "epoch:12 step:11248 [D loss: 0.564722, acc.: 71.09%] [G loss: 1.531404]\n",
      "epoch:12 step:11249 [D loss: 0.521143, acc.: 78.91%] [G loss: 1.204659]\n",
      "epoch:12 step:11250 [D loss: 0.622541, acc.: 62.50%] [G loss: 1.214412]\n",
      "epoch:12 step:11251 [D loss: 0.686300, acc.: 60.94%] [G loss: 1.047024]\n",
      "epoch:12 step:11252 [D loss: 0.540216, acc.: 73.44%] [G loss: 1.128495]\n",
      "epoch:12 step:11253 [D loss: 0.646134, acc.: 65.62%] [G loss: 1.095408]\n",
      "epoch:12 step:11254 [D loss: 0.606782, acc.: 65.62%] [G loss: 1.104004]\n",
      "epoch:12 step:11255 [D loss: 0.504632, acc.: 75.78%] [G loss: 1.148656]\n",
      "epoch:12 step:11256 [D loss: 0.535303, acc.: 77.34%] [G loss: 1.084084]\n",
      "epoch:12 step:11257 [D loss: 0.566232, acc.: 73.44%] [G loss: 1.109951]\n",
      "epoch:12 step:11258 [D loss: 0.641520, acc.: 62.50%] [G loss: 1.096578]\n",
      "epoch:12 step:11259 [D loss: 0.588382, acc.: 65.62%] [G loss: 0.998932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11260 [D loss: 0.512938, acc.: 76.56%] [G loss: 1.205888]\n",
      "epoch:12 step:11261 [D loss: 0.489168, acc.: 79.69%] [G loss: 1.235955]\n",
      "epoch:12 step:11262 [D loss: 0.515606, acc.: 73.44%] [G loss: 1.185325]\n",
      "epoch:12 step:11263 [D loss: 0.759902, acc.: 56.25%] [G loss: 1.150750]\n",
      "epoch:12 step:11264 [D loss: 0.647166, acc.: 62.50%] [G loss: 0.979826]\n",
      "epoch:12 step:11265 [D loss: 0.747303, acc.: 51.56%] [G loss: 1.187960]\n",
      "epoch:12 step:11266 [D loss: 0.595010, acc.: 67.97%] [G loss: 1.287891]\n",
      "epoch:12 step:11267 [D loss: 0.631164, acc.: 66.41%] [G loss: 1.214038]\n",
      "epoch:12 step:11268 [D loss: 0.491813, acc.: 78.91%] [G loss: 1.243036]\n",
      "epoch:12 step:11269 [D loss: 0.675241, acc.: 58.59%] [G loss: 1.183674]\n",
      "epoch:12 step:11270 [D loss: 0.561080, acc.: 69.53%] [G loss: 1.281058]\n",
      "epoch:12 step:11271 [D loss: 0.572297, acc.: 67.19%] [G loss: 1.082759]\n",
      "epoch:12 step:11272 [D loss: 0.580875, acc.: 69.53%] [G loss: 1.245901]\n",
      "epoch:12 step:11273 [D loss: 0.669765, acc.: 64.06%] [G loss: 1.142923]\n",
      "epoch:12 step:11274 [D loss: 0.603320, acc.: 65.62%] [G loss: 1.166239]\n",
      "epoch:12 step:11275 [D loss: 0.682322, acc.: 55.47%] [G loss: 1.106922]\n",
      "epoch:12 step:11276 [D loss: 0.583774, acc.: 70.31%] [G loss: 1.223025]\n",
      "epoch:12 step:11277 [D loss: 0.480399, acc.: 78.91%] [G loss: 1.385589]\n",
      "epoch:12 step:11278 [D loss: 0.654682, acc.: 62.50%] [G loss: 1.102480]\n",
      "epoch:12 step:11279 [D loss: 0.655074, acc.: 60.94%] [G loss: 1.265270]\n",
      "epoch:12 step:11280 [D loss: 0.556638, acc.: 75.00%] [G loss: 1.155358]\n",
      "epoch:12 step:11281 [D loss: 0.530567, acc.: 75.78%] [G loss: 1.100275]\n",
      "epoch:12 step:11282 [D loss: 0.575741, acc.: 67.19%] [G loss: 1.375220]\n",
      "epoch:12 step:11283 [D loss: 0.673301, acc.: 59.38%] [G loss: 1.412168]\n",
      "epoch:12 step:11284 [D loss: 0.561779, acc.: 78.91%] [G loss: 1.220218]\n",
      "epoch:12 step:11285 [D loss: 0.620583, acc.: 64.84%] [G loss: 1.278506]\n",
      "epoch:12 step:11286 [D loss: 0.559301, acc.: 67.97%] [G loss: 1.526646]\n",
      "epoch:12 step:11287 [D loss: 0.550195, acc.: 72.66%] [G loss: 1.125347]\n",
      "epoch:12 step:11288 [D loss: 0.618665, acc.: 67.19%] [G loss: 1.425777]\n",
      "epoch:12 step:11289 [D loss: 0.652409, acc.: 65.62%] [G loss: 0.966210]\n",
      "epoch:12 step:11290 [D loss: 0.787104, acc.: 50.00%] [G loss: 0.952527]\n",
      "epoch:12 step:11291 [D loss: 0.755838, acc.: 50.78%] [G loss: 1.183487]\n",
      "epoch:12 step:11292 [D loss: 0.524566, acc.: 75.78%] [G loss: 1.263315]\n",
      "epoch:12 step:11293 [D loss: 0.643416, acc.: 60.16%] [G loss: 1.391756]\n",
      "epoch:12 step:11294 [D loss: 0.498563, acc.: 78.91%] [G loss: 1.238586]\n",
      "epoch:12 step:11295 [D loss: 0.626670, acc.: 63.28%] [G loss: 1.202788]\n",
      "epoch:12 step:11296 [D loss: 0.770645, acc.: 48.44%] [G loss: 1.212309]\n",
      "epoch:12 step:11297 [D loss: 0.580694, acc.: 66.41%] [G loss: 1.167642]\n",
      "epoch:12 step:11298 [D loss: 0.594310, acc.: 71.88%] [G loss: 1.413205]\n",
      "epoch:12 step:11299 [D loss: 0.573701, acc.: 71.88%] [G loss: 1.184082]\n",
      "epoch:12 step:11300 [D loss: 0.603013, acc.: 67.97%] [G loss: 0.993918]\n",
      "epoch:12 step:11301 [D loss: 0.685454, acc.: 61.72%] [G loss: 1.091220]\n",
      "epoch:12 step:11302 [D loss: 0.645022, acc.: 64.06%] [G loss: 1.389462]\n",
      "epoch:12 step:11303 [D loss: 0.512845, acc.: 75.00%] [G loss: 1.427993]\n",
      "epoch:12 step:11304 [D loss: 0.502559, acc.: 75.78%] [G loss: 1.492819]\n",
      "epoch:12 step:11305 [D loss: 0.652420, acc.: 63.28%] [G loss: 1.145012]\n",
      "epoch:12 step:11306 [D loss: 0.626502, acc.: 65.62%] [G loss: 1.138061]\n",
      "epoch:12 step:11307 [D loss: 0.560161, acc.: 75.00%] [G loss: 1.249340]\n",
      "epoch:12 step:11308 [D loss: 0.457570, acc.: 83.59%] [G loss: 1.273875]\n",
      "epoch:12 step:11309 [D loss: 0.590788, acc.: 75.00%] [G loss: 1.032687]\n",
      "epoch:12 step:11310 [D loss: 0.604414, acc.: 67.97%] [G loss: 1.116636]\n",
      "epoch:12 step:11311 [D loss: 0.501598, acc.: 82.03%] [G loss: 0.973564]\n",
      "epoch:12 step:11312 [D loss: 0.538923, acc.: 75.78%] [G loss: 1.338983]\n",
      "epoch:12 step:11313 [D loss: 0.583853, acc.: 67.19%] [G loss: 1.386704]\n",
      "epoch:12 step:11314 [D loss: 0.623359, acc.: 64.84%] [G loss: 1.146273]\n",
      "epoch:12 step:11315 [D loss: 0.644034, acc.: 61.72%] [G loss: 1.159859]\n",
      "epoch:12 step:11316 [D loss: 0.595130, acc.: 65.62%] [G loss: 1.003780]\n",
      "epoch:12 step:11317 [D loss: 0.585474, acc.: 64.84%] [G loss: 1.090454]\n",
      "epoch:12 step:11318 [D loss: 0.554596, acc.: 71.88%] [G loss: 1.305741]\n",
      "epoch:12 step:11319 [D loss: 0.621428, acc.: 64.84%] [G loss: 1.241359]\n",
      "epoch:12 step:11320 [D loss: 0.613642, acc.: 68.75%] [G loss: 1.202125]\n",
      "epoch:12 step:11321 [D loss: 0.610800, acc.: 67.19%] [G loss: 1.219414]\n",
      "epoch:12 step:11322 [D loss: 0.791727, acc.: 53.12%] [G loss: 0.997613]\n",
      "epoch:12 step:11323 [D loss: 0.507561, acc.: 75.00%] [G loss: 1.372011]\n",
      "epoch:12 step:11324 [D loss: 0.539479, acc.: 72.66%] [G loss: 1.122163]\n",
      "epoch:12 step:11325 [D loss: 0.684716, acc.: 62.50%] [G loss: 1.168994]\n",
      "epoch:12 step:11326 [D loss: 0.511831, acc.: 71.88%] [G loss: 1.574542]\n",
      "epoch:12 step:11327 [D loss: 0.560774, acc.: 75.78%] [G loss: 1.296436]\n",
      "epoch:12 step:11328 [D loss: 0.525500, acc.: 76.56%] [G loss: 1.149782]\n",
      "epoch:12 step:11329 [D loss: 0.502104, acc.: 76.56%] [G loss: 1.160451]\n",
      "epoch:12 step:11330 [D loss: 0.605565, acc.: 62.50%] [G loss: 1.470948]\n",
      "epoch:12 step:11331 [D loss: 0.621133, acc.: 67.19%] [G loss: 1.232555]\n",
      "epoch:12 step:11332 [D loss: 0.594306, acc.: 64.84%] [G loss: 1.277593]\n",
      "epoch:12 step:11333 [D loss: 0.657896, acc.: 63.28%] [G loss: 1.138737]\n",
      "epoch:12 step:11334 [D loss: 0.524339, acc.: 73.44%] [G loss: 1.138744]\n",
      "epoch:12 step:11335 [D loss: 0.643524, acc.: 65.62%] [G loss: 1.165707]\n",
      "epoch:12 step:11336 [D loss: 0.588526, acc.: 69.53%] [G loss: 1.220981]\n",
      "epoch:12 step:11337 [D loss: 0.506469, acc.: 79.69%] [G loss: 1.250886]\n",
      "epoch:12 step:11338 [D loss: 0.540489, acc.: 73.44%] [G loss: 1.269281]\n",
      "epoch:12 step:11339 [D loss: 0.605557, acc.: 70.31%] [G loss: 1.177852]\n",
      "epoch:12 step:11340 [D loss: 0.522825, acc.: 73.44%] [G loss: 1.449190]\n",
      "epoch:12 step:11341 [D loss: 0.626955, acc.: 62.50%] [G loss: 1.418140]\n",
      "epoch:12 step:11342 [D loss: 0.539141, acc.: 74.22%] [G loss: 1.152787]\n",
      "epoch:12 step:11343 [D loss: 0.559706, acc.: 71.88%] [G loss: 1.292253]\n",
      "epoch:12 step:11344 [D loss: 0.530139, acc.: 75.00%] [G loss: 1.229857]\n",
      "epoch:12 step:11345 [D loss: 0.573185, acc.: 68.75%] [G loss: 1.152396]\n",
      "epoch:12 step:11346 [D loss: 0.654623, acc.: 64.06%] [G loss: 1.261244]\n",
      "epoch:12 step:11347 [D loss: 0.521178, acc.: 78.12%] [G loss: 1.412637]\n",
      "epoch:12 step:11348 [D loss: 0.625352, acc.: 63.28%] [G loss: 1.178489]\n",
      "epoch:12 step:11349 [D loss: 0.548161, acc.: 74.22%] [G loss: 1.215025]\n",
      "epoch:12 step:11350 [D loss: 0.758340, acc.: 53.12%] [G loss: 1.419492]\n",
      "epoch:12 step:11351 [D loss: 0.592020, acc.: 67.19%] [G loss: 1.214788]\n",
      "epoch:12 step:11352 [D loss: 0.650315, acc.: 63.28%] [G loss: 1.234797]\n",
      "epoch:12 step:11353 [D loss: 0.682703, acc.: 57.81%] [G loss: 1.069300]\n",
      "epoch:12 step:11354 [D loss: 0.670853, acc.: 61.72%] [G loss: 0.896919]\n",
      "epoch:12 step:11355 [D loss: 0.652476, acc.: 63.28%] [G loss: 1.050229]\n",
      "epoch:12 step:11356 [D loss: 0.577820, acc.: 67.19%] [G loss: 1.294401]\n",
      "epoch:12 step:11357 [D loss: 0.559668, acc.: 72.66%] [G loss: 1.275357]\n",
      "epoch:12 step:11358 [D loss: 0.431737, acc.: 84.38%] [G loss: 1.442001]\n",
      "epoch:12 step:11359 [D loss: 0.626221, acc.: 62.50%] [G loss: 1.124942]\n",
      "epoch:12 step:11360 [D loss: 0.640296, acc.: 63.28%] [G loss: 1.186435]\n",
      "epoch:12 step:11361 [D loss: 0.630740, acc.: 62.50%] [G loss: 0.988777]\n",
      "epoch:12 step:11362 [D loss: 0.596401, acc.: 66.41%] [G loss: 1.210889]\n",
      "epoch:12 step:11363 [D loss: 0.689874, acc.: 57.81%] [G loss: 1.017530]\n",
      "epoch:12 step:11364 [D loss: 0.742201, acc.: 52.34%] [G loss: 0.946261]\n",
      "epoch:12 step:11365 [D loss: 0.595779, acc.: 72.66%] [G loss: 0.970047]\n",
      "epoch:12 step:11366 [D loss: 0.575744, acc.: 71.88%] [G loss: 1.133455]\n",
      "epoch:12 step:11367 [D loss: 0.553259, acc.: 70.31%] [G loss: 1.164833]\n",
      "epoch:12 step:11368 [D loss: 0.576442, acc.: 68.75%] [G loss: 1.086510]\n",
      "epoch:12 step:11369 [D loss: 0.494628, acc.: 77.34%] [G loss: 1.389183]\n",
      "epoch:12 step:11370 [D loss: 0.641792, acc.: 64.06%] [G loss: 1.097253]\n",
      "epoch:12 step:11371 [D loss: 0.576966, acc.: 68.75%] [G loss: 1.233041]\n",
      "epoch:12 step:11372 [D loss: 0.592919, acc.: 64.06%] [G loss: 1.045072]\n",
      "epoch:12 step:11373 [D loss: 0.629395, acc.: 63.28%] [G loss: 1.383304]\n",
      "epoch:12 step:11374 [D loss: 0.612327, acc.: 67.97%] [G loss: 1.104390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11375 [D loss: 0.572715, acc.: 71.09%] [G loss: 1.299846]\n",
      "epoch:12 step:11376 [D loss: 0.603947, acc.: 68.75%] [G loss: 1.198954]\n",
      "epoch:12 step:11377 [D loss: 0.491453, acc.: 80.47%] [G loss: 1.044162]\n",
      "epoch:12 step:11378 [D loss: 0.594600, acc.: 67.97%] [G loss: 1.139469]\n",
      "epoch:12 step:11379 [D loss: 0.539203, acc.: 70.31%] [G loss: 1.098395]\n",
      "epoch:12 step:11380 [D loss: 0.640043, acc.: 62.50%] [G loss: 1.061266]\n",
      "epoch:12 step:11381 [D loss: 0.497898, acc.: 79.69%] [G loss: 1.376384]\n",
      "epoch:12 step:11382 [D loss: 0.586353, acc.: 64.06%] [G loss: 1.206641]\n",
      "epoch:12 step:11383 [D loss: 0.803609, acc.: 48.44%] [G loss: 1.073998]\n",
      "epoch:12 step:11384 [D loss: 0.592297, acc.: 69.53%] [G loss: 1.308354]\n",
      "epoch:12 step:11385 [D loss: 0.705775, acc.: 60.16%] [G loss: 1.340519]\n",
      "epoch:12 step:11386 [D loss: 0.511531, acc.: 72.66%] [G loss: 1.326072]\n",
      "epoch:12 step:11387 [D loss: 0.535739, acc.: 73.44%] [G loss: 1.041223]\n",
      "epoch:12 step:11388 [D loss: 0.650723, acc.: 66.41%] [G loss: 1.222483]\n",
      "epoch:12 step:11389 [D loss: 0.592669, acc.: 67.19%] [G loss: 0.923630]\n",
      "epoch:12 step:11390 [D loss: 0.567179, acc.: 71.88%] [G loss: 1.111292]\n",
      "epoch:12 step:11391 [D loss: 0.492766, acc.: 77.34%] [G loss: 1.443751]\n",
      "epoch:12 step:11392 [D loss: 0.637617, acc.: 62.50%] [G loss: 1.281989]\n",
      "epoch:12 step:11393 [D loss: 0.554703, acc.: 67.97%] [G loss: 1.382575]\n",
      "epoch:12 step:11394 [D loss: 0.547421, acc.: 75.78%] [G loss: 1.150179]\n",
      "epoch:12 step:11395 [D loss: 0.558845, acc.: 70.31%] [G loss: 1.179400]\n",
      "epoch:12 step:11396 [D loss: 0.680012, acc.: 60.16%] [G loss: 1.098867]\n",
      "epoch:12 step:11397 [D loss: 0.716459, acc.: 57.03%] [G loss: 0.853482]\n",
      "epoch:12 step:11398 [D loss: 0.495609, acc.: 77.34%] [G loss: 1.160932]\n",
      "epoch:12 step:11399 [D loss: 0.577739, acc.: 71.09%] [G loss: 1.237709]\n",
      "epoch:12 step:11400 [D loss: 0.564992, acc.: 70.31%] [G loss: 1.241716]\n",
      "##############\n",
      "[2.72992517 2.06210793 2.03767687 2.81806525 0.94584289 5.36913424\n",
      " 2.05643441 2.74690044 3.78058623 6.1148739 ]\n",
      "##########\n",
      "epoch:12 step:11401 [D loss: 0.551924, acc.: 71.88%] [G loss: 1.414176]\n",
      "epoch:12 step:11402 [D loss: 0.559817, acc.: 72.66%] [G loss: 1.245415]\n",
      "epoch:12 step:11403 [D loss: 0.551275, acc.: 75.00%] [G loss: 1.212613]\n",
      "epoch:12 step:11404 [D loss: 0.627146, acc.: 64.06%] [G loss: 1.003572]\n",
      "epoch:12 step:11405 [D loss: 0.551324, acc.: 70.31%] [G loss: 1.235770]\n",
      "epoch:12 step:11406 [D loss: 0.638332, acc.: 60.16%] [G loss: 1.240257]\n",
      "epoch:12 step:11407 [D loss: 0.602658, acc.: 65.62%] [G loss: 1.052195]\n",
      "epoch:12 step:11408 [D loss: 0.568025, acc.: 71.09%] [G loss: 1.066495]\n",
      "epoch:12 step:11409 [D loss: 0.589085, acc.: 68.75%] [G loss: 1.310653]\n",
      "epoch:12 step:11410 [D loss: 0.596399, acc.: 67.97%] [G loss: 1.095752]\n",
      "epoch:12 step:11411 [D loss: 0.560108, acc.: 73.44%] [G loss: 1.002912]\n",
      "epoch:12 step:11412 [D loss: 0.684972, acc.: 62.50%] [G loss: 1.211952]\n",
      "epoch:12 step:11413 [D loss: 0.604705, acc.: 67.19%] [G loss: 1.000810]\n",
      "epoch:12 step:11414 [D loss: 0.554065, acc.: 72.66%] [G loss: 1.070823]\n",
      "epoch:12 step:11415 [D loss: 0.796688, acc.: 54.69%] [G loss: 1.036033]\n",
      "epoch:12 step:11416 [D loss: 0.478960, acc.: 75.78%] [G loss: 1.386863]\n",
      "epoch:12 step:11417 [D loss: 0.695161, acc.: 57.81%] [G loss: 1.258827]\n",
      "epoch:12 step:11418 [D loss: 0.601356, acc.: 64.84%] [G loss: 1.321762]\n",
      "epoch:12 step:11419 [D loss: 0.525447, acc.: 78.12%] [G loss: 1.339272]\n",
      "epoch:12 step:11420 [D loss: 0.519533, acc.: 77.34%] [G loss: 1.029283]\n",
      "epoch:12 step:11421 [D loss: 0.637748, acc.: 60.16%] [G loss: 1.324412]\n",
      "epoch:12 step:11422 [D loss: 0.609032, acc.: 67.19%] [G loss: 1.210204]\n",
      "epoch:12 step:11423 [D loss: 0.596758, acc.: 65.62%] [G loss: 1.258255]\n",
      "epoch:12 step:11424 [D loss: 0.570758, acc.: 68.75%] [G loss: 1.018125]\n",
      "epoch:12 step:11425 [D loss: 0.680353, acc.: 61.72%] [G loss: 1.024669]\n",
      "epoch:12 step:11426 [D loss: 0.516261, acc.: 80.47%] [G loss: 1.305196]\n",
      "epoch:12 step:11427 [D loss: 0.693417, acc.: 60.94%] [G loss: 1.058418]\n",
      "epoch:12 step:11428 [D loss: 0.612273, acc.: 65.62%] [G loss: 1.098897]\n",
      "epoch:12 step:11429 [D loss: 0.639996, acc.: 64.84%] [G loss: 1.089789]\n",
      "epoch:12 step:11430 [D loss: 0.514348, acc.: 71.88%] [G loss: 0.910083]\n",
      "epoch:12 step:11431 [D loss: 0.542750, acc.: 71.88%] [G loss: 1.182925]\n",
      "epoch:12 step:11432 [D loss: 0.629327, acc.: 63.28%] [G loss: 1.388134]\n",
      "epoch:12 step:11433 [D loss: 0.577913, acc.: 75.78%] [G loss: 1.308632]\n",
      "epoch:12 step:11434 [D loss: 0.746897, acc.: 49.22%] [G loss: 1.035390]\n",
      "epoch:12 step:11435 [D loss: 0.566556, acc.: 71.09%] [G loss: 1.368405]\n",
      "epoch:12 step:11436 [D loss: 0.505577, acc.: 74.22%] [G loss: 1.175013]\n",
      "epoch:12 step:11437 [D loss: 0.661074, acc.: 67.19%] [G loss: 1.169176]\n",
      "epoch:12 step:11438 [D loss: 0.655878, acc.: 65.62%] [G loss: 1.171056]\n",
      "epoch:12 step:11439 [D loss: 0.639436, acc.: 70.31%] [G loss: 1.374693]\n",
      "epoch:12 step:11440 [D loss: 0.659551, acc.: 61.72%] [G loss: 1.056566]\n",
      "epoch:12 step:11441 [D loss: 0.619489, acc.: 64.84%] [G loss: 1.289989]\n",
      "epoch:12 step:11442 [D loss: 0.556031, acc.: 71.88%] [G loss: 1.465768]\n",
      "epoch:12 step:11443 [D loss: 0.608811, acc.: 65.62%] [G loss: 1.078812]\n",
      "epoch:12 step:11444 [D loss: 0.523231, acc.: 75.00%] [G loss: 1.042837]\n",
      "epoch:12 step:11445 [D loss: 0.486311, acc.: 75.78%] [G loss: 1.436867]\n",
      "epoch:12 step:11446 [D loss: 0.490078, acc.: 78.91%] [G loss: 1.368237]\n",
      "epoch:12 step:11447 [D loss: 0.653821, acc.: 62.50%] [G loss: 0.937577]\n",
      "epoch:12 step:11448 [D loss: 0.481310, acc.: 76.56%] [G loss: 1.490994]\n",
      "epoch:12 step:11449 [D loss: 0.633502, acc.: 63.28%] [G loss: 1.288225]\n",
      "epoch:12 step:11450 [D loss: 0.554706, acc.: 75.00%] [G loss: 1.308902]\n",
      "epoch:12 step:11451 [D loss: 0.635023, acc.: 60.94%] [G loss: 1.217198]\n",
      "epoch:12 step:11452 [D loss: 0.533380, acc.: 77.34%] [G loss: 1.364157]\n",
      "epoch:12 step:11453 [D loss: 0.491420, acc.: 78.91%] [G loss: 1.242537]\n",
      "epoch:12 step:11454 [D loss: 0.553512, acc.: 77.34%] [G loss: 1.259661]\n",
      "epoch:12 step:11455 [D loss: 0.631058, acc.: 63.28%] [G loss: 1.002415]\n",
      "epoch:12 step:11456 [D loss: 0.551631, acc.: 69.53%] [G loss: 1.252030]\n",
      "epoch:12 step:11457 [D loss: 0.650349, acc.: 64.84%] [G loss: 1.227841]\n",
      "epoch:12 step:11458 [D loss: 0.699837, acc.: 60.16%] [G loss: 1.010448]\n",
      "epoch:12 step:11459 [D loss: 0.706945, acc.: 51.56%] [G loss: 1.044645]\n",
      "epoch:12 step:11460 [D loss: 0.680807, acc.: 60.16%] [G loss: 1.210797]\n",
      "epoch:12 step:11461 [D loss: 0.595306, acc.: 67.97%] [G loss: 0.978385]\n",
      "epoch:12 step:11462 [D loss: 0.725950, acc.: 55.47%] [G loss: 1.243018]\n",
      "epoch:12 step:11463 [D loss: 0.528735, acc.: 75.78%] [G loss: 1.234287]\n",
      "epoch:12 step:11464 [D loss: 0.637256, acc.: 65.62%] [G loss: 1.082631]\n",
      "epoch:12 step:11465 [D loss: 0.580365, acc.: 68.75%] [G loss: 1.247693]\n",
      "epoch:12 step:11466 [D loss: 0.618809, acc.: 65.62%] [G loss: 1.020877]\n",
      "epoch:12 step:11467 [D loss: 0.526110, acc.: 78.12%] [G loss: 1.270763]\n",
      "epoch:12 step:11468 [D loss: 0.572795, acc.: 70.31%] [G loss: 1.216609]\n",
      "epoch:12 step:11469 [D loss: 0.514171, acc.: 72.66%] [G loss: 1.244007]\n",
      "epoch:12 step:11470 [D loss: 0.611957, acc.: 69.53%] [G loss: 1.068764]\n",
      "epoch:12 step:11471 [D loss: 0.591681, acc.: 64.06%] [G loss: 1.042206]\n",
      "epoch:12 step:11472 [D loss: 0.652284, acc.: 63.28%] [G loss: 1.153382]\n",
      "epoch:12 step:11473 [D loss: 0.531457, acc.: 75.00%] [G loss: 1.354122]\n",
      "epoch:12 step:11474 [D loss: 0.646758, acc.: 64.06%] [G loss: 1.107775]\n",
      "epoch:12 step:11475 [D loss: 0.532891, acc.: 75.78%] [G loss: 1.196440]\n",
      "epoch:12 step:11476 [D loss: 0.517753, acc.: 75.00%] [G loss: 1.303009]\n",
      "epoch:12 step:11477 [D loss: 0.467981, acc.: 80.47%] [G loss: 1.440687]\n",
      "epoch:12 step:11478 [D loss: 0.568750, acc.: 70.31%] [G loss: 1.179724]\n",
      "epoch:12 step:11479 [D loss: 0.650371, acc.: 60.16%] [G loss: 1.102465]\n",
      "epoch:12 step:11480 [D loss: 0.536587, acc.: 75.78%] [G loss: 1.317980]\n",
      "epoch:12 step:11481 [D loss: 0.589485, acc.: 68.75%] [G loss: 1.357149]\n",
      "epoch:12 step:11482 [D loss: 0.642133, acc.: 63.28%] [G loss: 1.175873]\n",
      "epoch:12 step:11483 [D loss: 0.650178, acc.: 60.16%] [G loss: 1.202211]\n",
      "epoch:12 step:11484 [D loss: 0.538264, acc.: 74.22%] [G loss: 1.110428]\n",
      "epoch:12 step:11485 [D loss: 0.489398, acc.: 78.91%] [G loss: 1.099273]\n",
      "epoch:12 step:11486 [D loss: 0.633802, acc.: 67.19%] [G loss: 1.260959]\n",
      "epoch:12 step:11487 [D loss: 0.520820, acc.: 75.00%] [G loss: 1.168527]\n",
      "epoch:12 step:11488 [D loss: 0.584882, acc.: 69.53%] [G loss: 1.031999]\n",
      "epoch:12 step:11489 [D loss: 0.614653, acc.: 66.41%] [G loss: 1.166782]\n",
      "epoch:12 step:11490 [D loss: 0.546896, acc.: 74.22%] [G loss: 1.059150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11491 [D loss: 0.635453, acc.: 64.06%] [G loss: 1.075711]\n",
      "epoch:12 step:11492 [D loss: 0.554994, acc.: 71.88%] [G loss: 1.476215]\n",
      "epoch:12 step:11493 [D loss: 0.592434, acc.: 67.19%] [G loss: 1.267194]\n",
      "epoch:12 step:11494 [D loss: 0.548641, acc.: 73.44%] [G loss: 1.235459]\n",
      "epoch:12 step:11495 [D loss: 0.584566, acc.: 67.19%] [G loss: 1.278369]\n",
      "epoch:12 step:11496 [D loss: 0.462280, acc.: 82.81%] [G loss: 1.225855]\n",
      "epoch:12 step:11497 [D loss: 0.525200, acc.: 72.66%] [G loss: 1.000108]\n",
      "epoch:12 step:11498 [D loss: 0.615110, acc.: 64.06%] [G loss: 0.975751]\n",
      "epoch:12 step:11499 [D loss: 0.539502, acc.: 75.00%] [G loss: 1.287026]\n",
      "epoch:12 step:11500 [D loss: 0.622493, acc.: 61.72%] [G loss: 1.110784]\n",
      "epoch:12 step:11501 [D loss: 0.654433, acc.: 60.94%] [G loss: 1.307469]\n",
      "epoch:12 step:11502 [D loss: 0.494233, acc.: 78.91%] [G loss: 1.602239]\n",
      "epoch:12 step:11503 [D loss: 0.670732, acc.: 58.59%] [G loss: 1.259567]\n",
      "epoch:12 step:11504 [D loss: 0.618767, acc.: 67.97%] [G loss: 1.057661]\n",
      "epoch:12 step:11505 [D loss: 0.554064, acc.: 72.66%] [G loss: 1.211393]\n",
      "epoch:12 step:11506 [D loss: 0.524780, acc.: 75.00%] [G loss: 1.092645]\n",
      "epoch:12 step:11507 [D loss: 0.610204, acc.: 64.06%] [G loss: 1.155393]\n",
      "epoch:12 step:11508 [D loss: 0.586646, acc.: 68.75%] [G loss: 1.286330]\n",
      "epoch:12 step:11509 [D loss: 0.488568, acc.: 76.56%] [G loss: 1.128609]\n",
      "epoch:12 step:11510 [D loss: 0.513902, acc.: 80.47%] [G loss: 1.512121]\n",
      "epoch:12 step:11511 [D loss: 0.566564, acc.: 74.22%] [G loss: 1.151090]\n",
      "epoch:12 step:11512 [D loss: 0.613106, acc.: 66.41%] [G loss: 1.193263]\n",
      "epoch:12 step:11513 [D loss: 0.475012, acc.: 82.81%] [G loss: 1.193386]\n",
      "epoch:12 step:11514 [D loss: 0.548250, acc.: 72.66%] [G loss: 1.404929]\n",
      "epoch:12 step:11515 [D loss: 0.639108, acc.: 62.50%] [G loss: 1.211806]\n",
      "epoch:12 step:11516 [D loss: 0.567903, acc.: 70.31%] [G loss: 1.307281]\n",
      "epoch:12 step:11517 [D loss: 0.506101, acc.: 73.44%] [G loss: 1.381994]\n",
      "epoch:12 step:11518 [D loss: 0.636166, acc.: 59.38%] [G loss: 1.126992]\n",
      "epoch:12 step:11519 [D loss: 0.650595, acc.: 68.75%] [G loss: 1.219668]\n",
      "epoch:12 step:11520 [D loss: 0.651383, acc.: 65.62%] [G loss: 0.898342]\n",
      "epoch:12 step:11521 [D loss: 0.544054, acc.: 70.31%] [G loss: 1.094512]\n",
      "epoch:12 step:11522 [D loss: 0.717928, acc.: 56.25%] [G loss: 1.072584]\n",
      "epoch:12 step:11523 [D loss: 0.577249, acc.: 72.66%] [G loss: 1.120516]\n",
      "epoch:12 step:11524 [D loss: 0.614775, acc.: 62.50%] [G loss: 1.125810]\n",
      "epoch:12 step:11525 [D loss: 0.537179, acc.: 75.78%] [G loss: 1.278052]\n",
      "epoch:12 step:11526 [D loss: 0.587876, acc.: 67.19%] [G loss: 1.112490]\n",
      "epoch:12 step:11527 [D loss: 0.588558, acc.: 68.75%] [G loss: 1.140889]\n",
      "epoch:12 step:11528 [D loss: 0.626764, acc.: 65.62%] [G loss: 1.288595]\n",
      "epoch:12 step:11529 [D loss: 0.540732, acc.: 77.34%] [G loss: 1.265336]\n",
      "epoch:12 step:11530 [D loss: 0.651468, acc.: 67.19%] [G loss: 1.086048]\n",
      "epoch:12 step:11531 [D loss: 0.611467, acc.: 67.97%] [G loss: 1.089348]\n",
      "epoch:12 step:11532 [D loss: 0.678937, acc.: 64.84%] [G loss: 1.070957]\n",
      "epoch:12 step:11533 [D loss: 0.529427, acc.: 77.34%] [G loss: 0.999442]\n",
      "epoch:12 step:11534 [D loss: 0.557819, acc.: 69.53%] [G loss: 1.082579]\n",
      "epoch:12 step:11535 [D loss: 0.612613, acc.: 64.06%] [G loss: 1.110399]\n",
      "epoch:12 step:11536 [D loss: 0.547656, acc.: 73.44%] [G loss: 1.152204]\n",
      "epoch:12 step:11537 [D loss: 0.504394, acc.: 77.34%] [G loss: 1.210058]\n",
      "epoch:12 step:11538 [D loss: 0.718092, acc.: 59.38%] [G loss: 0.998772]\n",
      "epoch:12 step:11539 [D loss: 0.481138, acc.: 78.12%] [G loss: 1.393303]\n",
      "epoch:12 step:11540 [D loss: 0.556636, acc.: 71.09%] [G loss: 0.952353]\n",
      "epoch:12 step:11541 [D loss: 0.710276, acc.: 56.25%] [G loss: 0.924362]\n",
      "epoch:12 step:11542 [D loss: 0.798026, acc.: 42.19%] [G loss: 1.260503]\n",
      "epoch:12 step:11543 [D loss: 0.698086, acc.: 57.03%] [G loss: 1.413332]\n",
      "epoch:12 step:11544 [D loss: 0.579015, acc.: 69.53%] [G loss: 1.201669]\n",
      "epoch:12 step:11545 [D loss: 0.665438, acc.: 61.72%] [G loss: 1.071857]\n",
      "epoch:12 step:11546 [D loss: 0.492146, acc.: 78.91%] [G loss: 1.376018]\n",
      "epoch:12 step:11547 [D loss: 0.599930, acc.: 64.84%] [G loss: 1.077865]\n",
      "epoch:12 step:11548 [D loss: 0.482804, acc.: 82.03%] [G loss: 1.349375]\n",
      "epoch:12 step:11549 [D loss: 0.661463, acc.: 61.72%] [G loss: 1.162690]\n",
      "epoch:12 step:11550 [D loss: 0.575302, acc.: 67.19%] [G loss: 1.228424]\n",
      "epoch:12 step:11551 [D loss: 0.673089, acc.: 64.06%] [G loss: 1.018816]\n",
      "epoch:12 step:11552 [D loss: 0.576219, acc.: 71.09%] [G loss: 1.158202]\n",
      "epoch:12 step:11553 [D loss: 0.507732, acc.: 78.91%] [G loss: 0.978911]\n",
      "epoch:12 step:11554 [D loss: 0.700091, acc.: 53.12%] [G loss: 1.265840]\n",
      "epoch:12 step:11555 [D loss: 0.572961, acc.: 68.75%] [G loss: 1.292799]\n",
      "epoch:12 step:11556 [D loss: 0.551929, acc.: 75.78%] [G loss: 1.308850]\n",
      "epoch:12 step:11557 [D loss: 0.622201, acc.: 62.50%] [G loss: 1.222721]\n",
      "epoch:12 step:11558 [D loss: 0.587957, acc.: 68.75%] [G loss: 1.126740]\n",
      "epoch:12 step:11559 [D loss: 0.554802, acc.: 72.66%] [G loss: 1.159039]\n",
      "epoch:12 step:11560 [D loss: 0.669090, acc.: 62.50%] [G loss: 1.295033]\n",
      "epoch:12 step:11561 [D loss: 0.545274, acc.: 77.34%] [G loss: 0.951234]\n",
      "epoch:12 step:11562 [D loss: 0.544196, acc.: 72.66%] [G loss: 1.326262]\n",
      "epoch:12 step:11563 [D loss: 0.669509, acc.: 61.72%] [G loss: 1.068629]\n",
      "epoch:12 step:11564 [D loss: 0.567757, acc.: 69.53%] [G loss: 1.286632]\n",
      "epoch:12 step:11565 [D loss: 0.576019, acc.: 69.53%] [G loss: 1.282820]\n",
      "epoch:12 step:11566 [D loss: 0.617557, acc.: 62.50%] [G loss: 1.172610]\n",
      "epoch:12 step:11567 [D loss: 0.629013, acc.: 68.75%] [G loss: 1.010273]\n",
      "epoch:12 step:11568 [D loss: 0.497674, acc.: 78.12%] [G loss: 1.233464]\n",
      "epoch:12 step:11569 [D loss: 0.603530, acc.: 70.31%] [G loss: 1.396113]\n",
      "epoch:12 step:11570 [D loss: 0.586377, acc.: 68.75%] [G loss: 1.295110]\n",
      "epoch:12 step:11571 [D loss: 0.490039, acc.: 75.78%] [G loss: 1.246956]\n",
      "epoch:12 step:11572 [D loss: 0.429967, acc.: 82.81%] [G loss: 1.292918]\n",
      "epoch:12 step:11573 [D loss: 0.727197, acc.: 55.47%] [G loss: 0.949417]\n",
      "epoch:12 step:11574 [D loss: 0.612992, acc.: 67.19%] [G loss: 1.212403]\n",
      "epoch:12 step:11575 [D loss: 0.512663, acc.: 77.34%] [G loss: 1.244679]\n",
      "epoch:12 step:11576 [D loss: 0.527611, acc.: 73.44%] [G loss: 1.279291]\n",
      "epoch:12 step:11577 [D loss: 0.585397, acc.: 70.31%] [G loss: 1.153617]\n",
      "epoch:12 step:11578 [D loss: 0.510482, acc.: 78.12%] [G loss: 1.380335]\n",
      "epoch:12 step:11579 [D loss: 0.560861, acc.: 71.88%] [G loss: 1.325105]\n",
      "epoch:12 step:11580 [D loss: 0.579378, acc.: 69.53%] [G loss: 1.261851]\n",
      "epoch:12 step:11581 [D loss: 0.610700, acc.: 64.84%] [G loss: 1.318330]\n",
      "epoch:12 step:11582 [D loss: 0.528447, acc.: 75.00%] [G loss: 1.325665]\n",
      "epoch:12 step:11583 [D loss: 0.673843, acc.: 53.91%] [G loss: 1.134473]\n",
      "epoch:12 step:11584 [D loss: 0.577933, acc.: 70.31%] [G loss: 1.527494]\n",
      "epoch:12 step:11585 [D loss: 0.566951, acc.: 69.53%] [G loss: 1.159000]\n",
      "epoch:12 step:11586 [D loss: 0.549867, acc.: 71.09%] [G loss: 1.333966]\n",
      "epoch:12 step:11587 [D loss: 0.600957, acc.: 65.62%] [G loss: 0.940779]\n",
      "epoch:12 step:11588 [D loss: 0.752741, acc.: 48.44%] [G loss: 0.835629]\n",
      "epoch:12 step:11589 [D loss: 0.544879, acc.: 72.66%] [G loss: 1.114171]\n",
      "epoch:12 step:11590 [D loss: 0.553352, acc.: 73.44%] [G loss: 1.242787]\n",
      "epoch:12 step:11591 [D loss: 0.656915, acc.: 61.72%] [G loss: 1.286438]\n",
      "epoch:12 step:11592 [D loss: 0.678941, acc.: 60.16%] [G loss: 1.331810]\n",
      "epoch:12 step:11593 [D loss: 0.476655, acc.: 77.34%] [G loss: 1.229523]\n",
      "epoch:12 step:11594 [D loss: 0.710588, acc.: 58.59%] [G loss: 0.846071]\n",
      "epoch:12 step:11595 [D loss: 0.542476, acc.: 70.31%] [G loss: 1.424505]\n",
      "epoch:12 step:11596 [D loss: 0.838041, acc.: 50.00%] [G loss: 0.995839]\n",
      "epoch:12 step:11597 [D loss: 0.493471, acc.: 76.56%] [G loss: 1.108389]\n",
      "epoch:12 step:11598 [D loss: 0.524766, acc.: 71.09%] [G loss: 1.337313]\n",
      "epoch:12 step:11599 [D loss: 0.695649, acc.: 57.81%] [G loss: 1.059125]\n",
      "epoch:12 step:11600 [D loss: 0.512942, acc.: 75.78%] [G loss: 1.149709]\n",
      "##############\n",
      "[2.71274922 2.17358704 2.17892894 3.03025184 1.14228219 6.53592924\n",
      " 2.15863798 3.12106049 4.05462069 8.14868929]\n",
      "##########\n",
      "epoch:12 step:11601 [D loss: 0.588156, acc.: 66.41%] [G loss: 1.198298]\n",
      "epoch:12 step:11602 [D loss: 0.589376, acc.: 67.19%] [G loss: 1.124520]\n",
      "epoch:12 step:11603 [D loss: 0.712809, acc.: 57.81%] [G loss: 1.004634]\n",
      "epoch:12 step:11604 [D loss: 0.693813, acc.: 58.59%] [G loss: 1.147934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11605 [D loss: 0.569556, acc.: 70.31%] [G loss: 1.035575]\n",
      "epoch:12 step:11606 [D loss: 0.478687, acc.: 77.34%] [G loss: 1.368395]\n",
      "epoch:12 step:11607 [D loss: 0.669925, acc.: 59.38%] [G loss: 1.349796]\n",
      "epoch:12 step:11608 [D loss: 0.547163, acc.: 71.88%] [G loss: 1.229152]\n",
      "epoch:12 step:11609 [D loss: 0.461424, acc.: 78.12%] [G loss: 1.132561]\n",
      "epoch:12 step:11610 [D loss: 0.565202, acc.: 67.97%] [G loss: 1.312148]\n",
      "epoch:12 step:11611 [D loss: 0.769399, acc.: 50.78%] [G loss: 1.136004]\n",
      "epoch:12 step:11612 [D loss: 0.577891, acc.: 71.09%] [G loss: 1.150494]\n",
      "epoch:12 step:11613 [D loss: 0.552621, acc.: 75.00%] [G loss: 1.057076]\n",
      "epoch:12 step:11614 [D loss: 0.497705, acc.: 78.91%] [G loss: 1.270522]\n",
      "epoch:12 step:11615 [D loss: 0.612596, acc.: 60.16%] [G loss: 1.144100]\n",
      "epoch:12 step:11616 [D loss: 0.521128, acc.: 75.00%] [G loss: 1.364224]\n",
      "epoch:12 step:11617 [D loss: 0.628561, acc.: 64.84%] [G loss: 1.374248]\n",
      "epoch:12 step:11618 [D loss: 0.647174, acc.: 60.94%] [G loss: 1.138818]\n",
      "epoch:12 step:11619 [D loss: 0.730061, acc.: 54.69%] [G loss: 0.971698]\n",
      "epoch:12 step:11620 [D loss: 0.542286, acc.: 71.88%] [G loss: 1.317965]\n",
      "epoch:12 step:11621 [D loss: 0.571496, acc.: 66.41%] [G loss: 1.042134]\n",
      "epoch:12 step:11622 [D loss: 0.701869, acc.: 61.72%] [G loss: 1.314051]\n",
      "epoch:12 step:11623 [D loss: 0.584222, acc.: 65.62%] [G loss: 1.198440]\n",
      "epoch:12 step:11624 [D loss: 0.597512, acc.: 67.97%] [G loss: 1.306564]\n",
      "epoch:12 step:11625 [D loss: 0.557202, acc.: 75.00%] [G loss: 1.437264]\n",
      "epoch:12 step:11626 [D loss: 0.606288, acc.: 63.28%] [G loss: 1.135628]\n",
      "epoch:12 step:11627 [D loss: 0.534514, acc.: 75.00%] [G loss: 1.327590]\n",
      "epoch:12 step:11628 [D loss: 0.701415, acc.: 57.03%] [G loss: 1.007817]\n",
      "epoch:12 step:11629 [D loss: 0.511753, acc.: 76.56%] [G loss: 1.284395]\n",
      "epoch:12 step:11630 [D loss: 0.519691, acc.: 74.22%] [G loss: 1.255699]\n",
      "epoch:12 step:11631 [D loss: 0.619727, acc.: 64.84%] [G loss: 1.155835]\n",
      "epoch:12 step:11632 [D loss: 0.628913, acc.: 61.72%] [G loss: 1.172807]\n",
      "epoch:12 step:11633 [D loss: 0.597046, acc.: 65.62%] [G loss: 1.128254]\n",
      "epoch:12 step:11634 [D loss: 0.613972, acc.: 65.62%] [G loss: 1.070725]\n",
      "epoch:12 step:11635 [D loss: 0.634516, acc.: 65.62%] [G loss: 0.909240]\n",
      "epoch:12 step:11636 [D loss: 0.636708, acc.: 59.38%] [G loss: 0.999426]\n",
      "epoch:12 step:11637 [D loss: 0.576089, acc.: 74.22%] [G loss: 1.371702]\n",
      "epoch:12 step:11638 [D loss: 0.609562, acc.: 67.97%] [G loss: 1.247526]\n",
      "epoch:12 step:11639 [D loss: 0.570146, acc.: 72.66%] [G loss: 1.098293]\n",
      "epoch:12 step:11640 [D loss: 0.675801, acc.: 60.16%] [G loss: 1.169491]\n",
      "epoch:12 step:11641 [D loss: 0.504018, acc.: 77.34%] [G loss: 1.154942]\n",
      "epoch:12 step:11642 [D loss: 0.644769, acc.: 62.50%] [G loss: 1.081571]\n",
      "epoch:12 step:11643 [D loss: 0.524633, acc.: 72.66%] [G loss: 1.244209]\n",
      "epoch:12 step:11644 [D loss: 0.539775, acc.: 68.75%] [G loss: 1.120379]\n",
      "epoch:12 step:11645 [D loss: 0.536957, acc.: 74.22%] [G loss: 1.064607]\n",
      "epoch:12 step:11646 [D loss: 0.475652, acc.: 79.69%] [G loss: 1.241737]\n",
      "epoch:12 step:11647 [D loss: 0.580176, acc.: 71.09%] [G loss: 1.072401]\n",
      "epoch:12 step:11648 [D loss: 0.501710, acc.: 75.78%] [G loss: 1.259097]\n",
      "epoch:12 step:11649 [D loss: 0.552937, acc.: 71.88%] [G loss: 1.069510]\n",
      "epoch:12 step:11650 [D loss: 0.520390, acc.: 73.44%] [G loss: 1.291505]\n",
      "epoch:12 step:11651 [D loss: 0.590408, acc.: 67.19%] [G loss: 1.207675]\n",
      "epoch:12 step:11652 [D loss: 0.573315, acc.: 67.19%] [G loss: 1.192691]\n",
      "epoch:12 step:11653 [D loss: 0.528843, acc.: 71.88%] [G loss: 1.021207]\n",
      "epoch:12 step:11654 [D loss: 0.559728, acc.: 74.22%] [G loss: 0.957517]\n",
      "epoch:12 step:11655 [D loss: 0.727459, acc.: 60.94%] [G loss: 1.108056]\n",
      "epoch:12 step:11656 [D loss: 0.762091, acc.: 53.12%] [G loss: 1.073852]\n",
      "epoch:12 step:11657 [D loss: 0.576776, acc.: 70.31%] [G loss: 1.304648]\n",
      "epoch:12 step:11658 [D loss: 0.548655, acc.: 73.44%] [G loss: 1.243680]\n",
      "epoch:12 step:11659 [D loss: 0.548417, acc.: 75.78%] [G loss: 1.274094]\n",
      "epoch:12 step:11660 [D loss: 0.550919, acc.: 66.41%] [G loss: 1.346307]\n",
      "epoch:12 step:11661 [D loss: 0.583817, acc.: 70.31%] [G loss: 1.219593]\n",
      "epoch:12 step:11662 [D loss: 0.611298, acc.: 67.19%] [G loss: 1.016635]\n",
      "epoch:12 step:11663 [D loss: 0.571683, acc.: 70.31%] [G loss: 1.033855]\n",
      "epoch:12 step:11664 [D loss: 0.636990, acc.: 64.06%] [G loss: 1.112967]\n",
      "epoch:12 step:11665 [D loss: 0.549371, acc.: 72.66%] [G loss: 1.258303]\n",
      "epoch:12 step:11666 [D loss: 0.632118, acc.: 64.06%] [G loss: 1.327095]\n",
      "epoch:12 step:11667 [D loss: 0.572069, acc.: 69.53%] [G loss: 1.113095]\n",
      "epoch:12 step:11668 [D loss: 0.636248, acc.: 60.16%] [G loss: 1.018902]\n",
      "epoch:12 step:11669 [D loss: 0.560577, acc.: 74.22%] [G loss: 1.162478]\n",
      "epoch:12 step:11670 [D loss: 0.704185, acc.: 61.72%] [G loss: 1.329762]\n",
      "epoch:12 step:11671 [D loss: 0.780965, acc.: 49.22%] [G loss: 1.056429]\n",
      "epoch:12 step:11672 [D loss: 0.630199, acc.: 67.19%] [G loss: 1.380910]\n",
      "epoch:12 step:11673 [D loss: 0.675029, acc.: 63.28%] [G loss: 1.199420]\n",
      "epoch:12 step:11674 [D loss: 0.671931, acc.: 61.72%] [G loss: 1.188824]\n",
      "epoch:12 step:11675 [D loss: 0.591859, acc.: 67.97%] [G loss: 1.316644]\n",
      "epoch:12 step:11676 [D loss: 0.486064, acc.: 80.47%] [G loss: 1.384200]\n",
      "epoch:12 step:11677 [D loss: 0.613110, acc.: 66.41%] [G loss: 1.267917]\n",
      "epoch:12 step:11678 [D loss: 0.597749, acc.: 69.53%] [G loss: 1.086602]\n",
      "epoch:12 step:11679 [D loss: 0.636366, acc.: 62.50%] [G loss: 1.151817]\n",
      "epoch:12 step:11680 [D loss: 0.569249, acc.: 69.53%] [G loss: 1.150469]\n",
      "epoch:12 step:11681 [D loss: 0.582518, acc.: 68.75%] [G loss: 1.032650]\n",
      "epoch:12 step:11682 [D loss: 0.638180, acc.: 60.94%] [G loss: 1.011622]\n",
      "epoch:12 step:11683 [D loss: 0.585645, acc.: 63.28%] [G loss: 1.179794]\n",
      "epoch:12 step:11684 [D loss: 0.664190, acc.: 63.28%] [G loss: 1.184513]\n",
      "epoch:12 step:11685 [D loss: 0.551530, acc.: 73.44%] [G loss: 1.185530]\n",
      "epoch:12 step:11686 [D loss: 0.611991, acc.: 67.19%] [G loss: 1.220088]\n",
      "epoch:12 step:11687 [D loss: 0.533113, acc.: 70.31%] [G loss: 1.155107]\n",
      "epoch:12 step:11688 [D loss: 0.515823, acc.: 71.09%] [G loss: 1.290205]\n",
      "epoch:12 step:11689 [D loss: 0.512211, acc.: 78.91%] [G loss: 1.536390]\n",
      "epoch:12 step:11690 [D loss: 0.584403, acc.: 66.41%] [G loss: 1.340668]\n",
      "epoch:12 step:11691 [D loss: 0.477094, acc.: 80.47%] [G loss: 1.293165]\n",
      "epoch:12 step:11692 [D loss: 0.641838, acc.: 64.06%] [G loss: 1.189736]\n",
      "epoch:12 step:11693 [D loss: 0.581062, acc.: 69.53%] [G loss: 1.069711]\n",
      "epoch:12 step:11694 [D loss: 0.609125, acc.: 64.84%] [G loss: 1.359552]\n",
      "epoch:12 step:11695 [D loss: 0.543320, acc.: 74.22%] [G loss: 1.258651]\n",
      "epoch:12 step:11696 [D loss: 0.600549, acc.: 65.62%] [G loss: 0.850282]\n",
      "epoch:12 step:11697 [D loss: 0.652517, acc.: 60.16%] [G loss: 1.152883]\n",
      "epoch:12 step:11698 [D loss: 0.483283, acc.: 81.25%] [G loss: 1.330507]\n",
      "epoch:12 step:11699 [D loss: 0.670454, acc.: 55.47%] [G loss: 1.011821]\n",
      "epoch:12 step:11700 [D loss: 0.635204, acc.: 60.16%] [G loss: 1.239959]\n",
      "epoch:12 step:11701 [D loss: 0.606913, acc.: 68.75%] [G loss: 1.508952]\n",
      "epoch:12 step:11702 [D loss: 0.562158, acc.: 73.44%] [G loss: 1.302161]\n",
      "epoch:12 step:11703 [D loss: 0.518426, acc.: 79.69%] [G loss: 1.437223]\n",
      "epoch:12 step:11704 [D loss: 0.466193, acc.: 83.59%] [G loss: 1.381608]\n",
      "epoch:12 step:11705 [D loss: 0.616269, acc.: 67.97%] [G loss: 1.365734]\n",
      "epoch:12 step:11706 [D loss: 0.522928, acc.: 75.78%] [G loss: 1.157365]\n",
      "epoch:12 step:11707 [D loss: 0.645156, acc.: 62.50%] [G loss: 1.068266]\n",
      "epoch:12 step:11708 [D loss: 0.627970, acc.: 60.16%] [G loss: 1.309785]\n",
      "epoch:12 step:11709 [D loss: 0.675002, acc.: 56.25%] [G loss: 1.246812]\n",
      "epoch:12 step:11710 [D loss: 0.589215, acc.: 68.75%] [G loss: 1.350418]\n",
      "epoch:12 step:11711 [D loss: 0.503896, acc.: 77.34%] [G loss: 1.574563]\n",
      "epoch:12 step:11712 [D loss: 0.542121, acc.: 75.00%] [G loss: 1.106376]\n",
      "epoch:12 step:11713 [D loss: 0.465884, acc.: 80.47%] [G loss: 1.429898]\n",
      "epoch:12 step:11714 [D loss: 0.710022, acc.: 55.47%] [G loss: 1.160858]\n",
      "epoch:12 step:11715 [D loss: 0.511518, acc.: 78.91%] [G loss: 1.147704]\n",
      "epoch:12 step:11716 [D loss: 0.632548, acc.: 63.28%] [G loss: 1.097596]\n",
      "epoch:12 step:11717 [D loss: 0.443613, acc.: 80.47%] [G loss: 1.278149]\n",
      "epoch:12 step:11718 [D loss: 0.556108, acc.: 69.53%] [G loss: 1.543788]\n",
      "epoch:12 step:11719 [D loss: 0.605158, acc.: 66.41%] [G loss: 1.144625]\n",
      "epoch:12 step:11720 [D loss: 0.519363, acc.: 77.34%] [G loss: 1.218629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11721 [D loss: 0.508919, acc.: 80.47%] [G loss: 1.411611]\n",
      "epoch:12 step:11722 [D loss: 0.640165, acc.: 65.62%] [G loss: 1.135918]\n",
      "epoch:12 step:11723 [D loss: 0.652097, acc.: 67.97%] [G loss: 1.081215]\n",
      "epoch:12 step:11724 [D loss: 0.588413, acc.: 67.97%] [G loss: 0.916634]\n",
      "epoch:12 step:11725 [D loss: 0.590471, acc.: 64.06%] [G loss: 1.308320]\n",
      "epoch:12 step:11726 [D loss: 0.509038, acc.: 73.44%] [G loss: 1.331164]\n",
      "epoch:12 step:11727 [D loss: 0.647758, acc.: 65.62%] [G loss: 1.059398]\n",
      "epoch:12 step:11728 [D loss: 0.460499, acc.: 84.38%] [G loss: 1.213622]\n",
      "epoch:12 step:11729 [D loss: 0.652161, acc.: 60.94%] [G loss: 1.209410]\n",
      "epoch:12 step:11730 [D loss: 0.618136, acc.: 67.19%] [G loss: 1.227600]\n",
      "epoch:12 step:11731 [D loss: 0.463308, acc.: 78.12%] [G loss: 1.410644]\n",
      "epoch:12 step:11732 [D loss: 0.490425, acc.: 78.91%] [G loss: 1.149107]\n",
      "epoch:12 step:11733 [D loss: 0.630955, acc.: 67.19%] [G loss: 1.266320]\n",
      "epoch:12 step:11734 [D loss: 0.468821, acc.: 80.47%] [G loss: 1.161873]\n",
      "epoch:12 step:11735 [D loss: 0.521511, acc.: 74.22%] [G loss: 1.489497]\n",
      "epoch:12 step:11736 [D loss: 0.589824, acc.: 68.75%] [G loss: 1.172507]\n",
      "epoch:12 step:11737 [D loss: 0.634019, acc.: 65.62%] [G loss: 1.100326]\n",
      "epoch:12 step:11738 [D loss: 0.592344, acc.: 67.97%] [G loss: 1.234191]\n",
      "epoch:12 step:11739 [D loss: 0.609317, acc.: 66.41%] [G loss: 1.438478]\n",
      "epoch:12 step:11740 [D loss: 0.480170, acc.: 78.12%] [G loss: 1.299363]\n",
      "epoch:12 step:11741 [D loss: 0.565673, acc.: 72.66%] [G loss: 1.224413]\n",
      "epoch:12 step:11742 [D loss: 0.477530, acc.: 79.69%] [G loss: 1.059817]\n",
      "epoch:12 step:11743 [D loss: 0.577676, acc.: 66.41%] [G loss: 1.279829]\n",
      "epoch:12 step:11744 [D loss: 0.708317, acc.: 53.91%] [G loss: 1.137831]\n",
      "epoch:12 step:11745 [D loss: 0.772696, acc.: 52.34%] [G loss: 1.323096]\n",
      "epoch:12 step:11746 [D loss: 0.589700, acc.: 67.97%] [G loss: 1.408755]\n",
      "epoch:12 step:11747 [D loss: 0.618911, acc.: 63.28%] [G loss: 1.239870]\n",
      "epoch:12 step:11748 [D loss: 0.468413, acc.: 84.38%] [G loss: 1.731417]\n",
      "epoch:12 step:11749 [D loss: 0.704187, acc.: 56.25%] [G loss: 1.253339]\n",
      "epoch:12 step:11750 [D loss: 0.500785, acc.: 77.34%] [G loss: 1.434702]\n",
      "epoch:12 step:11751 [D loss: 0.606767, acc.: 64.06%] [G loss: 0.963868]\n",
      "epoch:12 step:11752 [D loss: 0.636324, acc.: 60.94%] [G loss: 1.124402]\n",
      "epoch:12 step:11753 [D loss: 0.517356, acc.: 79.69%] [G loss: 1.063162]\n",
      "epoch:12 step:11754 [D loss: 0.617638, acc.: 65.62%] [G loss: 1.219134]\n",
      "epoch:12 step:11755 [D loss: 0.635723, acc.: 59.38%] [G loss: 1.186088]\n",
      "epoch:12 step:11756 [D loss: 0.501616, acc.: 75.00%] [G loss: 1.575543]\n",
      "epoch:12 step:11757 [D loss: 0.564339, acc.: 76.56%] [G loss: 1.046692]\n",
      "epoch:12 step:11758 [D loss: 0.497430, acc.: 79.69%] [G loss: 1.244665]\n",
      "epoch:12 step:11759 [D loss: 0.609765, acc.: 70.31%] [G loss: 1.075252]\n",
      "epoch:12 step:11760 [D loss: 0.572935, acc.: 69.53%] [G loss: 1.211262]\n",
      "epoch:12 step:11761 [D loss: 0.797855, acc.: 44.53%] [G loss: 1.213229]\n",
      "epoch:12 step:11762 [D loss: 0.639520, acc.: 67.97%] [G loss: 1.581398]\n",
      "epoch:12 step:11763 [D loss: 0.643071, acc.: 64.84%] [G loss: 1.246040]\n",
      "epoch:12 step:11764 [D loss: 0.645394, acc.: 62.50%] [G loss: 1.189502]\n",
      "epoch:12 step:11765 [D loss: 0.594933, acc.: 71.88%] [G loss: 1.133728]\n",
      "epoch:12 step:11766 [D loss: 0.639595, acc.: 64.84%] [G loss: 1.120126]\n",
      "epoch:12 step:11767 [D loss: 0.698012, acc.: 58.59%] [G loss: 1.086555]\n",
      "epoch:12 step:11768 [D loss: 0.563506, acc.: 72.66%] [G loss: 1.493356]\n",
      "epoch:12 step:11769 [D loss: 0.546773, acc.: 71.88%] [G loss: 1.464727]\n",
      "epoch:12 step:11770 [D loss: 0.677604, acc.: 58.59%] [G loss: 1.213490]\n",
      "epoch:12 step:11771 [D loss: 0.614505, acc.: 64.06%] [G loss: 1.189894]\n",
      "epoch:12 step:11772 [D loss: 0.572348, acc.: 67.97%] [G loss: 1.251265]\n",
      "epoch:12 step:11773 [D loss: 0.517923, acc.: 74.22%] [G loss: 1.350487]\n",
      "epoch:12 step:11774 [D loss: 0.665933, acc.: 64.06%] [G loss: 1.263219]\n",
      "epoch:12 step:11775 [D loss: 0.501191, acc.: 76.56%] [G loss: 1.267532]\n",
      "epoch:12 step:11776 [D loss: 0.653830, acc.: 61.72%] [G loss: 1.323059]\n",
      "epoch:12 step:11777 [D loss: 0.624000, acc.: 64.84%] [G loss: 1.401654]\n",
      "epoch:12 step:11778 [D loss: 0.697951, acc.: 59.38%] [G loss: 1.094898]\n",
      "epoch:12 step:11779 [D loss: 0.496050, acc.: 78.12%] [G loss: 1.322303]\n",
      "epoch:12 step:11780 [D loss: 0.516082, acc.: 77.34%] [G loss: 1.069267]\n",
      "epoch:12 step:11781 [D loss: 0.706457, acc.: 60.16%] [G loss: 1.176949]\n",
      "epoch:12 step:11782 [D loss: 0.641243, acc.: 61.72%] [G loss: 1.210515]\n",
      "epoch:12 step:11783 [D loss: 0.512635, acc.: 78.91%] [G loss: 1.238486]\n",
      "epoch:12 step:11784 [D loss: 0.575813, acc.: 66.41%] [G loss: 1.383617]\n",
      "epoch:12 step:11785 [D loss: 0.559984, acc.: 73.44%] [G loss: 1.009221]\n",
      "epoch:12 step:11786 [D loss: 0.566449, acc.: 68.75%] [G loss: 1.330650]\n",
      "epoch:12 step:11787 [D loss: 0.585937, acc.: 71.88%] [G loss: 1.044974]\n",
      "epoch:12 step:11788 [D loss: 0.618180, acc.: 67.97%] [G loss: 1.339657]\n",
      "epoch:12 step:11789 [D loss: 0.643930, acc.: 64.06%] [G loss: 1.120692]\n",
      "epoch:12 step:11790 [D loss: 0.498846, acc.: 77.34%] [G loss: 1.229293]\n",
      "epoch:12 step:11791 [D loss: 0.658140, acc.: 64.06%] [G loss: 1.105296]\n",
      "epoch:12 step:11792 [D loss: 0.581025, acc.: 75.00%] [G loss: 1.273422]\n",
      "epoch:12 step:11793 [D loss: 0.509291, acc.: 74.22%] [G loss: 1.001436]\n",
      "epoch:12 step:11794 [D loss: 0.606287, acc.: 67.97%] [G loss: 1.194072]\n",
      "epoch:12 step:11795 [D loss: 0.515056, acc.: 79.69%] [G loss: 1.265521]\n",
      "epoch:12 step:11796 [D loss: 0.605573, acc.: 68.75%] [G loss: 1.273679]\n",
      "epoch:12 step:11797 [D loss: 0.641035, acc.: 59.38%] [G loss: 1.172039]\n",
      "epoch:12 step:11798 [D loss: 0.595932, acc.: 67.19%] [G loss: 1.205029]\n",
      "epoch:12 step:11799 [D loss: 0.618043, acc.: 64.84%] [G loss: 1.355697]\n",
      "epoch:12 step:11800 [D loss: 0.614262, acc.: 65.62%] [G loss: 1.255520]\n",
      "##############\n",
      "[2.65597679 2.18953841 1.68440502 2.67459982 0.97739501 5.79378978\n",
      " 1.94591064 2.56375642 3.83024974 8.14868929]\n",
      "##########\n",
      "epoch:12 step:11801 [D loss: 0.625806, acc.: 66.41%] [G loss: 1.253656]\n",
      "epoch:12 step:11802 [D loss: 0.716857, acc.: 58.59%] [G loss: 1.206003]\n",
      "epoch:12 step:11803 [D loss: 0.480490, acc.: 76.56%] [G loss: 1.431620]\n",
      "epoch:12 step:11804 [D loss: 0.512476, acc.: 75.00%] [G loss: 1.172653]\n",
      "epoch:12 step:11805 [D loss: 0.666590, acc.: 60.16%] [G loss: 1.136322]\n",
      "epoch:12 step:11806 [D loss: 0.596453, acc.: 66.41%] [G loss: 1.289434]\n",
      "epoch:12 step:11807 [D loss: 0.623114, acc.: 64.06%] [G loss: 1.467931]\n",
      "epoch:12 step:11808 [D loss: 0.494637, acc.: 77.34%] [G loss: 1.259088]\n",
      "epoch:12 step:11809 [D loss: 0.622226, acc.: 68.75%] [G loss: 1.073493]\n",
      "epoch:12 step:11810 [D loss: 0.595690, acc.: 67.97%] [G loss: 1.064067]\n",
      "epoch:12 step:11811 [D loss: 0.708001, acc.: 56.25%] [G loss: 1.132614]\n",
      "epoch:12 step:11812 [D loss: 0.604773, acc.: 66.41%] [G loss: 1.166533]\n",
      "epoch:12 step:11813 [D loss: 0.530940, acc.: 72.66%] [G loss: 1.298366]\n",
      "epoch:12 step:11814 [D loss: 0.756609, acc.: 50.78%] [G loss: 1.065049]\n",
      "epoch:12 step:11815 [D loss: 0.568484, acc.: 67.19%] [G loss: 1.250423]\n",
      "epoch:12 step:11816 [D loss: 0.656516, acc.: 65.62%] [G loss: 1.117910]\n",
      "epoch:12 step:11817 [D loss: 0.755176, acc.: 53.91%] [G loss: 1.182424]\n",
      "epoch:12 step:11818 [D loss: 0.619669, acc.: 68.75%] [G loss: 0.948058]\n",
      "epoch:12 step:11819 [D loss: 0.669494, acc.: 57.81%] [G loss: 1.092527]\n",
      "epoch:12 step:11820 [D loss: 0.524582, acc.: 76.56%] [G loss: 1.301487]\n",
      "epoch:12 step:11821 [D loss: 0.526568, acc.: 80.47%] [G loss: 1.428170]\n",
      "epoch:12 step:11822 [D loss: 0.572343, acc.: 74.22%] [G loss: 1.400812]\n",
      "epoch:12 step:11823 [D loss: 0.498438, acc.: 75.78%] [G loss: 1.362893]\n",
      "epoch:12 step:11824 [D loss: 0.539821, acc.: 73.44%] [G loss: 1.032167]\n",
      "epoch:12 step:11825 [D loss: 0.565596, acc.: 71.09%] [G loss: 1.194030]\n",
      "epoch:12 step:11826 [D loss: 0.554289, acc.: 70.31%] [G loss: 1.260415]\n",
      "epoch:12 step:11827 [D loss: 0.485231, acc.: 79.69%] [G loss: 1.443156]\n",
      "epoch:12 step:11828 [D loss: 0.665184, acc.: 64.84%] [G loss: 1.175943]\n",
      "epoch:12 step:11829 [D loss: 0.620200, acc.: 65.62%] [G loss: 1.192189]\n",
      "epoch:12 step:11830 [D loss: 0.467458, acc.: 80.47%] [G loss: 1.455960]\n",
      "epoch:12 step:11831 [D loss: 0.639283, acc.: 63.28%] [G loss: 1.187264]\n",
      "epoch:12 step:11832 [D loss: 0.707100, acc.: 60.16%] [G loss: 1.197399]\n",
      "epoch:12 step:11833 [D loss: 0.722847, acc.: 59.38%] [G loss: 1.084254]\n",
      "epoch:12 step:11834 [D loss: 0.565503, acc.: 76.56%] [G loss: 1.368610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11835 [D loss: 0.562534, acc.: 71.09%] [G loss: 1.245893]\n",
      "epoch:12 step:11836 [D loss: 0.558300, acc.: 73.44%] [G loss: 1.346311]\n",
      "epoch:12 step:11837 [D loss: 0.535790, acc.: 75.78%] [G loss: 1.212073]\n",
      "epoch:12 step:11838 [D loss: 0.531463, acc.: 71.88%] [G loss: 1.090989]\n",
      "epoch:12 step:11839 [D loss: 0.538482, acc.: 71.88%] [G loss: 1.307166]\n",
      "epoch:12 step:11840 [D loss: 0.589978, acc.: 72.66%] [G loss: 1.092959]\n",
      "epoch:12 step:11841 [D loss: 0.611975, acc.: 65.62%] [G loss: 1.137863]\n",
      "epoch:12 step:11842 [D loss: 0.649760, acc.: 67.97%] [G loss: 1.424082]\n",
      "epoch:12 step:11843 [D loss: 0.459454, acc.: 82.03%] [G loss: 1.181039]\n",
      "epoch:12 step:11844 [D loss: 0.538576, acc.: 72.66%] [G loss: 1.138728]\n",
      "epoch:12 step:11845 [D loss: 0.630027, acc.: 64.84%] [G loss: 1.256081]\n",
      "epoch:12 step:11846 [D loss: 0.565916, acc.: 75.00%] [G loss: 1.417941]\n",
      "epoch:12 step:11847 [D loss: 0.650576, acc.: 64.84%] [G loss: 1.032178]\n",
      "epoch:12 step:11848 [D loss: 0.543277, acc.: 73.44%] [G loss: 1.039884]\n",
      "epoch:12 step:11849 [D loss: 0.555633, acc.: 75.78%] [G loss: 1.297788]\n",
      "epoch:12 step:11850 [D loss: 0.663441, acc.: 62.50%] [G loss: 1.289667]\n",
      "epoch:12 step:11851 [D loss: 0.555149, acc.: 71.88%] [G loss: 1.108026]\n",
      "epoch:12 step:11852 [D loss: 0.615400, acc.: 67.97%] [G loss: 1.227311]\n",
      "epoch:12 step:11853 [D loss: 0.569426, acc.: 67.97%] [G loss: 1.111082]\n",
      "epoch:12 step:11854 [D loss: 0.535013, acc.: 75.78%] [G loss: 1.500305]\n",
      "epoch:12 step:11855 [D loss: 0.657486, acc.: 60.16%] [G loss: 1.391105]\n",
      "epoch:12 step:11856 [D loss: 0.639869, acc.: 63.28%] [G loss: 1.035051]\n",
      "epoch:12 step:11857 [D loss: 0.566517, acc.: 71.88%] [G loss: 1.271104]\n",
      "epoch:12 step:11858 [D loss: 0.621782, acc.: 64.84%] [G loss: 1.032709]\n",
      "epoch:12 step:11859 [D loss: 0.618647, acc.: 64.84%] [G loss: 1.241669]\n",
      "epoch:12 step:11860 [D loss: 0.564163, acc.: 72.66%] [G loss: 1.268676]\n",
      "epoch:12 step:11861 [D loss: 0.602351, acc.: 67.19%] [G loss: 1.288956]\n",
      "epoch:12 step:11862 [D loss: 0.606713, acc.: 68.75%] [G loss: 1.321276]\n",
      "epoch:12 step:11863 [D loss: 0.609081, acc.: 70.31%] [G loss: 1.158255]\n",
      "epoch:12 step:11864 [D loss: 0.704593, acc.: 55.47%] [G loss: 0.982195]\n",
      "epoch:12 step:11865 [D loss: 0.661587, acc.: 60.16%] [G loss: 1.230422]\n",
      "epoch:12 step:11866 [D loss: 0.596204, acc.: 64.06%] [G loss: 0.952149]\n",
      "epoch:12 step:11867 [D loss: 0.608649, acc.: 66.41%] [G loss: 0.950842]\n",
      "epoch:12 step:11868 [D loss: 0.567348, acc.: 69.53%] [G loss: 1.192262]\n",
      "epoch:12 step:11869 [D loss: 0.534368, acc.: 74.22%] [G loss: 1.165211]\n",
      "epoch:12 step:11870 [D loss: 0.461434, acc.: 82.03%] [G loss: 1.035090]\n",
      "epoch:12 step:11871 [D loss: 0.605564, acc.: 66.41%] [G loss: 1.102961]\n",
      "epoch:12 step:11872 [D loss: 0.703371, acc.: 53.91%] [G loss: 1.247062]\n",
      "epoch:12 step:11873 [D loss: 0.496651, acc.: 79.69%] [G loss: 1.186020]\n",
      "epoch:12 step:11874 [D loss: 0.662195, acc.: 61.72%] [G loss: 1.005646]\n",
      "epoch:12 step:11875 [D loss: 0.639188, acc.: 62.50%] [G loss: 1.031142]\n",
      "epoch:12 step:11876 [D loss: 0.523759, acc.: 74.22%] [G loss: 1.319424]\n",
      "epoch:12 step:11877 [D loss: 0.564112, acc.: 72.66%] [G loss: 1.192674]\n",
      "epoch:12 step:11878 [D loss: 0.490902, acc.: 78.12%] [G loss: 1.591462]\n",
      "epoch:12 step:11879 [D loss: 0.477904, acc.: 78.12%] [G loss: 1.486124]\n",
      "epoch:12 step:11880 [D loss: 0.528576, acc.: 71.88%] [G loss: 1.242518]\n",
      "epoch:12 step:11881 [D loss: 0.683959, acc.: 59.38%] [G loss: 1.241023]\n",
      "epoch:12 step:11882 [D loss: 0.734551, acc.: 54.69%] [G loss: 1.142110]\n",
      "epoch:12 step:11883 [D loss: 0.677129, acc.: 62.50%] [G loss: 1.156061]\n",
      "epoch:12 step:11884 [D loss: 0.557194, acc.: 74.22%] [G loss: 1.050887]\n",
      "epoch:12 step:11885 [D loss: 0.559634, acc.: 71.88%] [G loss: 1.128332]\n",
      "epoch:12 step:11886 [D loss: 0.645958, acc.: 62.50%] [G loss: 1.025445]\n",
      "epoch:12 step:11887 [D loss: 0.757168, acc.: 55.47%] [G loss: 1.013594]\n",
      "epoch:12 step:11888 [D loss: 0.632381, acc.: 68.75%] [G loss: 1.350609]\n",
      "epoch:12 step:11889 [D loss: 0.576627, acc.: 72.66%] [G loss: 1.255423]\n",
      "epoch:12 step:11890 [D loss: 0.489771, acc.: 77.34%] [G loss: 1.281950]\n",
      "epoch:12 step:11891 [D loss: 0.551020, acc.: 75.00%] [G loss: 1.270102]\n",
      "epoch:12 step:11892 [D loss: 0.578476, acc.: 70.31%] [G loss: 1.281716]\n",
      "epoch:12 step:11893 [D loss: 0.546595, acc.: 72.66%] [G loss: 1.325826]\n",
      "epoch:12 step:11894 [D loss: 0.611254, acc.: 64.06%] [G loss: 1.178254]\n",
      "epoch:12 step:11895 [D loss: 0.587661, acc.: 74.22%] [G loss: 1.153227]\n",
      "epoch:12 step:11896 [D loss: 0.602523, acc.: 64.84%] [G loss: 1.300368]\n",
      "epoch:12 step:11897 [D loss: 0.574696, acc.: 69.53%] [G loss: 1.361516]\n",
      "epoch:12 step:11898 [D loss: 0.597322, acc.: 65.62%] [G loss: 1.317656]\n",
      "epoch:12 step:11899 [D loss: 0.609118, acc.: 64.06%] [G loss: 1.370854]\n",
      "epoch:12 step:11900 [D loss: 0.512964, acc.: 75.00%] [G loss: 1.209457]\n",
      "epoch:12 step:11901 [D loss: 0.739915, acc.: 53.91%] [G loss: 1.213236]\n",
      "epoch:12 step:11902 [D loss: 0.692198, acc.: 60.16%] [G loss: 0.988622]\n",
      "epoch:12 step:11903 [D loss: 0.685622, acc.: 60.94%] [G loss: 1.137423]\n",
      "epoch:12 step:11904 [D loss: 0.555943, acc.: 75.78%] [G loss: 1.325683]\n",
      "epoch:12 step:11905 [D loss: 0.573301, acc.: 72.66%] [G loss: 1.168653]\n",
      "epoch:12 step:11906 [D loss: 0.553295, acc.: 71.88%] [G loss: 1.272725]\n",
      "epoch:12 step:11907 [D loss: 0.589181, acc.: 66.41%] [G loss: 1.310196]\n",
      "epoch:12 step:11908 [D loss: 0.552170, acc.: 75.78%] [G loss: 1.290227]\n",
      "epoch:12 step:11909 [D loss: 0.627950, acc.: 68.75%] [G loss: 1.142099]\n",
      "epoch:12 step:11910 [D loss: 0.573112, acc.: 66.41%] [G loss: 0.975087]\n",
      "epoch:12 step:11911 [D loss: 0.599845, acc.: 68.75%] [G loss: 1.270307]\n",
      "epoch:12 step:11912 [D loss: 0.757897, acc.: 50.78%] [G loss: 1.031765]\n",
      "epoch:12 step:11913 [D loss: 0.574097, acc.: 64.84%] [G loss: 1.357735]\n",
      "epoch:12 step:11914 [D loss: 0.511057, acc.: 76.56%] [G loss: 0.916069]\n",
      "epoch:12 step:11915 [D loss: 0.631996, acc.: 63.28%] [G loss: 1.223410]\n",
      "epoch:12 step:11916 [D loss: 0.569325, acc.: 67.19%] [G loss: 1.118416]\n",
      "epoch:12 step:11917 [D loss: 0.697026, acc.: 62.50%] [G loss: 0.916482]\n",
      "epoch:12 step:11918 [D loss: 0.559770, acc.: 71.88%] [G loss: 1.151668]\n",
      "epoch:12 step:11919 [D loss: 0.639084, acc.: 64.06%] [G loss: 1.198956]\n",
      "epoch:12 step:11920 [D loss: 0.525274, acc.: 75.78%] [G loss: 1.403899]\n",
      "epoch:12 step:11921 [D loss: 0.500098, acc.: 76.56%] [G loss: 1.320424]\n",
      "epoch:12 step:11922 [D loss: 0.549894, acc.: 71.09%] [G loss: 1.138181]\n",
      "epoch:12 step:11923 [D loss: 0.541428, acc.: 74.22%] [G loss: 1.269943]\n",
      "epoch:12 step:11924 [D loss: 0.669287, acc.: 64.06%] [G loss: 1.294208]\n",
      "epoch:12 step:11925 [D loss: 0.547520, acc.: 71.88%] [G loss: 1.168774]\n",
      "epoch:12 step:11926 [D loss: 0.544205, acc.: 72.66%] [G loss: 1.141513]\n",
      "epoch:12 step:11927 [D loss: 0.666207, acc.: 60.16%] [G loss: 1.222235]\n",
      "epoch:12 step:11928 [D loss: 0.654953, acc.: 60.94%] [G loss: 1.225256]\n",
      "epoch:12 step:11929 [D loss: 0.569033, acc.: 67.97%] [G loss: 1.004868]\n",
      "epoch:12 step:11930 [D loss: 0.598259, acc.: 71.09%] [G loss: 1.586647]\n",
      "epoch:12 step:11931 [D loss: 0.696817, acc.: 57.03%] [G loss: 1.183363]\n",
      "epoch:12 step:11932 [D loss: 0.506558, acc.: 75.78%] [G loss: 1.193763]\n",
      "epoch:12 step:11933 [D loss: 0.618260, acc.: 66.41%] [G loss: 1.143720]\n",
      "epoch:12 step:11934 [D loss: 0.602550, acc.: 66.41%] [G loss: 1.398004]\n",
      "epoch:12 step:11935 [D loss: 0.696103, acc.: 57.03%] [G loss: 1.280908]\n",
      "epoch:12 step:11936 [D loss: 0.487843, acc.: 82.81%] [G loss: 1.296494]\n",
      "epoch:12 step:11937 [D loss: 0.729713, acc.: 54.69%] [G loss: 1.295192]\n",
      "epoch:12 step:11938 [D loss: 0.623127, acc.: 65.62%] [G loss: 1.158419]\n",
      "epoch:12 step:11939 [D loss: 0.473310, acc.: 78.12%] [G loss: 1.347164]\n",
      "epoch:12 step:11940 [D loss: 0.575137, acc.: 71.09%] [G loss: 1.328005]\n",
      "epoch:12 step:11941 [D loss: 0.636603, acc.: 64.84%] [G loss: 1.119954]\n",
      "epoch:12 step:11942 [D loss: 0.583941, acc.: 67.19%] [G loss: 1.199518]\n",
      "epoch:12 step:11943 [D loss: 0.554308, acc.: 72.66%] [G loss: 1.350487]\n",
      "epoch:12 step:11944 [D loss: 0.577079, acc.: 68.75%] [G loss: 0.976334]\n",
      "epoch:12 step:11945 [D loss: 0.520937, acc.: 76.56%] [G loss: 1.252679]\n",
      "epoch:12 step:11946 [D loss: 0.415746, acc.: 89.06%] [G loss: 1.124257]\n",
      "epoch:12 step:11947 [D loss: 0.666619, acc.: 62.50%] [G loss: 1.237809]\n",
      "epoch:12 step:11948 [D loss: 0.706008, acc.: 57.03%] [G loss: 1.041031]\n",
      "epoch:12 step:11949 [D loss: 0.528738, acc.: 78.12%] [G loss: 1.129384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11950 [D loss: 0.526651, acc.: 73.44%] [G loss: 1.484166]\n",
      "epoch:12 step:11951 [D loss: 0.486791, acc.: 81.25%] [G loss: 1.078610]\n",
      "epoch:12 step:11952 [D loss: 0.639607, acc.: 64.84%] [G loss: 1.014891]\n",
      "epoch:12 step:11953 [D loss: 0.685696, acc.: 54.69%] [G loss: 1.276668]\n",
      "epoch:12 step:11954 [D loss: 0.538207, acc.: 75.00%] [G loss: 1.274652]\n",
      "epoch:12 step:11955 [D loss: 0.507479, acc.: 74.22%] [G loss: 1.216804]\n",
      "epoch:12 step:11956 [D loss: 0.514816, acc.: 72.66%] [G loss: 1.277559]\n",
      "epoch:12 step:11957 [D loss: 0.463298, acc.: 81.25%] [G loss: 1.228326]\n",
      "epoch:12 step:11958 [D loss: 0.633367, acc.: 66.41%] [G loss: 1.251722]\n",
      "epoch:12 step:11959 [D loss: 0.809850, acc.: 42.97%] [G loss: 0.699712]\n",
      "epoch:12 step:11960 [D loss: 0.546281, acc.: 72.66%] [G loss: 1.034859]\n",
      "epoch:12 step:11961 [D loss: 0.578896, acc.: 71.09%] [G loss: 1.213513]\n",
      "epoch:12 step:11962 [D loss: 0.516510, acc.: 78.91%] [G loss: 1.081693]\n",
      "epoch:12 step:11963 [D loss: 0.687697, acc.: 56.25%] [G loss: 1.321310]\n",
      "epoch:12 step:11964 [D loss: 0.674319, acc.: 60.94%] [G loss: 1.235035]\n",
      "epoch:12 step:11965 [D loss: 0.663055, acc.: 60.16%] [G loss: 1.230358]\n",
      "epoch:12 step:11966 [D loss: 0.548728, acc.: 75.78%] [G loss: 1.469796]\n",
      "epoch:12 step:11967 [D loss: 0.501209, acc.: 78.12%] [G loss: 1.216965]\n",
      "epoch:12 step:11968 [D loss: 0.527439, acc.: 76.56%] [G loss: 1.243134]\n",
      "epoch:12 step:11969 [D loss: 0.727246, acc.: 53.91%] [G loss: 0.933927]\n",
      "epoch:12 step:11970 [D loss: 0.694692, acc.: 53.91%] [G loss: 1.046964]\n",
      "epoch:12 step:11971 [D loss: 0.549664, acc.: 75.00%] [G loss: 1.374089]\n",
      "epoch:12 step:11972 [D loss: 0.557685, acc.: 68.75%] [G loss: 1.167321]\n",
      "epoch:12 step:11973 [D loss: 0.608193, acc.: 71.09%] [G loss: 1.254709]\n",
      "epoch:12 step:11974 [D loss: 0.720852, acc.: 55.47%] [G loss: 1.068014]\n",
      "epoch:12 step:11975 [D loss: 0.527796, acc.: 74.22%] [G loss: 1.369849]\n",
      "epoch:12 step:11976 [D loss: 0.542200, acc.: 71.88%] [G loss: 1.050095]\n",
      "epoch:12 step:11977 [D loss: 0.615518, acc.: 64.06%] [G loss: 1.134310]\n",
      "epoch:12 step:11978 [D loss: 0.594325, acc.: 62.50%] [G loss: 1.054260]\n",
      "epoch:12 step:11979 [D loss: 0.511858, acc.: 78.12%] [G loss: 1.252777]\n",
      "epoch:12 step:11980 [D loss: 0.410055, acc.: 89.84%] [G loss: 1.320007]\n",
      "epoch:12 step:11981 [D loss: 0.693208, acc.: 60.94%] [G loss: 1.044102]\n",
      "epoch:12 step:11982 [D loss: 0.683410, acc.: 56.25%] [G loss: 1.214709]\n",
      "epoch:12 step:11983 [D loss: 0.612707, acc.: 65.62%] [G loss: 1.071058]\n",
      "epoch:12 step:11984 [D loss: 0.626523, acc.: 67.19%] [G loss: 0.898878]\n",
      "epoch:12 step:11985 [D loss: 0.616059, acc.: 71.09%] [G loss: 1.341525]\n",
      "epoch:12 step:11986 [D loss: 0.582990, acc.: 71.09%] [G loss: 1.290008]\n",
      "epoch:12 step:11987 [D loss: 0.671923, acc.: 63.28%] [G loss: 1.167038]\n",
      "epoch:12 step:11988 [D loss: 0.595415, acc.: 61.72%] [G loss: 1.028084]\n",
      "epoch:12 step:11989 [D loss: 0.534694, acc.: 78.12%] [G loss: 1.268594]\n",
      "epoch:12 step:11990 [D loss: 0.594980, acc.: 71.09%] [G loss: 1.167145]\n",
      "epoch:12 step:11991 [D loss: 0.573102, acc.: 67.97%] [G loss: 1.358728]\n",
      "epoch:12 step:11992 [D loss: 0.456602, acc.: 82.81%] [G loss: 1.285966]\n",
      "epoch:12 step:11993 [D loss: 0.592564, acc.: 72.66%] [G loss: 1.052212]\n",
      "epoch:12 step:11994 [D loss: 0.743795, acc.: 59.38%] [G loss: 1.010476]\n",
      "epoch:12 step:11995 [D loss: 0.610529, acc.: 68.75%] [G loss: 1.165024]\n",
      "epoch:12 step:11996 [D loss: 0.640153, acc.: 69.53%] [G loss: 1.117838]\n",
      "epoch:12 step:11997 [D loss: 0.567953, acc.: 72.66%] [G loss: 1.302077]\n",
      "epoch:12 step:11998 [D loss: 0.613065, acc.: 65.62%] [G loss: 1.119597]\n",
      "epoch:12 step:11999 [D loss: 0.627452, acc.: 64.84%] [G loss: 0.898362]\n",
      "epoch:12 step:12000 [D loss: 0.606768, acc.: 64.06%] [G loss: 0.986877]\n",
      "##############\n",
      "[2.73817604 2.0908354  2.0054209  2.90831944 0.65853295 5.73877148\n",
      " 2.11116262 2.57799577 4.01044416 5.39417491]\n",
      "##########\n",
      "epoch:12 step:12001 [D loss: 0.596969, acc.: 67.19%] [G loss: 1.214305]\n",
      "epoch:12 step:12002 [D loss: 0.482421, acc.: 78.91%] [G loss: 1.493075]\n",
      "epoch:12 step:12003 [D loss: 0.543594, acc.: 75.00%] [G loss: 1.306165]\n",
      "epoch:12 step:12004 [D loss: 0.596491, acc.: 67.19%] [G loss: 1.253263]\n",
      "epoch:12 step:12005 [D loss: 0.725930, acc.: 45.31%] [G loss: 1.220851]\n",
      "epoch:12 step:12006 [D loss: 0.536204, acc.: 75.00%] [G loss: 1.231516]\n",
      "epoch:12 step:12007 [D loss: 0.704896, acc.: 57.81%] [G loss: 1.331997]\n",
      "epoch:12 step:12008 [D loss: 0.564054, acc.: 72.66%] [G loss: 1.142280]\n",
      "epoch:12 step:12009 [D loss: 0.655979, acc.: 64.06%] [G loss: 0.918248]\n",
      "epoch:12 step:12010 [D loss: 0.470631, acc.: 82.03%] [G loss: 1.292464]\n",
      "epoch:12 step:12011 [D loss: 0.577043, acc.: 71.09%] [G loss: 1.227946]\n",
      "epoch:12 step:12012 [D loss: 0.543767, acc.: 71.88%] [G loss: 1.294624]\n",
      "epoch:12 step:12013 [D loss: 0.510951, acc.: 77.34%] [G loss: 1.464558]\n",
      "epoch:12 step:12014 [D loss: 0.589033, acc.: 67.97%] [G loss: 1.252427]\n",
      "epoch:12 step:12015 [D loss: 0.592704, acc.: 65.62%] [G loss: 1.083261]\n",
      "epoch:12 step:12016 [D loss: 0.579618, acc.: 61.72%] [G loss: 1.086614]\n",
      "epoch:12 step:12017 [D loss: 0.545602, acc.: 76.56%] [G loss: 1.282076]\n",
      "epoch:12 step:12018 [D loss: 0.625987, acc.: 64.06%] [G loss: 1.138453]\n",
      "epoch:12 step:12019 [D loss: 0.721190, acc.: 61.72%] [G loss: 0.854267]\n",
      "epoch:12 step:12020 [D loss: 0.573802, acc.: 65.62%] [G loss: 1.337456]\n",
      "epoch:12 step:12021 [D loss: 0.515992, acc.: 78.91%] [G loss: 1.230818]\n",
      "epoch:12 step:12022 [D loss: 0.602407, acc.: 71.09%] [G loss: 1.158164]\n",
      "epoch:12 step:12023 [D loss: 0.645860, acc.: 61.72%] [G loss: 0.929747]\n",
      "epoch:12 step:12024 [D loss: 0.655826, acc.: 60.94%] [G loss: 1.066058]\n",
      "epoch:12 step:12025 [D loss: 0.617691, acc.: 67.97%] [G loss: 1.327416]\n",
      "epoch:12 step:12026 [D loss: 0.563497, acc.: 70.31%] [G loss: 1.059833]\n",
      "epoch:12 step:12027 [D loss: 0.532530, acc.: 76.56%] [G loss: 1.380048]\n",
      "epoch:12 step:12028 [D loss: 0.574674, acc.: 67.19%] [G loss: 1.214122]\n",
      "epoch:12 step:12029 [D loss: 0.575238, acc.: 67.97%] [G loss: 1.234549]\n",
      "epoch:12 step:12030 [D loss: 0.604382, acc.: 69.53%] [G loss: 1.157772]\n",
      "epoch:12 step:12031 [D loss: 0.612341, acc.: 67.19%] [G loss: 1.184997]\n",
      "epoch:12 step:12032 [D loss: 0.603604, acc.: 68.75%] [G loss: 1.268527]\n",
      "epoch:12 step:12033 [D loss: 0.552985, acc.: 70.31%] [G loss: 1.174918]\n",
      "epoch:12 step:12034 [D loss: 0.492249, acc.: 79.69%] [G loss: 1.364408]\n",
      "epoch:12 step:12035 [D loss: 0.624285, acc.: 64.06%] [G loss: 1.211352]\n",
      "epoch:12 step:12036 [D loss: 0.533377, acc.: 77.34%] [G loss: 1.446009]\n",
      "epoch:12 step:12037 [D loss: 0.635408, acc.: 61.72%] [G loss: 1.247395]\n",
      "epoch:12 step:12038 [D loss: 0.510061, acc.: 78.91%] [G loss: 1.224926]\n",
      "epoch:12 step:12039 [D loss: 0.525327, acc.: 79.69%] [G loss: 1.294215]\n",
      "epoch:12 step:12040 [D loss: 0.698894, acc.: 62.50%] [G loss: 1.294034]\n",
      "epoch:12 step:12041 [D loss: 0.652472, acc.: 58.59%] [G loss: 1.439332]\n",
      "epoch:12 step:12042 [D loss: 0.690754, acc.: 59.38%] [G loss: 1.182317]\n",
      "epoch:12 step:12043 [D loss: 0.546496, acc.: 77.34%] [G loss: 1.345042]\n",
      "epoch:12 step:12044 [D loss: 0.559079, acc.: 71.09%] [G loss: 1.122802]\n",
      "epoch:12 step:12045 [D loss: 0.654020, acc.: 57.03%] [G loss: 1.210883]\n",
      "epoch:12 step:12046 [D loss: 0.661122, acc.: 62.50%] [G loss: 1.203634]\n",
      "epoch:12 step:12047 [D loss: 0.535748, acc.: 72.66%] [G loss: 1.179658]\n",
      "epoch:12 step:12048 [D loss: 0.617885, acc.: 61.72%] [G loss: 1.258897]\n",
      "epoch:12 step:12049 [D loss: 0.561561, acc.: 73.44%] [G loss: 1.379776]\n",
      "epoch:12 step:12050 [D loss: 0.607591, acc.: 67.97%] [G loss: 1.236524]\n",
      "epoch:12 step:12051 [D loss: 0.494713, acc.: 76.56%] [G loss: 1.567252]\n",
      "epoch:12 step:12052 [D loss: 0.604464, acc.: 67.97%] [G loss: 1.185798]\n",
      "epoch:12 step:12053 [D loss: 0.524336, acc.: 75.78%] [G loss: 1.275286]\n",
      "epoch:12 step:12054 [D loss: 0.609621, acc.: 67.19%] [G loss: 1.246895]\n",
      "epoch:12 step:12055 [D loss: 0.568773, acc.: 70.31%] [G loss: 1.413795]\n",
      "epoch:12 step:12056 [D loss: 0.643484, acc.: 64.06%] [G loss: 1.132432]\n",
      "epoch:12 step:12057 [D loss: 0.496664, acc.: 78.12%] [G loss: 1.397029]\n",
      "epoch:12 step:12058 [D loss: 0.509649, acc.: 78.12%] [G loss: 1.298470]\n",
      "epoch:12 step:12059 [D loss: 0.633254, acc.: 60.94%] [G loss: 1.195155]\n",
      "epoch:12 step:12060 [D loss: 0.527233, acc.: 71.09%] [G loss: 0.998966]\n",
      "epoch:12 step:12061 [D loss: 0.523282, acc.: 70.31%] [G loss: 1.574789]\n",
      "epoch:12 step:12062 [D loss: 0.631809, acc.: 59.38%] [G loss: 1.046706]\n",
      "epoch:12 step:12063 [D loss: 0.509803, acc.: 75.78%] [G loss: 1.507894]\n",
      "epoch:12 step:12064 [D loss: 0.570903, acc.: 72.66%] [G loss: 1.096016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12065 [D loss: 0.748577, acc.: 50.00%] [G loss: 0.881611]\n",
      "epoch:12 step:12066 [D loss: 0.499700, acc.: 77.34%] [G loss: 1.189712]\n",
      "epoch:12 step:12067 [D loss: 0.742582, acc.: 57.81%] [G loss: 1.321986]\n",
      "epoch:12 step:12068 [D loss: 0.571004, acc.: 74.22%] [G loss: 1.240795]\n",
      "epoch:12 step:12069 [D loss: 0.519947, acc.: 75.78%] [G loss: 1.119933]\n",
      "epoch:12 step:12070 [D loss: 0.575700, acc.: 70.31%] [G loss: 1.219838]\n",
      "epoch:12 step:12071 [D loss: 0.569140, acc.: 73.44%] [G loss: 1.286690]\n",
      "epoch:12 step:12072 [D loss: 0.620185, acc.: 68.75%] [G loss: 1.231488]\n",
      "epoch:12 step:12073 [D loss: 0.586548, acc.: 71.09%] [G loss: 1.142634]\n",
      "epoch:12 step:12074 [D loss: 0.639535, acc.: 63.28%] [G loss: 1.223724]\n",
      "epoch:12 step:12075 [D loss: 0.562452, acc.: 68.75%] [G loss: 1.170006]\n",
      "epoch:12 step:12076 [D loss: 0.627807, acc.: 63.28%] [G loss: 1.374460]\n",
      "epoch:12 step:12077 [D loss: 0.563636, acc.: 72.66%] [G loss: 1.052093]\n",
      "epoch:12 step:12078 [D loss: 0.653124, acc.: 62.50%] [G loss: 1.280621]\n",
      "epoch:12 step:12079 [D loss: 0.610241, acc.: 67.97%] [G loss: 1.082690]\n",
      "epoch:12 step:12080 [D loss: 0.582177, acc.: 72.66%] [G loss: 1.234737]\n",
      "epoch:12 step:12081 [D loss: 0.707390, acc.: 59.38%] [G loss: 0.968815]\n",
      "epoch:12 step:12082 [D loss: 0.540148, acc.: 75.00%] [G loss: 1.157777]\n",
      "epoch:12 step:12083 [D loss: 0.707660, acc.: 56.25%] [G loss: 0.900391]\n",
      "epoch:12 step:12084 [D loss: 0.502267, acc.: 78.91%] [G loss: 1.105203]\n",
      "epoch:12 step:12085 [D loss: 0.572570, acc.: 71.88%] [G loss: 1.284863]\n",
      "epoch:12 step:12086 [D loss: 0.582270, acc.: 68.75%] [G loss: 1.115995]\n",
      "epoch:12 step:12087 [D loss: 0.616830, acc.: 67.97%] [G loss: 1.044812]\n",
      "epoch:12 step:12088 [D loss: 0.748212, acc.: 50.78%] [G loss: 1.118878]\n",
      "epoch:12 step:12089 [D loss: 0.648252, acc.: 62.50%] [G loss: 1.028330]\n",
      "epoch:12 step:12090 [D loss: 0.728402, acc.: 53.91%] [G loss: 1.153602]\n",
      "epoch:12 step:12091 [D loss: 0.603254, acc.: 68.75%] [G loss: 1.189876]\n",
      "epoch:12 step:12092 [D loss: 0.681047, acc.: 60.94%] [G loss: 1.582992]\n",
      "epoch:12 step:12093 [D loss: 0.578802, acc.: 70.31%] [G loss: 1.382532]\n",
      "epoch:12 step:12094 [D loss: 0.549403, acc.: 70.31%] [G loss: 1.085044]\n",
      "epoch:12 step:12095 [D loss: 0.692081, acc.: 60.16%] [G loss: 1.121157]\n",
      "epoch:12 step:12096 [D loss: 0.535717, acc.: 72.66%] [G loss: 0.891602]\n",
      "epoch:12 step:12097 [D loss: 0.609921, acc.: 66.41%] [G loss: 1.288357]\n",
      "epoch:12 step:12098 [D loss: 0.614913, acc.: 67.97%] [G loss: 0.955914]\n",
      "epoch:12 step:12099 [D loss: 0.582768, acc.: 70.31%] [G loss: 1.363437]\n",
      "epoch:12 step:12100 [D loss: 0.676001, acc.: 57.81%] [G loss: 0.999569]\n",
      "epoch:12 step:12101 [D loss: 0.715971, acc.: 57.81%] [G loss: 1.326621]\n",
      "epoch:12 step:12102 [D loss: 0.585504, acc.: 66.41%] [G loss: 1.065516]\n",
      "epoch:12 step:12103 [D loss: 0.513335, acc.: 72.66%] [G loss: 1.310834]\n",
      "epoch:12 step:12104 [D loss: 0.731094, acc.: 53.91%] [G loss: 1.078880]\n",
      "epoch:12 step:12105 [D loss: 0.606369, acc.: 70.31%] [G loss: 1.244478]\n",
      "epoch:12 step:12106 [D loss: 0.557832, acc.: 73.44%] [G loss: 1.446916]\n",
      "epoch:12 step:12107 [D loss: 0.570281, acc.: 72.66%] [G loss: 1.382016]\n",
      "epoch:12 step:12108 [D loss: 0.571013, acc.: 68.75%] [G loss: 1.301629]\n",
      "epoch:12 step:12109 [D loss: 0.581492, acc.: 73.44%] [G loss: 1.262030]\n",
      "epoch:12 step:12110 [D loss: 0.508372, acc.: 75.00%] [G loss: 1.381243]\n",
      "epoch:12 step:12111 [D loss: 0.564728, acc.: 71.09%] [G loss: 1.310750]\n",
      "epoch:12 step:12112 [D loss: 0.492200, acc.: 80.47%] [G loss: 1.121210]\n",
      "epoch:12 step:12113 [D loss: 0.461560, acc.: 82.03%] [G loss: 1.624287]\n",
      "epoch:12 step:12114 [D loss: 0.599335, acc.: 67.19%] [G loss: 0.958372]\n",
      "epoch:12 step:12115 [D loss: 0.613638, acc.: 69.53%] [G loss: 1.203599]\n",
      "epoch:12 step:12116 [D loss: 0.542334, acc.: 74.22%] [G loss: 1.446015]\n",
      "epoch:12 step:12117 [D loss: 0.470547, acc.: 78.12%] [G loss: 1.332465]\n",
      "epoch:12 step:12118 [D loss: 0.641523, acc.: 58.59%] [G loss: 1.022655]\n",
      "epoch:12 step:12119 [D loss: 0.639142, acc.: 67.97%] [G loss: 1.136986]\n",
      "epoch:12 step:12120 [D loss: 0.661767, acc.: 63.28%] [G loss: 1.203059]\n",
      "epoch:12 step:12121 [D loss: 0.498605, acc.: 75.00%] [G loss: 1.044491]\n",
      "epoch:12 step:12122 [D loss: 0.739650, acc.: 55.47%] [G loss: 1.220021]\n",
      "epoch:12 step:12123 [D loss: 0.532779, acc.: 75.78%] [G loss: 1.023257]\n",
      "epoch:12 step:12124 [D loss: 0.560302, acc.: 72.66%] [G loss: 1.036910]\n",
      "epoch:12 step:12125 [D loss: 0.761224, acc.: 46.88%] [G loss: 1.117758]\n",
      "epoch:12 step:12126 [D loss: 0.562359, acc.: 71.09%] [G loss: 0.953028]\n",
      "epoch:12 step:12127 [D loss: 0.561646, acc.: 70.31%] [G loss: 1.290258]\n",
      "epoch:12 step:12128 [D loss: 0.559999, acc.: 69.53%] [G loss: 1.500781]\n",
      "epoch:12 step:12129 [D loss: 0.588041, acc.: 70.31%] [G loss: 1.303518]\n",
      "epoch:12 step:12130 [D loss: 0.568012, acc.: 74.22%] [G loss: 1.013632]\n",
      "epoch:12 step:12131 [D loss: 0.586341, acc.: 69.53%] [G loss: 1.212244]\n",
      "epoch:12 step:12132 [D loss: 0.626432, acc.: 64.84%] [G loss: 1.189664]\n",
      "epoch:12 step:12133 [D loss: 0.672687, acc.: 60.16%] [G loss: 1.317367]\n",
      "epoch:12 step:12134 [D loss: 0.595796, acc.: 66.41%] [G loss: 1.190268]\n",
      "epoch:12 step:12135 [D loss: 0.595309, acc.: 71.09%] [G loss: 1.209254]\n",
      "epoch:12 step:12136 [D loss: 0.561460, acc.: 69.53%] [G loss: 1.388116]\n",
      "epoch:12 step:12137 [D loss: 0.636873, acc.: 64.84%] [G loss: 1.096946]\n",
      "epoch:12 step:12138 [D loss: 0.709076, acc.: 55.47%] [G loss: 1.199749]\n",
      "epoch:12 step:12139 [D loss: 0.525917, acc.: 80.47%] [G loss: 1.527363]\n",
      "epoch:12 step:12140 [D loss: 0.518881, acc.: 77.34%] [G loss: 1.493658]\n",
      "epoch:12 step:12141 [D loss: 0.576948, acc.: 72.66%] [G loss: 1.263618]\n",
      "epoch:12 step:12142 [D loss: 0.567361, acc.: 68.75%] [G loss: 1.252262]\n",
      "epoch:12 step:12143 [D loss: 0.510940, acc.: 71.09%] [G loss: 1.170985]\n",
      "epoch:12 step:12144 [D loss: 0.528148, acc.: 76.56%] [G loss: 1.224698]\n",
      "epoch:12 step:12145 [D loss: 0.620869, acc.: 66.41%] [G loss: 1.334134]\n",
      "epoch:12 step:12146 [D loss: 0.649790, acc.: 62.50%] [G loss: 1.244326]\n",
      "epoch:12 step:12147 [D loss: 0.559310, acc.: 74.22%] [G loss: 1.182486]\n",
      "epoch:12 step:12148 [D loss: 0.620484, acc.: 66.41%] [G loss: 1.437571]\n",
      "epoch:12 step:12149 [D loss: 0.672560, acc.: 59.38%] [G loss: 1.075130]\n",
      "epoch:12 step:12150 [D loss: 0.478011, acc.: 85.16%] [G loss: 1.025802]\n",
      "epoch:12 step:12151 [D loss: 0.591985, acc.: 66.41%] [G loss: 1.174392]\n",
      "epoch:12 step:12152 [D loss: 0.616864, acc.: 61.72%] [G loss: 1.101123]\n",
      "epoch:12 step:12153 [D loss: 0.556817, acc.: 75.00%] [G loss: 1.355245]\n",
      "epoch:12 step:12154 [D loss: 0.510641, acc.: 74.22%] [G loss: 1.237573]\n",
      "epoch:12 step:12155 [D loss: 0.621874, acc.: 67.19%] [G loss: 1.190213]\n",
      "epoch:12 step:12156 [D loss: 0.704158, acc.: 58.59%] [G loss: 1.128399]\n",
      "epoch:12 step:12157 [D loss: 0.513882, acc.: 82.03%] [G loss: 1.017442]\n",
      "epoch:12 step:12158 [D loss: 0.630783, acc.: 63.28%] [G loss: 1.124192]\n",
      "epoch:12 step:12159 [D loss: 0.592418, acc.: 64.06%] [G loss: 1.128017]\n",
      "epoch:12 step:12160 [D loss: 0.469551, acc.: 82.03%] [G loss: 1.323611]\n",
      "epoch:12 step:12161 [D loss: 0.701409, acc.: 58.59%] [G loss: 1.176203]\n",
      "epoch:12 step:12162 [D loss: 0.559762, acc.: 71.09%] [G loss: 1.101025]\n",
      "epoch:12 step:12163 [D loss: 0.598424, acc.: 71.09%] [G loss: 1.144027]\n",
      "epoch:12 step:12164 [D loss: 0.621880, acc.: 63.28%] [G loss: 1.282814]\n",
      "epoch:12 step:12165 [D loss: 0.569789, acc.: 69.53%] [G loss: 1.110303]\n",
      "epoch:12 step:12166 [D loss: 0.593767, acc.: 64.84%] [G loss: 1.307041]\n",
      "epoch:12 step:12167 [D loss: 0.592813, acc.: 70.31%] [G loss: 1.256707]\n",
      "epoch:12 step:12168 [D loss: 0.662475, acc.: 58.59%] [G loss: 1.141435]\n",
      "epoch:12 step:12169 [D loss: 0.692693, acc.: 57.81%] [G loss: 1.042545]\n",
      "epoch:12 step:12170 [D loss: 0.525425, acc.: 67.19%] [G loss: 1.496035]\n",
      "epoch:12 step:12171 [D loss: 0.559418, acc.: 71.88%] [G loss: 1.148563]\n",
      "epoch:12 step:12172 [D loss: 0.537875, acc.: 73.44%] [G loss: 1.125942]\n",
      "epoch:12 step:12173 [D loss: 0.533526, acc.: 71.09%] [G loss: 1.292216]\n",
      "epoch:12 step:12174 [D loss: 0.588169, acc.: 71.09%] [G loss: 1.382306]\n",
      "epoch:12 step:12175 [D loss: 0.557413, acc.: 71.88%] [G loss: 1.104746]\n",
      "epoch:12 step:12176 [D loss: 0.583107, acc.: 68.75%] [G loss: 1.299126]\n",
      "epoch:12 step:12177 [D loss: 0.662182, acc.: 62.50%] [G loss: 1.160622]\n",
      "epoch:12 step:12178 [D loss: 0.514886, acc.: 78.12%] [G loss: 0.949973]\n",
      "epoch:12 step:12179 [D loss: 0.606820, acc.: 64.84%] [G loss: 1.110933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12180 [D loss: 0.427909, acc.: 84.38%] [G loss: 1.304691]\n",
      "epoch:12 step:12181 [D loss: 0.636547, acc.: 62.50%] [G loss: 1.063092]\n",
      "epoch:13 step:12182 [D loss: 0.601167, acc.: 64.06%] [G loss: 1.232008]\n",
      "epoch:13 step:12183 [D loss: 0.676939, acc.: 61.72%] [G loss: 1.045151]\n",
      "epoch:13 step:12184 [D loss: 0.635455, acc.: 66.41%] [G loss: 1.069802]\n",
      "epoch:13 step:12185 [D loss: 0.543544, acc.: 75.78%] [G loss: 1.157977]\n",
      "epoch:13 step:12186 [D loss: 0.568136, acc.: 70.31%] [G loss: 1.298174]\n",
      "epoch:13 step:12187 [D loss: 0.661524, acc.: 60.94%] [G loss: 1.268055]\n",
      "epoch:13 step:12188 [D loss: 0.685133, acc.: 57.81%] [G loss: 1.262481]\n",
      "epoch:13 step:12189 [D loss: 0.625427, acc.: 64.84%] [G loss: 1.137317]\n",
      "epoch:13 step:12190 [D loss: 0.586346, acc.: 71.88%] [G loss: 1.312554]\n",
      "epoch:13 step:12191 [D loss: 0.705030, acc.: 56.25%] [G loss: 1.109797]\n",
      "epoch:13 step:12192 [D loss: 0.627568, acc.: 69.53%] [G loss: 1.422783]\n",
      "epoch:13 step:12193 [D loss: 0.521155, acc.: 77.34%] [G loss: 1.298691]\n",
      "epoch:13 step:12194 [D loss: 0.695130, acc.: 57.81%] [G loss: 1.245511]\n",
      "epoch:13 step:12195 [D loss: 0.561497, acc.: 71.88%] [G loss: 1.285935]\n",
      "epoch:13 step:12196 [D loss: 0.431427, acc.: 85.94%] [G loss: 1.348224]\n",
      "epoch:13 step:12197 [D loss: 0.529441, acc.: 75.00%] [G loss: 1.107012]\n",
      "epoch:13 step:12198 [D loss: 0.594326, acc.: 65.62%] [G loss: 1.284431]\n",
      "epoch:13 step:12199 [D loss: 0.539370, acc.: 72.66%] [G loss: 1.156236]\n",
      "epoch:13 step:12200 [D loss: 0.670607, acc.: 61.72%] [G loss: 1.047812]\n",
      "##############\n",
      "[2.84081837 2.36150838 1.88119744 2.89580612 1.2693384  6.46846806\n",
      " 2.42730215 2.52165779 3.98770877 5.49506497]\n",
      "##########\n",
      "epoch:13 step:12201 [D loss: 0.550745, acc.: 72.66%] [G loss: 1.372692]\n",
      "epoch:13 step:12202 [D loss: 0.653786, acc.: 67.97%] [G loss: 1.091640]\n",
      "epoch:13 step:12203 [D loss: 0.519601, acc.: 75.00%] [G loss: 1.213807]\n",
      "epoch:13 step:12204 [D loss: 0.594019, acc.: 67.19%] [G loss: 1.395247]\n",
      "epoch:13 step:12205 [D loss: 0.525029, acc.: 75.00%] [G loss: 1.262715]\n",
      "epoch:13 step:12206 [D loss: 0.504045, acc.: 72.66%] [G loss: 1.090451]\n",
      "epoch:13 step:12207 [D loss: 0.590240, acc.: 70.31%] [G loss: 1.337833]\n",
      "epoch:13 step:12208 [D loss: 0.581956, acc.: 70.31%] [G loss: 1.088954]\n",
      "epoch:13 step:12209 [D loss: 0.477577, acc.: 80.47%] [G loss: 1.426092]\n",
      "epoch:13 step:12210 [D loss: 0.670712, acc.: 60.94%] [G loss: 1.294905]\n",
      "epoch:13 step:12211 [D loss: 0.492099, acc.: 78.12%] [G loss: 1.398899]\n",
      "epoch:13 step:12212 [D loss: 0.663232, acc.: 62.50%] [G loss: 1.072907]\n",
      "epoch:13 step:12213 [D loss: 0.622179, acc.: 64.84%] [G loss: 1.089090]\n",
      "epoch:13 step:12214 [D loss: 0.653762, acc.: 61.72%] [G loss: 1.134414]\n",
      "epoch:13 step:12215 [D loss: 0.611373, acc.: 69.53%] [G loss: 1.306173]\n",
      "epoch:13 step:12216 [D loss: 0.529239, acc.: 73.44%] [G loss: 1.172961]\n",
      "epoch:13 step:12217 [D loss: 0.406837, acc.: 86.72%] [G loss: 1.329157]\n",
      "epoch:13 step:12218 [D loss: 0.545099, acc.: 68.75%] [G loss: 1.196684]\n",
      "epoch:13 step:12219 [D loss: 0.677692, acc.: 57.81%] [G loss: 1.358014]\n",
      "epoch:13 step:12220 [D loss: 0.686748, acc.: 53.91%] [G loss: 1.337474]\n",
      "epoch:13 step:12221 [D loss: 0.652067, acc.: 61.72%] [G loss: 1.204077]\n",
      "epoch:13 step:12222 [D loss: 0.457483, acc.: 83.59%] [G loss: 1.159518]\n",
      "epoch:13 step:12223 [D loss: 0.545598, acc.: 75.00%] [G loss: 1.375636]\n",
      "epoch:13 step:12224 [D loss: 0.566870, acc.: 71.09%] [G loss: 1.474885]\n",
      "epoch:13 step:12225 [D loss: 0.529114, acc.: 76.56%] [G loss: 1.333240]\n",
      "epoch:13 step:12226 [D loss: 0.533975, acc.: 74.22%] [G loss: 1.409754]\n",
      "epoch:13 step:12227 [D loss: 0.660045, acc.: 64.06%] [G loss: 1.260479]\n",
      "epoch:13 step:12228 [D loss: 0.643441, acc.: 65.62%] [G loss: 1.184990]\n",
      "epoch:13 step:12229 [D loss: 0.692294, acc.: 56.25%] [G loss: 1.145295]\n",
      "epoch:13 step:12230 [D loss: 0.587781, acc.: 67.97%] [G loss: 1.105155]\n",
      "epoch:13 step:12231 [D loss: 0.529921, acc.: 73.44%] [G loss: 1.075821]\n",
      "epoch:13 step:12232 [D loss: 0.587798, acc.: 71.09%] [G loss: 1.202044]\n",
      "epoch:13 step:12233 [D loss: 0.482591, acc.: 78.91%] [G loss: 1.199694]\n",
      "epoch:13 step:12234 [D loss: 0.514056, acc.: 78.12%] [G loss: 1.115964]\n",
      "epoch:13 step:12235 [D loss: 0.598185, acc.: 63.28%] [G loss: 1.225970]\n",
      "epoch:13 step:12236 [D loss: 0.698539, acc.: 58.59%] [G loss: 1.080664]\n",
      "epoch:13 step:12237 [D loss: 0.567818, acc.: 71.09%] [G loss: 1.582626]\n",
      "epoch:13 step:12238 [D loss: 0.698451, acc.: 53.91%] [G loss: 1.039285]\n",
      "epoch:13 step:12239 [D loss: 0.647068, acc.: 64.06%] [G loss: 1.105068]\n",
      "epoch:13 step:12240 [D loss: 0.529089, acc.: 78.12%] [G loss: 1.227434]\n",
      "epoch:13 step:12241 [D loss: 0.596643, acc.: 65.62%] [G loss: 1.298729]\n",
      "epoch:13 step:12242 [D loss: 0.684880, acc.: 59.38%] [G loss: 1.199389]\n",
      "epoch:13 step:12243 [D loss: 0.556592, acc.: 71.88%] [G loss: 1.332429]\n",
      "epoch:13 step:12244 [D loss: 0.550361, acc.: 75.00%] [G loss: 0.859623]\n",
      "epoch:13 step:12245 [D loss: 0.621761, acc.: 66.41%] [G loss: 1.430815]\n",
      "epoch:13 step:12246 [D loss: 0.607236, acc.: 70.31%] [G loss: 1.452945]\n",
      "epoch:13 step:12247 [D loss: 0.612114, acc.: 65.62%] [G loss: 1.475790]\n",
      "epoch:13 step:12248 [D loss: 0.634648, acc.: 62.50%] [G loss: 1.132880]\n",
      "epoch:13 step:12249 [D loss: 0.574041, acc.: 69.53%] [G loss: 1.166030]\n",
      "epoch:13 step:12250 [D loss: 0.515111, acc.: 71.09%] [G loss: 1.320789]\n",
      "epoch:13 step:12251 [D loss: 0.588665, acc.: 71.09%] [G loss: 1.333676]\n",
      "epoch:13 step:12252 [D loss: 0.635208, acc.: 59.38%] [G loss: 1.170619]\n",
      "epoch:13 step:12253 [D loss: 0.653585, acc.: 63.28%] [G loss: 0.967412]\n",
      "epoch:13 step:12254 [D loss: 0.624339, acc.: 65.62%] [G loss: 1.420567]\n",
      "epoch:13 step:12255 [D loss: 0.531729, acc.: 67.19%] [G loss: 1.359011]\n",
      "epoch:13 step:12256 [D loss: 0.500865, acc.: 78.12%] [G loss: 1.293960]\n",
      "epoch:13 step:12257 [D loss: 0.764592, acc.: 52.34%] [G loss: 1.021548]\n",
      "epoch:13 step:12258 [D loss: 0.529304, acc.: 75.00%] [G loss: 1.216563]\n",
      "epoch:13 step:12259 [D loss: 0.660161, acc.: 60.16%] [G loss: 1.289180]\n",
      "epoch:13 step:12260 [D loss: 0.527610, acc.: 75.78%] [G loss: 1.168216]\n",
      "epoch:13 step:12261 [D loss: 0.513623, acc.: 76.56%] [G loss: 1.424611]\n",
      "epoch:13 step:12262 [D loss: 0.672879, acc.: 64.84%] [G loss: 1.011260]\n",
      "epoch:13 step:12263 [D loss: 0.637581, acc.: 60.94%] [G loss: 1.000309]\n",
      "epoch:13 step:12264 [D loss: 0.671539, acc.: 60.94%] [G loss: 1.225689]\n",
      "epoch:13 step:12265 [D loss: 0.676260, acc.: 62.50%] [G loss: 1.299697]\n",
      "epoch:13 step:12266 [D loss: 0.552622, acc.: 71.88%] [G loss: 1.156917]\n",
      "epoch:13 step:12267 [D loss: 0.546944, acc.: 69.53%] [G loss: 1.395333]\n",
      "epoch:13 step:12268 [D loss: 0.622635, acc.: 65.62%] [G loss: 1.153220]\n",
      "epoch:13 step:12269 [D loss: 0.502888, acc.: 78.12%] [G loss: 1.266409]\n",
      "epoch:13 step:12270 [D loss: 0.608406, acc.: 65.62%] [G loss: 1.250737]\n",
      "epoch:13 step:12271 [D loss: 0.451190, acc.: 82.03%] [G loss: 0.973580]\n",
      "epoch:13 step:12272 [D loss: 0.516159, acc.: 78.91%] [G loss: 1.236578]\n",
      "epoch:13 step:12273 [D loss: 0.538193, acc.: 73.44%] [G loss: 1.348292]\n",
      "epoch:13 step:12274 [D loss: 0.513172, acc.: 81.25%] [G loss: 1.148606]\n",
      "epoch:13 step:12275 [D loss: 0.627560, acc.: 61.72%] [G loss: 1.119597]\n",
      "epoch:13 step:12276 [D loss: 0.495885, acc.: 78.12%] [G loss: 1.256149]\n",
      "epoch:13 step:12277 [D loss: 0.506358, acc.: 76.56%] [G loss: 1.227315]\n",
      "epoch:13 step:12278 [D loss: 0.695684, acc.: 58.59%] [G loss: 0.989780]\n",
      "epoch:13 step:12279 [D loss: 0.539014, acc.: 70.31%] [G loss: 1.268791]\n",
      "epoch:13 step:12280 [D loss: 0.623747, acc.: 63.28%] [G loss: 1.103755]\n",
      "epoch:13 step:12281 [D loss: 0.606740, acc.: 70.31%] [G loss: 1.252384]\n",
      "epoch:13 step:12282 [D loss: 0.601803, acc.: 67.97%] [G loss: 1.269804]\n",
      "epoch:13 step:12283 [D loss: 0.663166, acc.: 61.72%] [G loss: 1.105867]\n",
      "epoch:13 step:12284 [D loss: 0.517728, acc.: 75.78%] [G loss: 1.496744]\n",
      "epoch:13 step:12285 [D loss: 0.579765, acc.: 71.09%] [G loss: 1.362255]\n",
      "epoch:13 step:12286 [D loss: 0.532271, acc.: 71.88%] [G loss: 0.999387]\n",
      "epoch:13 step:12287 [D loss: 0.643448, acc.: 62.50%] [G loss: 1.247642]\n",
      "epoch:13 step:12288 [D loss: 0.557301, acc.: 69.53%] [G loss: 1.358798]\n",
      "epoch:13 step:12289 [D loss: 0.629783, acc.: 63.28%] [G loss: 1.192255]\n",
      "epoch:13 step:12290 [D loss: 0.502514, acc.: 76.56%] [G loss: 1.474594]\n",
      "epoch:13 step:12291 [D loss: 0.688384, acc.: 60.94%] [G loss: 1.243617]\n",
      "epoch:13 step:12292 [D loss: 0.639487, acc.: 64.06%] [G loss: 1.358994]\n",
      "epoch:13 step:12293 [D loss: 0.621925, acc.: 62.50%] [G loss: 1.225613]\n",
      "epoch:13 step:12294 [D loss: 0.499740, acc.: 79.69%] [G loss: 1.115529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12295 [D loss: 0.460337, acc.: 79.69%] [G loss: 1.342778]\n",
      "epoch:13 step:12296 [D loss: 0.613515, acc.: 62.50%] [G loss: 1.354896]\n",
      "epoch:13 step:12297 [D loss: 0.657832, acc.: 66.41%] [G loss: 0.979511]\n",
      "epoch:13 step:12298 [D loss: 0.592782, acc.: 69.53%] [G loss: 1.324156]\n",
      "epoch:13 step:12299 [D loss: 0.478399, acc.: 79.69%] [G loss: 1.355822]\n",
      "epoch:13 step:12300 [D loss: 0.630683, acc.: 62.50%] [G loss: 1.158443]\n",
      "epoch:13 step:12301 [D loss: 0.549333, acc.: 74.22%] [G loss: 1.235024]\n",
      "epoch:13 step:12302 [D loss: 0.779057, acc.: 53.12%] [G loss: 1.042505]\n",
      "epoch:13 step:12303 [D loss: 0.715486, acc.: 58.59%] [G loss: 0.935235]\n",
      "epoch:13 step:12304 [D loss: 0.537052, acc.: 75.78%] [G loss: 1.122209]\n",
      "epoch:13 step:12305 [D loss: 0.551373, acc.: 67.19%] [G loss: 1.583059]\n",
      "epoch:13 step:12306 [D loss: 0.606098, acc.: 67.97%] [G loss: 1.210351]\n",
      "epoch:13 step:12307 [D loss: 0.459269, acc.: 75.78%] [G loss: 1.405273]\n",
      "epoch:13 step:12308 [D loss: 0.573389, acc.: 66.41%] [G loss: 1.370746]\n",
      "epoch:13 step:12309 [D loss: 0.621968, acc.: 68.75%] [G loss: 1.096788]\n",
      "epoch:13 step:12310 [D loss: 0.606466, acc.: 68.75%] [G loss: 1.398034]\n",
      "epoch:13 step:12311 [D loss: 0.690816, acc.: 54.69%] [G loss: 1.268188]\n",
      "epoch:13 step:12312 [D loss: 0.708583, acc.: 56.25%] [G loss: 1.170309]\n",
      "epoch:13 step:12313 [D loss: 0.573738, acc.: 69.53%] [G loss: 1.361970]\n",
      "epoch:13 step:12314 [D loss: 0.530975, acc.: 75.00%] [G loss: 1.360794]\n",
      "epoch:13 step:12315 [D loss: 0.611847, acc.: 66.41%] [G loss: 1.343073]\n",
      "epoch:13 step:12316 [D loss: 0.620701, acc.: 65.62%] [G loss: 1.274201]\n",
      "epoch:13 step:12317 [D loss: 0.740353, acc.: 50.00%] [G loss: 0.943734]\n",
      "epoch:13 step:12318 [D loss: 0.563996, acc.: 72.66%] [G loss: 1.297005]\n",
      "epoch:13 step:12319 [D loss: 0.581447, acc.: 69.53%] [G loss: 1.368874]\n",
      "epoch:13 step:12320 [D loss: 0.591707, acc.: 68.75%] [G loss: 1.172949]\n",
      "epoch:13 step:12321 [D loss: 0.575178, acc.: 70.31%] [G loss: 1.015719]\n",
      "epoch:13 step:12322 [D loss: 0.711253, acc.: 54.69%] [G loss: 0.901526]\n",
      "epoch:13 step:12323 [D loss: 0.646121, acc.: 60.94%] [G loss: 1.264603]\n",
      "epoch:13 step:12324 [D loss: 0.581800, acc.: 68.75%] [G loss: 1.333061]\n",
      "epoch:13 step:12325 [D loss: 0.681847, acc.: 56.25%] [G loss: 1.011818]\n",
      "epoch:13 step:12326 [D loss: 0.523934, acc.: 71.09%] [G loss: 1.067285]\n",
      "epoch:13 step:12327 [D loss: 0.583981, acc.: 68.75%] [G loss: 1.405256]\n",
      "epoch:13 step:12328 [D loss: 0.545247, acc.: 69.53%] [G loss: 1.234775]\n",
      "epoch:13 step:12329 [D loss: 0.561325, acc.: 70.31%] [G loss: 1.225342]\n",
      "epoch:13 step:12330 [D loss: 0.643261, acc.: 62.50%] [G loss: 1.519227]\n",
      "epoch:13 step:12331 [D loss: 0.547031, acc.: 76.56%] [G loss: 1.228646]\n",
      "epoch:13 step:12332 [D loss: 0.560989, acc.: 70.31%] [G loss: 1.231296]\n",
      "epoch:13 step:12333 [D loss: 0.578089, acc.: 67.97%] [G loss: 1.454014]\n",
      "epoch:13 step:12334 [D loss: 0.591284, acc.: 63.28%] [G loss: 1.313171]\n",
      "epoch:13 step:12335 [D loss: 0.569822, acc.: 71.88%] [G loss: 1.231602]\n",
      "epoch:13 step:12336 [D loss: 0.475806, acc.: 82.03%] [G loss: 1.279454]\n",
      "epoch:13 step:12337 [D loss: 0.549997, acc.: 71.09%] [G loss: 1.214004]\n",
      "epoch:13 step:12338 [D loss: 0.582918, acc.: 72.66%] [G loss: 1.078967]\n",
      "epoch:13 step:12339 [D loss: 0.702004, acc.: 58.59%] [G loss: 0.951492]\n",
      "epoch:13 step:12340 [D loss: 0.452227, acc.: 84.38%] [G loss: 1.388562]\n",
      "epoch:13 step:12341 [D loss: 0.418712, acc.: 79.69%] [G loss: 1.216823]\n",
      "epoch:13 step:12342 [D loss: 0.509262, acc.: 77.34%] [G loss: 1.323769]\n",
      "epoch:13 step:12343 [D loss: 0.661737, acc.: 61.72%] [G loss: 1.127036]\n",
      "epoch:13 step:12344 [D loss: 0.509178, acc.: 74.22%] [G loss: 1.096283]\n",
      "epoch:13 step:12345 [D loss: 0.605595, acc.: 65.62%] [G loss: 1.091964]\n",
      "epoch:13 step:12346 [D loss: 0.667273, acc.: 60.94%] [G loss: 1.321176]\n",
      "epoch:13 step:12347 [D loss: 0.595418, acc.: 69.53%] [G loss: 1.358328]\n",
      "epoch:13 step:12348 [D loss: 0.501506, acc.: 75.00%] [G loss: 1.462455]\n",
      "epoch:13 step:12349 [D loss: 0.667452, acc.: 59.38%] [G loss: 1.401659]\n",
      "epoch:13 step:12350 [D loss: 0.678711, acc.: 57.03%] [G loss: 1.208380]\n",
      "epoch:13 step:12351 [D loss: 0.635073, acc.: 64.06%] [G loss: 1.303212]\n",
      "epoch:13 step:12352 [D loss: 0.623780, acc.: 64.06%] [G loss: 1.362234]\n",
      "epoch:13 step:12353 [D loss: 0.567596, acc.: 71.09%] [G loss: 1.382670]\n",
      "epoch:13 step:12354 [D loss: 0.617966, acc.: 65.62%] [G loss: 1.489731]\n",
      "epoch:13 step:12355 [D loss: 0.633485, acc.: 63.28%] [G loss: 1.064612]\n",
      "epoch:13 step:12356 [D loss: 0.566393, acc.: 75.00%] [G loss: 0.977597]\n",
      "epoch:13 step:12357 [D loss: 0.596335, acc.: 67.97%] [G loss: 1.193213]\n",
      "epoch:13 step:12358 [D loss: 0.597760, acc.: 67.19%] [G loss: 1.331241]\n",
      "epoch:13 step:12359 [D loss: 0.540985, acc.: 74.22%] [G loss: 1.298572]\n",
      "epoch:13 step:12360 [D loss: 0.687120, acc.: 55.47%] [G loss: 1.150534]\n",
      "epoch:13 step:12361 [D loss: 0.583373, acc.: 73.44%] [G loss: 1.244759]\n",
      "epoch:13 step:12362 [D loss: 0.528373, acc.: 71.88%] [G loss: 1.409796]\n",
      "epoch:13 step:12363 [D loss: 0.662144, acc.: 62.50%] [G loss: 1.109757]\n",
      "epoch:13 step:12364 [D loss: 0.590986, acc.: 72.66%] [G loss: 1.231655]\n",
      "epoch:13 step:12365 [D loss: 0.506631, acc.: 78.12%] [G loss: 1.363228]\n",
      "epoch:13 step:12366 [D loss: 0.606133, acc.: 67.97%] [G loss: 1.149788]\n",
      "epoch:13 step:12367 [D loss: 0.560778, acc.: 69.53%] [G loss: 1.213094]\n",
      "epoch:13 step:12368 [D loss: 0.494873, acc.: 78.91%] [G loss: 1.461195]\n",
      "epoch:13 step:12369 [D loss: 0.517753, acc.: 71.09%] [G loss: 1.346047]\n",
      "epoch:13 step:12370 [D loss: 0.477895, acc.: 82.03%] [G loss: 1.321785]\n",
      "epoch:13 step:12371 [D loss: 0.647781, acc.: 60.94%] [G loss: 1.361467]\n",
      "epoch:13 step:12372 [D loss: 0.558820, acc.: 67.97%] [G loss: 1.276837]\n",
      "epoch:13 step:12373 [D loss: 0.589927, acc.: 63.28%] [G loss: 1.436376]\n",
      "epoch:13 step:12374 [D loss: 0.574657, acc.: 69.53%] [G loss: 1.169394]\n",
      "epoch:13 step:12375 [D loss: 0.557561, acc.: 71.88%] [G loss: 1.169002]\n",
      "epoch:13 step:12376 [D loss: 0.545013, acc.: 75.00%] [G loss: 1.259167]\n",
      "epoch:13 step:12377 [D loss: 0.545975, acc.: 71.88%] [G loss: 1.375069]\n",
      "epoch:13 step:12378 [D loss: 0.636294, acc.: 64.84%] [G loss: 1.073735]\n",
      "epoch:13 step:12379 [D loss: 0.602368, acc.: 69.53%] [G loss: 1.220644]\n",
      "epoch:13 step:12380 [D loss: 0.590877, acc.: 67.97%] [G loss: 1.315461]\n",
      "epoch:13 step:12381 [D loss: 0.557209, acc.: 70.31%] [G loss: 1.101444]\n",
      "epoch:13 step:12382 [D loss: 0.552552, acc.: 69.53%] [G loss: 1.167840]\n",
      "epoch:13 step:12383 [D loss: 0.475164, acc.: 80.47%] [G loss: 1.412263]\n",
      "epoch:13 step:12384 [D loss: 0.476639, acc.: 82.03%] [G loss: 1.517336]\n",
      "epoch:13 step:12385 [D loss: 0.539425, acc.: 73.44%] [G loss: 1.182173]\n",
      "epoch:13 step:12386 [D loss: 0.703660, acc.: 57.81%] [G loss: 1.171958]\n",
      "epoch:13 step:12387 [D loss: 0.464416, acc.: 82.81%] [G loss: 1.215284]\n",
      "epoch:13 step:12388 [D loss: 0.523618, acc.: 77.34%] [G loss: 1.101763]\n",
      "epoch:13 step:12389 [D loss: 0.571335, acc.: 71.09%] [G loss: 1.179848]\n",
      "epoch:13 step:12390 [D loss: 0.573130, acc.: 69.53%] [G loss: 1.131399]\n",
      "epoch:13 step:12391 [D loss: 0.584236, acc.: 72.66%] [G loss: 1.383844]\n",
      "epoch:13 step:12392 [D loss: 0.515576, acc.: 79.69%] [G loss: 1.403133]\n",
      "epoch:13 step:12393 [D loss: 0.536111, acc.: 78.91%] [G loss: 1.168498]\n",
      "epoch:13 step:12394 [D loss: 0.604314, acc.: 71.88%] [G loss: 1.149807]\n",
      "epoch:13 step:12395 [D loss: 0.530033, acc.: 71.09%] [G loss: 1.462989]\n",
      "epoch:13 step:12396 [D loss: 0.683075, acc.: 63.28%] [G loss: 1.089260]\n",
      "epoch:13 step:12397 [D loss: 0.639661, acc.: 67.19%] [G loss: 1.377277]\n",
      "epoch:13 step:12398 [D loss: 0.694374, acc.: 56.25%] [G loss: 1.122790]\n",
      "epoch:13 step:12399 [D loss: 0.608574, acc.: 69.53%] [G loss: 1.094632]\n",
      "epoch:13 step:12400 [D loss: 0.533183, acc.: 76.56%] [G loss: 1.136397]\n",
      "##############\n",
      "[2.82587828 2.40488189 2.00310658 2.94991729 1.30336107 6.03066902\n",
      " 2.36021353 3.04690872 4.00737352 4.64965504]\n",
      "##########\n",
      "epoch:13 step:12401 [D loss: 0.633037, acc.: 67.97%] [G loss: 1.029670]\n",
      "epoch:13 step:12402 [D loss: 0.569066, acc.: 69.53%] [G loss: 1.318930]\n",
      "epoch:13 step:12403 [D loss: 0.670977, acc.: 57.03%] [G loss: 1.104134]\n",
      "epoch:13 step:12404 [D loss: 0.666980, acc.: 64.06%] [G loss: 1.196252]\n",
      "epoch:13 step:12405 [D loss: 0.677215, acc.: 63.28%] [G loss: 1.438875]\n",
      "epoch:13 step:12406 [D loss: 0.548734, acc.: 75.00%] [G loss: 1.368261]\n",
      "epoch:13 step:12407 [D loss: 0.627795, acc.: 64.06%] [G loss: 1.067095]\n",
      "epoch:13 step:12408 [D loss: 0.569189, acc.: 69.53%] [G loss: 0.971610]\n",
      "epoch:13 step:12409 [D loss: 0.686897, acc.: 56.25%] [G loss: 1.102750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12410 [D loss: 0.413836, acc.: 85.16%] [G loss: 1.444205]\n",
      "epoch:13 step:12411 [D loss: 0.522263, acc.: 75.78%] [G loss: 1.334641]\n",
      "epoch:13 step:12412 [D loss: 0.557723, acc.: 72.66%] [G loss: 1.075778]\n",
      "epoch:13 step:12413 [D loss: 0.705953, acc.: 59.38%] [G loss: 1.231520]\n",
      "epoch:13 step:12414 [D loss: 0.522125, acc.: 81.25%] [G loss: 1.169300]\n",
      "epoch:13 step:12415 [D loss: 0.507767, acc.: 77.34%] [G loss: 1.158270]\n",
      "epoch:13 step:12416 [D loss: 0.594522, acc.: 68.75%] [G loss: 1.329611]\n",
      "epoch:13 step:12417 [D loss: 0.588010, acc.: 62.50%] [G loss: 1.331346]\n",
      "epoch:13 step:12418 [D loss: 0.501495, acc.: 76.56%] [G loss: 1.312228]\n",
      "epoch:13 step:12419 [D loss: 0.665824, acc.: 67.19%] [G loss: 0.917405]\n",
      "epoch:13 step:12420 [D loss: 0.704036, acc.: 57.81%] [G loss: 1.121254]\n",
      "epoch:13 step:12421 [D loss: 0.663329, acc.: 57.81%] [G loss: 1.257195]\n",
      "epoch:13 step:12422 [D loss: 0.478853, acc.: 79.69%] [G loss: 1.448208]\n",
      "epoch:13 step:12423 [D loss: 0.519396, acc.: 73.44%] [G loss: 1.266067]\n",
      "epoch:13 step:12424 [D loss: 0.679914, acc.: 62.50%] [G loss: 1.188758]\n",
      "epoch:13 step:12425 [D loss: 0.541595, acc.: 75.78%] [G loss: 1.193053]\n",
      "epoch:13 step:12426 [D loss: 0.611521, acc.: 63.28%] [G loss: 1.237342]\n",
      "epoch:13 step:12427 [D loss: 0.556061, acc.: 75.00%] [G loss: 1.600317]\n",
      "epoch:13 step:12428 [D loss: 0.550611, acc.: 71.88%] [G loss: 1.101884]\n",
      "epoch:13 step:12429 [D loss: 0.684006, acc.: 57.81%] [G loss: 1.150213]\n",
      "epoch:13 step:12430 [D loss: 0.579667, acc.: 65.62%] [G loss: 1.084540]\n",
      "epoch:13 step:12431 [D loss: 0.581309, acc.: 70.31%] [G loss: 1.142256]\n",
      "epoch:13 step:12432 [D loss: 0.683815, acc.: 57.81%] [G loss: 1.101460]\n",
      "epoch:13 step:12433 [D loss: 0.558558, acc.: 71.09%] [G loss: 1.110537]\n",
      "epoch:13 step:12434 [D loss: 0.543577, acc.: 74.22%] [G loss: 1.132794]\n",
      "epoch:13 step:12435 [D loss: 0.679789, acc.: 55.47%] [G loss: 1.372410]\n",
      "epoch:13 step:12436 [D loss: 0.511215, acc.: 77.34%] [G loss: 1.222789]\n",
      "epoch:13 step:12437 [D loss: 0.571299, acc.: 71.09%] [G loss: 1.245759]\n",
      "epoch:13 step:12438 [D loss: 0.514265, acc.: 77.34%] [G loss: 1.384233]\n",
      "epoch:13 step:12439 [D loss: 0.495727, acc.: 78.91%] [G loss: 1.458642]\n",
      "epoch:13 step:12440 [D loss: 0.574078, acc.: 64.84%] [G loss: 1.359016]\n",
      "epoch:13 step:12441 [D loss: 0.567703, acc.: 71.88%] [G loss: 1.144552]\n",
      "epoch:13 step:12442 [D loss: 0.683693, acc.: 58.59%] [G loss: 1.026650]\n",
      "epoch:13 step:12443 [D loss: 0.729534, acc.: 53.12%] [G loss: 1.481184]\n",
      "epoch:13 step:12444 [D loss: 0.617794, acc.: 64.84%] [G loss: 1.140007]\n",
      "epoch:13 step:12445 [D loss: 0.488238, acc.: 78.91%] [G loss: 1.451572]\n",
      "epoch:13 step:12446 [D loss: 0.578432, acc.: 67.19%] [G loss: 1.339532]\n",
      "epoch:13 step:12447 [D loss: 0.655070, acc.: 58.59%] [G loss: 1.252737]\n",
      "epoch:13 step:12448 [D loss: 0.592146, acc.: 71.88%] [G loss: 1.284042]\n",
      "epoch:13 step:12449 [D loss: 0.510371, acc.: 75.00%] [G loss: 1.383860]\n",
      "epoch:13 step:12450 [D loss: 0.578101, acc.: 72.66%] [G loss: 1.148416]\n",
      "epoch:13 step:12451 [D loss: 0.467283, acc.: 80.47%] [G loss: 1.119032]\n",
      "epoch:13 step:12452 [D loss: 0.525450, acc.: 72.66%] [G loss: 0.969116]\n",
      "epoch:13 step:12453 [D loss: 0.468156, acc.: 81.25%] [G loss: 1.255085]\n",
      "epoch:13 step:12454 [D loss: 0.676943, acc.: 63.28%] [G loss: 1.148014]\n",
      "epoch:13 step:12455 [D loss: 0.545761, acc.: 72.66%] [G loss: 1.167738]\n",
      "epoch:13 step:12456 [D loss: 0.731694, acc.: 57.81%] [G loss: 0.943133]\n",
      "epoch:13 step:12457 [D loss: 0.765523, acc.: 53.91%] [G loss: 1.280993]\n",
      "epoch:13 step:12458 [D loss: 0.505628, acc.: 76.56%] [G loss: 1.460968]\n",
      "epoch:13 step:12459 [D loss: 0.571757, acc.: 66.41%] [G loss: 1.268054]\n",
      "epoch:13 step:12460 [D loss: 0.691749, acc.: 57.81%] [G loss: 1.177637]\n",
      "epoch:13 step:12461 [D loss: 0.609033, acc.: 64.84%] [G loss: 1.532187]\n",
      "epoch:13 step:12462 [D loss: 0.595173, acc.: 66.41%] [G loss: 1.264109]\n",
      "epoch:13 step:12463 [D loss: 0.628375, acc.: 63.28%] [G loss: 1.282783]\n",
      "epoch:13 step:12464 [D loss: 0.476223, acc.: 78.91%] [G loss: 1.424753]\n",
      "epoch:13 step:12465 [D loss: 0.466285, acc.: 81.25%] [G loss: 1.336918]\n",
      "epoch:13 step:12466 [D loss: 0.582420, acc.: 70.31%] [G loss: 0.876206]\n",
      "epoch:13 step:12467 [D loss: 0.622193, acc.: 64.84%] [G loss: 1.321990]\n",
      "epoch:13 step:12468 [D loss: 0.636686, acc.: 60.16%] [G loss: 1.221545]\n",
      "epoch:13 step:12469 [D loss: 0.633243, acc.: 60.94%] [G loss: 1.440715]\n",
      "epoch:13 step:12470 [D loss: 0.655788, acc.: 55.47%] [G loss: 1.155762]\n",
      "epoch:13 step:12471 [D loss: 0.477813, acc.: 78.12%] [G loss: 1.263703]\n",
      "epoch:13 step:12472 [D loss: 0.586286, acc.: 69.53%] [G loss: 1.117534]\n",
      "epoch:13 step:12473 [D loss: 0.640465, acc.: 67.97%] [G loss: 1.045080]\n",
      "epoch:13 step:12474 [D loss: 0.615265, acc.: 63.28%] [G loss: 1.191708]\n",
      "epoch:13 step:12475 [D loss: 0.507544, acc.: 76.56%] [G loss: 1.248255]\n",
      "epoch:13 step:12476 [D loss: 0.488937, acc.: 73.44%] [G loss: 1.195270]\n",
      "epoch:13 step:12477 [D loss: 0.531403, acc.: 73.44%] [G loss: 1.145739]\n",
      "epoch:13 step:12478 [D loss: 0.613316, acc.: 64.84%] [G loss: 0.968205]\n",
      "epoch:13 step:12479 [D loss: 0.715811, acc.: 53.91%] [G loss: 1.344940]\n",
      "epoch:13 step:12480 [D loss: 0.655688, acc.: 61.72%] [G loss: 1.255304]\n",
      "epoch:13 step:12481 [D loss: 0.557182, acc.: 70.31%] [G loss: 1.493293]\n",
      "epoch:13 step:12482 [D loss: 0.625544, acc.: 67.97%] [G loss: 1.337385]\n",
      "epoch:13 step:12483 [D loss: 0.628110, acc.: 69.53%] [G loss: 1.047761]\n",
      "epoch:13 step:12484 [D loss: 0.503659, acc.: 73.44%] [G loss: 1.373762]\n",
      "epoch:13 step:12485 [D loss: 0.662609, acc.: 64.06%] [G loss: 1.064254]\n",
      "epoch:13 step:12486 [D loss: 0.571182, acc.: 70.31%] [G loss: 1.235739]\n",
      "epoch:13 step:12487 [D loss: 0.495469, acc.: 77.34%] [G loss: 1.353375]\n",
      "epoch:13 step:12488 [D loss: 0.568995, acc.: 70.31%] [G loss: 1.249522]\n",
      "epoch:13 step:12489 [D loss: 0.639257, acc.: 61.72%] [G loss: 1.174776]\n",
      "epoch:13 step:12490 [D loss: 0.543501, acc.: 71.88%] [G loss: 1.085680]\n",
      "epoch:13 step:12491 [D loss: 0.673381, acc.: 59.38%] [G loss: 1.317270]\n",
      "epoch:13 step:12492 [D loss: 0.612843, acc.: 68.75%] [G loss: 1.284644]\n",
      "epoch:13 step:12493 [D loss: 0.685730, acc.: 53.91%] [G loss: 1.299368]\n",
      "epoch:13 step:12494 [D loss: 0.551215, acc.: 74.22%] [G loss: 1.108362]\n",
      "epoch:13 step:12495 [D loss: 0.525872, acc.: 75.78%] [G loss: 1.146348]\n",
      "epoch:13 step:12496 [D loss: 0.552310, acc.: 68.75%] [G loss: 1.128406]\n",
      "epoch:13 step:12497 [D loss: 0.666895, acc.: 55.47%] [G loss: 1.243873]\n",
      "epoch:13 step:12498 [D loss: 0.469664, acc.: 82.03%] [G loss: 1.233571]\n",
      "epoch:13 step:12499 [D loss: 0.667596, acc.: 67.19%] [G loss: 1.294169]\n",
      "epoch:13 step:12500 [D loss: 0.634508, acc.: 62.50%] [G loss: 1.234289]\n",
      "epoch:13 step:12501 [D loss: 0.583677, acc.: 70.31%] [G loss: 1.112275]\n",
      "epoch:13 step:12502 [D loss: 0.566720, acc.: 67.19%] [G loss: 1.116889]\n",
      "epoch:13 step:12503 [D loss: 0.639622, acc.: 67.97%] [G loss: 1.326932]\n",
      "epoch:13 step:12504 [D loss: 0.561475, acc.: 72.66%] [G loss: 1.079700]\n",
      "epoch:13 step:12505 [D loss: 0.663279, acc.: 64.06%] [G loss: 1.098153]\n",
      "epoch:13 step:12506 [D loss: 0.587989, acc.: 66.41%] [G loss: 1.256762]\n",
      "epoch:13 step:12507 [D loss: 0.565859, acc.: 67.97%] [G loss: 1.298286]\n",
      "epoch:13 step:12508 [D loss: 0.589977, acc.: 66.41%] [G loss: 1.133710]\n",
      "epoch:13 step:12509 [D loss: 0.586366, acc.: 67.19%] [G loss: 1.358583]\n",
      "epoch:13 step:12510 [D loss: 0.673820, acc.: 56.25%] [G loss: 1.042822]\n",
      "epoch:13 step:12511 [D loss: 0.552147, acc.: 76.56%] [G loss: 1.125731]\n",
      "epoch:13 step:12512 [D loss: 0.470874, acc.: 79.69%] [G loss: 1.296865]\n",
      "epoch:13 step:12513 [D loss: 0.502407, acc.: 75.00%] [G loss: 1.177775]\n",
      "epoch:13 step:12514 [D loss: 0.521675, acc.: 75.00%] [G loss: 1.089973]\n",
      "epoch:13 step:12515 [D loss: 0.667052, acc.: 66.41%] [G loss: 0.992524]\n",
      "epoch:13 step:12516 [D loss: 0.575624, acc.: 72.66%] [G loss: 1.103404]\n",
      "epoch:13 step:12517 [D loss: 0.455844, acc.: 76.56%] [G loss: 1.260360]\n",
      "epoch:13 step:12518 [D loss: 0.553205, acc.: 74.22%] [G loss: 1.266322]\n",
      "epoch:13 step:12519 [D loss: 0.503731, acc.: 75.78%] [G loss: 1.091094]\n",
      "epoch:13 step:12520 [D loss: 0.501696, acc.: 75.78%] [G loss: 1.284261]\n",
      "epoch:13 step:12521 [D loss: 0.610312, acc.: 62.50%] [G loss: 1.154565]\n",
      "epoch:13 step:12522 [D loss: 0.544732, acc.: 72.66%] [G loss: 1.202182]\n",
      "epoch:13 step:12523 [D loss: 0.604634, acc.: 67.97%] [G loss: 1.090043]\n",
      "epoch:13 step:12524 [D loss: 0.458198, acc.: 83.59%] [G loss: 1.362426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12525 [D loss: 0.634307, acc.: 63.28%] [G loss: 1.149934]\n",
      "epoch:13 step:12526 [D loss: 0.517566, acc.: 76.56%] [G loss: 1.303694]\n",
      "epoch:13 step:12527 [D loss: 0.515619, acc.: 74.22%] [G loss: 1.297617]\n",
      "epoch:13 step:12528 [D loss: 0.628437, acc.: 68.75%] [G loss: 1.175143]\n",
      "epoch:13 step:12529 [D loss: 0.676943, acc.: 55.47%] [G loss: 1.024122]\n",
      "epoch:13 step:12530 [D loss: 0.590732, acc.: 65.62%] [G loss: 1.276940]\n",
      "epoch:13 step:12531 [D loss: 0.637688, acc.: 60.16%] [G loss: 1.419548]\n",
      "epoch:13 step:12532 [D loss: 0.557762, acc.: 67.19%] [G loss: 1.299586]\n",
      "epoch:13 step:12533 [D loss: 0.509740, acc.: 75.78%] [G loss: 1.530760]\n",
      "epoch:13 step:12534 [D loss: 0.445862, acc.: 84.38%] [G loss: 1.414597]\n",
      "epoch:13 step:12535 [D loss: 0.545067, acc.: 73.44%] [G loss: 1.007995]\n",
      "epoch:13 step:12536 [D loss: 0.536226, acc.: 74.22%] [G loss: 1.283310]\n",
      "epoch:13 step:12537 [D loss: 0.580579, acc.: 69.53%] [G loss: 1.301450]\n",
      "epoch:13 step:12538 [D loss: 0.494419, acc.: 78.12%] [G loss: 1.339367]\n",
      "epoch:13 step:12539 [D loss: 0.541018, acc.: 74.22%] [G loss: 1.413980]\n",
      "epoch:13 step:12540 [D loss: 0.708742, acc.: 53.91%] [G loss: 1.269273]\n",
      "epoch:13 step:12541 [D loss: 0.503990, acc.: 77.34%] [G loss: 1.188159]\n",
      "epoch:13 step:12542 [D loss: 0.522141, acc.: 70.31%] [G loss: 1.146494]\n",
      "epoch:13 step:12543 [D loss: 0.636698, acc.: 64.06%] [G loss: 1.320521]\n",
      "epoch:13 step:12544 [D loss: 0.569494, acc.: 67.97%] [G loss: 0.955463]\n",
      "epoch:13 step:12545 [D loss: 0.488803, acc.: 80.47%] [G loss: 1.344085]\n",
      "epoch:13 step:12546 [D loss: 0.572853, acc.: 66.41%] [G loss: 1.292259]\n",
      "epoch:13 step:12547 [D loss: 0.618535, acc.: 63.28%] [G loss: 1.157738]\n",
      "epoch:13 step:12548 [D loss: 0.721631, acc.: 53.91%] [G loss: 1.173836]\n",
      "epoch:13 step:12549 [D loss: 0.649119, acc.: 67.19%] [G loss: 1.298067]\n",
      "epoch:13 step:12550 [D loss: 0.498654, acc.: 76.56%] [G loss: 1.607146]\n",
      "epoch:13 step:12551 [D loss: 0.514684, acc.: 75.78%] [G loss: 1.121731]\n",
      "epoch:13 step:12552 [D loss: 0.681277, acc.: 59.38%] [G loss: 1.194420]\n",
      "epoch:13 step:12553 [D loss: 0.562063, acc.: 68.75%] [G loss: 1.439141]\n",
      "epoch:13 step:12554 [D loss: 0.682488, acc.: 60.16%] [G loss: 1.293734]\n",
      "epoch:13 step:12555 [D loss: 0.547586, acc.: 73.44%] [G loss: 1.278002]\n",
      "epoch:13 step:12556 [D loss: 0.661789, acc.: 58.59%] [G loss: 1.370811]\n",
      "epoch:13 step:12557 [D loss: 0.701464, acc.: 60.94%] [G loss: 0.809206]\n",
      "epoch:13 step:12558 [D loss: 0.572661, acc.: 71.88%] [G loss: 1.062543]\n",
      "epoch:13 step:12559 [D loss: 0.550861, acc.: 71.88%] [G loss: 1.317075]\n",
      "epoch:13 step:12560 [D loss: 0.546856, acc.: 75.00%] [G loss: 1.093327]\n",
      "epoch:13 step:12561 [D loss: 0.570096, acc.: 71.09%] [G loss: 1.353378]\n",
      "epoch:13 step:12562 [D loss: 0.584087, acc.: 67.97%] [G loss: 1.095699]\n",
      "epoch:13 step:12563 [D loss: 0.598734, acc.: 71.09%] [G loss: 1.064694]\n",
      "epoch:13 step:12564 [D loss: 0.448703, acc.: 82.81%] [G loss: 1.288461]\n",
      "epoch:13 step:12565 [D loss: 0.644174, acc.: 65.62%] [G loss: 1.231689]\n",
      "epoch:13 step:12566 [D loss: 0.461032, acc.: 82.03%] [G loss: 1.265225]\n",
      "epoch:13 step:12567 [D loss: 0.513949, acc.: 78.91%] [G loss: 1.328925]\n",
      "epoch:13 step:12568 [D loss: 0.519106, acc.: 76.56%] [G loss: 1.316553]\n",
      "epoch:13 step:12569 [D loss: 0.683833, acc.: 60.94%] [G loss: 1.130395]\n",
      "epoch:13 step:12570 [D loss: 0.604987, acc.: 68.75%] [G loss: 1.124055]\n",
      "epoch:13 step:12571 [D loss: 0.604869, acc.: 67.19%] [G loss: 1.444242]\n",
      "epoch:13 step:12572 [D loss: 0.633046, acc.: 59.38%] [G loss: 1.164258]\n",
      "epoch:13 step:12573 [D loss: 0.553576, acc.: 78.12%] [G loss: 1.201820]\n",
      "epoch:13 step:12574 [D loss: 0.592507, acc.: 69.53%] [G loss: 1.153243]\n",
      "epoch:13 step:12575 [D loss: 0.675034, acc.: 63.28%] [G loss: 1.169406]\n",
      "epoch:13 step:12576 [D loss: 0.706470, acc.: 57.81%] [G loss: 1.134382]\n",
      "epoch:13 step:12577 [D loss: 0.526076, acc.: 73.44%] [G loss: 1.201044]\n",
      "epoch:13 step:12578 [D loss: 0.555514, acc.: 71.88%] [G loss: 1.223117]\n",
      "epoch:13 step:12579 [D loss: 0.593460, acc.: 64.84%] [G loss: 1.158016]\n",
      "epoch:13 step:12580 [D loss: 0.563779, acc.: 70.31%] [G loss: 1.198230]\n",
      "epoch:13 step:12581 [D loss: 0.662800, acc.: 57.81%] [G loss: 1.028048]\n",
      "epoch:13 step:12582 [D loss: 0.558896, acc.: 70.31%] [G loss: 1.150121]\n",
      "epoch:13 step:12583 [D loss: 0.529717, acc.: 75.00%] [G loss: 1.240653]\n",
      "epoch:13 step:12584 [D loss: 0.613762, acc.: 64.06%] [G loss: 1.268114]\n",
      "epoch:13 step:12585 [D loss: 0.656096, acc.: 61.72%] [G loss: 1.305779]\n",
      "epoch:13 step:12586 [D loss: 0.528622, acc.: 71.88%] [G loss: 1.300036]\n",
      "epoch:13 step:12587 [D loss: 0.466293, acc.: 82.03%] [G loss: 1.476718]\n",
      "epoch:13 step:12588 [D loss: 0.545042, acc.: 71.88%] [G loss: 1.521797]\n",
      "epoch:13 step:12589 [D loss: 0.705747, acc.: 62.50%] [G loss: 1.107502]\n",
      "epoch:13 step:12590 [D loss: 0.610943, acc.: 63.28%] [G loss: 1.055607]\n",
      "epoch:13 step:12591 [D loss: 0.554509, acc.: 67.97%] [G loss: 1.341788]\n",
      "epoch:13 step:12592 [D loss: 0.603591, acc.: 67.97%] [G loss: 1.225751]\n",
      "epoch:13 step:12593 [D loss: 0.650209, acc.: 63.28%] [G loss: 1.332432]\n",
      "epoch:13 step:12594 [D loss: 0.677584, acc.: 57.81%] [G loss: 1.042697]\n",
      "epoch:13 step:12595 [D loss: 0.623527, acc.: 63.28%] [G loss: 1.052539]\n",
      "epoch:13 step:12596 [D loss: 0.540968, acc.: 73.44%] [G loss: 1.288175]\n",
      "epoch:13 step:12597 [D loss: 0.486989, acc.: 78.12%] [G loss: 1.379794]\n",
      "epoch:13 step:12598 [D loss: 0.597377, acc.: 65.62%] [G loss: 1.135578]\n",
      "epoch:13 step:12599 [D loss: 0.650869, acc.: 61.72%] [G loss: 1.181707]\n",
      "epoch:13 step:12600 [D loss: 0.519796, acc.: 70.31%] [G loss: 1.521363]\n",
      "##############\n",
      "[2.8688781  2.20137993 2.09077694 2.97597817 1.08520186 5.97385185\n",
      " 2.1377508  3.11662888 3.98476749 7.14868929]\n",
      "##########\n",
      "epoch:13 step:12601 [D loss: 0.573349, acc.: 70.31%] [G loss: 1.055053]\n",
      "epoch:13 step:12602 [D loss: 0.578000, acc.: 67.97%] [G loss: 1.229779]\n",
      "epoch:13 step:12603 [D loss: 0.689531, acc.: 62.50%] [G loss: 1.092897]\n",
      "epoch:13 step:12604 [D loss: 0.551633, acc.: 68.75%] [G loss: 1.159045]\n",
      "epoch:13 step:12605 [D loss: 0.463060, acc.: 84.38%] [G loss: 1.526168]\n",
      "epoch:13 step:12606 [D loss: 0.615706, acc.: 66.41%] [G loss: 1.022421]\n",
      "epoch:13 step:12607 [D loss: 0.707452, acc.: 56.25%] [G loss: 1.159271]\n",
      "epoch:13 step:12608 [D loss: 0.626922, acc.: 62.50%] [G loss: 1.087714]\n",
      "epoch:13 step:12609 [D loss: 0.670463, acc.: 64.06%] [G loss: 1.055664]\n",
      "epoch:13 step:12610 [D loss: 0.519173, acc.: 77.34%] [G loss: 1.350299]\n",
      "epoch:13 step:12611 [D loss: 0.618104, acc.: 67.19%] [G loss: 1.167452]\n",
      "epoch:13 step:12612 [D loss: 0.469934, acc.: 80.47%] [G loss: 1.311234]\n",
      "epoch:13 step:12613 [D loss: 0.553318, acc.: 73.44%] [G loss: 1.289041]\n",
      "epoch:13 step:12614 [D loss: 0.498634, acc.: 78.91%] [G loss: 1.287832]\n",
      "epoch:13 step:12615 [D loss: 0.682554, acc.: 53.91%] [G loss: 1.177215]\n",
      "epoch:13 step:12616 [D loss: 0.533808, acc.: 72.66%] [G loss: 1.353721]\n",
      "epoch:13 step:12617 [D loss: 0.597715, acc.: 66.41%] [G loss: 1.095342]\n",
      "epoch:13 step:12618 [D loss: 0.623408, acc.: 62.50%] [G loss: 1.183548]\n",
      "epoch:13 step:12619 [D loss: 0.551987, acc.: 73.44%] [G loss: 1.138969]\n",
      "epoch:13 step:12620 [D loss: 0.557216, acc.: 72.66%] [G loss: 1.224498]\n",
      "epoch:13 step:12621 [D loss: 0.660125, acc.: 61.72%] [G loss: 1.021092]\n",
      "epoch:13 step:12622 [D loss: 0.568484, acc.: 74.22%] [G loss: 1.478992]\n",
      "epoch:13 step:12623 [D loss: 0.557028, acc.: 71.88%] [G loss: 1.174093]\n",
      "epoch:13 step:12624 [D loss: 0.568446, acc.: 71.09%] [G loss: 1.525393]\n",
      "epoch:13 step:12625 [D loss: 0.558826, acc.: 67.19%] [G loss: 1.439632]\n",
      "epoch:13 step:12626 [D loss: 0.557692, acc.: 71.88%] [G loss: 1.135402]\n",
      "epoch:13 step:12627 [D loss: 0.597485, acc.: 64.06%] [G loss: 1.116063]\n",
      "epoch:13 step:12628 [D loss: 0.529701, acc.: 69.53%] [G loss: 1.243081]\n",
      "epoch:13 step:12629 [D loss: 0.650127, acc.: 58.59%] [G loss: 1.192617]\n",
      "epoch:13 step:12630 [D loss: 0.551859, acc.: 71.88%] [G loss: 1.555837]\n",
      "epoch:13 step:12631 [D loss: 0.594310, acc.: 67.19%] [G loss: 1.262592]\n",
      "epoch:13 step:12632 [D loss: 0.482589, acc.: 76.56%] [G loss: 1.322606]\n",
      "epoch:13 step:12633 [D loss: 0.548519, acc.: 74.22%] [G loss: 1.166567]\n",
      "epoch:13 step:12634 [D loss: 0.462343, acc.: 81.25%] [G loss: 1.163629]\n",
      "epoch:13 step:12635 [D loss: 0.594204, acc.: 67.97%] [G loss: 1.283570]\n",
      "epoch:13 step:12636 [D loss: 0.490315, acc.: 77.34%] [G loss: 1.200792]\n",
      "epoch:13 step:12637 [D loss: 0.630441, acc.: 62.50%] [G loss: 1.299997]\n",
      "epoch:13 step:12638 [D loss: 0.758587, acc.: 55.47%] [G loss: 0.887832]\n",
      "epoch:13 step:12639 [D loss: 0.558269, acc.: 71.88%] [G loss: 1.290515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12640 [D loss: 0.551650, acc.: 71.09%] [G loss: 1.380361]\n",
      "epoch:13 step:12641 [D loss: 0.533390, acc.: 72.66%] [G loss: 1.408064]\n",
      "epoch:13 step:12642 [D loss: 0.621571, acc.: 67.19%] [G loss: 1.338802]\n",
      "epoch:13 step:12643 [D loss: 0.642582, acc.: 65.62%] [G loss: 1.215327]\n",
      "epoch:13 step:12644 [D loss: 0.689519, acc.: 61.72%] [G loss: 1.099413]\n",
      "epoch:13 step:12645 [D loss: 0.586571, acc.: 67.97%] [G loss: 1.233214]\n",
      "epoch:13 step:12646 [D loss: 0.660637, acc.: 64.06%] [G loss: 1.521081]\n",
      "epoch:13 step:12647 [D loss: 0.583464, acc.: 64.06%] [G loss: 1.329965]\n",
      "epoch:13 step:12648 [D loss: 0.660255, acc.: 62.50%] [G loss: 1.454490]\n",
      "epoch:13 step:12649 [D loss: 0.548682, acc.: 69.53%] [G loss: 1.336351]\n",
      "epoch:13 step:12650 [D loss: 0.678428, acc.: 63.28%] [G loss: 1.355914]\n",
      "epoch:13 step:12651 [D loss: 0.655247, acc.: 60.94%] [G loss: 1.305119]\n",
      "epoch:13 step:12652 [D loss: 0.746698, acc.: 56.25%] [G loss: 1.051519]\n",
      "epoch:13 step:12653 [D loss: 0.700939, acc.: 57.03%] [G loss: 0.960645]\n",
      "epoch:13 step:12654 [D loss: 0.443445, acc.: 85.94%] [G loss: 1.612192]\n",
      "epoch:13 step:12655 [D loss: 0.691863, acc.: 53.91%] [G loss: 1.182940]\n",
      "epoch:13 step:12656 [D loss: 0.579126, acc.: 67.19%] [G loss: 1.138429]\n",
      "epoch:13 step:12657 [D loss: 0.490779, acc.: 75.78%] [G loss: 1.067700]\n",
      "epoch:13 step:12658 [D loss: 0.608259, acc.: 62.50%] [G loss: 1.298317]\n",
      "epoch:13 step:12659 [D loss: 0.634451, acc.: 67.97%] [G loss: 1.014451]\n",
      "epoch:13 step:12660 [D loss: 0.500092, acc.: 75.78%] [G loss: 1.172829]\n",
      "epoch:13 step:12661 [D loss: 0.459677, acc.: 83.59%] [G loss: 1.282467]\n",
      "epoch:13 step:12662 [D loss: 0.649173, acc.: 60.94%] [G loss: 1.031409]\n",
      "epoch:13 step:12663 [D loss: 0.570219, acc.: 73.44%] [G loss: 1.260572]\n",
      "epoch:13 step:12664 [D loss: 0.598410, acc.: 69.53%] [G loss: 1.170098]\n",
      "epoch:13 step:12665 [D loss: 0.604650, acc.: 67.97%] [G loss: 1.096438]\n",
      "epoch:13 step:12666 [D loss: 0.594244, acc.: 63.28%] [G loss: 1.238471]\n",
      "epoch:13 step:12667 [D loss: 0.464884, acc.: 79.69%] [G loss: 1.196553]\n",
      "epoch:13 step:12668 [D loss: 0.581447, acc.: 67.97%] [G loss: 1.184788]\n",
      "epoch:13 step:12669 [D loss: 0.449057, acc.: 82.81%] [G loss: 1.472840]\n",
      "epoch:13 step:12670 [D loss: 0.550688, acc.: 69.53%] [G loss: 1.449000]\n",
      "epoch:13 step:12671 [D loss: 0.478248, acc.: 78.91%] [G loss: 1.021415]\n",
      "epoch:13 step:12672 [D loss: 0.572379, acc.: 75.00%] [G loss: 1.319333]\n",
      "epoch:13 step:12673 [D loss: 0.481226, acc.: 75.00%] [G loss: 1.176148]\n",
      "epoch:13 step:12674 [D loss: 0.541382, acc.: 72.66%] [G loss: 1.032844]\n",
      "epoch:13 step:12675 [D loss: 0.500746, acc.: 75.78%] [G loss: 1.204669]\n",
      "epoch:13 step:12676 [D loss: 0.674097, acc.: 63.28%] [G loss: 1.228829]\n",
      "epoch:13 step:12677 [D loss: 0.654186, acc.: 63.28%] [G loss: 1.174972]\n",
      "epoch:13 step:12678 [D loss: 0.518384, acc.: 74.22%] [G loss: 1.190692]\n",
      "epoch:13 step:12679 [D loss: 0.533089, acc.: 74.22%] [G loss: 1.518642]\n",
      "epoch:13 step:12680 [D loss: 0.677665, acc.: 64.06%] [G loss: 1.354388]\n",
      "epoch:13 step:12681 [D loss: 0.584537, acc.: 70.31%] [G loss: 1.135077]\n",
      "epoch:13 step:12682 [D loss: 0.504794, acc.: 82.03%] [G loss: 0.952171]\n",
      "epoch:13 step:12683 [D loss: 0.543151, acc.: 75.78%] [G loss: 1.297880]\n",
      "epoch:13 step:12684 [D loss: 0.566081, acc.: 67.19%] [G loss: 1.118425]\n",
      "epoch:13 step:12685 [D loss: 0.572746, acc.: 73.44%] [G loss: 1.218906]\n",
      "epoch:13 step:12686 [D loss: 0.449839, acc.: 80.47%] [G loss: 1.550261]\n",
      "epoch:13 step:12687 [D loss: 0.530377, acc.: 75.00%] [G loss: 1.495609]\n",
      "epoch:13 step:12688 [D loss: 0.628566, acc.: 60.16%] [G loss: 1.432141]\n",
      "epoch:13 step:12689 [D loss: 0.601997, acc.: 68.75%] [G loss: 1.271193]\n",
      "epoch:13 step:12690 [D loss: 0.618456, acc.: 67.19%] [G loss: 1.020552]\n",
      "epoch:13 step:12691 [D loss: 0.642230, acc.: 61.72%] [G loss: 1.399518]\n",
      "epoch:13 step:12692 [D loss: 0.527802, acc.: 71.09%] [G loss: 1.372085]\n",
      "epoch:13 step:12693 [D loss: 0.500795, acc.: 79.69%] [G loss: 1.336643]\n",
      "epoch:13 step:12694 [D loss: 0.577374, acc.: 71.88%] [G loss: 1.516778]\n",
      "epoch:13 step:12695 [D loss: 0.608322, acc.: 66.41%] [G loss: 1.517817]\n",
      "epoch:13 step:12696 [D loss: 0.506913, acc.: 76.56%] [G loss: 1.293753]\n",
      "epoch:13 step:12697 [D loss: 0.591328, acc.: 65.62%] [G loss: 1.236115]\n",
      "epoch:13 step:12698 [D loss: 0.673931, acc.: 57.81%] [G loss: 1.002013]\n",
      "epoch:13 step:12699 [D loss: 0.523617, acc.: 76.56%] [G loss: 1.207003]\n",
      "epoch:13 step:12700 [D loss: 0.521414, acc.: 76.56%] [G loss: 1.400844]\n",
      "epoch:13 step:12701 [D loss: 0.590273, acc.: 65.62%] [G loss: 1.210631]\n",
      "epoch:13 step:12702 [D loss: 0.596954, acc.: 67.19%] [G loss: 1.140122]\n",
      "epoch:13 step:12703 [D loss: 0.630301, acc.: 70.31%] [G loss: 1.050623]\n",
      "epoch:13 step:12704 [D loss: 0.480935, acc.: 78.12%] [G loss: 1.226861]\n",
      "epoch:13 step:12705 [D loss: 0.523188, acc.: 75.78%] [G loss: 1.603026]\n",
      "epoch:13 step:12706 [D loss: 0.522227, acc.: 75.00%] [G loss: 1.206489]\n",
      "epoch:13 step:12707 [D loss: 0.632112, acc.: 62.50%] [G loss: 1.025510]\n",
      "epoch:13 step:12708 [D loss: 0.581951, acc.: 67.97%] [G loss: 1.134359]\n",
      "epoch:13 step:12709 [D loss: 0.553820, acc.: 73.44%] [G loss: 1.221507]\n",
      "epoch:13 step:12710 [D loss: 0.575783, acc.: 71.09%] [G loss: 1.225195]\n",
      "epoch:13 step:12711 [D loss: 0.682606, acc.: 65.62%] [G loss: 0.908239]\n",
      "epoch:13 step:12712 [D loss: 0.563854, acc.: 73.44%] [G loss: 1.088828]\n",
      "epoch:13 step:12713 [D loss: 0.515229, acc.: 71.88%] [G loss: 1.261870]\n",
      "epoch:13 step:12714 [D loss: 0.685658, acc.: 57.03%] [G loss: 1.492862]\n",
      "epoch:13 step:12715 [D loss: 0.618705, acc.: 64.84%] [G loss: 1.095248]\n",
      "epoch:13 step:12716 [D loss: 0.665020, acc.: 61.72%] [G loss: 1.145340]\n",
      "epoch:13 step:12717 [D loss: 0.541393, acc.: 72.66%] [G loss: 1.203115]\n",
      "epoch:13 step:12718 [D loss: 0.614238, acc.: 71.09%] [G loss: 1.209285]\n",
      "epoch:13 step:12719 [D loss: 0.539539, acc.: 73.44%] [G loss: 1.302745]\n",
      "epoch:13 step:12720 [D loss: 0.538222, acc.: 72.66%] [G loss: 1.089981]\n",
      "epoch:13 step:12721 [D loss: 0.569603, acc.: 71.88%] [G loss: 1.189381]\n",
      "epoch:13 step:12722 [D loss: 0.416977, acc.: 83.59%] [G loss: 1.345716]\n",
      "epoch:13 step:12723 [D loss: 0.609721, acc.: 67.97%] [G loss: 1.150570]\n",
      "epoch:13 step:12724 [D loss: 0.564012, acc.: 68.75%] [G loss: 1.199030]\n",
      "epoch:13 step:12725 [D loss: 0.618869, acc.: 62.50%] [G loss: 1.299459]\n",
      "epoch:13 step:12726 [D loss: 0.489095, acc.: 75.78%] [G loss: 1.363415]\n",
      "epoch:13 step:12727 [D loss: 0.573220, acc.: 68.75%] [G loss: 1.079825]\n",
      "epoch:13 step:12728 [D loss: 0.532320, acc.: 73.44%] [G loss: 1.162016]\n",
      "epoch:13 step:12729 [D loss: 0.588639, acc.: 70.31%] [G loss: 1.231320]\n",
      "epoch:13 step:12730 [D loss: 0.471807, acc.: 78.12%] [G loss: 1.104421]\n",
      "epoch:13 step:12731 [D loss: 0.575361, acc.: 69.53%] [G loss: 1.063633]\n",
      "epoch:13 step:12732 [D loss: 0.550149, acc.: 72.66%] [G loss: 1.332585]\n",
      "epoch:13 step:12733 [D loss: 0.503317, acc.: 73.44%] [G loss: 1.380197]\n",
      "epoch:13 step:12734 [D loss: 0.745739, acc.: 50.00%] [G loss: 1.118709]\n",
      "epoch:13 step:12735 [D loss: 0.502757, acc.: 78.12%] [G loss: 1.055709]\n",
      "epoch:13 step:12736 [D loss: 0.576943, acc.: 71.09%] [G loss: 1.205625]\n",
      "epoch:13 step:12737 [D loss: 0.531181, acc.: 71.88%] [G loss: 1.250213]\n",
      "epoch:13 step:12738 [D loss: 0.637895, acc.: 64.84%] [G loss: 1.203991]\n",
      "epoch:13 step:12739 [D loss: 0.707758, acc.: 57.81%] [G loss: 0.812452]\n",
      "epoch:13 step:12740 [D loss: 0.580089, acc.: 68.75%] [G loss: 1.108482]\n",
      "epoch:13 step:12741 [D loss: 0.700657, acc.: 57.81%] [G loss: 1.283935]\n",
      "epoch:13 step:12742 [D loss: 0.517099, acc.: 76.56%] [G loss: 1.158749]\n",
      "epoch:13 step:12743 [D loss: 0.574225, acc.: 72.66%] [G loss: 1.525686]\n",
      "epoch:13 step:12744 [D loss: 0.541197, acc.: 76.56%] [G loss: 1.343627]\n",
      "epoch:13 step:12745 [D loss: 0.441112, acc.: 82.03%] [G loss: 1.405664]\n",
      "epoch:13 step:12746 [D loss: 0.540578, acc.: 74.22%] [G loss: 1.260669]\n",
      "epoch:13 step:12747 [D loss: 0.621824, acc.: 64.84%] [G loss: 1.312651]\n",
      "epoch:13 step:12748 [D loss: 0.611600, acc.: 66.41%] [G loss: 1.235803]\n",
      "epoch:13 step:12749 [D loss: 0.658744, acc.: 62.50%] [G loss: 1.055628]\n",
      "epoch:13 step:12750 [D loss: 0.638202, acc.: 63.28%] [G loss: 1.102530]\n",
      "epoch:13 step:12751 [D loss: 0.632745, acc.: 67.19%] [G loss: 1.047432]\n",
      "epoch:13 step:12752 [D loss: 0.590638, acc.: 68.75%] [G loss: 1.355336]\n",
      "epoch:13 step:12753 [D loss: 0.490018, acc.: 78.12%] [G loss: 1.242169]\n",
      "epoch:13 step:12754 [D loss: 0.604765, acc.: 69.53%] [G loss: 1.271381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12755 [D loss: 0.601596, acc.: 64.84%] [G loss: 1.234220]\n",
      "epoch:13 step:12756 [D loss: 0.690162, acc.: 58.59%] [G loss: 1.130087]\n",
      "epoch:13 step:12757 [D loss: 0.519194, acc.: 76.56%] [G loss: 1.572453]\n",
      "epoch:13 step:12758 [D loss: 0.679093, acc.: 64.84%] [G loss: 0.850779]\n",
      "epoch:13 step:12759 [D loss: 0.489380, acc.: 77.34%] [G loss: 1.302644]\n",
      "epoch:13 step:12760 [D loss: 0.687313, acc.: 63.28%] [G loss: 0.966172]\n",
      "epoch:13 step:12761 [D loss: 0.516626, acc.: 73.44%] [G loss: 1.285236]\n",
      "epoch:13 step:12762 [D loss: 0.580938, acc.: 69.53%] [G loss: 1.346911]\n",
      "epoch:13 step:12763 [D loss: 0.563067, acc.: 70.31%] [G loss: 1.125933]\n",
      "epoch:13 step:12764 [D loss: 0.574243, acc.: 70.31%] [G loss: 1.276075]\n",
      "epoch:13 step:12765 [D loss: 0.475852, acc.: 78.12%] [G loss: 1.174606]\n",
      "epoch:13 step:12766 [D loss: 0.667930, acc.: 58.59%] [G loss: 1.253589]\n",
      "epoch:13 step:12767 [D loss: 0.588827, acc.: 68.75%] [G loss: 1.291503]\n",
      "epoch:13 step:12768 [D loss: 0.635361, acc.: 62.50%] [G loss: 1.075296]\n",
      "epoch:13 step:12769 [D loss: 0.622104, acc.: 64.06%] [G loss: 1.495307]\n",
      "epoch:13 step:12770 [D loss: 0.567547, acc.: 71.09%] [G loss: 1.245262]\n",
      "epoch:13 step:12771 [D loss: 0.476233, acc.: 76.56%] [G loss: 1.462970]\n",
      "epoch:13 step:12772 [D loss: 0.480362, acc.: 78.12%] [G loss: 1.396079]\n",
      "epoch:13 step:12773 [D loss: 0.548285, acc.: 74.22%] [G loss: 1.540634]\n",
      "epoch:13 step:12774 [D loss: 0.777547, acc.: 50.78%] [G loss: 0.791838]\n",
      "epoch:13 step:12775 [D loss: 0.548951, acc.: 73.44%] [G loss: 1.499137]\n",
      "epoch:13 step:12776 [D loss: 0.538646, acc.: 76.56%] [G loss: 1.271391]\n",
      "epoch:13 step:12777 [D loss: 0.477807, acc.: 75.78%] [G loss: 1.274574]\n",
      "epoch:13 step:12778 [D loss: 0.525002, acc.: 74.22%] [G loss: 1.282453]\n",
      "epoch:13 step:12779 [D loss: 0.512364, acc.: 71.88%] [G loss: 1.399450]\n",
      "epoch:13 step:12780 [D loss: 0.496838, acc.: 72.66%] [G loss: 1.369624]\n",
      "epoch:13 step:12781 [D loss: 0.536004, acc.: 71.88%] [G loss: 1.130266]\n",
      "epoch:13 step:12782 [D loss: 0.594094, acc.: 65.62%] [G loss: 1.150590]\n",
      "epoch:13 step:12783 [D loss: 0.518989, acc.: 72.66%] [G loss: 1.145883]\n",
      "epoch:13 step:12784 [D loss: 0.567757, acc.: 68.75%] [G loss: 1.157329]\n",
      "epoch:13 step:12785 [D loss: 0.545528, acc.: 72.66%] [G loss: 1.193282]\n",
      "epoch:13 step:12786 [D loss: 0.576926, acc.: 68.75%] [G loss: 0.973467]\n",
      "epoch:13 step:12787 [D loss: 0.690518, acc.: 57.81%] [G loss: 1.042675]\n",
      "epoch:13 step:12788 [D loss: 0.534728, acc.: 72.66%] [G loss: 1.176116]\n",
      "epoch:13 step:12789 [D loss: 0.495964, acc.: 78.91%] [G loss: 1.197409]\n",
      "epoch:13 step:12790 [D loss: 0.556854, acc.: 73.44%] [G loss: 1.254570]\n",
      "epoch:13 step:12791 [D loss: 0.709549, acc.: 56.25%] [G loss: 1.175782]\n",
      "epoch:13 step:12792 [D loss: 0.515748, acc.: 74.22%] [G loss: 1.452670]\n",
      "epoch:13 step:12793 [D loss: 0.557935, acc.: 73.44%] [G loss: 1.500201]\n",
      "epoch:13 step:12794 [D loss: 0.569355, acc.: 70.31%] [G loss: 1.176848]\n",
      "epoch:13 step:12795 [D loss: 0.559018, acc.: 67.97%] [G loss: 1.237192]\n",
      "epoch:13 step:12796 [D loss: 0.502669, acc.: 77.34%] [G loss: 1.350116]\n",
      "epoch:13 step:12797 [D loss: 0.505619, acc.: 76.56%] [G loss: 1.565401]\n",
      "epoch:13 step:12798 [D loss: 0.467866, acc.: 81.25%] [G loss: 1.487942]\n",
      "epoch:13 step:12799 [D loss: 0.670416, acc.: 58.59%] [G loss: 1.298989]\n",
      "epoch:13 step:12800 [D loss: 0.573319, acc.: 69.53%] [G loss: 1.256374]\n",
      "##############\n",
      "[2.84904313 2.05984328 1.66001763 2.73092533 0.89645023 5.49153119\n",
      " 2.20597309 2.56725986 3.85955603 6.28973379]\n",
      "##########\n",
      "epoch:13 step:12801 [D loss: 0.682529, acc.: 60.16%] [G loss: 1.140892]\n",
      "epoch:13 step:12802 [D loss: 0.566268, acc.: 71.88%] [G loss: 1.466047]\n",
      "epoch:13 step:12803 [D loss: 0.697121, acc.: 59.38%] [G loss: 0.968149]\n",
      "epoch:13 step:12804 [D loss: 0.444840, acc.: 80.47%] [G loss: 1.368749]\n",
      "epoch:13 step:12805 [D loss: 0.669186, acc.: 64.84%] [G loss: 1.259707]\n",
      "epoch:13 step:12806 [D loss: 0.500645, acc.: 76.56%] [G loss: 1.198345]\n",
      "epoch:13 step:12807 [D loss: 0.578121, acc.: 71.88%] [G loss: 1.472530]\n",
      "epoch:13 step:12808 [D loss: 0.384639, acc.: 84.38%] [G loss: 1.622196]\n",
      "epoch:13 step:12809 [D loss: 0.604895, acc.: 68.75%] [G loss: 1.323264]\n",
      "epoch:13 step:12810 [D loss: 0.626284, acc.: 60.94%] [G loss: 1.171979]\n",
      "epoch:13 step:12811 [D loss: 0.673459, acc.: 59.38%] [G loss: 1.142832]\n",
      "epoch:13 step:12812 [D loss: 0.548067, acc.: 69.53%] [G loss: 1.202456]\n",
      "epoch:13 step:12813 [D loss: 0.585848, acc.: 67.19%] [G loss: 1.396321]\n",
      "epoch:13 step:12814 [D loss: 0.433711, acc.: 85.16%] [G loss: 1.203935]\n",
      "epoch:13 step:12815 [D loss: 0.570538, acc.: 69.53%] [G loss: 1.434167]\n",
      "epoch:13 step:12816 [D loss: 0.478081, acc.: 82.03%] [G loss: 1.341415]\n",
      "epoch:13 step:12817 [D loss: 0.529438, acc.: 78.12%] [G loss: 1.369105]\n",
      "epoch:13 step:12818 [D loss: 0.623139, acc.: 66.41%] [G loss: 1.106973]\n",
      "epoch:13 step:12819 [D loss: 0.645704, acc.: 64.06%] [G loss: 1.010995]\n",
      "epoch:13 step:12820 [D loss: 0.526775, acc.: 73.44%] [G loss: 1.191848]\n",
      "epoch:13 step:12821 [D loss: 0.639749, acc.: 67.97%] [G loss: 1.267573]\n",
      "epoch:13 step:12822 [D loss: 0.500941, acc.: 78.91%] [G loss: 1.510852]\n",
      "epoch:13 step:12823 [D loss: 0.628331, acc.: 62.50%] [G loss: 1.070542]\n",
      "epoch:13 step:12824 [D loss: 0.666665, acc.: 59.38%] [G loss: 0.975561]\n",
      "epoch:13 step:12825 [D loss: 0.588006, acc.: 68.75%] [G loss: 1.372039]\n",
      "epoch:13 step:12826 [D loss: 0.555531, acc.: 75.00%] [G loss: 1.181960]\n",
      "epoch:13 step:12827 [D loss: 0.562043, acc.: 77.34%] [G loss: 1.209386]\n",
      "epoch:13 step:12828 [D loss: 0.626276, acc.: 60.94%] [G loss: 1.205298]\n",
      "epoch:13 step:12829 [D loss: 0.654247, acc.: 66.41%] [G loss: 0.983963]\n",
      "epoch:13 step:12830 [D loss: 0.593855, acc.: 65.62%] [G loss: 1.266555]\n",
      "epoch:13 step:12831 [D loss: 0.566802, acc.: 73.44%] [G loss: 1.080968]\n",
      "epoch:13 step:12832 [D loss: 0.629823, acc.: 68.75%] [G loss: 1.306587]\n",
      "epoch:13 step:12833 [D loss: 0.630275, acc.: 60.94%] [G loss: 1.360394]\n",
      "epoch:13 step:12834 [D loss: 0.564069, acc.: 71.09%] [G loss: 1.277489]\n",
      "epoch:13 step:12835 [D loss: 0.585202, acc.: 70.31%] [G loss: 1.132986]\n",
      "epoch:13 step:12836 [D loss: 0.690207, acc.: 60.94%] [G loss: 1.390855]\n",
      "epoch:13 step:12837 [D loss: 0.516173, acc.: 75.78%] [G loss: 1.322001]\n",
      "epoch:13 step:12838 [D loss: 0.623972, acc.: 65.62%] [G loss: 1.181644]\n",
      "epoch:13 step:12839 [D loss: 0.578568, acc.: 67.97%] [G loss: 1.304123]\n",
      "epoch:13 step:12840 [D loss: 0.615999, acc.: 61.72%] [G loss: 1.296051]\n",
      "epoch:13 step:12841 [D loss: 0.651863, acc.: 57.03%] [G loss: 0.943887]\n",
      "epoch:13 step:12842 [D loss: 0.582567, acc.: 71.88%] [G loss: 1.342995]\n",
      "epoch:13 step:12843 [D loss: 0.552480, acc.: 77.34%] [G loss: 1.249836]\n",
      "epoch:13 step:12844 [D loss: 0.597314, acc.: 67.19%] [G loss: 1.252581]\n",
      "epoch:13 step:12845 [D loss: 0.667373, acc.: 62.50%] [G loss: 1.036150]\n",
      "epoch:13 step:12846 [D loss: 0.634191, acc.: 70.31%] [G loss: 1.154138]\n",
      "epoch:13 step:12847 [D loss: 0.518637, acc.: 78.91%] [G loss: 0.982971]\n",
      "epoch:13 step:12848 [D loss: 0.595493, acc.: 69.53%] [G loss: 1.251061]\n",
      "epoch:13 step:12849 [D loss: 0.696716, acc.: 57.03%] [G loss: 1.286695]\n",
      "epoch:13 step:12850 [D loss: 0.589075, acc.: 67.97%] [G loss: 1.240582]\n",
      "epoch:13 step:12851 [D loss: 0.569835, acc.: 75.00%] [G loss: 1.230176]\n",
      "epoch:13 step:12852 [D loss: 0.635113, acc.: 60.16%] [G loss: 1.205354]\n",
      "epoch:13 step:12853 [D loss: 0.508449, acc.: 74.22%] [G loss: 1.428519]\n",
      "epoch:13 step:12854 [D loss: 0.669396, acc.: 54.69%] [G loss: 1.147461]\n",
      "epoch:13 step:12855 [D loss: 0.603297, acc.: 71.88%] [G loss: 1.397920]\n",
      "epoch:13 step:12856 [D loss: 0.604805, acc.: 69.53%] [G loss: 1.418612]\n",
      "epoch:13 step:12857 [D loss: 0.598662, acc.: 64.06%] [G loss: 1.200463]\n",
      "epoch:13 step:12858 [D loss: 0.625892, acc.: 66.41%] [G loss: 1.305535]\n",
      "epoch:13 step:12859 [D loss: 0.591561, acc.: 68.75%] [G loss: 1.221329]\n",
      "epoch:13 step:12860 [D loss: 0.657877, acc.: 60.94%] [G loss: 1.343368]\n",
      "epoch:13 step:12861 [D loss: 0.545272, acc.: 74.22%] [G loss: 1.296101]\n",
      "epoch:13 step:12862 [D loss: 0.699116, acc.: 61.72%] [G loss: 0.932802]\n",
      "epoch:13 step:12863 [D loss: 0.499065, acc.: 75.78%] [G loss: 1.349683]\n",
      "epoch:13 step:12864 [D loss: 0.672962, acc.: 62.50%] [G loss: 1.030670]\n",
      "epoch:13 step:12865 [D loss: 0.508530, acc.: 82.81%] [G loss: 1.192036]\n",
      "epoch:13 step:12866 [D loss: 0.592280, acc.: 70.31%] [G loss: 1.192795]\n",
      "epoch:13 step:12867 [D loss: 0.469932, acc.: 81.25%] [G loss: 1.171937]\n",
      "epoch:13 step:12868 [D loss: 0.582898, acc.: 71.88%] [G loss: 1.325198]\n",
      "epoch:13 step:12869 [D loss: 0.581342, acc.: 69.53%] [G loss: 1.289445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12870 [D loss: 0.639159, acc.: 60.16%] [G loss: 0.994232]\n",
      "epoch:13 step:12871 [D loss: 0.567775, acc.: 71.88%] [G loss: 0.973966]\n",
      "epoch:13 step:12872 [D loss: 0.529835, acc.: 76.56%] [G loss: 1.183608]\n",
      "epoch:13 step:12873 [D loss: 0.574706, acc.: 71.88%] [G loss: 1.273964]\n",
      "epoch:13 step:12874 [D loss: 0.700015, acc.: 61.72%] [G loss: 1.116090]\n",
      "epoch:13 step:12875 [D loss: 0.656313, acc.: 58.59%] [G loss: 1.117748]\n",
      "epoch:13 step:12876 [D loss: 0.647882, acc.: 64.06%] [G loss: 1.292235]\n",
      "epoch:13 step:12877 [D loss: 0.745117, acc.: 50.00%] [G loss: 1.160536]\n",
      "epoch:13 step:12878 [D loss: 0.540952, acc.: 78.12%] [G loss: 1.546660]\n",
      "epoch:13 step:12879 [D loss: 0.572895, acc.: 70.31%] [G loss: 1.353978]\n",
      "epoch:13 step:12880 [D loss: 0.514083, acc.: 76.56%] [G loss: 1.158558]\n",
      "epoch:13 step:12881 [D loss: 0.551832, acc.: 71.09%] [G loss: 1.211599]\n",
      "epoch:13 step:12882 [D loss: 0.398955, acc.: 87.50%] [G loss: 1.201514]\n",
      "epoch:13 step:12883 [D loss: 0.513495, acc.: 78.91%] [G loss: 1.461016]\n",
      "epoch:13 step:12884 [D loss: 0.681168, acc.: 56.25%] [G loss: 1.122188]\n",
      "epoch:13 step:12885 [D loss: 0.759432, acc.: 52.34%] [G loss: 1.047374]\n",
      "epoch:13 step:12886 [D loss: 0.446560, acc.: 86.72%] [G loss: 1.195452]\n",
      "epoch:13 step:12887 [D loss: 0.685512, acc.: 57.81%] [G loss: 1.097257]\n",
      "epoch:13 step:12888 [D loss: 0.656168, acc.: 56.25%] [G loss: 1.211436]\n",
      "epoch:13 step:12889 [D loss: 0.626269, acc.: 61.72%] [G loss: 0.871169]\n",
      "epoch:13 step:12890 [D loss: 0.659100, acc.: 60.94%] [G loss: 1.167974]\n",
      "epoch:13 step:12891 [D loss: 0.548065, acc.: 75.00%] [G loss: 1.165450]\n",
      "epoch:13 step:12892 [D loss: 0.480565, acc.: 81.25%] [G loss: 1.556839]\n",
      "epoch:13 step:12893 [D loss: 0.650860, acc.: 62.50%] [G loss: 1.330516]\n",
      "epoch:13 step:12894 [D loss: 0.539495, acc.: 71.09%] [G loss: 1.344186]\n",
      "epoch:13 step:12895 [D loss: 0.570782, acc.: 69.53%] [G loss: 1.324602]\n",
      "epoch:13 step:12896 [D loss: 0.489227, acc.: 78.91%] [G loss: 1.193664]\n",
      "epoch:13 step:12897 [D loss: 0.546469, acc.: 73.44%] [G loss: 0.984881]\n",
      "epoch:13 step:12898 [D loss: 0.568243, acc.: 73.44%] [G loss: 1.391264]\n",
      "epoch:13 step:12899 [D loss: 0.621263, acc.: 61.72%] [G loss: 1.273266]\n",
      "epoch:13 step:12900 [D loss: 0.560491, acc.: 74.22%] [G loss: 1.370557]\n",
      "epoch:13 step:12901 [D loss: 0.517160, acc.: 75.00%] [G loss: 1.340213]\n",
      "epoch:13 step:12902 [D loss: 0.653188, acc.: 60.16%] [G loss: 0.970075]\n",
      "epoch:13 step:12903 [D loss: 0.580555, acc.: 68.75%] [G loss: 1.304663]\n",
      "epoch:13 step:12904 [D loss: 0.599751, acc.: 64.06%] [G loss: 1.106509]\n",
      "epoch:13 step:12905 [D loss: 0.618125, acc.: 69.53%] [G loss: 0.986793]\n",
      "epoch:13 step:12906 [D loss: 0.599608, acc.: 67.19%] [G loss: 1.156625]\n",
      "epoch:13 step:12907 [D loss: 0.531022, acc.: 75.00%] [G loss: 1.170168]\n",
      "epoch:13 step:12908 [D loss: 0.493460, acc.: 78.91%] [G loss: 1.282356]\n",
      "epoch:13 step:12909 [D loss: 0.686075, acc.: 63.28%] [G loss: 1.339311]\n",
      "epoch:13 step:12910 [D loss: 0.589792, acc.: 67.19%] [G loss: 1.286780]\n",
      "epoch:13 step:12911 [D loss: 0.653368, acc.: 65.62%] [G loss: 1.329117]\n",
      "epoch:13 step:12912 [D loss: 0.598828, acc.: 68.75%] [G loss: 1.125531]\n",
      "epoch:13 step:12913 [D loss: 0.656064, acc.: 60.94%] [G loss: 0.998323]\n",
      "epoch:13 step:12914 [D loss: 0.513150, acc.: 75.78%] [G loss: 1.284289]\n",
      "epoch:13 step:12915 [D loss: 0.473405, acc.: 78.12%] [G loss: 1.447712]\n",
      "epoch:13 step:12916 [D loss: 0.470669, acc.: 75.78%] [G loss: 1.587789]\n",
      "epoch:13 step:12917 [D loss: 0.642070, acc.: 61.72%] [G loss: 1.057810]\n",
      "epoch:13 step:12918 [D loss: 0.509940, acc.: 78.91%] [G loss: 1.480845]\n",
      "epoch:13 step:12919 [D loss: 0.689763, acc.: 59.38%] [G loss: 1.127239]\n",
      "epoch:13 step:12920 [D loss: 0.524417, acc.: 74.22%] [G loss: 1.244695]\n",
      "epoch:13 step:12921 [D loss: 0.635448, acc.: 63.28%] [G loss: 1.312550]\n",
      "epoch:13 step:12922 [D loss: 0.611106, acc.: 68.75%] [G loss: 1.334316]\n",
      "epoch:13 step:12923 [D loss: 0.559666, acc.: 73.44%] [G loss: 1.343346]\n",
      "epoch:13 step:12924 [D loss: 0.721542, acc.: 59.38%] [G loss: 1.310776]\n",
      "epoch:13 step:12925 [D loss: 0.573703, acc.: 67.97%] [G loss: 1.286489]\n",
      "epoch:13 step:12926 [D loss: 0.660810, acc.: 60.94%] [G loss: 1.288257]\n",
      "epoch:13 step:12927 [D loss: 0.559135, acc.: 67.97%] [G loss: 1.385805]\n",
      "epoch:13 step:12928 [D loss: 0.583786, acc.: 68.75%] [G loss: 1.251191]\n",
      "epoch:13 step:12929 [D loss: 0.544906, acc.: 75.78%] [G loss: 1.196245]\n",
      "epoch:13 step:12930 [D loss: 0.577626, acc.: 68.75%] [G loss: 1.376206]\n",
      "epoch:13 step:12931 [D loss: 0.712324, acc.: 60.94%] [G loss: 1.088687]\n",
      "epoch:13 step:12932 [D loss: 0.518283, acc.: 77.34%] [G loss: 1.199826]\n",
      "epoch:13 step:12933 [D loss: 0.562958, acc.: 75.00%] [G loss: 1.321862]\n",
      "epoch:13 step:12934 [D loss: 0.580240, acc.: 69.53%] [G loss: 0.953826]\n",
      "epoch:13 step:12935 [D loss: 0.592214, acc.: 73.44%] [G loss: 1.548314]\n",
      "epoch:13 step:12936 [D loss: 0.511807, acc.: 77.34%] [G loss: 1.441366]\n",
      "epoch:13 step:12937 [D loss: 0.589459, acc.: 71.09%] [G loss: 1.193448]\n",
      "epoch:13 step:12938 [D loss: 0.496565, acc.: 75.78%] [G loss: 1.595533]\n",
      "epoch:13 step:12939 [D loss: 0.598254, acc.: 66.41%] [G loss: 1.295576]\n",
      "epoch:13 step:12940 [D loss: 0.533551, acc.: 75.00%] [G loss: 1.199738]\n",
      "epoch:13 step:12941 [D loss: 0.599026, acc.: 65.62%] [G loss: 1.104510]\n",
      "epoch:13 step:12942 [D loss: 0.793748, acc.: 47.66%] [G loss: 1.060680]\n",
      "epoch:13 step:12943 [D loss: 0.563284, acc.: 71.88%] [G loss: 1.255730]\n",
      "epoch:13 step:12944 [D loss: 0.717484, acc.: 60.16%] [G loss: 1.390115]\n",
      "epoch:13 step:12945 [D loss: 0.487345, acc.: 80.47%] [G loss: 1.166160]\n",
      "epoch:13 step:12946 [D loss: 0.564085, acc.: 67.97%] [G loss: 1.170577]\n",
      "epoch:13 step:12947 [D loss: 0.544499, acc.: 72.66%] [G loss: 1.030323]\n",
      "epoch:13 step:12948 [D loss: 0.515301, acc.: 74.22%] [G loss: 1.403424]\n",
      "epoch:13 step:12949 [D loss: 0.738683, acc.: 56.25%] [G loss: 1.190846]\n",
      "epoch:13 step:12950 [D loss: 0.645721, acc.: 60.94%] [G loss: 1.070385]\n",
      "epoch:13 step:12951 [D loss: 0.662399, acc.: 58.59%] [G loss: 1.314003]\n",
      "epoch:13 step:12952 [D loss: 0.613563, acc.: 67.97%] [G loss: 1.295782]\n",
      "epoch:13 step:12953 [D loss: 0.637885, acc.: 61.72%] [G loss: 1.263752]\n",
      "epoch:13 step:12954 [D loss: 0.629050, acc.: 63.28%] [G loss: 1.424948]\n",
      "epoch:13 step:12955 [D loss: 0.364625, acc.: 92.19%] [G loss: 1.529965]\n",
      "epoch:13 step:12956 [D loss: 0.624307, acc.: 62.50%] [G loss: 1.224730]\n",
      "epoch:13 step:12957 [D loss: 0.397699, acc.: 87.50%] [G loss: 1.322264]\n",
      "epoch:13 step:12958 [D loss: 0.586068, acc.: 71.09%] [G loss: 1.051593]\n",
      "epoch:13 step:12959 [D loss: 0.589154, acc.: 68.75%] [G loss: 1.174980]\n",
      "epoch:13 step:12960 [D loss: 0.588430, acc.: 68.75%] [G loss: 1.221415]\n",
      "epoch:13 step:12961 [D loss: 0.762665, acc.: 47.66%] [G loss: 1.153523]\n",
      "epoch:13 step:12962 [D loss: 0.706454, acc.: 60.16%] [G loss: 1.028303]\n",
      "epoch:13 step:12963 [D loss: 0.518897, acc.: 72.66%] [G loss: 1.395839]\n",
      "epoch:13 step:12964 [D loss: 0.673968, acc.: 60.94%] [G loss: 1.176727]\n",
      "epoch:13 step:12965 [D loss: 0.551526, acc.: 72.66%] [G loss: 1.173531]\n",
      "epoch:13 step:12966 [D loss: 0.452131, acc.: 82.81%] [G loss: 1.494174]\n",
      "epoch:13 step:12967 [D loss: 0.601504, acc.: 71.09%] [G loss: 1.165020]\n",
      "epoch:13 step:12968 [D loss: 0.479077, acc.: 78.91%] [G loss: 1.230005]\n",
      "epoch:13 step:12969 [D loss: 0.720266, acc.: 53.91%] [G loss: 1.236381]\n",
      "epoch:13 step:12970 [D loss: 0.680490, acc.: 56.25%] [G loss: 0.961522]\n",
      "epoch:13 step:12971 [D loss: 0.537765, acc.: 78.91%] [G loss: 1.468288]\n",
      "epoch:13 step:12972 [D loss: 0.584566, acc.: 64.06%] [G loss: 1.041247]\n",
      "epoch:13 step:12973 [D loss: 0.550900, acc.: 78.12%] [G loss: 1.091029]\n",
      "epoch:13 step:12974 [D loss: 0.615647, acc.: 68.75%] [G loss: 1.308295]\n",
      "epoch:13 step:12975 [D loss: 0.477844, acc.: 78.12%] [G loss: 1.402873]\n",
      "epoch:13 step:12976 [D loss: 0.484066, acc.: 77.34%] [G loss: 1.253555]\n",
      "epoch:13 step:12977 [D loss: 0.700220, acc.: 57.81%] [G loss: 1.229847]\n",
      "epoch:13 step:12978 [D loss: 0.622813, acc.: 64.84%] [G loss: 1.207101]\n",
      "epoch:13 step:12979 [D loss: 0.587536, acc.: 69.53%] [G loss: 1.366086]\n",
      "epoch:13 step:12980 [D loss: 0.595963, acc.: 66.41%] [G loss: 1.233655]\n",
      "epoch:13 step:12981 [D loss: 0.520135, acc.: 75.78%] [G loss: 1.147759]\n",
      "epoch:13 step:12982 [D loss: 0.538481, acc.: 75.78%] [G loss: 1.270592]\n",
      "epoch:13 step:12983 [D loss: 0.642683, acc.: 66.41%] [G loss: 1.211014]\n",
      "epoch:13 step:12984 [D loss: 0.627737, acc.: 61.72%] [G loss: 1.269075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12985 [D loss: 0.583967, acc.: 71.88%] [G loss: 1.208622]\n",
      "epoch:13 step:12986 [D loss: 0.598774, acc.: 73.44%] [G loss: 0.916805]\n",
      "epoch:13 step:12987 [D loss: 0.482669, acc.: 77.34%] [G loss: 1.335321]\n",
      "epoch:13 step:12988 [D loss: 0.562377, acc.: 64.84%] [G loss: 1.187749]\n",
      "epoch:13 step:12989 [D loss: 0.525621, acc.: 72.66%] [G loss: 1.267580]\n",
      "epoch:13 step:12990 [D loss: 0.474852, acc.: 82.03%] [G loss: 1.105500]\n",
      "epoch:13 step:12991 [D loss: 0.582431, acc.: 73.44%] [G loss: 1.194858]\n",
      "epoch:13 step:12992 [D loss: 0.591232, acc.: 69.53%] [G loss: 1.315297]\n",
      "epoch:13 step:12993 [D loss: 0.687823, acc.: 55.47%] [G loss: 1.447802]\n",
      "epoch:13 step:12994 [D loss: 0.627577, acc.: 62.50%] [G loss: 1.167821]\n",
      "epoch:13 step:12995 [D loss: 0.520775, acc.: 76.56%] [G loss: 1.390934]\n",
      "epoch:13 step:12996 [D loss: 0.747889, acc.: 57.03%] [G loss: 1.171544]\n",
      "epoch:13 step:12997 [D loss: 0.578175, acc.: 70.31%] [G loss: 1.067881]\n",
      "epoch:13 step:12998 [D loss: 0.734637, acc.: 53.91%] [G loss: 1.138921]\n",
      "epoch:13 step:12999 [D loss: 0.675515, acc.: 61.72%] [G loss: 1.311972]\n",
      "epoch:13 step:13000 [D loss: 0.486734, acc.: 75.00%] [G loss: 1.233627]\n",
      "##############\n",
      "[2.74212394 2.23769595 1.89829504 2.37227802 1.03082673 5.85560822\n",
      " 2.32508097 2.83791694 3.93908701 5.83082034]\n",
      "##########\n",
      "epoch:13 step:13001 [D loss: 0.531615, acc.: 73.44%] [G loss: 1.315511]\n",
      "epoch:13 step:13002 [D loss: 0.558794, acc.: 72.66%] [G loss: 1.037354]\n",
      "epoch:13 step:13003 [D loss: 0.571409, acc.: 72.66%] [G loss: 1.422991]\n",
      "epoch:13 step:13004 [D loss: 0.595920, acc.: 65.62%] [G loss: 1.306378]\n",
      "epoch:13 step:13005 [D loss: 0.665100, acc.: 66.41%] [G loss: 1.454732]\n",
      "epoch:13 step:13006 [D loss: 0.589673, acc.: 68.75%] [G loss: 1.171901]\n",
      "epoch:13 step:13007 [D loss: 0.523169, acc.: 77.34%] [G loss: 1.225043]\n",
      "epoch:13 step:13008 [D loss: 0.535925, acc.: 75.00%] [G loss: 1.457029]\n",
      "epoch:13 step:13009 [D loss: 0.713898, acc.: 58.59%] [G loss: 0.858938]\n",
      "epoch:13 step:13010 [D loss: 0.513063, acc.: 75.00%] [G loss: 1.143965]\n",
      "epoch:13 step:13011 [D loss: 0.535883, acc.: 74.22%] [G loss: 1.422173]\n",
      "epoch:13 step:13012 [D loss: 0.624318, acc.: 67.19%] [G loss: 1.191070]\n",
      "epoch:13 step:13013 [D loss: 0.584439, acc.: 70.31%] [G loss: 1.206428]\n",
      "epoch:13 step:13014 [D loss: 0.534017, acc.: 73.44%] [G loss: 1.255052]\n",
      "epoch:13 step:13015 [D loss: 0.583026, acc.: 70.31%] [G loss: 1.263467]\n",
      "epoch:13 step:13016 [D loss: 0.534734, acc.: 75.00%] [G loss: 1.037499]\n",
      "epoch:13 step:13017 [D loss: 0.572520, acc.: 75.78%] [G loss: 1.020927]\n",
      "epoch:13 step:13018 [D loss: 0.577783, acc.: 73.44%] [G loss: 1.000102]\n",
      "epoch:13 step:13019 [D loss: 0.465362, acc.: 78.91%] [G loss: 1.272697]\n",
      "epoch:13 step:13020 [D loss: 0.533589, acc.: 75.00%] [G loss: 1.085363]\n",
      "epoch:13 step:13021 [D loss: 0.503993, acc.: 72.66%] [G loss: 1.215787]\n",
      "epoch:13 step:13022 [D loss: 0.523249, acc.: 75.00%] [G loss: 1.463436]\n",
      "epoch:13 step:13023 [D loss: 0.535474, acc.: 75.00%] [G loss: 1.144479]\n",
      "epoch:13 step:13024 [D loss: 0.671972, acc.: 57.03%] [G loss: 1.244044]\n",
      "epoch:13 step:13025 [D loss: 0.678025, acc.: 54.69%] [G loss: 1.262081]\n",
      "epoch:13 step:13026 [D loss: 0.628951, acc.: 65.62%] [G loss: 1.043538]\n",
      "epoch:13 step:13027 [D loss: 0.668670, acc.: 62.50%] [G loss: 1.127303]\n",
      "epoch:13 step:13028 [D loss: 0.638824, acc.: 67.97%] [G loss: 0.983969]\n",
      "epoch:13 step:13029 [D loss: 0.605887, acc.: 62.50%] [G loss: 1.314760]\n",
      "epoch:13 step:13030 [D loss: 0.471780, acc.: 78.12%] [G loss: 1.418883]\n",
      "epoch:13 step:13031 [D loss: 0.635710, acc.: 67.19%] [G loss: 1.339686]\n",
      "epoch:13 step:13032 [D loss: 0.730771, acc.: 53.91%] [G loss: 1.175498]\n",
      "epoch:13 step:13033 [D loss: 0.623668, acc.: 63.28%] [G loss: 1.211239]\n",
      "epoch:13 step:13034 [D loss: 0.603764, acc.: 60.16%] [G loss: 1.245291]\n",
      "epoch:13 step:13035 [D loss: 0.611308, acc.: 67.19%] [G loss: 1.222301]\n",
      "epoch:13 step:13036 [D loss: 0.528858, acc.: 75.78%] [G loss: 1.190167]\n",
      "epoch:13 step:13037 [D loss: 0.518041, acc.: 75.00%] [G loss: 1.330477]\n",
      "epoch:13 step:13038 [D loss: 0.558296, acc.: 71.88%] [G loss: 1.199484]\n",
      "epoch:13 step:13039 [D loss: 0.500718, acc.: 79.69%] [G loss: 1.305819]\n",
      "epoch:13 step:13040 [D loss: 0.672404, acc.: 60.94%] [G loss: 1.431777]\n",
      "epoch:13 step:13041 [D loss: 0.538049, acc.: 74.22%] [G loss: 1.423212]\n",
      "epoch:13 step:13042 [D loss: 0.515756, acc.: 75.00%] [G loss: 1.166203]\n",
      "epoch:13 step:13043 [D loss: 0.528366, acc.: 70.31%] [G loss: 1.323889]\n",
      "epoch:13 step:13044 [D loss: 0.696349, acc.: 56.25%] [G loss: 1.471483]\n",
      "epoch:13 step:13045 [D loss: 0.658162, acc.: 58.59%] [G loss: 1.453729]\n",
      "epoch:13 step:13046 [D loss: 0.496701, acc.: 80.47%] [G loss: 1.411944]\n",
      "epoch:13 step:13047 [D loss: 0.584256, acc.: 66.41%] [G loss: 1.430573]\n",
      "epoch:13 step:13048 [D loss: 0.617490, acc.: 65.62%] [G loss: 0.944713]\n",
      "epoch:13 step:13049 [D loss: 0.554995, acc.: 69.53%] [G loss: 1.419841]\n",
      "epoch:13 step:13050 [D loss: 0.692510, acc.: 57.03%] [G loss: 1.122972]\n",
      "epoch:13 step:13051 [D loss: 0.478705, acc.: 82.03%] [G loss: 1.497172]\n",
      "epoch:13 step:13052 [D loss: 0.539843, acc.: 74.22%] [G loss: 1.294947]\n",
      "epoch:13 step:13053 [D loss: 0.629570, acc.: 65.62%] [G loss: 1.163724]\n",
      "epoch:13 step:13054 [D loss: 0.376306, acc.: 89.06%] [G loss: 1.513558]\n",
      "epoch:13 step:13055 [D loss: 0.552294, acc.: 72.66%] [G loss: 1.425685]\n",
      "epoch:13 step:13056 [D loss: 0.672763, acc.: 61.72%] [G loss: 1.260229]\n",
      "epoch:13 step:13057 [D loss: 0.599426, acc.: 68.75%] [G loss: 1.409344]\n",
      "epoch:13 step:13058 [D loss: 0.675731, acc.: 60.16%] [G loss: 1.136575]\n",
      "epoch:13 step:13059 [D loss: 0.533058, acc.: 73.44%] [G loss: 1.330894]\n",
      "epoch:13 step:13060 [D loss: 0.650430, acc.: 58.59%] [G loss: 1.091487]\n",
      "epoch:13 step:13061 [D loss: 0.610860, acc.: 67.19%] [G loss: 1.259505]\n",
      "epoch:13 step:13062 [D loss: 0.606688, acc.: 68.75%] [G loss: 1.253590]\n",
      "epoch:13 step:13063 [D loss: 0.648155, acc.: 66.41%] [G loss: 1.049911]\n",
      "epoch:13 step:13064 [D loss: 0.659144, acc.: 63.28%] [G loss: 1.312887]\n",
      "epoch:13 step:13065 [D loss: 0.612770, acc.: 66.41%] [G loss: 1.133297]\n",
      "epoch:13 step:13066 [D loss: 0.533453, acc.: 73.44%] [G loss: 1.192768]\n",
      "epoch:13 step:13067 [D loss: 0.598914, acc.: 68.75%] [G loss: 1.534317]\n",
      "epoch:13 step:13068 [D loss: 0.615266, acc.: 62.50%] [G loss: 1.532729]\n",
      "epoch:13 step:13069 [D loss: 0.618619, acc.: 61.72%] [G loss: 1.093525]\n",
      "epoch:13 step:13070 [D loss: 0.610256, acc.: 64.84%] [G loss: 1.359021]\n",
      "epoch:13 step:13071 [D loss: 0.567662, acc.: 68.75%] [G loss: 1.266312]\n",
      "epoch:13 step:13072 [D loss: 0.587638, acc.: 69.53%] [G loss: 1.068040]\n",
      "epoch:13 step:13073 [D loss: 0.518478, acc.: 79.69%] [G loss: 1.336056]\n",
      "epoch:13 step:13074 [D loss: 0.655953, acc.: 60.16%] [G loss: 1.215177]\n",
      "epoch:13 step:13075 [D loss: 0.629595, acc.: 67.19%] [G loss: 0.884449]\n",
      "epoch:13 step:13076 [D loss: 0.462451, acc.: 80.47%] [G loss: 1.194053]\n",
      "epoch:13 step:13077 [D loss: 0.559530, acc.: 68.75%] [G loss: 1.264306]\n",
      "epoch:13 step:13078 [D loss: 0.647475, acc.: 66.41%] [G loss: 1.391506]\n",
      "epoch:13 step:13079 [D loss: 0.653770, acc.: 60.16%] [G loss: 1.295197]\n",
      "epoch:13 step:13080 [D loss: 0.519898, acc.: 76.56%] [G loss: 1.230790]\n",
      "epoch:13 step:13081 [D loss: 0.490572, acc.: 82.03%] [G loss: 1.189664]\n",
      "epoch:13 step:13082 [D loss: 0.638100, acc.: 64.06%] [G loss: 1.092680]\n",
      "epoch:13 step:13083 [D loss: 0.589607, acc.: 65.62%] [G loss: 1.230565]\n",
      "epoch:13 step:13084 [D loss: 0.590316, acc.: 71.09%] [G loss: 1.294472]\n",
      "epoch:13 step:13085 [D loss: 0.501318, acc.: 78.12%] [G loss: 1.324223]\n",
      "epoch:13 step:13086 [D loss: 0.541605, acc.: 74.22%] [G loss: 1.347201]\n",
      "epoch:13 step:13087 [D loss: 0.517864, acc.: 74.22%] [G loss: 1.253977]\n",
      "epoch:13 step:13088 [D loss: 0.533444, acc.: 77.34%] [G loss: 1.403642]\n",
      "epoch:13 step:13089 [D loss: 0.647889, acc.: 66.41%] [G loss: 1.315386]\n",
      "epoch:13 step:13090 [D loss: 0.487081, acc.: 78.91%] [G loss: 1.170481]\n",
      "epoch:13 step:13091 [D loss: 0.493408, acc.: 77.34%] [G loss: 1.465144]\n",
      "epoch:13 step:13092 [D loss: 0.590375, acc.: 68.75%] [G loss: 1.368877]\n",
      "epoch:13 step:13093 [D loss: 0.797819, acc.: 51.56%] [G loss: 0.841764]\n",
      "epoch:13 step:13094 [D loss: 0.799777, acc.: 53.12%] [G loss: 1.538523]\n",
      "epoch:13 step:13095 [D loss: 0.502686, acc.: 75.78%] [G loss: 1.393220]\n",
      "epoch:13 step:13096 [D loss: 0.577288, acc.: 66.41%] [G loss: 1.186751]\n",
      "epoch:13 step:13097 [D loss: 0.583526, acc.: 67.97%] [G loss: 1.301394]\n",
      "epoch:13 step:13098 [D loss: 0.613548, acc.: 67.97%] [G loss: 1.054380]\n",
      "epoch:13 step:13099 [D loss: 0.601224, acc.: 67.19%] [G loss: 1.228096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13100 [D loss: 0.484438, acc.: 76.56%] [G loss: 1.322588]\n",
      "epoch:13 step:13101 [D loss: 0.684886, acc.: 60.94%] [G loss: 1.329750]\n",
      "epoch:13 step:13102 [D loss: 0.602052, acc.: 70.31%] [G loss: 1.082381]\n",
      "epoch:13 step:13103 [D loss: 0.530298, acc.: 73.44%] [G loss: 1.430417]\n",
      "epoch:13 step:13104 [D loss: 0.571961, acc.: 70.31%] [G loss: 1.092712]\n",
      "epoch:13 step:13105 [D loss: 0.553146, acc.: 74.22%] [G loss: 1.047225]\n",
      "epoch:13 step:13106 [D loss: 0.498613, acc.: 76.56%] [G loss: 1.480319]\n",
      "epoch:13 step:13107 [D loss: 0.474019, acc.: 78.12%] [G loss: 1.323936]\n",
      "epoch:13 step:13108 [D loss: 0.615309, acc.: 65.62%] [G loss: 1.150591]\n",
      "epoch:13 step:13109 [D loss: 0.595039, acc.: 69.53%] [G loss: 1.230976]\n",
      "epoch:13 step:13110 [D loss: 0.548733, acc.: 75.00%] [G loss: 1.112387]\n",
      "epoch:13 step:13111 [D loss: 0.593957, acc.: 69.53%] [G loss: 1.277440]\n",
      "epoch:13 step:13112 [D loss: 0.686431, acc.: 60.94%] [G loss: 1.064322]\n",
      "epoch:13 step:13113 [D loss: 0.578573, acc.: 64.06%] [G loss: 1.288961]\n",
      "epoch:13 step:13114 [D loss: 0.664791, acc.: 60.16%] [G loss: 1.230609]\n",
      "epoch:13 step:13115 [D loss: 0.512915, acc.: 78.91%] [G loss: 1.417820]\n",
      "epoch:13 step:13116 [D loss: 0.624769, acc.: 65.62%] [G loss: 1.292179]\n",
      "epoch:13 step:13117 [D loss: 0.620646, acc.: 69.53%] [G loss: 1.519195]\n",
      "epoch:13 step:13118 [D loss: 0.631799, acc.: 61.72%] [G loss: 1.550439]\n",
      "epoch:14 step:13119 [D loss: 0.555592, acc.: 72.66%] [G loss: 1.019526]\n",
      "epoch:14 step:13120 [D loss: 0.619887, acc.: 61.72%] [G loss: 1.189934]\n",
      "epoch:14 step:13121 [D loss: 0.585489, acc.: 67.19%] [G loss: 1.061481]\n",
      "epoch:14 step:13122 [D loss: 0.736532, acc.: 55.47%] [G loss: 1.116039]\n",
      "epoch:14 step:13123 [D loss: 0.638287, acc.: 64.06%] [G loss: 1.452236]\n",
      "epoch:14 step:13124 [D loss: 0.640841, acc.: 55.47%] [G loss: 1.222313]\n",
      "epoch:14 step:13125 [D loss: 0.522379, acc.: 73.44%] [G loss: 1.324649]\n",
      "epoch:14 step:13126 [D loss: 0.530537, acc.: 69.53%] [G loss: 1.109139]\n",
      "epoch:14 step:13127 [D loss: 0.601351, acc.: 69.53%] [G loss: 1.419431]\n",
      "epoch:14 step:13128 [D loss: 0.609101, acc.: 65.62%] [G loss: 1.163600]\n",
      "epoch:14 step:13129 [D loss: 0.395956, acc.: 87.50%] [G loss: 1.452988]\n",
      "epoch:14 step:13130 [D loss: 0.621840, acc.: 60.94%] [G loss: 1.422183]\n",
      "epoch:14 step:13131 [D loss: 0.687491, acc.: 58.59%] [G loss: 1.169262]\n",
      "epoch:14 step:13132 [D loss: 0.525267, acc.: 75.00%] [G loss: 1.107111]\n",
      "epoch:14 step:13133 [D loss: 0.444888, acc.: 81.25%] [G loss: 1.334218]\n",
      "epoch:14 step:13134 [D loss: 0.542524, acc.: 71.09%] [G loss: 1.381518]\n",
      "epoch:14 step:13135 [D loss: 0.542971, acc.: 71.88%] [G loss: 1.113257]\n",
      "epoch:14 step:13136 [D loss: 0.480266, acc.: 80.47%] [G loss: 1.174799]\n",
      "epoch:14 step:13137 [D loss: 0.707939, acc.: 57.81%] [G loss: 1.117860]\n",
      "epoch:14 step:13138 [D loss: 0.538293, acc.: 71.09%] [G loss: 1.086813]\n",
      "epoch:14 step:13139 [D loss: 0.613489, acc.: 68.75%] [G loss: 1.407023]\n",
      "epoch:14 step:13140 [D loss: 0.541984, acc.: 75.00%] [G loss: 1.364323]\n",
      "epoch:14 step:13141 [D loss: 0.707295, acc.: 56.25%] [G loss: 1.254447]\n",
      "epoch:14 step:13142 [D loss: 0.577266, acc.: 67.97%] [G loss: 1.227210]\n",
      "epoch:14 step:13143 [D loss: 0.497689, acc.: 80.47%] [G loss: 1.409292]\n",
      "epoch:14 step:13144 [D loss: 0.672861, acc.: 62.50%] [G loss: 1.231772]\n",
      "epoch:14 step:13145 [D loss: 0.676928, acc.: 64.84%] [G loss: 1.317701]\n",
      "epoch:14 step:13146 [D loss: 0.599554, acc.: 69.53%] [G loss: 1.662580]\n",
      "epoch:14 step:13147 [D loss: 0.571478, acc.: 71.09%] [G loss: 1.281790]\n",
      "epoch:14 step:13148 [D loss: 0.639665, acc.: 67.97%] [G loss: 1.092865]\n",
      "epoch:14 step:13149 [D loss: 0.662242, acc.: 61.72%] [G loss: 1.229651]\n",
      "epoch:14 step:13150 [D loss: 0.396901, acc.: 90.62%] [G loss: 1.181093]\n",
      "epoch:14 step:13151 [D loss: 0.488617, acc.: 79.69%] [G loss: 1.254575]\n",
      "epoch:14 step:13152 [D loss: 0.541944, acc.: 79.69%] [G loss: 1.405329]\n",
      "epoch:14 step:13153 [D loss: 0.571387, acc.: 71.09%] [G loss: 0.909806]\n",
      "epoch:14 step:13154 [D loss: 0.641473, acc.: 67.97%] [G loss: 1.277617]\n",
      "epoch:14 step:13155 [D loss: 0.626490, acc.: 71.09%] [G loss: 1.210811]\n",
      "epoch:14 step:13156 [D loss: 0.636802, acc.: 64.06%] [G loss: 1.227714]\n",
      "epoch:14 step:13157 [D loss: 0.550882, acc.: 69.53%] [G loss: 1.319018]\n",
      "epoch:14 step:13158 [D loss: 0.666121, acc.: 61.72%] [G loss: 1.180064]\n",
      "epoch:14 step:13159 [D loss: 0.642604, acc.: 67.97%] [G loss: 1.024687]\n",
      "epoch:14 step:13160 [D loss: 0.496507, acc.: 77.34%] [G loss: 1.143534]\n",
      "epoch:14 step:13161 [D loss: 0.706562, acc.: 53.91%] [G loss: 1.089421]\n",
      "epoch:14 step:13162 [D loss: 0.527607, acc.: 74.22%] [G loss: 1.442579]\n",
      "epoch:14 step:13163 [D loss: 0.555877, acc.: 76.56%] [G loss: 1.180635]\n",
      "epoch:14 step:13164 [D loss: 0.653261, acc.: 63.28%] [G loss: 0.986977]\n",
      "epoch:14 step:13165 [D loss: 0.594977, acc.: 67.97%] [G loss: 1.093187]\n",
      "epoch:14 step:13166 [D loss: 0.509363, acc.: 76.56%] [G loss: 1.435019]\n",
      "epoch:14 step:13167 [D loss: 0.524492, acc.: 74.22%] [G loss: 1.394496]\n",
      "epoch:14 step:13168 [D loss: 0.534046, acc.: 70.31%] [G loss: 1.401875]\n",
      "epoch:14 step:13169 [D loss: 0.635668, acc.: 70.31%] [G loss: 1.211332]\n",
      "epoch:14 step:13170 [D loss: 0.588289, acc.: 71.88%] [G loss: 1.233023]\n",
      "epoch:14 step:13171 [D loss: 0.532976, acc.: 75.00%] [G loss: 1.153342]\n",
      "epoch:14 step:13172 [D loss: 0.647920, acc.: 63.28%] [G loss: 1.212509]\n",
      "epoch:14 step:13173 [D loss: 0.629874, acc.: 66.41%] [G loss: 1.493682]\n",
      "epoch:14 step:13174 [D loss: 0.534912, acc.: 73.44%] [G loss: 1.580264]\n",
      "epoch:14 step:13175 [D loss: 0.596938, acc.: 63.28%] [G loss: 1.459805]\n",
      "epoch:14 step:13176 [D loss: 0.648718, acc.: 62.50%] [G loss: 1.169005]\n",
      "epoch:14 step:13177 [D loss: 0.504257, acc.: 78.12%] [G loss: 1.114713]\n",
      "epoch:14 step:13178 [D loss: 0.574063, acc.: 68.75%] [G loss: 0.934540]\n",
      "epoch:14 step:13179 [D loss: 0.502954, acc.: 76.56%] [G loss: 1.437096]\n",
      "epoch:14 step:13180 [D loss: 0.583182, acc.: 68.75%] [G loss: 1.302147]\n",
      "epoch:14 step:13181 [D loss: 0.593486, acc.: 67.19%] [G loss: 1.203131]\n",
      "epoch:14 step:13182 [D loss: 0.604540, acc.: 66.41%] [G loss: 1.212289]\n",
      "epoch:14 step:13183 [D loss: 0.490760, acc.: 82.03%] [G loss: 1.657721]\n",
      "epoch:14 step:13184 [D loss: 0.598611, acc.: 68.75%] [G loss: 1.267010]\n",
      "epoch:14 step:13185 [D loss: 0.481385, acc.: 78.12%] [G loss: 1.340222]\n",
      "epoch:14 step:13186 [D loss: 0.545017, acc.: 74.22%] [G loss: 1.124484]\n",
      "epoch:14 step:13187 [D loss: 0.669647, acc.: 61.72%] [G loss: 1.181698]\n",
      "epoch:14 step:13188 [D loss: 0.672575, acc.: 60.16%] [G loss: 1.120161]\n",
      "epoch:14 step:13189 [D loss: 0.616328, acc.: 68.75%] [G loss: 1.115531]\n",
      "epoch:14 step:13190 [D loss: 0.534115, acc.: 75.78%] [G loss: 1.279261]\n",
      "epoch:14 step:13191 [D loss: 0.422235, acc.: 84.38%] [G loss: 1.233101]\n",
      "epoch:14 step:13192 [D loss: 0.650407, acc.: 63.28%] [G loss: 1.153513]\n",
      "epoch:14 step:13193 [D loss: 0.548523, acc.: 74.22%] [G loss: 1.355580]\n",
      "epoch:14 step:13194 [D loss: 0.606915, acc.: 67.19%] [G loss: 1.276594]\n",
      "epoch:14 step:13195 [D loss: 0.637438, acc.: 61.72%] [G loss: 1.255039]\n",
      "epoch:14 step:13196 [D loss: 0.601324, acc.: 66.41%] [G loss: 1.204134]\n",
      "epoch:14 step:13197 [D loss: 0.574799, acc.: 71.09%] [G loss: 1.278815]\n",
      "epoch:14 step:13198 [D loss: 0.512323, acc.: 76.56%] [G loss: 1.214820]\n",
      "epoch:14 step:13199 [D loss: 0.684772, acc.: 61.72%] [G loss: 0.935611]\n",
      "epoch:14 step:13200 [D loss: 0.676726, acc.: 60.16%] [G loss: 1.236638]\n",
      "##############\n",
      "[2.67299934 2.09358596 1.86537427 2.68415274 0.80125675 5.69464809\n",
      " 2.28320717 2.66217699 3.97131804 5.38636718]\n",
      "##########\n",
      "epoch:14 step:13201 [D loss: 0.607666, acc.: 68.75%] [G loss: 1.055152]\n",
      "epoch:14 step:13202 [D loss: 0.571175, acc.: 71.88%] [G loss: 1.255829]\n",
      "epoch:14 step:13203 [D loss: 0.672039, acc.: 53.91%] [G loss: 1.118014]\n",
      "epoch:14 step:13204 [D loss: 0.533864, acc.: 75.00%] [G loss: 1.128840]\n",
      "epoch:14 step:13205 [D loss: 0.703318, acc.: 60.16%] [G loss: 1.147677]\n",
      "epoch:14 step:13206 [D loss: 0.429733, acc.: 82.03%] [G loss: 1.210575]\n",
      "epoch:14 step:13207 [D loss: 0.730882, acc.: 56.25%] [G loss: 0.977802]\n",
      "epoch:14 step:13208 [D loss: 0.472646, acc.: 78.91%] [G loss: 1.231923]\n",
      "epoch:14 step:13209 [D loss: 0.520330, acc.: 74.22%] [G loss: 1.140448]\n",
      "epoch:14 step:13210 [D loss: 0.674390, acc.: 64.06%] [G loss: 1.125034]\n",
      "epoch:14 step:13211 [D loss: 0.529751, acc.: 75.78%] [G loss: 1.541663]\n",
      "epoch:14 step:13212 [D loss: 0.669946, acc.: 60.16%] [G loss: 1.191679]\n",
      "epoch:14 step:13213 [D loss: 0.539716, acc.: 74.22%] [G loss: 1.417415]\n",
      "epoch:14 step:13214 [D loss: 0.596434, acc.: 66.41%] [G loss: 1.425644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13215 [D loss: 0.566904, acc.: 69.53%] [G loss: 1.528916]\n",
      "epoch:14 step:13216 [D loss: 0.531395, acc.: 73.44%] [G loss: 1.225550]\n",
      "epoch:14 step:13217 [D loss: 0.538982, acc.: 72.66%] [G loss: 1.361364]\n",
      "epoch:14 step:13218 [D loss: 0.579740, acc.: 71.88%] [G loss: 1.488564]\n",
      "epoch:14 step:13219 [D loss: 0.581724, acc.: 71.09%] [G loss: 1.072213]\n",
      "epoch:14 step:13220 [D loss: 0.722973, acc.: 57.81%] [G loss: 1.041334]\n",
      "epoch:14 step:13221 [D loss: 0.631685, acc.: 64.84%] [G loss: 1.192467]\n",
      "epoch:14 step:13222 [D loss: 0.542121, acc.: 76.56%] [G loss: 1.148808]\n",
      "epoch:14 step:13223 [D loss: 0.479348, acc.: 77.34%] [G loss: 1.311082]\n",
      "epoch:14 step:13224 [D loss: 0.617337, acc.: 70.31%] [G loss: 1.155574]\n",
      "epoch:14 step:13225 [D loss: 0.607119, acc.: 68.75%] [G loss: 1.084203]\n",
      "epoch:14 step:13226 [D loss: 0.550894, acc.: 71.88%] [G loss: 1.308576]\n",
      "epoch:14 step:13227 [D loss: 0.583982, acc.: 70.31%] [G loss: 1.397896]\n",
      "epoch:14 step:13228 [D loss: 0.532813, acc.: 71.09%] [G loss: 1.080250]\n",
      "epoch:14 step:13229 [D loss: 0.707774, acc.: 58.59%] [G loss: 1.016214]\n",
      "epoch:14 step:13230 [D loss: 0.652169, acc.: 64.84%] [G loss: 1.357035]\n",
      "epoch:14 step:13231 [D loss: 0.646824, acc.: 66.41%] [G loss: 1.267767]\n",
      "epoch:14 step:13232 [D loss: 0.486011, acc.: 74.22%] [G loss: 1.307368]\n",
      "epoch:14 step:13233 [D loss: 0.641541, acc.: 64.06%] [G loss: 1.106720]\n",
      "epoch:14 step:13234 [D loss: 0.640398, acc.: 66.41%] [G loss: 1.072076]\n",
      "epoch:14 step:13235 [D loss: 0.465735, acc.: 79.69%] [G loss: 1.209358]\n",
      "epoch:14 step:13236 [D loss: 0.596087, acc.: 68.75%] [G loss: 1.173340]\n",
      "epoch:14 step:13237 [D loss: 0.553025, acc.: 72.66%] [G loss: 1.175784]\n",
      "epoch:14 step:13238 [D loss: 0.571456, acc.: 65.62%] [G loss: 1.410011]\n",
      "epoch:14 step:13239 [D loss: 0.584390, acc.: 68.75%] [G loss: 1.319771]\n",
      "epoch:14 step:13240 [D loss: 0.597421, acc.: 64.84%] [G loss: 1.102086]\n",
      "epoch:14 step:13241 [D loss: 0.619484, acc.: 63.28%] [G loss: 1.350183]\n",
      "epoch:14 step:13242 [D loss: 0.443377, acc.: 82.03%] [G loss: 1.502028]\n",
      "epoch:14 step:13243 [D loss: 0.672398, acc.: 58.59%] [G loss: 1.224808]\n",
      "epoch:14 step:13244 [D loss: 0.552858, acc.: 73.44%] [G loss: 1.392745]\n",
      "epoch:14 step:13245 [D loss: 0.512005, acc.: 77.34%] [G loss: 1.227004]\n",
      "epoch:14 step:13246 [D loss: 0.569404, acc.: 73.44%] [G loss: 1.107781]\n",
      "epoch:14 step:13247 [D loss: 0.561964, acc.: 73.44%] [G loss: 1.151018]\n",
      "epoch:14 step:13248 [D loss: 0.478688, acc.: 78.12%] [G loss: 1.438579]\n",
      "epoch:14 step:13249 [D loss: 0.478519, acc.: 81.25%] [G loss: 1.321444]\n",
      "epoch:14 step:13250 [D loss: 0.757656, acc.: 51.56%] [G loss: 1.150247]\n",
      "epoch:14 step:13251 [D loss: 0.569888, acc.: 71.88%] [G loss: 1.074247]\n",
      "epoch:14 step:13252 [D loss: 0.545440, acc.: 71.09%] [G loss: 1.195515]\n",
      "epoch:14 step:13253 [D loss: 0.523238, acc.: 76.56%] [G loss: 1.429329]\n",
      "epoch:14 step:13254 [D loss: 0.743172, acc.: 53.12%] [G loss: 1.018241]\n",
      "epoch:14 step:13255 [D loss: 0.608101, acc.: 66.41%] [G loss: 1.073972]\n",
      "epoch:14 step:13256 [D loss: 0.528016, acc.: 72.66%] [G loss: 1.378127]\n",
      "epoch:14 step:13257 [D loss: 0.501910, acc.: 74.22%] [G loss: 1.300065]\n",
      "epoch:14 step:13258 [D loss: 0.578677, acc.: 70.31%] [G loss: 1.253224]\n",
      "epoch:14 step:13259 [D loss: 0.619792, acc.: 67.19%] [G loss: 1.431782]\n",
      "epoch:14 step:13260 [D loss: 0.497493, acc.: 79.69%] [G loss: 1.270191]\n",
      "epoch:14 step:13261 [D loss: 0.600401, acc.: 67.19%] [G loss: 1.338561]\n",
      "epoch:14 step:13262 [D loss: 0.704744, acc.: 59.38%] [G loss: 1.141400]\n",
      "epoch:14 step:13263 [D loss: 0.645225, acc.: 66.41%] [G loss: 1.233946]\n",
      "epoch:14 step:13264 [D loss: 0.628609, acc.: 65.62%] [G loss: 1.454110]\n",
      "epoch:14 step:13265 [D loss: 0.558842, acc.: 68.75%] [G loss: 1.096965]\n",
      "epoch:14 step:13266 [D loss: 0.567030, acc.: 75.00%] [G loss: 1.051238]\n",
      "epoch:14 step:13267 [D loss: 0.532011, acc.: 76.56%] [G loss: 1.162142]\n",
      "epoch:14 step:13268 [D loss: 0.680866, acc.: 54.69%] [G loss: 1.201092]\n",
      "epoch:14 step:13269 [D loss: 0.439521, acc.: 86.72%] [G loss: 1.385263]\n",
      "epoch:14 step:13270 [D loss: 0.671778, acc.: 57.03%] [G loss: 0.812396]\n",
      "epoch:14 step:13271 [D loss: 0.533569, acc.: 75.78%] [G loss: 1.364385]\n",
      "epoch:14 step:13272 [D loss: 0.537861, acc.: 72.66%] [G loss: 1.360796]\n",
      "epoch:14 step:13273 [D loss: 0.721634, acc.: 57.81%] [G loss: 1.117192]\n",
      "epoch:14 step:13274 [D loss: 0.491592, acc.: 78.91%] [G loss: 1.341592]\n",
      "epoch:14 step:13275 [D loss: 0.616354, acc.: 62.50%] [G loss: 1.274692]\n",
      "epoch:14 step:13276 [D loss: 0.632251, acc.: 62.50%] [G loss: 1.378988]\n",
      "epoch:14 step:13277 [D loss: 0.598477, acc.: 66.41%] [G loss: 1.443571]\n",
      "epoch:14 step:13278 [D loss: 0.552087, acc.: 71.88%] [G loss: 1.358303]\n",
      "epoch:14 step:13279 [D loss: 0.457672, acc.: 78.91%] [G loss: 1.511502]\n",
      "epoch:14 step:13280 [D loss: 0.643597, acc.: 66.41%] [G loss: 1.124842]\n",
      "epoch:14 step:13281 [D loss: 0.626907, acc.: 63.28%] [G loss: 1.359209]\n",
      "epoch:14 step:13282 [D loss: 0.481939, acc.: 77.34%] [G loss: 1.249971]\n",
      "epoch:14 step:13283 [D loss: 0.663886, acc.: 61.72%] [G loss: 0.982520]\n",
      "epoch:14 step:13284 [D loss: 0.600790, acc.: 68.75%] [G loss: 0.986421]\n",
      "epoch:14 step:13285 [D loss: 0.486397, acc.: 78.12%] [G loss: 1.434287]\n",
      "epoch:14 step:13286 [D loss: 0.554734, acc.: 72.66%] [G loss: 1.174974]\n",
      "epoch:14 step:13287 [D loss: 0.543556, acc.: 66.41%] [G loss: 1.374965]\n",
      "epoch:14 step:13288 [D loss: 0.654262, acc.: 63.28%] [G loss: 1.106423]\n",
      "epoch:14 step:13289 [D loss: 0.594863, acc.: 67.97%] [G loss: 1.503551]\n",
      "epoch:14 step:13290 [D loss: 0.582881, acc.: 67.97%] [G loss: 1.139166]\n",
      "epoch:14 step:13291 [D loss: 0.548949, acc.: 68.75%] [G loss: 1.232514]\n",
      "epoch:14 step:13292 [D loss: 0.551666, acc.: 75.00%] [G loss: 1.334920]\n",
      "epoch:14 step:13293 [D loss: 0.582037, acc.: 69.53%] [G loss: 1.235063]\n",
      "epoch:14 step:13294 [D loss: 0.523128, acc.: 78.12%] [G loss: 1.217081]\n",
      "epoch:14 step:13295 [D loss: 0.480524, acc.: 81.25%] [G loss: 1.074601]\n",
      "epoch:14 step:13296 [D loss: 0.612474, acc.: 67.19%] [G loss: 1.337683]\n",
      "epoch:14 step:13297 [D loss: 0.580592, acc.: 71.09%] [G loss: 1.237352]\n",
      "epoch:14 step:13298 [D loss: 0.392877, acc.: 82.81%] [G loss: 1.221902]\n",
      "epoch:14 step:13299 [D loss: 0.552206, acc.: 70.31%] [G loss: 1.131532]\n",
      "epoch:14 step:13300 [D loss: 0.567633, acc.: 70.31%] [G loss: 1.122855]\n",
      "epoch:14 step:13301 [D loss: 0.648162, acc.: 60.16%] [G loss: 1.354496]\n",
      "epoch:14 step:13302 [D loss: 0.621519, acc.: 68.75%] [G loss: 1.254225]\n",
      "epoch:14 step:13303 [D loss: 0.540179, acc.: 70.31%] [G loss: 1.392433]\n",
      "epoch:14 step:13304 [D loss: 0.597483, acc.: 67.19%] [G loss: 1.253488]\n",
      "epoch:14 step:13305 [D loss: 0.469734, acc.: 81.25%] [G loss: 1.451935]\n",
      "epoch:14 step:13306 [D loss: 0.612894, acc.: 64.06%] [G loss: 1.332246]\n",
      "epoch:14 step:13307 [D loss: 0.548536, acc.: 75.78%] [G loss: 1.598154]\n",
      "epoch:14 step:13308 [D loss: 0.739007, acc.: 56.25%] [G loss: 1.289274]\n",
      "epoch:14 step:13309 [D loss: 0.565338, acc.: 69.53%] [G loss: 1.277682]\n",
      "epoch:14 step:13310 [D loss: 0.478874, acc.: 78.91%] [G loss: 1.449557]\n",
      "epoch:14 step:13311 [D loss: 0.569773, acc.: 75.00%] [G loss: 1.392228]\n",
      "epoch:14 step:13312 [D loss: 0.655276, acc.: 61.72%] [G loss: 1.077401]\n",
      "epoch:14 step:13313 [D loss: 0.546620, acc.: 67.97%] [G loss: 1.232151]\n",
      "epoch:14 step:13314 [D loss: 0.641369, acc.: 64.84%] [G loss: 1.177730]\n",
      "epoch:14 step:13315 [D loss: 0.495148, acc.: 78.12%] [G loss: 1.551055]\n",
      "epoch:14 step:13316 [D loss: 0.563265, acc.: 67.97%] [G loss: 1.306668]\n",
      "epoch:14 step:13317 [D loss: 0.486096, acc.: 81.25%] [G loss: 1.312099]\n",
      "epoch:14 step:13318 [D loss: 0.543414, acc.: 70.31%] [G loss: 1.063630]\n",
      "epoch:14 step:13319 [D loss: 0.583430, acc.: 68.75%] [G loss: 1.232477]\n",
      "epoch:14 step:13320 [D loss: 0.423077, acc.: 82.03%] [G loss: 1.381057]\n",
      "epoch:14 step:13321 [D loss: 0.536497, acc.: 76.56%] [G loss: 1.232392]\n",
      "epoch:14 step:13322 [D loss: 0.619887, acc.: 65.62%] [G loss: 1.531885]\n",
      "epoch:14 step:13323 [D loss: 0.588348, acc.: 66.41%] [G loss: 1.253936]\n",
      "epoch:14 step:13324 [D loss: 0.509430, acc.: 74.22%] [G loss: 1.126186]\n",
      "epoch:14 step:13325 [D loss: 0.638666, acc.: 63.28%] [G loss: 1.095237]\n",
      "epoch:14 step:13326 [D loss: 0.454741, acc.: 78.91%] [G loss: 1.168184]\n",
      "epoch:14 step:13327 [D loss: 0.565707, acc.: 71.09%] [G loss: 1.288357]\n",
      "epoch:14 step:13328 [D loss: 0.543482, acc.: 70.31%] [G loss: 1.136751]\n",
      "epoch:14 step:13329 [D loss: 0.553712, acc.: 78.12%] [G loss: 1.096463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13330 [D loss: 0.540949, acc.: 71.09%] [G loss: 0.988594]\n",
      "epoch:14 step:13331 [D loss: 0.557465, acc.: 69.53%] [G loss: 1.104882]\n",
      "epoch:14 step:13332 [D loss: 0.699796, acc.: 60.94%] [G loss: 1.441021]\n",
      "epoch:14 step:13333 [D loss: 0.606085, acc.: 70.31%] [G loss: 1.265305]\n",
      "epoch:14 step:13334 [D loss: 0.774843, acc.: 52.34%] [G loss: 1.342601]\n",
      "epoch:14 step:13335 [D loss: 0.596494, acc.: 69.53%] [G loss: 1.194633]\n",
      "epoch:14 step:13336 [D loss: 0.679224, acc.: 60.94%] [G loss: 1.396315]\n",
      "epoch:14 step:13337 [D loss: 0.443460, acc.: 82.03%] [G loss: 1.093832]\n",
      "epoch:14 step:13338 [D loss: 0.645263, acc.: 65.62%] [G loss: 1.206407]\n",
      "epoch:14 step:13339 [D loss: 0.647146, acc.: 61.72%] [G loss: 1.308788]\n",
      "epoch:14 step:13340 [D loss: 0.631064, acc.: 71.09%] [G loss: 1.230736]\n",
      "epoch:14 step:13341 [D loss: 0.519131, acc.: 75.00%] [G loss: 1.389219]\n",
      "epoch:14 step:13342 [D loss: 0.584789, acc.: 66.41%] [G loss: 1.041251]\n",
      "epoch:14 step:13343 [D loss: 0.513420, acc.: 78.12%] [G loss: 1.317619]\n",
      "epoch:14 step:13344 [D loss: 0.704433, acc.: 58.59%] [G loss: 1.265044]\n",
      "epoch:14 step:13345 [D loss: 0.520299, acc.: 82.03%] [G loss: 1.234975]\n",
      "epoch:14 step:13346 [D loss: 0.693933, acc.: 56.25%] [G loss: 1.029895]\n",
      "epoch:14 step:13347 [D loss: 0.627996, acc.: 66.41%] [G loss: 1.145102]\n",
      "epoch:14 step:13348 [D loss: 0.524038, acc.: 75.78%] [G loss: 1.138649]\n",
      "epoch:14 step:13349 [D loss: 0.516861, acc.: 82.81%] [G loss: 1.290681]\n",
      "epoch:14 step:13350 [D loss: 0.562673, acc.: 70.31%] [G loss: 1.143291]\n",
      "epoch:14 step:13351 [D loss: 0.658596, acc.: 61.72%] [G loss: 1.433424]\n",
      "epoch:14 step:13352 [D loss: 0.534274, acc.: 71.88%] [G loss: 1.262625]\n",
      "epoch:14 step:13353 [D loss: 0.583238, acc.: 64.84%] [G loss: 1.368721]\n",
      "epoch:14 step:13354 [D loss: 0.645603, acc.: 64.84%] [G loss: 1.211028]\n",
      "epoch:14 step:13355 [D loss: 0.580593, acc.: 69.53%] [G loss: 1.317571]\n",
      "epoch:14 step:13356 [D loss: 0.620784, acc.: 66.41%] [G loss: 1.291784]\n",
      "epoch:14 step:13357 [D loss: 0.581603, acc.: 74.22%] [G loss: 1.118484]\n",
      "epoch:14 step:13358 [D loss: 0.553114, acc.: 75.78%] [G loss: 1.228913]\n",
      "epoch:14 step:13359 [D loss: 0.554992, acc.: 71.09%] [G loss: 1.571133]\n",
      "epoch:14 step:13360 [D loss: 0.654565, acc.: 60.16%] [G loss: 1.407054]\n",
      "epoch:14 step:13361 [D loss: 0.523099, acc.: 71.88%] [G loss: 1.241214]\n",
      "epoch:14 step:13362 [D loss: 0.640396, acc.: 66.41%] [G loss: 1.478013]\n",
      "epoch:14 step:13363 [D loss: 0.518406, acc.: 76.56%] [G loss: 1.126069]\n",
      "epoch:14 step:13364 [D loss: 0.509697, acc.: 78.91%] [G loss: 1.280504]\n",
      "epoch:14 step:13365 [D loss: 0.641644, acc.: 62.50%] [G loss: 1.232646]\n",
      "epoch:14 step:13366 [D loss: 0.602420, acc.: 64.06%] [G loss: 1.425729]\n",
      "epoch:14 step:13367 [D loss: 0.643894, acc.: 66.41%] [G loss: 1.329598]\n",
      "epoch:14 step:13368 [D loss: 0.493932, acc.: 78.91%] [G loss: 1.658043]\n",
      "epoch:14 step:13369 [D loss: 0.657248, acc.: 63.28%] [G loss: 1.120823]\n",
      "epoch:14 step:13370 [D loss: 0.461258, acc.: 78.91%] [G loss: 1.071001]\n",
      "epoch:14 step:13371 [D loss: 0.533968, acc.: 75.78%] [G loss: 1.321617]\n",
      "epoch:14 step:13372 [D loss: 0.609523, acc.: 67.97%] [G loss: 1.202031]\n",
      "epoch:14 step:13373 [D loss: 0.519511, acc.: 78.91%] [G loss: 1.252589]\n",
      "epoch:14 step:13374 [D loss: 0.478443, acc.: 83.59%] [G loss: 1.400753]\n",
      "epoch:14 step:13375 [D loss: 0.599249, acc.: 66.41%] [G loss: 1.348772]\n",
      "epoch:14 step:13376 [D loss: 0.590167, acc.: 70.31%] [G loss: 1.058778]\n",
      "epoch:14 step:13377 [D loss: 0.548022, acc.: 69.53%] [G loss: 1.248138]\n",
      "epoch:14 step:13378 [D loss: 0.550883, acc.: 67.97%] [G loss: 1.278000]\n",
      "epoch:14 step:13379 [D loss: 0.511205, acc.: 78.91%] [G loss: 1.113344]\n",
      "epoch:14 step:13380 [D loss: 0.707253, acc.: 57.81%] [G loss: 1.207136]\n",
      "epoch:14 step:13381 [D loss: 0.548702, acc.: 72.66%] [G loss: 1.017763]\n",
      "epoch:14 step:13382 [D loss: 0.663493, acc.: 63.28%] [G loss: 0.957330]\n",
      "epoch:14 step:13383 [D loss: 0.502470, acc.: 77.34%] [G loss: 1.256613]\n",
      "epoch:14 step:13384 [D loss: 0.514856, acc.: 74.22%] [G loss: 1.279082]\n",
      "epoch:14 step:13385 [D loss: 0.673025, acc.: 57.81%] [G loss: 1.129952]\n",
      "epoch:14 step:13386 [D loss: 0.639112, acc.: 60.16%] [G loss: 1.232366]\n",
      "epoch:14 step:13387 [D loss: 0.481722, acc.: 78.91%] [G loss: 1.208646]\n",
      "epoch:14 step:13388 [D loss: 0.602749, acc.: 65.62%] [G loss: 1.327313]\n",
      "epoch:14 step:13389 [D loss: 0.588126, acc.: 66.41%] [G loss: 1.392079]\n",
      "epoch:14 step:13390 [D loss: 0.632588, acc.: 64.84%] [G loss: 1.467095]\n",
      "epoch:14 step:13391 [D loss: 0.704825, acc.: 57.81%] [G loss: 0.990544]\n",
      "epoch:14 step:13392 [D loss: 0.794896, acc.: 46.88%] [G loss: 0.878331]\n",
      "epoch:14 step:13393 [D loss: 1.077681, acc.: 26.56%] [G loss: 1.011796]\n",
      "epoch:14 step:13394 [D loss: 0.644467, acc.: 65.62%] [G loss: 1.526649]\n",
      "epoch:14 step:13395 [D loss: 0.512174, acc.: 76.56%] [G loss: 1.220240]\n",
      "epoch:14 step:13396 [D loss: 0.433385, acc.: 80.47%] [G loss: 1.393208]\n",
      "epoch:14 step:13397 [D loss: 0.777166, acc.: 50.00%] [G loss: 1.036686]\n",
      "epoch:14 step:13398 [D loss: 0.642101, acc.: 68.75%] [G loss: 1.198474]\n",
      "epoch:14 step:13399 [D loss: 0.534186, acc.: 71.88%] [G loss: 1.314083]\n",
      "epoch:14 step:13400 [D loss: 0.589546, acc.: 69.53%] [G loss: 1.294237]\n",
      "##############\n",
      "[2.75100329 2.23095242 1.93005549 2.85702817 0.6785551  6.2685112\n",
      " 2.1531279  2.52946558 3.79399704 5.05335714]\n",
      "##########\n",
      "epoch:14 step:13401 [D loss: 0.500351, acc.: 79.69%] [G loss: 1.090065]\n",
      "epoch:14 step:13402 [D loss: 0.523809, acc.: 72.66%] [G loss: 1.521688]\n",
      "epoch:14 step:13403 [D loss: 0.572090, acc.: 74.22%] [G loss: 1.141816]\n",
      "epoch:14 step:13404 [D loss: 0.554250, acc.: 70.31%] [G loss: 1.447635]\n",
      "epoch:14 step:13405 [D loss: 0.452971, acc.: 79.69%] [G loss: 1.504272]\n",
      "epoch:14 step:13406 [D loss: 0.625071, acc.: 60.94%] [G loss: 1.761499]\n",
      "epoch:14 step:13407 [D loss: 0.643864, acc.: 62.50%] [G loss: 1.314379]\n",
      "epoch:14 step:13408 [D loss: 0.477103, acc.: 82.03%] [G loss: 1.107919]\n",
      "epoch:14 step:13409 [D loss: 0.563898, acc.: 70.31%] [G loss: 1.137297]\n",
      "epoch:14 step:13410 [D loss: 0.501965, acc.: 76.56%] [G loss: 1.199289]\n",
      "epoch:14 step:13411 [D loss: 0.583732, acc.: 67.19%] [G loss: 0.949298]\n",
      "epoch:14 step:13412 [D loss: 0.582154, acc.: 70.31%] [G loss: 1.010006]\n",
      "epoch:14 step:13413 [D loss: 0.611291, acc.: 66.41%] [G loss: 1.177067]\n",
      "epoch:14 step:13414 [D loss: 0.587039, acc.: 67.19%] [G loss: 1.083841]\n",
      "epoch:14 step:13415 [D loss: 0.745595, acc.: 55.47%] [G loss: 1.451331]\n",
      "epoch:14 step:13416 [D loss: 0.800522, acc.: 51.56%] [G loss: 1.001817]\n",
      "epoch:14 step:13417 [D loss: 0.657408, acc.: 59.38%] [G loss: 1.159750]\n",
      "epoch:14 step:13418 [D loss: 0.516857, acc.: 75.00%] [G loss: 1.506722]\n",
      "epoch:14 step:13419 [D loss: 0.630611, acc.: 60.16%] [G loss: 1.113579]\n",
      "epoch:14 step:13420 [D loss: 0.555026, acc.: 72.66%] [G loss: 1.008520]\n",
      "epoch:14 step:13421 [D loss: 0.566639, acc.: 75.78%] [G loss: 1.115766]\n",
      "epoch:14 step:13422 [D loss: 0.556411, acc.: 71.88%] [G loss: 1.308038]\n",
      "epoch:14 step:13423 [D loss: 0.640067, acc.: 64.06%] [G loss: 1.246485]\n",
      "epoch:14 step:13424 [D loss: 0.644695, acc.: 60.94%] [G loss: 1.230497]\n",
      "epoch:14 step:13425 [D loss: 0.491017, acc.: 75.00%] [G loss: 1.322479]\n",
      "epoch:14 step:13426 [D loss: 0.553195, acc.: 74.22%] [G loss: 1.289398]\n",
      "epoch:14 step:13427 [D loss: 0.593228, acc.: 63.28%] [G loss: 1.039412]\n",
      "epoch:14 step:13428 [D loss: 0.655591, acc.: 67.97%] [G loss: 1.254719]\n",
      "epoch:14 step:13429 [D loss: 0.474741, acc.: 82.81%] [G loss: 1.393559]\n",
      "epoch:14 step:13430 [D loss: 0.664730, acc.: 65.62%] [G loss: 1.283657]\n",
      "epoch:14 step:13431 [D loss: 0.616586, acc.: 68.75%] [G loss: 1.018095]\n",
      "epoch:14 step:13432 [D loss: 0.507918, acc.: 78.91%] [G loss: 1.281093]\n",
      "epoch:14 step:13433 [D loss: 0.498483, acc.: 75.00%] [G loss: 1.252132]\n",
      "epoch:14 step:13434 [D loss: 0.620442, acc.: 63.28%] [G loss: 1.077483]\n",
      "epoch:14 step:13435 [D loss: 0.535699, acc.: 67.97%] [G loss: 1.079946]\n",
      "epoch:14 step:13436 [D loss: 0.522743, acc.: 73.44%] [G loss: 1.308449]\n",
      "epoch:14 step:13437 [D loss: 0.573985, acc.: 70.31%] [G loss: 0.929934]\n",
      "epoch:14 step:13438 [D loss: 0.687075, acc.: 60.16%] [G loss: 1.132632]\n",
      "epoch:14 step:13439 [D loss: 0.619602, acc.: 67.19%] [G loss: 1.300165]\n",
      "epoch:14 step:13440 [D loss: 0.539403, acc.: 71.09%] [G loss: 1.147679]\n",
      "epoch:14 step:13441 [D loss: 0.513920, acc.: 75.00%] [G loss: 1.662194]\n",
      "epoch:14 step:13442 [D loss: 0.653398, acc.: 60.16%] [G loss: 1.300061]\n",
      "epoch:14 step:13443 [D loss: 0.512140, acc.: 77.34%] [G loss: 1.209360]\n",
      "epoch:14 step:13444 [D loss: 0.500187, acc.: 78.12%] [G loss: 1.336024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13445 [D loss: 0.515968, acc.: 77.34%] [G loss: 1.037431]\n",
      "epoch:14 step:13446 [D loss: 0.539329, acc.: 71.09%] [G loss: 1.348851]\n",
      "epoch:14 step:13447 [D loss: 0.539584, acc.: 73.44%] [G loss: 1.237213]\n",
      "epoch:14 step:13448 [D loss: 0.530800, acc.: 70.31%] [G loss: 1.166931]\n",
      "epoch:14 step:13449 [D loss: 0.580789, acc.: 67.97%] [G loss: 1.350922]\n",
      "epoch:14 step:13450 [D loss: 0.557093, acc.: 74.22%] [G loss: 1.339953]\n",
      "epoch:14 step:13451 [D loss: 0.580518, acc.: 70.31%] [G loss: 1.255129]\n",
      "epoch:14 step:13452 [D loss: 0.505034, acc.: 73.44%] [G loss: 1.149459]\n",
      "epoch:14 step:13453 [D loss: 0.471918, acc.: 82.03%] [G loss: 1.472656]\n",
      "epoch:14 step:13454 [D loss: 0.541113, acc.: 71.88%] [G loss: 1.371675]\n",
      "epoch:14 step:13455 [D loss: 0.607045, acc.: 67.19%] [G loss: 1.200814]\n",
      "epoch:14 step:13456 [D loss: 0.584140, acc.: 69.53%] [G loss: 1.264577]\n",
      "epoch:14 step:13457 [D loss: 0.589166, acc.: 64.06%] [G loss: 1.307464]\n",
      "epoch:14 step:13458 [D loss: 0.541032, acc.: 74.22%] [G loss: 0.927992]\n",
      "epoch:14 step:13459 [D loss: 0.612769, acc.: 67.97%] [G loss: 1.418256]\n",
      "epoch:14 step:13460 [D loss: 0.538171, acc.: 75.00%] [G loss: 1.577293]\n",
      "epoch:14 step:13461 [D loss: 0.573946, acc.: 67.19%] [G loss: 1.094414]\n",
      "epoch:14 step:13462 [D loss: 0.580723, acc.: 65.62%] [G loss: 0.932738]\n",
      "epoch:14 step:13463 [D loss: 0.532650, acc.: 76.56%] [G loss: 1.107152]\n",
      "epoch:14 step:13464 [D loss: 0.644129, acc.: 63.28%] [G loss: 1.218633]\n",
      "epoch:14 step:13465 [D loss: 0.675635, acc.: 57.81%] [G loss: 0.992185]\n",
      "epoch:14 step:13466 [D loss: 0.613307, acc.: 69.53%] [G loss: 1.224186]\n",
      "epoch:14 step:13467 [D loss: 0.450556, acc.: 80.47%] [G loss: 1.357610]\n",
      "epoch:14 step:13468 [D loss: 0.540055, acc.: 75.00%] [G loss: 1.117402]\n",
      "epoch:14 step:13469 [D loss: 0.566286, acc.: 74.22%] [G loss: 1.026606]\n",
      "epoch:14 step:13470 [D loss: 0.628676, acc.: 66.41%] [G loss: 1.255892]\n",
      "epoch:14 step:13471 [D loss: 0.618191, acc.: 64.06%] [G loss: 1.170233]\n",
      "epoch:14 step:13472 [D loss: 0.537292, acc.: 74.22%] [G loss: 1.361841]\n",
      "epoch:14 step:13473 [D loss: 0.490730, acc.: 78.91%] [G loss: 1.370934]\n",
      "epoch:14 step:13474 [D loss: 0.419661, acc.: 86.72%] [G loss: 1.386089]\n",
      "epoch:14 step:13475 [D loss: 0.573100, acc.: 68.75%] [G loss: 1.086709]\n",
      "epoch:14 step:13476 [D loss: 0.713134, acc.: 53.91%] [G loss: 0.854540]\n",
      "epoch:14 step:13477 [D loss: 0.614131, acc.: 67.19%] [G loss: 1.367828]\n",
      "epoch:14 step:13478 [D loss: 0.537168, acc.: 71.88%] [G loss: 1.404034]\n",
      "epoch:14 step:13479 [D loss: 0.568723, acc.: 75.78%] [G loss: 1.375179]\n",
      "epoch:14 step:13480 [D loss: 0.625037, acc.: 64.06%] [G loss: 1.037107]\n",
      "epoch:14 step:13481 [D loss: 0.537964, acc.: 77.34%] [G loss: 1.234322]\n",
      "epoch:14 step:13482 [D loss: 0.492513, acc.: 79.69%] [G loss: 1.239876]\n",
      "epoch:14 step:13483 [D loss: 0.605551, acc.: 64.84%] [G loss: 1.220307]\n",
      "epoch:14 step:13484 [D loss: 0.581091, acc.: 69.53%] [G loss: 1.165763]\n",
      "epoch:14 step:13485 [D loss: 0.545165, acc.: 71.88%] [G loss: 1.448617]\n",
      "epoch:14 step:13486 [D loss: 0.504862, acc.: 77.34%] [G loss: 1.292899]\n",
      "epoch:14 step:13487 [D loss: 0.634556, acc.: 64.06%] [G loss: 1.375219]\n",
      "epoch:14 step:13488 [D loss: 0.488322, acc.: 77.34%] [G loss: 1.136264]\n",
      "epoch:14 step:13489 [D loss: 0.537662, acc.: 73.44%] [G loss: 1.175273]\n",
      "epoch:14 step:13490 [D loss: 0.511603, acc.: 78.12%] [G loss: 1.253275]\n",
      "epoch:14 step:13491 [D loss: 0.621018, acc.: 65.62%] [G loss: 1.149546]\n",
      "epoch:14 step:13492 [D loss: 0.572577, acc.: 67.97%] [G loss: 1.450777]\n",
      "epoch:14 step:13493 [D loss: 0.777423, acc.: 50.78%] [G loss: 1.154969]\n",
      "epoch:14 step:13494 [D loss: 0.662655, acc.: 63.28%] [G loss: 1.237871]\n",
      "epoch:14 step:13495 [D loss: 0.465724, acc.: 75.00%] [G loss: 1.258525]\n",
      "epoch:14 step:13496 [D loss: 0.721258, acc.: 60.16%] [G loss: 1.170665]\n",
      "epoch:14 step:13497 [D loss: 0.451735, acc.: 82.81%] [G loss: 1.648580]\n",
      "epoch:14 step:13498 [D loss: 0.514612, acc.: 72.66%] [G loss: 1.110214]\n",
      "epoch:14 step:13499 [D loss: 0.618778, acc.: 64.84%] [G loss: 1.318115]\n",
      "epoch:14 step:13500 [D loss: 0.603704, acc.: 61.72%] [G loss: 1.170878]\n",
      "epoch:14 step:13501 [D loss: 0.497382, acc.: 72.66%] [G loss: 1.373741]\n",
      "epoch:14 step:13502 [D loss: 0.632367, acc.: 59.38%] [G loss: 1.188778]\n",
      "epoch:14 step:13503 [D loss: 0.672816, acc.: 67.19%] [G loss: 1.157408]\n",
      "epoch:14 step:13504 [D loss: 0.688659, acc.: 64.06%] [G loss: 1.189234]\n",
      "epoch:14 step:13505 [D loss: 0.465015, acc.: 80.47%] [G loss: 1.235333]\n",
      "epoch:14 step:13506 [D loss: 0.542285, acc.: 72.66%] [G loss: 1.212513]\n",
      "epoch:14 step:13507 [D loss: 0.711247, acc.: 57.03%] [G loss: 1.052435]\n",
      "epoch:14 step:13508 [D loss: 0.539985, acc.: 74.22%] [G loss: 1.130834]\n",
      "epoch:14 step:13509 [D loss: 0.744258, acc.: 56.25%] [G loss: 1.025192]\n",
      "epoch:14 step:13510 [D loss: 0.506819, acc.: 78.12%] [G loss: 1.094200]\n",
      "epoch:14 step:13511 [D loss: 0.674836, acc.: 61.72%] [G loss: 1.060090]\n",
      "epoch:14 step:13512 [D loss: 0.506072, acc.: 76.56%] [G loss: 1.309653]\n",
      "epoch:14 step:13513 [D loss: 0.488634, acc.: 82.81%] [G loss: 1.084231]\n",
      "epoch:14 step:13514 [D loss: 0.530820, acc.: 76.56%] [G loss: 1.152696]\n",
      "epoch:14 step:13515 [D loss: 0.594546, acc.: 69.53%] [G loss: 1.210702]\n",
      "epoch:14 step:13516 [D loss: 0.644687, acc.: 70.31%] [G loss: 1.084418]\n",
      "epoch:14 step:13517 [D loss: 0.432091, acc.: 86.72%] [G loss: 1.592073]\n",
      "epoch:14 step:13518 [D loss: 0.706170, acc.: 60.16%] [G loss: 1.199748]\n",
      "epoch:14 step:13519 [D loss: 0.456527, acc.: 78.12%] [G loss: 1.512903]\n",
      "epoch:14 step:13520 [D loss: 0.450050, acc.: 78.91%] [G loss: 1.425076]\n",
      "epoch:14 step:13521 [D loss: 0.629400, acc.: 65.62%] [G loss: 1.179114]\n",
      "epoch:14 step:13522 [D loss: 0.570389, acc.: 67.19%] [G loss: 1.210422]\n",
      "epoch:14 step:13523 [D loss: 0.524400, acc.: 71.88%] [G loss: 1.292642]\n",
      "epoch:14 step:13524 [D loss: 0.471798, acc.: 79.69%] [G loss: 1.241987]\n",
      "epoch:14 step:13525 [D loss: 0.636581, acc.: 61.72%] [G loss: 1.178874]\n",
      "epoch:14 step:13526 [D loss: 0.609786, acc.: 67.19%] [G loss: 1.053578]\n",
      "epoch:14 step:13527 [D loss: 0.660056, acc.: 57.81%] [G loss: 0.944421]\n",
      "epoch:14 step:13528 [D loss: 0.586957, acc.: 71.88%] [G loss: 1.151818]\n",
      "epoch:14 step:13529 [D loss: 0.627520, acc.: 60.16%] [G loss: 1.186432]\n",
      "epoch:14 step:13530 [D loss: 0.647012, acc.: 66.41%] [G loss: 1.217994]\n",
      "epoch:14 step:13531 [D loss: 0.618214, acc.: 60.94%] [G loss: 1.577837]\n",
      "epoch:14 step:13532 [D loss: 0.551092, acc.: 68.75%] [G loss: 1.182737]\n",
      "epoch:14 step:13533 [D loss: 0.565162, acc.: 67.19%] [G loss: 1.191372]\n",
      "epoch:14 step:13534 [D loss: 0.655704, acc.: 62.50%] [G loss: 0.956228]\n",
      "epoch:14 step:13535 [D loss: 0.560341, acc.: 74.22%] [G loss: 1.165661]\n",
      "epoch:14 step:13536 [D loss: 0.609652, acc.: 67.19%] [G loss: 1.293527]\n",
      "epoch:14 step:13537 [D loss: 0.587239, acc.: 67.19%] [G loss: 1.311825]\n",
      "epoch:14 step:13538 [D loss: 0.419575, acc.: 85.16%] [G loss: 1.455794]\n",
      "epoch:14 step:13539 [D loss: 0.507498, acc.: 77.34%] [G loss: 1.332914]\n",
      "epoch:14 step:13540 [D loss: 0.521539, acc.: 75.78%] [G loss: 1.477553]\n",
      "epoch:14 step:13541 [D loss: 0.650675, acc.: 63.28%] [G loss: 1.203001]\n",
      "epoch:14 step:13542 [D loss: 0.608276, acc.: 67.97%] [G loss: 1.500435]\n",
      "epoch:14 step:13543 [D loss: 0.644901, acc.: 60.94%] [G loss: 1.221053]\n",
      "epoch:14 step:13544 [D loss: 0.589100, acc.: 73.44%] [G loss: 1.154569]\n",
      "epoch:14 step:13545 [D loss: 0.699539, acc.: 57.03%] [G loss: 1.408884]\n",
      "epoch:14 step:13546 [D loss: 0.759025, acc.: 51.56%] [G loss: 0.995882]\n",
      "epoch:14 step:13547 [D loss: 0.643675, acc.: 62.50%] [G loss: 1.100125]\n",
      "epoch:14 step:13548 [D loss: 0.652216, acc.: 58.59%] [G loss: 1.190750]\n",
      "epoch:14 step:13549 [D loss: 0.639555, acc.: 63.28%] [G loss: 1.351796]\n",
      "epoch:14 step:13550 [D loss: 0.623653, acc.: 67.97%] [G loss: 1.368803]\n",
      "epoch:14 step:13551 [D loss: 0.526313, acc.: 75.00%] [G loss: 1.297922]\n",
      "epoch:14 step:13552 [D loss: 0.687279, acc.: 60.16%] [G loss: 1.257804]\n",
      "epoch:14 step:13553 [D loss: 0.542996, acc.: 77.34%] [G loss: 1.323406]\n",
      "epoch:14 step:13554 [D loss: 0.566183, acc.: 72.66%] [G loss: 1.591250]\n",
      "epoch:14 step:13555 [D loss: 0.585957, acc.: 67.97%] [G loss: 1.331422]\n",
      "epoch:14 step:13556 [D loss: 0.614595, acc.: 64.84%] [G loss: 1.034884]\n",
      "epoch:14 step:13557 [D loss: 0.508022, acc.: 73.44%] [G loss: 1.270001]\n",
      "epoch:14 step:13558 [D loss: 0.565069, acc.: 70.31%] [G loss: 1.156883]\n",
      "epoch:14 step:13559 [D loss: 0.491637, acc.: 75.78%] [G loss: 1.285811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13560 [D loss: 0.718396, acc.: 53.91%] [G loss: 1.447371]\n",
      "epoch:14 step:13561 [D loss: 0.714386, acc.: 56.25%] [G loss: 1.107189]\n",
      "epoch:14 step:13562 [D loss: 0.581114, acc.: 70.31%] [G loss: 1.304987]\n",
      "epoch:14 step:13563 [D loss: 0.558045, acc.: 66.41%] [G loss: 1.350056]\n",
      "epoch:14 step:13564 [D loss: 0.488196, acc.: 76.56%] [G loss: 1.333574]\n",
      "epoch:14 step:13565 [D loss: 0.610171, acc.: 66.41%] [G loss: 1.471278]\n",
      "epoch:14 step:13566 [D loss: 0.659992, acc.: 61.72%] [G loss: 1.321874]\n",
      "epoch:14 step:13567 [D loss: 0.588217, acc.: 67.19%] [G loss: 1.368805]\n",
      "epoch:14 step:13568 [D loss: 0.557186, acc.: 72.66%] [G loss: 1.470485]\n",
      "epoch:14 step:13569 [D loss: 0.520582, acc.: 72.66%] [G loss: 1.410933]\n",
      "epoch:14 step:13570 [D loss: 0.669166, acc.: 65.62%] [G loss: 1.271693]\n",
      "epoch:14 step:13571 [D loss: 0.608506, acc.: 68.75%] [G loss: 1.222759]\n",
      "epoch:14 step:13572 [D loss: 0.523101, acc.: 75.00%] [G loss: 1.621119]\n",
      "epoch:14 step:13573 [D loss: 0.445123, acc.: 83.59%] [G loss: 1.393070]\n",
      "epoch:14 step:13574 [D loss: 0.571751, acc.: 67.19%] [G loss: 1.269438]\n",
      "epoch:14 step:13575 [D loss: 0.520070, acc.: 73.44%] [G loss: 1.066427]\n",
      "epoch:14 step:13576 [D loss: 0.533768, acc.: 73.44%] [G loss: 1.249718]\n",
      "epoch:14 step:13577 [D loss: 0.497317, acc.: 80.47%] [G loss: 1.541349]\n",
      "epoch:14 step:13578 [D loss: 0.609698, acc.: 65.62%] [G loss: 1.267677]\n",
      "epoch:14 step:13579 [D loss: 0.591346, acc.: 74.22%] [G loss: 1.131459]\n",
      "epoch:14 step:13580 [D loss: 0.545958, acc.: 69.53%] [G loss: 1.094890]\n",
      "epoch:14 step:13581 [D loss: 0.626235, acc.: 64.84%] [G loss: 1.015397]\n",
      "epoch:14 step:13582 [D loss: 0.609054, acc.: 62.50%] [G loss: 1.208644]\n",
      "epoch:14 step:13583 [D loss: 0.542795, acc.: 73.44%] [G loss: 1.240513]\n",
      "epoch:14 step:13584 [D loss: 0.663731, acc.: 63.28%] [G loss: 1.040719]\n",
      "epoch:14 step:13585 [D loss: 0.594661, acc.: 60.16%] [G loss: 1.458053]\n",
      "epoch:14 step:13586 [D loss: 0.496438, acc.: 74.22%] [G loss: 1.332820]\n",
      "epoch:14 step:13587 [D loss: 0.572124, acc.: 70.31%] [G loss: 1.255518]\n",
      "epoch:14 step:13588 [D loss: 0.680346, acc.: 58.59%] [G loss: 0.905930]\n",
      "epoch:14 step:13589 [D loss: 0.657280, acc.: 63.28%] [G loss: 0.967131]\n",
      "epoch:14 step:13590 [D loss: 0.623562, acc.: 67.97%] [G loss: 1.446781]\n",
      "epoch:14 step:13591 [D loss: 0.523084, acc.: 75.78%] [G loss: 1.393222]\n",
      "epoch:14 step:13592 [D loss: 0.410920, acc.: 85.16%] [G loss: 1.481568]\n",
      "epoch:14 step:13593 [D loss: 0.483923, acc.: 78.91%] [G loss: 1.391897]\n",
      "epoch:14 step:13594 [D loss: 0.409295, acc.: 82.03%] [G loss: 1.474010]\n",
      "epoch:14 step:13595 [D loss: 0.587080, acc.: 66.41%] [G loss: 1.094960]\n",
      "epoch:14 step:13596 [D loss: 0.616377, acc.: 69.53%] [G loss: 1.210155]\n",
      "epoch:14 step:13597 [D loss: 0.582225, acc.: 72.66%] [G loss: 1.138449]\n",
      "epoch:14 step:13598 [D loss: 0.525662, acc.: 73.44%] [G loss: 1.520383]\n",
      "epoch:14 step:13599 [D loss: 0.611374, acc.: 68.75%] [G loss: 1.233425]\n",
      "epoch:14 step:13600 [D loss: 0.596709, acc.: 65.62%] [G loss: 1.158113]\n",
      "##############\n",
      "[2.55800081 1.95575323 1.92340287 2.81128549 0.72394226 5.54717966\n",
      " 2.05982451 2.5582461  3.96764727 7.14868929]\n",
      "##########\n",
      "epoch:14 step:13601 [D loss: 0.612291, acc.: 65.62%] [G loss: 1.237282]\n",
      "epoch:14 step:13602 [D loss: 0.391188, acc.: 88.28%] [G loss: 1.355289]\n",
      "epoch:14 step:13603 [D loss: 0.699610, acc.: 60.16%] [G loss: 1.079149]\n",
      "epoch:14 step:13604 [D loss: 0.480803, acc.: 78.12%] [G loss: 1.459970]\n",
      "epoch:14 step:13605 [D loss: 0.559296, acc.: 71.88%] [G loss: 1.550437]\n",
      "epoch:14 step:13606 [D loss: 0.534629, acc.: 74.22%] [G loss: 1.631107]\n",
      "epoch:14 step:13607 [D loss: 0.502107, acc.: 79.69%] [G loss: 1.430281]\n",
      "epoch:14 step:13608 [D loss: 0.607314, acc.: 64.84%] [G loss: 1.336879]\n",
      "epoch:14 step:13609 [D loss: 0.474659, acc.: 78.12%] [G loss: 1.259825]\n",
      "epoch:14 step:13610 [D loss: 0.466304, acc.: 78.12%] [G loss: 1.114558]\n",
      "epoch:14 step:13611 [D loss: 0.694248, acc.: 60.16%] [G loss: 1.186031]\n",
      "epoch:14 step:13612 [D loss: 0.561409, acc.: 72.66%] [G loss: 1.127515]\n",
      "epoch:14 step:13613 [D loss: 0.507455, acc.: 75.00%] [G loss: 1.334076]\n",
      "epoch:14 step:13614 [D loss: 0.491416, acc.: 75.00%] [G loss: 1.713108]\n",
      "epoch:14 step:13615 [D loss: 0.750502, acc.: 51.56%] [G loss: 1.267509]\n",
      "epoch:14 step:13616 [D loss: 0.517323, acc.: 76.56%] [G loss: 1.491869]\n",
      "epoch:14 step:13617 [D loss: 0.542253, acc.: 72.66%] [G loss: 1.489083]\n",
      "epoch:14 step:13618 [D loss: 0.767927, acc.: 54.69%] [G loss: 1.144308]\n",
      "epoch:14 step:13619 [D loss: 0.547583, acc.: 71.09%] [G loss: 1.550564]\n",
      "epoch:14 step:13620 [D loss: 0.559813, acc.: 68.75%] [G loss: 1.411010]\n",
      "epoch:14 step:13621 [D loss: 0.603101, acc.: 68.75%] [G loss: 1.036500]\n",
      "epoch:14 step:13622 [D loss: 0.759078, acc.: 53.91%] [G loss: 1.101005]\n",
      "epoch:14 step:13623 [D loss: 0.507781, acc.: 77.34%] [G loss: 1.449997]\n",
      "epoch:14 step:13624 [D loss: 0.501636, acc.: 75.78%] [G loss: 1.438481]\n",
      "epoch:14 step:13625 [D loss: 0.513256, acc.: 71.09%] [G loss: 1.358043]\n",
      "epoch:14 step:13626 [D loss: 0.562290, acc.: 71.88%] [G loss: 1.214256]\n",
      "epoch:14 step:13627 [D loss: 0.559209, acc.: 72.66%] [G loss: 1.272539]\n",
      "epoch:14 step:13628 [D loss: 0.535706, acc.: 75.00%] [G loss: 1.196588]\n",
      "epoch:14 step:13629 [D loss: 0.629573, acc.: 65.62%] [G loss: 0.974051]\n",
      "epoch:14 step:13630 [D loss: 0.481342, acc.: 78.91%] [G loss: 1.403184]\n",
      "epoch:14 step:13631 [D loss: 0.484199, acc.: 77.34%] [G loss: 1.460332]\n",
      "epoch:14 step:13632 [D loss: 0.531710, acc.: 69.53%] [G loss: 1.123349]\n",
      "epoch:14 step:13633 [D loss: 0.577634, acc.: 69.53%] [G loss: 1.379309]\n",
      "epoch:14 step:13634 [D loss: 0.677245, acc.: 57.81%] [G loss: 1.185141]\n",
      "epoch:14 step:13635 [D loss: 0.618613, acc.: 64.06%] [G loss: 1.217218]\n",
      "epoch:14 step:13636 [D loss: 0.545110, acc.: 75.78%] [G loss: 1.345609]\n",
      "epoch:14 step:13637 [D loss: 0.632429, acc.: 66.41%] [G loss: 1.580125]\n",
      "epoch:14 step:13638 [D loss: 0.523689, acc.: 78.91%] [G loss: 1.295159]\n",
      "epoch:14 step:13639 [D loss: 0.633878, acc.: 63.28%] [G loss: 1.379448]\n",
      "epoch:14 step:13640 [D loss: 0.648067, acc.: 62.50%] [G loss: 1.016000]\n",
      "epoch:14 step:13641 [D loss: 0.494842, acc.: 75.78%] [G loss: 1.182586]\n",
      "epoch:14 step:13642 [D loss: 0.452207, acc.: 81.25%] [G loss: 1.449803]\n",
      "epoch:14 step:13643 [D loss: 0.588776, acc.: 71.88%] [G loss: 1.328204]\n",
      "epoch:14 step:13644 [D loss: 0.588439, acc.: 71.88%] [G loss: 1.081909]\n",
      "epoch:14 step:13645 [D loss: 0.803346, acc.: 46.88%] [G loss: 1.094862]\n",
      "epoch:14 step:13646 [D loss: 0.602356, acc.: 67.97%] [G loss: 1.113060]\n",
      "epoch:14 step:13647 [D loss: 0.606207, acc.: 65.62%] [G loss: 1.071324]\n",
      "epoch:14 step:13648 [D loss: 0.481135, acc.: 78.91%] [G loss: 1.269478]\n",
      "epoch:14 step:13649 [D loss: 0.526977, acc.: 71.88%] [G loss: 1.154241]\n",
      "epoch:14 step:13650 [D loss: 0.667651, acc.: 63.28%] [G loss: 1.459446]\n",
      "epoch:14 step:13651 [D loss: 0.669223, acc.: 59.38%] [G loss: 1.049385]\n",
      "epoch:14 step:13652 [D loss: 0.671993, acc.: 64.84%] [G loss: 1.058098]\n",
      "epoch:14 step:13653 [D loss: 0.591378, acc.: 67.19%] [G loss: 1.256757]\n",
      "epoch:14 step:13654 [D loss: 0.661381, acc.: 60.16%] [G loss: 1.141693]\n",
      "epoch:14 step:13655 [D loss: 0.646520, acc.: 60.16%] [G loss: 1.124889]\n",
      "epoch:14 step:13656 [D loss: 0.610569, acc.: 64.84%] [G loss: 0.927831]\n",
      "epoch:14 step:13657 [D loss: 0.553314, acc.: 71.09%] [G loss: 1.323195]\n",
      "epoch:14 step:13658 [D loss: 0.547530, acc.: 68.75%] [G loss: 1.494673]\n",
      "epoch:14 step:13659 [D loss: 0.529114, acc.: 73.44%] [G loss: 1.355142]\n",
      "epoch:14 step:13660 [D loss: 0.647312, acc.: 64.84%] [G loss: 1.071760]\n",
      "epoch:14 step:13661 [D loss: 0.623232, acc.: 62.50%] [G loss: 1.457056]\n",
      "epoch:14 step:13662 [D loss: 0.584981, acc.: 69.53%] [G loss: 1.106176]\n",
      "epoch:14 step:13663 [D loss: 0.567798, acc.: 68.75%] [G loss: 1.534388]\n",
      "epoch:14 step:13664 [D loss: 0.559126, acc.: 74.22%] [G loss: 1.245044]\n",
      "epoch:14 step:13665 [D loss: 0.592632, acc.: 67.19%] [G loss: 1.187497]\n",
      "epoch:14 step:13666 [D loss: 0.619982, acc.: 64.06%] [G loss: 1.125137]\n",
      "epoch:14 step:13667 [D loss: 0.554641, acc.: 77.34%] [G loss: 1.305757]\n",
      "epoch:14 step:13668 [D loss: 0.486333, acc.: 72.66%] [G loss: 1.278622]\n",
      "epoch:14 step:13669 [D loss: 0.568550, acc.: 67.97%] [G loss: 1.428909]\n",
      "epoch:14 step:13670 [D loss: 0.593135, acc.: 71.88%] [G loss: 1.059211]\n",
      "epoch:14 step:13671 [D loss: 0.702801, acc.: 62.50%] [G loss: 1.135541]\n",
      "epoch:14 step:13672 [D loss: 0.615427, acc.: 66.41%] [G loss: 1.058409]\n",
      "epoch:14 step:13673 [D loss: 0.618109, acc.: 67.19%] [G loss: 1.308380]\n",
      "epoch:14 step:13674 [D loss: 0.708459, acc.: 59.38%] [G loss: 1.066456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13675 [D loss: 0.614138, acc.: 65.62%] [G loss: 1.131296]\n",
      "epoch:14 step:13676 [D loss: 0.581824, acc.: 71.88%] [G loss: 1.145517]\n",
      "epoch:14 step:13677 [D loss: 0.632851, acc.: 64.84%] [G loss: 1.159395]\n",
      "epoch:14 step:13678 [D loss: 0.460028, acc.: 81.25%] [G loss: 1.462998]\n",
      "epoch:14 step:13679 [D loss: 0.519381, acc.: 77.34%] [G loss: 1.263793]\n",
      "epoch:14 step:13680 [D loss: 0.687684, acc.: 61.72%] [G loss: 1.095305]\n",
      "epoch:14 step:13681 [D loss: 0.618908, acc.: 68.75%] [G loss: 1.345301]\n",
      "epoch:14 step:13682 [D loss: 0.588309, acc.: 72.66%] [G loss: 1.204452]\n",
      "epoch:14 step:13683 [D loss: 0.544106, acc.: 73.44%] [G loss: 1.541397]\n",
      "epoch:14 step:13684 [D loss: 0.639878, acc.: 63.28%] [G loss: 1.285443]\n",
      "epoch:14 step:13685 [D loss: 0.670467, acc.: 62.50%] [G loss: 1.234355]\n",
      "epoch:14 step:13686 [D loss: 0.497109, acc.: 78.12%] [G loss: 0.975355]\n",
      "epoch:14 step:13687 [D loss: 0.537420, acc.: 77.34%] [G loss: 1.396062]\n",
      "epoch:14 step:13688 [D loss: 0.471270, acc.: 77.34%] [G loss: 1.422195]\n",
      "epoch:14 step:13689 [D loss: 0.541396, acc.: 71.88%] [G loss: 1.331453]\n",
      "epoch:14 step:13690 [D loss: 0.461778, acc.: 79.69%] [G loss: 1.462717]\n",
      "epoch:14 step:13691 [D loss: 0.651830, acc.: 60.94%] [G loss: 1.340392]\n",
      "epoch:14 step:13692 [D loss: 0.463809, acc.: 79.69%] [G loss: 1.436920]\n",
      "epoch:14 step:13693 [D loss: 0.643459, acc.: 64.06%] [G loss: 1.136322]\n",
      "epoch:14 step:13694 [D loss: 0.499272, acc.: 77.34%] [G loss: 1.182461]\n",
      "epoch:14 step:13695 [D loss: 0.599578, acc.: 71.09%] [G loss: 1.287349]\n",
      "epoch:14 step:13696 [D loss: 0.616803, acc.: 64.84%] [G loss: 1.356886]\n",
      "epoch:14 step:13697 [D loss: 0.564169, acc.: 73.44%] [G loss: 1.497492]\n",
      "epoch:14 step:13698 [D loss: 0.718842, acc.: 56.25%] [G loss: 1.095906]\n",
      "epoch:14 step:13699 [D loss: 0.492129, acc.: 75.00%] [G loss: 1.181817]\n",
      "epoch:14 step:13700 [D loss: 0.544813, acc.: 75.78%] [G loss: 1.270488]\n",
      "epoch:14 step:13701 [D loss: 0.538565, acc.: 72.66%] [G loss: 1.381986]\n",
      "epoch:14 step:13702 [D loss: 0.741241, acc.: 57.81%] [G loss: 1.068128]\n",
      "epoch:14 step:13703 [D loss: 0.641680, acc.: 60.94%] [G loss: 1.053266]\n",
      "epoch:14 step:13704 [D loss: 0.633129, acc.: 63.28%] [G loss: 1.282950]\n",
      "epoch:14 step:13705 [D loss: 0.547254, acc.: 75.00%] [G loss: 1.354713]\n",
      "epoch:14 step:13706 [D loss: 0.543005, acc.: 72.66%] [G loss: 1.154674]\n",
      "epoch:14 step:13707 [D loss: 0.628065, acc.: 64.06%] [G loss: 1.111178]\n",
      "epoch:14 step:13708 [D loss: 0.534514, acc.: 75.00%] [G loss: 1.291191]\n",
      "epoch:14 step:13709 [D loss: 0.481340, acc.: 75.00%] [G loss: 1.330927]\n",
      "epoch:14 step:13710 [D loss: 0.683545, acc.: 57.81%] [G loss: 0.966486]\n",
      "epoch:14 step:13711 [D loss: 0.639470, acc.: 65.62%] [G loss: 1.000074]\n",
      "epoch:14 step:13712 [D loss: 0.434778, acc.: 82.03%] [G loss: 1.262514]\n",
      "epoch:14 step:13713 [D loss: 0.599431, acc.: 68.75%] [G loss: 1.510418]\n",
      "epoch:14 step:13714 [D loss: 0.525106, acc.: 75.78%] [G loss: 1.394197]\n",
      "epoch:14 step:13715 [D loss: 0.488811, acc.: 75.78%] [G loss: 1.339839]\n",
      "epoch:14 step:13716 [D loss: 0.532791, acc.: 80.47%] [G loss: 1.127655]\n",
      "epoch:14 step:13717 [D loss: 0.516799, acc.: 73.44%] [G loss: 1.172033]\n",
      "epoch:14 step:13718 [D loss: 0.610919, acc.: 66.41%] [G loss: 1.218555]\n",
      "epoch:14 step:13719 [D loss: 0.719019, acc.: 53.12%] [G loss: 1.297791]\n",
      "epoch:14 step:13720 [D loss: 0.404129, acc.: 88.28%] [G loss: 1.465962]\n",
      "epoch:14 step:13721 [D loss: 0.609959, acc.: 64.06%] [G loss: 1.210413]\n",
      "epoch:14 step:13722 [D loss: 0.557939, acc.: 70.31%] [G loss: 1.082350]\n",
      "epoch:14 step:13723 [D loss: 0.800548, acc.: 48.44%] [G loss: 0.816736]\n",
      "epoch:14 step:13724 [D loss: 0.648104, acc.: 57.81%] [G loss: 1.067453]\n",
      "epoch:14 step:13725 [D loss: 0.470706, acc.: 82.03%] [G loss: 1.212899]\n",
      "epoch:14 step:13726 [D loss: 0.738775, acc.: 53.12%] [G loss: 1.343250]\n",
      "epoch:14 step:13727 [D loss: 0.548542, acc.: 72.66%] [G loss: 1.389010]\n",
      "epoch:14 step:13728 [D loss: 0.541908, acc.: 68.75%] [G loss: 1.294730]\n",
      "epoch:14 step:13729 [D loss: 0.490177, acc.: 80.47%] [G loss: 1.339000]\n",
      "epoch:14 step:13730 [D loss: 0.672893, acc.: 57.81%] [G loss: 1.371039]\n",
      "epoch:14 step:13731 [D loss: 0.568385, acc.: 71.09%] [G loss: 1.428562]\n",
      "epoch:14 step:13732 [D loss: 0.485166, acc.: 81.25%] [G loss: 1.273384]\n",
      "epoch:14 step:13733 [D loss: 0.502955, acc.: 75.00%] [G loss: 1.273699]\n",
      "epoch:14 step:13734 [D loss: 0.588627, acc.: 70.31%] [G loss: 1.356015]\n",
      "epoch:14 step:13735 [D loss: 0.642652, acc.: 62.50%] [G loss: 1.267771]\n",
      "epoch:14 step:13736 [D loss: 0.656694, acc.: 60.16%] [G loss: 1.538847]\n",
      "epoch:14 step:13737 [D loss: 0.708413, acc.: 58.59%] [G loss: 1.284716]\n",
      "epoch:14 step:13738 [D loss: 0.682047, acc.: 60.94%] [G loss: 0.978406]\n",
      "epoch:14 step:13739 [D loss: 0.635705, acc.: 60.94%] [G loss: 1.081457]\n",
      "epoch:14 step:13740 [D loss: 0.665333, acc.: 63.28%] [G loss: 1.117881]\n",
      "epoch:14 step:13741 [D loss: 0.540019, acc.: 75.00%] [G loss: 1.039288]\n",
      "epoch:14 step:13742 [D loss: 0.550996, acc.: 70.31%] [G loss: 1.347061]\n",
      "epoch:14 step:13743 [D loss: 0.592893, acc.: 67.97%] [G loss: 0.966968]\n",
      "epoch:14 step:13744 [D loss: 0.594634, acc.: 69.53%] [G loss: 1.196825]\n",
      "epoch:14 step:13745 [D loss: 0.385831, acc.: 89.84%] [G loss: 1.394477]\n",
      "epoch:14 step:13746 [D loss: 0.654598, acc.: 62.50%] [G loss: 1.110786]\n",
      "epoch:14 step:13747 [D loss: 0.683110, acc.: 60.16%] [G loss: 1.274051]\n",
      "epoch:14 step:13748 [D loss: 0.571452, acc.: 71.09%] [G loss: 1.302144]\n",
      "epoch:14 step:13749 [D loss: 0.638133, acc.: 65.62%] [G loss: 1.341859]\n",
      "epoch:14 step:13750 [D loss: 0.546228, acc.: 71.09%] [G loss: 1.418107]\n",
      "epoch:14 step:13751 [D loss: 0.484517, acc.: 81.25%] [G loss: 1.448232]\n",
      "epoch:14 step:13752 [D loss: 0.692829, acc.: 55.47%] [G loss: 1.290543]\n",
      "epoch:14 step:13753 [D loss: 0.574074, acc.: 70.31%] [G loss: 1.208791]\n",
      "epoch:14 step:13754 [D loss: 0.516248, acc.: 72.66%] [G loss: 1.041279]\n",
      "epoch:14 step:13755 [D loss: 0.727486, acc.: 55.47%] [G loss: 1.202901]\n",
      "epoch:14 step:13756 [D loss: 0.408197, acc.: 85.16%] [G loss: 1.165247]\n",
      "epoch:14 step:13757 [D loss: 0.526296, acc.: 76.56%] [G loss: 1.272823]\n",
      "epoch:14 step:13758 [D loss: 0.803529, acc.: 50.00%] [G loss: 0.982599]\n",
      "epoch:14 step:13759 [D loss: 0.690487, acc.: 55.47%] [G loss: 1.159484]\n",
      "epoch:14 step:13760 [D loss: 0.568821, acc.: 72.66%] [G loss: 1.567680]\n",
      "epoch:14 step:13761 [D loss: 0.619469, acc.: 63.28%] [G loss: 1.288123]\n",
      "epoch:14 step:13762 [D loss: 0.524254, acc.: 72.66%] [G loss: 1.248933]\n",
      "epoch:14 step:13763 [D loss: 0.502749, acc.: 76.56%] [G loss: 1.107528]\n",
      "epoch:14 step:13764 [D loss: 0.623781, acc.: 66.41%] [G loss: 1.490703]\n",
      "epoch:14 step:13765 [D loss: 0.688198, acc.: 61.72%] [G loss: 1.058576]\n",
      "epoch:14 step:13766 [D loss: 0.418027, acc.: 81.25%] [G loss: 1.380507]\n",
      "epoch:14 step:13767 [D loss: 0.526986, acc.: 71.09%] [G loss: 1.355550]\n",
      "epoch:14 step:13768 [D loss: 0.653480, acc.: 62.50%] [G loss: 1.511051]\n",
      "epoch:14 step:13769 [D loss: 0.541654, acc.: 70.31%] [G loss: 1.505689]\n",
      "epoch:14 step:13770 [D loss: 0.634683, acc.: 60.94%] [G loss: 1.319716]\n",
      "epoch:14 step:13771 [D loss: 0.487077, acc.: 76.56%] [G loss: 1.224457]\n",
      "epoch:14 step:13772 [D loss: 0.605400, acc.: 64.06%] [G loss: 1.278008]\n",
      "epoch:14 step:13773 [D loss: 0.549064, acc.: 70.31%] [G loss: 1.294292]\n",
      "epoch:14 step:13774 [D loss: 0.397719, acc.: 84.38%] [G loss: 1.270316]\n",
      "epoch:14 step:13775 [D loss: 0.721860, acc.: 51.56%] [G loss: 1.158885]\n",
      "epoch:14 step:13776 [D loss: 0.708543, acc.: 57.81%] [G loss: 1.075600]\n",
      "epoch:14 step:13777 [D loss: 0.621457, acc.: 64.06%] [G loss: 1.110127]\n",
      "epoch:14 step:13778 [D loss: 0.528646, acc.: 71.88%] [G loss: 1.261865]\n",
      "epoch:14 step:13779 [D loss: 0.490667, acc.: 77.34%] [G loss: 1.467731]\n",
      "epoch:14 step:13780 [D loss: 0.588442, acc.: 69.53%] [G loss: 1.170148]\n",
      "epoch:14 step:13781 [D loss: 0.569236, acc.: 69.53%] [G loss: 1.048961]\n",
      "epoch:14 step:13782 [D loss: 0.537357, acc.: 72.66%] [G loss: 1.332025]\n",
      "epoch:14 step:13783 [D loss: 0.618443, acc.: 64.06%] [G loss: 1.169465]\n",
      "epoch:14 step:13784 [D loss: 0.382332, acc.: 92.19%] [G loss: 1.197825]\n",
      "epoch:14 step:13785 [D loss: 0.586442, acc.: 69.53%] [G loss: 1.125503]\n",
      "epoch:14 step:13786 [D loss: 0.703868, acc.: 59.38%] [G loss: 0.911318]\n",
      "epoch:14 step:13787 [D loss: 0.586061, acc.: 69.53%] [G loss: 1.363943]\n",
      "epoch:14 step:13788 [D loss: 0.683841, acc.: 60.94%] [G loss: 1.010365]\n",
      "epoch:14 step:13789 [D loss: 0.574777, acc.: 67.97%] [G loss: 1.628226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13790 [D loss: 0.493499, acc.: 75.78%] [G loss: 1.341237]\n",
      "epoch:14 step:13791 [D loss: 0.621936, acc.: 64.06%] [G loss: 1.411845]\n",
      "epoch:14 step:13792 [D loss: 0.602812, acc.: 68.75%] [G loss: 1.048631]\n",
      "epoch:14 step:13793 [D loss: 0.563378, acc.: 75.78%] [G loss: 1.403494]\n",
      "epoch:14 step:13794 [D loss: 0.537201, acc.: 76.56%] [G loss: 1.379306]\n",
      "epoch:14 step:13795 [D loss: 0.488878, acc.: 76.56%] [G loss: 1.507066]\n",
      "epoch:14 step:13796 [D loss: 0.533127, acc.: 74.22%] [G loss: 1.291982]\n",
      "epoch:14 step:13797 [D loss: 0.569578, acc.: 68.75%] [G loss: 1.321171]\n",
      "epoch:14 step:13798 [D loss: 0.557496, acc.: 71.09%] [G loss: 1.297724]\n",
      "epoch:14 step:13799 [D loss: 0.637699, acc.: 63.28%] [G loss: 1.345258]\n",
      "epoch:14 step:13800 [D loss: 0.540484, acc.: 70.31%] [G loss: 1.485842]\n",
      "##############\n",
      "[2.64969594 2.19511679 1.88351553 2.80993776 1.11646706 5.95953184\n",
      " 2.38642195 2.68072849 3.90991497 7.14868929]\n",
      "##########\n",
      "epoch:14 step:13801 [D loss: 0.651673, acc.: 64.84%] [G loss: 1.355046]\n",
      "epoch:14 step:13802 [D loss: 0.631643, acc.: 65.62%] [G loss: 1.358863]\n",
      "epoch:14 step:13803 [D loss: 0.584922, acc.: 67.97%] [G loss: 1.314437]\n",
      "epoch:14 step:13804 [D loss: 0.544164, acc.: 75.00%] [G loss: 1.088414]\n",
      "epoch:14 step:13805 [D loss: 0.620298, acc.: 66.41%] [G loss: 1.364043]\n",
      "epoch:14 step:13806 [D loss: 0.524696, acc.: 76.56%] [G loss: 1.208014]\n",
      "epoch:14 step:13807 [D loss: 0.635365, acc.: 61.72%] [G loss: 1.097228]\n",
      "epoch:14 step:13808 [D loss: 0.552900, acc.: 73.44%] [G loss: 0.947030]\n",
      "epoch:14 step:13809 [D loss: 0.511926, acc.: 73.44%] [G loss: 1.114371]\n",
      "epoch:14 step:13810 [D loss: 0.543654, acc.: 75.00%] [G loss: 1.180350]\n",
      "epoch:14 step:13811 [D loss: 0.627025, acc.: 64.06%] [G loss: 0.992739]\n",
      "epoch:14 step:13812 [D loss: 0.623958, acc.: 65.62%] [G loss: 1.046028]\n",
      "epoch:14 step:13813 [D loss: 0.492315, acc.: 77.34%] [G loss: 1.614670]\n",
      "epoch:14 step:13814 [D loss: 0.560438, acc.: 75.78%] [G loss: 0.850218]\n",
      "epoch:14 step:13815 [D loss: 0.650105, acc.: 67.19%] [G loss: 1.337220]\n",
      "epoch:14 step:13816 [D loss: 0.595445, acc.: 66.41%] [G loss: 1.133922]\n",
      "epoch:14 step:13817 [D loss: 0.581315, acc.: 68.75%] [G loss: 1.134133]\n",
      "epoch:14 step:13818 [D loss: 0.518135, acc.: 74.22%] [G loss: 1.159378]\n",
      "epoch:14 step:13819 [D loss: 0.430486, acc.: 82.81%] [G loss: 1.474920]\n",
      "epoch:14 step:13820 [D loss: 0.463315, acc.: 83.59%] [G loss: 1.217416]\n",
      "epoch:14 step:13821 [D loss: 0.640244, acc.: 64.06%] [G loss: 1.258971]\n",
      "epoch:14 step:13822 [D loss: 0.734576, acc.: 58.59%] [G loss: 1.215507]\n",
      "epoch:14 step:13823 [D loss: 0.597220, acc.: 68.75%] [G loss: 1.312381]\n",
      "epoch:14 step:13824 [D loss: 0.638656, acc.: 64.84%] [G loss: 1.533995]\n",
      "epoch:14 step:13825 [D loss: 0.509868, acc.: 79.69%] [G loss: 1.117613]\n",
      "epoch:14 step:13826 [D loss: 0.581551, acc.: 68.75%] [G loss: 1.089059]\n",
      "epoch:14 step:13827 [D loss: 0.798013, acc.: 46.88%] [G loss: 1.019164]\n",
      "epoch:14 step:13828 [D loss: 0.763365, acc.: 56.25%] [G loss: 1.079799]\n",
      "epoch:14 step:13829 [D loss: 0.463290, acc.: 78.91%] [G loss: 1.531795]\n",
      "epoch:14 step:13830 [D loss: 0.500716, acc.: 82.03%] [G loss: 1.378420]\n",
      "epoch:14 step:13831 [D loss: 0.587838, acc.: 70.31%] [G loss: 1.002823]\n",
      "epoch:14 step:13832 [D loss: 0.719599, acc.: 50.78%] [G loss: 1.115308]\n",
      "epoch:14 step:13833 [D loss: 0.615332, acc.: 63.28%] [G loss: 1.295944]\n",
      "epoch:14 step:13834 [D loss: 0.574261, acc.: 76.56%] [G loss: 1.255908]\n",
      "epoch:14 step:13835 [D loss: 0.617991, acc.: 61.72%] [G loss: 1.102748]\n",
      "epoch:14 step:13836 [D loss: 0.474569, acc.: 80.47%] [G loss: 1.604333]\n",
      "epoch:14 step:13837 [D loss: 0.671982, acc.: 59.38%] [G loss: 1.403586]\n",
      "epoch:14 step:13838 [D loss: 0.594270, acc.: 69.53%] [G loss: 1.445628]\n",
      "epoch:14 step:13839 [D loss: 0.557145, acc.: 74.22%] [G loss: 0.986718]\n",
      "epoch:14 step:13840 [D loss: 0.713475, acc.: 56.25%] [G loss: 0.993678]\n",
      "epoch:14 step:13841 [D loss: 0.665614, acc.: 64.06%] [G loss: 1.301409]\n",
      "epoch:14 step:13842 [D loss: 0.609084, acc.: 64.84%] [G loss: 1.350700]\n",
      "epoch:14 step:13843 [D loss: 0.586021, acc.: 70.31%] [G loss: 1.110149]\n",
      "epoch:14 step:13844 [D loss: 0.645735, acc.: 64.84%] [G loss: 1.419164]\n",
      "epoch:14 step:13845 [D loss: 0.532431, acc.: 74.22%] [G loss: 1.175267]\n",
      "epoch:14 step:13846 [D loss: 0.613471, acc.: 64.06%] [G loss: 1.259735]\n",
      "epoch:14 step:13847 [D loss: 0.635631, acc.: 61.72%] [G loss: 1.073731]\n",
      "epoch:14 step:13848 [D loss: 0.547629, acc.: 70.31%] [G loss: 1.523103]\n",
      "epoch:14 step:13849 [D loss: 0.494681, acc.: 81.25%] [G loss: 1.228233]\n",
      "epoch:14 step:13850 [D loss: 0.597709, acc.: 66.41%] [G loss: 1.121895]\n",
      "epoch:14 step:13851 [D loss: 0.481723, acc.: 78.91%] [G loss: 1.526499]\n",
      "epoch:14 step:13852 [D loss: 0.546013, acc.: 73.44%] [G loss: 1.363409]\n",
      "epoch:14 step:13853 [D loss: 0.605579, acc.: 68.75%] [G loss: 1.224627]\n",
      "epoch:14 step:13854 [D loss: 0.487273, acc.: 80.47%] [G loss: 1.481176]\n",
      "epoch:14 step:13855 [D loss: 0.605247, acc.: 63.28%] [G loss: 1.088101]\n",
      "epoch:14 step:13856 [D loss: 0.718585, acc.: 53.12%] [G loss: 0.954679]\n",
      "epoch:14 step:13857 [D loss: 0.575402, acc.: 71.09%] [G loss: 1.234988]\n",
      "epoch:14 step:13858 [D loss: 0.552531, acc.: 75.78%] [G loss: 1.265578]\n",
      "epoch:14 step:13859 [D loss: 0.563718, acc.: 67.97%] [G loss: 1.171839]\n",
      "epoch:14 step:13860 [D loss: 0.543316, acc.: 74.22%] [G loss: 1.385570]\n",
      "epoch:14 step:13861 [D loss: 0.642793, acc.: 60.16%] [G loss: 1.260050]\n",
      "epoch:14 step:13862 [D loss: 0.553472, acc.: 72.66%] [G loss: 1.236148]\n",
      "epoch:14 step:13863 [D loss: 0.691934, acc.: 53.91%] [G loss: 1.323774]\n",
      "epoch:14 step:13864 [D loss: 0.605762, acc.: 67.97%] [G loss: 1.073538]\n",
      "epoch:14 step:13865 [D loss: 0.484469, acc.: 83.59%] [G loss: 1.316631]\n",
      "epoch:14 step:13866 [D loss: 0.512336, acc.: 77.34%] [G loss: 1.248428]\n",
      "epoch:14 step:13867 [D loss: 0.639552, acc.: 65.62%] [G loss: 1.224351]\n",
      "epoch:14 step:13868 [D loss: 0.609542, acc.: 71.09%] [G loss: 1.101570]\n",
      "epoch:14 step:13869 [D loss: 0.436604, acc.: 85.94%] [G loss: 1.240303]\n",
      "epoch:14 step:13870 [D loss: 0.721376, acc.: 57.03%] [G loss: 1.093220]\n",
      "epoch:14 step:13871 [D loss: 0.604650, acc.: 67.97%] [G loss: 1.426518]\n",
      "epoch:14 step:13872 [D loss: 0.512132, acc.: 79.69%] [G loss: 1.227833]\n",
      "epoch:14 step:13873 [D loss: 0.537956, acc.: 71.88%] [G loss: 1.004891]\n",
      "epoch:14 step:13874 [D loss: 0.616017, acc.: 67.19%] [G loss: 1.335124]\n",
      "epoch:14 step:13875 [D loss: 0.502146, acc.: 77.34%] [G loss: 1.342413]\n",
      "epoch:14 step:13876 [D loss: 0.615612, acc.: 71.09%] [G loss: 1.379143]\n",
      "epoch:14 step:13877 [D loss: 0.502958, acc.: 78.12%] [G loss: 1.224350]\n",
      "epoch:14 step:13878 [D loss: 0.508696, acc.: 77.34%] [G loss: 1.150459]\n",
      "epoch:14 step:13879 [D loss: 0.652107, acc.: 66.41%] [G loss: 1.041376]\n",
      "epoch:14 step:13880 [D loss: 0.569198, acc.: 68.75%] [G loss: 1.441012]\n",
      "epoch:14 step:13881 [D loss: 0.613656, acc.: 66.41%] [G loss: 1.102398]\n",
      "epoch:14 step:13882 [D loss: 0.561906, acc.: 69.53%] [G loss: 1.413931]\n",
      "epoch:14 step:13883 [D loss: 0.564823, acc.: 67.97%] [G loss: 1.197134]\n",
      "epoch:14 step:13884 [D loss: 0.598541, acc.: 64.84%] [G loss: 1.306353]\n",
      "epoch:14 step:13885 [D loss: 0.631932, acc.: 65.62%] [G loss: 1.092970]\n",
      "epoch:14 step:13886 [D loss: 0.621805, acc.: 67.19%] [G loss: 1.478851]\n",
      "epoch:14 step:13887 [D loss: 0.465571, acc.: 77.34%] [G loss: 1.114164]\n",
      "epoch:14 step:13888 [D loss: 0.606422, acc.: 66.41%] [G loss: 1.188002]\n",
      "epoch:14 step:13889 [D loss: 0.543007, acc.: 75.78%] [G loss: 1.153890]\n",
      "epoch:14 step:13890 [D loss: 0.523887, acc.: 75.78%] [G loss: 1.417674]\n",
      "epoch:14 step:13891 [D loss: 0.482399, acc.: 81.25%] [G loss: 1.372600]\n",
      "epoch:14 step:13892 [D loss: 0.532179, acc.: 73.44%] [G loss: 1.280702]\n",
      "epoch:14 step:13893 [D loss: 0.547904, acc.: 69.53%] [G loss: 1.283164]\n",
      "epoch:14 step:13894 [D loss: 0.522309, acc.: 75.00%] [G loss: 1.135466]\n",
      "epoch:14 step:13895 [D loss: 0.681303, acc.: 59.38%] [G loss: 1.092447]\n",
      "epoch:14 step:13896 [D loss: 0.569356, acc.: 69.53%] [G loss: 1.041014]\n",
      "epoch:14 step:13897 [D loss: 0.633443, acc.: 65.62%] [G loss: 1.346778]\n",
      "epoch:14 step:13898 [D loss: 0.609289, acc.: 65.62%] [G loss: 1.163415]\n",
      "epoch:14 step:13899 [D loss: 0.721516, acc.: 57.03%] [G loss: 1.272409]\n",
      "epoch:14 step:13900 [D loss: 0.429648, acc.: 82.81%] [G loss: 1.589687]\n",
      "epoch:14 step:13901 [D loss: 0.514633, acc.: 77.34%] [G loss: 1.172737]\n",
      "epoch:14 step:13902 [D loss: 0.663821, acc.: 60.16%] [G loss: 1.149753]\n",
      "epoch:14 step:13903 [D loss: 0.756348, acc.: 48.44%] [G loss: 1.302523]\n",
      "epoch:14 step:13904 [D loss: 0.573951, acc.: 67.97%] [G loss: 1.286604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13905 [D loss: 0.530248, acc.: 74.22%] [G loss: 1.323685]\n",
      "epoch:14 step:13906 [D loss: 0.538686, acc.: 79.69%] [G loss: 1.192032]\n",
      "epoch:14 step:13907 [D loss: 0.571562, acc.: 69.53%] [G loss: 1.337549]\n",
      "epoch:14 step:13908 [D loss: 0.560126, acc.: 69.53%] [G loss: 1.205336]\n",
      "epoch:14 step:13909 [D loss: 0.629584, acc.: 69.53%] [G loss: 1.266447]\n",
      "epoch:14 step:13910 [D loss: 0.506201, acc.: 77.34%] [G loss: 1.333453]\n",
      "epoch:14 step:13911 [D loss: 0.532655, acc.: 72.66%] [G loss: 1.143327]\n",
      "epoch:14 step:13912 [D loss: 0.558287, acc.: 74.22%] [G loss: 1.583178]\n",
      "epoch:14 step:13913 [D loss: 0.572665, acc.: 69.53%] [G loss: 1.328189]\n",
      "epoch:14 step:13914 [D loss: 0.533679, acc.: 75.00%] [G loss: 1.165677]\n",
      "epoch:14 step:13915 [D loss: 0.789957, acc.: 48.44%] [G loss: 1.074012]\n",
      "epoch:14 step:13916 [D loss: 0.677408, acc.: 57.81%] [G loss: 1.318011]\n",
      "epoch:14 step:13917 [D loss: 0.632085, acc.: 67.19%] [G loss: 1.241733]\n",
      "epoch:14 step:13918 [D loss: 0.557762, acc.: 71.88%] [G loss: 1.184760]\n",
      "epoch:14 step:13919 [D loss: 0.614377, acc.: 64.06%] [G loss: 1.087364]\n",
      "epoch:14 step:13920 [D loss: 0.581920, acc.: 70.31%] [G loss: 1.175232]\n",
      "epoch:14 step:13921 [D loss: 0.531636, acc.: 72.66%] [G loss: 1.149103]\n",
      "epoch:14 step:13922 [D loss: 0.721871, acc.: 53.12%] [G loss: 1.461028]\n",
      "epoch:14 step:13923 [D loss: 0.562569, acc.: 69.53%] [G loss: 1.544192]\n",
      "epoch:14 step:13924 [D loss: 0.498980, acc.: 76.56%] [G loss: 1.093034]\n",
      "epoch:14 step:13925 [D loss: 0.533473, acc.: 73.44%] [G loss: 1.323727]\n",
      "epoch:14 step:13926 [D loss: 0.563581, acc.: 71.88%] [G loss: 1.026408]\n",
      "epoch:14 step:13927 [D loss: 0.712714, acc.: 59.38%] [G loss: 1.197110]\n",
      "epoch:14 step:13928 [D loss: 0.588008, acc.: 69.53%] [G loss: 1.065282]\n",
      "epoch:14 step:13929 [D loss: 0.566445, acc.: 74.22%] [G loss: 1.370022]\n",
      "epoch:14 step:13930 [D loss: 0.602501, acc.: 61.72%] [G loss: 1.293843]\n",
      "epoch:14 step:13931 [D loss: 0.525441, acc.: 71.88%] [G loss: 1.097733]\n",
      "epoch:14 step:13932 [D loss: 0.608748, acc.: 69.53%] [G loss: 1.207034]\n",
      "epoch:14 step:13933 [D loss: 0.557896, acc.: 70.31%] [G loss: 1.229755]\n",
      "epoch:14 step:13934 [D loss: 0.656330, acc.: 64.06%] [G loss: 1.376461]\n",
      "epoch:14 step:13935 [D loss: 0.596451, acc.: 67.19%] [G loss: 1.261246]\n",
      "epoch:14 step:13936 [D loss: 0.555702, acc.: 72.66%] [G loss: 1.566425]\n",
      "epoch:14 step:13937 [D loss: 0.656666, acc.: 60.94%] [G loss: 1.252839]\n",
      "epoch:14 step:13938 [D loss: 0.578036, acc.: 69.53%] [G loss: 1.354865]\n",
      "epoch:14 step:13939 [D loss: 0.675950, acc.: 58.59%] [G loss: 1.228355]\n",
      "epoch:14 step:13940 [D loss: 0.630030, acc.: 65.62%] [G loss: 1.244096]\n",
      "epoch:14 step:13941 [D loss: 0.593076, acc.: 71.88%] [G loss: 1.285595]\n",
      "epoch:14 step:13942 [D loss: 0.636326, acc.: 60.16%] [G loss: 1.123544]\n",
      "epoch:14 step:13943 [D loss: 0.642032, acc.: 58.59%] [G loss: 1.422669]\n",
      "epoch:14 step:13944 [D loss: 0.497097, acc.: 76.56%] [G loss: 1.414975]\n",
      "epoch:14 step:13945 [D loss: 0.684969, acc.: 62.50%] [G loss: 1.264123]\n",
      "epoch:14 step:13946 [D loss: 0.616334, acc.: 67.19%] [G loss: 1.426173]\n",
      "epoch:14 step:13947 [D loss: 0.614060, acc.: 67.97%] [G loss: 1.100702]\n",
      "epoch:14 step:13948 [D loss: 0.548163, acc.: 70.31%] [G loss: 1.025483]\n",
      "epoch:14 step:13949 [D loss: 0.742011, acc.: 55.47%] [G loss: 1.258070]\n",
      "epoch:14 step:13950 [D loss: 0.777693, acc.: 47.66%] [G loss: 1.029592]\n",
      "epoch:14 step:13951 [D loss: 0.776652, acc.: 54.69%] [G loss: 1.145576]\n",
      "epoch:14 step:13952 [D loss: 0.519107, acc.: 74.22%] [G loss: 1.086411]\n",
      "epoch:14 step:13953 [D loss: 0.650585, acc.: 60.94%] [G loss: 0.996289]\n",
      "epoch:14 step:13954 [D loss: 0.588675, acc.: 70.31%] [G loss: 1.188323]\n",
      "epoch:14 step:13955 [D loss: 0.683011, acc.: 58.59%] [G loss: 1.178293]\n",
      "epoch:14 step:13956 [D loss: 0.579258, acc.: 70.31%] [G loss: 1.200762]\n",
      "epoch:14 step:13957 [D loss: 0.692426, acc.: 57.03%] [G loss: 1.162441]\n",
      "epoch:14 step:13958 [D loss: 0.445525, acc.: 80.47%] [G loss: 1.395447]\n",
      "epoch:14 step:13959 [D loss: 0.614191, acc.: 64.84%] [G loss: 1.237241]\n",
      "epoch:14 step:13960 [D loss: 0.569325, acc.: 68.75%] [G loss: 1.160218]\n",
      "epoch:14 step:13961 [D loss: 0.568491, acc.: 67.97%] [G loss: 1.478195]\n",
      "epoch:14 step:13962 [D loss: 0.657994, acc.: 63.28%] [G loss: 1.061566]\n",
      "epoch:14 step:13963 [D loss: 0.566906, acc.: 70.31%] [G loss: 1.174888]\n",
      "epoch:14 step:13964 [D loss: 0.587065, acc.: 64.84%] [G loss: 1.431357]\n",
      "epoch:14 step:13965 [D loss: 0.559794, acc.: 67.97%] [G loss: 1.312617]\n",
      "epoch:14 step:13966 [D loss: 0.699536, acc.: 55.47%] [G loss: 1.047144]\n",
      "epoch:14 step:13967 [D loss: 0.503693, acc.: 75.00%] [G loss: 1.350052]\n",
      "epoch:14 step:13968 [D loss: 0.545372, acc.: 72.66%] [G loss: 1.432964]\n",
      "epoch:14 step:13969 [D loss: 0.661483, acc.: 63.28%] [G loss: 1.070322]\n",
      "epoch:14 step:13970 [D loss: 0.518293, acc.: 73.44%] [G loss: 1.421578]\n",
      "epoch:14 step:13971 [D loss: 0.541906, acc.: 73.44%] [G loss: 1.303395]\n",
      "epoch:14 step:13972 [D loss: 0.611186, acc.: 64.06%] [G loss: 1.222249]\n",
      "epoch:14 step:13973 [D loss: 0.614014, acc.: 63.28%] [G loss: 1.146224]\n",
      "epoch:14 step:13974 [D loss: 0.610992, acc.: 71.09%] [G loss: 0.994405]\n",
      "epoch:14 step:13975 [D loss: 0.546372, acc.: 75.00%] [G loss: 1.443260]\n",
      "epoch:14 step:13976 [D loss: 0.524683, acc.: 76.56%] [G loss: 1.339072]\n",
      "epoch:14 step:13977 [D loss: 0.582685, acc.: 68.75%] [G loss: 1.134561]\n",
      "epoch:14 step:13978 [D loss: 0.507636, acc.: 73.44%] [G loss: 1.304364]\n",
      "epoch:14 step:13979 [D loss: 0.554777, acc.: 75.00%] [G loss: 1.175649]\n",
      "epoch:14 step:13980 [D loss: 0.553813, acc.: 71.88%] [G loss: 1.356185]\n",
      "epoch:14 step:13981 [D loss: 0.596154, acc.: 70.31%] [G loss: 1.148428]\n",
      "epoch:14 step:13982 [D loss: 0.714249, acc.: 58.59%] [G loss: 1.161093]\n",
      "epoch:14 step:13983 [D loss: 0.565682, acc.: 77.34%] [G loss: 1.204040]\n",
      "epoch:14 step:13984 [D loss: 0.728688, acc.: 56.25%] [G loss: 1.075183]\n",
      "epoch:14 step:13985 [D loss: 0.592047, acc.: 70.31%] [G loss: 1.184926]\n",
      "epoch:14 step:13986 [D loss: 0.501910, acc.: 76.56%] [G loss: 1.291777]\n",
      "epoch:14 step:13987 [D loss: 0.533238, acc.: 77.34%] [G loss: 1.158172]\n",
      "epoch:14 step:13988 [D loss: 0.647849, acc.: 63.28%] [G loss: 1.477442]\n",
      "epoch:14 step:13989 [D loss: 0.511735, acc.: 77.34%] [G loss: 1.021110]\n",
      "epoch:14 step:13990 [D loss: 0.493963, acc.: 76.56%] [G loss: 1.264593]\n",
      "epoch:14 step:13991 [D loss: 0.443812, acc.: 77.34%] [G loss: 1.308707]\n",
      "epoch:14 step:13992 [D loss: 0.590224, acc.: 70.31%] [G loss: 1.460585]\n",
      "epoch:14 step:13993 [D loss: 0.552645, acc.: 73.44%] [G loss: 1.187735]\n",
      "epoch:14 step:13994 [D loss: 0.565063, acc.: 70.31%] [G loss: 1.181559]\n",
      "epoch:14 step:13995 [D loss: 0.692004, acc.: 56.25%] [G loss: 0.957092]\n",
      "epoch:14 step:13996 [D loss: 0.531320, acc.: 73.44%] [G loss: 1.106482]\n",
      "epoch:14 step:13997 [D loss: 0.661267, acc.: 61.72%] [G loss: 1.239965]\n",
      "epoch:14 step:13998 [D loss: 0.572798, acc.: 72.66%] [G loss: 1.476388]\n",
      "epoch:14 step:13999 [D loss: 0.680456, acc.: 64.06%] [G loss: 1.073494]\n",
      "epoch:14 step:14000 [D loss: 0.560258, acc.: 73.44%] [G loss: 1.247922]\n",
      "##############\n",
      "[2.73622731 1.93640984 1.94326559 2.96275008 1.05540605 6.31552757\n",
      " 2.33908189 2.86771089 4.06047384 5.88922717]\n",
      "##########\n",
      "epoch:14 step:14001 [D loss: 0.518203, acc.: 77.34%] [G loss: 1.261211]\n",
      "epoch:14 step:14002 [D loss: 0.458941, acc.: 77.34%] [G loss: 1.437831]\n",
      "epoch:14 step:14003 [D loss: 0.529094, acc.: 74.22%] [G loss: 1.371976]\n",
      "epoch:14 step:14004 [D loss: 0.560709, acc.: 70.31%] [G loss: 1.008701]\n",
      "epoch:14 step:14005 [D loss: 0.587413, acc.: 70.31%] [G loss: 1.315351]\n",
      "epoch:14 step:14006 [D loss: 0.569655, acc.: 68.75%] [G loss: 1.509209]\n",
      "epoch:14 step:14007 [D loss: 0.550504, acc.: 71.88%] [G loss: 1.450087]\n",
      "epoch:14 step:14008 [D loss: 0.568197, acc.: 71.09%] [G loss: 1.304498]\n",
      "epoch:14 step:14009 [D loss: 0.573634, acc.: 70.31%] [G loss: 1.283623]\n",
      "epoch:14 step:14010 [D loss: 0.552255, acc.: 67.19%] [G loss: 1.260831]\n",
      "epoch:14 step:14011 [D loss: 0.578435, acc.: 70.31%] [G loss: 1.166572]\n",
      "epoch:14 step:14012 [D loss: 0.585431, acc.: 67.19%] [G loss: 1.367769]\n",
      "epoch:14 step:14013 [D loss: 0.498264, acc.: 75.00%] [G loss: 1.419808]\n",
      "epoch:14 step:14014 [D loss: 0.575773, acc.: 65.62%] [G loss: 1.177178]\n",
      "epoch:14 step:14015 [D loss: 0.532884, acc.: 75.00%] [G loss: 1.220106]\n",
      "epoch:14 step:14016 [D loss: 0.528515, acc.: 76.56%] [G loss: 1.180610]\n",
      "epoch:14 step:14017 [D loss: 0.713257, acc.: 57.03%] [G loss: 1.060978]\n",
      "epoch:14 step:14018 [D loss: 0.511191, acc.: 76.56%] [G loss: 1.415014]\n",
      "epoch:14 step:14019 [D loss: 0.584072, acc.: 64.84%] [G loss: 1.628073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:14020 [D loss: 0.598049, acc.: 70.31%] [G loss: 1.310480]\n",
      "epoch:14 step:14021 [D loss: 0.496653, acc.: 77.34%] [G loss: 1.432075]\n",
      "epoch:14 step:14022 [D loss: 0.412849, acc.: 85.16%] [G loss: 1.461021]\n",
      "epoch:14 step:14023 [D loss: 0.497659, acc.: 77.34%] [G loss: 1.329567]\n",
      "epoch:14 step:14024 [D loss: 0.452323, acc.: 82.03%] [G loss: 1.388955]\n",
      "epoch:14 step:14025 [D loss: 0.517549, acc.: 72.66%] [G loss: 1.253937]\n",
      "epoch:14 step:14026 [D loss: 0.709043, acc.: 60.94%] [G loss: 1.024735]\n",
      "epoch:14 step:14027 [D loss: 0.661706, acc.: 60.94%] [G loss: 1.266227]\n",
      "epoch:14 step:14028 [D loss: 0.551402, acc.: 71.09%] [G loss: 1.353383]\n",
      "epoch:14 step:14029 [D loss: 0.581785, acc.: 66.41%] [G loss: 1.416464]\n",
      "epoch:14 step:14030 [D loss: 0.589055, acc.: 63.28%] [G loss: 1.374145]\n",
      "epoch:14 step:14031 [D loss: 0.603414, acc.: 67.19%] [G loss: 1.074586]\n",
      "epoch:14 step:14032 [D loss: 0.649524, acc.: 62.50%] [G loss: 1.093095]\n",
      "epoch:14 step:14033 [D loss: 0.664167, acc.: 60.94%] [G loss: 1.179502]\n",
      "epoch:14 step:14034 [D loss: 0.651318, acc.: 61.72%] [G loss: 1.015602]\n",
      "epoch:14 step:14035 [D loss: 0.609255, acc.: 67.19%] [G loss: 1.275523]\n",
      "epoch:14 step:14036 [D loss: 0.509973, acc.: 79.69%] [G loss: 1.286461]\n",
      "epoch:14 step:14037 [D loss: 0.662486, acc.: 60.16%] [G loss: 1.238501]\n",
      "epoch:14 step:14038 [D loss: 0.685203, acc.: 59.38%] [G loss: 1.126575]\n",
      "epoch:14 step:14039 [D loss: 0.616022, acc.: 62.50%] [G loss: 1.299042]\n",
      "epoch:14 step:14040 [D loss: 0.518858, acc.: 75.00%] [G loss: 1.084223]\n",
      "epoch:14 step:14041 [D loss: 0.522553, acc.: 78.12%] [G loss: 1.316408]\n",
      "epoch:14 step:14042 [D loss: 0.548987, acc.: 72.66%] [G loss: 1.441802]\n",
      "epoch:14 step:14043 [D loss: 0.592693, acc.: 67.97%] [G loss: 0.980471]\n",
      "epoch:14 step:14044 [D loss: 0.513047, acc.: 76.56%] [G loss: 1.291927]\n",
      "epoch:14 step:14045 [D loss: 0.734065, acc.: 57.03%] [G loss: 1.309899]\n",
      "epoch:14 step:14046 [D loss: 0.460418, acc.: 77.34%] [G loss: 1.117612]\n",
      "epoch:14 step:14047 [D loss: 0.594168, acc.: 64.84%] [G loss: 1.241568]\n",
      "epoch:14 step:14048 [D loss: 0.583370, acc.: 74.22%] [G loss: 1.384859]\n",
      "epoch:14 step:14049 [D loss: 0.560205, acc.: 71.88%] [G loss: 1.108856]\n",
      "epoch:14 step:14050 [D loss: 0.636318, acc.: 63.28%] [G loss: 1.280868]\n",
      "epoch:14 step:14051 [D loss: 0.665970, acc.: 60.16%] [G loss: 1.369083]\n",
      "epoch:14 step:14052 [D loss: 0.546975, acc.: 70.31%] [G loss: 1.211748]\n",
      "epoch:14 step:14053 [D loss: 0.642393, acc.: 64.84%] [G loss: 1.364925]\n",
      "epoch:14 step:14054 [D loss: 0.639331, acc.: 61.72%] [G loss: 1.292437]\n",
      "epoch:14 step:14055 [D loss: 0.569896, acc.: 68.75%] [G loss: 1.324037]\n",
      "epoch:15 step:14056 [D loss: 0.689434, acc.: 53.91%] [G loss: 1.421417]\n",
      "epoch:15 step:14057 [D loss: 0.788453, acc.: 55.47%] [G loss: 1.077566]\n",
      "epoch:15 step:14058 [D loss: 0.714679, acc.: 53.91%] [G loss: 1.317083]\n",
      "epoch:15 step:14059 [D loss: 0.469671, acc.: 81.25%] [G loss: 1.228798]\n",
      "epoch:15 step:14060 [D loss: 0.551387, acc.: 75.00%] [G loss: 1.111034]\n",
      "epoch:15 step:14061 [D loss: 0.659391, acc.: 60.94%] [G loss: 1.103619]\n",
      "epoch:15 step:14062 [D loss: 0.609738, acc.: 71.09%] [G loss: 0.815105]\n",
      "epoch:15 step:14063 [D loss: 0.476969, acc.: 81.25%] [G loss: 1.278513]\n",
      "epoch:15 step:14064 [D loss: 0.603933, acc.: 69.53%] [G loss: 1.477489]\n",
      "epoch:15 step:14065 [D loss: 0.711660, acc.: 55.47%] [G loss: 1.309442]\n",
      "epoch:15 step:14066 [D loss: 0.601523, acc.: 69.53%] [G loss: 1.136443]\n",
      "epoch:15 step:14067 [D loss: 0.488589, acc.: 75.78%] [G loss: 1.464977]\n",
      "epoch:15 step:14068 [D loss: 0.696792, acc.: 54.69%] [G loss: 0.999125]\n",
      "epoch:15 step:14069 [D loss: 0.660392, acc.: 59.38%] [G loss: 1.476249]\n",
      "epoch:15 step:14070 [D loss: 0.477722, acc.: 73.44%] [G loss: 1.671076]\n",
      "epoch:15 step:14071 [D loss: 0.489897, acc.: 81.25%] [G loss: 1.285705]\n",
      "epoch:15 step:14072 [D loss: 0.610682, acc.: 65.62%] [G loss: 1.101777]\n",
      "epoch:15 step:14073 [D loss: 0.607084, acc.: 64.84%] [G loss: 1.135337]\n",
      "epoch:15 step:14074 [D loss: 0.636501, acc.: 62.50%] [G loss: 1.179500]\n",
      "epoch:15 step:14075 [D loss: 0.670466, acc.: 64.84%] [G loss: 0.984979]\n",
      "epoch:15 step:14076 [D loss: 0.656935, acc.: 61.72%] [G loss: 1.196526]\n",
      "epoch:15 step:14077 [D loss: 0.509918, acc.: 74.22%] [G loss: 1.180910]\n",
      "epoch:15 step:14078 [D loss: 0.661682, acc.: 63.28%] [G loss: 1.167353]\n",
      "epoch:15 step:14079 [D loss: 0.463593, acc.: 81.25%] [G loss: 1.499931]\n",
      "epoch:15 step:14080 [D loss: 0.590856, acc.: 70.31%] [G loss: 1.383116]\n",
      "epoch:15 step:14081 [D loss: 0.592026, acc.: 65.62%] [G loss: 1.261508]\n",
      "epoch:15 step:14082 [D loss: 0.529456, acc.: 75.00%] [G loss: 1.293476]\n",
      "epoch:15 step:14083 [D loss: 0.458362, acc.: 82.03%] [G loss: 1.525495]\n",
      "epoch:15 step:14084 [D loss: 0.528531, acc.: 75.78%] [G loss: 1.178030]\n",
      "epoch:15 step:14085 [D loss: 0.512124, acc.: 78.91%] [G loss: 1.523174]\n",
      "epoch:15 step:14086 [D loss: 0.604799, acc.: 63.28%] [G loss: 1.158679]\n",
      "epoch:15 step:14087 [D loss: 0.523112, acc.: 73.44%] [G loss: 1.203283]\n",
      "epoch:15 step:14088 [D loss: 0.500151, acc.: 75.78%] [G loss: 1.258946]\n",
      "epoch:15 step:14089 [D loss: 0.631761, acc.: 61.72%] [G loss: 1.173043]\n",
      "epoch:15 step:14090 [D loss: 0.662953, acc.: 61.72%] [G loss: 1.422410]\n",
      "epoch:15 step:14091 [D loss: 0.539351, acc.: 75.00%] [G loss: 1.007410]\n",
      "epoch:15 step:14092 [D loss: 0.585736, acc.: 72.66%] [G loss: 1.221747]\n",
      "epoch:15 step:14093 [D loss: 0.682759, acc.: 60.94%] [G loss: 1.210091]\n",
      "epoch:15 step:14094 [D loss: 0.677036, acc.: 66.41%] [G loss: 0.950578]\n",
      "epoch:15 step:14095 [D loss: 0.649268, acc.: 62.50%] [G loss: 1.193739]\n",
      "epoch:15 step:14096 [D loss: 0.640077, acc.: 63.28%] [G loss: 1.442370]\n",
      "epoch:15 step:14097 [D loss: 0.621283, acc.: 63.28%] [G loss: 1.331020]\n",
      "epoch:15 step:14098 [D loss: 0.604254, acc.: 71.09%] [G loss: 1.406613]\n",
      "epoch:15 step:14099 [D loss: 0.510265, acc.: 74.22%] [G loss: 1.065534]\n",
      "epoch:15 step:14100 [D loss: 0.561756, acc.: 71.09%] [G loss: 1.099058]\n",
      "epoch:15 step:14101 [D loss: 0.621154, acc.: 66.41%] [G loss: 1.053470]\n",
      "epoch:15 step:14102 [D loss: 0.565161, acc.: 72.66%] [G loss: 1.410373]\n",
      "epoch:15 step:14103 [D loss: 0.654433, acc.: 58.59%] [G loss: 0.952482]\n",
      "epoch:15 step:14104 [D loss: 0.521967, acc.: 76.56%] [G loss: 1.374799]\n",
      "epoch:15 step:14105 [D loss: 0.477278, acc.: 84.38%] [G loss: 1.416358]\n",
      "epoch:15 step:14106 [D loss: 0.546749, acc.: 72.66%] [G loss: 1.263772]\n",
      "epoch:15 step:14107 [D loss: 0.555580, acc.: 71.09%] [G loss: 1.490002]\n",
      "epoch:15 step:14108 [D loss: 0.604120, acc.: 64.06%] [G loss: 1.195790]\n",
      "epoch:15 step:14109 [D loss: 0.441745, acc.: 81.25%] [G loss: 1.462786]\n",
      "epoch:15 step:14110 [D loss: 0.686935, acc.: 58.59%] [G loss: 1.453733]\n",
      "epoch:15 step:14111 [D loss: 0.607612, acc.: 67.19%] [G loss: 1.235999]\n",
      "epoch:15 step:14112 [D loss: 0.549808, acc.: 75.00%] [G loss: 1.237189]\n",
      "epoch:15 step:14113 [D loss: 0.436107, acc.: 85.94%] [G loss: 1.424143]\n",
      "epoch:15 step:14114 [D loss: 0.442487, acc.: 84.38%] [G loss: 1.255612]\n",
      "epoch:15 step:14115 [D loss: 0.652093, acc.: 60.16%] [G loss: 1.174086]\n",
      "epoch:15 step:14116 [D loss: 0.627159, acc.: 64.84%] [G loss: 1.491276]\n",
      "epoch:15 step:14117 [D loss: 0.601910, acc.: 67.19%] [G loss: 1.350616]\n",
      "epoch:15 step:14118 [D loss: 0.533768, acc.: 75.00%] [G loss: 1.408248]\n",
      "epoch:15 step:14119 [D loss: 0.604730, acc.: 68.75%] [G loss: 1.179611]\n",
      "epoch:15 step:14120 [D loss: 0.530961, acc.: 74.22%] [G loss: 1.292315]\n",
      "epoch:15 step:14121 [D loss: 0.627769, acc.: 63.28%] [G loss: 1.414659]\n",
      "epoch:15 step:14122 [D loss: 0.597114, acc.: 65.62%] [G loss: 1.233541]\n",
      "epoch:15 step:14123 [D loss: 0.543389, acc.: 71.88%] [G loss: 1.276222]\n",
      "epoch:15 step:14124 [D loss: 0.637597, acc.: 64.84%] [G loss: 1.382616]\n",
      "epoch:15 step:14125 [D loss: 0.536654, acc.: 72.66%] [G loss: 1.521981]\n",
      "epoch:15 step:14126 [D loss: 0.642473, acc.: 65.62%] [G loss: 1.195648]\n",
      "epoch:15 step:14127 [D loss: 0.634848, acc.: 64.06%] [G loss: 1.333063]\n",
      "epoch:15 step:14128 [D loss: 0.559346, acc.: 69.53%] [G loss: 1.179590]\n",
      "epoch:15 step:14129 [D loss: 0.497828, acc.: 74.22%] [G loss: 1.203530]\n",
      "epoch:15 step:14130 [D loss: 0.640335, acc.: 64.06%] [G loss: 1.324878]\n",
      "epoch:15 step:14131 [D loss: 0.590308, acc.: 65.62%] [G loss: 1.504347]\n",
      "epoch:15 step:14132 [D loss: 0.618208, acc.: 68.75%] [G loss: 1.685413]\n",
      "epoch:15 step:14133 [D loss: 0.625155, acc.: 63.28%] [G loss: 1.112256]\n",
      "epoch:15 step:14134 [D loss: 0.558547, acc.: 72.66%] [G loss: 1.465335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14135 [D loss: 0.549013, acc.: 70.31%] [G loss: 1.380540]\n",
      "epoch:15 step:14136 [D loss: 0.578199, acc.: 71.88%] [G loss: 1.342783]\n",
      "epoch:15 step:14137 [D loss: 0.581928, acc.: 71.09%] [G loss: 1.267658]\n",
      "epoch:15 step:14138 [D loss: 0.573278, acc.: 65.62%] [G loss: 1.222504]\n",
      "epoch:15 step:14139 [D loss: 0.716563, acc.: 53.12%] [G loss: 1.240342]\n",
      "epoch:15 step:14140 [D loss: 0.627474, acc.: 64.84%] [G loss: 1.269347]\n",
      "epoch:15 step:14141 [D loss: 0.562532, acc.: 73.44%] [G loss: 1.334251]\n",
      "epoch:15 step:14142 [D loss: 0.500559, acc.: 72.66%] [G loss: 1.439517]\n",
      "epoch:15 step:14143 [D loss: 0.525981, acc.: 71.88%] [G loss: 1.333367]\n",
      "epoch:15 step:14144 [D loss: 0.526523, acc.: 75.00%] [G loss: 1.229138]\n",
      "epoch:15 step:14145 [D loss: 0.707365, acc.: 53.91%] [G loss: 1.208974]\n",
      "epoch:15 step:14146 [D loss: 0.467343, acc.: 79.69%] [G loss: 1.517724]\n",
      "epoch:15 step:14147 [D loss: 0.513245, acc.: 72.66%] [G loss: 1.308908]\n",
      "epoch:15 step:14148 [D loss: 0.491301, acc.: 75.78%] [G loss: 1.432582]\n",
      "epoch:15 step:14149 [D loss: 0.535769, acc.: 71.09%] [G loss: 1.325705]\n",
      "epoch:15 step:14150 [D loss: 0.551017, acc.: 69.53%] [G loss: 1.409833]\n",
      "epoch:15 step:14151 [D loss: 0.640398, acc.: 64.84%] [G loss: 1.073076]\n",
      "epoch:15 step:14152 [D loss: 0.642263, acc.: 64.06%] [G loss: 1.276460]\n",
      "epoch:15 step:14153 [D loss: 0.528512, acc.: 75.78%] [G loss: 1.313207]\n",
      "epoch:15 step:14154 [D loss: 0.555195, acc.: 73.44%] [G loss: 1.120364]\n",
      "epoch:15 step:14155 [D loss: 0.513949, acc.: 76.56%] [G loss: 1.467500]\n",
      "epoch:15 step:14156 [D loss: 0.528534, acc.: 72.66%] [G loss: 1.135335]\n",
      "epoch:15 step:14157 [D loss: 0.695029, acc.: 56.25%] [G loss: 1.195102]\n",
      "epoch:15 step:14158 [D loss: 0.577995, acc.: 72.66%] [G loss: 1.309389]\n",
      "epoch:15 step:14159 [D loss: 0.538836, acc.: 71.88%] [G loss: 1.288427]\n",
      "epoch:15 step:14160 [D loss: 0.516519, acc.: 78.91%] [G loss: 1.394597]\n",
      "epoch:15 step:14161 [D loss: 0.611927, acc.: 65.62%] [G loss: 1.375819]\n",
      "epoch:15 step:14162 [D loss: 0.645043, acc.: 63.28%] [G loss: 1.267730]\n",
      "epoch:15 step:14163 [D loss: 0.522358, acc.: 74.22%] [G loss: 1.324246]\n",
      "epoch:15 step:14164 [D loss: 0.434071, acc.: 85.16%] [G loss: 1.085473]\n",
      "epoch:15 step:14165 [D loss: 0.688378, acc.: 65.62%] [G loss: 1.316167]\n",
      "epoch:15 step:14166 [D loss: 0.734255, acc.: 51.56%] [G loss: 1.179439]\n",
      "epoch:15 step:14167 [D loss: 0.484745, acc.: 76.56%] [G loss: 1.132319]\n",
      "epoch:15 step:14168 [D loss: 0.496581, acc.: 75.00%] [G loss: 1.297369]\n",
      "epoch:15 step:14169 [D loss: 0.567606, acc.: 69.53%] [G loss: 1.309404]\n",
      "epoch:15 step:14170 [D loss: 0.703220, acc.: 59.38%] [G loss: 1.035376]\n",
      "epoch:15 step:14171 [D loss: 0.614443, acc.: 66.41%] [G loss: 1.248089]\n",
      "epoch:15 step:14172 [D loss: 0.620628, acc.: 63.28%] [G loss: 1.491784]\n",
      "epoch:15 step:14173 [D loss: 0.690258, acc.: 56.25%] [G loss: 1.283095]\n",
      "epoch:15 step:14174 [D loss: 0.548252, acc.: 67.97%] [G loss: 1.332157]\n",
      "epoch:15 step:14175 [D loss: 0.642263, acc.: 57.81%] [G loss: 1.261152]\n",
      "epoch:15 step:14176 [D loss: 0.536457, acc.: 70.31%] [G loss: 1.367342]\n",
      "epoch:15 step:14177 [D loss: 0.517971, acc.: 75.78%] [G loss: 1.287582]\n",
      "epoch:15 step:14178 [D loss: 0.490950, acc.: 79.69%] [G loss: 1.204872]\n",
      "epoch:15 step:14179 [D loss: 0.767119, acc.: 50.00%] [G loss: 0.963529]\n",
      "epoch:15 step:14180 [D loss: 0.546189, acc.: 72.66%] [G loss: 1.274689]\n",
      "epoch:15 step:14181 [D loss: 0.624053, acc.: 69.53%] [G loss: 1.206920]\n",
      "epoch:15 step:14182 [D loss: 0.635158, acc.: 62.50%] [G loss: 1.112807]\n",
      "epoch:15 step:14183 [D loss: 0.579370, acc.: 66.41%] [G loss: 1.543658]\n",
      "epoch:15 step:14184 [D loss: 0.733455, acc.: 50.00%] [G loss: 1.022714]\n",
      "epoch:15 step:14185 [D loss: 0.533153, acc.: 73.44%] [G loss: 1.245321]\n",
      "epoch:15 step:14186 [D loss: 0.608763, acc.: 66.41%] [G loss: 1.261248]\n",
      "epoch:15 step:14187 [D loss: 0.606427, acc.: 69.53%] [G loss: 1.341461]\n",
      "epoch:15 step:14188 [D loss: 0.649348, acc.: 65.62%] [G loss: 1.218342]\n",
      "epoch:15 step:14189 [D loss: 0.568849, acc.: 67.97%] [G loss: 1.553296]\n",
      "epoch:15 step:14190 [D loss: 0.461669, acc.: 81.25%] [G loss: 1.203859]\n",
      "epoch:15 step:14191 [D loss: 0.572240, acc.: 67.97%] [G loss: 1.062422]\n",
      "epoch:15 step:14192 [D loss: 0.622662, acc.: 64.06%] [G loss: 1.268321]\n",
      "epoch:15 step:14193 [D loss: 0.550818, acc.: 68.75%] [G loss: 1.283135]\n",
      "epoch:15 step:14194 [D loss: 0.574383, acc.: 67.97%] [G loss: 1.538506]\n",
      "epoch:15 step:14195 [D loss: 0.538589, acc.: 73.44%] [G loss: 1.249990]\n",
      "epoch:15 step:14196 [D loss: 0.580885, acc.: 69.53%] [G loss: 1.302774]\n",
      "epoch:15 step:14197 [D loss: 0.470949, acc.: 82.81%] [G loss: 1.177140]\n",
      "epoch:15 step:14198 [D loss: 0.637592, acc.: 67.97%] [G loss: 1.042727]\n",
      "epoch:15 step:14199 [D loss: 0.574290, acc.: 67.97%] [G loss: 1.088675]\n",
      "epoch:15 step:14200 [D loss: 0.549222, acc.: 70.31%] [G loss: 1.093849]\n",
      "##############\n",
      "[2.69796269 2.1855048  1.82682987 2.53162031 1.04298896 6.09801689\n",
      " 2.09965193 2.28225611 3.86639919 5.98472967]\n",
      "##########\n",
      "epoch:15 step:14201 [D loss: 0.559255, acc.: 71.88%] [G loss: 1.606914]\n",
      "epoch:15 step:14202 [D loss: 0.512583, acc.: 79.69%] [G loss: 1.137338]\n",
      "epoch:15 step:14203 [D loss: 0.569820, acc.: 71.09%] [G loss: 1.302384]\n",
      "epoch:15 step:14204 [D loss: 0.593185, acc.: 68.75%] [G loss: 1.102084]\n",
      "epoch:15 step:14205 [D loss: 0.646731, acc.: 60.94%] [G loss: 0.783049]\n",
      "epoch:15 step:14206 [D loss: 0.608741, acc.: 67.97%] [G loss: 1.206800]\n",
      "epoch:15 step:14207 [D loss: 0.591814, acc.: 67.19%] [G loss: 1.222094]\n",
      "epoch:15 step:14208 [D loss: 0.720904, acc.: 61.72%] [G loss: 1.328502]\n",
      "epoch:15 step:14209 [D loss: 0.499305, acc.: 75.00%] [G loss: 1.201155]\n",
      "epoch:15 step:14210 [D loss: 0.718653, acc.: 50.78%] [G loss: 1.017081]\n",
      "epoch:15 step:14211 [D loss: 0.529201, acc.: 71.88%] [G loss: 1.313056]\n",
      "epoch:15 step:14212 [D loss: 0.581648, acc.: 71.09%] [G loss: 1.172433]\n",
      "epoch:15 step:14213 [D loss: 0.516850, acc.: 75.00%] [G loss: 1.305829]\n",
      "epoch:15 step:14214 [D loss: 0.625349, acc.: 67.19%] [G loss: 1.300734]\n",
      "epoch:15 step:14215 [D loss: 0.558582, acc.: 74.22%] [G loss: 1.152778]\n",
      "epoch:15 step:14216 [D loss: 0.531028, acc.: 74.22%] [G loss: 1.346684]\n",
      "epoch:15 step:14217 [D loss: 0.659909, acc.: 61.72%] [G loss: 1.018937]\n",
      "epoch:15 step:14218 [D loss: 0.545091, acc.: 70.31%] [G loss: 1.274082]\n",
      "epoch:15 step:14219 [D loss: 0.605128, acc.: 66.41%] [G loss: 1.362640]\n",
      "epoch:15 step:14220 [D loss: 0.646232, acc.: 61.72%] [G loss: 1.116502]\n",
      "epoch:15 step:14221 [D loss: 0.692869, acc.: 57.81%] [G loss: 1.113646]\n",
      "epoch:15 step:14222 [D loss: 0.660254, acc.: 62.50%] [G loss: 1.282448]\n",
      "epoch:15 step:14223 [D loss: 0.614463, acc.: 70.31%] [G loss: 1.329785]\n",
      "epoch:15 step:14224 [D loss: 0.519234, acc.: 76.56%] [G loss: 1.196090]\n",
      "epoch:15 step:14225 [D loss: 0.618060, acc.: 64.84%] [G loss: 1.316215]\n",
      "epoch:15 step:14226 [D loss: 0.603421, acc.: 64.84%] [G loss: 1.178360]\n",
      "epoch:15 step:14227 [D loss: 0.590834, acc.: 68.75%] [G loss: 1.220141]\n",
      "epoch:15 step:14228 [D loss: 0.544561, acc.: 77.34%] [G loss: 1.313167]\n",
      "epoch:15 step:14229 [D loss: 0.452576, acc.: 80.47%] [G loss: 1.207430]\n",
      "epoch:15 step:14230 [D loss: 0.476739, acc.: 76.56%] [G loss: 1.476068]\n",
      "epoch:15 step:14231 [D loss: 0.529616, acc.: 73.44%] [G loss: 1.325194]\n",
      "epoch:15 step:14232 [D loss: 0.485015, acc.: 75.00%] [G loss: 1.368599]\n",
      "epoch:15 step:14233 [D loss: 0.617821, acc.: 67.97%] [G loss: 1.097957]\n",
      "epoch:15 step:14234 [D loss: 0.575961, acc.: 69.53%] [G loss: 1.383213]\n",
      "epoch:15 step:14235 [D loss: 0.521972, acc.: 71.88%] [G loss: 1.163848]\n",
      "epoch:15 step:14236 [D loss: 0.678100, acc.: 57.81%] [G loss: 1.175292]\n",
      "epoch:15 step:14237 [D loss: 0.494718, acc.: 74.22%] [G loss: 1.280824]\n",
      "epoch:15 step:14238 [D loss: 0.635341, acc.: 62.50%] [G loss: 1.017763]\n",
      "epoch:15 step:14239 [D loss: 0.636708, acc.: 60.16%] [G loss: 1.187351]\n",
      "epoch:15 step:14240 [D loss: 0.741941, acc.: 53.12%] [G loss: 1.292987]\n",
      "epoch:15 step:14241 [D loss: 0.630461, acc.: 63.28%] [G loss: 1.299945]\n",
      "epoch:15 step:14242 [D loss: 0.518736, acc.: 78.12%] [G loss: 1.350746]\n",
      "epoch:15 step:14243 [D loss: 0.599538, acc.: 67.97%] [G loss: 1.103536]\n",
      "epoch:15 step:14244 [D loss: 0.634689, acc.: 58.59%] [G loss: 0.944742]\n",
      "epoch:15 step:14245 [D loss: 0.577956, acc.: 68.75%] [G loss: 1.487144]\n",
      "epoch:15 step:14246 [D loss: 0.692112, acc.: 58.59%] [G loss: 0.955732]\n",
      "epoch:15 step:14247 [D loss: 0.550099, acc.: 71.09%] [G loss: 1.117937]\n",
      "epoch:15 step:14248 [D loss: 0.597615, acc.: 69.53%] [G loss: 1.481605]\n",
      "epoch:15 step:14249 [D loss: 0.611657, acc.: 60.94%] [G loss: 1.430780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14250 [D loss: 0.552924, acc.: 67.19%] [G loss: 1.347693]\n",
      "epoch:15 step:14251 [D loss: 0.658836, acc.: 62.50%] [G loss: 1.185233]\n",
      "epoch:15 step:14252 [D loss: 0.542536, acc.: 70.31%] [G loss: 1.432710]\n",
      "epoch:15 step:14253 [D loss: 0.715705, acc.: 54.69%] [G loss: 1.400784]\n",
      "epoch:15 step:14254 [D loss: 0.609124, acc.: 67.19%] [G loss: 1.279005]\n",
      "epoch:15 step:14255 [D loss: 0.646953, acc.: 57.81%] [G loss: 1.414984]\n",
      "epoch:15 step:14256 [D loss: 0.454011, acc.: 81.25%] [G loss: 1.404025]\n",
      "epoch:15 step:14257 [D loss: 0.563862, acc.: 65.62%] [G loss: 1.407941]\n",
      "epoch:15 step:14258 [D loss: 0.587903, acc.: 65.62%] [G loss: 1.071789]\n",
      "epoch:15 step:14259 [D loss: 0.500208, acc.: 75.78%] [G loss: 1.505934]\n",
      "epoch:15 step:14260 [D loss: 0.542812, acc.: 75.00%] [G loss: 1.260565]\n",
      "epoch:15 step:14261 [D loss: 0.432501, acc.: 84.38%] [G loss: 1.345404]\n",
      "epoch:15 step:14262 [D loss: 0.498848, acc.: 77.34%] [G loss: 1.437464]\n",
      "epoch:15 step:14263 [D loss: 0.589187, acc.: 67.19%] [G loss: 1.319909]\n",
      "epoch:15 step:14264 [D loss: 0.675757, acc.: 60.16%] [G loss: 1.186636]\n",
      "epoch:15 step:14265 [D loss: 0.489834, acc.: 75.00%] [G loss: 1.186578]\n",
      "epoch:15 step:14266 [D loss: 0.481583, acc.: 78.12%] [G loss: 1.367430]\n",
      "epoch:15 step:14267 [D loss: 0.676842, acc.: 62.50%] [G loss: 0.936776]\n",
      "epoch:15 step:14268 [D loss: 0.761823, acc.: 52.34%] [G loss: 1.106830]\n",
      "epoch:15 step:14269 [D loss: 0.633527, acc.: 68.75%] [G loss: 1.324341]\n",
      "epoch:15 step:14270 [D loss: 0.706084, acc.: 60.94%] [G loss: 1.099910]\n",
      "epoch:15 step:14271 [D loss: 0.675846, acc.: 56.25%] [G loss: 0.971868]\n",
      "epoch:15 step:14272 [D loss: 0.652504, acc.: 63.28%] [G loss: 1.103533]\n",
      "epoch:15 step:14273 [D loss: 0.685496, acc.: 61.72%] [G loss: 1.135465]\n",
      "epoch:15 step:14274 [D loss: 0.557369, acc.: 69.53%] [G loss: 1.175146]\n",
      "epoch:15 step:14275 [D loss: 0.518615, acc.: 78.12%] [G loss: 1.292684]\n",
      "epoch:15 step:14276 [D loss: 0.585774, acc.: 67.19%] [G loss: 1.658237]\n",
      "epoch:15 step:14277 [D loss: 0.644547, acc.: 62.50%] [G loss: 1.537462]\n",
      "epoch:15 step:14278 [D loss: 0.643486, acc.: 66.41%] [G loss: 1.053261]\n",
      "epoch:15 step:14279 [D loss: 0.729442, acc.: 56.25%] [G loss: 1.295334]\n",
      "epoch:15 step:14280 [D loss: 0.648055, acc.: 62.50%] [G loss: 1.432658]\n",
      "epoch:15 step:14281 [D loss: 0.601127, acc.: 71.09%] [G loss: 1.263843]\n",
      "epoch:15 step:14282 [D loss: 0.585982, acc.: 68.75%] [G loss: 1.388245]\n",
      "epoch:15 step:14283 [D loss: 0.611580, acc.: 60.94%] [G loss: 0.895074]\n",
      "epoch:15 step:14284 [D loss: 0.528759, acc.: 71.09%] [G loss: 1.355944]\n",
      "epoch:15 step:14285 [D loss: 0.602297, acc.: 65.62%] [G loss: 1.223266]\n",
      "epoch:15 step:14286 [D loss: 0.570322, acc.: 73.44%] [G loss: 1.342118]\n",
      "epoch:15 step:14287 [D loss: 0.654134, acc.: 59.38%] [G loss: 1.079781]\n",
      "epoch:15 step:14288 [D loss: 0.550574, acc.: 75.00%] [G loss: 1.373470]\n",
      "epoch:15 step:14289 [D loss: 0.506292, acc.: 78.12%] [G loss: 1.340601]\n",
      "epoch:15 step:14290 [D loss: 0.663901, acc.: 59.38%] [G loss: 1.029275]\n",
      "epoch:15 step:14291 [D loss: 0.594537, acc.: 71.09%] [G loss: 1.097729]\n",
      "epoch:15 step:14292 [D loss: 0.590383, acc.: 67.19%] [G loss: 1.552925]\n",
      "epoch:15 step:14293 [D loss: 0.563541, acc.: 71.09%] [G loss: 1.581902]\n",
      "epoch:15 step:14294 [D loss: 0.551431, acc.: 69.53%] [G loss: 1.793132]\n",
      "epoch:15 step:14295 [D loss: 0.624989, acc.: 60.94%] [G loss: 1.161806]\n",
      "epoch:15 step:14296 [D loss: 0.573329, acc.: 71.88%] [G loss: 1.375534]\n",
      "epoch:15 step:14297 [D loss: 0.687136, acc.: 60.94%] [G loss: 1.109499]\n",
      "epoch:15 step:14298 [D loss: 0.559694, acc.: 73.44%] [G loss: 1.245533]\n",
      "epoch:15 step:14299 [D loss: 0.610461, acc.: 62.50%] [G loss: 1.050171]\n",
      "epoch:15 step:14300 [D loss: 0.497861, acc.: 76.56%] [G loss: 1.532926]\n",
      "epoch:15 step:14301 [D loss: 0.493870, acc.: 81.25%] [G loss: 1.231989]\n",
      "epoch:15 step:14302 [D loss: 0.605734, acc.: 64.84%] [G loss: 1.229439]\n",
      "epoch:15 step:14303 [D loss: 0.537851, acc.: 75.00%] [G loss: 1.041681]\n",
      "epoch:15 step:14304 [D loss: 0.529929, acc.: 70.31%] [G loss: 1.465246]\n",
      "epoch:15 step:14305 [D loss: 0.599113, acc.: 67.97%] [G loss: 1.130073]\n",
      "epoch:15 step:14306 [D loss: 0.581923, acc.: 68.75%] [G loss: 1.159982]\n",
      "epoch:15 step:14307 [D loss: 0.415009, acc.: 84.38%] [G loss: 1.313920]\n",
      "epoch:15 step:14308 [D loss: 0.468877, acc.: 81.25%] [G loss: 1.024298]\n",
      "epoch:15 step:14309 [D loss: 0.537850, acc.: 75.78%] [G loss: 1.346298]\n",
      "epoch:15 step:14310 [D loss: 0.412681, acc.: 84.38%] [G loss: 1.037203]\n",
      "epoch:15 step:14311 [D loss: 0.656137, acc.: 59.38%] [G loss: 1.185854]\n",
      "epoch:15 step:14312 [D loss: 0.461394, acc.: 84.38%] [G loss: 1.380554]\n",
      "epoch:15 step:14313 [D loss: 0.655335, acc.: 59.38%] [G loss: 1.168215]\n",
      "epoch:15 step:14314 [D loss: 0.623734, acc.: 65.62%] [G loss: 1.251317]\n",
      "epoch:15 step:14315 [D loss: 0.503707, acc.: 75.00%] [G loss: 1.363171]\n",
      "epoch:15 step:14316 [D loss: 0.508275, acc.: 80.47%] [G loss: 1.337475]\n",
      "epoch:15 step:14317 [D loss: 0.711277, acc.: 59.38%] [G loss: 1.165980]\n",
      "epoch:15 step:14318 [D loss: 0.623399, acc.: 64.84%] [G loss: 1.092019]\n",
      "epoch:15 step:14319 [D loss: 0.495212, acc.: 77.34%] [G loss: 1.293594]\n",
      "epoch:15 step:14320 [D loss: 0.440346, acc.: 84.38%] [G loss: 1.409071]\n",
      "epoch:15 step:14321 [D loss: 0.596238, acc.: 68.75%] [G loss: 1.452061]\n",
      "epoch:15 step:14322 [D loss: 0.636623, acc.: 63.28%] [G loss: 1.368040]\n",
      "epoch:15 step:14323 [D loss: 0.607232, acc.: 64.06%] [G loss: 1.450888]\n",
      "epoch:15 step:14324 [D loss: 0.539897, acc.: 76.56%] [G loss: 1.427008]\n",
      "epoch:15 step:14325 [D loss: 0.642283, acc.: 67.97%] [G loss: 1.169034]\n",
      "epoch:15 step:14326 [D loss: 0.509372, acc.: 75.78%] [G loss: 1.531508]\n",
      "epoch:15 step:14327 [D loss: 0.572097, acc.: 74.22%] [G loss: 1.549353]\n",
      "epoch:15 step:14328 [D loss: 0.658482, acc.: 64.06%] [G loss: 1.072693]\n",
      "epoch:15 step:14329 [D loss: 0.577727, acc.: 74.22%] [G loss: 1.308887]\n",
      "epoch:15 step:14330 [D loss: 0.798911, acc.: 46.88%] [G loss: 0.893910]\n",
      "epoch:15 step:14331 [D loss: 0.613529, acc.: 68.75%] [G loss: 1.154095]\n",
      "epoch:15 step:14332 [D loss: 0.590444, acc.: 71.09%] [G loss: 1.175264]\n",
      "epoch:15 step:14333 [D loss: 0.547408, acc.: 71.88%] [G loss: 1.295623]\n",
      "epoch:15 step:14334 [D loss: 0.450869, acc.: 80.47%] [G loss: 1.241104]\n",
      "epoch:15 step:14335 [D loss: 0.659074, acc.: 62.50%] [G loss: 1.206767]\n",
      "epoch:15 step:14336 [D loss: 0.587148, acc.: 71.09%] [G loss: 1.033612]\n",
      "epoch:15 step:14337 [D loss: 0.642038, acc.: 65.62%] [G loss: 1.351033]\n",
      "epoch:15 step:14338 [D loss: 0.672551, acc.: 60.16%] [G loss: 0.995286]\n",
      "epoch:15 step:14339 [D loss: 0.574716, acc.: 71.09%] [G loss: 1.408821]\n",
      "epoch:15 step:14340 [D loss: 0.557717, acc.: 76.56%] [G loss: 1.038671]\n",
      "epoch:15 step:14341 [D loss: 0.691195, acc.: 63.28%] [G loss: 1.471261]\n",
      "epoch:15 step:14342 [D loss: 0.623365, acc.: 69.53%] [G loss: 1.327629]\n",
      "epoch:15 step:14343 [D loss: 0.613121, acc.: 71.88%] [G loss: 1.574730]\n",
      "epoch:15 step:14344 [D loss: 0.543450, acc.: 70.31%] [G loss: 1.138300]\n",
      "epoch:15 step:14345 [D loss: 0.436419, acc.: 82.81%] [G loss: 1.334810]\n",
      "epoch:15 step:14346 [D loss: 0.557375, acc.: 69.53%] [G loss: 1.247159]\n",
      "epoch:15 step:14347 [D loss: 0.580853, acc.: 67.97%] [G loss: 1.071682]\n",
      "epoch:15 step:14348 [D loss: 0.543637, acc.: 78.91%] [G loss: 1.107393]\n",
      "epoch:15 step:14349 [D loss: 0.613366, acc.: 67.19%] [G loss: 1.323830]\n",
      "epoch:15 step:14350 [D loss: 0.619295, acc.: 67.19%] [G loss: 1.467544]\n",
      "epoch:15 step:14351 [D loss: 0.649807, acc.: 64.84%] [G loss: 1.218209]\n",
      "epoch:15 step:14352 [D loss: 0.530777, acc.: 78.91%] [G loss: 1.571354]\n",
      "epoch:15 step:14353 [D loss: 0.611524, acc.: 64.84%] [G loss: 1.248478]\n",
      "epoch:15 step:14354 [D loss: 0.651631, acc.: 63.28%] [G loss: 1.271775]\n",
      "epoch:15 step:14355 [D loss: 0.499510, acc.: 69.53%] [G loss: 1.128381]\n",
      "epoch:15 step:14356 [D loss: 0.596670, acc.: 67.97%] [G loss: 1.477759]\n",
      "epoch:15 step:14357 [D loss: 0.615203, acc.: 67.19%] [G loss: 1.252283]\n",
      "epoch:15 step:14358 [D loss: 0.561565, acc.: 75.00%] [G loss: 1.519452]\n",
      "epoch:15 step:14359 [D loss: 0.558190, acc.: 70.31%] [G loss: 1.458587]\n",
      "epoch:15 step:14360 [D loss: 0.613184, acc.: 61.72%] [G loss: 1.142310]\n",
      "epoch:15 step:14361 [D loss: 0.432950, acc.: 81.25%] [G loss: 1.499047]\n",
      "epoch:15 step:14362 [D loss: 0.663349, acc.: 60.16%] [G loss: 1.250496]\n",
      "epoch:15 step:14363 [D loss: 0.597621, acc.: 67.19%] [G loss: 1.170009]\n",
      "epoch:15 step:14364 [D loss: 0.500501, acc.: 76.56%] [G loss: 1.169147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14365 [D loss: 0.530142, acc.: 75.00%] [G loss: 1.121848]\n",
      "epoch:15 step:14366 [D loss: 0.574152, acc.: 71.09%] [G loss: 1.157827]\n",
      "epoch:15 step:14367 [D loss: 0.590889, acc.: 67.97%] [G loss: 1.372016]\n",
      "epoch:15 step:14368 [D loss: 0.531205, acc.: 70.31%] [G loss: 1.383969]\n",
      "epoch:15 step:14369 [D loss: 0.518506, acc.: 74.22%] [G loss: 1.122471]\n",
      "epoch:15 step:14370 [D loss: 0.527857, acc.: 77.34%] [G loss: 1.163895]\n",
      "epoch:15 step:14371 [D loss: 0.591647, acc.: 69.53%] [G loss: 1.327175]\n",
      "epoch:15 step:14372 [D loss: 0.566419, acc.: 68.75%] [G loss: 1.370381]\n",
      "epoch:15 step:14373 [D loss: 0.646462, acc.: 60.16%] [G loss: 1.399811]\n",
      "epoch:15 step:14374 [D loss: 0.526595, acc.: 75.78%] [G loss: 1.282985]\n",
      "epoch:15 step:14375 [D loss: 0.624501, acc.: 64.06%] [G loss: 1.020698]\n",
      "epoch:15 step:14376 [D loss: 0.680527, acc.: 56.25%] [G loss: 1.306107]\n",
      "epoch:15 step:14377 [D loss: 0.714350, acc.: 57.03%] [G loss: 1.046132]\n",
      "epoch:15 step:14378 [D loss: 0.540490, acc.: 71.88%] [G loss: 1.610884]\n",
      "epoch:15 step:14379 [D loss: 0.471682, acc.: 73.44%] [G loss: 1.186522]\n",
      "epoch:15 step:14380 [D loss: 0.490306, acc.: 81.25%] [G loss: 1.365666]\n",
      "epoch:15 step:14381 [D loss: 0.632600, acc.: 63.28%] [G loss: 1.194470]\n",
      "epoch:15 step:14382 [D loss: 0.547361, acc.: 69.53%] [G loss: 1.200117]\n",
      "epoch:15 step:14383 [D loss: 0.518850, acc.: 78.12%] [G loss: 1.513431]\n",
      "epoch:15 step:14384 [D loss: 0.551817, acc.: 67.19%] [G loss: 1.566702]\n",
      "epoch:15 step:14385 [D loss: 0.718551, acc.: 58.59%] [G loss: 1.370988]\n",
      "epoch:15 step:14386 [D loss: 0.553119, acc.: 72.66%] [G loss: 1.357094]\n",
      "epoch:15 step:14387 [D loss: 0.601389, acc.: 67.97%] [G loss: 1.280904]\n",
      "epoch:15 step:14388 [D loss: 0.603897, acc.: 68.75%] [G loss: 1.423144]\n",
      "epoch:15 step:14389 [D loss: 0.631094, acc.: 64.84%] [G loss: 1.091684]\n",
      "epoch:15 step:14390 [D loss: 0.496898, acc.: 81.25%] [G loss: 1.431721]\n",
      "epoch:15 step:14391 [D loss: 0.415716, acc.: 86.72%] [G loss: 1.560072]\n",
      "epoch:15 step:14392 [D loss: 0.589311, acc.: 70.31%] [G loss: 1.328418]\n",
      "epoch:15 step:14393 [D loss: 0.536292, acc.: 75.78%] [G loss: 1.256208]\n",
      "epoch:15 step:14394 [D loss: 0.632845, acc.: 57.81%] [G loss: 1.090838]\n",
      "epoch:15 step:14395 [D loss: 0.462339, acc.: 80.47%] [G loss: 1.181293]\n",
      "epoch:15 step:14396 [D loss: 0.537967, acc.: 72.66%] [G loss: 1.343064]\n",
      "epoch:15 step:14397 [D loss: 0.692612, acc.: 60.16%] [G loss: 1.096240]\n",
      "epoch:15 step:14398 [D loss: 0.576070, acc.: 71.09%] [G loss: 1.437712]\n",
      "epoch:15 step:14399 [D loss: 0.480042, acc.: 81.25%] [G loss: 1.455994]\n",
      "epoch:15 step:14400 [D loss: 0.453788, acc.: 82.81%] [G loss: 1.416806]\n",
      "##############\n",
      "[2.60456976 2.07208965 1.76675991 2.78146867 1.0076638  6.35690286\n",
      " 1.92033992 2.95943292 3.97534942 5.10498176]\n",
      "##########\n",
      "epoch:15 step:14401 [D loss: 0.654481, acc.: 66.41%] [G loss: 1.010385]\n",
      "epoch:15 step:14402 [D loss: 0.631476, acc.: 60.94%] [G loss: 1.097474]\n",
      "epoch:15 step:14403 [D loss: 0.671015, acc.: 66.41%] [G loss: 1.436697]\n",
      "epoch:15 step:14404 [D loss: 0.686008, acc.: 53.91%] [G loss: 1.172616]\n",
      "epoch:15 step:14405 [D loss: 0.588690, acc.: 67.19%] [G loss: 1.455735]\n",
      "epoch:15 step:14406 [D loss: 0.565432, acc.: 73.44%] [G loss: 1.452066]\n",
      "epoch:15 step:14407 [D loss: 0.705583, acc.: 57.81%] [G loss: 1.229029]\n",
      "epoch:15 step:14408 [D loss: 0.641667, acc.: 64.84%] [G loss: 1.211327]\n",
      "epoch:15 step:14409 [D loss: 0.578750, acc.: 65.62%] [G loss: 1.245000]\n",
      "epoch:15 step:14410 [D loss: 0.688372, acc.: 53.12%] [G loss: 1.053496]\n",
      "epoch:15 step:14411 [D loss: 0.444961, acc.: 82.03%] [G loss: 1.472179]\n",
      "epoch:15 step:14412 [D loss: 0.534892, acc.: 78.12%] [G loss: 1.290854]\n",
      "epoch:15 step:14413 [D loss: 0.605056, acc.: 64.06%] [G loss: 1.320289]\n",
      "epoch:15 step:14414 [D loss: 0.527156, acc.: 75.00%] [G loss: 1.399480]\n",
      "epoch:15 step:14415 [D loss: 0.509852, acc.: 71.09%] [G loss: 1.049042]\n",
      "epoch:15 step:14416 [D loss: 0.544562, acc.: 75.00%] [G loss: 1.433597]\n",
      "epoch:15 step:14417 [D loss: 0.621816, acc.: 66.41%] [G loss: 1.116551]\n",
      "epoch:15 step:14418 [D loss: 0.499075, acc.: 75.78%] [G loss: 1.132210]\n",
      "epoch:15 step:14419 [D loss: 0.615025, acc.: 62.50%] [G loss: 1.292449]\n",
      "epoch:15 step:14420 [D loss: 0.599304, acc.: 71.09%] [G loss: 1.160366]\n",
      "epoch:15 step:14421 [D loss: 0.519334, acc.: 74.22%] [G loss: 1.181255]\n",
      "epoch:15 step:14422 [D loss: 0.768341, acc.: 50.00%] [G loss: 1.003726]\n",
      "epoch:15 step:14423 [D loss: 0.469241, acc.: 78.91%] [G loss: 1.246768]\n",
      "epoch:15 step:14424 [D loss: 0.510828, acc.: 81.25%] [G loss: 1.542292]\n",
      "epoch:15 step:14425 [D loss: 0.674878, acc.: 60.94%] [G loss: 1.112164]\n",
      "epoch:15 step:14426 [D loss: 0.637278, acc.: 60.94%] [G loss: 1.420343]\n",
      "epoch:15 step:14427 [D loss: 0.523083, acc.: 75.78%] [G loss: 1.492068]\n",
      "epoch:15 step:14428 [D loss: 0.555192, acc.: 73.44%] [G loss: 1.095699]\n",
      "epoch:15 step:14429 [D loss: 0.621610, acc.: 62.50%] [G loss: 1.271706]\n",
      "epoch:15 step:14430 [D loss: 0.651450, acc.: 60.94%] [G loss: 1.289056]\n",
      "epoch:15 step:14431 [D loss: 0.576866, acc.: 67.19%] [G loss: 1.219248]\n",
      "epoch:15 step:14432 [D loss: 0.527091, acc.: 71.09%] [G loss: 1.539053]\n",
      "epoch:15 step:14433 [D loss: 0.609210, acc.: 62.50%] [G loss: 1.402085]\n",
      "epoch:15 step:14434 [D loss: 0.509555, acc.: 78.12%] [G loss: 1.335620]\n",
      "epoch:15 step:14435 [D loss: 0.565113, acc.: 73.44%] [G loss: 1.164376]\n",
      "epoch:15 step:14436 [D loss: 0.646194, acc.: 61.72%] [G loss: 1.601113]\n",
      "epoch:15 step:14437 [D loss: 0.506651, acc.: 74.22%] [G loss: 1.403378]\n",
      "epoch:15 step:14438 [D loss: 0.538150, acc.: 73.44%] [G loss: 1.375979]\n",
      "epoch:15 step:14439 [D loss: 0.719574, acc.: 55.47%] [G loss: 1.206255]\n",
      "epoch:15 step:14440 [D loss: 0.580151, acc.: 68.75%] [G loss: 1.298504]\n",
      "epoch:15 step:14441 [D loss: 0.701822, acc.: 55.47%] [G loss: 1.036792]\n",
      "epoch:15 step:14442 [D loss: 0.688885, acc.: 59.38%] [G loss: 1.239662]\n",
      "epoch:15 step:14443 [D loss: 0.690994, acc.: 60.16%] [G loss: 1.196834]\n",
      "epoch:15 step:14444 [D loss: 0.631336, acc.: 63.28%] [G loss: 1.313098]\n",
      "epoch:15 step:14445 [D loss: 0.606202, acc.: 66.41%] [G loss: 1.108164]\n",
      "epoch:15 step:14446 [D loss: 0.710926, acc.: 56.25%] [G loss: 1.238160]\n",
      "epoch:15 step:14447 [D loss: 0.545492, acc.: 71.88%] [G loss: 1.171852]\n",
      "epoch:15 step:14448 [D loss: 0.693955, acc.: 61.72%] [G loss: 1.017087]\n",
      "epoch:15 step:14449 [D loss: 0.556014, acc.: 74.22%] [G loss: 1.141300]\n",
      "epoch:15 step:14450 [D loss: 0.700457, acc.: 54.69%] [G loss: 1.263731]\n",
      "epoch:15 step:14451 [D loss: 0.544280, acc.: 69.53%] [G loss: 1.343445]\n",
      "epoch:15 step:14452 [D loss: 0.764049, acc.: 53.12%] [G loss: 1.112299]\n",
      "epoch:15 step:14453 [D loss: 0.530034, acc.: 68.75%] [G loss: 1.189240]\n",
      "epoch:15 step:14454 [D loss: 0.574960, acc.: 68.75%] [G loss: 1.080610]\n",
      "epoch:15 step:14455 [D loss: 0.657492, acc.: 58.59%] [G loss: 1.262580]\n",
      "epoch:15 step:14456 [D loss: 0.618713, acc.: 67.19%] [G loss: 1.485959]\n",
      "epoch:15 step:14457 [D loss: 0.456725, acc.: 80.47%] [G loss: 1.164968]\n",
      "epoch:15 step:14458 [D loss: 0.588868, acc.: 67.19%] [G loss: 1.276943]\n",
      "epoch:15 step:14459 [D loss: 0.523905, acc.: 78.91%] [G loss: 1.392699]\n",
      "epoch:15 step:14460 [D loss: 0.622240, acc.: 63.28%] [G loss: 1.270279]\n",
      "epoch:15 step:14461 [D loss: 0.429272, acc.: 83.59%] [G loss: 1.524749]\n",
      "epoch:15 step:14462 [D loss: 0.555456, acc.: 75.00%] [G loss: 1.050458]\n",
      "epoch:15 step:14463 [D loss: 0.558033, acc.: 67.97%] [G loss: 1.168663]\n",
      "epoch:15 step:14464 [D loss: 0.841790, acc.: 42.19%] [G loss: 1.077214]\n",
      "epoch:15 step:14465 [D loss: 0.572038, acc.: 69.53%] [G loss: 1.131181]\n",
      "epoch:15 step:14466 [D loss: 0.536389, acc.: 77.34%] [G loss: 1.238663]\n",
      "epoch:15 step:14467 [D loss: 0.675392, acc.: 56.25%] [G loss: 1.211076]\n",
      "epoch:15 step:14468 [D loss: 0.648874, acc.: 63.28%] [G loss: 1.391328]\n",
      "epoch:15 step:14469 [D loss: 0.504162, acc.: 74.22%] [G loss: 1.454123]\n",
      "epoch:15 step:14470 [D loss: 0.501305, acc.: 80.47%] [G loss: 1.419997]\n",
      "epoch:15 step:14471 [D loss: 0.708645, acc.: 56.25%] [G loss: 0.884458]\n",
      "epoch:15 step:14472 [D loss: 0.627459, acc.: 62.50%] [G loss: 1.186225]\n",
      "epoch:15 step:14473 [D loss: 0.541048, acc.: 75.78%] [G loss: 1.397229]\n",
      "epoch:15 step:14474 [D loss: 0.499914, acc.: 77.34%] [G loss: 1.148070]\n",
      "epoch:15 step:14475 [D loss: 0.552433, acc.: 75.78%] [G loss: 1.416562]\n",
      "epoch:15 step:14476 [D loss: 0.521770, acc.: 75.00%] [G loss: 1.281149]\n",
      "epoch:15 step:14477 [D loss: 0.532744, acc.: 71.88%] [G loss: 1.481393]\n",
      "epoch:15 step:14478 [D loss: 0.605803, acc.: 64.06%] [G loss: 1.479713]\n",
      "epoch:15 step:14479 [D loss: 0.450837, acc.: 83.59%] [G loss: 1.337540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14480 [D loss: 0.699035, acc.: 57.81%] [G loss: 1.281963]\n",
      "epoch:15 step:14481 [D loss: 0.576440, acc.: 71.88%] [G loss: 1.312871]\n",
      "epoch:15 step:14482 [D loss: 0.685208, acc.: 57.03%] [G loss: 1.114198]\n",
      "epoch:15 step:14483 [D loss: 0.694516, acc.: 60.16%] [G loss: 1.274289]\n",
      "epoch:15 step:14484 [D loss: 0.554539, acc.: 74.22%] [G loss: 1.154782]\n",
      "epoch:15 step:14485 [D loss: 0.683963, acc.: 61.72%] [G loss: 1.171988]\n",
      "epoch:15 step:14486 [D loss: 0.584547, acc.: 65.62%] [G loss: 1.104293]\n",
      "epoch:15 step:14487 [D loss: 0.421159, acc.: 80.47%] [G loss: 1.224689]\n",
      "epoch:15 step:14488 [D loss: 0.665742, acc.: 63.28%] [G loss: 1.233946]\n",
      "epoch:15 step:14489 [D loss: 0.576658, acc.: 65.62%] [G loss: 1.217566]\n",
      "epoch:15 step:14490 [D loss: 0.602915, acc.: 66.41%] [G loss: 1.395376]\n",
      "epoch:15 step:14491 [D loss: 0.579714, acc.: 70.31%] [G loss: 1.456001]\n",
      "epoch:15 step:14492 [D loss: 0.575715, acc.: 71.09%] [G loss: 1.400434]\n",
      "epoch:15 step:14493 [D loss: 0.685577, acc.: 63.28%] [G loss: 1.037436]\n",
      "epoch:15 step:14494 [D loss: 0.606037, acc.: 66.41%] [G loss: 0.871802]\n",
      "epoch:15 step:14495 [D loss: 0.559812, acc.: 71.88%] [G loss: 1.173859]\n",
      "epoch:15 step:14496 [D loss: 0.650107, acc.: 65.62%] [G loss: 1.377342]\n",
      "epoch:15 step:14497 [D loss: 0.646084, acc.: 60.16%] [G loss: 1.297093]\n",
      "epoch:15 step:14498 [D loss: 0.598722, acc.: 69.53%] [G loss: 1.365644]\n",
      "epoch:15 step:14499 [D loss: 0.444694, acc.: 84.38%] [G loss: 1.448852]\n",
      "epoch:15 step:14500 [D loss: 0.604759, acc.: 64.84%] [G loss: 1.160742]\n",
      "epoch:15 step:14501 [D loss: 0.775782, acc.: 50.00%] [G loss: 1.199728]\n",
      "epoch:15 step:14502 [D loss: 0.589848, acc.: 67.19%] [G loss: 1.254503]\n",
      "epoch:15 step:14503 [D loss: 0.681236, acc.: 60.94%] [G loss: 1.415196]\n",
      "epoch:15 step:14504 [D loss: 0.572611, acc.: 69.53%] [G loss: 1.391799]\n",
      "epoch:15 step:14505 [D loss: 0.597309, acc.: 63.28%] [G loss: 1.600121]\n",
      "epoch:15 step:14506 [D loss: 0.539013, acc.: 73.44%] [G loss: 1.232137]\n",
      "epoch:15 step:14507 [D loss: 0.501346, acc.: 81.25%] [G loss: 1.098564]\n",
      "epoch:15 step:14508 [D loss: 0.608218, acc.: 69.53%] [G loss: 1.210068]\n",
      "epoch:15 step:14509 [D loss: 0.507086, acc.: 78.91%] [G loss: 1.223151]\n",
      "epoch:15 step:14510 [D loss: 0.594219, acc.: 67.19%] [G loss: 1.155637]\n",
      "epoch:15 step:14511 [D loss: 0.570857, acc.: 70.31%] [G loss: 1.133323]\n",
      "epoch:15 step:14512 [D loss: 0.609098, acc.: 66.41%] [G loss: 1.409353]\n",
      "epoch:15 step:14513 [D loss: 0.599553, acc.: 71.09%] [G loss: 1.349053]\n",
      "epoch:15 step:14514 [D loss: 0.547365, acc.: 78.12%] [G loss: 1.180284]\n",
      "epoch:15 step:14515 [D loss: 0.480955, acc.: 75.78%] [G loss: 1.483245]\n",
      "epoch:15 step:14516 [D loss: 0.740889, acc.: 51.56%] [G loss: 1.045083]\n",
      "epoch:15 step:14517 [D loss: 0.654972, acc.: 61.72%] [G loss: 1.094857]\n",
      "epoch:15 step:14518 [D loss: 0.644449, acc.: 63.28%] [G loss: 1.053127]\n",
      "epoch:15 step:14519 [D loss: 0.687942, acc.: 60.94%] [G loss: 1.220967]\n",
      "epoch:15 step:14520 [D loss: 0.550283, acc.: 73.44%] [G loss: 1.526557]\n",
      "epoch:15 step:14521 [D loss: 0.604117, acc.: 66.41%] [G loss: 1.228965]\n",
      "epoch:15 step:14522 [D loss: 0.503760, acc.: 78.12%] [G loss: 1.257112]\n",
      "epoch:15 step:14523 [D loss: 0.509766, acc.: 76.56%] [G loss: 1.335714]\n",
      "epoch:15 step:14524 [D loss: 0.575629, acc.: 71.88%] [G loss: 1.556771]\n",
      "epoch:15 step:14525 [D loss: 0.788468, acc.: 51.56%] [G loss: 1.025647]\n",
      "epoch:15 step:14526 [D loss: 0.481935, acc.: 79.69%] [G loss: 1.441229]\n",
      "epoch:15 step:14527 [D loss: 0.614594, acc.: 67.97%] [G loss: 1.006900]\n",
      "epoch:15 step:14528 [D loss: 0.596692, acc.: 62.50%] [G loss: 1.002938]\n",
      "epoch:15 step:14529 [D loss: 0.521357, acc.: 75.00%] [G loss: 1.295053]\n",
      "epoch:15 step:14530 [D loss: 0.476945, acc.: 80.47%] [G loss: 1.046090]\n",
      "epoch:15 step:14531 [D loss: 0.552313, acc.: 71.88%] [G loss: 1.337581]\n",
      "epoch:15 step:14532 [D loss: 0.608412, acc.: 67.19%] [G loss: 1.216394]\n",
      "epoch:15 step:14533 [D loss: 0.620996, acc.: 60.16%] [G loss: 1.304443]\n",
      "epoch:15 step:14534 [D loss: 0.673046, acc.: 60.94%] [G loss: 1.020684]\n",
      "epoch:15 step:14535 [D loss: 0.579004, acc.: 74.22%] [G loss: 1.330361]\n",
      "epoch:15 step:14536 [D loss: 0.716384, acc.: 54.69%] [G loss: 1.238309]\n",
      "epoch:15 step:14537 [D loss: 0.438337, acc.: 82.03%] [G loss: 1.231546]\n",
      "epoch:15 step:14538 [D loss: 0.637421, acc.: 64.84%] [G loss: 1.143192]\n",
      "epoch:15 step:14539 [D loss: 0.561452, acc.: 71.09%] [G loss: 1.239286]\n",
      "epoch:15 step:14540 [D loss: 0.511730, acc.: 74.22%] [G loss: 1.229394]\n",
      "epoch:15 step:14541 [D loss: 0.504868, acc.: 77.34%] [G loss: 1.352169]\n",
      "epoch:15 step:14542 [D loss: 0.466620, acc.: 75.00%] [G loss: 1.066829]\n",
      "epoch:15 step:14543 [D loss: 0.512874, acc.: 78.91%] [G loss: 1.143691]\n",
      "epoch:15 step:14544 [D loss: 0.659499, acc.: 60.94%] [G loss: 1.316948]\n",
      "epoch:15 step:14545 [D loss: 0.420936, acc.: 85.16%] [G loss: 1.598716]\n",
      "epoch:15 step:14546 [D loss: 0.466226, acc.: 80.47%] [G loss: 1.332698]\n",
      "epoch:15 step:14547 [D loss: 0.593729, acc.: 66.41%] [G loss: 1.463673]\n",
      "epoch:15 step:14548 [D loss: 0.588949, acc.: 71.09%] [G loss: 1.370788]\n",
      "epoch:15 step:14549 [D loss: 0.650513, acc.: 61.72%] [G loss: 1.179234]\n",
      "epoch:15 step:14550 [D loss: 0.661873, acc.: 57.03%] [G loss: 1.388866]\n",
      "epoch:15 step:14551 [D loss: 0.547331, acc.: 70.31%] [G loss: 1.429225]\n",
      "epoch:15 step:14552 [D loss: 0.460079, acc.: 84.38%] [G loss: 1.566268]\n",
      "epoch:15 step:14553 [D loss: 0.522581, acc.: 73.44%] [G loss: 1.750037]\n",
      "epoch:15 step:14554 [D loss: 0.674020, acc.: 60.94%] [G loss: 0.953340]\n",
      "epoch:15 step:14555 [D loss: 0.605989, acc.: 67.19%] [G loss: 1.211591]\n",
      "epoch:15 step:14556 [D loss: 0.524722, acc.: 75.78%] [G loss: 1.425691]\n",
      "epoch:15 step:14557 [D loss: 0.528990, acc.: 73.44%] [G loss: 1.301596]\n",
      "epoch:15 step:14558 [D loss: 0.586190, acc.: 70.31%] [G loss: 1.103582]\n",
      "epoch:15 step:14559 [D loss: 0.633266, acc.: 66.41%] [G loss: 1.444452]\n",
      "epoch:15 step:14560 [D loss: 0.375971, acc.: 89.84%] [G loss: 1.823635]\n",
      "epoch:15 step:14561 [D loss: 0.547908, acc.: 69.53%] [G loss: 1.270646]\n",
      "epoch:15 step:14562 [D loss: 0.760964, acc.: 53.12%] [G loss: 1.289440]\n",
      "epoch:15 step:14563 [D loss: 0.612336, acc.: 71.88%] [G loss: 1.386656]\n",
      "epoch:15 step:14564 [D loss: 0.598780, acc.: 72.66%] [G loss: 1.453963]\n",
      "epoch:15 step:14565 [D loss: 0.635447, acc.: 57.03%] [G loss: 1.159431]\n",
      "epoch:15 step:14566 [D loss: 0.655015, acc.: 64.06%] [G loss: 1.155793]\n",
      "epoch:15 step:14567 [D loss: 0.649779, acc.: 64.84%] [G loss: 1.123307]\n",
      "epoch:15 step:14568 [D loss: 0.453760, acc.: 83.59%] [G loss: 1.253771]\n",
      "epoch:15 step:14569 [D loss: 0.664451, acc.: 60.94%] [G loss: 1.215060]\n",
      "epoch:15 step:14570 [D loss: 0.624635, acc.: 64.06%] [G loss: 1.354042]\n",
      "epoch:15 step:14571 [D loss: 0.563422, acc.: 71.09%] [G loss: 1.338129]\n",
      "epoch:15 step:14572 [D loss: 0.613969, acc.: 67.97%] [G loss: 1.160720]\n",
      "epoch:15 step:14573 [D loss: 0.487459, acc.: 78.12%] [G loss: 1.294473]\n",
      "epoch:15 step:14574 [D loss: 0.589930, acc.: 72.66%] [G loss: 1.241142]\n",
      "epoch:15 step:14575 [D loss: 0.591716, acc.: 67.19%] [G loss: 1.166720]\n",
      "epoch:15 step:14576 [D loss: 0.648417, acc.: 62.50%] [G loss: 1.194517]\n",
      "epoch:15 step:14577 [D loss: 0.585974, acc.: 67.97%] [G loss: 1.315134]\n",
      "epoch:15 step:14578 [D loss: 0.521031, acc.: 71.88%] [G loss: 1.465654]\n",
      "epoch:15 step:14579 [D loss: 0.596405, acc.: 71.88%] [G loss: 1.581646]\n",
      "epoch:15 step:14580 [D loss: 0.638754, acc.: 63.28%] [G loss: 1.460262]\n",
      "epoch:15 step:14581 [D loss: 0.665978, acc.: 63.28%] [G loss: 1.208324]\n",
      "epoch:15 step:14582 [D loss: 0.572074, acc.: 69.53%] [G loss: 1.198117]\n",
      "epoch:15 step:14583 [D loss: 0.591907, acc.: 65.62%] [G loss: 1.192005]\n",
      "epoch:15 step:14584 [D loss: 0.544615, acc.: 73.44%] [G loss: 1.605359]\n",
      "epoch:15 step:14585 [D loss: 0.624450, acc.: 66.41%] [G loss: 1.556790]\n",
      "epoch:15 step:14586 [D loss: 0.587803, acc.: 70.31%] [G loss: 0.970968]\n",
      "epoch:15 step:14587 [D loss: 0.612733, acc.: 66.41%] [G loss: 1.125402]\n",
      "epoch:15 step:14588 [D loss: 0.692897, acc.: 60.16%] [G loss: 1.310090]\n",
      "epoch:15 step:14589 [D loss: 0.737167, acc.: 56.25%] [G loss: 1.111874]\n",
      "epoch:15 step:14590 [D loss: 0.600973, acc.: 67.19%] [G loss: 1.435942]\n",
      "epoch:15 step:14591 [D loss: 0.678691, acc.: 63.28%] [G loss: 1.190037]\n",
      "epoch:15 step:14592 [D loss: 0.477459, acc.: 79.69%] [G loss: 1.260947]\n",
      "epoch:15 step:14593 [D loss: 0.449184, acc.: 86.72%] [G loss: 1.159119]\n",
      "epoch:15 step:14594 [D loss: 0.555135, acc.: 77.34%] [G loss: 1.234493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14595 [D loss: 0.606675, acc.: 65.62%] [G loss: 1.401553]\n",
      "epoch:15 step:14596 [D loss: 0.538139, acc.: 71.88%] [G loss: 1.434391]\n",
      "epoch:15 step:14597 [D loss: 0.510177, acc.: 78.91%] [G loss: 1.384580]\n",
      "epoch:15 step:14598 [D loss: 0.596983, acc.: 67.19%] [G loss: 1.134970]\n",
      "epoch:15 step:14599 [D loss: 0.594521, acc.: 69.53%] [G loss: 1.061637]\n",
      "epoch:15 step:14600 [D loss: 0.618239, acc.: 68.75%] [G loss: 1.224108]\n",
      "##############\n",
      "[2.6733779  1.85279944 2.0279215  2.9246395  0.70746461 6.10878896\n",
      " 1.99426839 2.53739503 3.8683651  5.58858635]\n",
      "##########\n",
      "epoch:15 step:14601 [D loss: 0.495259, acc.: 73.44%] [G loss: 1.333444]\n",
      "epoch:15 step:14602 [D loss: 0.638516, acc.: 66.41%] [G loss: 1.368411]\n",
      "epoch:15 step:14603 [D loss: 0.669768, acc.: 62.50%] [G loss: 1.202170]\n",
      "epoch:15 step:14604 [D loss: 0.442385, acc.: 82.81%] [G loss: 1.464340]\n",
      "epoch:15 step:14605 [D loss: 0.650250, acc.: 65.62%] [G loss: 1.407783]\n",
      "epoch:15 step:14606 [D loss: 0.506779, acc.: 78.12%] [G loss: 1.410929]\n",
      "epoch:15 step:14607 [D loss: 0.663672, acc.: 57.81%] [G loss: 1.223441]\n",
      "epoch:15 step:14608 [D loss: 0.490414, acc.: 77.34%] [G loss: 1.302368]\n",
      "epoch:15 step:14609 [D loss: 0.705018, acc.: 57.03%] [G loss: 0.957335]\n",
      "epoch:15 step:14610 [D loss: 0.603952, acc.: 67.19%] [G loss: 0.929333]\n",
      "epoch:15 step:14611 [D loss: 0.700431, acc.: 61.72%] [G loss: 1.034522]\n",
      "epoch:15 step:14612 [D loss: 0.556907, acc.: 75.00%] [G loss: 1.297069]\n",
      "epoch:15 step:14613 [D loss: 0.572097, acc.: 69.53%] [G loss: 1.162352]\n",
      "epoch:15 step:14614 [D loss: 0.619538, acc.: 64.84%] [G loss: 1.184676]\n",
      "epoch:15 step:14615 [D loss: 0.637371, acc.: 66.41%] [G loss: 1.306495]\n",
      "epoch:15 step:14616 [D loss: 0.631411, acc.: 58.59%] [G loss: 1.115535]\n",
      "epoch:15 step:14617 [D loss: 0.703720, acc.: 56.25%] [G loss: 1.166912]\n",
      "epoch:15 step:14618 [D loss: 0.464513, acc.: 82.03%] [G loss: 1.329172]\n",
      "epoch:15 step:14619 [D loss: 0.558117, acc.: 69.53%] [G loss: 1.322913]\n",
      "epoch:15 step:14620 [D loss: 0.667307, acc.: 58.59%] [G loss: 1.290131]\n",
      "epoch:15 step:14621 [D loss: 0.620821, acc.: 61.72%] [G loss: 1.019133]\n",
      "epoch:15 step:14622 [D loss: 0.547733, acc.: 71.09%] [G loss: 1.387262]\n",
      "epoch:15 step:14623 [D loss: 0.812932, acc.: 48.44%] [G loss: 0.921565]\n",
      "epoch:15 step:14624 [D loss: 0.527554, acc.: 75.00%] [G loss: 1.551376]\n",
      "epoch:15 step:14625 [D loss: 0.662832, acc.: 60.94%] [G loss: 1.107668]\n",
      "epoch:15 step:14626 [D loss: 0.610293, acc.: 67.19%] [G loss: 1.227354]\n",
      "epoch:15 step:14627 [D loss: 0.494475, acc.: 77.34%] [G loss: 1.349379]\n",
      "epoch:15 step:14628 [D loss: 0.716971, acc.: 55.47%] [G loss: 1.155975]\n",
      "epoch:15 step:14629 [D loss: 0.449654, acc.: 81.25%] [G loss: 1.111927]\n",
      "epoch:15 step:14630 [D loss: 0.471890, acc.: 80.47%] [G loss: 1.126488]\n",
      "epoch:15 step:14631 [D loss: 0.518820, acc.: 78.91%] [G loss: 1.345881]\n",
      "epoch:15 step:14632 [D loss: 0.562479, acc.: 69.53%] [G loss: 1.358588]\n",
      "epoch:15 step:14633 [D loss: 0.542834, acc.: 68.75%] [G loss: 1.500810]\n",
      "epoch:15 step:14634 [D loss: 0.691853, acc.: 58.59%] [G loss: 1.272263]\n",
      "epoch:15 step:14635 [D loss: 0.491603, acc.: 78.12%] [G loss: 1.294535]\n",
      "epoch:15 step:14636 [D loss: 0.436630, acc.: 78.12%] [G loss: 1.476981]\n",
      "epoch:15 step:14637 [D loss: 0.640806, acc.: 64.06%] [G loss: 1.329807]\n",
      "epoch:15 step:14638 [D loss: 0.657925, acc.: 58.59%] [G loss: 1.290618]\n",
      "epoch:15 step:14639 [D loss: 0.533677, acc.: 74.22%] [G loss: 1.145201]\n",
      "epoch:15 step:14640 [D loss: 0.573377, acc.: 68.75%] [G loss: 1.374982]\n",
      "epoch:15 step:14641 [D loss: 0.555689, acc.: 71.09%] [G loss: 1.349510]\n",
      "epoch:15 step:14642 [D loss: 0.587496, acc.: 67.97%] [G loss: 1.243885]\n",
      "epoch:15 step:14643 [D loss: 0.544992, acc.: 75.00%] [G loss: 1.291951]\n",
      "epoch:15 step:14644 [D loss: 0.672849, acc.: 60.16%] [G loss: 1.130259]\n",
      "epoch:15 step:14645 [D loss: 0.474372, acc.: 77.34%] [G loss: 1.219029]\n",
      "epoch:15 step:14646 [D loss: 0.511089, acc.: 78.91%] [G loss: 1.160553]\n",
      "epoch:15 step:14647 [D loss: 0.472713, acc.: 80.47%] [G loss: 1.352999]\n",
      "epoch:15 step:14648 [D loss: 0.687285, acc.: 58.59%] [G loss: 1.364110]\n",
      "epoch:15 step:14649 [D loss: 0.642861, acc.: 60.94%] [G loss: 1.397409]\n",
      "epoch:15 step:14650 [D loss: 0.494695, acc.: 74.22%] [G loss: 1.383787]\n",
      "epoch:15 step:14651 [D loss: 0.609830, acc.: 71.09%] [G loss: 1.042965]\n",
      "epoch:15 step:14652 [D loss: 0.662328, acc.: 60.16%] [G loss: 1.105807]\n",
      "epoch:15 step:14653 [D loss: 0.715495, acc.: 60.94%] [G loss: 1.385255]\n",
      "epoch:15 step:14654 [D loss: 0.492033, acc.: 78.91%] [G loss: 1.256320]\n",
      "epoch:15 step:14655 [D loss: 0.499467, acc.: 75.00%] [G loss: 1.220240]\n",
      "epoch:15 step:14656 [D loss: 0.657596, acc.: 63.28%] [G loss: 1.374329]\n",
      "epoch:15 step:14657 [D loss: 0.442755, acc.: 84.38%] [G loss: 1.465702]\n",
      "epoch:15 step:14658 [D loss: 0.616989, acc.: 63.28%] [G loss: 1.351569]\n",
      "epoch:15 step:14659 [D loss: 0.586047, acc.: 70.31%] [G loss: 1.242420]\n",
      "epoch:15 step:14660 [D loss: 0.573768, acc.: 71.09%] [G loss: 1.321493]\n",
      "epoch:15 step:14661 [D loss: 0.644831, acc.: 66.41%] [G loss: 1.129380]\n",
      "epoch:15 step:14662 [D loss: 0.472738, acc.: 75.78%] [G loss: 1.435207]\n",
      "epoch:15 step:14663 [D loss: 0.593361, acc.: 72.66%] [G loss: 1.146325]\n",
      "epoch:15 step:14664 [D loss: 0.592304, acc.: 68.75%] [G loss: 1.253548]\n",
      "epoch:15 step:14665 [D loss: 0.734556, acc.: 54.69%] [G loss: 1.221111]\n",
      "epoch:15 step:14666 [D loss: 0.468009, acc.: 78.91%] [G loss: 1.467991]\n",
      "epoch:15 step:14667 [D loss: 0.569037, acc.: 70.31%] [G loss: 1.342788]\n",
      "epoch:15 step:14668 [D loss: 0.545250, acc.: 73.44%] [G loss: 1.522130]\n",
      "epoch:15 step:14669 [D loss: 0.575725, acc.: 69.53%] [G loss: 0.998199]\n",
      "epoch:15 step:14670 [D loss: 0.552139, acc.: 71.88%] [G loss: 1.278845]\n",
      "epoch:15 step:14671 [D loss: 0.695951, acc.: 58.59%] [G loss: 1.300803]\n",
      "epoch:15 step:14672 [D loss: 0.522279, acc.: 75.78%] [G loss: 1.431635]\n",
      "epoch:15 step:14673 [D loss: 0.476286, acc.: 76.56%] [G loss: 1.313582]\n",
      "epoch:15 step:14674 [D loss: 0.693621, acc.: 55.47%] [G loss: 1.253065]\n",
      "epoch:15 step:14675 [D loss: 0.666544, acc.: 59.38%] [G loss: 1.136257]\n",
      "epoch:15 step:14676 [D loss: 0.721726, acc.: 59.38%] [G loss: 1.471460]\n",
      "epoch:15 step:14677 [D loss: 0.620616, acc.: 66.41%] [G loss: 1.148949]\n",
      "epoch:15 step:14678 [D loss: 0.615934, acc.: 63.28%] [G loss: 1.282135]\n",
      "epoch:15 step:14679 [D loss: 0.644050, acc.: 67.19%] [G loss: 1.140604]\n",
      "epoch:15 step:14680 [D loss: 0.512215, acc.: 76.56%] [G loss: 1.611839]\n",
      "epoch:15 step:14681 [D loss: 0.523578, acc.: 75.00%] [G loss: 1.217692]\n",
      "epoch:15 step:14682 [D loss: 0.582555, acc.: 67.97%] [G loss: 1.164750]\n",
      "epoch:15 step:14683 [D loss: 0.521794, acc.: 75.78%] [G loss: 1.389587]\n",
      "epoch:15 step:14684 [D loss: 0.689259, acc.: 58.59%] [G loss: 0.971010]\n",
      "epoch:15 step:14685 [D loss: 0.597843, acc.: 65.62%] [G loss: 1.267053]\n",
      "epoch:15 step:14686 [D loss: 0.588894, acc.: 66.41%] [G loss: 1.062217]\n",
      "epoch:15 step:14687 [D loss: 0.563470, acc.: 74.22%] [G loss: 1.198024]\n",
      "epoch:15 step:14688 [D loss: 0.451353, acc.: 78.12%] [G loss: 1.807787]\n",
      "epoch:15 step:14689 [D loss: 0.702192, acc.: 57.03%] [G loss: 1.270587]\n",
      "epoch:15 step:14690 [D loss: 0.716493, acc.: 57.81%] [G loss: 1.219339]\n",
      "epoch:15 step:14691 [D loss: 0.462802, acc.: 77.34%] [G loss: 1.606314]\n",
      "epoch:15 step:14692 [D loss: 0.561029, acc.: 76.56%] [G loss: 1.327081]\n",
      "epoch:15 step:14693 [D loss: 0.480626, acc.: 78.91%] [G loss: 1.539535]\n",
      "epoch:15 step:14694 [D loss: 0.612023, acc.: 66.41%] [G loss: 1.482985]\n",
      "epoch:15 step:14695 [D loss: 0.622838, acc.: 64.84%] [G loss: 1.194418]\n",
      "epoch:15 step:14696 [D loss: 0.639828, acc.: 60.16%] [G loss: 1.198791]\n",
      "epoch:15 step:14697 [D loss: 0.611323, acc.: 67.97%] [G loss: 1.148275]\n",
      "epoch:15 step:14698 [D loss: 0.603998, acc.: 65.62%] [G loss: 1.075840]\n",
      "epoch:15 step:14699 [D loss: 0.465020, acc.: 78.12%] [G loss: 0.982853]\n",
      "epoch:15 step:14700 [D loss: 0.473652, acc.: 82.81%] [G loss: 1.084533]\n",
      "epoch:15 step:14701 [D loss: 0.534898, acc.: 69.53%] [G loss: 1.472267]\n",
      "epoch:15 step:14702 [D loss: 0.651852, acc.: 68.75%] [G loss: 1.316633]\n",
      "epoch:15 step:14703 [D loss: 0.583826, acc.: 72.66%] [G loss: 1.486842]\n",
      "epoch:15 step:14704 [D loss: 0.633570, acc.: 64.84%] [G loss: 1.490544]\n",
      "epoch:15 step:14705 [D loss: 0.621596, acc.: 69.53%] [G loss: 1.492258]\n",
      "epoch:15 step:14706 [D loss: 0.638923, acc.: 63.28%] [G loss: 1.082830]\n",
      "epoch:15 step:14707 [D loss: 0.667504, acc.: 59.38%] [G loss: 1.090574]\n",
      "epoch:15 step:14708 [D loss: 0.531531, acc.: 79.69%] [G loss: 1.470386]\n",
      "epoch:15 step:14709 [D loss: 0.623332, acc.: 66.41%] [G loss: 1.052922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14710 [D loss: 0.636787, acc.: 61.72%] [G loss: 1.183395]\n",
      "epoch:15 step:14711 [D loss: 0.444303, acc.: 78.12%] [G loss: 1.360211]\n",
      "epoch:15 step:14712 [D loss: 0.520371, acc.: 73.44%] [G loss: 1.364923]\n",
      "epoch:15 step:14713 [D loss: 0.571786, acc.: 67.19%] [G loss: 1.149773]\n",
      "epoch:15 step:14714 [D loss: 0.696777, acc.: 55.47%] [G loss: 1.014343]\n",
      "epoch:15 step:14715 [D loss: 0.608365, acc.: 67.97%] [G loss: 1.348605]\n",
      "epoch:15 step:14716 [D loss: 0.551216, acc.: 72.66%] [G loss: 1.233535]\n",
      "epoch:15 step:14717 [D loss: 0.714882, acc.: 57.03%] [G loss: 1.173133]\n",
      "epoch:15 step:14718 [D loss: 0.549251, acc.: 71.88%] [G loss: 1.394834]\n",
      "epoch:15 step:14719 [D loss: 0.554737, acc.: 71.88%] [G loss: 1.165076]\n",
      "epoch:15 step:14720 [D loss: 0.653153, acc.: 60.16%] [G loss: 1.015436]\n",
      "epoch:15 step:14721 [D loss: 0.559747, acc.: 75.00%] [G loss: 1.197373]\n",
      "epoch:15 step:14722 [D loss: 0.725208, acc.: 57.81%] [G loss: 1.293471]\n",
      "epoch:15 step:14723 [D loss: 0.694374, acc.: 53.12%] [G loss: 1.291647]\n",
      "epoch:15 step:14724 [D loss: 0.537199, acc.: 74.22%] [G loss: 1.294939]\n",
      "epoch:15 step:14725 [D loss: 0.572679, acc.: 71.09%] [G loss: 1.339518]\n",
      "epoch:15 step:14726 [D loss: 0.595589, acc.: 66.41%] [G loss: 1.071545]\n",
      "epoch:15 step:14727 [D loss: 0.518319, acc.: 74.22%] [G loss: 1.303569]\n",
      "epoch:15 step:14728 [D loss: 0.640571, acc.: 65.62%] [G loss: 1.336048]\n",
      "epoch:15 step:14729 [D loss: 0.459526, acc.: 80.47%] [G loss: 1.407844]\n",
      "epoch:15 step:14730 [D loss: 0.582323, acc.: 69.53%] [G loss: 1.093876]\n",
      "epoch:15 step:14731 [D loss: 0.518041, acc.: 78.12%] [G loss: 1.430100]\n",
      "epoch:15 step:14732 [D loss: 0.535861, acc.: 68.75%] [G loss: 1.244822]\n",
      "epoch:15 step:14733 [D loss: 0.683612, acc.: 53.91%] [G loss: 1.150584]\n",
      "epoch:15 step:14734 [D loss: 0.607158, acc.: 70.31%] [G loss: 1.057412]\n",
      "epoch:15 step:14735 [D loss: 0.700975, acc.: 58.59%] [G loss: 1.410463]\n",
      "epoch:15 step:14736 [D loss: 0.585121, acc.: 65.62%] [G loss: 1.164643]\n",
      "epoch:15 step:14737 [D loss: 0.653314, acc.: 65.62%] [G loss: 1.073236]\n",
      "epoch:15 step:14738 [D loss: 0.649348, acc.: 61.72%] [G loss: 1.239699]\n",
      "epoch:15 step:14739 [D loss: 0.583170, acc.: 68.75%] [G loss: 1.516080]\n",
      "epoch:15 step:14740 [D loss: 0.602142, acc.: 67.97%] [G loss: 1.193560]\n",
      "epoch:15 step:14741 [D loss: 0.659649, acc.: 57.81%] [G loss: 1.076963]\n",
      "epoch:15 step:14742 [D loss: 0.474409, acc.: 78.12%] [G loss: 1.361968]\n",
      "epoch:15 step:14743 [D loss: 0.638113, acc.: 65.62%] [G loss: 1.184139]\n",
      "epoch:15 step:14744 [D loss: 0.619933, acc.: 65.62%] [G loss: 0.831752]\n",
      "epoch:15 step:14745 [D loss: 0.583004, acc.: 70.31%] [G loss: 1.267476]\n",
      "epoch:15 step:14746 [D loss: 0.472253, acc.: 77.34%] [G loss: 1.276769]\n",
      "epoch:15 step:14747 [D loss: 0.569512, acc.: 68.75%] [G loss: 1.050793]\n",
      "epoch:15 step:14748 [D loss: 0.535077, acc.: 71.09%] [G loss: 1.352726]\n",
      "epoch:15 step:14749 [D loss: 0.596886, acc.: 65.62%] [G loss: 1.286620]\n",
      "epoch:15 step:14750 [D loss: 0.496213, acc.: 78.12%] [G loss: 1.185250]\n",
      "epoch:15 step:14751 [D loss: 0.531970, acc.: 75.78%] [G loss: 1.334007]\n",
      "epoch:15 step:14752 [D loss: 0.398581, acc.: 86.72%] [G loss: 1.391984]\n",
      "epoch:15 step:14753 [D loss: 0.533644, acc.: 74.22%] [G loss: 1.591789]\n",
      "epoch:15 step:14754 [D loss: 0.565298, acc.: 69.53%] [G loss: 1.246483]\n",
      "epoch:15 step:14755 [D loss: 0.591753, acc.: 67.19%] [G loss: 1.002105]\n",
      "epoch:15 step:14756 [D loss: 0.443625, acc.: 81.25%] [G loss: 1.363910]\n",
      "epoch:15 step:14757 [D loss: 0.411716, acc.: 84.38%] [G loss: 1.492370]\n",
      "epoch:15 step:14758 [D loss: 0.601152, acc.: 71.09%] [G loss: 1.470169]\n",
      "epoch:15 step:14759 [D loss: 0.723568, acc.: 56.25%] [G loss: 1.271604]\n",
      "epoch:15 step:14760 [D loss: 0.531083, acc.: 67.97%] [G loss: 1.210407]\n",
      "epoch:15 step:14761 [D loss: 0.625930, acc.: 65.62%] [G loss: 1.077358]\n",
      "epoch:15 step:14762 [D loss: 0.689318, acc.: 62.50%] [G loss: 1.178884]\n",
      "epoch:15 step:14763 [D loss: 0.421035, acc.: 84.38%] [G loss: 1.623044]\n",
      "epoch:15 step:14764 [D loss: 0.580556, acc.: 67.19%] [G loss: 1.371840]\n",
      "epoch:15 step:14765 [D loss: 0.579902, acc.: 68.75%] [G loss: 1.243496]\n",
      "epoch:15 step:14766 [D loss: 0.418996, acc.: 83.59%] [G loss: 1.200469]\n",
      "epoch:15 step:14767 [D loss: 0.478442, acc.: 77.34%] [G loss: 1.289778]\n",
      "epoch:15 step:14768 [D loss: 0.450545, acc.: 82.81%] [G loss: 1.254968]\n",
      "epoch:15 step:14769 [D loss: 0.616680, acc.: 67.19%] [G loss: 1.227348]\n",
      "epoch:15 step:14770 [D loss: 0.568090, acc.: 72.66%] [G loss: 1.055937]\n",
      "epoch:15 step:14771 [D loss: 0.593623, acc.: 71.09%] [G loss: 1.176057]\n",
      "epoch:15 step:14772 [D loss: 0.440582, acc.: 82.03%] [G loss: 0.930283]\n",
      "epoch:15 step:14773 [D loss: 0.447330, acc.: 80.47%] [G loss: 1.098288]\n",
      "epoch:15 step:14774 [D loss: 0.497363, acc.: 74.22%] [G loss: 1.229383]\n",
      "epoch:15 step:14775 [D loss: 0.650197, acc.: 67.97%] [G loss: 1.397263]\n",
      "epoch:15 step:14776 [D loss: 0.575516, acc.: 70.31%] [G loss: 0.921516]\n",
      "epoch:15 step:14777 [D loss: 0.640335, acc.: 64.06%] [G loss: 1.031509]\n",
      "epoch:15 step:14778 [D loss: 0.569149, acc.: 71.88%] [G loss: 1.078887]\n",
      "epoch:15 step:14779 [D loss: 0.517577, acc.: 78.91%] [G loss: 1.160067]\n",
      "epoch:15 step:14780 [D loss: 0.651713, acc.: 65.62%] [G loss: 0.963527]\n",
      "epoch:15 step:14781 [D loss: 0.561444, acc.: 65.62%] [G loss: 1.318075]\n",
      "epoch:15 step:14782 [D loss: 0.511765, acc.: 81.25%] [G loss: 1.397198]\n",
      "epoch:15 step:14783 [D loss: 0.721546, acc.: 56.25%] [G loss: 1.170952]\n",
      "epoch:15 step:14784 [D loss: 0.497585, acc.: 78.12%] [G loss: 1.158664]\n",
      "epoch:15 step:14785 [D loss: 0.727407, acc.: 60.94%] [G loss: 1.542999]\n",
      "epoch:15 step:14786 [D loss: 0.583146, acc.: 68.75%] [G loss: 1.118267]\n",
      "epoch:15 step:14787 [D loss: 0.552028, acc.: 70.31%] [G loss: 1.453130]\n",
      "epoch:15 step:14788 [D loss: 0.597013, acc.: 67.19%] [G loss: 1.306363]\n",
      "epoch:15 step:14789 [D loss: 0.492655, acc.: 82.03%] [G loss: 1.433719]\n",
      "epoch:15 step:14790 [D loss: 0.586759, acc.: 70.31%] [G loss: 1.596476]\n",
      "epoch:15 step:14791 [D loss: 0.634206, acc.: 67.97%] [G loss: 1.029444]\n",
      "epoch:15 step:14792 [D loss: 0.707362, acc.: 59.38%] [G loss: 1.417990]\n",
      "epoch:15 step:14793 [D loss: 0.565473, acc.: 71.09%] [G loss: 1.164823]\n",
      "epoch:15 step:14794 [D loss: 0.703833, acc.: 58.59%] [G loss: 0.995671]\n",
      "epoch:15 step:14795 [D loss: 0.558616, acc.: 71.09%] [G loss: 1.524705]\n",
      "epoch:15 step:14796 [D loss: 0.612054, acc.: 67.19%] [G loss: 1.201712]\n",
      "epoch:15 step:14797 [D loss: 0.568690, acc.: 75.00%] [G loss: 1.154168]\n",
      "epoch:15 step:14798 [D loss: 0.594503, acc.: 70.31%] [G loss: 1.079509]\n",
      "epoch:15 step:14799 [D loss: 0.507819, acc.: 75.00%] [G loss: 1.149924]\n",
      "epoch:15 step:14800 [D loss: 0.489129, acc.: 78.12%] [G loss: 1.297103]\n",
      "##############\n",
      "[2.66353571 1.8964863  1.66824545 2.37095376 0.80097893 5.1820275\n",
      " 2.04798398 2.70408217 3.84960665 7.14868929]\n",
      "##########\n",
      "epoch:15 step:14801 [D loss: 0.464063, acc.: 81.25%] [G loss: 1.450212]\n",
      "epoch:15 step:14802 [D loss: 0.506899, acc.: 77.34%] [G loss: 1.407904]\n",
      "epoch:15 step:14803 [D loss: 0.553908, acc.: 71.09%] [G loss: 1.125101]\n",
      "epoch:15 step:14804 [D loss: 0.568242, acc.: 69.53%] [G loss: 1.303886]\n",
      "epoch:15 step:14805 [D loss: 0.516386, acc.: 77.34%] [G loss: 1.302495]\n",
      "epoch:15 step:14806 [D loss: 0.633567, acc.: 61.72%] [G loss: 1.149299]\n",
      "epoch:15 step:14807 [D loss: 0.540686, acc.: 76.56%] [G loss: 1.418472]\n",
      "epoch:15 step:14808 [D loss: 0.429652, acc.: 85.94%] [G loss: 1.505425]\n",
      "epoch:15 step:14809 [D loss: 0.576927, acc.: 75.00%] [G loss: 1.135373]\n",
      "epoch:15 step:14810 [D loss: 0.603699, acc.: 65.62%] [G loss: 1.182367]\n",
      "epoch:15 step:14811 [D loss: 0.551740, acc.: 69.53%] [G loss: 1.242083]\n",
      "epoch:15 step:14812 [D loss: 0.536055, acc.: 75.78%] [G loss: 1.250893]\n",
      "epoch:15 step:14813 [D loss: 0.575046, acc.: 65.62%] [G loss: 1.353031]\n",
      "epoch:15 step:14814 [D loss: 0.446693, acc.: 81.25%] [G loss: 1.125284]\n",
      "epoch:15 step:14815 [D loss: 0.602374, acc.: 71.88%] [G loss: 1.166776]\n",
      "epoch:15 step:14816 [D loss: 0.734900, acc.: 51.56%] [G loss: 1.165402]\n",
      "epoch:15 step:14817 [D loss: 0.614562, acc.: 64.06%] [G loss: 1.238174]\n",
      "epoch:15 step:14818 [D loss: 0.600250, acc.: 74.22%] [G loss: 1.178009]\n",
      "epoch:15 step:14819 [D loss: 0.438067, acc.: 85.16%] [G loss: 1.300241]\n",
      "epoch:15 step:14820 [D loss: 0.560571, acc.: 72.66%] [G loss: 1.099036]\n",
      "epoch:15 step:14821 [D loss: 0.554873, acc.: 67.97%] [G loss: 1.169142]\n",
      "epoch:15 step:14822 [D loss: 0.547428, acc.: 72.66%] [G loss: 1.286577]\n",
      "epoch:15 step:14823 [D loss: 0.619579, acc.: 64.84%] [G loss: 1.234752]\n",
      "epoch:15 step:14824 [D loss: 0.799511, acc.: 48.44%] [G loss: 1.135865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14825 [D loss: 0.540340, acc.: 74.22%] [G loss: 1.370844]\n",
      "epoch:15 step:14826 [D loss: 0.515956, acc.: 79.69%] [G loss: 1.246719]\n",
      "epoch:15 step:14827 [D loss: 0.451824, acc.: 81.25%] [G loss: 1.419569]\n",
      "epoch:15 step:14828 [D loss: 0.687810, acc.: 57.03%] [G loss: 1.121836]\n",
      "epoch:15 step:14829 [D loss: 0.474787, acc.: 78.91%] [G loss: 1.206642]\n",
      "epoch:15 step:14830 [D loss: 0.560840, acc.: 70.31%] [G loss: 1.493266]\n",
      "epoch:15 step:14831 [D loss: 0.684406, acc.: 63.28%] [G loss: 1.280376]\n",
      "epoch:15 step:14832 [D loss: 0.571065, acc.: 71.88%] [G loss: 1.358357]\n",
      "epoch:15 step:14833 [D loss: 0.676568, acc.: 57.81%] [G loss: 1.260985]\n",
      "epoch:15 step:14834 [D loss: 0.642312, acc.: 61.72%] [G loss: 1.140727]\n",
      "epoch:15 step:14835 [D loss: 0.553229, acc.: 67.19%] [G loss: 1.200365]\n",
      "epoch:15 step:14836 [D loss: 0.508184, acc.: 81.25%] [G loss: 1.294067]\n",
      "epoch:15 step:14837 [D loss: 0.564897, acc.: 71.09%] [G loss: 0.994540]\n",
      "epoch:15 step:14838 [D loss: 0.480024, acc.: 78.91%] [G loss: 1.446242]\n",
      "epoch:15 step:14839 [D loss: 0.693886, acc.: 61.72%] [G loss: 1.072510]\n",
      "epoch:15 step:14840 [D loss: 0.574739, acc.: 65.62%] [G loss: 1.267299]\n",
      "epoch:15 step:14841 [D loss: 0.613350, acc.: 66.41%] [G loss: 1.186665]\n",
      "epoch:15 step:14842 [D loss: 0.560787, acc.: 71.88%] [G loss: 1.304133]\n",
      "epoch:15 step:14843 [D loss: 0.493334, acc.: 80.47%] [G loss: 1.332780]\n",
      "epoch:15 step:14844 [D loss: 0.531906, acc.: 73.44%] [G loss: 0.966168]\n",
      "epoch:15 step:14845 [D loss: 0.602460, acc.: 62.50%] [G loss: 1.103674]\n",
      "epoch:15 step:14846 [D loss: 0.600455, acc.: 62.50%] [G loss: 1.533798]\n",
      "epoch:15 step:14847 [D loss: 0.498467, acc.: 77.34%] [G loss: 1.276446]\n",
      "epoch:15 step:14848 [D loss: 0.684839, acc.: 54.69%] [G loss: 1.430273]\n",
      "epoch:15 step:14849 [D loss: 0.476174, acc.: 78.91%] [G loss: 1.369610]\n",
      "epoch:15 step:14850 [D loss: 0.646178, acc.: 64.84%] [G loss: 1.398684]\n",
      "epoch:15 step:14851 [D loss: 0.707053, acc.: 55.47%] [G loss: 1.449705]\n",
      "epoch:15 step:14852 [D loss: 0.611317, acc.: 66.41%] [G loss: 1.015605]\n",
      "epoch:15 step:14853 [D loss: 0.546927, acc.: 75.00%] [G loss: 1.110122]\n",
      "epoch:15 step:14854 [D loss: 0.449920, acc.: 82.03%] [G loss: 1.417885]\n",
      "epoch:15 step:14855 [D loss: 0.572370, acc.: 70.31%] [G loss: 1.169175]\n",
      "epoch:15 step:14856 [D loss: 0.601356, acc.: 67.19%] [G loss: 1.398063]\n",
      "epoch:15 step:14857 [D loss: 0.731371, acc.: 53.91%] [G loss: 0.939342]\n",
      "epoch:15 step:14858 [D loss: 0.710977, acc.: 53.91%] [G loss: 1.111190]\n",
      "epoch:15 step:14859 [D loss: 0.509646, acc.: 71.88%] [G loss: 1.418171]\n",
      "epoch:15 step:14860 [D loss: 0.567038, acc.: 70.31%] [G loss: 1.091369]\n",
      "epoch:15 step:14861 [D loss: 0.532527, acc.: 75.78%] [G loss: 1.456437]\n",
      "epoch:15 step:14862 [D loss: 0.526371, acc.: 76.56%] [G loss: 1.251027]\n",
      "epoch:15 step:14863 [D loss: 0.518043, acc.: 74.22%] [G loss: 1.344920]\n",
      "epoch:15 step:14864 [D loss: 0.456739, acc.: 79.69%] [G loss: 1.445614]\n",
      "epoch:15 step:14865 [D loss: 0.595763, acc.: 66.41%] [G loss: 1.417541]\n",
      "epoch:15 step:14866 [D loss: 0.733097, acc.: 54.69%] [G loss: 1.196773]\n",
      "epoch:15 step:14867 [D loss: 0.542938, acc.: 74.22%] [G loss: 1.358939]\n",
      "epoch:15 step:14868 [D loss: 0.587259, acc.: 65.62%] [G loss: 1.283696]\n",
      "epoch:15 step:14869 [D loss: 0.566050, acc.: 70.31%] [G loss: 1.182708]\n",
      "epoch:15 step:14870 [D loss: 0.627455, acc.: 60.94%] [G loss: 1.245788]\n",
      "epoch:15 step:14871 [D loss: 0.544556, acc.: 75.78%] [G loss: 1.270537]\n",
      "epoch:15 step:14872 [D loss: 0.644854, acc.: 61.72%] [G loss: 1.365665]\n",
      "epoch:15 step:14873 [D loss: 0.511153, acc.: 75.78%] [G loss: 1.200989]\n",
      "epoch:15 step:14874 [D loss: 0.582992, acc.: 69.53%] [G loss: 1.178879]\n",
      "epoch:15 step:14875 [D loss: 0.533461, acc.: 74.22%] [G loss: 1.312525]\n",
      "epoch:15 step:14876 [D loss: 0.569506, acc.: 70.31%] [G loss: 1.241276]\n",
      "epoch:15 step:14877 [D loss: 0.577228, acc.: 69.53%] [G loss: 1.230537]\n",
      "epoch:15 step:14878 [D loss: 0.523102, acc.: 78.91%] [G loss: 1.131023]\n",
      "epoch:15 step:14879 [D loss: 0.651995, acc.: 63.28%] [G loss: 1.356314]\n",
      "epoch:15 step:14880 [D loss: 0.609802, acc.: 63.28%] [G loss: 1.082241]\n",
      "epoch:15 step:14881 [D loss: 0.569949, acc.: 70.31%] [G loss: 1.469984]\n",
      "epoch:15 step:14882 [D loss: 0.603154, acc.: 67.97%] [G loss: 1.129312]\n",
      "epoch:15 step:14883 [D loss: 0.735193, acc.: 58.59%] [G loss: 1.009589]\n",
      "epoch:15 step:14884 [D loss: 0.503093, acc.: 80.47%] [G loss: 1.275521]\n",
      "epoch:15 step:14885 [D loss: 0.585461, acc.: 67.19%] [G loss: 1.205739]\n",
      "epoch:15 step:14886 [D loss: 0.719874, acc.: 57.81%] [G loss: 1.172033]\n",
      "epoch:15 step:14887 [D loss: 0.606609, acc.: 63.28%] [G loss: 1.216686]\n",
      "epoch:15 step:14888 [D loss: 0.561109, acc.: 75.00%] [G loss: 1.303413]\n",
      "epoch:15 step:14889 [D loss: 0.553806, acc.: 74.22%] [G loss: 1.179081]\n",
      "epoch:15 step:14890 [D loss: 0.463049, acc.: 76.56%] [G loss: 1.210640]\n",
      "epoch:15 step:14891 [D loss: 0.542731, acc.: 73.44%] [G loss: 1.350980]\n",
      "epoch:15 step:14892 [D loss: 0.624841, acc.: 66.41%] [G loss: 1.222389]\n",
      "epoch:15 step:14893 [D loss: 0.491724, acc.: 80.47%] [G loss: 1.199470]\n",
      "epoch:15 step:14894 [D loss: 0.632899, acc.: 61.72%] [G loss: 1.339481]\n",
      "epoch:15 step:14895 [D loss: 0.484262, acc.: 78.12%] [G loss: 1.252446]\n",
      "epoch:15 step:14896 [D loss: 0.582002, acc.: 67.97%] [G loss: 1.015206]\n",
      "epoch:15 step:14897 [D loss: 0.559527, acc.: 71.09%] [G loss: 1.351803]\n",
      "epoch:15 step:14898 [D loss: 0.563778, acc.: 75.00%] [G loss: 1.256017]\n",
      "epoch:15 step:14899 [D loss: 0.651202, acc.: 60.16%] [G loss: 1.017463]\n",
      "epoch:15 step:14900 [D loss: 0.479478, acc.: 77.34%] [G loss: 1.276081]\n",
      "epoch:15 step:14901 [D loss: 0.588661, acc.: 68.75%] [G loss: 1.228802]\n",
      "epoch:15 step:14902 [D loss: 0.529405, acc.: 75.00%] [G loss: 1.053212]\n",
      "epoch:15 step:14903 [D loss: 0.655784, acc.: 63.28%] [G loss: 1.238188]\n",
      "epoch:15 step:14904 [D loss: 0.473024, acc.: 78.12%] [G loss: 1.319568]\n",
      "epoch:15 step:14905 [D loss: 0.550111, acc.: 75.00%] [G loss: 1.276817]\n",
      "epoch:15 step:14906 [D loss: 0.720450, acc.: 53.12%] [G loss: 1.148235]\n",
      "epoch:15 step:14907 [D loss: 0.468115, acc.: 78.12%] [G loss: 1.318397]\n",
      "epoch:15 step:14908 [D loss: 0.449291, acc.: 80.47%] [G loss: 1.585131]\n",
      "epoch:15 step:14909 [D loss: 0.592287, acc.: 69.53%] [G loss: 1.279792]\n",
      "epoch:15 step:14910 [D loss: 0.581887, acc.: 64.84%] [G loss: 1.177725]\n",
      "epoch:15 step:14911 [D loss: 0.618828, acc.: 63.28%] [G loss: 1.107510]\n",
      "epoch:15 step:14912 [D loss: 0.614943, acc.: 64.06%] [G loss: 1.254576]\n",
      "epoch:15 step:14913 [D loss: 0.524470, acc.: 79.69%] [G loss: 1.257612]\n",
      "epoch:15 step:14914 [D loss: 0.744262, acc.: 52.34%] [G loss: 1.373090]\n",
      "epoch:15 step:14915 [D loss: 0.546772, acc.: 78.12%] [G loss: 1.361605]\n",
      "epoch:15 step:14916 [D loss: 0.622190, acc.: 59.38%] [G loss: 1.388556]\n",
      "epoch:15 step:14917 [D loss: 0.564651, acc.: 70.31%] [G loss: 1.232585]\n",
      "epoch:15 step:14918 [D loss: 0.702818, acc.: 54.69%] [G loss: 1.644684]\n",
      "epoch:15 step:14919 [D loss: 0.587434, acc.: 67.97%] [G loss: 1.162350]\n",
      "epoch:15 step:14920 [D loss: 0.592954, acc.: 64.84%] [G loss: 1.103915]\n",
      "epoch:15 step:14921 [D loss: 0.505463, acc.: 76.56%] [G loss: 1.270536]\n",
      "epoch:15 step:14922 [D loss: 0.630980, acc.: 64.06%] [G loss: 1.118505]\n",
      "epoch:15 step:14923 [D loss: 0.519264, acc.: 75.00%] [G loss: 1.328099]\n",
      "epoch:15 step:14924 [D loss: 0.485767, acc.: 78.12%] [G loss: 1.480615]\n",
      "epoch:15 step:14925 [D loss: 0.499536, acc.: 78.91%] [G loss: 1.379756]\n",
      "epoch:15 step:14926 [D loss: 0.580571, acc.: 71.09%] [G loss: 1.044124]\n",
      "epoch:15 step:14927 [D loss: 0.411571, acc.: 88.28%] [G loss: 1.346210]\n",
      "epoch:15 step:14928 [D loss: 0.453119, acc.: 85.16%] [G loss: 1.251413]\n",
      "epoch:15 step:14929 [D loss: 0.535994, acc.: 74.22%] [G loss: 1.475633]\n",
      "epoch:15 step:14930 [D loss: 0.563961, acc.: 74.22%] [G loss: 1.243723]\n",
      "epoch:15 step:14931 [D loss: 0.552672, acc.: 69.53%] [G loss: 1.564572]\n",
      "epoch:15 step:14932 [D loss: 0.554766, acc.: 74.22%] [G loss: 1.472921]\n",
      "epoch:15 step:14933 [D loss: 0.596269, acc.: 67.97%] [G loss: 1.065691]\n",
      "epoch:15 step:14934 [D loss: 0.633372, acc.: 64.06%] [G loss: 0.944889]\n",
      "epoch:15 step:14935 [D loss: 0.556243, acc.: 72.66%] [G loss: 1.345626]\n",
      "epoch:15 step:14936 [D loss: 0.507111, acc.: 75.00%] [G loss: 1.216070]\n",
      "epoch:15 step:14937 [D loss: 0.532128, acc.: 75.00%] [G loss: 1.171356]\n",
      "epoch:15 step:14938 [D loss: 0.625134, acc.: 66.41%] [G loss: 1.258511]\n",
      "epoch:15 step:14939 [D loss: 0.654607, acc.: 67.19%] [G loss: 1.060159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14940 [D loss: 0.483831, acc.: 78.12%] [G loss: 1.125832]\n",
      "epoch:15 step:14941 [D loss: 0.614784, acc.: 67.97%] [G loss: 1.273815]\n",
      "epoch:15 step:14942 [D loss: 0.495283, acc.: 77.34%] [G loss: 1.702817]\n",
      "epoch:15 step:14943 [D loss: 0.746142, acc.: 48.44%] [G loss: 1.213886]\n",
      "epoch:15 step:14944 [D loss: 0.681338, acc.: 61.72%] [G loss: 1.215959]\n",
      "epoch:15 step:14945 [D loss: 0.577548, acc.: 71.88%] [G loss: 1.448316]\n",
      "epoch:15 step:14946 [D loss: 0.634369, acc.: 64.06%] [G loss: 1.216323]\n",
      "epoch:15 step:14947 [D loss: 0.515549, acc.: 73.44%] [G loss: 1.545624]\n",
      "epoch:15 step:14948 [D loss: 0.541335, acc.: 72.66%] [G loss: 1.334684]\n",
      "epoch:15 step:14949 [D loss: 0.658768, acc.: 64.84%] [G loss: 1.260647]\n",
      "epoch:15 step:14950 [D loss: 0.613302, acc.: 67.19%] [G loss: 1.259334]\n",
      "epoch:15 step:14951 [D loss: 0.727094, acc.: 53.91%] [G loss: 1.180940]\n",
      "epoch:15 step:14952 [D loss: 0.607511, acc.: 66.41%] [G loss: 1.187279]\n",
      "epoch:15 step:14953 [D loss: 0.607315, acc.: 64.84%] [G loss: 1.658814]\n",
      "epoch:15 step:14954 [D loss: 0.466803, acc.: 78.91%] [G loss: 1.571096]\n",
      "epoch:15 step:14955 [D loss: 0.523234, acc.: 75.00%] [G loss: 1.374898]\n",
      "epoch:15 step:14956 [D loss: 0.591347, acc.: 70.31%] [G loss: 1.228128]\n",
      "epoch:15 step:14957 [D loss: 0.561327, acc.: 69.53%] [G loss: 1.072450]\n",
      "epoch:15 step:14958 [D loss: 0.770599, acc.: 49.22%] [G loss: 1.274656]\n",
      "epoch:15 step:14959 [D loss: 0.603553, acc.: 63.28%] [G loss: 1.399584]\n",
      "epoch:15 step:14960 [D loss: 0.539161, acc.: 74.22%] [G loss: 1.402439]\n",
      "epoch:15 step:14961 [D loss: 0.592114, acc.: 65.62%] [G loss: 1.248350]\n",
      "epoch:15 step:14962 [D loss: 0.508638, acc.: 78.12%] [G loss: 1.336966]\n",
      "epoch:15 step:14963 [D loss: 0.685256, acc.: 55.47%] [G loss: 1.298362]\n",
      "epoch:15 step:14964 [D loss: 0.617167, acc.: 62.50%] [G loss: 1.114348]\n",
      "epoch:15 step:14965 [D loss: 0.520955, acc.: 78.91%] [G loss: 1.469324]\n",
      "epoch:15 step:14966 [D loss: 0.616165, acc.: 61.72%] [G loss: 1.110918]\n",
      "epoch:15 step:14967 [D loss: 0.823464, acc.: 48.44%] [G loss: 1.304552]\n",
      "epoch:15 step:14968 [D loss: 0.531462, acc.: 75.78%] [G loss: 1.483651]\n",
      "epoch:15 step:14969 [D loss: 0.544281, acc.: 74.22%] [G loss: 1.305280]\n",
      "epoch:15 step:14970 [D loss: 0.589087, acc.: 69.53%] [G loss: 1.485680]\n",
      "epoch:15 step:14971 [D loss: 0.577159, acc.: 66.41%] [G loss: 1.371195]\n",
      "epoch:15 step:14972 [D loss: 0.667299, acc.: 64.06%] [G loss: 1.274628]\n",
      "epoch:15 step:14973 [D loss: 0.513439, acc.: 75.00%] [G loss: 1.285266]\n",
      "epoch:15 step:14974 [D loss: 0.571013, acc.: 68.75%] [G loss: 1.152890]\n",
      "epoch:15 step:14975 [D loss: 0.550636, acc.: 69.53%] [G loss: 1.011162]\n",
      "epoch:15 step:14976 [D loss: 0.524537, acc.: 70.31%] [G loss: 1.450361]\n",
      "epoch:15 step:14977 [D loss: 0.473219, acc.: 78.91%] [G loss: 1.162941]\n",
      "epoch:15 step:14978 [D loss: 0.650534, acc.: 63.28%] [G loss: 0.957281]\n",
      "epoch:15 step:14979 [D loss: 0.483430, acc.: 80.47%] [G loss: 1.368113]\n",
      "epoch:15 step:14980 [D loss: 0.537082, acc.: 74.22%] [G loss: 1.181308]\n",
      "epoch:15 step:14981 [D loss: 0.661099, acc.: 63.28%] [G loss: 1.120824]\n",
      "epoch:15 step:14982 [D loss: 0.716959, acc.: 60.16%] [G loss: 1.145919]\n",
      "epoch:15 step:14983 [D loss: 0.560654, acc.: 72.66%] [G loss: 1.377659]\n",
      "epoch:15 step:14984 [D loss: 0.650448, acc.: 63.28%] [G loss: 1.107632]\n",
      "epoch:15 step:14985 [D loss: 0.540404, acc.: 69.53%] [G loss: 1.699790]\n",
      "epoch:15 step:14986 [D loss: 0.585639, acc.: 67.19%] [G loss: 1.503568]\n",
      "epoch:15 step:14987 [D loss: 0.651886, acc.: 61.72%] [G loss: 1.073525]\n",
      "epoch:15 step:14988 [D loss: 0.640071, acc.: 64.06%] [G loss: 1.080456]\n",
      "epoch:15 step:14989 [D loss: 0.519877, acc.: 75.78%] [G loss: 1.096057]\n",
      "epoch:15 step:14990 [D loss: 0.533711, acc.: 75.00%] [G loss: 1.076736]\n",
      "epoch:15 step:14991 [D loss: 0.527865, acc.: 73.44%] [G loss: 1.229765]\n",
      "epoch:15 step:14992 [D loss: 0.724711, acc.: 57.03%] [G loss: 1.080949]\n",
      "epoch:16 step:14993 [D loss: 0.650703, acc.: 69.53%] [G loss: 1.334672]\n",
      "epoch:16 step:14994 [D loss: 0.507176, acc.: 74.22%] [G loss: 1.127530]\n",
      "epoch:16 step:14995 [D loss: 0.652045, acc.: 69.53%] [G loss: 1.012712]\n",
      "epoch:16 step:14996 [D loss: 0.581787, acc.: 66.41%] [G loss: 1.294841]\n",
      "epoch:16 step:14997 [D loss: 0.515117, acc.: 75.78%] [G loss: 1.180765]\n",
      "epoch:16 step:14998 [D loss: 0.645107, acc.: 63.28%] [G loss: 1.363161]\n",
      "epoch:16 step:14999 [D loss: 0.623638, acc.: 64.06%] [G loss: 1.219105]\n",
      "epoch:16 step:15000 [D loss: 0.486270, acc.: 79.69%] [G loss: 1.322122]\n",
      "##############\n",
      "[2.61380168 1.90147991 1.71089895 2.78093643 0.82403228 7.61839146\n",
      " 2.0111187  2.27010021 3.85301878 8.14868929]\n",
      "##########\n",
      "epoch:16 step:15001 [D loss: 0.569636, acc.: 76.56%] [G loss: 1.096592]\n",
      "epoch:16 step:15002 [D loss: 0.635985, acc.: 64.84%] [G loss: 1.509213]\n",
      "epoch:16 step:15003 [D loss: 0.500643, acc.: 79.69%] [G loss: 1.274405]\n",
      "epoch:16 step:15004 [D loss: 0.609586, acc.: 65.62%] [G loss: 0.817542]\n",
      "epoch:16 step:15005 [D loss: 0.634461, acc.: 58.59%] [G loss: 1.294531]\n",
      "epoch:16 step:15006 [D loss: 0.603227, acc.: 69.53%] [G loss: 1.100500]\n",
      "epoch:16 step:15007 [D loss: 0.471201, acc.: 78.91%] [G loss: 1.587630]\n",
      "epoch:16 step:15008 [D loss: 0.544007, acc.: 71.88%] [G loss: 1.256847]\n",
      "epoch:16 step:15009 [D loss: 0.691526, acc.: 61.72%] [G loss: 0.953944]\n",
      "epoch:16 step:15010 [D loss: 0.576723, acc.: 67.19%] [G loss: 1.218589]\n",
      "epoch:16 step:15011 [D loss: 0.608304, acc.: 62.50%] [G loss: 1.131076]\n",
      "epoch:16 step:15012 [D loss: 0.522985, acc.: 75.00%] [G loss: 1.104833]\n",
      "epoch:16 step:15013 [D loss: 0.564381, acc.: 66.41%] [G loss: 1.400177]\n",
      "epoch:16 step:15014 [D loss: 0.602568, acc.: 66.41%] [G loss: 1.250808]\n",
      "epoch:16 step:15015 [D loss: 0.574837, acc.: 68.75%] [G loss: 1.338903]\n",
      "epoch:16 step:15016 [D loss: 0.546018, acc.: 71.88%] [G loss: 1.407100]\n",
      "epoch:16 step:15017 [D loss: 0.632020, acc.: 63.28%] [G loss: 1.029021]\n",
      "epoch:16 step:15018 [D loss: 0.682204, acc.: 55.47%] [G loss: 1.226241]\n",
      "epoch:16 step:15019 [D loss: 0.566318, acc.: 68.75%] [G loss: 1.181512]\n",
      "epoch:16 step:15020 [D loss: 0.538635, acc.: 71.88%] [G loss: 1.306074]\n",
      "epoch:16 step:15021 [D loss: 0.570119, acc.: 68.75%] [G loss: 1.161517]\n",
      "epoch:16 step:15022 [D loss: 0.512544, acc.: 74.22%] [G loss: 1.619848]\n",
      "epoch:16 step:15023 [D loss: 0.524234, acc.: 76.56%] [G loss: 1.580047]\n",
      "epoch:16 step:15024 [D loss: 0.699320, acc.: 58.59%] [G loss: 1.165082]\n",
      "epoch:16 step:15025 [D loss: 0.503501, acc.: 74.22%] [G loss: 1.527255]\n",
      "epoch:16 step:15026 [D loss: 0.541111, acc.: 75.78%] [G loss: 1.701077]\n",
      "epoch:16 step:15027 [D loss: 0.545512, acc.: 74.22%] [G loss: 1.301687]\n",
      "epoch:16 step:15028 [D loss: 0.539471, acc.: 74.22%] [G loss: 1.315239]\n",
      "epoch:16 step:15029 [D loss: 0.546826, acc.: 72.66%] [G loss: 1.383177]\n",
      "epoch:16 step:15030 [D loss: 0.666512, acc.: 63.28%] [G loss: 1.033470]\n",
      "epoch:16 step:15031 [D loss: 0.544856, acc.: 73.44%] [G loss: 1.263983]\n",
      "epoch:16 step:15032 [D loss: 0.577601, acc.: 66.41%] [G loss: 1.530324]\n",
      "epoch:16 step:15033 [D loss: 0.476847, acc.: 81.25%] [G loss: 1.421711]\n",
      "epoch:16 step:15034 [D loss: 0.544502, acc.: 69.53%] [G loss: 1.140434]\n",
      "epoch:16 step:15035 [D loss: 0.537108, acc.: 74.22%] [G loss: 1.178576]\n",
      "epoch:16 step:15036 [D loss: 0.610655, acc.: 63.28%] [G loss: 1.677816]\n",
      "epoch:16 step:15037 [D loss: 0.780892, acc.: 49.22%] [G loss: 1.165310]\n",
      "epoch:16 step:15038 [D loss: 0.550510, acc.: 68.75%] [G loss: 1.149813]\n",
      "epoch:16 step:15039 [D loss: 0.610928, acc.: 66.41%] [G loss: 1.386390]\n",
      "epoch:16 step:15040 [D loss: 0.588809, acc.: 68.75%] [G loss: 1.371800]\n",
      "epoch:16 step:15041 [D loss: 0.562360, acc.: 71.09%] [G loss: 1.171289]\n",
      "epoch:16 step:15042 [D loss: 0.466660, acc.: 82.03%] [G loss: 1.289512]\n",
      "epoch:16 step:15043 [D loss: 0.537974, acc.: 73.44%] [G loss: 1.302481]\n",
      "epoch:16 step:15044 [D loss: 0.472188, acc.: 83.59%] [G loss: 1.640095]\n",
      "epoch:16 step:15045 [D loss: 0.595768, acc.: 65.62%] [G loss: 1.112446]\n",
      "epoch:16 step:15046 [D loss: 0.602739, acc.: 68.75%] [G loss: 1.402270]\n",
      "epoch:16 step:15047 [D loss: 0.627163, acc.: 61.72%] [G loss: 1.082524]\n",
      "epoch:16 step:15048 [D loss: 0.685457, acc.: 60.94%] [G loss: 1.269255]\n",
      "epoch:16 step:15049 [D loss: 0.599648, acc.: 66.41%] [G loss: 1.302956]\n",
      "epoch:16 step:15050 [D loss: 0.519743, acc.: 75.00%] [G loss: 1.228095]\n",
      "epoch:16 step:15051 [D loss: 0.638317, acc.: 67.97%] [G loss: 0.990653]\n",
      "epoch:16 step:15052 [D loss: 0.576854, acc.: 67.97%] [G loss: 1.369482]\n",
      "epoch:16 step:15053 [D loss: 0.532538, acc.: 73.44%] [G loss: 1.376506]\n",
      "epoch:16 step:15054 [D loss: 0.560064, acc.: 71.88%] [G loss: 1.371359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15055 [D loss: 0.566781, acc.: 71.88%] [G loss: 1.867842]\n",
      "epoch:16 step:15056 [D loss: 0.641836, acc.: 59.38%] [G loss: 1.048091]\n",
      "epoch:16 step:15057 [D loss: 0.467128, acc.: 82.03%] [G loss: 1.389552]\n",
      "epoch:16 step:15058 [D loss: 0.650404, acc.: 62.50%] [G loss: 1.228427]\n",
      "epoch:16 step:15059 [D loss: 0.470192, acc.: 82.03%] [G loss: 1.051818]\n",
      "epoch:16 step:15060 [D loss: 0.668337, acc.: 59.38%] [G loss: 1.085195]\n",
      "epoch:16 step:15061 [D loss: 0.600411, acc.: 62.50%] [G loss: 1.466755]\n",
      "epoch:16 step:15062 [D loss: 0.625680, acc.: 61.72%] [G loss: 1.087574]\n",
      "epoch:16 step:15063 [D loss: 0.721289, acc.: 56.25%] [G loss: 1.220571]\n",
      "epoch:16 step:15064 [D loss: 0.637153, acc.: 60.16%] [G loss: 1.334179]\n",
      "epoch:16 step:15065 [D loss: 0.620572, acc.: 65.62%] [G loss: 1.209552]\n",
      "epoch:16 step:15066 [D loss: 0.665229, acc.: 61.72%] [G loss: 1.218781]\n",
      "epoch:16 step:15067 [D loss: 0.632905, acc.: 59.38%] [G loss: 1.322407]\n",
      "epoch:16 step:15068 [D loss: 0.568742, acc.: 68.75%] [G loss: 1.310658]\n",
      "epoch:16 step:15069 [D loss: 0.504880, acc.: 75.78%] [G loss: 1.494110]\n",
      "epoch:16 step:15070 [D loss: 0.523439, acc.: 76.56%] [G loss: 1.425560]\n",
      "epoch:16 step:15071 [D loss: 0.586224, acc.: 69.53%] [G loss: 1.074644]\n",
      "epoch:16 step:15072 [D loss: 0.640031, acc.: 60.94%] [G loss: 1.062299]\n",
      "epoch:16 step:15073 [D loss: 0.632294, acc.: 58.59%] [G loss: 0.914673]\n",
      "epoch:16 step:15074 [D loss: 0.586864, acc.: 70.31%] [G loss: 1.087244]\n",
      "epoch:16 step:15075 [D loss: 0.640592, acc.: 60.16%] [G loss: 1.135521]\n",
      "epoch:16 step:15076 [D loss: 0.605258, acc.: 66.41%] [G loss: 1.250966]\n",
      "epoch:16 step:15077 [D loss: 0.638178, acc.: 64.84%] [G loss: 1.242944]\n",
      "epoch:16 step:15078 [D loss: 0.581396, acc.: 67.19%] [G loss: 1.138181]\n",
      "epoch:16 step:15079 [D loss: 0.493360, acc.: 75.78%] [G loss: 1.386899]\n",
      "epoch:16 step:15080 [D loss: 0.706355, acc.: 59.38%] [G loss: 1.283986]\n",
      "epoch:16 step:15081 [D loss: 0.595495, acc.: 70.31%] [G loss: 1.154679]\n",
      "epoch:16 step:15082 [D loss: 0.626541, acc.: 64.84%] [G loss: 1.088200]\n",
      "epoch:16 step:15083 [D loss: 0.452814, acc.: 81.25%] [G loss: 1.505019]\n",
      "epoch:16 step:15084 [D loss: 0.602818, acc.: 68.75%] [G loss: 1.316486]\n",
      "epoch:16 step:15085 [D loss: 0.437641, acc.: 82.81%] [G loss: 1.240199]\n",
      "epoch:16 step:15086 [D loss: 0.394972, acc.: 89.06%] [G loss: 1.342615]\n",
      "epoch:16 step:15087 [D loss: 0.509538, acc.: 73.44%] [G loss: 1.509066]\n",
      "epoch:16 step:15088 [D loss: 0.432839, acc.: 84.38%] [G loss: 1.563300]\n",
      "epoch:16 step:15089 [D loss: 0.567648, acc.: 73.44%] [G loss: 1.193960]\n",
      "epoch:16 step:15090 [D loss: 0.576678, acc.: 69.53%] [G loss: 1.297198]\n",
      "epoch:16 step:15091 [D loss: 0.544740, acc.: 75.00%] [G loss: 1.140201]\n",
      "epoch:16 step:15092 [D loss: 0.545228, acc.: 72.66%] [G loss: 1.385389]\n",
      "epoch:16 step:15093 [D loss: 0.538950, acc.: 69.53%] [G loss: 1.538502]\n",
      "epoch:16 step:15094 [D loss: 0.626189, acc.: 66.41%] [G loss: 1.312453]\n",
      "epoch:16 step:15095 [D loss: 0.601555, acc.: 65.62%] [G loss: 0.998577]\n",
      "epoch:16 step:15096 [D loss: 0.507205, acc.: 78.12%] [G loss: 1.325643]\n",
      "epoch:16 step:15097 [D loss: 0.616193, acc.: 64.84%] [G loss: 1.313626]\n",
      "epoch:16 step:15098 [D loss: 0.504619, acc.: 78.12%] [G loss: 1.475985]\n",
      "epoch:16 step:15099 [D loss: 0.476779, acc.: 78.12%] [G loss: 1.357343]\n",
      "epoch:16 step:15100 [D loss: 0.652387, acc.: 64.06%] [G loss: 1.097107]\n",
      "epoch:16 step:15101 [D loss: 0.639131, acc.: 62.50%] [G loss: 1.332882]\n",
      "epoch:16 step:15102 [D loss: 0.700269, acc.: 57.81%] [G loss: 1.019731]\n",
      "epoch:16 step:15103 [D loss: 0.659410, acc.: 59.38%] [G loss: 0.872333]\n",
      "epoch:16 step:15104 [D loss: 0.525559, acc.: 72.66%] [G loss: 1.272065]\n",
      "epoch:16 step:15105 [D loss: 0.505934, acc.: 77.34%] [G loss: 1.436508]\n",
      "epoch:16 step:15106 [D loss: 0.444385, acc.: 87.50%] [G loss: 1.145893]\n",
      "epoch:16 step:15107 [D loss: 0.534350, acc.: 75.00%] [G loss: 1.147909]\n",
      "epoch:16 step:15108 [D loss: 0.607149, acc.: 64.06%] [G loss: 1.284270]\n",
      "epoch:16 step:15109 [D loss: 0.526951, acc.: 71.88%] [G loss: 1.094933]\n",
      "epoch:16 step:15110 [D loss: 0.663760, acc.: 62.50%] [G loss: 1.167352]\n",
      "epoch:16 step:15111 [D loss: 0.611923, acc.: 63.28%] [G loss: 1.400215]\n",
      "epoch:16 step:15112 [D loss: 0.693569, acc.: 61.72%] [G loss: 1.157584]\n",
      "epoch:16 step:15113 [D loss: 0.573341, acc.: 68.75%] [G loss: 1.342781]\n",
      "epoch:16 step:15114 [D loss: 0.624253, acc.: 64.84%] [G loss: 1.218318]\n",
      "epoch:16 step:15115 [D loss: 0.556280, acc.: 71.88%] [G loss: 1.089447]\n",
      "epoch:16 step:15116 [D loss: 0.533138, acc.: 75.78%] [G loss: 1.385141]\n",
      "epoch:16 step:15117 [D loss: 0.562569, acc.: 71.09%] [G loss: 1.392004]\n",
      "epoch:16 step:15118 [D loss: 0.622634, acc.: 65.62%] [G loss: 1.285290]\n",
      "epoch:16 step:15119 [D loss: 0.570801, acc.: 71.09%] [G loss: 1.301262]\n",
      "epoch:16 step:15120 [D loss: 0.654448, acc.: 58.59%] [G loss: 1.010492]\n",
      "epoch:16 step:15121 [D loss: 0.593069, acc.: 68.75%] [G loss: 1.353982]\n",
      "epoch:16 step:15122 [D loss: 0.534426, acc.: 75.00%] [G loss: 1.218049]\n",
      "epoch:16 step:15123 [D loss: 0.561452, acc.: 67.97%] [G loss: 1.481455]\n",
      "epoch:16 step:15124 [D loss: 0.697806, acc.: 62.50%] [G loss: 1.307885]\n",
      "epoch:16 step:15125 [D loss: 0.581982, acc.: 69.53%] [G loss: 1.187081]\n",
      "epoch:16 step:15126 [D loss: 0.618012, acc.: 68.75%] [G loss: 1.273682]\n",
      "epoch:16 step:15127 [D loss: 0.566933, acc.: 73.44%] [G loss: 1.311157]\n",
      "epoch:16 step:15128 [D loss: 0.578079, acc.: 70.31%] [G loss: 1.448076]\n",
      "epoch:16 step:15129 [D loss: 0.575448, acc.: 73.44%] [G loss: 1.348219]\n",
      "epoch:16 step:15130 [D loss: 0.679291, acc.: 64.84%] [G loss: 1.421084]\n",
      "epoch:16 step:15131 [D loss: 0.684374, acc.: 57.03%] [G loss: 1.093653]\n",
      "epoch:16 step:15132 [D loss: 0.541151, acc.: 74.22%] [G loss: 1.019747]\n",
      "epoch:16 step:15133 [D loss: 0.577946, acc.: 70.31%] [G loss: 1.295402]\n",
      "epoch:16 step:15134 [D loss: 0.474860, acc.: 79.69%] [G loss: 1.391338]\n",
      "epoch:16 step:15135 [D loss: 0.579634, acc.: 69.53%] [G loss: 1.267033]\n",
      "epoch:16 step:15136 [D loss: 0.626253, acc.: 66.41%] [G loss: 1.170785]\n",
      "epoch:16 step:15137 [D loss: 0.605710, acc.: 65.62%] [G loss: 1.318182]\n",
      "epoch:16 step:15138 [D loss: 0.530878, acc.: 75.00%] [G loss: 1.450758]\n",
      "epoch:16 step:15139 [D loss: 0.527701, acc.: 76.56%] [G loss: 1.402618]\n",
      "epoch:16 step:15140 [D loss: 0.594333, acc.: 69.53%] [G loss: 1.328224]\n",
      "epoch:16 step:15141 [D loss: 0.672748, acc.: 60.16%] [G loss: 1.144210]\n",
      "epoch:16 step:15142 [D loss: 0.598893, acc.: 67.19%] [G loss: 1.230386]\n",
      "epoch:16 step:15143 [D loss: 0.680064, acc.: 58.59%] [G loss: 0.902299]\n",
      "epoch:16 step:15144 [D loss: 0.533630, acc.: 71.09%] [G loss: 1.232061]\n",
      "epoch:16 step:15145 [D loss: 0.513157, acc.: 73.44%] [G loss: 1.341529]\n",
      "epoch:16 step:15146 [D loss: 0.544759, acc.: 73.44%] [G loss: 1.343735]\n",
      "epoch:16 step:15147 [D loss: 0.529464, acc.: 74.22%] [G loss: 1.586658]\n",
      "epoch:16 step:15148 [D loss: 0.497379, acc.: 80.47%] [G loss: 0.930565]\n",
      "epoch:16 step:15149 [D loss: 0.647020, acc.: 59.38%] [G loss: 1.047936]\n",
      "epoch:16 step:15150 [D loss: 0.548410, acc.: 75.00%] [G loss: 1.408453]\n",
      "epoch:16 step:15151 [D loss: 0.592241, acc.: 66.41%] [G loss: 1.462119]\n",
      "epoch:16 step:15152 [D loss: 0.542822, acc.: 75.00%] [G loss: 1.673879]\n",
      "epoch:16 step:15153 [D loss: 0.561383, acc.: 67.19%] [G loss: 1.815931]\n",
      "epoch:16 step:15154 [D loss: 0.551213, acc.: 68.75%] [G loss: 1.438947]\n",
      "epoch:16 step:15155 [D loss: 0.595072, acc.: 68.75%] [G loss: 1.398837]\n",
      "epoch:16 step:15156 [D loss: 0.516192, acc.: 78.12%] [G loss: 1.178182]\n",
      "epoch:16 step:15157 [D loss: 0.539685, acc.: 69.53%] [G loss: 1.322521]\n",
      "epoch:16 step:15158 [D loss: 0.499279, acc.: 80.47%] [G loss: 1.108029]\n",
      "epoch:16 step:15159 [D loss: 0.561171, acc.: 72.66%] [G loss: 1.392785]\n",
      "epoch:16 step:15160 [D loss: 0.505128, acc.: 75.00%] [G loss: 1.222973]\n",
      "epoch:16 step:15161 [D loss: 0.564718, acc.: 71.09%] [G loss: 1.189870]\n",
      "epoch:16 step:15162 [D loss: 0.524852, acc.: 73.44%] [G loss: 1.190569]\n",
      "epoch:16 step:15163 [D loss: 0.574063, acc.: 71.88%] [G loss: 1.239059]\n",
      "epoch:16 step:15164 [D loss: 0.493360, acc.: 77.34%] [G loss: 1.468434]\n",
      "epoch:16 step:15165 [D loss: 0.610439, acc.: 65.62%] [G loss: 1.463416]\n",
      "epoch:16 step:15166 [D loss: 0.553400, acc.: 74.22%] [G loss: 1.229586]\n",
      "epoch:16 step:15167 [D loss: 0.519804, acc.: 76.56%] [G loss: 1.790276]\n",
      "epoch:16 step:15168 [D loss: 0.662519, acc.: 62.50%] [G loss: 0.947023]\n",
      "epoch:16 step:15169 [D loss: 0.548589, acc.: 70.31%] [G loss: 1.280415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15170 [D loss: 0.743109, acc.: 49.22%] [G loss: 0.983114]\n",
      "epoch:16 step:15171 [D loss: 0.610887, acc.: 67.97%] [G loss: 1.053919]\n",
      "epoch:16 step:15172 [D loss: 0.529140, acc.: 71.88%] [G loss: 1.325082]\n",
      "epoch:16 step:15173 [D loss: 0.516686, acc.: 76.56%] [G loss: 1.374170]\n",
      "epoch:16 step:15174 [D loss: 0.520589, acc.: 74.22%] [G loss: 1.227048]\n",
      "epoch:16 step:15175 [D loss: 0.554943, acc.: 67.19%] [G loss: 1.469627]\n",
      "epoch:16 step:15176 [D loss: 0.490451, acc.: 77.34%] [G loss: 1.317957]\n",
      "epoch:16 step:15177 [D loss: 0.545544, acc.: 72.66%] [G loss: 1.742191]\n",
      "epoch:16 step:15178 [D loss: 0.563622, acc.: 67.19%] [G loss: 1.195110]\n",
      "epoch:16 step:15179 [D loss: 0.532923, acc.: 74.22%] [G loss: 1.389250]\n",
      "epoch:16 step:15180 [D loss: 0.532073, acc.: 75.00%] [G loss: 1.025296]\n",
      "epoch:16 step:15181 [D loss: 0.521116, acc.: 75.78%] [G loss: 1.224555]\n",
      "epoch:16 step:15182 [D loss: 0.669322, acc.: 62.50%] [G loss: 1.069916]\n",
      "epoch:16 step:15183 [D loss: 0.516131, acc.: 72.66%] [G loss: 1.389893]\n",
      "epoch:16 step:15184 [D loss: 0.705903, acc.: 53.12%] [G loss: 1.241690]\n",
      "epoch:16 step:15185 [D loss: 0.471839, acc.: 77.34%] [G loss: 1.295265]\n",
      "epoch:16 step:15186 [D loss: 0.661433, acc.: 64.84%] [G loss: 1.207085]\n",
      "epoch:16 step:15187 [D loss: 0.734477, acc.: 51.56%] [G loss: 1.074740]\n",
      "epoch:16 step:15188 [D loss: 0.570917, acc.: 64.06%] [G loss: 1.178510]\n",
      "epoch:16 step:15189 [D loss: 0.619933, acc.: 63.28%] [G loss: 1.523234]\n",
      "epoch:16 step:15190 [D loss: 0.654720, acc.: 64.84%] [G loss: 1.326667]\n",
      "epoch:16 step:15191 [D loss: 0.605837, acc.: 68.75%] [G loss: 1.173965]\n",
      "epoch:16 step:15192 [D loss: 0.616191, acc.: 64.06%] [G loss: 1.053795]\n",
      "epoch:16 step:15193 [D loss: 0.456526, acc.: 81.25%] [G loss: 1.377606]\n",
      "epoch:16 step:15194 [D loss: 0.509056, acc.: 75.78%] [G loss: 1.643760]\n",
      "epoch:16 step:15195 [D loss: 0.552430, acc.: 71.88%] [G loss: 1.002406]\n",
      "epoch:16 step:15196 [D loss: 0.556573, acc.: 72.66%] [G loss: 1.651848]\n",
      "epoch:16 step:15197 [D loss: 0.616415, acc.: 67.97%] [G loss: 1.468060]\n",
      "epoch:16 step:15198 [D loss: 0.553476, acc.: 71.88%] [G loss: 1.253176]\n",
      "epoch:16 step:15199 [D loss: 0.654376, acc.: 66.41%] [G loss: 1.214463]\n",
      "epoch:16 step:15200 [D loss: 0.475736, acc.: 78.12%] [G loss: 1.142627]\n",
      "##############\n",
      "[2.5638411  1.90481215 1.88062934 2.79513124 0.8814039  6.45429954\n",
      " 2.24883774 2.82142703 3.77648295 6.14874591]\n",
      "##########\n",
      "epoch:16 step:15201 [D loss: 0.563037, acc.: 72.66%] [G loss: 1.351507]\n",
      "epoch:16 step:15202 [D loss: 0.640908, acc.: 60.94%] [G loss: 1.278777]\n",
      "epoch:16 step:15203 [D loss: 0.581165, acc.: 68.75%] [G loss: 1.143793]\n",
      "epoch:16 step:15204 [D loss: 0.503232, acc.: 78.12%] [G loss: 1.447454]\n",
      "epoch:16 step:15205 [D loss: 0.724646, acc.: 58.59%] [G loss: 1.170413]\n",
      "epoch:16 step:15206 [D loss: 0.736861, acc.: 53.91%] [G loss: 1.079262]\n",
      "epoch:16 step:15207 [D loss: 0.575704, acc.: 70.31%] [G loss: 1.305994]\n",
      "epoch:16 step:15208 [D loss: 0.522798, acc.: 76.56%] [G loss: 1.292860]\n",
      "epoch:16 step:15209 [D loss: 0.540407, acc.: 77.34%] [G loss: 1.224500]\n",
      "epoch:16 step:15210 [D loss: 0.507605, acc.: 75.78%] [G loss: 1.164663]\n",
      "epoch:16 step:15211 [D loss: 0.534223, acc.: 74.22%] [G loss: 1.314498]\n",
      "epoch:16 step:15212 [D loss: 0.636530, acc.: 65.62%] [G loss: 1.193238]\n",
      "epoch:16 step:15213 [D loss: 0.756436, acc.: 56.25%] [G loss: 0.874508]\n",
      "epoch:16 step:15214 [D loss: 0.648603, acc.: 61.72%] [G loss: 1.252478]\n",
      "epoch:16 step:15215 [D loss: 0.691415, acc.: 60.16%] [G loss: 1.137045]\n",
      "epoch:16 step:15216 [D loss: 0.692600, acc.: 63.28%] [G loss: 1.519183]\n",
      "epoch:16 step:15217 [D loss: 0.552616, acc.: 74.22%] [G loss: 1.437574]\n",
      "epoch:16 step:15218 [D loss: 0.569373, acc.: 69.53%] [G loss: 1.171126]\n",
      "epoch:16 step:15219 [D loss: 0.633952, acc.: 62.50%] [G loss: 1.252717]\n",
      "epoch:16 step:15220 [D loss: 0.611561, acc.: 64.06%] [G loss: 1.194548]\n",
      "epoch:16 step:15221 [D loss: 0.582906, acc.: 72.66%] [G loss: 1.568857]\n",
      "epoch:16 step:15222 [D loss: 0.583696, acc.: 70.31%] [G loss: 1.114428]\n",
      "epoch:16 step:15223 [D loss: 0.494823, acc.: 75.78%] [G loss: 1.446169]\n",
      "epoch:16 step:15224 [D loss: 0.627316, acc.: 65.62%] [G loss: 1.117096]\n",
      "epoch:16 step:15225 [D loss: 0.604583, acc.: 69.53%] [G loss: 1.042340]\n",
      "epoch:16 step:15226 [D loss: 0.639557, acc.: 57.81%] [G loss: 1.010104]\n",
      "epoch:16 step:15227 [D loss: 0.529705, acc.: 77.34%] [G loss: 1.256857]\n",
      "epoch:16 step:15228 [D loss: 0.487563, acc.: 76.56%] [G loss: 1.281486]\n",
      "epoch:16 step:15229 [D loss: 0.501378, acc.: 78.12%] [G loss: 1.215784]\n",
      "epoch:16 step:15230 [D loss: 0.552996, acc.: 78.91%] [G loss: 1.456718]\n",
      "epoch:16 step:15231 [D loss: 0.620614, acc.: 65.62%] [G loss: 1.077315]\n",
      "epoch:16 step:15232 [D loss: 0.565406, acc.: 66.41%] [G loss: 1.199497]\n",
      "epoch:16 step:15233 [D loss: 0.534715, acc.: 72.66%] [G loss: 1.488848]\n",
      "epoch:16 step:15234 [D loss: 0.813077, acc.: 48.44%] [G loss: 1.121304]\n",
      "epoch:16 step:15235 [D loss: 0.556639, acc.: 71.09%] [G loss: 1.204707]\n",
      "epoch:16 step:15236 [D loss: 0.534021, acc.: 72.66%] [G loss: 1.222208]\n",
      "epoch:16 step:15237 [D loss: 0.497428, acc.: 77.34%] [G loss: 1.400072]\n",
      "epoch:16 step:15238 [D loss: 0.556412, acc.: 68.75%] [G loss: 1.010526]\n",
      "epoch:16 step:15239 [D loss: 0.684023, acc.: 65.62%] [G loss: 1.305658]\n",
      "epoch:16 step:15240 [D loss: 0.672776, acc.: 57.81%] [G loss: 1.281345]\n",
      "epoch:16 step:15241 [D loss: 0.531538, acc.: 76.56%] [G loss: 0.965872]\n",
      "epoch:16 step:15242 [D loss: 0.629072, acc.: 65.62%] [G loss: 1.547136]\n",
      "epoch:16 step:15243 [D loss: 0.522027, acc.: 75.00%] [G loss: 1.619411]\n",
      "epoch:16 step:15244 [D loss: 0.557595, acc.: 72.66%] [G loss: 1.334706]\n",
      "epoch:16 step:15245 [D loss: 0.566028, acc.: 74.22%] [G loss: 1.137616]\n",
      "epoch:16 step:15246 [D loss: 0.593005, acc.: 66.41%] [G loss: 1.145463]\n",
      "epoch:16 step:15247 [D loss: 0.517879, acc.: 75.78%] [G loss: 1.370790]\n",
      "epoch:16 step:15248 [D loss: 0.612508, acc.: 65.62%] [G loss: 1.118096]\n",
      "epoch:16 step:15249 [D loss: 0.499278, acc.: 76.56%] [G loss: 1.433507]\n",
      "epoch:16 step:15250 [D loss: 0.599665, acc.: 68.75%] [G loss: 1.219945]\n",
      "epoch:16 step:15251 [D loss: 0.496339, acc.: 71.09%] [G loss: 1.430165]\n",
      "epoch:16 step:15252 [D loss: 0.524396, acc.: 76.56%] [G loss: 1.317494]\n",
      "epoch:16 step:15253 [D loss: 0.539705, acc.: 78.12%] [G loss: 1.543509]\n",
      "epoch:16 step:15254 [D loss: 0.565963, acc.: 71.09%] [G loss: 0.984809]\n",
      "epoch:16 step:15255 [D loss: 0.740343, acc.: 53.12%] [G loss: 0.941347]\n",
      "epoch:16 step:15256 [D loss: 0.730295, acc.: 52.34%] [G loss: 1.073348]\n",
      "epoch:16 step:15257 [D loss: 0.473389, acc.: 81.25%] [G loss: 1.558721]\n",
      "epoch:16 step:15258 [D loss: 0.599995, acc.: 60.94%] [G loss: 1.292737]\n",
      "epoch:16 step:15259 [D loss: 0.513426, acc.: 75.78%] [G loss: 0.942180]\n",
      "epoch:16 step:15260 [D loss: 0.613960, acc.: 65.62%] [G loss: 1.277892]\n",
      "epoch:16 step:15261 [D loss: 0.509030, acc.: 75.78%] [G loss: 1.465655]\n",
      "epoch:16 step:15262 [D loss: 0.611067, acc.: 66.41%] [G loss: 1.408018]\n",
      "epoch:16 step:15263 [D loss: 0.525481, acc.: 70.31%] [G loss: 1.413184]\n",
      "epoch:16 step:15264 [D loss: 0.715138, acc.: 60.94%] [G loss: 1.154553]\n",
      "epoch:16 step:15265 [D loss: 0.585690, acc.: 67.97%] [G loss: 1.289617]\n",
      "epoch:16 step:15266 [D loss: 0.552697, acc.: 68.75%] [G loss: 1.196200]\n",
      "epoch:16 step:15267 [D loss: 0.711970, acc.: 59.38%] [G loss: 0.949924]\n",
      "epoch:16 step:15268 [D loss: 0.584284, acc.: 72.66%] [G loss: 1.177156]\n",
      "epoch:16 step:15269 [D loss: 0.522493, acc.: 74.22%] [G loss: 1.098045]\n",
      "epoch:16 step:15270 [D loss: 0.576079, acc.: 68.75%] [G loss: 1.297162]\n",
      "epoch:16 step:15271 [D loss: 0.641302, acc.: 62.50%] [G loss: 1.386204]\n",
      "epoch:16 step:15272 [D loss: 0.672219, acc.: 61.72%] [G loss: 1.172444]\n",
      "epoch:16 step:15273 [D loss: 0.550938, acc.: 74.22%] [G loss: 1.485037]\n",
      "epoch:16 step:15274 [D loss: 0.763617, acc.: 50.00%] [G loss: 1.068098]\n",
      "epoch:16 step:15275 [D loss: 0.674398, acc.: 56.25%] [G loss: 1.047881]\n",
      "epoch:16 step:15276 [D loss: 0.510771, acc.: 74.22%] [G loss: 1.580786]\n",
      "epoch:16 step:15277 [D loss: 0.584326, acc.: 70.31%] [G loss: 1.329771]\n",
      "epoch:16 step:15278 [D loss: 0.631345, acc.: 63.28%] [G loss: 1.283613]\n",
      "epoch:16 step:15279 [D loss: 0.562570, acc.: 66.41%] [G loss: 1.280181]\n",
      "epoch:16 step:15280 [D loss: 0.577576, acc.: 71.09%] [G loss: 1.204733]\n",
      "epoch:16 step:15281 [D loss: 0.624694, acc.: 64.06%] [G loss: 0.983088]\n",
      "epoch:16 step:15282 [D loss: 0.423053, acc.: 86.72%] [G loss: 1.324353]\n",
      "epoch:16 step:15283 [D loss: 0.588865, acc.: 67.97%] [G loss: 1.315674]\n",
      "epoch:16 step:15284 [D loss: 0.707113, acc.: 62.50%] [G loss: 1.215811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15285 [D loss: 0.645841, acc.: 59.38%] [G loss: 1.106542]\n",
      "epoch:16 step:15286 [D loss: 0.615145, acc.: 64.84%] [G loss: 1.050491]\n",
      "epoch:16 step:15287 [D loss: 0.605352, acc.: 71.09%] [G loss: 1.405499]\n",
      "epoch:16 step:15288 [D loss: 0.572288, acc.: 72.66%] [G loss: 1.334007]\n",
      "epoch:16 step:15289 [D loss: 0.579212, acc.: 68.75%] [G loss: 1.231152]\n",
      "epoch:16 step:15290 [D loss: 0.770311, acc.: 50.78%] [G loss: 1.220038]\n",
      "epoch:16 step:15291 [D loss: 0.596045, acc.: 67.97%] [G loss: 1.102154]\n",
      "epoch:16 step:15292 [D loss: 0.633763, acc.: 62.50%] [G loss: 1.244261]\n",
      "epoch:16 step:15293 [D loss: 0.745827, acc.: 53.91%] [G loss: 1.285679]\n",
      "epoch:16 step:15294 [D loss: 0.630654, acc.: 64.06%] [G loss: 1.064707]\n",
      "epoch:16 step:15295 [D loss: 0.637920, acc.: 64.06%] [G loss: 1.290212]\n",
      "epoch:16 step:15296 [D loss: 0.724349, acc.: 60.16%] [G loss: 1.043976]\n",
      "epoch:16 step:15297 [D loss: 0.696821, acc.: 60.16%] [G loss: 1.195496]\n",
      "epoch:16 step:15298 [D loss: 0.541062, acc.: 74.22%] [G loss: 1.372270]\n",
      "epoch:16 step:15299 [D loss: 0.587674, acc.: 66.41%] [G loss: 1.326352]\n",
      "epoch:16 step:15300 [D loss: 0.640020, acc.: 63.28%] [G loss: 1.373320]\n",
      "epoch:16 step:15301 [D loss: 0.592981, acc.: 72.66%] [G loss: 1.187912]\n",
      "epoch:16 step:15302 [D loss: 0.605104, acc.: 68.75%] [G loss: 1.292236]\n",
      "epoch:16 step:15303 [D loss: 0.575513, acc.: 71.09%] [G loss: 1.253071]\n",
      "epoch:16 step:15304 [D loss: 0.652467, acc.: 64.06%] [G loss: 1.300065]\n",
      "epoch:16 step:15305 [D loss: 0.676352, acc.: 62.50%] [G loss: 1.213998]\n",
      "epoch:16 step:15306 [D loss: 0.497856, acc.: 77.34%] [G loss: 1.421470]\n",
      "epoch:16 step:15307 [D loss: 0.571248, acc.: 71.09%] [G loss: 1.236528]\n",
      "epoch:16 step:15308 [D loss: 0.678663, acc.: 55.47%] [G loss: 1.184957]\n",
      "epoch:16 step:15309 [D loss: 0.631074, acc.: 67.97%] [G loss: 0.945112]\n",
      "epoch:16 step:15310 [D loss: 0.712346, acc.: 53.91%] [G loss: 1.396262]\n",
      "epoch:16 step:15311 [D loss: 0.658169, acc.: 65.62%] [G loss: 1.112253]\n",
      "epoch:16 step:15312 [D loss: 0.672537, acc.: 60.94%] [G loss: 1.240814]\n",
      "epoch:16 step:15313 [D loss: 0.568136, acc.: 74.22%] [G loss: 1.540580]\n",
      "epoch:16 step:15314 [D loss: 0.591727, acc.: 68.75%] [G loss: 1.031444]\n",
      "epoch:16 step:15315 [D loss: 0.781798, acc.: 46.88%] [G loss: 1.092548]\n",
      "epoch:16 step:15316 [D loss: 0.591944, acc.: 66.41%] [G loss: 1.277603]\n",
      "epoch:16 step:15317 [D loss: 0.493423, acc.: 78.91%] [G loss: 1.406332]\n",
      "epoch:16 step:15318 [D loss: 0.560886, acc.: 75.78%] [G loss: 1.575092]\n",
      "epoch:16 step:15319 [D loss: 0.516760, acc.: 75.78%] [G loss: 1.400483]\n",
      "epoch:16 step:15320 [D loss: 0.478212, acc.: 77.34%] [G loss: 1.553670]\n",
      "epoch:16 step:15321 [D loss: 0.717110, acc.: 53.91%] [G loss: 1.169681]\n",
      "epoch:16 step:15322 [D loss: 0.621821, acc.: 63.28%] [G loss: 1.248543]\n",
      "epoch:16 step:15323 [D loss: 0.564633, acc.: 71.09%] [G loss: 1.367619]\n",
      "epoch:16 step:15324 [D loss: 0.582236, acc.: 70.31%] [G loss: 1.441203]\n",
      "epoch:16 step:15325 [D loss: 0.632925, acc.: 66.41%] [G loss: 0.985525]\n",
      "epoch:16 step:15326 [D loss: 0.512195, acc.: 78.12%] [G loss: 1.007680]\n",
      "epoch:16 step:15327 [D loss: 0.496984, acc.: 80.47%] [G loss: 1.549508]\n",
      "epoch:16 step:15328 [D loss: 0.584356, acc.: 66.41%] [G loss: 1.422805]\n",
      "epoch:16 step:15329 [D loss: 0.653883, acc.: 59.38%] [G loss: 1.199112]\n",
      "epoch:16 step:15330 [D loss: 0.451967, acc.: 82.03%] [G loss: 1.308054]\n",
      "epoch:16 step:15331 [D loss: 0.547442, acc.: 71.09%] [G loss: 1.083931]\n",
      "epoch:16 step:15332 [D loss: 0.536626, acc.: 72.66%] [G loss: 1.260785]\n",
      "epoch:16 step:15333 [D loss: 0.490805, acc.: 80.47%] [G loss: 1.529955]\n",
      "epoch:16 step:15334 [D loss: 0.653274, acc.: 57.81%] [G loss: 1.366593]\n",
      "epoch:16 step:15335 [D loss: 0.674574, acc.: 63.28%] [G loss: 1.208824]\n",
      "epoch:16 step:15336 [D loss: 0.487297, acc.: 80.47%] [G loss: 1.291421]\n",
      "epoch:16 step:15337 [D loss: 0.484225, acc.: 78.91%] [G loss: 1.407646]\n",
      "epoch:16 step:15338 [D loss: 0.712002, acc.: 63.28%] [G loss: 1.177161]\n",
      "epoch:16 step:15339 [D loss: 0.573442, acc.: 69.53%] [G loss: 1.127729]\n",
      "epoch:16 step:15340 [D loss: 0.503001, acc.: 71.09%] [G loss: 1.319011]\n",
      "epoch:16 step:15341 [D loss: 0.484905, acc.: 76.56%] [G loss: 1.197685]\n",
      "epoch:16 step:15342 [D loss: 0.525389, acc.: 74.22%] [G loss: 1.343478]\n",
      "epoch:16 step:15343 [D loss: 0.751672, acc.: 53.91%] [G loss: 1.176798]\n",
      "epoch:16 step:15344 [D loss: 0.619568, acc.: 63.28%] [G loss: 0.965509]\n",
      "epoch:16 step:15345 [D loss: 0.553344, acc.: 75.00%] [G loss: 1.255004]\n",
      "epoch:16 step:15346 [D loss: 0.573242, acc.: 69.53%] [G loss: 1.235294]\n",
      "epoch:16 step:15347 [D loss: 0.602402, acc.: 66.41%] [G loss: 1.053358]\n",
      "epoch:16 step:15348 [D loss: 0.567562, acc.: 68.75%] [G loss: 1.613134]\n",
      "epoch:16 step:15349 [D loss: 0.610455, acc.: 64.06%] [G loss: 1.219713]\n",
      "epoch:16 step:15350 [D loss: 0.472824, acc.: 78.91%] [G loss: 1.254295]\n",
      "epoch:16 step:15351 [D loss: 0.652533, acc.: 59.38%] [G loss: 1.104533]\n",
      "epoch:16 step:15352 [D loss: 0.549342, acc.: 69.53%] [G loss: 1.306185]\n",
      "epoch:16 step:15353 [D loss: 0.636440, acc.: 65.62%] [G loss: 1.231070]\n",
      "epoch:16 step:15354 [D loss: 0.555325, acc.: 69.53%] [G loss: 1.460367]\n",
      "epoch:16 step:15355 [D loss: 0.538552, acc.: 71.09%] [G loss: 1.345176]\n",
      "epoch:16 step:15356 [D loss: 0.594659, acc.: 64.84%] [G loss: 1.093055]\n",
      "epoch:16 step:15357 [D loss: 0.611851, acc.: 64.84%] [G loss: 1.136004]\n",
      "epoch:16 step:15358 [D loss: 0.618935, acc.: 62.50%] [G loss: 1.280908]\n",
      "epoch:16 step:15359 [D loss: 0.619914, acc.: 66.41%] [G loss: 1.078494]\n",
      "epoch:16 step:15360 [D loss: 0.591846, acc.: 67.19%] [G loss: 1.119110]\n",
      "epoch:16 step:15361 [D loss: 0.456755, acc.: 80.47%] [G loss: 1.176630]\n",
      "epoch:16 step:15362 [D loss: 0.622395, acc.: 60.94%] [G loss: 1.049582]\n",
      "epoch:16 step:15363 [D loss: 0.471016, acc.: 77.34%] [G loss: 1.140039]\n",
      "epoch:16 step:15364 [D loss: 0.533450, acc.: 68.75%] [G loss: 1.415304]\n",
      "epoch:16 step:15365 [D loss: 0.573937, acc.: 68.75%] [G loss: 1.285294]\n",
      "epoch:16 step:15366 [D loss: 0.783948, acc.: 53.91%] [G loss: 0.971847]\n",
      "epoch:16 step:15367 [D loss: 0.573905, acc.: 67.97%] [G loss: 1.167300]\n",
      "epoch:16 step:15368 [D loss: 0.686674, acc.: 60.16%] [G loss: 0.824494]\n",
      "epoch:16 step:15369 [D loss: 0.502196, acc.: 78.91%] [G loss: 1.220340]\n",
      "epoch:16 step:15370 [D loss: 0.696995, acc.: 59.38%] [G loss: 1.241583]\n",
      "epoch:16 step:15371 [D loss: 0.666452, acc.: 61.72%] [G loss: 1.095846]\n",
      "epoch:16 step:15372 [D loss: 0.692167, acc.: 59.38%] [G loss: 1.251956]\n",
      "epoch:16 step:15373 [D loss: 0.712230, acc.: 60.16%] [G loss: 1.287335]\n",
      "epoch:16 step:15374 [D loss: 0.643217, acc.: 62.50%] [G loss: 1.027069]\n",
      "epoch:16 step:15375 [D loss: 0.529211, acc.: 74.22%] [G loss: 1.208442]\n",
      "epoch:16 step:15376 [D loss: 0.658277, acc.: 60.94%] [G loss: 1.109561]\n",
      "epoch:16 step:15377 [D loss: 0.531691, acc.: 77.34%] [G loss: 1.181355]\n",
      "epoch:16 step:15378 [D loss: 0.539426, acc.: 73.44%] [G loss: 1.250190]\n",
      "epoch:16 step:15379 [D loss: 0.607527, acc.: 66.41%] [G loss: 1.210603]\n",
      "epoch:16 step:15380 [D loss: 0.579949, acc.: 73.44%] [G loss: 1.248347]\n",
      "epoch:16 step:15381 [D loss: 0.742581, acc.: 50.00%] [G loss: 1.157022]\n",
      "epoch:16 step:15382 [D loss: 0.722888, acc.: 57.81%] [G loss: 0.998168]\n",
      "epoch:16 step:15383 [D loss: 0.579280, acc.: 71.09%] [G loss: 0.955932]\n",
      "epoch:16 step:15384 [D loss: 0.468089, acc.: 78.91%] [G loss: 1.119649]\n",
      "epoch:16 step:15385 [D loss: 0.651920, acc.: 65.62%] [G loss: 1.264302]\n",
      "epoch:16 step:15386 [D loss: 0.637815, acc.: 64.06%] [G loss: 1.282145]\n",
      "epoch:16 step:15387 [D loss: 0.692936, acc.: 56.25%] [G loss: 1.261324]\n",
      "epoch:16 step:15388 [D loss: 0.693308, acc.: 56.25%] [G loss: 0.937190]\n",
      "epoch:16 step:15389 [D loss: 0.618159, acc.: 63.28%] [G loss: 1.105961]\n",
      "epoch:16 step:15390 [D loss: 0.522519, acc.: 74.22%] [G loss: 1.152148]\n",
      "epoch:16 step:15391 [D loss: 0.625963, acc.: 70.31%] [G loss: 1.135810]\n",
      "epoch:16 step:15392 [D loss: 0.467115, acc.: 82.81%] [G loss: 1.493627]\n",
      "epoch:16 step:15393 [D loss: 0.600078, acc.: 66.41%] [G loss: 1.150525]\n",
      "epoch:16 step:15394 [D loss: 0.606135, acc.: 66.41%] [G loss: 1.181087]\n",
      "epoch:16 step:15395 [D loss: 0.561888, acc.: 71.88%] [G loss: 0.972614]\n",
      "epoch:16 step:15396 [D loss: 0.555249, acc.: 70.31%] [G loss: 1.219860]\n",
      "epoch:16 step:15397 [D loss: 0.547889, acc.: 75.00%] [G loss: 1.363806]\n",
      "epoch:16 step:15398 [D loss: 0.549210, acc.: 68.75%] [G loss: 1.371942]\n",
      "epoch:16 step:15399 [D loss: 0.704437, acc.: 57.03%] [G loss: 1.350094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15400 [D loss: 0.498514, acc.: 73.44%] [G loss: 1.538267]\n",
      "##############\n",
      "[2.69859074 2.1356534  2.02490722 2.80352534 1.00432883 5.15348139\n",
      " 2.16106076 2.97821242 3.84798967 7.14868929]\n",
      "##########\n",
      "epoch:16 step:15401 [D loss: 0.653318, acc.: 62.50%] [G loss: 1.164056]\n",
      "epoch:16 step:15402 [D loss: 0.543373, acc.: 68.75%] [G loss: 1.280252]\n",
      "epoch:16 step:15403 [D loss: 0.434830, acc.: 82.81%] [G loss: 1.637425]\n",
      "epoch:16 step:15404 [D loss: 0.612696, acc.: 66.41%] [G loss: 0.989999]\n",
      "epoch:16 step:15405 [D loss: 0.668106, acc.: 64.84%] [G loss: 1.208154]\n",
      "epoch:16 step:15406 [D loss: 0.508169, acc.: 79.69%] [G loss: 1.257209]\n",
      "epoch:16 step:15407 [D loss: 0.583249, acc.: 66.41%] [G loss: 1.205839]\n",
      "epoch:16 step:15408 [D loss: 0.722835, acc.: 59.38%] [G loss: 1.136305]\n",
      "epoch:16 step:15409 [D loss: 0.572514, acc.: 67.97%] [G loss: 1.428806]\n",
      "epoch:16 step:15410 [D loss: 0.561114, acc.: 74.22%] [G loss: 1.242780]\n",
      "epoch:16 step:15411 [D loss: 0.546273, acc.: 76.56%] [G loss: 1.566647]\n",
      "epoch:16 step:15412 [D loss: 0.474933, acc.: 82.03%] [G loss: 1.556705]\n",
      "epoch:16 step:15413 [D loss: 0.702087, acc.: 50.00%] [G loss: 1.000673]\n",
      "epoch:16 step:15414 [D loss: 0.508620, acc.: 75.78%] [G loss: 1.520385]\n",
      "epoch:16 step:15415 [D loss: 0.605746, acc.: 67.97%] [G loss: 1.375683]\n",
      "epoch:16 step:15416 [D loss: 0.581927, acc.: 68.75%] [G loss: 1.028820]\n",
      "epoch:16 step:15417 [D loss: 0.720527, acc.: 53.91%] [G loss: 1.163522]\n",
      "epoch:16 step:15418 [D loss: 0.555823, acc.: 72.66%] [G loss: 1.152578]\n",
      "epoch:16 step:15419 [D loss: 0.665477, acc.: 64.84%] [G loss: 0.997814]\n",
      "epoch:16 step:15420 [D loss: 0.635776, acc.: 60.16%] [G loss: 1.336092]\n",
      "epoch:16 step:15421 [D loss: 0.643466, acc.: 62.50%] [G loss: 1.439072]\n",
      "epoch:16 step:15422 [D loss: 0.633036, acc.: 60.16%] [G loss: 1.382300]\n",
      "epoch:16 step:15423 [D loss: 0.609656, acc.: 63.28%] [G loss: 1.112705]\n",
      "epoch:16 step:15424 [D loss: 0.605399, acc.: 67.97%] [G loss: 1.336154]\n",
      "epoch:16 step:15425 [D loss: 0.549816, acc.: 75.00%] [G loss: 1.097053]\n",
      "epoch:16 step:15426 [D loss: 0.658247, acc.: 62.50%] [G loss: 1.159975]\n",
      "epoch:16 step:15427 [D loss: 0.492543, acc.: 78.12%] [G loss: 1.463226]\n",
      "epoch:16 step:15428 [D loss: 0.538409, acc.: 75.78%] [G loss: 1.415274]\n",
      "epoch:16 step:15429 [D loss: 0.636469, acc.: 59.38%] [G loss: 1.088229]\n",
      "epoch:16 step:15430 [D loss: 0.567002, acc.: 68.75%] [G loss: 1.524441]\n",
      "epoch:16 step:15431 [D loss: 0.504850, acc.: 75.78%] [G loss: 1.185846]\n",
      "epoch:16 step:15432 [D loss: 0.556524, acc.: 69.53%] [G loss: 1.251740]\n",
      "epoch:16 step:15433 [D loss: 0.507364, acc.: 78.91%] [G loss: 1.384867]\n",
      "epoch:16 step:15434 [D loss: 0.695358, acc.: 60.94%] [G loss: 1.271826]\n",
      "epoch:16 step:15435 [D loss: 0.469841, acc.: 78.91%] [G loss: 1.547249]\n",
      "epoch:16 step:15436 [D loss: 0.570811, acc.: 71.88%] [G loss: 1.271578]\n",
      "epoch:16 step:15437 [D loss: 0.692818, acc.: 62.50%] [G loss: 1.171397]\n",
      "epoch:16 step:15438 [D loss: 0.595828, acc.: 66.41%] [G loss: 1.531158]\n",
      "epoch:16 step:15439 [D loss: 0.467881, acc.: 79.69%] [G loss: 1.288671]\n",
      "epoch:16 step:15440 [D loss: 0.601097, acc.: 64.06%] [G loss: 1.535518]\n",
      "epoch:16 step:15441 [D loss: 0.522469, acc.: 74.22%] [G loss: 1.373550]\n",
      "epoch:16 step:15442 [D loss: 0.765470, acc.: 52.34%] [G loss: 1.081057]\n",
      "epoch:16 step:15443 [D loss: 0.536929, acc.: 74.22%] [G loss: 1.184498]\n",
      "epoch:16 step:15444 [D loss: 0.645194, acc.: 62.50%] [G loss: 0.851041]\n",
      "epoch:16 step:15445 [D loss: 0.515477, acc.: 75.78%] [G loss: 1.470142]\n",
      "epoch:16 step:15446 [D loss: 0.621074, acc.: 64.84%] [G loss: 1.364566]\n",
      "epoch:16 step:15447 [D loss: 0.545714, acc.: 74.22%] [G loss: 1.427956]\n",
      "epoch:16 step:15448 [D loss: 0.595462, acc.: 65.62%] [G loss: 1.383218]\n",
      "epoch:16 step:15449 [D loss: 0.636679, acc.: 65.62%] [G loss: 1.088849]\n",
      "epoch:16 step:15450 [D loss: 0.615113, acc.: 67.19%] [G loss: 1.080904]\n",
      "epoch:16 step:15451 [D loss: 0.370173, acc.: 90.62%] [G loss: 1.430091]\n",
      "epoch:16 step:15452 [D loss: 0.466894, acc.: 79.69%] [G loss: 1.197667]\n",
      "epoch:16 step:15453 [D loss: 0.589703, acc.: 69.53%] [G loss: 1.363513]\n",
      "epoch:16 step:15454 [D loss: 0.570042, acc.: 73.44%] [G loss: 1.174584]\n",
      "epoch:16 step:15455 [D loss: 0.537618, acc.: 73.44%] [G loss: 1.178476]\n",
      "epoch:16 step:15456 [D loss: 0.527102, acc.: 79.69%] [G loss: 1.099160]\n",
      "epoch:16 step:15457 [D loss: 0.673985, acc.: 57.03%] [G loss: 1.590733]\n",
      "epoch:16 step:15458 [D loss: 0.499110, acc.: 76.56%] [G loss: 1.519518]\n",
      "epoch:16 step:15459 [D loss: 0.432297, acc.: 80.47%] [G loss: 1.371384]\n",
      "epoch:16 step:15460 [D loss: 0.472592, acc.: 78.12%] [G loss: 1.403548]\n",
      "epoch:16 step:15461 [D loss: 0.447519, acc.: 85.16%] [G loss: 1.627416]\n",
      "epoch:16 step:15462 [D loss: 0.640293, acc.: 67.19%] [G loss: 1.149753]\n",
      "epoch:16 step:15463 [D loss: 0.586687, acc.: 70.31%] [G loss: 1.412866]\n",
      "epoch:16 step:15464 [D loss: 0.459432, acc.: 79.69%] [G loss: 1.280145]\n",
      "epoch:16 step:15465 [D loss: 0.403213, acc.: 87.50%] [G loss: 1.251813]\n",
      "epoch:16 step:15466 [D loss: 0.508568, acc.: 76.56%] [G loss: 1.440738]\n",
      "epoch:16 step:15467 [D loss: 0.583684, acc.: 65.62%] [G loss: 1.372391]\n",
      "epoch:16 step:15468 [D loss: 0.443004, acc.: 82.81%] [G loss: 1.214155]\n",
      "epoch:16 step:15469 [D loss: 0.648486, acc.: 65.62%] [G loss: 1.376077]\n",
      "epoch:16 step:15470 [D loss: 0.540681, acc.: 71.09%] [G loss: 1.185880]\n",
      "epoch:16 step:15471 [D loss: 0.538409, acc.: 73.44%] [G loss: 1.245695]\n",
      "epoch:16 step:15472 [D loss: 0.622004, acc.: 60.16%] [G loss: 1.312431]\n",
      "epoch:16 step:15473 [D loss: 0.598290, acc.: 64.84%] [G loss: 1.315791]\n",
      "epoch:16 step:15474 [D loss: 0.732071, acc.: 59.38%] [G loss: 1.119340]\n",
      "epoch:16 step:15475 [D loss: 0.678626, acc.: 61.72%] [G loss: 1.423135]\n",
      "epoch:16 step:15476 [D loss: 0.476912, acc.: 80.47%] [G loss: 1.233244]\n",
      "epoch:16 step:15477 [D loss: 0.568350, acc.: 73.44%] [G loss: 1.433938]\n",
      "epoch:16 step:15478 [D loss: 0.519409, acc.: 77.34%] [G loss: 1.583683]\n",
      "epoch:16 step:15479 [D loss: 0.455088, acc.: 79.69%] [G loss: 1.611161]\n",
      "epoch:16 step:15480 [D loss: 0.505362, acc.: 77.34%] [G loss: 1.721114]\n",
      "epoch:16 step:15481 [D loss: 0.648957, acc.: 60.16%] [G loss: 1.124163]\n",
      "epoch:16 step:15482 [D loss: 0.563402, acc.: 71.09%] [G loss: 1.258192]\n",
      "epoch:16 step:15483 [D loss: 0.539419, acc.: 68.75%] [G loss: 1.437626]\n",
      "epoch:16 step:15484 [D loss: 0.555038, acc.: 71.09%] [G loss: 1.171112]\n",
      "epoch:16 step:15485 [D loss: 0.547934, acc.: 75.78%] [G loss: 1.334287]\n",
      "epoch:16 step:15486 [D loss: 0.640369, acc.: 65.62%] [G loss: 1.015581]\n",
      "epoch:16 step:15487 [D loss: 0.559175, acc.: 68.75%] [G loss: 1.157839]\n",
      "epoch:16 step:15488 [D loss: 0.484390, acc.: 79.69%] [G loss: 1.141317]\n",
      "epoch:16 step:15489 [D loss: 0.610726, acc.: 67.19%] [G loss: 1.557010]\n",
      "epoch:16 step:15490 [D loss: 0.579112, acc.: 69.53%] [G loss: 1.387936]\n",
      "epoch:16 step:15491 [D loss: 0.599012, acc.: 67.19%] [G loss: 1.318180]\n",
      "epoch:16 step:15492 [D loss: 0.632496, acc.: 64.06%] [G loss: 1.337170]\n",
      "epoch:16 step:15493 [D loss: 0.625311, acc.: 67.19%] [G loss: 1.405039]\n",
      "epoch:16 step:15494 [D loss: 0.493229, acc.: 75.78%] [G loss: 1.270066]\n",
      "epoch:16 step:15495 [D loss: 0.565311, acc.: 70.31%] [G loss: 1.377620]\n",
      "epoch:16 step:15496 [D loss: 0.607600, acc.: 65.62%] [G loss: 1.271339]\n",
      "epoch:16 step:15497 [D loss: 0.429546, acc.: 85.94%] [G loss: 1.471592]\n",
      "epoch:16 step:15498 [D loss: 0.416103, acc.: 82.03%] [G loss: 1.402326]\n",
      "epoch:16 step:15499 [D loss: 0.545757, acc.: 74.22%] [G loss: 1.402002]\n",
      "epoch:16 step:15500 [D loss: 0.643585, acc.: 65.62%] [G loss: 1.179446]\n",
      "epoch:16 step:15501 [D loss: 0.423848, acc.: 82.03%] [G loss: 1.408904]\n",
      "epoch:16 step:15502 [D loss: 0.581243, acc.: 65.62%] [G loss: 1.471058]\n",
      "epoch:16 step:15503 [D loss: 0.420674, acc.: 84.38%] [G loss: 1.175850]\n",
      "epoch:16 step:15504 [D loss: 0.553570, acc.: 74.22%] [G loss: 1.284991]\n",
      "epoch:16 step:15505 [D loss: 0.482076, acc.: 82.81%] [G loss: 1.561467]\n",
      "epoch:16 step:15506 [D loss: 0.553020, acc.: 66.41%] [G loss: 1.347113]\n",
      "epoch:16 step:15507 [D loss: 0.457720, acc.: 81.25%] [G loss: 1.648792]\n",
      "epoch:16 step:15508 [D loss: 0.462079, acc.: 82.81%] [G loss: 1.661843]\n",
      "epoch:16 step:15509 [D loss: 0.656336, acc.: 57.03%] [G loss: 1.350487]\n",
      "epoch:16 step:15510 [D loss: 0.550795, acc.: 71.09%] [G loss: 1.358339]\n",
      "epoch:16 step:15511 [D loss: 0.510288, acc.: 82.03%] [G loss: 1.640488]\n",
      "epoch:16 step:15512 [D loss: 0.574470, acc.: 67.97%] [G loss: 1.359941]\n",
      "epoch:16 step:15513 [D loss: 0.562882, acc.: 67.19%] [G loss: 1.507883]\n",
      "epoch:16 step:15514 [D loss: 0.673985, acc.: 58.59%] [G loss: 1.437705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15515 [D loss: 0.502065, acc.: 76.56%] [G loss: 1.336510]\n",
      "epoch:16 step:15516 [D loss: 0.518241, acc.: 75.78%] [G loss: 1.217881]\n",
      "epoch:16 step:15517 [D loss: 0.474443, acc.: 84.38%] [G loss: 1.559493]\n",
      "epoch:16 step:15518 [D loss: 0.609695, acc.: 68.75%] [G loss: 1.310931]\n",
      "epoch:16 step:15519 [D loss: 0.642195, acc.: 62.50%] [G loss: 1.270685]\n",
      "epoch:16 step:15520 [D loss: 0.525940, acc.: 72.66%] [G loss: 1.239311]\n",
      "epoch:16 step:15521 [D loss: 0.452283, acc.: 82.81%] [G loss: 1.546717]\n",
      "epoch:16 step:15522 [D loss: 0.589744, acc.: 67.19%] [G loss: 1.123783]\n",
      "epoch:16 step:15523 [D loss: 0.505063, acc.: 76.56%] [G loss: 1.285558]\n",
      "epoch:16 step:15524 [D loss: 0.680921, acc.: 57.81%] [G loss: 1.216860]\n",
      "epoch:16 step:15525 [D loss: 0.662674, acc.: 63.28%] [G loss: 1.414408]\n",
      "epoch:16 step:15526 [D loss: 0.554143, acc.: 68.75%] [G loss: 1.361571]\n",
      "epoch:16 step:15527 [D loss: 0.541166, acc.: 75.00%] [G loss: 1.352039]\n",
      "epoch:16 step:15528 [D loss: 0.560123, acc.: 67.97%] [G loss: 1.628444]\n",
      "epoch:16 step:15529 [D loss: 0.755092, acc.: 55.47%] [G loss: 1.310455]\n",
      "epoch:16 step:15530 [D loss: 0.547815, acc.: 73.44%] [G loss: 0.962713]\n",
      "epoch:16 step:15531 [D loss: 0.583192, acc.: 72.66%] [G loss: 1.099494]\n",
      "epoch:16 step:15532 [D loss: 0.467350, acc.: 78.12%] [G loss: 1.507111]\n",
      "epoch:16 step:15533 [D loss: 0.529221, acc.: 73.44%] [G loss: 1.218449]\n",
      "epoch:16 step:15534 [D loss: 0.503838, acc.: 78.91%] [G loss: 1.379189]\n",
      "epoch:16 step:15535 [D loss: 0.622381, acc.: 65.62%] [G loss: 1.177603]\n",
      "epoch:16 step:15536 [D loss: 0.606759, acc.: 62.50%] [G loss: 1.248618]\n",
      "epoch:16 step:15537 [D loss: 0.594597, acc.: 71.09%] [G loss: 1.270931]\n",
      "epoch:16 step:15538 [D loss: 0.569852, acc.: 71.09%] [G loss: 1.260100]\n",
      "epoch:16 step:15539 [D loss: 0.600566, acc.: 67.19%] [G loss: 1.194842]\n",
      "epoch:16 step:15540 [D loss: 0.532482, acc.: 76.56%] [G loss: 0.922193]\n",
      "epoch:16 step:15541 [D loss: 0.597894, acc.: 65.62%] [G loss: 1.318558]\n",
      "epoch:16 step:15542 [D loss: 0.465042, acc.: 84.38%] [G loss: 1.364067]\n",
      "epoch:16 step:15543 [D loss: 0.539303, acc.: 75.00%] [G loss: 1.334077]\n",
      "epoch:16 step:15544 [D loss: 0.552604, acc.: 77.34%] [G loss: 1.372542]\n",
      "epoch:16 step:15545 [D loss: 0.612120, acc.: 68.75%] [G loss: 1.247944]\n",
      "epoch:16 step:15546 [D loss: 0.595739, acc.: 67.97%] [G loss: 1.236236]\n",
      "epoch:16 step:15547 [D loss: 0.457136, acc.: 78.91%] [G loss: 1.265979]\n",
      "epoch:16 step:15548 [D loss: 0.646120, acc.: 63.28%] [G loss: 1.091102]\n",
      "epoch:16 step:15549 [D loss: 0.481010, acc.: 78.91%] [G loss: 1.236990]\n",
      "epoch:16 step:15550 [D loss: 0.562276, acc.: 75.00%] [G loss: 1.242225]\n",
      "epoch:16 step:15551 [D loss: 0.512995, acc.: 73.44%] [G loss: 1.854046]\n",
      "epoch:16 step:15552 [D loss: 0.495410, acc.: 78.12%] [G loss: 1.233953]\n",
      "epoch:16 step:15553 [D loss: 0.639793, acc.: 65.62%] [G loss: 1.392752]\n",
      "epoch:16 step:15554 [D loss: 0.484330, acc.: 79.69%] [G loss: 1.440689]\n",
      "epoch:16 step:15555 [D loss: 0.503923, acc.: 77.34%] [G loss: 1.309620]\n",
      "epoch:16 step:15556 [D loss: 0.451770, acc.: 82.81%] [G loss: 1.172514]\n",
      "epoch:16 step:15557 [D loss: 0.657496, acc.: 61.72%] [G loss: 1.599612]\n",
      "epoch:16 step:15558 [D loss: 0.598438, acc.: 69.53%] [G loss: 1.378147]\n",
      "epoch:16 step:15559 [D loss: 0.525571, acc.: 71.88%] [G loss: 1.304769]\n",
      "epoch:16 step:15560 [D loss: 0.678683, acc.: 59.38%] [G loss: 1.197979]\n",
      "epoch:16 step:15561 [D loss: 0.746623, acc.: 56.25%] [G loss: 1.047868]\n",
      "epoch:16 step:15562 [D loss: 0.461402, acc.: 79.69%] [G loss: 1.285515]\n",
      "epoch:16 step:15563 [D loss: 0.566764, acc.: 69.53%] [G loss: 1.093449]\n",
      "epoch:16 step:15564 [D loss: 0.589444, acc.: 68.75%] [G loss: 1.077050]\n",
      "epoch:16 step:15565 [D loss: 0.576301, acc.: 67.19%] [G loss: 1.181228]\n",
      "epoch:16 step:15566 [D loss: 0.560053, acc.: 74.22%] [G loss: 1.105049]\n",
      "epoch:16 step:15567 [D loss: 0.577466, acc.: 68.75%] [G loss: 1.347106]\n",
      "epoch:16 step:15568 [D loss: 0.439481, acc.: 79.69%] [G loss: 1.743115]\n",
      "epoch:16 step:15569 [D loss: 0.511387, acc.: 76.56%] [G loss: 1.205418]\n",
      "epoch:16 step:15570 [D loss: 0.645963, acc.: 60.94%] [G loss: 1.216233]\n",
      "epoch:16 step:15571 [D loss: 0.605079, acc.: 64.84%] [G loss: 1.389799]\n",
      "epoch:16 step:15572 [D loss: 0.521198, acc.: 79.69%] [G loss: 1.419923]\n",
      "epoch:16 step:15573 [D loss: 0.589298, acc.: 68.75%] [G loss: 0.820251]\n",
      "epoch:16 step:15574 [D loss: 0.631743, acc.: 60.94%] [G loss: 1.303461]\n",
      "epoch:16 step:15575 [D loss: 0.610024, acc.: 62.50%] [G loss: 1.241622]\n",
      "epoch:16 step:15576 [D loss: 0.579233, acc.: 67.19%] [G loss: 1.323874]\n",
      "epoch:16 step:15577 [D loss: 0.515727, acc.: 76.56%] [G loss: 1.428123]\n",
      "epoch:16 step:15578 [D loss: 0.569618, acc.: 69.53%] [G loss: 1.407647]\n",
      "epoch:16 step:15579 [D loss: 0.601858, acc.: 64.06%] [G loss: 1.523793]\n",
      "epoch:16 step:15580 [D loss: 0.655775, acc.: 63.28%] [G loss: 1.439389]\n",
      "epoch:16 step:15581 [D loss: 0.593822, acc.: 64.84%] [G loss: 1.378653]\n",
      "epoch:16 step:15582 [D loss: 0.483979, acc.: 78.12%] [G loss: 1.127261]\n",
      "epoch:16 step:15583 [D loss: 0.498709, acc.: 77.34%] [G loss: 1.085850]\n",
      "epoch:16 step:15584 [D loss: 0.418512, acc.: 84.38%] [G loss: 1.331669]\n",
      "epoch:16 step:15585 [D loss: 0.794123, acc.: 50.78%] [G loss: 1.064541]\n",
      "epoch:16 step:15586 [D loss: 0.564502, acc.: 73.44%] [G loss: 1.336329]\n",
      "epoch:16 step:15587 [D loss: 0.580567, acc.: 66.41%] [G loss: 1.509231]\n",
      "epoch:16 step:15588 [D loss: 0.617986, acc.: 65.62%] [G loss: 1.460979]\n",
      "epoch:16 step:15589 [D loss: 0.547625, acc.: 72.66%] [G loss: 1.168899]\n",
      "epoch:16 step:15590 [D loss: 0.591740, acc.: 67.19%] [G loss: 1.597703]\n",
      "epoch:16 step:15591 [D loss: 0.597522, acc.: 68.75%] [G loss: 1.488924]\n",
      "epoch:16 step:15592 [D loss: 0.539099, acc.: 72.66%] [G loss: 1.262368]\n",
      "epoch:16 step:15593 [D loss: 0.586535, acc.: 66.41%] [G loss: 1.434007]\n",
      "epoch:16 step:15594 [D loss: 0.524887, acc.: 77.34%] [G loss: 1.296426]\n",
      "epoch:16 step:15595 [D loss: 0.673610, acc.: 59.38%] [G loss: 1.055658]\n",
      "epoch:16 step:15596 [D loss: 0.697799, acc.: 60.94%] [G loss: 1.419890]\n",
      "epoch:16 step:15597 [D loss: 0.640744, acc.: 60.16%] [G loss: 1.596970]\n",
      "epoch:16 step:15598 [D loss: 0.610370, acc.: 69.53%] [G loss: 1.305453]\n",
      "epoch:16 step:15599 [D loss: 0.431751, acc.: 85.16%] [G loss: 1.331638]\n",
      "epoch:16 step:15600 [D loss: 0.608043, acc.: 64.84%] [G loss: 1.650847]\n",
      "##############\n",
      "[2.76078981 2.26947466 1.99878429 2.94077803 1.08315798 5.9299528\n",
      " 2.42766531 2.76722023 4.04558116 8.14868929]\n",
      "##########\n",
      "epoch:16 step:15601 [D loss: 0.510808, acc.: 79.69%] [G loss: 1.052604]\n",
      "epoch:16 step:15602 [D loss: 0.549503, acc.: 75.78%] [G loss: 1.243956]\n",
      "epoch:16 step:15603 [D loss: 0.545662, acc.: 73.44%] [G loss: 1.274600]\n",
      "epoch:16 step:15604 [D loss: 0.577370, acc.: 67.97%] [G loss: 1.407669]\n",
      "epoch:16 step:15605 [D loss: 0.592379, acc.: 62.50%] [G loss: 1.306722]\n",
      "epoch:16 step:15606 [D loss: 0.765635, acc.: 50.78%] [G loss: 1.221190]\n",
      "epoch:16 step:15607 [D loss: 0.638359, acc.: 60.16%] [G loss: 1.762317]\n",
      "epoch:16 step:15608 [D loss: 0.585203, acc.: 67.97%] [G loss: 1.479226]\n",
      "epoch:16 step:15609 [D loss: 0.633682, acc.: 62.50%] [G loss: 1.211399]\n",
      "epoch:16 step:15610 [D loss: 0.640090, acc.: 63.28%] [G loss: 1.186633]\n",
      "epoch:16 step:15611 [D loss: 0.590097, acc.: 71.88%] [G loss: 1.039072]\n",
      "epoch:16 step:15612 [D loss: 0.564382, acc.: 72.66%] [G loss: 1.130272]\n",
      "epoch:16 step:15613 [D loss: 0.620140, acc.: 68.75%] [G loss: 1.145673]\n",
      "epoch:16 step:15614 [D loss: 0.636728, acc.: 65.62%] [G loss: 1.108032]\n",
      "epoch:16 step:15615 [D loss: 0.588577, acc.: 70.31%] [G loss: 1.109455]\n",
      "epoch:16 step:15616 [D loss: 0.662925, acc.: 60.16%] [G loss: 1.268189]\n",
      "epoch:16 step:15617 [D loss: 0.415816, acc.: 84.38%] [G loss: 1.619277]\n",
      "epoch:16 step:15618 [D loss: 0.391456, acc.: 85.16%] [G loss: 1.449537]\n",
      "epoch:16 step:15619 [D loss: 0.440038, acc.: 84.38%] [G loss: 1.565372]\n",
      "epoch:16 step:15620 [D loss: 0.559853, acc.: 72.66%] [G loss: 1.112487]\n",
      "epoch:16 step:15621 [D loss: 0.581737, acc.: 69.53%] [G loss: 1.318494]\n",
      "epoch:16 step:15622 [D loss: 0.642036, acc.: 57.03%] [G loss: 1.078906]\n",
      "epoch:16 step:15623 [D loss: 0.558336, acc.: 67.19%] [G loss: 1.212259]\n",
      "epoch:16 step:15624 [D loss: 0.561242, acc.: 72.66%] [G loss: 1.337754]\n",
      "epoch:16 step:15625 [D loss: 0.551766, acc.: 71.09%] [G loss: 1.442723]\n",
      "epoch:16 step:15626 [D loss: 0.609004, acc.: 64.06%] [G loss: 1.249616]\n",
      "epoch:16 step:15627 [D loss: 0.643562, acc.: 64.84%] [G loss: 1.230447]\n",
      "epoch:16 step:15628 [D loss: 0.579751, acc.: 69.53%] [G loss: 1.441806]\n",
      "epoch:16 step:15629 [D loss: 0.483638, acc.: 82.03%] [G loss: 1.629670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15630 [D loss: 0.644591, acc.: 65.62%] [G loss: 1.347069]\n",
      "epoch:16 step:15631 [D loss: 0.538181, acc.: 77.34%] [G loss: 1.209853]\n",
      "epoch:16 step:15632 [D loss: 0.692314, acc.: 58.59%] [G loss: 1.096933]\n",
      "epoch:16 step:15633 [D loss: 0.596417, acc.: 64.06%] [G loss: 1.366277]\n",
      "epoch:16 step:15634 [D loss: 0.626874, acc.: 65.62%] [G loss: 1.115666]\n",
      "epoch:16 step:15635 [D loss: 0.818334, acc.: 50.00%] [G loss: 1.486763]\n",
      "epoch:16 step:15636 [D loss: 0.605524, acc.: 70.31%] [G loss: 1.271252]\n",
      "epoch:16 step:15637 [D loss: 0.544437, acc.: 70.31%] [G loss: 1.249205]\n",
      "epoch:16 step:15638 [D loss: 0.699008, acc.: 55.47%] [G loss: 0.934595]\n",
      "epoch:16 step:15639 [D loss: 0.557278, acc.: 69.53%] [G loss: 1.132631]\n",
      "epoch:16 step:15640 [D loss: 0.507079, acc.: 73.44%] [G loss: 1.234272]\n",
      "epoch:16 step:15641 [D loss: 0.549806, acc.: 73.44%] [G loss: 1.156479]\n",
      "epoch:16 step:15642 [D loss: 0.579392, acc.: 71.88%] [G loss: 1.345242]\n",
      "epoch:16 step:15643 [D loss: 0.536703, acc.: 76.56%] [G loss: 1.380094]\n",
      "epoch:16 step:15644 [D loss: 0.499158, acc.: 75.78%] [G loss: 1.527958]\n",
      "epoch:16 step:15645 [D loss: 0.571416, acc.: 69.53%] [G loss: 1.406792]\n",
      "epoch:16 step:15646 [D loss: 0.576401, acc.: 72.66%] [G loss: 1.358476]\n",
      "epoch:16 step:15647 [D loss: 0.593240, acc.: 69.53%] [G loss: 1.484821]\n",
      "epoch:16 step:15648 [D loss: 0.452545, acc.: 82.03%] [G loss: 1.164712]\n",
      "epoch:16 step:15649 [D loss: 0.603986, acc.: 67.19%] [G loss: 1.196369]\n",
      "epoch:16 step:15650 [D loss: 0.724805, acc.: 57.03%] [G loss: 1.294704]\n",
      "epoch:16 step:15651 [D loss: 0.590714, acc.: 68.75%] [G loss: 1.288304]\n",
      "epoch:16 step:15652 [D loss: 0.641136, acc.: 64.84%] [G loss: 1.066055]\n",
      "epoch:16 step:15653 [D loss: 0.536955, acc.: 75.78%] [G loss: 1.405272]\n",
      "epoch:16 step:15654 [D loss: 0.607272, acc.: 67.19%] [G loss: 1.232263]\n",
      "epoch:16 step:15655 [D loss: 0.560889, acc.: 75.78%] [G loss: 1.275840]\n",
      "epoch:16 step:15656 [D loss: 0.535267, acc.: 75.78%] [G loss: 1.208576]\n",
      "epoch:16 step:15657 [D loss: 0.538735, acc.: 73.44%] [G loss: 1.356278]\n",
      "epoch:16 step:15658 [D loss: 0.497714, acc.: 75.78%] [G loss: 1.402092]\n",
      "epoch:16 step:15659 [D loss: 0.564651, acc.: 68.75%] [G loss: 1.098229]\n",
      "epoch:16 step:15660 [D loss: 0.714380, acc.: 64.06%] [G loss: 0.898650]\n",
      "epoch:16 step:15661 [D loss: 0.491072, acc.: 82.81%] [G loss: 1.339259]\n",
      "epoch:16 step:15662 [D loss: 0.469452, acc.: 78.12%] [G loss: 1.261765]\n",
      "epoch:16 step:15663 [D loss: 0.592511, acc.: 69.53%] [G loss: 1.273845]\n",
      "epoch:16 step:15664 [D loss: 0.535583, acc.: 71.09%] [G loss: 1.557678]\n",
      "epoch:16 step:15665 [D loss: 0.691171, acc.: 61.72%] [G loss: 1.038854]\n",
      "epoch:16 step:15666 [D loss: 0.521215, acc.: 72.66%] [G loss: 1.155405]\n",
      "epoch:16 step:15667 [D loss: 0.799417, acc.: 46.09%] [G loss: 1.129752]\n",
      "epoch:16 step:15668 [D loss: 0.550590, acc.: 70.31%] [G loss: 1.252638]\n",
      "epoch:16 step:15669 [D loss: 0.604939, acc.: 64.84%] [G loss: 1.225735]\n",
      "epoch:16 step:15670 [D loss: 0.669089, acc.: 60.94%] [G loss: 1.498411]\n",
      "epoch:16 step:15671 [D loss: 0.671308, acc.: 62.50%] [G loss: 1.290888]\n",
      "epoch:16 step:15672 [D loss: 0.541928, acc.: 75.78%] [G loss: 1.471104]\n",
      "epoch:16 step:15673 [D loss: 0.639134, acc.: 67.19%] [G loss: 1.130216]\n",
      "epoch:16 step:15674 [D loss: 0.654105, acc.: 64.06%] [G loss: 1.280843]\n",
      "epoch:16 step:15675 [D loss: 0.559732, acc.: 70.31%] [G loss: 1.488855]\n",
      "epoch:16 step:15676 [D loss: 0.561722, acc.: 73.44%] [G loss: 1.373303]\n",
      "epoch:16 step:15677 [D loss: 0.644903, acc.: 62.50%] [G loss: 1.275077]\n",
      "epoch:16 step:15678 [D loss: 0.547332, acc.: 69.53%] [G loss: 1.148013]\n",
      "epoch:16 step:15679 [D loss: 0.493024, acc.: 77.34%] [G loss: 1.125405]\n",
      "epoch:16 step:15680 [D loss: 0.581641, acc.: 67.19%] [G loss: 1.207359]\n",
      "epoch:16 step:15681 [D loss: 0.550282, acc.: 75.78%] [G loss: 1.221967]\n",
      "epoch:16 step:15682 [D loss: 0.532457, acc.: 72.66%] [G loss: 1.177050]\n",
      "epoch:16 step:15683 [D loss: 0.623787, acc.: 60.94%] [G loss: 1.134296]\n",
      "epoch:16 step:15684 [D loss: 0.509621, acc.: 75.00%] [G loss: 1.320884]\n",
      "epoch:16 step:15685 [D loss: 0.526188, acc.: 71.09%] [G loss: 1.162934]\n",
      "epoch:16 step:15686 [D loss: 0.614163, acc.: 65.62%] [G loss: 1.541734]\n",
      "epoch:16 step:15687 [D loss: 0.552362, acc.: 75.78%] [G loss: 0.927818]\n",
      "epoch:16 step:15688 [D loss: 0.676614, acc.: 61.72%] [G loss: 1.358039]\n",
      "epoch:16 step:15689 [D loss: 0.566547, acc.: 68.75%] [G loss: 1.277833]\n",
      "epoch:16 step:15690 [D loss: 0.660643, acc.: 64.84%] [G loss: 1.167002]\n",
      "epoch:16 step:15691 [D loss: 0.640610, acc.: 67.97%] [G loss: 1.062672]\n",
      "epoch:16 step:15692 [D loss: 0.603527, acc.: 67.19%] [G loss: 1.246587]\n",
      "epoch:16 step:15693 [D loss: 0.398101, acc.: 85.16%] [G loss: 1.314809]\n",
      "epoch:16 step:15694 [D loss: 0.579270, acc.: 66.41%] [G loss: 0.927194]\n",
      "epoch:16 step:15695 [D loss: 0.630205, acc.: 64.06%] [G loss: 1.026672]\n",
      "epoch:16 step:15696 [D loss: 0.550376, acc.: 71.09%] [G loss: 1.061990]\n",
      "epoch:16 step:15697 [D loss: 0.513655, acc.: 67.19%] [G loss: 1.347992]\n",
      "epoch:16 step:15698 [D loss: 0.748438, acc.: 50.78%] [G loss: 0.914163]\n",
      "epoch:16 step:15699 [D loss: 0.603252, acc.: 67.19%] [G loss: 1.309834]\n",
      "epoch:16 step:15700 [D loss: 0.716542, acc.: 55.47%] [G loss: 1.289843]\n",
      "epoch:16 step:15701 [D loss: 0.881779, acc.: 46.88%] [G loss: 1.374734]\n",
      "epoch:16 step:15702 [D loss: 0.540320, acc.: 78.12%] [G loss: 1.318236]\n",
      "epoch:16 step:15703 [D loss: 0.740316, acc.: 53.91%] [G loss: 0.973686]\n",
      "epoch:16 step:15704 [D loss: 0.672285, acc.: 61.72%] [G loss: 1.280609]\n",
      "epoch:16 step:15705 [D loss: 0.594284, acc.: 62.50%] [G loss: 1.663625]\n",
      "epoch:16 step:15706 [D loss: 0.637857, acc.: 64.84%] [G loss: 1.462967]\n",
      "epoch:16 step:15707 [D loss: 0.755051, acc.: 57.03%] [G loss: 1.303802]\n",
      "epoch:16 step:15708 [D loss: 0.420101, acc.: 82.81%] [G loss: 1.410582]\n",
      "epoch:16 step:15709 [D loss: 0.681020, acc.: 56.25%] [G loss: 1.258110]\n",
      "epoch:16 step:15710 [D loss: 0.479810, acc.: 75.78%] [G loss: 1.249468]\n",
      "epoch:16 step:15711 [D loss: 0.593953, acc.: 71.09%] [G loss: 1.310901]\n",
      "epoch:16 step:15712 [D loss: 0.599042, acc.: 68.75%] [G loss: 1.413188]\n",
      "epoch:16 step:15713 [D loss: 0.723396, acc.: 55.47%] [G loss: 1.142508]\n",
      "epoch:16 step:15714 [D loss: 0.692395, acc.: 61.72%] [G loss: 1.222057]\n",
      "epoch:16 step:15715 [D loss: 0.754157, acc.: 51.56%] [G loss: 1.223977]\n",
      "epoch:16 step:15716 [D loss: 0.413976, acc.: 86.72%] [G loss: 1.496357]\n",
      "epoch:16 step:15717 [D loss: 0.759598, acc.: 53.91%] [G loss: 1.358786]\n",
      "epoch:16 step:15718 [D loss: 0.617053, acc.: 64.84%] [G loss: 1.263008]\n",
      "epoch:16 step:15719 [D loss: 0.567610, acc.: 67.97%] [G loss: 1.300267]\n",
      "epoch:16 step:15720 [D loss: 0.684401, acc.: 62.50%] [G loss: 1.464134]\n",
      "epoch:16 step:15721 [D loss: 0.607906, acc.: 64.06%] [G loss: 1.161693]\n",
      "epoch:16 step:15722 [D loss: 0.587662, acc.: 64.84%] [G loss: 1.336006]\n",
      "epoch:16 step:15723 [D loss: 0.609084, acc.: 62.50%] [G loss: 1.147527]\n",
      "epoch:16 step:15724 [D loss: 0.587761, acc.: 67.97%] [G loss: 0.970597]\n",
      "epoch:16 step:15725 [D loss: 0.545060, acc.: 75.00%] [G loss: 1.157607]\n",
      "epoch:16 step:15726 [D loss: 0.627805, acc.: 61.72%] [G loss: 1.184904]\n",
      "epoch:16 step:15727 [D loss: 0.612833, acc.: 66.41%] [G loss: 1.134012]\n",
      "epoch:16 step:15728 [D loss: 0.581500, acc.: 71.88%] [G loss: 1.259045]\n",
      "epoch:16 step:15729 [D loss: 0.588238, acc.: 65.62%] [G loss: 1.361876]\n",
      "epoch:16 step:15730 [D loss: 0.688706, acc.: 62.50%] [G loss: 1.171674]\n",
      "epoch:16 step:15731 [D loss: 0.491767, acc.: 75.78%] [G loss: 1.202788]\n",
      "epoch:16 step:15732 [D loss: 0.578028, acc.: 71.88%] [G loss: 1.088254]\n",
      "epoch:16 step:15733 [D loss: 0.762207, acc.: 52.34%] [G loss: 1.502589]\n",
      "epoch:16 step:15734 [D loss: 0.423584, acc.: 84.38%] [G loss: 1.155394]\n",
      "epoch:16 step:15735 [D loss: 0.648012, acc.: 64.06%] [G loss: 1.341280]\n",
      "epoch:16 step:15736 [D loss: 0.557954, acc.: 66.41%] [G loss: 0.991715]\n",
      "epoch:16 step:15737 [D loss: 0.526656, acc.: 74.22%] [G loss: 1.053954]\n",
      "epoch:16 step:15738 [D loss: 0.586054, acc.: 67.19%] [G loss: 1.303232]\n",
      "epoch:16 step:15739 [D loss: 0.483056, acc.: 78.91%] [G loss: 1.021618]\n",
      "epoch:16 step:15740 [D loss: 0.474206, acc.: 83.59%] [G loss: 1.380974]\n",
      "epoch:16 step:15741 [D loss: 0.698373, acc.: 59.38%] [G loss: 1.127778]\n",
      "epoch:16 step:15742 [D loss: 0.440961, acc.: 82.03%] [G loss: 1.219660]\n",
      "epoch:16 step:15743 [D loss: 0.551067, acc.: 71.09%] [G loss: 1.495867]\n",
      "epoch:16 step:15744 [D loss: 0.582160, acc.: 65.62%] [G loss: 1.323849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15745 [D loss: 0.567301, acc.: 69.53%] [G loss: 1.251273]\n",
      "epoch:16 step:15746 [D loss: 0.547525, acc.: 69.53%] [G loss: 1.232847]\n",
      "epoch:16 step:15747 [D loss: 0.650589, acc.: 63.28%] [G loss: 0.951178]\n",
      "epoch:16 step:15748 [D loss: 0.625083, acc.: 64.84%] [G loss: 0.876971]\n",
      "epoch:16 step:15749 [D loss: 0.532019, acc.: 66.41%] [G loss: 1.504748]\n",
      "epoch:16 step:15750 [D loss: 0.604752, acc.: 65.62%] [G loss: 1.391582]\n",
      "epoch:16 step:15751 [D loss: 0.530158, acc.: 75.78%] [G loss: 0.989960]\n",
      "epoch:16 step:15752 [D loss: 0.535358, acc.: 73.44%] [G loss: 1.496372]\n",
      "epoch:16 step:15753 [D loss: 0.807551, acc.: 50.78%] [G loss: 1.123042]\n",
      "epoch:16 step:15754 [D loss: 0.641555, acc.: 64.06%] [G loss: 0.987799]\n",
      "epoch:16 step:15755 [D loss: 0.612568, acc.: 70.31%] [G loss: 1.336992]\n",
      "epoch:16 step:15756 [D loss: 0.483176, acc.: 76.56%] [G loss: 1.285606]\n",
      "epoch:16 step:15757 [D loss: 0.581663, acc.: 69.53%] [G loss: 1.321045]\n",
      "epoch:16 step:15758 [D loss: 0.595334, acc.: 75.00%] [G loss: 1.243480]\n",
      "epoch:16 step:15759 [D loss: 0.545588, acc.: 76.56%] [G loss: 1.315886]\n",
      "epoch:16 step:15760 [D loss: 0.439621, acc.: 81.25%] [G loss: 1.494118]\n",
      "epoch:16 step:15761 [D loss: 0.501919, acc.: 75.78%] [G loss: 1.647068]\n",
      "epoch:16 step:15762 [D loss: 0.691887, acc.: 62.50%] [G loss: 1.287351]\n",
      "epoch:16 step:15763 [D loss: 0.551129, acc.: 75.00%] [G loss: 1.284448]\n",
      "epoch:16 step:15764 [D loss: 0.416770, acc.: 84.38%] [G loss: 1.277553]\n",
      "epoch:16 step:15765 [D loss: 0.632418, acc.: 67.97%] [G loss: 1.364186]\n",
      "epoch:16 step:15766 [D loss: 0.365053, acc.: 86.72%] [G loss: 1.589869]\n",
      "epoch:16 step:15767 [D loss: 0.642773, acc.: 61.72%] [G loss: 1.008338]\n",
      "epoch:16 step:15768 [D loss: 0.596402, acc.: 69.53%] [G loss: 1.411279]\n",
      "epoch:16 step:15769 [D loss: 0.516345, acc.: 73.44%] [G loss: 1.427725]\n",
      "epoch:16 step:15770 [D loss: 0.641950, acc.: 58.59%] [G loss: 1.027419]\n",
      "epoch:16 step:15771 [D loss: 0.495053, acc.: 79.69%] [G loss: 1.449936]\n",
      "epoch:16 step:15772 [D loss: 0.718877, acc.: 59.38%] [G loss: 1.124704]\n",
      "epoch:16 step:15773 [D loss: 0.626739, acc.: 63.28%] [G loss: 1.215951]\n",
      "epoch:16 step:15774 [D loss: 0.544896, acc.: 77.34%] [G loss: 1.082966]\n",
      "epoch:16 step:15775 [D loss: 0.510391, acc.: 77.34%] [G loss: 1.494285]\n",
      "epoch:16 step:15776 [D loss: 0.611260, acc.: 69.53%] [G loss: 1.642464]\n",
      "epoch:16 step:15777 [D loss: 0.554683, acc.: 71.09%] [G loss: 1.293148]\n",
      "epoch:16 step:15778 [D loss: 0.692434, acc.: 62.50%] [G loss: 1.276517]\n",
      "epoch:16 step:15779 [D loss: 0.528405, acc.: 71.09%] [G loss: 1.327645]\n",
      "epoch:16 step:15780 [D loss: 0.550473, acc.: 72.66%] [G loss: 0.980367]\n",
      "epoch:16 step:15781 [D loss: 0.643354, acc.: 64.06%] [G loss: 1.371319]\n",
      "epoch:16 step:15782 [D loss: 0.523001, acc.: 78.91%] [G loss: 1.099932]\n",
      "epoch:16 step:15783 [D loss: 0.600600, acc.: 71.09%] [G loss: 1.349688]\n",
      "epoch:16 step:15784 [D loss: 0.597405, acc.: 68.75%] [G loss: 1.045261]\n",
      "epoch:16 step:15785 [D loss: 0.528473, acc.: 73.44%] [G loss: 1.303269]\n",
      "epoch:16 step:15786 [D loss: 0.518256, acc.: 74.22%] [G loss: 1.249109]\n",
      "epoch:16 step:15787 [D loss: 0.721694, acc.: 57.03%] [G loss: 1.586590]\n",
      "epoch:16 step:15788 [D loss: 0.617072, acc.: 64.84%] [G loss: 1.402064]\n",
      "epoch:16 step:15789 [D loss: 0.609262, acc.: 66.41%] [G loss: 1.321760]\n",
      "epoch:16 step:15790 [D loss: 0.607755, acc.: 64.06%] [G loss: 1.026257]\n",
      "epoch:16 step:15791 [D loss: 0.674704, acc.: 59.38%] [G loss: 1.339879]\n",
      "epoch:16 step:15792 [D loss: 0.572048, acc.: 72.66%] [G loss: 1.643517]\n",
      "epoch:16 step:15793 [D loss: 0.745151, acc.: 53.12%] [G loss: 1.405835]\n",
      "epoch:16 step:15794 [D loss: 0.638477, acc.: 65.62%] [G loss: 1.197246]\n",
      "epoch:16 step:15795 [D loss: 0.589002, acc.: 66.41%] [G loss: 1.205361]\n",
      "epoch:16 step:15796 [D loss: 0.537648, acc.: 75.00%] [G loss: 0.961715]\n",
      "epoch:16 step:15797 [D loss: 0.583567, acc.: 71.88%] [G loss: 1.116951]\n",
      "epoch:16 step:15798 [D loss: 0.687355, acc.: 65.62%] [G loss: 1.118593]\n",
      "epoch:16 step:15799 [D loss: 0.542400, acc.: 74.22%] [G loss: 1.584975]\n",
      "epoch:16 step:15800 [D loss: 0.619937, acc.: 66.41%] [G loss: 1.493025]\n",
      "##############\n",
      "[2.67978074 2.08085355 2.01265672 2.76434413 0.87981788 6.13029388\n",
      " 2.02948397 2.99759621 3.89185067 6.32927986]\n",
      "##########\n",
      "epoch:16 step:15801 [D loss: 0.400387, acc.: 84.38%] [G loss: 1.570140]\n",
      "epoch:16 step:15802 [D loss: 0.443753, acc.: 80.47%] [G loss: 1.560171]\n",
      "epoch:16 step:15803 [D loss: 0.607476, acc.: 69.53%] [G loss: 1.453629]\n",
      "epoch:16 step:15804 [D loss: 0.673318, acc.: 64.06%] [G loss: 1.289517]\n",
      "epoch:16 step:15805 [D loss: 0.555713, acc.: 68.75%] [G loss: 1.096415]\n",
      "epoch:16 step:15806 [D loss: 0.621268, acc.: 67.19%] [G loss: 1.147305]\n",
      "epoch:16 step:15807 [D loss: 0.542816, acc.: 72.66%] [G loss: 1.345653]\n",
      "epoch:16 step:15808 [D loss: 0.466982, acc.: 78.91%] [G loss: 1.305441]\n",
      "epoch:16 step:15809 [D loss: 0.581489, acc.: 67.97%] [G loss: 1.206978]\n",
      "epoch:16 step:15810 [D loss: 0.602195, acc.: 66.41%] [G loss: 0.991171]\n",
      "epoch:16 step:15811 [D loss: 0.463056, acc.: 80.47%] [G loss: 1.326276]\n",
      "epoch:16 step:15812 [D loss: 0.495209, acc.: 78.12%] [G loss: 1.244669]\n",
      "epoch:16 step:15813 [D loss: 0.475658, acc.: 78.12%] [G loss: 1.259926]\n",
      "epoch:16 step:15814 [D loss: 0.518070, acc.: 79.69%] [G loss: 1.420656]\n",
      "epoch:16 step:15815 [D loss: 0.532940, acc.: 72.66%] [G loss: 1.431936]\n",
      "epoch:16 step:15816 [D loss: 0.575991, acc.: 70.31%] [G loss: 1.313208]\n",
      "epoch:16 step:15817 [D loss: 0.401674, acc.: 85.94%] [G loss: 1.343778]\n",
      "epoch:16 step:15818 [D loss: 0.575359, acc.: 69.53%] [G loss: 1.482876]\n",
      "epoch:16 step:15819 [D loss: 0.457444, acc.: 84.38%] [G loss: 1.228830]\n",
      "epoch:16 step:15820 [D loss: 0.670285, acc.: 60.94%] [G loss: 1.307890]\n",
      "epoch:16 step:15821 [D loss: 0.548603, acc.: 77.34%] [G loss: 1.101037]\n",
      "epoch:16 step:15822 [D loss: 0.640582, acc.: 66.41%] [G loss: 1.314070]\n",
      "epoch:16 step:15823 [D loss: 0.685344, acc.: 60.94%] [G loss: 1.488998]\n",
      "epoch:16 step:15824 [D loss: 0.727954, acc.: 57.81%] [G loss: 1.250020]\n",
      "epoch:16 step:15825 [D loss: 0.516216, acc.: 74.22%] [G loss: 1.443412]\n",
      "epoch:16 step:15826 [D loss: 0.539267, acc.: 73.44%] [G loss: 1.349915]\n",
      "epoch:16 step:15827 [D loss: 0.539683, acc.: 75.78%] [G loss: 1.238247]\n",
      "epoch:16 step:15828 [D loss: 0.609402, acc.: 67.97%] [G loss: 1.367836]\n",
      "epoch:16 step:15829 [D loss: 0.415203, acc.: 82.81%] [G loss: 1.582786]\n",
      "epoch:16 step:15830 [D loss: 0.524541, acc.: 77.34%] [G loss: 1.411778]\n",
      "epoch:16 step:15831 [D loss: 0.542358, acc.: 71.88%] [G loss: 1.496075]\n",
      "epoch:16 step:15832 [D loss: 0.471963, acc.: 79.69%] [G loss: 1.373022]\n",
      "epoch:16 step:15833 [D loss: 0.669904, acc.: 62.50%] [G loss: 1.415617]\n",
      "epoch:16 step:15834 [D loss: 0.500767, acc.: 75.00%] [G loss: 1.416922]\n",
      "epoch:16 step:15835 [D loss: 0.660629, acc.: 64.84%] [G loss: 1.361517]\n",
      "epoch:16 step:15836 [D loss: 0.626107, acc.: 64.06%] [G loss: 1.124507]\n",
      "epoch:16 step:15837 [D loss: 0.609771, acc.: 64.84%] [G loss: 1.106691]\n",
      "epoch:16 step:15838 [D loss: 0.541148, acc.: 73.44%] [G loss: 1.306400]\n",
      "epoch:16 step:15839 [D loss: 0.510323, acc.: 75.78%] [G loss: 1.061882]\n",
      "epoch:16 step:15840 [D loss: 0.864742, acc.: 46.88%] [G loss: 1.303790]\n",
      "epoch:16 step:15841 [D loss: 0.511525, acc.: 79.69%] [G loss: 1.516381]\n",
      "epoch:16 step:15842 [D loss: 0.454199, acc.: 82.81%] [G loss: 1.440489]\n",
      "epoch:16 step:15843 [D loss: 0.561082, acc.: 71.09%] [G loss: 1.303118]\n",
      "epoch:16 step:15844 [D loss: 0.575229, acc.: 70.31%] [G loss: 1.126344]\n",
      "epoch:16 step:15845 [D loss: 0.542841, acc.: 79.69%] [G loss: 1.349374]\n",
      "epoch:16 step:15846 [D loss: 0.756669, acc.: 55.47%] [G loss: 1.207138]\n",
      "epoch:16 step:15847 [D loss: 0.485063, acc.: 79.69%] [G loss: 1.562389]\n",
      "epoch:16 step:15848 [D loss: 0.420625, acc.: 84.38%] [G loss: 1.496815]\n",
      "epoch:16 step:15849 [D loss: 0.626599, acc.: 65.62%] [G loss: 1.543868]\n",
      "epoch:16 step:15850 [D loss: 0.614645, acc.: 64.84%] [G loss: 1.467162]\n",
      "epoch:16 step:15851 [D loss: 0.466623, acc.: 76.56%] [G loss: 1.122299]\n",
      "epoch:16 step:15852 [D loss: 0.593300, acc.: 66.41%] [G loss: 1.414214]\n",
      "epoch:16 step:15853 [D loss: 0.437828, acc.: 83.59%] [G loss: 1.425594]\n",
      "epoch:16 step:15854 [D loss: 0.659073, acc.: 59.38%] [G loss: 1.488095]\n",
      "epoch:16 step:15855 [D loss: 0.558104, acc.: 71.88%] [G loss: 1.294441]\n",
      "epoch:16 step:15856 [D loss: 0.657809, acc.: 53.12%] [G loss: 1.229996]\n",
      "epoch:16 step:15857 [D loss: 0.621025, acc.: 65.62%] [G loss: 1.316186]\n",
      "epoch:16 step:15858 [D loss: 0.545603, acc.: 71.09%] [G loss: 1.157512]\n",
      "epoch:16 step:15859 [D loss: 0.641541, acc.: 64.84%] [G loss: 1.254729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15860 [D loss: 0.531621, acc.: 75.00%] [G loss: 1.273138]\n",
      "epoch:16 step:15861 [D loss: 0.520382, acc.: 72.66%] [G loss: 1.278992]\n",
      "epoch:16 step:15862 [D loss: 0.555967, acc.: 67.19%] [G loss: 1.055707]\n",
      "epoch:16 step:15863 [D loss: 0.578706, acc.: 65.62%] [G loss: 1.252403]\n",
      "epoch:16 step:15864 [D loss: 0.684717, acc.: 61.72%] [G loss: 1.250261]\n",
      "epoch:16 step:15865 [D loss: 0.447178, acc.: 79.69%] [G loss: 1.155504]\n",
      "epoch:16 step:15866 [D loss: 0.726721, acc.: 57.03%] [G loss: 1.396995]\n",
      "epoch:16 step:15867 [D loss: 0.460929, acc.: 80.47%] [G loss: 1.403434]\n",
      "epoch:16 step:15868 [D loss: 0.675255, acc.: 56.25%] [G loss: 1.376721]\n",
      "epoch:16 step:15869 [D loss: 0.487392, acc.: 78.12%] [G loss: 1.562183]\n",
      "epoch:16 step:15870 [D loss: 0.602021, acc.: 62.50%] [G loss: 1.282653]\n",
      "epoch:16 step:15871 [D loss: 0.649495, acc.: 59.38%] [G loss: 1.536603]\n",
      "epoch:16 step:15872 [D loss: 0.628982, acc.: 67.97%] [G loss: 1.088272]\n",
      "epoch:16 step:15873 [D loss: 0.614121, acc.: 65.62%] [G loss: 0.960793]\n",
      "epoch:16 step:15874 [D loss: 0.482774, acc.: 77.34%] [G loss: 1.268063]\n",
      "epoch:16 step:15875 [D loss: 0.707653, acc.: 59.38%] [G loss: 1.263887]\n",
      "epoch:16 step:15876 [D loss: 0.636344, acc.: 60.94%] [G loss: 1.220540]\n",
      "epoch:16 step:15877 [D loss: 0.528301, acc.: 71.88%] [G loss: 1.711180]\n",
      "epoch:16 step:15878 [D loss: 0.636536, acc.: 68.75%] [G loss: 1.067812]\n",
      "epoch:16 step:15879 [D loss: 0.625754, acc.: 67.97%] [G loss: 1.274376]\n",
      "epoch:16 step:15880 [D loss: 0.571503, acc.: 66.41%] [G loss: 1.048049]\n",
      "epoch:16 step:15881 [D loss: 0.569551, acc.: 67.19%] [G loss: 1.446700]\n",
      "epoch:16 step:15882 [D loss: 0.609074, acc.: 68.75%] [G loss: 1.014103]\n",
      "epoch:16 step:15883 [D loss: 0.766828, acc.: 54.69%] [G loss: 1.168120]\n",
      "epoch:16 step:15884 [D loss: 0.718174, acc.: 57.81%] [G loss: 0.980089]\n",
      "epoch:16 step:15885 [D loss: 0.713366, acc.: 59.38%] [G loss: 1.386928]\n",
      "epoch:16 step:15886 [D loss: 0.707401, acc.: 53.12%] [G loss: 1.397155]\n",
      "epoch:16 step:15887 [D loss: 0.574922, acc.: 67.97%] [G loss: 1.071147]\n",
      "epoch:16 step:15888 [D loss: 0.549767, acc.: 71.09%] [G loss: 1.195764]\n",
      "epoch:16 step:15889 [D loss: 0.508236, acc.: 71.09%] [G loss: 1.623405]\n",
      "epoch:16 step:15890 [D loss: 0.697475, acc.: 58.59%] [G loss: 1.181706]\n",
      "epoch:16 step:15891 [D loss: 0.473865, acc.: 78.12%] [G loss: 1.263731]\n",
      "epoch:16 step:15892 [D loss: 0.419244, acc.: 80.47%] [G loss: 1.234602]\n",
      "epoch:16 step:15893 [D loss: 0.646980, acc.: 63.28%] [G loss: 1.367535]\n",
      "epoch:16 step:15894 [D loss: 0.729954, acc.: 56.25%] [G loss: 1.425771]\n",
      "epoch:16 step:15895 [D loss: 0.516806, acc.: 75.00%] [G loss: 1.467933]\n",
      "epoch:16 step:15896 [D loss: 0.450923, acc.: 85.16%] [G loss: 1.336759]\n",
      "epoch:16 step:15897 [D loss: 0.600369, acc.: 67.97%] [G loss: 1.088670]\n",
      "epoch:16 step:15898 [D loss: 0.529508, acc.: 75.00%] [G loss: 1.275214]\n",
      "epoch:16 step:15899 [D loss: 0.660836, acc.: 63.28%] [G loss: 0.862939]\n",
      "epoch:16 step:15900 [D loss: 0.527047, acc.: 76.56%] [G loss: 1.302572]\n",
      "epoch:16 step:15901 [D loss: 0.706810, acc.: 56.25%] [G loss: 0.917905]\n",
      "epoch:16 step:15902 [D loss: 0.411108, acc.: 82.81%] [G loss: 1.219085]\n",
      "epoch:16 step:15903 [D loss: 0.743751, acc.: 53.12%] [G loss: 1.195390]\n",
      "epoch:16 step:15904 [D loss: 0.639451, acc.: 65.62%] [G loss: 1.158027]\n",
      "epoch:16 step:15905 [D loss: 0.585047, acc.: 68.75%] [G loss: 1.535888]\n",
      "epoch:16 step:15906 [D loss: 0.589174, acc.: 67.97%] [G loss: 1.474842]\n",
      "epoch:16 step:15907 [D loss: 0.573930, acc.: 71.09%] [G loss: 1.081524]\n",
      "epoch:16 step:15908 [D loss: 0.560586, acc.: 67.19%] [G loss: 1.415708]\n",
      "epoch:16 step:15909 [D loss: 0.596644, acc.: 66.41%] [G loss: 1.194728]\n",
      "epoch:16 step:15910 [D loss: 0.608535, acc.: 63.28%] [G loss: 1.216997]\n",
      "epoch:16 step:15911 [D loss: 0.627748, acc.: 68.75%] [G loss: 1.146164]\n",
      "epoch:16 step:15912 [D loss: 0.481547, acc.: 78.12%] [G loss: 1.524840]\n",
      "epoch:16 step:15913 [D loss: 0.534744, acc.: 74.22%] [G loss: 1.198093]\n",
      "epoch:16 step:15914 [D loss: 0.572552, acc.: 72.66%] [G loss: 1.124818]\n",
      "epoch:16 step:15915 [D loss: 0.646149, acc.: 62.50%] [G loss: 1.396812]\n",
      "epoch:16 step:15916 [D loss: 0.497507, acc.: 71.09%] [G loss: 1.752975]\n",
      "epoch:16 step:15917 [D loss: 0.529183, acc.: 75.78%] [G loss: 1.064594]\n",
      "epoch:16 step:15918 [D loss: 0.520784, acc.: 75.00%] [G loss: 1.279590]\n",
      "epoch:16 step:15919 [D loss: 0.523706, acc.: 70.31%] [G loss: 1.173821]\n",
      "epoch:16 step:15920 [D loss: 0.503354, acc.: 76.56%] [G loss: 1.597013]\n",
      "epoch:16 step:15921 [D loss: 0.430910, acc.: 80.47%] [G loss: 1.618712]\n",
      "epoch:16 step:15922 [D loss: 0.487227, acc.: 75.78%] [G loss: 1.427202]\n",
      "epoch:16 step:15923 [D loss: 0.632893, acc.: 63.28%] [G loss: 1.105356]\n",
      "epoch:16 step:15924 [D loss: 0.631628, acc.: 66.41%] [G loss: 1.331391]\n",
      "epoch:16 step:15925 [D loss: 0.707579, acc.: 59.38%] [G loss: 1.319715]\n",
      "epoch:16 step:15926 [D loss: 0.591361, acc.: 67.19%] [G loss: 1.388602]\n",
      "epoch:16 step:15927 [D loss: 0.797242, acc.: 50.00%] [G loss: 0.988899]\n",
      "epoch:16 step:15928 [D loss: 0.519639, acc.: 76.56%] [G loss: 1.295977]\n",
      "epoch:16 step:15929 [D loss: 0.671465, acc.: 67.19%] [G loss: 1.149730]\n",
      "epoch:17 step:15930 [D loss: 0.699216, acc.: 54.69%] [G loss: 1.154037]\n",
      "epoch:17 step:15931 [D loss: 0.590570, acc.: 66.41%] [G loss: 1.359310]\n",
      "epoch:17 step:15932 [D loss: 0.534710, acc.: 74.22%] [G loss: 1.230314]\n",
      "epoch:17 step:15933 [D loss: 0.604623, acc.: 65.62%] [G loss: 0.927387]\n",
      "epoch:17 step:15934 [D loss: 0.527267, acc.: 75.00%] [G loss: 1.151855]\n",
      "epoch:17 step:15935 [D loss: 0.531337, acc.: 72.66%] [G loss: 1.309280]\n",
      "epoch:17 step:15936 [D loss: 0.762170, acc.: 55.47%] [G loss: 1.437493]\n",
      "epoch:17 step:15937 [D loss: 0.483057, acc.: 79.69%] [G loss: 1.200651]\n",
      "epoch:17 step:15938 [D loss: 0.625951, acc.: 64.84%] [G loss: 1.178455]\n",
      "epoch:17 step:15939 [D loss: 0.636665, acc.: 64.84%] [G loss: 1.310261]\n",
      "epoch:17 step:15940 [D loss: 0.690378, acc.: 61.72%] [G loss: 1.414919]\n",
      "epoch:17 step:15941 [D loss: 0.622385, acc.: 67.97%] [G loss: 1.403643]\n",
      "epoch:17 step:15942 [D loss: 0.566977, acc.: 71.09%] [G loss: 1.386586]\n",
      "epoch:17 step:15943 [D loss: 0.448767, acc.: 79.69%] [G loss: 1.141521]\n",
      "epoch:17 step:15944 [D loss: 0.509714, acc.: 75.78%] [G loss: 1.310836]\n",
      "epoch:17 step:15945 [D loss: 0.473686, acc.: 82.81%] [G loss: 1.561137]\n",
      "epoch:17 step:15946 [D loss: 0.613445, acc.: 69.53%] [G loss: 1.016146]\n",
      "epoch:17 step:15947 [D loss: 0.628895, acc.: 64.84%] [G loss: 1.362207]\n",
      "epoch:17 step:15948 [D loss: 0.570687, acc.: 75.00%] [G loss: 1.164856]\n",
      "epoch:17 step:15949 [D loss: 0.581395, acc.: 68.75%] [G loss: 1.219124]\n",
      "epoch:17 step:15950 [D loss: 0.544718, acc.: 73.44%] [G loss: 1.153757]\n",
      "epoch:17 step:15951 [D loss: 0.583407, acc.: 69.53%] [G loss: 1.132759]\n",
      "epoch:17 step:15952 [D loss: 0.651281, acc.: 57.03%] [G loss: 1.050337]\n",
      "epoch:17 step:15953 [D loss: 0.578092, acc.: 67.19%] [G loss: 1.275397]\n",
      "epoch:17 step:15954 [D loss: 0.732438, acc.: 60.94%] [G loss: 1.384202]\n",
      "epoch:17 step:15955 [D loss: 0.651203, acc.: 56.25%] [G loss: 1.094693]\n",
      "epoch:17 step:15956 [D loss: 0.549429, acc.: 73.44%] [G loss: 1.221837]\n",
      "epoch:17 step:15957 [D loss: 0.494054, acc.: 81.25%] [G loss: 1.483087]\n",
      "epoch:17 step:15958 [D loss: 0.717865, acc.: 54.69%] [G loss: 1.265663]\n",
      "epoch:17 step:15959 [D loss: 0.568797, acc.: 70.31%] [G loss: 1.488380]\n",
      "epoch:17 step:15960 [D loss: 0.628140, acc.: 67.19%] [G loss: 1.088006]\n",
      "epoch:17 step:15961 [D loss: 0.529087, acc.: 69.53%] [G loss: 1.300942]\n",
      "epoch:17 step:15962 [D loss: 0.516977, acc.: 74.22%] [G loss: 1.633067]\n",
      "epoch:17 step:15963 [D loss: 0.577263, acc.: 69.53%] [G loss: 1.346662]\n",
      "epoch:17 step:15964 [D loss: 0.625518, acc.: 60.94%] [G loss: 1.221092]\n",
      "epoch:17 step:15965 [D loss: 0.467104, acc.: 83.59%] [G loss: 1.469950]\n",
      "epoch:17 step:15966 [D loss: 0.406185, acc.: 85.94%] [G loss: 1.210078]\n",
      "epoch:17 step:15967 [D loss: 0.507170, acc.: 76.56%] [G loss: 1.345971]\n",
      "epoch:17 step:15968 [D loss: 0.584193, acc.: 71.09%] [G loss: 1.228364]\n",
      "epoch:17 step:15969 [D loss: 0.679303, acc.: 60.94%] [G loss: 1.062756]\n",
      "epoch:17 step:15970 [D loss: 0.570030, acc.: 70.31%] [G loss: 1.278290]\n",
      "epoch:17 step:15971 [D loss: 0.529546, acc.: 75.78%] [G loss: 1.497682]\n",
      "epoch:17 step:15972 [D loss: 0.568239, acc.: 70.31%] [G loss: 1.299828]\n",
      "epoch:17 step:15973 [D loss: 0.560205, acc.: 73.44%] [G loss: 1.157842]\n",
      "epoch:17 step:15974 [D loss: 0.561931, acc.: 69.53%] [G loss: 0.873593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15975 [D loss: 0.680267, acc.: 62.50%] [G loss: 1.405718]\n",
      "epoch:17 step:15976 [D loss: 0.567663, acc.: 69.53%] [G loss: 1.171372]\n",
      "epoch:17 step:15977 [D loss: 0.652665, acc.: 56.25%] [G loss: 1.264698]\n",
      "epoch:17 step:15978 [D loss: 0.517296, acc.: 77.34%] [G loss: 1.369346]\n",
      "epoch:17 step:15979 [D loss: 0.449271, acc.: 82.03%] [G loss: 1.264726]\n",
      "epoch:17 step:15980 [D loss: 0.605920, acc.: 67.19%] [G loss: 1.354705]\n",
      "epoch:17 step:15981 [D loss: 0.514977, acc.: 72.66%] [G loss: 1.236400]\n",
      "epoch:17 step:15982 [D loss: 0.605802, acc.: 69.53%] [G loss: 1.379247]\n",
      "epoch:17 step:15983 [D loss: 0.583521, acc.: 69.53%] [G loss: 1.386189]\n",
      "epoch:17 step:15984 [D loss: 0.822488, acc.: 44.53%] [G loss: 0.965686]\n",
      "epoch:17 step:15985 [D loss: 0.505064, acc.: 75.00%] [G loss: 1.554843]\n",
      "epoch:17 step:15986 [D loss: 0.635364, acc.: 60.94%] [G loss: 0.998114]\n",
      "epoch:17 step:15987 [D loss: 0.522424, acc.: 72.66%] [G loss: 1.320736]\n",
      "epoch:17 step:15988 [D loss: 0.519143, acc.: 71.88%] [G loss: 1.418522]\n",
      "epoch:17 step:15989 [D loss: 0.608274, acc.: 67.19%] [G loss: 1.413744]\n",
      "epoch:17 step:15990 [D loss: 0.600527, acc.: 67.97%] [G loss: 1.218415]\n",
      "epoch:17 step:15991 [D loss: 0.578282, acc.: 70.31%] [G loss: 1.241917]\n",
      "epoch:17 step:15992 [D loss: 0.584350, acc.: 67.97%] [G loss: 1.175205]\n",
      "epoch:17 step:15993 [D loss: 0.521505, acc.: 75.00%] [G loss: 1.412069]\n",
      "epoch:17 step:15994 [D loss: 0.516005, acc.: 78.91%] [G loss: 1.258865]\n",
      "epoch:17 step:15995 [D loss: 0.639725, acc.: 61.72%] [G loss: 1.419332]\n",
      "epoch:17 step:15996 [D loss: 0.595896, acc.: 64.84%] [G loss: 1.081143]\n",
      "epoch:17 step:15997 [D loss: 0.776215, acc.: 53.12%] [G loss: 1.027895]\n",
      "epoch:17 step:15998 [D loss: 0.500313, acc.: 75.00%] [G loss: 1.217755]\n",
      "epoch:17 step:15999 [D loss: 0.499220, acc.: 77.34%] [G loss: 1.628458]\n",
      "epoch:17 step:16000 [D loss: 0.616394, acc.: 64.06%] [G loss: 1.402964]\n",
      "##############\n",
      "[2.7081461  2.06528218 1.90708402 3.04262722 0.92383945 5.96792081\n",
      " 2.38369312 2.80636209 3.97640257 5.31571187]\n",
      "##########\n",
      "epoch:17 step:16001 [D loss: 0.530302, acc.: 71.88%] [G loss: 1.251394]\n",
      "epoch:17 step:16002 [D loss: 0.526733, acc.: 76.56%] [G loss: 1.149603]\n",
      "epoch:17 step:16003 [D loss: 0.550906, acc.: 71.88%] [G loss: 1.224906]\n",
      "epoch:17 step:16004 [D loss: 0.604433, acc.: 70.31%] [G loss: 1.186919]\n",
      "epoch:17 step:16005 [D loss: 0.545608, acc.: 72.66%] [G loss: 1.332492]\n",
      "epoch:17 step:16006 [D loss: 0.620561, acc.: 67.97%] [G loss: 1.168204]\n",
      "epoch:17 step:16007 [D loss: 0.560110, acc.: 73.44%] [G loss: 1.362393]\n",
      "epoch:17 step:16008 [D loss: 0.553908, acc.: 72.66%] [G loss: 1.289032]\n",
      "epoch:17 step:16009 [D loss: 0.534536, acc.: 71.88%] [G loss: 1.124063]\n",
      "epoch:17 step:16010 [D loss: 0.780680, acc.: 52.34%] [G loss: 1.265840]\n",
      "epoch:17 step:16011 [D loss: 0.467091, acc.: 78.91%] [G loss: 1.272049]\n",
      "epoch:17 step:16012 [D loss: 0.667844, acc.: 62.50%] [G loss: 1.359234]\n",
      "epoch:17 step:16013 [D loss: 0.632001, acc.: 67.19%] [G loss: 1.168481]\n",
      "epoch:17 step:16014 [D loss: 0.479849, acc.: 75.00%] [G loss: 1.295712]\n",
      "epoch:17 step:16015 [D loss: 0.724876, acc.: 53.12%] [G loss: 1.367268]\n",
      "epoch:17 step:16016 [D loss: 0.535528, acc.: 78.12%] [G loss: 1.726046]\n",
      "epoch:17 step:16017 [D loss: 0.560946, acc.: 72.66%] [G loss: 1.157615]\n",
      "epoch:17 step:16018 [D loss: 0.655247, acc.: 58.59%] [G loss: 1.187853]\n",
      "epoch:17 step:16019 [D loss: 0.604807, acc.: 67.97%] [G loss: 1.630767]\n",
      "epoch:17 step:16020 [D loss: 0.567230, acc.: 67.97%] [G loss: 1.137711]\n",
      "epoch:17 step:16021 [D loss: 0.685157, acc.: 58.59%] [G loss: 1.311291]\n",
      "epoch:17 step:16022 [D loss: 0.467868, acc.: 80.47%] [G loss: 1.330938]\n",
      "epoch:17 step:16023 [D loss: 0.513604, acc.: 75.00%] [G loss: 1.327093]\n",
      "epoch:17 step:16024 [D loss: 0.608268, acc.: 65.62%] [G loss: 1.377142]\n",
      "epoch:17 step:16025 [D loss: 0.484495, acc.: 77.34%] [G loss: 1.290635]\n",
      "epoch:17 step:16026 [D loss: 0.553565, acc.: 67.97%] [G loss: 1.511662]\n",
      "epoch:17 step:16027 [D loss: 0.561960, acc.: 71.88%] [G loss: 1.382941]\n",
      "epoch:17 step:16028 [D loss: 0.486555, acc.: 75.78%] [G loss: 1.345111]\n",
      "epoch:17 step:16029 [D loss: 0.629701, acc.: 64.06%] [G loss: 1.216955]\n",
      "epoch:17 step:16030 [D loss: 0.539132, acc.: 74.22%] [G loss: 1.084357]\n",
      "epoch:17 step:16031 [D loss: 0.556194, acc.: 76.56%] [G loss: 1.359019]\n",
      "epoch:17 step:16032 [D loss: 0.513809, acc.: 75.00%] [G loss: 1.252194]\n",
      "epoch:17 step:16033 [D loss: 0.599050, acc.: 62.50%] [G loss: 1.161725]\n",
      "epoch:17 step:16034 [D loss: 0.538689, acc.: 73.44%] [G loss: 1.125316]\n",
      "epoch:17 step:16035 [D loss: 0.626588, acc.: 64.84%] [G loss: 1.209878]\n",
      "epoch:17 step:16036 [D loss: 0.541679, acc.: 71.88%] [G loss: 1.212579]\n",
      "epoch:17 step:16037 [D loss: 0.504593, acc.: 75.78%] [G loss: 1.066015]\n",
      "epoch:17 step:16038 [D loss: 0.531684, acc.: 73.44%] [G loss: 1.252992]\n",
      "epoch:17 step:16039 [D loss: 0.677360, acc.: 60.16%] [G loss: 1.312336]\n",
      "epoch:17 step:16040 [D loss: 0.492938, acc.: 76.56%] [G loss: 1.315638]\n",
      "epoch:17 step:16041 [D loss: 0.542774, acc.: 75.78%] [G loss: 1.260046]\n",
      "epoch:17 step:16042 [D loss: 0.554424, acc.: 75.00%] [G loss: 1.036162]\n",
      "epoch:17 step:16043 [D loss: 0.561515, acc.: 69.53%] [G loss: 1.093165]\n",
      "epoch:17 step:16044 [D loss: 0.557116, acc.: 72.66%] [G loss: 1.186346]\n",
      "epoch:17 step:16045 [D loss: 0.521399, acc.: 71.88%] [G loss: 1.721710]\n",
      "epoch:17 step:16046 [D loss: 0.645845, acc.: 64.84%] [G loss: 1.089822]\n",
      "epoch:17 step:16047 [D loss: 0.569870, acc.: 67.97%] [G loss: 1.101604]\n",
      "epoch:17 step:16048 [D loss: 0.719251, acc.: 59.38%] [G loss: 1.025684]\n",
      "epoch:17 step:16049 [D loss: 0.734102, acc.: 55.47%] [G loss: 1.453267]\n",
      "epoch:17 step:16050 [D loss: 0.638918, acc.: 65.62%] [G loss: 1.374027]\n",
      "epoch:17 step:16051 [D loss: 0.574168, acc.: 68.75%] [G loss: 1.360959]\n",
      "epoch:17 step:16052 [D loss: 0.701946, acc.: 53.91%] [G loss: 1.127984]\n",
      "epoch:17 step:16053 [D loss: 0.636426, acc.: 60.94%] [G loss: 1.318918]\n",
      "epoch:17 step:16054 [D loss: 0.644620, acc.: 60.16%] [G loss: 1.060165]\n",
      "epoch:17 step:16055 [D loss: 0.531681, acc.: 74.22%] [G loss: 1.250614]\n",
      "epoch:17 step:16056 [D loss: 0.592875, acc.: 68.75%] [G loss: 1.091675]\n",
      "epoch:17 step:16057 [D loss: 0.631025, acc.: 64.06%] [G loss: 1.276211]\n",
      "epoch:17 step:16058 [D loss: 0.568479, acc.: 70.31%] [G loss: 1.129210]\n",
      "epoch:17 step:16059 [D loss: 0.594965, acc.: 67.19%] [G loss: 1.172744]\n",
      "epoch:17 step:16060 [D loss: 0.564571, acc.: 67.97%] [G loss: 1.414215]\n",
      "epoch:17 step:16061 [D loss: 0.574486, acc.: 71.88%] [G loss: 1.367460]\n",
      "epoch:17 step:16062 [D loss: 0.480923, acc.: 79.69%] [G loss: 1.335776]\n",
      "epoch:17 step:16063 [D loss: 0.625839, acc.: 67.19%] [G loss: 1.066434]\n",
      "epoch:17 step:16064 [D loss: 0.471744, acc.: 81.25%] [G loss: 1.433365]\n",
      "epoch:17 step:16065 [D loss: 0.691041, acc.: 59.38%] [G loss: 1.107736]\n",
      "epoch:17 step:16066 [D loss: 0.532126, acc.: 72.66%] [G loss: 1.113425]\n",
      "epoch:17 step:16067 [D loss: 0.633261, acc.: 60.94%] [G loss: 1.172052]\n",
      "epoch:17 step:16068 [D loss: 0.644806, acc.: 60.94%] [G loss: 0.886926]\n",
      "epoch:17 step:16069 [D loss: 0.463921, acc.: 78.12%] [G loss: 1.193060]\n",
      "epoch:17 step:16070 [D loss: 0.480165, acc.: 76.56%] [G loss: 1.502429]\n",
      "epoch:17 step:16071 [D loss: 0.533042, acc.: 71.09%] [G loss: 1.319965]\n",
      "epoch:17 step:16072 [D loss: 0.542877, acc.: 72.66%] [G loss: 1.344066]\n",
      "epoch:17 step:16073 [D loss: 0.743592, acc.: 56.25%] [G loss: 1.394164]\n",
      "epoch:17 step:16074 [D loss: 0.586571, acc.: 67.19%] [G loss: 1.324537]\n",
      "epoch:17 step:16075 [D loss: 0.617288, acc.: 64.06%] [G loss: 1.395848]\n",
      "epoch:17 step:16076 [D loss: 0.503362, acc.: 71.88%] [G loss: 1.327544]\n",
      "epoch:17 step:16077 [D loss: 0.537035, acc.: 71.88%] [G loss: 1.290199]\n",
      "epoch:17 step:16078 [D loss: 0.672104, acc.: 61.72%] [G loss: 1.184062]\n",
      "epoch:17 step:16079 [D loss: 0.594049, acc.: 68.75%] [G loss: 1.301636]\n",
      "epoch:17 step:16080 [D loss: 0.647349, acc.: 64.06%] [G loss: 1.302214]\n",
      "epoch:17 step:16081 [D loss: 0.560525, acc.: 68.75%] [G loss: 1.399263]\n",
      "epoch:17 step:16082 [D loss: 0.460656, acc.: 82.03%] [G loss: 1.314188]\n",
      "epoch:17 step:16083 [D loss: 0.431995, acc.: 81.25%] [G loss: 1.583722]\n",
      "epoch:17 step:16084 [D loss: 0.601419, acc.: 67.97%] [G loss: 1.283719]\n",
      "epoch:17 step:16085 [D loss: 0.604514, acc.: 64.84%] [G loss: 1.292512]\n",
      "epoch:17 step:16086 [D loss: 0.471964, acc.: 77.34%] [G loss: 1.452391]\n",
      "epoch:17 step:16087 [D loss: 0.557605, acc.: 72.66%] [G loss: 1.253435]\n",
      "epoch:17 step:16088 [D loss: 0.445659, acc.: 81.25%] [G loss: 1.222229]\n",
      "epoch:17 step:16089 [D loss: 0.505177, acc.: 77.34%] [G loss: 1.462817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16090 [D loss: 0.515671, acc.: 78.91%] [G loss: 1.178938]\n",
      "epoch:17 step:16091 [D loss: 0.664043, acc.: 67.19%] [G loss: 1.128849]\n",
      "epoch:17 step:16092 [D loss: 0.517841, acc.: 78.12%] [G loss: 1.062635]\n",
      "epoch:17 step:16093 [D loss: 0.595700, acc.: 67.19%] [G loss: 1.450069]\n",
      "epoch:17 step:16094 [D loss: 0.509894, acc.: 78.12%] [G loss: 1.525024]\n",
      "epoch:17 step:16095 [D loss: 0.737241, acc.: 56.25%] [G loss: 1.115450]\n",
      "epoch:17 step:16096 [D loss: 0.576289, acc.: 71.09%] [G loss: 1.301280]\n",
      "epoch:17 step:16097 [D loss: 0.498419, acc.: 76.56%] [G loss: 1.547228]\n",
      "epoch:17 step:16098 [D loss: 0.542037, acc.: 67.97%] [G loss: 1.161148]\n",
      "epoch:17 step:16099 [D loss: 0.606340, acc.: 69.53%] [G loss: 1.082088]\n",
      "epoch:17 step:16100 [D loss: 0.503270, acc.: 76.56%] [G loss: 1.408529]\n",
      "epoch:17 step:16101 [D loss: 0.505313, acc.: 82.03%] [G loss: 1.289460]\n",
      "epoch:17 step:16102 [D loss: 0.694364, acc.: 53.91%] [G loss: 1.139241]\n",
      "epoch:17 step:16103 [D loss: 0.618656, acc.: 63.28%] [G loss: 1.219162]\n",
      "epoch:17 step:16104 [D loss: 0.577350, acc.: 72.66%] [G loss: 1.589290]\n",
      "epoch:17 step:16105 [D loss: 0.482340, acc.: 79.69%] [G loss: 1.187410]\n",
      "epoch:17 step:16106 [D loss: 0.591827, acc.: 69.53%] [G loss: 1.155183]\n",
      "epoch:17 step:16107 [D loss: 0.596403, acc.: 64.06%] [G loss: 1.462979]\n",
      "epoch:17 step:16108 [D loss: 0.553413, acc.: 73.44%] [G loss: 1.328788]\n",
      "epoch:17 step:16109 [D loss: 0.571234, acc.: 74.22%] [G loss: 1.043025]\n",
      "epoch:17 step:16110 [D loss: 0.830668, acc.: 40.62%] [G loss: 1.294826]\n",
      "epoch:17 step:16111 [D loss: 0.589164, acc.: 72.66%] [G loss: 1.593017]\n",
      "epoch:17 step:16112 [D loss: 0.588792, acc.: 67.97%] [G loss: 1.518632]\n",
      "epoch:17 step:16113 [D loss: 0.597593, acc.: 67.19%] [G loss: 1.486527]\n",
      "epoch:17 step:16114 [D loss: 0.512775, acc.: 76.56%] [G loss: 1.352133]\n",
      "epoch:17 step:16115 [D loss: 0.456693, acc.: 78.91%] [G loss: 1.404040]\n",
      "epoch:17 step:16116 [D loss: 0.438648, acc.: 81.25%] [G loss: 1.497839]\n",
      "epoch:17 step:16117 [D loss: 0.576282, acc.: 69.53%] [G loss: 1.051400]\n",
      "epoch:17 step:16118 [D loss: 0.474920, acc.: 80.47%] [G loss: 1.239251]\n",
      "epoch:17 step:16119 [D loss: 0.616850, acc.: 63.28%] [G loss: 1.276549]\n",
      "epoch:17 step:16120 [D loss: 0.637889, acc.: 67.97%] [G loss: 1.478422]\n",
      "epoch:17 step:16121 [D loss: 0.725104, acc.: 57.03%] [G loss: 1.220026]\n",
      "epoch:17 step:16122 [D loss: 0.558669, acc.: 74.22%] [G loss: 1.324526]\n",
      "epoch:17 step:16123 [D loss: 0.716180, acc.: 63.28%] [G loss: 1.152249]\n",
      "epoch:17 step:16124 [D loss: 0.637277, acc.: 64.84%] [G loss: 1.155957]\n",
      "epoch:17 step:16125 [D loss: 0.721518, acc.: 56.25%] [G loss: 1.154033]\n",
      "epoch:17 step:16126 [D loss: 0.546654, acc.: 72.66%] [G loss: 1.329554]\n",
      "epoch:17 step:16127 [D loss: 0.560561, acc.: 73.44%] [G loss: 1.407277]\n",
      "epoch:17 step:16128 [D loss: 0.536085, acc.: 71.88%] [G loss: 1.600119]\n",
      "epoch:17 step:16129 [D loss: 0.563403, acc.: 75.00%] [G loss: 1.451203]\n",
      "epoch:17 step:16130 [D loss: 0.490956, acc.: 78.91%] [G loss: 1.549270]\n",
      "epoch:17 step:16131 [D loss: 0.632129, acc.: 67.19%] [G loss: 1.250672]\n",
      "epoch:17 step:16132 [D loss: 0.574535, acc.: 68.75%] [G loss: 1.834308]\n",
      "epoch:17 step:16133 [D loss: 0.535632, acc.: 75.00%] [G loss: 1.679018]\n",
      "epoch:17 step:16134 [D loss: 0.591234, acc.: 67.19%] [G loss: 1.627568]\n",
      "epoch:17 step:16135 [D loss: 0.546778, acc.: 67.97%] [G loss: 1.371890]\n",
      "epoch:17 step:16136 [D loss: 0.649685, acc.: 63.28%] [G loss: 1.296230]\n",
      "epoch:17 step:16137 [D loss: 0.565702, acc.: 72.66%] [G loss: 1.286949]\n",
      "epoch:17 step:16138 [D loss: 0.533835, acc.: 72.66%] [G loss: 1.220939]\n",
      "epoch:17 step:16139 [D loss: 0.571816, acc.: 68.75%] [G loss: 1.081300]\n",
      "epoch:17 step:16140 [D loss: 0.493425, acc.: 77.34%] [G loss: 1.253464]\n",
      "epoch:17 step:16141 [D loss: 0.606037, acc.: 63.28%] [G loss: 1.402691]\n",
      "epoch:17 step:16142 [D loss: 0.546528, acc.: 71.88%] [G loss: 1.401679]\n",
      "epoch:17 step:16143 [D loss: 0.751907, acc.: 55.47%] [G loss: 1.278961]\n",
      "epoch:17 step:16144 [D loss: 0.624777, acc.: 65.62%] [G loss: 1.309753]\n",
      "epoch:17 step:16145 [D loss: 0.644914, acc.: 64.84%] [G loss: 1.080055]\n",
      "epoch:17 step:16146 [D loss: 0.560701, acc.: 69.53%] [G loss: 1.323675]\n",
      "epoch:17 step:16147 [D loss: 0.687562, acc.: 57.03%] [G loss: 1.303603]\n",
      "epoch:17 step:16148 [D loss: 0.583798, acc.: 65.62%] [G loss: 1.182046]\n",
      "epoch:17 step:16149 [D loss: 0.551684, acc.: 74.22%] [G loss: 1.331246]\n",
      "epoch:17 step:16150 [D loss: 0.583600, acc.: 66.41%] [G loss: 1.243508]\n",
      "epoch:17 step:16151 [D loss: 0.568976, acc.: 72.66%] [G loss: 1.323110]\n",
      "epoch:17 step:16152 [D loss: 0.564947, acc.: 70.31%] [G loss: 1.176456]\n",
      "epoch:17 step:16153 [D loss: 0.617732, acc.: 70.31%] [G loss: 1.147903]\n",
      "epoch:17 step:16154 [D loss: 0.526068, acc.: 71.88%] [G loss: 1.168873]\n",
      "epoch:17 step:16155 [D loss: 0.554311, acc.: 72.66%] [G loss: 0.914038]\n",
      "epoch:17 step:16156 [D loss: 0.601909, acc.: 69.53%] [G loss: 1.109956]\n",
      "epoch:17 step:16157 [D loss: 0.534290, acc.: 75.00%] [G loss: 1.253938]\n",
      "epoch:17 step:16158 [D loss: 0.470500, acc.: 85.16%] [G loss: 1.368808]\n",
      "epoch:17 step:16159 [D loss: 0.616839, acc.: 63.28%] [G loss: 1.295209]\n",
      "epoch:17 step:16160 [D loss: 0.489236, acc.: 76.56%] [G loss: 1.647960]\n",
      "epoch:17 step:16161 [D loss: 0.621736, acc.: 65.62%] [G loss: 1.483209]\n",
      "epoch:17 step:16162 [D loss: 0.642319, acc.: 61.72%] [G loss: 1.603148]\n",
      "epoch:17 step:16163 [D loss: 0.572775, acc.: 67.97%] [G loss: 1.087344]\n",
      "epoch:17 step:16164 [D loss: 0.580862, acc.: 65.62%] [G loss: 1.296638]\n",
      "epoch:17 step:16165 [D loss: 0.547408, acc.: 75.78%] [G loss: 1.348205]\n",
      "epoch:17 step:16166 [D loss: 0.700280, acc.: 56.25%] [G loss: 1.270055]\n",
      "epoch:17 step:16167 [D loss: 0.769974, acc.: 54.69%] [G loss: 1.150179]\n",
      "epoch:17 step:16168 [D loss: 0.665898, acc.: 66.41%] [G loss: 1.271718]\n",
      "epoch:17 step:16169 [D loss: 0.671607, acc.: 57.81%] [G loss: 1.509762]\n",
      "epoch:17 step:16170 [D loss: 0.585653, acc.: 67.19%] [G loss: 1.251116]\n",
      "epoch:17 step:16171 [D loss: 0.738313, acc.: 56.25%] [G loss: 1.303572]\n",
      "epoch:17 step:16172 [D loss: 0.627751, acc.: 62.50%] [G loss: 1.220931]\n",
      "epoch:17 step:16173 [D loss: 0.569153, acc.: 71.88%] [G loss: 1.303764]\n",
      "epoch:17 step:16174 [D loss: 0.652282, acc.: 61.72%] [G loss: 0.879974]\n",
      "epoch:17 step:16175 [D loss: 0.575035, acc.: 70.31%] [G loss: 1.279541]\n",
      "epoch:17 step:16176 [D loss: 0.664997, acc.: 62.50%] [G loss: 1.327811]\n",
      "epoch:17 step:16177 [D loss: 0.553168, acc.: 72.66%] [G loss: 1.410801]\n",
      "epoch:17 step:16178 [D loss: 0.625364, acc.: 63.28%] [G loss: 1.384743]\n",
      "epoch:17 step:16179 [D loss: 0.511840, acc.: 71.88%] [G loss: 1.547803]\n",
      "epoch:17 step:16180 [D loss: 0.672320, acc.: 65.62%] [G loss: 1.171198]\n",
      "epoch:17 step:16181 [D loss: 0.433688, acc.: 83.59%] [G loss: 1.498797]\n",
      "epoch:17 step:16182 [D loss: 0.589727, acc.: 67.19%] [G loss: 1.272104]\n",
      "epoch:17 step:16183 [D loss: 0.605793, acc.: 67.19%] [G loss: 1.020885]\n",
      "epoch:17 step:16184 [D loss: 0.540962, acc.: 75.00%] [G loss: 1.295034]\n",
      "epoch:17 step:16185 [D loss: 0.665361, acc.: 60.16%] [G loss: 1.297457]\n",
      "epoch:17 step:16186 [D loss: 0.596636, acc.: 64.84%] [G loss: 1.310774]\n",
      "epoch:17 step:16187 [D loss: 0.649128, acc.: 66.41%] [G loss: 1.349644]\n",
      "epoch:17 step:16188 [D loss: 0.571895, acc.: 65.62%] [G loss: 1.498684]\n",
      "epoch:17 step:16189 [D loss: 0.670615, acc.: 57.81%] [G loss: 1.326284]\n",
      "epoch:17 step:16190 [D loss: 0.552849, acc.: 74.22%] [G loss: 1.381301]\n",
      "epoch:17 step:16191 [D loss: 0.671039, acc.: 62.50%] [G loss: 1.104349]\n",
      "epoch:17 step:16192 [D loss: 0.620925, acc.: 61.72%] [G loss: 1.135521]\n",
      "epoch:17 step:16193 [D loss: 0.582233, acc.: 69.53%] [G loss: 1.413157]\n",
      "epoch:17 step:16194 [D loss: 0.556347, acc.: 75.78%] [G loss: 1.394622]\n",
      "epoch:17 step:16195 [D loss: 0.515213, acc.: 78.12%] [G loss: 1.383649]\n",
      "epoch:17 step:16196 [D loss: 0.689093, acc.: 60.16%] [G loss: 1.458062]\n",
      "epoch:17 step:16197 [D loss: 0.626221, acc.: 68.75%] [G loss: 1.176443]\n",
      "epoch:17 step:16198 [D loss: 0.621482, acc.: 67.19%] [G loss: 1.383215]\n",
      "epoch:17 step:16199 [D loss: 0.593712, acc.: 69.53%] [G loss: 1.225250]\n",
      "epoch:17 step:16200 [D loss: 0.538483, acc.: 75.78%] [G loss: 1.308392]\n",
      "##############\n",
      "[2.60618374 2.00087371 1.69410955 2.74709285 0.78838046 5.79188621\n",
      " 2.33937086 2.82630229 4.00949747 8.14868929]\n",
      "##########\n",
      "epoch:17 step:16201 [D loss: 0.534385, acc.: 75.78%] [G loss: 1.348956]\n",
      "epoch:17 step:16202 [D loss: 0.680055, acc.: 62.50%] [G loss: 1.234069]\n",
      "epoch:17 step:16203 [D loss: 0.438874, acc.: 81.25%] [G loss: 1.302988]\n",
      "epoch:17 step:16204 [D loss: 0.679842, acc.: 64.84%] [G loss: 1.546680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16205 [D loss: 0.601468, acc.: 68.75%] [G loss: 1.176836]\n",
      "epoch:17 step:16206 [D loss: 0.507250, acc.: 74.22%] [G loss: 1.110435]\n",
      "epoch:17 step:16207 [D loss: 0.573105, acc.: 69.53%] [G loss: 1.344361]\n",
      "epoch:17 step:16208 [D loss: 0.614980, acc.: 65.62%] [G loss: 1.580145]\n",
      "epoch:17 step:16209 [D loss: 0.664723, acc.: 60.94%] [G loss: 1.410340]\n",
      "epoch:17 step:16210 [D loss: 0.603009, acc.: 64.84%] [G loss: 1.531433]\n",
      "epoch:17 step:16211 [D loss: 0.515442, acc.: 75.00%] [G loss: 1.286221]\n",
      "epoch:17 step:16212 [D loss: 0.622645, acc.: 70.31%] [G loss: 1.342755]\n",
      "epoch:17 step:16213 [D loss: 0.699846, acc.: 55.47%] [G loss: 1.043618]\n",
      "epoch:17 step:16214 [D loss: 0.601837, acc.: 70.31%] [G loss: 1.109507]\n",
      "epoch:17 step:16215 [D loss: 0.602486, acc.: 69.53%] [G loss: 1.176403]\n",
      "epoch:17 step:16216 [D loss: 0.468906, acc.: 81.25%] [G loss: 1.224239]\n",
      "epoch:17 step:16217 [D loss: 0.709131, acc.: 62.50%] [G loss: 1.397728]\n",
      "epoch:17 step:16218 [D loss: 0.467941, acc.: 77.34%] [G loss: 1.697223]\n",
      "epoch:17 step:16219 [D loss: 0.566241, acc.: 69.53%] [G loss: 1.354832]\n",
      "epoch:17 step:16220 [D loss: 0.490534, acc.: 77.34%] [G loss: 1.322465]\n",
      "epoch:17 step:16221 [D loss: 0.665541, acc.: 60.16%] [G loss: 1.297112]\n",
      "epoch:17 step:16222 [D loss: 0.647804, acc.: 67.19%] [G loss: 1.273739]\n",
      "epoch:17 step:16223 [D loss: 0.615445, acc.: 67.97%] [G loss: 1.429438]\n",
      "epoch:17 step:16224 [D loss: 0.626545, acc.: 59.38%] [G loss: 1.202597]\n",
      "epoch:17 step:16225 [D loss: 0.580503, acc.: 71.09%] [G loss: 1.173180]\n",
      "epoch:17 step:16226 [D loss: 0.586034, acc.: 72.66%] [G loss: 1.273602]\n",
      "epoch:17 step:16227 [D loss: 0.703299, acc.: 60.16%] [G loss: 1.259549]\n",
      "epoch:17 step:16228 [D loss: 0.565392, acc.: 69.53%] [G loss: 1.131597]\n",
      "epoch:17 step:16229 [D loss: 0.589481, acc.: 65.62%] [G loss: 1.381992]\n",
      "epoch:17 step:16230 [D loss: 0.521498, acc.: 75.00%] [G loss: 1.191123]\n",
      "epoch:17 step:16231 [D loss: 0.622703, acc.: 62.50%] [G loss: 1.209714]\n",
      "epoch:17 step:16232 [D loss: 0.688391, acc.: 57.03%] [G loss: 1.431802]\n",
      "epoch:17 step:16233 [D loss: 0.450043, acc.: 80.47%] [G loss: 1.353576]\n",
      "epoch:17 step:16234 [D loss: 0.496359, acc.: 76.56%] [G loss: 1.545063]\n",
      "epoch:17 step:16235 [D loss: 0.654292, acc.: 60.94%] [G loss: 0.970017]\n",
      "epoch:17 step:16236 [D loss: 0.689756, acc.: 59.38%] [G loss: 1.139608]\n",
      "epoch:17 step:16237 [D loss: 0.574343, acc.: 69.53%] [G loss: 1.457684]\n",
      "epoch:17 step:16238 [D loss: 0.444044, acc.: 81.25%] [G loss: 1.039527]\n",
      "epoch:17 step:16239 [D loss: 0.370577, acc.: 89.84%] [G loss: 1.485863]\n",
      "epoch:17 step:16240 [D loss: 0.433347, acc.: 79.69%] [G loss: 1.704909]\n",
      "epoch:17 step:16241 [D loss: 0.590491, acc.: 64.84%] [G loss: 1.357009]\n",
      "epoch:17 step:16242 [D loss: 0.563739, acc.: 74.22%] [G loss: 1.320887]\n",
      "epoch:17 step:16243 [D loss: 0.463118, acc.: 81.25%] [G loss: 1.350254]\n",
      "epoch:17 step:16244 [D loss: 0.617893, acc.: 62.50%] [G loss: 1.427115]\n",
      "epoch:17 step:16245 [D loss: 0.599617, acc.: 67.19%] [G loss: 1.166363]\n",
      "epoch:17 step:16246 [D loss: 0.628314, acc.: 67.19%] [G loss: 1.047462]\n",
      "epoch:17 step:16247 [D loss: 0.600432, acc.: 65.62%] [G loss: 1.256057]\n",
      "epoch:17 step:16248 [D loss: 0.548610, acc.: 71.09%] [G loss: 1.026150]\n",
      "epoch:17 step:16249 [D loss: 0.580595, acc.: 68.75%] [G loss: 1.429285]\n",
      "epoch:17 step:16250 [D loss: 0.644836, acc.: 62.50%] [G loss: 1.192999]\n",
      "epoch:17 step:16251 [D loss: 0.486635, acc.: 79.69%] [G loss: 1.254678]\n",
      "epoch:17 step:16252 [D loss: 0.620693, acc.: 62.50%] [G loss: 1.312586]\n",
      "epoch:17 step:16253 [D loss: 0.520505, acc.: 76.56%] [G loss: 1.242306]\n",
      "epoch:17 step:16254 [D loss: 0.604843, acc.: 64.84%] [G loss: 1.163186]\n",
      "epoch:17 step:16255 [D loss: 0.604055, acc.: 69.53%] [G loss: 1.228959]\n",
      "epoch:17 step:16256 [D loss: 0.506934, acc.: 78.12%] [G loss: 1.394619]\n",
      "epoch:17 step:16257 [D loss: 0.449837, acc.: 78.12%] [G loss: 1.220165]\n",
      "epoch:17 step:16258 [D loss: 0.672075, acc.: 60.16%] [G loss: 1.180300]\n",
      "epoch:17 step:16259 [D loss: 0.545466, acc.: 71.09%] [G loss: 1.076435]\n",
      "epoch:17 step:16260 [D loss: 0.515013, acc.: 71.88%] [G loss: 1.032758]\n",
      "epoch:17 step:16261 [D loss: 0.589266, acc.: 68.75%] [G loss: 1.145762]\n",
      "epoch:17 step:16262 [D loss: 0.623500, acc.: 66.41%] [G loss: 1.175465]\n",
      "epoch:17 step:16263 [D loss: 0.567680, acc.: 66.41%] [G loss: 1.285712]\n",
      "epoch:17 step:16264 [D loss: 0.608670, acc.: 66.41%] [G loss: 1.090745]\n",
      "epoch:17 step:16265 [D loss: 0.517173, acc.: 75.78%] [G loss: 1.115505]\n",
      "epoch:17 step:16266 [D loss: 0.513116, acc.: 70.31%] [G loss: 1.287703]\n",
      "epoch:17 step:16267 [D loss: 0.491993, acc.: 78.91%] [G loss: 1.529650]\n",
      "epoch:17 step:16268 [D loss: 0.446158, acc.: 80.47%] [G loss: 1.316132]\n",
      "epoch:17 step:16269 [D loss: 0.476383, acc.: 76.56%] [G loss: 1.359788]\n",
      "epoch:17 step:16270 [D loss: 0.638995, acc.: 66.41%] [G loss: 1.120339]\n",
      "epoch:17 step:16271 [D loss: 0.669546, acc.: 61.72%] [G loss: 1.362726]\n",
      "epoch:17 step:16272 [D loss: 0.548155, acc.: 74.22%] [G loss: 1.391737]\n",
      "epoch:17 step:16273 [D loss: 0.611012, acc.: 67.19%] [G loss: 1.099140]\n",
      "epoch:17 step:16274 [D loss: 0.656623, acc.: 61.72%] [G loss: 0.984165]\n",
      "epoch:17 step:16275 [D loss: 0.721957, acc.: 58.59%] [G loss: 1.103078]\n",
      "epoch:17 step:16276 [D loss: 0.572238, acc.: 71.88%] [G loss: 1.282510]\n",
      "epoch:17 step:16277 [D loss: 0.660198, acc.: 64.06%] [G loss: 1.189134]\n",
      "epoch:17 step:16278 [D loss: 0.513664, acc.: 77.34%] [G loss: 1.251212]\n",
      "epoch:17 step:16279 [D loss: 0.614592, acc.: 68.75%] [G loss: 1.071787]\n",
      "epoch:17 step:16280 [D loss: 0.562695, acc.: 67.97%] [G loss: 1.255989]\n",
      "epoch:17 step:16281 [D loss: 0.645880, acc.: 67.19%] [G loss: 1.589085]\n",
      "epoch:17 step:16282 [D loss: 0.473663, acc.: 79.69%] [G loss: 1.351647]\n",
      "epoch:17 step:16283 [D loss: 0.605210, acc.: 66.41%] [G loss: 1.243658]\n",
      "epoch:17 step:16284 [D loss: 0.514373, acc.: 79.69%] [G loss: 1.103315]\n",
      "epoch:17 step:16285 [D loss: 0.533597, acc.: 74.22%] [G loss: 1.445047]\n",
      "epoch:17 step:16286 [D loss: 0.589734, acc.: 68.75%] [G loss: 1.212672]\n",
      "epoch:17 step:16287 [D loss: 0.597103, acc.: 67.97%] [G loss: 1.171160]\n",
      "epoch:17 step:16288 [D loss: 0.570439, acc.: 69.53%] [G loss: 1.217204]\n",
      "epoch:17 step:16289 [D loss: 0.547720, acc.: 73.44%] [G loss: 1.188603]\n",
      "epoch:17 step:16290 [D loss: 0.563315, acc.: 70.31%] [G loss: 1.718265]\n",
      "epoch:17 step:16291 [D loss: 0.491742, acc.: 76.56%] [G loss: 1.335766]\n",
      "epoch:17 step:16292 [D loss: 0.545727, acc.: 70.31%] [G loss: 1.250776]\n",
      "epoch:17 step:16293 [D loss: 0.565764, acc.: 73.44%] [G loss: 1.376786]\n",
      "epoch:17 step:16294 [D loss: 0.474514, acc.: 78.12%] [G loss: 1.248415]\n",
      "epoch:17 step:16295 [D loss: 0.500582, acc.: 77.34%] [G loss: 1.087088]\n",
      "epoch:17 step:16296 [D loss: 0.496835, acc.: 77.34%] [G loss: 1.132369]\n",
      "epoch:17 step:16297 [D loss: 0.434242, acc.: 83.59%] [G loss: 1.281351]\n",
      "epoch:17 step:16298 [D loss: 0.770574, acc.: 51.56%] [G loss: 1.053485]\n",
      "epoch:17 step:16299 [D loss: 0.564892, acc.: 76.56%] [G loss: 1.190789]\n",
      "epoch:17 step:16300 [D loss: 0.497838, acc.: 75.00%] [G loss: 1.621441]\n",
      "epoch:17 step:16301 [D loss: 0.403265, acc.: 88.28%] [G loss: 1.241912]\n",
      "epoch:17 step:16302 [D loss: 0.604064, acc.: 69.53%] [G loss: 1.292281]\n",
      "epoch:17 step:16303 [D loss: 0.630315, acc.: 59.38%] [G loss: 1.233192]\n",
      "epoch:17 step:16304 [D loss: 0.580845, acc.: 66.41%] [G loss: 1.186667]\n",
      "epoch:17 step:16305 [D loss: 0.721615, acc.: 56.25%] [G loss: 1.227402]\n",
      "epoch:17 step:16306 [D loss: 0.473258, acc.: 78.12%] [G loss: 1.651907]\n",
      "epoch:17 step:16307 [D loss: 0.620165, acc.: 69.53%] [G loss: 1.442116]\n",
      "epoch:17 step:16308 [D loss: 0.502095, acc.: 75.78%] [G loss: 1.260403]\n",
      "epoch:17 step:16309 [D loss: 0.639831, acc.: 67.19%] [G loss: 1.539806]\n",
      "epoch:17 step:16310 [D loss: 0.712441, acc.: 54.69%] [G loss: 1.195600]\n",
      "epoch:17 step:16311 [D loss: 0.592372, acc.: 68.75%] [G loss: 1.560413]\n",
      "epoch:17 step:16312 [D loss: 0.405672, acc.: 87.50%] [G loss: 1.507742]\n",
      "epoch:17 step:16313 [D loss: 0.635656, acc.: 60.94%] [G loss: 1.526277]\n",
      "epoch:17 step:16314 [D loss: 0.529366, acc.: 78.12%] [G loss: 1.219747]\n",
      "epoch:17 step:16315 [D loss: 0.661266, acc.: 64.84%] [G loss: 1.354344]\n",
      "epoch:17 step:16316 [D loss: 0.614731, acc.: 64.06%] [G loss: 1.727422]\n",
      "epoch:17 step:16317 [D loss: 0.627891, acc.: 64.06%] [G loss: 1.283591]\n",
      "epoch:17 step:16318 [D loss: 0.579695, acc.: 71.09%] [G loss: 1.293719]\n",
      "epoch:17 step:16319 [D loss: 0.480516, acc.: 75.00%] [G loss: 1.573390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16320 [D loss: 0.684737, acc.: 62.50%] [G loss: 1.441234]\n",
      "epoch:17 step:16321 [D loss: 0.562693, acc.: 72.66%] [G loss: 1.322499]\n",
      "epoch:17 step:16322 [D loss: 0.568913, acc.: 70.31%] [G loss: 1.342425]\n",
      "epoch:17 step:16323 [D loss: 0.665213, acc.: 56.25%] [G loss: 1.192781]\n",
      "epoch:17 step:16324 [D loss: 0.541697, acc.: 74.22%] [G loss: 1.175342]\n",
      "epoch:17 step:16325 [D loss: 0.643621, acc.: 63.28%] [G loss: 1.447677]\n",
      "epoch:17 step:16326 [D loss: 0.589831, acc.: 67.19%] [G loss: 1.426664]\n",
      "epoch:17 step:16327 [D loss: 0.533404, acc.: 71.88%] [G loss: 1.389081]\n",
      "epoch:17 step:16328 [D loss: 0.511448, acc.: 76.56%] [G loss: 1.305281]\n",
      "epoch:17 step:16329 [D loss: 0.602996, acc.: 66.41%] [G loss: 1.289595]\n",
      "epoch:17 step:16330 [D loss: 0.676185, acc.: 61.72%] [G loss: 1.103975]\n",
      "epoch:17 step:16331 [D loss: 0.494387, acc.: 77.34%] [G loss: 1.404638]\n",
      "epoch:17 step:16332 [D loss: 0.673760, acc.: 67.19%] [G loss: 1.314399]\n",
      "epoch:17 step:16333 [D loss: 0.666767, acc.: 56.25%] [G loss: 1.270161]\n",
      "epoch:17 step:16334 [D loss: 0.555133, acc.: 68.75%] [G loss: 1.285732]\n",
      "epoch:17 step:16335 [D loss: 0.542150, acc.: 76.56%] [G loss: 1.409522]\n",
      "epoch:17 step:16336 [D loss: 0.656809, acc.: 66.41%] [G loss: 1.324306]\n",
      "epoch:17 step:16337 [D loss: 0.650830, acc.: 60.94%] [G loss: 1.267917]\n",
      "epoch:17 step:16338 [D loss: 0.490203, acc.: 77.34%] [G loss: 1.129919]\n",
      "epoch:17 step:16339 [D loss: 0.505476, acc.: 78.12%] [G loss: 1.304399]\n",
      "epoch:17 step:16340 [D loss: 0.554312, acc.: 66.41%] [G loss: 1.457667]\n",
      "epoch:17 step:16341 [D loss: 0.683319, acc.: 59.38%] [G loss: 1.060214]\n",
      "epoch:17 step:16342 [D loss: 0.655588, acc.: 61.72%] [G loss: 1.221493]\n",
      "epoch:17 step:16343 [D loss: 0.594349, acc.: 64.84%] [G loss: 1.373667]\n",
      "epoch:17 step:16344 [D loss: 0.456014, acc.: 83.59%] [G loss: 1.460360]\n",
      "epoch:17 step:16345 [D loss: 0.591441, acc.: 67.19%] [G loss: 1.169990]\n",
      "epoch:17 step:16346 [D loss: 0.651155, acc.: 64.84%] [G loss: 1.275457]\n",
      "epoch:17 step:16347 [D loss: 0.604169, acc.: 66.41%] [G loss: 1.166566]\n",
      "epoch:17 step:16348 [D loss: 0.430961, acc.: 84.38%] [G loss: 1.141122]\n",
      "epoch:17 step:16349 [D loss: 0.554156, acc.: 73.44%] [G loss: 1.254758]\n",
      "epoch:17 step:16350 [D loss: 0.559388, acc.: 71.88%] [G loss: 1.358439]\n",
      "epoch:17 step:16351 [D loss: 0.444721, acc.: 83.59%] [G loss: 1.425759]\n",
      "epoch:17 step:16352 [D loss: 0.552148, acc.: 76.56%] [G loss: 1.251118]\n",
      "epoch:17 step:16353 [D loss: 0.650832, acc.: 67.19%] [G loss: 1.496163]\n",
      "epoch:17 step:16354 [D loss: 0.485908, acc.: 81.25%] [G loss: 1.502632]\n",
      "epoch:17 step:16355 [D loss: 0.621064, acc.: 72.66%] [G loss: 1.294905]\n",
      "epoch:17 step:16356 [D loss: 0.808393, acc.: 53.91%] [G loss: 0.809962]\n",
      "epoch:17 step:16357 [D loss: 0.529477, acc.: 75.00%] [G loss: 1.176648]\n",
      "epoch:17 step:16358 [D loss: 0.701522, acc.: 53.91%] [G loss: 1.473461]\n",
      "epoch:17 step:16359 [D loss: 0.688930, acc.: 59.38%] [G loss: 0.823901]\n",
      "epoch:17 step:16360 [D loss: 0.544708, acc.: 74.22%] [G loss: 1.232330]\n",
      "epoch:17 step:16361 [D loss: 0.453437, acc.: 81.25%] [G loss: 1.314348]\n",
      "epoch:17 step:16362 [D loss: 0.628022, acc.: 64.84%] [G loss: 1.235792]\n",
      "epoch:17 step:16363 [D loss: 0.628152, acc.: 62.50%] [G loss: 1.155879]\n",
      "epoch:17 step:16364 [D loss: 0.599319, acc.: 70.31%] [G loss: 1.155002]\n",
      "epoch:17 step:16365 [D loss: 0.467723, acc.: 79.69%] [G loss: 1.410742]\n",
      "epoch:17 step:16366 [D loss: 0.612226, acc.: 67.19%] [G loss: 1.023481]\n",
      "epoch:17 step:16367 [D loss: 0.584043, acc.: 65.62%] [G loss: 1.171853]\n",
      "epoch:17 step:16368 [D loss: 0.491590, acc.: 85.16%] [G loss: 1.196313]\n",
      "epoch:17 step:16369 [D loss: 0.634998, acc.: 64.06%] [G loss: 1.017851]\n",
      "epoch:17 step:16370 [D loss: 0.571021, acc.: 74.22%] [G loss: 1.307125]\n",
      "epoch:17 step:16371 [D loss: 0.736812, acc.: 56.25%] [G loss: 1.247542]\n",
      "epoch:17 step:16372 [D loss: 0.479705, acc.: 78.12%] [G loss: 1.243161]\n",
      "epoch:17 step:16373 [D loss: 0.619566, acc.: 70.31%] [G loss: 1.306131]\n",
      "epoch:17 step:16374 [D loss: 0.441426, acc.: 82.81%] [G loss: 1.282275]\n",
      "epoch:17 step:16375 [D loss: 0.608256, acc.: 66.41%] [G loss: 1.532872]\n",
      "epoch:17 step:16376 [D loss: 0.660076, acc.: 61.72%] [G loss: 1.344265]\n",
      "epoch:17 step:16377 [D loss: 0.535211, acc.: 78.12%] [G loss: 1.512402]\n",
      "epoch:17 step:16378 [D loss: 0.476667, acc.: 75.78%] [G loss: 1.353183]\n",
      "epoch:17 step:16379 [D loss: 0.529548, acc.: 70.31%] [G loss: 1.284626]\n",
      "epoch:17 step:16380 [D loss: 0.560123, acc.: 71.88%] [G loss: 1.173099]\n",
      "epoch:17 step:16381 [D loss: 0.538127, acc.: 71.88%] [G loss: 1.604932]\n",
      "epoch:17 step:16382 [D loss: 0.503516, acc.: 75.78%] [G loss: 1.098980]\n",
      "epoch:17 step:16383 [D loss: 0.658634, acc.: 62.50%] [G loss: 1.128901]\n",
      "epoch:17 step:16384 [D loss: 0.529244, acc.: 74.22%] [G loss: 1.465811]\n",
      "epoch:17 step:16385 [D loss: 0.680202, acc.: 58.59%] [G loss: 1.228180]\n",
      "epoch:17 step:16386 [D loss: 0.630273, acc.: 67.19%] [G loss: 1.018390]\n",
      "epoch:17 step:16387 [D loss: 0.460569, acc.: 77.34%] [G loss: 1.443177]\n",
      "epoch:17 step:16388 [D loss: 0.548231, acc.: 70.31%] [G loss: 1.708523]\n",
      "epoch:17 step:16389 [D loss: 0.455029, acc.: 79.69%] [G loss: 1.602491]\n",
      "epoch:17 step:16390 [D loss: 0.621602, acc.: 66.41%] [G loss: 1.084292]\n",
      "epoch:17 step:16391 [D loss: 0.532923, acc.: 73.44%] [G loss: 1.363511]\n",
      "epoch:17 step:16392 [D loss: 0.671717, acc.: 60.94%] [G loss: 0.945108]\n",
      "epoch:17 step:16393 [D loss: 0.557430, acc.: 72.66%] [G loss: 1.263465]\n",
      "epoch:17 step:16394 [D loss: 0.686075, acc.: 60.94%] [G loss: 1.381141]\n",
      "epoch:17 step:16395 [D loss: 0.526299, acc.: 71.88%] [G loss: 1.119659]\n",
      "epoch:17 step:16396 [D loss: 0.651950, acc.: 59.38%] [G loss: 1.178810]\n",
      "epoch:17 step:16397 [D loss: 0.518117, acc.: 77.34%] [G loss: 1.323754]\n",
      "epoch:17 step:16398 [D loss: 0.664410, acc.: 64.06%] [G loss: 0.947715]\n",
      "epoch:17 step:16399 [D loss: 0.639159, acc.: 60.94%] [G loss: 1.292711]\n",
      "epoch:17 step:16400 [D loss: 0.658307, acc.: 67.19%] [G loss: 1.211466]\n",
      "##############\n",
      "[2.64056563 2.17548981 1.84481939 2.8602621  1.09220124 5.73055845\n",
      " 2.11586348 2.7723259  3.84490362 4.09737255]\n",
      "##########\n",
      "epoch:17 step:16401 [D loss: 0.564873, acc.: 67.19%] [G loss: 1.356720]\n",
      "epoch:17 step:16402 [D loss: 0.515035, acc.: 73.44%] [G loss: 1.622595]\n",
      "epoch:17 step:16403 [D loss: 0.556925, acc.: 68.75%] [G loss: 1.402382]\n",
      "epoch:17 step:16404 [D loss: 0.540577, acc.: 69.53%] [G loss: 1.150691]\n",
      "epoch:17 step:16405 [D loss: 0.518814, acc.: 74.22%] [G loss: 1.303954]\n",
      "epoch:17 step:16406 [D loss: 0.586310, acc.: 74.22%] [G loss: 1.712169]\n",
      "epoch:17 step:16407 [D loss: 0.562714, acc.: 71.09%] [G loss: 1.348426]\n",
      "epoch:17 step:16408 [D loss: 0.503424, acc.: 73.44%] [G loss: 1.571180]\n",
      "epoch:17 step:16409 [D loss: 0.560266, acc.: 67.19%] [G loss: 1.200067]\n",
      "epoch:17 step:16410 [D loss: 0.490171, acc.: 75.00%] [G loss: 1.812905]\n",
      "epoch:17 step:16411 [D loss: 0.541595, acc.: 69.53%] [G loss: 1.320046]\n",
      "epoch:17 step:16412 [D loss: 0.725917, acc.: 53.12%] [G loss: 1.285301]\n",
      "epoch:17 step:16413 [D loss: 0.459381, acc.: 78.91%] [G loss: 1.296292]\n",
      "epoch:17 step:16414 [D loss: 0.519515, acc.: 70.31%] [G loss: 1.626194]\n",
      "epoch:17 step:16415 [D loss: 0.554206, acc.: 71.09%] [G loss: 1.259317]\n",
      "epoch:17 step:16416 [D loss: 0.580235, acc.: 74.22%] [G loss: 1.293103]\n",
      "epoch:17 step:16417 [D loss: 0.336436, acc.: 92.19%] [G loss: 1.671179]\n",
      "epoch:17 step:16418 [D loss: 0.635470, acc.: 66.41%] [G loss: 1.148048]\n",
      "epoch:17 step:16419 [D loss: 0.368828, acc.: 88.28%] [G loss: 1.324919]\n",
      "epoch:17 step:16420 [D loss: 0.359352, acc.: 91.41%] [G loss: 1.326840]\n",
      "epoch:17 step:16421 [D loss: 0.524156, acc.: 74.22%] [G loss: 1.130687]\n",
      "epoch:17 step:16422 [D loss: 0.574813, acc.: 73.44%] [G loss: 1.328645]\n",
      "epoch:17 step:16423 [D loss: 0.582529, acc.: 68.75%] [G loss: 1.430398]\n",
      "epoch:17 step:16424 [D loss: 0.572966, acc.: 66.41%] [G loss: 1.467996]\n",
      "epoch:17 step:16425 [D loss: 0.728785, acc.: 57.81%] [G loss: 1.496362]\n",
      "epoch:17 step:16426 [D loss: 0.434203, acc.: 80.47%] [G loss: 1.599906]\n",
      "epoch:17 step:16427 [D loss: 0.519564, acc.: 78.12%] [G loss: 1.314790]\n",
      "epoch:17 step:16428 [D loss: 0.491244, acc.: 77.34%] [G loss: 1.396441]\n",
      "epoch:17 step:16429 [D loss: 0.499424, acc.: 79.69%] [G loss: 1.324492]\n",
      "epoch:17 step:16430 [D loss: 0.599624, acc.: 67.97%] [G loss: 1.135647]\n",
      "epoch:17 step:16431 [D loss: 0.569696, acc.: 71.09%] [G loss: 1.458183]\n",
      "epoch:17 step:16432 [D loss: 0.462967, acc.: 86.72%] [G loss: 1.380739]\n",
      "epoch:17 step:16433 [D loss: 0.641881, acc.: 62.50%] [G loss: 1.230369]\n",
      "epoch:17 step:16434 [D loss: 0.514303, acc.: 75.78%] [G loss: 1.436363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16435 [D loss: 0.539890, acc.: 71.88%] [G loss: 1.304150]\n",
      "epoch:17 step:16436 [D loss: 0.517858, acc.: 75.00%] [G loss: 1.635855]\n",
      "epoch:17 step:16437 [D loss: 0.571951, acc.: 69.53%] [G loss: 1.376602]\n",
      "epoch:17 step:16438 [D loss: 0.530612, acc.: 73.44%] [G loss: 1.506174]\n",
      "epoch:17 step:16439 [D loss: 0.525285, acc.: 74.22%] [G loss: 1.403133]\n",
      "epoch:17 step:16440 [D loss: 0.449597, acc.: 83.59%] [G loss: 1.553034]\n",
      "epoch:17 step:16441 [D loss: 0.694041, acc.: 60.94%] [G loss: 1.168569]\n",
      "epoch:17 step:16442 [D loss: 0.489538, acc.: 78.12%] [G loss: 1.375455]\n",
      "epoch:17 step:16443 [D loss: 0.577146, acc.: 67.19%] [G loss: 1.363401]\n",
      "epoch:17 step:16444 [D loss: 0.702522, acc.: 53.91%] [G loss: 1.055741]\n",
      "epoch:17 step:16445 [D loss: 0.551319, acc.: 73.44%] [G loss: 1.606712]\n",
      "epoch:17 step:16446 [D loss: 0.643383, acc.: 65.62%] [G loss: 1.229394]\n",
      "epoch:17 step:16447 [D loss: 0.527992, acc.: 76.56%] [G loss: 1.257395]\n",
      "epoch:17 step:16448 [D loss: 0.547681, acc.: 73.44%] [G loss: 1.126831]\n",
      "epoch:17 step:16449 [D loss: 0.641031, acc.: 68.75%] [G loss: 1.485260]\n",
      "epoch:17 step:16450 [D loss: 0.489535, acc.: 78.91%] [G loss: 1.550033]\n",
      "epoch:17 step:16451 [D loss: 0.558162, acc.: 71.09%] [G loss: 1.504118]\n",
      "epoch:17 step:16452 [D loss: 0.411619, acc.: 89.06%] [G loss: 1.424807]\n",
      "epoch:17 step:16453 [D loss: 0.530459, acc.: 75.00%] [G loss: 1.127185]\n",
      "epoch:17 step:16454 [D loss: 0.613983, acc.: 67.97%] [G loss: 1.112752]\n",
      "epoch:17 step:16455 [D loss: 0.411192, acc.: 86.72%] [G loss: 1.419814]\n",
      "epoch:17 step:16456 [D loss: 0.568833, acc.: 69.53%] [G loss: 1.067808]\n",
      "epoch:17 step:16457 [D loss: 0.718313, acc.: 55.47%] [G loss: 1.171760]\n",
      "epoch:17 step:16458 [D loss: 0.367738, acc.: 88.28%] [G loss: 1.812452]\n",
      "epoch:17 step:16459 [D loss: 0.485518, acc.: 79.69%] [G loss: 1.416296]\n",
      "epoch:17 step:16460 [D loss: 0.478938, acc.: 75.00%] [G loss: 1.066927]\n",
      "epoch:17 step:16461 [D loss: 0.505873, acc.: 74.22%] [G loss: 1.277728]\n",
      "epoch:17 step:16462 [D loss: 0.567049, acc.: 65.62%] [G loss: 1.365594]\n",
      "epoch:17 step:16463 [D loss: 0.629416, acc.: 67.97%] [G loss: 1.134468]\n",
      "epoch:17 step:16464 [D loss: 0.518155, acc.: 80.47%] [G loss: 1.143830]\n",
      "epoch:17 step:16465 [D loss: 0.658730, acc.: 62.50%] [G loss: 1.340069]\n",
      "epoch:17 step:16466 [D loss: 0.577657, acc.: 67.97%] [G loss: 1.439864]\n",
      "epoch:17 step:16467 [D loss: 0.635103, acc.: 64.06%] [G loss: 1.110240]\n",
      "epoch:17 step:16468 [D loss: 0.546122, acc.: 70.31%] [G loss: 1.108215]\n",
      "epoch:17 step:16469 [D loss: 0.420577, acc.: 82.03%] [G loss: 1.434135]\n",
      "epoch:17 step:16470 [D loss: 0.482155, acc.: 78.91%] [G loss: 1.235838]\n",
      "epoch:17 step:16471 [D loss: 0.461573, acc.: 84.38%] [G loss: 1.443599]\n",
      "epoch:17 step:16472 [D loss: 0.562238, acc.: 72.66%] [G loss: 1.201341]\n",
      "epoch:17 step:16473 [D loss: 0.552820, acc.: 67.97%] [G loss: 1.205649]\n",
      "epoch:17 step:16474 [D loss: 0.690032, acc.: 60.16%] [G loss: 1.178496]\n",
      "epoch:17 step:16475 [D loss: 0.663041, acc.: 62.50%] [G loss: 0.944995]\n",
      "epoch:17 step:16476 [D loss: 0.707034, acc.: 53.91%] [G loss: 1.273901]\n",
      "epoch:17 step:16477 [D loss: 0.575666, acc.: 67.97%] [G loss: 1.337004]\n",
      "epoch:17 step:16478 [D loss: 0.480381, acc.: 78.91%] [G loss: 1.383038]\n",
      "epoch:17 step:16479 [D loss: 0.613951, acc.: 67.97%] [G loss: 1.235450]\n",
      "epoch:17 step:16480 [D loss: 0.698511, acc.: 55.47%] [G loss: 1.318651]\n",
      "epoch:17 step:16481 [D loss: 0.640312, acc.: 61.72%] [G loss: 1.411642]\n",
      "epoch:17 step:16482 [D loss: 0.716303, acc.: 57.81%] [G loss: 1.380757]\n",
      "epoch:17 step:16483 [D loss: 0.626900, acc.: 63.28%] [G loss: 1.408509]\n",
      "epoch:17 step:16484 [D loss: 0.523277, acc.: 78.91%] [G loss: 1.344096]\n",
      "epoch:17 step:16485 [D loss: 0.580817, acc.: 67.97%] [G loss: 1.293105]\n",
      "epoch:17 step:16486 [D loss: 0.609491, acc.: 61.72%] [G loss: 1.182313]\n",
      "epoch:17 step:16487 [D loss: 0.636554, acc.: 65.62%] [G loss: 1.301197]\n",
      "epoch:17 step:16488 [D loss: 0.487888, acc.: 78.91%] [G loss: 1.214391]\n",
      "epoch:17 step:16489 [D loss: 0.699069, acc.: 58.59%] [G loss: 1.505531]\n",
      "epoch:17 step:16490 [D loss: 0.656365, acc.: 60.94%] [G loss: 1.444686]\n",
      "epoch:17 step:16491 [D loss: 0.587353, acc.: 71.88%] [G loss: 1.273585]\n",
      "epoch:17 step:16492 [D loss: 0.474066, acc.: 80.47%] [G loss: 1.253563]\n",
      "epoch:17 step:16493 [D loss: 0.463251, acc.: 79.69%] [G loss: 1.218493]\n",
      "epoch:17 step:16494 [D loss: 0.702901, acc.: 63.28%] [G loss: 1.262854]\n",
      "epoch:17 step:16495 [D loss: 0.501668, acc.: 78.91%] [G loss: 0.948935]\n",
      "epoch:17 step:16496 [D loss: 0.587212, acc.: 71.09%] [G loss: 1.031728]\n",
      "epoch:17 step:16497 [D loss: 0.752825, acc.: 55.47%] [G loss: 1.153525]\n",
      "epoch:17 step:16498 [D loss: 0.663236, acc.: 65.62%] [G loss: 1.308534]\n",
      "epoch:17 step:16499 [D loss: 0.615677, acc.: 66.41%] [G loss: 1.404615]\n",
      "epoch:17 step:16500 [D loss: 0.649901, acc.: 63.28%] [G loss: 1.629139]\n",
      "epoch:17 step:16501 [D loss: 0.529509, acc.: 76.56%] [G loss: 1.158556]\n",
      "epoch:17 step:16502 [D loss: 0.687575, acc.: 57.81%] [G loss: 1.291371]\n",
      "epoch:17 step:16503 [D loss: 0.750969, acc.: 46.88%] [G loss: 1.438415]\n",
      "epoch:17 step:16504 [D loss: 0.543802, acc.: 71.88%] [G loss: 1.678562]\n",
      "epoch:17 step:16505 [D loss: 0.365349, acc.: 88.28%] [G loss: 1.578244]\n",
      "epoch:17 step:16506 [D loss: 0.591287, acc.: 67.19%] [G loss: 1.161199]\n",
      "epoch:17 step:16507 [D loss: 0.593781, acc.: 67.19%] [G loss: 1.478136]\n",
      "epoch:17 step:16508 [D loss: 0.533167, acc.: 75.78%] [G loss: 1.361509]\n",
      "epoch:17 step:16509 [D loss: 0.556547, acc.: 70.31%] [G loss: 1.066609]\n",
      "epoch:17 step:16510 [D loss: 0.587667, acc.: 69.53%] [G loss: 1.224221]\n",
      "epoch:17 step:16511 [D loss: 0.564270, acc.: 70.31%] [G loss: 1.118586]\n",
      "epoch:17 step:16512 [D loss: 0.492878, acc.: 77.34%] [G loss: 1.171309]\n",
      "epoch:17 step:16513 [D loss: 0.461671, acc.: 78.12%] [G loss: 1.271577]\n",
      "epoch:17 step:16514 [D loss: 0.606488, acc.: 64.84%] [G loss: 1.014043]\n",
      "epoch:17 step:16515 [D loss: 0.437072, acc.: 79.69%] [G loss: 1.398356]\n",
      "epoch:17 step:16516 [D loss: 0.578005, acc.: 64.06%] [G loss: 1.222347]\n",
      "epoch:17 step:16517 [D loss: 0.521014, acc.: 73.44%] [G loss: 1.247772]\n",
      "epoch:17 step:16518 [D loss: 0.564804, acc.: 67.19%] [G loss: 1.144530]\n",
      "epoch:17 step:16519 [D loss: 0.600571, acc.: 67.19%] [G loss: 1.177303]\n",
      "epoch:17 step:16520 [D loss: 0.423491, acc.: 83.59%] [G loss: 1.464749]\n",
      "epoch:17 step:16521 [D loss: 0.639714, acc.: 63.28%] [G loss: 1.322241]\n",
      "epoch:17 step:16522 [D loss: 0.643315, acc.: 60.94%] [G loss: 1.260913]\n",
      "epoch:17 step:16523 [D loss: 0.665737, acc.: 60.16%] [G loss: 0.756725]\n",
      "epoch:17 step:16524 [D loss: 0.512262, acc.: 75.00%] [G loss: 1.519631]\n",
      "epoch:17 step:16525 [D loss: 0.679968, acc.: 62.50%] [G loss: 1.015028]\n",
      "epoch:17 step:16526 [D loss: 0.452112, acc.: 78.91%] [G loss: 1.271985]\n",
      "epoch:17 step:16527 [D loss: 0.724298, acc.: 55.47%] [G loss: 1.260819]\n",
      "epoch:17 step:16528 [D loss: 0.443633, acc.: 79.69%] [G loss: 1.389425]\n",
      "epoch:17 step:16529 [D loss: 0.467936, acc.: 82.03%] [G loss: 1.291065]\n",
      "epoch:17 step:16530 [D loss: 0.692472, acc.: 61.72%] [G loss: 1.007044]\n",
      "epoch:17 step:16531 [D loss: 0.469306, acc.: 80.47%] [G loss: 1.501735]\n",
      "epoch:17 step:16532 [D loss: 0.633809, acc.: 64.06%] [G loss: 1.158617]\n",
      "epoch:17 step:16533 [D loss: 0.517061, acc.: 73.44%] [G loss: 1.350103]\n",
      "epoch:17 step:16534 [D loss: 0.741575, acc.: 47.66%] [G loss: 0.936995]\n",
      "epoch:17 step:16535 [D loss: 0.660372, acc.: 66.41%] [G loss: 1.056217]\n",
      "epoch:17 step:16536 [D loss: 0.541898, acc.: 73.44%] [G loss: 1.208797]\n",
      "epoch:17 step:16537 [D loss: 0.657405, acc.: 62.50%] [G loss: 1.522380]\n",
      "epoch:17 step:16538 [D loss: 0.500566, acc.: 80.47%] [G loss: 1.275729]\n",
      "epoch:17 step:16539 [D loss: 0.565464, acc.: 68.75%] [G loss: 0.857267]\n",
      "epoch:17 step:16540 [D loss: 0.623945, acc.: 64.06%] [G loss: 0.983620]\n",
      "epoch:17 step:16541 [D loss: 0.618376, acc.: 71.88%] [G loss: 1.005890]\n",
      "epoch:17 step:16542 [D loss: 0.529220, acc.: 73.44%] [G loss: 1.620386]\n",
      "epoch:17 step:16543 [D loss: 0.684040, acc.: 57.81%] [G loss: 1.358979]\n",
      "epoch:17 step:16544 [D loss: 0.412206, acc.: 84.38%] [G loss: 1.499325]\n",
      "epoch:17 step:16545 [D loss: 0.563906, acc.: 71.88%] [G loss: 1.421430]\n",
      "epoch:17 step:16546 [D loss: 0.502519, acc.: 75.78%] [G loss: 1.622159]\n",
      "epoch:17 step:16547 [D loss: 0.487850, acc.: 78.91%] [G loss: 1.540793]\n",
      "epoch:17 step:16548 [D loss: 0.701676, acc.: 58.59%] [G loss: 1.385514]\n",
      "epoch:17 step:16549 [D loss: 0.516501, acc.: 75.00%] [G loss: 1.183700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16550 [D loss: 0.618855, acc.: 67.19%] [G loss: 1.288392]\n",
      "epoch:17 step:16551 [D loss: 0.734123, acc.: 52.34%] [G loss: 0.977692]\n",
      "epoch:17 step:16552 [D loss: 0.509518, acc.: 73.44%] [G loss: 1.062756]\n",
      "epoch:17 step:16553 [D loss: 0.597820, acc.: 71.88%] [G loss: 1.100789]\n",
      "epoch:17 step:16554 [D loss: 0.521908, acc.: 77.34%] [G loss: 1.430571]\n",
      "epoch:17 step:16555 [D loss: 0.522529, acc.: 78.12%] [G loss: 1.248636]\n",
      "epoch:17 step:16556 [D loss: 0.453335, acc.: 81.25%] [G loss: 1.060882]\n",
      "epoch:17 step:16557 [D loss: 0.736110, acc.: 53.12%] [G loss: 1.259884]\n",
      "epoch:17 step:16558 [D loss: 0.582728, acc.: 71.88%] [G loss: 1.011402]\n",
      "epoch:17 step:16559 [D loss: 0.458318, acc.: 81.25%] [G loss: 1.249659]\n",
      "epoch:17 step:16560 [D loss: 0.678706, acc.: 59.38%] [G loss: 1.260132]\n",
      "epoch:17 step:16561 [D loss: 0.567497, acc.: 71.09%] [G loss: 1.371052]\n",
      "epoch:17 step:16562 [D loss: 0.479633, acc.: 79.69%] [G loss: 1.327210]\n",
      "epoch:17 step:16563 [D loss: 0.478038, acc.: 78.12%] [G loss: 1.258900]\n",
      "epoch:17 step:16564 [D loss: 0.589400, acc.: 69.53%] [G loss: 1.149611]\n",
      "epoch:17 step:16565 [D loss: 0.534399, acc.: 75.78%] [G loss: 1.189886]\n",
      "epoch:17 step:16566 [D loss: 0.601333, acc.: 70.31%] [G loss: 1.569502]\n",
      "epoch:17 step:16567 [D loss: 0.640550, acc.: 62.50%] [G loss: 1.327184]\n",
      "epoch:17 step:16568 [D loss: 0.686085, acc.: 55.47%] [G loss: 1.443367]\n",
      "epoch:17 step:16569 [D loss: 0.576115, acc.: 70.31%] [G loss: 1.026046]\n",
      "epoch:17 step:16570 [D loss: 0.545516, acc.: 73.44%] [G loss: 1.169424]\n",
      "epoch:17 step:16571 [D loss: 0.547718, acc.: 70.31%] [G loss: 1.425744]\n",
      "epoch:17 step:16572 [D loss: 0.648189, acc.: 60.94%] [G loss: 1.329380]\n",
      "epoch:17 step:16573 [D loss: 0.605959, acc.: 63.28%] [G loss: 1.164560]\n",
      "epoch:17 step:16574 [D loss: 0.409996, acc.: 88.28%] [G loss: 1.595822]\n",
      "epoch:17 step:16575 [D loss: 0.617385, acc.: 67.97%] [G loss: 1.468828]\n",
      "epoch:17 step:16576 [D loss: 0.596910, acc.: 64.84%] [G loss: 1.117474]\n",
      "epoch:17 step:16577 [D loss: 0.483347, acc.: 78.91%] [G loss: 1.398619]\n",
      "epoch:17 step:16578 [D loss: 0.506846, acc.: 79.69%] [G loss: 1.131036]\n",
      "epoch:17 step:16579 [D loss: 0.456454, acc.: 83.59%] [G loss: 1.359588]\n",
      "epoch:17 step:16580 [D loss: 0.513294, acc.: 76.56%] [G loss: 1.418935]\n",
      "epoch:17 step:16581 [D loss: 0.645113, acc.: 64.06%] [G loss: 1.212268]\n",
      "epoch:17 step:16582 [D loss: 0.581549, acc.: 64.06%] [G loss: 1.286525]\n",
      "epoch:17 step:16583 [D loss: 0.450328, acc.: 82.81%] [G loss: 1.527647]\n",
      "epoch:17 step:16584 [D loss: 0.553224, acc.: 75.00%] [G loss: 1.349396]\n",
      "epoch:17 step:16585 [D loss: 0.412432, acc.: 85.94%] [G loss: 1.321069]\n",
      "epoch:17 step:16586 [D loss: 0.466300, acc.: 82.81%] [G loss: 1.441724]\n",
      "epoch:17 step:16587 [D loss: 0.652166, acc.: 64.84%] [G loss: 1.172004]\n",
      "epoch:17 step:16588 [D loss: 0.511405, acc.: 75.78%] [G loss: 1.327130]\n",
      "epoch:17 step:16589 [D loss: 0.640243, acc.: 64.06%] [G loss: 1.049959]\n",
      "epoch:17 step:16590 [D loss: 0.484327, acc.: 78.12%] [G loss: 1.450452]\n",
      "epoch:17 step:16591 [D loss: 0.579243, acc.: 69.53%] [G loss: 1.261861]\n",
      "epoch:17 step:16592 [D loss: 0.678483, acc.: 63.28%] [G loss: 1.375793]\n",
      "epoch:17 step:16593 [D loss: 0.595074, acc.: 71.09%] [G loss: 1.001390]\n",
      "epoch:17 step:16594 [D loss: 0.634507, acc.: 60.94%] [G loss: 1.150160]\n",
      "epoch:17 step:16595 [D loss: 0.371119, acc.: 87.50%] [G loss: 1.429986]\n",
      "epoch:17 step:16596 [D loss: 0.526510, acc.: 75.78%] [G loss: 1.049417]\n",
      "epoch:17 step:16597 [D loss: 0.603966, acc.: 67.97%] [G loss: 1.232855]\n",
      "epoch:17 step:16598 [D loss: 0.511579, acc.: 73.44%] [G loss: 1.092064]\n",
      "epoch:17 step:16599 [D loss: 0.510554, acc.: 75.00%] [G loss: 1.262079]\n",
      "epoch:17 step:16600 [D loss: 0.634224, acc.: 64.06%] [G loss: 1.593357]\n",
      "##############\n",
      "[2.70375738 1.9989228  1.809228   3.00600983 0.93780552 5.97796664\n",
      " 2.20013791 2.61438945 3.82399737 8.14868929]\n",
      "##########\n",
      "epoch:17 step:16601 [D loss: 0.585549, acc.: 68.75%] [G loss: 1.535051]\n",
      "epoch:17 step:16602 [D loss: 0.616073, acc.: 67.19%] [G loss: 1.385199]\n",
      "epoch:17 step:16603 [D loss: 0.570490, acc.: 72.66%] [G loss: 1.302027]\n",
      "epoch:17 step:16604 [D loss: 0.763472, acc.: 48.44%] [G loss: 1.318669]\n",
      "epoch:17 step:16605 [D loss: 0.618829, acc.: 66.41%] [G loss: 1.327518]\n",
      "epoch:17 step:16606 [D loss: 0.451036, acc.: 79.69%] [G loss: 1.563634]\n",
      "epoch:17 step:16607 [D loss: 0.541324, acc.: 76.56%] [G loss: 1.090430]\n",
      "epoch:17 step:16608 [D loss: 0.712829, acc.: 54.69%] [G loss: 1.163027]\n",
      "epoch:17 step:16609 [D loss: 0.675113, acc.: 60.16%] [G loss: 1.076367]\n",
      "epoch:17 step:16610 [D loss: 0.582985, acc.: 65.62%] [G loss: 1.315599]\n",
      "epoch:17 step:16611 [D loss: 0.580091, acc.: 69.53%] [G loss: 1.382576]\n",
      "epoch:17 step:16612 [D loss: 0.666256, acc.: 62.50%] [G loss: 1.364561]\n",
      "epoch:17 step:16613 [D loss: 0.553569, acc.: 69.53%] [G loss: 1.483568]\n",
      "epoch:17 step:16614 [D loss: 0.738379, acc.: 53.12%] [G loss: 1.189050]\n",
      "epoch:17 step:16615 [D loss: 0.389463, acc.: 88.28%] [G loss: 1.378299]\n",
      "epoch:17 step:16616 [D loss: 0.606225, acc.: 65.62%] [G loss: 1.451744]\n",
      "epoch:17 step:16617 [D loss: 0.553621, acc.: 72.66%] [G loss: 1.220193]\n",
      "epoch:17 step:16618 [D loss: 0.625804, acc.: 66.41%] [G loss: 1.175693]\n",
      "epoch:17 step:16619 [D loss: 0.555172, acc.: 71.09%] [G loss: 1.695875]\n",
      "epoch:17 step:16620 [D loss: 0.492465, acc.: 78.91%] [G loss: 1.704976]\n",
      "epoch:17 step:16621 [D loss: 0.566895, acc.: 68.75%] [G loss: 1.073951]\n",
      "epoch:17 step:16622 [D loss: 0.525954, acc.: 73.44%] [G loss: 1.596214]\n",
      "epoch:17 step:16623 [D loss: 0.679474, acc.: 60.16%] [G loss: 1.280705]\n",
      "epoch:17 step:16624 [D loss: 0.523363, acc.: 75.78%] [G loss: 1.241452]\n",
      "epoch:17 step:16625 [D loss: 0.555330, acc.: 71.88%] [G loss: 1.671180]\n",
      "epoch:17 step:16626 [D loss: 0.550395, acc.: 74.22%] [G loss: 1.513382]\n",
      "epoch:17 step:16627 [D loss: 0.596526, acc.: 69.53%] [G loss: 1.172639]\n",
      "epoch:17 step:16628 [D loss: 0.552875, acc.: 76.56%] [G loss: 1.279568]\n",
      "epoch:17 step:16629 [D loss: 0.649824, acc.: 62.50%] [G loss: 1.355939]\n",
      "epoch:17 step:16630 [D loss: 0.406200, acc.: 85.94%] [G loss: 1.465398]\n",
      "epoch:17 step:16631 [D loss: 0.486736, acc.: 82.03%] [G loss: 1.214524]\n",
      "epoch:17 step:16632 [D loss: 0.601438, acc.: 64.06%] [G loss: 1.269985]\n",
      "epoch:17 step:16633 [D loss: 0.760270, acc.: 47.66%] [G loss: 1.255020]\n",
      "epoch:17 step:16634 [D loss: 0.488168, acc.: 76.56%] [G loss: 1.266258]\n",
      "epoch:17 step:16635 [D loss: 0.694199, acc.: 60.94%] [G loss: 1.313470]\n",
      "epoch:17 step:16636 [D loss: 0.493781, acc.: 77.34%] [G loss: 1.311830]\n",
      "epoch:17 step:16637 [D loss: 0.728668, acc.: 60.94%] [G loss: 1.165014]\n",
      "epoch:17 step:16638 [D loss: 0.729851, acc.: 59.38%] [G loss: 1.255958]\n",
      "epoch:17 step:16639 [D loss: 0.543799, acc.: 75.00%] [G loss: 1.432074]\n",
      "epoch:17 step:16640 [D loss: 0.548416, acc.: 73.44%] [G loss: 1.484151]\n",
      "epoch:17 step:16641 [D loss: 0.530748, acc.: 73.44%] [G loss: 1.457408]\n",
      "epoch:17 step:16642 [D loss: 0.433288, acc.: 83.59%] [G loss: 1.714619]\n",
      "epoch:17 step:16643 [D loss: 0.662676, acc.: 57.81%] [G loss: 1.407044]\n",
      "epoch:17 step:16644 [D loss: 0.522845, acc.: 75.00%] [G loss: 0.934809]\n",
      "epoch:17 step:16645 [D loss: 0.626666, acc.: 68.75%] [G loss: 1.360188]\n",
      "epoch:17 step:16646 [D loss: 0.589109, acc.: 72.66%] [G loss: 1.522332]\n",
      "epoch:17 step:16647 [D loss: 0.528476, acc.: 71.88%] [G loss: 1.320754]\n",
      "epoch:17 step:16648 [D loss: 0.444938, acc.: 81.25%] [G loss: 1.446554]\n",
      "epoch:17 step:16649 [D loss: 0.728973, acc.: 57.81%] [G loss: 1.173284]\n",
      "epoch:17 step:16650 [D loss: 0.647442, acc.: 66.41%] [G loss: 1.211766]\n",
      "epoch:17 step:16651 [D loss: 0.616147, acc.: 67.19%] [G loss: 1.071650]\n",
      "epoch:17 step:16652 [D loss: 0.477715, acc.: 79.69%] [G loss: 1.373327]\n",
      "epoch:17 step:16653 [D loss: 0.528079, acc.: 71.88%] [G loss: 1.128125]\n",
      "epoch:17 step:16654 [D loss: 0.670448, acc.: 58.59%] [G loss: 0.927544]\n",
      "epoch:17 step:16655 [D loss: 0.582042, acc.: 68.75%] [G loss: 1.427383]\n",
      "epoch:17 step:16656 [D loss: 0.690561, acc.: 55.47%] [G loss: 1.213557]\n",
      "epoch:17 step:16657 [D loss: 0.496130, acc.: 75.78%] [G loss: 1.356635]\n",
      "epoch:17 step:16658 [D loss: 0.628358, acc.: 65.62%] [G loss: 1.141167]\n",
      "epoch:17 step:16659 [D loss: 0.687799, acc.: 62.50%] [G loss: 0.896333]\n",
      "epoch:17 step:16660 [D loss: 0.554970, acc.: 69.53%] [G loss: 1.517936]\n",
      "epoch:17 step:16661 [D loss: 0.578835, acc.: 67.97%] [G loss: 1.354001]\n",
      "epoch:17 step:16662 [D loss: 0.490809, acc.: 77.34%] [G loss: 1.570779]\n",
      "epoch:17 step:16663 [D loss: 0.544970, acc.: 74.22%] [G loss: 1.373111]\n",
      "epoch:17 step:16664 [D loss: 0.633097, acc.: 60.94%] [G loss: 1.146549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16665 [D loss: 0.535941, acc.: 75.00%] [G loss: 1.557589]\n",
      "epoch:17 step:16666 [D loss: 0.638805, acc.: 64.84%] [G loss: 1.204880]\n",
      "epoch:17 step:16667 [D loss: 0.664303, acc.: 62.50%] [G loss: 1.192211]\n",
      "epoch:17 step:16668 [D loss: 0.479574, acc.: 78.91%] [G loss: 1.437526]\n",
      "epoch:17 step:16669 [D loss: 0.592970, acc.: 67.97%] [G loss: 1.222958]\n",
      "epoch:17 step:16670 [D loss: 0.534968, acc.: 75.78%] [G loss: 1.280285]\n",
      "epoch:17 step:16671 [D loss: 0.491980, acc.: 75.00%] [G loss: 1.304268]\n",
      "epoch:17 step:16672 [D loss: 0.607722, acc.: 63.28%] [G loss: 1.101094]\n",
      "epoch:17 step:16673 [D loss: 0.528093, acc.: 71.09%] [G loss: 1.519801]\n",
      "epoch:17 step:16674 [D loss: 0.604204, acc.: 65.62%] [G loss: 1.360834]\n",
      "epoch:17 step:16675 [D loss: 0.601284, acc.: 66.41%] [G loss: 1.253711]\n",
      "epoch:17 step:16676 [D loss: 0.565279, acc.: 68.75%] [G loss: 1.355770]\n",
      "epoch:17 step:16677 [D loss: 0.621247, acc.: 66.41%] [G loss: 1.388241]\n",
      "epoch:17 step:16678 [D loss: 0.737011, acc.: 56.25%] [G loss: 1.208855]\n",
      "epoch:17 step:16679 [D loss: 0.623562, acc.: 61.72%] [G loss: 1.637984]\n",
      "epoch:17 step:16680 [D loss: 0.593581, acc.: 71.09%] [G loss: 1.133951]\n",
      "epoch:17 step:16681 [D loss: 0.574055, acc.: 69.53%] [G loss: 1.305543]\n",
      "epoch:17 step:16682 [D loss: 0.481430, acc.: 73.44%] [G loss: 1.505782]\n",
      "epoch:17 step:16683 [D loss: 0.727365, acc.: 55.47%] [G loss: 1.543530]\n",
      "epoch:17 step:16684 [D loss: 0.685104, acc.: 60.16%] [G loss: 1.120946]\n",
      "epoch:17 step:16685 [D loss: 0.592459, acc.: 67.19%] [G loss: 1.192488]\n",
      "epoch:17 step:16686 [D loss: 0.502971, acc.: 74.22%] [G loss: 1.278384]\n",
      "epoch:17 step:16687 [D loss: 0.657069, acc.: 67.19%] [G loss: 1.405665]\n",
      "epoch:17 step:16688 [D loss: 0.406872, acc.: 85.94%] [G loss: 1.529597]\n",
      "epoch:17 step:16689 [D loss: 0.559397, acc.: 73.44%] [G loss: 1.530417]\n",
      "epoch:17 step:16690 [D loss: 0.765130, acc.: 57.81%] [G loss: 1.226506]\n",
      "epoch:17 step:16691 [D loss: 0.661077, acc.: 58.59%] [G loss: 1.244687]\n",
      "epoch:17 step:16692 [D loss: 0.540489, acc.: 72.66%] [G loss: 1.251270]\n",
      "epoch:17 step:16693 [D loss: 0.495612, acc.: 75.00%] [G loss: 1.211675]\n",
      "epoch:17 step:16694 [D loss: 0.433481, acc.: 83.59%] [G loss: 1.668899]\n",
      "epoch:17 step:16695 [D loss: 0.534921, acc.: 74.22%] [G loss: 1.312437]\n",
      "epoch:17 step:16696 [D loss: 0.538383, acc.: 74.22%] [G loss: 1.535403]\n",
      "epoch:17 step:16697 [D loss: 0.494376, acc.: 78.12%] [G loss: 1.069196]\n",
      "epoch:17 step:16698 [D loss: 0.471421, acc.: 82.03%] [G loss: 1.046016]\n",
      "epoch:17 step:16699 [D loss: 0.458305, acc.: 80.47%] [G loss: 1.491681]\n",
      "epoch:17 step:16700 [D loss: 0.536711, acc.: 77.34%] [G loss: 1.099990]\n",
      "epoch:17 step:16701 [D loss: 0.498720, acc.: 77.34%] [G loss: 1.455354]\n",
      "epoch:17 step:16702 [D loss: 0.489007, acc.: 75.78%] [G loss: 1.210336]\n",
      "epoch:17 step:16703 [D loss: 0.485988, acc.: 75.78%] [G loss: 1.580955]\n",
      "epoch:17 step:16704 [D loss: 0.684236, acc.: 62.50%] [G loss: 1.444244]\n",
      "epoch:17 step:16705 [D loss: 0.548510, acc.: 72.66%] [G loss: 1.431211]\n",
      "epoch:17 step:16706 [D loss: 0.573909, acc.: 67.19%] [G loss: 1.482591]\n",
      "epoch:17 step:16707 [D loss: 0.634383, acc.: 62.50%] [G loss: 1.247691]\n",
      "epoch:17 step:16708 [D loss: 0.570727, acc.: 74.22%] [G loss: 1.393654]\n",
      "epoch:17 step:16709 [D loss: 0.645001, acc.: 59.38%] [G loss: 1.211280]\n",
      "epoch:17 step:16710 [D loss: 0.587989, acc.: 70.31%] [G loss: 1.140855]\n",
      "epoch:17 step:16711 [D loss: 0.558216, acc.: 71.09%] [G loss: 1.590770]\n",
      "epoch:17 step:16712 [D loss: 0.461876, acc.: 78.91%] [G loss: 1.296673]\n",
      "epoch:17 step:16713 [D loss: 0.530555, acc.: 74.22%] [G loss: 1.314379]\n",
      "epoch:17 step:16714 [D loss: 0.488032, acc.: 79.69%] [G loss: 1.584249]\n",
      "epoch:17 step:16715 [D loss: 0.668970, acc.: 57.81%] [G loss: 1.195751]\n",
      "epoch:17 step:16716 [D loss: 0.574224, acc.: 69.53%] [G loss: 1.469556]\n",
      "epoch:17 step:16717 [D loss: 0.627812, acc.: 67.19%] [G loss: 1.459181]\n",
      "epoch:17 step:16718 [D loss: 0.616723, acc.: 70.31%] [G loss: 1.137519]\n",
      "epoch:17 step:16719 [D loss: 0.458147, acc.: 79.69%] [G loss: 1.301532]\n",
      "epoch:17 step:16720 [D loss: 0.448694, acc.: 83.59%] [G loss: 1.695860]\n",
      "epoch:17 step:16721 [D loss: 0.662629, acc.: 59.38%] [G loss: 1.460946]\n",
      "epoch:17 step:16722 [D loss: 0.573576, acc.: 70.31%] [G loss: 1.205490]\n",
      "epoch:17 step:16723 [D loss: 0.509764, acc.: 73.44%] [G loss: 1.456971]\n",
      "epoch:17 step:16724 [D loss: 0.583412, acc.: 68.75%] [G loss: 1.118261]\n",
      "epoch:17 step:16725 [D loss: 0.563817, acc.: 67.97%] [G loss: 1.485322]\n",
      "epoch:17 step:16726 [D loss: 0.540029, acc.: 75.00%] [G loss: 1.444268]\n",
      "epoch:17 step:16727 [D loss: 0.631949, acc.: 68.75%] [G loss: 1.251581]\n",
      "epoch:17 step:16728 [D loss: 0.388918, acc.: 87.50%] [G loss: 1.629791]\n",
      "epoch:17 step:16729 [D loss: 0.610002, acc.: 69.53%] [G loss: 1.485134]\n",
      "epoch:17 step:16730 [D loss: 0.674491, acc.: 61.72%] [G loss: 1.421879]\n",
      "epoch:17 step:16731 [D loss: 0.713909, acc.: 57.03%] [G loss: 1.170981]\n",
      "epoch:17 step:16732 [D loss: 0.509572, acc.: 74.22%] [G loss: 1.139909]\n",
      "epoch:17 step:16733 [D loss: 0.488376, acc.: 77.34%] [G loss: 1.406500]\n",
      "epoch:17 step:16734 [D loss: 0.671540, acc.: 60.94%] [G loss: 1.240337]\n",
      "epoch:17 step:16735 [D loss: 0.606933, acc.: 68.75%] [G loss: 1.169881]\n",
      "epoch:17 step:16736 [D loss: 0.596570, acc.: 64.84%] [G loss: 1.206500]\n",
      "epoch:17 step:16737 [D loss: 0.475707, acc.: 81.25%] [G loss: 1.241954]\n",
      "epoch:17 step:16738 [D loss: 0.588406, acc.: 69.53%] [G loss: 1.150486]\n",
      "epoch:17 step:16739 [D loss: 0.520950, acc.: 78.12%] [G loss: 1.392325]\n",
      "epoch:17 step:16740 [D loss: 0.534304, acc.: 74.22%] [G loss: 1.544578]\n",
      "epoch:17 step:16741 [D loss: 0.598588, acc.: 65.62%] [G loss: 1.055310]\n",
      "epoch:17 step:16742 [D loss: 0.544440, acc.: 73.44%] [G loss: 1.201107]\n",
      "epoch:17 step:16743 [D loss: 0.514480, acc.: 76.56%] [G loss: 1.264410]\n",
      "epoch:17 step:16744 [D loss: 0.588958, acc.: 66.41%] [G loss: 1.296422]\n",
      "epoch:17 step:16745 [D loss: 0.531361, acc.: 75.00%] [G loss: 1.151325]\n",
      "epoch:17 step:16746 [D loss: 0.577474, acc.: 69.53%] [G loss: 1.582286]\n",
      "epoch:17 step:16747 [D loss: 0.565563, acc.: 71.09%] [G loss: 0.954533]\n",
      "epoch:17 step:16748 [D loss: 0.571439, acc.: 69.53%] [G loss: 1.095435]\n",
      "epoch:17 step:16749 [D loss: 0.695735, acc.: 57.03%] [G loss: 1.200693]\n",
      "epoch:17 step:16750 [D loss: 0.601597, acc.: 67.19%] [G loss: 1.187337]\n",
      "epoch:17 step:16751 [D loss: 0.632796, acc.: 68.75%] [G loss: 1.245577]\n",
      "epoch:17 step:16752 [D loss: 0.745227, acc.: 52.34%] [G loss: 0.988400]\n",
      "epoch:17 step:16753 [D loss: 0.604644, acc.: 69.53%] [G loss: 1.346262]\n",
      "epoch:17 step:16754 [D loss: 0.572786, acc.: 71.88%] [G loss: 1.171889]\n",
      "epoch:17 step:16755 [D loss: 0.494135, acc.: 76.56%] [G loss: 1.409694]\n",
      "epoch:17 step:16756 [D loss: 0.600157, acc.: 67.19%] [G loss: 1.636850]\n",
      "epoch:17 step:16757 [D loss: 0.632281, acc.: 63.28%] [G loss: 1.150171]\n",
      "epoch:17 step:16758 [D loss: 0.550200, acc.: 74.22%] [G loss: 1.248537]\n",
      "epoch:17 step:16759 [D loss: 0.571920, acc.: 65.62%] [G loss: 1.145086]\n",
      "epoch:17 step:16760 [D loss: 0.676449, acc.: 57.81%] [G loss: 1.130972]\n",
      "epoch:17 step:16761 [D loss: 0.521768, acc.: 73.44%] [G loss: 1.524718]\n",
      "epoch:17 step:16762 [D loss: 0.664694, acc.: 64.06%] [G loss: 1.339959]\n",
      "epoch:17 step:16763 [D loss: 0.610646, acc.: 67.97%] [G loss: 1.081576]\n",
      "epoch:17 step:16764 [D loss: 0.563126, acc.: 66.41%] [G loss: 1.199787]\n",
      "epoch:17 step:16765 [D loss: 0.664205, acc.: 63.28%] [G loss: 1.432965]\n",
      "epoch:17 step:16766 [D loss: 0.557991, acc.: 71.09%] [G loss: 1.406333]\n",
      "epoch:17 step:16767 [D loss: 0.463858, acc.: 83.59%] [G loss: 1.318852]\n",
      "epoch:17 step:16768 [D loss: 0.563407, acc.: 73.44%] [G loss: 1.344503]\n",
      "epoch:17 step:16769 [D loss: 0.502181, acc.: 79.69%] [G loss: 1.298162]\n",
      "epoch:17 step:16770 [D loss: 0.453733, acc.: 79.69%] [G loss: 1.575608]\n",
      "epoch:17 step:16771 [D loss: 0.571207, acc.: 71.88%] [G loss: 1.508189]\n",
      "epoch:17 step:16772 [D loss: 0.518624, acc.: 75.78%] [G loss: 1.384161]\n",
      "epoch:17 step:16773 [D loss: 0.669535, acc.: 64.06%] [G loss: 1.236457]\n",
      "epoch:17 step:16774 [D loss: 0.581791, acc.: 69.53%] [G loss: 1.083178]\n",
      "epoch:17 step:16775 [D loss: 0.586530, acc.: 71.88%] [G loss: 1.330592]\n",
      "epoch:17 step:16776 [D loss: 0.746526, acc.: 55.47%] [G loss: 1.023534]\n",
      "epoch:17 step:16777 [D loss: 0.622236, acc.: 62.50%] [G loss: 1.315023]\n",
      "epoch:17 step:16778 [D loss: 0.683547, acc.: 64.84%] [G loss: 1.281268]\n",
      "epoch:17 step:16779 [D loss: 0.534916, acc.: 75.00%] [G loss: 1.406696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16780 [D loss: 0.551010, acc.: 72.66%] [G loss: 1.285848]\n",
      "epoch:17 step:16781 [D loss: 0.514125, acc.: 73.44%] [G loss: 1.373624]\n",
      "epoch:17 step:16782 [D loss: 0.430083, acc.: 83.59%] [G loss: 1.457577]\n",
      "epoch:17 step:16783 [D loss: 0.701326, acc.: 59.38%] [G loss: 0.992620]\n",
      "epoch:17 step:16784 [D loss: 0.547930, acc.: 74.22%] [G loss: 1.104423]\n",
      "epoch:17 step:16785 [D loss: 0.481938, acc.: 78.12%] [G loss: 1.277776]\n",
      "epoch:17 step:16786 [D loss: 0.763547, acc.: 51.56%] [G loss: 1.004147]\n",
      "epoch:17 step:16787 [D loss: 0.461920, acc.: 78.91%] [G loss: 1.308905]\n",
      "epoch:17 step:16788 [D loss: 0.548544, acc.: 71.09%] [G loss: 1.175518]\n",
      "epoch:17 step:16789 [D loss: 0.771037, acc.: 50.78%] [G loss: 0.785839]\n",
      "epoch:17 step:16790 [D loss: 0.491811, acc.: 78.91%] [G loss: 1.242221]\n",
      "epoch:17 step:16791 [D loss: 0.549171, acc.: 67.97%] [G loss: 1.182566]\n",
      "epoch:17 step:16792 [D loss: 0.453002, acc.: 83.59%] [G loss: 1.278977]\n",
      "epoch:17 step:16793 [D loss: 0.592596, acc.: 68.75%] [G loss: 1.157434]\n",
      "epoch:17 step:16794 [D loss: 0.526610, acc.: 77.34%] [G loss: 1.457032]\n",
      "epoch:17 step:16795 [D loss: 0.589810, acc.: 67.97%] [G loss: 1.129881]\n",
      "epoch:17 step:16796 [D loss: 0.492169, acc.: 78.91%] [G loss: 1.183357]\n",
      "epoch:17 step:16797 [D loss: 0.603786, acc.: 71.09%] [G loss: 1.217062]\n",
      "epoch:17 step:16798 [D loss: 0.514572, acc.: 74.22%] [G loss: 1.416490]\n",
      "epoch:17 step:16799 [D loss: 0.562180, acc.: 73.44%] [G loss: 1.415999]\n",
      "epoch:17 step:16800 [D loss: 0.655237, acc.: 60.94%] [G loss: 1.295743]\n",
      "##############\n",
      "[2.60742977 1.91904787 1.85545177 2.89371877 0.68259254 6.34799122\n",
      " 2.20058995 2.29791482 3.89757217 4.38147186]\n",
      "##########\n",
      "epoch:17 step:16801 [D loss: 0.504872, acc.: 71.09%] [G loss: 1.397985]\n",
      "epoch:17 step:16802 [D loss: 0.465372, acc.: 81.25%] [G loss: 1.216029]\n",
      "epoch:17 step:16803 [D loss: 0.617054, acc.: 61.72%] [G loss: 1.426816]\n",
      "epoch:17 step:16804 [D loss: 0.499458, acc.: 74.22%] [G loss: 1.443620]\n",
      "epoch:17 step:16805 [D loss: 0.691839, acc.: 61.72%] [G loss: 1.079782]\n",
      "epoch:17 step:16806 [D loss: 0.572830, acc.: 67.97%] [G loss: 1.215042]\n",
      "epoch:17 step:16807 [D loss: 0.586719, acc.: 66.41%] [G loss: 1.373023]\n",
      "epoch:17 step:16808 [D loss: 0.732147, acc.: 53.12%] [G loss: 1.231699]\n",
      "epoch:17 step:16809 [D loss: 0.520428, acc.: 71.88%] [G loss: 1.118219]\n",
      "epoch:17 step:16810 [D loss: 0.464746, acc.: 79.69%] [G loss: 1.513697]\n",
      "epoch:17 step:16811 [D loss: 0.507355, acc.: 81.25%] [G loss: 1.231171]\n",
      "epoch:17 step:16812 [D loss: 0.681345, acc.: 61.72%] [G loss: 1.427004]\n",
      "epoch:17 step:16813 [D loss: 0.481313, acc.: 80.47%] [G loss: 1.527735]\n",
      "epoch:17 step:16814 [D loss: 0.537204, acc.: 74.22%] [G loss: 1.547425]\n",
      "epoch:17 step:16815 [D loss: 0.599463, acc.: 69.53%] [G loss: 1.260280]\n",
      "epoch:17 step:16816 [D loss: 0.684753, acc.: 60.94%] [G loss: 1.050329]\n",
      "epoch:17 step:16817 [D loss: 0.602853, acc.: 70.31%] [G loss: 1.148959]\n",
      "epoch:17 step:16818 [D loss: 0.506619, acc.: 78.12%] [G loss: 1.518395]\n",
      "epoch:17 step:16819 [D loss: 0.670835, acc.: 60.16%] [G loss: 1.047334]\n",
      "epoch:17 step:16820 [D loss: 0.594172, acc.: 67.97%] [G loss: 1.125438]\n",
      "epoch:17 step:16821 [D loss: 0.564623, acc.: 72.66%] [G loss: 1.004461]\n",
      "epoch:17 step:16822 [D loss: 0.666746, acc.: 60.94%] [G loss: 1.118848]\n",
      "epoch:17 step:16823 [D loss: 0.509637, acc.: 78.12%] [G loss: 1.227071]\n",
      "epoch:17 step:16824 [D loss: 0.552982, acc.: 77.34%] [G loss: 1.083420]\n",
      "epoch:17 step:16825 [D loss: 0.484962, acc.: 77.34%] [G loss: 1.888875]\n",
      "epoch:17 step:16826 [D loss: 0.481895, acc.: 74.22%] [G loss: 1.372336]\n",
      "epoch:17 step:16827 [D loss: 0.649433, acc.: 60.94%] [G loss: 1.372857]\n",
      "epoch:17 step:16828 [D loss: 0.533717, acc.: 70.31%] [G loss: 1.265947]\n",
      "epoch:17 step:16829 [D loss: 0.635542, acc.: 65.62%] [G loss: 1.193329]\n",
      "epoch:17 step:16830 [D loss: 0.564981, acc.: 64.06%] [G loss: 1.487625]\n",
      "epoch:17 step:16831 [D loss: 0.606291, acc.: 61.72%] [G loss: 1.252663]\n",
      "epoch:17 step:16832 [D loss: 0.554089, acc.: 75.00%] [G loss: 1.108549]\n",
      "epoch:17 step:16833 [D loss: 0.590727, acc.: 65.62%] [G loss: 1.318548]\n",
      "epoch:17 step:16834 [D loss: 0.696484, acc.: 59.38%] [G loss: 1.184768]\n",
      "epoch:17 step:16835 [D loss: 0.549486, acc.: 71.88%] [G loss: 1.449824]\n",
      "epoch:17 step:16836 [D loss: 0.594559, acc.: 68.75%] [G loss: 1.293975]\n",
      "epoch:17 step:16837 [D loss: 0.504215, acc.: 76.56%] [G loss: 1.120675]\n",
      "epoch:17 step:16838 [D loss: 0.571805, acc.: 71.09%] [G loss: 1.453370]\n",
      "epoch:17 step:16839 [D loss: 0.586941, acc.: 70.31%] [G loss: 1.412463]\n",
      "epoch:17 step:16840 [D loss: 0.436896, acc.: 82.03%] [G loss: 1.584098]\n",
      "epoch:17 step:16841 [D loss: 0.529018, acc.: 72.66%] [G loss: 1.504666]\n",
      "epoch:17 step:16842 [D loss: 0.580943, acc.: 71.88%] [G loss: 1.356972]\n",
      "epoch:17 step:16843 [D loss: 0.716943, acc.: 60.16%] [G loss: 1.228371]\n",
      "epoch:17 step:16844 [D loss: 0.607734, acc.: 64.84%] [G loss: 1.305816]\n",
      "epoch:17 step:16845 [D loss: 0.451454, acc.: 82.81%] [G loss: 1.179238]\n",
      "epoch:17 step:16846 [D loss: 0.659783, acc.: 62.50%] [G loss: 1.089482]\n",
      "epoch:17 step:16847 [D loss: 0.560463, acc.: 72.66%] [G loss: 1.485186]\n",
      "epoch:17 step:16848 [D loss: 0.625309, acc.: 69.53%] [G loss: 1.556043]\n",
      "epoch:17 step:16849 [D loss: 0.579901, acc.: 70.31%] [G loss: 1.420325]\n",
      "epoch:17 step:16850 [D loss: 0.558611, acc.: 71.09%] [G loss: 1.280311]\n",
      "epoch:17 step:16851 [D loss: 0.575229, acc.: 71.09%] [G loss: 1.407543]\n",
      "epoch:17 step:16852 [D loss: 0.659594, acc.: 57.03%] [G loss: 1.191948]\n",
      "epoch:17 step:16853 [D loss: 0.482024, acc.: 77.34%] [G loss: 1.630544]\n",
      "epoch:17 step:16854 [D loss: 0.655937, acc.: 64.06%] [G loss: 1.343776]\n",
      "epoch:17 step:16855 [D loss: 0.610977, acc.: 67.19%] [G loss: 1.569507]\n",
      "epoch:17 step:16856 [D loss: 0.656277, acc.: 65.62%] [G loss: 1.067169]\n",
      "epoch:17 step:16857 [D loss: 0.591471, acc.: 66.41%] [G loss: 1.443880]\n",
      "epoch:17 step:16858 [D loss: 0.751387, acc.: 52.34%] [G loss: 0.946940]\n",
      "epoch:17 step:16859 [D loss: 0.551901, acc.: 73.44%] [G loss: 1.440764]\n",
      "epoch:17 step:16860 [D loss: 0.687377, acc.: 57.03%] [G loss: 1.144447]\n",
      "epoch:17 step:16861 [D loss: 0.507011, acc.: 78.91%] [G loss: 1.182164]\n",
      "epoch:17 step:16862 [D loss: 0.726217, acc.: 55.47%] [G loss: 1.035417]\n",
      "epoch:17 step:16863 [D loss: 0.535971, acc.: 74.22%] [G loss: 1.560554]\n",
      "epoch:17 step:16864 [D loss: 0.610347, acc.: 67.19%] [G loss: 1.104949]\n",
      "epoch:17 step:16865 [D loss: 0.580316, acc.: 69.53%] [G loss: 1.470372]\n",
      "epoch:17 step:16866 [D loss: 0.540569, acc.: 73.44%] [G loss: 1.443860]\n",
      "epoch:18 step:16867 [D loss: 0.569498, acc.: 67.97%] [G loss: 1.219597]\n",
      "epoch:18 step:16868 [D loss: 0.524557, acc.: 75.78%] [G loss: 1.353468]\n",
      "epoch:18 step:16869 [D loss: 0.698597, acc.: 57.81%] [G loss: 1.113989]\n",
      "epoch:18 step:16870 [D loss: 0.553071, acc.: 68.75%] [G loss: 1.533468]\n",
      "epoch:18 step:16871 [D loss: 0.523393, acc.: 76.56%] [G loss: 1.654762]\n",
      "epoch:18 step:16872 [D loss: 0.703246, acc.: 53.91%] [G loss: 1.296251]\n",
      "epoch:18 step:16873 [D loss: 0.626884, acc.: 64.84%] [G loss: 1.359300]\n",
      "epoch:18 step:16874 [D loss: 0.505728, acc.: 75.78%] [G loss: 1.295958]\n",
      "epoch:18 step:16875 [D loss: 0.568403, acc.: 72.66%] [G loss: 1.390229]\n",
      "epoch:18 step:16876 [D loss: 0.486102, acc.: 78.91%] [G loss: 1.103123]\n",
      "epoch:18 step:16877 [D loss: 0.369343, acc.: 90.62%] [G loss: 1.343815]\n",
      "epoch:18 step:16878 [D loss: 0.453864, acc.: 81.25%] [G loss: 1.592719]\n",
      "epoch:18 step:16879 [D loss: 0.533714, acc.: 72.66%] [G loss: 1.307043]\n",
      "epoch:18 step:16880 [D loss: 0.476980, acc.: 79.69%] [G loss: 1.544682]\n",
      "epoch:18 step:16881 [D loss: 0.545327, acc.: 71.09%] [G loss: 1.570692]\n",
      "epoch:18 step:16882 [D loss: 0.679377, acc.: 59.38%] [G loss: 1.343984]\n",
      "epoch:18 step:16883 [D loss: 0.593524, acc.: 70.31%] [G loss: 1.351545]\n",
      "epoch:18 step:16884 [D loss: 0.676639, acc.: 61.72%] [G loss: 0.940230]\n",
      "epoch:18 step:16885 [D loss: 0.774697, acc.: 48.44%] [G loss: 1.071542]\n",
      "epoch:18 step:16886 [D loss: 0.573596, acc.: 70.31%] [G loss: 1.122050]\n",
      "epoch:18 step:16887 [D loss: 0.725487, acc.: 60.16%] [G loss: 1.128342]\n",
      "epoch:18 step:16888 [D loss: 0.569711, acc.: 69.53%] [G loss: 1.415564]\n",
      "epoch:18 step:16889 [D loss: 0.574205, acc.: 71.09%] [G loss: 1.665109]\n",
      "epoch:18 step:16890 [D loss: 0.574035, acc.: 68.75%] [G loss: 1.388602]\n",
      "epoch:18 step:16891 [D loss: 0.594755, acc.: 67.19%] [G loss: 1.431685]\n",
      "epoch:18 step:16892 [D loss: 0.671169, acc.: 57.03%] [G loss: 1.471689]\n",
      "epoch:18 step:16893 [D loss: 0.611041, acc.: 69.53%] [G loss: 1.404280]\n",
      "epoch:18 step:16894 [D loss: 0.650323, acc.: 62.50%] [G loss: 1.288383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16895 [D loss: 0.545445, acc.: 71.88%] [G loss: 1.302148]\n",
      "epoch:18 step:16896 [D loss: 0.514427, acc.: 72.66%] [G loss: 1.347736]\n",
      "epoch:18 step:16897 [D loss: 0.473780, acc.: 78.12%] [G loss: 1.105286]\n",
      "epoch:18 step:16898 [D loss: 0.538647, acc.: 71.09%] [G loss: 1.016914]\n",
      "epoch:18 step:16899 [D loss: 0.487590, acc.: 76.56%] [G loss: 1.630175]\n",
      "epoch:18 step:16900 [D loss: 0.585483, acc.: 68.75%] [G loss: 1.416893]\n",
      "epoch:18 step:16901 [D loss: 0.512849, acc.: 75.00%] [G loss: 1.681028]\n",
      "epoch:18 step:16902 [D loss: 0.419143, acc.: 84.38%] [G loss: 1.563115]\n",
      "epoch:18 step:16903 [D loss: 0.496832, acc.: 75.78%] [G loss: 1.493716]\n",
      "epoch:18 step:16904 [D loss: 0.699531, acc.: 62.50%] [G loss: 1.208362]\n",
      "epoch:18 step:16905 [D loss: 0.640295, acc.: 65.62%] [G loss: 1.226659]\n",
      "epoch:18 step:16906 [D loss: 0.648138, acc.: 65.62%] [G loss: 1.296851]\n",
      "epoch:18 step:16907 [D loss: 0.530152, acc.: 71.09%] [G loss: 1.353762]\n",
      "epoch:18 step:16908 [D loss: 0.498887, acc.: 79.69%] [G loss: 1.616410]\n",
      "epoch:18 step:16909 [D loss: 0.497189, acc.: 75.78%] [G loss: 1.569829]\n",
      "epoch:18 step:16910 [D loss: 0.522748, acc.: 76.56%] [G loss: 0.929497]\n",
      "epoch:18 step:16911 [D loss: 0.525066, acc.: 74.22%] [G loss: 1.243744]\n",
      "epoch:18 step:16912 [D loss: 0.722431, acc.: 56.25%] [G loss: 1.222561]\n",
      "epoch:18 step:16913 [D loss: 0.575087, acc.: 68.75%] [G loss: 1.406401]\n",
      "epoch:18 step:16914 [D loss: 0.625985, acc.: 69.53%] [G loss: 1.071501]\n",
      "epoch:18 step:16915 [D loss: 0.473709, acc.: 75.78%] [G loss: 1.305979]\n",
      "epoch:18 step:16916 [D loss: 0.514269, acc.: 72.66%] [G loss: 1.692221]\n",
      "epoch:18 step:16917 [D loss: 0.629110, acc.: 69.53%] [G loss: 1.455772]\n",
      "epoch:18 step:16918 [D loss: 0.652593, acc.: 63.28%] [G loss: 1.409707]\n",
      "epoch:18 step:16919 [D loss: 0.480445, acc.: 78.91%] [G loss: 1.402078]\n",
      "epoch:18 step:16920 [D loss: 0.514213, acc.: 76.56%] [G loss: 1.475845]\n",
      "epoch:18 step:16921 [D loss: 0.735509, acc.: 63.28%] [G loss: 1.295650]\n",
      "epoch:18 step:16922 [D loss: 0.577631, acc.: 72.66%] [G loss: 1.254708]\n",
      "epoch:18 step:16923 [D loss: 0.534100, acc.: 76.56%] [G loss: 1.369219]\n",
      "epoch:18 step:16924 [D loss: 0.451093, acc.: 78.91%] [G loss: 1.093177]\n",
      "epoch:18 step:16925 [D loss: 0.444866, acc.: 78.12%] [G loss: 1.375323]\n",
      "epoch:18 step:16926 [D loss: 0.558624, acc.: 71.09%] [G loss: 1.267972]\n",
      "epoch:18 step:16927 [D loss: 0.515231, acc.: 68.75%] [G loss: 1.246064]\n",
      "epoch:18 step:16928 [D loss: 0.502143, acc.: 77.34%] [G loss: 1.286797]\n",
      "epoch:18 step:16929 [D loss: 0.756253, acc.: 58.59%] [G loss: 0.926592]\n",
      "epoch:18 step:16930 [D loss: 0.602315, acc.: 67.97%] [G loss: 1.110581]\n",
      "epoch:18 step:16931 [D loss: 0.629011, acc.: 70.31%] [G loss: 1.305283]\n",
      "epoch:18 step:16932 [D loss: 0.750923, acc.: 49.22%] [G loss: 1.375610]\n",
      "epoch:18 step:16933 [D loss: 0.571573, acc.: 72.66%] [G loss: 1.502592]\n",
      "epoch:18 step:16934 [D loss: 0.552041, acc.: 70.31%] [G loss: 1.179015]\n",
      "epoch:18 step:16935 [D loss: 0.538902, acc.: 70.31%] [G loss: 1.076369]\n",
      "epoch:18 step:16936 [D loss: 0.698598, acc.: 58.59%] [G loss: 0.987132]\n",
      "epoch:18 step:16937 [D loss: 0.603266, acc.: 67.97%] [G loss: 1.318833]\n",
      "epoch:18 step:16938 [D loss: 0.472323, acc.: 80.47%] [G loss: 1.258884]\n",
      "epoch:18 step:16939 [D loss: 0.445898, acc.: 82.03%] [G loss: 1.231697]\n",
      "epoch:18 step:16940 [D loss: 0.475370, acc.: 81.25%] [G loss: 1.176440]\n",
      "epoch:18 step:16941 [D loss: 0.635761, acc.: 59.38%] [G loss: 1.308330]\n",
      "epoch:18 step:16942 [D loss: 0.572547, acc.: 69.53%] [G loss: 1.285146]\n",
      "epoch:18 step:16943 [D loss: 0.591133, acc.: 71.09%] [G loss: 1.178999]\n",
      "epoch:18 step:16944 [D loss: 0.654719, acc.: 64.06%] [G loss: 1.267273]\n",
      "epoch:18 step:16945 [D loss: 0.668437, acc.: 64.06%] [G loss: 1.164956]\n",
      "epoch:18 step:16946 [D loss: 0.482382, acc.: 79.69%] [G loss: 1.572667]\n",
      "epoch:18 step:16947 [D loss: 0.704046, acc.: 57.81%] [G loss: 1.154065]\n",
      "epoch:18 step:16948 [D loss: 0.364857, acc.: 85.94%] [G loss: 1.182278]\n",
      "epoch:18 step:16949 [D loss: 0.565662, acc.: 73.44%] [G loss: 1.311839]\n",
      "epoch:18 step:16950 [D loss: 0.698584, acc.: 59.38%] [G loss: 1.112436]\n",
      "epoch:18 step:16951 [D loss: 0.520413, acc.: 69.53%] [G loss: 1.637299]\n",
      "epoch:18 step:16952 [D loss: 0.551385, acc.: 74.22%] [G loss: 1.255489]\n",
      "epoch:18 step:16953 [D loss: 0.690032, acc.: 60.16%] [G loss: 1.519116]\n",
      "epoch:18 step:16954 [D loss: 0.518275, acc.: 76.56%] [G loss: 1.186701]\n",
      "epoch:18 step:16955 [D loss: 0.618034, acc.: 63.28%] [G loss: 1.397522]\n",
      "epoch:18 step:16956 [D loss: 0.457217, acc.: 80.47%] [G loss: 1.425263]\n",
      "epoch:18 step:16957 [D loss: 0.547666, acc.: 72.66%] [G loss: 1.200690]\n",
      "epoch:18 step:16958 [D loss: 0.535655, acc.: 71.88%] [G loss: 1.199621]\n",
      "epoch:18 step:16959 [D loss: 0.583046, acc.: 71.09%] [G loss: 1.369921]\n",
      "epoch:18 step:16960 [D loss: 0.440844, acc.: 83.59%] [G loss: 1.624525]\n",
      "epoch:18 step:16961 [D loss: 0.587779, acc.: 67.97%] [G loss: 1.211514]\n",
      "epoch:18 step:16962 [D loss: 0.575358, acc.: 66.41%] [G loss: 1.137206]\n",
      "epoch:18 step:16963 [D loss: 0.543821, acc.: 70.31%] [G loss: 1.451672]\n",
      "epoch:18 step:16964 [D loss: 0.696282, acc.: 56.25%] [G loss: 1.357604]\n",
      "epoch:18 step:16965 [D loss: 0.617485, acc.: 64.84%] [G loss: 1.582532]\n",
      "epoch:18 step:16966 [D loss: 0.523841, acc.: 74.22%] [G loss: 1.618194]\n",
      "epoch:18 step:16967 [D loss: 0.541492, acc.: 73.44%] [G loss: 1.365690]\n",
      "epoch:18 step:16968 [D loss: 0.546903, acc.: 71.09%] [G loss: 1.619699]\n",
      "epoch:18 step:16969 [D loss: 0.707443, acc.: 53.91%] [G loss: 0.935106]\n",
      "epoch:18 step:16970 [D loss: 0.593099, acc.: 62.50%] [G loss: 1.017882]\n",
      "epoch:18 step:16971 [D loss: 0.520196, acc.: 75.78%] [G loss: 1.326274]\n",
      "epoch:18 step:16972 [D loss: 0.560469, acc.: 70.31%] [G loss: 1.064276]\n",
      "epoch:18 step:16973 [D loss: 0.618649, acc.: 64.84%] [G loss: 1.353028]\n",
      "epoch:18 step:16974 [D loss: 0.726148, acc.: 55.47%] [G loss: 1.366434]\n",
      "epoch:18 step:16975 [D loss: 0.617165, acc.: 65.62%] [G loss: 1.243889]\n",
      "epoch:18 step:16976 [D loss: 0.658237, acc.: 67.97%] [G loss: 1.346459]\n",
      "epoch:18 step:16977 [D loss: 0.592597, acc.: 67.97%] [G loss: 1.452429]\n",
      "epoch:18 step:16978 [D loss: 0.593744, acc.: 69.53%] [G loss: 1.055097]\n",
      "epoch:18 step:16979 [D loss: 0.570167, acc.: 71.88%] [G loss: 1.281587]\n",
      "epoch:18 step:16980 [D loss: 0.516767, acc.: 76.56%] [G loss: 1.300858]\n",
      "epoch:18 step:16981 [D loss: 0.707722, acc.: 60.16%] [G loss: 1.388815]\n",
      "epoch:18 step:16982 [D loss: 0.665655, acc.: 64.84%] [G loss: 1.203561]\n",
      "epoch:18 step:16983 [D loss: 0.513645, acc.: 74.22%] [G loss: 1.285266]\n",
      "epoch:18 step:16984 [D loss: 0.566872, acc.: 65.62%] [G loss: 1.315166]\n",
      "epoch:18 step:16985 [D loss: 0.744651, acc.: 57.03%] [G loss: 1.036780]\n",
      "epoch:18 step:16986 [D loss: 0.647622, acc.: 63.28%] [G loss: 1.286911]\n",
      "epoch:18 step:16987 [D loss: 0.562444, acc.: 74.22%] [G loss: 1.036576]\n",
      "epoch:18 step:16988 [D loss: 0.467088, acc.: 78.12%] [G loss: 1.347095]\n",
      "epoch:18 step:16989 [D loss: 0.504758, acc.: 73.44%] [G loss: 1.291105]\n",
      "epoch:18 step:16990 [D loss: 0.536023, acc.: 71.88%] [G loss: 1.257442]\n",
      "epoch:18 step:16991 [D loss: 0.545468, acc.: 72.66%] [G loss: 1.302744]\n",
      "epoch:18 step:16992 [D loss: 0.484352, acc.: 75.78%] [G loss: 1.323625]\n",
      "epoch:18 step:16993 [D loss: 0.492785, acc.: 79.69%] [G loss: 1.492916]\n",
      "epoch:18 step:16994 [D loss: 0.429370, acc.: 84.38%] [G loss: 1.332315]\n",
      "epoch:18 step:16995 [D loss: 0.534626, acc.: 73.44%] [G loss: 1.259350]\n",
      "epoch:18 step:16996 [D loss: 0.654655, acc.: 61.72%] [G loss: 1.303750]\n",
      "epoch:18 step:16997 [D loss: 0.724601, acc.: 54.69%] [G loss: 0.986388]\n",
      "epoch:18 step:16998 [D loss: 0.692488, acc.: 60.94%] [G loss: 1.008815]\n",
      "epoch:18 step:16999 [D loss: 0.478278, acc.: 78.91%] [G loss: 1.250960]\n",
      "epoch:18 step:17000 [D loss: 0.507607, acc.: 71.88%] [G loss: 1.679054]\n",
      "##############\n",
      "[2.884048   2.28093155 2.02494458 2.97806887 0.99716954 5.84993886\n",
      " 2.06944062 2.32558259 3.88546856 5.23804285]\n",
      "##########\n",
      "epoch:18 step:17001 [D loss: 0.592697, acc.: 67.19%] [G loss: 1.148780]\n",
      "epoch:18 step:17002 [D loss: 0.634457, acc.: 66.41%] [G loss: 1.382990]\n",
      "epoch:18 step:17003 [D loss: 0.601315, acc.: 65.62%] [G loss: 1.025579]\n",
      "epoch:18 step:17004 [D loss: 0.662135, acc.: 65.62%] [G loss: 1.144179]\n",
      "epoch:18 step:17005 [D loss: 0.556364, acc.: 67.19%] [G loss: 1.515418]\n",
      "epoch:18 step:17006 [D loss: 0.790671, acc.: 52.34%] [G loss: 1.063254]\n",
      "epoch:18 step:17007 [D loss: 0.683271, acc.: 64.06%] [G loss: 1.183022]\n",
      "epoch:18 step:17008 [D loss: 0.556263, acc.: 67.97%] [G loss: 1.266265]\n",
      "epoch:18 step:17009 [D loss: 0.593016, acc.: 69.53%] [G loss: 1.414616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17010 [D loss: 0.641074, acc.: 67.19%] [G loss: 1.069327]\n",
      "epoch:18 step:17011 [D loss: 0.430958, acc.: 78.12%] [G loss: 1.633296]\n",
      "epoch:18 step:17012 [D loss: 0.421564, acc.: 85.16%] [G loss: 1.295465]\n",
      "epoch:18 step:17013 [D loss: 0.571571, acc.: 71.09%] [G loss: 1.112450]\n",
      "epoch:18 step:17014 [D loss: 0.488064, acc.: 79.69%] [G loss: 1.461268]\n",
      "epoch:18 step:17015 [D loss: 0.529213, acc.: 71.88%] [G loss: 1.105567]\n",
      "epoch:18 step:17016 [D loss: 0.660754, acc.: 59.38%] [G loss: 1.051098]\n",
      "epoch:18 step:17017 [D loss: 0.630918, acc.: 67.97%] [G loss: 1.178623]\n",
      "epoch:18 step:17018 [D loss: 0.651597, acc.: 62.50%] [G loss: 0.850988]\n",
      "epoch:18 step:17019 [D loss: 0.603647, acc.: 67.19%] [G loss: 1.217076]\n",
      "epoch:18 step:17020 [D loss: 0.581671, acc.: 67.97%] [G loss: 1.023940]\n",
      "epoch:18 step:17021 [D loss: 0.563142, acc.: 75.78%] [G loss: 1.688812]\n",
      "epoch:18 step:17022 [D loss: 0.576275, acc.: 71.09%] [G loss: 1.315622]\n",
      "epoch:18 step:17023 [D loss: 0.754603, acc.: 53.91%] [G loss: 1.389067]\n",
      "epoch:18 step:17024 [D loss: 0.498722, acc.: 73.44%] [G loss: 1.220464]\n",
      "epoch:18 step:17025 [D loss: 0.515506, acc.: 77.34%] [G loss: 1.430886]\n",
      "epoch:18 step:17026 [D loss: 0.598152, acc.: 72.66%] [G loss: 1.114638]\n",
      "epoch:18 step:17027 [D loss: 0.488338, acc.: 79.69%] [G loss: 1.574892]\n",
      "epoch:18 step:17028 [D loss: 0.538881, acc.: 75.00%] [G loss: 1.024192]\n",
      "epoch:18 step:17029 [D loss: 0.529963, acc.: 71.88%] [G loss: 1.090955]\n",
      "epoch:18 step:17030 [D loss: 0.485374, acc.: 76.56%] [G loss: 1.626317]\n",
      "epoch:18 step:17031 [D loss: 0.635939, acc.: 62.50%] [G loss: 1.577519]\n",
      "epoch:18 step:17032 [D loss: 0.551950, acc.: 71.88%] [G loss: 1.241999]\n",
      "epoch:18 step:17033 [D loss: 0.464334, acc.: 77.34%] [G loss: 1.492918]\n",
      "epoch:18 step:17034 [D loss: 0.567449, acc.: 72.66%] [G loss: 1.140651]\n",
      "epoch:18 step:17035 [D loss: 0.476457, acc.: 76.56%] [G loss: 1.241826]\n",
      "epoch:18 step:17036 [D loss: 0.511890, acc.: 78.12%] [G loss: 1.212528]\n",
      "epoch:18 step:17037 [D loss: 0.518039, acc.: 77.34%] [G loss: 1.547571]\n",
      "epoch:18 step:17038 [D loss: 0.463603, acc.: 81.25%] [G loss: 1.172640]\n",
      "epoch:18 step:17039 [D loss: 0.668967, acc.: 64.06%] [G loss: 1.567220]\n",
      "epoch:18 step:17040 [D loss: 0.469468, acc.: 79.69%] [G loss: 1.448620]\n",
      "epoch:18 step:17041 [D loss: 0.690951, acc.: 60.94%] [G loss: 1.599752]\n",
      "epoch:18 step:17042 [D loss: 0.519485, acc.: 72.66%] [G loss: 1.644485]\n",
      "epoch:18 step:17043 [D loss: 0.597488, acc.: 69.53%] [G loss: 1.380069]\n",
      "epoch:18 step:17044 [D loss: 0.566148, acc.: 71.88%] [G loss: 1.343534]\n",
      "epoch:18 step:17045 [D loss: 0.611564, acc.: 68.75%] [G loss: 1.081419]\n",
      "epoch:18 step:17046 [D loss: 0.490687, acc.: 72.66%] [G loss: 1.432548]\n",
      "epoch:18 step:17047 [D loss: 0.672206, acc.: 64.06%] [G loss: 1.154296]\n",
      "epoch:18 step:17048 [D loss: 0.565575, acc.: 74.22%] [G loss: 1.226358]\n",
      "epoch:18 step:17049 [D loss: 0.662537, acc.: 59.38%] [G loss: 1.419085]\n",
      "epoch:18 step:17050 [D loss: 0.481603, acc.: 80.47%] [G loss: 1.542500]\n",
      "epoch:18 step:17051 [D loss: 0.465982, acc.: 78.12%] [G loss: 1.575130]\n",
      "epoch:18 step:17052 [D loss: 0.543804, acc.: 71.88%] [G loss: 1.136970]\n",
      "epoch:18 step:17053 [D loss: 0.572331, acc.: 67.19%] [G loss: 1.428551]\n",
      "epoch:18 step:17054 [D loss: 0.602109, acc.: 67.97%] [G loss: 1.162709]\n",
      "epoch:18 step:17055 [D loss: 0.466469, acc.: 80.47%] [G loss: 1.644484]\n",
      "epoch:18 step:17056 [D loss: 0.540344, acc.: 74.22%] [G loss: 1.079660]\n",
      "epoch:18 step:17057 [D loss: 0.583308, acc.: 69.53%] [G loss: 1.436268]\n",
      "epoch:18 step:17058 [D loss: 0.763564, acc.: 50.00%] [G loss: 1.037028]\n",
      "epoch:18 step:17059 [D loss: 0.640429, acc.: 60.16%] [G loss: 1.322404]\n",
      "epoch:18 step:17060 [D loss: 0.548740, acc.: 72.66%] [G loss: 1.235584]\n",
      "epoch:18 step:17061 [D loss: 0.656357, acc.: 66.41%] [G loss: 1.695617]\n",
      "epoch:18 step:17062 [D loss: 0.478011, acc.: 79.69%] [G loss: 1.200405]\n",
      "epoch:18 step:17063 [D loss: 0.601726, acc.: 68.75%] [G loss: 1.357107]\n",
      "epoch:18 step:17064 [D loss: 0.621291, acc.: 68.75%] [G loss: 1.611377]\n",
      "epoch:18 step:17065 [D loss: 0.501623, acc.: 75.00%] [G loss: 1.306113]\n",
      "epoch:18 step:17066 [D loss: 0.504718, acc.: 78.91%] [G loss: 1.799019]\n",
      "epoch:18 step:17067 [D loss: 0.585801, acc.: 71.09%] [G loss: 1.051528]\n",
      "epoch:18 step:17068 [D loss: 0.501049, acc.: 79.69%] [G loss: 1.294941]\n",
      "epoch:18 step:17069 [D loss: 0.528956, acc.: 76.56%] [G loss: 1.388677]\n",
      "epoch:18 step:17070 [D loss: 0.340351, acc.: 87.50%] [G loss: 1.186647]\n",
      "epoch:18 step:17071 [D loss: 0.551757, acc.: 75.78%] [G loss: 1.244261]\n",
      "epoch:18 step:17072 [D loss: 0.672106, acc.: 59.38%] [G loss: 1.136176]\n",
      "epoch:18 step:17073 [D loss: 0.610107, acc.: 66.41%] [G loss: 1.495455]\n",
      "epoch:18 step:17074 [D loss: 0.579936, acc.: 71.88%] [G loss: 1.419146]\n",
      "epoch:18 step:17075 [D loss: 0.666950, acc.: 61.72%] [G loss: 1.049713]\n",
      "epoch:18 step:17076 [D loss: 0.592009, acc.: 67.19%] [G loss: 1.202591]\n",
      "epoch:18 step:17077 [D loss: 0.564189, acc.: 71.09%] [G loss: 1.078019]\n",
      "epoch:18 step:17078 [D loss: 0.615694, acc.: 64.84%] [G loss: 1.348794]\n",
      "epoch:18 step:17079 [D loss: 0.574313, acc.: 67.19%] [G loss: 1.290617]\n",
      "epoch:18 step:17080 [D loss: 0.731730, acc.: 55.47%] [G loss: 1.050875]\n",
      "epoch:18 step:17081 [D loss: 0.737832, acc.: 55.47%] [G loss: 1.044241]\n",
      "epoch:18 step:17082 [D loss: 0.515793, acc.: 80.47%] [G loss: 1.220857]\n",
      "epoch:18 step:17083 [D loss: 0.550815, acc.: 76.56%] [G loss: 1.358519]\n",
      "epoch:18 step:17084 [D loss: 0.568568, acc.: 68.75%] [G loss: 1.264220]\n",
      "epoch:18 step:17085 [D loss: 0.476142, acc.: 80.47%] [G loss: 1.275825]\n",
      "epoch:18 step:17086 [D loss: 0.607122, acc.: 64.84%] [G loss: 1.272146]\n",
      "epoch:18 step:17087 [D loss: 0.569944, acc.: 70.31%] [G loss: 1.593054]\n",
      "epoch:18 step:17088 [D loss: 0.478912, acc.: 75.00%] [G loss: 1.207462]\n",
      "epoch:18 step:17089 [D loss: 0.649761, acc.: 60.94%] [G loss: 1.385400]\n",
      "epoch:18 step:17090 [D loss: 0.613838, acc.: 67.97%] [G loss: 1.408549]\n",
      "epoch:18 step:17091 [D loss: 0.717677, acc.: 54.69%] [G loss: 1.370733]\n",
      "epoch:18 step:17092 [D loss: 0.456479, acc.: 82.81%] [G loss: 1.421827]\n",
      "epoch:18 step:17093 [D loss: 0.662196, acc.: 61.72%] [G loss: 1.305505]\n",
      "epoch:18 step:17094 [D loss: 0.564115, acc.: 67.19%] [G loss: 1.221676]\n",
      "epoch:18 step:17095 [D loss: 0.593091, acc.: 67.97%] [G loss: 1.037510]\n",
      "epoch:18 step:17096 [D loss: 0.607562, acc.: 66.41%] [G loss: 1.164786]\n",
      "epoch:18 step:17097 [D loss: 0.403304, acc.: 85.94%] [G loss: 1.579469]\n",
      "epoch:18 step:17098 [D loss: 0.521730, acc.: 75.78%] [G loss: 1.328526]\n",
      "epoch:18 step:17099 [D loss: 0.689195, acc.: 57.03%] [G loss: 1.126403]\n",
      "epoch:18 step:17100 [D loss: 0.558455, acc.: 70.31%] [G loss: 1.190958]\n",
      "epoch:18 step:17101 [D loss: 0.476160, acc.: 81.25%] [G loss: 1.228614]\n",
      "epoch:18 step:17102 [D loss: 0.443514, acc.: 81.25%] [G loss: 1.281278]\n",
      "epoch:18 step:17103 [D loss: 0.534969, acc.: 75.78%] [G loss: 1.228443]\n",
      "epoch:18 step:17104 [D loss: 0.574091, acc.: 69.53%] [G loss: 1.418453]\n",
      "epoch:18 step:17105 [D loss: 0.626417, acc.: 63.28%] [G loss: 1.110872]\n",
      "epoch:18 step:17106 [D loss: 0.652972, acc.: 61.72%] [G loss: 1.208323]\n",
      "epoch:18 step:17107 [D loss: 0.581551, acc.: 69.53%] [G loss: 1.299798]\n",
      "epoch:18 step:17108 [D loss: 0.519396, acc.: 74.22%] [G loss: 1.167771]\n",
      "epoch:18 step:17109 [D loss: 0.594662, acc.: 67.97%] [G loss: 1.460865]\n",
      "epoch:18 step:17110 [D loss: 0.522347, acc.: 74.22%] [G loss: 1.474294]\n",
      "epoch:18 step:17111 [D loss: 0.576982, acc.: 70.31%] [G loss: 1.182088]\n",
      "epoch:18 step:17112 [D loss: 0.517547, acc.: 74.22%] [G loss: 1.451125]\n",
      "epoch:18 step:17113 [D loss: 0.724446, acc.: 60.94%] [G loss: 1.416690]\n",
      "epoch:18 step:17114 [D loss: 0.597577, acc.: 71.09%] [G loss: 1.286050]\n",
      "epoch:18 step:17115 [D loss: 0.519550, acc.: 77.34%] [G loss: 1.312082]\n",
      "epoch:18 step:17116 [D loss: 0.541929, acc.: 70.31%] [G loss: 1.409277]\n",
      "epoch:18 step:17117 [D loss: 0.751112, acc.: 52.34%] [G loss: 1.240541]\n",
      "epoch:18 step:17118 [D loss: 0.424577, acc.: 81.25%] [G loss: 1.429046]\n",
      "epoch:18 step:17119 [D loss: 0.504630, acc.: 73.44%] [G loss: 1.178433]\n",
      "epoch:18 step:17120 [D loss: 0.563862, acc.: 66.41%] [G loss: 1.374645]\n",
      "epoch:18 step:17121 [D loss: 0.624506, acc.: 70.31%] [G loss: 1.660692]\n",
      "epoch:18 step:17122 [D loss: 0.723883, acc.: 54.69%] [G loss: 1.079309]\n",
      "epoch:18 step:17123 [D loss: 0.554293, acc.: 73.44%] [G loss: 1.167947]\n",
      "epoch:18 step:17124 [D loss: 0.468943, acc.: 79.69%] [G loss: 1.481091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17125 [D loss: 0.484240, acc.: 78.91%] [G loss: 1.312938]\n",
      "epoch:18 step:17126 [D loss: 0.532171, acc.: 75.78%] [G loss: 1.254687]\n",
      "epoch:18 step:17127 [D loss: 0.609771, acc.: 63.28%] [G loss: 1.445850]\n",
      "epoch:18 step:17128 [D loss: 0.680497, acc.: 60.16%] [G loss: 1.130566]\n",
      "epoch:18 step:17129 [D loss: 0.590189, acc.: 70.31%] [G loss: 1.190079]\n",
      "epoch:18 step:17130 [D loss: 0.483686, acc.: 78.12%] [G loss: 1.392360]\n",
      "epoch:18 step:17131 [D loss: 0.407424, acc.: 85.16%] [G loss: 1.609295]\n",
      "epoch:18 step:17132 [D loss: 0.634638, acc.: 67.19%] [G loss: 1.631154]\n",
      "epoch:18 step:17133 [D loss: 0.536470, acc.: 75.00%] [G loss: 1.405302]\n",
      "epoch:18 step:17134 [D loss: 0.583035, acc.: 67.19%] [G loss: 1.335636]\n",
      "epoch:18 step:17135 [D loss: 0.595202, acc.: 67.97%] [G loss: 1.220265]\n",
      "epoch:18 step:17136 [D loss: 0.477388, acc.: 78.12%] [G loss: 1.449835]\n",
      "epoch:18 step:17137 [D loss: 0.332238, acc.: 89.06%] [G loss: 1.436080]\n",
      "epoch:18 step:17138 [D loss: 0.653189, acc.: 60.94%] [G loss: 1.302610]\n",
      "epoch:18 step:17139 [D loss: 0.528486, acc.: 71.88%] [G loss: 1.522091]\n",
      "epoch:18 step:17140 [D loss: 0.458124, acc.: 81.25%] [G loss: 1.280641]\n",
      "epoch:18 step:17141 [D loss: 0.637492, acc.: 60.94%] [G loss: 1.060989]\n",
      "epoch:18 step:17142 [D loss: 0.620727, acc.: 62.50%] [G loss: 1.166808]\n",
      "epoch:18 step:17143 [D loss: 0.484369, acc.: 76.56%] [G loss: 1.219265]\n",
      "epoch:18 step:17144 [D loss: 0.457779, acc.: 78.91%] [G loss: 1.321227]\n",
      "epoch:18 step:17145 [D loss: 0.574588, acc.: 70.31%] [G loss: 1.258614]\n",
      "epoch:18 step:17146 [D loss: 0.725508, acc.: 56.25%] [G loss: 1.200368]\n",
      "epoch:18 step:17147 [D loss: 0.549756, acc.: 72.66%] [G loss: 1.401406]\n",
      "epoch:18 step:17148 [D loss: 0.580924, acc.: 65.62%] [G loss: 1.395179]\n",
      "epoch:18 step:17149 [D loss: 0.487626, acc.: 80.47%] [G loss: 1.479707]\n",
      "epoch:18 step:17150 [D loss: 0.613458, acc.: 64.06%] [G loss: 1.134172]\n",
      "epoch:18 step:17151 [D loss: 0.459742, acc.: 81.25%] [G loss: 1.512286]\n",
      "epoch:18 step:17152 [D loss: 0.534697, acc.: 70.31%] [G loss: 1.492394]\n",
      "epoch:18 step:17153 [D loss: 0.536301, acc.: 76.56%] [G loss: 1.435141]\n",
      "epoch:18 step:17154 [D loss: 0.572615, acc.: 71.88%] [G loss: 1.156320]\n",
      "epoch:18 step:17155 [D loss: 0.694517, acc.: 57.81%] [G loss: 1.202057]\n",
      "epoch:18 step:17156 [D loss: 0.468128, acc.: 78.91%] [G loss: 1.267596]\n",
      "epoch:18 step:17157 [D loss: 0.485953, acc.: 78.12%] [G loss: 1.284327]\n",
      "epoch:18 step:17158 [D loss: 0.474723, acc.: 77.34%] [G loss: 1.435460]\n",
      "epoch:18 step:17159 [D loss: 0.620978, acc.: 66.41%] [G loss: 1.209550]\n",
      "epoch:18 step:17160 [D loss: 0.499653, acc.: 77.34%] [G loss: 1.468200]\n",
      "epoch:18 step:17161 [D loss: 0.620648, acc.: 63.28%] [G loss: 1.041177]\n",
      "epoch:18 step:17162 [D loss: 0.652448, acc.: 63.28%] [G loss: 1.417450]\n",
      "epoch:18 step:17163 [D loss: 0.671726, acc.: 62.50%] [G loss: 1.284184]\n",
      "epoch:18 step:17164 [D loss: 0.670036, acc.: 60.94%] [G loss: 1.265932]\n",
      "epoch:18 step:17165 [D loss: 0.654492, acc.: 62.50%] [G loss: 1.283041]\n",
      "epoch:18 step:17166 [D loss: 0.509867, acc.: 72.66%] [G loss: 1.176747]\n",
      "epoch:18 step:17167 [D loss: 0.558036, acc.: 72.66%] [G loss: 1.426680]\n",
      "epoch:18 step:17168 [D loss: 0.511641, acc.: 74.22%] [G loss: 1.433278]\n",
      "epoch:18 step:17169 [D loss: 0.619052, acc.: 65.62%] [G loss: 1.265773]\n",
      "epoch:18 step:17170 [D loss: 0.538939, acc.: 71.09%] [G loss: 1.522649]\n",
      "epoch:18 step:17171 [D loss: 0.412441, acc.: 82.81%] [G loss: 1.458489]\n",
      "epoch:18 step:17172 [D loss: 0.567971, acc.: 71.09%] [G loss: 1.044246]\n",
      "epoch:18 step:17173 [D loss: 0.559669, acc.: 68.75%] [G loss: 1.047305]\n",
      "epoch:18 step:17174 [D loss: 0.555337, acc.: 72.66%] [G loss: 1.255919]\n",
      "epoch:18 step:17175 [D loss: 0.355040, acc.: 90.62%] [G loss: 1.252738]\n",
      "epoch:18 step:17176 [D loss: 0.501934, acc.: 75.78%] [G loss: 1.766301]\n",
      "epoch:18 step:17177 [D loss: 0.492461, acc.: 74.22%] [G loss: 1.565313]\n",
      "epoch:18 step:17178 [D loss: 0.583065, acc.: 66.41%] [G loss: 1.293851]\n",
      "epoch:18 step:17179 [D loss: 0.494245, acc.: 76.56%] [G loss: 1.162500]\n",
      "epoch:18 step:17180 [D loss: 0.460342, acc.: 77.34%] [G loss: 1.201883]\n",
      "epoch:18 step:17181 [D loss: 0.761169, acc.: 54.69%] [G loss: 1.198524]\n",
      "epoch:18 step:17182 [D loss: 0.556280, acc.: 71.09%] [G loss: 1.292786]\n",
      "epoch:18 step:17183 [D loss: 0.521568, acc.: 69.53%] [G loss: 1.127732]\n",
      "epoch:18 step:17184 [D loss: 0.631474, acc.: 62.50%] [G loss: 1.495220]\n",
      "epoch:18 step:17185 [D loss: 0.633528, acc.: 65.62%] [G loss: 1.148558]\n",
      "epoch:18 step:17186 [D loss: 0.600175, acc.: 69.53%] [G loss: 1.373827]\n",
      "epoch:18 step:17187 [D loss: 0.588924, acc.: 71.88%] [G loss: 1.071993]\n",
      "epoch:18 step:17188 [D loss: 0.605730, acc.: 66.41%] [G loss: 1.332061]\n",
      "epoch:18 step:17189 [D loss: 0.718119, acc.: 60.94%] [G loss: 1.088936]\n",
      "epoch:18 step:17190 [D loss: 0.487034, acc.: 75.00%] [G loss: 1.276256]\n",
      "epoch:18 step:17191 [D loss: 0.595997, acc.: 64.06%] [G loss: 1.188596]\n",
      "epoch:18 step:17192 [D loss: 0.590999, acc.: 71.88%] [G loss: 1.578300]\n",
      "epoch:18 step:17193 [D loss: 0.514896, acc.: 78.91%] [G loss: 1.414224]\n",
      "epoch:18 step:17194 [D loss: 0.603243, acc.: 72.66%] [G loss: 1.149332]\n",
      "epoch:18 step:17195 [D loss: 0.747894, acc.: 53.12%] [G loss: 1.401828]\n",
      "epoch:18 step:17196 [D loss: 0.528698, acc.: 72.66%] [G loss: 1.279517]\n",
      "epoch:18 step:17197 [D loss: 0.674713, acc.: 61.72%] [G loss: 1.018297]\n",
      "epoch:18 step:17198 [D loss: 0.522758, acc.: 78.12%] [G loss: 1.246146]\n",
      "epoch:18 step:17199 [D loss: 0.660282, acc.: 57.81%] [G loss: 1.115924]\n",
      "epoch:18 step:17200 [D loss: 0.546550, acc.: 75.00%] [G loss: 1.226775]\n",
      "##############\n",
      "[2.67209794 1.92071515 1.76244319 2.89025404 0.72425078 7.15111562\n",
      " 2.21886215 2.55537547 3.88765023 7.14868929]\n",
      "##########\n",
      "epoch:18 step:17201 [D loss: 0.532268, acc.: 73.44%] [G loss: 1.394906]\n",
      "epoch:18 step:17202 [D loss: 0.591724, acc.: 70.31%] [G loss: 1.426946]\n",
      "epoch:18 step:17203 [D loss: 0.759196, acc.: 54.69%] [G loss: 0.996826]\n",
      "epoch:18 step:17204 [D loss: 0.514219, acc.: 73.44%] [G loss: 1.186509]\n",
      "epoch:18 step:17205 [D loss: 0.518963, acc.: 78.12%] [G loss: 1.438082]\n",
      "epoch:18 step:17206 [D loss: 0.432031, acc.: 81.25%] [G loss: 1.247358]\n",
      "epoch:18 step:17207 [D loss: 0.621557, acc.: 65.62%] [G loss: 1.079184]\n",
      "epoch:18 step:17208 [D loss: 0.479556, acc.: 80.47%] [G loss: 1.153158]\n",
      "epoch:18 step:17209 [D loss: 0.619501, acc.: 67.97%] [G loss: 1.297750]\n",
      "epoch:18 step:17210 [D loss: 0.548691, acc.: 69.53%] [G loss: 1.159713]\n",
      "epoch:18 step:17211 [D loss: 0.508541, acc.: 76.56%] [G loss: 1.337403]\n",
      "epoch:18 step:17212 [D loss: 0.612618, acc.: 67.19%] [G loss: 1.102033]\n",
      "epoch:18 step:17213 [D loss: 0.646096, acc.: 63.28%] [G loss: 1.293984]\n",
      "epoch:18 step:17214 [D loss: 0.543628, acc.: 75.00%] [G loss: 1.308167]\n",
      "epoch:18 step:17215 [D loss: 0.441761, acc.: 80.47%] [G loss: 1.531007]\n",
      "epoch:18 step:17216 [D loss: 0.441598, acc.: 85.16%] [G loss: 1.567839]\n",
      "epoch:18 step:17217 [D loss: 0.708033, acc.: 54.69%] [G loss: 1.102698]\n",
      "epoch:18 step:17218 [D loss: 0.623372, acc.: 70.31%] [G loss: 1.035978]\n",
      "epoch:18 step:17219 [D loss: 0.672205, acc.: 64.84%] [G loss: 1.147448]\n",
      "epoch:18 step:17220 [D loss: 0.547322, acc.: 72.66%] [G loss: 1.354963]\n",
      "epoch:18 step:17221 [D loss: 0.601559, acc.: 67.19%] [G loss: 1.085906]\n",
      "epoch:18 step:17222 [D loss: 0.556476, acc.: 75.00%] [G loss: 1.413122]\n",
      "epoch:18 step:17223 [D loss: 0.724665, acc.: 57.03%] [G loss: 1.233930]\n",
      "epoch:18 step:17224 [D loss: 0.582582, acc.: 68.75%] [G loss: 1.009162]\n",
      "epoch:18 step:17225 [D loss: 0.602711, acc.: 66.41%] [G loss: 1.373172]\n",
      "epoch:18 step:17226 [D loss: 0.587697, acc.: 67.97%] [G loss: 1.514723]\n",
      "epoch:18 step:17227 [D loss: 0.518542, acc.: 74.22%] [G loss: 1.245346]\n",
      "epoch:18 step:17228 [D loss: 0.570511, acc.: 74.22%] [G loss: 1.205305]\n",
      "epoch:18 step:17229 [D loss: 0.546964, acc.: 75.00%] [G loss: 1.316386]\n",
      "epoch:18 step:17230 [D loss: 0.552873, acc.: 67.19%] [G loss: 1.321737]\n",
      "epoch:18 step:17231 [D loss: 0.464888, acc.: 82.03%] [G loss: 1.316360]\n",
      "epoch:18 step:17232 [D loss: 0.516073, acc.: 72.66%] [G loss: 1.162502]\n",
      "epoch:18 step:17233 [D loss: 0.681698, acc.: 59.38%] [G loss: 1.091270]\n",
      "epoch:18 step:17234 [D loss: 0.538487, acc.: 75.78%] [G loss: 1.345287]\n",
      "epoch:18 step:17235 [D loss: 0.669017, acc.: 64.84%] [G loss: 1.209697]\n",
      "epoch:18 step:17236 [D loss: 0.566302, acc.: 70.31%] [G loss: 1.374843]\n",
      "epoch:18 step:17237 [D loss: 0.417626, acc.: 85.16%] [G loss: 1.518811]\n",
      "epoch:18 step:17238 [D loss: 0.538067, acc.: 71.88%] [G loss: 1.150114]\n",
      "epoch:18 step:17239 [D loss: 0.622310, acc.: 63.28%] [G loss: 1.039177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17240 [D loss: 0.474332, acc.: 81.25%] [G loss: 1.274742]\n",
      "epoch:18 step:17241 [D loss: 0.632227, acc.: 68.75%] [G loss: 1.286642]\n",
      "epoch:18 step:17242 [D loss: 0.853989, acc.: 50.78%] [G loss: 0.981842]\n",
      "epoch:18 step:17243 [D loss: 0.547384, acc.: 73.44%] [G loss: 1.168369]\n",
      "epoch:18 step:17244 [D loss: 0.632963, acc.: 61.72%] [G loss: 1.262989]\n",
      "epoch:18 step:17245 [D loss: 0.436843, acc.: 82.81%] [G loss: 1.261904]\n",
      "epoch:18 step:17246 [D loss: 0.525698, acc.: 73.44%] [G loss: 1.460411]\n",
      "epoch:18 step:17247 [D loss: 0.583027, acc.: 69.53%] [G loss: 1.238642]\n",
      "epoch:18 step:17248 [D loss: 0.479700, acc.: 75.00%] [G loss: 1.570581]\n",
      "epoch:18 step:17249 [D loss: 0.652515, acc.: 61.72%] [G loss: 1.272874]\n",
      "epoch:18 step:17250 [D loss: 0.687193, acc.: 64.06%] [G loss: 1.503094]\n",
      "epoch:18 step:17251 [D loss: 0.549004, acc.: 74.22%] [G loss: 1.532125]\n",
      "epoch:18 step:17252 [D loss: 0.597954, acc.: 67.97%] [G loss: 1.336572]\n",
      "epoch:18 step:17253 [D loss: 0.524337, acc.: 78.91%] [G loss: 1.333918]\n",
      "epoch:18 step:17254 [D loss: 0.625939, acc.: 67.19%] [G loss: 1.265129]\n",
      "epoch:18 step:17255 [D loss: 0.633066, acc.: 60.16%] [G loss: 1.204182]\n",
      "epoch:18 step:17256 [D loss: 0.463589, acc.: 81.25%] [G loss: 1.438497]\n",
      "epoch:18 step:17257 [D loss: 0.614020, acc.: 65.62%] [G loss: 1.316327]\n",
      "epoch:18 step:17258 [D loss: 0.569883, acc.: 70.31%] [G loss: 1.269699]\n",
      "epoch:18 step:17259 [D loss: 0.786030, acc.: 53.91%] [G loss: 1.237517]\n",
      "epoch:18 step:17260 [D loss: 0.486785, acc.: 75.78%] [G loss: 0.987628]\n",
      "epoch:18 step:17261 [D loss: 0.528203, acc.: 77.34%] [G loss: 1.168291]\n",
      "epoch:18 step:17262 [D loss: 0.561263, acc.: 70.31%] [G loss: 1.407891]\n",
      "epoch:18 step:17263 [D loss: 0.657502, acc.: 61.72%] [G loss: 1.374110]\n",
      "epoch:18 step:17264 [D loss: 0.541903, acc.: 73.44%] [G loss: 1.302159]\n",
      "epoch:18 step:17265 [D loss: 0.667027, acc.: 61.72%] [G loss: 1.024065]\n",
      "epoch:18 step:17266 [D loss: 0.676148, acc.: 63.28%] [G loss: 1.316573]\n",
      "epoch:18 step:17267 [D loss: 0.641049, acc.: 62.50%] [G loss: 1.093264]\n",
      "epoch:18 step:17268 [D loss: 0.502441, acc.: 76.56%] [G loss: 1.596437]\n",
      "epoch:18 step:17269 [D loss: 0.552332, acc.: 71.88%] [G loss: 1.075362]\n",
      "epoch:18 step:17270 [D loss: 0.629272, acc.: 67.19%] [G loss: 1.394970]\n",
      "epoch:18 step:17271 [D loss: 0.533574, acc.: 77.34%] [G loss: 1.622332]\n",
      "epoch:18 step:17272 [D loss: 0.541260, acc.: 73.44%] [G loss: 1.600914]\n",
      "epoch:18 step:17273 [D loss: 0.789405, acc.: 50.00%] [G loss: 1.409713]\n",
      "epoch:18 step:17274 [D loss: 0.673763, acc.: 60.16%] [G loss: 1.201842]\n",
      "epoch:18 step:17275 [D loss: 0.690543, acc.: 57.81%] [G loss: 1.330182]\n",
      "epoch:18 step:17276 [D loss: 0.554316, acc.: 71.88%] [G loss: 1.271982]\n",
      "epoch:18 step:17277 [D loss: 0.557311, acc.: 70.31%] [G loss: 1.266397]\n",
      "epoch:18 step:17278 [D loss: 0.679710, acc.: 60.94%] [G loss: 1.269294]\n",
      "epoch:18 step:17279 [D loss: 0.595930, acc.: 67.19%] [G loss: 1.107063]\n",
      "epoch:18 step:17280 [D loss: 0.587153, acc.: 64.06%] [G loss: 1.156749]\n",
      "epoch:18 step:17281 [D loss: 0.504601, acc.: 75.78%] [G loss: 1.309434]\n",
      "epoch:18 step:17282 [D loss: 0.621628, acc.: 62.50%] [G loss: 1.275328]\n",
      "epoch:18 step:17283 [D loss: 0.751698, acc.: 58.59%] [G loss: 0.834704]\n",
      "epoch:18 step:17284 [D loss: 0.632290, acc.: 62.50%] [G loss: 1.303411]\n",
      "epoch:18 step:17285 [D loss: 0.463787, acc.: 80.47%] [G loss: 1.159178]\n",
      "epoch:18 step:17286 [D loss: 0.498688, acc.: 74.22%] [G loss: 1.367676]\n",
      "epoch:18 step:17287 [D loss: 0.540378, acc.: 74.22%] [G loss: 0.999253]\n",
      "epoch:18 step:17288 [D loss: 0.459315, acc.: 82.81%] [G loss: 1.302795]\n",
      "epoch:18 step:17289 [D loss: 0.687726, acc.: 60.16%] [G loss: 1.141347]\n",
      "epoch:18 step:17290 [D loss: 0.472701, acc.: 82.03%] [G loss: 1.287926]\n",
      "epoch:18 step:17291 [D loss: 0.581911, acc.: 64.84%] [G loss: 1.179714]\n",
      "epoch:18 step:17292 [D loss: 0.608060, acc.: 67.97%] [G loss: 1.231272]\n",
      "epoch:18 step:17293 [D loss: 0.675988, acc.: 60.94%] [G loss: 1.270704]\n",
      "epoch:18 step:17294 [D loss: 0.712158, acc.: 57.03%] [G loss: 1.229228]\n",
      "epoch:18 step:17295 [D loss: 0.594649, acc.: 69.53%] [G loss: 1.400236]\n",
      "epoch:18 step:17296 [D loss: 0.725445, acc.: 49.22%] [G loss: 1.153210]\n",
      "epoch:18 step:17297 [D loss: 0.608796, acc.: 67.97%] [G loss: 1.387092]\n",
      "epoch:18 step:17298 [D loss: 0.466332, acc.: 78.91%] [G loss: 1.229096]\n",
      "epoch:18 step:17299 [D loss: 0.541794, acc.: 75.00%] [G loss: 1.493392]\n",
      "epoch:18 step:17300 [D loss: 0.538456, acc.: 73.44%] [G loss: 1.560922]\n",
      "epoch:18 step:17301 [D loss: 0.559300, acc.: 71.88%] [G loss: 1.345860]\n",
      "epoch:18 step:17302 [D loss: 0.546902, acc.: 71.88%] [G loss: 1.510937]\n",
      "epoch:18 step:17303 [D loss: 0.712394, acc.: 60.16%] [G loss: 1.428149]\n",
      "epoch:18 step:17304 [D loss: 0.637887, acc.: 64.84%] [G loss: 1.352039]\n",
      "epoch:18 step:17305 [D loss: 0.630971, acc.: 67.97%] [G loss: 1.465931]\n",
      "epoch:18 step:17306 [D loss: 0.662863, acc.: 64.84%] [G loss: 1.434815]\n",
      "epoch:18 step:17307 [D loss: 0.599291, acc.: 64.06%] [G loss: 1.431511]\n",
      "epoch:18 step:17308 [D loss: 0.552141, acc.: 69.53%] [G loss: 1.238793]\n",
      "epoch:18 step:17309 [D loss: 0.486216, acc.: 78.12%] [G loss: 1.372001]\n",
      "epoch:18 step:17310 [D loss: 0.575459, acc.: 70.31%] [G loss: 1.266114]\n",
      "epoch:18 step:17311 [D loss: 0.567156, acc.: 67.97%] [G loss: 1.283440]\n",
      "epoch:18 step:17312 [D loss: 0.633613, acc.: 66.41%] [G loss: 1.401543]\n",
      "epoch:18 step:17313 [D loss: 0.525379, acc.: 78.91%] [G loss: 1.432377]\n",
      "epoch:18 step:17314 [D loss: 0.562469, acc.: 67.97%] [G loss: 1.417492]\n",
      "epoch:18 step:17315 [D loss: 0.613494, acc.: 67.19%] [G loss: 1.217572]\n",
      "epoch:18 step:17316 [D loss: 0.591486, acc.: 67.97%] [G loss: 0.965186]\n",
      "epoch:18 step:17317 [D loss: 0.466363, acc.: 77.34%] [G loss: 1.274587]\n",
      "epoch:18 step:17318 [D loss: 0.631794, acc.: 57.81%] [G loss: 1.233072]\n",
      "epoch:18 step:17319 [D loss: 0.531286, acc.: 76.56%] [G loss: 1.247814]\n",
      "epoch:18 step:17320 [D loss: 0.512576, acc.: 75.78%] [G loss: 1.404174]\n",
      "epoch:18 step:17321 [D loss: 0.485500, acc.: 79.69%] [G loss: 1.769140]\n",
      "epoch:18 step:17322 [D loss: 0.626468, acc.: 64.84%] [G loss: 1.213548]\n",
      "epoch:18 step:17323 [D loss: 0.692293, acc.: 59.38%] [G loss: 1.018100]\n",
      "epoch:18 step:17324 [D loss: 0.584754, acc.: 69.53%] [G loss: 1.253846]\n",
      "epoch:18 step:17325 [D loss: 0.591795, acc.: 65.62%] [G loss: 1.301452]\n",
      "epoch:18 step:17326 [D loss: 0.561321, acc.: 68.75%] [G loss: 1.432657]\n",
      "epoch:18 step:17327 [D loss: 0.536605, acc.: 76.56%] [G loss: 1.065470]\n",
      "epoch:18 step:17328 [D loss: 0.544062, acc.: 70.31%] [G loss: 1.219328]\n",
      "epoch:18 step:17329 [D loss: 0.668861, acc.: 57.81%] [G loss: 1.116174]\n",
      "epoch:18 step:17330 [D loss: 0.367699, acc.: 88.28%] [G loss: 1.374251]\n",
      "epoch:18 step:17331 [D loss: 0.564356, acc.: 67.97%] [G loss: 1.207524]\n",
      "epoch:18 step:17332 [D loss: 0.578931, acc.: 69.53%] [G loss: 1.362863]\n",
      "epoch:18 step:17333 [D loss: 0.532987, acc.: 75.00%] [G loss: 1.558242]\n",
      "epoch:18 step:17334 [D loss: 0.577418, acc.: 69.53%] [G loss: 1.270252]\n",
      "epoch:18 step:17335 [D loss: 0.459956, acc.: 82.03%] [G loss: 1.465594]\n",
      "epoch:18 step:17336 [D loss: 0.750622, acc.: 56.25%] [G loss: 1.287376]\n",
      "epoch:18 step:17337 [D loss: 0.578672, acc.: 71.09%] [G loss: 1.460239]\n",
      "epoch:18 step:17338 [D loss: 0.541768, acc.: 72.66%] [G loss: 1.325717]\n",
      "epoch:18 step:17339 [D loss: 0.467940, acc.: 78.91%] [G loss: 1.198651]\n",
      "epoch:18 step:17340 [D loss: 0.461500, acc.: 80.47%] [G loss: 1.557828]\n",
      "epoch:18 step:17341 [D loss: 0.478026, acc.: 76.56%] [G loss: 1.413263]\n",
      "epoch:18 step:17342 [D loss: 0.575461, acc.: 66.41%] [G loss: 1.464209]\n",
      "epoch:18 step:17343 [D loss: 0.588961, acc.: 63.28%] [G loss: 0.944599]\n",
      "epoch:18 step:17344 [D loss: 0.678450, acc.: 67.97%] [G loss: 1.069659]\n",
      "epoch:18 step:17345 [D loss: 0.555499, acc.: 64.84%] [G loss: 1.148423]\n",
      "epoch:18 step:17346 [D loss: 0.494660, acc.: 78.12%] [G loss: 1.250507]\n",
      "epoch:18 step:17347 [D loss: 0.580757, acc.: 70.31%] [G loss: 1.403840]\n",
      "epoch:18 step:17348 [D loss: 0.585717, acc.: 68.75%] [G loss: 1.296405]\n",
      "epoch:18 step:17349 [D loss: 0.613816, acc.: 60.94%] [G loss: 0.829526]\n",
      "epoch:18 step:17350 [D loss: 0.420800, acc.: 85.94%] [G loss: 1.420770]\n",
      "epoch:18 step:17351 [D loss: 0.601279, acc.: 70.31%] [G loss: 1.311267]\n",
      "epoch:18 step:17352 [D loss: 0.444824, acc.: 82.03%] [G loss: 1.579420]\n",
      "epoch:18 step:17353 [D loss: 0.623600, acc.: 67.97%] [G loss: 1.166090]\n",
      "epoch:18 step:17354 [D loss: 0.508883, acc.: 75.00%] [G loss: 1.474185]\n",
      "epoch:18 step:17355 [D loss: 0.559071, acc.: 69.53%] [G loss: 1.471577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17356 [D loss: 0.633950, acc.: 63.28%] [G loss: 1.079796]\n",
      "epoch:18 step:17357 [D loss: 0.489039, acc.: 80.47%] [G loss: 1.302818]\n",
      "epoch:18 step:17358 [D loss: 0.586777, acc.: 69.53%] [G loss: 1.452776]\n",
      "epoch:18 step:17359 [D loss: 0.644280, acc.: 61.72%] [G loss: 1.316569]\n",
      "epoch:18 step:17360 [D loss: 0.590912, acc.: 69.53%] [G loss: 1.113622]\n",
      "epoch:18 step:17361 [D loss: 0.654766, acc.: 64.06%] [G loss: 1.687706]\n",
      "epoch:18 step:17362 [D loss: 0.465223, acc.: 81.25%] [G loss: 1.526904]\n",
      "epoch:18 step:17363 [D loss: 0.577941, acc.: 66.41%] [G loss: 1.379750]\n",
      "epoch:18 step:17364 [D loss: 0.646665, acc.: 63.28%] [G loss: 1.416865]\n",
      "epoch:18 step:17365 [D loss: 0.550544, acc.: 79.69%] [G loss: 1.204636]\n",
      "epoch:18 step:17366 [D loss: 0.737172, acc.: 50.00%] [G loss: 1.061528]\n",
      "epoch:18 step:17367 [D loss: 0.562559, acc.: 71.09%] [G loss: 0.900374]\n",
      "epoch:18 step:17368 [D loss: 0.505763, acc.: 75.78%] [G loss: 1.342155]\n",
      "epoch:18 step:17369 [D loss: 0.588918, acc.: 73.44%] [G loss: 1.230703]\n",
      "epoch:18 step:17370 [D loss: 0.731800, acc.: 53.91%] [G loss: 1.256741]\n",
      "epoch:18 step:17371 [D loss: 0.507817, acc.: 75.78%] [G loss: 1.232116]\n",
      "epoch:18 step:17372 [D loss: 0.451952, acc.: 78.91%] [G loss: 1.378184]\n",
      "epoch:18 step:17373 [D loss: 0.568445, acc.: 69.53%] [G loss: 1.221761]\n",
      "epoch:18 step:17374 [D loss: 0.672208, acc.: 58.59%] [G loss: 1.304213]\n",
      "epoch:18 step:17375 [D loss: 0.538978, acc.: 67.19%] [G loss: 1.387226]\n",
      "epoch:18 step:17376 [D loss: 0.576330, acc.: 71.09%] [G loss: 0.964827]\n",
      "epoch:18 step:17377 [D loss: 0.776303, acc.: 51.56%] [G loss: 1.166461]\n",
      "epoch:18 step:17378 [D loss: 0.599003, acc.: 67.97%] [G loss: 1.605264]\n",
      "epoch:18 step:17379 [D loss: 0.665926, acc.: 57.03%] [G loss: 1.295153]\n",
      "epoch:18 step:17380 [D loss: 0.620199, acc.: 64.06%] [G loss: 1.290427]\n",
      "epoch:18 step:17381 [D loss: 0.564105, acc.: 73.44%] [G loss: 1.041917]\n",
      "epoch:18 step:17382 [D loss: 0.517814, acc.: 75.00%] [G loss: 1.306461]\n",
      "epoch:18 step:17383 [D loss: 0.394533, acc.: 83.59%] [G loss: 1.235500]\n",
      "epoch:18 step:17384 [D loss: 0.494990, acc.: 75.78%] [G loss: 1.226347]\n",
      "epoch:18 step:17385 [D loss: 0.604160, acc.: 64.84%] [G loss: 0.978456]\n",
      "epoch:18 step:17386 [D loss: 0.631187, acc.: 67.19%] [G loss: 1.240577]\n",
      "epoch:18 step:17387 [D loss: 0.520796, acc.: 75.78%] [G loss: 1.266285]\n",
      "epoch:18 step:17388 [D loss: 0.619099, acc.: 64.06%] [G loss: 1.126782]\n",
      "epoch:18 step:17389 [D loss: 0.751631, acc.: 55.47%] [G loss: 1.246660]\n",
      "epoch:18 step:17390 [D loss: 0.406017, acc.: 89.06%] [G loss: 1.459041]\n",
      "epoch:18 step:17391 [D loss: 0.745033, acc.: 53.91%] [G loss: 1.098570]\n",
      "epoch:18 step:17392 [D loss: 0.511779, acc.: 75.00%] [G loss: 1.375807]\n",
      "epoch:18 step:17393 [D loss: 0.599416, acc.: 72.66%] [G loss: 1.371354]\n",
      "epoch:18 step:17394 [D loss: 0.500235, acc.: 78.12%] [G loss: 1.769406]\n",
      "epoch:18 step:17395 [D loss: 0.484969, acc.: 78.91%] [G loss: 1.095478]\n",
      "epoch:18 step:17396 [D loss: 0.524594, acc.: 72.66%] [G loss: 0.876759]\n",
      "epoch:18 step:17397 [D loss: 0.546943, acc.: 71.09%] [G loss: 1.256447]\n",
      "epoch:18 step:17398 [D loss: 0.531960, acc.: 75.78%] [G loss: 1.605318]\n",
      "epoch:18 step:17399 [D loss: 0.635525, acc.: 61.72%] [G loss: 1.444054]\n",
      "epoch:18 step:17400 [D loss: 0.635548, acc.: 66.41%] [G loss: 1.305757]\n",
      "##############\n",
      "[2.65047262 2.0796774  1.7466612  2.49626538 1.06345484 6.22404896\n",
      " 2.18400086 2.64577831 3.84530293 5.14144852]\n",
      "##########\n",
      "epoch:18 step:17401 [D loss: 0.423706, acc.: 82.03%] [G loss: 1.412119]\n",
      "epoch:18 step:17402 [D loss: 0.584487, acc.: 71.09%] [G loss: 1.142189]\n",
      "epoch:18 step:17403 [D loss: 0.637400, acc.: 60.94%] [G loss: 1.315646]\n",
      "epoch:18 step:17404 [D loss: 0.575086, acc.: 67.97%] [G loss: 1.159935]\n",
      "epoch:18 step:17405 [D loss: 0.644413, acc.: 59.38%] [G loss: 1.125808]\n",
      "epoch:18 step:17406 [D loss: 0.599704, acc.: 72.66%] [G loss: 1.424470]\n",
      "epoch:18 step:17407 [D loss: 0.624117, acc.: 64.06%] [G loss: 1.695204]\n",
      "epoch:18 step:17408 [D loss: 0.689793, acc.: 57.03%] [G loss: 1.445332]\n",
      "epoch:18 step:17409 [D loss: 0.658718, acc.: 60.16%] [G loss: 1.220715]\n",
      "epoch:18 step:17410 [D loss: 0.575153, acc.: 74.22%] [G loss: 1.243785]\n",
      "epoch:18 step:17411 [D loss: 0.467461, acc.: 80.47%] [G loss: 1.453805]\n",
      "epoch:18 step:17412 [D loss: 0.596984, acc.: 71.88%] [G loss: 1.156232]\n",
      "epoch:18 step:17413 [D loss: 0.724719, acc.: 60.16%] [G loss: 0.903533]\n",
      "epoch:18 step:17414 [D loss: 0.507848, acc.: 78.12%] [G loss: 1.657291]\n",
      "epoch:18 step:17415 [D loss: 0.481699, acc.: 78.91%] [G loss: 1.288992]\n",
      "epoch:18 step:17416 [D loss: 0.428226, acc.: 87.50%] [G loss: 1.441799]\n",
      "epoch:18 step:17417 [D loss: 0.633258, acc.: 64.84%] [G loss: 1.205129]\n",
      "epoch:18 step:17418 [D loss: 0.574832, acc.: 67.97%] [G loss: 1.030014]\n",
      "epoch:18 step:17419 [D loss: 0.518574, acc.: 74.22%] [G loss: 1.218446]\n",
      "epoch:18 step:17420 [D loss: 0.644464, acc.: 67.19%] [G loss: 1.115978]\n",
      "epoch:18 step:17421 [D loss: 0.470455, acc.: 79.69%] [G loss: 1.070716]\n",
      "epoch:18 step:17422 [D loss: 0.585627, acc.: 65.62%] [G loss: 1.271142]\n",
      "epoch:18 step:17423 [D loss: 0.557693, acc.: 71.88%] [G loss: 1.159296]\n",
      "epoch:18 step:17424 [D loss: 0.492366, acc.: 79.69%] [G loss: 1.090200]\n",
      "epoch:18 step:17425 [D loss: 0.458584, acc.: 77.34%] [G loss: 1.197995]\n",
      "epoch:18 step:17426 [D loss: 0.612145, acc.: 66.41%] [G loss: 1.324252]\n",
      "epoch:18 step:17427 [D loss: 0.491577, acc.: 76.56%] [G loss: 1.689958]\n",
      "epoch:18 step:17428 [D loss: 0.542853, acc.: 74.22%] [G loss: 1.410287]\n",
      "epoch:18 step:17429 [D loss: 0.571562, acc.: 71.09%] [G loss: 1.302917]\n",
      "epoch:18 step:17430 [D loss: 0.403650, acc.: 88.28%] [G loss: 1.611626]\n",
      "epoch:18 step:17431 [D loss: 0.507155, acc.: 78.12%] [G loss: 1.416916]\n",
      "epoch:18 step:17432 [D loss: 0.561156, acc.: 71.09%] [G loss: 1.171843]\n",
      "epoch:18 step:17433 [D loss: 0.584716, acc.: 66.41%] [G loss: 0.985989]\n",
      "epoch:18 step:17434 [D loss: 0.576933, acc.: 70.31%] [G loss: 1.583165]\n",
      "epoch:18 step:17435 [D loss: 0.641784, acc.: 66.41%] [G loss: 1.251975]\n",
      "epoch:18 step:17436 [D loss: 0.709048, acc.: 61.72%] [G loss: 1.064487]\n",
      "epoch:18 step:17437 [D loss: 0.503945, acc.: 79.69%] [G loss: 1.326717]\n",
      "epoch:18 step:17438 [D loss: 0.607080, acc.: 64.84%] [G loss: 1.243274]\n",
      "epoch:18 step:17439 [D loss: 0.680712, acc.: 59.38%] [G loss: 1.227450]\n",
      "epoch:18 step:17440 [D loss: 0.537259, acc.: 74.22%] [G loss: 1.429272]\n",
      "epoch:18 step:17441 [D loss: 0.596974, acc.: 72.66%] [G loss: 1.209266]\n",
      "epoch:18 step:17442 [D loss: 0.398849, acc.: 87.50%] [G loss: 1.507555]\n",
      "epoch:18 step:17443 [D loss: 0.650687, acc.: 61.72%] [G loss: 1.406390]\n",
      "epoch:18 step:17444 [D loss: 0.517807, acc.: 77.34%] [G loss: 1.339090]\n",
      "epoch:18 step:17445 [D loss: 0.420920, acc.: 79.69%] [G loss: 1.502751]\n",
      "epoch:18 step:17446 [D loss: 0.626244, acc.: 67.19%] [G loss: 1.136993]\n",
      "epoch:18 step:17447 [D loss: 0.485005, acc.: 83.59%] [G loss: 1.323792]\n",
      "epoch:18 step:17448 [D loss: 0.530412, acc.: 75.00%] [G loss: 1.272668]\n",
      "epoch:18 step:17449 [D loss: 0.625975, acc.: 63.28%] [G loss: 1.145615]\n",
      "epoch:18 step:17450 [D loss: 0.452981, acc.: 81.25%] [G loss: 1.680469]\n",
      "epoch:18 step:17451 [D loss: 0.613075, acc.: 64.06%] [G loss: 1.155124]\n",
      "epoch:18 step:17452 [D loss: 0.599068, acc.: 67.97%] [G loss: 1.277811]\n",
      "epoch:18 step:17453 [D loss: 0.535789, acc.: 72.66%] [G loss: 1.340121]\n",
      "epoch:18 step:17454 [D loss: 0.883876, acc.: 42.97%] [G loss: 1.315731]\n",
      "epoch:18 step:17455 [D loss: 0.518777, acc.: 78.12%] [G loss: 1.133082]\n",
      "epoch:18 step:17456 [D loss: 0.633207, acc.: 63.28%] [G loss: 0.745978]\n",
      "epoch:18 step:17457 [D loss: 0.548452, acc.: 71.09%] [G loss: 1.402969]\n",
      "epoch:18 step:17458 [D loss: 0.522176, acc.: 75.78%] [G loss: 1.622431]\n",
      "epoch:18 step:17459 [D loss: 0.643176, acc.: 61.72%] [G loss: 1.354174]\n",
      "epoch:18 step:17460 [D loss: 0.566930, acc.: 68.75%] [G loss: 1.277660]\n",
      "epoch:18 step:17461 [D loss: 0.598750, acc.: 67.19%] [G loss: 1.322271]\n",
      "epoch:18 step:17462 [D loss: 0.571500, acc.: 67.19%] [G loss: 1.498160]\n",
      "epoch:18 step:17463 [D loss: 0.690041, acc.: 57.03%] [G loss: 1.189330]\n",
      "epoch:18 step:17464 [D loss: 0.559705, acc.: 69.53%] [G loss: 1.212173]\n",
      "epoch:18 step:17465 [D loss: 0.532832, acc.: 75.00%] [G loss: 1.258793]\n",
      "epoch:18 step:17466 [D loss: 0.480965, acc.: 78.91%] [G loss: 1.464393]\n",
      "epoch:18 step:17467 [D loss: 0.750083, acc.: 56.25%] [G loss: 1.471558]\n",
      "epoch:18 step:17468 [D loss: 0.531409, acc.: 75.00%] [G loss: 1.342646]\n",
      "epoch:18 step:17469 [D loss: 0.555926, acc.: 73.44%] [G loss: 1.385497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17470 [D loss: 0.484283, acc.: 78.12%] [G loss: 1.118783]\n",
      "epoch:18 step:17471 [D loss: 0.579039, acc.: 70.31%] [G loss: 1.070861]\n",
      "epoch:18 step:17472 [D loss: 0.560858, acc.: 71.09%] [G loss: 1.227424]\n",
      "epoch:18 step:17473 [D loss: 0.553548, acc.: 72.66%] [G loss: 1.400553]\n",
      "epoch:18 step:17474 [D loss: 0.646405, acc.: 64.84%] [G loss: 1.453389]\n",
      "epoch:18 step:17475 [D loss: 0.487375, acc.: 78.12%] [G loss: 1.461178]\n",
      "epoch:18 step:17476 [D loss: 0.523720, acc.: 76.56%] [G loss: 1.146637]\n",
      "epoch:18 step:17477 [D loss: 0.623738, acc.: 65.62%] [G loss: 1.350879]\n",
      "epoch:18 step:17478 [D loss: 0.660758, acc.: 60.16%] [G loss: 1.321606]\n",
      "epoch:18 step:17479 [D loss: 0.632644, acc.: 60.94%] [G loss: 1.480333]\n",
      "epoch:18 step:17480 [D loss: 0.627358, acc.: 65.62%] [G loss: 1.442265]\n",
      "epoch:18 step:17481 [D loss: 0.546248, acc.: 69.53%] [G loss: 1.374293]\n",
      "epoch:18 step:17482 [D loss: 0.592124, acc.: 71.09%] [G loss: 1.337033]\n",
      "epoch:18 step:17483 [D loss: 0.437021, acc.: 81.25%] [G loss: 1.357328]\n",
      "epoch:18 step:17484 [D loss: 0.540329, acc.: 70.31%] [G loss: 1.386900]\n",
      "epoch:18 step:17485 [D loss: 0.631091, acc.: 64.84%] [G loss: 1.460985]\n",
      "epoch:18 step:17486 [D loss: 0.512863, acc.: 74.22%] [G loss: 1.337230]\n",
      "epoch:18 step:17487 [D loss: 0.527017, acc.: 78.12%] [G loss: 1.451885]\n",
      "epoch:18 step:17488 [D loss: 0.700747, acc.: 57.81%] [G loss: 1.202044]\n",
      "epoch:18 step:17489 [D loss: 0.493590, acc.: 75.00%] [G loss: 1.475806]\n",
      "epoch:18 step:17490 [D loss: 0.582301, acc.: 67.19%] [G loss: 1.251779]\n",
      "epoch:18 step:17491 [D loss: 0.578880, acc.: 70.31%] [G loss: 1.261654]\n",
      "epoch:18 step:17492 [D loss: 0.490251, acc.: 77.34%] [G loss: 1.520120]\n",
      "epoch:18 step:17493 [D loss: 0.517661, acc.: 75.00%] [G loss: 1.250518]\n",
      "epoch:18 step:17494 [D loss: 0.612195, acc.: 64.06%] [G loss: 1.270733]\n",
      "epoch:18 step:17495 [D loss: 0.628106, acc.: 64.06%] [G loss: 1.301148]\n",
      "epoch:18 step:17496 [D loss: 0.563660, acc.: 65.62%] [G loss: 1.296786]\n",
      "epoch:18 step:17497 [D loss: 0.519912, acc.: 75.78%] [G loss: 1.147446]\n",
      "epoch:18 step:17498 [D loss: 0.588287, acc.: 68.75%] [G loss: 1.917169]\n",
      "epoch:18 step:17499 [D loss: 0.589304, acc.: 67.97%] [G loss: 1.772825]\n",
      "epoch:18 step:17500 [D loss: 0.470218, acc.: 79.69%] [G loss: 1.094379]\n",
      "epoch:18 step:17501 [D loss: 0.572480, acc.: 67.97%] [G loss: 0.848440]\n",
      "epoch:18 step:17502 [D loss: 0.614611, acc.: 67.19%] [G loss: 1.533950]\n",
      "epoch:18 step:17503 [D loss: 0.697845, acc.: 56.25%] [G loss: 1.234201]\n",
      "epoch:18 step:17504 [D loss: 0.621866, acc.: 65.62%] [G loss: 1.387860]\n",
      "epoch:18 step:17505 [D loss: 0.725056, acc.: 58.59%] [G loss: 1.305891]\n",
      "epoch:18 step:17506 [D loss: 0.566038, acc.: 68.75%] [G loss: 1.539068]\n",
      "epoch:18 step:17507 [D loss: 0.692793, acc.: 62.50%] [G loss: 1.738302]\n",
      "epoch:18 step:17508 [D loss: 0.437210, acc.: 82.81%] [G loss: 1.294024]\n",
      "epoch:18 step:17509 [D loss: 0.868284, acc.: 42.97%] [G loss: 1.042274]\n",
      "epoch:18 step:17510 [D loss: 0.535282, acc.: 71.09%] [G loss: 1.287184]\n",
      "epoch:18 step:17511 [D loss: 0.473880, acc.: 82.03%] [G loss: 1.259024]\n",
      "epoch:18 step:17512 [D loss: 0.624124, acc.: 67.19%] [G loss: 1.206829]\n",
      "epoch:18 step:17513 [D loss: 0.609698, acc.: 69.53%] [G loss: 1.140025]\n",
      "epoch:18 step:17514 [D loss: 0.416155, acc.: 82.03%] [G loss: 1.912676]\n",
      "epoch:18 step:17515 [D loss: 0.447862, acc.: 79.69%] [G loss: 1.792025]\n",
      "epoch:18 step:17516 [D loss: 0.620619, acc.: 66.41%] [G loss: 1.136755]\n",
      "epoch:18 step:17517 [D loss: 0.504994, acc.: 80.47%] [G loss: 1.376335]\n",
      "epoch:18 step:17518 [D loss: 0.567525, acc.: 68.75%] [G loss: 1.321850]\n",
      "epoch:18 step:17519 [D loss: 0.717038, acc.: 59.38%] [G loss: 1.128075]\n",
      "epoch:18 step:17520 [D loss: 0.625095, acc.: 60.94%] [G loss: 1.297143]\n",
      "epoch:18 step:17521 [D loss: 0.772369, acc.: 58.59%] [G loss: 1.048431]\n",
      "epoch:18 step:17522 [D loss: 0.612516, acc.: 68.75%] [G loss: 1.198528]\n",
      "epoch:18 step:17523 [D loss: 0.449468, acc.: 79.69%] [G loss: 1.519021]\n",
      "epoch:18 step:17524 [D loss: 0.588907, acc.: 67.19%] [G loss: 1.476500]\n",
      "epoch:18 step:17525 [D loss: 0.636484, acc.: 67.19%] [G loss: 1.017767]\n",
      "epoch:18 step:17526 [D loss: 0.489299, acc.: 76.56%] [G loss: 0.909459]\n",
      "epoch:18 step:17527 [D loss: 0.451050, acc.: 82.81%] [G loss: 1.257272]\n",
      "epoch:18 step:17528 [D loss: 0.747703, acc.: 50.78%] [G loss: 0.899355]\n",
      "epoch:18 step:17529 [D loss: 0.561432, acc.: 70.31%] [G loss: 1.517891]\n",
      "epoch:18 step:17530 [D loss: 0.581401, acc.: 72.66%] [G loss: 1.567856]\n",
      "epoch:18 step:17531 [D loss: 0.494719, acc.: 79.69%] [G loss: 1.301630]\n",
      "epoch:18 step:17532 [D loss: 0.694373, acc.: 58.59%] [G loss: 1.321494]\n",
      "epoch:18 step:17533 [D loss: 0.648253, acc.: 66.41%] [G loss: 1.097186]\n",
      "epoch:18 step:17534 [D loss: 0.741839, acc.: 56.25%] [G loss: 1.037414]\n",
      "epoch:18 step:17535 [D loss: 0.413280, acc.: 82.03%] [G loss: 1.615225]\n",
      "epoch:18 step:17536 [D loss: 0.518781, acc.: 73.44%] [G loss: 1.274006]\n",
      "epoch:18 step:17537 [D loss: 0.683168, acc.: 57.03%] [G loss: 1.208412]\n",
      "epoch:18 step:17538 [D loss: 0.529067, acc.: 76.56%] [G loss: 1.124536]\n",
      "epoch:18 step:17539 [D loss: 0.655867, acc.: 57.81%] [G loss: 1.347161]\n",
      "epoch:18 step:17540 [D loss: 0.447872, acc.: 82.81%] [G loss: 1.235811]\n",
      "epoch:18 step:17541 [D loss: 0.524621, acc.: 72.66%] [G loss: 1.040127]\n",
      "epoch:18 step:17542 [D loss: 0.525076, acc.: 75.00%] [G loss: 1.400141]\n",
      "epoch:18 step:17543 [D loss: 0.516227, acc.: 72.66%] [G loss: 1.048033]\n",
      "epoch:18 step:17544 [D loss: 0.511958, acc.: 76.56%] [G loss: 1.671067]\n",
      "epoch:18 step:17545 [D loss: 0.616860, acc.: 69.53%] [G loss: 1.330633]\n",
      "epoch:18 step:17546 [D loss: 0.688213, acc.: 57.03%] [G loss: 1.151099]\n",
      "epoch:18 step:17547 [D loss: 0.411962, acc.: 80.47%] [G loss: 1.598267]\n",
      "epoch:18 step:17548 [D loss: 0.477097, acc.: 78.12%] [G loss: 1.413962]\n",
      "epoch:18 step:17549 [D loss: 0.645048, acc.: 61.72%] [G loss: 1.222559]\n",
      "epoch:18 step:17550 [D loss: 0.526716, acc.: 76.56%] [G loss: 1.220433]\n",
      "epoch:18 step:17551 [D loss: 0.587379, acc.: 67.97%] [G loss: 1.212075]\n",
      "epoch:18 step:17552 [D loss: 0.526835, acc.: 80.47%] [G loss: 1.166792]\n",
      "epoch:18 step:17553 [D loss: 0.604292, acc.: 67.97%] [G loss: 1.065483]\n",
      "epoch:18 step:17554 [D loss: 0.686260, acc.: 53.91%] [G loss: 1.278524]\n",
      "epoch:18 step:17555 [D loss: 0.615270, acc.: 64.06%] [G loss: 1.417184]\n",
      "epoch:18 step:17556 [D loss: 0.459580, acc.: 79.69%] [G loss: 1.207555]\n",
      "epoch:18 step:17557 [D loss: 0.570570, acc.: 69.53%] [G loss: 1.181273]\n",
      "epoch:18 step:17558 [D loss: 0.484595, acc.: 77.34%] [G loss: 1.090476]\n",
      "epoch:18 step:17559 [D loss: 0.624775, acc.: 68.75%] [G loss: 1.288708]\n",
      "epoch:18 step:17560 [D loss: 0.584522, acc.: 62.50%] [G loss: 1.431700]\n",
      "epoch:18 step:17561 [D loss: 0.428710, acc.: 82.03%] [G loss: 1.415547]\n",
      "epoch:18 step:17562 [D loss: 0.439050, acc.: 83.59%] [G loss: 1.619646]\n",
      "epoch:18 step:17563 [D loss: 0.565130, acc.: 68.75%] [G loss: 1.167934]\n",
      "epoch:18 step:17564 [D loss: 0.740057, acc.: 54.69%] [G loss: 1.437845]\n",
      "epoch:18 step:17565 [D loss: 0.554294, acc.: 72.66%] [G loss: 1.581182]\n",
      "epoch:18 step:17566 [D loss: 0.634337, acc.: 65.62%] [G loss: 1.436777]\n",
      "epoch:18 step:17567 [D loss: 0.599596, acc.: 63.28%] [G loss: 1.497775]\n",
      "epoch:18 step:17568 [D loss: 0.414164, acc.: 83.59%] [G loss: 1.767833]\n",
      "epoch:18 step:17569 [D loss: 0.583586, acc.: 70.31%] [G loss: 1.315074]\n",
      "epoch:18 step:17570 [D loss: 0.646052, acc.: 62.50%] [G loss: 1.247937]\n",
      "epoch:18 step:17571 [D loss: 0.431000, acc.: 82.81%] [G loss: 1.339402]\n",
      "epoch:18 step:17572 [D loss: 0.553560, acc.: 76.56%] [G loss: 1.397860]\n",
      "epoch:18 step:17573 [D loss: 0.542326, acc.: 72.66%] [G loss: 1.233046]\n",
      "epoch:18 step:17574 [D loss: 0.688601, acc.: 58.59%] [G loss: 1.154721]\n",
      "epoch:18 step:17575 [D loss: 0.671789, acc.: 57.81%] [G loss: 1.099040]\n",
      "epoch:18 step:17576 [D loss: 0.476660, acc.: 78.12%] [G loss: 1.436534]\n",
      "epoch:18 step:17577 [D loss: 0.494793, acc.: 82.81%] [G loss: 1.086173]\n",
      "epoch:18 step:17578 [D loss: 0.605662, acc.: 65.62%] [G loss: 1.477611]\n",
      "epoch:18 step:17579 [D loss: 0.546017, acc.: 70.31%] [G loss: 1.054130]\n",
      "epoch:18 step:17580 [D loss: 0.626719, acc.: 69.53%] [G loss: 1.470618]\n",
      "epoch:18 step:17581 [D loss: 0.612310, acc.: 67.19%] [G loss: 1.477215]\n",
      "epoch:18 step:17582 [D loss: 0.439721, acc.: 82.03%] [G loss: 1.583540]\n",
      "epoch:18 step:17583 [D loss: 0.562963, acc.: 70.31%] [G loss: 1.723145]\n",
      "epoch:18 step:17584 [D loss: 0.478922, acc.: 78.91%] [G loss: 1.298671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17585 [D loss: 0.588168, acc.: 70.31%] [G loss: 1.527934]\n",
      "epoch:18 step:17586 [D loss: 0.641521, acc.: 57.81%] [G loss: 1.286423]\n",
      "epoch:18 step:17587 [D loss: 0.533313, acc.: 75.00%] [G loss: 1.529655]\n",
      "epoch:18 step:17588 [D loss: 0.641941, acc.: 63.28%] [G loss: 1.386238]\n",
      "epoch:18 step:17589 [D loss: 0.586539, acc.: 66.41%] [G loss: 1.393262]\n",
      "epoch:18 step:17590 [D loss: 0.546528, acc.: 72.66%] [G loss: 1.421893]\n",
      "epoch:18 step:17591 [D loss: 0.492151, acc.: 76.56%] [G loss: 1.478014]\n",
      "epoch:18 step:17592 [D loss: 0.617315, acc.: 64.84%] [G loss: 1.227821]\n",
      "epoch:18 step:17593 [D loss: 0.469013, acc.: 77.34%] [G loss: 1.514038]\n",
      "epoch:18 step:17594 [D loss: 0.581887, acc.: 74.22%] [G loss: 1.270543]\n",
      "epoch:18 step:17595 [D loss: 0.505171, acc.: 74.22%] [G loss: 1.184063]\n",
      "epoch:18 step:17596 [D loss: 0.765093, acc.: 47.66%] [G loss: 1.363795]\n",
      "epoch:18 step:17597 [D loss: 0.614331, acc.: 71.09%] [G loss: 1.304456]\n",
      "epoch:18 step:17598 [D loss: 0.645806, acc.: 60.94%] [G loss: 1.267540]\n",
      "epoch:18 step:17599 [D loss: 0.531304, acc.: 74.22%] [G loss: 1.195168]\n",
      "epoch:18 step:17600 [D loss: 0.638943, acc.: 67.19%] [G loss: 1.107146]\n",
      "##############\n",
      "[2.69446057 2.11843826 1.88014748 2.95343594 0.8395558  6.2738013\n",
      " 2.20610303 2.48857671 3.91860006 4.56987552]\n",
      "##########\n",
      "epoch:18 step:17601 [D loss: 0.688307, acc.: 57.03%] [G loss: 1.273257]\n",
      "epoch:18 step:17602 [D loss: 0.633660, acc.: 71.09%] [G loss: 1.187696]\n",
      "epoch:18 step:17603 [D loss: 0.646744, acc.: 63.28%] [G loss: 0.964591]\n",
      "epoch:18 step:17604 [D loss: 0.677404, acc.: 57.03%] [G loss: 1.145240]\n",
      "epoch:18 step:17605 [D loss: 0.721129, acc.: 68.75%] [G loss: 1.036538]\n",
      "epoch:18 step:17606 [D loss: 0.771623, acc.: 50.78%] [G loss: 1.333101]\n",
      "epoch:18 step:17607 [D loss: 0.685674, acc.: 57.81%] [G loss: 1.265537]\n",
      "epoch:18 step:17608 [D loss: 0.486634, acc.: 80.47%] [G loss: 1.300855]\n",
      "epoch:18 step:17609 [D loss: 0.680586, acc.: 58.59%] [G loss: 1.150613]\n",
      "epoch:18 step:17610 [D loss: 0.706355, acc.: 59.38%] [G loss: 1.011685]\n",
      "epoch:18 step:17611 [D loss: 0.509720, acc.: 74.22%] [G loss: 1.197819]\n",
      "epoch:18 step:17612 [D loss: 0.639721, acc.: 65.62%] [G loss: 0.973571]\n",
      "epoch:18 step:17613 [D loss: 0.355486, acc.: 89.84%] [G loss: 1.734180]\n",
      "epoch:18 step:17614 [D loss: 0.514809, acc.: 74.22%] [G loss: 1.279694]\n",
      "epoch:18 step:17615 [D loss: 0.448043, acc.: 79.69%] [G loss: 1.372501]\n",
      "epoch:18 step:17616 [D loss: 0.556989, acc.: 66.41%] [G loss: 1.086300]\n",
      "epoch:18 step:17617 [D loss: 0.656287, acc.: 60.94%] [G loss: 1.312605]\n",
      "epoch:18 step:17618 [D loss: 0.559469, acc.: 67.97%] [G loss: 1.264271]\n",
      "epoch:18 step:17619 [D loss: 0.646040, acc.: 65.62%] [G loss: 1.052695]\n",
      "epoch:18 step:17620 [D loss: 0.539398, acc.: 76.56%] [G loss: 1.092192]\n",
      "epoch:18 step:17621 [D loss: 0.466376, acc.: 82.81%] [G loss: 1.257791]\n",
      "epoch:18 step:17622 [D loss: 0.669362, acc.: 57.03%] [G loss: 1.063068]\n",
      "epoch:18 step:17623 [D loss: 0.557807, acc.: 75.00%] [G loss: 1.215910]\n",
      "epoch:18 step:17624 [D loss: 0.719859, acc.: 57.81%] [G loss: 1.422632]\n",
      "epoch:18 step:17625 [D loss: 0.620896, acc.: 64.06%] [G loss: 1.219064]\n",
      "epoch:18 step:17626 [D loss: 0.501364, acc.: 78.91%] [G loss: 1.362790]\n",
      "epoch:18 step:17627 [D loss: 0.699627, acc.: 53.91%] [G loss: 1.240003]\n",
      "epoch:18 step:17628 [D loss: 0.594355, acc.: 69.53%] [G loss: 1.305779]\n",
      "epoch:18 step:17629 [D loss: 0.470358, acc.: 78.91%] [G loss: 1.252676]\n",
      "epoch:18 step:17630 [D loss: 0.467860, acc.: 83.59%] [G loss: 1.427153]\n",
      "epoch:18 step:17631 [D loss: 0.574963, acc.: 67.97%] [G loss: 0.900293]\n",
      "epoch:18 step:17632 [D loss: 0.497269, acc.: 76.56%] [G loss: 1.119415]\n",
      "epoch:18 step:17633 [D loss: 0.626233, acc.: 67.97%] [G loss: 1.110188]\n",
      "epoch:18 step:17634 [D loss: 0.653819, acc.: 61.72%] [G loss: 1.562867]\n",
      "epoch:18 step:17635 [D loss: 0.475503, acc.: 82.81%] [G loss: 1.378812]\n",
      "epoch:18 step:17636 [D loss: 0.490112, acc.: 82.81%] [G loss: 1.473731]\n",
      "epoch:18 step:17637 [D loss: 0.698868, acc.: 57.81%] [G loss: 1.151303]\n",
      "epoch:18 step:17638 [D loss: 0.508809, acc.: 78.12%] [G loss: 1.271151]\n",
      "epoch:18 step:17639 [D loss: 0.553579, acc.: 68.75%] [G loss: 1.177497]\n",
      "epoch:18 step:17640 [D loss: 0.601172, acc.: 71.88%] [G loss: 1.061007]\n",
      "epoch:18 step:17641 [D loss: 0.604614, acc.: 70.31%] [G loss: 1.281580]\n",
      "epoch:18 step:17642 [D loss: 0.617699, acc.: 70.31%] [G loss: 1.023304]\n",
      "epoch:18 step:17643 [D loss: 0.623973, acc.: 72.66%] [G loss: 1.296045]\n",
      "epoch:18 step:17644 [D loss: 0.529321, acc.: 71.09%] [G loss: 1.351596]\n",
      "epoch:18 step:17645 [D loss: 0.584350, acc.: 64.84%] [G loss: 1.447917]\n",
      "epoch:18 step:17646 [D loss: 0.598949, acc.: 69.53%] [G loss: 1.108025]\n",
      "epoch:18 step:17647 [D loss: 0.557381, acc.: 70.31%] [G loss: 1.469264]\n",
      "epoch:18 step:17648 [D loss: 0.534200, acc.: 76.56%] [G loss: 1.468441]\n",
      "epoch:18 step:17649 [D loss: 0.567345, acc.: 63.28%] [G loss: 1.268301]\n",
      "epoch:18 step:17650 [D loss: 0.635998, acc.: 64.06%] [G loss: 1.303134]\n",
      "epoch:18 step:17651 [D loss: 0.627833, acc.: 66.41%] [G loss: 1.163707]\n",
      "epoch:18 step:17652 [D loss: 0.596500, acc.: 66.41%] [G loss: 1.464686]\n",
      "epoch:18 step:17653 [D loss: 0.552199, acc.: 70.31%] [G loss: 0.921645]\n",
      "epoch:18 step:17654 [D loss: 0.599008, acc.: 68.75%] [G loss: 0.941946]\n",
      "epoch:18 step:17655 [D loss: 0.607870, acc.: 68.75%] [G loss: 1.186864]\n",
      "epoch:18 step:17656 [D loss: 0.580799, acc.: 72.66%] [G loss: 1.574461]\n",
      "epoch:18 step:17657 [D loss: 0.505192, acc.: 75.78%] [G loss: 1.414260]\n",
      "epoch:18 step:17658 [D loss: 0.597922, acc.: 64.84%] [G loss: 1.266578]\n",
      "epoch:18 step:17659 [D loss: 0.479392, acc.: 78.12%] [G loss: 1.542442]\n",
      "epoch:18 step:17660 [D loss: 0.621432, acc.: 60.16%] [G loss: 1.099139]\n",
      "epoch:18 step:17661 [D loss: 0.606930, acc.: 66.41%] [G loss: 1.311544]\n",
      "epoch:18 step:17662 [D loss: 0.531042, acc.: 71.88%] [G loss: 1.424343]\n",
      "epoch:18 step:17663 [D loss: 0.578510, acc.: 71.09%] [G loss: 1.671895]\n",
      "epoch:18 step:17664 [D loss: 0.643882, acc.: 63.28%] [G loss: 1.102076]\n",
      "epoch:18 step:17665 [D loss: 0.546301, acc.: 70.31%] [G loss: 1.254568]\n",
      "epoch:18 step:17666 [D loss: 0.533103, acc.: 71.09%] [G loss: 1.140795]\n",
      "epoch:18 step:17667 [D loss: 0.646048, acc.: 62.50%] [G loss: 1.208736]\n",
      "epoch:18 step:17668 [D loss: 0.802226, acc.: 52.34%] [G loss: 1.276565]\n",
      "epoch:18 step:17669 [D loss: 0.550020, acc.: 71.09%] [G loss: 1.531165]\n",
      "epoch:18 step:17670 [D loss: 0.537906, acc.: 71.09%] [G loss: 1.635357]\n",
      "epoch:18 step:17671 [D loss: 0.563541, acc.: 70.31%] [G loss: 1.436823]\n",
      "epoch:18 step:17672 [D loss: 0.593890, acc.: 67.97%] [G loss: 1.468934]\n",
      "epoch:18 step:17673 [D loss: 0.564166, acc.: 67.19%] [G loss: 1.158029]\n",
      "epoch:18 step:17674 [D loss: 0.416398, acc.: 85.16%] [G loss: 1.496049]\n",
      "epoch:18 step:17675 [D loss: 0.524826, acc.: 73.44%] [G loss: 1.168506]\n",
      "epoch:18 step:17676 [D loss: 0.545514, acc.: 69.53%] [G loss: 1.426454]\n",
      "epoch:18 step:17677 [D loss: 0.866476, acc.: 44.53%] [G loss: 1.252390]\n",
      "epoch:18 step:17678 [D loss: 0.665567, acc.: 64.84%] [G loss: 0.845261]\n",
      "epoch:18 step:17679 [D loss: 0.486767, acc.: 75.78%] [G loss: 1.435899]\n",
      "epoch:18 step:17680 [D loss: 0.416594, acc.: 86.72%] [G loss: 1.817558]\n",
      "epoch:18 step:17681 [D loss: 0.557205, acc.: 75.00%] [G loss: 1.494924]\n",
      "epoch:18 step:17682 [D loss: 0.434607, acc.: 82.03%] [G loss: 1.218382]\n",
      "epoch:18 step:17683 [D loss: 0.563185, acc.: 72.66%] [G loss: 1.534957]\n",
      "epoch:18 step:17684 [D loss: 0.559612, acc.: 73.44%] [G loss: 1.285497]\n",
      "epoch:18 step:17685 [D loss: 0.530942, acc.: 77.34%] [G loss: 1.519273]\n",
      "epoch:18 step:17686 [D loss: 0.585082, acc.: 65.62%] [G loss: 1.085138]\n",
      "epoch:18 step:17687 [D loss: 0.571157, acc.: 69.53%] [G loss: 1.508274]\n",
      "epoch:18 step:17688 [D loss: 0.552997, acc.: 75.78%] [G loss: 1.200962]\n",
      "epoch:18 step:17689 [D loss: 0.565563, acc.: 68.75%] [G loss: 1.306245]\n",
      "epoch:18 step:17690 [D loss: 0.550875, acc.: 71.88%] [G loss: 1.575063]\n",
      "epoch:18 step:17691 [D loss: 0.632073, acc.: 60.16%] [G loss: 1.201239]\n",
      "epoch:18 step:17692 [D loss: 0.606749, acc.: 67.97%] [G loss: 1.056427]\n",
      "epoch:18 step:17693 [D loss: 0.500510, acc.: 74.22%] [G loss: 1.340996]\n",
      "epoch:18 step:17694 [D loss: 0.719329, acc.: 63.28%] [G loss: 1.261301]\n",
      "epoch:18 step:17695 [D loss: 0.527427, acc.: 73.44%] [G loss: 1.306541]\n",
      "epoch:18 step:17696 [D loss: 0.440888, acc.: 78.91%] [G loss: 1.704481]\n",
      "epoch:18 step:17697 [D loss: 0.713979, acc.: 56.25%] [G loss: 1.211586]\n",
      "epoch:18 step:17698 [D loss: 0.491770, acc.: 78.91%] [G loss: 1.806463]\n",
      "epoch:18 step:17699 [D loss: 0.595496, acc.: 67.97%] [G loss: 1.537265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17700 [D loss: 0.659374, acc.: 57.81%] [G loss: 0.944450]\n",
      "epoch:18 step:17701 [D loss: 0.479436, acc.: 78.12%] [G loss: 1.128599]\n",
      "epoch:18 step:17702 [D loss: 0.640227, acc.: 68.75%] [G loss: 1.177161]\n",
      "epoch:18 step:17703 [D loss: 0.516518, acc.: 78.12%] [G loss: 1.521264]\n",
      "epoch:18 step:17704 [D loss: 0.558012, acc.: 72.66%] [G loss: 1.338665]\n",
      "epoch:18 step:17705 [D loss: 0.655556, acc.: 64.84%] [G loss: 1.391290]\n",
      "epoch:18 step:17706 [D loss: 0.513448, acc.: 74.22%] [G loss: 1.156310]\n",
      "epoch:18 step:17707 [D loss: 0.593946, acc.: 63.28%] [G loss: 1.307468]\n",
      "epoch:18 step:17708 [D loss: 0.682486, acc.: 59.38%] [G loss: 0.923305]\n",
      "epoch:18 step:17709 [D loss: 0.563084, acc.: 72.66%] [G loss: 1.272686]\n",
      "epoch:18 step:17710 [D loss: 0.794277, acc.: 46.88%] [G loss: 1.196366]\n",
      "epoch:18 step:17711 [D loss: 0.647824, acc.: 65.62%] [G loss: 1.175781]\n",
      "epoch:18 step:17712 [D loss: 0.554684, acc.: 67.19%] [G loss: 1.365958]\n",
      "epoch:18 step:17713 [D loss: 0.500397, acc.: 78.12%] [G loss: 1.172349]\n",
      "epoch:18 step:17714 [D loss: 0.621420, acc.: 67.97%] [G loss: 1.416550]\n",
      "epoch:18 step:17715 [D loss: 0.444205, acc.: 78.91%] [G loss: 1.453298]\n",
      "epoch:18 step:17716 [D loss: 0.585867, acc.: 70.31%] [G loss: 1.137740]\n",
      "epoch:18 step:17717 [D loss: 0.620223, acc.: 65.62%] [G loss: 1.372944]\n",
      "epoch:18 step:17718 [D loss: 0.442619, acc.: 83.59%] [G loss: 1.362195]\n",
      "epoch:18 step:17719 [D loss: 0.458822, acc.: 78.12%] [G loss: 1.223785]\n",
      "epoch:18 step:17720 [D loss: 0.516234, acc.: 77.34%] [G loss: 1.053558]\n",
      "epoch:18 step:17721 [D loss: 0.503918, acc.: 76.56%] [G loss: 1.161649]\n",
      "epoch:18 step:17722 [D loss: 0.467492, acc.: 75.78%] [G loss: 1.564286]\n",
      "epoch:18 step:17723 [D loss: 0.461985, acc.: 82.03%] [G loss: 1.199798]\n",
      "epoch:18 step:17724 [D loss: 0.529105, acc.: 72.66%] [G loss: 1.127081]\n",
      "epoch:18 step:17725 [D loss: 0.595619, acc.: 70.31%] [G loss: 1.394673]\n",
      "epoch:18 step:17726 [D loss: 0.810400, acc.: 46.09%] [G loss: 1.303837]\n",
      "epoch:18 step:17727 [D loss: 0.554317, acc.: 69.53%] [G loss: 1.295115]\n",
      "epoch:18 step:17728 [D loss: 0.665228, acc.: 68.75%] [G loss: 1.306063]\n",
      "epoch:18 step:17729 [D loss: 0.471122, acc.: 82.81%] [G loss: 1.222233]\n",
      "epoch:18 step:17730 [D loss: 0.536052, acc.: 78.12%] [G loss: 1.439757]\n",
      "epoch:18 step:17731 [D loss: 0.596636, acc.: 65.62%] [G loss: 1.131591]\n",
      "epoch:18 step:17732 [D loss: 0.460157, acc.: 81.25%] [G loss: 1.116659]\n",
      "epoch:18 step:17733 [D loss: 0.422170, acc.: 86.72%] [G loss: 1.361388]\n",
      "epoch:18 step:17734 [D loss: 0.478793, acc.: 75.78%] [G loss: 1.788810]\n",
      "epoch:18 step:17735 [D loss: 0.491169, acc.: 78.91%] [G loss: 1.593768]\n",
      "epoch:18 step:17736 [D loss: 0.686080, acc.: 59.38%] [G loss: 1.296525]\n",
      "epoch:18 step:17737 [D loss: 0.653571, acc.: 60.94%] [G loss: 1.073783]\n",
      "epoch:18 step:17738 [D loss: 0.459155, acc.: 78.12%] [G loss: 1.410761]\n",
      "epoch:18 step:17739 [D loss: 0.481072, acc.: 81.25%] [G loss: 1.322447]\n",
      "epoch:18 step:17740 [D loss: 0.493959, acc.: 79.69%] [G loss: 1.488261]\n",
      "epoch:18 step:17741 [D loss: 0.432168, acc.: 83.59%] [G loss: 1.725156]\n",
      "epoch:18 step:17742 [D loss: 0.547930, acc.: 72.66%] [G loss: 1.305267]\n",
      "epoch:18 step:17743 [D loss: 0.669397, acc.: 60.94%] [G loss: 1.281605]\n",
      "epoch:18 step:17744 [D loss: 0.610754, acc.: 64.84%] [G loss: 1.052927]\n",
      "epoch:18 step:17745 [D loss: 0.535339, acc.: 75.00%] [G loss: 1.612463]\n",
      "epoch:18 step:17746 [D loss: 0.477935, acc.: 78.91%] [G loss: 1.442387]\n",
      "epoch:18 step:17747 [D loss: 0.678826, acc.: 59.38%] [G loss: 1.309874]\n",
      "epoch:18 step:17748 [D loss: 0.597451, acc.: 68.75%] [G loss: 1.270633]\n",
      "epoch:18 step:17749 [D loss: 0.441453, acc.: 83.59%] [G loss: 1.783038]\n",
      "epoch:18 step:17750 [D loss: 0.603161, acc.: 64.84%] [G loss: 1.339404]\n",
      "epoch:18 step:17751 [D loss: 0.576108, acc.: 68.75%] [G loss: 1.418535]\n",
      "epoch:18 step:17752 [D loss: 0.434089, acc.: 80.47%] [G loss: 1.692904]\n",
      "epoch:18 step:17753 [D loss: 0.677515, acc.: 59.38%] [G loss: 1.035291]\n",
      "epoch:18 step:17754 [D loss: 0.613195, acc.: 60.94%] [G loss: 1.531076]\n",
      "epoch:18 step:17755 [D loss: 0.518670, acc.: 77.34%] [G loss: 1.349046]\n",
      "epoch:18 step:17756 [D loss: 0.651322, acc.: 62.50%] [G loss: 1.073313]\n",
      "epoch:18 step:17757 [D loss: 0.596322, acc.: 66.41%] [G loss: 1.101030]\n",
      "epoch:18 step:17758 [D loss: 0.569313, acc.: 67.97%] [G loss: 1.220854]\n",
      "epoch:18 step:17759 [D loss: 0.465874, acc.: 83.59%] [G loss: 2.014914]\n",
      "epoch:18 step:17760 [D loss: 0.824875, acc.: 48.44%] [G loss: 1.291229]\n",
      "epoch:18 step:17761 [D loss: 0.593717, acc.: 70.31%] [G loss: 1.389659]\n",
      "epoch:18 step:17762 [D loss: 0.593740, acc.: 69.53%] [G loss: 1.414788]\n",
      "epoch:18 step:17763 [D loss: 0.648830, acc.: 64.06%] [G loss: 1.523535]\n",
      "epoch:18 step:17764 [D loss: 0.586592, acc.: 68.75%] [G loss: 1.164034]\n",
      "epoch:18 step:17765 [D loss: 0.517470, acc.: 77.34%] [G loss: 1.042082]\n",
      "epoch:18 step:17766 [D loss: 0.482204, acc.: 82.03%] [G loss: 1.327502]\n",
      "epoch:18 step:17767 [D loss: 0.477923, acc.: 79.69%] [G loss: 1.352851]\n",
      "epoch:18 step:17768 [D loss: 0.628802, acc.: 63.28%] [G loss: 1.394898]\n",
      "epoch:18 step:17769 [D loss: 0.499502, acc.: 76.56%] [G loss: 1.119721]\n",
      "epoch:18 step:17770 [D loss: 0.407642, acc.: 83.59%] [G loss: 1.498760]\n",
      "epoch:18 step:17771 [D loss: 0.607235, acc.: 67.19%] [G loss: 1.532446]\n",
      "epoch:18 step:17772 [D loss: 0.474504, acc.: 79.69%] [G loss: 1.141145]\n",
      "epoch:18 step:17773 [D loss: 0.673695, acc.: 64.06%] [G loss: 1.157870]\n",
      "epoch:18 step:17774 [D loss: 0.639710, acc.: 67.97%] [G loss: 1.184968]\n",
      "epoch:18 step:17775 [D loss: 0.654720, acc.: 60.94%] [G loss: 1.150689]\n",
      "epoch:18 step:17776 [D loss: 0.560599, acc.: 73.44%] [G loss: 1.269071]\n",
      "epoch:18 step:17777 [D loss: 0.507707, acc.: 77.34%] [G loss: 1.401785]\n",
      "epoch:18 step:17778 [D loss: 0.641247, acc.: 60.94%] [G loss: 1.219917]\n",
      "epoch:18 step:17779 [D loss: 0.578116, acc.: 69.53%] [G loss: 1.315182]\n",
      "epoch:18 step:17780 [D loss: 0.656453, acc.: 64.06%] [G loss: 0.903088]\n",
      "epoch:18 step:17781 [D loss: 0.571669, acc.: 67.19%] [G loss: 1.169898]\n",
      "epoch:18 step:17782 [D loss: 0.587177, acc.: 69.53%] [G loss: 1.356859]\n",
      "epoch:18 step:17783 [D loss: 0.544327, acc.: 71.09%] [G loss: 1.243572]\n",
      "epoch:18 step:17784 [D loss: 0.424509, acc.: 84.38%] [G loss: 1.439374]\n",
      "epoch:18 step:17785 [D loss: 0.610437, acc.: 63.28%] [G loss: 1.245976]\n",
      "epoch:18 step:17786 [D loss: 0.570348, acc.: 70.31%] [G loss: 1.292246]\n",
      "epoch:18 step:17787 [D loss: 0.566241, acc.: 68.75%] [G loss: 1.272870]\n",
      "epoch:18 step:17788 [D loss: 0.614717, acc.: 67.97%] [G loss: 1.504901]\n",
      "epoch:18 step:17789 [D loss: 0.617848, acc.: 64.06%] [G loss: 1.155113]\n",
      "epoch:18 step:17790 [D loss: 0.524973, acc.: 74.22%] [G loss: 1.090347]\n",
      "epoch:18 step:17791 [D loss: 0.488221, acc.: 78.12%] [G loss: 1.254255]\n",
      "epoch:18 step:17792 [D loss: 0.507508, acc.: 76.56%] [G loss: 1.134669]\n",
      "epoch:18 step:17793 [D loss: 0.576474, acc.: 71.88%] [G loss: 1.579797]\n",
      "epoch:18 step:17794 [D loss: 0.503054, acc.: 78.91%] [G loss: 1.372033]\n",
      "epoch:18 step:17795 [D loss: 0.528769, acc.: 71.88%] [G loss: 1.580546]\n",
      "epoch:18 step:17796 [D loss: 0.500592, acc.: 77.34%] [G loss: 1.731222]\n",
      "epoch:18 step:17797 [D loss: 0.603047, acc.: 64.84%] [G loss: 1.353014]\n",
      "epoch:18 step:17798 [D loss: 0.520930, acc.: 76.56%] [G loss: 1.356328]\n",
      "epoch:18 step:17799 [D loss: 0.586812, acc.: 67.97%] [G loss: 1.177894]\n",
      "epoch:18 step:17800 [D loss: 0.616655, acc.: 60.94%] [G loss: 1.416187]\n",
      "##############\n",
      "[2.75730642 2.05673625 1.82253484 2.79995433 0.8704005  5.66531435\n",
      " 2.11721733 2.56311645 3.93087112 4.98896622]\n",
      "##########\n",
      "epoch:18 step:17801 [D loss: 0.545323, acc.: 71.09%] [G loss: 1.359004]\n",
      "epoch:18 step:17802 [D loss: 0.544940, acc.: 72.66%] [G loss: 1.345751]\n",
      "epoch:18 step:17803 [D loss: 0.842197, acc.: 46.88%] [G loss: 0.995923]\n",
      "epoch:19 step:17804 [D loss: 0.404775, acc.: 85.94%] [G loss: 1.470770]\n",
      "epoch:19 step:17805 [D loss: 0.659628, acc.: 61.72%] [G loss: 0.888885]\n",
      "epoch:19 step:17806 [D loss: 0.799466, acc.: 50.78%] [G loss: 0.980875]\n",
      "epoch:19 step:17807 [D loss: 0.599564, acc.: 66.41%] [G loss: 1.503258]\n",
      "epoch:19 step:17808 [D loss: 0.646649, acc.: 60.94%] [G loss: 1.314178]\n",
      "epoch:19 step:17809 [D loss: 0.693833, acc.: 58.59%] [G loss: 1.339657]\n",
      "epoch:19 step:17810 [D loss: 0.840754, acc.: 43.75%] [G loss: 0.878643]\n",
      "epoch:19 step:17811 [D loss: 0.559428, acc.: 71.88%] [G loss: 1.436795]\n",
      "epoch:19 step:17812 [D loss: 0.507725, acc.: 76.56%] [G loss: 1.597981]\n",
      "epoch:19 step:17813 [D loss: 0.491932, acc.: 76.56%] [G loss: 1.159590]\n",
      "epoch:19 step:17814 [D loss: 0.523936, acc.: 71.09%] [G loss: 1.488413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17815 [D loss: 0.502495, acc.: 78.12%] [G loss: 1.361002]\n",
      "epoch:19 step:17816 [D loss: 0.692814, acc.: 60.16%] [G loss: 1.285517]\n",
      "epoch:19 step:17817 [D loss: 0.603727, acc.: 67.19%] [G loss: 1.608059]\n",
      "epoch:19 step:17818 [D loss: 0.588612, acc.: 71.09%] [G loss: 1.237393]\n",
      "epoch:19 step:17819 [D loss: 0.570900, acc.: 69.53%] [G loss: 1.526834]\n",
      "epoch:19 step:17820 [D loss: 0.468765, acc.: 82.03%] [G loss: 1.256336]\n",
      "epoch:19 step:17821 [D loss: 0.499726, acc.: 76.56%] [G loss: 1.608087]\n",
      "epoch:19 step:17822 [D loss: 0.441285, acc.: 81.25%] [G loss: 1.411666]\n",
      "epoch:19 step:17823 [D loss: 0.551438, acc.: 71.09%] [G loss: 1.394806]\n",
      "epoch:19 step:17824 [D loss: 0.634539, acc.: 64.06%] [G loss: 0.923897]\n",
      "epoch:19 step:17825 [D loss: 0.461410, acc.: 81.25%] [G loss: 1.382813]\n",
      "epoch:19 step:17826 [D loss: 0.570310, acc.: 67.19%] [G loss: 1.355259]\n",
      "epoch:19 step:17827 [D loss: 0.508698, acc.: 73.44%] [G loss: 1.434889]\n",
      "epoch:19 step:17828 [D loss: 0.559164, acc.: 69.53%] [G loss: 1.137965]\n",
      "epoch:19 step:17829 [D loss: 0.622555, acc.: 67.19%] [G loss: 1.396209]\n",
      "epoch:19 step:17830 [D loss: 0.576968, acc.: 67.97%] [G loss: 1.053368]\n",
      "epoch:19 step:17831 [D loss: 0.504309, acc.: 74.22%] [G loss: 1.112066]\n",
      "epoch:19 step:17832 [D loss: 0.712671, acc.: 60.16%] [G loss: 1.150259]\n",
      "epoch:19 step:17833 [D loss: 0.523381, acc.: 74.22%] [G loss: 1.156226]\n",
      "epoch:19 step:17834 [D loss: 0.703585, acc.: 51.56%] [G loss: 1.152605]\n",
      "epoch:19 step:17835 [D loss: 0.505926, acc.: 79.69%] [G loss: 1.062747]\n",
      "epoch:19 step:17836 [D loss: 0.427400, acc.: 80.47%] [G loss: 1.526009]\n",
      "epoch:19 step:17837 [D loss: 0.523643, acc.: 75.78%] [G loss: 1.156678]\n",
      "epoch:19 step:17838 [D loss: 0.456020, acc.: 77.34%] [G loss: 1.301788]\n",
      "epoch:19 step:17839 [D loss: 0.429294, acc.: 79.69%] [G loss: 1.201694]\n",
      "epoch:19 step:17840 [D loss: 0.638882, acc.: 59.38%] [G loss: 1.053870]\n",
      "epoch:19 step:17841 [D loss: 0.560190, acc.: 67.97%] [G loss: 1.582239]\n",
      "epoch:19 step:17842 [D loss: 0.607330, acc.: 70.31%] [G loss: 1.354882]\n",
      "epoch:19 step:17843 [D loss: 0.606445, acc.: 71.09%] [G loss: 1.458191]\n",
      "epoch:19 step:17844 [D loss: 0.538559, acc.: 69.53%] [G loss: 1.178480]\n",
      "epoch:19 step:17845 [D loss: 0.529198, acc.: 69.53%] [G loss: 1.688735]\n",
      "epoch:19 step:17846 [D loss: 0.534675, acc.: 75.00%] [G loss: 1.355971]\n",
      "epoch:19 step:17847 [D loss: 0.646266, acc.: 66.41%] [G loss: 1.343253]\n",
      "epoch:19 step:17848 [D loss: 0.558376, acc.: 68.75%] [G loss: 1.533247]\n",
      "epoch:19 step:17849 [D loss: 0.735761, acc.: 53.91%] [G loss: 1.168695]\n",
      "epoch:19 step:17850 [D loss: 0.498788, acc.: 75.78%] [G loss: 1.360108]\n",
      "epoch:19 step:17851 [D loss: 0.548078, acc.: 69.53%] [G loss: 1.653765]\n",
      "epoch:19 step:17852 [D loss: 0.611890, acc.: 65.62%] [G loss: 1.002092]\n",
      "epoch:19 step:17853 [D loss: 0.543878, acc.: 70.31%] [G loss: 1.364496]\n",
      "epoch:19 step:17854 [D loss: 0.468996, acc.: 76.56%] [G loss: 1.349340]\n",
      "epoch:19 step:17855 [D loss: 0.500747, acc.: 79.69%] [G loss: 1.725017]\n",
      "epoch:19 step:17856 [D loss: 0.584301, acc.: 67.97%] [G loss: 1.171878]\n",
      "epoch:19 step:17857 [D loss: 0.439376, acc.: 82.03%] [G loss: 1.732642]\n",
      "epoch:19 step:17858 [D loss: 0.649468, acc.: 65.62%] [G loss: 1.251066]\n",
      "epoch:19 step:17859 [D loss: 0.505101, acc.: 74.22%] [G loss: 1.325019]\n",
      "epoch:19 step:17860 [D loss: 0.661277, acc.: 57.81%] [G loss: 1.370684]\n",
      "epoch:19 step:17861 [D loss: 0.614159, acc.: 64.84%] [G loss: 1.324806]\n",
      "epoch:19 step:17862 [D loss: 0.441360, acc.: 78.91%] [G loss: 1.489445]\n",
      "epoch:19 step:17863 [D loss: 0.444504, acc.: 81.25%] [G loss: 1.778853]\n",
      "epoch:19 step:17864 [D loss: 0.582761, acc.: 71.88%] [G loss: 1.319412]\n",
      "epoch:19 step:17865 [D loss: 0.529782, acc.: 72.66%] [G loss: 1.346914]\n",
      "epoch:19 step:17866 [D loss: 0.573329, acc.: 75.00%] [G loss: 1.398327]\n",
      "epoch:19 step:17867 [D loss: 0.627671, acc.: 69.53%] [G loss: 0.889352]\n",
      "epoch:19 step:17868 [D loss: 0.557735, acc.: 68.75%] [G loss: 1.550569]\n",
      "epoch:19 step:17869 [D loss: 0.654516, acc.: 67.97%] [G loss: 1.354203]\n",
      "epoch:19 step:17870 [D loss: 0.489922, acc.: 78.12%] [G loss: 1.382804]\n",
      "epoch:19 step:17871 [D loss: 0.531728, acc.: 75.00%] [G loss: 1.359265]\n",
      "epoch:19 step:17872 [D loss: 0.738050, acc.: 53.91%] [G loss: 1.214206]\n",
      "epoch:19 step:17873 [D loss: 0.608233, acc.: 68.75%] [G loss: 1.164236]\n",
      "epoch:19 step:17874 [D loss: 0.534277, acc.: 74.22%] [G loss: 1.459445]\n",
      "epoch:19 step:17875 [D loss: 0.710723, acc.: 61.72%] [G loss: 1.593131]\n",
      "epoch:19 step:17876 [D loss: 0.486024, acc.: 82.03%] [G loss: 1.433585]\n",
      "epoch:19 step:17877 [D loss: 0.466475, acc.: 78.91%] [G loss: 1.741725]\n",
      "epoch:19 step:17878 [D loss: 0.469168, acc.: 80.47%] [G loss: 1.449682]\n",
      "epoch:19 step:17879 [D loss: 0.507383, acc.: 74.22%] [G loss: 1.330013]\n",
      "epoch:19 step:17880 [D loss: 0.617146, acc.: 64.06%] [G loss: 1.297307]\n",
      "epoch:19 step:17881 [D loss: 0.656095, acc.: 57.81%] [G loss: 1.375144]\n",
      "epoch:19 step:17882 [D loss: 0.630331, acc.: 64.06%] [G loss: 1.292897]\n",
      "epoch:19 step:17883 [D loss: 0.445440, acc.: 84.38%] [G loss: 1.673066]\n",
      "epoch:19 step:17884 [D loss: 0.729007, acc.: 58.59%] [G loss: 1.396078]\n",
      "epoch:19 step:17885 [D loss: 0.546740, acc.: 75.00%] [G loss: 1.077284]\n",
      "epoch:19 step:17886 [D loss: 0.626272, acc.: 64.06%] [G loss: 1.092487]\n",
      "epoch:19 step:17887 [D loss: 0.644692, acc.: 68.75%] [G loss: 1.116087]\n",
      "epoch:19 step:17888 [D loss: 0.587066, acc.: 67.19%] [G loss: 1.037143]\n",
      "epoch:19 step:17889 [D loss: 0.616596, acc.: 65.62%] [G loss: 1.459381]\n",
      "epoch:19 step:17890 [D loss: 0.522677, acc.: 77.34%] [G loss: 1.721407]\n",
      "epoch:19 step:17891 [D loss: 0.546760, acc.: 67.19%] [G loss: 1.337114]\n",
      "epoch:19 step:17892 [D loss: 0.567459, acc.: 65.62%] [G loss: 1.171914]\n",
      "epoch:19 step:17893 [D loss: 0.616150, acc.: 66.41%] [G loss: 1.394994]\n",
      "epoch:19 step:17894 [D loss: 0.535723, acc.: 76.56%] [G loss: 1.422371]\n",
      "epoch:19 step:17895 [D loss: 0.431308, acc.: 85.94%] [G loss: 1.555777]\n",
      "epoch:19 step:17896 [D loss: 0.540642, acc.: 69.53%] [G loss: 1.444418]\n",
      "epoch:19 step:17897 [D loss: 0.494132, acc.: 79.69%] [G loss: 1.364653]\n",
      "epoch:19 step:17898 [D loss: 0.549658, acc.: 69.53%] [G loss: 1.379875]\n",
      "epoch:19 step:17899 [D loss: 0.626797, acc.: 64.06%] [G loss: 1.145064]\n",
      "epoch:19 step:17900 [D loss: 0.614021, acc.: 70.31%] [G loss: 1.178129]\n",
      "epoch:19 step:17901 [D loss: 0.630623, acc.: 67.19%] [G loss: 1.361183]\n",
      "epoch:19 step:17902 [D loss: 0.450305, acc.: 82.81%] [G loss: 1.327257]\n",
      "epoch:19 step:17903 [D loss: 0.628613, acc.: 66.41%] [G loss: 1.544229]\n",
      "epoch:19 step:17904 [D loss: 0.557697, acc.: 71.09%] [G loss: 1.370481]\n",
      "epoch:19 step:17905 [D loss: 0.563339, acc.: 71.88%] [G loss: 1.129475]\n",
      "epoch:19 step:17906 [D loss: 0.545605, acc.: 75.78%] [G loss: 1.259653]\n",
      "epoch:19 step:17907 [D loss: 0.615965, acc.: 64.06%] [G loss: 1.443835]\n",
      "epoch:19 step:17908 [D loss: 0.437731, acc.: 81.25%] [G loss: 1.238227]\n",
      "epoch:19 step:17909 [D loss: 0.598206, acc.: 67.97%] [G loss: 0.979075]\n",
      "epoch:19 step:17910 [D loss: 0.553099, acc.: 75.78%] [G loss: 1.204128]\n",
      "epoch:19 step:17911 [D loss: 0.595224, acc.: 67.97%] [G loss: 1.224098]\n",
      "epoch:19 step:17912 [D loss: 0.618707, acc.: 67.19%] [G loss: 1.144433]\n",
      "epoch:19 step:17913 [D loss: 0.661633, acc.: 65.62%] [G loss: 1.277107]\n",
      "epoch:19 step:17914 [D loss: 0.443887, acc.: 79.69%] [G loss: 1.436945]\n",
      "epoch:19 step:17915 [D loss: 0.676366, acc.: 64.06%] [G loss: 1.163829]\n",
      "epoch:19 step:17916 [D loss: 0.548172, acc.: 73.44%] [G loss: 1.280410]\n",
      "epoch:19 step:17917 [D loss: 0.533550, acc.: 78.12%] [G loss: 1.336261]\n",
      "epoch:19 step:17918 [D loss: 0.619300, acc.: 71.09%] [G loss: 1.020300]\n",
      "epoch:19 step:17919 [D loss: 0.709527, acc.: 57.81%] [G loss: 1.068196]\n",
      "epoch:19 step:17920 [D loss: 0.497632, acc.: 76.56%] [G loss: 1.767180]\n",
      "epoch:19 step:17921 [D loss: 0.584456, acc.: 64.06%] [G loss: 1.257464]\n",
      "epoch:19 step:17922 [D loss: 0.539961, acc.: 71.88%] [G loss: 1.697382]\n",
      "epoch:19 step:17923 [D loss: 0.641481, acc.: 60.94%] [G loss: 1.119228]\n",
      "epoch:19 step:17924 [D loss: 0.470930, acc.: 80.47%] [G loss: 1.362162]\n",
      "epoch:19 step:17925 [D loss: 0.566085, acc.: 67.97%] [G loss: 1.014626]\n",
      "epoch:19 step:17926 [D loss: 0.528914, acc.: 72.66%] [G loss: 1.592004]\n",
      "epoch:19 step:17927 [D loss: 0.609441, acc.: 67.97%] [G loss: 1.257598]\n",
      "epoch:19 step:17928 [D loss: 0.648614, acc.: 63.28%] [G loss: 1.204259]\n",
      "epoch:19 step:17929 [D loss: 0.667902, acc.: 60.94%] [G loss: 1.121515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17930 [D loss: 0.606825, acc.: 64.06%] [G loss: 1.426905]\n",
      "epoch:19 step:17931 [D loss: 0.667969, acc.: 67.19%] [G loss: 1.150054]\n",
      "epoch:19 step:17932 [D loss: 0.679321, acc.: 62.50%] [G loss: 1.091054]\n",
      "epoch:19 step:17933 [D loss: 0.386793, acc.: 89.06%] [G loss: 2.004033]\n",
      "epoch:19 step:17934 [D loss: 0.644611, acc.: 60.16%] [G loss: 1.029456]\n",
      "epoch:19 step:17935 [D loss: 0.766991, acc.: 47.66%] [G loss: 1.137999]\n",
      "epoch:19 step:17936 [D loss: 0.615176, acc.: 69.53%] [G loss: 1.120721]\n",
      "epoch:19 step:17937 [D loss: 0.596622, acc.: 65.62%] [G loss: 1.399730]\n",
      "epoch:19 step:17938 [D loss: 0.702172, acc.: 56.25%] [G loss: 1.264907]\n",
      "epoch:19 step:17939 [D loss: 0.813791, acc.: 56.25%] [G loss: 0.888950]\n",
      "epoch:19 step:17940 [D loss: 0.611638, acc.: 68.75%] [G loss: 1.271356]\n",
      "epoch:19 step:17941 [D loss: 0.554468, acc.: 75.00%] [G loss: 1.628567]\n",
      "epoch:19 step:17942 [D loss: 0.612984, acc.: 64.84%] [G loss: 1.526665]\n",
      "epoch:19 step:17943 [D loss: 0.737372, acc.: 57.81%] [G loss: 1.388458]\n",
      "epoch:19 step:17944 [D loss: 0.704185, acc.: 56.25%] [G loss: 1.456774]\n",
      "epoch:19 step:17945 [D loss: 0.573042, acc.: 70.31%] [G loss: 1.324522]\n",
      "epoch:19 step:17946 [D loss: 0.424799, acc.: 83.59%] [G loss: 1.628072]\n",
      "epoch:19 step:17947 [D loss: 0.626267, acc.: 66.41%] [G loss: 1.255454]\n",
      "epoch:19 step:17948 [D loss: 0.485251, acc.: 74.22%] [G loss: 1.215330]\n",
      "epoch:19 step:17949 [D loss: 0.539326, acc.: 76.56%] [G loss: 1.223567]\n",
      "epoch:19 step:17950 [D loss: 0.558758, acc.: 70.31%] [G loss: 1.662207]\n",
      "epoch:19 step:17951 [D loss: 0.609050, acc.: 64.06%] [G loss: 1.264701]\n",
      "epoch:19 step:17952 [D loss: 0.634790, acc.: 60.94%] [G loss: 1.285878]\n",
      "epoch:19 step:17953 [D loss: 0.594316, acc.: 69.53%] [G loss: 1.190436]\n",
      "epoch:19 step:17954 [D loss: 0.512010, acc.: 71.88%] [G loss: 1.448278]\n",
      "epoch:19 step:17955 [D loss: 0.662077, acc.: 64.84%] [G loss: 1.141706]\n",
      "epoch:19 step:17956 [D loss: 0.447613, acc.: 83.59%] [G loss: 1.047922]\n",
      "epoch:19 step:17957 [D loss: 0.518546, acc.: 71.88%] [G loss: 1.170763]\n",
      "epoch:19 step:17958 [D loss: 0.618138, acc.: 65.62%] [G loss: 1.141790]\n",
      "epoch:19 step:17959 [D loss: 0.580216, acc.: 75.00%] [G loss: 1.058566]\n",
      "epoch:19 step:17960 [D loss: 0.622750, acc.: 64.06%] [G loss: 1.617481]\n",
      "epoch:19 step:17961 [D loss: 0.739465, acc.: 60.16%] [G loss: 1.317235]\n",
      "epoch:19 step:17962 [D loss: 0.459984, acc.: 77.34%] [G loss: 1.871905]\n",
      "epoch:19 step:17963 [D loss: 0.803327, acc.: 53.12%] [G loss: 1.172967]\n",
      "epoch:19 step:17964 [D loss: 0.524192, acc.: 75.00%] [G loss: 1.544440]\n",
      "epoch:19 step:17965 [D loss: 0.527390, acc.: 76.56%] [G loss: 1.494005]\n",
      "epoch:19 step:17966 [D loss: 0.585003, acc.: 67.19%] [G loss: 0.971068]\n",
      "epoch:19 step:17967 [D loss: 0.496053, acc.: 78.91%] [G loss: 1.271754]\n",
      "epoch:19 step:17968 [D loss: 0.624057, acc.: 64.06%] [G loss: 1.478662]\n",
      "epoch:19 step:17969 [D loss: 0.528711, acc.: 71.09%] [G loss: 1.324663]\n",
      "epoch:19 step:17970 [D loss: 0.559743, acc.: 67.19%] [G loss: 1.187047]\n",
      "epoch:19 step:17971 [D loss: 0.474148, acc.: 78.12%] [G loss: 1.634525]\n",
      "epoch:19 step:17972 [D loss: 0.584952, acc.: 70.31%] [G loss: 1.129531]\n",
      "epoch:19 step:17973 [D loss: 0.616169, acc.: 61.72%] [G loss: 1.031779]\n",
      "epoch:19 step:17974 [D loss: 0.537182, acc.: 71.09%] [G loss: 1.331989]\n",
      "epoch:19 step:17975 [D loss: 0.612218, acc.: 70.31%] [G loss: 1.319478]\n",
      "epoch:19 step:17976 [D loss: 0.685697, acc.: 59.38%] [G loss: 1.129418]\n",
      "epoch:19 step:17977 [D loss: 0.525675, acc.: 78.12%] [G loss: 1.091531]\n",
      "epoch:19 step:17978 [D loss: 0.540618, acc.: 74.22%] [G loss: 1.340283]\n",
      "epoch:19 step:17979 [D loss: 0.554307, acc.: 73.44%] [G loss: 1.265075]\n",
      "epoch:19 step:17980 [D loss: 0.495414, acc.: 78.91%] [G loss: 1.841305]\n",
      "epoch:19 step:17981 [D loss: 0.574479, acc.: 66.41%] [G loss: 1.460146]\n",
      "epoch:19 step:17982 [D loss: 0.551077, acc.: 72.66%] [G loss: 1.305282]\n",
      "epoch:19 step:17983 [D loss: 0.474082, acc.: 75.00%] [G loss: 1.041337]\n",
      "epoch:19 step:17984 [D loss: 0.575434, acc.: 70.31%] [G loss: 1.320136]\n",
      "epoch:19 step:17985 [D loss: 0.538686, acc.: 73.44%] [G loss: 1.323397]\n",
      "epoch:19 step:17986 [D loss: 0.615303, acc.: 65.62%] [G loss: 1.149512]\n",
      "epoch:19 step:17987 [D loss: 0.598485, acc.: 68.75%] [G loss: 0.876557]\n",
      "epoch:19 step:17988 [D loss: 0.487653, acc.: 77.34%] [G loss: 1.557818]\n",
      "epoch:19 step:17989 [D loss: 0.548135, acc.: 75.00%] [G loss: 1.487035]\n",
      "epoch:19 step:17990 [D loss: 0.555336, acc.: 71.88%] [G loss: 1.483414]\n",
      "epoch:19 step:17991 [D loss: 0.671487, acc.: 61.72%] [G loss: 1.329665]\n",
      "epoch:19 step:17992 [D loss: 0.611416, acc.: 65.62%] [G loss: 1.050575]\n",
      "epoch:19 step:17993 [D loss: 0.528691, acc.: 77.34%] [G loss: 1.227020]\n",
      "epoch:19 step:17994 [D loss: 0.544041, acc.: 71.88%] [G loss: 1.228373]\n",
      "epoch:19 step:17995 [D loss: 0.512970, acc.: 78.91%] [G loss: 1.333218]\n",
      "epoch:19 step:17996 [D loss: 0.605116, acc.: 68.75%] [G loss: 1.271958]\n",
      "epoch:19 step:17997 [D loss: 0.518052, acc.: 75.00%] [G loss: 1.326011]\n",
      "epoch:19 step:17998 [D loss: 0.620068, acc.: 64.06%] [G loss: 1.444043]\n",
      "epoch:19 step:17999 [D loss: 0.525638, acc.: 77.34%] [G loss: 1.429299]\n",
      "epoch:19 step:18000 [D loss: 0.545510, acc.: 78.12%] [G loss: 1.185176]\n",
      "##############\n",
      "[2.69335808 2.05077001 2.00453485 2.62381396 0.97742272 5.56125864\n",
      " 2.18936078 2.93145857 3.89121926 4.40437425]\n",
      "##########\n",
      "epoch:19 step:18001 [D loss: 0.461166, acc.: 74.22%] [G loss: 1.409024]\n",
      "epoch:19 step:18002 [D loss: 0.492908, acc.: 77.34%] [G loss: 1.366860]\n",
      "epoch:19 step:18003 [D loss: 0.521789, acc.: 79.69%] [G loss: 1.499258]\n",
      "epoch:19 step:18004 [D loss: 0.440707, acc.: 81.25%] [G loss: 1.223636]\n",
      "epoch:19 step:18005 [D loss: 0.747415, acc.: 52.34%] [G loss: 1.136892]\n",
      "epoch:19 step:18006 [D loss: 0.413960, acc.: 85.16%] [G loss: 1.224301]\n",
      "epoch:19 step:18007 [D loss: 0.531600, acc.: 73.44%] [G loss: 1.377847]\n",
      "epoch:19 step:18008 [D loss: 0.685316, acc.: 59.38%] [G loss: 1.426355]\n",
      "epoch:19 step:18009 [D loss: 0.564354, acc.: 72.66%] [G loss: 1.580766]\n",
      "epoch:19 step:18010 [D loss: 0.728636, acc.: 53.12%] [G loss: 1.195819]\n",
      "epoch:19 step:18011 [D loss: 0.501713, acc.: 80.47%] [G loss: 1.301169]\n",
      "epoch:19 step:18012 [D loss: 0.603171, acc.: 65.62%] [G loss: 1.383338]\n",
      "epoch:19 step:18013 [D loss: 0.614757, acc.: 66.41%] [G loss: 1.331910]\n",
      "epoch:19 step:18014 [D loss: 0.568651, acc.: 67.19%] [G loss: 1.138506]\n",
      "epoch:19 step:18015 [D loss: 0.615962, acc.: 67.97%] [G loss: 1.371769]\n",
      "epoch:19 step:18016 [D loss: 0.599552, acc.: 64.84%] [G loss: 1.545590]\n",
      "epoch:19 step:18017 [D loss: 0.688850, acc.: 57.03%] [G loss: 1.210757]\n",
      "epoch:19 step:18018 [D loss: 0.676963, acc.: 64.06%] [G loss: 1.089135]\n",
      "epoch:19 step:18019 [D loss: 0.544975, acc.: 75.00%] [G loss: 1.216744]\n",
      "epoch:19 step:18020 [D loss: 0.503101, acc.: 77.34%] [G loss: 1.384169]\n",
      "epoch:19 step:18021 [D loss: 0.621987, acc.: 64.84%] [G loss: 1.196833]\n",
      "epoch:19 step:18022 [D loss: 0.551958, acc.: 73.44%] [G loss: 1.701457]\n",
      "epoch:19 step:18023 [D loss: 0.497077, acc.: 76.56%] [G loss: 1.549486]\n",
      "epoch:19 step:18024 [D loss: 0.517140, acc.: 75.78%] [G loss: 1.591477]\n",
      "epoch:19 step:18025 [D loss: 0.728010, acc.: 53.91%] [G loss: 1.258836]\n",
      "epoch:19 step:18026 [D loss: 0.592416, acc.: 73.44%] [G loss: 1.302231]\n",
      "epoch:19 step:18027 [D loss: 0.563839, acc.: 72.66%] [G loss: 1.370698]\n",
      "epoch:19 step:18028 [D loss: 0.622780, acc.: 66.41%] [G loss: 1.304292]\n",
      "epoch:19 step:18029 [D loss: 0.476817, acc.: 78.91%] [G loss: 1.483456]\n",
      "epoch:19 step:18030 [D loss: 0.617264, acc.: 66.41%] [G loss: 1.291354]\n",
      "epoch:19 step:18031 [D loss: 0.589856, acc.: 71.09%] [G loss: 1.233310]\n",
      "epoch:19 step:18032 [D loss: 0.409825, acc.: 86.72%] [G loss: 1.461872]\n",
      "epoch:19 step:18033 [D loss: 0.667569, acc.: 57.81%] [G loss: 1.442505]\n",
      "epoch:19 step:18034 [D loss: 0.471955, acc.: 83.59%] [G loss: 1.502545]\n",
      "epoch:19 step:18035 [D loss: 0.716152, acc.: 56.25%] [G loss: 1.277832]\n",
      "epoch:19 step:18036 [D loss: 0.774213, acc.: 52.34%] [G loss: 1.447138]\n",
      "epoch:19 step:18037 [D loss: 0.753781, acc.: 53.12%] [G loss: 0.990593]\n",
      "epoch:19 step:18038 [D loss: 0.621795, acc.: 62.50%] [G loss: 1.247208]\n",
      "epoch:19 step:18039 [D loss: 0.545155, acc.: 76.56%] [G loss: 1.450278]\n",
      "epoch:19 step:18040 [D loss: 0.696751, acc.: 54.69%] [G loss: 1.295021]\n",
      "epoch:19 step:18041 [D loss: 0.643417, acc.: 63.28%] [G loss: 1.193610]\n",
      "epoch:19 step:18042 [D loss: 0.754429, acc.: 59.38%] [G loss: 1.047646]\n",
      "epoch:19 step:18043 [D loss: 0.482213, acc.: 78.12%] [G loss: 1.235916]\n",
      "epoch:19 step:18044 [D loss: 0.413746, acc.: 80.47%] [G loss: 1.568873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18045 [D loss: 0.681233, acc.: 58.59%] [G loss: 0.939783]\n",
      "epoch:19 step:18046 [D loss: 0.616524, acc.: 66.41%] [G loss: 0.905486]\n",
      "epoch:19 step:18047 [D loss: 0.663779, acc.: 60.16%] [G loss: 1.190538]\n",
      "epoch:19 step:18048 [D loss: 0.538524, acc.: 77.34%] [G loss: 1.245744]\n",
      "epoch:19 step:18049 [D loss: 0.484534, acc.: 74.22%] [G loss: 1.194074]\n",
      "epoch:19 step:18050 [D loss: 0.660338, acc.: 65.62%] [G loss: 1.373510]\n",
      "epoch:19 step:18051 [D loss: 0.650730, acc.: 64.06%] [G loss: 1.104045]\n",
      "epoch:19 step:18052 [D loss: 0.455684, acc.: 82.03%] [G loss: 1.480347]\n",
      "epoch:19 step:18053 [D loss: 0.537194, acc.: 72.66%] [G loss: 1.279978]\n",
      "epoch:19 step:18054 [D loss: 0.684676, acc.: 64.06%] [G loss: 1.281979]\n",
      "epoch:19 step:18055 [D loss: 0.403670, acc.: 86.72%] [G loss: 1.647518]\n",
      "epoch:19 step:18056 [D loss: 0.661727, acc.: 60.94%] [G loss: 1.328777]\n",
      "epoch:19 step:18057 [D loss: 0.505293, acc.: 75.78%] [G loss: 1.506623]\n",
      "epoch:19 step:18058 [D loss: 0.608860, acc.: 68.75%] [G loss: 1.057245]\n",
      "epoch:19 step:18059 [D loss: 0.739908, acc.: 53.91%] [G loss: 1.016972]\n",
      "epoch:19 step:18060 [D loss: 0.540376, acc.: 70.31%] [G loss: 1.468298]\n",
      "epoch:19 step:18061 [D loss: 0.550518, acc.: 74.22%] [G loss: 1.415991]\n",
      "epoch:19 step:18062 [D loss: 0.754795, acc.: 53.12%] [G loss: 1.081471]\n",
      "epoch:19 step:18063 [D loss: 0.646427, acc.: 65.62%] [G loss: 1.336116]\n",
      "epoch:19 step:18064 [D loss: 0.565501, acc.: 73.44%] [G loss: 1.644391]\n",
      "epoch:19 step:18065 [D loss: 0.701085, acc.: 59.38%] [G loss: 1.188634]\n",
      "epoch:19 step:18066 [D loss: 0.607012, acc.: 64.84%] [G loss: 1.296636]\n",
      "epoch:19 step:18067 [D loss: 0.507857, acc.: 73.44%] [G loss: 1.897409]\n",
      "epoch:19 step:18068 [D loss: 0.568182, acc.: 67.19%] [G loss: 1.149851]\n",
      "epoch:19 step:18069 [D loss: 0.559514, acc.: 71.88%] [G loss: 1.319172]\n",
      "epoch:19 step:18070 [D loss: 0.571704, acc.: 64.06%] [G loss: 1.369660]\n",
      "epoch:19 step:18071 [D loss: 0.520952, acc.: 69.53%] [G loss: 1.501788]\n",
      "epoch:19 step:18072 [D loss: 0.650401, acc.: 62.50%] [G loss: 1.459752]\n",
      "epoch:19 step:18073 [D loss: 0.718557, acc.: 56.25%] [G loss: 1.023823]\n",
      "epoch:19 step:18074 [D loss: 0.529187, acc.: 71.09%] [G loss: 1.186134]\n",
      "epoch:19 step:18075 [D loss: 0.608329, acc.: 66.41%] [G loss: 1.301445]\n",
      "epoch:19 step:18076 [D loss: 0.627913, acc.: 65.62%] [G loss: 1.273487]\n",
      "epoch:19 step:18077 [D loss: 0.555744, acc.: 67.97%] [G loss: 1.095920]\n",
      "epoch:19 step:18078 [D loss: 0.653939, acc.: 57.81%] [G loss: 1.365752]\n",
      "epoch:19 step:18079 [D loss: 0.691309, acc.: 64.06%] [G loss: 1.272551]\n",
      "epoch:19 step:18080 [D loss: 0.571559, acc.: 70.31%] [G loss: 1.193848]\n",
      "epoch:19 step:18081 [D loss: 0.616049, acc.: 62.50%] [G loss: 1.367851]\n",
      "epoch:19 step:18082 [D loss: 0.651558, acc.: 57.03%] [G loss: 1.459939]\n",
      "epoch:19 step:18083 [D loss: 0.564455, acc.: 69.53%] [G loss: 1.335491]\n",
      "epoch:19 step:18084 [D loss: 0.547222, acc.: 75.78%] [G loss: 1.204068]\n",
      "epoch:19 step:18085 [D loss: 0.539527, acc.: 71.09%] [G loss: 1.213470]\n",
      "epoch:19 step:18086 [D loss: 0.483837, acc.: 78.91%] [G loss: 1.031015]\n",
      "epoch:19 step:18087 [D loss: 0.607908, acc.: 66.41%] [G loss: 1.028604]\n",
      "epoch:19 step:18088 [D loss: 0.521002, acc.: 75.78%] [G loss: 1.588745]\n",
      "epoch:19 step:18089 [D loss: 0.773044, acc.: 51.56%] [G loss: 1.014339]\n",
      "epoch:19 step:18090 [D loss: 0.408114, acc.: 85.94%] [G loss: 1.521130]\n",
      "epoch:19 step:18091 [D loss: 0.532005, acc.: 75.00%] [G loss: 1.345902]\n",
      "epoch:19 step:18092 [D loss: 0.735329, acc.: 57.03%] [G loss: 1.231561]\n",
      "epoch:19 step:18093 [D loss: 0.428426, acc.: 85.94%] [G loss: 1.093493]\n",
      "epoch:19 step:18094 [D loss: 0.559812, acc.: 70.31%] [G loss: 1.218944]\n",
      "epoch:19 step:18095 [D loss: 0.554996, acc.: 72.66%] [G loss: 1.512610]\n",
      "epoch:19 step:18096 [D loss: 0.680947, acc.: 63.28%] [G loss: 1.126682]\n",
      "epoch:19 step:18097 [D loss: 0.700587, acc.: 59.38%] [G loss: 1.146792]\n",
      "epoch:19 step:18098 [D loss: 0.658108, acc.: 59.38%] [G loss: 1.205383]\n",
      "epoch:19 step:18099 [D loss: 0.548954, acc.: 71.88%] [G loss: 1.110336]\n",
      "epoch:19 step:18100 [D loss: 0.535430, acc.: 69.53%] [G loss: 1.489181]\n",
      "epoch:19 step:18101 [D loss: 0.643191, acc.: 66.41%] [G loss: 1.574748]\n",
      "epoch:19 step:18102 [D loss: 0.615480, acc.: 67.97%] [G loss: 0.783183]\n",
      "epoch:19 step:18103 [D loss: 0.443901, acc.: 82.81%] [G loss: 1.386657]\n",
      "epoch:19 step:18104 [D loss: 0.534637, acc.: 75.00%] [G loss: 1.024925]\n",
      "epoch:19 step:18105 [D loss: 0.559882, acc.: 75.78%] [G loss: 1.148050]\n",
      "epoch:19 step:18106 [D loss: 0.587514, acc.: 70.31%] [G loss: 1.074515]\n",
      "epoch:19 step:18107 [D loss: 0.579578, acc.: 70.31%] [G loss: 1.553832]\n",
      "epoch:19 step:18108 [D loss: 0.500929, acc.: 80.47%] [G loss: 1.224843]\n",
      "epoch:19 step:18109 [D loss: 0.587839, acc.: 64.84%] [G loss: 1.258763]\n",
      "epoch:19 step:18110 [D loss: 0.541647, acc.: 77.34%] [G loss: 1.496651]\n",
      "epoch:19 step:18111 [D loss: 0.524782, acc.: 68.75%] [G loss: 1.073038]\n",
      "epoch:19 step:18112 [D loss: 0.551396, acc.: 71.88%] [G loss: 1.464257]\n",
      "epoch:19 step:18113 [D loss: 0.549079, acc.: 71.09%] [G loss: 1.157305]\n",
      "epoch:19 step:18114 [D loss: 0.444994, acc.: 81.25%] [G loss: 1.674681]\n",
      "epoch:19 step:18115 [D loss: 0.785038, acc.: 50.00%] [G loss: 1.078974]\n",
      "epoch:19 step:18116 [D loss: 0.645550, acc.: 64.06%] [G loss: 1.161024]\n",
      "epoch:19 step:18117 [D loss: 0.524002, acc.: 69.53%] [G loss: 1.183793]\n",
      "epoch:19 step:18118 [D loss: 0.656374, acc.: 60.94%] [G loss: 1.221726]\n",
      "epoch:19 step:18119 [D loss: 0.767157, acc.: 55.47%] [G loss: 1.219627]\n",
      "epoch:19 step:18120 [D loss: 0.529586, acc.: 75.00%] [G loss: 1.327261]\n",
      "epoch:19 step:18121 [D loss: 0.649860, acc.: 63.28%] [G loss: 1.522882]\n",
      "epoch:19 step:18122 [D loss: 0.610486, acc.: 68.75%] [G loss: 1.423159]\n",
      "epoch:19 step:18123 [D loss: 0.614682, acc.: 64.06%] [G loss: 1.602223]\n",
      "epoch:19 step:18124 [D loss: 0.614776, acc.: 64.84%] [G loss: 1.371665]\n",
      "epoch:19 step:18125 [D loss: 0.445391, acc.: 82.03%] [G loss: 1.386284]\n",
      "epoch:19 step:18126 [D loss: 0.539077, acc.: 73.44%] [G loss: 1.301090]\n",
      "epoch:19 step:18127 [D loss: 0.619438, acc.: 62.50%] [G loss: 1.196627]\n",
      "epoch:19 step:18128 [D loss: 0.567486, acc.: 70.31%] [G loss: 1.084023]\n",
      "epoch:19 step:18129 [D loss: 0.537632, acc.: 71.09%] [G loss: 1.176229]\n",
      "epoch:19 step:18130 [D loss: 0.584511, acc.: 66.41%] [G loss: 1.440502]\n",
      "epoch:19 step:18131 [D loss: 0.563305, acc.: 71.88%] [G loss: 1.377799]\n",
      "epoch:19 step:18132 [D loss: 0.539668, acc.: 71.09%] [G loss: 1.286960]\n",
      "epoch:19 step:18133 [D loss: 0.563869, acc.: 71.09%] [G loss: 1.171905]\n",
      "epoch:19 step:18134 [D loss: 0.592141, acc.: 67.97%] [G loss: 1.211819]\n",
      "epoch:19 step:18135 [D loss: 0.538842, acc.: 74.22%] [G loss: 1.323764]\n",
      "epoch:19 step:18136 [D loss: 0.722122, acc.: 57.03%] [G loss: 1.299019]\n",
      "epoch:19 step:18137 [D loss: 0.607450, acc.: 67.97%] [G loss: 1.551917]\n",
      "epoch:19 step:18138 [D loss: 0.704867, acc.: 57.81%] [G loss: 1.665045]\n",
      "epoch:19 step:18139 [D loss: 0.491486, acc.: 77.34%] [G loss: 1.168017]\n",
      "epoch:19 step:18140 [D loss: 0.764359, acc.: 55.47%] [G loss: 1.188655]\n",
      "epoch:19 step:18141 [D loss: 0.541030, acc.: 76.56%] [G loss: 1.621690]\n",
      "epoch:19 step:18142 [D loss: 0.578763, acc.: 71.09%] [G loss: 1.715223]\n",
      "epoch:19 step:18143 [D loss: 0.612480, acc.: 66.41%] [G loss: 1.318363]\n",
      "epoch:19 step:18144 [D loss: 0.573218, acc.: 73.44%] [G loss: 1.173074]\n",
      "epoch:19 step:18145 [D loss: 0.624276, acc.: 61.72%] [G loss: 1.599930]\n",
      "epoch:19 step:18146 [D loss: 0.578943, acc.: 72.66%] [G loss: 1.137399]\n",
      "epoch:19 step:18147 [D loss: 0.526184, acc.: 74.22%] [G loss: 1.266719]\n",
      "epoch:19 step:18148 [D loss: 0.535267, acc.: 73.44%] [G loss: 1.233498]\n",
      "epoch:19 step:18149 [D loss: 0.672943, acc.: 60.16%] [G loss: 1.299953]\n",
      "epoch:19 step:18150 [D loss: 0.593930, acc.: 69.53%] [G loss: 1.257372]\n",
      "epoch:19 step:18151 [D loss: 0.414930, acc.: 82.81%] [G loss: 1.347993]\n",
      "epoch:19 step:18152 [D loss: 0.496636, acc.: 75.78%] [G loss: 1.084813]\n",
      "epoch:19 step:18153 [D loss: 0.481895, acc.: 80.47%] [G loss: 1.305100]\n",
      "epoch:19 step:18154 [D loss: 0.510176, acc.: 78.91%] [G loss: 1.328925]\n",
      "epoch:19 step:18155 [D loss: 0.530913, acc.: 76.56%] [G loss: 1.325824]\n",
      "epoch:19 step:18156 [D loss: 0.563999, acc.: 70.31%] [G loss: 1.048154]\n",
      "epoch:19 step:18157 [D loss: 0.555410, acc.: 70.31%] [G loss: 1.227386]\n",
      "epoch:19 step:18158 [D loss: 0.467103, acc.: 78.91%] [G loss: 1.476144]\n",
      "epoch:19 step:18159 [D loss: 0.539374, acc.: 72.66%] [G loss: 1.197859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18160 [D loss: 0.491750, acc.: 79.69%] [G loss: 1.382902]\n",
      "epoch:19 step:18161 [D loss: 0.536483, acc.: 73.44%] [G loss: 1.673240]\n",
      "epoch:19 step:18162 [D loss: 0.557110, acc.: 71.09%] [G loss: 1.389648]\n",
      "epoch:19 step:18163 [D loss: 0.533254, acc.: 78.12%] [G loss: 1.391581]\n",
      "epoch:19 step:18164 [D loss: 0.625520, acc.: 67.19%] [G loss: 1.460365]\n",
      "epoch:19 step:18165 [D loss: 0.541189, acc.: 70.31%] [G loss: 1.488192]\n",
      "epoch:19 step:18166 [D loss: 0.482170, acc.: 82.03%] [G loss: 1.278649]\n",
      "epoch:19 step:18167 [D loss: 0.488334, acc.: 80.47%] [G loss: 1.421820]\n",
      "epoch:19 step:18168 [D loss: 0.535217, acc.: 71.88%] [G loss: 1.187471]\n",
      "epoch:19 step:18169 [D loss: 0.520357, acc.: 72.66%] [G loss: 1.554302]\n",
      "epoch:19 step:18170 [D loss: 0.633224, acc.: 60.94%] [G loss: 1.258946]\n",
      "epoch:19 step:18171 [D loss: 0.428515, acc.: 85.16%] [G loss: 1.330345]\n",
      "epoch:19 step:18172 [D loss: 0.552175, acc.: 76.56%] [G loss: 1.784927]\n",
      "epoch:19 step:18173 [D loss: 0.701547, acc.: 59.38%] [G loss: 1.134232]\n",
      "epoch:19 step:18174 [D loss: 0.610646, acc.: 67.19%] [G loss: 1.510031]\n",
      "epoch:19 step:18175 [D loss: 0.575679, acc.: 67.97%] [G loss: 1.822462]\n",
      "epoch:19 step:18176 [D loss: 0.432563, acc.: 82.03%] [G loss: 1.336837]\n",
      "epoch:19 step:18177 [D loss: 0.808494, acc.: 50.78%] [G loss: 1.244241]\n",
      "epoch:19 step:18178 [D loss: 0.632827, acc.: 68.75%] [G loss: 1.247396]\n",
      "epoch:19 step:18179 [D loss: 0.674399, acc.: 61.72%] [G loss: 1.155503]\n",
      "epoch:19 step:18180 [D loss: 0.563738, acc.: 71.88%] [G loss: 1.097744]\n",
      "epoch:19 step:18181 [D loss: 0.631549, acc.: 62.50%] [G loss: 0.794157]\n",
      "epoch:19 step:18182 [D loss: 0.519711, acc.: 75.78%] [G loss: 1.366326]\n",
      "epoch:19 step:18183 [D loss: 0.491011, acc.: 75.78%] [G loss: 1.499059]\n",
      "epoch:19 step:18184 [D loss: 0.686193, acc.: 53.12%] [G loss: 1.288086]\n",
      "epoch:19 step:18185 [D loss: 0.543217, acc.: 71.88%] [G loss: 1.341761]\n",
      "epoch:19 step:18186 [D loss: 0.634666, acc.: 70.31%] [G loss: 1.278728]\n",
      "epoch:19 step:18187 [D loss: 0.664536, acc.: 60.94%] [G loss: 1.147479]\n",
      "epoch:19 step:18188 [D loss: 0.594476, acc.: 69.53%] [G loss: 1.214037]\n",
      "epoch:19 step:18189 [D loss: 0.620121, acc.: 64.06%] [G loss: 1.370436]\n",
      "epoch:19 step:18190 [D loss: 0.535007, acc.: 76.56%] [G loss: 1.357978]\n",
      "epoch:19 step:18191 [D loss: 0.516065, acc.: 78.91%] [G loss: 1.187658]\n",
      "epoch:19 step:18192 [D loss: 0.463200, acc.: 77.34%] [G loss: 1.317227]\n",
      "epoch:19 step:18193 [D loss: 0.535741, acc.: 73.44%] [G loss: 0.928662]\n",
      "epoch:19 step:18194 [D loss: 0.737859, acc.: 56.25%] [G loss: 1.381089]\n",
      "epoch:19 step:18195 [D loss: 0.474327, acc.: 80.47%] [G loss: 1.385908]\n",
      "epoch:19 step:18196 [D loss: 0.638971, acc.: 59.38%] [G loss: 1.413425]\n",
      "epoch:19 step:18197 [D loss: 0.666205, acc.: 64.84%] [G loss: 1.401446]\n",
      "epoch:19 step:18198 [D loss: 0.644269, acc.: 67.97%] [G loss: 1.621117]\n",
      "epoch:19 step:18199 [D loss: 0.519819, acc.: 73.44%] [G loss: 1.608274]\n",
      "epoch:19 step:18200 [D loss: 0.671599, acc.: 57.03%] [G loss: 0.960498]\n",
      "##############\n",
      "[2.76178147 1.97447958 2.18066556 2.9274188  0.87770887 6.08310508\n",
      " 2.27689138 2.89880671 3.86432642 7.14868929]\n",
      "##########\n",
      "epoch:19 step:18201 [D loss: 0.525181, acc.: 72.66%] [G loss: 1.136492]\n",
      "epoch:19 step:18202 [D loss: 0.569673, acc.: 71.09%] [G loss: 1.492728]\n",
      "epoch:19 step:18203 [D loss: 0.578191, acc.: 72.66%] [G loss: 1.200290]\n",
      "epoch:19 step:18204 [D loss: 0.517466, acc.: 77.34%] [G loss: 1.096880]\n",
      "epoch:19 step:18205 [D loss: 0.461978, acc.: 75.78%] [G loss: 1.472843]\n",
      "epoch:19 step:18206 [D loss: 0.498957, acc.: 76.56%] [G loss: 1.078344]\n",
      "epoch:19 step:18207 [D loss: 0.496520, acc.: 78.91%] [G loss: 1.403839]\n",
      "epoch:19 step:18208 [D loss: 0.490552, acc.: 77.34%] [G loss: 1.252293]\n",
      "epoch:19 step:18209 [D loss: 0.589310, acc.: 67.97%] [G loss: 1.506201]\n",
      "epoch:19 step:18210 [D loss: 0.607562, acc.: 63.28%] [G loss: 1.285853]\n",
      "epoch:19 step:18211 [D loss: 0.598712, acc.: 67.97%] [G loss: 1.364360]\n",
      "epoch:19 step:18212 [D loss: 0.578079, acc.: 70.31%] [G loss: 1.400087]\n",
      "epoch:19 step:18213 [D loss: 0.637036, acc.: 64.06%] [G loss: 1.251132]\n",
      "epoch:19 step:18214 [D loss: 0.637172, acc.: 64.84%] [G loss: 1.153126]\n",
      "epoch:19 step:18215 [D loss: 0.616529, acc.: 67.19%] [G loss: 1.254722]\n",
      "epoch:19 step:18216 [D loss: 0.544906, acc.: 69.53%] [G loss: 1.634309]\n",
      "epoch:19 step:18217 [D loss: 0.591283, acc.: 66.41%] [G loss: 0.918397]\n",
      "epoch:19 step:18218 [D loss: 0.635044, acc.: 60.94%] [G loss: 1.256566]\n",
      "epoch:19 step:18219 [D loss: 0.611173, acc.: 63.28%] [G loss: 1.549372]\n",
      "epoch:19 step:18220 [D loss: 0.448289, acc.: 85.16%] [G loss: 1.822191]\n",
      "epoch:19 step:18221 [D loss: 0.559242, acc.: 70.31%] [G loss: 1.506683]\n",
      "epoch:19 step:18222 [D loss: 0.501433, acc.: 81.25%] [G loss: 1.412979]\n",
      "epoch:19 step:18223 [D loss: 0.484083, acc.: 81.25%] [G loss: 1.289244]\n",
      "epoch:19 step:18224 [D loss: 0.514933, acc.: 75.00%] [G loss: 1.093890]\n",
      "epoch:19 step:18225 [D loss: 0.680006, acc.: 60.94%] [G loss: 1.096568]\n",
      "epoch:19 step:18226 [D loss: 0.596555, acc.: 71.09%] [G loss: 1.313895]\n",
      "epoch:19 step:18227 [D loss: 0.566413, acc.: 70.31%] [G loss: 1.625914]\n",
      "epoch:19 step:18228 [D loss: 0.566837, acc.: 69.53%] [G loss: 1.406103]\n",
      "epoch:19 step:18229 [D loss: 0.589109, acc.: 64.06%] [G loss: 1.453971]\n",
      "epoch:19 step:18230 [D loss: 0.566034, acc.: 71.09%] [G loss: 1.280152]\n",
      "epoch:19 step:18231 [D loss: 0.583623, acc.: 69.53%] [G loss: 1.150229]\n",
      "epoch:19 step:18232 [D loss: 0.573879, acc.: 69.53%] [G loss: 1.242301]\n",
      "epoch:19 step:18233 [D loss: 0.641542, acc.: 62.50%] [G loss: 1.658645]\n",
      "epoch:19 step:18234 [D loss: 0.508408, acc.: 75.78%] [G loss: 1.471671]\n",
      "epoch:19 step:18235 [D loss: 0.540557, acc.: 71.88%] [G loss: 1.337058]\n",
      "epoch:19 step:18236 [D loss: 0.587561, acc.: 68.75%] [G loss: 1.248221]\n",
      "epoch:19 step:18237 [D loss: 0.627843, acc.: 67.19%] [G loss: 1.353207]\n",
      "epoch:19 step:18238 [D loss: 0.487760, acc.: 75.78%] [G loss: 1.331933]\n",
      "epoch:19 step:18239 [D loss: 0.573218, acc.: 71.88%] [G loss: 1.422861]\n",
      "epoch:19 step:18240 [D loss: 0.478492, acc.: 80.47%] [G loss: 1.223323]\n",
      "epoch:19 step:18241 [D loss: 0.644281, acc.: 66.41%] [G loss: 0.999910]\n",
      "epoch:19 step:18242 [D loss: 0.492226, acc.: 76.56%] [G loss: 1.354940]\n",
      "epoch:19 step:18243 [D loss: 0.649677, acc.: 69.53%] [G loss: 1.143215]\n",
      "epoch:19 step:18244 [D loss: 0.454642, acc.: 85.94%] [G loss: 0.934718]\n",
      "epoch:19 step:18245 [D loss: 0.759286, acc.: 51.56%] [G loss: 0.968105]\n",
      "epoch:19 step:18246 [D loss: 0.506624, acc.: 77.34%] [G loss: 1.221544]\n",
      "epoch:19 step:18247 [D loss: 0.576345, acc.: 71.88%] [G loss: 1.243388]\n",
      "epoch:19 step:18248 [D loss: 0.415445, acc.: 82.81%] [G loss: 1.423614]\n",
      "epoch:19 step:18249 [D loss: 0.583879, acc.: 65.62%] [G loss: 1.105243]\n",
      "epoch:19 step:18250 [D loss: 0.677537, acc.: 61.72%] [G loss: 1.557148]\n",
      "epoch:19 step:18251 [D loss: 0.577706, acc.: 69.53%] [G loss: 1.742264]\n",
      "epoch:19 step:18252 [D loss: 0.689953, acc.: 57.03%] [G loss: 1.383406]\n",
      "epoch:19 step:18253 [D loss: 0.603935, acc.: 64.84%] [G loss: 1.478476]\n",
      "epoch:19 step:18254 [D loss: 0.565385, acc.: 73.44%] [G loss: 1.621657]\n",
      "epoch:19 step:18255 [D loss: 0.593011, acc.: 68.75%] [G loss: 1.391997]\n",
      "epoch:19 step:18256 [D loss: 0.467283, acc.: 81.25%] [G loss: 1.948258]\n",
      "epoch:19 step:18257 [D loss: 0.489825, acc.: 80.47%] [G loss: 1.543370]\n",
      "epoch:19 step:18258 [D loss: 0.577417, acc.: 66.41%] [G loss: 1.551259]\n",
      "epoch:19 step:18259 [D loss: 0.567876, acc.: 70.31%] [G loss: 1.139476]\n",
      "epoch:19 step:18260 [D loss: 0.495948, acc.: 76.56%] [G loss: 1.161769]\n",
      "epoch:19 step:18261 [D loss: 0.496201, acc.: 78.12%] [G loss: 1.231294]\n",
      "epoch:19 step:18262 [D loss: 0.447520, acc.: 78.91%] [G loss: 1.522553]\n",
      "epoch:19 step:18263 [D loss: 0.772989, acc.: 51.56%] [G loss: 1.055546]\n",
      "epoch:19 step:18264 [D loss: 0.572231, acc.: 69.53%] [G loss: 1.533780]\n",
      "epoch:19 step:18265 [D loss: 0.646704, acc.: 62.50%] [G loss: 1.286065]\n",
      "epoch:19 step:18266 [D loss: 0.602931, acc.: 67.19%] [G loss: 1.525753]\n",
      "epoch:19 step:18267 [D loss: 0.558905, acc.: 71.88%] [G loss: 1.565246]\n",
      "epoch:19 step:18268 [D loss: 0.671223, acc.: 57.81%] [G loss: 1.057361]\n",
      "epoch:19 step:18269 [D loss: 0.398704, acc.: 85.16%] [G loss: 1.569966]\n",
      "epoch:19 step:18270 [D loss: 0.437064, acc.: 81.25%] [G loss: 1.540376]\n",
      "epoch:19 step:18271 [D loss: 0.453999, acc.: 82.03%] [G loss: 1.023135]\n",
      "epoch:19 step:18272 [D loss: 0.650573, acc.: 59.38%] [G loss: 1.549772]\n",
      "epoch:19 step:18273 [D loss: 0.694386, acc.: 57.03%] [G loss: 1.242904]\n",
      "epoch:19 step:18274 [D loss: 0.440870, acc.: 84.38%] [G loss: 1.593409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18275 [D loss: 0.522876, acc.: 69.53%] [G loss: 1.349824]\n",
      "epoch:19 step:18276 [D loss: 0.481551, acc.: 77.34%] [G loss: 1.407374]\n",
      "epoch:19 step:18277 [D loss: 0.688357, acc.: 60.94%] [G loss: 1.418366]\n",
      "epoch:19 step:18278 [D loss: 0.411607, acc.: 85.16%] [G loss: 1.640549]\n",
      "epoch:19 step:18279 [D loss: 0.547477, acc.: 74.22%] [G loss: 1.206941]\n",
      "epoch:19 step:18280 [D loss: 0.584832, acc.: 67.97%] [G loss: 1.338806]\n",
      "epoch:19 step:18281 [D loss: 0.573488, acc.: 69.53%] [G loss: 1.783981]\n",
      "epoch:19 step:18282 [D loss: 0.483886, acc.: 76.56%] [G loss: 1.663322]\n",
      "epoch:19 step:18283 [D loss: 0.582113, acc.: 67.97%] [G loss: 1.075186]\n",
      "epoch:19 step:18284 [D loss: 0.714422, acc.: 57.03%] [G loss: 1.330090]\n",
      "epoch:19 step:18285 [D loss: 0.666745, acc.: 61.72%] [G loss: 1.160876]\n",
      "epoch:19 step:18286 [D loss: 0.653759, acc.: 64.84%] [G loss: 1.360285]\n",
      "epoch:19 step:18287 [D loss: 0.584070, acc.: 71.09%] [G loss: 1.288862]\n",
      "epoch:19 step:18288 [D loss: 0.521368, acc.: 76.56%] [G loss: 1.442595]\n",
      "epoch:19 step:18289 [D loss: 0.649164, acc.: 64.84%] [G loss: 1.191221]\n",
      "epoch:19 step:18290 [D loss: 0.479379, acc.: 81.25%] [G loss: 1.368164]\n",
      "epoch:19 step:18291 [D loss: 0.581969, acc.: 69.53%] [G loss: 1.638776]\n",
      "epoch:19 step:18292 [D loss: 0.477957, acc.: 79.69%] [G loss: 1.434977]\n",
      "epoch:19 step:18293 [D loss: 0.446272, acc.: 79.69%] [G loss: 1.585511]\n",
      "epoch:19 step:18294 [D loss: 0.609097, acc.: 60.94%] [G loss: 1.179144]\n",
      "epoch:19 step:18295 [D loss: 0.558641, acc.: 71.88%] [G loss: 1.200318]\n",
      "epoch:19 step:18296 [D loss: 0.586624, acc.: 69.53%] [G loss: 1.553884]\n",
      "epoch:19 step:18297 [D loss: 0.479620, acc.: 82.81%] [G loss: 1.344288]\n",
      "epoch:19 step:18298 [D loss: 0.486336, acc.: 78.91%] [G loss: 1.656505]\n",
      "epoch:19 step:18299 [D loss: 0.560004, acc.: 69.53%] [G loss: 1.379727]\n",
      "epoch:19 step:18300 [D loss: 0.487189, acc.: 82.81%] [G loss: 1.327489]\n",
      "epoch:19 step:18301 [D loss: 0.537989, acc.: 73.44%] [G loss: 1.185517]\n",
      "epoch:19 step:18302 [D loss: 0.694976, acc.: 61.72%] [G loss: 0.987763]\n",
      "epoch:19 step:18303 [D loss: 0.720644, acc.: 55.47%] [G loss: 1.166325]\n",
      "epoch:19 step:18304 [D loss: 0.455166, acc.: 78.91%] [G loss: 1.358939]\n",
      "epoch:19 step:18305 [D loss: 0.508112, acc.: 77.34%] [G loss: 1.142687]\n",
      "epoch:19 step:18306 [D loss: 0.650549, acc.: 64.06%] [G loss: 0.904741]\n",
      "epoch:19 step:18307 [D loss: 0.553787, acc.: 75.00%] [G loss: 1.184843]\n",
      "epoch:19 step:18308 [D loss: 0.466166, acc.: 77.34%] [G loss: 1.138827]\n",
      "epoch:19 step:18309 [D loss: 0.477395, acc.: 78.12%] [G loss: 1.434991]\n",
      "epoch:19 step:18310 [D loss: 0.665390, acc.: 59.38%] [G loss: 1.250453]\n",
      "epoch:19 step:18311 [D loss: 0.469097, acc.: 80.47%] [G loss: 1.184147]\n",
      "epoch:19 step:18312 [D loss: 0.727625, acc.: 57.03%] [G loss: 1.446594]\n",
      "epoch:19 step:18313 [D loss: 0.662814, acc.: 58.59%] [G loss: 1.050975]\n",
      "epoch:19 step:18314 [D loss: 0.529785, acc.: 72.66%] [G loss: 1.617307]\n",
      "epoch:19 step:18315 [D loss: 0.507001, acc.: 75.00%] [G loss: 1.515512]\n",
      "epoch:19 step:18316 [D loss: 0.560036, acc.: 74.22%] [G loss: 1.267518]\n",
      "epoch:19 step:18317 [D loss: 0.584678, acc.: 67.97%] [G loss: 1.206812]\n",
      "epoch:19 step:18318 [D loss: 0.357680, acc.: 89.84%] [G loss: 1.386039]\n",
      "epoch:19 step:18319 [D loss: 0.668256, acc.: 60.16%] [G loss: 1.360703]\n",
      "epoch:19 step:18320 [D loss: 0.572359, acc.: 70.31%] [G loss: 0.896819]\n",
      "epoch:19 step:18321 [D loss: 0.525342, acc.: 75.00%] [G loss: 1.290460]\n",
      "epoch:19 step:18322 [D loss: 0.461021, acc.: 80.47%] [G loss: 1.155989]\n",
      "epoch:19 step:18323 [D loss: 0.673715, acc.: 58.59%] [G loss: 1.111336]\n",
      "epoch:19 step:18324 [D loss: 0.659362, acc.: 64.06%] [G loss: 1.344429]\n",
      "epoch:19 step:18325 [D loss: 0.633563, acc.: 64.84%] [G loss: 1.185036]\n",
      "epoch:19 step:18326 [D loss: 0.647740, acc.: 64.84%] [G loss: 1.077429]\n",
      "epoch:19 step:18327 [D loss: 0.642065, acc.: 68.75%] [G loss: 0.971341]\n",
      "epoch:19 step:18328 [D loss: 0.530388, acc.: 73.44%] [G loss: 1.665669]\n",
      "epoch:19 step:18329 [D loss: 0.578641, acc.: 73.44%] [G loss: 1.369444]\n",
      "epoch:19 step:18330 [D loss: 0.651132, acc.: 60.16%] [G loss: 1.157914]\n",
      "epoch:19 step:18331 [D loss: 0.528845, acc.: 76.56%] [G loss: 1.041342]\n",
      "epoch:19 step:18332 [D loss: 0.401450, acc.: 82.81%] [G loss: 1.346076]\n",
      "epoch:19 step:18333 [D loss: 0.530213, acc.: 73.44%] [G loss: 1.456794]\n",
      "epoch:19 step:18334 [D loss: 0.549917, acc.: 75.00%] [G loss: 1.070918]\n",
      "epoch:19 step:18335 [D loss: 0.697497, acc.: 60.94%] [G loss: 1.251568]\n",
      "epoch:19 step:18336 [D loss: 0.595138, acc.: 71.88%] [G loss: 1.224463]\n",
      "epoch:19 step:18337 [D loss: 0.692241, acc.: 60.94%] [G loss: 0.943190]\n",
      "epoch:19 step:18338 [D loss: 0.511953, acc.: 71.09%] [G loss: 1.308664]\n",
      "epoch:19 step:18339 [D loss: 0.637430, acc.: 62.50%] [G loss: 1.145787]\n",
      "epoch:19 step:18340 [D loss: 0.624138, acc.: 62.50%] [G loss: 1.199133]\n",
      "epoch:19 step:18341 [D loss: 0.657306, acc.: 63.28%] [G loss: 1.279896]\n",
      "epoch:19 step:18342 [D loss: 0.590369, acc.: 69.53%] [G loss: 1.162027]\n",
      "epoch:19 step:18343 [D loss: 0.463047, acc.: 78.12%] [G loss: 1.480643]\n",
      "epoch:19 step:18344 [D loss: 0.437497, acc.: 85.16%] [G loss: 1.418257]\n",
      "epoch:19 step:18345 [D loss: 0.478118, acc.: 79.69%] [G loss: 0.948588]\n",
      "epoch:19 step:18346 [D loss: 0.523624, acc.: 74.22%] [G loss: 1.402736]\n",
      "epoch:19 step:18347 [D loss: 0.705487, acc.: 63.28%] [G loss: 1.321707]\n",
      "epoch:19 step:18348 [D loss: 0.514738, acc.: 75.78%] [G loss: 1.700734]\n",
      "epoch:19 step:18349 [D loss: 0.623713, acc.: 67.19%] [G loss: 1.469822]\n",
      "epoch:19 step:18350 [D loss: 0.723002, acc.: 51.56%] [G loss: 1.146127]\n",
      "epoch:19 step:18351 [D loss: 0.577235, acc.: 66.41%] [G loss: 1.390688]\n",
      "epoch:19 step:18352 [D loss: 0.543094, acc.: 71.88%] [G loss: 1.276670]\n",
      "epoch:19 step:18353 [D loss: 0.707537, acc.: 53.91%] [G loss: 1.279098]\n",
      "epoch:19 step:18354 [D loss: 0.456792, acc.: 85.16%] [G loss: 1.752878]\n",
      "epoch:19 step:18355 [D loss: 0.687035, acc.: 59.38%] [G loss: 1.046486]\n",
      "epoch:19 step:18356 [D loss: 0.645221, acc.: 63.28%] [G loss: 1.228237]\n",
      "epoch:19 step:18357 [D loss: 0.670340, acc.: 57.81%] [G loss: 1.143628]\n",
      "epoch:19 step:18358 [D loss: 0.531614, acc.: 75.78%] [G loss: 1.232702]\n",
      "epoch:19 step:18359 [D loss: 0.623564, acc.: 64.84%] [G loss: 1.263744]\n",
      "epoch:19 step:18360 [D loss: 0.570772, acc.: 71.88%] [G loss: 1.412059]\n",
      "epoch:19 step:18361 [D loss: 0.510704, acc.: 74.22%] [G loss: 1.318030]\n",
      "epoch:19 step:18362 [D loss: 0.534316, acc.: 72.66%] [G loss: 1.291019]\n",
      "epoch:19 step:18363 [D loss: 0.475072, acc.: 79.69%] [G loss: 1.510953]\n",
      "epoch:19 step:18364 [D loss: 0.504838, acc.: 73.44%] [G loss: 1.296341]\n",
      "epoch:19 step:18365 [D loss: 0.443147, acc.: 82.81%] [G loss: 1.031608]\n",
      "epoch:19 step:18366 [D loss: 0.514164, acc.: 81.25%] [G loss: 1.287611]\n",
      "epoch:19 step:18367 [D loss: 0.570511, acc.: 71.88%] [G loss: 1.134192]\n",
      "epoch:19 step:18368 [D loss: 0.598567, acc.: 71.09%] [G loss: 1.462371]\n",
      "epoch:19 step:18369 [D loss: 0.562633, acc.: 71.09%] [G loss: 1.229789]\n",
      "epoch:19 step:18370 [D loss: 0.586848, acc.: 74.22%] [G loss: 1.420114]\n",
      "epoch:19 step:18371 [D loss: 0.548952, acc.: 68.75%] [G loss: 1.282912]\n",
      "epoch:19 step:18372 [D loss: 0.662957, acc.: 63.28%] [G loss: 0.962977]\n",
      "epoch:19 step:18373 [D loss: 0.527021, acc.: 71.09%] [G loss: 0.975458]\n",
      "epoch:19 step:18374 [D loss: 0.492344, acc.: 77.34%] [G loss: 1.469489]\n",
      "epoch:19 step:18375 [D loss: 0.524209, acc.: 72.66%] [G loss: 1.360713]\n",
      "epoch:19 step:18376 [D loss: 0.659456, acc.: 60.94%] [G loss: 1.621198]\n",
      "epoch:19 step:18377 [D loss: 0.692254, acc.: 52.34%] [G loss: 1.027597]\n",
      "epoch:19 step:18378 [D loss: 0.470345, acc.: 78.12%] [G loss: 1.047153]\n",
      "epoch:19 step:18379 [D loss: 0.508055, acc.: 77.34%] [G loss: 1.615760]\n",
      "epoch:19 step:18380 [D loss: 0.647779, acc.: 63.28%] [G loss: 1.026170]\n",
      "epoch:19 step:18381 [D loss: 0.717630, acc.: 55.47%] [G loss: 1.125365]\n",
      "epoch:19 step:18382 [D loss: 0.504980, acc.: 75.00%] [G loss: 1.617019]\n",
      "epoch:19 step:18383 [D loss: 0.609775, acc.: 68.75%] [G loss: 1.166286]\n",
      "epoch:19 step:18384 [D loss: 0.634770, acc.: 61.72%] [G loss: 1.348636]\n",
      "epoch:19 step:18385 [D loss: 0.673161, acc.: 59.38%] [G loss: 1.467122]\n",
      "epoch:19 step:18386 [D loss: 0.647036, acc.: 58.59%] [G loss: 1.243090]\n",
      "epoch:19 step:18387 [D loss: 0.697596, acc.: 59.38%] [G loss: 1.197425]\n",
      "epoch:19 step:18388 [D loss: 0.601140, acc.: 63.28%] [G loss: 1.436191]\n",
      "epoch:19 step:18389 [D loss: 0.572888, acc.: 67.97%] [G loss: 1.538047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18390 [D loss: 0.613004, acc.: 61.72%] [G loss: 1.330432]\n",
      "epoch:19 step:18391 [D loss: 0.550861, acc.: 71.09%] [G loss: 1.100974]\n",
      "epoch:19 step:18392 [D loss: 0.563455, acc.: 71.88%] [G loss: 1.034820]\n",
      "epoch:19 step:18393 [D loss: 0.462922, acc.: 78.12%] [G loss: 1.355086]\n",
      "epoch:19 step:18394 [D loss: 0.514768, acc.: 74.22%] [G loss: 1.172906]\n",
      "epoch:19 step:18395 [D loss: 0.388134, acc.: 85.94%] [G loss: 1.509441]\n",
      "epoch:19 step:18396 [D loss: 0.676980, acc.: 59.38%] [G loss: 1.074270]\n",
      "epoch:19 step:18397 [D loss: 0.486031, acc.: 78.12%] [G loss: 1.439722]\n",
      "epoch:19 step:18398 [D loss: 0.694027, acc.: 60.16%] [G loss: 1.118214]\n",
      "epoch:19 step:18399 [D loss: 0.469030, acc.: 80.47%] [G loss: 1.591086]\n",
      "epoch:19 step:18400 [D loss: 0.575231, acc.: 66.41%] [G loss: 1.313118]\n",
      "##############\n",
      "[2.66704478 2.05149899 1.67694986 2.78348944 0.77013635 6.10764418\n",
      " 2.09082524 2.13685002 3.8616981  8.14868929]\n",
      "##########\n",
      "epoch:19 step:18401 [D loss: 0.632381, acc.: 66.41%] [G loss: 1.163389]\n",
      "epoch:19 step:18402 [D loss: 0.448363, acc.: 83.59%] [G loss: 1.506228]\n",
      "epoch:19 step:18403 [D loss: 0.569229, acc.: 69.53%] [G loss: 1.217234]\n",
      "epoch:19 step:18404 [D loss: 0.786678, acc.: 52.34%] [G loss: 1.151524]\n",
      "epoch:19 step:18405 [D loss: 0.550879, acc.: 75.78%] [G loss: 1.266787]\n",
      "epoch:19 step:18406 [D loss: 0.665897, acc.: 60.94%] [G loss: 1.355279]\n",
      "epoch:19 step:18407 [D loss: 0.607271, acc.: 67.97%] [G loss: 1.355718]\n",
      "epoch:19 step:18408 [D loss: 0.680200, acc.: 61.72%] [G loss: 1.132241]\n",
      "epoch:19 step:18409 [D loss: 0.728929, acc.: 56.25%] [G loss: 1.568057]\n",
      "epoch:19 step:18410 [D loss: 0.526927, acc.: 70.31%] [G loss: 1.591175]\n",
      "epoch:19 step:18411 [D loss: 0.510889, acc.: 77.34%] [G loss: 1.322452]\n",
      "epoch:19 step:18412 [D loss: 0.522041, acc.: 73.44%] [G loss: 1.527288]\n",
      "epoch:19 step:18413 [D loss: 0.509144, acc.: 75.78%] [G loss: 1.383709]\n",
      "epoch:19 step:18414 [D loss: 0.618448, acc.: 67.19%] [G loss: 1.110154]\n",
      "epoch:19 step:18415 [D loss: 0.564603, acc.: 69.53%] [G loss: 1.179107]\n",
      "epoch:19 step:18416 [D loss: 0.550035, acc.: 68.75%] [G loss: 1.200947]\n",
      "epoch:19 step:18417 [D loss: 0.484834, acc.: 79.69%] [G loss: 1.700416]\n",
      "epoch:19 step:18418 [D loss: 0.545230, acc.: 69.53%] [G loss: 1.039619]\n",
      "epoch:19 step:18419 [D loss: 0.643105, acc.: 60.94%] [G loss: 1.402083]\n",
      "epoch:19 step:18420 [D loss: 0.560010, acc.: 67.19%] [G loss: 1.312723]\n",
      "epoch:19 step:18421 [D loss: 0.443204, acc.: 82.81%] [G loss: 1.436170]\n",
      "epoch:19 step:18422 [D loss: 0.560127, acc.: 68.75%] [G loss: 1.358979]\n",
      "epoch:19 step:18423 [D loss: 0.658475, acc.: 62.50%] [G loss: 1.544532]\n",
      "epoch:19 step:18424 [D loss: 0.633888, acc.: 67.97%] [G loss: 1.260068]\n",
      "epoch:19 step:18425 [D loss: 0.616640, acc.: 67.19%] [G loss: 1.117530]\n",
      "epoch:19 step:18426 [D loss: 0.410740, acc.: 84.38%] [G loss: 1.183022]\n",
      "epoch:19 step:18427 [D loss: 0.434095, acc.: 78.91%] [G loss: 1.235513]\n",
      "epoch:19 step:18428 [D loss: 0.453915, acc.: 82.81%] [G loss: 1.389570]\n",
      "epoch:19 step:18429 [D loss: 0.632631, acc.: 64.84%] [G loss: 1.306476]\n",
      "epoch:19 step:18430 [D loss: 0.451945, acc.: 82.81%] [G loss: 1.570655]\n",
      "epoch:19 step:18431 [D loss: 0.527769, acc.: 72.66%] [G loss: 0.967788]\n",
      "epoch:19 step:18432 [D loss: 0.671263, acc.: 64.84%] [G loss: 1.361526]\n",
      "epoch:19 step:18433 [D loss: 0.601566, acc.: 66.41%] [G loss: 1.244293]\n",
      "epoch:19 step:18434 [D loss: 0.509556, acc.: 73.44%] [G loss: 1.468207]\n",
      "epoch:19 step:18435 [D loss: 0.550069, acc.: 77.34%] [G loss: 1.088117]\n",
      "epoch:19 step:18436 [D loss: 0.478734, acc.: 75.78%] [G loss: 1.000623]\n",
      "epoch:19 step:18437 [D loss: 0.548267, acc.: 76.56%] [G loss: 1.286517]\n",
      "epoch:19 step:18438 [D loss: 0.432627, acc.: 82.81%] [G loss: 1.478078]\n",
      "epoch:19 step:18439 [D loss: 0.584905, acc.: 66.41%] [G loss: 1.226626]\n",
      "epoch:19 step:18440 [D loss: 0.572058, acc.: 67.19%] [G loss: 1.318746]\n",
      "epoch:19 step:18441 [D loss: 0.579387, acc.: 68.75%] [G loss: 1.157730]\n",
      "epoch:19 step:18442 [D loss: 0.648862, acc.: 66.41%] [G loss: 1.229882]\n",
      "epoch:19 step:18443 [D loss: 0.551512, acc.: 70.31%] [G loss: 1.420038]\n",
      "epoch:19 step:18444 [D loss: 0.714972, acc.: 54.69%] [G loss: 1.090418]\n",
      "epoch:19 step:18445 [D loss: 0.585193, acc.: 71.09%] [G loss: 1.258838]\n",
      "epoch:19 step:18446 [D loss: 0.661256, acc.: 62.50%] [G loss: 1.412740]\n",
      "epoch:19 step:18447 [D loss: 0.613898, acc.: 65.62%] [G loss: 1.453719]\n",
      "epoch:19 step:18448 [D loss: 0.486499, acc.: 77.34%] [G loss: 1.182384]\n",
      "epoch:19 step:18449 [D loss: 0.720166, acc.: 63.28%] [G loss: 1.248172]\n",
      "epoch:19 step:18450 [D loss: 0.665671, acc.: 62.50%] [G loss: 1.339463]\n",
      "epoch:19 step:18451 [D loss: 0.777020, acc.: 53.91%] [G loss: 1.389699]\n",
      "epoch:19 step:18452 [D loss: 0.598944, acc.: 65.62%] [G loss: 1.393907]\n",
      "epoch:19 step:18453 [D loss: 0.531238, acc.: 75.00%] [G loss: 1.461340]\n",
      "epoch:19 step:18454 [D loss: 0.430833, acc.: 85.16%] [G loss: 1.522195]\n",
      "epoch:19 step:18455 [D loss: 0.529763, acc.: 73.44%] [G loss: 1.426323]\n",
      "epoch:19 step:18456 [D loss: 0.564569, acc.: 72.66%] [G loss: 1.246398]\n",
      "epoch:19 step:18457 [D loss: 0.554679, acc.: 71.88%] [G loss: 1.447586]\n",
      "epoch:19 step:18458 [D loss: 0.849092, acc.: 39.84%] [G loss: 0.924137]\n",
      "epoch:19 step:18459 [D loss: 0.391943, acc.: 88.28%] [G loss: 1.419692]\n",
      "epoch:19 step:18460 [D loss: 0.574254, acc.: 74.22%] [G loss: 1.446752]\n",
      "epoch:19 step:18461 [D loss: 0.634807, acc.: 64.84%] [G loss: 1.482033]\n",
      "epoch:19 step:18462 [D loss: 0.608543, acc.: 63.28%] [G loss: 1.445533]\n",
      "epoch:19 step:18463 [D loss: 0.684918, acc.: 61.72%] [G loss: 1.083955]\n",
      "epoch:19 step:18464 [D loss: 0.512176, acc.: 73.44%] [G loss: 1.473783]\n",
      "epoch:19 step:18465 [D loss: 0.778262, acc.: 56.25%] [G loss: 1.238872]\n",
      "epoch:19 step:18466 [D loss: 0.574211, acc.: 67.97%] [G loss: 1.524662]\n",
      "epoch:19 step:18467 [D loss: 0.551409, acc.: 70.31%] [G loss: 1.089327]\n",
      "epoch:19 step:18468 [D loss: 0.615085, acc.: 60.16%] [G loss: 1.330128]\n",
      "epoch:19 step:18469 [D loss: 0.558047, acc.: 67.97%] [G loss: 1.298575]\n",
      "epoch:19 step:18470 [D loss: 0.645947, acc.: 62.50%] [G loss: 1.024786]\n",
      "epoch:19 step:18471 [D loss: 0.654451, acc.: 62.50%] [G loss: 1.397857]\n",
      "epoch:19 step:18472 [D loss: 0.676356, acc.: 58.59%] [G loss: 1.357764]\n",
      "epoch:19 step:18473 [D loss: 0.561671, acc.: 73.44%] [G loss: 1.084696]\n",
      "epoch:19 step:18474 [D loss: 0.751756, acc.: 49.22%] [G loss: 1.066401]\n",
      "epoch:19 step:18475 [D loss: 0.412873, acc.: 85.94%] [G loss: 1.084671]\n",
      "epoch:19 step:18476 [D loss: 0.710449, acc.: 55.47%] [G loss: 1.313128]\n",
      "epoch:19 step:18477 [D loss: 0.469394, acc.: 83.59%] [G loss: 1.483771]\n",
      "epoch:19 step:18478 [D loss: 0.540144, acc.: 75.00%] [G loss: 1.315875]\n",
      "epoch:19 step:18479 [D loss: 0.396135, acc.: 85.94%] [G loss: 1.592946]\n",
      "epoch:19 step:18480 [D loss: 0.491144, acc.: 75.78%] [G loss: 1.016463]\n",
      "epoch:19 step:18481 [D loss: 0.599172, acc.: 69.53%] [G loss: 1.230808]\n",
      "epoch:19 step:18482 [D loss: 0.530032, acc.: 74.22%] [G loss: 1.374359]\n",
      "epoch:19 step:18483 [D loss: 0.770413, acc.: 50.78%] [G loss: 1.339316]\n",
      "epoch:19 step:18484 [D loss: 0.594426, acc.: 69.53%] [G loss: 1.316683]\n",
      "epoch:19 step:18485 [D loss: 0.455539, acc.: 86.72%] [G loss: 1.524549]\n",
      "epoch:19 step:18486 [D loss: 0.668357, acc.: 58.59%] [G loss: 0.921447]\n",
      "epoch:19 step:18487 [D loss: 0.445298, acc.: 78.91%] [G loss: 1.663959]\n",
      "epoch:19 step:18488 [D loss: 0.647191, acc.: 65.62%] [G loss: 1.079100]\n",
      "epoch:19 step:18489 [D loss: 0.583296, acc.: 69.53%] [G loss: 1.434142]\n",
      "epoch:19 step:18490 [D loss: 0.497118, acc.: 72.66%] [G loss: 1.795083]\n",
      "epoch:19 step:18491 [D loss: 0.453208, acc.: 83.59%] [G loss: 1.402595]\n",
      "epoch:19 step:18492 [D loss: 0.575140, acc.: 68.75%] [G loss: 1.145958]\n",
      "epoch:19 step:18493 [D loss: 0.532905, acc.: 73.44%] [G loss: 1.634793]\n",
      "epoch:19 step:18494 [D loss: 0.592601, acc.: 68.75%] [G loss: 1.180065]\n",
      "epoch:19 step:18495 [D loss: 0.437404, acc.: 82.81%] [G loss: 1.204396]\n",
      "epoch:19 step:18496 [D loss: 0.717033, acc.: 61.72%] [G loss: 1.217424]\n",
      "epoch:19 step:18497 [D loss: 0.731872, acc.: 55.47%] [G loss: 1.204200]\n",
      "epoch:19 step:18498 [D loss: 0.610208, acc.: 66.41%] [G loss: 1.546618]\n",
      "epoch:19 step:18499 [D loss: 0.506035, acc.: 75.00%] [G loss: 1.681286]\n",
      "epoch:19 step:18500 [D loss: 0.375960, acc.: 87.50%] [G loss: 1.351890]\n",
      "epoch:19 step:18501 [D loss: 0.553480, acc.: 75.78%] [G loss: 1.030577]\n",
      "epoch:19 step:18502 [D loss: 0.472579, acc.: 80.47%] [G loss: 1.278891]\n",
      "epoch:19 step:18503 [D loss: 0.565621, acc.: 71.09%] [G loss: 1.237626]\n",
      "epoch:19 step:18504 [D loss: 0.427377, acc.: 82.81%] [G loss: 1.246032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18505 [D loss: 0.561185, acc.: 73.44%] [G loss: 1.439032]\n",
      "epoch:19 step:18506 [D loss: 0.763164, acc.: 51.56%] [G loss: 1.108493]\n",
      "epoch:19 step:18507 [D loss: 0.673520, acc.: 54.69%] [G loss: 1.276280]\n",
      "epoch:19 step:18508 [D loss: 0.467917, acc.: 81.25%] [G loss: 1.038512]\n",
      "epoch:19 step:18509 [D loss: 0.664882, acc.: 56.25%] [G loss: 1.306505]\n",
      "epoch:19 step:18510 [D loss: 0.468604, acc.: 77.34%] [G loss: 1.587046]\n",
      "epoch:19 step:18511 [D loss: 0.645499, acc.: 63.28%] [G loss: 1.200630]\n",
      "epoch:19 step:18512 [D loss: 0.610782, acc.: 64.84%] [G loss: 1.274411]\n",
      "epoch:19 step:18513 [D loss: 0.638107, acc.: 63.28%] [G loss: 1.379818]\n",
      "epoch:19 step:18514 [D loss: 0.527795, acc.: 75.78%] [G loss: 1.515231]\n",
      "epoch:19 step:18515 [D loss: 0.484273, acc.: 77.34%] [G loss: 1.749711]\n",
      "epoch:19 step:18516 [D loss: 0.658613, acc.: 64.84%] [G loss: 1.134585]\n",
      "epoch:19 step:18517 [D loss: 0.487368, acc.: 77.34%] [G loss: 1.245564]\n",
      "epoch:19 step:18518 [D loss: 0.607653, acc.: 65.62%] [G loss: 1.072646]\n",
      "epoch:19 step:18519 [D loss: 0.474331, acc.: 78.91%] [G loss: 1.372537]\n",
      "epoch:19 step:18520 [D loss: 0.477679, acc.: 79.69%] [G loss: 1.706852]\n",
      "epoch:19 step:18521 [D loss: 0.349485, acc.: 85.94%] [G loss: 1.685392]\n",
      "epoch:19 step:18522 [D loss: 0.522644, acc.: 75.00%] [G loss: 1.569629]\n",
      "epoch:19 step:18523 [D loss: 0.591118, acc.: 65.62%] [G loss: 1.415412]\n",
      "epoch:19 step:18524 [D loss: 0.447436, acc.: 85.94%] [G loss: 1.420328]\n",
      "epoch:19 step:18525 [D loss: 0.547185, acc.: 73.44%] [G loss: 1.264665]\n",
      "epoch:19 step:18526 [D loss: 0.504033, acc.: 75.00%] [G loss: 1.281783]\n",
      "epoch:19 step:18527 [D loss: 0.595762, acc.: 71.88%] [G loss: 1.830943]\n",
      "epoch:19 step:18528 [D loss: 0.625629, acc.: 62.50%] [G loss: 1.183724]\n",
      "epoch:19 step:18529 [D loss: 0.522574, acc.: 73.44%] [G loss: 1.219091]\n",
      "epoch:19 step:18530 [D loss: 0.449664, acc.: 81.25%] [G loss: 1.120971]\n",
      "epoch:19 step:18531 [D loss: 0.675778, acc.: 58.59%] [G loss: 1.074354]\n",
      "epoch:19 step:18532 [D loss: 0.533601, acc.: 77.34%] [G loss: 1.153033]\n",
      "epoch:19 step:18533 [D loss: 0.664075, acc.: 62.50%] [G loss: 1.422075]\n",
      "epoch:19 step:18534 [D loss: 0.889723, acc.: 43.75%] [G loss: 1.101493]\n",
      "epoch:19 step:18535 [D loss: 0.627004, acc.: 65.62%] [G loss: 1.066459]\n",
      "epoch:19 step:18536 [D loss: 0.585899, acc.: 67.19%] [G loss: 1.227703]\n",
      "epoch:19 step:18537 [D loss: 0.621640, acc.: 65.62%] [G loss: 1.059461]\n",
      "epoch:19 step:18538 [D loss: 0.521207, acc.: 71.88%] [G loss: 1.507980]\n",
      "epoch:19 step:18539 [D loss: 0.745321, acc.: 56.25%] [G loss: 1.043753]\n",
      "epoch:19 step:18540 [D loss: 0.434003, acc.: 84.38%] [G loss: 1.038405]\n",
      "epoch:19 step:18541 [D loss: 0.567183, acc.: 73.44%] [G loss: 1.185766]\n",
      "epoch:19 step:18542 [D loss: 0.639463, acc.: 60.16%] [G loss: 1.432383]\n",
      "epoch:19 step:18543 [D loss: 0.727003, acc.: 53.91%] [G loss: 0.831837]\n",
      "epoch:19 step:18544 [D loss: 0.492863, acc.: 70.31%] [G loss: 1.400869]\n",
      "epoch:19 step:18545 [D loss: 0.435733, acc.: 82.81%] [G loss: 1.544625]\n",
      "epoch:19 step:18546 [D loss: 0.584917, acc.: 68.75%] [G loss: 1.418152]\n",
      "epoch:19 step:18547 [D loss: 0.637176, acc.: 60.16%] [G loss: 1.220118]\n",
      "epoch:19 step:18548 [D loss: 0.579652, acc.: 72.66%] [G loss: 1.065316]\n",
      "epoch:19 step:18549 [D loss: 0.558649, acc.: 70.31%] [G loss: 1.154594]\n",
      "epoch:19 step:18550 [D loss: 0.488385, acc.: 78.91%] [G loss: 1.328452]\n",
      "epoch:19 step:18551 [D loss: 0.531614, acc.: 72.66%] [G loss: 1.250116]\n",
      "epoch:19 step:18552 [D loss: 0.632458, acc.: 65.62%] [G loss: 1.489868]\n",
      "epoch:19 step:18553 [D loss: 0.479454, acc.: 78.91%] [G loss: 1.184814]\n",
      "epoch:19 step:18554 [D loss: 0.497058, acc.: 75.00%] [G loss: 1.285434]\n",
      "epoch:19 step:18555 [D loss: 0.541112, acc.: 75.00%] [G loss: 1.552511]\n",
      "epoch:19 step:18556 [D loss: 0.648950, acc.: 64.06%] [G loss: 1.265049]\n",
      "epoch:19 step:18557 [D loss: 0.491102, acc.: 79.69%] [G loss: 1.274143]\n",
      "epoch:19 step:18558 [D loss: 0.586733, acc.: 71.09%] [G loss: 1.084828]\n",
      "epoch:19 step:18559 [D loss: 0.539387, acc.: 67.19%] [G loss: 1.307462]\n",
      "epoch:19 step:18560 [D loss: 0.626075, acc.: 64.84%] [G loss: 1.479435]\n",
      "epoch:19 step:18561 [D loss: 0.552979, acc.: 71.09%] [G loss: 0.992373]\n",
      "epoch:19 step:18562 [D loss: 0.494366, acc.: 77.34%] [G loss: 1.326138]\n",
      "epoch:19 step:18563 [D loss: 0.540440, acc.: 76.56%] [G loss: 1.265568]\n",
      "epoch:19 step:18564 [D loss: 0.591685, acc.: 66.41%] [G loss: 1.451199]\n",
      "epoch:19 step:18565 [D loss: 0.498667, acc.: 78.91%] [G loss: 1.178366]\n",
      "epoch:19 step:18566 [D loss: 0.600409, acc.: 69.53%] [G loss: 1.508470]\n",
      "epoch:19 step:18567 [D loss: 0.457838, acc.: 78.12%] [G loss: 1.408775]\n",
      "epoch:19 step:18568 [D loss: 0.460173, acc.: 75.78%] [G loss: 1.400406]\n",
      "epoch:19 step:18569 [D loss: 0.583640, acc.: 72.66%] [G loss: 1.261245]\n",
      "epoch:19 step:18570 [D loss: 0.478979, acc.: 79.69%] [G loss: 1.523152]\n",
      "epoch:19 step:18571 [D loss: 0.515610, acc.: 73.44%] [G loss: 1.491840]\n",
      "epoch:19 step:18572 [D loss: 0.552308, acc.: 71.88%] [G loss: 1.499591]\n",
      "epoch:19 step:18573 [D loss: 0.602032, acc.: 64.84%] [G loss: 1.251415]\n",
      "epoch:19 step:18574 [D loss: 0.537206, acc.: 71.09%] [G loss: 1.205718]\n",
      "epoch:19 step:18575 [D loss: 0.543423, acc.: 73.44%] [G loss: 1.175065]\n",
      "epoch:19 step:18576 [D loss: 0.518950, acc.: 73.44%] [G loss: 1.424634]\n",
      "epoch:19 step:18577 [D loss: 0.491568, acc.: 75.78%] [G loss: 1.399300]\n",
      "epoch:19 step:18578 [D loss: 0.685972, acc.: 61.72%] [G loss: 1.347799]\n",
      "epoch:19 step:18579 [D loss: 0.605339, acc.: 61.72%] [G loss: 1.577905]\n",
      "epoch:19 step:18580 [D loss: 0.597975, acc.: 71.09%] [G loss: 1.123207]\n",
      "epoch:19 step:18581 [D loss: 0.511495, acc.: 75.78%] [G loss: 1.399319]\n",
      "epoch:19 step:18582 [D loss: 0.526098, acc.: 77.34%] [G loss: 1.485694]\n",
      "epoch:19 step:18583 [D loss: 0.545511, acc.: 68.75%] [G loss: 1.481745]\n",
      "epoch:19 step:18584 [D loss: 0.514830, acc.: 77.34%] [G loss: 1.187049]\n",
      "epoch:19 step:18585 [D loss: 0.465065, acc.: 82.03%] [G loss: 1.519905]\n",
      "epoch:19 step:18586 [D loss: 0.649536, acc.: 64.84%] [G loss: 1.227743]\n",
      "epoch:19 step:18587 [D loss: 0.680218, acc.: 58.59%] [G loss: 1.428779]\n",
      "epoch:19 step:18588 [D loss: 0.494645, acc.: 78.12%] [G loss: 1.404761]\n",
      "epoch:19 step:18589 [D loss: 0.581928, acc.: 67.97%] [G loss: 1.069001]\n",
      "epoch:19 step:18590 [D loss: 0.651120, acc.: 64.84%] [G loss: 1.388064]\n",
      "epoch:19 step:18591 [D loss: 0.652851, acc.: 63.28%] [G loss: 1.156044]\n",
      "epoch:19 step:18592 [D loss: 0.679475, acc.: 63.28%] [G loss: 1.224226]\n",
      "epoch:19 step:18593 [D loss: 0.503598, acc.: 77.34%] [G loss: 1.178389]\n",
      "epoch:19 step:18594 [D loss: 0.544637, acc.: 75.00%] [G loss: 1.457178]\n",
      "epoch:19 step:18595 [D loss: 0.605896, acc.: 63.28%] [G loss: 0.838168]\n",
      "epoch:19 step:18596 [D loss: 0.596483, acc.: 67.97%] [G loss: 1.346979]\n",
      "epoch:19 step:18597 [D loss: 0.489861, acc.: 75.78%] [G loss: 0.978643]\n",
      "epoch:19 step:18598 [D loss: 0.666860, acc.: 59.38%] [G loss: 1.085634]\n",
      "epoch:19 step:18599 [D loss: 0.552501, acc.: 71.09%] [G loss: 1.546357]\n",
      "epoch:19 step:18600 [D loss: 0.691933, acc.: 60.94%] [G loss: 1.131075]\n",
      "##############\n",
      "[2.67036034 2.04361708 2.2198209  2.9120813  1.02836021 6.19840555\n",
      " 2.13485447 2.68193329 4.06128525 4.56834127]\n",
      "##########\n",
      "epoch:19 step:18601 [D loss: 0.548648, acc.: 75.78%] [G loss: 1.082162]\n",
      "epoch:19 step:18602 [D loss: 0.544336, acc.: 70.31%] [G loss: 1.257590]\n",
      "epoch:19 step:18603 [D loss: 0.485964, acc.: 78.12%] [G loss: 1.030593]\n",
      "epoch:19 step:18604 [D loss: 0.587957, acc.: 71.09%] [G loss: 1.200912]\n",
      "epoch:19 step:18605 [D loss: 0.665630, acc.: 62.50%] [G loss: 1.266207]\n",
      "epoch:19 step:18606 [D loss: 0.518660, acc.: 74.22%] [G loss: 1.359999]\n",
      "epoch:19 step:18607 [D loss: 0.540810, acc.: 71.09%] [G loss: 1.600480]\n",
      "epoch:19 step:18608 [D loss: 0.890278, acc.: 42.97%] [G loss: 1.513200]\n",
      "epoch:19 step:18609 [D loss: 0.454471, acc.: 85.16%] [G loss: 1.489159]\n",
      "epoch:19 step:18610 [D loss: 0.551727, acc.: 71.88%] [G loss: 1.405790]\n",
      "epoch:19 step:18611 [D loss: 0.562549, acc.: 69.53%] [G loss: 1.505178]\n",
      "epoch:19 step:18612 [D loss: 0.521815, acc.: 77.34%] [G loss: 1.332610]\n",
      "epoch:19 step:18613 [D loss: 0.436198, acc.: 80.47%] [G loss: 1.726346]\n",
      "epoch:19 step:18614 [D loss: 0.661597, acc.: 62.50%] [G loss: 1.137681]\n",
      "epoch:19 step:18615 [D loss: 0.465554, acc.: 81.25%] [G loss: 1.495390]\n",
      "epoch:19 step:18616 [D loss: 0.685313, acc.: 62.50%] [G loss: 1.451322]\n",
      "epoch:19 step:18617 [D loss: 0.607929, acc.: 67.19%] [G loss: 1.213114]\n",
      "epoch:19 step:18618 [D loss: 0.847810, acc.: 46.88%] [G loss: 1.236540]\n",
      "epoch:19 step:18619 [D loss: 0.491557, acc.: 78.12%] [G loss: 1.226076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18620 [D loss: 0.587675, acc.: 66.41%] [G loss: 1.471975]\n",
      "epoch:19 step:18621 [D loss: 0.666739, acc.: 62.50%] [G loss: 1.336253]\n",
      "epoch:19 step:18622 [D loss: 0.652872, acc.: 60.94%] [G loss: 1.113132]\n",
      "epoch:19 step:18623 [D loss: 0.482946, acc.: 75.00%] [G loss: 1.051979]\n",
      "epoch:19 step:18624 [D loss: 0.726014, acc.: 57.03%] [G loss: 1.155376]\n",
      "epoch:19 step:18625 [D loss: 0.587768, acc.: 68.75%] [G loss: 1.136009]\n",
      "epoch:19 step:18626 [D loss: 0.592626, acc.: 69.53%] [G loss: 1.428555]\n",
      "epoch:19 step:18627 [D loss: 0.652439, acc.: 62.50%] [G loss: 0.976383]\n",
      "epoch:19 step:18628 [D loss: 0.577676, acc.: 71.09%] [G loss: 1.576043]\n",
      "epoch:19 step:18629 [D loss: 0.650184, acc.: 62.50%] [G loss: 1.502916]\n",
      "epoch:19 step:18630 [D loss: 0.576755, acc.: 67.97%] [G loss: 1.517721]\n",
      "epoch:19 step:18631 [D loss: 0.684900, acc.: 62.50%] [G loss: 1.157611]\n",
      "epoch:19 step:18632 [D loss: 0.666613, acc.: 61.72%] [G loss: 1.216609]\n",
      "epoch:19 step:18633 [D loss: 0.590842, acc.: 70.31%] [G loss: 1.324186]\n",
      "epoch:19 step:18634 [D loss: 0.577938, acc.: 71.88%] [G loss: 1.136969]\n",
      "epoch:19 step:18635 [D loss: 0.723360, acc.: 58.59%] [G loss: 1.144235]\n",
      "epoch:19 step:18636 [D loss: 0.804154, acc.: 48.44%] [G loss: 1.382543]\n",
      "epoch:19 step:18637 [D loss: 0.673925, acc.: 64.84%] [G loss: 1.173619]\n",
      "epoch:19 step:18638 [D loss: 0.604288, acc.: 67.19%] [G loss: 1.590986]\n",
      "epoch:19 step:18639 [D loss: 0.533829, acc.: 75.00%] [G loss: 1.621683]\n",
      "epoch:19 step:18640 [D loss: 0.693174, acc.: 59.38%] [G loss: 1.067949]\n",
      "epoch:19 step:18641 [D loss: 0.684940, acc.: 54.69%] [G loss: 1.027812]\n",
      "epoch:19 step:18642 [D loss: 0.680977, acc.: 62.50%] [G loss: 1.013717]\n",
      "epoch:19 step:18643 [D loss: 0.566338, acc.: 73.44%] [G loss: 1.564014]\n",
      "epoch:19 step:18644 [D loss: 0.621735, acc.: 69.53%] [G loss: 1.014239]\n",
      "epoch:19 step:18645 [D loss: 0.604721, acc.: 66.41%] [G loss: 1.564532]\n",
      "epoch:19 step:18646 [D loss: 0.537120, acc.: 74.22%] [G loss: 1.331067]\n",
      "epoch:19 step:18647 [D loss: 0.561265, acc.: 69.53%] [G loss: 0.986057]\n",
      "epoch:19 step:18648 [D loss: 0.559238, acc.: 73.44%] [G loss: 1.332366]\n",
      "epoch:19 step:18649 [D loss: 0.506668, acc.: 78.91%] [G loss: 1.356184]\n",
      "epoch:19 step:18650 [D loss: 0.548054, acc.: 75.00%] [G loss: 1.255218]\n",
      "epoch:19 step:18651 [D loss: 0.575188, acc.: 71.09%] [G loss: 1.138058]\n",
      "epoch:19 step:18652 [D loss: 0.521550, acc.: 76.56%] [G loss: 1.473626]\n",
      "epoch:19 step:18653 [D loss: 0.473081, acc.: 82.03%] [G loss: 1.743822]\n",
      "epoch:19 step:18654 [D loss: 0.655756, acc.: 62.50%] [G loss: 1.051257]\n",
      "epoch:19 step:18655 [D loss: 0.590350, acc.: 71.09%] [G loss: 1.034298]\n",
      "epoch:19 step:18656 [D loss: 0.613470, acc.: 66.41%] [G loss: 1.476513]\n",
      "epoch:19 step:18657 [D loss: 0.705789, acc.: 53.12%] [G loss: 1.387710]\n",
      "epoch:19 step:18658 [D loss: 0.546922, acc.: 71.09%] [G loss: 1.352481]\n",
      "epoch:19 step:18659 [D loss: 0.525233, acc.: 73.44%] [G loss: 1.586263]\n",
      "epoch:19 step:18660 [D loss: 0.502264, acc.: 78.91%] [G loss: 1.144602]\n",
      "epoch:19 step:18661 [D loss: 0.403747, acc.: 85.16%] [G loss: 1.084150]\n",
      "epoch:19 step:18662 [D loss: 0.510314, acc.: 78.91%] [G loss: 1.428409]\n",
      "epoch:19 step:18663 [D loss: 0.503884, acc.: 75.00%] [G loss: 1.472457]\n",
      "epoch:19 step:18664 [D loss: 0.557909, acc.: 75.78%] [G loss: 1.236140]\n",
      "epoch:19 step:18665 [D loss: 0.845007, acc.: 50.00%] [G loss: 1.210323]\n",
      "epoch:19 step:18666 [D loss: 0.397746, acc.: 89.06%] [G loss: 1.689114]\n",
      "epoch:19 step:18667 [D loss: 0.626524, acc.: 61.72%] [G loss: 1.328028]\n",
      "epoch:19 step:18668 [D loss: 0.637953, acc.: 64.06%] [G loss: 1.176820]\n",
      "epoch:19 step:18669 [D loss: 0.523167, acc.: 75.78%] [G loss: 1.141617]\n",
      "epoch:19 step:18670 [D loss: 0.642693, acc.: 63.28%] [G loss: 1.510183]\n",
      "epoch:19 step:18671 [D loss: 0.420358, acc.: 81.25%] [G loss: 1.360947]\n",
      "epoch:19 step:18672 [D loss: 0.366131, acc.: 93.75%] [G loss: 1.893440]\n",
      "epoch:19 step:18673 [D loss: 0.392415, acc.: 87.50%] [G loss: 1.489071]\n",
      "epoch:19 step:18674 [D loss: 0.629974, acc.: 60.94%] [G loss: 1.115569]\n",
      "epoch:19 step:18675 [D loss: 0.558232, acc.: 70.31%] [G loss: 1.065986]\n",
      "epoch:19 step:18676 [D loss: 0.480903, acc.: 77.34%] [G loss: 1.369737]\n",
      "epoch:19 step:18677 [D loss: 0.604713, acc.: 67.97%] [G loss: 1.091868]\n",
      "epoch:19 step:18678 [D loss: 0.589626, acc.: 71.09%] [G loss: 1.000665]\n",
      "epoch:19 step:18679 [D loss: 0.495333, acc.: 79.69%] [G loss: 1.363590]\n",
      "epoch:19 step:18680 [D loss: 0.606326, acc.: 66.41%] [G loss: 1.317119]\n",
      "epoch:19 step:18681 [D loss: 0.519510, acc.: 75.78%] [G loss: 1.471143]\n",
      "epoch:19 step:18682 [D loss: 0.564341, acc.: 68.75%] [G loss: 1.021463]\n",
      "epoch:19 step:18683 [D loss: 0.463877, acc.: 78.12%] [G loss: 1.404394]\n",
      "epoch:19 step:18684 [D loss: 0.525106, acc.: 73.44%] [G loss: 1.172346]\n",
      "epoch:19 step:18685 [D loss: 0.586627, acc.: 67.97%] [G loss: 1.106491]\n",
      "epoch:19 step:18686 [D loss: 0.566073, acc.: 73.44%] [G loss: 1.468000]\n",
      "epoch:19 step:18687 [D loss: 0.559929, acc.: 64.84%] [G loss: 1.288478]\n",
      "epoch:19 step:18688 [D loss: 0.576331, acc.: 71.09%] [G loss: 0.966852]\n",
      "epoch:19 step:18689 [D loss: 0.593855, acc.: 71.88%] [G loss: 1.550339]\n",
      "epoch:19 step:18690 [D loss: 0.509659, acc.: 73.44%] [G loss: 1.403092]\n",
      "epoch:19 step:18691 [D loss: 0.600502, acc.: 61.72%] [G loss: 1.467979]\n",
      "epoch:19 step:18692 [D loss: 0.571679, acc.: 65.62%] [G loss: 1.103947]\n",
      "epoch:19 step:18693 [D loss: 0.761137, acc.: 50.00%] [G loss: 0.956426]\n",
      "epoch:19 step:18694 [D loss: 0.364991, acc.: 85.94%] [G loss: 1.394476]\n",
      "epoch:19 step:18695 [D loss: 0.704405, acc.: 53.12%] [G loss: 1.284365]\n",
      "epoch:19 step:18696 [D loss: 0.732834, acc.: 57.03%] [G loss: 1.243246]\n",
      "epoch:19 step:18697 [D loss: 0.643745, acc.: 64.06%] [G loss: 1.343083]\n",
      "epoch:19 step:18698 [D loss: 0.485702, acc.: 79.69%] [G loss: 1.557772]\n",
      "epoch:19 step:18699 [D loss: 0.483845, acc.: 72.66%] [G loss: 1.492003]\n",
      "epoch:19 step:18700 [D loss: 0.377310, acc.: 87.50%] [G loss: 1.722333]\n",
      "epoch:19 step:18701 [D loss: 0.534600, acc.: 74.22%] [G loss: 1.306262]\n",
      "epoch:19 step:18702 [D loss: 0.438847, acc.: 79.69%] [G loss: 1.760901]\n",
      "epoch:19 step:18703 [D loss: 0.530709, acc.: 78.12%] [G loss: 1.121374]\n",
      "epoch:19 step:18704 [D loss: 0.511910, acc.: 76.56%] [G loss: 1.463087]\n",
      "epoch:19 step:18705 [D loss: 0.436932, acc.: 84.38%] [G loss: 1.640512]\n",
      "epoch:19 step:18706 [D loss: 0.671183, acc.: 64.06%] [G loss: 1.314776]\n",
      "epoch:19 step:18707 [D loss: 0.510872, acc.: 71.88%] [G loss: 1.349776]\n",
      "epoch:19 step:18708 [D loss: 0.597330, acc.: 74.22%] [G loss: 1.214333]\n",
      "epoch:19 step:18709 [D loss: 0.687968, acc.: 62.50%] [G loss: 1.077190]\n",
      "epoch:19 step:18710 [D loss: 0.522881, acc.: 77.34%] [G loss: 1.530019]\n",
      "epoch:19 step:18711 [D loss: 0.799628, acc.: 52.34%] [G loss: 1.241830]\n",
      "epoch:19 step:18712 [D loss: 0.574000, acc.: 68.75%] [G loss: 1.461580]\n",
      "epoch:19 step:18713 [D loss: 0.580181, acc.: 68.75%] [G loss: 1.722139]\n",
      "epoch:19 step:18714 [D loss: 0.561753, acc.: 69.53%] [G loss: 1.666252]\n",
      "epoch:19 step:18715 [D loss: 0.537337, acc.: 71.88%] [G loss: 1.487872]\n",
      "epoch:19 step:18716 [D loss: 0.640996, acc.: 60.16%] [G loss: 1.324069]\n",
      "epoch:19 step:18717 [D loss: 0.517420, acc.: 75.00%] [G loss: 1.266914]\n",
      "epoch:19 step:18718 [D loss: 0.508882, acc.: 73.44%] [G loss: 1.452654]\n",
      "epoch:19 step:18719 [D loss: 0.658323, acc.: 64.84%] [G loss: 1.292217]\n",
      "epoch:19 step:18720 [D loss: 0.559452, acc.: 71.88%] [G loss: 1.553235]\n",
      "epoch:19 step:18721 [D loss: 0.608019, acc.: 69.53%] [G loss: 1.449127]\n",
      "epoch:19 step:18722 [D loss: 0.488198, acc.: 79.69%] [G loss: 1.152205]\n",
      "epoch:19 step:18723 [D loss: 0.595124, acc.: 70.31%] [G loss: 1.407806]\n",
      "epoch:19 step:18724 [D loss: 0.607984, acc.: 63.28%] [G loss: 1.264353]\n",
      "epoch:19 step:18725 [D loss: 0.552940, acc.: 69.53%] [G loss: 1.156545]\n",
      "epoch:19 step:18726 [D loss: 0.510553, acc.: 74.22%] [G loss: 1.400712]\n",
      "epoch:19 step:18727 [D loss: 0.451241, acc.: 80.47%] [G loss: 1.438089]\n",
      "epoch:19 step:18728 [D loss: 0.485372, acc.: 75.78%] [G loss: 1.119681]\n",
      "epoch:19 step:18729 [D loss: 0.660915, acc.: 64.06%] [G loss: 1.236062]\n",
      "epoch:19 step:18730 [D loss: 0.593737, acc.: 69.53%] [G loss: 1.158050]\n",
      "epoch:19 step:18731 [D loss: 0.464330, acc.: 82.03%] [G loss: 1.318435]\n",
      "epoch:19 step:18732 [D loss: 0.525662, acc.: 75.78%] [G loss: 1.454278]\n",
      "epoch:19 step:18733 [D loss: 0.514712, acc.: 81.25%] [G loss: 1.415271]\n",
      "epoch:19 step:18734 [D loss: 0.636872, acc.: 62.50%] [G loss: 1.353341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18735 [D loss: 0.596778, acc.: 66.41%] [G loss: 1.588691]\n",
      "epoch:19 step:18736 [D loss: 0.872123, acc.: 40.62%] [G loss: 1.547544]\n",
      "epoch:19 step:18737 [D loss: 0.614587, acc.: 67.97%] [G loss: 1.004805]\n",
      "epoch:19 step:18738 [D loss: 0.473274, acc.: 80.47%] [G loss: 1.338627]\n",
      "epoch:19 step:18739 [D loss: 0.525108, acc.: 75.00%] [G loss: 1.605227]\n",
      "epoch:19 step:18740 [D loss: 0.687240, acc.: 61.72%] [G loss: 1.242836]\n",
      "epoch:20 step:18741 [D loss: 0.640018, acc.: 60.16%] [G loss: 0.976336]\n",
      "epoch:20 step:18742 [D loss: 0.547198, acc.: 69.53%] [G loss: 1.551086]\n",
      "epoch:20 step:18743 [D loss: 0.601950, acc.: 71.88%] [G loss: 1.222216]\n",
      "epoch:20 step:18744 [D loss: 0.680833, acc.: 61.72%] [G loss: 1.028207]\n",
      "epoch:20 step:18745 [D loss: 0.599299, acc.: 67.19%] [G loss: 1.101522]\n",
      "epoch:20 step:18746 [D loss: 0.714631, acc.: 60.16%] [G loss: 1.308518]\n",
      "epoch:20 step:18747 [D loss: 0.543950, acc.: 75.00%] [G loss: 1.165763]\n",
      "epoch:20 step:18748 [D loss: 0.557572, acc.: 74.22%] [G loss: 1.167473]\n",
      "epoch:20 step:18749 [D loss: 0.464061, acc.: 77.34%] [G loss: 1.219739]\n",
      "epoch:20 step:18750 [D loss: 0.451716, acc.: 79.69%] [G loss: 1.490525]\n",
      "epoch:20 step:18751 [D loss: 0.444448, acc.: 83.59%] [G loss: 1.400955]\n",
      "epoch:20 step:18752 [D loss: 0.729179, acc.: 57.03%] [G loss: 1.258688]\n",
      "epoch:20 step:18753 [D loss: 0.744989, acc.: 58.59%] [G loss: 0.994568]\n",
      "epoch:20 step:18754 [D loss: 0.530342, acc.: 75.78%] [G loss: 1.141005]\n",
      "epoch:20 step:18755 [D loss: 0.565109, acc.: 67.97%] [G loss: 1.179863]\n",
      "epoch:20 step:18756 [D loss: 0.426966, acc.: 82.81%] [G loss: 1.679875]\n",
      "epoch:20 step:18757 [D loss: 0.548585, acc.: 74.22%] [G loss: 1.116511]\n",
      "epoch:20 step:18758 [D loss: 0.693062, acc.: 59.38%] [G loss: 1.151023]\n",
      "epoch:20 step:18759 [D loss: 0.617568, acc.: 66.41%] [G loss: 1.332415]\n",
      "epoch:20 step:18760 [D loss: 0.554742, acc.: 70.31%] [G loss: 1.266453]\n",
      "epoch:20 step:18761 [D loss: 0.694088, acc.: 55.47%] [G loss: 1.214143]\n",
      "epoch:20 step:18762 [D loss: 0.476251, acc.: 78.91%] [G loss: 1.412146]\n",
      "epoch:20 step:18763 [D loss: 0.667870, acc.: 60.16%] [G loss: 1.218786]\n",
      "epoch:20 step:18764 [D loss: 0.663277, acc.: 62.50%] [G loss: 1.495931]\n",
      "epoch:20 step:18765 [D loss: 0.654925, acc.: 62.50%] [G loss: 1.150349]\n",
      "epoch:20 step:18766 [D loss: 0.601727, acc.: 68.75%] [G loss: 0.978134]\n",
      "epoch:20 step:18767 [D loss: 0.525013, acc.: 77.34%] [G loss: 1.465983]\n",
      "epoch:20 step:18768 [D loss: 0.630667, acc.: 64.84%] [G loss: 1.218959]\n",
      "epoch:20 step:18769 [D loss: 0.566114, acc.: 69.53%] [G loss: 1.224252]\n",
      "epoch:20 step:18770 [D loss: 0.612988, acc.: 66.41%] [G loss: 1.239192]\n",
      "epoch:20 step:18771 [D loss: 0.514402, acc.: 75.78%] [G loss: 1.288778]\n",
      "epoch:20 step:18772 [D loss: 0.611517, acc.: 70.31%] [G loss: 1.319733]\n",
      "epoch:20 step:18773 [D loss: 0.546473, acc.: 69.53%] [G loss: 1.174312]\n",
      "epoch:20 step:18774 [D loss: 0.549300, acc.: 78.12%] [G loss: 1.163717]\n",
      "epoch:20 step:18775 [D loss: 0.475647, acc.: 74.22%] [G loss: 1.464455]\n",
      "epoch:20 step:18776 [D loss: 0.435768, acc.: 85.94%] [G loss: 1.234356]\n",
      "epoch:20 step:18777 [D loss: 0.472123, acc.: 80.47%] [G loss: 1.404880]\n",
      "epoch:20 step:18778 [D loss: 0.587594, acc.: 74.22%] [G loss: 1.431945]\n",
      "epoch:20 step:18779 [D loss: 0.535683, acc.: 75.00%] [G loss: 1.163123]\n",
      "epoch:20 step:18780 [D loss: 0.540957, acc.: 70.31%] [G loss: 1.447083]\n",
      "epoch:20 step:18781 [D loss: 0.525832, acc.: 71.09%] [G loss: 1.406462]\n",
      "epoch:20 step:18782 [D loss: 0.566941, acc.: 73.44%] [G loss: 1.353891]\n",
      "epoch:20 step:18783 [D loss: 0.580235, acc.: 71.09%] [G loss: 1.158222]\n",
      "epoch:20 step:18784 [D loss: 0.544062, acc.: 69.53%] [G loss: 1.551247]\n",
      "epoch:20 step:18785 [D loss: 0.531473, acc.: 76.56%] [G loss: 1.670911]\n",
      "epoch:20 step:18786 [D loss: 0.677873, acc.: 61.72%] [G loss: 1.365015]\n",
      "epoch:20 step:18787 [D loss: 0.632756, acc.: 63.28%] [G loss: 1.039014]\n",
      "epoch:20 step:18788 [D loss: 0.432104, acc.: 86.72%] [G loss: 1.698561]\n",
      "epoch:20 step:18789 [D loss: 0.547984, acc.: 71.88%] [G loss: 1.349995]\n",
      "epoch:20 step:18790 [D loss: 0.456730, acc.: 78.91%] [G loss: 1.584182]\n",
      "epoch:20 step:18791 [D loss: 0.501262, acc.: 75.78%] [G loss: 1.350657]\n",
      "epoch:20 step:18792 [D loss: 0.537452, acc.: 77.34%] [G loss: 1.148557]\n",
      "epoch:20 step:18793 [D loss: 0.579973, acc.: 70.31%] [G loss: 1.377245]\n",
      "epoch:20 step:18794 [D loss: 0.399711, acc.: 84.38%] [G loss: 1.884959]\n",
      "epoch:20 step:18795 [D loss: 0.660921, acc.: 62.50%] [G loss: 1.324466]\n",
      "epoch:20 step:18796 [D loss: 0.616236, acc.: 71.09%] [G loss: 1.514867]\n",
      "epoch:20 step:18797 [D loss: 0.538015, acc.: 72.66%] [G loss: 1.743683]\n",
      "epoch:20 step:18798 [D loss: 0.622933, acc.: 67.97%] [G loss: 1.176663]\n",
      "epoch:20 step:18799 [D loss: 0.573407, acc.: 71.09%] [G loss: 1.494203]\n",
      "epoch:20 step:18800 [D loss: 0.521211, acc.: 74.22%] [G loss: 1.607339]\n",
      "##############\n",
      "[2.63677898 1.85629593 1.73843869 2.93928451 0.48767116 5.67150283\n",
      " 2.04897115 2.66838667 3.79966378 5.1639724 ]\n",
      "##########\n",
      "epoch:20 step:18801 [D loss: 0.513147, acc.: 73.44%] [G loss: 1.684397]\n",
      "epoch:20 step:18802 [D loss: 0.523601, acc.: 74.22%] [G loss: 1.120211]\n",
      "epoch:20 step:18803 [D loss: 0.623078, acc.: 62.50%] [G loss: 1.450498]\n",
      "epoch:20 step:18804 [D loss: 0.455512, acc.: 82.81%] [G loss: 1.307799]\n",
      "epoch:20 step:18805 [D loss: 0.454671, acc.: 83.59%] [G loss: 1.759112]\n",
      "epoch:20 step:18806 [D loss: 0.584977, acc.: 67.97%] [G loss: 1.351390]\n",
      "epoch:20 step:18807 [D loss: 0.495833, acc.: 80.47%] [G loss: 1.066423]\n",
      "epoch:20 step:18808 [D loss: 0.442376, acc.: 81.25%] [G loss: 1.677454]\n",
      "epoch:20 step:18809 [D loss: 0.505705, acc.: 79.69%] [G loss: 1.469480]\n",
      "epoch:20 step:18810 [D loss: 0.635672, acc.: 69.53%] [G loss: 1.145296]\n",
      "epoch:20 step:18811 [D loss: 0.558903, acc.: 69.53%] [G loss: 1.415503]\n",
      "epoch:20 step:18812 [D loss: 0.643464, acc.: 62.50%] [G loss: 1.297201]\n",
      "epoch:20 step:18813 [D loss: 0.464523, acc.: 78.12%] [G loss: 1.189424]\n",
      "epoch:20 step:18814 [D loss: 0.412916, acc.: 82.03%] [G loss: 1.437747]\n",
      "epoch:20 step:18815 [D loss: 0.549202, acc.: 70.31%] [G loss: 1.008170]\n",
      "epoch:20 step:18816 [D loss: 0.551124, acc.: 73.44%] [G loss: 1.462993]\n",
      "epoch:20 step:18817 [D loss: 0.530738, acc.: 70.31%] [G loss: 1.161255]\n",
      "epoch:20 step:18818 [D loss: 0.636864, acc.: 60.16%] [G loss: 1.257427]\n",
      "epoch:20 step:18819 [D loss: 0.532525, acc.: 75.00%] [G loss: 1.649441]\n",
      "epoch:20 step:18820 [D loss: 0.576653, acc.: 67.97%] [G loss: 1.312322]\n",
      "epoch:20 step:18821 [D loss: 0.520570, acc.: 72.66%] [G loss: 1.232019]\n",
      "epoch:20 step:18822 [D loss: 0.517161, acc.: 77.34%] [G loss: 1.104179]\n",
      "epoch:20 step:18823 [D loss: 0.778748, acc.: 50.00%] [G loss: 1.246606]\n",
      "epoch:20 step:18824 [D loss: 0.670268, acc.: 60.94%] [G loss: 1.516137]\n",
      "epoch:20 step:18825 [D loss: 0.419129, acc.: 84.38%] [G loss: 1.679932]\n",
      "epoch:20 step:18826 [D loss: 0.669989, acc.: 59.38%] [G loss: 1.295296]\n",
      "epoch:20 step:18827 [D loss: 0.510579, acc.: 72.66%] [G loss: 1.413577]\n",
      "epoch:20 step:18828 [D loss: 0.535396, acc.: 71.09%] [G loss: 1.114236]\n",
      "epoch:20 step:18829 [D loss: 0.533828, acc.: 74.22%] [G loss: 1.602484]\n",
      "epoch:20 step:18830 [D loss: 0.662811, acc.: 63.28%] [G loss: 1.647961]\n",
      "epoch:20 step:18831 [D loss: 0.568927, acc.: 74.22%] [G loss: 1.671128]\n",
      "epoch:20 step:18832 [D loss: 0.653059, acc.: 65.62%] [G loss: 1.350528]\n",
      "epoch:20 step:18833 [D loss: 0.493039, acc.: 78.91%] [G loss: 1.588573]\n",
      "epoch:20 step:18834 [D loss: 0.480556, acc.: 82.81%] [G loss: 1.663459]\n",
      "epoch:20 step:18835 [D loss: 0.581770, acc.: 62.50%] [G loss: 1.265522]\n",
      "epoch:20 step:18836 [D loss: 0.468747, acc.: 83.59%] [G loss: 1.220973]\n",
      "epoch:20 step:18837 [D loss: 0.649559, acc.: 58.59%] [G loss: 1.423799]\n",
      "epoch:20 step:18838 [D loss: 0.547438, acc.: 72.66%] [G loss: 1.145400]\n",
      "epoch:20 step:18839 [D loss: 0.580659, acc.: 70.31%] [G loss: 1.261701]\n",
      "epoch:20 step:18840 [D loss: 0.631649, acc.: 64.84%] [G loss: 1.117812]\n",
      "epoch:20 step:18841 [D loss: 0.542542, acc.: 71.09%] [G loss: 1.491657]\n",
      "epoch:20 step:18842 [D loss: 0.628179, acc.: 67.19%] [G loss: 1.201673]\n",
      "epoch:20 step:18843 [D loss: 0.527473, acc.: 74.22%] [G loss: 1.530790]\n",
      "epoch:20 step:18844 [D loss: 0.534915, acc.: 74.22%] [G loss: 1.553441]\n",
      "epoch:20 step:18845 [D loss: 0.690998, acc.: 60.94%] [G loss: 1.527285]\n",
      "epoch:20 step:18846 [D loss: 0.665665, acc.: 62.50%] [G loss: 1.435153]\n",
      "epoch:20 step:18847 [D loss: 0.563311, acc.: 71.09%] [G loss: 1.452583]\n",
      "epoch:20 step:18848 [D loss: 0.588228, acc.: 66.41%] [G loss: 1.352157]\n",
      "epoch:20 step:18849 [D loss: 0.602700, acc.: 67.97%] [G loss: 1.211760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18850 [D loss: 0.605714, acc.: 64.84%] [G loss: 1.402940]\n",
      "epoch:20 step:18851 [D loss: 0.568525, acc.: 64.84%] [G loss: 1.501765]\n",
      "epoch:20 step:18852 [D loss: 0.547624, acc.: 73.44%] [G loss: 1.486710]\n",
      "epoch:20 step:18853 [D loss: 0.488205, acc.: 79.69%] [G loss: 1.173903]\n",
      "epoch:20 step:18854 [D loss: 0.498098, acc.: 77.34%] [G loss: 1.382443]\n",
      "epoch:20 step:18855 [D loss: 0.512319, acc.: 71.09%] [G loss: 1.387255]\n",
      "epoch:20 step:18856 [D loss: 0.653527, acc.: 66.41%] [G loss: 1.258772]\n",
      "epoch:20 step:18857 [D loss: 0.641133, acc.: 70.31%] [G loss: 1.398140]\n",
      "epoch:20 step:18858 [D loss: 0.690899, acc.: 61.72%] [G loss: 0.736740]\n",
      "epoch:20 step:18859 [D loss: 0.453523, acc.: 82.81%] [G loss: 1.698715]\n",
      "epoch:20 step:18860 [D loss: 0.639447, acc.: 61.72%] [G loss: 1.210742]\n",
      "epoch:20 step:18861 [D loss: 0.760463, acc.: 60.16%] [G loss: 1.271235]\n",
      "epoch:20 step:18862 [D loss: 0.566322, acc.: 71.09%] [G loss: 1.314750]\n",
      "epoch:20 step:18863 [D loss: 0.609577, acc.: 62.50%] [G loss: 1.247914]\n",
      "epoch:20 step:18864 [D loss: 0.685953, acc.: 60.16%] [G loss: 1.206628]\n",
      "epoch:20 step:18865 [D loss: 0.610046, acc.: 62.50%] [G loss: 1.588237]\n",
      "epoch:20 step:18866 [D loss: 0.587091, acc.: 70.31%] [G loss: 1.502421]\n",
      "epoch:20 step:18867 [D loss: 0.553115, acc.: 73.44%] [G loss: 1.126658]\n",
      "epoch:20 step:18868 [D loss: 0.496644, acc.: 76.56%] [G loss: 1.324837]\n",
      "epoch:20 step:18869 [D loss: 0.555756, acc.: 71.88%] [G loss: 1.276240]\n",
      "epoch:20 step:18870 [D loss: 0.538483, acc.: 75.78%] [G loss: 1.589738]\n",
      "epoch:20 step:18871 [D loss: 0.509332, acc.: 79.69%] [G loss: 1.345855]\n",
      "epoch:20 step:18872 [D loss: 0.547885, acc.: 73.44%] [G loss: 1.228023]\n",
      "epoch:20 step:18873 [D loss: 0.524515, acc.: 73.44%] [G loss: 1.174783]\n",
      "epoch:20 step:18874 [D loss: 0.690889, acc.: 62.50%] [G loss: 1.271808]\n",
      "epoch:20 step:18875 [D loss: 0.692586, acc.: 64.06%] [G loss: 1.464068]\n",
      "epoch:20 step:18876 [D loss: 0.678890, acc.: 61.72%] [G loss: 1.495756]\n",
      "epoch:20 step:18877 [D loss: 0.561684, acc.: 71.88%] [G loss: 1.218854]\n",
      "epoch:20 step:18878 [D loss: 0.614387, acc.: 71.09%] [G loss: 1.049563]\n",
      "epoch:20 step:18879 [D loss: 0.618686, acc.: 71.88%] [G loss: 1.043839]\n",
      "epoch:20 step:18880 [D loss: 0.567475, acc.: 71.09%] [G loss: 1.250292]\n",
      "epoch:20 step:18881 [D loss: 0.577974, acc.: 68.75%] [G loss: 1.237558]\n",
      "epoch:20 step:18882 [D loss: 0.503005, acc.: 75.78%] [G loss: 1.290801]\n",
      "epoch:20 step:18883 [D loss: 0.544390, acc.: 71.09%] [G loss: 1.214324]\n",
      "epoch:20 step:18884 [D loss: 0.568541, acc.: 72.66%] [G loss: 1.519588]\n",
      "epoch:20 step:18885 [D loss: 0.613602, acc.: 62.50%] [G loss: 1.030298]\n",
      "epoch:20 step:18886 [D loss: 0.453475, acc.: 78.91%] [G loss: 1.298136]\n",
      "epoch:20 step:18887 [D loss: 0.576491, acc.: 68.75%] [G loss: 1.359056]\n",
      "epoch:20 step:18888 [D loss: 0.503872, acc.: 80.47%] [G loss: 1.335073]\n",
      "epoch:20 step:18889 [D loss: 0.553129, acc.: 71.88%] [G loss: 1.463365]\n",
      "epoch:20 step:18890 [D loss: 0.694872, acc.: 60.16%] [G loss: 1.213678]\n",
      "epoch:20 step:18891 [D loss: 0.553944, acc.: 72.66%] [G loss: 1.336622]\n",
      "epoch:20 step:18892 [D loss: 0.666687, acc.: 60.16%] [G loss: 1.164579]\n",
      "epoch:20 step:18893 [D loss: 0.609906, acc.: 69.53%] [G loss: 1.318186]\n",
      "epoch:20 step:18894 [D loss: 0.574538, acc.: 68.75%] [G loss: 1.141327]\n",
      "epoch:20 step:18895 [D loss: 0.546872, acc.: 70.31%] [G loss: 1.019122]\n",
      "epoch:20 step:18896 [D loss: 0.547858, acc.: 71.88%] [G loss: 1.177037]\n",
      "epoch:20 step:18897 [D loss: 0.671586, acc.: 57.03%] [G loss: 1.176339]\n",
      "epoch:20 step:18898 [D loss: 0.578001, acc.: 71.09%] [G loss: 1.276829]\n",
      "epoch:20 step:18899 [D loss: 0.486894, acc.: 78.12%] [G loss: 1.194306]\n",
      "epoch:20 step:18900 [D loss: 0.502516, acc.: 73.44%] [G loss: 1.057132]\n",
      "epoch:20 step:18901 [D loss: 0.565697, acc.: 69.53%] [G loss: 1.402117]\n",
      "epoch:20 step:18902 [D loss: 0.805041, acc.: 51.56%] [G loss: 1.345612]\n",
      "epoch:20 step:18903 [D loss: 0.639957, acc.: 64.06%] [G loss: 1.537770]\n",
      "epoch:20 step:18904 [D loss: 0.524961, acc.: 74.22%] [G loss: 1.845260]\n",
      "epoch:20 step:18905 [D loss: 0.575772, acc.: 70.31%] [G loss: 1.241965]\n",
      "epoch:20 step:18906 [D loss: 0.521560, acc.: 73.44%] [G loss: 1.191975]\n",
      "epoch:20 step:18907 [D loss: 0.515710, acc.: 73.44%] [G loss: 1.735194]\n",
      "epoch:20 step:18908 [D loss: 0.502865, acc.: 77.34%] [G loss: 1.120388]\n",
      "epoch:20 step:18909 [D loss: 0.676852, acc.: 60.16%] [G loss: 1.138019]\n",
      "epoch:20 step:18910 [D loss: 0.435384, acc.: 83.59%] [G loss: 1.455305]\n",
      "epoch:20 step:18911 [D loss: 0.558961, acc.: 71.88%] [G loss: 1.248406]\n",
      "epoch:20 step:18912 [D loss: 0.467537, acc.: 85.16%] [G loss: 1.476775]\n",
      "epoch:20 step:18913 [D loss: 0.581107, acc.: 68.75%] [G loss: 1.229783]\n",
      "epoch:20 step:18914 [D loss: 0.541327, acc.: 70.31%] [G loss: 1.342244]\n",
      "epoch:20 step:18915 [D loss: 0.543213, acc.: 71.09%] [G loss: 1.409143]\n",
      "epoch:20 step:18916 [D loss: 0.538298, acc.: 75.00%] [G loss: 1.437196]\n",
      "epoch:20 step:18917 [D loss: 0.530091, acc.: 75.78%] [G loss: 1.360001]\n",
      "epoch:20 step:18918 [D loss: 0.429306, acc.: 84.38%] [G loss: 1.308337]\n",
      "epoch:20 step:18919 [D loss: 0.742338, acc.: 58.59%] [G loss: 1.612353]\n",
      "epoch:20 step:18920 [D loss: 0.583114, acc.: 71.88%] [G loss: 1.724901]\n",
      "epoch:20 step:18921 [D loss: 0.615547, acc.: 62.50%] [G loss: 1.409212]\n",
      "epoch:20 step:18922 [D loss: 0.475847, acc.: 81.25%] [G loss: 1.714278]\n",
      "epoch:20 step:18923 [D loss: 0.599205, acc.: 71.09%] [G loss: 1.191600]\n",
      "epoch:20 step:18924 [D loss: 0.713920, acc.: 58.59%] [G loss: 1.234189]\n",
      "epoch:20 step:18925 [D loss: 0.614905, acc.: 68.75%] [G loss: 1.167631]\n",
      "epoch:20 step:18926 [D loss: 0.536595, acc.: 76.56%] [G loss: 1.428598]\n",
      "epoch:20 step:18927 [D loss: 0.437575, acc.: 81.25%] [G loss: 1.414250]\n",
      "epoch:20 step:18928 [D loss: 0.668939, acc.: 60.94%] [G loss: 1.349090]\n",
      "epoch:20 step:18929 [D loss: 0.602328, acc.: 67.97%] [G loss: 0.924530]\n",
      "epoch:20 step:18930 [D loss: 0.654559, acc.: 62.50%] [G loss: 1.225646]\n",
      "epoch:20 step:18931 [D loss: 0.578284, acc.: 75.78%] [G loss: 1.206744]\n",
      "epoch:20 step:18932 [D loss: 0.498442, acc.: 80.47%] [G loss: 1.272110]\n",
      "epoch:20 step:18933 [D loss: 0.460941, acc.: 81.25%] [G loss: 1.291900]\n",
      "epoch:20 step:18934 [D loss: 0.748307, acc.: 50.78%] [G loss: 1.288382]\n",
      "epoch:20 step:18935 [D loss: 0.614345, acc.: 60.94%] [G loss: 1.449020]\n",
      "epoch:20 step:18936 [D loss: 0.524267, acc.: 74.22%] [G loss: 0.970997]\n",
      "epoch:20 step:18937 [D loss: 0.465600, acc.: 80.47%] [G loss: 1.713946]\n",
      "epoch:20 step:18938 [D loss: 0.575611, acc.: 65.62%] [G loss: 1.334879]\n",
      "epoch:20 step:18939 [D loss: 0.516537, acc.: 77.34%] [G loss: 1.735702]\n",
      "epoch:20 step:18940 [D loss: 0.499428, acc.: 78.91%] [G loss: 1.407192]\n",
      "epoch:20 step:18941 [D loss: 0.518716, acc.: 75.78%] [G loss: 1.439797]\n",
      "epoch:20 step:18942 [D loss: 0.492100, acc.: 76.56%] [G loss: 1.145581]\n",
      "epoch:20 step:18943 [D loss: 0.480035, acc.: 78.12%] [G loss: 1.171172]\n",
      "epoch:20 step:18944 [D loss: 0.487020, acc.: 82.03%] [G loss: 1.401870]\n",
      "epoch:20 step:18945 [D loss: 0.588324, acc.: 65.62%] [G loss: 1.290000]\n",
      "epoch:20 step:18946 [D loss: 0.612804, acc.: 63.28%] [G loss: 1.381101]\n",
      "epoch:20 step:18947 [D loss: 0.684672, acc.: 60.94%] [G loss: 1.226399]\n",
      "epoch:20 step:18948 [D loss: 0.496217, acc.: 74.22%] [G loss: 1.412945]\n",
      "epoch:20 step:18949 [D loss: 0.634017, acc.: 67.97%] [G loss: 1.497685]\n",
      "epoch:20 step:18950 [D loss: 0.463966, acc.: 79.69%] [G loss: 1.405620]\n",
      "epoch:20 step:18951 [D loss: 0.569023, acc.: 68.75%] [G loss: 1.377480]\n",
      "epoch:20 step:18952 [D loss: 0.574547, acc.: 66.41%] [G loss: 1.608618]\n",
      "epoch:20 step:18953 [D loss: 0.513593, acc.: 72.66%] [G loss: 1.464071]\n",
      "epoch:20 step:18954 [D loss: 0.667141, acc.: 57.81%] [G loss: 1.298195]\n",
      "epoch:20 step:18955 [D loss: 0.591604, acc.: 65.62%] [G loss: 1.402074]\n",
      "epoch:20 step:18956 [D loss: 0.520798, acc.: 76.56%] [G loss: 1.486257]\n",
      "epoch:20 step:18957 [D loss: 0.513853, acc.: 76.56%] [G loss: 1.178382]\n",
      "epoch:20 step:18958 [D loss: 0.569862, acc.: 70.31%] [G loss: 1.120916]\n",
      "epoch:20 step:18959 [D loss: 0.521847, acc.: 72.66%] [G loss: 1.073232]\n",
      "epoch:20 step:18960 [D loss: 0.347707, acc.: 92.97%] [G loss: 1.539038]\n",
      "epoch:20 step:18961 [D loss: 0.651017, acc.: 60.16%] [G loss: 1.209005]\n",
      "epoch:20 step:18962 [D loss: 0.771597, acc.: 56.25%] [G loss: 1.564126]\n",
      "epoch:20 step:18963 [D loss: 0.492040, acc.: 77.34%] [G loss: 1.372611]\n",
      "epoch:20 step:18964 [D loss: 0.826344, acc.: 48.44%] [G loss: 1.281205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18965 [D loss: 0.563484, acc.: 75.00%] [G loss: 1.113167]\n",
      "epoch:20 step:18966 [D loss: 0.724483, acc.: 59.38%] [G loss: 1.358455]\n",
      "epoch:20 step:18967 [D loss: 0.495610, acc.: 82.03%] [G loss: 1.281027]\n",
      "epoch:20 step:18968 [D loss: 0.525294, acc.: 71.88%] [G loss: 1.396731]\n",
      "epoch:20 step:18969 [D loss: 0.614285, acc.: 65.62%] [G loss: 1.280525]\n",
      "epoch:20 step:18970 [D loss: 0.597667, acc.: 71.88%] [G loss: 1.133105]\n",
      "epoch:20 step:18971 [D loss: 0.413084, acc.: 84.38%] [G loss: 1.587945]\n",
      "epoch:20 step:18972 [D loss: 0.623101, acc.: 61.72%] [G loss: 1.264297]\n",
      "epoch:20 step:18973 [D loss: 0.556798, acc.: 71.09%] [G loss: 1.381541]\n",
      "epoch:20 step:18974 [D loss: 0.626705, acc.: 65.62%] [G loss: 1.436929]\n",
      "epoch:20 step:18975 [D loss: 0.551264, acc.: 71.09%] [G loss: 0.997679]\n",
      "epoch:20 step:18976 [D loss: 0.533115, acc.: 74.22%] [G loss: 1.153527]\n",
      "epoch:20 step:18977 [D loss: 0.559559, acc.: 68.75%] [G loss: 1.525811]\n",
      "epoch:20 step:18978 [D loss: 0.650530, acc.: 67.19%] [G loss: 1.380451]\n",
      "epoch:20 step:18979 [D loss: 0.667692, acc.: 63.28%] [G loss: 1.153803]\n",
      "epoch:20 step:18980 [D loss: 0.601344, acc.: 70.31%] [G loss: 1.099736]\n",
      "epoch:20 step:18981 [D loss: 0.523262, acc.: 77.34%] [G loss: 1.499391]\n",
      "epoch:20 step:18982 [D loss: 0.641816, acc.: 61.72%] [G loss: 1.200843]\n",
      "epoch:20 step:18983 [D loss: 0.606868, acc.: 67.97%] [G loss: 1.054499]\n",
      "epoch:20 step:18984 [D loss: 0.537474, acc.: 75.78%] [G loss: 1.078513]\n",
      "epoch:20 step:18985 [D loss: 0.508474, acc.: 80.47%] [G loss: 1.098641]\n",
      "epoch:20 step:18986 [D loss: 0.479348, acc.: 77.34%] [G loss: 1.299005]\n",
      "epoch:20 step:18987 [D loss: 0.466749, acc.: 75.78%] [G loss: 1.658827]\n",
      "epoch:20 step:18988 [D loss: 0.512665, acc.: 75.00%] [G loss: 1.362305]\n",
      "epoch:20 step:18989 [D loss: 0.413325, acc.: 82.03%] [G loss: 1.361025]\n",
      "epoch:20 step:18990 [D loss: 0.519310, acc.: 75.78%] [G loss: 1.512640]\n",
      "epoch:20 step:18991 [D loss: 0.516498, acc.: 75.78%] [G loss: 1.361175]\n",
      "epoch:20 step:18992 [D loss: 0.491899, acc.: 79.69%] [G loss: 1.732206]\n",
      "epoch:20 step:18993 [D loss: 0.571069, acc.: 68.75%] [G loss: 1.446608]\n",
      "epoch:20 step:18994 [D loss: 0.646582, acc.: 63.28%] [G loss: 1.510778]\n",
      "epoch:20 step:18995 [D loss: 0.716693, acc.: 54.69%] [G loss: 1.208639]\n",
      "epoch:20 step:18996 [D loss: 0.615990, acc.: 63.28%] [G loss: 1.011394]\n",
      "epoch:20 step:18997 [D loss: 0.397829, acc.: 86.72%] [G loss: 1.489469]\n",
      "epoch:20 step:18998 [D loss: 0.634513, acc.: 63.28%] [G loss: 1.207367]\n",
      "epoch:20 step:18999 [D loss: 0.701039, acc.: 61.72%] [G loss: 1.129934]\n",
      "epoch:20 step:19000 [D loss: 0.642796, acc.: 60.16%] [G loss: 1.133568]\n",
      "##############\n",
      "[2.58462417 1.8957596  1.85204897 2.91587824 0.6461282  7.22098731\n",
      " 2.34719323 3.04137141 3.83689794 5.16704455]\n",
      "##########\n",
      "epoch:20 step:19001 [D loss: 0.460280, acc.: 77.34%] [G loss: 1.587118]\n",
      "epoch:20 step:19002 [D loss: 0.594199, acc.: 73.44%] [G loss: 1.296580]\n",
      "epoch:20 step:19003 [D loss: 0.658695, acc.: 62.50%] [G loss: 1.473349]\n",
      "epoch:20 step:19004 [D loss: 0.521206, acc.: 73.44%] [G loss: 1.071580]\n",
      "epoch:20 step:19005 [D loss: 0.367929, acc.: 89.84%] [G loss: 1.483654]\n",
      "epoch:20 step:19006 [D loss: 0.529427, acc.: 73.44%] [G loss: 1.197822]\n",
      "epoch:20 step:19007 [D loss: 0.451956, acc.: 81.25%] [G loss: 1.418089]\n",
      "epoch:20 step:19008 [D loss: 0.712944, acc.: 58.59%] [G loss: 1.140172]\n",
      "epoch:20 step:19009 [D loss: 0.481724, acc.: 78.12%] [G loss: 1.476813]\n",
      "epoch:20 step:19010 [D loss: 0.534514, acc.: 73.44%] [G loss: 1.632905]\n",
      "epoch:20 step:19011 [D loss: 0.549507, acc.: 71.09%] [G loss: 1.465323]\n",
      "epoch:20 step:19012 [D loss: 0.568334, acc.: 69.53%] [G loss: 1.260758]\n",
      "epoch:20 step:19013 [D loss: 0.563457, acc.: 73.44%] [G loss: 1.475736]\n",
      "epoch:20 step:19014 [D loss: 0.706657, acc.: 59.38%] [G loss: 1.526462]\n",
      "epoch:20 step:19015 [D loss: 0.602969, acc.: 64.84%] [G loss: 1.172908]\n",
      "epoch:20 step:19016 [D loss: 0.666857, acc.: 61.72%] [G loss: 0.996744]\n",
      "epoch:20 step:19017 [D loss: 0.412870, acc.: 85.94%] [G loss: 1.206777]\n",
      "epoch:20 step:19018 [D loss: 0.746141, acc.: 53.12%] [G loss: 1.090371]\n",
      "epoch:20 step:19019 [D loss: 0.491053, acc.: 78.12%] [G loss: 1.281537]\n",
      "epoch:20 step:19020 [D loss: 0.589756, acc.: 64.06%] [G loss: 1.036601]\n",
      "epoch:20 step:19021 [D loss: 0.495618, acc.: 75.00%] [G loss: 1.176087]\n",
      "epoch:20 step:19022 [D loss: 0.664279, acc.: 60.16%] [G loss: 1.332242]\n",
      "epoch:20 step:19023 [D loss: 0.453488, acc.: 80.47%] [G loss: 1.038376]\n",
      "epoch:20 step:19024 [D loss: 0.497136, acc.: 78.12%] [G loss: 1.471730]\n",
      "epoch:20 step:19025 [D loss: 0.567565, acc.: 71.09%] [G loss: 1.493211]\n",
      "epoch:20 step:19026 [D loss: 0.607709, acc.: 66.41%] [G loss: 1.501941]\n",
      "epoch:20 step:19027 [D loss: 0.485767, acc.: 75.78%] [G loss: 1.252724]\n",
      "epoch:20 step:19028 [D loss: 0.570146, acc.: 72.66%] [G loss: 1.360022]\n",
      "epoch:20 step:19029 [D loss: 0.596515, acc.: 67.97%] [G loss: 1.121157]\n",
      "epoch:20 step:19030 [D loss: 0.514744, acc.: 76.56%] [G loss: 1.255817]\n",
      "epoch:20 step:19031 [D loss: 0.623848, acc.: 60.94%] [G loss: 1.405301]\n",
      "epoch:20 step:19032 [D loss: 0.574421, acc.: 68.75%] [G loss: 1.356552]\n",
      "epoch:20 step:19033 [D loss: 0.679100, acc.: 61.72%] [G loss: 1.149092]\n",
      "epoch:20 step:19034 [D loss: 0.355303, acc.: 89.06%] [G loss: 1.294427]\n",
      "epoch:20 step:19035 [D loss: 0.578909, acc.: 66.41%] [G loss: 1.233332]\n",
      "epoch:20 step:19036 [D loss: 0.513553, acc.: 73.44%] [G loss: 1.028023]\n",
      "epoch:20 step:19037 [D loss: 0.586148, acc.: 67.19%] [G loss: 1.498513]\n",
      "epoch:20 step:19038 [D loss: 0.597619, acc.: 65.62%] [G loss: 1.417711]\n",
      "epoch:20 step:19039 [D loss: 0.586922, acc.: 67.19%] [G loss: 1.361264]\n",
      "epoch:20 step:19040 [D loss: 0.502723, acc.: 70.31%] [G loss: 0.939642]\n",
      "epoch:20 step:19041 [D loss: 0.647155, acc.: 58.59%] [G loss: 1.152207]\n",
      "epoch:20 step:19042 [D loss: 0.619194, acc.: 64.06%] [G loss: 1.110905]\n",
      "epoch:20 step:19043 [D loss: 0.616185, acc.: 63.28%] [G loss: 1.397067]\n",
      "epoch:20 step:19044 [D loss: 0.478162, acc.: 81.25%] [G loss: 1.434522]\n",
      "epoch:20 step:19045 [D loss: 0.624735, acc.: 64.06%] [G loss: 1.334793]\n",
      "epoch:20 step:19046 [D loss: 0.511722, acc.: 75.78%] [G loss: 1.212227]\n",
      "epoch:20 step:19047 [D loss: 0.551284, acc.: 69.53%] [G loss: 1.101689]\n",
      "epoch:20 step:19048 [D loss: 0.510567, acc.: 75.00%] [G loss: 1.227841]\n",
      "epoch:20 step:19049 [D loss: 0.552652, acc.: 75.78%] [G loss: 1.267288]\n",
      "epoch:20 step:19050 [D loss: 0.641823, acc.: 69.53%] [G loss: 1.434094]\n",
      "epoch:20 step:19051 [D loss: 0.422870, acc.: 81.25%] [G loss: 1.186683]\n",
      "epoch:20 step:19052 [D loss: 0.555988, acc.: 67.19%] [G loss: 1.429980]\n",
      "epoch:20 step:19053 [D loss: 0.646216, acc.: 63.28%] [G loss: 1.121081]\n",
      "epoch:20 step:19054 [D loss: 0.437129, acc.: 82.03%] [G loss: 1.617071]\n",
      "epoch:20 step:19055 [D loss: 0.473031, acc.: 79.69%] [G loss: 1.308124]\n",
      "epoch:20 step:19056 [D loss: 0.796771, acc.: 53.91%] [G loss: 1.073776]\n",
      "epoch:20 step:19057 [D loss: 0.582023, acc.: 64.84%] [G loss: 1.180718]\n",
      "epoch:20 step:19058 [D loss: 0.518045, acc.: 78.12%] [G loss: 1.371385]\n",
      "epoch:20 step:19059 [D loss: 0.681647, acc.: 58.59%] [G loss: 1.205432]\n",
      "epoch:20 step:19060 [D loss: 0.511097, acc.: 75.00%] [G loss: 1.306702]\n",
      "epoch:20 step:19061 [D loss: 0.664320, acc.: 64.84%] [G loss: 1.420944]\n",
      "epoch:20 step:19062 [D loss: 0.515088, acc.: 76.56%] [G loss: 1.240187]\n",
      "epoch:20 step:19063 [D loss: 0.621135, acc.: 64.84%] [G loss: 1.011916]\n",
      "epoch:20 step:19064 [D loss: 0.475193, acc.: 78.91%] [G loss: 1.482903]\n",
      "epoch:20 step:19065 [D loss: 0.471772, acc.: 76.56%] [G loss: 1.412623]\n",
      "epoch:20 step:19066 [D loss: 0.586208, acc.: 66.41%] [G loss: 1.392432]\n",
      "epoch:20 step:19067 [D loss: 0.492679, acc.: 79.69%] [G loss: 1.444074]\n",
      "epoch:20 step:19068 [D loss: 0.370446, acc.: 88.28%] [G loss: 1.501932]\n",
      "epoch:20 step:19069 [D loss: 0.439720, acc.: 85.16%] [G loss: 1.534209]\n",
      "epoch:20 step:19070 [D loss: 0.561389, acc.: 73.44%] [G loss: 1.242798]\n",
      "epoch:20 step:19071 [D loss: 0.702561, acc.: 64.06%] [G loss: 1.171519]\n",
      "epoch:20 step:19072 [D loss: 0.541486, acc.: 73.44%] [G loss: 1.250772]\n",
      "epoch:20 step:19073 [D loss: 0.637637, acc.: 59.38%] [G loss: 1.389345]\n",
      "epoch:20 step:19074 [D loss: 0.589673, acc.: 64.84%] [G loss: 1.290880]\n",
      "epoch:20 step:19075 [D loss: 0.418316, acc.: 79.69%] [G loss: 1.269424]\n",
      "epoch:20 step:19076 [D loss: 0.470920, acc.: 78.91%] [G loss: 1.534243]\n",
      "epoch:20 step:19077 [D loss: 0.562064, acc.: 75.78%] [G loss: 1.413795]\n",
      "epoch:20 step:19078 [D loss: 0.504899, acc.: 76.56%] [G loss: 1.271211]\n",
      "epoch:20 step:19079 [D loss: 0.553981, acc.: 72.66%] [G loss: 1.448482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19080 [D loss: 0.512553, acc.: 78.12%] [G loss: 1.230821]\n",
      "epoch:20 step:19081 [D loss: 0.722649, acc.: 49.22%] [G loss: 1.587765]\n",
      "epoch:20 step:19082 [D loss: 0.452648, acc.: 78.91%] [G loss: 1.270243]\n",
      "epoch:20 step:19083 [D loss: 0.532597, acc.: 71.09%] [G loss: 1.405413]\n",
      "epoch:20 step:19084 [D loss: 0.593247, acc.: 69.53%] [G loss: 1.316029]\n",
      "epoch:20 step:19085 [D loss: 0.522299, acc.: 77.34%] [G loss: 1.283496]\n",
      "epoch:20 step:19086 [D loss: 0.605786, acc.: 66.41%] [G loss: 1.443114]\n",
      "epoch:20 step:19087 [D loss: 0.581914, acc.: 71.09%] [G loss: 1.244048]\n",
      "epoch:20 step:19088 [D loss: 0.714823, acc.: 60.16%] [G loss: 1.128362]\n",
      "epoch:20 step:19089 [D loss: 0.645583, acc.: 60.94%] [G loss: 1.045046]\n",
      "epoch:20 step:19090 [D loss: 0.414826, acc.: 81.25%] [G loss: 1.370084]\n",
      "epoch:20 step:19091 [D loss: 0.664456, acc.: 62.50%] [G loss: 1.399703]\n",
      "epoch:20 step:19092 [D loss: 0.613257, acc.: 64.84%] [G loss: 1.112034]\n",
      "epoch:20 step:19093 [D loss: 0.432603, acc.: 79.69%] [G loss: 1.157439]\n",
      "epoch:20 step:19094 [D loss: 0.424124, acc.: 82.03%] [G loss: 1.470114]\n",
      "epoch:20 step:19095 [D loss: 0.615063, acc.: 70.31%] [G loss: 1.287047]\n",
      "epoch:20 step:19096 [D loss: 0.658921, acc.: 65.62%] [G loss: 1.368567]\n",
      "epoch:20 step:19097 [D loss: 0.537727, acc.: 75.78%] [G loss: 1.504256]\n",
      "epoch:20 step:19098 [D loss: 0.610887, acc.: 67.97%] [G loss: 1.315722]\n",
      "epoch:20 step:19099 [D loss: 0.520332, acc.: 76.56%] [G loss: 1.380229]\n",
      "epoch:20 step:19100 [D loss: 0.488385, acc.: 76.56%] [G loss: 1.457406]\n",
      "epoch:20 step:19101 [D loss: 0.520235, acc.: 77.34%] [G loss: 1.310409]\n",
      "epoch:20 step:19102 [D loss: 0.555851, acc.: 73.44%] [G loss: 1.331662]\n",
      "epoch:20 step:19103 [D loss: 0.451183, acc.: 83.59%] [G loss: 1.247970]\n",
      "epoch:20 step:19104 [D loss: 0.578471, acc.: 71.09%] [G loss: 1.186364]\n",
      "epoch:20 step:19105 [D loss: 0.563548, acc.: 67.19%] [G loss: 1.393589]\n",
      "epoch:20 step:19106 [D loss: 0.439699, acc.: 82.81%] [G loss: 1.242503]\n",
      "epoch:20 step:19107 [D loss: 0.529691, acc.: 74.22%] [G loss: 1.547195]\n",
      "epoch:20 step:19108 [D loss: 0.539562, acc.: 73.44%] [G loss: 1.299621]\n",
      "epoch:20 step:19109 [D loss: 0.528996, acc.: 75.78%] [G loss: 1.664444]\n",
      "epoch:20 step:19110 [D loss: 0.653835, acc.: 56.25%] [G loss: 1.383585]\n",
      "epoch:20 step:19111 [D loss: 0.528014, acc.: 79.69%] [G loss: 1.298614]\n",
      "epoch:20 step:19112 [D loss: 0.672968, acc.: 63.28%] [G loss: 1.151508]\n",
      "epoch:20 step:19113 [D loss: 0.589164, acc.: 67.19%] [G loss: 1.452256]\n",
      "epoch:20 step:19114 [D loss: 0.827810, acc.: 40.62%] [G loss: 0.969490]\n",
      "epoch:20 step:19115 [D loss: 0.613992, acc.: 64.84%] [G loss: 1.168241]\n",
      "epoch:20 step:19116 [D loss: 0.735861, acc.: 58.59%] [G loss: 1.289236]\n",
      "epoch:20 step:19117 [D loss: 0.458600, acc.: 78.12%] [G loss: 1.066076]\n",
      "epoch:20 step:19118 [D loss: 0.468876, acc.: 78.91%] [G loss: 1.187935]\n",
      "epoch:20 step:19119 [D loss: 0.535981, acc.: 73.44%] [G loss: 1.465027]\n",
      "epoch:20 step:19120 [D loss: 0.627677, acc.: 64.84%] [G loss: 1.187432]\n",
      "epoch:20 step:19121 [D loss: 0.580044, acc.: 69.53%] [G loss: 1.232409]\n",
      "epoch:20 step:19122 [D loss: 0.615291, acc.: 64.84%] [G loss: 1.537144]\n",
      "epoch:20 step:19123 [D loss: 0.598414, acc.: 66.41%] [G loss: 1.238713]\n",
      "epoch:20 step:19124 [D loss: 0.528342, acc.: 72.66%] [G loss: 1.294484]\n",
      "epoch:20 step:19125 [D loss: 0.741908, acc.: 55.47%] [G loss: 1.074159]\n",
      "epoch:20 step:19126 [D loss: 0.570921, acc.: 67.97%] [G loss: 1.571572]\n",
      "epoch:20 step:19127 [D loss: 0.553229, acc.: 67.97%] [G loss: 1.800231]\n",
      "epoch:20 step:19128 [D loss: 0.670068, acc.: 62.50%] [G loss: 1.395222]\n",
      "epoch:20 step:19129 [D loss: 0.674568, acc.: 63.28%] [G loss: 1.113337]\n",
      "epoch:20 step:19130 [D loss: 0.462913, acc.: 79.69%] [G loss: 1.213429]\n",
      "epoch:20 step:19131 [D loss: 0.708419, acc.: 54.69%] [G loss: 1.127858]\n",
      "epoch:20 step:19132 [D loss: 0.562298, acc.: 67.97%] [G loss: 1.289124]\n",
      "epoch:20 step:19133 [D loss: 0.561917, acc.: 71.09%] [G loss: 1.303555]\n",
      "epoch:20 step:19134 [D loss: 0.592081, acc.: 69.53%] [G loss: 1.098856]\n",
      "epoch:20 step:19135 [D loss: 0.605220, acc.: 71.09%] [G loss: 1.353641]\n",
      "epoch:20 step:19136 [D loss: 0.803150, acc.: 45.31%] [G loss: 1.461369]\n",
      "epoch:20 step:19137 [D loss: 0.485875, acc.: 78.91%] [G loss: 1.676685]\n",
      "epoch:20 step:19138 [D loss: 0.647941, acc.: 60.94%] [G loss: 1.398224]\n",
      "epoch:20 step:19139 [D loss: 0.668976, acc.: 60.94%] [G loss: 1.458725]\n",
      "epoch:20 step:19140 [D loss: 0.527557, acc.: 75.00%] [G loss: 1.373208]\n",
      "epoch:20 step:19141 [D loss: 0.713326, acc.: 54.69%] [G loss: 1.290563]\n",
      "epoch:20 step:19142 [D loss: 0.572283, acc.: 67.97%] [G loss: 1.182906]\n",
      "epoch:20 step:19143 [D loss: 0.424705, acc.: 79.69%] [G loss: 1.534363]\n",
      "epoch:20 step:19144 [D loss: 0.572550, acc.: 71.09%] [G loss: 1.601773]\n",
      "epoch:20 step:19145 [D loss: 0.544586, acc.: 72.66%] [G loss: 1.528582]\n",
      "epoch:20 step:19146 [D loss: 0.562957, acc.: 68.75%] [G loss: 1.684051]\n",
      "epoch:20 step:19147 [D loss: 0.796167, acc.: 50.00%] [G loss: 1.293904]\n",
      "epoch:20 step:19148 [D loss: 0.451727, acc.: 79.69%] [G loss: 1.674518]\n",
      "epoch:20 step:19149 [D loss: 0.582093, acc.: 73.44%] [G loss: 1.519601]\n",
      "epoch:20 step:19150 [D loss: 0.537025, acc.: 75.00%] [G loss: 1.261895]\n",
      "epoch:20 step:19151 [D loss: 0.655631, acc.: 60.94%] [G loss: 1.277994]\n",
      "epoch:20 step:19152 [D loss: 0.511314, acc.: 75.00%] [G loss: 1.296134]\n",
      "epoch:20 step:19153 [D loss: 0.446508, acc.: 78.91%] [G loss: 1.612122]\n",
      "epoch:20 step:19154 [D loss: 0.578297, acc.: 71.09%] [G loss: 1.505303]\n",
      "epoch:20 step:19155 [D loss: 0.502032, acc.: 80.47%] [G loss: 1.009988]\n",
      "epoch:20 step:19156 [D loss: 0.657165, acc.: 60.16%] [G loss: 1.130823]\n",
      "epoch:20 step:19157 [D loss: 0.561301, acc.: 72.66%] [G loss: 1.238707]\n",
      "epoch:20 step:19158 [D loss: 0.539135, acc.: 70.31%] [G loss: 1.262437]\n",
      "epoch:20 step:19159 [D loss: 0.396662, acc.: 82.81%] [G loss: 1.351295]\n",
      "epoch:20 step:19160 [D loss: 0.451385, acc.: 84.38%] [G loss: 1.760513]\n",
      "epoch:20 step:19161 [D loss: 0.695860, acc.: 58.59%] [G loss: 1.316151]\n",
      "epoch:20 step:19162 [D loss: 0.588232, acc.: 64.84%] [G loss: 1.442482]\n",
      "epoch:20 step:19163 [D loss: 0.642230, acc.: 60.16%] [G loss: 1.142735]\n",
      "epoch:20 step:19164 [D loss: 0.528912, acc.: 73.44%] [G loss: 1.428372]\n",
      "epoch:20 step:19165 [D loss: 0.538138, acc.: 74.22%] [G loss: 1.263448]\n",
      "epoch:20 step:19166 [D loss: 0.530616, acc.: 72.66%] [G loss: 1.555641]\n",
      "epoch:20 step:19167 [D loss: 0.666648, acc.: 59.38%] [G loss: 1.147412]\n",
      "epoch:20 step:19168 [D loss: 0.541366, acc.: 69.53%] [G loss: 1.205459]\n",
      "epoch:20 step:19169 [D loss: 0.517883, acc.: 75.78%] [G loss: 1.201651]\n",
      "epoch:20 step:19170 [D loss: 0.667148, acc.: 61.72%] [G loss: 1.452517]\n",
      "epoch:20 step:19171 [D loss: 0.579225, acc.: 73.44%] [G loss: 1.328230]\n",
      "epoch:20 step:19172 [D loss: 0.585221, acc.: 67.97%] [G loss: 1.187706]\n",
      "epoch:20 step:19173 [D loss: 0.537263, acc.: 73.44%] [G loss: 1.271030]\n",
      "epoch:20 step:19174 [D loss: 0.565299, acc.: 72.66%] [G loss: 1.640459]\n",
      "epoch:20 step:19175 [D loss: 0.564901, acc.: 71.09%] [G loss: 1.223853]\n",
      "epoch:20 step:19176 [D loss: 0.572897, acc.: 70.31%] [G loss: 1.507944]\n",
      "epoch:20 step:19177 [D loss: 0.474242, acc.: 80.47%] [G loss: 1.272044]\n",
      "epoch:20 step:19178 [D loss: 0.737564, acc.: 56.25%] [G loss: 1.187325]\n",
      "epoch:20 step:19179 [D loss: 0.634654, acc.: 61.72%] [G loss: 1.458004]\n",
      "epoch:20 step:19180 [D loss: 0.536192, acc.: 73.44%] [G loss: 1.275971]\n",
      "epoch:20 step:19181 [D loss: 0.551453, acc.: 68.75%] [G loss: 1.248735]\n",
      "epoch:20 step:19182 [D loss: 0.628977, acc.: 61.72%] [G loss: 1.407918]\n",
      "epoch:20 step:19183 [D loss: 0.584567, acc.: 66.41%] [G loss: 1.397238]\n",
      "epoch:20 step:19184 [D loss: 0.633946, acc.: 63.28%] [G loss: 1.553397]\n",
      "epoch:20 step:19185 [D loss: 0.524887, acc.: 75.00%] [G loss: 1.318009]\n",
      "epoch:20 step:19186 [D loss: 0.438332, acc.: 82.81%] [G loss: 1.169379]\n",
      "epoch:20 step:19187 [D loss: 0.411854, acc.: 85.16%] [G loss: 1.223681]\n",
      "epoch:20 step:19188 [D loss: 0.569274, acc.: 64.84%] [G loss: 1.246996]\n",
      "epoch:20 step:19189 [D loss: 0.655632, acc.: 64.06%] [G loss: 1.219000]\n",
      "epoch:20 step:19190 [D loss: 0.521991, acc.: 76.56%] [G loss: 1.346494]\n",
      "epoch:20 step:19191 [D loss: 0.542145, acc.: 70.31%] [G loss: 1.383333]\n",
      "epoch:20 step:19192 [D loss: 0.453932, acc.: 82.81%] [G loss: 1.629363]\n",
      "epoch:20 step:19193 [D loss: 0.548861, acc.: 71.09%] [G loss: 1.319046]\n",
      "epoch:20 step:19194 [D loss: 0.552304, acc.: 67.19%] [G loss: 1.171073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19195 [D loss: 0.491833, acc.: 77.34%] [G loss: 1.372349]\n",
      "epoch:20 step:19196 [D loss: 0.655136, acc.: 64.84%] [G loss: 1.169545]\n",
      "epoch:20 step:19197 [D loss: 0.661189, acc.: 62.50%] [G loss: 1.290272]\n",
      "epoch:20 step:19198 [D loss: 0.419807, acc.: 82.03%] [G loss: 1.320084]\n",
      "epoch:20 step:19199 [D loss: 0.490915, acc.: 78.91%] [G loss: 1.429634]\n",
      "epoch:20 step:19200 [D loss: 0.522844, acc.: 73.44%] [G loss: 1.419264]\n",
      "##############\n",
      "[2.87870582 2.08266579 2.28563928 2.73166246 0.81847435 6.10598573\n",
      " 2.321695   2.33161939 3.98100282 7.14868929]\n",
      "##########\n",
      "epoch:20 step:19201 [D loss: 0.716601, acc.: 60.94%] [G loss: 1.110200]\n",
      "epoch:20 step:19202 [D loss: 0.611097, acc.: 70.31%] [G loss: 1.135740]\n",
      "epoch:20 step:19203 [D loss: 0.700301, acc.: 55.47%] [G loss: 0.830075]\n",
      "epoch:20 step:19204 [D loss: 0.752304, acc.: 51.56%] [G loss: 1.015340]\n",
      "epoch:20 step:19205 [D loss: 0.654716, acc.: 61.72%] [G loss: 1.315459]\n",
      "epoch:20 step:19206 [D loss: 0.547547, acc.: 74.22%] [G loss: 1.737361]\n",
      "epoch:20 step:19207 [D loss: 0.558158, acc.: 72.66%] [G loss: 1.489988]\n",
      "epoch:20 step:19208 [D loss: 0.588651, acc.: 69.53%] [G loss: 1.519518]\n",
      "epoch:20 step:19209 [D loss: 0.557247, acc.: 66.41%] [G loss: 1.321236]\n",
      "epoch:20 step:19210 [D loss: 0.812756, acc.: 49.22%] [G loss: 1.032739]\n",
      "epoch:20 step:19211 [D loss: 0.527575, acc.: 77.34%] [G loss: 1.654289]\n",
      "epoch:20 step:19212 [D loss: 0.590965, acc.: 70.31%] [G loss: 1.157028]\n",
      "epoch:20 step:19213 [D loss: 0.574069, acc.: 67.97%] [G loss: 1.450764]\n",
      "epoch:20 step:19214 [D loss: 0.659020, acc.: 64.84%] [G loss: 0.977221]\n",
      "epoch:20 step:19215 [D loss: 0.528113, acc.: 74.22%] [G loss: 1.559593]\n",
      "epoch:20 step:19216 [D loss: 0.462437, acc.: 83.59%] [G loss: 1.721311]\n",
      "epoch:20 step:19217 [D loss: 0.535205, acc.: 70.31%] [G loss: 1.197513]\n",
      "epoch:20 step:19218 [D loss: 0.645305, acc.: 64.84%] [G loss: 1.250185]\n",
      "epoch:20 step:19219 [D loss: 0.633936, acc.: 68.75%] [G loss: 1.206911]\n",
      "epoch:20 step:19220 [D loss: 0.606822, acc.: 70.31%] [G loss: 1.460182]\n",
      "epoch:20 step:19221 [D loss: 0.819194, acc.: 44.53%] [G loss: 0.969793]\n",
      "epoch:20 step:19222 [D loss: 0.540142, acc.: 72.66%] [G loss: 1.481920]\n",
      "epoch:20 step:19223 [D loss: 0.616480, acc.: 60.94%] [G loss: 1.402931]\n",
      "epoch:20 step:19224 [D loss: 0.561356, acc.: 67.97%] [G loss: 1.458237]\n",
      "epoch:20 step:19225 [D loss: 0.652194, acc.: 59.38%] [G loss: 1.130318]\n",
      "epoch:20 step:19226 [D loss: 0.450223, acc.: 81.25%] [G loss: 1.386769]\n",
      "epoch:20 step:19227 [D loss: 0.356646, acc.: 88.28%] [G loss: 1.594903]\n",
      "epoch:20 step:19228 [D loss: 0.449636, acc.: 79.69%] [G loss: 1.363331]\n",
      "epoch:20 step:19229 [D loss: 0.570683, acc.: 69.53%] [G loss: 1.326942]\n",
      "epoch:20 step:19230 [D loss: 0.423708, acc.: 84.38%] [G loss: 1.585021]\n",
      "epoch:20 step:19231 [D loss: 0.464446, acc.: 78.12%] [G loss: 1.488556]\n",
      "epoch:20 step:19232 [D loss: 0.643787, acc.: 64.84%] [G loss: 1.172512]\n",
      "epoch:20 step:19233 [D loss: 0.563962, acc.: 69.53%] [G loss: 1.525282]\n",
      "epoch:20 step:19234 [D loss: 0.552729, acc.: 71.88%] [G loss: 1.349094]\n",
      "epoch:20 step:19235 [D loss: 0.459889, acc.: 79.69%] [G loss: 1.482707]\n",
      "epoch:20 step:19236 [D loss: 0.520717, acc.: 73.44%] [G loss: 1.356863]\n",
      "epoch:20 step:19237 [D loss: 0.556209, acc.: 67.19%] [G loss: 1.506473]\n",
      "epoch:20 step:19238 [D loss: 0.447976, acc.: 78.91%] [G loss: 1.266373]\n",
      "epoch:20 step:19239 [D loss: 0.582877, acc.: 73.44%] [G loss: 1.141388]\n",
      "epoch:20 step:19240 [D loss: 0.492685, acc.: 78.12%] [G loss: 1.387081]\n",
      "epoch:20 step:19241 [D loss: 0.535627, acc.: 71.88%] [G loss: 1.223850]\n",
      "epoch:20 step:19242 [D loss: 0.637666, acc.: 64.84%] [G loss: 1.323852]\n",
      "epoch:20 step:19243 [D loss: 0.484360, acc.: 75.00%] [G loss: 1.493923]\n",
      "epoch:20 step:19244 [D loss: 0.536417, acc.: 69.53%] [G loss: 1.261120]\n",
      "epoch:20 step:19245 [D loss: 0.499759, acc.: 81.25%] [G loss: 1.236848]\n",
      "epoch:20 step:19246 [D loss: 0.473136, acc.: 76.56%] [G loss: 1.729685]\n",
      "epoch:20 step:19247 [D loss: 0.626119, acc.: 68.75%] [G loss: 1.427532]\n",
      "epoch:20 step:19248 [D loss: 0.559863, acc.: 68.75%] [G loss: 1.194702]\n",
      "epoch:20 step:19249 [D loss: 0.614999, acc.: 68.75%] [G loss: 1.352358]\n",
      "epoch:20 step:19250 [D loss: 0.635369, acc.: 63.28%] [G loss: 1.322622]\n",
      "epoch:20 step:19251 [D loss: 0.493875, acc.: 74.22%] [G loss: 0.967496]\n",
      "epoch:20 step:19252 [D loss: 0.460794, acc.: 79.69%] [G loss: 1.560730]\n",
      "epoch:20 step:19253 [D loss: 0.568256, acc.: 71.09%] [G loss: 1.526573]\n",
      "epoch:20 step:19254 [D loss: 0.602367, acc.: 64.06%] [G loss: 1.613339]\n",
      "epoch:20 step:19255 [D loss: 0.511091, acc.: 73.44%] [G loss: 1.319664]\n",
      "epoch:20 step:19256 [D loss: 0.539413, acc.: 71.09%] [G loss: 1.534331]\n",
      "epoch:20 step:19257 [D loss: 0.614130, acc.: 66.41%] [G loss: 1.407182]\n",
      "epoch:20 step:19258 [D loss: 0.527301, acc.: 73.44%] [G loss: 1.438148]\n",
      "epoch:20 step:19259 [D loss: 0.562257, acc.: 71.88%] [G loss: 1.486800]\n",
      "epoch:20 step:19260 [D loss: 0.434644, acc.: 84.38%] [G loss: 1.315720]\n",
      "epoch:20 step:19261 [D loss: 0.575258, acc.: 71.88%] [G loss: 1.055417]\n",
      "epoch:20 step:19262 [D loss: 0.622390, acc.: 62.50%] [G loss: 1.198083]\n",
      "epoch:20 step:19263 [D loss: 0.593023, acc.: 67.19%] [G loss: 1.316320]\n",
      "epoch:20 step:19264 [D loss: 0.580208, acc.: 67.19%] [G loss: 1.566098]\n",
      "epoch:20 step:19265 [D loss: 0.494793, acc.: 73.44%] [G loss: 1.433795]\n",
      "epoch:20 step:19266 [D loss: 0.642051, acc.: 66.41%] [G loss: 1.197351]\n",
      "epoch:20 step:19267 [D loss: 0.613482, acc.: 64.84%] [G loss: 1.047465]\n",
      "epoch:20 step:19268 [D loss: 0.561703, acc.: 71.09%] [G loss: 1.334799]\n",
      "epoch:20 step:19269 [D loss: 0.509313, acc.: 77.34%] [G loss: 1.549003]\n",
      "epoch:20 step:19270 [D loss: 0.517413, acc.: 75.00%] [G loss: 1.434359]\n",
      "epoch:20 step:19271 [D loss: 0.580318, acc.: 69.53%] [G loss: 1.373190]\n",
      "epoch:20 step:19272 [D loss: 0.682626, acc.: 60.94%] [G loss: 1.193410]\n",
      "epoch:20 step:19273 [D loss: 0.632959, acc.: 60.94%] [G loss: 1.398837]\n",
      "epoch:20 step:19274 [D loss: 0.676338, acc.: 60.94%] [G loss: 1.189814]\n",
      "epoch:20 step:19275 [D loss: 0.580980, acc.: 67.97%] [G loss: 1.040551]\n",
      "epoch:20 step:19276 [D loss: 0.669904, acc.: 61.72%] [G loss: 1.681786]\n",
      "epoch:20 step:19277 [D loss: 0.588699, acc.: 66.41%] [G loss: 1.686077]\n",
      "epoch:20 step:19278 [D loss: 0.732939, acc.: 55.47%] [G loss: 1.107276]\n",
      "epoch:20 step:19279 [D loss: 0.488202, acc.: 83.59%] [G loss: 1.241819]\n",
      "epoch:20 step:19280 [D loss: 0.418264, acc.: 83.59%] [G loss: 1.539397]\n",
      "epoch:20 step:19281 [D loss: 0.563556, acc.: 71.88%] [G loss: 1.320474]\n",
      "epoch:20 step:19282 [D loss: 0.454410, acc.: 80.47%] [G loss: 1.351914]\n",
      "epoch:20 step:19283 [D loss: 0.590541, acc.: 69.53%] [G loss: 1.506999]\n",
      "epoch:20 step:19284 [D loss: 0.607609, acc.: 64.84%] [G loss: 1.232882]\n",
      "epoch:20 step:19285 [D loss: 0.709701, acc.: 61.72%] [G loss: 1.291115]\n",
      "epoch:20 step:19286 [D loss: 0.578105, acc.: 68.75%] [G loss: 1.323304]\n",
      "epoch:20 step:19287 [D loss: 0.559086, acc.: 72.66%] [G loss: 1.082385]\n",
      "epoch:20 step:19288 [D loss: 0.581204, acc.: 67.19%] [G loss: 1.427873]\n",
      "epoch:20 step:19289 [D loss: 0.603935, acc.: 61.72%] [G loss: 1.159876]\n",
      "epoch:20 step:19290 [D loss: 0.550225, acc.: 74.22%] [G loss: 1.153435]\n",
      "epoch:20 step:19291 [D loss: 0.488634, acc.: 78.12%] [G loss: 1.659006]\n",
      "epoch:20 step:19292 [D loss: 0.483321, acc.: 77.34%] [G loss: 1.293246]\n",
      "epoch:20 step:19293 [D loss: 0.505500, acc.: 75.00%] [G loss: 1.053654]\n",
      "epoch:20 step:19294 [D loss: 0.518043, acc.: 72.66%] [G loss: 1.000377]\n",
      "epoch:20 step:19295 [D loss: 0.512162, acc.: 78.91%] [G loss: 1.086633]\n",
      "epoch:20 step:19296 [D loss: 0.629443, acc.: 58.59%] [G loss: 1.585780]\n",
      "epoch:20 step:19297 [D loss: 0.505483, acc.: 78.12%] [G loss: 1.528470]\n",
      "epoch:20 step:19298 [D loss: 0.593800, acc.: 67.97%] [G loss: 1.262693]\n",
      "epoch:20 step:19299 [D loss: 0.507916, acc.: 73.44%] [G loss: 1.206639]\n",
      "epoch:20 step:19300 [D loss: 0.508309, acc.: 82.03%] [G loss: 1.146307]\n",
      "epoch:20 step:19301 [D loss: 0.506040, acc.: 76.56%] [G loss: 1.119438]\n",
      "epoch:20 step:19302 [D loss: 0.461614, acc.: 82.03%] [G loss: 1.265767]\n",
      "epoch:20 step:19303 [D loss: 0.542416, acc.: 71.09%] [G loss: 1.549967]\n",
      "epoch:20 step:19304 [D loss: 0.388249, acc.: 85.94%] [G loss: 1.402336]\n",
      "epoch:20 step:19305 [D loss: 0.477259, acc.: 73.44%] [G loss: 1.599516]\n",
      "epoch:20 step:19306 [D loss: 0.488942, acc.: 78.12%] [G loss: 1.464968]\n",
      "epoch:20 step:19307 [D loss: 0.457380, acc.: 81.25%] [G loss: 1.671506]\n",
      "epoch:20 step:19308 [D loss: 0.527011, acc.: 73.44%] [G loss: 1.433939]\n",
      "epoch:20 step:19309 [D loss: 0.681689, acc.: 60.94%] [G loss: 1.166710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19310 [D loss: 0.645921, acc.: 62.50%] [G loss: 1.516600]\n",
      "epoch:20 step:19311 [D loss: 0.504426, acc.: 75.00%] [G loss: 1.471518]\n",
      "epoch:20 step:19312 [D loss: 0.486007, acc.: 75.78%] [G loss: 1.511078]\n",
      "epoch:20 step:19313 [D loss: 0.639889, acc.: 68.75%] [G loss: 1.335718]\n",
      "epoch:20 step:19314 [D loss: 0.812867, acc.: 45.31%] [G loss: 1.070145]\n",
      "epoch:20 step:19315 [D loss: 0.476239, acc.: 82.81%] [G loss: 1.271886]\n",
      "epoch:20 step:19316 [D loss: 0.428873, acc.: 83.59%] [G loss: 1.032636]\n",
      "epoch:20 step:19317 [D loss: 0.637470, acc.: 65.62%] [G loss: 1.325119]\n",
      "epoch:20 step:19318 [D loss: 0.464360, acc.: 78.91%] [G loss: 1.225518]\n",
      "epoch:20 step:19319 [D loss: 0.685373, acc.: 60.16%] [G loss: 1.201014]\n",
      "epoch:20 step:19320 [D loss: 0.528834, acc.: 71.09%] [G loss: 1.534653]\n",
      "epoch:20 step:19321 [D loss: 0.574650, acc.: 72.66%] [G loss: 1.095715]\n",
      "epoch:20 step:19322 [D loss: 0.494588, acc.: 77.34%] [G loss: 1.294805]\n",
      "epoch:20 step:19323 [D loss: 0.571636, acc.: 71.09%] [G loss: 1.170989]\n",
      "epoch:20 step:19324 [D loss: 0.533833, acc.: 71.88%] [G loss: 1.600430]\n",
      "epoch:20 step:19325 [D loss: 0.696015, acc.: 54.69%] [G loss: 1.369006]\n",
      "epoch:20 step:19326 [D loss: 0.516332, acc.: 73.44%] [G loss: 1.497590]\n",
      "epoch:20 step:19327 [D loss: 0.507088, acc.: 76.56%] [G loss: 1.255726]\n",
      "epoch:20 step:19328 [D loss: 0.562163, acc.: 69.53%] [G loss: 1.291490]\n",
      "epoch:20 step:19329 [D loss: 0.531749, acc.: 72.66%] [G loss: 1.073578]\n",
      "epoch:20 step:19330 [D loss: 0.554855, acc.: 72.66%] [G loss: 1.394686]\n",
      "epoch:20 step:19331 [D loss: 0.404540, acc.: 87.50%] [G loss: 1.514700]\n",
      "epoch:20 step:19332 [D loss: 0.497609, acc.: 81.25%] [G loss: 1.417199]\n",
      "epoch:20 step:19333 [D loss: 0.763643, acc.: 48.44%] [G loss: 1.133224]\n",
      "epoch:20 step:19334 [D loss: 0.614074, acc.: 67.97%] [G loss: 1.431568]\n",
      "epoch:20 step:19335 [D loss: 0.509619, acc.: 75.78%] [G loss: 1.543082]\n",
      "epoch:20 step:19336 [D loss: 0.710183, acc.: 59.38%] [G loss: 1.363070]\n",
      "epoch:20 step:19337 [D loss: 0.551217, acc.: 71.88%] [G loss: 1.382455]\n",
      "epoch:20 step:19338 [D loss: 0.677073, acc.: 58.59%] [G loss: 1.364382]\n",
      "epoch:20 step:19339 [D loss: 0.633135, acc.: 67.19%] [G loss: 1.553598]\n",
      "epoch:20 step:19340 [D loss: 0.415391, acc.: 82.81%] [G loss: 1.526248]\n",
      "epoch:20 step:19341 [D loss: 0.617767, acc.: 62.50%] [G loss: 1.411379]\n",
      "epoch:20 step:19342 [D loss: 0.460862, acc.: 83.59%] [G loss: 1.493449]\n",
      "epoch:20 step:19343 [D loss: 0.593118, acc.: 69.53%] [G loss: 1.230944]\n",
      "epoch:20 step:19344 [D loss: 0.571931, acc.: 71.09%] [G loss: 1.167211]\n",
      "epoch:20 step:19345 [D loss: 0.625074, acc.: 67.19%] [G loss: 1.093542]\n",
      "epoch:20 step:19346 [D loss: 0.632074, acc.: 67.19%] [G loss: 1.024387]\n",
      "epoch:20 step:19347 [D loss: 0.568172, acc.: 76.56%] [G loss: 1.223999]\n",
      "epoch:20 step:19348 [D loss: 0.547367, acc.: 75.78%] [G loss: 1.295412]\n",
      "epoch:20 step:19349 [D loss: 0.581758, acc.: 71.88%] [G loss: 1.669557]\n",
      "epoch:20 step:19350 [D loss: 0.437362, acc.: 81.25%] [G loss: 1.490138]\n",
      "epoch:20 step:19351 [D loss: 0.576974, acc.: 70.31%] [G loss: 1.450390]\n",
      "epoch:20 step:19352 [D loss: 0.647651, acc.: 64.84%] [G loss: 1.473651]\n",
      "epoch:20 step:19353 [D loss: 0.572239, acc.: 71.88%] [G loss: 1.601642]\n",
      "epoch:20 step:19354 [D loss: 0.815930, acc.: 47.66%] [G loss: 1.532997]\n",
      "epoch:20 step:19355 [D loss: 0.609942, acc.: 69.53%] [G loss: 1.838174]\n",
      "epoch:20 step:19356 [D loss: 0.638409, acc.: 64.84%] [G loss: 1.558163]\n",
      "epoch:20 step:19357 [D loss: 0.601057, acc.: 68.75%] [G loss: 1.491708]\n",
      "epoch:20 step:19358 [D loss: 0.680542, acc.: 59.38%] [G loss: 1.377438]\n",
      "epoch:20 step:19359 [D loss: 0.543863, acc.: 73.44%] [G loss: 1.697122]\n",
      "epoch:20 step:19360 [D loss: 0.500425, acc.: 78.12%] [G loss: 1.253649]\n",
      "epoch:20 step:19361 [D loss: 0.616819, acc.: 63.28%] [G loss: 1.104675]\n",
      "epoch:20 step:19362 [D loss: 0.752436, acc.: 52.34%] [G loss: 0.953706]\n",
      "epoch:20 step:19363 [D loss: 0.611215, acc.: 64.84%] [G loss: 1.272496]\n",
      "epoch:20 step:19364 [D loss: 0.591632, acc.: 71.88%] [G loss: 1.286816]\n",
      "epoch:20 step:19365 [D loss: 0.498173, acc.: 76.56%] [G loss: 1.294646]\n",
      "epoch:20 step:19366 [D loss: 0.619605, acc.: 67.97%] [G loss: 1.560900]\n",
      "epoch:20 step:19367 [D loss: 0.584110, acc.: 73.44%] [G loss: 1.632369]\n",
      "epoch:20 step:19368 [D loss: 0.586094, acc.: 65.62%] [G loss: 1.523023]\n",
      "epoch:20 step:19369 [D loss: 0.615592, acc.: 67.19%] [G loss: 1.566761]\n",
      "epoch:20 step:19370 [D loss: 0.646765, acc.: 67.19%] [G loss: 1.101869]\n",
      "epoch:20 step:19371 [D loss: 0.476696, acc.: 73.44%] [G loss: 1.302040]\n",
      "epoch:20 step:19372 [D loss: 0.643856, acc.: 62.50%] [G loss: 1.256843]\n",
      "epoch:20 step:19373 [D loss: 0.509410, acc.: 72.66%] [G loss: 1.479526]\n",
      "epoch:20 step:19374 [D loss: 0.610423, acc.: 68.75%] [G loss: 1.158168]\n",
      "epoch:20 step:19375 [D loss: 0.523540, acc.: 72.66%] [G loss: 1.552060]\n",
      "epoch:20 step:19376 [D loss: 0.609516, acc.: 66.41%] [G loss: 1.128723]\n",
      "epoch:20 step:19377 [D loss: 0.652928, acc.: 63.28%] [G loss: 1.240710]\n",
      "epoch:20 step:19378 [D loss: 0.580986, acc.: 71.88%] [G loss: 1.372110]\n",
      "epoch:20 step:19379 [D loss: 0.579827, acc.: 73.44%] [G loss: 1.310101]\n",
      "epoch:20 step:19380 [D loss: 0.550131, acc.: 73.44%] [G loss: 1.292338]\n",
      "epoch:20 step:19381 [D loss: 0.584399, acc.: 66.41%] [G loss: 1.164617]\n",
      "epoch:20 step:19382 [D loss: 0.563727, acc.: 74.22%] [G loss: 1.422561]\n",
      "epoch:20 step:19383 [D loss: 0.704423, acc.: 57.03%] [G loss: 1.084003]\n",
      "epoch:20 step:19384 [D loss: 0.597272, acc.: 65.62%] [G loss: 1.440900]\n",
      "epoch:20 step:19385 [D loss: 0.467403, acc.: 81.25%] [G loss: 1.594091]\n",
      "epoch:20 step:19386 [D loss: 0.619720, acc.: 63.28%] [G loss: 1.166819]\n",
      "epoch:20 step:19387 [D loss: 0.742906, acc.: 53.91%] [G loss: 1.493942]\n",
      "epoch:20 step:19388 [D loss: 0.436157, acc.: 79.69%] [G loss: 1.512743]\n",
      "epoch:20 step:19389 [D loss: 0.656163, acc.: 61.72%] [G loss: 1.419645]\n",
      "epoch:20 step:19390 [D loss: 0.624139, acc.: 62.50%] [G loss: 1.193895]\n",
      "epoch:20 step:19391 [D loss: 0.505832, acc.: 77.34%] [G loss: 1.420877]\n",
      "epoch:20 step:19392 [D loss: 0.511134, acc.: 80.47%] [G loss: 1.436429]\n",
      "epoch:20 step:19393 [D loss: 0.630010, acc.: 67.19%] [G loss: 1.186182]\n",
      "epoch:20 step:19394 [D loss: 0.757762, acc.: 57.81%] [G loss: 1.217415]\n",
      "epoch:20 step:19395 [D loss: 0.465661, acc.: 81.25%] [G loss: 1.481081]\n",
      "epoch:20 step:19396 [D loss: 0.513431, acc.: 75.78%] [G loss: 1.137323]\n",
      "epoch:20 step:19397 [D loss: 0.563399, acc.: 75.78%] [G loss: 1.293208]\n",
      "epoch:20 step:19398 [D loss: 0.589848, acc.: 70.31%] [G loss: 1.030092]\n",
      "epoch:20 step:19399 [D loss: 0.583957, acc.: 65.62%] [G loss: 1.336620]\n",
      "epoch:20 step:19400 [D loss: 0.486803, acc.: 76.56%] [G loss: 1.609546]\n",
      "##############\n",
      "[2.76716049 1.97217219 2.00340702 3.03067935 0.80259491 5.41974712\n",
      " 2.23080107 2.30383557 3.93072331 5.24912355]\n",
      "##########\n",
      "epoch:20 step:19401 [D loss: 0.572955, acc.: 71.88%] [G loss: 1.033343]\n",
      "epoch:20 step:19402 [D loss: 0.671619, acc.: 58.59%] [G loss: 1.192631]\n",
      "epoch:20 step:19403 [D loss: 0.649686, acc.: 61.72%] [G loss: 1.141741]\n",
      "epoch:20 step:19404 [D loss: 0.540072, acc.: 70.31%] [G loss: 1.205663]\n",
      "epoch:20 step:19405 [D loss: 0.611959, acc.: 64.06%] [G loss: 1.146927]\n",
      "epoch:20 step:19406 [D loss: 0.605691, acc.: 70.31%] [G loss: 1.144231]\n",
      "epoch:20 step:19407 [D loss: 0.554596, acc.: 70.31%] [G loss: 1.400683]\n",
      "epoch:20 step:19408 [D loss: 0.759074, acc.: 51.56%] [G loss: 1.020198]\n",
      "epoch:20 step:19409 [D loss: 0.607313, acc.: 65.62%] [G loss: 1.403330]\n",
      "epoch:20 step:19410 [D loss: 0.485485, acc.: 77.34%] [G loss: 1.498019]\n",
      "epoch:20 step:19411 [D loss: 0.591361, acc.: 64.84%] [G loss: 1.555558]\n",
      "epoch:20 step:19412 [D loss: 0.594023, acc.: 65.62%] [G loss: 1.376900]\n",
      "epoch:20 step:19413 [D loss: 0.679469, acc.: 61.72%] [G loss: 1.231653]\n",
      "epoch:20 step:19414 [D loss: 0.513645, acc.: 75.78%] [G loss: 1.324769]\n",
      "epoch:20 step:19415 [D loss: 0.535309, acc.: 75.00%] [G loss: 1.394683]\n",
      "epoch:20 step:19416 [D loss: 0.502459, acc.: 75.78%] [G loss: 1.125312]\n",
      "epoch:20 step:19417 [D loss: 0.533631, acc.: 71.88%] [G loss: 1.199127]\n",
      "epoch:20 step:19418 [D loss: 0.574038, acc.: 67.97%] [G loss: 1.440410]\n",
      "epoch:20 step:19419 [D loss: 0.520417, acc.: 71.88%] [G loss: 1.406115]\n",
      "epoch:20 step:19420 [D loss: 0.751758, acc.: 60.16%] [G loss: 1.141728]\n",
      "epoch:20 step:19421 [D loss: 0.715696, acc.: 60.94%] [G loss: 1.093521]\n",
      "epoch:20 step:19422 [D loss: 0.539689, acc.: 73.44%] [G loss: 1.142022]\n",
      "epoch:20 step:19423 [D loss: 0.648808, acc.: 60.16%] [G loss: 1.497555]\n",
      "epoch:20 step:19424 [D loss: 0.556921, acc.: 74.22%] [G loss: 1.161960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19425 [D loss: 0.765646, acc.: 55.47%] [G loss: 0.779110]\n",
      "epoch:20 step:19426 [D loss: 0.513681, acc.: 75.78%] [G loss: 1.324345]\n",
      "epoch:20 step:19427 [D loss: 0.574220, acc.: 67.97%] [G loss: 1.385065]\n",
      "epoch:20 step:19428 [D loss: 0.402009, acc.: 84.38%] [G loss: 1.344769]\n",
      "epoch:20 step:19429 [D loss: 0.619225, acc.: 67.97%] [G loss: 1.140333]\n",
      "epoch:20 step:19430 [D loss: 0.498320, acc.: 82.03%] [G loss: 1.192505]\n",
      "epoch:20 step:19431 [D loss: 0.594031, acc.: 71.09%] [G loss: 1.353367]\n",
      "epoch:20 step:19432 [D loss: 0.504174, acc.: 77.34%] [G loss: 1.326661]\n",
      "epoch:20 step:19433 [D loss: 0.485192, acc.: 76.56%] [G loss: 1.439357]\n",
      "epoch:20 step:19434 [D loss: 0.632198, acc.: 69.53%] [G loss: 1.399373]\n",
      "epoch:20 step:19435 [D loss: 0.580829, acc.: 69.53%] [G loss: 0.916932]\n",
      "epoch:20 step:19436 [D loss: 0.624097, acc.: 68.75%] [G loss: 1.560379]\n",
      "epoch:20 step:19437 [D loss: 0.519494, acc.: 77.34%] [G loss: 1.661892]\n",
      "epoch:20 step:19438 [D loss: 0.521802, acc.: 71.09%] [G loss: 1.447104]\n",
      "epoch:20 step:19439 [D loss: 0.724781, acc.: 59.38%] [G loss: 1.044666]\n",
      "epoch:20 step:19440 [D loss: 0.562944, acc.: 67.19%] [G loss: 1.272788]\n",
      "epoch:20 step:19441 [D loss: 0.340711, acc.: 89.06%] [G loss: 1.471874]\n",
      "epoch:20 step:19442 [D loss: 0.433794, acc.: 83.59%] [G loss: 1.364479]\n",
      "epoch:20 step:19443 [D loss: 0.426800, acc.: 84.38%] [G loss: 1.496690]\n",
      "epoch:20 step:19444 [D loss: 0.579671, acc.: 68.75%] [G loss: 1.447909]\n",
      "epoch:20 step:19445 [D loss: 0.409536, acc.: 82.03%] [G loss: 1.535568]\n",
      "epoch:20 step:19446 [D loss: 0.630042, acc.: 66.41%] [G loss: 1.270258]\n",
      "epoch:20 step:19447 [D loss: 0.481740, acc.: 75.00%] [G loss: 1.300065]\n",
      "epoch:20 step:19448 [D loss: 0.523182, acc.: 75.78%] [G loss: 1.396074]\n",
      "epoch:20 step:19449 [D loss: 0.627169, acc.: 67.97%] [G loss: 1.515861]\n",
      "epoch:20 step:19450 [D loss: 0.513940, acc.: 73.44%] [G loss: 1.046437]\n",
      "epoch:20 step:19451 [D loss: 0.458901, acc.: 82.03%] [G loss: 1.200306]\n",
      "epoch:20 step:19452 [D loss: 0.693621, acc.: 56.25%] [G loss: 1.554728]\n",
      "epoch:20 step:19453 [D loss: 0.500510, acc.: 75.00%] [G loss: 1.536633]\n",
      "epoch:20 step:19454 [D loss: 0.675054, acc.: 59.38%] [G loss: 1.361952]\n",
      "epoch:20 step:19455 [D loss: 0.766774, acc.: 50.78%] [G loss: 1.193809]\n",
      "epoch:20 step:19456 [D loss: 0.585561, acc.: 69.53%] [G loss: 1.066554]\n",
      "epoch:20 step:19457 [D loss: 0.455196, acc.: 79.69%] [G loss: 1.536862]\n",
      "epoch:20 step:19458 [D loss: 0.379230, acc.: 88.28%] [G loss: 1.586854]\n",
      "epoch:20 step:19459 [D loss: 0.762917, acc.: 53.12%] [G loss: 1.398611]\n",
      "epoch:20 step:19460 [D loss: 0.659373, acc.: 58.59%] [G loss: 1.335032]\n",
      "epoch:20 step:19461 [D loss: 0.586995, acc.: 69.53%] [G loss: 1.297348]\n",
      "epoch:20 step:19462 [D loss: 0.621415, acc.: 69.53%] [G loss: 1.013043]\n",
      "epoch:20 step:19463 [D loss: 0.400560, acc.: 83.59%] [G loss: 1.597972]\n",
      "epoch:20 step:19464 [D loss: 0.502780, acc.: 75.00%] [G loss: 1.527101]\n",
      "epoch:20 step:19465 [D loss: 0.471115, acc.: 81.25%] [G loss: 1.542197]\n",
      "epoch:20 step:19466 [D loss: 0.509744, acc.: 75.00%] [G loss: 1.689486]\n",
      "epoch:20 step:19467 [D loss: 0.434335, acc.: 84.38%] [G loss: 1.306683]\n",
      "epoch:20 step:19468 [D loss: 0.546681, acc.: 69.53%] [G loss: 1.346982]\n",
      "epoch:20 step:19469 [D loss: 0.651820, acc.: 59.38%] [G loss: 0.916689]\n",
      "epoch:20 step:19470 [D loss: 0.621613, acc.: 64.84%] [G loss: 1.127923]\n",
      "epoch:20 step:19471 [D loss: 0.679623, acc.: 59.38%] [G loss: 1.283661]\n",
      "epoch:20 step:19472 [D loss: 0.604291, acc.: 67.19%] [G loss: 1.376613]\n",
      "epoch:20 step:19473 [D loss: 0.650163, acc.: 60.94%] [G loss: 1.343301]\n",
      "epoch:20 step:19474 [D loss: 0.572405, acc.: 70.31%] [G loss: 1.254615]\n",
      "epoch:20 step:19475 [D loss: 0.742034, acc.: 58.59%] [G loss: 1.269189]\n",
      "epoch:20 step:19476 [D loss: 0.489971, acc.: 78.12%] [G loss: 1.442884]\n",
      "epoch:20 step:19477 [D loss: 0.616790, acc.: 64.06%] [G loss: 1.250008]\n",
      "epoch:20 step:19478 [D loss: 0.671831, acc.: 61.72%] [G loss: 1.334248]\n",
      "epoch:20 step:19479 [D loss: 0.456927, acc.: 81.25%] [G loss: 1.705781]\n",
      "epoch:20 step:19480 [D loss: 0.662090, acc.: 56.25%] [G loss: 1.226152]\n",
      "epoch:20 step:19481 [D loss: 0.541367, acc.: 74.22%] [G loss: 0.782709]\n",
      "epoch:20 step:19482 [D loss: 0.575399, acc.: 61.72%] [G loss: 1.491177]\n",
      "epoch:20 step:19483 [D loss: 0.563834, acc.: 66.41%] [G loss: 1.165992]\n",
      "epoch:20 step:19484 [D loss: 0.588246, acc.: 71.09%] [G loss: 1.168350]\n",
      "epoch:20 step:19485 [D loss: 0.683287, acc.: 59.38%] [G loss: 1.418516]\n",
      "epoch:20 step:19486 [D loss: 0.430336, acc.: 82.81%] [G loss: 1.497018]\n",
      "epoch:20 step:19487 [D loss: 0.438581, acc.: 82.03%] [G loss: 1.492035]\n",
      "epoch:20 step:19488 [D loss: 0.640550, acc.: 68.75%] [G loss: 1.323119]\n",
      "epoch:20 step:19489 [D loss: 0.616850, acc.: 64.06%] [G loss: 1.594846]\n",
      "epoch:20 step:19490 [D loss: 0.555551, acc.: 74.22%] [G loss: 1.184563]\n",
      "epoch:20 step:19491 [D loss: 0.646370, acc.: 61.72%] [G loss: 1.140391]\n",
      "epoch:20 step:19492 [D loss: 0.554264, acc.: 71.09%] [G loss: 1.111513]\n",
      "epoch:20 step:19493 [D loss: 0.546117, acc.: 75.00%] [G loss: 1.785431]\n",
      "epoch:20 step:19494 [D loss: 0.709853, acc.: 57.03%] [G loss: 1.385311]\n",
      "epoch:20 step:19495 [D loss: 0.492052, acc.: 74.22%] [G loss: 1.674049]\n",
      "epoch:20 step:19496 [D loss: 0.734107, acc.: 53.91%] [G loss: 1.391837]\n",
      "epoch:20 step:19497 [D loss: 0.498931, acc.: 76.56%] [G loss: 1.393124]\n",
      "epoch:20 step:19498 [D loss: 0.609758, acc.: 65.62%] [G loss: 1.116611]\n",
      "epoch:20 step:19499 [D loss: 0.405887, acc.: 82.03%] [G loss: 1.234576]\n",
      "epoch:20 step:19500 [D loss: 0.516913, acc.: 75.78%] [G loss: 1.610491]\n",
      "epoch:20 step:19501 [D loss: 0.770639, acc.: 56.25%] [G loss: 1.164713]\n",
      "epoch:20 step:19502 [D loss: 0.487307, acc.: 76.56%] [G loss: 1.412935]\n",
      "epoch:20 step:19503 [D loss: 0.642292, acc.: 60.16%] [G loss: 1.293756]\n",
      "epoch:20 step:19504 [D loss: 0.639820, acc.: 66.41%] [G loss: 1.093655]\n",
      "epoch:20 step:19505 [D loss: 0.533909, acc.: 78.12%] [G loss: 1.457246]\n",
      "epoch:20 step:19506 [D loss: 0.489315, acc.: 77.34%] [G loss: 1.550595]\n",
      "epoch:20 step:19507 [D loss: 0.654179, acc.: 66.41%] [G loss: 1.076029]\n",
      "epoch:20 step:19508 [D loss: 0.552783, acc.: 71.09%] [G loss: 1.507564]\n",
      "epoch:20 step:19509 [D loss: 0.484443, acc.: 78.91%] [G loss: 1.711704]\n",
      "epoch:20 step:19510 [D loss: 0.586779, acc.: 68.75%] [G loss: 1.271502]\n",
      "epoch:20 step:19511 [D loss: 0.589328, acc.: 71.09%] [G loss: 1.256539]\n",
      "epoch:20 step:19512 [D loss: 0.582808, acc.: 71.88%] [G loss: 1.353105]\n",
      "epoch:20 step:19513 [D loss: 0.518304, acc.: 77.34%] [G loss: 1.586112]\n",
      "epoch:20 step:19514 [D loss: 0.437008, acc.: 84.38%] [G loss: 1.402052]\n",
      "epoch:20 step:19515 [D loss: 0.700718, acc.: 58.59%] [G loss: 1.324626]\n",
      "epoch:20 step:19516 [D loss: 0.513386, acc.: 78.91%] [G loss: 1.459926]\n",
      "epoch:20 step:19517 [D loss: 0.635764, acc.: 60.94%] [G loss: 1.104464]\n",
      "epoch:20 step:19518 [D loss: 0.552337, acc.: 70.31%] [G loss: 1.500984]\n",
      "epoch:20 step:19519 [D loss: 0.606324, acc.: 70.31%] [G loss: 1.992675]\n",
      "epoch:20 step:19520 [D loss: 0.721613, acc.: 59.38%] [G loss: 1.562441]\n",
      "epoch:20 step:19521 [D loss: 0.752958, acc.: 57.03%] [G loss: 1.466553]\n",
      "epoch:20 step:19522 [D loss: 0.623036, acc.: 63.28%] [G loss: 1.420065]\n",
      "epoch:20 step:19523 [D loss: 0.604316, acc.: 60.16%] [G loss: 0.970609]\n",
      "epoch:20 step:19524 [D loss: 0.510819, acc.: 77.34%] [G loss: 1.234634]\n",
      "epoch:20 step:19525 [D loss: 0.532562, acc.: 69.53%] [G loss: 1.183923]\n",
      "epoch:20 step:19526 [D loss: 0.649655, acc.: 64.84%] [G loss: 1.009458]\n",
      "epoch:20 step:19527 [D loss: 0.563618, acc.: 69.53%] [G loss: 1.295361]\n",
      "epoch:20 step:19528 [D loss: 0.503106, acc.: 77.34%] [G loss: 1.310775]\n",
      "epoch:20 step:19529 [D loss: 0.545827, acc.: 73.44%] [G loss: 1.233251]\n",
      "epoch:20 step:19530 [D loss: 0.501571, acc.: 76.56%] [G loss: 1.653475]\n",
      "epoch:20 step:19531 [D loss: 0.455448, acc.: 79.69%] [G loss: 1.470019]\n",
      "epoch:20 step:19532 [D loss: 0.536987, acc.: 75.78%] [G loss: 0.994996]\n",
      "epoch:20 step:19533 [D loss: 0.447527, acc.: 82.03%] [G loss: 1.335161]\n",
      "epoch:20 step:19534 [D loss: 0.622329, acc.: 64.84%] [G loss: 1.505671]\n",
      "epoch:20 step:19535 [D loss: 0.568437, acc.: 67.97%] [G loss: 1.293413]\n",
      "epoch:20 step:19536 [D loss: 0.609021, acc.: 67.19%] [G loss: 1.323741]\n",
      "epoch:20 step:19537 [D loss: 0.636370, acc.: 61.72%] [G loss: 1.582381]\n",
      "epoch:20 step:19538 [D loss: 0.586060, acc.: 71.09%] [G loss: 1.590493]\n",
      "epoch:20 step:19539 [D loss: 0.516292, acc.: 74.22%] [G loss: 1.054506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19540 [D loss: 0.577998, acc.: 65.62%] [G loss: 1.495607]\n",
      "epoch:20 step:19541 [D loss: 0.683551, acc.: 61.72%] [G loss: 1.306793]\n",
      "epoch:20 step:19542 [D loss: 0.589087, acc.: 71.88%] [G loss: 1.095022]\n",
      "epoch:20 step:19543 [D loss: 0.592141, acc.: 67.97%] [G loss: 1.106423]\n",
      "epoch:20 step:19544 [D loss: 0.521450, acc.: 76.56%] [G loss: 1.387497]\n",
      "epoch:20 step:19545 [D loss: 0.605375, acc.: 67.19%] [G loss: 1.215839]\n",
      "epoch:20 step:19546 [D loss: 0.598350, acc.: 67.19%] [G loss: 1.312670]\n",
      "epoch:20 step:19547 [D loss: 0.571195, acc.: 71.09%] [G loss: 1.084038]\n",
      "epoch:20 step:19548 [D loss: 0.585612, acc.: 73.44%] [G loss: 1.266477]\n",
      "epoch:20 step:19549 [D loss: 0.619190, acc.: 68.75%] [G loss: 1.258475]\n",
      "epoch:20 step:19550 [D loss: 0.491630, acc.: 77.34%] [G loss: 1.932760]\n",
      "epoch:20 step:19551 [D loss: 0.488719, acc.: 80.47%] [G loss: 1.650003]\n",
      "epoch:20 step:19552 [D loss: 0.631821, acc.: 60.16%] [G loss: 1.559129]\n",
      "epoch:20 step:19553 [D loss: 0.518163, acc.: 76.56%] [G loss: 1.455499]\n",
      "epoch:20 step:19554 [D loss: 0.503733, acc.: 79.69%] [G loss: 1.336453]\n",
      "epoch:20 step:19555 [D loss: 0.570888, acc.: 69.53%] [G loss: 1.521446]\n",
      "epoch:20 step:19556 [D loss: 0.527190, acc.: 73.44%] [G loss: 1.251030]\n",
      "epoch:20 step:19557 [D loss: 0.650558, acc.: 60.94%] [G loss: 1.195295]\n",
      "epoch:20 step:19558 [D loss: 0.537370, acc.: 74.22%] [G loss: 1.655246]\n",
      "epoch:20 step:19559 [D loss: 0.414895, acc.: 85.94%] [G loss: 1.380253]\n",
      "epoch:20 step:19560 [D loss: 0.560739, acc.: 71.09%] [G loss: 1.504152]\n",
      "epoch:20 step:19561 [D loss: 0.505680, acc.: 75.00%] [G loss: 1.103355]\n",
      "epoch:20 step:19562 [D loss: 0.540311, acc.: 71.88%] [G loss: 1.079590]\n",
      "epoch:20 step:19563 [D loss: 0.676438, acc.: 62.50%] [G loss: 1.478346]\n",
      "epoch:20 step:19564 [D loss: 0.522302, acc.: 75.78%] [G loss: 1.462395]\n",
      "epoch:20 step:19565 [D loss: 0.516332, acc.: 75.00%] [G loss: 1.414668]\n",
      "epoch:20 step:19566 [D loss: 0.680072, acc.: 60.16%] [G loss: 1.560318]\n",
      "epoch:20 step:19567 [D loss: 0.495931, acc.: 75.78%] [G loss: 1.575062]\n",
      "epoch:20 step:19568 [D loss: 0.547543, acc.: 73.44%] [G loss: 1.105001]\n",
      "epoch:20 step:19569 [D loss: 0.635528, acc.: 65.62%] [G loss: 1.174721]\n",
      "epoch:20 step:19570 [D loss: 0.516833, acc.: 73.44%] [G loss: 1.506235]\n",
      "epoch:20 step:19571 [D loss: 0.749395, acc.: 52.34%] [G loss: 1.145610]\n",
      "epoch:20 step:19572 [D loss: 0.494591, acc.: 75.78%] [G loss: 1.120879]\n",
      "epoch:20 step:19573 [D loss: 0.542490, acc.: 74.22%] [G loss: 1.272501]\n",
      "epoch:20 step:19574 [D loss: 0.602904, acc.: 68.75%] [G loss: 1.501816]\n",
      "epoch:20 step:19575 [D loss: 0.577603, acc.: 66.41%] [G loss: 1.126946]\n",
      "epoch:20 step:19576 [D loss: 0.504921, acc.: 79.69%] [G loss: 1.564003]\n",
      "epoch:20 step:19577 [D loss: 0.553117, acc.: 67.97%] [G loss: 1.252346]\n",
      "epoch:20 step:19578 [D loss: 0.628880, acc.: 61.72%] [G loss: 0.897422]\n",
      "epoch:20 step:19579 [D loss: 0.643814, acc.: 60.16%] [G loss: 1.381951]\n",
      "epoch:20 step:19580 [D loss: 0.502338, acc.: 75.78%] [G loss: 1.324517]\n",
      "epoch:20 step:19581 [D loss: 0.449143, acc.: 78.12%] [G loss: 1.181965]\n",
      "epoch:20 step:19582 [D loss: 0.421981, acc.: 82.81%] [G loss: 1.271466]\n",
      "epoch:20 step:19583 [D loss: 0.681803, acc.: 59.38%] [G loss: 1.246667]\n",
      "epoch:20 step:19584 [D loss: 0.754527, acc.: 53.12%] [G loss: 1.096489]\n",
      "epoch:20 step:19585 [D loss: 0.609792, acc.: 64.84%] [G loss: 1.192381]\n",
      "epoch:20 step:19586 [D loss: 0.565850, acc.: 71.88%] [G loss: 1.311828]\n",
      "epoch:20 step:19587 [D loss: 0.479864, acc.: 79.69%] [G loss: 1.319905]\n",
      "epoch:20 step:19588 [D loss: 0.734999, acc.: 59.38%] [G loss: 1.216223]\n",
      "epoch:20 step:19589 [D loss: 0.684099, acc.: 64.06%] [G loss: 0.991147]\n",
      "epoch:20 step:19590 [D loss: 0.459702, acc.: 82.03%] [G loss: 1.474391]\n",
      "epoch:20 step:19591 [D loss: 0.684563, acc.: 65.62%] [G loss: 0.977011]\n",
      "epoch:20 step:19592 [D loss: 0.373336, acc.: 85.16%] [G loss: 1.318839]\n",
      "epoch:20 step:19593 [D loss: 0.594250, acc.: 66.41%] [G loss: 1.480568]\n",
      "epoch:20 step:19594 [D loss: 0.638400, acc.: 67.97%] [G loss: 1.389633]\n",
      "epoch:20 step:19595 [D loss: 0.572336, acc.: 73.44%] [G loss: 1.301825]\n",
      "epoch:20 step:19596 [D loss: 0.385697, acc.: 88.28%] [G loss: 1.660116]\n",
      "epoch:20 step:19597 [D loss: 0.514044, acc.: 72.66%] [G loss: 1.232720]\n",
      "epoch:20 step:19598 [D loss: 0.644565, acc.: 64.06%] [G loss: 1.296246]\n",
      "epoch:20 step:19599 [D loss: 0.559580, acc.: 71.88%] [G loss: 1.442586]\n",
      "epoch:20 step:19600 [D loss: 0.647743, acc.: 67.97%] [G loss: 1.171206]\n",
      "##############\n",
      "[2.71815997 2.06821409 1.82542088 2.85786315 1.02021084 6.03134239\n",
      " 2.09831032 2.71318456 3.96579914 8.14868929]\n",
      "##########\n",
      "epoch:20 step:19601 [D loss: 0.518509, acc.: 74.22%] [G loss: 1.386003]\n",
      "epoch:20 step:19602 [D loss: 0.641790, acc.: 62.50%] [G loss: 1.195662]\n",
      "epoch:20 step:19603 [D loss: 0.576912, acc.: 69.53%] [G loss: 1.090258]\n",
      "epoch:20 step:19604 [D loss: 0.557586, acc.: 68.75%] [G loss: 1.268537]\n",
      "epoch:20 step:19605 [D loss: 0.533606, acc.: 74.22%] [G loss: 1.339356]\n",
      "epoch:20 step:19606 [D loss: 0.480771, acc.: 76.56%] [G loss: 1.405802]\n",
      "epoch:20 step:19607 [D loss: 0.378711, acc.: 87.50%] [G loss: 1.708337]\n",
      "epoch:20 step:19608 [D loss: 0.390451, acc.: 84.38%] [G loss: 1.596575]\n",
      "epoch:20 step:19609 [D loss: 0.498123, acc.: 78.91%] [G loss: 1.138982]\n",
      "epoch:20 step:19610 [D loss: 0.507921, acc.: 79.69%] [G loss: 1.508705]\n",
      "epoch:20 step:19611 [D loss: 0.633455, acc.: 67.19%] [G loss: 1.556411]\n",
      "epoch:20 step:19612 [D loss: 0.692697, acc.: 54.69%] [G loss: 1.462769]\n",
      "epoch:20 step:19613 [D loss: 0.443843, acc.: 83.59%] [G loss: 1.295133]\n",
      "epoch:20 step:19614 [D loss: 0.644960, acc.: 60.16%] [G loss: 1.369607]\n",
      "epoch:20 step:19615 [D loss: 0.654073, acc.: 57.81%] [G loss: 1.277456]\n",
      "epoch:20 step:19616 [D loss: 0.670633, acc.: 61.72%] [G loss: 1.634175]\n",
      "epoch:20 step:19617 [D loss: 0.531895, acc.: 78.12%] [G loss: 1.220080]\n",
      "epoch:20 step:19618 [D loss: 0.538424, acc.: 75.00%] [G loss: 1.247138]\n",
      "epoch:20 step:19619 [D loss: 0.698616, acc.: 57.03%] [G loss: 1.522093]\n",
      "epoch:20 step:19620 [D loss: 0.592271, acc.: 65.62%] [G loss: 1.041077]\n",
      "epoch:20 step:19621 [D loss: 0.638024, acc.: 64.06%] [G loss: 1.118776]\n",
      "epoch:20 step:19622 [D loss: 0.388584, acc.: 84.38%] [G loss: 1.303952]\n",
      "epoch:20 step:19623 [D loss: 0.647415, acc.: 60.16%] [G loss: 1.343420]\n",
      "epoch:20 step:19624 [D loss: 0.629694, acc.: 64.84%] [G loss: 1.331336]\n",
      "epoch:20 step:19625 [D loss: 0.466952, acc.: 81.25%] [G loss: 1.819263]\n",
      "epoch:20 step:19626 [D loss: 0.528432, acc.: 75.78%] [G loss: 1.314287]\n",
      "epoch:20 step:19627 [D loss: 0.633091, acc.: 68.75%] [G loss: 1.457174]\n",
      "epoch:20 step:19628 [D loss: 0.564277, acc.: 71.88%] [G loss: 1.327386]\n",
      "epoch:20 step:19629 [D loss: 0.663497, acc.: 64.06%] [G loss: 1.626225]\n",
      "epoch:20 step:19630 [D loss: 0.668769, acc.: 60.94%] [G loss: 1.272715]\n",
      "epoch:20 step:19631 [D loss: 0.617023, acc.: 66.41%] [G loss: 1.165675]\n",
      "epoch:20 step:19632 [D loss: 0.716084, acc.: 53.12%] [G loss: 1.641629]\n",
      "epoch:20 step:19633 [D loss: 0.533414, acc.: 76.56%] [G loss: 1.243912]\n",
      "epoch:20 step:19634 [D loss: 0.620292, acc.: 67.97%] [G loss: 1.107896]\n",
      "epoch:20 step:19635 [D loss: 0.580965, acc.: 69.53%] [G loss: 1.315418]\n",
      "epoch:20 step:19636 [D loss: 0.782381, acc.: 51.56%] [G loss: 1.330743]\n",
      "epoch:20 step:19637 [D loss: 0.576594, acc.: 67.19%] [G loss: 1.470881]\n",
      "epoch:20 step:19638 [D loss: 0.509468, acc.: 74.22%] [G loss: 1.697608]\n",
      "epoch:20 step:19639 [D loss: 0.463003, acc.: 81.25%] [G loss: 1.376260]\n",
      "epoch:20 step:19640 [D loss: 0.469348, acc.: 79.69%] [G loss: 1.675073]\n",
      "epoch:20 step:19641 [D loss: 0.599910, acc.: 63.28%] [G loss: 1.171532]\n",
      "epoch:20 step:19642 [D loss: 0.473163, acc.: 78.91%] [G loss: 1.545459]\n",
      "epoch:20 step:19643 [D loss: 0.533981, acc.: 78.12%] [G loss: 1.465919]\n",
      "epoch:20 step:19644 [D loss: 0.477488, acc.: 78.12%] [G loss: 1.699630]\n",
      "epoch:20 step:19645 [D loss: 0.664158, acc.: 62.50%] [G loss: 1.342042]\n",
      "epoch:20 step:19646 [D loss: 0.420460, acc.: 82.03%] [G loss: 1.471834]\n",
      "epoch:20 step:19647 [D loss: 0.777637, acc.: 55.47%] [G loss: 1.590037]\n",
      "epoch:20 step:19648 [D loss: 0.676743, acc.: 59.38%] [G loss: 1.569425]\n",
      "epoch:20 step:19649 [D loss: 0.507240, acc.: 75.78%] [G loss: 1.432897]\n",
      "epoch:20 step:19650 [D loss: 0.543690, acc.: 76.56%] [G loss: 1.372368]\n",
      "epoch:20 step:19651 [D loss: 0.628210, acc.: 61.72%] [G loss: 1.092701]\n",
      "epoch:20 step:19652 [D loss: 0.630372, acc.: 62.50%] [G loss: 1.440512]\n",
      "epoch:20 step:19653 [D loss: 0.646828, acc.: 63.28%] [G loss: 1.191293]\n",
      "epoch:20 step:19654 [D loss: 0.631222, acc.: 61.72%] [G loss: 0.949061]\n",
      "epoch:20 step:19655 [D loss: 0.576473, acc.: 70.31%] [G loss: 1.302474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19656 [D loss: 0.633568, acc.: 69.53%] [G loss: 1.268777]\n",
      "epoch:20 step:19657 [D loss: 0.544698, acc.: 72.66%] [G loss: 0.993347]\n",
      "epoch:20 step:19658 [D loss: 0.424555, acc.: 87.50%] [G loss: 1.295209]\n",
      "epoch:20 step:19659 [D loss: 0.513112, acc.: 77.34%] [G loss: 1.474167]\n",
      "epoch:20 step:19660 [D loss: 0.472849, acc.: 83.59%] [G loss: 1.638471]\n",
      "epoch:20 step:19661 [D loss: 0.689043, acc.: 62.50%] [G loss: 1.131078]\n",
      "epoch:20 step:19662 [D loss: 0.573375, acc.: 66.41%] [G loss: 1.685891]\n",
      "epoch:20 step:19663 [D loss: 0.543673, acc.: 74.22%] [G loss: 1.352707]\n",
      "epoch:20 step:19664 [D loss: 0.574376, acc.: 70.31%] [G loss: 1.644462]\n",
      "epoch:20 step:19665 [D loss: 0.675945, acc.: 59.38%] [G loss: 1.251965]\n",
      "epoch:20 step:19666 [D loss: 0.523009, acc.: 72.66%] [G loss: 1.129460]\n",
      "epoch:20 step:19667 [D loss: 0.784811, acc.: 51.56%] [G loss: 1.125569]\n",
      "epoch:20 step:19668 [D loss: 0.549696, acc.: 72.66%] [G loss: 1.431477]\n",
      "epoch:20 step:19669 [D loss: 0.755080, acc.: 53.91%] [G loss: 1.369654]\n",
      "epoch:20 step:19670 [D loss: 0.447812, acc.: 82.03%] [G loss: 1.312438]\n",
      "epoch:20 step:19671 [D loss: 0.544481, acc.: 68.75%] [G loss: 1.269937]\n",
      "epoch:20 step:19672 [D loss: 0.472157, acc.: 78.12%] [G loss: 1.111823]\n",
      "epoch:20 step:19673 [D loss: 0.582588, acc.: 71.09%] [G loss: 1.484199]\n",
      "epoch:20 step:19674 [D loss: 0.636939, acc.: 67.19%] [G loss: 1.240584]\n",
      "epoch:20 step:19675 [D loss: 0.626568, acc.: 64.84%] [G loss: 1.588340]\n",
      "epoch:20 step:19676 [D loss: 0.499937, acc.: 79.69%] [G loss: 1.377000]\n",
      "epoch:20 step:19677 [D loss: 0.631091, acc.: 64.06%] [G loss: 1.155324]\n",
      "epoch:21 step:19678 [D loss: 0.592986, acc.: 69.53%] [G loss: 1.022671]\n",
      "epoch:21 step:19679 [D loss: 0.532944, acc.: 75.78%] [G loss: 1.710612]\n",
      "epoch:21 step:19680 [D loss: 0.659542, acc.: 64.84%] [G loss: 1.282402]\n",
      "epoch:21 step:19681 [D loss: 0.521886, acc.: 76.56%] [G loss: 1.075721]\n",
      "epoch:21 step:19682 [D loss: 0.601325, acc.: 67.19%] [G loss: 1.263694]\n",
      "epoch:21 step:19683 [D loss: 0.612592, acc.: 68.75%] [G loss: 1.345043]\n",
      "epoch:21 step:19684 [D loss: 0.651236, acc.: 68.75%] [G loss: 1.304539]\n",
      "epoch:21 step:19685 [D loss: 0.569616, acc.: 71.88%] [G loss: 1.044266]\n",
      "epoch:21 step:19686 [D loss: 0.589170, acc.: 69.53%] [G loss: 1.317288]\n",
      "epoch:21 step:19687 [D loss: 0.525990, acc.: 73.44%] [G loss: 1.507696]\n",
      "epoch:21 step:19688 [D loss: 0.443729, acc.: 85.16%] [G loss: 1.676654]\n",
      "epoch:21 step:19689 [D loss: 0.600291, acc.: 64.84%] [G loss: 1.070111]\n",
      "epoch:21 step:19690 [D loss: 0.743349, acc.: 52.34%] [G loss: 1.205579]\n",
      "epoch:21 step:19691 [D loss: 0.586610, acc.: 70.31%] [G loss: 1.335834]\n",
      "epoch:21 step:19692 [D loss: 0.431486, acc.: 82.81%] [G loss: 1.410614]\n",
      "epoch:21 step:19693 [D loss: 0.618734, acc.: 66.41%] [G loss: 1.460015]\n",
      "epoch:21 step:19694 [D loss: 0.563520, acc.: 75.00%] [G loss: 1.257133]\n",
      "epoch:21 step:19695 [D loss: 0.594270, acc.: 67.97%] [G loss: 1.322274]\n",
      "epoch:21 step:19696 [D loss: 0.691610, acc.: 56.25%] [G loss: 1.240333]\n",
      "epoch:21 step:19697 [D loss: 0.683641, acc.: 58.59%] [G loss: 1.228424]\n",
      "epoch:21 step:19698 [D loss: 0.509218, acc.: 75.78%] [G loss: 1.604432]\n",
      "epoch:21 step:19699 [D loss: 0.615577, acc.: 72.66%] [G loss: 1.355567]\n",
      "epoch:21 step:19700 [D loss: 0.438020, acc.: 80.47%] [G loss: 1.140949]\n",
      "epoch:21 step:19701 [D loss: 0.596901, acc.: 68.75%] [G loss: 0.917901]\n",
      "epoch:21 step:19702 [D loss: 0.530765, acc.: 70.31%] [G loss: 1.510714]\n",
      "epoch:21 step:19703 [D loss: 0.550720, acc.: 65.62%] [G loss: 1.007214]\n",
      "epoch:21 step:19704 [D loss: 0.557212, acc.: 73.44%] [G loss: 1.410305]\n",
      "epoch:21 step:19705 [D loss: 0.542851, acc.: 71.09%] [G loss: 1.350933]\n",
      "epoch:21 step:19706 [D loss: 0.686050, acc.: 57.03%] [G loss: 1.359271]\n",
      "epoch:21 step:19707 [D loss: 0.611786, acc.: 66.41%] [G loss: 1.349266]\n",
      "epoch:21 step:19708 [D loss: 0.624925, acc.: 62.50%] [G loss: 1.003653]\n",
      "epoch:21 step:19709 [D loss: 0.497173, acc.: 76.56%] [G loss: 1.363288]\n",
      "epoch:21 step:19710 [D loss: 0.575913, acc.: 71.88%] [G loss: 0.994618]\n",
      "epoch:21 step:19711 [D loss: 0.537419, acc.: 72.66%] [G loss: 1.522510]\n",
      "epoch:21 step:19712 [D loss: 0.647363, acc.: 68.75%] [G loss: 1.132745]\n",
      "epoch:21 step:19713 [D loss: 0.559466, acc.: 75.00%] [G loss: 1.391300]\n",
      "epoch:21 step:19714 [D loss: 0.506921, acc.: 79.69%] [G loss: 1.117961]\n",
      "epoch:21 step:19715 [D loss: 0.690827, acc.: 58.59%] [G loss: 1.373207]\n",
      "epoch:21 step:19716 [D loss: 0.637591, acc.: 63.28%] [G loss: 1.332923]\n",
      "epoch:21 step:19717 [D loss: 0.541452, acc.: 75.00%] [G loss: 1.155842]\n",
      "epoch:21 step:19718 [D loss: 0.475485, acc.: 77.34%] [G loss: 1.525550]\n",
      "epoch:21 step:19719 [D loss: 0.673192, acc.: 56.25%] [G loss: 1.501812]\n",
      "epoch:21 step:19720 [D loss: 0.509932, acc.: 81.25%] [G loss: 1.634049]\n",
      "epoch:21 step:19721 [D loss: 0.623438, acc.: 68.75%] [G loss: 1.379680]\n",
      "epoch:21 step:19722 [D loss: 0.568899, acc.: 71.09%] [G loss: 1.501048]\n",
      "epoch:21 step:19723 [D loss: 0.666457, acc.: 65.62%] [G loss: 1.455096]\n",
      "epoch:21 step:19724 [D loss: 0.776474, acc.: 46.88%] [G loss: 1.130278]\n",
      "epoch:21 step:19725 [D loss: 0.579510, acc.: 67.19%] [G loss: 0.989401]\n",
      "epoch:21 step:19726 [D loss: 0.618703, acc.: 68.75%] [G loss: 1.143860]\n",
      "epoch:21 step:19727 [D loss: 0.831677, acc.: 43.75%] [G loss: 1.268575]\n",
      "epoch:21 step:19728 [D loss: 0.632084, acc.: 64.84%] [G loss: 1.297544]\n",
      "epoch:21 step:19729 [D loss: 0.707684, acc.: 57.81%] [G loss: 1.001997]\n",
      "epoch:21 step:19730 [D loss: 0.408054, acc.: 84.38%] [G loss: 1.152200]\n",
      "epoch:21 step:19731 [D loss: 0.368808, acc.: 85.94%] [G loss: 1.431959]\n",
      "epoch:21 step:19732 [D loss: 0.578721, acc.: 70.31%] [G loss: 1.133556]\n",
      "epoch:21 step:19733 [D loss: 0.570954, acc.: 70.31%] [G loss: 1.613014]\n",
      "epoch:21 step:19734 [D loss: 0.522896, acc.: 71.09%] [G loss: 1.150670]\n",
      "epoch:21 step:19735 [D loss: 0.576899, acc.: 71.09%] [G loss: 0.994949]\n",
      "epoch:21 step:19736 [D loss: 0.505597, acc.: 78.12%] [G loss: 1.172290]\n",
      "epoch:21 step:19737 [D loss: 0.436535, acc.: 80.47%] [G loss: 1.506719]\n",
      "epoch:21 step:19738 [D loss: 0.629206, acc.: 64.06%] [G loss: 1.308729]\n",
      "epoch:21 step:19739 [D loss: 0.495567, acc.: 75.00%] [G loss: 1.192803]\n",
      "epoch:21 step:19740 [D loss: 0.751815, acc.: 57.81%] [G loss: 1.370335]\n",
      "epoch:21 step:19741 [D loss: 0.521228, acc.: 75.78%] [G loss: 1.331979]\n",
      "epoch:21 step:19742 [D loss: 0.699179, acc.: 65.62%] [G loss: 1.278658]\n",
      "epoch:21 step:19743 [D loss: 0.725890, acc.: 60.16%] [G loss: 1.236476]\n",
      "epoch:21 step:19744 [D loss: 0.445443, acc.: 78.91%] [G loss: 1.440873]\n",
      "epoch:21 step:19745 [D loss: 0.671614, acc.: 60.16%] [G loss: 1.187631]\n",
      "epoch:21 step:19746 [D loss: 0.449723, acc.: 78.91%] [G loss: 1.814940]\n",
      "epoch:21 step:19747 [D loss: 0.771910, acc.: 48.44%] [G loss: 1.284653]\n",
      "epoch:21 step:19748 [D loss: 0.578510, acc.: 68.75%] [G loss: 1.662757]\n",
      "epoch:21 step:19749 [D loss: 0.506094, acc.: 82.03%] [G loss: 1.671481]\n",
      "epoch:21 step:19750 [D loss: 0.546221, acc.: 75.00%] [G loss: 1.744808]\n",
      "epoch:21 step:19751 [D loss: 0.499953, acc.: 76.56%] [G loss: 1.339213]\n",
      "epoch:21 step:19752 [D loss: 0.670145, acc.: 62.50%] [G loss: 1.236782]\n",
      "epoch:21 step:19753 [D loss: 0.562254, acc.: 72.66%] [G loss: 1.382725]\n",
      "epoch:21 step:19754 [D loss: 0.741016, acc.: 54.69%] [G loss: 1.091651]\n",
      "epoch:21 step:19755 [D loss: 0.644986, acc.: 60.16%] [G loss: 1.274726]\n",
      "epoch:21 step:19756 [D loss: 0.743189, acc.: 54.69%] [G loss: 1.190612]\n",
      "epoch:21 step:19757 [D loss: 0.459419, acc.: 80.47%] [G loss: 1.760087]\n",
      "epoch:21 step:19758 [D loss: 0.593473, acc.: 67.19%] [G loss: 1.254957]\n",
      "epoch:21 step:19759 [D loss: 0.497410, acc.: 79.69%] [G loss: 1.192930]\n",
      "epoch:21 step:19760 [D loss: 0.605337, acc.: 67.19%] [G loss: 1.177122]\n",
      "epoch:21 step:19761 [D loss: 0.696369, acc.: 59.38%] [G loss: 1.229749]\n",
      "epoch:21 step:19762 [D loss: 0.481091, acc.: 82.81%] [G loss: 1.415501]\n",
      "epoch:21 step:19763 [D loss: 0.520689, acc.: 73.44%] [G loss: 1.385099]\n",
      "epoch:21 step:19764 [D loss: 0.569429, acc.: 71.88%] [G loss: 1.274318]\n",
      "epoch:21 step:19765 [D loss: 0.589151, acc.: 66.41%] [G loss: 1.586483]\n",
      "epoch:21 step:19766 [D loss: 0.582293, acc.: 75.00%] [G loss: 1.247202]\n",
      "epoch:21 step:19767 [D loss: 0.701829, acc.: 53.12%] [G loss: 0.911876]\n",
      "epoch:21 step:19768 [D loss: 0.594581, acc.: 65.62%] [G loss: 1.777685]\n",
      "epoch:21 step:19769 [D loss: 0.615576, acc.: 65.62%] [G loss: 1.442200]\n",
      "epoch:21 step:19770 [D loss: 0.532388, acc.: 69.53%] [G loss: 1.622871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19771 [D loss: 0.439388, acc.: 81.25%] [G loss: 1.467353]\n",
      "epoch:21 step:19772 [D loss: 0.699483, acc.: 59.38%] [G loss: 1.314664]\n",
      "epoch:21 step:19773 [D loss: 0.600549, acc.: 67.19%] [G loss: 1.332944]\n",
      "epoch:21 step:19774 [D loss: 0.457308, acc.: 82.81%] [G loss: 1.589927]\n",
      "epoch:21 step:19775 [D loss: 0.619552, acc.: 60.16%] [G loss: 1.219828]\n",
      "epoch:21 step:19776 [D loss: 0.481536, acc.: 76.56%] [G loss: 1.559851]\n",
      "epoch:21 step:19777 [D loss: 0.582743, acc.: 67.97%] [G loss: 1.100095]\n",
      "epoch:21 step:19778 [D loss: 0.612306, acc.: 68.75%] [G loss: 1.104697]\n",
      "epoch:21 step:19779 [D loss: 0.584605, acc.: 69.53%] [G loss: 1.505172]\n",
      "epoch:21 step:19780 [D loss: 0.728689, acc.: 57.81%] [G loss: 1.098990]\n",
      "epoch:21 step:19781 [D loss: 0.489814, acc.: 76.56%] [G loss: 1.212266]\n",
      "epoch:21 step:19782 [D loss: 0.455196, acc.: 78.91%] [G loss: 1.354120]\n",
      "epoch:21 step:19783 [D loss: 0.625318, acc.: 66.41%] [G loss: 1.224764]\n",
      "epoch:21 step:19784 [D loss: 0.555681, acc.: 68.75%] [G loss: 1.206189]\n",
      "epoch:21 step:19785 [D loss: 0.488391, acc.: 75.78%] [G loss: 1.359719]\n",
      "epoch:21 step:19786 [D loss: 0.736670, acc.: 58.59%] [G loss: 1.408134]\n",
      "epoch:21 step:19787 [D loss: 0.647923, acc.: 69.53%] [G loss: 1.336107]\n",
      "epoch:21 step:19788 [D loss: 0.679540, acc.: 64.06%] [G loss: 1.369760]\n",
      "epoch:21 step:19789 [D loss: 0.448235, acc.: 80.47%] [G loss: 1.746200]\n",
      "epoch:21 step:19790 [D loss: 0.501034, acc.: 73.44%] [G loss: 1.283610]\n",
      "epoch:21 step:19791 [D loss: 0.445519, acc.: 82.81%] [G loss: 1.448710]\n",
      "epoch:21 step:19792 [D loss: 0.602635, acc.: 67.19%] [G loss: 1.213131]\n",
      "epoch:21 step:19793 [D loss: 0.644853, acc.: 60.94%] [G loss: 1.039112]\n",
      "epoch:21 step:19794 [D loss: 0.629429, acc.: 67.19%] [G loss: 1.589625]\n",
      "epoch:21 step:19795 [D loss: 0.516745, acc.: 78.12%] [G loss: 1.197359]\n",
      "epoch:21 step:19796 [D loss: 0.651957, acc.: 58.59%] [G loss: 1.010249]\n",
      "epoch:21 step:19797 [D loss: 0.575035, acc.: 69.53%] [G loss: 1.522709]\n",
      "epoch:21 step:19798 [D loss: 0.424176, acc.: 85.16%] [G loss: 1.177299]\n",
      "epoch:21 step:19799 [D loss: 0.558929, acc.: 71.09%] [G loss: 1.308153]\n",
      "epoch:21 step:19800 [D loss: 0.485232, acc.: 78.12%] [G loss: 1.286597]\n",
      "##############\n",
      "[2.66538723 2.05640371 1.87839931 2.64160894 0.76425552 5.29778288\n",
      " 2.34826173 2.34838542 3.77740072 7.14868929]\n",
      "##########\n",
      "epoch:21 step:19801 [D loss: 0.484716, acc.: 79.69%] [G loss: 1.299260]\n",
      "epoch:21 step:19802 [D loss: 0.458314, acc.: 81.25%] [G loss: 1.488851]\n",
      "epoch:21 step:19803 [D loss: 0.540632, acc.: 67.97%] [G loss: 1.239482]\n",
      "epoch:21 step:19804 [D loss: 0.587385, acc.: 68.75%] [G loss: 1.440207]\n",
      "epoch:21 step:19805 [D loss: 0.483330, acc.: 75.78%] [G loss: 1.524288]\n",
      "epoch:21 step:19806 [D loss: 0.540109, acc.: 73.44%] [G loss: 1.251733]\n",
      "epoch:21 step:19807 [D loss: 0.452559, acc.: 78.91%] [G loss: 1.603724]\n",
      "epoch:21 step:19808 [D loss: 0.560195, acc.: 71.88%] [G loss: 1.618400]\n",
      "epoch:21 step:19809 [D loss: 0.758774, acc.: 55.47%] [G loss: 1.181228]\n",
      "epoch:21 step:19810 [D loss: 0.578058, acc.: 72.66%] [G loss: 1.012602]\n",
      "epoch:21 step:19811 [D loss: 0.648196, acc.: 67.19%] [G loss: 0.846973]\n",
      "epoch:21 step:19812 [D loss: 0.559304, acc.: 74.22%] [G loss: 1.360372]\n",
      "epoch:21 step:19813 [D loss: 0.626893, acc.: 64.84%] [G loss: 1.680378]\n",
      "epoch:21 step:19814 [D loss: 0.466480, acc.: 79.69%] [G loss: 1.080769]\n",
      "epoch:21 step:19815 [D loss: 0.420121, acc.: 85.94%] [G loss: 1.419698]\n",
      "epoch:21 step:19816 [D loss: 0.620062, acc.: 67.97%] [G loss: 1.173926]\n",
      "epoch:21 step:19817 [D loss: 0.611987, acc.: 66.41%] [G loss: 1.356421]\n",
      "epoch:21 step:19818 [D loss: 0.708209, acc.: 53.12%] [G loss: 1.111354]\n",
      "epoch:21 step:19819 [D loss: 0.528337, acc.: 77.34%] [G loss: 1.568708]\n",
      "epoch:21 step:19820 [D loss: 0.502005, acc.: 75.00%] [G loss: 1.529757]\n",
      "epoch:21 step:19821 [D loss: 0.626121, acc.: 66.41%] [G loss: 1.278883]\n",
      "epoch:21 step:19822 [D loss: 0.471538, acc.: 79.69%] [G loss: 1.673737]\n",
      "epoch:21 step:19823 [D loss: 0.447466, acc.: 80.47%] [G loss: 1.312916]\n",
      "epoch:21 step:19824 [D loss: 0.387034, acc.: 83.59%] [G loss: 1.230794]\n",
      "epoch:21 step:19825 [D loss: 0.647074, acc.: 63.28%] [G loss: 1.134863]\n",
      "epoch:21 step:19826 [D loss: 0.571044, acc.: 71.88%] [G loss: 1.703341]\n",
      "epoch:21 step:19827 [D loss: 0.598093, acc.: 68.75%] [G loss: 1.163437]\n",
      "epoch:21 step:19828 [D loss: 0.609947, acc.: 63.28%] [G loss: 1.362271]\n",
      "epoch:21 step:19829 [D loss: 0.534403, acc.: 78.12%] [G loss: 1.167416]\n",
      "epoch:21 step:19830 [D loss: 0.625997, acc.: 67.97%] [G loss: 1.054584]\n",
      "epoch:21 step:19831 [D loss: 0.502483, acc.: 75.78%] [G loss: 1.356572]\n",
      "epoch:21 step:19832 [D loss: 0.499720, acc.: 74.22%] [G loss: 1.181658]\n",
      "epoch:21 step:19833 [D loss: 0.562451, acc.: 71.88%] [G loss: 1.207798]\n",
      "epoch:21 step:19834 [D loss: 0.696507, acc.: 57.03%] [G loss: 1.239467]\n",
      "epoch:21 step:19835 [D loss: 0.561122, acc.: 67.19%] [G loss: 1.416681]\n",
      "epoch:21 step:19836 [D loss: 0.535577, acc.: 74.22%] [G loss: 1.051143]\n",
      "epoch:21 step:19837 [D loss: 0.470581, acc.: 78.91%] [G loss: 1.363015]\n",
      "epoch:21 step:19838 [D loss: 0.487687, acc.: 75.78%] [G loss: 1.601397]\n",
      "epoch:21 step:19839 [D loss: 0.654189, acc.: 61.72%] [G loss: 1.440893]\n",
      "epoch:21 step:19840 [D loss: 0.541075, acc.: 69.53%] [G loss: 1.359866]\n",
      "epoch:21 step:19841 [D loss: 0.469867, acc.: 76.56%] [G loss: 1.605319]\n",
      "epoch:21 step:19842 [D loss: 0.554875, acc.: 69.53%] [G loss: 1.122882]\n",
      "epoch:21 step:19843 [D loss: 0.518203, acc.: 75.78%] [G loss: 1.101285]\n",
      "epoch:21 step:19844 [D loss: 0.399765, acc.: 85.16%] [G loss: 1.540008]\n",
      "epoch:21 step:19845 [D loss: 0.608153, acc.: 64.84%] [G loss: 1.312429]\n",
      "epoch:21 step:19846 [D loss: 0.600767, acc.: 67.19%] [G loss: 1.279157]\n",
      "epoch:21 step:19847 [D loss: 0.458391, acc.: 80.47%] [G loss: 1.446726]\n",
      "epoch:21 step:19848 [D loss: 0.703633, acc.: 59.38%] [G loss: 1.182387]\n",
      "epoch:21 step:19849 [D loss: 0.564122, acc.: 67.97%] [G loss: 1.518407]\n",
      "epoch:21 step:19850 [D loss: 0.563713, acc.: 70.31%] [G loss: 1.232925]\n",
      "epoch:21 step:19851 [D loss: 0.447235, acc.: 82.03%] [G loss: 1.645965]\n",
      "epoch:21 step:19852 [D loss: 0.464900, acc.: 80.47%] [G loss: 1.307424]\n",
      "epoch:21 step:19853 [D loss: 0.592760, acc.: 64.84%] [G loss: 1.657013]\n",
      "epoch:21 step:19854 [D loss: 0.462433, acc.: 81.25%] [G loss: 1.656961]\n",
      "epoch:21 step:19855 [D loss: 0.582777, acc.: 71.09%] [G loss: 1.420803]\n",
      "epoch:21 step:19856 [D loss: 0.659201, acc.: 64.06%] [G loss: 1.391574]\n",
      "epoch:21 step:19857 [D loss: 0.536717, acc.: 71.88%] [G loss: 1.334395]\n",
      "epoch:21 step:19858 [D loss: 0.527864, acc.: 73.44%] [G loss: 1.571968]\n",
      "epoch:21 step:19859 [D loss: 0.545363, acc.: 71.09%] [G loss: 1.396788]\n",
      "epoch:21 step:19860 [D loss: 0.598569, acc.: 65.62%] [G loss: 1.305802]\n",
      "epoch:21 step:19861 [D loss: 0.516849, acc.: 76.56%] [G loss: 1.382066]\n",
      "epoch:21 step:19862 [D loss: 0.451887, acc.: 82.03%] [G loss: 1.300543]\n",
      "epoch:21 step:19863 [D loss: 0.441905, acc.: 83.59%] [G loss: 1.523680]\n",
      "epoch:21 step:19864 [D loss: 0.512165, acc.: 76.56%] [G loss: 1.141371]\n",
      "epoch:21 step:19865 [D loss: 0.575928, acc.: 67.97%] [G loss: 1.621659]\n",
      "epoch:21 step:19866 [D loss: 0.482363, acc.: 77.34%] [G loss: 1.545648]\n",
      "epoch:21 step:19867 [D loss: 0.764574, acc.: 50.00%] [G loss: 1.380360]\n",
      "epoch:21 step:19868 [D loss: 0.602635, acc.: 66.41%] [G loss: 1.598659]\n",
      "epoch:21 step:19869 [D loss: 0.627936, acc.: 70.31%] [G loss: 1.383418]\n",
      "epoch:21 step:19870 [D loss: 0.617173, acc.: 62.50%] [G loss: 1.316583]\n",
      "epoch:21 step:19871 [D loss: 0.821215, acc.: 49.22%] [G loss: 1.238760]\n",
      "epoch:21 step:19872 [D loss: 0.681027, acc.: 55.47%] [G loss: 1.299280]\n",
      "epoch:21 step:19873 [D loss: 0.639188, acc.: 62.50%] [G loss: 1.329011]\n",
      "epoch:21 step:19874 [D loss: 0.595354, acc.: 67.19%] [G loss: 1.116797]\n",
      "epoch:21 step:19875 [D loss: 0.618631, acc.: 61.72%] [G loss: 1.381363]\n",
      "epoch:21 step:19876 [D loss: 0.482440, acc.: 77.34%] [G loss: 1.720632]\n",
      "epoch:21 step:19877 [D loss: 0.466664, acc.: 80.47%] [G loss: 1.631614]\n",
      "epoch:21 step:19878 [D loss: 0.446019, acc.: 80.47%] [G loss: 1.564263]\n",
      "epoch:21 step:19879 [D loss: 0.591546, acc.: 67.97%] [G loss: 1.444965]\n",
      "epoch:21 step:19880 [D loss: 0.457818, acc.: 78.91%] [G loss: 1.563475]\n",
      "epoch:21 step:19881 [D loss: 0.391550, acc.: 86.72%] [G loss: 1.332744]\n",
      "epoch:21 step:19882 [D loss: 0.542843, acc.: 72.66%] [G loss: 1.471889]\n",
      "epoch:21 step:19883 [D loss: 0.546683, acc.: 73.44%] [G loss: 1.167343]\n",
      "epoch:21 step:19884 [D loss: 0.619372, acc.: 67.19%] [G loss: 1.380097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19885 [D loss: 0.386544, acc.: 81.25%] [G loss: 1.409295]\n",
      "epoch:21 step:19886 [D loss: 0.687449, acc.: 59.38%] [G loss: 1.347591]\n",
      "epoch:21 step:19887 [D loss: 0.522491, acc.: 76.56%] [G loss: 1.154320]\n",
      "epoch:21 step:19888 [D loss: 0.436216, acc.: 79.69%] [G loss: 1.583935]\n",
      "epoch:21 step:19889 [D loss: 0.540794, acc.: 73.44%] [G loss: 1.486752]\n",
      "epoch:21 step:19890 [D loss: 0.519660, acc.: 71.09%] [G loss: 1.565594]\n",
      "epoch:21 step:19891 [D loss: 0.759422, acc.: 52.34%] [G loss: 1.419312]\n",
      "epoch:21 step:19892 [D loss: 0.582792, acc.: 71.09%] [G loss: 1.096324]\n",
      "epoch:21 step:19893 [D loss: 0.790241, acc.: 49.22%] [G loss: 1.236383]\n",
      "epoch:21 step:19894 [D loss: 0.390096, acc.: 86.72%] [G loss: 1.557812]\n",
      "epoch:21 step:19895 [D loss: 0.535950, acc.: 71.09%] [G loss: 1.430099]\n",
      "epoch:21 step:19896 [D loss: 0.457058, acc.: 75.00%] [G loss: 1.578205]\n",
      "epoch:21 step:19897 [D loss: 0.546251, acc.: 75.00%] [G loss: 1.562812]\n",
      "epoch:21 step:19898 [D loss: 0.610373, acc.: 67.19%] [G loss: 1.358924]\n",
      "epoch:21 step:19899 [D loss: 0.729186, acc.: 58.59%] [G loss: 1.207353]\n",
      "epoch:21 step:19900 [D loss: 0.502074, acc.: 77.34%] [G loss: 1.491343]\n",
      "epoch:21 step:19901 [D loss: 0.598691, acc.: 69.53%] [G loss: 1.180879]\n",
      "epoch:21 step:19902 [D loss: 0.521495, acc.: 73.44%] [G loss: 1.424080]\n",
      "epoch:21 step:19903 [D loss: 0.599317, acc.: 63.28%] [G loss: 1.218773]\n",
      "epoch:21 step:19904 [D loss: 0.477651, acc.: 77.34%] [G loss: 1.070499]\n",
      "epoch:21 step:19905 [D loss: 0.553238, acc.: 71.88%] [G loss: 1.419059]\n",
      "epoch:21 step:19906 [D loss: 0.485056, acc.: 76.56%] [G loss: 1.460088]\n",
      "epoch:21 step:19907 [D loss: 0.655436, acc.: 58.59%] [G loss: 1.219101]\n",
      "epoch:21 step:19908 [D loss: 0.596449, acc.: 65.62%] [G loss: 1.365372]\n",
      "epoch:21 step:19909 [D loss: 0.638843, acc.: 58.59%] [G loss: 1.427309]\n",
      "epoch:21 step:19910 [D loss: 0.420887, acc.: 82.03%] [G loss: 1.464206]\n",
      "epoch:21 step:19911 [D loss: 0.624838, acc.: 65.62%] [G loss: 1.196998]\n",
      "epoch:21 step:19912 [D loss: 0.585702, acc.: 72.66%] [G loss: 1.191238]\n",
      "epoch:21 step:19913 [D loss: 0.555562, acc.: 74.22%] [G loss: 1.420849]\n",
      "epoch:21 step:19914 [D loss: 0.615469, acc.: 64.06%] [G loss: 0.949145]\n",
      "epoch:21 step:19915 [D loss: 0.553539, acc.: 71.88%] [G loss: 1.531650]\n",
      "epoch:21 step:19916 [D loss: 0.461215, acc.: 78.91%] [G loss: 0.992321]\n",
      "epoch:21 step:19917 [D loss: 0.631167, acc.: 65.62%] [G loss: 1.075189]\n",
      "epoch:21 step:19918 [D loss: 0.487126, acc.: 78.12%] [G loss: 1.105094]\n",
      "epoch:21 step:19919 [D loss: 0.651727, acc.: 63.28%] [G loss: 1.170316]\n",
      "epoch:21 step:19920 [D loss: 0.689338, acc.: 61.72%] [G loss: 1.040108]\n",
      "epoch:21 step:19921 [D loss: 0.484606, acc.: 79.69%] [G loss: 0.998118]\n",
      "epoch:21 step:19922 [D loss: 0.486260, acc.: 78.12%] [G loss: 1.185380]\n",
      "epoch:21 step:19923 [D loss: 0.470816, acc.: 82.81%] [G loss: 1.666122]\n",
      "epoch:21 step:19924 [D loss: 0.506265, acc.: 76.56%] [G loss: 1.063978]\n",
      "epoch:21 step:19925 [D loss: 0.594443, acc.: 69.53%] [G loss: 1.307067]\n",
      "epoch:21 step:19926 [D loss: 0.377952, acc.: 87.50%] [G loss: 1.408218]\n",
      "epoch:21 step:19927 [D loss: 0.635043, acc.: 60.16%] [G loss: 1.397737]\n",
      "epoch:21 step:19928 [D loss: 0.531617, acc.: 72.66%] [G loss: 1.170251]\n",
      "epoch:21 step:19929 [D loss: 0.427680, acc.: 82.03%] [G loss: 1.391872]\n",
      "epoch:21 step:19930 [D loss: 0.607900, acc.: 67.97%] [G loss: 1.078715]\n",
      "epoch:21 step:19931 [D loss: 0.574604, acc.: 71.09%] [G loss: 1.388609]\n",
      "epoch:21 step:19932 [D loss: 0.541571, acc.: 69.53%] [G loss: 1.372311]\n",
      "epoch:21 step:19933 [D loss: 0.609019, acc.: 63.28%] [G loss: 1.355967]\n",
      "epoch:21 step:19934 [D loss: 0.597327, acc.: 66.41%] [G loss: 1.043959]\n",
      "epoch:21 step:19935 [D loss: 0.544652, acc.: 75.00%] [G loss: 1.496448]\n",
      "epoch:21 step:19936 [D loss: 0.672745, acc.: 58.59%] [G loss: 1.316384]\n",
      "epoch:21 step:19937 [D loss: 0.639375, acc.: 58.59%] [G loss: 1.213981]\n",
      "epoch:21 step:19938 [D loss: 0.603999, acc.: 62.50%] [G loss: 1.443715]\n",
      "epoch:21 step:19939 [D loss: 0.704518, acc.: 57.03%] [G loss: 1.257469]\n",
      "epoch:21 step:19940 [D loss: 0.733701, acc.: 55.47%] [G loss: 1.156286]\n",
      "epoch:21 step:19941 [D loss: 0.562747, acc.: 68.75%] [G loss: 1.433045]\n",
      "epoch:21 step:19942 [D loss: 0.443271, acc.: 83.59%] [G loss: 1.528633]\n",
      "epoch:21 step:19943 [D loss: 0.662935, acc.: 60.94%] [G loss: 1.574522]\n",
      "epoch:21 step:19944 [D loss: 0.587915, acc.: 69.53%] [G loss: 1.432357]\n",
      "epoch:21 step:19945 [D loss: 0.625053, acc.: 67.97%] [G loss: 1.532458]\n",
      "epoch:21 step:19946 [D loss: 0.509005, acc.: 75.78%] [G loss: 1.252483]\n",
      "epoch:21 step:19947 [D loss: 0.640389, acc.: 63.28%] [G loss: 1.409601]\n",
      "epoch:21 step:19948 [D loss: 0.447622, acc.: 81.25%] [G loss: 1.516551]\n",
      "epoch:21 step:19949 [D loss: 0.697747, acc.: 61.72%] [G loss: 1.319825]\n",
      "epoch:21 step:19950 [D loss: 0.645079, acc.: 64.84%] [G loss: 1.233858]\n",
      "epoch:21 step:19951 [D loss: 0.578035, acc.: 68.75%] [G loss: 1.040022]\n",
      "epoch:21 step:19952 [D loss: 0.696504, acc.: 60.94%] [G loss: 1.212593]\n",
      "epoch:21 step:19953 [D loss: 0.564137, acc.: 72.66%] [G loss: 1.120316]\n",
      "epoch:21 step:19954 [D loss: 0.503487, acc.: 76.56%] [G loss: 1.126713]\n",
      "epoch:21 step:19955 [D loss: 0.709430, acc.: 59.38%] [G loss: 0.907417]\n",
      "epoch:21 step:19956 [D loss: 0.559750, acc.: 70.31%] [G loss: 1.230630]\n",
      "epoch:21 step:19957 [D loss: 0.662281, acc.: 60.16%] [G loss: 1.200916]\n",
      "epoch:21 step:19958 [D loss: 0.513406, acc.: 74.22%] [G loss: 1.477684]\n",
      "epoch:21 step:19959 [D loss: 0.475875, acc.: 79.69%] [G loss: 1.247953]\n",
      "epoch:21 step:19960 [D loss: 0.487505, acc.: 74.22%] [G loss: 1.349320]\n",
      "epoch:21 step:19961 [D loss: 0.685635, acc.: 64.06%] [G loss: 1.398281]\n",
      "epoch:21 step:19962 [D loss: 0.498836, acc.: 77.34%] [G loss: 1.410310]\n",
      "epoch:21 step:19963 [D loss: 0.702669, acc.: 59.38%] [G loss: 1.096139]\n",
      "epoch:21 step:19964 [D loss: 0.467338, acc.: 78.12%] [G loss: 1.507812]\n",
      "epoch:21 step:19965 [D loss: 0.653416, acc.: 67.97%] [G loss: 1.288344]\n",
      "epoch:21 step:19966 [D loss: 0.727244, acc.: 58.59%] [G loss: 1.259163]\n",
      "epoch:21 step:19967 [D loss: 0.460965, acc.: 78.12%] [G loss: 1.247583]\n",
      "epoch:21 step:19968 [D loss: 0.627050, acc.: 65.62%] [G loss: 1.232468]\n",
      "epoch:21 step:19969 [D loss: 0.601554, acc.: 70.31%] [G loss: 1.309937]\n",
      "epoch:21 step:19970 [D loss: 0.592315, acc.: 72.66%] [G loss: 1.545295]\n",
      "epoch:21 step:19971 [D loss: 0.484938, acc.: 76.56%] [G loss: 1.544450]\n",
      "epoch:21 step:19972 [D loss: 0.739433, acc.: 55.47%] [G loss: 1.346622]\n",
      "epoch:21 step:19973 [D loss: 0.508427, acc.: 78.91%] [G loss: 1.379732]\n",
      "epoch:21 step:19974 [D loss: 0.528161, acc.: 76.56%] [G loss: 1.312199]\n",
      "epoch:21 step:19975 [D loss: 0.456941, acc.: 81.25%] [G loss: 1.625273]\n",
      "epoch:21 step:19976 [D loss: 0.593086, acc.: 65.62%] [G loss: 1.148464]\n",
      "epoch:21 step:19977 [D loss: 0.598249, acc.: 68.75%] [G loss: 1.273559]\n",
      "epoch:21 step:19978 [D loss: 0.675511, acc.: 64.84%] [G loss: 1.445225]\n",
      "epoch:21 step:19979 [D loss: 0.548008, acc.: 72.66%] [G loss: 1.399561]\n",
      "epoch:21 step:19980 [D loss: 0.576453, acc.: 71.09%] [G loss: 1.134690]\n",
      "epoch:21 step:19981 [D loss: 0.524665, acc.: 73.44%] [G loss: 1.164423]\n",
      "epoch:21 step:19982 [D loss: 0.661171, acc.: 64.06%] [G loss: 1.196311]\n",
      "epoch:21 step:19983 [D loss: 0.538521, acc.: 70.31%] [G loss: 1.268801]\n",
      "epoch:21 step:19984 [D loss: 0.490432, acc.: 77.34%] [G loss: 1.410136]\n",
      "epoch:21 step:19985 [D loss: 0.629238, acc.: 61.72%] [G loss: 1.771442]\n",
      "epoch:21 step:19986 [D loss: 0.427682, acc.: 82.81%] [G loss: 1.530481]\n",
      "epoch:21 step:19987 [D loss: 0.471677, acc.: 78.91%] [G loss: 1.402672]\n",
      "epoch:21 step:19988 [D loss: 0.504802, acc.: 78.91%] [G loss: 1.830020]\n",
      "epoch:21 step:19989 [D loss: 0.568815, acc.: 69.53%] [G loss: 1.118998]\n",
      "epoch:21 step:19990 [D loss: 0.580375, acc.: 70.31%] [G loss: 1.327441]\n",
      "epoch:21 step:19991 [D loss: 0.496464, acc.: 78.91%] [G loss: 1.166107]\n",
      "epoch:21 step:19992 [D loss: 0.474768, acc.: 78.91%] [G loss: 1.716052]\n",
      "epoch:21 step:19993 [D loss: 0.695384, acc.: 59.38%] [G loss: 1.327711]\n",
      "epoch:21 step:19994 [D loss: 0.539481, acc.: 77.34%] [G loss: 1.122212]\n",
      "epoch:21 step:19995 [D loss: 0.626394, acc.: 66.41%] [G loss: 1.208810]\n",
      "epoch:21 step:19996 [D loss: 0.728254, acc.: 58.59%] [G loss: 1.236567]\n",
      "epoch:21 step:19997 [D loss: 0.630440, acc.: 64.84%] [G loss: 1.185852]\n",
      "epoch:21 step:19998 [D loss: 0.534997, acc.: 72.66%] [G loss: 1.615329]\n",
      "epoch:21 step:19999 [D loss: 0.505104, acc.: 78.12%] [G loss: 1.275296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20000 [D loss: 0.679957, acc.: 59.38%] [G loss: 1.297910]\n",
      "##############\n",
      "[2.80494896 2.35611486 2.44953241 3.1017718  1.50257014 5.81868523\n",
      " 2.2690291  2.67956611 4.09864778 8.14868929]\n",
      "##########\n",
      "epoch:21 step:20001 [D loss: 0.491544, acc.: 78.91%] [G loss: 1.463426]\n",
      "epoch:21 step:20002 [D loss: 0.605595, acc.: 67.19%] [G loss: 1.147317]\n",
      "epoch:21 step:20003 [D loss: 0.658788, acc.: 65.62%] [G loss: 1.229052]\n",
      "epoch:21 step:20004 [D loss: 0.387089, acc.: 83.59%] [G loss: 1.493223]\n",
      "epoch:21 step:20005 [D loss: 0.427422, acc.: 82.03%] [G loss: 1.183569]\n",
      "epoch:21 step:20006 [D loss: 0.471793, acc.: 77.34%] [G loss: 1.377598]\n",
      "epoch:21 step:20007 [D loss: 0.602355, acc.: 64.06%] [G loss: 1.360710]\n",
      "epoch:21 step:20008 [D loss: 0.485084, acc.: 75.78%] [G loss: 1.275788]\n",
      "epoch:21 step:20009 [D loss: 0.564959, acc.: 72.66%] [G loss: 1.228324]\n",
      "epoch:21 step:20010 [D loss: 0.567617, acc.: 71.09%] [G loss: 1.353597]\n",
      "epoch:21 step:20011 [D loss: 0.426930, acc.: 82.03%] [G loss: 1.433780]\n",
      "epoch:21 step:20012 [D loss: 0.547392, acc.: 67.19%] [G loss: 1.072297]\n",
      "epoch:21 step:20013 [D loss: 0.437518, acc.: 82.81%] [G loss: 1.981090]\n",
      "epoch:21 step:20014 [D loss: 0.672928, acc.: 64.06%] [G loss: 1.605290]\n",
      "epoch:21 step:20015 [D loss: 0.623088, acc.: 65.62%] [G loss: 0.933721]\n",
      "epoch:21 step:20016 [D loss: 0.441659, acc.: 85.94%] [G loss: 1.375684]\n",
      "epoch:21 step:20017 [D loss: 0.555694, acc.: 71.09%] [G loss: 1.362694]\n",
      "epoch:21 step:20018 [D loss: 0.543119, acc.: 71.88%] [G loss: 1.494376]\n",
      "epoch:21 step:20019 [D loss: 0.619945, acc.: 64.84%] [G loss: 1.423782]\n",
      "epoch:21 step:20020 [D loss: 0.623693, acc.: 67.19%] [G loss: 1.214602]\n",
      "epoch:21 step:20021 [D loss: 0.527901, acc.: 72.66%] [G loss: 1.142204]\n",
      "epoch:21 step:20022 [D loss: 0.628092, acc.: 62.50%] [G loss: 1.487869]\n",
      "epoch:21 step:20023 [D loss: 0.532606, acc.: 72.66%] [G loss: 1.767277]\n",
      "epoch:21 step:20024 [D loss: 0.750561, acc.: 58.59%] [G loss: 1.082342]\n",
      "epoch:21 step:20025 [D loss: 0.541233, acc.: 71.09%] [G loss: 1.437326]\n",
      "epoch:21 step:20026 [D loss: 0.481475, acc.: 78.12%] [G loss: 1.433812]\n",
      "epoch:21 step:20027 [D loss: 0.646487, acc.: 65.62%] [G loss: 1.114238]\n",
      "epoch:21 step:20028 [D loss: 0.546279, acc.: 75.78%] [G loss: 1.509053]\n",
      "epoch:21 step:20029 [D loss: 0.648734, acc.: 64.84%] [G loss: 1.166250]\n",
      "epoch:21 step:20030 [D loss: 0.563923, acc.: 68.75%] [G loss: 1.307310]\n",
      "epoch:21 step:20031 [D loss: 0.630003, acc.: 62.50%] [G loss: 1.326780]\n",
      "epoch:21 step:20032 [D loss: 0.640399, acc.: 63.28%] [G loss: 1.133281]\n",
      "epoch:21 step:20033 [D loss: 0.511546, acc.: 78.12%] [G loss: 1.215450]\n",
      "epoch:21 step:20034 [D loss: 0.533666, acc.: 76.56%] [G loss: 1.465115]\n",
      "epoch:21 step:20035 [D loss: 0.494834, acc.: 79.69%] [G loss: 1.674016]\n",
      "epoch:21 step:20036 [D loss: 0.679125, acc.: 60.16%] [G loss: 1.341326]\n",
      "epoch:21 step:20037 [D loss: 0.503708, acc.: 74.22%] [G loss: 1.637289]\n",
      "epoch:21 step:20038 [D loss: 0.647254, acc.: 65.62%] [G loss: 1.149727]\n",
      "epoch:21 step:20039 [D loss: 0.547343, acc.: 68.75%] [G loss: 1.513858]\n",
      "epoch:21 step:20040 [D loss: 0.500719, acc.: 73.44%] [G loss: 1.245171]\n",
      "epoch:21 step:20041 [D loss: 0.399801, acc.: 85.16%] [G loss: 1.363809]\n",
      "epoch:21 step:20042 [D loss: 0.631347, acc.: 64.84%] [G loss: 1.569734]\n",
      "epoch:21 step:20043 [D loss: 0.498247, acc.: 80.47%] [G loss: 1.252644]\n",
      "epoch:21 step:20044 [D loss: 0.533167, acc.: 71.09%] [G loss: 1.386494]\n",
      "epoch:21 step:20045 [D loss: 0.466370, acc.: 81.25%] [G loss: 1.269811]\n",
      "epoch:21 step:20046 [D loss: 0.610492, acc.: 65.62%] [G loss: 1.295016]\n",
      "epoch:21 step:20047 [D loss: 0.540018, acc.: 74.22%] [G loss: 1.624215]\n",
      "epoch:21 step:20048 [D loss: 0.578848, acc.: 70.31%] [G loss: 1.412368]\n",
      "epoch:21 step:20049 [D loss: 0.472088, acc.: 79.69%] [G loss: 1.437676]\n",
      "epoch:21 step:20050 [D loss: 0.614191, acc.: 66.41%] [G loss: 1.355726]\n",
      "epoch:21 step:20051 [D loss: 0.808106, acc.: 46.88%] [G loss: 1.087787]\n",
      "epoch:21 step:20052 [D loss: 0.625650, acc.: 65.62%] [G loss: 1.245715]\n",
      "epoch:21 step:20053 [D loss: 0.442394, acc.: 82.03%] [G loss: 1.209080]\n",
      "epoch:21 step:20054 [D loss: 0.526495, acc.: 75.78%] [G loss: 0.970867]\n",
      "epoch:21 step:20055 [D loss: 0.558443, acc.: 71.88%] [G loss: 1.576393]\n",
      "epoch:21 step:20056 [D loss: 0.454286, acc.: 76.56%] [G loss: 1.025693]\n",
      "epoch:21 step:20057 [D loss: 0.512146, acc.: 73.44%] [G loss: 1.249883]\n",
      "epoch:21 step:20058 [D loss: 0.406887, acc.: 87.50%] [G loss: 1.480659]\n",
      "epoch:21 step:20059 [D loss: 0.599135, acc.: 70.31%] [G loss: 1.223455]\n",
      "epoch:21 step:20060 [D loss: 0.582875, acc.: 62.50%] [G loss: 1.328442]\n",
      "epoch:21 step:20061 [D loss: 0.712972, acc.: 63.28%] [G loss: 1.544079]\n",
      "epoch:21 step:20062 [D loss: 0.495741, acc.: 78.12%] [G loss: 1.492346]\n",
      "epoch:21 step:20063 [D loss: 0.610461, acc.: 69.53%] [G loss: 1.644284]\n",
      "epoch:21 step:20064 [D loss: 0.476485, acc.: 80.47%] [G loss: 1.445287]\n",
      "epoch:21 step:20065 [D loss: 0.607332, acc.: 66.41%] [G loss: 1.295218]\n",
      "epoch:21 step:20066 [D loss: 0.580186, acc.: 68.75%] [G loss: 1.306898]\n",
      "epoch:21 step:20067 [D loss: 0.601137, acc.: 71.88%] [G loss: 1.222301]\n",
      "epoch:21 step:20068 [D loss: 0.494801, acc.: 78.12%] [G loss: 1.037632]\n",
      "epoch:21 step:20069 [D loss: 0.506291, acc.: 75.78%] [G loss: 1.484583]\n",
      "epoch:21 step:20070 [D loss: 0.645523, acc.: 64.06%] [G loss: 1.318064]\n",
      "epoch:21 step:20071 [D loss: 0.694507, acc.: 57.81%] [G loss: 1.375514]\n",
      "epoch:21 step:20072 [D loss: 0.558429, acc.: 69.53%] [G loss: 1.611968]\n",
      "epoch:21 step:20073 [D loss: 0.549794, acc.: 74.22%] [G loss: 1.547711]\n",
      "epoch:21 step:20074 [D loss: 0.694406, acc.: 59.38%] [G loss: 1.295070]\n",
      "epoch:21 step:20075 [D loss: 0.555799, acc.: 71.09%] [G loss: 1.255224]\n",
      "epoch:21 step:20076 [D loss: 0.631153, acc.: 60.94%] [G loss: 1.426260]\n",
      "epoch:21 step:20077 [D loss: 0.555279, acc.: 71.88%] [G loss: 1.324878]\n",
      "epoch:21 step:20078 [D loss: 0.493658, acc.: 78.12%] [G loss: 1.329462]\n",
      "epoch:21 step:20079 [D loss: 0.529415, acc.: 75.00%] [G loss: 1.403195]\n",
      "epoch:21 step:20080 [D loss: 0.563903, acc.: 69.53%] [G loss: 1.683845]\n",
      "epoch:21 step:20081 [D loss: 0.588040, acc.: 66.41%] [G loss: 1.333477]\n",
      "epoch:21 step:20082 [D loss: 0.462536, acc.: 78.12%] [G loss: 1.270800]\n",
      "epoch:21 step:20083 [D loss: 0.458172, acc.: 85.16%] [G loss: 1.166771]\n",
      "epoch:21 step:20084 [D loss: 0.581086, acc.: 75.00%] [G loss: 1.396025]\n",
      "epoch:21 step:20085 [D loss: 0.575461, acc.: 69.53%] [G loss: 1.237493]\n",
      "epoch:21 step:20086 [D loss: 0.532396, acc.: 75.78%] [G loss: 1.572817]\n",
      "epoch:21 step:20087 [D loss: 0.599232, acc.: 67.97%] [G loss: 1.111477]\n",
      "epoch:21 step:20088 [D loss: 0.580444, acc.: 64.84%] [G loss: 1.266312]\n",
      "epoch:21 step:20089 [D loss: 0.696753, acc.: 55.47%] [G loss: 1.255669]\n",
      "epoch:21 step:20090 [D loss: 0.602667, acc.: 68.75%] [G loss: 1.204721]\n",
      "epoch:21 step:20091 [D loss: 0.628812, acc.: 60.16%] [G loss: 1.497061]\n",
      "epoch:21 step:20092 [D loss: 0.474907, acc.: 81.25%] [G loss: 1.638401]\n",
      "epoch:21 step:20093 [D loss: 0.480102, acc.: 77.34%] [G loss: 1.438253]\n",
      "epoch:21 step:20094 [D loss: 0.481316, acc.: 78.91%] [G loss: 1.745065]\n",
      "epoch:21 step:20095 [D loss: 0.554499, acc.: 78.12%] [G loss: 1.550653]\n",
      "epoch:21 step:20096 [D loss: 0.632360, acc.: 62.50%] [G loss: 1.319016]\n",
      "epoch:21 step:20097 [D loss: 0.488981, acc.: 78.12%] [G loss: 1.425575]\n",
      "epoch:21 step:20098 [D loss: 0.599215, acc.: 64.06%] [G loss: 1.398786]\n",
      "epoch:21 step:20099 [D loss: 0.666586, acc.: 57.03%] [G loss: 1.148175]\n",
      "epoch:21 step:20100 [D loss: 0.502690, acc.: 75.00%] [G loss: 1.248756]\n",
      "epoch:21 step:20101 [D loss: 0.420846, acc.: 86.72%] [G loss: 1.419454]\n",
      "epoch:21 step:20102 [D loss: 0.663288, acc.: 60.94%] [G loss: 1.083030]\n",
      "epoch:21 step:20103 [D loss: 0.702512, acc.: 60.16%] [G loss: 1.349840]\n",
      "epoch:21 step:20104 [D loss: 0.639990, acc.: 59.38%] [G loss: 1.394455]\n",
      "epoch:21 step:20105 [D loss: 0.558509, acc.: 73.44%] [G loss: 1.424754]\n",
      "epoch:21 step:20106 [D loss: 0.577113, acc.: 65.62%] [G loss: 1.645517]\n",
      "epoch:21 step:20107 [D loss: 0.615838, acc.: 66.41%] [G loss: 1.485262]\n",
      "epoch:21 step:20108 [D loss: 0.580716, acc.: 66.41%] [G loss: 1.184816]\n",
      "epoch:21 step:20109 [D loss: 0.419913, acc.: 85.16%] [G loss: 1.475141]\n",
      "epoch:21 step:20110 [D loss: 0.656143, acc.: 64.06%] [G loss: 1.016535]\n",
      "epoch:21 step:20111 [D loss: 0.631883, acc.: 65.62%] [G loss: 1.595602]\n",
      "epoch:21 step:20112 [D loss: 0.604457, acc.: 60.94%] [G loss: 1.168707]\n",
      "epoch:21 step:20113 [D loss: 0.621015, acc.: 70.31%] [G loss: 1.487487]\n",
      "epoch:21 step:20114 [D loss: 0.519849, acc.: 75.00%] [G loss: 1.350647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20115 [D loss: 0.720453, acc.: 53.91%] [G loss: 1.335789]\n",
      "epoch:21 step:20116 [D loss: 0.521447, acc.: 77.34%] [G loss: 1.485762]\n",
      "epoch:21 step:20117 [D loss: 0.674833, acc.: 64.06%] [G loss: 1.321579]\n",
      "epoch:21 step:20118 [D loss: 0.628944, acc.: 65.62%] [G loss: 1.515277]\n",
      "epoch:21 step:20119 [D loss: 0.482401, acc.: 76.56%] [G loss: 1.293382]\n",
      "epoch:21 step:20120 [D loss: 0.503372, acc.: 75.78%] [G loss: 1.320096]\n",
      "epoch:21 step:20121 [D loss: 0.599007, acc.: 68.75%] [G loss: 1.541918]\n",
      "epoch:21 step:20122 [D loss: 0.461677, acc.: 81.25%] [G loss: 1.193408]\n",
      "epoch:21 step:20123 [D loss: 0.491455, acc.: 75.78%] [G loss: 1.582652]\n",
      "epoch:21 step:20124 [D loss: 0.461759, acc.: 79.69%] [G loss: 1.588324]\n",
      "epoch:21 step:20125 [D loss: 0.732166, acc.: 58.59%] [G loss: 1.334999]\n",
      "epoch:21 step:20126 [D loss: 0.614535, acc.: 72.66%] [G loss: 1.016595]\n",
      "epoch:21 step:20127 [D loss: 0.460244, acc.: 82.81%] [G loss: 1.346057]\n",
      "epoch:21 step:20128 [D loss: 0.549631, acc.: 72.66%] [G loss: 1.550680]\n",
      "epoch:21 step:20129 [D loss: 0.589147, acc.: 65.62%] [G loss: 1.546143]\n",
      "epoch:21 step:20130 [D loss: 0.541987, acc.: 71.09%] [G loss: 1.487450]\n",
      "epoch:21 step:20131 [D loss: 0.559551, acc.: 70.31%] [G loss: 1.171696]\n",
      "epoch:21 step:20132 [D loss: 0.774086, acc.: 50.78%] [G loss: 1.074348]\n",
      "epoch:21 step:20133 [D loss: 0.470068, acc.: 78.91%] [G loss: 1.516093]\n",
      "epoch:21 step:20134 [D loss: 0.643813, acc.: 62.50%] [G loss: 1.229520]\n",
      "epoch:21 step:20135 [D loss: 0.459344, acc.: 83.59%] [G loss: 1.162732]\n",
      "epoch:21 step:20136 [D loss: 0.479179, acc.: 78.91%] [G loss: 1.452344]\n",
      "epoch:21 step:20137 [D loss: 0.509314, acc.: 75.78%] [G loss: 1.606055]\n",
      "epoch:21 step:20138 [D loss: 0.576748, acc.: 67.19%] [G loss: 1.265327]\n",
      "epoch:21 step:20139 [D loss: 0.502265, acc.: 80.47%] [G loss: 1.146837]\n",
      "epoch:21 step:20140 [D loss: 0.652822, acc.: 63.28%] [G loss: 1.308360]\n",
      "epoch:21 step:20141 [D loss: 0.581735, acc.: 71.88%] [G loss: 1.352776]\n",
      "epoch:21 step:20142 [D loss: 0.666949, acc.: 60.94%] [G loss: 1.265447]\n",
      "epoch:21 step:20143 [D loss: 0.465665, acc.: 75.00%] [G loss: 1.664950]\n",
      "epoch:21 step:20144 [D loss: 0.524162, acc.: 71.09%] [G loss: 1.473375]\n",
      "epoch:21 step:20145 [D loss: 0.465390, acc.: 83.59%] [G loss: 1.261033]\n",
      "epoch:21 step:20146 [D loss: 0.583609, acc.: 71.09%] [G loss: 1.466139]\n",
      "epoch:21 step:20147 [D loss: 0.633470, acc.: 64.84%] [G loss: 1.493517]\n",
      "epoch:21 step:20148 [D loss: 0.472182, acc.: 78.12%] [G loss: 1.204525]\n",
      "epoch:21 step:20149 [D loss: 0.510961, acc.: 78.12%] [G loss: 1.863147]\n",
      "epoch:21 step:20150 [D loss: 0.422080, acc.: 84.38%] [G loss: 1.661011]\n",
      "epoch:21 step:20151 [D loss: 0.583068, acc.: 73.44%] [G loss: 1.101720]\n",
      "epoch:21 step:20152 [D loss: 0.515958, acc.: 72.66%] [G loss: 1.689809]\n",
      "epoch:21 step:20153 [D loss: 0.451409, acc.: 79.69%] [G loss: 1.639271]\n",
      "epoch:21 step:20154 [D loss: 0.561494, acc.: 71.09%] [G loss: 1.207416]\n",
      "epoch:21 step:20155 [D loss: 0.617800, acc.: 60.16%] [G loss: 1.545494]\n",
      "epoch:21 step:20156 [D loss: 0.480274, acc.: 75.00%] [G loss: 1.556494]\n",
      "epoch:21 step:20157 [D loss: 0.545994, acc.: 70.31%] [G loss: 1.338737]\n",
      "epoch:21 step:20158 [D loss: 0.684672, acc.: 61.72%] [G loss: 1.084748]\n",
      "epoch:21 step:20159 [D loss: 0.567649, acc.: 69.53%] [G loss: 1.376935]\n",
      "epoch:21 step:20160 [D loss: 0.542713, acc.: 75.78%] [G loss: 1.415649]\n",
      "epoch:21 step:20161 [D loss: 0.568774, acc.: 70.31%] [G loss: 1.115326]\n",
      "epoch:21 step:20162 [D loss: 0.572755, acc.: 71.09%] [G loss: 1.006107]\n",
      "epoch:21 step:20163 [D loss: 0.573009, acc.: 71.88%] [G loss: 1.409281]\n",
      "epoch:21 step:20164 [D loss: 0.545984, acc.: 75.00%] [G loss: 1.540531]\n",
      "epoch:21 step:20165 [D loss: 0.556261, acc.: 76.56%] [G loss: 1.395568]\n",
      "epoch:21 step:20166 [D loss: 0.497320, acc.: 73.44%] [G loss: 1.107946]\n",
      "epoch:21 step:20167 [D loss: 0.483346, acc.: 76.56%] [G loss: 1.177541]\n",
      "epoch:21 step:20168 [D loss: 0.556811, acc.: 74.22%] [G loss: 0.834723]\n",
      "epoch:21 step:20169 [D loss: 0.508352, acc.: 75.78%] [G loss: 1.502382]\n",
      "epoch:21 step:20170 [D loss: 0.489752, acc.: 75.78%] [G loss: 1.449004]\n",
      "epoch:21 step:20171 [D loss: 0.641232, acc.: 61.72%] [G loss: 1.368590]\n",
      "epoch:21 step:20172 [D loss: 0.576945, acc.: 71.88%] [G loss: 1.166821]\n",
      "epoch:21 step:20173 [D loss: 0.619122, acc.: 63.28%] [G loss: 1.298627]\n",
      "epoch:21 step:20174 [D loss: 0.518012, acc.: 74.22%] [G loss: 0.982973]\n",
      "epoch:21 step:20175 [D loss: 0.511095, acc.: 73.44%] [G loss: 1.604741]\n",
      "epoch:21 step:20176 [D loss: 0.540565, acc.: 76.56%] [G loss: 1.625186]\n",
      "epoch:21 step:20177 [D loss: 0.560240, acc.: 72.66%] [G loss: 1.658458]\n",
      "epoch:21 step:20178 [D loss: 0.506227, acc.: 76.56%] [G loss: 1.267103]\n",
      "epoch:21 step:20179 [D loss: 0.618916, acc.: 70.31%] [G loss: 1.295415]\n",
      "epoch:21 step:20180 [D loss: 0.596300, acc.: 72.66%] [G loss: 1.146818]\n",
      "epoch:21 step:20181 [D loss: 0.534054, acc.: 75.78%] [G loss: 0.954010]\n",
      "epoch:21 step:20182 [D loss: 0.466422, acc.: 79.69%] [G loss: 1.094304]\n",
      "epoch:21 step:20183 [D loss: 0.465755, acc.: 76.56%] [G loss: 1.423724]\n",
      "epoch:21 step:20184 [D loss: 0.584758, acc.: 71.09%] [G loss: 1.529854]\n",
      "epoch:21 step:20185 [D loss: 0.461304, acc.: 82.81%] [G loss: 1.182163]\n",
      "epoch:21 step:20186 [D loss: 0.495987, acc.: 75.78%] [G loss: 1.779606]\n",
      "epoch:21 step:20187 [D loss: 0.698812, acc.: 60.16%] [G loss: 1.205113]\n",
      "epoch:21 step:20188 [D loss: 0.479921, acc.: 82.03%] [G loss: 1.526181]\n",
      "epoch:21 step:20189 [D loss: 0.666600, acc.: 57.81%] [G loss: 1.271386]\n",
      "epoch:21 step:20190 [D loss: 0.495087, acc.: 78.91%] [G loss: 1.288937]\n",
      "epoch:21 step:20191 [D loss: 0.575180, acc.: 68.75%] [G loss: 1.470767]\n",
      "epoch:21 step:20192 [D loss: 0.525158, acc.: 72.66%] [G loss: 1.351517]\n",
      "epoch:21 step:20193 [D loss: 0.565546, acc.: 74.22%] [G loss: 1.824250]\n",
      "epoch:21 step:20194 [D loss: 0.560393, acc.: 75.00%] [G loss: 1.483133]\n",
      "epoch:21 step:20195 [D loss: 0.516879, acc.: 71.88%] [G loss: 1.149649]\n",
      "epoch:21 step:20196 [D loss: 0.506211, acc.: 76.56%] [G loss: 1.113044]\n",
      "epoch:21 step:20197 [D loss: 0.530407, acc.: 71.88%] [G loss: 1.367234]\n",
      "epoch:21 step:20198 [D loss: 0.493229, acc.: 78.91%] [G loss: 1.156877]\n",
      "epoch:21 step:20199 [D loss: 0.610391, acc.: 68.75%] [G loss: 1.229145]\n",
      "epoch:21 step:20200 [D loss: 0.551393, acc.: 68.75%] [G loss: 1.174216]\n",
      "##############\n",
      "[2.66879602 1.90416512 1.75792664 2.71998825 0.70856348 5.1878119\n",
      " 2.10252328 2.31468147 3.85382886 7.14868929]\n",
      "##########\n",
      "epoch:21 step:20201 [D loss: 0.597407, acc.: 69.53%] [G loss: 1.152952]\n",
      "epoch:21 step:20202 [D loss: 0.594983, acc.: 67.97%] [G loss: 1.346467]\n",
      "epoch:21 step:20203 [D loss: 0.623412, acc.: 66.41%] [G loss: 1.168637]\n",
      "epoch:21 step:20204 [D loss: 0.835514, acc.: 48.44%] [G loss: 1.423426]\n",
      "epoch:21 step:20205 [D loss: 0.540572, acc.: 75.00%] [G loss: 1.544568]\n",
      "epoch:21 step:20206 [D loss: 0.357199, acc.: 91.41%] [G loss: 1.246953]\n",
      "epoch:21 step:20207 [D loss: 0.632628, acc.: 67.97%] [G loss: 1.143614]\n",
      "epoch:21 step:20208 [D loss: 0.533858, acc.: 71.09%] [G loss: 1.372775]\n",
      "epoch:21 step:20209 [D loss: 0.757262, acc.: 51.56%] [G loss: 1.102250]\n",
      "epoch:21 step:20210 [D loss: 0.614222, acc.: 63.28%] [G loss: 1.187942]\n",
      "epoch:21 step:20211 [D loss: 0.639621, acc.: 66.41%] [G loss: 1.389814]\n",
      "epoch:21 step:20212 [D loss: 0.515328, acc.: 77.34%] [G loss: 1.394098]\n",
      "epoch:21 step:20213 [D loss: 0.820403, acc.: 45.31%] [G loss: 1.263099]\n",
      "epoch:21 step:20214 [D loss: 0.622257, acc.: 64.84%] [G loss: 1.350894]\n",
      "epoch:21 step:20215 [D loss: 0.550584, acc.: 74.22%] [G loss: 1.227854]\n",
      "epoch:21 step:20216 [D loss: 0.580032, acc.: 64.84%] [G loss: 0.953998]\n",
      "epoch:21 step:20217 [D loss: 0.433780, acc.: 83.59%] [G loss: 1.322898]\n",
      "epoch:21 step:20218 [D loss: 0.405341, acc.: 83.59%] [G loss: 1.475539]\n",
      "epoch:21 step:20219 [D loss: 0.612707, acc.: 71.88%] [G loss: 1.100699]\n",
      "epoch:21 step:20220 [D loss: 0.649489, acc.: 60.16%] [G loss: 1.374677]\n",
      "epoch:21 step:20221 [D loss: 0.521910, acc.: 73.44%] [G loss: 1.690945]\n",
      "epoch:21 step:20222 [D loss: 0.611250, acc.: 69.53%] [G loss: 1.412523]\n",
      "epoch:21 step:20223 [D loss: 0.484812, acc.: 78.91%] [G loss: 1.501107]\n",
      "epoch:21 step:20224 [D loss: 0.623977, acc.: 67.97%] [G loss: 1.189958]\n",
      "epoch:21 step:20225 [D loss: 0.717560, acc.: 60.16%] [G loss: 1.014423]\n",
      "epoch:21 step:20226 [D loss: 0.498208, acc.: 75.00%] [G loss: 1.322803]\n",
      "epoch:21 step:20227 [D loss: 0.407177, acc.: 83.59%] [G loss: 1.657458]\n",
      "epoch:21 step:20228 [D loss: 0.478420, acc.: 75.78%] [G loss: 1.793794]\n",
      "epoch:21 step:20229 [D loss: 0.706777, acc.: 60.16%] [G loss: 1.071379]\n",
      "epoch:21 step:20230 [D loss: 0.536242, acc.: 69.53%] [G loss: 1.542208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20231 [D loss: 0.550756, acc.: 75.78%] [G loss: 1.089004]\n",
      "epoch:21 step:20232 [D loss: 0.518635, acc.: 78.91%] [G loss: 1.003735]\n",
      "epoch:21 step:20233 [D loss: 0.573528, acc.: 68.75%] [G loss: 1.276739]\n",
      "epoch:21 step:20234 [D loss: 0.490001, acc.: 75.78%] [G loss: 1.471582]\n",
      "epoch:21 step:20235 [D loss: 0.494024, acc.: 78.91%] [G loss: 1.563039]\n",
      "epoch:21 step:20236 [D loss: 0.479331, acc.: 78.91%] [G loss: 1.213988]\n",
      "epoch:21 step:20237 [D loss: 0.696231, acc.: 57.03%] [G loss: 1.119495]\n",
      "epoch:21 step:20238 [D loss: 0.465671, acc.: 76.56%] [G loss: 1.567244]\n",
      "epoch:21 step:20239 [D loss: 0.727392, acc.: 58.59%] [G loss: 1.400823]\n",
      "epoch:21 step:20240 [D loss: 0.580775, acc.: 67.19%] [G loss: 1.317378]\n",
      "epoch:21 step:20241 [D loss: 0.636225, acc.: 67.19%] [G loss: 1.287604]\n",
      "epoch:21 step:20242 [D loss: 0.448226, acc.: 83.59%] [G loss: 1.322852]\n",
      "epoch:21 step:20243 [D loss: 0.453298, acc.: 82.81%] [G loss: 1.528049]\n",
      "epoch:21 step:20244 [D loss: 0.504777, acc.: 78.91%] [G loss: 1.405853]\n",
      "epoch:21 step:20245 [D loss: 0.558079, acc.: 72.66%] [G loss: 1.498267]\n",
      "epoch:21 step:20246 [D loss: 0.556131, acc.: 73.44%] [G loss: 1.295137]\n",
      "epoch:21 step:20247 [D loss: 0.736992, acc.: 48.44%] [G loss: 1.295363]\n",
      "epoch:21 step:20248 [D loss: 0.470982, acc.: 75.00%] [G loss: 1.511117]\n",
      "epoch:21 step:20249 [D loss: 0.460860, acc.: 78.91%] [G loss: 1.197669]\n",
      "epoch:21 step:20250 [D loss: 0.684147, acc.: 59.38%] [G loss: 1.398062]\n",
      "epoch:21 step:20251 [D loss: 0.631081, acc.: 64.84%] [G loss: 1.147982]\n",
      "epoch:21 step:20252 [D loss: 0.488383, acc.: 75.00%] [G loss: 1.329977]\n",
      "epoch:21 step:20253 [D loss: 0.597452, acc.: 65.62%] [G loss: 1.265922]\n",
      "epoch:21 step:20254 [D loss: 0.779349, acc.: 53.12%] [G loss: 1.154677]\n",
      "epoch:21 step:20255 [D loss: 0.619391, acc.: 64.84%] [G loss: 1.071314]\n",
      "epoch:21 step:20256 [D loss: 0.669403, acc.: 62.50%] [G loss: 1.384207]\n",
      "epoch:21 step:20257 [D loss: 0.623388, acc.: 63.28%] [G loss: 1.218429]\n",
      "epoch:21 step:20258 [D loss: 0.594442, acc.: 65.62%] [G loss: 1.339733]\n",
      "epoch:21 step:20259 [D loss: 0.430728, acc.: 78.12%] [G loss: 1.765223]\n",
      "epoch:21 step:20260 [D loss: 0.519378, acc.: 73.44%] [G loss: 1.281444]\n",
      "epoch:21 step:20261 [D loss: 0.501326, acc.: 73.44%] [G loss: 1.248561]\n",
      "epoch:21 step:20262 [D loss: 0.541515, acc.: 71.88%] [G loss: 1.771300]\n",
      "epoch:21 step:20263 [D loss: 0.669653, acc.: 64.06%] [G loss: 1.061818]\n",
      "epoch:21 step:20264 [D loss: 0.529997, acc.: 75.78%] [G loss: 1.705950]\n",
      "epoch:21 step:20265 [D loss: 0.578560, acc.: 68.75%] [G loss: 1.363062]\n",
      "epoch:21 step:20266 [D loss: 0.514355, acc.: 74.22%] [G loss: 1.564195]\n",
      "epoch:21 step:20267 [D loss: 0.655956, acc.: 60.94%] [G loss: 0.996503]\n",
      "epoch:21 step:20268 [D loss: 0.542377, acc.: 74.22%] [G loss: 1.296367]\n",
      "epoch:21 step:20269 [D loss: 0.593972, acc.: 67.19%] [G loss: 1.486566]\n",
      "epoch:21 step:20270 [D loss: 0.715436, acc.: 62.50%] [G loss: 1.375435]\n",
      "epoch:21 step:20271 [D loss: 0.569009, acc.: 70.31%] [G loss: 1.344875]\n",
      "epoch:21 step:20272 [D loss: 0.635957, acc.: 63.28%] [G loss: 1.394267]\n",
      "epoch:21 step:20273 [D loss: 0.520620, acc.: 78.91%] [G loss: 1.200480]\n",
      "epoch:21 step:20274 [D loss: 0.659016, acc.: 60.94%] [G loss: 1.323802]\n",
      "epoch:21 step:20275 [D loss: 0.614584, acc.: 61.72%] [G loss: 1.574728]\n",
      "epoch:21 step:20276 [D loss: 0.442765, acc.: 81.25%] [G loss: 1.693274]\n",
      "epoch:21 step:20277 [D loss: 0.513933, acc.: 79.69%] [G loss: 1.429666]\n",
      "epoch:21 step:20278 [D loss: 0.647977, acc.: 66.41%] [G loss: 1.319011]\n",
      "epoch:21 step:20279 [D loss: 0.474546, acc.: 78.12%] [G loss: 1.686386]\n",
      "epoch:21 step:20280 [D loss: 0.521392, acc.: 75.00%] [G loss: 1.661281]\n",
      "epoch:21 step:20281 [D loss: 0.584090, acc.: 70.31%] [G loss: 1.151871]\n",
      "epoch:21 step:20282 [D loss: 0.493136, acc.: 79.69%] [G loss: 1.806542]\n",
      "epoch:21 step:20283 [D loss: 0.618397, acc.: 67.19%] [G loss: 1.296595]\n",
      "epoch:21 step:20284 [D loss: 0.593025, acc.: 67.97%] [G loss: 1.358893]\n",
      "epoch:21 step:20285 [D loss: 0.732716, acc.: 53.91%] [G loss: 1.207572]\n",
      "epoch:21 step:20286 [D loss: 0.492949, acc.: 76.56%] [G loss: 1.601902]\n",
      "epoch:21 step:20287 [D loss: 0.518467, acc.: 77.34%] [G loss: 1.421902]\n",
      "epoch:21 step:20288 [D loss: 0.457330, acc.: 79.69%] [G loss: 1.382988]\n",
      "epoch:21 step:20289 [D loss: 0.593478, acc.: 67.97%] [G loss: 1.228574]\n",
      "epoch:21 step:20290 [D loss: 0.598502, acc.: 66.41%] [G loss: 1.532742]\n",
      "epoch:21 step:20291 [D loss: 0.524391, acc.: 76.56%] [G loss: 1.115389]\n",
      "epoch:21 step:20292 [D loss: 0.500330, acc.: 82.81%] [G loss: 1.672125]\n",
      "epoch:21 step:20293 [D loss: 0.634230, acc.: 60.94%] [G loss: 1.440342]\n",
      "epoch:21 step:20294 [D loss: 0.524536, acc.: 74.22%] [G loss: 1.209428]\n",
      "epoch:21 step:20295 [D loss: 0.637726, acc.: 60.16%] [G loss: 1.271072]\n",
      "epoch:21 step:20296 [D loss: 0.698670, acc.: 55.47%] [G loss: 1.184037]\n",
      "epoch:21 step:20297 [D loss: 0.617979, acc.: 61.72%] [G loss: 1.355361]\n",
      "epoch:21 step:20298 [D loss: 0.712190, acc.: 57.81%] [G loss: 0.974959]\n",
      "epoch:21 step:20299 [D loss: 0.502093, acc.: 78.12%] [G loss: 1.309506]\n",
      "epoch:21 step:20300 [D loss: 0.509346, acc.: 77.34%] [G loss: 1.857041]\n",
      "epoch:21 step:20301 [D loss: 0.549721, acc.: 71.88%] [G loss: 1.525941]\n",
      "epoch:21 step:20302 [D loss: 0.477337, acc.: 78.91%] [G loss: 1.279338]\n",
      "epoch:21 step:20303 [D loss: 0.636629, acc.: 64.84%] [G loss: 1.227659]\n",
      "epoch:21 step:20304 [D loss: 0.566201, acc.: 75.00%] [G loss: 1.689932]\n",
      "epoch:21 step:20305 [D loss: 0.571794, acc.: 69.53%] [G loss: 1.260554]\n",
      "epoch:21 step:20306 [D loss: 0.530088, acc.: 77.34%] [G loss: 1.046313]\n",
      "epoch:21 step:20307 [D loss: 0.605272, acc.: 65.62%] [G loss: 1.520866]\n",
      "epoch:21 step:20308 [D loss: 0.425031, acc.: 84.38%] [G loss: 1.454494]\n",
      "epoch:21 step:20309 [D loss: 0.690809, acc.: 60.94%] [G loss: 1.008898]\n",
      "epoch:21 step:20310 [D loss: 0.365358, acc.: 85.94%] [G loss: 1.607931]\n",
      "epoch:21 step:20311 [D loss: 0.598893, acc.: 69.53%] [G loss: 1.230803]\n",
      "epoch:21 step:20312 [D loss: 0.572838, acc.: 73.44%] [G loss: 1.239300]\n",
      "epoch:21 step:20313 [D loss: 0.514766, acc.: 77.34%] [G loss: 1.305695]\n",
      "epoch:21 step:20314 [D loss: 0.667724, acc.: 64.84%] [G loss: 1.221139]\n",
      "epoch:21 step:20315 [D loss: 0.587902, acc.: 68.75%] [G loss: 1.255115]\n",
      "epoch:21 step:20316 [D loss: 0.458756, acc.: 78.12%] [G loss: 1.516305]\n",
      "epoch:21 step:20317 [D loss: 0.560106, acc.: 64.84%] [G loss: 1.480296]\n",
      "epoch:21 step:20318 [D loss: 0.513480, acc.: 71.09%] [G loss: 1.416479]\n",
      "epoch:21 step:20319 [D loss: 0.486643, acc.: 77.34%] [G loss: 1.536138]\n",
      "epoch:21 step:20320 [D loss: 0.737320, acc.: 55.47%] [G loss: 1.197655]\n",
      "epoch:21 step:20321 [D loss: 0.444077, acc.: 77.34%] [G loss: 1.467096]\n",
      "epoch:21 step:20322 [D loss: 0.528152, acc.: 75.00%] [G loss: 1.530739]\n",
      "epoch:21 step:20323 [D loss: 0.530203, acc.: 75.78%] [G loss: 1.425997]\n",
      "epoch:21 step:20324 [D loss: 0.647666, acc.: 63.28%] [G loss: 1.178049]\n",
      "epoch:21 step:20325 [D loss: 0.440515, acc.: 84.38%] [G loss: 1.253679]\n",
      "epoch:21 step:20326 [D loss: 0.530681, acc.: 73.44%] [G loss: 1.228341]\n",
      "epoch:21 step:20327 [D loss: 0.579008, acc.: 67.97%] [G loss: 1.365191]\n",
      "epoch:21 step:20328 [D loss: 0.623300, acc.: 68.75%] [G loss: 1.360235]\n",
      "epoch:21 step:20329 [D loss: 0.633412, acc.: 64.84%] [G loss: 1.119252]\n",
      "epoch:21 step:20330 [D loss: 0.444332, acc.: 80.47%] [G loss: 1.176808]\n",
      "epoch:21 step:20331 [D loss: 0.408512, acc.: 85.94%] [G loss: 1.397179]\n",
      "epoch:21 step:20332 [D loss: 0.495358, acc.: 78.91%] [G loss: 1.570943]\n",
      "epoch:21 step:20333 [D loss: 0.403920, acc.: 80.47%] [G loss: 1.389089]\n",
      "epoch:21 step:20334 [D loss: 0.799767, acc.: 49.22%] [G loss: 1.091653]\n",
      "epoch:21 step:20335 [D loss: 0.581704, acc.: 67.97%] [G loss: 1.451643]\n",
      "epoch:21 step:20336 [D loss: 0.558123, acc.: 71.88%] [G loss: 1.366527]\n",
      "epoch:21 step:20337 [D loss: 0.470449, acc.: 78.12%] [G loss: 1.341126]\n",
      "epoch:21 step:20338 [D loss: 0.546761, acc.: 77.34%] [G loss: 1.209818]\n",
      "epoch:21 step:20339 [D loss: 0.806851, acc.: 59.38%] [G loss: 1.209563]\n",
      "epoch:21 step:20340 [D loss: 0.700628, acc.: 58.59%] [G loss: 1.216885]\n",
      "epoch:21 step:20341 [D loss: 0.548424, acc.: 73.44%] [G loss: 1.097901]\n",
      "epoch:21 step:20342 [D loss: 0.582197, acc.: 69.53%] [G loss: 1.343028]\n",
      "epoch:21 step:20343 [D loss: 0.429134, acc.: 79.69%] [G loss: 1.123124]\n",
      "epoch:21 step:20344 [D loss: 0.455098, acc.: 79.69%] [G loss: 1.588459]\n",
      "epoch:21 step:20345 [D loss: 0.674333, acc.: 60.16%] [G loss: 0.817349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20346 [D loss: 0.427036, acc.: 83.59%] [G loss: 1.331016]\n",
      "epoch:21 step:20347 [D loss: 0.566953, acc.: 70.31%] [G loss: 1.107338]\n",
      "epoch:21 step:20348 [D loss: 0.418168, acc.: 82.03%] [G loss: 1.384609]\n",
      "epoch:21 step:20349 [D loss: 0.503545, acc.: 75.00%] [G loss: 1.429857]\n",
      "epoch:21 step:20350 [D loss: 0.562139, acc.: 68.75%] [G loss: 1.051442]\n",
      "epoch:21 step:20351 [D loss: 0.592068, acc.: 70.31%] [G loss: 1.124692]\n",
      "epoch:21 step:20352 [D loss: 0.706687, acc.: 53.91%] [G loss: 1.325489]\n",
      "epoch:21 step:20353 [D loss: 0.434134, acc.: 83.59%] [G loss: 1.405848]\n",
      "epoch:21 step:20354 [D loss: 0.627406, acc.: 64.06%] [G loss: 1.576522]\n",
      "epoch:21 step:20355 [D loss: 0.671318, acc.: 63.28%] [G loss: 1.384259]\n",
      "epoch:21 step:20356 [D loss: 0.584517, acc.: 66.41%] [G loss: 1.300787]\n",
      "epoch:21 step:20357 [D loss: 0.737775, acc.: 56.25%] [G loss: 0.842383]\n",
      "epoch:21 step:20358 [D loss: 0.517071, acc.: 77.34%] [G loss: 1.334823]\n",
      "epoch:21 step:20359 [D loss: 0.693972, acc.: 62.50%] [G loss: 1.047061]\n",
      "epoch:21 step:20360 [D loss: 0.614955, acc.: 71.88%] [G loss: 1.136159]\n",
      "epoch:21 step:20361 [D loss: 0.458240, acc.: 79.69%] [G loss: 1.151741]\n",
      "epoch:21 step:20362 [D loss: 0.663302, acc.: 60.16%] [G loss: 0.893552]\n",
      "epoch:21 step:20363 [D loss: 0.563723, acc.: 70.31%] [G loss: 1.344006]\n",
      "epoch:21 step:20364 [D loss: 0.613189, acc.: 70.31%] [G loss: 0.867646]\n",
      "epoch:21 step:20365 [D loss: 0.639940, acc.: 65.62%] [G loss: 1.503118]\n",
      "epoch:21 step:20366 [D loss: 0.638125, acc.: 63.28%] [G loss: 1.528297]\n",
      "epoch:21 step:20367 [D loss: 0.460082, acc.: 76.56%] [G loss: 1.278137]\n",
      "epoch:21 step:20368 [D loss: 0.553571, acc.: 71.88%] [G loss: 1.315840]\n",
      "epoch:21 step:20369 [D loss: 0.379991, acc.: 87.50%] [G loss: 1.184015]\n",
      "epoch:21 step:20370 [D loss: 0.482391, acc.: 82.81%] [G loss: 1.275161]\n",
      "epoch:21 step:20371 [D loss: 0.673954, acc.: 62.50%] [G loss: 1.019864]\n",
      "epoch:21 step:20372 [D loss: 0.480037, acc.: 75.00%] [G loss: 1.440468]\n",
      "epoch:21 step:20373 [D loss: 0.447794, acc.: 77.34%] [G loss: 1.397788]\n",
      "epoch:21 step:20374 [D loss: 0.580324, acc.: 67.97%] [G loss: 1.641650]\n",
      "epoch:21 step:20375 [D loss: 0.551176, acc.: 73.44%] [G loss: 1.418729]\n",
      "epoch:21 step:20376 [D loss: 0.720203, acc.: 57.81%] [G loss: 1.290509]\n",
      "epoch:21 step:20377 [D loss: 0.589217, acc.: 67.19%] [G loss: 1.320493]\n",
      "epoch:21 step:20378 [D loss: 0.348412, acc.: 86.72%] [G loss: 1.672033]\n",
      "epoch:21 step:20379 [D loss: 0.605747, acc.: 71.09%] [G loss: 1.332356]\n",
      "epoch:21 step:20380 [D loss: 0.688917, acc.: 60.94%] [G loss: 1.140293]\n",
      "epoch:21 step:20381 [D loss: 0.662584, acc.: 60.94%] [G loss: 1.048281]\n",
      "epoch:21 step:20382 [D loss: 0.402470, acc.: 82.81%] [G loss: 1.228771]\n",
      "epoch:21 step:20383 [D loss: 0.583845, acc.: 74.22%] [G loss: 1.245041]\n",
      "epoch:21 step:20384 [D loss: 0.461844, acc.: 78.12%] [G loss: 1.490840]\n",
      "epoch:21 step:20385 [D loss: 0.535820, acc.: 73.44%] [G loss: 1.572527]\n",
      "epoch:21 step:20386 [D loss: 0.553557, acc.: 71.09%] [G loss: 1.174749]\n",
      "epoch:21 step:20387 [D loss: 0.534001, acc.: 72.66%] [G loss: 1.322630]\n",
      "epoch:21 step:20388 [D loss: 0.445907, acc.: 80.47%] [G loss: 1.473636]\n",
      "epoch:21 step:20389 [D loss: 0.579717, acc.: 72.66%] [G loss: 1.432801]\n",
      "epoch:21 step:20390 [D loss: 0.666806, acc.: 60.94%] [G loss: 1.309322]\n",
      "epoch:21 step:20391 [D loss: 0.687509, acc.: 57.81%] [G loss: 1.867312]\n",
      "epoch:21 step:20392 [D loss: 0.582082, acc.: 67.97%] [G loss: 1.566483]\n",
      "epoch:21 step:20393 [D loss: 0.476790, acc.: 80.47%] [G loss: 1.476511]\n",
      "epoch:21 step:20394 [D loss: 0.465087, acc.: 79.69%] [G loss: 1.590746]\n",
      "epoch:21 step:20395 [D loss: 0.474771, acc.: 75.00%] [G loss: 1.358067]\n",
      "epoch:21 step:20396 [D loss: 0.533629, acc.: 69.53%] [G loss: 1.361523]\n",
      "epoch:21 step:20397 [D loss: 0.477873, acc.: 80.47%] [G loss: 1.336374]\n",
      "epoch:21 step:20398 [D loss: 0.636563, acc.: 63.28%] [G loss: 1.438127]\n",
      "epoch:21 step:20399 [D loss: 0.508634, acc.: 73.44%] [G loss: 0.937309]\n",
      "epoch:21 step:20400 [D loss: 0.544000, acc.: 75.00%] [G loss: 1.589652]\n",
      "##############\n",
      "[2.53877038 1.96017127 1.66213054 2.63101128 0.87387036 5.67422887\n",
      " 2.33959153 2.62367738 3.87134007 7.14799875]\n",
      "##########\n",
      "epoch:21 step:20401 [D loss: 0.517087, acc.: 78.91%] [G loss: 1.100877]\n",
      "epoch:21 step:20402 [D loss: 0.600883, acc.: 66.41%] [G loss: 1.610592]\n",
      "epoch:21 step:20403 [D loss: 0.668026, acc.: 61.72%] [G loss: 1.328795]\n",
      "epoch:21 step:20404 [D loss: 0.462074, acc.: 82.03%] [G loss: 1.569338]\n",
      "epoch:21 step:20405 [D loss: 0.555511, acc.: 67.19%] [G loss: 1.368091]\n",
      "epoch:21 step:20406 [D loss: 0.550792, acc.: 71.88%] [G loss: 0.982693]\n",
      "epoch:21 step:20407 [D loss: 0.650667, acc.: 63.28%] [G loss: 1.204442]\n",
      "epoch:21 step:20408 [D loss: 0.652833, acc.: 61.72%] [G loss: 1.399553]\n",
      "epoch:21 step:20409 [D loss: 0.513185, acc.: 76.56%] [G loss: 0.920473]\n",
      "epoch:21 step:20410 [D loss: 0.405999, acc.: 82.81%] [G loss: 1.518516]\n",
      "epoch:21 step:20411 [D loss: 0.633540, acc.: 67.19%] [G loss: 1.188909]\n",
      "epoch:21 step:20412 [D loss: 0.656089, acc.: 58.59%] [G loss: 1.198766]\n",
      "epoch:21 step:20413 [D loss: 0.583512, acc.: 71.09%] [G loss: 1.400181]\n",
      "epoch:21 step:20414 [D loss: 0.567447, acc.: 72.66%] [G loss: 1.208757]\n",
      "epoch:21 step:20415 [D loss: 0.677488, acc.: 56.25%] [G loss: 1.379618]\n",
      "epoch:21 step:20416 [D loss: 0.612483, acc.: 70.31%] [G loss: 1.529402]\n",
      "epoch:21 step:20417 [D loss: 0.505170, acc.: 79.69%] [G loss: 1.488992]\n",
      "epoch:21 step:20418 [D loss: 0.540689, acc.: 71.88%] [G loss: 1.617603]\n",
      "epoch:21 step:20419 [D loss: 0.544858, acc.: 71.88%] [G loss: 1.369761]\n",
      "epoch:21 step:20420 [D loss: 0.801029, acc.: 51.56%] [G loss: 1.185248]\n",
      "epoch:21 step:20421 [D loss: 0.516261, acc.: 75.00%] [G loss: 1.291924]\n",
      "epoch:21 step:20422 [D loss: 0.463256, acc.: 79.69%] [G loss: 1.242624]\n",
      "epoch:21 step:20423 [D loss: 0.527954, acc.: 71.88%] [G loss: 1.324113]\n",
      "epoch:21 step:20424 [D loss: 0.518675, acc.: 77.34%] [G loss: 1.406606]\n",
      "epoch:21 step:20425 [D loss: 0.679537, acc.: 60.94%] [G loss: 1.164399]\n",
      "epoch:21 step:20426 [D loss: 0.531504, acc.: 72.66%] [G loss: 1.162956]\n",
      "epoch:21 step:20427 [D loss: 0.690865, acc.: 52.34%] [G loss: 1.122298]\n",
      "epoch:21 step:20428 [D loss: 0.480963, acc.: 77.34%] [G loss: 1.626535]\n",
      "epoch:21 step:20429 [D loss: 0.683697, acc.: 59.38%] [G loss: 1.044192]\n",
      "epoch:21 step:20430 [D loss: 0.429696, acc.: 83.59%] [G loss: 1.315947]\n",
      "epoch:21 step:20431 [D loss: 0.517627, acc.: 71.09%] [G loss: 1.154668]\n",
      "epoch:21 step:20432 [D loss: 0.562401, acc.: 70.31%] [G loss: 1.508543]\n",
      "epoch:21 step:20433 [D loss: 0.548803, acc.: 69.53%] [G loss: 1.619586]\n",
      "epoch:21 step:20434 [D loss: 0.473701, acc.: 74.22%] [G loss: 1.461754]\n",
      "epoch:21 step:20435 [D loss: 0.911471, acc.: 45.31%] [G loss: 1.244942]\n",
      "epoch:21 step:20436 [D loss: 0.442988, acc.: 84.38%] [G loss: 1.423838]\n",
      "epoch:21 step:20437 [D loss: 0.516463, acc.: 75.00%] [G loss: 1.533242]\n",
      "epoch:21 step:20438 [D loss: 0.687321, acc.: 58.59%] [G loss: 1.705927]\n",
      "epoch:21 step:20439 [D loss: 0.635858, acc.: 66.41%] [G loss: 1.580583]\n",
      "epoch:21 step:20440 [D loss: 0.638709, acc.: 61.72%] [G loss: 1.166148]\n",
      "epoch:21 step:20441 [D loss: 0.514184, acc.: 71.88%] [G loss: 1.476603]\n",
      "epoch:21 step:20442 [D loss: 0.614779, acc.: 71.88%] [G loss: 1.090244]\n",
      "epoch:21 step:20443 [D loss: 0.422220, acc.: 82.81%] [G loss: 1.601373]\n",
      "epoch:21 step:20444 [D loss: 0.378670, acc.: 87.50%] [G loss: 1.367702]\n",
      "epoch:21 step:20445 [D loss: 0.736402, acc.: 53.91%] [G loss: 1.244167]\n",
      "epoch:21 step:20446 [D loss: 0.516257, acc.: 70.31%] [G loss: 1.382708]\n",
      "epoch:21 step:20447 [D loss: 0.605842, acc.: 67.19%] [G loss: 1.275330]\n",
      "epoch:21 step:20448 [D loss: 0.552301, acc.: 73.44%] [G loss: 1.588056]\n",
      "epoch:21 step:20449 [D loss: 0.623972, acc.: 63.28%] [G loss: 1.128979]\n",
      "epoch:21 step:20450 [D loss: 0.584943, acc.: 68.75%] [G loss: 1.232474]\n",
      "epoch:21 step:20451 [D loss: 0.504476, acc.: 78.12%] [G loss: 1.411027]\n",
      "epoch:21 step:20452 [D loss: 0.641694, acc.: 64.06%] [G loss: 1.378613]\n",
      "epoch:21 step:20453 [D loss: 0.533246, acc.: 71.88%] [G loss: 1.146972]\n",
      "epoch:21 step:20454 [D loss: 0.595258, acc.: 71.88%] [G loss: 1.413470]\n",
      "epoch:21 step:20455 [D loss: 0.495553, acc.: 76.56%] [G loss: 1.101636]\n",
      "epoch:21 step:20456 [D loss: 0.567667, acc.: 69.53%] [G loss: 1.279409]\n",
      "epoch:21 step:20457 [D loss: 0.699704, acc.: 58.59%] [G loss: 1.079753]\n",
      "epoch:21 step:20458 [D loss: 0.476418, acc.: 78.91%] [G loss: 1.164231]\n",
      "epoch:21 step:20459 [D loss: 0.568376, acc.: 70.31%] [G loss: 1.197361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20460 [D loss: 0.597085, acc.: 67.97%] [G loss: 1.299658]\n",
      "epoch:21 step:20461 [D loss: 0.510586, acc.: 75.78%] [G loss: 1.485300]\n",
      "epoch:21 step:20462 [D loss: 0.563547, acc.: 70.31%] [G loss: 0.990801]\n",
      "epoch:21 step:20463 [D loss: 0.666582, acc.: 64.84%] [G loss: 1.332230]\n",
      "epoch:21 step:20464 [D loss: 0.522238, acc.: 73.44%] [G loss: 1.440262]\n",
      "epoch:21 step:20465 [D loss: 0.527709, acc.: 74.22%] [G loss: 1.217129]\n",
      "epoch:21 step:20466 [D loss: 0.542335, acc.: 73.44%] [G loss: 1.667603]\n",
      "epoch:21 step:20467 [D loss: 0.498761, acc.: 75.78%] [G loss: 1.472974]\n",
      "epoch:21 step:20468 [D loss: 0.556047, acc.: 67.19%] [G loss: 1.363781]\n",
      "epoch:21 step:20469 [D loss: 0.528646, acc.: 78.12%] [G loss: 1.566803]\n",
      "epoch:21 step:20470 [D loss: 0.568393, acc.: 67.19%] [G loss: 1.537832]\n",
      "epoch:21 step:20471 [D loss: 0.532265, acc.: 74.22%] [G loss: 1.304471]\n",
      "epoch:21 step:20472 [D loss: 0.548697, acc.: 68.75%] [G loss: 1.131815]\n",
      "epoch:21 step:20473 [D loss: 0.512491, acc.: 74.22%] [G loss: 1.663995]\n",
      "epoch:21 step:20474 [D loss: 0.574329, acc.: 71.88%] [G loss: 1.412921]\n",
      "epoch:21 step:20475 [D loss: 0.642324, acc.: 65.62%] [G loss: 1.614509]\n",
      "epoch:21 step:20476 [D loss: 0.509707, acc.: 73.44%] [G loss: 1.696914]\n",
      "epoch:21 step:20477 [D loss: 0.616893, acc.: 64.84%] [G loss: 1.521042]\n",
      "epoch:21 step:20478 [D loss: 0.550711, acc.: 71.88%] [G loss: 1.029838]\n",
      "epoch:21 step:20479 [D loss: 0.634174, acc.: 62.50%] [G loss: 1.029201]\n",
      "epoch:21 step:20480 [D loss: 0.641655, acc.: 61.72%] [G loss: 1.067947]\n",
      "epoch:21 step:20481 [D loss: 0.546317, acc.: 71.88%] [G loss: 1.442127]\n",
      "epoch:21 step:20482 [D loss: 0.437092, acc.: 81.25%] [G loss: 1.473015]\n",
      "epoch:21 step:20483 [D loss: 0.472108, acc.: 82.03%] [G loss: 1.428591]\n",
      "epoch:21 step:20484 [D loss: 0.548821, acc.: 70.31%] [G loss: 1.370228]\n",
      "epoch:21 step:20485 [D loss: 0.421011, acc.: 83.59%] [G loss: 1.506951]\n",
      "epoch:21 step:20486 [D loss: 0.509358, acc.: 75.78%] [G loss: 1.363810]\n",
      "epoch:21 step:20487 [D loss: 0.455754, acc.: 81.25%] [G loss: 0.950022]\n",
      "epoch:21 step:20488 [D loss: 0.594102, acc.: 67.97%] [G loss: 1.483893]\n",
      "epoch:21 step:20489 [D loss: 0.454518, acc.: 85.16%] [G loss: 1.166457]\n",
      "epoch:21 step:20490 [D loss: 0.608091, acc.: 68.75%] [G loss: 1.581585]\n",
      "epoch:21 step:20491 [D loss: 0.421815, acc.: 84.38%] [G loss: 1.259543]\n",
      "epoch:21 step:20492 [D loss: 0.667156, acc.: 57.03%] [G loss: 1.556355]\n",
      "epoch:21 step:20493 [D loss: 0.548381, acc.: 67.97%] [G loss: 1.568969]\n",
      "epoch:21 step:20494 [D loss: 0.449655, acc.: 80.47%] [G loss: 1.447048]\n",
      "epoch:21 step:20495 [D loss: 0.407904, acc.: 83.59%] [G loss: 1.288641]\n",
      "epoch:21 step:20496 [D loss: 0.575554, acc.: 75.78%] [G loss: 1.461058]\n",
      "epoch:21 step:20497 [D loss: 0.508892, acc.: 71.88%] [G loss: 1.350941]\n",
      "epoch:21 step:20498 [D loss: 0.733524, acc.: 46.09%] [G loss: 1.479864]\n",
      "epoch:21 step:20499 [D loss: 0.439719, acc.: 83.59%] [G loss: 1.479437]\n",
      "epoch:21 step:20500 [D loss: 0.439989, acc.: 82.03%] [G loss: 1.543434]\n",
      "epoch:21 step:20501 [D loss: 0.776296, acc.: 53.12%] [G loss: 1.055045]\n",
      "epoch:21 step:20502 [D loss: 0.414125, acc.: 85.94%] [G loss: 1.265048]\n",
      "epoch:21 step:20503 [D loss: 0.598093, acc.: 70.31%] [G loss: 1.240235]\n",
      "epoch:21 step:20504 [D loss: 0.501862, acc.: 75.78%] [G loss: 1.718384]\n",
      "epoch:21 step:20505 [D loss: 0.660157, acc.: 64.06%] [G loss: 1.369378]\n",
      "epoch:21 step:20506 [D loss: 0.518996, acc.: 78.12%] [G loss: 1.413771]\n",
      "epoch:21 step:20507 [D loss: 0.499011, acc.: 74.22%] [G loss: 1.213064]\n",
      "epoch:21 step:20508 [D loss: 0.694856, acc.: 62.50%] [G loss: 1.077780]\n",
      "epoch:21 step:20509 [D loss: 0.575596, acc.: 67.97%] [G loss: 0.919105]\n",
      "epoch:21 step:20510 [D loss: 0.624801, acc.: 64.84%] [G loss: 0.953927]\n",
      "epoch:21 step:20511 [D loss: 0.524601, acc.: 77.34%] [G loss: 1.538698]\n",
      "epoch:21 step:20512 [D loss: 0.628616, acc.: 66.41%] [G loss: 1.065643]\n",
      "epoch:21 step:20513 [D loss: 0.693485, acc.: 63.28%] [G loss: 1.457907]\n",
      "epoch:21 step:20514 [D loss: 0.699069, acc.: 53.12%] [G loss: 1.231760]\n",
      "epoch:21 step:20515 [D loss: 0.614769, acc.: 65.62%] [G loss: 1.584763]\n",
      "epoch:21 step:20516 [D loss: 0.471667, acc.: 78.91%] [G loss: 1.255249]\n",
      "epoch:21 step:20517 [D loss: 0.453709, acc.: 79.69%] [G loss: 1.423294]\n",
      "epoch:21 step:20518 [D loss: 0.387786, acc.: 84.38%] [G loss: 1.937993]\n",
      "epoch:21 step:20519 [D loss: 0.671419, acc.: 61.72%] [G loss: 1.136524]\n",
      "epoch:21 step:20520 [D loss: 0.489413, acc.: 73.44%] [G loss: 1.541319]\n",
      "epoch:21 step:20521 [D loss: 0.733971, acc.: 53.12%] [G loss: 1.477452]\n",
      "epoch:21 step:20522 [D loss: 0.563251, acc.: 75.00%] [G loss: 1.415612]\n",
      "epoch:21 step:20523 [D loss: 0.602362, acc.: 66.41%] [G loss: 1.267442]\n",
      "epoch:21 step:20524 [D loss: 0.554896, acc.: 71.09%] [G loss: 1.580295]\n",
      "epoch:21 step:20525 [D loss: 0.557659, acc.: 73.44%] [G loss: 1.357687]\n",
      "epoch:21 step:20526 [D loss: 0.541320, acc.: 69.53%] [G loss: 1.255348]\n",
      "epoch:21 step:20527 [D loss: 0.445858, acc.: 78.91%] [G loss: 1.354225]\n",
      "epoch:21 step:20528 [D loss: 0.618176, acc.: 62.50%] [G loss: 1.169700]\n",
      "epoch:21 step:20529 [D loss: 0.625575, acc.: 67.19%] [G loss: 1.142302]\n",
      "epoch:21 step:20530 [D loss: 0.598128, acc.: 68.75%] [G loss: 1.182401]\n",
      "epoch:21 step:20531 [D loss: 0.554922, acc.: 68.75%] [G loss: 1.115593]\n",
      "epoch:21 step:20532 [D loss: 0.529820, acc.: 73.44%] [G loss: 1.563501]\n",
      "epoch:21 step:20533 [D loss: 0.532317, acc.: 75.00%] [G loss: 1.167946]\n",
      "epoch:21 step:20534 [D loss: 0.490467, acc.: 75.00%] [G loss: 1.388558]\n",
      "epoch:21 step:20535 [D loss: 0.385990, acc.: 85.16%] [G loss: 1.566296]\n",
      "epoch:21 step:20536 [D loss: 0.573216, acc.: 70.31%] [G loss: 1.535463]\n",
      "epoch:21 step:20537 [D loss: 0.546670, acc.: 74.22%] [G loss: 1.008347]\n",
      "epoch:21 step:20538 [D loss: 0.405159, acc.: 86.72%] [G loss: 1.568924]\n",
      "epoch:21 step:20539 [D loss: 0.659842, acc.: 65.62%] [G loss: 0.939407]\n",
      "epoch:21 step:20540 [D loss: 0.495970, acc.: 71.09%] [G loss: 1.118485]\n",
      "epoch:21 step:20541 [D loss: 0.675473, acc.: 60.94%] [G loss: 1.071671]\n",
      "epoch:21 step:20542 [D loss: 0.617405, acc.: 64.84%] [G loss: 1.393086]\n",
      "epoch:21 step:20543 [D loss: 0.694335, acc.: 58.59%] [G loss: 1.332546]\n",
      "epoch:21 step:20544 [D loss: 0.580280, acc.: 63.28%] [G loss: 1.589936]\n",
      "epoch:21 step:20545 [D loss: 0.575857, acc.: 69.53%] [G loss: 1.054895]\n",
      "epoch:21 step:20546 [D loss: 0.475999, acc.: 81.25%] [G loss: 1.298090]\n",
      "epoch:21 step:20547 [D loss: 0.640863, acc.: 67.19%] [G loss: 1.321277]\n",
      "epoch:21 step:20548 [D loss: 0.535571, acc.: 69.53%] [G loss: 1.731655]\n",
      "epoch:21 step:20549 [D loss: 0.494296, acc.: 77.34%] [G loss: 1.234603]\n",
      "epoch:21 step:20550 [D loss: 0.408815, acc.: 84.38%] [G loss: 1.511321]\n",
      "epoch:21 step:20551 [D loss: 0.622997, acc.: 66.41%] [G loss: 1.287225]\n",
      "epoch:21 step:20552 [D loss: 0.475064, acc.: 78.91%] [G loss: 1.166804]\n",
      "epoch:21 step:20553 [D loss: 0.687353, acc.: 63.28%] [G loss: 1.709904]\n",
      "epoch:21 step:20554 [D loss: 0.514293, acc.: 75.00%] [G loss: 1.607138]\n",
      "epoch:21 step:20555 [D loss: 0.754702, acc.: 57.81%] [G loss: 1.384938]\n",
      "epoch:21 step:20556 [D loss: 0.596649, acc.: 68.75%] [G loss: 1.240225]\n",
      "epoch:21 step:20557 [D loss: 0.570958, acc.: 71.88%] [G loss: 1.423348]\n",
      "epoch:21 step:20558 [D loss: 0.515692, acc.: 75.00%] [G loss: 1.299735]\n",
      "epoch:21 step:20559 [D loss: 0.566620, acc.: 68.75%] [G loss: 1.206438]\n",
      "epoch:21 step:20560 [D loss: 0.557664, acc.: 66.41%] [G loss: 1.619068]\n",
      "epoch:21 step:20561 [D loss: 0.494538, acc.: 78.91%] [G loss: 1.461973]\n",
      "epoch:21 step:20562 [D loss: 0.626258, acc.: 62.50%] [G loss: 1.634207]\n",
      "epoch:21 step:20563 [D loss: 0.609645, acc.: 66.41%] [G loss: 1.269507]\n",
      "epoch:21 step:20564 [D loss: 0.654785, acc.: 61.72%] [G loss: 1.096857]\n",
      "epoch:21 step:20565 [D loss: 0.607161, acc.: 64.06%] [G loss: 1.358162]\n",
      "epoch:21 step:20566 [D loss: 0.647134, acc.: 62.50%] [G loss: 1.041471]\n",
      "epoch:21 step:20567 [D loss: 0.770749, acc.: 52.34%] [G loss: 0.886505]\n",
      "epoch:21 step:20568 [D loss: 0.485827, acc.: 78.91%] [G loss: 1.451428]\n",
      "epoch:21 step:20569 [D loss: 0.675458, acc.: 59.38%] [G loss: 0.888869]\n",
      "epoch:21 step:20570 [D loss: 0.533763, acc.: 72.66%] [G loss: 1.233323]\n",
      "epoch:21 step:20571 [D loss: 0.590816, acc.: 71.09%] [G loss: 1.215499]\n",
      "epoch:21 step:20572 [D loss: 0.640629, acc.: 62.50%] [G loss: 1.595555]\n",
      "epoch:21 step:20573 [D loss: 0.424269, acc.: 82.81%] [G loss: 1.625801]\n",
      "epoch:21 step:20574 [D loss: 0.545666, acc.: 73.44%] [G loss: 1.307680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20575 [D loss: 0.591946, acc.: 70.31%] [G loss: 1.044835]\n",
      "epoch:21 step:20576 [D loss: 0.664277, acc.: 60.94%] [G loss: 1.404880]\n",
      "epoch:21 step:20577 [D loss: 0.536874, acc.: 71.09%] [G loss: 0.957030]\n",
      "epoch:21 step:20578 [D loss: 0.684271, acc.: 62.50%] [G loss: 1.639917]\n",
      "epoch:21 step:20579 [D loss: 0.590544, acc.: 63.28%] [G loss: 1.132900]\n",
      "epoch:21 step:20580 [D loss: 0.592207, acc.: 62.50%] [G loss: 1.395712]\n",
      "epoch:21 step:20581 [D loss: 0.527121, acc.: 76.56%] [G loss: 1.451582]\n",
      "epoch:21 step:20582 [D loss: 0.684225, acc.: 57.81%] [G loss: 1.355214]\n",
      "epoch:21 step:20583 [D loss: 0.620752, acc.: 67.97%] [G loss: 1.288458]\n",
      "epoch:21 step:20584 [D loss: 0.618731, acc.: 64.06%] [G loss: 1.508785]\n",
      "epoch:21 step:20585 [D loss: 0.531802, acc.: 73.44%] [G loss: 1.270911]\n",
      "epoch:21 step:20586 [D loss: 0.619056, acc.: 70.31%] [G loss: 1.371070]\n",
      "epoch:21 step:20587 [D loss: 0.599251, acc.: 68.75%] [G loss: 1.310632]\n",
      "epoch:21 step:20588 [D loss: 0.509282, acc.: 71.88%] [G loss: 1.414739]\n",
      "epoch:21 step:20589 [D loss: 0.723208, acc.: 53.91%] [G loss: 1.210763]\n",
      "epoch:21 step:20590 [D loss: 0.648853, acc.: 61.72%] [G loss: 1.240689]\n",
      "epoch:21 step:20591 [D loss: 0.538379, acc.: 73.44%] [G loss: 1.244683]\n",
      "epoch:21 step:20592 [D loss: 0.511606, acc.: 73.44%] [G loss: 1.363254]\n",
      "epoch:21 step:20593 [D loss: 0.389282, acc.: 85.94%] [G loss: 1.741424]\n",
      "epoch:21 step:20594 [D loss: 0.592213, acc.: 64.84%] [G loss: 1.612556]\n",
      "epoch:21 step:20595 [D loss: 0.555499, acc.: 67.19%] [G loss: 1.475025]\n",
      "epoch:21 step:20596 [D loss: 0.622452, acc.: 67.97%] [G loss: 1.289202]\n",
      "epoch:21 step:20597 [D loss: 0.685645, acc.: 60.16%] [G loss: 1.461211]\n",
      "epoch:21 step:20598 [D loss: 0.797852, acc.: 53.12%] [G loss: 1.503172]\n",
      "epoch:21 step:20599 [D loss: 0.469192, acc.: 78.12%] [G loss: 1.649058]\n",
      "epoch:21 step:20600 [D loss: 0.619792, acc.: 71.09%] [G loss: 1.742810]\n",
      "##############\n",
      "[2.61172742 2.07110729 1.58849063 2.93407081 0.77861282 6.38459103\n",
      " 2.10743933 2.63061876 3.93741842 4.64277608]\n",
      "##########\n",
      "epoch:21 step:20601 [D loss: 0.460433, acc.: 80.47%] [G loss: 1.196551]\n",
      "epoch:21 step:20602 [D loss: 0.548930, acc.: 67.97%] [G loss: 1.356609]\n",
      "epoch:21 step:20603 [D loss: 0.582788, acc.: 71.88%] [G loss: 1.324140]\n",
      "epoch:21 step:20604 [D loss: 0.471565, acc.: 80.47%] [G loss: 1.342885]\n",
      "epoch:21 step:20605 [D loss: 0.505923, acc.: 78.12%] [G loss: 1.261804]\n",
      "epoch:21 step:20606 [D loss: 0.520405, acc.: 72.66%] [G loss: 1.736391]\n",
      "epoch:21 step:20607 [D loss: 0.513674, acc.: 74.22%] [G loss: 1.534185]\n",
      "epoch:21 step:20608 [D loss: 0.564274, acc.: 70.31%] [G loss: 1.116345]\n",
      "epoch:21 step:20609 [D loss: 0.383318, acc.: 87.50%] [G loss: 1.788047]\n",
      "epoch:21 step:20610 [D loss: 0.730159, acc.: 55.47%] [G loss: 1.465005]\n",
      "epoch:21 step:20611 [D loss: 0.564077, acc.: 67.19%] [G loss: 1.506628]\n",
      "epoch:21 step:20612 [D loss: 0.496657, acc.: 76.56%] [G loss: 1.638049]\n",
      "epoch:21 step:20613 [D loss: 0.477600, acc.: 75.00%] [G loss: 1.448097]\n",
      "epoch:21 step:20614 [D loss: 0.668516, acc.: 61.72%] [G loss: 1.561385]\n",
      "epoch:22 step:20615 [D loss: 0.645042, acc.: 64.06%] [G loss: 1.262240]\n",
      "epoch:22 step:20616 [D loss: 0.501199, acc.: 80.47%] [G loss: 1.353288]\n",
      "epoch:22 step:20617 [D loss: 0.546427, acc.: 71.88%] [G loss: 1.524870]\n",
      "epoch:22 step:20618 [D loss: 0.518468, acc.: 69.53%] [G loss: 1.352018]\n",
      "epoch:22 step:20619 [D loss: 0.518689, acc.: 73.44%] [G loss: 1.178104]\n",
      "epoch:22 step:20620 [D loss: 0.618285, acc.: 66.41%] [G loss: 1.283818]\n",
      "epoch:22 step:20621 [D loss: 0.601373, acc.: 67.97%] [G loss: 1.218512]\n",
      "epoch:22 step:20622 [D loss: 0.600552, acc.: 66.41%] [G loss: 1.224260]\n",
      "epoch:22 step:20623 [D loss: 0.648860, acc.: 64.06%] [G loss: 1.064110]\n",
      "epoch:22 step:20624 [D loss: 0.713892, acc.: 58.59%] [G loss: 1.313377]\n",
      "epoch:22 step:20625 [D loss: 0.572095, acc.: 73.44%] [G loss: 1.674363]\n",
      "epoch:22 step:20626 [D loss: 0.502766, acc.: 75.00%] [G loss: 1.513146]\n",
      "epoch:22 step:20627 [D loss: 0.614018, acc.: 64.84%] [G loss: 1.515529]\n",
      "epoch:22 step:20628 [D loss: 0.631532, acc.: 67.19%] [G loss: 1.175147]\n",
      "epoch:22 step:20629 [D loss: 0.526595, acc.: 72.66%] [G loss: 1.400649]\n",
      "epoch:22 step:20630 [D loss: 0.461176, acc.: 78.91%] [G loss: 1.620964]\n",
      "epoch:22 step:20631 [D loss: 0.548639, acc.: 75.00%] [G loss: 1.337130]\n",
      "epoch:22 step:20632 [D loss: 0.509913, acc.: 75.78%] [G loss: 1.181666]\n",
      "epoch:22 step:20633 [D loss: 0.655716, acc.: 64.84%] [G loss: 0.753866]\n",
      "epoch:22 step:20634 [D loss: 0.516457, acc.: 75.78%] [G loss: 1.398583]\n",
      "epoch:22 step:20635 [D loss: 0.517243, acc.: 79.69%] [G loss: 1.276869]\n",
      "epoch:22 step:20636 [D loss: 0.531914, acc.: 78.12%] [G loss: 1.074823]\n",
      "epoch:22 step:20637 [D loss: 0.428750, acc.: 82.03%] [G loss: 1.456233]\n",
      "epoch:22 step:20638 [D loss: 0.452683, acc.: 82.81%] [G loss: 1.428334]\n",
      "epoch:22 step:20639 [D loss: 0.535761, acc.: 73.44%] [G loss: 1.196818]\n",
      "epoch:22 step:20640 [D loss: 0.546792, acc.: 71.09%] [G loss: 1.365904]\n",
      "epoch:22 step:20641 [D loss: 0.520113, acc.: 76.56%] [G loss: 1.686038]\n",
      "epoch:22 step:20642 [D loss: 0.591244, acc.: 67.19%] [G loss: 1.341596]\n",
      "epoch:22 step:20643 [D loss: 0.564551, acc.: 71.09%] [G loss: 1.534652]\n",
      "epoch:22 step:20644 [D loss: 0.594218, acc.: 71.09%] [G loss: 1.270996]\n",
      "epoch:22 step:20645 [D loss: 0.606926, acc.: 68.75%] [G loss: 1.417606]\n",
      "epoch:22 step:20646 [D loss: 0.566223, acc.: 71.88%] [G loss: 1.458332]\n",
      "epoch:22 step:20647 [D loss: 0.603742, acc.: 67.97%] [G loss: 1.356259]\n",
      "epoch:22 step:20648 [D loss: 0.722111, acc.: 58.59%] [G loss: 1.002782]\n",
      "epoch:22 step:20649 [D loss: 0.407455, acc.: 85.16%] [G loss: 1.029155]\n",
      "epoch:22 step:20650 [D loss: 0.365541, acc.: 88.28%] [G loss: 1.424778]\n",
      "epoch:22 step:20651 [D loss: 0.642479, acc.: 61.72%] [G loss: 1.165297]\n",
      "epoch:22 step:20652 [D loss: 0.559672, acc.: 74.22%] [G loss: 1.553428]\n",
      "epoch:22 step:20653 [D loss: 0.621356, acc.: 67.19%] [G loss: 0.811459]\n",
      "epoch:22 step:20654 [D loss: 0.524249, acc.: 76.56%] [G loss: 1.472657]\n",
      "epoch:22 step:20655 [D loss: 0.521028, acc.: 73.44%] [G loss: 1.695318]\n",
      "epoch:22 step:20656 [D loss: 0.656055, acc.: 62.50%] [G loss: 1.536894]\n",
      "epoch:22 step:20657 [D loss: 0.462590, acc.: 79.69%] [G loss: 1.354067]\n",
      "epoch:22 step:20658 [D loss: 0.629058, acc.: 62.50%] [G loss: 1.443336]\n",
      "epoch:22 step:20659 [D loss: 0.426782, acc.: 85.16%] [G loss: 1.875020]\n",
      "epoch:22 step:20660 [D loss: 0.603247, acc.: 63.28%] [G loss: 1.208501]\n",
      "epoch:22 step:20661 [D loss: 0.685839, acc.: 60.94%] [G loss: 1.536667]\n",
      "epoch:22 step:20662 [D loss: 0.669570, acc.: 64.06%] [G loss: 1.410997]\n",
      "epoch:22 step:20663 [D loss: 0.648650, acc.: 63.28%] [G loss: 1.354040]\n",
      "epoch:22 step:20664 [D loss: 0.450148, acc.: 80.47%] [G loss: 1.380303]\n",
      "epoch:22 step:20665 [D loss: 0.464440, acc.: 81.25%] [G loss: 1.881149]\n",
      "epoch:22 step:20666 [D loss: 0.661945, acc.: 60.16%] [G loss: 1.389854]\n",
      "epoch:22 step:20667 [D loss: 0.514985, acc.: 74.22%] [G loss: 1.368113]\n",
      "epoch:22 step:20668 [D loss: 0.577837, acc.: 70.31%] [G loss: 1.341890]\n",
      "epoch:22 step:20669 [D loss: 0.512389, acc.: 78.12%] [G loss: 1.004838]\n",
      "epoch:22 step:20670 [D loss: 0.559222, acc.: 73.44%] [G loss: 1.232850]\n",
      "epoch:22 step:20671 [D loss: 0.577666, acc.: 71.88%] [G loss: 1.107283]\n",
      "epoch:22 step:20672 [D loss: 0.612304, acc.: 60.94%] [G loss: 1.603318]\n",
      "epoch:22 step:20673 [D loss: 0.451639, acc.: 84.38%] [G loss: 1.294152]\n",
      "epoch:22 step:20674 [D loss: 0.578572, acc.: 70.31%] [G loss: 1.409231]\n",
      "epoch:22 step:20675 [D loss: 0.603895, acc.: 65.62%] [G loss: 1.444767]\n",
      "epoch:22 step:20676 [D loss: 0.671087, acc.: 60.94%] [G loss: 1.027307]\n",
      "epoch:22 step:20677 [D loss: 0.527542, acc.: 75.78%] [G loss: 1.136160]\n",
      "epoch:22 step:20678 [D loss: 0.458620, acc.: 81.25%] [G loss: 1.555333]\n",
      "epoch:22 step:20679 [D loss: 0.519093, acc.: 76.56%] [G loss: 1.450935]\n",
      "epoch:22 step:20680 [D loss: 0.669178, acc.: 56.25%] [G loss: 1.228708]\n",
      "epoch:22 step:20681 [D loss: 0.416941, acc.: 83.59%] [G loss: 1.709926]\n",
      "epoch:22 step:20682 [D loss: 0.569486, acc.: 68.75%] [G loss: 1.468852]\n",
      "epoch:22 step:20683 [D loss: 0.540885, acc.: 73.44%] [G loss: 1.262358]\n",
      "epoch:22 step:20684 [D loss: 0.700722, acc.: 60.16%] [G loss: 1.297088]\n",
      "epoch:22 step:20685 [D loss: 0.400383, acc.: 85.94%] [G loss: 1.185428]\n",
      "epoch:22 step:20686 [D loss: 0.509876, acc.: 74.22%] [G loss: 1.191447]\n",
      "epoch:22 step:20687 [D loss: 0.539630, acc.: 70.31%] [G loss: 1.292167]\n",
      "epoch:22 step:20688 [D loss: 0.426361, acc.: 83.59%] [G loss: 1.428906]\n",
      "epoch:22 step:20689 [D loss: 0.687413, acc.: 58.59%] [G loss: 1.189646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20690 [D loss: 0.359565, acc.: 89.06%] [G loss: 1.408845]\n",
      "epoch:22 step:20691 [D loss: 0.595237, acc.: 68.75%] [G loss: 1.323245]\n",
      "epoch:22 step:20692 [D loss: 0.492562, acc.: 77.34%] [G loss: 1.198509]\n",
      "epoch:22 step:20693 [D loss: 0.648200, acc.: 67.97%] [G loss: 1.041953]\n",
      "epoch:22 step:20694 [D loss: 0.585969, acc.: 71.09%] [G loss: 1.237404]\n",
      "epoch:22 step:20695 [D loss: 0.692800, acc.: 61.72%] [G loss: 1.368309]\n",
      "epoch:22 step:20696 [D loss: 0.583631, acc.: 70.31%] [G loss: 1.481966]\n",
      "epoch:22 step:20697 [D loss: 0.644464, acc.: 65.62%] [G loss: 1.255077]\n",
      "epoch:22 step:20698 [D loss: 0.538769, acc.: 76.56%] [G loss: 1.334303]\n",
      "epoch:22 step:20699 [D loss: 0.588603, acc.: 67.97%] [G loss: 1.271085]\n",
      "epoch:22 step:20700 [D loss: 0.545421, acc.: 75.78%] [G loss: 1.231910]\n",
      "epoch:22 step:20701 [D loss: 0.483041, acc.: 78.12%] [G loss: 1.496081]\n",
      "epoch:22 step:20702 [D loss: 0.659612, acc.: 63.28%] [G loss: 1.193138]\n",
      "epoch:22 step:20703 [D loss: 0.475648, acc.: 78.91%] [G loss: 1.592721]\n",
      "epoch:22 step:20704 [D loss: 0.530532, acc.: 70.31%] [G loss: 1.343418]\n",
      "epoch:22 step:20705 [D loss: 0.650488, acc.: 64.06%] [G loss: 1.106444]\n",
      "epoch:22 step:20706 [D loss: 0.533872, acc.: 71.09%] [G loss: 1.322054]\n",
      "epoch:22 step:20707 [D loss: 0.499653, acc.: 74.22%] [G loss: 1.410685]\n",
      "epoch:22 step:20708 [D loss: 0.595162, acc.: 69.53%] [G loss: 1.293581]\n",
      "epoch:22 step:20709 [D loss: 0.585240, acc.: 71.09%] [G loss: 0.945576]\n",
      "epoch:22 step:20710 [D loss: 0.551035, acc.: 68.75%] [G loss: 1.047080]\n",
      "epoch:22 step:20711 [D loss: 0.707759, acc.: 56.25%] [G loss: 1.629361]\n",
      "epoch:22 step:20712 [D loss: 0.473418, acc.: 75.78%] [G loss: 1.718825]\n",
      "epoch:22 step:20713 [D loss: 0.534005, acc.: 71.88%] [G loss: 1.461337]\n",
      "epoch:22 step:20714 [D loss: 0.612688, acc.: 64.84%] [G loss: 1.261387]\n",
      "epoch:22 step:20715 [D loss: 0.463512, acc.: 80.47%] [G loss: 1.222219]\n",
      "epoch:22 step:20716 [D loss: 0.624998, acc.: 65.62%] [G loss: 0.941153]\n",
      "epoch:22 step:20717 [D loss: 0.584562, acc.: 67.19%] [G loss: 1.496391]\n",
      "epoch:22 step:20718 [D loss: 0.601822, acc.: 67.19%] [G loss: 1.280490]\n",
      "epoch:22 step:20719 [D loss: 0.441140, acc.: 85.16%] [G loss: 1.518384]\n",
      "epoch:22 step:20720 [D loss: 0.641900, acc.: 62.50%] [G loss: 1.495825]\n",
      "epoch:22 step:20721 [D loss: 0.575969, acc.: 69.53%] [G loss: 1.612306]\n",
      "epoch:22 step:20722 [D loss: 0.502965, acc.: 80.47%] [G loss: 1.497804]\n",
      "epoch:22 step:20723 [D loss: 0.626265, acc.: 59.38%] [G loss: 1.452336]\n",
      "epoch:22 step:20724 [D loss: 0.558433, acc.: 71.88%] [G loss: 1.642247]\n",
      "epoch:22 step:20725 [D loss: 0.907065, acc.: 43.75%] [G loss: 1.021510]\n",
      "epoch:22 step:20726 [D loss: 0.536400, acc.: 71.88%] [G loss: 1.605239]\n",
      "epoch:22 step:20727 [D loss: 0.512322, acc.: 69.53%] [G loss: 1.253451]\n",
      "epoch:22 step:20728 [D loss: 0.459409, acc.: 78.91%] [G loss: 1.286206]\n",
      "epoch:22 step:20729 [D loss: 0.569962, acc.: 72.66%] [G loss: 1.578665]\n",
      "epoch:22 step:20730 [D loss: 0.718862, acc.: 60.16%] [G loss: 1.420197]\n",
      "epoch:22 step:20731 [D loss: 0.581888, acc.: 68.75%] [G loss: 1.124733]\n",
      "epoch:22 step:20732 [D loss: 0.525159, acc.: 81.25%] [G loss: 1.089703]\n",
      "epoch:22 step:20733 [D loss: 0.355673, acc.: 85.94%] [G loss: 1.589759]\n",
      "epoch:22 step:20734 [D loss: 0.745783, acc.: 50.00%] [G loss: 1.256220]\n",
      "epoch:22 step:20735 [D loss: 0.544087, acc.: 75.78%] [G loss: 1.494652]\n",
      "epoch:22 step:20736 [D loss: 0.690478, acc.: 64.84%] [G loss: 1.257989]\n",
      "epoch:22 step:20737 [D loss: 0.503874, acc.: 72.66%] [G loss: 1.352928]\n",
      "epoch:22 step:20738 [D loss: 0.519803, acc.: 76.56%] [G loss: 1.111876]\n",
      "epoch:22 step:20739 [D loss: 0.481917, acc.: 76.56%] [G loss: 1.708709]\n",
      "epoch:22 step:20740 [D loss: 0.462551, acc.: 81.25%] [G loss: 1.430110]\n",
      "epoch:22 step:20741 [D loss: 0.442477, acc.: 82.81%] [G loss: 1.343410]\n",
      "epoch:22 step:20742 [D loss: 0.543248, acc.: 71.88%] [G loss: 1.204115]\n",
      "epoch:22 step:20743 [D loss: 0.496051, acc.: 78.91%] [G loss: 1.363449]\n",
      "epoch:22 step:20744 [D loss: 0.495881, acc.: 75.00%] [G loss: 1.664179]\n",
      "epoch:22 step:20745 [D loss: 0.478826, acc.: 80.47%] [G loss: 1.700176]\n",
      "epoch:22 step:20746 [D loss: 0.486344, acc.: 76.56%] [G loss: 1.331664]\n",
      "epoch:22 step:20747 [D loss: 0.566031, acc.: 67.19%] [G loss: 1.305033]\n",
      "epoch:22 step:20748 [D loss: 0.470518, acc.: 79.69%] [G loss: 1.618329]\n",
      "epoch:22 step:20749 [D loss: 0.498624, acc.: 75.78%] [G loss: 1.535926]\n",
      "epoch:22 step:20750 [D loss: 0.676960, acc.: 57.03%] [G loss: 1.206608]\n",
      "epoch:22 step:20751 [D loss: 0.619146, acc.: 65.62%] [G loss: 1.277414]\n",
      "epoch:22 step:20752 [D loss: 0.863120, acc.: 41.41%] [G loss: 1.351099]\n",
      "epoch:22 step:20753 [D loss: 0.672347, acc.: 58.59%] [G loss: 1.374039]\n",
      "epoch:22 step:20754 [D loss: 0.576062, acc.: 71.88%] [G loss: 1.355689]\n",
      "epoch:22 step:20755 [D loss: 0.636312, acc.: 67.97%] [G loss: 1.344504]\n",
      "epoch:22 step:20756 [D loss: 0.522163, acc.: 73.44%] [G loss: 1.324491]\n",
      "epoch:22 step:20757 [D loss: 0.670055, acc.: 58.59%] [G loss: 1.265353]\n",
      "epoch:22 step:20758 [D loss: 0.645905, acc.: 64.06%] [G loss: 1.850190]\n",
      "epoch:22 step:20759 [D loss: 0.499469, acc.: 75.78%] [G loss: 1.321915]\n",
      "epoch:22 step:20760 [D loss: 0.618049, acc.: 68.75%] [G loss: 1.364038]\n",
      "epoch:22 step:20761 [D loss: 0.455201, acc.: 80.47%] [G loss: 1.739212]\n",
      "epoch:22 step:20762 [D loss: 0.606543, acc.: 70.31%] [G loss: 1.640560]\n",
      "epoch:22 step:20763 [D loss: 0.629684, acc.: 61.72%] [G loss: 1.163109]\n",
      "epoch:22 step:20764 [D loss: 0.537720, acc.: 74.22%] [G loss: 1.853145]\n",
      "epoch:22 step:20765 [D loss: 0.624740, acc.: 67.19%] [G loss: 1.189891]\n",
      "epoch:22 step:20766 [D loss: 0.499042, acc.: 78.12%] [G loss: 1.507482]\n",
      "epoch:22 step:20767 [D loss: 0.399699, acc.: 85.94%] [G loss: 1.548623]\n",
      "epoch:22 step:20768 [D loss: 0.600227, acc.: 67.97%] [G loss: 1.649353]\n",
      "epoch:22 step:20769 [D loss: 0.579500, acc.: 67.97%] [G loss: 1.009127]\n",
      "epoch:22 step:20770 [D loss: 0.633712, acc.: 69.53%] [G loss: 1.146330]\n",
      "epoch:22 step:20771 [D loss: 0.537997, acc.: 72.66%] [G loss: 1.103618]\n",
      "epoch:22 step:20772 [D loss: 0.545234, acc.: 71.88%] [G loss: 1.419554]\n",
      "epoch:22 step:20773 [D loss: 0.512170, acc.: 75.78%] [G loss: 1.214715]\n",
      "epoch:22 step:20774 [D loss: 0.636564, acc.: 60.94%] [G loss: 1.745731]\n",
      "epoch:22 step:20775 [D loss: 0.560121, acc.: 72.66%] [G loss: 1.464005]\n",
      "epoch:22 step:20776 [D loss: 0.620279, acc.: 64.84%] [G loss: 1.476651]\n",
      "epoch:22 step:20777 [D loss: 0.577184, acc.: 72.66%] [G loss: 1.489586]\n",
      "epoch:22 step:20778 [D loss: 0.479499, acc.: 74.22%] [G loss: 1.374382]\n",
      "epoch:22 step:20779 [D loss: 0.430958, acc.: 83.59%] [G loss: 1.438883]\n",
      "epoch:22 step:20780 [D loss: 0.572132, acc.: 69.53%] [G loss: 1.549966]\n",
      "epoch:22 step:20781 [D loss: 0.545040, acc.: 72.66%] [G loss: 1.378437]\n",
      "epoch:22 step:20782 [D loss: 0.518708, acc.: 75.00%] [G loss: 1.342745]\n",
      "epoch:22 step:20783 [D loss: 0.549677, acc.: 70.31%] [G loss: 1.230554]\n",
      "epoch:22 step:20784 [D loss: 0.489728, acc.: 71.88%] [G loss: 1.236142]\n",
      "epoch:22 step:20785 [D loss: 0.643048, acc.: 64.06%] [G loss: 1.535471]\n",
      "epoch:22 step:20786 [D loss: 0.491780, acc.: 78.12%] [G loss: 1.645399]\n",
      "epoch:22 step:20787 [D loss: 0.589247, acc.: 65.62%] [G loss: 1.465020]\n",
      "epoch:22 step:20788 [D loss: 0.374616, acc.: 85.94%] [G loss: 1.550470]\n",
      "epoch:22 step:20789 [D loss: 0.608375, acc.: 68.75%] [G loss: 1.026530]\n",
      "epoch:22 step:20790 [D loss: 0.665793, acc.: 64.06%] [G loss: 1.089216]\n",
      "epoch:22 step:20791 [D loss: 0.725958, acc.: 55.47%] [G loss: 1.112344]\n",
      "epoch:22 step:20792 [D loss: 0.379881, acc.: 87.50%] [G loss: 1.632577]\n",
      "epoch:22 step:20793 [D loss: 0.510792, acc.: 74.22%] [G loss: 1.613240]\n",
      "epoch:22 step:20794 [D loss: 0.516827, acc.: 75.00%] [G loss: 1.515721]\n",
      "epoch:22 step:20795 [D loss: 0.532218, acc.: 78.91%] [G loss: 1.440251]\n",
      "epoch:22 step:20796 [D loss: 0.598715, acc.: 64.06%] [G loss: 1.332380]\n",
      "epoch:22 step:20797 [D loss: 0.588967, acc.: 73.44%] [G loss: 1.172552]\n",
      "epoch:22 step:20798 [D loss: 0.580997, acc.: 68.75%] [G loss: 1.808870]\n",
      "epoch:22 step:20799 [D loss: 0.462269, acc.: 78.12%] [G loss: 1.713598]\n",
      "epoch:22 step:20800 [D loss: 0.516381, acc.: 78.12%] [G loss: 1.322560]\n",
      "##############\n",
      "[2.64820174 2.06251633 1.77121444 2.79101028 0.84212299 6.17270408\n",
      " 2.08343383 2.59551121 3.80519617 5.40455391]\n",
      "##########\n",
      "epoch:22 step:20801 [D loss: 0.399083, acc.: 86.72%] [G loss: 1.248042]\n",
      "epoch:22 step:20802 [D loss: 0.523726, acc.: 73.44%] [G loss: 1.671304]\n",
      "epoch:22 step:20803 [D loss: 0.612453, acc.: 67.19%] [G loss: 1.779347]\n",
      "epoch:22 step:20804 [D loss: 0.718662, acc.: 56.25%] [G loss: 1.518854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20805 [D loss: 0.641765, acc.: 64.84%] [G loss: 1.424235]\n",
      "epoch:22 step:20806 [D loss: 0.556005, acc.: 68.75%] [G loss: 1.587867]\n",
      "epoch:22 step:20807 [D loss: 0.701343, acc.: 57.03%] [G loss: 1.392758]\n",
      "epoch:22 step:20808 [D loss: 0.526376, acc.: 72.66%] [G loss: 1.493448]\n",
      "epoch:22 step:20809 [D loss: 0.590021, acc.: 67.97%] [G loss: 1.257784]\n",
      "epoch:22 step:20810 [D loss: 0.594913, acc.: 69.53%] [G loss: 1.394665]\n",
      "epoch:22 step:20811 [D loss: 0.501751, acc.: 76.56%] [G loss: 1.456666]\n",
      "epoch:22 step:20812 [D loss: 0.770257, acc.: 56.25%] [G loss: 1.644633]\n",
      "epoch:22 step:20813 [D loss: 0.436432, acc.: 81.25%] [G loss: 1.671656]\n",
      "epoch:22 step:20814 [D loss: 0.501156, acc.: 78.91%] [G loss: 1.287993]\n",
      "epoch:22 step:20815 [D loss: 0.410517, acc.: 82.81%] [G loss: 1.685466]\n",
      "epoch:22 step:20816 [D loss: 0.480260, acc.: 78.12%] [G loss: 1.505728]\n",
      "epoch:22 step:20817 [D loss: 0.591088, acc.: 70.31%] [G loss: 1.423823]\n",
      "epoch:22 step:20818 [D loss: 0.438864, acc.: 85.94%] [G loss: 1.234444]\n",
      "epoch:22 step:20819 [D loss: 0.799207, acc.: 50.00%] [G loss: 1.259428]\n",
      "epoch:22 step:20820 [D loss: 0.491326, acc.: 76.56%] [G loss: 1.767568]\n",
      "epoch:22 step:20821 [D loss: 0.657759, acc.: 61.72%] [G loss: 1.736050]\n",
      "epoch:22 step:20822 [D loss: 0.470918, acc.: 78.91%] [G loss: 1.706090]\n",
      "epoch:22 step:20823 [D loss: 0.653159, acc.: 63.28%] [G loss: 1.751657]\n",
      "epoch:22 step:20824 [D loss: 0.634462, acc.: 67.19%] [G loss: 1.447693]\n",
      "epoch:22 step:20825 [D loss: 0.458654, acc.: 80.47%] [G loss: 1.463334]\n",
      "epoch:22 step:20826 [D loss: 0.567038, acc.: 71.88%] [G loss: 1.191247]\n",
      "epoch:22 step:20827 [D loss: 0.541850, acc.: 73.44%] [G loss: 1.557676]\n",
      "epoch:22 step:20828 [D loss: 0.740486, acc.: 53.12%] [G loss: 1.162012]\n",
      "epoch:22 step:20829 [D loss: 0.583487, acc.: 67.97%] [G loss: 1.184715]\n",
      "epoch:22 step:20830 [D loss: 0.619718, acc.: 60.16%] [G loss: 1.382514]\n",
      "epoch:22 step:20831 [D loss: 0.507227, acc.: 75.00%] [G loss: 1.712686]\n",
      "epoch:22 step:20832 [D loss: 0.506248, acc.: 74.22%] [G loss: 1.341839]\n",
      "epoch:22 step:20833 [D loss: 0.545840, acc.: 74.22%] [G loss: 1.609939]\n",
      "epoch:22 step:20834 [D loss: 0.559128, acc.: 71.09%] [G loss: 1.139321]\n",
      "epoch:22 step:20835 [D loss: 0.570136, acc.: 68.75%] [G loss: 1.455136]\n",
      "epoch:22 step:20836 [D loss: 0.690546, acc.: 56.25%] [G loss: 1.134778]\n",
      "epoch:22 step:20837 [D loss: 0.532107, acc.: 75.78%] [G loss: 1.549696]\n",
      "epoch:22 step:20838 [D loss: 0.544526, acc.: 75.00%] [G loss: 1.201735]\n",
      "epoch:22 step:20839 [D loss: 0.580865, acc.: 69.53%] [G loss: 1.275063]\n",
      "epoch:22 step:20840 [D loss: 0.686768, acc.: 60.94%] [G loss: 0.989827]\n",
      "epoch:22 step:20841 [D loss: 0.545200, acc.: 71.09%] [G loss: 1.068935]\n",
      "epoch:22 step:20842 [D loss: 0.583081, acc.: 67.19%] [G loss: 1.466232]\n",
      "epoch:22 step:20843 [D loss: 0.538285, acc.: 75.00%] [G loss: 1.250735]\n",
      "epoch:22 step:20844 [D loss: 0.531649, acc.: 73.44%] [G loss: 1.385667]\n",
      "epoch:22 step:20845 [D loss: 0.414675, acc.: 84.38%] [G loss: 1.558005]\n",
      "epoch:22 step:20846 [D loss: 0.469129, acc.: 82.03%] [G loss: 1.417254]\n",
      "epoch:22 step:20847 [D loss: 0.631153, acc.: 62.50%] [G loss: 1.424861]\n",
      "epoch:22 step:20848 [D loss: 0.536783, acc.: 72.66%] [G loss: 1.396946]\n",
      "epoch:22 step:20849 [D loss: 0.468261, acc.: 78.12%] [G loss: 1.293226]\n",
      "epoch:22 step:20850 [D loss: 0.608478, acc.: 71.09%] [G loss: 1.270339]\n",
      "epoch:22 step:20851 [D loss: 0.708825, acc.: 59.38%] [G loss: 1.245712]\n",
      "epoch:22 step:20852 [D loss: 0.729678, acc.: 57.81%] [G loss: 1.326916]\n",
      "epoch:22 step:20853 [D loss: 0.606896, acc.: 64.84%] [G loss: 1.192147]\n",
      "epoch:22 step:20854 [D loss: 0.418840, acc.: 83.59%] [G loss: 1.247683]\n",
      "epoch:22 step:20855 [D loss: 0.511041, acc.: 77.34%] [G loss: 1.488102]\n",
      "epoch:22 step:20856 [D loss: 0.686579, acc.: 58.59%] [G loss: 0.970235]\n",
      "epoch:22 step:20857 [D loss: 0.646957, acc.: 63.28%] [G loss: 1.240318]\n",
      "epoch:22 step:20858 [D loss: 0.522072, acc.: 75.00%] [G loss: 1.135975]\n",
      "epoch:22 step:20859 [D loss: 0.648727, acc.: 62.50%] [G loss: 1.295765]\n",
      "epoch:22 step:20860 [D loss: 0.480043, acc.: 78.91%] [G loss: 1.190651]\n",
      "epoch:22 step:20861 [D loss: 0.658138, acc.: 64.06%] [G loss: 1.483332]\n",
      "epoch:22 step:20862 [D loss: 0.447086, acc.: 80.47%] [G loss: 1.725949]\n",
      "epoch:22 step:20863 [D loss: 0.342761, acc.: 91.41%] [G loss: 1.843692]\n",
      "epoch:22 step:20864 [D loss: 0.575052, acc.: 73.44%] [G loss: 1.533048]\n",
      "epoch:22 step:20865 [D loss: 0.667642, acc.: 63.28%] [G loss: 1.299238]\n",
      "epoch:22 step:20866 [D loss: 0.518201, acc.: 72.66%] [G loss: 1.350286]\n",
      "epoch:22 step:20867 [D loss: 0.669908, acc.: 61.72%] [G loss: 1.093797]\n",
      "epoch:22 step:20868 [D loss: 0.507361, acc.: 78.91%] [G loss: 1.582120]\n",
      "epoch:22 step:20869 [D loss: 0.757798, acc.: 48.44%] [G loss: 1.132316]\n",
      "epoch:22 step:20870 [D loss: 0.626489, acc.: 67.19%] [G loss: 1.084378]\n",
      "epoch:22 step:20871 [D loss: 0.450491, acc.: 84.38%] [G loss: 1.296232]\n",
      "epoch:22 step:20872 [D loss: 0.429549, acc.: 81.25%] [G loss: 1.199501]\n",
      "epoch:22 step:20873 [D loss: 0.535970, acc.: 70.31%] [G loss: 1.384348]\n",
      "epoch:22 step:20874 [D loss: 0.542521, acc.: 74.22%] [G loss: 1.037004]\n",
      "epoch:22 step:20875 [D loss: 0.444564, acc.: 78.12%] [G loss: 1.206205]\n",
      "epoch:22 step:20876 [D loss: 0.727246, acc.: 55.47%] [G loss: 1.175144]\n",
      "epoch:22 step:20877 [D loss: 0.579193, acc.: 72.66%] [G loss: 1.220120]\n",
      "epoch:22 step:20878 [D loss: 0.477322, acc.: 80.47%] [G loss: 1.394806]\n",
      "epoch:22 step:20879 [D loss: 0.386766, acc.: 85.16%] [G loss: 1.298689]\n",
      "epoch:22 step:20880 [D loss: 0.522482, acc.: 71.09%] [G loss: 1.528120]\n",
      "epoch:22 step:20881 [D loss: 0.489281, acc.: 75.78%] [G loss: 1.763304]\n",
      "epoch:22 step:20882 [D loss: 0.587403, acc.: 70.31%] [G loss: 1.510409]\n",
      "epoch:22 step:20883 [D loss: 0.323505, acc.: 89.84%] [G loss: 1.413035]\n",
      "epoch:22 step:20884 [D loss: 0.586218, acc.: 73.44%] [G loss: 1.257615]\n",
      "epoch:22 step:20885 [D loss: 0.363686, acc.: 85.94%] [G loss: 1.790280]\n",
      "epoch:22 step:20886 [D loss: 0.545608, acc.: 71.09%] [G loss: 1.231719]\n",
      "epoch:22 step:20887 [D loss: 0.621314, acc.: 66.41%] [G loss: 1.209426]\n",
      "epoch:22 step:20888 [D loss: 0.475787, acc.: 77.34%] [G loss: 1.120950]\n",
      "epoch:22 step:20889 [D loss: 0.575013, acc.: 67.97%] [G loss: 1.357399]\n",
      "epoch:22 step:20890 [D loss: 0.735107, acc.: 60.94%] [G loss: 1.099308]\n",
      "epoch:22 step:20891 [D loss: 0.435496, acc.: 78.91%] [G loss: 1.781189]\n",
      "epoch:22 step:20892 [D loss: 0.517363, acc.: 77.34%] [G loss: 1.117747]\n",
      "epoch:22 step:20893 [D loss: 0.507099, acc.: 78.12%] [G loss: 1.331932]\n",
      "epoch:22 step:20894 [D loss: 0.516534, acc.: 73.44%] [G loss: 1.148700]\n",
      "epoch:22 step:20895 [D loss: 0.566900, acc.: 73.44%] [G loss: 1.317548]\n",
      "epoch:22 step:20896 [D loss: 0.637991, acc.: 64.06%] [G loss: 1.191579]\n",
      "epoch:22 step:20897 [D loss: 0.694545, acc.: 58.59%] [G loss: 1.343503]\n",
      "epoch:22 step:20898 [D loss: 0.649761, acc.: 60.16%] [G loss: 1.033247]\n",
      "epoch:22 step:20899 [D loss: 0.486666, acc.: 78.12%] [G loss: 1.448240]\n",
      "epoch:22 step:20900 [D loss: 0.594492, acc.: 73.44%] [G loss: 1.618571]\n",
      "epoch:22 step:20901 [D loss: 0.615301, acc.: 66.41%] [G loss: 1.298306]\n",
      "epoch:22 step:20902 [D loss: 0.542696, acc.: 68.75%] [G loss: 1.231567]\n",
      "epoch:22 step:20903 [D loss: 0.530760, acc.: 73.44%] [G loss: 1.540634]\n",
      "epoch:22 step:20904 [D loss: 0.511445, acc.: 73.44%] [G loss: 1.633353]\n",
      "epoch:22 step:20905 [D loss: 0.635109, acc.: 65.62%] [G loss: 1.584962]\n",
      "epoch:22 step:20906 [D loss: 0.712791, acc.: 55.47%] [G loss: 1.656767]\n",
      "epoch:22 step:20907 [D loss: 0.642813, acc.: 63.28%] [G loss: 1.046673]\n",
      "epoch:22 step:20908 [D loss: 0.704481, acc.: 57.81%] [G loss: 1.306871]\n",
      "epoch:22 step:20909 [D loss: 0.594589, acc.: 63.28%] [G loss: 1.203826]\n",
      "epoch:22 step:20910 [D loss: 0.525617, acc.: 72.66%] [G loss: 1.206980]\n",
      "epoch:22 step:20911 [D loss: 0.559788, acc.: 74.22%] [G loss: 1.679442]\n",
      "epoch:22 step:20912 [D loss: 0.783290, acc.: 45.31%] [G loss: 1.423235]\n",
      "epoch:22 step:20913 [D loss: 0.586281, acc.: 69.53%] [G loss: 1.197584]\n",
      "epoch:22 step:20914 [D loss: 0.487479, acc.: 79.69%] [G loss: 1.270055]\n",
      "epoch:22 step:20915 [D loss: 0.639325, acc.: 64.84%] [G loss: 1.156454]\n",
      "epoch:22 step:20916 [D loss: 0.477866, acc.: 79.69%] [G loss: 1.400010]\n",
      "epoch:22 step:20917 [D loss: 0.598668, acc.: 67.97%] [G loss: 1.200252]\n",
      "epoch:22 step:20918 [D loss: 0.536421, acc.: 71.09%] [G loss: 1.076130]\n",
      "epoch:22 step:20919 [D loss: 0.525941, acc.: 71.88%] [G loss: 1.550677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20920 [D loss: 0.558144, acc.: 70.31%] [G loss: 1.568728]\n",
      "epoch:22 step:20921 [D loss: 0.624820, acc.: 68.75%] [G loss: 1.436342]\n",
      "epoch:22 step:20922 [D loss: 0.443770, acc.: 82.81%] [G loss: 1.378085]\n",
      "epoch:22 step:20923 [D loss: 0.686040, acc.: 59.38%] [G loss: 1.143629]\n",
      "epoch:22 step:20924 [D loss: 0.646800, acc.: 64.06%] [G loss: 1.124016]\n",
      "epoch:22 step:20925 [D loss: 0.509454, acc.: 74.22%] [G loss: 1.590802]\n",
      "epoch:22 step:20926 [D loss: 0.505287, acc.: 77.34%] [G loss: 1.460508]\n",
      "epoch:22 step:20927 [D loss: 0.643363, acc.: 64.06%] [G loss: 1.075116]\n",
      "epoch:22 step:20928 [D loss: 0.465601, acc.: 78.91%] [G loss: 1.344468]\n",
      "epoch:22 step:20929 [D loss: 0.676893, acc.: 57.81%] [G loss: 1.246895]\n",
      "epoch:22 step:20930 [D loss: 0.592344, acc.: 67.97%] [G loss: 1.673589]\n",
      "epoch:22 step:20931 [D loss: 0.511154, acc.: 80.47%] [G loss: 1.406523]\n",
      "epoch:22 step:20932 [D loss: 0.658724, acc.: 64.06%] [G loss: 1.589422]\n",
      "epoch:22 step:20933 [D loss: 0.556753, acc.: 73.44%] [G loss: 1.589281]\n",
      "epoch:22 step:20934 [D loss: 0.564302, acc.: 71.88%] [G loss: 1.533124]\n",
      "epoch:22 step:20935 [D loss: 0.435297, acc.: 83.59%] [G loss: 1.157186]\n",
      "epoch:22 step:20936 [D loss: 0.625692, acc.: 65.62%] [G loss: 1.218275]\n",
      "epoch:22 step:20937 [D loss: 0.502492, acc.: 74.22%] [G loss: 1.772658]\n",
      "epoch:22 step:20938 [D loss: 0.468893, acc.: 78.91%] [G loss: 1.450982]\n",
      "epoch:22 step:20939 [D loss: 0.599142, acc.: 68.75%] [G loss: 1.230606]\n",
      "epoch:22 step:20940 [D loss: 0.688193, acc.: 60.16%] [G loss: 1.399464]\n",
      "epoch:22 step:20941 [D loss: 0.467676, acc.: 77.34%] [G loss: 1.374726]\n",
      "epoch:22 step:20942 [D loss: 0.363116, acc.: 85.94%] [G loss: 1.443078]\n",
      "epoch:22 step:20943 [D loss: 0.584979, acc.: 65.62%] [G loss: 1.399558]\n",
      "epoch:22 step:20944 [D loss: 0.633441, acc.: 64.84%] [G loss: 1.287835]\n",
      "epoch:22 step:20945 [D loss: 0.413129, acc.: 82.81%] [G loss: 1.494889]\n",
      "epoch:22 step:20946 [D loss: 0.593301, acc.: 67.19%] [G loss: 1.332226]\n",
      "epoch:22 step:20947 [D loss: 0.527474, acc.: 73.44%] [G loss: 1.030612]\n",
      "epoch:22 step:20948 [D loss: 0.565132, acc.: 72.66%] [G loss: 1.168949]\n",
      "epoch:22 step:20949 [D loss: 0.586448, acc.: 67.97%] [G loss: 1.194536]\n",
      "epoch:22 step:20950 [D loss: 0.464396, acc.: 78.12%] [G loss: 1.299299]\n",
      "epoch:22 step:20951 [D loss: 0.534713, acc.: 72.66%] [G loss: 1.387675]\n",
      "epoch:22 step:20952 [D loss: 0.592133, acc.: 70.31%] [G loss: 1.329654]\n",
      "epoch:22 step:20953 [D loss: 0.475761, acc.: 80.47%] [G loss: 1.649974]\n",
      "epoch:22 step:20954 [D loss: 0.428105, acc.: 84.38%] [G loss: 1.377540]\n",
      "epoch:22 step:20955 [D loss: 0.498320, acc.: 72.66%] [G loss: 1.185125]\n",
      "epoch:22 step:20956 [D loss: 0.629659, acc.: 66.41%] [G loss: 1.309805]\n",
      "epoch:22 step:20957 [D loss: 0.482911, acc.: 81.25%] [G loss: 1.246391]\n",
      "epoch:22 step:20958 [D loss: 0.644656, acc.: 65.62%] [G loss: 1.060919]\n",
      "epoch:22 step:20959 [D loss: 0.422799, acc.: 85.16%] [G loss: 1.576501]\n",
      "epoch:22 step:20960 [D loss: 0.639610, acc.: 62.50%] [G loss: 1.240730]\n",
      "epoch:22 step:20961 [D loss: 0.532185, acc.: 75.78%] [G loss: 1.331217]\n",
      "epoch:22 step:20962 [D loss: 0.613836, acc.: 65.62%] [G loss: 1.429249]\n",
      "epoch:22 step:20963 [D loss: 0.544959, acc.: 71.88%] [G loss: 1.268127]\n",
      "epoch:22 step:20964 [D loss: 0.601050, acc.: 70.31%] [G loss: 1.308533]\n",
      "epoch:22 step:20965 [D loss: 0.598298, acc.: 70.31%] [G loss: 1.209417]\n",
      "epoch:22 step:20966 [D loss: 0.535479, acc.: 71.88%] [G loss: 1.306113]\n",
      "epoch:22 step:20967 [D loss: 0.540775, acc.: 78.12%] [G loss: 1.107936]\n",
      "epoch:22 step:20968 [D loss: 0.581482, acc.: 69.53%] [G loss: 1.362896]\n",
      "epoch:22 step:20969 [D loss: 0.573696, acc.: 70.31%] [G loss: 1.220627]\n",
      "epoch:22 step:20970 [D loss: 0.365705, acc.: 85.94%] [G loss: 1.193771]\n",
      "epoch:22 step:20971 [D loss: 0.588244, acc.: 70.31%] [G loss: 1.485564]\n",
      "epoch:22 step:20972 [D loss: 0.531552, acc.: 71.09%] [G loss: 1.641817]\n",
      "epoch:22 step:20973 [D loss: 0.732286, acc.: 54.69%] [G loss: 1.012313]\n",
      "epoch:22 step:20974 [D loss: 0.508351, acc.: 78.91%] [G loss: 1.581194]\n",
      "epoch:22 step:20975 [D loss: 0.617899, acc.: 69.53%] [G loss: 1.173730]\n",
      "epoch:22 step:20976 [D loss: 0.556357, acc.: 71.88%] [G loss: 1.551206]\n",
      "epoch:22 step:20977 [D loss: 0.651765, acc.: 61.72%] [G loss: 1.175920]\n",
      "epoch:22 step:20978 [D loss: 0.520045, acc.: 70.31%] [G loss: 1.203838]\n",
      "epoch:22 step:20979 [D loss: 0.589479, acc.: 65.62%] [G loss: 1.411010]\n",
      "epoch:22 step:20980 [D loss: 0.595891, acc.: 70.31%] [G loss: 1.371621]\n",
      "epoch:22 step:20981 [D loss: 0.597806, acc.: 60.16%] [G loss: 1.319605]\n",
      "epoch:22 step:20982 [D loss: 0.532136, acc.: 74.22%] [G loss: 1.167410]\n",
      "epoch:22 step:20983 [D loss: 0.587418, acc.: 71.88%] [G loss: 1.223578]\n",
      "epoch:22 step:20984 [D loss: 0.540565, acc.: 75.78%] [G loss: 1.368176]\n",
      "epoch:22 step:20985 [D loss: 0.557166, acc.: 74.22%] [G loss: 1.352657]\n",
      "epoch:22 step:20986 [D loss: 0.498009, acc.: 76.56%] [G loss: 1.366518]\n",
      "epoch:22 step:20987 [D loss: 0.614472, acc.: 67.97%] [G loss: 1.380471]\n",
      "epoch:22 step:20988 [D loss: 0.708364, acc.: 59.38%] [G loss: 1.279421]\n",
      "epoch:22 step:20989 [D loss: 0.657558, acc.: 64.84%] [G loss: 0.987497]\n",
      "epoch:22 step:20990 [D loss: 0.587873, acc.: 71.09%] [G loss: 1.562479]\n",
      "epoch:22 step:20991 [D loss: 0.652810, acc.: 64.06%] [G loss: 1.517486]\n",
      "epoch:22 step:20992 [D loss: 0.517608, acc.: 74.22%] [G loss: 1.451917]\n",
      "epoch:22 step:20993 [D loss: 0.687761, acc.: 57.03%] [G loss: 1.196796]\n",
      "epoch:22 step:20994 [D loss: 0.661316, acc.: 61.72%] [G loss: 1.022544]\n",
      "epoch:22 step:20995 [D loss: 0.695543, acc.: 62.50%] [G loss: 1.240376]\n",
      "epoch:22 step:20996 [D loss: 0.447500, acc.: 79.69%] [G loss: 1.713104]\n",
      "epoch:22 step:20997 [D loss: 0.581338, acc.: 66.41%] [G loss: 1.158253]\n",
      "epoch:22 step:20998 [D loss: 0.587046, acc.: 73.44%] [G loss: 1.601380]\n",
      "epoch:22 step:20999 [D loss: 0.650561, acc.: 64.84%] [G loss: 0.964534]\n",
      "epoch:22 step:21000 [D loss: 0.510926, acc.: 75.78%] [G loss: 1.560351]\n",
      "##############\n",
      "[2.72841626 1.99026881 1.97613402 3.16088385 0.85577913 5.91075365\n",
      " 2.48534792 2.85862998 4.00190168 8.14868929]\n",
      "##########\n",
      "epoch:22 step:21001 [D loss: 0.550961, acc.: 72.66%] [G loss: 1.381742]\n",
      "epoch:22 step:21002 [D loss: 0.498140, acc.: 79.69%] [G loss: 1.613327]\n",
      "epoch:22 step:21003 [D loss: 0.471785, acc.: 77.34%] [G loss: 1.202441]\n",
      "epoch:22 step:21004 [D loss: 0.473747, acc.: 82.03%] [G loss: 1.160733]\n",
      "epoch:22 step:21005 [D loss: 0.568991, acc.: 74.22%] [G loss: 1.213749]\n",
      "epoch:22 step:21006 [D loss: 0.502335, acc.: 73.44%] [G loss: 0.764829]\n",
      "epoch:22 step:21007 [D loss: 0.567484, acc.: 72.66%] [G loss: 1.219727]\n",
      "epoch:22 step:21008 [D loss: 0.551225, acc.: 71.88%] [G loss: 1.185694]\n",
      "epoch:22 step:21009 [D loss: 0.616457, acc.: 70.31%] [G loss: 1.034032]\n",
      "epoch:22 step:21010 [D loss: 0.468594, acc.: 78.12%] [G loss: 1.404069]\n",
      "epoch:22 step:21011 [D loss: 0.655773, acc.: 57.81%] [G loss: 1.321120]\n",
      "epoch:22 step:21012 [D loss: 0.660970, acc.: 64.84%] [G loss: 1.295768]\n",
      "epoch:22 step:21013 [D loss: 0.680722, acc.: 64.84%] [G loss: 1.628544]\n",
      "epoch:22 step:21014 [D loss: 0.598497, acc.: 70.31%] [G loss: 1.249013]\n",
      "epoch:22 step:21015 [D loss: 0.574978, acc.: 66.41%] [G loss: 1.311336]\n",
      "epoch:22 step:21016 [D loss: 0.559190, acc.: 72.66%] [G loss: 1.318753]\n",
      "epoch:22 step:21017 [D loss: 0.505610, acc.: 76.56%] [G loss: 1.669660]\n",
      "epoch:22 step:21018 [D loss: 0.539058, acc.: 75.78%] [G loss: 1.408444]\n",
      "epoch:22 step:21019 [D loss: 0.480706, acc.: 75.78%] [G loss: 1.416982]\n",
      "epoch:22 step:21020 [D loss: 0.642034, acc.: 64.06%] [G loss: 1.502483]\n",
      "epoch:22 step:21021 [D loss: 0.698591, acc.: 53.91%] [G loss: 1.420212]\n",
      "epoch:22 step:21022 [D loss: 0.605355, acc.: 65.62%] [G loss: 1.582314]\n",
      "epoch:22 step:21023 [D loss: 0.607272, acc.: 66.41%] [G loss: 1.269330]\n",
      "epoch:22 step:21024 [D loss: 0.423698, acc.: 82.81%] [G loss: 1.494056]\n",
      "epoch:22 step:21025 [D loss: 0.476017, acc.: 79.69%] [G loss: 1.593686]\n",
      "epoch:22 step:21026 [D loss: 0.737476, acc.: 55.47%] [G loss: 1.267473]\n",
      "epoch:22 step:21027 [D loss: 0.487127, acc.: 75.00%] [G loss: 1.229260]\n",
      "epoch:22 step:21028 [D loss: 0.634883, acc.: 63.28%] [G loss: 1.101547]\n",
      "epoch:22 step:21029 [D loss: 0.512512, acc.: 75.00%] [G loss: 1.308208]\n",
      "epoch:22 step:21030 [D loss: 0.636378, acc.: 63.28%] [G loss: 1.292122]\n",
      "epoch:22 step:21031 [D loss: 0.672990, acc.: 59.38%] [G loss: 1.343624]\n",
      "epoch:22 step:21032 [D loss: 0.640544, acc.: 63.28%] [G loss: 1.078108]\n",
      "epoch:22 step:21033 [D loss: 0.557378, acc.: 73.44%] [G loss: 1.338343]\n",
      "epoch:22 step:21034 [D loss: 0.552623, acc.: 71.88%] [G loss: 1.282414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21035 [D loss: 0.681889, acc.: 56.25%] [G loss: 1.461144]\n",
      "epoch:22 step:21036 [D loss: 0.596554, acc.: 66.41%] [G loss: 1.320985]\n",
      "epoch:22 step:21037 [D loss: 0.662129, acc.: 66.41%] [G loss: 1.440292]\n",
      "epoch:22 step:21038 [D loss: 0.575254, acc.: 66.41%] [G loss: 1.910287]\n",
      "epoch:22 step:21039 [D loss: 0.485812, acc.: 72.66%] [G loss: 1.456252]\n",
      "epoch:22 step:21040 [D loss: 0.614311, acc.: 65.62%] [G loss: 1.325016]\n",
      "epoch:22 step:21041 [D loss: 0.617312, acc.: 71.09%] [G loss: 1.512208]\n",
      "epoch:22 step:21042 [D loss: 0.571476, acc.: 71.88%] [G loss: 1.211599]\n",
      "epoch:22 step:21043 [D loss: 0.568549, acc.: 74.22%] [G loss: 1.378070]\n",
      "epoch:22 step:21044 [D loss: 0.752245, acc.: 56.25%] [G loss: 1.121068]\n",
      "epoch:22 step:21045 [D loss: 0.466532, acc.: 79.69%] [G loss: 1.303012]\n",
      "epoch:22 step:21046 [D loss: 0.514521, acc.: 72.66%] [G loss: 1.154351]\n",
      "epoch:22 step:21047 [D loss: 0.424879, acc.: 82.81%] [G loss: 1.738476]\n",
      "epoch:22 step:21048 [D loss: 0.622433, acc.: 64.06%] [G loss: 1.269689]\n",
      "epoch:22 step:21049 [D loss: 0.492030, acc.: 76.56%] [G loss: 1.459048]\n",
      "epoch:22 step:21050 [D loss: 0.452505, acc.: 82.03%] [G loss: 1.637231]\n",
      "epoch:22 step:21051 [D loss: 0.556210, acc.: 70.31%] [G loss: 1.183524]\n",
      "epoch:22 step:21052 [D loss: 0.543256, acc.: 69.53%] [G loss: 0.896832]\n",
      "epoch:22 step:21053 [D loss: 0.540943, acc.: 72.66%] [G loss: 1.443853]\n",
      "epoch:22 step:21054 [D loss: 0.591020, acc.: 68.75%] [G loss: 1.884012]\n",
      "epoch:22 step:21055 [D loss: 0.490472, acc.: 76.56%] [G loss: 1.221594]\n",
      "epoch:22 step:21056 [D loss: 0.561049, acc.: 71.09%] [G loss: 1.248785]\n",
      "epoch:22 step:21057 [D loss: 0.491158, acc.: 76.56%] [G loss: 1.252115]\n",
      "epoch:22 step:21058 [D loss: 0.571456, acc.: 72.66%] [G loss: 1.213935]\n",
      "epoch:22 step:21059 [D loss: 0.428349, acc.: 79.69%] [G loss: 1.071503]\n",
      "epoch:22 step:21060 [D loss: 0.625545, acc.: 63.28%] [G loss: 1.562331]\n",
      "epoch:22 step:21061 [D loss: 0.610265, acc.: 66.41%] [G loss: 1.483917]\n",
      "epoch:22 step:21062 [D loss: 0.450849, acc.: 80.47%] [G loss: 1.432397]\n",
      "epoch:22 step:21063 [D loss: 0.507044, acc.: 78.12%] [G loss: 1.519459]\n",
      "epoch:22 step:21064 [D loss: 0.571872, acc.: 66.41%] [G loss: 1.340524]\n",
      "epoch:22 step:21065 [D loss: 0.339159, acc.: 88.28%] [G loss: 1.748359]\n",
      "epoch:22 step:21066 [D loss: 0.561798, acc.: 71.09%] [G loss: 1.043006]\n",
      "epoch:22 step:21067 [D loss: 0.390895, acc.: 83.59%] [G loss: 1.730564]\n",
      "epoch:22 step:21068 [D loss: 0.561968, acc.: 72.66%] [G loss: 1.379942]\n",
      "epoch:22 step:21069 [D loss: 0.452659, acc.: 84.38%] [G loss: 1.343051]\n",
      "epoch:22 step:21070 [D loss: 0.541820, acc.: 72.66%] [G loss: 1.253833]\n",
      "epoch:22 step:21071 [D loss: 0.557763, acc.: 75.00%] [G loss: 1.387815]\n",
      "epoch:22 step:21072 [D loss: 0.379839, acc.: 85.94%] [G loss: 1.470314]\n",
      "epoch:22 step:21073 [D loss: 0.514022, acc.: 76.56%] [G loss: 1.128803]\n",
      "epoch:22 step:21074 [D loss: 0.377188, acc.: 87.50%] [G loss: 1.391786]\n",
      "epoch:22 step:21075 [D loss: 0.586585, acc.: 75.00%] [G loss: 1.267546]\n",
      "epoch:22 step:21076 [D loss: 0.754481, acc.: 59.38%] [G loss: 1.078331]\n",
      "epoch:22 step:21077 [D loss: 0.519891, acc.: 75.78%] [G loss: 1.509712]\n",
      "epoch:22 step:21078 [D loss: 0.610615, acc.: 63.28%] [G loss: 1.449739]\n",
      "epoch:22 step:21079 [D loss: 0.684812, acc.: 59.38%] [G loss: 1.336579]\n",
      "epoch:22 step:21080 [D loss: 0.451138, acc.: 81.25%] [G loss: 1.361397]\n",
      "epoch:22 step:21081 [D loss: 0.403862, acc.: 83.59%] [G loss: 1.386951]\n",
      "epoch:22 step:21082 [D loss: 0.453756, acc.: 76.56%] [G loss: 1.657300]\n",
      "epoch:22 step:21083 [D loss: 0.463044, acc.: 79.69%] [G loss: 1.317960]\n",
      "epoch:22 step:21084 [D loss: 0.786442, acc.: 52.34%] [G loss: 0.844484]\n",
      "epoch:22 step:21085 [D loss: 0.601021, acc.: 64.84%] [G loss: 1.087401]\n",
      "epoch:22 step:21086 [D loss: 0.536574, acc.: 72.66%] [G loss: 1.210089]\n",
      "epoch:22 step:21087 [D loss: 0.514473, acc.: 71.09%] [G loss: 1.311627]\n",
      "epoch:22 step:21088 [D loss: 0.638295, acc.: 64.06%] [G loss: 1.429176]\n",
      "epoch:22 step:21089 [D loss: 0.467471, acc.: 82.03%] [G loss: 1.430432]\n",
      "epoch:22 step:21090 [D loss: 0.586964, acc.: 67.19%] [G loss: 1.485925]\n",
      "epoch:22 step:21091 [D loss: 0.515400, acc.: 70.31%] [G loss: 1.453594]\n",
      "epoch:22 step:21092 [D loss: 0.493020, acc.: 76.56%] [G loss: 1.304435]\n",
      "epoch:22 step:21093 [D loss: 0.432505, acc.: 85.16%] [G loss: 1.480999]\n",
      "epoch:22 step:21094 [D loss: 0.499062, acc.: 81.25%] [G loss: 1.513448]\n",
      "epoch:22 step:21095 [D loss: 0.681022, acc.: 57.81%] [G loss: 1.525207]\n",
      "epoch:22 step:21096 [D loss: 0.517419, acc.: 72.66%] [G loss: 1.170718]\n",
      "epoch:22 step:21097 [D loss: 0.627727, acc.: 64.06%] [G loss: 1.007179]\n",
      "epoch:22 step:21098 [D loss: 0.630567, acc.: 63.28%] [G loss: 0.946689]\n",
      "epoch:22 step:21099 [D loss: 0.482893, acc.: 78.91%] [G loss: 1.028402]\n",
      "epoch:22 step:21100 [D loss: 0.482154, acc.: 78.91%] [G loss: 1.428582]\n",
      "epoch:22 step:21101 [D loss: 0.513615, acc.: 74.22%] [G loss: 1.548383]\n",
      "epoch:22 step:21102 [D loss: 0.397684, acc.: 80.47%] [G loss: 1.775919]\n",
      "epoch:22 step:21103 [D loss: 0.583870, acc.: 68.75%] [G loss: 1.546468]\n",
      "epoch:22 step:21104 [D loss: 0.499904, acc.: 77.34%] [G loss: 1.523398]\n",
      "epoch:22 step:21105 [D loss: 0.687136, acc.: 61.72%] [G loss: 1.018535]\n",
      "epoch:22 step:21106 [D loss: 0.535050, acc.: 70.31%] [G loss: 1.410479]\n",
      "epoch:22 step:21107 [D loss: 0.652576, acc.: 59.38%] [G loss: 1.061900]\n",
      "epoch:22 step:21108 [D loss: 0.536015, acc.: 77.34%] [G loss: 1.464013]\n",
      "epoch:22 step:21109 [D loss: 0.587291, acc.: 67.19%] [G loss: 1.223294]\n",
      "epoch:22 step:21110 [D loss: 0.635478, acc.: 57.03%] [G loss: 1.583451]\n",
      "epoch:22 step:21111 [D loss: 0.486928, acc.: 79.69%] [G loss: 1.626390]\n",
      "epoch:22 step:21112 [D loss: 0.554760, acc.: 68.75%] [G loss: 1.019144]\n",
      "epoch:22 step:21113 [D loss: 0.632764, acc.: 66.41%] [G loss: 1.561438]\n",
      "epoch:22 step:21114 [D loss: 0.653192, acc.: 60.94%] [G loss: 1.501387]\n",
      "epoch:22 step:21115 [D loss: 0.501743, acc.: 79.69%] [G loss: 1.380432]\n",
      "epoch:22 step:21116 [D loss: 0.590027, acc.: 71.09%] [G loss: 1.582142]\n",
      "epoch:22 step:21117 [D loss: 0.486090, acc.: 79.69%] [G loss: 1.475763]\n",
      "epoch:22 step:21118 [D loss: 0.640841, acc.: 62.50%] [G loss: 1.218263]\n",
      "epoch:22 step:21119 [D loss: 0.546105, acc.: 72.66%] [G loss: 1.886036]\n",
      "epoch:22 step:21120 [D loss: 0.490912, acc.: 77.34%] [G loss: 1.555212]\n",
      "epoch:22 step:21121 [D loss: 0.631173, acc.: 61.72%] [G loss: 1.663377]\n",
      "epoch:22 step:21122 [D loss: 0.626709, acc.: 64.06%] [G loss: 1.144551]\n",
      "epoch:22 step:21123 [D loss: 0.457502, acc.: 78.91%] [G loss: 1.321074]\n",
      "epoch:22 step:21124 [D loss: 0.588994, acc.: 67.19%] [G loss: 1.344995]\n",
      "epoch:22 step:21125 [D loss: 0.512216, acc.: 78.12%] [G loss: 1.291866]\n",
      "epoch:22 step:21126 [D loss: 0.486310, acc.: 76.56%] [G loss: 1.109404]\n",
      "epoch:22 step:21127 [D loss: 0.400878, acc.: 83.59%] [G loss: 1.327026]\n",
      "epoch:22 step:21128 [D loss: 0.498160, acc.: 78.91%] [G loss: 1.131373]\n",
      "epoch:22 step:21129 [D loss: 0.485712, acc.: 81.25%] [G loss: 1.266048]\n",
      "epoch:22 step:21130 [D loss: 0.426423, acc.: 82.81%] [G loss: 1.210735]\n",
      "epoch:22 step:21131 [D loss: 0.693717, acc.: 57.03%] [G loss: 1.612328]\n",
      "epoch:22 step:21132 [D loss: 0.630190, acc.: 67.97%] [G loss: 1.384243]\n",
      "epoch:22 step:21133 [D loss: 0.530116, acc.: 72.66%] [G loss: 1.479807]\n",
      "epoch:22 step:21134 [D loss: 0.572354, acc.: 72.66%] [G loss: 1.312409]\n",
      "epoch:22 step:21135 [D loss: 0.667529, acc.: 58.59%] [G loss: 1.488936]\n",
      "epoch:22 step:21136 [D loss: 0.700649, acc.: 60.94%] [G loss: 1.884144]\n",
      "epoch:22 step:21137 [D loss: 0.575744, acc.: 68.75%] [G loss: 1.301912]\n",
      "epoch:22 step:21138 [D loss: 0.651230, acc.: 64.84%] [G loss: 1.188053]\n",
      "epoch:22 step:21139 [D loss: 0.621158, acc.: 61.72%] [G loss: 1.175144]\n",
      "epoch:22 step:21140 [D loss: 0.607786, acc.: 68.75%] [G loss: 1.266276]\n",
      "epoch:22 step:21141 [D loss: 0.571577, acc.: 69.53%] [G loss: 1.407396]\n",
      "epoch:22 step:21142 [D loss: 0.483570, acc.: 70.31%] [G loss: 1.502291]\n",
      "epoch:22 step:21143 [D loss: 0.538557, acc.: 72.66%] [G loss: 1.748334]\n",
      "epoch:22 step:21144 [D loss: 0.653883, acc.: 60.94%] [G loss: 1.856675]\n",
      "epoch:22 step:21145 [D loss: 0.768629, acc.: 60.16%] [G loss: 1.468171]\n",
      "epoch:22 step:21146 [D loss: 0.659091, acc.: 66.41%] [G loss: 1.314728]\n",
      "epoch:22 step:21147 [D loss: 0.669938, acc.: 64.84%] [G loss: 1.361139]\n",
      "epoch:22 step:21148 [D loss: 0.561619, acc.: 76.56%] [G loss: 1.650248]\n",
      "epoch:22 step:21149 [D loss: 0.511263, acc.: 74.22%] [G loss: 1.255776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21150 [D loss: 0.594697, acc.: 68.75%] [G loss: 1.248681]\n",
      "epoch:22 step:21151 [D loss: 0.559315, acc.: 70.31%] [G loss: 1.543599]\n",
      "epoch:22 step:21152 [D loss: 0.587319, acc.: 69.53%] [G loss: 1.440545]\n",
      "epoch:22 step:21153 [D loss: 0.560033, acc.: 73.44%] [G loss: 1.289589]\n",
      "epoch:22 step:21154 [D loss: 0.407790, acc.: 85.94%] [G loss: 1.629712]\n",
      "epoch:22 step:21155 [D loss: 0.448555, acc.: 78.91%] [G loss: 1.357672]\n",
      "epoch:22 step:21156 [D loss: 0.559659, acc.: 71.09%] [G loss: 1.225037]\n",
      "epoch:22 step:21157 [D loss: 0.549742, acc.: 67.19%] [G loss: 1.255442]\n",
      "epoch:22 step:21158 [D loss: 0.542244, acc.: 78.12%] [G loss: 1.657222]\n",
      "epoch:22 step:21159 [D loss: 0.678468, acc.: 67.97%] [G loss: 1.545927]\n",
      "epoch:22 step:21160 [D loss: 0.530903, acc.: 74.22%] [G loss: 1.195642]\n",
      "epoch:22 step:21161 [D loss: 0.545915, acc.: 69.53%] [G loss: 1.274684]\n",
      "epoch:22 step:21162 [D loss: 0.630402, acc.: 63.28%] [G loss: 1.513514]\n",
      "epoch:22 step:21163 [D loss: 0.677652, acc.: 58.59%] [G loss: 1.390353]\n",
      "epoch:22 step:21164 [D loss: 0.477107, acc.: 79.69%] [G loss: 1.369215]\n",
      "epoch:22 step:21165 [D loss: 0.705869, acc.: 57.03%] [G loss: 1.598995]\n",
      "epoch:22 step:21166 [D loss: 0.607307, acc.: 69.53%] [G loss: 1.406410]\n",
      "epoch:22 step:21167 [D loss: 0.567954, acc.: 67.97%] [G loss: 1.263738]\n",
      "epoch:22 step:21168 [D loss: 0.721953, acc.: 62.50%] [G loss: 1.239714]\n",
      "epoch:22 step:21169 [D loss: 0.530893, acc.: 70.31%] [G loss: 1.261624]\n",
      "epoch:22 step:21170 [D loss: 0.557283, acc.: 66.41%] [G loss: 1.658190]\n",
      "epoch:22 step:21171 [D loss: 0.490263, acc.: 76.56%] [G loss: 1.346976]\n",
      "epoch:22 step:21172 [D loss: 0.590768, acc.: 69.53%] [G loss: 1.442607]\n",
      "epoch:22 step:21173 [D loss: 0.508949, acc.: 72.66%] [G loss: 1.247221]\n",
      "epoch:22 step:21174 [D loss: 0.612328, acc.: 65.62%] [G loss: 1.133965]\n",
      "epoch:22 step:21175 [D loss: 0.554942, acc.: 68.75%] [G loss: 1.436386]\n",
      "epoch:22 step:21176 [D loss: 0.494293, acc.: 81.25%] [G loss: 1.524261]\n",
      "epoch:22 step:21177 [D loss: 0.479886, acc.: 74.22%] [G loss: 1.305180]\n",
      "epoch:22 step:21178 [D loss: 0.539802, acc.: 73.44%] [G loss: 1.290885]\n",
      "epoch:22 step:21179 [D loss: 0.462311, acc.: 78.91%] [G loss: 1.626260]\n",
      "epoch:22 step:21180 [D loss: 0.481007, acc.: 78.91%] [G loss: 1.459939]\n",
      "epoch:22 step:21181 [D loss: 0.482780, acc.: 80.47%] [G loss: 1.717779]\n",
      "epoch:22 step:21182 [D loss: 0.646643, acc.: 64.06%] [G loss: 1.389638]\n",
      "epoch:22 step:21183 [D loss: 0.626590, acc.: 65.62%] [G loss: 1.303827]\n",
      "epoch:22 step:21184 [D loss: 0.670504, acc.: 60.94%] [G loss: 1.401811]\n",
      "epoch:22 step:21185 [D loss: 0.432017, acc.: 79.69%] [G loss: 1.705629]\n",
      "epoch:22 step:21186 [D loss: 0.512853, acc.: 74.22%] [G loss: 1.432955]\n",
      "epoch:22 step:21187 [D loss: 0.554462, acc.: 72.66%] [G loss: 1.334244]\n",
      "epoch:22 step:21188 [D loss: 0.591606, acc.: 68.75%] [G loss: 1.243133]\n",
      "epoch:22 step:21189 [D loss: 0.619188, acc.: 62.50%] [G loss: 1.165841]\n",
      "epoch:22 step:21190 [D loss: 0.624468, acc.: 65.62%] [G loss: 1.529630]\n",
      "epoch:22 step:21191 [D loss: 0.557332, acc.: 67.97%] [G loss: 1.444580]\n",
      "epoch:22 step:21192 [D loss: 0.640580, acc.: 64.06%] [G loss: 1.107238]\n",
      "epoch:22 step:21193 [D loss: 0.544107, acc.: 71.09%] [G loss: 1.173394]\n",
      "epoch:22 step:21194 [D loss: 0.583636, acc.: 64.84%] [G loss: 1.179923]\n",
      "epoch:22 step:21195 [D loss: 0.603096, acc.: 66.41%] [G loss: 1.092977]\n",
      "epoch:22 step:21196 [D loss: 0.500481, acc.: 77.34%] [G loss: 1.497620]\n",
      "epoch:22 step:21197 [D loss: 0.537153, acc.: 72.66%] [G loss: 1.621173]\n",
      "epoch:22 step:21198 [D loss: 0.630707, acc.: 66.41%] [G loss: 1.201051]\n",
      "epoch:22 step:21199 [D loss: 0.585336, acc.: 69.53%] [G loss: 1.233646]\n",
      "epoch:22 step:21200 [D loss: 0.602606, acc.: 73.44%] [G loss: 1.557168]\n",
      "##############\n",
      "[2.71462479 2.24700702 1.94983064 3.13564928 1.31737521 6.35974265\n",
      " 2.31287082 2.84218877 3.95777348 8.14868929]\n",
      "##########\n",
      "epoch:22 step:21201 [D loss: 0.631122, acc.: 58.59%] [G loss: 0.992911]\n",
      "epoch:22 step:21202 [D loss: 0.629069, acc.: 69.53%] [G loss: 1.203113]\n",
      "epoch:22 step:21203 [D loss: 0.619459, acc.: 69.53%] [G loss: 0.934837]\n",
      "epoch:22 step:21204 [D loss: 0.650495, acc.: 62.50%] [G loss: 1.148011]\n",
      "epoch:22 step:21205 [D loss: 0.447497, acc.: 81.25%] [G loss: 1.414298]\n",
      "epoch:22 step:21206 [D loss: 0.495862, acc.: 77.34%] [G loss: 1.522346]\n",
      "epoch:22 step:21207 [D loss: 0.634761, acc.: 67.19%] [G loss: 1.228131]\n",
      "epoch:22 step:21208 [D loss: 0.605211, acc.: 69.53%] [G loss: 1.296819]\n",
      "epoch:22 step:21209 [D loss: 0.733522, acc.: 53.91%] [G loss: 1.600087]\n",
      "epoch:22 step:21210 [D loss: 0.481910, acc.: 79.69%] [G loss: 1.613186]\n",
      "epoch:22 step:21211 [D loss: 0.674999, acc.: 57.81%] [G loss: 1.339642]\n",
      "epoch:22 step:21212 [D loss: 0.585921, acc.: 67.97%] [G loss: 1.545394]\n",
      "epoch:22 step:21213 [D loss: 0.556522, acc.: 73.44%] [G loss: 1.816962]\n",
      "epoch:22 step:21214 [D loss: 0.424908, acc.: 82.81%] [G loss: 1.641512]\n",
      "epoch:22 step:21215 [D loss: 0.585784, acc.: 71.09%] [G loss: 1.526775]\n",
      "epoch:22 step:21216 [D loss: 0.608891, acc.: 67.97%] [G loss: 1.238617]\n",
      "epoch:22 step:21217 [D loss: 0.682361, acc.: 57.03%] [G loss: 1.143806]\n",
      "epoch:22 step:21218 [D loss: 0.656494, acc.: 64.06%] [G loss: 1.060235]\n",
      "epoch:22 step:21219 [D loss: 0.692934, acc.: 57.81%] [G loss: 1.319244]\n",
      "epoch:22 step:21220 [D loss: 0.548013, acc.: 72.66%] [G loss: 1.249016]\n",
      "epoch:22 step:21221 [D loss: 0.682272, acc.: 60.94%] [G loss: 1.288876]\n",
      "epoch:22 step:21222 [D loss: 0.727329, acc.: 56.25%] [G loss: 1.832545]\n",
      "epoch:22 step:21223 [D loss: 0.635973, acc.: 66.41%] [G loss: 1.881526]\n",
      "epoch:22 step:21224 [D loss: 0.496752, acc.: 79.69%] [G loss: 1.380585]\n",
      "epoch:22 step:21225 [D loss: 0.583615, acc.: 67.97%] [G loss: 1.227618]\n",
      "epoch:22 step:21226 [D loss: 0.759938, acc.: 58.59%] [G loss: 1.565324]\n",
      "epoch:22 step:21227 [D loss: 0.428231, acc.: 85.16%] [G loss: 1.452423]\n",
      "epoch:22 step:21228 [D loss: 0.502705, acc.: 78.91%] [G loss: 1.532892]\n",
      "epoch:22 step:21229 [D loss: 0.498920, acc.: 78.12%] [G loss: 1.353808]\n",
      "epoch:22 step:21230 [D loss: 0.508357, acc.: 78.12%] [G loss: 1.658598]\n",
      "epoch:22 step:21231 [D loss: 0.471720, acc.: 81.25%] [G loss: 1.362907]\n",
      "epoch:22 step:21232 [D loss: 0.353784, acc.: 87.50%] [G loss: 1.624734]\n",
      "epoch:22 step:21233 [D loss: 0.644116, acc.: 66.41%] [G loss: 1.653443]\n",
      "epoch:22 step:21234 [D loss: 0.496599, acc.: 76.56%] [G loss: 1.203399]\n",
      "epoch:22 step:21235 [D loss: 0.589125, acc.: 64.84%] [G loss: 1.399045]\n",
      "epoch:22 step:21236 [D loss: 0.700345, acc.: 53.91%] [G loss: 1.069508]\n",
      "epoch:22 step:21237 [D loss: 0.642997, acc.: 64.84%] [G loss: 1.216019]\n",
      "epoch:22 step:21238 [D loss: 0.571597, acc.: 73.44%] [G loss: 1.003813]\n",
      "epoch:22 step:21239 [D loss: 0.453652, acc.: 82.03%] [G loss: 1.593085]\n",
      "epoch:22 step:21240 [D loss: 0.654385, acc.: 66.41%] [G loss: 1.287318]\n",
      "epoch:22 step:21241 [D loss: 0.509267, acc.: 75.00%] [G loss: 1.301782]\n",
      "epoch:22 step:21242 [D loss: 0.676525, acc.: 61.72%] [G loss: 1.224150]\n",
      "epoch:22 step:21243 [D loss: 0.602898, acc.: 64.06%] [G loss: 1.392806]\n",
      "epoch:22 step:21244 [D loss: 0.457496, acc.: 79.69%] [G loss: 1.250660]\n",
      "epoch:22 step:21245 [D loss: 0.585292, acc.: 68.75%] [G loss: 1.252763]\n",
      "epoch:22 step:21246 [D loss: 0.408020, acc.: 82.03%] [G loss: 1.577933]\n",
      "epoch:22 step:21247 [D loss: 0.511470, acc.: 75.00%] [G loss: 1.212081]\n",
      "epoch:22 step:21248 [D loss: 0.635723, acc.: 65.62%] [G loss: 1.580491]\n",
      "epoch:22 step:21249 [D loss: 0.696348, acc.: 60.94%] [G loss: 1.369084]\n",
      "epoch:22 step:21250 [D loss: 0.486997, acc.: 78.12%] [G loss: 1.568701]\n",
      "epoch:22 step:21251 [D loss: 0.665388, acc.: 60.94%] [G loss: 1.079421]\n",
      "epoch:22 step:21252 [D loss: 0.604766, acc.: 71.09%] [G loss: 1.126770]\n",
      "epoch:22 step:21253 [D loss: 0.712673, acc.: 53.91%] [G loss: 1.256597]\n",
      "epoch:22 step:21254 [D loss: 0.551018, acc.: 70.31%] [G loss: 1.241964]\n",
      "epoch:22 step:21255 [D loss: 0.587886, acc.: 70.31%] [G loss: 1.788236]\n",
      "epoch:22 step:21256 [D loss: 0.645465, acc.: 66.41%] [G loss: 1.746910]\n",
      "epoch:22 step:21257 [D loss: 0.755575, acc.: 53.91%] [G loss: 1.256221]\n",
      "epoch:22 step:21258 [D loss: 0.646736, acc.: 63.28%] [G loss: 1.364637]\n",
      "epoch:22 step:21259 [D loss: 0.433942, acc.: 86.72%] [G loss: 1.688180]\n",
      "epoch:22 step:21260 [D loss: 0.534933, acc.: 77.34%] [G loss: 1.139392]\n",
      "epoch:22 step:21261 [D loss: 0.492578, acc.: 75.78%] [G loss: 1.499376]\n",
      "epoch:22 step:21262 [D loss: 0.593110, acc.: 67.97%] [G loss: 1.314413]\n",
      "epoch:22 step:21263 [D loss: 0.565814, acc.: 70.31%] [G loss: 1.319186]\n",
      "epoch:22 step:21264 [D loss: 0.494351, acc.: 75.78%] [G loss: 1.301525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21265 [D loss: 0.354381, acc.: 89.84%] [G loss: 1.617019]\n",
      "epoch:22 step:21266 [D loss: 0.575608, acc.: 66.41%] [G loss: 1.506251]\n",
      "epoch:22 step:21267 [D loss: 0.661954, acc.: 63.28%] [G loss: 1.443894]\n",
      "epoch:22 step:21268 [D loss: 0.658765, acc.: 59.38%] [G loss: 1.087809]\n",
      "epoch:22 step:21269 [D loss: 0.585284, acc.: 69.53%] [G loss: 1.502971]\n",
      "epoch:22 step:21270 [D loss: 0.474626, acc.: 80.47%] [G loss: 1.528214]\n",
      "epoch:22 step:21271 [D loss: 0.658347, acc.: 62.50%] [G loss: 1.453248]\n",
      "epoch:22 step:21272 [D loss: 0.639619, acc.: 63.28%] [G loss: 1.391188]\n",
      "epoch:22 step:21273 [D loss: 0.506761, acc.: 78.91%] [G loss: 1.502621]\n",
      "epoch:22 step:21274 [D loss: 0.502127, acc.: 79.69%] [G loss: 1.235063]\n",
      "epoch:22 step:21275 [D loss: 0.626976, acc.: 63.28%] [G loss: 1.326007]\n",
      "epoch:22 step:21276 [D loss: 0.661797, acc.: 61.72%] [G loss: 1.612711]\n",
      "epoch:22 step:21277 [D loss: 0.391171, acc.: 85.94%] [G loss: 1.442184]\n",
      "epoch:22 step:21278 [D loss: 0.701781, acc.: 63.28%] [G loss: 1.458857]\n",
      "epoch:22 step:21279 [D loss: 0.688609, acc.: 59.38%] [G loss: 1.258640]\n",
      "epoch:22 step:21280 [D loss: 0.647906, acc.: 58.59%] [G loss: 1.304160]\n",
      "epoch:22 step:21281 [D loss: 0.657661, acc.: 62.50%] [G loss: 1.151993]\n",
      "epoch:22 step:21282 [D loss: 0.840040, acc.: 48.44%] [G loss: 1.203743]\n",
      "epoch:22 step:21283 [D loss: 0.476974, acc.: 77.34%] [G loss: 1.207650]\n",
      "epoch:22 step:21284 [D loss: 0.507260, acc.: 75.00%] [G loss: 1.055859]\n",
      "epoch:22 step:21285 [D loss: 0.463122, acc.: 80.47%] [G loss: 1.371709]\n",
      "epoch:22 step:21286 [D loss: 0.555342, acc.: 72.66%] [G loss: 1.448072]\n",
      "epoch:22 step:21287 [D loss: 0.556037, acc.: 67.19%] [G loss: 1.402733]\n",
      "epoch:22 step:21288 [D loss: 0.560361, acc.: 70.31%] [G loss: 1.373254]\n",
      "epoch:22 step:21289 [D loss: 0.692368, acc.: 59.38%] [G loss: 1.069402]\n",
      "epoch:22 step:21290 [D loss: 0.486320, acc.: 76.56%] [G loss: 1.643440]\n",
      "epoch:22 step:21291 [D loss: 0.665061, acc.: 64.06%] [G loss: 0.928799]\n",
      "epoch:22 step:21292 [D loss: 0.496798, acc.: 75.00%] [G loss: 1.363740]\n",
      "epoch:22 step:21293 [D loss: 0.551312, acc.: 71.09%] [G loss: 1.060099]\n",
      "epoch:22 step:21294 [D loss: 0.690873, acc.: 59.38%] [G loss: 0.868213]\n",
      "epoch:22 step:21295 [D loss: 0.545580, acc.: 71.88%] [G loss: 1.283363]\n",
      "epoch:22 step:21296 [D loss: 0.503274, acc.: 74.22%] [G loss: 1.378500]\n",
      "epoch:22 step:21297 [D loss: 0.666529, acc.: 64.06%] [G loss: 1.407316]\n",
      "epoch:22 step:21298 [D loss: 0.462073, acc.: 78.12%] [G loss: 1.114183]\n",
      "epoch:22 step:21299 [D loss: 0.608084, acc.: 64.06%] [G loss: 1.293497]\n",
      "epoch:22 step:21300 [D loss: 0.595279, acc.: 65.62%] [G loss: 1.412744]\n",
      "epoch:22 step:21301 [D loss: 0.675230, acc.: 60.94%] [G loss: 1.141973]\n",
      "epoch:22 step:21302 [D loss: 0.419713, acc.: 82.81%] [G loss: 1.489842]\n",
      "epoch:22 step:21303 [D loss: 0.639487, acc.: 62.50%] [G loss: 1.605534]\n",
      "epoch:22 step:21304 [D loss: 0.678739, acc.: 58.59%] [G loss: 1.106924]\n",
      "epoch:22 step:21305 [D loss: 0.632236, acc.: 60.16%] [G loss: 1.459307]\n",
      "epoch:22 step:21306 [D loss: 0.483465, acc.: 78.12%] [G loss: 1.251961]\n",
      "epoch:22 step:21307 [D loss: 0.563219, acc.: 71.88%] [G loss: 1.712352]\n",
      "epoch:22 step:21308 [D loss: 0.500073, acc.: 73.44%] [G loss: 1.615386]\n",
      "epoch:22 step:21309 [D loss: 0.489290, acc.: 77.34%] [G loss: 1.074954]\n",
      "epoch:22 step:21310 [D loss: 0.615358, acc.: 68.75%] [G loss: 1.650840]\n",
      "epoch:22 step:21311 [D loss: 0.534396, acc.: 73.44%] [G loss: 1.383235]\n",
      "epoch:22 step:21312 [D loss: 0.584501, acc.: 64.84%] [G loss: 1.339245]\n",
      "epoch:22 step:21313 [D loss: 0.470307, acc.: 78.12%] [G loss: 1.286431]\n",
      "epoch:22 step:21314 [D loss: 0.591171, acc.: 67.97%] [G loss: 0.971033]\n",
      "epoch:22 step:21315 [D loss: 0.386308, acc.: 85.16%] [G loss: 1.615232]\n",
      "epoch:22 step:21316 [D loss: 0.392685, acc.: 87.50%] [G loss: 1.573186]\n",
      "epoch:22 step:21317 [D loss: 0.684885, acc.: 60.16%] [G loss: 1.148096]\n",
      "epoch:22 step:21318 [D loss: 0.712697, acc.: 61.72%] [G loss: 1.127880]\n",
      "epoch:22 step:21319 [D loss: 0.390844, acc.: 85.94%] [G loss: 1.221266]\n",
      "epoch:22 step:21320 [D loss: 0.548528, acc.: 72.66%] [G loss: 1.269563]\n",
      "epoch:22 step:21321 [D loss: 0.394692, acc.: 85.94%] [G loss: 1.410337]\n",
      "epoch:22 step:21322 [D loss: 0.427262, acc.: 85.16%] [G loss: 1.230945]\n",
      "epoch:22 step:21323 [D loss: 0.614689, acc.: 62.50%] [G loss: 1.333563]\n",
      "epoch:22 step:21324 [D loss: 0.639460, acc.: 66.41%] [G loss: 1.076861]\n",
      "epoch:22 step:21325 [D loss: 0.521705, acc.: 75.00%] [G loss: 1.447393]\n",
      "epoch:22 step:21326 [D loss: 0.599652, acc.: 68.75%] [G loss: 1.493212]\n",
      "epoch:22 step:21327 [D loss: 0.534564, acc.: 68.75%] [G loss: 1.236730]\n",
      "epoch:22 step:21328 [D loss: 0.753617, acc.: 53.12%] [G loss: 0.930609]\n",
      "epoch:22 step:21329 [D loss: 0.634664, acc.: 68.75%] [G loss: 1.287058]\n",
      "epoch:22 step:21330 [D loss: 0.476346, acc.: 78.91%] [G loss: 1.630605]\n",
      "epoch:22 step:21331 [D loss: 0.537262, acc.: 71.88%] [G loss: 1.610898]\n",
      "epoch:22 step:21332 [D loss: 0.338755, acc.: 88.28%] [G loss: 1.647611]\n",
      "epoch:22 step:21333 [D loss: 0.462625, acc.: 79.69%] [G loss: 1.723771]\n",
      "epoch:22 step:21334 [D loss: 0.534576, acc.: 76.56%] [G loss: 1.286513]\n",
      "epoch:22 step:21335 [D loss: 0.595971, acc.: 65.62%] [G loss: 1.317117]\n",
      "epoch:22 step:21336 [D loss: 0.459269, acc.: 77.34%] [G loss: 1.580040]\n",
      "epoch:22 step:21337 [D loss: 0.675621, acc.: 61.72%] [G loss: 1.330479]\n",
      "epoch:22 step:21338 [D loss: 0.508938, acc.: 75.00%] [G loss: 1.185413]\n",
      "epoch:22 step:21339 [D loss: 0.446806, acc.: 79.69%] [G loss: 1.858662]\n",
      "epoch:22 step:21340 [D loss: 0.559081, acc.: 71.09%] [G loss: 1.572728]\n",
      "epoch:22 step:21341 [D loss: 0.609398, acc.: 69.53%] [G loss: 1.213555]\n",
      "epoch:22 step:21342 [D loss: 0.585086, acc.: 69.53%] [G loss: 1.186505]\n",
      "epoch:22 step:21343 [D loss: 0.524937, acc.: 75.00%] [G loss: 1.400376]\n",
      "epoch:22 step:21344 [D loss: 0.643369, acc.: 63.28%] [G loss: 1.328249]\n",
      "epoch:22 step:21345 [D loss: 0.587469, acc.: 71.09%] [G loss: 1.304448]\n",
      "epoch:22 step:21346 [D loss: 0.531901, acc.: 74.22%] [G loss: 0.915472]\n",
      "epoch:22 step:21347 [D loss: 0.495117, acc.: 79.69%] [G loss: 1.421183]\n",
      "epoch:22 step:21348 [D loss: 0.651701, acc.: 60.94%] [G loss: 0.783723]\n",
      "epoch:22 step:21349 [D loss: 0.688418, acc.: 62.50%] [G loss: 1.472591]\n",
      "epoch:22 step:21350 [D loss: 0.452548, acc.: 78.91%] [G loss: 1.531633]\n",
      "epoch:22 step:21351 [D loss: 0.617569, acc.: 60.94%] [G loss: 1.209972]\n",
      "epoch:22 step:21352 [D loss: 0.632841, acc.: 66.41%] [G loss: 1.173090]\n",
      "epoch:22 step:21353 [D loss: 0.438149, acc.: 81.25%] [G loss: 1.686545]\n",
      "epoch:22 step:21354 [D loss: 0.626301, acc.: 68.75%] [G loss: 1.233662]\n",
      "epoch:22 step:21355 [D loss: 0.571510, acc.: 69.53%] [G loss: 1.008310]\n",
      "epoch:22 step:21356 [D loss: 0.426653, acc.: 84.38%] [G loss: 1.088606]\n",
      "epoch:22 step:21357 [D loss: 0.594351, acc.: 64.84%] [G loss: 1.386978]\n",
      "epoch:22 step:21358 [D loss: 0.499983, acc.: 74.22%] [G loss: 1.216581]\n",
      "epoch:22 step:21359 [D loss: 0.441918, acc.: 75.78%] [G loss: 1.231208]\n",
      "epoch:22 step:21360 [D loss: 0.538664, acc.: 71.88%] [G loss: 1.127290]\n",
      "epoch:22 step:21361 [D loss: 0.367686, acc.: 88.28%] [G loss: 1.640713]\n",
      "epoch:22 step:21362 [D loss: 0.459581, acc.: 82.81%] [G loss: 1.356063]\n",
      "epoch:22 step:21363 [D loss: 0.584488, acc.: 66.41%] [G loss: 1.595527]\n",
      "epoch:22 step:21364 [D loss: 0.494198, acc.: 78.91%] [G loss: 1.366046]\n",
      "epoch:22 step:21365 [D loss: 0.420729, acc.: 85.94%] [G loss: 1.620858]\n",
      "epoch:22 step:21366 [D loss: 0.521582, acc.: 73.44%] [G loss: 1.186639]\n",
      "epoch:22 step:21367 [D loss: 0.438505, acc.: 80.47%] [G loss: 1.565549]\n",
      "epoch:22 step:21368 [D loss: 0.587597, acc.: 69.53%] [G loss: 1.412513]\n",
      "epoch:22 step:21369 [D loss: 0.699301, acc.: 60.94%] [G loss: 0.809944]\n",
      "epoch:22 step:21370 [D loss: 0.541502, acc.: 71.88%] [G loss: 1.611557]\n",
      "epoch:22 step:21371 [D loss: 0.572643, acc.: 73.44%] [G loss: 1.284284]\n",
      "epoch:22 step:21372 [D loss: 0.506189, acc.: 71.88%] [G loss: 1.561986]\n",
      "epoch:22 step:21373 [D loss: 0.541107, acc.: 73.44%] [G loss: 1.762830]\n",
      "epoch:22 step:21374 [D loss: 0.522285, acc.: 71.88%] [G loss: 1.633137]\n",
      "epoch:22 step:21375 [D loss: 0.594088, acc.: 68.75%] [G loss: 1.463515]\n",
      "epoch:22 step:21376 [D loss: 0.643908, acc.: 64.06%] [G loss: 1.144090]\n",
      "epoch:22 step:21377 [D loss: 0.472670, acc.: 78.91%] [G loss: 1.127314]\n",
      "epoch:22 step:21378 [D loss: 0.650233, acc.: 64.06%] [G loss: 1.402707]\n",
      "epoch:22 step:21379 [D loss: 0.482046, acc.: 73.44%] [G loss: 1.140278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21380 [D loss: 0.594049, acc.: 67.19%] [G loss: 1.172290]\n",
      "epoch:22 step:21381 [D loss: 0.481840, acc.: 80.47%] [G loss: 0.839658]\n",
      "epoch:22 step:21382 [D loss: 0.677693, acc.: 62.50%] [G loss: 1.596257]\n",
      "epoch:22 step:21383 [D loss: 0.498668, acc.: 77.34%] [G loss: 1.652254]\n",
      "epoch:22 step:21384 [D loss: 0.749070, acc.: 56.25%] [G loss: 1.092842]\n",
      "epoch:22 step:21385 [D loss: 0.528897, acc.: 71.88%] [G loss: 1.279204]\n",
      "epoch:22 step:21386 [D loss: 0.596843, acc.: 70.31%] [G loss: 1.383763]\n",
      "epoch:22 step:21387 [D loss: 0.538064, acc.: 74.22%] [G loss: 1.080115]\n",
      "epoch:22 step:21388 [D loss: 0.572858, acc.: 66.41%] [G loss: 1.289091]\n",
      "epoch:22 step:21389 [D loss: 0.477985, acc.: 82.03%] [G loss: 1.426114]\n",
      "epoch:22 step:21390 [D loss: 0.484276, acc.: 76.56%] [G loss: 1.807274]\n",
      "epoch:22 step:21391 [D loss: 0.483418, acc.: 79.69%] [G loss: 1.359193]\n",
      "epoch:22 step:21392 [D loss: 0.565088, acc.: 67.19%] [G loss: 1.307715]\n",
      "epoch:22 step:21393 [D loss: 0.436802, acc.: 84.38%] [G loss: 1.530389]\n",
      "epoch:22 step:21394 [D loss: 0.597030, acc.: 67.19%] [G loss: 1.725099]\n",
      "epoch:22 step:21395 [D loss: 0.513709, acc.: 76.56%] [G loss: 1.200054]\n",
      "epoch:22 step:21396 [D loss: 0.392300, acc.: 86.72%] [G loss: 1.440331]\n",
      "epoch:22 step:21397 [D loss: 0.655573, acc.: 61.72%] [G loss: 1.397798]\n",
      "epoch:22 step:21398 [D loss: 0.534111, acc.: 73.44%] [G loss: 1.424781]\n",
      "epoch:22 step:21399 [D loss: 0.403778, acc.: 85.16%] [G loss: 1.423173]\n",
      "epoch:22 step:21400 [D loss: 0.659126, acc.: 67.19%] [G loss: 1.329660]\n",
      "##############\n",
      "[2.78160663 2.03531925 1.74738848 2.90846478 0.88317333 5.64374826\n",
      " 2.03518617 2.9155525  3.84412056 6.03274538]\n",
      "##########\n",
      "epoch:22 step:21401 [D loss: 0.506027, acc.: 76.56%] [G loss: 1.191616]\n",
      "epoch:22 step:21402 [D loss: 0.540128, acc.: 71.09%] [G loss: 1.373681]\n",
      "epoch:22 step:21403 [D loss: 0.529175, acc.: 70.31%] [G loss: 1.244095]\n",
      "epoch:22 step:21404 [D loss: 0.548750, acc.: 71.88%] [G loss: 1.924535]\n",
      "epoch:22 step:21405 [D loss: 0.473432, acc.: 78.12%] [G loss: 1.201197]\n",
      "epoch:22 step:21406 [D loss: 0.625040, acc.: 64.84%] [G loss: 1.295536]\n",
      "epoch:22 step:21407 [D loss: 0.534916, acc.: 74.22%] [G loss: 1.524493]\n",
      "epoch:22 step:21408 [D loss: 0.523572, acc.: 74.22%] [G loss: 1.364247]\n",
      "epoch:22 step:21409 [D loss: 0.524307, acc.: 75.78%] [G loss: 1.394871]\n",
      "epoch:22 step:21410 [D loss: 0.476558, acc.: 79.69%] [G loss: 1.393180]\n",
      "epoch:22 step:21411 [D loss: 0.604312, acc.: 69.53%] [G loss: 1.281894]\n",
      "epoch:22 step:21412 [D loss: 0.681883, acc.: 57.03%] [G loss: 1.131394]\n",
      "epoch:22 step:21413 [D loss: 0.466146, acc.: 81.25%] [G loss: 1.462477]\n",
      "epoch:22 step:21414 [D loss: 0.612059, acc.: 65.62%] [G loss: 1.590365]\n",
      "epoch:22 step:21415 [D loss: 0.643394, acc.: 65.62%] [G loss: 1.267532]\n",
      "epoch:22 step:21416 [D loss: 0.665892, acc.: 65.62%] [G loss: 1.361498]\n",
      "epoch:22 step:21417 [D loss: 0.509495, acc.: 73.44%] [G loss: 1.022278]\n",
      "epoch:22 step:21418 [D loss: 0.548809, acc.: 71.88%] [G loss: 1.083813]\n",
      "epoch:22 step:21419 [D loss: 0.653106, acc.: 61.72%] [G loss: 1.408669]\n",
      "epoch:22 step:21420 [D loss: 0.692524, acc.: 59.38%] [G loss: 1.589687]\n",
      "epoch:22 step:21421 [D loss: 0.539337, acc.: 71.88%] [G loss: 1.599016]\n",
      "epoch:22 step:21422 [D loss: 0.505739, acc.: 73.44%] [G loss: 1.724704]\n",
      "epoch:22 step:21423 [D loss: 0.611903, acc.: 62.50%] [G loss: 1.216879]\n",
      "epoch:22 step:21424 [D loss: 0.416923, acc.: 85.94%] [G loss: 1.410463]\n",
      "epoch:22 step:21425 [D loss: 0.505103, acc.: 71.88%] [G loss: 1.545879]\n",
      "epoch:22 step:21426 [D loss: 0.566128, acc.: 70.31%] [G loss: 1.521548]\n",
      "epoch:22 step:21427 [D loss: 0.578533, acc.: 72.66%] [G loss: 1.419216]\n",
      "epoch:22 step:21428 [D loss: 0.554091, acc.: 69.53%] [G loss: 1.193855]\n",
      "epoch:22 step:21429 [D loss: 0.590170, acc.: 65.62%] [G loss: 1.646905]\n",
      "epoch:22 step:21430 [D loss: 0.446683, acc.: 79.69%] [G loss: 1.084532]\n",
      "epoch:22 step:21431 [D loss: 0.531724, acc.: 72.66%] [G loss: 1.648599]\n",
      "epoch:22 step:21432 [D loss: 0.567396, acc.: 68.75%] [G loss: 1.314264]\n",
      "epoch:22 step:21433 [D loss: 0.526720, acc.: 72.66%] [G loss: 1.476331]\n",
      "epoch:22 step:21434 [D loss: 0.601795, acc.: 65.62%] [G loss: 1.154701]\n",
      "epoch:22 step:21435 [D loss: 0.541435, acc.: 74.22%] [G loss: 1.281229]\n",
      "epoch:22 step:21436 [D loss: 0.660948, acc.: 66.41%] [G loss: 1.168724]\n",
      "epoch:22 step:21437 [D loss: 0.591537, acc.: 73.44%] [G loss: 1.439346]\n",
      "epoch:22 step:21438 [D loss: 0.503407, acc.: 81.25%] [G loss: 1.244674]\n",
      "epoch:22 step:21439 [D loss: 0.383924, acc.: 85.94%] [G loss: 1.558210]\n",
      "epoch:22 step:21440 [D loss: 0.545154, acc.: 70.31%] [G loss: 1.535069]\n",
      "epoch:22 step:21441 [D loss: 0.538949, acc.: 68.75%] [G loss: 1.197272]\n",
      "epoch:22 step:21442 [D loss: 0.424296, acc.: 82.81%] [G loss: 1.587137]\n",
      "epoch:22 step:21443 [D loss: 0.597757, acc.: 68.75%] [G loss: 1.427902]\n",
      "epoch:22 step:21444 [D loss: 0.485999, acc.: 75.78%] [G loss: 1.441121]\n",
      "epoch:22 step:21445 [D loss: 0.645426, acc.: 63.28%] [G loss: 1.241482]\n",
      "epoch:22 step:21446 [D loss: 0.511852, acc.: 75.78%] [G loss: 1.761635]\n",
      "epoch:22 step:21447 [D loss: 0.576152, acc.: 70.31%] [G loss: 1.107360]\n",
      "epoch:22 step:21448 [D loss: 0.617497, acc.: 63.28%] [G loss: 1.102101]\n",
      "epoch:22 step:21449 [D loss: 0.554908, acc.: 71.88%] [G loss: 1.540790]\n",
      "epoch:22 step:21450 [D loss: 0.463722, acc.: 81.25%] [G loss: 1.454573]\n",
      "epoch:22 step:21451 [D loss: 0.811698, acc.: 53.12%] [G loss: 1.041083]\n",
      "epoch:22 step:21452 [D loss: 0.509462, acc.: 75.00%] [G loss: 1.273053]\n",
      "epoch:22 step:21453 [D loss: 0.477558, acc.: 80.47%] [G loss: 1.306419]\n",
      "epoch:22 step:21454 [D loss: 0.560846, acc.: 72.66%] [G loss: 1.121129]\n",
      "epoch:22 step:21455 [D loss: 0.475975, acc.: 78.91%] [G loss: 1.370810]\n",
      "epoch:22 step:21456 [D loss: 0.505693, acc.: 77.34%] [G loss: 1.178554]\n",
      "epoch:22 step:21457 [D loss: 0.616739, acc.: 66.41%] [G loss: 1.534530]\n",
      "epoch:22 step:21458 [D loss: 0.814990, acc.: 46.88%] [G loss: 1.467916]\n",
      "epoch:22 step:21459 [D loss: 0.507571, acc.: 75.78%] [G loss: 1.230649]\n",
      "epoch:22 step:21460 [D loss: 0.608156, acc.: 65.62%] [G loss: 1.633298]\n",
      "epoch:22 step:21461 [D loss: 0.700057, acc.: 56.25%] [G loss: 1.519616]\n",
      "epoch:22 step:21462 [D loss: 0.636345, acc.: 60.94%] [G loss: 1.331536]\n",
      "epoch:22 step:21463 [D loss: 0.707072, acc.: 50.78%] [G loss: 1.063730]\n",
      "epoch:22 step:21464 [D loss: 0.534762, acc.: 71.09%] [G loss: 1.404034]\n",
      "epoch:22 step:21465 [D loss: 0.692807, acc.: 57.81%] [G loss: 1.795670]\n",
      "epoch:22 step:21466 [D loss: 0.510207, acc.: 78.12%] [G loss: 1.394127]\n",
      "epoch:22 step:21467 [D loss: 0.488774, acc.: 76.56%] [G loss: 1.230387]\n",
      "epoch:22 step:21468 [D loss: 0.564945, acc.: 71.09%] [G loss: 1.120797]\n",
      "epoch:22 step:21469 [D loss: 0.615638, acc.: 67.97%] [G loss: 1.136897]\n",
      "epoch:22 step:21470 [D loss: 0.342353, acc.: 91.41%] [G loss: 1.646582]\n",
      "epoch:22 step:21471 [D loss: 0.590072, acc.: 67.19%] [G loss: 1.574955]\n",
      "epoch:22 step:21472 [D loss: 0.507147, acc.: 74.22%] [G loss: 1.665915]\n",
      "epoch:22 step:21473 [D loss: 0.625375, acc.: 69.53%] [G loss: 1.327602]\n",
      "epoch:22 step:21474 [D loss: 0.598168, acc.: 64.84%] [G loss: 1.992186]\n",
      "epoch:22 step:21475 [D loss: 0.443261, acc.: 78.91%] [G loss: 1.076406]\n",
      "epoch:22 step:21476 [D loss: 0.649793, acc.: 62.50%] [G loss: 1.724524]\n",
      "epoch:22 step:21477 [D loss: 0.467998, acc.: 82.81%] [G loss: 1.345053]\n",
      "epoch:22 step:21478 [D loss: 0.584062, acc.: 71.88%] [G loss: 1.336870]\n",
      "epoch:22 step:21479 [D loss: 0.512491, acc.: 72.66%] [G loss: 1.288225]\n",
      "epoch:22 step:21480 [D loss: 0.414486, acc.: 89.06%] [G loss: 0.999052]\n",
      "epoch:22 step:21481 [D loss: 0.582141, acc.: 67.97%] [G loss: 1.286260]\n",
      "epoch:22 step:21482 [D loss: 0.685084, acc.: 64.84%] [G loss: 1.403668]\n",
      "epoch:22 step:21483 [D loss: 0.404738, acc.: 85.16%] [G loss: 1.320715]\n",
      "epoch:22 step:21484 [D loss: 0.572928, acc.: 72.66%] [G loss: 1.047900]\n",
      "epoch:22 step:21485 [D loss: 0.607074, acc.: 65.62%] [G loss: 1.536892]\n",
      "epoch:22 step:21486 [D loss: 0.481172, acc.: 80.47%] [G loss: 0.965460]\n",
      "epoch:22 step:21487 [D loss: 0.558863, acc.: 70.31%] [G loss: 1.430604]\n",
      "epoch:22 step:21488 [D loss: 0.522132, acc.: 71.09%] [G loss: 1.379707]\n",
      "epoch:22 step:21489 [D loss: 0.752972, acc.: 58.59%] [G loss: 1.259287]\n",
      "epoch:22 step:21490 [D loss: 0.688966, acc.: 58.59%] [G loss: 1.304273]\n",
      "epoch:22 step:21491 [D loss: 0.599949, acc.: 70.31%] [G loss: 1.406713]\n",
      "epoch:22 step:21492 [D loss: 0.492217, acc.: 75.78%] [G loss: 1.418168]\n",
      "epoch:22 step:21493 [D loss: 0.467863, acc.: 81.25%] [G loss: 1.144545]\n",
      "epoch:22 step:21494 [D loss: 0.431415, acc.: 85.94%] [G loss: 1.658247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21495 [D loss: 0.669858, acc.: 63.28%] [G loss: 1.474230]\n",
      "epoch:22 step:21496 [D loss: 0.483279, acc.: 83.59%] [G loss: 1.731041]\n",
      "epoch:22 step:21497 [D loss: 0.625411, acc.: 62.50%] [G loss: 1.083515]\n",
      "epoch:22 step:21498 [D loss: 0.518910, acc.: 77.34%] [G loss: 1.277251]\n",
      "epoch:22 step:21499 [D loss: 0.591047, acc.: 67.97%] [G loss: 1.158285]\n",
      "epoch:22 step:21500 [D loss: 0.609355, acc.: 65.62%] [G loss: 1.398643]\n",
      "epoch:22 step:21501 [D loss: 0.566791, acc.: 75.78%] [G loss: 1.489799]\n",
      "epoch:22 step:21502 [D loss: 0.689200, acc.: 58.59%] [G loss: 1.014279]\n",
      "epoch:22 step:21503 [D loss: 0.689649, acc.: 58.59%] [G loss: 1.263715]\n",
      "epoch:22 step:21504 [D loss: 0.573009, acc.: 66.41%] [G loss: 1.351183]\n",
      "epoch:22 step:21505 [D loss: 0.792083, acc.: 55.47%] [G loss: 1.269791]\n",
      "epoch:22 step:21506 [D loss: 0.523382, acc.: 80.47%] [G loss: 1.180408]\n",
      "epoch:22 step:21507 [D loss: 0.531416, acc.: 75.78%] [G loss: 1.298908]\n",
      "epoch:22 step:21508 [D loss: 0.498985, acc.: 76.56%] [G loss: 1.326691]\n",
      "epoch:22 step:21509 [D loss: 0.613940, acc.: 63.28%] [G loss: 1.246606]\n",
      "epoch:22 step:21510 [D loss: 0.493832, acc.: 74.22%] [G loss: 1.562866]\n",
      "epoch:22 step:21511 [D loss: 0.482747, acc.: 80.47%] [G loss: 1.355481]\n",
      "epoch:22 step:21512 [D loss: 0.513701, acc.: 73.44%] [G loss: 1.189897]\n",
      "epoch:22 step:21513 [D loss: 0.509210, acc.: 73.44%] [G loss: 1.324309]\n",
      "epoch:22 step:21514 [D loss: 0.599495, acc.: 65.62%] [G loss: 1.347845]\n",
      "epoch:22 step:21515 [D loss: 0.658538, acc.: 60.16%] [G loss: 1.257192]\n",
      "epoch:22 step:21516 [D loss: 0.488540, acc.: 81.25%] [G loss: 1.591561]\n",
      "epoch:22 step:21517 [D loss: 0.508614, acc.: 78.91%] [G loss: 1.472457]\n",
      "epoch:22 step:21518 [D loss: 0.518861, acc.: 75.78%] [G loss: 1.527013]\n",
      "epoch:22 step:21519 [D loss: 0.664720, acc.: 62.50%] [G loss: 1.114474]\n",
      "epoch:22 step:21520 [D loss: 0.612338, acc.: 65.62%] [G loss: 1.232339]\n",
      "epoch:22 step:21521 [D loss: 0.593483, acc.: 75.00%] [G loss: 1.215094]\n",
      "epoch:22 step:21522 [D loss: 0.589410, acc.: 68.75%] [G loss: 1.325207]\n",
      "epoch:22 step:21523 [D loss: 0.595912, acc.: 64.84%] [G loss: 1.491490]\n",
      "epoch:22 step:21524 [D loss: 0.481452, acc.: 79.69%] [G loss: 1.313938]\n",
      "epoch:22 step:21525 [D loss: 0.677775, acc.: 62.50%] [G loss: 0.954330]\n",
      "epoch:22 step:21526 [D loss: 0.583812, acc.: 66.41%] [G loss: 1.657065]\n",
      "epoch:22 step:21527 [D loss: 0.598589, acc.: 67.19%] [G loss: 1.342677]\n",
      "epoch:22 step:21528 [D loss: 0.569716, acc.: 68.75%] [G loss: 0.982450]\n",
      "epoch:22 step:21529 [D loss: 0.648973, acc.: 66.41%] [G loss: 1.167411]\n",
      "epoch:22 step:21530 [D loss: 0.628602, acc.: 62.50%] [G loss: 1.394865]\n",
      "epoch:22 step:21531 [D loss: 0.593588, acc.: 67.19%] [G loss: 1.251247]\n",
      "epoch:22 step:21532 [D loss: 0.475060, acc.: 79.69%] [G loss: 1.324565]\n",
      "epoch:22 step:21533 [D loss: 0.490771, acc.: 77.34%] [G loss: 1.423853]\n",
      "epoch:22 step:21534 [D loss: 0.643502, acc.: 64.06%] [G loss: 1.050860]\n",
      "epoch:22 step:21535 [D loss: 0.561849, acc.: 68.75%] [G loss: 1.512814]\n",
      "epoch:22 step:21536 [D loss: 0.503519, acc.: 75.00%] [G loss: 1.283038]\n",
      "epoch:22 step:21537 [D loss: 0.695864, acc.: 62.50%] [G loss: 1.238820]\n",
      "epoch:22 step:21538 [D loss: 0.533815, acc.: 75.00%] [G loss: 1.370038]\n",
      "epoch:22 step:21539 [D loss: 0.419736, acc.: 82.81%] [G loss: 1.142734]\n",
      "epoch:22 step:21540 [D loss: 0.509179, acc.: 78.12%] [G loss: 1.265870]\n",
      "epoch:22 step:21541 [D loss: 0.625901, acc.: 65.62%] [G loss: 1.480271]\n",
      "epoch:22 step:21542 [D loss: 0.669665, acc.: 60.94%] [G loss: 1.157686]\n",
      "epoch:22 step:21543 [D loss: 0.602262, acc.: 68.75%] [G loss: 1.199502]\n",
      "epoch:22 step:21544 [D loss: 0.402264, acc.: 82.81%] [G loss: 1.560992]\n",
      "epoch:22 step:21545 [D loss: 0.706141, acc.: 57.81%] [G loss: 1.253464]\n",
      "epoch:22 step:21546 [D loss: 0.459431, acc.: 82.03%] [G loss: 1.310330]\n",
      "epoch:22 step:21547 [D loss: 0.663863, acc.: 63.28%] [G loss: 1.348900]\n",
      "epoch:22 step:21548 [D loss: 0.614367, acc.: 65.62%] [G loss: 1.400777]\n",
      "epoch:22 step:21549 [D loss: 0.554433, acc.: 71.09%] [G loss: 1.533783]\n",
      "epoch:22 step:21550 [D loss: 0.438611, acc.: 82.03%] [G loss: 1.819066]\n",
      "epoch:22 step:21551 [D loss: 0.598869, acc.: 67.97%] [G loss: 1.562023]\n",
      "epoch:23 step:21552 [D loss: 0.481485, acc.: 77.34%] [G loss: 1.568836]\n",
      "epoch:23 step:21553 [D loss: 0.420728, acc.: 83.59%] [G loss: 1.662007]\n",
      "epoch:23 step:21554 [D loss: 0.565603, acc.: 68.75%] [G loss: 1.359849]\n",
      "epoch:23 step:21555 [D loss: 0.589232, acc.: 64.84%] [G loss: 1.417565]\n",
      "epoch:23 step:21556 [D loss: 0.453712, acc.: 84.38%] [G loss: 1.487897]\n",
      "epoch:23 step:21557 [D loss: 0.804250, acc.: 50.00%] [G loss: 1.330470]\n",
      "epoch:23 step:21558 [D loss: 0.705459, acc.: 51.56%] [G loss: 0.956675]\n",
      "epoch:23 step:21559 [D loss: 0.425152, acc.: 86.72%] [G loss: 1.269399]\n",
      "epoch:23 step:21560 [D loss: 0.601856, acc.: 66.41%] [G loss: 1.264492]\n",
      "epoch:23 step:21561 [D loss: 0.696600, acc.: 57.03%] [G loss: 0.997535]\n",
      "epoch:23 step:21562 [D loss: 0.580476, acc.: 67.97%] [G loss: 1.399347]\n",
      "epoch:23 step:21563 [D loss: 0.488939, acc.: 80.47%] [G loss: 1.660145]\n",
      "epoch:23 step:21564 [D loss: 0.682369, acc.: 57.03%] [G loss: 1.204474]\n",
      "epoch:23 step:21565 [D loss: 0.574635, acc.: 64.84%] [G loss: 1.053109]\n",
      "epoch:23 step:21566 [D loss: 0.604183, acc.: 69.53%] [G loss: 1.321317]\n",
      "epoch:23 step:21567 [D loss: 0.561453, acc.: 72.66%] [G loss: 1.367996]\n",
      "epoch:23 step:21568 [D loss: 0.450494, acc.: 79.69%] [G loss: 1.481064]\n",
      "epoch:23 step:21569 [D loss: 0.561648, acc.: 71.09%] [G loss: 1.223305]\n",
      "epoch:23 step:21570 [D loss: 0.627984, acc.: 59.38%] [G loss: 0.879552]\n",
      "epoch:23 step:21571 [D loss: 0.653320, acc.: 66.41%] [G loss: 1.214195]\n",
      "epoch:23 step:21572 [D loss: 0.523879, acc.: 75.00%] [G loss: 0.936647]\n",
      "epoch:23 step:21573 [D loss: 0.589019, acc.: 67.19%] [G loss: 1.199890]\n",
      "epoch:23 step:21574 [D loss: 0.442838, acc.: 80.47%] [G loss: 1.587913]\n",
      "epoch:23 step:21575 [D loss: 0.742359, acc.: 52.34%] [G loss: 1.282372]\n",
      "epoch:23 step:21576 [D loss: 0.518387, acc.: 73.44%] [G loss: 1.533383]\n",
      "epoch:23 step:21577 [D loss: 0.510515, acc.: 73.44%] [G loss: 1.102717]\n",
      "epoch:23 step:21578 [D loss: 0.580265, acc.: 67.97%] [G loss: 1.287234]\n",
      "epoch:23 step:21579 [D loss: 0.549467, acc.: 73.44%] [G loss: 1.279708]\n",
      "epoch:23 step:21580 [D loss: 0.552090, acc.: 69.53%] [G loss: 1.736408]\n",
      "epoch:23 step:21581 [D loss: 0.567182, acc.: 70.31%] [G loss: 1.716221]\n",
      "epoch:23 step:21582 [D loss: 0.563529, acc.: 72.66%] [G loss: 1.218328]\n",
      "epoch:23 step:21583 [D loss: 0.615951, acc.: 71.09%] [G loss: 1.679196]\n",
      "epoch:23 step:21584 [D loss: 0.494818, acc.: 72.66%] [G loss: 1.582002]\n",
      "epoch:23 step:21585 [D loss: 0.555987, acc.: 71.09%] [G loss: 1.437038]\n",
      "epoch:23 step:21586 [D loss: 0.652145, acc.: 60.94%] [G loss: 1.683695]\n",
      "epoch:23 step:21587 [D loss: 0.503764, acc.: 81.25%] [G loss: 0.853608]\n",
      "epoch:23 step:21588 [D loss: 0.432878, acc.: 81.25%] [G loss: 1.371720]\n",
      "epoch:23 step:21589 [D loss: 0.655568, acc.: 58.59%] [G loss: 1.524813]\n",
      "epoch:23 step:21590 [D loss: 0.572944, acc.: 72.66%] [G loss: 1.219870]\n",
      "epoch:23 step:21591 [D loss: 0.505823, acc.: 73.44%] [G loss: 1.294101]\n",
      "epoch:23 step:21592 [D loss: 0.574792, acc.: 71.09%] [G loss: 1.114269]\n",
      "epoch:23 step:21593 [D loss: 0.550028, acc.: 72.66%] [G loss: 1.235469]\n",
      "epoch:23 step:21594 [D loss: 0.717407, acc.: 64.06%] [G loss: 1.031727]\n",
      "epoch:23 step:21595 [D loss: 0.747014, acc.: 57.81%] [G loss: 1.097106]\n",
      "epoch:23 step:21596 [D loss: 0.577910, acc.: 69.53%] [G loss: 1.441572]\n",
      "epoch:23 step:21597 [D loss: 0.694577, acc.: 58.59%] [G loss: 0.907926]\n",
      "epoch:23 step:21598 [D loss: 0.603591, acc.: 68.75%] [G loss: 1.548656]\n",
      "epoch:23 step:21599 [D loss: 0.554925, acc.: 74.22%] [G loss: 1.540120]\n",
      "epoch:23 step:21600 [D loss: 0.596634, acc.: 68.75%] [G loss: 1.166464]\n",
      "##############\n",
      "[2.63903669 1.88344495 1.88992626 2.64106576 0.74586285 5.93272388\n",
      " 2.19972158 2.80085577 3.7849035  5.79445996]\n",
      "##########\n",
      "epoch:23 step:21601 [D loss: 0.523198, acc.: 75.78%] [G loss: 1.618533]\n",
      "epoch:23 step:21602 [D loss: 0.579997, acc.: 69.53%] [G loss: 1.533507]\n",
      "epoch:23 step:21603 [D loss: 0.513452, acc.: 71.09%] [G loss: 1.168690]\n",
      "epoch:23 step:21604 [D loss: 0.456921, acc.: 77.34%] [G loss: 1.658255]\n",
      "epoch:23 step:21605 [D loss: 0.332774, acc.: 90.62%] [G loss: 1.237170]\n",
      "epoch:23 step:21606 [D loss: 0.520625, acc.: 75.00%] [G loss: 1.143620]\n",
      "epoch:23 step:21607 [D loss: 0.409885, acc.: 83.59%] [G loss: 1.255421]\n",
      "epoch:23 step:21608 [D loss: 0.721829, acc.: 57.03%] [G loss: 1.011486]\n",
      "epoch:23 step:21609 [D loss: 0.520548, acc.: 75.00%] [G loss: 1.295774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21610 [D loss: 0.488153, acc.: 75.78%] [G loss: 1.429721]\n",
      "epoch:23 step:21611 [D loss: 0.501853, acc.: 74.22%] [G loss: 1.473839]\n",
      "epoch:23 step:21612 [D loss: 0.618441, acc.: 61.72%] [G loss: 1.262764]\n",
      "epoch:23 step:21613 [D loss: 0.557089, acc.: 71.88%] [G loss: 1.235492]\n",
      "epoch:23 step:21614 [D loss: 0.630625, acc.: 67.19%] [G loss: 1.182129]\n",
      "epoch:23 step:21615 [D loss: 0.588076, acc.: 68.75%] [G loss: 1.382879]\n",
      "epoch:23 step:21616 [D loss: 0.540524, acc.: 72.66%] [G loss: 1.182832]\n",
      "epoch:23 step:21617 [D loss: 0.485421, acc.: 78.91%] [G loss: 1.348903]\n",
      "epoch:23 step:21618 [D loss: 0.630985, acc.: 64.06%] [G loss: 1.434215]\n",
      "epoch:23 step:21619 [D loss: 0.541784, acc.: 71.88%] [G loss: 1.310424]\n",
      "epoch:23 step:21620 [D loss: 0.626778, acc.: 61.72%] [G loss: 1.378104]\n",
      "epoch:23 step:21621 [D loss: 0.541870, acc.: 71.88%] [G loss: 1.128959]\n",
      "epoch:23 step:21622 [D loss: 0.713399, acc.: 57.81%] [G loss: 1.341336]\n",
      "epoch:23 step:21623 [D loss: 0.546670, acc.: 73.44%] [G loss: 1.001554]\n",
      "epoch:23 step:21624 [D loss: 0.438948, acc.: 80.47%] [G loss: 1.576339]\n",
      "epoch:23 step:21625 [D loss: 0.526405, acc.: 74.22%] [G loss: 1.234231]\n",
      "epoch:23 step:21626 [D loss: 0.530014, acc.: 74.22%] [G loss: 1.399670]\n",
      "epoch:23 step:21627 [D loss: 0.606429, acc.: 67.19%] [G loss: 1.537753]\n",
      "epoch:23 step:21628 [D loss: 0.675704, acc.: 56.25%] [G loss: 1.605630]\n",
      "epoch:23 step:21629 [D loss: 0.565647, acc.: 66.41%] [G loss: 1.287526]\n",
      "epoch:23 step:21630 [D loss: 0.528086, acc.: 74.22%] [G loss: 1.216566]\n",
      "epoch:23 step:21631 [D loss: 0.386526, acc.: 84.38%] [G loss: 1.764350]\n",
      "epoch:23 step:21632 [D loss: 0.677663, acc.: 64.06%] [G loss: 1.428938]\n",
      "epoch:23 step:21633 [D loss: 0.523373, acc.: 77.34%] [G loss: 1.408739]\n",
      "epoch:23 step:21634 [D loss: 0.464422, acc.: 80.47%] [G loss: 1.269293]\n",
      "epoch:23 step:21635 [D loss: 0.512113, acc.: 71.88%] [G loss: 1.278932]\n",
      "epoch:23 step:21636 [D loss: 0.631261, acc.: 65.62%] [G loss: 1.352642]\n",
      "epoch:23 step:21637 [D loss: 0.365667, acc.: 87.50%] [G loss: 1.716938]\n",
      "epoch:23 step:21638 [D loss: 0.496756, acc.: 77.34%] [G loss: 1.458713]\n",
      "epoch:23 step:21639 [D loss: 0.593731, acc.: 67.97%] [G loss: 1.057264]\n",
      "epoch:23 step:21640 [D loss: 0.626071, acc.: 62.50%] [G loss: 1.107307]\n",
      "epoch:23 step:21641 [D loss: 0.494427, acc.: 78.12%] [G loss: 1.521070]\n",
      "epoch:23 step:21642 [D loss: 0.545070, acc.: 75.00%] [G loss: 1.124109]\n",
      "epoch:23 step:21643 [D loss: 0.540703, acc.: 71.09%] [G loss: 1.325881]\n",
      "epoch:23 step:21644 [D loss: 0.530274, acc.: 75.78%] [G loss: 1.741020]\n",
      "epoch:23 step:21645 [D loss: 0.536468, acc.: 78.12%] [G loss: 1.463551]\n",
      "epoch:23 step:21646 [D loss: 0.584656, acc.: 69.53%] [G loss: 1.199462]\n",
      "epoch:23 step:21647 [D loss: 0.595056, acc.: 67.19%] [G loss: 1.648263]\n",
      "epoch:23 step:21648 [D loss: 0.705313, acc.: 55.47%] [G loss: 1.346177]\n",
      "epoch:23 step:21649 [D loss: 0.696732, acc.: 56.25%] [G loss: 1.153514]\n",
      "epoch:23 step:21650 [D loss: 0.680451, acc.: 67.97%] [G loss: 1.321932]\n",
      "epoch:23 step:21651 [D loss: 0.636961, acc.: 60.16%] [G loss: 1.646130]\n",
      "epoch:23 step:21652 [D loss: 0.487758, acc.: 77.34%] [G loss: 1.616751]\n",
      "epoch:23 step:21653 [D loss: 0.667775, acc.: 65.62%] [G loss: 1.364249]\n",
      "epoch:23 step:21654 [D loss: 0.644543, acc.: 64.84%] [G loss: 1.263117]\n",
      "epoch:23 step:21655 [D loss: 0.551482, acc.: 71.09%] [G loss: 1.453725]\n",
      "epoch:23 step:21656 [D loss: 0.534885, acc.: 72.66%] [G loss: 1.460603]\n",
      "epoch:23 step:21657 [D loss: 0.641274, acc.: 67.19%] [G loss: 1.213449]\n",
      "epoch:23 step:21658 [D loss: 0.519955, acc.: 77.34%] [G loss: 1.384281]\n",
      "epoch:23 step:21659 [D loss: 0.544102, acc.: 74.22%] [G loss: 1.025403]\n",
      "epoch:23 step:21660 [D loss: 0.616633, acc.: 67.19%] [G loss: 0.991615]\n",
      "epoch:23 step:21661 [D loss: 0.679373, acc.: 66.41%] [G loss: 1.382157]\n",
      "epoch:23 step:21662 [D loss: 0.504122, acc.: 78.12%] [G loss: 1.281066]\n",
      "epoch:23 step:21663 [D loss: 0.501003, acc.: 78.91%] [G loss: 1.190889]\n",
      "epoch:23 step:21664 [D loss: 0.428783, acc.: 85.16%] [G loss: 1.307596]\n",
      "epoch:23 step:21665 [D loss: 0.490559, acc.: 76.56%] [G loss: 0.991024]\n",
      "epoch:23 step:21666 [D loss: 0.577210, acc.: 69.53%] [G loss: 1.310694]\n",
      "epoch:23 step:21667 [D loss: 0.574190, acc.: 67.19%] [G loss: 1.603554]\n",
      "epoch:23 step:21668 [D loss: 0.497179, acc.: 78.91%] [G loss: 1.357729]\n",
      "epoch:23 step:21669 [D loss: 0.638214, acc.: 62.50%] [G loss: 1.467703]\n",
      "epoch:23 step:21670 [D loss: 0.468386, acc.: 78.12%] [G loss: 1.465003]\n",
      "epoch:23 step:21671 [D loss: 0.693353, acc.: 61.72%] [G loss: 1.552923]\n",
      "epoch:23 step:21672 [D loss: 0.549405, acc.: 71.88%] [G loss: 1.458843]\n",
      "epoch:23 step:21673 [D loss: 0.650854, acc.: 64.84%] [G loss: 1.185857]\n",
      "epoch:23 step:21674 [D loss: 0.476149, acc.: 76.56%] [G loss: 1.610835]\n",
      "epoch:23 step:21675 [D loss: 0.613849, acc.: 66.41%] [G loss: 1.325034]\n",
      "epoch:23 step:21676 [D loss: 0.605534, acc.: 61.72%] [G loss: 1.136834]\n",
      "epoch:23 step:21677 [D loss: 0.544088, acc.: 72.66%] [G loss: 1.383620]\n",
      "epoch:23 step:21678 [D loss: 0.534968, acc.: 68.75%] [G loss: 1.223223]\n",
      "epoch:23 step:21679 [D loss: 0.522249, acc.: 75.78%] [G loss: 1.036338]\n",
      "epoch:23 step:21680 [D loss: 0.567053, acc.: 71.09%] [G loss: 1.466040]\n",
      "epoch:23 step:21681 [D loss: 0.737782, acc.: 58.59%] [G loss: 1.049950]\n",
      "epoch:23 step:21682 [D loss: 0.659745, acc.: 63.28%] [G loss: 1.147071]\n",
      "epoch:23 step:21683 [D loss: 0.612514, acc.: 66.41%] [G loss: 1.542537]\n",
      "epoch:23 step:21684 [D loss: 0.525268, acc.: 75.00%] [G loss: 1.234926]\n",
      "epoch:23 step:21685 [D loss: 0.500018, acc.: 72.66%] [G loss: 1.297317]\n",
      "epoch:23 step:21686 [D loss: 0.522916, acc.: 72.66%] [G loss: 1.354288]\n",
      "epoch:23 step:21687 [D loss: 0.628064, acc.: 63.28%] [G loss: 1.337202]\n",
      "epoch:23 step:21688 [D loss: 0.637692, acc.: 62.50%] [G loss: 1.565803]\n",
      "epoch:23 step:21689 [D loss: 0.659406, acc.: 65.62%] [G loss: 1.398954]\n",
      "epoch:23 step:21690 [D loss: 0.390203, acc.: 88.28%] [G loss: 1.513835]\n",
      "epoch:23 step:21691 [D loss: 0.657392, acc.: 64.84%] [G loss: 1.400883]\n",
      "epoch:23 step:21692 [D loss: 0.662961, acc.: 53.91%] [G loss: 1.603973]\n",
      "epoch:23 step:21693 [D loss: 0.479517, acc.: 76.56%] [G loss: 1.433900]\n",
      "epoch:23 step:21694 [D loss: 0.424075, acc.: 85.94%] [G loss: 1.423422]\n",
      "epoch:23 step:21695 [D loss: 0.484910, acc.: 75.78%] [G loss: 1.348775]\n",
      "epoch:23 step:21696 [D loss: 0.594034, acc.: 66.41%] [G loss: 1.309764]\n",
      "epoch:23 step:21697 [D loss: 0.488067, acc.: 75.00%] [G loss: 1.418402]\n",
      "epoch:23 step:21698 [D loss: 0.383090, acc.: 88.28%] [G loss: 1.325313]\n",
      "epoch:23 step:21699 [D loss: 0.522653, acc.: 75.00%] [G loss: 1.547886]\n",
      "epoch:23 step:21700 [D loss: 0.921548, acc.: 41.41%] [G loss: 0.962009]\n",
      "epoch:23 step:21701 [D loss: 0.478874, acc.: 75.78%] [G loss: 1.605442]\n",
      "epoch:23 step:21702 [D loss: 0.521040, acc.: 80.47%] [G loss: 1.933956]\n",
      "epoch:23 step:21703 [D loss: 0.583532, acc.: 67.97%] [G loss: 1.637541]\n",
      "epoch:23 step:21704 [D loss: 0.454047, acc.: 78.91%] [G loss: 1.035006]\n",
      "epoch:23 step:21705 [D loss: 0.445021, acc.: 85.16%] [G loss: 1.513747]\n",
      "epoch:23 step:21706 [D loss: 0.588428, acc.: 73.44%] [G loss: 1.307992]\n",
      "epoch:23 step:21707 [D loss: 0.443561, acc.: 78.91%] [G loss: 1.204928]\n",
      "epoch:23 step:21708 [D loss: 0.586058, acc.: 67.19%] [G loss: 1.155062]\n",
      "epoch:23 step:21709 [D loss: 0.530443, acc.: 74.22%] [G loss: 1.234871]\n",
      "epoch:23 step:21710 [D loss: 0.688096, acc.: 58.59%] [G loss: 1.200235]\n",
      "epoch:23 step:21711 [D loss: 0.523753, acc.: 73.44%] [G loss: 1.475982]\n",
      "epoch:23 step:21712 [D loss: 0.465106, acc.: 80.47%] [G loss: 1.720768]\n",
      "epoch:23 step:21713 [D loss: 0.675711, acc.: 59.38%] [G loss: 1.209050]\n",
      "epoch:23 step:21714 [D loss: 0.567499, acc.: 73.44%] [G loss: 1.309126]\n",
      "epoch:23 step:21715 [D loss: 0.608495, acc.: 64.84%] [G loss: 1.580346]\n",
      "epoch:23 step:21716 [D loss: 0.532976, acc.: 75.00%] [G loss: 1.284196]\n",
      "epoch:23 step:21717 [D loss: 0.513250, acc.: 74.22%] [G loss: 1.207214]\n",
      "epoch:23 step:21718 [D loss: 0.702375, acc.: 56.25%] [G loss: 1.311085]\n",
      "epoch:23 step:21719 [D loss: 0.605677, acc.: 67.19%] [G loss: 1.355475]\n",
      "epoch:23 step:21720 [D loss: 0.478078, acc.: 79.69%] [G loss: 1.271211]\n",
      "epoch:23 step:21721 [D loss: 0.641964, acc.: 63.28%] [G loss: 1.579581]\n",
      "epoch:23 step:21722 [D loss: 0.519732, acc.: 73.44%] [G loss: 1.365913]\n",
      "epoch:23 step:21723 [D loss: 0.475408, acc.: 76.56%] [G loss: 1.200911]\n",
      "epoch:23 step:21724 [D loss: 0.685149, acc.: 63.28%] [G loss: 1.612388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21725 [D loss: 0.422800, acc.: 81.25%] [G loss: 1.331474]\n",
      "epoch:23 step:21726 [D loss: 0.509709, acc.: 68.75%] [G loss: 1.606729]\n",
      "epoch:23 step:21727 [D loss: 0.505251, acc.: 78.12%] [G loss: 1.236057]\n",
      "epoch:23 step:21728 [D loss: 0.592502, acc.: 71.09%] [G loss: 1.412529]\n",
      "epoch:23 step:21729 [D loss: 0.674460, acc.: 56.25%] [G loss: 1.100990]\n",
      "epoch:23 step:21730 [D loss: 0.493890, acc.: 77.34%] [G loss: 1.319979]\n",
      "epoch:23 step:21731 [D loss: 0.366531, acc.: 89.84%] [G loss: 1.459433]\n",
      "epoch:23 step:21732 [D loss: 0.753253, acc.: 55.47%] [G loss: 1.353052]\n",
      "epoch:23 step:21733 [D loss: 0.340833, acc.: 92.97%] [G loss: 1.729171]\n",
      "epoch:23 step:21734 [D loss: 0.514752, acc.: 76.56%] [G loss: 1.524312]\n",
      "epoch:23 step:21735 [D loss: 0.676950, acc.: 62.50%] [G loss: 1.201081]\n",
      "epoch:23 step:21736 [D loss: 0.417685, acc.: 82.81%] [G loss: 1.310141]\n",
      "epoch:23 step:21737 [D loss: 0.619017, acc.: 63.28%] [G loss: 1.172579]\n",
      "epoch:23 step:21738 [D loss: 0.516865, acc.: 74.22%] [G loss: 1.568536]\n",
      "epoch:23 step:21739 [D loss: 0.494938, acc.: 73.44%] [G loss: 1.774574]\n",
      "epoch:23 step:21740 [D loss: 0.439810, acc.: 81.25%] [G loss: 1.548179]\n",
      "epoch:23 step:21741 [D loss: 0.528810, acc.: 74.22%] [G loss: 1.587863]\n",
      "epoch:23 step:21742 [D loss: 0.479320, acc.: 77.34%] [G loss: 1.251608]\n",
      "epoch:23 step:21743 [D loss: 0.392155, acc.: 89.84%] [G loss: 1.634970]\n",
      "epoch:23 step:21744 [D loss: 0.496050, acc.: 78.12%] [G loss: 1.441563]\n",
      "epoch:23 step:21745 [D loss: 0.464321, acc.: 77.34%] [G loss: 1.385451]\n",
      "epoch:23 step:21746 [D loss: 0.517787, acc.: 80.47%] [G loss: 1.515522]\n",
      "epoch:23 step:21747 [D loss: 0.542649, acc.: 75.00%] [G loss: 1.495023]\n",
      "epoch:23 step:21748 [D loss: 0.465893, acc.: 73.44%] [G loss: 1.710524]\n",
      "epoch:23 step:21749 [D loss: 0.579909, acc.: 69.53%] [G loss: 1.538986]\n",
      "epoch:23 step:21750 [D loss: 0.615933, acc.: 60.16%] [G loss: 0.976072]\n",
      "epoch:23 step:21751 [D loss: 0.609344, acc.: 69.53%] [G loss: 1.109187]\n",
      "epoch:23 step:21752 [D loss: 0.350770, acc.: 89.06%] [G loss: 1.453912]\n",
      "epoch:23 step:21753 [D loss: 0.574879, acc.: 71.09%] [G loss: 1.808871]\n",
      "epoch:23 step:21754 [D loss: 0.451116, acc.: 80.47%] [G loss: 1.667101]\n",
      "epoch:23 step:21755 [D loss: 0.409423, acc.: 86.72%] [G loss: 1.628234]\n",
      "epoch:23 step:21756 [D loss: 0.714554, acc.: 59.38%] [G loss: 1.444422]\n",
      "epoch:23 step:21757 [D loss: 0.485913, acc.: 78.12%] [G loss: 1.306456]\n",
      "epoch:23 step:21758 [D loss: 0.584840, acc.: 67.19%] [G loss: 1.358700]\n",
      "epoch:23 step:21759 [D loss: 0.487125, acc.: 76.56%] [G loss: 1.067503]\n",
      "epoch:23 step:21760 [D loss: 0.763523, acc.: 51.56%] [G loss: 1.243626]\n",
      "epoch:23 step:21761 [D loss: 0.576995, acc.: 68.75%] [G loss: 1.116381]\n",
      "epoch:23 step:21762 [D loss: 0.512278, acc.: 76.56%] [G loss: 1.088213]\n",
      "epoch:23 step:21763 [D loss: 0.646526, acc.: 60.16%] [G loss: 1.247913]\n",
      "epoch:23 step:21764 [D loss: 0.659797, acc.: 63.28%] [G loss: 1.102651]\n",
      "epoch:23 step:21765 [D loss: 0.753255, acc.: 57.81%] [G loss: 1.370657]\n",
      "epoch:23 step:21766 [D loss: 0.510917, acc.: 71.09%] [G loss: 1.178614]\n",
      "epoch:23 step:21767 [D loss: 0.698167, acc.: 56.25%] [G loss: 1.535330]\n",
      "epoch:23 step:21768 [D loss: 0.457273, acc.: 80.47%] [G loss: 1.815106]\n",
      "epoch:23 step:21769 [D loss: 0.390432, acc.: 83.59%] [G loss: 1.570730]\n",
      "epoch:23 step:21770 [D loss: 0.655357, acc.: 64.06%] [G loss: 1.106580]\n",
      "epoch:23 step:21771 [D loss: 0.433713, acc.: 82.81%] [G loss: 1.645207]\n",
      "epoch:23 step:21772 [D loss: 0.562982, acc.: 67.97%] [G loss: 1.404158]\n",
      "epoch:23 step:21773 [D loss: 0.535745, acc.: 71.09%] [G loss: 1.608313]\n",
      "epoch:23 step:21774 [D loss: 0.484383, acc.: 79.69%] [G loss: 1.232599]\n",
      "epoch:23 step:21775 [D loss: 0.421999, acc.: 82.03%] [G loss: 1.438944]\n",
      "epoch:23 step:21776 [D loss: 0.624672, acc.: 68.75%] [G loss: 1.100829]\n",
      "epoch:23 step:21777 [D loss: 0.455612, acc.: 80.47%] [G loss: 1.786299]\n",
      "epoch:23 step:21778 [D loss: 0.494646, acc.: 78.12%] [G loss: 1.427509]\n",
      "epoch:23 step:21779 [D loss: 0.579271, acc.: 67.19%] [G loss: 1.460820]\n",
      "epoch:23 step:21780 [D loss: 0.489473, acc.: 73.44%] [G loss: 1.188137]\n",
      "epoch:23 step:21781 [D loss: 0.559010, acc.: 68.75%] [G loss: 1.294996]\n",
      "epoch:23 step:21782 [D loss: 0.387866, acc.: 85.16%] [G loss: 1.416097]\n",
      "epoch:23 step:21783 [D loss: 0.848833, acc.: 48.44%] [G loss: 1.078699]\n",
      "epoch:23 step:21784 [D loss: 0.555537, acc.: 69.53%] [G loss: 1.173939]\n",
      "epoch:23 step:21785 [D loss: 0.654120, acc.: 62.50%] [G loss: 0.974277]\n",
      "epoch:23 step:21786 [D loss: 0.463196, acc.: 84.38%] [G loss: 1.308475]\n",
      "epoch:23 step:21787 [D loss: 0.578721, acc.: 68.75%] [G loss: 1.347774]\n",
      "epoch:23 step:21788 [D loss: 0.683505, acc.: 61.72%] [G loss: 1.307578]\n",
      "epoch:23 step:21789 [D loss: 0.629165, acc.: 64.84%] [G loss: 1.638402]\n",
      "epoch:23 step:21790 [D loss: 0.556200, acc.: 78.12%] [G loss: 1.424398]\n",
      "epoch:23 step:21791 [D loss: 0.539877, acc.: 75.00%] [G loss: 1.064684]\n",
      "epoch:23 step:21792 [D loss: 0.476408, acc.: 81.25%] [G loss: 1.568699]\n",
      "epoch:23 step:21793 [D loss: 0.631735, acc.: 64.84%] [G loss: 1.442738]\n",
      "epoch:23 step:21794 [D loss: 0.580409, acc.: 71.88%] [G loss: 1.072790]\n",
      "epoch:23 step:21795 [D loss: 0.598693, acc.: 71.88%] [G loss: 1.367155]\n",
      "epoch:23 step:21796 [D loss: 0.563138, acc.: 71.09%] [G loss: 1.310474]\n",
      "epoch:23 step:21797 [D loss: 0.468430, acc.: 78.91%] [G loss: 1.423270]\n",
      "epoch:23 step:21798 [D loss: 0.590737, acc.: 66.41%] [G loss: 1.242843]\n",
      "epoch:23 step:21799 [D loss: 0.684772, acc.: 57.81%] [G loss: 1.249447]\n",
      "epoch:23 step:21800 [D loss: 0.326416, acc.: 91.41%] [G loss: 1.666445]\n",
      "##############\n",
      "[2.75085489 2.01534155 2.00782092 2.89283517 0.95190328 6.15958314\n",
      " 2.24461956 2.72412942 3.92982886 7.14868929]\n",
      "##########\n",
      "epoch:23 step:21801 [D loss: 0.444919, acc.: 79.69%] [G loss: 1.673178]\n",
      "epoch:23 step:21802 [D loss: 0.587190, acc.: 69.53%] [G loss: 1.025793]\n",
      "epoch:23 step:21803 [D loss: 0.533807, acc.: 72.66%] [G loss: 1.217469]\n",
      "epoch:23 step:21804 [D loss: 0.558212, acc.: 70.31%] [G loss: 1.728873]\n",
      "epoch:23 step:21805 [D loss: 0.598536, acc.: 71.88%] [G loss: 1.089410]\n",
      "epoch:23 step:21806 [D loss: 0.552379, acc.: 68.75%] [G loss: 1.209955]\n",
      "epoch:23 step:21807 [D loss: 0.488851, acc.: 76.56%] [G loss: 1.087780]\n",
      "epoch:23 step:21808 [D loss: 0.469929, acc.: 78.12%] [G loss: 1.605697]\n",
      "epoch:23 step:21809 [D loss: 0.404699, acc.: 82.03%] [G loss: 1.517629]\n",
      "epoch:23 step:21810 [D loss: 0.553448, acc.: 70.31%] [G loss: 1.092770]\n",
      "epoch:23 step:21811 [D loss: 0.455012, acc.: 82.81%] [G loss: 1.431464]\n",
      "epoch:23 step:21812 [D loss: 0.597428, acc.: 66.41%] [G loss: 0.960408]\n",
      "epoch:23 step:21813 [D loss: 0.432129, acc.: 78.91%] [G loss: 1.452104]\n",
      "epoch:23 step:21814 [D loss: 0.657638, acc.: 59.38%] [G loss: 1.312714]\n",
      "epoch:23 step:21815 [D loss: 0.469793, acc.: 78.91%] [G loss: 1.451894]\n",
      "epoch:23 step:21816 [D loss: 0.495748, acc.: 80.47%] [G loss: 1.194444]\n",
      "epoch:23 step:21817 [D loss: 0.469234, acc.: 77.34%] [G loss: 1.449859]\n",
      "epoch:23 step:21818 [D loss: 0.522365, acc.: 77.34%] [G loss: 1.609442]\n",
      "epoch:23 step:21819 [D loss: 0.741484, acc.: 54.69%] [G loss: 1.158461]\n",
      "epoch:23 step:21820 [D loss: 0.410725, acc.: 86.72%] [G loss: 1.608598]\n",
      "epoch:23 step:21821 [D loss: 0.566711, acc.: 70.31%] [G loss: 1.211328]\n",
      "epoch:23 step:21822 [D loss: 0.593346, acc.: 68.75%] [G loss: 1.462842]\n",
      "epoch:23 step:21823 [D loss: 0.510569, acc.: 78.12%] [G loss: 1.485018]\n",
      "epoch:23 step:21824 [D loss: 0.650224, acc.: 57.03%] [G loss: 1.000749]\n",
      "epoch:23 step:21825 [D loss: 0.506008, acc.: 76.56%] [G loss: 1.344846]\n",
      "epoch:23 step:21826 [D loss: 0.678618, acc.: 60.94%] [G loss: 1.420396]\n",
      "epoch:23 step:21827 [D loss: 0.796048, acc.: 56.25%] [G loss: 1.195475]\n",
      "epoch:23 step:21828 [D loss: 0.461808, acc.: 80.47%] [G loss: 1.359169]\n",
      "epoch:23 step:21829 [D loss: 0.432252, acc.: 83.59%] [G loss: 1.497087]\n",
      "epoch:23 step:21830 [D loss: 0.468790, acc.: 77.34%] [G loss: 1.627569]\n",
      "epoch:23 step:21831 [D loss: 0.531123, acc.: 76.56%] [G loss: 1.509445]\n",
      "epoch:23 step:21832 [D loss: 0.543045, acc.: 74.22%] [G loss: 1.104923]\n",
      "epoch:23 step:21833 [D loss: 0.625395, acc.: 69.53%] [G loss: 1.374183]\n",
      "epoch:23 step:21834 [D loss: 0.522469, acc.: 73.44%] [G loss: 1.425582]\n",
      "epoch:23 step:21835 [D loss: 0.533069, acc.: 75.00%] [G loss: 1.297837]\n",
      "epoch:23 step:21836 [D loss: 0.604244, acc.: 68.75%] [G loss: 1.275373]\n",
      "epoch:23 step:21837 [D loss: 0.613692, acc.: 63.28%] [G loss: 1.141869]\n",
      "epoch:23 step:21838 [D loss: 0.517612, acc.: 76.56%] [G loss: 0.922600]\n",
      "epoch:23 step:21839 [D loss: 0.615957, acc.: 66.41%] [G loss: 1.336983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21840 [D loss: 0.749052, acc.: 54.69%] [G loss: 0.897339]\n",
      "epoch:23 step:21841 [D loss: 0.514496, acc.: 74.22%] [G loss: 1.661826]\n",
      "epoch:23 step:21842 [D loss: 0.485319, acc.: 75.78%] [G loss: 1.405783]\n",
      "epoch:23 step:21843 [D loss: 0.579111, acc.: 69.53%] [G loss: 1.411778]\n",
      "epoch:23 step:21844 [D loss: 0.680596, acc.: 56.25%] [G loss: 1.196234]\n",
      "epoch:23 step:21845 [D loss: 0.603228, acc.: 64.84%] [G loss: 1.416900]\n",
      "epoch:23 step:21846 [D loss: 0.601756, acc.: 64.84%] [G loss: 1.433416]\n",
      "epoch:23 step:21847 [D loss: 0.546452, acc.: 70.31%] [G loss: 1.242494]\n",
      "epoch:23 step:21848 [D loss: 0.597297, acc.: 71.09%] [G loss: 1.441115]\n",
      "epoch:23 step:21849 [D loss: 0.592370, acc.: 67.97%] [G loss: 1.162145]\n",
      "epoch:23 step:21850 [D loss: 0.559482, acc.: 71.88%] [G loss: 1.160567]\n",
      "epoch:23 step:21851 [D loss: 0.574180, acc.: 68.75%] [G loss: 1.493611]\n",
      "epoch:23 step:21852 [D loss: 0.664908, acc.: 66.41%] [G loss: 1.162495]\n",
      "epoch:23 step:21853 [D loss: 0.593736, acc.: 64.84%] [G loss: 1.387374]\n",
      "epoch:23 step:21854 [D loss: 0.668447, acc.: 60.94%] [G loss: 0.995552]\n",
      "epoch:23 step:21855 [D loss: 0.589258, acc.: 65.62%] [G loss: 1.319163]\n",
      "epoch:23 step:21856 [D loss: 0.555138, acc.: 72.66%] [G loss: 1.504230]\n",
      "epoch:23 step:21857 [D loss: 0.525176, acc.: 72.66%] [G loss: 0.910511]\n",
      "epoch:23 step:21858 [D loss: 0.616887, acc.: 67.97%] [G loss: 1.527818]\n",
      "epoch:23 step:21859 [D loss: 0.546591, acc.: 71.88%] [G loss: 1.362438]\n",
      "epoch:23 step:21860 [D loss: 0.539113, acc.: 70.31%] [G loss: 1.275418]\n",
      "epoch:23 step:21861 [D loss: 0.683866, acc.: 62.50%] [G loss: 1.048499]\n",
      "epoch:23 step:21862 [D loss: 0.600390, acc.: 68.75%] [G loss: 1.323723]\n",
      "epoch:23 step:21863 [D loss: 0.656837, acc.: 65.62%] [G loss: 0.949545]\n",
      "epoch:23 step:21864 [D loss: 0.588887, acc.: 71.09%] [G loss: 1.425499]\n",
      "epoch:23 step:21865 [D loss: 0.408687, acc.: 83.59%] [G loss: 1.406508]\n",
      "epoch:23 step:21866 [D loss: 0.523412, acc.: 78.12%] [G loss: 1.399025]\n",
      "epoch:23 step:21867 [D loss: 0.673525, acc.: 59.38%] [G loss: 1.501146]\n",
      "epoch:23 step:21868 [D loss: 0.567714, acc.: 74.22%] [G loss: 1.211846]\n",
      "epoch:23 step:21869 [D loss: 0.689273, acc.: 59.38%] [G loss: 1.558974]\n",
      "epoch:23 step:21870 [D loss: 0.511271, acc.: 75.78%] [G loss: 1.576402]\n",
      "epoch:23 step:21871 [D loss: 0.586570, acc.: 68.75%] [G loss: 1.540098]\n",
      "epoch:23 step:21872 [D loss: 0.642090, acc.: 64.06%] [G loss: 1.012181]\n",
      "epoch:23 step:21873 [D loss: 0.562826, acc.: 66.41%] [G loss: 1.477035]\n",
      "epoch:23 step:21874 [D loss: 0.530249, acc.: 72.66%] [G loss: 1.371149]\n",
      "epoch:23 step:21875 [D loss: 0.492447, acc.: 75.78%] [G loss: 1.492447]\n",
      "epoch:23 step:21876 [D loss: 0.517604, acc.: 74.22%] [G loss: 1.117792]\n",
      "epoch:23 step:21877 [D loss: 0.468281, acc.: 82.81%] [G loss: 1.829450]\n",
      "epoch:23 step:21878 [D loss: 0.489476, acc.: 80.47%] [G loss: 1.576241]\n",
      "epoch:23 step:21879 [D loss: 0.492313, acc.: 80.47%] [G loss: 1.507467]\n",
      "epoch:23 step:21880 [D loss: 0.505829, acc.: 76.56%] [G loss: 1.163733]\n",
      "epoch:23 step:21881 [D loss: 0.356829, acc.: 89.06%] [G loss: 1.539085]\n",
      "epoch:23 step:21882 [D loss: 0.356266, acc.: 87.50%] [G loss: 1.248287]\n",
      "epoch:23 step:21883 [D loss: 0.612496, acc.: 69.53%] [G loss: 1.397090]\n",
      "epoch:23 step:21884 [D loss: 0.674302, acc.: 63.28%] [G loss: 1.504801]\n",
      "epoch:23 step:21885 [D loss: 0.473601, acc.: 78.91%] [G loss: 1.406768]\n",
      "epoch:23 step:21886 [D loss: 0.495884, acc.: 75.00%] [G loss: 1.325781]\n",
      "epoch:23 step:21887 [D loss: 0.542433, acc.: 71.88%] [G loss: 1.516705]\n",
      "epoch:23 step:21888 [D loss: 0.594288, acc.: 67.19%] [G loss: 1.441927]\n",
      "epoch:23 step:21889 [D loss: 0.566088, acc.: 74.22%] [G loss: 1.497123]\n",
      "epoch:23 step:21890 [D loss: 0.555966, acc.: 70.31%] [G loss: 1.525958]\n",
      "epoch:23 step:21891 [D loss: 0.453603, acc.: 77.34%] [G loss: 1.332956]\n",
      "epoch:23 step:21892 [D loss: 0.514797, acc.: 77.34%] [G loss: 2.070732]\n",
      "epoch:23 step:21893 [D loss: 0.551227, acc.: 66.41%] [G loss: 1.873954]\n",
      "epoch:23 step:21894 [D loss: 0.663634, acc.: 57.81%] [G loss: 1.254560]\n",
      "epoch:23 step:21895 [D loss: 0.656039, acc.: 64.06%] [G loss: 1.409106]\n",
      "epoch:23 step:21896 [D loss: 0.429608, acc.: 78.91%] [G loss: 1.527454]\n",
      "epoch:23 step:21897 [D loss: 0.556552, acc.: 75.00%] [G loss: 1.285532]\n",
      "epoch:23 step:21898 [D loss: 0.510476, acc.: 75.00%] [G loss: 1.233758]\n",
      "epoch:23 step:21899 [D loss: 0.556743, acc.: 73.44%] [G loss: 1.427152]\n",
      "epoch:23 step:21900 [D loss: 0.724414, acc.: 54.69%] [G loss: 1.449307]\n",
      "epoch:23 step:21901 [D loss: 0.559159, acc.: 74.22%] [G loss: 1.470647]\n",
      "epoch:23 step:21902 [D loss: 0.568665, acc.: 72.66%] [G loss: 1.407836]\n",
      "epoch:23 step:21903 [D loss: 0.437298, acc.: 82.81%] [G loss: 1.427642]\n",
      "epoch:23 step:21904 [D loss: 0.497781, acc.: 75.78%] [G loss: 1.140882]\n",
      "epoch:23 step:21905 [D loss: 0.526533, acc.: 74.22%] [G loss: 1.257321]\n",
      "epoch:23 step:21906 [D loss: 0.482062, acc.: 78.12%] [G loss: 1.589273]\n",
      "epoch:23 step:21907 [D loss: 0.526875, acc.: 75.78%] [G loss: 1.136248]\n",
      "epoch:23 step:21908 [D loss: 0.659323, acc.: 64.06%] [G loss: 1.195143]\n",
      "epoch:23 step:21909 [D loss: 0.575608, acc.: 67.19%] [G loss: 1.530090]\n",
      "epoch:23 step:21910 [D loss: 0.657502, acc.: 60.16%] [G loss: 1.194081]\n",
      "epoch:23 step:21911 [D loss: 0.492920, acc.: 78.12%] [G loss: 1.155142]\n",
      "epoch:23 step:21912 [D loss: 0.474734, acc.: 78.91%] [G loss: 1.284462]\n",
      "epoch:23 step:21913 [D loss: 0.388711, acc.: 85.16%] [G loss: 1.318698]\n",
      "epoch:23 step:21914 [D loss: 0.406943, acc.: 83.59%] [G loss: 1.507057]\n",
      "epoch:23 step:21915 [D loss: 0.587854, acc.: 67.97%] [G loss: 1.633462]\n",
      "epoch:23 step:21916 [D loss: 0.453545, acc.: 77.34%] [G loss: 1.648170]\n",
      "epoch:23 step:21917 [D loss: 0.590733, acc.: 65.62%] [G loss: 1.056764]\n",
      "epoch:23 step:21918 [D loss: 0.496648, acc.: 77.34%] [G loss: 0.973983]\n",
      "epoch:23 step:21919 [D loss: 0.430700, acc.: 85.16%] [G loss: 1.412940]\n",
      "epoch:23 step:21920 [D loss: 0.517263, acc.: 71.88%] [G loss: 1.351994]\n",
      "epoch:23 step:21921 [D loss: 0.564246, acc.: 71.88%] [G loss: 1.407541]\n",
      "epoch:23 step:21922 [D loss: 0.488621, acc.: 75.78%] [G loss: 1.614110]\n",
      "epoch:23 step:21923 [D loss: 0.558782, acc.: 73.44%] [G loss: 1.499489]\n",
      "epoch:23 step:21924 [D loss: 0.549348, acc.: 74.22%] [G loss: 1.701493]\n",
      "epoch:23 step:21925 [D loss: 0.668420, acc.: 61.72%] [G loss: 1.410289]\n",
      "epoch:23 step:21926 [D loss: 0.651936, acc.: 60.16%] [G loss: 1.237090]\n",
      "epoch:23 step:21927 [D loss: 0.652183, acc.: 62.50%] [G loss: 1.150619]\n",
      "epoch:23 step:21928 [D loss: 0.515423, acc.: 75.00%] [G loss: 1.221352]\n",
      "epoch:23 step:21929 [D loss: 0.517824, acc.: 72.66%] [G loss: 1.045632]\n",
      "epoch:23 step:21930 [D loss: 0.487784, acc.: 78.91%] [G loss: 1.581905]\n",
      "epoch:23 step:21931 [D loss: 0.708009, acc.: 57.81%] [G loss: 1.196660]\n",
      "epoch:23 step:21932 [D loss: 0.592118, acc.: 67.97%] [G loss: 1.055984]\n",
      "epoch:23 step:21933 [D loss: 0.434305, acc.: 80.47%] [G loss: 1.765250]\n",
      "epoch:23 step:21934 [D loss: 0.493094, acc.: 78.91%] [G loss: 1.037861]\n",
      "epoch:23 step:21935 [D loss: 0.620643, acc.: 66.41%] [G loss: 1.078785]\n",
      "epoch:23 step:21936 [D loss: 0.725444, acc.: 56.25%] [G loss: 1.001556]\n",
      "epoch:23 step:21937 [D loss: 0.538176, acc.: 70.31%] [G loss: 1.579828]\n",
      "epoch:23 step:21938 [D loss: 0.437421, acc.: 83.59%] [G loss: 1.733818]\n",
      "epoch:23 step:21939 [D loss: 0.562969, acc.: 66.41%] [G loss: 1.330162]\n",
      "epoch:23 step:21940 [D loss: 0.534558, acc.: 75.00%] [G loss: 1.294920]\n",
      "epoch:23 step:21941 [D loss: 0.554876, acc.: 70.31%] [G loss: 1.209969]\n",
      "epoch:23 step:21942 [D loss: 0.523275, acc.: 76.56%] [G loss: 1.341313]\n",
      "epoch:23 step:21943 [D loss: 0.550207, acc.: 75.00%] [G loss: 1.458195]\n",
      "epoch:23 step:21944 [D loss: 0.548160, acc.: 74.22%] [G loss: 1.411637]\n",
      "epoch:23 step:21945 [D loss: 0.640678, acc.: 64.84%] [G loss: 1.532416]\n",
      "epoch:23 step:21946 [D loss: 0.467201, acc.: 80.47%] [G loss: 1.332015]\n",
      "epoch:23 step:21947 [D loss: 0.635292, acc.: 61.72%] [G loss: 1.612816]\n",
      "epoch:23 step:21948 [D loss: 0.592447, acc.: 70.31%] [G loss: 1.537889]\n",
      "epoch:23 step:21949 [D loss: 0.589337, acc.: 68.75%] [G loss: 1.308953]\n",
      "epoch:23 step:21950 [D loss: 0.427301, acc.: 84.38%] [G loss: 1.241295]\n",
      "epoch:23 step:21951 [D loss: 0.608333, acc.: 68.75%] [G loss: 1.271421]\n",
      "epoch:23 step:21952 [D loss: 0.340084, acc.: 90.62%] [G loss: 1.566585]\n",
      "epoch:23 step:21953 [D loss: 0.519483, acc.: 77.34%] [G loss: 1.414019]\n",
      "epoch:23 step:21954 [D loss: 0.481510, acc.: 79.69%] [G loss: 1.751982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21955 [D loss: 0.612318, acc.: 66.41%] [G loss: 1.495904]\n",
      "epoch:23 step:21956 [D loss: 0.594693, acc.: 67.19%] [G loss: 1.260798]\n",
      "epoch:23 step:21957 [D loss: 0.514237, acc.: 76.56%] [G loss: 1.353654]\n",
      "epoch:23 step:21958 [D loss: 0.734828, acc.: 54.69%] [G loss: 1.167629]\n",
      "epoch:23 step:21959 [D loss: 0.517216, acc.: 73.44%] [G loss: 1.423366]\n",
      "epoch:23 step:21960 [D loss: 0.513287, acc.: 76.56%] [G loss: 1.734005]\n",
      "epoch:23 step:21961 [D loss: 0.629322, acc.: 63.28%] [G loss: 1.081198]\n",
      "epoch:23 step:21962 [D loss: 0.467541, acc.: 78.91%] [G loss: 1.325887]\n",
      "epoch:23 step:21963 [D loss: 0.781096, acc.: 47.66%] [G loss: 1.135472]\n",
      "epoch:23 step:21964 [D loss: 0.604323, acc.: 64.84%] [G loss: 1.179882]\n",
      "epoch:23 step:21965 [D loss: 0.570943, acc.: 69.53%] [G loss: 1.482275]\n",
      "epoch:23 step:21966 [D loss: 0.647676, acc.: 64.84%] [G loss: 1.114119]\n",
      "epoch:23 step:21967 [D loss: 0.622800, acc.: 61.72%] [G loss: 1.322442]\n",
      "epoch:23 step:21968 [D loss: 0.617007, acc.: 65.62%] [G loss: 1.289059]\n",
      "epoch:23 step:21969 [D loss: 0.497209, acc.: 78.91%] [G loss: 1.558735]\n",
      "epoch:23 step:21970 [D loss: 0.439910, acc.: 83.59%] [G loss: 1.726701]\n",
      "epoch:23 step:21971 [D loss: 0.551758, acc.: 68.75%] [G loss: 1.647838]\n",
      "epoch:23 step:21972 [D loss: 0.649524, acc.: 64.84%] [G loss: 1.347564]\n",
      "epoch:23 step:21973 [D loss: 0.550745, acc.: 69.53%] [G loss: 1.056903]\n",
      "epoch:23 step:21974 [D loss: 0.520340, acc.: 75.78%] [G loss: 1.552735]\n",
      "epoch:23 step:21975 [D loss: 0.579863, acc.: 65.62%] [G loss: 1.235411]\n",
      "epoch:23 step:21976 [D loss: 0.487030, acc.: 77.34%] [G loss: 1.676300]\n",
      "epoch:23 step:21977 [D loss: 0.571137, acc.: 69.53%] [G loss: 1.441745]\n",
      "epoch:23 step:21978 [D loss: 0.559654, acc.: 71.09%] [G loss: 1.280808]\n",
      "epoch:23 step:21979 [D loss: 0.567473, acc.: 71.09%] [G loss: 1.521060]\n",
      "epoch:23 step:21980 [D loss: 0.743860, acc.: 55.47%] [G loss: 1.140586]\n",
      "epoch:23 step:21981 [D loss: 0.617024, acc.: 64.84%] [G loss: 0.887378]\n",
      "epoch:23 step:21982 [D loss: 0.440709, acc.: 79.69%] [G loss: 1.284037]\n",
      "epoch:23 step:21983 [D loss: 0.507071, acc.: 75.00%] [G loss: 1.652233]\n",
      "epoch:23 step:21984 [D loss: 0.566989, acc.: 71.09%] [G loss: 1.423258]\n",
      "epoch:23 step:21985 [D loss: 0.537019, acc.: 72.66%] [G loss: 1.397289]\n",
      "epoch:23 step:21986 [D loss: 0.462574, acc.: 78.12%] [G loss: 1.794106]\n",
      "epoch:23 step:21987 [D loss: 0.698448, acc.: 60.16%] [G loss: 1.273819]\n",
      "epoch:23 step:21988 [D loss: 0.432080, acc.: 83.59%] [G loss: 1.583920]\n",
      "epoch:23 step:21989 [D loss: 0.644464, acc.: 64.84%] [G loss: 0.953648]\n",
      "epoch:23 step:21990 [D loss: 0.459162, acc.: 81.25%] [G loss: 1.102544]\n",
      "epoch:23 step:21991 [D loss: 0.560321, acc.: 71.09%] [G loss: 1.324117]\n",
      "epoch:23 step:21992 [D loss: 0.565525, acc.: 67.19%] [G loss: 1.534256]\n",
      "epoch:23 step:21993 [D loss: 0.565905, acc.: 67.19%] [G loss: 1.458753]\n",
      "epoch:23 step:21994 [D loss: 0.520841, acc.: 77.34%] [G loss: 1.231959]\n",
      "epoch:23 step:21995 [D loss: 0.577045, acc.: 65.62%] [G loss: 1.194108]\n",
      "epoch:23 step:21996 [D loss: 0.577888, acc.: 72.66%] [G loss: 1.241093]\n",
      "epoch:23 step:21997 [D loss: 0.571392, acc.: 71.09%] [G loss: 0.994588]\n",
      "epoch:23 step:21998 [D loss: 0.413235, acc.: 84.38%] [G loss: 1.548552]\n",
      "epoch:23 step:21999 [D loss: 0.558717, acc.: 71.09%] [G loss: 1.399163]\n",
      "epoch:23 step:22000 [D loss: 0.582661, acc.: 67.19%] [G loss: 1.342620]\n",
      "##############\n",
      "[2.79518409 1.90832972 1.88870915 2.82430201 0.68793973 6.04889297\n",
      " 2.14780699 2.83582443 3.81982409 7.14868929]\n",
      "##########\n",
      "epoch:23 step:22001 [D loss: 0.464336, acc.: 82.81%] [G loss: 1.678072]\n",
      "epoch:23 step:22002 [D loss: 0.417619, acc.: 82.81%] [G loss: 1.597734]\n",
      "epoch:23 step:22003 [D loss: 0.564196, acc.: 77.34%] [G loss: 1.468215]\n",
      "epoch:23 step:22004 [D loss: 0.444421, acc.: 82.81%] [G loss: 1.300929]\n",
      "epoch:23 step:22005 [D loss: 0.475130, acc.: 79.69%] [G loss: 1.479400]\n",
      "epoch:23 step:22006 [D loss: 0.580513, acc.: 70.31%] [G loss: 1.489984]\n",
      "epoch:23 step:22007 [D loss: 0.502617, acc.: 76.56%] [G loss: 1.184551]\n",
      "epoch:23 step:22008 [D loss: 0.625804, acc.: 64.84%] [G loss: 0.919879]\n",
      "epoch:23 step:22009 [D loss: 0.488628, acc.: 76.56%] [G loss: 1.425366]\n",
      "epoch:23 step:22010 [D loss: 0.444414, acc.: 82.81%] [G loss: 1.502773]\n",
      "epoch:23 step:22011 [D loss: 0.458045, acc.: 80.47%] [G loss: 1.803275]\n",
      "epoch:23 step:22012 [D loss: 0.573781, acc.: 70.31%] [G loss: 1.261539]\n",
      "epoch:23 step:22013 [D loss: 0.753028, acc.: 53.12%] [G loss: 1.330315]\n",
      "epoch:23 step:22014 [D loss: 0.584794, acc.: 71.09%] [G loss: 1.389321]\n",
      "epoch:23 step:22015 [D loss: 0.493113, acc.: 75.78%] [G loss: 1.456509]\n",
      "epoch:23 step:22016 [D loss: 0.566212, acc.: 71.09%] [G loss: 1.350176]\n",
      "epoch:23 step:22017 [D loss: 0.496219, acc.: 76.56%] [G loss: 1.650393]\n",
      "epoch:23 step:22018 [D loss: 0.663288, acc.: 64.84%] [G loss: 1.313340]\n",
      "epoch:23 step:22019 [D loss: 0.431245, acc.: 84.38%] [G loss: 1.352110]\n",
      "epoch:23 step:22020 [D loss: 0.627965, acc.: 60.94%] [G loss: 1.082319]\n",
      "epoch:23 step:22021 [D loss: 0.627329, acc.: 66.41%] [G loss: 1.504305]\n",
      "epoch:23 step:22022 [D loss: 0.486331, acc.: 78.12%] [G loss: 1.374403]\n",
      "epoch:23 step:22023 [D loss: 0.485847, acc.: 75.00%] [G loss: 1.092306]\n",
      "epoch:23 step:22024 [D loss: 0.461092, acc.: 82.03%] [G loss: 1.394041]\n",
      "epoch:23 step:22025 [D loss: 0.638258, acc.: 64.06%] [G loss: 1.101694]\n",
      "epoch:23 step:22026 [D loss: 0.491123, acc.: 75.78%] [G loss: 1.680898]\n",
      "epoch:23 step:22027 [D loss: 0.660489, acc.: 61.72%] [G loss: 1.453792]\n",
      "epoch:23 step:22028 [D loss: 0.520432, acc.: 72.66%] [G loss: 1.839329]\n",
      "epoch:23 step:22029 [D loss: 0.583062, acc.: 68.75%] [G loss: 1.337080]\n",
      "epoch:23 step:22030 [D loss: 0.556278, acc.: 68.75%] [G loss: 1.254761]\n",
      "epoch:23 step:22031 [D loss: 0.672220, acc.: 60.16%] [G loss: 1.440223]\n",
      "epoch:23 step:22032 [D loss: 0.599467, acc.: 71.88%] [G loss: 1.689730]\n",
      "epoch:23 step:22033 [D loss: 0.579458, acc.: 74.22%] [G loss: 1.918905]\n",
      "epoch:23 step:22034 [D loss: 0.714379, acc.: 58.59%] [G loss: 0.944064]\n",
      "epoch:23 step:22035 [D loss: 0.774214, acc.: 52.34%] [G loss: 1.222427]\n",
      "epoch:23 step:22036 [D loss: 0.449776, acc.: 78.91%] [G loss: 1.319906]\n",
      "epoch:23 step:22037 [D loss: 0.427563, acc.: 85.16%] [G loss: 1.736088]\n",
      "epoch:23 step:22038 [D loss: 0.552408, acc.: 68.75%] [G loss: 1.320782]\n",
      "epoch:23 step:22039 [D loss: 0.485752, acc.: 76.56%] [G loss: 1.452387]\n",
      "epoch:23 step:22040 [D loss: 0.570354, acc.: 72.66%] [G loss: 1.231620]\n",
      "epoch:23 step:22041 [D loss: 0.444988, acc.: 79.69%] [G loss: 1.452651]\n",
      "epoch:23 step:22042 [D loss: 0.400009, acc.: 85.16%] [G loss: 1.805051]\n",
      "epoch:23 step:22043 [D loss: 0.683386, acc.: 61.72%] [G loss: 1.410388]\n",
      "epoch:23 step:22044 [D loss: 0.539051, acc.: 71.88%] [G loss: 1.536172]\n",
      "epoch:23 step:22045 [D loss: 0.619474, acc.: 63.28%] [G loss: 1.401346]\n",
      "epoch:23 step:22046 [D loss: 0.515580, acc.: 79.69%] [G loss: 1.193238]\n",
      "epoch:23 step:22047 [D loss: 0.598430, acc.: 67.97%] [G loss: 1.502345]\n",
      "epoch:23 step:22048 [D loss: 0.550185, acc.: 68.75%] [G loss: 1.351199]\n",
      "epoch:23 step:22049 [D loss: 0.502723, acc.: 76.56%] [G loss: 1.211617]\n",
      "epoch:23 step:22050 [D loss: 0.471054, acc.: 75.00%] [G loss: 1.363644]\n",
      "epoch:23 step:22051 [D loss: 0.504838, acc.: 75.00%] [G loss: 1.181004]\n",
      "epoch:23 step:22052 [D loss: 0.590779, acc.: 65.62%] [G loss: 1.439472]\n",
      "epoch:23 step:22053 [D loss: 0.384382, acc.: 85.94%] [G loss: 1.526703]\n",
      "epoch:23 step:22054 [D loss: 0.485672, acc.: 81.25%] [G loss: 1.301693]\n",
      "epoch:23 step:22055 [D loss: 0.680957, acc.: 62.50%] [G loss: 1.360296]\n",
      "epoch:23 step:22056 [D loss: 0.438173, acc.: 83.59%] [G loss: 1.855560]\n",
      "epoch:23 step:22057 [D loss: 0.554516, acc.: 69.53%] [G loss: 1.389763]\n",
      "epoch:23 step:22058 [D loss: 0.657189, acc.: 60.16%] [G loss: 1.356079]\n",
      "epoch:23 step:22059 [D loss: 0.550950, acc.: 72.66%] [G loss: 1.414192]\n",
      "epoch:23 step:22060 [D loss: 0.411304, acc.: 84.38%] [G loss: 1.198882]\n",
      "epoch:23 step:22061 [D loss: 0.684759, acc.: 60.94%] [G loss: 1.580293]\n",
      "epoch:23 step:22062 [D loss: 0.558977, acc.: 69.53%] [G loss: 0.992480]\n",
      "epoch:23 step:22063 [D loss: 0.642797, acc.: 64.84%] [G loss: 1.463285]\n",
      "epoch:23 step:22064 [D loss: 0.626240, acc.: 62.50%] [G loss: 1.629954]\n",
      "epoch:23 step:22065 [D loss: 0.712057, acc.: 60.94%] [G loss: 1.409939]\n",
      "epoch:23 step:22066 [D loss: 0.642386, acc.: 61.72%] [G loss: 1.157626]\n",
      "epoch:23 step:22067 [D loss: 0.493065, acc.: 78.12%] [G loss: 1.723950]\n",
      "epoch:23 step:22068 [D loss: 0.512858, acc.: 77.34%] [G loss: 1.513538]\n",
      "epoch:23 step:22069 [D loss: 0.489707, acc.: 76.56%] [G loss: 1.443981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22070 [D loss: 0.537212, acc.: 71.88%] [G loss: 1.698293]\n",
      "epoch:23 step:22071 [D loss: 0.567197, acc.: 69.53%] [G loss: 1.258869]\n",
      "epoch:23 step:22072 [D loss: 0.441293, acc.: 82.81%] [G loss: 1.262847]\n",
      "epoch:23 step:22073 [D loss: 0.730789, acc.: 49.22%] [G loss: 1.048366]\n",
      "epoch:23 step:22074 [D loss: 0.637214, acc.: 67.97%] [G loss: 1.304994]\n",
      "epoch:23 step:22075 [D loss: 0.600566, acc.: 64.84%] [G loss: 1.535580]\n",
      "epoch:23 step:22076 [D loss: 0.452807, acc.: 77.34%] [G loss: 1.510912]\n",
      "epoch:23 step:22077 [D loss: 0.529168, acc.: 75.00%] [G loss: 1.836138]\n",
      "epoch:23 step:22078 [D loss: 0.623987, acc.: 64.84%] [G loss: 1.676952]\n",
      "epoch:23 step:22079 [D loss: 0.514154, acc.: 75.78%] [G loss: 1.100027]\n",
      "epoch:23 step:22080 [D loss: 0.580386, acc.: 75.00%] [G loss: 1.404272]\n",
      "epoch:23 step:22081 [D loss: 0.741429, acc.: 55.47%] [G loss: 1.044128]\n",
      "epoch:23 step:22082 [D loss: 0.694183, acc.: 56.25%] [G loss: 1.174365]\n",
      "epoch:23 step:22083 [D loss: 0.541461, acc.: 69.53%] [G loss: 1.180403]\n",
      "epoch:23 step:22084 [D loss: 0.713686, acc.: 60.16%] [G loss: 1.028116]\n",
      "epoch:23 step:22085 [D loss: 0.568823, acc.: 68.75%] [G loss: 1.663250]\n",
      "epoch:23 step:22086 [D loss: 0.454576, acc.: 85.16%] [G loss: 1.658617]\n",
      "epoch:23 step:22087 [D loss: 0.758883, acc.: 57.81%] [G loss: 1.790294]\n",
      "epoch:23 step:22088 [D loss: 0.561921, acc.: 72.66%] [G loss: 1.266361]\n",
      "epoch:23 step:22089 [D loss: 0.564521, acc.: 71.09%] [G loss: 1.480708]\n",
      "epoch:23 step:22090 [D loss: 0.687491, acc.: 64.84%] [G loss: 1.168195]\n",
      "epoch:23 step:22091 [D loss: 0.295700, acc.: 92.97%] [G loss: 1.699548]\n",
      "epoch:23 step:22092 [D loss: 0.559553, acc.: 67.97%] [G loss: 1.499331]\n",
      "epoch:23 step:22093 [D loss: 0.520190, acc.: 76.56%] [G loss: 1.257000]\n",
      "epoch:23 step:22094 [D loss: 0.656367, acc.: 61.72%] [G loss: 1.355988]\n",
      "epoch:23 step:22095 [D loss: 0.457495, acc.: 80.47%] [G loss: 1.365387]\n",
      "epoch:23 step:22096 [D loss: 0.556786, acc.: 75.00%] [G loss: 1.313315]\n",
      "epoch:23 step:22097 [D loss: 0.538360, acc.: 72.66%] [G loss: 1.335626]\n",
      "epoch:23 step:22098 [D loss: 0.551794, acc.: 73.44%] [G loss: 1.327718]\n",
      "epoch:23 step:22099 [D loss: 0.485767, acc.: 75.00%] [G loss: 1.538198]\n",
      "epoch:23 step:22100 [D loss: 0.471815, acc.: 78.91%] [G loss: 1.114240]\n",
      "epoch:23 step:22101 [D loss: 0.616799, acc.: 61.72%] [G loss: 1.165175]\n",
      "epoch:23 step:22102 [D loss: 0.655795, acc.: 70.31%] [G loss: 1.327657]\n",
      "epoch:23 step:22103 [D loss: 0.404042, acc.: 84.38%] [G loss: 1.364002]\n",
      "epoch:23 step:22104 [D loss: 0.698852, acc.: 64.84%] [G loss: 1.394609]\n",
      "epoch:23 step:22105 [D loss: 0.661988, acc.: 60.94%] [G loss: 1.337586]\n",
      "epoch:23 step:22106 [D loss: 0.442154, acc.: 81.25%] [G loss: 1.277047]\n",
      "epoch:23 step:22107 [D loss: 0.598315, acc.: 67.97%] [G loss: 1.237485]\n",
      "epoch:23 step:22108 [D loss: 0.446693, acc.: 78.91%] [G loss: 1.311501]\n",
      "epoch:23 step:22109 [D loss: 0.576070, acc.: 70.31%] [G loss: 1.410832]\n",
      "epoch:23 step:22110 [D loss: 0.585763, acc.: 64.06%] [G loss: 1.058009]\n",
      "epoch:23 step:22111 [D loss: 0.494287, acc.: 77.34%] [G loss: 1.343408]\n",
      "epoch:23 step:22112 [D loss: 0.601295, acc.: 67.19%] [G loss: 1.250515]\n",
      "epoch:23 step:22113 [D loss: 0.640456, acc.: 64.84%] [G loss: 1.431703]\n",
      "epoch:23 step:22114 [D loss: 0.398404, acc.: 85.94%] [G loss: 1.504474]\n",
      "epoch:23 step:22115 [D loss: 0.409039, acc.: 83.59%] [G loss: 1.596403]\n",
      "epoch:23 step:22116 [D loss: 0.671465, acc.: 60.16%] [G loss: 1.388475]\n",
      "epoch:23 step:22117 [D loss: 0.445905, acc.: 84.38%] [G loss: 1.420489]\n",
      "epoch:23 step:22118 [D loss: 0.539289, acc.: 69.53%] [G loss: 1.339004]\n",
      "epoch:23 step:22119 [D loss: 0.571193, acc.: 74.22%] [G loss: 1.702732]\n",
      "epoch:23 step:22120 [D loss: 0.567573, acc.: 71.88%] [G loss: 1.450719]\n",
      "epoch:23 step:22121 [D loss: 0.482844, acc.: 78.91%] [G loss: 1.570318]\n",
      "epoch:23 step:22122 [D loss: 0.547942, acc.: 75.78%] [G loss: 1.431626]\n",
      "epoch:23 step:22123 [D loss: 0.569493, acc.: 73.44%] [G loss: 1.528652]\n",
      "epoch:23 step:22124 [D loss: 0.591076, acc.: 67.97%] [G loss: 1.594234]\n",
      "epoch:23 step:22125 [D loss: 0.600710, acc.: 70.31%] [G loss: 1.356807]\n",
      "epoch:23 step:22126 [D loss: 0.550303, acc.: 71.09%] [G loss: 1.531669]\n",
      "epoch:23 step:22127 [D loss: 0.705716, acc.: 58.59%] [G loss: 0.934218]\n",
      "epoch:23 step:22128 [D loss: 0.622764, acc.: 67.19%] [G loss: 1.349388]\n",
      "epoch:23 step:22129 [D loss: 0.625409, acc.: 67.19%] [G loss: 1.474664]\n",
      "epoch:23 step:22130 [D loss: 0.469327, acc.: 78.12%] [G loss: 1.464668]\n",
      "epoch:23 step:22131 [D loss: 0.581337, acc.: 69.53%] [G loss: 1.076777]\n",
      "epoch:23 step:22132 [D loss: 0.558198, acc.: 71.88%] [G loss: 1.634585]\n",
      "epoch:23 step:22133 [D loss: 0.517217, acc.: 74.22%] [G loss: 1.257869]\n",
      "epoch:23 step:22134 [D loss: 0.611862, acc.: 67.97%] [G loss: 1.203350]\n",
      "epoch:23 step:22135 [D loss: 0.585884, acc.: 66.41%] [G loss: 1.322519]\n",
      "epoch:23 step:22136 [D loss: 0.606711, acc.: 63.28%] [G loss: 0.951847]\n",
      "epoch:23 step:22137 [D loss: 0.559718, acc.: 75.00%] [G loss: 1.399705]\n",
      "epoch:23 step:22138 [D loss: 0.516686, acc.: 69.53%] [G loss: 1.551674]\n",
      "epoch:23 step:22139 [D loss: 0.682244, acc.: 59.38%] [G loss: 1.577364]\n",
      "epoch:23 step:22140 [D loss: 0.531728, acc.: 71.88%] [G loss: 1.311633]\n",
      "epoch:23 step:22141 [D loss: 0.427770, acc.: 77.34%] [G loss: 1.451189]\n",
      "epoch:23 step:22142 [D loss: 0.648537, acc.: 62.50%] [G loss: 1.274762]\n",
      "epoch:23 step:22143 [D loss: 0.554638, acc.: 71.09%] [G loss: 1.220336]\n",
      "epoch:23 step:22144 [D loss: 0.801188, acc.: 48.44%] [G loss: 0.867604]\n",
      "epoch:23 step:22145 [D loss: 0.541264, acc.: 72.66%] [G loss: 1.764474]\n",
      "epoch:23 step:22146 [D loss: 0.481065, acc.: 75.00%] [G loss: 1.492800]\n",
      "epoch:23 step:22147 [D loss: 0.484704, acc.: 75.00%] [G loss: 1.459995]\n",
      "epoch:23 step:22148 [D loss: 0.612642, acc.: 64.84%] [G loss: 1.510757]\n",
      "epoch:23 step:22149 [D loss: 0.601895, acc.: 71.09%] [G loss: 1.607225]\n",
      "epoch:23 step:22150 [D loss: 0.530499, acc.: 74.22%] [G loss: 1.762245]\n",
      "epoch:23 step:22151 [D loss: 0.351814, acc.: 87.50%] [G loss: 1.475127]\n",
      "epoch:23 step:22152 [D loss: 0.797990, acc.: 55.47%] [G loss: 1.249335]\n",
      "epoch:23 step:22153 [D loss: 0.394339, acc.: 87.50%] [G loss: 1.555791]\n",
      "epoch:23 step:22154 [D loss: 0.567776, acc.: 67.97%] [G loss: 1.414397]\n",
      "epoch:23 step:22155 [D loss: 0.779910, acc.: 56.25%] [G loss: 1.180538]\n",
      "epoch:23 step:22156 [D loss: 0.595964, acc.: 68.75%] [G loss: 1.198290]\n",
      "epoch:23 step:22157 [D loss: 0.588039, acc.: 71.09%] [G loss: 1.131427]\n",
      "epoch:23 step:22158 [D loss: 0.456280, acc.: 78.91%] [G loss: 1.773274]\n",
      "epoch:23 step:22159 [D loss: 0.473327, acc.: 75.78%] [G loss: 1.196818]\n",
      "epoch:23 step:22160 [D loss: 0.534827, acc.: 75.00%] [G loss: 1.095388]\n",
      "epoch:23 step:22161 [D loss: 0.537520, acc.: 72.66%] [G loss: 1.469893]\n",
      "epoch:23 step:22162 [D loss: 0.535391, acc.: 72.66%] [G loss: 1.222819]\n",
      "epoch:23 step:22163 [D loss: 0.578487, acc.: 70.31%] [G loss: 1.267731]\n",
      "epoch:23 step:22164 [D loss: 0.462030, acc.: 78.91%] [G loss: 1.418831]\n",
      "epoch:23 step:22165 [D loss: 0.584634, acc.: 67.19%] [G loss: 1.499710]\n",
      "epoch:23 step:22166 [D loss: 0.651090, acc.: 62.50%] [G loss: 1.617413]\n",
      "epoch:23 step:22167 [D loss: 0.500615, acc.: 73.44%] [G loss: 1.630045]\n",
      "epoch:23 step:22168 [D loss: 0.506282, acc.: 76.56%] [G loss: 1.612224]\n",
      "epoch:23 step:22169 [D loss: 0.445939, acc.: 79.69%] [G loss: 1.681913]\n",
      "epoch:23 step:22170 [D loss: 0.648401, acc.: 62.50%] [G loss: 1.366805]\n",
      "epoch:23 step:22171 [D loss: 0.654952, acc.: 61.72%] [G loss: 1.123466]\n",
      "epoch:23 step:22172 [D loss: 0.656096, acc.: 66.41%] [G loss: 1.237249]\n",
      "epoch:23 step:22173 [D loss: 0.474196, acc.: 82.03%] [G loss: 1.091727]\n",
      "epoch:23 step:22174 [D loss: 0.604756, acc.: 65.62%] [G loss: 1.184671]\n",
      "epoch:23 step:22175 [D loss: 0.537389, acc.: 74.22%] [G loss: 1.462999]\n",
      "epoch:23 step:22176 [D loss: 0.432820, acc.: 79.69%] [G loss: 1.723472]\n",
      "epoch:23 step:22177 [D loss: 0.571941, acc.: 73.44%] [G loss: 1.423973]\n",
      "epoch:23 step:22178 [D loss: 0.558009, acc.: 75.00%] [G loss: 1.461185]\n",
      "epoch:23 step:22179 [D loss: 0.698010, acc.: 60.94%] [G loss: 1.189231]\n",
      "epoch:23 step:22180 [D loss: 0.513940, acc.: 78.12%] [G loss: 1.154125]\n",
      "epoch:23 step:22181 [D loss: 0.736696, acc.: 51.56%] [G loss: 1.481339]\n",
      "epoch:23 step:22182 [D loss: 0.511599, acc.: 76.56%] [G loss: 1.441632]\n",
      "epoch:23 step:22183 [D loss: 0.579464, acc.: 72.66%] [G loss: 1.334266]\n",
      "epoch:23 step:22184 [D loss: 0.481404, acc.: 77.34%] [G loss: 1.663742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22185 [D loss: 0.484636, acc.: 76.56%] [G loss: 1.354488]\n",
      "epoch:23 step:22186 [D loss: 0.560650, acc.: 74.22%] [G loss: 1.618250]\n",
      "epoch:23 step:22187 [D loss: 0.443348, acc.: 78.12%] [G loss: 1.260247]\n",
      "epoch:23 step:22188 [D loss: 0.594074, acc.: 70.31%] [G loss: 1.507990]\n",
      "epoch:23 step:22189 [D loss: 0.808232, acc.: 51.56%] [G loss: 1.108291]\n",
      "epoch:23 step:22190 [D loss: 0.455081, acc.: 78.91%] [G loss: 1.483079]\n",
      "epoch:23 step:22191 [D loss: 0.368967, acc.: 89.06%] [G loss: 1.446708]\n",
      "epoch:23 step:22192 [D loss: 0.588731, acc.: 73.44%] [G loss: 1.077385]\n",
      "epoch:23 step:22193 [D loss: 0.496196, acc.: 78.12%] [G loss: 1.072693]\n",
      "epoch:23 step:22194 [D loss: 0.638383, acc.: 64.06%] [G loss: 1.176336]\n",
      "epoch:23 step:22195 [D loss: 0.581943, acc.: 70.31%] [G loss: 1.826033]\n",
      "epoch:23 step:22196 [D loss: 0.517630, acc.: 71.09%] [G loss: 1.570212]\n",
      "epoch:23 step:22197 [D loss: 0.548482, acc.: 73.44%] [G loss: 1.453661]\n",
      "epoch:23 step:22198 [D loss: 0.786628, acc.: 53.91%] [G loss: 0.971736]\n",
      "epoch:23 step:22199 [D loss: 0.531820, acc.: 71.88%] [G loss: 1.242653]\n",
      "epoch:23 step:22200 [D loss: 0.651298, acc.: 63.28%] [G loss: 1.567713]\n",
      "##############\n",
      "[2.71528357 2.16137327 1.76869529 2.92046322 1.01901543 6.15358909\n",
      " 2.53569658 2.94247379 4.05452508 8.14868929]\n",
      "##########\n",
      "epoch:23 step:22201 [D loss: 0.674638, acc.: 57.03%] [G loss: 1.427629]\n",
      "epoch:23 step:22202 [D loss: 0.319063, acc.: 92.19%] [G loss: 1.913101]\n",
      "epoch:23 step:22203 [D loss: 0.513385, acc.: 75.78%] [G loss: 1.369230]\n",
      "epoch:23 step:22204 [D loss: 0.558822, acc.: 73.44%] [G loss: 1.643812]\n",
      "epoch:23 step:22205 [D loss: 0.587462, acc.: 69.53%] [G loss: 1.544842]\n",
      "epoch:23 step:22206 [D loss: 0.655858, acc.: 62.50%] [G loss: 1.485797]\n",
      "epoch:23 step:22207 [D loss: 0.448746, acc.: 82.81%] [G loss: 1.623454]\n",
      "epoch:23 step:22208 [D loss: 0.674083, acc.: 60.94%] [G loss: 1.328920]\n",
      "epoch:23 step:22209 [D loss: 0.418291, acc.: 83.59%] [G loss: 1.407449]\n",
      "epoch:23 step:22210 [D loss: 0.557885, acc.: 71.88%] [G loss: 1.458911]\n",
      "epoch:23 step:22211 [D loss: 0.616908, acc.: 66.41%] [G loss: 1.633075]\n",
      "epoch:23 step:22212 [D loss: 0.519667, acc.: 74.22%] [G loss: 1.213559]\n",
      "epoch:23 step:22213 [D loss: 0.680615, acc.: 60.94%] [G loss: 1.062134]\n",
      "epoch:23 step:22214 [D loss: 0.526163, acc.: 72.66%] [G loss: 1.108700]\n",
      "epoch:23 step:22215 [D loss: 0.486940, acc.: 76.56%] [G loss: 1.487296]\n",
      "epoch:23 step:22216 [D loss: 0.478750, acc.: 79.69%] [G loss: 1.368097]\n",
      "epoch:23 step:22217 [D loss: 0.484519, acc.: 75.78%] [G loss: 1.476520]\n",
      "epoch:23 step:22218 [D loss: 0.534563, acc.: 76.56%] [G loss: 1.165448]\n",
      "epoch:23 step:22219 [D loss: 0.576118, acc.: 70.31%] [G loss: 1.236008]\n",
      "epoch:23 step:22220 [D loss: 0.533418, acc.: 76.56%] [G loss: 1.249961]\n",
      "epoch:23 step:22221 [D loss: 0.561081, acc.: 72.66%] [G loss: 1.687699]\n",
      "epoch:23 step:22222 [D loss: 0.576337, acc.: 71.09%] [G loss: 1.342127]\n",
      "epoch:23 step:22223 [D loss: 0.462139, acc.: 77.34%] [G loss: 1.544609]\n",
      "epoch:23 step:22224 [D loss: 0.671277, acc.: 60.16%] [G loss: 1.219451]\n",
      "epoch:23 step:22225 [D loss: 0.579190, acc.: 71.09%] [G loss: 1.731211]\n",
      "epoch:23 step:22226 [D loss: 0.689800, acc.: 63.28%] [G loss: 1.308049]\n",
      "epoch:23 step:22227 [D loss: 0.500910, acc.: 78.91%] [G loss: 1.282239]\n",
      "epoch:23 step:22228 [D loss: 0.485960, acc.: 75.78%] [G loss: 1.252895]\n",
      "epoch:23 step:22229 [D loss: 0.630044, acc.: 64.06%] [G loss: 1.457903]\n",
      "epoch:23 step:22230 [D loss: 0.567285, acc.: 72.66%] [G loss: 1.455664]\n",
      "epoch:23 step:22231 [D loss: 0.626800, acc.: 64.06%] [G loss: 1.559723]\n",
      "epoch:23 step:22232 [D loss: 0.586654, acc.: 71.09%] [G loss: 1.409978]\n",
      "epoch:23 step:22233 [D loss: 0.622529, acc.: 68.75%] [G loss: 1.430081]\n",
      "epoch:23 step:22234 [D loss: 0.910210, acc.: 46.09%] [G loss: 1.479327]\n",
      "epoch:23 step:22235 [D loss: 0.633294, acc.: 64.06%] [G loss: 1.263594]\n",
      "epoch:23 step:22236 [D loss: 0.812438, acc.: 43.75%] [G loss: 0.770775]\n",
      "epoch:23 step:22237 [D loss: 0.604184, acc.: 64.84%] [G loss: 1.337209]\n",
      "epoch:23 step:22238 [D loss: 0.425967, acc.: 79.69%] [G loss: 1.377263]\n",
      "epoch:23 step:22239 [D loss: 0.506890, acc.: 75.00%] [G loss: 1.223855]\n",
      "epoch:23 step:22240 [D loss: 0.668361, acc.: 61.72%] [G loss: 0.936139]\n",
      "epoch:23 step:22241 [D loss: 0.505963, acc.: 77.34%] [G loss: 1.765331]\n",
      "epoch:23 step:22242 [D loss: 0.584268, acc.: 67.97%] [G loss: 1.255698]\n",
      "epoch:23 step:22243 [D loss: 0.557209, acc.: 69.53%] [G loss: 0.905774]\n",
      "epoch:23 step:22244 [D loss: 0.633204, acc.: 61.72%] [G loss: 1.402074]\n",
      "epoch:23 step:22245 [D loss: 0.583468, acc.: 63.28%] [G loss: 1.263672]\n",
      "epoch:23 step:22246 [D loss: 0.399333, acc.: 84.38%] [G loss: 1.481479]\n",
      "epoch:23 step:22247 [D loss: 0.531501, acc.: 71.09%] [G loss: 1.302473]\n",
      "epoch:23 step:22248 [D loss: 0.557398, acc.: 72.66%] [G loss: 1.227490]\n",
      "epoch:23 step:22249 [D loss: 0.458563, acc.: 79.69%] [G loss: 1.359879]\n",
      "epoch:23 step:22250 [D loss: 0.536204, acc.: 67.97%] [G loss: 1.138624]\n",
      "epoch:23 step:22251 [D loss: 0.740132, acc.: 53.12%] [G loss: 1.150789]\n",
      "epoch:23 step:22252 [D loss: 0.357386, acc.: 89.06%] [G loss: 1.046984]\n",
      "epoch:23 step:22253 [D loss: 0.498100, acc.: 75.78%] [G loss: 1.379398]\n",
      "epoch:23 step:22254 [D loss: 0.700283, acc.: 57.81%] [G loss: 1.531524]\n",
      "epoch:23 step:22255 [D loss: 0.769218, acc.: 50.78%] [G loss: 1.106856]\n",
      "epoch:23 step:22256 [D loss: 0.536123, acc.: 70.31%] [G loss: 0.913133]\n",
      "epoch:23 step:22257 [D loss: 0.693118, acc.: 62.50%] [G loss: 1.166838]\n",
      "epoch:23 step:22258 [D loss: 0.381271, acc.: 89.84%] [G loss: 1.815428]\n",
      "epoch:23 step:22259 [D loss: 0.618875, acc.: 64.84%] [G loss: 1.329485]\n",
      "epoch:23 step:22260 [D loss: 0.688510, acc.: 65.62%] [G loss: 1.072965]\n",
      "epoch:23 step:22261 [D loss: 0.547335, acc.: 73.44%] [G loss: 1.258018]\n",
      "epoch:23 step:22262 [D loss: 0.516440, acc.: 72.66%] [G loss: 1.142453]\n",
      "epoch:23 step:22263 [D loss: 0.558571, acc.: 68.75%] [G loss: 1.600383]\n",
      "epoch:23 step:22264 [D loss: 0.604695, acc.: 64.84%] [G loss: 1.381656]\n",
      "epoch:23 step:22265 [D loss: 0.644555, acc.: 60.16%] [G loss: 1.231657]\n",
      "epoch:23 step:22266 [D loss: 0.574899, acc.: 68.75%] [G loss: 1.039559]\n",
      "epoch:23 step:22267 [D loss: 0.529974, acc.: 76.56%] [G loss: 1.233264]\n",
      "epoch:23 step:22268 [D loss: 0.660714, acc.: 61.72%] [G loss: 1.033490]\n",
      "epoch:23 step:22269 [D loss: 0.366808, acc.: 91.41%] [G loss: 1.343894]\n",
      "epoch:23 step:22270 [D loss: 0.679836, acc.: 62.50%] [G loss: 1.195387]\n",
      "epoch:23 step:22271 [D loss: 0.555145, acc.: 73.44%] [G loss: 1.123134]\n",
      "epoch:23 step:22272 [D loss: 0.592166, acc.: 67.19%] [G loss: 1.078307]\n",
      "epoch:23 step:22273 [D loss: 0.576137, acc.: 68.75%] [G loss: 1.723747]\n",
      "epoch:23 step:22274 [D loss: 0.575288, acc.: 73.44%] [G loss: 1.094625]\n",
      "epoch:23 step:22275 [D loss: 0.522313, acc.: 77.34%] [G loss: 1.041561]\n",
      "epoch:23 step:22276 [D loss: 0.467130, acc.: 76.56%] [G loss: 1.407779]\n",
      "epoch:23 step:22277 [D loss: 0.665676, acc.: 62.50%] [G loss: 1.167583]\n",
      "epoch:23 step:22278 [D loss: 0.468061, acc.: 74.22%] [G loss: 0.973197]\n",
      "epoch:23 step:22279 [D loss: 0.653423, acc.: 62.50%] [G loss: 1.320373]\n",
      "epoch:23 step:22280 [D loss: 0.634441, acc.: 57.81%] [G loss: 1.283978]\n",
      "epoch:23 step:22281 [D loss: 0.669640, acc.: 60.94%] [G loss: 1.276577]\n",
      "epoch:23 step:22282 [D loss: 0.614735, acc.: 65.62%] [G loss: 1.496828]\n",
      "epoch:23 step:22283 [D loss: 0.497261, acc.: 72.66%] [G loss: 1.374073]\n",
      "epoch:23 step:22284 [D loss: 0.589597, acc.: 60.16%] [G loss: 1.269957]\n",
      "epoch:23 step:22285 [D loss: 0.525895, acc.: 74.22%] [G loss: 1.365260]\n",
      "epoch:23 step:22286 [D loss: 0.451125, acc.: 80.47%] [G loss: 1.380532]\n",
      "epoch:23 step:22287 [D loss: 0.553724, acc.: 68.75%] [G loss: 1.256834]\n",
      "epoch:23 step:22288 [D loss: 0.527787, acc.: 71.09%] [G loss: 1.305427]\n",
      "epoch:23 step:22289 [D loss: 0.723374, acc.: 57.03%] [G loss: 1.049745]\n",
      "epoch:23 step:22290 [D loss: 0.589002, acc.: 67.97%] [G loss: 1.076241]\n",
      "epoch:23 step:22291 [D loss: 0.530494, acc.: 75.78%] [G loss: 1.217175]\n",
      "epoch:23 step:22292 [D loss: 0.532645, acc.: 73.44%] [G loss: 1.414339]\n",
      "epoch:23 step:22293 [D loss: 0.429390, acc.: 81.25%] [G loss: 1.526435]\n",
      "epoch:23 step:22294 [D loss: 0.661628, acc.: 67.97%] [G loss: 1.505397]\n",
      "epoch:23 step:22295 [D loss: 0.514343, acc.: 75.00%] [G loss: 1.250767]\n",
      "epoch:23 step:22296 [D loss: 0.565046, acc.: 71.09%] [G loss: 0.970383]\n",
      "epoch:23 step:22297 [D loss: 0.580966, acc.: 68.75%] [G loss: 1.578113]\n",
      "epoch:23 step:22298 [D loss: 0.441372, acc.: 82.03%] [G loss: 1.382531]\n",
      "epoch:23 step:22299 [D loss: 0.472879, acc.: 81.25%] [G loss: 1.727669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22300 [D loss: 0.852214, acc.: 46.09%] [G loss: 0.999673]\n",
      "epoch:23 step:22301 [D loss: 0.501772, acc.: 73.44%] [G loss: 1.191980]\n",
      "epoch:23 step:22302 [D loss: 0.592905, acc.: 72.66%] [G loss: 0.967436]\n",
      "epoch:23 step:22303 [D loss: 0.480041, acc.: 78.91%] [G loss: 1.352281]\n",
      "epoch:23 step:22304 [D loss: 0.493933, acc.: 78.12%] [G loss: 1.496536]\n",
      "epoch:23 step:22305 [D loss: 0.504694, acc.: 76.56%] [G loss: 1.361346]\n",
      "epoch:23 step:22306 [D loss: 0.552255, acc.: 71.09%] [G loss: 1.092848]\n",
      "epoch:23 step:22307 [D loss: 0.524540, acc.: 74.22%] [G loss: 1.194134]\n",
      "epoch:23 step:22308 [D loss: 0.561567, acc.: 69.53%] [G loss: 1.354591]\n",
      "epoch:23 step:22309 [D loss: 0.580244, acc.: 71.09%] [G loss: 1.357837]\n",
      "epoch:23 step:22310 [D loss: 0.509999, acc.: 75.00%] [G loss: 1.218376]\n",
      "epoch:23 step:22311 [D loss: 0.516124, acc.: 71.88%] [G loss: 1.432663]\n",
      "epoch:23 step:22312 [D loss: 0.539693, acc.: 71.88%] [G loss: 1.343435]\n",
      "epoch:23 step:22313 [D loss: 0.549988, acc.: 69.53%] [G loss: 1.172723]\n",
      "epoch:23 step:22314 [D loss: 0.501606, acc.: 75.00%] [G loss: 1.227359]\n",
      "epoch:23 step:22315 [D loss: 0.384677, acc.: 86.72%] [G loss: 1.622346]\n",
      "epoch:23 step:22316 [D loss: 0.572871, acc.: 70.31%] [G loss: 1.577889]\n",
      "epoch:23 step:22317 [D loss: 0.566000, acc.: 72.66%] [G loss: 1.321933]\n",
      "epoch:23 step:22318 [D loss: 0.498742, acc.: 78.12%] [G loss: 1.478110]\n",
      "epoch:23 step:22319 [D loss: 0.549479, acc.: 74.22%] [G loss: 1.616887]\n",
      "epoch:23 step:22320 [D loss: 0.618329, acc.: 66.41%] [G loss: 1.463323]\n",
      "epoch:23 step:22321 [D loss: 0.586703, acc.: 70.31%] [G loss: 1.790120]\n",
      "epoch:23 step:22322 [D loss: 0.476214, acc.: 74.22%] [G loss: 1.258004]\n",
      "epoch:23 step:22323 [D loss: 0.442556, acc.: 80.47%] [G loss: 1.373264]\n",
      "epoch:23 step:22324 [D loss: 0.461857, acc.: 82.03%] [G loss: 1.416315]\n",
      "epoch:23 step:22325 [D loss: 0.473698, acc.: 81.25%] [G loss: 1.316954]\n",
      "epoch:23 step:22326 [D loss: 0.562647, acc.: 68.75%] [G loss: 1.684472]\n",
      "epoch:23 step:22327 [D loss: 0.478709, acc.: 75.78%] [G loss: 1.675757]\n",
      "epoch:23 step:22328 [D loss: 0.567375, acc.: 71.09%] [G loss: 1.349856]\n",
      "epoch:23 step:22329 [D loss: 0.650889, acc.: 65.62%] [G loss: 1.696223]\n",
      "epoch:23 step:22330 [D loss: 0.557495, acc.: 74.22%] [G loss: 1.492988]\n",
      "epoch:23 step:22331 [D loss: 0.614075, acc.: 67.19%] [G loss: 1.146144]\n",
      "epoch:23 step:22332 [D loss: 0.599336, acc.: 66.41%] [G loss: 1.018889]\n",
      "epoch:23 step:22333 [D loss: 0.448475, acc.: 78.91%] [G loss: 1.440473]\n",
      "epoch:23 step:22334 [D loss: 0.564289, acc.: 69.53%] [G loss: 1.427369]\n",
      "epoch:23 step:22335 [D loss: 0.541222, acc.: 73.44%] [G loss: 1.544805]\n",
      "epoch:23 step:22336 [D loss: 0.446580, acc.: 81.25%] [G loss: 1.287024]\n",
      "epoch:23 step:22337 [D loss: 0.675359, acc.: 60.16%] [G loss: 1.643290]\n",
      "epoch:23 step:22338 [D loss: 0.549752, acc.: 69.53%] [G loss: 1.255208]\n",
      "epoch:23 step:22339 [D loss: 0.483916, acc.: 82.81%] [G loss: 1.217991]\n",
      "epoch:23 step:22340 [D loss: 0.558963, acc.: 67.19%] [G loss: 1.100022]\n",
      "epoch:23 step:22341 [D loss: 0.673366, acc.: 58.59%] [G loss: 1.483366]\n",
      "epoch:23 step:22342 [D loss: 0.567675, acc.: 69.53%] [G loss: 1.335956]\n",
      "epoch:23 step:22343 [D loss: 0.481379, acc.: 77.34%] [G loss: 1.460452]\n",
      "epoch:23 step:22344 [D loss: 0.640345, acc.: 70.31%] [G loss: 1.611666]\n",
      "epoch:23 step:22345 [D loss: 0.585240, acc.: 72.66%] [G loss: 1.278235]\n",
      "epoch:23 step:22346 [D loss: 0.535381, acc.: 74.22%] [G loss: 1.083166]\n",
      "epoch:23 step:22347 [D loss: 0.530515, acc.: 78.12%] [G loss: 1.444407]\n",
      "epoch:23 step:22348 [D loss: 0.449840, acc.: 76.56%] [G loss: 1.157770]\n",
      "epoch:23 step:22349 [D loss: 0.477679, acc.: 76.56%] [G loss: 1.282991]\n",
      "epoch:23 step:22350 [D loss: 0.608542, acc.: 64.06%] [G loss: 1.312041]\n",
      "epoch:23 step:22351 [D loss: 0.592104, acc.: 69.53%] [G loss: 1.197331]\n",
      "epoch:23 step:22352 [D loss: 0.612090, acc.: 70.31%] [G loss: 1.355663]\n",
      "epoch:23 step:22353 [D loss: 0.555013, acc.: 69.53%] [G loss: 1.200749]\n",
      "epoch:23 step:22354 [D loss: 0.607734, acc.: 65.62%] [G loss: 1.365002]\n",
      "epoch:23 step:22355 [D loss: 0.509149, acc.: 79.69%] [G loss: 1.166942]\n",
      "epoch:23 step:22356 [D loss: 0.634765, acc.: 64.06%] [G loss: 1.351399]\n",
      "epoch:23 step:22357 [D loss: 0.428215, acc.: 86.72%] [G loss: 1.416965]\n",
      "epoch:23 step:22358 [D loss: 0.607333, acc.: 66.41%] [G loss: 1.146690]\n",
      "epoch:23 step:22359 [D loss: 0.475212, acc.: 78.91%] [G loss: 2.041691]\n",
      "epoch:23 step:22360 [D loss: 0.508161, acc.: 80.47%] [G loss: 1.412084]\n",
      "epoch:23 step:22361 [D loss: 0.656187, acc.: 64.06%] [G loss: 1.426949]\n",
      "epoch:23 step:22362 [D loss: 0.715243, acc.: 61.72%] [G loss: 1.402591]\n",
      "epoch:23 step:22363 [D loss: 0.436715, acc.: 82.81%] [G loss: 1.388471]\n",
      "epoch:23 step:22364 [D loss: 0.470699, acc.: 77.34%] [G loss: 1.417744]\n",
      "epoch:23 step:22365 [D loss: 0.492624, acc.: 73.44%] [G loss: 1.537097]\n",
      "epoch:23 step:22366 [D loss: 0.650956, acc.: 64.84%] [G loss: 1.527212]\n",
      "epoch:23 step:22367 [D loss: 0.675562, acc.: 60.16%] [G loss: 1.425007]\n",
      "epoch:23 step:22368 [D loss: 0.592019, acc.: 67.19%] [G loss: 1.227175]\n",
      "epoch:23 step:22369 [D loss: 0.530535, acc.: 73.44%] [G loss: 1.286068]\n",
      "epoch:23 step:22370 [D loss: 0.501897, acc.: 75.00%] [G loss: 1.512574]\n",
      "epoch:23 step:22371 [D loss: 0.641874, acc.: 63.28%] [G loss: 1.415509]\n",
      "epoch:23 step:22372 [D loss: 0.495335, acc.: 81.25%] [G loss: 1.403960]\n",
      "epoch:23 step:22373 [D loss: 0.516499, acc.: 68.75%] [G loss: 1.300823]\n",
      "epoch:23 step:22374 [D loss: 0.465522, acc.: 78.12%] [G loss: 1.233843]\n",
      "epoch:23 step:22375 [D loss: 0.519063, acc.: 71.88%] [G loss: 1.354534]\n",
      "epoch:23 step:22376 [D loss: 0.531461, acc.: 74.22%] [G loss: 1.390535]\n",
      "epoch:23 step:22377 [D loss: 0.621316, acc.: 64.06%] [G loss: 1.215508]\n",
      "epoch:23 step:22378 [D loss: 0.429614, acc.: 78.91%] [G loss: 1.556772]\n",
      "epoch:23 step:22379 [D loss: 0.709164, acc.: 57.03%] [G loss: 1.480882]\n",
      "epoch:23 step:22380 [D loss: 0.556376, acc.: 70.31%] [G loss: 1.218567]\n",
      "epoch:23 step:22381 [D loss: 0.435062, acc.: 81.25%] [G loss: 1.510403]\n",
      "epoch:23 step:22382 [D loss: 0.600844, acc.: 62.50%] [G loss: 1.063251]\n",
      "epoch:23 step:22383 [D loss: 0.517348, acc.: 75.00%] [G loss: 1.216081]\n",
      "epoch:23 step:22384 [D loss: 0.534037, acc.: 71.88%] [G loss: 1.509063]\n",
      "epoch:23 step:22385 [D loss: 0.502260, acc.: 80.47%] [G loss: 1.558571]\n",
      "epoch:23 step:22386 [D loss: 0.485447, acc.: 76.56%] [G loss: 1.283234]\n",
      "epoch:23 step:22387 [D loss: 0.683259, acc.: 62.50%] [G loss: 1.424675]\n",
      "epoch:23 step:22388 [D loss: 0.726512, acc.: 51.56%] [G loss: 1.104418]\n",
      "epoch:23 step:22389 [D loss: 0.552792, acc.: 73.44%] [G loss: 1.490490]\n",
      "epoch:23 step:22390 [D loss: 0.518677, acc.: 74.22%] [G loss: 1.698917]\n",
      "epoch:23 step:22391 [D loss: 0.577305, acc.: 73.44%] [G loss: 1.229598]\n",
      "epoch:23 step:22392 [D loss: 0.538515, acc.: 74.22%] [G loss: 1.031312]\n",
      "epoch:23 step:22393 [D loss: 0.614840, acc.: 67.97%] [G loss: 1.198127]\n",
      "epoch:23 step:22394 [D loss: 0.565482, acc.: 71.09%] [G loss: 1.808030]\n",
      "epoch:23 step:22395 [D loss: 0.611378, acc.: 71.88%] [G loss: 1.561103]\n",
      "epoch:23 step:22396 [D loss: 0.545146, acc.: 67.97%] [G loss: 1.192199]\n",
      "epoch:23 step:22397 [D loss: 0.646022, acc.: 65.62%] [G loss: 1.215904]\n",
      "epoch:23 step:22398 [D loss: 0.513548, acc.: 78.12%] [G loss: 1.507987]\n",
      "epoch:23 step:22399 [D loss: 0.672220, acc.: 59.38%] [G loss: 1.438560]\n",
      "epoch:23 step:22400 [D loss: 0.579480, acc.: 67.19%] [G loss: 1.687237]\n",
      "##############\n",
      "[2.71438847 1.95298251 2.22466091 2.93570824 0.60376556 6.24426317\n",
      " 2.00189838 3.17117683 3.91235806 7.14868929]\n",
      "##########\n",
      "epoch:23 step:22401 [D loss: 0.415882, acc.: 87.50%] [G loss: 1.530820]\n",
      "epoch:23 step:22402 [D loss: 0.831086, acc.: 46.09%] [G loss: 1.335145]\n",
      "epoch:23 step:22403 [D loss: 0.447318, acc.: 82.03%] [G loss: 1.757806]\n",
      "epoch:23 step:22404 [D loss: 0.507342, acc.: 76.56%] [G loss: 1.773486]\n",
      "epoch:23 step:22405 [D loss: 0.756771, acc.: 55.47%] [G loss: 1.068755]\n",
      "epoch:23 step:22406 [D loss: 0.694092, acc.: 60.16%] [G loss: 1.383118]\n",
      "epoch:23 step:22407 [D loss: 0.485430, acc.: 75.78%] [G loss: 1.614943]\n",
      "epoch:23 step:22408 [D loss: 0.545862, acc.: 72.66%] [G loss: 1.784703]\n",
      "epoch:23 step:22409 [D loss: 0.497365, acc.: 73.44%] [G loss: 1.241714]\n",
      "epoch:23 step:22410 [D loss: 0.608576, acc.: 66.41%] [G loss: 1.620759]\n",
      "epoch:23 step:22411 [D loss: 0.544041, acc.: 72.66%] [G loss: 1.209445]\n",
      "epoch:23 step:22412 [D loss: 0.477344, acc.: 77.34%] [G loss: 1.390207]\n",
      "epoch:23 step:22413 [D loss: 0.544360, acc.: 73.44%] [G loss: 1.473357]\n",
      "epoch:23 step:22414 [D loss: 0.512368, acc.: 74.22%] [G loss: 1.166584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22415 [D loss: 0.671021, acc.: 64.06%] [G loss: 1.045657]\n",
      "epoch:23 step:22416 [D loss: 0.623043, acc.: 67.19%] [G loss: 1.375601]\n",
      "epoch:23 step:22417 [D loss: 0.432485, acc.: 81.25%] [G loss: 1.443537]\n",
      "epoch:23 step:22418 [D loss: 0.647938, acc.: 67.19%] [G loss: 1.318872]\n",
      "epoch:23 step:22419 [D loss: 0.315584, acc.: 90.62%] [G loss: 1.344191]\n",
      "epoch:23 step:22420 [D loss: 0.646922, acc.: 60.94%] [G loss: 1.252662]\n",
      "epoch:23 step:22421 [D loss: 0.511594, acc.: 78.12%] [G loss: 1.614914]\n",
      "epoch:23 step:22422 [D loss: 0.733631, acc.: 60.94%] [G loss: 1.568293]\n",
      "epoch:23 step:22423 [D loss: 0.464735, acc.: 77.34%] [G loss: 1.345302]\n",
      "epoch:23 step:22424 [D loss: 0.504302, acc.: 75.78%] [G loss: 1.329727]\n",
      "epoch:23 step:22425 [D loss: 0.439437, acc.: 81.25%] [G loss: 1.748078]\n",
      "epoch:23 step:22426 [D loss: 0.512006, acc.: 80.47%] [G loss: 1.420708]\n",
      "epoch:23 step:22427 [D loss: 0.643686, acc.: 67.97%] [G loss: 1.848044]\n",
      "epoch:23 step:22428 [D loss: 0.591730, acc.: 67.19%] [G loss: 1.336657]\n",
      "epoch:23 step:22429 [D loss: 0.455010, acc.: 82.81%] [G loss: 1.138945]\n",
      "epoch:23 step:22430 [D loss: 0.773961, acc.: 56.25%] [G loss: 1.203500]\n",
      "epoch:23 step:22431 [D loss: 0.466912, acc.: 78.12%] [G loss: 1.472826]\n",
      "epoch:23 step:22432 [D loss: 0.562277, acc.: 72.66%] [G loss: 1.242506]\n",
      "epoch:23 step:22433 [D loss: 0.490645, acc.: 76.56%] [G loss: 1.619043]\n",
      "epoch:23 step:22434 [D loss: 0.482493, acc.: 78.12%] [G loss: 1.123637]\n",
      "epoch:23 step:22435 [D loss: 0.469793, acc.: 80.47%] [G loss: 0.992185]\n",
      "epoch:23 step:22436 [D loss: 0.432937, acc.: 83.59%] [G loss: 1.383361]\n",
      "epoch:23 step:22437 [D loss: 0.472118, acc.: 82.81%] [G loss: 1.419902]\n",
      "epoch:23 step:22438 [D loss: 0.392800, acc.: 84.38%] [G loss: 1.683080]\n",
      "epoch:23 step:22439 [D loss: 0.676953, acc.: 53.91%] [G loss: 1.102074]\n",
      "epoch:23 step:22440 [D loss: 0.751027, acc.: 54.69%] [G loss: 0.853129]\n",
      "epoch:23 step:22441 [D loss: 0.583376, acc.: 67.97%] [G loss: 1.184899]\n",
      "epoch:23 step:22442 [D loss: 0.551641, acc.: 74.22%] [G loss: 1.149950]\n",
      "epoch:23 step:22443 [D loss: 0.433231, acc.: 82.03%] [G loss: 1.622864]\n",
      "epoch:23 step:22444 [D loss: 0.701867, acc.: 62.50%] [G loss: 1.357108]\n",
      "epoch:23 step:22445 [D loss: 0.568577, acc.: 69.53%] [G loss: 1.325731]\n",
      "epoch:23 step:22446 [D loss: 0.555521, acc.: 71.09%] [G loss: 1.487653]\n",
      "epoch:23 step:22447 [D loss: 0.449169, acc.: 79.69%] [G loss: 1.982408]\n",
      "epoch:23 step:22448 [D loss: 0.488638, acc.: 77.34%] [G loss: 1.173494]\n",
      "epoch:23 step:22449 [D loss: 0.587315, acc.: 68.75%] [G loss: 1.196093]\n",
      "epoch:23 step:22450 [D loss: 0.519503, acc.: 77.34%] [G loss: 1.642826]\n",
      "epoch:23 step:22451 [D loss: 0.519808, acc.: 73.44%] [G loss: 1.468290]\n",
      "epoch:23 step:22452 [D loss: 0.451975, acc.: 81.25%] [G loss: 1.378816]\n",
      "epoch:23 step:22453 [D loss: 0.411776, acc.: 85.16%] [G loss: 1.467094]\n",
      "epoch:23 step:22454 [D loss: 0.706831, acc.: 59.38%] [G loss: 1.034570]\n",
      "epoch:23 step:22455 [D loss: 0.417307, acc.: 88.28%] [G loss: 1.213079]\n",
      "epoch:23 step:22456 [D loss: 0.556673, acc.: 73.44%] [G loss: 1.377342]\n",
      "epoch:23 step:22457 [D loss: 0.452550, acc.: 78.91%] [G loss: 1.018495]\n",
      "epoch:23 step:22458 [D loss: 0.634046, acc.: 61.72%] [G loss: 1.051880]\n",
      "epoch:23 step:22459 [D loss: 0.520484, acc.: 75.00%] [G loss: 1.585186]\n",
      "epoch:23 step:22460 [D loss: 0.425341, acc.: 79.69%] [G loss: 1.751314]\n",
      "epoch:23 step:22461 [D loss: 0.502785, acc.: 75.78%] [G loss: 1.444261]\n",
      "epoch:23 step:22462 [D loss: 0.601915, acc.: 69.53%] [G loss: 1.793175]\n",
      "epoch:23 step:22463 [D loss: 0.578038, acc.: 71.88%] [G loss: 1.431177]\n",
      "epoch:23 step:22464 [D loss: 0.617141, acc.: 72.66%] [G loss: 1.257829]\n",
      "epoch:23 step:22465 [D loss: 0.514585, acc.: 78.12%] [G loss: 1.736919]\n",
      "epoch:23 step:22466 [D loss: 0.655836, acc.: 60.94%] [G loss: 1.172254]\n",
      "epoch:23 step:22467 [D loss: 0.559508, acc.: 71.09%] [G loss: 1.415189]\n",
      "epoch:23 step:22468 [D loss: 0.530414, acc.: 75.00%] [G loss: 1.370792]\n",
      "epoch:23 step:22469 [D loss: 0.578544, acc.: 67.97%] [G loss: 1.339408]\n",
      "epoch:23 step:22470 [D loss: 0.488179, acc.: 76.56%] [G loss: 1.074285]\n",
      "epoch:23 step:22471 [D loss: 0.494343, acc.: 78.12%] [G loss: 1.217945]\n",
      "epoch:23 step:22472 [D loss: 0.650660, acc.: 63.28%] [G loss: 1.638392]\n",
      "epoch:23 step:22473 [D loss: 0.524790, acc.: 75.00%] [G loss: 1.294196]\n",
      "epoch:23 step:22474 [D loss: 0.583477, acc.: 67.19%] [G loss: 1.563148]\n",
      "epoch:23 step:22475 [D loss: 0.488843, acc.: 75.78%] [G loss: 0.903039]\n",
      "epoch:23 step:22476 [D loss: 0.624170, acc.: 64.06%] [G loss: 1.342198]\n",
      "epoch:23 step:22477 [D loss: 0.620787, acc.: 68.75%] [G loss: 1.281861]\n",
      "epoch:23 step:22478 [D loss: 0.620840, acc.: 64.06%] [G loss: 1.527246]\n",
      "epoch:23 step:22479 [D loss: 0.483718, acc.: 77.34%] [G loss: 1.669899]\n",
      "epoch:23 step:22480 [D loss: 0.604519, acc.: 67.19%] [G loss: 1.300888]\n",
      "epoch:23 step:22481 [D loss: 0.564708, acc.: 71.88%] [G loss: 1.707051]\n",
      "epoch:23 step:22482 [D loss: 0.694573, acc.: 60.16%] [G loss: 1.172313]\n",
      "epoch:23 step:22483 [D loss: 0.653958, acc.: 60.94%] [G loss: 0.944693]\n",
      "epoch:23 step:22484 [D loss: 0.518281, acc.: 74.22%] [G loss: 1.309203]\n",
      "epoch:23 step:22485 [D loss: 0.676001, acc.: 60.94%] [G loss: 1.055598]\n",
      "epoch:23 step:22486 [D loss: 0.496263, acc.: 78.12%] [G loss: 1.504081]\n",
      "epoch:23 step:22487 [D loss: 0.647555, acc.: 65.62%] [G loss: 1.435974]\n",
      "epoch:23 step:22488 [D loss: 0.648546, acc.: 67.19%] [G loss: 1.592418]\n",
      "epoch:24 step:22489 [D loss: 0.501505, acc.: 77.34%] [G loss: 1.202412]\n",
      "epoch:24 step:22490 [D loss: 0.557290, acc.: 71.09%] [G loss: 1.438540]\n",
      "epoch:24 step:22491 [D loss: 0.513719, acc.: 77.34%] [G loss: 1.496106]\n",
      "epoch:24 step:22492 [D loss: 0.551854, acc.: 78.91%] [G loss: 1.249245]\n",
      "epoch:24 step:22493 [D loss: 0.491396, acc.: 78.91%] [G loss: 1.475659]\n",
      "epoch:24 step:22494 [D loss: 0.570080, acc.: 65.62%] [G loss: 1.350403]\n",
      "epoch:24 step:22495 [D loss: 0.529344, acc.: 71.09%] [G loss: 1.445048]\n",
      "epoch:24 step:22496 [D loss: 0.541684, acc.: 73.44%] [G loss: 1.312858]\n",
      "epoch:24 step:22497 [D loss: 0.459013, acc.: 78.12%] [G loss: 1.294593]\n",
      "epoch:24 step:22498 [D loss: 0.494418, acc.: 74.22%] [G loss: 1.162449]\n",
      "epoch:24 step:22499 [D loss: 0.525396, acc.: 72.66%] [G loss: 1.051965]\n",
      "epoch:24 step:22500 [D loss: 0.546949, acc.: 71.88%] [G loss: 1.223857]\n",
      "epoch:24 step:22501 [D loss: 0.723071, acc.: 52.34%] [G loss: 1.032911]\n",
      "epoch:24 step:22502 [D loss: 0.502774, acc.: 75.78%] [G loss: 1.631860]\n",
      "epoch:24 step:22503 [D loss: 0.465720, acc.: 80.47%] [G loss: 1.554462]\n",
      "epoch:24 step:22504 [D loss: 0.567258, acc.: 68.75%] [G loss: 1.215781]\n",
      "epoch:24 step:22505 [D loss: 0.586901, acc.: 64.84%] [G loss: 1.234738]\n",
      "epoch:24 step:22506 [D loss: 0.691630, acc.: 57.81%] [G loss: 1.343023]\n",
      "epoch:24 step:22507 [D loss: 0.575486, acc.: 65.62%] [G loss: 1.337725]\n",
      "epoch:24 step:22508 [D loss: 0.629977, acc.: 64.84%] [G loss: 0.935736]\n",
      "epoch:24 step:22509 [D loss: 0.734006, acc.: 52.34%] [G loss: 1.168308]\n",
      "epoch:24 step:22510 [D loss: 0.616397, acc.: 67.97%] [G loss: 1.461953]\n",
      "epoch:24 step:22511 [D loss: 0.455744, acc.: 78.12%] [G loss: 1.548561]\n",
      "epoch:24 step:22512 [D loss: 0.720222, acc.: 54.69%] [G loss: 1.233444]\n",
      "epoch:24 step:22513 [D loss: 0.596703, acc.: 67.19%] [G loss: 1.610463]\n",
      "epoch:24 step:22514 [D loss: 0.757694, acc.: 58.59%] [G loss: 1.384543]\n",
      "epoch:24 step:22515 [D loss: 0.512349, acc.: 76.56%] [G loss: 1.084829]\n",
      "epoch:24 step:22516 [D loss: 0.459620, acc.: 76.56%] [G loss: 1.623477]\n",
      "epoch:24 step:22517 [D loss: 0.520930, acc.: 69.53%] [G loss: 1.297839]\n",
      "epoch:24 step:22518 [D loss: 0.516746, acc.: 75.78%] [G loss: 1.643693]\n",
      "epoch:24 step:22519 [D loss: 0.655052, acc.: 60.16%] [G loss: 1.260606]\n",
      "epoch:24 step:22520 [D loss: 0.579893, acc.: 68.75%] [G loss: 1.323277]\n",
      "epoch:24 step:22521 [D loss: 0.538920, acc.: 75.00%] [G loss: 1.337177]\n",
      "epoch:24 step:22522 [D loss: 0.505151, acc.: 75.78%] [G loss: 1.202169]\n",
      "epoch:24 step:22523 [D loss: 0.605647, acc.: 61.72%] [G loss: 0.884798]\n",
      "epoch:24 step:22524 [D loss: 0.708763, acc.: 57.03%] [G loss: 1.179518]\n",
      "epoch:24 step:22525 [D loss: 0.443050, acc.: 82.03%] [G loss: 1.373084]\n",
      "epoch:24 step:22526 [D loss: 0.667743, acc.: 66.41%] [G loss: 1.308955]\n",
      "epoch:24 step:22527 [D loss: 0.618260, acc.: 67.97%] [G loss: 1.538958]\n",
      "epoch:24 step:22528 [D loss: 0.698367, acc.: 54.69%] [G loss: 1.452556]\n",
      "epoch:24 step:22529 [D loss: 0.496312, acc.: 79.69%] [G loss: 1.213948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22530 [D loss: 0.508665, acc.: 73.44%] [G loss: 1.335744]\n",
      "epoch:24 step:22531 [D loss: 0.390736, acc.: 82.81%] [G loss: 1.212567]\n",
      "epoch:24 step:22532 [D loss: 0.728999, acc.: 59.38%] [G loss: 1.139485]\n",
      "epoch:24 step:22533 [D loss: 0.534930, acc.: 70.31%] [G loss: 1.283734]\n",
      "epoch:24 step:22534 [D loss: 0.482324, acc.: 79.69%] [G loss: 1.420280]\n",
      "epoch:24 step:22535 [D loss: 0.666122, acc.: 60.16%] [G loss: 1.442412]\n",
      "epoch:24 step:22536 [D loss: 0.590288, acc.: 67.19%] [G loss: 1.638346]\n",
      "epoch:24 step:22537 [D loss: 0.618111, acc.: 68.75%] [G loss: 1.804676]\n",
      "epoch:24 step:22538 [D loss: 0.661865, acc.: 56.25%] [G loss: 1.424726]\n",
      "epoch:24 step:22539 [D loss: 0.578940, acc.: 65.62%] [G loss: 1.507911]\n",
      "epoch:24 step:22540 [D loss: 0.462893, acc.: 78.12%] [G loss: 1.241375]\n",
      "epoch:24 step:22541 [D loss: 0.595150, acc.: 65.62%] [G loss: 1.392614]\n",
      "epoch:24 step:22542 [D loss: 0.529026, acc.: 68.75%] [G loss: 1.687783]\n",
      "epoch:24 step:22543 [D loss: 0.633600, acc.: 67.19%] [G loss: 1.115248]\n",
      "epoch:24 step:22544 [D loss: 0.662393, acc.: 60.94%] [G loss: 1.072582]\n",
      "epoch:24 step:22545 [D loss: 0.549245, acc.: 72.66%] [G loss: 1.310317]\n",
      "epoch:24 step:22546 [D loss: 0.578127, acc.: 67.19%] [G loss: 1.282740]\n",
      "epoch:24 step:22547 [D loss: 0.523309, acc.: 74.22%] [G loss: 1.214180]\n",
      "epoch:24 step:22548 [D loss: 0.626842, acc.: 64.06%] [G loss: 1.588520]\n",
      "epoch:24 step:22549 [D loss: 0.575812, acc.: 75.00%] [G loss: 1.039855]\n",
      "epoch:24 step:22550 [D loss: 0.673720, acc.: 62.50%] [G loss: 1.316157]\n",
      "epoch:24 step:22551 [D loss: 0.657399, acc.: 61.72%] [G loss: 1.338423]\n",
      "epoch:24 step:22552 [D loss: 0.534565, acc.: 71.09%] [G loss: 1.411456]\n",
      "epoch:24 step:22553 [D loss: 0.577877, acc.: 71.09%] [G loss: 1.419220]\n",
      "epoch:24 step:22554 [D loss: 0.551377, acc.: 71.88%] [G loss: 1.438143]\n",
      "epoch:24 step:22555 [D loss: 0.508577, acc.: 75.78%] [G loss: 1.386392]\n",
      "epoch:24 step:22556 [D loss: 0.603627, acc.: 67.97%] [G loss: 1.066885]\n",
      "epoch:24 step:22557 [D loss: 0.582893, acc.: 70.31%] [G loss: 0.873430]\n",
      "epoch:24 step:22558 [D loss: 0.543574, acc.: 71.09%] [G loss: 1.706025]\n",
      "epoch:24 step:22559 [D loss: 0.616117, acc.: 67.97%] [G loss: 1.401860]\n",
      "epoch:24 step:22560 [D loss: 0.664560, acc.: 60.94%] [G loss: 1.117830]\n",
      "epoch:24 step:22561 [D loss: 0.536074, acc.: 70.31%] [G loss: 1.335897]\n",
      "epoch:24 step:22562 [D loss: 0.573186, acc.: 68.75%] [G loss: 1.423905]\n",
      "epoch:24 step:22563 [D loss: 0.507032, acc.: 74.22%] [G loss: 1.299051]\n",
      "epoch:24 step:22564 [D loss: 0.651233, acc.: 67.19%] [G loss: 1.497880]\n",
      "epoch:24 step:22565 [D loss: 0.644455, acc.: 67.97%] [G loss: 1.115545]\n",
      "epoch:24 step:22566 [D loss: 0.610174, acc.: 66.41%] [G loss: 1.177268]\n",
      "epoch:24 step:22567 [D loss: 0.544430, acc.: 71.09%] [G loss: 1.312091]\n",
      "epoch:24 step:22568 [D loss: 0.652601, acc.: 64.06%] [G loss: 1.351157]\n",
      "epoch:24 step:22569 [D loss: 0.683827, acc.: 60.16%] [G loss: 1.574625]\n",
      "epoch:24 step:22570 [D loss: 0.368467, acc.: 89.84%] [G loss: 1.555404]\n",
      "epoch:24 step:22571 [D loss: 0.561754, acc.: 68.75%] [G loss: 1.199890]\n",
      "epoch:24 step:22572 [D loss: 0.725429, acc.: 55.47%] [G loss: 1.002564]\n",
      "epoch:24 step:22573 [D loss: 0.654267, acc.: 61.72%] [G loss: 0.939129]\n",
      "epoch:24 step:22574 [D loss: 0.577648, acc.: 71.09%] [G loss: 1.186955]\n",
      "epoch:24 step:22575 [D loss: 0.471365, acc.: 78.91%] [G loss: 1.340659]\n",
      "epoch:24 step:22576 [D loss: 0.542667, acc.: 69.53%] [G loss: 1.869649]\n",
      "epoch:24 step:22577 [D loss: 0.505868, acc.: 75.00%] [G loss: 1.530077]\n",
      "epoch:24 step:22578 [D loss: 0.785382, acc.: 49.22%] [G loss: 1.427302]\n",
      "epoch:24 step:22579 [D loss: 0.601513, acc.: 68.75%] [G loss: 1.366750]\n",
      "epoch:24 step:22580 [D loss: 0.488825, acc.: 78.12%] [G loss: 1.094502]\n",
      "epoch:24 step:22581 [D loss: 0.520939, acc.: 71.88%] [G loss: 1.547683]\n",
      "epoch:24 step:22582 [D loss: 0.544634, acc.: 73.44%] [G loss: 1.220206]\n",
      "epoch:24 step:22583 [D loss: 0.617669, acc.: 66.41%] [G loss: 1.189083]\n",
      "epoch:24 step:22584 [D loss: 0.603634, acc.: 68.75%] [G loss: 1.184259]\n",
      "epoch:24 step:22585 [D loss: 0.596057, acc.: 70.31%] [G loss: 1.447650]\n",
      "epoch:24 step:22586 [D loss: 0.590865, acc.: 65.62%] [G loss: 1.034249]\n",
      "epoch:24 step:22587 [D loss: 0.600601, acc.: 67.19%] [G loss: 1.844091]\n",
      "epoch:24 step:22588 [D loss: 0.640290, acc.: 59.38%] [G loss: 1.381825]\n",
      "epoch:24 step:22589 [D loss: 0.489156, acc.: 77.34%] [G loss: 1.663272]\n",
      "epoch:24 step:22590 [D loss: 0.500394, acc.: 75.78%] [G loss: 1.531335]\n",
      "epoch:24 step:22591 [D loss: 0.511646, acc.: 76.56%] [G loss: 1.779599]\n",
      "epoch:24 step:22592 [D loss: 0.432708, acc.: 85.94%] [G loss: 1.151851]\n",
      "epoch:24 step:22593 [D loss: 0.427294, acc.: 79.69%] [G loss: 1.501692]\n",
      "epoch:24 step:22594 [D loss: 0.512462, acc.: 73.44%] [G loss: 1.328659]\n",
      "epoch:24 step:22595 [D loss: 0.475986, acc.: 76.56%] [G loss: 1.426821]\n",
      "epoch:24 step:22596 [D loss: 0.518706, acc.: 74.22%] [G loss: 1.453351]\n",
      "epoch:24 step:22597 [D loss: 0.425609, acc.: 81.25%] [G loss: 1.449597]\n",
      "epoch:24 step:22598 [D loss: 0.596607, acc.: 67.97%] [G loss: 1.407851]\n",
      "epoch:24 step:22599 [D loss: 0.748900, acc.: 55.47%] [G loss: 1.319502]\n",
      "epoch:24 step:22600 [D loss: 0.609936, acc.: 61.72%] [G loss: 1.201432]\n",
      "##############\n",
      "[2.66324548 2.08956461 1.83403911 3.01423638 0.79193374 6.74777269\n",
      " 2.21975974 2.62172025 3.86682071 8.14868929]\n",
      "##########\n",
      "epoch:24 step:22601 [D loss: 0.495923, acc.: 78.12%] [G loss: 1.411383]\n",
      "epoch:24 step:22602 [D loss: 0.455247, acc.: 81.25%] [G loss: 1.577943]\n",
      "epoch:24 step:22603 [D loss: 0.493291, acc.: 72.66%] [G loss: 1.615826]\n",
      "epoch:24 step:22604 [D loss: 0.771780, acc.: 54.69%] [G loss: 1.450997]\n",
      "epoch:24 step:22605 [D loss: 0.491975, acc.: 73.44%] [G loss: 1.490060]\n",
      "epoch:24 step:22606 [D loss: 0.620975, acc.: 67.19%] [G loss: 1.375813]\n",
      "epoch:24 step:22607 [D loss: 0.318663, acc.: 89.06%] [G loss: 1.506248]\n",
      "epoch:24 step:22608 [D loss: 0.661709, acc.: 58.59%] [G loss: 1.291509]\n",
      "epoch:24 step:22609 [D loss: 0.513454, acc.: 75.78%] [G loss: 1.002297]\n",
      "epoch:24 step:22610 [D loss: 0.502240, acc.: 71.88%] [G loss: 1.735541]\n",
      "epoch:24 step:22611 [D loss: 0.599208, acc.: 65.62%] [G loss: 1.507352]\n",
      "epoch:24 step:22612 [D loss: 0.551055, acc.: 70.31%] [G loss: 1.144195]\n",
      "epoch:24 step:22613 [D loss: 0.448228, acc.: 80.47%] [G loss: 1.580955]\n",
      "epoch:24 step:22614 [D loss: 0.538935, acc.: 75.78%] [G loss: 1.520495]\n",
      "epoch:24 step:22615 [D loss: 0.620308, acc.: 63.28%] [G loss: 1.270860]\n",
      "epoch:24 step:22616 [D loss: 0.457809, acc.: 78.12%] [G loss: 1.385887]\n",
      "epoch:24 step:22617 [D loss: 0.534675, acc.: 75.00%] [G loss: 1.083277]\n",
      "epoch:24 step:22618 [D loss: 0.400210, acc.: 90.62%] [G loss: 1.354493]\n",
      "epoch:24 step:22619 [D loss: 0.496124, acc.: 80.47%] [G loss: 1.194524]\n",
      "epoch:24 step:22620 [D loss: 0.742363, acc.: 57.03%] [G loss: 1.494171]\n",
      "epoch:24 step:22621 [D loss: 0.478401, acc.: 78.12%] [G loss: 1.433689]\n",
      "epoch:24 step:22622 [D loss: 0.689035, acc.: 58.59%] [G loss: 1.775297]\n",
      "epoch:24 step:22623 [D loss: 0.445821, acc.: 83.59%] [G loss: 1.761093]\n",
      "epoch:24 step:22624 [D loss: 0.829127, acc.: 48.44%] [G loss: 1.142932]\n",
      "epoch:24 step:22625 [D loss: 0.632580, acc.: 68.75%] [G loss: 1.455150]\n",
      "epoch:24 step:22626 [D loss: 0.489255, acc.: 72.66%] [G loss: 1.634099]\n",
      "epoch:24 step:22627 [D loss: 0.418500, acc.: 82.03%] [G loss: 1.430547]\n",
      "epoch:24 step:22628 [D loss: 0.509627, acc.: 75.78%] [G loss: 1.135753]\n",
      "epoch:24 step:22629 [D loss: 0.651563, acc.: 64.06%] [G loss: 1.299002]\n",
      "epoch:24 step:22630 [D loss: 0.455009, acc.: 78.91%] [G loss: 1.086441]\n",
      "epoch:24 step:22631 [D loss: 0.583823, acc.: 68.75%] [G loss: 1.388468]\n",
      "epoch:24 step:22632 [D loss: 0.474781, acc.: 80.47%] [G loss: 1.676423]\n",
      "epoch:24 step:22633 [D loss: 0.545281, acc.: 68.75%] [G loss: 1.322400]\n",
      "epoch:24 step:22634 [D loss: 0.677322, acc.: 57.81%] [G loss: 1.289481]\n",
      "epoch:24 step:22635 [D loss: 0.584031, acc.: 70.31%] [G loss: 1.367948]\n",
      "epoch:24 step:22636 [D loss: 0.489544, acc.: 81.25%] [G loss: 1.275381]\n",
      "epoch:24 step:22637 [D loss: 0.725796, acc.: 52.34%] [G loss: 1.312408]\n",
      "epoch:24 step:22638 [D loss: 0.740357, acc.: 54.69%] [G loss: 1.390515]\n",
      "epoch:24 step:22639 [D loss: 0.498562, acc.: 75.00%] [G loss: 1.449181]\n",
      "epoch:24 step:22640 [D loss: 0.557440, acc.: 72.66%] [G loss: 1.358552]\n",
      "epoch:24 step:22641 [D loss: 0.550162, acc.: 71.88%] [G loss: 1.055080]\n",
      "epoch:24 step:22642 [D loss: 0.492040, acc.: 79.69%] [G loss: 1.190425]\n",
      "epoch:24 step:22643 [D loss: 0.756185, acc.: 58.59%] [G loss: 1.093336]\n",
      "epoch:24 step:22644 [D loss: 0.382940, acc.: 89.84%] [G loss: 1.427616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22645 [D loss: 0.616882, acc.: 66.41%] [G loss: 1.537220]\n",
      "epoch:24 step:22646 [D loss: 0.788558, acc.: 47.66%] [G loss: 1.071071]\n",
      "epoch:24 step:22647 [D loss: 0.441146, acc.: 85.94%] [G loss: 1.359083]\n",
      "epoch:24 step:22648 [D loss: 0.727313, acc.: 57.03%] [G loss: 1.019643]\n",
      "epoch:24 step:22649 [D loss: 0.575966, acc.: 71.88%] [G loss: 0.957050]\n",
      "epoch:24 step:22650 [D loss: 0.601167, acc.: 70.31%] [G loss: 1.141319]\n",
      "epoch:24 step:22651 [D loss: 0.580001, acc.: 70.31%] [G loss: 1.646564]\n",
      "epoch:24 step:22652 [D loss: 0.644134, acc.: 66.41%] [G loss: 1.142835]\n",
      "epoch:24 step:22653 [D loss: 0.639083, acc.: 64.84%] [G loss: 1.502699]\n",
      "epoch:24 step:22654 [D loss: 0.553652, acc.: 70.31%] [G loss: 1.402721]\n",
      "epoch:24 step:22655 [D loss: 0.807276, acc.: 52.34%] [G loss: 1.129865]\n",
      "epoch:24 step:22656 [D loss: 0.637410, acc.: 64.06%] [G loss: 1.529042]\n",
      "epoch:24 step:22657 [D loss: 0.586685, acc.: 70.31%] [G loss: 1.462710]\n",
      "epoch:24 step:22658 [D loss: 0.530102, acc.: 74.22%] [G loss: 1.368033]\n",
      "epoch:24 step:22659 [D loss: 0.597495, acc.: 64.84%] [G loss: 1.224670]\n",
      "epoch:24 step:22660 [D loss: 0.412708, acc.: 83.59%] [G loss: 1.319062]\n",
      "epoch:24 step:22661 [D loss: 0.493612, acc.: 74.22%] [G loss: 1.138089]\n",
      "epoch:24 step:22662 [D loss: 0.406444, acc.: 84.38%] [G loss: 1.473415]\n",
      "epoch:24 step:22663 [D loss: 0.498227, acc.: 76.56%] [G loss: 1.649519]\n",
      "epoch:24 step:22664 [D loss: 0.600312, acc.: 67.19%] [G loss: 1.294538]\n",
      "epoch:24 step:22665 [D loss: 0.595674, acc.: 65.62%] [G loss: 1.305696]\n",
      "epoch:24 step:22666 [D loss: 0.461688, acc.: 77.34%] [G loss: 1.414805]\n",
      "epoch:24 step:22667 [D loss: 0.673770, acc.: 61.72%] [G loss: 1.022388]\n",
      "epoch:24 step:22668 [D loss: 0.548251, acc.: 71.09%] [G loss: 1.288976]\n",
      "epoch:24 step:22669 [D loss: 0.498153, acc.: 75.78%] [G loss: 1.611120]\n",
      "epoch:24 step:22670 [D loss: 0.607747, acc.: 68.75%] [G loss: 1.648363]\n",
      "epoch:24 step:22671 [D loss: 0.589225, acc.: 68.75%] [G loss: 1.326522]\n",
      "epoch:24 step:22672 [D loss: 0.544374, acc.: 71.88%] [G loss: 1.494350]\n",
      "epoch:24 step:22673 [D loss: 0.548262, acc.: 73.44%] [G loss: 1.345859]\n",
      "epoch:24 step:22674 [D loss: 0.408514, acc.: 81.25%] [G loss: 1.936691]\n",
      "epoch:24 step:22675 [D loss: 0.424673, acc.: 85.16%] [G loss: 1.600726]\n",
      "epoch:24 step:22676 [D loss: 0.512793, acc.: 75.78%] [G loss: 1.395957]\n",
      "epoch:24 step:22677 [D loss: 0.624316, acc.: 65.62%] [G loss: 1.586859]\n",
      "epoch:24 step:22678 [D loss: 0.692589, acc.: 58.59%] [G loss: 1.227198]\n",
      "epoch:24 step:22679 [D loss: 0.678890, acc.: 59.38%] [G loss: 1.450938]\n",
      "epoch:24 step:22680 [D loss: 0.659568, acc.: 63.28%] [G loss: 1.635459]\n",
      "epoch:24 step:22681 [D loss: 0.627985, acc.: 64.06%] [G loss: 1.466955]\n",
      "epoch:24 step:22682 [D loss: 0.635885, acc.: 67.97%] [G loss: 1.019062]\n",
      "epoch:24 step:22683 [D loss: 0.525980, acc.: 76.56%] [G loss: 1.293368]\n",
      "epoch:24 step:22684 [D loss: 0.499614, acc.: 71.88%] [G loss: 1.129715]\n",
      "epoch:24 step:22685 [D loss: 0.503093, acc.: 78.91%] [G loss: 1.192282]\n",
      "epoch:24 step:22686 [D loss: 0.591266, acc.: 74.22%] [G loss: 1.267277]\n",
      "epoch:24 step:22687 [D loss: 0.544340, acc.: 69.53%] [G loss: 1.678841]\n",
      "epoch:24 step:22688 [D loss: 0.747729, acc.: 54.69%] [G loss: 1.734469]\n",
      "epoch:24 step:22689 [D loss: 0.547063, acc.: 73.44%] [G loss: 1.537851]\n",
      "epoch:24 step:22690 [D loss: 0.462857, acc.: 78.12%] [G loss: 1.396875]\n",
      "epoch:24 step:22691 [D loss: 0.610147, acc.: 69.53%] [G loss: 1.536147]\n",
      "epoch:24 step:22692 [D loss: 0.450309, acc.: 78.91%] [G loss: 1.430846]\n",
      "epoch:24 step:22693 [D loss: 0.705487, acc.: 58.59%] [G loss: 1.304569]\n",
      "epoch:24 step:22694 [D loss: 0.769619, acc.: 52.34%] [G loss: 1.224813]\n",
      "epoch:24 step:22695 [D loss: 0.695924, acc.: 55.47%] [G loss: 1.512149]\n",
      "epoch:24 step:22696 [D loss: 0.405694, acc.: 82.03%] [G loss: 1.557297]\n",
      "epoch:24 step:22697 [D loss: 0.643739, acc.: 67.19%] [G loss: 1.099229]\n",
      "epoch:24 step:22698 [D loss: 0.659043, acc.: 60.94%] [G loss: 1.284375]\n",
      "epoch:24 step:22699 [D loss: 0.475843, acc.: 78.12%] [G loss: 1.791504]\n",
      "epoch:24 step:22700 [D loss: 0.578789, acc.: 70.31%] [G loss: 1.280395]\n",
      "epoch:24 step:22701 [D loss: 0.687085, acc.: 60.16%] [G loss: 1.280634]\n",
      "epoch:24 step:22702 [D loss: 0.499009, acc.: 71.88%] [G loss: 1.507983]\n",
      "epoch:24 step:22703 [D loss: 0.488796, acc.: 78.91%] [G loss: 1.209156]\n",
      "epoch:24 step:22704 [D loss: 0.508224, acc.: 76.56%] [G loss: 1.241330]\n",
      "epoch:24 step:22705 [D loss: 0.624613, acc.: 68.75%] [G loss: 1.050978]\n",
      "epoch:24 step:22706 [D loss: 0.602633, acc.: 67.97%] [G loss: 1.084017]\n",
      "epoch:24 step:22707 [D loss: 0.705146, acc.: 60.16%] [G loss: 1.285708]\n",
      "epoch:24 step:22708 [D loss: 0.428501, acc.: 82.03%] [G loss: 1.479029]\n",
      "epoch:24 step:22709 [D loss: 0.477393, acc.: 82.81%] [G loss: 1.396574]\n",
      "epoch:24 step:22710 [D loss: 0.597374, acc.: 69.53%] [G loss: 1.569429]\n",
      "epoch:24 step:22711 [D loss: 0.521609, acc.: 76.56%] [G loss: 1.240498]\n",
      "epoch:24 step:22712 [D loss: 0.564445, acc.: 71.09%] [G loss: 0.998414]\n",
      "epoch:24 step:22713 [D loss: 0.580147, acc.: 65.62%] [G loss: 1.560875]\n",
      "epoch:24 step:22714 [D loss: 0.428409, acc.: 81.25%] [G loss: 1.833774]\n",
      "epoch:24 step:22715 [D loss: 0.670825, acc.: 64.06%] [G loss: 1.160048]\n",
      "epoch:24 step:22716 [D loss: 0.563575, acc.: 67.19%] [G loss: 1.612757]\n",
      "epoch:24 step:22717 [D loss: 0.491399, acc.: 76.56%] [G loss: 1.727602]\n",
      "epoch:24 step:22718 [D loss: 0.577995, acc.: 68.75%] [G loss: 1.151984]\n",
      "epoch:24 step:22719 [D loss: 0.561942, acc.: 71.09%] [G loss: 1.510143]\n",
      "epoch:24 step:22720 [D loss: 0.512700, acc.: 77.34%] [G loss: 1.449968]\n",
      "epoch:24 step:22721 [D loss: 0.534554, acc.: 70.31%] [G loss: 1.248652]\n",
      "epoch:24 step:22722 [D loss: 0.526812, acc.: 77.34%] [G loss: 1.421516]\n",
      "epoch:24 step:22723 [D loss: 0.468695, acc.: 80.47%] [G loss: 1.278054]\n",
      "epoch:24 step:22724 [D loss: 0.548727, acc.: 72.66%] [G loss: 1.605583]\n",
      "epoch:24 step:22725 [D loss: 0.654236, acc.: 60.94%] [G loss: 1.255582]\n",
      "epoch:24 step:22726 [D loss: 0.656117, acc.: 63.28%] [G loss: 1.462523]\n",
      "epoch:24 step:22727 [D loss: 0.425819, acc.: 82.03%] [G loss: 1.850635]\n",
      "epoch:24 step:22728 [D loss: 0.625867, acc.: 68.75%] [G loss: 1.379665]\n",
      "epoch:24 step:22729 [D loss: 0.558639, acc.: 74.22%] [G loss: 1.100890]\n",
      "epoch:24 step:22730 [D loss: 0.631982, acc.: 60.16%] [G loss: 1.471285]\n",
      "epoch:24 step:22731 [D loss: 0.537627, acc.: 70.31%] [G loss: 1.382185]\n",
      "epoch:24 step:22732 [D loss: 0.560288, acc.: 67.97%] [G loss: 1.247289]\n",
      "epoch:24 step:22733 [D loss: 0.410373, acc.: 87.50%] [G loss: 1.490820]\n",
      "epoch:24 step:22734 [D loss: 0.490284, acc.: 78.91%] [G loss: 1.377651]\n",
      "epoch:24 step:22735 [D loss: 0.664247, acc.: 58.59%] [G loss: 1.113181]\n",
      "epoch:24 step:22736 [D loss: 0.482106, acc.: 78.12%] [G loss: 1.569775]\n",
      "epoch:24 step:22737 [D loss: 0.419004, acc.: 85.94%] [G loss: 1.382154]\n",
      "epoch:24 step:22738 [D loss: 0.394040, acc.: 86.72%] [G loss: 1.652468]\n",
      "epoch:24 step:22739 [D loss: 0.700829, acc.: 53.91%] [G loss: 1.281550]\n",
      "epoch:24 step:22740 [D loss: 0.660484, acc.: 62.50%] [G loss: 1.405582]\n",
      "epoch:24 step:22741 [D loss: 0.533803, acc.: 76.56%] [G loss: 1.227375]\n",
      "epoch:24 step:22742 [D loss: 0.521157, acc.: 75.00%] [G loss: 1.561034]\n",
      "epoch:24 step:22743 [D loss: 0.613187, acc.: 71.09%] [G loss: 1.227429]\n",
      "epoch:24 step:22744 [D loss: 0.511116, acc.: 77.34%] [G loss: 1.307686]\n",
      "epoch:24 step:22745 [D loss: 0.643384, acc.: 67.97%] [G loss: 1.250151]\n",
      "epoch:24 step:22746 [D loss: 0.382786, acc.: 87.50%] [G loss: 1.532497]\n",
      "epoch:24 step:22747 [D loss: 0.497377, acc.: 75.78%] [G loss: 1.566470]\n",
      "epoch:24 step:22748 [D loss: 0.405827, acc.: 85.16%] [G loss: 1.634452]\n",
      "epoch:24 step:22749 [D loss: 0.520004, acc.: 69.53%] [G loss: 1.293323]\n",
      "epoch:24 step:22750 [D loss: 0.587337, acc.: 67.97%] [G loss: 1.276175]\n",
      "epoch:24 step:22751 [D loss: 0.479566, acc.: 78.12%] [G loss: 1.162111]\n",
      "epoch:24 step:22752 [D loss: 0.474987, acc.: 78.12%] [G loss: 1.088386]\n",
      "epoch:24 step:22753 [D loss: 0.356977, acc.: 86.72%] [G loss: 1.423484]\n",
      "epoch:24 step:22754 [D loss: 0.535687, acc.: 69.53%] [G loss: 0.978323]\n",
      "epoch:24 step:22755 [D loss: 0.539962, acc.: 68.75%] [G loss: 1.484589]\n",
      "epoch:24 step:22756 [D loss: 0.580969, acc.: 73.44%] [G loss: 1.345747]\n",
      "epoch:24 step:22757 [D loss: 0.468804, acc.: 78.91%] [G loss: 1.380672]\n",
      "epoch:24 step:22758 [D loss: 0.616233, acc.: 67.97%] [G loss: 1.353843]\n",
      "epoch:24 step:22759 [D loss: 0.379713, acc.: 85.94%] [G loss: 1.479610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22760 [D loss: 0.809908, acc.: 48.44%] [G loss: 0.847620]\n",
      "epoch:24 step:22761 [D loss: 0.601391, acc.: 66.41%] [G loss: 1.363282]\n",
      "epoch:24 step:22762 [D loss: 0.564177, acc.: 71.88%] [G loss: 1.514972]\n",
      "epoch:24 step:22763 [D loss: 0.739463, acc.: 55.47%] [G loss: 1.465867]\n",
      "epoch:24 step:22764 [D loss: 0.596824, acc.: 65.62%] [G loss: 1.332379]\n",
      "epoch:24 step:22765 [D loss: 0.665955, acc.: 61.72%] [G loss: 1.249470]\n",
      "epoch:24 step:22766 [D loss: 0.686939, acc.: 58.59%] [G loss: 1.340986]\n",
      "epoch:24 step:22767 [D loss: 0.690083, acc.: 60.94%] [G loss: 1.279163]\n",
      "epoch:24 step:22768 [D loss: 0.620288, acc.: 65.62%] [G loss: 1.258107]\n",
      "epoch:24 step:22769 [D loss: 0.502959, acc.: 77.34%] [G loss: 1.190715]\n",
      "epoch:24 step:22770 [D loss: 0.567889, acc.: 72.66%] [G loss: 1.142245]\n",
      "epoch:24 step:22771 [D loss: 0.501840, acc.: 79.69%] [G loss: 1.712283]\n",
      "epoch:24 step:22772 [D loss: 0.678738, acc.: 60.16%] [G loss: 1.221678]\n",
      "epoch:24 step:22773 [D loss: 0.516748, acc.: 73.44%] [G loss: 1.203912]\n",
      "epoch:24 step:22774 [D loss: 0.529133, acc.: 75.78%] [G loss: 1.552602]\n",
      "epoch:24 step:22775 [D loss: 0.459118, acc.: 81.25%] [G loss: 1.603366]\n",
      "epoch:24 step:22776 [D loss: 0.764598, acc.: 56.25%] [G loss: 1.138055]\n",
      "epoch:24 step:22777 [D loss: 0.675368, acc.: 62.50%] [G loss: 1.333639]\n",
      "epoch:24 step:22778 [D loss: 0.615779, acc.: 67.97%] [G loss: 1.117935]\n",
      "epoch:24 step:22779 [D loss: 0.651978, acc.: 63.28%] [G loss: 1.212092]\n",
      "epoch:24 step:22780 [D loss: 0.517514, acc.: 76.56%] [G loss: 1.243038]\n",
      "epoch:24 step:22781 [D loss: 0.795242, acc.: 48.44%] [G loss: 1.151953]\n",
      "epoch:24 step:22782 [D loss: 0.516310, acc.: 73.44%] [G loss: 1.354430]\n",
      "epoch:24 step:22783 [D loss: 0.519956, acc.: 72.66%] [G loss: 1.511758]\n",
      "epoch:24 step:22784 [D loss: 0.588000, acc.: 71.09%] [G loss: 1.135889]\n",
      "epoch:24 step:22785 [D loss: 0.647640, acc.: 61.72%] [G loss: 1.140987]\n",
      "epoch:24 step:22786 [D loss: 0.536958, acc.: 72.66%] [G loss: 1.416378]\n",
      "epoch:24 step:22787 [D loss: 0.632373, acc.: 65.62%] [G loss: 1.549796]\n",
      "epoch:24 step:22788 [D loss: 0.540938, acc.: 73.44%] [G loss: 1.402744]\n",
      "epoch:24 step:22789 [D loss: 0.684349, acc.: 59.38%] [G loss: 1.263983]\n",
      "epoch:24 step:22790 [D loss: 0.526324, acc.: 76.56%] [G loss: 1.472018]\n",
      "epoch:24 step:22791 [D loss: 0.741539, acc.: 56.25%] [G loss: 1.177390]\n",
      "epoch:24 step:22792 [D loss: 0.642925, acc.: 65.62%] [G loss: 0.993347]\n",
      "epoch:24 step:22793 [D loss: 0.799595, acc.: 50.00%] [G loss: 1.052099]\n",
      "epoch:24 step:22794 [D loss: 0.545350, acc.: 70.31%] [G loss: 1.508739]\n",
      "epoch:24 step:22795 [D loss: 0.652376, acc.: 64.06%] [G loss: 0.982853]\n",
      "epoch:24 step:22796 [D loss: 0.488710, acc.: 81.25%] [G loss: 1.418213]\n",
      "epoch:24 step:22797 [D loss: 0.568450, acc.: 71.88%] [G loss: 1.270568]\n",
      "epoch:24 step:22798 [D loss: 0.595614, acc.: 67.19%] [G loss: 1.498151]\n",
      "epoch:24 step:22799 [D loss: 0.521864, acc.: 72.66%] [G loss: 1.522353]\n",
      "epoch:24 step:22800 [D loss: 0.677725, acc.: 62.50%] [G loss: 1.179146]\n",
      "##############\n",
      "[2.75479175 2.31929006 2.1434428  2.98019232 1.20085083 5.83950157\n",
      " 2.21032347 2.80834839 4.03433908 4.79106953]\n",
      "##########\n",
      "epoch:24 step:22801 [D loss: 0.478031, acc.: 78.91%] [G loss: 1.210681]\n",
      "epoch:24 step:22802 [D loss: 0.497761, acc.: 76.56%] [G loss: 1.449961]\n",
      "epoch:24 step:22803 [D loss: 0.515758, acc.: 73.44%] [G loss: 1.514926]\n",
      "epoch:24 step:22804 [D loss: 0.568425, acc.: 68.75%] [G loss: 1.472860]\n",
      "epoch:24 step:22805 [D loss: 0.549637, acc.: 75.00%] [G loss: 1.335856]\n",
      "epoch:24 step:22806 [D loss: 0.617037, acc.: 69.53%] [G loss: 1.185516]\n",
      "epoch:24 step:22807 [D loss: 0.547751, acc.: 75.78%] [G loss: 1.233798]\n",
      "epoch:24 step:22808 [D loss: 0.548840, acc.: 67.97%] [G loss: 1.225672]\n",
      "epoch:24 step:22809 [D loss: 0.631867, acc.: 62.50%] [G loss: 1.421821]\n",
      "epoch:24 step:22810 [D loss: 0.434680, acc.: 82.81%] [G loss: 1.444960]\n",
      "epoch:24 step:22811 [D loss: 0.543651, acc.: 73.44%] [G loss: 1.560676]\n",
      "epoch:24 step:22812 [D loss: 0.649699, acc.: 59.38%] [G loss: 1.234247]\n",
      "epoch:24 step:22813 [D loss: 0.465557, acc.: 78.91%] [G loss: 1.093448]\n",
      "epoch:24 step:22814 [D loss: 0.544691, acc.: 72.66%] [G loss: 1.026408]\n",
      "epoch:24 step:22815 [D loss: 0.360504, acc.: 88.28%] [G loss: 1.353513]\n",
      "epoch:24 step:22816 [D loss: 0.542698, acc.: 74.22%] [G loss: 1.509055]\n",
      "epoch:24 step:22817 [D loss: 0.533121, acc.: 75.78%] [G loss: 1.360335]\n",
      "epoch:24 step:22818 [D loss: 0.604438, acc.: 64.06%] [G loss: 1.309924]\n",
      "epoch:24 step:22819 [D loss: 0.623064, acc.: 67.19%] [G loss: 1.459359]\n",
      "epoch:24 step:22820 [D loss: 0.503922, acc.: 71.88%] [G loss: 1.289976]\n",
      "epoch:24 step:22821 [D loss: 0.455714, acc.: 79.69%] [G loss: 1.347078]\n",
      "epoch:24 step:22822 [D loss: 0.537192, acc.: 73.44%] [G loss: 1.346569]\n",
      "epoch:24 step:22823 [D loss: 0.665091, acc.: 64.84%] [G loss: 1.701381]\n",
      "epoch:24 step:22824 [D loss: 0.586922, acc.: 67.19%] [G loss: 1.287016]\n",
      "epoch:24 step:22825 [D loss: 0.469102, acc.: 84.38%] [G loss: 1.015549]\n",
      "epoch:24 step:22826 [D loss: 0.518973, acc.: 75.00%] [G loss: 1.535136]\n",
      "epoch:24 step:22827 [D loss: 0.564455, acc.: 72.66%] [G loss: 1.495702]\n",
      "epoch:24 step:22828 [D loss: 0.455406, acc.: 82.81%] [G loss: 1.353032]\n",
      "epoch:24 step:22829 [D loss: 0.560174, acc.: 71.88%] [G loss: 1.348522]\n",
      "epoch:24 step:22830 [D loss: 0.553826, acc.: 78.12%] [G loss: 1.332112]\n",
      "epoch:24 step:22831 [D loss: 0.571033, acc.: 67.19%] [G loss: 1.326778]\n",
      "epoch:24 step:22832 [D loss: 0.553769, acc.: 77.34%] [G loss: 0.873217]\n",
      "epoch:24 step:22833 [D loss: 0.573526, acc.: 67.97%] [G loss: 1.191553]\n",
      "epoch:24 step:22834 [D loss: 0.626345, acc.: 60.16%] [G loss: 1.305169]\n",
      "epoch:24 step:22835 [D loss: 0.490043, acc.: 75.00%] [G loss: 0.916627]\n",
      "epoch:24 step:22836 [D loss: 0.527744, acc.: 75.78%] [G loss: 1.276165]\n",
      "epoch:24 step:22837 [D loss: 0.460806, acc.: 80.47%] [G loss: 1.219906]\n",
      "epoch:24 step:22838 [D loss: 0.631361, acc.: 67.97%] [G loss: 1.173599]\n",
      "epoch:24 step:22839 [D loss: 0.505054, acc.: 71.88%] [G loss: 1.466287]\n",
      "epoch:24 step:22840 [D loss: 0.519987, acc.: 71.88%] [G loss: 1.654783]\n",
      "epoch:24 step:22841 [D loss: 0.516944, acc.: 76.56%] [G loss: 1.270554]\n",
      "epoch:24 step:22842 [D loss: 0.538725, acc.: 74.22%] [G loss: 1.289418]\n",
      "epoch:24 step:22843 [D loss: 0.399794, acc.: 86.72%] [G loss: 1.612496]\n",
      "epoch:24 step:22844 [D loss: 0.555800, acc.: 71.88%] [G loss: 0.987575]\n",
      "epoch:24 step:22845 [D loss: 0.537480, acc.: 73.44%] [G loss: 1.622062]\n",
      "epoch:24 step:22846 [D loss: 0.666442, acc.: 71.09%] [G loss: 1.182570]\n",
      "epoch:24 step:22847 [D loss: 0.787446, acc.: 42.97%] [G loss: 0.913994]\n",
      "epoch:24 step:22848 [D loss: 0.531022, acc.: 73.44%] [G loss: 1.164411]\n",
      "epoch:24 step:22849 [D loss: 0.714627, acc.: 58.59%] [G loss: 1.403754]\n",
      "epoch:24 step:22850 [D loss: 0.515005, acc.: 78.91%] [G loss: 1.342073]\n",
      "epoch:24 step:22851 [D loss: 0.597155, acc.: 71.09%] [G loss: 1.350020]\n",
      "epoch:24 step:22852 [D loss: 0.583576, acc.: 68.75%] [G loss: 1.400985]\n",
      "epoch:24 step:22853 [D loss: 0.508906, acc.: 76.56%] [G loss: 1.176907]\n",
      "epoch:24 step:22854 [D loss: 0.566891, acc.: 70.31%] [G loss: 1.348483]\n",
      "epoch:24 step:22855 [D loss: 0.522777, acc.: 68.75%] [G loss: 1.148045]\n",
      "epoch:24 step:22856 [D loss: 0.527427, acc.: 75.00%] [G loss: 1.541674]\n",
      "epoch:24 step:22857 [D loss: 0.434045, acc.: 81.25%] [G loss: 1.433663]\n",
      "epoch:24 step:22858 [D loss: 0.560676, acc.: 75.00%] [G loss: 1.182664]\n",
      "epoch:24 step:22859 [D loss: 0.562219, acc.: 74.22%] [G loss: 1.085439]\n",
      "epoch:24 step:22860 [D loss: 0.565064, acc.: 76.56%] [G loss: 1.825621]\n",
      "epoch:24 step:22861 [D loss: 0.704008, acc.: 60.16%] [G loss: 1.416659]\n",
      "epoch:24 step:22862 [D loss: 0.669519, acc.: 64.84%] [G loss: 1.381418]\n",
      "epoch:24 step:22863 [D loss: 0.700813, acc.: 59.38%] [G loss: 1.277456]\n",
      "epoch:24 step:22864 [D loss: 0.524068, acc.: 72.66%] [G loss: 1.376806]\n",
      "epoch:24 step:22865 [D loss: 0.595025, acc.: 61.72%] [G loss: 1.481104]\n",
      "epoch:24 step:22866 [D loss: 0.527894, acc.: 76.56%] [G loss: 1.835594]\n",
      "epoch:24 step:22867 [D loss: 0.586367, acc.: 67.19%] [G loss: 1.430997]\n",
      "epoch:24 step:22868 [D loss: 0.591861, acc.: 67.19%] [G loss: 2.011360]\n",
      "epoch:24 step:22869 [D loss: 0.519278, acc.: 71.09%] [G loss: 1.404723]\n",
      "epoch:24 step:22870 [D loss: 0.606048, acc.: 67.19%] [G loss: 1.482688]\n",
      "epoch:24 step:22871 [D loss: 0.526568, acc.: 75.00%] [G loss: 1.278529]\n",
      "epoch:24 step:22872 [D loss: 0.577406, acc.: 71.88%] [G loss: 1.406685]\n",
      "epoch:24 step:22873 [D loss: 0.549010, acc.: 73.44%] [G loss: 1.647736]\n",
      "epoch:24 step:22874 [D loss: 0.505565, acc.: 79.69%] [G loss: 1.459619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22875 [D loss: 0.490176, acc.: 78.12%] [G loss: 1.046117]\n",
      "epoch:24 step:22876 [D loss: 0.846452, acc.: 47.66%] [G loss: 1.006572]\n",
      "epoch:24 step:22877 [D loss: 0.504851, acc.: 78.12%] [G loss: 1.401468]\n",
      "epoch:24 step:22878 [D loss: 0.670155, acc.: 62.50%] [G loss: 0.923668]\n",
      "epoch:24 step:22879 [D loss: 0.691188, acc.: 58.59%] [G loss: 1.265150]\n",
      "epoch:24 step:22880 [D loss: 0.513668, acc.: 77.34%] [G loss: 1.568751]\n",
      "epoch:24 step:22881 [D loss: 0.662408, acc.: 57.81%] [G loss: 1.348871]\n",
      "epoch:24 step:22882 [D loss: 0.736272, acc.: 57.81%] [G loss: 1.381005]\n",
      "epoch:24 step:22883 [D loss: 0.429787, acc.: 79.69%] [G loss: 1.559319]\n",
      "epoch:24 step:22884 [D loss: 0.727820, acc.: 57.03%] [G loss: 1.402293]\n",
      "epoch:24 step:22885 [D loss: 0.504167, acc.: 75.00%] [G loss: 1.363664]\n",
      "epoch:24 step:22886 [D loss: 0.428036, acc.: 82.03%] [G loss: 1.698942]\n",
      "epoch:24 step:22887 [D loss: 0.523850, acc.: 71.09%] [G loss: 1.466712]\n",
      "epoch:24 step:22888 [D loss: 0.654766, acc.: 60.94%] [G loss: 1.372719]\n",
      "epoch:24 step:22889 [D loss: 0.651075, acc.: 59.38%] [G loss: 1.498749]\n",
      "epoch:24 step:22890 [D loss: 0.373421, acc.: 85.94%] [G loss: 1.271126]\n",
      "epoch:24 step:22891 [D loss: 0.474680, acc.: 77.34%] [G loss: 1.745314]\n",
      "epoch:24 step:22892 [D loss: 0.493631, acc.: 76.56%] [G loss: 1.389754]\n",
      "epoch:24 step:22893 [D loss: 0.576681, acc.: 69.53%] [G loss: 1.866666]\n",
      "epoch:24 step:22894 [D loss: 0.483224, acc.: 76.56%] [G loss: 1.534748]\n",
      "epoch:24 step:22895 [D loss: 0.767475, acc.: 52.34%] [G loss: 1.047212]\n",
      "epoch:24 step:22896 [D loss: 0.456272, acc.: 82.03%] [G loss: 1.344017]\n",
      "epoch:24 step:22897 [D loss: 0.813667, acc.: 48.44%] [G loss: 1.362702]\n",
      "epoch:24 step:22898 [D loss: 0.490446, acc.: 73.44%] [G loss: 1.905202]\n",
      "epoch:24 step:22899 [D loss: 0.671441, acc.: 62.50%] [G loss: 1.138664]\n",
      "epoch:24 step:22900 [D loss: 0.601368, acc.: 70.31%] [G loss: 0.990173]\n",
      "epoch:24 step:22901 [D loss: 0.582607, acc.: 71.09%] [G loss: 0.977977]\n",
      "epoch:24 step:22902 [D loss: 0.547989, acc.: 76.56%] [G loss: 1.255593]\n",
      "epoch:24 step:22903 [D loss: 0.400498, acc.: 87.50%] [G loss: 1.665076]\n",
      "epoch:24 step:22904 [D loss: 0.543609, acc.: 70.31%] [G loss: 1.450682]\n",
      "epoch:24 step:22905 [D loss: 0.597379, acc.: 67.97%] [G loss: 1.512624]\n",
      "epoch:24 step:22906 [D loss: 0.596176, acc.: 70.31%] [G loss: 1.081218]\n",
      "epoch:24 step:22907 [D loss: 0.496916, acc.: 77.34%] [G loss: 1.744281]\n",
      "epoch:24 step:22908 [D loss: 0.620635, acc.: 64.84%] [G loss: 1.139012]\n",
      "epoch:24 step:22909 [D loss: 0.474233, acc.: 82.03%] [G loss: 1.272961]\n",
      "epoch:24 step:22910 [D loss: 0.539579, acc.: 73.44%] [G loss: 1.289593]\n",
      "epoch:24 step:22911 [D loss: 0.672528, acc.: 60.16%] [G loss: 1.256485]\n",
      "epoch:24 step:22912 [D loss: 0.551091, acc.: 70.31%] [G loss: 1.116176]\n",
      "epoch:24 step:22913 [D loss: 0.517988, acc.: 77.34%] [G loss: 1.544610]\n",
      "epoch:24 step:22914 [D loss: 0.622258, acc.: 67.19%] [G loss: 1.311927]\n",
      "epoch:24 step:22915 [D loss: 0.499169, acc.: 73.44%] [G loss: 1.466743]\n",
      "epoch:24 step:22916 [D loss: 0.538934, acc.: 72.66%] [G loss: 1.109668]\n",
      "epoch:24 step:22917 [D loss: 0.549155, acc.: 77.34%] [G loss: 1.075388]\n",
      "epoch:24 step:22918 [D loss: 0.518637, acc.: 74.22%] [G loss: 1.280146]\n",
      "epoch:24 step:22919 [D loss: 0.538772, acc.: 71.88%] [G loss: 1.583878]\n",
      "epoch:24 step:22920 [D loss: 0.583081, acc.: 72.66%] [G loss: 1.194473]\n",
      "epoch:24 step:22921 [D loss: 0.497510, acc.: 80.47%] [G loss: 1.268256]\n",
      "epoch:24 step:22922 [D loss: 0.564348, acc.: 73.44%] [G loss: 1.418920]\n",
      "epoch:24 step:22923 [D loss: 0.504316, acc.: 77.34%] [G loss: 1.396795]\n",
      "epoch:24 step:22924 [D loss: 0.562469, acc.: 70.31%] [G loss: 1.514779]\n",
      "epoch:24 step:22925 [D loss: 0.590475, acc.: 67.19%] [G loss: 0.903695]\n",
      "epoch:24 step:22926 [D loss: 0.605156, acc.: 67.19%] [G loss: 1.199329]\n",
      "epoch:24 step:22927 [D loss: 0.580153, acc.: 68.75%] [G loss: 1.737845]\n",
      "epoch:24 step:22928 [D loss: 0.697942, acc.: 60.16%] [G loss: 1.520988]\n",
      "epoch:24 step:22929 [D loss: 0.412774, acc.: 85.94%] [G loss: 1.593055]\n",
      "epoch:24 step:22930 [D loss: 0.701939, acc.: 60.16%] [G loss: 1.231618]\n",
      "epoch:24 step:22931 [D loss: 0.546416, acc.: 74.22%] [G loss: 1.174754]\n",
      "epoch:24 step:22932 [D loss: 0.490273, acc.: 75.78%] [G loss: 1.600898]\n",
      "epoch:24 step:22933 [D loss: 0.487874, acc.: 76.56%] [G loss: 1.164449]\n",
      "epoch:24 step:22934 [D loss: 0.559108, acc.: 67.97%] [G loss: 1.525169]\n",
      "epoch:24 step:22935 [D loss: 0.543528, acc.: 71.09%] [G loss: 1.898296]\n",
      "epoch:24 step:22936 [D loss: 0.554079, acc.: 71.09%] [G loss: 1.484420]\n",
      "epoch:24 step:22937 [D loss: 0.532375, acc.: 69.53%] [G loss: 1.660802]\n",
      "epoch:24 step:22938 [D loss: 0.688111, acc.: 63.28%] [G loss: 1.248502]\n",
      "epoch:24 step:22939 [D loss: 0.513199, acc.: 77.34%] [G loss: 1.538816]\n",
      "epoch:24 step:22940 [D loss: 0.455402, acc.: 77.34%] [G loss: 1.643322]\n",
      "epoch:24 step:22941 [D loss: 0.410319, acc.: 84.38%] [G loss: 1.274977]\n",
      "epoch:24 step:22942 [D loss: 0.705835, acc.: 62.50%] [G loss: 1.280375]\n",
      "epoch:24 step:22943 [D loss: 0.595292, acc.: 68.75%] [G loss: 1.230613]\n",
      "epoch:24 step:22944 [D loss: 0.669927, acc.: 62.50%] [G loss: 1.105192]\n",
      "epoch:24 step:22945 [D loss: 0.430198, acc.: 84.38%] [G loss: 1.931623]\n",
      "epoch:24 step:22946 [D loss: 0.484050, acc.: 76.56%] [G loss: 1.230812]\n",
      "epoch:24 step:22947 [D loss: 0.557911, acc.: 71.88%] [G loss: 1.336575]\n",
      "epoch:24 step:22948 [D loss: 0.532045, acc.: 72.66%] [G loss: 1.520812]\n",
      "epoch:24 step:22949 [D loss: 0.635613, acc.: 68.75%] [G loss: 1.159341]\n",
      "epoch:24 step:22950 [D loss: 0.507120, acc.: 75.78%] [G loss: 1.491781]\n",
      "epoch:24 step:22951 [D loss: 0.686507, acc.: 57.81%] [G loss: 1.185978]\n",
      "epoch:24 step:22952 [D loss: 0.646049, acc.: 64.06%] [G loss: 1.322306]\n",
      "epoch:24 step:22953 [D loss: 0.666597, acc.: 62.50%] [G loss: 1.149911]\n",
      "epoch:24 step:22954 [D loss: 0.583090, acc.: 67.19%] [G loss: 1.066857]\n",
      "epoch:24 step:22955 [D loss: 0.510876, acc.: 75.78%] [G loss: 1.716423]\n",
      "epoch:24 step:22956 [D loss: 0.649608, acc.: 62.50%] [G loss: 1.192457]\n",
      "epoch:24 step:22957 [D loss: 0.615488, acc.: 65.62%] [G loss: 1.299173]\n",
      "epoch:24 step:22958 [D loss: 0.773175, acc.: 53.91%] [G loss: 1.107385]\n",
      "epoch:24 step:22959 [D loss: 0.743593, acc.: 51.56%] [G loss: 1.053289]\n",
      "epoch:24 step:22960 [D loss: 0.487080, acc.: 77.34%] [G loss: 1.525671]\n",
      "epoch:24 step:22961 [D loss: 0.323442, acc.: 92.97%] [G loss: 1.441128]\n",
      "epoch:24 step:22962 [D loss: 0.622476, acc.: 67.19%] [G loss: 1.562452]\n",
      "epoch:24 step:22963 [D loss: 0.533853, acc.: 75.78%] [G loss: 1.438717]\n",
      "epoch:24 step:22964 [D loss: 0.691674, acc.: 64.06%] [G loss: 1.378041]\n",
      "epoch:24 step:22965 [D loss: 0.677229, acc.: 64.06%] [G loss: 1.465652]\n",
      "epoch:24 step:22966 [D loss: 0.595544, acc.: 67.97%] [G loss: 1.618826]\n",
      "epoch:24 step:22967 [D loss: 0.669718, acc.: 64.06%] [G loss: 1.188881]\n",
      "epoch:24 step:22968 [D loss: 0.514412, acc.: 75.78%] [G loss: 1.397972]\n",
      "epoch:24 step:22969 [D loss: 0.531270, acc.: 74.22%] [G loss: 1.143573]\n",
      "epoch:24 step:22970 [D loss: 0.562517, acc.: 71.88%] [G loss: 1.435870]\n",
      "epoch:24 step:22971 [D loss: 0.706306, acc.: 59.38%] [G loss: 1.193070]\n",
      "epoch:24 step:22972 [D loss: 0.564266, acc.: 71.09%] [G loss: 1.554568]\n",
      "epoch:24 step:22973 [D loss: 0.361876, acc.: 86.72%] [G loss: 1.334207]\n",
      "epoch:24 step:22974 [D loss: 0.568002, acc.: 71.88%] [G loss: 1.411091]\n",
      "epoch:24 step:22975 [D loss: 0.600438, acc.: 64.84%] [G loss: 1.418947]\n",
      "epoch:24 step:22976 [D loss: 0.380721, acc.: 84.38%] [G loss: 1.613622]\n",
      "epoch:24 step:22977 [D loss: 0.365267, acc.: 89.84%] [G loss: 1.544995]\n",
      "epoch:24 step:22978 [D loss: 0.622476, acc.: 65.62%] [G loss: 1.571971]\n",
      "epoch:24 step:22979 [D loss: 0.539160, acc.: 71.88%] [G loss: 1.297335]\n",
      "epoch:24 step:22980 [D loss: 0.839583, acc.: 51.56%] [G loss: 1.158243]\n",
      "epoch:24 step:22981 [D loss: 0.456016, acc.: 79.69%] [G loss: 1.591239]\n",
      "epoch:24 step:22982 [D loss: 0.463312, acc.: 80.47%] [G loss: 1.571620]\n",
      "epoch:24 step:22983 [D loss: 0.508543, acc.: 81.25%] [G loss: 1.480284]\n",
      "epoch:24 step:22984 [D loss: 0.652396, acc.: 60.94%] [G loss: 1.174436]\n",
      "epoch:24 step:22985 [D loss: 0.627183, acc.: 63.28%] [G loss: 1.336292]\n",
      "epoch:24 step:22986 [D loss: 0.413692, acc.: 87.50%] [G loss: 1.425672]\n",
      "epoch:24 step:22987 [D loss: 0.454496, acc.: 81.25%] [G loss: 1.374358]\n",
      "epoch:24 step:22988 [D loss: 0.509224, acc.: 71.88%] [G loss: 1.358423]\n",
      "epoch:24 step:22989 [D loss: 0.498192, acc.: 74.22%] [G loss: 1.257684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22990 [D loss: 0.621695, acc.: 69.53%] [G loss: 1.590080]\n",
      "epoch:24 step:22991 [D loss: 0.499911, acc.: 75.78%] [G loss: 1.476749]\n",
      "epoch:24 step:22992 [D loss: 0.506354, acc.: 74.22%] [G loss: 1.274667]\n",
      "epoch:24 step:22993 [D loss: 0.464031, acc.: 78.91%] [G loss: 1.419364]\n",
      "epoch:24 step:22994 [D loss: 0.424753, acc.: 80.47%] [G loss: 1.413274]\n",
      "epoch:24 step:22995 [D loss: 0.705242, acc.: 60.94%] [G loss: 1.178415]\n",
      "epoch:24 step:22996 [D loss: 0.460759, acc.: 77.34%] [G loss: 1.524549]\n",
      "epoch:24 step:22997 [D loss: 0.415402, acc.: 84.38%] [G loss: 1.507506]\n",
      "epoch:24 step:22998 [D loss: 0.480490, acc.: 78.91%] [G loss: 1.405011]\n",
      "epoch:24 step:22999 [D loss: 0.436876, acc.: 80.47%] [G loss: 1.351437]\n",
      "epoch:24 step:23000 [D loss: 0.544954, acc.: 73.44%] [G loss: 1.292235]\n",
      "##############\n",
      "[2.78442984 2.14190713 2.2327443  2.75285388 1.14170984 5.69157061\n",
      " 2.52122136 2.55468689 3.86198566 5.4466181 ]\n",
      "##########\n",
      "epoch:24 step:23001 [D loss: 0.513645, acc.: 74.22%] [G loss: 1.440532]\n",
      "epoch:24 step:23002 [D loss: 0.395015, acc.: 80.47%] [G loss: 1.574546]\n",
      "epoch:24 step:23003 [D loss: 0.581475, acc.: 72.66%] [G loss: 1.463673]\n",
      "epoch:24 step:23004 [D loss: 0.491988, acc.: 75.78%] [G loss: 1.489831]\n",
      "epoch:24 step:23005 [D loss: 0.702922, acc.: 58.59%] [G loss: 1.277822]\n",
      "epoch:24 step:23006 [D loss: 0.542265, acc.: 71.09%] [G loss: 1.561128]\n",
      "epoch:24 step:23007 [D loss: 0.509959, acc.: 72.66%] [G loss: 1.370092]\n",
      "epoch:24 step:23008 [D loss: 0.542778, acc.: 71.88%] [G loss: 1.737918]\n",
      "epoch:24 step:23009 [D loss: 0.514952, acc.: 75.00%] [G loss: 1.318759]\n",
      "epoch:24 step:23010 [D loss: 0.545977, acc.: 71.88%] [G loss: 1.307545]\n",
      "epoch:24 step:23011 [D loss: 0.485027, acc.: 74.22%] [G loss: 1.359697]\n",
      "epoch:24 step:23012 [D loss: 0.596481, acc.: 70.31%] [G loss: 1.579454]\n",
      "epoch:24 step:23013 [D loss: 0.672239, acc.: 62.50%] [G loss: 1.188483]\n",
      "epoch:24 step:23014 [D loss: 0.615996, acc.: 68.75%] [G loss: 1.760342]\n",
      "epoch:24 step:23015 [D loss: 0.646034, acc.: 64.84%] [G loss: 1.963232]\n",
      "epoch:24 step:23016 [D loss: 0.451444, acc.: 78.91%] [G loss: 1.308973]\n",
      "epoch:24 step:23017 [D loss: 0.435102, acc.: 82.03%] [G loss: 1.619478]\n",
      "epoch:24 step:23018 [D loss: 0.607359, acc.: 67.19%] [G loss: 1.875535]\n",
      "epoch:24 step:23019 [D loss: 0.450688, acc.: 82.03%] [G loss: 1.625701]\n",
      "epoch:24 step:23020 [D loss: 0.653270, acc.: 67.97%] [G loss: 1.257593]\n",
      "epoch:24 step:23021 [D loss: 0.588305, acc.: 66.41%] [G loss: 1.151395]\n",
      "epoch:24 step:23022 [D loss: 0.697481, acc.: 57.03%] [G loss: 0.904653]\n",
      "epoch:24 step:23023 [D loss: 0.509489, acc.: 71.88%] [G loss: 1.766054]\n",
      "epoch:24 step:23024 [D loss: 0.710472, acc.: 60.16%] [G loss: 1.468982]\n",
      "epoch:24 step:23025 [D loss: 0.626160, acc.: 63.28%] [G loss: 1.044424]\n",
      "epoch:24 step:23026 [D loss: 0.518225, acc.: 75.00%] [G loss: 1.151694]\n",
      "epoch:24 step:23027 [D loss: 0.564446, acc.: 71.09%] [G loss: 1.360476]\n",
      "epoch:24 step:23028 [D loss: 0.571252, acc.: 67.97%] [G loss: 1.530183]\n",
      "epoch:24 step:23029 [D loss: 0.452925, acc.: 79.69%] [G loss: 1.817222]\n",
      "epoch:24 step:23030 [D loss: 0.623082, acc.: 69.53%] [G loss: 1.590471]\n",
      "epoch:24 step:23031 [D loss: 0.643564, acc.: 68.75%] [G loss: 1.253690]\n",
      "epoch:24 step:23032 [D loss: 0.513161, acc.: 74.22%] [G loss: 1.572144]\n",
      "epoch:24 step:23033 [D loss: 0.460322, acc.: 78.91%] [G loss: 1.623434]\n",
      "epoch:24 step:23034 [D loss: 0.613406, acc.: 65.62%] [G loss: 1.018828]\n",
      "epoch:24 step:23035 [D loss: 0.497497, acc.: 77.34%] [G loss: 1.199385]\n",
      "epoch:24 step:23036 [D loss: 0.603122, acc.: 69.53%] [G loss: 1.541369]\n",
      "epoch:24 step:23037 [D loss: 0.552139, acc.: 71.88%] [G loss: 0.989765]\n",
      "epoch:24 step:23038 [D loss: 0.380652, acc.: 85.94%] [G loss: 1.238597]\n",
      "epoch:24 step:23039 [D loss: 0.540350, acc.: 65.62%] [G loss: 1.217838]\n",
      "epoch:24 step:23040 [D loss: 0.551795, acc.: 73.44%] [G loss: 1.300657]\n",
      "epoch:24 step:23041 [D loss: 0.529667, acc.: 75.78%] [G loss: 1.821076]\n",
      "epoch:24 step:23042 [D loss: 0.648280, acc.: 60.94%] [G loss: 1.590243]\n",
      "epoch:24 step:23043 [D loss: 0.581682, acc.: 74.22%] [G loss: 1.451902]\n",
      "epoch:24 step:23044 [D loss: 0.624772, acc.: 64.06%] [G loss: 1.089916]\n",
      "epoch:24 step:23045 [D loss: 0.482703, acc.: 75.00%] [G loss: 1.390353]\n",
      "epoch:24 step:23046 [D loss: 0.515124, acc.: 78.12%] [G loss: 1.097803]\n",
      "epoch:24 step:23047 [D loss: 0.673663, acc.: 62.50%] [G loss: 1.139988]\n",
      "epoch:24 step:23048 [D loss: 0.430442, acc.: 83.59%] [G loss: 1.291318]\n",
      "epoch:24 step:23049 [D loss: 0.398414, acc.: 82.03%] [G loss: 1.747885]\n",
      "epoch:24 step:23050 [D loss: 0.552046, acc.: 70.31%] [G loss: 1.318133]\n",
      "epoch:24 step:23051 [D loss: 0.412118, acc.: 87.50%] [G loss: 1.053456]\n",
      "epoch:24 step:23052 [D loss: 0.601070, acc.: 67.97%] [G loss: 1.452793]\n",
      "epoch:24 step:23053 [D loss: 0.536829, acc.: 75.78%] [G loss: 1.525634]\n",
      "epoch:24 step:23054 [D loss: 0.607857, acc.: 72.66%] [G loss: 1.259857]\n",
      "epoch:24 step:23055 [D loss: 0.503767, acc.: 78.91%] [G loss: 1.638584]\n",
      "epoch:24 step:23056 [D loss: 0.670356, acc.: 64.06%] [G loss: 1.594761]\n",
      "epoch:24 step:23057 [D loss: 0.590940, acc.: 73.44%] [G loss: 1.676241]\n",
      "epoch:24 step:23058 [D loss: 0.548217, acc.: 68.75%] [G loss: 1.841281]\n",
      "epoch:24 step:23059 [D loss: 0.592761, acc.: 67.97%] [G loss: 1.406937]\n",
      "epoch:24 step:23060 [D loss: 0.484743, acc.: 75.00%] [G loss: 1.532090]\n",
      "epoch:24 step:23061 [D loss: 0.580025, acc.: 71.88%] [G loss: 1.463544]\n",
      "epoch:24 step:23062 [D loss: 0.495644, acc.: 78.12%] [G loss: 1.167614]\n",
      "epoch:24 step:23063 [D loss: 0.463335, acc.: 78.91%] [G loss: 1.289890]\n",
      "epoch:24 step:23064 [D loss: 0.429782, acc.: 80.47%] [G loss: 1.338836]\n",
      "epoch:24 step:23065 [D loss: 0.537697, acc.: 73.44%] [G loss: 1.146465]\n",
      "epoch:24 step:23066 [D loss: 0.610625, acc.: 66.41%] [G loss: 1.189659]\n",
      "epoch:24 step:23067 [D loss: 0.530629, acc.: 72.66%] [G loss: 1.189572]\n",
      "epoch:24 step:23068 [D loss: 0.516170, acc.: 72.66%] [G loss: 1.328333]\n",
      "epoch:24 step:23069 [D loss: 0.681150, acc.: 58.59%] [G loss: 1.387293]\n",
      "epoch:24 step:23070 [D loss: 0.532972, acc.: 72.66%] [G loss: 1.336808]\n",
      "epoch:24 step:23071 [D loss: 0.685306, acc.: 59.38%] [G loss: 1.172916]\n",
      "epoch:24 step:23072 [D loss: 0.516611, acc.: 71.09%] [G loss: 1.463949]\n",
      "epoch:24 step:23073 [D loss: 0.571701, acc.: 71.88%] [G loss: 1.553256]\n",
      "epoch:24 step:23074 [D loss: 0.506622, acc.: 75.00%] [G loss: 1.344563]\n",
      "epoch:24 step:23075 [D loss: 0.619192, acc.: 71.09%] [G loss: 0.903258]\n",
      "epoch:24 step:23076 [D loss: 0.737174, acc.: 55.47%] [G loss: 1.110864]\n",
      "epoch:24 step:23077 [D loss: 0.505551, acc.: 73.44%] [G loss: 0.958371]\n",
      "epoch:24 step:23078 [D loss: 0.457348, acc.: 82.81%] [G loss: 1.626844]\n",
      "epoch:24 step:23079 [D loss: 0.669352, acc.: 59.38%] [G loss: 1.654020]\n",
      "epoch:24 step:23080 [D loss: 0.450911, acc.: 79.69%] [G loss: 1.506884]\n",
      "epoch:24 step:23081 [D loss: 0.596076, acc.: 71.88%] [G loss: 1.560724]\n",
      "epoch:24 step:23082 [D loss: 0.712799, acc.: 58.59%] [G loss: 0.875119]\n",
      "epoch:24 step:23083 [D loss: 0.606196, acc.: 67.97%] [G loss: 1.492802]\n",
      "epoch:24 step:23084 [D loss: 0.409175, acc.: 85.16%] [G loss: 1.336123]\n",
      "epoch:24 step:23085 [D loss: 0.704596, acc.: 60.94%] [G loss: 1.372533]\n",
      "epoch:24 step:23086 [D loss: 0.520736, acc.: 73.44%] [G loss: 1.869243]\n",
      "epoch:24 step:23087 [D loss: 0.510703, acc.: 77.34%] [G loss: 1.320219]\n",
      "epoch:24 step:23088 [D loss: 0.435298, acc.: 82.81%] [G loss: 1.333749]\n",
      "epoch:24 step:23089 [D loss: 0.793715, acc.: 50.00%] [G loss: 1.302631]\n",
      "epoch:24 step:23090 [D loss: 0.560456, acc.: 72.66%] [G loss: 1.602660]\n",
      "epoch:24 step:23091 [D loss: 0.651128, acc.: 64.84%] [G loss: 1.396655]\n",
      "epoch:24 step:23092 [D loss: 0.538403, acc.: 71.09%] [G loss: 1.573980]\n",
      "epoch:24 step:23093 [D loss: 0.719362, acc.: 56.25%] [G loss: 1.077501]\n",
      "epoch:24 step:23094 [D loss: 0.546497, acc.: 75.00%] [G loss: 1.197765]\n",
      "epoch:24 step:23095 [D loss: 0.443943, acc.: 80.47%] [G loss: 1.277621]\n",
      "epoch:24 step:23096 [D loss: 0.468527, acc.: 79.69%] [G loss: 1.292195]\n",
      "epoch:24 step:23097 [D loss: 0.560844, acc.: 73.44%] [G loss: 1.395930]\n",
      "epoch:24 step:23098 [D loss: 0.566163, acc.: 72.66%] [G loss: 1.323206]\n",
      "epoch:24 step:23099 [D loss: 0.663687, acc.: 63.28%] [G loss: 1.403307]\n",
      "epoch:24 step:23100 [D loss: 0.449685, acc.: 78.12%] [G loss: 1.624058]\n",
      "epoch:24 step:23101 [D loss: 0.598225, acc.: 68.75%] [G loss: 1.400934]\n",
      "epoch:24 step:23102 [D loss: 0.610420, acc.: 69.53%] [G loss: 0.902973]\n",
      "epoch:24 step:23103 [D loss: 0.648152, acc.: 65.62%] [G loss: 1.477925]\n",
      "epoch:24 step:23104 [D loss: 0.621876, acc.: 66.41%] [G loss: 1.095353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23105 [D loss: 0.427965, acc.: 88.28%] [G loss: 1.377464]\n",
      "epoch:24 step:23106 [D loss: 0.567878, acc.: 72.66%] [G loss: 1.238667]\n",
      "epoch:24 step:23107 [D loss: 0.621650, acc.: 67.19%] [G loss: 1.443057]\n",
      "epoch:24 step:23108 [D loss: 0.689285, acc.: 59.38%] [G loss: 0.973297]\n",
      "epoch:24 step:23109 [D loss: 0.592208, acc.: 67.19%] [G loss: 1.305047]\n",
      "epoch:24 step:23110 [D loss: 0.697552, acc.: 58.59%] [G loss: 1.285742]\n",
      "epoch:24 step:23111 [D loss: 0.607353, acc.: 68.75%] [G loss: 1.314977]\n",
      "epoch:24 step:23112 [D loss: 0.721819, acc.: 60.16%] [G loss: 1.437641]\n",
      "epoch:24 step:23113 [D loss: 0.510764, acc.: 74.22%] [G loss: 1.752882]\n",
      "epoch:24 step:23114 [D loss: 0.575916, acc.: 71.09%] [G loss: 1.408612]\n",
      "epoch:24 step:23115 [D loss: 0.654900, acc.: 61.72%] [G loss: 1.368323]\n",
      "epoch:24 step:23116 [D loss: 0.656538, acc.: 63.28%] [G loss: 1.380906]\n",
      "epoch:24 step:23117 [D loss: 0.648129, acc.: 61.72%] [G loss: 1.247816]\n",
      "epoch:24 step:23118 [D loss: 0.561431, acc.: 67.97%] [G loss: 0.953457]\n",
      "epoch:24 step:23119 [D loss: 0.401287, acc.: 84.38%] [G loss: 1.282444]\n",
      "epoch:24 step:23120 [D loss: 0.547956, acc.: 73.44%] [G loss: 1.392994]\n",
      "epoch:24 step:23121 [D loss: 0.479592, acc.: 80.47%] [G loss: 1.382716]\n",
      "epoch:24 step:23122 [D loss: 0.524311, acc.: 73.44%] [G loss: 1.077388]\n",
      "epoch:24 step:23123 [D loss: 0.701143, acc.: 57.03%] [G loss: 1.455103]\n",
      "epoch:24 step:23124 [D loss: 0.337386, acc.: 90.62%] [G loss: 1.591307]\n",
      "epoch:24 step:23125 [D loss: 0.608576, acc.: 63.28%] [G loss: 1.042444]\n",
      "epoch:24 step:23126 [D loss: 0.574202, acc.: 66.41%] [G loss: 1.262762]\n",
      "epoch:24 step:23127 [D loss: 0.528067, acc.: 77.34%] [G loss: 1.469482]\n",
      "epoch:24 step:23128 [D loss: 0.437090, acc.: 82.81%] [G loss: 1.202360]\n",
      "epoch:24 step:23129 [D loss: 0.496969, acc.: 81.25%] [G loss: 1.307474]\n",
      "epoch:24 step:23130 [D loss: 0.463284, acc.: 82.81%] [G loss: 1.436754]\n",
      "epoch:24 step:23131 [D loss: 0.809197, acc.: 53.91%] [G loss: 1.131637]\n",
      "epoch:24 step:23132 [D loss: 0.506955, acc.: 75.78%] [G loss: 1.334848]\n",
      "epoch:24 step:23133 [D loss: 0.510771, acc.: 79.69%] [G loss: 1.732324]\n",
      "epoch:24 step:23134 [D loss: 0.685733, acc.: 62.50%] [G loss: 1.023110]\n",
      "epoch:24 step:23135 [D loss: 0.624300, acc.: 64.06%] [G loss: 1.371979]\n",
      "epoch:24 step:23136 [D loss: 0.530257, acc.: 71.09%] [G loss: 1.200550]\n",
      "epoch:24 step:23137 [D loss: 0.463078, acc.: 80.47%] [G loss: 1.263018]\n",
      "epoch:24 step:23138 [D loss: 0.572061, acc.: 71.09%] [G loss: 1.522774]\n",
      "epoch:24 step:23139 [D loss: 0.371993, acc.: 85.94%] [G loss: 1.841215]\n",
      "epoch:24 step:23140 [D loss: 0.580521, acc.: 69.53%] [G loss: 1.582360]\n",
      "epoch:24 step:23141 [D loss: 0.642011, acc.: 68.75%] [G loss: 1.069938]\n",
      "epoch:24 step:23142 [D loss: 0.419952, acc.: 83.59%] [G loss: 1.183541]\n",
      "epoch:24 step:23143 [D loss: 0.690683, acc.: 60.16%] [G loss: 1.123157]\n",
      "epoch:24 step:23144 [D loss: 0.446290, acc.: 80.47%] [G loss: 1.305920]\n",
      "epoch:24 step:23145 [D loss: 0.495705, acc.: 77.34%] [G loss: 1.376793]\n",
      "epoch:24 step:23146 [D loss: 0.623893, acc.: 67.97%] [G loss: 1.348364]\n",
      "epoch:24 step:23147 [D loss: 0.421241, acc.: 82.81%] [G loss: 1.185602]\n",
      "epoch:24 step:23148 [D loss: 0.581406, acc.: 71.09%] [G loss: 1.285759]\n",
      "epoch:24 step:23149 [D loss: 0.446627, acc.: 83.59%] [G loss: 1.477705]\n",
      "epoch:24 step:23150 [D loss: 0.612508, acc.: 67.19%] [G loss: 1.074961]\n",
      "epoch:24 step:23151 [D loss: 0.606681, acc.: 68.75%] [G loss: 1.225245]\n",
      "epoch:24 step:23152 [D loss: 0.581238, acc.: 69.53%] [G loss: 1.508678]\n",
      "epoch:24 step:23153 [D loss: 0.514783, acc.: 73.44%] [G loss: 1.583087]\n",
      "epoch:24 step:23154 [D loss: 0.444755, acc.: 79.69%] [G loss: 1.301765]\n",
      "epoch:24 step:23155 [D loss: 0.664051, acc.: 66.41%] [G loss: 1.485060]\n",
      "epoch:24 step:23156 [D loss: 0.572579, acc.: 69.53%] [G loss: 1.470105]\n",
      "epoch:24 step:23157 [D loss: 0.393436, acc.: 86.72%] [G loss: 2.055167]\n",
      "epoch:24 step:23158 [D loss: 0.637580, acc.: 63.28%] [G loss: 1.412279]\n",
      "epoch:24 step:23159 [D loss: 0.476453, acc.: 77.34%] [G loss: 1.428341]\n",
      "epoch:24 step:23160 [D loss: 0.482064, acc.: 78.91%] [G loss: 1.238256]\n",
      "epoch:24 step:23161 [D loss: 0.520331, acc.: 75.00%] [G loss: 1.496218]\n",
      "epoch:24 step:23162 [D loss: 0.509692, acc.: 77.34%] [G loss: 1.205126]\n",
      "epoch:24 step:23163 [D loss: 0.523172, acc.: 75.78%] [G loss: 1.284489]\n",
      "epoch:24 step:23164 [D loss: 0.494898, acc.: 78.91%] [G loss: 1.208298]\n",
      "epoch:24 step:23165 [D loss: 0.586362, acc.: 68.75%] [G loss: 1.378700]\n",
      "epoch:24 step:23166 [D loss: 0.656211, acc.: 64.06%] [G loss: 1.094061]\n",
      "epoch:24 step:23167 [D loss: 0.494557, acc.: 76.56%] [G loss: 1.052798]\n",
      "epoch:24 step:23168 [D loss: 0.688845, acc.: 59.38%] [G loss: 1.180592]\n",
      "epoch:24 step:23169 [D loss: 0.489673, acc.: 78.91%] [G loss: 1.664597]\n",
      "epoch:24 step:23170 [D loss: 0.636432, acc.: 62.50%] [G loss: 1.388617]\n",
      "epoch:24 step:23171 [D loss: 0.736401, acc.: 62.50%] [G loss: 1.188426]\n",
      "epoch:24 step:23172 [D loss: 0.471196, acc.: 79.69%] [G loss: 1.505934]\n",
      "epoch:24 step:23173 [D loss: 0.672177, acc.: 58.59%] [G loss: 1.232371]\n",
      "epoch:24 step:23174 [D loss: 0.518358, acc.: 74.22%] [G loss: 1.222462]\n",
      "epoch:24 step:23175 [D loss: 0.435191, acc.: 77.34%] [G loss: 1.552632]\n",
      "epoch:24 step:23176 [D loss: 0.526341, acc.: 73.44%] [G loss: 1.544274]\n",
      "epoch:24 step:23177 [D loss: 0.566751, acc.: 68.75%] [G loss: 1.464944]\n",
      "epoch:24 step:23178 [D loss: 0.563915, acc.: 69.53%] [G loss: 1.115370]\n",
      "epoch:24 step:23179 [D loss: 0.649450, acc.: 55.47%] [G loss: 1.195049]\n",
      "epoch:24 step:23180 [D loss: 0.643093, acc.: 63.28%] [G loss: 1.255477]\n",
      "epoch:24 step:23181 [D loss: 0.454677, acc.: 81.25%] [G loss: 1.494784]\n",
      "epoch:24 step:23182 [D loss: 0.490930, acc.: 76.56%] [G loss: 1.510420]\n",
      "epoch:24 step:23183 [D loss: 0.436184, acc.: 88.28%] [G loss: 1.155667]\n",
      "epoch:24 step:23184 [D loss: 0.796822, acc.: 53.12%] [G loss: 1.431234]\n",
      "epoch:24 step:23185 [D loss: 0.391915, acc.: 87.50%] [G loss: 1.408743]\n",
      "epoch:24 step:23186 [D loss: 0.443095, acc.: 81.25%] [G loss: 1.404689]\n",
      "epoch:24 step:23187 [D loss: 0.504688, acc.: 75.78%] [G loss: 1.532583]\n",
      "epoch:24 step:23188 [D loss: 0.498371, acc.: 74.22%] [G loss: 1.539305]\n",
      "epoch:24 step:23189 [D loss: 0.366192, acc.: 87.50%] [G loss: 1.735430]\n",
      "epoch:24 step:23190 [D loss: 0.441170, acc.: 79.69%] [G loss: 1.623129]\n",
      "epoch:24 step:23191 [D loss: 0.489211, acc.: 78.12%] [G loss: 1.561951]\n",
      "epoch:24 step:23192 [D loss: 0.650859, acc.: 63.28%] [G loss: 1.187244]\n",
      "epoch:24 step:23193 [D loss: 0.432398, acc.: 80.47%] [G loss: 1.250076]\n",
      "epoch:24 step:23194 [D loss: 0.629250, acc.: 60.16%] [G loss: 1.444179]\n",
      "epoch:24 step:23195 [D loss: 0.427412, acc.: 85.16%] [G loss: 2.054934]\n",
      "epoch:24 step:23196 [D loss: 0.543355, acc.: 77.34%] [G loss: 1.340798]\n",
      "epoch:24 step:23197 [D loss: 0.643390, acc.: 60.16%] [G loss: 1.432581]\n",
      "epoch:24 step:23198 [D loss: 0.538603, acc.: 70.31%] [G loss: 1.331751]\n",
      "epoch:24 step:23199 [D loss: 0.553992, acc.: 73.44%] [G loss: 1.097823]\n",
      "epoch:24 step:23200 [D loss: 0.483770, acc.: 82.03%] [G loss: 1.386515]\n",
      "##############\n",
      "[2.78748794 2.02559436 1.89364276 2.81730609 0.94923923 6.11014085\n",
      " 2.27109011 2.86527125 3.98917302 7.14868929]\n",
      "##########\n",
      "epoch:24 step:23201 [D loss: 0.778554, acc.: 51.56%] [G loss: 1.216170]\n",
      "epoch:24 step:23202 [D loss: 0.514937, acc.: 75.00%] [G loss: 1.652460]\n",
      "epoch:24 step:23203 [D loss: 0.591561, acc.: 72.66%] [G loss: 1.268194]\n",
      "epoch:24 step:23204 [D loss: 0.637323, acc.: 59.38%] [G loss: 1.622956]\n",
      "epoch:24 step:23205 [D loss: 0.549098, acc.: 71.88%] [G loss: 1.499486]\n",
      "epoch:24 step:23206 [D loss: 0.431779, acc.: 79.69%] [G loss: 1.377115]\n",
      "epoch:24 step:23207 [D loss: 0.712172, acc.: 52.34%] [G loss: 1.365998]\n",
      "epoch:24 step:23208 [D loss: 0.521093, acc.: 72.66%] [G loss: 1.305014]\n",
      "epoch:24 step:23209 [D loss: 0.513617, acc.: 78.91%] [G loss: 1.376371]\n",
      "epoch:24 step:23210 [D loss: 0.726331, acc.: 59.38%] [G loss: 1.595844]\n",
      "epoch:24 step:23211 [D loss: 0.583298, acc.: 69.53%] [G loss: 1.295449]\n",
      "epoch:24 step:23212 [D loss: 0.478105, acc.: 76.56%] [G loss: 1.467518]\n",
      "epoch:24 step:23213 [D loss: 0.456433, acc.: 82.81%] [G loss: 1.599393]\n",
      "epoch:24 step:23214 [D loss: 0.547249, acc.: 73.44%] [G loss: 1.444345]\n",
      "epoch:24 step:23215 [D loss: 0.569651, acc.: 64.06%] [G loss: 1.507716]\n",
      "epoch:24 step:23216 [D loss: 0.488177, acc.: 75.00%] [G loss: 1.315333]\n",
      "epoch:24 step:23217 [D loss: 0.494722, acc.: 75.78%] [G loss: 1.489689]\n",
      "epoch:24 step:23218 [D loss: 0.590584, acc.: 71.09%] [G loss: 1.415205]\n",
      "epoch:24 step:23219 [D loss: 0.660310, acc.: 60.16%] [G loss: 1.019000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23220 [D loss: 0.541299, acc.: 72.66%] [G loss: 1.582099]\n",
      "epoch:24 step:23221 [D loss: 0.446499, acc.: 82.03%] [G loss: 1.173061]\n",
      "epoch:24 step:23222 [D loss: 0.513224, acc.: 74.22%] [G loss: 1.235317]\n",
      "epoch:24 step:23223 [D loss: 0.634338, acc.: 64.84%] [G loss: 1.679479]\n",
      "epoch:24 step:23224 [D loss: 0.656075, acc.: 61.72%] [G loss: 1.454479]\n",
      "epoch:24 step:23225 [D loss: 0.422793, acc.: 84.38%] [G loss: 1.462720]\n",
      "epoch:24 step:23226 [D loss: 0.771382, acc.: 50.78%] [G loss: 0.934874]\n",
      "epoch:24 step:23227 [D loss: 0.479989, acc.: 75.78%] [G loss: 1.458615]\n",
      "epoch:24 step:23228 [D loss: 0.676371, acc.: 63.28%] [G loss: 1.174196]\n",
      "epoch:24 step:23229 [D loss: 0.536370, acc.: 71.09%] [G loss: 1.622702]\n",
      "epoch:24 step:23230 [D loss: 0.372286, acc.: 85.16%] [G loss: 1.705179]\n",
      "epoch:24 step:23231 [D loss: 0.680045, acc.: 60.94%] [G loss: 1.509058]\n",
      "epoch:24 step:23232 [D loss: 0.579393, acc.: 64.84%] [G loss: 1.379465]\n",
      "epoch:24 step:23233 [D loss: 0.416090, acc.: 82.81%] [G loss: 1.450251]\n",
      "epoch:24 step:23234 [D loss: 0.571158, acc.: 70.31%] [G loss: 1.429923]\n",
      "epoch:24 step:23235 [D loss: 0.412567, acc.: 82.03%] [G loss: 1.291338]\n",
      "epoch:24 step:23236 [D loss: 0.555356, acc.: 78.12%] [G loss: 1.540302]\n",
      "epoch:24 step:23237 [D loss: 0.724651, acc.: 62.50%] [G loss: 1.447874]\n",
      "epoch:24 step:23238 [D loss: 0.526114, acc.: 76.56%] [G loss: 1.478665]\n",
      "epoch:24 step:23239 [D loss: 0.464710, acc.: 78.91%] [G loss: 1.809237]\n",
      "epoch:24 step:23240 [D loss: 0.512241, acc.: 77.34%] [G loss: 1.572389]\n",
      "epoch:24 step:23241 [D loss: 0.482595, acc.: 74.22%] [G loss: 1.164642]\n",
      "epoch:24 step:23242 [D loss: 0.485470, acc.: 75.00%] [G loss: 1.786130]\n",
      "epoch:24 step:23243 [D loss: 0.466716, acc.: 78.91%] [G loss: 1.081277]\n",
      "epoch:24 step:23244 [D loss: 0.592393, acc.: 73.44%] [G loss: 1.125482]\n",
      "epoch:24 step:23245 [D loss: 0.493346, acc.: 78.12%] [G loss: 1.348314]\n",
      "epoch:24 step:23246 [D loss: 0.526701, acc.: 73.44%] [G loss: 1.405344]\n",
      "epoch:24 step:23247 [D loss: 0.401756, acc.: 82.81%] [G loss: 1.354094]\n",
      "epoch:24 step:23248 [D loss: 0.560727, acc.: 74.22%] [G loss: 1.135039]\n",
      "epoch:24 step:23249 [D loss: 0.652339, acc.: 60.94%] [G loss: 1.353996]\n",
      "epoch:24 step:23250 [D loss: 0.618017, acc.: 64.06%] [G loss: 1.606737]\n",
      "epoch:24 step:23251 [D loss: 0.522511, acc.: 75.00%] [G loss: 1.177991]\n",
      "epoch:24 step:23252 [D loss: 0.568056, acc.: 71.88%] [G loss: 1.507590]\n",
      "epoch:24 step:23253 [D loss: 0.628643, acc.: 66.41%] [G loss: 1.337381]\n",
      "epoch:24 step:23254 [D loss: 0.485074, acc.: 75.78%] [G loss: 1.689007]\n",
      "epoch:24 step:23255 [D loss: 0.471817, acc.: 78.91%] [G loss: 1.943397]\n",
      "epoch:24 step:23256 [D loss: 0.573675, acc.: 68.75%] [G loss: 1.491997]\n",
      "epoch:24 step:23257 [D loss: 0.469683, acc.: 79.69%] [G loss: 1.732421]\n",
      "epoch:24 step:23258 [D loss: 0.532813, acc.: 69.53%] [G loss: 1.537287]\n",
      "epoch:24 step:23259 [D loss: 0.546470, acc.: 72.66%] [G loss: 1.182783]\n",
      "epoch:24 step:23260 [D loss: 0.631892, acc.: 64.84%] [G loss: 1.390344]\n",
      "epoch:24 step:23261 [D loss: 0.561442, acc.: 70.31%] [G loss: 1.619340]\n",
      "epoch:24 step:23262 [D loss: 0.666028, acc.: 61.72%] [G loss: 1.266692]\n",
      "epoch:24 step:23263 [D loss: 0.614006, acc.: 62.50%] [G loss: 1.484275]\n",
      "epoch:24 step:23264 [D loss: 0.589018, acc.: 71.09%] [G loss: 1.093304]\n",
      "epoch:24 step:23265 [D loss: 0.633391, acc.: 64.06%] [G loss: 1.424364]\n",
      "epoch:24 step:23266 [D loss: 0.577667, acc.: 71.88%] [G loss: 1.092131]\n",
      "epoch:24 step:23267 [D loss: 0.599959, acc.: 71.09%] [G loss: 1.520113]\n",
      "epoch:24 step:23268 [D loss: 0.689948, acc.: 57.03%] [G loss: 1.470365]\n",
      "epoch:24 step:23269 [D loss: 0.600433, acc.: 65.62%] [G loss: 1.430102]\n",
      "epoch:24 step:23270 [D loss: 0.465250, acc.: 80.47%] [G loss: 1.673768]\n",
      "epoch:24 step:23271 [D loss: 0.473046, acc.: 77.34%] [G loss: 1.619210]\n",
      "epoch:24 step:23272 [D loss: 0.480982, acc.: 78.91%] [G loss: 1.328718]\n",
      "epoch:24 step:23273 [D loss: 0.547396, acc.: 75.00%] [G loss: 1.385926]\n",
      "epoch:24 step:23274 [D loss: 0.676921, acc.: 62.50%] [G loss: 1.339372]\n",
      "epoch:24 step:23275 [D loss: 0.457698, acc.: 82.03%] [G loss: 1.164294]\n",
      "epoch:24 step:23276 [D loss: 0.451530, acc.: 78.91%] [G loss: 1.569405]\n",
      "epoch:24 step:23277 [D loss: 0.588724, acc.: 70.31%] [G loss: 1.155220]\n",
      "epoch:24 step:23278 [D loss: 0.384223, acc.: 88.28%] [G loss: 1.255784]\n",
      "epoch:24 step:23279 [D loss: 0.416251, acc.: 85.16%] [G loss: 1.737486]\n",
      "epoch:24 step:23280 [D loss: 0.472663, acc.: 78.12%] [G loss: 1.227982]\n",
      "epoch:24 step:23281 [D loss: 0.453795, acc.: 81.25%] [G loss: 1.440766]\n",
      "epoch:24 step:23282 [D loss: 0.542256, acc.: 74.22%] [G loss: 1.599219]\n",
      "epoch:24 step:23283 [D loss: 0.599830, acc.: 71.09%] [G loss: 1.070241]\n",
      "epoch:24 step:23284 [D loss: 0.397284, acc.: 83.59%] [G loss: 1.399136]\n",
      "epoch:24 step:23285 [D loss: 0.588057, acc.: 70.31%] [G loss: 1.172758]\n",
      "epoch:24 step:23286 [D loss: 0.588806, acc.: 68.75%] [G loss: 1.751304]\n",
      "epoch:24 step:23287 [D loss: 0.749656, acc.: 53.12%] [G loss: 1.137353]\n",
      "epoch:24 step:23288 [D loss: 0.536699, acc.: 71.88%] [G loss: 1.467654]\n",
      "epoch:24 step:23289 [D loss: 0.657496, acc.: 67.19%] [G loss: 1.493240]\n",
      "epoch:24 step:23290 [D loss: 0.678690, acc.: 61.72%] [G loss: 1.413154]\n",
      "epoch:24 step:23291 [D loss: 0.495614, acc.: 78.91%] [G loss: 1.248993]\n",
      "epoch:24 step:23292 [D loss: 0.464500, acc.: 83.59%] [G loss: 1.766219]\n",
      "epoch:24 step:23293 [D loss: 0.626393, acc.: 67.19%] [G loss: 1.116287]\n",
      "epoch:24 step:23294 [D loss: 0.570448, acc.: 68.75%] [G loss: 1.470889]\n",
      "epoch:24 step:23295 [D loss: 0.489735, acc.: 77.34%] [G loss: 1.700112]\n",
      "epoch:24 step:23296 [D loss: 0.574300, acc.: 71.09%] [G loss: 1.135364]\n",
      "epoch:24 step:23297 [D loss: 0.494221, acc.: 76.56%] [G loss: 1.479342]\n",
      "epoch:24 step:23298 [D loss: 0.413188, acc.: 84.38%] [G loss: 1.643987]\n",
      "epoch:24 step:23299 [D loss: 0.524184, acc.: 78.12%] [G loss: 1.320060]\n",
      "epoch:24 step:23300 [D loss: 0.478335, acc.: 77.34%] [G loss: 1.445109]\n",
      "epoch:24 step:23301 [D loss: 0.666256, acc.: 60.16%] [G loss: 1.379166]\n",
      "epoch:24 step:23302 [D loss: 0.589914, acc.: 67.19%] [G loss: 1.736940]\n",
      "epoch:24 step:23303 [D loss: 0.568708, acc.: 68.75%] [G loss: 1.282626]\n",
      "epoch:24 step:23304 [D loss: 0.528356, acc.: 77.34%] [G loss: 1.717147]\n",
      "epoch:24 step:23305 [D loss: 0.527226, acc.: 71.88%] [G loss: 1.374524]\n",
      "epoch:24 step:23306 [D loss: 0.505947, acc.: 69.53%] [G loss: 1.248718]\n",
      "epoch:24 step:23307 [D loss: 0.557030, acc.: 73.44%] [G loss: 1.333481]\n",
      "epoch:24 step:23308 [D loss: 0.594538, acc.: 66.41%] [G loss: 1.250506]\n",
      "epoch:24 step:23309 [D loss: 0.564756, acc.: 70.31%] [G loss: 1.319910]\n",
      "epoch:24 step:23310 [D loss: 0.486043, acc.: 74.22%] [G loss: 0.997924]\n",
      "epoch:24 step:23311 [D loss: 0.468592, acc.: 82.81%] [G loss: 1.140221]\n",
      "epoch:24 step:23312 [D loss: 0.593419, acc.: 70.31%] [G loss: 1.257994]\n",
      "epoch:24 step:23313 [D loss: 0.460511, acc.: 75.78%] [G loss: 1.699800]\n",
      "epoch:24 step:23314 [D loss: 0.638870, acc.: 63.28%] [G loss: 1.400477]\n",
      "epoch:24 step:23315 [D loss: 0.560962, acc.: 69.53%] [G loss: 1.063582]\n",
      "epoch:24 step:23316 [D loss: 0.708252, acc.: 56.25%] [G loss: 1.450074]\n",
      "epoch:24 step:23317 [D loss: 0.442623, acc.: 82.03%] [G loss: 1.137950]\n",
      "epoch:24 step:23318 [D loss: 0.636724, acc.: 66.41%] [G loss: 1.228220]\n",
      "epoch:24 step:23319 [D loss: 0.590238, acc.: 67.19%] [G loss: 1.399550]\n",
      "epoch:24 step:23320 [D loss: 0.558983, acc.: 72.66%] [G loss: 1.610714]\n",
      "epoch:24 step:23321 [D loss: 0.715427, acc.: 57.03%] [G loss: 1.641881]\n",
      "epoch:24 step:23322 [D loss: 0.609973, acc.: 64.84%] [G loss: 1.294006]\n",
      "epoch:24 step:23323 [D loss: 0.562873, acc.: 67.97%] [G loss: 1.586309]\n",
      "epoch:24 step:23324 [D loss: 0.619343, acc.: 66.41%] [G loss: 1.113396]\n",
      "epoch:24 step:23325 [D loss: 0.635244, acc.: 67.19%] [G loss: 1.542850]\n",
      "epoch:24 step:23326 [D loss: 0.656531, acc.: 63.28%] [G loss: 1.359741]\n",
      "epoch:24 step:23327 [D loss: 0.443448, acc.: 78.12%] [G loss: 1.553942]\n",
      "epoch:24 step:23328 [D loss: 0.558755, acc.: 71.88%] [G loss: 1.367182]\n",
      "epoch:24 step:23329 [D loss: 0.385028, acc.: 84.38%] [G loss: 1.387445]\n",
      "epoch:24 step:23330 [D loss: 0.386573, acc.: 88.28%] [G loss: 1.448407]\n",
      "epoch:24 step:23331 [D loss: 0.827621, acc.: 53.91%] [G loss: 1.075847]\n",
      "epoch:24 step:23332 [D loss: 0.685969, acc.: 62.50%] [G loss: 1.200904]\n",
      "epoch:24 step:23333 [D loss: 0.533997, acc.: 74.22%] [G loss: 1.331245]\n",
      "epoch:24 step:23334 [D loss: 0.598828, acc.: 72.66%] [G loss: 1.503637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23335 [D loss: 0.622239, acc.: 66.41%] [G loss: 1.122400]\n",
      "epoch:24 step:23336 [D loss: 0.527901, acc.: 71.88%] [G loss: 1.176999]\n",
      "epoch:24 step:23337 [D loss: 0.500371, acc.: 78.91%] [G loss: 1.221386]\n",
      "epoch:24 step:23338 [D loss: 0.455768, acc.: 79.69%] [G loss: 1.455440]\n",
      "epoch:24 step:23339 [D loss: 0.737030, acc.: 60.16%] [G loss: 1.018598]\n",
      "epoch:24 step:23340 [D loss: 0.587575, acc.: 72.66%] [G loss: 1.166429]\n",
      "epoch:24 step:23341 [D loss: 0.593123, acc.: 69.53%] [G loss: 1.324389]\n",
      "epoch:24 step:23342 [D loss: 0.600125, acc.: 68.75%] [G loss: 1.350519]\n",
      "epoch:24 step:23343 [D loss: 0.583354, acc.: 72.66%] [G loss: 1.541763]\n",
      "epoch:24 step:23344 [D loss: 0.358358, acc.: 88.28%] [G loss: 1.596172]\n",
      "epoch:24 step:23345 [D loss: 0.656684, acc.: 63.28%] [G loss: 1.548516]\n",
      "epoch:24 step:23346 [D loss: 0.527382, acc.: 72.66%] [G loss: 1.312561]\n",
      "epoch:24 step:23347 [D loss: 0.547148, acc.: 75.00%] [G loss: 1.484188]\n",
      "epoch:24 step:23348 [D loss: 0.527429, acc.: 75.78%] [G loss: 1.611820]\n",
      "epoch:24 step:23349 [D loss: 0.505903, acc.: 76.56%] [G loss: 1.126687]\n",
      "epoch:24 step:23350 [D loss: 0.718701, acc.: 58.59%] [G loss: 0.977781]\n",
      "epoch:24 step:23351 [D loss: 0.568358, acc.: 70.31%] [G loss: 1.491398]\n",
      "epoch:24 step:23352 [D loss: 0.772751, acc.: 57.03%] [G loss: 1.386780]\n",
      "epoch:24 step:23353 [D loss: 0.483847, acc.: 78.91%] [G loss: 1.250290]\n",
      "epoch:24 step:23354 [D loss: 0.721676, acc.: 64.06%] [G loss: 1.515953]\n",
      "epoch:24 step:23355 [D loss: 0.636364, acc.: 64.84%] [G loss: 1.278015]\n",
      "epoch:24 step:23356 [D loss: 0.466463, acc.: 75.78%] [G loss: 1.439801]\n",
      "epoch:24 step:23357 [D loss: 0.568552, acc.: 74.22%] [G loss: 1.576352]\n",
      "epoch:24 step:23358 [D loss: 0.526517, acc.: 74.22%] [G loss: 1.484182]\n",
      "epoch:24 step:23359 [D loss: 0.595641, acc.: 68.75%] [G loss: 0.929613]\n",
      "epoch:24 step:23360 [D loss: 0.426688, acc.: 82.81%] [G loss: 1.590527]\n",
      "epoch:24 step:23361 [D loss: 0.420049, acc.: 83.59%] [G loss: 1.679552]\n",
      "epoch:24 step:23362 [D loss: 0.528177, acc.: 75.00%] [G loss: 1.844567]\n",
      "epoch:24 step:23363 [D loss: 0.509118, acc.: 75.78%] [G loss: 1.493338]\n",
      "epoch:24 step:23364 [D loss: 0.626346, acc.: 65.62%] [G loss: 1.277940]\n",
      "epoch:24 step:23365 [D loss: 0.620406, acc.: 65.62%] [G loss: 1.350395]\n",
      "epoch:24 step:23366 [D loss: 0.581687, acc.: 72.66%] [G loss: 1.269255]\n",
      "epoch:24 step:23367 [D loss: 0.631319, acc.: 64.84%] [G loss: 1.416451]\n",
      "epoch:24 step:23368 [D loss: 0.461394, acc.: 78.91%] [G loss: 1.530346]\n",
      "epoch:24 step:23369 [D loss: 0.657055, acc.: 60.16%] [G loss: 1.214391]\n",
      "epoch:24 step:23370 [D loss: 0.575190, acc.: 65.62%] [G loss: 1.314077]\n",
      "epoch:24 step:23371 [D loss: 0.500569, acc.: 74.22%] [G loss: 1.444090]\n",
      "epoch:24 step:23372 [D loss: 0.594978, acc.: 64.84%] [G loss: 1.343264]\n",
      "epoch:24 step:23373 [D loss: 0.661218, acc.: 58.59%] [G loss: 1.295844]\n",
      "epoch:24 step:23374 [D loss: 0.544388, acc.: 71.88%] [G loss: 1.283238]\n",
      "epoch:24 step:23375 [D loss: 0.709598, acc.: 54.69%] [G loss: 1.569285]\n",
      "epoch:24 step:23376 [D loss: 0.731737, acc.: 59.38%] [G loss: 1.421873]\n",
      "epoch:24 step:23377 [D loss: 0.655546, acc.: 62.50%] [G loss: 1.354727]\n",
      "epoch:24 step:23378 [D loss: 0.627878, acc.: 64.84%] [G loss: 1.378635]\n",
      "epoch:24 step:23379 [D loss: 0.635384, acc.: 67.97%] [G loss: 1.512640]\n",
      "epoch:24 step:23380 [D loss: 0.607482, acc.: 70.31%] [G loss: 1.357938]\n",
      "epoch:24 step:23381 [D loss: 0.570865, acc.: 71.09%] [G loss: 1.138175]\n",
      "epoch:24 step:23382 [D loss: 0.565795, acc.: 69.53%] [G loss: 1.311466]\n",
      "epoch:24 step:23383 [D loss: 0.563389, acc.: 72.66%] [G loss: 0.929980]\n",
      "epoch:24 step:23384 [D loss: 0.493671, acc.: 73.44%] [G loss: 1.475276]\n",
      "epoch:24 step:23385 [D loss: 0.601163, acc.: 69.53%] [G loss: 1.665438]\n",
      "epoch:24 step:23386 [D loss: 0.585023, acc.: 71.88%] [G loss: 1.489755]\n",
      "epoch:24 step:23387 [D loss: 0.497379, acc.: 78.12%] [G loss: 1.035537]\n",
      "epoch:24 step:23388 [D loss: 0.690801, acc.: 58.59%] [G loss: 1.124141]\n",
      "epoch:24 step:23389 [D loss: 0.624389, acc.: 63.28%] [G loss: 1.506968]\n",
      "epoch:24 step:23390 [D loss: 0.498145, acc.: 77.34%] [G loss: 1.611446]\n",
      "epoch:24 step:23391 [D loss: 0.547778, acc.: 72.66%] [G loss: 1.396170]\n",
      "epoch:24 step:23392 [D loss: 0.449177, acc.: 83.59%] [G loss: 1.855775]\n",
      "epoch:24 step:23393 [D loss: 0.563737, acc.: 70.31%] [G loss: 1.141974]\n",
      "epoch:24 step:23394 [D loss: 0.612216, acc.: 64.06%] [G loss: 1.384398]\n",
      "epoch:24 step:23395 [D loss: 0.668926, acc.: 64.84%] [G loss: 1.251853]\n",
      "epoch:24 step:23396 [D loss: 0.778712, acc.: 51.56%] [G loss: 1.495535]\n",
      "epoch:24 step:23397 [D loss: 0.668612, acc.: 63.28%] [G loss: 1.309338]\n",
      "epoch:24 step:23398 [D loss: 0.472633, acc.: 79.69%] [G loss: 1.316295]\n",
      "epoch:24 step:23399 [D loss: 0.569216, acc.: 71.09%] [G loss: 1.425145]\n",
      "epoch:24 step:23400 [D loss: 0.665416, acc.: 64.84%] [G loss: 1.115883]\n",
      "##############\n",
      "[2.84650113 2.04514313 1.91236905 2.9021989  0.79400674 6.64402961\n",
      " 2.17126618 2.64830158 3.94066621 7.14868929]\n",
      "##########\n",
      "epoch:24 step:23401 [D loss: 0.797752, acc.: 50.00%] [G loss: 1.143442]\n",
      "epoch:24 step:23402 [D loss: 0.417987, acc.: 82.81%] [G loss: 1.133788]\n",
      "epoch:24 step:23403 [D loss: 0.636196, acc.: 63.28%] [G loss: 1.439701]\n",
      "epoch:24 step:23404 [D loss: 0.407173, acc.: 86.72%] [G loss: 1.378759]\n",
      "epoch:24 step:23405 [D loss: 0.407331, acc.: 81.25%] [G loss: 1.051704]\n",
      "epoch:24 step:23406 [D loss: 0.451076, acc.: 84.38%] [G loss: 1.157938]\n",
      "epoch:24 step:23407 [D loss: 0.554743, acc.: 75.00%] [G loss: 1.493221]\n",
      "epoch:24 step:23408 [D loss: 0.524514, acc.: 78.12%] [G loss: 1.429256]\n",
      "epoch:24 step:23409 [D loss: 0.560012, acc.: 74.22%] [G loss: 1.875125]\n",
      "epoch:24 step:23410 [D loss: 0.514354, acc.: 74.22%] [G loss: 1.108138]\n",
      "epoch:24 step:23411 [D loss: 0.603866, acc.: 68.75%] [G loss: 1.460763]\n",
      "epoch:24 step:23412 [D loss: 0.645942, acc.: 65.62%] [G loss: 1.449476]\n",
      "epoch:24 step:23413 [D loss: 0.683773, acc.: 59.38%] [G loss: 1.466072]\n",
      "epoch:24 step:23414 [D loss: 0.604489, acc.: 68.75%] [G loss: 1.416445]\n",
      "epoch:24 step:23415 [D loss: 0.597248, acc.: 64.84%] [G loss: 1.455058]\n",
      "epoch:24 step:23416 [D loss: 0.733152, acc.: 57.81%] [G loss: 1.082192]\n",
      "epoch:24 step:23417 [D loss: 0.441535, acc.: 81.25%] [G loss: 1.555165]\n",
      "epoch:24 step:23418 [D loss: 0.488141, acc.: 78.91%] [G loss: 1.143451]\n",
      "epoch:24 step:23419 [D loss: 0.518213, acc.: 73.44%] [G loss: 1.528659]\n",
      "epoch:24 step:23420 [D loss: 0.502660, acc.: 74.22%] [G loss: 1.130368]\n",
      "epoch:24 step:23421 [D loss: 0.716572, acc.: 60.16%] [G loss: 0.986506]\n",
      "epoch:24 step:23422 [D loss: 0.431466, acc.: 79.69%] [G loss: 1.467867]\n",
      "epoch:24 step:23423 [D loss: 0.426951, acc.: 84.38%] [G loss: 1.374618]\n",
      "epoch:24 step:23424 [D loss: 0.662121, acc.: 63.28%] [G loss: 1.528343]\n",
      "epoch:24 step:23425 [D loss: 0.558822, acc.: 73.44%] [G loss: 0.955792]\n",
      "epoch:25 step:23426 [D loss: 0.630227, acc.: 64.84%] [G loss: 1.099282]\n",
      "epoch:25 step:23427 [D loss: 0.567019, acc.: 71.88%] [G loss: 1.262140]\n",
      "epoch:25 step:23428 [D loss: 0.462276, acc.: 79.69%] [G loss: 1.415468]\n",
      "epoch:25 step:23429 [D loss: 0.553296, acc.: 71.09%] [G loss: 1.108857]\n",
      "epoch:25 step:23430 [D loss: 0.572477, acc.: 71.88%] [G loss: 1.385669]\n",
      "epoch:25 step:23431 [D loss: 0.725657, acc.: 57.81%] [G loss: 1.103470]\n",
      "epoch:25 step:23432 [D loss: 0.630321, acc.: 66.41%] [G loss: 1.171264]\n",
      "epoch:25 step:23433 [D loss: 0.453889, acc.: 79.69%] [G loss: 1.399044]\n",
      "epoch:25 step:23434 [D loss: 0.626195, acc.: 66.41%] [G loss: 1.416054]\n",
      "epoch:25 step:23435 [D loss: 0.499890, acc.: 72.66%] [G loss: 1.390167]\n",
      "epoch:25 step:23436 [D loss: 0.517606, acc.: 71.88%] [G loss: 1.694265]\n",
      "epoch:25 step:23437 [D loss: 0.600408, acc.: 67.97%] [G loss: 1.099130]\n",
      "epoch:25 step:23438 [D loss: 0.632729, acc.: 65.62%] [G loss: 1.619779]\n",
      "epoch:25 step:23439 [D loss: 0.520533, acc.: 75.78%] [G loss: 1.291099]\n",
      "epoch:25 step:23440 [D loss: 0.517284, acc.: 73.44%] [G loss: 1.358473]\n",
      "epoch:25 step:23441 [D loss: 0.710537, acc.: 58.59%] [G loss: 1.369559]\n",
      "epoch:25 step:23442 [D loss: 0.642084, acc.: 67.97%] [G loss: 1.734249]\n",
      "epoch:25 step:23443 [D loss: 0.485255, acc.: 78.91%] [G loss: 2.011453]\n",
      "epoch:25 step:23444 [D loss: 0.712440, acc.: 55.47%] [G loss: 1.061699]\n",
      "epoch:25 step:23445 [D loss: 0.545872, acc.: 71.09%] [G loss: 1.204654]\n",
      "epoch:25 step:23446 [D loss: 0.594601, acc.: 67.97%] [G loss: 1.314909]\n",
      "epoch:25 step:23447 [D loss: 0.518482, acc.: 78.12%] [G loss: 1.566257]\n",
      "epoch:25 step:23448 [D loss: 0.592001, acc.: 69.53%] [G loss: 1.098395]\n",
      "epoch:25 step:23449 [D loss: 0.448681, acc.: 81.25%] [G loss: 1.580376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23450 [D loss: 0.598848, acc.: 70.31%] [G loss: 1.213296]\n",
      "epoch:25 step:23451 [D loss: 0.460612, acc.: 79.69%] [G loss: 1.056744]\n",
      "epoch:25 step:23452 [D loss: 0.444569, acc.: 84.38%] [G loss: 1.156057]\n",
      "epoch:25 step:23453 [D loss: 0.641759, acc.: 67.97%] [G loss: 1.386172]\n",
      "epoch:25 step:23454 [D loss: 0.646325, acc.: 60.94%] [G loss: 1.564309]\n",
      "epoch:25 step:23455 [D loss: 0.576700, acc.: 74.22%] [G loss: 1.067316]\n",
      "epoch:25 step:23456 [D loss: 0.734697, acc.: 57.81%] [G loss: 1.181700]\n",
      "epoch:25 step:23457 [D loss: 0.581900, acc.: 67.19%] [G loss: 1.417000]\n",
      "epoch:25 step:23458 [D loss: 0.430961, acc.: 80.47%] [G loss: 1.355915]\n",
      "epoch:25 step:23459 [D loss: 0.507670, acc.: 76.56%] [G loss: 1.532267]\n",
      "epoch:25 step:23460 [D loss: 0.614273, acc.: 67.19%] [G loss: 1.512066]\n",
      "epoch:25 step:23461 [D loss: 0.480365, acc.: 75.78%] [G loss: 1.416487]\n",
      "epoch:25 step:23462 [D loss: 0.491891, acc.: 78.12%] [G loss: 1.594390]\n",
      "epoch:25 step:23463 [D loss: 0.625331, acc.: 64.84%] [G loss: 1.268980]\n",
      "epoch:25 step:23464 [D loss: 0.556592, acc.: 74.22%] [G loss: 1.361283]\n",
      "epoch:25 step:23465 [D loss: 0.622349, acc.: 65.62%] [G loss: 1.216526]\n",
      "epoch:25 step:23466 [D loss: 0.494136, acc.: 71.09%] [G loss: 1.426645]\n",
      "epoch:25 step:23467 [D loss: 0.553506, acc.: 69.53%] [G loss: 1.008676]\n",
      "epoch:25 step:23468 [D loss: 0.484686, acc.: 78.12%] [G loss: 1.119972]\n",
      "epoch:25 step:23469 [D loss: 0.572880, acc.: 72.66%] [G loss: 1.024908]\n",
      "epoch:25 step:23470 [D loss: 0.441474, acc.: 82.81%] [G loss: 1.583097]\n",
      "epoch:25 step:23471 [D loss: 0.448463, acc.: 76.56%] [G loss: 1.828299]\n",
      "epoch:25 step:23472 [D loss: 0.572484, acc.: 71.09%] [G loss: 1.466176]\n",
      "epoch:25 step:23473 [D loss: 0.619318, acc.: 68.75%] [G loss: 1.583393]\n",
      "epoch:25 step:23474 [D loss: 0.544787, acc.: 68.75%] [G loss: 1.333353]\n",
      "epoch:25 step:23475 [D loss: 0.476701, acc.: 78.12%] [G loss: 1.550642]\n",
      "epoch:25 step:23476 [D loss: 0.462347, acc.: 77.34%] [G loss: 1.821920]\n",
      "epoch:25 step:23477 [D loss: 0.459504, acc.: 78.91%] [G loss: 1.776809]\n",
      "epoch:25 step:23478 [D loss: 0.403409, acc.: 85.94%] [G loss: 1.605994]\n",
      "epoch:25 step:23479 [D loss: 0.460086, acc.: 78.12%] [G loss: 1.313605]\n",
      "epoch:25 step:23480 [D loss: 0.568909, acc.: 73.44%] [G loss: 0.967329]\n",
      "epoch:25 step:23481 [D loss: 0.522421, acc.: 78.91%] [G loss: 0.887761]\n",
      "epoch:25 step:23482 [D loss: 0.505570, acc.: 75.78%] [G loss: 1.439912]\n",
      "epoch:25 step:23483 [D loss: 0.560717, acc.: 71.09%] [G loss: 1.355494]\n",
      "epoch:25 step:23484 [D loss: 0.333193, acc.: 89.06%] [G loss: 1.463087]\n",
      "epoch:25 step:23485 [D loss: 0.621635, acc.: 64.84%] [G loss: 1.271037]\n",
      "epoch:25 step:23486 [D loss: 0.486241, acc.: 78.12%] [G loss: 1.592612]\n",
      "epoch:25 step:23487 [D loss: 0.542163, acc.: 71.09%] [G loss: 1.485423]\n",
      "epoch:25 step:23488 [D loss: 0.481063, acc.: 77.34%] [G loss: 1.231739]\n",
      "epoch:25 step:23489 [D loss: 0.533059, acc.: 75.78%] [G loss: 1.464959]\n",
      "epoch:25 step:23490 [D loss: 0.513451, acc.: 72.66%] [G loss: 1.243419]\n",
      "epoch:25 step:23491 [D loss: 0.667968, acc.: 59.38%] [G loss: 1.312782]\n",
      "epoch:25 step:23492 [D loss: 0.396505, acc.: 85.94%] [G loss: 1.507225]\n",
      "epoch:25 step:23493 [D loss: 0.641764, acc.: 62.50%] [G loss: 1.353578]\n",
      "epoch:25 step:23494 [D loss: 0.417792, acc.: 85.16%] [G loss: 1.428857]\n",
      "epoch:25 step:23495 [D loss: 0.683009, acc.: 63.28%] [G loss: 1.332104]\n",
      "epoch:25 step:23496 [D loss: 0.685802, acc.: 58.59%] [G loss: 1.253176]\n",
      "epoch:25 step:23497 [D loss: 0.452205, acc.: 79.69%] [G loss: 1.274869]\n",
      "epoch:25 step:23498 [D loss: 0.516003, acc.: 74.22%] [G loss: 1.282209]\n",
      "epoch:25 step:23499 [D loss: 0.410399, acc.: 82.03%] [G loss: 1.509272]\n",
      "epoch:25 step:23500 [D loss: 0.601318, acc.: 71.88%] [G loss: 1.703204]\n",
      "epoch:25 step:23501 [D loss: 0.504483, acc.: 81.25%] [G loss: 1.332564]\n",
      "epoch:25 step:23502 [D loss: 0.602651, acc.: 68.75%] [G loss: 1.204435]\n",
      "epoch:25 step:23503 [D loss: 0.652967, acc.: 61.72%] [G loss: 1.574673]\n",
      "epoch:25 step:23504 [D loss: 0.776738, acc.: 55.47%] [G loss: 0.769789]\n",
      "epoch:25 step:23505 [D loss: 0.512426, acc.: 72.66%] [G loss: 1.451418]\n",
      "epoch:25 step:23506 [D loss: 0.712863, acc.: 57.03%] [G loss: 1.231564]\n",
      "epoch:25 step:23507 [D loss: 0.364606, acc.: 84.38%] [G loss: 1.324279]\n",
      "epoch:25 step:23508 [D loss: 0.604091, acc.: 64.84%] [G loss: 1.349576]\n",
      "epoch:25 step:23509 [D loss: 0.823733, acc.: 52.34%] [G loss: 1.406747]\n",
      "epoch:25 step:23510 [D loss: 0.506040, acc.: 80.47%] [G loss: 1.753544]\n",
      "epoch:25 step:23511 [D loss: 0.598742, acc.: 66.41%] [G loss: 1.769553]\n",
      "epoch:25 step:23512 [D loss: 0.641417, acc.: 64.06%] [G loss: 1.458940]\n",
      "epoch:25 step:23513 [D loss: 0.576116, acc.: 71.09%] [G loss: 1.445678]\n",
      "epoch:25 step:23514 [D loss: 0.608786, acc.: 64.06%] [G loss: 1.787219]\n",
      "epoch:25 step:23515 [D loss: 0.662798, acc.: 62.50%] [G loss: 1.453479]\n",
      "epoch:25 step:23516 [D loss: 0.648623, acc.: 63.28%] [G loss: 1.501460]\n",
      "epoch:25 step:23517 [D loss: 0.462301, acc.: 79.69%] [G loss: 1.644653]\n",
      "epoch:25 step:23518 [D loss: 0.530347, acc.: 73.44%] [G loss: 1.428905]\n",
      "epoch:25 step:23519 [D loss: 0.485625, acc.: 75.00%] [G loss: 1.518642]\n",
      "epoch:25 step:23520 [D loss: 0.572893, acc.: 72.66%] [G loss: 1.385072]\n",
      "epoch:25 step:23521 [D loss: 0.797997, acc.: 53.12%] [G loss: 1.185676]\n",
      "epoch:25 step:23522 [D loss: 0.711192, acc.: 60.16%] [G loss: 1.153964]\n",
      "epoch:25 step:23523 [D loss: 0.616464, acc.: 67.19%] [G loss: 1.119784]\n",
      "epoch:25 step:23524 [D loss: 0.469029, acc.: 82.03%] [G loss: 1.460759]\n",
      "epoch:25 step:23525 [D loss: 0.584136, acc.: 69.53%] [G loss: 1.334035]\n",
      "epoch:25 step:23526 [D loss: 0.523413, acc.: 75.00%] [G loss: 1.243080]\n",
      "epoch:25 step:23527 [D loss: 0.570740, acc.: 72.66%] [G loss: 1.389132]\n",
      "epoch:25 step:23528 [D loss: 0.489910, acc.: 76.56%] [G loss: 1.185636]\n",
      "epoch:25 step:23529 [D loss: 0.671675, acc.: 64.84%] [G loss: 1.438271]\n",
      "epoch:25 step:23530 [D loss: 0.407804, acc.: 85.94%] [G loss: 1.142047]\n",
      "epoch:25 step:23531 [D loss: 0.674051, acc.: 58.59%] [G loss: 1.579866]\n",
      "epoch:25 step:23532 [D loss: 0.589467, acc.: 70.31%] [G loss: 1.149201]\n",
      "epoch:25 step:23533 [D loss: 0.505617, acc.: 78.91%] [G loss: 1.496397]\n",
      "epoch:25 step:23534 [D loss: 0.617628, acc.: 63.28%] [G loss: 1.550014]\n",
      "epoch:25 step:23535 [D loss: 0.619012, acc.: 62.50%] [G loss: 1.163009]\n",
      "epoch:25 step:23536 [D loss: 0.521063, acc.: 72.66%] [G loss: 1.531652]\n",
      "epoch:25 step:23537 [D loss: 0.502123, acc.: 74.22%] [G loss: 1.473739]\n",
      "epoch:25 step:23538 [D loss: 0.521752, acc.: 72.66%] [G loss: 1.216095]\n",
      "epoch:25 step:23539 [D loss: 0.486372, acc.: 76.56%] [G loss: 1.241440]\n",
      "epoch:25 step:23540 [D loss: 0.391891, acc.: 86.72%] [G loss: 1.695782]\n",
      "epoch:25 step:23541 [D loss: 0.832572, acc.: 49.22%] [G loss: 1.123913]\n",
      "epoch:25 step:23542 [D loss: 0.538831, acc.: 72.66%] [G loss: 1.575020]\n",
      "epoch:25 step:23543 [D loss: 0.557524, acc.: 71.88%] [G loss: 1.607151]\n",
      "epoch:25 step:23544 [D loss: 0.504040, acc.: 75.00%] [G loss: 1.444524]\n",
      "epoch:25 step:23545 [D loss: 0.679663, acc.: 61.72%] [G loss: 1.150582]\n",
      "epoch:25 step:23546 [D loss: 0.584169, acc.: 75.78%] [G loss: 0.978742]\n",
      "epoch:25 step:23547 [D loss: 0.601413, acc.: 68.75%] [G loss: 1.131438]\n",
      "epoch:25 step:23548 [D loss: 0.544418, acc.: 69.53%] [G loss: 1.464572]\n",
      "epoch:25 step:23549 [D loss: 0.546448, acc.: 70.31%] [G loss: 1.387098]\n",
      "epoch:25 step:23550 [D loss: 0.643545, acc.: 64.06%] [G loss: 1.212492]\n",
      "epoch:25 step:23551 [D loss: 0.508129, acc.: 75.00%] [G loss: 1.301633]\n",
      "epoch:25 step:23552 [D loss: 0.769416, acc.: 55.47%] [G loss: 1.649214]\n",
      "epoch:25 step:23553 [D loss: 0.638755, acc.: 66.41%] [G loss: 1.585779]\n",
      "epoch:25 step:23554 [D loss: 0.590380, acc.: 64.84%] [G loss: 1.045541]\n",
      "epoch:25 step:23555 [D loss: 0.452005, acc.: 80.47%] [G loss: 1.708602]\n",
      "epoch:25 step:23556 [D loss: 0.566578, acc.: 67.97%] [G loss: 1.500431]\n",
      "epoch:25 step:23557 [D loss: 0.516152, acc.: 71.88%] [G loss: 1.275200]\n",
      "epoch:25 step:23558 [D loss: 0.623338, acc.: 65.62%] [G loss: 0.997869]\n",
      "epoch:25 step:23559 [D loss: 0.731491, acc.: 53.12%] [G loss: 1.120352]\n",
      "epoch:25 step:23560 [D loss: 0.615876, acc.: 62.50%] [G loss: 1.178096]\n",
      "epoch:25 step:23561 [D loss: 0.706399, acc.: 61.72%] [G loss: 1.813431]\n",
      "epoch:25 step:23562 [D loss: 0.627634, acc.: 65.62%] [G loss: 1.339578]\n",
      "epoch:25 step:23563 [D loss: 0.503966, acc.: 80.47%] [G loss: 1.019396]\n",
      "epoch:25 step:23564 [D loss: 0.624776, acc.: 67.19%] [G loss: 1.231305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23565 [D loss: 0.585200, acc.: 69.53%] [G loss: 1.426240]\n",
      "epoch:25 step:23566 [D loss: 0.570568, acc.: 69.53%] [G loss: 1.473755]\n",
      "epoch:25 step:23567 [D loss: 0.520109, acc.: 75.00%] [G loss: 1.938372]\n",
      "epoch:25 step:23568 [D loss: 0.623202, acc.: 64.06%] [G loss: 1.414362]\n",
      "epoch:25 step:23569 [D loss: 0.584561, acc.: 70.31%] [G loss: 1.419468]\n",
      "epoch:25 step:23570 [D loss: 0.506221, acc.: 75.78%] [G loss: 1.412395]\n",
      "epoch:25 step:23571 [D loss: 0.585883, acc.: 67.97%] [G loss: 1.240006]\n",
      "epoch:25 step:23572 [D loss: 0.684061, acc.: 59.38%] [G loss: 1.397738]\n",
      "epoch:25 step:23573 [D loss: 0.473609, acc.: 78.12%] [G loss: 1.193956]\n",
      "epoch:25 step:23574 [D loss: 0.670112, acc.: 60.94%] [G loss: 1.518103]\n",
      "epoch:25 step:23575 [D loss: 0.601712, acc.: 64.84%] [G loss: 1.478772]\n",
      "epoch:25 step:23576 [D loss: 0.495575, acc.: 74.22%] [G loss: 1.086278]\n",
      "epoch:25 step:23577 [D loss: 0.414162, acc.: 80.47%] [G loss: 1.683687]\n",
      "epoch:25 step:23578 [D loss: 0.466206, acc.: 81.25%] [G loss: 1.575010]\n",
      "epoch:25 step:23579 [D loss: 0.598901, acc.: 64.06%] [G loss: 1.225800]\n",
      "epoch:25 step:23580 [D loss: 0.489241, acc.: 75.78%] [G loss: 1.342458]\n",
      "epoch:25 step:23581 [D loss: 0.591991, acc.: 69.53%] [G loss: 1.445765]\n",
      "epoch:25 step:23582 [D loss: 0.637357, acc.: 65.62%] [G loss: 1.231930]\n",
      "epoch:25 step:23583 [D loss: 0.575270, acc.: 74.22%] [G loss: 1.733727]\n",
      "epoch:25 step:23584 [D loss: 0.584944, acc.: 70.31%] [G loss: 1.171348]\n",
      "epoch:25 step:23585 [D loss: 0.426452, acc.: 89.06%] [G loss: 1.311116]\n",
      "epoch:25 step:23586 [D loss: 0.545271, acc.: 73.44%] [G loss: 1.446189]\n",
      "epoch:25 step:23587 [D loss: 0.614058, acc.: 67.19%] [G loss: 1.251683]\n",
      "epoch:25 step:23588 [D loss: 0.594667, acc.: 64.06%] [G loss: 1.426268]\n",
      "epoch:25 step:23589 [D loss: 0.606743, acc.: 64.84%] [G loss: 1.065969]\n",
      "epoch:25 step:23590 [D loss: 0.434045, acc.: 78.91%] [G loss: 1.663105]\n",
      "epoch:25 step:23591 [D loss: 0.499575, acc.: 76.56%] [G loss: 1.608096]\n",
      "epoch:25 step:23592 [D loss: 0.491640, acc.: 77.34%] [G loss: 1.525909]\n",
      "epoch:25 step:23593 [D loss: 0.574574, acc.: 71.09%] [G loss: 1.456186]\n",
      "epoch:25 step:23594 [D loss: 0.614619, acc.: 66.41%] [G loss: 1.161842]\n",
      "epoch:25 step:23595 [D loss: 0.518081, acc.: 73.44%] [G loss: 1.452916]\n",
      "epoch:25 step:23596 [D loss: 0.544448, acc.: 72.66%] [G loss: 1.755825]\n",
      "epoch:25 step:23597 [D loss: 0.382121, acc.: 82.81%] [G loss: 1.530362]\n",
      "epoch:25 step:23598 [D loss: 0.533792, acc.: 71.88%] [G loss: 1.483416]\n",
      "epoch:25 step:23599 [D loss: 0.569169, acc.: 60.16%] [G loss: 1.531798]\n",
      "epoch:25 step:23600 [D loss: 0.398191, acc.: 82.81%] [G loss: 1.484259]\n",
      "##############\n",
      "[2.76217157 2.10184462 2.045803   3.16250113 1.03104766 5.72337216\n",
      " 2.36295251 2.595324   3.92247837 7.14868929]\n",
      "##########\n",
      "epoch:25 step:23601 [D loss: 0.489413, acc.: 74.22%] [G loss: 1.613787]\n",
      "epoch:25 step:23602 [D loss: 0.502435, acc.: 78.12%] [G loss: 1.700096]\n",
      "epoch:25 step:23603 [D loss: 0.620076, acc.: 67.97%] [G loss: 1.498490]\n",
      "epoch:25 step:23604 [D loss: 0.550974, acc.: 70.31%] [G loss: 1.127817]\n",
      "epoch:25 step:23605 [D loss: 0.516167, acc.: 72.66%] [G loss: 1.422337]\n",
      "epoch:25 step:23606 [D loss: 0.573583, acc.: 70.31%] [G loss: 1.450567]\n",
      "epoch:25 step:23607 [D loss: 0.625440, acc.: 64.84%] [G loss: 0.922108]\n",
      "epoch:25 step:23608 [D loss: 0.466488, acc.: 75.00%] [G loss: 1.587876]\n",
      "epoch:25 step:23609 [D loss: 0.685567, acc.: 60.94%] [G loss: 1.200017]\n",
      "epoch:25 step:23610 [D loss: 0.571249, acc.: 66.41%] [G loss: 1.785670]\n",
      "epoch:25 step:23611 [D loss: 0.509870, acc.: 75.78%] [G loss: 1.366225]\n",
      "epoch:25 step:23612 [D loss: 0.452027, acc.: 78.12%] [G loss: 1.417452]\n",
      "epoch:25 step:23613 [D loss: 0.523879, acc.: 71.09%] [G loss: 1.386611]\n",
      "epoch:25 step:23614 [D loss: 0.581645, acc.: 65.62%] [G loss: 1.281491]\n",
      "epoch:25 step:23615 [D loss: 0.800040, acc.: 50.78%] [G loss: 1.009332]\n",
      "epoch:25 step:23616 [D loss: 0.688165, acc.: 58.59%] [G loss: 1.280922]\n",
      "epoch:25 step:23617 [D loss: 0.629433, acc.: 69.53%] [G loss: 1.576710]\n",
      "epoch:25 step:23618 [D loss: 0.521108, acc.: 77.34%] [G loss: 1.432326]\n",
      "epoch:25 step:23619 [D loss: 0.641707, acc.: 62.50%] [G loss: 0.886563]\n",
      "epoch:25 step:23620 [D loss: 0.755011, acc.: 56.25%] [G loss: 1.093373]\n",
      "epoch:25 step:23621 [D loss: 0.429538, acc.: 79.69%] [G loss: 1.604455]\n",
      "epoch:25 step:23622 [D loss: 0.641794, acc.: 62.50%] [G loss: 1.335912]\n",
      "epoch:25 step:23623 [D loss: 0.691375, acc.: 61.72%] [G loss: 1.816512]\n",
      "epoch:25 step:23624 [D loss: 0.379907, acc.: 86.72%] [G loss: 1.453426]\n",
      "epoch:25 step:23625 [D loss: 0.539731, acc.: 69.53%] [G loss: 1.400270]\n",
      "epoch:25 step:23626 [D loss: 0.388660, acc.: 85.16%] [G loss: 1.541873]\n",
      "epoch:25 step:23627 [D loss: 0.548100, acc.: 72.66%] [G loss: 1.343803]\n",
      "epoch:25 step:23628 [D loss: 0.514780, acc.: 72.66%] [G loss: 1.328506]\n",
      "epoch:25 step:23629 [D loss: 0.431829, acc.: 79.69%] [G loss: 1.649783]\n",
      "epoch:25 step:23630 [D loss: 0.615558, acc.: 70.31%] [G loss: 1.531752]\n",
      "epoch:25 step:23631 [D loss: 0.592921, acc.: 68.75%] [G loss: 1.028836]\n",
      "epoch:25 step:23632 [D loss: 0.539812, acc.: 75.78%] [G loss: 1.350082]\n",
      "epoch:25 step:23633 [D loss: 0.595181, acc.: 68.75%] [G loss: 0.934888]\n",
      "epoch:25 step:23634 [D loss: 0.679223, acc.: 61.72%] [G loss: 1.222085]\n",
      "epoch:25 step:23635 [D loss: 0.490180, acc.: 78.12%] [G loss: 1.506220]\n",
      "epoch:25 step:23636 [D loss: 0.452199, acc.: 75.78%] [G loss: 1.450776]\n",
      "epoch:25 step:23637 [D loss: 0.588771, acc.: 67.19%] [G loss: 1.631466]\n",
      "epoch:25 step:23638 [D loss: 0.528670, acc.: 76.56%] [G loss: 1.727089]\n",
      "epoch:25 step:23639 [D loss: 0.763180, acc.: 56.25%] [G loss: 1.276414]\n",
      "epoch:25 step:23640 [D loss: 0.442397, acc.: 82.81%] [G loss: 1.639000]\n",
      "epoch:25 step:23641 [D loss: 0.495232, acc.: 78.12%] [G loss: 1.389233]\n",
      "epoch:25 step:23642 [D loss: 0.483151, acc.: 75.78%] [G loss: 1.694173]\n",
      "epoch:25 step:23643 [D loss: 0.540726, acc.: 71.09%] [G loss: 1.359588]\n",
      "epoch:25 step:23644 [D loss: 0.486412, acc.: 78.12%] [G loss: 1.578699]\n",
      "epoch:25 step:23645 [D loss: 0.530153, acc.: 74.22%] [G loss: 0.985163]\n",
      "epoch:25 step:23646 [D loss: 0.481991, acc.: 76.56%] [G loss: 1.489322]\n",
      "epoch:25 step:23647 [D loss: 0.585935, acc.: 66.41%] [G loss: 1.543033]\n",
      "epoch:25 step:23648 [D loss: 0.636100, acc.: 61.72%] [G loss: 0.989297]\n",
      "epoch:25 step:23649 [D loss: 0.670248, acc.: 64.06%] [G loss: 1.404240]\n",
      "epoch:25 step:23650 [D loss: 0.567833, acc.: 69.53%] [G loss: 1.407612]\n",
      "epoch:25 step:23651 [D loss: 0.561869, acc.: 71.09%] [G loss: 1.657728]\n",
      "epoch:25 step:23652 [D loss: 0.510214, acc.: 75.00%] [G loss: 1.375461]\n",
      "epoch:25 step:23653 [D loss: 0.472740, acc.: 75.78%] [G loss: 1.572819]\n",
      "epoch:25 step:23654 [D loss: 0.468731, acc.: 77.34%] [G loss: 1.423593]\n",
      "epoch:25 step:23655 [D loss: 0.441570, acc.: 79.69%] [G loss: 1.654165]\n",
      "epoch:25 step:23656 [D loss: 0.544082, acc.: 71.09%] [G loss: 1.378031]\n",
      "epoch:25 step:23657 [D loss: 0.453867, acc.: 79.69%] [G loss: 1.909656]\n",
      "epoch:25 step:23658 [D loss: 0.654044, acc.: 60.94%] [G loss: 1.350607]\n",
      "epoch:25 step:23659 [D loss: 0.787391, acc.: 52.34%] [G loss: 1.325914]\n",
      "epoch:25 step:23660 [D loss: 0.429610, acc.: 80.47%] [G loss: 1.503075]\n",
      "epoch:25 step:23661 [D loss: 0.472792, acc.: 79.69%] [G loss: 1.528871]\n",
      "epoch:25 step:23662 [D loss: 0.753694, acc.: 55.47%] [G loss: 1.124182]\n",
      "epoch:25 step:23663 [D loss: 0.715190, acc.: 60.94%] [G loss: 1.809630]\n",
      "epoch:25 step:23664 [D loss: 0.595649, acc.: 67.19%] [G loss: 1.626088]\n",
      "epoch:25 step:23665 [D loss: 0.424755, acc.: 81.25%] [G loss: 1.724626]\n",
      "epoch:25 step:23666 [D loss: 0.502360, acc.: 78.91%] [G loss: 1.225089]\n",
      "epoch:25 step:23667 [D loss: 0.562478, acc.: 71.09%] [G loss: 1.363841]\n",
      "epoch:25 step:23668 [D loss: 0.743082, acc.: 54.69%] [G loss: 1.069896]\n",
      "epoch:25 step:23669 [D loss: 0.444542, acc.: 82.03%] [G loss: 1.310233]\n",
      "epoch:25 step:23670 [D loss: 0.535992, acc.: 75.00%] [G loss: 1.265470]\n",
      "epoch:25 step:23671 [D loss: 0.562432, acc.: 72.66%] [G loss: 1.427728]\n",
      "epoch:25 step:23672 [D loss: 0.704066, acc.: 56.25%] [G loss: 0.967814]\n",
      "epoch:25 step:23673 [D loss: 0.563929, acc.: 73.44%] [G loss: 1.437800]\n",
      "epoch:25 step:23674 [D loss: 0.544273, acc.: 67.97%] [G loss: 1.594793]\n",
      "epoch:25 step:23675 [D loss: 0.402295, acc.: 86.72%] [G loss: 1.653710]\n",
      "epoch:25 step:23676 [D loss: 0.552767, acc.: 70.31%] [G loss: 1.729926]\n",
      "epoch:25 step:23677 [D loss: 0.415919, acc.: 81.25%] [G loss: 1.424003]\n",
      "epoch:25 step:23678 [D loss: 0.647326, acc.: 61.72%] [G loss: 1.483502]\n",
      "epoch:25 step:23679 [D loss: 0.530356, acc.: 74.22%] [G loss: 1.956129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23680 [D loss: 0.466377, acc.: 78.12%] [G loss: 1.367480]\n",
      "epoch:25 step:23681 [D loss: 0.664406, acc.: 60.94%] [G loss: 1.159908]\n",
      "epoch:25 step:23682 [D loss: 0.502114, acc.: 73.44%] [G loss: 1.983816]\n",
      "epoch:25 step:23683 [D loss: 0.671498, acc.: 53.91%] [G loss: 1.445730]\n",
      "epoch:25 step:23684 [D loss: 0.546789, acc.: 73.44%] [G loss: 1.098287]\n",
      "epoch:25 step:23685 [D loss: 0.673397, acc.: 60.16%] [G loss: 1.250813]\n",
      "epoch:25 step:23686 [D loss: 0.662431, acc.: 60.16%] [G loss: 1.186201]\n",
      "epoch:25 step:23687 [D loss: 0.646368, acc.: 64.84%] [G loss: 1.217064]\n",
      "epoch:25 step:23688 [D loss: 0.593837, acc.: 66.41%] [G loss: 1.589320]\n",
      "epoch:25 step:23689 [D loss: 0.531516, acc.: 70.31%] [G loss: 1.106834]\n",
      "epoch:25 step:23690 [D loss: 0.427399, acc.: 79.69%] [G loss: 1.307650]\n",
      "epoch:25 step:23691 [D loss: 0.536582, acc.: 71.88%] [G loss: 1.817022]\n",
      "epoch:25 step:23692 [D loss: 0.705383, acc.: 56.25%] [G loss: 1.154257]\n",
      "epoch:25 step:23693 [D loss: 0.558135, acc.: 72.66%] [G loss: 1.426176]\n",
      "epoch:25 step:23694 [D loss: 0.530993, acc.: 76.56%] [G loss: 1.370627]\n",
      "epoch:25 step:23695 [D loss: 0.675839, acc.: 64.84%] [G loss: 1.464790]\n",
      "epoch:25 step:23696 [D loss: 0.467853, acc.: 78.91%] [G loss: 1.740826]\n",
      "epoch:25 step:23697 [D loss: 0.602522, acc.: 71.09%] [G loss: 1.187786]\n",
      "epoch:25 step:23698 [D loss: 0.760675, acc.: 54.69%] [G loss: 1.053851]\n",
      "epoch:25 step:23699 [D loss: 0.498860, acc.: 73.44%] [G loss: 1.989139]\n",
      "epoch:25 step:23700 [D loss: 0.814966, acc.: 56.25%] [G loss: 1.221184]\n",
      "epoch:25 step:23701 [D loss: 0.581044, acc.: 67.19%] [G loss: 1.454687]\n",
      "epoch:25 step:23702 [D loss: 0.509464, acc.: 77.34%] [G loss: 1.546509]\n",
      "epoch:25 step:23703 [D loss: 0.382855, acc.: 86.72%] [G loss: 1.015364]\n",
      "epoch:25 step:23704 [D loss: 0.506297, acc.: 78.91%] [G loss: 1.401388]\n",
      "epoch:25 step:23705 [D loss: 0.777850, acc.: 50.00%] [G loss: 1.003326]\n",
      "epoch:25 step:23706 [D loss: 0.598893, acc.: 69.53%] [G loss: 1.407458]\n",
      "epoch:25 step:23707 [D loss: 0.767469, acc.: 50.78%] [G loss: 1.114637]\n",
      "epoch:25 step:23708 [D loss: 0.609765, acc.: 67.97%] [G loss: 1.748090]\n",
      "epoch:25 step:23709 [D loss: 0.466996, acc.: 82.03%] [G loss: 1.362339]\n",
      "epoch:25 step:23710 [D loss: 0.536229, acc.: 71.88%] [G loss: 1.095515]\n",
      "epoch:25 step:23711 [D loss: 0.592702, acc.: 69.53%] [G loss: 1.262290]\n",
      "epoch:25 step:23712 [D loss: 0.472986, acc.: 85.16%] [G loss: 1.492928]\n",
      "epoch:25 step:23713 [D loss: 0.619688, acc.: 66.41%] [G loss: 1.326781]\n",
      "epoch:25 step:23714 [D loss: 0.665739, acc.: 61.72%] [G loss: 1.155828]\n",
      "epoch:25 step:23715 [D loss: 0.385217, acc.: 82.81%] [G loss: 1.254014]\n",
      "epoch:25 step:23716 [D loss: 0.553584, acc.: 71.09%] [G loss: 1.357633]\n",
      "epoch:25 step:23717 [D loss: 0.537454, acc.: 74.22%] [G loss: 1.177786]\n",
      "epoch:25 step:23718 [D loss: 0.525765, acc.: 74.22%] [G loss: 1.222195]\n",
      "epoch:25 step:23719 [D loss: 0.661828, acc.: 63.28%] [G loss: 1.028192]\n",
      "epoch:25 step:23720 [D loss: 0.560784, acc.: 67.19%] [G loss: 1.528790]\n",
      "epoch:25 step:23721 [D loss: 0.560662, acc.: 67.97%] [G loss: 1.472411]\n",
      "epoch:25 step:23722 [D loss: 0.525654, acc.: 74.22%] [G loss: 1.462529]\n",
      "epoch:25 step:23723 [D loss: 0.634063, acc.: 66.41%] [G loss: 1.672891]\n",
      "epoch:25 step:23724 [D loss: 0.590151, acc.: 67.19%] [G loss: 1.252576]\n",
      "epoch:25 step:23725 [D loss: 0.683276, acc.: 59.38%] [G loss: 1.291259]\n",
      "epoch:25 step:23726 [D loss: 0.618457, acc.: 62.50%] [G loss: 1.442121]\n",
      "epoch:25 step:23727 [D loss: 0.426477, acc.: 80.47%] [G loss: 1.919627]\n",
      "epoch:25 step:23728 [D loss: 0.638342, acc.: 62.50%] [G loss: 1.381997]\n",
      "epoch:25 step:23729 [D loss: 0.630618, acc.: 67.19%] [G loss: 1.390098]\n",
      "epoch:25 step:23730 [D loss: 0.556998, acc.: 77.34%] [G loss: 1.385433]\n",
      "epoch:25 step:23731 [D loss: 0.524556, acc.: 71.09%] [G loss: 1.138210]\n",
      "epoch:25 step:23732 [D loss: 0.594052, acc.: 67.97%] [G loss: 1.290917]\n",
      "epoch:25 step:23733 [D loss: 0.497215, acc.: 74.22%] [G loss: 1.308397]\n",
      "epoch:25 step:23734 [D loss: 0.531744, acc.: 71.88%] [G loss: 1.096241]\n",
      "epoch:25 step:23735 [D loss: 0.557280, acc.: 71.88%] [G loss: 1.289465]\n",
      "epoch:25 step:23736 [D loss: 0.585043, acc.: 72.66%] [G loss: 1.286299]\n",
      "epoch:25 step:23737 [D loss: 0.495606, acc.: 77.34%] [G loss: 1.283115]\n",
      "epoch:25 step:23738 [D loss: 0.640190, acc.: 66.41%] [G loss: 1.296317]\n",
      "epoch:25 step:23739 [D loss: 0.513837, acc.: 76.56%] [G loss: 1.121305]\n",
      "epoch:25 step:23740 [D loss: 0.606425, acc.: 74.22%] [G loss: 1.526046]\n",
      "epoch:25 step:23741 [D loss: 0.620582, acc.: 61.72%] [G loss: 1.333563]\n",
      "epoch:25 step:23742 [D loss: 0.489083, acc.: 76.56%] [G loss: 1.318990]\n",
      "epoch:25 step:23743 [D loss: 0.595587, acc.: 65.62%] [G loss: 1.259450]\n",
      "epoch:25 step:23744 [D loss: 0.631347, acc.: 67.19%] [G loss: 1.142570]\n",
      "epoch:25 step:23745 [D loss: 0.397749, acc.: 86.72%] [G loss: 1.330641]\n",
      "epoch:25 step:23746 [D loss: 0.600240, acc.: 68.75%] [G loss: 1.399822]\n",
      "epoch:25 step:23747 [D loss: 0.594775, acc.: 69.53%] [G loss: 1.254268]\n",
      "epoch:25 step:23748 [D loss: 0.527425, acc.: 71.88%] [G loss: 1.395251]\n",
      "epoch:25 step:23749 [D loss: 0.527449, acc.: 73.44%] [G loss: 1.773149]\n",
      "epoch:25 step:23750 [D loss: 0.586745, acc.: 70.31%] [G loss: 1.414180]\n",
      "epoch:25 step:23751 [D loss: 0.474087, acc.: 78.91%] [G loss: 1.574010]\n",
      "epoch:25 step:23752 [D loss: 0.436328, acc.: 80.47%] [G loss: 1.830193]\n",
      "epoch:25 step:23753 [D loss: 0.536616, acc.: 71.88%] [G loss: 1.559942]\n",
      "epoch:25 step:23754 [D loss: 0.631959, acc.: 66.41%] [G loss: 1.362322]\n",
      "epoch:25 step:23755 [D loss: 0.474820, acc.: 76.56%] [G loss: 1.833036]\n",
      "epoch:25 step:23756 [D loss: 0.559250, acc.: 66.41%] [G loss: 1.725825]\n",
      "epoch:25 step:23757 [D loss: 0.679596, acc.: 61.72%] [G loss: 1.214426]\n",
      "epoch:25 step:23758 [D loss: 0.538045, acc.: 70.31%] [G loss: 1.068469]\n",
      "epoch:25 step:23759 [D loss: 0.517853, acc.: 74.22%] [G loss: 1.592482]\n",
      "epoch:25 step:23760 [D loss: 0.629752, acc.: 67.19%] [G loss: 1.364058]\n",
      "epoch:25 step:23761 [D loss: 0.503113, acc.: 75.78%] [G loss: 1.712223]\n",
      "epoch:25 step:23762 [D loss: 0.739554, acc.: 57.81%] [G loss: 1.473176]\n",
      "epoch:25 step:23763 [D loss: 0.348202, acc.: 90.62%] [G loss: 1.911927]\n",
      "epoch:25 step:23764 [D loss: 0.539687, acc.: 72.66%] [G loss: 1.283707]\n",
      "epoch:25 step:23765 [D loss: 0.349075, acc.: 89.84%] [G loss: 1.489201]\n",
      "epoch:25 step:23766 [D loss: 0.445965, acc.: 79.69%] [G loss: 1.118460]\n",
      "epoch:25 step:23767 [D loss: 0.439659, acc.: 78.91%] [G loss: 1.454266]\n",
      "epoch:25 step:23768 [D loss: 0.670963, acc.: 61.72%] [G loss: 0.981034]\n",
      "epoch:25 step:23769 [D loss: 0.580041, acc.: 71.09%] [G loss: 1.175952]\n",
      "epoch:25 step:23770 [D loss: 0.491879, acc.: 78.91%] [G loss: 1.538495]\n",
      "epoch:25 step:23771 [D loss: 0.669014, acc.: 63.28%] [G loss: 1.388269]\n",
      "epoch:25 step:23772 [D loss: 0.559814, acc.: 75.00%] [G loss: 1.361141]\n",
      "epoch:25 step:23773 [D loss: 0.739883, acc.: 53.12%] [G loss: 1.149006]\n",
      "epoch:25 step:23774 [D loss: 0.517727, acc.: 74.22%] [G loss: 1.267735]\n",
      "epoch:25 step:23775 [D loss: 0.544190, acc.: 71.88%] [G loss: 1.446616]\n",
      "epoch:25 step:23776 [D loss: 0.586603, acc.: 67.19%] [G loss: 1.451649]\n",
      "epoch:25 step:23777 [D loss: 0.585093, acc.: 72.66%] [G loss: 1.211172]\n",
      "epoch:25 step:23778 [D loss: 0.565544, acc.: 74.22%] [G loss: 1.485532]\n",
      "epoch:25 step:23779 [D loss: 0.600446, acc.: 69.53%] [G loss: 1.603963]\n",
      "epoch:25 step:23780 [D loss: 0.532742, acc.: 75.00%] [G loss: 1.592587]\n",
      "epoch:25 step:23781 [D loss: 0.544079, acc.: 71.88%] [G loss: 1.511149]\n",
      "epoch:25 step:23782 [D loss: 0.605896, acc.: 67.97%] [G loss: 1.396737]\n",
      "epoch:25 step:23783 [D loss: 0.785734, acc.: 52.34%] [G loss: 1.280268]\n",
      "epoch:25 step:23784 [D loss: 0.593759, acc.: 66.41%] [G loss: 1.524191]\n",
      "epoch:25 step:23785 [D loss: 0.489640, acc.: 78.12%] [G loss: 1.729074]\n",
      "epoch:25 step:23786 [D loss: 0.396502, acc.: 86.72%] [G loss: 1.475582]\n",
      "epoch:25 step:23787 [D loss: 0.548768, acc.: 71.09%] [G loss: 1.453779]\n",
      "epoch:25 step:23788 [D loss: 0.476500, acc.: 80.47%] [G loss: 1.327425]\n",
      "epoch:25 step:23789 [D loss: 0.532995, acc.: 70.31%] [G loss: 1.197520]\n",
      "epoch:25 step:23790 [D loss: 0.524133, acc.: 75.78%] [G loss: 1.431787]\n",
      "epoch:25 step:23791 [D loss: 0.539167, acc.: 71.09%] [G loss: 1.248315]\n",
      "epoch:25 step:23792 [D loss: 0.407796, acc.: 82.81%] [G loss: 1.293245]\n",
      "epoch:25 step:23793 [D loss: 0.522414, acc.: 77.34%] [G loss: 1.590229]\n",
      "epoch:25 step:23794 [D loss: 0.542852, acc.: 71.09%] [G loss: 1.399569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23795 [D loss: 0.516772, acc.: 76.56%] [G loss: 1.569100]\n",
      "epoch:25 step:23796 [D loss: 0.416421, acc.: 85.94%] [G loss: 1.618597]\n",
      "epoch:25 step:23797 [D loss: 0.541058, acc.: 75.78%] [G loss: 1.427489]\n",
      "epoch:25 step:23798 [D loss: 0.694460, acc.: 60.94%] [G loss: 1.382038]\n",
      "epoch:25 step:23799 [D loss: 0.565550, acc.: 72.66%] [G loss: 1.391657]\n",
      "epoch:25 step:23800 [D loss: 0.649347, acc.: 64.06%] [G loss: 1.298353]\n",
      "##############\n",
      "[2.72593008 1.99470778 2.12762201 2.73936518 0.52497033 6.01092647\n",
      " 2.11435792 2.5410054  3.79734814 5.05171926]\n",
      "##########\n",
      "epoch:25 step:23801 [D loss: 0.627973, acc.: 66.41%] [G loss: 1.160843]\n",
      "epoch:25 step:23802 [D loss: 0.423241, acc.: 82.03%] [G loss: 1.133533]\n",
      "epoch:25 step:23803 [D loss: 0.463140, acc.: 81.25%] [G loss: 1.521339]\n",
      "epoch:25 step:23804 [D loss: 0.545549, acc.: 67.97%] [G loss: 1.406794]\n",
      "epoch:25 step:23805 [D loss: 0.398201, acc.: 86.72%] [G loss: 1.568937]\n",
      "epoch:25 step:23806 [D loss: 0.442929, acc.: 80.47%] [G loss: 1.353493]\n",
      "epoch:25 step:23807 [D loss: 0.477002, acc.: 78.91%] [G loss: 1.500463]\n",
      "epoch:25 step:23808 [D loss: 0.474483, acc.: 82.03%] [G loss: 1.294545]\n",
      "epoch:25 step:23809 [D loss: 0.597016, acc.: 64.84%] [G loss: 0.950749]\n",
      "epoch:25 step:23810 [D loss: 0.569432, acc.: 70.31%] [G loss: 1.225139]\n",
      "epoch:25 step:23811 [D loss: 0.553587, acc.: 73.44%] [G loss: 1.552975]\n",
      "epoch:25 step:23812 [D loss: 0.543689, acc.: 74.22%] [G loss: 1.252450]\n",
      "epoch:25 step:23813 [D loss: 0.578158, acc.: 68.75%] [G loss: 1.159124]\n",
      "epoch:25 step:23814 [D loss: 0.684481, acc.: 64.06%] [G loss: 1.330117]\n",
      "epoch:25 step:23815 [D loss: 0.500164, acc.: 73.44%] [G loss: 1.073806]\n",
      "epoch:25 step:23816 [D loss: 0.558622, acc.: 71.88%] [G loss: 1.633754]\n",
      "epoch:25 step:23817 [D loss: 0.518973, acc.: 72.66%] [G loss: 1.536971]\n",
      "epoch:25 step:23818 [D loss: 0.764683, acc.: 53.12%] [G loss: 1.451564]\n",
      "epoch:25 step:23819 [D loss: 0.559358, acc.: 74.22%] [G loss: 1.098876]\n",
      "epoch:25 step:23820 [D loss: 0.477410, acc.: 73.44%] [G loss: 1.556063]\n",
      "epoch:25 step:23821 [D loss: 0.494995, acc.: 78.91%] [G loss: 1.424535]\n",
      "epoch:25 step:23822 [D loss: 0.628280, acc.: 67.97%] [G loss: 1.406807]\n",
      "epoch:25 step:23823 [D loss: 0.702050, acc.: 54.69%] [G loss: 1.250639]\n",
      "epoch:25 step:23824 [D loss: 0.543477, acc.: 74.22%] [G loss: 1.184503]\n",
      "epoch:25 step:23825 [D loss: 0.658265, acc.: 59.38%] [G loss: 1.723631]\n",
      "epoch:25 step:23826 [D loss: 0.599125, acc.: 71.88%] [G loss: 1.603884]\n",
      "epoch:25 step:23827 [D loss: 0.357795, acc.: 86.72%] [G loss: 1.553484]\n",
      "epoch:25 step:23828 [D loss: 0.363585, acc.: 88.28%] [G loss: 1.639530]\n",
      "epoch:25 step:23829 [D loss: 0.548736, acc.: 71.09%] [G loss: 1.257468]\n",
      "epoch:25 step:23830 [D loss: 0.550076, acc.: 71.09%] [G loss: 1.532832]\n",
      "epoch:25 step:23831 [D loss: 0.480848, acc.: 72.66%] [G loss: 1.342997]\n",
      "epoch:25 step:23832 [D loss: 0.610333, acc.: 68.75%] [G loss: 1.341873]\n",
      "epoch:25 step:23833 [D loss: 0.480228, acc.: 78.12%] [G loss: 1.588771]\n",
      "epoch:25 step:23834 [D loss: 0.566040, acc.: 74.22%] [G loss: 1.296685]\n",
      "epoch:25 step:23835 [D loss: 0.554995, acc.: 72.66%] [G loss: 1.262586]\n",
      "epoch:25 step:23836 [D loss: 0.550579, acc.: 75.00%] [G loss: 1.342492]\n",
      "epoch:25 step:23837 [D loss: 0.600923, acc.: 72.66%] [G loss: 1.706245]\n",
      "epoch:25 step:23838 [D loss: 0.601820, acc.: 67.19%] [G loss: 1.533149]\n",
      "epoch:25 step:23839 [D loss: 0.596355, acc.: 68.75%] [G loss: 1.262833]\n",
      "epoch:25 step:23840 [D loss: 0.462595, acc.: 82.03%] [G loss: 1.867250]\n",
      "epoch:25 step:23841 [D loss: 0.376634, acc.: 86.72%] [G loss: 1.087871]\n",
      "epoch:25 step:23842 [D loss: 0.592856, acc.: 71.09%] [G loss: 1.009369]\n",
      "epoch:25 step:23843 [D loss: 0.491886, acc.: 75.78%] [G loss: 1.522733]\n",
      "epoch:25 step:23844 [D loss: 0.494127, acc.: 77.34%] [G loss: 1.380503]\n",
      "epoch:25 step:23845 [D loss: 0.502653, acc.: 76.56%] [G loss: 1.225329]\n",
      "epoch:25 step:23846 [D loss: 0.687772, acc.: 57.81%] [G loss: 1.327765]\n",
      "epoch:25 step:23847 [D loss: 0.526483, acc.: 76.56%] [G loss: 1.389842]\n",
      "epoch:25 step:23848 [D loss: 0.689358, acc.: 55.47%] [G loss: 1.574537]\n",
      "epoch:25 step:23849 [D loss: 0.634681, acc.: 65.62%] [G loss: 1.553347]\n",
      "epoch:25 step:23850 [D loss: 0.569262, acc.: 73.44%] [G loss: 1.728407]\n",
      "epoch:25 step:23851 [D loss: 0.605966, acc.: 68.75%] [G loss: 1.272643]\n",
      "epoch:25 step:23852 [D loss: 0.616215, acc.: 66.41%] [G loss: 1.250506]\n",
      "epoch:25 step:23853 [D loss: 0.550646, acc.: 71.09%] [G loss: 1.069456]\n",
      "epoch:25 step:23854 [D loss: 0.449261, acc.: 78.91%] [G loss: 1.221616]\n",
      "epoch:25 step:23855 [D loss: 0.662392, acc.: 67.97%] [G loss: 0.797501]\n",
      "epoch:25 step:23856 [D loss: 0.595515, acc.: 62.50%] [G loss: 0.863322]\n",
      "epoch:25 step:23857 [D loss: 0.410765, acc.: 83.59%] [G loss: 1.187608]\n",
      "epoch:25 step:23858 [D loss: 0.624004, acc.: 64.06%] [G loss: 1.178984]\n",
      "epoch:25 step:23859 [D loss: 0.651690, acc.: 62.50%] [G loss: 1.315104]\n",
      "epoch:25 step:23860 [D loss: 0.597632, acc.: 67.97%] [G loss: 1.263563]\n",
      "epoch:25 step:23861 [D loss: 0.471247, acc.: 78.12%] [G loss: 1.180120]\n",
      "epoch:25 step:23862 [D loss: 0.540560, acc.: 73.44%] [G loss: 1.340382]\n",
      "epoch:25 step:23863 [D loss: 0.549800, acc.: 67.19%] [G loss: 1.241939]\n",
      "epoch:25 step:23864 [D loss: 0.631189, acc.: 64.06%] [G loss: 1.444438]\n",
      "epoch:25 step:23865 [D loss: 0.504167, acc.: 78.12%] [G loss: 1.532636]\n",
      "epoch:25 step:23866 [D loss: 0.631969, acc.: 64.84%] [G loss: 1.214368]\n",
      "epoch:25 step:23867 [D loss: 0.361033, acc.: 88.28%] [G loss: 1.086104]\n",
      "epoch:25 step:23868 [D loss: 0.597959, acc.: 66.41%] [G loss: 1.440778]\n",
      "epoch:25 step:23869 [D loss: 0.676761, acc.: 62.50%] [G loss: 1.356217]\n",
      "epoch:25 step:23870 [D loss: 0.397453, acc.: 86.72%] [G loss: 1.649351]\n",
      "epoch:25 step:23871 [D loss: 0.940744, acc.: 39.84%] [G loss: 1.202516]\n",
      "epoch:25 step:23872 [D loss: 0.551796, acc.: 72.66%] [G loss: 1.491008]\n",
      "epoch:25 step:23873 [D loss: 0.575503, acc.: 68.75%] [G loss: 1.778267]\n",
      "epoch:25 step:23874 [D loss: 0.602144, acc.: 67.19%] [G loss: 1.264115]\n",
      "epoch:25 step:23875 [D loss: 0.634448, acc.: 62.50%] [G loss: 1.330061]\n",
      "epoch:25 step:23876 [D loss: 0.466820, acc.: 79.69%] [G loss: 1.202161]\n",
      "epoch:25 step:23877 [D loss: 0.607536, acc.: 67.19%] [G loss: 1.137039]\n",
      "epoch:25 step:23878 [D loss: 0.411316, acc.: 84.38%] [G loss: 1.739712]\n",
      "epoch:25 step:23879 [D loss: 0.459992, acc.: 78.12%] [G loss: 1.263686]\n",
      "epoch:25 step:23880 [D loss: 0.407070, acc.: 85.94%] [G loss: 1.231651]\n",
      "epoch:25 step:23881 [D loss: 0.553200, acc.: 67.97%] [G loss: 1.545683]\n",
      "epoch:25 step:23882 [D loss: 0.630065, acc.: 64.84%] [G loss: 0.990982]\n",
      "epoch:25 step:23883 [D loss: 0.490442, acc.: 82.81%] [G loss: 1.328556]\n",
      "epoch:25 step:23884 [D loss: 0.672986, acc.: 58.59%] [G loss: 1.215644]\n",
      "epoch:25 step:23885 [D loss: 0.464739, acc.: 79.69%] [G loss: 1.556317]\n",
      "epoch:25 step:23886 [D loss: 0.510484, acc.: 80.47%] [G loss: 1.622632]\n",
      "epoch:25 step:23887 [D loss: 0.761798, acc.: 53.12%] [G loss: 1.335430]\n",
      "epoch:25 step:23888 [D loss: 0.608426, acc.: 65.62%] [G loss: 1.657894]\n",
      "epoch:25 step:23889 [D loss: 0.621833, acc.: 63.28%] [G loss: 1.324665]\n",
      "epoch:25 step:23890 [D loss: 0.562375, acc.: 68.75%] [G loss: 1.558245]\n",
      "epoch:25 step:23891 [D loss: 0.627159, acc.: 70.31%] [G loss: 1.512790]\n",
      "epoch:25 step:23892 [D loss: 0.644795, acc.: 63.28%] [G loss: 1.175865]\n",
      "epoch:25 step:23893 [D loss: 0.488129, acc.: 78.12%] [G loss: 1.422979]\n",
      "epoch:25 step:23894 [D loss: 0.569962, acc.: 69.53%] [G loss: 1.855275]\n",
      "epoch:25 step:23895 [D loss: 0.768155, acc.: 50.00%] [G loss: 1.273704]\n",
      "epoch:25 step:23896 [D loss: 0.757892, acc.: 53.12%] [G loss: 1.021390]\n",
      "epoch:25 step:23897 [D loss: 0.527813, acc.: 70.31%] [G loss: 1.606457]\n",
      "epoch:25 step:23898 [D loss: 0.415087, acc.: 82.81%] [G loss: 1.281056]\n",
      "epoch:25 step:23899 [D loss: 0.579852, acc.: 73.44%] [G loss: 1.187181]\n",
      "epoch:25 step:23900 [D loss: 0.464634, acc.: 77.34%] [G loss: 1.348353]\n",
      "epoch:25 step:23901 [D loss: 0.525836, acc.: 77.34%] [G loss: 1.427321]\n",
      "epoch:25 step:23902 [D loss: 0.570526, acc.: 75.00%] [G loss: 1.229897]\n",
      "epoch:25 step:23903 [D loss: 0.415148, acc.: 84.38%] [G loss: 2.272489]\n",
      "epoch:25 step:23904 [D loss: 0.487192, acc.: 77.34%] [G loss: 1.065793]\n",
      "epoch:25 step:23905 [D loss: 0.393109, acc.: 88.28%] [G loss: 1.535729]\n",
      "epoch:25 step:23906 [D loss: 0.563226, acc.: 70.31%] [G loss: 1.547637]\n",
      "epoch:25 step:23907 [D loss: 0.495189, acc.: 76.56%] [G loss: 1.495171]\n",
      "epoch:25 step:23908 [D loss: 0.655704, acc.: 60.94%] [G loss: 1.050116]\n",
      "epoch:25 step:23909 [D loss: 0.594639, acc.: 72.66%] [G loss: 1.450658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23910 [D loss: 0.710443, acc.: 57.81%] [G loss: 1.210973]\n",
      "epoch:25 step:23911 [D loss: 0.496258, acc.: 77.34%] [G loss: 1.445895]\n",
      "epoch:25 step:23912 [D loss: 0.531455, acc.: 75.00%] [G loss: 1.410853]\n",
      "epoch:25 step:23913 [D loss: 0.473012, acc.: 78.12%] [G loss: 1.642828]\n",
      "epoch:25 step:23914 [D loss: 0.698101, acc.: 61.72%] [G loss: 1.240057]\n",
      "epoch:25 step:23915 [D loss: 0.644226, acc.: 60.94%] [G loss: 0.945558]\n",
      "epoch:25 step:23916 [D loss: 0.616962, acc.: 69.53%] [G loss: 1.334373]\n",
      "epoch:25 step:23917 [D loss: 0.533409, acc.: 71.09%] [G loss: 1.970100]\n",
      "epoch:25 step:23918 [D loss: 0.425222, acc.: 79.69%] [G loss: 1.718435]\n",
      "epoch:25 step:23919 [D loss: 0.617739, acc.: 67.19%] [G loss: 1.429757]\n",
      "epoch:25 step:23920 [D loss: 0.570483, acc.: 67.97%] [G loss: 1.438200]\n",
      "epoch:25 step:23921 [D loss: 0.562809, acc.: 70.31%] [G loss: 1.658607]\n",
      "epoch:25 step:23922 [D loss: 0.553650, acc.: 72.66%] [G loss: 1.682704]\n",
      "epoch:25 step:23923 [D loss: 0.618856, acc.: 63.28%] [G loss: 1.039098]\n",
      "epoch:25 step:23924 [D loss: 0.634827, acc.: 68.75%] [G loss: 1.454641]\n",
      "epoch:25 step:23925 [D loss: 0.573455, acc.: 77.34%] [G loss: 1.723118]\n",
      "epoch:25 step:23926 [D loss: 0.418857, acc.: 82.03%] [G loss: 1.626264]\n",
      "epoch:25 step:23927 [D loss: 0.505371, acc.: 77.34%] [G loss: 1.326325]\n",
      "epoch:25 step:23928 [D loss: 0.376688, acc.: 85.16%] [G loss: 1.716318]\n",
      "epoch:25 step:23929 [D loss: 0.541607, acc.: 72.66%] [G loss: 1.376698]\n",
      "epoch:25 step:23930 [D loss: 0.528988, acc.: 75.78%] [G loss: 1.122007]\n",
      "epoch:25 step:23931 [D loss: 0.618761, acc.: 63.28%] [G loss: 1.019313]\n",
      "epoch:25 step:23932 [D loss: 0.528772, acc.: 75.78%] [G loss: 1.867934]\n",
      "epoch:25 step:23933 [D loss: 0.500723, acc.: 70.31%] [G loss: 1.090468]\n",
      "epoch:25 step:23934 [D loss: 0.361127, acc.: 89.06%] [G loss: 1.777794]\n",
      "epoch:25 step:23935 [D loss: 0.531914, acc.: 74.22%] [G loss: 1.323578]\n",
      "epoch:25 step:23936 [D loss: 0.502345, acc.: 78.12%] [G loss: 1.365867]\n",
      "epoch:25 step:23937 [D loss: 0.533016, acc.: 73.44%] [G loss: 1.385155]\n",
      "epoch:25 step:23938 [D loss: 0.420936, acc.: 82.03%] [G loss: 1.577103]\n",
      "epoch:25 step:23939 [D loss: 0.553088, acc.: 71.09%] [G loss: 1.785251]\n",
      "epoch:25 step:23940 [D loss: 0.640956, acc.: 63.28%] [G loss: 1.462367]\n",
      "epoch:25 step:23941 [D loss: 0.782145, acc.: 53.12%] [G loss: 1.326231]\n",
      "epoch:25 step:23942 [D loss: 0.461567, acc.: 79.69%] [G loss: 1.369235]\n",
      "epoch:25 step:23943 [D loss: 0.655277, acc.: 65.62%] [G loss: 1.413162]\n",
      "epoch:25 step:23944 [D loss: 0.578725, acc.: 69.53%] [G loss: 1.486404]\n",
      "epoch:25 step:23945 [D loss: 0.519900, acc.: 73.44%] [G loss: 1.082496]\n",
      "epoch:25 step:23946 [D loss: 0.745231, acc.: 52.34%] [G loss: 1.325975]\n",
      "epoch:25 step:23947 [D loss: 0.716204, acc.: 57.81%] [G loss: 0.956116]\n",
      "epoch:25 step:23948 [D loss: 0.584589, acc.: 69.53%] [G loss: 1.229214]\n",
      "epoch:25 step:23949 [D loss: 0.592453, acc.: 69.53%] [G loss: 1.443228]\n",
      "epoch:25 step:23950 [D loss: 0.523363, acc.: 75.00%] [G loss: 1.163949]\n",
      "epoch:25 step:23951 [D loss: 0.510078, acc.: 76.56%] [G loss: 1.214563]\n",
      "epoch:25 step:23952 [D loss: 0.539901, acc.: 71.09%] [G loss: 1.562018]\n",
      "epoch:25 step:23953 [D loss: 0.452532, acc.: 82.03%] [G loss: 1.583355]\n",
      "epoch:25 step:23954 [D loss: 0.417539, acc.: 85.94%] [G loss: 1.426044]\n",
      "epoch:25 step:23955 [D loss: 0.606390, acc.: 62.50%] [G loss: 1.226846]\n",
      "epoch:25 step:23956 [D loss: 0.488573, acc.: 78.12%] [G loss: 1.490028]\n",
      "epoch:25 step:23957 [D loss: 0.629032, acc.: 67.97%] [G loss: 1.565368]\n",
      "epoch:25 step:23958 [D loss: 0.650506, acc.: 58.59%] [G loss: 1.226108]\n",
      "epoch:25 step:23959 [D loss: 0.624643, acc.: 67.97%] [G loss: 1.140379]\n",
      "epoch:25 step:23960 [D loss: 0.523985, acc.: 72.66%] [G loss: 1.334780]\n",
      "epoch:25 step:23961 [D loss: 0.723942, acc.: 53.91%] [G loss: 1.232494]\n",
      "epoch:25 step:23962 [D loss: 0.569490, acc.: 71.09%] [G loss: 1.054754]\n",
      "epoch:25 step:23963 [D loss: 0.546999, acc.: 71.09%] [G loss: 1.026372]\n",
      "epoch:25 step:23964 [D loss: 0.458590, acc.: 82.81%] [G loss: 1.436496]\n",
      "epoch:25 step:23965 [D loss: 0.423461, acc.: 83.59%] [G loss: 1.309760]\n",
      "epoch:25 step:23966 [D loss: 0.384993, acc.: 85.94%] [G loss: 1.380415]\n",
      "epoch:25 step:23967 [D loss: 0.427000, acc.: 87.50%] [G loss: 1.343140]\n",
      "epoch:25 step:23968 [D loss: 0.704296, acc.: 57.03%] [G loss: 1.575833]\n",
      "epoch:25 step:23969 [D loss: 0.386074, acc.: 85.94%] [G loss: 1.664672]\n",
      "epoch:25 step:23970 [D loss: 0.516134, acc.: 75.78%] [G loss: 1.544590]\n",
      "epoch:25 step:23971 [D loss: 0.463382, acc.: 80.47%] [G loss: 0.951540]\n",
      "epoch:25 step:23972 [D loss: 0.561081, acc.: 71.09%] [G loss: 0.945690]\n",
      "epoch:25 step:23973 [D loss: 0.591511, acc.: 66.41%] [G loss: 1.246343]\n",
      "epoch:25 step:23974 [D loss: 0.404559, acc.: 84.38%] [G loss: 1.171715]\n",
      "epoch:25 step:23975 [D loss: 0.458325, acc.: 78.12%] [G loss: 1.351830]\n",
      "epoch:25 step:23976 [D loss: 0.582208, acc.: 69.53%] [G loss: 1.310296]\n",
      "epoch:25 step:23977 [D loss: 0.360968, acc.: 90.62%] [G loss: 1.815247]\n",
      "epoch:25 step:23978 [D loss: 0.459974, acc.: 79.69%] [G loss: 1.523327]\n",
      "epoch:25 step:23979 [D loss: 0.771971, acc.: 52.34%] [G loss: 1.457773]\n",
      "epoch:25 step:23980 [D loss: 0.596871, acc.: 71.09%] [G loss: 1.489598]\n",
      "epoch:25 step:23981 [D loss: 0.744407, acc.: 56.25%] [G loss: 1.542305]\n",
      "epoch:25 step:23982 [D loss: 0.600202, acc.: 65.62%] [G loss: 1.334763]\n",
      "epoch:25 step:23983 [D loss: 0.588985, acc.: 67.19%] [G loss: 1.437858]\n",
      "epoch:25 step:23984 [D loss: 0.517706, acc.: 72.66%] [G loss: 0.990331]\n",
      "epoch:25 step:23985 [D loss: 0.527355, acc.: 75.78%] [G loss: 1.225639]\n",
      "epoch:25 step:23986 [D loss: 0.523628, acc.: 76.56%] [G loss: 1.401849]\n",
      "epoch:25 step:23987 [D loss: 0.619876, acc.: 63.28%] [G loss: 1.241812]\n",
      "epoch:25 step:23988 [D loss: 0.632942, acc.: 67.97%] [G loss: 1.291209]\n",
      "epoch:25 step:23989 [D loss: 0.565294, acc.: 71.09%] [G loss: 1.664941]\n",
      "epoch:25 step:23990 [D loss: 0.489875, acc.: 75.78%] [G loss: 1.276537]\n",
      "epoch:25 step:23991 [D loss: 0.502317, acc.: 76.56%] [G loss: 1.460742]\n",
      "epoch:25 step:23992 [D loss: 0.626277, acc.: 64.84%] [G loss: 1.178259]\n",
      "epoch:25 step:23993 [D loss: 0.610927, acc.: 62.50%] [G loss: 1.568193]\n",
      "epoch:25 step:23994 [D loss: 0.680216, acc.: 67.19%] [G loss: 1.290447]\n",
      "epoch:25 step:23995 [D loss: 0.690006, acc.: 60.94%] [G loss: 1.421498]\n",
      "epoch:25 step:23996 [D loss: 0.608486, acc.: 64.06%] [G loss: 1.596281]\n",
      "epoch:25 step:23997 [D loss: 0.457524, acc.: 80.47%] [G loss: 1.267948]\n",
      "epoch:25 step:23998 [D loss: 0.578718, acc.: 72.66%] [G loss: 1.036672]\n",
      "epoch:25 step:23999 [D loss: 0.668566, acc.: 60.94%] [G loss: 1.462517]\n",
      "epoch:25 step:24000 [D loss: 0.521245, acc.: 72.66%] [G loss: 1.336568]\n",
      "##############\n",
      "[2.73370522 2.03706855 2.02318863 3.23797587 1.06369824 5.93048858\n",
      " 2.14459903 2.93081454 4.07194357 5.0833018 ]\n",
      "##########\n",
      "epoch:25 step:24001 [D loss: 0.360363, acc.: 87.50%] [G loss: 1.278421]\n",
      "epoch:25 step:24002 [D loss: 0.561987, acc.: 75.00%] [G loss: 1.598238]\n",
      "epoch:25 step:24003 [D loss: 0.481638, acc.: 77.34%] [G loss: 1.138510]\n",
      "epoch:25 step:24004 [D loss: 0.572720, acc.: 71.09%] [G loss: 1.400423]\n",
      "epoch:25 step:24005 [D loss: 0.461062, acc.: 82.03%] [G loss: 0.930824]\n",
      "epoch:25 step:24006 [D loss: 0.616285, acc.: 71.09%] [G loss: 0.846892]\n",
      "epoch:25 step:24007 [D loss: 0.517346, acc.: 76.56%] [G loss: 1.482419]\n",
      "epoch:25 step:24008 [D loss: 0.551666, acc.: 71.88%] [G loss: 1.033084]\n",
      "epoch:25 step:24009 [D loss: 0.580187, acc.: 69.53%] [G loss: 1.455532]\n",
      "epoch:25 step:24010 [D loss: 0.510224, acc.: 75.78%] [G loss: 1.652734]\n",
      "epoch:25 step:24011 [D loss: 0.529768, acc.: 72.66%] [G loss: 1.940096]\n",
      "epoch:25 step:24012 [D loss: 0.581032, acc.: 72.66%] [G loss: 1.665234]\n",
      "epoch:25 step:24013 [D loss: 0.580129, acc.: 67.97%] [G loss: 1.896599]\n",
      "epoch:25 step:24014 [D loss: 0.596232, acc.: 65.62%] [G loss: 1.420070]\n",
      "epoch:25 step:24015 [D loss: 0.487339, acc.: 79.69%] [G loss: 0.908705]\n",
      "epoch:25 step:24016 [D loss: 0.436022, acc.: 81.25%] [G loss: 1.201923]\n",
      "epoch:25 step:24017 [D loss: 0.534121, acc.: 71.88%] [G loss: 1.694126]\n",
      "epoch:25 step:24018 [D loss: 0.614255, acc.: 66.41%] [G loss: 1.486539]\n",
      "epoch:25 step:24019 [D loss: 0.607919, acc.: 70.31%] [G loss: 1.394294]\n",
      "epoch:25 step:24020 [D loss: 0.624795, acc.: 69.53%] [G loss: 1.785092]\n",
      "epoch:25 step:24021 [D loss: 0.582187, acc.: 71.09%] [G loss: 1.441776]\n",
      "epoch:25 step:24022 [D loss: 0.498699, acc.: 77.34%] [G loss: 1.580270]\n",
      "epoch:25 step:24023 [D loss: 0.817884, acc.: 53.12%] [G loss: 0.934895]\n",
      "epoch:25 step:24024 [D loss: 0.516889, acc.: 74.22%] [G loss: 1.293288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24025 [D loss: 0.391236, acc.: 83.59%] [G loss: 1.578507]\n",
      "epoch:25 step:24026 [D loss: 0.560503, acc.: 71.09%] [G loss: 1.820437]\n",
      "epoch:25 step:24027 [D loss: 0.505711, acc.: 73.44%] [G loss: 0.903550]\n",
      "epoch:25 step:24028 [D loss: 0.658208, acc.: 61.72%] [G loss: 1.200310]\n",
      "epoch:25 step:24029 [D loss: 0.511202, acc.: 72.66%] [G loss: 1.159507]\n",
      "epoch:25 step:24030 [D loss: 0.624395, acc.: 65.62%] [G loss: 1.135924]\n",
      "epoch:25 step:24031 [D loss: 0.567351, acc.: 70.31%] [G loss: 1.020417]\n",
      "epoch:25 step:24032 [D loss: 0.571772, acc.: 64.06%] [G loss: 1.369642]\n",
      "epoch:25 step:24033 [D loss: 0.449792, acc.: 77.34%] [G loss: 1.504070]\n",
      "epoch:25 step:24034 [D loss: 0.568871, acc.: 67.19%] [G loss: 1.529224]\n",
      "epoch:25 step:24035 [D loss: 0.515404, acc.: 77.34%] [G loss: 1.441121]\n",
      "epoch:25 step:24036 [D loss: 0.565416, acc.: 67.97%] [G loss: 1.780505]\n",
      "epoch:25 step:24037 [D loss: 0.635292, acc.: 60.94%] [G loss: 1.858625]\n",
      "epoch:25 step:24038 [D loss: 0.494835, acc.: 75.78%] [G loss: 1.773286]\n",
      "epoch:25 step:24039 [D loss: 0.648305, acc.: 60.94%] [G loss: 1.327127]\n",
      "epoch:25 step:24040 [D loss: 0.645455, acc.: 67.19%] [G loss: 1.350009]\n",
      "epoch:25 step:24041 [D loss: 0.596100, acc.: 71.09%] [G loss: 1.341092]\n",
      "epoch:25 step:24042 [D loss: 0.447476, acc.: 81.25%] [G loss: 1.619124]\n",
      "epoch:25 step:24043 [D loss: 0.592881, acc.: 68.75%] [G loss: 1.300516]\n",
      "epoch:25 step:24044 [D loss: 0.762955, acc.: 48.44%] [G loss: 1.078464]\n",
      "epoch:25 step:24045 [D loss: 0.651099, acc.: 60.16%] [G loss: 1.150160]\n",
      "epoch:25 step:24046 [D loss: 0.741464, acc.: 57.03%] [G loss: 1.344386]\n",
      "epoch:25 step:24047 [D loss: 0.618063, acc.: 65.62%] [G loss: 1.225047]\n",
      "epoch:25 step:24048 [D loss: 0.578169, acc.: 67.19%] [G loss: 1.466306]\n",
      "epoch:25 step:24049 [D loss: 0.584300, acc.: 67.19%] [G loss: 1.411137]\n",
      "epoch:25 step:24050 [D loss: 0.434203, acc.: 82.81%] [G loss: 1.478986]\n",
      "epoch:25 step:24051 [D loss: 0.417462, acc.: 83.59%] [G loss: 1.868728]\n",
      "epoch:25 step:24052 [D loss: 0.438101, acc.: 82.81%] [G loss: 1.473250]\n",
      "epoch:25 step:24053 [D loss: 0.733821, acc.: 58.59%] [G loss: 1.136571]\n",
      "epoch:25 step:24054 [D loss: 0.388430, acc.: 84.38%] [G loss: 1.581718]\n",
      "epoch:25 step:24055 [D loss: 0.499838, acc.: 78.12%] [G loss: 1.237272]\n",
      "epoch:25 step:24056 [D loss: 0.488283, acc.: 74.22%] [G loss: 1.159483]\n",
      "epoch:25 step:24057 [D loss: 0.412539, acc.: 83.59%] [G loss: 1.103974]\n",
      "epoch:25 step:24058 [D loss: 0.507128, acc.: 74.22%] [G loss: 1.417419]\n",
      "epoch:25 step:24059 [D loss: 0.439777, acc.: 83.59%] [G loss: 1.541365]\n",
      "epoch:25 step:24060 [D loss: 0.660619, acc.: 66.41%] [G loss: 1.055752]\n",
      "epoch:25 step:24061 [D loss: 0.415207, acc.: 85.16%] [G loss: 1.544864]\n",
      "epoch:25 step:24062 [D loss: 0.614025, acc.: 66.41%] [G loss: 1.275625]\n",
      "epoch:25 step:24063 [D loss: 0.512413, acc.: 76.56%] [G loss: 1.353636]\n",
      "epoch:25 step:24064 [D loss: 0.574091, acc.: 71.88%] [G loss: 1.379885]\n",
      "epoch:25 step:24065 [D loss: 0.417125, acc.: 84.38%] [G loss: 1.806209]\n",
      "epoch:25 step:24066 [D loss: 0.655262, acc.: 63.28%] [G loss: 0.956043]\n",
      "epoch:25 step:24067 [D loss: 0.614700, acc.: 62.50%] [G loss: 0.802772]\n",
      "epoch:25 step:24068 [D loss: 0.713936, acc.: 60.94%] [G loss: 1.177410]\n",
      "epoch:25 step:24069 [D loss: 0.579249, acc.: 68.75%] [G loss: 1.404692]\n",
      "epoch:25 step:24070 [D loss: 0.471957, acc.: 78.91%] [G loss: 1.457894]\n",
      "epoch:25 step:24071 [D loss: 0.710479, acc.: 57.81%] [G loss: 1.140886]\n",
      "epoch:25 step:24072 [D loss: 0.490447, acc.: 77.34%] [G loss: 1.459272]\n",
      "epoch:25 step:24073 [D loss: 0.577289, acc.: 66.41%] [G loss: 1.258728]\n",
      "epoch:25 step:24074 [D loss: 0.454254, acc.: 82.03%] [G loss: 1.717613]\n",
      "epoch:25 step:24075 [D loss: 0.566969, acc.: 71.88%] [G loss: 1.384702]\n",
      "epoch:25 step:24076 [D loss: 0.473503, acc.: 78.91%] [G loss: 1.545254]\n",
      "epoch:25 step:24077 [D loss: 0.609911, acc.: 68.75%] [G loss: 1.432046]\n",
      "epoch:25 step:24078 [D loss: 0.499618, acc.: 78.91%] [G loss: 1.828201]\n",
      "epoch:25 step:24079 [D loss: 0.513564, acc.: 77.34%] [G loss: 1.375284]\n",
      "epoch:25 step:24080 [D loss: 0.799553, acc.: 50.78%] [G loss: 1.170924]\n",
      "epoch:25 step:24081 [D loss: 0.570501, acc.: 69.53%] [G loss: 1.113384]\n",
      "epoch:25 step:24082 [D loss: 0.452370, acc.: 82.03%] [G loss: 1.322074]\n",
      "epoch:25 step:24083 [D loss: 0.610565, acc.: 67.19%] [G loss: 1.473164]\n",
      "epoch:25 step:24084 [D loss: 0.486062, acc.: 75.78%] [G loss: 1.563712]\n",
      "epoch:25 step:24085 [D loss: 0.598593, acc.: 71.88%] [G loss: 1.355087]\n",
      "epoch:25 step:24086 [D loss: 0.710734, acc.: 59.38%] [G loss: 1.833568]\n",
      "epoch:25 step:24087 [D loss: 0.598998, acc.: 68.75%] [G loss: 1.965024]\n",
      "epoch:25 step:24088 [D loss: 0.589929, acc.: 68.75%] [G loss: 1.532062]\n",
      "epoch:25 step:24089 [D loss: 0.418289, acc.: 85.16%] [G loss: 1.396467]\n",
      "epoch:25 step:24090 [D loss: 0.493263, acc.: 76.56%] [G loss: 1.337062]\n",
      "epoch:25 step:24091 [D loss: 0.511131, acc.: 75.78%] [G loss: 1.694544]\n",
      "epoch:25 step:24092 [D loss: 0.608904, acc.: 67.19%] [G loss: 1.432907]\n",
      "epoch:25 step:24093 [D loss: 0.754824, acc.: 59.38%] [G loss: 1.468442]\n",
      "epoch:25 step:24094 [D loss: 0.482000, acc.: 73.44%] [G loss: 1.120834]\n",
      "epoch:25 step:24095 [D loss: 0.566182, acc.: 72.66%] [G loss: 1.201217]\n",
      "epoch:25 step:24096 [D loss: 0.568838, acc.: 67.97%] [G loss: 1.297400]\n",
      "epoch:25 step:24097 [D loss: 0.389500, acc.: 86.72%] [G loss: 1.096121]\n",
      "epoch:25 step:24098 [D loss: 0.656272, acc.: 62.50%] [G loss: 1.551016]\n",
      "epoch:25 step:24099 [D loss: 0.571383, acc.: 70.31%] [G loss: 1.687418]\n",
      "epoch:25 step:24100 [D loss: 0.766841, acc.: 55.47%] [G loss: 1.291693]\n",
      "epoch:25 step:24101 [D loss: 0.586847, acc.: 67.97%] [G loss: 1.329245]\n",
      "epoch:25 step:24102 [D loss: 0.518737, acc.: 74.22%] [G loss: 1.484782]\n",
      "epoch:25 step:24103 [D loss: 0.539301, acc.: 67.19%] [G loss: 1.858097]\n",
      "epoch:25 step:24104 [D loss: 0.756438, acc.: 57.03%] [G loss: 1.497519]\n",
      "epoch:25 step:24105 [D loss: 0.542101, acc.: 71.09%] [G loss: 1.549656]\n",
      "epoch:25 step:24106 [D loss: 0.433356, acc.: 82.03%] [G loss: 1.636998]\n",
      "epoch:25 step:24107 [D loss: 0.384780, acc.: 88.28%] [G loss: 1.394357]\n",
      "epoch:25 step:24108 [D loss: 0.520377, acc.: 78.12%] [G loss: 1.337510]\n",
      "epoch:25 step:24109 [D loss: 0.452677, acc.: 78.12%] [G loss: 1.549176]\n",
      "epoch:25 step:24110 [D loss: 0.497411, acc.: 78.12%] [G loss: 1.304956]\n",
      "epoch:25 step:24111 [D loss: 0.587315, acc.: 72.66%] [G loss: 1.180828]\n",
      "epoch:25 step:24112 [D loss: 0.522771, acc.: 75.00%] [G loss: 1.716968]\n",
      "epoch:25 step:24113 [D loss: 0.639391, acc.: 64.06%] [G loss: 1.172229]\n",
      "epoch:25 step:24114 [D loss: 0.672614, acc.: 55.47%] [G loss: 1.615966]\n",
      "epoch:25 step:24115 [D loss: 0.456008, acc.: 79.69%] [G loss: 1.187742]\n",
      "epoch:25 step:24116 [D loss: 0.487799, acc.: 82.03%] [G loss: 1.319404]\n",
      "epoch:25 step:24117 [D loss: 0.652089, acc.: 60.16%] [G loss: 1.076787]\n",
      "epoch:25 step:24118 [D loss: 0.661321, acc.: 66.41%] [G loss: 1.526426]\n",
      "epoch:25 step:24119 [D loss: 0.567798, acc.: 71.09%] [G loss: 1.663866]\n",
      "epoch:25 step:24120 [D loss: 0.565206, acc.: 72.66%] [G loss: 1.116893]\n",
      "epoch:25 step:24121 [D loss: 0.464517, acc.: 76.56%] [G loss: 1.595433]\n",
      "epoch:25 step:24122 [D loss: 0.357730, acc.: 86.72%] [G loss: 1.829869]\n",
      "epoch:25 step:24123 [D loss: 0.518824, acc.: 75.00%] [G loss: 1.021278]\n",
      "epoch:25 step:24124 [D loss: 0.605021, acc.: 64.84%] [G loss: 1.324860]\n",
      "epoch:25 step:24125 [D loss: 0.611865, acc.: 64.84%] [G loss: 1.109430]\n",
      "epoch:25 step:24126 [D loss: 0.356430, acc.: 87.50%] [G loss: 1.557358]\n",
      "epoch:25 step:24127 [D loss: 0.390150, acc.: 83.59%] [G loss: 1.730804]\n",
      "epoch:25 step:24128 [D loss: 0.471854, acc.: 79.69%] [G loss: 1.514283]\n",
      "epoch:25 step:24129 [D loss: 0.649659, acc.: 62.50%] [G loss: 1.100535]\n",
      "epoch:25 step:24130 [D loss: 0.406778, acc.: 83.59%] [G loss: 1.379321]\n",
      "epoch:25 step:24131 [D loss: 0.625136, acc.: 70.31%] [G loss: 1.070819]\n",
      "epoch:25 step:24132 [D loss: 0.494366, acc.: 77.34%] [G loss: 1.576505]\n",
      "epoch:25 step:24133 [D loss: 0.492994, acc.: 73.44%] [G loss: 1.418861]\n",
      "epoch:25 step:24134 [D loss: 0.487175, acc.: 82.03%] [G loss: 1.195927]\n",
      "epoch:25 step:24135 [D loss: 0.601142, acc.: 68.75%] [G loss: 1.525428]\n",
      "epoch:25 step:24136 [D loss: 0.654562, acc.: 60.94%] [G loss: 1.340376]\n",
      "epoch:25 step:24137 [D loss: 0.577425, acc.: 65.62%] [G loss: 1.196167]\n",
      "epoch:25 step:24138 [D loss: 0.559313, acc.: 71.88%] [G loss: 1.218739]\n",
      "epoch:25 step:24139 [D loss: 0.460652, acc.: 80.47%] [G loss: 1.800427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24140 [D loss: 0.573757, acc.: 72.66%] [G loss: 1.333103]\n",
      "epoch:25 step:24141 [D loss: 0.417168, acc.: 80.47%] [G loss: 1.335339]\n",
      "epoch:25 step:24142 [D loss: 0.511891, acc.: 75.78%] [G loss: 1.369130]\n",
      "epoch:25 step:24143 [D loss: 0.466816, acc.: 83.59%] [G loss: 1.427329]\n",
      "epoch:25 step:24144 [D loss: 0.625414, acc.: 61.72%] [G loss: 1.628514]\n",
      "epoch:25 step:24145 [D loss: 0.640709, acc.: 67.19%] [G loss: 1.204983]\n",
      "epoch:25 step:24146 [D loss: 0.465144, acc.: 81.25%] [G loss: 1.540364]\n",
      "epoch:25 step:24147 [D loss: 0.633460, acc.: 64.84%] [G loss: 1.466371]\n",
      "epoch:25 step:24148 [D loss: 0.738226, acc.: 58.59%] [G loss: 1.508021]\n",
      "epoch:25 step:24149 [D loss: 0.456980, acc.: 82.03%] [G loss: 1.316229]\n",
      "epoch:25 step:24150 [D loss: 0.467557, acc.: 81.25%] [G loss: 1.255899]\n",
      "epoch:25 step:24151 [D loss: 0.696608, acc.: 59.38%] [G loss: 1.586064]\n",
      "epoch:25 step:24152 [D loss: 0.375626, acc.: 86.72%] [G loss: 1.325790]\n",
      "epoch:25 step:24153 [D loss: 0.680504, acc.: 53.91%] [G loss: 1.383355]\n",
      "epoch:25 step:24154 [D loss: 0.588603, acc.: 67.97%] [G loss: 1.657797]\n",
      "epoch:25 step:24155 [D loss: 0.595677, acc.: 63.28%] [G loss: 1.535778]\n",
      "epoch:25 step:24156 [D loss: 0.593749, acc.: 64.06%] [G loss: 1.268293]\n",
      "epoch:25 step:24157 [D loss: 0.544625, acc.: 72.66%] [G loss: 0.982532]\n",
      "epoch:25 step:24158 [D loss: 0.522598, acc.: 76.56%] [G loss: 1.160708]\n",
      "epoch:25 step:24159 [D loss: 0.457096, acc.: 78.91%] [G loss: 1.371178]\n",
      "epoch:25 step:24160 [D loss: 0.534876, acc.: 70.31%] [G loss: 1.561685]\n",
      "epoch:25 step:24161 [D loss: 0.522670, acc.: 69.53%] [G loss: 1.459012]\n",
      "epoch:25 step:24162 [D loss: 0.638911, acc.: 70.31%] [G loss: 1.583804]\n",
      "epoch:25 step:24163 [D loss: 0.912560, acc.: 43.75%] [G loss: 1.073145]\n",
      "epoch:25 step:24164 [D loss: 0.533609, acc.: 68.75%] [G loss: 1.214345]\n",
      "epoch:25 step:24165 [D loss: 0.495838, acc.: 76.56%] [G loss: 1.815273]\n",
      "epoch:25 step:24166 [D loss: 0.576337, acc.: 64.84%] [G loss: 1.185195]\n",
      "epoch:25 step:24167 [D loss: 0.546660, acc.: 68.75%] [G loss: 1.436692]\n",
      "epoch:25 step:24168 [D loss: 0.778726, acc.: 50.78%] [G loss: 1.391057]\n",
      "epoch:25 step:24169 [D loss: 0.347944, acc.: 89.06%] [G loss: 1.308883]\n",
      "epoch:25 step:24170 [D loss: 0.661653, acc.: 60.94%] [G loss: 0.984782]\n",
      "epoch:25 step:24171 [D loss: 0.570354, acc.: 71.09%] [G loss: 1.215520]\n",
      "epoch:25 step:24172 [D loss: 0.500880, acc.: 75.00%] [G loss: 1.527704]\n",
      "epoch:25 step:24173 [D loss: 0.532885, acc.: 73.44%] [G loss: 1.579188]\n",
      "epoch:25 step:24174 [D loss: 0.579246, acc.: 66.41%] [G loss: 1.142938]\n",
      "epoch:25 step:24175 [D loss: 0.618654, acc.: 69.53%] [G loss: 1.427415]\n",
      "epoch:25 step:24176 [D loss: 0.549432, acc.: 73.44%] [G loss: 1.233101]\n",
      "epoch:25 step:24177 [D loss: 0.546930, acc.: 71.88%] [G loss: 1.372282]\n",
      "epoch:25 step:24178 [D loss: 0.590255, acc.: 67.19%] [G loss: 1.317974]\n",
      "epoch:25 step:24179 [D loss: 0.531288, acc.: 74.22%] [G loss: 1.257943]\n",
      "epoch:25 step:24180 [D loss: 0.578435, acc.: 67.97%] [G loss: 1.305931]\n",
      "epoch:25 step:24181 [D loss: 0.505134, acc.: 75.00%] [G loss: 1.147539]\n",
      "epoch:25 step:24182 [D loss: 0.644205, acc.: 55.47%] [G loss: 1.194873]\n",
      "epoch:25 step:24183 [D loss: 0.538942, acc.: 71.09%] [G loss: 1.366669]\n",
      "epoch:25 step:24184 [D loss: 0.407427, acc.: 87.50%] [G loss: 1.055478]\n",
      "epoch:25 step:24185 [D loss: 0.595593, acc.: 71.09%] [G loss: 1.438701]\n",
      "epoch:25 step:24186 [D loss: 0.513271, acc.: 79.69%] [G loss: 1.124487]\n",
      "epoch:25 step:24187 [D loss: 0.705543, acc.: 57.03%] [G loss: 1.213078]\n",
      "epoch:25 step:24188 [D loss: 0.554742, acc.: 70.31%] [G loss: 1.431104]\n",
      "epoch:25 step:24189 [D loss: 0.343810, acc.: 89.84%] [G loss: 1.832125]\n",
      "epoch:25 step:24190 [D loss: 0.561378, acc.: 69.53%] [G loss: 1.195899]\n",
      "epoch:25 step:24191 [D loss: 0.587397, acc.: 69.53%] [G loss: 1.076434]\n",
      "epoch:25 step:24192 [D loss: 0.427883, acc.: 78.91%] [G loss: 1.455887]\n",
      "epoch:25 step:24193 [D loss: 0.392125, acc.: 88.28%] [G loss: 1.247873]\n",
      "epoch:25 step:24194 [D loss: 0.458509, acc.: 81.25%] [G loss: 1.776207]\n",
      "epoch:25 step:24195 [D loss: 0.592659, acc.: 73.44%] [G loss: 1.107538]\n",
      "epoch:25 step:24196 [D loss: 0.521033, acc.: 71.88%] [G loss: 1.635625]\n",
      "epoch:25 step:24197 [D loss: 0.490393, acc.: 74.22%] [G loss: 1.209011]\n",
      "epoch:25 step:24198 [D loss: 0.774233, acc.: 51.56%] [G loss: 1.115554]\n",
      "epoch:25 step:24199 [D loss: 0.489253, acc.: 78.91%] [G loss: 1.538581]\n",
      "epoch:25 step:24200 [D loss: 0.619128, acc.: 69.53%] [G loss: 1.329276]\n",
      "##############\n",
      "[2.68441485 2.00502938 2.04640343 2.6807473  0.81235397 6.21853107\n",
      " 2.29116983 2.46049621 3.86181242 8.14868929]\n",
      "##########\n",
      "epoch:25 step:24201 [D loss: 0.636822, acc.: 71.09%] [G loss: 1.608248]\n",
      "epoch:25 step:24202 [D loss: 0.578502, acc.: 67.19%] [G loss: 1.833246]\n",
      "epoch:25 step:24203 [D loss: 0.772946, acc.: 53.91%] [G loss: 1.145386]\n",
      "epoch:25 step:24204 [D loss: 0.594450, acc.: 68.75%] [G loss: 1.321096]\n",
      "epoch:25 step:24205 [D loss: 0.465754, acc.: 78.91%] [G loss: 1.568475]\n",
      "epoch:25 step:24206 [D loss: 0.617372, acc.: 64.06%] [G loss: 1.252385]\n",
      "epoch:25 step:24207 [D loss: 0.406324, acc.: 86.72%] [G loss: 1.353077]\n",
      "epoch:25 step:24208 [D loss: 0.444437, acc.: 83.59%] [G loss: 1.238605]\n",
      "epoch:25 step:24209 [D loss: 0.443345, acc.: 79.69%] [G loss: 1.586100]\n",
      "epoch:25 step:24210 [D loss: 0.459888, acc.: 78.91%] [G loss: 1.513918]\n",
      "epoch:25 step:24211 [D loss: 0.635694, acc.: 67.97%] [G loss: 1.799366]\n",
      "epoch:25 step:24212 [D loss: 0.634797, acc.: 66.41%] [G loss: 1.311218]\n",
      "epoch:25 step:24213 [D loss: 0.572873, acc.: 68.75%] [G loss: 1.478108]\n",
      "epoch:25 step:24214 [D loss: 0.540727, acc.: 71.88%] [G loss: 1.230992]\n",
      "epoch:25 step:24215 [D loss: 0.592463, acc.: 68.75%] [G loss: 1.494537]\n",
      "epoch:25 step:24216 [D loss: 0.496700, acc.: 75.78%] [G loss: 1.021682]\n",
      "epoch:25 step:24217 [D loss: 0.652777, acc.: 62.50%] [G loss: 0.983773]\n",
      "epoch:25 step:24218 [D loss: 0.558767, acc.: 70.31%] [G loss: 1.794400]\n",
      "epoch:25 step:24219 [D loss: 0.563550, acc.: 72.66%] [G loss: 1.395833]\n",
      "epoch:25 step:24220 [D loss: 0.598224, acc.: 68.75%] [G loss: 1.660763]\n",
      "epoch:25 step:24221 [D loss: 0.686819, acc.: 64.06%] [G loss: 1.055419]\n",
      "epoch:25 step:24222 [D loss: 0.451470, acc.: 78.12%] [G loss: 1.466984]\n",
      "epoch:25 step:24223 [D loss: 0.721383, acc.: 57.81%] [G loss: 0.771267]\n",
      "epoch:25 step:24224 [D loss: 0.645649, acc.: 64.84%] [G loss: 1.654780]\n",
      "epoch:25 step:24225 [D loss: 0.628786, acc.: 64.84%] [G loss: 1.324136]\n",
      "epoch:25 step:24226 [D loss: 0.563996, acc.: 67.97%] [G loss: 1.401459]\n",
      "epoch:25 step:24227 [D loss: 0.528907, acc.: 74.22%] [G loss: 1.680964]\n",
      "epoch:25 step:24228 [D loss: 0.737216, acc.: 56.25%] [G loss: 1.100159]\n",
      "epoch:25 step:24229 [D loss: 0.449791, acc.: 83.59%] [G loss: 1.395961]\n",
      "epoch:25 step:24230 [D loss: 0.432375, acc.: 85.16%] [G loss: 1.230214]\n",
      "epoch:25 step:24231 [D loss: 0.438173, acc.: 76.56%] [G loss: 1.425306]\n",
      "epoch:25 step:24232 [D loss: 0.607684, acc.: 63.28%] [G loss: 1.278551]\n",
      "epoch:25 step:24233 [D loss: 0.517418, acc.: 76.56%] [G loss: 1.419887]\n",
      "epoch:25 step:24234 [D loss: 0.454441, acc.: 77.34%] [G loss: 1.806802]\n",
      "epoch:25 step:24235 [D loss: 0.457646, acc.: 82.81%] [G loss: 1.505515]\n",
      "epoch:25 step:24236 [D loss: 0.554252, acc.: 71.88%] [G loss: 1.238024]\n",
      "epoch:25 step:24237 [D loss: 0.514101, acc.: 71.88%] [G loss: 1.313832]\n",
      "epoch:25 step:24238 [D loss: 0.491698, acc.: 76.56%] [G loss: 1.131320]\n",
      "epoch:25 step:24239 [D loss: 0.376306, acc.: 88.28%] [G loss: 1.586388]\n",
      "epoch:25 step:24240 [D loss: 0.655322, acc.: 60.16%] [G loss: 1.728515]\n",
      "epoch:25 step:24241 [D loss: 0.614972, acc.: 69.53%] [G loss: 1.224259]\n",
      "epoch:25 step:24242 [D loss: 0.452772, acc.: 78.12%] [G loss: 1.728717]\n",
      "epoch:25 step:24243 [D loss: 0.563033, acc.: 69.53%] [G loss: 0.918804]\n",
      "epoch:25 step:24244 [D loss: 0.521546, acc.: 69.53%] [G loss: 1.195065]\n",
      "epoch:25 step:24245 [D loss: 0.589245, acc.: 65.62%] [G loss: 1.429409]\n",
      "epoch:25 step:24246 [D loss: 0.509692, acc.: 75.78%] [G loss: 1.558883]\n",
      "epoch:25 step:24247 [D loss: 0.573944, acc.: 67.19%] [G loss: 0.791907]\n",
      "epoch:25 step:24248 [D loss: 0.605815, acc.: 64.84%] [G loss: 1.163440]\n",
      "epoch:25 step:24249 [D loss: 0.412471, acc.: 80.47%] [G loss: 1.582083]\n",
      "epoch:25 step:24250 [D loss: 0.537377, acc.: 75.78%] [G loss: 1.423249]\n",
      "epoch:25 step:24251 [D loss: 0.418034, acc.: 83.59%] [G loss: 1.990585]\n",
      "epoch:25 step:24252 [D loss: 0.451319, acc.: 84.38%] [G loss: 1.573635]\n",
      "epoch:25 step:24253 [D loss: 0.861230, acc.: 50.78%] [G loss: 0.990458]\n",
      "epoch:25 step:24254 [D loss: 0.579796, acc.: 66.41%] [G loss: 1.071365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24255 [D loss: 0.427396, acc.: 83.59%] [G loss: 1.434136]\n",
      "epoch:25 step:24256 [D loss: 0.649922, acc.: 66.41%] [G loss: 1.518227]\n",
      "epoch:25 step:24257 [D loss: 0.392177, acc.: 84.38%] [G loss: 1.776191]\n",
      "epoch:25 step:24258 [D loss: 0.624605, acc.: 66.41%] [G loss: 1.328490]\n",
      "epoch:25 step:24259 [D loss: 0.606805, acc.: 68.75%] [G loss: 0.934579]\n",
      "epoch:25 step:24260 [D loss: 0.575806, acc.: 70.31%] [G loss: 1.400182]\n",
      "epoch:25 step:24261 [D loss: 0.575814, acc.: 68.75%] [G loss: 1.315889]\n",
      "epoch:25 step:24262 [D loss: 0.598567, acc.: 66.41%] [G loss: 1.397646]\n",
      "epoch:25 step:24263 [D loss: 0.482423, acc.: 78.12%] [G loss: 1.518477]\n",
      "epoch:25 step:24264 [D loss: 0.565157, acc.: 73.44%] [G loss: 1.531055]\n",
      "epoch:25 step:24265 [D loss: 0.526883, acc.: 78.12%] [G loss: 1.325889]\n",
      "epoch:25 step:24266 [D loss: 0.557005, acc.: 74.22%] [G loss: 1.643248]\n",
      "epoch:25 step:24267 [D loss: 0.596126, acc.: 69.53%] [G loss: 1.267486]\n",
      "epoch:25 step:24268 [D loss: 0.617351, acc.: 64.84%] [G loss: 1.531532]\n",
      "epoch:25 step:24269 [D loss: 0.517001, acc.: 75.00%] [G loss: 1.019762]\n",
      "epoch:25 step:24270 [D loss: 0.743533, acc.: 58.59%] [G loss: 1.113611]\n",
      "epoch:25 step:24271 [D loss: 0.509683, acc.: 80.47%] [G loss: 1.557643]\n",
      "epoch:25 step:24272 [D loss: 0.560214, acc.: 70.31%] [G loss: 0.970729]\n",
      "epoch:25 step:24273 [D loss: 0.746287, acc.: 59.38%] [G loss: 1.468685]\n",
      "epoch:25 step:24274 [D loss: 0.450117, acc.: 81.25%] [G loss: 1.634013]\n",
      "epoch:25 step:24275 [D loss: 0.560449, acc.: 69.53%] [G loss: 1.218379]\n",
      "epoch:25 step:24276 [D loss: 0.485986, acc.: 77.34%] [G loss: 1.463494]\n",
      "epoch:25 step:24277 [D loss: 0.515699, acc.: 75.00%] [G loss: 1.401864]\n",
      "epoch:25 step:24278 [D loss: 0.561670, acc.: 71.09%] [G loss: 1.485339]\n",
      "epoch:25 step:24279 [D loss: 0.641755, acc.: 64.84%] [G loss: 1.891679]\n",
      "epoch:25 step:24280 [D loss: 0.635249, acc.: 66.41%] [G loss: 1.477148]\n",
      "epoch:25 step:24281 [D loss: 0.588250, acc.: 71.88%] [G loss: 1.521940]\n",
      "epoch:25 step:24282 [D loss: 0.605734, acc.: 67.19%] [G loss: 1.529782]\n",
      "epoch:25 step:24283 [D loss: 0.439328, acc.: 79.69%] [G loss: 1.220303]\n",
      "epoch:25 step:24284 [D loss: 0.607152, acc.: 65.62%] [G loss: 1.583165]\n",
      "epoch:25 step:24285 [D loss: 0.514573, acc.: 78.12%] [G loss: 1.038746]\n",
      "epoch:25 step:24286 [D loss: 0.643361, acc.: 65.62%] [G loss: 1.191085]\n",
      "epoch:25 step:24287 [D loss: 0.570241, acc.: 68.75%] [G loss: 1.263603]\n",
      "epoch:25 step:24288 [D loss: 0.448755, acc.: 85.94%] [G loss: 1.318081]\n",
      "epoch:25 step:24289 [D loss: 0.685803, acc.: 58.59%] [G loss: 1.849310]\n",
      "epoch:25 step:24290 [D loss: 0.493129, acc.: 75.78%] [G loss: 1.631646]\n",
      "epoch:25 step:24291 [D loss: 0.639714, acc.: 60.94%] [G loss: 1.470765]\n",
      "epoch:25 step:24292 [D loss: 0.555350, acc.: 71.09%] [G loss: 1.295681]\n",
      "epoch:25 step:24293 [D loss: 0.578830, acc.: 69.53%] [G loss: 1.113215]\n",
      "epoch:25 step:24294 [D loss: 0.534512, acc.: 71.88%] [G loss: 1.610179]\n",
      "epoch:25 step:24295 [D loss: 0.470818, acc.: 80.47%] [G loss: 1.532055]\n",
      "epoch:25 step:24296 [D loss: 0.852840, acc.: 46.88%] [G loss: 1.338617]\n",
      "epoch:25 step:24297 [D loss: 0.461799, acc.: 80.47%] [G loss: 1.344086]\n",
      "epoch:25 step:24298 [D loss: 0.473084, acc.: 78.12%] [G loss: 1.459001]\n",
      "epoch:25 step:24299 [D loss: 0.496319, acc.: 79.69%] [G loss: 1.526055]\n",
      "epoch:25 step:24300 [D loss: 0.517162, acc.: 78.12%] [G loss: 1.225589]\n",
      "epoch:25 step:24301 [D loss: 0.569797, acc.: 71.09%] [G loss: 1.945894]\n",
      "epoch:25 step:24302 [D loss: 0.615502, acc.: 64.84%] [G loss: 1.476692]\n",
      "epoch:25 step:24303 [D loss: 0.527521, acc.: 78.12%] [G loss: 1.776015]\n",
      "epoch:25 step:24304 [D loss: 0.645343, acc.: 61.72%] [G loss: 1.256243]\n",
      "epoch:25 step:24305 [D loss: 0.572320, acc.: 73.44%] [G loss: 1.808940]\n",
      "epoch:25 step:24306 [D loss: 0.708322, acc.: 55.47%] [G loss: 1.311312]\n",
      "epoch:25 step:24307 [D loss: 0.468859, acc.: 75.00%] [G loss: 1.513877]\n",
      "epoch:25 step:24308 [D loss: 0.578849, acc.: 70.31%] [G loss: 1.543624]\n",
      "epoch:25 step:24309 [D loss: 0.607494, acc.: 65.62%] [G loss: 1.491225]\n",
      "epoch:25 step:24310 [D loss: 0.742219, acc.: 60.16%] [G loss: 1.493820]\n",
      "epoch:25 step:24311 [D loss: 0.436072, acc.: 80.47%] [G loss: 1.638217]\n",
      "epoch:25 step:24312 [D loss: 0.565017, acc.: 76.56%] [G loss: 1.499404]\n",
      "epoch:25 step:24313 [D loss: 0.569331, acc.: 74.22%] [G loss: 1.249630]\n",
      "epoch:25 step:24314 [D loss: 0.756985, acc.: 54.69%] [G loss: 1.237479]\n",
      "epoch:25 step:24315 [D loss: 0.732808, acc.: 62.50%] [G loss: 1.234846]\n",
      "epoch:25 step:24316 [D loss: 0.542171, acc.: 71.09%] [G loss: 1.632660]\n",
      "epoch:25 step:24317 [D loss: 0.560481, acc.: 67.19%] [G loss: 1.377773]\n",
      "epoch:25 step:24318 [D loss: 0.501042, acc.: 75.00%] [G loss: 1.449852]\n",
      "epoch:25 step:24319 [D loss: 0.570529, acc.: 69.53%] [G loss: 1.271976]\n",
      "epoch:25 step:24320 [D loss: 0.389914, acc.: 83.59%] [G loss: 1.517563]\n",
      "epoch:25 step:24321 [D loss: 0.439742, acc.: 79.69%] [G loss: 1.696658]\n",
      "epoch:25 step:24322 [D loss: 0.551262, acc.: 69.53%] [G loss: 1.416155]\n",
      "epoch:25 step:24323 [D loss: 0.463686, acc.: 78.91%] [G loss: 1.414980]\n",
      "epoch:25 step:24324 [D loss: 0.682676, acc.: 66.41%] [G loss: 1.262311]\n",
      "epoch:25 step:24325 [D loss: 0.517815, acc.: 72.66%] [G loss: 1.224395]\n",
      "epoch:25 step:24326 [D loss: 0.565863, acc.: 70.31%] [G loss: 1.567720]\n",
      "epoch:25 step:24327 [D loss: 0.522647, acc.: 73.44%] [G loss: 1.505719]\n",
      "epoch:25 step:24328 [D loss: 0.619163, acc.: 61.72%] [G loss: 1.339663]\n",
      "epoch:25 step:24329 [D loss: 0.497088, acc.: 80.47%] [G loss: 1.930872]\n",
      "epoch:25 step:24330 [D loss: 0.748035, acc.: 56.25%] [G loss: 1.234348]\n",
      "epoch:25 step:24331 [D loss: 0.532948, acc.: 71.09%] [G loss: 1.129347]\n",
      "epoch:25 step:24332 [D loss: 0.483021, acc.: 79.69%] [G loss: 1.570817]\n",
      "epoch:25 step:24333 [D loss: 0.725588, acc.: 60.16%] [G loss: 1.147899]\n",
      "epoch:25 step:24334 [D loss: 0.577280, acc.: 73.44%] [G loss: 1.347245]\n",
      "epoch:25 step:24335 [D loss: 0.686958, acc.: 60.94%] [G loss: 1.209514]\n",
      "epoch:25 step:24336 [D loss: 0.449337, acc.: 81.25%] [G loss: 1.542659]\n",
      "epoch:25 step:24337 [D loss: 0.530786, acc.: 72.66%] [G loss: 1.627222]\n",
      "epoch:25 step:24338 [D loss: 0.670633, acc.: 64.84%] [G loss: 1.493507]\n",
      "epoch:25 step:24339 [D loss: 0.607382, acc.: 66.41%] [G loss: 1.540577]\n",
      "epoch:25 step:24340 [D loss: 0.505944, acc.: 73.44%] [G loss: 1.205131]\n",
      "epoch:25 step:24341 [D loss: 0.673426, acc.: 61.72%] [G loss: 0.766985]\n",
      "epoch:25 step:24342 [D loss: 0.465665, acc.: 77.34%] [G loss: 1.569989]\n",
      "epoch:25 step:24343 [D loss: 0.464887, acc.: 83.59%] [G loss: 1.123130]\n",
      "epoch:25 step:24344 [D loss: 0.484924, acc.: 75.78%] [G loss: 1.202300]\n",
      "epoch:25 step:24345 [D loss: 0.484442, acc.: 77.34%] [G loss: 1.503399]\n",
      "epoch:25 step:24346 [D loss: 0.598010, acc.: 66.41%] [G loss: 1.606913]\n",
      "epoch:25 step:24347 [D loss: 0.366738, acc.: 91.41%] [G loss: 1.768543]\n",
      "epoch:25 step:24348 [D loss: 0.493846, acc.: 73.44%] [G loss: 1.457534]\n",
      "epoch:25 step:24349 [D loss: 0.589024, acc.: 67.19%] [G loss: 1.018683]\n",
      "epoch:25 step:24350 [D loss: 0.470849, acc.: 79.69%] [G loss: 1.672975]\n",
      "epoch:25 step:24351 [D loss: 0.636894, acc.: 60.94%] [G loss: 1.630609]\n",
      "epoch:25 step:24352 [D loss: 0.640307, acc.: 62.50%] [G loss: 1.225269]\n",
      "epoch:25 step:24353 [D loss: 0.405093, acc.: 82.03%] [G loss: 1.549288]\n",
      "epoch:25 step:24354 [D loss: 0.477520, acc.: 76.56%] [G loss: 1.801466]\n",
      "epoch:25 step:24355 [D loss: 0.753570, acc.: 53.91%] [G loss: 1.215651]\n",
      "epoch:25 step:24356 [D loss: 0.596374, acc.: 69.53%] [G loss: 1.176240]\n",
      "epoch:25 step:24357 [D loss: 0.595520, acc.: 70.31%] [G loss: 1.170587]\n",
      "epoch:25 step:24358 [D loss: 0.599917, acc.: 68.75%] [G loss: 1.460439]\n",
      "epoch:25 step:24359 [D loss: 0.537833, acc.: 74.22%] [G loss: 1.773535]\n",
      "epoch:25 step:24360 [D loss: 0.458183, acc.: 82.03%] [G loss: 1.264402]\n",
      "epoch:25 step:24361 [D loss: 0.420956, acc.: 83.59%] [G loss: 1.453862]\n",
      "epoch:25 step:24362 [D loss: 0.702947, acc.: 57.81%] [G loss: 1.195915]\n",
      "epoch:26 step:24363 [D loss: 0.599800, acc.: 64.06%] [G loss: 1.346827]\n",
      "epoch:26 step:24364 [D loss: 0.587001, acc.: 64.84%] [G loss: 1.592019]\n",
      "epoch:26 step:24365 [D loss: 0.603740, acc.: 63.28%] [G loss: 1.382619]\n",
      "epoch:26 step:24366 [D loss: 0.641668, acc.: 64.06%] [G loss: 1.513753]\n",
      "epoch:26 step:24367 [D loss: 0.707487, acc.: 58.59%] [G loss: 1.505624]\n",
      "epoch:26 step:24368 [D loss: 0.597136, acc.: 71.88%] [G loss: 1.717102]\n",
      "epoch:26 step:24369 [D loss: 0.773571, acc.: 53.91%] [G loss: 0.985517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24370 [D loss: 0.449383, acc.: 82.81%] [G loss: 2.090913]\n",
      "epoch:26 step:24371 [D loss: 0.613318, acc.: 70.31%] [G loss: 1.471082]\n",
      "epoch:26 step:24372 [D loss: 0.668297, acc.: 63.28%] [G loss: 1.141305]\n",
      "epoch:26 step:24373 [D loss: 0.664784, acc.: 61.72%] [G loss: 1.576605]\n",
      "epoch:26 step:24374 [D loss: 0.433576, acc.: 78.91%] [G loss: 1.650796]\n",
      "epoch:26 step:24375 [D loss: 0.607074, acc.: 70.31%] [G loss: 1.630059]\n",
      "epoch:26 step:24376 [D loss: 0.508424, acc.: 75.78%] [G loss: 1.739665]\n",
      "epoch:26 step:24377 [D loss: 0.395150, acc.: 85.16%] [G loss: 1.205423]\n",
      "epoch:26 step:24378 [D loss: 0.553625, acc.: 75.00%] [G loss: 1.198107]\n",
      "epoch:26 step:24379 [D loss: 0.490102, acc.: 75.00%] [G loss: 1.291999]\n",
      "epoch:26 step:24380 [D loss: 0.529195, acc.: 76.56%] [G loss: 0.981057]\n",
      "epoch:26 step:24381 [D loss: 0.845672, acc.: 51.56%] [G loss: 0.877549]\n",
      "epoch:26 step:24382 [D loss: 0.537957, acc.: 72.66%] [G loss: 1.344449]\n",
      "epoch:26 step:24383 [D loss: 0.709940, acc.: 60.16%] [G loss: 1.173848]\n",
      "epoch:26 step:24384 [D loss: 0.430250, acc.: 82.03%] [G loss: 1.269287]\n",
      "epoch:26 step:24385 [D loss: 0.594691, acc.: 64.06%] [G loss: 1.439507]\n",
      "epoch:26 step:24386 [D loss: 0.443955, acc.: 83.59%] [G loss: 1.520945]\n",
      "epoch:26 step:24387 [D loss: 0.968462, acc.: 40.62%] [G loss: 1.034205]\n",
      "epoch:26 step:24388 [D loss: 0.586630, acc.: 68.75%] [G loss: 1.483639]\n",
      "epoch:26 step:24389 [D loss: 0.531425, acc.: 73.44%] [G loss: 1.366417]\n",
      "epoch:26 step:24390 [D loss: 0.606116, acc.: 71.09%] [G loss: 1.347055]\n",
      "epoch:26 step:24391 [D loss: 0.523273, acc.: 75.00%] [G loss: 1.917502]\n",
      "epoch:26 step:24392 [D loss: 0.678564, acc.: 57.81%] [G loss: 1.470638]\n",
      "epoch:26 step:24393 [D loss: 0.474115, acc.: 78.91%] [G loss: 1.647276]\n",
      "epoch:26 step:24394 [D loss: 0.534280, acc.: 73.44%] [G loss: 1.513707]\n",
      "epoch:26 step:24395 [D loss: 0.532995, acc.: 71.09%] [G loss: 1.534333]\n",
      "epoch:26 step:24396 [D loss: 0.500427, acc.: 78.12%] [G loss: 1.632980]\n",
      "epoch:26 step:24397 [D loss: 0.446916, acc.: 81.25%] [G loss: 1.592158]\n",
      "epoch:26 step:24398 [D loss: 0.568608, acc.: 70.31%] [G loss: 1.388126]\n",
      "epoch:26 step:24399 [D loss: 0.428601, acc.: 84.38%] [G loss: 1.970301]\n",
      "epoch:26 step:24400 [D loss: 0.514887, acc.: 73.44%] [G loss: 1.747181]\n",
      "##############\n",
      "[2.78130138 1.9411225  1.79668627 3.09906972 0.82813082 6.10000504\n",
      " 2.23226675 2.90346316 4.00660766 7.14868929]\n",
      "##########\n",
      "epoch:26 step:24401 [D loss: 0.552853, acc.: 70.31%] [G loss: 1.260895]\n",
      "epoch:26 step:24402 [D loss: 0.675710, acc.: 60.94%] [G loss: 1.368821]\n",
      "epoch:26 step:24403 [D loss: 0.417970, acc.: 82.81%] [G loss: 1.339858]\n",
      "epoch:26 step:24404 [D loss: 0.620439, acc.: 63.28%] [G loss: 1.389460]\n",
      "epoch:26 step:24405 [D loss: 0.552860, acc.: 70.31%] [G loss: 1.714288]\n",
      "epoch:26 step:24406 [D loss: 0.522742, acc.: 73.44%] [G loss: 1.765844]\n",
      "epoch:26 step:24407 [D loss: 0.404284, acc.: 85.94%] [G loss: 1.489706]\n",
      "epoch:26 step:24408 [D loss: 0.629809, acc.: 64.06%] [G loss: 1.207672]\n",
      "epoch:26 step:24409 [D loss: 0.481653, acc.: 76.56%] [G loss: 1.480505]\n",
      "epoch:26 step:24410 [D loss: 0.542650, acc.: 69.53%] [G loss: 1.508714]\n",
      "epoch:26 step:24411 [D loss: 0.487538, acc.: 74.22%] [G loss: 1.315206]\n",
      "epoch:26 step:24412 [D loss: 0.507174, acc.: 75.78%] [G loss: 1.430355]\n",
      "epoch:26 step:24413 [D loss: 0.647954, acc.: 62.50%] [G loss: 1.621739]\n",
      "epoch:26 step:24414 [D loss: 0.654744, acc.: 62.50%] [G loss: 1.701720]\n",
      "epoch:26 step:24415 [D loss: 0.430181, acc.: 81.25%] [G loss: 1.522199]\n",
      "epoch:26 step:24416 [D loss: 0.550172, acc.: 72.66%] [G loss: 1.381582]\n",
      "epoch:26 step:24417 [D loss: 0.642478, acc.: 60.94%] [G loss: 0.987125]\n",
      "epoch:26 step:24418 [D loss: 0.410396, acc.: 85.16%] [G loss: 1.604323]\n",
      "epoch:26 step:24419 [D loss: 0.640411, acc.: 68.75%] [G loss: 1.695998]\n",
      "epoch:26 step:24420 [D loss: 0.607597, acc.: 67.97%] [G loss: 1.483003]\n",
      "epoch:26 step:24421 [D loss: 0.584993, acc.: 71.09%] [G loss: 1.814631]\n",
      "epoch:26 step:24422 [D loss: 0.637467, acc.: 64.84%] [G loss: 1.163247]\n",
      "epoch:26 step:24423 [D loss: 0.809145, acc.: 51.56%] [G loss: 0.869366]\n",
      "epoch:26 step:24424 [D loss: 0.531017, acc.: 76.56%] [G loss: 1.385194]\n",
      "epoch:26 step:24425 [D loss: 0.627928, acc.: 66.41%] [G loss: 1.202523]\n",
      "epoch:26 step:24426 [D loss: 0.582718, acc.: 74.22%] [G loss: 1.633227]\n",
      "epoch:26 step:24427 [D loss: 0.546383, acc.: 72.66%] [G loss: 1.165450]\n",
      "epoch:26 step:24428 [D loss: 0.678629, acc.: 63.28%] [G loss: 1.486413]\n",
      "epoch:26 step:24429 [D loss: 0.482572, acc.: 76.56%] [G loss: 1.803256]\n",
      "epoch:26 step:24430 [D loss: 0.642636, acc.: 64.06%] [G loss: 1.467358]\n",
      "epoch:26 step:24431 [D loss: 0.605656, acc.: 67.97%] [G loss: 1.294968]\n",
      "epoch:26 step:24432 [D loss: 0.695361, acc.: 59.38%] [G loss: 1.185809]\n",
      "epoch:26 step:24433 [D loss: 0.642420, acc.: 66.41%] [G loss: 1.916515]\n",
      "epoch:26 step:24434 [D loss: 0.580331, acc.: 68.75%] [G loss: 1.397329]\n",
      "epoch:26 step:24435 [D loss: 0.546951, acc.: 74.22%] [G loss: 1.239794]\n",
      "epoch:26 step:24436 [D loss: 0.514395, acc.: 74.22%] [G loss: 1.287948]\n",
      "epoch:26 step:24437 [D loss: 0.512480, acc.: 80.47%] [G loss: 1.396620]\n",
      "epoch:26 step:24438 [D loss: 0.562829, acc.: 70.31%] [G loss: 1.517899]\n",
      "epoch:26 step:24439 [D loss: 0.641682, acc.: 65.62%] [G loss: 1.180308]\n",
      "epoch:26 step:24440 [D loss: 0.536837, acc.: 75.00%] [G loss: 1.349457]\n",
      "epoch:26 step:24441 [D loss: 0.629617, acc.: 64.06%] [G loss: 1.281970]\n",
      "epoch:26 step:24442 [D loss: 0.461189, acc.: 80.47%] [G loss: 1.253202]\n",
      "epoch:26 step:24443 [D loss: 0.643932, acc.: 61.72%] [G loss: 1.178323]\n",
      "epoch:26 step:24444 [D loss: 0.664424, acc.: 64.06%] [G loss: 1.501686]\n",
      "epoch:26 step:24445 [D loss: 0.581139, acc.: 67.97%] [G loss: 1.458915]\n",
      "epoch:26 step:24446 [D loss: 0.738809, acc.: 51.56%] [G loss: 1.373158]\n",
      "epoch:26 step:24447 [D loss: 0.599174, acc.: 67.97%] [G loss: 0.962232]\n",
      "epoch:26 step:24448 [D loss: 0.511807, acc.: 78.12%] [G loss: 1.296755]\n",
      "epoch:26 step:24449 [D loss: 0.427967, acc.: 78.91%] [G loss: 1.536097]\n",
      "epoch:26 step:24450 [D loss: 0.585041, acc.: 67.97%] [G loss: 1.481765]\n",
      "epoch:26 step:24451 [D loss: 0.627042, acc.: 62.50%] [G loss: 1.868207]\n",
      "epoch:26 step:24452 [D loss: 0.639624, acc.: 64.84%] [G loss: 1.249429]\n",
      "epoch:26 step:24453 [D loss: 0.692201, acc.: 60.16%] [G loss: 1.378852]\n",
      "epoch:26 step:24454 [D loss: 0.650384, acc.: 64.06%] [G loss: 1.576116]\n",
      "epoch:26 step:24455 [D loss: 0.636408, acc.: 64.84%] [G loss: 1.520437]\n",
      "epoch:26 step:24456 [D loss: 0.599294, acc.: 67.19%] [G loss: 1.430062]\n",
      "epoch:26 step:24457 [D loss: 0.480818, acc.: 82.81%] [G loss: 1.335154]\n",
      "epoch:26 step:24458 [D loss: 0.454886, acc.: 82.81%] [G loss: 1.406244]\n",
      "epoch:26 step:24459 [D loss: 0.679607, acc.: 67.19%] [G loss: 1.084490]\n",
      "epoch:26 step:24460 [D loss: 0.392884, acc.: 85.16%] [G loss: 1.604616]\n",
      "epoch:26 step:24461 [D loss: 0.480469, acc.: 78.12%] [G loss: 1.617021]\n",
      "epoch:26 step:24462 [D loss: 0.605131, acc.: 61.72%] [G loss: 1.629902]\n",
      "epoch:26 step:24463 [D loss: 0.380666, acc.: 85.16%] [G loss: 1.587265]\n",
      "epoch:26 step:24464 [D loss: 0.732617, acc.: 53.12%] [G loss: 1.201150]\n",
      "epoch:26 step:24465 [D loss: 0.579037, acc.: 71.88%] [G loss: 1.638666]\n",
      "epoch:26 step:24466 [D loss: 0.514727, acc.: 77.34%] [G loss: 1.393273]\n",
      "epoch:26 step:24467 [D loss: 0.450919, acc.: 83.59%] [G loss: 1.790273]\n",
      "epoch:26 step:24468 [D loss: 0.652603, acc.: 62.50%] [G loss: 1.790171]\n",
      "epoch:26 step:24469 [D loss: 0.464032, acc.: 81.25%] [G loss: 1.444760]\n",
      "epoch:26 step:24470 [D loss: 0.622199, acc.: 67.97%] [G loss: 1.275446]\n",
      "epoch:26 step:24471 [D loss: 0.525693, acc.: 75.00%] [G loss: 1.192881]\n",
      "epoch:26 step:24472 [D loss: 0.570417, acc.: 69.53%] [G loss: 1.601650]\n",
      "epoch:26 step:24473 [D loss: 0.539717, acc.: 75.78%] [G loss: 1.121526]\n",
      "epoch:26 step:24474 [D loss: 0.521838, acc.: 75.00%] [G loss: 1.148329]\n",
      "epoch:26 step:24475 [D loss: 0.637274, acc.: 64.06%] [G loss: 1.107165]\n",
      "epoch:26 step:24476 [D loss: 0.604257, acc.: 68.75%] [G loss: 1.254853]\n",
      "epoch:26 step:24477 [D loss: 0.660390, acc.: 66.41%] [G loss: 1.626188]\n",
      "epoch:26 step:24478 [D loss: 0.598637, acc.: 67.19%] [G loss: 1.637460]\n",
      "epoch:26 step:24479 [D loss: 0.528931, acc.: 74.22%] [G loss: 1.425992]\n",
      "epoch:26 step:24480 [D loss: 0.499216, acc.: 76.56%] [G loss: 1.638524]\n",
      "epoch:26 step:24481 [D loss: 0.472544, acc.: 79.69%] [G loss: 1.472980]\n",
      "epoch:26 step:24482 [D loss: 0.775594, acc.: 54.69%] [G loss: 1.669918]\n",
      "epoch:26 step:24483 [D loss: 0.470601, acc.: 79.69%] [G loss: 1.233682]\n",
      "epoch:26 step:24484 [D loss: 0.541192, acc.: 70.31%] [G loss: 1.538600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24485 [D loss: 0.545484, acc.: 74.22%] [G loss: 1.277646]\n",
      "epoch:26 step:24486 [D loss: 0.544068, acc.: 75.00%] [G loss: 1.600199]\n",
      "epoch:26 step:24487 [D loss: 0.631217, acc.: 62.50%] [G loss: 1.621099]\n",
      "epoch:26 step:24488 [D loss: 0.433192, acc.: 82.81%] [G loss: 1.205294]\n",
      "epoch:26 step:24489 [D loss: 0.523853, acc.: 76.56%] [G loss: 1.402031]\n",
      "epoch:26 step:24490 [D loss: 0.547491, acc.: 74.22%] [G loss: 1.328341]\n",
      "epoch:26 step:24491 [D loss: 0.519208, acc.: 78.91%] [G loss: 1.538712]\n",
      "epoch:26 step:24492 [D loss: 0.525799, acc.: 74.22%] [G loss: 1.311778]\n",
      "epoch:26 step:24493 [D loss: 0.420101, acc.: 82.81%] [G loss: 1.646433]\n",
      "epoch:26 step:24494 [D loss: 0.480726, acc.: 78.91%] [G loss: 1.691840]\n",
      "epoch:26 step:24495 [D loss: 0.619204, acc.: 62.50%] [G loss: 1.475012]\n",
      "epoch:26 step:24496 [D loss: 0.644240, acc.: 67.19%] [G loss: 1.565149]\n",
      "epoch:26 step:24497 [D loss: 0.653311, acc.: 64.84%] [G loss: 1.488661]\n",
      "epoch:26 step:24498 [D loss: 0.590248, acc.: 68.75%] [G loss: 1.548470]\n",
      "epoch:26 step:24499 [D loss: 0.512716, acc.: 73.44%] [G loss: 1.373952]\n",
      "epoch:26 step:24500 [D loss: 0.517405, acc.: 76.56%] [G loss: 1.328047]\n",
      "epoch:26 step:24501 [D loss: 0.638943, acc.: 61.72%] [G loss: 1.055129]\n",
      "epoch:26 step:24502 [D loss: 0.472856, acc.: 78.12%] [G loss: 1.501093]\n",
      "epoch:26 step:24503 [D loss: 0.633395, acc.: 66.41%] [G loss: 1.195224]\n",
      "epoch:26 step:24504 [D loss: 0.497579, acc.: 77.34%] [G loss: 1.088561]\n",
      "epoch:26 step:24505 [D loss: 0.601260, acc.: 69.53%] [G loss: 1.520980]\n",
      "epoch:26 step:24506 [D loss: 0.599416, acc.: 70.31%] [G loss: 1.313534]\n",
      "epoch:26 step:24507 [D loss: 0.384804, acc.: 83.59%] [G loss: 1.351407]\n",
      "epoch:26 step:24508 [D loss: 0.665323, acc.: 61.72%] [G loss: 1.588469]\n",
      "epoch:26 step:24509 [D loss: 0.639707, acc.: 68.75%] [G loss: 1.337617]\n",
      "epoch:26 step:24510 [D loss: 0.500503, acc.: 76.56%] [G loss: 1.301311]\n",
      "epoch:26 step:24511 [D loss: 0.640124, acc.: 63.28%] [G loss: 1.323514]\n",
      "epoch:26 step:24512 [D loss: 0.688922, acc.: 64.06%] [G loss: 1.307577]\n",
      "epoch:26 step:24513 [D loss: 0.493321, acc.: 72.66%] [G loss: 1.339332]\n",
      "epoch:26 step:24514 [D loss: 0.532101, acc.: 75.00%] [G loss: 1.174311]\n",
      "epoch:26 step:24515 [D loss: 0.443282, acc.: 81.25%] [G loss: 1.353874]\n",
      "epoch:26 step:24516 [D loss: 0.617190, acc.: 69.53%] [G loss: 1.090075]\n",
      "epoch:26 step:24517 [D loss: 0.636207, acc.: 59.38%] [G loss: 1.274667]\n",
      "epoch:26 step:24518 [D loss: 0.695568, acc.: 60.16%] [G loss: 1.343454]\n",
      "epoch:26 step:24519 [D loss: 0.675126, acc.: 60.16%] [G loss: 1.737943]\n",
      "epoch:26 step:24520 [D loss: 0.439236, acc.: 82.81%] [G loss: 1.873621]\n",
      "epoch:26 step:24521 [D loss: 0.451459, acc.: 83.59%] [G loss: 1.623570]\n",
      "epoch:26 step:24522 [D loss: 0.645545, acc.: 59.38%] [G loss: 1.283383]\n",
      "epoch:26 step:24523 [D loss: 0.523846, acc.: 78.12%] [G loss: 1.442550]\n",
      "epoch:26 step:24524 [D loss: 0.512888, acc.: 76.56%] [G loss: 1.433705]\n",
      "epoch:26 step:24525 [D loss: 0.476344, acc.: 80.47%] [G loss: 1.382128]\n",
      "epoch:26 step:24526 [D loss: 0.486371, acc.: 73.44%] [G loss: 1.515713]\n",
      "epoch:26 step:24527 [D loss: 0.584658, acc.: 71.09%] [G loss: 1.294797]\n",
      "epoch:26 step:24528 [D loss: 0.532133, acc.: 75.00%] [G loss: 1.670092]\n",
      "epoch:26 step:24529 [D loss: 0.547576, acc.: 71.88%] [G loss: 1.143733]\n",
      "epoch:26 step:24530 [D loss: 0.620065, acc.: 65.62%] [G loss: 1.263272]\n",
      "epoch:26 step:24531 [D loss: 0.509095, acc.: 75.00%] [G loss: 1.143708]\n",
      "epoch:26 step:24532 [D loss: 0.502353, acc.: 75.78%] [G loss: 1.487491]\n",
      "epoch:26 step:24533 [D loss: 0.648737, acc.: 58.59%] [G loss: 1.663206]\n",
      "epoch:26 step:24534 [D loss: 0.401783, acc.: 86.72%] [G loss: 2.204160]\n",
      "epoch:26 step:24535 [D loss: 0.676232, acc.: 61.72%] [G loss: 1.225845]\n",
      "epoch:26 step:24536 [D loss: 0.514778, acc.: 81.25%] [G loss: 1.160862]\n",
      "epoch:26 step:24537 [D loss: 0.427834, acc.: 83.59%] [G loss: 1.567748]\n",
      "epoch:26 step:24538 [D loss: 0.584325, acc.: 67.97%] [G loss: 1.165078]\n",
      "epoch:26 step:24539 [D loss: 0.413741, acc.: 82.81%] [G loss: 1.715367]\n",
      "epoch:26 step:24540 [D loss: 0.482853, acc.: 76.56%] [G loss: 1.173037]\n",
      "epoch:26 step:24541 [D loss: 0.559388, acc.: 71.09%] [G loss: 1.096358]\n",
      "epoch:26 step:24542 [D loss: 0.517679, acc.: 69.53%] [G loss: 1.315935]\n",
      "epoch:26 step:24543 [D loss: 0.763668, acc.: 50.78%] [G loss: 1.136682]\n",
      "epoch:26 step:24544 [D loss: 0.557660, acc.: 67.97%] [G loss: 1.268347]\n",
      "epoch:26 step:24545 [D loss: 0.580812, acc.: 68.75%] [G loss: 1.912536]\n",
      "epoch:26 step:24546 [D loss: 0.626424, acc.: 67.97%] [G loss: 1.608455]\n",
      "epoch:26 step:24547 [D loss: 0.451903, acc.: 81.25%] [G loss: 1.327439]\n",
      "epoch:26 step:24548 [D loss: 0.443798, acc.: 80.47%] [G loss: 1.480374]\n",
      "epoch:26 step:24549 [D loss: 0.489695, acc.: 76.56%] [G loss: 1.357089]\n",
      "epoch:26 step:24550 [D loss: 0.526983, acc.: 78.91%] [G loss: 1.569457]\n",
      "epoch:26 step:24551 [D loss: 0.493365, acc.: 77.34%] [G loss: 1.551352]\n",
      "epoch:26 step:24552 [D loss: 0.490222, acc.: 75.78%] [G loss: 1.473896]\n",
      "epoch:26 step:24553 [D loss: 0.709646, acc.: 61.72%] [G loss: 1.261295]\n",
      "epoch:26 step:24554 [D loss: 0.489386, acc.: 75.78%] [G loss: 1.999037]\n",
      "epoch:26 step:24555 [D loss: 0.744869, acc.: 57.81%] [G loss: 1.603188]\n",
      "epoch:26 step:24556 [D loss: 0.522199, acc.: 75.78%] [G loss: 1.427117]\n",
      "epoch:26 step:24557 [D loss: 0.555188, acc.: 75.00%] [G loss: 1.074260]\n",
      "epoch:26 step:24558 [D loss: 0.562304, acc.: 71.88%] [G loss: 1.331828]\n",
      "epoch:26 step:24559 [D loss: 0.479286, acc.: 76.56%] [G loss: 1.271022]\n",
      "epoch:26 step:24560 [D loss: 0.509119, acc.: 74.22%] [G loss: 1.392011]\n",
      "epoch:26 step:24561 [D loss: 0.392244, acc.: 88.28%] [G loss: 1.496479]\n",
      "epoch:26 step:24562 [D loss: 0.574582, acc.: 69.53%] [G loss: 1.071750]\n",
      "epoch:26 step:24563 [D loss: 0.429082, acc.: 82.03%] [G loss: 1.213024]\n",
      "epoch:26 step:24564 [D loss: 0.586255, acc.: 68.75%] [G loss: 1.024133]\n",
      "epoch:26 step:24565 [D loss: 0.476629, acc.: 76.56%] [G loss: 1.369449]\n",
      "epoch:26 step:24566 [D loss: 0.365642, acc.: 87.50%] [G loss: 1.689356]\n",
      "epoch:26 step:24567 [D loss: 0.515632, acc.: 75.78%] [G loss: 1.450980]\n",
      "epoch:26 step:24568 [D loss: 0.537385, acc.: 76.56%] [G loss: 1.107675]\n",
      "epoch:26 step:24569 [D loss: 0.616025, acc.: 63.28%] [G loss: 1.708709]\n",
      "epoch:26 step:24570 [D loss: 0.370464, acc.: 88.28%] [G loss: 1.496012]\n",
      "epoch:26 step:24571 [D loss: 0.454187, acc.: 83.59%] [G loss: 1.657076]\n",
      "epoch:26 step:24572 [D loss: 0.500811, acc.: 77.34%] [G loss: 1.412125]\n",
      "epoch:26 step:24573 [D loss: 0.335931, acc.: 91.41%] [G loss: 1.636063]\n",
      "epoch:26 step:24574 [D loss: 0.638217, acc.: 62.50%] [G loss: 1.754959]\n",
      "epoch:26 step:24575 [D loss: 0.564419, acc.: 69.53%] [G loss: 1.325237]\n",
      "epoch:26 step:24576 [D loss: 0.531167, acc.: 74.22%] [G loss: 1.265935]\n",
      "epoch:26 step:24577 [D loss: 0.441402, acc.: 82.03%] [G loss: 1.786883]\n",
      "epoch:26 step:24578 [D loss: 0.674983, acc.: 57.03%] [G loss: 1.345560]\n",
      "epoch:26 step:24579 [D loss: 0.470874, acc.: 80.47%] [G loss: 1.498857]\n",
      "epoch:26 step:24580 [D loss: 0.560577, acc.: 69.53%] [G loss: 1.435072]\n",
      "epoch:26 step:24581 [D loss: 0.416726, acc.: 82.81%] [G loss: 1.468128]\n",
      "epoch:26 step:24582 [D loss: 0.548272, acc.: 69.53%] [G loss: 1.270481]\n",
      "epoch:26 step:24583 [D loss: 0.531389, acc.: 75.00%] [G loss: 1.050813]\n",
      "epoch:26 step:24584 [D loss: 0.501051, acc.: 75.00%] [G loss: 1.461581]\n",
      "epoch:26 step:24585 [D loss: 0.683696, acc.: 60.94%] [G loss: 1.329877]\n",
      "epoch:26 step:24586 [D loss: 0.672866, acc.: 64.84%] [G loss: 1.375262]\n",
      "epoch:26 step:24587 [D loss: 0.635116, acc.: 64.06%] [G loss: 1.211597]\n",
      "epoch:26 step:24588 [D loss: 0.716530, acc.: 60.16%] [G loss: 1.499762]\n",
      "epoch:26 step:24589 [D loss: 0.563332, acc.: 70.31%] [G loss: 1.620310]\n",
      "epoch:26 step:24590 [D loss: 0.522447, acc.: 71.09%] [G loss: 1.764940]\n",
      "epoch:26 step:24591 [D loss: 0.513729, acc.: 75.78%] [G loss: 1.610209]\n",
      "epoch:26 step:24592 [D loss: 0.737709, acc.: 57.03%] [G loss: 1.018801]\n",
      "epoch:26 step:24593 [D loss: 0.688554, acc.: 63.28%] [G loss: 1.556835]\n",
      "epoch:26 step:24594 [D loss: 0.711486, acc.: 55.47%] [G loss: 1.855592]\n",
      "epoch:26 step:24595 [D loss: 0.572887, acc.: 73.44%] [G loss: 1.683429]\n",
      "epoch:26 step:24596 [D loss: 0.651301, acc.: 65.62%] [G loss: 1.793372]\n",
      "epoch:26 step:24597 [D loss: 0.340356, acc.: 89.06%] [G loss: 1.792779]\n",
      "epoch:26 step:24598 [D loss: 0.543678, acc.: 72.66%] [G loss: 1.611327]\n",
      "epoch:26 step:24599 [D loss: 0.464738, acc.: 76.56%] [G loss: 1.634474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24600 [D loss: 0.443204, acc.: 75.78%] [G loss: 1.402094]\n",
      "##############\n",
      "[2.79676881 2.25482858 1.95557536 2.69211437 1.1047671  6.46481556\n",
      " 2.42614792 2.85051111 3.95121321 8.14868929]\n",
      "##########\n",
      "epoch:26 step:24601 [D loss: 0.557048, acc.: 67.97%] [G loss: 1.402804]\n",
      "epoch:26 step:24602 [D loss: 0.570789, acc.: 71.09%] [G loss: 1.281589]\n",
      "epoch:26 step:24603 [D loss: 0.281559, acc.: 92.97%] [G loss: 1.738755]\n",
      "epoch:26 step:24604 [D loss: 0.543126, acc.: 74.22%] [G loss: 1.488418]\n",
      "epoch:26 step:24605 [D loss: 0.493760, acc.: 76.56%] [G loss: 1.545722]\n",
      "epoch:26 step:24606 [D loss: 0.476286, acc.: 80.47%] [G loss: 1.595433]\n",
      "epoch:26 step:24607 [D loss: 0.742048, acc.: 53.91%] [G loss: 1.467515]\n",
      "epoch:26 step:24608 [D loss: 0.597029, acc.: 65.62%] [G loss: 1.266715]\n",
      "epoch:26 step:24609 [D loss: 0.846156, acc.: 46.09%] [G loss: 1.207523]\n",
      "epoch:26 step:24610 [D loss: 0.648944, acc.: 59.38%] [G loss: 1.301643]\n",
      "epoch:26 step:24611 [D loss: 0.393349, acc.: 85.94%] [G loss: 1.770869]\n",
      "epoch:26 step:24612 [D loss: 0.489863, acc.: 75.78%] [G loss: 1.735924]\n",
      "epoch:26 step:24613 [D loss: 0.495608, acc.: 75.78%] [G loss: 1.527728]\n",
      "epoch:26 step:24614 [D loss: 0.451111, acc.: 80.47%] [G loss: 1.473867]\n",
      "epoch:26 step:24615 [D loss: 0.596393, acc.: 68.75%] [G loss: 1.394915]\n",
      "epoch:26 step:24616 [D loss: 0.574267, acc.: 67.97%] [G loss: 1.269907]\n",
      "epoch:26 step:24617 [D loss: 0.568851, acc.: 67.97%] [G loss: 1.292331]\n",
      "epoch:26 step:24618 [D loss: 0.489743, acc.: 77.34%] [G loss: 1.366219]\n",
      "epoch:26 step:24619 [D loss: 0.471477, acc.: 78.12%] [G loss: 1.621504]\n",
      "epoch:26 step:24620 [D loss: 0.526925, acc.: 73.44%] [G loss: 1.494763]\n",
      "epoch:26 step:24621 [D loss: 0.438318, acc.: 82.03%] [G loss: 1.207759]\n",
      "epoch:26 step:24622 [D loss: 0.398472, acc.: 86.72%] [G loss: 1.535883]\n",
      "epoch:26 step:24623 [D loss: 0.506778, acc.: 75.00%] [G loss: 1.586687]\n",
      "epoch:26 step:24624 [D loss: 0.697645, acc.: 61.72%] [G loss: 1.256766]\n",
      "epoch:26 step:24625 [D loss: 0.540315, acc.: 69.53%] [G loss: 1.359733]\n",
      "epoch:26 step:24626 [D loss: 0.546308, acc.: 74.22%] [G loss: 1.390112]\n",
      "epoch:26 step:24627 [D loss: 0.386689, acc.: 85.16%] [G loss: 1.296242]\n",
      "epoch:26 step:24628 [D loss: 0.503249, acc.: 74.22%] [G loss: 1.469470]\n",
      "epoch:26 step:24629 [D loss: 0.685446, acc.: 62.50%] [G loss: 1.476241]\n",
      "epoch:26 step:24630 [D loss: 0.586174, acc.: 70.31%] [G loss: 1.593729]\n",
      "epoch:26 step:24631 [D loss: 0.457960, acc.: 80.47%] [G loss: 1.211731]\n",
      "epoch:26 step:24632 [D loss: 0.498695, acc.: 76.56%] [G loss: 1.470383]\n",
      "epoch:26 step:24633 [D loss: 0.566273, acc.: 70.31%] [G loss: 1.203107]\n",
      "epoch:26 step:24634 [D loss: 0.549130, acc.: 72.66%] [G loss: 1.220839]\n",
      "epoch:26 step:24635 [D loss: 0.545686, acc.: 70.31%] [G loss: 1.461051]\n",
      "epoch:26 step:24636 [D loss: 0.455103, acc.: 81.25%] [G loss: 1.789612]\n",
      "epoch:26 step:24637 [D loss: 0.812191, acc.: 49.22%] [G loss: 0.780398]\n",
      "epoch:26 step:24638 [D loss: 0.644977, acc.: 63.28%] [G loss: 1.517849]\n",
      "epoch:26 step:24639 [D loss: 0.784064, acc.: 50.00%] [G loss: 1.149732]\n",
      "epoch:26 step:24640 [D loss: 0.518951, acc.: 74.22%] [G loss: 1.095768]\n",
      "epoch:26 step:24641 [D loss: 0.624093, acc.: 60.16%] [G loss: 1.377915]\n",
      "epoch:26 step:24642 [D loss: 0.608040, acc.: 61.72%] [G loss: 1.416867]\n",
      "epoch:26 step:24643 [D loss: 0.602728, acc.: 73.44%] [G loss: 0.976631]\n",
      "epoch:26 step:24644 [D loss: 0.567617, acc.: 70.31%] [G loss: 1.359511]\n",
      "epoch:26 step:24645 [D loss: 0.671964, acc.: 64.84%] [G loss: 1.189321]\n",
      "epoch:26 step:24646 [D loss: 0.588975, acc.: 72.66%] [G loss: 1.681884]\n",
      "epoch:26 step:24647 [D loss: 0.500099, acc.: 73.44%] [G loss: 1.553892]\n",
      "epoch:26 step:24648 [D loss: 0.630067, acc.: 68.75%] [G loss: 1.492307]\n",
      "epoch:26 step:24649 [D loss: 0.422301, acc.: 85.94%] [G loss: 1.338521]\n",
      "epoch:26 step:24650 [D loss: 0.435671, acc.: 82.03%] [G loss: 1.775022]\n",
      "epoch:26 step:24651 [D loss: 0.645772, acc.: 64.06%] [G loss: 1.236340]\n",
      "epoch:26 step:24652 [D loss: 0.709604, acc.: 57.03%] [G loss: 1.376641]\n",
      "epoch:26 step:24653 [D loss: 0.654202, acc.: 65.62%] [G loss: 1.113011]\n",
      "epoch:26 step:24654 [D loss: 0.516046, acc.: 69.53%] [G loss: 1.569541]\n",
      "epoch:26 step:24655 [D loss: 0.423878, acc.: 78.12%] [G loss: 1.540563]\n",
      "epoch:26 step:24656 [D loss: 0.550413, acc.: 71.09%] [G loss: 1.313016]\n",
      "epoch:26 step:24657 [D loss: 0.492026, acc.: 76.56%] [G loss: 1.715151]\n",
      "epoch:26 step:24658 [D loss: 0.513785, acc.: 77.34%] [G loss: 1.408269]\n",
      "epoch:26 step:24659 [D loss: 0.682446, acc.: 57.03%] [G loss: 1.391352]\n",
      "epoch:26 step:24660 [D loss: 0.520819, acc.: 73.44%] [G loss: 1.610155]\n",
      "epoch:26 step:24661 [D loss: 0.744460, acc.: 51.56%] [G loss: 1.066589]\n",
      "epoch:26 step:24662 [D loss: 0.440887, acc.: 82.81%] [G loss: 1.488757]\n",
      "epoch:26 step:24663 [D loss: 0.720232, acc.: 55.47%] [G loss: 1.003315]\n",
      "epoch:26 step:24664 [D loss: 0.524466, acc.: 74.22%] [G loss: 1.641847]\n",
      "epoch:26 step:24665 [D loss: 0.563048, acc.: 74.22%] [G loss: 0.906077]\n",
      "epoch:26 step:24666 [D loss: 0.697036, acc.: 60.16%] [G loss: 0.929544]\n",
      "epoch:26 step:24667 [D loss: 0.806994, acc.: 46.88%] [G loss: 1.461426]\n",
      "epoch:26 step:24668 [D loss: 0.437411, acc.: 78.91%] [G loss: 1.383234]\n",
      "epoch:26 step:24669 [D loss: 0.652100, acc.: 64.84%] [G loss: 1.107731]\n",
      "epoch:26 step:24670 [D loss: 0.543473, acc.: 70.31%] [G loss: 1.123402]\n",
      "epoch:26 step:24671 [D loss: 0.472576, acc.: 81.25%] [G loss: 1.546227]\n",
      "epoch:26 step:24672 [D loss: 0.634412, acc.: 66.41%] [G loss: 1.642430]\n",
      "epoch:26 step:24673 [D loss: 0.426978, acc.: 85.16%] [G loss: 1.964484]\n",
      "epoch:26 step:24674 [D loss: 0.641868, acc.: 65.62%] [G loss: 1.368026]\n",
      "epoch:26 step:24675 [D loss: 0.705966, acc.: 57.81%] [G loss: 1.519795]\n",
      "epoch:26 step:24676 [D loss: 0.522218, acc.: 78.12%] [G loss: 1.120015]\n",
      "epoch:26 step:24677 [D loss: 0.571461, acc.: 66.41%] [G loss: 1.053873]\n",
      "epoch:26 step:24678 [D loss: 0.550546, acc.: 70.31%] [G loss: 1.267858]\n",
      "epoch:26 step:24679 [D loss: 0.445199, acc.: 81.25%] [G loss: 1.756910]\n",
      "epoch:26 step:24680 [D loss: 0.607725, acc.: 64.06%] [G loss: 1.245599]\n",
      "epoch:26 step:24681 [D loss: 0.529860, acc.: 71.88%] [G loss: 1.123500]\n",
      "epoch:26 step:24682 [D loss: 0.600394, acc.: 67.97%] [G loss: 1.501279]\n",
      "epoch:26 step:24683 [D loss: 0.775260, acc.: 57.03%] [G loss: 1.524353]\n",
      "epoch:26 step:24684 [D loss: 0.552796, acc.: 67.19%] [G loss: 1.291189]\n",
      "epoch:26 step:24685 [D loss: 0.659871, acc.: 59.38%] [G loss: 1.123910]\n",
      "epoch:26 step:24686 [D loss: 0.418832, acc.: 84.38%] [G loss: 1.227879]\n",
      "epoch:26 step:24687 [D loss: 0.603138, acc.: 65.62%] [G loss: 1.488901]\n",
      "epoch:26 step:24688 [D loss: 0.356986, acc.: 85.16%] [G loss: 1.701703]\n",
      "epoch:26 step:24689 [D loss: 0.532709, acc.: 74.22%] [G loss: 1.495731]\n",
      "epoch:26 step:24690 [D loss: 0.648106, acc.: 64.06%] [G loss: 1.450444]\n",
      "epoch:26 step:24691 [D loss: 0.620757, acc.: 66.41%] [G loss: 1.570575]\n",
      "epoch:26 step:24692 [D loss: 0.507643, acc.: 81.25%] [G loss: 1.443198]\n",
      "epoch:26 step:24693 [D loss: 0.427783, acc.: 82.81%] [G loss: 1.934353]\n",
      "epoch:26 step:24694 [D loss: 0.501876, acc.: 72.66%] [G loss: 1.337822]\n",
      "epoch:26 step:24695 [D loss: 0.498777, acc.: 79.69%] [G loss: 1.763428]\n",
      "epoch:26 step:24696 [D loss: 0.585428, acc.: 68.75%] [G loss: 1.612120]\n",
      "epoch:26 step:24697 [D loss: 0.511131, acc.: 75.00%] [G loss: 1.202260]\n",
      "epoch:26 step:24698 [D loss: 0.410217, acc.: 85.94%] [G loss: 1.459759]\n",
      "epoch:26 step:24699 [D loss: 0.482347, acc.: 74.22%] [G loss: 1.613962]\n",
      "epoch:26 step:24700 [D loss: 0.486185, acc.: 74.22%] [G loss: 1.439896]\n",
      "epoch:26 step:24701 [D loss: 0.698273, acc.: 55.47%] [G loss: 1.189355]\n",
      "epoch:26 step:24702 [D loss: 0.437351, acc.: 83.59%] [G loss: 1.534595]\n",
      "epoch:26 step:24703 [D loss: 0.538870, acc.: 72.66%] [G loss: 1.333085]\n",
      "epoch:26 step:24704 [D loss: 0.583009, acc.: 72.66%] [G loss: 1.533204]\n",
      "epoch:26 step:24705 [D loss: 0.540177, acc.: 72.66%] [G loss: 1.851631]\n",
      "epoch:26 step:24706 [D loss: 0.634888, acc.: 60.16%] [G loss: 1.172611]\n",
      "epoch:26 step:24707 [D loss: 0.727668, acc.: 55.47%] [G loss: 1.204697]\n",
      "epoch:26 step:24708 [D loss: 0.599475, acc.: 66.41%] [G loss: 1.661451]\n",
      "epoch:26 step:24709 [D loss: 0.575902, acc.: 67.97%] [G loss: 1.255602]\n",
      "epoch:26 step:24710 [D loss: 0.581987, acc.: 66.41%] [G loss: 1.459192]\n",
      "epoch:26 step:24711 [D loss: 0.365815, acc.: 85.94%] [G loss: 1.643883]\n",
      "epoch:26 step:24712 [D loss: 0.524598, acc.: 73.44%] [G loss: 1.261340]\n",
      "epoch:26 step:24713 [D loss: 0.437170, acc.: 80.47%] [G loss: 1.781159]\n",
      "epoch:26 step:24714 [D loss: 0.558013, acc.: 75.00%] [G loss: 1.609596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24715 [D loss: 0.519491, acc.: 78.12%] [G loss: 1.521779]\n",
      "epoch:26 step:24716 [D loss: 0.587461, acc.: 68.75%] [G loss: 1.351981]\n",
      "epoch:26 step:24717 [D loss: 0.594375, acc.: 70.31%] [G loss: 1.109254]\n",
      "epoch:26 step:24718 [D loss: 0.569648, acc.: 71.88%] [G loss: 1.237906]\n",
      "epoch:26 step:24719 [D loss: 0.534246, acc.: 74.22%] [G loss: 1.421010]\n",
      "epoch:26 step:24720 [D loss: 0.708791, acc.: 56.25%] [G loss: 1.201860]\n",
      "epoch:26 step:24721 [D loss: 0.454234, acc.: 80.47%] [G loss: 1.938108]\n",
      "epoch:26 step:24722 [D loss: 0.692760, acc.: 59.38%] [G loss: 0.913944]\n",
      "epoch:26 step:24723 [D loss: 0.500758, acc.: 74.22%] [G loss: 2.011900]\n",
      "epoch:26 step:24724 [D loss: 0.618738, acc.: 67.97%] [G loss: 1.317298]\n",
      "epoch:26 step:24725 [D loss: 0.651566, acc.: 64.06%] [G loss: 1.128921]\n",
      "epoch:26 step:24726 [D loss: 0.611287, acc.: 64.84%] [G loss: 1.383651]\n",
      "epoch:26 step:24727 [D loss: 0.575054, acc.: 70.31%] [G loss: 1.036024]\n",
      "epoch:26 step:24728 [D loss: 0.528936, acc.: 75.00%] [G loss: 1.383272]\n",
      "epoch:26 step:24729 [D loss: 0.483481, acc.: 82.03%] [G loss: 1.570905]\n",
      "epoch:26 step:24730 [D loss: 0.509760, acc.: 76.56%] [G loss: 1.205775]\n",
      "epoch:26 step:24731 [D loss: 0.475359, acc.: 80.47%] [G loss: 1.516046]\n",
      "epoch:26 step:24732 [D loss: 0.549834, acc.: 70.31%] [G loss: 1.337024]\n",
      "epoch:26 step:24733 [D loss: 0.654924, acc.: 63.28%] [G loss: 1.412076]\n",
      "epoch:26 step:24734 [D loss: 0.504268, acc.: 79.69%] [G loss: 1.656978]\n",
      "epoch:26 step:24735 [D loss: 0.339933, acc.: 88.28%] [G loss: 1.788307]\n",
      "epoch:26 step:24736 [D loss: 0.633300, acc.: 68.75%] [G loss: 1.265349]\n",
      "epoch:26 step:24737 [D loss: 0.551720, acc.: 68.75%] [G loss: 1.432112]\n",
      "epoch:26 step:24738 [D loss: 0.487786, acc.: 78.91%] [G loss: 1.050413]\n",
      "epoch:26 step:24739 [D loss: 0.380647, acc.: 85.16%] [G loss: 1.391464]\n",
      "epoch:26 step:24740 [D loss: 0.698743, acc.: 57.03%] [G loss: 1.213311]\n",
      "epoch:26 step:24741 [D loss: 0.512160, acc.: 76.56%] [G loss: 1.467739]\n",
      "epoch:26 step:24742 [D loss: 0.441508, acc.: 82.03%] [G loss: 1.511908]\n",
      "epoch:26 step:24743 [D loss: 0.530547, acc.: 75.78%] [G loss: 1.432513]\n",
      "epoch:26 step:24744 [D loss: 0.482750, acc.: 75.78%] [G loss: 1.114102]\n",
      "epoch:26 step:24745 [D loss: 0.508149, acc.: 77.34%] [G loss: 1.878988]\n",
      "epoch:26 step:24746 [D loss: 0.582192, acc.: 70.31%] [G loss: 1.025680]\n",
      "epoch:26 step:24747 [D loss: 0.635390, acc.: 64.84%] [G loss: 1.407210]\n",
      "epoch:26 step:24748 [D loss: 0.620603, acc.: 64.84%] [G loss: 1.499870]\n",
      "epoch:26 step:24749 [D loss: 0.420940, acc.: 83.59%] [G loss: 1.746176]\n",
      "epoch:26 step:24750 [D loss: 0.624056, acc.: 69.53%] [G loss: 1.453555]\n",
      "epoch:26 step:24751 [D loss: 0.681739, acc.: 62.50%] [G loss: 1.318765]\n",
      "epoch:26 step:24752 [D loss: 0.601787, acc.: 66.41%] [G loss: 1.442159]\n",
      "epoch:26 step:24753 [D loss: 0.613950, acc.: 68.75%] [G loss: 1.234275]\n",
      "epoch:26 step:24754 [D loss: 0.509042, acc.: 78.91%] [G loss: 1.295387]\n",
      "epoch:26 step:24755 [D loss: 0.623653, acc.: 68.75%] [G loss: 1.000492]\n",
      "epoch:26 step:24756 [D loss: 0.600438, acc.: 66.41%] [G loss: 1.358948]\n",
      "epoch:26 step:24757 [D loss: 0.462683, acc.: 79.69%] [G loss: 1.179485]\n",
      "epoch:26 step:24758 [D loss: 0.497946, acc.: 75.78%] [G loss: 1.408717]\n",
      "epoch:26 step:24759 [D loss: 0.608528, acc.: 67.97%] [G loss: 1.439932]\n",
      "epoch:26 step:24760 [D loss: 0.619198, acc.: 67.19%] [G loss: 1.541354]\n",
      "epoch:26 step:24761 [D loss: 0.572792, acc.: 67.19%] [G loss: 1.607336]\n",
      "epoch:26 step:24762 [D loss: 0.633867, acc.: 63.28%] [G loss: 1.335578]\n",
      "epoch:26 step:24763 [D loss: 0.496561, acc.: 76.56%] [G loss: 1.115405]\n",
      "epoch:26 step:24764 [D loss: 0.345013, acc.: 92.19%] [G loss: 1.441054]\n",
      "epoch:26 step:24765 [D loss: 0.551473, acc.: 67.19%] [G loss: 1.682620]\n",
      "epoch:26 step:24766 [D loss: 0.557405, acc.: 75.78%] [G loss: 1.342772]\n",
      "epoch:26 step:24767 [D loss: 0.452007, acc.: 79.69%] [G loss: 1.414597]\n",
      "epoch:26 step:24768 [D loss: 0.455254, acc.: 84.38%] [G loss: 1.229924]\n",
      "epoch:26 step:24769 [D loss: 0.594008, acc.: 63.28%] [G loss: 1.002567]\n",
      "epoch:26 step:24770 [D loss: 0.542561, acc.: 73.44%] [G loss: 1.104517]\n",
      "epoch:26 step:24771 [D loss: 0.515124, acc.: 73.44%] [G loss: 1.119574]\n",
      "epoch:26 step:24772 [D loss: 0.450561, acc.: 80.47%] [G loss: 1.348585]\n",
      "epoch:26 step:24773 [D loss: 0.575198, acc.: 67.19%] [G loss: 1.529414]\n",
      "epoch:26 step:24774 [D loss: 0.653609, acc.: 65.62%] [G loss: 1.186391]\n",
      "epoch:26 step:24775 [D loss: 0.442363, acc.: 82.03%] [G loss: 1.350446]\n",
      "epoch:26 step:24776 [D loss: 0.662620, acc.: 64.06%] [G loss: 1.525958]\n",
      "epoch:26 step:24777 [D loss: 0.474058, acc.: 76.56%] [G loss: 1.493786]\n",
      "epoch:26 step:24778 [D loss: 0.685833, acc.: 60.94%] [G loss: 0.925048]\n",
      "epoch:26 step:24779 [D loss: 0.639387, acc.: 68.75%] [G loss: 1.055306]\n",
      "epoch:26 step:24780 [D loss: 0.790566, acc.: 50.78%] [G loss: 1.200430]\n",
      "epoch:26 step:24781 [D loss: 0.316176, acc.: 91.41%] [G loss: 1.641350]\n",
      "epoch:26 step:24782 [D loss: 0.610888, acc.: 68.75%] [G loss: 1.961233]\n",
      "epoch:26 step:24783 [D loss: 0.536871, acc.: 70.31%] [G loss: 1.144517]\n",
      "epoch:26 step:24784 [D loss: 0.590706, acc.: 70.31%] [G loss: 1.609478]\n",
      "epoch:26 step:24785 [D loss: 0.542836, acc.: 78.91%] [G loss: 1.417978]\n",
      "epoch:26 step:24786 [D loss: 0.421781, acc.: 82.81%] [G loss: 1.627672]\n",
      "epoch:26 step:24787 [D loss: 0.607991, acc.: 67.19%] [G loss: 1.199778]\n",
      "epoch:26 step:24788 [D loss: 0.549688, acc.: 70.31%] [G loss: 1.465883]\n",
      "epoch:26 step:24789 [D loss: 0.803655, acc.: 52.34%] [G loss: 1.585144]\n",
      "epoch:26 step:24790 [D loss: 0.795287, acc.: 49.22%] [G loss: 0.992365]\n",
      "epoch:26 step:24791 [D loss: 0.479969, acc.: 79.69%] [G loss: 1.285545]\n",
      "epoch:26 step:24792 [D loss: 0.567080, acc.: 69.53%] [G loss: 1.250848]\n",
      "epoch:26 step:24793 [D loss: 0.428511, acc.: 84.38%] [G loss: 1.639296]\n",
      "epoch:26 step:24794 [D loss: 0.351270, acc.: 88.28%] [G loss: 2.027308]\n",
      "epoch:26 step:24795 [D loss: 0.561194, acc.: 74.22%] [G loss: 1.416312]\n",
      "epoch:26 step:24796 [D loss: 0.510335, acc.: 71.88%] [G loss: 1.380523]\n",
      "epoch:26 step:24797 [D loss: 0.561656, acc.: 67.19%] [G loss: 1.279537]\n",
      "epoch:26 step:24798 [D loss: 0.523392, acc.: 75.78%] [G loss: 1.720707]\n",
      "epoch:26 step:24799 [D loss: 0.589416, acc.: 71.09%] [G loss: 1.159143]\n",
      "epoch:26 step:24800 [D loss: 0.499764, acc.: 74.22%] [G loss: 1.169337]\n",
      "##############\n",
      "[2.72039715 2.22197324 1.89465342 2.43988165 1.04615755 5.62597295\n",
      " 2.15807324 2.74517924 3.90604257 5.82334091]\n",
      "##########\n",
      "epoch:26 step:24801 [D loss: 0.533769, acc.: 75.00%] [G loss: 1.335834]\n",
      "epoch:26 step:24802 [D loss: 0.438869, acc.: 80.47%] [G loss: 1.635272]\n",
      "epoch:26 step:24803 [D loss: 0.529767, acc.: 74.22%] [G loss: 1.753099]\n",
      "epoch:26 step:24804 [D loss: 0.590389, acc.: 68.75%] [G loss: 1.217103]\n",
      "epoch:26 step:24805 [D loss: 0.501373, acc.: 75.78%] [G loss: 1.676450]\n",
      "epoch:26 step:24806 [D loss: 0.540781, acc.: 70.31%] [G loss: 1.207318]\n",
      "epoch:26 step:24807 [D loss: 0.532386, acc.: 71.88%] [G loss: 1.085111]\n",
      "epoch:26 step:24808 [D loss: 0.425652, acc.: 85.16%] [G loss: 1.559926]\n",
      "epoch:26 step:24809 [D loss: 0.549466, acc.: 71.88%] [G loss: 1.272483]\n",
      "epoch:26 step:24810 [D loss: 0.441754, acc.: 82.03%] [G loss: 1.592431]\n",
      "epoch:26 step:24811 [D loss: 0.694609, acc.: 60.16%] [G loss: 0.894382]\n",
      "epoch:26 step:24812 [D loss: 0.485336, acc.: 81.25%] [G loss: 1.667533]\n",
      "epoch:26 step:24813 [D loss: 0.467532, acc.: 79.69%] [G loss: 1.634341]\n",
      "epoch:26 step:24814 [D loss: 0.720295, acc.: 60.94%] [G loss: 1.378302]\n",
      "epoch:26 step:24815 [D loss: 0.618382, acc.: 66.41%] [G loss: 1.402034]\n",
      "epoch:26 step:24816 [D loss: 0.408480, acc.: 82.03%] [G loss: 1.729152]\n",
      "epoch:26 step:24817 [D loss: 0.580389, acc.: 70.31%] [G loss: 1.438335]\n",
      "epoch:26 step:24818 [D loss: 0.544911, acc.: 71.88%] [G loss: 1.465786]\n",
      "epoch:26 step:24819 [D loss: 0.660246, acc.: 57.03%] [G loss: 1.359799]\n",
      "epoch:26 step:24820 [D loss: 0.537521, acc.: 71.88%] [G loss: 1.336856]\n",
      "epoch:26 step:24821 [D loss: 0.552865, acc.: 75.00%] [G loss: 1.564935]\n",
      "epoch:26 step:24822 [D loss: 0.744579, acc.: 59.38%] [G loss: 1.574282]\n",
      "epoch:26 step:24823 [D loss: 0.586306, acc.: 68.75%] [G loss: 1.255905]\n",
      "epoch:26 step:24824 [D loss: 1.018183, acc.: 30.47%] [G loss: 0.770441]\n",
      "epoch:26 step:24825 [D loss: 0.609736, acc.: 64.84%] [G loss: 1.208027]\n",
      "epoch:26 step:24826 [D loss: 0.608559, acc.: 65.62%] [G loss: 1.554565]\n",
      "epoch:26 step:24827 [D loss: 0.799930, acc.: 57.81%] [G loss: 1.562027]\n",
      "epoch:26 step:24828 [D loss: 0.471491, acc.: 79.69%] [G loss: 1.523574]\n",
      "epoch:26 step:24829 [D loss: 0.454014, acc.: 81.25%] [G loss: 1.762323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24830 [D loss: 0.566849, acc.: 72.66%] [G loss: 1.411459]\n",
      "epoch:26 step:24831 [D loss: 0.429578, acc.: 81.25%] [G loss: 1.982646]\n",
      "epoch:26 step:24832 [D loss: 0.713612, acc.: 60.16%] [G loss: 1.076795]\n",
      "epoch:26 step:24833 [D loss: 0.573172, acc.: 71.09%] [G loss: 1.484424]\n",
      "epoch:26 step:24834 [D loss: 0.577168, acc.: 75.00%] [G loss: 1.258280]\n",
      "epoch:26 step:24835 [D loss: 0.464911, acc.: 76.56%] [G loss: 1.564637]\n",
      "epoch:26 step:24836 [D loss: 0.594207, acc.: 68.75%] [G loss: 1.233105]\n",
      "epoch:26 step:24837 [D loss: 0.496054, acc.: 78.12%] [G loss: 1.648456]\n",
      "epoch:26 step:24838 [D loss: 0.419263, acc.: 82.81%] [G loss: 1.234308]\n",
      "epoch:26 step:24839 [D loss: 0.601336, acc.: 64.06%] [G loss: 1.418043]\n",
      "epoch:26 step:24840 [D loss: 0.450962, acc.: 82.81%] [G loss: 1.620258]\n",
      "epoch:26 step:24841 [D loss: 0.559343, acc.: 70.31%] [G loss: 1.021361]\n",
      "epoch:26 step:24842 [D loss: 0.438035, acc.: 81.25%] [G loss: 1.230479]\n",
      "epoch:26 step:24843 [D loss: 0.508492, acc.: 76.56%] [G loss: 1.658463]\n",
      "epoch:26 step:24844 [D loss: 0.799718, acc.: 46.88%] [G loss: 1.258371]\n",
      "epoch:26 step:24845 [D loss: 0.672892, acc.: 63.28%] [G loss: 1.314635]\n",
      "epoch:26 step:24846 [D loss: 0.380018, acc.: 89.06%] [G loss: 1.217509]\n",
      "epoch:26 step:24847 [D loss: 0.562034, acc.: 71.09%] [G loss: 1.456690]\n",
      "epoch:26 step:24848 [D loss: 0.424999, acc.: 85.16%] [G loss: 1.463424]\n",
      "epoch:26 step:24849 [D loss: 0.381720, acc.: 84.38%] [G loss: 1.227840]\n",
      "epoch:26 step:24850 [D loss: 0.445622, acc.: 81.25%] [G loss: 1.544865]\n",
      "epoch:26 step:24851 [D loss: 0.636699, acc.: 66.41%] [G loss: 1.354995]\n",
      "epoch:26 step:24852 [D loss: 0.598046, acc.: 66.41%] [G loss: 1.465171]\n",
      "epoch:26 step:24853 [D loss: 0.442760, acc.: 81.25%] [G loss: 1.476752]\n",
      "epoch:26 step:24854 [D loss: 0.649940, acc.: 64.06%] [G loss: 1.444316]\n",
      "epoch:26 step:24855 [D loss: 0.465654, acc.: 78.12%] [G loss: 1.334986]\n",
      "epoch:26 step:24856 [D loss: 0.465538, acc.: 80.47%] [G loss: 1.740596]\n",
      "epoch:26 step:24857 [D loss: 0.595085, acc.: 70.31%] [G loss: 0.957973]\n",
      "epoch:26 step:24858 [D loss: 0.550631, acc.: 70.31%] [G loss: 1.436252]\n",
      "epoch:26 step:24859 [D loss: 0.611713, acc.: 64.84%] [G loss: 1.381044]\n",
      "epoch:26 step:24860 [D loss: 0.516771, acc.: 74.22%] [G loss: 1.484928]\n",
      "epoch:26 step:24861 [D loss: 0.489568, acc.: 74.22%] [G loss: 1.264652]\n",
      "epoch:26 step:24862 [D loss: 0.438419, acc.: 80.47%] [G loss: 1.706242]\n",
      "epoch:26 step:24863 [D loss: 0.443167, acc.: 82.81%] [G loss: 1.795693]\n",
      "epoch:26 step:24864 [D loss: 0.612810, acc.: 67.97%] [G loss: 1.055407]\n",
      "epoch:26 step:24865 [D loss: 0.344616, acc.: 89.84%] [G loss: 1.755579]\n",
      "epoch:26 step:24866 [D loss: 0.608833, acc.: 66.41%] [G loss: 1.308376]\n",
      "epoch:26 step:24867 [D loss: 0.467783, acc.: 79.69%] [G loss: 1.241822]\n",
      "epoch:26 step:24868 [D loss: 0.395809, acc.: 86.72%] [G loss: 1.322980]\n",
      "epoch:26 step:24869 [D loss: 0.550864, acc.: 76.56%] [G loss: 1.392403]\n",
      "epoch:26 step:24870 [D loss: 0.489001, acc.: 76.56%] [G loss: 1.523466]\n",
      "epoch:26 step:24871 [D loss: 0.535985, acc.: 69.53%] [G loss: 1.568882]\n",
      "epoch:26 step:24872 [D loss: 0.458635, acc.: 78.12%] [G loss: 1.371471]\n",
      "epoch:26 step:24873 [D loss: 0.609579, acc.: 66.41%] [G loss: 1.359276]\n",
      "epoch:26 step:24874 [D loss: 0.496056, acc.: 78.12%] [G loss: 1.141959]\n",
      "epoch:26 step:24875 [D loss: 0.500436, acc.: 73.44%] [G loss: 1.846977]\n",
      "epoch:26 step:24876 [D loss: 0.378475, acc.: 83.59%] [G loss: 1.774724]\n",
      "epoch:26 step:24877 [D loss: 0.461106, acc.: 80.47%] [G loss: 1.844452]\n",
      "epoch:26 step:24878 [D loss: 0.451517, acc.: 78.91%] [G loss: 1.476833]\n",
      "epoch:26 step:24879 [D loss: 0.475230, acc.: 79.69%] [G loss: 1.566725]\n",
      "epoch:26 step:24880 [D loss: 0.601666, acc.: 67.19%] [G loss: 1.330797]\n",
      "epoch:26 step:24881 [D loss: 0.471849, acc.: 80.47%] [G loss: 1.714595]\n",
      "epoch:26 step:24882 [D loss: 0.547604, acc.: 75.00%] [G loss: 0.938556]\n",
      "epoch:26 step:24883 [D loss: 0.559727, acc.: 75.78%] [G loss: 1.363508]\n",
      "epoch:26 step:24884 [D loss: 0.477990, acc.: 80.47%] [G loss: 1.354512]\n",
      "epoch:26 step:24885 [D loss: 0.704588, acc.: 58.59%] [G loss: 1.002996]\n",
      "epoch:26 step:24886 [D loss: 0.599299, acc.: 72.66%] [G loss: 1.225876]\n",
      "epoch:26 step:24887 [D loss: 0.491840, acc.: 74.22%] [G loss: 1.309391]\n",
      "epoch:26 step:24888 [D loss: 0.626062, acc.: 68.75%] [G loss: 1.486550]\n",
      "epoch:26 step:24889 [D loss: 0.576497, acc.: 69.53%] [G loss: 0.946142]\n",
      "epoch:26 step:24890 [D loss: 0.618865, acc.: 61.72%] [G loss: 1.553324]\n",
      "epoch:26 step:24891 [D loss: 0.405440, acc.: 82.81%] [G loss: 1.214598]\n",
      "epoch:26 step:24892 [D loss: 0.555701, acc.: 64.84%] [G loss: 1.196239]\n",
      "epoch:26 step:24893 [D loss: 0.600518, acc.: 65.62%] [G loss: 0.908366]\n",
      "epoch:26 step:24894 [D loss: 0.758603, acc.: 57.03%] [G loss: 1.317220]\n",
      "epoch:26 step:24895 [D loss: 0.484094, acc.: 78.12%] [G loss: 1.454470]\n",
      "epoch:26 step:24896 [D loss: 0.655469, acc.: 65.62%] [G loss: 1.700835]\n",
      "epoch:26 step:24897 [D loss: 0.464146, acc.: 78.12%] [G loss: 1.494240]\n",
      "epoch:26 step:24898 [D loss: 0.481941, acc.: 76.56%] [G loss: 1.056803]\n",
      "epoch:26 step:24899 [D loss: 0.488695, acc.: 78.91%] [G loss: 1.384955]\n",
      "epoch:26 step:24900 [D loss: 0.821032, acc.: 50.78%] [G loss: 1.485808]\n",
      "epoch:26 step:24901 [D loss: 0.506337, acc.: 76.56%] [G loss: 1.472407]\n",
      "epoch:26 step:24902 [D loss: 0.577375, acc.: 69.53%] [G loss: 1.244313]\n",
      "epoch:26 step:24903 [D loss: 0.510117, acc.: 78.91%] [G loss: 1.615547]\n",
      "epoch:26 step:24904 [D loss: 0.539154, acc.: 71.88%] [G loss: 1.167053]\n",
      "epoch:26 step:24905 [D loss: 0.507372, acc.: 74.22%] [G loss: 1.591071]\n",
      "epoch:26 step:24906 [D loss: 0.526215, acc.: 75.78%] [G loss: 1.065616]\n",
      "epoch:26 step:24907 [D loss: 0.474293, acc.: 79.69%] [G loss: 1.687653]\n",
      "epoch:26 step:24908 [D loss: 0.611252, acc.: 68.75%] [G loss: 1.969567]\n",
      "epoch:26 step:24909 [D loss: 0.656134, acc.: 67.19%] [G loss: 1.668727]\n",
      "epoch:26 step:24910 [D loss: 0.596033, acc.: 63.28%] [G loss: 1.315009]\n",
      "epoch:26 step:24911 [D loss: 0.579741, acc.: 71.88%] [G loss: 1.343673]\n",
      "epoch:26 step:24912 [D loss: 0.531468, acc.: 74.22%] [G loss: 0.868284]\n",
      "epoch:26 step:24913 [D loss: 0.865778, acc.: 48.44%] [G loss: 1.313431]\n",
      "epoch:26 step:24914 [D loss: 0.527712, acc.: 75.00%] [G loss: 1.390939]\n",
      "epoch:26 step:24915 [D loss: 0.423521, acc.: 81.25%] [G loss: 1.662184]\n",
      "epoch:26 step:24916 [D loss: 0.531832, acc.: 77.34%] [G loss: 1.482759]\n",
      "epoch:26 step:24917 [D loss: 0.579974, acc.: 64.06%] [G loss: 1.358683]\n",
      "epoch:26 step:24918 [D loss: 0.473073, acc.: 76.56%] [G loss: 1.259655]\n",
      "epoch:26 step:24919 [D loss: 0.471304, acc.: 75.00%] [G loss: 1.636577]\n",
      "epoch:26 step:24920 [D loss: 0.542053, acc.: 74.22%] [G loss: 1.304091]\n",
      "epoch:26 step:24921 [D loss: 0.471169, acc.: 77.34%] [G loss: 1.631639]\n",
      "epoch:26 step:24922 [D loss: 0.563196, acc.: 68.75%] [G loss: 1.291050]\n",
      "epoch:26 step:24923 [D loss: 0.507180, acc.: 70.31%] [G loss: 1.443039]\n",
      "epoch:26 step:24924 [D loss: 0.331261, acc.: 92.19%] [G loss: 1.833868]\n",
      "epoch:26 step:24925 [D loss: 0.436197, acc.: 79.69%] [G loss: 1.520700]\n",
      "epoch:26 step:24926 [D loss: 0.458081, acc.: 81.25%] [G loss: 1.425950]\n",
      "epoch:26 step:24927 [D loss: 0.516084, acc.: 74.22%] [G loss: 1.206352]\n",
      "epoch:26 step:24928 [D loss: 0.548734, acc.: 67.97%] [G loss: 1.705374]\n",
      "epoch:26 step:24929 [D loss: 0.761488, acc.: 59.38%] [G loss: 1.241395]\n",
      "epoch:26 step:24930 [D loss: 0.712597, acc.: 60.94%] [G loss: 1.125661]\n",
      "epoch:26 step:24931 [D loss: 0.513387, acc.: 76.56%] [G loss: 1.598800]\n",
      "epoch:26 step:24932 [D loss: 0.489827, acc.: 75.78%] [G loss: 1.701899]\n",
      "epoch:26 step:24933 [D loss: 0.506740, acc.: 75.00%] [G loss: 1.670127]\n",
      "epoch:26 step:24934 [D loss: 0.490826, acc.: 79.69%] [G loss: 1.776538]\n",
      "epoch:26 step:24935 [D loss: 0.670514, acc.: 60.94%] [G loss: 1.292682]\n",
      "epoch:26 step:24936 [D loss: 0.632391, acc.: 64.06%] [G loss: 1.064075]\n",
      "epoch:26 step:24937 [D loss: 0.420684, acc.: 82.03%] [G loss: 1.421188]\n",
      "epoch:26 step:24938 [D loss: 0.300269, acc.: 92.97%] [G loss: 1.294307]\n",
      "epoch:26 step:24939 [D loss: 0.553494, acc.: 75.78%] [G loss: 2.014019]\n",
      "epoch:26 step:24940 [D loss: 0.492683, acc.: 79.69%] [G loss: 1.686531]\n",
      "epoch:26 step:24941 [D loss: 0.630108, acc.: 67.19%] [G loss: 1.104950]\n",
      "epoch:26 step:24942 [D loss: 0.438733, acc.: 82.81%] [G loss: 1.634107]\n",
      "epoch:26 step:24943 [D loss: 0.579581, acc.: 77.34%] [G loss: 1.586425]\n",
      "epoch:26 step:24944 [D loss: 0.478911, acc.: 75.78%] [G loss: 1.437269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24945 [D loss: 0.539693, acc.: 71.88%] [G loss: 1.419645]\n",
      "epoch:26 step:24946 [D loss: 0.498226, acc.: 79.69%] [G loss: 1.315135]\n",
      "epoch:26 step:24947 [D loss: 0.567201, acc.: 67.97%] [G loss: 1.566618]\n",
      "epoch:26 step:24948 [D loss: 0.508211, acc.: 75.00%] [G loss: 1.500954]\n",
      "epoch:26 step:24949 [D loss: 0.404100, acc.: 87.50%] [G loss: 1.672870]\n",
      "epoch:26 step:24950 [D loss: 0.571249, acc.: 70.31%] [G loss: 1.578300]\n",
      "epoch:26 step:24951 [D loss: 0.436056, acc.: 80.47%] [G loss: 1.219089]\n",
      "epoch:26 step:24952 [D loss: 0.449934, acc.: 78.91%] [G loss: 1.102679]\n",
      "epoch:26 step:24953 [D loss: 0.464212, acc.: 77.34%] [G loss: 1.787331]\n",
      "epoch:26 step:24954 [D loss: 0.685371, acc.: 59.38%] [G loss: 1.709022]\n",
      "epoch:26 step:24955 [D loss: 0.703731, acc.: 60.16%] [G loss: 1.655787]\n",
      "epoch:26 step:24956 [D loss: 0.518590, acc.: 74.22%] [G loss: 1.447849]\n",
      "epoch:26 step:24957 [D loss: 0.495402, acc.: 73.44%] [G loss: 1.829312]\n",
      "epoch:26 step:24958 [D loss: 0.401026, acc.: 86.72%] [G loss: 1.419354]\n",
      "epoch:26 step:24959 [D loss: 0.597647, acc.: 71.09%] [G loss: 1.144568]\n",
      "epoch:26 step:24960 [D loss: 0.679183, acc.: 57.81%] [G loss: 1.370506]\n",
      "epoch:26 step:24961 [D loss: 0.484422, acc.: 78.91%] [G loss: 1.592124]\n",
      "epoch:26 step:24962 [D loss: 0.409065, acc.: 83.59%] [G loss: 2.017894]\n",
      "epoch:26 step:24963 [D loss: 0.788460, acc.: 54.69%] [G loss: 1.962454]\n",
      "epoch:26 step:24964 [D loss: 0.410009, acc.: 86.72%] [G loss: 1.357161]\n",
      "epoch:26 step:24965 [D loss: 0.609599, acc.: 71.09%] [G loss: 1.695328]\n",
      "epoch:26 step:24966 [D loss: 0.511912, acc.: 80.47%] [G loss: 1.710304]\n",
      "epoch:26 step:24967 [D loss: 0.522264, acc.: 75.78%] [G loss: 1.566365]\n",
      "epoch:26 step:24968 [D loss: 0.760315, acc.: 54.69%] [G loss: 1.037649]\n",
      "epoch:26 step:24969 [D loss: 0.636948, acc.: 65.62%] [G loss: 1.482357]\n",
      "epoch:26 step:24970 [D loss: 0.436173, acc.: 80.47%] [G loss: 1.564463]\n",
      "epoch:26 step:24971 [D loss: 0.715011, acc.: 55.47%] [G loss: 1.434977]\n",
      "epoch:26 step:24972 [D loss: 0.451199, acc.: 80.47%] [G loss: 1.434618]\n",
      "epoch:26 step:24973 [D loss: 0.603687, acc.: 67.19%] [G loss: 1.158184]\n",
      "epoch:26 step:24974 [D loss: 0.540715, acc.: 75.00%] [G loss: 1.778085]\n",
      "epoch:26 step:24975 [D loss: 0.523389, acc.: 75.78%] [G loss: 1.690373]\n",
      "epoch:26 step:24976 [D loss: 0.689855, acc.: 58.59%] [G loss: 1.565965]\n",
      "epoch:26 step:24977 [D loss: 0.429563, acc.: 79.69%] [G loss: 1.765980]\n",
      "epoch:26 step:24978 [D loss: 0.377225, acc.: 85.94%] [G loss: 1.955076]\n",
      "epoch:26 step:24979 [D loss: 0.389898, acc.: 89.84%] [G loss: 1.358510]\n",
      "epoch:26 step:24980 [D loss: 0.399996, acc.: 85.94%] [G loss: 1.448177]\n",
      "epoch:26 step:24981 [D loss: 0.486136, acc.: 75.00%] [G loss: 1.092380]\n",
      "epoch:26 step:24982 [D loss: 0.636822, acc.: 67.97%] [G loss: 1.143911]\n",
      "epoch:26 step:24983 [D loss: 0.642134, acc.: 65.62%] [G loss: 1.222043]\n",
      "epoch:26 step:24984 [D loss: 0.600070, acc.: 63.28%] [G loss: 1.385966]\n",
      "epoch:26 step:24985 [D loss: 0.426483, acc.: 83.59%] [G loss: 1.523731]\n",
      "epoch:26 step:24986 [D loss: 0.696808, acc.: 60.16%] [G loss: 1.072182]\n",
      "epoch:26 step:24987 [D loss: 0.444991, acc.: 82.81%] [G loss: 1.472289]\n",
      "epoch:26 step:24988 [D loss: 0.428473, acc.: 83.59%] [G loss: 1.517578]\n",
      "epoch:26 step:24989 [D loss: 0.482904, acc.: 79.69%] [G loss: 1.582007]\n",
      "epoch:26 step:24990 [D loss: 0.514944, acc.: 76.56%] [G loss: 1.605793]\n",
      "epoch:26 step:24991 [D loss: 0.468060, acc.: 85.94%] [G loss: 1.579027]\n",
      "epoch:26 step:24992 [D loss: 0.568710, acc.: 75.78%] [G loss: 1.135379]\n",
      "epoch:26 step:24993 [D loss: 0.503229, acc.: 73.44%] [G loss: 1.170976]\n",
      "epoch:26 step:24994 [D loss: 0.466658, acc.: 78.12%] [G loss: 1.445789]\n",
      "epoch:26 step:24995 [D loss: 0.409409, acc.: 83.59%] [G loss: 1.524867]\n",
      "epoch:26 step:24996 [D loss: 0.540537, acc.: 70.31%] [G loss: 1.346844]\n",
      "epoch:26 step:24997 [D loss: 0.905204, acc.: 45.31%] [G loss: 1.246732]\n",
      "epoch:26 step:24998 [D loss: 0.457151, acc.: 79.69%] [G loss: 1.323631]\n",
      "epoch:26 step:24999 [D loss: 0.651312, acc.: 62.50%] [G loss: 1.573280]\n",
      "epoch:26 step:25000 [D loss: 0.409188, acc.: 82.03%] [G loss: 1.616491]\n",
      "##############\n",
      "[2.6763854  1.92096329 1.92198585 2.91008913 0.99759331 6.78150155\n",
      " 2.37147596 2.76694796 4.06085839 5.79980137]\n",
      "##########\n",
      "epoch:26 step:25001 [D loss: 0.508777, acc.: 72.66%] [G loss: 1.322877]\n",
      "epoch:26 step:25002 [D loss: 0.388232, acc.: 83.59%] [G loss: 1.570867]\n",
      "epoch:26 step:25003 [D loss: 0.670290, acc.: 64.06%] [G loss: 1.338293]\n",
      "epoch:26 step:25004 [D loss: 0.548914, acc.: 70.31%] [G loss: 1.111534]\n",
      "epoch:26 step:25005 [D loss: 0.741961, acc.: 55.47%] [G loss: 1.229899]\n",
      "epoch:26 step:25006 [D loss: 0.634245, acc.: 66.41%] [G loss: 1.156313]\n",
      "epoch:26 step:25007 [D loss: 0.550299, acc.: 71.88%] [G loss: 1.168207]\n",
      "epoch:26 step:25008 [D loss: 0.465669, acc.: 78.91%] [G loss: 2.118956]\n",
      "epoch:26 step:25009 [D loss: 0.719167, acc.: 54.69%] [G loss: 1.446446]\n",
      "epoch:26 step:25010 [D loss: 0.593916, acc.: 64.84%] [G loss: 1.162198]\n",
      "epoch:26 step:25011 [D loss: 0.542083, acc.: 70.31%] [G loss: 1.074876]\n",
      "epoch:26 step:25012 [D loss: 0.651644, acc.: 60.94%] [G loss: 1.202371]\n",
      "epoch:26 step:25013 [D loss: 0.461136, acc.: 78.91%] [G loss: 1.227522]\n",
      "epoch:26 step:25014 [D loss: 0.573385, acc.: 71.88%] [G loss: 1.779636]\n",
      "epoch:26 step:25015 [D loss: 0.646478, acc.: 65.62%] [G loss: 1.806082]\n",
      "epoch:26 step:25016 [D loss: 0.449528, acc.: 81.25%] [G loss: 1.565089]\n",
      "epoch:26 step:25017 [D loss: 0.717703, acc.: 61.72%] [G loss: 1.061890]\n",
      "epoch:26 step:25018 [D loss: 0.418332, acc.: 81.25%] [G loss: 1.496666]\n",
      "epoch:26 step:25019 [D loss: 0.686728, acc.: 60.94%] [G loss: 1.265024]\n",
      "epoch:26 step:25020 [D loss: 0.597905, acc.: 66.41%] [G loss: 1.485193]\n",
      "epoch:26 step:25021 [D loss: 0.531345, acc.: 71.09%] [G loss: 1.582216]\n",
      "epoch:26 step:25022 [D loss: 0.372350, acc.: 86.72%] [G loss: 1.488968]\n",
      "epoch:26 step:25023 [D loss: 0.542839, acc.: 76.56%] [G loss: 1.576626]\n",
      "epoch:26 step:25024 [D loss: 0.568462, acc.: 73.44%] [G loss: 1.309792]\n",
      "epoch:26 step:25025 [D loss: 0.508352, acc.: 77.34%] [G loss: 1.632119]\n",
      "epoch:26 step:25026 [D loss: 0.737942, acc.: 55.47%] [G loss: 0.999545]\n",
      "epoch:26 step:25027 [D loss: 0.568282, acc.: 67.19%] [G loss: 1.481657]\n",
      "epoch:26 step:25028 [D loss: 0.479623, acc.: 79.69%] [G loss: 1.282137]\n",
      "epoch:26 step:25029 [D loss: 0.469659, acc.: 81.25%] [G loss: 1.801243]\n",
      "epoch:26 step:25030 [D loss: 0.831817, acc.: 50.00%] [G loss: 1.624994]\n",
      "epoch:26 step:25031 [D loss: 0.401274, acc.: 82.81%] [G loss: 1.425647]\n",
      "epoch:26 step:25032 [D loss: 0.557293, acc.: 68.75%] [G loss: 1.412225]\n",
      "epoch:26 step:25033 [D loss: 0.584549, acc.: 67.19%] [G loss: 1.224692]\n",
      "epoch:26 step:25034 [D loss: 0.698517, acc.: 56.25%] [G loss: 1.143559]\n",
      "epoch:26 step:25035 [D loss: 0.732398, acc.: 57.81%] [G loss: 1.141819]\n",
      "epoch:26 step:25036 [D loss: 0.488780, acc.: 78.12%] [G loss: 1.029408]\n",
      "epoch:26 step:25037 [D loss: 0.577560, acc.: 69.53%] [G loss: 1.139705]\n",
      "epoch:26 step:25038 [D loss: 0.516519, acc.: 70.31%] [G loss: 1.372312]\n",
      "epoch:26 step:25039 [D loss: 0.566119, acc.: 71.09%] [G loss: 1.503129]\n",
      "epoch:26 step:25040 [D loss: 0.700592, acc.: 57.81%] [G loss: 1.715502]\n",
      "epoch:26 step:25041 [D loss: 0.737099, acc.: 58.59%] [G loss: 1.517161]\n",
      "epoch:26 step:25042 [D loss: 0.552432, acc.: 75.00%] [G loss: 1.314528]\n",
      "epoch:26 step:25043 [D loss: 0.494445, acc.: 78.91%] [G loss: 1.737365]\n",
      "epoch:26 step:25044 [D loss: 0.575383, acc.: 68.75%] [G loss: 1.498711]\n",
      "epoch:26 step:25045 [D loss: 0.591521, acc.: 65.62%] [G loss: 1.832079]\n",
      "epoch:26 step:25046 [D loss: 0.624155, acc.: 64.84%] [G loss: 1.334026]\n",
      "epoch:26 step:25047 [D loss: 0.694499, acc.: 57.03%] [G loss: 1.033154]\n",
      "epoch:26 step:25048 [D loss: 0.408181, acc.: 83.59%] [G loss: 1.630447]\n",
      "epoch:26 step:25049 [D loss: 0.567684, acc.: 72.66%] [G loss: 1.382883]\n",
      "epoch:26 step:25050 [D loss: 0.577911, acc.: 67.97%] [G loss: 1.347610]\n",
      "epoch:26 step:25051 [D loss: 0.582292, acc.: 67.97%] [G loss: 1.650428]\n",
      "epoch:26 step:25052 [D loss: 0.478100, acc.: 75.78%] [G loss: 1.532001]\n",
      "epoch:26 step:25053 [D loss: 0.514585, acc.: 75.78%] [G loss: 1.346975]\n",
      "epoch:26 step:25054 [D loss: 0.408722, acc.: 81.25%] [G loss: 1.538826]\n",
      "epoch:26 step:25055 [D loss: 0.492289, acc.: 78.91%] [G loss: 1.358816]\n",
      "epoch:26 step:25056 [D loss: 0.627989, acc.: 66.41%] [G loss: 0.947222]\n",
      "epoch:26 step:25057 [D loss: 0.634967, acc.: 63.28%] [G loss: 1.186147]\n",
      "epoch:26 step:25058 [D loss: 0.587904, acc.: 67.97%] [G loss: 1.525702]\n",
      "epoch:26 step:25059 [D loss: 0.416629, acc.: 78.12%] [G loss: 1.549867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25060 [D loss: 0.771122, acc.: 54.69%] [G loss: 1.153575]\n",
      "epoch:26 step:25061 [D loss: 0.561515, acc.: 75.00%] [G loss: 1.325565]\n",
      "epoch:26 step:25062 [D loss: 0.578003, acc.: 69.53%] [G loss: 1.271540]\n",
      "epoch:26 step:25063 [D loss: 0.409572, acc.: 79.69%] [G loss: 1.495425]\n",
      "epoch:26 step:25064 [D loss: 0.441455, acc.: 83.59%] [G loss: 1.617921]\n",
      "epoch:26 step:25065 [D loss: 0.547367, acc.: 70.31%] [G loss: 1.836301]\n",
      "epoch:26 step:25066 [D loss: 0.606731, acc.: 67.97%] [G loss: 1.412236]\n",
      "epoch:26 step:25067 [D loss: 0.471351, acc.: 75.00%] [G loss: 1.609351]\n",
      "epoch:26 step:25068 [D loss: 0.466652, acc.: 78.91%] [G loss: 1.193878]\n",
      "epoch:26 step:25069 [D loss: 0.423406, acc.: 79.69%] [G loss: 1.494113]\n",
      "epoch:26 step:25070 [D loss: 0.495953, acc.: 78.12%] [G loss: 1.890331]\n",
      "epoch:26 step:25071 [D loss: 0.579701, acc.: 70.31%] [G loss: 1.680448]\n",
      "epoch:26 step:25072 [D loss: 0.580131, acc.: 71.09%] [G loss: 1.371476]\n",
      "epoch:26 step:25073 [D loss: 0.460844, acc.: 79.69%] [G loss: 1.815891]\n",
      "epoch:26 step:25074 [D loss: 0.524692, acc.: 76.56%] [G loss: 1.674708]\n",
      "epoch:26 step:25075 [D loss: 0.625045, acc.: 65.62%] [G loss: 1.313930]\n",
      "epoch:26 step:25076 [D loss: 0.785107, acc.: 54.69%] [G loss: 1.477432]\n",
      "epoch:26 step:25077 [D loss: 0.556470, acc.: 75.00%] [G loss: 1.441276]\n",
      "epoch:26 step:25078 [D loss: 0.498143, acc.: 75.78%] [G loss: 1.168468]\n",
      "epoch:26 step:25079 [D loss: 0.636380, acc.: 64.06%] [G loss: 1.597559]\n",
      "epoch:26 step:25080 [D loss: 0.356732, acc.: 86.72%] [G loss: 1.827138]\n",
      "epoch:26 step:25081 [D loss: 0.645848, acc.: 63.28%] [G loss: 1.521304]\n",
      "epoch:26 step:25082 [D loss: 0.555590, acc.: 72.66%] [G loss: 1.499660]\n",
      "epoch:26 step:25083 [D loss: 0.656684, acc.: 68.75%] [G loss: 1.206818]\n",
      "epoch:26 step:25084 [D loss: 0.570095, acc.: 69.53%] [G loss: 1.256065]\n",
      "epoch:26 step:25085 [D loss: 0.566433, acc.: 68.75%] [G loss: 1.153712]\n",
      "epoch:26 step:25086 [D loss: 0.453377, acc.: 75.78%] [G loss: 1.495649]\n",
      "epoch:26 step:25087 [D loss: 0.719783, acc.: 58.59%] [G loss: 1.156333]\n",
      "epoch:26 step:25088 [D loss: 0.735955, acc.: 57.03%] [G loss: 1.607035]\n",
      "epoch:26 step:25089 [D loss: 0.494244, acc.: 76.56%] [G loss: 1.303672]\n",
      "epoch:26 step:25090 [D loss: 0.401690, acc.: 86.72%] [G loss: 1.557933]\n",
      "epoch:26 step:25091 [D loss: 0.601175, acc.: 63.28%] [G loss: 1.479124]\n",
      "epoch:26 step:25092 [D loss: 0.596840, acc.: 69.53%] [G loss: 1.144699]\n",
      "epoch:26 step:25093 [D loss: 0.664223, acc.: 63.28%] [G loss: 1.308480]\n",
      "epoch:26 step:25094 [D loss: 0.502302, acc.: 76.56%] [G loss: 1.541278]\n",
      "epoch:26 step:25095 [D loss: 0.613859, acc.: 62.50%] [G loss: 1.241881]\n",
      "epoch:26 step:25096 [D loss: 0.506235, acc.: 75.78%] [G loss: 1.217055]\n",
      "epoch:26 step:25097 [D loss: 0.667230, acc.: 60.94%] [G loss: 1.275411]\n",
      "epoch:26 step:25098 [D loss: 0.503316, acc.: 74.22%] [G loss: 1.552640]\n",
      "epoch:26 step:25099 [D loss: 0.571538, acc.: 71.09%] [G loss: 1.523902]\n",
      "epoch:26 step:25100 [D loss: 0.690945, acc.: 57.03%] [G loss: 1.479290]\n",
      "epoch:26 step:25101 [D loss: 0.568626, acc.: 71.09%] [G loss: 1.031998]\n",
      "epoch:26 step:25102 [D loss: 0.652287, acc.: 57.03%] [G loss: 1.281178]\n",
      "epoch:26 step:25103 [D loss: 0.513590, acc.: 75.78%] [G loss: 1.501592]\n",
      "epoch:26 step:25104 [D loss: 0.386389, acc.: 86.72%] [G loss: 1.111899]\n",
      "epoch:26 step:25105 [D loss: 0.731308, acc.: 58.59%] [G loss: 1.068790]\n",
      "epoch:26 step:25106 [D loss: 0.361378, acc.: 90.62%] [G loss: 1.107684]\n",
      "epoch:26 step:25107 [D loss: 0.455742, acc.: 79.69%] [G loss: 1.338005]\n",
      "epoch:26 step:25108 [D loss: 0.551834, acc.: 72.66%] [G loss: 1.313226]\n",
      "epoch:26 step:25109 [D loss: 0.433564, acc.: 82.03%] [G loss: 1.591591]\n",
      "epoch:26 step:25110 [D loss: 0.518588, acc.: 75.00%] [G loss: 1.263562]\n",
      "epoch:26 step:25111 [D loss: 0.783311, acc.: 49.22%] [G loss: 1.138762]\n",
      "epoch:26 step:25112 [D loss: 0.664899, acc.: 60.94%] [G loss: 1.279685]\n",
      "epoch:26 step:25113 [D loss: 0.500263, acc.: 78.91%] [G loss: 1.422925]\n",
      "epoch:26 step:25114 [D loss: 0.550015, acc.: 75.00%] [G loss: 1.437151]\n",
      "epoch:26 step:25115 [D loss: 0.530410, acc.: 71.88%] [G loss: 1.262311]\n",
      "epoch:26 step:25116 [D loss: 0.548048, acc.: 76.56%] [G loss: 1.972619]\n",
      "epoch:26 step:25117 [D loss: 0.524424, acc.: 78.12%] [G loss: 1.598733]\n",
      "epoch:26 step:25118 [D loss: 0.510273, acc.: 74.22%] [G loss: 1.121755]\n",
      "epoch:26 step:25119 [D loss: 0.479751, acc.: 78.91%] [G loss: 1.165996]\n",
      "epoch:26 step:25120 [D loss: 0.651811, acc.: 63.28%] [G loss: 1.329494]\n",
      "epoch:26 step:25121 [D loss: 0.437378, acc.: 77.34%] [G loss: 1.368131]\n",
      "epoch:26 step:25122 [D loss: 0.731324, acc.: 58.59%] [G loss: 1.169270]\n",
      "epoch:26 step:25123 [D loss: 0.669788, acc.: 65.62%] [G loss: 1.295817]\n",
      "epoch:26 step:25124 [D loss: 0.446585, acc.: 82.03%] [G loss: 1.490427]\n",
      "epoch:26 step:25125 [D loss: 0.811573, acc.: 47.66%] [G loss: 1.071609]\n",
      "epoch:26 step:25126 [D loss: 0.415803, acc.: 84.38%] [G loss: 1.953125]\n",
      "epoch:26 step:25127 [D loss: 0.533454, acc.: 75.00%] [G loss: 1.337600]\n",
      "epoch:26 step:25128 [D loss: 0.653081, acc.: 64.06%] [G loss: 1.228752]\n",
      "epoch:26 step:25129 [D loss: 0.549700, acc.: 71.09%] [G loss: 1.337656]\n",
      "epoch:26 step:25130 [D loss: 0.552306, acc.: 67.97%] [G loss: 1.740995]\n",
      "epoch:26 step:25131 [D loss: 0.603990, acc.: 71.88%] [G loss: 1.641670]\n",
      "epoch:26 step:25132 [D loss: 0.604033, acc.: 63.28%] [G loss: 1.728149]\n",
      "epoch:26 step:25133 [D loss: 0.472995, acc.: 78.12%] [G loss: 1.415126]\n",
      "epoch:26 step:25134 [D loss: 0.490623, acc.: 79.69%] [G loss: 1.973058]\n",
      "epoch:26 step:25135 [D loss: 0.650114, acc.: 64.06%] [G loss: 1.515424]\n",
      "epoch:26 step:25136 [D loss: 0.656979, acc.: 61.72%] [G loss: 1.260110]\n",
      "epoch:26 step:25137 [D loss: 0.552025, acc.: 75.78%] [G loss: 1.237450]\n",
      "epoch:26 step:25138 [D loss: 0.438704, acc.: 80.47%] [G loss: 1.362261]\n",
      "epoch:26 step:25139 [D loss: 0.589861, acc.: 67.97%] [G loss: 1.164910]\n",
      "epoch:26 step:25140 [D loss: 0.392299, acc.: 85.94%] [G loss: 1.612642]\n",
      "epoch:26 step:25141 [D loss: 0.604575, acc.: 71.09%] [G loss: 1.572209]\n",
      "epoch:26 step:25142 [D loss: 0.677185, acc.: 59.38%] [G loss: 1.526891]\n",
      "epoch:26 step:25143 [D loss: 0.628795, acc.: 65.62%] [G loss: 1.381694]\n",
      "epoch:26 step:25144 [D loss: 0.351534, acc.: 87.50%] [G loss: 1.809867]\n",
      "epoch:26 step:25145 [D loss: 0.652762, acc.: 65.62%] [G loss: 1.637208]\n",
      "epoch:26 step:25146 [D loss: 0.464895, acc.: 79.69%] [G loss: 1.157109]\n",
      "epoch:26 step:25147 [D loss: 0.467819, acc.: 78.91%] [G loss: 1.099276]\n",
      "epoch:26 step:25148 [D loss: 0.734438, acc.: 55.47%] [G loss: 1.207316]\n",
      "epoch:26 step:25149 [D loss: 0.492781, acc.: 78.12%] [G loss: 1.811203]\n",
      "epoch:26 step:25150 [D loss: 0.587929, acc.: 69.53%] [G loss: 1.369862]\n",
      "epoch:26 step:25151 [D loss: 0.542809, acc.: 71.09%] [G loss: 1.603018]\n",
      "epoch:26 step:25152 [D loss: 0.597192, acc.: 66.41%] [G loss: 1.503235]\n",
      "epoch:26 step:25153 [D loss: 0.625725, acc.: 69.53%] [G loss: 1.469852]\n",
      "epoch:26 step:25154 [D loss: 0.553384, acc.: 72.66%] [G loss: 1.683255]\n",
      "epoch:26 step:25155 [D loss: 0.700973, acc.: 61.72%] [G loss: 1.127831]\n",
      "epoch:26 step:25156 [D loss: 0.573580, acc.: 66.41%] [G loss: 1.391826]\n",
      "epoch:26 step:25157 [D loss: 0.594537, acc.: 67.19%] [G loss: 1.508919]\n",
      "epoch:26 step:25158 [D loss: 0.688574, acc.: 57.03%] [G loss: 1.203761]\n",
      "epoch:26 step:25159 [D loss: 0.643506, acc.: 64.06%] [G loss: 1.443706]\n",
      "epoch:26 step:25160 [D loss: 0.654475, acc.: 64.84%] [G loss: 1.396137]\n",
      "epoch:26 step:25161 [D loss: 0.491788, acc.: 75.78%] [G loss: 1.152936]\n",
      "epoch:26 step:25162 [D loss: 0.697173, acc.: 58.59%] [G loss: 1.389492]\n",
      "epoch:26 step:25163 [D loss: 0.698542, acc.: 59.38%] [G loss: 1.363833]\n",
      "epoch:26 step:25164 [D loss: 0.551304, acc.: 69.53%] [G loss: 1.315197]\n",
      "epoch:26 step:25165 [D loss: 0.611449, acc.: 67.19%] [G loss: 1.572960]\n",
      "epoch:26 step:25166 [D loss: 0.517314, acc.: 73.44%] [G loss: 1.270007]\n",
      "epoch:26 step:25167 [D loss: 0.487110, acc.: 75.00%] [G loss: 1.641762]\n",
      "epoch:26 step:25168 [D loss: 0.455162, acc.: 78.91%] [G loss: 1.220601]\n",
      "epoch:26 step:25169 [D loss: 0.681619, acc.: 56.25%] [G loss: 1.387322]\n",
      "epoch:26 step:25170 [D loss: 0.405285, acc.: 85.16%] [G loss: 1.744136]\n",
      "epoch:26 step:25171 [D loss: 0.686989, acc.: 61.72%] [G loss: 1.308844]\n",
      "epoch:26 step:25172 [D loss: 0.545511, acc.: 70.31%] [G loss: 1.861813]\n",
      "epoch:26 step:25173 [D loss: 0.617167, acc.: 63.28%] [G loss: 1.548115]\n",
      "epoch:26 step:25174 [D loss: 0.519232, acc.: 71.88%] [G loss: 1.339870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25175 [D loss: 0.711001, acc.: 66.41%] [G loss: 1.314398]\n",
      "epoch:26 step:25176 [D loss: 0.519656, acc.: 71.88%] [G loss: 1.340460]\n",
      "epoch:26 step:25177 [D loss: 0.517004, acc.: 75.00%] [G loss: 1.641012]\n",
      "epoch:26 step:25178 [D loss: 0.473075, acc.: 77.34%] [G loss: 1.503365]\n",
      "epoch:26 step:25179 [D loss: 0.461746, acc.: 85.16%] [G loss: 1.748395]\n",
      "epoch:26 step:25180 [D loss: 0.505050, acc.: 76.56%] [G loss: 1.932856]\n",
      "epoch:26 step:25181 [D loss: 0.468462, acc.: 81.25%] [G loss: 1.774695]\n",
      "epoch:26 step:25182 [D loss: 0.556456, acc.: 71.09%] [G loss: 1.144473]\n",
      "epoch:26 step:25183 [D loss: 0.461797, acc.: 79.69%] [G loss: 1.506718]\n",
      "epoch:26 step:25184 [D loss: 0.711780, acc.: 56.25%] [G loss: 0.940902]\n",
      "epoch:26 step:25185 [D loss: 0.599219, acc.: 67.97%] [G loss: 1.555926]\n",
      "epoch:26 step:25186 [D loss: 0.581042, acc.: 69.53%] [G loss: 1.090310]\n",
      "epoch:26 step:25187 [D loss: 0.595775, acc.: 70.31%] [G loss: 2.086586]\n",
      "epoch:26 step:25188 [D loss: 0.619126, acc.: 62.50%] [G loss: 1.356941]\n",
      "epoch:26 step:25189 [D loss: 0.415422, acc.: 81.25%] [G loss: 1.259658]\n",
      "epoch:26 step:25190 [D loss: 0.787819, acc.: 54.69%] [G loss: 1.131772]\n",
      "epoch:26 step:25191 [D loss: 0.555983, acc.: 71.09%] [G loss: 1.063131]\n",
      "epoch:26 step:25192 [D loss: 0.486116, acc.: 79.69%] [G loss: 1.270296]\n",
      "epoch:26 step:25193 [D loss: 0.658505, acc.: 64.06%] [G loss: 1.201669]\n",
      "epoch:26 step:25194 [D loss: 0.632213, acc.: 65.62%] [G loss: 1.313612]\n",
      "epoch:26 step:25195 [D loss: 0.517877, acc.: 75.00%] [G loss: 1.370587]\n",
      "epoch:26 step:25196 [D loss: 0.530779, acc.: 71.09%] [G loss: 1.378495]\n",
      "epoch:26 step:25197 [D loss: 0.307053, acc.: 91.41%] [G loss: 1.692480]\n",
      "epoch:26 step:25198 [D loss: 0.556494, acc.: 71.09%] [G loss: 1.416927]\n",
      "epoch:26 step:25199 [D loss: 0.506714, acc.: 77.34%] [G loss: 1.534674]\n",
      "epoch:26 step:25200 [D loss: 0.497409, acc.: 72.66%] [G loss: 1.390545]\n",
      "##############\n",
      "[2.78454556 2.08292924 2.0975988  3.18572375 1.08876569 5.48312126\n",
      " 2.21965415 2.73411215 3.89399975 7.14868929]\n",
      "##########\n",
      "epoch:26 step:25201 [D loss: 0.639087, acc.: 64.06%] [G loss: 1.078813]\n",
      "epoch:26 step:25202 [D loss: 0.361080, acc.: 89.84%] [G loss: 1.482214]\n",
      "epoch:26 step:25203 [D loss: 0.461236, acc.: 82.81%] [G loss: 1.503595]\n",
      "epoch:26 step:25204 [D loss: 0.362814, acc.: 88.28%] [G loss: 1.532502]\n",
      "epoch:26 step:25205 [D loss: 0.633184, acc.: 67.97%] [G loss: 1.178540]\n",
      "epoch:26 step:25206 [D loss: 0.741015, acc.: 58.59%] [G loss: 1.213466]\n",
      "epoch:26 step:25207 [D loss: 0.386979, acc.: 82.81%] [G loss: 1.357150]\n",
      "epoch:26 step:25208 [D loss: 0.503539, acc.: 73.44%] [G loss: 1.259841]\n",
      "epoch:26 step:25209 [D loss: 0.492985, acc.: 75.00%] [G loss: 1.203364]\n",
      "epoch:26 step:25210 [D loss: 0.585436, acc.: 64.84%] [G loss: 1.167627]\n",
      "epoch:26 step:25211 [D loss: 0.566314, acc.: 67.97%] [G loss: 1.379277]\n",
      "epoch:26 step:25212 [D loss: 0.395066, acc.: 84.38%] [G loss: 1.554965]\n",
      "epoch:26 step:25213 [D loss: 0.689747, acc.: 60.16%] [G loss: 1.201954]\n",
      "epoch:26 step:25214 [D loss: 0.449947, acc.: 82.03%] [G loss: 1.593034]\n",
      "epoch:26 step:25215 [D loss: 0.604871, acc.: 67.97%] [G loss: 1.357263]\n",
      "epoch:26 step:25216 [D loss: 0.712822, acc.: 59.38%] [G loss: 1.195199]\n",
      "epoch:26 step:25217 [D loss: 0.486402, acc.: 79.69%] [G loss: 1.260415]\n",
      "epoch:26 step:25218 [D loss: 0.359003, acc.: 89.84%] [G loss: 2.053022]\n",
      "epoch:26 step:25219 [D loss: 0.512169, acc.: 74.22%] [G loss: 1.061454]\n",
      "epoch:26 step:25220 [D loss: 0.427005, acc.: 82.81%] [G loss: 1.918898]\n",
      "epoch:26 step:25221 [D loss: 0.447067, acc.: 82.81%] [G loss: 1.540284]\n",
      "epoch:26 step:25222 [D loss: 0.460493, acc.: 77.34%] [G loss: 1.613326]\n",
      "epoch:26 step:25223 [D loss: 0.413294, acc.: 82.03%] [G loss: 1.591413]\n",
      "epoch:26 step:25224 [D loss: 0.527615, acc.: 71.88%] [G loss: 1.504342]\n",
      "epoch:26 step:25225 [D loss: 0.642658, acc.: 64.06%] [G loss: 1.175447]\n",
      "epoch:26 step:25226 [D loss: 0.501565, acc.: 74.22%] [G loss: 1.588991]\n",
      "epoch:26 step:25227 [D loss: 0.521166, acc.: 78.91%] [G loss: 1.454308]\n",
      "epoch:26 step:25228 [D loss: 0.580479, acc.: 73.44%] [G loss: 1.331667]\n",
      "epoch:26 step:25229 [D loss: 0.531882, acc.: 75.00%] [G loss: 1.234197]\n",
      "epoch:26 step:25230 [D loss: 0.682696, acc.: 61.72%] [G loss: 0.924253]\n",
      "epoch:26 step:25231 [D loss: 0.538010, acc.: 70.31%] [G loss: 1.790473]\n",
      "epoch:26 step:25232 [D loss: 0.334135, acc.: 89.06%] [G loss: 1.984633]\n",
      "epoch:26 step:25233 [D loss: 0.561398, acc.: 71.88%] [G loss: 1.634758]\n",
      "epoch:26 step:25234 [D loss: 0.477731, acc.: 78.12%] [G loss: 1.434299]\n",
      "epoch:26 step:25235 [D loss: 0.499039, acc.: 78.12%] [G loss: 1.518154]\n",
      "epoch:26 step:25236 [D loss: 0.441510, acc.: 82.03%] [G loss: 1.418876]\n",
      "epoch:26 step:25237 [D loss: 0.609836, acc.: 66.41%] [G loss: 1.253528]\n",
      "epoch:26 step:25238 [D loss: 0.830805, acc.: 52.34%] [G loss: 1.528853]\n",
      "epoch:26 step:25239 [D loss: 0.499395, acc.: 75.00%] [G loss: 1.741186]\n",
      "epoch:26 step:25240 [D loss: 0.440260, acc.: 81.25%] [G loss: 1.246219]\n",
      "epoch:26 step:25241 [D loss: 0.549069, acc.: 73.44%] [G loss: 1.137300]\n",
      "epoch:26 step:25242 [D loss: 0.518777, acc.: 77.34%] [G loss: 1.574053]\n",
      "epoch:26 step:25243 [D loss: 0.576228, acc.: 75.00%] [G loss: 1.240405]\n",
      "epoch:26 step:25244 [D loss: 0.432422, acc.: 82.81%] [G loss: 1.410955]\n",
      "epoch:26 step:25245 [D loss: 0.494467, acc.: 79.69%] [G loss: 1.612589]\n",
      "epoch:26 step:25246 [D loss: 0.429709, acc.: 81.25%] [G loss: 1.597196]\n",
      "epoch:26 step:25247 [D loss: 0.605596, acc.: 67.19%] [G loss: 1.344152]\n",
      "epoch:26 step:25248 [D loss: 0.479280, acc.: 78.12%] [G loss: 1.460477]\n",
      "epoch:26 step:25249 [D loss: 0.658394, acc.: 63.28%] [G loss: 1.530124]\n",
      "epoch:26 step:25250 [D loss: 0.624472, acc.: 64.84%] [G loss: 1.571677]\n",
      "epoch:26 step:25251 [D loss: 0.776759, acc.: 54.69%] [G loss: 1.323908]\n",
      "epoch:26 step:25252 [D loss: 0.653696, acc.: 60.94%] [G loss: 1.004676]\n",
      "epoch:26 step:25253 [D loss: 0.481460, acc.: 75.78%] [G loss: 1.535992]\n",
      "epoch:26 step:25254 [D loss: 0.526003, acc.: 76.56%] [G loss: 1.508994]\n",
      "epoch:26 step:25255 [D loss: 0.421950, acc.: 82.81%] [G loss: 1.205624]\n",
      "epoch:26 step:25256 [D loss: 0.554068, acc.: 72.66%] [G loss: 1.171862]\n",
      "epoch:26 step:25257 [D loss: 0.458567, acc.: 80.47%] [G loss: 0.993527]\n",
      "epoch:26 step:25258 [D loss: 0.630510, acc.: 65.62%] [G loss: 1.427676]\n",
      "epoch:26 step:25259 [D loss: 0.510217, acc.: 74.22%] [G loss: 1.643835]\n",
      "epoch:26 step:25260 [D loss: 0.456478, acc.: 78.12%] [G loss: 1.479950]\n",
      "epoch:26 step:25261 [D loss: 0.516636, acc.: 77.34%] [G loss: 1.419536]\n",
      "epoch:26 step:25262 [D loss: 0.554687, acc.: 72.66%] [G loss: 1.577592]\n",
      "epoch:26 step:25263 [D loss: 0.599232, acc.: 69.53%] [G loss: 1.525911]\n",
      "epoch:26 step:25264 [D loss: 0.581362, acc.: 67.19%] [G loss: 1.250134]\n",
      "epoch:26 step:25265 [D loss: 0.647919, acc.: 63.28%] [G loss: 1.336828]\n",
      "epoch:26 step:25266 [D loss: 0.535080, acc.: 75.00%] [G loss: 1.593893]\n",
      "epoch:26 step:25267 [D loss: 0.591301, acc.: 68.75%] [G loss: 1.243476]\n",
      "epoch:26 step:25268 [D loss: 0.453798, acc.: 80.47%] [G loss: 1.328866]\n",
      "epoch:26 step:25269 [D loss: 0.578052, acc.: 69.53%] [G loss: 1.222117]\n",
      "epoch:26 step:25270 [D loss: 0.595098, acc.: 72.66%] [G loss: 1.929125]\n",
      "epoch:26 step:25271 [D loss: 0.535383, acc.: 73.44%] [G loss: 1.204363]\n",
      "epoch:26 step:25272 [D loss: 0.497782, acc.: 76.56%] [G loss: 1.604342]\n",
      "epoch:26 step:25273 [D loss: 0.484186, acc.: 80.47%] [G loss: 1.597872]\n",
      "epoch:26 step:25274 [D loss: 0.693532, acc.: 57.81%] [G loss: 1.232313]\n",
      "epoch:26 step:25275 [D loss: 0.593296, acc.: 72.66%] [G loss: 1.312115]\n",
      "epoch:26 step:25276 [D loss: 0.718990, acc.: 56.25%] [G loss: 1.522438]\n",
      "epoch:26 step:25277 [D loss: 0.616054, acc.: 68.75%] [G loss: 1.049871]\n",
      "epoch:26 step:25278 [D loss: 0.568835, acc.: 70.31%] [G loss: 2.003797]\n",
      "epoch:26 step:25279 [D loss: 0.649306, acc.: 63.28%] [G loss: 1.130348]\n",
      "epoch:26 step:25280 [D loss: 0.604602, acc.: 71.09%] [G loss: 1.062585]\n",
      "epoch:26 step:25281 [D loss: 0.739892, acc.: 60.94%] [G loss: 1.385450]\n",
      "epoch:26 step:25282 [D loss: 0.520347, acc.: 76.56%] [G loss: 1.767959]\n",
      "epoch:26 step:25283 [D loss: 0.556189, acc.: 70.31%] [G loss: 1.412000]\n",
      "epoch:26 step:25284 [D loss: 0.530156, acc.: 74.22%] [G loss: 1.276916]\n",
      "epoch:26 step:25285 [D loss: 0.710611, acc.: 57.81%] [G loss: 1.178494]\n",
      "epoch:26 step:25286 [D loss: 0.600807, acc.: 68.75%] [G loss: 1.642027]\n",
      "epoch:26 step:25287 [D loss: 0.488009, acc.: 76.56%] [G loss: 1.544184]\n",
      "epoch:26 step:25288 [D loss: 0.520977, acc.: 76.56%] [G loss: 1.447584]\n",
      "epoch:26 step:25289 [D loss: 0.681547, acc.: 61.72%] [G loss: 1.254864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25290 [D loss: 0.450475, acc.: 76.56%] [G loss: 1.424383]\n",
      "epoch:26 step:25291 [D loss: 0.706794, acc.: 58.59%] [G loss: 1.464446]\n",
      "epoch:26 step:25292 [D loss: 0.773558, acc.: 54.69%] [G loss: 1.284199]\n",
      "epoch:26 step:25293 [D loss: 0.560765, acc.: 66.41%] [G loss: 2.064811]\n",
      "epoch:26 step:25294 [D loss: 0.491659, acc.: 75.00%] [G loss: 1.990021]\n",
      "epoch:26 step:25295 [D loss: 0.610551, acc.: 64.06%] [G loss: 1.425530]\n",
      "epoch:26 step:25296 [D loss: 0.592891, acc.: 67.97%] [G loss: 1.523578]\n",
      "epoch:26 step:25297 [D loss: 0.535676, acc.: 75.00%] [G loss: 1.267521]\n",
      "epoch:26 step:25298 [D loss: 0.455684, acc.: 81.25%] [G loss: 1.678227]\n",
      "epoch:26 step:25299 [D loss: 0.607540, acc.: 60.16%] [G loss: 1.387487]\n",
      "epoch:27 step:25300 [D loss: 0.572182, acc.: 68.75%] [G loss: 1.420017]\n",
      "epoch:27 step:25301 [D loss: 0.466563, acc.: 78.12%] [G loss: 1.304716]\n",
      "epoch:27 step:25302 [D loss: 0.519098, acc.: 77.34%] [G loss: 1.488566]\n",
      "epoch:27 step:25303 [D loss: 0.563729, acc.: 66.41%] [G loss: 1.480757]\n",
      "epoch:27 step:25304 [D loss: 0.658121, acc.: 64.06%] [G loss: 1.354819]\n",
      "epoch:27 step:25305 [D loss: 0.621907, acc.: 65.62%] [G loss: 1.527451]\n",
      "epoch:27 step:25306 [D loss: 0.602884, acc.: 64.84%] [G loss: 1.226187]\n",
      "epoch:27 step:25307 [D loss: 0.563812, acc.: 67.19%] [G loss: 1.517664]\n",
      "epoch:27 step:25308 [D loss: 0.447421, acc.: 78.12%] [G loss: 1.349140]\n",
      "epoch:27 step:25309 [D loss: 0.611426, acc.: 63.28%] [G loss: 1.833114]\n",
      "epoch:27 step:25310 [D loss: 0.485609, acc.: 78.12%] [G loss: 1.313269]\n",
      "epoch:27 step:25311 [D loss: 0.486667, acc.: 81.25%] [G loss: 1.476247]\n",
      "epoch:27 step:25312 [D loss: 0.744368, acc.: 57.81%] [G loss: 1.381954]\n",
      "epoch:27 step:25313 [D loss: 0.607780, acc.: 69.53%] [G loss: 1.481049]\n",
      "epoch:27 step:25314 [D loss: 0.501273, acc.: 77.34%] [G loss: 1.362635]\n",
      "epoch:27 step:25315 [D loss: 0.520569, acc.: 78.12%] [G loss: 1.883532]\n",
      "epoch:27 step:25316 [D loss: 0.647439, acc.: 67.19%] [G loss: 1.194365]\n",
      "epoch:27 step:25317 [D loss: 0.572454, acc.: 66.41%] [G loss: 1.437390]\n",
      "epoch:27 step:25318 [D loss: 0.636096, acc.: 64.06%] [G loss: 1.157674]\n",
      "epoch:27 step:25319 [D loss: 0.766180, acc.: 58.59%] [G loss: 1.344832]\n",
      "epoch:27 step:25320 [D loss: 0.543648, acc.: 67.97%] [G loss: 1.530850]\n",
      "epoch:27 step:25321 [D loss: 0.574193, acc.: 66.41%] [G loss: 1.483643]\n",
      "epoch:27 step:25322 [D loss: 0.630492, acc.: 60.94%] [G loss: 1.074082]\n",
      "epoch:27 step:25323 [D loss: 0.374253, acc.: 87.50%] [G loss: 1.261318]\n",
      "epoch:27 step:25324 [D loss: 0.630546, acc.: 64.06%] [G loss: 1.056689]\n",
      "epoch:27 step:25325 [D loss: 0.616522, acc.: 69.53%] [G loss: 1.510015]\n",
      "epoch:27 step:25326 [D loss: 0.548599, acc.: 70.31%] [G loss: 1.425843]\n",
      "epoch:27 step:25327 [D loss: 0.503273, acc.: 76.56%] [G loss: 1.163572]\n",
      "epoch:27 step:25328 [D loss: 0.724385, acc.: 56.25%] [G loss: 1.431038]\n",
      "epoch:27 step:25329 [D loss: 0.555045, acc.: 67.97%] [G loss: 1.094300]\n",
      "epoch:27 step:25330 [D loss: 0.457966, acc.: 76.56%] [G loss: 1.312232]\n",
      "epoch:27 step:25331 [D loss: 0.579834, acc.: 71.88%] [G loss: 1.215140]\n",
      "epoch:27 step:25332 [D loss: 0.559608, acc.: 66.41%] [G loss: 1.334052]\n",
      "epoch:27 step:25333 [D loss: 0.502220, acc.: 71.88%] [G loss: 1.723276]\n",
      "epoch:27 step:25334 [D loss: 0.545168, acc.: 68.75%] [G loss: 1.118605]\n",
      "epoch:27 step:25335 [D loss: 0.683276, acc.: 57.03%] [G loss: 1.427567]\n",
      "epoch:27 step:25336 [D loss: 0.439087, acc.: 80.47%] [G loss: 1.589217]\n",
      "epoch:27 step:25337 [D loss: 0.481075, acc.: 75.78%] [G loss: 1.117555]\n",
      "epoch:27 step:25338 [D loss: 0.528162, acc.: 76.56%] [G loss: 1.571248]\n",
      "epoch:27 step:25339 [D loss: 0.575867, acc.: 74.22%] [G loss: 1.589202]\n",
      "epoch:27 step:25340 [D loss: 0.496928, acc.: 78.91%] [G loss: 1.511548]\n",
      "epoch:27 step:25341 [D loss: 0.666459, acc.: 63.28%] [G loss: 1.337063]\n",
      "epoch:27 step:25342 [D loss: 0.464378, acc.: 82.81%] [G loss: 1.241993]\n",
      "epoch:27 step:25343 [D loss: 0.620062, acc.: 70.31%] [G loss: 1.506947]\n",
      "epoch:27 step:25344 [D loss: 0.538958, acc.: 73.44%] [G loss: 1.249779]\n",
      "epoch:27 step:25345 [D loss: 0.497477, acc.: 80.47%] [G loss: 1.497848]\n",
      "epoch:27 step:25346 [D loss: 0.543733, acc.: 71.09%] [G loss: 1.098954]\n",
      "epoch:27 step:25347 [D loss: 0.592640, acc.: 69.53%] [G loss: 0.973336]\n",
      "epoch:27 step:25348 [D loss: 0.471076, acc.: 80.47%] [G loss: 1.380748]\n",
      "epoch:27 step:25349 [D loss: 0.499513, acc.: 74.22%] [G loss: 1.465884]\n",
      "epoch:27 step:25350 [D loss: 0.625329, acc.: 67.19%] [G loss: 1.515864]\n",
      "epoch:27 step:25351 [D loss: 0.401004, acc.: 87.50%] [G loss: 1.741626]\n",
      "epoch:27 step:25352 [D loss: 0.419205, acc.: 85.94%] [G loss: 1.607180]\n",
      "epoch:27 step:25353 [D loss: 0.417509, acc.: 82.03%] [G loss: 1.744102]\n",
      "epoch:27 step:25354 [D loss: 0.458246, acc.: 79.69%] [G loss: 1.300683]\n",
      "epoch:27 step:25355 [D loss: 0.618684, acc.: 69.53%] [G loss: 1.553191]\n",
      "epoch:27 step:25356 [D loss: 0.610179, acc.: 66.41%] [G loss: 1.056098]\n",
      "epoch:27 step:25357 [D loss: 0.504701, acc.: 78.12%] [G loss: 1.351711]\n",
      "epoch:27 step:25358 [D loss: 0.435799, acc.: 79.69%] [G loss: 1.571764]\n",
      "epoch:27 step:25359 [D loss: 0.507181, acc.: 75.00%] [G loss: 1.835420]\n",
      "epoch:27 step:25360 [D loss: 0.692638, acc.: 57.81%] [G loss: 1.297492]\n",
      "epoch:27 step:25361 [D loss: 0.599864, acc.: 66.41%] [G loss: 0.831408]\n",
      "epoch:27 step:25362 [D loss: 0.544312, acc.: 75.78%] [G loss: 1.313896]\n",
      "epoch:27 step:25363 [D loss: 0.451199, acc.: 80.47%] [G loss: 1.508560]\n",
      "epoch:27 step:25364 [D loss: 0.510651, acc.: 76.56%] [G loss: 1.585158]\n",
      "epoch:27 step:25365 [D loss: 0.674510, acc.: 64.06%] [G loss: 1.592628]\n",
      "epoch:27 step:25366 [D loss: 0.543885, acc.: 70.31%] [G loss: 1.580568]\n",
      "epoch:27 step:25367 [D loss: 0.485355, acc.: 78.12%] [G loss: 1.492536]\n",
      "epoch:27 step:25368 [D loss: 0.508565, acc.: 75.00%] [G loss: 1.548646]\n",
      "epoch:27 step:25369 [D loss: 0.635955, acc.: 66.41%] [G loss: 1.660259]\n",
      "epoch:27 step:25370 [D loss: 0.628152, acc.: 65.62%] [G loss: 1.784632]\n",
      "epoch:27 step:25371 [D loss: 0.528526, acc.: 75.78%] [G loss: 1.519994]\n",
      "epoch:27 step:25372 [D loss: 0.342250, acc.: 86.72%] [G loss: 1.609304]\n",
      "epoch:27 step:25373 [D loss: 0.512275, acc.: 78.91%] [G loss: 1.542801]\n",
      "epoch:27 step:25374 [D loss: 0.670053, acc.: 60.94%] [G loss: 1.610632]\n",
      "epoch:27 step:25375 [D loss: 0.723473, acc.: 57.81%] [G loss: 1.153871]\n",
      "epoch:27 step:25376 [D loss: 0.628750, acc.: 63.28%] [G loss: 1.615139]\n",
      "epoch:27 step:25377 [D loss: 0.678851, acc.: 62.50%] [G loss: 1.166710]\n",
      "epoch:27 step:25378 [D loss: 0.693999, acc.: 55.47%] [G loss: 1.125475]\n",
      "epoch:27 step:25379 [D loss: 0.546663, acc.: 77.34%] [G loss: 1.173707]\n",
      "epoch:27 step:25380 [D loss: 0.766403, acc.: 52.34%] [G loss: 1.048977]\n",
      "epoch:27 step:25381 [D loss: 0.324274, acc.: 89.84%] [G loss: 1.668044]\n",
      "epoch:27 step:25382 [D loss: 0.716745, acc.: 56.25%] [G loss: 1.466546]\n",
      "epoch:27 step:25383 [D loss: 0.600065, acc.: 66.41%] [G loss: 1.479224]\n",
      "epoch:27 step:25384 [D loss: 0.636025, acc.: 61.72%] [G loss: 1.140526]\n",
      "epoch:27 step:25385 [D loss: 0.768546, acc.: 57.81%] [G loss: 1.122164]\n",
      "epoch:27 step:25386 [D loss: 0.518242, acc.: 74.22%] [G loss: 1.220213]\n",
      "epoch:27 step:25387 [D loss: 0.557693, acc.: 71.88%] [G loss: 1.196132]\n",
      "epoch:27 step:25388 [D loss: 0.762692, acc.: 51.56%] [G loss: 1.328272]\n",
      "epoch:27 step:25389 [D loss: 0.459923, acc.: 77.34%] [G loss: 1.512830]\n",
      "epoch:27 step:25390 [D loss: 0.607373, acc.: 64.06%] [G loss: 0.982107]\n",
      "epoch:27 step:25391 [D loss: 0.463247, acc.: 84.38%] [G loss: 1.460916]\n",
      "epoch:27 step:25392 [D loss: 0.596444, acc.: 68.75%] [G loss: 1.302088]\n",
      "epoch:27 step:25393 [D loss: 0.363347, acc.: 85.94%] [G loss: 1.572381]\n",
      "epoch:27 step:25394 [D loss: 0.685952, acc.: 63.28%] [G loss: 0.838061]\n",
      "epoch:27 step:25395 [D loss: 0.554987, acc.: 69.53%] [G loss: 1.222570]\n",
      "epoch:27 step:25396 [D loss: 0.500557, acc.: 73.44%] [G loss: 1.439241]\n",
      "epoch:27 step:25397 [D loss: 0.463311, acc.: 78.91%] [G loss: 1.531368]\n",
      "epoch:27 step:25398 [D loss: 0.568111, acc.: 72.66%] [G loss: 1.531151]\n",
      "epoch:27 step:25399 [D loss: 0.544235, acc.: 73.44%] [G loss: 0.989709]\n",
      "epoch:27 step:25400 [D loss: 0.539181, acc.: 74.22%] [G loss: 1.345023]\n",
      "##############\n",
      "[2.80121595 2.20715248 2.14502503 2.77783424 1.12866255 5.86941761\n",
      " 2.50504881 2.7297489  4.01111708 8.14868929]\n",
      "##########\n",
      "epoch:27 step:25401 [D loss: 0.490339, acc.: 75.00%] [G loss: 1.224128]\n",
      "epoch:27 step:25402 [D loss: 0.634844, acc.: 66.41%] [G loss: 1.041916]\n",
      "epoch:27 step:25403 [D loss: 0.589734, acc.: 67.97%] [G loss: 1.639240]\n",
      "epoch:27 step:25404 [D loss: 0.413573, acc.: 83.59%] [G loss: 1.622330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25405 [D loss: 0.752761, acc.: 57.03%] [G loss: 1.363257]\n",
      "epoch:27 step:25406 [D loss: 0.519650, acc.: 69.53%] [G loss: 1.112293]\n",
      "epoch:27 step:25407 [D loss: 0.706919, acc.: 57.03%] [G loss: 1.194117]\n",
      "epoch:27 step:25408 [D loss: 0.625630, acc.: 63.28%] [G loss: 1.448340]\n",
      "epoch:27 step:25409 [D loss: 0.508899, acc.: 72.66%] [G loss: 1.617258]\n",
      "epoch:27 step:25410 [D loss: 0.633180, acc.: 62.50%] [G loss: 1.290416]\n",
      "epoch:27 step:25411 [D loss: 0.474512, acc.: 78.91%] [G loss: 1.502697]\n",
      "epoch:27 step:25412 [D loss: 0.436647, acc.: 79.69%] [G loss: 1.609362]\n",
      "epoch:27 step:25413 [D loss: 0.483608, acc.: 77.34%] [G loss: 1.703050]\n",
      "epoch:27 step:25414 [D loss: 0.570768, acc.: 70.31%] [G loss: 1.330460]\n",
      "epoch:27 step:25415 [D loss: 0.700359, acc.: 58.59%] [G loss: 1.232667]\n",
      "epoch:27 step:25416 [D loss: 0.401498, acc.: 81.25%] [G loss: 1.182720]\n",
      "epoch:27 step:25417 [D loss: 0.637724, acc.: 64.84%] [G loss: 1.127188]\n",
      "epoch:27 step:25418 [D loss: 0.424573, acc.: 83.59%] [G loss: 1.976569]\n",
      "epoch:27 step:25419 [D loss: 0.687627, acc.: 58.59%] [G loss: 1.500960]\n",
      "epoch:27 step:25420 [D loss: 0.464090, acc.: 80.47%] [G loss: 1.296355]\n",
      "epoch:27 step:25421 [D loss: 0.588784, acc.: 69.53%] [G loss: 1.576387]\n",
      "epoch:27 step:25422 [D loss: 0.642664, acc.: 64.84%] [G loss: 1.351350]\n",
      "epoch:27 step:25423 [D loss: 0.736225, acc.: 56.25%] [G loss: 1.502920]\n",
      "epoch:27 step:25424 [D loss: 0.606273, acc.: 66.41%] [G loss: 0.941783]\n",
      "epoch:27 step:25425 [D loss: 0.683267, acc.: 63.28%] [G loss: 1.157558]\n",
      "epoch:27 step:25426 [D loss: 0.434756, acc.: 82.03%] [G loss: 1.594778]\n",
      "epoch:27 step:25427 [D loss: 0.539336, acc.: 71.09%] [G loss: 1.243545]\n",
      "epoch:27 step:25428 [D loss: 0.710574, acc.: 56.25%] [G loss: 1.217255]\n",
      "epoch:27 step:25429 [D loss: 0.523784, acc.: 70.31%] [G loss: 1.185263]\n",
      "epoch:27 step:25430 [D loss: 0.563824, acc.: 70.31%] [G loss: 1.334489]\n",
      "epoch:27 step:25431 [D loss: 0.660044, acc.: 64.84%] [G loss: 1.526875]\n",
      "epoch:27 step:25432 [D loss: 0.526001, acc.: 74.22%] [G loss: 1.196979]\n",
      "epoch:27 step:25433 [D loss: 0.613453, acc.: 70.31%] [G loss: 1.039620]\n",
      "epoch:27 step:25434 [D loss: 0.537454, acc.: 72.66%] [G loss: 1.618366]\n",
      "epoch:27 step:25435 [D loss: 0.660117, acc.: 65.62%] [G loss: 1.092259]\n",
      "epoch:27 step:25436 [D loss: 0.542447, acc.: 75.00%] [G loss: 1.539739]\n",
      "epoch:27 step:25437 [D loss: 0.604406, acc.: 69.53%] [G loss: 1.664756]\n",
      "epoch:27 step:25438 [D loss: 0.463157, acc.: 75.78%] [G loss: 1.362865]\n",
      "epoch:27 step:25439 [D loss: 0.431622, acc.: 81.25%] [G loss: 1.299068]\n",
      "epoch:27 step:25440 [D loss: 0.539675, acc.: 75.00%] [G loss: 0.931203]\n",
      "epoch:27 step:25441 [D loss: 0.588473, acc.: 71.09%] [G loss: 1.123657]\n",
      "epoch:27 step:25442 [D loss: 0.437237, acc.: 82.81%] [G loss: 1.140804]\n",
      "epoch:27 step:25443 [D loss: 0.463505, acc.: 82.03%] [G loss: 1.457915]\n",
      "epoch:27 step:25444 [D loss: 0.679173, acc.: 58.59%] [G loss: 1.434981]\n",
      "epoch:27 step:25445 [D loss: 0.679060, acc.: 64.84%] [G loss: 1.459321]\n",
      "epoch:27 step:25446 [D loss: 0.375295, acc.: 86.72%] [G loss: 1.286735]\n",
      "epoch:27 step:25447 [D loss: 0.369165, acc.: 91.41%] [G loss: 1.463703]\n",
      "epoch:27 step:25448 [D loss: 0.736264, acc.: 57.81%] [G loss: 1.349720]\n",
      "epoch:27 step:25449 [D loss: 0.643594, acc.: 64.06%] [G loss: 1.489659]\n",
      "epoch:27 step:25450 [D loss: 0.402752, acc.: 86.72%] [G loss: 1.355139]\n",
      "epoch:27 step:25451 [D loss: 0.577812, acc.: 67.19%] [G loss: 1.191041]\n",
      "epoch:27 step:25452 [D loss: 0.625545, acc.: 71.09%] [G loss: 1.546355]\n",
      "epoch:27 step:25453 [D loss: 0.475766, acc.: 75.00%] [G loss: 1.319865]\n",
      "epoch:27 step:25454 [D loss: 0.593820, acc.: 68.75%] [G loss: 1.314074]\n",
      "epoch:27 step:25455 [D loss: 0.500692, acc.: 76.56%] [G loss: 1.417700]\n",
      "epoch:27 step:25456 [D loss: 0.702887, acc.: 64.84%] [G loss: 1.186638]\n",
      "epoch:27 step:25457 [D loss: 0.571877, acc.: 68.75%] [G loss: 0.965756]\n",
      "epoch:27 step:25458 [D loss: 0.471233, acc.: 75.78%] [G loss: 1.824603]\n",
      "epoch:27 step:25459 [D loss: 0.562083, acc.: 69.53%] [G loss: 1.279297]\n",
      "epoch:27 step:25460 [D loss: 0.591465, acc.: 68.75%] [G loss: 2.031657]\n",
      "epoch:27 step:25461 [D loss: 0.646962, acc.: 64.06%] [G loss: 1.886778]\n",
      "epoch:27 step:25462 [D loss: 0.502858, acc.: 75.00%] [G loss: 1.816061]\n",
      "epoch:27 step:25463 [D loss: 0.411937, acc.: 83.59%] [G loss: 1.369807]\n",
      "epoch:27 step:25464 [D loss: 0.534024, acc.: 76.56%] [G loss: 1.404730]\n",
      "epoch:27 step:25465 [D loss: 0.571036, acc.: 69.53%] [G loss: 1.029883]\n",
      "epoch:27 step:25466 [D loss: 0.488562, acc.: 76.56%] [G loss: 1.337022]\n",
      "epoch:27 step:25467 [D loss: 0.598763, acc.: 70.31%] [G loss: 1.567741]\n",
      "epoch:27 step:25468 [D loss: 0.513947, acc.: 73.44%] [G loss: 1.308500]\n",
      "epoch:27 step:25469 [D loss: 0.677432, acc.: 57.81%] [G loss: 1.460248]\n",
      "epoch:27 step:25470 [D loss: 0.583617, acc.: 71.88%] [G loss: 1.624322]\n",
      "epoch:27 step:25471 [D loss: 0.582960, acc.: 68.75%] [G loss: 1.668020]\n",
      "epoch:27 step:25472 [D loss: 0.591639, acc.: 71.09%] [G loss: 1.093946]\n",
      "epoch:27 step:25473 [D loss: 0.540688, acc.: 71.88%] [G loss: 1.431017]\n",
      "epoch:27 step:25474 [D loss: 0.522597, acc.: 75.00%] [G loss: 1.469687]\n",
      "epoch:27 step:25475 [D loss: 0.615000, acc.: 66.41%] [G loss: 1.683802]\n",
      "epoch:27 step:25476 [D loss: 0.531845, acc.: 75.78%] [G loss: 1.512818]\n",
      "epoch:27 step:25477 [D loss: 0.607992, acc.: 67.19%] [G loss: 1.368165]\n",
      "epoch:27 step:25478 [D loss: 0.598849, acc.: 66.41%] [G loss: 1.300167]\n",
      "epoch:27 step:25479 [D loss: 0.488512, acc.: 75.78%] [G loss: 1.555457]\n",
      "epoch:27 step:25480 [D loss: 0.765930, acc.: 57.03%] [G loss: 0.788643]\n",
      "epoch:27 step:25481 [D loss: 0.420425, acc.: 82.81%] [G loss: 1.315379]\n",
      "epoch:27 step:25482 [D loss: 0.564156, acc.: 67.19%] [G loss: 1.295985]\n",
      "epoch:27 step:25483 [D loss: 0.631658, acc.: 60.16%] [G loss: 1.177798]\n",
      "epoch:27 step:25484 [D loss: 0.561053, acc.: 70.31%] [G loss: 1.221761]\n",
      "epoch:27 step:25485 [D loss: 0.499879, acc.: 74.22%] [G loss: 1.322472]\n",
      "epoch:27 step:25486 [D loss: 0.592837, acc.: 64.06%] [G loss: 1.114606]\n",
      "epoch:27 step:25487 [D loss: 0.489262, acc.: 76.56%] [G loss: 1.872606]\n",
      "epoch:27 step:25488 [D loss: 0.564769, acc.: 69.53%] [G loss: 1.188552]\n",
      "epoch:27 step:25489 [D loss: 0.615008, acc.: 64.06%] [G loss: 1.683190]\n",
      "epoch:27 step:25490 [D loss: 0.484015, acc.: 78.12%] [G loss: 1.341016]\n",
      "epoch:27 step:25491 [D loss: 0.564599, acc.: 71.09%] [G loss: 1.974063]\n",
      "epoch:27 step:25492 [D loss: 0.614200, acc.: 67.97%] [G loss: 1.653512]\n",
      "epoch:27 step:25493 [D loss: 0.680800, acc.: 62.50%] [G loss: 1.294473]\n",
      "epoch:27 step:25494 [D loss: 0.641436, acc.: 62.50%] [G loss: 1.651699]\n",
      "epoch:27 step:25495 [D loss: 0.468072, acc.: 80.47%] [G loss: 1.070725]\n",
      "epoch:27 step:25496 [D loss: 0.476586, acc.: 73.44%] [G loss: 1.662926]\n",
      "epoch:27 step:25497 [D loss: 0.510566, acc.: 74.22%] [G loss: 1.587620]\n",
      "epoch:27 step:25498 [D loss: 0.520829, acc.: 75.00%] [G loss: 1.403209]\n",
      "epoch:27 step:25499 [D loss: 0.460485, acc.: 73.44%] [G loss: 1.271852]\n",
      "epoch:27 step:25500 [D loss: 0.466956, acc.: 78.91%] [G loss: 1.616051]\n",
      "epoch:27 step:25501 [D loss: 0.633688, acc.: 64.84%] [G loss: 1.319577]\n",
      "epoch:27 step:25502 [D loss: 0.489067, acc.: 76.56%] [G loss: 1.436578]\n",
      "epoch:27 step:25503 [D loss: 0.511309, acc.: 73.44%] [G loss: 1.447656]\n",
      "epoch:27 step:25504 [D loss: 0.722360, acc.: 56.25%] [G loss: 1.391669]\n",
      "epoch:27 step:25505 [D loss: 0.583136, acc.: 67.97%] [G loss: 1.036049]\n",
      "epoch:27 step:25506 [D loss: 0.549686, acc.: 72.66%] [G loss: 1.203731]\n",
      "epoch:27 step:25507 [D loss: 0.702142, acc.: 59.38%] [G loss: 0.932629]\n",
      "epoch:27 step:25508 [D loss: 0.551798, acc.: 72.66%] [G loss: 1.128561]\n",
      "epoch:27 step:25509 [D loss: 0.382310, acc.: 85.16%] [G loss: 1.664128]\n",
      "epoch:27 step:25510 [D loss: 0.584610, acc.: 69.53%] [G loss: 1.232220]\n",
      "epoch:27 step:25511 [D loss: 0.587434, acc.: 65.62%] [G loss: 1.246634]\n",
      "epoch:27 step:25512 [D loss: 0.567759, acc.: 68.75%] [G loss: 1.655150]\n",
      "epoch:27 step:25513 [D loss: 0.727470, acc.: 58.59%] [G loss: 1.556949]\n",
      "epoch:27 step:25514 [D loss: 0.665941, acc.: 63.28%] [G loss: 1.107467]\n",
      "epoch:27 step:25515 [D loss: 0.540485, acc.: 71.09%] [G loss: 1.587710]\n",
      "epoch:27 step:25516 [D loss: 0.361640, acc.: 86.72%] [G loss: 1.659618]\n",
      "epoch:27 step:25517 [D loss: 0.674886, acc.: 60.16%] [G loss: 1.016657]\n",
      "epoch:27 step:25518 [D loss: 0.602100, acc.: 63.28%] [G loss: 1.194508]\n",
      "epoch:27 step:25519 [D loss: 0.538463, acc.: 78.12%] [G loss: 1.753881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25520 [D loss: 0.548545, acc.: 71.09%] [G loss: 1.495934]\n",
      "epoch:27 step:25521 [D loss: 0.531041, acc.: 73.44%] [G loss: 1.610709]\n",
      "epoch:27 step:25522 [D loss: 0.748314, acc.: 53.91%] [G loss: 1.391917]\n",
      "epoch:27 step:25523 [D loss: 0.618152, acc.: 65.62%] [G loss: 1.278061]\n",
      "epoch:27 step:25524 [D loss: 0.583870, acc.: 71.88%] [G loss: 1.163052]\n",
      "epoch:27 step:25525 [D loss: 0.671485, acc.: 53.91%] [G loss: 1.573421]\n",
      "epoch:27 step:25526 [D loss: 0.532529, acc.: 76.56%] [G loss: 1.459818]\n",
      "epoch:27 step:25527 [D loss: 0.690621, acc.: 57.81%] [G loss: 1.324391]\n",
      "epoch:27 step:25528 [D loss: 0.452745, acc.: 78.12%] [G loss: 1.419156]\n",
      "epoch:27 step:25529 [D loss: 0.547479, acc.: 71.88%] [G loss: 1.695442]\n",
      "epoch:27 step:25530 [D loss: 0.511286, acc.: 68.75%] [G loss: 1.592907]\n",
      "epoch:27 step:25531 [D loss: 0.508152, acc.: 74.22%] [G loss: 1.667479]\n",
      "epoch:27 step:25532 [D loss: 0.698980, acc.: 54.69%] [G loss: 1.129351]\n",
      "epoch:27 step:25533 [D loss: 0.520326, acc.: 71.88%] [G loss: 1.705487]\n",
      "epoch:27 step:25534 [D loss: 0.556251, acc.: 71.88%] [G loss: 1.354605]\n",
      "epoch:27 step:25535 [D loss: 0.498795, acc.: 78.12%] [G loss: 1.245397]\n",
      "epoch:27 step:25536 [D loss: 0.802917, acc.: 50.78%] [G loss: 1.004506]\n",
      "epoch:27 step:25537 [D loss: 0.516717, acc.: 75.00%] [G loss: 1.109459]\n",
      "epoch:27 step:25538 [D loss: 0.842830, acc.: 48.44%] [G loss: 1.052634]\n",
      "epoch:27 step:25539 [D loss: 0.550851, acc.: 71.88%] [G loss: 1.399252]\n",
      "epoch:27 step:25540 [D loss: 0.556967, acc.: 72.66%] [G loss: 1.533077]\n",
      "epoch:27 step:25541 [D loss: 0.567454, acc.: 69.53%] [G loss: 1.078361]\n",
      "epoch:27 step:25542 [D loss: 0.590694, acc.: 67.97%] [G loss: 1.554297]\n",
      "epoch:27 step:25543 [D loss: 0.544029, acc.: 75.78%] [G loss: 1.565593]\n",
      "epoch:27 step:25544 [D loss: 0.410066, acc.: 84.38%] [G loss: 1.274229]\n",
      "epoch:27 step:25545 [D loss: 0.508205, acc.: 75.78%] [G loss: 1.323541]\n",
      "epoch:27 step:25546 [D loss: 0.659103, acc.: 65.62%] [G loss: 1.326423]\n",
      "epoch:27 step:25547 [D loss: 0.344828, acc.: 92.19%] [G loss: 1.473447]\n",
      "epoch:27 step:25548 [D loss: 0.360348, acc.: 86.72%] [G loss: 1.215157]\n",
      "epoch:27 step:25549 [D loss: 0.547674, acc.: 71.88%] [G loss: 1.500291]\n",
      "epoch:27 step:25550 [D loss: 0.475351, acc.: 80.47%] [G loss: 1.596526]\n",
      "epoch:27 step:25551 [D loss: 0.441056, acc.: 85.16%] [G loss: 1.508423]\n",
      "epoch:27 step:25552 [D loss: 0.502914, acc.: 72.66%] [G loss: 1.689276]\n",
      "epoch:27 step:25553 [D loss: 0.557214, acc.: 71.88%] [G loss: 1.091868]\n",
      "epoch:27 step:25554 [D loss: 0.572293, acc.: 65.62%] [G loss: 1.417687]\n",
      "epoch:27 step:25555 [D loss: 0.578716, acc.: 64.84%] [G loss: 1.546126]\n",
      "epoch:27 step:25556 [D loss: 0.512841, acc.: 77.34%] [G loss: 1.586092]\n",
      "epoch:27 step:25557 [D loss: 0.467614, acc.: 78.91%] [G loss: 1.185862]\n",
      "epoch:27 step:25558 [D loss: 0.614513, acc.: 65.62%] [G loss: 1.479873]\n",
      "epoch:27 step:25559 [D loss: 0.608115, acc.: 64.84%] [G loss: 1.313631]\n",
      "epoch:27 step:25560 [D loss: 0.575564, acc.: 74.22%] [G loss: 1.176098]\n",
      "epoch:27 step:25561 [D loss: 0.687308, acc.: 57.81%] [G loss: 1.436026]\n",
      "epoch:27 step:25562 [D loss: 0.660680, acc.: 60.16%] [G loss: 1.542092]\n",
      "epoch:27 step:25563 [D loss: 0.473900, acc.: 76.56%] [G loss: 1.384508]\n",
      "epoch:27 step:25564 [D loss: 0.337211, acc.: 88.28%] [G loss: 1.811758]\n",
      "epoch:27 step:25565 [D loss: 0.474070, acc.: 79.69%] [G loss: 1.582900]\n",
      "epoch:27 step:25566 [D loss: 0.491175, acc.: 78.91%] [G loss: 1.476904]\n",
      "epoch:27 step:25567 [D loss: 0.669802, acc.: 60.94%] [G loss: 1.912650]\n",
      "epoch:27 step:25568 [D loss: 0.450210, acc.: 80.47%] [G loss: 1.307855]\n",
      "epoch:27 step:25569 [D loss: 0.546856, acc.: 71.09%] [G loss: 1.576225]\n",
      "epoch:27 step:25570 [D loss: 0.530884, acc.: 72.66%] [G loss: 1.277346]\n",
      "epoch:27 step:25571 [D loss: 0.469124, acc.: 75.78%] [G loss: 1.850985]\n",
      "epoch:27 step:25572 [D loss: 0.633396, acc.: 70.31%] [G loss: 1.111697]\n",
      "epoch:27 step:25573 [D loss: 0.440312, acc.: 85.94%] [G loss: 1.267091]\n",
      "epoch:27 step:25574 [D loss: 0.695890, acc.: 60.94%] [G loss: 1.564136]\n",
      "epoch:27 step:25575 [D loss: 0.394507, acc.: 86.72%] [G loss: 1.925997]\n",
      "epoch:27 step:25576 [D loss: 0.501851, acc.: 74.22%] [G loss: 1.227867]\n",
      "epoch:27 step:25577 [D loss: 0.759513, acc.: 56.25%] [G loss: 1.456534]\n",
      "epoch:27 step:25578 [D loss: 0.510587, acc.: 80.47%] [G loss: 1.449195]\n",
      "epoch:27 step:25579 [D loss: 0.512128, acc.: 71.88%] [G loss: 1.383604]\n",
      "epoch:27 step:25580 [D loss: 0.635087, acc.: 65.62%] [G loss: 1.394944]\n",
      "epoch:27 step:25581 [D loss: 0.647699, acc.: 64.84%] [G loss: 1.473017]\n",
      "epoch:27 step:25582 [D loss: 0.529930, acc.: 72.66%] [G loss: 1.445520]\n",
      "epoch:27 step:25583 [D loss: 0.578914, acc.: 71.09%] [G loss: 1.144168]\n",
      "epoch:27 step:25584 [D loss: 0.430601, acc.: 82.81%] [G loss: 1.554893]\n",
      "epoch:27 step:25585 [D loss: 0.556451, acc.: 74.22%] [G loss: 1.550809]\n",
      "epoch:27 step:25586 [D loss: 0.483647, acc.: 75.00%] [G loss: 1.167734]\n",
      "epoch:27 step:25587 [D loss: 0.467557, acc.: 77.34%] [G loss: 1.767805]\n",
      "epoch:27 step:25588 [D loss: 0.560269, acc.: 71.09%] [G loss: 1.852290]\n",
      "epoch:27 step:25589 [D loss: 0.537417, acc.: 75.78%] [G loss: 1.441996]\n",
      "epoch:27 step:25590 [D loss: 0.852653, acc.: 54.69%] [G loss: 1.795148]\n",
      "epoch:27 step:25591 [D loss: 0.460163, acc.: 82.81%] [G loss: 1.603969]\n",
      "epoch:27 step:25592 [D loss: 0.492260, acc.: 76.56%] [G loss: 1.656606]\n",
      "epoch:27 step:25593 [D loss: 0.515058, acc.: 73.44%] [G loss: 1.558723]\n",
      "epoch:27 step:25594 [D loss: 0.671744, acc.: 60.94%] [G loss: 1.584925]\n",
      "epoch:27 step:25595 [D loss: 0.465309, acc.: 79.69%] [G loss: 1.541905]\n",
      "epoch:27 step:25596 [D loss: 0.511699, acc.: 71.09%] [G loss: 1.695133]\n",
      "epoch:27 step:25597 [D loss: 0.630405, acc.: 61.72%] [G loss: 1.324633]\n",
      "epoch:27 step:25598 [D loss: 0.530892, acc.: 77.34%] [G loss: 1.439204]\n",
      "epoch:27 step:25599 [D loss: 0.405863, acc.: 81.25%] [G loss: 1.354545]\n",
      "epoch:27 step:25600 [D loss: 0.514442, acc.: 73.44%] [G loss: 1.179876]\n",
      "##############\n",
      "[2.74610993 2.12421579 2.08158612 2.80586968 0.83912217 6.13126224\n",
      " 2.14419822 2.91147528 3.91442799 4.25483324]\n",
      "##########\n",
      "epoch:27 step:25601 [D loss: 0.415873, acc.: 78.91%] [G loss: 1.371192]\n",
      "epoch:27 step:25602 [D loss: 0.546852, acc.: 70.31%] [G loss: 1.135158]\n",
      "epoch:27 step:25603 [D loss: 0.613903, acc.: 68.75%] [G loss: 0.994869]\n",
      "epoch:27 step:25604 [D loss: 0.493561, acc.: 79.69%] [G loss: 1.378804]\n",
      "epoch:27 step:25605 [D loss: 0.426702, acc.: 88.28%] [G loss: 1.420199]\n",
      "epoch:27 step:25606 [D loss: 0.568713, acc.: 64.84%] [G loss: 1.393403]\n",
      "epoch:27 step:25607 [D loss: 0.546788, acc.: 67.97%] [G loss: 1.403722]\n",
      "epoch:27 step:25608 [D loss: 0.566083, acc.: 68.75%] [G loss: 1.341402]\n",
      "epoch:27 step:25609 [D loss: 0.480428, acc.: 77.34%] [G loss: 1.565535]\n",
      "epoch:27 step:25610 [D loss: 0.442180, acc.: 78.12%] [G loss: 1.317642]\n",
      "epoch:27 step:25611 [D loss: 0.675994, acc.: 55.47%] [G loss: 1.086280]\n",
      "epoch:27 step:25612 [D loss: 0.625759, acc.: 64.84%] [G loss: 1.377672]\n",
      "epoch:27 step:25613 [D loss: 0.676099, acc.: 64.06%] [G loss: 1.245145]\n",
      "epoch:27 step:25614 [D loss: 0.525151, acc.: 75.00%] [G loss: 1.398032]\n",
      "epoch:27 step:25615 [D loss: 0.600078, acc.: 64.84%] [G loss: 1.143987]\n",
      "epoch:27 step:25616 [D loss: 0.277625, acc.: 93.75%] [G loss: 1.291729]\n",
      "epoch:27 step:25617 [D loss: 0.867295, acc.: 50.78%] [G loss: 1.102367]\n",
      "epoch:27 step:25618 [D loss: 0.639378, acc.: 60.16%] [G loss: 1.530992]\n",
      "epoch:27 step:25619 [D loss: 0.517053, acc.: 73.44%] [G loss: 1.528455]\n",
      "epoch:27 step:25620 [D loss: 0.586980, acc.: 71.88%] [G loss: 1.331671]\n",
      "epoch:27 step:25621 [D loss: 0.609602, acc.: 67.19%] [G loss: 1.451464]\n",
      "epoch:27 step:25622 [D loss: 0.482877, acc.: 71.88%] [G loss: 1.512136]\n",
      "epoch:27 step:25623 [D loss: 0.704606, acc.: 56.25%] [G loss: 1.173526]\n",
      "epoch:27 step:25624 [D loss: 0.466242, acc.: 78.12%] [G loss: 1.312852]\n",
      "epoch:27 step:25625 [D loss: 0.731638, acc.: 57.81%] [G loss: 1.246367]\n",
      "epoch:27 step:25626 [D loss: 0.456641, acc.: 82.03%] [G loss: 1.460770]\n",
      "epoch:27 step:25627 [D loss: 0.496184, acc.: 76.56%] [G loss: 1.058714]\n",
      "epoch:27 step:25628 [D loss: 0.493061, acc.: 73.44%] [G loss: 1.778412]\n",
      "epoch:27 step:25629 [D loss: 0.491251, acc.: 75.78%] [G loss: 1.692322]\n",
      "epoch:27 step:25630 [D loss: 0.397174, acc.: 84.38%] [G loss: 0.971367]\n",
      "epoch:27 step:25631 [D loss: 0.569279, acc.: 72.66%] [G loss: 1.721435]\n",
      "epoch:27 step:25632 [D loss: 0.597869, acc.: 69.53%] [G loss: 1.083852]\n",
      "epoch:27 step:25633 [D loss: 0.314751, acc.: 86.72%] [G loss: 1.538227]\n",
      "epoch:27 step:25634 [D loss: 0.553063, acc.: 71.09%] [G loss: 1.466684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25635 [D loss: 0.508381, acc.: 75.78%] [G loss: 1.868785]\n",
      "epoch:27 step:25636 [D loss: 0.570156, acc.: 71.09%] [G loss: 1.284263]\n",
      "epoch:27 step:25637 [D loss: 0.485990, acc.: 80.47%] [G loss: 1.680848]\n",
      "epoch:27 step:25638 [D loss: 0.637066, acc.: 65.62%] [G loss: 1.438834]\n",
      "epoch:27 step:25639 [D loss: 0.360971, acc.: 89.84%] [G loss: 1.822241]\n",
      "epoch:27 step:25640 [D loss: 0.524478, acc.: 74.22%] [G loss: 1.658362]\n",
      "epoch:27 step:25641 [D loss: 0.672992, acc.: 62.50%] [G loss: 1.056583]\n",
      "epoch:27 step:25642 [D loss: 0.574163, acc.: 69.53%] [G loss: 1.335140]\n",
      "epoch:27 step:25643 [D loss: 0.472718, acc.: 80.47%] [G loss: 1.665637]\n",
      "epoch:27 step:25644 [D loss: 0.518235, acc.: 74.22%] [G loss: 1.739956]\n",
      "epoch:27 step:25645 [D loss: 0.701204, acc.: 55.47%] [G loss: 1.471256]\n",
      "epoch:27 step:25646 [D loss: 0.524090, acc.: 75.00%] [G loss: 1.368519]\n",
      "epoch:27 step:25647 [D loss: 0.553474, acc.: 72.66%] [G loss: 1.326465]\n",
      "epoch:27 step:25648 [D loss: 0.541479, acc.: 68.75%] [G loss: 1.175650]\n",
      "epoch:27 step:25649 [D loss: 0.555020, acc.: 75.00%] [G loss: 1.231001]\n",
      "epoch:27 step:25650 [D loss: 0.703557, acc.: 57.81%] [G loss: 0.970513]\n",
      "epoch:27 step:25651 [D loss: 0.494330, acc.: 75.78%] [G loss: 1.271198]\n",
      "epoch:27 step:25652 [D loss: 0.565948, acc.: 71.09%] [G loss: 0.989012]\n",
      "epoch:27 step:25653 [D loss: 0.533159, acc.: 72.66%] [G loss: 1.257499]\n",
      "epoch:27 step:25654 [D loss: 0.566920, acc.: 69.53%] [G loss: 1.343863]\n",
      "epoch:27 step:25655 [D loss: 0.421921, acc.: 82.81%] [G loss: 1.527587]\n",
      "epoch:27 step:25656 [D loss: 0.513212, acc.: 71.88%] [G loss: 1.706400]\n",
      "epoch:27 step:25657 [D loss: 0.543134, acc.: 75.00%] [G loss: 1.320854]\n",
      "epoch:27 step:25658 [D loss: 0.587323, acc.: 71.88%] [G loss: 1.323686]\n",
      "epoch:27 step:25659 [D loss: 0.508117, acc.: 76.56%] [G loss: 1.441881]\n",
      "epoch:27 step:25660 [D loss: 0.551072, acc.: 72.66%] [G loss: 1.752719]\n",
      "epoch:27 step:25661 [D loss: 0.477668, acc.: 80.47%] [G loss: 1.561007]\n",
      "epoch:27 step:25662 [D loss: 0.458935, acc.: 82.03%] [G loss: 1.559497]\n",
      "epoch:27 step:25663 [D loss: 0.683395, acc.: 64.06%] [G loss: 1.057431]\n",
      "epoch:27 step:25664 [D loss: 0.640412, acc.: 60.16%] [G loss: 1.271592]\n",
      "epoch:27 step:25665 [D loss: 0.657887, acc.: 60.16%] [G loss: 1.408325]\n",
      "epoch:27 step:25666 [D loss: 0.586305, acc.: 68.75%] [G loss: 1.423392]\n",
      "epoch:27 step:25667 [D loss: 0.585489, acc.: 67.19%] [G loss: 1.084183]\n",
      "epoch:27 step:25668 [D loss: 0.660687, acc.: 61.72%] [G loss: 1.196687]\n",
      "epoch:27 step:25669 [D loss: 0.662081, acc.: 61.72%] [G loss: 1.235276]\n",
      "epoch:27 step:25670 [D loss: 0.533580, acc.: 71.88%] [G loss: 1.403450]\n",
      "epoch:27 step:25671 [D loss: 0.506742, acc.: 80.47%] [G loss: 1.562683]\n",
      "epoch:27 step:25672 [D loss: 0.683910, acc.: 57.81%] [G loss: 1.259447]\n",
      "epoch:27 step:25673 [D loss: 0.798767, acc.: 57.81%] [G loss: 0.863172]\n",
      "epoch:27 step:25674 [D loss: 0.549091, acc.: 71.88%] [G loss: 1.581051]\n",
      "epoch:27 step:25675 [D loss: 0.688008, acc.: 61.72%] [G loss: 1.102346]\n",
      "epoch:27 step:25676 [D loss: 0.382412, acc.: 82.81%] [G loss: 1.886618]\n",
      "epoch:27 step:25677 [D loss: 0.481535, acc.: 81.25%] [G loss: 1.505782]\n",
      "epoch:27 step:25678 [D loss: 0.474594, acc.: 80.47%] [G loss: 1.454365]\n",
      "epoch:27 step:25679 [D loss: 0.463488, acc.: 76.56%] [G loss: 1.831325]\n",
      "epoch:27 step:25680 [D loss: 0.593977, acc.: 66.41%] [G loss: 1.206983]\n",
      "epoch:27 step:25681 [D loss: 0.486114, acc.: 76.56%] [G loss: 1.476311]\n",
      "epoch:27 step:25682 [D loss: 0.637572, acc.: 64.84%] [G loss: 1.385909]\n",
      "epoch:27 step:25683 [D loss: 0.651966, acc.: 58.59%] [G loss: 1.674264]\n",
      "epoch:27 step:25684 [D loss: 0.590711, acc.: 73.44%] [G loss: 1.424416]\n",
      "epoch:27 step:25685 [D loss: 0.577671, acc.: 75.78%] [G loss: 1.569294]\n",
      "epoch:27 step:25686 [D loss: 0.426625, acc.: 79.69%] [G loss: 1.694253]\n",
      "epoch:27 step:25687 [D loss: 0.588570, acc.: 69.53%] [G loss: 1.171671]\n",
      "epoch:27 step:25688 [D loss: 0.531502, acc.: 76.56%] [G loss: 1.527073]\n",
      "epoch:27 step:25689 [D loss: 0.742262, acc.: 54.69%] [G loss: 1.611388]\n",
      "epoch:27 step:25690 [D loss: 0.542717, acc.: 74.22%] [G loss: 1.579296]\n",
      "epoch:27 step:25691 [D loss: 0.488732, acc.: 82.03%] [G loss: 1.264730]\n",
      "epoch:27 step:25692 [D loss: 0.701072, acc.: 59.38%] [G loss: 1.296198]\n",
      "epoch:27 step:25693 [D loss: 0.527017, acc.: 71.09%] [G loss: 1.074896]\n",
      "epoch:27 step:25694 [D loss: 0.518371, acc.: 72.66%] [G loss: 1.098350]\n",
      "epoch:27 step:25695 [D loss: 0.476735, acc.: 74.22%] [G loss: 1.611960]\n",
      "epoch:27 step:25696 [D loss: 0.580676, acc.: 72.66%] [G loss: 1.729580]\n",
      "epoch:27 step:25697 [D loss: 0.403132, acc.: 80.47%] [G loss: 1.838530]\n",
      "epoch:27 step:25698 [D loss: 0.686101, acc.: 67.19%] [G loss: 1.283827]\n",
      "epoch:27 step:25699 [D loss: 0.421947, acc.: 82.81%] [G loss: 1.850145]\n",
      "epoch:27 step:25700 [D loss: 0.706712, acc.: 57.81%] [G loss: 1.526136]\n",
      "epoch:27 step:25701 [D loss: 0.526827, acc.: 73.44%] [G loss: 1.516742]\n",
      "epoch:27 step:25702 [D loss: 0.560118, acc.: 74.22%] [G loss: 1.765866]\n",
      "epoch:27 step:25703 [D loss: 0.595617, acc.: 68.75%] [G loss: 1.633320]\n",
      "epoch:27 step:25704 [D loss: 0.558460, acc.: 70.31%] [G loss: 1.193059]\n",
      "epoch:27 step:25705 [D loss: 0.479423, acc.: 79.69%] [G loss: 1.197328]\n",
      "epoch:27 step:25706 [D loss: 0.648684, acc.: 67.19%] [G loss: 1.565680]\n",
      "epoch:27 step:25707 [D loss: 0.675371, acc.: 64.84%] [G loss: 1.391850]\n",
      "epoch:27 step:25708 [D loss: 0.760551, acc.: 47.66%] [G loss: 1.281988]\n",
      "epoch:27 step:25709 [D loss: 0.521377, acc.: 76.56%] [G loss: 1.419802]\n",
      "epoch:27 step:25710 [D loss: 0.485903, acc.: 74.22%] [G loss: 1.364167]\n",
      "epoch:27 step:25711 [D loss: 0.473070, acc.: 80.47%] [G loss: 1.675311]\n",
      "epoch:27 step:25712 [D loss: 0.576228, acc.: 69.53%] [G loss: 1.724578]\n",
      "epoch:27 step:25713 [D loss: 0.576885, acc.: 67.19%] [G loss: 1.765435]\n",
      "epoch:27 step:25714 [D loss: 0.476990, acc.: 81.25%] [G loss: 1.701883]\n",
      "epoch:27 step:25715 [D loss: 0.727539, acc.: 59.38%] [G loss: 0.971910]\n",
      "epoch:27 step:25716 [D loss: 0.511552, acc.: 78.12%] [G loss: 1.528179]\n",
      "epoch:27 step:25717 [D loss: 0.507517, acc.: 77.34%] [G loss: 1.590654]\n",
      "epoch:27 step:25718 [D loss: 0.580524, acc.: 69.53%] [G loss: 1.365858]\n",
      "epoch:27 step:25719 [D loss: 0.434777, acc.: 86.72%] [G loss: 1.358264]\n",
      "epoch:27 step:25720 [D loss: 0.579334, acc.: 70.31%] [G loss: 1.844485]\n",
      "epoch:27 step:25721 [D loss: 0.526765, acc.: 72.66%] [G loss: 1.264094]\n",
      "epoch:27 step:25722 [D loss: 0.539603, acc.: 71.88%] [G loss: 1.454138]\n",
      "epoch:27 step:25723 [D loss: 0.576810, acc.: 67.97%] [G loss: 1.491597]\n",
      "epoch:27 step:25724 [D loss: 0.451024, acc.: 78.91%] [G loss: 1.559124]\n",
      "epoch:27 step:25725 [D loss: 0.527902, acc.: 74.22%] [G loss: 1.541137]\n",
      "epoch:27 step:25726 [D loss: 0.553877, acc.: 70.31%] [G loss: 1.582409]\n",
      "epoch:27 step:25727 [D loss: 0.560155, acc.: 67.97%] [G loss: 1.503217]\n",
      "epoch:27 step:25728 [D loss: 0.479328, acc.: 80.47%] [G loss: 1.482142]\n",
      "epoch:27 step:25729 [D loss: 0.492036, acc.: 76.56%] [G loss: 1.731686]\n",
      "epoch:27 step:25730 [D loss: 0.584442, acc.: 68.75%] [G loss: 1.237537]\n",
      "epoch:27 step:25731 [D loss: 0.486809, acc.: 80.47%] [G loss: 1.177326]\n",
      "epoch:27 step:25732 [D loss: 0.477995, acc.: 78.12%] [G loss: 1.375515]\n",
      "epoch:27 step:25733 [D loss: 0.512228, acc.: 78.91%] [G loss: 1.470743]\n",
      "epoch:27 step:25734 [D loss: 0.394835, acc.: 91.41%] [G loss: 1.582519]\n",
      "epoch:27 step:25735 [D loss: 0.504926, acc.: 76.56%] [G loss: 1.547451]\n",
      "epoch:27 step:25736 [D loss: 0.516240, acc.: 76.56%] [G loss: 1.388500]\n",
      "epoch:27 step:25737 [D loss: 0.550128, acc.: 72.66%] [G loss: 1.436977]\n",
      "epoch:27 step:25738 [D loss: 0.464952, acc.: 78.12%] [G loss: 1.824086]\n",
      "epoch:27 step:25739 [D loss: 0.475770, acc.: 82.81%] [G loss: 1.409480]\n",
      "epoch:27 step:25740 [D loss: 0.335121, acc.: 89.84%] [G loss: 1.539952]\n",
      "epoch:27 step:25741 [D loss: 0.450943, acc.: 81.25%] [G loss: 1.497549]\n",
      "epoch:27 step:25742 [D loss: 0.634556, acc.: 66.41%] [G loss: 1.609088]\n",
      "epoch:27 step:25743 [D loss: 0.497681, acc.: 79.69%] [G loss: 1.327951]\n",
      "epoch:27 step:25744 [D loss: 0.420287, acc.: 82.81%] [G loss: 1.644687]\n",
      "epoch:27 step:25745 [D loss: 0.506046, acc.: 74.22%] [G loss: 1.522841]\n",
      "epoch:27 step:25746 [D loss: 0.425830, acc.: 80.47%] [G loss: 1.374430]\n",
      "epoch:27 step:25747 [D loss: 0.515275, acc.: 72.66%] [G loss: 1.748900]\n",
      "epoch:27 step:25748 [D loss: 0.704777, acc.: 60.16%] [G loss: 1.392217]\n",
      "epoch:27 step:25749 [D loss: 0.413552, acc.: 86.72%] [G loss: 1.544201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25750 [D loss: 0.455427, acc.: 81.25%] [G loss: 1.628699]\n",
      "epoch:27 step:25751 [D loss: 0.611386, acc.: 63.28%] [G loss: 1.212849]\n",
      "epoch:27 step:25752 [D loss: 0.529742, acc.: 75.78%] [G loss: 1.267478]\n",
      "epoch:27 step:25753 [D loss: 0.470737, acc.: 75.78%] [G loss: 1.501084]\n",
      "epoch:27 step:25754 [D loss: 0.494430, acc.: 79.69%] [G loss: 1.434644]\n",
      "epoch:27 step:25755 [D loss: 0.685722, acc.: 58.59%] [G loss: 1.419808]\n",
      "epoch:27 step:25756 [D loss: 0.628770, acc.: 64.06%] [G loss: 1.374248]\n",
      "epoch:27 step:25757 [D loss: 0.499912, acc.: 71.88%] [G loss: 1.254505]\n",
      "epoch:27 step:25758 [D loss: 0.522081, acc.: 75.78%] [G loss: 1.426252]\n",
      "epoch:27 step:25759 [D loss: 0.519184, acc.: 74.22%] [G loss: 1.732438]\n",
      "epoch:27 step:25760 [D loss: 0.585319, acc.: 67.97%] [G loss: 1.622905]\n",
      "epoch:27 step:25761 [D loss: 0.648960, acc.: 64.06%] [G loss: 1.306740]\n",
      "epoch:27 step:25762 [D loss: 0.523849, acc.: 73.44%] [G loss: 1.115770]\n",
      "epoch:27 step:25763 [D loss: 0.512184, acc.: 74.22%] [G loss: 1.235418]\n",
      "epoch:27 step:25764 [D loss: 0.691752, acc.: 58.59%] [G loss: 1.203807]\n",
      "epoch:27 step:25765 [D loss: 0.631800, acc.: 64.84%] [G loss: 1.263209]\n",
      "epoch:27 step:25766 [D loss: 0.430703, acc.: 81.25%] [G loss: 1.502741]\n",
      "epoch:27 step:25767 [D loss: 0.384228, acc.: 85.16%] [G loss: 1.817639]\n",
      "epoch:27 step:25768 [D loss: 0.706017, acc.: 54.69%] [G loss: 1.602544]\n",
      "epoch:27 step:25769 [D loss: 0.734035, acc.: 53.12%] [G loss: 1.054033]\n",
      "epoch:27 step:25770 [D loss: 0.424683, acc.: 82.03%] [G loss: 1.731266]\n",
      "epoch:27 step:25771 [D loss: 0.643084, acc.: 61.72%] [G loss: 1.343702]\n",
      "epoch:27 step:25772 [D loss: 0.439401, acc.: 84.38%] [G loss: 1.384754]\n",
      "epoch:27 step:25773 [D loss: 0.621230, acc.: 66.41%] [G loss: 1.707928]\n",
      "epoch:27 step:25774 [D loss: 0.442626, acc.: 85.94%] [G loss: 1.361191]\n",
      "epoch:27 step:25775 [D loss: 0.477593, acc.: 76.56%] [G loss: 1.439260]\n",
      "epoch:27 step:25776 [D loss: 0.476645, acc.: 80.47%] [G loss: 1.900158]\n",
      "epoch:27 step:25777 [D loss: 0.574745, acc.: 69.53%] [G loss: 1.628031]\n",
      "epoch:27 step:25778 [D loss: 0.428222, acc.: 81.25%] [G loss: 2.030550]\n",
      "epoch:27 step:25779 [D loss: 0.509777, acc.: 76.56%] [G loss: 1.371292]\n",
      "epoch:27 step:25780 [D loss: 0.668700, acc.: 63.28%] [G loss: 1.297004]\n",
      "epoch:27 step:25781 [D loss: 0.577447, acc.: 64.06%] [G loss: 0.901575]\n",
      "epoch:27 step:25782 [D loss: 0.467351, acc.: 81.25%] [G loss: 1.650310]\n",
      "epoch:27 step:25783 [D loss: 0.539489, acc.: 72.66%] [G loss: 1.393192]\n",
      "epoch:27 step:25784 [D loss: 0.493114, acc.: 76.56%] [G loss: 1.563273]\n",
      "epoch:27 step:25785 [D loss: 0.389294, acc.: 85.16%] [G loss: 1.491790]\n",
      "epoch:27 step:25786 [D loss: 0.476707, acc.: 80.47%] [G loss: 2.134226]\n",
      "epoch:27 step:25787 [D loss: 0.517846, acc.: 75.78%] [G loss: 1.077828]\n",
      "epoch:27 step:25788 [D loss: 0.382891, acc.: 86.72%] [G loss: 1.511830]\n",
      "epoch:27 step:25789 [D loss: 0.616144, acc.: 65.62%] [G loss: 1.493741]\n",
      "epoch:27 step:25790 [D loss: 0.460096, acc.: 76.56%] [G loss: 1.755599]\n",
      "epoch:27 step:25791 [D loss: 0.599325, acc.: 68.75%] [G loss: 1.087557]\n",
      "epoch:27 step:25792 [D loss: 0.407854, acc.: 83.59%] [G loss: 1.350892]\n",
      "epoch:27 step:25793 [D loss: 0.478953, acc.: 79.69%] [G loss: 1.601534]\n",
      "epoch:27 step:25794 [D loss: 0.536364, acc.: 72.66%] [G loss: 1.563332]\n",
      "epoch:27 step:25795 [D loss: 0.473613, acc.: 80.47%] [G loss: 1.363528]\n",
      "epoch:27 step:25796 [D loss: 0.634211, acc.: 61.72%] [G loss: 1.143096]\n",
      "epoch:27 step:25797 [D loss: 0.442127, acc.: 80.47%] [G loss: 1.473646]\n",
      "epoch:27 step:25798 [D loss: 0.541142, acc.: 71.88%] [G loss: 1.403839]\n",
      "epoch:27 step:25799 [D loss: 0.622641, acc.: 67.97%] [G loss: 1.331241]\n",
      "epoch:27 step:25800 [D loss: 0.471776, acc.: 76.56%] [G loss: 1.270282]\n",
      "##############\n",
      "[2.74483924 2.17867803 1.91993403 3.01403883 0.867662   5.70839217\n",
      " 2.24013342 2.84023215 3.84214846 8.14868929]\n",
      "##########\n",
      "epoch:27 step:25801 [D loss: 0.416308, acc.: 79.69%] [G loss: 1.164584]\n",
      "epoch:27 step:25802 [D loss: 0.533549, acc.: 69.53%] [G loss: 1.358053]\n",
      "epoch:27 step:25803 [D loss: 0.422298, acc.: 80.47%] [G loss: 1.658839]\n",
      "epoch:27 step:25804 [D loss: 0.484186, acc.: 75.78%] [G loss: 1.692199]\n",
      "epoch:27 step:25805 [D loss: 0.463109, acc.: 80.47%] [G loss: 1.200918]\n",
      "epoch:27 step:25806 [D loss: 0.360216, acc.: 88.28%] [G loss: 1.994724]\n",
      "epoch:27 step:25807 [D loss: 0.539131, acc.: 75.00%] [G loss: 1.027931]\n",
      "epoch:27 step:25808 [D loss: 0.679478, acc.: 64.06%] [G loss: 1.128213]\n",
      "epoch:27 step:25809 [D loss: 0.347439, acc.: 89.06%] [G loss: 1.086871]\n",
      "epoch:27 step:25810 [D loss: 0.464665, acc.: 78.12%] [G loss: 1.555608]\n",
      "epoch:27 step:25811 [D loss: 0.512648, acc.: 78.12%] [G loss: 1.032157]\n",
      "epoch:27 step:25812 [D loss: 0.589577, acc.: 65.62%] [G loss: 1.698960]\n",
      "epoch:27 step:25813 [D loss: 0.525436, acc.: 72.66%] [G loss: 1.291859]\n",
      "epoch:27 step:25814 [D loss: 0.457179, acc.: 83.59%] [G loss: 1.517476]\n",
      "epoch:27 step:25815 [D loss: 0.518160, acc.: 78.91%] [G loss: 1.883359]\n",
      "epoch:27 step:25816 [D loss: 0.489137, acc.: 79.69%] [G loss: 1.270804]\n",
      "epoch:27 step:25817 [D loss: 0.480156, acc.: 76.56%] [G loss: 1.297779]\n",
      "epoch:27 step:25818 [D loss: 0.449189, acc.: 78.91%] [G loss: 1.229450]\n",
      "epoch:27 step:25819 [D loss: 0.479132, acc.: 79.69%] [G loss: 1.529775]\n",
      "epoch:27 step:25820 [D loss: 0.456258, acc.: 79.69%] [G loss: 1.575145]\n",
      "epoch:27 step:25821 [D loss: 0.722150, acc.: 58.59%] [G loss: 1.177241]\n",
      "epoch:27 step:25822 [D loss: 0.601829, acc.: 69.53%] [G loss: 1.226480]\n",
      "epoch:27 step:25823 [D loss: 0.682451, acc.: 59.38%] [G loss: 1.458514]\n",
      "epoch:27 step:25824 [D loss: 0.390283, acc.: 85.16%] [G loss: 1.572483]\n",
      "epoch:27 step:25825 [D loss: 0.438311, acc.: 84.38%] [G loss: 1.666821]\n",
      "epoch:27 step:25826 [D loss: 0.545088, acc.: 72.66%] [G loss: 1.765188]\n",
      "epoch:27 step:25827 [D loss: 0.616210, acc.: 64.84%] [G loss: 1.643496]\n",
      "epoch:27 step:25828 [D loss: 0.439175, acc.: 82.03%] [G loss: 2.041468]\n",
      "epoch:27 step:25829 [D loss: 0.499794, acc.: 73.44%] [G loss: 1.691722]\n",
      "epoch:27 step:25830 [D loss: 0.454752, acc.: 82.81%] [G loss: 1.518139]\n",
      "epoch:27 step:25831 [D loss: 0.638619, acc.: 61.72%] [G loss: 1.074890]\n",
      "epoch:27 step:25832 [D loss: 0.595306, acc.: 64.84%] [G loss: 1.222921]\n",
      "epoch:27 step:25833 [D loss: 0.700232, acc.: 58.59%] [G loss: 1.791841]\n",
      "epoch:27 step:25834 [D loss: 0.590505, acc.: 71.09%] [G loss: 1.308827]\n",
      "epoch:27 step:25835 [D loss: 0.786996, acc.: 50.78%] [G loss: 1.582431]\n",
      "epoch:27 step:25836 [D loss: 0.692804, acc.: 60.16%] [G loss: 1.300146]\n",
      "epoch:27 step:25837 [D loss: 0.696787, acc.: 59.38%] [G loss: 0.815811]\n",
      "epoch:27 step:25838 [D loss: 0.295079, acc.: 92.97%] [G loss: 1.817682]\n",
      "epoch:27 step:25839 [D loss: 0.373937, acc.: 89.06%] [G loss: 1.596829]\n",
      "epoch:27 step:25840 [D loss: 0.610512, acc.: 65.62%] [G loss: 1.371939]\n",
      "epoch:27 step:25841 [D loss: 0.543368, acc.: 70.31%] [G loss: 1.195715]\n",
      "epoch:27 step:25842 [D loss: 0.531613, acc.: 73.44%] [G loss: 1.745149]\n",
      "epoch:27 step:25843 [D loss: 0.655958, acc.: 64.06%] [G loss: 1.367479]\n",
      "epoch:27 step:25844 [D loss: 0.513135, acc.: 70.31%] [G loss: 1.582626]\n",
      "epoch:27 step:25845 [D loss: 0.529529, acc.: 72.66%] [G loss: 1.412102]\n",
      "epoch:27 step:25846 [D loss: 0.625332, acc.: 58.59%] [G loss: 1.519687]\n",
      "epoch:27 step:25847 [D loss: 0.672974, acc.: 60.94%] [G loss: 0.987189]\n",
      "epoch:27 step:25848 [D loss: 0.541696, acc.: 75.78%] [G loss: 1.465871]\n",
      "epoch:27 step:25849 [D loss: 0.354549, acc.: 85.16%] [G loss: 1.531051]\n",
      "epoch:27 step:25850 [D loss: 0.613585, acc.: 66.41%] [G loss: 1.352979]\n",
      "epoch:27 step:25851 [D loss: 0.452615, acc.: 79.69%] [G loss: 1.470407]\n",
      "epoch:27 step:25852 [D loss: 0.543359, acc.: 75.00%] [G loss: 1.741487]\n",
      "epoch:27 step:25853 [D loss: 0.624692, acc.: 65.62%] [G loss: 1.406305]\n",
      "epoch:27 step:25854 [D loss: 0.472064, acc.: 76.56%] [G loss: 1.795907]\n",
      "epoch:27 step:25855 [D loss: 0.421598, acc.: 79.69%] [G loss: 1.490985]\n",
      "epoch:27 step:25856 [D loss: 0.569463, acc.: 77.34%] [G loss: 1.330200]\n",
      "epoch:27 step:25857 [D loss: 0.560048, acc.: 72.66%] [G loss: 1.198844]\n",
      "epoch:27 step:25858 [D loss: 0.540829, acc.: 73.44%] [G loss: 1.134306]\n",
      "epoch:27 step:25859 [D loss: 0.639810, acc.: 65.62%] [G loss: 1.171456]\n",
      "epoch:27 step:25860 [D loss: 0.646825, acc.: 67.97%] [G loss: 1.153564]\n",
      "epoch:27 step:25861 [D loss: 0.323476, acc.: 91.41%] [G loss: 1.886405]\n",
      "epoch:27 step:25862 [D loss: 0.484728, acc.: 78.12%] [G loss: 1.729795]\n",
      "epoch:27 step:25863 [D loss: 0.421444, acc.: 84.38%] [G loss: 1.479968]\n",
      "epoch:27 step:25864 [D loss: 0.701513, acc.: 62.50%] [G loss: 1.226415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25865 [D loss: 0.432247, acc.: 82.81%] [G loss: 1.439211]\n",
      "epoch:27 step:25866 [D loss: 0.716998, acc.: 57.03%] [G loss: 1.196230]\n",
      "epoch:27 step:25867 [D loss: 0.712676, acc.: 54.69%] [G loss: 1.430776]\n",
      "epoch:27 step:25868 [D loss: 0.586029, acc.: 64.84%] [G loss: 1.651284]\n",
      "epoch:27 step:25869 [D loss: 0.638581, acc.: 61.72%] [G loss: 1.562838]\n",
      "epoch:27 step:25870 [D loss: 0.431404, acc.: 81.25%] [G loss: 1.967166]\n",
      "epoch:27 step:25871 [D loss: 0.397544, acc.: 82.81%] [G loss: 1.593523]\n",
      "epoch:27 step:25872 [D loss: 0.665399, acc.: 63.28%] [G loss: 1.511704]\n",
      "epoch:27 step:25873 [D loss: 0.509880, acc.: 74.22%] [G loss: 1.337799]\n",
      "epoch:27 step:25874 [D loss: 0.478962, acc.: 78.91%] [G loss: 0.913523]\n",
      "epoch:27 step:25875 [D loss: 0.431214, acc.: 82.03%] [G loss: 1.360463]\n",
      "epoch:27 step:25876 [D loss: 0.668332, acc.: 57.81%] [G loss: 1.348962]\n",
      "epoch:27 step:25877 [D loss: 0.513667, acc.: 75.00%] [G loss: 1.634324]\n",
      "epoch:27 step:25878 [D loss: 0.676730, acc.: 64.84%] [G loss: 1.290121]\n",
      "epoch:27 step:25879 [D loss: 0.573954, acc.: 68.75%] [G loss: 1.121548]\n",
      "epoch:27 step:25880 [D loss: 0.573582, acc.: 71.09%] [G loss: 0.960434]\n",
      "epoch:27 step:25881 [D loss: 0.729142, acc.: 56.25%] [G loss: 1.104692]\n",
      "epoch:27 step:25882 [D loss: 0.601950, acc.: 65.62%] [G loss: 1.585797]\n",
      "epoch:27 step:25883 [D loss: 0.542592, acc.: 77.34%] [G loss: 1.404628]\n",
      "epoch:27 step:25884 [D loss: 0.434583, acc.: 83.59%] [G loss: 1.783728]\n",
      "epoch:27 step:25885 [D loss: 0.541657, acc.: 71.09%] [G loss: 1.245724]\n",
      "epoch:27 step:25886 [D loss: 0.421158, acc.: 83.59%] [G loss: 1.106750]\n",
      "epoch:27 step:25887 [D loss: 0.742905, acc.: 58.59%] [G loss: 1.350482]\n",
      "epoch:27 step:25888 [D loss: 0.591552, acc.: 69.53%] [G loss: 1.193490]\n",
      "epoch:27 step:25889 [D loss: 0.364275, acc.: 86.72%] [G loss: 1.556966]\n",
      "epoch:27 step:25890 [D loss: 0.457706, acc.: 79.69%] [G loss: 1.364446]\n",
      "epoch:27 step:25891 [D loss: 0.537516, acc.: 73.44%] [G loss: 1.805407]\n",
      "epoch:27 step:25892 [D loss: 0.586925, acc.: 66.41%] [G loss: 1.446029]\n",
      "epoch:27 step:25893 [D loss: 0.675095, acc.: 60.16%] [G loss: 1.487527]\n",
      "epoch:27 step:25894 [D loss: 0.615106, acc.: 67.97%] [G loss: 1.867476]\n",
      "epoch:27 step:25895 [D loss: 0.661477, acc.: 60.16%] [G loss: 1.616652]\n",
      "epoch:27 step:25896 [D loss: 0.710831, acc.: 56.25%] [G loss: 1.245468]\n",
      "epoch:27 step:25897 [D loss: 0.731610, acc.: 57.81%] [G loss: 1.222078]\n",
      "epoch:27 step:25898 [D loss: 0.572749, acc.: 70.31%] [G loss: 1.740401]\n",
      "epoch:27 step:25899 [D loss: 0.349900, acc.: 85.94%] [G loss: 1.620940]\n",
      "epoch:27 step:25900 [D loss: 0.738914, acc.: 61.72%] [G loss: 1.232388]\n",
      "epoch:27 step:25901 [D loss: 0.582937, acc.: 68.75%] [G loss: 1.401299]\n",
      "epoch:27 step:25902 [D loss: 0.641999, acc.: 60.16%] [G loss: 1.383231]\n",
      "epoch:27 step:25903 [D loss: 0.519735, acc.: 75.00%] [G loss: 1.418630]\n",
      "epoch:27 step:25904 [D loss: 0.580424, acc.: 70.31%] [G loss: 1.878296]\n",
      "epoch:27 step:25905 [D loss: 0.712290, acc.: 57.81%] [G loss: 1.075838]\n",
      "epoch:27 step:25906 [D loss: 0.571712, acc.: 67.19%] [G loss: 1.805195]\n",
      "epoch:27 step:25907 [D loss: 0.447404, acc.: 78.12%] [G loss: 1.381385]\n",
      "epoch:27 step:25908 [D loss: 0.567855, acc.: 67.19%] [G loss: 1.640960]\n",
      "epoch:27 step:25909 [D loss: 0.477545, acc.: 80.47%] [G loss: 1.700968]\n",
      "epoch:27 step:25910 [D loss: 0.669964, acc.: 54.69%] [G loss: 1.432366]\n",
      "epoch:27 step:25911 [D loss: 0.683855, acc.: 56.25%] [G loss: 1.595218]\n",
      "epoch:27 step:25912 [D loss: 0.522362, acc.: 73.44%] [G loss: 1.490942]\n",
      "epoch:27 step:25913 [D loss: 0.694725, acc.: 57.81%] [G loss: 1.299025]\n",
      "epoch:27 step:25914 [D loss: 0.376967, acc.: 84.38%] [G loss: 1.514300]\n",
      "epoch:27 step:25915 [D loss: 0.708810, acc.: 55.47%] [G loss: 1.516252]\n",
      "epoch:27 step:25916 [D loss: 0.473882, acc.: 81.25%] [G loss: 1.861714]\n",
      "epoch:27 step:25917 [D loss: 0.426545, acc.: 82.81%] [G loss: 1.626403]\n",
      "epoch:27 step:25918 [D loss: 0.551855, acc.: 72.66%] [G loss: 1.552379]\n",
      "epoch:27 step:25919 [D loss: 0.522378, acc.: 70.31%] [G loss: 1.509241]\n",
      "epoch:27 step:25920 [D loss: 0.551137, acc.: 75.00%] [G loss: 1.453414]\n",
      "epoch:27 step:25921 [D loss: 0.519488, acc.: 75.78%] [G loss: 1.454494]\n",
      "epoch:27 step:25922 [D loss: 0.528770, acc.: 69.53%] [G loss: 1.428408]\n",
      "epoch:27 step:25923 [D loss: 0.603650, acc.: 64.06%] [G loss: 1.690904]\n",
      "epoch:27 step:25924 [D loss: 0.471882, acc.: 79.69%] [G loss: 1.512089]\n",
      "epoch:27 step:25925 [D loss: 0.411645, acc.: 85.94%] [G loss: 1.995745]\n",
      "epoch:27 step:25926 [D loss: 0.373085, acc.: 87.50%] [G loss: 2.269713]\n",
      "epoch:27 step:25927 [D loss: 0.447227, acc.: 75.78%] [G loss: 1.772884]\n",
      "epoch:27 step:25928 [D loss: 0.506207, acc.: 76.56%] [G loss: 1.429983]\n",
      "epoch:27 step:25929 [D loss: 0.411088, acc.: 84.38%] [G loss: 1.834711]\n",
      "epoch:27 step:25930 [D loss: 0.487947, acc.: 78.12%] [G loss: 1.436605]\n",
      "epoch:27 step:25931 [D loss: 0.392522, acc.: 85.16%] [G loss: 1.339721]\n",
      "epoch:27 step:25932 [D loss: 0.463278, acc.: 79.69%] [G loss: 1.382704]\n",
      "epoch:27 step:25933 [D loss: 0.530024, acc.: 70.31%] [G loss: 1.422015]\n",
      "epoch:27 step:25934 [D loss: 0.421287, acc.: 84.38%] [G loss: 1.818199]\n",
      "epoch:27 step:25935 [D loss: 0.491719, acc.: 79.69%] [G loss: 1.481046]\n",
      "epoch:27 step:25936 [D loss: 0.542815, acc.: 72.66%] [G loss: 1.374893]\n",
      "epoch:27 step:25937 [D loss: 0.439065, acc.: 79.69%] [G loss: 1.630433]\n",
      "epoch:27 step:25938 [D loss: 0.542879, acc.: 71.88%] [G loss: 1.313119]\n",
      "epoch:27 step:25939 [D loss: 0.430970, acc.: 82.03%] [G loss: 1.412149]\n",
      "epoch:27 step:25940 [D loss: 0.799023, acc.: 52.34%] [G loss: 1.212694]\n",
      "epoch:27 step:25941 [D loss: 0.517207, acc.: 73.44%] [G loss: 1.453864]\n",
      "epoch:27 step:25942 [D loss: 0.623374, acc.: 67.97%] [G loss: 1.077411]\n",
      "epoch:27 step:25943 [D loss: 0.582016, acc.: 68.75%] [G loss: 1.364538]\n",
      "epoch:27 step:25944 [D loss: 0.459900, acc.: 77.34%] [G loss: 1.485365]\n",
      "epoch:27 step:25945 [D loss: 0.597029, acc.: 66.41%] [G loss: 1.616615]\n",
      "epoch:27 step:25946 [D loss: 0.745055, acc.: 57.03%] [G loss: 0.981769]\n",
      "epoch:27 step:25947 [D loss: 0.562924, acc.: 67.19%] [G loss: 1.387294]\n",
      "epoch:27 step:25948 [D loss: 0.533006, acc.: 73.44%] [G loss: 1.427347]\n",
      "epoch:27 step:25949 [D loss: 0.581561, acc.: 66.41%] [G loss: 1.167938]\n",
      "epoch:27 step:25950 [D loss: 0.386649, acc.: 85.94%] [G loss: 1.251158]\n",
      "epoch:27 step:25951 [D loss: 0.706715, acc.: 60.16%] [G loss: 1.373902]\n",
      "epoch:27 step:25952 [D loss: 0.636510, acc.: 67.19%] [G loss: 1.373150]\n",
      "epoch:27 step:25953 [D loss: 0.541524, acc.: 73.44%] [G loss: 1.638764]\n",
      "epoch:27 step:25954 [D loss: 0.659609, acc.: 63.28%] [G loss: 0.975669]\n",
      "epoch:27 step:25955 [D loss: 0.406339, acc.: 82.03%] [G loss: 1.396806]\n",
      "epoch:27 step:25956 [D loss: 0.576012, acc.: 69.53%] [G loss: 1.381074]\n",
      "epoch:27 step:25957 [D loss: 0.624739, acc.: 66.41%] [G loss: 1.152052]\n",
      "epoch:27 step:25958 [D loss: 0.538840, acc.: 72.66%] [G loss: 1.624279]\n",
      "epoch:27 step:25959 [D loss: 0.396836, acc.: 82.81%] [G loss: 1.850847]\n",
      "epoch:27 step:25960 [D loss: 0.737630, acc.: 60.16%] [G loss: 0.911529]\n",
      "epoch:27 step:25961 [D loss: 0.641004, acc.: 63.28%] [G loss: 1.221952]\n",
      "epoch:27 step:25962 [D loss: 0.700195, acc.: 59.38%] [G loss: 1.151620]\n",
      "epoch:27 step:25963 [D loss: 0.575002, acc.: 71.88%] [G loss: 1.217162]\n",
      "epoch:27 step:25964 [D loss: 0.452399, acc.: 78.91%] [G loss: 1.840492]\n",
      "epoch:27 step:25965 [D loss: 0.495657, acc.: 73.44%] [G loss: 1.593430]\n",
      "epoch:27 step:25966 [D loss: 0.439090, acc.: 77.34%] [G loss: 0.997060]\n",
      "epoch:27 step:25967 [D loss: 0.761332, acc.: 58.59%] [G loss: 1.055045]\n",
      "epoch:27 step:25968 [D loss: 0.462585, acc.: 79.69%] [G loss: 1.500694]\n",
      "epoch:27 step:25969 [D loss: 0.507341, acc.: 76.56%] [G loss: 1.305279]\n",
      "epoch:27 step:25970 [D loss: 0.489844, acc.: 78.91%] [G loss: 1.398318]\n",
      "epoch:27 step:25971 [D loss: 0.512320, acc.: 78.12%] [G loss: 1.638808]\n",
      "epoch:27 step:25972 [D loss: 0.776795, acc.: 49.22%] [G loss: 0.615528]\n",
      "epoch:27 step:25973 [D loss: 0.497549, acc.: 77.34%] [G loss: 1.313856]\n",
      "epoch:27 step:25974 [D loss: 0.417333, acc.: 84.38%] [G loss: 1.552496]\n",
      "epoch:27 step:25975 [D loss: 0.562679, acc.: 74.22%] [G loss: 1.084324]\n",
      "epoch:27 step:25976 [D loss: 0.567464, acc.: 67.97%] [G loss: 1.473777]\n",
      "epoch:27 step:25977 [D loss: 0.593741, acc.: 65.62%] [G loss: 1.458333]\n",
      "epoch:27 step:25978 [D loss: 0.609707, acc.: 67.19%] [G loss: 1.536852]\n",
      "epoch:27 step:25979 [D loss: 0.660212, acc.: 63.28%] [G loss: 1.688925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25980 [D loss: 0.569987, acc.: 67.97%] [G loss: 1.634803]\n",
      "epoch:27 step:25981 [D loss: 0.547532, acc.: 71.88%] [G loss: 1.221295]\n",
      "epoch:27 step:25982 [D loss: 0.472360, acc.: 78.12%] [G loss: 1.201192]\n",
      "epoch:27 step:25983 [D loss: 0.479871, acc.: 75.00%] [G loss: 1.383775]\n",
      "epoch:27 step:25984 [D loss: 0.681425, acc.: 64.06%] [G loss: 1.433468]\n",
      "epoch:27 step:25985 [D loss: 0.432284, acc.: 83.59%] [G loss: 1.238052]\n",
      "epoch:27 step:25986 [D loss: 0.422032, acc.: 83.59%] [G loss: 1.365865]\n",
      "epoch:27 step:25987 [D loss: 0.544178, acc.: 73.44%] [G loss: 1.394273]\n",
      "epoch:27 step:25988 [D loss: 0.726282, acc.: 60.94%] [G loss: 1.341249]\n",
      "epoch:27 step:25989 [D loss: 0.506150, acc.: 74.22%] [G loss: 1.200169]\n",
      "epoch:27 step:25990 [D loss: 0.824336, acc.: 49.22%] [G loss: 1.414727]\n",
      "epoch:27 step:25991 [D loss: 0.590863, acc.: 71.09%] [G loss: 1.062874]\n",
      "epoch:27 step:25992 [D loss: 0.565698, acc.: 68.75%] [G loss: 1.664998]\n",
      "epoch:27 step:25993 [D loss: 0.681207, acc.: 59.38%] [G loss: 1.183069]\n",
      "epoch:27 step:25994 [D loss: 0.544481, acc.: 70.31%] [G loss: 1.334630]\n",
      "epoch:27 step:25995 [D loss: 0.531116, acc.: 75.00%] [G loss: 1.539645]\n",
      "epoch:27 step:25996 [D loss: 0.534495, acc.: 71.88%] [G loss: 1.353521]\n",
      "epoch:27 step:25997 [D loss: 0.563829, acc.: 70.31%] [G loss: 1.733869]\n",
      "epoch:27 step:25998 [D loss: 0.378958, acc.: 83.59%] [G loss: 1.460263]\n",
      "epoch:27 step:25999 [D loss: 0.594535, acc.: 67.19%] [G loss: 1.431022]\n",
      "epoch:27 step:26000 [D loss: 0.588876, acc.: 71.09%] [G loss: 1.462242]\n",
      "##############\n",
      "[2.78767496 1.9817787  1.91367514 2.73803603 0.89427462 6.29058956\n",
      " 2.16604412 2.41593632 3.88353793 6.21053286]\n",
      "##########\n",
      "epoch:27 step:26001 [D loss: 0.715413, acc.: 53.91%] [G loss: 1.692742]\n",
      "epoch:27 step:26002 [D loss: 0.591545, acc.: 64.06%] [G loss: 1.455998]\n",
      "epoch:27 step:26003 [D loss: 0.666385, acc.: 63.28%] [G loss: 1.349366]\n",
      "epoch:27 step:26004 [D loss: 0.458373, acc.: 76.56%] [G loss: 1.461400]\n",
      "epoch:27 step:26005 [D loss: 0.808165, acc.: 50.00%] [G loss: 1.212779]\n",
      "epoch:27 step:26006 [D loss: 0.600469, acc.: 68.75%] [G loss: 1.422449]\n",
      "epoch:27 step:26007 [D loss: 0.456940, acc.: 79.69%] [G loss: 2.033657]\n",
      "epoch:27 step:26008 [D loss: 0.593649, acc.: 67.97%] [G loss: 1.372881]\n",
      "epoch:27 step:26009 [D loss: 0.498090, acc.: 75.78%] [G loss: 1.477431]\n",
      "epoch:27 step:26010 [D loss: 0.465065, acc.: 78.91%] [G loss: 1.526491]\n",
      "epoch:27 step:26011 [D loss: 0.486113, acc.: 78.91%] [G loss: 1.688157]\n",
      "epoch:27 step:26012 [D loss: 0.476933, acc.: 79.69%] [G loss: 1.284948]\n",
      "epoch:27 step:26013 [D loss: 0.564275, acc.: 76.56%] [G loss: 1.785634]\n",
      "epoch:27 step:26014 [D loss: 0.518637, acc.: 78.12%] [G loss: 1.639315]\n",
      "epoch:27 step:26015 [D loss: 0.563682, acc.: 73.44%] [G loss: 1.508225]\n",
      "epoch:27 step:26016 [D loss: 0.612946, acc.: 62.50%] [G loss: 1.133515]\n",
      "epoch:27 step:26017 [D loss: 0.381954, acc.: 87.50%] [G loss: 1.398879]\n",
      "epoch:27 step:26018 [D loss: 0.581849, acc.: 69.53%] [G loss: 1.310535]\n",
      "epoch:27 step:26019 [D loss: 0.502342, acc.: 74.22%] [G loss: 1.940138]\n",
      "epoch:27 step:26020 [D loss: 0.636184, acc.: 61.72%] [G loss: 1.702613]\n",
      "epoch:27 step:26021 [D loss: 0.634337, acc.: 61.72%] [G loss: 1.328801]\n",
      "epoch:27 step:26022 [D loss: 0.629316, acc.: 66.41%] [G loss: 1.216311]\n",
      "epoch:27 step:26023 [D loss: 0.467656, acc.: 80.47%] [G loss: 1.624783]\n",
      "epoch:27 step:26024 [D loss: 0.488327, acc.: 76.56%] [G loss: 1.520259]\n",
      "epoch:27 step:26025 [D loss: 0.675831, acc.: 57.03%] [G loss: 1.230773]\n",
      "epoch:27 step:26026 [D loss: 0.503202, acc.: 76.56%] [G loss: 1.588165]\n",
      "epoch:27 step:26027 [D loss: 0.619066, acc.: 64.06%] [G loss: 1.356939]\n",
      "epoch:27 step:26028 [D loss: 0.507800, acc.: 77.34%] [G loss: 1.587529]\n",
      "epoch:27 step:26029 [D loss: 0.534068, acc.: 77.34%] [G loss: 1.426058]\n",
      "epoch:27 step:26030 [D loss: 0.612361, acc.: 65.62%] [G loss: 1.055202]\n",
      "epoch:27 step:26031 [D loss: 0.560859, acc.: 70.31%] [G loss: 1.272607]\n",
      "epoch:27 step:26032 [D loss: 0.492707, acc.: 78.12%] [G loss: 1.762241]\n",
      "epoch:27 step:26033 [D loss: 0.432197, acc.: 81.25%] [G loss: 1.575149]\n",
      "epoch:27 step:26034 [D loss: 0.673947, acc.: 66.41%] [G loss: 1.613445]\n",
      "epoch:27 step:26035 [D loss: 0.543172, acc.: 76.56%] [G loss: 1.491653]\n",
      "epoch:27 step:26036 [D loss: 0.607622, acc.: 68.75%] [G loss: 1.338732]\n",
      "epoch:27 step:26037 [D loss: 0.743110, acc.: 52.34%] [G loss: 1.541369]\n",
      "epoch:27 step:26038 [D loss: 0.486112, acc.: 77.34%] [G loss: 1.711198]\n",
      "epoch:27 step:26039 [D loss: 0.567414, acc.: 69.53%] [G loss: 1.289704]\n",
      "epoch:27 step:26040 [D loss: 0.499809, acc.: 75.78%] [G loss: 1.367357]\n",
      "epoch:27 step:26041 [D loss: 0.441458, acc.: 85.16%] [G loss: 1.124901]\n",
      "epoch:27 step:26042 [D loss: 0.514203, acc.: 71.88%] [G loss: 1.530038]\n",
      "epoch:27 step:26043 [D loss: 0.718317, acc.: 53.91%] [G loss: 1.052583]\n",
      "epoch:27 step:26044 [D loss: 0.467480, acc.: 79.69%] [G loss: 1.422972]\n",
      "epoch:27 step:26045 [D loss: 0.461303, acc.: 80.47%] [G loss: 1.487063]\n",
      "epoch:27 step:26046 [D loss: 0.336706, acc.: 90.62%] [G loss: 1.746117]\n",
      "epoch:27 step:26047 [D loss: 0.575615, acc.: 69.53%] [G loss: 1.733493]\n",
      "epoch:27 step:26048 [D loss: 0.671359, acc.: 59.38%] [G loss: 1.700010]\n",
      "epoch:27 step:26049 [D loss: 0.558362, acc.: 70.31%] [G loss: 1.352573]\n",
      "epoch:27 step:26050 [D loss: 0.396567, acc.: 87.50%] [G loss: 1.419872]\n",
      "epoch:27 step:26051 [D loss: 0.543877, acc.: 75.78%] [G loss: 1.537314]\n",
      "epoch:27 step:26052 [D loss: 0.535544, acc.: 78.12%] [G loss: 1.494412]\n",
      "epoch:27 step:26053 [D loss: 0.656108, acc.: 65.62%] [G loss: 1.580460]\n",
      "epoch:27 step:26054 [D loss: 0.580826, acc.: 71.09%] [G loss: 1.355624]\n",
      "epoch:27 step:26055 [D loss: 0.665031, acc.: 64.06%] [G loss: 1.262651]\n",
      "epoch:27 step:26056 [D loss: 0.472726, acc.: 78.91%] [G loss: 1.213237]\n",
      "epoch:27 step:26057 [D loss: 0.614531, acc.: 67.97%] [G loss: 1.092199]\n",
      "epoch:27 step:26058 [D loss: 0.450394, acc.: 79.69%] [G loss: 1.368975]\n",
      "epoch:27 step:26059 [D loss: 0.437778, acc.: 83.59%] [G loss: 1.770266]\n",
      "epoch:27 step:26060 [D loss: 0.749956, acc.: 53.12%] [G loss: 1.226407]\n",
      "epoch:27 step:26061 [D loss: 0.522898, acc.: 71.09%] [G loss: 1.614082]\n",
      "epoch:27 step:26062 [D loss: 0.619801, acc.: 66.41%] [G loss: 1.516580]\n",
      "epoch:27 step:26063 [D loss: 0.419766, acc.: 82.81%] [G loss: 1.558523]\n",
      "epoch:27 step:26064 [D loss: 0.494738, acc.: 75.78%] [G loss: 1.700095]\n",
      "epoch:27 step:26065 [D loss: 0.505729, acc.: 76.56%] [G loss: 1.204687]\n",
      "epoch:27 step:26066 [D loss: 0.364039, acc.: 85.16%] [G loss: 2.134257]\n",
      "epoch:27 step:26067 [D loss: 0.552306, acc.: 71.09%] [G loss: 1.474181]\n",
      "epoch:27 step:26068 [D loss: 0.506379, acc.: 76.56%] [G loss: 1.619616]\n",
      "epoch:27 step:26069 [D loss: 0.565116, acc.: 74.22%] [G loss: 1.178332]\n",
      "epoch:27 step:26070 [D loss: 0.563961, acc.: 75.00%] [G loss: 1.178276]\n",
      "epoch:27 step:26071 [D loss: 0.585323, acc.: 66.41%] [G loss: 1.311615]\n",
      "epoch:27 step:26072 [D loss: 0.644693, acc.: 65.62%] [G loss: 1.247284]\n",
      "epoch:27 step:26073 [D loss: 0.683039, acc.: 60.16%] [G loss: 1.757509]\n",
      "epoch:27 step:26074 [D loss: 0.348403, acc.: 89.84%] [G loss: 2.094853]\n",
      "epoch:27 step:26075 [D loss: 0.439532, acc.: 82.03%] [G loss: 1.281030]\n",
      "epoch:27 step:26076 [D loss: 0.562315, acc.: 70.31%] [G loss: 1.109696]\n",
      "epoch:27 step:26077 [D loss: 0.507972, acc.: 74.22%] [G loss: 1.393700]\n",
      "epoch:27 step:26078 [D loss: 0.636573, acc.: 66.41%] [G loss: 1.545834]\n",
      "epoch:27 step:26079 [D loss: 0.583651, acc.: 71.09%] [G loss: 1.534665]\n",
      "epoch:27 step:26080 [D loss: 0.509846, acc.: 73.44%] [G loss: 1.103607]\n",
      "epoch:27 step:26081 [D loss: 0.484946, acc.: 78.12%] [G loss: 1.303478]\n",
      "epoch:27 step:26082 [D loss: 0.504871, acc.: 75.00%] [G loss: 1.665325]\n",
      "epoch:27 step:26083 [D loss: 0.472147, acc.: 79.69%] [G loss: 1.022912]\n",
      "epoch:27 step:26084 [D loss: 0.477157, acc.: 79.69%] [G loss: 1.496311]\n",
      "epoch:27 step:26085 [D loss: 0.866314, acc.: 46.88%] [G loss: 0.937557]\n",
      "epoch:27 step:26086 [D loss: 0.617251, acc.: 67.19%] [G loss: 1.666660]\n",
      "epoch:27 step:26087 [D loss: 0.468057, acc.: 82.03%] [G loss: 1.356506]\n",
      "epoch:27 step:26088 [D loss: 0.391256, acc.: 89.06%] [G loss: 1.579434]\n",
      "epoch:27 step:26089 [D loss: 0.481167, acc.: 79.69%] [G loss: 1.308830]\n",
      "epoch:27 step:26090 [D loss: 0.405168, acc.: 87.50%] [G loss: 1.321246]\n",
      "epoch:27 step:26091 [D loss: 0.693375, acc.: 59.38%] [G loss: 1.278156]\n",
      "epoch:27 step:26092 [D loss: 0.607630, acc.: 66.41%] [G loss: 1.889502]\n",
      "epoch:27 step:26093 [D loss: 0.461521, acc.: 83.59%] [G loss: 1.606606]\n",
      "epoch:27 step:26094 [D loss: 0.502917, acc.: 75.00%] [G loss: 1.269103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26095 [D loss: 0.607367, acc.: 69.53%] [G loss: 1.376662]\n",
      "epoch:27 step:26096 [D loss: 0.554963, acc.: 71.09%] [G loss: 1.428451]\n",
      "epoch:27 step:26097 [D loss: 0.605274, acc.: 68.75%] [G loss: 1.058180]\n",
      "epoch:27 step:26098 [D loss: 0.497047, acc.: 76.56%] [G loss: 1.946387]\n",
      "epoch:27 step:26099 [D loss: 0.774880, acc.: 51.56%] [G loss: 1.275034]\n",
      "epoch:27 step:26100 [D loss: 0.565375, acc.: 71.09%] [G loss: 1.233511]\n",
      "epoch:27 step:26101 [D loss: 0.566484, acc.: 71.09%] [G loss: 1.701598]\n",
      "epoch:27 step:26102 [D loss: 0.505983, acc.: 75.78%] [G loss: 1.223402]\n",
      "epoch:27 step:26103 [D loss: 0.484624, acc.: 76.56%] [G loss: 1.220484]\n",
      "epoch:27 step:26104 [D loss: 0.611673, acc.: 67.19%] [G loss: 1.157985]\n",
      "epoch:27 step:26105 [D loss: 0.426430, acc.: 81.25%] [G loss: 1.591774]\n",
      "epoch:27 step:26106 [D loss: 0.675859, acc.: 63.28%] [G loss: 1.100502]\n",
      "epoch:27 step:26107 [D loss: 0.462525, acc.: 75.00%] [G loss: 1.347762]\n",
      "epoch:27 step:26108 [D loss: 0.504381, acc.: 77.34%] [G loss: 1.311807]\n",
      "epoch:27 step:26109 [D loss: 0.388870, acc.: 86.72%] [G loss: 1.815059]\n",
      "epoch:27 step:26110 [D loss: 0.579956, acc.: 68.75%] [G loss: 1.349840]\n",
      "epoch:27 step:26111 [D loss: 0.505350, acc.: 75.78%] [G loss: 1.662660]\n",
      "epoch:27 step:26112 [D loss: 0.396043, acc.: 86.72%] [G loss: 1.620992]\n",
      "epoch:27 step:26113 [D loss: 0.461409, acc.: 79.69%] [G loss: 1.082592]\n",
      "epoch:27 step:26114 [D loss: 0.626479, acc.: 66.41%] [G loss: 1.301494]\n",
      "epoch:27 step:26115 [D loss: 0.505338, acc.: 76.56%] [G loss: 1.438422]\n",
      "epoch:27 step:26116 [D loss: 0.630198, acc.: 64.06%] [G loss: 1.208426]\n",
      "epoch:27 step:26117 [D loss: 0.508185, acc.: 76.56%] [G loss: 1.998428]\n",
      "epoch:27 step:26118 [D loss: 0.374316, acc.: 87.50%] [G loss: 1.579664]\n",
      "epoch:27 step:26119 [D loss: 0.646315, acc.: 67.19%] [G loss: 1.307857]\n",
      "epoch:27 step:26120 [D loss: 0.678078, acc.: 66.41%] [G loss: 1.300499]\n",
      "epoch:27 step:26121 [D loss: 0.421381, acc.: 81.25%] [G loss: 1.318930]\n",
      "epoch:27 step:26122 [D loss: 0.693794, acc.: 60.94%] [G loss: 1.341649]\n",
      "epoch:27 step:26123 [D loss: 0.559819, acc.: 71.09%] [G loss: 1.539060]\n",
      "epoch:27 step:26124 [D loss: 0.439880, acc.: 82.03%] [G loss: 2.064708]\n",
      "epoch:27 step:26125 [D loss: 0.642871, acc.: 61.72%] [G loss: 1.239464]\n",
      "epoch:27 step:26126 [D loss: 0.640114, acc.: 64.84%] [G loss: 1.943435]\n",
      "epoch:27 step:26127 [D loss: 0.623715, acc.: 67.19%] [G loss: 1.523952]\n",
      "epoch:27 step:26128 [D loss: 0.598671, acc.: 67.97%] [G loss: 1.459474]\n",
      "epoch:27 step:26129 [D loss: 0.516391, acc.: 77.34%] [G loss: 1.542234]\n",
      "epoch:27 step:26130 [D loss: 0.715065, acc.: 58.59%] [G loss: 1.458049]\n",
      "epoch:27 step:26131 [D loss: 0.481870, acc.: 75.00%] [G loss: 1.877507]\n",
      "epoch:27 step:26132 [D loss: 0.653073, acc.: 61.72%] [G loss: 1.148515]\n",
      "epoch:27 step:26133 [D loss: 0.592141, acc.: 74.22%] [G loss: 0.948377]\n",
      "epoch:27 step:26134 [D loss: 0.492025, acc.: 75.78%] [G loss: 1.498868]\n",
      "epoch:27 step:26135 [D loss: 0.630221, acc.: 63.28%] [G loss: 1.203216]\n",
      "epoch:27 step:26136 [D loss: 0.675544, acc.: 64.06%] [G loss: 1.467977]\n",
      "epoch:27 step:26137 [D loss: 0.444053, acc.: 78.12%] [G loss: 1.530422]\n",
      "epoch:27 step:26138 [D loss: 0.644004, acc.: 64.84%] [G loss: 1.349526]\n",
      "epoch:27 step:26139 [D loss: 0.566202, acc.: 69.53%] [G loss: 1.580815]\n",
      "epoch:27 step:26140 [D loss: 0.610529, acc.: 65.62%] [G loss: 0.618220]\n",
      "epoch:27 step:26141 [D loss: 0.601967, acc.: 65.62%] [G loss: 1.369987]\n",
      "epoch:27 step:26142 [D loss: 0.817852, acc.: 53.91%] [G loss: 1.107942]\n",
      "epoch:27 step:26143 [D loss: 0.700649, acc.: 58.59%] [G loss: 1.536802]\n",
      "epoch:27 step:26144 [D loss: 0.489488, acc.: 76.56%] [G loss: 1.946511]\n",
      "epoch:27 step:26145 [D loss: 0.628241, acc.: 70.31%] [G loss: 1.445943]\n",
      "epoch:27 step:26146 [D loss: 0.457800, acc.: 78.12%] [G loss: 1.372333]\n",
      "epoch:27 step:26147 [D loss: 0.585903, acc.: 67.97%] [G loss: 1.401412]\n",
      "epoch:27 step:26148 [D loss: 0.635512, acc.: 64.06%] [G loss: 1.391372]\n",
      "epoch:27 step:26149 [D loss: 0.543784, acc.: 71.09%] [G loss: 1.496172]\n",
      "epoch:27 step:26150 [D loss: 0.706419, acc.: 61.72%] [G loss: 1.039210]\n",
      "epoch:27 step:26151 [D loss: 0.316987, acc.: 92.97%] [G loss: 1.680781]\n",
      "epoch:27 step:26152 [D loss: 0.398236, acc.: 86.72%] [G loss: 1.681433]\n",
      "epoch:27 step:26153 [D loss: 0.536295, acc.: 72.66%] [G loss: 1.642389]\n",
      "epoch:27 step:26154 [D loss: 0.544419, acc.: 71.09%] [G loss: 1.521458]\n",
      "epoch:27 step:26155 [D loss: 0.403621, acc.: 81.25%] [G loss: 1.492046]\n",
      "epoch:27 step:26156 [D loss: 0.566217, acc.: 68.75%] [G loss: 1.725462]\n",
      "epoch:27 step:26157 [D loss: 0.422334, acc.: 85.94%] [G loss: 1.992191]\n",
      "epoch:27 step:26158 [D loss: 0.583854, acc.: 68.75%] [G loss: 1.328816]\n",
      "epoch:27 step:26159 [D loss: 0.724798, acc.: 55.47%] [G loss: 1.271721]\n",
      "epoch:27 step:26160 [D loss: 0.490394, acc.: 77.34%] [G loss: 1.598151]\n",
      "epoch:27 step:26161 [D loss: 0.637775, acc.: 64.84%] [G loss: 1.238250]\n",
      "epoch:27 step:26162 [D loss: 0.647797, acc.: 67.97%] [G loss: 1.310491]\n",
      "epoch:27 step:26163 [D loss: 0.570135, acc.: 69.53%] [G loss: 1.217210]\n",
      "epoch:27 step:26164 [D loss: 0.539532, acc.: 76.56%] [G loss: 1.308541]\n",
      "epoch:27 step:26165 [D loss: 0.432051, acc.: 79.69%] [G loss: 1.536166]\n",
      "epoch:27 step:26166 [D loss: 0.501486, acc.: 78.12%] [G loss: 1.592274]\n",
      "epoch:27 step:26167 [D loss: 0.658647, acc.: 64.84%] [G loss: 1.452868]\n",
      "epoch:27 step:26168 [D loss: 0.598794, acc.: 66.41%] [G loss: 1.214285]\n",
      "epoch:27 step:26169 [D loss: 0.387624, acc.: 85.16%] [G loss: 1.629183]\n",
      "epoch:27 step:26170 [D loss: 0.485189, acc.: 78.91%] [G loss: 1.323497]\n",
      "epoch:27 step:26171 [D loss: 0.410961, acc.: 82.03%] [G loss: 1.288898]\n",
      "epoch:27 step:26172 [D loss: 0.452989, acc.: 84.38%] [G loss: 1.610278]\n",
      "epoch:27 step:26173 [D loss: 0.521103, acc.: 72.66%] [G loss: 1.428541]\n",
      "epoch:27 step:26174 [D loss: 0.613378, acc.: 62.50%] [G loss: 1.423452]\n",
      "epoch:27 step:26175 [D loss: 0.620252, acc.: 69.53%] [G loss: 1.371771]\n",
      "epoch:27 step:26176 [D loss: 0.592440, acc.: 67.97%] [G loss: 1.524165]\n",
      "epoch:27 step:26177 [D loss: 0.427825, acc.: 80.47%] [G loss: 1.034545]\n",
      "epoch:27 step:26178 [D loss: 0.494739, acc.: 76.56%] [G loss: 1.550560]\n",
      "epoch:27 step:26179 [D loss: 0.449567, acc.: 78.91%] [G loss: 1.426047]\n",
      "epoch:27 step:26180 [D loss: 0.544528, acc.: 67.97%] [G loss: 1.611222]\n",
      "epoch:27 step:26181 [D loss: 0.331045, acc.: 92.19%] [G loss: 1.649374]\n",
      "epoch:27 step:26182 [D loss: 0.660719, acc.: 61.72%] [G loss: 1.479262]\n",
      "epoch:27 step:26183 [D loss: 0.410881, acc.: 82.81%] [G loss: 1.600931]\n",
      "epoch:27 step:26184 [D loss: 0.562362, acc.: 72.66%] [G loss: 1.591297]\n",
      "epoch:27 step:26185 [D loss: 0.525595, acc.: 75.00%] [G loss: 1.961368]\n",
      "epoch:27 step:26186 [D loss: 0.587410, acc.: 69.53%] [G loss: 1.481923]\n",
      "epoch:27 step:26187 [D loss: 0.654042, acc.: 60.94%] [G loss: 0.998086]\n",
      "epoch:27 step:26188 [D loss: 0.752632, acc.: 52.34%] [G loss: 1.193837]\n",
      "epoch:27 step:26189 [D loss: 0.572791, acc.: 65.62%] [G loss: 1.278313]\n",
      "epoch:27 step:26190 [D loss: 0.452003, acc.: 80.47%] [G loss: 1.647602]\n",
      "epoch:27 step:26191 [D loss: 0.468226, acc.: 78.12%] [G loss: 1.277523]\n",
      "epoch:27 step:26192 [D loss: 0.648640, acc.: 58.59%] [G loss: 0.962378]\n",
      "epoch:27 step:26193 [D loss: 0.598650, acc.: 67.97%] [G loss: 1.069111]\n",
      "epoch:27 step:26194 [D loss: 0.586277, acc.: 71.09%] [G loss: 1.490110]\n",
      "epoch:27 step:26195 [D loss: 0.812124, acc.: 50.78%] [G loss: 1.099869]\n",
      "epoch:27 step:26196 [D loss: 0.604571, acc.: 68.75%] [G loss: 1.417400]\n",
      "epoch:27 step:26197 [D loss: 0.601674, acc.: 67.97%] [G loss: 1.295638]\n",
      "epoch:27 step:26198 [D loss: 0.524270, acc.: 71.09%] [G loss: 1.039587]\n",
      "epoch:27 step:26199 [D loss: 0.430445, acc.: 78.12%] [G loss: 1.870411]\n",
      "epoch:27 step:26200 [D loss: 0.543807, acc.: 70.31%] [G loss: 1.292713]\n",
      "##############\n",
      "[2.78801739 1.97132601 1.74303948 2.76455891 0.84326941 6.10756465\n",
      " 2.18277327 2.80468636 3.83282023 5.84623184]\n",
      "##########\n",
      "epoch:27 step:26201 [D loss: 0.745436, acc.: 60.94%] [G loss: 1.039901]\n",
      "epoch:27 step:26202 [D loss: 0.544953, acc.: 71.09%] [G loss: 1.480251]\n",
      "epoch:27 step:26203 [D loss: 0.513390, acc.: 75.78%] [G loss: 1.182462]\n",
      "epoch:27 step:26204 [D loss: 0.632089, acc.: 69.53%] [G loss: 1.406065]\n",
      "epoch:27 step:26205 [D loss: 0.674360, acc.: 62.50%] [G loss: 1.818258]\n",
      "epoch:27 step:26206 [D loss: 0.445284, acc.: 78.12%] [G loss: 1.672973]\n",
      "epoch:27 step:26207 [D loss: 0.758449, acc.: 53.91%] [G loss: 1.434976]\n",
      "epoch:27 step:26208 [D loss: 0.411103, acc.: 85.16%] [G loss: 1.714135]\n",
      "epoch:27 step:26209 [D loss: 0.516754, acc.: 71.88%] [G loss: 1.411559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26210 [D loss: 0.468199, acc.: 77.34%] [G loss: 1.581308]\n",
      "epoch:27 step:26211 [D loss: 0.523043, acc.: 76.56%] [G loss: 1.154655]\n",
      "epoch:27 step:26212 [D loss: 0.590327, acc.: 67.19%] [G loss: 1.230814]\n",
      "epoch:27 step:26213 [D loss: 0.670051, acc.: 61.72%] [G loss: 1.167320]\n",
      "epoch:27 step:26214 [D loss: 0.627539, acc.: 65.62%] [G loss: 1.354386]\n",
      "epoch:27 step:26215 [D loss: 0.630899, acc.: 63.28%] [G loss: 1.548738]\n",
      "epoch:27 step:26216 [D loss: 0.576574, acc.: 71.09%] [G loss: 1.600618]\n",
      "epoch:27 step:26217 [D loss: 0.722879, acc.: 57.03%] [G loss: 1.350128]\n",
      "epoch:27 step:26218 [D loss: 0.531522, acc.: 75.78%] [G loss: 1.301429]\n",
      "epoch:27 step:26219 [D loss: 0.530729, acc.: 74.22%] [G loss: 1.743933]\n",
      "epoch:27 step:26220 [D loss: 0.565795, acc.: 70.31%] [G loss: 1.305492]\n",
      "epoch:27 step:26221 [D loss: 0.600255, acc.: 70.31%] [G loss: 1.543005]\n",
      "epoch:27 step:26222 [D loss: 0.577176, acc.: 70.31%] [G loss: 1.530942]\n",
      "epoch:27 step:26223 [D loss: 0.522991, acc.: 78.12%] [G loss: 1.272057]\n",
      "epoch:27 step:26224 [D loss: 0.527145, acc.: 79.69%] [G loss: 1.004686]\n",
      "epoch:27 step:26225 [D loss: 0.479502, acc.: 78.12%] [G loss: 1.594030]\n",
      "epoch:27 step:26226 [D loss: 0.525962, acc.: 70.31%] [G loss: 1.341601]\n",
      "epoch:27 step:26227 [D loss: 0.515470, acc.: 75.00%] [G loss: 1.612063]\n",
      "epoch:27 step:26228 [D loss: 0.576716, acc.: 70.31%] [G loss: 1.422452]\n",
      "epoch:27 step:26229 [D loss: 0.692140, acc.: 60.94%] [G loss: 2.208323]\n",
      "epoch:27 step:26230 [D loss: 0.501748, acc.: 78.91%] [G loss: 1.434450]\n",
      "epoch:27 step:26231 [D loss: 0.677494, acc.: 64.06%] [G loss: 1.278568]\n",
      "epoch:27 step:26232 [D loss: 0.665387, acc.: 62.50%] [G loss: 1.565949]\n",
      "epoch:27 step:26233 [D loss: 0.588709, acc.: 67.19%] [G loss: 1.388024]\n",
      "epoch:27 step:26234 [D loss: 0.430283, acc.: 81.25%] [G loss: 1.395580]\n",
      "epoch:27 step:26235 [D loss: 0.527977, acc.: 76.56%] [G loss: 1.672072]\n",
      "epoch:27 step:26236 [D loss: 0.464683, acc.: 80.47%] [G loss: 1.412123]\n",
      "epoch:28 step:26237 [D loss: 0.798515, acc.: 51.56%] [G loss: 1.289305]\n",
      "epoch:28 step:26238 [D loss: 0.539331, acc.: 78.12%] [G loss: 1.466551]\n",
      "epoch:28 step:26239 [D loss: 0.469390, acc.: 80.47%] [G loss: 1.274460]\n",
      "epoch:28 step:26240 [D loss: 0.728662, acc.: 51.56%] [G loss: 1.109790]\n",
      "epoch:28 step:26241 [D loss: 0.738379, acc.: 57.81%] [G loss: 1.498944]\n",
      "epoch:28 step:26242 [D loss: 0.590140, acc.: 66.41%] [G loss: 1.672725]\n",
      "epoch:28 step:26243 [D loss: 0.667580, acc.: 60.16%] [G loss: 1.833199]\n",
      "epoch:28 step:26244 [D loss: 0.371186, acc.: 85.16%] [G loss: 1.841141]\n",
      "epoch:28 step:26245 [D loss: 0.641688, acc.: 62.50%] [G loss: 1.100063]\n",
      "epoch:28 step:26246 [D loss: 0.534968, acc.: 73.44%] [G loss: 1.862279]\n",
      "epoch:28 step:26247 [D loss: 0.377495, acc.: 84.38%] [G loss: 1.399853]\n",
      "epoch:28 step:26248 [D loss: 0.486207, acc.: 76.56%] [G loss: 1.104143]\n",
      "epoch:28 step:26249 [D loss: 0.720516, acc.: 56.25%] [G loss: 1.264297]\n",
      "epoch:28 step:26250 [D loss: 0.515942, acc.: 71.88%] [G loss: 1.279824]\n",
      "epoch:28 step:26251 [D loss: 0.379826, acc.: 85.94%] [G loss: 1.858752]\n",
      "epoch:28 step:26252 [D loss: 0.635798, acc.: 65.62%] [G loss: 1.573954]\n",
      "epoch:28 step:26253 [D loss: 0.584960, acc.: 69.53%] [G loss: 1.494674]\n",
      "epoch:28 step:26254 [D loss: 0.336026, acc.: 92.19%] [G loss: 1.292197]\n",
      "epoch:28 step:26255 [D loss: 0.477004, acc.: 80.47%] [G loss: 1.194482]\n",
      "epoch:28 step:26256 [D loss: 0.509796, acc.: 75.78%] [G loss: 1.242210]\n",
      "epoch:28 step:26257 [D loss: 0.654975, acc.: 61.72%] [G loss: 1.230428]\n",
      "epoch:28 step:26258 [D loss: 0.445153, acc.: 79.69%] [G loss: 1.367480]\n",
      "epoch:28 step:26259 [D loss: 0.532043, acc.: 68.75%] [G loss: 1.363844]\n",
      "epoch:28 step:26260 [D loss: 0.438525, acc.: 78.91%] [G loss: 1.205926]\n",
      "epoch:28 step:26261 [D loss: 0.488769, acc.: 75.78%] [G loss: 1.228896]\n",
      "epoch:28 step:26262 [D loss: 0.499494, acc.: 76.56%] [G loss: 1.522951]\n",
      "epoch:28 step:26263 [D loss: 0.554453, acc.: 75.00%] [G loss: 0.878117]\n",
      "epoch:28 step:26264 [D loss: 0.932582, acc.: 50.78%] [G loss: 1.117488]\n",
      "epoch:28 step:26265 [D loss: 0.482318, acc.: 75.78%] [G loss: 1.665806]\n",
      "epoch:28 step:26266 [D loss: 0.499825, acc.: 79.69%] [G loss: 1.432380]\n",
      "epoch:28 step:26267 [D loss: 0.527472, acc.: 74.22%] [G loss: 1.178994]\n",
      "epoch:28 step:26268 [D loss: 0.518352, acc.: 74.22%] [G loss: 1.466194]\n",
      "epoch:28 step:26269 [D loss: 0.627093, acc.: 64.84%] [G loss: 1.807789]\n",
      "epoch:28 step:26270 [D loss: 0.441629, acc.: 81.25%] [G loss: 1.434128]\n",
      "epoch:28 step:26271 [D loss: 0.585302, acc.: 73.44%] [G loss: 1.402514]\n",
      "epoch:28 step:26272 [D loss: 0.379820, acc.: 88.28%] [G loss: 1.528623]\n",
      "epoch:28 step:26273 [D loss: 0.670949, acc.: 64.06%] [G loss: 1.431478]\n",
      "epoch:28 step:26274 [D loss: 0.518442, acc.: 75.78%] [G loss: 1.406354]\n",
      "epoch:28 step:26275 [D loss: 0.725845, acc.: 62.50%] [G loss: 1.140391]\n",
      "epoch:28 step:26276 [D loss: 0.704688, acc.: 55.47%] [G loss: 1.747162]\n",
      "epoch:28 step:26277 [D loss: 0.669554, acc.: 64.84%] [G loss: 1.468706]\n",
      "epoch:28 step:26278 [D loss: 0.552078, acc.: 70.31%] [G loss: 1.536865]\n",
      "epoch:28 step:26279 [D loss: 0.512320, acc.: 75.78%] [G loss: 1.399729]\n",
      "epoch:28 step:26280 [D loss: 0.511498, acc.: 75.78%] [G loss: 1.653654]\n",
      "epoch:28 step:26281 [D loss: 0.508813, acc.: 78.12%] [G loss: 1.158447]\n",
      "epoch:28 step:26282 [D loss: 0.638497, acc.: 64.06%] [G loss: 1.016744]\n",
      "epoch:28 step:26283 [D loss: 0.656175, acc.: 63.28%] [G loss: 1.625139]\n",
      "epoch:28 step:26284 [D loss: 0.488245, acc.: 76.56%] [G loss: 1.731724]\n",
      "epoch:28 step:26285 [D loss: 0.447894, acc.: 82.81%] [G loss: 1.658269]\n",
      "epoch:28 step:26286 [D loss: 0.460323, acc.: 79.69%] [G loss: 1.867868]\n",
      "epoch:28 step:26287 [D loss: 0.559038, acc.: 70.31%] [G loss: 1.793516]\n",
      "epoch:28 step:26288 [D loss: 0.402335, acc.: 84.38%] [G loss: 1.796337]\n",
      "epoch:28 step:26289 [D loss: 0.599770, acc.: 62.50%] [G loss: 1.359417]\n",
      "epoch:28 step:26290 [D loss: 0.374306, acc.: 85.94%] [G loss: 1.443613]\n",
      "epoch:28 step:26291 [D loss: 0.569051, acc.: 69.53%] [G loss: 1.064348]\n",
      "epoch:28 step:26292 [D loss: 0.441508, acc.: 82.81%] [G loss: 1.281538]\n",
      "epoch:28 step:26293 [D loss: 0.493289, acc.: 78.12%] [G loss: 1.619836]\n",
      "epoch:28 step:26294 [D loss: 0.565016, acc.: 73.44%] [G loss: 1.748789]\n",
      "epoch:28 step:26295 [D loss: 0.423883, acc.: 87.50%] [G loss: 1.604395]\n",
      "epoch:28 step:26296 [D loss: 0.592676, acc.: 71.09%] [G loss: 0.917744]\n",
      "epoch:28 step:26297 [D loss: 0.837390, acc.: 46.88%] [G loss: 1.250135]\n",
      "epoch:28 step:26298 [D loss: 0.610313, acc.: 66.41%] [G loss: 1.402763]\n",
      "epoch:28 step:26299 [D loss: 0.511561, acc.: 75.78%] [G loss: 1.036138]\n",
      "epoch:28 step:26300 [D loss: 0.429910, acc.: 80.47%] [G loss: 1.461081]\n",
      "epoch:28 step:26301 [D loss: 0.390498, acc.: 83.59%] [G loss: 1.332503]\n",
      "epoch:28 step:26302 [D loss: 0.711277, acc.: 59.38%] [G loss: 1.273079]\n",
      "epoch:28 step:26303 [D loss: 0.504137, acc.: 79.69%] [G loss: 1.391371]\n",
      "epoch:28 step:26304 [D loss: 0.443913, acc.: 82.81%] [G loss: 1.367328]\n",
      "epoch:28 step:26305 [D loss: 0.537609, acc.: 71.88%] [G loss: 1.369356]\n",
      "epoch:28 step:26306 [D loss: 0.787864, acc.: 53.91%] [G loss: 1.537699]\n",
      "epoch:28 step:26307 [D loss: 0.402920, acc.: 85.94%] [G loss: 1.537359]\n",
      "epoch:28 step:26308 [D loss: 0.634741, acc.: 66.41%] [G loss: 1.321101]\n",
      "epoch:28 step:26309 [D loss: 0.589300, acc.: 67.19%] [G loss: 1.230019]\n",
      "epoch:28 step:26310 [D loss: 0.513971, acc.: 78.12%] [G loss: 2.015637]\n",
      "epoch:28 step:26311 [D loss: 0.557807, acc.: 71.09%] [G loss: 1.466507]\n",
      "epoch:28 step:26312 [D loss: 0.437781, acc.: 84.38%] [G loss: 1.500007]\n",
      "epoch:28 step:26313 [D loss: 0.629079, acc.: 69.53%] [G loss: 1.373339]\n",
      "epoch:28 step:26314 [D loss: 0.677702, acc.: 62.50%] [G loss: 1.177287]\n",
      "epoch:28 step:26315 [D loss: 0.527668, acc.: 70.31%] [G loss: 1.765644]\n",
      "epoch:28 step:26316 [D loss: 0.501586, acc.: 73.44%] [G loss: 1.260587]\n",
      "epoch:28 step:26317 [D loss: 0.514398, acc.: 71.88%] [G loss: 1.538002]\n",
      "epoch:28 step:26318 [D loss: 0.630132, acc.: 58.59%] [G loss: 1.361144]\n",
      "epoch:28 step:26319 [D loss: 0.589488, acc.: 64.06%] [G loss: 1.280912]\n",
      "epoch:28 step:26320 [D loss: 0.440768, acc.: 81.25%] [G loss: 1.648435]\n",
      "epoch:28 step:26321 [D loss: 0.613414, acc.: 68.75%] [G loss: 1.326401]\n",
      "epoch:28 step:26322 [D loss: 0.544353, acc.: 74.22%] [G loss: 1.584406]\n",
      "epoch:28 step:26323 [D loss: 0.759876, acc.: 53.91%] [G loss: 1.221334]\n",
      "epoch:28 step:26324 [D loss: 0.574349, acc.: 67.19%] [G loss: 1.196091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26325 [D loss: 0.542989, acc.: 72.66%] [G loss: 1.508459]\n",
      "epoch:28 step:26326 [D loss: 0.582096, acc.: 71.09%] [G loss: 1.695928]\n",
      "epoch:28 step:26327 [D loss: 0.542140, acc.: 71.88%] [G loss: 1.380965]\n",
      "epoch:28 step:26328 [D loss: 0.379789, acc.: 82.03%] [G loss: 1.462233]\n",
      "epoch:28 step:26329 [D loss: 0.705343, acc.: 58.59%] [G loss: 1.196049]\n",
      "epoch:28 step:26330 [D loss: 0.426004, acc.: 80.47%] [G loss: 1.336226]\n",
      "epoch:28 step:26331 [D loss: 0.558661, acc.: 73.44%] [G loss: 1.420909]\n",
      "epoch:28 step:26332 [D loss: 0.468412, acc.: 78.12%] [G loss: 1.937430]\n",
      "epoch:28 step:26333 [D loss: 0.631733, acc.: 64.84%] [G loss: 1.398266]\n",
      "epoch:28 step:26334 [D loss: 0.718351, acc.: 59.38%] [G loss: 1.215660]\n",
      "epoch:28 step:26335 [D loss: 0.575774, acc.: 70.31%] [G loss: 1.438086]\n",
      "epoch:28 step:26336 [D loss: 0.511165, acc.: 75.00%] [G loss: 1.434633]\n",
      "epoch:28 step:26337 [D loss: 0.360430, acc.: 90.62%] [G loss: 1.236644]\n",
      "epoch:28 step:26338 [D loss: 0.549041, acc.: 67.97%] [G loss: 1.642245]\n",
      "epoch:28 step:26339 [D loss: 0.368583, acc.: 87.50%] [G loss: 1.204048]\n",
      "epoch:28 step:26340 [D loss: 0.451028, acc.: 78.91%] [G loss: 1.664921]\n",
      "epoch:28 step:26341 [D loss: 0.472802, acc.: 78.91%] [G loss: 1.227528]\n",
      "epoch:28 step:26342 [D loss: 0.563069, acc.: 67.19%] [G loss: 1.493863]\n",
      "epoch:28 step:26343 [D loss: 0.486209, acc.: 74.22%] [G loss: 1.677127]\n",
      "epoch:28 step:26344 [D loss: 0.598262, acc.: 71.88%] [G loss: 0.966610]\n",
      "epoch:28 step:26345 [D loss: 0.508814, acc.: 75.78%] [G loss: 1.327994]\n",
      "epoch:28 step:26346 [D loss: 0.626212, acc.: 67.97%] [G loss: 1.289535]\n",
      "epoch:28 step:26347 [D loss: 0.496640, acc.: 78.91%] [G loss: 1.404174]\n",
      "epoch:28 step:26348 [D loss: 0.658211, acc.: 62.50%] [G loss: 1.493688]\n",
      "epoch:28 step:26349 [D loss: 0.440621, acc.: 78.91%] [G loss: 1.570290]\n",
      "epoch:28 step:26350 [D loss: 0.479420, acc.: 75.78%] [G loss: 1.493168]\n",
      "epoch:28 step:26351 [D loss: 0.673596, acc.: 60.94%] [G loss: 1.207816]\n",
      "epoch:28 step:26352 [D loss: 0.601140, acc.: 66.41%] [G loss: 1.620625]\n",
      "epoch:28 step:26353 [D loss: 0.515918, acc.: 75.00%] [G loss: 1.312222]\n",
      "epoch:28 step:26354 [D loss: 0.630682, acc.: 66.41%] [G loss: 1.362591]\n",
      "epoch:28 step:26355 [D loss: 0.484383, acc.: 76.56%] [G loss: 1.345078]\n",
      "epoch:28 step:26356 [D loss: 0.695704, acc.: 59.38%] [G loss: 1.149867]\n",
      "epoch:28 step:26357 [D loss: 0.445756, acc.: 85.16%] [G loss: 1.174489]\n",
      "epoch:28 step:26358 [D loss: 0.673329, acc.: 58.59%] [G loss: 1.237377]\n",
      "epoch:28 step:26359 [D loss: 0.366181, acc.: 85.16%] [G loss: 1.624863]\n",
      "epoch:28 step:26360 [D loss: 0.573290, acc.: 69.53%] [G loss: 1.270658]\n",
      "epoch:28 step:26361 [D loss: 0.666728, acc.: 65.62%] [G loss: 1.506067]\n",
      "epoch:28 step:26362 [D loss: 0.495784, acc.: 78.91%] [G loss: 1.186252]\n",
      "epoch:28 step:26363 [D loss: 0.410318, acc.: 85.16%] [G loss: 1.884979]\n",
      "epoch:28 step:26364 [D loss: 0.664078, acc.: 62.50%] [G loss: 1.626254]\n",
      "epoch:28 step:26365 [D loss: 0.357592, acc.: 88.28%] [G loss: 1.308129]\n",
      "epoch:28 step:26366 [D loss: 0.519373, acc.: 74.22%] [G loss: 1.727698]\n",
      "epoch:28 step:26367 [D loss: 0.352814, acc.: 86.72%] [G loss: 1.553077]\n",
      "epoch:28 step:26368 [D loss: 0.526010, acc.: 74.22%] [G loss: 1.505867]\n",
      "epoch:28 step:26369 [D loss: 0.432084, acc.: 78.91%] [G loss: 1.048406]\n",
      "epoch:28 step:26370 [D loss: 0.615506, acc.: 64.84%] [G loss: 1.530365]\n",
      "epoch:28 step:26371 [D loss: 0.508291, acc.: 76.56%] [G loss: 1.556943]\n",
      "epoch:28 step:26372 [D loss: 0.721629, acc.: 53.91%] [G loss: 1.448574]\n",
      "epoch:28 step:26373 [D loss: 0.434904, acc.: 85.16%] [G loss: 1.425528]\n",
      "epoch:28 step:26374 [D loss: 0.720144, acc.: 53.12%] [G loss: 1.140160]\n",
      "epoch:28 step:26375 [D loss: 0.417675, acc.: 82.81%] [G loss: 1.394430]\n",
      "epoch:28 step:26376 [D loss: 0.595569, acc.: 65.62%] [G loss: 0.947767]\n",
      "epoch:28 step:26377 [D loss: 0.624191, acc.: 64.06%] [G loss: 1.438782]\n",
      "epoch:28 step:26378 [D loss: 0.558652, acc.: 70.31%] [G loss: 1.456457]\n",
      "epoch:28 step:26379 [D loss: 0.458519, acc.: 80.47%] [G loss: 1.445161]\n",
      "epoch:28 step:26380 [D loss: 0.527699, acc.: 75.78%] [G loss: 1.524270]\n",
      "epoch:28 step:26381 [D loss: 0.528130, acc.: 75.78%] [G loss: 1.373441]\n",
      "epoch:28 step:26382 [D loss: 0.595617, acc.: 65.62%] [G loss: 1.366378]\n",
      "epoch:28 step:26383 [D loss: 0.608574, acc.: 61.72%] [G loss: 1.356822]\n",
      "epoch:28 step:26384 [D loss: 0.630912, acc.: 65.62%] [G loss: 1.844996]\n",
      "epoch:28 step:26385 [D loss: 0.601473, acc.: 64.06%] [G loss: 1.146582]\n",
      "epoch:28 step:26386 [D loss: 0.519979, acc.: 71.09%] [G loss: 1.582804]\n",
      "epoch:28 step:26387 [D loss: 0.556062, acc.: 73.44%] [G loss: 1.491148]\n",
      "epoch:28 step:26388 [D loss: 0.563306, acc.: 70.31%] [G loss: 1.224221]\n",
      "epoch:28 step:26389 [D loss: 0.552140, acc.: 79.69%] [G loss: 1.721057]\n",
      "epoch:28 step:26390 [D loss: 0.695470, acc.: 57.03%] [G loss: 1.271269]\n",
      "epoch:28 step:26391 [D loss: 0.501988, acc.: 75.00%] [G loss: 1.548645]\n",
      "epoch:28 step:26392 [D loss: 0.467101, acc.: 80.47%] [G loss: 1.084801]\n",
      "epoch:28 step:26393 [D loss: 0.569181, acc.: 74.22%] [G loss: 1.463030]\n",
      "epoch:28 step:26394 [D loss: 0.654431, acc.: 66.41%] [G loss: 1.090024]\n",
      "epoch:28 step:26395 [D loss: 0.422445, acc.: 80.47%] [G loss: 1.116327]\n",
      "epoch:28 step:26396 [D loss: 0.283380, acc.: 91.41%] [G loss: 1.697877]\n",
      "epoch:28 step:26397 [D loss: 0.444317, acc.: 81.25%] [G loss: 2.040387]\n",
      "epoch:28 step:26398 [D loss: 0.627606, acc.: 67.19%] [G loss: 1.621379]\n",
      "epoch:28 step:26399 [D loss: 0.551350, acc.: 71.09%] [G loss: 1.681797]\n",
      "epoch:28 step:26400 [D loss: 0.634324, acc.: 64.84%] [G loss: 1.274928]\n",
      "##############\n",
      "[2.58098336 2.10391625 1.72616297 2.93048283 0.78562476 6.00358964\n",
      " 2.39052192 2.12372849 3.87417868 8.14868929]\n",
      "##########\n",
      "epoch:28 step:26401 [D loss: 0.546586, acc.: 70.31%] [G loss: 1.577500]\n",
      "epoch:28 step:26402 [D loss: 0.517038, acc.: 71.88%] [G loss: 1.525367]\n",
      "epoch:28 step:26403 [D loss: 0.382303, acc.: 88.28%] [G loss: 1.400761]\n",
      "epoch:28 step:26404 [D loss: 0.794156, acc.: 50.00%] [G loss: 1.599852]\n",
      "epoch:28 step:26405 [D loss: 0.483137, acc.: 78.12%] [G loss: 1.457793]\n",
      "epoch:28 step:26406 [D loss: 0.552965, acc.: 75.00%] [G loss: 1.619167]\n",
      "epoch:28 step:26407 [D loss: 0.521006, acc.: 75.00%] [G loss: 1.090773]\n",
      "epoch:28 step:26408 [D loss: 0.455714, acc.: 79.69%] [G loss: 2.022784]\n",
      "epoch:28 step:26409 [D loss: 0.603781, acc.: 70.31%] [G loss: 1.458100]\n",
      "epoch:28 step:26410 [D loss: 0.461984, acc.: 78.91%] [G loss: 1.665503]\n",
      "epoch:28 step:26411 [D loss: 0.785757, acc.: 50.00%] [G loss: 1.131682]\n",
      "epoch:28 step:26412 [D loss: 0.662306, acc.: 58.59%] [G loss: 0.822677]\n",
      "epoch:28 step:26413 [D loss: 0.507883, acc.: 78.91%] [G loss: 1.434068]\n",
      "epoch:28 step:26414 [D loss: 0.470463, acc.: 80.47%] [G loss: 1.282927]\n",
      "epoch:28 step:26415 [D loss: 0.445841, acc.: 79.69%] [G loss: 1.231550]\n",
      "epoch:28 step:26416 [D loss: 0.637414, acc.: 68.75%] [G loss: 1.397053]\n",
      "epoch:28 step:26417 [D loss: 0.462389, acc.: 78.91%] [G loss: 1.595652]\n",
      "epoch:28 step:26418 [D loss: 0.547483, acc.: 69.53%] [G loss: 1.409162]\n",
      "epoch:28 step:26419 [D loss: 0.577908, acc.: 77.34%] [G loss: 1.333179]\n",
      "epoch:28 step:26420 [D loss: 0.619837, acc.: 67.19%] [G loss: 1.527209]\n",
      "epoch:28 step:26421 [D loss: 0.450478, acc.: 83.59%] [G loss: 1.374450]\n",
      "epoch:28 step:26422 [D loss: 0.512365, acc.: 78.12%] [G loss: 1.713107]\n",
      "epoch:28 step:26423 [D loss: 0.586659, acc.: 69.53%] [G loss: 1.443599]\n",
      "epoch:28 step:26424 [D loss: 0.540096, acc.: 70.31%] [G loss: 1.609828]\n",
      "epoch:28 step:26425 [D loss: 0.505713, acc.: 76.56%] [G loss: 1.665119]\n",
      "epoch:28 step:26426 [D loss: 0.609445, acc.: 64.84%] [G loss: 1.813363]\n",
      "epoch:28 step:26427 [D loss: 0.745285, acc.: 60.16%] [G loss: 1.371014]\n",
      "epoch:28 step:26428 [D loss: 0.493760, acc.: 76.56%] [G loss: 1.264830]\n",
      "epoch:28 step:26429 [D loss: 0.839591, acc.: 51.56%] [G loss: 1.631567]\n",
      "epoch:28 step:26430 [D loss: 0.541530, acc.: 73.44%] [G loss: 1.384655]\n",
      "epoch:28 step:26431 [D loss: 0.707636, acc.: 54.69%] [G loss: 1.255333]\n",
      "epoch:28 step:26432 [D loss: 0.467458, acc.: 80.47%] [G loss: 1.176860]\n",
      "epoch:28 step:26433 [D loss: 0.682608, acc.: 58.59%] [G loss: 1.422333]\n",
      "epoch:28 step:26434 [D loss: 0.483310, acc.: 77.34%] [G loss: 1.402146]\n",
      "epoch:28 step:26435 [D loss: 0.442915, acc.: 85.94%] [G loss: 1.844703]\n",
      "epoch:28 step:26436 [D loss: 0.523754, acc.: 76.56%] [G loss: 1.609608]\n",
      "epoch:28 step:26437 [D loss: 0.422972, acc.: 83.59%] [G loss: 1.583708]\n",
      "epoch:28 step:26438 [D loss: 0.513421, acc.: 78.91%] [G loss: 1.987804]\n",
      "epoch:28 step:26439 [D loss: 0.563196, acc.: 71.88%] [G loss: 1.436078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26440 [D loss: 0.508647, acc.: 71.88%] [G loss: 1.200659]\n",
      "epoch:28 step:26441 [D loss: 0.586974, acc.: 67.97%] [G loss: 1.323190]\n",
      "epoch:28 step:26442 [D loss: 0.560774, acc.: 70.31%] [G loss: 1.450151]\n",
      "epoch:28 step:26443 [D loss: 0.567178, acc.: 74.22%] [G loss: 1.609499]\n",
      "epoch:28 step:26444 [D loss: 0.595144, acc.: 64.84%] [G loss: 0.974292]\n",
      "epoch:28 step:26445 [D loss: 0.576039, acc.: 64.84%] [G loss: 1.462787]\n",
      "epoch:28 step:26446 [D loss: 0.534099, acc.: 75.00%] [G loss: 1.779780]\n",
      "epoch:28 step:26447 [D loss: 0.409432, acc.: 85.94%] [G loss: 1.668028]\n",
      "epoch:28 step:26448 [D loss: 0.618523, acc.: 64.84%] [G loss: 1.084879]\n",
      "epoch:28 step:26449 [D loss: 0.642712, acc.: 55.47%] [G loss: 1.073096]\n",
      "epoch:28 step:26450 [D loss: 0.645906, acc.: 67.19%] [G loss: 1.164627]\n",
      "epoch:28 step:26451 [D loss: 0.718657, acc.: 60.94%] [G loss: 1.199105]\n",
      "epoch:28 step:26452 [D loss: 0.564451, acc.: 73.44%] [G loss: 1.232842]\n",
      "epoch:28 step:26453 [D loss: 0.463862, acc.: 81.25%] [G loss: 1.343291]\n",
      "epoch:28 step:26454 [D loss: 0.511241, acc.: 74.22%] [G loss: 1.383407]\n",
      "epoch:28 step:26455 [D loss: 0.579620, acc.: 71.09%] [G loss: 1.635780]\n",
      "epoch:28 step:26456 [D loss: 0.606155, acc.: 66.41%] [G loss: 1.198460]\n",
      "epoch:28 step:26457 [D loss: 0.595541, acc.: 69.53%] [G loss: 1.451051]\n",
      "epoch:28 step:26458 [D loss: 0.513819, acc.: 73.44%] [G loss: 1.477674]\n",
      "epoch:28 step:26459 [D loss: 0.560288, acc.: 71.09%] [G loss: 1.290926]\n",
      "epoch:28 step:26460 [D loss: 0.636420, acc.: 66.41%] [G loss: 1.546634]\n",
      "epoch:28 step:26461 [D loss: 0.613839, acc.: 64.06%] [G loss: 1.245266]\n",
      "epoch:28 step:26462 [D loss: 0.586130, acc.: 68.75%] [G loss: 1.069905]\n",
      "epoch:28 step:26463 [D loss: 0.447603, acc.: 79.69%] [G loss: 1.330440]\n",
      "epoch:28 step:26464 [D loss: 0.743912, acc.: 57.03%] [G loss: 1.266520]\n",
      "epoch:28 step:26465 [D loss: 0.547992, acc.: 70.31%] [G loss: 1.226983]\n",
      "epoch:28 step:26466 [D loss: 0.726187, acc.: 61.72%] [G loss: 1.165165]\n",
      "epoch:28 step:26467 [D loss: 0.483446, acc.: 75.00%] [G loss: 1.780775]\n",
      "epoch:28 step:26468 [D loss: 0.475747, acc.: 82.81%] [G loss: 1.416200]\n",
      "epoch:28 step:26469 [D loss: 0.514660, acc.: 76.56%] [G loss: 1.788982]\n",
      "epoch:28 step:26470 [D loss: 0.597980, acc.: 67.97%] [G loss: 1.640936]\n",
      "epoch:28 step:26471 [D loss: 0.665552, acc.: 63.28%] [G loss: 0.913512]\n",
      "epoch:28 step:26472 [D loss: 0.514709, acc.: 78.91%] [G loss: 1.459580]\n",
      "epoch:28 step:26473 [D loss: 0.502277, acc.: 71.09%] [G loss: 1.275678]\n",
      "epoch:28 step:26474 [D loss: 0.389471, acc.: 87.50%] [G loss: 2.055189]\n",
      "epoch:28 step:26475 [D loss: 0.521325, acc.: 67.97%] [G loss: 1.475837]\n",
      "epoch:28 step:26476 [D loss: 0.676576, acc.: 64.06%] [G loss: 1.027364]\n",
      "epoch:28 step:26477 [D loss: 0.458696, acc.: 78.91%] [G loss: 1.525021]\n",
      "epoch:28 step:26478 [D loss: 0.562372, acc.: 70.31%] [G loss: 1.395357]\n",
      "epoch:28 step:26479 [D loss: 0.740690, acc.: 57.81%] [G loss: 1.471830]\n",
      "epoch:28 step:26480 [D loss: 0.607611, acc.: 66.41%] [G loss: 1.198592]\n",
      "epoch:28 step:26481 [D loss: 0.496089, acc.: 78.91%] [G loss: 1.597556]\n",
      "epoch:28 step:26482 [D loss: 0.508272, acc.: 72.66%] [G loss: 1.399304]\n",
      "epoch:28 step:26483 [D loss: 0.833964, acc.: 46.88%] [G loss: 1.285027]\n",
      "epoch:28 step:26484 [D loss: 0.697612, acc.: 60.16%] [G loss: 1.412705]\n",
      "epoch:28 step:26485 [D loss: 0.325782, acc.: 90.62%] [G loss: 1.501815]\n",
      "epoch:28 step:26486 [D loss: 0.598611, acc.: 67.19%] [G loss: 1.575714]\n",
      "epoch:28 step:26487 [D loss: 0.478811, acc.: 80.47%] [G loss: 1.902042]\n",
      "epoch:28 step:26488 [D loss: 0.359340, acc.: 85.94%] [G loss: 1.536514]\n",
      "epoch:28 step:26489 [D loss: 0.387589, acc.: 82.81%] [G loss: 1.642177]\n",
      "epoch:28 step:26490 [D loss: 0.547904, acc.: 72.66%] [G loss: 1.566094]\n",
      "epoch:28 step:26491 [D loss: 0.487698, acc.: 75.00%] [G loss: 1.299115]\n",
      "epoch:28 step:26492 [D loss: 0.503230, acc.: 72.66%] [G loss: 1.734304]\n",
      "epoch:28 step:26493 [D loss: 0.443831, acc.: 80.47%] [G loss: 1.219141]\n",
      "epoch:28 step:26494 [D loss: 0.497069, acc.: 75.78%] [G loss: 1.157201]\n",
      "epoch:28 step:26495 [D loss: 0.468495, acc.: 79.69%] [G loss: 1.657593]\n",
      "epoch:28 step:26496 [D loss: 0.601786, acc.: 69.53%] [G loss: 1.427908]\n",
      "epoch:28 step:26497 [D loss: 0.453578, acc.: 78.91%] [G loss: 1.766393]\n",
      "epoch:28 step:26498 [D loss: 0.703900, acc.: 58.59%] [G loss: 1.362458]\n",
      "epoch:28 step:26499 [D loss: 0.617736, acc.: 65.62%] [G loss: 1.616653]\n",
      "epoch:28 step:26500 [D loss: 0.502954, acc.: 74.22%] [G loss: 1.349488]\n",
      "epoch:28 step:26501 [D loss: 0.366994, acc.: 84.38%] [G loss: 1.425058]\n",
      "epoch:28 step:26502 [D loss: 0.548744, acc.: 73.44%] [G loss: 1.797016]\n",
      "epoch:28 step:26503 [D loss: 0.552278, acc.: 71.09%] [G loss: 1.415992]\n",
      "epoch:28 step:26504 [D loss: 0.706545, acc.: 60.94%] [G loss: 1.191546]\n",
      "epoch:28 step:26505 [D loss: 0.515989, acc.: 74.22%] [G loss: 1.391599]\n",
      "epoch:28 step:26506 [D loss: 0.526479, acc.: 72.66%] [G loss: 1.614422]\n",
      "epoch:28 step:26507 [D loss: 0.331616, acc.: 92.97%] [G loss: 1.917358]\n",
      "epoch:28 step:26508 [D loss: 0.438844, acc.: 82.03%] [G loss: 1.386545]\n",
      "epoch:28 step:26509 [D loss: 0.668909, acc.: 64.84%] [G loss: 1.281620]\n",
      "epoch:28 step:26510 [D loss: 0.481554, acc.: 78.12%] [G loss: 1.094572]\n",
      "epoch:28 step:26511 [D loss: 0.760478, acc.: 52.34%] [G loss: 1.354163]\n",
      "epoch:28 step:26512 [D loss: 0.724774, acc.: 57.03%] [G loss: 1.043788]\n",
      "epoch:28 step:26513 [D loss: 0.571744, acc.: 75.78%] [G loss: 1.543105]\n",
      "epoch:28 step:26514 [D loss: 0.567919, acc.: 67.97%] [G loss: 1.257849]\n",
      "epoch:28 step:26515 [D loss: 0.533295, acc.: 76.56%] [G loss: 2.143322]\n",
      "epoch:28 step:26516 [D loss: 0.603889, acc.: 66.41%] [G loss: 1.407841]\n",
      "epoch:28 step:26517 [D loss: 0.506284, acc.: 76.56%] [G loss: 1.184511]\n",
      "epoch:28 step:26518 [D loss: 0.554851, acc.: 72.66%] [G loss: 1.412360]\n",
      "epoch:28 step:26519 [D loss: 0.562427, acc.: 72.66%] [G loss: 1.256260]\n",
      "epoch:28 step:26520 [D loss: 0.520295, acc.: 73.44%] [G loss: 1.370437]\n",
      "epoch:28 step:26521 [D loss: 0.507408, acc.: 71.09%] [G loss: 1.715799]\n",
      "epoch:28 step:26522 [D loss: 0.574718, acc.: 68.75%] [G loss: 1.757563]\n",
      "epoch:28 step:26523 [D loss: 0.551276, acc.: 71.09%] [G loss: 1.586998]\n",
      "epoch:28 step:26524 [D loss: 0.683292, acc.: 58.59%] [G loss: 1.527847]\n",
      "epoch:28 step:26525 [D loss: 0.598875, acc.: 65.62%] [G loss: 1.386837]\n",
      "epoch:28 step:26526 [D loss: 0.462258, acc.: 82.81%] [G loss: 1.489851]\n",
      "epoch:28 step:26527 [D loss: 0.363703, acc.: 88.28%] [G loss: 1.600986]\n",
      "epoch:28 step:26528 [D loss: 0.515364, acc.: 77.34%] [G loss: 1.477278]\n",
      "epoch:28 step:26529 [D loss: 0.493894, acc.: 77.34%] [G loss: 1.434564]\n",
      "epoch:28 step:26530 [D loss: 0.518613, acc.: 75.78%] [G loss: 1.370921]\n",
      "epoch:28 step:26531 [D loss: 0.394618, acc.: 85.94%] [G loss: 1.331517]\n",
      "epoch:28 step:26532 [D loss: 0.534638, acc.: 75.78%] [G loss: 1.124277]\n",
      "epoch:28 step:26533 [D loss: 0.343656, acc.: 89.84%] [G loss: 1.484046]\n",
      "epoch:28 step:26534 [D loss: 0.689570, acc.: 56.25%] [G loss: 1.549664]\n",
      "epoch:28 step:26535 [D loss: 0.621800, acc.: 67.19%] [G loss: 1.298765]\n",
      "epoch:28 step:26536 [D loss: 0.454767, acc.: 77.34%] [G loss: 1.269103]\n",
      "epoch:28 step:26537 [D loss: 0.671779, acc.: 57.03%] [G loss: 1.013138]\n",
      "epoch:28 step:26538 [D loss: 0.534431, acc.: 75.00%] [G loss: 1.813563]\n",
      "epoch:28 step:26539 [D loss: 0.724344, acc.: 57.81%] [G loss: 1.341812]\n",
      "epoch:28 step:26540 [D loss: 0.551982, acc.: 72.66%] [G loss: 1.321964]\n",
      "epoch:28 step:26541 [D loss: 0.484990, acc.: 78.12%] [G loss: 1.821061]\n",
      "epoch:28 step:26542 [D loss: 0.549241, acc.: 72.66%] [G loss: 1.627443]\n",
      "epoch:28 step:26543 [D loss: 0.589991, acc.: 66.41%] [G loss: 1.140910]\n",
      "epoch:28 step:26544 [D loss: 0.571242, acc.: 69.53%] [G loss: 1.233357]\n",
      "epoch:28 step:26545 [D loss: 0.474607, acc.: 78.12%] [G loss: 1.664937]\n",
      "epoch:28 step:26546 [D loss: 0.481751, acc.: 77.34%] [G loss: 1.270066]\n",
      "epoch:28 step:26547 [D loss: 0.494981, acc.: 72.66%] [G loss: 1.783619]\n",
      "epoch:28 step:26548 [D loss: 0.600883, acc.: 67.19%] [G loss: 1.179354]\n",
      "epoch:28 step:26549 [D loss: 0.560519, acc.: 71.09%] [G loss: 1.466217]\n",
      "epoch:28 step:26550 [D loss: 0.384983, acc.: 87.50%] [G loss: 1.387352]\n",
      "epoch:28 step:26551 [D loss: 0.408642, acc.: 85.16%] [G loss: 1.390094]\n",
      "epoch:28 step:26552 [D loss: 0.508181, acc.: 70.31%] [G loss: 1.545664]\n",
      "epoch:28 step:26553 [D loss: 0.626920, acc.: 67.97%] [G loss: 1.133759]\n",
      "epoch:28 step:26554 [D loss: 0.627930, acc.: 67.97%] [G loss: 1.424156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26555 [D loss: 0.598752, acc.: 68.75%] [G loss: 0.975475]\n",
      "epoch:28 step:26556 [D loss: 0.578183, acc.: 69.53%] [G loss: 0.986292]\n",
      "epoch:28 step:26557 [D loss: 0.684744, acc.: 60.94%] [G loss: 1.487064]\n",
      "epoch:28 step:26558 [D loss: 0.598247, acc.: 68.75%] [G loss: 1.657033]\n",
      "epoch:28 step:26559 [D loss: 0.590898, acc.: 69.53%] [G loss: 1.068149]\n",
      "epoch:28 step:26560 [D loss: 0.568663, acc.: 72.66%] [G loss: 1.628963]\n",
      "epoch:28 step:26561 [D loss: 0.689931, acc.: 57.81%] [G loss: 1.286749]\n",
      "epoch:28 step:26562 [D loss: 0.593700, acc.: 64.84%] [G loss: 1.450692]\n",
      "epoch:28 step:26563 [D loss: 0.354772, acc.: 89.84%] [G loss: 1.672677]\n",
      "epoch:28 step:26564 [D loss: 0.479881, acc.: 79.69%] [G loss: 1.929084]\n",
      "epoch:28 step:26565 [D loss: 0.583288, acc.: 66.41%] [G loss: 1.531186]\n",
      "epoch:28 step:26566 [D loss: 0.413620, acc.: 80.47%] [G loss: 1.605163]\n",
      "epoch:28 step:26567 [D loss: 0.359859, acc.: 85.94%] [G loss: 1.390756]\n",
      "epoch:28 step:26568 [D loss: 0.616451, acc.: 62.50%] [G loss: 1.115496]\n",
      "epoch:28 step:26569 [D loss: 0.715393, acc.: 56.25%] [G loss: 1.609302]\n",
      "epoch:28 step:26570 [D loss: 0.530650, acc.: 78.12%] [G loss: 1.405242]\n",
      "epoch:28 step:26571 [D loss: 0.677869, acc.: 59.38%] [G loss: 1.314075]\n",
      "epoch:28 step:26572 [D loss: 0.495746, acc.: 77.34%] [G loss: 1.744556]\n",
      "epoch:28 step:26573 [D loss: 0.947047, acc.: 37.50%] [G loss: 1.248747]\n",
      "epoch:28 step:26574 [D loss: 0.413019, acc.: 85.16%] [G loss: 1.518520]\n",
      "epoch:28 step:26575 [D loss: 0.569952, acc.: 73.44%] [G loss: 1.279291]\n",
      "epoch:28 step:26576 [D loss: 0.458582, acc.: 78.12%] [G loss: 1.479719]\n",
      "epoch:28 step:26577 [D loss: 0.622292, acc.: 65.62%] [G loss: 1.639326]\n",
      "epoch:28 step:26578 [D loss: 0.659677, acc.: 64.06%] [G loss: 1.211058]\n",
      "epoch:28 step:26579 [D loss: 0.637615, acc.: 61.72%] [G loss: 1.280725]\n",
      "epoch:28 step:26580 [D loss: 0.417696, acc.: 82.81%] [G loss: 1.374971]\n",
      "epoch:28 step:26581 [D loss: 0.447127, acc.: 81.25%] [G loss: 1.412861]\n",
      "epoch:28 step:26582 [D loss: 0.713059, acc.: 59.38%] [G loss: 1.498019]\n",
      "epoch:28 step:26583 [D loss: 0.501651, acc.: 75.00%] [G loss: 1.330289]\n",
      "epoch:28 step:26584 [D loss: 0.651659, acc.: 58.59%] [G loss: 1.599313]\n",
      "epoch:28 step:26585 [D loss: 0.435217, acc.: 78.12%] [G loss: 1.341663]\n",
      "epoch:28 step:26586 [D loss: 0.406877, acc.: 88.28%] [G loss: 1.071753]\n",
      "epoch:28 step:26587 [D loss: 0.608460, acc.: 68.75%] [G loss: 1.660600]\n",
      "epoch:28 step:26588 [D loss: 0.629803, acc.: 65.62%] [G loss: 1.444218]\n",
      "epoch:28 step:26589 [D loss: 0.363023, acc.: 87.50%] [G loss: 1.516668]\n",
      "epoch:28 step:26590 [D loss: 0.700074, acc.: 53.91%] [G loss: 1.180165]\n",
      "epoch:28 step:26591 [D loss: 0.435561, acc.: 82.03%] [G loss: 2.001089]\n",
      "epoch:28 step:26592 [D loss: 0.475759, acc.: 74.22%] [G loss: 1.471541]\n",
      "epoch:28 step:26593 [D loss: 0.581127, acc.: 65.62%] [G loss: 1.449565]\n",
      "epoch:28 step:26594 [D loss: 0.543610, acc.: 71.88%] [G loss: 1.433535]\n",
      "epoch:28 step:26595 [D loss: 0.551143, acc.: 78.12%] [G loss: 1.720765]\n",
      "epoch:28 step:26596 [D loss: 0.417549, acc.: 81.25%] [G loss: 1.352453]\n",
      "epoch:28 step:26597 [D loss: 0.582317, acc.: 71.09%] [G loss: 1.510505]\n",
      "epoch:28 step:26598 [D loss: 0.623383, acc.: 67.97%] [G loss: 1.452461]\n",
      "epoch:28 step:26599 [D loss: 0.573392, acc.: 67.19%] [G loss: 1.304417]\n",
      "epoch:28 step:26600 [D loss: 0.534187, acc.: 70.31%] [G loss: 1.481410]\n",
      "##############\n",
      "[2.7957607  2.22056655 2.07292079 2.92716642 0.93274667 6.09437927\n",
      " 2.53786385 2.73734968 3.9703248  5.1826209 ]\n",
      "##########\n",
      "epoch:28 step:26601 [D loss: 0.591780, acc.: 67.19%] [G loss: 1.110952]\n",
      "epoch:28 step:26602 [D loss: 0.734477, acc.: 53.12%] [G loss: 1.119682]\n",
      "epoch:28 step:26603 [D loss: 0.570928, acc.: 71.88%] [G loss: 1.162907]\n",
      "epoch:28 step:26604 [D loss: 0.448644, acc.: 78.91%] [G loss: 1.505955]\n",
      "epoch:28 step:26605 [D loss: 0.551989, acc.: 71.88%] [G loss: 1.383397]\n",
      "epoch:28 step:26606 [D loss: 0.527948, acc.: 74.22%] [G loss: 1.294921]\n",
      "epoch:28 step:26607 [D loss: 0.491418, acc.: 78.91%] [G loss: 1.214281]\n",
      "epoch:28 step:26608 [D loss: 0.602363, acc.: 66.41%] [G loss: 1.324719]\n",
      "epoch:28 step:26609 [D loss: 0.656517, acc.: 59.38%] [G loss: 1.818586]\n",
      "epoch:28 step:26610 [D loss: 0.575782, acc.: 71.09%] [G loss: 1.522502]\n",
      "epoch:28 step:26611 [D loss: 0.692206, acc.: 57.81%] [G loss: 0.952130]\n",
      "epoch:28 step:26612 [D loss: 0.552320, acc.: 72.66%] [G loss: 1.332667]\n",
      "epoch:28 step:26613 [D loss: 0.435395, acc.: 81.25%] [G loss: 1.358521]\n",
      "epoch:28 step:26614 [D loss: 0.511153, acc.: 76.56%] [G loss: 1.344933]\n",
      "epoch:28 step:26615 [D loss: 0.429734, acc.: 79.69%] [G loss: 1.555077]\n",
      "epoch:28 step:26616 [D loss: 0.748492, acc.: 57.81%] [G loss: 1.397802]\n",
      "epoch:28 step:26617 [D loss: 0.632529, acc.: 70.31%] [G loss: 1.448200]\n",
      "epoch:28 step:26618 [D loss: 0.730375, acc.: 57.81%] [G loss: 1.228391]\n",
      "epoch:28 step:26619 [D loss: 0.348179, acc.: 89.84%] [G loss: 1.616735]\n",
      "epoch:28 step:26620 [D loss: 0.670175, acc.: 64.84%] [G loss: 1.393914]\n",
      "epoch:28 step:26621 [D loss: 0.620362, acc.: 68.75%] [G loss: 1.491377]\n",
      "epoch:28 step:26622 [D loss: 0.707335, acc.: 58.59%] [G loss: 1.126088]\n",
      "epoch:28 step:26623 [D loss: 0.462596, acc.: 80.47%] [G loss: 1.422551]\n",
      "epoch:28 step:26624 [D loss: 0.701983, acc.: 57.81%] [G loss: 1.043654]\n",
      "epoch:28 step:26625 [D loss: 0.592653, acc.: 65.62%] [G loss: 0.999127]\n",
      "epoch:28 step:26626 [D loss: 0.590442, acc.: 71.09%] [G loss: 1.645525]\n",
      "epoch:28 step:26627 [D loss: 0.773588, acc.: 54.69%] [G loss: 1.186899]\n",
      "epoch:28 step:26628 [D loss: 0.428515, acc.: 85.94%] [G loss: 1.569104]\n",
      "epoch:28 step:26629 [D loss: 0.559798, acc.: 68.75%] [G loss: 0.890766]\n",
      "epoch:28 step:26630 [D loss: 0.579774, acc.: 64.84%] [G loss: 1.112206]\n",
      "epoch:28 step:26631 [D loss: 0.395817, acc.: 87.50%] [G loss: 1.103398]\n",
      "epoch:28 step:26632 [D loss: 0.545487, acc.: 73.44%] [G loss: 1.323482]\n",
      "epoch:28 step:26633 [D loss: 0.662362, acc.: 61.72%] [G loss: 1.380182]\n",
      "epoch:28 step:26634 [D loss: 0.445995, acc.: 82.81%] [G loss: 1.485389]\n",
      "epoch:28 step:26635 [D loss: 0.486012, acc.: 78.91%] [G loss: 1.854793]\n",
      "epoch:28 step:26636 [D loss: 0.596844, acc.: 64.84%] [G loss: 1.374998]\n",
      "epoch:28 step:26637 [D loss: 0.437417, acc.: 81.25%] [G loss: 1.603686]\n",
      "epoch:28 step:26638 [D loss: 0.722741, acc.: 57.03%] [G loss: 1.605604]\n",
      "epoch:28 step:26639 [D loss: 0.471838, acc.: 82.03%] [G loss: 1.577002]\n",
      "epoch:28 step:26640 [D loss: 0.655831, acc.: 62.50%] [G loss: 1.139141]\n",
      "epoch:28 step:26641 [D loss: 0.510025, acc.: 77.34%] [G loss: 1.346282]\n",
      "epoch:28 step:26642 [D loss: 0.451432, acc.: 79.69%] [G loss: 1.535525]\n",
      "epoch:28 step:26643 [D loss: 0.554550, acc.: 75.78%] [G loss: 1.545607]\n",
      "epoch:28 step:26644 [D loss: 0.475028, acc.: 74.22%] [G loss: 1.605712]\n",
      "epoch:28 step:26645 [D loss: 0.547650, acc.: 73.44%] [G loss: 1.598448]\n",
      "epoch:28 step:26646 [D loss: 0.543863, acc.: 71.09%] [G loss: 1.578716]\n",
      "epoch:28 step:26647 [D loss: 0.475305, acc.: 81.25%] [G loss: 1.495718]\n",
      "epoch:28 step:26648 [D loss: 0.595729, acc.: 65.62%] [G loss: 1.859251]\n",
      "epoch:28 step:26649 [D loss: 0.644028, acc.: 63.28%] [G loss: 1.365007]\n",
      "epoch:28 step:26650 [D loss: 0.536191, acc.: 68.75%] [G loss: 0.905868]\n",
      "epoch:28 step:26651 [D loss: 0.339348, acc.: 89.84%] [G loss: 1.415071]\n",
      "epoch:28 step:26652 [D loss: 0.508727, acc.: 72.66%] [G loss: 1.637474]\n",
      "epoch:28 step:26653 [D loss: 0.698308, acc.: 60.16%] [G loss: 1.144340]\n",
      "epoch:28 step:26654 [D loss: 0.571636, acc.: 64.06%] [G loss: 1.500325]\n",
      "epoch:28 step:26655 [D loss: 0.509481, acc.: 74.22%] [G loss: 1.529621]\n",
      "epoch:28 step:26656 [D loss: 0.383098, acc.: 87.50%] [G loss: 1.905549]\n",
      "epoch:28 step:26657 [D loss: 0.436967, acc.: 84.38%] [G loss: 1.235965]\n",
      "epoch:28 step:26658 [D loss: 0.790638, acc.: 47.66%] [G loss: 1.404115]\n",
      "epoch:28 step:26659 [D loss: 0.449805, acc.: 83.59%] [G loss: 1.345294]\n",
      "epoch:28 step:26660 [D loss: 0.533655, acc.: 76.56%] [G loss: 1.521806]\n",
      "epoch:28 step:26661 [D loss: 0.653036, acc.: 61.72%] [G loss: 1.310190]\n",
      "epoch:28 step:26662 [D loss: 0.571008, acc.: 69.53%] [G loss: 1.441820]\n",
      "epoch:28 step:26663 [D loss: 0.699047, acc.: 60.94%] [G loss: 1.970050]\n",
      "epoch:28 step:26664 [D loss: 0.692056, acc.: 60.16%] [G loss: 1.241117]\n",
      "epoch:28 step:26665 [D loss: 0.638530, acc.: 68.75%] [G loss: 1.287378]\n",
      "epoch:28 step:26666 [D loss: 0.657099, acc.: 60.94%] [G loss: 0.873064]\n",
      "epoch:28 step:26667 [D loss: 0.501305, acc.: 76.56%] [G loss: 1.684625]\n",
      "epoch:28 step:26668 [D loss: 0.653188, acc.: 63.28%] [G loss: 1.004861]\n",
      "epoch:28 step:26669 [D loss: 0.541842, acc.: 75.78%] [G loss: 1.364076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26670 [D loss: 0.470259, acc.: 79.69%] [G loss: 1.215689]\n",
      "epoch:28 step:26671 [D loss: 0.476329, acc.: 78.91%] [G loss: 1.810058]\n",
      "epoch:28 step:26672 [D loss: 0.456199, acc.: 79.69%] [G loss: 1.156240]\n",
      "epoch:28 step:26673 [D loss: 0.713677, acc.: 60.94%] [G loss: 1.101617]\n",
      "epoch:28 step:26674 [D loss: 0.555980, acc.: 68.75%] [G loss: 1.879054]\n",
      "epoch:28 step:26675 [D loss: 0.583191, acc.: 69.53%] [G loss: 1.645367]\n",
      "epoch:28 step:26676 [D loss: 0.462152, acc.: 79.69%] [G loss: 1.525993]\n",
      "epoch:28 step:26677 [D loss: 0.363584, acc.: 88.28%] [G loss: 1.342929]\n",
      "epoch:28 step:26678 [D loss: 0.664402, acc.: 63.28%] [G loss: 1.130552]\n",
      "epoch:28 step:26679 [D loss: 0.415874, acc.: 83.59%] [G loss: 1.038669]\n",
      "epoch:28 step:26680 [D loss: 0.665656, acc.: 65.62%] [G loss: 1.581177]\n",
      "epoch:28 step:26681 [D loss: 0.657091, acc.: 59.38%] [G loss: 1.391343]\n",
      "epoch:28 step:26682 [D loss: 0.483499, acc.: 74.22%] [G loss: 1.671508]\n",
      "epoch:28 step:26683 [D loss: 0.489441, acc.: 75.00%] [G loss: 1.677944]\n",
      "epoch:28 step:26684 [D loss: 0.545949, acc.: 71.09%] [G loss: 1.374247]\n",
      "epoch:28 step:26685 [D loss: 0.478796, acc.: 78.12%] [G loss: 1.392992]\n",
      "epoch:28 step:26686 [D loss: 0.661816, acc.: 64.06%] [G loss: 1.347467]\n",
      "epoch:28 step:26687 [D loss: 0.532795, acc.: 74.22%] [G loss: 1.044631]\n",
      "epoch:28 step:26688 [D loss: 0.425376, acc.: 83.59%] [G loss: 1.545110]\n",
      "epoch:28 step:26689 [D loss: 0.603632, acc.: 64.84%] [G loss: 1.550569]\n",
      "epoch:28 step:26690 [D loss: 0.413059, acc.: 83.59%] [G loss: 1.384348]\n",
      "epoch:28 step:26691 [D loss: 0.573107, acc.: 72.66%] [G loss: 0.983572]\n",
      "epoch:28 step:26692 [D loss: 0.649492, acc.: 64.06%] [G loss: 1.088252]\n",
      "epoch:28 step:26693 [D loss: 0.603271, acc.: 67.19%] [G loss: 1.482712]\n",
      "epoch:28 step:26694 [D loss: 0.512393, acc.: 75.00%] [G loss: 1.502038]\n",
      "epoch:28 step:26695 [D loss: 0.414078, acc.: 87.50%] [G loss: 1.405689]\n",
      "epoch:28 step:26696 [D loss: 0.394745, acc.: 85.16%] [G loss: 1.964664]\n",
      "epoch:28 step:26697 [D loss: 0.691332, acc.: 64.06%] [G loss: 0.984112]\n",
      "epoch:28 step:26698 [D loss: 0.520243, acc.: 72.66%] [G loss: 1.319324]\n",
      "epoch:28 step:26699 [D loss: 0.791110, acc.: 51.56%] [G loss: 0.700876]\n",
      "epoch:28 step:26700 [D loss: 0.694779, acc.: 60.16%] [G loss: 1.450768]\n",
      "epoch:28 step:26701 [D loss: 0.706263, acc.: 57.03%] [G loss: 1.380178]\n",
      "epoch:28 step:26702 [D loss: 0.461095, acc.: 82.03%] [G loss: 1.592020]\n",
      "epoch:28 step:26703 [D loss: 0.389860, acc.: 84.38%] [G loss: 1.949479]\n",
      "epoch:28 step:26704 [D loss: 0.328842, acc.: 90.62%] [G loss: 1.396715]\n",
      "epoch:28 step:26705 [D loss: 0.475024, acc.: 79.69%] [G loss: 1.342034]\n",
      "epoch:28 step:26706 [D loss: 0.760238, acc.: 52.34%] [G loss: 1.308763]\n",
      "epoch:28 step:26707 [D loss: 0.597104, acc.: 71.09%] [G loss: 1.440087]\n",
      "epoch:28 step:26708 [D loss: 0.708629, acc.: 61.72%] [G loss: 1.200207]\n",
      "epoch:28 step:26709 [D loss: 0.426462, acc.: 82.81%] [G loss: 1.409234]\n",
      "epoch:28 step:26710 [D loss: 0.539217, acc.: 74.22%] [G loss: 1.463111]\n",
      "epoch:28 step:26711 [D loss: 0.663087, acc.: 66.41%] [G loss: 1.586020]\n",
      "epoch:28 step:26712 [D loss: 0.407245, acc.: 84.38%] [G loss: 1.463858]\n",
      "epoch:28 step:26713 [D loss: 0.556145, acc.: 72.66%] [G loss: 1.204102]\n",
      "epoch:28 step:26714 [D loss: 0.444735, acc.: 81.25%] [G loss: 1.530161]\n",
      "epoch:28 step:26715 [D loss: 0.477184, acc.: 75.78%] [G loss: 1.511775]\n",
      "epoch:28 step:26716 [D loss: 0.454106, acc.: 79.69%] [G loss: 1.523813]\n",
      "epoch:28 step:26717 [D loss: 0.562099, acc.: 68.75%] [G loss: 1.467999]\n",
      "epoch:28 step:26718 [D loss: 0.566885, acc.: 69.53%] [G loss: 1.193230]\n",
      "epoch:28 step:26719 [D loss: 0.466565, acc.: 81.25%] [G loss: 1.749508]\n",
      "epoch:28 step:26720 [D loss: 0.494091, acc.: 75.78%] [G loss: 1.164677]\n",
      "epoch:28 step:26721 [D loss: 0.475054, acc.: 78.12%] [G loss: 1.731605]\n",
      "epoch:28 step:26722 [D loss: 0.755445, acc.: 57.03%] [G loss: 1.456626]\n",
      "epoch:28 step:26723 [D loss: 0.513384, acc.: 72.66%] [G loss: 1.410464]\n",
      "epoch:28 step:26724 [D loss: 0.453834, acc.: 81.25%] [G loss: 1.584386]\n",
      "epoch:28 step:26725 [D loss: 0.613629, acc.: 67.97%] [G loss: 1.360380]\n",
      "epoch:28 step:26726 [D loss: 0.522187, acc.: 73.44%] [G loss: 1.466562]\n",
      "epoch:28 step:26727 [D loss: 0.851629, acc.: 46.88%] [G loss: 1.068766]\n",
      "epoch:28 step:26728 [D loss: 0.698357, acc.: 60.94%] [G loss: 1.551364]\n",
      "epoch:28 step:26729 [D loss: 0.473909, acc.: 81.25%] [G loss: 1.705339]\n",
      "epoch:28 step:26730 [D loss: 0.438857, acc.: 77.34%] [G loss: 1.685429]\n",
      "epoch:28 step:26731 [D loss: 0.654549, acc.: 63.28%] [G loss: 1.270718]\n",
      "epoch:28 step:26732 [D loss: 0.608728, acc.: 67.97%] [G loss: 1.186538]\n",
      "epoch:28 step:26733 [D loss: 0.548580, acc.: 73.44%] [G loss: 1.248443]\n",
      "epoch:28 step:26734 [D loss: 0.805912, acc.: 50.00%] [G loss: 1.260742]\n",
      "epoch:28 step:26735 [D loss: 0.435420, acc.: 81.25%] [G loss: 1.824282]\n",
      "epoch:28 step:26736 [D loss: 0.592831, acc.: 71.09%] [G loss: 1.145586]\n",
      "epoch:28 step:26737 [D loss: 0.413578, acc.: 84.38%] [G loss: 1.883895]\n",
      "epoch:28 step:26738 [D loss: 0.410236, acc.: 85.16%] [G loss: 2.138605]\n",
      "epoch:28 step:26739 [D loss: 0.641152, acc.: 63.28%] [G loss: 1.604856]\n",
      "epoch:28 step:26740 [D loss: 0.574953, acc.: 69.53%] [G loss: 1.718506]\n",
      "epoch:28 step:26741 [D loss: 0.323512, acc.: 89.84%] [G loss: 1.483409]\n",
      "epoch:28 step:26742 [D loss: 0.505502, acc.: 74.22%] [G loss: 1.228644]\n",
      "epoch:28 step:26743 [D loss: 0.563650, acc.: 73.44%] [G loss: 1.107594]\n",
      "epoch:28 step:26744 [D loss: 0.421038, acc.: 85.16%] [G loss: 1.532404]\n",
      "epoch:28 step:26745 [D loss: 0.592951, acc.: 67.19%] [G loss: 1.118966]\n",
      "epoch:28 step:26746 [D loss: 0.524543, acc.: 72.66%] [G loss: 1.614066]\n",
      "epoch:28 step:26747 [D loss: 0.653355, acc.: 62.50%] [G loss: 1.163272]\n",
      "epoch:28 step:26748 [D loss: 0.508499, acc.: 71.88%] [G loss: 1.880874]\n",
      "epoch:28 step:26749 [D loss: 0.434991, acc.: 83.59%] [G loss: 1.545370]\n",
      "epoch:28 step:26750 [D loss: 0.655605, acc.: 61.72%] [G loss: 1.586276]\n",
      "epoch:28 step:26751 [D loss: 0.626164, acc.: 67.19%] [G loss: 1.397404]\n",
      "epoch:28 step:26752 [D loss: 0.572577, acc.: 69.53%] [G loss: 1.540798]\n",
      "epoch:28 step:26753 [D loss: 0.414814, acc.: 83.59%] [G loss: 1.638169]\n",
      "epoch:28 step:26754 [D loss: 0.688942, acc.: 57.81%] [G loss: 1.292091]\n",
      "epoch:28 step:26755 [D loss: 0.576240, acc.: 68.75%] [G loss: 1.614992]\n",
      "epoch:28 step:26756 [D loss: 0.497709, acc.: 75.00%] [G loss: 1.917879]\n",
      "epoch:28 step:26757 [D loss: 0.810999, acc.: 50.00%] [G loss: 1.196684]\n",
      "epoch:28 step:26758 [D loss: 0.629330, acc.: 67.19%] [G loss: 1.164366]\n",
      "epoch:28 step:26759 [D loss: 0.698545, acc.: 61.72%] [G loss: 1.206082]\n",
      "epoch:28 step:26760 [D loss: 0.532727, acc.: 78.12%] [G loss: 1.369017]\n",
      "epoch:28 step:26761 [D loss: 0.721198, acc.: 59.38%] [G loss: 1.482091]\n",
      "epoch:28 step:26762 [D loss: 0.475724, acc.: 75.78%] [G loss: 1.740486]\n",
      "epoch:28 step:26763 [D loss: 0.568048, acc.: 70.31%] [G loss: 1.348239]\n",
      "epoch:28 step:26764 [D loss: 0.660956, acc.: 63.28%] [G loss: 1.593704]\n",
      "epoch:28 step:26765 [D loss: 0.406989, acc.: 82.03%] [G loss: 1.564770]\n",
      "epoch:28 step:26766 [D loss: 0.873564, acc.: 43.75%] [G loss: 1.345741]\n",
      "epoch:28 step:26767 [D loss: 0.706574, acc.: 53.12%] [G loss: 1.219904]\n",
      "epoch:28 step:26768 [D loss: 0.735937, acc.: 56.25%] [G loss: 2.074679]\n",
      "epoch:28 step:26769 [D loss: 0.688503, acc.: 59.38%] [G loss: 0.944328]\n",
      "epoch:28 step:26770 [D loss: 0.688702, acc.: 62.50%] [G loss: 1.851243]\n",
      "epoch:28 step:26771 [D loss: 0.554768, acc.: 69.53%] [G loss: 1.585349]\n",
      "epoch:28 step:26772 [D loss: 0.560771, acc.: 71.88%] [G loss: 1.185792]\n",
      "epoch:28 step:26773 [D loss: 0.509366, acc.: 74.22%] [G loss: 1.432479]\n",
      "epoch:28 step:26774 [D loss: 0.628343, acc.: 64.06%] [G loss: 1.563888]\n",
      "epoch:28 step:26775 [D loss: 0.558282, acc.: 69.53%] [G loss: 1.375413]\n",
      "epoch:28 step:26776 [D loss: 0.412220, acc.: 79.69%] [G loss: 1.521026]\n",
      "epoch:28 step:26777 [D loss: 0.551429, acc.: 73.44%] [G loss: 1.067402]\n",
      "epoch:28 step:26778 [D loss: 0.538718, acc.: 72.66%] [G loss: 1.815975]\n",
      "epoch:28 step:26779 [D loss: 0.433034, acc.: 82.81%] [G loss: 1.498261]\n",
      "epoch:28 step:26780 [D loss: 0.455935, acc.: 81.25%] [G loss: 1.578853]\n",
      "epoch:28 step:26781 [D loss: 0.450850, acc.: 82.81%] [G loss: 1.650760]\n",
      "epoch:28 step:26782 [D loss: 0.550084, acc.: 70.31%] [G loss: 1.203580]\n",
      "epoch:28 step:26783 [D loss: 0.617031, acc.: 68.75%] [G loss: 1.417332]\n",
      "epoch:28 step:26784 [D loss: 0.709802, acc.: 56.25%] [G loss: 1.250496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26785 [D loss: 0.515102, acc.: 74.22%] [G loss: 1.305959]\n",
      "epoch:28 step:26786 [D loss: 0.405137, acc.: 84.38%] [G loss: 1.506888]\n",
      "epoch:28 step:26787 [D loss: 0.565641, acc.: 71.09%] [G loss: 1.773012]\n",
      "epoch:28 step:26788 [D loss: 0.426458, acc.: 82.81%] [G loss: 1.769321]\n",
      "epoch:28 step:26789 [D loss: 0.591329, acc.: 73.44%] [G loss: 1.589876]\n",
      "epoch:28 step:26790 [D loss: 0.607423, acc.: 64.06%] [G loss: 1.234337]\n",
      "epoch:28 step:26791 [D loss: 0.574891, acc.: 68.75%] [G loss: 1.365455]\n",
      "epoch:28 step:26792 [D loss: 0.522750, acc.: 74.22%] [G loss: 1.418969]\n",
      "epoch:28 step:26793 [D loss: 0.394093, acc.: 82.81%] [G loss: 1.575786]\n",
      "epoch:28 step:26794 [D loss: 0.389747, acc.: 84.38%] [G loss: 1.422722]\n",
      "epoch:28 step:26795 [D loss: 0.766264, acc.: 55.47%] [G loss: 1.038137]\n",
      "epoch:28 step:26796 [D loss: 0.483474, acc.: 78.12%] [G loss: 1.815617]\n",
      "epoch:28 step:26797 [D loss: 0.582500, acc.: 67.97%] [G loss: 1.643831]\n",
      "epoch:28 step:26798 [D loss: 0.466960, acc.: 75.78%] [G loss: 1.266141]\n",
      "epoch:28 step:26799 [D loss: 0.595663, acc.: 71.09%] [G loss: 1.104907]\n",
      "epoch:28 step:26800 [D loss: 0.325080, acc.: 89.84%] [G loss: 1.832289]\n",
      "##############\n",
      "[2.85210108 2.15399914 2.0487822  3.1854941  1.15408692 6.43158578\n",
      " 2.38105376 2.72019063 4.0901612  5.37912582]\n",
      "##########\n",
      "epoch:28 step:26801 [D loss: 0.611935, acc.: 67.19%] [G loss: 1.118464]\n",
      "epoch:28 step:26802 [D loss: 0.476321, acc.: 78.12%] [G loss: 1.289592]\n",
      "epoch:28 step:26803 [D loss: 0.536597, acc.: 72.66%] [G loss: 1.162563]\n",
      "epoch:28 step:26804 [D loss: 0.506632, acc.: 74.22%] [G loss: 1.378012]\n",
      "epoch:28 step:26805 [D loss: 0.422470, acc.: 84.38%] [G loss: 1.426921]\n",
      "epoch:28 step:26806 [D loss: 0.455617, acc.: 78.91%] [G loss: 1.259600]\n",
      "epoch:28 step:26807 [D loss: 0.526411, acc.: 75.78%] [G loss: 1.552042]\n",
      "epoch:28 step:26808 [D loss: 0.458005, acc.: 83.59%] [G loss: 1.574537]\n",
      "epoch:28 step:26809 [D loss: 0.528705, acc.: 71.09%] [G loss: 1.496976]\n",
      "epoch:28 step:26810 [D loss: 0.513183, acc.: 77.34%] [G loss: 1.336195]\n",
      "epoch:28 step:26811 [D loss: 0.438278, acc.: 77.34%] [G loss: 1.341087]\n",
      "epoch:28 step:26812 [D loss: 0.344081, acc.: 87.50%] [G loss: 1.572374]\n",
      "epoch:28 step:26813 [D loss: 0.658613, acc.: 65.62%] [G loss: 1.435641]\n",
      "epoch:28 step:26814 [D loss: 0.544924, acc.: 74.22%] [G loss: 1.524195]\n",
      "epoch:28 step:26815 [D loss: 0.553844, acc.: 75.00%] [G loss: 1.408552]\n",
      "epoch:28 step:26816 [D loss: 0.514993, acc.: 75.00%] [G loss: 0.957324]\n",
      "epoch:28 step:26817 [D loss: 0.656768, acc.: 68.75%] [G loss: 1.156174]\n",
      "epoch:28 step:26818 [D loss: 0.499663, acc.: 75.78%] [G loss: 1.581079]\n",
      "epoch:28 step:26819 [D loss: 0.493853, acc.: 75.00%] [G loss: 1.427875]\n",
      "epoch:28 step:26820 [D loss: 0.664655, acc.: 64.84%] [G loss: 1.023801]\n",
      "epoch:28 step:26821 [D loss: 0.525258, acc.: 73.44%] [G loss: 1.645090]\n",
      "epoch:28 step:26822 [D loss: 0.457252, acc.: 84.38%] [G loss: 1.431649]\n",
      "epoch:28 step:26823 [D loss: 0.353692, acc.: 87.50%] [G loss: 1.858845]\n",
      "epoch:28 step:26824 [D loss: 0.690516, acc.: 64.06%] [G loss: 1.295995]\n",
      "epoch:28 step:26825 [D loss: 0.814113, acc.: 49.22%] [G loss: 1.086662]\n",
      "epoch:28 step:26826 [D loss: 0.364361, acc.: 85.94%] [G loss: 1.523091]\n",
      "epoch:28 step:26827 [D loss: 0.476839, acc.: 80.47%] [G loss: 1.590472]\n",
      "epoch:28 step:26828 [D loss: 0.488251, acc.: 78.12%] [G loss: 1.383255]\n",
      "epoch:28 step:26829 [D loss: 0.580769, acc.: 67.97%] [G loss: 1.444164]\n",
      "epoch:28 step:26830 [D loss: 0.799355, acc.: 45.31%] [G loss: 1.493594]\n",
      "epoch:28 step:26831 [D loss: 0.540185, acc.: 69.53%] [G loss: 1.670425]\n",
      "epoch:28 step:26832 [D loss: 0.503225, acc.: 78.91%] [G loss: 1.627690]\n",
      "epoch:28 step:26833 [D loss: 0.459060, acc.: 79.69%] [G loss: 1.562179]\n",
      "epoch:28 step:26834 [D loss: 0.594049, acc.: 66.41%] [G loss: 1.540390]\n",
      "epoch:28 step:26835 [D loss: 0.517344, acc.: 74.22%] [G loss: 1.967947]\n",
      "epoch:28 step:26836 [D loss: 0.401887, acc.: 84.38%] [G loss: 1.437801]\n",
      "epoch:28 step:26837 [D loss: 0.641917, acc.: 63.28%] [G loss: 1.375159]\n",
      "epoch:28 step:26838 [D loss: 0.486007, acc.: 78.91%] [G loss: 1.256571]\n",
      "epoch:28 step:26839 [D loss: 0.536851, acc.: 72.66%] [G loss: 1.401150]\n",
      "epoch:28 step:26840 [D loss: 0.567539, acc.: 71.09%] [G loss: 1.660067]\n",
      "epoch:28 step:26841 [D loss: 0.696059, acc.: 63.28%] [G loss: 1.921713]\n",
      "epoch:28 step:26842 [D loss: 0.613652, acc.: 66.41%] [G loss: 1.180060]\n",
      "epoch:28 step:26843 [D loss: 0.628937, acc.: 68.75%] [G loss: 1.426768]\n",
      "epoch:28 step:26844 [D loss: 0.578002, acc.: 63.28%] [G loss: 0.947549]\n",
      "epoch:28 step:26845 [D loss: 0.404217, acc.: 82.03%] [G loss: 1.895029]\n",
      "epoch:28 step:26846 [D loss: 0.467095, acc.: 78.91%] [G loss: 1.201115]\n",
      "epoch:28 step:26847 [D loss: 0.681838, acc.: 60.16%] [G loss: 1.562858]\n",
      "epoch:28 step:26848 [D loss: 0.592987, acc.: 62.50%] [G loss: 1.867023]\n",
      "epoch:28 step:26849 [D loss: 0.487255, acc.: 75.78%] [G loss: 1.824970]\n",
      "epoch:28 step:26850 [D loss: 0.547500, acc.: 71.88%] [G loss: 1.705006]\n",
      "epoch:28 step:26851 [D loss: 0.587649, acc.: 71.09%] [G loss: 1.585329]\n",
      "epoch:28 step:26852 [D loss: 0.511953, acc.: 78.12%] [G loss: 1.233725]\n",
      "epoch:28 step:26853 [D loss: 0.557447, acc.: 71.88%] [G loss: 1.606337]\n",
      "epoch:28 step:26854 [D loss: 0.520738, acc.: 75.00%] [G loss: 1.380863]\n",
      "epoch:28 step:26855 [D loss: 0.399909, acc.: 84.38%] [G loss: 1.637692]\n",
      "epoch:28 step:26856 [D loss: 0.555135, acc.: 71.88%] [G loss: 1.443587]\n",
      "epoch:28 step:26857 [D loss: 0.792070, acc.: 49.22%] [G loss: 1.130875]\n",
      "epoch:28 step:26858 [D loss: 0.556410, acc.: 71.09%] [G loss: 1.196648]\n",
      "epoch:28 step:26859 [D loss: 0.571930, acc.: 71.09%] [G loss: 1.467103]\n",
      "epoch:28 step:26860 [D loss: 0.604055, acc.: 71.09%] [G loss: 1.278103]\n",
      "epoch:28 step:26861 [D loss: 0.434067, acc.: 82.03%] [G loss: 1.352553]\n",
      "epoch:28 step:26862 [D loss: 0.507204, acc.: 74.22%] [G loss: 1.267700]\n",
      "epoch:28 step:26863 [D loss: 0.337222, acc.: 87.50%] [G loss: 1.847467]\n",
      "epoch:28 step:26864 [D loss: 0.605438, acc.: 71.88%] [G loss: 1.482388]\n",
      "epoch:28 step:26865 [D loss: 0.535424, acc.: 71.88%] [G loss: 1.183415]\n",
      "epoch:28 step:26866 [D loss: 0.520611, acc.: 73.44%] [G loss: 1.266657]\n",
      "epoch:28 step:26867 [D loss: 0.592194, acc.: 70.31%] [G loss: 1.313523]\n",
      "epoch:28 step:26868 [D loss: 0.380766, acc.: 83.59%] [G loss: 1.409217]\n",
      "epoch:28 step:26869 [D loss: 0.617674, acc.: 64.84%] [G loss: 1.888830]\n",
      "epoch:28 step:26870 [D loss: 0.501029, acc.: 77.34%] [G loss: 1.418662]\n",
      "epoch:28 step:26871 [D loss: 0.554186, acc.: 69.53%] [G loss: 1.215495]\n",
      "epoch:28 step:26872 [D loss: 0.546663, acc.: 72.66%] [G loss: 1.629837]\n",
      "epoch:28 step:26873 [D loss: 0.534803, acc.: 71.09%] [G loss: 1.275324]\n",
      "epoch:28 step:26874 [D loss: 0.660975, acc.: 64.06%] [G loss: 1.261552]\n",
      "epoch:28 step:26875 [D loss: 0.519340, acc.: 75.78%] [G loss: 1.855829]\n",
      "epoch:28 step:26876 [D loss: 0.475276, acc.: 77.34%] [G loss: 1.248035]\n",
      "epoch:28 step:26877 [D loss: 0.531265, acc.: 70.31%] [G loss: 1.879344]\n",
      "epoch:28 step:26878 [D loss: 0.483869, acc.: 75.78%] [G loss: 1.257033]\n",
      "epoch:28 step:26879 [D loss: 0.714252, acc.: 53.12%] [G loss: 1.142538]\n",
      "epoch:28 step:26880 [D loss: 0.548274, acc.: 71.09%] [G loss: 1.992730]\n",
      "epoch:28 step:26881 [D loss: 0.464482, acc.: 78.91%] [G loss: 1.520724]\n",
      "epoch:28 step:26882 [D loss: 0.442819, acc.: 82.81%] [G loss: 1.711283]\n",
      "epoch:28 step:26883 [D loss: 0.508905, acc.: 74.22%] [G loss: 1.494370]\n",
      "epoch:28 step:26884 [D loss: 0.421883, acc.: 85.16%] [G loss: 1.538644]\n",
      "epoch:28 step:26885 [D loss: 0.688919, acc.: 58.59%] [G loss: 1.085811]\n",
      "epoch:28 step:26886 [D loss: 0.521702, acc.: 70.31%] [G loss: 1.534091]\n",
      "epoch:28 step:26887 [D loss: 0.428089, acc.: 84.38%] [G loss: 1.583878]\n",
      "epoch:28 step:26888 [D loss: 0.638490, acc.: 64.06%] [G loss: 1.163929]\n",
      "epoch:28 step:26889 [D loss: 0.474508, acc.: 77.34%] [G loss: 1.402074]\n",
      "epoch:28 step:26890 [D loss: 0.696670, acc.: 56.25%] [G loss: 0.825747]\n",
      "epoch:28 step:26891 [D loss: 0.508651, acc.: 75.00%] [G loss: 1.594851]\n",
      "epoch:28 step:26892 [D loss: 0.414833, acc.: 76.56%] [G loss: 1.511902]\n",
      "epoch:28 step:26893 [D loss: 0.628799, acc.: 64.06%] [G loss: 1.396773]\n",
      "epoch:28 step:26894 [D loss: 0.730621, acc.: 51.56%] [G loss: 1.223329]\n",
      "epoch:28 step:26895 [D loss: 0.548262, acc.: 75.78%] [G loss: 1.268382]\n",
      "epoch:28 step:26896 [D loss: 0.692070, acc.: 57.81%] [G loss: 1.027575]\n",
      "epoch:28 step:26897 [D loss: 0.597243, acc.: 66.41%] [G loss: 1.602020]\n",
      "epoch:28 step:26898 [D loss: 0.624591, acc.: 66.41%] [G loss: 1.239527]\n",
      "epoch:28 step:26899 [D loss: 0.663819, acc.: 61.72%] [G loss: 1.125298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26900 [D loss: 0.592848, acc.: 69.53%] [G loss: 1.280990]\n",
      "epoch:28 step:26901 [D loss: 0.567857, acc.: 68.75%] [G loss: 1.076099]\n",
      "epoch:28 step:26902 [D loss: 0.490810, acc.: 75.78%] [G loss: 1.413027]\n",
      "epoch:28 step:26903 [D loss: 0.698354, acc.: 57.03%] [G loss: 1.204681]\n",
      "epoch:28 step:26904 [D loss: 0.574322, acc.: 64.84%] [G loss: 1.447798]\n",
      "epoch:28 step:26905 [D loss: 0.521460, acc.: 72.66%] [G loss: 1.372691]\n",
      "epoch:28 step:26906 [D loss: 0.403548, acc.: 81.25%] [G loss: 1.694454]\n",
      "epoch:28 step:26907 [D loss: 0.595969, acc.: 67.97%] [G loss: 1.467084]\n",
      "epoch:28 step:26908 [D loss: 0.488220, acc.: 76.56%] [G loss: 1.785799]\n",
      "epoch:28 step:26909 [D loss: 0.708102, acc.: 60.16%] [G loss: 1.796749]\n",
      "epoch:28 step:26910 [D loss: 0.513341, acc.: 75.78%] [G loss: 1.391019]\n",
      "epoch:28 step:26911 [D loss: 0.625989, acc.: 64.06%] [G loss: 1.452571]\n",
      "epoch:28 step:26912 [D loss: 0.525458, acc.: 77.34%] [G loss: 1.433578]\n",
      "epoch:28 step:26913 [D loss: 0.511085, acc.: 72.66%] [G loss: 1.595369]\n",
      "epoch:28 step:26914 [D loss: 0.561160, acc.: 68.75%] [G loss: 1.600059]\n",
      "epoch:28 step:26915 [D loss: 0.640258, acc.: 65.62%] [G loss: 1.095068]\n",
      "epoch:28 step:26916 [D loss: 0.758298, acc.: 53.91%] [G loss: 1.583849]\n",
      "epoch:28 step:26917 [D loss: 0.535636, acc.: 76.56%] [G loss: 1.465687]\n",
      "epoch:28 step:26918 [D loss: 0.570662, acc.: 70.31%] [G loss: 1.411591]\n",
      "epoch:28 step:26919 [D loss: 0.492752, acc.: 77.34%] [G loss: 1.575217]\n",
      "epoch:28 step:26920 [D loss: 0.467387, acc.: 79.69%] [G loss: 1.476570]\n",
      "epoch:28 step:26921 [D loss: 0.487198, acc.: 82.03%] [G loss: 1.442090]\n",
      "epoch:28 step:26922 [D loss: 0.537432, acc.: 71.88%] [G loss: 1.423263]\n",
      "epoch:28 step:26923 [D loss: 0.612923, acc.: 64.84%] [G loss: 1.250297]\n",
      "epoch:28 step:26924 [D loss: 0.443882, acc.: 80.47%] [G loss: 1.553695]\n",
      "epoch:28 step:26925 [D loss: 0.554104, acc.: 73.44%] [G loss: 1.732002]\n",
      "epoch:28 step:26926 [D loss: 0.656120, acc.: 61.72%] [G loss: 1.191747]\n",
      "epoch:28 step:26927 [D loss: 0.472876, acc.: 77.34%] [G loss: 1.582008]\n",
      "epoch:28 step:26928 [D loss: 0.989440, acc.: 42.19%] [G loss: 1.197019]\n",
      "epoch:28 step:26929 [D loss: 0.500403, acc.: 80.47%] [G loss: 1.422033]\n",
      "epoch:28 step:26930 [D loss: 0.605894, acc.: 62.50%] [G loss: 1.256561]\n",
      "epoch:28 step:26931 [D loss: 0.473115, acc.: 78.91%] [G loss: 1.578862]\n",
      "epoch:28 step:26932 [D loss: 0.757250, acc.: 51.56%] [G loss: 1.499780]\n",
      "epoch:28 step:26933 [D loss: 0.413346, acc.: 82.03%] [G loss: 1.442735]\n",
      "epoch:28 step:26934 [D loss: 0.499289, acc.: 70.31%] [G loss: 1.434857]\n",
      "epoch:28 step:26935 [D loss: 0.568119, acc.: 69.53%] [G loss: 1.101654]\n",
      "epoch:28 step:26936 [D loss: 0.576903, acc.: 67.97%] [G loss: 1.072038]\n",
      "epoch:28 step:26937 [D loss: 0.339049, acc.: 88.28%] [G loss: 1.610254]\n",
      "epoch:28 step:26938 [D loss: 0.486652, acc.: 76.56%] [G loss: 1.073764]\n",
      "epoch:28 step:26939 [D loss: 0.617113, acc.: 64.06%] [G loss: 1.357104]\n",
      "epoch:28 step:26940 [D loss: 0.649144, acc.: 64.84%] [G loss: 1.405932]\n",
      "epoch:28 step:26941 [D loss: 0.543451, acc.: 76.56%] [G loss: 1.671369]\n",
      "epoch:28 step:26942 [D loss: 0.595221, acc.: 63.28%] [G loss: 1.604981]\n",
      "epoch:28 step:26943 [D loss: 0.418693, acc.: 84.38%] [G loss: 1.394904]\n",
      "epoch:28 step:26944 [D loss: 0.575409, acc.: 71.09%] [G loss: 1.610659]\n",
      "epoch:28 step:26945 [D loss: 0.674961, acc.: 65.62%] [G loss: 1.333620]\n",
      "epoch:28 step:26946 [D loss: 0.511475, acc.: 72.66%] [G loss: 1.451382]\n",
      "epoch:28 step:26947 [D loss: 0.318421, acc.: 91.41%] [G loss: 1.717638]\n",
      "epoch:28 step:26948 [D loss: 0.580221, acc.: 71.88%] [G loss: 0.856871]\n",
      "epoch:28 step:26949 [D loss: 0.512457, acc.: 75.78%] [G loss: 1.511705]\n",
      "epoch:28 step:26950 [D loss: 0.661635, acc.: 64.06%] [G loss: 1.188322]\n",
      "epoch:28 step:26951 [D loss: 0.632354, acc.: 65.62%] [G loss: 1.147864]\n",
      "epoch:28 step:26952 [D loss: 0.478982, acc.: 79.69%] [G loss: 1.347089]\n",
      "epoch:28 step:26953 [D loss: 0.633135, acc.: 63.28%] [G loss: 1.575609]\n",
      "epoch:28 step:26954 [D loss: 0.606661, acc.: 67.97%] [G loss: 1.874890]\n",
      "epoch:28 step:26955 [D loss: 0.646420, acc.: 62.50%] [G loss: 1.720124]\n",
      "epoch:28 step:26956 [D loss: 0.519493, acc.: 75.78%] [G loss: 1.579247]\n",
      "epoch:28 step:26957 [D loss: 0.663087, acc.: 61.72%] [G loss: 1.781461]\n",
      "epoch:28 step:26958 [D loss: 0.634830, acc.: 66.41%] [G loss: 1.287186]\n",
      "epoch:28 step:26959 [D loss: 0.446704, acc.: 82.81%] [G loss: 1.450132]\n",
      "epoch:28 step:26960 [D loss: 0.397709, acc.: 83.59%] [G loss: 1.156124]\n",
      "epoch:28 step:26961 [D loss: 0.390247, acc.: 85.16%] [G loss: 1.630872]\n",
      "epoch:28 step:26962 [D loss: 0.612700, acc.: 66.41%] [G loss: 0.857111]\n",
      "epoch:28 step:26963 [D loss: 0.643239, acc.: 67.97%] [G loss: 1.109702]\n",
      "epoch:28 step:26964 [D loss: 0.550545, acc.: 69.53%] [G loss: 1.642276]\n",
      "epoch:28 step:26965 [D loss: 0.407910, acc.: 82.81%] [G loss: 1.578134]\n",
      "epoch:28 step:26966 [D loss: 0.570696, acc.: 68.75%] [G loss: 1.448629]\n",
      "epoch:28 step:26967 [D loss: 0.748723, acc.: 55.47%] [G loss: 1.412869]\n",
      "epoch:28 step:26968 [D loss: 0.535213, acc.: 74.22%] [G loss: 1.348985]\n",
      "epoch:28 step:26969 [D loss: 0.430309, acc.: 85.94%] [G loss: 1.406406]\n",
      "epoch:28 step:26970 [D loss: 0.717007, acc.: 59.38%] [G loss: 1.425265]\n",
      "epoch:28 step:26971 [D loss: 0.719523, acc.: 63.28%] [G loss: 1.290294]\n",
      "epoch:28 step:26972 [D loss: 0.580544, acc.: 70.31%] [G loss: 1.789561]\n",
      "epoch:28 step:26973 [D loss: 0.543347, acc.: 71.88%] [G loss: 1.542762]\n",
      "epoch:28 step:26974 [D loss: 0.483650, acc.: 80.47%] [G loss: 1.322297]\n",
      "epoch:28 step:26975 [D loss: 0.536991, acc.: 71.88%] [G loss: 1.696837]\n",
      "epoch:28 step:26976 [D loss: 0.833630, acc.: 47.66%] [G loss: 1.115777]\n",
      "epoch:28 step:26977 [D loss: 0.706643, acc.: 58.59%] [G loss: 1.467466]\n",
      "epoch:28 step:26978 [D loss: 0.700558, acc.: 63.28%] [G loss: 1.425200]\n",
      "epoch:28 step:26979 [D loss: 0.712988, acc.: 60.16%] [G loss: 1.867771]\n",
      "epoch:28 step:26980 [D loss: 0.571271, acc.: 70.31%] [G loss: 1.762516]\n",
      "epoch:28 step:26981 [D loss: 0.530846, acc.: 75.78%] [G loss: 1.511274]\n",
      "epoch:28 step:26982 [D loss: 0.468773, acc.: 73.44%] [G loss: 1.440795]\n",
      "epoch:28 step:26983 [D loss: 0.384590, acc.: 87.50%] [G loss: 1.780499]\n",
      "epoch:28 step:26984 [D loss: 0.603763, acc.: 68.75%] [G loss: 1.692978]\n",
      "epoch:28 step:26985 [D loss: 0.567396, acc.: 69.53%] [G loss: 1.521885]\n",
      "epoch:28 step:26986 [D loss: 0.533872, acc.: 74.22%] [G loss: 1.344095]\n",
      "epoch:28 step:26987 [D loss: 0.650655, acc.: 64.06%] [G loss: 1.248492]\n",
      "epoch:28 step:26988 [D loss: 0.579715, acc.: 67.97%] [G loss: 1.185881]\n",
      "epoch:28 step:26989 [D loss: 0.458460, acc.: 78.91%] [G loss: 1.373955]\n",
      "epoch:28 step:26990 [D loss: 0.714484, acc.: 62.50%] [G loss: 0.974119]\n",
      "epoch:28 step:26991 [D loss: 0.441823, acc.: 80.47%] [G loss: 1.632557]\n",
      "epoch:28 step:26992 [D loss: 0.494894, acc.: 78.12%] [G loss: 1.596836]\n",
      "epoch:28 step:26993 [D loss: 0.472504, acc.: 78.12%] [G loss: 1.829978]\n",
      "epoch:28 step:26994 [D loss: 0.513892, acc.: 76.56%] [G loss: 2.003369]\n",
      "epoch:28 step:26995 [D loss: 0.649964, acc.: 62.50%] [G loss: 1.525713]\n",
      "epoch:28 step:26996 [D loss: 0.601978, acc.: 72.66%] [G loss: 1.093098]\n",
      "epoch:28 step:26997 [D loss: 1.109346, acc.: 28.12%] [G loss: 0.935177]\n",
      "epoch:28 step:26998 [D loss: 0.383185, acc.: 88.28%] [G loss: 1.664925]\n",
      "epoch:28 step:26999 [D loss: 0.662876, acc.: 63.28%] [G loss: 1.445841]\n",
      "epoch:28 step:27000 [D loss: 0.439031, acc.: 82.81%] [G loss: 1.692195]\n",
      "##############\n",
      "[2.6093462  2.1068188  2.02025079 2.90597889 1.08093389 6.66064041\n",
      " 2.01072451 3.10488061 3.95395388 5.65183582]\n",
      "##########\n",
      "epoch:28 step:27001 [D loss: 0.586491, acc.: 67.19%] [G loss: 1.568301]\n",
      "epoch:28 step:27002 [D loss: 0.483216, acc.: 75.00%] [G loss: 1.191346]\n",
      "epoch:28 step:27003 [D loss: 0.409936, acc.: 85.16%] [G loss: 1.563551]\n",
      "epoch:28 step:27004 [D loss: 0.434027, acc.: 78.91%] [G loss: 2.029880]\n",
      "epoch:28 step:27005 [D loss: 0.577369, acc.: 69.53%] [G loss: 1.440695]\n",
      "epoch:28 step:27006 [D loss: 0.677062, acc.: 61.72%] [G loss: 1.299122]\n",
      "epoch:28 step:27007 [D loss: 0.425992, acc.: 79.69%] [G loss: 1.694671]\n",
      "epoch:28 step:27008 [D loss: 0.379753, acc.: 86.72%] [G loss: 1.159375]\n",
      "epoch:28 step:27009 [D loss: 0.462744, acc.: 79.69%] [G loss: 1.460225]\n",
      "epoch:28 step:27010 [D loss: 0.419962, acc.: 79.69%] [G loss: 1.239482]\n",
      "epoch:28 step:27011 [D loss: 0.590142, acc.: 71.09%] [G loss: 1.254551]\n",
      "epoch:28 step:27012 [D loss: 0.486628, acc.: 79.69%] [G loss: 1.197234]\n",
      "epoch:28 step:27013 [D loss: 0.764509, acc.: 53.91%] [G loss: 1.127105]\n",
      "epoch:28 step:27014 [D loss: 0.592685, acc.: 69.53%] [G loss: 1.459411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27015 [D loss: 0.469455, acc.: 78.12%] [G loss: 1.783454]\n",
      "epoch:28 step:27016 [D loss: 0.554917, acc.: 67.97%] [G loss: 1.415243]\n",
      "epoch:28 step:27017 [D loss: 0.505479, acc.: 78.12%] [G loss: 1.103788]\n",
      "epoch:28 step:27018 [D loss: 0.404900, acc.: 85.94%] [G loss: 1.569307]\n",
      "epoch:28 step:27019 [D loss: 0.517096, acc.: 72.66%] [G loss: 1.663913]\n",
      "epoch:28 step:27020 [D loss: 0.538775, acc.: 73.44%] [G loss: 1.417196]\n",
      "epoch:28 step:27021 [D loss: 0.276456, acc.: 92.97%] [G loss: 1.431111]\n",
      "epoch:28 step:27022 [D loss: 0.508478, acc.: 75.00%] [G loss: 1.298105]\n",
      "epoch:28 step:27023 [D loss: 0.586788, acc.: 70.31%] [G loss: 1.247613]\n",
      "epoch:28 step:27024 [D loss: 0.619183, acc.: 65.62%] [G loss: 1.429725]\n",
      "epoch:28 step:27025 [D loss: 0.330122, acc.: 89.84%] [G loss: 1.718867]\n",
      "epoch:28 step:27026 [D loss: 0.581062, acc.: 68.75%] [G loss: 1.441097]\n",
      "epoch:28 step:27027 [D loss: 0.579702, acc.: 68.75%] [G loss: 1.708072]\n",
      "epoch:28 step:27028 [D loss: 0.496043, acc.: 80.47%] [G loss: 1.349905]\n",
      "epoch:28 step:27029 [D loss: 0.558998, acc.: 72.66%] [G loss: 1.185049]\n",
      "epoch:28 step:27030 [D loss: 0.369638, acc.: 85.16%] [G loss: 1.593409]\n",
      "epoch:28 step:27031 [D loss: 0.693507, acc.: 57.03%] [G loss: 1.333951]\n",
      "epoch:28 step:27032 [D loss: 0.588211, acc.: 69.53%] [G loss: 1.393706]\n",
      "epoch:28 step:27033 [D loss: 0.517378, acc.: 76.56%] [G loss: 1.561685]\n",
      "epoch:28 step:27034 [D loss: 0.438902, acc.: 78.91%] [G loss: 1.573907]\n",
      "epoch:28 step:27035 [D loss: 0.414664, acc.: 83.59%] [G loss: 1.539590]\n",
      "epoch:28 step:27036 [D loss: 0.443914, acc.: 77.34%] [G loss: 1.695992]\n",
      "epoch:28 step:27037 [D loss: 0.796357, acc.: 51.56%] [G loss: 1.450755]\n",
      "epoch:28 step:27038 [D loss: 0.545215, acc.: 71.09%] [G loss: 1.429558]\n",
      "epoch:28 step:27039 [D loss: 0.575851, acc.: 71.09%] [G loss: 1.231889]\n",
      "epoch:28 step:27040 [D loss: 0.501995, acc.: 75.78%] [G loss: 1.752033]\n",
      "epoch:28 step:27041 [D loss: 0.689941, acc.: 62.50%] [G loss: 1.767751]\n",
      "epoch:28 step:27042 [D loss: 0.518812, acc.: 70.31%] [G loss: 1.738172]\n",
      "epoch:28 step:27043 [D loss: 0.443185, acc.: 79.69%] [G loss: 1.646634]\n",
      "epoch:28 step:27044 [D loss: 0.537949, acc.: 70.31%] [G loss: 1.499184]\n",
      "epoch:28 step:27045 [D loss: 0.564777, acc.: 75.00%] [G loss: 1.439592]\n",
      "epoch:28 step:27046 [D loss: 0.354367, acc.: 87.50%] [G loss: 1.649378]\n",
      "epoch:28 step:27047 [D loss: 0.588072, acc.: 67.97%] [G loss: 1.826733]\n",
      "epoch:28 step:27048 [D loss: 0.416591, acc.: 85.94%] [G loss: 1.697847]\n",
      "epoch:28 step:27049 [D loss: 0.750103, acc.: 57.81%] [G loss: 1.146971]\n",
      "epoch:28 step:27050 [D loss: 0.522893, acc.: 77.34%] [G loss: 1.094059]\n",
      "epoch:28 step:27051 [D loss: 0.486833, acc.: 79.69%] [G loss: 1.209059]\n",
      "epoch:28 step:27052 [D loss: 0.495402, acc.: 78.91%] [G loss: 1.460174]\n",
      "epoch:28 step:27053 [D loss: 0.458649, acc.: 83.59%] [G loss: 1.509965]\n",
      "epoch:28 step:27054 [D loss: 0.482946, acc.: 74.22%] [G loss: 1.431266]\n",
      "epoch:28 step:27055 [D loss: 0.466454, acc.: 78.12%] [G loss: 1.490103]\n",
      "epoch:28 step:27056 [D loss: 0.639442, acc.: 64.84%] [G loss: 1.817604]\n",
      "epoch:28 step:27057 [D loss: 0.623904, acc.: 63.28%] [G loss: 1.460738]\n",
      "epoch:28 step:27058 [D loss: 0.672104, acc.: 65.62%] [G loss: 1.513005]\n",
      "epoch:28 step:27059 [D loss: 0.462630, acc.: 80.47%] [G loss: 1.541975]\n",
      "epoch:28 step:27060 [D loss: 0.561160, acc.: 72.66%] [G loss: 1.366657]\n",
      "epoch:28 step:27061 [D loss: 0.549020, acc.: 71.88%] [G loss: 1.470158]\n",
      "epoch:28 step:27062 [D loss: 0.481235, acc.: 76.56%] [G loss: 1.542958]\n",
      "epoch:28 step:27063 [D loss: 0.407184, acc.: 83.59%] [G loss: 1.764303]\n",
      "epoch:28 step:27064 [D loss: 0.848831, acc.: 53.91%] [G loss: 1.414549]\n",
      "epoch:28 step:27065 [D loss: 0.652560, acc.: 60.94%] [G loss: 1.433619]\n",
      "epoch:28 step:27066 [D loss: 0.511322, acc.: 76.56%] [G loss: 1.599288]\n",
      "epoch:28 step:27067 [D loss: 0.650687, acc.: 66.41%] [G loss: 1.315845]\n",
      "epoch:28 step:27068 [D loss: 0.560265, acc.: 71.88%] [G loss: 1.140580]\n",
      "epoch:28 step:27069 [D loss: 0.730912, acc.: 57.03%] [G loss: 1.244816]\n",
      "epoch:28 step:27070 [D loss: 0.552565, acc.: 74.22%] [G loss: 1.514284]\n",
      "epoch:28 step:27071 [D loss: 0.538419, acc.: 73.44%] [G loss: 1.844739]\n",
      "epoch:28 step:27072 [D loss: 0.652448, acc.: 64.84%] [G loss: 1.475759]\n",
      "epoch:28 step:27073 [D loss: 0.686887, acc.: 62.50%] [G loss: 1.177530]\n",
      "epoch:28 step:27074 [D loss: 0.558711, acc.: 68.75%] [G loss: 1.054376]\n",
      "epoch:28 step:27075 [D loss: 0.553802, acc.: 71.88%] [G loss: 1.663157]\n",
      "epoch:28 step:27076 [D loss: 0.555886, acc.: 71.09%] [G loss: 1.958457]\n",
      "epoch:28 step:27077 [D loss: 0.578981, acc.: 73.44%] [G loss: 1.277545]\n",
      "epoch:28 step:27078 [D loss: 0.554857, acc.: 69.53%] [G loss: 1.335564]\n",
      "epoch:28 step:27079 [D loss: 0.636418, acc.: 70.31%] [G loss: 1.751184]\n",
      "epoch:28 step:27080 [D loss: 0.882832, acc.: 43.75%] [G loss: 1.171177]\n",
      "epoch:28 step:27081 [D loss: 0.607057, acc.: 67.97%] [G loss: 1.258784]\n",
      "epoch:28 step:27082 [D loss: 0.616416, acc.: 63.28%] [G loss: 1.169494]\n",
      "epoch:28 step:27083 [D loss: 0.494135, acc.: 82.03%] [G loss: 1.827300]\n",
      "epoch:28 step:27084 [D loss: 0.573463, acc.: 70.31%] [G loss: 1.482291]\n",
      "epoch:28 step:27085 [D loss: 0.423259, acc.: 80.47%] [G loss: 1.684411]\n",
      "epoch:28 step:27086 [D loss: 0.336859, acc.: 88.28%] [G loss: 1.686225]\n",
      "epoch:28 step:27087 [D loss: 0.580735, acc.: 73.44%] [G loss: 1.378585]\n",
      "epoch:28 step:27088 [D loss: 0.574868, acc.: 71.09%] [G loss: 1.389318]\n",
      "epoch:28 step:27089 [D loss: 0.526709, acc.: 73.44%] [G loss: 1.638811]\n",
      "epoch:28 step:27090 [D loss: 0.638951, acc.: 61.72%] [G loss: 1.558669]\n",
      "epoch:28 step:27091 [D loss: 0.754931, acc.: 53.91%] [G loss: 0.999198]\n",
      "epoch:28 step:27092 [D loss: 0.507137, acc.: 77.34%] [G loss: 1.573094]\n",
      "epoch:28 step:27093 [D loss: 0.678436, acc.: 59.38%] [G loss: 1.717389]\n",
      "epoch:28 step:27094 [D loss: 0.428279, acc.: 83.59%] [G loss: 1.318945]\n",
      "epoch:28 step:27095 [D loss: 0.622353, acc.: 63.28%] [G loss: 1.320007]\n",
      "epoch:28 step:27096 [D loss: 0.567176, acc.: 67.97%] [G loss: 1.437744]\n",
      "epoch:28 step:27097 [D loss: 0.431756, acc.: 82.03%] [G loss: 1.379323]\n",
      "epoch:28 step:27098 [D loss: 0.516855, acc.: 71.88%] [G loss: 1.361368]\n",
      "epoch:28 step:27099 [D loss: 0.631526, acc.: 64.84%] [G loss: 1.510525]\n",
      "epoch:28 step:27100 [D loss: 0.648086, acc.: 68.75%] [G loss: 1.227028]\n",
      "epoch:28 step:27101 [D loss: 0.712318, acc.: 59.38%] [G loss: 1.441889]\n",
      "epoch:28 step:27102 [D loss: 0.538930, acc.: 70.31%] [G loss: 1.614332]\n",
      "epoch:28 step:27103 [D loss: 0.431483, acc.: 81.25%] [G loss: 1.495022]\n",
      "epoch:28 step:27104 [D loss: 0.629445, acc.: 65.62%] [G loss: 1.072504]\n",
      "epoch:28 step:27105 [D loss: 0.431376, acc.: 82.81%] [G loss: 1.614074]\n",
      "epoch:28 step:27106 [D loss: 0.446109, acc.: 76.56%] [G loss: 1.813861]\n",
      "epoch:28 step:27107 [D loss: 0.650043, acc.: 64.84%] [G loss: 1.731212]\n",
      "epoch:28 step:27108 [D loss: 0.596858, acc.: 68.75%] [G loss: 1.051692]\n",
      "epoch:28 step:27109 [D loss: 0.545705, acc.: 72.66%] [G loss: 1.470730]\n",
      "epoch:28 step:27110 [D loss: 0.607723, acc.: 67.97%] [G loss: 1.001774]\n",
      "epoch:28 step:27111 [D loss: 0.644554, acc.: 61.72%] [G loss: 0.730891]\n",
      "epoch:28 step:27112 [D loss: 0.517230, acc.: 77.34%] [G loss: 1.688837]\n",
      "epoch:28 step:27113 [D loss: 0.498277, acc.: 78.12%] [G loss: 1.294623]\n",
      "epoch:28 step:27114 [D loss: 0.535947, acc.: 74.22%] [G loss: 1.519389]\n",
      "epoch:28 step:27115 [D loss: 0.597506, acc.: 65.62%] [G loss: 1.311412]\n",
      "epoch:28 step:27116 [D loss: 0.591951, acc.: 66.41%] [G loss: 1.335786]\n",
      "epoch:28 step:27117 [D loss: 0.756316, acc.: 54.69%] [G loss: 1.297825]\n",
      "epoch:28 step:27118 [D loss: 0.467661, acc.: 78.91%] [G loss: 1.332047]\n",
      "epoch:28 step:27119 [D loss: 0.388327, acc.: 85.16%] [G loss: 1.455559]\n",
      "epoch:28 step:27120 [D loss: 0.418386, acc.: 85.16%] [G loss: 1.285608]\n",
      "epoch:28 step:27121 [D loss: 0.384749, acc.: 85.16%] [G loss: 1.448733]\n",
      "epoch:28 step:27122 [D loss: 0.485061, acc.: 75.78%] [G loss: 1.512341]\n",
      "epoch:28 step:27123 [D loss: 0.648896, acc.: 62.50%] [G loss: 1.133105]\n",
      "epoch:28 step:27124 [D loss: 0.588607, acc.: 66.41%] [G loss: 1.197347]\n",
      "epoch:28 step:27125 [D loss: 0.563153, acc.: 73.44%] [G loss: 1.328113]\n",
      "epoch:28 step:27126 [D loss: 0.525515, acc.: 72.66%] [G loss: 1.461351]\n",
      "epoch:28 step:27127 [D loss: 0.509323, acc.: 76.56%] [G loss: 1.545722]\n",
      "epoch:28 step:27128 [D loss: 0.698141, acc.: 63.28%] [G loss: 1.573872]\n",
      "epoch:28 step:27129 [D loss: 0.468085, acc.: 81.25%] [G loss: 1.511446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27130 [D loss: 0.471501, acc.: 78.12%] [G loss: 1.668521]\n",
      "epoch:28 step:27131 [D loss: 0.402970, acc.: 84.38%] [G loss: 1.636617]\n",
      "epoch:28 step:27132 [D loss: 0.554020, acc.: 71.09%] [G loss: 1.528045]\n",
      "epoch:28 step:27133 [D loss: 0.342796, acc.: 89.84%] [G loss: 1.692710]\n",
      "epoch:28 step:27134 [D loss: 0.444115, acc.: 80.47%] [G loss: 1.222579]\n",
      "epoch:28 step:27135 [D loss: 0.472820, acc.: 81.25%] [G loss: 1.246893]\n",
      "epoch:28 step:27136 [D loss: 0.516573, acc.: 73.44%] [G loss: 1.723077]\n",
      "epoch:28 step:27137 [D loss: 0.783905, acc.: 53.12%] [G loss: 0.976958]\n",
      "epoch:28 step:27138 [D loss: 0.502551, acc.: 76.56%] [G loss: 1.884269]\n",
      "epoch:28 step:27139 [D loss: 0.508378, acc.: 75.78%] [G loss: 1.642284]\n",
      "epoch:28 step:27140 [D loss: 0.564259, acc.: 71.09%] [G loss: 1.445579]\n",
      "epoch:28 step:27141 [D loss: 0.670905, acc.: 62.50%] [G loss: 1.404984]\n",
      "epoch:28 step:27142 [D loss: 0.534141, acc.: 74.22%] [G loss: 1.870494]\n",
      "epoch:28 step:27143 [D loss: 0.511154, acc.: 71.88%] [G loss: 1.578455]\n",
      "epoch:28 step:27144 [D loss: 0.519813, acc.: 73.44%] [G loss: 1.436132]\n",
      "epoch:28 step:27145 [D loss: 0.461156, acc.: 78.12%] [G loss: 1.542523]\n",
      "epoch:28 step:27146 [D loss: 0.476563, acc.: 79.69%] [G loss: 1.573722]\n",
      "epoch:28 step:27147 [D loss: 0.454619, acc.: 80.47%] [G loss: 1.118666]\n",
      "epoch:28 step:27148 [D loss: 0.602374, acc.: 66.41%] [G loss: 1.341960]\n",
      "epoch:28 step:27149 [D loss: 0.519079, acc.: 75.78%] [G loss: 1.533803]\n",
      "epoch:28 step:27150 [D loss: 0.714407, acc.: 57.03%] [G loss: 1.199548]\n",
      "epoch:28 step:27151 [D loss: 0.509418, acc.: 71.88%] [G loss: 1.325450]\n",
      "epoch:28 step:27152 [D loss: 0.647411, acc.: 59.38%] [G loss: 1.182170]\n",
      "epoch:28 step:27153 [D loss: 0.547696, acc.: 72.66%] [G loss: 1.405538]\n",
      "epoch:28 step:27154 [D loss: 0.578872, acc.: 68.75%] [G loss: 1.507408]\n",
      "epoch:28 step:27155 [D loss: 0.557546, acc.: 72.66%] [G loss: 1.524650]\n",
      "epoch:28 step:27156 [D loss: 0.410435, acc.: 83.59%] [G loss: 1.247364]\n",
      "epoch:28 step:27157 [D loss: 0.733314, acc.: 57.03%] [G loss: 1.369053]\n",
      "epoch:28 step:27158 [D loss: 0.476419, acc.: 78.12%] [G loss: 1.611391]\n",
      "epoch:28 step:27159 [D loss: 0.742690, acc.: 57.03%] [G loss: 1.515432]\n",
      "epoch:28 step:27160 [D loss: 0.565741, acc.: 71.09%] [G loss: 1.755830]\n",
      "epoch:28 step:27161 [D loss: 0.511730, acc.: 78.91%] [G loss: 1.451221]\n",
      "epoch:28 step:27162 [D loss: 0.484903, acc.: 83.59%] [G loss: 1.548405]\n",
      "epoch:28 step:27163 [D loss: 0.842507, acc.: 49.22%] [G loss: 1.695275]\n",
      "epoch:28 step:27164 [D loss: 0.467065, acc.: 78.91%] [G loss: 1.554064]\n",
      "epoch:28 step:27165 [D loss: 0.675925, acc.: 61.72%] [G loss: 1.148624]\n",
      "epoch:28 step:27166 [D loss: 0.545901, acc.: 70.31%] [G loss: 1.203015]\n",
      "epoch:28 step:27167 [D loss: 0.606543, acc.: 64.84%] [G loss: 0.945803]\n",
      "epoch:28 step:27168 [D loss: 0.418740, acc.: 82.03%] [G loss: 1.295659]\n",
      "epoch:28 step:27169 [D loss: 0.495922, acc.: 79.69%] [G loss: 1.369098]\n",
      "epoch:28 step:27170 [D loss: 0.470887, acc.: 76.56%] [G loss: 1.544816]\n",
      "epoch:28 step:27171 [D loss: 0.539134, acc.: 75.00%] [G loss: 1.171009]\n",
      "epoch:28 step:27172 [D loss: 0.285907, acc.: 94.53%] [G loss: 1.900151]\n",
      "epoch:28 step:27173 [D loss: 0.482062, acc.: 76.56%] [G loss: 1.528265]\n",
      "epoch:29 step:27174 [D loss: 0.709051, acc.: 54.69%] [G loss: 1.665991]\n",
      "epoch:29 step:27175 [D loss: 0.450441, acc.: 78.12%] [G loss: 1.318816]\n",
      "epoch:29 step:27176 [D loss: 0.462024, acc.: 81.25%] [G loss: 1.412042]\n",
      "epoch:29 step:27177 [D loss: 0.633965, acc.: 60.16%] [G loss: 1.279899]\n",
      "epoch:29 step:27178 [D loss: 0.575529, acc.: 71.09%] [G loss: 1.497270]\n",
      "epoch:29 step:27179 [D loss: 0.804004, acc.: 55.47%] [G loss: 1.011056]\n",
      "epoch:29 step:27180 [D loss: 0.629025, acc.: 64.84%] [G loss: 1.265193]\n",
      "epoch:29 step:27181 [D loss: 0.437753, acc.: 81.25%] [G loss: 1.728329]\n",
      "epoch:29 step:27182 [D loss: 0.549985, acc.: 73.44%] [G loss: 1.411727]\n",
      "epoch:29 step:27183 [D loss: 0.603366, acc.: 67.97%] [G loss: 1.275130]\n",
      "epoch:29 step:27184 [D loss: 0.361917, acc.: 89.84%] [G loss: 1.294482]\n",
      "epoch:29 step:27185 [D loss: 0.529775, acc.: 79.69%] [G loss: 1.272324]\n",
      "epoch:29 step:27186 [D loss: 0.710337, acc.: 56.25%] [G loss: 1.259771]\n",
      "epoch:29 step:27187 [D loss: 0.547093, acc.: 75.00%] [G loss: 1.532438]\n",
      "epoch:29 step:27188 [D loss: 0.651872, acc.: 67.19%] [G loss: 1.471988]\n",
      "epoch:29 step:27189 [D loss: 0.551196, acc.: 67.19%] [G loss: 1.769621]\n",
      "epoch:29 step:27190 [D loss: 0.640105, acc.: 58.59%] [G loss: 1.498603]\n",
      "epoch:29 step:27191 [D loss: 0.600040, acc.: 62.50%] [G loss: 1.637456]\n",
      "epoch:29 step:27192 [D loss: 0.493603, acc.: 81.25%] [G loss: 1.721168]\n",
      "epoch:29 step:27193 [D loss: 0.662305, acc.: 63.28%] [G loss: 1.169515]\n",
      "epoch:29 step:27194 [D loss: 0.521175, acc.: 73.44%] [G loss: 1.550240]\n",
      "epoch:29 step:27195 [D loss: 0.593201, acc.: 67.19%] [G loss: 1.564562]\n",
      "epoch:29 step:27196 [D loss: 0.418484, acc.: 85.16%] [G loss: 1.269941]\n",
      "epoch:29 step:27197 [D loss: 0.523245, acc.: 70.31%] [G loss: 1.375731]\n",
      "epoch:29 step:27198 [D loss: 0.588232, acc.: 66.41%] [G loss: 1.227658]\n",
      "epoch:29 step:27199 [D loss: 0.547928, acc.: 76.56%] [G loss: 1.496032]\n",
      "epoch:29 step:27200 [D loss: 0.608424, acc.: 66.41%] [G loss: 1.581435]\n",
      "##############\n",
      "[2.6566195  2.08931    1.6814576  2.68103325 0.96083089 6.05973765\n",
      " 2.21226131 2.99695273 3.88407279 4.87142898]\n",
      "##########\n",
      "epoch:29 step:27201 [D loss: 0.415187, acc.: 83.59%] [G loss: 1.601213]\n",
      "epoch:29 step:27202 [D loss: 0.463474, acc.: 82.81%] [G loss: 1.456074]\n",
      "epoch:29 step:27203 [D loss: 0.610158, acc.: 64.84%] [G loss: 1.274835]\n",
      "epoch:29 step:27204 [D loss: 0.360277, acc.: 88.28%] [G loss: 1.763514]\n",
      "epoch:29 step:27205 [D loss: 0.795150, acc.: 49.22%] [G loss: 1.344732]\n",
      "epoch:29 step:27206 [D loss: 0.357922, acc.: 84.38%] [G loss: 2.027446]\n",
      "epoch:29 step:27207 [D loss: 0.519579, acc.: 78.12%] [G loss: 1.326666]\n",
      "epoch:29 step:27208 [D loss: 0.590952, acc.: 70.31%] [G loss: 1.314153]\n",
      "epoch:29 step:27209 [D loss: 0.375610, acc.: 85.16%] [G loss: 1.296316]\n",
      "epoch:29 step:27210 [D loss: 0.547670, acc.: 74.22%] [G loss: 1.846417]\n",
      "epoch:29 step:27211 [D loss: 0.605204, acc.: 65.62%] [G loss: 1.224294]\n",
      "epoch:29 step:27212 [D loss: 0.489942, acc.: 79.69%] [G loss: 1.209537]\n",
      "epoch:29 step:27213 [D loss: 0.566246, acc.: 66.41%] [G loss: 1.577315]\n",
      "epoch:29 step:27214 [D loss: 0.517271, acc.: 75.00%] [G loss: 1.304626]\n",
      "epoch:29 step:27215 [D loss: 0.519597, acc.: 71.88%] [G loss: 1.606730]\n",
      "epoch:29 step:27216 [D loss: 0.680936, acc.: 66.41%] [G loss: 1.435521]\n",
      "epoch:29 step:27217 [D loss: 0.441621, acc.: 81.25%] [G loss: 1.403858]\n",
      "epoch:29 step:27218 [D loss: 0.493080, acc.: 75.78%] [G loss: 1.672066]\n",
      "epoch:29 step:27219 [D loss: 0.595468, acc.: 67.97%] [G loss: 1.194637]\n",
      "epoch:29 step:27220 [D loss: 0.412522, acc.: 84.38%] [G loss: 1.287337]\n",
      "epoch:29 step:27221 [D loss: 0.463109, acc.: 75.78%] [G loss: 1.239295]\n",
      "epoch:29 step:27222 [D loss: 0.557070, acc.: 67.19%] [G loss: 1.283778]\n",
      "epoch:29 step:27223 [D loss: 0.449085, acc.: 82.03%] [G loss: 1.807274]\n",
      "epoch:29 step:27224 [D loss: 0.422462, acc.: 84.38%] [G loss: 1.329693]\n",
      "epoch:29 step:27225 [D loss: 0.769087, acc.: 56.25%] [G loss: 1.305172]\n",
      "epoch:29 step:27226 [D loss: 0.634830, acc.: 62.50%] [G loss: 1.667870]\n",
      "epoch:29 step:27227 [D loss: 0.429881, acc.: 82.81%] [G loss: 1.166536]\n",
      "epoch:29 step:27228 [D loss: 0.624178, acc.: 63.28%] [G loss: 1.288726]\n",
      "epoch:29 step:27229 [D loss: 0.468202, acc.: 78.12%] [G loss: 1.751187]\n",
      "epoch:29 step:27230 [D loss: 0.467013, acc.: 77.34%] [G loss: 1.330038]\n",
      "epoch:29 step:27231 [D loss: 0.569071, acc.: 68.75%] [G loss: 1.559953]\n",
      "epoch:29 step:27232 [D loss: 0.318657, acc.: 90.62%] [G loss: 1.744672]\n",
      "epoch:29 step:27233 [D loss: 0.467532, acc.: 78.91%] [G loss: 1.167698]\n",
      "epoch:29 step:27234 [D loss: 0.544675, acc.: 74.22%] [G loss: 1.359973]\n",
      "epoch:29 step:27235 [D loss: 0.435709, acc.: 81.25%] [G loss: 1.588388]\n",
      "epoch:29 step:27236 [D loss: 0.417451, acc.: 82.81%] [G loss: 1.417314]\n",
      "epoch:29 step:27237 [D loss: 0.493273, acc.: 78.12%] [G loss: 1.107059]\n",
      "epoch:29 step:27238 [D loss: 0.423200, acc.: 82.81%] [G loss: 1.367319]\n",
      "epoch:29 step:27239 [D loss: 0.685060, acc.: 62.50%] [G loss: 1.122419]\n",
      "epoch:29 step:27240 [D loss: 0.590625, acc.: 66.41%] [G loss: 1.363006]\n",
      "epoch:29 step:27241 [D loss: 0.497102, acc.: 75.00%] [G loss: 1.822198]\n",
      "epoch:29 step:27242 [D loss: 0.547765, acc.: 72.66%] [G loss: 1.441195]\n",
      "epoch:29 step:27243 [D loss: 0.581842, acc.: 67.97%] [G loss: 1.494347]\n",
      "epoch:29 step:27244 [D loss: 0.746578, acc.: 57.81%] [G loss: 0.886812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27245 [D loss: 0.551434, acc.: 70.31%] [G loss: 1.771797]\n",
      "epoch:29 step:27246 [D loss: 0.492023, acc.: 75.78%] [G loss: 1.451514]\n",
      "epoch:29 step:27247 [D loss: 0.449169, acc.: 78.12%] [G loss: 1.571124]\n",
      "epoch:29 step:27248 [D loss: 0.490387, acc.: 77.34%] [G loss: 1.915568]\n",
      "epoch:29 step:27249 [D loss: 0.553642, acc.: 73.44%] [G loss: 1.387169]\n",
      "epoch:29 step:27250 [D loss: 0.701101, acc.: 60.16%] [G loss: 1.384086]\n",
      "epoch:29 step:27251 [D loss: 0.614313, acc.: 69.53%] [G loss: 1.463619]\n",
      "epoch:29 step:27252 [D loss: 0.563683, acc.: 68.75%] [G loss: 1.084384]\n",
      "epoch:29 step:27253 [D loss: 0.509964, acc.: 73.44%] [G loss: 1.545937]\n",
      "epoch:29 step:27254 [D loss: 0.641445, acc.: 62.50%] [G loss: 1.529750]\n",
      "epoch:29 step:27255 [D loss: 0.564461, acc.: 71.88%] [G loss: 1.761209]\n",
      "epoch:29 step:27256 [D loss: 0.695866, acc.: 60.16%] [G loss: 1.229539]\n",
      "epoch:29 step:27257 [D loss: 0.699876, acc.: 57.03%] [G loss: 1.016360]\n",
      "epoch:29 step:27258 [D loss: 0.546460, acc.: 74.22%] [G loss: 1.232749]\n",
      "epoch:29 step:27259 [D loss: 0.499075, acc.: 76.56%] [G loss: 1.293887]\n",
      "epoch:29 step:27260 [D loss: 0.544159, acc.: 77.34%] [G loss: 1.096116]\n",
      "epoch:29 step:27261 [D loss: 0.574654, acc.: 71.88%] [G loss: 1.268836]\n",
      "epoch:29 step:27262 [D loss: 0.712668, acc.: 64.06%] [G loss: 1.303852]\n",
      "epoch:29 step:27263 [D loss: 0.495236, acc.: 79.69%] [G loss: 1.165563]\n",
      "epoch:29 step:27264 [D loss: 0.643701, acc.: 66.41%] [G loss: 1.241512]\n",
      "epoch:29 step:27265 [D loss: 0.497832, acc.: 75.00%] [G loss: 1.442551]\n",
      "epoch:29 step:27266 [D loss: 0.534070, acc.: 71.88%] [G loss: 1.496691]\n",
      "epoch:29 step:27267 [D loss: 0.392043, acc.: 84.38%] [G loss: 1.148531]\n",
      "epoch:29 step:27268 [D loss: 0.546144, acc.: 72.66%] [G loss: 1.693455]\n",
      "epoch:29 step:27269 [D loss: 0.425387, acc.: 78.12%] [G loss: 1.620441]\n",
      "epoch:29 step:27270 [D loss: 0.520657, acc.: 74.22%] [G loss: 1.314752]\n",
      "epoch:29 step:27271 [D loss: 0.516587, acc.: 77.34%] [G loss: 1.541982]\n",
      "epoch:29 step:27272 [D loss: 0.532433, acc.: 75.00%] [G loss: 1.187220]\n",
      "epoch:29 step:27273 [D loss: 0.680096, acc.: 64.06%] [G loss: 1.037839]\n",
      "epoch:29 step:27274 [D loss: 0.278035, acc.: 91.41%] [G loss: 1.454514]\n",
      "epoch:29 step:27275 [D loss: 0.670154, acc.: 60.16%] [G loss: 1.673549]\n",
      "epoch:29 step:27276 [D loss: 0.545053, acc.: 71.88%] [G loss: 1.356291]\n",
      "epoch:29 step:27277 [D loss: 0.641006, acc.: 61.72%] [G loss: 1.192706]\n",
      "epoch:29 step:27278 [D loss: 0.354733, acc.: 86.72%] [G loss: 1.768507]\n",
      "epoch:29 step:27279 [D loss: 0.575820, acc.: 71.09%] [G loss: 1.465269]\n",
      "epoch:29 step:27280 [D loss: 0.545809, acc.: 71.09%] [G loss: 1.156302]\n",
      "epoch:29 step:27281 [D loss: 0.439345, acc.: 81.25%] [G loss: 1.537585]\n",
      "epoch:29 step:27282 [D loss: 0.553854, acc.: 71.88%] [G loss: 1.488102]\n",
      "epoch:29 step:27283 [D loss: 0.591163, acc.: 68.75%] [G loss: 1.292089]\n",
      "epoch:29 step:27284 [D loss: 0.609306, acc.: 66.41%] [G loss: 1.329640]\n",
      "epoch:29 step:27285 [D loss: 0.613155, acc.: 66.41%] [G loss: 1.542900]\n",
      "epoch:29 step:27286 [D loss: 0.419235, acc.: 82.03%] [G loss: 1.712622]\n",
      "epoch:29 step:27287 [D loss: 0.633997, acc.: 68.75%] [G loss: 1.276345]\n",
      "epoch:29 step:27288 [D loss: 0.409220, acc.: 81.25%] [G loss: 1.779691]\n",
      "epoch:29 step:27289 [D loss: 0.615210, acc.: 69.53%] [G loss: 1.928671]\n",
      "epoch:29 step:27290 [D loss: 0.576147, acc.: 70.31%] [G loss: 1.429066]\n",
      "epoch:29 step:27291 [D loss: 0.660906, acc.: 66.41%] [G loss: 1.708170]\n",
      "epoch:29 step:27292 [D loss: 0.489886, acc.: 77.34%] [G loss: 1.849855]\n",
      "epoch:29 step:27293 [D loss: 0.481096, acc.: 78.12%] [G loss: 1.743428]\n",
      "epoch:29 step:27294 [D loss: 0.600317, acc.: 70.31%] [G loss: 1.427701]\n",
      "epoch:29 step:27295 [D loss: 0.584924, acc.: 66.41%] [G loss: 1.419974]\n",
      "epoch:29 step:27296 [D loss: 0.517156, acc.: 75.00%] [G loss: 1.139885]\n",
      "epoch:29 step:27297 [D loss: 0.476505, acc.: 78.12%] [G loss: 1.734248]\n",
      "epoch:29 step:27298 [D loss: 0.520386, acc.: 75.78%] [G loss: 1.181845]\n",
      "epoch:29 step:27299 [D loss: 0.549025, acc.: 75.00%] [G loss: 1.237229]\n",
      "epoch:29 step:27300 [D loss: 0.605905, acc.: 70.31%] [G loss: 1.292173]\n",
      "epoch:29 step:27301 [D loss: 0.598363, acc.: 69.53%] [G loss: 1.289972]\n",
      "epoch:29 step:27302 [D loss: 0.575425, acc.: 71.09%] [G loss: 1.256073]\n",
      "epoch:29 step:27303 [D loss: 0.432724, acc.: 77.34%] [G loss: 1.632971]\n",
      "epoch:29 step:27304 [D loss: 0.580325, acc.: 67.19%] [G loss: 1.529773]\n",
      "epoch:29 step:27305 [D loss: 0.444243, acc.: 84.38%] [G loss: 1.544796]\n",
      "epoch:29 step:27306 [D loss: 0.497662, acc.: 75.78%] [G loss: 1.243196]\n",
      "epoch:29 step:27307 [D loss: 0.701833, acc.: 57.03%] [G loss: 1.173225]\n",
      "epoch:29 step:27308 [D loss: 0.399362, acc.: 86.72%] [G loss: 1.116217]\n",
      "epoch:29 step:27309 [D loss: 0.457803, acc.: 78.91%] [G loss: 1.471203]\n",
      "epoch:29 step:27310 [D loss: 0.479373, acc.: 75.00%] [G loss: 1.458151]\n",
      "epoch:29 step:27311 [D loss: 0.691439, acc.: 57.81%] [G loss: 0.986158]\n",
      "epoch:29 step:27312 [D loss: 0.490286, acc.: 78.91%] [G loss: 0.820124]\n",
      "epoch:29 step:27313 [D loss: 0.574338, acc.: 69.53%] [G loss: 1.573974]\n",
      "epoch:29 step:27314 [D loss: 0.555814, acc.: 71.09%] [G loss: 1.392626]\n",
      "epoch:29 step:27315 [D loss: 0.319094, acc.: 90.62%] [G loss: 1.644376]\n",
      "epoch:29 step:27316 [D loss: 0.447153, acc.: 83.59%] [G loss: 1.448031]\n",
      "epoch:29 step:27317 [D loss: 0.494892, acc.: 79.69%] [G loss: 1.304193]\n",
      "epoch:29 step:27318 [D loss: 0.464580, acc.: 78.91%] [G loss: 1.718180]\n",
      "epoch:29 step:27319 [D loss: 0.675014, acc.: 60.94%] [G loss: 1.408397]\n",
      "epoch:29 step:27320 [D loss: 0.514031, acc.: 75.00%] [G loss: 1.332785]\n",
      "epoch:29 step:27321 [D loss: 0.631208, acc.: 68.75%] [G loss: 1.374866]\n",
      "epoch:29 step:27322 [D loss: 0.480978, acc.: 78.91%] [G loss: 1.690441]\n",
      "epoch:29 step:27323 [D loss: 0.603068, acc.: 68.75%] [G loss: 1.148523]\n",
      "epoch:29 step:27324 [D loss: 0.415870, acc.: 83.59%] [G loss: 1.522422]\n",
      "epoch:29 step:27325 [D loss: 0.641765, acc.: 67.19%] [G loss: 1.178048]\n",
      "epoch:29 step:27326 [D loss: 0.475349, acc.: 82.03%] [G loss: 1.757841]\n",
      "epoch:29 step:27327 [D loss: 0.641109, acc.: 70.31%] [G loss: 1.419960]\n",
      "epoch:29 step:27328 [D loss: 0.557123, acc.: 70.31%] [G loss: 1.715496]\n",
      "epoch:29 step:27329 [D loss: 0.413494, acc.: 78.91%] [G loss: 1.586542]\n",
      "epoch:29 step:27330 [D loss: 0.676873, acc.: 54.69%] [G loss: 1.303853]\n",
      "epoch:29 step:27331 [D loss: 0.534186, acc.: 76.56%] [G loss: 1.462116]\n",
      "epoch:29 step:27332 [D loss: 0.486883, acc.: 74.22%] [G loss: 1.206261]\n",
      "epoch:29 step:27333 [D loss: 0.550531, acc.: 71.88%] [G loss: 0.952651]\n",
      "epoch:29 step:27334 [D loss: 0.459060, acc.: 76.56%] [G loss: 1.555914]\n",
      "epoch:29 step:27335 [D loss: 0.590596, acc.: 68.75%] [G loss: 1.274489]\n",
      "epoch:29 step:27336 [D loss: 0.592185, acc.: 67.19%] [G loss: 1.557083]\n",
      "epoch:29 step:27337 [D loss: 0.533758, acc.: 74.22%] [G loss: 1.308208]\n",
      "epoch:29 step:27338 [D loss: 0.621487, acc.: 66.41%] [G loss: 1.553308]\n",
      "epoch:29 step:27339 [D loss: 0.469793, acc.: 78.12%] [G loss: 1.775248]\n",
      "epoch:29 step:27340 [D loss: 0.448388, acc.: 82.03%] [G loss: 1.745613]\n",
      "epoch:29 step:27341 [D loss: 0.734242, acc.: 53.91%] [G loss: 1.358296]\n",
      "epoch:29 step:27342 [D loss: 0.471509, acc.: 80.47%] [G loss: 1.532294]\n",
      "epoch:29 step:27343 [D loss: 0.639431, acc.: 62.50%] [G loss: 1.100610]\n",
      "epoch:29 step:27344 [D loss: 0.682165, acc.: 60.94%] [G loss: 1.416686]\n",
      "epoch:29 step:27345 [D loss: 0.404949, acc.: 81.25%] [G loss: 1.551839]\n",
      "epoch:29 step:27346 [D loss: 0.839357, acc.: 46.09%] [G loss: 1.444554]\n",
      "epoch:29 step:27347 [D loss: 0.691016, acc.: 60.16%] [G loss: 1.263915]\n",
      "epoch:29 step:27348 [D loss: 0.697954, acc.: 58.59%] [G loss: 1.319059]\n",
      "epoch:29 step:27349 [D loss: 0.560603, acc.: 70.31%] [G loss: 1.468347]\n",
      "epoch:29 step:27350 [D loss: 0.531224, acc.: 75.00%] [G loss: 1.364758]\n",
      "epoch:29 step:27351 [D loss: 0.381807, acc.: 85.16%] [G loss: 1.247506]\n",
      "epoch:29 step:27352 [D loss: 0.470858, acc.: 78.91%] [G loss: 1.369972]\n",
      "epoch:29 step:27353 [D loss: 0.457537, acc.: 81.25%] [G loss: 1.727674]\n",
      "epoch:29 step:27354 [D loss: 0.560427, acc.: 70.31%] [G loss: 1.355868]\n",
      "epoch:29 step:27355 [D loss: 0.552842, acc.: 73.44%] [G loss: 1.312418]\n",
      "epoch:29 step:27356 [D loss: 0.502865, acc.: 77.34%] [G loss: 1.911669]\n",
      "epoch:29 step:27357 [D loss: 0.769104, acc.: 52.34%] [G loss: 1.201268]\n",
      "epoch:29 step:27358 [D loss: 0.573314, acc.: 68.75%] [G loss: 1.299172]\n",
      "epoch:29 step:27359 [D loss: 0.383627, acc.: 87.50%] [G loss: 1.608937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27360 [D loss: 0.537184, acc.: 75.00%] [G loss: 1.324231]\n",
      "epoch:29 step:27361 [D loss: 0.589034, acc.: 75.00%] [G loss: 1.711692]\n",
      "epoch:29 step:27362 [D loss: 0.722287, acc.: 54.69%] [G loss: 1.094873]\n",
      "epoch:29 step:27363 [D loss: 0.538772, acc.: 72.66%] [G loss: 1.483758]\n",
      "epoch:29 step:27364 [D loss: 0.396069, acc.: 84.38%] [G loss: 1.914040]\n",
      "epoch:29 step:27365 [D loss: 0.665361, acc.: 58.59%] [G loss: 1.097017]\n",
      "epoch:29 step:27366 [D loss: 0.466488, acc.: 74.22%] [G loss: 1.140523]\n",
      "epoch:29 step:27367 [D loss: 0.543529, acc.: 73.44%] [G loss: 1.451488]\n",
      "epoch:29 step:27368 [D loss: 0.549030, acc.: 71.09%] [G loss: 1.326614]\n",
      "epoch:29 step:27369 [D loss: 0.531523, acc.: 73.44%] [G loss: 1.247111]\n",
      "epoch:29 step:27370 [D loss: 0.559894, acc.: 73.44%] [G loss: 1.260504]\n",
      "epoch:29 step:27371 [D loss: 0.577681, acc.: 67.97%] [G loss: 1.438791]\n",
      "epoch:29 step:27372 [D loss: 0.556698, acc.: 71.88%] [G loss: 1.520316]\n",
      "epoch:29 step:27373 [D loss: 0.542533, acc.: 71.88%] [G loss: 1.647361]\n",
      "epoch:29 step:27374 [D loss: 0.442824, acc.: 81.25%] [G loss: 1.440449]\n",
      "epoch:29 step:27375 [D loss: 0.476906, acc.: 80.47%] [G loss: 1.181504]\n",
      "epoch:29 step:27376 [D loss: 0.712982, acc.: 59.38%] [G loss: 1.439418]\n",
      "epoch:29 step:27377 [D loss: 0.383032, acc.: 85.94%] [G loss: 1.474517]\n",
      "epoch:29 step:27378 [D loss: 0.654115, acc.: 64.84%] [G loss: 1.540595]\n",
      "epoch:29 step:27379 [D loss: 0.509113, acc.: 77.34%] [G loss: 1.529952]\n",
      "epoch:29 step:27380 [D loss: 0.603556, acc.: 70.31%] [G loss: 1.431009]\n",
      "epoch:29 step:27381 [D loss: 0.458977, acc.: 78.91%] [G loss: 1.283628]\n",
      "epoch:29 step:27382 [D loss: 0.780858, acc.: 53.12%] [G loss: 1.450435]\n",
      "epoch:29 step:27383 [D loss: 0.455795, acc.: 84.38%] [G loss: 1.540544]\n",
      "epoch:29 step:27384 [D loss: 0.470053, acc.: 78.12%] [G loss: 1.210358]\n",
      "epoch:29 step:27385 [D loss: 0.561619, acc.: 72.66%] [G loss: 1.302899]\n",
      "epoch:29 step:27386 [D loss: 0.583520, acc.: 69.53%] [G loss: 1.691575]\n",
      "epoch:29 step:27387 [D loss: 0.670908, acc.: 60.16%] [G loss: 2.030337]\n",
      "epoch:29 step:27388 [D loss: 0.646048, acc.: 63.28%] [G loss: 1.341843]\n",
      "epoch:29 step:27389 [D loss: 0.599247, acc.: 64.06%] [G loss: 1.503469]\n",
      "epoch:29 step:27390 [D loss: 0.451357, acc.: 75.78%] [G loss: 1.464671]\n",
      "epoch:29 step:27391 [D loss: 0.812130, acc.: 50.78%] [G loss: 1.453351]\n",
      "epoch:29 step:27392 [D loss: 0.466275, acc.: 78.91%] [G loss: 1.421585]\n",
      "epoch:29 step:27393 [D loss: 0.503286, acc.: 78.12%] [G loss: 1.570773]\n",
      "epoch:29 step:27394 [D loss: 0.654061, acc.: 65.62%] [G loss: 1.368395]\n",
      "epoch:29 step:27395 [D loss: 0.529595, acc.: 77.34%] [G loss: 1.573384]\n",
      "epoch:29 step:27396 [D loss: 0.504679, acc.: 80.47%] [G loss: 1.000197]\n",
      "epoch:29 step:27397 [D loss: 0.565436, acc.: 76.56%] [G loss: 1.437537]\n",
      "epoch:29 step:27398 [D loss: 0.653656, acc.: 59.38%] [G loss: 1.660122]\n",
      "epoch:29 step:27399 [D loss: 0.814211, acc.: 51.56%] [G loss: 1.224986]\n",
      "epoch:29 step:27400 [D loss: 0.464828, acc.: 73.44%] [G loss: 1.640551]\n",
      "##############\n",
      "[2.63652765 1.89134252 1.82498111 2.89226597 0.49486805 5.83512321\n",
      " 2.29439377 2.52290767 3.82373037 7.14868929]\n",
      "##########\n",
      "epoch:29 step:27401 [D loss: 0.615271, acc.: 67.19%] [G loss: 1.266940]\n",
      "epoch:29 step:27402 [D loss: 0.484271, acc.: 78.91%] [G loss: 1.199220]\n",
      "epoch:29 step:27403 [D loss: 0.596005, acc.: 65.62%] [G loss: 1.713252]\n",
      "epoch:29 step:27404 [D loss: 0.443860, acc.: 80.47%] [G loss: 1.186252]\n",
      "epoch:29 step:27405 [D loss: 0.554024, acc.: 67.97%] [G loss: 1.322253]\n",
      "epoch:29 step:27406 [D loss: 0.613559, acc.: 59.38%] [G loss: 1.477416]\n",
      "epoch:29 step:27407 [D loss: 0.576458, acc.: 69.53%] [G loss: 1.556144]\n",
      "epoch:29 step:27408 [D loss: 0.703067, acc.: 59.38%] [G loss: 1.653176]\n",
      "epoch:29 step:27409 [D loss: 0.493096, acc.: 74.22%] [G loss: 1.756867]\n",
      "epoch:29 step:27410 [D loss: 0.571234, acc.: 66.41%] [G loss: 1.442140]\n",
      "epoch:29 step:27411 [D loss: 0.525870, acc.: 75.78%] [G loss: 1.164614]\n",
      "epoch:29 step:27412 [D loss: 0.459134, acc.: 78.91%] [G loss: 1.507209]\n",
      "epoch:29 step:27413 [D loss: 0.573022, acc.: 69.53%] [G loss: 1.170441]\n",
      "epoch:29 step:27414 [D loss: 0.584864, acc.: 67.19%] [G loss: 1.289065]\n",
      "epoch:29 step:27415 [D loss: 0.722672, acc.: 57.81%] [G loss: 1.258621]\n",
      "epoch:29 step:27416 [D loss: 0.494434, acc.: 75.78%] [G loss: 1.197196]\n",
      "epoch:29 step:27417 [D loss: 0.701654, acc.: 60.94%] [G loss: 1.488033]\n",
      "epoch:29 step:27418 [D loss: 0.490872, acc.: 74.22%] [G loss: 1.402330]\n",
      "epoch:29 step:27419 [D loss: 0.494456, acc.: 79.69%] [G loss: 1.730084]\n",
      "epoch:29 step:27420 [D loss: 0.926726, acc.: 43.75%] [G loss: 0.903929]\n",
      "epoch:29 step:27421 [D loss: 0.604345, acc.: 64.84%] [G loss: 1.115534]\n",
      "epoch:29 step:27422 [D loss: 0.341041, acc.: 88.28%] [G loss: 1.647296]\n",
      "epoch:29 step:27423 [D loss: 0.485434, acc.: 75.78%] [G loss: 1.891829]\n",
      "epoch:29 step:27424 [D loss: 0.487114, acc.: 76.56%] [G loss: 1.497011]\n",
      "epoch:29 step:27425 [D loss: 0.659764, acc.: 66.41%] [G loss: 1.129723]\n",
      "epoch:29 step:27426 [D loss: 0.549290, acc.: 71.09%] [G loss: 1.161930]\n",
      "epoch:29 step:27427 [D loss: 0.556747, acc.: 70.31%] [G loss: 1.026976]\n",
      "epoch:29 step:27428 [D loss: 0.705149, acc.: 57.81%] [G loss: 1.120201]\n",
      "epoch:29 step:27429 [D loss: 0.607600, acc.: 66.41%] [G loss: 1.441229]\n",
      "epoch:29 step:27430 [D loss: 0.550087, acc.: 71.09%] [G loss: 1.233788]\n",
      "epoch:29 step:27431 [D loss: 0.575617, acc.: 64.84%] [G loss: 1.319978]\n",
      "epoch:29 step:27432 [D loss: 0.421547, acc.: 82.03%] [G loss: 1.174000]\n",
      "epoch:29 step:27433 [D loss: 0.648640, acc.: 64.84%] [G loss: 1.474123]\n",
      "epoch:29 step:27434 [D loss: 0.446520, acc.: 82.81%] [G loss: 1.506055]\n",
      "epoch:29 step:27435 [D loss: 0.776633, acc.: 54.69%] [G loss: 1.012816]\n",
      "epoch:29 step:27436 [D loss: 0.650938, acc.: 66.41%] [G loss: 1.458217]\n",
      "epoch:29 step:27437 [D loss: 0.382945, acc.: 87.50%] [G loss: 1.457500]\n",
      "epoch:29 step:27438 [D loss: 0.485127, acc.: 76.56%] [G loss: 1.803681]\n",
      "epoch:29 step:27439 [D loss: 0.515093, acc.: 78.12%] [G loss: 1.421170]\n",
      "epoch:29 step:27440 [D loss: 0.683215, acc.: 64.06%] [G loss: 1.218309]\n",
      "epoch:29 step:27441 [D loss: 0.414980, acc.: 80.47%] [G loss: 1.306163]\n",
      "epoch:29 step:27442 [D loss: 0.543256, acc.: 72.66%] [G loss: 1.207272]\n",
      "epoch:29 step:27443 [D loss: 0.528140, acc.: 74.22%] [G loss: 1.347082]\n",
      "epoch:29 step:27444 [D loss: 0.488814, acc.: 75.00%] [G loss: 1.018440]\n",
      "epoch:29 step:27445 [D loss: 0.450450, acc.: 81.25%] [G loss: 1.506289]\n",
      "epoch:29 step:27446 [D loss: 0.572080, acc.: 72.66%] [G loss: 1.238796]\n",
      "epoch:29 step:27447 [D loss: 0.495587, acc.: 77.34%] [G loss: 1.307325]\n",
      "epoch:29 step:27448 [D loss: 0.676920, acc.: 61.72%] [G loss: 1.303022]\n",
      "epoch:29 step:27449 [D loss: 0.568626, acc.: 71.09%] [G loss: 1.257427]\n",
      "epoch:29 step:27450 [D loss: 0.533282, acc.: 74.22%] [G loss: 1.690237]\n",
      "epoch:29 step:27451 [D loss: 0.506288, acc.: 77.34%] [G loss: 1.731296]\n",
      "epoch:29 step:27452 [D loss: 0.519383, acc.: 72.66%] [G loss: 1.541574]\n",
      "epoch:29 step:27453 [D loss: 0.768102, acc.: 53.91%] [G loss: 1.221797]\n",
      "epoch:29 step:27454 [D loss: 0.539319, acc.: 72.66%] [G loss: 1.319853]\n",
      "epoch:29 step:27455 [D loss: 0.518164, acc.: 76.56%] [G loss: 1.666748]\n",
      "epoch:29 step:27456 [D loss: 0.385001, acc.: 88.28%] [G loss: 1.430389]\n",
      "epoch:29 step:27457 [D loss: 0.583431, acc.: 68.75%] [G loss: 1.424834]\n",
      "epoch:29 step:27458 [D loss: 0.584457, acc.: 69.53%] [G loss: 1.492843]\n",
      "epoch:29 step:27459 [D loss: 0.647521, acc.: 63.28%] [G loss: 1.370888]\n",
      "epoch:29 step:27460 [D loss: 0.406080, acc.: 84.38%] [G loss: 1.718139]\n",
      "epoch:29 step:27461 [D loss: 0.816321, acc.: 50.00%] [G loss: 1.539423]\n",
      "epoch:29 step:27462 [D loss: 0.550497, acc.: 69.53%] [G loss: 1.455987]\n",
      "epoch:29 step:27463 [D loss: 0.610268, acc.: 67.19%] [G loss: 1.163774]\n",
      "epoch:29 step:27464 [D loss: 0.636379, acc.: 65.62%] [G loss: 0.945622]\n",
      "epoch:29 step:27465 [D loss: 0.459605, acc.: 81.25%] [G loss: 1.291213]\n",
      "epoch:29 step:27466 [D loss: 0.641460, acc.: 63.28%] [G loss: 1.274412]\n",
      "epoch:29 step:27467 [D loss: 0.556255, acc.: 75.00%] [G loss: 1.147597]\n",
      "epoch:29 step:27468 [D loss: 0.732015, acc.: 54.69%] [G loss: 1.551336]\n",
      "epoch:29 step:27469 [D loss: 0.599496, acc.: 68.75%] [G loss: 1.515352]\n",
      "epoch:29 step:27470 [D loss: 0.588819, acc.: 69.53%] [G loss: 1.245303]\n",
      "epoch:29 step:27471 [D loss: 0.647342, acc.: 60.94%] [G loss: 1.397578]\n",
      "epoch:29 step:27472 [D loss: 0.631347, acc.: 65.62%] [G loss: 1.528502]\n",
      "epoch:29 step:27473 [D loss: 0.423578, acc.: 81.25%] [G loss: 1.714275]\n",
      "epoch:29 step:27474 [D loss: 0.596984, acc.: 64.84%] [G loss: 1.171884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27475 [D loss: 0.598007, acc.: 69.53%] [G loss: 1.407747]\n",
      "epoch:29 step:27476 [D loss: 0.774570, acc.: 53.12%] [G loss: 1.264108]\n",
      "epoch:29 step:27477 [D loss: 0.552615, acc.: 71.88%] [G loss: 1.294657]\n",
      "epoch:29 step:27478 [D loss: 0.583220, acc.: 71.09%] [G loss: 1.400957]\n",
      "epoch:29 step:27479 [D loss: 0.419921, acc.: 84.38%] [G loss: 1.763138]\n",
      "epoch:29 step:27480 [D loss: 0.581364, acc.: 70.31%] [G loss: 1.340068]\n",
      "epoch:29 step:27481 [D loss: 0.564062, acc.: 71.09%] [G loss: 1.336923]\n",
      "epoch:29 step:27482 [D loss: 0.352253, acc.: 88.28%] [G loss: 1.829362]\n",
      "epoch:29 step:27483 [D loss: 0.563298, acc.: 71.88%] [G loss: 1.287928]\n",
      "epoch:29 step:27484 [D loss: 0.364723, acc.: 88.28%] [G loss: 2.347929]\n",
      "epoch:29 step:27485 [D loss: 0.597158, acc.: 69.53%] [G loss: 1.171479]\n",
      "epoch:29 step:27486 [D loss: 0.480888, acc.: 78.91%] [G loss: 1.603129]\n",
      "epoch:29 step:27487 [D loss: 0.444298, acc.: 78.12%] [G loss: 1.346478]\n",
      "epoch:29 step:27488 [D loss: 0.539182, acc.: 72.66%] [G loss: 1.379736]\n",
      "epoch:29 step:27489 [D loss: 0.582739, acc.: 67.19%] [G loss: 1.432427]\n",
      "epoch:29 step:27490 [D loss: 0.698458, acc.: 60.16%] [G loss: 1.121986]\n",
      "epoch:29 step:27491 [D loss: 0.364347, acc.: 86.72%] [G loss: 1.505574]\n",
      "epoch:29 step:27492 [D loss: 0.709444, acc.: 57.81%] [G loss: 1.294395]\n",
      "epoch:29 step:27493 [D loss: 0.596300, acc.: 68.75%] [G loss: 1.249251]\n",
      "epoch:29 step:27494 [D loss: 0.533966, acc.: 72.66%] [G loss: 1.315373]\n",
      "epoch:29 step:27495 [D loss: 0.482909, acc.: 73.44%] [G loss: 1.394452]\n",
      "epoch:29 step:27496 [D loss: 0.609108, acc.: 67.97%] [G loss: 1.322714]\n",
      "epoch:29 step:27497 [D loss: 0.525318, acc.: 74.22%] [G loss: 1.486490]\n",
      "epoch:29 step:27498 [D loss: 0.522998, acc.: 71.09%] [G loss: 1.425086]\n",
      "epoch:29 step:27499 [D loss: 0.508193, acc.: 74.22%] [G loss: 1.167891]\n",
      "epoch:29 step:27500 [D loss: 0.438543, acc.: 80.47%] [G loss: 1.125216]\n",
      "epoch:29 step:27501 [D loss: 0.509882, acc.: 75.78%] [G loss: 1.208354]\n",
      "epoch:29 step:27502 [D loss: 0.542254, acc.: 71.88%] [G loss: 1.685899]\n",
      "epoch:29 step:27503 [D loss: 0.521699, acc.: 75.78%] [G loss: 1.373413]\n",
      "epoch:29 step:27504 [D loss: 0.535508, acc.: 80.47%] [G loss: 1.566616]\n",
      "epoch:29 step:27505 [D loss: 0.443906, acc.: 80.47%] [G loss: 1.526269]\n",
      "epoch:29 step:27506 [D loss: 0.783221, acc.: 53.91%] [G loss: 1.343042]\n",
      "epoch:29 step:27507 [D loss: 0.476688, acc.: 82.81%] [G loss: 2.033709]\n",
      "epoch:29 step:27508 [D loss: 0.482589, acc.: 78.12%] [G loss: 1.415763]\n",
      "epoch:29 step:27509 [D loss: 0.671673, acc.: 66.41%] [G loss: 1.687252]\n",
      "epoch:29 step:27510 [D loss: 0.539597, acc.: 71.09%] [G loss: 1.802541]\n",
      "epoch:29 step:27511 [D loss: 0.524801, acc.: 74.22%] [G loss: 1.283370]\n",
      "epoch:29 step:27512 [D loss: 0.487789, acc.: 77.34%] [G loss: 1.151725]\n",
      "epoch:29 step:27513 [D loss: 0.447748, acc.: 76.56%] [G loss: 1.375516]\n",
      "epoch:29 step:27514 [D loss: 0.699536, acc.: 61.72%] [G loss: 1.439234]\n",
      "epoch:29 step:27515 [D loss: 0.633303, acc.: 67.97%] [G loss: 1.672463]\n",
      "epoch:29 step:27516 [D loss: 0.540232, acc.: 72.66%] [G loss: 1.214473]\n",
      "epoch:29 step:27517 [D loss: 0.447264, acc.: 81.25%] [G loss: 1.128930]\n",
      "epoch:29 step:27518 [D loss: 0.609946, acc.: 71.09%] [G loss: 1.246481]\n",
      "epoch:29 step:27519 [D loss: 0.536692, acc.: 71.09%] [G loss: 0.885365]\n",
      "epoch:29 step:27520 [D loss: 0.658766, acc.: 57.81%] [G loss: 1.384653]\n",
      "epoch:29 step:27521 [D loss: 0.664813, acc.: 61.72%] [G loss: 1.402333]\n",
      "epoch:29 step:27522 [D loss: 0.566899, acc.: 73.44%] [G loss: 1.423589]\n",
      "epoch:29 step:27523 [D loss: 0.343283, acc.: 90.62%] [G loss: 1.277837]\n",
      "epoch:29 step:27524 [D loss: 0.632639, acc.: 62.50%] [G loss: 1.525284]\n",
      "epoch:29 step:27525 [D loss: 0.524554, acc.: 74.22%] [G loss: 1.086129]\n",
      "epoch:29 step:27526 [D loss: 0.524710, acc.: 75.00%] [G loss: 1.027739]\n",
      "epoch:29 step:27527 [D loss: 0.593124, acc.: 65.62%] [G loss: 1.386426]\n",
      "epoch:29 step:27528 [D loss: 0.551370, acc.: 70.31%] [G loss: 1.364176]\n",
      "epoch:29 step:27529 [D loss: 0.499528, acc.: 71.09%] [G loss: 1.655772]\n",
      "epoch:29 step:27530 [D loss: 0.497185, acc.: 73.44%] [G loss: 1.291021]\n",
      "epoch:29 step:27531 [D loss: 0.624138, acc.: 60.16%] [G loss: 1.526275]\n",
      "epoch:29 step:27532 [D loss: 0.624091, acc.: 64.84%] [G loss: 1.312057]\n",
      "epoch:29 step:27533 [D loss: 0.656051, acc.: 64.84%] [G loss: 1.331905]\n",
      "epoch:29 step:27534 [D loss: 0.543900, acc.: 71.09%] [G loss: 2.008650]\n",
      "epoch:29 step:27535 [D loss: 0.572803, acc.: 71.09%] [G loss: 1.397651]\n",
      "epoch:29 step:27536 [D loss: 0.479938, acc.: 79.69%] [G loss: 1.172920]\n",
      "epoch:29 step:27537 [D loss: 0.599504, acc.: 68.75%] [G loss: 1.687125]\n",
      "epoch:29 step:27538 [D loss: 0.552374, acc.: 69.53%] [G loss: 1.025891]\n",
      "epoch:29 step:27539 [D loss: 0.604202, acc.: 64.84%] [G loss: 1.760481]\n",
      "epoch:29 step:27540 [D loss: 0.666153, acc.: 62.50%] [G loss: 1.314746]\n",
      "epoch:29 step:27541 [D loss: 0.389337, acc.: 87.50%] [G loss: 1.325272]\n",
      "epoch:29 step:27542 [D loss: 0.649414, acc.: 69.53%] [G loss: 1.530173]\n",
      "epoch:29 step:27543 [D loss: 0.718726, acc.: 60.16%] [G loss: 1.299823]\n",
      "epoch:29 step:27544 [D loss: 0.489379, acc.: 79.69%] [G loss: 1.593295]\n",
      "epoch:29 step:27545 [D loss: 0.571630, acc.: 65.62%] [G loss: 1.904031]\n",
      "epoch:29 step:27546 [D loss: 0.490988, acc.: 75.00%] [G loss: 1.218764]\n",
      "epoch:29 step:27547 [D loss: 0.554204, acc.: 72.66%] [G loss: 1.193690]\n",
      "epoch:29 step:27548 [D loss: 0.582718, acc.: 72.66%] [G loss: 1.385162]\n",
      "epoch:29 step:27549 [D loss: 0.699650, acc.: 57.81%] [G loss: 0.862088]\n",
      "epoch:29 step:27550 [D loss: 0.460340, acc.: 81.25%] [G loss: 1.188358]\n",
      "epoch:29 step:27551 [D loss: 0.476742, acc.: 76.56%] [G loss: 1.399830]\n",
      "epoch:29 step:27552 [D loss: 0.565621, acc.: 71.09%] [G loss: 1.687049]\n",
      "epoch:29 step:27553 [D loss: 0.579149, acc.: 64.84%] [G loss: 1.447524]\n",
      "epoch:29 step:27554 [D loss: 0.441425, acc.: 80.47%] [G loss: 1.330798]\n",
      "epoch:29 step:27555 [D loss: 0.586072, acc.: 70.31%] [G loss: 1.646454]\n",
      "epoch:29 step:27556 [D loss: 0.707502, acc.: 56.25%] [G loss: 1.241688]\n",
      "epoch:29 step:27557 [D loss: 0.647448, acc.: 59.38%] [G loss: 1.318830]\n",
      "epoch:29 step:27558 [D loss: 0.428824, acc.: 81.25%] [G loss: 1.842468]\n",
      "epoch:29 step:27559 [D loss: 0.506949, acc.: 81.25%] [G loss: 1.356377]\n",
      "epoch:29 step:27560 [D loss: 0.659805, acc.: 65.62%] [G loss: 1.665290]\n",
      "epoch:29 step:27561 [D loss: 0.712847, acc.: 59.38%] [G loss: 1.427723]\n",
      "epoch:29 step:27562 [D loss: 0.503189, acc.: 76.56%] [G loss: 1.664992]\n",
      "epoch:29 step:27563 [D loss: 0.514519, acc.: 72.66%] [G loss: 1.526759]\n",
      "epoch:29 step:27564 [D loss: 0.652635, acc.: 64.84%] [G loss: 1.130969]\n",
      "epoch:29 step:27565 [D loss: 0.546072, acc.: 69.53%] [G loss: 1.108267]\n",
      "epoch:29 step:27566 [D loss: 0.525013, acc.: 75.78%] [G loss: 1.288279]\n",
      "epoch:29 step:27567 [D loss: 0.568802, acc.: 70.31%] [G loss: 1.142900]\n",
      "epoch:29 step:27568 [D loss: 0.386779, acc.: 83.59%] [G loss: 1.338803]\n",
      "epoch:29 step:27569 [D loss: 0.659692, acc.: 59.38%] [G loss: 1.211312]\n",
      "epoch:29 step:27570 [D loss: 0.622312, acc.: 64.06%] [G loss: 1.416646]\n",
      "epoch:29 step:27571 [D loss: 0.528483, acc.: 71.88%] [G loss: 1.108608]\n",
      "epoch:29 step:27572 [D loss: 0.610119, acc.: 71.09%] [G loss: 1.384217]\n",
      "epoch:29 step:27573 [D loss: 0.507132, acc.: 78.12%] [G loss: 1.430692]\n",
      "epoch:29 step:27574 [D loss: 0.700840, acc.: 57.81%] [G loss: 1.320235]\n",
      "epoch:29 step:27575 [D loss: 0.466402, acc.: 80.47%] [G loss: 1.820869]\n",
      "epoch:29 step:27576 [D loss: 0.615685, acc.: 67.97%] [G loss: 1.589727]\n",
      "epoch:29 step:27577 [D loss: 0.455626, acc.: 81.25%] [G loss: 1.738719]\n",
      "epoch:29 step:27578 [D loss: 0.568301, acc.: 67.97%] [G loss: 1.484900]\n",
      "epoch:29 step:27579 [D loss: 0.438640, acc.: 78.91%] [G loss: 1.485647]\n",
      "epoch:29 step:27580 [D loss: 0.747105, acc.: 52.34%] [G loss: 1.178135]\n",
      "epoch:29 step:27581 [D loss: 0.539066, acc.: 71.09%] [G loss: 1.452053]\n",
      "epoch:29 step:27582 [D loss: 0.531616, acc.: 75.00%] [G loss: 1.259301]\n",
      "epoch:29 step:27583 [D loss: 0.548656, acc.: 71.88%] [G loss: 1.839004]\n",
      "epoch:29 step:27584 [D loss: 0.463876, acc.: 82.03%] [G loss: 1.835526]\n",
      "epoch:29 step:27585 [D loss: 0.460756, acc.: 79.69%] [G loss: 1.381249]\n",
      "epoch:29 step:27586 [D loss: 0.719021, acc.: 65.62%] [G loss: 1.250080]\n",
      "epoch:29 step:27587 [D loss: 0.651993, acc.: 67.97%] [G loss: 1.632640]\n",
      "epoch:29 step:27588 [D loss: 0.357292, acc.: 85.16%] [G loss: 1.664782]\n",
      "epoch:29 step:27589 [D loss: 0.547270, acc.: 71.88%] [G loss: 1.267894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27590 [D loss: 0.646160, acc.: 60.16%] [G loss: 1.375147]\n",
      "epoch:29 step:27591 [D loss: 0.517898, acc.: 71.88%] [G loss: 1.562068]\n",
      "epoch:29 step:27592 [D loss: 0.399777, acc.: 82.03%] [G loss: 1.126137]\n",
      "epoch:29 step:27593 [D loss: 0.548649, acc.: 72.66%] [G loss: 1.390164]\n",
      "epoch:29 step:27594 [D loss: 0.406074, acc.: 82.03%] [G loss: 1.901721]\n",
      "epoch:29 step:27595 [D loss: 0.649790, acc.: 63.28%] [G loss: 1.002280]\n",
      "epoch:29 step:27596 [D loss: 0.418652, acc.: 83.59%] [G loss: 1.696009]\n",
      "epoch:29 step:27597 [D loss: 0.474631, acc.: 80.47%] [G loss: 1.243755]\n",
      "epoch:29 step:27598 [D loss: 0.523503, acc.: 78.12%] [G loss: 1.646458]\n",
      "epoch:29 step:27599 [D loss: 0.597837, acc.: 67.97%] [G loss: 1.559138]\n",
      "epoch:29 step:27600 [D loss: 0.490815, acc.: 74.22%] [G loss: 1.458381]\n",
      "##############\n",
      "[2.73334763 2.0707091  2.08768427 2.46654358 0.9087486  6.83187566\n",
      " 2.23364417 2.94347508 3.87643399 3.96373263]\n",
      "##########\n",
      "epoch:29 step:27601 [D loss: 0.739206, acc.: 53.91%] [G loss: 1.128181]\n",
      "epoch:29 step:27602 [D loss: 0.630289, acc.: 68.75%] [G loss: 1.081412]\n",
      "epoch:29 step:27603 [D loss: 0.556502, acc.: 70.31%] [G loss: 1.340581]\n",
      "epoch:29 step:27604 [D loss: 0.476034, acc.: 79.69%] [G loss: 1.503775]\n",
      "epoch:29 step:27605 [D loss: 0.556606, acc.: 71.88%] [G loss: 1.529054]\n",
      "epoch:29 step:27606 [D loss: 0.647947, acc.: 62.50%] [G loss: 1.213366]\n",
      "epoch:29 step:27607 [D loss: 0.585709, acc.: 62.50%] [G loss: 1.365471]\n",
      "epoch:29 step:27608 [D loss: 0.402407, acc.: 84.38%] [G loss: 1.443434]\n",
      "epoch:29 step:27609 [D loss: 0.674788, acc.: 60.94%] [G loss: 1.072637]\n",
      "epoch:29 step:27610 [D loss: 0.492452, acc.: 78.12%] [G loss: 1.457687]\n",
      "epoch:29 step:27611 [D loss: 0.624591, acc.: 60.94%] [G loss: 1.234182]\n",
      "epoch:29 step:27612 [D loss: 0.417599, acc.: 81.25%] [G loss: 1.519256]\n",
      "epoch:29 step:27613 [D loss: 0.569945, acc.: 67.97%] [G loss: 0.928908]\n",
      "epoch:29 step:27614 [D loss: 0.459513, acc.: 78.91%] [G loss: 1.429114]\n",
      "epoch:29 step:27615 [D loss: 0.444990, acc.: 77.34%] [G loss: 1.786559]\n",
      "epoch:29 step:27616 [D loss: 0.644089, acc.: 60.94%] [G loss: 1.154469]\n",
      "epoch:29 step:27617 [D loss: 0.573186, acc.: 66.41%] [G loss: 1.448419]\n",
      "epoch:29 step:27618 [D loss: 0.614645, acc.: 67.19%] [G loss: 1.715963]\n",
      "epoch:29 step:27619 [D loss: 0.547372, acc.: 72.66%] [G loss: 1.881699]\n",
      "epoch:29 step:27620 [D loss: 0.501568, acc.: 73.44%] [G loss: 1.591480]\n",
      "epoch:29 step:27621 [D loss: 0.473363, acc.: 78.12%] [G loss: 1.672309]\n",
      "epoch:29 step:27622 [D loss: 0.589302, acc.: 64.84%] [G loss: 1.708279]\n",
      "epoch:29 step:27623 [D loss: 0.583715, acc.: 71.09%] [G loss: 1.465153]\n",
      "epoch:29 step:27624 [D loss: 0.365066, acc.: 85.94%] [G loss: 1.423140]\n",
      "epoch:29 step:27625 [D loss: 0.476654, acc.: 81.25%] [G loss: 1.363735]\n",
      "epoch:29 step:27626 [D loss: 0.559333, acc.: 71.09%] [G loss: 1.584810]\n",
      "epoch:29 step:27627 [D loss: 0.484336, acc.: 79.69%] [G loss: 1.557730]\n",
      "epoch:29 step:27628 [D loss: 0.604150, acc.: 67.97%] [G loss: 1.110304]\n",
      "epoch:29 step:27629 [D loss: 0.605214, acc.: 68.75%] [G loss: 1.266086]\n",
      "epoch:29 step:27630 [D loss: 0.517341, acc.: 73.44%] [G loss: 1.308702]\n",
      "epoch:29 step:27631 [D loss: 0.491881, acc.: 77.34%] [G loss: 1.492482]\n",
      "epoch:29 step:27632 [D loss: 0.476144, acc.: 78.91%] [G loss: 1.393065]\n",
      "epoch:29 step:27633 [D loss: 0.586730, acc.: 68.75%] [G loss: 1.776301]\n",
      "epoch:29 step:27634 [D loss: 0.752560, acc.: 50.78%] [G loss: 1.507689]\n",
      "epoch:29 step:27635 [D loss: 0.593565, acc.: 66.41%] [G loss: 1.387332]\n",
      "epoch:29 step:27636 [D loss: 0.504200, acc.: 73.44%] [G loss: 1.452667]\n",
      "epoch:29 step:27637 [D loss: 0.515062, acc.: 71.88%] [G loss: 1.346999]\n",
      "epoch:29 step:27638 [D loss: 0.568504, acc.: 71.88%] [G loss: 1.645583]\n",
      "epoch:29 step:27639 [D loss: 0.368464, acc.: 84.38%] [G loss: 1.447651]\n",
      "epoch:29 step:27640 [D loss: 0.740908, acc.: 53.12%] [G loss: 0.994413]\n",
      "epoch:29 step:27641 [D loss: 0.401135, acc.: 82.81%] [G loss: 1.684633]\n",
      "epoch:29 step:27642 [D loss: 0.518293, acc.: 73.44%] [G loss: 1.127167]\n",
      "epoch:29 step:27643 [D loss: 0.622091, acc.: 64.84%] [G loss: 1.094460]\n",
      "epoch:29 step:27644 [D loss: 0.579099, acc.: 68.75%] [G loss: 1.317471]\n",
      "epoch:29 step:27645 [D loss: 0.481672, acc.: 78.12%] [G loss: 1.286936]\n",
      "epoch:29 step:27646 [D loss: 0.429965, acc.: 81.25%] [G loss: 1.840185]\n",
      "epoch:29 step:27647 [D loss: 0.508732, acc.: 73.44%] [G loss: 2.064515]\n",
      "epoch:29 step:27648 [D loss: 0.619105, acc.: 67.19%] [G loss: 1.198097]\n",
      "epoch:29 step:27649 [D loss: 0.550496, acc.: 75.00%] [G loss: 1.279449]\n",
      "epoch:29 step:27650 [D loss: 0.567945, acc.: 67.97%] [G loss: 1.347750]\n",
      "epoch:29 step:27651 [D loss: 0.443619, acc.: 78.12%] [G loss: 1.434554]\n",
      "epoch:29 step:27652 [D loss: 0.550155, acc.: 72.66%] [G loss: 1.715554]\n",
      "epoch:29 step:27653 [D loss: 0.452471, acc.: 81.25%] [G loss: 1.337525]\n",
      "epoch:29 step:27654 [D loss: 0.604222, acc.: 62.50%] [G loss: 0.955461]\n",
      "epoch:29 step:27655 [D loss: 0.524343, acc.: 73.44%] [G loss: 1.401356]\n",
      "epoch:29 step:27656 [D loss: 0.779521, acc.: 53.91%] [G loss: 1.252622]\n",
      "epoch:29 step:27657 [D loss: 0.516164, acc.: 70.31%] [G loss: 1.877631]\n",
      "epoch:29 step:27658 [D loss: 0.473453, acc.: 77.34%] [G loss: 1.391269]\n",
      "epoch:29 step:27659 [D loss: 0.588776, acc.: 71.09%] [G loss: 1.730989]\n",
      "epoch:29 step:27660 [D loss: 0.510145, acc.: 74.22%] [G loss: 1.777617]\n",
      "epoch:29 step:27661 [D loss: 0.463399, acc.: 77.34%] [G loss: 1.604483]\n",
      "epoch:29 step:27662 [D loss: 0.475470, acc.: 77.34%] [G loss: 1.620566]\n",
      "epoch:29 step:27663 [D loss: 0.434378, acc.: 81.25%] [G loss: 1.635454]\n",
      "epoch:29 step:27664 [D loss: 0.615741, acc.: 72.66%] [G loss: 1.309413]\n",
      "epoch:29 step:27665 [D loss: 0.440119, acc.: 80.47%] [G loss: 1.292374]\n",
      "epoch:29 step:27666 [D loss: 0.551268, acc.: 75.00%] [G loss: 0.963004]\n",
      "epoch:29 step:27667 [D loss: 0.418552, acc.: 84.38%] [G loss: 1.561425]\n",
      "epoch:29 step:27668 [D loss: 0.512559, acc.: 75.78%] [G loss: 1.313896]\n",
      "epoch:29 step:27669 [D loss: 0.469339, acc.: 76.56%] [G loss: 1.645171]\n",
      "epoch:29 step:27670 [D loss: 0.681497, acc.: 57.81%] [G loss: 1.013756]\n",
      "epoch:29 step:27671 [D loss: 0.573562, acc.: 73.44%] [G loss: 1.295142]\n",
      "epoch:29 step:27672 [D loss: 0.506969, acc.: 75.00%] [G loss: 1.349473]\n",
      "epoch:29 step:27673 [D loss: 0.597039, acc.: 67.97%] [G loss: 1.291914]\n",
      "epoch:29 step:27674 [D loss: 0.415763, acc.: 82.81%] [G loss: 1.776870]\n",
      "epoch:29 step:27675 [D loss: 0.620200, acc.: 69.53%] [G loss: 1.394323]\n",
      "epoch:29 step:27676 [D loss: 0.511998, acc.: 79.69%] [G loss: 1.368740]\n",
      "epoch:29 step:27677 [D loss: 0.385378, acc.: 83.59%] [G loss: 1.222266]\n",
      "epoch:29 step:27678 [D loss: 0.329944, acc.: 85.94%] [G loss: 1.344008]\n",
      "epoch:29 step:27679 [D loss: 0.514621, acc.: 78.12%] [G loss: 1.534823]\n",
      "epoch:29 step:27680 [D loss: 0.643613, acc.: 64.06%] [G loss: 1.686111]\n",
      "epoch:29 step:27681 [D loss: 0.604784, acc.: 64.84%] [G loss: 1.626261]\n",
      "epoch:29 step:27682 [D loss: 0.439071, acc.: 80.47%] [G loss: 1.572167]\n",
      "epoch:29 step:27683 [D loss: 0.657191, acc.: 64.84%] [G loss: 1.610741]\n",
      "epoch:29 step:27684 [D loss: 0.402102, acc.: 82.81%] [G loss: 1.706286]\n",
      "epoch:29 step:27685 [D loss: 0.551833, acc.: 69.53%] [G loss: 1.736421]\n",
      "epoch:29 step:27686 [D loss: 0.394226, acc.: 83.59%] [G loss: 1.376908]\n",
      "epoch:29 step:27687 [D loss: 0.467490, acc.: 77.34%] [G loss: 1.264411]\n",
      "epoch:29 step:27688 [D loss: 0.578089, acc.: 69.53%] [G loss: 1.433438]\n",
      "epoch:29 step:27689 [D loss: 0.528680, acc.: 73.44%] [G loss: 1.150485]\n",
      "epoch:29 step:27690 [D loss: 0.492653, acc.: 77.34%] [G loss: 1.290065]\n",
      "epoch:29 step:27691 [D loss: 0.522379, acc.: 72.66%] [G loss: 1.583003]\n",
      "epoch:29 step:27692 [D loss: 0.409969, acc.: 84.38%] [G loss: 1.429965]\n",
      "epoch:29 step:27693 [D loss: 0.705027, acc.: 59.38%] [G loss: 1.365071]\n",
      "epoch:29 step:27694 [D loss: 0.475012, acc.: 73.44%] [G loss: 1.508725]\n",
      "epoch:29 step:27695 [D loss: 0.603407, acc.: 67.97%] [G loss: 1.252408]\n",
      "epoch:29 step:27696 [D loss: 0.544865, acc.: 73.44%] [G loss: 1.851627]\n",
      "epoch:29 step:27697 [D loss: 0.646801, acc.: 61.72%] [G loss: 1.400426]\n",
      "epoch:29 step:27698 [D loss: 0.553567, acc.: 73.44%] [G loss: 1.372730]\n",
      "epoch:29 step:27699 [D loss: 0.572440, acc.: 71.09%] [G loss: 1.079242]\n",
      "epoch:29 step:27700 [D loss: 0.369216, acc.: 82.81%] [G loss: 1.386583]\n",
      "epoch:29 step:27701 [D loss: 0.534074, acc.: 71.09%] [G loss: 1.192001]\n",
      "epoch:29 step:27702 [D loss: 0.521153, acc.: 75.00%] [G loss: 0.952284]\n",
      "epoch:29 step:27703 [D loss: 0.453067, acc.: 79.69%] [G loss: 1.419534]\n",
      "epoch:29 step:27704 [D loss: 0.489554, acc.: 78.91%] [G loss: 1.483683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27705 [D loss: 0.396265, acc.: 83.59%] [G loss: 1.529360]\n",
      "epoch:29 step:27706 [D loss: 0.425674, acc.: 77.34%] [G loss: 1.657130]\n",
      "epoch:29 step:27707 [D loss: 0.625961, acc.: 64.06%] [G loss: 1.368628]\n",
      "epoch:29 step:27708 [D loss: 0.584688, acc.: 75.78%] [G loss: 1.447264]\n",
      "epoch:29 step:27709 [D loss: 0.572770, acc.: 71.88%] [G loss: 1.745512]\n",
      "epoch:29 step:27710 [D loss: 0.538931, acc.: 72.66%] [G loss: 1.450412]\n",
      "epoch:29 step:27711 [D loss: 0.754586, acc.: 56.25%] [G loss: 1.163697]\n",
      "epoch:29 step:27712 [D loss: 0.657808, acc.: 61.72%] [G loss: 1.402118]\n",
      "epoch:29 step:27713 [D loss: 0.459235, acc.: 79.69%] [G loss: 1.474378]\n",
      "epoch:29 step:27714 [D loss: 0.454861, acc.: 82.81%] [G loss: 1.739989]\n",
      "epoch:29 step:27715 [D loss: 0.598564, acc.: 64.84%] [G loss: 1.722509]\n",
      "epoch:29 step:27716 [D loss: 0.349923, acc.: 89.84%] [G loss: 1.905707]\n",
      "epoch:29 step:27717 [D loss: 0.705949, acc.: 60.16%] [G loss: 1.576117]\n",
      "epoch:29 step:27718 [D loss: 0.436945, acc.: 84.38%] [G loss: 1.335706]\n",
      "epoch:29 step:27719 [D loss: 0.403137, acc.: 79.69%] [G loss: 1.444845]\n",
      "epoch:29 step:27720 [D loss: 0.434402, acc.: 85.16%] [G loss: 1.162318]\n",
      "epoch:29 step:27721 [D loss: 0.576783, acc.: 68.75%] [G loss: 1.074351]\n",
      "epoch:29 step:27722 [D loss: 0.495607, acc.: 79.69%] [G loss: 1.483483]\n",
      "epoch:29 step:27723 [D loss: 0.491587, acc.: 77.34%] [G loss: 1.553189]\n",
      "epoch:29 step:27724 [D loss: 0.421102, acc.: 80.47%] [G loss: 2.008433]\n",
      "epoch:29 step:27725 [D loss: 0.598227, acc.: 63.28%] [G loss: 1.640018]\n",
      "epoch:29 step:27726 [D loss: 0.485944, acc.: 76.56%] [G loss: 1.445982]\n",
      "epoch:29 step:27727 [D loss: 0.517399, acc.: 77.34%] [G loss: 1.374533]\n",
      "epoch:29 step:27728 [D loss: 0.559942, acc.: 69.53%] [G loss: 1.439141]\n",
      "epoch:29 step:27729 [D loss: 0.639067, acc.: 69.53%] [G loss: 1.333007]\n",
      "epoch:29 step:27730 [D loss: 0.528519, acc.: 75.78%] [G loss: 1.618249]\n",
      "epoch:29 step:27731 [D loss: 0.501922, acc.: 75.00%] [G loss: 1.532839]\n",
      "epoch:29 step:27732 [D loss: 0.783431, acc.: 57.03%] [G loss: 1.113132]\n",
      "epoch:29 step:27733 [D loss: 0.663832, acc.: 62.50%] [G loss: 1.553784]\n",
      "epoch:29 step:27734 [D loss: 0.494895, acc.: 78.91%] [G loss: 1.473983]\n",
      "epoch:29 step:27735 [D loss: 0.803589, acc.: 53.12%] [G loss: 1.701890]\n",
      "epoch:29 step:27736 [D loss: 0.473392, acc.: 74.22%] [G loss: 1.809841]\n",
      "epoch:29 step:27737 [D loss: 0.556338, acc.: 69.53%] [G loss: 1.249321]\n",
      "epoch:29 step:27738 [D loss: 0.547847, acc.: 72.66%] [G loss: 1.510459]\n",
      "epoch:29 step:27739 [D loss: 0.559971, acc.: 75.00%] [G loss: 1.518856]\n",
      "epoch:29 step:27740 [D loss: 0.337735, acc.: 91.41%] [G loss: 1.544991]\n",
      "epoch:29 step:27741 [D loss: 0.638445, acc.: 69.53%] [G loss: 1.424013]\n",
      "epoch:29 step:27742 [D loss: 0.674317, acc.: 58.59%] [G loss: 1.260237]\n",
      "epoch:29 step:27743 [D loss: 0.531165, acc.: 75.78%] [G loss: 1.694080]\n",
      "epoch:29 step:27744 [D loss: 0.384958, acc.: 82.81%] [G loss: 1.543769]\n",
      "epoch:29 step:27745 [D loss: 0.478264, acc.: 75.00%] [G loss: 1.690906]\n",
      "epoch:29 step:27746 [D loss: 0.595022, acc.: 68.75%] [G loss: 1.622201]\n",
      "epoch:29 step:27747 [D loss: 0.689847, acc.: 63.28%] [G loss: 1.167414]\n",
      "epoch:29 step:27748 [D loss: 0.447285, acc.: 78.91%] [G loss: 1.710502]\n",
      "epoch:29 step:27749 [D loss: 0.426165, acc.: 82.03%] [G loss: 1.328562]\n",
      "epoch:29 step:27750 [D loss: 0.719073, acc.: 59.38%] [G loss: 1.531288]\n",
      "epoch:29 step:27751 [D loss: 0.605231, acc.: 67.19%] [G loss: 1.620222]\n",
      "epoch:29 step:27752 [D loss: 0.457223, acc.: 78.12%] [G loss: 2.103888]\n",
      "epoch:29 step:27753 [D loss: 0.514632, acc.: 76.56%] [G loss: 1.475920]\n",
      "epoch:29 step:27754 [D loss: 0.590327, acc.: 65.62%] [G loss: 1.139809]\n",
      "epoch:29 step:27755 [D loss: 0.550765, acc.: 72.66%] [G loss: 1.425106]\n",
      "epoch:29 step:27756 [D loss: 0.512926, acc.: 79.69%] [G loss: 1.491473]\n",
      "epoch:29 step:27757 [D loss: 0.432632, acc.: 79.69%] [G loss: 1.822373]\n",
      "epoch:29 step:27758 [D loss: 0.627791, acc.: 65.62%] [G loss: 1.806594]\n",
      "epoch:29 step:27759 [D loss: 0.493841, acc.: 76.56%] [G loss: 1.586798]\n",
      "epoch:29 step:27760 [D loss: 0.470692, acc.: 75.00%] [G loss: 1.762344]\n",
      "epoch:29 step:27761 [D loss: 0.510518, acc.: 75.78%] [G loss: 1.212826]\n",
      "epoch:29 step:27762 [D loss: 0.534360, acc.: 73.44%] [G loss: 1.948192]\n",
      "epoch:29 step:27763 [D loss: 0.561516, acc.: 72.66%] [G loss: 1.208972]\n",
      "epoch:29 step:27764 [D loss: 0.443488, acc.: 83.59%] [G loss: 1.791094]\n",
      "epoch:29 step:27765 [D loss: 0.558536, acc.: 71.09%] [G loss: 1.191325]\n",
      "epoch:29 step:27766 [D loss: 0.682864, acc.: 60.94%] [G loss: 1.168275]\n",
      "epoch:29 step:27767 [D loss: 0.470408, acc.: 80.47%] [G loss: 1.039442]\n",
      "epoch:29 step:27768 [D loss: 0.672576, acc.: 64.06%] [G loss: 1.160423]\n",
      "epoch:29 step:27769 [D loss: 0.656979, acc.: 57.81%] [G loss: 1.341397]\n",
      "epoch:29 step:27770 [D loss: 0.709332, acc.: 57.81%] [G loss: 1.596100]\n",
      "epoch:29 step:27771 [D loss: 0.559254, acc.: 67.19%] [G loss: 1.397074]\n",
      "epoch:29 step:27772 [D loss: 0.372085, acc.: 86.72%] [G loss: 1.678249]\n",
      "epoch:29 step:27773 [D loss: 0.411667, acc.: 82.81%] [G loss: 1.647512]\n",
      "epoch:29 step:27774 [D loss: 0.634999, acc.: 65.62%] [G loss: 1.221155]\n",
      "epoch:29 step:27775 [D loss: 0.426027, acc.: 82.03%] [G loss: 1.795903]\n",
      "epoch:29 step:27776 [D loss: 0.662804, acc.: 61.72%] [G loss: 1.468595]\n",
      "epoch:29 step:27777 [D loss: 0.585279, acc.: 64.84%] [G loss: 1.142945]\n",
      "epoch:29 step:27778 [D loss: 0.582031, acc.: 70.31%] [G loss: 1.479784]\n",
      "epoch:29 step:27779 [D loss: 0.430453, acc.: 82.81%] [G loss: 1.289388]\n",
      "epoch:29 step:27780 [D loss: 0.614105, acc.: 65.62%] [G loss: 1.492955]\n",
      "epoch:29 step:27781 [D loss: 0.505747, acc.: 78.12%] [G loss: 1.456090]\n",
      "epoch:29 step:27782 [D loss: 0.737997, acc.: 57.81%] [G loss: 1.603714]\n",
      "epoch:29 step:27783 [D loss: 0.548491, acc.: 67.19%] [G loss: 1.464875]\n",
      "epoch:29 step:27784 [D loss: 0.460209, acc.: 82.03%] [G loss: 1.716999]\n",
      "epoch:29 step:27785 [D loss: 0.536036, acc.: 71.88%] [G loss: 1.427089]\n",
      "epoch:29 step:27786 [D loss: 0.477079, acc.: 80.47%] [G loss: 1.244532]\n",
      "epoch:29 step:27787 [D loss: 0.549891, acc.: 71.88%] [G loss: 1.431774]\n",
      "epoch:29 step:27788 [D loss: 0.562474, acc.: 71.88%] [G loss: 1.363407]\n",
      "epoch:29 step:27789 [D loss: 0.684105, acc.: 58.59%] [G loss: 1.555028]\n",
      "epoch:29 step:27790 [D loss: 0.468209, acc.: 81.25%] [G loss: 1.356923]\n",
      "epoch:29 step:27791 [D loss: 0.384016, acc.: 85.94%] [G loss: 1.337478]\n",
      "epoch:29 step:27792 [D loss: 0.714767, acc.: 55.47%] [G loss: 1.335502]\n",
      "epoch:29 step:27793 [D loss: 0.517220, acc.: 71.88%] [G loss: 1.731776]\n",
      "epoch:29 step:27794 [D loss: 0.576259, acc.: 69.53%] [G loss: 1.320713]\n",
      "epoch:29 step:27795 [D loss: 0.609191, acc.: 71.09%] [G loss: 1.682931]\n",
      "epoch:29 step:27796 [D loss: 0.539419, acc.: 73.44%] [G loss: 1.394587]\n",
      "epoch:29 step:27797 [D loss: 0.491365, acc.: 75.00%] [G loss: 1.470921]\n",
      "epoch:29 step:27798 [D loss: 0.465145, acc.: 82.81%] [G loss: 1.075826]\n",
      "epoch:29 step:27799 [D loss: 0.483493, acc.: 75.78%] [G loss: 1.270150]\n",
      "epoch:29 step:27800 [D loss: 0.517476, acc.: 71.88%] [G loss: 1.460617]\n",
      "##############\n",
      "[2.70090274 2.15186289 1.89791803 2.94965348 0.88460402 6.21890878\n",
      " 2.17096175 2.46730678 3.98986301 8.14868929]\n",
      "##########\n",
      "epoch:29 step:27801 [D loss: 0.739961, acc.: 54.69%] [G loss: 1.076791]\n",
      "epoch:29 step:27802 [D loss: 0.528154, acc.: 74.22%] [G loss: 1.830911]\n",
      "epoch:29 step:27803 [D loss: 0.613869, acc.: 62.50%] [G loss: 1.723599]\n",
      "epoch:29 step:27804 [D loss: 0.459865, acc.: 82.81%] [G loss: 1.598670]\n",
      "epoch:29 step:27805 [D loss: 0.432002, acc.: 79.69%] [G loss: 1.579452]\n",
      "epoch:29 step:27806 [D loss: 0.521531, acc.: 71.09%] [G loss: 1.225035]\n",
      "epoch:29 step:27807 [D loss: 0.412764, acc.: 81.25%] [G loss: 1.566540]\n",
      "epoch:29 step:27808 [D loss: 0.475196, acc.: 78.91%] [G loss: 1.529889]\n",
      "epoch:29 step:27809 [D loss: 0.422857, acc.: 82.03%] [G loss: 1.328039]\n",
      "epoch:29 step:27810 [D loss: 0.727214, acc.: 57.81%] [G loss: 1.636763]\n",
      "epoch:29 step:27811 [D loss: 0.453188, acc.: 78.12%] [G loss: 1.687552]\n",
      "epoch:29 step:27812 [D loss: 0.481969, acc.: 75.00%] [G loss: 1.490541]\n",
      "epoch:29 step:27813 [D loss: 0.369476, acc.: 86.72%] [G loss: 1.418261]\n",
      "epoch:29 step:27814 [D loss: 0.840630, acc.: 49.22%] [G loss: 1.146583]\n",
      "epoch:29 step:27815 [D loss: 0.590258, acc.: 69.53%] [G loss: 1.095950]\n",
      "epoch:29 step:27816 [D loss: 0.527493, acc.: 73.44%] [G loss: 1.426134]\n",
      "epoch:29 step:27817 [D loss: 0.602670, acc.: 71.09%] [G loss: 1.474623]\n",
      "epoch:29 step:27818 [D loss: 0.649323, acc.: 66.41%] [G loss: 1.290641]\n",
      "epoch:29 step:27819 [D loss: 0.380909, acc.: 86.72%] [G loss: 1.915514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27820 [D loss: 0.737607, acc.: 53.12%] [G loss: 1.338813]\n",
      "epoch:29 step:27821 [D loss: 0.373210, acc.: 88.28%] [G loss: 2.016433]\n",
      "epoch:29 step:27822 [D loss: 0.718389, acc.: 57.03%] [G loss: 1.735342]\n",
      "epoch:29 step:27823 [D loss: 0.528534, acc.: 71.09%] [G loss: 1.752252]\n",
      "epoch:29 step:27824 [D loss: 0.494463, acc.: 78.91%] [G loss: 1.705617]\n",
      "epoch:29 step:27825 [D loss: 0.692592, acc.: 61.72%] [G loss: 1.262492]\n",
      "epoch:29 step:27826 [D loss: 0.656061, acc.: 61.72%] [G loss: 1.466630]\n",
      "epoch:29 step:27827 [D loss: 0.611399, acc.: 69.53%] [G loss: 1.838797]\n",
      "epoch:29 step:27828 [D loss: 0.594486, acc.: 69.53%] [G loss: 1.339090]\n",
      "epoch:29 step:27829 [D loss: 0.424634, acc.: 83.59%] [G loss: 1.799366]\n",
      "epoch:29 step:27830 [D loss: 0.798434, acc.: 51.56%] [G loss: 1.015157]\n",
      "epoch:29 step:27831 [D loss: 0.595233, acc.: 63.28%] [G loss: 0.817058]\n",
      "epoch:29 step:27832 [D loss: 0.567739, acc.: 68.75%] [G loss: 1.198005]\n",
      "epoch:29 step:27833 [D loss: 0.511539, acc.: 75.00%] [G loss: 1.703928]\n",
      "epoch:29 step:27834 [D loss: 0.442332, acc.: 77.34%] [G loss: 1.357740]\n",
      "epoch:29 step:27835 [D loss: 0.694488, acc.: 58.59%] [G loss: 1.138011]\n",
      "epoch:29 step:27836 [D loss: 0.427701, acc.: 78.91%] [G loss: 1.598442]\n",
      "epoch:29 step:27837 [D loss: 0.562585, acc.: 73.44%] [G loss: 1.531463]\n",
      "epoch:29 step:27838 [D loss: 0.646550, acc.: 64.84%] [G loss: 1.432930]\n",
      "epoch:29 step:27839 [D loss: 0.589296, acc.: 72.66%] [G loss: 1.138836]\n",
      "epoch:29 step:27840 [D loss: 0.532675, acc.: 72.66%] [G loss: 1.245892]\n",
      "epoch:29 step:27841 [D loss: 0.688827, acc.: 58.59%] [G loss: 1.483176]\n",
      "epoch:29 step:27842 [D loss: 0.767538, acc.: 50.78%] [G loss: 1.572524]\n",
      "epoch:29 step:27843 [D loss: 0.355901, acc.: 88.28%] [G loss: 1.527046]\n",
      "epoch:29 step:27844 [D loss: 0.387510, acc.: 85.16%] [G loss: 1.964333]\n",
      "epoch:29 step:27845 [D loss: 0.460012, acc.: 82.81%] [G loss: 1.506594]\n",
      "epoch:29 step:27846 [D loss: 0.591052, acc.: 66.41%] [G loss: 1.560144]\n",
      "epoch:29 step:27847 [D loss: 0.555825, acc.: 70.31%] [G loss: 0.665466]\n",
      "epoch:29 step:27848 [D loss: 0.619290, acc.: 64.84%] [G loss: 1.233402]\n",
      "epoch:29 step:27849 [D loss: 0.366158, acc.: 85.94%] [G loss: 1.255401]\n",
      "epoch:29 step:27850 [D loss: 0.411905, acc.: 82.81%] [G loss: 1.546719]\n",
      "epoch:29 step:27851 [D loss: 0.604508, acc.: 66.41%] [G loss: 1.583450]\n",
      "epoch:29 step:27852 [D loss: 0.610804, acc.: 64.84%] [G loss: 1.443203]\n",
      "epoch:29 step:27853 [D loss: 0.557516, acc.: 69.53%] [G loss: 1.469163]\n",
      "epoch:29 step:27854 [D loss: 0.630585, acc.: 64.84%] [G loss: 1.340126]\n",
      "epoch:29 step:27855 [D loss: 0.461859, acc.: 83.59%] [G loss: 1.677947]\n",
      "epoch:29 step:27856 [D loss: 0.950589, acc.: 41.41%] [G loss: 1.479757]\n",
      "epoch:29 step:27857 [D loss: 0.615162, acc.: 63.28%] [G loss: 1.024752]\n",
      "epoch:29 step:27858 [D loss: 0.619616, acc.: 67.97%] [G loss: 1.606713]\n",
      "epoch:29 step:27859 [D loss: 0.485506, acc.: 75.00%] [G loss: 1.665945]\n",
      "epoch:29 step:27860 [D loss: 0.774681, acc.: 57.03%] [G loss: 1.274683]\n",
      "epoch:29 step:27861 [D loss: 0.683614, acc.: 58.59%] [G loss: 1.790600]\n",
      "epoch:29 step:27862 [D loss: 0.605361, acc.: 70.31%] [G loss: 1.161652]\n",
      "epoch:29 step:27863 [D loss: 0.459305, acc.: 75.00%] [G loss: 1.294534]\n",
      "epoch:29 step:27864 [D loss: 0.425675, acc.: 83.59%] [G loss: 1.554741]\n",
      "epoch:29 step:27865 [D loss: 0.666984, acc.: 60.94%] [G loss: 1.185361]\n",
      "epoch:29 step:27866 [D loss: 0.828480, acc.: 48.44%] [G loss: 1.175181]\n",
      "epoch:29 step:27867 [D loss: 0.587790, acc.: 70.31%] [G loss: 1.469150]\n",
      "epoch:29 step:27868 [D loss: 0.558783, acc.: 78.12%] [G loss: 1.411965]\n",
      "epoch:29 step:27869 [D loss: 0.487876, acc.: 78.91%] [G loss: 1.564113]\n",
      "epoch:29 step:27870 [D loss: 0.569275, acc.: 66.41%] [G loss: 1.262125]\n",
      "epoch:29 step:27871 [D loss: 0.652776, acc.: 63.28%] [G loss: 1.034825]\n",
      "epoch:29 step:27872 [D loss: 0.570005, acc.: 72.66%] [G loss: 1.527950]\n",
      "epoch:29 step:27873 [D loss: 0.665499, acc.: 64.84%] [G loss: 1.114490]\n",
      "epoch:29 step:27874 [D loss: 0.350689, acc.: 88.28%] [G loss: 1.403194]\n",
      "epoch:29 step:27875 [D loss: 0.384429, acc.: 87.50%] [G loss: 1.738065]\n",
      "epoch:29 step:27876 [D loss: 0.453680, acc.: 77.34%] [G loss: 1.391708]\n",
      "epoch:29 step:27877 [D loss: 0.716919, acc.: 60.16%] [G loss: 1.115046]\n",
      "epoch:29 step:27878 [D loss: 0.534677, acc.: 70.31%] [G loss: 1.124063]\n",
      "epoch:29 step:27879 [D loss: 0.570084, acc.: 71.09%] [G loss: 1.240697]\n",
      "epoch:29 step:27880 [D loss: 0.292651, acc.: 92.19%] [G loss: 1.573136]\n",
      "epoch:29 step:27881 [D loss: 0.570871, acc.: 67.97%] [G loss: 1.534902]\n",
      "epoch:29 step:27882 [D loss: 0.595032, acc.: 65.62%] [G loss: 1.195898]\n",
      "epoch:29 step:27883 [D loss: 0.495644, acc.: 75.78%] [G loss: 1.359119]\n",
      "epoch:29 step:27884 [D loss: 0.546317, acc.: 73.44%] [G loss: 1.354032]\n",
      "epoch:29 step:27885 [D loss: 0.604039, acc.: 71.09%] [G loss: 1.151699]\n",
      "epoch:29 step:27886 [D loss: 0.488471, acc.: 80.47%] [G loss: 1.721275]\n",
      "epoch:29 step:27887 [D loss: 0.696115, acc.: 57.81%] [G loss: 1.724025]\n",
      "epoch:29 step:27888 [D loss: 0.771411, acc.: 51.56%] [G loss: 1.082898]\n",
      "epoch:29 step:27889 [D loss: 0.613534, acc.: 62.50%] [G loss: 1.454447]\n",
      "epoch:29 step:27890 [D loss: 0.501757, acc.: 80.47%] [G loss: 1.403388]\n",
      "epoch:29 step:27891 [D loss: 0.467087, acc.: 77.34%] [G loss: 1.454118]\n",
      "epoch:29 step:27892 [D loss: 0.591036, acc.: 71.88%] [G loss: 1.593624]\n",
      "epoch:29 step:27893 [D loss: 0.615696, acc.: 63.28%] [G loss: 1.275450]\n",
      "epoch:29 step:27894 [D loss: 0.527919, acc.: 72.66%] [G loss: 1.589674]\n",
      "epoch:29 step:27895 [D loss: 0.566149, acc.: 73.44%] [G loss: 1.388157]\n",
      "epoch:29 step:27896 [D loss: 0.483210, acc.: 78.12%] [G loss: 1.254808]\n",
      "epoch:29 step:27897 [D loss: 0.509418, acc.: 78.12%] [G loss: 1.075957]\n",
      "epoch:29 step:27898 [D loss: 0.535608, acc.: 67.97%] [G loss: 1.393552]\n",
      "epoch:29 step:27899 [D loss: 0.459201, acc.: 75.78%] [G loss: 1.840685]\n",
      "epoch:29 step:27900 [D loss: 0.609458, acc.: 67.97%] [G loss: 0.795479]\n",
      "epoch:29 step:27901 [D loss: 0.629365, acc.: 64.84%] [G loss: 1.171263]\n",
      "epoch:29 step:27902 [D loss: 0.548932, acc.: 72.66%] [G loss: 1.512934]\n",
      "epoch:29 step:27903 [D loss: 0.535992, acc.: 67.19%] [G loss: 1.934138]\n",
      "epoch:29 step:27904 [D loss: 0.567767, acc.: 71.09%] [G loss: 1.285976]\n",
      "epoch:29 step:27905 [D loss: 0.492033, acc.: 78.91%] [G loss: 1.721688]\n",
      "epoch:29 step:27906 [D loss: 0.477591, acc.: 78.91%] [G loss: 1.666799]\n",
      "epoch:29 step:27907 [D loss: 0.422629, acc.: 80.47%] [G loss: 1.261601]\n",
      "epoch:29 step:27908 [D loss: 0.493796, acc.: 75.78%] [G loss: 1.154734]\n",
      "epoch:29 step:27909 [D loss: 0.392476, acc.: 89.84%] [G loss: 1.389526]\n",
      "epoch:29 step:27910 [D loss: 0.452198, acc.: 78.91%] [G loss: 1.496064]\n",
      "epoch:29 step:27911 [D loss: 0.663645, acc.: 60.94%] [G loss: 1.221323]\n",
      "epoch:29 step:27912 [D loss: 0.446987, acc.: 81.25%] [G loss: 1.581195]\n",
      "epoch:29 step:27913 [D loss: 0.709081, acc.: 58.59%] [G loss: 1.147977]\n",
      "epoch:29 step:27914 [D loss: 0.553114, acc.: 71.09%] [G loss: 1.754548]\n",
      "epoch:29 step:27915 [D loss: 0.415221, acc.: 79.69%] [G loss: 1.644932]\n",
      "epoch:29 step:27916 [D loss: 0.722660, acc.: 54.69%] [G loss: 1.175809]\n",
      "epoch:29 step:27917 [D loss: 0.666593, acc.: 60.94%] [G loss: 1.435975]\n",
      "epoch:29 step:27918 [D loss: 0.540284, acc.: 72.66%] [G loss: 1.493441]\n",
      "epoch:29 step:27919 [D loss: 0.568114, acc.: 68.75%] [G loss: 1.310951]\n",
      "epoch:29 step:27920 [D loss: 0.472692, acc.: 78.12%] [G loss: 1.764791]\n",
      "epoch:29 step:27921 [D loss: 0.526321, acc.: 74.22%] [G loss: 1.130615]\n",
      "epoch:29 step:27922 [D loss: 0.585641, acc.: 69.53%] [G loss: 1.323606]\n",
      "epoch:29 step:27923 [D loss: 0.479071, acc.: 78.12%] [G loss: 1.569078]\n",
      "epoch:29 step:27924 [D loss: 0.393201, acc.: 84.38%] [G loss: 1.668733]\n",
      "epoch:29 step:27925 [D loss: 0.771031, acc.: 49.22%] [G loss: 1.386381]\n",
      "epoch:29 step:27926 [D loss: 0.563013, acc.: 72.66%] [G loss: 1.179596]\n",
      "epoch:29 step:27927 [D loss: 0.424502, acc.: 79.69%] [G loss: 1.157192]\n",
      "epoch:29 step:27928 [D loss: 0.459918, acc.: 80.47%] [G loss: 1.427191]\n",
      "epoch:29 step:27929 [D loss: 0.463653, acc.: 80.47%] [G loss: 1.238461]\n",
      "epoch:29 step:27930 [D loss: 0.494448, acc.: 78.91%] [G loss: 1.585250]\n",
      "epoch:29 step:27931 [D loss: 0.692471, acc.: 58.59%] [G loss: 1.430906]\n",
      "epoch:29 step:27932 [D loss: 0.507049, acc.: 75.78%] [G loss: 1.555692]\n",
      "epoch:29 step:27933 [D loss: 0.443229, acc.: 78.91%] [G loss: 1.363883]\n",
      "epoch:29 step:27934 [D loss: 0.632284, acc.: 64.84%] [G loss: 1.277567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27935 [D loss: 0.610896, acc.: 65.62%] [G loss: 0.931722]\n",
      "epoch:29 step:27936 [D loss: 0.598205, acc.: 66.41%] [G loss: 1.235698]\n",
      "epoch:29 step:27937 [D loss: 0.480137, acc.: 80.47%] [G loss: 1.218508]\n",
      "epoch:29 step:27938 [D loss: 0.358346, acc.: 86.72%] [G loss: 1.594010]\n",
      "epoch:29 step:27939 [D loss: 0.594099, acc.: 74.22%] [G loss: 1.440986]\n",
      "epoch:29 step:27940 [D loss: 0.417997, acc.: 83.59%] [G loss: 1.533143]\n",
      "epoch:29 step:27941 [D loss: 0.523579, acc.: 73.44%] [G loss: 1.461390]\n",
      "epoch:29 step:27942 [D loss: 0.532435, acc.: 71.88%] [G loss: 1.379302]\n",
      "epoch:29 step:27943 [D loss: 0.555041, acc.: 69.53%] [G loss: 1.458590]\n",
      "epoch:29 step:27944 [D loss: 0.583595, acc.: 71.88%] [G loss: 1.793467]\n",
      "epoch:29 step:27945 [D loss: 0.468101, acc.: 78.91%] [G loss: 1.455937]\n",
      "epoch:29 step:27946 [D loss: 0.587772, acc.: 67.19%] [G loss: 1.278348]\n",
      "epoch:29 step:27947 [D loss: 0.253732, acc.: 94.53%] [G loss: 1.630357]\n",
      "epoch:29 step:27948 [D loss: 0.584643, acc.: 66.41%] [G loss: 1.272925]\n",
      "epoch:29 step:27949 [D loss: 0.666578, acc.: 60.16%] [G loss: 1.222157]\n",
      "epoch:29 step:27950 [D loss: 0.474975, acc.: 77.34%] [G loss: 1.231520]\n",
      "epoch:29 step:27951 [D loss: 0.534467, acc.: 72.66%] [G loss: 1.663790]\n",
      "epoch:29 step:27952 [D loss: 0.738453, acc.: 54.69%] [G loss: 1.078814]\n",
      "epoch:29 step:27953 [D loss: 0.846997, acc.: 47.66%] [G loss: 1.160888]\n",
      "epoch:29 step:27954 [D loss: 0.479267, acc.: 76.56%] [G loss: 1.652203]\n",
      "epoch:29 step:27955 [D loss: 0.638691, acc.: 66.41%] [G loss: 1.722715]\n",
      "epoch:29 step:27956 [D loss: 0.548030, acc.: 73.44%] [G loss: 1.536444]\n",
      "epoch:29 step:27957 [D loss: 0.632991, acc.: 64.06%] [G loss: 1.716468]\n",
      "epoch:29 step:27958 [D loss: 0.481926, acc.: 77.34%] [G loss: 1.848782]\n",
      "epoch:29 step:27959 [D loss: 0.529304, acc.: 71.88%] [G loss: 1.540670]\n",
      "epoch:29 step:27960 [D loss: 0.697610, acc.: 57.81%] [G loss: 1.512812]\n",
      "epoch:29 step:27961 [D loss: 0.582360, acc.: 67.97%] [G loss: 1.761817]\n",
      "epoch:29 step:27962 [D loss: 0.387238, acc.: 88.28%] [G loss: 2.124483]\n",
      "epoch:29 step:27963 [D loss: 0.562809, acc.: 70.31%] [G loss: 1.211222]\n",
      "epoch:29 step:27964 [D loss: 0.472104, acc.: 78.12%] [G loss: 1.664434]\n",
      "epoch:29 step:27965 [D loss: 0.503640, acc.: 78.91%] [G loss: 1.418406]\n",
      "epoch:29 step:27966 [D loss: 0.551914, acc.: 71.88%] [G loss: 0.902396]\n",
      "epoch:29 step:27967 [D loss: 0.539004, acc.: 70.31%] [G loss: 1.283795]\n",
      "epoch:29 step:27968 [D loss: 0.488536, acc.: 80.47%] [G loss: 1.633446]\n",
      "epoch:29 step:27969 [D loss: 0.509867, acc.: 71.09%] [G loss: 1.691396]\n",
      "epoch:29 step:27970 [D loss: 0.603056, acc.: 69.53%] [G loss: 1.162663]\n",
      "epoch:29 step:27971 [D loss: 0.610575, acc.: 67.19%] [G loss: 1.390996]\n",
      "epoch:29 step:27972 [D loss: 0.517167, acc.: 78.12%] [G loss: 1.490922]\n",
      "epoch:29 step:27973 [D loss: 0.664691, acc.: 65.62%] [G loss: 1.578085]\n",
      "epoch:29 step:27974 [D loss: 0.696280, acc.: 64.06%] [G loss: 1.257014]\n",
      "epoch:29 step:27975 [D loss: 0.569513, acc.: 71.09%] [G loss: 1.306493]\n",
      "epoch:29 step:27976 [D loss: 0.484676, acc.: 81.25%] [G loss: 1.876514]\n",
      "epoch:29 step:27977 [D loss: 0.343324, acc.: 90.62%] [G loss: 1.936635]\n",
      "epoch:29 step:27978 [D loss: 0.577053, acc.: 69.53%] [G loss: 1.501048]\n",
      "epoch:29 step:27979 [D loss: 0.459282, acc.: 82.03%] [G loss: 1.184723]\n",
      "epoch:29 step:27980 [D loss: 0.421101, acc.: 83.59%] [G loss: 1.831078]\n",
      "epoch:29 step:27981 [D loss: 0.501755, acc.: 75.00%] [G loss: 1.660596]\n",
      "epoch:29 step:27982 [D loss: 0.457670, acc.: 79.69%] [G loss: 1.634537]\n",
      "epoch:29 step:27983 [D loss: 0.590406, acc.: 75.00%] [G loss: 1.222279]\n",
      "epoch:29 step:27984 [D loss: 0.546651, acc.: 68.75%] [G loss: 1.740583]\n",
      "epoch:29 step:27985 [D loss: 0.473724, acc.: 76.56%] [G loss: 1.753493]\n",
      "epoch:29 step:27986 [D loss: 0.442054, acc.: 80.47%] [G loss: 1.840437]\n",
      "epoch:29 step:27987 [D loss: 0.538157, acc.: 76.56%] [G loss: 1.534939]\n",
      "epoch:29 step:27988 [D loss: 0.604064, acc.: 67.19%] [G loss: 1.690093]\n",
      "epoch:29 step:27989 [D loss: 0.420479, acc.: 84.38%] [G loss: 1.876946]\n",
      "epoch:29 step:27990 [D loss: 0.691622, acc.: 57.81%] [G loss: 1.330578]\n",
      "epoch:29 step:27991 [D loss: 0.459167, acc.: 80.47%] [G loss: 1.347826]\n",
      "epoch:29 step:27992 [D loss: 0.563098, acc.: 75.00%] [G loss: 1.469770]\n",
      "epoch:29 step:27993 [D loss: 0.530174, acc.: 74.22%] [G loss: 1.369026]\n",
      "epoch:29 step:27994 [D loss: 0.584784, acc.: 71.09%] [G loss: 1.778930]\n",
      "epoch:29 step:27995 [D loss: 0.533524, acc.: 75.00%] [G loss: 1.017492]\n",
      "epoch:29 step:27996 [D loss: 0.559180, acc.: 71.09%] [G loss: 1.274590]\n",
      "epoch:29 step:27997 [D loss: 0.610316, acc.: 67.97%] [G loss: 1.394745]\n",
      "epoch:29 step:27998 [D loss: 0.358100, acc.: 89.06%] [G loss: 1.581738]\n",
      "epoch:29 step:27999 [D loss: 0.515728, acc.: 75.00%] [G loss: 1.721693]\n",
      "epoch:29 step:28000 [D loss: 0.486632, acc.: 75.00%] [G loss: 0.953906]\n",
      "##############\n",
      "[2.62855144 1.98139669 2.01793966 2.56477942 0.82025417 6.35438757\n",
      " 2.08882091 2.27708165 3.81339867 7.14771273]\n",
      "##########\n",
      "epoch:29 step:28001 [D loss: 0.667928, acc.: 57.81%] [G loss: 1.340725]\n",
      "epoch:29 step:28002 [D loss: 0.718271, acc.: 52.34%] [G loss: 1.051436]\n",
      "epoch:29 step:28003 [D loss: 0.568500, acc.: 72.66%] [G loss: 1.542460]\n",
      "epoch:29 step:28004 [D loss: 0.694098, acc.: 60.16%] [G loss: 1.248676]\n",
      "epoch:29 step:28005 [D loss: 0.665893, acc.: 60.94%] [G loss: 1.111937]\n",
      "epoch:29 step:28006 [D loss: 0.522273, acc.: 70.31%] [G loss: 1.101086]\n",
      "epoch:29 step:28007 [D loss: 0.546886, acc.: 68.75%] [G loss: 1.239032]\n",
      "epoch:29 step:28008 [D loss: 0.421220, acc.: 82.81%] [G loss: 1.316756]\n",
      "epoch:29 step:28009 [D loss: 0.563988, acc.: 72.66%] [G loss: 1.341948]\n",
      "epoch:29 step:28010 [D loss: 0.828452, acc.: 48.44%] [G loss: 1.346732]\n",
      "epoch:29 step:28011 [D loss: 0.428909, acc.: 82.81%] [G loss: 1.160537]\n",
      "epoch:29 step:28012 [D loss: 0.714136, acc.: 59.38%] [G loss: 1.423315]\n",
      "epoch:29 step:28013 [D loss: 0.389615, acc.: 85.16%] [G loss: 1.579752]\n",
      "epoch:29 step:28014 [D loss: 0.524111, acc.: 75.00%] [G loss: 1.914275]\n",
      "epoch:29 step:28015 [D loss: 0.442924, acc.: 82.81%] [G loss: 1.564595]\n",
      "epoch:29 step:28016 [D loss: 0.571619, acc.: 72.66%] [G loss: 1.109310]\n",
      "epoch:29 step:28017 [D loss: 0.758316, acc.: 50.78%] [G loss: 1.040557]\n",
      "epoch:29 step:28018 [D loss: 0.606309, acc.: 66.41%] [G loss: 1.608446]\n",
      "epoch:29 step:28019 [D loss: 0.475892, acc.: 83.59%] [G loss: 1.738622]\n",
      "epoch:29 step:28020 [D loss: 0.643405, acc.: 63.28%] [G loss: 1.251323]\n",
      "epoch:29 step:28021 [D loss: 0.495733, acc.: 77.34%] [G loss: 1.406563]\n",
      "epoch:29 step:28022 [D loss: 0.613008, acc.: 67.97%] [G loss: 1.236091]\n",
      "epoch:29 step:28023 [D loss: 0.350778, acc.: 89.06%] [G loss: 1.574436]\n",
      "epoch:29 step:28024 [D loss: 0.776384, acc.: 52.34%] [G loss: 1.768934]\n",
      "epoch:29 step:28025 [D loss: 0.336557, acc.: 86.72%] [G loss: 1.564760]\n",
      "epoch:29 step:28026 [D loss: 0.621183, acc.: 60.94%] [G loss: 1.301503]\n",
      "epoch:29 step:28027 [D loss: 0.458321, acc.: 81.25%] [G loss: 1.389576]\n",
      "epoch:29 step:28028 [D loss: 0.670187, acc.: 61.72%] [G loss: 1.448360]\n",
      "epoch:29 step:28029 [D loss: 0.557002, acc.: 68.75%] [G loss: 1.588763]\n",
      "epoch:29 step:28030 [D loss: 0.511593, acc.: 74.22%] [G loss: 1.778696]\n",
      "epoch:29 step:28031 [D loss: 0.449865, acc.: 77.34%] [G loss: 1.845332]\n",
      "epoch:29 step:28032 [D loss: 0.414186, acc.: 79.69%] [G loss: 1.225197]\n",
      "epoch:29 step:28033 [D loss: 0.455837, acc.: 82.03%] [G loss: 1.540462]\n",
      "epoch:29 step:28034 [D loss: 0.395386, acc.: 85.16%] [G loss: 1.660069]\n",
      "epoch:29 step:28035 [D loss: 0.503710, acc.: 80.47%] [G loss: 1.377059]\n",
      "epoch:29 step:28036 [D loss: 0.474089, acc.: 80.47%] [G loss: 1.597404]\n",
      "epoch:29 step:28037 [D loss: 0.617816, acc.: 70.31%] [G loss: 1.142667]\n",
      "epoch:29 step:28038 [D loss: 0.468601, acc.: 79.69%] [G loss: 1.427837]\n",
      "epoch:29 step:28039 [D loss: 0.376734, acc.: 83.59%] [G loss: 1.465252]\n",
      "epoch:29 step:28040 [D loss: 0.386231, acc.: 85.94%] [G loss: 1.262362]\n",
      "epoch:29 step:28041 [D loss: 0.409135, acc.: 82.81%] [G loss: 1.317840]\n",
      "epoch:29 step:28042 [D loss: 0.495121, acc.: 77.34%] [G loss: 1.823521]\n",
      "epoch:29 step:28043 [D loss: 0.444720, acc.: 77.34%] [G loss: 1.404368]\n",
      "epoch:29 step:28044 [D loss: 0.564332, acc.: 67.97%] [G loss: 1.584791]\n",
      "epoch:29 step:28045 [D loss: 0.659058, acc.: 60.94%] [G loss: 1.199658]\n",
      "epoch:29 step:28046 [D loss: 0.421927, acc.: 84.38%] [G loss: 1.708280]\n",
      "epoch:29 step:28047 [D loss: 0.323989, acc.: 91.41%] [G loss: 1.491826]\n",
      "epoch:29 step:28048 [D loss: 0.515089, acc.: 73.44%] [G loss: 1.501554]\n",
      "epoch:29 step:28049 [D loss: 0.647599, acc.: 61.72%] [G loss: 1.138962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28050 [D loss: 0.569294, acc.: 71.09%] [G loss: 1.791924]\n",
      "epoch:29 step:28051 [D loss: 0.488773, acc.: 75.00%] [G loss: 1.244989]\n",
      "epoch:29 step:28052 [D loss: 0.544560, acc.: 71.88%] [G loss: 1.232252]\n",
      "epoch:29 step:28053 [D loss: 0.488645, acc.: 77.34%] [G loss: 1.433345]\n",
      "epoch:29 step:28054 [D loss: 0.631108, acc.: 63.28%] [G loss: 1.210460]\n",
      "epoch:29 step:28055 [D loss: 0.467787, acc.: 78.12%] [G loss: 1.566522]\n",
      "epoch:29 step:28056 [D loss: 0.744206, acc.: 59.38%] [G loss: 1.190151]\n",
      "epoch:29 step:28057 [D loss: 0.650968, acc.: 60.16%] [G loss: 1.357784]\n",
      "epoch:29 step:28058 [D loss: 0.527277, acc.: 74.22%] [G loss: 1.565322]\n",
      "epoch:29 step:28059 [D loss: 0.541874, acc.: 74.22%] [G loss: 1.766760]\n",
      "epoch:29 step:28060 [D loss: 0.507474, acc.: 75.00%] [G loss: 1.246748]\n",
      "epoch:29 step:28061 [D loss: 0.434015, acc.: 82.03%] [G loss: 1.555421]\n",
      "epoch:29 step:28062 [D loss: 0.552278, acc.: 68.75%] [G loss: 1.331518]\n",
      "epoch:29 step:28063 [D loss: 0.654719, acc.: 60.16%] [G loss: 1.579176]\n",
      "epoch:29 step:28064 [D loss: 0.742951, acc.: 57.81%] [G loss: 1.490172]\n",
      "epoch:29 step:28065 [D loss: 0.630077, acc.: 66.41%] [G loss: 1.610489]\n",
      "epoch:29 step:28066 [D loss: 0.514183, acc.: 76.56%] [G loss: 1.048526]\n",
      "epoch:29 step:28067 [D loss: 0.498565, acc.: 80.47%] [G loss: 1.178347]\n",
      "epoch:29 step:28068 [D loss: 0.483706, acc.: 81.25%] [G loss: 1.478364]\n",
      "epoch:29 step:28069 [D loss: 0.525948, acc.: 75.00%] [G loss: 1.453264]\n",
      "epoch:29 step:28070 [D loss: 0.442332, acc.: 82.03%] [G loss: 1.183982]\n",
      "epoch:29 step:28071 [D loss: 0.551461, acc.: 73.44%] [G loss: 1.104898]\n",
      "epoch:29 step:28072 [D loss: 0.506606, acc.: 77.34%] [G loss: 1.395719]\n",
      "epoch:29 step:28073 [D loss: 0.564850, acc.: 71.09%] [G loss: 1.611976]\n",
      "epoch:29 step:28074 [D loss: 0.842798, acc.: 44.53%] [G loss: 1.466557]\n",
      "epoch:29 step:28075 [D loss: 0.653271, acc.: 64.84%] [G loss: 1.292831]\n",
      "epoch:29 step:28076 [D loss: 0.526388, acc.: 76.56%] [G loss: 1.775631]\n",
      "epoch:29 step:28077 [D loss: 0.643115, acc.: 65.62%] [G loss: 1.472209]\n",
      "epoch:29 step:28078 [D loss: 0.699059, acc.: 54.69%] [G loss: 1.082110]\n",
      "epoch:29 step:28079 [D loss: 0.626601, acc.: 64.06%] [G loss: 0.906533]\n",
      "epoch:29 step:28080 [D loss: 0.456769, acc.: 81.25%] [G loss: 1.220506]\n",
      "epoch:29 step:28081 [D loss: 0.520779, acc.: 78.12%] [G loss: 1.271862]\n",
      "epoch:29 step:28082 [D loss: 0.495546, acc.: 78.91%] [G loss: 1.208259]\n",
      "epoch:29 step:28083 [D loss: 0.493072, acc.: 72.66%] [G loss: 1.257586]\n",
      "epoch:29 step:28084 [D loss: 0.418641, acc.: 85.16%] [G loss: 1.730679]\n",
      "epoch:29 step:28085 [D loss: 0.626197, acc.: 67.97%] [G loss: 1.426238]\n",
      "epoch:29 step:28086 [D loss: 0.536930, acc.: 73.44%] [G loss: 1.400285]\n",
      "epoch:29 step:28087 [D loss: 0.474628, acc.: 76.56%] [G loss: 1.547699]\n",
      "epoch:29 step:28088 [D loss: 0.518475, acc.: 67.97%] [G loss: 1.489560]\n",
      "epoch:29 step:28089 [D loss: 0.514967, acc.: 77.34%] [G loss: 1.969435]\n",
      "epoch:29 step:28090 [D loss: 0.415965, acc.: 79.69%] [G loss: 1.393064]\n",
      "epoch:29 step:28091 [D loss: 0.626601, acc.: 64.06%] [G loss: 1.225001]\n",
      "epoch:29 step:28092 [D loss: 0.672122, acc.: 63.28%] [G loss: 1.325859]\n",
      "epoch:29 step:28093 [D loss: 0.539234, acc.: 75.00%] [G loss: 1.483262]\n",
      "epoch:29 step:28094 [D loss: 0.604841, acc.: 67.19%] [G loss: 0.910609]\n",
      "epoch:29 step:28095 [D loss: 0.629010, acc.: 64.06%] [G loss: 1.766752]\n",
      "epoch:29 step:28096 [D loss: 0.545052, acc.: 68.75%] [G loss: 1.640476]\n",
      "epoch:29 step:28097 [D loss: 0.579285, acc.: 69.53%] [G loss: 1.408876]\n",
      "epoch:29 step:28098 [D loss: 0.523159, acc.: 73.44%] [G loss: 1.269846]\n",
      "epoch:29 step:28099 [D loss: 0.466982, acc.: 78.91%] [G loss: 1.442276]\n",
      "epoch:29 step:28100 [D loss: 0.618305, acc.: 60.94%] [G loss: 1.282697]\n",
      "epoch:29 step:28101 [D loss: 0.475324, acc.: 78.12%] [G loss: 1.278945]\n",
      "epoch:29 step:28102 [D loss: 0.786480, acc.: 55.47%] [G loss: 0.934015]\n",
      "epoch:29 step:28103 [D loss: 0.398241, acc.: 86.72%] [G loss: 2.284070]\n",
      "epoch:29 step:28104 [D loss: 0.596575, acc.: 70.31%] [G loss: 1.494533]\n",
      "epoch:29 step:28105 [D loss: 0.362026, acc.: 89.06%] [G loss: 1.657360]\n",
      "epoch:29 step:28106 [D loss: 0.616439, acc.: 65.62%] [G loss: 1.176994]\n",
      "epoch:29 step:28107 [D loss: 0.608057, acc.: 64.84%] [G loss: 0.949684]\n",
      "epoch:29 step:28108 [D loss: 0.438681, acc.: 84.38%] [G loss: 1.121316]\n",
      "epoch:29 step:28109 [D loss: 0.508148, acc.: 71.09%] [G loss: 1.043538]\n",
      "epoch:29 step:28110 [D loss: 0.584908, acc.: 68.75%] [G loss: 1.152124]\n",
      "epoch:30 step:28111 [D loss: 0.353369, acc.: 86.72%] [G loss: 1.846791]\n",
      "epoch:30 step:28112 [D loss: 0.696100, acc.: 59.38%] [G loss: 1.371703]\n",
      "epoch:30 step:28113 [D loss: 0.527571, acc.: 74.22%] [G loss: 1.157829]\n",
      "epoch:30 step:28114 [D loss: 0.607436, acc.: 67.97%] [G loss: 1.316003]\n",
      "epoch:30 step:28115 [D loss: 0.775449, acc.: 52.34%] [G loss: 1.073457]\n",
      "epoch:30 step:28116 [D loss: 0.504239, acc.: 78.91%] [G loss: 1.379924]\n",
      "epoch:30 step:28117 [D loss: 0.546633, acc.: 71.09%] [G loss: 1.443341]\n",
      "epoch:30 step:28118 [D loss: 0.417401, acc.: 85.16%] [G loss: 1.445253]\n",
      "epoch:30 step:28119 [D loss: 0.609018, acc.: 68.75%] [G loss: 1.340094]\n",
      "epoch:30 step:28120 [D loss: 0.482313, acc.: 77.34%] [G loss: 1.390192]\n",
      "epoch:30 step:28121 [D loss: 0.426098, acc.: 82.03%] [G loss: 1.218914]\n",
      "epoch:30 step:28122 [D loss: 0.435062, acc.: 83.59%] [G loss: 1.201375]\n",
      "epoch:30 step:28123 [D loss: 0.534255, acc.: 71.09%] [G loss: 1.930155]\n",
      "epoch:30 step:28124 [D loss: 0.407442, acc.: 82.81%] [G loss: 1.386337]\n",
      "epoch:30 step:28125 [D loss: 0.639878, acc.: 63.28%] [G loss: 1.349111]\n",
      "epoch:30 step:28126 [D loss: 0.485848, acc.: 75.00%] [G loss: 1.599378]\n",
      "epoch:30 step:28127 [D loss: 0.524182, acc.: 75.00%] [G loss: 1.367839]\n",
      "epoch:30 step:28128 [D loss: 0.426546, acc.: 78.12%] [G loss: 1.821104]\n",
      "epoch:30 step:28129 [D loss: 0.639435, acc.: 60.16%] [G loss: 1.155523]\n",
      "epoch:30 step:28130 [D loss: 0.477287, acc.: 77.34%] [G loss: 1.381914]\n",
      "epoch:30 step:28131 [D loss: 0.658226, acc.: 60.16%] [G loss: 1.396104]\n",
      "epoch:30 step:28132 [D loss: 0.615652, acc.: 63.28%] [G loss: 1.189954]\n",
      "epoch:30 step:28133 [D loss: 0.514156, acc.: 74.22%] [G loss: 1.459139]\n",
      "epoch:30 step:28134 [D loss: 0.464371, acc.: 79.69%] [G loss: 1.288694]\n",
      "epoch:30 step:28135 [D loss: 0.587658, acc.: 71.09%] [G loss: 1.646251]\n",
      "epoch:30 step:28136 [D loss: 0.433093, acc.: 80.47%] [G loss: 1.621975]\n",
      "epoch:30 step:28137 [D loss: 0.592704, acc.: 67.19%] [G loss: 1.201427]\n",
      "epoch:30 step:28138 [D loss: 0.549441, acc.: 79.69%] [G loss: 1.971754]\n",
      "epoch:30 step:28139 [D loss: 0.681486, acc.: 61.72%] [G loss: 1.315805]\n",
      "epoch:30 step:28140 [D loss: 0.602344, acc.: 67.97%] [G loss: 1.323637]\n",
      "epoch:30 step:28141 [D loss: 0.650834, acc.: 67.19%] [G loss: 0.932238]\n",
      "epoch:30 step:28142 [D loss: 0.496016, acc.: 71.88%] [G loss: 1.172770]\n",
      "epoch:30 step:28143 [D loss: 0.439729, acc.: 82.81%] [G loss: 1.478456]\n",
      "epoch:30 step:28144 [D loss: 0.358613, acc.: 86.72%] [G loss: 1.465230]\n",
      "epoch:30 step:28145 [D loss: 0.509001, acc.: 76.56%] [G loss: 1.364370]\n",
      "epoch:30 step:28146 [D loss: 0.452595, acc.: 78.12%] [G loss: 1.748233]\n",
      "epoch:30 step:28147 [D loss: 0.578821, acc.: 69.53%] [G loss: 1.254681]\n",
      "epoch:30 step:28148 [D loss: 0.627979, acc.: 67.97%] [G loss: 1.597027]\n",
      "epoch:30 step:28149 [D loss: 0.387131, acc.: 92.19%] [G loss: 1.149741]\n",
      "epoch:30 step:28150 [D loss: 0.455095, acc.: 80.47%] [G loss: 2.063015]\n",
      "epoch:30 step:28151 [D loss: 0.409337, acc.: 82.81%] [G loss: 1.551724]\n",
      "epoch:30 step:28152 [D loss: 0.397724, acc.: 80.47%] [G loss: 1.499830]\n",
      "epoch:30 step:28153 [D loss: 0.517517, acc.: 75.00%] [G loss: 1.279473]\n",
      "epoch:30 step:28154 [D loss: 0.560010, acc.: 70.31%] [G loss: 1.461680]\n",
      "epoch:30 step:28155 [D loss: 0.465049, acc.: 78.12%] [G loss: 1.243681]\n",
      "epoch:30 step:28156 [D loss: 0.570514, acc.: 70.31%] [G loss: 1.077188]\n",
      "epoch:30 step:28157 [D loss: 0.499663, acc.: 72.66%] [G loss: 1.097741]\n",
      "epoch:30 step:28158 [D loss: 0.468707, acc.: 79.69%] [G loss: 1.093027]\n",
      "epoch:30 step:28159 [D loss: 0.436401, acc.: 82.81%] [G loss: 1.506907]\n",
      "epoch:30 step:28160 [D loss: 0.543173, acc.: 71.09%] [G loss: 1.511026]\n",
      "epoch:30 step:28161 [D loss: 0.488673, acc.: 80.47%] [G loss: 1.561318]\n",
      "epoch:30 step:28162 [D loss: 0.520452, acc.: 75.78%] [G loss: 1.523757]\n",
      "epoch:30 step:28163 [D loss: 0.363632, acc.: 89.06%] [G loss: 1.262842]\n",
      "epoch:30 step:28164 [D loss: 0.426759, acc.: 81.25%] [G loss: 1.699216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28165 [D loss: 0.616899, acc.: 67.19%] [G loss: 1.348387]\n",
      "epoch:30 step:28166 [D loss: 0.502477, acc.: 75.78%] [G loss: 1.267491]\n",
      "epoch:30 step:28167 [D loss: 0.638083, acc.: 65.62%] [G loss: 0.889967]\n",
      "epoch:30 step:28168 [D loss: 0.451472, acc.: 79.69%] [G loss: 1.628493]\n",
      "epoch:30 step:28169 [D loss: 0.406544, acc.: 84.38%] [G loss: 1.604916]\n",
      "epoch:30 step:28170 [D loss: 0.475223, acc.: 78.12%] [G loss: 1.335068]\n",
      "epoch:30 step:28171 [D loss: 0.533577, acc.: 68.75%] [G loss: 1.488786]\n",
      "epoch:30 step:28172 [D loss: 0.562720, acc.: 69.53%] [G loss: 1.686694]\n",
      "epoch:30 step:28173 [D loss: 0.512674, acc.: 78.12%] [G loss: 1.379767]\n",
      "epoch:30 step:28174 [D loss: 0.472605, acc.: 77.34%] [G loss: 1.821006]\n",
      "epoch:30 step:28175 [D loss: 0.438978, acc.: 82.81%] [G loss: 1.679043]\n",
      "epoch:30 step:28176 [D loss: 0.544733, acc.: 75.00%] [G loss: 1.399331]\n",
      "epoch:30 step:28177 [D loss: 0.494235, acc.: 75.78%] [G loss: 1.491069]\n",
      "epoch:30 step:28178 [D loss: 0.521756, acc.: 76.56%] [G loss: 1.318055]\n",
      "epoch:30 step:28179 [D loss: 0.475365, acc.: 77.34%] [G loss: 1.368923]\n",
      "epoch:30 step:28180 [D loss: 0.593148, acc.: 67.97%] [G loss: 1.825772]\n",
      "epoch:30 step:28181 [D loss: 0.414014, acc.: 79.69%] [G loss: 1.650756]\n",
      "epoch:30 step:28182 [D loss: 0.621076, acc.: 67.19%] [G loss: 1.516222]\n",
      "epoch:30 step:28183 [D loss: 0.559676, acc.: 74.22%] [G loss: 2.055836]\n",
      "epoch:30 step:28184 [D loss: 0.531355, acc.: 78.12%] [G loss: 1.684892]\n",
      "epoch:30 step:28185 [D loss: 0.618360, acc.: 69.53%] [G loss: 1.858839]\n",
      "epoch:30 step:28186 [D loss: 0.575582, acc.: 71.88%] [G loss: 1.451931]\n",
      "epoch:30 step:28187 [D loss: 0.631700, acc.: 70.31%] [G loss: 1.427519]\n",
      "epoch:30 step:28188 [D loss: 0.509547, acc.: 74.22%] [G loss: 1.234136]\n",
      "epoch:30 step:28189 [D loss: 0.623917, acc.: 63.28%] [G loss: 1.203187]\n",
      "epoch:30 step:28190 [D loss: 0.639767, acc.: 68.75%] [G loss: 1.028722]\n",
      "epoch:30 step:28191 [D loss: 0.645792, acc.: 64.84%] [G loss: 1.416570]\n",
      "epoch:30 step:28192 [D loss: 0.354323, acc.: 91.41%] [G loss: 1.233522]\n",
      "epoch:30 step:28193 [D loss: 0.634480, acc.: 60.94%] [G loss: 1.663488]\n",
      "epoch:30 step:28194 [D loss: 0.632779, acc.: 67.97%] [G loss: 1.674759]\n",
      "epoch:30 step:28195 [D loss: 0.704352, acc.: 56.25%] [G loss: 1.157317]\n",
      "epoch:30 step:28196 [D loss: 0.527127, acc.: 77.34%] [G loss: 1.336787]\n",
      "epoch:30 step:28197 [D loss: 0.717655, acc.: 54.69%] [G loss: 1.788254]\n",
      "epoch:30 step:28198 [D loss: 0.500724, acc.: 75.00%] [G loss: 1.208777]\n",
      "epoch:30 step:28199 [D loss: 0.395876, acc.: 82.81%] [G loss: 1.913385]\n",
      "epoch:30 step:28200 [D loss: 0.474310, acc.: 80.47%] [G loss: 1.592638]\n",
      "##############\n",
      "[2.59399084 1.97193719 2.01688844 2.89889835 0.95934879 6.14781968\n",
      " 2.0337424  2.61187979 3.98628641 5.03498885]\n",
      "##########\n",
      "epoch:30 step:28201 [D loss: 0.587224, acc.: 66.41%] [G loss: 1.056279]\n",
      "epoch:30 step:28202 [D loss: 0.371616, acc.: 86.72%] [G loss: 1.426502]\n",
      "epoch:30 step:28203 [D loss: 0.599264, acc.: 71.09%] [G loss: 1.281002]\n",
      "epoch:30 step:28204 [D loss: 0.664378, acc.: 61.72%] [G loss: 1.363816]\n",
      "epoch:30 step:28205 [D loss: 0.630516, acc.: 62.50%] [G loss: 1.487917]\n",
      "epoch:30 step:28206 [D loss: 0.520639, acc.: 75.78%] [G loss: 1.548148]\n",
      "epoch:30 step:28207 [D loss: 0.528692, acc.: 74.22%] [G loss: 1.288214]\n",
      "epoch:30 step:28208 [D loss: 0.492571, acc.: 78.12%] [G loss: 1.227570]\n",
      "epoch:30 step:28209 [D loss: 0.478650, acc.: 80.47%] [G loss: 1.704957]\n",
      "epoch:30 step:28210 [D loss: 0.759071, acc.: 51.56%] [G loss: 1.115103]\n",
      "epoch:30 step:28211 [D loss: 0.459897, acc.: 78.91%] [G loss: 1.341460]\n",
      "epoch:30 step:28212 [D loss: 0.686949, acc.: 63.28%] [G loss: 1.480294]\n",
      "epoch:30 step:28213 [D loss: 0.521112, acc.: 72.66%] [G loss: 1.543072]\n",
      "epoch:30 step:28214 [D loss: 0.450687, acc.: 83.59%] [G loss: 1.413521]\n",
      "epoch:30 step:28215 [D loss: 0.404112, acc.: 81.25%] [G loss: 1.398340]\n",
      "epoch:30 step:28216 [D loss: 0.558640, acc.: 70.31%] [G loss: 1.548486]\n",
      "epoch:30 step:28217 [D loss: 0.684432, acc.: 64.06%] [G loss: 1.246708]\n",
      "epoch:30 step:28218 [D loss: 0.447878, acc.: 79.69%] [G loss: 1.307613]\n",
      "epoch:30 step:28219 [D loss: 0.458153, acc.: 78.91%] [G loss: 1.260873]\n",
      "epoch:30 step:28220 [D loss: 0.638846, acc.: 61.72%] [G loss: 1.712452]\n",
      "epoch:30 step:28221 [D loss: 0.610566, acc.: 64.06%] [G loss: 1.368747]\n",
      "epoch:30 step:28222 [D loss: 0.411763, acc.: 82.81%] [G loss: 1.316701]\n",
      "epoch:30 step:28223 [D loss: 0.547565, acc.: 74.22%] [G loss: 1.214797]\n",
      "epoch:30 step:28224 [D loss: 0.575516, acc.: 68.75%] [G loss: 1.891450]\n",
      "epoch:30 step:28225 [D loss: 0.410387, acc.: 82.03%] [G loss: 1.932090]\n",
      "epoch:30 step:28226 [D loss: 0.485749, acc.: 78.12%] [G loss: 1.282186]\n",
      "epoch:30 step:28227 [D loss: 0.560777, acc.: 73.44%] [G loss: 1.307981]\n",
      "epoch:30 step:28228 [D loss: 0.553823, acc.: 71.09%] [G loss: 1.778496]\n",
      "epoch:30 step:28229 [D loss: 0.413478, acc.: 86.72%] [G loss: 1.331135]\n",
      "epoch:30 step:28230 [D loss: 0.575260, acc.: 68.75%] [G loss: 1.247916]\n",
      "epoch:30 step:28231 [D loss: 0.584808, acc.: 67.97%] [G loss: 1.246034]\n",
      "epoch:30 step:28232 [D loss: 0.501568, acc.: 78.12%] [G loss: 1.749152]\n",
      "epoch:30 step:28233 [D loss: 0.405516, acc.: 81.25%] [G loss: 1.351776]\n",
      "epoch:30 step:28234 [D loss: 0.778409, acc.: 56.25%] [G loss: 2.031092]\n",
      "epoch:30 step:28235 [D loss: 0.656952, acc.: 64.84%] [G loss: 1.345242]\n",
      "epoch:30 step:28236 [D loss: 0.740535, acc.: 55.47%] [G loss: 1.239896]\n",
      "epoch:30 step:28237 [D loss: 0.434463, acc.: 83.59%] [G loss: 1.343510]\n",
      "epoch:30 step:28238 [D loss: 0.533878, acc.: 74.22%] [G loss: 1.122518]\n",
      "epoch:30 step:28239 [D loss: 0.579323, acc.: 72.66%] [G loss: 1.562821]\n",
      "epoch:30 step:28240 [D loss: 0.475329, acc.: 74.22%] [G loss: 1.714440]\n",
      "epoch:30 step:28241 [D loss: 0.589939, acc.: 67.97%] [G loss: 1.481647]\n",
      "epoch:30 step:28242 [D loss: 0.588306, acc.: 67.97%] [G loss: 1.149873]\n",
      "epoch:30 step:28243 [D loss: 0.468417, acc.: 76.56%] [G loss: 1.653653]\n",
      "epoch:30 step:28244 [D loss: 0.499970, acc.: 76.56%] [G loss: 1.870147]\n",
      "epoch:30 step:28245 [D loss: 0.447499, acc.: 82.03%] [G loss: 1.753416]\n",
      "epoch:30 step:28246 [D loss: 0.729669, acc.: 56.25%] [G loss: 1.385348]\n",
      "epoch:30 step:28247 [D loss: 0.554864, acc.: 69.53%] [G loss: 1.455097]\n",
      "epoch:30 step:28248 [D loss: 0.653189, acc.: 64.06%] [G loss: 1.206538]\n",
      "epoch:30 step:28249 [D loss: 0.531075, acc.: 72.66%] [G loss: 1.793059]\n",
      "epoch:30 step:28250 [D loss: 0.461636, acc.: 78.91%] [G loss: 1.706013]\n",
      "epoch:30 step:28251 [D loss: 0.546361, acc.: 74.22%] [G loss: 1.311213]\n",
      "epoch:30 step:28252 [D loss: 0.452706, acc.: 80.47%] [G loss: 1.729113]\n",
      "epoch:30 step:28253 [D loss: 0.494981, acc.: 79.69%] [G loss: 1.666040]\n",
      "epoch:30 step:28254 [D loss: 0.427039, acc.: 80.47%] [G loss: 1.535934]\n",
      "epoch:30 step:28255 [D loss: 0.500040, acc.: 79.69%] [G loss: 1.176451]\n",
      "epoch:30 step:28256 [D loss: 0.420304, acc.: 83.59%] [G loss: 1.546769]\n",
      "epoch:30 step:28257 [D loss: 0.623123, acc.: 68.75%] [G loss: 1.178069]\n",
      "epoch:30 step:28258 [D loss: 0.540634, acc.: 71.88%] [G loss: 1.537038]\n",
      "epoch:30 step:28259 [D loss: 0.400673, acc.: 84.38%] [G loss: 1.201415]\n",
      "epoch:30 step:28260 [D loss: 0.567173, acc.: 71.88%] [G loss: 1.456620]\n",
      "epoch:30 step:28261 [D loss: 0.463397, acc.: 75.00%] [G loss: 2.174043]\n",
      "epoch:30 step:28262 [D loss: 0.811055, acc.: 53.91%] [G loss: 1.123660]\n",
      "epoch:30 step:28263 [D loss: 0.445201, acc.: 82.81%] [G loss: 1.188401]\n",
      "epoch:30 step:28264 [D loss: 0.437911, acc.: 80.47%] [G loss: 1.747967]\n",
      "epoch:30 step:28265 [D loss: 0.662183, acc.: 65.62%] [G loss: 1.572877]\n",
      "epoch:30 step:28266 [D loss: 0.538719, acc.: 73.44%] [G loss: 1.522768]\n",
      "epoch:30 step:28267 [D loss: 0.693240, acc.: 63.28%] [G loss: 1.443572]\n",
      "epoch:30 step:28268 [D loss: 0.585995, acc.: 67.19%] [G loss: 1.539992]\n",
      "epoch:30 step:28269 [D loss: 0.465637, acc.: 80.47%] [G loss: 1.465714]\n",
      "epoch:30 step:28270 [D loss: 0.401835, acc.: 85.94%] [G loss: 1.648754]\n",
      "epoch:30 step:28271 [D loss: 0.417303, acc.: 83.59%] [G loss: 1.624198]\n",
      "epoch:30 step:28272 [D loss: 0.490771, acc.: 78.12%] [G loss: 1.500003]\n",
      "epoch:30 step:28273 [D loss: 0.475685, acc.: 75.00%] [G loss: 1.539726]\n",
      "epoch:30 step:28274 [D loss: 0.505844, acc.: 74.22%] [G loss: 1.841894]\n",
      "epoch:30 step:28275 [D loss: 0.466436, acc.: 75.78%] [G loss: 0.974867]\n",
      "epoch:30 step:28276 [D loss: 0.642823, acc.: 58.59%] [G loss: 1.214857]\n",
      "epoch:30 step:28277 [D loss: 0.454513, acc.: 81.25%] [G loss: 1.705558]\n",
      "epoch:30 step:28278 [D loss: 0.481240, acc.: 80.47%] [G loss: 1.442169]\n",
      "epoch:30 step:28279 [D loss: 0.542014, acc.: 71.09%] [G loss: 1.600023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28280 [D loss: 0.477447, acc.: 79.69%] [G loss: 1.200933]\n",
      "epoch:30 step:28281 [D loss: 0.408015, acc.: 85.16%] [G loss: 1.448891]\n",
      "epoch:30 step:28282 [D loss: 0.344250, acc.: 91.41%] [G loss: 1.496268]\n",
      "epoch:30 step:28283 [D loss: 0.317413, acc.: 85.94%] [G loss: 1.403253]\n",
      "epoch:30 step:28284 [D loss: 0.390515, acc.: 85.16%] [G loss: 1.503528]\n",
      "epoch:30 step:28285 [D loss: 0.613124, acc.: 64.84%] [G loss: 1.638133]\n",
      "epoch:30 step:28286 [D loss: 0.550998, acc.: 71.88%] [G loss: 1.958288]\n",
      "epoch:30 step:28287 [D loss: 0.556388, acc.: 73.44%] [G loss: 1.582559]\n",
      "epoch:30 step:28288 [D loss: 0.442791, acc.: 81.25%] [G loss: 1.348628]\n",
      "epoch:30 step:28289 [D loss: 0.675372, acc.: 59.38%] [G loss: 1.378494]\n",
      "epoch:30 step:28290 [D loss: 0.526798, acc.: 74.22%] [G loss: 1.222734]\n",
      "epoch:30 step:28291 [D loss: 0.762369, acc.: 56.25%] [G loss: 1.769179]\n",
      "epoch:30 step:28292 [D loss: 0.665654, acc.: 60.94%] [G loss: 1.208137]\n",
      "epoch:30 step:28293 [D loss: 0.630905, acc.: 67.19%] [G loss: 1.694276]\n",
      "epoch:30 step:28294 [D loss: 0.768047, acc.: 52.34%] [G loss: 1.381032]\n",
      "epoch:30 step:28295 [D loss: 0.489165, acc.: 75.00%] [G loss: 1.330418]\n",
      "epoch:30 step:28296 [D loss: 0.634652, acc.: 64.84%] [G loss: 1.162592]\n",
      "epoch:30 step:28297 [D loss: 0.506454, acc.: 78.12%] [G loss: 1.616106]\n",
      "epoch:30 step:28298 [D loss: 0.687764, acc.: 63.28%] [G loss: 1.153085]\n",
      "epoch:30 step:28299 [D loss: 0.720111, acc.: 52.34%] [G loss: 1.407025]\n",
      "epoch:30 step:28300 [D loss: 0.629545, acc.: 67.19%] [G loss: 1.341524]\n",
      "epoch:30 step:28301 [D loss: 0.538691, acc.: 70.31%] [G loss: 1.331886]\n",
      "epoch:30 step:28302 [D loss: 0.410175, acc.: 88.28%] [G loss: 1.108625]\n",
      "epoch:30 step:28303 [D loss: 0.613379, acc.: 64.84%] [G loss: 1.560272]\n",
      "epoch:30 step:28304 [D loss: 0.540817, acc.: 76.56%] [G loss: 1.114536]\n",
      "epoch:30 step:28305 [D loss: 0.512384, acc.: 69.53%] [G loss: 1.541225]\n",
      "epoch:30 step:28306 [D loss: 0.552091, acc.: 71.09%] [G loss: 1.353778]\n",
      "epoch:30 step:28307 [D loss: 0.430650, acc.: 82.81%] [G loss: 1.439110]\n",
      "epoch:30 step:28308 [D loss: 0.473658, acc.: 79.69%] [G loss: 1.940607]\n",
      "epoch:30 step:28309 [D loss: 0.587293, acc.: 67.19%] [G loss: 1.332057]\n",
      "epoch:30 step:28310 [D loss: 0.517291, acc.: 72.66%] [G loss: 1.598205]\n",
      "epoch:30 step:28311 [D loss: 0.412913, acc.: 87.50%] [G loss: 1.182798]\n",
      "epoch:30 step:28312 [D loss: 0.592521, acc.: 67.97%] [G loss: 1.352036]\n",
      "epoch:30 step:28313 [D loss: 0.558779, acc.: 69.53%] [G loss: 1.551945]\n",
      "epoch:30 step:28314 [D loss: 0.557973, acc.: 71.88%] [G loss: 1.254840]\n",
      "epoch:30 step:28315 [D loss: 0.725357, acc.: 54.69%] [G loss: 1.156458]\n",
      "epoch:30 step:28316 [D loss: 0.497595, acc.: 74.22%] [G loss: 1.202979]\n",
      "epoch:30 step:28317 [D loss: 0.493676, acc.: 75.78%] [G loss: 1.902943]\n",
      "epoch:30 step:28318 [D loss: 0.536485, acc.: 75.00%] [G loss: 1.121832]\n",
      "epoch:30 step:28319 [D loss: 0.669031, acc.: 63.28%] [G loss: 1.451791]\n",
      "epoch:30 step:28320 [D loss: 0.612415, acc.: 69.53%] [G loss: 1.167521]\n",
      "epoch:30 step:28321 [D loss: 0.376158, acc.: 84.38%] [G loss: 1.499200]\n",
      "epoch:30 step:28322 [D loss: 0.492219, acc.: 75.78%] [G loss: 1.713630]\n",
      "epoch:30 step:28323 [D loss: 0.641243, acc.: 64.84%] [G loss: 1.658030]\n",
      "epoch:30 step:28324 [D loss: 0.591981, acc.: 71.09%] [G loss: 1.462139]\n",
      "epoch:30 step:28325 [D loss: 0.558268, acc.: 71.09%] [G loss: 1.429136]\n",
      "epoch:30 step:28326 [D loss: 0.506314, acc.: 77.34%] [G loss: 1.543156]\n",
      "epoch:30 step:28327 [D loss: 0.513280, acc.: 76.56%] [G loss: 1.065524]\n",
      "epoch:30 step:28328 [D loss: 0.487811, acc.: 78.12%] [G loss: 1.522116]\n",
      "epoch:30 step:28329 [D loss: 0.471747, acc.: 78.91%] [G loss: 1.436302]\n",
      "epoch:30 step:28330 [D loss: 0.581348, acc.: 62.50%] [G loss: 1.048057]\n",
      "epoch:30 step:28331 [D loss: 0.448418, acc.: 80.47%] [G loss: 1.422397]\n",
      "epoch:30 step:28332 [D loss: 0.752963, acc.: 56.25%] [G loss: 1.475495]\n",
      "epoch:30 step:28333 [D loss: 0.400383, acc.: 83.59%] [G loss: 1.543447]\n",
      "epoch:30 step:28334 [D loss: 0.619089, acc.: 66.41%] [G loss: 1.193855]\n",
      "epoch:30 step:28335 [D loss: 0.387699, acc.: 84.38%] [G loss: 1.623517]\n",
      "epoch:30 step:28336 [D loss: 0.459706, acc.: 79.69%] [G loss: 1.683188]\n",
      "epoch:30 step:28337 [D loss: 0.412085, acc.: 88.28%] [G loss: 1.031704]\n",
      "epoch:30 step:28338 [D loss: 0.566051, acc.: 71.88%] [G loss: 1.088613]\n",
      "epoch:30 step:28339 [D loss: 0.625102, acc.: 68.75%] [G loss: 1.280219]\n",
      "epoch:30 step:28340 [D loss: 0.576597, acc.: 67.97%] [G loss: 1.144052]\n",
      "epoch:30 step:28341 [D loss: 0.681066, acc.: 60.16%] [G loss: 1.088887]\n",
      "epoch:30 step:28342 [D loss: 0.542946, acc.: 73.44%] [G loss: 1.637519]\n",
      "epoch:30 step:28343 [D loss: 0.696338, acc.: 59.38%] [G loss: 1.371026]\n",
      "epoch:30 step:28344 [D loss: 0.647831, acc.: 64.06%] [G loss: 1.997844]\n",
      "epoch:30 step:28345 [D loss: 0.720205, acc.: 59.38%] [G loss: 1.397964]\n",
      "epoch:30 step:28346 [D loss: 0.425882, acc.: 80.47%] [G loss: 1.900237]\n",
      "epoch:30 step:28347 [D loss: 0.538270, acc.: 72.66%] [G loss: 1.444545]\n",
      "epoch:30 step:28348 [D loss: 0.648874, acc.: 64.84%] [G loss: 1.475188]\n",
      "epoch:30 step:28349 [D loss: 0.506886, acc.: 78.91%] [G loss: 1.425825]\n",
      "epoch:30 step:28350 [D loss: 0.712605, acc.: 57.81%] [G loss: 1.496406]\n",
      "epoch:30 step:28351 [D loss: 0.504649, acc.: 72.66%] [G loss: 1.423908]\n",
      "epoch:30 step:28352 [D loss: 0.499038, acc.: 73.44%] [G loss: 1.709406]\n",
      "epoch:30 step:28353 [D loss: 0.644584, acc.: 64.06%] [G loss: 1.208927]\n",
      "epoch:30 step:28354 [D loss: 0.464013, acc.: 87.50%] [G loss: 1.000499]\n",
      "epoch:30 step:28355 [D loss: 0.509587, acc.: 72.66%] [G loss: 1.817148]\n",
      "epoch:30 step:28356 [D loss: 0.498520, acc.: 79.69%] [G loss: 1.685381]\n",
      "epoch:30 step:28357 [D loss: 0.539023, acc.: 75.00%] [G loss: 1.630072]\n",
      "epoch:30 step:28358 [D loss: 0.510035, acc.: 78.12%] [G loss: 1.285074]\n",
      "epoch:30 step:28359 [D loss: 0.264531, acc.: 94.53%] [G loss: 1.597316]\n",
      "epoch:30 step:28360 [D loss: 0.652206, acc.: 59.38%] [G loss: 1.628315]\n",
      "epoch:30 step:28361 [D loss: 0.451654, acc.: 78.91%] [G loss: 1.945370]\n",
      "epoch:30 step:28362 [D loss: 0.487695, acc.: 78.12%] [G loss: 1.477571]\n",
      "epoch:30 step:28363 [D loss: 0.491821, acc.: 76.56%] [G loss: 1.418922]\n",
      "epoch:30 step:28364 [D loss: 0.722985, acc.: 58.59%] [G loss: 1.416265]\n",
      "epoch:30 step:28365 [D loss: 0.588794, acc.: 69.53%] [G loss: 1.530869]\n",
      "epoch:30 step:28366 [D loss: 0.536336, acc.: 69.53%] [G loss: 1.421312]\n",
      "epoch:30 step:28367 [D loss: 0.631449, acc.: 64.84%] [G loss: 1.267909]\n",
      "epoch:30 step:28368 [D loss: 0.612154, acc.: 67.19%] [G loss: 1.684083]\n",
      "epoch:30 step:28369 [D loss: 0.698581, acc.: 62.50%] [G loss: 0.981896]\n",
      "epoch:30 step:28370 [D loss: 0.486920, acc.: 76.56%] [G loss: 1.634878]\n",
      "epoch:30 step:28371 [D loss: 0.468460, acc.: 76.56%] [G loss: 1.491803]\n",
      "epoch:30 step:28372 [D loss: 0.573453, acc.: 70.31%] [G loss: 1.539364]\n",
      "epoch:30 step:28373 [D loss: 0.577238, acc.: 70.31%] [G loss: 1.245470]\n",
      "epoch:30 step:28374 [D loss: 0.693244, acc.: 57.81%] [G loss: 1.286382]\n",
      "epoch:30 step:28375 [D loss: 0.426896, acc.: 78.91%] [G loss: 1.927464]\n",
      "epoch:30 step:28376 [D loss: 0.477005, acc.: 76.56%] [G loss: 1.506573]\n",
      "epoch:30 step:28377 [D loss: 0.453818, acc.: 80.47%] [G loss: 1.432831]\n",
      "epoch:30 step:28378 [D loss: 0.353789, acc.: 89.84%] [G loss: 1.436295]\n",
      "epoch:30 step:28379 [D loss: 0.460502, acc.: 78.91%] [G loss: 1.794698]\n",
      "epoch:30 step:28380 [D loss: 0.532462, acc.: 75.00%] [G loss: 1.220478]\n",
      "epoch:30 step:28381 [D loss: 0.295418, acc.: 92.19%] [G loss: 1.675302]\n",
      "epoch:30 step:28382 [D loss: 0.841110, acc.: 48.44%] [G loss: 0.917478]\n",
      "epoch:30 step:28383 [D loss: 0.435180, acc.: 82.03%] [G loss: 1.875607]\n",
      "epoch:30 step:28384 [D loss: 0.398627, acc.: 84.38%] [G loss: 1.728583]\n",
      "epoch:30 step:28385 [D loss: 0.708246, acc.: 60.16%] [G loss: 1.005363]\n",
      "epoch:30 step:28386 [D loss: 0.698582, acc.: 59.38%] [G loss: 0.983258]\n",
      "epoch:30 step:28387 [D loss: 0.555528, acc.: 70.31%] [G loss: 1.550138]\n",
      "epoch:30 step:28388 [D loss: 0.658227, acc.: 64.06%] [G loss: 1.585688]\n",
      "epoch:30 step:28389 [D loss: 0.663697, acc.: 60.16%] [G loss: 0.980251]\n",
      "epoch:30 step:28390 [D loss: 0.720387, acc.: 63.28%] [G loss: 1.265874]\n",
      "epoch:30 step:28391 [D loss: 0.450600, acc.: 77.34%] [G loss: 1.535290]\n",
      "epoch:30 step:28392 [D loss: 0.576049, acc.: 71.09%] [G loss: 1.543251]\n",
      "epoch:30 step:28393 [D loss: 0.495976, acc.: 78.12%] [G loss: 1.382541]\n",
      "epoch:30 step:28394 [D loss: 0.506235, acc.: 75.78%] [G loss: 1.342502]\n",
      "epoch:30 step:28395 [D loss: 0.397766, acc.: 86.72%] [G loss: 1.942385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28396 [D loss: 0.731946, acc.: 57.03%] [G loss: 0.911158]\n",
      "epoch:30 step:28397 [D loss: 0.515584, acc.: 80.47%] [G loss: 1.550488]\n",
      "epoch:30 step:28398 [D loss: 0.744641, acc.: 53.91%] [G loss: 1.307910]\n",
      "epoch:30 step:28399 [D loss: 0.648528, acc.: 65.62%] [G loss: 1.333854]\n",
      "epoch:30 step:28400 [D loss: 0.325080, acc.: 90.62%] [G loss: 1.380146]\n",
      "##############\n",
      "[2.65579907 2.08989808 1.84989536 3.15682188 0.87543003 6.24337365\n",
      " 2.293526   2.93967045 3.97469931 4.73797418]\n",
      "##########\n",
      "epoch:30 step:28401 [D loss: 0.597222, acc.: 64.06%] [G loss: 1.363198]\n",
      "epoch:30 step:28402 [D loss: 0.702758, acc.: 57.81%] [G loss: 1.667996]\n",
      "epoch:30 step:28403 [D loss: 0.823529, acc.: 50.00%] [G loss: 1.447989]\n",
      "epoch:30 step:28404 [D loss: 0.574688, acc.: 67.19%] [G loss: 1.339613]\n",
      "epoch:30 step:28405 [D loss: 0.589068, acc.: 73.44%] [G loss: 1.576948]\n",
      "epoch:30 step:28406 [D loss: 0.471646, acc.: 75.78%] [G loss: 1.671427]\n",
      "epoch:30 step:28407 [D loss: 0.499359, acc.: 75.00%] [G loss: 1.824013]\n",
      "epoch:30 step:28408 [D loss: 0.610813, acc.: 69.53%] [G loss: 1.544863]\n",
      "epoch:30 step:28409 [D loss: 0.572196, acc.: 73.44%] [G loss: 1.391397]\n",
      "epoch:30 step:28410 [D loss: 0.343488, acc.: 91.41%] [G loss: 1.715691]\n",
      "epoch:30 step:28411 [D loss: 0.655699, acc.: 62.50%] [G loss: 1.956987]\n",
      "epoch:30 step:28412 [D loss: 0.547227, acc.: 75.78%] [G loss: 1.651895]\n",
      "epoch:30 step:28413 [D loss: 0.603303, acc.: 71.09%] [G loss: 1.717920]\n",
      "epoch:30 step:28414 [D loss: 0.419830, acc.: 81.25%] [G loss: 1.357043]\n",
      "epoch:30 step:28415 [D loss: 0.566252, acc.: 70.31%] [G loss: 1.269716]\n",
      "epoch:30 step:28416 [D loss: 0.550088, acc.: 72.66%] [G loss: 1.298297]\n",
      "epoch:30 step:28417 [D loss: 0.592613, acc.: 64.06%] [G loss: 1.049086]\n",
      "epoch:30 step:28418 [D loss: 0.626632, acc.: 60.94%] [G loss: 1.319070]\n",
      "epoch:30 step:28419 [D loss: 0.719409, acc.: 59.38%] [G loss: 1.326144]\n",
      "epoch:30 step:28420 [D loss: 0.503598, acc.: 79.69%] [G loss: 1.914215]\n",
      "epoch:30 step:28421 [D loss: 0.320939, acc.: 90.62%] [G loss: 1.535831]\n",
      "epoch:30 step:28422 [D loss: 0.703484, acc.: 60.16%] [G loss: 1.699641]\n",
      "epoch:30 step:28423 [D loss: 0.572811, acc.: 68.75%] [G loss: 1.779509]\n",
      "epoch:30 step:28424 [D loss: 0.507369, acc.: 77.34%] [G loss: 1.816457]\n",
      "epoch:30 step:28425 [D loss: 0.402591, acc.: 85.16%] [G loss: 1.163115]\n",
      "epoch:30 step:28426 [D loss: 0.486169, acc.: 72.66%] [G loss: 1.356047]\n",
      "epoch:30 step:28427 [D loss: 0.404039, acc.: 88.28%] [G loss: 1.444158]\n",
      "epoch:30 step:28428 [D loss: 0.498377, acc.: 75.78%] [G loss: 1.664215]\n",
      "epoch:30 step:28429 [D loss: 0.579438, acc.: 65.62%] [G loss: 1.204263]\n",
      "epoch:30 step:28430 [D loss: 0.480720, acc.: 75.78%] [G loss: 1.607242]\n",
      "epoch:30 step:28431 [D loss: 0.751380, acc.: 53.91%] [G loss: 1.673514]\n",
      "epoch:30 step:28432 [D loss: 0.500127, acc.: 75.00%] [G loss: 1.193375]\n",
      "epoch:30 step:28433 [D loss: 0.746388, acc.: 57.81%] [G loss: 1.245551]\n",
      "epoch:30 step:28434 [D loss: 0.471511, acc.: 78.91%] [G loss: 1.368048]\n",
      "epoch:30 step:28435 [D loss: 0.439484, acc.: 79.69%] [G loss: 1.491731]\n",
      "epoch:30 step:28436 [D loss: 0.554846, acc.: 72.66%] [G loss: 1.494994]\n",
      "epoch:30 step:28437 [D loss: 0.412889, acc.: 82.03%] [G loss: 1.598711]\n",
      "epoch:30 step:28438 [D loss: 0.908544, acc.: 44.53%] [G loss: 0.974780]\n",
      "epoch:30 step:28439 [D loss: 0.486006, acc.: 76.56%] [G loss: 1.802919]\n",
      "epoch:30 step:28440 [D loss: 0.556840, acc.: 69.53%] [G loss: 1.341984]\n",
      "epoch:30 step:28441 [D loss: 0.686876, acc.: 65.62%] [G loss: 1.233013]\n",
      "epoch:30 step:28442 [D loss: 0.445199, acc.: 76.56%] [G loss: 1.518678]\n",
      "epoch:30 step:28443 [D loss: 0.637926, acc.: 67.19%] [G loss: 1.716722]\n",
      "epoch:30 step:28444 [D loss: 0.469201, acc.: 80.47%] [G loss: 1.993933]\n",
      "epoch:30 step:28445 [D loss: 0.532135, acc.: 70.31%] [G loss: 1.702240]\n",
      "epoch:30 step:28446 [D loss: 0.439156, acc.: 82.81%] [G loss: 1.652735]\n",
      "epoch:30 step:28447 [D loss: 0.743187, acc.: 56.25%] [G loss: 1.694131]\n",
      "epoch:30 step:28448 [D loss: 0.440394, acc.: 83.59%] [G loss: 1.782747]\n",
      "epoch:30 step:28449 [D loss: 0.481324, acc.: 75.00%] [G loss: 1.332974]\n",
      "epoch:30 step:28450 [D loss: 0.376979, acc.: 85.16%] [G loss: 1.384303]\n",
      "epoch:30 step:28451 [D loss: 0.392208, acc.: 84.38%] [G loss: 1.312215]\n",
      "epoch:30 step:28452 [D loss: 0.581556, acc.: 67.19%] [G loss: 1.337646]\n",
      "epoch:30 step:28453 [D loss: 0.523958, acc.: 74.22%] [G loss: 1.041448]\n",
      "epoch:30 step:28454 [D loss: 0.476451, acc.: 76.56%] [G loss: 1.109339]\n",
      "epoch:30 step:28455 [D loss: 0.476403, acc.: 75.78%] [G loss: 1.620177]\n",
      "epoch:30 step:28456 [D loss: 0.620711, acc.: 64.84%] [G loss: 1.442817]\n",
      "epoch:30 step:28457 [D loss: 0.719800, acc.: 56.25%] [G loss: 1.324050]\n",
      "epoch:30 step:28458 [D loss: 0.614811, acc.: 63.28%] [G loss: 1.248022]\n",
      "epoch:30 step:28459 [D loss: 0.356918, acc.: 87.50%] [G loss: 1.852057]\n",
      "epoch:30 step:28460 [D loss: 0.385957, acc.: 82.03%] [G loss: 1.335995]\n",
      "epoch:30 step:28461 [D loss: 0.555489, acc.: 71.88%] [G loss: 1.597867]\n",
      "epoch:30 step:28462 [D loss: 0.353926, acc.: 89.84%] [G loss: 1.734096]\n",
      "epoch:30 step:28463 [D loss: 0.523519, acc.: 71.09%] [G loss: 1.147011]\n",
      "epoch:30 step:28464 [D loss: 0.467439, acc.: 77.34%] [G loss: 1.315524]\n",
      "epoch:30 step:28465 [D loss: 0.640376, acc.: 63.28%] [G loss: 1.241954]\n",
      "epoch:30 step:28466 [D loss: 0.295375, acc.: 92.19%] [G loss: 1.640407]\n",
      "epoch:30 step:28467 [D loss: 0.483010, acc.: 78.12%] [G loss: 1.557290]\n",
      "epoch:30 step:28468 [D loss: 0.507641, acc.: 74.22%] [G loss: 1.274616]\n",
      "epoch:30 step:28469 [D loss: 0.663424, acc.: 63.28%] [G loss: 1.091991]\n",
      "epoch:30 step:28470 [D loss: 0.445680, acc.: 79.69%] [G loss: 1.167124]\n",
      "epoch:30 step:28471 [D loss: 0.544699, acc.: 75.00%] [G loss: 1.300573]\n",
      "epoch:30 step:28472 [D loss: 0.378994, acc.: 87.50%] [G loss: 2.081216]\n",
      "epoch:30 step:28473 [D loss: 0.450243, acc.: 78.91%] [G loss: 1.890288]\n",
      "epoch:30 step:28474 [D loss: 0.609551, acc.: 70.31%] [G loss: 1.533691]\n",
      "epoch:30 step:28475 [D loss: 0.490553, acc.: 75.78%] [G loss: 2.111496]\n",
      "epoch:30 step:28476 [D loss: 0.338375, acc.: 89.06%] [G loss: 1.836754]\n",
      "epoch:30 step:28477 [D loss: 0.739809, acc.: 55.47%] [G loss: 1.187680]\n",
      "epoch:30 step:28478 [D loss: 0.600645, acc.: 67.97%] [G loss: 1.068264]\n",
      "epoch:30 step:28479 [D loss: 0.709952, acc.: 63.28%] [G loss: 1.521734]\n",
      "epoch:30 step:28480 [D loss: 0.384900, acc.: 85.16%] [G loss: 1.167472]\n",
      "epoch:30 step:28481 [D loss: 0.553270, acc.: 72.66%] [G loss: 1.834519]\n",
      "epoch:30 step:28482 [D loss: 0.500555, acc.: 75.78%] [G loss: 1.533483]\n",
      "epoch:30 step:28483 [D loss: 0.531738, acc.: 71.09%] [G loss: 1.521469]\n",
      "epoch:30 step:28484 [D loss: 0.748499, acc.: 53.91%] [G loss: 1.300739]\n",
      "epoch:30 step:28485 [D loss: 0.637259, acc.: 66.41%] [G loss: 1.328763]\n",
      "epoch:30 step:28486 [D loss: 0.836765, acc.: 50.00%] [G loss: 1.590546]\n",
      "epoch:30 step:28487 [D loss: 0.606924, acc.: 69.53%] [G loss: 1.601217]\n",
      "epoch:30 step:28488 [D loss: 0.374581, acc.: 88.28%] [G loss: 1.395585]\n",
      "epoch:30 step:28489 [D loss: 0.480883, acc.: 78.12%] [G loss: 1.387748]\n",
      "epoch:30 step:28490 [D loss: 0.550708, acc.: 72.66%] [G loss: 1.180393]\n",
      "epoch:30 step:28491 [D loss: 0.611988, acc.: 67.19%] [G loss: 1.487069]\n",
      "epoch:30 step:28492 [D loss: 0.629585, acc.: 63.28%] [G loss: 1.060258]\n",
      "epoch:30 step:28493 [D loss: 0.571694, acc.: 64.84%] [G loss: 1.245309]\n",
      "epoch:30 step:28494 [D loss: 0.509806, acc.: 74.22%] [G loss: 1.423656]\n",
      "epoch:30 step:28495 [D loss: 0.731691, acc.: 48.44%] [G loss: 1.423985]\n",
      "epoch:30 step:28496 [D loss: 0.450655, acc.: 82.81%] [G loss: 1.735677]\n",
      "epoch:30 step:28497 [D loss: 0.397598, acc.: 85.16%] [G loss: 1.635774]\n",
      "epoch:30 step:28498 [D loss: 0.551575, acc.: 71.09%] [G loss: 1.293588]\n",
      "epoch:30 step:28499 [D loss: 0.517615, acc.: 71.88%] [G loss: 1.173302]\n",
      "epoch:30 step:28500 [D loss: 0.444240, acc.: 78.12%] [G loss: 1.330292]\n",
      "epoch:30 step:28501 [D loss: 0.649545, acc.: 63.28%] [G loss: 1.060958]\n",
      "epoch:30 step:28502 [D loss: 0.438122, acc.: 77.34%] [G loss: 1.187968]\n",
      "epoch:30 step:28503 [D loss: 0.526911, acc.: 72.66%] [G loss: 1.457575]\n",
      "epoch:30 step:28504 [D loss: 0.592947, acc.: 72.66%] [G loss: 1.507227]\n",
      "epoch:30 step:28505 [D loss: 0.622788, acc.: 62.50%] [G loss: 1.827569]\n",
      "epoch:30 step:28506 [D loss: 0.603047, acc.: 63.28%] [G loss: 1.700263]\n",
      "epoch:30 step:28507 [D loss: 0.622380, acc.: 66.41%] [G loss: 1.679510]\n",
      "epoch:30 step:28508 [D loss: 0.455783, acc.: 77.34%] [G loss: 1.204806]\n",
      "epoch:30 step:28509 [D loss: 0.522774, acc.: 71.09%] [G loss: 1.614560]\n",
      "epoch:30 step:28510 [D loss: 0.544268, acc.: 69.53%] [G loss: 1.620377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28511 [D loss: 0.510079, acc.: 72.66%] [G loss: 1.436785]\n",
      "epoch:30 step:28512 [D loss: 0.624946, acc.: 67.19%] [G loss: 0.968148]\n",
      "epoch:30 step:28513 [D loss: 0.590054, acc.: 67.19%] [G loss: 1.863765]\n",
      "epoch:30 step:28514 [D loss: 0.537382, acc.: 75.00%] [G loss: 1.594691]\n",
      "epoch:30 step:28515 [D loss: 0.592772, acc.: 69.53%] [G loss: 1.756514]\n",
      "epoch:30 step:28516 [D loss: 0.526641, acc.: 72.66%] [G loss: 1.913284]\n",
      "epoch:30 step:28517 [D loss: 0.358928, acc.: 88.28%] [G loss: 1.790351]\n",
      "epoch:30 step:28518 [D loss: 0.412101, acc.: 81.25%] [G loss: 1.894269]\n",
      "epoch:30 step:28519 [D loss: 0.573458, acc.: 64.06%] [G loss: 1.555312]\n",
      "epoch:30 step:28520 [D loss: 0.563575, acc.: 69.53%] [G loss: 1.269967]\n",
      "epoch:30 step:28521 [D loss: 0.468790, acc.: 78.12%] [G loss: 1.284811]\n",
      "epoch:30 step:28522 [D loss: 0.517270, acc.: 76.56%] [G loss: 2.137341]\n",
      "epoch:30 step:28523 [D loss: 0.606572, acc.: 71.09%] [G loss: 1.530918]\n",
      "epoch:30 step:28524 [D loss: 0.447021, acc.: 82.81%] [G loss: 1.732798]\n",
      "epoch:30 step:28525 [D loss: 0.634996, acc.: 64.84%] [G loss: 1.250304]\n",
      "epoch:30 step:28526 [D loss: 0.495773, acc.: 75.78%] [G loss: 1.310993]\n",
      "epoch:30 step:28527 [D loss: 0.521107, acc.: 76.56%] [G loss: 1.854468]\n",
      "epoch:30 step:28528 [D loss: 0.488954, acc.: 75.78%] [G loss: 1.546757]\n",
      "epoch:30 step:28529 [D loss: 0.565332, acc.: 69.53%] [G loss: 1.467856]\n",
      "epoch:30 step:28530 [D loss: 0.374992, acc.: 85.16%] [G loss: 1.597143]\n",
      "epoch:30 step:28531 [D loss: 0.559630, acc.: 72.66%] [G loss: 1.301571]\n",
      "epoch:30 step:28532 [D loss: 0.557608, acc.: 69.53%] [G loss: 1.863361]\n",
      "epoch:30 step:28533 [D loss: 0.631816, acc.: 67.97%] [G loss: 1.257230]\n",
      "epoch:30 step:28534 [D loss: 0.384842, acc.: 83.59%] [G loss: 1.182453]\n",
      "epoch:30 step:28535 [D loss: 0.562682, acc.: 74.22%] [G loss: 1.481235]\n",
      "epoch:30 step:28536 [D loss: 0.656971, acc.: 60.94%] [G loss: 1.601463]\n",
      "epoch:30 step:28537 [D loss: 0.619139, acc.: 67.19%] [G loss: 1.709978]\n",
      "epoch:30 step:28538 [D loss: 0.504503, acc.: 78.91%] [G loss: 1.422055]\n",
      "epoch:30 step:28539 [D loss: 0.505622, acc.: 75.00%] [G loss: 1.103835]\n",
      "epoch:30 step:28540 [D loss: 0.572324, acc.: 71.09%] [G loss: 1.482139]\n",
      "epoch:30 step:28541 [D loss: 0.545180, acc.: 68.75%] [G loss: 1.433833]\n",
      "epoch:30 step:28542 [D loss: 0.391229, acc.: 83.59%] [G loss: 1.706770]\n",
      "epoch:30 step:28543 [D loss: 0.579958, acc.: 69.53%] [G loss: 1.415327]\n",
      "epoch:30 step:28544 [D loss: 0.609901, acc.: 65.62%] [G loss: 1.400269]\n",
      "epoch:30 step:28545 [D loss: 0.415538, acc.: 82.03%] [G loss: 1.885090]\n",
      "epoch:30 step:28546 [D loss: 0.671641, acc.: 63.28%] [G loss: 1.474918]\n",
      "epoch:30 step:28547 [D loss: 0.644209, acc.: 64.06%] [G loss: 1.912757]\n",
      "epoch:30 step:28548 [D loss: 0.483302, acc.: 78.12%] [G loss: 1.475730]\n",
      "epoch:30 step:28549 [D loss: 0.553325, acc.: 67.19%] [G loss: 1.443767]\n",
      "epoch:30 step:28550 [D loss: 0.531652, acc.: 71.88%] [G loss: 1.718716]\n",
      "epoch:30 step:28551 [D loss: 0.460222, acc.: 78.12%] [G loss: 1.308163]\n",
      "epoch:30 step:28552 [D loss: 0.629391, acc.: 65.62%] [G loss: 0.951086]\n",
      "epoch:30 step:28553 [D loss: 0.455945, acc.: 78.91%] [G loss: 1.838840]\n",
      "epoch:30 step:28554 [D loss: 0.643394, acc.: 65.62%] [G loss: 1.125365]\n",
      "epoch:30 step:28555 [D loss: 0.545815, acc.: 67.97%] [G loss: 1.815156]\n",
      "epoch:30 step:28556 [D loss: 0.391943, acc.: 85.16%] [G loss: 1.756437]\n",
      "epoch:30 step:28557 [D loss: 0.363894, acc.: 90.62%] [G loss: 2.336454]\n",
      "epoch:30 step:28558 [D loss: 0.446051, acc.: 78.91%] [G loss: 1.484399]\n",
      "epoch:30 step:28559 [D loss: 0.612983, acc.: 65.62%] [G loss: 1.300584]\n",
      "epoch:30 step:28560 [D loss: 0.752102, acc.: 53.12%] [G loss: 0.987520]\n",
      "epoch:30 step:28561 [D loss: 0.595060, acc.: 64.84%] [G loss: 1.615688]\n",
      "epoch:30 step:28562 [D loss: 0.547393, acc.: 68.75%] [G loss: 1.784391]\n",
      "epoch:30 step:28563 [D loss: 0.279127, acc.: 94.53%] [G loss: 1.660134]\n",
      "epoch:30 step:28564 [D loss: 0.523139, acc.: 74.22%] [G loss: 1.370173]\n",
      "epoch:30 step:28565 [D loss: 0.606293, acc.: 64.84%] [G loss: 1.994178]\n",
      "epoch:30 step:28566 [D loss: 0.759997, acc.: 59.38%] [G loss: 1.324327]\n",
      "epoch:30 step:28567 [D loss: 0.537780, acc.: 77.34%] [G loss: 1.224678]\n",
      "epoch:30 step:28568 [D loss: 0.402434, acc.: 83.59%] [G loss: 1.738398]\n",
      "epoch:30 step:28569 [D loss: 0.443374, acc.: 79.69%] [G loss: 2.115301]\n",
      "epoch:30 step:28570 [D loss: 0.434793, acc.: 83.59%] [G loss: 1.281027]\n",
      "epoch:30 step:28571 [D loss: 0.686749, acc.: 57.03%] [G loss: 1.334401]\n",
      "epoch:30 step:28572 [D loss: 0.645681, acc.: 57.03%] [G loss: 1.613796]\n",
      "epoch:30 step:28573 [D loss: 0.449785, acc.: 81.25%] [G loss: 1.049449]\n",
      "epoch:30 step:28574 [D loss: 0.420033, acc.: 82.03%] [G loss: 1.883753]\n",
      "epoch:30 step:28575 [D loss: 0.536497, acc.: 74.22%] [G loss: 1.551832]\n",
      "epoch:30 step:28576 [D loss: 0.475552, acc.: 78.91%] [G loss: 1.651327]\n",
      "epoch:30 step:28577 [D loss: 0.458739, acc.: 79.69%] [G loss: 1.444037]\n",
      "epoch:30 step:28578 [D loss: 0.569601, acc.: 71.09%] [G loss: 1.815249]\n",
      "epoch:30 step:28579 [D loss: 0.512433, acc.: 76.56%] [G loss: 1.628427]\n",
      "epoch:30 step:28580 [D loss: 0.475060, acc.: 78.12%] [G loss: 1.243317]\n",
      "epoch:30 step:28581 [D loss: 0.461829, acc.: 82.03%] [G loss: 1.284620]\n",
      "epoch:30 step:28582 [D loss: 0.737473, acc.: 56.25%] [G loss: 1.497811]\n",
      "epoch:30 step:28583 [D loss: 0.378371, acc.: 89.84%] [G loss: 1.394867]\n",
      "epoch:30 step:28584 [D loss: 0.539220, acc.: 73.44%] [G loss: 1.598055]\n",
      "epoch:30 step:28585 [D loss: 0.494539, acc.: 78.91%] [G loss: 1.711558]\n",
      "epoch:30 step:28586 [D loss: 0.431479, acc.: 82.03%] [G loss: 1.541015]\n",
      "epoch:30 step:28587 [D loss: 0.533014, acc.: 73.44%] [G loss: 1.330358]\n",
      "epoch:30 step:28588 [D loss: 0.451846, acc.: 82.03%] [G loss: 1.657856]\n",
      "epoch:30 step:28589 [D loss: 0.497412, acc.: 72.66%] [G loss: 1.332064]\n",
      "epoch:30 step:28590 [D loss: 0.454757, acc.: 78.91%] [G loss: 1.598711]\n",
      "epoch:30 step:28591 [D loss: 0.684682, acc.: 57.81%] [G loss: 1.187302]\n",
      "epoch:30 step:28592 [D loss: 0.493715, acc.: 75.78%] [G loss: 1.400074]\n",
      "epoch:30 step:28593 [D loss: 0.482101, acc.: 78.91%] [G loss: 1.388279]\n",
      "epoch:30 step:28594 [D loss: 0.614370, acc.: 66.41%] [G loss: 1.560101]\n",
      "epoch:30 step:28595 [D loss: 0.621374, acc.: 65.62%] [G loss: 1.652533]\n",
      "epoch:30 step:28596 [D loss: 0.483644, acc.: 75.78%] [G loss: 1.293178]\n",
      "epoch:30 step:28597 [D loss: 0.726238, acc.: 60.16%] [G loss: 1.657797]\n",
      "epoch:30 step:28598 [D loss: 0.390166, acc.: 86.72%] [G loss: 1.722021]\n",
      "epoch:30 step:28599 [D loss: 0.622948, acc.: 64.06%] [G loss: 2.106012]\n",
      "epoch:30 step:28600 [D loss: 0.573813, acc.: 71.88%] [G loss: 1.684713]\n",
      "##############\n",
      "[2.65502979 2.20524221 1.93994115 2.99827594 0.94312968 6.48617982\n",
      " 2.01262581 2.88293425 3.86718455 8.14868929]\n",
      "##########\n",
      "epoch:30 step:28601 [D loss: 0.450271, acc.: 79.69%] [G loss: 1.474089]\n",
      "epoch:30 step:28602 [D loss: 0.654446, acc.: 64.06%] [G loss: 1.111180]\n",
      "epoch:30 step:28603 [D loss: 0.449023, acc.: 79.69%] [G loss: 1.268714]\n",
      "epoch:30 step:28604 [D loss: 0.522395, acc.: 71.09%] [G loss: 1.375153]\n",
      "epoch:30 step:28605 [D loss: 0.528534, acc.: 78.91%] [G loss: 1.608379]\n",
      "epoch:30 step:28606 [D loss: 0.690306, acc.: 59.38%] [G loss: 1.299912]\n",
      "epoch:30 step:28607 [D loss: 0.522315, acc.: 69.53%] [G loss: 1.269677]\n",
      "epoch:30 step:28608 [D loss: 0.635562, acc.: 68.75%] [G loss: 1.228089]\n",
      "epoch:30 step:28609 [D loss: 0.635352, acc.: 63.28%] [G loss: 1.217313]\n",
      "epoch:30 step:28610 [D loss: 0.548297, acc.: 74.22%] [G loss: 1.489481]\n",
      "epoch:30 step:28611 [D loss: 0.617540, acc.: 64.84%] [G loss: 1.536930]\n",
      "epoch:30 step:28612 [D loss: 0.507415, acc.: 75.78%] [G loss: 1.715217]\n",
      "epoch:30 step:28613 [D loss: 0.608474, acc.: 66.41%] [G loss: 1.417035]\n",
      "epoch:30 step:28614 [D loss: 0.540954, acc.: 74.22%] [G loss: 1.391190]\n",
      "epoch:30 step:28615 [D loss: 0.390973, acc.: 81.25%] [G loss: 1.822649]\n",
      "epoch:30 step:28616 [D loss: 0.607017, acc.: 70.31%] [G loss: 1.292846]\n",
      "epoch:30 step:28617 [D loss: 0.416271, acc.: 83.59%] [G loss: 1.513250]\n",
      "epoch:30 step:28618 [D loss: 0.599005, acc.: 71.88%] [G loss: 1.147864]\n",
      "epoch:30 step:28619 [D loss: 0.617466, acc.: 64.84%] [G loss: 1.082872]\n",
      "epoch:30 step:28620 [D loss: 0.590770, acc.: 67.19%] [G loss: 1.158469]\n",
      "epoch:30 step:28621 [D loss: 0.489737, acc.: 78.91%] [G loss: 1.426773]\n",
      "epoch:30 step:28622 [D loss: 0.454245, acc.: 81.25%] [G loss: 1.618593]\n",
      "epoch:30 step:28623 [D loss: 0.467224, acc.: 82.03%] [G loss: 1.171022]\n",
      "epoch:30 step:28624 [D loss: 0.553511, acc.: 77.34%] [G loss: 1.103444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28625 [D loss: 0.660420, acc.: 60.94%] [G loss: 1.916127]\n",
      "epoch:30 step:28626 [D loss: 0.403741, acc.: 85.16%] [G loss: 1.661563]\n",
      "epoch:30 step:28627 [D loss: 0.892483, acc.: 42.97%] [G loss: 1.221861]\n",
      "epoch:30 step:28628 [D loss: 0.459071, acc.: 82.81%] [G loss: 1.687910]\n",
      "epoch:30 step:28629 [D loss: 0.389256, acc.: 87.50%] [G loss: 1.512189]\n",
      "epoch:30 step:28630 [D loss: 0.453127, acc.: 82.81%] [G loss: 1.155237]\n",
      "epoch:30 step:28631 [D loss: 0.324354, acc.: 89.06%] [G loss: 1.659918]\n",
      "epoch:30 step:28632 [D loss: 0.722611, acc.: 56.25%] [G loss: 1.091985]\n",
      "epoch:30 step:28633 [D loss: 0.591178, acc.: 60.94%] [G loss: 1.402302]\n",
      "epoch:30 step:28634 [D loss: 0.520495, acc.: 75.00%] [G loss: 1.193677]\n",
      "epoch:30 step:28635 [D loss: 0.433838, acc.: 78.12%] [G loss: 1.694388]\n",
      "epoch:30 step:28636 [D loss: 0.513926, acc.: 81.25%] [G loss: 1.871894]\n",
      "epoch:30 step:28637 [D loss: 0.487402, acc.: 78.91%] [G loss: 1.299402]\n",
      "epoch:30 step:28638 [D loss: 0.585826, acc.: 67.97%] [G loss: 1.114646]\n",
      "epoch:30 step:28639 [D loss: 0.450403, acc.: 80.47%] [G loss: 1.569125]\n",
      "epoch:30 step:28640 [D loss: 0.533101, acc.: 69.53%] [G loss: 1.719711]\n",
      "epoch:30 step:28641 [D loss: 0.597891, acc.: 62.50%] [G loss: 1.478103]\n",
      "epoch:30 step:28642 [D loss: 0.472073, acc.: 77.34%] [G loss: 1.329570]\n",
      "epoch:30 step:28643 [D loss: 0.561706, acc.: 67.97%] [G loss: 1.365772]\n",
      "epoch:30 step:28644 [D loss: 0.591342, acc.: 67.97%] [G loss: 1.440108]\n",
      "epoch:30 step:28645 [D loss: 0.622262, acc.: 68.75%] [G loss: 1.329648]\n",
      "epoch:30 step:28646 [D loss: 0.597419, acc.: 67.19%] [G loss: 1.539176]\n",
      "epoch:30 step:28647 [D loss: 0.436528, acc.: 80.47%] [G loss: 1.875824]\n",
      "epoch:30 step:28648 [D loss: 0.495528, acc.: 78.12%] [G loss: 1.552348]\n",
      "epoch:30 step:28649 [D loss: 0.512836, acc.: 71.88%] [G loss: 1.573296]\n",
      "epoch:30 step:28650 [D loss: 0.478191, acc.: 79.69%] [G loss: 1.588659]\n",
      "epoch:30 step:28651 [D loss: 0.443974, acc.: 80.47%] [G loss: 1.446243]\n",
      "epoch:30 step:28652 [D loss: 0.489297, acc.: 78.12%] [G loss: 1.461033]\n",
      "epoch:30 step:28653 [D loss: 0.631318, acc.: 67.97%] [G loss: 1.314044]\n",
      "epoch:30 step:28654 [D loss: 0.307550, acc.: 90.62%] [G loss: 1.729175]\n",
      "epoch:30 step:28655 [D loss: 0.629642, acc.: 65.62%] [G loss: 1.711953]\n",
      "epoch:30 step:28656 [D loss: 0.727513, acc.: 59.38%] [G loss: 1.419234]\n",
      "epoch:30 step:28657 [D loss: 0.589922, acc.: 65.62%] [G loss: 1.875050]\n",
      "epoch:30 step:28658 [D loss: 0.576587, acc.: 71.88%] [G loss: 1.385595]\n",
      "epoch:30 step:28659 [D loss: 0.625806, acc.: 60.94%] [G loss: 1.209429]\n",
      "epoch:30 step:28660 [D loss: 0.496495, acc.: 77.34%] [G loss: 1.359350]\n",
      "epoch:30 step:28661 [D loss: 0.549749, acc.: 74.22%] [G loss: 1.373096]\n",
      "epoch:30 step:28662 [D loss: 0.533957, acc.: 78.91%] [G loss: 1.208820]\n",
      "epoch:30 step:28663 [D loss: 0.593036, acc.: 67.97%] [G loss: 1.496281]\n",
      "epoch:30 step:28664 [D loss: 0.709321, acc.: 60.94%] [G loss: 1.696357]\n",
      "epoch:30 step:28665 [D loss: 0.547101, acc.: 75.00%] [G loss: 1.462204]\n",
      "epoch:30 step:28666 [D loss: 0.550462, acc.: 67.97%] [G loss: 1.405349]\n",
      "epoch:30 step:28667 [D loss: 0.497704, acc.: 77.34%] [G loss: 1.537508]\n",
      "epoch:30 step:28668 [D loss: 0.568007, acc.: 71.88%] [G loss: 1.494749]\n",
      "epoch:30 step:28669 [D loss: 0.555779, acc.: 71.09%] [G loss: 1.151256]\n",
      "epoch:30 step:28670 [D loss: 0.579019, acc.: 71.88%] [G loss: 1.577410]\n",
      "epoch:30 step:28671 [D loss: 0.551401, acc.: 75.78%] [G loss: 1.295756]\n",
      "epoch:30 step:28672 [D loss: 0.488823, acc.: 80.47%] [G loss: 1.350989]\n",
      "epoch:30 step:28673 [D loss: 0.396416, acc.: 80.47%] [G loss: 1.279130]\n",
      "epoch:30 step:28674 [D loss: 0.487642, acc.: 81.25%] [G loss: 1.741138]\n",
      "epoch:30 step:28675 [D loss: 0.568344, acc.: 70.31%] [G loss: 1.372879]\n",
      "epoch:30 step:28676 [D loss: 0.471056, acc.: 82.03%] [G loss: 1.266448]\n",
      "epoch:30 step:28677 [D loss: 0.521044, acc.: 76.56%] [G loss: 1.524154]\n",
      "epoch:30 step:28678 [D loss: 0.566366, acc.: 68.75%] [G loss: 1.556924]\n",
      "epoch:30 step:28679 [D loss: 0.485322, acc.: 82.03%] [G loss: 1.505665]\n",
      "epoch:30 step:28680 [D loss: 0.557695, acc.: 71.09%] [G loss: 1.343859]\n",
      "epoch:30 step:28681 [D loss: 0.581096, acc.: 67.97%] [G loss: 1.942553]\n",
      "epoch:30 step:28682 [D loss: 0.546954, acc.: 71.88%] [G loss: 1.163397]\n",
      "epoch:30 step:28683 [D loss: 0.602846, acc.: 65.62%] [G loss: 1.731799]\n",
      "epoch:30 step:28684 [D loss: 0.572419, acc.: 70.31%] [G loss: 1.015103]\n",
      "epoch:30 step:28685 [D loss: 0.562498, acc.: 73.44%] [G loss: 1.162758]\n",
      "epoch:30 step:28686 [D loss: 0.424792, acc.: 83.59%] [G loss: 1.580934]\n",
      "epoch:30 step:28687 [D loss: 0.599471, acc.: 69.53%] [G loss: 1.295073]\n",
      "epoch:30 step:28688 [D loss: 0.390482, acc.: 82.81%] [G loss: 1.279740]\n",
      "epoch:30 step:28689 [D loss: 0.501390, acc.: 78.91%] [G loss: 1.436805]\n",
      "epoch:30 step:28690 [D loss: 0.613270, acc.: 64.84%] [G loss: 1.127784]\n",
      "epoch:30 step:28691 [D loss: 0.488868, acc.: 79.69%] [G loss: 1.506997]\n",
      "epoch:30 step:28692 [D loss: 0.416537, acc.: 82.81%] [G loss: 2.062897]\n",
      "epoch:30 step:28693 [D loss: 0.578331, acc.: 71.09%] [G loss: 1.274012]\n",
      "epoch:30 step:28694 [D loss: 0.553998, acc.: 67.19%] [G loss: 1.264342]\n",
      "epoch:30 step:28695 [D loss: 0.493336, acc.: 75.78%] [G loss: 1.486060]\n",
      "epoch:30 step:28696 [D loss: 0.452704, acc.: 77.34%] [G loss: 1.360882]\n",
      "epoch:30 step:28697 [D loss: 0.560351, acc.: 70.31%] [G loss: 1.475507]\n",
      "epoch:30 step:28698 [D loss: 0.566251, acc.: 74.22%] [G loss: 1.695817]\n",
      "epoch:30 step:28699 [D loss: 0.462178, acc.: 78.91%] [G loss: 1.794024]\n",
      "epoch:30 step:28700 [D loss: 0.410363, acc.: 79.69%] [G loss: 1.447676]\n",
      "epoch:30 step:28701 [D loss: 0.547320, acc.: 75.00%] [G loss: 1.598468]\n",
      "epoch:30 step:28702 [D loss: 0.426503, acc.: 83.59%] [G loss: 1.522723]\n",
      "epoch:30 step:28703 [D loss: 0.771956, acc.: 59.38%] [G loss: 1.442189]\n",
      "epoch:30 step:28704 [D loss: 0.743082, acc.: 57.81%] [G loss: 1.165506]\n",
      "epoch:30 step:28705 [D loss: 0.428945, acc.: 82.81%] [G loss: 1.527891]\n",
      "epoch:30 step:28706 [D loss: 0.436792, acc.: 80.47%] [G loss: 2.055443]\n",
      "epoch:30 step:28707 [D loss: 0.466211, acc.: 79.69%] [G loss: 1.507725]\n",
      "epoch:30 step:28708 [D loss: 0.563588, acc.: 69.53%] [G loss: 1.468459]\n",
      "epoch:30 step:28709 [D loss: 0.439266, acc.: 82.03%] [G loss: 1.618046]\n",
      "epoch:30 step:28710 [D loss: 0.344221, acc.: 89.84%] [G loss: 1.290027]\n",
      "epoch:30 step:28711 [D loss: 0.656744, acc.: 60.94%] [G loss: 1.253061]\n",
      "epoch:30 step:28712 [D loss: 0.551643, acc.: 72.66%] [G loss: 1.484501]\n",
      "epoch:30 step:28713 [D loss: 0.450409, acc.: 78.91%] [G loss: 1.712285]\n",
      "epoch:30 step:28714 [D loss: 0.535446, acc.: 75.00%] [G loss: 1.312541]\n",
      "epoch:30 step:28715 [D loss: 0.695566, acc.: 60.16%] [G loss: 1.410138]\n",
      "epoch:30 step:28716 [D loss: 0.561096, acc.: 72.66%] [G loss: 0.934877]\n",
      "epoch:30 step:28717 [D loss: 0.532396, acc.: 75.00%] [G loss: 1.669268]\n",
      "epoch:30 step:28718 [D loss: 0.529967, acc.: 72.66%] [G loss: 1.621530]\n",
      "epoch:30 step:28719 [D loss: 0.714100, acc.: 60.94%] [G loss: 1.403273]\n",
      "epoch:30 step:28720 [D loss: 0.444920, acc.: 81.25%] [G loss: 1.821951]\n",
      "epoch:30 step:28721 [D loss: 0.646971, acc.: 64.84%] [G loss: 1.471032]\n",
      "epoch:30 step:28722 [D loss: 0.631971, acc.: 63.28%] [G loss: 1.385471]\n",
      "epoch:30 step:28723 [D loss: 0.466929, acc.: 79.69%] [G loss: 1.309537]\n",
      "epoch:30 step:28724 [D loss: 0.458735, acc.: 80.47%] [G loss: 1.684265]\n",
      "epoch:30 step:28725 [D loss: 0.598811, acc.: 69.53%] [G loss: 1.564483]\n",
      "epoch:30 step:28726 [D loss: 0.663695, acc.: 59.38%] [G loss: 1.322346]\n",
      "epoch:30 step:28727 [D loss: 0.458341, acc.: 80.47%] [G loss: 1.589782]\n",
      "epoch:30 step:28728 [D loss: 0.452444, acc.: 80.47%] [G loss: 1.601328]\n",
      "epoch:30 step:28729 [D loss: 0.600070, acc.: 64.06%] [G loss: 1.533827]\n",
      "epoch:30 step:28730 [D loss: 0.571789, acc.: 72.66%] [G loss: 1.456976]\n",
      "epoch:30 step:28731 [D loss: 0.711801, acc.: 58.59%] [G loss: 1.124960]\n",
      "epoch:30 step:28732 [D loss: 0.544553, acc.: 75.00%] [G loss: 1.749843]\n",
      "epoch:30 step:28733 [D loss: 0.572649, acc.: 74.22%] [G loss: 1.371372]\n",
      "epoch:30 step:28734 [D loss: 0.612264, acc.: 67.97%] [G loss: 1.684094]\n",
      "epoch:30 step:28735 [D loss: 0.402037, acc.: 85.16%] [G loss: 1.787734]\n",
      "epoch:30 step:28736 [D loss: 0.422023, acc.: 82.81%] [G loss: 1.953395]\n",
      "epoch:30 step:28737 [D loss: 0.558240, acc.: 73.44%] [G loss: 1.358657]\n",
      "epoch:30 step:28738 [D loss: 0.499053, acc.: 77.34%] [G loss: 1.557528]\n",
      "epoch:30 step:28739 [D loss: 0.650075, acc.: 57.81%] [G loss: 1.356103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28740 [D loss: 0.404814, acc.: 85.16%] [G loss: 1.469365]\n",
      "epoch:30 step:28741 [D loss: 0.526876, acc.: 72.66%] [G loss: 1.887770]\n",
      "epoch:30 step:28742 [D loss: 0.596197, acc.: 66.41%] [G loss: 1.305648]\n",
      "epoch:30 step:28743 [D loss: 0.434291, acc.: 78.12%] [G loss: 1.987766]\n",
      "epoch:30 step:28744 [D loss: 0.418694, acc.: 85.94%] [G loss: 1.642429]\n",
      "epoch:30 step:28745 [D loss: 0.596945, acc.: 66.41%] [G loss: 1.318326]\n",
      "epoch:30 step:28746 [D loss: 0.456660, acc.: 79.69%] [G loss: 1.731559]\n",
      "epoch:30 step:28747 [D loss: 0.546661, acc.: 71.88%] [G loss: 1.311751]\n",
      "epoch:30 step:28748 [D loss: 0.596953, acc.: 69.53%] [G loss: 1.284381]\n",
      "epoch:30 step:28749 [D loss: 0.706951, acc.: 53.12%] [G loss: 1.307256]\n",
      "epoch:30 step:28750 [D loss: 0.435848, acc.: 80.47%] [G loss: 1.497375]\n",
      "epoch:30 step:28751 [D loss: 0.679127, acc.: 60.16%] [G loss: 1.471923]\n",
      "epoch:30 step:28752 [D loss: 0.613378, acc.: 66.41%] [G loss: 1.266714]\n",
      "epoch:30 step:28753 [D loss: 0.658825, acc.: 63.28%] [G loss: 1.077617]\n",
      "epoch:30 step:28754 [D loss: 0.645961, acc.: 64.06%] [G loss: 1.367184]\n",
      "epoch:30 step:28755 [D loss: 0.375478, acc.: 85.94%] [G loss: 1.705211]\n",
      "epoch:30 step:28756 [D loss: 0.655637, acc.: 63.28%] [G loss: 1.491200]\n",
      "epoch:30 step:28757 [D loss: 0.674969, acc.: 63.28%] [G loss: 1.317794]\n",
      "epoch:30 step:28758 [D loss: 0.408514, acc.: 85.94%] [G loss: 1.527288]\n",
      "epoch:30 step:28759 [D loss: 0.410414, acc.: 85.16%] [G loss: 1.389891]\n",
      "epoch:30 step:28760 [D loss: 0.454944, acc.: 78.12%] [G loss: 1.803825]\n",
      "epoch:30 step:28761 [D loss: 0.502202, acc.: 80.47%] [G loss: 1.473260]\n",
      "epoch:30 step:28762 [D loss: 0.495758, acc.: 78.12%] [G loss: 1.601438]\n",
      "epoch:30 step:28763 [D loss: 0.680792, acc.: 58.59%] [G loss: 1.342629]\n",
      "epoch:30 step:28764 [D loss: 0.582691, acc.: 67.97%] [G loss: 1.275819]\n",
      "epoch:30 step:28765 [D loss: 0.529041, acc.: 68.75%] [G loss: 1.180737]\n",
      "epoch:30 step:28766 [D loss: 0.491362, acc.: 76.56%] [G loss: 1.390940]\n",
      "epoch:30 step:28767 [D loss: 0.472604, acc.: 80.47%] [G loss: 1.672422]\n",
      "epoch:30 step:28768 [D loss: 0.529772, acc.: 72.66%] [G loss: 1.485983]\n",
      "epoch:30 step:28769 [D loss: 0.547309, acc.: 69.53%] [G loss: 1.707322]\n",
      "epoch:30 step:28770 [D loss: 0.527081, acc.: 74.22%] [G loss: 1.715911]\n",
      "epoch:30 step:28771 [D loss: 0.620808, acc.: 66.41%] [G loss: 1.751712]\n",
      "epoch:30 step:28772 [D loss: 0.575856, acc.: 73.44%] [G loss: 1.701797]\n",
      "epoch:30 step:28773 [D loss: 0.614871, acc.: 70.31%] [G loss: 1.555654]\n",
      "epoch:30 step:28774 [D loss: 0.550644, acc.: 71.09%] [G loss: 1.638201]\n",
      "epoch:30 step:28775 [D loss: 0.500017, acc.: 75.78%] [G loss: 1.502412]\n",
      "epoch:30 step:28776 [D loss: 0.361345, acc.: 85.16%] [G loss: 1.535450]\n",
      "epoch:30 step:28777 [D loss: 0.513413, acc.: 74.22%] [G loss: 1.272256]\n",
      "epoch:30 step:28778 [D loss: 0.565377, acc.: 65.62%] [G loss: 1.304841]\n",
      "epoch:30 step:28779 [D loss: 0.368355, acc.: 88.28%] [G loss: 2.185663]\n",
      "epoch:30 step:28780 [D loss: 0.482403, acc.: 75.00%] [G loss: 1.966837]\n",
      "epoch:30 step:28781 [D loss: 0.645846, acc.: 67.19%] [G loss: 1.348692]\n",
      "epoch:30 step:28782 [D loss: 0.680324, acc.: 59.38%] [G loss: 1.581293]\n",
      "epoch:30 step:28783 [D loss: 0.600843, acc.: 66.41%] [G loss: 1.739121]\n",
      "epoch:30 step:28784 [D loss: 0.637921, acc.: 65.62%] [G loss: 1.336671]\n",
      "epoch:30 step:28785 [D loss: 0.505919, acc.: 79.69%] [G loss: 1.601615]\n",
      "epoch:30 step:28786 [D loss: 0.424919, acc.: 81.25%] [G loss: 1.882443]\n",
      "epoch:30 step:28787 [D loss: 0.390753, acc.: 84.38%] [G loss: 1.194910]\n",
      "epoch:30 step:28788 [D loss: 0.746994, acc.: 60.16%] [G loss: 1.236773]\n",
      "epoch:30 step:28789 [D loss: 0.337908, acc.: 89.84%] [G loss: 1.404961]\n",
      "epoch:30 step:28790 [D loss: 0.653046, acc.: 67.19%] [G loss: 1.330355]\n",
      "epoch:30 step:28791 [D loss: 0.438147, acc.: 83.59%] [G loss: 1.456157]\n",
      "epoch:30 step:28792 [D loss: 0.361972, acc.: 89.84%] [G loss: 1.954610]\n",
      "epoch:30 step:28793 [D loss: 0.593911, acc.: 71.09%] [G loss: 1.769253]\n",
      "epoch:30 step:28794 [D loss: 0.408500, acc.: 85.16%] [G loss: 1.355063]\n",
      "epoch:30 step:28795 [D loss: 0.616875, acc.: 71.09%] [G loss: 1.162342]\n",
      "epoch:30 step:28796 [D loss: 0.621325, acc.: 67.19%] [G loss: 1.438968]\n",
      "epoch:30 step:28797 [D loss: 0.440895, acc.: 82.81%] [G loss: 1.865776]\n",
      "epoch:30 step:28798 [D loss: 0.609258, acc.: 64.84%] [G loss: 1.487196]\n",
      "epoch:30 step:28799 [D loss: 0.562254, acc.: 69.53%] [G loss: 1.215903]\n",
      "epoch:30 step:28800 [D loss: 0.425250, acc.: 85.94%] [G loss: 1.531477]\n",
      "##############\n",
      "[2.74194351 1.98320281 1.73828728 3.05744853 0.87337045 6.11056148\n",
      " 2.21696143 2.3330809  3.98439179 8.14868929]\n",
      "##########\n",
      "epoch:30 step:28801 [D loss: 0.386722, acc.: 85.16%] [G loss: 1.575433]\n",
      "epoch:30 step:28802 [D loss: 0.483702, acc.: 78.12%] [G loss: 1.405362]\n",
      "epoch:30 step:28803 [D loss: 0.612735, acc.: 61.72%] [G loss: 1.418700]\n",
      "epoch:30 step:28804 [D loss: 0.525640, acc.: 75.78%] [G loss: 1.491844]\n",
      "epoch:30 step:28805 [D loss: 0.306289, acc.: 89.84%] [G loss: 1.393910]\n",
      "epoch:30 step:28806 [D loss: 0.895160, acc.: 50.00%] [G loss: 0.902107]\n",
      "epoch:30 step:28807 [D loss: 0.440209, acc.: 77.34%] [G loss: 1.415332]\n",
      "epoch:30 step:28808 [D loss: 0.606715, acc.: 65.62%] [G loss: 1.654352]\n",
      "epoch:30 step:28809 [D loss: 0.527881, acc.: 74.22%] [G loss: 1.353165]\n",
      "epoch:30 step:28810 [D loss: 0.657626, acc.: 61.72%] [G loss: 1.296718]\n",
      "epoch:30 step:28811 [D loss: 0.584351, acc.: 71.09%] [G loss: 1.297750]\n",
      "epoch:30 step:28812 [D loss: 0.432830, acc.: 76.56%] [G loss: 1.311383]\n",
      "epoch:30 step:28813 [D loss: 0.564748, acc.: 71.88%] [G loss: 1.210734]\n",
      "epoch:30 step:28814 [D loss: 0.697075, acc.: 58.59%] [G loss: 1.498218]\n",
      "epoch:30 step:28815 [D loss: 0.423300, acc.: 81.25%] [G loss: 1.429586]\n",
      "epoch:30 step:28816 [D loss: 0.622055, acc.: 62.50%] [G loss: 1.340479]\n",
      "epoch:30 step:28817 [D loss: 0.485582, acc.: 78.91%] [G loss: 1.531034]\n",
      "epoch:30 step:28818 [D loss: 0.581285, acc.: 69.53%] [G loss: 1.277224]\n",
      "epoch:30 step:28819 [D loss: 0.825153, acc.: 53.12%] [G loss: 1.607021]\n",
      "epoch:30 step:28820 [D loss: 0.301717, acc.: 89.84%] [G loss: 1.575624]\n",
      "epoch:30 step:28821 [D loss: 0.414506, acc.: 82.81%] [G loss: 1.856504]\n",
      "epoch:30 step:28822 [D loss: 0.407805, acc.: 82.81%] [G loss: 1.904411]\n",
      "epoch:30 step:28823 [D loss: 0.491671, acc.: 72.66%] [G loss: 1.203813]\n",
      "epoch:30 step:28824 [D loss: 0.531223, acc.: 75.78%] [G loss: 1.189757]\n",
      "epoch:30 step:28825 [D loss: 0.607180, acc.: 68.75%] [G loss: 1.347492]\n",
      "epoch:30 step:28826 [D loss: 0.397646, acc.: 82.81%] [G loss: 1.354257]\n",
      "epoch:30 step:28827 [D loss: 0.466345, acc.: 78.12%] [G loss: 1.480053]\n",
      "epoch:30 step:28828 [D loss: 0.500371, acc.: 74.22%] [G loss: 1.533577]\n",
      "epoch:30 step:28829 [D loss: 0.580481, acc.: 68.75%] [G loss: 1.586585]\n",
      "epoch:30 step:28830 [D loss: 0.495375, acc.: 80.47%] [G loss: 1.612715]\n",
      "epoch:30 step:28831 [D loss: 0.567431, acc.: 70.31%] [G loss: 1.064893]\n",
      "epoch:30 step:28832 [D loss: 0.770231, acc.: 55.47%] [G loss: 1.466879]\n",
      "epoch:30 step:28833 [D loss: 0.710549, acc.: 59.38%] [G loss: 1.339410]\n",
      "epoch:30 step:28834 [D loss: 0.428483, acc.: 78.12%] [G loss: 1.365117]\n",
      "epoch:30 step:28835 [D loss: 0.538991, acc.: 73.44%] [G loss: 1.595310]\n",
      "epoch:30 step:28836 [D loss: 0.612823, acc.: 65.62%] [G loss: 1.158614]\n",
      "epoch:30 step:28837 [D loss: 0.440373, acc.: 76.56%] [G loss: 1.394813]\n",
      "epoch:30 step:28838 [D loss: 0.517760, acc.: 72.66%] [G loss: 1.556888]\n",
      "epoch:30 step:28839 [D loss: 0.414343, acc.: 82.81%] [G loss: 0.966896]\n",
      "epoch:30 step:28840 [D loss: 0.615381, acc.: 68.75%] [G loss: 0.765628]\n",
      "epoch:30 step:28841 [D loss: 0.843455, acc.: 47.66%] [G loss: 1.740393]\n",
      "epoch:30 step:28842 [D loss: 0.570746, acc.: 70.31%] [G loss: 1.265140]\n",
      "epoch:30 step:28843 [D loss: 0.543031, acc.: 71.09%] [G loss: 1.125050]\n",
      "epoch:30 step:28844 [D loss: 0.414846, acc.: 82.03%] [G loss: 1.173243]\n",
      "epoch:30 step:28845 [D loss: 0.656681, acc.: 64.06%] [G loss: 1.509573]\n",
      "epoch:30 step:28846 [D loss: 0.672457, acc.: 64.06%] [G loss: 1.378142]\n",
      "epoch:30 step:28847 [D loss: 0.367457, acc.: 87.50%] [G loss: 1.431859]\n",
      "epoch:30 step:28848 [D loss: 0.775784, acc.: 50.78%] [G loss: 1.606538]\n",
      "epoch:30 step:28849 [D loss: 0.657908, acc.: 64.84%] [G loss: 1.273305]\n",
      "epoch:30 step:28850 [D loss: 0.542161, acc.: 71.88%] [G loss: 1.665379]\n",
      "epoch:30 step:28851 [D loss: 0.698577, acc.: 60.16%] [G loss: 1.347842]\n",
      "epoch:30 step:28852 [D loss: 0.511158, acc.: 74.22%] [G loss: 2.220033]\n",
      "epoch:30 step:28853 [D loss: 0.625946, acc.: 67.97%] [G loss: 1.813911]\n",
      "epoch:30 step:28854 [D loss: 0.541215, acc.: 72.66%] [G loss: 1.450269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28855 [D loss: 0.586430, acc.: 69.53%] [G loss: 1.288118]\n",
      "epoch:30 step:28856 [D loss: 0.629439, acc.: 64.06%] [G loss: 0.979419]\n",
      "epoch:30 step:28857 [D loss: 0.307721, acc.: 90.62%] [G loss: 1.880787]\n",
      "epoch:30 step:28858 [D loss: 0.324198, acc.: 85.94%] [G loss: 1.640763]\n",
      "epoch:30 step:28859 [D loss: 0.576346, acc.: 71.09%] [G loss: 1.370917]\n",
      "epoch:30 step:28860 [D loss: 0.584063, acc.: 69.53%] [G loss: 1.521398]\n",
      "epoch:30 step:28861 [D loss: 0.464801, acc.: 80.47%] [G loss: 1.996323]\n",
      "epoch:30 step:28862 [D loss: 0.596350, acc.: 65.62%] [G loss: 1.770230]\n",
      "epoch:30 step:28863 [D loss: 0.496320, acc.: 73.44%] [G loss: 1.434481]\n",
      "epoch:30 step:28864 [D loss: 0.538232, acc.: 69.53%] [G loss: 1.400322]\n",
      "epoch:30 step:28865 [D loss: 0.492690, acc.: 75.78%] [G loss: 1.557521]\n",
      "epoch:30 step:28866 [D loss: 0.531877, acc.: 71.88%] [G loss: 1.588981]\n",
      "epoch:30 step:28867 [D loss: 0.558841, acc.: 71.09%] [G loss: 1.304800]\n",
      "epoch:30 step:28868 [D loss: 0.476347, acc.: 78.12%] [G loss: 1.470992]\n",
      "epoch:30 step:28869 [D loss: 0.377810, acc.: 88.28%] [G loss: 1.589584]\n",
      "epoch:30 step:28870 [D loss: 0.411239, acc.: 85.16%] [G loss: 1.662286]\n",
      "epoch:30 step:28871 [D loss: 0.594523, acc.: 64.06%] [G loss: 1.571110]\n",
      "epoch:30 step:28872 [D loss: 0.697309, acc.: 62.50%] [G loss: 1.499496]\n",
      "epoch:30 step:28873 [D loss: 0.445650, acc.: 83.59%] [G loss: 1.318412]\n",
      "epoch:30 step:28874 [D loss: 0.551995, acc.: 69.53%] [G loss: 1.297475]\n",
      "epoch:30 step:28875 [D loss: 0.637082, acc.: 64.84%] [G loss: 1.286289]\n",
      "epoch:30 step:28876 [D loss: 0.399207, acc.: 81.25%] [G loss: 1.749920]\n",
      "epoch:30 step:28877 [D loss: 0.483148, acc.: 75.78%] [G loss: 1.277703]\n",
      "epoch:30 step:28878 [D loss: 0.467621, acc.: 77.34%] [G loss: 2.120225]\n",
      "epoch:30 step:28879 [D loss: 0.426267, acc.: 82.03%] [G loss: 1.764422]\n",
      "epoch:30 step:28880 [D loss: 0.533139, acc.: 72.66%] [G loss: 1.563039]\n",
      "epoch:30 step:28881 [D loss: 0.736717, acc.: 54.69%] [G loss: 1.251987]\n",
      "epoch:30 step:28882 [D loss: 0.496049, acc.: 77.34%] [G loss: 1.663573]\n",
      "epoch:30 step:28883 [D loss: 0.822256, acc.: 57.03%] [G loss: 1.217282]\n",
      "epoch:30 step:28884 [D loss: 0.535937, acc.: 71.09%] [G loss: 1.414768]\n",
      "epoch:30 step:28885 [D loss: 0.459561, acc.: 77.34%] [G loss: 1.380414]\n",
      "epoch:30 step:28886 [D loss: 0.397134, acc.: 82.03%] [G loss: 1.580018]\n",
      "epoch:30 step:28887 [D loss: 0.513433, acc.: 75.00%] [G loss: 1.389604]\n",
      "epoch:30 step:28888 [D loss: 0.597675, acc.: 65.62%] [G loss: 1.535920]\n",
      "epoch:30 step:28889 [D loss: 0.552850, acc.: 69.53%] [G loss: 1.383644]\n",
      "epoch:30 step:28890 [D loss: 0.637576, acc.: 68.75%] [G loss: 0.996402]\n",
      "epoch:30 step:28891 [D loss: 0.521544, acc.: 73.44%] [G loss: 1.369614]\n",
      "epoch:30 step:28892 [D loss: 0.481474, acc.: 79.69%] [G loss: 1.634643]\n",
      "epoch:30 step:28893 [D loss: 0.516058, acc.: 71.09%] [G loss: 1.622268]\n",
      "epoch:30 step:28894 [D loss: 0.389880, acc.: 85.94%] [G loss: 1.560916]\n",
      "epoch:30 step:28895 [D loss: 0.601656, acc.: 66.41%] [G loss: 1.651610]\n",
      "epoch:30 step:28896 [D loss: 0.566803, acc.: 74.22%] [G loss: 1.323564]\n",
      "epoch:30 step:28897 [D loss: 0.539267, acc.: 75.00%] [G loss: 1.416209]\n",
      "epoch:30 step:28898 [D loss: 0.367463, acc.: 84.38%] [G loss: 1.396241]\n",
      "epoch:30 step:28899 [D loss: 0.391660, acc.: 83.59%] [G loss: 1.447066]\n",
      "epoch:30 step:28900 [D loss: 0.789696, acc.: 51.56%] [G loss: 1.928514]\n",
      "epoch:30 step:28901 [D loss: 0.672253, acc.: 60.94%] [G loss: 1.411803]\n",
      "epoch:30 step:28902 [D loss: 0.504323, acc.: 73.44%] [G loss: 1.447135]\n",
      "epoch:30 step:28903 [D loss: 0.576791, acc.: 69.53%] [G loss: 1.686397]\n",
      "epoch:30 step:28904 [D loss: 0.508390, acc.: 75.78%] [G loss: 1.316914]\n",
      "epoch:30 step:28905 [D loss: 0.506190, acc.: 75.00%] [G loss: 1.224054]\n",
      "epoch:30 step:28906 [D loss: 0.578341, acc.: 71.09%] [G loss: 1.588665]\n",
      "epoch:30 step:28907 [D loss: 0.420271, acc.: 83.59%] [G loss: 0.924344]\n",
      "epoch:30 step:28908 [D loss: 0.426215, acc.: 82.03%] [G loss: 1.307878]\n",
      "epoch:30 step:28909 [D loss: 0.602710, acc.: 64.84%] [G loss: 1.590274]\n",
      "epoch:30 step:28910 [D loss: 0.857619, acc.: 49.22%] [G loss: 1.254161]\n",
      "epoch:30 step:28911 [D loss: 0.664259, acc.: 59.38%] [G loss: 1.667981]\n",
      "epoch:30 step:28912 [D loss: 0.646423, acc.: 60.94%] [G loss: 1.328629]\n",
      "epoch:30 step:28913 [D loss: 0.722861, acc.: 58.59%] [G loss: 1.376540]\n",
      "epoch:30 step:28914 [D loss: 0.741418, acc.: 54.69%] [G loss: 1.384469]\n",
      "epoch:30 step:28915 [D loss: 0.653242, acc.: 64.84%] [G loss: 1.803058]\n",
      "epoch:30 step:28916 [D loss: 0.502888, acc.: 79.69%] [G loss: 1.466642]\n",
      "epoch:30 step:28917 [D loss: 0.441850, acc.: 78.91%] [G loss: 1.613273]\n",
      "epoch:30 step:28918 [D loss: 0.530609, acc.: 74.22%] [G loss: 1.439669]\n",
      "epoch:30 step:28919 [D loss: 0.549674, acc.: 75.78%] [G loss: 1.085249]\n",
      "epoch:30 step:28920 [D loss: 0.384380, acc.: 87.50%] [G loss: 1.859458]\n",
      "epoch:30 step:28921 [D loss: 0.614005, acc.: 62.50%] [G loss: 1.211089]\n",
      "epoch:30 step:28922 [D loss: 0.672533, acc.: 63.28%] [G loss: 1.571977]\n",
      "epoch:30 step:28923 [D loss: 0.755255, acc.: 56.25%] [G loss: 1.574861]\n",
      "epoch:30 step:28924 [D loss: 0.478185, acc.: 78.91%] [G loss: 1.661529]\n",
      "epoch:30 step:28925 [D loss: 0.476517, acc.: 80.47%] [G loss: 1.886484]\n",
      "epoch:30 step:28926 [D loss: 0.478386, acc.: 80.47%] [G loss: 1.221010]\n",
      "epoch:30 step:28927 [D loss: 0.691909, acc.: 57.81%] [G loss: 1.109556]\n",
      "epoch:30 step:28928 [D loss: 0.643731, acc.: 64.84%] [G loss: 1.554044]\n",
      "epoch:30 step:28929 [D loss: 0.476148, acc.: 78.12%] [G loss: 2.051328]\n",
      "epoch:30 step:28930 [D loss: 0.629974, acc.: 64.06%] [G loss: 1.504715]\n",
      "epoch:30 step:28931 [D loss: 0.644427, acc.: 64.84%] [G loss: 1.341655]\n",
      "epoch:30 step:28932 [D loss: 0.581276, acc.: 69.53%] [G loss: 0.971795]\n",
      "epoch:30 step:28933 [D loss: 0.493128, acc.: 75.00%] [G loss: 1.608454]\n",
      "epoch:30 step:28934 [D loss: 0.571905, acc.: 69.53%] [G loss: 1.385779]\n",
      "epoch:30 step:28935 [D loss: 0.533970, acc.: 71.88%] [G loss: 1.257315]\n",
      "epoch:30 step:28936 [D loss: 0.562287, acc.: 74.22%] [G loss: 1.637280]\n",
      "epoch:30 step:28937 [D loss: 0.650329, acc.: 59.38%] [G loss: 1.700126]\n",
      "epoch:30 step:28938 [D loss: 0.630327, acc.: 67.19%] [G loss: 1.451983]\n",
      "epoch:30 step:28939 [D loss: 0.561444, acc.: 71.88%] [G loss: 1.163807]\n",
      "epoch:30 step:28940 [D loss: 0.475259, acc.: 76.56%] [G loss: 1.264325]\n",
      "epoch:30 step:28941 [D loss: 0.590890, acc.: 70.31%] [G loss: 1.330607]\n",
      "epoch:30 step:28942 [D loss: 0.524366, acc.: 79.69%] [G loss: 1.010222]\n",
      "epoch:30 step:28943 [D loss: 0.492636, acc.: 75.00%] [G loss: 1.266458]\n",
      "epoch:30 step:28944 [D loss: 0.688017, acc.: 61.72%] [G loss: 1.369260]\n",
      "epoch:30 step:28945 [D loss: 0.547108, acc.: 74.22%] [G loss: 1.291425]\n",
      "epoch:30 step:28946 [D loss: 0.685733, acc.: 60.16%] [G loss: 1.376602]\n",
      "epoch:30 step:28947 [D loss: 0.666379, acc.: 59.38%] [G loss: 1.099426]\n",
      "epoch:30 step:28948 [D loss: 0.306318, acc.: 88.28%] [G loss: 1.939106]\n",
      "epoch:30 step:28949 [D loss: 0.672791, acc.: 65.62%] [G loss: 1.611141]\n",
      "epoch:30 step:28950 [D loss: 0.482119, acc.: 78.91%] [G loss: 1.287095]\n",
      "epoch:30 step:28951 [D loss: 0.432759, acc.: 82.03%] [G loss: 1.420992]\n",
      "epoch:30 step:28952 [D loss: 0.586433, acc.: 67.97%] [G loss: 1.093292]\n",
      "epoch:30 step:28953 [D loss: 0.451211, acc.: 82.81%] [G loss: 1.536073]\n",
      "epoch:30 step:28954 [D loss: 0.604634, acc.: 64.06%] [G loss: 1.007913]\n",
      "epoch:30 step:28955 [D loss: 0.539225, acc.: 78.12%] [G loss: 1.190720]\n",
      "epoch:30 step:28956 [D loss: 0.558473, acc.: 75.00%] [G loss: 1.643860]\n",
      "epoch:30 step:28957 [D loss: 0.618520, acc.: 69.53%] [G loss: 1.211678]\n",
      "epoch:30 step:28958 [D loss: 0.443779, acc.: 81.25%] [G loss: 1.403780]\n",
      "epoch:30 step:28959 [D loss: 0.539122, acc.: 67.97%] [G loss: 1.536277]\n",
      "epoch:30 step:28960 [D loss: 0.582594, acc.: 66.41%] [G loss: 1.871172]\n",
      "epoch:30 step:28961 [D loss: 0.593547, acc.: 70.31%] [G loss: 1.183459]\n",
      "epoch:30 step:28962 [D loss: 0.664328, acc.: 65.62%] [G loss: 1.038765]\n",
      "epoch:30 step:28963 [D loss: 0.535241, acc.: 75.00%] [G loss: 1.784786]\n",
      "epoch:30 step:28964 [D loss: 0.684423, acc.: 60.16%] [G loss: 1.088619]\n",
      "epoch:30 step:28965 [D loss: 0.556783, acc.: 73.44%] [G loss: 1.563228]\n",
      "epoch:30 step:28966 [D loss: 0.447316, acc.: 78.91%] [G loss: 1.859598]\n",
      "epoch:30 step:28967 [D loss: 0.449145, acc.: 79.69%] [G loss: 1.814430]\n",
      "epoch:30 step:28968 [D loss: 0.419388, acc.: 84.38%] [G loss: 1.331868]\n",
      "epoch:30 step:28969 [D loss: 0.647892, acc.: 67.19%] [G loss: 1.366450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28970 [D loss: 0.548038, acc.: 70.31%] [G loss: 1.368483]\n",
      "epoch:30 step:28971 [D loss: 0.397819, acc.: 88.28%] [G loss: 1.521899]\n",
      "epoch:30 step:28972 [D loss: 0.476135, acc.: 80.47%] [G loss: 1.399661]\n",
      "epoch:30 step:28973 [D loss: 0.389003, acc.: 84.38%] [G loss: 1.257607]\n",
      "epoch:30 step:28974 [D loss: 0.514159, acc.: 75.00%] [G loss: 1.573065]\n",
      "epoch:30 step:28975 [D loss: 0.345528, acc.: 89.84%] [G loss: 1.376239]\n",
      "epoch:30 step:28976 [D loss: 0.438498, acc.: 85.16%] [G loss: 1.371386]\n",
      "epoch:30 step:28977 [D loss: 0.462693, acc.: 80.47%] [G loss: 1.066459]\n",
      "epoch:30 step:28978 [D loss: 0.456066, acc.: 81.25%] [G loss: 1.923837]\n",
      "epoch:30 step:28979 [D loss: 0.406945, acc.: 85.94%] [G loss: 1.465859]\n",
      "epoch:30 step:28980 [D loss: 0.527253, acc.: 77.34%] [G loss: 2.140294]\n",
      "epoch:30 step:28981 [D loss: 0.514785, acc.: 75.78%] [G loss: 1.092102]\n",
      "epoch:30 step:28982 [D loss: 0.422430, acc.: 79.69%] [G loss: 1.438542]\n",
      "epoch:30 step:28983 [D loss: 0.518489, acc.: 71.09%] [G loss: 1.381735]\n",
      "epoch:30 step:28984 [D loss: 0.499335, acc.: 76.56%] [G loss: 1.743062]\n",
      "epoch:30 step:28985 [D loss: 0.452711, acc.: 77.34%] [G loss: 1.569561]\n",
      "epoch:30 step:28986 [D loss: 0.448395, acc.: 80.47%] [G loss: 1.285564]\n",
      "epoch:30 step:28987 [D loss: 0.643418, acc.: 66.41%] [G loss: 1.505248]\n",
      "epoch:30 step:28988 [D loss: 0.513187, acc.: 71.88%] [G loss: 1.270297]\n",
      "epoch:30 step:28989 [D loss: 0.563066, acc.: 67.97%] [G loss: 1.227085]\n",
      "epoch:30 step:28990 [D loss: 0.537296, acc.: 75.78%] [G loss: 1.361185]\n",
      "epoch:30 step:28991 [D loss: 0.623792, acc.: 68.75%] [G loss: 1.302220]\n",
      "epoch:30 step:28992 [D loss: 0.398670, acc.: 82.03%] [G loss: 1.736847]\n",
      "epoch:30 step:28993 [D loss: 0.673994, acc.: 62.50%] [G loss: 1.511314]\n",
      "epoch:30 step:28994 [D loss: 0.484842, acc.: 75.78%] [G loss: 1.593160]\n",
      "epoch:30 step:28995 [D loss: 0.478405, acc.: 78.91%] [G loss: 1.323373]\n",
      "epoch:30 step:28996 [D loss: 0.495780, acc.: 78.12%] [G loss: 1.718537]\n",
      "epoch:30 step:28997 [D loss: 0.622813, acc.: 69.53%] [G loss: 1.605751]\n",
      "epoch:30 step:28998 [D loss: 0.636017, acc.: 60.94%] [G loss: 1.409348]\n",
      "epoch:30 step:28999 [D loss: 0.581204, acc.: 71.88%] [G loss: 1.138455]\n",
      "epoch:30 step:29000 [D loss: 0.445299, acc.: 82.81%] [G loss: 1.650572]\n",
      "##############\n",
      "[2.64348858 1.90014584 1.95382104 2.89542036 1.07119985 6.07088523\n",
      " 2.32048216 2.55278676 3.84799535 8.14868929]\n",
      "##########\n",
      "epoch:30 step:29001 [D loss: 0.523332, acc.: 74.22%] [G loss: 1.387312]\n",
      "epoch:30 step:29002 [D loss: 0.385775, acc.: 86.72%] [G loss: 1.600264]\n",
      "epoch:30 step:29003 [D loss: 0.470354, acc.: 75.00%] [G loss: 2.013578]\n",
      "epoch:30 step:29004 [D loss: 0.516148, acc.: 75.00%] [G loss: 1.401186]\n",
      "epoch:30 step:29005 [D loss: 0.581864, acc.: 72.66%] [G loss: 1.432994]\n",
      "epoch:30 step:29006 [D loss: 0.648291, acc.: 64.06%] [G loss: 1.426136]\n",
      "epoch:30 step:29007 [D loss: 0.517677, acc.: 75.00%] [G loss: 1.291325]\n",
      "epoch:30 step:29008 [D loss: 0.530435, acc.: 75.78%] [G loss: 1.904405]\n",
      "epoch:30 step:29009 [D loss: 0.573024, acc.: 71.09%] [G loss: 1.582561]\n",
      "epoch:30 step:29010 [D loss: 0.763051, acc.: 49.22%] [G loss: 1.270515]\n",
      "epoch:30 step:29011 [D loss: 0.605474, acc.: 67.19%] [G loss: 1.616255]\n",
      "epoch:30 step:29012 [D loss: 0.490642, acc.: 78.91%] [G loss: 1.669241]\n",
      "epoch:30 step:29013 [D loss: 0.453256, acc.: 80.47%] [G loss: 1.201425]\n",
      "epoch:30 step:29014 [D loss: 0.517601, acc.: 73.44%] [G loss: 1.512183]\n",
      "epoch:30 step:29015 [D loss: 0.658563, acc.: 60.16%] [G loss: 1.199787]\n",
      "epoch:30 step:29016 [D loss: 0.443044, acc.: 79.69%] [G loss: 1.279827]\n",
      "epoch:30 step:29017 [D loss: 0.561383, acc.: 71.88%] [G loss: 1.298793]\n",
      "epoch:30 step:29018 [D loss: 0.554907, acc.: 70.31%] [G loss: 1.314256]\n",
      "epoch:30 step:29019 [D loss: 0.405860, acc.: 87.50%] [G loss: 1.859084]\n",
      "epoch:30 step:29020 [D loss: 0.411366, acc.: 80.47%] [G loss: 1.738018]\n",
      "epoch:30 step:29021 [D loss: 0.527211, acc.: 75.78%] [G loss: 1.948388]\n",
      "epoch:30 step:29022 [D loss: 0.504741, acc.: 77.34%] [G loss: 1.116473]\n",
      "epoch:30 step:29023 [D loss: 0.535789, acc.: 73.44%] [G loss: 1.261182]\n",
      "epoch:30 step:29024 [D loss: 0.670233, acc.: 60.94%] [G loss: 1.705024]\n",
      "epoch:30 step:29025 [D loss: 0.492873, acc.: 76.56%] [G loss: 1.379384]\n",
      "epoch:30 step:29026 [D loss: 0.524397, acc.: 75.00%] [G loss: 1.528028]\n",
      "epoch:30 step:29027 [D loss: 0.453126, acc.: 81.25%] [G loss: 2.008202]\n",
      "epoch:30 step:29028 [D loss: 0.466781, acc.: 82.81%] [G loss: 2.087113]\n",
      "epoch:30 step:29029 [D loss: 0.556968, acc.: 71.88%] [G loss: 1.634997]\n",
      "epoch:30 step:29030 [D loss: 0.639306, acc.: 66.41%] [G loss: 1.170217]\n",
      "epoch:30 step:29031 [D loss: 0.605614, acc.: 70.31%] [G loss: 1.283998]\n",
      "epoch:30 step:29032 [D loss: 0.435898, acc.: 81.25%] [G loss: 1.289117]\n",
      "epoch:30 step:29033 [D loss: 0.456291, acc.: 79.69%] [G loss: 1.351905]\n",
      "epoch:30 step:29034 [D loss: 0.483803, acc.: 75.78%] [G loss: 1.522704]\n",
      "epoch:30 step:29035 [D loss: 0.517562, acc.: 75.00%] [G loss: 1.900660]\n",
      "epoch:30 step:29036 [D loss: 0.630485, acc.: 61.72%] [G loss: 1.299847]\n",
      "epoch:30 step:29037 [D loss: 0.638295, acc.: 65.62%] [G loss: 2.020406]\n",
      "epoch:30 step:29038 [D loss: 0.446262, acc.: 81.25%] [G loss: 2.200012]\n",
      "epoch:30 step:29039 [D loss: 0.729618, acc.: 59.38%] [G loss: 1.455601]\n",
      "epoch:30 step:29040 [D loss: 0.537335, acc.: 72.66%] [G loss: 1.867360]\n",
      "epoch:30 step:29041 [D loss: 0.441551, acc.: 82.81%] [G loss: 1.435588]\n",
      "epoch:30 step:29042 [D loss: 0.508455, acc.: 76.56%] [G loss: 1.400867]\n",
      "epoch:30 step:29043 [D loss: 0.606165, acc.: 69.53%] [G loss: 1.154296]\n",
      "epoch:30 step:29044 [D loss: 0.732154, acc.: 57.03%] [G loss: 1.186499]\n",
      "epoch:30 step:29045 [D loss: 0.383933, acc.: 86.72%] [G loss: 1.874678]\n",
      "epoch:30 step:29046 [D loss: 0.502685, acc.: 75.00%] [G loss: 1.805655]\n",
      "epoch:30 step:29047 [D loss: 0.501076, acc.: 77.34%] [G loss: 1.419249]\n",
      "epoch:31 step:29048 [D loss: 0.579231, acc.: 75.00%] [G loss: 1.276355]\n",
      "epoch:31 step:29049 [D loss: 0.691818, acc.: 62.50%] [G loss: 1.180136]\n",
      "epoch:31 step:29050 [D loss: 0.500326, acc.: 77.34%] [G loss: 1.334977]\n",
      "epoch:31 step:29051 [D loss: 0.593519, acc.: 70.31%] [G loss: 1.669158]\n",
      "epoch:31 step:29052 [D loss: 0.485295, acc.: 71.88%] [G loss: 1.810682]\n",
      "epoch:31 step:29053 [D loss: 0.506527, acc.: 71.88%] [G loss: 1.096871]\n",
      "epoch:31 step:29054 [D loss: 0.706110, acc.: 54.69%] [G loss: 1.425413]\n",
      "epoch:31 step:29055 [D loss: 0.409727, acc.: 82.81%] [G loss: 1.921166]\n",
      "epoch:31 step:29056 [D loss: 0.362699, acc.: 87.50%] [G loss: 1.415994]\n",
      "epoch:31 step:29057 [D loss: 0.355596, acc.: 87.50%] [G loss: 1.286028]\n",
      "epoch:31 step:29058 [D loss: 0.433465, acc.: 82.81%] [G loss: 2.025471]\n",
      "epoch:31 step:29059 [D loss: 0.803857, acc.: 53.12%] [G loss: 1.166491]\n",
      "epoch:31 step:29060 [D loss: 0.421437, acc.: 82.81%] [G loss: 1.969388]\n",
      "epoch:31 step:29061 [D loss: 0.327120, acc.: 89.06%] [G loss: 1.717146]\n",
      "epoch:31 step:29062 [D loss: 0.611588, acc.: 66.41%] [G loss: 1.453064]\n",
      "epoch:31 step:29063 [D loss: 0.440331, acc.: 82.81%] [G loss: 1.517599]\n",
      "epoch:31 step:29064 [D loss: 0.603770, acc.: 67.97%] [G loss: 1.563561]\n",
      "epoch:31 step:29065 [D loss: 0.494952, acc.: 75.78%] [G loss: 1.630852]\n",
      "epoch:31 step:29066 [D loss: 0.495083, acc.: 75.00%] [G loss: 1.274817]\n",
      "epoch:31 step:29067 [D loss: 0.535216, acc.: 72.66%] [G loss: 1.745131]\n",
      "epoch:31 step:29068 [D loss: 0.386074, acc.: 85.16%] [G loss: 1.565289]\n",
      "epoch:31 step:29069 [D loss: 0.738201, acc.: 55.47%] [G loss: 0.849907]\n",
      "epoch:31 step:29070 [D loss: 0.466356, acc.: 76.56%] [G loss: 1.632424]\n",
      "epoch:31 step:29071 [D loss: 0.643231, acc.: 63.28%] [G loss: 1.492627]\n",
      "epoch:31 step:29072 [D loss: 0.628738, acc.: 66.41%] [G loss: 1.573771]\n",
      "epoch:31 step:29073 [D loss: 0.599319, acc.: 70.31%] [G loss: 1.596117]\n",
      "epoch:31 step:29074 [D loss: 0.451410, acc.: 83.59%] [G loss: 1.206454]\n",
      "epoch:31 step:29075 [D loss: 0.473378, acc.: 79.69%] [G loss: 1.515032]\n",
      "epoch:31 step:29076 [D loss: 0.471071, acc.: 79.69%] [G loss: 1.553999]\n",
      "epoch:31 step:29077 [D loss: 0.327329, acc.: 92.97%] [G loss: 1.510247]\n",
      "epoch:31 step:29078 [D loss: 0.590480, acc.: 75.00%] [G loss: 1.587517]\n",
      "epoch:31 step:29079 [D loss: 0.395155, acc.: 85.16%] [G loss: 1.747321]\n",
      "epoch:31 step:29080 [D loss: 0.478990, acc.: 75.00%] [G loss: 1.239283]\n",
      "epoch:31 step:29081 [D loss: 0.466479, acc.: 76.56%] [G loss: 1.466684]\n",
      "epoch:31 step:29082 [D loss: 0.581576, acc.: 69.53%] [G loss: 1.518670]\n",
      "epoch:31 step:29083 [D loss: 0.548084, acc.: 73.44%] [G loss: 1.677094]\n",
      "epoch:31 step:29084 [D loss: 0.406504, acc.: 86.72%] [G loss: 1.503909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29085 [D loss: 0.572025, acc.: 71.09%] [G loss: 1.584059]\n",
      "epoch:31 step:29086 [D loss: 0.476349, acc.: 77.34%] [G loss: 1.595160]\n",
      "epoch:31 step:29087 [D loss: 0.736636, acc.: 57.03%] [G loss: 1.088320]\n",
      "epoch:31 step:29088 [D loss: 0.658994, acc.: 60.94%] [G loss: 0.911782]\n",
      "epoch:31 step:29089 [D loss: 0.619869, acc.: 68.75%] [G loss: 1.826976]\n",
      "epoch:31 step:29090 [D loss: 0.614988, acc.: 65.62%] [G loss: 1.605641]\n",
      "epoch:31 step:29091 [D loss: 0.490309, acc.: 79.69%] [G loss: 1.288612]\n",
      "epoch:31 step:29092 [D loss: 0.550246, acc.: 71.09%] [G loss: 1.163437]\n",
      "epoch:31 step:29093 [D loss: 0.550057, acc.: 74.22%] [G loss: 1.306717]\n",
      "epoch:31 step:29094 [D loss: 0.496929, acc.: 75.78%] [G loss: 1.671273]\n",
      "epoch:31 step:29095 [D loss: 0.587469, acc.: 64.84%] [G loss: 1.223757]\n",
      "epoch:31 step:29096 [D loss: 0.507972, acc.: 71.88%] [G loss: 1.527262]\n",
      "epoch:31 step:29097 [D loss: 0.556818, acc.: 73.44%] [G loss: 1.205478]\n",
      "epoch:31 step:29098 [D loss: 0.360259, acc.: 91.41%] [G loss: 1.693841]\n",
      "epoch:31 step:29099 [D loss: 0.619928, acc.: 66.41%] [G loss: 1.446794]\n",
      "epoch:31 step:29100 [D loss: 0.449460, acc.: 81.25%] [G loss: 1.287187]\n",
      "epoch:31 step:29101 [D loss: 0.431318, acc.: 81.25%] [G loss: 1.362233]\n",
      "epoch:31 step:29102 [D loss: 0.770117, acc.: 51.56%] [G loss: 1.046084]\n",
      "epoch:31 step:29103 [D loss: 0.443459, acc.: 81.25%] [G loss: 1.295167]\n",
      "epoch:31 step:29104 [D loss: 0.473185, acc.: 76.56%] [G loss: 1.458682]\n",
      "epoch:31 step:29105 [D loss: 0.479498, acc.: 77.34%] [G loss: 1.717061]\n",
      "epoch:31 step:29106 [D loss: 0.447608, acc.: 82.03%] [G loss: 1.315428]\n",
      "epoch:31 step:29107 [D loss: 0.460280, acc.: 78.12%] [G loss: 1.646609]\n",
      "epoch:31 step:29108 [D loss: 0.472452, acc.: 79.69%] [G loss: 1.120262]\n",
      "epoch:31 step:29109 [D loss: 0.556680, acc.: 67.19%] [G loss: 1.486811]\n",
      "epoch:31 step:29110 [D loss: 0.462744, acc.: 78.91%] [G loss: 1.565069]\n",
      "epoch:31 step:29111 [D loss: 0.722246, acc.: 60.16%] [G loss: 1.564510]\n",
      "epoch:31 step:29112 [D loss: 0.357250, acc.: 85.94%] [G loss: 1.630630]\n",
      "epoch:31 step:29113 [D loss: 0.707977, acc.: 60.16%] [G loss: 1.433428]\n",
      "epoch:31 step:29114 [D loss: 0.524915, acc.: 76.56%] [G loss: 1.409500]\n",
      "epoch:31 step:29115 [D loss: 0.577182, acc.: 67.97%] [G loss: 1.767381]\n",
      "epoch:31 step:29116 [D loss: 0.494482, acc.: 75.00%] [G loss: 2.028560]\n",
      "epoch:31 step:29117 [D loss: 0.461656, acc.: 81.25%] [G loss: 0.965614]\n",
      "epoch:31 step:29118 [D loss: 0.446372, acc.: 82.03%] [G loss: 1.502535]\n",
      "epoch:31 step:29119 [D loss: 0.438176, acc.: 84.38%] [G loss: 1.458411]\n",
      "epoch:31 step:29120 [D loss: 0.504602, acc.: 76.56%] [G loss: 1.275802]\n",
      "epoch:31 step:29121 [D loss: 0.592098, acc.: 74.22%] [G loss: 1.407592]\n",
      "epoch:31 step:29122 [D loss: 0.570167, acc.: 70.31%] [G loss: 1.780302]\n",
      "epoch:31 step:29123 [D loss: 0.607432, acc.: 67.19%] [G loss: 1.720622]\n",
      "epoch:31 step:29124 [D loss: 0.541917, acc.: 75.78%] [G loss: 1.454481]\n",
      "epoch:31 step:29125 [D loss: 0.579456, acc.: 69.53%] [G loss: 1.456235]\n",
      "epoch:31 step:29126 [D loss: 0.468244, acc.: 78.91%] [G loss: 1.295239]\n",
      "epoch:31 step:29127 [D loss: 0.644185, acc.: 62.50%] [G loss: 1.260715]\n",
      "epoch:31 step:29128 [D loss: 0.528803, acc.: 75.00%] [G loss: 1.278412]\n",
      "epoch:31 step:29129 [D loss: 0.444379, acc.: 78.91%] [G loss: 1.327757]\n",
      "epoch:31 step:29130 [D loss: 0.409262, acc.: 81.25%] [G loss: 1.609023]\n",
      "epoch:31 step:29131 [D loss: 0.654781, acc.: 60.94%] [G loss: 1.591651]\n",
      "epoch:31 step:29132 [D loss: 0.603419, acc.: 69.53%] [G loss: 1.190921]\n",
      "epoch:31 step:29133 [D loss: 0.478235, acc.: 77.34%] [G loss: 1.610626]\n",
      "epoch:31 step:29134 [D loss: 0.458703, acc.: 81.25%] [G loss: 1.338107]\n",
      "epoch:31 step:29135 [D loss: 0.678200, acc.: 58.59%] [G loss: 1.210842]\n",
      "epoch:31 step:29136 [D loss: 0.554554, acc.: 69.53%] [G loss: 1.421421]\n",
      "epoch:31 step:29137 [D loss: 0.473222, acc.: 78.91%] [G loss: 1.506665]\n",
      "epoch:31 step:29138 [D loss: 0.677608, acc.: 63.28%] [G loss: 1.525044]\n",
      "epoch:31 step:29139 [D loss: 0.409608, acc.: 81.25%] [G loss: 1.372287]\n",
      "epoch:31 step:29140 [D loss: 0.361881, acc.: 86.72%] [G loss: 1.338916]\n",
      "epoch:31 step:29141 [D loss: 0.465917, acc.: 80.47%] [G loss: 1.561024]\n",
      "epoch:31 step:29142 [D loss: 0.611912, acc.: 65.62%] [G loss: 1.270373]\n",
      "epoch:31 step:29143 [D loss: 0.419521, acc.: 85.16%] [G loss: 1.370770]\n",
      "epoch:31 step:29144 [D loss: 0.603282, acc.: 69.53%] [G loss: 1.288302]\n",
      "epoch:31 step:29145 [D loss: 0.579158, acc.: 69.53%] [G loss: 1.748220]\n",
      "epoch:31 step:29146 [D loss: 0.621943, acc.: 69.53%] [G loss: 1.679947]\n",
      "epoch:31 step:29147 [D loss: 0.530407, acc.: 77.34%] [G loss: 1.327785]\n",
      "epoch:31 step:29148 [D loss: 0.521066, acc.: 75.00%] [G loss: 1.614809]\n",
      "epoch:31 step:29149 [D loss: 0.820430, acc.: 47.66%] [G loss: 1.542563]\n",
      "epoch:31 step:29150 [D loss: 0.555268, acc.: 68.75%] [G loss: 1.381313]\n",
      "epoch:31 step:29151 [D loss: 0.576342, acc.: 71.09%] [G loss: 1.370954]\n",
      "epoch:31 step:29152 [D loss: 0.508898, acc.: 75.78%] [G loss: 2.068216]\n",
      "epoch:31 step:29153 [D loss: 0.643136, acc.: 65.62%] [G loss: 1.603227]\n",
      "epoch:31 step:29154 [D loss: 0.483139, acc.: 78.91%] [G loss: 1.402912]\n",
      "epoch:31 step:29155 [D loss: 0.348598, acc.: 86.72%] [G loss: 1.618077]\n",
      "epoch:31 step:29156 [D loss: 0.438251, acc.: 86.72%] [G loss: 1.385991]\n",
      "epoch:31 step:29157 [D loss: 0.660396, acc.: 61.72%] [G loss: 1.296457]\n",
      "epoch:31 step:29158 [D loss: 0.826035, acc.: 44.53%] [G loss: 1.216504]\n",
      "epoch:31 step:29159 [D loss: 0.514603, acc.: 75.00%] [G loss: 1.916319]\n",
      "epoch:31 step:29160 [D loss: 0.686891, acc.: 58.59%] [G loss: 1.550912]\n",
      "epoch:31 step:29161 [D loss: 0.511617, acc.: 76.56%] [G loss: 1.575345]\n",
      "epoch:31 step:29162 [D loss: 0.453813, acc.: 80.47%] [G loss: 1.189463]\n",
      "epoch:31 step:29163 [D loss: 0.585303, acc.: 68.75%] [G loss: 1.475808]\n",
      "epoch:31 step:29164 [D loss: 0.400572, acc.: 84.38%] [G loss: 1.926280]\n",
      "epoch:31 step:29165 [D loss: 0.525519, acc.: 77.34%] [G loss: 1.891717]\n",
      "epoch:31 step:29166 [D loss: 0.391241, acc.: 84.38%] [G loss: 1.485551]\n",
      "epoch:31 step:29167 [D loss: 0.775999, acc.: 53.12%] [G loss: 1.500113]\n",
      "epoch:31 step:29168 [D loss: 0.514123, acc.: 73.44%] [G loss: 1.731538]\n",
      "epoch:31 step:29169 [D loss: 0.664717, acc.: 60.94%] [G loss: 1.419203]\n",
      "epoch:31 step:29170 [D loss: 0.468395, acc.: 80.47%] [G loss: 1.948274]\n",
      "epoch:31 step:29171 [D loss: 0.440605, acc.: 80.47%] [G loss: 1.326409]\n",
      "epoch:31 step:29172 [D loss: 0.745531, acc.: 51.56%] [G loss: 1.673406]\n",
      "epoch:31 step:29173 [D loss: 0.609571, acc.: 70.31%] [G loss: 1.179872]\n",
      "epoch:31 step:29174 [D loss: 0.891061, acc.: 43.75%] [G loss: 1.229437]\n",
      "epoch:31 step:29175 [D loss: 0.646304, acc.: 64.06%] [G loss: 1.184777]\n",
      "epoch:31 step:29176 [D loss: 0.503069, acc.: 75.00%] [G loss: 1.493601]\n",
      "epoch:31 step:29177 [D loss: 0.454207, acc.: 80.47%] [G loss: 1.187285]\n",
      "epoch:31 step:29178 [D loss: 0.421526, acc.: 83.59%] [G loss: 1.556868]\n",
      "epoch:31 step:29179 [D loss: 0.600354, acc.: 67.97%] [G loss: 1.260887]\n",
      "epoch:31 step:29180 [D loss: 0.543293, acc.: 75.00%] [G loss: 1.363854]\n",
      "epoch:31 step:29181 [D loss: 0.701937, acc.: 59.38%] [G loss: 2.026327]\n",
      "epoch:31 step:29182 [D loss: 0.409236, acc.: 78.91%] [G loss: 1.361683]\n",
      "epoch:31 step:29183 [D loss: 0.485781, acc.: 74.22%] [G loss: 1.494158]\n",
      "epoch:31 step:29184 [D loss: 0.581258, acc.: 69.53%] [G loss: 1.620326]\n",
      "epoch:31 step:29185 [D loss: 0.628196, acc.: 66.41%] [G loss: 1.476176]\n",
      "epoch:31 step:29186 [D loss: 0.489355, acc.: 79.69%] [G loss: 0.821045]\n",
      "epoch:31 step:29187 [D loss: 0.435392, acc.: 81.25%] [G loss: 1.518198]\n",
      "epoch:31 step:29188 [D loss: 0.708786, acc.: 52.34%] [G loss: 1.100023]\n",
      "epoch:31 step:29189 [D loss: 0.243177, acc.: 95.31%] [G loss: 1.022533]\n",
      "epoch:31 step:29190 [D loss: 0.453223, acc.: 80.47%] [G loss: 1.441926]\n",
      "epoch:31 step:29191 [D loss: 0.392899, acc.: 85.16%] [G loss: 1.689121]\n",
      "epoch:31 step:29192 [D loss: 0.510813, acc.: 77.34%] [G loss: 1.463102]\n",
      "epoch:31 step:29193 [D loss: 0.508374, acc.: 73.44%] [G loss: 1.497971]\n",
      "epoch:31 step:29194 [D loss: 0.634233, acc.: 67.97%] [G loss: 1.206900]\n",
      "epoch:31 step:29195 [D loss: 0.575460, acc.: 70.31%] [G loss: 1.815138]\n",
      "epoch:31 step:29196 [D loss: 0.557813, acc.: 67.97%] [G loss: 1.454804]\n",
      "epoch:31 step:29197 [D loss: 0.662302, acc.: 60.16%] [G loss: 1.221409]\n",
      "epoch:31 step:29198 [D loss: 0.521520, acc.: 74.22%] [G loss: 1.274043]\n",
      "epoch:31 step:29199 [D loss: 0.367578, acc.: 85.16%] [G loss: 1.339938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29200 [D loss: 0.511052, acc.: 76.56%] [G loss: 1.314696]\n",
      "##############\n",
      "[2.77712942 2.20466571 1.75095856 2.91320417 0.8061092  5.95189607\n",
      " 2.20658406 2.90079269 3.88906365 7.14868929]\n",
      "##########\n",
      "epoch:31 step:29201 [D loss: 0.562387, acc.: 71.09%] [G loss: 1.510226]\n",
      "epoch:31 step:29202 [D loss: 0.501157, acc.: 74.22%] [G loss: 1.285641]\n",
      "epoch:31 step:29203 [D loss: 0.549047, acc.: 72.66%] [G loss: 1.351014]\n",
      "epoch:31 step:29204 [D loss: 0.485957, acc.: 75.78%] [G loss: 1.765578]\n",
      "epoch:31 step:29205 [D loss: 0.619692, acc.: 65.62%] [G loss: 1.152078]\n",
      "epoch:31 step:29206 [D loss: 0.352132, acc.: 89.06%] [G loss: 1.929585]\n",
      "epoch:31 step:29207 [D loss: 0.501601, acc.: 78.12%] [G loss: 1.818472]\n",
      "epoch:31 step:29208 [D loss: 0.567784, acc.: 68.75%] [G loss: 1.588904]\n",
      "epoch:31 step:29209 [D loss: 0.597454, acc.: 66.41%] [G loss: 1.496620]\n",
      "epoch:31 step:29210 [D loss: 0.539772, acc.: 71.88%] [G loss: 1.275912]\n",
      "epoch:31 step:29211 [D loss: 0.342491, acc.: 88.28%] [G loss: 1.269139]\n",
      "epoch:31 step:29212 [D loss: 0.439205, acc.: 80.47%] [G loss: 1.139922]\n",
      "epoch:31 step:29213 [D loss: 0.334729, acc.: 87.50%] [G loss: 1.199955]\n",
      "epoch:31 step:29214 [D loss: 0.584882, acc.: 68.75%] [G loss: 1.664541]\n",
      "epoch:31 step:29215 [D loss: 0.354689, acc.: 89.06%] [G loss: 1.188520]\n",
      "epoch:31 step:29216 [D loss: 0.458358, acc.: 83.59%] [G loss: 1.331997]\n",
      "epoch:31 step:29217 [D loss: 0.629047, acc.: 67.97%] [G loss: 1.033115]\n",
      "epoch:31 step:29218 [D loss: 0.523301, acc.: 71.88%] [G loss: 1.761056]\n",
      "epoch:31 step:29219 [D loss: 0.480665, acc.: 78.91%] [G loss: 1.558363]\n",
      "epoch:31 step:29220 [D loss: 0.555928, acc.: 70.31%] [G loss: 1.511514]\n",
      "epoch:31 step:29221 [D loss: 0.524456, acc.: 75.78%] [G loss: 1.238626]\n",
      "epoch:31 step:29222 [D loss: 0.591549, acc.: 66.41%] [G loss: 1.175969]\n",
      "epoch:31 step:29223 [D loss: 0.443181, acc.: 78.12%] [G loss: 1.225773]\n",
      "epoch:31 step:29224 [D loss: 0.461839, acc.: 78.91%] [G loss: 1.164838]\n",
      "epoch:31 step:29225 [D loss: 0.471681, acc.: 81.25%] [G loss: 1.645245]\n",
      "epoch:31 step:29226 [D loss: 0.570049, acc.: 68.75%] [G loss: 1.702807]\n",
      "epoch:31 step:29227 [D loss: 0.622179, acc.: 68.75%] [G loss: 1.375913]\n",
      "epoch:31 step:29228 [D loss: 0.654311, acc.: 64.84%] [G loss: 1.722964]\n",
      "epoch:31 step:29229 [D loss: 0.458606, acc.: 78.91%] [G loss: 1.759463]\n",
      "epoch:31 step:29230 [D loss: 0.437553, acc.: 80.47%] [G loss: 1.642990]\n",
      "epoch:31 step:29231 [D loss: 0.557450, acc.: 69.53%] [G loss: 1.870441]\n",
      "epoch:31 step:29232 [D loss: 0.469755, acc.: 79.69%] [G loss: 1.207967]\n",
      "epoch:31 step:29233 [D loss: 0.576282, acc.: 70.31%] [G loss: 1.072660]\n",
      "epoch:31 step:29234 [D loss: 0.524260, acc.: 76.56%] [G loss: 1.202274]\n",
      "epoch:31 step:29235 [D loss: 0.495961, acc.: 75.00%] [G loss: 1.184401]\n",
      "epoch:31 step:29236 [D loss: 0.627585, acc.: 63.28%] [G loss: 1.522112]\n",
      "epoch:31 step:29237 [D loss: 0.561274, acc.: 70.31%] [G loss: 1.036640]\n",
      "epoch:31 step:29238 [D loss: 0.629027, acc.: 60.16%] [G loss: 1.660754]\n",
      "epoch:31 step:29239 [D loss: 0.599629, acc.: 65.62%] [G loss: 1.449692]\n",
      "epoch:31 step:29240 [D loss: 0.450859, acc.: 81.25%] [G loss: 1.799879]\n",
      "epoch:31 step:29241 [D loss: 0.677320, acc.: 60.16%] [G loss: 1.266179]\n",
      "epoch:31 step:29242 [D loss: 0.687895, acc.: 64.84%] [G loss: 1.685479]\n",
      "epoch:31 step:29243 [D loss: 0.426622, acc.: 79.69%] [G loss: 1.650061]\n",
      "epoch:31 step:29244 [D loss: 0.518081, acc.: 75.78%] [G loss: 1.526399]\n",
      "epoch:31 step:29245 [D loss: 0.502067, acc.: 77.34%] [G loss: 1.466249]\n",
      "epoch:31 step:29246 [D loss: 0.662673, acc.: 61.72%] [G loss: 1.451481]\n",
      "epoch:31 step:29247 [D loss: 0.518338, acc.: 75.00%] [G loss: 1.766487]\n",
      "epoch:31 step:29248 [D loss: 0.423098, acc.: 82.81%] [G loss: 1.134674]\n",
      "epoch:31 step:29249 [D loss: 0.682537, acc.: 61.72%] [G loss: 1.630270]\n",
      "epoch:31 step:29250 [D loss: 0.485832, acc.: 74.22%] [G loss: 1.072904]\n",
      "epoch:31 step:29251 [D loss: 0.369803, acc.: 86.72%] [G loss: 1.637740]\n",
      "epoch:31 step:29252 [D loss: 0.646387, acc.: 59.38%] [G loss: 1.432343]\n",
      "epoch:31 step:29253 [D loss: 0.425478, acc.: 80.47%] [G loss: 1.364225]\n",
      "epoch:31 step:29254 [D loss: 0.457081, acc.: 80.47%] [G loss: 1.297250]\n",
      "epoch:31 step:29255 [D loss: 0.657229, acc.: 64.06%] [G loss: 1.201719]\n",
      "epoch:31 step:29256 [D loss: 0.359556, acc.: 85.16%] [G loss: 2.347553]\n",
      "epoch:31 step:29257 [D loss: 0.344252, acc.: 86.72%] [G loss: 1.480309]\n",
      "epoch:31 step:29258 [D loss: 0.416834, acc.: 83.59%] [G loss: 2.070324]\n",
      "epoch:31 step:29259 [D loss: 0.393966, acc.: 85.16%] [G loss: 1.434208]\n",
      "epoch:31 step:29260 [D loss: 0.517463, acc.: 73.44%] [G loss: 1.642301]\n",
      "epoch:31 step:29261 [D loss: 0.755396, acc.: 55.47%] [G loss: 1.206898]\n",
      "epoch:31 step:29262 [D loss: 0.459029, acc.: 81.25%] [G loss: 1.551555]\n",
      "epoch:31 step:29263 [D loss: 0.469217, acc.: 75.00%] [G loss: 1.523007]\n",
      "epoch:31 step:29264 [D loss: 0.447187, acc.: 82.03%] [G loss: 1.583380]\n",
      "epoch:31 step:29265 [D loss: 0.518332, acc.: 76.56%] [G loss: 1.810433]\n",
      "epoch:31 step:29266 [D loss: 0.399939, acc.: 85.16%] [G loss: 1.331946]\n",
      "epoch:31 step:29267 [D loss: 0.460521, acc.: 77.34%] [G loss: 1.830170]\n",
      "epoch:31 step:29268 [D loss: 0.438227, acc.: 83.59%] [G loss: 2.013941]\n",
      "epoch:31 step:29269 [D loss: 0.850444, acc.: 46.09%] [G loss: 1.667379]\n",
      "epoch:31 step:29270 [D loss: 0.584782, acc.: 70.31%] [G loss: 1.080017]\n",
      "epoch:31 step:29271 [D loss: 0.549906, acc.: 72.66%] [G loss: 1.226516]\n",
      "epoch:31 step:29272 [D loss: 0.608761, acc.: 67.97%] [G loss: 1.119440]\n",
      "epoch:31 step:29273 [D loss: 0.399331, acc.: 80.47%] [G loss: 2.181128]\n",
      "epoch:31 step:29274 [D loss: 0.621404, acc.: 67.19%] [G loss: 1.028885]\n",
      "epoch:31 step:29275 [D loss: 0.647888, acc.: 62.50%] [G loss: 1.566031]\n",
      "epoch:31 step:29276 [D loss: 0.434606, acc.: 81.25%] [G loss: 1.663521]\n",
      "epoch:31 step:29277 [D loss: 0.459694, acc.: 83.59%] [G loss: 1.633290]\n",
      "epoch:31 step:29278 [D loss: 0.416747, acc.: 79.69%] [G loss: 1.630233]\n",
      "epoch:31 step:29279 [D loss: 0.450787, acc.: 79.69%] [G loss: 1.574955]\n",
      "epoch:31 step:29280 [D loss: 0.599944, acc.: 66.41%] [G loss: 1.071055]\n",
      "epoch:31 step:29281 [D loss: 0.465263, acc.: 81.25%] [G loss: 1.449580]\n",
      "epoch:31 step:29282 [D loss: 0.530598, acc.: 72.66%] [G loss: 1.784613]\n",
      "epoch:31 step:29283 [D loss: 0.478794, acc.: 78.12%] [G loss: 1.355584]\n",
      "epoch:31 step:29284 [D loss: 0.613606, acc.: 63.28%] [G loss: 1.230545]\n",
      "epoch:31 step:29285 [D loss: 0.541808, acc.: 73.44%] [G loss: 1.200066]\n",
      "epoch:31 step:29286 [D loss: 0.388309, acc.: 85.94%] [G loss: 1.875901]\n",
      "epoch:31 step:29287 [D loss: 0.635879, acc.: 60.94%] [G loss: 1.700674]\n",
      "epoch:31 step:29288 [D loss: 0.486629, acc.: 79.69%] [G loss: 1.864904]\n",
      "epoch:31 step:29289 [D loss: 0.560938, acc.: 67.97%] [G loss: 1.144883]\n",
      "epoch:31 step:29290 [D loss: 0.496519, acc.: 76.56%] [G loss: 1.523080]\n",
      "epoch:31 step:29291 [D loss: 0.438557, acc.: 81.25%] [G loss: 1.205410]\n",
      "epoch:31 step:29292 [D loss: 0.464275, acc.: 79.69%] [G loss: 0.914628]\n",
      "epoch:31 step:29293 [D loss: 0.574358, acc.: 67.19%] [G loss: 1.318194]\n",
      "epoch:31 step:29294 [D loss: 0.544685, acc.: 71.09%] [G loss: 1.728666]\n",
      "epoch:31 step:29295 [D loss: 0.454866, acc.: 81.25%] [G loss: 1.249284]\n",
      "epoch:31 step:29296 [D loss: 0.242503, acc.: 95.31%] [G loss: 2.178167]\n",
      "epoch:31 step:29297 [D loss: 0.393279, acc.: 84.38%] [G loss: 1.716226]\n",
      "epoch:31 step:29298 [D loss: 0.599270, acc.: 66.41%] [G loss: 1.593001]\n",
      "epoch:31 step:29299 [D loss: 0.426116, acc.: 83.59%] [G loss: 1.767413]\n",
      "epoch:31 step:29300 [D loss: 0.450620, acc.: 78.91%] [G loss: 1.320771]\n",
      "epoch:31 step:29301 [D loss: 0.648652, acc.: 67.19%] [G loss: 1.515138]\n",
      "epoch:31 step:29302 [D loss: 0.550790, acc.: 68.75%] [G loss: 1.400206]\n",
      "epoch:31 step:29303 [D loss: 0.584928, acc.: 69.53%] [G loss: 1.283708]\n",
      "epoch:31 step:29304 [D loss: 0.457119, acc.: 81.25%] [G loss: 0.997995]\n",
      "epoch:31 step:29305 [D loss: 0.427804, acc.: 81.25%] [G loss: 1.348219]\n",
      "epoch:31 step:29306 [D loss: 0.697659, acc.: 57.81%] [G loss: 1.110693]\n",
      "epoch:31 step:29307 [D loss: 0.494009, acc.: 76.56%] [G loss: 1.273396]\n",
      "epoch:31 step:29308 [D loss: 0.586831, acc.: 70.31%] [G loss: 1.237741]\n",
      "epoch:31 step:29309 [D loss: 0.664578, acc.: 60.16%] [G loss: 1.314440]\n",
      "epoch:31 step:29310 [D loss: 0.656566, acc.: 60.16%] [G loss: 1.399786]\n",
      "epoch:31 step:29311 [D loss: 0.462802, acc.: 79.69%] [G loss: 1.830054]\n",
      "epoch:31 step:29312 [D loss: 0.420795, acc.: 85.16%] [G loss: 1.839694]\n",
      "epoch:31 step:29313 [D loss: 0.606656, acc.: 70.31%] [G loss: 1.706586]\n",
      "epoch:31 step:29314 [D loss: 0.587771, acc.: 70.31%] [G loss: 0.978872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29315 [D loss: 0.599267, acc.: 66.41%] [G loss: 1.717881]\n",
      "epoch:31 step:29316 [D loss: 0.503805, acc.: 73.44%] [G loss: 1.433698]\n",
      "epoch:31 step:29317 [D loss: 0.779470, acc.: 53.12%] [G loss: 1.100155]\n",
      "epoch:31 step:29318 [D loss: 0.390217, acc.: 86.72%] [G loss: 1.413946]\n",
      "epoch:31 step:29319 [D loss: 0.545783, acc.: 75.00%] [G loss: 1.636751]\n",
      "epoch:31 step:29320 [D loss: 0.685170, acc.: 63.28%] [G loss: 1.522249]\n",
      "epoch:31 step:29321 [D loss: 0.483423, acc.: 79.69%] [G loss: 1.032153]\n",
      "epoch:31 step:29322 [D loss: 0.731170, acc.: 54.69%] [G loss: 1.370007]\n",
      "epoch:31 step:29323 [D loss: 0.564258, acc.: 74.22%] [G loss: 1.341186]\n",
      "epoch:31 step:29324 [D loss: 0.596798, acc.: 71.88%] [G loss: 1.038474]\n",
      "epoch:31 step:29325 [D loss: 0.677028, acc.: 59.38%] [G loss: 1.294353]\n",
      "epoch:31 step:29326 [D loss: 0.511877, acc.: 71.09%] [G loss: 1.795241]\n",
      "epoch:31 step:29327 [D loss: 0.588674, acc.: 69.53%] [G loss: 1.598283]\n",
      "epoch:31 step:29328 [D loss: 0.624339, acc.: 67.97%] [G loss: 1.109556]\n",
      "epoch:31 step:29329 [D loss: 0.442657, acc.: 78.12%] [G loss: 1.309254]\n",
      "epoch:31 step:29330 [D loss: 0.530413, acc.: 72.66%] [G loss: 1.480984]\n",
      "epoch:31 step:29331 [D loss: 0.471357, acc.: 82.81%] [G loss: 1.752932]\n",
      "epoch:31 step:29332 [D loss: 0.435527, acc.: 82.03%] [G loss: 0.929662]\n",
      "epoch:31 step:29333 [D loss: 0.614554, acc.: 74.22%] [G loss: 1.568694]\n",
      "epoch:31 step:29334 [D loss: 0.592623, acc.: 67.97%] [G loss: 0.893529]\n",
      "epoch:31 step:29335 [D loss: 0.549204, acc.: 75.78%] [G loss: 1.146379]\n",
      "epoch:31 step:29336 [D loss: 0.738275, acc.: 54.69%] [G loss: 1.245993]\n",
      "epoch:31 step:29337 [D loss: 0.525371, acc.: 67.97%] [G loss: 1.625922]\n",
      "epoch:31 step:29338 [D loss: 0.534173, acc.: 77.34%] [G loss: 1.793849]\n",
      "epoch:31 step:29339 [D loss: 0.448402, acc.: 80.47%] [G loss: 1.233094]\n",
      "epoch:31 step:29340 [D loss: 0.627224, acc.: 66.41%] [G loss: 1.231984]\n",
      "epoch:31 step:29341 [D loss: 0.685711, acc.: 62.50%] [G loss: 1.040684]\n",
      "epoch:31 step:29342 [D loss: 0.416000, acc.: 82.81%] [G loss: 1.782856]\n",
      "epoch:31 step:29343 [D loss: 0.632985, acc.: 60.94%] [G loss: 1.334932]\n",
      "epoch:31 step:29344 [D loss: 0.493556, acc.: 75.00%] [G loss: 1.303627]\n",
      "epoch:31 step:29345 [D loss: 0.606540, acc.: 62.50%] [G loss: 2.012406]\n",
      "epoch:31 step:29346 [D loss: 0.434086, acc.: 83.59%] [G loss: 1.040494]\n",
      "epoch:31 step:29347 [D loss: 0.494918, acc.: 75.00%] [G loss: 0.786556]\n",
      "epoch:31 step:29348 [D loss: 0.670665, acc.: 59.38%] [G loss: 1.585277]\n",
      "epoch:31 step:29349 [D loss: 0.497558, acc.: 76.56%] [G loss: 1.234701]\n",
      "epoch:31 step:29350 [D loss: 0.787606, acc.: 51.56%] [G loss: 1.257249]\n",
      "epoch:31 step:29351 [D loss: 0.475438, acc.: 78.91%] [G loss: 1.295622]\n",
      "epoch:31 step:29352 [D loss: 0.407547, acc.: 84.38%] [G loss: 1.828691]\n",
      "epoch:31 step:29353 [D loss: 0.499273, acc.: 75.78%] [G loss: 1.367100]\n",
      "epoch:31 step:29354 [D loss: 0.690567, acc.: 60.16%] [G loss: 1.289975]\n",
      "epoch:31 step:29355 [D loss: 0.464077, acc.: 82.03%] [G loss: 1.330343]\n",
      "epoch:31 step:29356 [D loss: 0.571658, acc.: 72.66%] [G loss: 1.202782]\n",
      "epoch:31 step:29357 [D loss: 0.529384, acc.: 71.09%] [G loss: 1.516247]\n",
      "epoch:31 step:29358 [D loss: 0.424271, acc.: 78.91%] [G loss: 1.750329]\n",
      "epoch:31 step:29359 [D loss: 0.394365, acc.: 85.16%] [G loss: 1.380560]\n",
      "epoch:31 step:29360 [D loss: 0.632780, acc.: 64.84%] [G loss: 1.205745]\n",
      "epoch:31 step:29361 [D loss: 0.434300, acc.: 78.12%] [G loss: 1.314797]\n",
      "epoch:31 step:29362 [D loss: 0.620793, acc.: 67.97%] [G loss: 1.876166]\n",
      "epoch:31 step:29363 [D loss: 0.681175, acc.: 62.50%] [G loss: 1.122812]\n",
      "epoch:31 step:29364 [D loss: 0.692007, acc.: 57.81%] [G loss: 1.142745]\n",
      "epoch:31 step:29365 [D loss: 0.538519, acc.: 74.22%] [G loss: 1.536994]\n",
      "epoch:31 step:29366 [D loss: 0.532454, acc.: 77.34%] [G loss: 1.713534]\n",
      "epoch:31 step:29367 [D loss: 0.390892, acc.: 83.59%] [G loss: 1.597947]\n",
      "epoch:31 step:29368 [D loss: 0.595787, acc.: 67.97%] [G loss: 1.298454]\n",
      "epoch:31 step:29369 [D loss: 0.617394, acc.: 69.53%] [G loss: 1.774613]\n",
      "epoch:31 step:29370 [D loss: 0.374514, acc.: 85.94%] [G loss: 1.382771]\n",
      "epoch:31 step:29371 [D loss: 0.353242, acc.: 89.06%] [G loss: 1.354392]\n",
      "epoch:31 step:29372 [D loss: 0.548931, acc.: 73.44%] [G loss: 1.335618]\n",
      "epoch:31 step:29373 [D loss: 0.474383, acc.: 80.47%] [G loss: 1.535568]\n",
      "epoch:31 step:29374 [D loss: 0.437260, acc.: 83.59%] [G loss: 1.634291]\n",
      "epoch:31 step:29375 [D loss: 0.446599, acc.: 78.12%] [G loss: 1.494021]\n",
      "epoch:31 step:29376 [D loss: 0.499775, acc.: 78.12%] [G loss: 1.661226]\n",
      "epoch:31 step:29377 [D loss: 0.654751, acc.: 60.94%] [G loss: 1.317578]\n",
      "epoch:31 step:29378 [D loss: 0.409561, acc.: 81.25%] [G loss: 1.390625]\n",
      "epoch:31 step:29379 [D loss: 0.586716, acc.: 71.09%] [G loss: 1.444369]\n",
      "epoch:31 step:29380 [D loss: 0.401567, acc.: 80.47%] [G loss: 1.670442]\n",
      "epoch:31 step:29381 [D loss: 0.465449, acc.: 80.47%] [G loss: 1.465235]\n",
      "epoch:31 step:29382 [D loss: 0.756231, acc.: 55.47%] [G loss: 1.302104]\n",
      "epoch:31 step:29383 [D loss: 0.404548, acc.: 82.03%] [G loss: 1.577973]\n",
      "epoch:31 step:29384 [D loss: 0.541153, acc.: 75.78%] [G loss: 1.652881]\n",
      "epoch:31 step:29385 [D loss: 0.526111, acc.: 78.12%] [G loss: 1.369039]\n",
      "epoch:31 step:29386 [D loss: 0.555184, acc.: 70.31%] [G loss: 1.564406]\n",
      "epoch:31 step:29387 [D loss: 0.545840, acc.: 69.53%] [G loss: 1.369127]\n",
      "epoch:31 step:29388 [D loss: 0.548888, acc.: 69.53%] [G loss: 1.172627]\n",
      "epoch:31 step:29389 [D loss: 0.608772, acc.: 67.19%] [G loss: 1.711209]\n",
      "epoch:31 step:29390 [D loss: 0.650188, acc.: 62.50%] [G loss: 1.109401]\n",
      "epoch:31 step:29391 [D loss: 0.447139, acc.: 82.03%] [G loss: 1.636060]\n",
      "epoch:31 step:29392 [D loss: 0.464872, acc.: 78.91%] [G loss: 1.537402]\n",
      "epoch:31 step:29393 [D loss: 0.491469, acc.: 74.22%] [G loss: 1.214558]\n",
      "epoch:31 step:29394 [D loss: 0.672986, acc.: 65.62%] [G loss: 1.237959]\n",
      "epoch:31 step:29395 [D loss: 0.658071, acc.: 61.72%] [G loss: 1.594648]\n",
      "epoch:31 step:29396 [D loss: 0.502601, acc.: 75.00%] [G loss: 1.379020]\n",
      "epoch:31 step:29397 [D loss: 0.579709, acc.: 74.22%] [G loss: 1.587351]\n",
      "epoch:31 step:29398 [D loss: 0.594329, acc.: 67.97%] [G loss: 1.671413]\n",
      "epoch:31 step:29399 [D loss: 0.700325, acc.: 57.03%] [G loss: 1.280042]\n",
      "epoch:31 step:29400 [D loss: 0.496182, acc.: 74.22%] [G loss: 1.196041]\n",
      "##############\n",
      "[2.55128376 1.85445654 1.90103555 3.11078118 0.77819306 6.87111411\n",
      " 2.37371105 2.6929465  3.97001505 7.14868929]\n",
      "##########\n",
      "epoch:31 step:29401 [D loss: 0.669267, acc.: 58.59%] [G loss: 1.424325]\n",
      "epoch:31 step:29402 [D loss: 0.635683, acc.: 64.06%] [G loss: 1.236078]\n",
      "epoch:31 step:29403 [D loss: 0.460481, acc.: 80.47%] [G loss: 1.749386]\n",
      "epoch:31 step:29404 [D loss: 0.512904, acc.: 74.22%] [G loss: 1.500481]\n",
      "epoch:31 step:29405 [D loss: 0.666530, acc.: 60.94%] [G loss: 1.289405]\n",
      "epoch:31 step:29406 [D loss: 0.550949, acc.: 75.00%] [G loss: 1.345989]\n",
      "epoch:31 step:29407 [D loss: 0.499313, acc.: 76.56%] [G loss: 1.243339]\n",
      "epoch:31 step:29408 [D loss: 0.639969, acc.: 64.84%] [G loss: 1.566664]\n",
      "epoch:31 step:29409 [D loss: 0.543844, acc.: 75.78%] [G loss: 1.158149]\n",
      "epoch:31 step:29410 [D loss: 0.466188, acc.: 78.12%] [G loss: 1.552647]\n",
      "epoch:31 step:29411 [D loss: 0.586144, acc.: 70.31%] [G loss: 1.243552]\n",
      "epoch:31 step:29412 [D loss: 0.410641, acc.: 82.03%] [G loss: 1.386012]\n",
      "epoch:31 step:29413 [D loss: 0.476159, acc.: 76.56%] [G loss: 1.384689]\n",
      "epoch:31 step:29414 [D loss: 0.606672, acc.: 68.75%] [G loss: 1.274539]\n",
      "epoch:31 step:29415 [D loss: 0.356615, acc.: 87.50%] [G loss: 1.604437]\n",
      "epoch:31 step:29416 [D loss: 0.509465, acc.: 78.12%] [G loss: 1.598428]\n",
      "epoch:31 step:29417 [D loss: 0.393945, acc.: 81.25%] [G loss: 1.450007]\n",
      "epoch:31 step:29418 [D loss: 0.528373, acc.: 74.22%] [G loss: 1.462973]\n",
      "epoch:31 step:29419 [D loss: 0.548859, acc.: 69.53%] [G loss: 1.338088]\n",
      "epoch:31 step:29420 [D loss: 0.526223, acc.: 78.91%] [G loss: 1.363016]\n",
      "epoch:31 step:29421 [D loss: 0.549179, acc.: 70.31%] [G loss: 1.592429]\n",
      "epoch:31 step:29422 [D loss: 0.502631, acc.: 77.34%] [G loss: 1.600723]\n",
      "epoch:31 step:29423 [D loss: 0.630474, acc.: 60.16%] [G loss: 1.322816]\n",
      "epoch:31 step:29424 [D loss: 0.446859, acc.: 77.34%] [G loss: 1.365572]\n",
      "epoch:31 step:29425 [D loss: 0.542629, acc.: 71.88%] [G loss: 1.610770]\n",
      "epoch:31 step:29426 [D loss: 0.582435, acc.: 71.09%] [G loss: 1.554986]\n",
      "epoch:31 step:29427 [D loss: 0.487070, acc.: 82.03%] [G loss: 1.483708]\n",
      "epoch:31 step:29428 [D loss: 0.625844, acc.: 69.53%] [G loss: 1.162431]\n",
      "epoch:31 step:29429 [D loss: 0.371362, acc.: 85.94%] [G loss: 1.814922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29430 [D loss: 0.507381, acc.: 75.00%] [G loss: 1.249196]\n",
      "epoch:31 step:29431 [D loss: 0.500288, acc.: 76.56%] [G loss: 2.018197]\n",
      "epoch:31 step:29432 [D loss: 0.519945, acc.: 72.66%] [G loss: 1.699480]\n",
      "epoch:31 step:29433 [D loss: 0.370513, acc.: 85.94%] [G loss: 1.288525]\n",
      "epoch:31 step:29434 [D loss: 0.526722, acc.: 73.44%] [G loss: 1.802151]\n",
      "epoch:31 step:29435 [D loss: 0.827447, acc.: 52.34%] [G loss: 1.109651]\n",
      "epoch:31 step:29436 [D loss: 0.515973, acc.: 72.66%] [G loss: 1.263663]\n",
      "epoch:31 step:29437 [D loss: 0.633666, acc.: 67.19%] [G loss: 1.544012]\n",
      "epoch:31 step:29438 [D loss: 0.591032, acc.: 65.62%] [G loss: 1.212566]\n",
      "epoch:31 step:29439 [D loss: 0.537690, acc.: 73.44%] [G loss: 1.063832]\n",
      "epoch:31 step:29440 [D loss: 0.707692, acc.: 58.59%] [G loss: 1.684227]\n",
      "epoch:31 step:29441 [D loss: 0.466266, acc.: 79.69%] [G loss: 1.529910]\n",
      "epoch:31 step:29442 [D loss: 0.617342, acc.: 68.75%] [G loss: 1.418255]\n",
      "epoch:31 step:29443 [D loss: 0.435511, acc.: 84.38%] [G loss: 1.318967]\n",
      "epoch:31 step:29444 [D loss: 0.597504, acc.: 64.84%] [G loss: 1.123604]\n",
      "epoch:31 step:29445 [D loss: 0.501187, acc.: 81.25%] [G loss: 1.259694]\n",
      "epoch:31 step:29446 [D loss: 0.529011, acc.: 71.09%] [G loss: 1.648013]\n",
      "epoch:31 step:29447 [D loss: 0.615684, acc.: 66.41%] [G loss: 1.083725]\n",
      "epoch:31 step:29448 [D loss: 0.472356, acc.: 78.12%] [G loss: 1.349904]\n",
      "epoch:31 step:29449 [D loss: 0.736940, acc.: 56.25%] [G loss: 1.273515]\n",
      "epoch:31 step:29450 [D loss: 0.451630, acc.: 78.12%] [G loss: 1.638386]\n",
      "epoch:31 step:29451 [D loss: 0.777036, acc.: 53.91%] [G loss: 1.397602]\n",
      "epoch:31 step:29452 [D loss: 0.673264, acc.: 61.72%] [G loss: 1.580581]\n",
      "epoch:31 step:29453 [D loss: 0.483770, acc.: 75.78%] [G loss: 1.500613]\n",
      "epoch:31 step:29454 [D loss: 0.484174, acc.: 75.78%] [G loss: 1.481829]\n",
      "epoch:31 step:29455 [D loss: 0.623611, acc.: 70.31%] [G loss: 1.194152]\n",
      "epoch:31 step:29456 [D loss: 0.555534, acc.: 76.56%] [G loss: 1.920648]\n",
      "epoch:31 step:29457 [D loss: 0.568782, acc.: 67.97%] [G loss: 0.885249]\n",
      "epoch:31 step:29458 [D loss: 0.393631, acc.: 84.38%] [G loss: 1.822787]\n",
      "epoch:31 step:29459 [D loss: 0.509349, acc.: 76.56%] [G loss: 1.752851]\n",
      "epoch:31 step:29460 [D loss: 0.524354, acc.: 76.56%] [G loss: 1.328597]\n",
      "epoch:31 step:29461 [D loss: 0.537257, acc.: 75.00%] [G loss: 1.611341]\n",
      "epoch:31 step:29462 [D loss: 0.320101, acc.: 88.28%] [G loss: 1.617395]\n",
      "epoch:31 step:29463 [D loss: 0.621712, acc.: 64.84%] [G loss: 1.098096]\n",
      "epoch:31 step:29464 [D loss: 0.588592, acc.: 64.06%] [G loss: 1.351007]\n",
      "epoch:31 step:29465 [D loss: 0.522710, acc.: 73.44%] [G loss: 1.339382]\n",
      "epoch:31 step:29466 [D loss: 0.414810, acc.: 79.69%] [G loss: 1.483813]\n",
      "epoch:31 step:29467 [D loss: 0.461147, acc.: 80.47%] [G loss: 1.491436]\n",
      "epoch:31 step:29468 [D loss: 0.392056, acc.: 86.72%] [G loss: 1.229826]\n",
      "epoch:31 step:29469 [D loss: 0.680075, acc.: 63.28%] [G loss: 1.242404]\n",
      "epoch:31 step:29470 [D loss: 0.437396, acc.: 82.03%] [G loss: 1.299493]\n",
      "epoch:31 step:29471 [D loss: 0.588446, acc.: 69.53%] [G loss: 1.319674]\n",
      "epoch:31 step:29472 [D loss: 0.474523, acc.: 80.47%] [G loss: 1.838947]\n",
      "epoch:31 step:29473 [D loss: 0.664205, acc.: 64.06%] [G loss: 1.239061]\n",
      "epoch:31 step:29474 [D loss: 0.492053, acc.: 74.22%] [G loss: 1.265360]\n",
      "epoch:31 step:29475 [D loss: 0.573005, acc.: 76.56%] [G loss: 1.383604]\n",
      "epoch:31 step:29476 [D loss: 0.626158, acc.: 68.75%] [G loss: 0.672405]\n",
      "epoch:31 step:29477 [D loss: 0.617660, acc.: 63.28%] [G loss: 1.267268]\n",
      "epoch:31 step:29478 [D loss: 0.431315, acc.: 83.59%] [G loss: 1.563812]\n",
      "epoch:31 step:29479 [D loss: 0.612526, acc.: 70.31%] [G loss: 1.222756]\n",
      "epoch:31 step:29480 [D loss: 0.588194, acc.: 71.09%] [G loss: 1.142210]\n",
      "epoch:31 step:29481 [D loss: 0.445558, acc.: 75.78%] [G loss: 1.880749]\n",
      "epoch:31 step:29482 [D loss: 0.494652, acc.: 75.00%] [G loss: 1.826208]\n",
      "epoch:31 step:29483 [D loss: 0.483867, acc.: 76.56%] [G loss: 1.320798]\n",
      "epoch:31 step:29484 [D loss: 0.700151, acc.: 57.81%] [G loss: 1.130931]\n",
      "epoch:31 step:29485 [D loss: 0.561095, acc.: 68.75%] [G loss: 1.345366]\n",
      "epoch:31 step:29486 [D loss: 0.779200, acc.: 47.66%] [G loss: 1.538778]\n",
      "epoch:31 step:29487 [D loss: 0.520779, acc.: 75.00%] [G loss: 1.839198]\n",
      "epoch:31 step:29488 [D loss: 0.372491, acc.: 87.50%] [G loss: 1.503386]\n",
      "epoch:31 step:29489 [D loss: 0.447524, acc.: 80.47%] [G loss: 1.841865]\n",
      "epoch:31 step:29490 [D loss: 0.398716, acc.: 86.72%] [G loss: 1.529799]\n",
      "epoch:31 step:29491 [D loss: 0.593507, acc.: 71.88%] [G loss: 1.152397]\n",
      "epoch:31 step:29492 [D loss: 0.498364, acc.: 78.12%] [G loss: 1.608015]\n",
      "epoch:31 step:29493 [D loss: 0.401782, acc.: 82.81%] [G loss: 1.677927]\n",
      "epoch:31 step:29494 [D loss: 0.479689, acc.: 76.56%] [G loss: 1.814734]\n",
      "epoch:31 step:29495 [D loss: 0.372905, acc.: 85.94%] [G loss: 1.594981]\n",
      "epoch:31 step:29496 [D loss: 0.636636, acc.: 60.94%] [G loss: 1.235502]\n",
      "epoch:31 step:29497 [D loss: 0.680339, acc.: 64.06%] [G loss: 1.501903]\n",
      "epoch:31 step:29498 [D loss: 0.616475, acc.: 68.75%] [G loss: 1.175611]\n",
      "epoch:31 step:29499 [D loss: 0.468871, acc.: 78.12%] [G loss: 1.477429]\n",
      "epoch:31 step:29500 [D loss: 0.488189, acc.: 77.34%] [G loss: 1.138109]\n",
      "epoch:31 step:29501 [D loss: 0.683793, acc.: 64.06%] [G loss: 1.352664]\n",
      "epoch:31 step:29502 [D loss: 0.365779, acc.: 85.16%] [G loss: 1.870642]\n",
      "epoch:31 step:29503 [D loss: 0.641173, acc.: 60.16%] [G loss: 1.622945]\n",
      "epoch:31 step:29504 [D loss: 0.535871, acc.: 75.78%] [G loss: 1.628925]\n",
      "epoch:31 step:29505 [D loss: 0.496138, acc.: 77.34%] [G loss: 1.406644]\n",
      "epoch:31 step:29506 [D loss: 0.459346, acc.: 78.91%] [G loss: 1.582673]\n",
      "epoch:31 step:29507 [D loss: 0.464253, acc.: 81.25%] [G loss: 1.456509]\n",
      "epoch:31 step:29508 [D loss: 0.623398, acc.: 64.06%] [G loss: 1.629452]\n",
      "epoch:31 step:29509 [D loss: 0.562797, acc.: 66.41%] [G loss: 1.753305]\n",
      "epoch:31 step:29510 [D loss: 0.520176, acc.: 74.22%] [G loss: 1.737237]\n",
      "epoch:31 step:29511 [D loss: 0.448594, acc.: 77.34%] [G loss: 2.195320]\n",
      "epoch:31 step:29512 [D loss: 0.460156, acc.: 82.81%] [G loss: 1.503040]\n",
      "epoch:31 step:29513 [D loss: 0.315673, acc.: 91.41%] [G loss: 1.662974]\n",
      "epoch:31 step:29514 [D loss: 0.474923, acc.: 71.88%] [G loss: 1.755063]\n",
      "epoch:31 step:29515 [D loss: 0.582894, acc.: 67.97%] [G loss: 1.845726]\n",
      "epoch:31 step:29516 [D loss: 0.549271, acc.: 71.88%] [G loss: 1.622921]\n",
      "epoch:31 step:29517 [D loss: 0.746549, acc.: 60.94%] [G loss: 1.555394]\n",
      "epoch:31 step:29518 [D loss: 0.372589, acc.: 85.94%] [G loss: 1.378057]\n",
      "epoch:31 step:29519 [D loss: 0.487783, acc.: 77.34%] [G loss: 1.743214]\n",
      "epoch:31 step:29520 [D loss: 0.333323, acc.: 87.50%] [G loss: 1.354315]\n",
      "epoch:31 step:29521 [D loss: 0.473595, acc.: 75.78%] [G loss: 1.916336]\n",
      "epoch:31 step:29522 [D loss: 0.489987, acc.: 81.25%] [G loss: 1.377691]\n",
      "epoch:31 step:29523 [D loss: 0.440279, acc.: 78.91%] [G loss: 1.143956]\n",
      "epoch:31 step:29524 [D loss: 0.619337, acc.: 65.62%] [G loss: 1.645941]\n",
      "epoch:31 step:29525 [D loss: 0.679560, acc.: 65.62%] [G loss: 1.501650]\n",
      "epoch:31 step:29526 [D loss: 0.514263, acc.: 76.56%] [G loss: 1.521431]\n",
      "epoch:31 step:29527 [D loss: 0.542045, acc.: 77.34%] [G loss: 1.470626]\n",
      "epoch:31 step:29528 [D loss: 0.660393, acc.: 63.28%] [G loss: 1.640913]\n",
      "epoch:31 step:29529 [D loss: 0.469619, acc.: 79.69%] [G loss: 1.324445]\n",
      "epoch:31 step:29530 [D loss: 0.579118, acc.: 69.53%] [G loss: 1.517159]\n",
      "epoch:31 step:29531 [D loss: 0.457717, acc.: 78.12%] [G loss: 1.768058]\n",
      "epoch:31 step:29532 [D loss: 0.658736, acc.: 64.84%] [G loss: 1.662606]\n",
      "epoch:31 step:29533 [D loss: 0.547016, acc.: 71.88%] [G loss: 1.699373]\n",
      "epoch:31 step:29534 [D loss: 0.485993, acc.: 72.66%] [G loss: 1.950384]\n",
      "epoch:31 step:29535 [D loss: 0.325914, acc.: 89.84%] [G loss: 1.601784]\n",
      "epoch:31 step:29536 [D loss: 0.594835, acc.: 71.09%] [G loss: 1.771829]\n",
      "epoch:31 step:29537 [D loss: 0.419575, acc.: 79.69%] [G loss: 1.225105]\n",
      "epoch:31 step:29538 [D loss: 0.572020, acc.: 69.53%] [G loss: 1.262855]\n",
      "epoch:31 step:29539 [D loss: 0.585636, acc.: 68.75%] [G loss: 1.899527]\n",
      "epoch:31 step:29540 [D loss: 0.594687, acc.: 63.28%] [G loss: 1.422256]\n",
      "epoch:31 step:29541 [D loss: 0.377816, acc.: 84.38%] [G loss: 1.635081]\n",
      "epoch:31 step:29542 [D loss: 0.593084, acc.: 67.19%] [G loss: 1.375897]\n",
      "epoch:31 step:29543 [D loss: 0.662901, acc.: 64.06%] [G loss: 1.453476]\n",
      "epoch:31 step:29544 [D loss: 0.548666, acc.: 72.66%] [G loss: 1.486392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29545 [D loss: 0.602903, acc.: 64.06%] [G loss: 1.709100]\n",
      "epoch:31 step:29546 [D loss: 0.794114, acc.: 53.12%] [G loss: 1.824476]\n",
      "epoch:31 step:29547 [D loss: 0.407611, acc.: 85.94%] [G loss: 1.915063]\n",
      "epoch:31 step:29548 [D loss: 0.469187, acc.: 75.00%] [G loss: 1.725437]\n",
      "epoch:31 step:29549 [D loss: 0.475581, acc.: 78.12%] [G loss: 1.447213]\n",
      "epoch:31 step:29550 [D loss: 0.436262, acc.: 85.16%] [G loss: 1.579247]\n",
      "epoch:31 step:29551 [D loss: 0.506661, acc.: 79.69%] [G loss: 1.126268]\n",
      "epoch:31 step:29552 [D loss: 0.379370, acc.: 86.72%] [G loss: 1.885523]\n",
      "epoch:31 step:29553 [D loss: 0.629766, acc.: 64.06%] [G loss: 1.365690]\n",
      "epoch:31 step:29554 [D loss: 0.490144, acc.: 80.47%] [G loss: 1.448329]\n",
      "epoch:31 step:29555 [D loss: 0.429010, acc.: 80.47%] [G loss: 1.143333]\n",
      "epoch:31 step:29556 [D loss: 0.559865, acc.: 71.09%] [G loss: 1.265705]\n",
      "epoch:31 step:29557 [D loss: 0.440361, acc.: 80.47%] [G loss: 1.221913]\n",
      "epoch:31 step:29558 [D loss: 0.361446, acc.: 89.06%] [G loss: 1.604834]\n",
      "epoch:31 step:29559 [D loss: 0.491432, acc.: 75.00%] [G loss: 1.374614]\n",
      "epoch:31 step:29560 [D loss: 0.517044, acc.: 76.56%] [G loss: 1.630722]\n",
      "epoch:31 step:29561 [D loss: 0.553777, acc.: 72.66%] [G loss: 2.221101]\n",
      "epoch:31 step:29562 [D loss: 0.708291, acc.: 64.06%] [G loss: 1.828939]\n",
      "epoch:31 step:29563 [D loss: 0.502094, acc.: 73.44%] [G loss: 1.699570]\n",
      "epoch:31 step:29564 [D loss: 0.528949, acc.: 71.09%] [G loss: 1.417515]\n",
      "epoch:31 step:29565 [D loss: 0.368949, acc.: 83.59%] [G loss: 2.053336]\n",
      "epoch:31 step:29566 [D loss: 0.714244, acc.: 56.25%] [G loss: 1.268594]\n",
      "epoch:31 step:29567 [D loss: 0.626275, acc.: 67.19%] [G loss: 1.387589]\n",
      "epoch:31 step:29568 [D loss: 0.531098, acc.: 74.22%] [G loss: 1.523964]\n",
      "epoch:31 step:29569 [D loss: 0.636769, acc.: 68.75%] [G loss: 1.208272]\n",
      "epoch:31 step:29570 [D loss: 0.536088, acc.: 73.44%] [G loss: 1.192977]\n",
      "epoch:31 step:29571 [D loss: 0.503718, acc.: 75.78%] [G loss: 1.354370]\n",
      "epoch:31 step:29572 [D loss: 0.460814, acc.: 78.12%] [G loss: 1.470095]\n",
      "epoch:31 step:29573 [D loss: 0.541108, acc.: 74.22%] [G loss: 1.418097]\n",
      "epoch:31 step:29574 [D loss: 0.593574, acc.: 66.41%] [G loss: 1.497637]\n",
      "epoch:31 step:29575 [D loss: 0.383207, acc.: 83.59%] [G loss: 1.688427]\n",
      "epoch:31 step:29576 [D loss: 0.550852, acc.: 74.22%] [G loss: 1.529866]\n",
      "epoch:31 step:29577 [D loss: 0.593357, acc.: 67.97%] [G loss: 1.394321]\n",
      "epoch:31 step:29578 [D loss: 0.373802, acc.: 87.50%] [G loss: 1.233050]\n",
      "epoch:31 step:29579 [D loss: 0.584647, acc.: 66.41%] [G loss: 1.223537]\n",
      "epoch:31 step:29580 [D loss: 0.694781, acc.: 64.06%] [G loss: 1.600913]\n",
      "epoch:31 step:29581 [D loss: 0.601049, acc.: 65.62%] [G loss: 1.321154]\n",
      "epoch:31 step:29582 [D loss: 0.454413, acc.: 78.91%] [G loss: 1.360203]\n",
      "epoch:31 step:29583 [D loss: 0.610327, acc.: 69.53%] [G loss: 1.333824]\n",
      "epoch:31 step:29584 [D loss: 0.682968, acc.: 63.28%] [G loss: 1.069555]\n",
      "epoch:31 step:29585 [D loss: 0.560420, acc.: 69.53%] [G loss: 1.654299]\n",
      "epoch:31 step:29586 [D loss: 0.698035, acc.: 61.72%] [G loss: 0.901082]\n",
      "epoch:31 step:29587 [D loss: 0.468043, acc.: 84.38%] [G loss: 1.121275]\n",
      "epoch:31 step:29588 [D loss: 0.494644, acc.: 74.22%] [G loss: 1.382684]\n",
      "epoch:31 step:29589 [D loss: 0.446589, acc.: 82.81%] [G loss: 1.094422]\n",
      "epoch:31 step:29590 [D loss: 0.446262, acc.: 82.03%] [G loss: 1.061313]\n",
      "epoch:31 step:29591 [D loss: 0.368821, acc.: 84.38%] [G loss: 1.929007]\n",
      "epoch:31 step:29592 [D loss: 0.458595, acc.: 79.69%] [G loss: 1.546277]\n",
      "epoch:31 step:29593 [D loss: 0.514291, acc.: 73.44%] [G loss: 1.638891]\n",
      "epoch:31 step:29594 [D loss: 0.468152, acc.: 78.12%] [G loss: 1.165973]\n",
      "epoch:31 step:29595 [D loss: 0.819617, acc.: 45.31%] [G loss: 1.452273]\n",
      "epoch:31 step:29596 [D loss: 0.609651, acc.: 63.28%] [G loss: 1.886181]\n",
      "epoch:31 step:29597 [D loss: 0.412543, acc.: 82.03%] [G loss: 1.403803]\n",
      "epoch:31 step:29598 [D loss: 0.503503, acc.: 79.69%] [G loss: 1.670905]\n",
      "epoch:31 step:29599 [D loss: 0.421162, acc.: 80.47%] [G loss: 1.610489]\n",
      "epoch:31 step:29600 [D loss: 0.478490, acc.: 79.69%] [G loss: 1.430031]\n",
      "##############\n",
      "[2.83548557 2.03240552 1.94989321 2.60817994 0.98135363 5.72241613\n",
      " 2.44978053 2.19922325 3.92506791 7.14730822]\n",
      "##########\n",
      "epoch:31 step:29601 [D loss: 0.714704, acc.: 53.91%] [G loss: 1.455127]\n",
      "epoch:31 step:29602 [D loss: 0.607285, acc.: 70.31%] [G loss: 1.282325]\n",
      "epoch:31 step:29603 [D loss: 0.428360, acc.: 82.03%] [G loss: 1.565246]\n",
      "epoch:31 step:29604 [D loss: 0.588465, acc.: 67.97%] [G loss: 1.027374]\n",
      "epoch:31 step:29605 [D loss: 0.543786, acc.: 73.44%] [G loss: 1.311294]\n",
      "epoch:31 step:29606 [D loss: 0.526164, acc.: 77.34%] [G loss: 0.972334]\n",
      "epoch:31 step:29607 [D loss: 0.659960, acc.: 66.41%] [G loss: 1.667597]\n",
      "epoch:31 step:29608 [D loss: 0.556494, acc.: 71.09%] [G loss: 1.343544]\n",
      "epoch:31 step:29609 [D loss: 0.590845, acc.: 73.44%] [G loss: 1.548913]\n",
      "epoch:31 step:29610 [D loss: 0.452080, acc.: 78.91%] [G loss: 1.236551]\n",
      "epoch:31 step:29611 [D loss: 0.365326, acc.: 85.94%] [G loss: 1.892486]\n",
      "epoch:31 step:29612 [D loss: 0.432350, acc.: 82.03%] [G loss: 1.387863]\n",
      "epoch:31 step:29613 [D loss: 0.434957, acc.: 85.16%] [G loss: 1.547248]\n",
      "epoch:31 step:29614 [D loss: 0.480318, acc.: 81.25%] [G loss: 1.497679]\n",
      "epoch:31 step:29615 [D loss: 0.820404, acc.: 48.44%] [G loss: 1.674254]\n",
      "epoch:31 step:29616 [D loss: 0.564158, acc.: 72.66%] [G loss: 1.505322]\n",
      "epoch:31 step:29617 [D loss: 0.590955, acc.: 69.53%] [G loss: 1.666725]\n",
      "epoch:31 step:29618 [D loss: 0.502032, acc.: 75.00%] [G loss: 1.804310]\n",
      "epoch:31 step:29619 [D loss: 0.486926, acc.: 78.91%] [G loss: 1.527834]\n",
      "epoch:31 step:29620 [D loss: 0.463704, acc.: 80.47%] [G loss: 2.414029]\n",
      "epoch:31 step:29621 [D loss: 0.702842, acc.: 62.50%] [G loss: 1.700922]\n",
      "epoch:31 step:29622 [D loss: 0.394806, acc.: 83.59%] [G loss: 1.320243]\n",
      "epoch:31 step:29623 [D loss: 0.462993, acc.: 82.03%] [G loss: 1.590684]\n",
      "epoch:31 step:29624 [D loss: 0.757210, acc.: 51.56%] [G loss: 1.685422]\n",
      "epoch:31 step:29625 [D loss: 0.436271, acc.: 80.47%] [G loss: 1.720521]\n",
      "epoch:31 step:29626 [D loss: 0.734878, acc.: 56.25%] [G loss: 1.259103]\n",
      "epoch:31 step:29627 [D loss: 0.663380, acc.: 64.84%] [G loss: 1.338719]\n",
      "epoch:31 step:29628 [D loss: 0.484249, acc.: 78.91%] [G loss: 0.961132]\n",
      "epoch:31 step:29629 [D loss: 0.570148, acc.: 71.88%] [G loss: 1.476666]\n",
      "epoch:31 step:29630 [D loss: 0.643003, acc.: 68.75%] [G loss: 1.311114]\n",
      "epoch:31 step:29631 [D loss: 0.406730, acc.: 82.81%] [G loss: 1.398372]\n",
      "epoch:31 step:29632 [D loss: 0.683399, acc.: 64.84%] [G loss: 1.344384]\n",
      "epoch:31 step:29633 [D loss: 0.432396, acc.: 80.47%] [G loss: 1.897994]\n",
      "epoch:31 step:29634 [D loss: 0.503570, acc.: 74.22%] [G loss: 1.637650]\n",
      "epoch:31 step:29635 [D loss: 0.732402, acc.: 57.81%] [G loss: 1.222594]\n",
      "epoch:31 step:29636 [D loss: 0.468555, acc.: 80.47%] [G loss: 1.621221]\n",
      "epoch:31 step:29637 [D loss: 0.433484, acc.: 79.69%] [G loss: 1.639788]\n",
      "epoch:31 step:29638 [D loss: 0.732346, acc.: 60.16%] [G loss: 1.293973]\n",
      "epoch:31 step:29639 [D loss: 0.425588, acc.: 83.59%] [G loss: 1.577817]\n",
      "epoch:31 step:29640 [D loss: 0.700889, acc.: 55.47%] [G loss: 1.387372]\n",
      "epoch:31 step:29641 [D loss: 0.588933, acc.: 68.75%] [G loss: 1.137508]\n",
      "epoch:31 step:29642 [D loss: 0.646346, acc.: 66.41%] [G loss: 1.431155]\n",
      "epoch:31 step:29643 [D loss: 0.604404, acc.: 66.41%] [G loss: 1.507440]\n",
      "epoch:31 step:29644 [D loss: 0.381363, acc.: 85.16%] [G loss: 1.881314]\n",
      "epoch:31 step:29645 [D loss: 0.696101, acc.: 60.94%] [G loss: 2.038815]\n",
      "epoch:31 step:29646 [D loss: 0.623099, acc.: 64.84%] [G loss: 1.030762]\n",
      "epoch:31 step:29647 [D loss: 0.445973, acc.: 80.47%] [G loss: 2.073586]\n",
      "epoch:31 step:29648 [D loss: 0.934169, acc.: 46.88%] [G loss: 1.122973]\n",
      "epoch:31 step:29649 [D loss: 0.542822, acc.: 72.66%] [G loss: 1.370903]\n",
      "epoch:31 step:29650 [D loss: 0.498329, acc.: 71.88%] [G loss: 1.629658]\n",
      "epoch:31 step:29651 [D loss: 0.574746, acc.: 71.88%] [G loss: 0.961182]\n",
      "epoch:31 step:29652 [D loss: 0.601859, acc.: 67.97%] [G loss: 1.350600]\n",
      "epoch:31 step:29653 [D loss: 0.674221, acc.: 64.84%] [G loss: 1.030804]\n",
      "epoch:31 step:29654 [D loss: 0.464320, acc.: 80.47%] [G loss: 1.576362]\n",
      "epoch:31 step:29655 [D loss: 0.466878, acc.: 81.25%] [G loss: 1.376070]\n",
      "epoch:31 step:29656 [D loss: 0.392259, acc.: 84.38%] [G loss: 1.498982]\n",
      "epoch:31 step:29657 [D loss: 0.590493, acc.: 72.66%] [G loss: 1.866679]\n",
      "epoch:31 step:29658 [D loss: 0.502576, acc.: 73.44%] [G loss: 1.595160]\n",
      "epoch:31 step:29659 [D loss: 0.624485, acc.: 64.84%] [G loss: 1.708757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29660 [D loss: 0.544756, acc.: 78.12%] [G loss: 1.633036]\n",
      "epoch:31 step:29661 [D loss: 0.695853, acc.: 63.28%] [G loss: 1.550319]\n",
      "epoch:31 step:29662 [D loss: 0.594726, acc.: 74.22%] [G loss: 1.548840]\n",
      "epoch:31 step:29663 [D loss: 0.483203, acc.: 78.12%] [G loss: 1.860213]\n",
      "epoch:31 step:29664 [D loss: 0.557177, acc.: 74.22%] [G loss: 1.703678]\n",
      "epoch:31 step:29665 [D loss: 0.546328, acc.: 72.66%] [G loss: 1.244831]\n",
      "epoch:31 step:29666 [D loss: 0.531054, acc.: 71.88%] [G loss: 1.412383]\n",
      "epoch:31 step:29667 [D loss: 0.451341, acc.: 82.03%] [G loss: 1.522334]\n",
      "epoch:31 step:29668 [D loss: 0.637191, acc.: 61.72%] [G loss: 1.149630]\n",
      "epoch:31 step:29669 [D loss: 0.801456, acc.: 51.56%] [G loss: 1.530615]\n",
      "epoch:31 step:29670 [D loss: 0.519625, acc.: 75.78%] [G loss: 1.290661]\n",
      "epoch:31 step:29671 [D loss: 0.714662, acc.: 57.81%] [G loss: 1.753947]\n",
      "epoch:31 step:29672 [D loss: 0.410097, acc.: 82.81%] [G loss: 1.942926]\n",
      "epoch:31 step:29673 [D loss: 0.473177, acc.: 76.56%] [G loss: 2.001941]\n",
      "epoch:31 step:29674 [D loss: 0.432933, acc.: 80.47%] [G loss: 2.302078]\n",
      "epoch:31 step:29675 [D loss: 0.674088, acc.: 60.16%] [G loss: 1.352208]\n",
      "epoch:31 step:29676 [D loss: 0.460916, acc.: 78.91%] [G loss: 1.154344]\n",
      "epoch:31 step:29677 [D loss: 0.676626, acc.: 59.38%] [G loss: 0.955176]\n",
      "epoch:31 step:29678 [D loss: 0.497972, acc.: 75.78%] [G loss: 2.090839]\n",
      "epoch:31 step:29679 [D loss: 0.454704, acc.: 81.25%] [G loss: 1.398906]\n",
      "epoch:31 step:29680 [D loss: 0.569951, acc.: 68.75%] [G loss: 1.739902]\n",
      "epoch:31 step:29681 [D loss: 0.416205, acc.: 84.38%] [G loss: 1.743166]\n",
      "epoch:31 step:29682 [D loss: 0.809456, acc.: 50.78%] [G loss: 1.251420]\n",
      "epoch:31 step:29683 [D loss: 0.741589, acc.: 57.03%] [G loss: 1.414275]\n",
      "epoch:31 step:29684 [D loss: 0.503818, acc.: 78.12%] [G loss: 1.724493]\n",
      "epoch:31 step:29685 [D loss: 0.528329, acc.: 71.88%] [G loss: 1.404161]\n",
      "epoch:31 step:29686 [D loss: 0.528070, acc.: 70.31%] [G loss: 1.082345]\n",
      "epoch:31 step:29687 [D loss: 0.474382, acc.: 77.34%] [G loss: 1.543920]\n",
      "epoch:31 step:29688 [D loss: 0.577390, acc.: 69.53%] [G loss: 1.625263]\n",
      "epoch:31 step:29689 [D loss: 0.514712, acc.: 73.44%] [G loss: 1.185623]\n",
      "epoch:31 step:29690 [D loss: 0.671495, acc.: 60.94%] [G loss: 0.875543]\n",
      "epoch:31 step:29691 [D loss: 0.736822, acc.: 55.47%] [G loss: 1.373725]\n",
      "epoch:31 step:29692 [D loss: 0.361017, acc.: 87.50%] [G loss: 1.622567]\n",
      "epoch:31 step:29693 [D loss: 0.453300, acc.: 80.47%] [G loss: 1.739527]\n",
      "epoch:31 step:29694 [D loss: 0.640390, acc.: 61.72%] [G loss: 1.404207]\n",
      "epoch:31 step:29695 [D loss: 0.462619, acc.: 80.47%] [G loss: 1.347769]\n",
      "epoch:31 step:29696 [D loss: 0.452492, acc.: 78.91%] [G loss: 1.779214]\n",
      "epoch:31 step:29697 [D loss: 0.624670, acc.: 60.16%] [G loss: 1.394760]\n",
      "epoch:31 step:29698 [D loss: 0.291058, acc.: 91.41%] [G loss: 1.769118]\n",
      "epoch:31 step:29699 [D loss: 0.538805, acc.: 72.66%] [G loss: 1.153968]\n",
      "epoch:31 step:29700 [D loss: 0.787804, acc.: 54.69%] [G loss: 1.158289]\n",
      "epoch:31 step:29701 [D loss: 0.544661, acc.: 74.22%] [G loss: 1.448014]\n",
      "epoch:31 step:29702 [D loss: 0.727621, acc.: 56.25%] [G loss: 1.098278]\n",
      "epoch:31 step:29703 [D loss: 0.591327, acc.: 68.75%] [G loss: 1.064405]\n",
      "epoch:31 step:29704 [D loss: 0.582209, acc.: 68.75%] [G loss: 1.738169]\n",
      "epoch:31 step:29705 [D loss: 0.566123, acc.: 72.66%] [G loss: 1.096196]\n",
      "epoch:31 step:29706 [D loss: 0.587800, acc.: 67.19%] [G loss: 1.502030]\n",
      "epoch:31 step:29707 [D loss: 0.457322, acc.: 82.81%] [G loss: 1.438324]\n",
      "epoch:31 step:29708 [D loss: 0.705262, acc.: 63.28%] [G loss: 1.524276]\n",
      "epoch:31 step:29709 [D loss: 0.560324, acc.: 67.97%] [G loss: 1.401210]\n",
      "epoch:31 step:29710 [D loss: 0.694299, acc.: 57.03%] [G loss: 1.441120]\n",
      "epoch:31 step:29711 [D loss: 0.640635, acc.: 63.28%] [G loss: 1.031835]\n",
      "epoch:31 step:29712 [D loss: 0.570378, acc.: 72.66%] [G loss: 1.652044]\n",
      "epoch:31 step:29713 [D loss: 0.336684, acc.: 90.62%] [G loss: 1.351671]\n",
      "epoch:31 step:29714 [D loss: 0.465881, acc.: 78.91%] [G loss: 1.395102]\n",
      "epoch:31 step:29715 [D loss: 0.519039, acc.: 75.78%] [G loss: 1.458517]\n",
      "epoch:31 step:29716 [D loss: 0.510175, acc.: 78.12%] [G loss: 1.419548]\n",
      "epoch:31 step:29717 [D loss: 0.502563, acc.: 75.00%] [G loss: 1.468643]\n",
      "epoch:31 step:29718 [D loss: 0.640741, acc.: 67.19%] [G loss: 1.489699]\n",
      "epoch:31 step:29719 [D loss: 0.545383, acc.: 76.56%] [G loss: 1.741652]\n",
      "epoch:31 step:29720 [D loss: 0.724965, acc.: 57.81%] [G loss: 1.015713]\n",
      "epoch:31 step:29721 [D loss: 0.376906, acc.: 84.38%] [G loss: 2.124538]\n",
      "epoch:31 step:29722 [D loss: 0.644079, acc.: 60.16%] [G loss: 1.765184]\n",
      "epoch:31 step:29723 [D loss: 0.433207, acc.: 82.03%] [G loss: 1.785953]\n",
      "epoch:31 step:29724 [D loss: 0.498577, acc.: 73.44%] [G loss: 1.051065]\n",
      "epoch:31 step:29725 [D loss: 0.558999, acc.: 72.66%] [G loss: 1.560647]\n",
      "epoch:31 step:29726 [D loss: 0.461887, acc.: 77.34%] [G loss: 1.486835]\n",
      "epoch:31 step:29727 [D loss: 0.759051, acc.: 54.69%] [G loss: 1.264807]\n",
      "epoch:31 step:29728 [D loss: 0.464806, acc.: 81.25%] [G loss: 1.325831]\n",
      "epoch:31 step:29729 [D loss: 0.454682, acc.: 82.03%] [G loss: 1.591406]\n",
      "epoch:31 step:29730 [D loss: 0.520736, acc.: 75.00%] [G loss: 1.797061]\n",
      "epoch:31 step:29731 [D loss: 0.618822, acc.: 63.28%] [G loss: 1.508579]\n",
      "epoch:31 step:29732 [D loss: 0.440816, acc.: 82.03%] [G loss: 1.706123]\n",
      "epoch:31 step:29733 [D loss: 0.579752, acc.: 68.75%] [G loss: 1.423511]\n",
      "epoch:31 step:29734 [D loss: 0.424964, acc.: 85.16%] [G loss: 1.537506]\n",
      "epoch:31 step:29735 [D loss: 0.534295, acc.: 78.12%] [G loss: 1.854496]\n",
      "epoch:31 step:29736 [D loss: 0.429284, acc.: 81.25%] [G loss: 1.569383]\n",
      "epoch:31 step:29737 [D loss: 0.436111, acc.: 77.34%] [G loss: 1.549221]\n",
      "epoch:31 step:29738 [D loss: 0.486905, acc.: 75.78%] [G loss: 1.525419]\n",
      "epoch:31 step:29739 [D loss: 0.526629, acc.: 73.44%] [G loss: 1.046462]\n",
      "epoch:31 step:29740 [D loss: 0.394493, acc.: 86.72%] [G loss: 1.196800]\n",
      "epoch:31 step:29741 [D loss: 0.543976, acc.: 74.22%] [G loss: 1.460784]\n",
      "epoch:31 step:29742 [D loss: 0.562455, acc.: 69.53%] [G loss: 1.664429]\n",
      "epoch:31 step:29743 [D loss: 0.564853, acc.: 69.53%] [G loss: 1.525598]\n",
      "epoch:31 step:29744 [D loss: 0.368642, acc.: 83.59%] [G loss: 1.599111]\n",
      "epoch:31 step:29745 [D loss: 0.542144, acc.: 72.66%] [G loss: 1.101217]\n",
      "epoch:31 step:29746 [D loss: 0.488207, acc.: 78.12%] [G loss: 1.688720]\n",
      "epoch:31 step:29747 [D loss: 0.426037, acc.: 83.59%] [G loss: 1.625854]\n",
      "epoch:31 step:29748 [D loss: 0.461412, acc.: 78.91%] [G loss: 1.612401]\n",
      "epoch:31 step:29749 [D loss: 0.384287, acc.: 84.38%] [G loss: 1.842430]\n",
      "epoch:31 step:29750 [D loss: 0.601082, acc.: 69.53%] [G loss: 1.240984]\n",
      "epoch:31 step:29751 [D loss: 0.614851, acc.: 69.53%] [G loss: 1.515624]\n",
      "epoch:31 step:29752 [D loss: 0.550985, acc.: 77.34%] [G loss: 1.490152]\n",
      "epoch:31 step:29753 [D loss: 0.622197, acc.: 64.84%] [G loss: 1.072998]\n",
      "epoch:31 step:29754 [D loss: 0.410544, acc.: 84.38%] [G loss: 1.713817]\n",
      "epoch:31 step:29755 [D loss: 0.502797, acc.: 75.78%] [G loss: 1.473518]\n",
      "epoch:31 step:29756 [D loss: 0.576430, acc.: 68.75%] [G loss: 1.221218]\n",
      "epoch:31 step:29757 [D loss: 0.583974, acc.: 68.75%] [G loss: 1.549554]\n",
      "epoch:31 step:29758 [D loss: 0.450803, acc.: 81.25%] [G loss: 1.786545]\n",
      "epoch:31 step:29759 [D loss: 0.521884, acc.: 73.44%] [G loss: 1.733137]\n",
      "epoch:31 step:29760 [D loss: 0.624383, acc.: 66.41%] [G loss: 1.258629]\n",
      "epoch:31 step:29761 [D loss: 0.736340, acc.: 63.28%] [G loss: 1.170610]\n",
      "epoch:31 step:29762 [D loss: 0.647967, acc.: 62.50%] [G loss: 1.530112]\n",
      "epoch:31 step:29763 [D loss: 0.335470, acc.: 89.06%] [G loss: 1.753244]\n",
      "epoch:31 step:29764 [D loss: 0.512938, acc.: 71.88%] [G loss: 1.151628]\n",
      "epoch:31 step:29765 [D loss: 0.257813, acc.: 92.19%] [G loss: 1.818920]\n",
      "epoch:31 step:29766 [D loss: 0.495845, acc.: 77.34%] [G loss: 1.677120]\n",
      "epoch:31 step:29767 [D loss: 0.547489, acc.: 76.56%] [G loss: 1.848129]\n",
      "epoch:31 step:29768 [D loss: 0.359705, acc.: 86.72%] [G loss: 1.670250]\n",
      "epoch:31 step:29769 [D loss: 0.545998, acc.: 72.66%] [G loss: 1.540269]\n",
      "epoch:31 step:29770 [D loss: 0.527482, acc.: 73.44%] [G loss: 1.866652]\n",
      "epoch:31 step:29771 [D loss: 0.452100, acc.: 79.69%] [G loss: 1.658639]\n",
      "epoch:31 step:29772 [D loss: 0.394325, acc.: 85.94%] [G loss: 1.657614]\n",
      "epoch:31 step:29773 [D loss: 0.686946, acc.: 64.06%] [G loss: 1.729099]\n",
      "epoch:31 step:29774 [D loss: 0.556337, acc.: 73.44%] [G loss: 1.604552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29775 [D loss: 0.370558, acc.: 87.50%] [G loss: 1.485506]\n",
      "epoch:31 step:29776 [D loss: 0.595104, acc.: 64.84%] [G loss: 1.461694]\n",
      "epoch:31 step:29777 [D loss: 0.509022, acc.: 73.44%] [G loss: 1.367144]\n",
      "epoch:31 step:29778 [D loss: 0.602740, acc.: 67.19%] [G loss: 1.011603]\n",
      "epoch:31 step:29779 [D loss: 0.550023, acc.: 71.88%] [G loss: 1.378266]\n",
      "epoch:31 step:29780 [D loss: 0.413355, acc.: 82.03%] [G loss: 1.534277]\n",
      "epoch:31 step:29781 [D loss: 0.557012, acc.: 74.22%] [G loss: 1.588821]\n",
      "epoch:31 step:29782 [D loss: 0.467382, acc.: 78.91%] [G loss: 1.572167]\n",
      "epoch:31 step:29783 [D loss: 0.513482, acc.: 77.34%] [G loss: 1.926468]\n",
      "epoch:31 step:29784 [D loss: 0.497026, acc.: 78.12%] [G loss: 1.318174]\n",
      "epoch:31 step:29785 [D loss: 0.686438, acc.: 60.16%] [G loss: 1.437387]\n",
      "epoch:31 step:29786 [D loss: 0.456990, acc.: 77.34%] [G loss: 1.733130]\n",
      "epoch:31 step:29787 [D loss: 0.420543, acc.: 86.72%] [G loss: 1.349465]\n",
      "epoch:31 step:29788 [D loss: 0.684565, acc.: 60.94%] [G loss: 1.393732]\n",
      "epoch:31 step:29789 [D loss: 0.503118, acc.: 76.56%] [G loss: 1.464473]\n",
      "epoch:31 step:29790 [D loss: 0.679189, acc.: 60.94%] [G loss: 1.469796]\n",
      "epoch:31 step:29791 [D loss: 0.432791, acc.: 78.91%] [G loss: 1.840539]\n",
      "epoch:31 step:29792 [D loss: 0.578169, acc.: 66.41%] [G loss: 1.739532]\n",
      "epoch:31 step:29793 [D loss: 0.530855, acc.: 70.31%] [G loss: 1.647893]\n",
      "epoch:31 step:29794 [D loss: 0.525672, acc.: 76.56%] [G loss: 1.224373]\n",
      "epoch:31 step:29795 [D loss: 0.593208, acc.: 67.97%] [G loss: 1.586625]\n",
      "epoch:31 step:29796 [D loss: 0.629610, acc.: 65.62%] [G loss: 1.745368]\n",
      "epoch:31 step:29797 [D loss: 0.492494, acc.: 79.69%] [G loss: 1.736256]\n",
      "epoch:31 step:29798 [D loss: 0.744880, acc.: 57.03%] [G loss: 1.674593]\n",
      "epoch:31 step:29799 [D loss: 0.414744, acc.: 82.81%] [G loss: 1.809192]\n",
      "epoch:31 step:29800 [D loss: 0.384406, acc.: 87.50%] [G loss: 2.027679]\n",
      "##############\n",
      "[2.60944253 1.92839095 1.696887   3.00190381 0.69784242 5.763036\n",
      " 2.30337051 2.75036422 3.88367365 8.14868929]\n",
      "##########\n",
      "epoch:31 step:29801 [D loss: 0.585656, acc.: 71.09%] [G loss: 1.096296]\n",
      "epoch:31 step:29802 [D loss: 0.534447, acc.: 75.78%] [G loss: 1.413719]\n",
      "epoch:31 step:29803 [D loss: 0.558742, acc.: 73.44%] [G loss: 1.728204]\n",
      "epoch:31 step:29804 [D loss: 0.678200, acc.: 60.16%] [G loss: 1.519107]\n",
      "epoch:31 step:29805 [D loss: 0.636493, acc.: 62.50%] [G loss: 1.638437]\n",
      "epoch:31 step:29806 [D loss: 0.374762, acc.: 85.94%] [G loss: 1.465306]\n",
      "epoch:31 step:29807 [D loss: 0.686913, acc.: 62.50%] [G loss: 1.421037]\n",
      "epoch:31 step:29808 [D loss: 0.703134, acc.: 54.69%] [G loss: 1.533009]\n",
      "epoch:31 step:29809 [D loss: 0.528072, acc.: 76.56%] [G loss: 1.227489]\n",
      "epoch:31 step:29810 [D loss: 0.702629, acc.: 63.28%] [G loss: 1.101921]\n",
      "epoch:31 step:29811 [D loss: 0.556064, acc.: 71.09%] [G loss: 1.698470]\n",
      "epoch:31 step:29812 [D loss: 0.537435, acc.: 74.22%] [G loss: 1.526417]\n",
      "epoch:31 step:29813 [D loss: 0.440602, acc.: 85.16%] [G loss: 1.239032]\n",
      "epoch:31 step:29814 [D loss: 0.315757, acc.: 91.41%] [G loss: 1.447876]\n",
      "epoch:31 step:29815 [D loss: 0.530978, acc.: 75.00%] [G loss: 1.560999]\n",
      "epoch:31 step:29816 [D loss: 0.568446, acc.: 71.09%] [G loss: 1.191473]\n",
      "epoch:31 step:29817 [D loss: 0.607867, acc.: 62.50%] [G loss: 1.168431]\n",
      "epoch:31 step:29818 [D loss: 0.694922, acc.: 62.50%] [G loss: 1.105509]\n",
      "epoch:31 step:29819 [D loss: 0.366287, acc.: 89.06%] [G loss: 1.761546]\n",
      "epoch:31 step:29820 [D loss: 0.593569, acc.: 70.31%] [G loss: 1.569934]\n",
      "epoch:31 step:29821 [D loss: 0.625137, acc.: 66.41%] [G loss: 1.437919]\n",
      "epoch:31 step:29822 [D loss: 0.770291, acc.: 54.69%] [G loss: 1.676186]\n",
      "epoch:31 step:29823 [D loss: 0.437929, acc.: 84.38%] [G loss: 1.668607]\n",
      "epoch:31 step:29824 [D loss: 0.626832, acc.: 65.62%] [G loss: 1.628386]\n",
      "epoch:31 step:29825 [D loss: 0.545836, acc.: 74.22%] [G loss: 1.794744]\n",
      "epoch:31 step:29826 [D loss: 0.637408, acc.: 58.59%] [G loss: 1.736414]\n",
      "epoch:31 step:29827 [D loss: 0.670460, acc.: 59.38%] [G loss: 1.888360]\n",
      "epoch:31 step:29828 [D loss: 0.432174, acc.: 81.25%] [G loss: 1.365234]\n",
      "epoch:31 step:29829 [D loss: 0.563403, acc.: 71.09%] [G loss: 1.559637]\n",
      "epoch:31 step:29830 [D loss: 0.442489, acc.: 80.47%] [G loss: 1.658167]\n",
      "epoch:31 step:29831 [D loss: 0.507379, acc.: 75.00%] [G loss: 1.327044]\n",
      "epoch:31 step:29832 [D loss: 0.405853, acc.: 82.81%] [G loss: 1.426369]\n",
      "epoch:31 step:29833 [D loss: 0.539143, acc.: 72.66%] [G loss: 1.655414]\n",
      "epoch:31 step:29834 [D loss: 0.440279, acc.: 85.16%] [G loss: 1.972349]\n",
      "epoch:31 step:29835 [D loss: 0.566056, acc.: 69.53%] [G loss: 2.048708]\n",
      "epoch:31 step:29836 [D loss: 0.512055, acc.: 78.12%] [G loss: 1.217680]\n",
      "epoch:31 step:29837 [D loss: 0.499341, acc.: 75.00%] [G loss: 1.387913]\n",
      "epoch:31 step:29838 [D loss: 0.561969, acc.: 75.00%] [G loss: 1.395358]\n",
      "epoch:31 step:29839 [D loss: 0.451854, acc.: 78.91%] [G loss: 1.892009]\n",
      "epoch:31 step:29840 [D loss: 0.461834, acc.: 82.03%] [G loss: 1.868635]\n",
      "epoch:31 step:29841 [D loss: 0.584130, acc.: 70.31%] [G loss: 1.585192]\n",
      "epoch:31 step:29842 [D loss: 0.632965, acc.: 67.97%] [G loss: 1.839114]\n",
      "epoch:31 step:29843 [D loss: 0.518982, acc.: 72.66%] [G loss: 1.637424]\n",
      "epoch:31 step:29844 [D loss: 0.365766, acc.: 86.72%] [G loss: 1.270007]\n",
      "epoch:31 step:29845 [D loss: 0.669025, acc.: 64.06%] [G loss: 1.189243]\n",
      "epoch:31 step:29846 [D loss: 0.475546, acc.: 77.34%] [G loss: 1.429499]\n",
      "epoch:31 step:29847 [D loss: 0.574331, acc.: 70.31%] [G loss: 1.178653]\n",
      "epoch:31 step:29848 [D loss: 0.456998, acc.: 78.91%] [G loss: 1.490560]\n",
      "epoch:31 step:29849 [D loss: 0.648134, acc.: 65.62%] [G loss: 1.317556]\n",
      "epoch:31 step:29850 [D loss: 0.574173, acc.: 68.75%] [G loss: 1.290655]\n",
      "epoch:31 step:29851 [D loss: 0.353817, acc.: 86.72%] [G loss: 2.079876]\n",
      "epoch:31 step:29852 [D loss: 0.471342, acc.: 78.91%] [G loss: 1.893104]\n",
      "epoch:31 step:29853 [D loss: 0.577647, acc.: 74.22%] [G loss: 1.352570]\n",
      "epoch:31 step:29854 [D loss: 0.680443, acc.: 57.03%] [G loss: 1.042950]\n",
      "epoch:31 step:29855 [D loss: 0.510327, acc.: 75.00%] [G loss: 1.296600]\n",
      "epoch:31 step:29856 [D loss: 0.656689, acc.: 59.38%] [G loss: 0.915112]\n",
      "epoch:31 step:29857 [D loss: 0.494468, acc.: 75.78%] [G loss: 1.456112]\n",
      "epoch:31 step:29858 [D loss: 0.622233, acc.: 67.19%] [G loss: 1.271384]\n",
      "epoch:31 step:29859 [D loss: 0.531701, acc.: 76.56%] [G loss: 1.141958]\n",
      "epoch:31 step:29860 [D loss: 0.386705, acc.: 84.38%] [G loss: 1.478459]\n",
      "epoch:31 step:29861 [D loss: 0.544676, acc.: 72.66%] [G loss: 1.368073]\n",
      "epoch:31 step:29862 [D loss: 0.575797, acc.: 69.53%] [G loss: 1.186676]\n",
      "epoch:31 step:29863 [D loss: 0.727564, acc.: 55.47%] [G loss: 1.326206]\n",
      "epoch:31 step:29864 [D loss: 0.497481, acc.: 77.34%] [G loss: 1.558167]\n",
      "epoch:31 step:29865 [D loss: 0.498776, acc.: 77.34%] [G loss: 1.532432]\n",
      "epoch:31 step:29866 [D loss: 0.377009, acc.: 84.38%] [G loss: 1.378020]\n",
      "epoch:31 step:29867 [D loss: 0.532997, acc.: 71.09%] [G loss: 1.419239]\n",
      "epoch:31 step:29868 [D loss: 0.650597, acc.: 65.62%] [G loss: 1.231951]\n",
      "epoch:31 step:29869 [D loss: 0.524308, acc.: 71.88%] [G loss: 1.247316]\n",
      "epoch:31 step:29870 [D loss: 0.542040, acc.: 74.22%] [G loss: 1.323110]\n",
      "epoch:31 step:29871 [D loss: 0.534518, acc.: 71.88%] [G loss: 1.579381]\n",
      "epoch:31 step:29872 [D loss: 0.508683, acc.: 75.00%] [G loss: 1.563717]\n",
      "epoch:31 step:29873 [D loss: 0.622151, acc.: 70.31%] [G loss: 1.657219]\n",
      "epoch:31 step:29874 [D loss: 0.601239, acc.: 67.19%] [G loss: 1.588323]\n",
      "epoch:31 step:29875 [D loss: 0.692480, acc.: 63.28%] [G loss: 1.126161]\n",
      "epoch:31 step:29876 [D loss: 0.617926, acc.: 67.97%] [G loss: 1.305154]\n",
      "epoch:31 step:29877 [D loss: 0.506287, acc.: 75.00%] [G loss: 1.565199]\n",
      "epoch:31 step:29878 [D loss: 0.504517, acc.: 71.09%] [G loss: 1.796869]\n",
      "epoch:31 step:29879 [D loss: 0.598521, acc.: 68.75%] [G loss: 1.270341]\n",
      "epoch:31 step:29880 [D loss: 0.655960, acc.: 62.50%] [G loss: 1.432474]\n",
      "epoch:31 step:29881 [D loss: 0.743707, acc.: 59.38%] [G loss: 1.321782]\n",
      "epoch:31 step:29882 [D loss: 0.421650, acc.: 83.59%] [G loss: 1.637697]\n",
      "epoch:31 step:29883 [D loss: 0.637750, acc.: 64.06%] [G loss: 1.420410]\n",
      "epoch:31 step:29884 [D loss: 0.541804, acc.: 69.53%] [G loss: 1.341813]\n",
      "epoch:31 step:29885 [D loss: 0.398326, acc.: 83.59%] [G loss: 1.169594]\n",
      "epoch:31 step:29886 [D loss: 0.441619, acc.: 79.69%] [G loss: 1.556137]\n",
      "epoch:31 step:29887 [D loss: 0.332538, acc.: 91.41%] [G loss: 2.153451]\n",
      "epoch:31 step:29888 [D loss: 0.587718, acc.: 64.06%] [G loss: 1.734012]\n",
      "epoch:31 step:29889 [D loss: 0.474834, acc.: 81.25%] [G loss: 1.476610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29890 [D loss: 0.805584, acc.: 47.66%] [G loss: 1.057019]\n",
      "epoch:31 step:29891 [D loss: 0.563162, acc.: 69.53%] [G loss: 1.601436]\n",
      "epoch:31 step:29892 [D loss: 0.406782, acc.: 80.47%] [G loss: 1.035259]\n",
      "epoch:31 step:29893 [D loss: 0.454482, acc.: 80.47%] [G loss: 1.554433]\n",
      "epoch:31 step:29894 [D loss: 0.442180, acc.: 80.47%] [G loss: 1.318207]\n",
      "epoch:31 step:29895 [D loss: 0.639554, acc.: 62.50%] [G loss: 1.620142]\n",
      "epoch:31 step:29896 [D loss: 0.436818, acc.: 78.91%] [G loss: 1.465646]\n",
      "epoch:31 step:29897 [D loss: 0.489584, acc.: 75.00%] [G loss: 1.224392]\n",
      "epoch:31 step:29898 [D loss: 0.515554, acc.: 75.00%] [G loss: 1.328544]\n",
      "epoch:31 step:29899 [D loss: 0.561387, acc.: 73.44%] [G loss: 1.334080]\n",
      "epoch:31 step:29900 [D loss: 0.619653, acc.: 69.53%] [G loss: 1.332179]\n",
      "epoch:31 step:29901 [D loss: 0.502981, acc.: 75.00%] [G loss: 1.251652]\n",
      "epoch:31 step:29902 [D loss: 0.608675, acc.: 69.53%] [G loss: 0.938486]\n",
      "epoch:31 step:29903 [D loss: 0.424459, acc.: 82.81%] [G loss: 1.515307]\n",
      "epoch:31 step:29904 [D loss: 0.430201, acc.: 84.38%] [G loss: 1.530214]\n",
      "epoch:31 step:29905 [D loss: 0.291821, acc.: 91.41%] [G loss: 2.000113]\n",
      "epoch:31 step:29906 [D loss: 0.752647, acc.: 53.12%] [G loss: 1.641369]\n",
      "epoch:31 step:29907 [D loss: 0.583137, acc.: 67.97%] [G loss: 1.495771]\n",
      "epoch:31 step:29908 [D loss: 0.444498, acc.: 78.12%] [G loss: 1.687964]\n",
      "epoch:31 step:29909 [D loss: 0.669017, acc.: 63.28%] [G loss: 1.361951]\n",
      "epoch:31 step:29910 [D loss: 0.411825, acc.: 85.94%] [G loss: 1.495209]\n",
      "epoch:31 step:29911 [D loss: 0.463208, acc.: 79.69%] [G loss: 1.779096]\n",
      "epoch:31 step:29912 [D loss: 0.628976, acc.: 61.72%] [G loss: 1.260852]\n",
      "epoch:31 step:29913 [D loss: 0.516627, acc.: 74.22%] [G loss: 1.586171]\n",
      "epoch:31 step:29914 [D loss: 0.578001, acc.: 72.66%] [G loss: 1.225363]\n",
      "epoch:31 step:29915 [D loss: 0.407368, acc.: 82.81%] [G loss: 2.087940]\n",
      "epoch:31 step:29916 [D loss: 0.397613, acc.: 82.81%] [G loss: 1.235575]\n",
      "epoch:31 step:29917 [D loss: 0.742783, acc.: 60.16%] [G loss: 1.509717]\n",
      "epoch:31 step:29918 [D loss: 0.498416, acc.: 77.34%] [G loss: 1.284560]\n",
      "epoch:31 step:29919 [D loss: 0.372780, acc.: 88.28%] [G loss: 1.545406]\n",
      "epoch:31 step:29920 [D loss: 0.553548, acc.: 70.31%] [G loss: 1.477015]\n",
      "epoch:31 step:29921 [D loss: 0.422149, acc.: 82.81%] [G loss: 1.333675]\n",
      "epoch:31 step:29922 [D loss: 0.478030, acc.: 75.00%] [G loss: 1.535117]\n",
      "epoch:31 step:29923 [D loss: 0.660269, acc.: 67.19%] [G loss: 1.890141]\n",
      "epoch:31 step:29924 [D loss: 0.443195, acc.: 77.34%] [G loss: 1.385895]\n",
      "epoch:31 step:29925 [D loss: 0.526826, acc.: 69.53%] [G loss: 2.005107]\n",
      "epoch:31 step:29926 [D loss: 0.620811, acc.: 68.75%] [G loss: 1.640882]\n",
      "epoch:31 step:29927 [D loss: 0.495578, acc.: 78.91%] [G loss: 1.567840]\n",
      "epoch:31 step:29928 [D loss: 0.745459, acc.: 63.28%] [G loss: 2.087224]\n",
      "epoch:31 step:29929 [D loss: 0.556711, acc.: 70.31%] [G loss: 1.556179]\n",
      "epoch:31 step:29930 [D loss: 0.744954, acc.: 55.47%] [G loss: 1.297952]\n",
      "epoch:31 step:29931 [D loss: 0.419876, acc.: 84.38%] [G loss: 1.438562]\n",
      "epoch:31 step:29932 [D loss: 0.572979, acc.: 69.53%] [G loss: 1.549262]\n",
      "epoch:31 step:29933 [D loss: 0.598828, acc.: 66.41%] [G loss: 1.469414]\n",
      "epoch:31 step:29934 [D loss: 0.549287, acc.: 71.88%] [G loss: 1.404484]\n",
      "epoch:31 step:29935 [D loss: 0.422462, acc.: 81.25%] [G loss: 1.503074]\n",
      "epoch:31 step:29936 [D loss: 0.511373, acc.: 75.00%] [G loss: 1.410755]\n",
      "epoch:31 step:29937 [D loss: 0.545378, acc.: 74.22%] [G loss: 1.599982]\n",
      "epoch:31 step:29938 [D loss: 0.560859, acc.: 74.22%] [G loss: 2.061279]\n",
      "epoch:31 step:29939 [D loss: 0.678562, acc.: 64.06%] [G loss: 1.167439]\n",
      "epoch:31 step:29940 [D loss: 0.746050, acc.: 57.03%] [G loss: 1.278801]\n",
      "epoch:31 step:29941 [D loss: 0.660045, acc.: 62.50%] [G loss: 1.684895]\n",
      "epoch:31 step:29942 [D loss: 0.500647, acc.: 76.56%] [G loss: 1.818903]\n",
      "epoch:31 step:29943 [D loss: 0.632679, acc.: 65.62%] [G loss: 2.007146]\n",
      "epoch:31 step:29944 [D loss: 0.554112, acc.: 71.09%] [G loss: 1.572012]\n",
      "epoch:31 step:29945 [D loss: 0.430776, acc.: 82.03%] [G loss: 1.554409]\n",
      "epoch:31 step:29946 [D loss: 0.717250, acc.: 55.47%] [G loss: 1.259499]\n",
      "epoch:31 step:29947 [D loss: 0.406199, acc.: 82.03%] [G loss: 1.646147]\n",
      "epoch:31 step:29948 [D loss: 0.512010, acc.: 77.34%] [G loss: 1.533600]\n",
      "epoch:31 step:29949 [D loss: 0.682059, acc.: 57.03%] [G loss: 0.967946]\n",
      "epoch:31 step:29950 [D loss: 0.607401, acc.: 69.53%] [G loss: 1.545717]\n",
      "epoch:31 step:29951 [D loss: 0.623054, acc.: 69.53%] [G loss: 1.799663]\n",
      "epoch:31 step:29952 [D loss: 0.744086, acc.: 56.25%] [G loss: 1.690593]\n",
      "epoch:31 step:29953 [D loss: 0.558175, acc.: 71.88%] [G loss: 1.654171]\n",
      "epoch:31 step:29954 [D loss: 0.532418, acc.: 75.78%] [G loss: 1.224780]\n",
      "epoch:31 step:29955 [D loss: 0.575953, acc.: 71.88%] [G loss: 1.025847]\n",
      "epoch:31 step:29956 [D loss: 0.607945, acc.: 69.53%] [G loss: 1.448236]\n",
      "epoch:31 step:29957 [D loss: 0.486698, acc.: 78.91%] [G loss: 1.335297]\n",
      "epoch:31 step:29958 [D loss: 0.673983, acc.: 63.28%] [G loss: 1.356679]\n",
      "epoch:31 step:29959 [D loss: 0.669078, acc.: 61.72%] [G loss: 2.155085]\n",
      "epoch:31 step:29960 [D loss: 0.553708, acc.: 68.75%] [G loss: 1.565123]\n",
      "epoch:31 step:29961 [D loss: 0.614473, acc.: 66.41%] [G loss: 1.231278]\n",
      "epoch:31 step:29962 [D loss: 0.574151, acc.: 71.09%] [G loss: 1.486819]\n",
      "epoch:31 step:29963 [D loss: 0.363162, acc.: 85.94%] [G loss: 1.621636]\n",
      "epoch:31 step:29964 [D loss: 0.716272, acc.: 55.47%] [G loss: 1.244224]\n",
      "epoch:31 step:29965 [D loss: 0.423478, acc.: 86.72%] [G loss: 2.165045]\n",
      "epoch:31 step:29966 [D loss: 0.519959, acc.: 73.44%] [G loss: 1.188188]\n",
      "epoch:31 step:29967 [D loss: 0.530727, acc.: 77.34%] [G loss: 1.301980]\n",
      "epoch:31 step:29968 [D loss: 0.590477, acc.: 70.31%] [G loss: 1.904846]\n",
      "epoch:31 step:29969 [D loss: 0.362779, acc.: 85.94%] [G loss: 1.495276]\n",
      "epoch:31 step:29970 [D loss: 0.374786, acc.: 84.38%] [G loss: 1.534434]\n",
      "epoch:31 step:29971 [D loss: 0.570135, acc.: 70.31%] [G loss: 1.532378]\n",
      "epoch:31 step:29972 [D loss: 0.391646, acc.: 86.72%] [G loss: 1.815872]\n",
      "epoch:31 step:29973 [D loss: 0.414610, acc.: 81.25%] [G loss: 1.528919]\n",
      "epoch:31 step:29974 [D loss: 0.430360, acc.: 77.34%] [G loss: 1.331406]\n",
      "epoch:31 step:29975 [D loss: 0.340670, acc.: 86.72%] [G loss: 1.792741]\n",
      "epoch:31 step:29976 [D loss: 0.521034, acc.: 79.69%] [G loss: 1.199144]\n",
      "epoch:31 step:29977 [D loss: 0.446225, acc.: 81.25%] [G loss: 1.693779]\n",
      "epoch:31 step:29978 [D loss: 0.585503, acc.: 71.09%] [G loss: 0.907907]\n",
      "epoch:31 step:29979 [D loss: 0.414662, acc.: 84.38%] [G loss: 1.987474]\n",
      "epoch:31 step:29980 [D loss: 0.657962, acc.: 65.62%] [G loss: 1.845801]\n",
      "epoch:31 step:29981 [D loss: 0.682503, acc.: 63.28%] [G loss: 1.297954]\n",
      "epoch:31 step:29982 [D loss: 0.297739, acc.: 89.84%] [G loss: 1.456568]\n",
      "epoch:31 step:29983 [D loss: 0.513023, acc.: 75.00%] [G loss: 1.396792]\n",
      "epoch:31 step:29984 [D loss: 0.569747, acc.: 71.88%] [G loss: 1.869021]\n",
      "epoch:32 step:29985 [D loss: 0.615467, acc.: 64.06%] [G loss: 0.852681]\n",
      "epoch:32 step:29986 [D loss: 0.588223, acc.: 67.19%] [G loss: 1.331470]\n",
      "epoch:32 step:29987 [D loss: 0.537677, acc.: 68.75%] [G loss: 1.281409]\n",
      "epoch:32 step:29988 [D loss: 0.539842, acc.: 71.09%] [G loss: 1.620287]\n",
      "epoch:32 step:29989 [D loss: 0.452734, acc.: 78.12%] [G loss: 1.909123]\n",
      "epoch:32 step:29990 [D loss: 0.773577, acc.: 50.78%] [G loss: 1.115932]\n",
      "epoch:32 step:29991 [D loss: 0.489153, acc.: 72.66%] [G loss: 1.562994]\n",
      "epoch:32 step:29992 [D loss: 0.299095, acc.: 91.41%] [G loss: 1.633231]\n",
      "epoch:32 step:29993 [D loss: 0.341524, acc.: 86.72%] [G loss: 1.885760]\n",
      "epoch:32 step:29994 [D loss: 0.380946, acc.: 88.28%] [G loss: 2.029167]\n",
      "epoch:32 step:29995 [D loss: 0.454283, acc.: 79.69%] [G loss: 1.385520]\n",
      "epoch:32 step:29996 [D loss: 0.549117, acc.: 72.66%] [G loss: 1.519412]\n",
      "epoch:32 step:29997 [D loss: 0.708870, acc.: 64.84%] [G loss: 1.780233]\n",
      "epoch:32 step:29998 [D loss: 0.534501, acc.: 76.56%] [G loss: 1.501235]\n",
      "epoch:32 step:29999 [D loss: 0.477800, acc.: 79.69%] [G loss: 1.374243]\n",
      "epoch:32 step:30000 [D loss: 0.647935, acc.: 64.06%] [G loss: 1.806846]\n",
      "##############\n",
      "[2.80839537 2.25515371 2.03306067 2.84738802 1.20694218 6.19463138\n",
      " 2.22125241 2.44101218 4.0378809  5.58843018]\n",
      "##########\n",
      "epoch:32 step:30001 [D loss: 0.655264, acc.: 66.41%] [G loss: 1.653060]\n",
      "epoch:32 step:30002 [D loss: 0.559203, acc.: 64.06%] [G loss: 1.396994]\n",
      "epoch:32 step:30003 [D loss: 0.454476, acc.: 82.81%] [G loss: 1.318233]\n",
      "epoch:32 step:30004 [D loss: 0.547795, acc.: 76.56%] [G loss: 0.854342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30005 [D loss: 0.670244, acc.: 64.84%] [G loss: 0.987680]\n",
      "epoch:32 step:30006 [D loss: 0.565879, acc.: 71.88%] [G loss: 1.297541]\n",
      "epoch:32 step:30007 [D loss: 0.455560, acc.: 78.91%] [G loss: 1.637711]\n",
      "epoch:32 step:30008 [D loss: 0.489407, acc.: 77.34%] [G loss: 2.133132]\n",
      "epoch:32 step:30009 [D loss: 0.356928, acc.: 89.06%] [G loss: 1.851011]\n",
      "epoch:32 step:30010 [D loss: 0.544306, acc.: 69.53%] [G loss: 1.021916]\n",
      "epoch:32 step:30011 [D loss: 0.664520, acc.: 61.72%] [G loss: 1.257267]\n",
      "epoch:32 step:30012 [D loss: 0.624067, acc.: 67.19%] [G loss: 1.404530]\n",
      "epoch:32 step:30013 [D loss: 0.471565, acc.: 78.91%] [G loss: 1.327481]\n",
      "epoch:32 step:30014 [D loss: 0.427592, acc.: 82.03%] [G loss: 1.455305]\n",
      "epoch:32 step:30015 [D loss: 0.539368, acc.: 71.88%] [G loss: 1.541035]\n",
      "epoch:32 step:30016 [D loss: 0.546531, acc.: 67.97%] [G loss: 1.300970]\n",
      "epoch:32 step:30017 [D loss: 0.553653, acc.: 70.31%] [G loss: 1.174735]\n",
      "epoch:32 step:30018 [D loss: 0.428075, acc.: 83.59%] [G loss: 1.614165]\n",
      "epoch:32 step:30019 [D loss: 0.553658, acc.: 72.66%] [G loss: 1.317559]\n",
      "epoch:32 step:30020 [D loss: 0.336328, acc.: 88.28%] [G loss: 1.813290]\n",
      "epoch:32 step:30021 [D loss: 0.585267, acc.: 70.31%] [G loss: 1.724920]\n",
      "epoch:32 step:30022 [D loss: 0.455645, acc.: 78.91%] [G loss: 1.450129]\n",
      "epoch:32 step:30023 [D loss: 0.452784, acc.: 82.81%] [G loss: 1.173610]\n",
      "epoch:32 step:30024 [D loss: 0.549431, acc.: 70.31%] [G loss: 1.379714]\n",
      "epoch:32 step:30025 [D loss: 0.481095, acc.: 76.56%] [G loss: 1.726763]\n",
      "epoch:32 step:30026 [D loss: 0.612285, acc.: 65.62%] [G loss: 1.898265]\n",
      "epoch:32 step:30027 [D loss: 0.507062, acc.: 75.78%] [G loss: 1.928414]\n",
      "epoch:32 step:30028 [D loss: 0.703984, acc.: 57.81%] [G loss: 1.350687]\n",
      "epoch:32 step:30029 [D loss: 0.387140, acc.: 82.81%] [G loss: 1.918214]\n",
      "epoch:32 step:30030 [D loss: 0.560987, acc.: 74.22%] [G loss: 1.462313]\n",
      "epoch:32 step:30031 [D loss: 0.481742, acc.: 78.12%] [G loss: 1.705676]\n",
      "epoch:32 step:30032 [D loss: 0.618147, acc.: 66.41%] [G loss: 1.306054]\n",
      "epoch:32 step:30033 [D loss: 0.421651, acc.: 89.06%] [G loss: 1.180263]\n",
      "epoch:32 step:30034 [D loss: 0.653766, acc.: 65.62%] [G loss: 1.203738]\n",
      "epoch:32 step:30035 [D loss: 0.519940, acc.: 74.22%] [G loss: 1.461678]\n",
      "epoch:32 step:30036 [D loss: 0.543252, acc.: 74.22%] [G loss: 1.232975]\n",
      "epoch:32 step:30037 [D loss: 0.513044, acc.: 77.34%] [G loss: 1.468834]\n",
      "epoch:32 step:30038 [D loss: 0.494175, acc.: 75.78%] [G loss: 1.472320]\n",
      "epoch:32 step:30039 [D loss: 0.542146, acc.: 73.44%] [G loss: 1.423071]\n",
      "epoch:32 step:30040 [D loss: 0.467343, acc.: 79.69%] [G loss: 1.271633]\n",
      "epoch:32 step:30041 [D loss: 0.458804, acc.: 78.91%] [G loss: 1.449893]\n",
      "epoch:32 step:30042 [D loss: 0.367135, acc.: 87.50%] [G loss: 1.657756]\n",
      "epoch:32 step:30043 [D loss: 0.420796, acc.: 82.81%] [G loss: 1.904840]\n",
      "epoch:32 step:30044 [D loss: 0.613252, acc.: 64.84%] [G loss: 1.154742]\n",
      "epoch:32 step:30045 [D loss: 0.640397, acc.: 64.06%] [G loss: 1.672724]\n",
      "epoch:32 step:30046 [D loss: 0.579248, acc.: 72.66%] [G loss: 1.757127]\n",
      "epoch:32 step:30047 [D loss: 0.577017, acc.: 69.53%] [G loss: 1.249977]\n",
      "epoch:32 step:30048 [D loss: 0.435515, acc.: 78.12%] [G loss: 1.804660]\n",
      "epoch:32 step:30049 [D loss: 0.527609, acc.: 72.66%] [G loss: 1.365548]\n",
      "epoch:32 step:30050 [D loss: 0.613785, acc.: 68.75%] [G loss: 1.288602]\n",
      "epoch:32 step:30051 [D loss: 0.457586, acc.: 79.69%] [G loss: 1.301215]\n",
      "epoch:32 step:30052 [D loss: 0.495725, acc.: 75.00%] [G loss: 1.753659]\n",
      "epoch:32 step:30053 [D loss: 0.453459, acc.: 82.81%] [G loss: 1.376339]\n",
      "epoch:32 step:30054 [D loss: 0.586190, acc.: 71.88%] [G loss: 1.383838]\n",
      "epoch:32 step:30055 [D loss: 0.670447, acc.: 60.94%] [G loss: 1.559526]\n",
      "epoch:32 step:30056 [D loss: 0.600746, acc.: 65.62%] [G loss: 1.704385]\n",
      "epoch:32 step:30057 [D loss: 0.388174, acc.: 85.16%] [G loss: 1.431046]\n",
      "epoch:32 step:30058 [D loss: 0.378909, acc.: 88.28%] [G loss: 1.737972]\n",
      "epoch:32 step:30059 [D loss: 0.717831, acc.: 56.25%] [G loss: 1.470623]\n",
      "epoch:32 step:30060 [D loss: 0.697761, acc.: 58.59%] [G loss: 1.359562]\n",
      "epoch:32 step:30061 [D loss: 0.702697, acc.: 64.84%] [G loss: 0.878551]\n",
      "epoch:32 step:30062 [D loss: 0.730644, acc.: 55.47%] [G loss: 1.143953]\n",
      "epoch:32 step:30063 [D loss: 0.551142, acc.: 71.88%] [G loss: 1.219396]\n",
      "epoch:32 step:30064 [D loss: 0.432544, acc.: 82.81%] [G loss: 1.513800]\n",
      "epoch:32 step:30065 [D loss: 0.831969, acc.: 47.66%] [G loss: 1.534706]\n",
      "epoch:32 step:30066 [D loss: 0.354252, acc.: 87.50%] [G loss: 1.671032]\n",
      "epoch:32 step:30067 [D loss: 0.598855, acc.: 69.53%] [G loss: 0.946825]\n",
      "epoch:32 step:30068 [D loss: 0.617446, acc.: 75.00%] [G loss: 1.094820]\n",
      "epoch:32 step:30069 [D loss: 0.404268, acc.: 85.16%] [G loss: 1.386652]\n",
      "epoch:32 step:30070 [D loss: 0.516627, acc.: 77.34%] [G loss: 1.964317]\n",
      "epoch:32 step:30071 [D loss: 0.520511, acc.: 74.22%] [G loss: 1.578106]\n",
      "epoch:32 step:30072 [D loss: 0.460963, acc.: 77.34%] [G loss: 1.036055]\n",
      "epoch:32 step:30073 [D loss: 0.534997, acc.: 75.00%] [G loss: 1.636937]\n",
      "epoch:32 step:30074 [D loss: 0.596648, acc.: 67.19%] [G loss: 1.423904]\n",
      "epoch:32 step:30075 [D loss: 0.494897, acc.: 76.56%] [G loss: 1.486984]\n",
      "epoch:32 step:30076 [D loss: 0.462813, acc.: 79.69%] [G loss: 1.517300]\n",
      "epoch:32 step:30077 [D loss: 0.499358, acc.: 79.69%] [G loss: 1.632728]\n",
      "epoch:32 step:30078 [D loss: 0.645918, acc.: 61.72%] [G loss: 1.340897]\n",
      "epoch:32 step:30079 [D loss: 0.462105, acc.: 78.91%] [G loss: 1.669078]\n",
      "epoch:32 step:30080 [D loss: 0.428024, acc.: 83.59%] [G loss: 2.071856]\n",
      "epoch:32 step:30081 [D loss: 0.628865, acc.: 66.41%] [G loss: 1.505245]\n",
      "epoch:32 step:30082 [D loss: 0.464796, acc.: 78.12%] [G loss: 1.389811]\n",
      "epoch:32 step:30083 [D loss: 0.411650, acc.: 82.03%] [G loss: 1.241796]\n",
      "epoch:32 step:30084 [D loss: 0.693701, acc.: 56.25%] [G loss: 1.073974]\n",
      "epoch:32 step:30085 [D loss: 0.435171, acc.: 80.47%] [G loss: 1.834381]\n",
      "epoch:32 step:30086 [D loss: 0.508744, acc.: 74.22%] [G loss: 1.636700]\n",
      "epoch:32 step:30087 [D loss: 0.383715, acc.: 85.94%] [G loss: 1.512819]\n",
      "epoch:32 step:30088 [D loss: 0.484989, acc.: 77.34%] [G loss: 1.547316]\n",
      "epoch:32 step:30089 [D loss: 0.393786, acc.: 85.94%] [G loss: 1.367571]\n",
      "epoch:32 step:30090 [D loss: 0.467713, acc.: 75.00%] [G loss: 1.303532]\n",
      "epoch:32 step:30091 [D loss: 0.563190, acc.: 74.22%] [G loss: 1.518250]\n",
      "epoch:32 step:30092 [D loss: 0.575081, acc.: 74.22%] [G loss: 1.633293]\n",
      "epoch:32 step:30093 [D loss: 0.606353, acc.: 67.19%] [G loss: 1.450132]\n",
      "epoch:32 step:30094 [D loss: 0.739991, acc.: 54.69%] [G loss: 1.099620]\n",
      "epoch:32 step:30095 [D loss: 0.621555, acc.: 73.44%] [G loss: 1.090191]\n",
      "epoch:32 step:30096 [D loss: 0.487497, acc.: 75.78%] [G loss: 1.250225]\n",
      "epoch:32 step:30097 [D loss: 0.515645, acc.: 75.00%] [G loss: 1.376106]\n",
      "epoch:32 step:30098 [D loss: 0.431925, acc.: 79.69%] [G loss: 1.534632]\n",
      "epoch:32 step:30099 [D loss: 0.418862, acc.: 86.72%] [G loss: 1.503347]\n",
      "epoch:32 step:30100 [D loss: 0.652393, acc.: 65.62%] [G loss: 1.387910]\n",
      "epoch:32 step:30101 [D loss: 0.444637, acc.: 78.12%] [G loss: 1.185760]\n",
      "epoch:32 step:30102 [D loss: 0.635987, acc.: 62.50%] [G loss: 1.173675]\n",
      "epoch:32 step:30103 [D loss: 0.469835, acc.: 79.69%] [G loss: 1.632941]\n",
      "epoch:32 step:30104 [D loss: 0.728567, acc.: 57.81%] [G loss: 1.633133]\n",
      "epoch:32 step:30105 [D loss: 0.576050, acc.: 66.41%] [G loss: 1.548150]\n",
      "epoch:32 step:30106 [D loss: 0.500261, acc.: 78.12%] [G loss: 1.769669]\n",
      "epoch:32 step:30107 [D loss: 0.389963, acc.: 85.16%] [G loss: 1.342486]\n",
      "epoch:32 step:30108 [D loss: 0.355591, acc.: 87.50%] [G loss: 1.564918]\n",
      "epoch:32 step:30109 [D loss: 0.711525, acc.: 57.03%] [G loss: 1.167997]\n",
      "epoch:32 step:30110 [D loss: 0.376120, acc.: 88.28%] [G loss: 1.725357]\n",
      "epoch:32 step:30111 [D loss: 0.493180, acc.: 77.34%] [G loss: 1.643842]\n",
      "epoch:32 step:30112 [D loss: 0.505918, acc.: 74.22%] [G loss: 1.863844]\n",
      "epoch:32 step:30113 [D loss: 0.540920, acc.: 75.78%] [G loss: 1.278358]\n",
      "epoch:32 step:30114 [D loss: 0.532295, acc.: 74.22%] [G loss: 1.539447]\n",
      "epoch:32 step:30115 [D loss: 0.484735, acc.: 75.78%] [G loss: 1.853467]\n",
      "epoch:32 step:30116 [D loss: 0.566323, acc.: 72.66%] [G loss: 1.403798]\n",
      "epoch:32 step:30117 [D loss: 0.465104, acc.: 82.03%] [G loss: 1.837281]\n",
      "epoch:32 step:30118 [D loss: 0.590652, acc.: 72.66%] [G loss: 1.585055]\n",
      "epoch:32 step:30119 [D loss: 0.650313, acc.: 63.28%] [G loss: 1.342521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30120 [D loss: 0.849254, acc.: 50.00%] [G loss: 1.081945]\n",
      "epoch:32 step:30121 [D loss: 0.422886, acc.: 82.03%] [G loss: 1.701527]\n",
      "epoch:32 step:30122 [D loss: 0.609528, acc.: 65.62%] [G loss: 1.690138]\n",
      "epoch:32 step:30123 [D loss: 0.564689, acc.: 69.53%] [G loss: 1.260497]\n",
      "epoch:32 step:30124 [D loss: 0.628647, acc.: 63.28%] [G loss: 1.863183]\n",
      "epoch:32 step:30125 [D loss: 0.642535, acc.: 61.72%] [G loss: 1.131950]\n",
      "epoch:32 step:30126 [D loss: 0.358613, acc.: 91.41%] [G loss: 1.636636]\n",
      "epoch:32 step:30127 [D loss: 0.569112, acc.: 67.19%] [G loss: 1.477593]\n",
      "epoch:32 step:30128 [D loss: 0.611025, acc.: 67.19%] [G loss: 1.393994]\n",
      "epoch:32 step:30129 [D loss: 0.374753, acc.: 86.72%] [G loss: 1.570472]\n",
      "epoch:32 step:30130 [D loss: 0.463369, acc.: 78.12%] [G loss: 1.935093]\n",
      "epoch:32 step:30131 [D loss: 0.429168, acc.: 80.47%] [G loss: 1.306703]\n",
      "epoch:32 step:30132 [D loss: 0.474701, acc.: 77.34%] [G loss: 1.776371]\n",
      "epoch:32 step:30133 [D loss: 0.695919, acc.: 62.50%] [G loss: 0.944146]\n",
      "epoch:32 step:30134 [D loss: 0.551113, acc.: 75.78%] [G loss: 1.193563]\n",
      "epoch:32 step:30135 [D loss: 0.352624, acc.: 89.06%] [G loss: 1.738962]\n",
      "epoch:32 step:30136 [D loss: 0.704100, acc.: 63.28%] [G loss: 1.077739]\n",
      "epoch:32 step:30137 [D loss: 0.551749, acc.: 70.31%] [G loss: 1.225173]\n",
      "epoch:32 step:30138 [D loss: 0.482283, acc.: 79.69%] [G loss: 1.367984]\n",
      "epoch:32 step:30139 [D loss: 0.513938, acc.: 75.78%] [G loss: 1.543767]\n",
      "epoch:32 step:30140 [D loss: 0.600002, acc.: 65.62%] [G loss: 1.403286]\n",
      "epoch:32 step:30141 [D loss: 0.614460, acc.: 64.06%] [G loss: 1.611934]\n",
      "epoch:32 step:30142 [D loss: 0.443817, acc.: 85.94%] [G loss: 1.175084]\n",
      "epoch:32 step:30143 [D loss: 0.470063, acc.: 79.69%] [G loss: 1.232610]\n",
      "epoch:32 step:30144 [D loss: 0.660091, acc.: 58.59%] [G loss: 1.491107]\n",
      "epoch:32 step:30145 [D loss: 0.415789, acc.: 79.69%] [G loss: 1.525323]\n",
      "epoch:32 step:30146 [D loss: 0.748670, acc.: 60.16%] [G loss: 1.589158]\n",
      "epoch:32 step:30147 [D loss: 0.619502, acc.: 65.62%] [G loss: 1.521917]\n",
      "epoch:32 step:30148 [D loss: 0.467466, acc.: 77.34%] [G loss: 1.229216]\n",
      "epoch:32 step:30149 [D loss: 0.780589, acc.: 50.00%] [G loss: 1.030349]\n",
      "epoch:32 step:30150 [D loss: 0.418280, acc.: 80.47%] [G loss: 1.134646]\n",
      "epoch:32 step:30151 [D loss: 0.417767, acc.: 77.34%] [G loss: 1.537395]\n",
      "epoch:32 step:30152 [D loss: 0.507289, acc.: 76.56%] [G loss: 1.503503]\n",
      "epoch:32 step:30153 [D loss: 0.382958, acc.: 85.16%] [G loss: 1.421595]\n",
      "epoch:32 step:30154 [D loss: 0.472855, acc.: 76.56%] [G loss: 2.030677]\n",
      "epoch:32 step:30155 [D loss: 0.618166, acc.: 62.50%] [G loss: 1.522014]\n",
      "epoch:32 step:30156 [D loss: 0.451485, acc.: 83.59%] [G loss: 1.429804]\n",
      "epoch:32 step:30157 [D loss: 0.703110, acc.: 55.47%] [G loss: 1.358682]\n",
      "epoch:32 step:30158 [D loss: 0.371374, acc.: 86.72%] [G loss: 1.646955]\n",
      "epoch:32 step:30159 [D loss: 0.624073, acc.: 63.28%] [G loss: 1.164042]\n",
      "epoch:32 step:30160 [D loss: 0.530832, acc.: 69.53%] [G loss: 1.329733]\n",
      "epoch:32 step:30161 [D loss: 0.560980, acc.: 72.66%] [G loss: 2.192970]\n",
      "epoch:32 step:30162 [D loss: 0.423888, acc.: 83.59%] [G loss: 1.426412]\n",
      "epoch:32 step:30163 [D loss: 0.655650, acc.: 64.06%] [G loss: 1.116887]\n",
      "epoch:32 step:30164 [D loss: 0.466996, acc.: 79.69%] [G loss: 1.660537]\n",
      "epoch:32 step:30165 [D loss: 0.613249, acc.: 70.31%] [G loss: 1.180237]\n",
      "epoch:32 step:30166 [D loss: 0.507491, acc.: 75.00%] [G loss: 1.348504]\n",
      "epoch:32 step:30167 [D loss: 0.425896, acc.: 80.47%] [G loss: 1.378297]\n",
      "epoch:32 step:30168 [D loss: 0.494351, acc.: 75.00%] [G loss: 1.874070]\n",
      "epoch:32 step:30169 [D loss: 0.527740, acc.: 73.44%] [G loss: 1.592947]\n",
      "epoch:32 step:30170 [D loss: 0.512050, acc.: 75.00%] [G loss: 1.983477]\n",
      "epoch:32 step:30171 [D loss: 0.312532, acc.: 90.62%] [G loss: 1.539072]\n",
      "epoch:32 step:30172 [D loss: 0.495581, acc.: 78.12%] [G loss: 1.262373]\n",
      "epoch:32 step:30173 [D loss: 0.692358, acc.: 58.59%] [G loss: 1.852670]\n",
      "epoch:32 step:30174 [D loss: 0.606234, acc.: 68.75%] [G loss: 1.187540]\n",
      "epoch:32 step:30175 [D loss: 0.404599, acc.: 81.25%] [G loss: 1.621091]\n",
      "epoch:32 step:30176 [D loss: 0.486086, acc.: 77.34%] [G loss: 1.737979]\n",
      "epoch:32 step:30177 [D loss: 0.635202, acc.: 62.50%] [G loss: 1.212569]\n",
      "epoch:32 step:30178 [D loss: 0.558692, acc.: 72.66%] [G loss: 0.853204]\n",
      "epoch:32 step:30179 [D loss: 0.603731, acc.: 61.72%] [G loss: 1.661196]\n",
      "epoch:32 step:30180 [D loss: 0.433979, acc.: 79.69%] [G loss: 1.439363]\n",
      "epoch:32 step:30181 [D loss: 0.516210, acc.: 72.66%] [G loss: 1.378259]\n",
      "epoch:32 step:30182 [D loss: 0.616863, acc.: 67.19%] [G loss: 1.755387]\n",
      "epoch:32 step:30183 [D loss: 0.539160, acc.: 74.22%] [G loss: 1.420687]\n",
      "epoch:32 step:30184 [D loss: 0.617199, acc.: 65.62%] [G loss: 1.164568]\n",
      "epoch:32 step:30185 [D loss: 0.376982, acc.: 85.16%] [G loss: 1.688190]\n",
      "epoch:32 step:30186 [D loss: 0.497808, acc.: 75.00%] [G loss: 1.700784]\n",
      "epoch:32 step:30187 [D loss: 0.503194, acc.: 78.91%] [G loss: 1.390935]\n",
      "epoch:32 step:30188 [D loss: 0.423124, acc.: 79.69%] [G loss: 0.878636]\n",
      "epoch:32 step:30189 [D loss: 0.468316, acc.: 78.91%] [G loss: 1.669880]\n",
      "epoch:32 step:30190 [D loss: 0.439547, acc.: 79.69%] [G loss: 1.387763]\n",
      "epoch:32 step:30191 [D loss: 0.538010, acc.: 74.22%] [G loss: 1.394616]\n",
      "epoch:32 step:30192 [D loss: 0.336884, acc.: 88.28%] [G loss: 1.647175]\n",
      "epoch:32 step:30193 [D loss: 0.572065, acc.: 69.53%] [G loss: 1.641349]\n",
      "epoch:32 step:30194 [D loss: 0.559324, acc.: 69.53%] [G loss: 1.392873]\n",
      "epoch:32 step:30195 [D loss: 0.427148, acc.: 82.81%] [G loss: 1.534247]\n",
      "epoch:32 step:30196 [D loss: 0.600443, acc.: 70.31%] [G loss: 1.705055]\n",
      "epoch:32 step:30197 [D loss: 0.540417, acc.: 75.00%] [G loss: 1.240374]\n",
      "epoch:32 step:30198 [D loss: 0.847980, acc.: 46.09%] [G loss: 0.690174]\n",
      "epoch:32 step:30199 [D loss: 0.646167, acc.: 60.16%] [G loss: 1.317819]\n",
      "epoch:32 step:30200 [D loss: 0.704550, acc.: 60.94%] [G loss: 1.510017]\n",
      "##############\n",
      "[2.69897582 2.02206167 2.14155168 2.58132461 0.79597921 5.14807373\n",
      " 2.06642336 2.54118698 3.98964015 7.14868929]\n",
      "##########\n",
      "epoch:32 step:30201 [D loss: 0.332289, acc.: 88.28%] [G loss: 1.389150]\n",
      "epoch:32 step:30202 [D loss: 0.604863, acc.: 71.88%] [G loss: 1.162052]\n",
      "epoch:32 step:30203 [D loss: 0.463392, acc.: 79.69%] [G loss: 1.283614]\n",
      "epoch:32 step:30204 [D loss: 0.350915, acc.: 90.62%] [G loss: 1.743960]\n",
      "epoch:32 step:30205 [D loss: 0.720057, acc.: 50.00%] [G loss: 1.183982]\n",
      "epoch:32 step:30206 [D loss: 0.522553, acc.: 75.00%] [G loss: 1.628731]\n",
      "epoch:32 step:30207 [D loss: 0.408035, acc.: 81.25%] [G loss: 1.792664]\n",
      "epoch:32 step:30208 [D loss: 0.685036, acc.: 60.16%] [G loss: 1.603631]\n",
      "epoch:32 step:30209 [D loss: 0.564732, acc.: 71.88%] [G loss: 1.572864]\n",
      "epoch:32 step:30210 [D loss: 0.368523, acc.: 83.59%] [G loss: 1.524632]\n",
      "epoch:32 step:30211 [D loss: 0.746443, acc.: 57.81%] [G loss: 1.130869]\n",
      "epoch:32 step:30212 [D loss: 0.500583, acc.: 72.66%] [G loss: 1.651286]\n",
      "epoch:32 step:30213 [D loss: 0.661330, acc.: 64.06%] [G loss: 1.493862]\n",
      "epoch:32 step:30214 [D loss: 0.542364, acc.: 71.88%] [G loss: 1.142133]\n",
      "epoch:32 step:30215 [D loss: 0.599726, acc.: 67.97%] [G loss: 1.434112]\n",
      "epoch:32 step:30216 [D loss: 0.542841, acc.: 72.66%] [G loss: 1.935987]\n",
      "epoch:32 step:30217 [D loss: 0.486127, acc.: 78.91%] [G loss: 1.477854]\n",
      "epoch:32 step:30218 [D loss: 0.686704, acc.: 57.81%] [G loss: 1.257946]\n",
      "epoch:32 step:30219 [D loss: 0.540192, acc.: 73.44%] [G loss: 1.720373]\n",
      "epoch:32 step:30220 [D loss: 0.618739, acc.: 66.41%] [G loss: 1.473553]\n",
      "epoch:32 step:30221 [D loss: 0.655292, acc.: 61.72%] [G loss: 1.145772]\n",
      "epoch:32 step:30222 [D loss: 0.537900, acc.: 72.66%] [G loss: 1.378791]\n",
      "epoch:32 step:30223 [D loss: 0.567824, acc.: 66.41%] [G loss: 1.390371]\n",
      "epoch:32 step:30224 [D loss: 0.529042, acc.: 71.88%] [G loss: 1.426865]\n",
      "epoch:32 step:30225 [D loss: 0.588054, acc.: 71.09%] [G loss: 1.406934]\n",
      "epoch:32 step:30226 [D loss: 0.520751, acc.: 78.12%] [G loss: 1.356956]\n",
      "epoch:32 step:30227 [D loss: 0.513946, acc.: 73.44%] [G loss: 1.179275]\n",
      "epoch:32 step:30228 [D loss: 0.563308, acc.: 71.09%] [G loss: 1.162827]\n",
      "epoch:32 step:30229 [D loss: 0.424955, acc.: 81.25%] [G loss: 1.469586]\n",
      "epoch:32 step:30230 [D loss: 0.515973, acc.: 77.34%] [G loss: 1.223982]\n",
      "epoch:32 step:30231 [D loss: 0.716952, acc.: 60.16%] [G loss: 0.709600]\n",
      "epoch:32 step:30232 [D loss: 0.708146, acc.: 53.91%] [G loss: 1.379457]\n",
      "epoch:32 step:30233 [D loss: 0.283570, acc.: 91.41%] [G loss: 1.744316]\n",
      "epoch:32 step:30234 [D loss: 0.547734, acc.: 67.97%] [G loss: 1.403538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30235 [D loss: 0.567280, acc.: 69.53%] [G loss: 1.400692]\n",
      "epoch:32 step:30236 [D loss: 0.461288, acc.: 82.03%] [G loss: 1.655244]\n",
      "epoch:32 step:30237 [D loss: 0.607754, acc.: 64.84%] [G loss: 1.178593]\n",
      "epoch:32 step:30238 [D loss: 0.475043, acc.: 78.91%] [G loss: 1.941488]\n",
      "epoch:32 step:30239 [D loss: 0.475848, acc.: 82.81%] [G loss: 1.756819]\n",
      "epoch:32 step:30240 [D loss: 0.611848, acc.: 65.62%] [G loss: 1.517779]\n",
      "epoch:32 step:30241 [D loss: 0.330680, acc.: 92.97%] [G loss: 1.489485]\n",
      "epoch:32 step:30242 [D loss: 0.438010, acc.: 81.25%] [G loss: 1.961254]\n",
      "epoch:32 step:30243 [D loss: 0.564608, acc.: 66.41%] [G loss: 1.533375]\n",
      "epoch:32 step:30244 [D loss: 0.549214, acc.: 70.31%] [G loss: 1.585799]\n",
      "epoch:32 step:30245 [D loss: 0.266781, acc.: 94.53%] [G loss: 1.673792]\n",
      "epoch:32 step:30246 [D loss: 0.687142, acc.: 57.81%] [G loss: 1.037156]\n",
      "epoch:32 step:30247 [D loss: 0.518994, acc.: 72.66%] [G loss: 1.394150]\n",
      "epoch:32 step:30248 [D loss: 0.353926, acc.: 85.94%] [G loss: 1.302028]\n",
      "epoch:32 step:30249 [D loss: 0.406237, acc.: 83.59%] [G loss: 1.031326]\n",
      "epoch:32 step:30250 [D loss: 0.505846, acc.: 74.22%] [G loss: 1.845529]\n",
      "epoch:32 step:30251 [D loss: 0.516563, acc.: 77.34%] [G loss: 1.891517]\n",
      "epoch:32 step:30252 [D loss: 0.570331, acc.: 71.09%] [G loss: 1.855188]\n",
      "epoch:32 step:30253 [D loss: 0.495211, acc.: 75.78%] [G loss: 1.619829]\n",
      "epoch:32 step:30254 [D loss: 0.563003, acc.: 70.31%] [G loss: 1.861122]\n",
      "epoch:32 step:30255 [D loss: 0.415107, acc.: 80.47%] [G loss: 1.741104]\n",
      "epoch:32 step:30256 [D loss: 0.611909, acc.: 67.19%] [G loss: 1.424205]\n",
      "epoch:32 step:30257 [D loss: 0.685835, acc.: 54.69%] [G loss: 1.661470]\n",
      "epoch:32 step:30258 [D loss: 0.651819, acc.: 67.97%] [G loss: 1.618375]\n",
      "epoch:32 step:30259 [D loss: 0.587463, acc.: 71.09%] [G loss: 1.745614]\n",
      "epoch:32 step:30260 [D loss: 0.671456, acc.: 64.06%] [G loss: 1.416615]\n",
      "epoch:32 step:30261 [D loss: 0.444326, acc.: 78.91%] [G loss: 1.324495]\n",
      "epoch:32 step:30262 [D loss: 0.507960, acc.: 79.69%] [G loss: 1.016865]\n",
      "epoch:32 step:30263 [D loss: 0.500155, acc.: 78.91%] [G loss: 1.472953]\n",
      "epoch:32 step:30264 [D loss: 0.552706, acc.: 72.66%] [G loss: 1.546558]\n",
      "epoch:32 step:30265 [D loss: 0.609019, acc.: 64.06%] [G loss: 1.884466]\n",
      "epoch:32 step:30266 [D loss: 0.470229, acc.: 78.12%] [G loss: 1.222804]\n",
      "epoch:32 step:30267 [D loss: 0.427234, acc.: 84.38%] [G loss: 0.938420]\n",
      "epoch:32 step:30268 [D loss: 0.569685, acc.: 74.22%] [G loss: 1.642686]\n",
      "epoch:32 step:30269 [D loss: 0.452238, acc.: 81.25%] [G loss: 1.517172]\n",
      "epoch:32 step:30270 [D loss: 0.437044, acc.: 84.38%] [G loss: 1.593212]\n",
      "epoch:32 step:30271 [D loss: 0.498880, acc.: 78.12%] [G loss: 1.683765]\n",
      "epoch:32 step:30272 [D loss: 0.547346, acc.: 68.75%] [G loss: 1.418532]\n",
      "epoch:32 step:30273 [D loss: 0.548076, acc.: 71.88%] [G loss: 1.058035]\n",
      "epoch:32 step:30274 [D loss: 0.518343, acc.: 74.22%] [G loss: 1.716448]\n",
      "epoch:32 step:30275 [D loss: 0.529308, acc.: 76.56%] [G loss: 1.743155]\n",
      "epoch:32 step:30276 [D loss: 0.418271, acc.: 84.38%] [G loss: 1.379742]\n",
      "epoch:32 step:30277 [D loss: 0.737482, acc.: 51.56%] [G loss: 1.570709]\n",
      "epoch:32 step:30278 [D loss: 0.359691, acc.: 87.50%] [G loss: 1.675442]\n",
      "epoch:32 step:30279 [D loss: 0.557397, acc.: 70.31%] [G loss: 1.653615]\n",
      "epoch:32 step:30280 [D loss: 0.568404, acc.: 69.53%] [G loss: 1.534893]\n",
      "epoch:32 step:30281 [D loss: 0.589174, acc.: 65.62%] [G loss: 1.372259]\n",
      "epoch:32 step:30282 [D loss: 0.437150, acc.: 84.38%] [G loss: 1.427757]\n",
      "epoch:32 step:30283 [D loss: 0.547602, acc.: 67.97%] [G loss: 1.311857]\n",
      "epoch:32 step:30284 [D loss: 0.517336, acc.: 75.00%] [G loss: 1.704846]\n",
      "epoch:32 step:30285 [D loss: 0.442700, acc.: 79.69%] [G loss: 1.509042]\n",
      "epoch:32 step:30286 [D loss: 0.408239, acc.: 85.16%] [G loss: 1.599881]\n",
      "epoch:32 step:30287 [D loss: 0.564358, acc.: 69.53%] [G loss: 1.704479]\n",
      "epoch:32 step:30288 [D loss: 0.550294, acc.: 72.66%] [G loss: 1.029892]\n",
      "epoch:32 step:30289 [D loss: 0.588091, acc.: 71.09%] [G loss: 1.510690]\n",
      "epoch:32 step:30290 [D loss: 0.526062, acc.: 70.31%] [G loss: 1.279821]\n",
      "epoch:32 step:30291 [D loss: 0.529482, acc.: 75.00%] [G loss: 1.782220]\n",
      "epoch:32 step:30292 [D loss: 0.441927, acc.: 80.47%] [G loss: 1.839147]\n",
      "epoch:32 step:30293 [D loss: 0.450462, acc.: 78.12%] [G loss: 1.685953]\n",
      "epoch:32 step:30294 [D loss: 0.522760, acc.: 68.75%] [G loss: 1.680546]\n",
      "epoch:32 step:30295 [D loss: 0.454327, acc.: 81.25%] [G loss: 1.579441]\n",
      "epoch:32 step:30296 [D loss: 0.605914, acc.: 67.19%] [G loss: 1.818590]\n",
      "epoch:32 step:30297 [D loss: 0.667231, acc.: 62.50%] [G loss: 1.421594]\n",
      "epoch:32 step:30298 [D loss: 0.420946, acc.: 80.47%] [G loss: 1.818909]\n",
      "epoch:32 step:30299 [D loss: 0.448208, acc.: 78.12%] [G loss: 1.524477]\n",
      "epoch:32 step:30300 [D loss: 0.537761, acc.: 68.75%] [G loss: 0.891485]\n",
      "epoch:32 step:30301 [D loss: 0.620362, acc.: 64.84%] [G loss: 1.421315]\n",
      "epoch:32 step:30302 [D loss: 0.456101, acc.: 82.03%] [G loss: 1.818936]\n",
      "epoch:32 step:30303 [D loss: 0.596418, acc.: 69.53%] [G loss: 0.872704]\n",
      "epoch:32 step:30304 [D loss: 0.509440, acc.: 73.44%] [G loss: 1.672999]\n",
      "epoch:32 step:30305 [D loss: 0.721171, acc.: 57.81%] [G loss: 1.374622]\n",
      "epoch:32 step:30306 [D loss: 0.478200, acc.: 76.56%] [G loss: 0.762232]\n",
      "epoch:32 step:30307 [D loss: 0.639718, acc.: 64.06%] [G loss: 1.663187]\n",
      "epoch:32 step:30308 [D loss: 0.647782, acc.: 61.72%] [G loss: 1.462972]\n",
      "epoch:32 step:30309 [D loss: 0.442961, acc.: 79.69%] [G loss: 1.468568]\n",
      "epoch:32 step:30310 [D loss: 0.548227, acc.: 69.53%] [G loss: 1.248993]\n",
      "epoch:32 step:30311 [D loss: 0.501921, acc.: 75.78%] [G loss: 1.591000]\n",
      "epoch:32 step:30312 [D loss: 0.422832, acc.: 82.03%] [G loss: 1.268580]\n",
      "epoch:32 step:30313 [D loss: 0.462107, acc.: 75.78%] [G loss: 1.461995]\n",
      "epoch:32 step:30314 [D loss: 0.587119, acc.: 65.62%] [G loss: 1.207267]\n",
      "epoch:32 step:30315 [D loss: 0.446753, acc.: 77.34%] [G loss: 1.392523]\n",
      "epoch:32 step:30316 [D loss: 0.523125, acc.: 77.34%] [G loss: 1.490488]\n",
      "epoch:32 step:30317 [D loss: 0.577707, acc.: 70.31%] [G loss: 1.423412]\n",
      "epoch:32 step:30318 [D loss: 0.504037, acc.: 75.78%] [G loss: 1.728430]\n",
      "epoch:32 step:30319 [D loss: 0.403515, acc.: 85.94%] [G loss: 1.714535]\n",
      "epoch:32 step:30320 [D loss: 0.322409, acc.: 89.84%] [G loss: 1.525535]\n",
      "epoch:32 step:30321 [D loss: 0.489903, acc.: 78.91%] [G loss: 1.739391]\n",
      "epoch:32 step:30322 [D loss: 0.553307, acc.: 71.88%] [G loss: 1.639675]\n",
      "epoch:32 step:30323 [D loss: 0.393950, acc.: 85.16%] [G loss: 0.876858]\n",
      "epoch:32 step:30324 [D loss: 0.584358, acc.: 67.97%] [G loss: 1.762882]\n",
      "epoch:32 step:30325 [D loss: 0.544783, acc.: 73.44%] [G loss: 1.940691]\n",
      "epoch:32 step:30326 [D loss: 0.563461, acc.: 71.88%] [G loss: 1.617079]\n",
      "epoch:32 step:30327 [D loss: 0.479673, acc.: 82.03%] [G loss: 1.932468]\n",
      "epoch:32 step:30328 [D loss: 0.508593, acc.: 77.34%] [G loss: 1.305378]\n",
      "epoch:32 step:30329 [D loss: 0.434397, acc.: 80.47%] [G loss: 1.516968]\n",
      "epoch:32 step:30330 [D loss: 0.606602, acc.: 65.62%] [G loss: 1.651312]\n",
      "epoch:32 step:30331 [D loss: 0.612929, acc.: 62.50%] [G loss: 1.489203]\n",
      "epoch:32 step:30332 [D loss: 0.895031, acc.: 42.97%] [G loss: 1.242417]\n",
      "epoch:32 step:30333 [D loss: 0.473807, acc.: 78.12%] [G loss: 1.796756]\n",
      "epoch:32 step:30334 [D loss: 0.473448, acc.: 78.91%] [G loss: 1.732871]\n",
      "epoch:32 step:30335 [D loss: 0.470393, acc.: 77.34%] [G loss: 1.230274]\n",
      "epoch:32 step:30336 [D loss: 0.755367, acc.: 51.56%] [G loss: 1.205294]\n",
      "epoch:32 step:30337 [D loss: 0.457459, acc.: 82.03%] [G loss: 1.867288]\n",
      "epoch:32 step:30338 [D loss: 0.535352, acc.: 71.09%] [G loss: 1.837545]\n",
      "epoch:32 step:30339 [D loss: 0.546949, acc.: 74.22%] [G loss: 1.285355]\n",
      "epoch:32 step:30340 [D loss: 0.538058, acc.: 71.88%] [G loss: 1.743011]\n",
      "epoch:32 step:30341 [D loss: 0.608073, acc.: 65.62%] [G loss: 1.327554]\n",
      "epoch:32 step:30342 [D loss: 0.552855, acc.: 71.09%] [G loss: 1.258501]\n",
      "epoch:32 step:30343 [D loss: 0.567940, acc.: 68.75%] [G loss: 1.629478]\n",
      "epoch:32 step:30344 [D loss: 0.375761, acc.: 84.38%] [G loss: 1.632709]\n",
      "epoch:32 step:30345 [D loss: 0.406970, acc.: 83.59%] [G loss: 1.712150]\n",
      "epoch:32 step:30346 [D loss: 0.517708, acc.: 75.00%] [G loss: 1.057541]\n",
      "epoch:32 step:30347 [D loss: 0.505248, acc.: 72.66%] [G loss: 1.179898]\n",
      "epoch:32 step:30348 [D loss: 0.476956, acc.: 82.81%] [G loss: 1.329889]\n",
      "epoch:32 step:30349 [D loss: 0.635175, acc.: 60.94%] [G loss: 1.372445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30350 [D loss: 0.407618, acc.: 84.38%] [G loss: 1.343464]\n",
      "epoch:32 step:30351 [D loss: 0.497878, acc.: 78.12%] [G loss: 1.385670]\n",
      "epoch:32 step:30352 [D loss: 0.429663, acc.: 85.94%] [G loss: 1.339586]\n",
      "epoch:32 step:30353 [D loss: 0.755613, acc.: 56.25%] [G loss: 1.120199]\n",
      "epoch:32 step:30354 [D loss: 0.561982, acc.: 75.00%] [G loss: 1.782864]\n",
      "epoch:32 step:30355 [D loss: 0.593710, acc.: 65.62%] [G loss: 1.805713]\n",
      "epoch:32 step:30356 [D loss: 0.347113, acc.: 87.50%] [G loss: 1.986211]\n",
      "epoch:32 step:30357 [D loss: 0.537513, acc.: 71.88%] [G loss: 1.243270]\n",
      "epoch:32 step:30358 [D loss: 0.559501, acc.: 71.09%] [G loss: 1.374601]\n",
      "epoch:32 step:30359 [D loss: 0.531241, acc.: 74.22%] [G loss: 1.674099]\n",
      "epoch:32 step:30360 [D loss: 0.439783, acc.: 84.38%] [G loss: 1.082117]\n",
      "epoch:32 step:30361 [D loss: 0.540113, acc.: 74.22%] [G loss: 1.341876]\n",
      "epoch:32 step:30362 [D loss: 0.403072, acc.: 83.59%] [G loss: 1.576066]\n",
      "epoch:32 step:30363 [D loss: 0.465692, acc.: 78.12%] [G loss: 1.372092]\n",
      "epoch:32 step:30364 [D loss: 0.544264, acc.: 67.19%] [G loss: 1.543020]\n",
      "epoch:32 step:30365 [D loss: 0.531597, acc.: 72.66%] [G loss: 1.077685]\n",
      "epoch:32 step:30366 [D loss: 0.419959, acc.: 85.16%] [G loss: 1.172333]\n",
      "epoch:32 step:30367 [D loss: 0.412065, acc.: 87.50%] [G loss: 1.339143]\n",
      "epoch:32 step:30368 [D loss: 0.602267, acc.: 71.88%] [G loss: 1.013526]\n",
      "epoch:32 step:30369 [D loss: 0.499999, acc.: 78.91%] [G loss: 1.643977]\n",
      "epoch:32 step:30370 [D loss: 0.521640, acc.: 71.88%] [G loss: 1.624671]\n",
      "epoch:32 step:30371 [D loss: 0.497540, acc.: 73.44%] [G loss: 1.406404]\n",
      "epoch:32 step:30372 [D loss: 0.599649, acc.: 65.62%] [G loss: 1.755545]\n",
      "epoch:32 step:30373 [D loss: 0.474427, acc.: 78.12%] [G loss: 1.226928]\n",
      "epoch:32 step:30374 [D loss: 0.497906, acc.: 75.00%] [G loss: 1.211529]\n",
      "epoch:32 step:30375 [D loss: 0.568145, acc.: 71.88%] [G loss: 1.450693]\n",
      "epoch:32 step:30376 [D loss: 0.546471, acc.: 72.66%] [G loss: 1.176775]\n",
      "epoch:32 step:30377 [D loss: 0.568672, acc.: 66.41%] [G loss: 1.596300]\n",
      "epoch:32 step:30378 [D loss: 0.638120, acc.: 64.84%] [G loss: 1.266720]\n",
      "epoch:32 step:30379 [D loss: 0.315472, acc.: 89.06%] [G loss: 2.182078]\n",
      "epoch:32 step:30380 [D loss: 0.660766, acc.: 59.38%] [G loss: 1.780709]\n",
      "epoch:32 step:30381 [D loss: 0.518785, acc.: 78.12%] [G loss: 1.431373]\n",
      "epoch:32 step:30382 [D loss: 0.477580, acc.: 79.69%] [G loss: 1.672275]\n",
      "epoch:32 step:30383 [D loss: 0.665025, acc.: 60.94%] [G loss: 1.527694]\n",
      "epoch:32 step:30384 [D loss: 0.703865, acc.: 57.81%] [G loss: 1.324998]\n",
      "epoch:32 step:30385 [D loss: 0.419123, acc.: 85.94%] [G loss: 1.632678]\n",
      "epoch:32 step:30386 [D loss: 0.471210, acc.: 75.78%] [G loss: 1.931449]\n",
      "epoch:32 step:30387 [D loss: 0.625769, acc.: 64.06%] [G loss: 1.653087]\n",
      "epoch:32 step:30388 [D loss: 0.574553, acc.: 73.44%] [G loss: 1.468383]\n",
      "epoch:32 step:30389 [D loss: 0.476795, acc.: 78.12%] [G loss: 1.628779]\n",
      "epoch:32 step:30390 [D loss: 0.426662, acc.: 84.38%] [G loss: 1.775314]\n",
      "epoch:32 step:30391 [D loss: 0.585397, acc.: 72.66%] [G loss: 1.426107]\n",
      "epoch:32 step:30392 [D loss: 0.508807, acc.: 75.78%] [G loss: 1.031266]\n",
      "epoch:32 step:30393 [D loss: 0.454017, acc.: 80.47%] [G loss: 1.202208]\n",
      "epoch:32 step:30394 [D loss: 0.514922, acc.: 73.44%] [G loss: 1.236973]\n",
      "epoch:32 step:30395 [D loss: 0.413021, acc.: 81.25%] [G loss: 1.475574]\n",
      "epoch:32 step:30396 [D loss: 0.728172, acc.: 60.16%] [G loss: 1.281590]\n",
      "epoch:32 step:30397 [D loss: 0.491377, acc.: 78.12%] [G loss: 1.169200]\n",
      "epoch:32 step:30398 [D loss: 0.576054, acc.: 67.19%] [G loss: 1.713922]\n",
      "epoch:32 step:30399 [D loss: 0.387010, acc.: 82.03%] [G loss: 1.532660]\n",
      "epoch:32 step:30400 [D loss: 0.570983, acc.: 75.78%] [G loss: 1.551334]\n",
      "##############\n",
      "[2.85288627 2.0653506  2.01933236 3.00983127 0.88011065 6.22591723\n",
      " 2.28981604 2.66837352 4.00440947 8.14868929]\n",
      "##########\n",
      "epoch:32 step:30401 [D loss: 0.464318, acc.: 79.69%] [G loss: 1.571237]\n",
      "epoch:32 step:30402 [D loss: 0.672664, acc.: 62.50%] [G loss: 1.070338]\n",
      "epoch:32 step:30403 [D loss: 0.468884, acc.: 75.00%] [G loss: 1.388049]\n",
      "epoch:32 step:30404 [D loss: 0.527996, acc.: 76.56%] [G loss: 1.553651]\n",
      "epoch:32 step:30405 [D loss: 0.519132, acc.: 73.44%] [G loss: 1.265945]\n",
      "epoch:32 step:30406 [D loss: 0.589893, acc.: 71.09%] [G loss: 1.234367]\n",
      "epoch:32 step:30407 [D loss: 0.483046, acc.: 80.47%] [G loss: 1.474399]\n",
      "epoch:32 step:30408 [D loss: 0.574148, acc.: 71.09%] [G loss: 1.263188]\n",
      "epoch:32 step:30409 [D loss: 0.641312, acc.: 63.28%] [G loss: 1.589413]\n",
      "epoch:32 step:30410 [D loss: 0.501364, acc.: 77.34%] [G loss: 1.770629]\n",
      "epoch:32 step:30411 [D loss: 0.434305, acc.: 77.34%] [G loss: 1.598456]\n",
      "epoch:32 step:30412 [D loss: 0.603775, acc.: 72.66%] [G loss: 1.551752]\n",
      "epoch:32 step:30413 [D loss: 0.509775, acc.: 75.00%] [G loss: 1.231068]\n",
      "epoch:32 step:30414 [D loss: 0.763074, acc.: 57.81%] [G loss: 1.245898]\n",
      "epoch:32 step:30415 [D loss: 0.556720, acc.: 70.31%] [G loss: 1.265062]\n",
      "epoch:32 step:30416 [D loss: 0.557377, acc.: 71.09%] [G loss: 1.603996]\n",
      "epoch:32 step:30417 [D loss: 0.470840, acc.: 77.34%] [G loss: 2.250283]\n",
      "epoch:32 step:30418 [D loss: 0.569292, acc.: 69.53%] [G loss: 1.353896]\n",
      "epoch:32 step:30419 [D loss: 0.491616, acc.: 78.91%] [G loss: 1.442056]\n",
      "epoch:32 step:30420 [D loss: 0.441907, acc.: 76.56%] [G loss: 1.414143]\n",
      "epoch:32 step:30421 [D loss: 0.516105, acc.: 75.00%] [G loss: 1.752238]\n",
      "epoch:32 step:30422 [D loss: 0.515756, acc.: 76.56%] [G loss: 1.442311]\n",
      "epoch:32 step:30423 [D loss: 0.533233, acc.: 72.66%] [G loss: 1.553312]\n",
      "epoch:32 step:30424 [D loss: 0.588734, acc.: 67.97%] [G loss: 1.285570]\n",
      "epoch:32 step:30425 [D loss: 0.480306, acc.: 80.47%] [G loss: 1.550639]\n",
      "epoch:32 step:30426 [D loss: 0.500350, acc.: 75.00%] [G loss: 1.164068]\n",
      "epoch:32 step:30427 [D loss: 0.551830, acc.: 75.78%] [G loss: 1.803893]\n",
      "epoch:32 step:30428 [D loss: 0.641989, acc.: 65.62%] [G loss: 1.291004]\n",
      "epoch:32 step:30429 [D loss: 0.471749, acc.: 76.56%] [G loss: 1.250855]\n",
      "epoch:32 step:30430 [D loss: 0.500210, acc.: 75.78%] [G loss: 1.593404]\n",
      "epoch:32 step:30431 [D loss: 0.728001, acc.: 57.81%] [G loss: 1.289701]\n",
      "epoch:32 step:30432 [D loss: 0.591497, acc.: 73.44%] [G loss: 1.334096]\n",
      "epoch:32 step:30433 [D loss: 0.476330, acc.: 76.56%] [G loss: 1.440145]\n",
      "epoch:32 step:30434 [D loss: 0.494397, acc.: 76.56%] [G loss: 1.450881]\n",
      "epoch:32 step:30435 [D loss: 0.389186, acc.: 85.94%] [G loss: 1.176436]\n",
      "epoch:32 step:30436 [D loss: 0.601338, acc.: 65.62%] [G loss: 1.205620]\n",
      "epoch:32 step:30437 [D loss: 0.530272, acc.: 71.09%] [G loss: 1.653678]\n",
      "epoch:32 step:30438 [D loss: 0.634800, acc.: 60.94%] [G loss: 2.158226]\n",
      "epoch:32 step:30439 [D loss: 0.426983, acc.: 82.81%] [G loss: 1.873558]\n",
      "epoch:32 step:30440 [D loss: 0.716648, acc.: 57.03%] [G loss: 1.412275]\n",
      "epoch:32 step:30441 [D loss: 0.869831, acc.: 54.69%] [G loss: 1.140648]\n",
      "epoch:32 step:30442 [D loss: 0.620316, acc.: 67.97%] [G loss: 1.405651]\n",
      "epoch:32 step:30443 [D loss: 0.466518, acc.: 76.56%] [G loss: 1.564905]\n",
      "epoch:32 step:30444 [D loss: 0.385094, acc.: 87.50%] [G loss: 1.855101]\n",
      "epoch:32 step:30445 [D loss: 0.712529, acc.: 55.47%] [G loss: 1.111821]\n",
      "epoch:32 step:30446 [D loss: 0.636167, acc.: 61.72%] [G loss: 1.535353]\n",
      "epoch:32 step:30447 [D loss: 0.896487, acc.: 44.53%] [G loss: 1.304096]\n",
      "epoch:32 step:30448 [D loss: 0.596738, acc.: 67.19%] [G loss: 1.570053]\n",
      "epoch:32 step:30449 [D loss: 0.380322, acc.: 86.72%] [G loss: 1.690003]\n",
      "epoch:32 step:30450 [D loss: 0.466026, acc.: 78.91%] [G loss: 1.597565]\n",
      "epoch:32 step:30451 [D loss: 0.540499, acc.: 74.22%] [G loss: 1.156537]\n",
      "epoch:32 step:30452 [D loss: 0.546237, acc.: 73.44%] [G loss: 1.823232]\n",
      "epoch:32 step:30453 [D loss: 0.422305, acc.: 82.81%] [G loss: 1.739698]\n",
      "epoch:32 step:30454 [D loss: 0.736083, acc.: 59.38%] [G loss: 1.281142]\n",
      "epoch:32 step:30455 [D loss: 0.576568, acc.: 67.97%] [G loss: 1.717762]\n",
      "epoch:32 step:30456 [D loss: 0.588367, acc.: 74.22%] [G loss: 2.025415]\n",
      "epoch:32 step:30457 [D loss: 0.419532, acc.: 79.69%] [G loss: 1.416414]\n",
      "epoch:32 step:30458 [D loss: 0.518705, acc.: 74.22%] [G loss: 1.235123]\n",
      "epoch:32 step:30459 [D loss: 0.467755, acc.: 76.56%] [G loss: 1.579134]\n",
      "epoch:32 step:30460 [D loss: 0.689878, acc.: 60.94%] [G loss: 1.490415]\n",
      "epoch:32 step:30461 [D loss: 0.507856, acc.: 73.44%] [G loss: 2.052722]\n",
      "epoch:32 step:30462 [D loss: 0.511890, acc.: 77.34%] [G loss: 1.471765]\n",
      "epoch:32 step:30463 [D loss: 0.516386, acc.: 74.22%] [G loss: 1.571078]\n",
      "epoch:32 step:30464 [D loss: 0.493665, acc.: 77.34%] [G loss: 1.609320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30465 [D loss: 0.708700, acc.: 56.25%] [G loss: 1.571108]\n",
      "epoch:32 step:30466 [D loss: 0.508104, acc.: 76.56%] [G loss: 1.617229]\n",
      "epoch:32 step:30467 [D loss: 0.511245, acc.: 75.78%] [G loss: 1.303461]\n",
      "epoch:32 step:30468 [D loss: 0.526119, acc.: 72.66%] [G loss: 1.542381]\n",
      "epoch:32 step:30469 [D loss: 0.565813, acc.: 73.44%] [G loss: 1.219089]\n",
      "epoch:32 step:30470 [D loss: 0.717335, acc.: 58.59%] [G loss: 1.479996]\n",
      "epoch:32 step:30471 [D loss: 0.554897, acc.: 72.66%] [G loss: 1.303831]\n",
      "epoch:32 step:30472 [D loss: 0.451607, acc.: 81.25%] [G loss: 1.728912]\n",
      "epoch:32 step:30473 [D loss: 0.359987, acc.: 88.28%] [G loss: 1.406449]\n",
      "epoch:32 step:30474 [D loss: 0.704300, acc.: 64.06%] [G loss: 1.371847]\n",
      "epoch:32 step:30475 [D loss: 0.473718, acc.: 77.34%] [G loss: 2.122310]\n",
      "epoch:32 step:30476 [D loss: 0.644772, acc.: 65.62%] [G loss: 2.072669]\n",
      "epoch:32 step:30477 [D loss: 0.460340, acc.: 79.69%] [G loss: 1.625057]\n",
      "epoch:32 step:30478 [D loss: 0.656943, acc.: 60.94%] [G loss: 1.277649]\n",
      "epoch:32 step:30479 [D loss: 0.552480, acc.: 73.44%] [G loss: 1.631640]\n",
      "epoch:32 step:30480 [D loss: 0.572377, acc.: 73.44%] [G loss: 1.189571]\n",
      "epoch:32 step:30481 [D loss: 0.505314, acc.: 74.22%] [G loss: 1.669171]\n",
      "epoch:32 step:30482 [D loss: 0.429657, acc.: 85.16%] [G loss: 1.775389]\n",
      "epoch:32 step:30483 [D loss: 0.581871, acc.: 69.53%] [G loss: 1.353123]\n",
      "epoch:32 step:30484 [D loss: 0.644555, acc.: 63.28%] [G loss: 1.895144]\n",
      "epoch:32 step:30485 [D loss: 0.623912, acc.: 67.97%] [G loss: 1.743492]\n",
      "epoch:32 step:30486 [D loss: 0.432614, acc.: 78.91%] [G loss: 1.603697]\n",
      "epoch:32 step:30487 [D loss: 0.646950, acc.: 61.72%] [G loss: 1.502229]\n",
      "epoch:32 step:30488 [D loss: 0.572141, acc.: 65.62%] [G loss: 1.458738]\n",
      "epoch:32 step:30489 [D loss: 0.372546, acc.: 84.38%] [G loss: 1.596206]\n",
      "epoch:32 step:30490 [D loss: 0.489636, acc.: 78.12%] [G loss: 2.015338]\n",
      "epoch:32 step:30491 [D loss: 0.518255, acc.: 75.78%] [G loss: 2.090859]\n",
      "epoch:32 step:30492 [D loss: 0.453264, acc.: 82.03%] [G loss: 1.732798]\n",
      "epoch:32 step:30493 [D loss: 0.615074, acc.: 70.31%] [G loss: 1.822477]\n",
      "epoch:32 step:30494 [D loss: 0.542612, acc.: 71.88%] [G loss: 1.701156]\n",
      "epoch:32 step:30495 [D loss: 0.613247, acc.: 72.66%] [G loss: 1.557913]\n",
      "epoch:32 step:30496 [D loss: 0.583182, acc.: 64.84%] [G loss: 1.348671]\n",
      "epoch:32 step:30497 [D loss: 0.695518, acc.: 57.03%] [G loss: 1.154767]\n",
      "epoch:32 step:30498 [D loss: 0.376563, acc.: 89.06%] [G loss: 1.622444]\n",
      "epoch:32 step:30499 [D loss: 0.638106, acc.: 69.53%] [G loss: 1.440954]\n",
      "epoch:32 step:30500 [D loss: 0.499162, acc.: 77.34%] [G loss: 1.745923]\n",
      "epoch:32 step:30501 [D loss: 0.372790, acc.: 85.16%] [G loss: 1.401929]\n",
      "epoch:32 step:30502 [D loss: 0.695130, acc.: 60.94%] [G loss: 1.574227]\n",
      "epoch:32 step:30503 [D loss: 0.594065, acc.: 68.75%] [G loss: 1.700524]\n",
      "epoch:32 step:30504 [D loss: 0.543386, acc.: 72.66%] [G loss: 1.832085]\n",
      "epoch:32 step:30505 [D loss: 0.478498, acc.: 75.00%] [G loss: 1.753938]\n",
      "epoch:32 step:30506 [D loss: 0.356304, acc.: 89.84%] [G loss: 1.423482]\n",
      "epoch:32 step:30507 [D loss: 0.455202, acc.: 77.34%] [G loss: 1.218866]\n",
      "epoch:32 step:30508 [D loss: 0.490946, acc.: 75.78%] [G loss: 1.436197]\n",
      "epoch:32 step:30509 [D loss: 0.391041, acc.: 82.81%] [G loss: 1.723937]\n",
      "epoch:32 step:30510 [D loss: 0.513899, acc.: 78.12%] [G loss: 1.517151]\n",
      "epoch:32 step:30511 [D loss: 0.559520, acc.: 70.31%] [G loss: 2.060872]\n",
      "epoch:32 step:30512 [D loss: 0.602487, acc.: 66.41%] [G loss: 1.372684]\n",
      "epoch:32 step:30513 [D loss: 0.422700, acc.: 82.81%] [G loss: 1.925882]\n",
      "epoch:32 step:30514 [D loss: 0.524348, acc.: 74.22%] [G loss: 1.426034]\n",
      "epoch:32 step:30515 [D loss: 0.623670, acc.: 67.19%] [G loss: 1.135438]\n",
      "epoch:32 step:30516 [D loss: 0.693537, acc.: 65.62%] [G loss: 1.486127]\n",
      "epoch:32 step:30517 [D loss: 0.667837, acc.: 64.06%] [G loss: 1.491470]\n",
      "epoch:32 step:30518 [D loss: 0.341199, acc.: 89.06%] [G loss: 1.395590]\n",
      "epoch:32 step:30519 [D loss: 0.523127, acc.: 76.56%] [G loss: 1.489766]\n",
      "epoch:32 step:30520 [D loss: 0.730893, acc.: 57.03%] [G loss: 1.147732]\n",
      "epoch:32 step:30521 [D loss: 0.479328, acc.: 80.47%] [G loss: 1.355314]\n",
      "epoch:32 step:30522 [D loss: 0.362732, acc.: 89.84%] [G loss: 2.087307]\n",
      "epoch:32 step:30523 [D loss: 0.723776, acc.: 57.03%] [G loss: 1.259794]\n",
      "epoch:32 step:30524 [D loss: 0.269880, acc.: 92.97%] [G loss: 1.479576]\n",
      "epoch:32 step:30525 [D loss: 0.552758, acc.: 71.88%] [G loss: 2.024491]\n",
      "epoch:32 step:30526 [D loss: 0.560106, acc.: 72.66%] [G loss: 1.807413]\n",
      "epoch:32 step:30527 [D loss: 0.431119, acc.: 81.25%] [G loss: 1.447923]\n",
      "epoch:32 step:30528 [D loss: 0.661092, acc.: 61.72%] [G loss: 1.459845]\n",
      "epoch:32 step:30529 [D loss: 0.464651, acc.: 80.47%] [G loss: 1.615632]\n",
      "epoch:32 step:30530 [D loss: 0.727184, acc.: 53.12%] [G loss: 1.520544]\n",
      "epoch:32 step:30531 [D loss: 0.448574, acc.: 80.47%] [G loss: 1.782973]\n",
      "epoch:32 step:30532 [D loss: 0.549007, acc.: 69.53%] [G loss: 1.527558]\n",
      "epoch:32 step:30533 [D loss: 0.721480, acc.: 63.28%] [G loss: 1.559973]\n",
      "epoch:32 step:30534 [D loss: 0.602985, acc.: 65.62%] [G loss: 1.627845]\n",
      "epoch:32 step:30535 [D loss: 0.702353, acc.: 60.94%] [G loss: 1.171473]\n",
      "epoch:32 step:30536 [D loss: 0.497394, acc.: 75.78%] [G loss: 1.466995]\n",
      "epoch:32 step:30537 [D loss: 0.771068, acc.: 54.69%] [G loss: 1.314623]\n",
      "epoch:32 step:30538 [D loss: 0.592525, acc.: 67.97%] [G loss: 1.325519]\n",
      "epoch:32 step:30539 [D loss: 0.530544, acc.: 73.44%] [G loss: 1.394759]\n",
      "epoch:32 step:30540 [D loss: 0.510249, acc.: 75.78%] [G loss: 1.884256]\n",
      "epoch:32 step:30541 [D loss: 0.563007, acc.: 72.66%] [G loss: 1.278640]\n",
      "epoch:32 step:30542 [D loss: 0.545917, acc.: 69.53%] [G loss: 0.976211]\n",
      "epoch:32 step:30543 [D loss: 0.734499, acc.: 58.59%] [G loss: 1.467139]\n",
      "epoch:32 step:30544 [D loss: 0.499614, acc.: 75.78%] [G loss: 1.815309]\n",
      "epoch:32 step:30545 [D loss: 0.568353, acc.: 71.09%] [G loss: 1.750239]\n",
      "epoch:32 step:30546 [D loss: 0.380915, acc.: 85.16%] [G loss: 1.412516]\n",
      "epoch:32 step:30547 [D loss: 0.576245, acc.: 67.19%] [G loss: 1.087986]\n",
      "epoch:32 step:30548 [D loss: 0.358606, acc.: 85.94%] [G loss: 1.401800]\n",
      "epoch:32 step:30549 [D loss: 0.587322, acc.: 65.62%] [G loss: 1.491364]\n",
      "epoch:32 step:30550 [D loss: 0.600189, acc.: 69.53%] [G loss: 1.450789]\n",
      "epoch:32 step:30551 [D loss: 0.376850, acc.: 86.72%] [G loss: 1.944172]\n",
      "epoch:32 step:30552 [D loss: 0.721107, acc.: 60.94%] [G loss: 1.038482]\n",
      "epoch:32 step:30553 [D loss: 0.593056, acc.: 70.31%] [G loss: 1.511578]\n",
      "epoch:32 step:30554 [D loss: 0.442480, acc.: 78.91%] [G loss: 1.875959]\n",
      "epoch:32 step:30555 [D loss: 0.429676, acc.: 83.59%] [G loss: 1.467620]\n",
      "epoch:32 step:30556 [D loss: 0.444653, acc.: 83.59%] [G loss: 1.461570]\n",
      "epoch:32 step:30557 [D loss: 0.411183, acc.: 85.16%] [G loss: 1.657495]\n",
      "epoch:32 step:30558 [D loss: 0.563690, acc.: 69.53%] [G loss: 1.303550]\n",
      "epoch:32 step:30559 [D loss: 0.473429, acc.: 75.78%] [G loss: 1.670528]\n",
      "epoch:32 step:30560 [D loss: 0.379633, acc.: 85.16%] [G loss: 1.180537]\n",
      "epoch:32 step:30561 [D loss: 0.530256, acc.: 75.00%] [G loss: 1.724476]\n",
      "epoch:32 step:30562 [D loss: 0.572054, acc.: 66.41%] [G loss: 1.296927]\n",
      "epoch:32 step:30563 [D loss: 0.493551, acc.: 77.34%] [G loss: 1.368597]\n",
      "epoch:32 step:30564 [D loss: 0.317113, acc.: 89.06%] [G loss: 1.377014]\n",
      "epoch:32 step:30565 [D loss: 0.471281, acc.: 80.47%] [G loss: 1.560008]\n",
      "epoch:32 step:30566 [D loss: 0.691104, acc.: 64.84%] [G loss: 1.575475]\n",
      "epoch:32 step:30567 [D loss: 0.475220, acc.: 78.12%] [G loss: 1.633629]\n",
      "epoch:32 step:30568 [D loss: 0.636815, acc.: 67.19%] [G loss: 1.707190]\n",
      "epoch:32 step:30569 [D loss: 0.466940, acc.: 80.47%] [G loss: 2.040366]\n",
      "epoch:32 step:30570 [D loss: 0.472711, acc.: 76.56%] [G loss: 1.443590]\n",
      "epoch:32 step:30571 [D loss: 0.577484, acc.: 69.53%] [G loss: 1.115744]\n",
      "epoch:32 step:30572 [D loss: 0.578451, acc.: 74.22%] [G loss: 2.055531]\n",
      "epoch:32 step:30573 [D loss: 0.604473, acc.: 68.75%] [G loss: 1.456565]\n",
      "epoch:32 step:30574 [D loss: 0.573878, acc.: 73.44%] [G loss: 0.873550]\n",
      "epoch:32 step:30575 [D loss: 0.433686, acc.: 82.03%] [G loss: 1.602660]\n",
      "epoch:32 step:30576 [D loss: 0.399317, acc.: 84.38%] [G loss: 1.629924]\n",
      "epoch:32 step:30577 [D loss: 0.666177, acc.: 64.84%] [G loss: 1.093547]\n",
      "epoch:32 step:30578 [D loss: 0.609820, acc.: 73.44%] [G loss: 1.545449]\n",
      "epoch:32 step:30579 [D loss: 0.622782, acc.: 63.28%] [G loss: 1.745557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30580 [D loss: 0.407079, acc.: 85.16%] [G loss: 1.727712]\n",
      "epoch:32 step:30581 [D loss: 0.620479, acc.: 64.06%] [G loss: 1.206049]\n",
      "epoch:32 step:30582 [D loss: 0.541827, acc.: 71.88%] [G loss: 1.464264]\n",
      "epoch:32 step:30583 [D loss: 0.342078, acc.: 87.50%] [G loss: 1.644569]\n",
      "epoch:32 step:30584 [D loss: 0.349429, acc.: 86.72%] [G loss: 1.491776]\n",
      "epoch:32 step:30585 [D loss: 0.912198, acc.: 47.66%] [G loss: 1.555941]\n",
      "epoch:32 step:30586 [D loss: 0.478490, acc.: 78.12%] [G loss: 1.498634]\n",
      "epoch:32 step:30587 [D loss: 0.564403, acc.: 71.09%] [G loss: 1.305360]\n",
      "epoch:32 step:30588 [D loss: 0.636358, acc.: 68.75%] [G loss: 1.285370]\n",
      "epoch:32 step:30589 [D loss: 0.520505, acc.: 69.53%] [G loss: 1.438836]\n",
      "epoch:32 step:30590 [D loss: 0.447309, acc.: 80.47%] [G loss: 1.755412]\n",
      "epoch:32 step:30591 [D loss: 0.539278, acc.: 68.75%] [G loss: 1.374120]\n",
      "epoch:32 step:30592 [D loss: 0.599145, acc.: 68.75%] [G loss: 1.207696]\n",
      "epoch:32 step:30593 [D loss: 0.708474, acc.: 56.25%] [G loss: 1.055198]\n",
      "epoch:32 step:30594 [D loss: 0.561695, acc.: 67.97%] [G loss: 1.567005]\n",
      "epoch:32 step:30595 [D loss: 0.441095, acc.: 81.25%] [G loss: 1.756693]\n",
      "epoch:32 step:30596 [D loss: 0.804222, acc.: 53.12%] [G loss: 1.155345]\n",
      "epoch:32 step:30597 [D loss: 0.467062, acc.: 79.69%] [G loss: 1.603792]\n",
      "epoch:32 step:30598 [D loss: 0.453336, acc.: 77.34%] [G loss: 1.239316]\n",
      "epoch:32 step:30599 [D loss: 0.579367, acc.: 71.09%] [G loss: 1.186581]\n",
      "epoch:32 step:30600 [D loss: 0.434277, acc.: 80.47%] [G loss: 1.409646]\n",
      "##############\n",
      "[2.76021023 1.91424683 1.87199242 2.89185472 0.66853512 6.41741959\n",
      " 2.38318185 2.71011771 3.84198008 5.10320975]\n",
      "##########\n",
      "epoch:32 step:30601 [D loss: 0.430667, acc.: 82.03%] [G loss: 1.789730]\n",
      "epoch:32 step:30602 [D loss: 0.533174, acc.: 75.78%] [G loss: 1.298131]\n",
      "epoch:32 step:30603 [D loss: 0.573763, acc.: 70.31%] [G loss: 1.440632]\n",
      "epoch:32 step:30604 [D loss: 0.567291, acc.: 70.31%] [G loss: 1.322241]\n",
      "epoch:32 step:30605 [D loss: 0.574311, acc.: 71.88%] [G loss: 1.288575]\n",
      "epoch:32 step:30606 [D loss: 0.502760, acc.: 76.56%] [G loss: 1.668967]\n",
      "epoch:32 step:30607 [D loss: 0.612182, acc.: 65.62%] [G loss: 1.444012]\n",
      "epoch:32 step:30608 [D loss: 0.457581, acc.: 79.69%] [G loss: 1.614207]\n",
      "epoch:32 step:30609 [D loss: 0.587383, acc.: 67.97%] [G loss: 1.514144]\n",
      "epoch:32 step:30610 [D loss: 0.353942, acc.: 89.06%] [G loss: 1.703159]\n",
      "epoch:32 step:30611 [D loss: 0.520409, acc.: 76.56%] [G loss: 1.299296]\n",
      "epoch:32 step:30612 [D loss: 0.624274, acc.: 71.88%] [G loss: 1.492259]\n",
      "epoch:32 step:30613 [D loss: 0.373825, acc.: 86.72%] [G loss: 2.015290]\n",
      "epoch:32 step:30614 [D loss: 0.654401, acc.: 62.50%] [G loss: 1.179524]\n",
      "epoch:32 step:30615 [D loss: 0.493433, acc.: 75.78%] [G loss: 1.524782]\n",
      "epoch:32 step:30616 [D loss: 0.605642, acc.: 66.41%] [G loss: 1.206019]\n",
      "epoch:32 step:30617 [D loss: 0.602201, acc.: 66.41%] [G loss: 1.602359]\n",
      "epoch:32 step:30618 [D loss: 0.546711, acc.: 75.00%] [G loss: 1.449563]\n",
      "epoch:32 step:30619 [D loss: 0.486284, acc.: 78.12%] [G loss: 1.283741]\n",
      "epoch:32 step:30620 [D loss: 0.506980, acc.: 77.34%] [G loss: 1.845870]\n",
      "epoch:32 step:30621 [D loss: 0.618007, acc.: 68.75%] [G loss: 1.436936]\n",
      "epoch:32 step:30622 [D loss: 0.485095, acc.: 74.22%] [G loss: 1.297448]\n",
      "epoch:32 step:30623 [D loss: 0.588668, acc.: 67.97%] [G loss: 1.416332]\n",
      "epoch:32 step:30624 [D loss: 0.450490, acc.: 78.91%] [G loss: 1.754166]\n",
      "epoch:32 step:30625 [D loss: 0.661828, acc.: 61.72%] [G loss: 1.213624]\n",
      "epoch:32 step:30626 [D loss: 0.491541, acc.: 76.56%] [G loss: 1.437176]\n",
      "epoch:32 step:30627 [D loss: 0.823540, acc.: 53.91%] [G loss: 1.478947]\n",
      "epoch:32 step:30628 [D loss: 0.598473, acc.: 68.75%] [G loss: 1.299726]\n",
      "epoch:32 step:30629 [D loss: 0.404400, acc.: 86.72%] [G loss: 1.806013]\n",
      "epoch:32 step:30630 [D loss: 0.777668, acc.: 54.69%] [G loss: 1.510990]\n",
      "epoch:32 step:30631 [D loss: 0.602250, acc.: 64.84%] [G loss: 1.418957]\n",
      "epoch:32 step:30632 [D loss: 0.429589, acc.: 82.81%] [G loss: 1.627153]\n",
      "epoch:32 step:30633 [D loss: 0.705332, acc.: 61.72%] [G loss: 1.331700]\n",
      "epoch:32 step:30634 [D loss: 0.769977, acc.: 54.69%] [G loss: 1.424352]\n",
      "epoch:32 step:30635 [D loss: 0.286627, acc.: 90.62%] [G loss: 1.394534]\n",
      "epoch:32 step:30636 [D loss: 0.690891, acc.: 63.28%] [G loss: 1.441833]\n",
      "epoch:32 step:30637 [D loss: 0.510643, acc.: 76.56%] [G loss: 1.462105]\n",
      "epoch:32 step:30638 [D loss: 0.668145, acc.: 61.72%] [G loss: 1.386366]\n",
      "epoch:32 step:30639 [D loss: 0.767191, acc.: 51.56%] [G loss: 1.329270]\n",
      "epoch:32 step:30640 [D loss: 0.274013, acc.: 92.19%] [G loss: 1.933025]\n",
      "epoch:32 step:30641 [D loss: 0.789434, acc.: 53.12%] [G loss: 1.200269]\n",
      "epoch:32 step:30642 [D loss: 0.526972, acc.: 75.78%] [G loss: 1.749933]\n",
      "epoch:32 step:30643 [D loss: 0.430165, acc.: 83.59%] [G loss: 1.529663]\n",
      "epoch:32 step:30644 [D loss: 0.613766, acc.: 68.75%] [G loss: 1.370402]\n",
      "epoch:32 step:30645 [D loss: 0.448414, acc.: 76.56%] [G loss: 1.523601]\n",
      "epoch:32 step:30646 [D loss: 0.569443, acc.: 70.31%] [G loss: 1.578933]\n",
      "epoch:32 step:30647 [D loss: 0.756462, acc.: 54.69%] [G loss: 1.137764]\n",
      "epoch:32 step:30648 [D loss: 0.564516, acc.: 71.88%] [G loss: 1.597124]\n",
      "epoch:32 step:30649 [D loss: 0.760691, acc.: 55.47%] [G loss: 1.034065]\n",
      "epoch:32 step:30650 [D loss: 0.499497, acc.: 75.78%] [G loss: 1.249181]\n",
      "epoch:32 step:30651 [D loss: 0.764656, acc.: 59.38%] [G loss: 1.167223]\n",
      "epoch:32 step:30652 [D loss: 0.576570, acc.: 68.75%] [G loss: 2.026041]\n",
      "epoch:32 step:30653 [D loss: 0.567183, acc.: 70.31%] [G loss: 1.329329]\n",
      "epoch:32 step:30654 [D loss: 0.551916, acc.: 74.22%] [G loss: 1.742564]\n",
      "epoch:32 step:30655 [D loss: 0.484340, acc.: 75.00%] [G loss: 1.529322]\n",
      "epoch:32 step:30656 [D loss: 0.336428, acc.: 85.94%] [G loss: 1.825299]\n",
      "epoch:32 step:30657 [D loss: 0.593959, acc.: 71.88%] [G loss: 1.303682]\n",
      "epoch:32 step:30658 [D loss: 0.394599, acc.: 85.94%] [G loss: 1.870388]\n",
      "epoch:32 step:30659 [D loss: 0.583161, acc.: 73.44%] [G loss: 1.413228]\n",
      "epoch:32 step:30660 [D loss: 0.660078, acc.: 66.41%] [G loss: 1.109718]\n",
      "epoch:32 step:30661 [D loss: 0.363137, acc.: 87.50%] [G loss: 1.491790]\n",
      "epoch:32 step:30662 [D loss: 0.551400, acc.: 71.88%] [G loss: 1.907400]\n",
      "epoch:32 step:30663 [D loss: 0.505488, acc.: 76.56%] [G loss: 1.282146]\n",
      "epoch:32 step:30664 [D loss: 0.536238, acc.: 71.88%] [G loss: 1.108651]\n",
      "epoch:32 step:30665 [D loss: 0.496270, acc.: 75.78%] [G loss: 1.542068]\n",
      "epoch:32 step:30666 [D loss: 0.487515, acc.: 78.91%] [G loss: 1.413078]\n",
      "epoch:32 step:30667 [D loss: 0.652119, acc.: 66.41%] [G loss: 1.404986]\n",
      "epoch:32 step:30668 [D loss: 0.645443, acc.: 62.50%] [G loss: 1.499290]\n",
      "epoch:32 step:30669 [D loss: 0.531360, acc.: 70.31%] [G loss: 1.639316]\n",
      "epoch:32 step:30670 [D loss: 0.423160, acc.: 83.59%] [G loss: 1.589200]\n",
      "epoch:32 step:30671 [D loss: 0.494767, acc.: 74.22%] [G loss: 1.599985]\n",
      "epoch:32 step:30672 [D loss: 0.540470, acc.: 70.31%] [G loss: 1.647273]\n",
      "epoch:32 step:30673 [D loss: 0.600116, acc.: 67.19%] [G loss: 1.117313]\n",
      "epoch:32 step:30674 [D loss: 0.499352, acc.: 76.56%] [G loss: 1.812853]\n",
      "epoch:32 step:30675 [D loss: 0.521646, acc.: 75.78%] [G loss: 1.174426]\n",
      "epoch:32 step:30676 [D loss: 0.410079, acc.: 82.81%] [G loss: 1.395190]\n",
      "epoch:32 step:30677 [D loss: 0.563429, acc.: 71.09%] [G loss: 1.318995]\n",
      "epoch:32 step:30678 [D loss: 0.556633, acc.: 69.53%] [G loss: 1.561016]\n",
      "epoch:32 step:30679 [D loss: 0.709346, acc.: 60.16%] [G loss: 1.406733]\n",
      "epoch:32 step:30680 [D loss: 0.336568, acc.: 94.53%] [G loss: 1.625821]\n",
      "epoch:32 step:30681 [D loss: 0.440385, acc.: 82.81%] [G loss: 1.541484]\n",
      "epoch:32 step:30682 [D loss: 0.497008, acc.: 81.25%] [G loss: 0.993123]\n",
      "epoch:32 step:30683 [D loss: 0.549164, acc.: 71.88%] [G loss: 1.025740]\n",
      "epoch:32 step:30684 [D loss: 0.637527, acc.: 66.41%] [G loss: 1.226016]\n",
      "epoch:32 step:30685 [D loss: 0.286727, acc.: 90.62%] [G loss: 1.805540]\n",
      "epoch:32 step:30686 [D loss: 0.535389, acc.: 69.53%] [G loss: 1.692915]\n",
      "epoch:32 step:30687 [D loss: 0.602821, acc.: 67.19%] [G loss: 1.243452]\n",
      "epoch:32 step:30688 [D loss: 0.799953, acc.: 54.69%] [G loss: 1.462676]\n",
      "epoch:32 step:30689 [D loss: 0.352647, acc.: 88.28%] [G loss: 1.264574]\n",
      "epoch:32 step:30690 [D loss: 0.707169, acc.: 63.28%] [G loss: 1.256004]\n",
      "epoch:32 step:30691 [D loss: 0.536239, acc.: 71.88%] [G loss: 1.545308]\n",
      "epoch:32 step:30692 [D loss: 0.412401, acc.: 82.81%] [G loss: 1.967553]\n",
      "epoch:32 step:30693 [D loss: 0.714500, acc.: 62.50%] [G loss: 2.047084]\n",
      "epoch:32 step:30694 [D loss: 0.526793, acc.: 75.00%] [G loss: 1.511913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30695 [D loss: 0.607398, acc.: 69.53%] [G loss: 1.506677]\n",
      "epoch:32 step:30696 [D loss: 0.561263, acc.: 71.09%] [G loss: 1.318696]\n",
      "epoch:32 step:30697 [D loss: 0.535811, acc.: 75.78%] [G loss: 1.382124]\n",
      "epoch:32 step:30698 [D loss: 0.611964, acc.: 66.41%] [G loss: 1.726972]\n",
      "epoch:32 step:30699 [D loss: 0.593225, acc.: 67.19%] [G loss: 1.384105]\n",
      "epoch:32 step:30700 [D loss: 0.494087, acc.: 75.78%] [G loss: 1.630215]\n",
      "epoch:32 step:30701 [D loss: 0.424417, acc.: 83.59%] [G loss: 1.447352]\n",
      "epoch:32 step:30702 [D loss: 0.294314, acc.: 92.97%] [G loss: 2.036999]\n",
      "epoch:32 step:30703 [D loss: 0.843087, acc.: 49.22%] [G loss: 1.134695]\n",
      "epoch:32 step:30704 [D loss: 0.351275, acc.: 88.28%] [G loss: 1.774278]\n",
      "epoch:32 step:30705 [D loss: 0.587896, acc.: 71.09%] [G loss: 1.749166]\n",
      "epoch:32 step:30706 [D loss: 0.612098, acc.: 64.84%] [G loss: 1.372458]\n",
      "epoch:32 step:30707 [D loss: 0.467086, acc.: 80.47%] [G loss: 1.973783]\n",
      "epoch:32 step:30708 [D loss: 0.448407, acc.: 78.91%] [G loss: 1.108760]\n",
      "epoch:32 step:30709 [D loss: 0.472193, acc.: 82.03%] [G loss: 1.384995]\n",
      "epoch:32 step:30710 [D loss: 0.407024, acc.: 82.03%] [G loss: 1.522431]\n",
      "epoch:32 step:30711 [D loss: 0.645707, acc.: 61.72%] [G loss: 1.834138]\n",
      "epoch:32 step:30712 [D loss: 0.643063, acc.: 66.41%] [G loss: 1.604634]\n",
      "epoch:32 step:30713 [D loss: 0.436461, acc.: 79.69%] [G loss: 1.584366]\n",
      "epoch:32 step:30714 [D loss: 0.709191, acc.: 57.81%] [G loss: 1.406842]\n",
      "epoch:32 step:30715 [D loss: 0.750690, acc.: 60.16%] [G loss: 1.226767]\n",
      "epoch:32 step:30716 [D loss: 0.385526, acc.: 82.81%] [G loss: 1.568755]\n",
      "epoch:32 step:30717 [D loss: 0.529864, acc.: 74.22%] [G loss: 1.352207]\n",
      "epoch:32 step:30718 [D loss: 0.357498, acc.: 88.28%] [G loss: 1.269078]\n",
      "epoch:32 step:30719 [D loss: 0.531077, acc.: 73.44%] [G loss: 1.326961]\n",
      "epoch:32 step:30720 [D loss: 0.574859, acc.: 71.88%] [G loss: 1.788022]\n",
      "epoch:32 step:30721 [D loss: 0.581887, acc.: 71.88%] [G loss: 1.410709]\n",
      "epoch:32 step:30722 [D loss: 0.689105, acc.: 59.38%] [G loss: 1.351731]\n",
      "epoch:32 step:30723 [D loss: 0.369995, acc.: 85.16%] [G loss: 1.745662]\n",
      "epoch:32 step:30724 [D loss: 0.509014, acc.: 75.00%] [G loss: 1.343014]\n",
      "epoch:32 step:30725 [D loss: 0.462374, acc.: 80.47%] [G loss: 1.223545]\n",
      "epoch:32 step:30726 [D loss: 0.405895, acc.: 86.72%] [G loss: 1.613049]\n",
      "epoch:32 step:30727 [D loss: 0.408341, acc.: 82.03%] [G loss: 1.580923]\n",
      "epoch:32 step:30728 [D loss: 0.495897, acc.: 78.12%] [G loss: 1.349138]\n",
      "epoch:32 step:30729 [D loss: 0.698470, acc.: 60.16%] [G loss: 1.554625]\n",
      "epoch:32 step:30730 [D loss: 0.771402, acc.: 55.47%] [G loss: 1.864274]\n",
      "epoch:32 step:30731 [D loss: 0.307835, acc.: 91.41%] [G loss: 1.740784]\n",
      "epoch:32 step:30732 [D loss: 0.373400, acc.: 84.38%] [G loss: 2.101463]\n",
      "epoch:32 step:30733 [D loss: 0.518974, acc.: 78.91%] [G loss: 1.640112]\n",
      "epoch:32 step:30734 [D loss: 0.470054, acc.: 77.34%] [G loss: 1.693800]\n",
      "epoch:32 step:30735 [D loss: 0.690692, acc.: 59.38%] [G loss: 1.053061]\n",
      "epoch:32 step:30736 [D loss: 0.502861, acc.: 75.78%] [G loss: 1.556433]\n",
      "epoch:32 step:30737 [D loss: 0.656577, acc.: 67.19%] [G loss: 1.337918]\n",
      "epoch:32 step:30738 [D loss: 0.494497, acc.: 75.78%] [G loss: 1.090646]\n",
      "epoch:32 step:30739 [D loss: 0.411980, acc.: 81.25%] [G loss: 1.788817]\n",
      "epoch:32 step:30740 [D loss: 0.686347, acc.: 57.81%] [G loss: 1.412132]\n",
      "epoch:32 step:30741 [D loss: 0.430802, acc.: 82.03%] [G loss: 1.387086]\n",
      "epoch:32 step:30742 [D loss: 0.661694, acc.: 66.41%] [G loss: 1.281090]\n",
      "epoch:32 step:30743 [D loss: 0.391782, acc.: 82.81%] [G loss: 1.237972]\n",
      "epoch:32 step:30744 [D loss: 0.483135, acc.: 78.91%] [G loss: 1.446960]\n",
      "epoch:32 step:30745 [D loss: 0.774482, acc.: 59.38%] [G loss: 1.658998]\n",
      "epoch:32 step:30746 [D loss: 0.853717, acc.: 49.22%] [G loss: 1.440914]\n",
      "epoch:32 step:30747 [D loss: 0.483487, acc.: 79.69%] [G loss: 1.350661]\n",
      "epoch:32 step:30748 [D loss: 0.378743, acc.: 84.38%] [G loss: 1.522367]\n",
      "epoch:32 step:30749 [D loss: 0.674886, acc.: 62.50%] [G loss: 1.026537]\n",
      "epoch:32 step:30750 [D loss: 0.491253, acc.: 71.88%] [G loss: 1.373709]\n",
      "epoch:32 step:30751 [D loss: 0.395442, acc.: 86.72%] [G loss: 1.632589]\n",
      "epoch:32 step:30752 [D loss: 0.532922, acc.: 71.88%] [G loss: 1.607425]\n",
      "epoch:32 step:30753 [D loss: 0.630763, acc.: 64.06%] [G loss: 1.677346]\n",
      "epoch:32 step:30754 [D loss: 0.517970, acc.: 79.69%] [G loss: 1.069078]\n",
      "epoch:32 step:30755 [D loss: 0.577196, acc.: 69.53%] [G loss: 1.444566]\n",
      "epoch:32 step:30756 [D loss: 0.580344, acc.: 71.88%] [G loss: 1.655106]\n",
      "epoch:32 step:30757 [D loss: 0.755697, acc.: 52.34%] [G loss: 1.903621]\n",
      "epoch:32 step:30758 [D loss: 0.581890, acc.: 72.66%] [G loss: 1.426440]\n",
      "epoch:32 step:30759 [D loss: 0.459984, acc.: 77.34%] [G loss: 1.422662]\n",
      "epoch:32 step:30760 [D loss: 0.335021, acc.: 90.62%] [G loss: 1.780083]\n",
      "epoch:32 step:30761 [D loss: 0.600629, acc.: 68.75%] [G loss: 1.231414]\n",
      "epoch:32 step:30762 [D loss: 0.429410, acc.: 81.25%] [G loss: 1.763965]\n",
      "epoch:32 step:30763 [D loss: 0.393234, acc.: 82.81%] [G loss: 1.907112]\n",
      "epoch:32 step:30764 [D loss: 0.514164, acc.: 73.44%] [G loss: 2.015263]\n",
      "epoch:32 step:30765 [D loss: 0.539967, acc.: 70.31%] [G loss: 1.377201]\n",
      "epoch:32 step:30766 [D loss: 0.403443, acc.: 84.38%] [G loss: 1.341098]\n",
      "epoch:32 step:30767 [D loss: 0.518176, acc.: 72.66%] [G loss: 1.891514]\n",
      "epoch:32 step:30768 [D loss: 0.541121, acc.: 71.88%] [G loss: 1.343189]\n",
      "epoch:32 step:30769 [D loss: 0.421976, acc.: 79.69%] [G loss: 1.603921]\n",
      "epoch:32 step:30770 [D loss: 0.650924, acc.: 61.72%] [G loss: 1.198186]\n",
      "epoch:32 step:30771 [D loss: 0.421940, acc.: 81.25%] [G loss: 1.394417]\n",
      "epoch:32 step:30772 [D loss: 0.568578, acc.: 75.78%] [G loss: 1.560428]\n",
      "epoch:32 step:30773 [D loss: 0.417991, acc.: 83.59%] [G loss: 2.102728]\n",
      "epoch:32 step:30774 [D loss: 0.592918, acc.: 67.97%] [G loss: 1.532052]\n",
      "epoch:32 step:30775 [D loss: 0.376176, acc.: 89.84%] [G loss: 1.965276]\n",
      "epoch:32 step:30776 [D loss: 0.518494, acc.: 69.53%] [G loss: 1.513552]\n",
      "epoch:32 step:30777 [D loss: 0.437736, acc.: 85.16%] [G loss: 2.173214]\n",
      "epoch:32 step:30778 [D loss: 0.489221, acc.: 75.78%] [G loss: 1.311888]\n",
      "epoch:32 step:30779 [D loss: 0.540164, acc.: 73.44%] [G loss: 1.478649]\n",
      "epoch:32 step:30780 [D loss: 0.560780, acc.: 71.09%] [G loss: 1.204769]\n",
      "epoch:32 step:30781 [D loss: 0.533720, acc.: 75.00%] [G loss: 1.309891]\n",
      "epoch:32 step:30782 [D loss: 0.570395, acc.: 71.88%] [G loss: 1.054192]\n",
      "epoch:32 step:30783 [D loss: 0.511010, acc.: 72.66%] [G loss: 1.243699]\n",
      "epoch:32 step:30784 [D loss: 0.549508, acc.: 72.66%] [G loss: 1.866848]\n",
      "epoch:32 step:30785 [D loss: 0.560403, acc.: 73.44%] [G loss: 1.518345]\n",
      "epoch:32 step:30786 [D loss: 0.653945, acc.: 61.72%] [G loss: 1.042087]\n",
      "epoch:32 step:30787 [D loss: 0.532059, acc.: 72.66%] [G loss: 1.441498]\n",
      "epoch:32 step:30788 [D loss: 0.537206, acc.: 75.78%] [G loss: 1.775825]\n",
      "epoch:32 step:30789 [D loss: 0.469537, acc.: 73.44%] [G loss: 1.786547]\n",
      "epoch:32 step:30790 [D loss: 0.475280, acc.: 74.22%] [G loss: 1.746969]\n",
      "epoch:32 step:30791 [D loss: 0.564006, acc.: 67.19%] [G loss: 1.484167]\n",
      "epoch:32 step:30792 [D loss: 0.567039, acc.: 69.53%] [G loss: 1.356656]\n",
      "epoch:32 step:30793 [D loss: 0.589812, acc.: 71.88%] [G loss: 1.303877]\n",
      "epoch:32 step:30794 [D loss: 0.393724, acc.: 84.38%] [G loss: 1.800306]\n",
      "epoch:32 step:30795 [D loss: 0.865803, acc.: 47.66%] [G loss: 1.446217]\n",
      "epoch:32 step:30796 [D loss: 0.549393, acc.: 73.44%] [G loss: 1.643851]\n",
      "epoch:32 step:30797 [D loss: 0.322622, acc.: 90.62%] [G loss: 1.469563]\n",
      "epoch:32 step:30798 [D loss: 0.398786, acc.: 85.16%] [G loss: 1.352931]\n",
      "epoch:32 step:30799 [D loss: 0.629857, acc.: 64.06%] [G loss: 1.157898]\n",
      "epoch:32 step:30800 [D loss: 0.446169, acc.: 76.56%] [G loss: 1.481917]\n",
      "##############\n",
      "[2.73484008 2.1085703  1.90409631 3.17124089 0.96340328 6.27645895\n",
      " 2.27629928 2.87309555 3.92835408 7.14868929]\n",
      "##########\n",
      "epoch:32 step:30801 [D loss: 0.678898, acc.: 63.28%] [G loss: 0.873091]\n",
      "epoch:32 step:30802 [D loss: 0.624848, acc.: 59.38%] [G loss: 1.705618]\n",
      "epoch:32 step:30803 [D loss: 0.540703, acc.: 73.44%] [G loss: 1.346974]\n",
      "epoch:32 step:30804 [D loss: 0.401994, acc.: 83.59%] [G loss: 1.442107]\n",
      "epoch:32 step:30805 [D loss: 0.582762, acc.: 65.62%] [G loss: 1.379493]\n",
      "epoch:32 step:30806 [D loss: 0.715623, acc.: 57.03%] [G loss: 1.290862]\n",
      "epoch:32 step:30807 [D loss: 0.566173, acc.: 71.09%] [G loss: 1.483870]\n",
      "epoch:32 step:30808 [D loss: 0.463362, acc.: 77.34%] [G loss: 1.896487]\n",
      "epoch:32 step:30809 [D loss: 0.529018, acc.: 72.66%] [G loss: 1.373029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30810 [D loss: 0.576630, acc.: 70.31%] [G loss: 1.571326]\n",
      "epoch:32 step:30811 [D loss: 0.585331, acc.: 70.31%] [G loss: 1.846692]\n",
      "epoch:32 step:30812 [D loss: 0.769399, acc.: 51.56%] [G loss: 1.805234]\n",
      "epoch:32 step:30813 [D loss: 0.758884, acc.: 56.25%] [G loss: 1.485190]\n",
      "epoch:32 step:30814 [D loss: 0.620104, acc.: 69.53%] [G loss: 1.110541]\n",
      "epoch:32 step:30815 [D loss: 0.860123, acc.: 48.44%] [G loss: 1.292030]\n",
      "epoch:32 step:30816 [D loss: 0.641881, acc.: 67.97%] [G loss: 1.324894]\n",
      "epoch:32 step:30817 [D loss: 0.648259, acc.: 68.75%] [G loss: 1.374826]\n",
      "epoch:32 step:30818 [D loss: 0.457174, acc.: 76.56%] [G loss: 1.590987]\n",
      "epoch:32 step:30819 [D loss: 0.486951, acc.: 77.34%] [G loss: 1.274843]\n",
      "epoch:32 step:30820 [D loss: 0.737034, acc.: 57.81%] [G loss: 1.279484]\n",
      "epoch:32 step:30821 [D loss: 0.604501, acc.: 69.53%] [G loss: 1.422734]\n",
      "epoch:32 step:30822 [D loss: 0.426017, acc.: 83.59%] [G loss: 1.349952]\n",
      "epoch:32 step:30823 [D loss: 0.724957, acc.: 60.16%] [G loss: 1.361495]\n",
      "epoch:32 step:30824 [D loss: 0.583216, acc.: 63.28%] [G loss: 1.356689]\n",
      "epoch:32 step:30825 [D loss: 0.502819, acc.: 74.22%] [G loss: 2.013298]\n",
      "epoch:32 step:30826 [D loss: 0.449735, acc.: 80.47%] [G loss: 2.138786]\n",
      "epoch:32 step:30827 [D loss: 0.755549, acc.: 55.47%] [G loss: 1.413271]\n",
      "epoch:32 step:30828 [D loss: 0.548519, acc.: 73.44%] [G loss: 1.116175]\n",
      "epoch:32 step:30829 [D loss: 0.749141, acc.: 59.38%] [G loss: 1.200837]\n",
      "epoch:32 step:30830 [D loss: 0.641044, acc.: 60.94%] [G loss: 1.129982]\n",
      "epoch:32 step:30831 [D loss: 0.633969, acc.: 63.28%] [G loss: 1.168751]\n",
      "epoch:32 step:30832 [D loss: 0.471309, acc.: 77.34%] [G loss: 1.806329]\n",
      "epoch:32 step:30833 [D loss: 0.372592, acc.: 81.25%] [G loss: 1.672976]\n",
      "epoch:32 step:30834 [D loss: 0.469631, acc.: 78.12%] [G loss: 1.784901]\n",
      "epoch:32 step:30835 [D loss: 0.673822, acc.: 58.59%] [G loss: 1.778220]\n",
      "epoch:32 step:30836 [D loss: 0.322727, acc.: 86.72%] [G loss: 1.364428]\n",
      "epoch:32 step:30837 [D loss: 0.430050, acc.: 83.59%] [G loss: 1.787384]\n",
      "epoch:32 step:30838 [D loss: 0.551776, acc.: 67.97%] [G loss: 1.268367]\n",
      "epoch:32 step:30839 [D loss: 0.556347, acc.: 74.22%] [G loss: 1.254992]\n",
      "epoch:32 step:30840 [D loss: 0.253980, acc.: 95.31%] [G loss: 1.346261]\n",
      "epoch:32 step:30841 [D loss: 0.534489, acc.: 79.69%] [G loss: 1.338458]\n",
      "epoch:32 step:30842 [D loss: 0.475338, acc.: 75.00%] [G loss: 1.420070]\n",
      "epoch:32 step:30843 [D loss: 0.391089, acc.: 85.16%] [G loss: 1.517706]\n",
      "epoch:32 step:30844 [D loss: 0.671027, acc.: 66.41%] [G loss: 1.658686]\n",
      "epoch:32 step:30845 [D loss: 0.432117, acc.: 82.81%] [G loss: 1.638754]\n",
      "epoch:32 step:30846 [D loss: 0.628705, acc.: 65.62%] [G loss: 1.311838]\n",
      "epoch:32 step:30847 [D loss: 0.582183, acc.: 64.84%] [G loss: 1.112711]\n",
      "epoch:32 step:30848 [D loss: 0.538266, acc.: 68.75%] [G loss: 1.675750]\n",
      "epoch:32 step:30849 [D loss: 0.453514, acc.: 80.47%] [G loss: 1.691477]\n",
      "epoch:32 step:30850 [D loss: 0.391875, acc.: 85.94%] [G loss: 1.463480]\n",
      "epoch:32 step:30851 [D loss: 0.544383, acc.: 72.66%] [G loss: 1.866339]\n",
      "epoch:32 step:30852 [D loss: 0.591534, acc.: 68.75%] [G loss: 1.029461]\n",
      "epoch:32 step:30853 [D loss: 0.386298, acc.: 83.59%] [G loss: 1.339137]\n",
      "epoch:32 step:30854 [D loss: 0.531743, acc.: 74.22%] [G loss: 1.748680]\n",
      "epoch:32 step:30855 [D loss: 0.472171, acc.: 79.69%] [G loss: 1.112949]\n",
      "epoch:32 step:30856 [D loss: 0.496726, acc.: 74.22%] [G loss: 1.734366]\n",
      "epoch:32 step:30857 [D loss: 0.554660, acc.: 67.97%] [G loss: 1.555689]\n",
      "epoch:32 step:30858 [D loss: 0.426089, acc.: 80.47%] [G loss: 1.855101]\n",
      "epoch:32 step:30859 [D loss: 0.546880, acc.: 71.88%] [G loss: 1.559706]\n",
      "epoch:32 step:30860 [D loss: 0.737289, acc.: 60.94%] [G loss: 1.484314]\n",
      "epoch:32 step:30861 [D loss: 0.535880, acc.: 69.53%] [G loss: 1.374749]\n",
      "epoch:32 step:30862 [D loss: 0.611195, acc.: 68.75%] [G loss: 1.393096]\n",
      "epoch:32 step:30863 [D loss: 0.586669, acc.: 68.75%] [G loss: 1.400547]\n",
      "epoch:32 step:30864 [D loss: 0.505895, acc.: 75.78%] [G loss: 1.339544]\n",
      "epoch:32 step:30865 [D loss: 0.428733, acc.: 83.59%] [G loss: 1.938231]\n",
      "epoch:32 step:30866 [D loss: 0.400596, acc.: 85.94%] [G loss: 1.393243]\n",
      "epoch:32 step:30867 [D loss: 0.617563, acc.: 64.06%] [G loss: 1.409609]\n",
      "epoch:32 step:30868 [D loss: 0.607185, acc.: 66.41%] [G loss: 1.465453]\n",
      "epoch:32 step:30869 [D loss: 0.480444, acc.: 76.56%] [G loss: 1.088465]\n",
      "epoch:32 step:30870 [D loss: 0.411192, acc.: 85.94%] [G loss: 1.333779]\n",
      "epoch:32 step:30871 [D loss: 0.511707, acc.: 78.12%] [G loss: 1.264981]\n",
      "epoch:32 step:30872 [D loss: 0.529619, acc.: 75.00%] [G loss: 0.891142]\n",
      "epoch:32 step:30873 [D loss: 0.555667, acc.: 74.22%] [G loss: 1.084715]\n",
      "epoch:32 step:30874 [D loss: 0.803973, acc.: 48.44%] [G loss: 1.426144]\n",
      "epoch:32 step:30875 [D loss: 0.516380, acc.: 75.00%] [G loss: 1.273485]\n",
      "epoch:32 step:30876 [D loss: 0.555007, acc.: 67.19%] [G loss: 1.201288]\n",
      "epoch:32 step:30877 [D loss: 0.547962, acc.: 73.44%] [G loss: 1.046725]\n",
      "epoch:32 step:30878 [D loss: 0.704658, acc.: 64.84%] [G loss: 1.442482]\n",
      "epoch:32 step:30879 [D loss: 0.479383, acc.: 78.12%] [G loss: 1.682488]\n",
      "epoch:32 step:30880 [D loss: 0.590634, acc.: 71.88%] [G loss: 1.163252]\n",
      "epoch:32 step:30881 [D loss: 0.439149, acc.: 82.03%] [G loss: 1.751121]\n",
      "epoch:32 step:30882 [D loss: 0.521795, acc.: 71.09%] [G loss: 1.899750]\n",
      "epoch:32 step:30883 [D loss: 0.713382, acc.: 59.38%] [G loss: 1.727594]\n",
      "epoch:32 step:30884 [D loss: 0.486538, acc.: 76.56%] [G loss: 1.817975]\n",
      "epoch:32 step:30885 [D loss: 0.479078, acc.: 78.91%] [G loss: 1.634044]\n",
      "epoch:32 step:30886 [D loss: 0.579985, acc.: 64.84%] [G loss: 1.180640]\n",
      "epoch:32 step:30887 [D loss: 0.359979, acc.: 86.72%] [G loss: 1.993245]\n",
      "epoch:32 step:30888 [D loss: 0.387456, acc.: 89.06%] [G loss: 1.299207]\n",
      "epoch:32 step:30889 [D loss: 0.831430, acc.: 53.12%] [G loss: 1.389755]\n",
      "epoch:32 step:30890 [D loss: 0.539544, acc.: 69.53%] [G loss: 1.800971]\n",
      "epoch:32 step:30891 [D loss: 0.584494, acc.: 73.44%] [G loss: 1.565364]\n",
      "epoch:32 step:30892 [D loss: 0.461700, acc.: 73.44%] [G loss: 1.835160]\n",
      "epoch:32 step:30893 [D loss: 0.520342, acc.: 71.88%] [G loss: 1.566558]\n",
      "epoch:32 step:30894 [D loss: 0.536395, acc.: 70.31%] [G loss: 1.579841]\n",
      "epoch:32 step:30895 [D loss: 0.595110, acc.: 75.00%] [G loss: 1.727113]\n",
      "epoch:32 step:30896 [D loss: 0.518139, acc.: 76.56%] [G loss: 1.315471]\n",
      "epoch:32 step:30897 [D loss: 0.527671, acc.: 72.66%] [G loss: 1.912385]\n",
      "epoch:32 step:30898 [D loss: 0.518210, acc.: 73.44%] [G loss: 1.420934]\n",
      "epoch:32 step:30899 [D loss: 0.586490, acc.: 68.75%] [G loss: 1.521109]\n",
      "epoch:32 step:30900 [D loss: 0.571031, acc.: 69.53%] [G loss: 1.231118]\n",
      "epoch:32 step:30901 [D loss: 0.647171, acc.: 64.06%] [G loss: 1.728683]\n",
      "epoch:32 step:30902 [D loss: 0.285928, acc.: 94.53%] [G loss: 2.212367]\n",
      "epoch:32 step:30903 [D loss: 0.568911, acc.: 66.41%] [G loss: 1.217076]\n",
      "epoch:32 step:30904 [D loss: 0.606072, acc.: 66.41%] [G loss: 1.491547]\n",
      "epoch:32 step:30905 [D loss: 0.526338, acc.: 71.88%] [G loss: 1.414607]\n",
      "epoch:32 step:30906 [D loss: 0.595552, acc.: 63.28%] [G loss: 1.875357]\n",
      "epoch:32 step:30907 [D loss: 0.599768, acc.: 67.97%] [G loss: 1.406514]\n",
      "epoch:32 step:30908 [D loss: 0.500404, acc.: 77.34%] [G loss: 1.283909]\n",
      "epoch:32 step:30909 [D loss: 0.445126, acc.: 78.91%] [G loss: 1.649964]\n",
      "epoch:32 step:30910 [D loss: 0.349162, acc.: 86.72%] [G loss: 1.825681]\n",
      "epoch:32 step:30911 [D loss: 0.491243, acc.: 76.56%] [G loss: 2.043020]\n",
      "epoch:32 step:30912 [D loss: 0.617136, acc.: 68.75%] [G loss: 1.492510]\n",
      "epoch:32 step:30913 [D loss: 0.593074, acc.: 67.19%] [G loss: 1.251728]\n",
      "epoch:32 step:30914 [D loss: 0.408603, acc.: 81.25%] [G loss: 1.570791]\n",
      "epoch:32 step:30915 [D loss: 0.699303, acc.: 62.50%] [G loss: 1.620620]\n",
      "epoch:32 step:30916 [D loss: 0.386140, acc.: 85.94%] [G loss: 1.926797]\n",
      "epoch:32 step:30917 [D loss: 0.723231, acc.: 55.47%] [G loss: 1.303172]\n",
      "epoch:32 step:30918 [D loss: 0.425387, acc.: 81.25%] [G loss: 1.067500]\n",
      "epoch:32 step:30919 [D loss: 0.442479, acc.: 81.25%] [G loss: 1.729048]\n",
      "epoch:32 step:30920 [D loss: 0.516934, acc.: 74.22%] [G loss: 1.319665]\n",
      "epoch:32 step:30921 [D loss: 0.488803, acc.: 75.00%] [G loss: 1.711185]\n",
      "epoch:33 step:30922 [D loss: 0.565756, acc.: 69.53%] [G loss: 1.606152]\n",
      "epoch:33 step:30923 [D loss: 0.590367, acc.: 71.88%] [G loss: 1.558444]\n",
      "epoch:33 step:30924 [D loss: 0.644825, acc.: 65.62%] [G loss: 1.111747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:30925 [D loss: 0.477416, acc.: 78.12%] [G loss: 1.677876]\n",
      "epoch:33 step:30926 [D loss: 0.558683, acc.: 71.88%] [G loss: 1.518268]\n",
      "epoch:33 step:30927 [D loss: 0.447259, acc.: 81.25%] [G loss: 1.552357]\n",
      "epoch:33 step:30928 [D loss: 0.689480, acc.: 62.50%] [G loss: 1.173251]\n",
      "epoch:33 step:30929 [D loss: 0.556305, acc.: 67.97%] [G loss: 1.697405]\n",
      "epoch:33 step:30930 [D loss: 0.536314, acc.: 71.88%] [G loss: 1.934114]\n",
      "epoch:33 step:30931 [D loss: 0.499576, acc.: 75.00%] [G loss: 1.756985]\n",
      "epoch:33 step:30932 [D loss: 0.569015, acc.: 70.31%] [G loss: 1.206682]\n",
      "epoch:33 step:30933 [D loss: 0.556183, acc.: 72.66%] [G loss: 1.268509]\n",
      "epoch:33 step:30934 [D loss: 0.588572, acc.: 67.97%] [G loss: 1.445586]\n",
      "epoch:33 step:30935 [D loss: 0.466662, acc.: 78.12%] [G loss: 1.672497]\n",
      "epoch:33 step:30936 [D loss: 0.407203, acc.: 86.72%] [G loss: 1.809777]\n",
      "epoch:33 step:30937 [D loss: 0.500003, acc.: 79.69%] [G loss: 1.443534]\n",
      "epoch:33 step:30938 [D loss: 0.656087, acc.: 64.06%] [G loss: 1.186452]\n",
      "epoch:33 step:30939 [D loss: 0.487650, acc.: 78.12%] [G loss: 1.344947]\n",
      "epoch:33 step:30940 [D loss: 0.583377, acc.: 70.31%] [G loss: 1.548737]\n",
      "epoch:33 step:30941 [D loss: 0.611672, acc.: 67.19%] [G loss: 1.669931]\n",
      "epoch:33 step:30942 [D loss: 0.587051, acc.: 67.19%] [G loss: 1.576705]\n",
      "epoch:33 step:30943 [D loss: 0.652106, acc.: 65.62%] [G loss: 1.648585]\n",
      "epoch:33 step:30944 [D loss: 0.448425, acc.: 82.81%] [G loss: 1.546656]\n",
      "epoch:33 step:30945 [D loss: 0.616482, acc.: 65.62%] [G loss: 1.309525]\n",
      "epoch:33 step:30946 [D loss: 0.504550, acc.: 74.22%] [G loss: 1.179651]\n",
      "epoch:33 step:30947 [D loss: 0.469100, acc.: 75.00%] [G loss: 1.508500]\n",
      "epoch:33 step:30948 [D loss: 0.666871, acc.: 60.16%] [G loss: 1.376250]\n",
      "epoch:33 step:30949 [D loss: 0.402304, acc.: 82.81%] [G loss: 1.565535]\n",
      "epoch:33 step:30950 [D loss: 0.606294, acc.: 66.41%] [G loss: 1.613539]\n",
      "epoch:33 step:30951 [D loss: 0.564430, acc.: 75.00%] [G loss: 1.212858]\n",
      "epoch:33 step:30952 [D loss: 0.754025, acc.: 57.03%] [G loss: 1.417568]\n",
      "epoch:33 step:30953 [D loss: 0.564068, acc.: 69.53%] [G loss: 1.480935]\n",
      "epoch:33 step:30954 [D loss: 0.426936, acc.: 81.25%] [G loss: 1.504421]\n",
      "epoch:33 step:30955 [D loss: 0.436612, acc.: 80.47%] [G loss: 1.396575]\n",
      "epoch:33 step:30956 [D loss: 0.540907, acc.: 71.09%] [G loss: 1.385782]\n",
      "epoch:33 step:30957 [D loss: 0.710667, acc.: 63.28%] [G loss: 1.359373]\n",
      "epoch:33 step:30958 [D loss: 0.417267, acc.: 82.03%] [G loss: 1.764488]\n",
      "epoch:33 step:30959 [D loss: 0.608884, acc.: 64.84%] [G loss: 1.928535]\n",
      "epoch:33 step:30960 [D loss: 0.506448, acc.: 71.88%] [G loss: 1.382913]\n",
      "epoch:33 step:30961 [D loss: 0.485855, acc.: 78.12%] [G loss: 1.383797]\n",
      "epoch:33 step:30962 [D loss: 0.630662, acc.: 65.62%] [G loss: 1.471649]\n",
      "epoch:33 step:30963 [D loss: 0.569868, acc.: 70.31%] [G loss: 1.687521]\n",
      "epoch:33 step:30964 [D loss: 0.550287, acc.: 70.31%] [G loss: 1.283228]\n",
      "epoch:33 step:30965 [D loss: 0.611628, acc.: 63.28%] [G loss: 1.188826]\n",
      "epoch:33 step:30966 [D loss: 0.614528, acc.: 64.84%] [G loss: 1.471286]\n",
      "epoch:33 step:30967 [D loss: 0.495550, acc.: 71.09%] [G loss: 1.198156]\n",
      "epoch:33 step:30968 [D loss: 0.515915, acc.: 75.00%] [G loss: 1.305083]\n",
      "epoch:33 step:30969 [D loss: 0.454038, acc.: 82.81%] [G loss: 1.584073]\n",
      "epoch:33 step:30970 [D loss: 0.525996, acc.: 71.88%] [G loss: 1.853429]\n",
      "epoch:33 step:30971 [D loss: 0.454554, acc.: 85.16%] [G loss: 1.716411]\n",
      "epoch:33 step:30972 [D loss: 0.652708, acc.: 62.50%] [G loss: 1.664004]\n",
      "epoch:33 step:30973 [D loss: 0.538376, acc.: 73.44%] [G loss: 1.746936]\n",
      "epoch:33 step:30974 [D loss: 0.472585, acc.: 81.25%] [G loss: 1.655022]\n",
      "epoch:33 step:30975 [D loss: 0.579512, acc.: 67.97%] [G loss: 1.773666]\n",
      "epoch:33 step:30976 [D loss: 0.620716, acc.: 68.75%] [G loss: 1.502939]\n",
      "epoch:33 step:30977 [D loss: 0.521778, acc.: 69.53%] [G loss: 2.125377]\n",
      "epoch:33 step:30978 [D loss: 0.510667, acc.: 78.12%] [G loss: 1.400922]\n",
      "epoch:33 step:30979 [D loss: 0.460751, acc.: 80.47%] [G loss: 1.201345]\n",
      "epoch:33 step:30980 [D loss: 0.479964, acc.: 77.34%] [G loss: 1.611140]\n",
      "epoch:33 step:30981 [D loss: 0.552342, acc.: 71.88%] [G loss: 1.421928]\n",
      "epoch:33 step:30982 [D loss: 0.855246, acc.: 47.66%] [G loss: 1.270237]\n",
      "epoch:33 step:30983 [D loss: 0.659342, acc.: 64.06%] [G loss: 1.172360]\n",
      "epoch:33 step:30984 [D loss: 0.669067, acc.: 63.28%] [G loss: 1.271719]\n",
      "epoch:33 step:30985 [D loss: 0.380740, acc.: 84.38%] [G loss: 1.189611]\n",
      "epoch:33 step:30986 [D loss: 0.464362, acc.: 79.69%] [G loss: 1.222335]\n",
      "epoch:33 step:30987 [D loss: 0.448562, acc.: 76.56%] [G loss: 1.451569]\n",
      "epoch:33 step:30988 [D loss: 0.352558, acc.: 86.72%] [G loss: 1.605994]\n",
      "epoch:33 step:30989 [D loss: 0.527420, acc.: 73.44%] [G loss: 1.316141]\n",
      "epoch:33 step:30990 [D loss: 0.506677, acc.: 77.34%] [G loss: 1.141131]\n",
      "epoch:33 step:30991 [D loss: 0.731837, acc.: 57.03%] [G loss: 1.219685]\n",
      "epoch:33 step:30992 [D loss: 0.700085, acc.: 58.59%] [G loss: 1.464161]\n",
      "epoch:33 step:30993 [D loss: 0.628172, acc.: 63.28%] [G loss: 1.568050]\n",
      "epoch:33 step:30994 [D loss: 0.467329, acc.: 79.69%] [G loss: 1.499231]\n",
      "epoch:33 step:30995 [D loss: 0.397564, acc.: 82.03%] [G loss: 1.831297]\n",
      "epoch:33 step:30996 [D loss: 0.477183, acc.: 78.91%] [G loss: 1.522311]\n",
      "epoch:33 step:30997 [D loss: 0.631144, acc.: 61.72%] [G loss: 1.576671]\n",
      "epoch:33 step:30998 [D loss: 0.616120, acc.: 66.41%] [G loss: 0.751350]\n",
      "epoch:33 step:30999 [D loss: 0.607540, acc.: 71.88%] [G loss: 1.359495]\n",
      "epoch:33 step:31000 [D loss: 0.623388, acc.: 67.19%] [G loss: 1.241353]\n",
      "##############\n",
      "[2.66719344 2.00928435 1.95781398 2.8459788  0.92838293 5.95372342\n",
      " 2.07149599 2.40854612 3.80433912 7.14868929]\n",
      "##########\n",
      "epoch:33 step:31001 [D loss: 0.521096, acc.: 73.44%] [G loss: 1.638743]\n",
      "epoch:33 step:31002 [D loss: 0.680618, acc.: 64.06%] [G loss: 1.287540]\n",
      "epoch:33 step:31003 [D loss: 0.402774, acc.: 85.94%] [G loss: 1.257674]\n",
      "epoch:33 step:31004 [D loss: 0.616152, acc.: 70.31%] [G loss: 1.473773]\n",
      "epoch:33 step:31005 [D loss: 0.712509, acc.: 63.28%] [G loss: 1.455767]\n",
      "epoch:33 step:31006 [D loss: 0.571566, acc.: 65.62%] [G loss: 1.219444]\n",
      "epoch:33 step:31007 [D loss: 0.684006, acc.: 64.06%] [G loss: 1.536460]\n",
      "epoch:33 step:31008 [D loss: 0.572315, acc.: 67.19%] [G loss: 1.723671]\n",
      "epoch:33 step:31009 [D loss: 0.653511, acc.: 60.94%] [G loss: 1.589024]\n",
      "epoch:33 step:31010 [D loss: 0.456370, acc.: 77.34%] [G loss: 1.939112]\n",
      "epoch:33 step:31011 [D loss: 0.433835, acc.: 81.25%] [G loss: 1.301148]\n",
      "epoch:33 step:31012 [D loss: 0.569549, acc.: 66.41%] [G loss: 1.169050]\n",
      "epoch:33 step:31013 [D loss: 0.549558, acc.: 70.31%] [G loss: 1.530381]\n",
      "epoch:33 step:31014 [D loss: 0.494180, acc.: 80.47%] [G loss: 1.330665]\n",
      "epoch:33 step:31015 [D loss: 0.420689, acc.: 81.25%] [G loss: 1.602888]\n",
      "epoch:33 step:31016 [D loss: 0.502796, acc.: 74.22%] [G loss: 1.221854]\n",
      "epoch:33 step:31017 [D loss: 0.576879, acc.: 68.75%] [G loss: 1.522352]\n",
      "epoch:33 step:31018 [D loss: 0.555820, acc.: 70.31%] [G loss: 1.571970]\n",
      "epoch:33 step:31019 [D loss: 0.581654, acc.: 69.53%] [G loss: 1.592801]\n",
      "epoch:33 step:31020 [D loss: 0.593380, acc.: 68.75%] [G loss: 1.072471]\n",
      "epoch:33 step:31021 [D loss: 0.624419, acc.: 67.97%] [G loss: 1.359044]\n",
      "epoch:33 step:31022 [D loss: 0.402493, acc.: 85.16%] [G loss: 1.632796]\n",
      "epoch:33 step:31023 [D loss: 0.440052, acc.: 76.56%] [G loss: 1.165440]\n",
      "epoch:33 step:31024 [D loss: 0.603812, acc.: 64.06%] [G loss: 1.306991]\n",
      "epoch:33 step:31025 [D loss: 0.609443, acc.: 64.84%] [G loss: 1.111754]\n",
      "epoch:33 step:31026 [D loss: 0.417731, acc.: 86.72%] [G loss: 1.759253]\n",
      "epoch:33 step:31027 [D loss: 0.520152, acc.: 73.44%] [G loss: 1.506009]\n",
      "epoch:33 step:31028 [D loss: 0.430036, acc.: 84.38%] [G loss: 1.476730]\n",
      "epoch:33 step:31029 [D loss: 0.587936, acc.: 67.19%] [G loss: 1.271344]\n",
      "epoch:33 step:31030 [D loss: 0.428328, acc.: 83.59%] [G loss: 1.576993]\n",
      "epoch:33 step:31031 [D loss: 0.436128, acc.: 82.81%] [G loss: 2.018155]\n",
      "epoch:33 step:31032 [D loss: 0.693577, acc.: 60.16%] [G loss: 1.490043]\n",
      "epoch:33 step:31033 [D loss: 0.475600, acc.: 78.91%] [G loss: 1.952818]\n",
      "epoch:33 step:31034 [D loss: 0.558861, acc.: 70.31%] [G loss: 1.621542]\n",
      "epoch:33 step:31035 [D loss: 0.326128, acc.: 89.06%] [G loss: 1.211497]\n",
      "epoch:33 step:31036 [D loss: 0.376868, acc.: 86.72%] [G loss: 1.806325]\n",
      "epoch:33 step:31037 [D loss: 0.604255, acc.: 66.41%] [G loss: 1.114175]\n",
      "epoch:33 step:31038 [D loss: 0.702010, acc.: 60.94%] [G loss: 1.451483]\n",
      "epoch:33 step:31039 [D loss: 0.489681, acc.: 72.66%] [G loss: 1.552151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31040 [D loss: 0.484661, acc.: 77.34%] [G loss: 1.405897]\n",
      "epoch:33 step:31041 [D loss: 0.640527, acc.: 63.28%] [G loss: 1.689700]\n",
      "epoch:33 step:31042 [D loss: 0.509884, acc.: 78.12%] [G loss: 1.694842]\n",
      "epoch:33 step:31043 [D loss: 0.566998, acc.: 72.66%] [G loss: 1.427636]\n",
      "epoch:33 step:31044 [D loss: 0.491967, acc.: 77.34%] [G loss: 1.233293]\n",
      "epoch:33 step:31045 [D loss: 0.683964, acc.: 60.94%] [G loss: 1.739661]\n",
      "epoch:33 step:31046 [D loss: 0.438277, acc.: 78.91%] [G loss: 1.599990]\n",
      "epoch:33 step:31047 [D loss: 0.445807, acc.: 78.91%] [G loss: 1.886031]\n",
      "epoch:33 step:31048 [D loss: 0.450776, acc.: 75.78%] [G loss: 1.653460]\n",
      "epoch:33 step:31049 [D loss: 0.583869, acc.: 67.19%] [G loss: 1.536120]\n",
      "epoch:33 step:31050 [D loss: 0.414662, acc.: 82.03%] [G loss: 1.306413]\n",
      "epoch:33 step:31051 [D loss: 0.428520, acc.: 83.59%] [G loss: 1.135422]\n",
      "epoch:33 step:31052 [D loss: 0.437652, acc.: 81.25%] [G loss: 1.360006]\n",
      "epoch:33 step:31053 [D loss: 0.638628, acc.: 64.84%] [G loss: 1.585344]\n",
      "epoch:33 step:31054 [D loss: 0.470196, acc.: 78.91%] [G loss: 1.365108]\n",
      "epoch:33 step:31055 [D loss: 0.627958, acc.: 65.62%] [G loss: 1.226051]\n",
      "epoch:33 step:31056 [D loss: 0.436147, acc.: 82.03%] [G loss: 1.283001]\n",
      "epoch:33 step:31057 [D loss: 1.011095, acc.: 38.28%] [G loss: 1.569395]\n",
      "epoch:33 step:31058 [D loss: 0.406675, acc.: 85.94%] [G loss: 1.909797]\n",
      "epoch:33 step:31059 [D loss: 0.696760, acc.: 56.25%] [G loss: 1.035021]\n",
      "epoch:33 step:31060 [D loss: 0.520047, acc.: 75.78%] [G loss: 1.829874]\n",
      "epoch:33 step:31061 [D loss: 0.534602, acc.: 73.44%] [G loss: 1.724340]\n",
      "epoch:33 step:31062 [D loss: 0.601896, acc.: 64.84%] [G loss: 1.444362]\n",
      "epoch:33 step:31063 [D loss: 0.413461, acc.: 78.91%] [G loss: 1.223531]\n",
      "epoch:33 step:31064 [D loss: 0.571553, acc.: 72.66%] [G loss: 1.034190]\n",
      "epoch:33 step:31065 [D loss: 0.649380, acc.: 61.72%] [G loss: 1.811283]\n",
      "epoch:33 step:31066 [D loss: 0.456459, acc.: 76.56%] [G loss: 1.440844]\n",
      "epoch:33 step:31067 [D loss: 0.416384, acc.: 81.25%] [G loss: 1.387327]\n",
      "epoch:33 step:31068 [D loss: 0.406317, acc.: 85.16%] [G loss: 1.133466]\n",
      "epoch:33 step:31069 [D loss: 0.648255, acc.: 64.06%] [G loss: 1.365022]\n",
      "epoch:33 step:31070 [D loss: 0.602423, acc.: 68.75%] [G loss: 1.484752]\n",
      "epoch:33 step:31071 [D loss: 0.615328, acc.: 67.97%] [G loss: 1.663112]\n",
      "epoch:33 step:31072 [D loss: 0.524847, acc.: 70.31%] [G loss: 1.181786]\n",
      "epoch:33 step:31073 [D loss: 0.540274, acc.: 72.66%] [G loss: 1.530022]\n",
      "epoch:33 step:31074 [D loss: 0.689668, acc.: 62.50%] [G loss: 1.957847]\n",
      "epoch:33 step:31075 [D loss: 0.443326, acc.: 81.25%] [G loss: 1.455422]\n",
      "epoch:33 step:31076 [D loss: 0.675612, acc.: 61.72%] [G loss: 1.424324]\n",
      "epoch:33 step:31077 [D loss: 0.350226, acc.: 86.72%] [G loss: 1.640857]\n",
      "epoch:33 step:31078 [D loss: 0.580797, acc.: 70.31%] [G loss: 1.267796]\n",
      "epoch:33 step:31079 [D loss: 0.634149, acc.: 65.62%] [G loss: 1.025781]\n",
      "epoch:33 step:31080 [D loss: 0.479984, acc.: 79.69%] [G loss: 1.302576]\n",
      "epoch:33 step:31081 [D loss: 0.384087, acc.: 83.59%] [G loss: 1.488696]\n",
      "epoch:33 step:31082 [D loss: 0.618580, acc.: 62.50%] [G loss: 1.433942]\n",
      "epoch:33 step:31083 [D loss: 0.705839, acc.: 59.38%] [G loss: 1.313550]\n",
      "epoch:33 step:31084 [D loss: 0.488028, acc.: 75.78%] [G loss: 1.375479]\n",
      "epoch:33 step:31085 [D loss: 0.296471, acc.: 95.31%] [G loss: 1.582276]\n",
      "epoch:33 step:31086 [D loss: 0.666429, acc.: 60.16%] [G loss: 1.358772]\n",
      "epoch:33 step:31087 [D loss: 0.478674, acc.: 80.47%] [G loss: 1.411617]\n",
      "epoch:33 step:31088 [D loss: 0.559570, acc.: 71.88%] [G loss: 0.939890]\n",
      "epoch:33 step:31089 [D loss: 0.557870, acc.: 69.53%] [G loss: 1.279734]\n",
      "epoch:33 step:31090 [D loss: 0.733620, acc.: 56.25%] [G loss: 1.294111]\n",
      "epoch:33 step:31091 [D loss: 0.403693, acc.: 83.59%] [G loss: 1.997138]\n",
      "epoch:33 step:31092 [D loss: 0.408256, acc.: 82.81%] [G loss: 1.747609]\n",
      "epoch:33 step:31093 [D loss: 0.408205, acc.: 82.03%] [G loss: 1.997362]\n",
      "epoch:33 step:31094 [D loss: 0.513718, acc.: 75.00%] [G loss: 1.725742]\n",
      "epoch:33 step:31095 [D loss: 0.573956, acc.: 64.06%] [G loss: 1.244155]\n",
      "epoch:33 step:31096 [D loss: 0.442188, acc.: 78.12%] [G loss: 1.728191]\n",
      "epoch:33 step:31097 [D loss: 0.576423, acc.: 75.78%] [G loss: 1.107465]\n",
      "epoch:33 step:31098 [D loss: 0.457224, acc.: 81.25%] [G loss: 1.102254]\n",
      "epoch:33 step:31099 [D loss: 0.502064, acc.: 74.22%] [G loss: 1.639359]\n",
      "epoch:33 step:31100 [D loss: 0.672199, acc.: 62.50%] [G loss: 1.476198]\n",
      "epoch:33 step:31101 [D loss: 0.448125, acc.: 79.69%] [G loss: 2.018666]\n",
      "epoch:33 step:31102 [D loss: 0.463878, acc.: 78.91%] [G loss: 1.369316]\n",
      "epoch:33 step:31103 [D loss: 0.527366, acc.: 75.00%] [G loss: 1.510199]\n",
      "epoch:33 step:31104 [D loss: 0.592557, acc.: 67.19%] [G loss: 1.008938]\n",
      "epoch:33 step:31105 [D loss: 0.533478, acc.: 71.88%] [G loss: 1.259568]\n",
      "epoch:33 step:31106 [D loss: 0.489544, acc.: 79.69%] [G loss: 1.516181]\n",
      "epoch:33 step:31107 [D loss: 0.585872, acc.: 67.97%] [G loss: 1.528790]\n",
      "epoch:33 step:31108 [D loss: 0.463637, acc.: 77.34%] [G loss: 1.439281]\n",
      "epoch:33 step:31109 [D loss: 0.599668, acc.: 71.88%] [G loss: 1.472857]\n",
      "epoch:33 step:31110 [D loss: 0.598399, acc.: 70.31%] [G loss: 1.995068]\n",
      "epoch:33 step:31111 [D loss: 0.458282, acc.: 77.34%] [G loss: 2.340239]\n",
      "epoch:33 step:31112 [D loss: 0.656505, acc.: 58.59%] [G loss: 1.370721]\n",
      "epoch:33 step:31113 [D loss: 0.658670, acc.: 62.50%] [G loss: 1.403271]\n",
      "epoch:33 step:31114 [D loss: 0.587627, acc.: 68.75%] [G loss: 1.415927]\n",
      "epoch:33 step:31115 [D loss: 0.510314, acc.: 78.91%] [G loss: 1.674213]\n",
      "epoch:33 step:31116 [D loss: 0.705207, acc.: 57.03%] [G loss: 1.186199]\n",
      "epoch:33 step:31117 [D loss: 0.323362, acc.: 89.84%] [G loss: 1.644500]\n",
      "epoch:33 step:31118 [D loss: 0.447622, acc.: 82.03%] [G loss: 1.356859]\n",
      "epoch:33 step:31119 [D loss: 0.434930, acc.: 82.03%] [G loss: 1.414712]\n",
      "epoch:33 step:31120 [D loss: 0.571218, acc.: 70.31%] [G loss: 1.484376]\n",
      "epoch:33 step:31121 [D loss: 0.423446, acc.: 81.25%] [G loss: 1.831839]\n",
      "epoch:33 step:31122 [D loss: 0.301740, acc.: 92.19%] [G loss: 1.841930]\n",
      "epoch:33 step:31123 [D loss: 0.441585, acc.: 82.81%] [G loss: 1.589794]\n",
      "epoch:33 step:31124 [D loss: 0.632729, acc.: 64.06%] [G loss: 1.462775]\n",
      "epoch:33 step:31125 [D loss: 0.271799, acc.: 93.75%] [G loss: 2.139861]\n",
      "epoch:33 step:31126 [D loss: 0.445674, acc.: 81.25%] [G loss: 1.665736]\n",
      "epoch:33 step:31127 [D loss: 0.437696, acc.: 84.38%] [G loss: 1.251722]\n",
      "epoch:33 step:31128 [D loss: 0.742670, acc.: 59.38%] [G loss: 2.021568]\n",
      "epoch:33 step:31129 [D loss: 0.341986, acc.: 89.84%] [G loss: 1.546080]\n",
      "epoch:33 step:31130 [D loss: 0.383513, acc.: 87.50%] [G loss: 1.622885]\n",
      "epoch:33 step:31131 [D loss: 0.392332, acc.: 84.38%] [G loss: 1.490948]\n",
      "epoch:33 step:31132 [D loss: 0.869063, acc.: 51.56%] [G loss: 1.182841]\n",
      "epoch:33 step:31133 [D loss: 0.555886, acc.: 75.78%] [G loss: 1.731410]\n",
      "epoch:33 step:31134 [D loss: 0.746644, acc.: 53.12%] [G loss: 1.659828]\n",
      "epoch:33 step:31135 [D loss: 0.589839, acc.: 69.53%] [G loss: 1.633489]\n",
      "epoch:33 step:31136 [D loss: 0.557831, acc.: 68.75%] [G loss: 1.640549]\n",
      "epoch:33 step:31137 [D loss: 0.488269, acc.: 76.56%] [G loss: 1.521837]\n",
      "epoch:33 step:31138 [D loss: 0.506027, acc.: 71.09%] [G loss: 1.784888]\n",
      "epoch:33 step:31139 [D loss: 0.619254, acc.: 65.62%] [G loss: 1.060062]\n",
      "epoch:33 step:31140 [D loss: 0.497396, acc.: 76.56%] [G loss: 1.585752]\n",
      "epoch:33 step:31141 [D loss: 0.579570, acc.: 70.31%] [G loss: 1.385597]\n",
      "epoch:33 step:31142 [D loss: 0.500421, acc.: 78.12%] [G loss: 1.226524]\n",
      "epoch:33 step:31143 [D loss: 0.594516, acc.: 71.09%] [G loss: 1.444563]\n",
      "epoch:33 step:31144 [D loss: 0.565623, acc.: 71.88%] [G loss: 1.320502]\n",
      "epoch:33 step:31145 [D loss: 0.600402, acc.: 67.97%] [G loss: 1.651521]\n",
      "epoch:33 step:31146 [D loss: 0.587942, acc.: 74.22%] [G loss: 1.114313]\n",
      "epoch:33 step:31147 [D loss: 0.561786, acc.: 69.53%] [G loss: 1.491864]\n",
      "epoch:33 step:31148 [D loss: 0.609470, acc.: 63.28%] [G loss: 1.263039]\n",
      "epoch:33 step:31149 [D loss: 0.524575, acc.: 78.12%] [G loss: 2.035752]\n",
      "epoch:33 step:31150 [D loss: 0.583900, acc.: 69.53%] [G loss: 1.665175]\n",
      "epoch:33 step:31151 [D loss: 0.489497, acc.: 78.12%] [G loss: 1.382360]\n",
      "epoch:33 step:31152 [D loss: 0.429537, acc.: 81.25%] [G loss: 1.455469]\n",
      "epoch:33 step:31153 [D loss: 0.753981, acc.: 57.81%] [G loss: 1.564196]\n",
      "epoch:33 step:31154 [D loss: 0.596010, acc.: 63.28%] [G loss: 1.336931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31155 [D loss: 0.555927, acc.: 69.53%] [G loss: 0.970020]\n",
      "epoch:33 step:31156 [D loss: 0.667743, acc.: 66.41%] [G loss: 1.625993]\n",
      "epoch:33 step:31157 [D loss: 0.722788, acc.: 54.69%] [G loss: 1.505568]\n",
      "epoch:33 step:31158 [D loss: 0.505547, acc.: 77.34%] [G loss: 1.063888]\n",
      "epoch:33 step:31159 [D loss: 0.392558, acc.: 83.59%] [G loss: 1.861705]\n",
      "epoch:33 step:31160 [D loss: 0.591411, acc.: 69.53%] [G loss: 1.225946]\n",
      "epoch:33 step:31161 [D loss: 0.641251, acc.: 65.62%] [G loss: 1.509000]\n",
      "epoch:33 step:31162 [D loss: 0.521142, acc.: 73.44%] [G loss: 1.601250]\n",
      "epoch:33 step:31163 [D loss: 0.470095, acc.: 78.12%] [G loss: 1.311403]\n",
      "epoch:33 step:31164 [D loss: 0.467779, acc.: 78.91%] [G loss: 1.602153]\n",
      "epoch:33 step:31165 [D loss: 0.585278, acc.: 67.19%] [G loss: 1.505574]\n",
      "epoch:33 step:31166 [D loss: 0.552191, acc.: 70.31%] [G loss: 1.463733]\n",
      "epoch:33 step:31167 [D loss: 0.377603, acc.: 82.81%] [G loss: 2.150639]\n",
      "epoch:33 step:31168 [D loss: 0.433672, acc.: 79.69%] [G loss: 1.389074]\n",
      "epoch:33 step:31169 [D loss: 0.455676, acc.: 84.38%] [G loss: 1.520870]\n",
      "epoch:33 step:31170 [D loss: 0.391812, acc.: 84.38%] [G loss: 1.274188]\n",
      "epoch:33 step:31171 [D loss: 0.491232, acc.: 75.00%] [G loss: 1.196555]\n",
      "epoch:33 step:31172 [D loss: 0.549802, acc.: 71.88%] [G loss: 1.805466]\n",
      "epoch:33 step:31173 [D loss: 0.445015, acc.: 75.78%] [G loss: 1.440570]\n",
      "epoch:33 step:31174 [D loss: 0.533854, acc.: 67.97%] [G loss: 1.495491]\n",
      "epoch:33 step:31175 [D loss: 0.657994, acc.: 63.28%] [G loss: 1.058415]\n",
      "epoch:33 step:31176 [D loss: 0.641424, acc.: 74.22%] [G loss: 1.183623]\n",
      "epoch:33 step:31177 [D loss: 0.484920, acc.: 75.78%] [G loss: 0.961055]\n",
      "epoch:33 step:31178 [D loss: 0.574925, acc.: 73.44%] [G loss: 1.287322]\n",
      "epoch:33 step:31179 [D loss: 0.606493, acc.: 70.31%] [G loss: 1.586163]\n",
      "epoch:33 step:31180 [D loss: 0.564219, acc.: 67.97%] [G loss: 1.732295]\n",
      "epoch:33 step:31181 [D loss: 0.708376, acc.: 57.81%] [G loss: 0.896161]\n",
      "epoch:33 step:31182 [D loss: 0.585378, acc.: 77.34%] [G loss: 1.474599]\n",
      "epoch:33 step:31183 [D loss: 0.585611, acc.: 65.62%] [G loss: 1.571321]\n",
      "epoch:33 step:31184 [D loss: 0.536484, acc.: 73.44%] [G loss: 1.427105]\n",
      "epoch:33 step:31185 [D loss: 0.597290, acc.: 71.88%] [G loss: 1.373928]\n",
      "epoch:33 step:31186 [D loss: 0.392104, acc.: 82.03%] [G loss: 1.630769]\n",
      "epoch:33 step:31187 [D loss: 0.685226, acc.: 63.28%] [G loss: 1.281228]\n",
      "epoch:33 step:31188 [D loss: 0.742616, acc.: 51.56%] [G loss: 1.135949]\n",
      "epoch:33 step:31189 [D loss: 0.463594, acc.: 75.78%] [G loss: 1.837652]\n",
      "epoch:33 step:31190 [D loss: 0.489657, acc.: 79.69%] [G loss: 1.483100]\n",
      "epoch:33 step:31191 [D loss: 0.475481, acc.: 76.56%] [G loss: 1.737651]\n",
      "epoch:33 step:31192 [D loss: 0.406926, acc.: 84.38%] [G loss: 1.245821]\n",
      "epoch:33 step:31193 [D loss: 0.525131, acc.: 73.44%] [G loss: 1.238297]\n",
      "epoch:33 step:31194 [D loss: 0.571895, acc.: 71.09%] [G loss: 1.027823]\n",
      "epoch:33 step:31195 [D loss: 0.490081, acc.: 78.12%] [G loss: 1.269274]\n",
      "epoch:33 step:31196 [D loss: 0.665798, acc.: 61.72%] [G loss: 1.271950]\n",
      "epoch:33 step:31197 [D loss: 0.565643, acc.: 69.53%] [G loss: 1.289872]\n",
      "epoch:33 step:31198 [D loss: 0.377272, acc.: 85.16%] [G loss: 1.372406]\n",
      "epoch:33 step:31199 [D loss: 0.617834, acc.: 68.75%] [G loss: 1.405045]\n",
      "epoch:33 step:31200 [D loss: 0.502966, acc.: 78.91%] [G loss: 1.755572]\n",
      "##############\n",
      "[2.63840798 1.93951242 1.62641948 2.71978684 0.47497294 5.65866211\n",
      " 2.30702513 2.28587171 3.95469943 7.14868929]\n",
      "##########\n",
      "epoch:33 step:31201 [D loss: 0.578912, acc.: 71.88%] [G loss: 1.076392]\n",
      "epoch:33 step:31202 [D loss: 0.290315, acc.: 93.75%] [G loss: 1.348130]\n",
      "epoch:33 step:31203 [D loss: 0.714284, acc.: 60.16%] [G loss: 1.392966]\n",
      "epoch:33 step:31204 [D loss: 0.286113, acc.: 91.41%] [G loss: 1.305859]\n",
      "epoch:33 step:31205 [D loss: 0.569687, acc.: 71.09%] [G loss: 1.115970]\n",
      "epoch:33 step:31206 [D loss: 0.924861, acc.: 43.75%] [G loss: 0.999234]\n",
      "epoch:33 step:31207 [D loss: 0.591363, acc.: 66.41%] [G loss: 2.025247]\n",
      "epoch:33 step:31208 [D loss: 0.430099, acc.: 80.47%] [G loss: 1.697576]\n",
      "epoch:33 step:31209 [D loss: 0.725152, acc.: 53.12%] [G loss: 1.329023]\n",
      "epoch:33 step:31210 [D loss: 0.678422, acc.: 60.16%] [G loss: 1.359576]\n",
      "epoch:33 step:31211 [D loss: 0.384859, acc.: 84.38%] [G loss: 2.115916]\n",
      "epoch:33 step:31212 [D loss: 0.864017, acc.: 48.44%] [G loss: 1.519783]\n",
      "epoch:33 step:31213 [D loss: 0.450462, acc.: 77.34%] [G loss: 1.446212]\n",
      "epoch:33 step:31214 [D loss: 0.870667, acc.: 45.31%] [G loss: 1.194665]\n",
      "epoch:33 step:31215 [D loss: 0.680403, acc.: 57.03%] [G loss: 1.693624]\n",
      "epoch:33 step:31216 [D loss: 0.729121, acc.: 59.38%] [G loss: 1.122731]\n",
      "epoch:33 step:31217 [D loss: 0.492587, acc.: 74.22%] [G loss: 1.244703]\n",
      "epoch:33 step:31218 [D loss: 0.473861, acc.: 78.12%] [G loss: 1.451369]\n",
      "epoch:33 step:31219 [D loss: 0.587698, acc.: 69.53%] [G loss: 1.085638]\n",
      "epoch:33 step:31220 [D loss: 0.636890, acc.: 65.62%] [G loss: 1.283787]\n",
      "epoch:33 step:31221 [D loss: 0.329865, acc.: 85.16%] [G loss: 1.700608]\n",
      "epoch:33 step:31222 [D loss: 0.565421, acc.: 75.00%] [G loss: 1.687751]\n",
      "epoch:33 step:31223 [D loss: 0.642607, acc.: 67.19%] [G loss: 1.513508]\n",
      "epoch:33 step:31224 [D loss: 0.541469, acc.: 67.19%] [G loss: 1.640497]\n",
      "epoch:33 step:31225 [D loss: 0.402143, acc.: 85.16%] [G loss: 1.274428]\n",
      "epoch:33 step:31226 [D loss: 0.611055, acc.: 69.53%] [G loss: 1.086315]\n",
      "epoch:33 step:31227 [D loss: 0.432252, acc.: 82.03%] [G loss: 0.906320]\n",
      "epoch:33 step:31228 [D loss: 0.364075, acc.: 85.94%] [G loss: 1.555520]\n",
      "epoch:33 step:31229 [D loss: 0.693220, acc.: 62.50%] [G loss: 1.296548]\n",
      "epoch:33 step:31230 [D loss: 0.627434, acc.: 64.06%] [G loss: 1.542295]\n",
      "epoch:33 step:31231 [D loss: 0.348814, acc.: 83.59%] [G loss: 1.801355]\n",
      "epoch:33 step:31232 [D loss: 0.389907, acc.: 85.94%] [G loss: 1.538083]\n",
      "epoch:33 step:31233 [D loss: 0.523925, acc.: 73.44%] [G loss: 1.718427]\n",
      "epoch:33 step:31234 [D loss: 0.537056, acc.: 75.78%] [G loss: 1.605761]\n",
      "epoch:33 step:31235 [D loss: 0.539560, acc.: 69.53%] [G loss: 1.472582]\n",
      "epoch:33 step:31236 [D loss: 0.377674, acc.: 85.94%] [G loss: 1.556815]\n",
      "epoch:33 step:31237 [D loss: 0.587899, acc.: 68.75%] [G loss: 1.501915]\n",
      "epoch:33 step:31238 [D loss: 0.491882, acc.: 71.88%] [G loss: 1.450492]\n",
      "epoch:33 step:31239 [D loss: 0.486295, acc.: 78.91%] [G loss: 1.587617]\n",
      "epoch:33 step:31240 [D loss: 0.576427, acc.: 67.19%] [G loss: 1.790016]\n",
      "epoch:33 step:31241 [D loss: 0.497648, acc.: 78.12%] [G loss: 1.729779]\n",
      "epoch:33 step:31242 [D loss: 0.457702, acc.: 78.12%] [G loss: 1.047929]\n",
      "epoch:33 step:31243 [D loss: 0.549490, acc.: 74.22%] [G loss: 1.317106]\n",
      "epoch:33 step:31244 [D loss: 0.719529, acc.: 59.38%] [G loss: 1.262155]\n",
      "epoch:33 step:31245 [D loss: 0.703538, acc.: 57.03%] [G loss: 1.994959]\n",
      "epoch:33 step:31246 [D loss: 0.628378, acc.: 67.97%] [G loss: 1.824955]\n",
      "epoch:33 step:31247 [D loss: 0.564073, acc.: 74.22%] [G loss: 1.481122]\n",
      "epoch:33 step:31248 [D loss: 0.414454, acc.: 82.81%] [G loss: 1.491073]\n",
      "epoch:33 step:31249 [D loss: 0.532486, acc.: 71.88%] [G loss: 1.345714]\n",
      "epoch:33 step:31250 [D loss: 0.457354, acc.: 79.69%] [G loss: 1.559872]\n",
      "epoch:33 step:31251 [D loss: 0.532816, acc.: 79.69%] [G loss: 1.628242]\n",
      "epoch:33 step:31252 [D loss: 0.342829, acc.: 89.06%] [G loss: 1.769855]\n",
      "epoch:33 step:31253 [D loss: 0.383073, acc.: 83.59%] [G loss: 1.631491]\n",
      "epoch:33 step:31254 [D loss: 0.485165, acc.: 79.69%] [G loss: 1.381800]\n",
      "epoch:33 step:31255 [D loss: 0.466585, acc.: 77.34%] [G loss: 1.824654]\n",
      "epoch:33 step:31256 [D loss: 0.414619, acc.: 82.03%] [G loss: 1.684563]\n",
      "epoch:33 step:31257 [D loss: 0.391038, acc.: 85.94%] [G loss: 1.423881]\n",
      "epoch:33 step:31258 [D loss: 0.595443, acc.: 71.09%] [G loss: 1.297185]\n",
      "epoch:33 step:31259 [D loss: 0.520084, acc.: 71.09%] [G loss: 1.192816]\n",
      "epoch:33 step:31260 [D loss: 0.626633, acc.: 64.84%] [G loss: 1.640379]\n",
      "epoch:33 step:31261 [D loss: 0.297328, acc.: 92.19%] [G loss: 1.931151]\n",
      "epoch:33 step:31262 [D loss: 0.577629, acc.: 72.66%] [G loss: 1.575314]\n",
      "epoch:33 step:31263 [D loss: 0.669283, acc.: 60.94%] [G loss: 1.180942]\n",
      "epoch:33 step:31264 [D loss: 0.473941, acc.: 77.34%] [G loss: 1.399212]\n",
      "epoch:33 step:31265 [D loss: 0.358319, acc.: 87.50%] [G loss: 1.693651]\n",
      "epoch:33 step:31266 [D loss: 0.628095, acc.: 64.84%] [G loss: 1.917811]\n",
      "epoch:33 step:31267 [D loss: 0.511679, acc.: 74.22%] [G loss: 1.093294]\n",
      "epoch:33 step:31268 [D loss: 0.569925, acc.: 70.31%] [G loss: 1.672842]\n",
      "epoch:33 step:31269 [D loss: 0.637991, acc.: 65.62%] [G loss: 1.373300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31270 [D loss: 0.619259, acc.: 70.31%] [G loss: 1.307266]\n",
      "epoch:33 step:31271 [D loss: 0.362466, acc.: 88.28%] [G loss: 1.900975]\n",
      "epoch:33 step:31272 [D loss: 0.352608, acc.: 86.72%] [G loss: 1.882787]\n",
      "epoch:33 step:31273 [D loss: 0.541669, acc.: 66.41%] [G loss: 1.640526]\n",
      "epoch:33 step:31274 [D loss: 0.491776, acc.: 79.69%] [G loss: 1.364333]\n",
      "epoch:33 step:31275 [D loss: 0.470643, acc.: 78.91%] [G loss: 1.325726]\n",
      "epoch:33 step:31276 [D loss: 0.726227, acc.: 57.03%] [G loss: 0.847859]\n",
      "epoch:33 step:31277 [D loss: 0.486755, acc.: 76.56%] [G loss: 1.801279]\n",
      "epoch:33 step:31278 [D loss: 0.633346, acc.: 70.31%] [G loss: 1.488175]\n",
      "epoch:33 step:31279 [D loss: 0.679049, acc.: 65.62%] [G loss: 1.936836]\n",
      "epoch:33 step:31280 [D loss: 0.671530, acc.: 58.59%] [G loss: 1.454047]\n",
      "epoch:33 step:31281 [D loss: 0.755401, acc.: 56.25%] [G loss: 0.955894]\n",
      "epoch:33 step:31282 [D loss: 0.441894, acc.: 78.12%] [G loss: 1.421638]\n",
      "epoch:33 step:31283 [D loss: 0.633300, acc.: 63.28%] [G loss: 1.620799]\n",
      "epoch:33 step:31284 [D loss: 0.606852, acc.: 60.16%] [G loss: 1.321236]\n",
      "epoch:33 step:31285 [D loss: 0.483732, acc.: 75.78%] [G loss: 1.996367]\n",
      "epoch:33 step:31286 [D loss: 0.630673, acc.: 66.41%] [G loss: 1.282088]\n",
      "epoch:33 step:31287 [D loss: 0.381151, acc.: 84.38%] [G loss: 1.283618]\n",
      "epoch:33 step:31288 [D loss: 0.503209, acc.: 80.47%] [G loss: 2.087372]\n",
      "epoch:33 step:31289 [D loss: 0.397471, acc.: 85.16%] [G loss: 1.674896]\n",
      "epoch:33 step:31290 [D loss: 0.574709, acc.: 70.31%] [G loss: 1.628962]\n",
      "epoch:33 step:31291 [D loss: 0.466107, acc.: 79.69%] [G loss: 1.474117]\n",
      "epoch:33 step:31292 [D loss: 0.579740, acc.: 67.97%] [G loss: 1.400301]\n",
      "epoch:33 step:31293 [D loss: 0.434882, acc.: 83.59%] [G loss: 1.621511]\n",
      "epoch:33 step:31294 [D loss: 0.488811, acc.: 72.66%] [G loss: 1.878531]\n",
      "epoch:33 step:31295 [D loss: 0.582601, acc.: 67.19%] [G loss: 1.598304]\n",
      "epoch:33 step:31296 [D loss: 0.673898, acc.: 61.72%] [G loss: 1.099991]\n",
      "epoch:33 step:31297 [D loss: 0.638498, acc.: 66.41%] [G loss: 1.281675]\n",
      "epoch:33 step:31298 [D loss: 0.375290, acc.: 85.94%] [G loss: 1.457644]\n",
      "epoch:33 step:31299 [D loss: 0.494472, acc.: 73.44%] [G loss: 1.627451]\n",
      "epoch:33 step:31300 [D loss: 0.445701, acc.: 78.91%] [G loss: 1.839936]\n",
      "epoch:33 step:31301 [D loss: 0.535925, acc.: 69.53%] [G loss: 1.640507]\n",
      "epoch:33 step:31302 [D loss: 0.612362, acc.: 69.53%] [G loss: 1.377155]\n",
      "epoch:33 step:31303 [D loss: 0.644282, acc.: 60.16%] [G loss: 1.131414]\n",
      "epoch:33 step:31304 [D loss: 0.458863, acc.: 83.59%] [G loss: 1.598163]\n",
      "epoch:33 step:31305 [D loss: 0.596286, acc.: 68.75%] [G loss: 1.820857]\n",
      "epoch:33 step:31306 [D loss: 0.603263, acc.: 70.31%] [G loss: 1.503211]\n",
      "epoch:33 step:31307 [D loss: 0.579557, acc.: 68.75%] [G loss: 1.580920]\n",
      "epoch:33 step:31308 [D loss: 0.447297, acc.: 78.12%] [G loss: 1.628825]\n",
      "epoch:33 step:31309 [D loss: 0.324047, acc.: 90.62%] [G loss: 1.633188]\n",
      "epoch:33 step:31310 [D loss: 0.678649, acc.: 64.06%] [G loss: 1.607912]\n",
      "epoch:33 step:31311 [D loss: 0.504157, acc.: 78.12%] [G loss: 1.406709]\n",
      "epoch:33 step:31312 [D loss: 0.556065, acc.: 69.53%] [G loss: 1.256274]\n",
      "epoch:33 step:31313 [D loss: 0.399718, acc.: 86.72%] [G loss: 1.668327]\n",
      "epoch:33 step:31314 [D loss: 0.516710, acc.: 75.78%] [G loss: 1.501884]\n",
      "epoch:33 step:31315 [D loss: 0.482176, acc.: 79.69%] [G loss: 1.644467]\n",
      "epoch:33 step:31316 [D loss: 0.488370, acc.: 80.47%] [G loss: 1.801802]\n",
      "epoch:33 step:31317 [D loss: 0.571991, acc.: 71.09%] [G loss: 1.590330]\n",
      "epoch:33 step:31318 [D loss: 0.639249, acc.: 64.06%] [G loss: 1.595973]\n",
      "epoch:33 step:31319 [D loss: 0.454931, acc.: 80.47%] [G loss: 1.229171]\n",
      "epoch:33 step:31320 [D loss: 0.546481, acc.: 70.31%] [G loss: 1.395076]\n",
      "epoch:33 step:31321 [D loss: 0.621744, acc.: 70.31%] [G loss: 1.413183]\n",
      "epoch:33 step:31322 [D loss: 0.531435, acc.: 73.44%] [G loss: 1.968934]\n",
      "epoch:33 step:31323 [D loss: 0.529694, acc.: 71.88%] [G loss: 1.811038]\n",
      "epoch:33 step:31324 [D loss: 0.652545, acc.: 64.06%] [G loss: 1.707188]\n",
      "epoch:33 step:31325 [D loss: 0.472125, acc.: 76.56%] [G loss: 1.499520]\n",
      "epoch:33 step:31326 [D loss: 0.457728, acc.: 76.56%] [G loss: 1.410651]\n",
      "epoch:33 step:31327 [D loss: 0.494133, acc.: 71.88%] [G loss: 1.437789]\n",
      "epoch:33 step:31328 [D loss: 0.494459, acc.: 75.00%] [G loss: 1.559529]\n",
      "epoch:33 step:31329 [D loss: 0.532638, acc.: 72.66%] [G loss: 1.920116]\n",
      "epoch:33 step:31330 [D loss: 0.658169, acc.: 64.84%] [G loss: 1.195462]\n",
      "epoch:33 step:31331 [D loss: 0.396763, acc.: 85.16%] [G loss: 1.504865]\n",
      "epoch:33 step:31332 [D loss: 0.524589, acc.: 73.44%] [G loss: 1.110041]\n",
      "epoch:33 step:31333 [D loss: 0.741113, acc.: 58.59%] [G loss: 1.445052]\n",
      "epoch:33 step:31334 [D loss: 0.709094, acc.: 51.56%] [G loss: 1.301236]\n",
      "epoch:33 step:31335 [D loss: 0.738133, acc.: 57.03%] [G loss: 1.369446]\n",
      "epoch:33 step:31336 [D loss: 0.303246, acc.: 91.41%] [G loss: 1.568196]\n",
      "epoch:33 step:31337 [D loss: 0.631093, acc.: 65.62%] [G loss: 1.050067]\n",
      "epoch:33 step:31338 [D loss: 0.525332, acc.: 73.44%] [G loss: 1.731160]\n",
      "epoch:33 step:31339 [D loss: 0.558039, acc.: 76.56%] [G loss: 1.182031]\n",
      "epoch:33 step:31340 [D loss: 0.527811, acc.: 69.53%] [G loss: 1.780324]\n",
      "epoch:33 step:31341 [D loss: 0.481208, acc.: 75.78%] [G loss: 1.478126]\n",
      "epoch:33 step:31342 [D loss: 0.441562, acc.: 78.12%] [G loss: 1.704789]\n",
      "epoch:33 step:31343 [D loss: 0.469996, acc.: 78.12%] [G loss: 1.528758]\n",
      "epoch:33 step:31344 [D loss: 0.787661, acc.: 53.91%] [G loss: 1.555372]\n",
      "epoch:33 step:31345 [D loss: 0.583038, acc.: 68.75%] [G loss: 1.307824]\n",
      "epoch:33 step:31346 [D loss: 0.510288, acc.: 77.34%] [G loss: 1.914100]\n",
      "epoch:33 step:31347 [D loss: 0.540354, acc.: 71.09%] [G loss: 1.404697]\n",
      "epoch:33 step:31348 [D loss: 0.404466, acc.: 80.47%] [G loss: 1.630997]\n",
      "epoch:33 step:31349 [D loss: 0.614034, acc.: 69.53%] [G loss: 1.523011]\n",
      "epoch:33 step:31350 [D loss: 0.539292, acc.: 75.00%] [G loss: 1.114895]\n",
      "epoch:33 step:31351 [D loss: 0.658020, acc.: 67.97%] [G loss: 1.573743]\n",
      "epoch:33 step:31352 [D loss: 0.488261, acc.: 80.47%] [G loss: 1.682008]\n",
      "epoch:33 step:31353 [D loss: 0.497670, acc.: 74.22%] [G loss: 1.694439]\n",
      "epoch:33 step:31354 [D loss: 0.532908, acc.: 74.22%] [G loss: 1.869946]\n",
      "epoch:33 step:31355 [D loss: 0.598370, acc.: 71.09%] [G loss: 1.263974]\n",
      "epoch:33 step:31356 [D loss: 0.647477, acc.: 64.06%] [G loss: 1.543916]\n",
      "epoch:33 step:31357 [D loss: 0.485333, acc.: 79.69%] [G loss: 1.442243]\n",
      "epoch:33 step:31358 [D loss: 0.712785, acc.: 57.81%] [G loss: 1.202580]\n",
      "epoch:33 step:31359 [D loss: 0.452534, acc.: 76.56%] [G loss: 2.003832]\n",
      "epoch:33 step:31360 [D loss: 0.528660, acc.: 77.34%] [G loss: 1.794514]\n",
      "epoch:33 step:31361 [D loss: 0.483634, acc.: 74.22%] [G loss: 1.584396]\n",
      "epoch:33 step:31362 [D loss: 0.527306, acc.: 70.31%] [G loss: 1.310403]\n",
      "epoch:33 step:31363 [D loss: 0.393698, acc.: 82.81%] [G loss: 1.711757]\n",
      "epoch:33 step:31364 [D loss: 0.321038, acc.: 89.84%] [G loss: 2.042377]\n",
      "epoch:33 step:31365 [D loss: 0.759514, acc.: 54.69%] [G loss: 1.605010]\n",
      "epoch:33 step:31366 [D loss: 0.513129, acc.: 75.00%] [G loss: 1.565809]\n",
      "epoch:33 step:31367 [D loss: 0.557786, acc.: 67.19%] [G loss: 1.449855]\n",
      "epoch:33 step:31368 [D loss: 0.441871, acc.: 82.03%] [G loss: 1.649707]\n",
      "epoch:33 step:31369 [D loss: 0.622504, acc.: 66.41%] [G loss: 1.426419]\n",
      "epoch:33 step:31370 [D loss: 0.627806, acc.: 66.41%] [G loss: 1.719449]\n",
      "epoch:33 step:31371 [D loss: 0.510888, acc.: 72.66%] [G loss: 1.476926]\n",
      "epoch:33 step:31372 [D loss: 0.521079, acc.: 76.56%] [G loss: 1.971091]\n",
      "epoch:33 step:31373 [D loss: 0.439588, acc.: 81.25%] [G loss: 1.401138]\n",
      "epoch:33 step:31374 [D loss: 0.366697, acc.: 85.16%] [G loss: 1.367388]\n",
      "epoch:33 step:31375 [D loss: 0.374588, acc.: 86.72%] [G loss: 1.861262]\n",
      "epoch:33 step:31376 [D loss: 0.452853, acc.: 82.81%] [G loss: 1.278561]\n",
      "epoch:33 step:31377 [D loss: 0.599149, acc.: 70.31%] [G loss: 1.182365]\n",
      "epoch:33 step:31378 [D loss: 0.691778, acc.: 59.38%] [G loss: 0.896420]\n",
      "epoch:33 step:31379 [D loss: 0.298729, acc.: 92.19%] [G loss: 1.728021]\n",
      "epoch:33 step:31380 [D loss: 0.421871, acc.: 79.69%] [G loss: 1.663968]\n",
      "epoch:33 step:31381 [D loss: 0.421344, acc.: 85.94%] [G loss: 1.709683]\n",
      "epoch:33 step:31382 [D loss: 0.739135, acc.: 59.38%] [G loss: 1.367898]\n",
      "epoch:33 step:31383 [D loss: 0.812787, acc.: 50.78%] [G loss: 1.372829]\n",
      "epoch:33 step:31384 [D loss: 0.641644, acc.: 62.50%] [G loss: 1.212944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31385 [D loss: 0.551782, acc.: 69.53%] [G loss: 1.831831]\n",
      "epoch:33 step:31386 [D loss: 0.483826, acc.: 75.00%] [G loss: 1.573517]\n",
      "epoch:33 step:31387 [D loss: 0.441478, acc.: 82.03%] [G loss: 1.025357]\n",
      "epoch:33 step:31388 [D loss: 0.397104, acc.: 78.91%] [G loss: 1.135294]\n",
      "epoch:33 step:31389 [D loss: 0.517045, acc.: 75.78%] [G loss: 1.552770]\n",
      "epoch:33 step:31390 [D loss: 0.605775, acc.: 64.84%] [G loss: 1.439131]\n",
      "epoch:33 step:31391 [D loss: 0.706095, acc.: 59.38%] [G loss: 1.387559]\n",
      "epoch:33 step:31392 [D loss: 0.366312, acc.: 88.28%] [G loss: 1.699867]\n",
      "epoch:33 step:31393 [D loss: 0.538899, acc.: 74.22%] [G loss: 1.937595]\n",
      "epoch:33 step:31394 [D loss: 0.405530, acc.: 88.28%] [G loss: 1.467759]\n",
      "epoch:33 step:31395 [D loss: 0.809632, acc.: 56.25%] [G loss: 1.377342]\n",
      "epoch:33 step:31396 [D loss: 0.461989, acc.: 77.34%] [G loss: 1.393275]\n",
      "epoch:33 step:31397 [D loss: 0.509077, acc.: 74.22%] [G loss: 1.848035]\n",
      "epoch:33 step:31398 [D loss: 0.618902, acc.: 60.16%] [G loss: 1.265425]\n",
      "epoch:33 step:31399 [D loss: 0.630223, acc.: 66.41%] [G loss: 1.470079]\n",
      "epoch:33 step:31400 [D loss: 0.441232, acc.: 80.47%] [G loss: 1.420399]\n",
      "##############\n",
      "[2.76709594 2.14888099 1.97760548 3.03375193 1.00548886 6.28897666\n",
      " 2.33744321 2.57302597 4.01074486 8.14868929]\n",
      "##########\n",
      "epoch:33 step:31401 [D loss: 0.591282, acc.: 71.88%] [G loss: 1.606726]\n",
      "epoch:33 step:31402 [D loss: 0.726730, acc.: 60.16%] [G loss: 1.996876]\n",
      "epoch:33 step:31403 [D loss: 0.618399, acc.: 61.72%] [G loss: 1.218632]\n",
      "epoch:33 step:31404 [D loss: 0.609021, acc.: 67.19%] [G loss: 1.530740]\n",
      "epoch:33 step:31405 [D loss: 0.360834, acc.: 85.94%] [G loss: 1.095775]\n",
      "epoch:33 step:31406 [D loss: 0.512533, acc.: 71.09%] [G loss: 1.695704]\n",
      "epoch:33 step:31407 [D loss: 0.529080, acc.: 71.09%] [G loss: 1.256884]\n",
      "epoch:33 step:31408 [D loss: 0.494929, acc.: 77.34%] [G loss: 1.933826]\n",
      "epoch:33 step:31409 [D loss: 0.471953, acc.: 77.34%] [G loss: 2.050245]\n",
      "epoch:33 step:31410 [D loss: 0.534279, acc.: 75.00%] [G loss: 1.533599]\n",
      "epoch:33 step:31411 [D loss: 0.371327, acc.: 82.03%] [G loss: 2.144251]\n",
      "epoch:33 step:31412 [D loss: 0.621303, acc.: 65.62%] [G loss: 1.336732]\n",
      "epoch:33 step:31413 [D loss: 0.438982, acc.: 78.91%] [G loss: 1.678827]\n",
      "epoch:33 step:31414 [D loss: 0.397591, acc.: 81.25%] [G loss: 1.777864]\n",
      "epoch:33 step:31415 [D loss: 0.607368, acc.: 66.41%] [G loss: 1.647038]\n",
      "epoch:33 step:31416 [D loss: 0.515020, acc.: 75.00%] [G loss: 2.026362]\n",
      "epoch:33 step:31417 [D loss: 0.580345, acc.: 72.66%] [G loss: 1.149200]\n",
      "epoch:33 step:31418 [D loss: 0.528440, acc.: 71.88%] [G loss: 1.308623]\n",
      "epoch:33 step:31419 [D loss: 0.471527, acc.: 77.34%] [G loss: 1.734473]\n",
      "epoch:33 step:31420 [D loss: 0.500982, acc.: 76.56%] [G loss: 1.381076]\n",
      "epoch:33 step:31421 [D loss: 0.735572, acc.: 59.38%] [G loss: 1.572802]\n",
      "epoch:33 step:31422 [D loss: 0.492917, acc.: 81.25%] [G loss: 1.768066]\n",
      "epoch:33 step:31423 [D loss: 0.464469, acc.: 80.47%] [G loss: 1.487280]\n",
      "epoch:33 step:31424 [D loss: 0.481846, acc.: 78.12%] [G loss: 1.642147]\n",
      "epoch:33 step:31425 [D loss: 0.574948, acc.: 67.97%] [G loss: 1.886286]\n",
      "epoch:33 step:31426 [D loss: 0.375678, acc.: 89.84%] [G loss: 1.448511]\n",
      "epoch:33 step:31427 [D loss: 0.367002, acc.: 89.06%] [G loss: 1.643005]\n",
      "epoch:33 step:31428 [D loss: 0.434979, acc.: 82.81%] [G loss: 1.554986]\n",
      "epoch:33 step:31429 [D loss: 0.423014, acc.: 82.81%] [G loss: 1.793889]\n",
      "epoch:33 step:31430 [D loss: 0.680326, acc.: 62.50%] [G loss: 1.653097]\n",
      "epoch:33 step:31431 [D loss: 0.369873, acc.: 85.94%] [G loss: 1.846795]\n",
      "epoch:33 step:31432 [D loss: 0.460777, acc.: 81.25%] [G loss: 2.071415]\n",
      "epoch:33 step:31433 [D loss: 0.447544, acc.: 79.69%] [G loss: 1.867495]\n",
      "epoch:33 step:31434 [D loss: 0.463129, acc.: 76.56%] [G loss: 2.052923]\n",
      "epoch:33 step:31435 [D loss: 0.456881, acc.: 75.00%] [G loss: 1.502301]\n",
      "epoch:33 step:31436 [D loss: 0.670392, acc.: 63.28%] [G loss: 1.156542]\n",
      "epoch:33 step:31437 [D loss: 0.602570, acc.: 65.62%] [G loss: 1.083913]\n",
      "epoch:33 step:31438 [D loss: 0.385533, acc.: 85.94%] [G loss: 1.674232]\n",
      "epoch:33 step:31439 [D loss: 0.463198, acc.: 79.69%] [G loss: 1.221532]\n",
      "epoch:33 step:31440 [D loss: 0.588347, acc.: 69.53%] [G loss: 1.155290]\n",
      "epoch:33 step:31441 [D loss: 0.502348, acc.: 75.78%] [G loss: 1.407080]\n",
      "epoch:33 step:31442 [D loss: 0.532008, acc.: 75.00%] [G loss: 1.462166]\n",
      "epoch:33 step:31443 [D loss: 0.693526, acc.: 64.84%] [G loss: 1.202252]\n",
      "epoch:33 step:31444 [D loss: 0.417923, acc.: 82.81%] [G loss: 1.487401]\n",
      "epoch:33 step:31445 [D loss: 0.583099, acc.: 71.88%] [G loss: 1.404475]\n",
      "epoch:33 step:31446 [D loss: 0.727677, acc.: 52.34%] [G loss: 1.311440]\n",
      "epoch:33 step:31447 [D loss: 0.491938, acc.: 78.91%] [G loss: 1.572657]\n",
      "epoch:33 step:31448 [D loss: 0.510509, acc.: 75.00%] [G loss: 1.547314]\n",
      "epoch:33 step:31449 [D loss: 0.347584, acc.: 89.06%] [G loss: 2.163449]\n",
      "epoch:33 step:31450 [D loss: 0.472037, acc.: 82.03%] [G loss: 1.430391]\n",
      "epoch:33 step:31451 [D loss: 0.422312, acc.: 82.03%] [G loss: 1.334317]\n",
      "epoch:33 step:31452 [D loss: 0.433612, acc.: 79.69%] [G loss: 1.695833]\n",
      "epoch:33 step:31453 [D loss: 0.628894, acc.: 68.75%] [G loss: 1.526061]\n",
      "epoch:33 step:31454 [D loss: 0.776848, acc.: 53.12%] [G loss: 1.108156]\n",
      "epoch:33 step:31455 [D loss: 0.713691, acc.: 57.03%] [G loss: 1.609334]\n",
      "epoch:33 step:31456 [D loss: 0.631163, acc.: 67.97%] [G loss: 1.544874]\n",
      "epoch:33 step:31457 [D loss: 0.591694, acc.: 71.88%] [G loss: 2.149440]\n",
      "epoch:33 step:31458 [D loss: 0.532453, acc.: 71.09%] [G loss: 1.407040]\n",
      "epoch:33 step:31459 [D loss: 0.851163, acc.: 52.34%] [G loss: 1.405156]\n",
      "epoch:33 step:31460 [D loss: 0.539750, acc.: 72.66%] [G loss: 1.540649]\n",
      "epoch:33 step:31461 [D loss: 0.370758, acc.: 85.94%] [G loss: 1.580797]\n",
      "epoch:33 step:31462 [D loss: 0.415988, acc.: 81.25%] [G loss: 1.671428]\n",
      "epoch:33 step:31463 [D loss: 0.602970, acc.: 67.19%] [G loss: 1.391202]\n",
      "epoch:33 step:31464 [D loss: 0.499037, acc.: 78.91%] [G loss: 1.246627]\n",
      "epoch:33 step:31465 [D loss: 0.405373, acc.: 85.16%] [G loss: 0.954505]\n",
      "epoch:33 step:31466 [D loss: 0.708656, acc.: 62.50%] [G loss: 1.113078]\n",
      "epoch:33 step:31467 [D loss: 0.414539, acc.: 83.59%] [G loss: 1.625408]\n",
      "epoch:33 step:31468 [D loss: 0.411439, acc.: 78.91%] [G loss: 1.704526]\n",
      "epoch:33 step:31469 [D loss: 0.372420, acc.: 82.81%] [G loss: 1.925321]\n",
      "epoch:33 step:31470 [D loss: 0.508910, acc.: 77.34%] [G loss: 1.250718]\n",
      "epoch:33 step:31471 [D loss: 0.536704, acc.: 67.97%] [G loss: 1.636647]\n",
      "epoch:33 step:31472 [D loss: 0.621037, acc.: 65.62%] [G loss: 1.510178]\n",
      "epoch:33 step:31473 [D loss: 0.570589, acc.: 71.09%] [G loss: 1.750433]\n",
      "epoch:33 step:31474 [D loss: 0.505558, acc.: 77.34%] [G loss: 1.557005]\n",
      "epoch:33 step:31475 [D loss: 0.666118, acc.: 65.62%] [G loss: 1.575527]\n",
      "epoch:33 step:31476 [D loss: 0.405318, acc.: 85.94%] [G loss: 1.849293]\n",
      "epoch:33 step:31477 [D loss: 0.530401, acc.: 77.34%] [G loss: 2.077223]\n",
      "epoch:33 step:31478 [D loss: 0.623894, acc.: 62.50%] [G loss: 1.076844]\n",
      "epoch:33 step:31479 [D loss: 0.528188, acc.: 72.66%] [G loss: 1.349754]\n",
      "epoch:33 step:31480 [D loss: 0.493943, acc.: 80.47%] [G loss: 1.798063]\n",
      "epoch:33 step:31481 [D loss: 0.516105, acc.: 72.66%] [G loss: 1.730674]\n",
      "epoch:33 step:31482 [D loss: 0.584682, acc.: 70.31%] [G loss: 1.461406]\n",
      "epoch:33 step:31483 [D loss: 0.517683, acc.: 74.22%] [G loss: 1.762746]\n",
      "epoch:33 step:31484 [D loss: 0.535496, acc.: 74.22%] [G loss: 1.586072]\n",
      "epoch:33 step:31485 [D loss: 0.556167, acc.: 66.41%] [G loss: 1.286379]\n",
      "epoch:33 step:31486 [D loss: 0.403298, acc.: 84.38%] [G loss: 1.774928]\n",
      "epoch:33 step:31487 [D loss: 0.385490, acc.: 82.03%] [G loss: 1.738397]\n",
      "epoch:33 step:31488 [D loss: 0.515258, acc.: 73.44%] [G loss: 1.435099]\n",
      "epoch:33 step:31489 [D loss: 0.526705, acc.: 70.31%] [G loss: 1.166983]\n",
      "epoch:33 step:31490 [D loss: 0.668410, acc.: 64.06%] [G loss: 1.317745]\n",
      "epoch:33 step:31491 [D loss: 0.394810, acc.: 86.72%] [G loss: 1.698797]\n",
      "epoch:33 step:31492 [D loss: 0.475271, acc.: 77.34%] [G loss: 2.064923]\n",
      "epoch:33 step:31493 [D loss: 0.269482, acc.: 93.75%] [G loss: 2.018029]\n",
      "epoch:33 step:31494 [D loss: 0.594441, acc.: 68.75%] [G loss: 1.309571]\n",
      "epoch:33 step:31495 [D loss: 0.694095, acc.: 58.59%] [G loss: 1.212306]\n",
      "epoch:33 step:31496 [D loss: 0.616576, acc.: 67.19%] [G loss: 1.450889]\n",
      "epoch:33 step:31497 [D loss: 0.453370, acc.: 81.25%] [G loss: 1.207726]\n",
      "epoch:33 step:31498 [D loss: 0.496686, acc.: 76.56%] [G loss: 1.378157]\n",
      "epoch:33 step:31499 [D loss: 0.370348, acc.: 86.72%] [G loss: 2.341961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31500 [D loss: 0.782586, acc.: 57.03%] [G loss: 1.180746]\n",
      "epoch:33 step:31501 [D loss: 0.602848, acc.: 64.84%] [G loss: 1.651004]\n",
      "epoch:33 step:31502 [D loss: 0.691339, acc.: 58.59%] [G loss: 1.041146]\n",
      "epoch:33 step:31503 [D loss: 0.450848, acc.: 80.47%] [G loss: 1.016242]\n",
      "epoch:33 step:31504 [D loss: 0.456844, acc.: 81.25%] [G loss: 1.282484]\n",
      "epoch:33 step:31505 [D loss: 0.442422, acc.: 80.47%] [G loss: 1.649510]\n",
      "epoch:33 step:31506 [D loss: 0.430644, acc.: 82.03%] [G loss: 1.759330]\n",
      "epoch:33 step:31507 [D loss: 0.413914, acc.: 82.81%] [G loss: 1.625082]\n",
      "epoch:33 step:31508 [D loss: 0.454665, acc.: 80.47%] [G loss: 1.627614]\n",
      "epoch:33 step:31509 [D loss: 0.589866, acc.: 71.09%] [G loss: 1.419541]\n",
      "epoch:33 step:31510 [D loss: 0.605830, acc.: 67.19%] [G loss: 1.240206]\n",
      "epoch:33 step:31511 [D loss: 0.432484, acc.: 77.34%] [G loss: 1.470037]\n",
      "epoch:33 step:31512 [D loss: 0.500720, acc.: 75.78%] [G loss: 2.066724]\n",
      "epoch:33 step:31513 [D loss: 0.326369, acc.: 89.06%] [G loss: 1.671965]\n",
      "epoch:33 step:31514 [D loss: 0.543653, acc.: 71.88%] [G loss: 1.385367]\n",
      "epoch:33 step:31515 [D loss: 0.453364, acc.: 78.12%] [G loss: 1.071430]\n",
      "epoch:33 step:31516 [D loss: 0.386780, acc.: 85.94%] [G loss: 1.173025]\n",
      "epoch:33 step:31517 [D loss: 0.378662, acc.: 85.16%] [G loss: 1.845322]\n",
      "epoch:33 step:31518 [D loss: 0.391863, acc.: 87.50%] [G loss: 1.718109]\n",
      "epoch:33 step:31519 [D loss: 0.602808, acc.: 68.75%] [G loss: 1.277977]\n",
      "epoch:33 step:31520 [D loss: 0.360662, acc.: 85.94%] [G loss: 1.778973]\n",
      "epoch:33 step:31521 [D loss: 0.261727, acc.: 92.19%] [G loss: 1.464859]\n",
      "epoch:33 step:31522 [D loss: 0.651955, acc.: 64.84%] [G loss: 1.494783]\n",
      "epoch:33 step:31523 [D loss: 0.625256, acc.: 73.44%] [G loss: 1.542346]\n",
      "epoch:33 step:31524 [D loss: 0.609980, acc.: 68.75%] [G loss: 1.267243]\n",
      "epoch:33 step:31525 [D loss: 0.523724, acc.: 76.56%] [G loss: 1.589534]\n",
      "epoch:33 step:31526 [D loss: 0.795423, acc.: 51.56%] [G loss: 1.453764]\n",
      "epoch:33 step:31527 [D loss: 0.562796, acc.: 67.19%] [G loss: 1.452659]\n",
      "epoch:33 step:31528 [D loss: 0.515165, acc.: 75.78%] [G loss: 1.568928]\n",
      "epoch:33 step:31529 [D loss: 0.501549, acc.: 76.56%] [G loss: 1.480282]\n",
      "epoch:33 step:31530 [D loss: 0.634166, acc.: 66.41%] [G loss: 1.601907]\n",
      "epoch:33 step:31531 [D loss: 0.647606, acc.: 62.50%] [G loss: 1.635936]\n",
      "epoch:33 step:31532 [D loss: 0.460983, acc.: 78.91%] [G loss: 1.568715]\n",
      "epoch:33 step:31533 [D loss: 0.656801, acc.: 65.62%] [G loss: 1.072432]\n",
      "epoch:33 step:31534 [D loss: 0.515298, acc.: 72.66%] [G loss: 1.134684]\n",
      "epoch:33 step:31535 [D loss: 0.619340, acc.: 67.19%] [G loss: 1.807972]\n",
      "epoch:33 step:31536 [D loss: 0.647722, acc.: 64.84%] [G loss: 1.420627]\n",
      "epoch:33 step:31537 [D loss: 0.491058, acc.: 79.69%] [G loss: 1.610817]\n",
      "epoch:33 step:31538 [D loss: 0.440371, acc.: 83.59%] [G loss: 1.579403]\n",
      "epoch:33 step:31539 [D loss: 0.425076, acc.: 81.25%] [G loss: 1.937405]\n",
      "epoch:33 step:31540 [D loss: 0.440568, acc.: 79.69%] [G loss: 1.450530]\n",
      "epoch:33 step:31541 [D loss: 0.408392, acc.: 84.38%] [G loss: 1.329571]\n",
      "epoch:33 step:31542 [D loss: 0.665733, acc.: 60.94%] [G loss: 1.447585]\n",
      "epoch:33 step:31543 [D loss: 0.809341, acc.: 53.12%] [G loss: 1.284143]\n",
      "epoch:33 step:31544 [D loss: 0.548562, acc.: 75.00%] [G loss: 1.182265]\n",
      "epoch:33 step:31545 [D loss: 0.566215, acc.: 72.66%] [G loss: 1.990727]\n",
      "epoch:33 step:31546 [D loss: 0.486130, acc.: 78.91%] [G loss: 1.264574]\n",
      "epoch:33 step:31547 [D loss: 0.505795, acc.: 75.78%] [G loss: 1.437014]\n",
      "epoch:33 step:31548 [D loss: 0.647648, acc.: 69.53%] [G loss: 0.974920]\n",
      "epoch:33 step:31549 [D loss: 0.464501, acc.: 79.69%] [G loss: 1.819968]\n",
      "epoch:33 step:31550 [D loss: 0.560592, acc.: 67.19%] [G loss: 1.308072]\n",
      "epoch:33 step:31551 [D loss: 0.499945, acc.: 80.47%] [G loss: 1.772831]\n",
      "epoch:33 step:31552 [D loss: 0.534770, acc.: 71.88%] [G loss: 2.078947]\n",
      "epoch:33 step:31553 [D loss: 0.632503, acc.: 61.72%] [G loss: 1.640309]\n",
      "epoch:33 step:31554 [D loss: 0.464748, acc.: 80.47%] [G loss: 1.840113]\n",
      "epoch:33 step:31555 [D loss: 0.593602, acc.: 67.97%] [G loss: 1.319490]\n",
      "epoch:33 step:31556 [D loss: 0.526558, acc.: 71.09%] [G loss: 1.193479]\n",
      "epoch:33 step:31557 [D loss: 0.351198, acc.: 85.94%] [G loss: 2.064597]\n",
      "epoch:33 step:31558 [D loss: 0.550221, acc.: 68.75%] [G loss: 1.579350]\n",
      "epoch:33 step:31559 [D loss: 0.448655, acc.: 78.12%] [G loss: 1.093666]\n",
      "epoch:33 step:31560 [D loss: 0.597566, acc.: 68.75%] [G loss: 1.535354]\n",
      "epoch:33 step:31561 [D loss: 0.277755, acc.: 95.31%] [G loss: 1.644149]\n",
      "epoch:33 step:31562 [D loss: 0.594645, acc.: 71.88%] [G loss: 1.592570]\n",
      "epoch:33 step:31563 [D loss: 0.594885, acc.: 69.53%] [G loss: 1.454423]\n",
      "epoch:33 step:31564 [D loss: 0.644901, acc.: 67.19%] [G loss: 1.462589]\n",
      "epoch:33 step:31565 [D loss: 0.679161, acc.: 61.72%] [G loss: 1.143837]\n",
      "epoch:33 step:31566 [D loss: 0.508048, acc.: 77.34%] [G loss: 1.440571]\n",
      "epoch:33 step:31567 [D loss: 0.753619, acc.: 59.38%] [G loss: 1.004483]\n",
      "epoch:33 step:31568 [D loss: 0.648515, acc.: 58.59%] [G loss: 1.520342]\n",
      "epoch:33 step:31569 [D loss: 0.355196, acc.: 89.84%] [G loss: 1.813398]\n",
      "epoch:33 step:31570 [D loss: 0.372763, acc.: 85.16%] [G loss: 1.641722]\n",
      "epoch:33 step:31571 [D loss: 0.501315, acc.: 78.12%] [G loss: 1.682277]\n",
      "epoch:33 step:31572 [D loss: 0.415225, acc.: 80.47%] [G loss: 1.575262]\n",
      "epoch:33 step:31573 [D loss: 0.637641, acc.: 63.28%] [G loss: 1.925654]\n",
      "epoch:33 step:31574 [D loss: 0.654385, acc.: 68.75%] [G loss: 1.431120]\n",
      "epoch:33 step:31575 [D loss: 0.666789, acc.: 60.16%] [G loss: 1.378507]\n",
      "epoch:33 step:31576 [D loss: 0.564176, acc.: 70.31%] [G loss: 1.285255]\n",
      "epoch:33 step:31577 [D loss: 0.315306, acc.: 92.97%] [G loss: 1.477441]\n",
      "epoch:33 step:31578 [D loss: 0.403217, acc.: 84.38%] [G loss: 1.903902]\n",
      "epoch:33 step:31579 [D loss: 0.569911, acc.: 71.88%] [G loss: 1.294669]\n",
      "epoch:33 step:31580 [D loss: 0.503866, acc.: 73.44%] [G loss: 1.618412]\n",
      "epoch:33 step:31581 [D loss: 0.434359, acc.: 75.00%] [G loss: 1.582441]\n",
      "epoch:33 step:31582 [D loss: 0.546042, acc.: 74.22%] [G loss: 1.816164]\n",
      "epoch:33 step:31583 [D loss: 0.632850, acc.: 64.06%] [G loss: 1.843287]\n",
      "epoch:33 step:31584 [D loss: 0.684620, acc.: 61.72%] [G loss: 2.087284]\n",
      "epoch:33 step:31585 [D loss: 0.900387, acc.: 50.00%] [G loss: 0.904886]\n",
      "epoch:33 step:31586 [D loss: 0.550109, acc.: 77.34%] [G loss: 1.503512]\n",
      "epoch:33 step:31587 [D loss: 0.359465, acc.: 87.50%] [G loss: 1.832782]\n",
      "epoch:33 step:31588 [D loss: 0.428730, acc.: 83.59%] [G loss: 1.518397]\n",
      "epoch:33 step:31589 [D loss: 0.525648, acc.: 71.88%] [G loss: 1.287816]\n",
      "epoch:33 step:31590 [D loss: 0.736156, acc.: 63.28%] [G loss: 1.505398]\n",
      "epoch:33 step:31591 [D loss: 0.619121, acc.: 66.41%] [G loss: 1.533527]\n",
      "epoch:33 step:31592 [D loss: 0.444498, acc.: 78.12%] [G loss: 1.941025]\n",
      "epoch:33 step:31593 [D loss: 0.382837, acc.: 83.59%] [G loss: 1.973078]\n",
      "epoch:33 step:31594 [D loss: 0.619561, acc.: 68.75%] [G loss: 1.095526]\n",
      "epoch:33 step:31595 [D loss: 0.445759, acc.: 83.59%] [G loss: 1.082628]\n",
      "epoch:33 step:31596 [D loss: 0.406851, acc.: 85.16%] [G loss: 1.284051]\n",
      "epoch:33 step:31597 [D loss: 0.460082, acc.: 81.25%] [G loss: 0.922027]\n",
      "epoch:33 step:31598 [D loss: 0.508011, acc.: 81.25%] [G loss: 1.241129]\n",
      "epoch:33 step:31599 [D loss: 0.513795, acc.: 76.56%] [G loss: 1.306660]\n",
      "epoch:33 step:31600 [D loss: 0.562165, acc.: 69.53%] [G loss: 1.765044]\n",
      "##############\n",
      "[2.76701539 2.11264999 1.89334679 3.07500066 1.14307694 5.877177\n",
      " 2.4801791  2.51057297 4.03341393 4.95807861]\n",
      "##########\n",
      "epoch:33 step:31601 [D loss: 0.573632, acc.: 67.19%] [G loss: 1.218483]\n",
      "epoch:33 step:31602 [D loss: 0.521414, acc.: 75.00%] [G loss: 1.711577]\n",
      "epoch:33 step:31603 [D loss: 0.541573, acc.: 70.31%] [G loss: 1.230250]\n",
      "epoch:33 step:31604 [D loss: 0.922372, acc.: 46.09%] [G loss: 0.795279]\n",
      "epoch:33 step:31605 [D loss: 0.442826, acc.: 82.03%] [G loss: 1.773075]\n",
      "epoch:33 step:31606 [D loss: 0.788646, acc.: 51.56%] [G loss: 1.110315]\n",
      "epoch:33 step:31607 [D loss: 0.783495, acc.: 53.12%] [G loss: 1.639872]\n",
      "epoch:33 step:31608 [D loss: 0.486289, acc.: 73.44%] [G loss: 1.458516]\n",
      "epoch:33 step:31609 [D loss: 0.636357, acc.: 64.84%] [G loss: 1.847299]\n",
      "epoch:33 step:31610 [D loss: 0.665017, acc.: 56.25%] [G loss: 1.802235]\n",
      "epoch:33 step:31611 [D loss: 0.627010, acc.: 68.75%] [G loss: 1.251590]\n",
      "epoch:33 step:31612 [D loss: 0.478854, acc.: 76.56%] [G loss: 1.343475]\n",
      "epoch:33 step:31613 [D loss: 0.719924, acc.: 59.38%] [G loss: 1.630278]\n",
      "epoch:33 step:31614 [D loss: 0.563570, acc.: 68.75%] [G loss: 1.513827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31615 [D loss: 0.763358, acc.: 55.47%] [G loss: 1.281763]\n",
      "epoch:33 step:31616 [D loss: 0.637139, acc.: 65.62%] [G loss: 1.370072]\n",
      "epoch:33 step:31617 [D loss: 0.446354, acc.: 81.25%] [G loss: 1.833417]\n",
      "epoch:33 step:31618 [D loss: 0.521615, acc.: 71.09%] [G loss: 1.794619]\n",
      "epoch:33 step:31619 [D loss: 0.519992, acc.: 75.78%] [G loss: 0.874020]\n",
      "epoch:33 step:31620 [D loss: 0.417672, acc.: 80.47%] [G loss: 1.202086]\n",
      "epoch:33 step:31621 [D loss: 0.635369, acc.: 68.75%] [G loss: 1.164090]\n",
      "epoch:33 step:31622 [D loss: 0.480443, acc.: 77.34%] [G loss: 1.356750]\n",
      "epoch:33 step:31623 [D loss: 0.436750, acc.: 79.69%] [G loss: 2.007742]\n",
      "epoch:33 step:31624 [D loss: 0.693497, acc.: 57.81%] [G loss: 1.464950]\n",
      "epoch:33 step:31625 [D loss: 0.664125, acc.: 62.50%] [G loss: 1.729680]\n",
      "epoch:33 step:31626 [D loss: 0.474209, acc.: 78.91%] [G loss: 1.737310]\n",
      "epoch:33 step:31627 [D loss: 0.535793, acc.: 74.22%] [G loss: 1.479600]\n",
      "epoch:33 step:31628 [D loss: 0.594890, acc.: 66.41%] [G loss: 1.669554]\n",
      "epoch:33 step:31629 [D loss: 0.413988, acc.: 84.38%] [G loss: 1.523587]\n",
      "epoch:33 step:31630 [D loss: 0.657395, acc.: 64.84%] [G loss: 1.669349]\n",
      "epoch:33 step:31631 [D loss: 0.694398, acc.: 60.16%] [G loss: 1.196573]\n",
      "epoch:33 step:31632 [D loss: 0.506906, acc.: 71.88%] [G loss: 1.550281]\n",
      "epoch:33 step:31633 [D loss: 0.472702, acc.: 74.22%] [G loss: 2.025739]\n",
      "epoch:33 step:31634 [D loss: 0.525017, acc.: 71.09%] [G loss: 1.134869]\n",
      "epoch:33 step:31635 [D loss: 0.551810, acc.: 72.66%] [G loss: 0.895550]\n",
      "epoch:33 step:31636 [D loss: 0.670077, acc.: 60.94%] [G loss: 1.523514]\n",
      "epoch:33 step:31637 [D loss: 0.429067, acc.: 81.25%] [G loss: 1.379656]\n",
      "epoch:33 step:31638 [D loss: 0.799646, acc.: 48.44%] [G loss: 1.452248]\n",
      "epoch:33 step:31639 [D loss: 0.334128, acc.: 89.06%] [G loss: 2.258871]\n",
      "epoch:33 step:31640 [D loss: 0.656312, acc.: 64.84%] [G loss: 1.403867]\n",
      "epoch:33 step:31641 [D loss: 0.576324, acc.: 69.53%] [G loss: 1.669627]\n",
      "epoch:33 step:31642 [D loss: 0.630634, acc.: 67.19%] [G loss: 1.601436]\n",
      "epoch:33 step:31643 [D loss: 0.715361, acc.: 65.62%] [G loss: 1.505878]\n",
      "epoch:33 step:31644 [D loss: 0.538692, acc.: 71.88%] [G loss: 1.663200]\n",
      "epoch:33 step:31645 [D loss: 0.453111, acc.: 82.03%] [G loss: 1.495036]\n",
      "epoch:33 step:31646 [D loss: 0.504565, acc.: 81.25%] [G loss: 1.398945]\n",
      "epoch:33 step:31647 [D loss: 0.644981, acc.: 68.75%] [G loss: 1.466920]\n",
      "epoch:33 step:31648 [D loss: 0.424664, acc.: 82.81%] [G loss: 1.445282]\n",
      "epoch:33 step:31649 [D loss: 0.399094, acc.: 86.72%] [G loss: 1.906613]\n",
      "epoch:33 step:31650 [D loss: 0.491769, acc.: 78.12%] [G loss: 1.643713]\n",
      "epoch:33 step:31651 [D loss: 0.730324, acc.: 57.81%] [G loss: 1.245388]\n",
      "epoch:33 step:31652 [D loss: 0.720188, acc.: 59.38%] [G loss: 1.162584]\n",
      "epoch:33 step:31653 [D loss: 0.521189, acc.: 78.12%] [G loss: 1.520255]\n",
      "epoch:33 step:31654 [D loss: 0.638853, acc.: 65.62%] [G loss: 1.891104]\n",
      "epoch:33 step:31655 [D loss: 0.515425, acc.: 70.31%] [G loss: 1.843445]\n",
      "epoch:33 step:31656 [D loss: 0.419864, acc.: 83.59%] [G loss: 1.062315]\n",
      "epoch:33 step:31657 [D loss: 0.312692, acc.: 88.28%] [G loss: 1.769548]\n",
      "epoch:33 step:31658 [D loss: 0.741244, acc.: 60.16%] [G loss: 1.441259]\n",
      "epoch:33 step:31659 [D loss: 0.639616, acc.: 67.19%] [G loss: 1.479416]\n",
      "epoch:33 step:31660 [D loss: 0.557580, acc.: 73.44%] [G loss: 1.599764]\n",
      "epoch:33 step:31661 [D loss: 0.619671, acc.: 68.75%] [G loss: 1.556873]\n",
      "epoch:33 step:31662 [D loss: 0.379952, acc.: 88.28%] [G loss: 1.469616]\n",
      "epoch:33 step:31663 [D loss: 0.526964, acc.: 77.34%] [G loss: 1.449295]\n",
      "epoch:33 step:31664 [D loss: 0.759338, acc.: 52.34%] [G loss: 1.883765]\n",
      "epoch:33 step:31665 [D loss: 0.450830, acc.: 82.81%] [G loss: 1.439456]\n",
      "epoch:33 step:31666 [D loss: 0.681789, acc.: 64.06%] [G loss: 1.957434]\n",
      "epoch:33 step:31667 [D loss: 0.541926, acc.: 75.78%] [G loss: 2.180462]\n",
      "epoch:33 step:31668 [D loss: 0.407542, acc.: 81.25%] [G loss: 1.823220]\n",
      "epoch:33 step:31669 [D loss: 0.369312, acc.: 84.38%] [G loss: 1.532357]\n",
      "epoch:33 step:31670 [D loss: 0.561411, acc.: 68.75%] [G loss: 1.121392]\n",
      "epoch:33 step:31671 [D loss: 0.494885, acc.: 75.78%] [G loss: 1.512813]\n",
      "epoch:33 step:31672 [D loss: 0.409059, acc.: 85.16%] [G loss: 1.487310]\n",
      "epoch:33 step:31673 [D loss: 0.518285, acc.: 72.66%] [G loss: 1.568144]\n",
      "epoch:33 step:31674 [D loss: 0.297044, acc.: 91.41%] [G loss: 1.540905]\n",
      "epoch:33 step:31675 [D loss: 0.460762, acc.: 79.69%] [G loss: 1.180153]\n",
      "epoch:33 step:31676 [D loss: 0.497338, acc.: 71.88%] [G loss: 1.373860]\n",
      "epoch:33 step:31677 [D loss: 0.378170, acc.: 85.16%] [G loss: 1.388039]\n",
      "epoch:33 step:31678 [D loss: 0.624098, acc.: 67.19%] [G loss: 1.284460]\n",
      "epoch:33 step:31679 [D loss: 0.663701, acc.: 63.28%] [G loss: 1.242468]\n",
      "epoch:33 step:31680 [D loss: 0.413014, acc.: 86.72%] [G loss: 1.473383]\n",
      "epoch:33 step:31681 [D loss: 0.483696, acc.: 78.91%] [G loss: 1.971311]\n",
      "epoch:33 step:31682 [D loss: 0.588862, acc.: 71.09%] [G loss: 1.273873]\n",
      "epoch:33 step:31683 [D loss: 0.593589, acc.: 71.09%] [G loss: 1.716792]\n",
      "epoch:33 step:31684 [D loss: 0.361328, acc.: 88.28%] [G loss: 1.576254]\n",
      "epoch:33 step:31685 [D loss: 0.530863, acc.: 76.56%] [G loss: 1.961085]\n",
      "epoch:33 step:31686 [D loss: 0.571498, acc.: 67.97%] [G loss: 1.538957]\n",
      "epoch:33 step:31687 [D loss: 0.579399, acc.: 67.19%] [G loss: 1.884378]\n",
      "epoch:33 step:31688 [D loss: 0.310231, acc.: 88.28%] [G loss: 1.377422]\n",
      "epoch:33 step:31689 [D loss: 0.461254, acc.: 81.25%] [G loss: 1.405730]\n",
      "epoch:33 step:31690 [D loss: 0.573124, acc.: 70.31%] [G loss: 1.374996]\n",
      "epoch:33 step:31691 [D loss: 0.549900, acc.: 72.66%] [G loss: 1.319000]\n",
      "epoch:33 step:31692 [D loss: 0.713476, acc.: 54.69%] [G loss: 1.585124]\n",
      "epoch:33 step:31693 [D loss: 0.421046, acc.: 84.38%] [G loss: 2.012808]\n",
      "epoch:33 step:31694 [D loss: 0.528938, acc.: 74.22%] [G loss: 1.837142]\n",
      "epoch:33 step:31695 [D loss: 0.740614, acc.: 53.12%] [G loss: 1.373259]\n",
      "epoch:33 step:31696 [D loss: 0.475287, acc.: 78.91%] [G loss: 1.452186]\n",
      "epoch:33 step:31697 [D loss: 0.450586, acc.: 77.34%] [G loss: 1.649169]\n",
      "epoch:33 step:31698 [D loss: 0.431707, acc.: 84.38%] [G loss: 1.454992]\n",
      "epoch:33 step:31699 [D loss: 0.507348, acc.: 73.44%] [G loss: 1.339959]\n",
      "epoch:33 step:31700 [D loss: 0.451293, acc.: 81.25%] [G loss: 1.235173]\n",
      "epoch:33 step:31701 [D loss: 0.480546, acc.: 78.12%] [G loss: 1.232283]\n",
      "epoch:33 step:31702 [D loss: 0.508011, acc.: 75.78%] [G loss: 1.187571]\n",
      "epoch:33 step:31703 [D loss: 0.507876, acc.: 72.66%] [G loss: 1.028696]\n",
      "epoch:33 step:31704 [D loss: 0.358888, acc.: 87.50%] [G loss: 1.962750]\n",
      "epoch:33 step:31705 [D loss: 0.333600, acc.: 87.50%] [G loss: 1.582646]\n",
      "epoch:33 step:31706 [D loss: 0.358362, acc.: 85.94%] [G loss: 1.328006]\n",
      "epoch:33 step:31707 [D loss: 0.500625, acc.: 75.78%] [G loss: 1.482450]\n",
      "epoch:33 step:31708 [D loss: 0.620250, acc.: 65.62%] [G loss: 1.399074]\n",
      "epoch:33 step:31709 [D loss: 0.631800, acc.: 70.31%] [G loss: 1.327302]\n",
      "epoch:33 step:31710 [D loss: 0.394840, acc.: 83.59%] [G loss: 1.528009]\n",
      "epoch:33 step:31711 [D loss: 0.575155, acc.: 71.88%] [G loss: 1.804049]\n",
      "epoch:33 step:31712 [D loss: 0.673808, acc.: 64.06%] [G loss: 2.022337]\n",
      "epoch:33 step:31713 [D loss: 0.514260, acc.: 72.66%] [G loss: 1.348660]\n",
      "epoch:33 step:31714 [D loss: 0.598564, acc.: 67.19%] [G loss: 1.342847]\n",
      "epoch:33 step:31715 [D loss: 0.414551, acc.: 83.59%] [G loss: 1.529121]\n",
      "epoch:33 step:31716 [D loss: 0.492800, acc.: 74.22%] [G loss: 1.238047]\n",
      "epoch:33 step:31717 [D loss: 0.464207, acc.: 75.78%] [G loss: 1.783174]\n",
      "epoch:33 step:31718 [D loss: 0.366153, acc.: 86.72%] [G loss: 1.941402]\n",
      "epoch:33 step:31719 [D loss: 0.434210, acc.: 82.03%] [G loss: 1.504035]\n",
      "epoch:33 step:31720 [D loss: 0.527979, acc.: 77.34%] [G loss: 1.221355]\n",
      "epoch:33 step:31721 [D loss: 0.572082, acc.: 67.97%] [G loss: 1.157152]\n",
      "epoch:33 step:31722 [D loss: 0.626392, acc.: 62.50%] [G loss: 1.606969]\n",
      "epoch:33 step:31723 [D loss: 0.665614, acc.: 66.41%] [G loss: 1.327989]\n",
      "epoch:33 step:31724 [D loss: 0.583969, acc.: 73.44%] [G loss: 1.283169]\n",
      "epoch:33 step:31725 [D loss: 0.348149, acc.: 85.16%] [G loss: 2.088870]\n",
      "epoch:33 step:31726 [D loss: 0.588028, acc.: 66.41%] [G loss: 1.688636]\n",
      "epoch:33 step:31727 [D loss: 0.424887, acc.: 85.16%] [G loss: 1.715151]\n",
      "epoch:33 step:31728 [D loss: 0.435608, acc.: 82.81%] [G loss: 1.693859]\n",
      "epoch:33 step:31729 [D loss: 0.457147, acc.: 83.59%] [G loss: 1.637765]\n",
      "epoch:33 step:31730 [D loss: 0.451541, acc.: 80.47%] [G loss: 1.214547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31731 [D loss: 0.251153, acc.: 93.75%] [G loss: 2.195694]\n",
      "epoch:33 step:31732 [D loss: 0.588134, acc.: 70.31%] [G loss: 1.252318]\n",
      "epoch:33 step:31733 [D loss: 0.343930, acc.: 89.06%] [G loss: 1.646905]\n",
      "epoch:33 step:31734 [D loss: 0.535267, acc.: 75.00%] [G loss: 1.140236]\n",
      "epoch:33 step:31735 [D loss: 0.348747, acc.: 85.16%] [G loss: 1.928996]\n",
      "epoch:33 step:31736 [D loss: 0.520047, acc.: 74.22%] [G loss: 1.561934]\n",
      "epoch:33 step:31737 [D loss: 0.406600, acc.: 81.25%] [G loss: 1.667199]\n",
      "epoch:33 step:31738 [D loss: 0.714897, acc.: 62.50%] [G loss: 1.644163]\n",
      "epoch:33 step:31739 [D loss: 0.511629, acc.: 74.22%] [G loss: 1.779466]\n",
      "epoch:33 step:31740 [D loss: 0.418091, acc.: 82.81%] [G loss: 1.353749]\n",
      "epoch:33 step:31741 [D loss: 0.603322, acc.: 63.28%] [G loss: 1.471873]\n",
      "epoch:33 step:31742 [D loss: 0.549920, acc.: 70.31%] [G loss: 1.541412]\n",
      "epoch:33 step:31743 [D loss: 0.579474, acc.: 67.19%] [G loss: 1.612843]\n",
      "epoch:33 step:31744 [D loss: 0.475336, acc.: 79.69%] [G loss: 1.859367]\n",
      "epoch:33 step:31745 [D loss: 0.624385, acc.: 62.50%] [G loss: 1.905863]\n",
      "epoch:33 step:31746 [D loss: 0.477912, acc.: 79.69%] [G loss: 1.430124]\n",
      "epoch:33 step:31747 [D loss: 0.414071, acc.: 84.38%] [G loss: 1.787570]\n",
      "epoch:33 step:31748 [D loss: 0.405381, acc.: 81.25%] [G loss: 1.900286]\n",
      "epoch:33 step:31749 [D loss: 0.725956, acc.: 55.47%] [G loss: 1.354294]\n",
      "epoch:33 step:31750 [D loss: 0.611410, acc.: 64.84%] [G loss: 2.287204]\n",
      "epoch:33 step:31751 [D loss: 0.589005, acc.: 67.97%] [G loss: 1.407047]\n",
      "epoch:33 step:31752 [D loss: 0.691601, acc.: 53.12%] [G loss: 1.211410]\n",
      "epoch:33 step:31753 [D loss: 0.734569, acc.: 60.16%] [G loss: 1.570646]\n",
      "epoch:33 step:31754 [D loss: 0.542821, acc.: 73.44%] [G loss: 1.802329]\n",
      "epoch:33 step:31755 [D loss: 0.637367, acc.: 64.06%] [G loss: 1.562891]\n",
      "epoch:33 step:31756 [D loss: 0.383564, acc.: 85.16%] [G loss: 1.863364]\n",
      "epoch:33 step:31757 [D loss: 0.513754, acc.: 75.78%] [G loss: 1.497667]\n",
      "epoch:33 step:31758 [D loss: 0.585674, acc.: 66.41%] [G loss: 1.764822]\n",
      "epoch:33 step:31759 [D loss: 0.779396, acc.: 53.12%] [G loss: 1.376281]\n",
      "epoch:33 step:31760 [D loss: 0.550464, acc.: 71.09%] [G loss: 1.366270]\n",
      "epoch:33 step:31761 [D loss: 0.483600, acc.: 73.44%] [G loss: 1.687015]\n",
      "epoch:33 step:31762 [D loss: 0.795875, acc.: 52.34%] [G loss: 1.386427]\n",
      "epoch:33 step:31763 [D loss: 0.474473, acc.: 75.00%] [G loss: 1.398933]\n",
      "epoch:33 step:31764 [D loss: 0.721714, acc.: 59.38%] [G loss: 1.525894]\n",
      "epoch:33 step:31765 [D loss: 0.559253, acc.: 70.31%] [G loss: 1.497784]\n",
      "epoch:33 step:31766 [D loss: 0.524067, acc.: 71.88%] [G loss: 1.219844]\n",
      "epoch:33 step:31767 [D loss: 0.529949, acc.: 71.88%] [G loss: 1.045794]\n",
      "epoch:33 step:31768 [D loss: 0.649809, acc.: 64.06%] [G loss: 1.472949]\n",
      "epoch:33 step:31769 [D loss: 0.645870, acc.: 67.97%] [G loss: 1.844095]\n",
      "epoch:33 step:31770 [D loss: 0.576138, acc.: 69.53%] [G loss: 2.004113]\n",
      "epoch:33 step:31771 [D loss: 0.611293, acc.: 64.06%] [G loss: 0.925048]\n",
      "epoch:33 step:31772 [D loss: 0.487353, acc.: 76.56%] [G loss: 1.344664]\n",
      "epoch:33 step:31773 [D loss: 0.481302, acc.: 80.47%] [G loss: 1.735187]\n",
      "epoch:33 step:31774 [D loss: 0.656827, acc.: 66.41%] [G loss: 1.290690]\n",
      "epoch:33 step:31775 [D loss: 0.618186, acc.: 69.53%] [G loss: 1.827307]\n",
      "epoch:33 step:31776 [D loss: 0.387194, acc.: 85.94%] [G loss: 1.970641]\n",
      "epoch:33 step:31777 [D loss: 0.460946, acc.: 82.81%] [G loss: 1.999591]\n",
      "epoch:33 step:31778 [D loss: 0.495775, acc.: 74.22%] [G loss: 1.490833]\n",
      "epoch:33 step:31779 [D loss: 0.376521, acc.: 87.50%] [G loss: 1.344180]\n",
      "epoch:33 step:31780 [D loss: 0.493533, acc.: 78.12%] [G loss: 1.824319]\n",
      "epoch:33 step:31781 [D loss: 0.671665, acc.: 61.72%] [G loss: 1.524873]\n",
      "epoch:33 step:31782 [D loss: 0.553884, acc.: 67.19%] [G loss: 1.813053]\n",
      "epoch:33 step:31783 [D loss: 0.488355, acc.: 79.69%] [G loss: 1.655202]\n",
      "epoch:33 step:31784 [D loss: 0.606886, acc.: 70.31%] [G loss: 1.510448]\n",
      "epoch:33 step:31785 [D loss: 0.370221, acc.: 88.28%] [G loss: 1.836967]\n",
      "epoch:33 step:31786 [D loss: 0.513309, acc.: 73.44%] [G loss: 1.211126]\n",
      "epoch:33 step:31787 [D loss: 0.354439, acc.: 86.72%] [G loss: 1.803876]\n",
      "epoch:33 step:31788 [D loss: 0.608785, acc.: 61.72%] [G loss: 1.410051]\n",
      "epoch:33 step:31789 [D loss: 0.614645, acc.: 69.53%] [G loss: 1.629843]\n",
      "epoch:33 step:31790 [D loss: 0.358359, acc.: 88.28%] [G loss: 1.299492]\n",
      "epoch:33 step:31791 [D loss: 0.387388, acc.: 88.28%] [G loss: 2.279832]\n",
      "epoch:33 step:31792 [D loss: 0.601485, acc.: 67.97%] [G loss: 1.570476]\n",
      "epoch:33 step:31793 [D loss: 0.781356, acc.: 53.12%] [G loss: 1.137767]\n",
      "epoch:33 step:31794 [D loss: 0.340390, acc.: 88.28%] [G loss: 1.922552]\n",
      "epoch:33 step:31795 [D loss: 0.317318, acc.: 87.50%] [G loss: 1.917585]\n",
      "epoch:33 step:31796 [D loss: 0.491998, acc.: 78.12%] [G loss: 1.339939]\n",
      "epoch:33 step:31797 [D loss: 0.505791, acc.: 79.69%] [G loss: 1.728277]\n",
      "epoch:33 step:31798 [D loss: 0.644478, acc.: 63.28%] [G loss: 1.087274]\n",
      "epoch:33 step:31799 [D loss: 0.696328, acc.: 64.06%] [G loss: 1.638656]\n",
      "epoch:33 step:31800 [D loss: 0.776512, acc.: 55.47%] [G loss: 1.200214]\n",
      "##############\n",
      "[2.74295018 1.922663   1.98258752 3.30799463 0.83758462 6.05613515\n",
      " 2.08880806 2.79861233 3.89423786 8.14868929]\n",
      "##########\n",
      "epoch:33 step:31801 [D loss: 0.620434, acc.: 64.84%] [G loss: 1.502699]\n",
      "epoch:33 step:31802 [D loss: 0.571372, acc.: 67.97%] [G loss: 1.766799]\n",
      "epoch:33 step:31803 [D loss: 0.478408, acc.: 75.78%] [G loss: 1.506938]\n",
      "epoch:33 step:31804 [D loss: 0.502171, acc.: 75.00%] [G loss: 1.377092]\n",
      "epoch:33 step:31805 [D loss: 0.381011, acc.: 85.16%] [G loss: 1.864462]\n",
      "epoch:33 step:31806 [D loss: 0.614717, acc.: 67.19%] [G loss: 1.443666]\n",
      "epoch:33 step:31807 [D loss: 0.314700, acc.: 88.28%] [G loss: 1.757315]\n",
      "epoch:33 step:31808 [D loss: 0.440767, acc.: 82.03%] [G loss: 1.375001]\n",
      "epoch:33 step:31809 [D loss: 0.430179, acc.: 84.38%] [G loss: 1.861027]\n",
      "epoch:33 step:31810 [D loss: 0.522673, acc.: 78.91%] [G loss: 1.683744]\n",
      "epoch:33 step:31811 [D loss: 0.660304, acc.: 62.50%] [G loss: 1.171919]\n",
      "epoch:33 step:31812 [D loss: 0.490237, acc.: 76.56%] [G loss: 0.807030]\n",
      "epoch:33 step:31813 [D loss: 0.658829, acc.: 61.72%] [G loss: 1.097643]\n",
      "epoch:33 step:31814 [D loss: 0.417050, acc.: 83.59%] [G loss: 1.656712]\n",
      "epoch:33 step:31815 [D loss: 0.774966, acc.: 57.03%] [G loss: 1.621019]\n",
      "epoch:33 step:31816 [D loss: 0.526411, acc.: 76.56%] [G loss: 1.260700]\n",
      "epoch:33 step:31817 [D loss: 0.637743, acc.: 65.62%] [G loss: 1.009455]\n",
      "epoch:33 step:31818 [D loss: 0.372331, acc.: 87.50%] [G loss: 1.633527]\n",
      "epoch:33 step:31819 [D loss: 0.379366, acc.: 85.16%] [G loss: 1.964527]\n",
      "epoch:33 step:31820 [D loss: 0.548685, acc.: 73.44%] [G loss: 2.074301]\n",
      "epoch:33 step:31821 [D loss: 0.847648, acc.: 46.09%] [G loss: 1.592815]\n",
      "epoch:33 step:31822 [D loss: 0.618327, acc.: 67.19%] [G loss: 1.619629]\n",
      "epoch:33 step:31823 [D loss: 0.550014, acc.: 71.09%] [G loss: 1.288678]\n",
      "epoch:33 step:31824 [D loss: 0.424384, acc.: 85.94%] [G loss: 1.615991]\n",
      "epoch:33 step:31825 [D loss: 0.546568, acc.: 73.44%] [G loss: 1.577090]\n",
      "epoch:33 step:31826 [D loss: 0.462335, acc.: 78.91%] [G loss: 1.696908]\n",
      "epoch:33 step:31827 [D loss: 0.481565, acc.: 78.12%] [G loss: 1.470641]\n",
      "epoch:33 step:31828 [D loss: 0.636374, acc.: 62.50%] [G loss: 1.216949]\n",
      "epoch:33 step:31829 [D loss: 0.511423, acc.: 76.56%] [G loss: 1.741421]\n",
      "epoch:33 step:31830 [D loss: 0.591275, acc.: 68.75%] [G loss: 1.683619]\n",
      "epoch:33 step:31831 [D loss: 0.455679, acc.: 78.12%] [G loss: 1.814837]\n",
      "epoch:33 step:31832 [D loss: 0.665134, acc.: 63.28%] [G loss: 2.113137]\n",
      "epoch:33 step:31833 [D loss: 0.752127, acc.: 51.56%] [G loss: 1.516282]\n",
      "epoch:33 step:31834 [D loss: 0.559389, acc.: 71.09%] [G loss: 1.316084]\n",
      "epoch:33 step:31835 [D loss: 0.414286, acc.: 82.03%] [G loss: 0.963843]\n",
      "epoch:33 step:31836 [D loss: 0.480769, acc.: 75.00%] [G loss: 1.351327]\n",
      "epoch:33 step:31837 [D loss: 0.475334, acc.: 75.78%] [G loss: 1.459005]\n",
      "epoch:33 step:31838 [D loss: 0.434174, acc.: 78.12%] [G loss: 1.690370]\n",
      "epoch:33 step:31839 [D loss: 0.564346, acc.: 66.41%] [G loss: 1.266104]\n",
      "epoch:33 step:31840 [D loss: 0.314375, acc.: 88.28%] [G loss: 1.851067]\n",
      "epoch:33 step:31841 [D loss: 0.458209, acc.: 78.91%] [G loss: 1.678869]\n",
      "epoch:33 step:31842 [D loss: 0.511780, acc.: 75.78%] [G loss: 1.141564]\n",
      "epoch:33 step:31843 [D loss: 0.742801, acc.: 59.38%] [G loss: 1.192242]\n",
      "epoch:33 step:31844 [D loss: 0.545812, acc.: 67.19%] [G loss: 2.047073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31845 [D loss: 0.469430, acc.: 75.78%] [G loss: 1.940604]\n",
      "epoch:33 step:31846 [D loss: 0.543814, acc.: 71.09%] [G loss: 1.802576]\n",
      "epoch:33 step:31847 [D loss: 0.566563, acc.: 68.75%] [G loss: 1.765532]\n",
      "epoch:33 step:31848 [D loss: 0.486242, acc.: 79.69%] [G loss: 1.571172]\n",
      "epoch:33 step:31849 [D loss: 0.486009, acc.: 75.00%] [G loss: 1.276671]\n",
      "epoch:33 step:31850 [D loss: 0.441258, acc.: 77.34%] [G loss: 1.636242]\n",
      "epoch:33 step:31851 [D loss: 0.549576, acc.: 71.88%] [G loss: 1.979307]\n",
      "epoch:33 step:31852 [D loss: 0.751675, acc.: 57.81%] [G loss: 1.068813]\n",
      "epoch:33 step:31853 [D loss: 0.367839, acc.: 85.94%] [G loss: 1.463218]\n",
      "epoch:33 step:31854 [D loss: 0.417877, acc.: 82.03%] [G loss: 1.814781]\n",
      "epoch:33 step:31855 [D loss: 0.690737, acc.: 56.25%] [G loss: 1.192130]\n",
      "epoch:33 step:31856 [D loss: 0.684191, acc.: 60.94%] [G loss: 1.638556]\n",
      "epoch:33 step:31857 [D loss: 0.431129, acc.: 79.69%] [G loss: 2.163151]\n",
      "epoch:33 step:31858 [D loss: 0.730254, acc.: 58.59%] [G loss: 1.757729]\n",
      "epoch:34 step:31859 [D loss: 0.580442, acc.: 72.66%] [G loss: 1.298381]\n",
      "epoch:34 step:31860 [D loss: 0.604503, acc.: 67.97%] [G loss: 1.100976]\n",
      "epoch:34 step:31861 [D loss: 0.600019, acc.: 67.97%] [G loss: 1.321096]\n",
      "epoch:34 step:31862 [D loss: 0.480838, acc.: 75.78%] [G loss: 1.575275]\n",
      "epoch:34 step:31863 [D loss: 0.444079, acc.: 80.47%] [G loss: 1.626057]\n",
      "epoch:34 step:31864 [D loss: 0.579113, acc.: 64.84%] [G loss: 1.706647]\n",
      "epoch:34 step:31865 [D loss: 0.506118, acc.: 77.34%] [G loss: 1.673825]\n",
      "epoch:34 step:31866 [D loss: 0.517330, acc.: 75.78%] [G loss: 1.258517]\n",
      "epoch:34 step:31867 [D loss: 0.488188, acc.: 74.22%] [G loss: 1.469162]\n",
      "epoch:34 step:31868 [D loss: 0.550724, acc.: 74.22%] [G loss: 1.774170]\n",
      "epoch:34 step:31869 [D loss: 0.576468, acc.: 71.09%] [G loss: 1.710207]\n",
      "epoch:34 step:31870 [D loss: 0.538273, acc.: 73.44%] [G loss: 1.288370]\n",
      "epoch:34 step:31871 [D loss: 0.499378, acc.: 75.00%] [G loss: 1.917676]\n",
      "epoch:34 step:31872 [D loss: 0.564198, acc.: 67.97%] [G loss: 1.078969]\n",
      "epoch:34 step:31873 [D loss: 0.542214, acc.: 75.00%] [G loss: 1.819613]\n",
      "epoch:34 step:31874 [D loss: 0.440170, acc.: 82.03%] [G loss: 1.474753]\n",
      "epoch:34 step:31875 [D loss: 0.445062, acc.: 80.47%] [G loss: 1.292742]\n",
      "epoch:34 step:31876 [D loss: 0.619751, acc.: 66.41%] [G loss: 1.025970]\n",
      "epoch:34 step:31877 [D loss: 0.492430, acc.: 80.47%] [G loss: 1.416606]\n",
      "epoch:34 step:31878 [D loss: 0.662192, acc.: 65.62%] [G loss: 1.434423]\n",
      "epoch:34 step:31879 [D loss: 0.708213, acc.: 60.94%] [G loss: 1.436915]\n",
      "epoch:34 step:31880 [D loss: 0.521591, acc.: 76.56%] [G loss: 1.510327]\n",
      "epoch:34 step:31881 [D loss: 0.552502, acc.: 71.88%] [G loss: 1.790501]\n",
      "epoch:34 step:31882 [D loss: 0.414603, acc.: 82.81%] [G loss: 1.742925]\n",
      "epoch:34 step:31883 [D loss: 0.498248, acc.: 76.56%] [G loss: 1.461716]\n",
      "epoch:34 step:31884 [D loss: 0.603963, acc.: 66.41%] [G loss: 1.428275]\n",
      "epoch:34 step:31885 [D loss: 0.326597, acc.: 89.06%] [G loss: 1.918493]\n",
      "epoch:34 step:31886 [D loss: 0.504113, acc.: 75.00%] [G loss: 1.585626]\n",
      "epoch:34 step:31887 [D loss: 0.439955, acc.: 84.38%] [G loss: 1.632029]\n",
      "epoch:34 step:31888 [D loss: 0.698855, acc.: 63.28%] [G loss: 1.138618]\n",
      "epoch:34 step:31889 [D loss: 0.517251, acc.: 72.66%] [G loss: 1.085063]\n",
      "epoch:34 step:31890 [D loss: 0.371052, acc.: 88.28%] [G loss: 1.670212]\n",
      "epoch:34 step:31891 [D loss: 0.621866, acc.: 65.62%] [G loss: 1.209464]\n",
      "epoch:34 step:31892 [D loss: 0.451717, acc.: 80.47%] [G loss: 1.835921]\n",
      "epoch:34 step:31893 [D loss: 0.447277, acc.: 80.47%] [G loss: 1.987048]\n",
      "epoch:34 step:31894 [D loss: 0.560934, acc.: 73.44%] [G loss: 1.209880]\n",
      "epoch:34 step:31895 [D loss: 0.334610, acc.: 86.72%] [G loss: 2.110734]\n",
      "epoch:34 step:31896 [D loss: 0.672670, acc.: 64.06%] [G loss: 1.448277]\n",
      "epoch:34 step:31897 [D loss: 0.512866, acc.: 77.34%] [G loss: 1.393479]\n",
      "epoch:34 step:31898 [D loss: 0.560768, acc.: 74.22%] [G loss: 1.116155]\n",
      "epoch:34 step:31899 [D loss: 0.740874, acc.: 58.59%] [G loss: 1.217433]\n",
      "epoch:34 step:31900 [D loss: 0.433912, acc.: 82.81%] [G loss: 1.694028]\n",
      "epoch:34 step:31901 [D loss: 0.532749, acc.: 72.66%] [G loss: 1.488506]\n",
      "epoch:34 step:31902 [D loss: 0.709704, acc.: 57.81%] [G loss: 0.967529]\n",
      "epoch:34 step:31903 [D loss: 0.447048, acc.: 78.91%] [G loss: 1.262338]\n",
      "epoch:34 step:31904 [D loss: 0.725354, acc.: 57.81%] [G loss: 1.420332]\n",
      "epoch:34 step:31905 [D loss: 0.518356, acc.: 75.00%] [G loss: 1.345136]\n",
      "epoch:34 step:31906 [D loss: 0.610592, acc.: 67.19%] [G loss: 1.574124]\n",
      "epoch:34 step:31907 [D loss: 0.565290, acc.: 70.31%] [G loss: 1.423773]\n",
      "epoch:34 step:31908 [D loss: 0.498388, acc.: 77.34%] [G loss: 1.384846]\n",
      "epoch:34 step:31909 [D loss: 0.790316, acc.: 50.00%] [G loss: 1.490083]\n",
      "epoch:34 step:31910 [D loss: 0.532226, acc.: 71.88%] [G loss: 0.877446]\n",
      "epoch:34 step:31911 [D loss: 0.393199, acc.: 85.16%] [G loss: 1.063189]\n",
      "epoch:34 step:31912 [D loss: 0.488485, acc.: 78.12%] [G loss: 1.797976]\n",
      "epoch:34 step:31913 [D loss: 0.496089, acc.: 76.56%] [G loss: 1.322156]\n",
      "epoch:34 step:31914 [D loss: 0.808967, acc.: 53.12%] [G loss: 1.241967]\n",
      "epoch:34 step:31915 [D loss: 0.606339, acc.: 71.09%] [G loss: 1.173620]\n",
      "epoch:34 step:31916 [D loss: 0.511839, acc.: 72.66%] [G loss: 1.493073]\n",
      "epoch:34 step:31917 [D loss: 0.431861, acc.: 82.81%] [G loss: 1.635893]\n",
      "epoch:34 step:31918 [D loss: 0.532006, acc.: 72.66%] [G loss: 1.569207]\n",
      "epoch:34 step:31919 [D loss: 0.732795, acc.: 53.91%] [G loss: 1.772740]\n",
      "epoch:34 step:31920 [D loss: 0.672098, acc.: 58.59%] [G loss: 1.423669]\n",
      "epoch:34 step:31921 [D loss: 0.526078, acc.: 71.09%] [G loss: 1.594142]\n",
      "epoch:34 step:31922 [D loss: 0.453150, acc.: 77.34%] [G loss: 1.545145]\n",
      "epoch:34 step:31923 [D loss: 0.475284, acc.: 78.91%] [G loss: 1.532551]\n",
      "epoch:34 step:31924 [D loss: 0.448537, acc.: 77.34%] [G loss: 2.086648]\n",
      "epoch:34 step:31925 [D loss: 0.348121, acc.: 86.72%] [G loss: 1.901870]\n",
      "epoch:34 step:31926 [D loss: 0.639319, acc.: 67.19%] [G loss: 1.848968]\n",
      "epoch:34 step:31927 [D loss: 0.456155, acc.: 77.34%] [G loss: 1.871856]\n",
      "epoch:34 step:31928 [D loss: 0.712606, acc.: 60.94%] [G loss: 1.087592]\n",
      "epoch:34 step:31929 [D loss: 0.579706, acc.: 74.22%] [G loss: 1.106152]\n",
      "epoch:34 step:31930 [D loss: 0.446059, acc.: 78.91%] [G loss: 1.367329]\n",
      "epoch:34 step:31931 [D loss: 0.455681, acc.: 79.69%] [G loss: 1.480728]\n",
      "epoch:34 step:31932 [D loss: 0.347455, acc.: 85.94%] [G loss: 1.653955]\n",
      "epoch:34 step:31933 [D loss: 0.572478, acc.: 68.75%] [G loss: 1.620041]\n",
      "epoch:34 step:31934 [D loss: 0.368148, acc.: 85.16%] [G loss: 1.395501]\n",
      "epoch:34 step:31935 [D loss: 0.524269, acc.: 73.44%] [G loss: 1.660609]\n",
      "epoch:34 step:31936 [D loss: 0.751713, acc.: 55.47%] [G loss: 1.417161]\n",
      "epoch:34 step:31937 [D loss: 0.436649, acc.: 82.03%] [G loss: 1.360940]\n",
      "epoch:34 step:31938 [D loss: 0.548171, acc.: 72.66%] [G loss: 1.632051]\n",
      "epoch:34 step:31939 [D loss: 0.705277, acc.: 59.38%] [G loss: 1.243869]\n",
      "epoch:34 step:31940 [D loss: 0.460192, acc.: 81.25%] [G loss: 1.067210]\n",
      "epoch:34 step:31941 [D loss: 0.474686, acc.: 80.47%] [G loss: 1.606868]\n",
      "epoch:34 step:31942 [D loss: 0.567967, acc.: 74.22%] [G loss: 1.379647]\n",
      "epoch:34 step:31943 [D loss: 0.591574, acc.: 67.97%] [G loss: 1.399931]\n",
      "epoch:34 step:31944 [D loss: 0.659084, acc.: 66.41%] [G loss: 1.669459]\n",
      "epoch:34 step:31945 [D loss: 0.497940, acc.: 74.22%] [G loss: 1.810210]\n",
      "epoch:34 step:31946 [D loss: 0.524188, acc.: 73.44%] [G loss: 1.569716]\n",
      "epoch:34 step:31947 [D loss: 0.611354, acc.: 68.75%] [G loss: 1.465002]\n",
      "epoch:34 step:31948 [D loss: 0.611435, acc.: 64.84%] [G loss: 1.324748]\n",
      "epoch:34 step:31949 [D loss: 0.512837, acc.: 75.78%] [G loss: 0.924666]\n",
      "epoch:34 step:31950 [D loss: 0.410875, acc.: 82.81%] [G loss: 1.246011]\n",
      "epoch:34 step:31951 [D loss: 0.343092, acc.: 90.62%] [G loss: 1.287292]\n",
      "epoch:34 step:31952 [D loss: 0.546621, acc.: 68.75%] [G loss: 1.176777]\n",
      "epoch:34 step:31953 [D loss: 0.525498, acc.: 71.09%] [G loss: 1.809811]\n",
      "epoch:34 step:31954 [D loss: 0.620892, acc.: 67.97%] [G loss: 1.446409]\n",
      "epoch:34 step:31955 [D loss: 0.428885, acc.: 82.81%] [G loss: 1.560108]\n",
      "epoch:34 step:31956 [D loss: 0.449061, acc.: 82.03%] [G loss: 1.599100]\n",
      "epoch:34 step:31957 [D loss: 0.581423, acc.: 70.31%] [G loss: 1.181408]\n",
      "epoch:34 step:31958 [D loss: 0.620701, acc.: 65.62%] [G loss: 1.603989]\n",
      "epoch:34 step:31959 [D loss: 0.442063, acc.: 82.81%] [G loss: 1.805380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:31960 [D loss: 0.455960, acc.: 78.91%] [G loss: 1.619761]\n",
      "epoch:34 step:31961 [D loss: 0.597984, acc.: 64.84%] [G loss: 1.480955]\n",
      "epoch:34 step:31962 [D loss: 0.599609, acc.: 66.41%] [G loss: 1.457611]\n",
      "epoch:34 step:31963 [D loss: 0.361534, acc.: 87.50%] [G loss: 1.858034]\n",
      "epoch:34 step:31964 [D loss: 0.689025, acc.: 59.38%] [G loss: 1.567380]\n",
      "epoch:34 step:31965 [D loss: 0.390163, acc.: 85.94%] [G loss: 1.095479]\n",
      "epoch:34 step:31966 [D loss: 0.399294, acc.: 83.59%] [G loss: 1.699709]\n",
      "epoch:34 step:31967 [D loss: 0.477112, acc.: 79.69%] [G loss: 0.991918]\n",
      "epoch:34 step:31968 [D loss: 0.430274, acc.: 78.12%] [G loss: 1.429720]\n",
      "epoch:34 step:31969 [D loss: 0.538353, acc.: 72.66%] [G loss: 1.417627]\n",
      "epoch:34 step:31970 [D loss: 0.640679, acc.: 65.62%] [G loss: 1.415968]\n",
      "epoch:34 step:31971 [D loss: 0.523895, acc.: 74.22%] [G loss: 1.050241]\n",
      "epoch:34 step:31972 [D loss: 0.470193, acc.: 78.12%] [G loss: 1.778037]\n",
      "epoch:34 step:31973 [D loss: 0.512413, acc.: 78.12%] [G loss: 1.201758]\n",
      "epoch:34 step:31974 [D loss: 0.583915, acc.: 67.97%] [G loss: 1.416683]\n",
      "epoch:34 step:31975 [D loss: 0.573062, acc.: 68.75%] [G loss: 1.615815]\n",
      "epoch:34 step:31976 [D loss: 0.574146, acc.: 67.19%] [G loss: 1.804675]\n",
      "epoch:34 step:31977 [D loss: 0.331818, acc.: 90.62%] [G loss: 1.704608]\n",
      "epoch:34 step:31978 [D loss: 0.700839, acc.: 58.59%] [G loss: 1.749915]\n",
      "epoch:34 step:31979 [D loss: 0.587183, acc.: 71.88%] [G loss: 1.266596]\n",
      "epoch:34 step:31980 [D loss: 0.516116, acc.: 74.22%] [G loss: 1.171021]\n",
      "epoch:34 step:31981 [D loss: 0.344105, acc.: 86.72%] [G loss: 1.565441]\n",
      "epoch:34 step:31982 [D loss: 0.711415, acc.: 59.38%] [G loss: 1.471199]\n",
      "epoch:34 step:31983 [D loss: 0.585557, acc.: 64.06%] [G loss: 1.349200]\n",
      "epoch:34 step:31984 [D loss: 0.461495, acc.: 78.91%] [G loss: 1.873635]\n",
      "epoch:34 step:31985 [D loss: 0.399238, acc.: 81.25%] [G loss: 1.370615]\n",
      "epoch:34 step:31986 [D loss: 0.547476, acc.: 72.66%] [G loss: 1.499475]\n",
      "epoch:34 step:31987 [D loss: 0.466372, acc.: 81.25%] [G loss: 1.142171]\n",
      "epoch:34 step:31988 [D loss: 0.486723, acc.: 75.78%] [G loss: 1.370354]\n",
      "epoch:34 step:31989 [D loss: 0.435084, acc.: 82.81%] [G loss: 2.054604]\n",
      "epoch:34 step:31990 [D loss: 0.529947, acc.: 72.66%] [G loss: 1.479232]\n",
      "epoch:34 step:31991 [D loss: 0.499829, acc.: 77.34%] [G loss: 1.336049]\n",
      "epoch:34 step:31992 [D loss: 0.512215, acc.: 77.34%] [G loss: 1.564717]\n",
      "epoch:34 step:31993 [D loss: 0.523909, acc.: 77.34%] [G loss: 1.990969]\n",
      "epoch:34 step:31994 [D loss: 0.746420, acc.: 54.69%] [G loss: 1.222164]\n",
      "epoch:34 step:31995 [D loss: 0.553721, acc.: 73.44%] [G loss: 1.443782]\n",
      "epoch:34 step:31996 [D loss: 0.618130, acc.: 63.28%] [G loss: 1.175071]\n",
      "epoch:34 step:31997 [D loss: 0.631412, acc.: 66.41%] [G loss: 1.491146]\n",
      "epoch:34 step:31998 [D loss: 0.704924, acc.: 59.38%] [G loss: 1.302114]\n",
      "epoch:34 step:31999 [D loss: 0.545259, acc.: 74.22%] [G loss: 1.418841]\n",
      "epoch:34 step:32000 [D loss: 0.503633, acc.: 74.22%] [G loss: 1.200134]\n",
      "##############\n",
      "[2.76186756 2.06418981 1.74925394 2.96597561 0.8190067  5.91990915\n",
      " 2.53746108 2.59652472 3.99729486 8.14868929]\n",
      "##########\n",
      "epoch:34 step:32001 [D loss: 0.390657, acc.: 85.16%] [G loss: 2.130404]\n",
      "epoch:34 step:32002 [D loss: 0.549337, acc.: 73.44%] [G loss: 1.729167]\n",
      "epoch:34 step:32003 [D loss: 0.469259, acc.: 75.78%] [G loss: 1.726907]\n",
      "epoch:34 step:32004 [D loss: 0.600406, acc.: 71.88%] [G loss: 1.231833]\n",
      "epoch:34 step:32005 [D loss: 0.484233, acc.: 77.34%] [G loss: 1.424275]\n",
      "epoch:34 step:32006 [D loss: 0.475765, acc.: 77.34%] [G loss: 1.344652]\n",
      "epoch:34 step:32007 [D loss: 0.688753, acc.: 60.94%] [G loss: 0.995024]\n",
      "epoch:34 step:32008 [D loss: 0.615339, acc.: 70.31%] [G loss: 1.208739]\n",
      "epoch:34 step:32009 [D loss: 0.350730, acc.: 86.72%] [G loss: 1.920204]\n",
      "epoch:34 step:32010 [D loss: 0.479123, acc.: 77.34%] [G loss: 1.731074]\n",
      "epoch:34 step:32011 [D loss: 0.511749, acc.: 76.56%] [G loss: 1.297912]\n",
      "epoch:34 step:32012 [D loss: 0.514915, acc.: 75.00%] [G loss: 1.312946]\n",
      "epoch:34 step:32013 [D loss: 0.687594, acc.: 62.50%] [G loss: 1.373293]\n",
      "epoch:34 step:32014 [D loss: 0.434036, acc.: 81.25%] [G loss: 2.177428]\n",
      "epoch:34 step:32015 [D loss: 0.625700, acc.: 67.19%] [G loss: 1.909429]\n",
      "epoch:34 step:32016 [D loss: 0.735359, acc.: 58.59%] [G loss: 0.986888]\n",
      "epoch:34 step:32017 [D loss: 0.416322, acc.: 83.59%] [G loss: 1.686462]\n",
      "epoch:34 step:32018 [D loss: 0.348769, acc.: 89.84%] [G loss: 1.714111]\n",
      "epoch:34 step:32019 [D loss: 0.417663, acc.: 85.94%] [G loss: 1.784543]\n",
      "epoch:34 step:32020 [D loss: 0.437672, acc.: 82.81%] [G loss: 2.242208]\n",
      "epoch:34 step:32021 [D loss: 0.691387, acc.: 60.16%] [G loss: 1.906548]\n",
      "epoch:34 step:32022 [D loss: 0.497750, acc.: 81.25%] [G loss: 1.678106]\n",
      "epoch:34 step:32023 [D loss: 0.470069, acc.: 80.47%] [G loss: 1.049001]\n",
      "epoch:34 step:32024 [D loss: 0.625782, acc.: 64.06%] [G loss: 2.035707]\n",
      "epoch:34 step:32025 [D loss: 0.650645, acc.: 64.06%] [G loss: 1.874560]\n",
      "epoch:34 step:32026 [D loss: 0.517940, acc.: 75.78%] [G loss: 1.480952]\n",
      "epoch:34 step:32027 [D loss: 0.549196, acc.: 71.09%] [G loss: 1.445748]\n",
      "epoch:34 step:32028 [D loss: 0.665914, acc.: 58.59%] [G loss: 1.758412]\n",
      "epoch:34 step:32029 [D loss: 0.637759, acc.: 60.16%] [G loss: 1.049246]\n",
      "epoch:34 step:32030 [D loss: 0.454824, acc.: 81.25%] [G loss: 1.808217]\n",
      "epoch:34 step:32031 [D loss: 0.673919, acc.: 61.72%] [G loss: 1.243150]\n",
      "epoch:34 step:32032 [D loss: 0.525814, acc.: 69.53%] [G loss: 2.188383]\n",
      "epoch:34 step:32033 [D loss: 0.483996, acc.: 76.56%] [G loss: 1.262559]\n",
      "epoch:34 step:32034 [D loss: 0.427311, acc.: 82.81%] [G loss: 1.284314]\n",
      "epoch:34 step:32035 [D loss: 0.448167, acc.: 79.69%] [G loss: 0.913123]\n",
      "epoch:34 step:32036 [D loss: 0.528302, acc.: 73.44%] [G loss: 1.271887]\n",
      "epoch:34 step:32037 [D loss: 0.431879, acc.: 80.47%] [G loss: 1.669549]\n",
      "epoch:34 step:32038 [D loss: 0.439244, acc.: 83.59%] [G loss: 1.013903]\n",
      "epoch:34 step:32039 [D loss: 0.423723, acc.: 87.50%] [G loss: 1.376158]\n",
      "epoch:34 step:32040 [D loss: 0.393317, acc.: 84.38%] [G loss: 1.275836]\n",
      "epoch:34 step:32041 [D loss: 0.576162, acc.: 75.00%] [G loss: 1.396492]\n",
      "epoch:34 step:32042 [D loss: 0.422270, acc.: 81.25%] [G loss: 1.294699]\n",
      "epoch:34 step:32043 [D loss: 0.551555, acc.: 69.53%] [G loss: 1.395561]\n",
      "epoch:34 step:32044 [D loss: 0.353701, acc.: 87.50%] [G loss: 1.237968]\n",
      "epoch:34 step:32045 [D loss: 0.453352, acc.: 81.25%] [G loss: 1.943964]\n",
      "epoch:34 step:32046 [D loss: 0.434999, acc.: 84.38%] [G loss: 1.501035]\n",
      "epoch:34 step:32047 [D loss: 0.614893, acc.: 60.94%] [G loss: 1.574353]\n",
      "epoch:34 step:32048 [D loss: 0.632993, acc.: 65.62%] [G loss: 1.732762]\n",
      "epoch:34 step:32049 [D loss: 0.626676, acc.: 68.75%] [G loss: 1.257654]\n",
      "epoch:34 step:32050 [D loss: 0.569733, acc.: 72.66%] [G loss: 1.460639]\n",
      "epoch:34 step:32051 [D loss: 0.507984, acc.: 77.34%] [G loss: 1.263439]\n",
      "epoch:34 step:32052 [D loss: 0.544754, acc.: 72.66%] [G loss: 1.168682]\n",
      "epoch:34 step:32053 [D loss: 0.555386, acc.: 70.31%] [G loss: 1.301086]\n",
      "epoch:34 step:32054 [D loss: 0.586099, acc.: 66.41%] [G loss: 1.519093]\n",
      "epoch:34 step:32055 [D loss: 0.513545, acc.: 74.22%] [G loss: 1.720825]\n",
      "epoch:34 step:32056 [D loss: 0.320054, acc.: 89.06%] [G loss: 1.891140]\n",
      "epoch:34 step:32057 [D loss: 0.630428, acc.: 61.72%] [G loss: 1.633367]\n",
      "epoch:34 step:32058 [D loss: 0.900495, acc.: 48.44%] [G loss: 1.211378]\n",
      "epoch:34 step:32059 [D loss: 0.404912, acc.: 81.25%] [G loss: 1.769465]\n",
      "epoch:34 step:32060 [D loss: 0.473798, acc.: 76.56%] [G loss: 2.146951]\n",
      "epoch:34 step:32061 [D loss: 0.356314, acc.: 86.72%] [G loss: 1.537450]\n",
      "epoch:34 step:32062 [D loss: 0.308749, acc.: 87.50%] [G loss: 1.804187]\n",
      "epoch:34 step:32063 [D loss: 0.647306, acc.: 60.16%] [G loss: 1.350032]\n",
      "epoch:34 step:32064 [D loss: 0.472667, acc.: 75.78%] [G loss: 1.491657]\n",
      "epoch:34 step:32065 [D loss: 0.501720, acc.: 72.66%] [G loss: 1.917446]\n",
      "epoch:34 step:32066 [D loss: 0.520502, acc.: 77.34%] [G loss: 1.531141]\n",
      "epoch:34 step:32067 [D loss: 0.594893, acc.: 71.88%] [G loss: 1.428037]\n",
      "epoch:34 step:32068 [D loss: 0.585203, acc.: 71.88%] [G loss: 1.654374]\n",
      "epoch:34 step:32069 [D loss: 0.524884, acc.: 75.00%] [G loss: 1.424062]\n",
      "epoch:34 step:32070 [D loss: 0.621237, acc.: 68.75%] [G loss: 1.664880]\n",
      "epoch:34 step:32071 [D loss: 0.605412, acc.: 63.28%] [G loss: 1.583073]\n",
      "epoch:34 step:32072 [D loss: 0.698225, acc.: 63.28%] [G loss: 1.388029]\n",
      "epoch:34 step:32073 [D loss: 0.514218, acc.: 77.34%] [G loss: 0.856335]\n",
      "epoch:34 step:32074 [D loss: 0.450108, acc.: 78.91%] [G loss: 1.312633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32075 [D loss: 0.463850, acc.: 82.81%] [G loss: 1.311269]\n",
      "epoch:34 step:32076 [D loss: 0.619056, acc.: 67.19%] [G loss: 1.505972]\n",
      "epoch:34 step:32077 [D loss: 0.435572, acc.: 83.59%] [G loss: 1.418971]\n",
      "epoch:34 step:32078 [D loss: 0.388003, acc.: 86.72%] [G loss: 1.678542]\n",
      "epoch:34 step:32079 [D loss: 0.419361, acc.: 79.69%] [G loss: 1.599270]\n",
      "epoch:34 step:32080 [D loss: 0.798933, acc.: 49.22%] [G loss: 1.643006]\n",
      "epoch:34 step:32081 [D loss: 0.502182, acc.: 79.69%] [G loss: 1.284966]\n",
      "epoch:34 step:32082 [D loss: 0.585644, acc.: 68.75%] [G loss: 1.268786]\n",
      "epoch:34 step:32083 [D loss: 0.338300, acc.: 86.72%] [G loss: 1.231893]\n",
      "epoch:34 step:32084 [D loss: 0.569440, acc.: 66.41%] [G loss: 1.425903]\n",
      "epoch:34 step:32085 [D loss: 0.675927, acc.: 58.59%] [G loss: 1.029166]\n",
      "epoch:34 step:32086 [D loss: 0.723517, acc.: 58.59%] [G loss: 0.913441]\n",
      "epoch:34 step:32087 [D loss: 0.445271, acc.: 81.25%] [G loss: 2.015662]\n",
      "epoch:34 step:32088 [D loss: 0.630423, acc.: 64.06%] [G loss: 1.643421]\n",
      "epoch:34 step:32089 [D loss: 0.335638, acc.: 88.28%] [G loss: 1.750840]\n",
      "epoch:34 step:32090 [D loss: 0.613450, acc.: 60.94%] [G loss: 1.321495]\n",
      "epoch:34 step:32091 [D loss: 0.709237, acc.: 60.16%] [G loss: 1.291487]\n",
      "epoch:34 step:32092 [D loss: 0.635571, acc.: 65.62%] [G loss: 1.432703]\n",
      "epoch:34 step:32093 [D loss: 0.433972, acc.: 83.59%] [G loss: 1.843514]\n",
      "epoch:34 step:32094 [D loss: 0.567838, acc.: 72.66%] [G loss: 1.602454]\n",
      "epoch:34 step:32095 [D loss: 0.602451, acc.: 64.06%] [G loss: 1.411545]\n",
      "epoch:34 step:32096 [D loss: 0.606840, acc.: 71.09%] [G loss: 1.152667]\n",
      "epoch:34 step:32097 [D loss: 0.511080, acc.: 78.91%] [G loss: 1.201962]\n",
      "epoch:34 step:32098 [D loss: 0.709729, acc.: 60.94%] [G loss: 1.692041]\n",
      "epoch:34 step:32099 [D loss: 0.590275, acc.: 72.66%] [G loss: 1.618981]\n",
      "epoch:34 step:32100 [D loss: 0.651288, acc.: 62.50%] [G loss: 1.545405]\n",
      "epoch:34 step:32101 [D loss: 0.518667, acc.: 72.66%] [G loss: 1.703696]\n",
      "epoch:34 step:32102 [D loss: 0.467282, acc.: 79.69%] [G loss: 1.577391]\n",
      "epoch:34 step:32103 [D loss: 0.551718, acc.: 72.66%] [G loss: 1.315083]\n",
      "epoch:34 step:32104 [D loss: 0.432097, acc.: 78.12%] [G loss: 1.477177]\n",
      "epoch:34 step:32105 [D loss: 0.586173, acc.: 69.53%] [G loss: 1.760413]\n",
      "epoch:34 step:32106 [D loss: 0.682227, acc.: 57.03%] [G loss: 1.335141]\n",
      "epoch:34 step:32107 [D loss: 0.263633, acc.: 92.97%] [G loss: 1.620795]\n",
      "epoch:34 step:32108 [D loss: 0.448327, acc.: 82.03%] [G loss: 1.867466]\n",
      "epoch:34 step:32109 [D loss: 0.616055, acc.: 66.41%] [G loss: 1.598735]\n",
      "epoch:34 step:32110 [D loss: 0.547741, acc.: 75.78%] [G loss: 1.369911]\n",
      "epoch:34 step:32111 [D loss: 0.636650, acc.: 62.50%] [G loss: 2.042217]\n",
      "epoch:34 step:32112 [D loss: 0.616329, acc.: 66.41%] [G loss: 1.368236]\n",
      "epoch:34 step:32113 [D loss: 0.510116, acc.: 76.56%] [G loss: 1.682067]\n",
      "epoch:34 step:32114 [D loss: 0.516326, acc.: 73.44%] [G loss: 1.713533]\n",
      "epoch:34 step:32115 [D loss: 0.567700, acc.: 76.56%] [G loss: 1.412315]\n",
      "epoch:34 step:32116 [D loss: 0.649991, acc.: 64.06%] [G loss: 1.028982]\n",
      "epoch:34 step:32117 [D loss: 0.454070, acc.: 82.81%] [G loss: 1.598814]\n",
      "epoch:34 step:32118 [D loss: 0.465021, acc.: 78.91%] [G loss: 1.463687]\n",
      "epoch:34 step:32119 [D loss: 0.518336, acc.: 72.66%] [G loss: 1.747594]\n",
      "epoch:34 step:32120 [D loss: 0.727121, acc.: 53.12%] [G loss: 1.487314]\n",
      "epoch:34 step:32121 [D loss: 0.649331, acc.: 68.75%] [G loss: 0.981264]\n",
      "epoch:34 step:32122 [D loss: 0.551801, acc.: 71.09%] [G loss: 1.600204]\n",
      "epoch:34 step:32123 [D loss: 0.423581, acc.: 82.81%] [G loss: 1.851113]\n",
      "epoch:34 step:32124 [D loss: 0.445944, acc.: 78.91%] [G loss: 1.690484]\n",
      "epoch:34 step:32125 [D loss: 0.687901, acc.: 60.16%] [G loss: 1.629656]\n",
      "epoch:34 step:32126 [D loss: 0.799090, acc.: 53.12%] [G loss: 1.333257]\n",
      "epoch:34 step:32127 [D loss: 0.348248, acc.: 88.28%] [G loss: 1.831723]\n",
      "epoch:34 step:32128 [D loss: 0.593332, acc.: 71.09%] [G loss: 1.885730]\n",
      "epoch:34 step:32129 [D loss: 0.335044, acc.: 90.62%] [G loss: 2.095015]\n",
      "epoch:34 step:32130 [D loss: 0.424820, acc.: 82.03%] [G loss: 1.622736]\n",
      "epoch:34 step:32131 [D loss: 0.515622, acc.: 74.22%] [G loss: 1.419672]\n",
      "epoch:34 step:32132 [D loss: 0.428509, acc.: 78.91%] [G loss: 1.540559]\n",
      "epoch:34 step:32133 [D loss: 0.666611, acc.: 66.41%] [G loss: 1.613538]\n",
      "epoch:34 step:32134 [D loss: 0.465439, acc.: 80.47%] [G loss: 1.590438]\n",
      "epoch:34 step:32135 [D loss: 0.531673, acc.: 74.22%] [G loss: 1.715290]\n",
      "epoch:34 step:32136 [D loss: 0.427106, acc.: 82.03%] [G loss: 1.676871]\n",
      "epoch:34 step:32137 [D loss: 0.562276, acc.: 67.19%] [G loss: 1.161455]\n",
      "epoch:34 step:32138 [D loss: 0.644488, acc.: 64.84%] [G loss: 1.258236]\n",
      "epoch:34 step:32139 [D loss: 0.530796, acc.: 73.44%] [G loss: 1.748198]\n",
      "epoch:34 step:32140 [D loss: 0.501572, acc.: 79.69%] [G loss: 1.446745]\n",
      "epoch:34 step:32141 [D loss: 0.418975, acc.: 85.16%] [G loss: 1.465480]\n",
      "epoch:34 step:32142 [D loss: 0.399754, acc.: 84.38%] [G loss: 1.346555]\n",
      "epoch:34 step:32143 [D loss: 0.431291, acc.: 78.12%] [G loss: 1.410373]\n",
      "epoch:34 step:32144 [D loss: 0.539546, acc.: 75.78%] [G loss: 1.673673]\n",
      "epoch:34 step:32145 [D loss: 0.591822, acc.: 69.53%] [G loss: 1.822929]\n",
      "epoch:34 step:32146 [D loss: 0.667785, acc.: 62.50%] [G loss: 1.654403]\n",
      "epoch:34 step:32147 [D loss: 0.543857, acc.: 77.34%] [G loss: 1.316869]\n",
      "epoch:34 step:32148 [D loss: 0.494233, acc.: 74.22%] [G loss: 1.344127]\n",
      "epoch:34 step:32149 [D loss: 0.504668, acc.: 77.34%] [G loss: 1.265222]\n",
      "epoch:34 step:32150 [D loss: 0.632281, acc.: 63.28%] [G loss: 1.135118]\n",
      "epoch:34 step:32151 [D loss: 0.679267, acc.: 61.72%] [G loss: 1.171887]\n",
      "epoch:34 step:32152 [D loss: 0.576931, acc.: 71.09%] [G loss: 1.573195]\n",
      "epoch:34 step:32153 [D loss: 0.429856, acc.: 82.03%] [G loss: 1.427535]\n",
      "epoch:34 step:32154 [D loss: 0.418770, acc.: 84.38%] [G loss: 1.722713]\n",
      "epoch:34 step:32155 [D loss: 0.508421, acc.: 74.22%] [G loss: 1.514276]\n",
      "epoch:34 step:32156 [D loss: 0.719442, acc.: 58.59%] [G loss: 1.515669]\n",
      "epoch:34 step:32157 [D loss: 0.723503, acc.: 54.69%] [G loss: 1.384617]\n",
      "epoch:34 step:32158 [D loss: 0.543811, acc.: 73.44%] [G loss: 1.457985]\n",
      "epoch:34 step:32159 [D loss: 0.422121, acc.: 82.81%] [G loss: 1.470683]\n",
      "epoch:34 step:32160 [D loss: 0.639279, acc.: 62.50%] [G loss: 1.004117]\n",
      "epoch:34 step:32161 [D loss: 0.545227, acc.: 70.31%] [G loss: 1.663923]\n",
      "epoch:34 step:32162 [D loss: 0.404800, acc.: 82.03%] [G loss: 1.840989]\n",
      "epoch:34 step:32163 [D loss: 0.679780, acc.: 57.03%] [G loss: 1.577896]\n",
      "epoch:34 step:32164 [D loss: 0.511450, acc.: 73.44%] [G loss: 1.469145]\n",
      "epoch:34 step:32165 [D loss: 0.606690, acc.: 64.84%] [G loss: 1.422405]\n",
      "epoch:34 step:32166 [D loss: 0.510206, acc.: 73.44%] [G loss: 1.589979]\n",
      "epoch:34 step:32167 [D loss: 0.438572, acc.: 80.47%] [G loss: 1.764356]\n",
      "epoch:34 step:32168 [D loss: 0.535583, acc.: 70.31%] [G loss: 1.654350]\n",
      "epoch:34 step:32169 [D loss: 0.457579, acc.: 78.91%] [G loss: 1.672539]\n",
      "epoch:34 step:32170 [D loss: 0.529701, acc.: 78.91%] [G loss: 1.933219]\n",
      "epoch:34 step:32171 [D loss: 0.457360, acc.: 78.91%] [G loss: 1.530001]\n",
      "epoch:34 step:32172 [D loss: 0.530808, acc.: 72.66%] [G loss: 1.378272]\n",
      "epoch:34 step:32173 [D loss: 0.385581, acc.: 83.59%] [G loss: 1.483927]\n",
      "epoch:34 step:32174 [D loss: 0.470496, acc.: 79.69%] [G loss: 1.722908]\n",
      "epoch:34 step:32175 [D loss: 0.459356, acc.: 76.56%] [G loss: 1.810524]\n",
      "epoch:34 step:32176 [D loss: 0.562841, acc.: 71.88%] [G loss: 1.377167]\n",
      "epoch:34 step:32177 [D loss: 0.813354, acc.: 48.44%] [G loss: 1.259833]\n",
      "epoch:34 step:32178 [D loss: 0.467216, acc.: 82.03%] [G loss: 1.638523]\n",
      "epoch:34 step:32179 [D loss: 0.659173, acc.: 63.28%] [G loss: 1.101395]\n",
      "epoch:34 step:32180 [D loss: 0.418682, acc.: 82.03%] [G loss: 1.401653]\n",
      "epoch:34 step:32181 [D loss: 0.778629, acc.: 49.22%] [G loss: 1.463117]\n",
      "epoch:34 step:32182 [D loss: 0.519076, acc.: 74.22%] [G loss: 1.372450]\n",
      "epoch:34 step:32183 [D loss: 0.656845, acc.: 60.16%] [G loss: 1.477982]\n",
      "epoch:34 step:32184 [D loss: 0.555351, acc.: 73.44%] [G loss: 1.303405]\n",
      "epoch:34 step:32185 [D loss: 0.455114, acc.: 78.12%] [G loss: 2.086013]\n",
      "epoch:34 step:32186 [D loss: 0.359159, acc.: 88.28%] [G loss: 2.006476]\n",
      "epoch:34 step:32187 [D loss: 0.430311, acc.: 84.38%] [G loss: 1.289855]\n",
      "epoch:34 step:32188 [D loss: 0.420855, acc.: 82.03%] [G loss: 1.457646]\n",
      "epoch:34 step:32189 [D loss: 0.402222, acc.: 86.72%] [G loss: 1.532057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32190 [D loss: 0.415266, acc.: 82.81%] [G loss: 1.769158]\n",
      "epoch:34 step:32191 [D loss: 0.478439, acc.: 77.34%] [G loss: 1.839238]\n",
      "epoch:34 step:32192 [D loss: 0.478191, acc.: 77.34%] [G loss: 1.500681]\n",
      "epoch:34 step:32193 [D loss: 0.500583, acc.: 76.56%] [G loss: 1.497750]\n",
      "epoch:34 step:32194 [D loss: 0.412043, acc.: 85.94%] [G loss: 1.480499]\n",
      "epoch:34 step:32195 [D loss: 0.579741, acc.: 71.09%] [G loss: 1.385715]\n",
      "epoch:34 step:32196 [D loss: 0.332147, acc.: 89.84%] [G loss: 1.531595]\n",
      "epoch:34 step:32197 [D loss: 0.379684, acc.: 83.59%] [G loss: 1.609378]\n",
      "epoch:34 step:32198 [D loss: 0.469341, acc.: 78.12%] [G loss: 1.704381]\n",
      "epoch:34 step:32199 [D loss: 0.611505, acc.: 68.75%] [G loss: 1.621691]\n",
      "epoch:34 step:32200 [D loss: 0.698255, acc.: 60.94%] [G loss: 1.426814]\n",
      "##############\n",
      "[2.68413176 2.15207241 2.16642034 2.98598465 0.91904028 5.86393577\n",
      " 2.29348658 2.61772507 4.00790152 5.25252153]\n",
      "##########\n",
      "epoch:34 step:32201 [D loss: 0.630559, acc.: 64.84%] [G loss: 1.801840]\n",
      "epoch:34 step:32202 [D loss: 0.500656, acc.: 75.00%] [G loss: 1.408483]\n",
      "epoch:34 step:32203 [D loss: 0.522181, acc.: 78.12%] [G loss: 1.021459]\n",
      "epoch:34 step:32204 [D loss: 0.456371, acc.: 80.47%] [G loss: 1.440229]\n",
      "epoch:34 step:32205 [D loss: 0.657440, acc.: 67.19%] [G loss: 1.153480]\n",
      "epoch:34 step:32206 [D loss: 0.548039, acc.: 71.09%] [G loss: 1.207212]\n",
      "epoch:34 step:32207 [D loss: 0.686546, acc.: 60.94%] [G loss: 1.821148]\n",
      "epoch:34 step:32208 [D loss: 0.357850, acc.: 88.28%] [G loss: 1.908549]\n",
      "epoch:34 step:32209 [D loss: 0.519636, acc.: 73.44%] [G loss: 1.401782]\n",
      "epoch:34 step:32210 [D loss: 0.711938, acc.: 61.72%] [G loss: 0.948984]\n",
      "epoch:34 step:32211 [D loss: 0.492954, acc.: 75.78%] [G loss: 1.376607]\n",
      "epoch:34 step:32212 [D loss: 0.508021, acc.: 74.22%] [G loss: 1.594167]\n",
      "epoch:34 step:32213 [D loss: 0.448802, acc.: 79.69%] [G loss: 1.369617]\n",
      "epoch:34 step:32214 [D loss: 0.429728, acc.: 85.94%] [G loss: 1.274465]\n",
      "epoch:34 step:32215 [D loss: 0.597854, acc.: 70.31%] [G loss: 1.196150]\n",
      "epoch:34 step:32216 [D loss: 0.411032, acc.: 81.25%] [G loss: 1.131985]\n",
      "epoch:34 step:32217 [D loss: 0.595624, acc.: 71.09%] [G loss: 1.451393]\n",
      "epoch:34 step:32218 [D loss: 0.355272, acc.: 85.94%] [G loss: 1.762298]\n",
      "epoch:34 step:32219 [D loss: 0.431056, acc.: 79.69%] [G loss: 1.602685]\n",
      "epoch:34 step:32220 [D loss: 0.433131, acc.: 83.59%] [G loss: 1.755646]\n",
      "epoch:34 step:32221 [D loss: 0.485902, acc.: 76.56%] [G loss: 1.835605]\n",
      "epoch:34 step:32222 [D loss: 0.670353, acc.: 59.38%] [G loss: 1.266928]\n",
      "epoch:34 step:32223 [D loss: 0.478899, acc.: 76.56%] [G loss: 1.724749]\n",
      "epoch:34 step:32224 [D loss: 0.502351, acc.: 74.22%] [G loss: 1.449689]\n",
      "epoch:34 step:32225 [D loss: 0.747696, acc.: 53.91%] [G loss: 1.469488]\n",
      "epoch:34 step:32226 [D loss: 0.381776, acc.: 85.94%] [G loss: 1.865206]\n",
      "epoch:34 step:32227 [D loss: 0.519245, acc.: 67.19%] [G loss: 1.931868]\n",
      "epoch:34 step:32228 [D loss: 0.480158, acc.: 75.00%] [G loss: 2.275264]\n",
      "epoch:34 step:32229 [D loss: 0.519491, acc.: 76.56%] [G loss: 1.867974]\n",
      "epoch:34 step:32230 [D loss: 0.753768, acc.: 56.25%] [G loss: 1.321492]\n",
      "epoch:34 step:32231 [D loss: 0.429512, acc.: 85.16%] [G loss: 1.484919]\n",
      "epoch:34 step:32232 [D loss: 0.550122, acc.: 73.44%] [G loss: 1.139356]\n",
      "epoch:34 step:32233 [D loss: 0.526410, acc.: 72.66%] [G loss: 1.420462]\n",
      "epoch:34 step:32234 [D loss: 0.491232, acc.: 75.00%] [G loss: 1.679788]\n",
      "epoch:34 step:32235 [D loss: 0.452524, acc.: 79.69%] [G loss: 1.544964]\n",
      "epoch:34 step:32236 [D loss: 0.522675, acc.: 72.66%] [G loss: 1.286756]\n",
      "epoch:34 step:32237 [D loss: 0.376119, acc.: 84.38%] [G loss: 1.692462]\n",
      "epoch:34 step:32238 [D loss: 0.661804, acc.: 60.16%] [G loss: 1.457278]\n",
      "epoch:34 step:32239 [D loss: 0.470335, acc.: 78.91%] [G loss: 1.179445]\n",
      "epoch:34 step:32240 [D loss: 0.474288, acc.: 76.56%] [G loss: 1.190623]\n",
      "epoch:34 step:32241 [D loss: 0.294149, acc.: 92.97%] [G loss: 1.415096]\n",
      "epoch:34 step:32242 [D loss: 0.386400, acc.: 86.72%] [G loss: 1.600758]\n",
      "epoch:34 step:32243 [D loss: 0.610310, acc.: 65.62%] [G loss: 1.514013]\n",
      "epoch:34 step:32244 [D loss: 0.432251, acc.: 80.47%] [G loss: 1.442351]\n",
      "epoch:34 step:32245 [D loss: 0.464274, acc.: 80.47%] [G loss: 1.931418]\n",
      "epoch:34 step:32246 [D loss: 0.722589, acc.: 51.56%] [G loss: 1.182590]\n",
      "epoch:34 step:32247 [D loss: 0.558934, acc.: 73.44%] [G loss: 1.826271]\n",
      "epoch:34 step:32248 [D loss: 0.613050, acc.: 65.62%] [G loss: 1.300989]\n",
      "epoch:34 step:32249 [D loss: 0.609822, acc.: 68.75%] [G loss: 1.358049]\n",
      "epoch:34 step:32250 [D loss: 0.717292, acc.: 55.47%] [G loss: 1.350726]\n",
      "epoch:34 step:32251 [D loss: 0.548358, acc.: 72.66%] [G loss: 1.850051]\n",
      "epoch:34 step:32252 [D loss: 0.586212, acc.: 70.31%] [G loss: 1.030971]\n",
      "epoch:34 step:32253 [D loss: 0.570350, acc.: 75.78%] [G loss: 1.345851]\n",
      "epoch:34 step:32254 [D loss: 0.548371, acc.: 73.44%] [G loss: 2.243367]\n",
      "epoch:34 step:32255 [D loss: 0.523839, acc.: 76.56%] [G loss: 1.378083]\n",
      "epoch:34 step:32256 [D loss: 0.442879, acc.: 78.91%] [G loss: 1.829562]\n",
      "epoch:34 step:32257 [D loss: 0.384008, acc.: 85.94%] [G loss: 1.301199]\n",
      "epoch:34 step:32258 [D loss: 0.573736, acc.: 68.75%] [G loss: 1.740412]\n",
      "epoch:34 step:32259 [D loss: 0.605979, acc.: 69.53%] [G loss: 1.558756]\n",
      "epoch:34 step:32260 [D loss: 0.372810, acc.: 82.81%] [G loss: 1.540572]\n",
      "epoch:34 step:32261 [D loss: 0.679235, acc.: 63.28%] [G loss: 1.190482]\n",
      "epoch:34 step:32262 [D loss: 0.470302, acc.: 81.25%] [G loss: 1.487171]\n",
      "epoch:34 step:32263 [D loss: 0.463396, acc.: 77.34%] [G loss: 1.563607]\n",
      "epoch:34 step:32264 [D loss: 0.537389, acc.: 72.66%] [G loss: 1.482049]\n",
      "epoch:34 step:32265 [D loss: 0.535614, acc.: 76.56%] [G loss: 1.313498]\n",
      "epoch:34 step:32266 [D loss: 0.376437, acc.: 91.41%] [G loss: 1.547686]\n",
      "epoch:34 step:32267 [D loss: 0.575381, acc.: 68.75%] [G loss: 1.114703]\n",
      "epoch:34 step:32268 [D loss: 0.697119, acc.: 56.25%] [G loss: 1.102094]\n",
      "epoch:34 step:32269 [D loss: 0.441647, acc.: 83.59%] [G loss: 2.048372]\n",
      "epoch:34 step:32270 [D loss: 0.448145, acc.: 82.03%] [G loss: 1.459809]\n",
      "epoch:34 step:32271 [D loss: 0.412436, acc.: 83.59%] [G loss: 1.104704]\n",
      "epoch:34 step:32272 [D loss: 0.522891, acc.: 78.91%] [G loss: 1.446568]\n",
      "epoch:34 step:32273 [D loss: 0.438176, acc.: 78.91%] [G loss: 1.523729]\n",
      "epoch:34 step:32274 [D loss: 0.553016, acc.: 72.66%] [G loss: 1.730732]\n",
      "epoch:34 step:32275 [D loss: 0.551823, acc.: 70.31%] [G loss: 1.732286]\n",
      "epoch:34 step:32276 [D loss: 0.853256, acc.: 51.56%] [G loss: 1.323256]\n",
      "epoch:34 step:32277 [D loss: 0.525798, acc.: 72.66%] [G loss: 1.575407]\n",
      "epoch:34 step:32278 [D loss: 0.475323, acc.: 78.12%] [G loss: 1.626463]\n",
      "epoch:34 step:32279 [D loss: 0.445134, acc.: 80.47%] [G loss: 1.343746]\n",
      "epoch:34 step:32280 [D loss: 0.794706, acc.: 50.00%] [G loss: 1.485099]\n",
      "epoch:34 step:32281 [D loss: 0.508015, acc.: 73.44%] [G loss: 1.420630]\n",
      "epoch:34 step:32282 [D loss: 0.523296, acc.: 75.00%] [G loss: 1.348240]\n",
      "epoch:34 step:32283 [D loss: 0.577812, acc.: 69.53%] [G loss: 1.221055]\n",
      "epoch:34 step:32284 [D loss: 0.636583, acc.: 64.84%] [G loss: 1.486242]\n",
      "epoch:34 step:32285 [D loss: 0.741375, acc.: 57.03%] [G loss: 1.001742]\n",
      "epoch:34 step:32286 [D loss: 0.691723, acc.: 62.50%] [G loss: 1.528273]\n",
      "epoch:34 step:32287 [D loss: 0.685711, acc.: 56.25%] [G loss: 1.814924]\n",
      "epoch:34 step:32288 [D loss: 0.464800, acc.: 78.12%] [G loss: 1.408155]\n",
      "epoch:34 step:32289 [D loss: 0.560184, acc.: 78.12%] [G loss: 1.533575]\n",
      "epoch:34 step:32290 [D loss: 0.630357, acc.: 64.06%] [G loss: 1.367343]\n",
      "epoch:34 step:32291 [D loss: 0.582142, acc.: 71.09%] [G loss: 1.763462]\n",
      "epoch:34 step:32292 [D loss: 0.499520, acc.: 74.22%] [G loss: 1.735387]\n",
      "epoch:34 step:32293 [D loss: 0.404420, acc.: 82.03%] [G loss: 1.573238]\n",
      "epoch:34 step:32294 [D loss: 0.523071, acc.: 73.44%] [G loss: 1.624955]\n",
      "epoch:34 step:32295 [D loss: 0.593463, acc.: 69.53%] [G loss: 1.104453]\n",
      "epoch:34 step:32296 [D loss: 0.468162, acc.: 75.78%] [G loss: 1.434003]\n",
      "epoch:34 step:32297 [D loss: 0.552390, acc.: 72.66%] [G loss: 1.405960]\n",
      "epoch:34 step:32298 [D loss: 0.716583, acc.: 57.03%] [G loss: 1.170456]\n",
      "epoch:34 step:32299 [D loss: 0.448596, acc.: 82.03%] [G loss: 1.523331]\n",
      "epoch:34 step:32300 [D loss: 0.602055, acc.: 70.31%] [G loss: 1.728902]\n",
      "epoch:34 step:32301 [D loss: 0.655031, acc.: 60.16%] [G loss: 1.251131]\n",
      "epoch:34 step:32302 [D loss: 0.390559, acc.: 83.59%] [G loss: 1.688287]\n",
      "epoch:34 step:32303 [D loss: 0.340305, acc.: 91.41%] [G loss: 1.435345]\n",
      "epoch:34 step:32304 [D loss: 0.643236, acc.: 62.50%] [G loss: 1.230459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32305 [D loss: 0.378765, acc.: 81.25%] [G loss: 1.987788]\n",
      "epoch:34 step:32306 [D loss: 0.506904, acc.: 75.00%] [G loss: 1.416027]\n",
      "epoch:34 step:32307 [D loss: 0.473326, acc.: 78.12%] [G loss: 1.060174]\n",
      "epoch:34 step:32308 [D loss: 0.723555, acc.: 54.69%] [G loss: 1.156883]\n",
      "epoch:34 step:32309 [D loss: 0.312026, acc.: 89.06%] [G loss: 1.945861]\n",
      "epoch:34 step:32310 [D loss: 0.670140, acc.: 61.72%] [G loss: 1.233611]\n",
      "epoch:34 step:32311 [D loss: 0.416284, acc.: 82.03%] [G loss: 1.494701]\n",
      "epoch:34 step:32312 [D loss: 0.876241, acc.: 46.09%] [G loss: 1.660447]\n",
      "epoch:34 step:32313 [D loss: 0.602734, acc.: 72.66%] [G loss: 1.731087]\n",
      "epoch:34 step:32314 [D loss: 0.906042, acc.: 39.06%] [G loss: 1.200037]\n",
      "epoch:34 step:32315 [D loss: 0.570069, acc.: 69.53%] [G loss: 1.442992]\n",
      "epoch:34 step:32316 [D loss: 0.419145, acc.: 81.25%] [G loss: 1.510111]\n",
      "epoch:34 step:32317 [D loss: 0.475860, acc.: 75.78%] [G loss: 1.173256]\n",
      "epoch:34 step:32318 [D loss: 0.400788, acc.: 84.38%] [G loss: 1.621743]\n",
      "epoch:34 step:32319 [D loss: 0.736224, acc.: 55.47%] [G loss: 1.617915]\n",
      "epoch:34 step:32320 [D loss: 1.049992, acc.: 37.50%] [G loss: 1.385990]\n",
      "epoch:34 step:32321 [D loss: 0.590352, acc.: 64.84%] [G loss: 1.521470]\n",
      "epoch:34 step:32322 [D loss: 0.657830, acc.: 64.84%] [G loss: 1.708690]\n",
      "epoch:34 step:32323 [D loss: 0.410361, acc.: 84.38%] [G loss: 1.379390]\n",
      "epoch:34 step:32324 [D loss: 0.497507, acc.: 76.56%] [G loss: 1.150406]\n",
      "epoch:34 step:32325 [D loss: 0.600716, acc.: 66.41%] [G loss: 1.387783]\n",
      "epoch:34 step:32326 [D loss: 0.420659, acc.: 81.25%] [G loss: 1.498096]\n",
      "epoch:34 step:32327 [D loss: 0.600137, acc.: 64.06%] [G loss: 1.337058]\n",
      "epoch:34 step:32328 [D loss: 0.581589, acc.: 71.88%] [G loss: 1.205982]\n",
      "epoch:34 step:32329 [D loss: 0.668859, acc.: 64.06%] [G loss: 1.359538]\n",
      "epoch:34 step:32330 [D loss: 0.423058, acc.: 85.16%] [G loss: 1.720905]\n",
      "epoch:34 step:32331 [D loss: 0.387422, acc.: 83.59%] [G loss: 1.630085]\n",
      "epoch:34 step:32332 [D loss: 0.514597, acc.: 71.88%] [G loss: 1.815006]\n",
      "epoch:34 step:32333 [D loss: 0.477565, acc.: 78.91%] [G loss: 1.101063]\n",
      "epoch:34 step:32334 [D loss: 0.629332, acc.: 65.62%] [G loss: 1.755203]\n",
      "epoch:34 step:32335 [D loss: 0.554229, acc.: 71.88%] [G loss: 1.677719]\n",
      "epoch:34 step:32336 [D loss: 0.517795, acc.: 71.88%] [G loss: 1.260260]\n",
      "epoch:34 step:32337 [D loss: 0.497012, acc.: 78.91%] [G loss: 1.762412]\n",
      "epoch:34 step:32338 [D loss: 0.521483, acc.: 73.44%] [G loss: 1.358019]\n",
      "epoch:34 step:32339 [D loss: 0.443687, acc.: 80.47%] [G loss: 1.553307]\n",
      "epoch:34 step:32340 [D loss: 0.555324, acc.: 74.22%] [G loss: 1.444760]\n",
      "epoch:34 step:32341 [D loss: 0.527120, acc.: 74.22%] [G loss: 1.290126]\n",
      "epoch:34 step:32342 [D loss: 0.436697, acc.: 82.81%] [G loss: 1.731747]\n",
      "epoch:34 step:32343 [D loss: 0.609060, acc.: 66.41%] [G loss: 1.723589]\n",
      "epoch:34 step:32344 [D loss: 0.646854, acc.: 61.72%] [G loss: 1.479891]\n",
      "epoch:34 step:32345 [D loss: 0.597193, acc.: 69.53%] [G loss: 1.382649]\n",
      "epoch:34 step:32346 [D loss: 0.519599, acc.: 75.00%] [G loss: 1.458626]\n",
      "epoch:34 step:32347 [D loss: 0.409365, acc.: 85.94%] [G loss: 1.448071]\n",
      "epoch:34 step:32348 [D loss: 0.353261, acc.: 85.94%] [G loss: 1.618648]\n",
      "epoch:34 step:32349 [D loss: 0.432861, acc.: 81.25%] [G loss: 1.082985]\n",
      "epoch:34 step:32350 [D loss: 0.545651, acc.: 71.88%] [G loss: 1.272153]\n",
      "epoch:34 step:32351 [D loss: 0.500993, acc.: 72.66%] [G loss: 1.689804]\n",
      "epoch:34 step:32352 [D loss: 0.422177, acc.: 82.81%] [G loss: 1.834917]\n",
      "epoch:34 step:32353 [D loss: 0.576342, acc.: 68.75%] [G loss: 1.635064]\n",
      "epoch:34 step:32354 [D loss: 0.722798, acc.: 57.81%] [G loss: 0.869526]\n",
      "epoch:34 step:32355 [D loss: 0.487910, acc.: 80.47%] [G loss: 1.646049]\n",
      "epoch:34 step:32356 [D loss: 0.397657, acc.: 82.03%] [G loss: 1.296274]\n",
      "epoch:34 step:32357 [D loss: 0.551893, acc.: 74.22%] [G loss: 1.447944]\n",
      "epoch:34 step:32358 [D loss: 0.397491, acc.: 86.72%] [G loss: 1.791723]\n",
      "epoch:34 step:32359 [D loss: 0.350175, acc.: 85.16%] [G loss: 1.849412]\n",
      "epoch:34 step:32360 [D loss: 0.512883, acc.: 79.69%] [G loss: 1.542601]\n",
      "epoch:34 step:32361 [D loss: 0.512159, acc.: 76.56%] [G loss: 1.114335]\n",
      "epoch:34 step:32362 [D loss: 0.703419, acc.: 57.03%] [G loss: 1.203881]\n",
      "epoch:34 step:32363 [D loss: 0.501846, acc.: 78.91%] [G loss: 1.289624]\n",
      "epoch:34 step:32364 [D loss: 0.360352, acc.: 89.84%] [G loss: 1.490336]\n",
      "epoch:34 step:32365 [D loss: 0.479167, acc.: 78.12%] [G loss: 2.248471]\n",
      "epoch:34 step:32366 [D loss: 0.593501, acc.: 68.75%] [G loss: 1.575271]\n",
      "epoch:34 step:32367 [D loss: 0.467154, acc.: 81.25%] [G loss: 1.189762]\n",
      "epoch:34 step:32368 [D loss: 0.436727, acc.: 80.47%] [G loss: 1.884700]\n",
      "epoch:34 step:32369 [D loss: 0.444281, acc.: 82.81%] [G loss: 2.137283]\n",
      "epoch:34 step:32370 [D loss: 0.518143, acc.: 73.44%] [G loss: 1.312091]\n",
      "epoch:34 step:32371 [D loss: 0.365939, acc.: 89.06%] [G loss: 1.577419]\n",
      "epoch:34 step:32372 [D loss: 0.575596, acc.: 71.88%] [G loss: 1.329250]\n",
      "epoch:34 step:32373 [D loss: 0.351086, acc.: 87.50%] [G loss: 1.467879]\n",
      "epoch:34 step:32374 [D loss: 0.624558, acc.: 60.94%] [G loss: 1.433696]\n",
      "epoch:34 step:32375 [D loss: 0.587701, acc.: 70.31%] [G loss: 1.632071]\n",
      "epoch:34 step:32376 [D loss: 0.446139, acc.: 81.25%] [G loss: 1.403407]\n",
      "epoch:34 step:32377 [D loss: 0.473974, acc.: 78.91%] [G loss: 1.731690]\n",
      "epoch:34 step:32378 [D loss: 0.542127, acc.: 73.44%] [G loss: 1.203765]\n",
      "epoch:34 step:32379 [D loss: 0.677721, acc.: 60.94%] [G loss: 1.351367]\n",
      "epoch:34 step:32380 [D loss: 0.688724, acc.: 64.06%] [G loss: 1.159048]\n",
      "epoch:34 step:32381 [D loss: 0.577813, acc.: 72.66%] [G loss: 1.219581]\n",
      "epoch:34 step:32382 [D loss: 0.535274, acc.: 70.31%] [G loss: 1.458005]\n",
      "epoch:34 step:32383 [D loss: 0.719419, acc.: 59.38%] [G loss: 1.182672]\n",
      "epoch:34 step:32384 [D loss: 0.295756, acc.: 91.41%] [G loss: 1.761734]\n",
      "epoch:34 step:32385 [D loss: 0.612448, acc.: 65.62%] [G loss: 1.211425]\n",
      "epoch:34 step:32386 [D loss: 0.648494, acc.: 62.50%] [G loss: 1.859402]\n",
      "epoch:34 step:32387 [D loss: 0.504875, acc.: 75.00%] [G loss: 1.482598]\n",
      "epoch:34 step:32388 [D loss: 0.475385, acc.: 75.00%] [G loss: 0.989713]\n",
      "epoch:34 step:32389 [D loss: 0.639815, acc.: 69.53%] [G loss: 1.869354]\n",
      "epoch:34 step:32390 [D loss: 0.664313, acc.: 63.28%] [G loss: 1.349177]\n",
      "epoch:34 step:32391 [D loss: 0.566091, acc.: 71.88%] [G loss: 1.515879]\n",
      "epoch:34 step:32392 [D loss: 0.532119, acc.: 76.56%] [G loss: 1.497750]\n",
      "epoch:34 step:32393 [D loss: 0.510848, acc.: 76.56%] [G loss: 1.131510]\n",
      "epoch:34 step:32394 [D loss: 0.566364, acc.: 67.97%] [G loss: 1.196385]\n",
      "epoch:34 step:32395 [D loss: 0.586418, acc.: 69.53%] [G loss: 1.280645]\n",
      "epoch:34 step:32396 [D loss: 0.393969, acc.: 84.38%] [G loss: 1.276307]\n",
      "epoch:34 step:32397 [D loss: 0.570875, acc.: 68.75%] [G loss: 1.585132]\n",
      "epoch:34 step:32398 [D loss: 0.416800, acc.: 80.47%] [G loss: 1.718672]\n",
      "epoch:34 step:32399 [D loss: 0.514958, acc.: 71.88%] [G loss: 1.564711]\n",
      "epoch:34 step:32400 [D loss: 0.290262, acc.: 89.84%] [G loss: 1.132622]\n",
      "##############\n",
      "[2.712819   2.15405899 1.79669405 2.8566142  1.05276538 6.03342095\n",
      " 2.20532999 2.58200619 3.85727688 7.14868929]\n",
      "##########\n",
      "epoch:34 step:32401 [D loss: 0.444896, acc.: 82.81%] [G loss: 1.357627]\n",
      "epoch:34 step:32402 [D loss: 0.617806, acc.: 67.97%] [G loss: 1.398635]\n",
      "epoch:34 step:32403 [D loss: 0.700419, acc.: 60.94%] [G loss: 1.567518]\n",
      "epoch:34 step:32404 [D loss: 0.622120, acc.: 67.97%] [G loss: 1.526420]\n",
      "epoch:34 step:32405 [D loss: 0.576879, acc.: 77.34%] [G loss: 1.583957]\n",
      "epoch:34 step:32406 [D loss: 0.529867, acc.: 74.22%] [G loss: 1.404528]\n",
      "epoch:34 step:32407 [D loss: 0.505337, acc.: 74.22%] [G loss: 1.325078]\n",
      "epoch:34 step:32408 [D loss: 0.638045, acc.: 65.62%] [G loss: 1.623846]\n",
      "epoch:34 step:32409 [D loss: 0.728591, acc.: 54.69%] [G loss: 1.385482]\n",
      "epoch:34 step:32410 [D loss: 0.778245, acc.: 54.69%] [G loss: 1.498082]\n",
      "epoch:34 step:32411 [D loss: 0.649432, acc.: 64.06%] [G loss: 1.510116]\n",
      "epoch:34 step:32412 [D loss: 0.557110, acc.: 71.09%] [G loss: 1.357501]\n",
      "epoch:34 step:32413 [D loss: 0.699194, acc.: 59.38%] [G loss: 1.402631]\n",
      "epoch:34 step:32414 [D loss: 0.564992, acc.: 72.66%] [G loss: 1.285327]\n",
      "epoch:34 step:32415 [D loss: 0.571904, acc.: 67.97%] [G loss: 1.546645]\n",
      "epoch:34 step:32416 [D loss: 0.585451, acc.: 74.22%] [G loss: 1.633830]\n",
      "epoch:34 step:32417 [D loss: 0.384741, acc.: 85.94%] [G loss: 1.595203]\n",
      "epoch:34 step:32418 [D loss: 0.548147, acc.: 75.00%] [G loss: 1.634886]\n",
      "epoch:34 step:32419 [D loss: 0.485410, acc.: 79.69%] [G loss: 1.358628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32420 [D loss: 0.525074, acc.: 72.66%] [G loss: 1.789136]\n",
      "epoch:34 step:32421 [D loss: 0.351654, acc.: 87.50%] [G loss: 1.576563]\n",
      "epoch:34 step:32422 [D loss: 0.316420, acc.: 89.84%] [G loss: 1.559772]\n",
      "epoch:34 step:32423 [D loss: 0.345014, acc.: 91.41%] [G loss: 1.283316]\n",
      "epoch:34 step:32424 [D loss: 0.391343, acc.: 84.38%] [G loss: 1.490897]\n",
      "epoch:34 step:32425 [D loss: 0.435696, acc.: 77.34%] [G loss: 1.600655]\n",
      "epoch:34 step:32426 [D loss: 0.920584, acc.: 42.97%] [G loss: 0.862379]\n",
      "epoch:34 step:32427 [D loss: 0.451509, acc.: 80.47%] [G loss: 1.621782]\n",
      "epoch:34 step:32428 [D loss: 0.592399, acc.: 67.19%] [G loss: 1.489975]\n",
      "epoch:34 step:32429 [D loss: 0.393549, acc.: 83.59%] [G loss: 2.113482]\n",
      "epoch:34 step:32430 [D loss: 0.438707, acc.: 79.69%] [G loss: 1.588022]\n",
      "epoch:34 step:32431 [D loss: 0.639609, acc.: 62.50%] [G loss: 1.554063]\n",
      "epoch:34 step:32432 [D loss: 0.777047, acc.: 53.12%] [G loss: 1.461459]\n",
      "epoch:34 step:32433 [D loss: 0.561863, acc.: 71.88%] [G loss: 1.218538]\n",
      "epoch:34 step:32434 [D loss: 0.593601, acc.: 68.75%] [G loss: 1.006692]\n",
      "epoch:34 step:32435 [D loss: 0.507695, acc.: 75.00%] [G loss: 1.312933]\n",
      "epoch:34 step:32436 [D loss: 0.443299, acc.: 80.47%] [G loss: 1.499465]\n",
      "epoch:34 step:32437 [D loss: 0.409906, acc.: 82.81%] [G loss: 1.659905]\n",
      "epoch:34 step:32438 [D loss: 0.632690, acc.: 66.41%] [G loss: 2.017438]\n",
      "epoch:34 step:32439 [D loss: 0.527225, acc.: 74.22%] [G loss: 1.437776]\n",
      "epoch:34 step:32440 [D loss: 0.454683, acc.: 78.91%] [G loss: 1.497242]\n",
      "epoch:34 step:32441 [D loss: 0.592494, acc.: 64.84%] [G loss: 2.099800]\n",
      "epoch:34 step:32442 [D loss: 0.483819, acc.: 77.34%] [G loss: 1.812941]\n",
      "epoch:34 step:32443 [D loss: 0.428633, acc.: 78.91%] [G loss: 1.548036]\n",
      "epoch:34 step:32444 [D loss: 0.534439, acc.: 76.56%] [G loss: 1.284649]\n",
      "epoch:34 step:32445 [D loss: 0.547692, acc.: 68.75%] [G loss: 1.209358]\n",
      "epoch:34 step:32446 [D loss: 0.825852, acc.: 52.34%] [G loss: 1.126883]\n",
      "epoch:34 step:32447 [D loss: 0.524811, acc.: 72.66%] [G loss: 2.165709]\n",
      "epoch:34 step:32448 [D loss: 0.498279, acc.: 77.34%] [G loss: 1.575627]\n",
      "epoch:34 step:32449 [D loss: 0.434690, acc.: 83.59%] [G loss: 1.847500]\n",
      "epoch:34 step:32450 [D loss: 0.419521, acc.: 83.59%] [G loss: 1.322835]\n",
      "epoch:34 step:32451 [D loss: 0.566995, acc.: 71.88%] [G loss: 1.585645]\n",
      "epoch:34 step:32452 [D loss: 0.442391, acc.: 82.03%] [G loss: 1.177801]\n",
      "epoch:34 step:32453 [D loss: 0.745682, acc.: 52.34%] [G loss: 1.177992]\n",
      "epoch:34 step:32454 [D loss: 0.764508, acc.: 53.91%] [G loss: 1.302150]\n",
      "epoch:34 step:32455 [D loss: 0.376479, acc.: 83.59%] [G loss: 1.181060]\n",
      "epoch:34 step:32456 [D loss: 0.673564, acc.: 64.06%] [G loss: 1.006247]\n",
      "epoch:34 step:32457 [D loss: 0.459994, acc.: 82.81%] [G loss: 1.273525]\n",
      "epoch:34 step:32458 [D loss: 0.453881, acc.: 80.47%] [G loss: 1.214740]\n",
      "epoch:34 step:32459 [D loss: 0.570107, acc.: 67.19%] [G loss: 1.398054]\n",
      "epoch:34 step:32460 [D loss: 0.462012, acc.: 81.25%] [G loss: 1.936661]\n",
      "epoch:34 step:32461 [D loss: 0.474541, acc.: 79.69%] [G loss: 1.538985]\n",
      "epoch:34 step:32462 [D loss: 0.570090, acc.: 69.53%] [G loss: 1.224275]\n",
      "epoch:34 step:32463 [D loss: 0.516360, acc.: 73.44%] [G loss: 1.376123]\n",
      "epoch:34 step:32464 [D loss: 0.544159, acc.: 72.66%] [G loss: 1.064416]\n",
      "epoch:34 step:32465 [D loss: 0.480843, acc.: 75.78%] [G loss: 0.708869]\n",
      "epoch:34 step:32466 [D loss: 0.411045, acc.: 82.81%] [G loss: 1.230378]\n",
      "epoch:34 step:32467 [D loss: 0.881004, acc.: 52.34%] [G loss: 1.490109]\n",
      "epoch:34 step:32468 [D loss: 0.464550, acc.: 77.34%] [G loss: 1.952338]\n",
      "epoch:34 step:32469 [D loss: 0.518631, acc.: 77.34%] [G loss: 1.491981]\n",
      "epoch:34 step:32470 [D loss: 0.696938, acc.: 60.16%] [G loss: 1.473516]\n",
      "epoch:34 step:32471 [D loss: 0.554573, acc.: 74.22%] [G loss: 1.088910]\n",
      "epoch:34 step:32472 [D loss: 0.414002, acc.: 81.25%] [G loss: 1.731298]\n",
      "epoch:34 step:32473 [D loss: 0.422579, acc.: 79.69%] [G loss: 1.684682]\n",
      "epoch:34 step:32474 [D loss: 0.487096, acc.: 75.78%] [G loss: 1.317135]\n",
      "epoch:34 step:32475 [D loss: 0.661060, acc.: 63.28%] [G loss: 1.059873]\n",
      "epoch:34 step:32476 [D loss: 0.539173, acc.: 72.66%] [G loss: 0.984142]\n",
      "epoch:34 step:32477 [D loss: 0.571698, acc.: 68.75%] [G loss: 1.156976]\n",
      "epoch:34 step:32478 [D loss: 0.821272, acc.: 51.56%] [G loss: 1.584740]\n",
      "epoch:34 step:32479 [D loss: 0.663283, acc.: 62.50%] [G loss: 1.383058]\n",
      "epoch:34 step:32480 [D loss: 0.643145, acc.: 65.62%] [G loss: 1.444802]\n",
      "epoch:34 step:32481 [D loss: 0.448816, acc.: 78.12%] [G loss: 2.313817]\n",
      "epoch:34 step:32482 [D loss: 0.638660, acc.: 66.41%] [G loss: 1.859860]\n",
      "epoch:34 step:32483 [D loss: 0.467999, acc.: 79.69%] [G loss: 2.006117]\n",
      "epoch:34 step:32484 [D loss: 0.520690, acc.: 75.00%] [G loss: 1.430750]\n",
      "epoch:34 step:32485 [D loss: 0.472198, acc.: 80.47%] [G loss: 1.638944]\n",
      "epoch:34 step:32486 [D loss: 0.630392, acc.: 68.75%] [G loss: 1.325252]\n",
      "epoch:34 step:32487 [D loss: 0.469045, acc.: 73.44%] [G loss: 1.304293]\n",
      "epoch:34 step:32488 [D loss: 0.480978, acc.: 78.12%] [G loss: 1.707813]\n",
      "epoch:34 step:32489 [D loss: 0.587197, acc.: 71.09%] [G loss: 1.264521]\n",
      "epoch:34 step:32490 [D loss: 0.387346, acc.: 86.72%] [G loss: 1.480885]\n",
      "epoch:34 step:32491 [D loss: 0.351288, acc.: 86.72%] [G loss: 1.853822]\n",
      "epoch:34 step:32492 [D loss: 0.459148, acc.: 81.25%] [G loss: 1.815762]\n",
      "epoch:34 step:32493 [D loss: 0.533255, acc.: 74.22%] [G loss: 1.446681]\n",
      "epoch:34 step:32494 [D loss: 0.551926, acc.: 75.78%] [G loss: 1.297154]\n",
      "epoch:34 step:32495 [D loss: 0.548704, acc.: 74.22%] [G loss: 1.149448]\n",
      "epoch:34 step:32496 [D loss: 0.559984, acc.: 71.09%] [G loss: 1.577782]\n",
      "epoch:34 step:32497 [D loss: 0.521309, acc.: 73.44%] [G loss: 1.270298]\n",
      "epoch:34 step:32498 [D loss: 0.474134, acc.: 75.00%] [G loss: 1.990839]\n",
      "epoch:34 step:32499 [D loss: 0.649632, acc.: 64.06%] [G loss: 1.674001]\n",
      "epoch:34 step:32500 [D loss: 0.418668, acc.: 83.59%] [G loss: 1.995247]\n",
      "epoch:34 step:32501 [D loss: 0.665882, acc.: 68.75%] [G loss: 1.838870]\n",
      "epoch:34 step:32502 [D loss: 0.448349, acc.: 80.47%] [G loss: 1.417407]\n",
      "epoch:34 step:32503 [D loss: 0.518362, acc.: 78.12%] [G loss: 1.731520]\n",
      "epoch:34 step:32504 [D loss: 0.526269, acc.: 74.22%] [G loss: 1.773315]\n",
      "epoch:34 step:32505 [D loss: 0.730505, acc.: 61.72%] [G loss: 1.435516]\n",
      "epoch:34 step:32506 [D loss: 0.513478, acc.: 74.22%] [G loss: 1.617028]\n",
      "epoch:34 step:32507 [D loss: 0.474246, acc.: 79.69%] [G loss: 2.332047]\n",
      "epoch:34 step:32508 [D loss: 0.533057, acc.: 71.09%] [G loss: 1.824545]\n",
      "epoch:34 step:32509 [D loss: 0.162593, acc.: 96.09%] [G loss: 1.664790]\n",
      "epoch:34 step:32510 [D loss: 0.374081, acc.: 89.06%] [G loss: 1.549801]\n",
      "epoch:34 step:32511 [D loss: 0.416643, acc.: 84.38%] [G loss: 1.727659]\n",
      "epoch:34 step:32512 [D loss: 0.432574, acc.: 78.12%] [G loss: 0.966291]\n",
      "epoch:34 step:32513 [D loss: 0.626493, acc.: 67.97%] [G loss: 1.236103]\n",
      "epoch:34 step:32514 [D loss: 0.436529, acc.: 83.59%] [G loss: 1.256274]\n",
      "epoch:34 step:32515 [D loss: 0.668738, acc.: 57.03%] [G loss: 1.434902]\n",
      "epoch:34 step:32516 [D loss: 0.559799, acc.: 71.09%] [G loss: 1.016690]\n",
      "epoch:34 step:32517 [D loss: 0.491460, acc.: 74.22%] [G loss: 1.576423]\n",
      "epoch:34 step:32518 [D loss: 0.692042, acc.: 62.50%] [G loss: 1.459979]\n",
      "epoch:34 step:32519 [D loss: 0.804981, acc.: 59.38%] [G loss: 1.377922]\n",
      "epoch:34 step:32520 [D loss: 0.614881, acc.: 61.72%] [G loss: 1.387131]\n",
      "epoch:34 step:32521 [D loss: 0.518421, acc.: 72.66%] [G loss: 1.667614]\n",
      "epoch:34 step:32522 [D loss: 0.594654, acc.: 70.31%] [G loss: 1.204359]\n",
      "epoch:34 step:32523 [D loss: 0.521279, acc.: 74.22%] [G loss: 1.644945]\n",
      "epoch:34 step:32524 [D loss: 0.313303, acc.: 89.06%] [G loss: 1.351406]\n",
      "epoch:34 step:32525 [D loss: 0.443927, acc.: 82.81%] [G loss: 1.354887]\n",
      "epoch:34 step:32526 [D loss: 0.603432, acc.: 66.41%] [G loss: 1.544798]\n",
      "epoch:34 step:32527 [D loss: 0.469066, acc.: 78.12%] [G loss: 1.599269]\n",
      "epoch:34 step:32528 [D loss: 0.453151, acc.: 80.47%] [G loss: 1.342130]\n",
      "epoch:34 step:32529 [D loss: 0.614597, acc.: 68.75%] [G loss: 1.454677]\n",
      "epoch:34 step:32530 [D loss: 0.620296, acc.: 66.41%] [G loss: 1.108376]\n",
      "epoch:34 step:32531 [D loss: 0.571082, acc.: 67.97%] [G loss: 1.840426]\n",
      "epoch:34 step:32532 [D loss: 0.601224, acc.: 64.06%] [G loss: 1.546851]\n",
      "epoch:34 step:32533 [D loss: 0.723819, acc.: 54.69%] [G loss: 1.228863]\n",
      "epoch:34 step:32534 [D loss: 0.399732, acc.: 84.38%] [G loss: 1.829018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32535 [D loss: 0.691060, acc.: 58.59%] [G loss: 1.819173]\n",
      "epoch:34 step:32536 [D loss: 0.510871, acc.: 68.75%] [G loss: 1.453522]\n",
      "epoch:34 step:32537 [D loss: 0.630651, acc.: 64.06%] [G loss: 1.358520]\n",
      "epoch:34 step:32538 [D loss: 0.597671, acc.: 67.97%] [G loss: 1.240454]\n",
      "epoch:34 step:32539 [D loss: 0.633541, acc.: 70.31%] [G loss: 0.792530]\n",
      "epoch:34 step:32540 [D loss: 0.332086, acc.: 89.06%] [G loss: 1.230035]\n",
      "epoch:34 step:32541 [D loss: 0.591846, acc.: 67.19%] [G loss: 1.821393]\n",
      "epoch:34 step:32542 [D loss: 0.754696, acc.: 58.59%] [G loss: 1.593483]\n",
      "epoch:34 step:32543 [D loss: 0.387301, acc.: 83.59%] [G loss: 1.667184]\n",
      "epoch:34 step:32544 [D loss: 0.502793, acc.: 80.47%] [G loss: 1.320886]\n",
      "epoch:34 step:32545 [D loss: 0.385530, acc.: 86.72%] [G loss: 1.515984]\n",
      "epoch:34 step:32546 [D loss: 0.707300, acc.: 57.03%] [G loss: 1.732930]\n",
      "epoch:34 step:32547 [D loss: 0.618068, acc.: 60.16%] [G loss: 1.795836]\n",
      "epoch:34 step:32548 [D loss: 0.389625, acc.: 84.38%] [G loss: 1.419887]\n",
      "epoch:34 step:32549 [D loss: 0.454787, acc.: 77.34%] [G loss: 1.487288]\n",
      "epoch:34 step:32550 [D loss: 0.513169, acc.: 73.44%] [G loss: 1.142280]\n",
      "epoch:34 step:32551 [D loss: 0.337440, acc.: 89.06%] [G loss: 1.375879]\n",
      "epoch:34 step:32552 [D loss: 0.628031, acc.: 65.62%] [G loss: 1.369694]\n",
      "epoch:34 step:32553 [D loss: 0.496830, acc.: 75.78%] [G loss: 1.318519]\n",
      "epoch:34 step:32554 [D loss: 0.530098, acc.: 73.44%] [G loss: 1.370632]\n",
      "epoch:34 step:32555 [D loss: 0.441388, acc.: 79.69%] [G loss: 1.203254]\n",
      "epoch:34 step:32556 [D loss: 0.564517, acc.: 69.53%] [G loss: 1.320942]\n",
      "epoch:34 step:32557 [D loss: 0.724219, acc.: 52.34%] [G loss: 1.375851]\n",
      "epoch:34 step:32558 [D loss: 0.511742, acc.: 76.56%] [G loss: 1.202764]\n",
      "epoch:34 step:32559 [D loss: 0.560653, acc.: 71.09%] [G loss: 1.395805]\n",
      "epoch:34 step:32560 [D loss: 0.360828, acc.: 87.50%] [G loss: 2.004130]\n",
      "epoch:34 step:32561 [D loss: 0.467105, acc.: 75.78%] [G loss: 1.533025]\n",
      "epoch:34 step:32562 [D loss: 0.906779, acc.: 41.41%] [G loss: 0.953405]\n",
      "epoch:34 step:32563 [D loss: 0.350775, acc.: 88.28%] [G loss: 1.591770]\n",
      "epoch:34 step:32564 [D loss: 0.576463, acc.: 71.09%] [G loss: 1.662843]\n",
      "epoch:34 step:32565 [D loss: 0.482285, acc.: 78.12%] [G loss: 1.400046]\n",
      "epoch:34 step:32566 [D loss: 0.478646, acc.: 82.03%] [G loss: 2.083914]\n",
      "epoch:34 step:32567 [D loss: 0.655452, acc.: 67.19%] [G loss: 1.677806]\n",
      "epoch:34 step:32568 [D loss: 0.498669, acc.: 72.66%] [G loss: 1.414348]\n",
      "epoch:34 step:32569 [D loss: 0.504767, acc.: 75.78%] [G loss: 1.560827]\n",
      "epoch:34 step:32570 [D loss: 0.590358, acc.: 71.88%] [G loss: 1.629385]\n",
      "epoch:34 step:32571 [D loss: 0.472816, acc.: 79.69%] [G loss: 2.209630]\n",
      "epoch:34 step:32572 [D loss: 0.489988, acc.: 75.00%] [G loss: 1.178272]\n",
      "epoch:34 step:32573 [D loss: 0.562695, acc.: 69.53%] [G loss: 1.600724]\n",
      "epoch:34 step:32574 [D loss: 0.538033, acc.: 74.22%] [G loss: 1.828657]\n",
      "epoch:34 step:32575 [D loss: 0.583990, acc.: 67.97%] [G loss: 1.756717]\n",
      "epoch:34 step:32576 [D loss: 0.333776, acc.: 87.50%] [G loss: 1.848992]\n",
      "epoch:34 step:32577 [D loss: 0.779886, acc.: 56.25%] [G loss: 1.498839]\n",
      "epoch:34 step:32578 [D loss: 0.629551, acc.: 62.50%] [G loss: 1.903673]\n",
      "epoch:34 step:32579 [D loss: 0.674090, acc.: 63.28%] [G loss: 1.550225]\n",
      "epoch:34 step:32580 [D loss: 0.514178, acc.: 74.22%] [G loss: 1.270954]\n",
      "epoch:34 step:32581 [D loss: 0.520993, acc.: 72.66%] [G loss: 1.235503]\n",
      "epoch:34 step:32582 [D loss: 0.506744, acc.: 76.56%] [G loss: 1.907186]\n",
      "epoch:34 step:32583 [D loss: 0.514104, acc.: 77.34%] [G loss: 1.403735]\n",
      "epoch:34 step:32584 [D loss: 0.441458, acc.: 82.81%] [G loss: 1.181170]\n",
      "epoch:34 step:32585 [D loss: 0.459079, acc.: 78.12%] [G loss: 1.439556]\n",
      "epoch:34 step:32586 [D loss: 0.573333, acc.: 68.75%] [G loss: 1.394040]\n",
      "epoch:34 step:32587 [D loss: 0.569453, acc.: 71.88%] [G loss: 1.282403]\n",
      "epoch:34 step:32588 [D loss: 0.584198, acc.: 67.97%] [G loss: 1.454624]\n",
      "epoch:34 step:32589 [D loss: 0.613102, acc.: 67.97%] [G loss: 1.167647]\n",
      "epoch:34 step:32590 [D loss: 0.437224, acc.: 80.47%] [G loss: 1.914594]\n",
      "epoch:34 step:32591 [D loss: 0.383519, acc.: 83.59%] [G loss: 2.004351]\n",
      "epoch:34 step:32592 [D loss: 0.481792, acc.: 77.34%] [G loss: 1.875993]\n",
      "epoch:34 step:32593 [D loss: 0.588570, acc.: 67.19%] [G loss: 1.691670]\n",
      "epoch:34 step:32594 [D loss: 0.655448, acc.: 58.59%] [G loss: 1.311250]\n",
      "epoch:34 step:32595 [D loss: 0.410028, acc.: 82.03%] [G loss: 2.323451]\n",
      "epoch:34 step:32596 [D loss: 0.691089, acc.: 59.38%] [G loss: 1.530043]\n",
      "epoch:34 step:32597 [D loss: 0.729864, acc.: 54.69%] [G loss: 1.054072]\n",
      "epoch:34 step:32598 [D loss: 0.503148, acc.: 72.66%] [G loss: 1.288749]\n",
      "epoch:34 step:32599 [D loss: 0.452978, acc.: 82.81%] [G loss: 1.529544]\n",
      "epoch:34 step:32600 [D loss: 0.411716, acc.: 86.72%] [G loss: 1.403200]\n",
      "##############\n",
      "[2.69859238 1.96047649 1.8866421  2.74332055 0.62740139 5.55649619\n",
      " 2.46291227 2.49300778 3.94231506 5.37733578]\n",
      "##########\n",
      "epoch:34 step:32601 [D loss: 0.561898, acc.: 71.09%] [G loss: 1.429599]\n",
      "epoch:34 step:32602 [D loss: 0.531147, acc.: 72.66%] [G loss: 1.859359]\n",
      "epoch:34 step:32603 [D loss: 0.446612, acc.: 82.03%] [G loss: 2.038946]\n",
      "epoch:34 step:32604 [D loss: 0.488012, acc.: 71.88%] [G loss: 1.453205]\n",
      "epoch:34 step:32605 [D loss: 0.289552, acc.: 92.19%] [G loss: 1.718379]\n",
      "epoch:34 step:32606 [D loss: 0.539977, acc.: 71.88%] [G loss: 1.111532]\n",
      "epoch:34 step:32607 [D loss: 0.460606, acc.: 78.12%] [G loss: 1.600968]\n",
      "epoch:34 step:32608 [D loss: 0.513364, acc.: 75.78%] [G loss: 1.853573]\n",
      "epoch:34 step:32609 [D loss: 0.534069, acc.: 73.44%] [G loss: 1.277008]\n",
      "epoch:34 step:32610 [D loss: 0.429637, acc.: 81.25%] [G loss: 1.245368]\n",
      "epoch:34 step:32611 [D loss: 0.643540, acc.: 66.41%] [G loss: 1.455130]\n",
      "epoch:34 step:32612 [D loss: 0.617074, acc.: 67.97%] [G loss: 1.576027]\n",
      "epoch:34 step:32613 [D loss: 0.646069, acc.: 69.53%] [G loss: 1.299883]\n",
      "epoch:34 step:32614 [D loss: 0.646397, acc.: 60.94%] [G loss: 1.385205]\n",
      "epoch:34 step:32615 [D loss: 0.580962, acc.: 68.75%] [G loss: 1.457369]\n",
      "epoch:34 step:32616 [D loss: 0.605411, acc.: 64.84%] [G loss: 1.633584]\n",
      "epoch:34 step:32617 [D loss: 0.362349, acc.: 86.72%] [G loss: 1.478009]\n",
      "epoch:34 step:32618 [D loss: 0.539847, acc.: 77.34%] [G loss: 1.514912]\n",
      "epoch:34 step:32619 [D loss: 0.642230, acc.: 64.06%] [G loss: 1.804908]\n",
      "epoch:34 step:32620 [D loss: 0.509097, acc.: 78.91%] [G loss: 1.556966]\n",
      "epoch:34 step:32621 [D loss: 0.501745, acc.: 75.78%] [G loss: 1.581104]\n",
      "epoch:34 step:32622 [D loss: 0.517008, acc.: 73.44%] [G loss: 1.711847]\n",
      "epoch:34 step:32623 [D loss: 0.396495, acc.: 85.94%] [G loss: 1.385877]\n",
      "epoch:34 step:32624 [D loss: 0.497511, acc.: 78.91%] [G loss: 1.548833]\n",
      "epoch:34 step:32625 [D loss: 0.301275, acc.: 89.84%] [G loss: 2.029817]\n",
      "epoch:34 step:32626 [D loss: 0.413447, acc.: 82.03%] [G loss: 1.953295]\n",
      "epoch:34 step:32627 [D loss: 0.551948, acc.: 71.88%] [G loss: 1.895123]\n",
      "epoch:34 step:32628 [D loss: 0.501696, acc.: 78.91%] [G loss: 1.323862]\n",
      "epoch:34 step:32629 [D loss: 0.540848, acc.: 74.22%] [G loss: 1.203138]\n",
      "epoch:34 step:32630 [D loss: 0.586574, acc.: 69.53%] [G loss: 1.526257]\n",
      "epoch:34 step:32631 [D loss: 0.656347, acc.: 64.06%] [G loss: 1.344540]\n",
      "epoch:34 step:32632 [D loss: 0.678817, acc.: 60.16%] [G loss: 1.490884]\n",
      "epoch:34 step:32633 [D loss: 0.387266, acc.: 85.16%] [G loss: 1.863980]\n",
      "epoch:34 step:32634 [D loss: 0.576261, acc.: 72.66%] [G loss: 2.128835]\n",
      "epoch:34 step:32635 [D loss: 0.616572, acc.: 66.41%] [G loss: 1.860770]\n",
      "epoch:34 step:32636 [D loss: 0.644155, acc.: 66.41%] [G loss: 1.366790]\n",
      "epoch:34 step:32637 [D loss: 0.531903, acc.: 72.66%] [G loss: 1.559704]\n",
      "epoch:34 step:32638 [D loss: 0.599885, acc.: 66.41%] [G loss: 1.254219]\n",
      "epoch:34 step:32639 [D loss: 0.614670, acc.: 66.41%] [G loss: 1.517704]\n",
      "epoch:34 step:32640 [D loss: 0.452285, acc.: 75.78%] [G loss: 1.354790]\n",
      "epoch:34 step:32641 [D loss: 0.629836, acc.: 64.84%] [G loss: 1.324124]\n",
      "epoch:34 step:32642 [D loss: 0.766262, acc.: 55.47%] [G loss: 1.436580]\n",
      "epoch:34 step:32643 [D loss: 0.409381, acc.: 82.03%] [G loss: 1.614439]\n",
      "epoch:34 step:32644 [D loss: 0.762316, acc.: 58.59%] [G loss: 1.773437]\n",
      "epoch:34 step:32645 [D loss: 0.502870, acc.: 74.22%] [G loss: 1.738388]\n",
      "epoch:34 step:32646 [D loss: 0.396046, acc.: 86.72%] [G loss: 1.470470]\n",
      "epoch:34 step:32647 [D loss: 0.413460, acc.: 82.03%] [G loss: 1.528715]\n",
      "epoch:34 step:32648 [D loss: 0.709292, acc.: 60.16%] [G loss: 1.110182]\n",
      "epoch:34 step:32649 [D loss: 0.385816, acc.: 83.59%] [G loss: 2.103027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32650 [D loss: 0.528055, acc.: 74.22%] [G loss: 1.847395]\n",
      "epoch:34 step:32651 [D loss: 0.455833, acc.: 79.69%] [G loss: 1.500921]\n",
      "epoch:34 step:32652 [D loss: 0.378826, acc.: 84.38%] [G loss: 2.204771]\n",
      "epoch:34 step:32653 [D loss: 0.405656, acc.: 86.72%] [G loss: 1.229705]\n",
      "epoch:34 step:32654 [D loss: 0.726899, acc.: 53.91%] [G loss: 1.180961]\n",
      "epoch:34 step:32655 [D loss: 0.448045, acc.: 82.81%] [G loss: 1.776875]\n",
      "epoch:34 step:32656 [D loss: 0.625458, acc.: 66.41%] [G loss: 1.622139]\n",
      "epoch:34 step:32657 [D loss: 0.426455, acc.: 82.03%] [G loss: 1.389127]\n",
      "epoch:34 step:32658 [D loss: 0.884071, acc.: 48.44%] [G loss: 1.065081]\n",
      "epoch:34 step:32659 [D loss: 0.555765, acc.: 71.09%] [G loss: 1.000929]\n",
      "epoch:34 step:32660 [D loss: 0.689177, acc.: 62.50%] [G loss: 1.271550]\n",
      "epoch:34 step:32661 [D loss: 0.583943, acc.: 68.75%] [G loss: 1.230654]\n",
      "epoch:34 step:32662 [D loss: 0.404923, acc.: 86.72%] [G loss: 1.933300]\n",
      "epoch:34 step:32663 [D loss: 0.544648, acc.: 71.88%] [G loss: 1.671194]\n",
      "epoch:34 step:32664 [D loss: 0.393468, acc.: 84.38%] [G loss: 1.858338]\n",
      "epoch:34 step:32665 [D loss: 0.576096, acc.: 71.09%] [G loss: 1.626853]\n",
      "epoch:34 step:32666 [D loss: 0.387741, acc.: 84.38%] [G loss: 1.525773]\n",
      "epoch:34 step:32667 [D loss: 0.565253, acc.: 67.97%] [G loss: 1.518424]\n",
      "epoch:34 step:32668 [D loss: 0.343398, acc.: 85.94%] [G loss: 1.712207]\n",
      "epoch:34 step:32669 [D loss: 0.477516, acc.: 77.34%] [G loss: 2.277815]\n",
      "epoch:34 step:32670 [D loss: 0.599349, acc.: 70.31%] [G loss: 1.537379]\n",
      "epoch:34 step:32671 [D loss: 0.348344, acc.: 90.62%] [G loss: 2.164314]\n",
      "epoch:34 step:32672 [D loss: 0.891284, acc.: 43.75%] [G loss: 1.445413]\n",
      "epoch:34 step:32673 [D loss: 0.502830, acc.: 76.56%] [G loss: 1.237516]\n",
      "epoch:34 step:32674 [D loss: 0.451350, acc.: 78.91%] [G loss: 1.181299]\n",
      "epoch:34 step:32675 [D loss: 0.529815, acc.: 75.00%] [G loss: 1.687054]\n",
      "epoch:34 step:32676 [D loss: 0.443183, acc.: 81.25%] [G loss: 1.383625]\n",
      "epoch:34 step:32677 [D loss: 0.466993, acc.: 82.03%] [G loss: 1.642617]\n",
      "epoch:34 step:32678 [D loss: 0.567561, acc.: 67.19%] [G loss: 1.771760]\n",
      "epoch:34 step:32679 [D loss: 0.550779, acc.: 71.88%] [G loss: 2.011634]\n",
      "epoch:34 step:32680 [D loss: 0.392667, acc.: 82.81%] [G loss: 1.244074]\n",
      "epoch:34 step:32681 [D loss: 0.382276, acc.: 84.38%] [G loss: 1.045759]\n",
      "epoch:34 step:32682 [D loss: 0.551029, acc.: 72.66%] [G loss: 1.209260]\n",
      "epoch:34 step:32683 [D loss: 0.542074, acc.: 75.78%] [G loss: 1.534051]\n",
      "epoch:34 step:32684 [D loss: 0.649651, acc.: 64.06%] [G loss: 1.107165]\n",
      "epoch:34 step:32685 [D loss: 0.544624, acc.: 71.09%] [G loss: 1.824793]\n",
      "epoch:34 step:32686 [D loss: 0.538185, acc.: 71.88%] [G loss: 0.965250]\n",
      "epoch:34 step:32687 [D loss: 0.491504, acc.: 75.78%] [G loss: 1.773861]\n",
      "epoch:34 step:32688 [D loss: 0.594239, acc.: 70.31%] [G loss: 1.327451]\n",
      "epoch:34 step:32689 [D loss: 0.543595, acc.: 68.75%] [G loss: 1.328820]\n",
      "epoch:34 step:32690 [D loss: 0.528023, acc.: 71.09%] [G loss: 1.444353]\n",
      "epoch:34 step:32691 [D loss: 0.441853, acc.: 76.56%] [G loss: 1.313287]\n",
      "epoch:34 step:32692 [D loss: 0.488361, acc.: 77.34%] [G loss: 1.536975]\n",
      "epoch:34 step:32693 [D loss: 0.519410, acc.: 76.56%] [G loss: 1.442391]\n",
      "epoch:34 step:32694 [D loss: 0.594185, acc.: 67.97%] [G loss: 1.839716]\n",
      "epoch:34 step:32695 [D loss: 0.726031, acc.: 54.69%] [G loss: 1.466586]\n",
      "epoch:34 step:32696 [D loss: 0.320811, acc.: 91.41%] [G loss: 1.502724]\n",
      "epoch:34 step:32697 [D loss: 0.530803, acc.: 69.53%] [G loss: 1.476529]\n",
      "epoch:34 step:32698 [D loss: 0.460883, acc.: 83.59%] [G loss: 1.813630]\n",
      "epoch:34 step:32699 [D loss: 0.461317, acc.: 80.47%] [G loss: 1.823255]\n",
      "epoch:34 step:32700 [D loss: 0.597095, acc.: 69.53%] [G loss: 1.624899]\n",
      "epoch:34 step:32701 [D loss: 0.559081, acc.: 72.66%] [G loss: 1.181165]\n",
      "epoch:34 step:32702 [D loss: 0.776997, acc.: 50.78%] [G loss: 1.357273]\n",
      "epoch:34 step:32703 [D loss: 0.467545, acc.: 79.69%] [G loss: 1.137215]\n",
      "epoch:34 step:32704 [D loss: 0.570165, acc.: 70.31%] [G loss: 1.634087]\n",
      "epoch:34 step:32705 [D loss: 0.319743, acc.: 92.19%] [G loss: 1.520539]\n",
      "epoch:34 step:32706 [D loss: 0.583998, acc.: 71.09%] [G loss: 1.220899]\n",
      "epoch:34 step:32707 [D loss: 0.421162, acc.: 82.03%] [G loss: 1.441515]\n",
      "epoch:34 step:32708 [D loss: 0.487314, acc.: 76.56%] [G loss: 1.455070]\n",
      "epoch:34 step:32709 [D loss: 0.686731, acc.: 57.03%] [G loss: 1.237755]\n",
      "epoch:34 step:32710 [D loss: 0.535874, acc.: 70.31%] [G loss: 1.978872]\n",
      "epoch:34 step:32711 [D loss: 0.587733, acc.: 69.53%] [G loss: 2.087715]\n",
      "epoch:34 step:32712 [D loss: 0.541166, acc.: 70.31%] [G loss: 2.315697]\n",
      "epoch:34 step:32713 [D loss: 0.508534, acc.: 75.78%] [G loss: 1.662928]\n",
      "epoch:34 step:32714 [D loss: 0.409723, acc.: 84.38%] [G loss: 1.750476]\n",
      "epoch:34 step:32715 [D loss: 0.638808, acc.: 62.50%] [G loss: 1.543831]\n",
      "epoch:34 step:32716 [D loss: 0.570394, acc.: 67.97%] [G loss: 1.654644]\n",
      "epoch:34 step:32717 [D loss: 0.512350, acc.: 78.12%] [G loss: 1.278514]\n",
      "epoch:34 step:32718 [D loss: 0.687316, acc.: 58.59%] [G loss: 1.504843]\n",
      "epoch:34 step:32719 [D loss: 0.427752, acc.: 83.59%] [G loss: 1.719811]\n",
      "epoch:34 step:32720 [D loss: 0.573266, acc.: 68.75%] [G loss: 1.844455]\n",
      "epoch:34 step:32721 [D loss: 0.422228, acc.: 86.72%] [G loss: 1.235219]\n",
      "epoch:34 step:32722 [D loss: 0.576170, acc.: 71.09%] [G loss: 1.518305]\n",
      "epoch:34 step:32723 [D loss: 0.579318, acc.: 71.88%] [G loss: 1.269177]\n",
      "epoch:34 step:32724 [D loss: 0.807214, acc.: 53.12%] [G loss: 0.923168]\n",
      "epoch:34 step:32725 [D loss: 0.604493, acc.: 68.75%] [G loss: 1.256962]\n",
      "epoch:34 step:32726 [D loss: 0.535176, acc.: 76.56%] [G loss: 1.249259]\n",
      "epoch:34 step:32727 [D loss: 0.343433, acc.: 90.62%] [G loss: 1.927659]\n",
      "epoch:34 step:32728 [D loss: 0.433741, acc.: 83.59%] [G loss: 1.589498]\n",
      "epoch:34 step:32729 [D loss: 0.679489, acc.: 65.62%] [G loss: 1.584316]\n",
      "epoch:34 step:32730 [D loss: 0.578866, acc.: 66.41%] [G loss: 2.388804]\n",
      "epoch:34 step:32731 [D loss: 0.335438, acc.: 92.19%] [G loss: 1.547455]\n",
      "epoch:34 step:32732 [D loss: 0.558316, acc.: 72.66%] [G loss: 1.526876]\n",
      "epoch:34 step:32733 [D loss: 0.567701, acc.: 74.22%] [G loss: 1.832464]\n",
      "epoch:34 step:32734 [D loss: 0.577899, acc.: 67.97%] [G loss: 1.361729]\n",
      "epoch:34 step:32735 [D loss: 0.658163, acc.: 60.16%] [G loss: 2.298551]\n",
      "epoch:34 step:32736 [D loss: 0.670523, acc.: 57.81%] [G loss: 1.274983]\n",
      "epoch:34 step:32737 [D loss: 0.843455, acc.: 50.00%] [G loss: 1.275391]\n",
      "epoch:34 step:32738 [D loss: 0.576242, acc.: 69.53%] [G loss: 1.539612]\n",
      "epoch:34 step:32739 [D loss: 0.445529, acc.: 78.91%] [G loss: 1.860243]\n",
      "epoch:34 step:32740 [D loss: 0.417199, acc.: 83.59%] [G loss: 1.426301]\n",
      "epoch:34 step:32741 [D loss: 0.564970, acc.: 66.41%] [G loss: 1.685745]\n",
      "epoch:34 step:32742 [D loss: 0.434787, acc.: 82.03%] [G loss: 1.522942]\n",
      "epoch:34 step:32743 [D loss: 0.579815, acc.: 67.19%] [G loss: 1.616510]\n",
      "epoch:34 step:32744 [D loss: 0.566817, acc.: 67.19%] [G loss: 1.480130]\n",
      "epoch:34 step:32745 [D loss: 0.463502, acc.: 76.56%] [G loss: 1.375286]\n",
      "epoch:34 step:32746 [D loss: 0.595473, acc.: 68.75%] [G loss: 1.931564]\n",
      "epoch:34 step:32747 [D loss: 0.702713, acc.: 64.06%] [G loss: 1.076451]\n",
      "epoch:34 step:32748 [D loss: 0.525065, acc.: 72.66%] [G loss: 1.503830]\n",
      "epoch:34 step:32749 [D loss: 0.564875, acc.: 70.31%] [G loss: 1.730295]\n",
      "epoch:34 step:32750 [D loss: 0.625419, acc.: 71.09%] [G loss: 0.988241]\n",
      "epoch:34 step:32751 [D loss: 0.483360, acc.: 85.16%] [G loss: 1.822645]\n",
      "epoch:34 step:32752 [D loss: 0.505097, acc.: 75.00%] [G loss: 1.917404]\n",
      "epoch:34 step:32753 [D loss: 0.486117, acc.: 76.56%] [G loss: 1.594762]\n",
      "epoch:34 step:32754 [D loss: 0.569434, acc.: 69.53%] [G loss: 1.697967]\n",
      "epoch:34 step:32755 [D loss: 0.424645, acc.: 82.81%] [G loss: 1.494003]\n",
      "epoch:34 step:32756 [D loss: 0.359191, acc.: 86.72%] [G loss: 1.708608]\n",
      "epoch:34 step:32757 [D loss: 0.412480, acc.: 85.16%] [G loss: 1.458868]\n",
      "epoch:34 step:32758 [D loss: 0.519602, acc.: 72.66%] [G loss: 1.250467]\n",
      "epoch:34 step:32759 [D loss: 0.414238, acc.: 85.94%] [G loss: 1.350287]\n",
      "epoch:34 step:32760 [D loss: 0.560157, acc.: 74.22%] [G loss: 1.215005]\n",
      "epoch:34 step:32761 [D loss: 0.386690, acc.: 83.59%] [G loss: 1.389647]\n",
      "epoch:34 step:32762 [D loss: 0.544493, acc.: 73.44%] [G loss: 1.752375]\n",
      "epoch:34 step:32763 [D loss: 0.527311, acc.: 71.09%] [G loss: 1.983392]\n",
      "epoch:34 step:32764 [D loss: 0.568789, acc.: 67.97%] [G loss: 1.148774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32765 [D loss: 0.399987, acc.: 82.03%] [G loss: 2.241280]\n",
      "epoch:34 step:32766 [D loss: 0.635912, acc.: 63.28%] [G loss: 1.386141]\n",
      "epoch:34 step:32767 [D loss: 0.560298, acc.: 75.00%] [G loss: 1.865773]\n",
      "epoch:34 step:32768 [D loss: 0.359651, acc.: 84.38%] [G loss: 1.423190]\n",
      "epoch:34 step:32769 [D loss: 0.445398, acc.: 79.69%] [G loss: 1.412924]\n",
      "epoch:34 step:32770 [D loss: 0.658738, acc.: 67.19%] [G loss: 1.650662]\n",
      "epoch:34 step:32771 [D loss: 0.590111, acc.: 67.97%] [G loss: 1.114964]\n",
      "epoch:34 step:32772 [D loss: 0.479588, acc.: 78.12%] [G loss: 1.331984]\n",
      "epoch:34 step:32773 [D loss: 0.775381, acc.: 57.81%] [G loss: 1.384241]\n",
      "epoch:34 step:32774 [D loss: 0.475250, acc.: 82.03%] [G loss: 1.831499]\n",
      "epoch:34 step:32775 [D loss: 0.639297, acc.: 64.84%] [G loss: 1.319407]\n",
      "epoch:34 step:32776 [D loss: 0.461387, acc.: 82.03%] [G loss: 1.751553]\n",
      "epoch:34 step:32777 [D loss: 0.569246, acc.: 71.88%] [G loss: 1.632709]\n",
      "epoch:34 step:32778 [D loss: 0.619719, acc.: 60.94%] [G loss: 1.831898]\n",
      "epoch:34 step:32779 [D loss: 0.595107, acc.: 67.19%] [G loss: 1.460362]\n",
      "epoch:34 step:32780 [D loss: 0.653739, acc.: 64.06%] [G loss: 1.621690]\n",
      "epoch:34 step:32781 [D loss: 0.363080, acc.: 86.72%] [G loss: 1.517602]\n",
      "epoch:34 step:32782 [D loss: 0.631909, acc.: 61.72%] [G loss: 1.585742]\n",
      "epoch:34 step:32783 [D loss: 0.715423, acc.: 57.81%] [G loss: 1.873439]\n",
      "epoch:34 step:32784 [D loss: 0.588330, acc.: 68.75%] [G loss: 1.675577]\n",
      "epoch:34 step:32785 [D loss: 0.564885, acc.: 71.09%] [G loss: 1.646686]\n",
      "epoch:34 step:32786 [D loss: 0.512884, acc.: 77.34%] [G loss: 1.179464]\n",
      "epoch:34 step:32787 [D loss: 0.429545, acc.: 79.69%] [G loss: 1.088416]\n",
      "epoch:34 step:32788 [D loss: 0.492505, acc.: 73.44%] [G loss: 1.996980]\n",
      "epoch:34 step:32789 [D loss: 0.464431, acc.: 78.91%] [G loss: 1.518288]\n",
      "epoch:34 step:32790 [D loss: 0.357037, acc.: 89.06%] [G loss: 1.697805]\n",
      "epoch:34 step:32791 [D loss: 0.627088, acc.: 67.19%] [G loss: 1.630404]\n",
      "epoch:34 step:32792 [D loss: 0.404992, acc.: 84.38%] [G loss: 1.531521]\n",
      "epoch:34 step:32793 [D loss: 0.294976, acc.: 91.41%] [G loss: 1.755452]\n",
      "epoch:34 step:32794 [D loss: 0.573821, acc.: 73.44%] [G loss: 1.731384]\n",
      "epoch:34 step:32795 [D loss: 0.620505, acc.: 68.75%] [G loss: 1.568650]\n",
      "epoch:35 step:32796 [D loss: 0.477200, acc.: 73.44%] [G loss: 1.725157]\n",
      "epoch:35 step:32797 [D loss: 0.495421, acc.: 75.00%] [G loss: 1.287947]\n",
      "epoch:35 step:32798 [D loss: 0.735701, acc.: 54.69%] [G loss: 1.300879]\n",
      "epoch:35 step:32799 [D loss: 0.537287, acc.: 69.53%] [G loss: 1.271161]\n",
      "epoch:35 step:32800 [D loss: 0.477883, acc.: 77.34%] [G loss: 1.247540]\n",
      "##############\n",
      "[2.70629931 1.92265748 1.8997889  2.9775066  1.04286497 6.19825016\n",
      " 2.38580888 2.50427305 4.00422273 7.14868929]\n",
      "##########\n",
      "epoch:35 step:32801 [D loss: 0.570687, acc.: 74.22%] [G loss: 1.098625]\n",
      "epoch:35 step:32802 [D loss: 0.779775, acc.: 50.78%] [G loss: 1.210507]\n",
      "epoch:35 step:32803 [D loss: 0.359491, acc.: 86.72%] [G loss: 1.712874]\n",
      "epoch:35 step:32804 [D loss: 0.373327, acc.: 88.28%] [G loss: 1.761026]\n",
      "epoch:35 step:32805 [D loss: 0.629781, acc.: 65.62%] [G loss: 1.764004]\n",
      "epoch:35 step:32806 [D loss: 0.432157, acc.: 78.91%] [G loss: 1.787555]\n",
      "epoch:35 step:32807 [D loss: 0.518972, acc.: 76.56%] [G loss: 1.417763]\n",
      "epoch:35 step:32808 [D loss: 0.768500, acc.: 56.25%] [G loss: 1.530929]\n",
      "epoch:35 step:32809 [D loss: 0.429118, acc.: 82.81%] [G loss: 1.512416]\n",
      "epoch:35 step:32810 [D loss: 0.402515, acc.: 85.16%] [G loss: 1.487126]\n",
      "epoch:35 step:32811 [D loss: 0.401769, acc.: 82.81%] [G loss: 1.308635]\n",
      "epoch:35 step:32812 [D loss: 0.424017, acc.: 82.81%] [G loss: 1.931602]\n",
      "epoch:35 step:32813 [D loss: 0.394846, acc.: 84.38%] [G loss: 1.709615]\n",
      "epoch:35 step:32814 [D loss: 0.587250, acc.: 71.88%] [G loss: 1.293900]\n",
      "epoch:35 step:32815 [D loss: 0.454199, acc.: 76.56%] [G loss: 1.849960]\n",
      "epoch:35 step:32816 [D loss: 0.593851, acc.: 66.41%] [G loss: 1.718673]\n",
      "epoch:35 step:32817 [D loss: 0.538813, acc.: 72.66%] [G loss: 1.616524]\n",
      "epoch:35 step:32818 [D loss: 0.347924, acc.: 91.41%] [G loss: 1.680723]\n",
      "epoch:35 step:32819 [D loss: 0.473375, acc.: 78.12%] [G loss: 1.769418]\n",
      "epoch:35 step:32820 [D loss: 0.505494, acc.: 78.91%] [G loss: 1.810477]\n",
      "epoch:35 step:32821 [D loss: 0.600145, acc.: 62.50%] [G loss: 1.075629]\n",
      "epoch:35 step:32822 [D loss: 0.418778, acc.: 84.38%] [G loss: 1.672924]\n",
      "epoch:35 step:32823 [D loss: 0.568543, acc.: 72.66%] [G loss: 2.091122]\n",
      "epoch:35 step:32824 [D loss: 0.352921, acc.: 89.06%] [G loss: 1.635369]\n",
      "epoch:35 step:32825 [D loss: 0.613743, acc.: 66.41%] [G loss: 1.568040]\n",
      "epoch:35 step:32826 [D loss: 0.486149, acc.: 78.12%] [G loss: 1.533095]\n",
      "epoch:35 step:32827 [D loss: 0.574257, acc.: 67.19%] [G loss: 1.315355]\n",
      "epoch:35 step:32828 [D loss: 0.363565, acc.: 86.72%] [G loss: 1.709836]\n",
      "epoch:35 step:32829 [D loss: 0.670374, acc.: 59.38%] [G loss: 1.260860]\n",
      "epoch:35 step:32830 [D loss: 0.349893, acc.: 86.72%] [G loss: 1.913289]\n",
      "epoch:35 step:32831 [D loss: 0.602776, acc.: 64.84%] [G loss: 1.651116]\n",
      "epoch:35 step:32832 [D loss: 0.395933, acc.: 85.16%] [G loss: 1.603860]\n",
      "epoch:35 step:32833 [D loss: 0.462926, acc.: 81.25%] [G loss: 1.645381]\n",
      "epoch:35 step:32834 [D loss: 0.502111, acc.: 72.66%] [G loss: 1.609881]\n",
      "epoch:35 step:32835 [D loss: 0.714687, acc.: 60.94%] [G loss: 1.219558]\n",
      "epoch:35 step:32836 [D loss: 0.354503, acc.: 86.72%] [G loss: 1.478887]\n",
      "epoch:35 step:32837 [D loss: 0.585613, acc.: 72.66%] [G loss: 1.321453]\n",
      "epoch:35 step:32838 [D loss: 0.517468, acc.: 78.91%] [G loss: 0.945647]\n",
      "epoch:35 step:32839 [D loss: 0.510318, acc.: 70.31%] [G loss: 1.643357]\n",
      "epoch:35 step:32840 [D loss: 0.373735, acc.: 86.72%] [G loss: 1.945237]\n",
      "epoch:35 step:32841 [D loss: 0.524466, acc.: 73.44%] [G loss: 1.257472]\n",
      "epoch:35 step:32842 [D loss: 0.473417, acc.: 81.25%] [G loss: 1.241504]\n",
      "epoch:35 step:32843 [D loss: 0.667865, acc.: 67.19%] [G loss: 1.372828]\n",
      "epoch:35 step:32844 [D loss: 0.524084, acc.: 76.56%] [G loss: 1.556362]\n",
      "epoch:35 step:32845 [D loss: 0.382565, acc.: 88.28%] [G loss: 1.780380]\n",
      "epoch:35 step:32846 [D loss: 0.414322, acc.: 84.38%] [G loss: 1.528948]\n",
      "epoch:35 step:32847 [D loss: 0.511754, acc.: 72.66%] [G loss: 1.782723]\n",
      "epoch:35 step:32848 [D loss: 0.648453, acc.: 62.50%] [G loss: 1.207226]\n",
      "epoch:35 step:32849 [D loss: 0.351497, acc.: 88.28%] [G loss: 1.504996]\n",
      "epoch:35 step:32850 [D loss: 0.405743, acc.: 82.81%] [G loss: 1.527440]\n",
      "epoch:35 step:32851 [D loss: 0.539895, acc.: 74.22%] [G loss: 1.540142]\n",
      "epoch:35 step:32852 [D loss: 0.673980, acc.: 58.59%] [G loss: 1.077680]\n",
      "epoch:35 step:32853 [D loss: 0.471702, acc.: 75.78%] [G loss: 1.081532]\n",
      "epoch:35 step:32854 [D loss: 0.432693, acc.: 85.94%] [G loss: 1.774850]\n",
      "epoch:35 step:32855 [D loss: 0.366235, acc.: 87.50%] [G loss: 1.634142]\n",
      "epoch:35 step:32856 [D loss: 0.541642, acc.: 78.12%] [G loss: 1.137133]\n",
      "epoch:35 step:32857 [D loss: 0.673548, acc.: 60.94%] [G loss: 1.683985]\n",
      "epoch:35 step:32858 [D loss: 0.647533, acc.: 66.41%] [G loss: 1.058619]\n",
      "epoch:35 step:32859 [D loss: 0.593482, acc.: 71.09%] [G loss: 1.503158]\n",
      "epoch:35 step:32860 [D loss: 0.374354, acc.: 88.28%] [G loss: 1.560597]\n",
      "epoch:35 step:32861 [D loss: 0.586617, acc.: 71.09%] [G loss: 1.908099]\n",
      "epoch:35 step:32862 [D loss: 0.447415, acc.: 76.56%] [G loss: 1.431700]\n",
      "epoch:35 step:32863 [D loss: 0.412356, acc.: 85.16%] [G loss: 1.261477]\n",
      "epoch:35 step:32864 [D loss: 0.498145, acc.: 75.00%] [G loss: 1.613862]\n",
      "epoch:35 step:32865 [D loss: 0.576623, acc.: 68.75%] [G loss: 1.323059]\n",
      "epoch:35 step:32866 [D loss: 0.543172, acc.: 71.88%] [G loss: 1.333073]\n",
      "epoch:35 step:32867 [D loss: 0.588374, acc.: 71.88%] [G loss: 1.351914]\n",
      "epoch:35 step:32868 [D loss: 0.537831, acc.: 72.66%] [G loss: 1.519682]\n",
      "epoch:35 step:32869 [D loss: 0.317799, acc.: 92.19%] [G loss: 1.634354]\n",
      "epoch:35 step:32870 [D loss: 0.473334, acc.: 75.78%] [G loss: 1.208880]\n",
      "epoch:35 step:32871 [D loss: 0.572418, acc.: 70.31%] [G loss: 1.537911]\n",
      "epoch:35 step:32872 [D loss: 0.554754, acc.: 70.31%] [G loss: 1.262336]\n",
      "epoch:35 step:32873 [D loss: 0.662161, acc.: 62.50%] [G loss: 1.530079]\n",
      "epoch:35 step:32874 [D loss: 0.761626, acc.: 59.38%] [G loss: 0.938509]\n",
      "epoch:35 step:32875 [D loss: 0.471842, acc.: 74.22%] [G loss: 1.391349]\n",
      "epoch:35 step:32876 [D loss: 0.497639, acc.: 75.00%] [G loss: 1.871768]\n",
      "epoch:35 step:32877 [D loss: 0.477101, acc.: 75.00%] [G loss: 1.350935]\n",
      "epoch:35 step:32878 [D loss: 0.357166, acc.: 88.28%] [G loss: 1.512660]\n",
      "epoch:35 step:32879 [D loss: 0.613094, acc.: 63.28%] [G loss: 1.820896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:32880 [D loss: 0.497623, acc.: 78.91%] [G loss: 1.653288]\n",
      "epoch:35 step:32881 [D loss: 0.530755, acc.: 73.44%] [G loss: 1.424194]\n",
      "epoch:35 step:32882 [D loss: 0.500478, acc.: 74.22%] [G loss: 1.738801]\n",
      "epoch:35 step:32883 [D loss: 0.549493, acc.: 72.66%] [G loss: 1.206601]\n",
      "epoch:35 step:32884 [D loss: 0.447433, acc.: 82.81%] [G loss: 1.528733]\n",
      "epoch:35 step:32885 [D loss: 0.803645, acc.: 57.81%] [G loss: 1.124727]\n",
      "epoch:35 step:32886 [D loss: 0.483219, acc.: 78.12%] [G loss: 2.114953]\n",
      "epoch:35 step:32887 [D loss: 0.584791, acc.: 68.75%] [G loss: 1.770556]\n",
      "epoch:35 step:32888 [D loss: 0.626836, acc.: 65.62%] [G loss: 1.794098]\n",
      "epoch:35 step:32889 [D loss: 0.367444, acc.: 89.06%] [G loss: 1.805983]\n",
      "epoch:35 step:32890 [D loss: 0.659055, acc.: 60.94%] [G loss: 1.536230]\n",
      "epoch:35 step:32891 [D loss: 0.625711, acc.: 63.28%] [G loss: 1.608528]\n",
      "epoch:35 step:32892 [D loss: 0.572170, acc.: 73.44%] [G loss: 1.261501]\n",
      "epoch:35 step:32893 [D loss: 0.508051, acc.: 75.00%] [G loss: 1.308937]\n",
      "epoch:35 step:32894 [D loss: 0.585644, acc.: 67.19%] [G loss: 1.285592]\n",
      "epoch:35 step:32895 [D loss: 0.482094, acc.: 78.12%] [G loss: 1.470013]\n",
      "epoch:35 step:32896 [D loss: 0.596369, acc.: 68.75%] [G loss: 1.133492]\n",
      "epoch:35 step:32897 [D loss: 0.341221, acc.: 87.50%] [G loss: 1.564755]\n",
      "epoch:35 step:32898 [D loss: 0.642749, acc.: 59.38%] [G loss: 0.888302]\n",
      "epoch:35 step:32899 [D loss: 0.496932, acc.: 74.22%] [G loss: 1.909958]\n",
      "epoch:35 step:32900 [D loss: 0.480544, acc.: 79.69%] [G loss: 1.585056]\n",
      "epoch:35 step:32901 [D loss: 0.614601, acc.: 67.97%] [G loss: 1.434972]\n",
      "epoch:35 step:32902 [D loss: 0.638792, acc.: 61.72%] [G loss: 1.360262]\n",
      "epoch:35 step:32903 [D loss: 0.508471, acc.: 74.22%] [G loss: 1.530639]\n",
      "epoch:35 step:32904 [D loss: 0.578128, acc.: 73.44%] [G loss: 2.032526]\n",
      "epoch:35 step:32905 [D loss: 0.576439, acc.: 69.53%] [G loss: 1.327223]\n",
      "epoch:35 step:32906 [D loss: 0.805188, acc.: 45.31%] [G loss: 1.536573]\n",
      "epoch:35 step:32907 [D loss: 0.568100, acc.: 71.09%] [G loss: 1.570022]\n",
      "epoch:35 step:32908 [D loss: 0.418234, acc.: 85.16%] [G loss: 1.585892]\n",
      "epoch:35 step:32909 [D loss: 0.551152, acc.: 70.31%] [G loss: 1.740454]\n",
      "epoch:35 step:32910 [D loss: 0.499907, acc.: 77.34%] [G loss: 1.272649]\n",
      "epoch:35 step:32911 [D loss: 0.608250, acc.: 69.53%] [G loss: 2.536837]\n",
      "epoch:35 step:32912 [D loss: 0.567014, acc.: 67.97%] [G loss: 1.156884]\n",
      "epoch:35 step:32913 [D loss: 0.368071, acc.: 89.06%] [G loss: 1.319266]\n",
      "epoch:35 step:32914 [D loss: 0.405823, acc.: 83.59%] [G loss: 1.739801]\n",
      "epoch:35 step:32915 [D loss: 0.615037, acc.: 68.75%] [G loss: 1.208608]\n",
      "epoch:35 step:32916 [D loss: 0.731824, acc.: 62.50%] [G loss: 1.071871]\n",
      "epoch:35 step:32917 [D loss: 0.508328, acc.: 75.78%] [G loss: 1.564943]\n",
      "epoch:35 step:32918 [D loss: 0.351218, acc.: 88.28%] [G loss: 1.747072]\n",
      "epoch:35 step:32919 [D loss: 0.551299, acc.: 71.09%] [G loss: 1.826453]\n",
      "epoch:35 step:32920 [D loss: 0.478632, acc.: 75.78%] [G loss: 1.390804]\n",
      "epoch:35 step:32921 [D loss: 0.594373, acc.: 71.88%] [G loss: 0.875696]\n",
      "epoch:35 step:32922 [D loss: 0.453551, acc.: 78.91%] [G loss: 1.812393]\n",
      "epoch:35 step:32923 [D loss: 0.416099, acc.: 82.81%] [G loss: 1.983540]\n",
      "epoch:35 step:32924 [D loss: 0.497271, acc.: 76.56%] [G loss: 1.669317]\n",
      "epoch:35 step:32925 [D loss: 0.435494, acc.: 79.69%] [G loss: 1.267196]\n",
      "epoch:35 step:32926 [D loss: 0.572161, acc.: 69.53%] [G loss: 1.263016]\n",
      "epoch:35 step:32927 [D loss: 0.318434, acc.: 92.97%] [G loss: 1.721528]\n",
      "epoch:35 step:32928 [D loss: 0.568272, acc.: 73.44%] [G loss: 1.604261]\n",
      "epoch:35 step:32929 [D loss: 0.644905, acc.: 59.38%] [G loss: 1.309848]\n",
      "epoch:35 step:32930 [D loss: 0.576723, acc.: 71.88%] [G loss: 1.853108]\n",
      "epoch:35 step:32931 [D loss: 0.638083, acc.: 60.94%] [G loss: 1.913367]\n",
      "epoch:35 step:32932 [D loss: 0.420079, acc.: 84.38%] [G loss: 1.447525]\n",
      "epoch:35 step:32933 [D loss: 0.727661, acc.: 58.59%] [G loss: 1.347266]\n",
      "epoch:35 step:32934 [D loss: 0.539798, acc.: 72.66%] [G loss: 1.311752]\n",
      "epoch:35 step:32935 [D loss: 0.582938, acc.: 65.62%] [G loss: 1.706326]\n",
      "epoch:35 step:32936 [D loss: 0.607337, acc.: 63.28%] [G loss: 1.201107]\n",
      "epoch:35 step:32937 [D loss: 0.432781, acc.: 80.47%] [G loss: 1.044414]\n",
      "epoch:35 step:32938 [D loss: 0.417801, acc.: 80.47%] [G loss: 1.489975]\n",
      "epoch:35 step:32939 [D loss: 0.797768, acc.: 52.34%] [G loss: 0.819637]\n",
      "epoch:35 step:32940 [D loss: 0.450036, acc.: 79.69%] [G loss: 1.985602]\n",
      "epoch:35 step:32941 [D loss: 0.500143, acc.: 75.00%] [G loss: 1.252450]\n",
      "epoch:35 step:32942 [D loss: 0.636167, acc.: 68.75%] [G loss: 1.953357]\n",
      "epoch:35 step:32943 [D loss: 0.384792, acc.: 84.38%] [G loss: 1.605450]\n",
      "epoch:35 step:32944 [D loss: 0.814796, acc.: 45.31%] [G loss: 0.993937]\n",
      "epoch:35 step:32945 [D loss: 0.532632, acc.: 71.88%] [G loss: 1.471990]\n",
      "epoch:35 step:32946 [D loss: 0.481901, acc.: 79.69%] [G loss: 1.677269]\n",
      "epoch:35 step:32947 [D loss: 0.497988, acc.: 77.34%] [G loss: 1.587907]\n",
      "epoch:35 step:32948 [D loss: 0.415166, acc.: 81.25%] [G loss: 1.300619]\n",
      "epoch:35 step:32949 [D loss: 0.660088, acc.: 65.62%] [G loss: 1.399681]\n",
      "epoch:35 step:32950 [D loss: 0.609694, acc.: 61.72%] [G loss: 1.428433]\n",
      "epoch:35 step:32951 [D loss: 0.571406, acc.: 70.31%] [G loss: 1.717427]\n",
      "epoch:35 step:32952 [D loss: 0.433541, acc.: 78.12%] [G loss: 1.748831]\n",
      "epoch:35 step:32953 [D loss: 0.658337, acc.: 62.50%] [G loss: 1.280318]\n",
      "epoch:35 step:32954 [D loss: 0.578241, acc.: 72.66%] [G loss: 1.316468]\n",
      "epoch:35 step:32955 [D loss: 0.640091, acc.: 60.94%] [G loss: 1.537530]\n",
      "epoch:35 step:32956 [D loss: 0.483568, acc.: 75.00%] [G loss: 1.643351]\n",
      "epoch:35 step:32957 [D loss: 0.352552, acc.: 85.94%] [G loss: 1.465356]\n",
      "epoch:35 step:32958 [D loss: 0.573202, acc.: 75.00%] [G loss: 1.314935]\n",
      "epoch:35 step:32959 [D loss: 0.269487, acc.: 92.19%] [G loss: 1.865000]\n",
      "epoch:35 step:32960 [D loss: 0.520199, acc.: 76.56%] [G loss: 1.449612]\n",
      "epoch:35 step:32961 [D loss: 0.578202, acc.: 68.75%] [G loss: 1.414488]\n",
      "epoch:35 step:32962 [D loss: 0.561274, acc.: 67.19%] [G loss: 1.692692]\n",
      "epoch:35 step:32963 [D loss: 0.576101, acc.: 69.53%] [G loss: 1.789885]\n",
      "epoch:35 step:32964 [D loss: 0.642857, acc.: 65.62%] [G loss: 1.425593]\n",
      "epoch:35 step:32965 [D loss: 0.377494, acc.: 85.94%] [G loss: 1.793652]\n",
      "epoch:35 step:32966 [D loss: 0.526662, acc.: 71.88%] [G loss: 2.381427]\n",
      "epoch:35 step:32967 [D loss: 0.539453, acc.: 68.75%] [G loss: 1.758670]\n",
      "epoch:35 step:32968 [D loss: 0.613297, acc.: 64.84%] [G loss: 1.727388]\n",
      "epoch:35 step:32969 [D loss: 0.460731, acc.: 75.00%] [G loss: 1.379202]\n",
      "epoch:35 step:32970 [D loss: 0.431886, acc.: 82.03%] [G loss: 1.599046]\n",
      "epoch:35 step:32971 [D loss: 0.367431, acc.: 88.28%] [G loss: 1.376653]\n",
      "epoch:35 step:32972 [D loss: 0.597629, acc.: 70.31%] [G loss: 1.795549]\n",
      "epoch:35 step:32973 [D loss: 0.385435, acc.: 85.16%] [G loss: 1.976291]\n",
      "epoch:35 step:32974 [D loss: 0.437757, acc.: 82.03%] [G loss: 1.477218]\n",
      "epoch:35 step:32975 [D loss: 0.574819, acc.: 75.00%] [G loss: 1.358274]\n",
      "epoch:35 step:32976 [D loss: 0.479744, acc.: 79.69%] [G loss: 1.661776]\n",
      "epoch:35 step:32977 [D loss: 0.603288, acc.: 69.53%] [G loss: 2.081665]\n",
      "epoch:35 step:32978 [D loss: 0.458412, acc.: 79.69%] [G loss: 1.612513]\n",
      "epoch:35 step:32979 [D loss: 0.750400, acc.: 53.91%] [G loss: 1.120127]\n",
      "epoch:35 step:32980 [D loss: 0.509178, acc.: 75.78%] [G loss: 1.595889]\n",
      "epoch:35 step:32981 [D loss: 0.583714, acc.: 73.44%] [G loss: 1.348910]\n",
      "epoch:35 step:32982 [D loss: 0.299655, acc.: 91.41%] [G loss: 1.700649]\n",
      "epoch:35 step:32983 [D loss: 0.496901, acc.: 77.34%] [G loss: 1.298965]\n",
      "epoch:35 step:32984 [D loss: 0.527265, acc.: 74.22%] [G loss: 1.426947]\n",
      "epoch:35 step:32985 [D loss: 0.599030, acc.: 71.88%] [G loss: 1.324972]\n",
      "epoch:35 step:32986 [D loss: 0.585057, acc.: 68.75%] [G loss: 1.740595]\n",
      "epoch:35 step:32987 [D loss: 0.493097, acc.: 79.69%] [G loss: 1.075975]\n",
      "epoch:35 step:32988 [D loss: 0.452942, acc.: 79.69%] [G loss: 1.373024]\n",
      "epoch:35 step:32989 [D loss: 0.750885, acc.: 60.16%] [G loss: 0.796586]\n",
      "epoch:35 step:32990 [D loss: 0.475987, acc.: 75.78%] [G loss: 1.404012]\n",
      "epoch:35 step:32991 [D loss: 0.666176, acc.: 57.81%] [G loss: 1.553947]\n",
      "epoch:35 step:32992 [D loss: 0.550101, acc.: 71.09%] [G loss: 1.427917]\n",
      "epoch:35 step:32993 [D loss: 0.480269, acc.: 71.09%] [G loss: 1.618701]\n",
      "epoch:35 step:32994 [D loss: 0.416118, acc.: 82.03%] [G loss: 1.360471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:32995 [D loss: 0.753782, acc.: 52.34%] [G loss: 1.353907]\n",
      "epoch:35 step:32996 [D loss: 0.562565, acc.: 70.31%] [G loss: 1.651627]\n",
      "epoch:35 step:32997 [D loss: 0.599463, acc.: 66.41%] [G loss: 1.178648]\n",
      "epoch:35 step:32998 [D loss: 0.394620, acc.: 85.94%] [G loss: 1.722058]\n",
      "epoch:35 step:32999 [D loss: 0.616576, acc.: 69.53%] [G loss: 1.464337]\n",
      "epoch:35 step:33000 [D loss: 0.572146, acc.: 72.66%] [G loss: 1.699303]\n",
      "##############\n",
      "[2.70511188 2.01086841 2.00036388 2.86977539 0.76700752 6.15815916\n",
      " 2.1682339  2.22924614 3.87418821 7.14868929]\n",
      "##########\n",
      "epoch:35 step:33001 [D loss: 0.320864, acc.: 88.28%] [G loss: 1.480643]\n",
      "epoch:35 step:33002 [D loss: 0.819756, acc.: 48.44%] [G loss: 1.150561]\n",
      "epoch:35 step:33003 [D loss: 0.447341, acc.: 76.56%] [G loss: 2.114818]\n",
      "epoch:35 step:33004 [D loss: 0.592131, acc.: 68.75%] [G loss: 1.223925]\n",
      "epoch:35 step:33005 [D loss: 0.439629, acc.: 80.47%] [G loss: 1.919909]\n",
      "epoch:35 step:33006 [D loss: 0.555714, acc.: 69.53%] [G loss: 1.887662]\n",
      "epoch:35 step:33007 [D loss: 0.448482, acc.: 80.47%] [G loss: 1.626414]\n",
      "epoch:35 step:33008 [D loss: 0.646811, acc.: 60.94%] [G loss: 1.629971]\n",
      "epoch:35 step:33009 [D loss: 0.586504, acc.: 70.31%] [G loss: 1.691011]\n",
      "epoch:35 step:33010 [D loss: 0.499145, acc.: 78.12%] [G loss: 2.021430]\n",
      "epoch:35 step:33011 [D loss: 0.563590, acc.: 71.09%] [G loss: 1.444976]\n",
      "epoch:35 step:33012 [D loss: 0.431359, acc.: 82.81%] [G loss: 1.546886]\n",
      "epoch:35 step:33013 [D loss: 0.456583, acc.: 79.69%] [G loss: 1.804663]\n",
      "epoch:35 step:33014 [D loss: 0.598311, acc.: 71.09%] [G loss: 1.436601]\n",
      "epoch:35 step:33015 [D loss: 0.470416, acc.: 79.69%] [G loss: 1.613210]\n",
      "epoch:35 step:33016 [D loss: 0.627313, acc.: 62.50%] [G loss: 1.752387]\n",
      "epoch:35 step:33017 [D loss: 0.727091, acc.: 59.38%] [G loss: 1.634465]\n",
      "epoch:35 step:33018 [D loss: 0.635812, acc.: 67.19%] [G loss: 1.401382]\n",
      "epoch:35 step:33019 [D loss: 0.578048, acc.: 66.41%] [G loss: 1.311774]\n",
      "epoch:35 step:33020 [D loss: 0.591084, acc.: 70.31%] [G loss: 1.323084]\n",
      "epoch:35 step:33021 [D loss: 0.525595, acc.: 75.00%] [G loss: 1.428974]\n",
      "epoch:35 step:33022 [D loss: 0.519425, acc.: 72.66%] [G loss: 1.440693]\n",
      "epoch:35 step:33023 [D loss: 0.398409, acc.: 82.81%] [G loss: 1.804916]\n",
      "epoch:35 step:33024 [D loss: 0.528866, acc.: 74.22%] [G loss: 1.981693]\n",
      "epoch:35 step:33025 [D loss: 0.549888, acc.: 74.22%] [G loss: 1.356275]\n",
      "epoch:35 step:33026 [D loss: 0.344187, acc.: 89.06%] [G loss: 1.590441]\n",
      "epoch:35 step:33027 [D loss: 0.642336, acc.: 67.19%] [G loss: 1.351249]\n",
      "epoch:35 step:33028 [D loss: 0.660527, acc.: 66.41%] [G loss: 2.310941]\n",
      "epoch:35 step:33029 [D loss: 0.425717, acc.: 82.81%] [G loss: 1.838305]\n",
      "epoch:35 step:33030 [D loss: 0.508062, acc.: 78.91%] [G loss: 1.557171]\n",
      "epoch:35 step:33031 [D loss: 0.453920, acc.: 80.47%] [G loss: 1.470840]\n",
      "epoch:35 step:33032 [D loss: 0.721575, acc.: 61.72%] [G loss: 1.581697]\n",
      "epoch:35 step:33033 [D loss: 0.609592, acc.: 70.31%] [G loss: 1.584483]\n",
      "epoch:35 step:33034 [D loss: 0.532416, acc.: 73.44%] [G loss: 1.694494]\n",
      "epoch:35 step:33035 [D loss: 0.475299, acc.: 80.47%] [G loss: 1.355799]\n",
      "epoch:35 step:33036 [D loss: 0.546992, acc.: 74.22%] [G loss: 1.327249]\n",
      "epoch:35 step:33037 [D loss: 0.538971, acc.: 71.09%] [G loss: 1.541010]\n",
      "epoch:35 step:33038 [D loss: 0.595831, acc.: 67.19%] [G loss: 1.406861]\n",
      "epoch:35 step:33039 [D loss: 0.426654, acc.: 81.25%] [G loss: 1.281667]\n",
      "epoch:35 step:33040 [D loss: 0.721889, acc.: 57.81%] [G loss: 1.271271]\n",
      "epoch:35 step:33041 [D loss: 0.370351, acc.: 84.38%] [G loss: 1.858223]\n",
      "epoch:35 step:33042 [D loss: 0.552465, acc.: 71.09%] [G loss: 1.504650]\n",
      "epoch:35 step:33043 [D loss: 0.540926, acc.: 72.66%] [G loss: 0.946003]\n",
      "epoch:35 step:33044 [D loss: 0.372971, acc.: 85.16%] [G loss: 1.922586]\n",
      "epoch:35 step:33045 [D loss: 0.527327, acc.: 73.44%] [G loss: 1.503369]\n",
      "epoch:35 step:33046 [D loss: 0.523161, acc.: 69.53%] [G loss: 1.516824]\n",
      "epoch:35 step:33047 [D loss: 0.421927, acc.: 82.81%] [G loss: 1.903028]\n",
      "epoch:35 step:33048 [D loss: 0.521730, acc.: 71.88%] [G loss: 1.600592]\n",
      "epoch:35 step:33049 [D loss: 0.441067, acc.: 77.34%] [G loss: 1.365396]\n",
      "epoch:35 step:33050 [D loss: 0.480248, acc.: 78.91%] [G loss: 1.555924]\n",
      "epoch:35 step:33051 [D loss: 0.508040, acc.: 75.00%] [G loss: 1.547727]\n",
      "epoch:35 step:33052 [D loss: 0.460473, acc.: 82.81%] [G loss: 1.365053]\n",
      "epoch:35 step:33053 [D loss: 0.539956, acc.: 68.75%] [G loss: 1.308321]\n",
      "epoch:35 step:33054 [D loss: 0.393763, acc.: 89.06%] [G loss: 1.776664]\n",
      "epoch:35 step:33055 [D loss: 0.561224, acc.: 70.31%] [G loss: 1.461892]\n",
      "epoch:35 step:33056 [D loss: 0.638844, acc.: 61.72%] [G loss: 1.482388]\n",
      "epoch:35 step:33057 [D loss: 0.704852, acc.: 55.47%] [G loss: 1.403559]\n",
      "epoch:35 step:33058 [D loss: 0.659105, acc.: 65.62%] [G loss: 1.203606]\n",
      "epoch:35 step:33059 [D loss: 0.537530, acc.: 71.88%] [G loss: 1.323146]\n",
      "epoch:35 step:33060 [D loss: 0.516255, acc.: 71.88%] [G loss: 1.399783]\n",
      "epoch:35 step:33061 [D loss: 0.608333, acc.: 66.41%] [G loss: 1.321190]\n",
      "epoch:35 step:33062 [D loss: 0.554013, acc.: 72.66%] [G loss: 1.545677]\n",
      "epoch:35 step:33063 [D loss: 0.635966, acc.: 64.06%] [G loss: 1.708685]\n",
      "epoch:35 step:33064 [D loss: 0.457743, acc.: 76.56%] [G loss: 2.153196]\n",
      "epoch:35 step:33065 [D loss: 0.575756, acc.: 71.88%] [G loss: 2.269183]\n",
      "epoch:35 step:33066 [D loss: 0.379632, acc.: 84.38%] [G loss: 1.203663]\n",
      "epoch:35 step:33067 [D loss: 0.439083, acc.: 80.47%] [G loss: 1.757949]\n",
      "epoch:35 step:33068 [D loss: 0.718647, acc.: 57.81%] [G loss: 1.034519]\n",
      "epoch:35 step:33069 [D loss: 0.489959, acc.: 77.34%] [G loss: 1.344790]\n",
      "epoch:35 step:33070 [D loss: 0.926469, acc.: 43.75%] [G loss: 1.263730]\n",
      "epoch:35 step:33071 [D loss: 0.302651, acc.: 89.06%] [G loss: 1.906342]\n",
      "epoch:35 step:33072 [D loss: 0.551143, acc.: 74.22%] [G loss: 1.562724]\n",
      "epoch:35 step:33073 [D loss: 0.452573, acc.: 77.34%] [G loss: 1.733384]\n",
      "epoch:35 step:33074 [D loss: 0.747778, acc.: 60.94%] [G loss: 1.219209]\n",
      "epoch:35 step:33075 [D loss: 0.641972, acc.: 68.75%] [G loss: 1.608172]\n",
      "epoch:35 step:33076 [D loss: 0.448102, acc.: 80.47%] [G loss: 1.460839]\n",
      "epoch:35 step:33077 [D loss: 0.495743, acc.: 77.34%] [G loss: 1.312582]\n",
      "epoch:35 step:33078 [D loss: 0.517102, acc.: 75.78%] [G loss: 1.040969]\n",
      "epoch:35 step:33079 [D loss: 0.706812, acc.: 64.84%] [G loss: 1.373373]\n",
      "epoch:35 step:33080 [D loss: 0.464918, acc.: 76.56%] [G loss: 1.454300]\n",
      "epoch:35 step:33081 [D loss: 0.526445, acc.: 76.56%] [G loss: 1.735380]\n",
      "epoch:35 step:33082 [D loss: 0.391014, acc.: 83.59%] [G loss: 1.970367]\n",
      "epoch:35 step:33083 [D loss: 0.522413, acc.: 70.31%] [G loss: 1.617764]\n",
      "epoch:35 step:33084 [D loss: 0.540514, acc.: 70.31%] [G loss: 1.252217]\n",
      "epoch:35 step:33085 [D loss: 0.343036, acc.: 87.50%] [G loss: 1.834200]\n",
      "epoch:35 step:33086 [D loss: 0.512985, acc.: 71.09%] [G loss: 1.499521]\n",
      "epoch:35 step:33087 [D loss: 0.473119, acc.: 78.12%] [G loss: 1.524691]\n",
      "epoch:35 step:33088 [D loss: 0.511827, acc.: 75.00%] [G loss: 1.421252]\n",
      "epoch:35 step:33089 [D loss: 0.561691, acc.: 71.09%] [G loss: 1.324469]\n",
      "epoch:35 step:33090 [D loss: 0.489193, acc.: 71.88%] [G loss: 2.025207]\n",
      "epoch:35 step:33091 [D loss: 0.414542, acc.: 81.25%] [G loss: 1.664483]\n",
      "epoch:35 step:33092 [D loss: 0.466203, acc.: 79.69%] [G loss: 1.990676]\n",
      "epoch:35 step:33093 [D loss: 0.519533, acc.: 75.78%] [G loss: 1.393369]\n",
      "epoch:35 step:33094 [D loss: 0.490139, acc.: 78.12%] [G loss: 1.890187]\n",
      "epoch:35 step:33095 [D loss: 0.352686, acc.: 85.94%] [G loss: 1.525786]\n",
      "epoch:35 step:33096 [D loss: 0.573197, acc.: 71.09%] [G loss: 1.116879]\n",
      "epoch:35 step:33097 [D loss: 0.538764, acc.: 75.78%] [G loss: 1.407019]\n",
      "epoch:35 step:33098 [D loss: 0.475361, acc.: 76.56%] [G loss: 1.484544]\n",
      "epoch:35 step:33099 [D loss: 0.601463, acc.: 67.97%] [G loss: 1.602327]\n",
      "epoch:35 step:33100 [D loss: 0.368846, acc.: 84.38%] [G loss: 1.879781]\n",
      "epoch:35 step:33101 [D loss: 0.448618, acc.: 77.34%] [G loss: 1.197981]\n",
      "epoch:35 step:33102 [D loss: 0.731849, acc.: 59.38%] [G loss: 1.255147]\n",
      "epoch:35 step:33103 [D loss: 0.566399, acc.: 67.97%] [G loss: 1.303967]\n",
      "epoch:35 step:33104 [D loss: 0.296280, acc.: 92.19%] [G loss: 1.622842]\n",
      "epoch:35 step:33105 [D loss: 0.735608, acc.: 59.38%] [G loss: 1.392438]\n",
      "epoch:35 step:33106 [D loss: 0.543256, acc.: 76.56%] [G loss: 1.684476]\n",
      "epoch:35 step:33107 [D loss: 0.598165, acc.: 70.31%] [G loss: 1.804114]\n",
      "epoch:35 step:33108 [D loss: 0.384739, acc.: 83.59%] [G loss: 1.877086]\n",
      "epoch:35 step:33109 [D loss: 0.446712, acc.: 78.12%] [G loss: 1.533930]\n",
      "epoch:35 step:33110 [D loss: 0.321584, acc.: 90.62%] [G loss: 1.548006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33111 [D loss: 0.583112, acc.: 69.53%] [G loss: 1.189081]\n",
      "epoch:35 step:33112 [D loss: 0.567357, acc.: 69.53%] [G loss: 0.965154]\n",
      "epoch:35 step:33113 [D loss: 0.563042, acc.: 68.75%] [G loss: 1.499687]\n",
      "epoch:35 step:33114 [D loss: 0.500251, acc.: 78.12%] [G loss: 1.302555]\n",
      "epoch:35 step:33115 [D loss: 0.476380, acc.: 79.69%] [G loss: 1.093560]\n",
      "epoch:35 step:33116 [D loss: 0.591582, acc.: 68.75%] [G loss: 0.920010]\n",
      "epoch:35 step:33117 [D loss: 0.453110, acc.: 81.25%] [G loss: 1.523281]\n",
      "epoch:35 step:33118 [D loss: 0.464782, acc.: 81.25%] [G loss: 1.627560]\n",
      "epoch:35 step:33119 [D loss: 0.529654, acc.: 70.31%] [G loss: 1.655139]\n",
      "epoch:35 step:33120 [D loss: 0.620560, acc.: 64.84%] [G loss: 1.993039]\n",
      "epoch:35 step:33121 [D loss: 0.600757, acc.: 64.06%] [G loss: 1.408826]\n",
      "epoch:35 step:33122 [D loss: 0.308578, acc.: 88.28%] [G loss: 1.497718]\n",
      "epoch:35 step:33123 [D loss: 0.418762, acc.: 83.59%] [G loss: 1.903535]\n",
      "epoch:35 step:33124 [D loss: 0.754250, acc.: 54.69%] [G loss: 1.272967]\n",
      "epoch:35 step:33125 [D loss: 0.485002, acc.: 76.56%] [G loss: 1.481204]\n",
      "epoch:35 step:33126 [D loss: 0.556640, acc.: 71.88%] [G loss: 1.229268]\n",
      "epoch:35 step:33127 [D loss: 0.512777, acc.: 77.34%] [G loss: 1.647651]\n",
      "epoch:35 step:33128 [D loss: 0.530972, acc.: 77.34%] [G loss: 1.514517]\n",
      "epoch:35 step:33129 [D loss: 0.418918, acc.: 79.69%] [G loss: 1.474854]\n",
      "epoch:35 step:33130 [D loss: 0.483706, acc.: 78.91%] [G loss: 1.635359]\n",
      "epoch:35 step:33131 [D loss: 0.581428, acc.: 71.88%] [G loss: 1.160120]\n",
      "epoch:35 step:33132 [D loss: 0.410740, acc.: 85.16%] [G loss: 1.641579]\n",
      "epoch:35 step:33133 [D loss: 0.284108, acc.: 91.41%] [G loss: 2.373442]\n",
      "epoch:35 step:33134 [D loss: 0.638436, acc.: 66.41%] [G loss: 1.530251]\n",
      "epoch:35 step:33135 [D loss: 0.380010, acc.: 86.72%] [G loss: 1.614671]\n",
      "epoch:35 step:33136 [D loss: 0.434701, acc.: 82.03%] [G loss: 1.481026]\n",
      "epoch:35 step:33137 [D loss: 0.674633, acc.: 58.59%] [G loss: 1.511048]\n",
      "epoch:35 step:33138 [D loss: 0.365482, acc.: 85.16%] [G loss: 1.564529]\n",
      "epoch:35 step:33139 [D loss: 0.483120, acc.: 77.34%] [G loss: 1.258574]\n",
      "epoch:35 step:33140 [D loss: 0.445389, acc.: 81.25%] [G loss: 1.394898]\n",
      "epoch:35 step:33141 [D loss: 0.422262, acc.: 80.47%] [G loss: 1.524327]\n",
      "epoch:35 step:33142 [D loss: 0.451038, acc.: 78.12%] [G loss: 1.837724]\n",
      "epoch:35 step:33143 [D loss: 0.739332, acc.: 60.16%] [G loss: 1.458875]\n",
      "epoch:35 step:33144 [D loss: 0.537630, acc.: 75.78%] [G loss: 1.324600]\n",
      "epoch:35 step:33145 [D loss: 0.385869, acc.: 85.16%] [G loss: 1.854738]\n",
      "epoch:35 step:33146 [D loss: 0.643353, acc.: 62.50%] [G loss: 1.371211]\n",
      "epoch:35 step:33147 [D loss: 0.490536, acc.: 71.09%] [G loss: 1.656598]\n",
      "epoch:35 step:33148 [D loss: 0.566682, acc.: 68.75%] [G loss: 1.713244]\n",
      "epoch:35 step:33149 [D loss: 0.653647, acc.: 61.72%] [G loss: 1.739932]\n",
      "epoch:35 step:33150 [D loss: 0.444025, acc.: 82.81%] [G loss: 1.653121]\n",
      "epoch:35 step:33151 [D loss: 0.595896, acc.: 69.53%] [G loss: 1.414682]\n",
      "epoch:35 step:33152 [D loss: 0.489608, acc.: 75.00%] [G loss: 1.569004]\n",
      "epoch:35 step:33153 [D loss: 0.624476, acc.: 64.06%] [G loss: 1.330138]\n",
      "epoch:35 step:33154 [D loss: 0.528812, acc.: 73.44%] [G loss: 1.012428]\n",
      "epoch:35 step:33155 [D loss: 0.450460, acc.: 79.69%] [G loss: 1.101657]\n",
      "epoch:35 step:33156 [D loss: 0.610292, acc.: 71.88%] [G loss: 1.627810]\n",
      "epoch:35 step:33157 [D loss: 0.514720, acc.: 74.22%] [G loss: 1.511527]\n",
      "epoch:35 step:33158 [D loss: 0.577281, acc.: 68.75%] [G loss: 2.116332]\n",
      "epoch:35 step:33159 [D loss: 0.389901, acc.: 82.81%] [G loss: 1.797356]\n",
      "epoch:35 step:33160 [D loss: 0.480730, acc.: 78.12%] [G loss: 1.414940]\n",
      "epoch:35 step:33161 [D loss: 0.790079, acc.: 50.78%] [G loss: 1.168922]\n",
      "epoch:35 step:33162 [D loss: 0.520902, acc.: 77.34%] [G loss: 0.965217]\n",
      "epoch:35 step:33163 [D loss: 0.317560, acc.: 88.28%] [G loss: 1.642783]\n",
      "epoch:35 step:33164 [D loss: 0.493009, acc.: 73.44%] [G loss: 1.559665]\n",
      "epoch:35 step:33165 [D loss: 0.558753, acc.: 73.44%] [G loss: 1.640645]\n",
      "epoch:35 step:33166 [D loss: 0.405310, acc.: 80.47%] [G loss: 1.504900]\n",
      "epoch:35 step:33167 [D loss: 0.538583, acc.: 71.09%] [G loss: 1.554018]\n",
      "epoch:35 step:33168 [D loss: 0.750847, acc.: 52.34%] [G loss: 1.461318]\n",
      "epoch:35 step:33169 [D loss: 0.575560, acc.: 74.22%] [G loss: 1.696593]\n",
      "epoch:35 step:33170 [D loss: 0.449590, acc.: 82.81%] [G loss: 1.688480]\n",
      "epoch:35 step:33171 [D loss: 0.723400, acc.: 57.81%] [G loss: 1.654730]\n",
      "epoch:35 step:33172 [D loss: 0.447083, acc.: 78.91%] [G loss: 1.612952]\n",
      "epoch:35 step:33173 [D loss: 0.523602, acc.: 75.00%] [G loss: 1.842574]\n",
      "epoch:35 step:33174 [D loss: 0.641957, acc.: 67.97%] [G loss: 1.377174]\n",
      "epoch:35 step:33175 [D loss: 0.404937, acc.: 85.16%] [G loss: 2.251073]\n",
      "epoch:35 step:33176 [D loss: 0.710269, acc.: 59.38%] [G loss: 1.472135]\n",
      "epoch:35 step:33177 [D loss: 0.487948, acc.: 77.34%] [G loss: 1.823021]\n",
      "epoch:35 step:33178 [D loss: 0.584894, acc.: 66.41%] [G loss: 2.049461]\n",
      "epoch:35 step:33179 [D loss: 0.448353, acc.: 79.69%] [G loss: 1.520491]\n",
      "epoch:35 step:33180 [D loss: 0.518704, acc.: 71.88%] [G loss: 1.396388]\n",
      "epoch:35 step:33181 [D loss: 0.485331, acc.: 78.91%] [G loss: 1.355961]\n",
      "epoch:35 step:33182 [D loss: 0.430889, acc.: 85.94%] [G loss: 1.406957]\n",
      "epoch:35 step:33183 [D loss: 0.643678, acc.: 61.72%] [G loss: 2.004079]\n",
      "epoch:35 step:33184 [D loss: 0.741740, acc.: 53.91%] [G loss: 2.093856]\n",
      "epoch:35 step:33185 [D loss: 0.598439, acc.: 67.19%] [G loss: 1.403847]\n",
      "epoch:35 step:33186 [D loss: 0.781022, acc.: 51.56%] [G loss: 1.484876]\n",
      "epoch:35 step:33187 [D loss: 0.565544, acc.: 72.66%] [G loss: 1.484320]\n",
      "epoch:35 step:33188 [D loss: 0.670377, acc.: 60.16%] [G loss: 1.563780]\n",
      "epoch:35 step:33189 [D loss: 0.498202, acc.: 74.22%] [G loss: 1.355236]\n",
      "epoch:35 step:33190 [D loss: 0.356814, acc.: 88.28%] [G loss: 1.587646]\n",
      "epoch:35 step:33191 [D loss: 0.575437, acc.: 70.31%] [G loss: 1.744732]\n",
      "epoch:35 step:33192 [D loss: 0.481699, acc.: 76.56%] [G loss: 1.197732]\n",
      "epoch:35 step:33193 [D loss: 0.535903, acc.: 70.31%] [G loss: 0.925220]\n",
      "epoch:35 step:33194 [D loss: 0.426030, acc.: 78.12%] [G loss: 1.923835]\n",
      "epoch:35 step:33195 [D loss: 0.720476, acc.: 64.06%] [G loss: 1.749439]\n",
      "epoch:35 step:33196 [D loss: 0.532137, acc.: 71.09%] [G loss: 1.287360]\n",
      "epoch:35 step:33197 [D loss: 0.723848, acc.: 61.72%] [G loss: 0.999812]\n",
      "epoch:35 step:33198 [D loss: 0.593886, acc.: 66.41%] [G loss: 1.652729]\n",
      "epoch:35 step:33199 [D loss: 0.830171, acc.: 50.78%] [G loss: 1.292782]\n",
      "epoch:35 step:33200 [D loss: 0.416587, acc.: 78.91%] [G loss: 1.862868]\n",
      "##############\n",
      "[2.83819708 2.00122742 2.02961678 2.80611791 0.82150756 6.07532837\n",
      " 1.97536953 2.62738495 3.9090207  7.14868929]\n",
      "##########\n",
      "epoch:35 step:33201 [D loss: 0.414332, acc.: 81.25%] [G loss: 1.512047]\n",
      "epoch:35 step:33202 [D loss: 0.451858, acc.: 78.12%] [G loss: 2.090691]\n",
      "epoch:35 step:33203 [D loss: 0.540077, acc.: 71.09%] [G loss: 1.291479]\n",
      "epoch:35 step:33204 [D loss: 0.443225, acc.: 80.47%] [G loss: 2.019662]\n",
      "epoch:35 step:33205 [D loss: 0.419028, acc.: 79.69%] [G loss: 1.904948]\n",
      "epoch:35 step:33206 [D loss: 0.439292, acc.: 81.25%] [G loss: 1.778409]\n",
      "epoch:35 step:33207 [D loss: 0.515175, acc.: 73.44%] [G loss: 1.511013]\n",
      "epoch:35 step:33208 [D loss: 0.635286, acc.: 63.28%] [G loss: 0.974529]\n",
      "epoch:35 step:33209 [D loss: 0.793662, acc.: 55.47%] [G loss: 1.187567]\n",
      "epoch:35 step:33210 [D loss: 0.629885, acc.: 67.97%] [G loss: 1.357720]\n",
      "epoch:35 step:33211 [D loss: 0.536167, acc.: 72.66%] [G loss: 1.685288]\n",
      "epoch:35 step:33212 [D loss: 0.645780, acc.: 71.88%] [G loss: 1.900439]\n",
      "epoch:35 step:33213 [D loss: 0.509676, acc.: 74.22%] [G loss: 1.795482]\n",
      "epoch:35 step:33214 [D loss: 0.554059, acc.: 72.66%] [G loss: 1.895556]\n",
      "epoch:35 step:33215 [D loss: 0.559317, acc.: 70.31%] [G loss: 1.797009]\n",
      "epoch:35 step:33216 [D loss: 0.537149, acc.: 71.09%] [G loss: 1.087635]\n",
      "epoch:35 step:33217 [D loss: 0.569585, acc.: 74.22%] [G loss: 1.474792]\n",
      "epoch:35 step:33218 [D loss: 0.540726, acc.: 74.22%] [G loss: 1.533116]\n",
      "epoch:35 step:33219 [D loss: 0.453011, acc.: 81.25%] [G loss: 1.725581]\n",
      "epoch:35 step:33220 [D loss: 0.610502, acc.: 68.75%] [G loss: 1.528170]\n",
      "epoch:35 step:33221 [D loss: 0.563279, acc.: 68.75%] [G loss: 1.034824]\n",
      "epoch:35 step:33222 [D loss: 0.592044, acc.: 70.31%] [G loss: 1.786624]\n",
      "epoch:35 step:33223 [D loss: 0.432956, acc.: 85.16%] [G loss: 1.575518]\n",
      "epoch:35 step:33224 [D loss: 0.498882, acc.: 73.44%] [G loss: 1.613396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33225 [D loss: 0.691030, acc.: 57.81%] [G loss: 1.634709]\n",
      "epoch:35 step:33226 [D loss: 0.514022, acc.: 74.22%] [G loss: 1.352351]\n",
      "epoch:35 step:33227 [D loss: 0.421386, acc.: 82.81%] [G loss: 1.592298]\n",
      "epoch:35 step:33228 [D loss: 0.439299, acc.: 79.69%] [G loss: 1.645012]\n",
      "epoch:35 step:33229 [D loss: 0.563318, acc.: 72.66%] [G loss: 1.338201]\n",
      "epoch:35 step:33230 [D loss: 0.385740, acc.: 85.94%] [G loss: 1.623071]\n",
      "epoch:35 step:33231 [D loss: 0.458238, acc.: 78.12%] [G loss: 1.613575]\n",
      "epoch:35 step:33232 [D loss: 0.518083, acc.: 71.88%] [G loss: 1.374188]\n",
      "epoch:35 step:33233 [D loss: 0.574521, acc.: 64.06%] [G loss: 1.680908]\n",
      "epoch:35 step:33234 [D loss: 0.551505, acc.: 74.22%] [G loss: 1.138431]\n",
      "epoch:35 step:33235 [D loss: 0.419423, acc.: 82.03%] [G loss: 1.416127]\n",
      "epoch:35 step:33236 [D loss: 0.365508, acc.: 88.28%] [G loss: 0.944994]\n",
      "epoch:35 step:33237 [D loss: 0.573286, acc.: 68.75%] [G loss: 1.644843]\n",
      "epoch:35 step:33238 [D loss: 0.448689, acc.: 79.69%] [G loss: 1.674059]\n",
      "epoch:35 step:33239 [D loss: 0.523340, acc.: 73.44%] [G loss: 1.903818]\n",
      "epoch:35 step:33240 [D loss: 0.601019, acc.: 71.88%] [G loss: 2.243417]\n",
      "epoch:35 step:33241 [D loss: 0.568520, acc.: 70.31%] [G loss: 1.526881]\n",
      "epoch:35 step:33242 [D loss: 0.539040, acc.: 71.88%] [G loss: 1.608218]\n",
      "epoch:35 step:33243 [D loss: 0.478557, acc.: 76.56%] [G loss: 1.655504]\n",
      "epoch:35 step:33244 [D loss: 0.674341, acc.: 64.06%] [G loss: 1.397838]\n",
      "epoch:35 step:33245 [D loss: 0.654150, acc.: 59.38%] [G loss: 1.467526]\n",
      "epoch:35 step:33246 [D loss: 0.550606, acc.: 72.66%] [G loss: 1.374245]\n",
      "epoch:35 step:33247 [D loss: 0.662364, acc.: 64.06%] [G loss: 1.740553]\n",
      "epoch:35 step:33248 [D loss: 0.446330, acc.: 80.47%] [G loss: 1.428837]\n",
      "epoch:35 step:33249 [D loss: 0.407938, acc.: 85.94%] [G loss: 1.696046]\n",
      "epoch:35 step:33250 [D loss: 0.372453, acc.: 84.38%] [G loss: 1.621962]\n",
      "epoch:35 step:33251 [D loss: 0.695326, acc.: 57.81%] [G loss: 1.429696]\n",
      "epoch:35 step:33252 [D loss: 0.544076, acc.: 69.53%] [G loss: 1.368185]\n",
      "epoch:35 step:33253 [D loss: 0.321396, acc.: 90.62%] [G loss: 1.713920]\n",
      "epoch:35 step:33254 [D loss: 0.315813, acc.: 90.62%] [G loss: 1.386979]\n",
      "epoch:35 step:33255 [D loss: 0.603994, acc.: 66.41%] [G loss: 1.816122]\n",
      "epoch:35 step:33256 [D loss: 0.600827, acc.: 71.09%] [G loss: 1.344673]\n",
      "epoch:35 step:33257 [D loss: 0.649650, acc.: 65.62%] [G loss: 1.629485]\n",
      "epoch:35 step:33258 [D loss: 0.704951, acc.: 57.03%] [G loss: 1.628514]\n",
      "epoch:35 step:33259 [D loss: 0.561541, acc.: 67.97%] [G loss: 1.395437]\n",
      "epoch:35 step:33260 [D loss: 0.583198, acc.: 78.91%] [G loss: 1.241441]\n",
      "epoch:35 step:33261 [D loss: 0.443043, acc.: 82.81%] [G loss: 1.950404]\n",
      "epoch:35 step:33262 [D loss: 0.597829, acc.: 67.19%] [G loss: 1.517653]\n",
      "epoch:35 step:33263 [D loss: 0.364539, acc.: 85.16%] [G loss: 1.546472]\n",
      "epoch:35 step:33264 [D loss: 0.601046, acc.: 67.19%] [G loss: 1.300445]\n",
      "epoch:35 step:33265 [D loss: 0.765645, acc.: 57.03%] [G loss: 1.022267]\n",
      "epoch:35 step:33266 [D loss: 0.504430, acc.: 75.00%] [G loss: 1.724863]\n",
      "epoch:35 step:33267 [D loss: 0.769209, acc.: 53.12%] [G loss: 1.276670]\n",
      "epoch:35 step:33268 [D loss: 0.407086, acc.: 84.38%] [G loss: 1.957443]\n",
      "epoch:35 step:33269 [D loss: 0.515108, acc.: 73.44%] [G loss: 2.225837]\n",
      "epoch:35 step:33270 [D loss: 0.703462, acc.: 58.59%] [G loss: 1.552564]\n",
      "epoch:35 step:33271 [D loss: 0.549831, acc.: 67.19%] [G loss: 1.367610]\n",
      "epoch:35 step:33272 [D loss: 0.530197, acc.: 76.56%] [G loss: 2.117221]\n",
      "epoch:35 step:33273 [D loss: 0.748017, acc.: 53.91%] [G loss: 1.637799]\n",
      "epoch:35 step:33274 [D loss: 0.376414, acc.: 82.81%] [G loss: 1.703525]\n",
      "epoch:35 step:33275 [D loss: 0.383808, acc.: 83.59%] [G loss: 1.952336]\n",
      "epoch:35 step:33276 [D loss: 0.888475, acc.: 53.91%] [G loss: 1.754558]\n",
      "epoch:35 step:33277 [D loss: 0.485156, acc.: 75.00%] [G loss: 1.535136]\n",
      "epoch:35 step:33278 [D loss: 0.562955, acc.: 71.09%] [G loss: 1.359568]\n",
      "epoch:35 step:33279 [D loss: 0.383812, acc.: 85.16%] [G loss: 1.737241]\n",
      "epoch:35 step:33280 [D loss: 0.528656, acc.: 71.09%] [G loss: 1.295675]\n",
      "epoch:35 step:33281 [D loss: 0.507072, acc.: 75.00%] [G loss: 1.336977]\n",
      "epoch:35 step:33282 [D loss: 0.462206, acc.: 78.12%] [G loss: 1.884769]\n",
      "epoch:35 step:33283 [D loss: 0.561844, acc.: 71.09%] [G loss: 1.554226]\n",
      "epoch:35 step:33284 [D loss: 0.514532, acc.: 73.44%] [G loss: 1.483809]\n",
      "epoch:35 step:33285 [D loss: 0.592576, acc.: 68.75%] [G loss: 1.579816]\n",
      "epoch:35 step:33286 [D loss: 0.823739, acc.: 51.56%] [G loss: 0.821417]\n",
      "epoch:35 step:33287 [D loss: 0.479615, acc.: 78.12%] [G loss: 1.459908]\n",
      "epoch:35 step:33288 [D loss: 0.567745, acc.: 69.53%] [G loss: 1.682564]\n",
      "epoch:35 step:33289 [D loss: 0.477765, acc.: 76.56%] [G loss: 1.775779]\n",
      "epoch:35 step:33290 [D loss: 0.451320, acc.: 79.69%] [G loss: 1.373123]\n",
      "epoch:35 step:33291 [D loss: 0.543397, acc.: 72.66%] [G loss: 1.475209]\n",
      "epoch:35 step:33292 [D loss: 0.630697, acc.: 62.50%] [G loss: 1.918806]\n",
      "epoch:35 step:33293 [D loss: 0.665299, acc.: 59.38%] [G loss: 1.294448]\n",
      "epoch:35 step:33294 [D loss: 0.748368, acc.: 56.25%] [G loss: 1.539206]\n",
      "epoch:35 step:33295 [D loss: 0.505419, acc.: 72.66%] [G loss: 1.874332]\n",
      "epoch:35 step:33296 [D loss: 0.454059, acc.: 81.25%] [G loss: 1.748613]\n",
      "epoch:35 step:33297 [D loss: 0.407967, acc.: 82.03%] [G loss: 2.104222]\n",
      "epoch:35 step:33298 [D loss: 0.495212, acc.: 75.00%] [G loss: 2.158482]\n",
      "epoch:35 step:33299 [D loss: 0.577223, acc.: 70.31%] [G loss: 1.383116]\n",
      "epoch:35 step:33300 [D loss: 0.515461, acc.: 73.44%] [G loss: 1.504859]\n",
      "epoch:35 step:33301 [D loss: 0.531027, acc.: 73.44%] [G loss: 1.847524]\n",
      "epoch:35 step:33302 [D loss: 0.477807, acc.: 75.78%] [G loss: 1.553095]\n",
      "epoch:35 step:33303 [D loss: 0.514758, acc.: 72.66%] [G loss: 1.580061]\n",
      "epoch:35 step:33304 [D loss: 0.432428, acc.: 79.69%] [G loss: 1.921048]\n",
      "epoch:35 step:33305 [D loss: 0.802524, acc.: 52.34%] [G loss: 1.521668]\n",
      "epoch:35 step:33306 [D loss: 0.529453, acc.: 75.00%] [G loss: 1.957278]\n",
      "epoch:35 step:33307 [D loss: 0.598355, acc.: 65.62%] [G loss: 1.831761]\n",
      "epoch:35 step:33308 [D loss: 0.402816, acc.: 85.94%] [G loss: 1.883035]\n",
      "epoch:35 step:33309 [D loss: 0.416635, acc.: 85.94%] [G loss: 1.931498]\n",
      "epoch:35 step:33310 [D loss: 0.820822, acc.: 57.03%] [G loss: 2.030216]\n",
      "epoch:35 step:33311 [D loss: 0.582298, acc.: 67.19%] [G loss: 1.262697]\n",
      "epoch:35 step:33312 [D loss: 0.612923, acc.: 66.41%] [G loss: 1.708122]\n",
      "epoch:35 step:33313 [D loss: 0.485571, acc.: 76.56%] [G loss: 1.340843]\n",
      "epoch:35 step:33314 [D loss: 0.421884, acc.: 83.59%] [G loss: 1.704093]\n",
      "epoch:35 step:33315 [D loss: 0.401598, acc.: 83.59%] [G loss: 1.876665]\n",
      "epoch:35 step:33316 [D loss: 0.435675, acc.: 85.94%] [G loss: 1.725627]\n",
      "epoch:35 step:33317 [D loss: 0.437954, acc.: 80.47%] [G loss: 1.481827]\n",
      "epoch:35 step:33318 [D loss: 0.592810, acc.: 67.97%] [G loss: 1.083738]\n",
      "epoch:35 step:33319 [D loss: 0.581547, acc.: 67.19%] [G loss: 2.013916]\n",
      "epoch:35 step:33320 [D loss: 0.560913, acc.: 74.22%] [G loss: 1.883988]\n",
      "epoch:35 step:33321 [D loss: 0.598871, acc.: 68.75%] [G loss: 1.150033]\n",
      "epoch:35 step:33322 [D loss: 0.511113, acc.: 74.22%] [G loss: 1.547176]\n",
      "epoch:35 step:33323 [D loss: 0.607346, acc.: 68.75%] [G loss: 1.440890]\n",
      "epoch:35 step:33324 [D loss: 0.327641, acc.: 90.62%] [G loss: 1.804686]\n",
      "epoch:35 step:33325 [D loss: 0.605758, acc.: 65.62%] [G loss: 1.682615]\n",
      "epoch:35 step:33326 [D loss: 0.492056, acc.: 73.44%] [G loss: 1.772790]\n",
      "epoch:35 step:33327 [D loss: 0.604523, acc.: 67.97%] [G loss: 1.390715]\n",
      "epoch:35 step:33328 [D loss: 0.471376, acc.: 78.91%] [G loss: 1.789921]\n",
      "epoch:35 step:33329 [D loss: 0.628757, acc.: 64.06%] [G loss: 1.466730]\n",
      "epoch:35 step:33330 [D loss: 0.694384, acc.: 59.38%] [G loss: 1.889984]\n",
      "epoch:35 step:33331 [D loss: 0.680554, acc.: 69.53%] [G loss: 1.289670]\n",
      "epoch:35 step:33332 [D loss: 0.427325, acc.: 81.25%] [G loss: 1.249204]\n",
      "epoch:35 step:33333 [D loss: 0.442215, acc.: 82.81%] [G loss: 1.643877]\n",
      "epoch:35 step:33334 [D loss: 0.565582, acc.: 71.09%] [G loss: 1.992855]\n",
      "epoch:35 step:33335 [D loss: 0.546988, acc.: 75.00%] [G loss: 1.343328]\n",
      "epoch:35 step:33336 [D loss: 0.503477, acc.: 76.56%] [G loss: 1.592064]\n",
      "epoch:35 step:33337 [D loss: 0.391761, acc.: 88.28%] [G loss: 1.741805]\n",
      "epoch:35 step:33338 [D loss: 0.461980, acc.: 77.34%] [G loss: 1.817224]\n",
      "epoch:35 step:33339 [D loss: 0.612111, acc.: 62.50%] [G loss: 1.180026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33340 [D loss: 0.713595, acc.: 60.94%] [G loss: 1.390960]\n",
      "epoch:35 step:33341 [D loss: 0.524537, acc.: 74.22%] [G loss: 1.212891]\n",
      "epoch:35 step:33342 [D loss: 0.628890, acc.: 66.41%] [G loss: 1.304068]\n",
      "epoch:35 step:33343 [D loss: 0.367683, acc.: 83.59%] [G loss: 1.904863]\n",
      "epoch:35 step:33344 [D loss: 0.607923, acc.: 65.62%] [G loss: 2.267910]\n",
      "epoch:35 step:33345 [D loss: 0.540771, acc.: 77.34%] [G loss: 1.438920]\n",
      "epoch:35 step:33346 [D loss: 0.538813, acc.: 72.66%] [G loss: 1.367699]\n",
      "epoch:35 step:33347 [D loss: 0.489087, acc.: 76.56%] [G loss: 1.596803]\n",
      "epoch:35 step:33348 [D loss: 0.485626, acc.: 77.34%] [G loss: 1.488639]\n",
      "epoch:35 step:33349 [D loss: 0.624910, acc.: 64.06%] [G loss: 1.399322]\n",
      "epoch:35 step:33350 [D loss: 0.393552, acc.: 84.38%] [G loss: 1.315279]\n",
      "epoch:35 step:33351 [D loss: 0.581668, acc.: 70.31%] [G loss: 1.101134]\n",
      "epoch:35 step:33352 [D loss: 0.500374, acc.: 77.34%] [G loss: 1.400723]\n",
      "epoch:35 step:33353 [D loss: 0.574483, acc.: 71.88%] [G loss: 1.210723]\n",
      "epoch:35 step:33354 [D loss: 0.567870, acc.: 74.22%] [G loss: 1.612715]\n",
      "epoch:35 step:33355 [D loss: 0.552958, acc.: 71.88%] [G loss: 1.441278]\n",
      "epoch:35 step:33356 [D loss: 0.589093, acc.: 67.19%] [G loss: 1.276519]\n",
      "epoch:35 step:33357 [D loss: 0.623770, acc.: 63.28%] [G loss: 1.689696]\n",
      "epoch:35 step:33358 [D loss: 0.517677, acc.: 74.22%] [G loss: 1.390783]\n",
      "epoch:35 step:33359 [D loss: 0.646852, acc.: 62.50%] [G loss: 1.544868]\n",
      "epoch:35 step:33360 [D loss: 0.577108, acc.: 71.09%] [G loss: 1.338847]\n",
      "epoch:35 step:33361 [D loss: 0.452658, acc.: 75.00%] [G loss: 1.452338]\n",
      "epoch:35 step:33362 [D loss: 0.445138, acc.: 80.47%] [G loss: 1.574286]\n",
      "epoch:35 step:33363 [D loss: 0.786747, acc.: 53.12%] [G loss: 1.315821]\n",
      "epoch:35 step:33364 [D loss: 0.459941, acc.: 84.38%] [G loss: 1.180179]\n",
      "epoch:35 step:33365 [D loss: 0.502121, acc.: 75.78%] [G loss: 1.067710]\n",
      "epoch:35 step:33366 [D loss: 0.528070, acc.: 73.44%] [G loss: 1.187317]\n",
      "epoch:35 step:33367 [D loss: 0.591523, acc.: 71.09%] [G loss: 1.511493]\n",
      "epoch:35 step:33368 [D loss: 0.878730, acc.: 46.88%] [G loss: 1.472368]\n",
      "epoch:35 step:33369 [D loss: 0.634095, acc.: 65.62%] [G loss: 1.394059]\n",
      "epoch:35 step:33370 [D loss: 0.483528, acc.: 77.34%] [G loss: 1.470308]\n",
      "epoch:35 step:33371 [D loss: 0.531351, acc.: 75.78%] [G loss: 1.973130]\n",
      "epoch:35 step:33372 [D loss: 0.659316, acc.: 67.97%] [G loss: 1.327613]\n",
      "epoch:35 step:33373 [D loss: 0.440785, acc.: 82.81%] [G loss: 1.424610]\n",
      "epoch:35 step:33374 [D loss: 0.492868, acc.: 74.22%] [G loss: 0.893371]\n",
      "epoch:35 step:33375 [D loss: 0.494134, acc.: 80.47%] [G loss: 1.136718]\n",
      "epoch:35 step:33376 [D loss: 0.527681, acc.: 72.66%] [G loss: 1.050704]\n",
      "epoch:35 step:33377 [D loss: 0.545185, acc.: 75.00%] [G loss: 1.552360]\n",
      "epoch:35 step:33378 [D loss: 0.597220, acc.: 70.31%] [G loss: 1.686261]\n",
      "epoch:35 step:33379 [D loss: 0.602932, acc.: 71.88%] [G loss: 1.295503]\n",
      "epoch:35 step:33380 [D loss: 0.553860, acc.: 71.09%] [G loss: 1.540921]\n",
      "epoch:35 step:33381 [D loss: 0.560252, acc.: 69.53%] [G loss: 1.514403]\n",
      "epoch:35 step:33382 [D loss: 0.448248, acc.: 75.00%] [G loss: 1.619752]\n",
      "epoch:35 step:33383 [D loss: 0.543844, acc.: 74.22%] [G loss: 2.132476]\n",
      "epoch:35 step:33384 [D loss: 0.520018, acc.: 74.22%] [G loss: 1.582353]\n",
      "epoch:35 step:33385 [D loss: 0.338574, acc.: 89.06%] [G loss: 1.763149]\n",
      "epoch:35 step:33386 [D loss: 0.520630, acc.: 71.88%] [G loss: 1.077461]\n",
      "epoch:35 step:33387 [D loss: 0.335215, acc.: 89.06%] [G loss: 1.470969]\n",
      "epoch:35 step:33388 [D loss: 0.673091, acc.: 60.16%] [G loss: 1.355671]\n",
      "epoch:35 step:33389 [D loss: 0.447893, acc.: 82.81%] [G loss: 1.508346]\n",
      "epoch:35 step:33390 [D loss: 0.498828, acc.: 75.00%] [G loss: 1.382933]\n",
      "epoch:35 step:33391 [D loss: 0.499600, acc.: 78.91%] [G loss: 1.211855]\n",
      "epoch:35 step:33392 [D loss: 0.392499, acc.: 86.72%] [G loss: 1.796922]\n",
      "epoch:35 step:33393 [D loss: 0.600568, acc.: 60.94%] [G loss: 1.922709]\n",
      "epoch:35 step:33394 [D loss: 0.483246, acc.: 75.78%] [G loss: 1.017861]\n",
      "epoch:35 step:33395 [D loss: 0.353204, acc.: 85.94%] [G loss: 1.887330]\n",
      "epoch:35 step:33396 [D loss: 0.876691, acc.: 52.34%] [G loss: 1.234122]\n",
      "epoch:35 step:33397 [D loss: 0.558079, acc.: 70.31%] [G loss: 1.462755]\n",
      "epoch:35 step:33398 [D loss: 0.566980, acc.: 72.66%] [G loss: 1.643052]\n",
      "epoch:35 step:33399 [D loss: 0.696248, acc.: 54.69%] [G loss: 1.522937]\n",
      "epoch:35 step:33400 [D loss: 0.582085, acc.: 67.97%] [G loss: 1.319154]\n",
      "##############\n",
      "[2.75005892 2.10621049 1.64785099 2.78362733 0.73954877 5.71496097\n",
      " 2.09787504 2.46031181 3.86322052 4.68111159]\n",
      "##########\n",
      "epoch:35 step:33401 [D loss: 0.461437, acc.: 76.56%] [G loss: 1.051176]\n",
      "epoch:35 step:33402 [D loss: 0.509959, acc.: 75.78%] [G loss: 1.779243]\n",
      "epoch:35 step:33403 [D loss: 0.524402, acc.: 71.88%] [G loss: 1.609692]\n",
      "epoch:35 step:33404 [D loss: 0.498121, acc.: 75.78%] [G loss: 1.456977]\n",
      "epoch:35 step:33405 [D loss: 0.420035, acc.: 79.69%] [G loss: 1.461737]\n",
      "epoch:35 step:33406 [D loss: 0.524978, acc.: 73.44%] [G loss: 1.326616]\n",
      "epoch:35 step:33407 [D loss: 0.853217, acc.: 44.53%] [G loss: 1.258320]\n",
      "epoch:35 step:33408 [D loss: 0.412199, acc.: 81.25%] [G loss: 1.749430]\n",
      "epoch:35 step:33409 [D loss: 0.627111, acc.: 66.41%] [G loss: 1.240268]\n",
      "epoch:35 step:33410 [D loss: 0.502334, acc.: 75.78%] [G loss: 1.532080]\n",
      "epoch:35 step:33411 [D loss: 0.496383, acc.: 77.34%] [G loss: 1.606503]\n",
      "epoch:35 step:33412 [D loss: 0.454982, acc.: 78.12%] [G loss: 1.414300]\n",
      "epoch:35 step:33413 [D loss: 0.473134, acc.: 75.78%] [G loss: 1.900865]\n",
      "epoch:35 step:33414 [D loss: 0.488153, acc.: 75.78%] [G loss: 2.009129]\n",
      "epoch:35 step:33415 [D loss: 0.617930, acc.: 67.97%] [G loss: 1.658289]\n",
      "epoch:35 step:33416 [D loss: 0.607793, acc.: 65.62%] [G loss: 1.385237]\n",
      "epoch:35 step:33417 [D loss: 0.658703, acc.: 62.50%] [G loss: 1.249875]\n",
      "epoch:35 step:33418 [D loss: 0.364261, acc.: 88.28%] [G loss: 1.978592]\n",
      "epoch:35 step:33419 [D loss: 0.629223, acc.: 61.72%] [G loss: 1.747365]\n",
      "epoch:35 step:33420 [D loss: 0.519021, acc.: 75.78%] [G loss: 1.751342]\n",
      "epoch:35 step:33421 [D loss: 0.459349, acc.: 81.25%] [G loss: 2.121960]\n",
      "epoch:35 step:33422 [D loss: 0.492148, acc.: 76.56%] [G loss: 1.744064]\n",
      "epoch:35 step:33423 [D loss: 0.615866, acc.: 67.19%] [G loss: 1.150819]\n",
      "epoch:35 step:33424 [D loss: 0.564489, acc.: 74.22%] [G loss: 1.677539]\n",
      "epoch:35 step:33425 [D loss: 0.566968, acc.: 71.09%] [G loss: 1.194588]\n",
      "epoch:35 step:33426 [D loss: 0.553021, acc.: 75.00%] [G loss: 1.379513]\n",
      "epoch:35 step:33427 [D loss: 0.504412, acc.: 77.34%] [G loss: 1.432144]\n",
      "epoch:35 step:33428 [D loss: 0.469084, acc.: 78.12%] [G loss: 1.579756]\n",
      "epoch:35 step:33429 [D loss: 0.495971, acc.: 78.91%] [G loss: 1.749382]\n",
      "epoch:35 step:33430 [D loss: 0.507338, acc.: 72.66%] [G loss: 1.729926]\n",
      "epoch:35 step:33431 [D loss: 0.470873, acc.: 77.34%] [G loss: 1.445904]\n",
      "epoch:35 step:33432 [D loss: 0.493804, acc.: 77.34%] [G loss: 1.176601]\n",
      "epoch:35 step:33433 [D loss: 0.619305, acc.: 67.19%] [G loss: 1.172463]\n",
      "epoch:35 step:33434 [D loss: 0.564555, acc.: 70.31%] [G loss: 1.708157]\n",
      "epoch:35 step:33435 [D loss: 0.417050, acc.: 80.47%] [G loss: 1.993652]\n",
      "epoch:35 step:33436 [D loss: 0.406183, acc.: 87.50%] [G loss: 2.077955]\n",
      "epoch:35 step:33437 [D loss: 0.501602, acc.: 76.56%] [G loss: 2.101186]\n",
      "epoch:35 step:33438 [D loss: 0.743227, acc.: 56.25%] [G loss: 1.249571]\n",
      "epoch:35 step:33439 [D loss: 0.516701, acc.: 74.22%] [G loss: 1.276347]\n",
      "epoch:35 step:33440 [D loss: 0.475985, acc.: 74.22%] [G loss: 1.669649]\n",
      "epoch:35 step:33441 [D loss: 0.456496, acc.: 78.91%] [G loss: 1.080091]\n",
      "epoch:35 step:33442 [D loss: 0.694787, acc.: 57.03%] [G loss: 1.205909]\n",
      "epoch:35 step:33443 [D loss: 0.233114, acc.: 93.75%] [G loss: 1.872449]\n",
      "epoch:35 step:33444 [D loss: 0.425948, acc.: 82.03%] [G loss: 1.552969]\n",
      "epoch:35 step:33445 [D loss: 0.620941, acc.: 61.72%] [G loss: 1.463763]\n",
      "epoch:35 step:33446 [D loss: 0.321092, acc.: 89.84%] [G loss: 1.836300]\n",
      "epoch:35 step:33447 [D loss: 0.528108, acc.: 74.22%] [G loss: 1.440584]\n",
      "epoch:35 step:33448 [D loss: 0.508223, acc.: 79.69%] [G loss: 1.563732]\n",
      "epoch:35 step:33449 [D loss: 0.635331, acc.: 64.84%] [G loss: 1.354014]\n",
      "epoch:35 step:33450 [D loss: 0.546883, acc.: 70.31%] [G loss: 1.266120]\n",
      "epoch:35 step:33451 [D loss: 0.427853, acc.: 84.38%] [G loss: 1.416329]\n",
      "epoch:35 step:33452 [D loss: 0.561857, acc.: 69.53%] [G loss: 1.442805]\n",
      "epoch:35 step:33453 [D loss: 0.499050, acc.: 80.47%] [G loss: 1.035997]\n",
      "epoch:35 step:33454 [D loss: 0.669502, acc.: 65.62%] [G loss: 1.279021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33455 [D loss: 0.332241, acc.: 89.84%] [G loss: 1.380745]\n",
      "epoch:35 step:33456 [D loss: 0.333896, acc.: 87.50%] [G loss: 1.371771]\n",
      "epoch:35 step:33457 [D loss: 0.713547, acc.: 59.38%] [G loss: 1.227684]\n",
      "epoch:35 step:33458 [D loss: 0.555737, acc.: 68.75%] [G loss: 1.393808]\n",
      "epoch:35 step:33459 [D loss: 0.692481, acc.: 61.72%] [G loss: 1.390518]\n",
      "epoch:35 step:33460 [D loss: 0.609755, acc.: 65.62%] [G loss: 0.923618]\n",
      "epoch:35 step:33461 [D loss: 0.552227, acc.: 68.75%] [G loss: 1.517848]\n",
      "epoch:35 step:33462 [D loss: 0.493116, acc.: 78.12%] [G loss: 1.844599]\n",
      "epoch:35 step:33463 [D loss: 0.493184, acc.: 74.22%] [G loss: 1.252621]\n",
      "epoch:35 step:33464 [D loss: 0.521386, acc.: 71.09%] [G loss: 1.226483]\n",
      "epoch:35 step:33465 [D loss: 0.340429, acc.: 89.06%] [G loss: 2.089462]\n",
      "epoch:35 step:33466 [D loss: 0.423176, acc.: 85.16%] [G loss: 1.453128]\n",
      "epoch:35 step:33467 [D loss: 0.429264, acc.: 80.47%] [G loss: 1.511681]\n",
      "epoch:35 step:33468 [D loss: 0.563471, acc.: 74.22%] [G loss: 1.324593]\n",
      "epoch:35 step:33469 [D loss: 0.576023, acc.: 75.00%] [G loss: 1.706865]\n",
      "epoch:35 step:33470 [D loss: 0.521704, acc.: 76.56%] [G loss: 1.152616]\n",
      "epoch:35 step:33471 [D loss: 0.507651, acc.: 77.34%] [G loss: 1.632659]\n",
      "epoch:35 step:33472 [D loss: 0.643960, acc.: 63.28%] [G loss: 1.456994]\n",
      "epoch:35 step:33473 [D loss: 0.562153, acc.: 70.31%] [G loss: 1.050512]\n",
      "epoch:35 step:33474 [D loss: 0.482883, acc.: 78.12%] [G loss: 1.763532]\n",
      "epoch:35 step:33475 [D loss: 0.488686, acc.: 78.12%] [G loss: 1.627352]\n",
      "epoch:35 step:33476 [D loss: 0.432495, acc.: 82.81%] [G loss: 1.442701]\n",
      "epoch:35 step:33477 [D loss: 0.441047, acc.: 82.81%] [G loss: 1.152957]\n",
      "epoch:35 step:33478 [D loss: 0.679327, acc.: 65.62%] [G loss: 1.836736]\n",
      "epoch:35 step:33479 [D loss: 0.680008, acc.: 66.41%] [G loss: 1.187616]\n",
      "epoch:35 step:33480 [D loss: 0.494482, acc.: 78.91%] [G loss: 1.269941]\n",
      "epoch:35 step:33481 [D loss: 0.447434, acc.: 80.47%] [G loss: 1.944010]\n",
      "epoch:35 step:33482 [D loss: 0.551274, acc.: 71.88%] [G loss: 1.586616]\n",
      "epoch:35 step:33483 [D loss: 0.519738, acc.: 76.56%] [G loss: 1.562980]\n",
      "epoch:35 step:33484 [D loss: 0.628384, acc.: 65.62%] [G loss: 1.556551]\n",
      "epoch:35 step:33485 [D loss: 0.587432, acc.: 62.50%] [G loss: 1.171897]\n",
      "epoch:35 step:33486 [D loss: 0.521913, acc.: 73.44%] [G loss: 2.045433]\n",
      "epoch:35 step:33487 [D loss: 0.456281, acc.: 81.25%] [G loss: 1.519649]\n",
      "epoch:35 step:33488 [D loss: 0.553960, acc.: 73.44%] [G loss: 0.741625]\n",
      "epoch:35 step:33489 [D loss: 0.738571, acc.: 53.91%] [G loss: 1.276421]\n",
      "epoch:35 step:33490 [D loss: 0.417046, acc.: 84.38%] [G loss: 1.333136]\n",
      "epoch:35 step:33491 [D loss: 0.553645, acc.: 75.00%] [G loss: 1.619024]\n",
      "epoch:35 step:33492 [D loss: 0.375771, acc.: 88.28%] [G loss: 2.171237]\n",
      "epoch:35 step:33493 [D loss: 0.598970, acc.: 70.31%] [G loss: 1.819397]\n",
      "epoch:35 step:33494 [D loss: 0.661073, acc.: 71.09%] [G loss: 1.164050]\n",
      "epoch:35 step:33495 [D loss: 0.740793, acc.: 51.56%] [G loss: 1.525405]\n",
      "epoch:35 step:33496 [D loss: 0.266888, acc.: 90.62%] [G loss: 1.825704]\n",
      "epoch:35 step:33497 [D loss: 0.366255, acc.: 86.72%] [G loss: 1.414470]\n",
      "epoch:35 step:33498 [D loss: 0.542578, acc.: 71.09%] [G loss: 1.420911]\n",
      "epoch:35 step:33499 [D loss: 0.604734, acc.: 65.62%] [G loss: 1.752855]\n",
      "epoch:35 step:33500 [D loss: 0.433737, acc.: 86.72%] [G loss: 1.372694]\n",
      "epoch:35 step:33501 [D loss: 0.737213, acc.: 58.59%] [G loss: 1.529838]\n",
      "epoch:35 step:33502 [D loss: 0.495797, acc.: 76.56%] [G loss: 1.142430]\n",
      "epoch:35 step:33503 [D loss: 0.447919, acc.: 77.34%] [G loss: 1.893751]\n",
      "epoch:35 step:33504 [D loss: 0.939453, acc.: 42.97%] [G loss: 1.316605]\n",
      "epoch:35 step:33505 [D loss: 0.356201, acc.: 85.94%] [G loss: 1.821993]\n",
      "epoch:35 step:33506 [D loss: 0.470656, acc.: 80.47%] [G loss: 1.383815]\n",
      "epoch:35 step:33507 [D loss: 0.644856, acc.: 60.94%] [G loss: 0.938687]\n",
      "epoch:35 step:33508 [D loss: 0.492110, acc.: 78.12%] [G loss: 1.452057]\n",
      "epoch:35 step:33509 [D loss: 0.493404, acc.: 77.34%] [G loss: 1.439044]\n",
      "epoch:35 step:33510 [D loss: 0.636130, acc.: 66.41%] [G loss: 1.289441]\n",
      "epoch:35 step:33511 [D loss: 0.409783, acc.: 80.47%] [G loss: 1.387020]\n",
      "epoch:35 step:33512 [D loss: 0.505286, acc.: 77.34%] [G loss: 1.513669]\n",
      "epoch:35 step:33513 [D loss: 0.337555, acc.: 89.84%] [G loss: 1.948141]\n",
      "epoch:35 step:33514 [D loss: 0.748598, acc.: 53.12%] [G loss: 1.316319]\n",
      "epoch:35 step:33515 [D loss: 0.478143, acc.: 78.12%] [G loss: 1.522346]\n",
      "epoch:35 step:33516 [D loss: 0.270105, acc.: 92.19%] [G loss: 1.469613]\n",
      "epoch:35 step:33517 [D loss: 0.635303, acc.: 63.28%] [G loss: 1.073331]\n",
      "epoch:35 step:33518 [D loss: 0.505985, acc.: 75.00%] [G loss: 1.415633]\n",
      "epoch:35 step:33519 [D loss: 0.491286, acc.: 75.00%] [G loss: 2.037976]\n",
      "epoch:35 step:33520 [D loss: 0.538086, acc.: 67.97%] [G loss: 2.023755]\n",
      "epoch:35 step:33521 [D loss: 0.557970, acc.: 67.97%] [G loss: 2.075861]\n",
      "epoch:35 step:33522 [D loss: 0.523702, acc.: 75.78%] [G loss: 1.414437]\n",
      "epoch:35 step:33523 [D loss: 0.725082, acc.: 59.38%] [G loss: 1.609876]\n",
      "epoch:35 step:33524 [D loss: 0.497673, acc.: 75.00%] [G loss: 1.399778]\n",
      "epoch:35 step:33525 [D loss: 0.434808, acc.: 79.69%] [G loss: 1.586241]\n",
      "epoch:35 step:33526 [D loss: 0.676848, acc.: 64.06%] [G loss: 1.173036]\n",
      "epoch:35 step:33527 [D loss: 0.443358, acc.: 82.03%] [G loss: 1.360831]\n",
      "epoch:35 step:33528 [D loss: 0.645582, acc.: 65.62%] [G loss: 1.062881]\n",
      "epoch:35 step:33529 [D loss: 0.379851, acc.: 81.25%] [G loss: 1.535245]\n",
      "epoch:35 step:33530 [D loss: 0.424209, acc.: 82.81%] [G loss: 1.719625]\n",
      "epoch:35 step:33531 [D loss: 0.399139, acc.: 83.59%] [G loss: 1.635717]\n",
      "epoch:35 step:33532 [D loss: 0.331478, acc.: 89.06%] [G loss: 1.303725]\n",
      "epoch:35 step:33533 [D loss: 0.524553, acc.: 78.12%] [G loss: 1.116268]\n",
      "epoch:35 step:33534 [D loss: 0.500496, acc.: 75.78%] [G loss: 1.095083]\n",
      "epoch:35 step:33535 [D loss: 0.536512, acc.: 71.09%] [G loss: 1.688580]\n",
      "epoch:35 step:33536 [D loss: 0.465675, acc.: 80.47%] [G loss: 1.639663]\n",
      "epoch:35 step:33537 [D loss: 0.511834, acc.: 76.56%] [G loss: 1.538252]\n",
      "epoch:35 step:33538 [D loss: 0.745641, acc.: 57.81%] [G loss: 1.460634]\n",
      "epoch:35 step:33539 [D loss: 0.560523, acc.: 71.88%] [G loss: 1.775007]\n",
      "epoch:35 step:33540 [D loss: 0.493248, acc.: 76.56%] [G loss: 1.482784]\n",
      "epoch:35 step:33541 [D loss: 0.649673, acc.: 62.50%] [G loss: 1.599867]\n",
      "epoch:35 step:33542 [D loss: 0.450691, acc.: 77.34%] [G loss: 1.880202]\n",
      "epoch:35 step:33543 [D loss: 0.539479, acc.: 71.88%] [G loss: 1.366166]\n",
      "epoch:35 step:33544 [D loss: 0.513693, acc.: 74.22%] [G loss: 1.679338]\n",
      "epoch:35 step:33545 [D loss: 0.613709, acc.: 64.06%] [G loss: 0.944370]\n",
      "epoch:35 step:33546 [D loss: 0.488081, acc.: 71.88%] [G loss: 1.281172]\n",
      "epoch:35 step:33547 [D loss: 0.504376, acc.: 78.12%] [G loss: 1.929474]\n",
      "epoch:35 step:33548 [D loss: 0.660288, acc.: 60.16%] [G loss: 1.832622]\n",
      "epoch:35 step:33549 [D loss: 0.378605, acc.: 85.94%] [G loss: 2.188673]\n",
      "epoch:35 step:33550 [D loss: 0.451394, acc.: 79.69%] [G loss: 1.230480]\n",
      "epoch:35 step:33551 [D loss: 0.439683, acc.: 80.47%] [G loss: 1.437717]\n",
      "epoch:35 step:33552 [D loss: 0.419134, acc.: 82.03%] [G loss: 1.521102]\n",
      "epoch:35 step:33553 [D loss: 0.639151, acc.: 66.41%] [G loss: 1.084326]\n",
      "epoch:35 step:33554 [D loss: 0.335007, acc.: 88.28%] [G loss: 1.860740]\n",
      "epoch:35 step:33555 [D loss: 0.429954, acc.: 85.16%] [G loss: 1.628174]\n",
      "epoch:35 step:33556 [D loss: 0.786857, acc.: 53.12%] [G loss: 1.439871]\n",
      "epoch:35 step:33557 [D loss: 0.673678, acc.: 63.28%] [G loss: 1.292042]\n",
      "epoch:35 step:33558 [D loss: 0.460628, acc.: 79.69%] [G loss: 1.597455]\n",
      "epoch:35 step:33559 [D loss: 0.418087, acc.: 82.81%] [G loss: 1.745564]\n",
      "epoch:35 step:33560 [D loss: 0.398412, acc.: 82.81%] [G loss: 1.437267]\n",
      "epoch:35 step:33561 [D loss: 0.551594, acc.: 71.09%] [G loss: 1.469874]\n",
      "epoch:35 step:33562 [D loss: 0.289795, acc.: 91.41%] [G loss: 1.648060]\n",
      "epoch:35 step:33563 [D loss: 0.603384, acc.: 65.62%] [G loss: 1.802060]\n",
      "epoch:35 step:33564 [D loss: 0.469107, acc.: 79.69%] [G loss: 1.291480]\n",
      "epoch:35 step:33565 [D loss: 0.519949, acc.: 78.12%] [G loss: 1.430299]\n",
      "epoch:35 step:33566 [D loss: 0.587249, acc.: 69.53%] [G loss: 1.127680]\n",
      "epoch:35 step:33567 [D loss: 0.563728, acc.: 69.53%] [G loss: 1.600612]\n",
      "epoch:35 step:33568 [D loss: 0.574993, acc.: 71.88%] [G loss: 1.382889]\n",
      "epoch:35 step:33569 [D loss: 0.584356, acc.: 69.53%] [G loss: 1.421855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33570 [D loss: 0.464929, acc.: 80.47%] [G loss: 1.538702]\n",
      "epoch:35 step:33571 [D loss: 0.342648, acc.: 89.84%] [G loss: 2.228560]\n",
      "epoch:35 step:33572 [D loss: 0.615264, acc.: 71.09%] [G loss: 1.023604]\n",
      "epoch:35 step:33573 [D loss: 0.592960, acc.: 71.09%] [G loss: 1.774825]\n",
      "epoch:35 step:33574 [D loss: 0.589544, acc.: 68.75%] [G loss: 0.885206]\n",
      "epoch:35 step:33575 [D loss: 0.452776, acc.: 78.12%] [G loss: 1.917412]\n",
      "epoch:35 step:33576 [D loss: 0.535209, acc.: 74.22%] [G loss: 1.652094]\n",
      "epoch:35 step:33577 [D loss: 0.650636, acc.: 63.28%] [G loss: 1.079171]\n",
      "epoch:35 step:33578 [D loss: 0.359171, acc.: 85.16%] [G loss: 2.037987]\n",
      "epoch:35 step:33579 [D loss: 0.493003, acc.: 75.00%] [G loss: 1.281855]\n",
      "epoch:35 step:33580 [D loss: 0.327859, acc.: 87.50%] [G loss: 1.783412]\n",
      "epoch:35 step:33581 [D loss: 0.703159, acc.: 57.03%] [G loss: 1.433726]\n",
      "epoch:35 step:33582 [D loss: 0.396867, acc.: 81.25%] [G loss: 1.294191]\n",
      "epoch:35 step:33583 [D loss: 0.463669, acc.: 78.12%] [G loss: 1.588369]\n",
      "epoch:35 step:33584 [D loss: 0.423004, acc.: 82.03%] [G loss: 1.359250]\n",
      "epoch:35 step:33585 [D loss: 0.660038, acc.: 61.72%] [G loss: 1.308337]\n",
      "epoch:35 step:33586 [D loss: 0.690316, acc.: 62.50%] [G loss: 1.046421]\n",
      "epoch:35 step:33587 [D loss: 0.346403, acc.: 87.50%] [G loss: 2.001847]\n",
      "epoch:35 step:33588 [D loss: 0.483535, acc.: 75.00%] [G loss: 1.626095]\n",
      "epoch:35 step:33589 [D loss: 0.410959, acc.: 82.03%] [G loss: 1.486110]\n",
      "epoch:35 step:33590 [D loss: 0.439267, acc.: 78.12%] [G loss: 1.315653]\n",
      "epoch:35 step:33591 [D loss: 0.465631, acc.: 77.34%] [G loss: 2.016445]\n",
      "epoch:35 step:33592 [D loss: 0.550508, acc.: 71.09%] [G loss: 1.189922]\n",
      "epoch:35 step:33593 [D loss: 0.770399, acc.: 60.94%] [G loss: 1.199694]\n",
      "epoch:35 step:33594 [D loss: 0.434808, acc.: 83.59%] [G loss: 1.296579]\n",
      "epoch:35 step:33595 [D loss: 0.517213, acc.: 72.66%] [G loss: 0.834885]\n",
      "epoch:35 step:33596 [D loss: 0.595448, acc.: 65.62%] [G loss: 1.401661]\n",
      "epoch:35 step:33597 [D loss: 0.463743, acc.: 82.03%] [G loss: 1.041278]\n",
      "epoch:35 step:33598 [D loss: 0.548726, acc.: 73.44%] [G loss: 1.053902]\n",
      "epoch:35 step:33599 [D loss: 0.478880, acc.: 80.47%] [G loss: 1.825783]\n",
      "epoch:35 step:33600 [D loss: 0.476014, acc.: 78.91%] [G loss: 1.458507]\n",
      "##############\n",
      "[2.63822882 1.92266342 1.68132529 2.83039082 0.78218066 6.53994377\n",
      " 2.07849321 2.79256495 3.90873979 8.14868929]\n",
      "##########\n",
      "epoch:35 step:33601 [D loss: 0.539801, acc.: 73.44%] [G loss: 1.392839]\n",
      "epoch:35 step:33602 [D loss: 0.420623, acc.: 84.38%] [G loss: 1.843156]\n",
      "epoch:35 step:33603 [D loss: 0.340272, acc.: 88.28%] [G loss: 1.719846]\n",
      "epoch:35 step:33604 [D loss: 0.405694, acc.: 84.38%] [G loss: 1.265239]\n",
      "epoch:35 step:33605 [D loss: 0.465008, acc.: 77.34%] [G loss: 1.882267]\n",
      "epoch:35 step:33606 [D loss: 0.589226, acc.: 71.88%] [G loss: 2.480011]\n",
      "epoch:35 step:33607 [D loss: 0.640443, acc.: 63.28%] [G loss: 1.511877]\n",
      "epoch:35 step:33608 [D loss: 0.441417, acc.: 79.69%] [G loss: 1.722061]\n",
      "epoch:35 step:33609 [D loss: 0.344941, acc.: 86.72%] [G loss: 1.591381]\n",
      "epoch:35 step:33610 [D loss: 0.433140, acc.: 80.47%] [G loss: 1.289769]\n",
      "epoch:35 step:33611 [D loss: 0.510874, acc.: 76.56%] [G loss: 1.435688]\n",
      "epoch:35 step:33612 [D loss: 0.360629, acc.: 85.94%] [G loss: 2.011580]\n",
      "epoch:35 step:33613 [D loss: 0.422560, acc.: 82.03%] [G loss: 1.474096]\n",
      "epoch:35 step:33614 [D loss: 0.601284, acc.: 67.97%] [G loss: 1.258261]\n",
      "epoch:35 step:33615 [D loss: 0.626473, acc.: 66.41%] [G loss: 1.686221]\n",
      "epoch:35 step:33616 [D loss: 0.653799, acc.: 57.03%] [G loss: 2.041464]\n",
      "epoch:35 step:33617 [D loss: 0.518230, acc.: 78.91%] [G loss: 1.717215]\n",
      "epoch:35 step:33618 [D loss: 0.460381, acc.: 80.47%] [G loss: 2.032992]\n",
      "epoch:35 step:33619 [D loss: 0.528009, acc.: 74.22%] [G loss: 1.304379]\n",
      "epoch:35 step:33620 [D loss: 0.403990, acc.: 83.59%] [G loss: 1.697130]\n",
      "epoch:35 step:33621 [D loss: 0.415238, acc.: 84.38%] [G loss: 1.820121]\n",
      "epoch:35 step:33622 [D loss: 0.338849, acc.: 88.28%] [G loss: 1.884711]\n",
      "epoch:35 step:33623 [D loss: 0.854132, acc.: 44.53%] [G loss: 1.205704]\n",
      "epoch:35 step:33624 [D loss: 0.533689, acc.: 67.97%] [G loss: 1.681583]\n",
      "epoch:35 step:33625 [D loss: 0.523326, acc.: 72.66%] [G loss: 1.763361]\n",
      "epoch:35 step:33626 [D loss: 0.688627, acc.: 58.59%] [G loss: 1.737717]\n",
      "epoch:35 step:33627 [D loss: 0.840925, acc.: 48.44%] [G loss: 1.017846]\n",
      "epoch:35 step:33628 [D loss: 0.672620, acc.: 58.59%] [G loss: 1.459501]\n",
      "epoch:35 step:33629 [D loss: 0.434687, acc.: 80.47%] [G loss: 1.779015]\n",
      "epoch:35 step:33630 [D loss: 0.642942, acc.: 62.50%] [G loss: 1.261847]\n",
      "epoch:35 step:33631 [D loss: 0.694810, acc.: 59.38%] [G loss: 1.716438]\n",
      "epoch:35 step:33632 [D loss: 0.665170, acc.: 64.84%] [G loss: 0.708386]\n",
      "epoch:35 step:33633 [D loss: 0.561190, acc.: 71.88%] [G loss: 1.280082]\n",
      "epoch:35 step:33634 [D loss: 0.513365, acc.: 75.78%] [G loss: 1.549888]\n",
      "epoch:35 step:33635 [D loss: 0.541868, acc.: 71.09%] [G loss: 1.679030]\n",
      "epoch:35 step:33636 [D loss: 0.465013, acc.: 78.91%] [G loss: 1.462304]\n",
      "epoch:35 step:33637 [D loss: 0.455827, acc.: 79.69%] [G loss: 1.333763]\n",
      "epoch:35 step:33638 [D loss: 0.526291, acc.: 71.09%] [G loss: 1.180510]\n",
      "epoch:35 step:33639 [D loss: 0.693546, acc.: 57.03%] [G loss: 1.606035]\n",
      "epoch:35 step:33640 [D loss: 0.529669, acc.: 78.91%] [G loss: 1.561124]\n",
      "epoch:35 step:33641 [D loss: 0.554899, acc.: 71.88%] [G loss: 1.607535]\n",
      "epoch:35 step:33642 [D loss: 0.441426, acc.: 78.12%] [G loss: 1.852074]\n",
      "epoch:35 step:33643 [D loss: 0.684789, acc.: 54.69%] [G loss: 1.281037]\n",
      "epoch:35 step:33644 [D loss: 0.350128, acc.: 89.06%] [G loss: 1.763402]\n",
      "epoch:35 step:33645 [D loss: 0.448913, acc.: 79.69%] [G loss: 1.659870]\n",
      "epoch:35 step:33646 [D loss: 0.526303, acc.: 72.66%] [G loss: 1.346053]\n",
      "epoch:35 step:33647 [D loss: 0.421068, acc.: 83.59%] [G loss: 1.548622]\n",
      "epoch:35 step:33648 [D loss: 0.556980, acc.: 68.75%] [G loss: 1.719181]\n",
      "epoch:35 step:33649 [D loss: 0.568316, acc.: 71.88%] [G loss: 1.340715]\n",
      "epoch:35 step:33650 [D loss: 0.500555, acc.: 76.56%] [G loss: 1.571682]\n",
      "epoch:35 step:33651 [D loss: 0.414020, acc.: 82.03%] [G loss: 1.574587]\n",
      "epoch:35 step:33652 [D loss: 0.450896, acc.: 83.59%] [G loss: 1.321713]\n",
      "epoch:35 step:33653 [D loss: 0.297135, acc.: 87.50%] [G loss: 1.926965]\n",
      "epoch:35 step:33654 [D loss: 0.482015, acc.: 78.12%] [G loss: 1.481642]\n",
      "epoch:35 step:33655 [D loss: 0.597556, acc.: 69.53%] [G loss: 1.724058]\n",
      "epoch:35 step:33656 [D loss: 0.377310, acc.: 82.81%] [G loss: 1.336887]\n",
      "epoch:35 step:33657 [D loss: 0.546028, acc.: 74.22%] [G loss: 1.424064]\n",
      "epoch:35 step:33658 [D loss: 0.592890, acc.: 68.75%] [G loss: 1.635305]\n",
      "epoch:35 step:33659 [D loss: 0.490371, acc.: 77.34%] [G loss: 1.930939]\n",
      "epoch:35 step:33660 [D loss: 0.441729, acc.: 78.91%] [G loss: 1.696383]\n",
      "epoch:35 step:33661 [D loss: 0.479018, acc.: 79.69%] [G loss: 1.867217]\n",
      "epoch:35 step:33662 [D loss: 0.646215, acc.: 67.97%] [G loss: 1.131287]\n",
      "epoch:35 step:33663 [D loss: 0.830753, acc.: 50.00%] [G loss: 1.266994]\n",
      "epoch:35 step:33664 [D loss: 0.409896, acc.: 85.16%] [G loss: 1.672323]\n",
      "epoch:35 step:33665 [D loss: 0.314889, acc.: 89.84%] [G loss: 2.289133]\n",
      "epoch:35 step:33666 [D loss: 0.498036, acc.: 76.56%] [G loss: 1.027535]\n",
      "epoch:35 step:33667 [D loss: 0.336464, acc.: 89.84%] [G loss: 1.378220]\n",
      "epoch:35 step:33668 [D loss: 0.597737, acc.: 67.19%] [G loss: 1.799062]\n",
      "epoch:35 step:33669 [D loss: 0.324591, acc.: 91.41%] [G loss: 1.671951]\n",
      "epoch:35 step:33670 [D loss: 0.551144, acc.: 68.75%] [G loss: 2.319609]\n",
      "epoch:35 step:33671 [D loss: 0.566460, acc.: 72.66%] [G loss: 1.726060]\n",
      "epoch:35 step:33672 [D loss: 0.354002, acc.: 86.72%] [G loss: 1.850128]\n",
      "epoch:35 step:33673 [D loss: 0.506790, acc.: 76.56%] [G loss: 1.631548]\n",
      "epoch:35 step:33674 [D loss: 0.657577, acc.: 60.16%] [G loss: 1.628622]\n",
      "epoch:35 step:33675 [D loss: 0.554086, acc.: 75.00%] [G loss: 1.416515]\n",
      "epoch:35 step:33676 [D loss: 0.524339, acc.: 75.00%] [G loss: 1.909731]\n",
      "epoch:35 step:33677 [D loss: 0.612272, acc.: 67.97%] [G loss: 1.481503]\n",
      "epoch:35 step:33678 [D loss: 0.476872, acc.: 78.12%] [G loss: 1.809415]\n",
      "epoch:35 step:33679 [D loss: 0.671874, acc.: 62.50%] [G loss: 1.842518]\n",
      "epoch:35 step:33680 [D loss: 0.503283, acc.: 71.09%] [G loss: 1.745965]\n",
      "epoch:35 step:33681 [D loss: 0.370652, acc.: 86.72%] [G loss: 1.568217]\n",
      "epoch:35 step:33682 [D loss: 0.401793, acc.: 83.59%] [G loss: 1.722930]\n",
      "epoch:35 step:33683 [D loss: 0.644080, acc.: 65.62%] [G loss: 1.372239]\n",
      "epoch:35 step:33684 [D loss: 0.645517, acc.: 66.41%] [G loss: 1.568972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33685 [D loss: 0.709199, acc.: 60.94%] [G loss: 1.220516]\n",
      "epoch:35 step:33686 [D loss: 0.354857, acc.: 89.06%] [G loss: 2.005744]\n",
      "epoch:35 step:33687 [D loss: 0.600997, acc.: 67.19%] [G loss: 1.622234]\n",
      "epoch:35 step:33688 [D loss: 0.455826, acc.: 78.91%] [G loss: 2.047715]\n",
      "epoch:35 step:33689 [D loss: 0.385299, acc.: 82.03%] [G loss: 1.492501]\n",
      "epoch:35 step:33690 [D loss: 0.341813, acc.: 88.28%] [G loss: 1.667878]\n",
      "epoch:35 step:33691 [D loss: 0.480894, acc.: 80.47%] [G loss: 1.985505]\n",
      "epoch:35 step:33692 [D loss: 0.352876, acc.: 88.28%] [G loss: 1.541993]\n",
      "epoch:35 step:33693 [D loss: 0.296292, acc.: 89.84%] [G loss: 1.636834]\n",
      "epoch:35 step:33694 [D loss: 0.740481, acc.: 57.81%] [G loss: 1.485903]\n",
      "epoch:35 step:33695 [D loss: 0.469416, acc.: 75.00%] [G loss: 1.738373]\n",
      "epoch:35 step:33696 [D loss: 0.785229, acc.: 57.03%] [G loss: 1.470036]\n",
      "epoch:35 step:33697 [D loss: 0.704948, acc.: 57.81%] [G loss: 1.822581]\n",
      "epoch:35 step:33698 [D loss: 0.577830, acc.: 71.88%] [G loss: 1.797104]\n",
      "epoch:35 step:33699 [D loss: 0.353183, acc.: 85.16%] [G loss: 1.552121]\n",
      "epoch:35 step:33700 [D loss: 0.659918, acc.: 63.28%] [G loss: 1.701906]\n",
      "epoch:35 step:33701 [D loss: 0.568707, acc.: 67.97%] [G loss: 1.440805]\n",
      "epoch:35 step:33702 [D loss: 0.483909, acc.: 77.34%] [G loss: 1.536540]\n",
      "epoch:35 step:33703 [D loss: 0.496692, acc.: 73.44%] [G loss: 1.455963]\n",
      "epoch:35 step:33704 [D loss: 0.564910, acc.: 74.22%] [G loss: 1.260776]\n",
      "epoch:35 step:33705 [D loss: 0.608345, acc.: 65.62%] [G loss: 1.149710]\n",
      "epoch:35 step:33706 [D loss: 0.526730, acc.: 72.66%] [G loss: 1.827645]\n",
      "epoch:35 step:33707 [D loss: 0.626782, acc.: 65.62%] [G loss: 1.343215]\n",
      "epoch:35 step:33708 [D loss: 0.853921, acc.: 48.44%] [G loss: 1.610425]\n",
      "epoch:35 step:33709 [D loss: 0.360483, acc.: 88.28%] [G loss: 1.931670]\n",
      "epoch:35 step:33710 [D loss: 0.611806, acc.: 71.88%] [G loss: 1.234790]\n",
      "epoch:35 step:33711 [D loss: 0.519103, acc.: 73.44%] [G loss: 1.554863]\n",
      "epoch:35 step:33712 [D loss: 0.432460, acc.: 75.78%] [G loss: 1.495422]\n",
      "epoch:35 step:33713 [D loss: 0.754316, acc.: 57.81%] [G loss: 1.430442]\n",
      "epoch:35 step:33714 [D loss: 0.572637, acc.: 71.09%] [G loss: 1.704385]\n",
      "epoch:35 step:33715 [D loss: 0.506520, acc.: 74.22%] [G loss: 1.959124]\n",
      "epoch:35 step:33716 [D loss: 0.595102, acc.: 72.66%] [G loss: 1.757945]\n",
      "epoch:35 step:33717 [D loss: 0.592460, acc.: 65.62%] [G loss: 1.586265]\n",
      "epoch:35 step:33718 [D loss: 0.619949, acc.: 66.41%] [G loss: 2.097509]\n",
      "epoch:35 step:33719 [D loss: 0.658394, acc.: 65.62%] [G loss: 1.542299]\n",
      "epoch:35 step:33720 [D loss: 0.478118, acc.: 76.56%] [G loss: 2.021137]\n",
      "epoch:35 step:33721 [D loss: 0.625564, acc.: 63.28%] [G loss: 1.932463]\n",
      "epoch:35 step:33722 [D loss: 0.453913, acc.: 82.03%] [G loss: 1.279694]\n",
      "epoch:35 step:33723 [D loss: 0.700706, acc.: 61.72%] [G loss: 1.297328]\n",
      "epoch:35 step:33724 [D loss: 0.522773, acc.: 75.00%] [G loss: 2.145194]\n",
      "epoch:35 step:33725 [D loss: 0.591413, acc.: 67.19%] [G loss: 1.675248]\n",
      "epoch:35 step:33726 [D loss: 0.470472, acc.: 81.25%] [G loss: 1.681466]\n",
      "epoch:35 step:33727 [D loss: 0.411581, acc.: 80.47%] [G loss: 1.981087]\n",
      "epoch:35 step:33728 [D loss: 0.485057, acc.: 76.56%] [G loss: 0.921323]\n",
      "epoch:35 step:33729 [D loss: 0.452466, acc.: 78.91%] [G loss: 1.904815]\n",
      "epoch:35 step:33730 [D loss: 0.517353, acc.: 78.12%] [G loss: 2.029371]\n",
      "epoch:35 step:33731 [D loss: 0.548028, acc.: 72.66%] [G loss: 1.457325]\n",
      "epoch:35 step:33732 [D loss: 0.525463, acc.: 76.56%] [G loss: 1.920008]\n",
      "epoch:36 step:33733 [D loss: 0.811310, acc.: 55.47%] [G loss: 1.482783]\n",
      "epoch:36 step:33734 [D loss: 0.458760, acc.: 75.78%] [G loss: 1.267444]\n",
      "epoch:36 step:33735 [D loss: 0.555525, acc.: 73.44%] [G loss: 1.045485]\n",
      "epoch:36 step:33736 [D loss: 0.521226, acc.: 74.22%] [G loss: 1.509682]\n",
      "epoch:36 step:33737 [D loss: 0.608100, acc.: 65.62%] [G loss: 1.135720]\n",
      "epoch:36 step:33738 [D loss: 0.591322, acc.: 67.97%] [G loss: 1.331062]\n",
      "epoch:36 step:33739 [D loss: 0.677236, acc.: 56.25%] [G loss: 1.042092]\n",
      "epoch:36 step:33740 [D loss: 0.477341, acc.: 76.56%] [G loss: 1.421382]\n",
      "epoch:36 step:33741 [D loss: 0.556965, acc.: 67.97%] [G loss: 1.271923]\n",
      "epoch:36 step:33742 [D loss: 0.657675, acc.: 65.62%] [G loss: 1.098776]\n",
      "epoch:36 step:33743 [D loss: 0.531050, acc.: 74.22%] [G loss: 1.389694]\n",
      "epoch:36 step:33744 [D loss: 0.638390, acc.: 64.06%] [G loss: 1.620681]\n",
      "epoch:36 step:33745 [D loss: 0.519206, acc.: 74.22%] [G loss: 1.071372]\n",
      "epoch:36 step:33746 [D loss: 0.467164, acc.: 75.00%] [G loss: 1.498077]\n",
      "epoch:36 step:33747 [D loss: 0.431987, acc.: 85.16%] [G loss: 1.769372]\n",
      "epoch:36 step:33748 [D loss: 0.402665, acc.: 82.03%] [G loss: 1.455205]\n",
      "epoch:36 step:33749 [D loss: 0.562208, acc.: 73.44%] [G loss: 1.535368]\n",
      "epoch:36 step:33750 [D loss: 0.420216, acc.: 79.69%] [G loss: 1.693126]\n",
      "epoch:36 step:33751 [D loss: 0.611796, acc.: 71.09%] [G loss: 1.537822]\n",
      "epoch:36 step:33752 [D loss: 0.672595, acc.: 61.72%] [G loss: 1.328292]\n",
      "epoch:36 step:33753 [D loss: 0.753581, acc.: 55.47%] [G loss: 1.582554]\n",
      "epoch:36 step:33754 [D loss: 0.444348, acc.: 83.59%] [G loss: 1.380719]\n",
      "epoch:36 step:33755 [D loss: 0.610228, acc.: 67.97%] [G loss: 1.520877]\n",
      "epoch:36 step:33756 [D loss: 0.489649, acc.: 82.03%] [G loss: 1.708817]\n",
      "epoch:36 step:33757 [D loss: 0.477336, acc.: 75.78%] [G loss: 1.016681]\n",
      "epoch:36 step:33758 [D loss: 0.517714, acc.: 77.34%] [G loss: 0.948105]\n",
      "epoch:36 step:33759 [D loss: 0.622388, acc.: 64.06%] [G loss: 1.613464]\n",
      "epoch:36 step:33760 [D loss: 0.450939, acc.: 84.38%] [G loss: 1.829557]\n",
      "epoch:36 step:33761 [D loss: 0.473352, acc.: 79.69%] [G loss: 1.228534]\n",
      "epoch:36 step:33762 [D loss: 0.616707, acc.: 67.97%] [G loss: 1.410686]\n",
      "epoch:36 step:33763 [D loss: 0.682884, acc.: 61.72%] [G loss: 1.850876]\n",
      "epoch:36 step:33764 [D loss: 0.542237, acc.: 71.88%] [G loss: 1.934220]\n",
      "epoch:36 step:33765 [D loss: 0.413078, acc.: 82.03%] [G loss: 1.674068]\n",
      "epoch:36 step:33766 [D loss: 0.444681, acc.: 82.03%] [G loss: 1.050180]\n",
      "epoch:36 step:33767 [D loss: 0.538201, acc.: 73.44%] [G loss: 1.574430]\n",
      "epoch:36 step:33768 [D loss: 0.484211, acc.: 78.91%] [G loss: 1.717810]\n",
      "epoch:36 step:33769 [D loss: 0.529392, acc.: 72.66%] [G loss: 1.465503]\n",
      "epoch:36 step:33770 [D loss: 0.369700, acc.: 87.50%] [G loss: 1.620922]\n",
      "epoch:36 step:33771 [D loss: 0.502453, acc.: 76.56%] [G loss: 1.558094]\n",
      "epoch:36 step:33772 [D loss: 0.463210, acc.: 81.25%] [G loss: 1.318026]\n",
      "epoch:36 step:33773 [D loss: 0.575644, acc.: 69.53%] [G loss: 1.396432]\n",
      "epoch:36 step:33774 [D loss: 0.542599, acc.: 77.34%] [G loss: 0.897686]\n",
      "epoch:36 step:33775 [D loss: 0.466899, acc.: 82.03%] [G loss: 1.328809]\n",
      "epoch:36 step:33776 [D loss: 0.487604, acc.: 75.00%] [G loss: 1.457259]\n",
      "epoch:36 step:33777 [D loss: 0.478993, acc.: 73.44%] [G loss: 1.917181]\n",
      "epoch:36 step:33778 [D loss: 0.462523, acc.: 79.69%] [G loss: 1.762384]\n",
      "epoch:36 step:33779 [D loss: 0.412360, acc.: 83.59%] [G loss: 1.990341]\n",
      "epoch:36 step:33780 [D loss: 0.548931, acc.: 72.66%] [G loss: 1.093170]\n",
      "epoch:36 step:33781 [D loss: 0.497807, acc.: 71.09%] [G loss: 1.524174]\n",
      "epoch:36 step:33782 [D loss: 0.481933, acc.: 78.91%] [G loss: 2.025529]\n",
      "epoch:36 step:33783 [D loss: 0.352685, acc.: 86.72%] [G loss: 1.230541]\n",
      "epoch:36 step:33784 [D loss: 0.706885, acc.: 58.59%] [G loss: 1.351577]\n",
      "epoch:36 step:33785 [D loss: 0.648329, acc.: 64.06%] [G loss: 1.868466]\n",
      "epoch:36 step:33786 [D loss: 0.464565, acc.: 82.03%] [G loss: 1.444217]\n",
      "epoch:36 step:33787 [D loss: 0.919483, acc.: 45.31%] [G loss: 1.146343]\n",
      "epoch:36 step:33788 [D loss: 0.493044, acc.: 75.78%] [G loss: 1.362296]\n",
      "epoch:36 step:33789 [D loss: 0.520702, acc.: 74.22%] [G loss: 1.182530]\n",
      "epoch:36 step:33790 [D loss: 0.479859, acc.: 78.91%] [G loss: 2.038543]\n",
      "epoch:36 step:33791 [D loss: 0.486458, acc.: 77.34%] [G loss: 1.904283]\n",
      "epoch:36 step:33792 [D loss: 0.656493, acc.: 66.41%] [G loss: 1.382523]\n",
      "epoch:36 step:33793 [D loss: 0.650105, acc.: 62.50%] [G loss: 1.510304]\n",
      "epoch:36 step:33794 [D loss: 0.461952, acc.: 84.38%] [G loss: 1.298344]\n",
      "epoch:36 step:33795 [D loss: 0.449598, acc.: 82.03%] [G loss: 1.432598]\n",
      "epoch:36 step:33796 [D loss: 0.813661, acc.: 48.44%] [G loss: 1.525864]\n",
      "epoch:36 step:33797 [D loss: 0.467456, acc.: 78.91%] [G loss: 1.775751]\n",
      "epoch:36 step:33798 [D loss: 0.581499, acc.: 70.31%] [G loss: 1.604855]\n",
      "epoch:36 step:33799 [D loss: 0.649089, acc.: 62.50%] [G loss: 1.870937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:33800 [D loss: 0.575265, acc.: 72.66%] [G loss: 1.777253]\n",
      "##############\n",
      "[2.64101042 2.0945513  1.80437209 2.62691101 0.87422019 5.99746823\n",
      " 2.26096742 2.5742479  3.90744024 5.50566651]\n",
      "##########\n",
      "epoch:36 step:33801 [D loss: 0.408192, acc.: 80.47%] [G loss: 1.792726]\n",
      "epoch:36 step:33802 [D loss: 0.490118, acc.: 75.00%] [G loss: 1.862059]\n",
      "epoch:36 step:33803 [D loss: 0.711175, acc.: 60.16%] [G loss: 1.679643]\n",
      "epoch:36 step:33804 [D loss: 0.574781, acc.: 69.53%] [G loss: 1.355443]\n",
      "epoch:36 step:33805 [D loss: 0.499837, acc.: 75.78%] [G loss: 1.922328]\n",
      "epoch:36 step:33806 [D loss: 0.492005, acc.: 78.12%] [G loss: 1.631375]\n",
      "epoch:36 step:33807 [D loss: 0.602483, acc.: 65.62%] [G loss: 1.273308]\n",
      "epoch:36 step:33808 [D loss: 0.390552, acc.: 87.50%] [G loss: 1.424489]\n",
      "epoch:36 step:33809 [D loss: 0.455180, acc.: 78.91%] [G loss: 1.530820]\n",
      "epoch:36 step:33810 [D loss: 0.535807, acc.: 77.34%] [G loss: 1.259721]\n",
      "epoch:36 step:33811 [D loss: 0.570071, acc.: 71.88%] [G loss: 1.246100]\n",
      "epoch:36 step:33812 [D loss: 0.432747, acc.: 81.25%] [G loss: 1.310459]\n",
      "epoch:36 step:33813 [D loss: 0.574380, acc.: 72.66%] [G loss: 1.616185]\n",
      "epoch:36 step:33814 [D loss: 0.466671, acc.: 85.94%] [G loss: 1.476729]\n",
      "epoch:36 step:33815 [D loss: 0.559881, acc.: 72.66%] [G loss: 1.332332]\n",
      "epoch:36 step:33816 [D loss: 0.692410, acc.: 63.28%] [G loss: 1.304421]\n",
      "epoch:36 step:33817 [D loss: 0.528370, acc.: 75.78%] [G loss: 1.461994]\n",
      "epoch:36 step:33818 [D loss: 0.606478, acc.: 66.41%] [G loss: 1.527580]\n",
      "epoch:36 step:33819 [D loss: 0.699915, acc.: 54.69%] [G loss: 1.502285]\n",
      "epoch:36 step:33820 [D loss: 0.535248, acc.: 71.88%] [G loss: 1.530808]\n",
      "epoch:36 step:33821 [D loss: 0.433720, acc.: 82.81%] [G loss: 1.532161]\n",
      "epoch:36 step:33822 [D loss: 0.460327, acc.: 81.25%] [G loss: 1.011367]\n",
      "epoch:36 step:33823 [D loss: 0.399007, acc.: 83.59%] [G loss: 1.264577]\n",
      "epoch:36 step:33824 [D loss: 0.487881, acc.: 75.78%] [G loss: 1.671878]\n",
      "epoch:36 step:33825 [D loss: 0.507765, acc.: 74.22%] [G loss: 1.185420]\n",
      "epoch:36 step:33826 [D loss: 0.435043, acc.: 80.47%] [G loss: 1.593853]\n",
      "epoch:36 step:33827 [D loss: 0.453313, acc.: 77.34%] [G loss: 1.212904]\n",
      "epoch:36 step:33828 [D loss: 0.458457, acc.: 81.25%] [G loss: 1.299154]\n",
      "epoch:36 step:33829 [D loss: 0.619466, acc.: 68.75%] [G loss: 1.175681]\n",
      "epoch:36 step:33830 [D loss: 0.642338, acc.: 64.06%] [G loss: 1.402960]\n",
      "epoch:36 step:33831 [D loss: 0.571452, acc.: 71.09%] [G loss: 2.097942]\n",
      "epoch:36 step:33832 [D loss: 0.844727, acc.: 48.44%] [G loss: 1.368751]\n",
      "epoch:36 step:33833 [D loss: 0.387581, acc.: 83.59%] [G loss: 2.543518]\n",
      "epoch:36 step:33834 [D loss: 0.569944, acc.: 69.53%] [G loss: 1.717818]\n",
      "epoch:36 step:33835 [D loss: 0.429662, acc.: 79.69%] [G loss: 1.412419]\n",
      "epoch:36 step:33836 [D loss: 0.377224, acc.: 86.72%] [G loss: 2.208746]\n",
      "epoch:36 step:33837 [D loss: 0.311193, acc.: 89.84%] [G loss: 1.820681]\n",
      "epoch:36 step:33838 [D loss: 0.505634, acc.: 76.56%] [G loss: 1.244449]\n",
      "epoch:36 step:33839 [D loss: 0.795091, acc.: 50.78%] [G loss: 1.338492]\n",
      "epoch:36 step:33840 [D loss: 0.588085, acc.: 71.09%] [G loss: 1.686063]\n",
      "epoch:36 step:33841 [D loss: 0.468726, acc.: 78.91%] [G loss: 1.753884]\n",
      "epoch:36 step:33842 [D loss: 0.605036, acc.: 71.09%] [G loss: 1.753135]\n",
      "epoch:36 step:33843 [D loss: 0.626186, acc.: 65.62%] [G loss: 1.456179]\n",
      "epoch:36 step:33844 [D loss: 0.555048, acc.: 69.53%] [G loss: 1.811839]\n",
      "epoch:36 step:33845 [D loss: 0.345150, acc.: 86.72%] [G loss: 1.712416]\n",
      "epoch:36 step:33846 [D loss: 0.508080, acc.: 75.00%] [G loss: 1.239841]\n",
      "epoch:36 step:33847 [D loss: 0.220978, acc.: 95.31%] [G loss: 2.033792]\n",
      "epoch:36 step:33848 [D loss: 0.406070, acc.: 85.94%] [G loss: 1.845763]\n",
      "epoch:36 step:33849 [D loss: 0.496263, acc.: 78.91%] [G loss: 0.965632]\n",
      "epoch:36 step:33850 [D loss: 0.536652, acc.: 67.97%] [G loss: 1.480475]\n",
      "epoch:36 step:33851 [D loss: 0.347266, acc.: 84.38%] [G loss: 1.740527]\n",
      "epoch:36 step:33852 [D loss: 0.624001, acc.: 69.53%] [G loss: 2.062141]\n",
      "epoch:36 step:33853 [D loss: 0.594041, acc.: 66.41%] [G loss: 1.639793]\n",
      "epoch:36 step:33854 [D loss: 0.599057, acc.: 68.75%] [G loss: 1.870483]\n",
      "epoch:36 step:33855 [D loss: 0.348528, acc.: 84.38%] [G loss: 1.389852]\n",
      "epoch:36 step:33856 [D loss: 0.428423, acc.: 82.81%] [G loss: 1.429366]\n",
      "epoch:36 step:33857 [D loss: 0.497419, acc.: 78.91%] [G loss: 0.990099]\n",
      "epoch:36 step:33858 [D loss: 0.685182, acc.: 63.28%] [G loss: 1.401381]\n",
      "epoch:36 step:33859 [D loss: 0.465147, acc.: 77.34%] [G loss: 2.030375]\n",
      "epoch:36 step:33860 [D loss: 0.522626, acc.: 69.53%] [G loss: 1.720575]\n",
      "epoch:36 step:33861 [D loss: 0.500363, acc.: 78.91%] [G loss: 1.643905]\n",
      "epoch:36 step:33862 [D loss: 0.496933, acc.: 77.34%] [G loss: 1.209530]\n",
      "epoch:36 step:33863 [D loss: 0.449049, acc.: 79.69%] [G loss: 1.642548]\n",
      "epoch:36 step:33864 [D loss: 0.644311, acc.: 63.28%] [G loss: 1.290814]\n",
      "epoch:36 step:33865 [D loss: 0.623105, acc.: 66.41%] [G loss: 1.393037]\n",
      "epoch:36 step:33866 [D loss: 0.629590, acc.: 61.72%] [G loss: 1.419408]\n",
      "epoch:36 step:33867 [D loss: 0.381321, acc.: 85.94%] [G loss: 1.980774]\n",
      "epoch:36 step:33868 [D loss: 0.758377, acc.: 56.25%] [G loss: 1.828122]\n",
      "epoch:36 step:33869 [D loss: 0.486330, acc.: 77.34%] [G loss: 1.509667]\n",
      "epoch:36 step:33870 [D loss: 0.414616, acc.: 82.81%] [G loss: 1.341249]\n",
      "epoch:36 step:33871 [D loss: 0.579959, acc.: 75.00%] [G loss: 1.509012]\n",
      "epoch:36 step:33872 [D loss: 0.623051, acc.: 67.19%] [G loss: 1.515118]\n",
      "epoch:36 step:33873 [D loss: 0.985855, acc.: 42.97%] [G loss: 1.428382]\n",
      "epoch:36 step:33874 [D loss: 0.468248, acc.: 80.47%] [G loss: 1.459650]\n",
      "epoch:36 step:33875 [D loss: 0.510749, acc.: 76.56%] [G loss: 2.025327]\n",
      "epoch:36 step:33876 [D loss: 0.565151, acc.: 69.53%] [G loss: 1.504597]\n",
      "epoch:36 step:33877 [D loss: 0.316303, acc.: 89.84%] [G loss: 1.448868]\n",
      "epoch:36 step:33878 [D loss: 0.643109, acc.: 64.84%] [G loss: 1.837462]\n",
      "epoch:36 step:33879 [D loss: 0.449968, acc.: 83.59%] [G loss: 1.466879]\n",
      "epoch:36 step:33880 [D loss: 0.460511, acc.: 78.12%] [G loss: 1.954479]\n",
      "epoch:36 step:33881 [D loss: 0.428275, acc.: 78.91%] [G loss: 1.419726]\n",
      "epoch:36 step:33882 [D loss: 0.569191, acc.: 71.09%] [G loss: 1.221007]\n",
      "epoch:36 step:33883 [D loss: 0.420100, acc.: 83.59%] [G loss: 1.159818]\n",
      "epoch:36 step:33884 [D loss: 0.580049, acc.: 71.09%] [G loss: 1.420262]\n",
      "epoch:36 step:33885 [D loss: 0.471646, acc.: 78.12%] [G loss: 1.503963]\n",
      "epoch:36 step:33886 [D loss: 0.428156, acc.: 81.25%] [G loss: 1.803250]\n",
      "epoch:36 step:33887 [D loss: 0.568871, acc.: 70.31%] [G loss: 1.392072]\n",
      "epoch:36 step:33888 [D loss: 0.455667, acc.: 80.47%] [G loss: 1.451157]\n",
      "epoch:36 step:33889 [D loss: 0.581912, acc.: 65.62%] [G loss: 1.038596]\n",
      "epoch:36 step:33890 [D loss: 0.626346, acc.: 64.06%] [G loss: 1.636482]\n",
      "epoch:36 step:33891 [D loss: 0.375127, acc.: 84.38%] [G loss: 1.095594]\n",
      "epoch:36 step:33892 [D loss: 0.484940, acc.: 78.91%] [G loss: 1.604017]\n",
      "epoch:36 step:33893 [D loss: 0.314744, acc.: 89.84%] [G loss: 1.945689]\n",
      "epoch:36 step:33894 [D loss: 0.479714, acc.: 77.34%] [G loss: 1.457567]\n",
      "epoch:36 step:33895 [D loss: 0.482241, acc.: 80.47%] [G loss: 1.864828]\n",
      "epoch:36 step:33896 [D loss: 0.523754, acc.: 69.53%] [G loss: 1.424152]\n",
      "epoch:36 step:33897 [D loss: 0.579621, acc.: 69.53%] [G loss: 1.538359]\n",
      "epoch:36 step:33898 [D loss: 0.410154, acc.: 78.12%] [G loss: 1.301014]\n",
      "epoch:36 step:33899 [D loss: 0.440391, acc.: 83.59%] [G loss: 1.571084]\n",
      "epoch:36 step:33900 [D loss: 0.678306, acc.: 64.06%] [G loss: 1.451955]\n",
      "epoch:36 step:33901 [D loss: 0.609481, acc.: 67.19%] [G loss: 1.310876]\n",
      "epoch:36 step:33902 [D loss: 0.459856, acc.: 82.03%] [G loss: 1.782296]\n",
      "epoch:36 step:33903 [D loss: 0.673485, acc.: 64.84%] [G loss: 1.208558]\n",
      "epoch:36 step:33904 [D loss: 0.501886, acc.: 80.47%] [G loss: 1.683212]\n",
      "epoch:36 step:33905 [D loss: 0.741368, acc.: 53.91%] [G loss: 1.319346]\n",
      "epoch:36 step:33906 [D loss: 0.421229, acc.: 83.59%] [G loss: 1.268033]\n",
      "epoch:36 step:33907 [D loss: 0.473828, acc.: 77.34%] [G loss: 1.576818]\n",
      "epoch:36 step:33908 [D loss: 0.709030, acc.: 60.16%] [G loss: 1.458074]\n",
      "epoch:36 step:33909 [D loss: 0.384114, acc.: 85.16%] [G loss: 1.461016]\n",
      "epoch:36 step:33910 [D loss: 0.703750, acc.: 62.50%] [G loss: 0.824224]\n",
      "epoch:36 step:33911 [D loss: 0.509508, acc.: 77.34%] [G loss: 1.635969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:33912 [D loss: 0.426367, acc.: 75.00%] [G loss: 1.630337]\n",
      "epoch:36 step:33913 [D loss: 0.505003, acc.: 75.78%] [G loss: 1.894986]\n",
      "epoch:36 step:33914 [D loss: 0.363991, acc.: 87.50%] [G loss: 1.346430]\n",
      "epoch:36 step:33915 [D loss: 0.479526, acc.: 80.47%] [G loss: 2.154782]\n",
      "epoch:36 step:33916 [D loss: 0.519593, acc.: 73.44%] [G loss: 1.653020]\n",
      "epoch:36 step:33917 [D loss: 0.465097, acc.: 80.47%] [G loss: 1.664206]\n",
      "epoch:36 step:33918 [D loss: 0.512423, acc.: 75.78%] [G loss: 1.791397]\n",
      "epoch:36 step:33919 [D loss: 0.346764, acc.: 87.50%] [G loss: 1.800273]\n",
      "epoch:36 step:33920 [D loss: 0.588943, acc.: 63.28%] [G loss: 1.242581]\n",
      "epoch:36 step:33921 [D loss: 0.467784, acc.: 76.56%] [G loss: 0.991725]\n",
      "epoch:36 step:33922 [D loss: 0.698385, acc.: 59.38%] [G loss: 1.701233]\n",
      "epoch:36 step:33923 [D loss: 0.546662, acc.: 74.22%] [G loss: 1.777548]\n",
      "epoch:36 step:33924 [D loss: 0.422824, acc.: 84.38%] [G loss: 1.495352]\n",
      "epoch:36 step:33925 [D loss: 0.545613, acc.: 74.22%] [G loss: 1.014231]\n",
      "epoch:36 step:33926 [D loss: 0.613117, acc.: 60.16%] [G loss: 1.670101]\n",
      "epoch:36 step:33927 [D loss: 0.451045, acc.: 82.03%] [G loss: 1.358487]\n",
      "epoch:36 step:33928 [D loss: 0.592485, acc.: 65.62%] [G loss: 1.069755]\n",
      "epoch:36 step:33929 [D loss: 0.542607, acc.: 66.41%] [G loss: 1.559711]\n",
      "epoch:36 step:33930 [D loss: 0.467055, acc.: 74.22%] [G loss: 1.953490]\n",
      "epoch:36 step:33931 [D loss: 0.359852, acc.: 85.94%] [G loss: 1.450958]\n",
      "epoch:36 step:33932 [D loss: 0.690801, acc.: 60.16%] [G loss: 1.502268]\n",
      "epoch:36 step:33933 [D loss: 0.460712, acc.: 80.47%] [G loss: 1.469764]\n",
      "epoch:36 step:33934 [D loss: 0.442427, acc.: 81.25%] [G loss: 1.885923]\n",
      "epoch:36 step:33935 [D loss: 0.484417, acc.: 75.78%] [G loss: 1.662452]\n",
      "epoch:36 step:33936 [D loss: 0.435839, acc.: 82.81%] [G loss: 1.652268]\n",
      "epoch:36 step:33937 [D loss: 0.660608, acc.: 64.84%] [G loss: 1.367769]\n",
      "epoch:36 step:33938 [D loss: 0.548786, acc.: 72.66%] [G loss: 1.654227]\n",
      "epoch:36 step:33939 [D loss: 0.533374, acc.: 75.00%] [G loss: 1.526867]\n",
      "epoch:36 step:33940 [D loss: 0.396336, acc.: 80.47%] [G loss: 1.454521]\n",
      "epoch:36 step:33941 [D loss: 0.776636, acc.: 54.69%] [G loss: 1.248522]\n",
      "epoch:36 step:33942 [D loss: 0.572236, acc.: 74.22%] [G loss: 1.658159]\n",
      "epoch:36 step:33943 [D loss: 0.257535, acc.: 92.97%] [G loss: 1.428841]\n",
      "epoch:36 step:33944 [D loss: 0.561684, acc.: 73.44%] [G loss: 1.286581]\n",
      "epoch:36 step:33945 [D loss: 0.764543, acc.: 53.91%] [G loss: 1.533314]\n",
      "epoch:36 step:33946 [D loss: 0.807279, acc.: 57.03%] [G loss: 1.720822]\n",
      "epoch:36 step:33947 [D loss: 0.666180, acc.: 63.28%] [G loss: 1.862098]\n",
      "epoch:36 step:33948 [D loss: 0.633654, acc.: 60.94%] [G loss: 1.496740]\n",
      "epoch:36 step:33949 [D loss: 0.566950, acc.: 70.31%] [G loss: 1.646708]\n",
      "epoch:36 step:33950 [D loss: 0.652035, acc.: 62.50%] [G loss: 1.566219]\n",
      "epoch:36 step:33951 [D loss: 0.542981, acc.: 70.31%] [G loss: 1.803357]\n",
      "epoch:36 step:33952 [D loss: 0.509480, acc.: 75.78%] [G loss: 1.630966]\n",
      "epoch:36 step:33953 [D loss: 0.602142, acc.: 66.41%] [G loss: 1.392420]\n",
      "epoch:36 step:33954 [D loss: 0.457442, acc.: 77.34%] [G loss: 1.081707]\n",
      "epoch:36 step:33955 [D loss: 0.527004, acc.: 74.22%] [G loss: 1.328167]\n",
      "epoch:36 step:33956 [D loss: 0.536646, acc.: 71.09%] [G loss: 1.295276]\n",
      "epoch:36 step:33957 [D loss: 0.648858, acc.: 61.72%] [G loss: 1.818975]\n",
      "epoch:36 step:33958 [D loss: 0.339966, acc.: 87.50%] [G loss: 1.988835]\n",
      "epoch:36 step:33959 [D loss: 0.451198, acc.: 77.34%] [G loss: 1.821410]\n",
      "epoch:36 step:33960 [D loss: 0.677925, acc.: 66.41%] [G loss: 1.184410]\n",
      "epoch:36 step:33961 [D loss: 0.624968, acc.: 62.50%] [G loss: 1.570365]\n",
      "epoch:36 step:33962 [D loss: 0.470458, acc.: 78.91%] [G loss: 1.127205]\n",
      "epoch:36 step:33963 [D loss: 0.434630, acc.: 81.25%] [G loss: 2.140275]\n",
      "epoch:36 step:33964 [D loss: 0.529836, acc.: 68.75%] [G loss: 1.803087]\n",
      "epoch:36 step:33965 [D loss: 0.474678, acc.: 76.56%] [G loss: 2.035765]\n",
      "epoch:36 step:33966 [D loss: 0.401467, acc.: 85.94%] [G loss: 1.804940]\n",
      "epoch:36 step:33967 [D loss: 0.463015, acc.: 77.34%] [G loss: 1.892681]\n",
      "epoch:36 step:33968 [D loss: 0.550768, acc.: 73.44%] [G loss: 1.380117]\n",
      "epoch:36 step:33969 [D loss: 0.498344, acc.: 73.44%] [G loss: 1.402287]\n",
      "epoch:36 step:33970 [D loss: 0.517204, acc.: 72.66%] [G loss: 1.443673]\n",
      "epoch:36 step:33971 [D loss: 0.497946, acc.: 79.69%] [G loss: 1.441464]\n",
      "epoch:36 step:33972 [D loss: 0.650110, acc.: 61.72%] [G loss: 1.496519]\n",
      "epoch:36 step:33973 [D loss: 0.586441, acc.: 67.19%] [G loss: 1.467573]\n",
      "epoch:36 step:33974 [D loss: 0.559573, acc.: 69.53%] [G loss: 1.553045]\n",
      "epoch:36 step:33975 [D loss: 0.428984, acc.: 82.81%] [G loss: 1.027689]\n",
      "epoch:36 step:33976 [D loss: 0.473997, acc.: 77.34%] [G loss: 1.488551]\n",
      "epoch:36 step:33977 [D loss: 0.441099, acc.: 85.16%] [G loss: 1.450818]\n",
      "epoch:36 step:33978 [D loss: 0.421747, acc.: 82.81%] [G loss: 0.997384]\n",
      "epoch:36 step:33979 [D loss: 0.421820, acc.: 81.25%] [G loss: 1.444211]\n",
      "epoch:36 step:33980 [D loss: 0.497614, acc.: 75.78%] [G loss: 1.288498]\n",
      "epoch:36 step:33981 [D loss: 0.389246, acc.: 85.16%] [G loss: 1.821706]\n",
      "epoch:36 step:33982 [D loss: 0.348783, acc.: 90.62%] [G loss: 1.621540]\n",
      "epoch:36 step:33983 [D loss: 0.640227, acc.: 68.75%] [G loss: 1.387884]\n",
      "epoch:36 step:33984 [D loss: 0.514168, acc.: 78.91%] [G loss: 1.250971]\n",
      "epoch:36 step:33985 [D loss: 0.543743, acc.: 75.78%] [G loss: 2.014538]\n",
      "epoch:36 step:33986 [D loss: 0.588307, acc.: 69.53%] [G loss: 1.661141]\n",
      "epoch:36 step:33987 [D loss: 0.586437, acc.: 69.53%] [G loss: 1.052360]\n",
      "epoch:36 step:33988 [D loss: 0.797090, acc.: 51.56%] [G loss: 1.276291]\n",
      "epoch:36 step:33989 [D loss: 0.495952, acc.: 75.00%] [G loss: 1.511096]\n",
      "epoch:36 step:33990 [D loss: 0.472662, acc.: 78.91%] [G loss: 1.282332]\n",
      "epoch:36 step:33991 [D loss: 0.578803, acc.: 71.88%] [G loss: 2.160240]\n",
      "epoch:36 step:33992 [D loss: 0.739259, acc.: 60.94%] [G loss: 1.483342]\n",
      "epoch:36 step:33993 [D loss: 0.482275, acc.: 76.56%] [G loss: 1.006401]\n",
      "epoch:36 step:33994 [D loss: 0.713990, acc.: 60.94%] [G loss: 1.220921]\n",
      "epoch:36 step:33995 [D loss: 0.518766, acc.: 75.78%] [G loss: 1.628250]\n",
      "epoch:36 step:33996 [D loss: 0.449489, acc.: 78.91%] [G loss: 1.187880]\n",
      "epoch:36 step:33997 [D loss: 0.320086, acc.: 91.41%] [G loss: 1.691566]\n",
      "epoch:36 step:33998 [D loss: 0.521451, acc.: 71.09%] [G loss: 1.602597]\n",
      "epoch:36 step:33999 [D loss: 0.276786, acc.: 92.19%] [G loss: 2.097703]\n",
      "epoch:36 step:34000 [D loss: 0.483113, acc.: 77.34%] [G loss: 1.160416]\n",
      "##############\n",
      "[2.66437184 2.13133597 1.83697885 3.1103322  1.0583168  6.12213436\n",
      " 2.23587443 3.20395304 4.04612918 8.14868929]\n",
      "##########\n",
      "epoch:36 step:34001 [D loss: 0.523190, acc.: 78.12%] [G loss: 1.612174]\n",
      "epoch:36 step:34002 [D loss: 0.303962, acc.: 87.50%] [G loss: 2.193758]\n",
      "epoch:36 step:34003 [D loss: 0.504715, acc.: 77.34%] [G loss: 1.541287]\n",
      "epoch:36 step:34004 [D loss: 0.721530, acc.: 58.59%] [G loss: 1.598007]\n",
      "epoch:36 step:34005 [D loss: 0.607021, acc.: 64.84%] [G loss: 2.130837]\n",
      "epoch:36 step:34006 [D loss: 0.512284, acc.: 78.12%] [G loss: 1.402238]\n",
      "epoch:36 step:34007 [D loss: 0.713054, acc.: 53.91%] [G loss: 1.485675]\n",
      "epoch:36 step:34008 [D loss: 0.682002, acc.: 67.19%] [G loss: 1.427275]\n",
      "epoch:36 step:34009 [D loss: 0.791177, acc.: 53.12%] [G loss: 1.595726]\n",
      "epoch:36 step:34010 [D loss: 0.403152, acc.: 87.50%] [G loss: 2.151812]\n",
      "epoch:36 step:34011 [D loss: 0.447094, acc.: 82.81%] [G loss: 1.578702]\n",
      "epoch:36 step:34012 [D loss: 0.899310, acc.: 42.97%] [G loss: 1.570833]\n",
      "epoch:36 step:34013 [D loss: 0.501736, acc.: 74.22%] [G loss: 1.459705]\n",
      "epoch:36 step:34014 [D loss: 0.588676, acc.: 67.97%] [G loss: 1.911315]\n",
      "epoch:36 step:34015 [D loss: 0.501197, acc.: 76.56%] [G loss: 1.340380]\n",
      "epoch:36 step:34016 [D loss: 0.308418, acc.: 91.41%] [G loss: 2.020533]\n",
      "epoch:36 step:34017 [D loss: 0.541904, acc.: 71.09%] [G loss: 1.286083]\n",
      "epoch:36 step:34018 [D loss: 0.535255, acc.: 67.97%] [G loss: 1.660654]\n",
      "epoch:36 step:34019 [D loss: 0.468102, acc.: 81.25%] [G loss: 1.095657]\n",
      "epoch:36 step:34020 [D loss: 0.525736, acc.: 71.09%] [G loss: 1.027666]\n",
      "epoch:36 step:34021 [D loss: 0.643318, acc.: 67.97%] [G loss: 1.851869]\n",
      "epoch:36 step:34022 [D loss: 0.548684, acc.: 71.09%] [G loss: 1.232842]\n",
      "epoch:36 step:34023 [D loss: 0.757302, acc.: 57.03%] [G loss: 1.372140]\n",
      "epoch:36 step:34024 [D loss: 0.413948, acc.: 82.03%] [G loss: 1.275973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34025 [D loss: 0.496704, acc.: 75.78%] [G loss: 1.255896]\n",
      "epoch:36 step:34026 [D loss: 0.613975, acc.: 68.75%] [G loss: 1.085969]\n",
      "epoch:36 step:34027 [D loss: 0.488946, acc.: 74.22%] [G loss: 0.966768]\n",
      "epoch:36 step:34028 [D loss: 0.412272, acc.: 85.94%] [G loss: 1.628186]\n",
      "epoch:36 step:34029 [D loss: 0.462274, acc.: 79.69%] [G loss: 1.484984]\n",
      "epoch:36 step:34030 [D loss: 0.675076, acc.: 61.72%] [G loss: 1.484066]\n",
      "epoch:36 step:34031 [D loss: 0.501476, acc.: 75.00%] [G loss: 1.361143]\n",
      "epoch:36 step:34032 [D loss: 0.356953, acc.: 85.94%] [G loss: 2.187495]\n",
      "epoch:36 step:34033 [D loss: 0.418290, acc.: 84.38%] [G loss: 1.423427]\n",
      "epoch:36 step:34034 [D loss: 0.586312, acc.: 69.53%] [G loss: 1.523834]\n",
      "epoch:36 step:34035 [D loss: 0.804472, acc.: 52.34%] [G loss: 1.741154]\n",
      "epoch:36 step:34036 [D loss: 0.765975, acc.: 56.25%] [G loss: 1.271938]\n",
      "epoch:36 step:34037 [D loss: 0.324013, acc.: 92.97%] [G loss: 2.218330]\n",
      "epoch:36 step:34038 [D loss: 0.554533, acc.: 71.88%] [G loss: 1.848353]\n",
      "epoch:36 step:34039 [D loss: 0.737639, acc.: 63.28%] [G loss: 1.397613]\n",
      "epoch:36 step:34040 [D loss: 0.426484, acc.: 82.81%] [G loss: 1.194690]\n",
      "epoch:36 step:34041 [D loss: 0.360041, acc.: 86.72%] [G loss: 1.738766]\n",
      "epoch:36 step:34042 [D loss: 0.679370, acc.: 62.50%] [G loss: 1.652375]\n",
      "epoch:36 step:34043 [D loss: 0.426892, acc.: 84.38%] [G loss: 1.377888]\n",
      "epoch:36 step:34044 [D loss: 0.422744, acc.: 84.38%] [G loss: 1.878655]\n",
      "epoch:36 step:34045 [D loss: 0.621327, acc.: 67.97%] [G loss: 1.264833]\n",
      "epoch:36 step:34046 [D loss: 0.402434, acc.: 85.94%] [G loss: 1.393496]\n",
      "epoch:36 step:34047 [D loss: 0.589384, acc.: 71.09%] [G loss: 2.055144]\n",
      "epoch:36 step:34048 [D loss: 0.826196, acc.: 50.00%] [G loss: 1.749723]\n",
      "epoch:36 step:34049 [D loss: 0.365551, acc.: 85.16%] [G loss: 2.003329]\n",
      "epoch:36 step:34050 [D loss: 0.469429, acc.: 77.34%] [G loss: 1.810540]\n",
      "epoch:36 step:34051 [D loss: 0.542836, acc.: 72.66%] [G loss: 1.443918]\n",
      "epoch:36 step:34052 [D loss: 0.615239, acc.: 62.50%] [G loss: 1.870836]\n",
      "epoch:36 step:34053 [D loss: 0.494711, acc.: 75.78%] [G loss: 1.818615]\n",
      "epoch:36 step:34054 [D loss: 0.583112, acc.: 68.75%] [G loss: 1.120507]\n",
      "epoch:36 step:34055 [D loss: 0.376185, acc.: 84.38%] [G loss: 1.581150]\n",
      "epoch:36 step:34056 [D loss: 0.549658, acc.: 75.78%] [G loss: 1.588922]\n",
      "epoch:36 step:34057 [D loss: 0.542917, acc.: 69.53%] [G loss: 1.252808]\n",
      "epoch:36 step:34058 [D loss: 0.449519, acc.: 78.12%] [G loss: 1.654684]\n",
      "epoch:36 step:34059 [D loss: 0.626118, acc.: 71.88%] [G loss: 1.438141]\n",
      "epoch:36 step:34060 [D loss: 0.405554, acc.: 81.25%] [G loss: 1.700570]\n",
      "epoch:36 step:34061 [D loss: 0.423398, acc.: 80.47%] [G loss: 2.028860]\n",
      "epoch:36 step:34062 [D loss: 0.507779, acc.: 74.22%] [G loss: 2.204258]\n",
      "epoch:36 step:34063 [D loss: 0.532215, acc.: 72.66%] [G loss: 1.687627]\n",
      "epoch:36 step:34064 [D loss: 0.618013, acc.: 62.50%] [G loss: 1.233058]\n",
      "epoch:36 step:34065 [D loss: 0.601522, acc.: 64.84%] [G loss: 1.484116]\n",
      "epoch:36 step:34066 [D loss: 0.427087, acc.: 78.91%] [G loss: 1.510100]\n",
      "epoch:36 step:34067 [D loss: 0.417856, acc.: 83.59%] [G loss: 1.238039]\n",
      "epoch:36 step:34068 [D loss: 0.585163, acc.: 70.31%] [G loss: 1.527027]\n",
      "epoch:36 step:34069 [D loss: 0.478766, acc.: 74.22%] [G loss: 1.686022]\n",
      "epoch:36 step:34070 [D loss: 0.395774, acc.: 85.94%] [G loss: 1.205682]\n",
      "epoch:36 step:34071 [D loss: 0.339399, acc.: 86.72%] [G loss: 1.332411]\n",
      "epoch:36 step:34072 [D loss: 0.553602, acc.: 75.00%] [G loss: 1.768588]\n",
      "epoch:36 step:34073 [D loss: 0.628215, acc.: 65.62%] [G loss: 1.353972]\n",
      "epoch:36 step:34074 [D loss: 0.488816, acc.: 75.00%] [G loss: 1.398869]\n",
      "epoch:36 step:34075 [D loss: 0.732214, acc.: 57.81%] [G loss: 1.320711]\n",
      "epoch:36 step:34076 [D loss: 0.359798, acc.: 85.94%] [G loss: 1.470490]\n",
      "epoch:36 step:34077 [D loss: 0.558367, acc.: 74.22%] [G loss: 1.306485]\n",
      "epoch:36 step:34078 [D loss: 0.518330, acc.: 76.56%] [G loss: 1.741589]\n",
      "epoch:36 step:34079 [D loss: 0.365139, acc.: 85.94%] [G loss: 1.920138]\n",
      "epoch:36 step:34080 [D loss: 0.432060, acc.: 78.91%] [G loss: 1.442819]\n",
      "epoch:36 step:34081 [D loss: 0.851753, acc.: 51.56%] [G loss: 1.441264]\n",
      "epoch:36 step:34082 [D loss: 0.395735, acc.: 84.38%] [G loss: 1.557193]\n",
      "epoch:36 step:34083 [D loss: 0.460480, acc.: 75.00%] [G loss: 1.451581]\n",
      "epoch:36 step:34084 [D loss: 0.576794, acc.: 66.41%] [G loss: 1.386230]\n",
      "epoch:36 step:34085 [D loss: 0.864767, acc.: 50.78%] [G loss: 1.665771]\n",
      "epoch:36 step:34086 [D loss: 0.520235, acc.: 73.44%] [G loss: 1.590578]\n",
      "epoch:36 step:34087 [D loss: 0.532011, acc.: 79.69%] [G loss: 1.332822]\n",
      "epoch:36 step:34088 [D loss: 0.453582, acc.: 82.81%] [G loss: 1.454314]\n",
      "epoch:36 step:34089 [D loss: 0.419758, acc.: 82.03%] [G loss: 1.439106]\n",
      "epoch:36 step:34090 [D loss: 0.489999, acc.: 76.56%] [G loss: 1.091189]\n",
      "epoch:36 step:34091 [D loss: 0.527923, acc.: 75.00%] [G loss: 1.439412]\n",
      "epoch:36 step:34092 [D loss: 0.414730, acc.: 81.25%] [G loss: 1.505045]\n",
      "epoch:36 step:34093 [D loss: 0.572720, acc.: 72.66%] [G loss: 1.661592]\n",
      "epoch:36 step:34094 [D loss: 0.604989, acc.: 70.31%] [G loss: 1.347830]\n",
      "epoch:36 step:34095 [D loss: 0.548232, acc.: 72.66%] [G loss: 1.853416]\n",
      "epoch:36 step:34096 [D loss: 0.459877, acc.: 80.47%] [G loss: 1.681461]\n",
      "epoch:36 step:34097 [D loss: 0.556114, acc.: 72.66%] [G loss: 1.061585]\n",
      "epoch:36 step:34098 [D loss: 0.717573, acc.: 62.50%] [G loss: 1.510360]\n",
      "epoch:36 step:34099 [D loss: 0.475669, acc.: 76.56%] [G loss: 1.582038]\n",
      "epoch:36 step:34100 [D loss: 0.572996, acc.: 69.53%] [G loss: 1.636438]\n",
      "epoch:36 step:34101 [D loss: 0.494602, acc.: 72.66%] [G loss: 1.276549]\n",
      "epoch:36 step:34102 [D loss: 0.464097, acc.: 81.25%] [G loss: 1.288144]\n",
      "epoch:36 step:34103 [D loss: 0.361082, acc.: 87.50%] [G loss: 1.101508]\n",
      "epoch:36 step:34104 [D loss: 0.560552, acc.: 70.31%] [G loss: 1.083541]\n",
      "epoch:36 step:34105 [D loss: 0.465573, acc.: 77.34%] [G loss: 1.257549]\n",
      "epoch:36 step:34106 [D loss: 0.676839, acc.: 64.06%] [G loss: 1.189670]\n",
      "epoch:36 step:34107 [D loss: 0.374632, acc.: 82.81%] [G loss: 1.318676]\n",
      "epoch:36 step:34108 [D loss: 0.452179, acc.: 80.47%] [G loss: 1.515277]\n",
      "epoch:36 step:34109 [D loss: 0.465684, acc.: 77.34%] [G loss: 1.515026]\n",
      "epoch:36 step:34110 [D loss: 0.483434, acc.: 80.47%] [G loss: 1.381378]\n",
      "epoch:36 step:34111 [D loss: 0.309582, acc.: 90.62%] [G loss: 1.900198]\n",
      "epoch:36 step:34112 [D loss: 0.416073, acc.: 82.81%] [G loss: 1.532119]\n",
      "epoch:36 step:34113 [D loss: 0.468568, acc.: 83.59%] [G loss: 1.541539]\n",
      "epoch:36 step:34114 [D loss: 0.511513, acc.: 77.34%] [G loss: 1.540515]\n",
      "epoch:36 step:34115 [D loss: 0.654073, acc.: 61.72%] [G loss: 1.686256]\n",
      "epoch:36 step:34116 [D loss: 0.349531, acc.: 87.50%] [G loss: 1.903869]\n",
      "epoch:36 step:34117 [D loss: 0.481649, acc.: 78.91%] [G loss: 1.508034]\n",
      "epoch:36 step:34118 [D loss: 0.660187, acc.: 60.16%] [G loss: 1.323428]\n",
      "epoch:36 step:34119 [D loss: 0.508503, acc.: 76.56%] [G loss: 1.559861]\n",
      "epoch:36 step:34120 [D loss: 0.637879, acc.: 61.72%] [G loss: 1.535041]\n",
      "epoch:36 step:34121 [D loss: 0.672642, acc.: 60.16%] [G loss: 1.421613]\n",
      "epoch:36 step:34122 [D loss: 0.600711, acc.: 67.97%] [G loss: 1.878343]\n",
      "epoch:36 step:34123 [D loss: 0.834589, acc.: 53.12%] [G loss: 1.462090]\n",
      "epoch:36 step:34124 [D loss: 0.684232, acc.: 63.28%] [G loss: 0.875346]\n",
      "epoch:36 step:34125 [D loss: 0.595977, acc.: 63.28%] [G loss: 1.628775]\n",
      "epoch:36 step:34126 [D loss: 0.653548, acc.: 64.06%] [G loss: 1.573991]\n",
      "epoch:36 step:34127 [D loss: 0.527900, acc.: 76.56%] [G loss: 1.321225]\n",
      "epoch:36 step:34128 [D loss: 0.394040, acc.: 85.16%] [G loss: 1.966774]\n",
      "epoch:36 step:34129 [D loss: 0.769438, acc.: 53.12%] [G loss: 0.954961]\n",
      "epoch:36 step:34130 [D loss: 0.530542, acc.: 74.22%] [G loss: 1.032837]\n",
      "epoch:36 step:34131 [D loss: 0.490662, acc.: 79.69%] [G loss: 1.465257]\n",
      "epoch:36 step:34132 [D loss: 0.513771, acc.: 71.09%] [G loss: 1.681338]\n",
      "epoch:36 step:34133 [D loss: 0.364111, acc.: 87.50%] [G loss: 1.992322]\n",
      "epoch:36 step:34134 [D loss: 0.638748, acc.: 69.53%] [G loss: 1.386567]\n",
      "epoch:36 step:34135 [D loss: 0.625614, acc.: 64.06%] [G loss: 1.115063]\n",
      "epoch:36 step:34136 [D loss: 0.533707, acc.: 72.66%] [G loss: 1.104698]\n",
      "epoch:36 step:34137 [D loss: 0.462734, acc.: 78.91%] [G loss: 1.795985]\n",
      "epoch:36 step:34138 [D loss: 0.653431, acc.: 58.59%] [G loss: 1.537546]\n",
      "epoch:36 step:34139 [D loss: 0.514259, acc.: 75.00%] [G loss: 1.489655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34140 [D loss: 0.387600, acc.: 85.94%] [G loss: 1.837731]\n",
      "epoch:36 step:34141 [D loss: 0.797975, acc.: 50.00%] [G loss: 1.075118]\n",
      "epoch:36 step:34142 [D loss: 0.473145, acc.: 78.12%] [G loss: 1.272264]\n",
      "epoch:36 step:34143 [D loss: 0.506194, acc.: 71.88%] [G loss: 1.641157]\n",
      "epoch:36 step:34144 [D loss: 0.632816, acc.: 60.16%] [G loss: 1.687103]\n",
      "epoch:36 step:34145 [D loss: 0.649805, acc.: 64.84%] [G loss: 1.818830]\n",
      "epoch:36 step:34146 [D loss: 0.377149, acc.: 85.16%] [G loss: 1.319817]\n",
      "epoch:36 step:34147 [D loss: 0.632041, acc.: 65.62%] [G loss: 1.300859]\n",
      "epoch:36 step:34148 [D loss: 0.783930, acc.: 57.81%] [G loss: 1.383282]\n",
      "epoch:36 step:34149 [D loss: 0.469114, acc.: 75.78%] [G loss: 1.251526]\n",
      "epoch:36 step:34150 [D loss: 0.709009, acc.: 55.47%] [G loss: 1.123476]\n",
      "epoch:36 step:34151 [D loss: 0.450277, acc.: 79.69%] [G loss: 2.035193]\n",
      "epoch:36 step:34152 [D loss: 0.547721, acc.: 68.75%] [G loss: 1.741628]\n",
      "epoch:36 step:34153 [D loss: 0.744521, acc.: 53.91%] [G loss: 1.242029]\n",
      "epoch:36 step:34154 [D loss: 0.523070, acc.: 72.66%] [G loss: 1.728771]\n",
      "epoch:36 step:34155 [D loss: 0.856956, acc.: 45.31%] [G loss: 1.189020]\n",
      "epoch:36 step:34156 [D loss: 0.486802, acc.: 71.88%] [G loss: 1.183844]\n",
      "epoch:36 step:34157 [D loss: 0.701182, acc.: 60.94%] [G loss: 2.014509]\n",
      "epoch:36 step:34158 [D loss: 0.446222, acc.: 80.47%] [G loss: 0.881891]\n",
      "epoch:36 step:34159 [D loss: 0.479379, acc.: 75.00%] [G loss: 1.523944]\n",
      "epoch:36 step:34160 [D loss: 0.706930, acc.: 58.59%] [G loss: 1.115535]\n",
      "epoch:36 step:34161 [D loss: 0.686515, acc.: 61.72%] [G loss: 1.473281]\n",
      "epoch:36 step:34162 [D loss: 0.536590, acc.: 73.44%] [G loss: 1.291135]\n",
      "epoch:36 step:34163 [D loss: 0.739423, acc.: 56.25%] [G loss: 1.288585]\n",
      "epoch:36 step:34164 [D loss: 0.529685, acc.: 72.66%] [G loss: 1.704504]\n",
      "epoch:36 step:34165 [D loss: 0.552729, acc.: 71.88%] [G loss: 1.687768]\n",
      "epoch:36 step:34166 [D loss: 0.616721, acc.: 67.19%] [G loss: 1.550205]\n",
      "epoch:36 step:34167 [D loss: 0.479927, acc.: 78.91%] [G loss: 1.800429]\n",
      "epoch:36 step:34168 [D loss: 0.829240, acc.: 46.09%] [G loss: 1.564647]\n",
      "epoch:36 step:34169 [D loss: 0.544488, acc.: 72.66%] [G loss: 1.580650]\n",
      "epoch:36 step:34170 [D loss: 0.531558, acc.: 71.88%] [G loss: 1.486018]\n",
      "epoch:36 step:34171 [D loss: 0.362644, acc.: 89.06%] [G loss: 1.726726]\n",
      "epoch:36 step:34172 [D loss: 0.470197, acc.: 76.56%] [G loss: 1.985075]\n",
      "epoch:36 step:34173 [D loss: 0.498749, acc.: 78.12%] [G loss: 1.547590]\n",
      "epoch:36 step:34174 [D loss: 0.506811, acc.: 77.34%] [G loss: 1.700068]\n",
      "epoch:36 step:34175 [D loss: 0.494717, acc.: 75.00%] [G loss: 1.191337]\n",
      "epoch:36 step:34176 [D loss: 0.384099, acc.: 84.38%] [G loss: 1.342933]\n",
      "epoch:36 step:34177 [D loss: 0.458486, acc.: 78.12%] [G loss: 1.278507]\n",
      "epoch:36 step:34178 [D loss: 0.535535, acc.: 74.22%] [G loss: 1.896034]\n",
      "epoch:36 step:34179 [D loss: 0.383516, acc.: 82.81%] [G loss: 1.517780]\n",
      "epoch:36 step:34180 [D loss: 0.536492, acc.: 68.75%] [G loss: 0.777378]\n",
      "epoch:36 step:34181 [D loss: 0.449951, acc.: 78.91%] [G loss: 1.432121]\n",
      "epoch:36 step:34182 [D loss: 0.804289, acc.: 56.25%] [G loss: 1.474577]\n",
      "epoch:36 step:34183 [D loss: 0.420204, acc.: 79.69%] [G loss: 1.730548]\n",
      "epoch:36 step:34184 [D loss: 0.670658, acc.: 63.28%] [G loss: 1.339637]\n",
      "epoch:36 step:34185 [D loss: 0.397709, acc.: 84.38%] [G loss: 1.803939]\n",
      "epoch:36 step:34186 [D loss: 0.337777, acc.: 88.28%] [G loss: 1.633823]\n",
      "epoch:36 step:34187 [D loss: 0.433032, acc.: 82.81%] [G loss: 1.803253]\n",
      "epoch:36 step:34188 [D loss: 0.430110, acc.: 80.47%] [G loss: 1.873202]\n",
      "epoch:36 step:34189 [D loss: 0.596566, acc.: 67.97%] [G loss: 1.232205]\n",
      "epoch:36 step:34190 [D loss: 0.574336, acc.: 67.19%] [G loss: 1.665575]\n",
      "epoch:36 step:34191 [D loss: 0.438809, acc.: 78.12%] [G loss: 1.051877]\n",
      "epoch:36 step:34192 [D loss: 0.465053, acc.: 77.34%] [G loss: 1.632076]\n",
      "epoch:36 step:34193 [D loss: 0.649443, acc.: 59.38%] [G loss: 1.977164]\n",
      "epoch:36 step:34194 [D loss: 0.517287, acc.: 74.22%] [G loss: 1.610224]\n",
      "epoch:36 step:34195 [D loss: 0.557549, acc.: 68.75%] [G loss: 1.858252]\n",
      "epoch:36 step:34196 [D loss: 0.555333, acc.: 72.66%] [G loss: 1.708150]\n",
      "epoch:36 step:34197 [D loss: 0.543567, acc.: 70.31%] [G loss: 1.236138]\n",
      "epoch:36 step:34198 [D loss: 0.429236, acc.: 79.69%] [G loss: 1.369816]\n",
      "epoch:36 step:34199 [D loss: 0.453758, acc.: 79.69%] [G loss: 1.740560]\n",
      "epoch:36 step:34200 [D loss: 0.488746, acc.: 80.47%] [G loss: 2.066424]\n",
      "##############\n",
      "[2.63897622 2.06563952 2.00633908 2.76917216 1.05836997 6.51988731\n",
      " 2.19666563 2.95514243 3.92402884 7.14868929]\n",
      "##########\n",
      "epoch:36 step:34201 [D loss: 0.540356, acc.: 73.44%] [G loss: 1.768898]\n",
      "epoch:36 step:34202 [D loss: 0.927580, acc.: 48.44%] [G loss: 1.417802]\n",
      "epoch:36 step:34203 [D loss: 0.558574, acc.: 72.66%] [G loss: 1.531484]\n",
      "epoch:36 step:34204 [D loss: 0.613341, acc.: 64.84%] [G loss: 1.642885]\n",
      "epoch:36 step:34205 [D loss: 0.304303, acc.: 89.06%] [G loss: 1.727624]\n",
      "epoch:36 step:34206 [D loss: 0.589929, acc.: 69.53%] [G loss: 2.015416]\n",
      "epoch:36 step:34207 [D loss: 0.490878, acc.: 75.78%] [G loss: 1.224838]\n",
      "epoch:36 step:34208 [D loss: 0.462418, acc.: 78.12%] [G loss: 1.867087]\n",
      "epoch:36 step:34209 [D loss: 0.589972, acc.: 71.09%] [G loss: 1.011939]\n",
      "epoch:36 step:34210 [D loss: 0.562763, acc.: 67.19%] [G loss: 1.558539]\n",
      "epoch:36 step:34211 [D loss: 0.312592, acc.: 92.19%] [G loss: 1.369935]\n",
      "epoch:36 step:34212 [D loss: 0.458135, acc.: 82.03%] [G loss: 1.506802]\n",
      "epoch:36 step:34213 [D loss: 0.608918, acc.: 70.31%] [G loss: 1.693611]\n",
      "epoch:36 step:34214 [D loss: 0.463176, acc.: 76.56%] [G loss: 1.544360]\n",
      "epoch:36 step:34215 [D loss: 0.579882, acc.: 67.97%] [G loss: 1.678839]\n",
      "epoch:36 step:34216 [D loss: 0.314487, acc.: 91.41%] [G loss: 1.135742]\n",
      "epoch:36 step:34217 [D loss: 0.626298, acc.: 66.41%] [G loss: 1.850368]\n",
      "epoch:36 step:34218 [D loss: 0.577215, acc.: 69.53%] [G loss: 1.484370]\n",
      "epoch:36 step:34219 [D loss: 0.381582, acc.: 82.81%] [G loss: 2.148390]\n",
      "epoch:36 step:34220 [D loss: 0.351939, acc.: 84.38%] [G loss: 1.429193]\n",
      "epoch:36 step:34221 [D loss: 0.520133, acc.: 77.34%] [G loss: 1.337261]\n",
      "epoch:36 step:34222 [D loss: 0.541205, acc.: 68.75%] [G loss: 1.628991]\n",
      "epoch:36 step:34223 [D loss: 0.513558, acc.: 76.56%] [G loss: 1.644243]\n",
      "epoch:36 step:34224 [D loss: 0.488880, acc.: 77.34%] [G loss: 1.497893]\n",
      "epoch:36 step:34225 [D loss: 0.504382, acc.: 78.91%] [G loss: 1.330005]\n",
      "epoch:36 step:34226 [D loss: 0.641466, acc.: 67.19%] [G loss: 1.765832]\n",
      "epoch:36 step:34227 [D loss: 0.468214, acc.: 77.34%] [G loss: 1.392730]\n",
      "epoch:36 step:34228 [D loss: 0.518932, acc.: 73.44%] [G loss: 0.877794]\n",
      "epoch:36 step:34229 [D loss: 0.580442, acc.: 70.31%] [G loss: 0.798623]\n",
      "epoch:36 step:34230 [D loss: 0.570153, acc.: 63.28%] [G loss: 1.562312]\n",
      "epoch:36 step:34231 [D loss: 0.432957, acc.: 81.25%] [G loss: 1.092699]\n",
      "epoch:36 step:34232 [D loss: 0.535160, acc.: 75.00%] [G loss: 1.575924]\n",
      "epoch:36 step:34233 [D loss: 0.314543, acc.: 92.97%] [G loss: 1.333790]\n",
      "epoch:36 step:34234 [D loss: 0.435211, acc.: 82.81%] [G loss: 1.612601]\n",
      "epoch:36 step:34235 [D loss: 0.713864, acc.: 59.38%] [G loss: 1.860559]\n",
      "epoch:36 step:34236 [D loss: 0.483765, acc.: 75.78%] [G loss: 1.441354]\n",
      "epoch:36 step:34237 [D loss: 0.189102, acc.: 96.09%] [G loss: 1.469884]\n",
      "epoch:36 step:34238 [D loss: 0.326915, acc.: 89.84%] [G loss: 1.935745]\n",
      "epoch:36 step:34239 [D loss: 0.540971, acc.: 69.53%] [G loss: 1.383258]\n",
      "epoch:36 step:34240 [D loss: 0.488479, acc.: 79.69%] [G loss: 1.082613]\n",
      "epoch:36 step:34241 [D loss: 0.434785, acc.: 82.81%] [G loss: 1.623613]\n",
      "epoch:36 step:34242 [D loss: 0.372831, acc.: 86.72%] [G loss: 1.431219]\n",
      "epoch:36 step:34243 [D loss: 0.505432, acc.: 73.44%] [G loss: 1.952314]\n",
      "epoch:36 step:34244 [D loss: 0.668254, acc.: 61.72%] [G loss: 1.667462]\n",
      "epoch:36 step:34245 [D loss: 0.595004, acc.: 70.31%] [G loss: 1.451886]\n",
      "epoch:36 step:34246 [D loss: 0.328125, acc.: 89.06%] [G loss: 2.296484]\n",
      "epoch:36 step:34247 [D loss: 0.767757, acc.: 59.38%] [G loss: 1.063318]\n",
      "epoch:36 step:34248 [D loss: 0.520953, acc.: 72.66%] [G loss: 1.210591]\n",
      "epoch:36 step:34249 [D loss: 0.450270, acc.: 80.47%] [G loss: 1.615754]\n",
      "epoch:36 step:34250 [D loss: 0.493063, acc.: 78.12%] [G loss: 1.631967]\n",
      "epoch:36 step:34251 [D loss: 0.604755, acc.: 68.75%] [G loss: 1.584298]\n",
      "epoch:36 step:34252 [D loss: 0.373268, acc.: 85.94%] [G loss: 2.090935]\n",
      "epoch:36 step:34253 [D loss: 0.758474, acc.: 50.78%] [G loss: 1.840152]\n",
      "epoch:36 step:34254 [D loss: 0.521977, acc.: 72.66%] [G loss: 1.884715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34255 [D loss: 0.686664, acc.: 63.28%] [G loss: 0.957065]\n",
      "epoch:36 step:34256 [D loss: 0.617035, acc.: 63.28%] [G loss: 1.203743]\n",
      "epoch:36 step:34257 [D loss: 0.548881, acc.: 69.53%] [G loss: 1.060816]\n",
      "epoch:36 step:34258 [D loss: 0.502836, acc.: 75.00%] [G loss: 1.030823]\n",
      "epoch:36 step:34259 [D loss: 0.470402, acc.: 76.56%] [G loss: 1.523062]\n",
      "epoch:36 step:34260 [D loss: 0.628804, acc.: 66.41%] [G loss: 1.891237]\n",
      "epoch:36 step:34261 [D loss: 0.547273, acc.: 75.00%] [G loss: 1.742782]\n",
      "epoch:36 step:34262 [D loss: 0.518557, acc.: 77.34%] [G loss: 2.194529]\n",
      "epoch:36 step:34263 [D loss: 0.382070, acc.: 88.28%] [G loss: 1.703505]\n",
      "epoch:36 step:34264 [D loss: 0.468231, acc.: 76.56%] [G loss: 1.956966]\n",
      "epoch:36 step:34265 [D loss: 0.623738, acc.: 68.75%] [G loss: 1.077205]\n",
      "epoch:36 step:34266 [D loss: 0.493233, acc.: 77.34%] [G loss: 1.162935]\n",
      "epoch:36 step:34267 [D loss: 0.410433, acc.: 84.38%] [G loss: 1.204906]\n",
      "epoch:36 step:34268 [D loss: 0.670221, acc.: 64.06%] [G loss: 1.070903]\n",
      "epoch:36 step:34269 [D loss: 0.337892, acc.: 89.84%] [G loss: 1.604981]\n",
      "epoch:36 step:34270 [D loss: 0.654704, acc.: 63.28%] [G loss: 2.046114]\n",
      "epoch:36 step:34271 [D loss: 0.533976, acc.: 73.44%] [G loss: 1.671490]\n",
      "epoch:36 step:34272 [D loss: 0.469566, acc.: 79.69%] [G loss: 1.722429]\n",
      "epoch:36 step:34273 [D loss: 0.438482, acc.: 82.03%] [G loss: 1.388323]\n",
      "epoch:36 step:34274 [D loss: 0.358624, acc.: 86.72%] [G loss: 1.746528]\n",
      "epoch:36 step:34275 [D loss: 0.606235, acc.: 67.19%] [G loss: 1.556085]\n",
      "epoch:36 step:34276 [D loss: 0.499533, acc.: 77.34%] [G loss: 1.833379]\n",
      "epoch:36 step:34277 [D loss: 0.540613, acc.: 73.44%] [G loss: 1.647561]\n",
      "epoch:36 step:34278 [D loss: 0.484766, acc.: 77.34%] [G loss: 1.740477]\n",
      "epoch:36 step:34279 [D loss: 0.684730, acc.: 55.47%] [G loss: 1.904450]\n",
      "epoch:36 step:34280 [D loss: 0.559751, acc.: 67.97%] [G loss: 1.184800]\n",
      "epoch:36 step:34281 [D loss: 0.412882, acc.: 82.03%] [G loss: 1.467222]\n",
      "epoch:36 step:34282 [D loss: 0.467855, acc.: 77.34%] [G loss: 1.543944]\n",
      "epoch:36 step:34283 [D loss: 0.493193, acc.: 75.00%] [G loss: 1.455247]\n",
      "epoch:36 step:34284 [D loss: 0.403554, acc.: 79.69%] [G loss: 1.798084]\n",
      "epoch:36 step:34285 [D loss: 0.529280, acc.: 75.78%] [G loss: 1.605821]\n",
      "epoch:36 step:34286 [D loss: 0.387965, acc.: 82.81%] [G loss: 1.431419]\n",
      "epoch:36 step:34287 [D loss: 0.503135, acc.: 75.78%] [G loss: 1.768214]\n",
      "epoch:36 step:34288 [D loss: 0.426514, acc.: 82.81%] [G loss: 1.942207]\n",
      "epoch:36 step:34289 [D loss: 0.530634, acc.: 73.44%] [G loss: 1.518614]\n",
      "epoch:36 step:34290 [D loss: 0.447217, acc.: 78.91%] [G loss: 1.404199]\n",
      "epoch:36 step:34291 [D loss: 0.401195, acc.: 82.03%] [G loss: 1.779275]\n",
      "epoch:36 step:34292 [D loss: 0.539606, acc.: 75.78%] [G loss: 1.675858]\n",
      "epoch:36 step:34293 [D loss: 0.443485, acc.: 81.25%] [G loss: 1.395536]\n",
      "epoch:36 step:34294 [D loss: 0.609334, acc.: 67.97%] [G loss: 1.621794]\n",
      "epoch:36 step:34295 [D loss: 0.328277, acc.: 88.28%] [G loss: 1.516480]\n",
      "epoch:36 step:34296 [D loss: 0.342408, acc.: 89.06%] [G loss: 1.248226]\n",
      "epoch:36 step:34297 [D loss: 0.519209, acc.: 69.53%] [G loss: 1.574756]\n",
      "epoch:36 step:34298 [D loss: 0.399060, acc.: 85.94%] [G loss: 1.931728]\n",
      "epoch:36 step:34299 [D loss: 0.588243, acc.: 72.66%] [G loss: 1.623880]\n",
      "epoch:36 step:34300 [D loss: 0.551244, acc.: 67.97%] [G loss: 1.261731]\n",
      "epoch:36 step:34301 [D loss: 0.413969, acc.: 80.47%] [G loss: 1.605962]\n",
      "epoch:36 step:34302 [D loss: 0.717841, acc.: 57.81%] [G loss: 1.211967]\n",
      "epoch:36 step:34303 [D loss: 0.340573, acc.: 89.84%] [G loss: 1.660327]\n",
      "epoch:36 step:34304 [D loss: 0.556953, acc.: 71.09%] [G loss: 1.373778]\n",
      "epoch:36 step:34305 [D loss: 0.464576, acc.: 77.34%] [G loss: 1.376139]\n",
      "epoch:36 step:34306 [D loss: 0.776185, acc.: 50.78%] [G loss: 1.543414]\n",
      "epoch:36 step:34307 [D loss: 0.556069, acc.: 75.78%] [G loss: 1.728639]\n",
      "epoch:36 step:34308 [D loss: 0.343399, acc.: 89.06%] [G loss: 2.071021]\n",
      "epoch:36 step:34309 [D loss: 0.662595, acc.: 59.38%] [G loss: 1.627178]\n",
      "epoch:36 step:34310 [D loss: 0.488873, acc.: 75.00%] [G loss: 1.670495]\n",
      "epoch:36 step:34311 [D loss: 0.541966, acc.: 72.66%] [G loss: 1.824710]\n",
      "epoch:36 step:34312 [D loss: 0.593724, acc.: 68.75%] [G loss: 1.564398]\n",
      "epoch:36 step:34313 [D loss: 0.468627, acc.: 76.56%] [G loss: 1.331140]\n",
      "epoch:36 step:34314 [D loss: 0.514534, acc.: 77.34%] [G loss: 1.191351]\n",
      "epoch:36 step:34315 [D loss: 0.615817, acc.: 64.06%] [G loss: 1.116136]\n",
      "epoch:36 step:34316 [D loss: 0.598465, acc.: 71.88%] [G loss: 1.359851]\n",
      "epoch:36 step:34317 [D loss: 0.605852, acc.: 63.28%] [G loss: 1.349541]\n",
      "epoch:36 step:34318 [D loss: 0.410554, acc.: 82.03%] [G loss: 1.779047]\n",
      "epoch:36 step:34319 [D loss: 0.434994, acc.: 80.47%] [G loss: 2.159786]\n",
      "epoch:36 step:34320 [D loss: 0.562211, acc.: 71.88%] [G loss: 1.298487]\n",
      "epoch:36 step:34321 [D loss: 0.655533, acc.: 61.72%] [G loss: 1.601981]\n",
      "epoch:36 step:34322 [D loss: 0.533675, acc.: 74.22%] [G loss: 1.100087]\n",
      "epoch:36 step:34323 [D loss: 0.648290, acc.: 64.06%] [G loss: 2.364544]\n",
      "epoch:36 step:34324 [D loss: 0.636931, acc.: 60.94%] [G loss: 1.498095]\n",
      "epoch:36 step:34325 [D loss: 0.752951, acc.: 54.69%] [G loss: 1.196033]\n",
      "epoch:36 step:34326 [D loss: 0.402677, acc.: 85.16%] [G loss: 1.799647]\n",
      "epoch:36 step:34327 [D loss: 0.589018, acc.: 68.75%] [G loss: 1.559489]\n",
      "epoch:36 step:34328 [D loss: 0.452773, acc.: 80.47%] [G loss: 1.852862]\n",
      "epoch:36 step:34329 [D loss: 0.472821, acc.: 79.69%] [G loss: 1.494621]\n",
      "epoch:36 step:34330 [D loss: 0.740574, acc.: 62.50%] [G loss: 1.252513]\n",
      "epoch:36 step:34331 [D loss: 0.435074, acc.: 80.47%] [G loss: 1.348515]\n",
      "epoch:36 step:34332 [D loss: 0.426976, acc.: 77.34%] [G loss: 1.531543]\n",
      "epoch:36 step:34333 [D loss: 0.669825, acc.: 59.38%] [G loss: 1.450054]\n",
      "epoch:36 step:34334 [D loss: 0.441675, acc.: 79.69%] [G loss: 1.534387]\n",
      "epoch:36 step:34335 [D loss: 0.592985, acc.: 71.09%] [G loss: 1.460820]\n",
      "epoch:36 step:34336 [D loss: 0.403530, acc.: 83.59%] [G loss: 1.611549]\n",
      "epoch:36 step:34337 [D loss: 0.505806, acc.: 74.22%] [G loss: 1.705191]\n",
      "epoch:36 step:34338 [D loss: 0.651075, acc.: 63.28%] [G loss: 0.901013]\n",
      "epoch:36 step:34339 [D loss: 0.512666, acc.: 70.31%] [G loss: 1.337631]\n",
      "epoch:36 step:34340 [D loss: 0.326735, acc.: 92.19%] [G loss: 1.891776]\n",
      "epoch:36 step:34341 [D loss: 0.442367, acc.: 78.12%] [G loss: 1.713084]\n",
      "epoch:36 step:34342 [D loss: 0.414063, acc.: 83.59%] [G loss: 1.776106]\n",
      "epoch:36 step:34343 [D loss: 0.707375, acc.: 61.72%] [G loss: 1.362700]\n",
      "epoch:36 step:34344 [D loss: 0.817967, acc.: 52.34%] [G loss: 1.237050]\n",
      "epoch:36 step:34345 [D loss: 0.429038, acc.: 77.34%] [G loss: 1.241136]\n",
      "epoch:36 step:34346 [D loss: 0.557592, acc.: 70.31%] [G loss: 1.481982]\n",
      "epoch:36 step:34347 [D loss: 0.388097, acc.: 85.16%] [G loss: 1.789014]\n",
      "epoch:36 step:34348 [D loss: 0.734043, acc.: 58.59%] [G loss: 1.772655]\n",
      "epoch:36 step:34349 [D loss: 0.404138, acc.: 82.03%] [G loss: 1.064726]\n",
      "epoch:36 step:34350 [D loss: 0.499970, acc.: 77.34%] [G loss: 1.028400]\n",
      "epoch:36 step:34351 [D loss: 0.605173, acc.: 64.84%] [G loss: 1.356798]\n",
      "epoch:36 step:34352 [D loss: 0.396788, acc.: 85.94%] [G loss: 1.455201]\n",
      "epoch:36 step:34353 [D loss: 0.898531, acc.: 46.09%] [G loss: 1.166614]\n",
      "epoch:36 step:34354 [D loss: 0.596611, acc.: 68.75%] [G loss: 1.408563]\n",
      "epoch:36 step:34355 [D loss: 0.528417, acc.: 74.22%] [G loss: 1.882639]\n",
      "epoch:36 step:34356 [D loss: 0.596246, acc.: 71.09%] [G loss: 1.228098]\n",
      "epoch:36 step:34357 [D loss: 0.508342, acc.: 75.00%] [G loss: 1.374477]\n",
      "epoch:36 step:34358 [D loss: 0.448455, acc.: 77.34%] [G loss: 0.855021]\n",
      "epoch:36 step:34359 [D loss: 0.623876, acc.: 64.84%] [G loss: 1.320611]\n",
      "epoch:36 step:34360 [D loss: 0.484943, acc.: 78.91%] [G loss: 1.245304]\n",
      "epoch:36 step:34361 [D loss: 0.565171, acc.: 71.88%] [G loss: 1.556977]\n",
      "epoch:36 step:34362 [D loss: 0.599095, acc.: 64.06%] [G loss: 1.324465]\n",
      "epoch:36 step:34363 [D loss: 0.478318, acc.: 75.00%] [G loss: 1.169440]\n",
      "epoch:36 step:34364 [D loss: 0.597473, acc.: 70.31%] [G loss: 1.731823]\n",
      "epoch:36 step:34365 [D loss: 0.386547, acc.: 87.50%] [G loss: 1.713321]\n",
      "epoch:36 step:34366 [D loss: 0.419468, acc.: 82.03%] [G loss: 2.103508]\n",
      "epoch:36 step:34367 [D loss: 0.432356, acc.: 83.59%] [G loss: 1.217170]\n",
      "epoch:36 step:34368 [D loss: 0.369033, acc.: 82.81%] [G loss: 1.518213]\n",
      "epoch:36 step:34369 [D loss: 0.607974, acc.: 68.75%] [G loss: 1.511978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34370 [D loss: 0.557173, acc.: 70.31%] [G loss: 1.305532]\n",
      "epoch:36 step:34371 [D loss: 0.357858, acc.: 86.72%] [G loss: 1.864605]\n",
      "epoch:36 step:34372 [D loss: 0.495570, acc.: 76.56%] [G loss: 1.276576]\n",
      "epoch:36 step:34373 [D loss: 0.636737, acc.: 60.94%] [G loss: 1.632141]\n",
      "epoch:36 step:34374 [D loss: 0.726758, acc.: 56.25%] [G loss: 1.201177]\n",
      "epoch:36 step:34375 [D loss: 0.857729, acc.: 47.66%] [G loss: 1.700925]\n",
      "epoch:36 step:34376 [D loss: 0.436606, acc.: 78.12%] [G loss: 2.053061]\n",
      "epoch:36 step:34377 [D loss: 0.754922, acc.: 61.72%] [G loss: 1.420640]\n",
      "epoch:36 step:34378 [D loss: 0.732357, acc.: 59.38%] [G loss: 1.473294]\n",
      "epoch:36 step:34379 [D loss: 0.693519, acc.: 60.16%] [G loss: 1.237982]\n",
      "epoch:36 step:34380 [D loss: 0.369221, acc.: 89.06%] [G loss: 1.543725]\n",
      "epoch:36 step:34381 [D loss: 0.571810, acc.: 65.62%] [G loss: 1.588053]\n",
      "epoch:36 step:34382 [D loss: 0.325160, acc.: 89.06%] [G loss: 1.721252]\n",
      "epoch:36 step:34383 [D loss: 0.402819, acc.: 83.59%] [G loss: 1.534080]\n",
      "epoch:36 step:34384 [D loss: 0.493766, acc.: 78.12%] [G loss: 1.330356]\n",
      "epoch:36 step:34385 [D loss: 0.652199, acc.: 66.41%] [G loss: 1.465997]\n",
      "epoch:36 step:34386 [D loss: 0.822505, acc.: 50.00%] [G loss: 1.453487]\n",
      "epoch:36 step:34387 [D loss: 0.429282, acc.: 80.47%] [G loss: 2.079458]\n",
      "epoch:36 step:34388 [D loss: 0.392193, acc.: 81.25%] [G loss: 1.913207]\n",
      "epoch:36 step:34389 [D loss: 0.591341, acc.: 67.19%] [G loss: 1.611727]\n",
      "epoch:36 step:34390 [D loss: 0.753258, acc.: 57.81%] [G loss: 1.537014]\n",
      "epoch:36 step:34391 [D loss: 0.667520, acc.: 68.75%] [G loss: 1.599155]\n",
      "epoch:36 step:34392 [D loss: 0.380633, acc.: 89.06%] [G loss: 1.623447]\n",
      "epoch:36 step:34393 [D loss: 0.493274, acc.: 78.12%] [G loss: 1.390995]\n",
      "epoch:36 step:34394 [D loss: 0.658842, acc.: 61.72%] [G loss: 1.521635]\n",
      "epoch:36 step:34395 [D loss: 0.534527, acc.: 75.78%] [G loss: 1.112205]\n",
      "epoch:36 step:34396 [D loss: 0.568262, acc.: 67.97%] [G loss: 0.790743]\n",
      "epoch:36 step:34397 [D loss: 0.504478, acc.: 75.78%] [G loss: 1.228692]\n",
      "epoch:36 step:34398 [D loss: 0.343446, acc.: 87.50%] [G loss: 1.437389]\n",
      "epoch:36 step:34399 [D loss: 0.630454, acc.: 64.84%] [G loss: 1.477399]\n",
      "epoch:36 step:34400 [D loss: 0.584136, acc.: 65.62%] [G loss: 1.864696]\n",
      "##############\n",
      "[2.6549462  2.20180037 1.8660529  3.31129467 1.06923896 6.50279245\n",
      " 2.32466971 2.99609321 3.89286026 7.14868929]\n",
      "##########\n",
      "epoch:36 step:34401 [D loss: 0.340701, acc.: 86.72%] [G loss: 1.870266]\n",
      "epoch:36 step:34402 [D loss: 0.583759, acc.: 69.53%] [G loss: 1.912366]\n",
      "epoch:36 step:34403 [D loss: 0.373968, acc.: 83.59%] [G loss: 1.842383]\n",
      "epoch:36 step:34404 [D loss: 0.481257, acc.: 77.34%] [G loss: 1.422604]\n",
      "epoch:36 step:34405 [D loss: 0.713911, acc.: 60.94%] [G loss: 1.060173]\n",
      "epoch:36 step:34406 [D loss: 0.461838, acc.: 77.34%] [G loss: 1.456898]\n",
      "epoch:36 step:34407 [D loss: 0.663750, acc.: 65.62%] [G loss: 1.605813]\n",
      "epoch:36 step:34408 [D loss: 0.553776, acc.: 67.19%] [G loss: 1.409799]\n",
      "epoch:36 step:34409 [D loss: 0.613376, acc.: 64.06%] [G loss: 1.882433]\n",
      "epoch:36 step:34410 [D loss: 0.719195, acc.: 54.69%] [G loss: 1.790938]\n",
      "epoch:36 step:34411 [D loss: 0.647972, acc.: 64.84%] [G loss: 1.329419]\n",
      "epoch:36 step:34412 [D loss: 0.564584, acc.: 65.62%] [G loss: 1.247582]\n",
      "epoch:36 step:34413 [D loss: 0.519123, acc.: 78.91%] [G loss: 1.809167]\n",
      "epoch:36 step:34414 [D loss: 0.618677, acc.: 64.84%] [G loss: 2.182024]\n",
      "epoch:36 step:34415 [D loss: 0.661851, acc.: 60.94%] [G loss: 1.505648]\n",
      "epoch:36 step:34416 [D loss: 0.556470, acc.: 73.44%] [G loss: 1.712193]\n",
      "epoch:36 step:34417 [D loss: 0.535231, acc.: 73.44%] [G loss: 1.762305]\n",
      "epoch:36 step:34418 [D loss: 0.388055, acc.: 83.59%] [G loss: 1.738081]\n",
      "epoch:36 step:34419 [D loss: 0.584073, acc.: 64.06%] [G loss: 2.097704]\n",
      "epoch:36 step:34420 [D loss: 0.520860, acc.: 71.88%] [G loss: 1.794166]\n",
      "epoch:36 step:34421 [D loss: 0.650359, acc.: 63.28%] [G loss: 1.190398]\n",
      "epoch:36 step:34422 [D loss: 0.627516, acc.: 64.84%] [G loss: 1.224378]\n",
      "epoch:36 step:34423 [D loss: 0.591295, acc.: 67.19%] [G loss: 1.946553]\n",
      "epoch:36 step:34424 [D loss: 0.348081, acc.: 88.28%] [G loss: 1.517850]\n",
      "epoch:36 step:34425 [D loss: 0.836129, acc.: 50.00%] [G loss: 1.258411]\n",
      "epoch:36 step:34426 [D loss: 0.460449, acc.: 78.91%] [G loss: 1.752479]\n",
      "epoch:36 step:34427 [D loss: 0.504349, acc.: 77.34%] [G loss: 1.653895]\n",
      "epoch:36 step:34428 [D loss: 0.651220, acc.: 60.94%] [G loss: 1.222810]\n",
      "epoch:36 step:34429 [D loss: 0.405442, acc.: 83.59%] [G loss: 1.714972]\n",
      "epoch:36 step:34430 [D loss: 0.660049, acc.: 58.59%] [G loss: 1.190413]\n",
      "epoch:36 step:34431 [D loss: 0.521637, acc.: 78.12%] [G loss: 1.714463]\n",
      "epoch:36 step:34432 [D loss: 0.636268, acc.: 67.19%] [G loss: 1.273299]\n",
      "epoch:36 step:34433 [D loss: 0.330888, acc.: 86.72%] [G loss: 1.766896]\n",
      "epoch:36 step:34434 [D loss: 0.316285, acc.: 90.62%] [G loss: 1.724974]\n",
      "epoch:36 step:34435 [D loss: 0.470701, acc.: 76.56%] [G loss: 1.979059]\n",
      "epoch:36 step:34436 [D loss: 0.592579, acc.: 67.97%] [G loss: 1.421898]\n",
      "epoch:36 step:34437 [D loss: 0.509782, acc.: 73.44%] [G loss: 1.248448]\n",
      "epoch:36 step:34438 [D loss: 0.622533, acc.: 67.97%] [G loss: 1.329493]\n",
      "epoch:36 step:34439 [D loss: 0.452474, acc.: 83.59%] [G loss: 1.841052]\n",
      "epoch:36 step:34440 [D loss: 0.473817, acc.: 74.22%] [G loss: 1.404312]\n",
      "epoch:36 step:34441 [D loss: 0.511488, acc.: 75.78%] [G loss: 1.992728]\n",
      "epoch:36 step:34442 [D loss: 0.560621, acc.: 70.31%] [G loss: 1.712606]\n",
      "epoch:36 step:34443 [D loss: 0.403095, acc.: 79.69%] [G loss: 1.970130]\n",
      "epoch:36 step:34444 [D loss: 0.570028, acc.: 67.97%] [G loss: 1.845061]\n",
      "epoch:36 step:34445 [D loss: 0.676577, acc.: 64.84%] [G loss: 1.540571]\n",
      "epoch:36 step:34446 [D loss: 0.808395, acc.: 52.34%] [G loss: 1.421653]\n",
      "epoch:36 step:34447 [D loss: 0.559110, acc.: 70.31%] [G loss: 1.327955]\n",
      "epoch:36 step:34448 [D loss: 0.782242, acc.: 49.22%] [G loss: 1.170330]\n",
      "epoch:36 step:34449 [D loss: 0.623224, acc.: 64.84%] [G loss: 1.653233]\n",
      "epoch:36 step:34450 [D loss: 0.283170, acc.: 92.19%] [G loss: 1.593702]\n",
      "epoch:36 step:34451 [D loss: 0.546316, acc.: 68.75%] [G loss: 1.134675]\n",
      "epoch:36 step:34452 [D loss: 0.518849, acc.: 73.44%] [G loss: 1.637199]\n",
      "epoch:36 step:34453 [D loss: 0.514738, acc.: 75.78%] [G loss: 1.356660]\n",
      "epoch:36 step:34454 [D loss: 0.641090, acc.: 60.16%] [G loss: 1.337459]\n",
      "epoch:36 step:34455 [D loss: 0.744261, acc.: 55.47%] [G loss: 1.482538]\n",
      "epoch:36 step:34456 [D loss: 0.590324, acc.: 67.97%] [G loss: 1.946048]\n",
      "epoch:36 step:34457 [D loss: 0.448143, acc.: 81.25%] [G loss: 1.486774]\n",
      "epoch:36 step:34458 [D loss: 0.599386, acc.: 68.75%] [G loss: 1.384655]\n",
      "epoch:36 step:34459 [D loss: 0.606411, acc.: 70.31%] [G loss: 1.435778]\n",
      "epoch:36 step:34460 [D loss: 0.686280, acc.: 60.94%] [G loss: 1.208956]\n",
      "epoch:36 step:34461 [D loss: 0.434852, acc.: 84.38%] [G loss: 1.749982]\n",
      "epoch:36 step:34462 [D loss: 0.572402, acc.: 74.22%] [G loss: 1.308995]\n",
      "epoch:36 step:34463 [D loss: 0.641281, acc.: 64.06%] [G loss: 1.455134]\n",
      "epoch:36 step:34464 [D loss: 0.691989, acc.: 57.81%] [G loss: 1.297671]\n",
      "epoch:36 step:34465 [D loss: 0.331766, acc.: 91.41%] [G loss: 1.879528]\n",
      "epoch:36 step:34466 [D loss: 0.511931, acc.: 75.00%] [G loss: 1.051472]\n",
      "epoch:36 step:34467 [D loss: 0.535289, acc.: 78.91%] [G loss: 1.123552]\n",
      "epoch:36 step:34468 [D loss: 0.501900, acc.: 75.00%] [G loss: 1.244169]\n",
      "epoch:36 step:34469 [D loss: 0.469126, acc.: 77.34%] [G loss: 1.666526]\n",
      "epoch:36 step:34470 [D loss: 0.652215, acc.: 64.84%] [G loss: 1.039282]\n",
      "epoch:36 step:34471 [D loss: 0.597803, acc.: 71.09%] [G loss: 1.669328]\n",
      "epoch:36 step:34472 [D loss: 0.653510, acc.: 60.16%] [G loss: 1.112855]\n",
      "epoch:36 step:34473 [D loss: 0.444014, acc.: 81.25%] [G loss: 1.528086]\n",
      "epoch:36 step:34474 [D loss: 0.422273, acc.: 82.03%] [G loss: 1.686685]\n",
      "epoch:36 step:34475 [D loss: 0.663070, acc.: 60.94%] [G loss: 1.723333]\n",
      "epoch:36 step:34476 [D loss: 0.370066, acc.: 84.38%] [G loss: 1.359462]\n",
      "epoch:36 step:34477 [D loss: 0.605775, acc.: 67.97%] [G loss: 1.776514]\n",
      "epoch:36 step:34478 [D loss: 0.388016, acc.: 82.81%] [G loss: 1.825258]\n",
      "epoch:36 step:34479 [D loss: 0.455907, acc.: 82.81%] [G loss: 1.585762]\n",
      "epoch:36 step:34480 [D loss: 0.519077, acc.: 74.22%] [G loss: 1.534158]\n",
      "epoch:36 step:34481 [D loss: 0.478961, acc.: 75.78%] [G loss: 2.196785]\n",
      "epoch:36 step:34482 [D loss: 0.523359, acc.: 75.00%] [G loss: 1.555848]\n",
      "epoch:36 step:34483 [D loss: 0.522330, acc.: 71.09%] [G loss: 1.247968]\n",
      "epoch:36 step:34484 [D loss: 0.611196, acc.: 68.75%] [G loss: 1.470236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34485 [D loss: 0.696387, acc.: 59.38%] [G loss: 0.713319]\n",
      "epoch:36 step:34486 [D loss: 0.794281, acc.: 50.78%] [G loss: 0.992487]\n",
      "epoch:36 step:34487 [D loss: 0.411212, acc.: 82.81%] [G loss: 1.496118]\n",
      "epoch:36 step:34488 [D loss: 0.841175, acc.: 53.12%] [G loss: 1.164745]\n",
      "epoch:36 step:34489 [D loss: 0.356732, acc.: 88.28%] [G loss: 2.357428]\n",
      "epoch:36 step:34490 [D loss: 0.631398, acc.: 61.72%] [G loss: 0.808685]\n",
      "epoch:36 step:34491 [D loss: 0.362768, acc.: 85.94%] [G loss: 2.075153]\n",
      "epoch:36 step:34492 [D loss: 0.514646, acc.: 76.56%] [G loss: 1.328696]\n",
      "epoch:36 step:34493 [D loss: 0.569731, acc.: 69.53%] [G loss: 1.063119]\n",
      "epoch:36 step:34494 [D loss: 0.511430, acc.: 75.78%] [G loss: 1.509114]\n",
      "epoch:36 step:34495 [D loss: 0.709421, acc.: 60.94%] [G loss: 1.299275]\n",
      "epoch:36 step:34496 [D loss: 0.357335, acc.: 87.50%] [G loss: 1.335168]\n",
      "epoch:36 step:34497 [D loss: 0.449852, acc.: 81.25%] [G loss: 1.726506]\n",
      "epoch:36 step:34498 [D loss: 0.442164, acc.: 79.69%] [G loss: 1.142683]\n",
      "epoch:36 step:34499 [D loss: 0.209137, acc.: 96.09%] [G loss: 1.889665]\n",
      "epoch:36 step:34500 [D loss: 0.535808, acc.: 75.00%] [G loss: 1.745519]\n",
      "epoch:36 step:34501 [D loss: 0.390915, acc.: 85.94%] [G loss: 1.733586]\n",
      "epoch:36 step:34502 [D loss: 0.618520, acc.: 65.62%] [G loss: 1.802218]\n",
      "epoch:36 step:34503 [D loss: 0.534189, acc.: 69.53%] [G loss: 1.409701]\n",
      "epoch:36 step:34504 [D loss: 0.778736, acc.: 53.12%] [G loss: 1.024302]\n",
      "epoch:36 step:34505 [D loss: 0.666888, acc.: 61.72%] [G loss: 1.195940]\n",
      "epoch:36 step:34506 [D loss: 0.538653, acc.: 74.22%] [G loss: 1.555671]\n",
      "epoch:36 step:34507 [D loss: 0.508347, acc.: 78.91%] [G loss: 2.069422]\n",
      "epoch:36 step:34508 [D loss: 0.503581, acc.: 75.78%] [G loss: 1.619717]\n",
      "epoch:36 step:34509 [D loss: 0.577106, acc.: 69.53%] [G loss: 1.921234]\n",
      "epoch:36 step:34510 [D loss: 0.475949, acc.: 79.69%] [G loss: 1.547067]\n",
      "epoch:36 step:34511 [D loss: 0.716628, acc.: 57.81%] [G loss: 1.208062]\n",
      "epoch:36 step:34512 [D loss: 0.681320, acc.: 59.38%] [G loss: 1.149806]\n",
      "epoch:36 step:34513 [D loss: 0.557206, acc.: 74.22%] [G loss: 1.789036]\n",
      "epoch:36 step:34514 [D loss: 0.700729, acc.: 61.72%] [G loss: 1.733718]\n",
      "epoch:36 step:34515 [D loss: 0.587822, acc.: 66.41%] [G loss: 1.168705]\n",
      "epoch:36 step:34516 [D loss: 0.487364, acc.: 80.47%] [G loss: 1.522995]\n",
      "epoch:36 step:34517 [D loss: 0.392270, acc.: 82.81%] [G loss: 1.610574]\n",
      "epoch:36 step:34518 [D loss: 0.875987, acc.: 51.56%] [G loss: 1.332528]\n",
      "epoch:36 step:34519 [D loss: 0.538005, acc.: 75.78%] [G loss: 1.186207]\n",
      "epoch:36 step:34520 [D loss: 0.432699, acc.: 79.69%] [G loss: 1.617582]\n",
      "epoch:36 step:34521 [D loss: 0.497304, acc.: 75.78%] [G loss: 1.401491]\n",
      "epoch:36 step:34522 [D loss: 0.447023, acc.: 78.91%] [G loss: 1.493585]\n",
      "epoch:36 step:34523 [D loss: 0.638915, acc.: 67.97%] [G loss: 1.228541]\n",
      "epoch:36 step:34524 [D loss: 0.490667, acc.: 77.34%] [G loss: 1.690343]\n",
      "epoch:36 step:34525 [D loss: 0.468996, acc.: 77.34%] [G loss: 1.812398]\n",
      "epoch:36 step:34526 [D loss: 0.531437, acc.: 69.53%] [G loss: 1.671760]\n",
      "epoch:36 step:34527 [D loss: 0.453839, acc.: 81.25%] [G loss: 1.472126]\n",
      "epoch:36 step:34528 [D loss: 0.740412, acc.: 57.81%] [G loss: 1.115182]\n",
      "epoch:36 step:34529 [D loss: 0.507978, acc.: 77.34%] [G loss: 1.300795]\n",
      "epoch:36 step:34530 [D loss: 0.727157, acc.: 56.25%] [G loss: 1.229356]\n",
      "epoch:36 step:34531 [D loss: 0.524551, acc.: 73.44%] [G loss: 1.809302]\n",
      "epoch:36 step:34532 [D loss: 0.610306, acc.: 66.41%] [G loss: 1.445006]\n",
      "epoch:36 step:34533 [D loss: 0.461432, acc.: 81.25%] [G loss: 1.218305]\n",
      "epoch:36 step:34534 [D loss: 0.590338, acc.: 70.31%] [G loss: 1.241179]\n",
      "epoch:36 step:34535 [D loss: 0.625895, acc.: 60.94%] [G loss: 1.391010]\n",
      "epoch:36 step:34536 [D loss: 0.317096, acc.: 90.62%] [G loss: 1.692280]\n",
      "epoch:36 step:34537 [D loss: 0.815006, acc.: 50.78%] [G loss: 1.765037]\n",
      "epoch:36 step:34538 [D loss: 0.684494, acc.: 61.72%] [G loss: 1.507201]\n",
      "epoch:36 step:34539 [D loss: 0.565513, acc.: 66.41%] [G loss: 1.572241]\n",
      "epoch:36 step:34540 [D loss: 0.376494, acc.: 85.16%] [G loss: 1.565664]\n",
      "epoch:36 step:34541 [D loss: 0.503456, acc.: 75.00%] [G loss: 1.191812]\n",
      "epoch:36 step:34542 [D loss: 0.403795, acc.: 90.62%] [G loss: 1.415525]\n",
      "epoch:36 step:34543 [D loss: 0.803903, acc.: 52.34%] [G loss: 0.895768]\n",
      "epoch:36 step:34544 [D loss: 0.485319, acc.: 74.22%] [G loss: 1.593927]\n",
      "epoch:36 step:34545 [D loss: 0.780748, acc.: 51.56%] [G loss: 1.218951]\n",
      "epoch:36 step:34546 [D loss: 0.377878, acc.: 89.06%] [G loss: 1.547821]\n",
      "epoch:36 step:34547 [D loss: 0.877929, acc.: 48.44%] [G loss: 1.280488]\n",
      "epoch:36 step:34548 [D loss: 0.517928, acc.: 75.00%] [G loss: 2.006384]\n",
      "epoch:36 step:34549 [D loss: 0.682371, acc.: 64.84%] [G loss: 1.386343]\n",
      "epoch:36 step:34550 [D loss: 0.431136, acc.: 82.03%] [G loss: 1.181689]\n",
      "epoch:36 step:34551 [D loss: 0.414599, acc.: 81.25%] [G loss: 1.778075]\n",
      "epoch:36 step:34552 [D loss: 0.389431, acc.: 86.72%] [G loss: 2.180109]\n",
      "epoch:36 step:34553 [D loss: 0.510305, acc.: 73.44%] [G loss: 1.407744]\n",
      "epoch:36 step:34554 [D loss: 0.465895, acc.: 78.91%] [G loss: 1.676096]\n",
      "epoch:36 step:34555 [D loss: 0.593836, acc.: 68.75%] [G loss: 1.421574]\n",
      "epoch:36 step:34556 [D loss: 0.452108, acc.: 78.91%] [G loss: 1.618232]\n",
      "epoch:36 step:34557 [D loss: 0.416859, acc.: 78.91%] [G loss: 1.420391]\n",
      "epoch:36 step:34558 [D loss: 0.506423, acc.: 73.44%] [G loss: 1.733486]\n",
      "epoch:36 step:34559 [D loss: 0.485520, acc.: 77.34%] [G loss: 1.026576]\n",
      "epoch:36 step:34560 [D loss: 0.798653, acc.: 54.69%] [G loss: 1.092686]\n",
      "epoch:36 step:34561 [D loss: 0.553128, acc.: 72.66%] [G loss: 1.509998]\n",
      "epoch:36 step:34562 [D loss: 0.460457, acc.: 82.81%] [G loss: 1.122319]\n",
      "epoch:36 step:34563 [D loss: 0.464911, acc.: 78.91%] [G loss: 1.861160]\n",
      "epoch:36 step:34564 [D loss: 0.727027, acc.: 56.25%] [G loss: 1.480942]\n",
      "epoch:36 step:34565 [D loss: 0.473654, acc.: 75.00%] [G loss: 1.266853]\n",
      "epoch:36 step:34566 [D loss: 0.549265, acc.: 74.22%] [G loss: 1.220143]\n",
      "epoch:36 step:34567 [D loss: 0.499693, acc.: 75.78%] [G loss: 1.352997]\n",
      "epoch:36 step:34568 [D loss: 0.585273, acc.: 71.09%] [G loss: 1.729347]\n",
      "epoch:36 step:34569 [D loss: 0.687806, acc.: 63.28%] [G loss: 1.631056]\n",
      "epoch:36 step:34570 [D loss: 0.588852, acc.: 69.53%] [G loss: 1.771580]\n",
      "epoch:36 step:34571 [D loss: 0.435936, acc.: 78.91%] [G loss: 1.663038]\n",
      "epoch:36 step:34572 [D loss: 0.407437, acc.: 88.28%] [G loss: 1.465615]\n",
      "epoch:36 step:34573 [D loss: 0.582496, acc.: 69.53%] [G loss: 1.227966]\n",
      "epoch:36 step:34574 [D loss: 0.378462, acc.: 80.47%] [G loss: 2.114817]\n",
      "epoch:36 step:34575 [D loss: 0.667924, acc.: 64.06%] [G loss: 1.487364]\n",
      "epoch:36 step:34576 [D loss: 0.802944, acc.: 53.12%] [G loss: 1.321947]\n",
      "epoch:36 step:34577 [D loss: 0.500417, acc.: 76.56%] [G loss: 1.643306]\n",
      "epoch:36 step:34578 [D loss: 0.631892, acc.: 64.06%] [G loss: 1.236611]\n",
      "epoch:36 step:34579 [D loss: 0.533501, acc.: 71.88%] [G loss: 1.475725]\n",
      "epoch:36 step:34580 [D loss: 0.734537, acc.: 60.94%] [G loss: 1.629895]\n",
      "epoch:36 step:34581 [D loss: 0.459364, acc.: 80.47%] [G loss: 1.345371]\n",
      "epoch:36 step:34582 [D loss: 0.517206, acc.: 78.91%] [G loss: 1.924958]\n",
      "epoch:36 step:34583 [D loss: 0.644942, acc.: 60.16%] [G loss: 1.035946]\n",
      "epoch:36 step:34584 [D loss: 0.423975, acc.: 84.38%] [G loss: 1.470803]\n",
      "epoch:36 step:34585 [D loss: 0.724180, acc.: 49.22%] [G loss: 1.472891]\n",
      "epoch:36 step:34586 [D loss: 0.446959, acc.: 76.56%] [G loss: 1.879353]\n",
      "epoch:36 step:34587 [D loss: 0.595209, acc.: 71.09%] [G loss: 1.585200]\n",
      "epoch:36 step:34588 [D loss: 0.435123, acc.: 80.47%] [G loss: 1.664259]\n",
      "epoch:36 step:34589 [D loss: 0.596266, acc.: 71.09%] [G loss: 1.534129]\n",
      "epoch:36 step:34590 [D loss: 0.312412, acc.: 88.28%] [G loss: 1.357813]\n",
      "epoch:36 step:34591 [D loss: 0.470454, acc.: 79.69%] [G loss: 1.691229]\n",
      "epoch:36 step:34592 [D loss: 0.698889, acc.: 64.06%] [G loss: 1.097275]\n",
      "epoch:36 step:34593 [D loss: 0.626123, acc.: 65.62%] [G loss: 1.211544]\n",
      "epoch:36 step:34594 [D loss: 0.689979, acc.: 60.16%] [G loss: 0.917485]\n",
      "epoch:36 step:34595 [D loss: 0.588234, acc.: 67.97%] [G loss: 1.239785]\n",
      "epoch:36 step:34596 [D loss: 0.450695, acc.: 82.03%] [G loss: 1.811515]\n",
      "epoch:36 step:34597 [D loss: 0.362405, acc.: 89.06%] [G loss: 2.000446]\n",
      "epoch:36 step:34598 [D loss: 0.770823, acc.: 53.12%] [G loss: 1.442256]\n",
      "epoch:36 step:34599 [D loss: 0.486832, acc.: 76.56%] [G loss: 1.902282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34600 [D loss: 0.526504, acc.: 71.88%] [G loss: 1.131413]\n",
      "##############\n",
      "[2.69841848 2.0132264  2.01941743 3.02589662 0.7611282  6.23704826\n",
      " 2.04638203 3.02824697 4.08963673 5.21967985]\n",
      "##########\n",
      "epoch:36 step:34601 [D loss: 0.311251, acc.: 90.62%] [G loss: 1.498747]\n",
      "epoch:36 step:34602 [D loss: 0.423576, acc.: 85.94%] [G loss: 1.661862]\n",
      "epoch:36 step:34603 [D loss: 0.632273, acc.: 62.50%] [G loss: 1.541531]\n",
      "epoch:36 step:34604 [D loss: 0.496221, acc.: 77.34%] [G loss: 1.432280]\n",
      "epoch:36 step:34605 [D loss: 0.261380, acc.: 94.53%] [G loss: 1.326730]\n",
      "epoch:36 step:34606 [D loss: 0.464348, acc.: 75.78%] [G loss: 1.038357]\n",
      "epoch:36 step:34607 [D loss: 0.494988, acc.: 74.22%] [G loss: 1.828971]\n",
      "epoch:36 step:34608 [D loss: 0.435078, acc.: 82.03%] [G loss: 1.626555]\n",
      "epoch:36 step:34609 [D loss: 0.635736, acc.: 61.72%] [G loss: 1.052850]\n",
      "epoch:36 step:34610 [D loss: 0.450701, acc.: 82.03%] [G loss: 1.521048]\n",
      "epoch:36 step:34611 [D loss: 0.499404, acc.: 70.31%] [G loss: 1.408132]\n",
      "epoch:36 step:34612 [D loss: 0.500641, acc.: 74.22%] [G loss: 1.473379]\n",
      "epoch:36 step:34613 [D loss: 0.692243, acc.: 62.50%] [G loss: 1.765217]\n",
      "epoch:36 step:34614 [D loss: 0.566813, acc.: 71.88%] [G loss: 1.314569]\n",
      "epoch:36 step:34615 [D loss: 0.401323, acc.: 83.59%] [G loss: 2.193422]\n",
      "epoch:36 step:34616 [D loss: 0.593028, acc.: 71.88%] [G loss: 1.686141]\n",
      "epoch:36 step:34617 [D loss: 0.558271, acc.: 71.88%] [G loss: 1.862567]\n",
      "epoch:36 step:34618 [D loss: 0.376144, acc.: 85.16%] [G loss: 1.547579]\n",
      "epoch:36 step:34619 [D loss: 0.502663, acc.: 74.22%] [G loss: 1.401854]\n",
      "epoch:36 step:34620 [D loss: 0.881527, acc.: 42.97%] [G loss: 1.658872]\n",
      "epoch:36 step:34621 [D loss: 0.916842, acc.: 39.84%] [G loss: 1.335350]\n",
      "epoch:36 step:34622 [D loss: 0.617733, acc.: 66.41%] [G loss: 1.270802]\n",
      "epoch:36 step:34623 [D loss: 0.479256, acc.: 82.81%] [G loss: 1.666640]\n",
      "epoch:36 step:34624 [D loss: 0.513560, acc.: 71.09%] [G loss: 2.055429]\n",
      "epoch:36 step:34625 [D loss: 0.571811, acc.: 72.66%] [G loss: 1.657341]\n",
      "epoch:36 step:34626 [D loss: 0.280085, acc.: 92.19%] [G loss: 1.610302]\n",
      "epoch:36 step:34627 [D loss: 0.346424, acc.: 89.06%] [G loss: 1.222874]\n",
      "epoch:36 step:34628 [D loss: 0.830231, acc.: 49.22%] [G loss: 1.333333]\n",
      "epoch:36 step:34629 [D loss: 0.486124, acc.: 80.47%] [G loss: 1.603125]\n",
      "epoch:36 step:34630 [D loss: 0.540697, acc.: 67.97%] [G loss: 1.142645]\n",
      "epoch:36 step:34631 [D loss: 0.498851, acc.: 75.00%] [G loss: 1.457891]\n",
      "epoch:36 step:34632 [D loss: 0.486839, acc.: 75.78%] [G loss: 1.741799]\n",
      "epoch:36 step:34633 [D loss: 0.463704, acc.: 78.91%] [G loss: 1.178267]\n",
      "epoch:36 step:34634 [D loss: 0.596417, acc.: 64.84%] [G loss: 1.021717]\n",
      "epoch:36 step:34635 [D loss: 0.566622, acc.: 67.97%] [G loss: 1.607459]\n",
      "epoch:36 step:34636 [D loss: 0.734490, acc.: 59.38%] [G loss: 1.574154]\n",
      "epoch:36 step:34637 [D loss: 0.477252, acc.: 78.12%] [G loss: 2.097402]\n",
      "epoch:36 step:34638 [D loss: 0.584297, acc.: 66.41%] [G loss: 1.778838]\n",
      "epoch:36 step:34639 [D loss: 0.397783, acc.: 86.72%] [G loss: 1.673591]\n",
      "epoch:36 step:34640 [D loss: 0.590470, acc.: 67.97%] [G loss: 1.749325]\n",
      "epoch:36 step:34641 [D loss: 0.597573, acc.: 67.19%] [G loss: 1.569900]\n",
      "epoch:36 step:34642 [D loss: 0.466144, acc.: 82.81%] [G loss: 1.379337]\n",
      "epoch:36 step:34643 [D loss: 0.492209, acc.: 75.78%] [G loss: 2.088306]\n",
      "epoch:36 step:34644 [D loss: 0.697621, acc.: 55.47%] [G loss: 1.163349]\n",
      "epoch:36 step:34645 [D loss: 0.553656, acc.: 75.78%] [G loss: 1.848490]\n",
      "epoch:36 step:34646 [D loss: 0.581922, acc.: 70.31%] [G loss: 1.487973]\n",
      "epoch:36 step:34647 [D loss: 0.445847, acc.: 78.12%] [G loss: 1.349152]\n",
      "epoch:36 step:34648 [D loss: 0.411879, acc.: 81.25%] [G loss: 1.747478]\n",
      "epoch:36 step:34649 [D loss: 0.604348, acc.: 65.62%] [G loss: 1.490669]\n",
      "epoch:36 step:34650 [D loss: 0.365144, acc.: 86.72%] [G loss: 1.451019]\n",
      "epoch:36 step:34651 [D loss: 0.489303, acc.: 75.78%] [G loss: 1.184071]\n",
      "epoch:36 step:34652 [D loss: 0.579881, acc.: 67.19%] [G loss: 1.437476]\n",
      "epoch:36 step:34653 [D loss: 0.556245, acc.: 70.31%] [G loss: 1.190770]\n",
      "epoch:36 step:34654 [D loss: 0.523733, acc.: 78.12%] [G loss: 1.442778]\n",
      "epoch:36 step:34655 [D loss: 0.603274, acc.: 66.41%] [G loss: 1.380321]\n",
      "epoch:36 step:34656 [D loss: 0.614466, acc.: 63.28%] [G loss: 1.404112]\n",
      "epoch:36 step:34657 [D loss: 0.771182, acc.: 53.91%] [G loss: 1.115284]\n",
      "epoch:36 step:34658 [D loss: 0.429561, acc.: 85.16%] [G loss: 1.645845]\n",
      "epoch:36 step:34659 [D loss: 0.724025, acc.: 53.91%] [G loss: 1.104382]\n",
      "epoch:36 step:34660 [D loss: 0.377184, acc.: 82.81%] [G loss: 1.798414]\n",
      "epoch:36 step:34661 [D loss: 0.797000, acc.: 54.69%] [G loss: 2.005381]\n",
      "epoch:36 step:34662 [D loss: 0.666184, acc.: 60.94%] [G loss: 1.594895]\n",
      "epoch:36 step:34663 [D loss: 0.864453, acc.: 46.88%] [G loss: 2.041087]\n",
      "epoch:36 step:34664 [D loss: 0.344238, acc.: 88.28%] [G loss: 1.775261]\n",
      "epoch:36 step:34665 [D loss: 0.477049, acc.: 76.56%] [G loss: 1.259246]\n",
      "epoch:36 step:34666 [D loss: 0.430731, acc.: 81.25%] [G loss: 1.492435]\n",
      "epoch:36 step:34667 [D loss: 0.696378, acc.: 61.72%] [G loss: 1.338277]\n",
      "epoch:36 step:34668 [D loss: 0.484375, acc.: 78.12%] [G loss: 1.228161]\n",
      "epoch:36 step:34669 [D loss: 0.443370, acc.: 81.25%] [G loss: 1.517306]\n",
      "epoch:37 step:34670 [D loss: 0.581107, acc.: 66.41%] [G loss: 1.427506]\n",
      "epoch:37 step:34671 [D loss: 0.521276, acc.: 76.56%] [G loss: 1.455066]\n",
      "epoch:37 step:34672 [D loss: 0.528429, acc.: 78.91%] [G loss: 1.333665]\n",
      "epoch:37 step:34673 [D loss: 0.468687, acc.: 75.00%] [G loss: 1.298306]\n",
      "epoch:37 step:34674 [D loss: 0.566795, acc.: 68.75%] [G loss: 2.065351]\n",
      "epoch:37 step:34675 [D loss: 0.544212, acc.: 70.31%] [G loss: 1.669637]\n",
      "epoch:37 step:34676 [D loss: 0.637407, acc.: 62.50%] [G loss: 1.047428]\n",
      "epoch:37 step:34677 [D loss: 0.395417, acc.: 82.81%] [G loss: 1.883805]\n",
      "epoch:37 step:34678 [D loss: 0.310918, acc.: 95.31%] [G loss: 2.150502]\n",
      "epoch:37 step:34679 [D loss: 0.679292, acc.: 58.59%] [G loss: 1.451123]\n",
      "epoch:37 step:34680 [D loss: 0.598856, acc.: 66.41%] [G loss: 1.490704]\n",
      "epoch:37 step:34681 [D loss: 0.474620, acc.: 79.69%] [G loss: 1.260879]\n",
      "epoch:37 step:34682 [D loss: 0.755723, acc.: 57.81%] [G loss: 1.282608]\n",
      "epoch:37 step:34683 [D loss: 0.385753, acc.: 85.94%] [G loss: 1.589844]\n",
      "epoch:37 step:34684 [D loss: 0.590221, acc.: 68.75%] [G loss: 1.390716]\n",
      "epoch:37 step:34685 [D loss: 0.588184, acc.: 68.75%] [G loss: 1.403830]\n",
      "epoch:37 step:34686 [D loss: 0.609989, acc.: 67.19%] [G loss: 1.403754]\n",
      "epoch:37 step:34687 [D loss: 0.520232, acc.: 77.34%] [G loss: 1.541326]\n",
      "epoch:37 step:34688 [D loss: 0.719789, acc.: 59.38%] [G loss: 1.698954]\n",
      "epoch:37 step:34689 [D loss: 0.569868, acc.: 72.66%] [G loss: 1.498202]\n",
      "epoch:37 step:34690 [D loss: 0.411485, acc.: 82.81%] [G loss: 1.572105]\n",
      "epoch:37 step:34691 [D loss: 0.485797, acc.: 77.34%] [G loss: 1.718091]\n",
      "epoch:37 step:34692 [D loss: 0.471664, acc.: 78.12%] [G loss: 1.328987]\n",
      "epoch:37 step:34693 [D loss: 0.506455, acc.: 76.56%] [G loss: 1.245383]\n",
      "epoch:37 step:34694 [D loss: 0.396198, acc.: 89.84%] [G loss: 2.103816]\n",
      "epoch:37 step:34695 [D loss: 0.618097, acc.: 65.62%] [G loss: 1.187859]\n",
      "epoch:37 step:34696 [D loss: 0.554173, acc.: 64.84%] [G loss: 1.111632]\n",
      "epoch:37 step:34697 [D loss: 0.509704, acc.: 80.47%] [G loss: 1.959716]\n",
      "epoch:37 step:34698 [D loss: 0.513855, acc.: 76.56%] [G loss: 1.349223]\n",
      "epoch:37 step:34699 [D loss: 0.660068, acc.: 60.94%] [G loss: 1.805510]\n",
      "epoch:37 step:34700 [D loss: 0.453899, acc.: 75.78%] [G loss: 1.473562]\n",
      "epoch:37 step:34701 [D loss: 0.491112, acc.: 76.56%] [G loss: 2.004065]\n",
      "epoch:37 step:34702 [D loss: 0.711357, acc.: 57.03%] [G loss: 1.412307]\n",
      "epoch:37 step:34703 [D loss: 0.478971, acc.: 77.34%] [G loss: 1.175035]\n",
      "epoch:37 step:34704 [D loss: 0.641412, acc.: 68.75%] [G loss: 0.959236]\n",
      "epoch:37 step:34705 [D loss: 0.480089, acc.: 79.69%] [G loss: 1.455541]\n",
      "epoch:37 step:34706 [D loss: 0.656623, acc.: 62.50%] [G loss: 1.397814]\n",
      "epoch:37 step:34707 [D loss: 0.500339, acc.: 78.91%] [G loss: 1.568999]\n",
      "epoch:37 step:34708 [D loss: 0.490642, acc.: 78.91%] [G loss: 1.610129]\n",
      "epoch:37 step:34709 [D loss: 0.476036, acc.: 80.47%] [G loss: 1.766678]\n",
      "epoch:37 step:34710 [D loss: 0.481013, acc.: 78.12%] [G loss: 1.649314]\n",
      "epoch:37 step:34711 [D loss: 0.504583, acc.: 73.44%] [G loss: 1.630472]\n",
      "epoch:37 step:34712 [D loss: 0.560725, acc.: 69.53%] [G loss: 1.300511]\n",
      "epoch:37 step:34713 [D loss: 0.398162, acc.: 85.16%] [G loss: 1.310216]\n",
      "epoch:37 step:34714 [D loss: 0.549595, acc.: 73.44%] [G loss: 1.399657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34715 [D loss: 0.552978, acc.: 74.22%] [G loss: 1.799404]\n",
      "epoch:37 step:34716 [D loss: 0.616205, acc.: 74.22%] [G loss: 1.880994]\n",
      "epoch:37 step:34717 [D loss: 0.540118, acc.: 75.78%] [G loss: 1.489546]\n",
      "epoch:37 step:34718 [D loss: 0.856247, acc.: 42.97%] [G loss: 1.747135]\n",
      "epoch:37 step:34719 [D loss: 0.504354, acc.: 74.22%] [G loss: 1.373127]\n",
      "epoch:37 step:34720 [D loss: 0.488345, acc.: 76.56%] [G loss: 1.637267]\n",
      "epoch:37 step:34721 [D loss: 0.594440, acc.: 67.97%] [G loss: 0.854534]\n",
      "epoch:37 step:34722 [D loss: 0.442918, acc.: 78.91%] [G loss: 1.795007]\n",
      "epoch:37 step:34723 [D loss: 0.372209, acc.: 85.16%] [G loss: 1.281854]\n",
      "epoch:37 step:34724 [D loss: 0.582968, acc.: 67.19%] [G loss: 1.410864]\n",
      "epoch:37 step:34725 [D loss: 0.773206, acc.: 58.59%] [G loss: 1.368955]\n",
      "epoch:37 step:34726 [D loss: 0.402656, acc.: 84.38%] [G loss: 1.930866]\n",
      "epoch:37 step:34727 [D loss: 0.500662, acc.: 71.88%] [G loss: 1.616845]\n",
      "epoch:37 step:34728 [D loss: 0.363755, acc.: 84.38%] [G loss: 1.471470]\n",
      "epoch:37 step:34729 [D loss: 0.592063, acc.: 68.75%] [G loss: 1.599042]\n",
      "epoch:37 step:34730 [D loss: 0.486409, acc.: 78.91%] [G loss: 1.587493]\n",
      "epoch:37 step:34731 [D loss: 0.407882, acc.: 85.94%] [G loss: 1.241760]\n",
      "epoch:37 step:34732 [D loss: 0.438080, acc.: 82.81%] [G loss: 2.121826]\n",
      "epoch:37 step:34733 [D loss: 0.519788, acc.: 74.22%] [G loss: 1.659544]\n",
      "epoch:37 step:34734 [D loss: 0.553538, acc.: 73.44%] [G loss: 2.035843]\n",
      "epoch:37 step:34735 [D loss: 0.720487, acc.: 60.94%] [G loss: 1.347787]\n",
      "epoch:37 step:34736 [D loss: 0.423687, acc.: 82.81%] [G loss: 2.240038]\n",
      "epoch:37 step:34737 [D loss: 0.537821, acc.: 69.53%] [G loss: 1.666556]\n",
      "epoch:37 step:34738 [D loss: 0.325716, acc.: 91.41%] [G loss: 1.333866]\n",
      "epoch:37 step:34739 [D loss: 0.396190, acc.: 86.72%] [G loss: 1.277457]\n",
      "epoch:37 step:34740 [D loss: 0.662262, acc.: 60.16%] [G loss: 1.217088]\n",
      "epoch:37 step:34741 [D loss: 0.494363, acc.: 80.47%] [G loss: 1.452525]\n",
      "epoch:37 step:34742 [D loss: 0.405916, acc.: 85.94%] [G loss: 1.311400]\n",
      "epoch:37 step:34743 [D loss: 0.382186, acc.: 85.94%] [G loss: 1.822762]\n",
      "epoch:37 step:34744 [D loss: 0.757696, acc.: 59.38%] [G loss: 1.508643]\n",
      "epoch:37 step:34745 [D loss: 0.533326, acc.: 71.88%] [G loss: 1.769867]\n",
      "epoch:37 step:34746 [D loss: 0.463263, acc.: 76.56%] [G loss: 1.273924]\n",
      "epoch:37 step:34747 [D loss: 0.492014, acc.: 77.34%] [G loss: 1.023756]\n",
      "epoch:37 step:34748 [D loss: 0.520018, acc.: 75.78%] [G loss: 1.048523]\n",
      "epoch:37 step:34749 [D loss: 0.356836, acc.: 85.16%] [G loss: 1.757651]\n",
      "epoch:37 step:34750 [D loss: 0.633282, acc.: 63.28%] [G loss: 1.789255]\n",
      "epoch:37 step:34751 [D loss: 0.539671, acc.: 74.22%] [G loss: 1.347880]\n",
      "epoch:37 step:34752 [D loss: 0.481498, acc.: 75.78%] [G loss: 1.464839]\n",
      "epoch:37 step:34753 [D loss: 0.546799, acc.: 70.31%] [G loss: 1.472630]\n",
      "epoch:37 step:34754 [D loss: 0.421151, acc.: 84.38%] [G loss: 1.711720]\n",
      "epoch:37 step:34755 [D loss: 0.473997, acc.: 79.69%] [G loss: 1.693585]\n",
      "epoch:37 step:34756 [D loss: 0.393967, acc.: 82.03%] [G loss: 1.695369]\n",
      "epoch:37 step:34757 [D loss: 0.523067, acc.: 75.00%] [G loss: 1.678125]\n",
      "epoch:37 step:34758 [D loss: 0.609616, acc.: 70.31%] [G loss: 1.659047]\n",
      "epoch:37 step:34759 [D loss: 0.495012, acc.: 76.56%] [G loss: 1.587905]\n",
      "epoch:37 step:34760 [D loss: 0.476174, acc.: 78.12%] [G loss: 1.752432]\n",
      "epoch:37 step:34761 [D loss: 0.441971, acc.: 78.91%] [G loss: 1.517789]\n",
      "epoch:37 step:34762 [D loss: 0.505794, acc.: 77.34%] [G loss: 1.755892]\n",
      "epoch:37 step:34763 [D loss: 0.512569, acc.: 73.44%] [G loss: 1.667980]\n",
      "epoch:37 step:34764 [D loss: 0.733522, acc.: 53.12%] [G loss: 1.652513]\n",
      "epoch:37 step:34765 [D loss: 0.450390, acc.: 82.03%] [G loss: 1.833909]\n",
      "epoch:37 step:34766 [D loss: 0.556453, acc.: 78.12%] [G loss: 1.129145]\n",
      "epoch:37 step:34767 [D loss: 0.428436, acc.: 80.47%] [G loss: 1.622728]\n",
      "epoch:37 step:34768 [D loss: 0.457615, acc.: 82.03%] [G loss: 1.584931]\n",
      "epoch:37 step:34769 [D loss: 0.500374, acc.: 79.69%] [G loss: 1.337640]\n",
      "epoch:37 step:34770 [D loss: 0.325954, acc.: 89.06%] [G loss: 1.806340]\n",
      "epoch:37 step:34771 [D loss: 0.547425, acc.: 75.78%] [G loss: 1.426695]\n",
      "epoch:37 step:34772 [D loss: 0.539487, acc.: 75.78%] [G loss: 1.582923]\n",
      "epoch:37 step:34773 [D loss: 0.307002, acc.: 92.97%] [G loss: 1.970338]\n",
      "epoch:37 step:34774 [D loss: 0.345138, acc.: 89.06%] [G loss: 1.452917]\n",
      "epoch:37 step:34775 [D loss: 0.628800, acc.: 64.06%] [G loss: 1.505206]\n",
      "epoch:37 step:34776 [D loss: 0.699848, acc.: 60.94%] [G loss: 1.089630]\n",
      "epoch:37 step:34777 [D loss: 0.428453, acc.: 79.69%] [G loss: 1.747900]\n",
      "epoch:37 step:34778 [D loss: 0.484120, acc.: 80.47%] [G loss: 1.959609]\n",
      "epoch:37 step:34779 [D loss: 0.467344, acc.: 82.03%] [G loss: 1.253516]\n",
      "epoch:37 step:34780 [D loss: 0.667733, acc.: 61.72%] [G loss: 1.592184]\n",
      "epoch:37 step:34781 [D loss: 0.457775, acc.: 76.56%] [G loss: 1.422536]\n",
      "epoch:37 step:34782 [D loss: 0.561092, acc.: 68.75%] [G loss: 1.656935]\n",
      "epoch:37 step:34783 [D loss: 0.409581, acc.: 85.16%] [G loss: 1.242707]\n",
      "epoch:37 step:34784 [D loss: 0.504949, acc.: 71.88%] [G loss: 1.800081]\n",
      "epoch:37 step:34785 [D loss: 0.446919, acc.: 82.03%] [G loss: 1.293025]\n",
      "epoch:37 step:34786 [D loss: 0.736778, acc.: 58.59%] [G loss: 1.651291]\n",
      "epoch:37 step:34787 [D loss: 0.603456, acc.: 64.06%] [G loss: 1.049736]\n",
      "epoch:37 step:34788 [D loss: 0.364205, acc.: 85.94%] [G loss: 1.461925]\n",
      "epoch:37 step:34789 [D loss: 0.558130, acc.: 68.75%] [G loss: 1.497981]\n",
      "epoch:37 step:34790 [D loss: 0.617389, acc.: 61.72%] [G loss: 1.900514]\n",
      "epoch:37 step:34791 [D loss: 0.450369, acc.: 78.12%] [G loss: 2.151442]\n",
      "epoch:37 step:34792 [D loss: 0.553572, acc.: 74.22%] [G loss: 1.429517]\n",
      "epoch:37 step:34793 [D loss: 0.663387, acc.: 65.62%] [G loss: 1.640933]\n",
      "epoch:37 step:34794 [D loss: 0.438078, acc.: 78.12%] [G loss: 1.527042]\n",
      "epoch:37 step:34795 [D loss: 0.730453, acc.: 51.56%] [G loss: 1.404410]\n",
      "epoch:37 step:34796 [D loss: 0.573070, acc.: 73.44%] [G loss: 1.296545]\n",
      "epoch:37 step:34797 [D loss: 0.375305, acc.: 90.62%] [G loss: 1.855125]\n",
      "epoch:37 step:34798 [D loss: 0.466086, acc.: 79.69%] [G loss: 1.215150]\n",
      "epoch:37 step:34799 [D loss: 0.516221, acc.: 75.00%] [G loss: 1.669332]\n",
      "epoch:37 step:34800 [D loss: 0.462390, acc.: 82.81%] [G loss: 1.857947]\n",
      "##############\n",
      "[2.7225209  2.0618203  1.87548127 2.86786675 0.70829658 6.28566487\n",
      " 2.21581799 2.56619926 3.95137498 4.79237283]\n",
      "##########\n",
      "epoch:37 step:34801 [D loss: 0.499754, acc.: 71.88%] [G loss: 1.403721]\n",
      "epoch:37 step:34802 [D loss: 0.379868, acc.: 84.38%] [G loss: 1.444848]\n",
      "epoch:37 step:34803 [D loss: 0.517604, acc.: 75.78%] [G loss: 1.653920]\n",
      "epoch:37 step:34804 [D loss: 0.420570, acc.: 82.03%] [G loss: 1.834212]\n",
      "epoch:37 step:34805 [D loss: 0.555988, acc.: 75.00%] [G loss: 1.437851]\n",
      "epoch:37 step:34806 [D loss: 0.433370, acc.: 78.91%] [G loss: 1.725523]\n",
      "epoch:37 step:34807 [D loss: 0.580269, acc.: 69.53%] [G loss: 1.096232]\n",
      "epoch:37 step:34808 [D loss: 0.522168, acc.: 75.78%] [G loss: 1.819329]\n",
      "epoch:37 step:34809 [D loss: 0.427028, acc.: 82.81%] [G loss: 1.549214]\n",
      "epoch:37 step:34810 [D loss: 0.620793, acc.: 64.84%] [G loss: 1.228323]\n",
      "epoch:37 step:34811 [D loss: 0.440901, acc.: 76.56%] [G loss: 1.432894]\n",
      "epoch:37 step:34812 [D loss: 0.562515, acc.: 70.31%] [G loss: 1.603791]\n",
      "epoch:37 step:34813 [D loss: 0.704349, acc.: 60.16%] [G loss: 0.993843]\n",
      "epoch:37 step:34814 [D loss: 0.465652, acc.: 81.25%] [G loss: 1.623298]\n",
      "epoch:37 step:34815 [D loss: 0.463283, acc.: 77.34%] [G loss: 1.990942]\n",
      "epoch:37 step:34816 [D loss: 0.268526, acc.: 93.75%] [G loss: 1.278244]\n",
      "epoch:37 step:34817 [D loss: 0.484252, acc.: 78.12%] [G loss: 1.368332]\n",
      "epoch:37 step:34818 [D loss: 0.537658, acc.: 71.88%] [G loss: 1.301708]\n",
      "epoch:37 step:34819 [D loss: 0.572214, acc.: 67.97%] [G loss: 1.677042]\n",
      "epoch:37 step:34820 [D loss: 0.327949, acc.: 88.28%] [G loss: 1.162247]\n",
      "epoch:37 step:34821 [D loss: 0.843518, acc.: 46.88%] [G loss: 1.154663]\n",
      "epoch:37 step:34822 [D loss: 0.585554, acc.: 73.44%] [G loss: 1.417986]\n",
      "epoch:37 step:34823 [D loss: 0.481047, acc.: 79.69%] [G loss: 1.722030]\n",
      "epoch:37 step:34824 [D loss: 0.469094, acc.: 77.34%] [G loss: 2.158421]\n",
      "epoch:37 step:34825 [D loss: 0.479028, acc.: 81.25%] [G loss: 1.524038]\n",
      "epoch:37 step:34826 [D loss: 0.766001, acc.: 57.03%] [G loss: 1.515394]\n",
      "epoch:37 step:34827 [D loss: 0.709542, acc.: 56.25%] [G loss: 1.236646]\n",
      "epoch:37 step:34828 [D loss: 0.518499, acc.: 75.00%] [G loss: 1.099114]\n",
      "epoch:37 step:34829 [D loss: 0.630857, acc.: 60.16%] [G loss: 1.348363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34830 [D loss: 0.397744, acc.: 83.59%] [G loss: 1.539150]\n",
      "epoch:37 step:34831 [D loss: 0.489591, acc.: 77.34%] [G loss: 1.861502]\n",
      "epoch:37 step:34832 [D loss: 0.570261, acc.: 66.41%] [G loss: 1.651546]\n",
      "epoch:37 step:34833 [D loss: 0.399691, acc.: 85.16%] [G loss: 1.276439]\n",
      "epoch:37 step:34834 [D loss: 0.305722, acc.: 89.06%] [G loss: 1.906824]\n",
      "epoch:37 step:34835 [D loss: 0.616098, acc.: 65.62%] [G loss: 1.745884]\n",
      "epoch:37 step:34836 [D loss: 0.550726, acc.: 73.44%] [G loss: 1.006431]\n",
      "epoch:37 step:34837 [D loss: 0.463358, acc.: 80.47%] [G loss: 1.243721]\n",
      "epoch:37 step:34838 [D loss: 0.484477, acc.: 78.12%] [G loss: 1.613390]\n",
      "epoch:37 step:34839 [D loss: 0.453374, acc.: 76.56%] [G loss: 1.513108]\n",
      "epoch:37 step:34840 [D loss: 0.693398, acc.: 59.38%] [G loss: 1.159867]\n",
      "epoch:37 step:34841 [D loss: 0.427626, acc.: 82.03%] [G loss: 1.808963]\n",
      "epoch:37 step:34842 [D loss: 0.594921, acc.: 68.75%] [G loss: 1.750506]\n",
      "epoch:37 step:34843 [D loss: 0.480999, acc.: 75.78%] [G loss: 1.192113]\n",
      "epoch:37 step:34844 [D loss: 0.471580, acc.: 80.47%] [G loss: 1.015060]\n",
      "epoch:37 step:34845 [D loss: 0.844628, acc.: 51.56%] [G loss: 1.263275]\n",
      "epoch:37 step:34846 [D loss: 0.418754, acc.: 84.38%] [G loss: 1.419686]\n",
      "epoch:37 step:34847 [D loss: 0.433153, acc.: 82.03%] [G loss: 1.677080]\n",
      "epoch:37 step:34848 [D loss: 0.731997, acc.: 53.12%] [G loss: 0.899557]\n",
      "epoch:37 step:34849 [D loss: 0.364172, acc.: 89.84%] [G loss: 2.119412]\n",
      "epoch:37 step:34850 [D loss: 0.469957, acc.: 77.34%] [G loss: 1.862149]\n",
      "epoch:37 step:34851 [D loss: 0.435164, acc.: 82.81%] [G loss: 1.341612]\n",
      "epoch:37 step:34852 [D loss: 0.576467, acc.: 68.75%] [G loss: 1.472616]\n",
      "epoch:37 step:34853 [D loss: 0.435027, acc.: 81.25%] [G loss: 1.688104]\n",
      "epoch:37 step:34854 [D loss: 0.527134, acc.: 74.22%] [G loss: 1.521084]\n",
      "epoch:37 step:34855 [D loss: 0.442979, acc.: 82.03%] [G loss: 1.335970]\n",
      "epoch:37 step:34856 [D loss: 0.311995, acc.: 91.41%] [G loss: 1.511506]\n",
      "epoch:37 step:34857 [D loss: 0.466814, acc.: 79.69%] [G loss: 1.278852]\n",
      "epoch:37 step:34858 [D loss: 0.520605, acc.: 71.88%] [G loss: 1.642538]\n",
      "epoch:37 step:34859 [D loss: 0.477070, acc.: 75.00%] [G loss: 1.575312]\n",
      "epoch:37 step:34860 [D loss: 0.736237, acc.: 60.94%] [G loss: 1.227190]\n",
      "epoch:37 step:34861 [D loss: 0.630453, acc.: 68.75%] [G loss: 1.510786]\n",
      "epoch:37 step:34862 [D loss: 0.423531, acc.: 79.69%] [G loss: 1.838428]\n",
      "epoch:37 step:34863 [D loss: 0.592139, acc.: 64.84%] [G loss: 1.472739]\n",
      "epoch:37 step:34864 [D loss: 0.576203, acc.: 66.41%] [G loss: 1.333097]\n",
      "epoch:37 step:34865 [D loss: 0.466994, acc.: 78.91%] [G loss: 1.259869]\n",
      "epoch:37 step:34866 [D loss: 0.510012, acc.: 78.91%] [G loss: 1.774443]\n",
      "epoch:37 step:34867 [D loss: 0.344371, acc.: 85.94%] [G loss: 1.781684]\n",
      "epoch:37 step:34868 [D loss: 0.434708, acc.: 82.03%] [G loss: 1.816601]\n",
      "epoch:37 step:34869 [D loss: 0.540558, acc.: 75.00%] [G loss: 1.940865]\n",
      "epoch:37 step:34870 [D loss: 0.493836, acc.: 75.00%] [G loss: 2.037575]\n",
      "epoch:37 step:34871 [D loss: 0.525193, acc.: 75.00%] [G loss: 1.787167]\n",
      "epoch:37 step:34872 [D loss: 0.494041, acc.: 76.56%] [G loss: 1.827065]\n",
      "epoch:37 step:34873 [D loss: 0.271641, acc.: 92.97%] [G loss: 2.210166]\n",
      "epoch:37 step:34874 [D loss: 0.691198, acc.: 63.28%] [G loss: 1.180179]\n",
      "epoch:37 step:34875 [D loss: 0.399184, acc.: 84.38%] [G loss: 1.170342]\n",
      "epoch:37 step:34876 [D loss: 0.592393, acc.: 65.62%] [G loss: 1.360807]\n",
      "epoch:37 step:34877 [D loss: 0.385422, acc.: 82.03%] [G loss: 1.523790]\n",
      "epoch:37 step:34878 [D loss: 0.554822, acc.: 71.09%] [G loss: 1.419889]\n",
      "epoch:37 step:34879 [D loss: 0.284346, acc.: 95.31%] [G loss: 1.830821]\n",
      "epoch:37 step:34880 [D loss: 0.584297, acc.: 68.75%] [G loss: 1.671752]\n",
      "epoch:37 step:34881 [D loss: 0.522190, acc.: 74.22%] [G loss: 1.637441]\n",
      "epoch:37 step:34882 [D loss: 0.514533, acc.: 78.91%] [G loss: 1.244354]\n",
      "epoch:37 step:34883 [D loss: 0.707312, acc.: 58.59%] [G loss: 1.358088]\n",
      "epoch:37 step:34884 [D loss: 0.772865, acc.: 56.25%] [G loss: 1.751316]\n",
      "epoch:37 step:34885 [D loss: 0.490875, acc.: 82.03%] [G loss: 1.215656]\n",
      "epoch:37 step:34886 [D loss: 0.403417, acc.: 83.59%] [G loss: 1.273876]\n",
      "epoch:37 step:34887 [D loss: 0.557819, acc.: 76.56%] [G loss: 1.734710]\n",
      "epoch:37 step:34888 [D loss: 0.383836, acc.: 84.38%] [G loss: 1.701304]\n",
      "epoch:37 step:34889 [D loss: 0.647880, acc.: 61.72%] [G loss: 1.966411]\n",
      "epoch:37 step:34890 [D loss: 0.539143, acc.: 71.88%] [G loss: 1.735999]\n",
      "epoch:37 step:34891 [D loss: 0.702397, acc.: 64.84%] [G loss: 0.818196]\n",
      "epoch:37 step:34892 [D loss: 0.388531, acc.: 88.28%] [G loss: 1.748414]\n",
      "epoch:37 step:34893 [D loss: 0.475193, acc.: 77.34%] [G loss: 1.373339]\n",
      "epoch:37 step:34894 [D loss: 0.456462, acc.: 79.69%] [G loss: 1.617266]\n",
      "epoch:37 step:34895 [D loss: 0.560315, acc.: 71.09%] [G loss: 1.406898]\n",
      "epoch:37 step:34896 [D loss: 0.466057, acc.: 75.78%] [G loss: 1.737427]\n",
      "epoch:37 step:34897 [D loss: 0.468582, acc.: 78.12%] [G loss: 1.734286]\n",
      "epoch:37 step:34898 [D loss: 0.536150, acc.: 73.44%] [G loss: 1.191594]\n",
      "epoch:37 step:34899 [D loss: 0.443765, acc.: 82.03%] [G loss: 1.603307]\n",
      "epoch:37 step:34900 [D loss: 0.479544, acc.: 76.56%] [G loss: 1.350222]\n",
      "epoch:37 step:34901 [D loss: 0.453650, acc.: 77.34%] [G loss: 1.274864]\n",
      "epoch:37 step:34902 [D loss: 0.524001, acc.: 72.66%] [G loss: 1.502952]\n",
      "epoch:37 step:34903 [D loss: 0.371342, acc.: 82.81%] [G loss: 1.722405]\n",
      "epoch:37 step:34904 [D loss: 0.592739, acc.: 68.75%] [G loss: 1.015380]\n",
      "epoch:37 step:34905 [D loss: 0.489971, acc.: 77.34%] [G loss: 1.505096]\n",
      "epoch:37 step:34906 [D loss: 0.737925, acc.: 56.25%] [G loss: 1.366650]\n",
      "epoch:37 step:34907 [D loss: 0.644258, acc.: 67.97%] [G loss: 1.432116]\n",
      "epoch:37 step:34908 [D loss: 0.390410, acc.: 85.94%] [G loss: 1.495414]\n",
      "epoch:37 step:34909 [D loss: 0.718316, acc.: 57.81%] [G loss: 1.316626]\n",
      "epoch:37 step:34910 [D loss: 0.416954, acc.: 81.25%] [G loss: 1.310026]\n",
      "epoch:37 step:34911 [D loss: 0.532914, acc.: 76.56%] [G loss: 1.677881]\n",
      "epoch:37 step:34912 [D loss: 0.498456, acc.: 76.56%] [G loss: 1.758496]\n",
      "epoch:37 step:34913 [D loss: 0.431703, acc.: 79.69%] [G loss: 1.866537]\n",
      "epoch:37 step:34914 [D loss: 0.471623, acc.: 78.12%] [G loss: 1.383282]\n",
      "epoch:37 step:34915 [D loss: 0.404542, acc.: 82.03%] [G loss: 1.564611]\n",
      "epoch:37 step:34916 [D loss: 0.496800, acc.: 78.12%] [G loss: 1.363833]\n",
      "epoch:37 step:34917 [D loss: 0.576128, acc.: 70.31%] [G loss: 1.045800]\n",
      "epoch:37 step:34918 [D loss: 0.390600, acc.: 82.81%] [G loss: 1.927063]\n",
      "epoch:37 step:34919 [D loss: 0.431619, acc.: 86.72%] [G loss: 1.506798]\n",
      "epoch:37 step:34920 [D loss: 0.666737, acc.: 64.84%] [G loss: 2.090734]\n",
      "epoch:37 step:34921 [D loss: 0.423807, acc.: 81.25%] [G loss: 1.851892]\n",
      "epoch:37 step:34922 [D loss: 0.584231, acc.: 65.62%] [G loss: 1.388166]\n",
      "epoch:37 step:34923 [D loss: 0.550381, acc.: 71.09%] [G loss: 1.640695]\n",
      "epoch:37 step:34924 [D loss: 0.507888, acc.: 75.00%] [G loss: 2.016011]\n",
      "epoch:37 step:34925 [D loss: 0.421865, acc.: 80.47%] [G loss: 1.592268]\n",
      "epoch:37 step:34926 [D loss: 0.539412, acc.: 76.56%] [G loss: 1.033934]\n",
      "epoch:37 step:34927 [D loss: 0.550408, acc.: 71.88%] [G loss: 0.953700]\n",
      "epoch:37 step:34928 [D loss: 0.564495, acc.: 72.66%] [G loss: 1.024230]\n",
      "epoch:37 step:34929 [D loss: 0.519959, acc.: 73.44%] [G loss: 1.596105]\n",
      "epoch:37 step:34930 [D loss: 0.613617, acc.: 65.62%] [G loss: 1.861155]\n",
      "epoch:37 step:34931 [D loss: 0.684481, acc.: 62.50%] [G loss: 1.204318]\n",
      "epoch:37 step:34932 [D loss: 0.590852, acc.: 71.09%] [G loss: 1.611191]\n",
      "epoch:37 step:34933 [D loss: 0.447313, acc.: 81.25%] [G loss: 1.686281]\n",
      "epoch:37 step:34934 [D loss: 0.402769, acc.: 82.81%] [G loss: 1.766737]\n",
      "epoch:37 step:34935 [D loss: 0.382339, acc.: 85.94%] [G loss: 1.522833]\n",
      "epoch:37 step:34936 [D loss: 0.398130, acc.: 85.16%] [G loss: 1.706317]\n",
      "epoch:37 step:34937 [D loss: 0.281372, acc.: 94.53%] [G loss: 1.422975]\n",
      "epoch:37 step:34938 [D loss: 0.453249, acc.: 82.81%] [G loss: 1.255087]\n",
      "epoch:37 step:34939 [D loss: 0.516102, acc.: 71.88%] [G loss: 1.367446]\n",
      "epoch:37 step:34940 [D loss: 0.319194, acc.: 92.97%] [G loss: 1.513722]\n",
      "epoch:37 step:34941 [D loss: 0.442252, acc.: 82.03%] [G loss: 1.095760]\n",
      "epoch:37 step:34942 [D loss: 0.611947, acc.: 71.09%] [G loss: 1.514871]\n",
      "epoch:37 step:34943 [D loss: 0.398756, acc.: 83.59%] [G loss: 1.936393]\n",
      "epoch:37 step:34944 [D loss: 0.649977, acc.: 62.50%] [G loss: 1.277526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34945 [D loss: 0.604001, acc.: 68.75%] [G loss: 1.361717]\n",
      "epoch:37 step:34946 [D loss: 0.480827, acc.: 74.22%] [G loss: 1.558278]\n",
      "epoch:37 step:34947 [D loss: 0.558797, acc.: 71.88%] [G loss: 1.326801]\n",
      "epoch:37 step:34948 [D loss: 0.528446, acc.: 76.56%] [G loss: 1.486391]\n",
      "epoch:37 step:34949 [D loss: 0.616905, acc.: 66.41%] [G loss: 1.805275]\n",
      "epoch:37 step:34950 [D loss: 0.334623, acc.: 89.06%] [G loss: 1.354815]\n",
      "epoch:37 step:34951 [D loss: 0.522376, acc.: 77.34%] [G loss: 1.421804]\n",
      "epoch:37 step:34952 [D loss: 0.574377, acc.: 72.66%] [G loss: 1.374127]\n",
      "epoch:37 step:34953 [D loss: 0.513778, acc.: 72.66%] [G loss: 1.525416]\n",
      "epoch:37 step:34954 [D loss: 0.512447, acc.: 73.44%] [G loss: 1.665638]\n",
      "epoch:37 step:34955 [D loss: 0.436303, acc.: 82.03%] [G loss: 1.641357]\n",
      "epoch:37 step:34956 [D loss: 0.407741, acc.: 82.81%] [G loss: 1.649652]\n",
      "epoch:37 step:34957 [D loss: 0.564453, acc.: 68.75%] [G loss: 1.247470]\n",
      "epoch:37 step:34958 [D loss: 0.698349, acc.: 57.03%] [G loss: 1.629457]\n",
      "epoch:37 step:34959 [D loss: 0.424717, acc.: 82.81%] [G loss: 1.827184]\n",
      "epoch:37 step:34960 [D loss: 0.599102, acc.: 64.84%] [G loss: 1.413092]\n",
      "epoch:37 step:34961 [D loss: 0.397385, acc.: 85.16%] [G loss: 1.657173]\n",
      "epoch:37 step:34962 [D loss: 0.581114, acc.: 71.88%] [G loss: 1.960135]\n",
      "epoch:37 step:34963 [D loss: 0.497421, acc.: 75.78%] [G loss: 1.586156]\n",
      "epoch:37 step:34964 [D loss: 0.752559, acc.: 53.12%] [G loss: 1.738528]\n",
      "epoch:37 step:34965 [D loss: 0.615321, acc.: 68.75%] [G loss: 1.309095]\n",
      "epoch:37 step:34966 [D loss: 0.380365, acc.: 86.72%] [G loss: 2.009678]\n",
      "epoch:37 step:34967 [D loss: 0.570553, acc.: 67.19%] [G loss: 1.885672]\n",
      "epoch:37 step:34968 [D loss: 0.767374, acc.: 54.69%] [G loss: 0.969736]\n",
      "epoch:37 step:34969 [D loss: 0.421265, acc.: 85.16%] [G loss: 1.417518]\n",
      "epoch:37 step:34970 [D loss: 0.552611, acc.: 71.09%] [G loss: 0.906839]\n",
      "epoch:37 step:34971 [D loss: 0.600936, acc.: 69.53%] [G loss: 1.329842]\n",
      "epoch:37 step:34972 [D loss: 0.540586, acc.: 74.22%] [G loss: 1.427457]\n",
      "epoch:37 step:34973 [D loss: 0.645150, acc.: 67.97%] [G loss: 1.378362]\n",
      "epoch:37 step:34974 [D loss: 0.607275, acc.: 71.09%] [G loss: 1.369206]\n",
      "epoch:37 step:34975 [D loss: 0.371587, acc.: 82.81%] [G loss: 1.723731]\n",
      "epoch:37 step:34976 [D loss: 0.512990, acc.: 73.44%] [G loss: 1.776869]\n",
      "epoch:37 step:34977 [D loss: 0.516847, acc.: 77.34%] [G loss: 1.508776]\n",
      "epoch:37 step:34978 [D loss: 0.483768, acc.: 75.00%] [G loss: 1.033272]\n",
      "epoch:37 step:34979 [D loss: 0.533547, acc.: 73.44%] [G loss: 1.702374]\n",
      "epoch:37 step:34980 [D loss: 0.536604, acc.: 71.88%] [G loss: 1.673849]\n",
      "epoch:37 step:34981 [D loss: 0.466653, acc.: 78.12%] [G loss: 1.952007]\n",
      "epoch:37 step:34982 [D loss: 0.459516, acc.: 78.12%] [G loss: 1.672929]\n",
      "epoch:37 step:34983 [D loss: 0.470197, acc.: 76.56%] [G loss: 2.059489]\n",
      "epoch:37 step:34984 [D loss: 0.385308, acc.: 86.72%] [G loss: 2.120907]\n",
      "epoch:37 step:34985 [D loss: 0.557056, acc.: 69.53%] [G loss: 1.805103]\n",
      "epoch:37 step:34986 [D loss: 0.444330, acc.: 80.47%] [G loss: 1.661249]\n",
      "epoch:37 step:34987 [D loss: 0.641254, acc.: 65.62%] [G loss: 1.359162]\n",
      "epoch:37 step:34988 [D loss: 0.418083, acc.: 85.16%] [G loss: 1.603003]\n",
      "epoch:37 step:34989 [D loss: 0.371916, acc.: 88.28%] [G loss: 2.207475]\n",
      "epoch:37 step:34990 [D loss: 0.491349, acc.: 78.12%] [G loss: 1.724955]\n",
      "epoch:37 step:34991 [D loss: 0.601936, acc.: 69.53%] [G loss: 1.459189]\n",
      "epoch:37 step:34992 [D loss: 0.505103, acc.: 75.00%] [G loss: 1.055698]\n",
      "epoch:37 step:34993 [D loss: 0.532751, acc.: 78.12%] [G loss: 1.934305]\n",
      "epoch:37 step:34994 [D loss: 0.478788, acc.: 77.34%] [G loss: 1.347020]\n",
      "epoch:37 step:34995 [D loss: 0.481615, acc.: 75.78%] [G loss: 0.947597]\n",
      "epoch:37 step:34996 [D loss: 0.400074, acc.: 82.81%] [G loss: 1.069203]\n",
      "epoch:37 step:34997 [D loss: 0.455297, acc.: 78.91%] [G loss: 1.212355]\n",
      "epoch:37 step:34998 [D loss: 0.347220, acc.: 89.84%] [G loss: 1.330058]\n",
      "epoch:37 step:34999 [D loss: 0.422751, acc.: 80.47%] [G loss: 1.834420]\n",
      "epoch:37 step:35000 [D loss: 0.631526, acc.: 70.31%] [G loss: 1.927169]\n",
      "##############\n",
      "[2.60064017 2.00664703 2.0542173  2.98891052 0.76344846 6.47906569\n",
      " 2.30930308 2.70310811 4.06872315 8.14868929]\n",
      "##########\n",
      "epoch:37 step:35001 [D loss: 0.514450, acc.: 80.47%] [G loss: 1.636057]\n",
      "epoch:37 step:35002 [D loss: 0.608037, acc.: 62.50%] [G loss: 1.776918]\n",
      "epoch:37 step:35003 [D loss: 0.481563, acc.: 76.56%] [G loss: 1.639540]\n",
      "epoch:37 step:35004 [D loss: 0.478151, acc.: 77.34%] [G loss: 1.117563]\n",
      "epoch:37 step:35005 [D loss: 0.522718, acc.: 75.00%] [G loss: 1.374264]\n",
      "epoch:37 step:35006 [D loss: 0.553065, acc.: 67.19%] [G loss: 1.743548]\n",
      "epoch:37 step:35007 [D loss: 0.494666, acc.: 75.78%] [G loss: 1.440138]\n",
      "epoch:37 step:35008 [D loss: 0.492310, acc.: 77.34%] [G loss: 1.819250]\n",
      "epoch:37 step:35009 [D loss: 0.522761, acc.: 72.66%] [G loss: 1.382295]\n",
      "epoch:37 step:35010 [D loss: 0.542649, acc.: 73.44%] [G loss: 1.401303]\n",
      "epoch:37 step:35011 [D loss: 0.746568, acc.: 57.03%] [G loss: 1.273698]\n",
      "epoch:37 step:35012 [D loss: 0.567379, acc.: 68.75%] [G loss: 1.959822]\n",
      "epoch:37 step:35013 [D loss: 0.698370, acc.: 60.94%] [G loss: 1.486166]\n",
      "epoch:37 step:35014 [D loss: 0.445573, acc.: 78.91%] [G loss: 1.578884]\n",
      "epoch:37 step:35015 [D loss: 0.673284, acc.: 59.38%] [G loss: 0.962225]\n",
      "epoch:37 step:35016 [D loss: 0.523799, acc.: 72.66%] [G loss: 1.509973]\n",
      "epoch:37 step:35017 [D loss: 0.614716, acc.: 65.62%] [G loss: 1.212766]\n",
      "epoch:37 step:35018 [D loss: 0.469244, acc.: 78.91%] [G loss: 1.877420]\n",
      "epoch:37 step:35019 [D loss: 0.384650, acc.: 81.25%] [G loss: 1.641565]\n",
      "epoch:37 step:35020 [D loss: 0.755267, acc.: 56.25%] [G loss: 2.206930]\n",
      "epoch:37 step:35021 [D loss: 0.329450, acc.: 92.19%] [G loss: 1.116467]\n",
      "epoch:37 step:35022 [D loss: 0.484270, acc.: 74.22%] [G loss: 1.318205]\n",
      "epoch:37 step:35023 [D loss: 0.432961, acc.: 82.03%] [G loss: 1.167873]\n",
      "epoch:37 step:35024 [D loss: 0.498172, acc.: 75.00%] [G loss: 1.256603]\n",
      "epoch:37 step:35025 [D loss: 0.642318, acc.: 66.41%] [G loss: 1.287337]\n",
      "epoch:37 step:35026 [D loss: 0.430504, acc.: 82.03%] [G loss: 1.544032]\n",
      "epoch:37 step:35027 [D loss: 0.628954, acc.: 68.75%] [G loss: 1.394920]\n",
      "epoch:37 step:35028 [D loss: 0.512838, acc.: 73.44%] [G loss: 1.464708]\n",
      "epoch:37 step:35029 [D loss: 0.522767, acc.: 75.78%] [G loss: 1.216712]\n",
      "epoch:37 step:35030 [D loss: 0.530994, acc.: 75.78%] [G loss: 1.638047]\n",
      "epoch:37 step:35031 [D loss: 0.480758, acc.: 78.12%] [G loss: 1.407397]\n",
      "epoch:37 step:35032 [D loss: 0.390215, acc.: 82.81%] [G loss: 2.192087]\n",
      "epoch:37 step:35033 [D loss: 0.487815, acc.: 75.78%] [G loss: 1.646164]\n",
      "epoch:37 step:35034 [D loss: 0.615417, acc.: 68.75%] [G loss: 1.449393]\n",
      "epoch:37 step:35035 [D loss: 0.428393, acc.: 80.47%] [G loss: 2.478994]\n",
      "epoch:37 step:35036 [D loss: 0.492071, acc.: 78.12%] [G loss: 1.219840]\n",
      "epoch:37 step:35037 [D loss: 0.378724, acc.: 84.38%] [G loss: 1.105317]\n",
      "epoch:37 step:35038 [D loss: 0.508627, acc.: 75.00%] [G loss: 1.990615]\n",
      "epoch:37 step:35039 [D loss: 0.414958, acc.: 83.59%] [G loss: 1.352622]\n",
      "epoch:37 step:35040 [D loss: 0.551437, acc.: 73.44%] [G loss: 1.307704]\n",
      "epoch:37 step:35041 [D loss: 0.632771, acc.: 65.62%] [G loss: 1.312545]\n",
      "epoch:37 step:35042 [D loss: 0.556550, acc.: 70.31%] [G loss: 1.708671]\n",
      "epoch:37 step:35043 [D loss: 0.806679, acc.: 51.56%] [G loss: 1.312432]\n",
      "epoch:37 step:35044 [D loss: 0.563377, acc.: 67.97%] [G loss: 1.646012]\n",
      "epoch:37 step:35045 [D loss: 0.666241, acc.: 64.84%] [G loss: 1.550004]\n",
      "epoch:37 step:35046 [D loss: 0.532073, acc.: 71.09%] [G loss: 1.393576]\n",
      "epoch:37 step:35047 [D loss: 0.366235, acc.: 86.72%] [G loss: 1.601090]\n",
      "epoch:37 step:35048 [D loss: 0.515842, acc.: 69.53%] [G loss: 1.435219]\n",
      "epoch:37 step:35049 [D loss: 0.507718, acc.: 80.47%] [G loss: 1.922214]\n",
      "epoch:37 step:35050 [D loss: 0.445413, acc.: 77.34%] [G loss: 2.035375]\n",
      "epoch:37 step:35051 [D loss: 0.758381, acc.: 55.47%] [G loss: 1.312391]\n",
      "epoch:37 step:35052 [D loss: 0.492842, acc.: 75.78%] [G loss: 1.185709]\n",
      "epoch:37 step:35053 [D loss: 0.741045, acc.: 59.38%] [G loss: 1.423255]\n",
      "epoch:37 step:35054 [D loss: 0.626057, acc.: 61.72%] [G loss: 1.091274]\n",
      "epoch:37 step:35055 [D loss: 0.559351, acc.: 71.09%] [G loss: 1.370460]\n",
      "epoch:37 step:35056 [D loss: 0.341996, acc.: 89.06%] [G loss: 1.909120]\n",
      "epoch:37 step:35057 [D loss: 0.327676, acc.: 89.06%] [G loss: 1.153684]\n",
      "epoch:37 step:35058 [D loss: 0.535519, acc.: 76.56%] [G loss: 1.666287]\n",
      "epoch:37 step:35059 [D loss: 0.471571, acc.: 76.56%] [G loss: 1.773333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35060 [D loss: 0.594243, acc.: 62.50%] [G loss: 1.606227]\n",
      "epoch:37 step:35061 [D loss: 0.561713, acc.: 71.88%] [G loss: 2.206508]\n",
      "epoch:37 step:35062 [D loss: 0.615057, acc.: 64.06%] [G loss: 2.154583]\n",
      "epoch:37 step:35063 [D loss: 0.707966, acc.: 57.81%] [G loss: 1.277063]\n",
      "epoch:37 step:35064 [D loss: 0.463440, acc.: 79.69%] [G loss: 1.776623]\n",
      "epoch:37 step:35065 [D loss: 0.666830, acc.: 60.94%] [G loss: 2.282370]\n",
      "epoch:37 step:35066 [D loss: 0.530292, acc.: 69.53%] [G loss: 1.526160]\n",
      "epoch:37 step:35067 [D loss: 0.870643, acc.: 45.31%] [G loss: 1.673545]\n",
      "epoch:37 step:35068 [D loss: 0.463049, acc.: 77.34%] [G loss: 1.716578]\n",
      "epoch:37 step:35069 [D loss: 0.458862, acc.: 80.47%] [G loss: 1.242039]\n",
      "epoch:37 step:35070 [D loss: 0.440603, acc.: 82.03%] [G loss: 2.083443]\n",
      "epoch:37 step:35071 [D loss: 0.540597, acc.: 72.66%] [G loss: 1.834756]\n",
      "epoch:37 step:35072 [D loss: 0.649925, acc.: 65.62%] [G loss: 1.583680]\n",
      "epoch:37 step:35073 [D loss: 0.381011, acc.: 87.50%] [G loss: 1.414047]\n",
      "epoch:37 step:35074 [D loss: 0.558430, acc.: 71.88%] [G loss: 1.553723]\n",
      "epoch:37 step:35075 [D loss: 0.532815, acc.: 72.66%] [G loss: 1.731810]\n",
      "epoch:37 step:35076 [D loss: 0.752815, acc.: 53.91%] [G loss: 1.522357]\n",
      "epoch:37 step:35077 [D loss: 0.449724, acc.: 77.34%] [G loss: 1.913150]\n",
      "epoch:37 step:35078 [D loss: 0.643170, acc.: 65.62%] [G loss: 1.391066]\n",
      "epoch:37 step:35079 [D loss: 0.664140, acc.: 61.72%] [G loss: 1.127037]\n",
      "epoch:37 step:35080 [D loss: 0.553738, acc.: 68.75%] [G loss: 1.358745]\n",
      "epoch:37 step:35081 [D loss: 0.501785, acc.: 75.00%] [G loss: 0.994137]\n",
      "epoch:37 step:35082 [D loss: 0.571886, acc.: 70.31%] [G loss: 1.534141]\n",
      "epoch:37 step:35083 [D loss: 0.478014, acc.: 77.34%] [G loss: 1.212363]\n",
      "epoch:37 step:35084 [D loss: 0.459397, acc.: 77.34%] [G loss: 1.719507]\n",
      "epoch:37 step:35085 [D loss: 0.607745, acc.: 65.62%] [G loss: 1.437886]\n",
      "epoch:37 step:35086 [D loss: 0.435682, acc.: 80.47%] [G loss: 1.613442]\n",
      "epoch:37 step:35087 [D loss: 0.748088, acc.: 53.91%] [G loss: 1.251459]\n",
      "epoch:37 step:35088 [D loss: 0.344290, acc.: 88.28%] [G loss: 1.442826]\n",
      "epoch:37 step:35089 [D loss: 0.430881, acc.: 80.47%] [G loss: 1.323477]\n",
      "epoch:37 step:35090 [D loss: 0.660211, acc.: 63.28%] [G loss: 1.874557]\n",
      "epoch:37 step:35091 [D loss: 0.536203, acc.: 71.88%] [G loss: 1.373477]\n",
      "epoch:37 step:35092 [D loss: 0.636243, acc.: 64.84%] [G loss: 1.265414]\n",
      "epoch:37 step:35093 [D loss: 0.306266, acc.: 89.06%] [G loss: 1.634181]\n",
      "epoch:37 step:35094 [D loss: 0.545684, acc.: 75.00%] [G loss: 1.839522]\n",
      "epoch:37 step:35095 [D loss: 0.547061, acc.: 74.22%] [G loss: 1.446545]\n",
      "epoch:37 step:35096 [D loss: 0.559889, acc.: 73.44%] [G loss: 1.922341]\n",
      "epoch:37 step:35097 [D loss: 0.586140, acc.: 69.53%] [G loss: 1.271669]\n",
      "epoch:37 step:35098 [D loss: 0.600720, acc.: 68.75%] [G loss: 1.401715]\n",
      "epoch:37 step:35099 [D loss: 0.428382, acc.: 88.28%] [G loss: 1.385164]\n",
      "epoch:37 step:35100 [D loss: 0.486543, acc.: 79.69%] [G loss: 1.520932]\n",
      "epoch:37 step:35101 [D loss: 0.491611, acc.: 77.34%] [G loss: 1.336917]\n",
      "epoch:37 step:35102 [D loss: 0.642485, acc.: 71.88%] [G loss: 1.021289]\n",
      "epoch:37 step:35103 [D loss: 0.532437, acc.: 75.00%] [G loss: 1.106941]\n",
      "epoch:37 step:35104 [D loss: 0.483911, acc.: 77.34%] [G loss: 1.143091]\n",
      "epoch:37 step:35105 [D loss: 0.421466, acc.: 83.59%] [G loss: 1.609607]\n",
      "epoch:37 step:35106 [D loss: 0.678962, acc.: 63.28%] [G loss: 1.599545]\n",
      "epoch:37 step:35107 [D loss: 0.455071, acc.: 82.03%] [G loss: 1.738945]\n",
      "epoch:37 step:35108 [D loss: 0.474886, acc.: 75.78%] [G loss: 1.793008]\n",
      "epoch:37 step:35109 [D loss: 0.875914, acc.: 45.31%] [G loss: 1.060729]\n",
      "epoch:37 step:35110 [D loss: 0.419885, acc.: 83.59%] [G loss: 1.816853]\n",
      "epoch:37 step:35111 [D loss: 0.471427, acc.: 79.69%] [G loss: 1.247915]\n",
      "epoch:37 step:35112 [D loss: 0.539394, acc.: 67.97%] [G loss: 1.470508]\n",
      "epoch:37 step:35113 [D loss: 0.505123, acc.: 75.00%] [G loss: 1.878356]\n",
      "epoch:37 step:35114 [D loss: 0.417392, acc.: 84.38%] [G loss: 2.310291]\n",
      "epoch:37 step:35115 [D loss: 0.658838, acc.: 64.06%] [G loss: 1.646586]\n",
      "epoch:37 step:35116 [D loss: 0.421991, acc.: 80.47%] [G loss: 1.185934]\n",
      "epoch:37 step:35117 [D loss: 0.482510, acc.: 80.47%] [G loss: 1.585435]\n",
      "epoch:37 step:35118 [D loss: 0.588513, acc.: 71.88%] [G loss: 1.467380]\n",
      "epoch:37 step:35119 [D loss: 0.682244, acc.: 61.72%] [G loss: 1.550958]\n",
      "epoch:37 step:35120 [D loss: 0.590647, acc.: 67.97%] [G loss: 1.498536]\n",
      "epoch:37 step:35121 [D loss: 0.604520, acc.: 68.75%] [G loss: 1.916464]\n",
      "epoch:37 step:35122 [D loss: 0.376851, acc.: 84.38%] [G loss: 1.423391]\n",
      "epoch:37 step:35123 [D loss: 0.465708, acc.: 80.47%] [G loss: 1.969904]\n",
      "epoch:37 step:35124 [D loss: 0.723436, acc.: 65.62%] [G loss: 1.576155]\n",
      "epoch:37 step:35125 [D loss: 0.557149, acc.: 66.41%] [G loss: 1.530237]\n",
      "epoch:37 step:35126 [D loss: 0.777212, acc.: 53.12%] [G loss: 1.110520]\n",
      "epoch:37 step:35127 [D loss: 0.313366, acc.: 92.19%] [G loss: 1.368333]\n",
      "epoch:37 step:35128 [D loss: 0.540030, acc.: 74.22%] [G loss: 1.426409]\n",
      "epoch:37 step:35129 [D loss: 0.427261, acc.: 82.81%] [G loss: 1.170573]\n",
      "epoch:37 step:35130 [D loss: 0.605806, acc.: 68.75%] [G loss: 1.262654]\n",
      "epoch:37 step:35131 [D loss: 0.525957, acc.: 74.22%] [G loss: 1.522351]\n",
      "epoch:37 step:35132 [D loss: 0.646759, acc.: 65.62%] [G loss: 1.043186]\n",
      "epoch:37 step:35133 [D loss: 0.529953, acc.: 71.88%] [G loss: 1.907257]\n",
      "epoch:37 step:35134 [D loss: 0.565960, acc.: 70.31%] [G loss: 1.128130]\n",
      "epoch:37 step:35135 [D loss: 0.495039, acc.: 79.69%] [G loss: 1.560382]\n",
      "epoch:37 step:35136 [D loss: 0.394294, acc.: 81.25%] [G loss: 1.786493]\n",
      "epoch:37 step:35137 [D loss: 0.520443, acc.: 73.44%] [G loss: 1.387976]\n",
      "epoch:37 step:35138 [D loss: 0.564448, acc.: 69.53%] [G loss: 1.589761]\n",
      "epoch:37 step:35139 [D loss: 0.568221, acc.: 71.88%] [G loss: 1.315554]\n",
      "epoch:37 step:35140 [D loss: 0.307481, acc.: 93.75%] [G loss: 1.609106]\n",
      "epoch:37 step:35141 [D loss: 0.492849, acc.: 72.66%] [G loss: 2.017471]\n",
      "epoch:37 step:35142 [D loss: 0.341866, acc.: 86.72%] [G loss: 1.606202]\n",
      "epoch:37 step:35143 [D loss: 0.562041, acc.: 68.75%] [G loss: 2.034029]\n",
      "epoch:37 step:35144 [D loss: 0.540127, acc.: 76.56%] [G loss: 1.596738]\n",
      "epoch:37 step:35145 [D loss: 0.470533, acc.: 78.91%] [G loss: 1.598513]\n",
      "epoch:37 step:35146 [D loss: 0.402414, acc.: 82.03%] [G loss: 1.930842]\n",
      "epoch:37 step:35147 [D loss: 0.596156, acc.: 68.75%] [G loss: 1.784851]\n",
      "epoch:37 step:35148 [D loss: 0.391052, acc.: 84.38%] [G loss: 1.278029]\n",
      "epoch:37 step:35149 [D loss: 0.415669, acc.: 82.81%] [G loss: 1.263431]\n",
      "epoch:37 step:35150 [D loss: 0.343972, acc.: 86.72%] [G loss: 2.017157]\n",
      "epoch:37 step:35151 [D loss: 0.398176, acc.: 80.47%] [G loss: 1.488556]\n",
      "epoch:37 step:35152 [D loss: 0.667033, acc.: 62.50%] [G loss: 1.817738]\n",
      "epoch:37 step:35153 [D loss: 0.458438, acc.: 78.91%] [G loss: 1.748543]\n",
      "epoch:37 step:35154 [D loss: 0.436687, acc.: 78.91%] [G loss: 1.376484]\n",
      "epoch:37 step:35155 [D loss: 0.372568, acc.: 85.16%] [G loss: 1.551694]\n",
      "epoch:37 step:35156 [D loss: 0.510345, acc.: 75.78%] [G loss: 1.858804]\n",
      "epoch:37 step:35157 [D loss: 0.353758, acc.: 88.28%] [G loss: 1.277593]\n",
      "epoch:37 step:35158 [D loss: 0.763440, acc.: 52.34%] [G loss: 1.833947]\n",
      "epoch:37 step:35159 [D loss: 0.502512, acc.: 76.56%] [G loss: 1.762450]\n",
      "epoch:37 step:35160 [D loss: 0.370065, acc.: 85.94%] [G loss: 1.487046]\n",
      "epoch:37 step:35161 [D loss: 0.597134, acc.: 70.31%] [G loss: 1.864518]\n",
      "epoch:37 step:35162 [D loss: 0.464038, acc.: 78.12%] [G loss: 1.191690]\n",
      "epoch:37 step:35163 [D loss: 0.391684, acc.: 84.38%] [G loss: 2.010672]\n",
      "epoch:37 step:35164 [D loss: 0.468575, acc.: 73.44%] [G loss: 1.950083]\n",
      "epoch:37 step:35165 [D loss: 0.554568, acc.: 71.88%] [G loss: 1.222353]\n",
      "epoch:37 step:35166 [D loss: 0.496273, acc.: 78.12%] [G loss: 1.847778]\n",
      "epoch:37 step:35167 [D loss: 0.335745, acc.: 85.94%] [G loss: 1.945567]\n",
      "epoch:37 step:35168 [D loss: 0.440736, acc.: 81.25%] [G loss: 1.526905]\n",
      "epoch:37 step:35169 [D loss: 0.433744, acc.: 78.91%] [G loss: 1.415991]\n",
      "epoch:37 step:35170 [D loss: 0.311580, acc.: 88.28%] [G loss: 1.801022]\n",
      "epoch:37 step:35171 [D loss: 0.523756, acc.: 71.88%] [G loss: 1.739825]\n",
      "epoch:37 step:35172 [D loss: 0.409801, acc.: 82.81%] [G loss: 1.533922]\n",
      "epoch:37 step:35173 [D loss: 0.285521, acc.: 95.31%] [G loss: 2.580245]\n",
      "epoch:37 step:35174 [D loss: 0.244660, acc.: 95.31%] [G loss: 2.110008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35175 [D loss: 0.407637, acc.: 83.59%] [G loss: 1.883605]\n",
      "epoch:37 step:35176 [D loss: 0.565834, acc.: 67.19%] [G loss: 1.266893]\n",
      "epoch:37 step:35177 [D loss: 0.584069, acc.: 67.97%] [G loss: 1.169625]\n",
      "epoch:37 step:35178 [D loss: 0.409825, acc.: 83.59%] [G loss: 1.297099]\n",
      "epoch:37 step:35179 [D loss: 0.543828, acc.: 71.09%] [G loss: 1.898293]\n",
      "epoch:37 step:35180 [D loss: 0.368815, acc.: 87.50%] [G loss: 1.682988]\n",
      "epoch:37 step:35181 [D loss: 0.927932, acc.: 47.66%] [G loss: 1.421277]\n",
      "epoch:37 step:35182 [D loss: 0.390954, acc.: 82.03%] [G loss: 1.199510]\n",
      "epoch:37 step:35183 [D loss: 0.523278, acc.: 75.00%] [G loss: 1.900358]\n",
      "epoch:37 step:35184 [D loss: 0.418647, acc.: 82.03%] [G loss: 1.832069]\n",
      "epoch:37 step:35185 [D loss: 0.418432, acc.: 83.59%] [G loss: 1.766150]\n",
      "epoch:37 step:35186 [D loss: 0.539066, acc.: 75.00%] [G loss: 1.684769]\n",
      "epoch:37 step:35187 [D loss: 0.428438, acc.: 80.47%] [G loss: 2.211874]\n",
      "epoch:37 step:35188 [D loss: 0.442696, acc.: 78.12%] [G loss: 1.830158]\n",
      "epoch:37 step:35189 [D loss: 0.428785, acc.: 80.47%] [G loss: 1.436072]\n",
      "epoch:37 step:35190 [D loss: 0.490955, acc.: 79.69%] [G loss: 1.711216]\n",
      "epoch:37 step:35191 [D loss: 0.564591, acc.: 71.88%] [G loss: 1.541613]\n",
      "epoch:37 step:35192 [D loss: 0.554415, acc.: 70.31%] [G loss: 1.569212]\n",
      "epoch:37 step:35193 [D loss: 0.485722, acc.: 75.00%] [G loss: 1.270065]\n",
      "epoch:37 step:35194 [D loss: 0.671203, acc.: 62.50%] [G loss: 1.387408]\n",
      "epoch:37 step:35195 [D loss: 0.396726, acc.: 83.59%] [G loss: 1.624243]\n",
      "epoch:37 step:35196 [D loss: 0.871209, acc.: 48.44%] [G loss: 1.160512]\n",
      "epoch:37 step:35197 [D loss: 0.512631, acc.: 71.88%] [G loss: 1.306376]\n",
      "epoch:37 step:35198 [D loss: 0.384953, acc.: 84.38%] [G loss: 1.520499]\n",
      "epoch:37 step:35199 [D loss: 0.467829, acc.: 75.00%] [G loss: 1.490111]\n",
      "epoch:37 step:35200 [D loss: 0.435311, acc.: 82.81%] [G loss: 1.513396]\n",
      "##############\n",
      "[2.67970131 1.98828525 1.70909135 2.42125817 0.84635829 5.20606607\n",
      " 1.83624855 2.83834466 3.76525921 8.14868929]\n",
      "##########\n",
      "epoch:37 step:35201 [D loss: 0.562534, acc.: 75.00%] [G loss: 1.330012]\n",
      "epoch:37 step:35202 [D loss: 0.716891, acc.: 55.47%] [G loss: 0.782585]\n",
      "epoch:37 step:35203 [D loss: 0.534749, acc.: 72.66%] [G loss: 1.562786]\n",
      "epoch:37 step:35204 [D loss: 0.638106, acc.: 64.06%] [G loss: 1.316389]\n",
      "epoch:37 step:35205 [D loss: 0.510770, acc.: 77.34%] [G loss: 1.873365]\n",
      "epoch:37 step:35206 [D loss: 0.317879, acc.: 90.62%] [G loss: 1.645436]\n",
      "epoch:37 step:35207 [D loss: 0.565662, acc.: 70.31%] [G loss: 1.245339]\n",
      "epoch:37 step:35208 [D loss: 0.666090, acc.: 65.62%] [G loss: 1.572224]\n",
      "epoch:37 step:35209 [D loss: 0.481120, acc.: 75.00%] [G loss: 1.804440]\n",
      "epoch:37 step:35210 [D loss: 0.443274, acc.: 75.00%] [G loss: 2.110951]\n",
      "epoch:37 step:35211 [D loss: 0.468793, acc.: 77.34%] [G loss: 1.392725]\n",
      "epoch:37 step:35212 [D loss: 0.531997, acc.: 75.00%] [G loss: 1.429277]\n",
      "epoch:37 step:35213 [D loss: 0.533077, acc.: 72.66%] [G loss: 1.043476]\n",
      "epoch:37 step:35214 [D loss: 0.460098, acc.: 75.78%] [G loss: 1.497500]\n",
      "epoch:37 step:35215 [D loss: 0.405826, acc.: 83.59%] [G loss: 1.208856]\n",
      "epoch:37 step:35216 [D loss: 0.386701, acc.: 87.50%] [G loss: 1.705147]\n",
      "epoch:37 step:35217 [D loss: 0.457615, acc.: 78.91%] [G loss: 1.561183]\n",
      "epoch:37 step:35218 [D loss: 0.526514, acc.: 73.44%] [G loss: 1.302477]\n",
      "epoch:37 step:35219 [D loss: 0.899700, acc.: 47.66%] [G loss: 1.369932]\n",
      "epoch:37 step:35220 [D loss: 0.642740, acc.: 67.97%] [G loss: 1.710423]\n",
      "epoch:37 step:35221 [D loss: 0.548529, acc.: 71.09%] [G loss: 1.642741]\n",
      "epoch:37 step:35222 [D loss: 0.411290, acc.: 82.81%] [G loss: 1.172176]\n",
      "epoch:37 step:35223 [D loss: 0.353746, acc.: 85.16%] [G loss: 1.877359]\n",
      "epoch:37 step:35224 [D loss: 0.685101, acc.: 64.06%] [G loss: 1.459597]\n",
      "epoch:37 step:35225 [D loss: 0.498499, acc.: 77.34%] [G loss: 2.010773]\n",
      "epoch:37 step:35226 [D loss: 0.448528, acc.: 82.03%] [G loss: 2.013372]\n",
      "epoch:37 step:35227 [D loss: 0.489041, acc.: 78.12%] [G loss: 1.453230]\n",
      "epoch:37 step:35228 [D loss: 0.383384, acc.: 84.38%] [G loss: 1.478079]\n",
      "epoch:37 step:35229 [D loss: 0.564325, acc.: 72.66%] [G loss: 1.942344]\n",
      "epoch:37 step:35230 [D loss: 0.357871, acc.: 88.28%] [G loss: 1.650281]\n",
      "epoch:37 step:35231 [D loss: 0.525924, acc.: 72.66%] [G loss: 2.053860]\n",
      "epoch:37 step:35232 [D loss: 0.373121, acc.: 84.38%] [G loss: 1.626134]\n",
      "epoch:37 step:35233 [D loss: 0.572266, acc.: 74.22%] [G loss: 2.322879]\n",
      "epoch:37 step:35234 [D loss: 0.504854, acc.: 75.00%] [G loss: 1.461451]\n",
      "epoch:37 step:35235 [D loss: 0.476069, acc.: 78.12%] [G loss: 1.763197]\n",
      "epoch:37 step:35236 [D loss: 0.337222, acc.: 88.28%] [G loss: 1.469335]\n",
      "epoch:37 step:35237 [D loss: 0.691427, acc.: 62.50%] [G loss: 1.215163]\n",
      "epoch:37 step:35238 [D loss: 0.393670, acc.: 85.16%] [G loss: 1.715511]\n",
      "epoch:37 step:35239 [D loss: 0.735200, acc.: 53.12%] [G loss: 1.342155]\n",
      "epoch:37 step:35240 [D loss: 0.362459, acc.: 84.38%] [G loss: 1.568427]\n",
      "epoch:37 step:35241 [D loss: 0.349921, acc.: 85.94%] [G loss: 1.355182]\n",
      "epoch:37 step:35242 [D loss: 0.567070, acc.: 69.53%] [G loss: 1.615159]\n",
      "epoch:37 step:35243 [D loss: 0.509980, acc.: 74.22%] [G loss: 1.191921]\n",
      "epoch:37 step:35244 [D loss: 0.383148, acc.: 85.16%] [G loss: 2.038278]\n",
      "epoch:37 step:35245 [D loss: 0.366032, acc.: 85.94%] [G loss: 2.031873]\n",
      "epoch:37 step:35246 [D loss: 0.552374, acc.: 69.53%] [G loss: 2.271936]\n",
      "epoch:37 step:35247 [D loss: 0.609903, acc.: 64.84%] [G loss: 1.664391]\n",
      "epoch:37 step:35248 [D loss: 0.418765, acc.: 82.03%] [G loss: 1.291463]\n",
      "epoch:37 step:35249 [D loss: 0.506650, acc.: 69.53%] [G loss: 1.844107]\n",
      "epoch:37 step:35250 [D loss: 0.633445, acc.: 64.84%] [G loss: 1.466948]\n",
      "epoch:37 step:35251 [D loss: 0.485122, acc.: 78.12%] [G loss: 2.221470]\n",
      "epoch:37 step:35252 [D loss: 0.444034, acc.: 76.56%] [G loss: 1.510768]\n",
      "epoch:37 step:35253 [D loss: 0.559549, acc.: 71.88%] [G loss: 1.238008]\n",
      "epoch:37 step:35254 [D loss: 0.505686, acc.: 73.44%] [G loss: 1.552426]\n",
      "epoch:37 step:35255 [D loss: 0.433858, acc.: 79.69%] [G loss: 1.254732]\n",
      "epoch:37 step:35256 [D loss: 0.408309, acc.: 85.16%] [G loss: 1.557734]\n",
      "epoch:37 step:35257 [D loss: 0.379771, acc.: 87.50%] [G loss: 1.951950]\n",
      "epoch:37 step:35258 [D loss: 0.370432, acc.: 84.38%] [G loss: 2.146253]\n",
      "epoch:37 step:35259 [D loss: 0.460046, acc.: 82.03%] [G loss: 1.178629]\n",
      "epoch:37 step:35260 [D loss: 0.425689, acc.: 81.25%] [G loss: 1.058253]\n",
      "epoch:37 step:35261 [D loss: 0.499782, acc.: 74.22%] [G loss: 1.557254]\n",
      "epoch:37 step:35262 [D loss: 0.598074, acc.: 72.66%] [G loss: 1.466785]\n",
      "epoch:37 step:35263 [D loss: 0.541221, acc.: 71.09%] [G loss: 1.706714]\n",
      "epoch:37 step:35264 [D loss: 0.522335, acc.: 71.88%] [G loss: 1.681634]\n",
      "epoch:37 step:35265 [D loss: 0.646986, acc.: 65.62%] [G loss: 1.458852]\n",
      "epoch:37 step:35266 [D loss: 0.637042, acc.: 65.62%] [G loss: 1.995970]\n",
      "epoch:37 step:35267 [D loss: 0.400829, acc.: 82.81%] [G loss: 1.704908]\n",
      "epoch:37 step:35268 [D loss: 0.298541, acc.: 90.62%] [G loss: 2.460083]\n",
      "epoch:37 step:35269 [D loss: 0.509938, acc.: 73.44%] [G loss: 1.838503]\n",
      "epoch:37 step:35270 [D loss: 0.528794, acc.: 72.66%] [G loss: 1.567081]\n",
      "epoch:37 step:35271 [D loss: 0.639542, acc.: 64.06%] [G loss: 0.949111]\n",
      "epoch:37 step:35272 [D loss: 0.559958, acc.: 72.66%] [G loss: 0.933216]\n",
      "epoch:37 step:35273 [D loss: 0.475065, acc.: 77.34%] [G loss: 1.268198]\n",
      "epoch:37 step:35274 [D loss: 0.542341, acc.: 70.31%] [G loss: 1.741216]\n",
      "epoch:37 step:35275 [D loss: 0.555202, acc.: 70.31%] [G loss: 1.725183]\n",
      "epoch:37 step:35276 [D loss: 0.695858, acc.: 62.50%] [G loss: 1.309674]\n",
      "epoch:37 step:35277 [D loss: 0.441524, acc.: 80.47%] [G loss: 1.040458]\n",
      "epoch:37 step:35278 [D loss: 0.606538, acc.: 68.75%] [G loss: 1.614670]\n",
      "epoch:37 step:35279 [D loss: 0.555167, acc.: 70.31%] [G loss: 1.772485]\n",
      "epoch:37 step:35280 [D loss: 0.471070, acc.: 76.56%] [G loss: 1.692817]\n",
      "epoch:37 step:35281 [D loss: 0.469491, acc.: 78.91%] [G loss: 0.850011]\n",
      "epoch:37 step:35282 [D loss: 0.312963, acc.: 90.62%] [G loss: 1.885025]\n",
      "epoch:37 step:35283 [D loss: 0.460880, acc.: 78.91%] [G loss: 1.583207]\n",
      "epoch:37 step:35284 [D loss: 0.640066, acc.: 64.06%] [G loss: 1.776210]\n",
      "epoch:37 step:35285 [D loss: 0.514909, acc.: 75.00%] [G loss: 1.400731]\n",
      "epoch:37 step:35286 [D loss: 0.335972, acc.: 87.50%] [G loss: 1.440535]\n",
      "epoch:37 step:35287 [D loss: 0.374328, acc.: 84.38%] [G loss: 1.393038]\n",
      "epoch:37 step:35288 [D loss: 0.453064, acc.: 78.12%] [G loss: 1.849692]\n",
      "epoch:37 step:35289 [D loss: 0.459430, acc.: 83.59%] [G loss: 1.630288]\n",
      "epoch:37 step:35290 [D loss: 0.476764, acc.: 76.56%] [G loss: 1.318238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35291 [D loss: 0.634247, acc.: 67.97%] [G loss: 1.325663]\n",
      "epoch:37 step:35292 [D loss: 0.652939, acc.: 59.38%] [G loss: 1.844636]\n",
      "epoch:37 step:35293 [D loss: 0.501075, acc.: 74.22%] [G loss: 1.835140]\n",
      "epoch:37 step:35294 [D loss: 0.362743, acc.: 88.28%] [G loss: 1.655782]\n",
      "epoch:37 step:35295 [D loss: 0.422159, acc.: 85.94%] [G loss: 1.417234]\n",
      "epoch:37 step:35296 [D loss: 0.600429, acc.: 66.41%] [G loss: 1.097776]\n",
      "epoch:37 step:35297 [D loss: 0.327451, acc.: 86.72%] [G loss: 1.480750]\n",
      "epoch:37 step:35298 [D loss: 0.656114, acc.: 65.62%] [G loss: 0.978336]\n",
      "epoch:37 step:35299 [D loss: 0.568365, acc.: 75.78%] [G loss: 1.209590]\n",
      "epoch:37 step:35300 [D loss: 0.597203, acc.: 68.75%] [G loss: 1.508959]\n",
      "epoch:37 step:35301 [D loss: 0.535003, acc.: 72.66%] [G loss: 2.233908]\n",
      "epoch:37 step:35302 [D loss: 0.377710, acc.: 87.50%] [G loss: 1.825707]\n",
      "epoch:37 step:35303 [D loss: 0.425515, acc.: 83.59%] [G loss: 2.019556]\n",
      "epoch:37 step:35304 [D loss: 0.571903, acc.: 69.53%] [G loss: 1.580579]\n",
      "epoch:37 step:35305 [D loss: 0.655434, acc.: 67.97%] [G loss: 1.589097]\n",
      "epoch:37 step:35306 [D loss: 0.432480, acc.: 82.03%] [G loss: 1.503468]\n",
      "epoch:37 step:35307 [D loss: 0.347419, acc.: 85.94%] [G loss: 1.591740]\n",
      "epoch:37 step:35308 [D loss: 0.561432, acc.: 71.88%] [G loss: 1.292546]\n",
      "epoch:37 step:35309 [D loss: 0.286755, acc.: 91.41%] [G loss: 1.541621]\n",
      "epoch:37 step:35310 [D loss: 0.494204, acc.: 75.00%] [G loss: 1.317745]\n",
      "epoch:37 step:35311 [D loss: 0.543997, acc.: 71.88%] [G loss: 0.915492]\n",
      "epoch:37 step:35312 [D loss: 0.688131, acc.: 67.19%] [G loss: 1.321313]\n",
      "epoch:37 step:35313 [D loss: 0.369658, acc.: 88.28%] [G loss: 1.916976]\n",
      "epoch:37 step:35314 [D loss: 0.483213, acc.: 77.34%] [G loss: 1.799106]\n",
      "epoch:37 step:35315 [D loss: 0.532813, acc.: 73.44%] [G loss: 1.126311]\n",
      "epoch:37 step:35316 [D loss: 0.577579, acc.: 71.09%] [G loss: 1.544364]\n",
      "epoch:37 step:35317 [D loss: 0.434473, acc.: 82.03%] [G loss: 1.640037]\n",
      "epoch:37 step:35318 [D loss: 0.570421, acc.: 67.19%] [G loss: 1.902647]\n",
      "epoch:37 step:35319 [D loss: 0.547214, acc.: 72.66%] [G loss: 2.289831]\n",
      "epoch:37 step:35320 [D loss: 0.275246, acc.: 93.75%] [G loss: 1.554422]\n",
      "epoch:37 step:35321 [D loss: 0.539006, acc.: 73.44%] [G loss: 1.399506]\n",
      "epoch:37 step:35322 [D loss: 0.706147, acc.: 64.06%] [G loss: 1.281061]\n",
      "epoch:37 step:35323 [D loss: 0.389363, acc.: 84.38%] [G loss: 1.098834]\n",
      "epoch:37 step:35324 [D loss: 0.527337, acc.: 76.56%] [G loss: 1.315920]\n",
      "epoch:37 step:35325 [D loss: 0.503499, acc.: 75.78%] [G loss: 1.419292]\n",
      "epoch:37 step:35326 [D loss: 0.499443, acc.: 78.12%] [G loss: 2.260353]\n",
      "epoch:37 step:35327 [D loss: 0.816346, acc.: 50.00%] [G loss: 1.594018]\n",
      "epoch:37 step:35328 [D loss: 0.499946, acc.: 75.00%] [G loss: 2.144446]\n",
      "epoch:37 step:35329 [D loss: 0.402628, acc.: 82.03%] [G loss: 1.400880]\n",
      "epoch:37 step:35330 [D loss: 0.347281, acc.: 82.81%] [G loss: 1.311303]\n",
      "epoch:37 step:35331 [D loss: 0.567213, acc.: 70.31%] [G loss: 1.673486]\n",
      "epoch:37 step:35332 [D loss: 0.600960, acc.: 67.97%] [G loss: 1.318096]\n",
      "epoch:37 step:35333 [D loss: 0.495042, acc.: 77.34%] [G loss: 1.493189]\n",
      "epoch:37 step:35334 [D loss: 0.601306, acc.: 67.97%] [G loss: 0.969888]\n",
      "epoch:37 step:35335 [D loss: 0.552052, acc.: 72.66%] [G loss: 1.904794]\n",
      "epoch:37 step:35336 [D loss: 0.403037, acc.: 84.38%] [G loss: 1.760678]\n",
      "epoch:37 step:35337 [D loss: 0.500045, acc.: 77.34%] [G loss: 1.464654]\n",
      "epoch:37 step:35338 [D loss: 0.408216, acc.: 82.81%] [G loss: 1.436688]\n",
      "epoch:37 step:35339 [D loss: 0.531590, acc.: 71.88%] [G loss: 1.796466]\n",
      "epoch:37 step:35340 [D loss: 0.516362, acc.: 75.00%] [G loss: 1.714708]\n",
      "epoch:37 step:35341 [D loss: 0.416267, acc.: 76.56%] [G loss: 1.706326]\n",
      "epoch:37 step:35342 [D loss: 0.827587, acc.: 49.22%] [G loss: 1.060102]\n",
      "epoch:37 step:35343 [D loss: 0.424550, acc.: 83.59%] [G loss: 1.855944]\n",
      "epoch:37 step:35344 [D loss: 0.644950, acc.: 65.62%] [G loss: 1.307950]\n",
      "epoch:37 step:35345 [D loss: 0.566010, acc.: 71.88%] [G loss: 1.071367]\n",
      "epoch:37 step:35346 [D loss: 0.341395, acc.: 86.72%] [G loss: 2.028656]\n",
      "epoch:37 step:35347 [D loss: 0.588141, acc.: 71.09%] [G loss: 1.630837]\n",
      "epoch:37 step:35348 [D loss: 0.334288, acc.: 85.16%] [G loss: 1.970922]\n",
      "epoch:37 step:35349 [D loss: 0.508314, acc.: 71.88%] [G loss: 1.972557]\n",
      "epoch:37 step:35350 [D loss: 0.407070, acc.: 79.69%] [G loss: 1.570607]\n",
      "epoch:37 step:35351 [D loss: 0.417639, acc.: 85.94%] [G loss: 1.358739]\n",
      "epoch:37 step:35352 [D loss: 0.522392, acc.: 75.00%] [G loss: 1.623909]\n",
      "epoch:37 step:35353 [D loss: 0.517608, acc.: 71.88%] [G loss: 1.443696]\n",
      "epoch:37 step:35354 [D loss: 0.565576, acc.: 69.53%] [G loss: 1.412330]\n",
      "epoch:37 step:35355 [D loss: 0.417985, acc.: 84.38%] [G loss: 1.717826]\n",
      "epoch:37 step:35356 [D loss: 0.627619, acc.: 65.62%] [G loss: 1.313835]\n",
      "epoch:37 step:35357 [D loss: 0.570895, acc.: 66.41%] [G loss: 1.126248]\n",
      "epoch:37 step:35358 [D loss: 0.657718, acc.: 64.06%] [G loss: 1.623578]\n",
      "epoch:37 step:35359 [D loss: 0.382593, acc.: 83.59%] [G loss: 1.191905]\n",
      "epoch:37 step:35360 [D loss: 0.464296, acc.: 78.12%] [G loss: 1.812595]\n",
      "epoch:37 step:35361 [D loss: 0.644443, acc.: 66.41%] [G loss: 1.252629]\n",
      "epoch:37 step:35362 [D loss: 0.567299, acc.: 70.31%] [G loss: 1.852119]\n",
      "epoch:37 step:35363 [D loss: 0.551619, acc.: 75.00%] [G loss: 1.640677]\n",
      "epoch:37 step:35364 [D loss: 0.470053, acc.: 76.56%] [G loss: 0.948152]\n",
      "epoch:37 step:35365 [D loss: 0.646085, acc.: 59.38%] [G loss: 1.503627]\n",
      "epoch:37 step:35366 [D loss: 0.478822, acc.: 79.69%] [G loss: 1.415831]\n",
      "epoch:37 step:35367 [D loss: 0.602747, acc.: 67.19%] [G loss: 1.267833]\n",
      "epoch:37 step:35368 [D loss: 0.453683, acc.: 78.12%] [G loss: 1.732121]\n",
      "epoch:37 step:35369 [D loss: 0.448783, acc.: 83.59%] [G loss: 1.467409]\n",
      "epoch:37 step:35370 [D loss: 0.281794, acc.: 91.41%] [G loss: 1.782347]\n",
      "epoch:37 step:35371 [D loss: 0.452917, acc.: 80.47%] [G loss: 1.654505]\n",
      "epoch:37 step:35372 [D loss: 0.451664, acc.: 79.69%] [G loss: 1.452103]\n",
      "epoch:37 step:35373 [D loss: 0.665474, acc.: 64.06%] [G loss: 1.598250]\n",
      "epoch:37 step:35374 [D loss: 0.407896, acc.: 85.16%] [G loss: 1.345955]\n",
      "epoch:37 step:35375 [D loss: 0.746344, acc.: 57.03%] [G loss: 1.839967]\n",
      "epoch:37 step:35376 [D loss: 0.483235, acc.: 78.12%] [G loss: 1.439195]\n",
      "epoch:37 step:35377 [D loss: 0.493411, acc.: 72.66%] [G loss: 1.427431]\n",
      "epoch:37 step:35378 [D loss: 0.768838, acc.: 54.69%] [G loss: 1.614974]\n",
      "epoch:37 step:35379 [D loss: 0.571799, acc.: 72.66%] [G loss: 1.271236]\n",
      "epoch:37 step:35380 [D loss: 0.471806, acc.: 79.69%] [G loss: 1.630977]\n",
      "epoch:37 step:35381 [D loss: 0.500945, acc.: 70.31%] [G loss: 1.945096]\n",
      "epoch:37 step:35382 [D loss: 0.619135, acc.: 71.09%] [G loss: 1.320533]\n",
      "epoch:37 step:35383 [D loss: 0.538623, acc.: 71.88%] [G loss: 1.258168]\n",
      "epoch:37 step:35384 [D loss: 0.536715, acc.: 70.31%] [G loss: 1.664861]\n",
      "epoch:37 step:35385 [D loss: 0.438257, acc.: 79.69%] [G loss: 1.448384]\n",
      "epoch:37 step:35386 [D loss: 0.471212, acc.: 78.12%] [G loss: 1.529835]\n",
      "epoch:37 step:35387 [D loss: 0.393638, acc.: 84.38%] [G loss: 1.971191]\n",
      "epoch:37 step:35388 [D loss: 0.855459, acc.: 51.56%] [G loss: 1.728804]\n",
      "epoch:37 step:35389 [D loss: 0.359109, acc.: 88.28%] [G loss: 1.928774]\n",
      "epoch:37 step:35390 [D loss: 0.584624, acc.: 71.88%] [G loss: 1.732799]\n",
      "epoch:37 step:35391 [D loss: 0.832542, acc.: 50.00%] [G loss: 1.296798]\n",
      "epoch:37 step:35392 [D loss: 0.612992, acc.: 63.28%] [G loss: 1.393768]\n",
      "epoch:37 step:35393 [D loss: 0.425884, acc.: 87.50%] [G loss: 1.319696]\n",
      "epoch:37 step:35394 [D loss: 0.573390, acc.: 70.31%] [G loss: 1.563131]\n",
      "epoch:37 step:35395 [D loss: 0.579258, acc.: 71.88%] [G loss: 1.695909]\n",
      "epoch:37 step:35396 [D loss: 0.464230, acc.: 75.78%] [G loss: 1.941700]\n",
      "epoch:37 step:35397 [D loss: 0.530368, acc.: 75.00%] [G loss: 1.771909]\n",
      "epoch:37 step:35398 [D loss: 0.472107, acc.: 76.56%] [G loss: 1.616303]\n",
      "epoch:37 step:35399 [D loss: 0.595856, acc.: 65.62%] [G loss: 1.190490]\n",
      "epoch:37 step:35400 [D loss: 0.655940, acc.: 66.41%] [G loss: 1.181621]\n",
      "##############\n",
      "[2.60448532 1.97472671 1.77634379 2.8105738  0.80253986 6.50339042\n",
      " 2.24264401 3.02678577 3.96324399 5.89617862]\n",
      "##########\n",
      "epoch:37 step:35401 [D loss: 0.676847, acc.: 64.84%] [G loss: 1.531463]\n",
      "epoch:37 step:35402 [D loss: 0.519917, acc.: 74.22%] [G loss: 1.365071]\n",
      "epoch:37 step:35403 [D loss: 0.493707, acc.: 76.56%] [G loss: 1.907683]\n",
      "epoch:37 step:35404 [D loss: 0.357420, acc.: 86.72%] [G loss: 1.608813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35405 [D loss: 0.436671, acc.: 76.56%] [G loss: 1.744557]\n",
      "epoch:37 step:35406 [D loss: 0.524200, acc.: 73.44%] [G loss: 1.262843]\n",
      "epoch:37 step:35407 [D loss: 0.712573, acc.: 57.81%] [G loss: 1.136789]\n",
      "epoch:37 step:35408 [D loss: 0.482062, acc.: 77.34%] [G loss: 1.669520]\n",
      "epoch:37 step:35409 [D loss: 0.583246, acc.: 70.31%] [G loss: 1.562726]\n",
      "epoch:37 step:35410 [D loss: 0.582057, acc.: 65.62%] [G loss: 1.013638]\n",
      "epoch:37 step:35411 [D loss: 0.382182, acc.: 81.25%] [G loss: 1.563877]\n",
      "epoch:37 step:35412 [D loss: 0.673025, acc.: 65.62%] [G loss: 1.680966]\n",
      "epoch:37 step:35413 [D loss: 0.516035, acc.: 72.66%] [G loss: 2.228915]\n",
      "epoch:37 step:35414 [D loss: 0.371170, acc.: 90.62%] [G loss: 1.607666]\n",
      "epoch:37 step:35415 [D loss: 0.545651, acc.: 70.31%] [G loss: 2.358257]\n",
      "epoch:37 step:35416 [D loss: 0.541666, acc.: 76.56%] [G loss: 1.647821]\n",
      "epoch:37 step:35417 [D loss: 0.719771, acc.: 55.47%] [G loss: 1.831666]\n",
      "epoch:37 step:35418 [D loss: 0.558981, acc.: 67.19%] [G loss: 1.859299]\n",
      "epoch:37 step:35419 [D loss: 0.677461, acc.: 58.59%] [G loss: 1.526313]\n",
      "epoch:37 step:35420 [D loss: 0.507646, acc.: 74.22%] [G loss: 1.338706]\n",
      "epoch:37 step:35421 [D loss: 0.310929, acc.: 89.84%] [G loss: 2.015623]\n",
      "epoch:37 step:35422 [D loss: 0.544186, acc.: 75.78%] [G loss: 1.641092]\n",
      "epoch:37 step:35423 [D loss: 0.524034, acc.: 74.22%] [G loss: 1.881229]\n",
      "epoch:37 step:35424 [D loss: 0.422597, acc.: 82.81%] [G loss: 1.322343]\n",
      "epoch:37 step:35425 [D loss: 0.507064, acc.: 77.34%] [G loss: 1.439007]\n",
      "epoch:37 step:35426 [D loss: 0.483637, acc.: 78.91%] [G loss: 1.450347]\n",
      "epoch:37 step:35427 [D loss: 0.498793, acc.: 71.09%] [G loss: 1.475271]\n",
      "epoch:37 step:35428 [D loss: 0.389496, acc.: 83.59%] [G loss: 1.482587]\n",
      "epoch:37 step:35429 [D loss: 0.427441, acc.: 83.59%] [G loss: 1.976980]\n",
      "epoch:37 step:35430 [D loss: 0.664260, acc.: 59.38%] [G loss: 1.759833]\n",
      "epoch:37 step:35431 [D loss: 0.703470, acc.: 57.81%] [G loss: 1.322338]\n",
      "epoch:37 step:35432 [D loss: 0.444335, acc.: 79.69%] [G loss: 1.979064]\n",
      "epoch:37 step:35433 [D loss: 0.402176, acc.: 80.47%] [G loss: 2.055330]\n",
      "epoch:37 step:35434 [D loss: 0.470867, acc.: 78.91%] [G loss: 1.879581]\n",
      "epoch:37 step:35435 [D loss: 0.360319, acc.: 89.84%] [G loss: 1.833444]\n",
      "epoch:37 step:35436 [D loss: 0.351564, acc.: 85.94%] [G loss: 1.536180]\n",
      "epoch:37 step:35437 [D loss: 0.387086, acc.: 83.59%] [G loss: 1.689132]\n",
      "epoch:37 step:35438 [D loss: 0.320755, acc.: 89.84%] [G loss: 1.628039]\n",
      "epoch:37 step:35439 [D loss: 0.427481, acc.: 82.03%] [G loss: 1.521992]\n",
      "epoch:37 step:35440 [D loss: 0.559569, acc.: 69.53%] [G loss: 1.331353]\n",
      "epoch:37 step:35441 [D loss: 0.594958, acc.: 68.75%] [G loss: 1.359139]\n",
      "epoch:37 step:35442 [D loss: 0.702169, acc.: 64.06%] [G loss: 1.194124]\n",
      "epoch:37 step:35443 [D loss: 0.774823, acc.: 54.69%] [G loss: 0.988001]\n",
      "epoch:37 step:35444 [D loss: 0.427040, acc.: 85.94%] [G loss: 1.828740]\n",
      "epoch:37 step:35445 [D loss: 0.423509, acc.: 82.81%] [G loss: 1.925336]\n",
      "epoch:37 step:35446 [D loss: 0.676669, acc.: 62.50%] [G loss: 1.089932]\n",
      "epoch:37 step:35447 [D loss: 0.592191, acc.: 66.41%] [G loss: 1.170597]\n",
      "epoch:37 step:35448 [D loss: 0.442740, acc.: 81.25%] [G loss: 1.883761]\n",
      "epoch:37 step:35449 [D loss: 0.686277, acc.: 55.47%] [G loss: 1.291480]\n",
      "epoch:37 step:35450 [D loss: 0.617261, acc.: 70.31%] [G loss: 1.894905]\n",
      "epoch:37 step:35451 [D loss: 0.673281, acc.: 60.16%] [G loss: 2.164401]\n",
      "epoch:37 step:35452 [D loss: 0.586829, acc.: 65.62%] [G loss: 1.349619]\n",
      "epoch:37 step:35453 [D loss: 0.586481, acc.: 69.53%] [G loss: 1.198083]\n",
      "epoch:37 step:35454 [D loss: 0.181385, acc.: 98.44%] [G loss: 1.985415]\n",
      "epoch:37 step:35455 [D loss: 1.024819, acc.: 32.81%] [G loss: 0.963964]\n",
      "epoch:37 step:35456 [D loss: 0.552418, acc.: 68.75%] [G loss: 1.505975]\n",
      "epoch:37 step:35457 [D loss: 0.516342, acc.: 78.12%] [G loss: 2.326574]\n",
      "epoch:37 step:35458 [D loss: 0.524795, acc.: 75.00%] [G loss: 1.712127]\n",
      "epoch:37 step:35459 [D loss: 0.538526, acc.: 68.75%] [G loss: 1.493582]\n",
      "epoch:37 step:35460 [D loss: 0.623702, acc.: 69.53%] [G loss: 1.855134]\n",
      "epoch:37 step:35461 [D loss: 0.452317, acc.: 81.25%] [G loss: 1.409163]\n",
      "epoch:37 step:35462 [D loss: 0.595003, acc.: 67.97%] [G loss: 1.435756]\n",
      "epoch:37 step:35463 [D loss: 0.570274, acc.: 69.53%] [G loss: 1.695758]\n",
      "epoch:37 step:35464 [D loss: 0.433025, acc.: 78.12%] [G loss: 1.760909]\n",
      "epoch:37 step:35465 [D loss: 0.552413, acc.: 70.31%] [G loss: 1.548633]\n",
      "epoch:37 step:35466 [D loss: 0.577970, acc.: 67.97%] [G loss: 1.641102]\n",
      "epoch:37 step:35467 [D loss: 0.529515, acc.: 67.19%] [G loss: 1.114403]\n",
      "epoch:37 step:35468 [D loss: 0.634959, acc.: 65.62%] [G loss: 1.644413]\n",
      "epoch:37 step:35469 [D loss: 0.436354, acc.: 80.47%] [G loss: 1.673128]\n",
      "epoch:37 step:35470 [D loss: 0.672490, acc.: 64.06%] [G loss: 1.236667]\n",
      "epoch:37 step:35471 [D loss: 0.471561, acc.: 78.91%] [G loss: 1.258914]\n",
      "epoch:37 step:35472 [D loss: 0.638697, acc.: 66.41%] [G loss: 1.081432]\n",
      "epoch:37 step:35473 [D loss: 0.460242, acc.: 76.56%] [G loss: 1.774235]\n",
      "epoch:37 step:35474 [D loss: 0.663763, acc.: 64.06%] [G loss: 1.592152]\n",
      "epoch:37 step:35475 [D loss: 0.597638, acc.: 68.75%] [G loss: 1.831027]\n",
      "epoch:37 step:35476 [D loss: 0.555873, acc.: 71.88%] [G loss: 1.692786]\n",
      "epoch:37 step:35477 [D loss: 0.384919, acc.: 85.94%] [G loss: 1.418198]\n",
      "epoch:37 step:35478 [D loss: 0.544604, acc.: 74.22%] [G loss: 2.324829]\n",
      "epoch:37 step:35479 [D loss: 0.312004, acc.: 89.84%] [G loss: 1.408266]\n",
      "epoch:37 step:35480 [D loss: 0.696189, acc.: 60.16%] [G loss: 1.856517]\n",
      "epoch:37 step:35481 [D loss: 0.668033, acc.: 65.62%] [G loss: 1.393059]\n",
      "epoch:37 step:35482 [D loss: 0.428614, acc.: 81.25%] [G loss: 1.303019]\n",
      "epoch:37 step:35483 [D loss: 0.574462, acc.: 68.75%] [G loss: 1.445475]\n",
      "epoch:37 step:35484 [D loss: 0.440894, acc.: 78.91%] [G loss: 1.538235]\n",
      "epoch:37 step:35485 [D loss: 0.528853, acc.: 71.88%] [G loss: 1.576632]\n",
      "epoch:37 step:35486 [D loss: 0.496933, acc.: 77.34%] [G loss: 2.290659]\n",
      "epoch:37 step:35487 [D loss: 0.331094, acc.: 87.50%] [G loss: 1.992231]\n",
      "epoch:37 step:35488 [D loss: 0.391485, acc.: 82.03%] [G loss: 1.795024]\n",
      "epoch:37 step:35489 [D loss: 0.402902, acc.: 88.28%] [G loss: 1.362084]\n",
      "epoch:37 step:35490 [D loss: 0.366303, acc.: 86.72%] [G loss: 1.607551]\n",
      "epoch:37 step:35491 [D loss: 0.562400, acc.: 75.00%] [G loss: 0.915535]\n",
      "epoch:37 step:35492 [D loss: 0.279129, acc.: 92.97%] [G loss: 1.755384]\n",
      "epoch:37 step:35493 [D loss: 0.555522, acc.: 73.44%] [G loss: 1.376101]\n",
      "epoch:37 step:35494 [D loss: 0.514043, acc.: 77.34%] [G loss: 1.367977]\n",
      "epoch:37 step:35495 [D loss: 0.587192, acc.: 74.22%] [G loss: 1.353659]\n",
      "epoch:37 step:35496 [D loss: 0.464954, acc.: 81.25%] [G loss: 2.127782]\n",
      "epoch:37 step:35497 [D loss: 0.606140, acc.: 72.66%] [G loss: 1.561183]\n",
      "epoch:37 step:35498 [D loss: 0.487032, acc.: 82.03%] [G loss: 1.355385]\n",
      "epoch:37 step:35499 [D loss: 0.558378, acc.: 74.22%] [G loss: 1.089438]\n",
      "epoch:37 step:35500 [D loss: 0.618707, acc.: 67.97%] [G loss: 1.567383]\n",
      "epoch:37 step:35501 [D loss: 0.884810, acc.: 49.22%] [G loss: 1.349624]\n",
      "epoch:37 step:35502 [D loss: 0.731901, acc.: 58.59%] [G loss: 1.340340]\n",
      "epoch:37 step:35503 [D loss: 0.454830, acc.: 78.91%] [G loss: 1.360920]\n",
      "epoch:37 step:35504 [D loss: 0.589886, acc.: 71.09%] [G loss: 1.893021]\n",
      "epoch:37 step:35505 [D loss: 0.587445, acc.: 69.53%] [G loss: 1.362587]\n",
      "epoch:37 step:35506 [D loss: 0.726255, acc.: 60.16%] [G loss: 1.104549]\n",
      "epoch:37 step:35507 [D loss: 0.434674, acc.: 78.12%] [G loss: 1.053275]\n",
      "epoch:37 step:35508 [D loss: 0.430904, acc.: 82.81%] [G loss: 1.092824]\n",
      "epoch:37 step:35509 [D loss: 0.628284, acc.: 65.62%] [G loss: 1.423208]\n",
      "epoch:37 step:35510 [D loss: 0.465966, acc.: 80.47%] [G loss: 1.422049]\n",
      "epoch:37 step:35511 [D loss: 0.351689, acc.: 86.72%] [G loss: 1.663165]\n",
      "epoch:37 step:35512 [D loss: 0.414518, acc.: 85.94%] [G loss: 1.515312]\n",
      "epoch:37 step:35513 [D loss: 0.657266, acc.: 61.72%] [G loss: 1.153540]\n",
      "epoch:37 step:35514 [D loss: 0.510240, acc.: 78.91%] [G loss: 1.427127]\n",
      "epoch:37 step:35515 [D loss: 0.511244, acc.: 71.09%] [G loss: 1.823699]\n",
      "epoch:37 step:35516 [D loss: 0.592510, acc.: 72.66%] [G loss: 1.713531]\n",
      "epoch:37 step:35517 [D loss: 0.685145, acc.: 61.72%] [G loss: 1.683621]\n",
      "epoch:37 step:35518 [D loss: 0.602989, acc.: 69.53%] [G loss: 1.546667]\n",
      "epoch:37 step:35519 [D loss: 0.611761, acc.: 69.53%] [G loss: 1.601820]\n",
      "epoch:37 step:35520 [D loss: 0.775128, acc.: 54.69%] [G loss: 1.218416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35521 [D loss: 0.383325, acc.: 86.72%] [G loss: 1.996048]\n",
      "epoch:37 step:35522 [D loss: 0.472104, acc.: 81.25%] [G loss: 2.011084]\n",
      "epoch:37 step:35523 [D loss: 0.425185, acc.: 82.81%] [G loss: 1.471258]\n",
      "epoch:37 step:35524 [D loss: 0.585586, acc.: 68.75%] [G loss: 1.488164]\n",
      "epoch:37 step:35525 [D loss: 0.406533, acc.: 82.81%] [G loss: 2.236127]\n",
      "epoch:37 step:35526 [D loss: 0.597062, acc.: 64.06%] [G loss: 1.372576]\n",
      "epoch:37 step:35527 [D loss: 0.357767, acc.: 88.28%] [G loss: 1.919137]\n",
      "epoch:37 step:35528 [D loss: 0.497750, acc.: 78.12%] [G loss: 1.434350]\n",
      "epoch:37 step:35529 [D loss: 0.744693, acc.: 57.03%] [G loss: 1.217979]\n",
      "epoch:37 step:35530 [D loss: 0.491049, acc.: 82.03%] [G loss: 2.137427]\n",
      "epoch:37 step:35531 [D loss: 0.642454, acc.: 67.19%] [G loss: 1.761878]\n",
      "epoch:37 step:35532 [D loss: 0.434898, acc.: 82.81%] [G loss: 1.763022]\n",
      "epoch:37 step:35533 [D loss: 0.523478, acc.: 77.34%] [G loss: 1.431942]\n",
      "epoch:37 step:35534 [D loss: 0.478168, acc.: 79.69%] [G loss: 1.623277]\n",
      "epoch:37 step:35535 [D loss: 0.311268, acc.: 89.84%] [G loss: 2.177532]\n",
      "epoch:37 step:35536 [D loss: 0.385738, acc.: 81.25%] [G loss: 1.421785]\n",
      "epoch:37 step:35537 [D loss: 0.374771, acc.: 86.72%] [G loss: 1.355265]\n",
      "epoch:37 step:35538 [D loss: 0.363379, acc.: 85.16%] [G loss: 1.601903]\n",
      "epoch:37 step:35539 [D loss: 0.467692, acc.: 76.56%] [G loss: 1.531497]\n",
      "epoch:37 step:35540 [D loss: 0.538350, acc.: 68.75%] [G loss: 0.935688]\n",
      "epoch:37 step:35541 [D loss: 0.424641, acc.: 80.47%] [G loss: 1.653746]\n",
      "epoch:37 step:35542 [D loss: 0.413305, acc.: 82.81%] [G loss: 1.931586]\n",
      "epoch:37 step:35543 [D loss: 0.431261, acc.: 82.81%] [G loss: 1.733169]\n",
      "epoch:37 step:35544 [D loss: 0.500198, acc.: 71.09%] [G loss: 1.412900]\n",
      "epoch:37 step:35545 [D loss: 0.497426, acc.: 75.00%] [G loss: 1.633237]\n",
      "epoch:37 step:35546 [D loss: 0.778529, acc.: 52.34%] [G loss: 2.191392]\n",
      "epoch:37 step:35547 [D loss: 0.366584, acc.: 89.06%] [G loss: 1.847637]\n",
      "epoch:37 step:35548 [D loss: 0.701817, acc.: 59.38%] [G loss: 1.595942]\n",
      "epoch:37 step:35549 [D loss: 0.486572, acc.: 74.22%] [G loss: 1.504622]\n",
      "epoch:37 step:35550 [D loss: 0.651530, acc.: 62.50%] [G loss: 2.009957]\n",
      "epoch:37 step:35551 [D loss: 0.442891, acc.: 81.25%] [G loss: 1.511390]\n",
      "epoch:37 step:35552 [D loss: 0.801239, acc.: 52.34%] [G loss: 1.803434]\n",
      "epoch:37 step:35553 [D loss: 0.431982, acc.: 78.91%] [G loss: 1.592656]\n",
      "epoch:37 step:35554 [D loss: 0.582211, acc.: 67.97%] [G loss: 1.522766]\n",
      "epoch:37 step:35555 [D loss: 0.545765, acc.: 77.34%] [G loss: 1.166571]\n",
      "epoch:37 step:35556 [D loss: 0.367518, acc.: 85.94%] [G loss: 2.111082]\n",
      "epoch:37 step:35557 [D loss: 0.533537, acc.: 76.56%] [G loss: 1.400090]\n",
      "epoch:37 step:35558 [D loss: 0.674507, acc.: 65.62%] [G loss: 1.194400]\n",
      "epoch:37 step:35559 [D loss: 0.584959, acc.: 70.31%] [G loss: 1.899912]\n",
      "epoch:37 step:35560 [D loss: 0.351012, acc.: 90.62%] [G loss: 1.594095]\n",
      "epoch:37 step:35561 [D loss: 0.510293, acc.: 74.22%] [G loss: 1.475073]\n",
      "epoch:37 step:35562 [D loss: 0.430671, acc.: 78.12%] [G loss: 1.195460]\n",
      "epoch:37 step:35563 [D loss: 0.601197, acc.: 68.75%] [G loss: 1.248312]\n",
      "epoch:37 step:35564 [D loss: 0.373861, acc.: 88.28%] [G loss: 1.846093]\n",
      "epoch:37 step:35565 [D loss: 0.529158, acc.: 72.66%] [G loss: 2.029235]\n",
      "epoch:37 step:35566 [D loss: 0.667716, acc.: 63.28%] [G loss: 1.355667]\n",
      "epoch:37 step:35567 [D loss: 0.385617, acc.: 85.94%] [G loss: 1.597129]\n",
      "epoch:37 step:35568 [D loss: 0.879130, acc.: 47.66%] [G loss: 0.986223]\n",
      "epoch:37 step:35569 [D loss: 0.536471, acc.: 75.78%] [G loss: 1.589640]\n",
      "epoch:37 step:35570 [D loss: 0.616109, acc.: 67.19%] [G loss: 1.402354]\n",
      "epoch:37 step:35571 [D loss: 0.538944, acc.: 75.78%] [G loss: 1.861999]\n",
      "epoch:37 step:35572 [D loss: 0.326967, acc.: 90.62%] [G loss: 1.708143]\n",
      "epoch:37 step:35573 [D loss: 0.397501, acc.: 83.59%] [G loss: 1.429049]\n",
      "epoch:37 step:35574 [D loss: 0.586556, acc.: 68.75%] [G loss: 1.762824]\n",
      "epoch:37 step:35575 [D loss: 1.000709, acc.: 45.31%] [G loss: 0.960214]\n",
      "epoch:37 step:35576 [D loss: 0.424341, acc.: 83.59%] [G loss: 1.839696]\n",
      "epoch:37 step:35577 [D loss: 0.506534, acc.: 75.78%] [G loss: 1.533873]\n",
      "epoch:37 step:35578 [D loss: 0.427295, acc.: 82.81%] [G loss: 1.586410]\n",
      "epoch:37 step:35579 [D loss: 0.482503, acc.: 75.00%] [G loss: 1.828281]\n",
      "epoch:37 step:35580 [D loss: 0.341878, acc.: 84.38%] [G loss: 1.756895]\n",
      "epoch:37 step:35581 [D loss: 0.669250, acc.: 58.59%] [G loss: 2.153369]\n",
      "epoch:37 step:35582 [D loss: 0.572979, acc.: 72.66%] [G loss: 1.530004]\n",
      "epoch:37 step:35583 [D loss: 0.595698, acc.: 70.31%] [G loss: 1.370008]\n",
      "epoch:37 step:35584 [D loss: 0.547653, acc.: 71.88%] [G loss: 1.024602]\n",
      "epoch:37 step:35585 [D loss: 0.437127, acc.: 82.03%] [G loss: 1.307810]\n",
      "epoch:37 step:35586 [D loss: 0.561856, acc.: 67.97%] [G loss: 1.913673]\n",
      "epoch:37 step:35587 [D loss: 0.500898, acc.: 74.22%] [G loss: 1.722433]\n",
      "epoch:37 step:35588 [D loss: 0.518888, acc.: 79.69%] [G loss: 1.796395]\n",
      "epoch:37 step:35589 [D loss: 0.588913, acc.: 69.53%] [G loss: 1.249397]\n",
      "epoch:37 step:35590 [D loss: 0.682990, acc.: 58.59%] [G loss: 1.528804]\n",
      "epoch:37 step:35591 [D loss: 0.492560, acc.: 75.00%] [G loss: 1.561999]\n",
      "epoch:37 step:35592 [D loss: 0.332161, acc.: 89.84%] [G loss: 1.822021]\n",
      "epoch:37 step:35593 [D loss: 0.386755, acc.: 85.16%] [G loss: 1.152734]\n",
      "epoch:37 step:35594 [D loss: 0.649751, acc.: 61.72%] [G loss: 1.180877]\n",
      "epoch:37 step:35595 [D loss: 0.663406, acc.: 59.38%] [G loss: 1.487135]\n",
      "epoch:37 step:35596 [D loss: 0.546896, acc.: 76.56%] [G loss: 1.558500]\n",
      "epoch:37 step:35597 [D loss: 0.390212, acc.: 82.03%] [G loss: 1.835523]\n",
      "epoch:37 step:35598 [D loss: 0.609119, acc.: 67.97%] [G loss: 2.094065]\n",
      "epoch:37 step:35599 [D loss: 0.480273, acc.: 76.56%] [G loss: 1.692641]\n",
      "epoch:37 step:35600 [D loss: 0.564795, acc.: 70.31%] [G loss: 1.578738]\n",
      "##############\n",
      "[2.74802468 2.02720564 2.14077226 3.23099613 1.1616352  6.77966616\n",
      " 2.2121877  2.56752133 3.89081737 7.14771273]\n",
      "##########\n",
      "epoch:37 step:35601 [D loss: 0.408722, acc.: 81.25%] [G loss: 1.422279]\n",
      "epoch:37 step:35602 [D loss: 0.532613, acc.: 75.78%] [G loss: 1.771944]\n",
      "epoch:37 step:35603 [D loss: 0.592403, acc.: 64.84%] [G loss: 1.118344]\n",
      "epoch:37 step:35604 [D loss: 0.292407, acc.: 91.41%] [G loss: 1.358346]\n",
      "epoch:37 step:35605 [D loss: 0.498301, acc.: 79.69%] [G loss: 1.769066]\n",
      "epoch:37 step:35606 [D loss: 0.406307, acc.: 82.81%] [G loss: 1.158987]\n",
      "epoch:38 step:35607 [D loss: 0.535582, acc.: 73.44%] [G loss: 1.339918]\n",
      "epoch:38 step:35608 [D loss: 0.557616, acc.: 67.97%] [G loss: 1.254634]\n",
      "epoch:38 step:35609 [D loss: 0.714414, acc.: 59.38%] [G loss: 0.921164]\n",
      "epoch:38 step:35610 [D loss: 0.521169, acc.: 74.22%] [G loss: 1.257139]\n",
      "epoch:38 step:35611 [D loss: 0.367192, acc.: 86.72%] [G loss: 1.925633]\n",
      "epoch:38 step:35612 [D loss: 0.575419, acc.: 68.75%] [G loss: 1.182942]\n",
      "epoch:38 step:35613 [D loss: 0.555541, acc.: 69.53%] [G loss: 1.630217]\n",
      "epoch:38 step:35614 [D loss: 0.557876, acc.: 70.31%] [G loss: 1.579494]\n",
      "epoch:38 step:35615 [D loss: 0.591745, acc.: 65.62%] [G loss: 2.091148]\n",
      "epoch:38 step:35616 [D loss: 0.722045, acc.: 58.59%] [G loss: 1.063493]\n",
      "epoch:38 step:35617 [D loss: 0.426057, acc.: 81.25%] [G loss: 2.029286]\n",
      "epoch:38 step:35618 [D loss: 0.626429, acc.: 63.28%] [G loss: 1.618075]\n",
      "epoch:38 step:35619 [D loss: 0.443380, acc.: 78.91%] [G loss: 1.710642]\n",
      "epoch:38 step:35620 [D loss: 0.444193, acc.: 78.12%] [G loss: 1.752599]\n",
      "epoch:38 step:35621 [D loss: 0.535389, acc.: 73.44%] [G loss: 1.869861]\n",
      "epoch:38 step:35622 [D loss: 0.566113, acc.: 67.97%] [G loss: 1.368046]\n",
      "epoch:38 step:35623 [D loss: 0.491801, acc.: 71.88%] [G loss: 1.891884]\n",
      "epoch:38 step:35624 [D loss: 0.513811, acc.: 73.44%] [G loss: 1.829217]\n",
      "epoch:38 step:35625 [D loss: 0.479456, acc.: 78.12%] [G loss: 1.337546]\n",
      "epoch:38 step:35626 [D loss: 0.473411, acc.: 75.00%] [G loss: 1.043319]\n",
      "epoch:38 step:35627 [D loss: 0.488984, acc.: 77.34%] [G loss: 1.159139]\n",
      "epoch:38 step:35628 [D loss: 0.484078, acc.: 77.34%] [G loss: 1.319066]\n",
      "epoch:38 step:35629 [D loss: 0.517236, acc.: 70.31%] [G loss: 2.066092]\n",
      "epoch:38 step:35630 [D loss: 0.509081, acc.: 82.03%] [G loss: 1.824010]\n",
      "epoch:38 step:35631 [D loss: 0.437578, acc.: 77.34%] [G loss: 2.060608]\n",
      "epoch:38 step:35632 [D loss: 0.594292, acc.: 74.22%] [G loss: 1.866749]\n",
      "epoch:38 step:35633 [D loss: 0.326690, acc.: 92.19%] [G loss: 2.124782]\n",
      "epoch:38 step:35634 [D loss: 0.463001, acc.: 81.25%] [G loss: 1.012479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35635 [D loss: 0.351244, acc.: 86.72%] [G loss: 1.473281]\n",
      "epoch:38 step:35636 [D loss: 0.353141, acc.: 85.94%] [G loss: 1.192583]\n",
      "epoch:38 step:35637 [D loss: 0.572217, acc.: 70.31%] [G loss: 0.917159]\n",
      "epoch:38 step:35638 [D loss: 0.405254, acc.: 82.03%] [G loss: 1.776645]\n",
      "epoch:38 step:35639 [D loss: 0.280683, acc.: 91.41%] [G loss: 2.153811]\n",
      "epoch:38 step:35640 [D loss: 0.531697, acc.: 71.09%] [G loss: 1.649386]\n",
      "epoch:38 step:35641 [D loss: 0.374640, acc.: 85.94%] [G loss: 1.428027]\n",
      "epoch:38 step:35642 [D loss: 0.354244, acc.: 87.50%] [G loss: 1.652474]\n",
      "epoch:38 step:35643 [D loss: 0.535032, acc.: 71.88%] [G loss: 1.645059]\n",
      "epoch:38 step:35644 [D loss: 0.509576, acc.: 71.88%] [G loss: 1.418140]\n",
      "epoch:38 step:35645 [D loss: 0.599980, acc.: 69.53%] [G loss: 1.214036]\n",
      "epoch:38 step:35646 [D loss: 0.543162, acc.: 73.44%] [G loss: 1.016583]\n",
      "epoch:38 step:35647 [D loss: 0.533527, acc.: 73.44%] [G loss: 2.479379]\n",
      "epoch:38 step:35648 [D loss: 0.546347, acc.: 73.44%] [G loss: 1.814592]\n",
      "epoch:38 step:35649 [D loss: 0.732394, acc.: 58.59%] [G loss: 1.330066]\n",
      "epoch:38 step:35650 [D loss: 0.752194, acc.: 54.69%] [G loss: 1.081093]\n",
      "epoch:38 step:35651 [D loss: 0.618621, acc.: 63.28%] [G loss: 1.691439]\n",
      "epoch:38 step:35652 [D loss: 0.572934, acc.: 72.66%] [G loss: 1.367219]\n",
      "epoch:38 step:35653 [D loss: 0.497250, acc.: 77.34%] [G loss: 1.533043]\n",
      "epoch:38 step:35654 [D loss: 0.520643, acc.: 71.88%] [G loss: 1.481644]\n",
      "epoch:38 step:35655 [D loss: 0.425291, acc.: 79.69%] [G loss: 1.127165]\n",
      "epoch:38 step:35656 [D loss: 0.483708, acc.: 78.91%] [G loss: 1.323880]\n",
      "epoch:38 step:35657 [D loss: 0.524236, acc.: 73.44%] [G loss: 1.094443]\n",
      "epoch:38 step:35658 [D loss: 0.546706, acc.: 71.09%] [G loss: 1.447447]\n",
      "epoch:38 step:35659 [D loss: 0.408020, acc.: 82.81%] [G loss: 2.305648]\n",
      "epoch:38 step:35660 [D loss: 0.397076, acc.: 85.16%] [G loss: 1.485544]\n",
      "epoch:38 step:35661 [D loss: 0.841803, acc.: 46.88%] [G loss: 1.415210]\n",
      "epoch:38 step:35662 [D loss: 0.517690, acc.: 71.09%] [G loss: 2.346172]\n",
      "epoch:38 step:35663 [D loss: 0.564069, acc.: 70.31%] [G loss: 1.388412]\n",
      "epoch:38 step:35664 [D loss: 0.444684, acc.: 79.69%] [G loss: 2.071746]\n",
      "epoch:38 step:35665 [D loss: 0.358207, acc.: 86.72%] [G loss: 1.170445]\n",
      "epoch:38 step:35666 [D loss: 0.482726, acc.: 75.78%] [G loss: 1.957434]\n",
      "epoch:38 step:35667 [D loss: 0.553731, acc.: 68.75%] [G loss: 1.255804]\n",
      "epoch:38 step:35668 [D loss: 0.483629, acc.: 76.56%] [G loss: 1.358935]\n",
      "epoch:38 step:35669 [D loss: 0.580444, acc.: 67.97%] [G loss: 1.274354]\n",
      "epoch:38 step:35670 [D loss: 0.449128, acc.: 82.81%] [G loss: 1.180045]\n",
      "epoch:38 step:35671 [D loss: 0.398261, acc.: 85.16%] [G loss: 1.372660]\n",
      "epoch:38 step:35672 [D loss: 0.616150, acc.: 67.19%] [G loss: 1.314210]\n",
      "epoch:38 step:35673 [D loss: 0.524887, acc.: 71.88%] [G loss: 1.776609]\n",
      "epoch:38 step:35674 [D loss: 0.389406, acc.: 82.03%] [G loss: 2.557782]\n",
      "epoch:38 step:35675 [D loss: 0.464288, acc.: 79.69%] [G loss: 1.357645]\n",
      "epoch:38 step:35676 [D loss: 0.503942, acc.: 73.44%] [G loss: 1.506843]\n",
      "epoch:38 step:35677 [D loss: 0.432581, acc.: 78.12%] [G loss: 1.302528]\n",
      "epoch:38 step:35678 [D loss: 0.621515, acc.: 70.31%] [G loss: 1.578573]\n",
      "epoch:38 step:35679 [D loss: 0.359689, acc.: 84.38%] [G loss: 1.517623]\n",
      "epoch:38 step:35680 [D loss: 0.397005, acc.: 85.16%] [G loss: 2.120546]\n",
      "epoch:38 step:35681 [D loss: 0.429473, acc.: 80.47%] [G loss: 2.834897]\n",
      "epoch:38 step:35682 [D loss: 0.606468, acc.: 70.31%] [G loss: 1.813845]\n",
      "epoch:38 step:35683 [D loss: 0.493572, acc.: 77.34%] [G loss: 1.151932]\n",
      "epoch:38 step:35684 [D loss: 0.729835, acc.: 63.28%] [G loss: 1.315035]\n",
      "epoch:38 step:35685 [D loss: 0.634818, acc.: 65.62%] [G loss: 0.887090]\n",
      "epoch:38 step:35686 [D loss: 0.569616, acc.: 71.09%] [G loss: 1.425207]\n",
      "epoch:38 step:35687 [D loss: 0.534755, acc.: 75.00%] [G loss: 1.702995]\n",
      "epoch:38 step:35688 [D loss: 0.709587, acc.: 61.72%] [G loss: 1.530311]\n",
      "epoch:38 step:35689 [D loss: 0.762958, acc.: 53.12%] [G loss: 1.761701]\n",
      "epoch:38 step:35690 [D loss: 0.705582, acc.: 59.38%] [G loss: 1.627239]\n",
      "epoch:38 step:35691 [D loss: 0.591513, acc.: 68.75%] [G loss: 1.106885]\n",
      "epoch:38 step:35692 [D loss: 0.662768, acc.: 66.41%] [G loss: 1.320862]\n",
      "epoch:38 step:35693 [D loss: 0.435766, acc.: 80.47%] [G loss: 1.392226]\n",
      "epoch:38 step:35694 [D loss: 0.708255, acc.: 57.03%] [G loss: 1.419720]\n",
      "epoch:38 step:35695 [D loss: 0.376067, acc.: 85.16%] [G loss: 1.930297]\n",
      "epoch:38 step:35696 [D loss: 0.378434, acc.: 87.50%] [G loss: 1.393202]\n",
      "epoch:38 step:35697 [D loss: 0.604801, acc.: 65.62%] [G loss: 1.379230]\n",
      "epoch:38 step:35698 [D loss: 0.522075, acc.: 77.34%] [G loss: 1.048277]\n",
      "epoch:38 step:35699 [D loss: 0.548355, acc.: 71.88%] [G loss: 1.327393]\n",
      "epoch:38 step:35700 [D loss: 0.420513, acc.: 80.47%] [G loss: 1.511449]\n",
      "epoch:38 step:35701 [D loss: 0.411760, acc.: 79.69%] [G loss: 1.728020]\n",
      "epoch:38 step:35702 [D loss: 0.576681, acc.: 71.09%] [G loss: 1.587321]\n",
      "epoch:38 step:35703 [D loss: 0.623127, acc.: 64.84%] [G loss: 1.930271]\n",
      "epoch:38 step:35704 [D loss: 0.645341, acc.: 64.06%] [G loss: 2.105745]\n",
      "epoch:38 step:35705 [D loss: 0.601172, acc.: 66.41%] [G loss: 2.365833]\n",
      "epoch:38 step:35706 [D loss: 0.440270, acc.: 77.34%] [G loss: 1.613565]\n",
      "epoch:38 step:35707 [D loss: 0.832958, acc.: 43.75%] [G loss: 0.896231]\n",
      "epoch:38 step:35708 [D loss: 0.392465, acc.: 84.38%] [G loss: 1.587800]\n",
      "epoch:38 step:35709 [D loss: 0.455189, acc.: 77.34%] [G loss: 1.532004]\n",
      "epoch:38 step:35710 [D loss: 0.309809, acc.: 87.50%] [G loss: 1.321489]\n",
      "epoch:38 step:35711 [D loss: 0.332521, acc.: 87.50%] [G loss: 1.717739]\n",
      "epoch:38 step:35712 [D loss: 0.544733, acc.: 71.88%] [G loss: 1.103014]\n",
      "epoch:38 step:35713 [D loss: 0.446271, acc.: 79.69%] [G loss: 1.584111]\n",
      "epoch:38 step:35714 [D loss: 0.433588, acc.: 82.03%] [G loss: 1.587311]\n",
      "epoch:38 step:35715 [D loss: 0.581354, acc.: 70.31%] [G loss: 0.909068]\n",
      "epoch:38 step:35716 [D loss: 0.781522, acc.: 52.34%] [G loss: 1.350463]\n",
      "epoch:38 step:35717 [D loss: 0.533929, acc.: 75.78%] [G loss: 1.354646]\n",
      "epoch:38 step:35718 [D loss: 0.727328, acc.: 61.72%] [G loss: 1.422073]\n",
      "epoch:38 step:35719 [D loss: 0.369996, acc.: 86.72%] [G loss: 1.581938]\n",
      "epoch:38 step:35720 [D loss: 0.582379, acc.: 73.44%] [G loss: 1.700393]\n",
      "epoch:38 step:35721 [D loss: 0.228341, acc.: 96.09%] [G loss: 1.712878]\n",
      "epoch:38 step:35722 [D loss: 0.611214, acc.: 67.19%] [G loss: 1.426140]\n",
      "epoch:38 step:35723 [D loss: 0.639685, acc.: 62.50%] [G loss: 1.526478]\n",
      "epoch:38 step:35724 [D loss: 0.666781, acc.: 60.94%] [G loss: 2.070069]\n",
      "epoch:38 step:35725 [D loss: 0.346074, acc.: 85.94%] [G loss: 1.622863]\n",
      "epoch:38 step:35726 [D loss: 0.542417, acc.: 72.66%] [G loss: 1.376799]\n",
      "epoch:38 step:35727 [D loss: 0.706844, acc.: 58.59%] [G loss: 1.395557]\n",
      "epoch:38 step:35728 [D loss: 0.738641, acc.: 60.16%] [G loss: 1.781358]\n",
      "epoch:38 step:35729 [D loss: 0.663864, acc.: 63.28%] [G loss: 1.466483]\n",
      "epoch:38 step:35730 [D loss: 0.457880, acc.: 75.00%] [G loss: 1.818304]\n",
      "epoch:38 step:35731 [D loss: 0.832683, acc.: 48.44%] [G loss: 1.700271]\n",
      "epoch:38 step:35732 [D loss: 0.575023, acc.: 71.88%] [G loss: 1.645814]\n",
      "epoch:38 step:35733 [D loss: 0.585527, acc.: 71.09%] [G loss: 1.441438]\n",
      "epoch:38 step:35734 [D loss: 0.461794, acc.: 80.47%] [G loss: 1.484596]\n",
      "epoch:38 step:35735 [D loss: 0.589437, acc.: 69.53%] [G loss: 2.028204]\n",
      "epoch:38 step:35736 [D loss: 0.440217, acc.: 76.56%] [G loss: 1.983281]\n",
      "epoch:38 step:35737 [D loss: 0.491380, acc.: 78.91%] [G loss: 1.836752]\n",
      "epoch:38 step:35738 [D loss: 0.599275, acc.: 68.75%] [G loss: 1.323359]\n",
      "epoch:38 step:35739 [D loss: 0.396001, acc.: 85.94%] [G loss: 2.246126]\n",
      "epoch:38 step:35740 [D loss: 0.620910, acc.: 67.97%] [G loss: 1.794964]\n",
      "epoch:38 step:35741 [D loss: 0.360838, acc.: 85.94%] [G loss: 1.121214]\n",
      "epoch:38 step:35742 [D loss: 0.659946, acc.: 59.38%] [G loss: 1.335168]\n",
      "epoch:38 step:35743 [D loss: 0.766318, acc.: 53.91%] [G loss: 1.483374]\n",
      "epoch:38 step:35744 [D loss: 0.334736, acc.: 92.19%] [G loss: 2.027076]\n",
      "epoch:38 step:35745 [D loss: 0.766121, acc.: 64.06%] [G loss: 1.476468]\n",
      "epoch:38 step:35746 [D loss: 0.637610, acc.: 67.19%] [G loss: 1.053961]\n",
      "epoch:38 step:35747 [D loss: 0.549683, acc.: 72.66%] [G loss: 1.477372]\n",
      "epoch:38 step:35748 [D loss: 0.500658, acc.: 71.09%] [G loss: 1.844577]\n",
      "epoch:38 step:35749 [D loss: 0.731441, acc.: 57.81%] [G loss: 1.495970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35750 [D loss: 0.401349, acc.: 82.03%] [G loss: 1.792056]\n",
      "epoch:38 step:35751 [D loss: 0.346245, acc.: 87.50%] [G loss: 2.044639]\n",
      "epoch:38 step:35752 [D loss: 0.523506, acc.: 75.78%] [G loss: 1.970600]\n",
      "epoch:38 step:35753 [D loss: 0.352314, acc.: 89.84%] [G loss: 1.787398]\n",
      "epoch:38 step:35754 [D loss: 0.537973, acc.: 71.88%] [G loss: 2.001659]\n",
      "epoch:38 step:35755 [D loss: 0.756128, acc.: 60.16%] [G loss: 1.153007]\n",
      "epoch:38 step:35756 [D loss: 0.586443, acc.: 69.53%] [G loss: 1.222708]\n",
      "epoch:38 step:35757 [D loss: 0.457022, acc.: 80.47%] [G loss: 1.402018]\n",
      "epoch:38 step:35758 [D loss: 0.495828, acc.: 74.22%] [G loss: 1.013635]\n",
      "epoch:38 step:35759 [D loss: 0.294036, acc.: 93.75%] [G loss: 1.479236]\n",
      "epoch:38 step:35760 [D loss: 0.565856, acc.: 71.88%] [G loss: 1.942232]\n",
      "epoch:38 step:35761 [D loss: 0.578817, acc.: 67.97%] [G loss: 1.015592]\n",
      "epoch:38 step:35762 [D loss: 0.496384, acc.: 76.56%] [G loss: 1.640782]\n",
      "epoch:38 step:35763 [D loss: 0.493993, acc.: 77.34%] [G loss: 1.570636]\n",
      "epoch:38 step:35764 [D loss: 0.567216, acc.: 71.88%] [G loss: 1.812511]\n",
      "epoch:38 step:35765 [D loss: 0.431660, acc.: 80.47%] [G loss: 1.741627]\n",
      "epoch:38 step:35766 [D loss: 0.418866, acc.: 82.81%] [G loss: 1.708573]\n",
      "epoch:38 step:35767 [D loss: 0.608266, acc.: 70.31%] [G loss: 2.258801]\n",
      "epoch:38 step:35768 [D loss: 0.514447, acc.: 75.00%] [G loss: 2.271586]\n",
      "epoch:38 step:35769 [D loss: 0.538175, acc.: 71.09%] [G loss: 1.144289]\n",
      "epoch:38 step:35770 [D loss: 0.515060, acc.: 75.78%] [G loss: 1.633127]\n",
      "epoch:38 step:35771 [D loss: 0.309910, acc.: 89.84%] [G loss: 1.787087]\n",
      "epoch:38 step:35772 [D loss: 0.570739, acc.: 67.19%] [G loss: 2.124948]\n",
      "epoch:38 step:35773 [D loss: 0.422484, acc.: 81.25%] [G loss: 1.289534]\n",
      "epoch:38 step:35774 [D loss: 0.586246, acc.: 68.75%] [G loss: 1.194761]\n",
      "epoch:38 step:35775 [D loss: 0.279409, acc.: 90.62%] [G loss: 1.295622]\n",
      "epoch:38 step:35776 [D loss: 0.426544, acc.: 79.69%] [G loss: 1.661167]\n",
      "epoch:38 step:35777 [D loss: 0.541646, acc.: 74.22%] [G loss: 1.639467]\n",
      "epoch:38 step:35778 [D loss: 0.434561, acc.: 80.47%] [G loss: 1.330828]\n",
      "epoch:38 step:35779 [D loss: 0.614723, acc.: 70.31%] [G loss: 1.552560]\n",
      "epoch:38 step:35780 [D loss: 0.359409, acc.: 82.81%] [G loss: 2.144153]\n",
      "epoch:38 step:35781 [D loss: 0.900642, acc.: 46.09%] [G loss: 1.038965]\n",
      "epoch:38 step:35782 [D loss: 0.560959, acc.: 69.53%] [G loss: 1.879286]\n",
      "epoch:38 step:35783 [D loss: 0.392688, acc.: 87.50%] [G loss: 1.476208]\n",
      "epoch:38 step:35784 [D loss: 0.633049, acc.: 67.19%] [G loss: 1.511502]\n",
      "epoch:38 step:35785 [D loss: 0.406474, acc.: 85.94%] [G loss: 1.317691]\n",
      "epoch:38 step:35786 [D loss: 0.435814, acc.: 78.91%] [G loss: 1.553214]\n",
      "epoch:38 step:35787 [D loss: 0.528964, acc.: 71.09%] [G loss: 1.919730]\n",
      "epoch:38 step:35788 [D loss: 0.522961, acc.: 78.12%] [G loss: 1.589555]\n",
      "epoch:38 step:35789 [D loss: 0.784944, acc.: 53.12%] [G loss: 1.327379]\n",
      "epoch:38 step:35790 [D loss: 0.572241, acc.: 73.44%] [G loss: 1.815066]\n",
      "epoch:38 step:35791 [D loss: 0.458983, acc.: 78.12%] [G loss: 2.256936]\n",
      "epoch:38 step:35792 [D loss: 0.460836, acc.: 82.81%] [G loss: 1.193352]\n",
      "epoch:38 step:35793 [D loss: 0.389033, acc.: 81.25%] [G loss: 1.743867]\n",
      "epoch:38 step:35794 [D loss: 0.431514, acc.: 85.16%] [G loss: 1.468575]\n",
      "epoch:38 step:35795 [D loss: 0.518870, acc.: 72.66%] [G loss: 1.861590]\n",
      "epoch:38 step:35796 [D loss: 0.652628, acc.: 69.53%] [G loss: 1.670652]\n",
      "epoch:38 step:35797 [D loss: 0.534512, acc.: 76.56%] [G loss: 1.618002]\n",
      "epoch:38 step:35798 [D loss: 0.564246, acc.: 70.31%] [G loss: 1.833457]\n",
      "epoch:38 step:35799 [D loss: 0.453787, acc.: 80.47%] [G loss: 1.559053]\n",
      "epoch:38 step:35800 [D loss: 0.523165, acc.: 70.31%] [G loss: 1.674351]\n",
      "##############\n",
      "[2.78196956 2.18593489 1.94432924 3.11925457 0.99316668 6.25068333\n",
      " 2.11249905 2.80958466 4.0125115  5.35437946]\n",
      "##########\n",
      "epoch:38 step:35801 [D loss: 0.602658, acc.: 67.19%] [G loss: 1.749607]\n",
      "epoch:38 step:35802 [D loss: 0.385584, acc.: 86.72%] [G loss: 1.758711]\n",
      "epoch:38 step:35803 [D loss: 0.460844, acc.: 82.03%] [G loss: 1.943532]\n",
      "epoch:38 step:35804 [D loss: 0.393030, acc.: 84.38%] [G loss: 1.198248]\n",
      "epoch:38 step:35805 [D loss: 0.456420, acc.: 81.25%] [G loss: 1.725284]\n",
      "epoch:38 step:35806 [D loss: 0.463382, acc.: 76.56%] [G loss: 1.869251]\n",
      "epoch:38 step:35807 [D loss: 0.254911, acc.: 93.75%] [G loss: 1.528076]\n",
      "epoch:38 step:35808 [D loss: 0.355048, acc.: 86.72%] [G loss: 1.794686]\n",
      "epoch:38 step:35809 [D loss: 0.382042, acc.: 85.94%] [G loss: 1.930757]\n",
      "epoch:38 step:35810 [D loss: 0.369628, acc.: 85.16%] [G loss: 1.862654]\n",
      "epoch:38 step:35811 [D loss: 0.581647, acc.: 71.09%] [G loss: 1.707816]\n",
      "epoch:38 step:35812 [D loss: 0.798453, acc.: 58.59%] [G loss: 1.885805]\n",
      "epoch:38 step:35813 [D loss: 0.480100, acc.: 78.91%] [G loss: 1.470793]\n",
      "epoch:38 step:35814 [D loss: 0.418108, acc.: 79.69%] [G loss: 1.857942]\n",
      "epoch:38 step:35815 [D loss: 0.546478, acc.: 71.88%] [G loss: 1.246914]\n",
      "epoch:38 step:35816 [D loss: 0.436702, acc.: 80.47%] [G loss: 1.521829]\n",
      "epoch:38 step:35817 [D loss: 0.440350, acc.: 78.91%] [G loss: 1.639149]\n",
      "epoch:38 step:35818 [D loss: 0.708061, acc.: 60.94%] [G loss: 1.016238]\n",
      "epoch:38 step:35819 [D loss: 0.630468, acc.: 64.84%] [G loss: 1.645986]\n",
      "epoch:38 step:35820 [D loss: 0.722284, acc.: 57.03%] [G loss: 1.971039]\n",
      "epoch:38 step:35821 [D loss: 0.517992, acc.: 73.44%] [G loss: 1.427750]\n",
      "epoch:38 step:35822 [D loss: 0.530489, acc.: 75.78%] [G loss: 1.429404]\n",
      "epoch:38 step:35823 [D loss: 0.442772, acc.: 79.69%] [G loss: 1.888755]\n",
      "epoch:38 step:35824 [D loss: 0.634837, acc.: 68.75%] [G loss: 1.157588]\n",
      "epoch:38 step:35825 [D loss: 0.698189, acc.: 59.38%] [G loss: 1.090594]\n",
      "epoch:38 step:35826 [D loss: 0.729023, acc.: 57.03%] [G loss: 1.152445]\n",
      "epoch:38 step:35827 [D loss: 0.556056, acc.: 72.66%] [G loss: 1.562500]\n",
      "epoch:38 step:35828 [D loss: 0.621628, acc.: 64.84%] [G loss: 1.399705]\n",
      "epoch:38 step:35829 [D loss: 0.411460, acc.: 86.72%] [G loss: 1.391260]\n",
      "epoch:38 step:35830 [D loss: 0.503189, acc.: 77.34%] [G loss: 1.730691]\n",
      "epoch:38 step:35831 [D loss: 0.757444, acc.: 59.38%] [G loss: 1.711386]\n",
      "epoch:38 step:35832 [D loss: 0.467440, acc.: 79.69%] [G loss: 1.430212]\n",
      "epoch:38 step:35833 [D loss: 0.532448, acc.: 69.53%] [G loss: 1.616520]\n",
      "epoch:38 step:35834 [D loss: 0.566994, acc.: 68.75%] [G loss: 1.136828]\n",
      "epoch:38 step:35835 [D loss: 0.631880, acc.: 63.28%] [G loss: 1.179849]\n",
      "epoch:38 step:35836 [D loss: 0.414050, acc.: 82.03%] [G loss: 1.145291]\n",
      "epoch:38 step:35837 [D loss: 0.474802, acc.: 75.78%] [G loss: 1.775348]\n",
      "epoch:38 step:35838 [D loss: 0.495117, acc.: 75.78%] [G loss: 1.952656]\n",
      "epoch:38 step:35839 [D loss: 0.518605, acc.: 74.22%] [G loss: 2.386661]\n",
      "epoch:38 step:35840 [D loss: 0.434013, acc.: 78.91%] [G loss: 1.592205]\n",
      "epoch:38 step:35841 [D loss: 0.761670, acc.: 53.91%] [G loss: 1.101857]\n",
      "epoch:38 step:35842 [D loss: 0.432371, acc.: 78.91%] [G loss: 1.419599]\n",
      "epoch:38 step:35843 [D loss: 0.447240, acc.: 82.81%] [G loss: 2.051696]\n",
      "epoch:38 step:35844 [D loss: 0.651385, acc.: 64.06%] [G loss: 1.621701]\n",
      "epoch:38 step:35845 [D loss: 0.300027, acc.: 91.41%] [G loss: 1.585272]\n",
      "epoch:38 step:35846 [D loss: 0.575512, acc.: 71.88%] [G loss: 1.722744]\n",
      "epoch:38 step:35847 [D loss: 0.654216, acc.: 66.41%] [G loss: 1.326677]\n",
      "epoch:38 step:35848 [D loss: 0.647625, acc.: 61.72%] [G loss: 1.555430]\n",
      "epoch:38 step:35849 [D loss: 0.666753, acc.: 64.06%] [G loss: 1.336035]\n",
      "epoch:38 step:35850 [D loss: 0.427707, acc.: 82.03%] [G loss: 1.752550]\n",
      "epoch:38 step:35851 [D loss: 0.512354, acc.: 76.56%] [G loss: 1.394173]\n",
      "epoch:38 step:35852 [D loss: 0.499463, acc.: 76.56%] [G loss: 1.590015]\n",
      "epoch:38 step:35853 [D loss: 0.692238, acc.: 61.72%] [G loss: 1.515705]\n",
      "epoch:38 step:35854 [D loss: 0.545066, acc.: 71.09%] [G loss: 1.272307]\n",
      "epoch:38 step:35855 [D loss: 0.278360, acc.: 91.41%] [G loss: 1.137139]\n",
      "epoch:38 step:35856 [D loss: 0.573199, acc.: 69.53%] [G loss: 1.388879]\n",
      "epoch:38 step:35857 [D loss: 0.471932, acc.: 76.56%] [G loss: 1.796273]\n",
      "epoch:38 step:35858 [D loss: 0.538674, acc.: 71.09%] [G loss: 1.911376]\n",
      "epoch:38 step:35859 [D loss: 0.599184, acc.: 70.31%] [G loss: 1.575823]\n",
      "epoch:38 step:35860 [D loss: 0.577819, acc.: 65.62%] [G loss: 1.063683]\n",
      "epoch:38 step:35861 [D loss: 0.334983, acc.: 89.06%] [G loss: 1.727403]\n",
      "epoch:38 step:35862 [D loss: 0.653840, acc.: 64.84%] [G loss: 1.412241]\n",
      "epoch:38 step:35863 [D loss: 0.657947, acc.: 65.62%] [G loss: 0.950848]\n",
      "epoch:38 step:35864 [D loss: 0.623554, acc.: 68.75%] [G loss: 1.633031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35865 [D loss: 0.682448, acc.: 59.38%] [G loss: 1.319280]\n",
      "epoch:38 step:35866 [D loss: 0.613236, acc.: 67.19%] [G loss: 1.538366]\n",
      "epoch:38 step:35867 [D loss: 0.516392, acc.: 71.88%] [G loss: 1.886452]\n",
      "epoch:38 step:35868 [D loss: 0.770224, acc.: 53.91%] [G loss: 0.972848]\n",
      "epoch:38 step:35869 [D loss: 0.397340, acc.: 83.59%] [G loss: 1.250895]\n",
      "epoch:38 step:35870 [D loss: 0.557545, acc.: 69.53%] [G loss: 1.629360]\n",
      "epoch:38 step:35871 [D loss: 0.258823, acc.: 92.19%] [G loss: 2.094791]\n",
      "epoch:38 step:35872 [D loss: 0.475451, acc.: 77.34%] [G loss: 1.746099]\n",
      "epoch:38 step:35873 [D loss: 0.620093, acc.: 65.62%] [G loss: 0.960735]\n",
      "epoch:38 step:35874 [D loss: 0.335154, acc.: 89.06%] [G loss: 1.426126]\n",
      "epoch:38 step:35875 [D loss: 0.461624, acc.: 77.34%] [G loss: 1.815343]\n",
      "epoch:38 step:35876 [D loss: 0.480110, acc.: 79.69%] [G loss: 1.916573]\n",
      "epoch:38 step:35877 [D loss: 0.472339, acc.: 77.34%] [G loss: 1.501429]\n",
      "epoch:38 step:35878 [D loss: 0.404354, acc.: 81.25%] [G loss: 1.362274]\n",
      "epoch:38 step:35879 [D loss: 0.566742, acc.: 75.00%] [G loss: 1.177480]\n",
      "epoch:38 step:35880 [D loss: 0.359573, acc.: 89.06%] [G loss: 1.465632]\n",
      "epoch:38 step:35881 [D loss: 0.490006, acc.: 78.91%] [G loss: 1.400580]\n",
      "epoch:38 step:35882 [D loss: 0.463532, acc.: 79.69%] [G loss: 1.598920]\n",
      "epoch:38 step:35883 [D loss: 0.547379, acc.: 69.53%] [G loss: 1.349677]\n",
      "epoch:38 step:35884 [D loss: 0.459524, acc.: 78.91%] [G loss: 1.995019]\n",
      "epoch:38 step:35885 [D loss: 0.722998, acc.: 57.81%] [G loss: 1.098325]\n",
      "epoch:38 step:35886 [D loss: 0.548358, acc.: 71.09%] [G loss: 1.309975]\n",
      "epoch:38 step:35887 [D loss: 0.541923, acc.: 69.53%] [G loss: 1.284822]\n",
      "epoch:38 step:35888 [D loss: 0.447973, acc.: 82.03%] [G loss: 1.253815]\n",
      "epoch:38 step:35889 [D loss: 0.545368, acc.: 69.53%] [G loss: 1.472891]\n",
      "epoch:38 step:35890 [D loss: 0.733256, acc.: 57.81%] [G loss: 1.729228]\n",
      "epoch:38 step:35891 [D loss: 0.386694, acc.: 85.94%] [G loss: 1.326520]\n",
      "epoch:38 step:35892 [D loss: 0.600036, acc.: 66.41%] [G loss: 1.591180]\n",
      "epoch:38 step:35893 [D loss: 0.458632, acc.: 80.47%] [G loss: 1.200422]\n",
      "epoch:38 step:35894 [D loss: 0.562505, acc.: 70.31%] [G loss: 1.310810]\n",
      "epoch:38 step:35895 [D loss: 0.617234, acc.: 64.84%] [G loss: 1.101273]\n",
      "epoch:38 step:35896 [D loss: 0.371629, acc.: 84.38%] [G loss: 1.802100]\n",
      "epoch:38 step:35897 [D loss: 0.602787, acc.: 66.41%] [G loss: 1.677800]\n",
      "epoch:38 step:35898 [D loss: 0.509852, acc.: 75.00%] [G loss: 1.385513]\n",
      "epoch:38 step:35899 [D loss: 0.683821, acc.: 63.28%] [G loss: 1.575252]\n",
      "epoch:38 step:35900 [D loss: 0.564784, acc.: 71.09%] [G loss: 1.445235]\n",
      "epoch:38 step:35901 [D loss: 0.762166, acc.: 53.12%] [G loss: 1.646929]\n",
      "epoch:38 step:35902 [D loss: 0.305314, acc.: 91.41%] [G loss: 1.871587]\n",
      "epoch:38 step:35903 [D loss: 0.423769, acc.: 78.91%] [G loss: 2.016344]\n",
      "epoch:38 step:35904 [D loss: 0.550651, acc.: 69.53%] [G loss: 1.397766]\n",
      "epoch:38 step:35905 [D loss: 0.462605, acc.: 78.91%] [G loss: 1.129407]\n",
      "epoch:38 step:35906 [D loss: 0.569978, acc.: 68.75%] [G loss: 0.921126]\n",
      "epoch:38 step:35907 [D loss: 0.759569, acc.: 59.38%] [G loss: 1.680513]\n",
      "epoch:38 step:35908 [D loss: 0.327877, acc.: 89.84%] [G loss: 1.615272]\n",
      "epoch:38 step:35909 [D loss: 0.592597, acc.: 68.75%] [G loss: 1.038704]\n",
      "epoch:38 step:35910 [D loss: 0.700018, acc.: 56.25%] [G loss: 1.462856]\n",
      "epoch:38 step:35911 [D loss: 0.516385, acc.: 72.66%] [G loss: 1.123055]\n",
      "epoch:38 step:35912 [D loss: 0.446993, acc.: 78.12%] [G loss: 1.822062]\n",
      "epoch:38 step:35913 [D loss: 0.456181, acc.: 77.34%] [G loss: 1.783576]\n",
      "epoch:38 step:35914 [D loss: 0.445098, acc.: 80.47%] [G loss: 1.442537]\n",
      "epoch:38 step:35915 [D loss: 0.565457, acc.: 71.09%] [G loss: 1.877953]\n",
      "epoch:38 step:35916 [D loss: 0.510126, acc.: 74.22%] [G loss: 1.376378]\n",
      "epoch:38 step:35917 [D loss: 0.302128, acc.: 93.75%] [G loss: 1.817291]\n",
      "epoch:38 step:35918 [D loss: 0.549073, acc.: 71.88%] [G loss: 1.797952]\n",
      "epoch:38 step:35919 [D loss: 0.461950, acc.: 75.00%] [G loss: 1.139884]\n",
      "epoch:38 step:35920 [D loss: 0.345254, acc.: 86.72%] [G loss: 1.759407]\n",
      "epoch:38 step:35921 [D loss: 0.481742, acc.: 77.34%] [G loss: 1.567342]\n",
      "epoch:38 step:35922 [D loss: 0.500196, acc.: 75.78%] [G loss: 1.027988]\n",
      "epoch:38 step:35923 [D loss: 0.776288, acc.: 50.78%] [G loss: 1.186498]\n",
      "epoch:38 step:35924 [D loss: 0.555453, acc.: 71.09%] [G loss: 1.608821]\n",
      "epoch:38 step:35925 [D loss: 0.352355, acc.: 88.28%] [G loss: 2.480569]\n",
      "epoch:38 step:35926 [D loss: 0.543614, acc.: 69.53%] [G loss: 1.793270]\n",
      "epoch:38 step:35927 [D loss: 0.788521, acc.: 55.47%] [G loss: 1.171375]\n",
      "epoch:38 step:35928 [D loss: 0.450971, acc.: 75.00%] [G loss: 1.873115]\n",
      "epoch:38 step:35929 [D loss: 0.673347, acc.: 58.59%] [G loss: 1.390661]\n",
      "epoch:38 step:35930 [D loss: 0.473210, acc.: 78.12%] [G loss: 2.428154]\n",
      "epoch:38 step:35931 [D loss: 0.395040, acc.: 83.59%] [G loss: 1.854317]\n",
      "epoch:38 step:35932 [D loss: 0.506592, acc.: 77.34%] [G loss: 1.555151]\n",
      "epoch:38 step:35933 [D loss: 0.488553, acc.: 79.69%] [G loss: 1.719089]\n",
      "epoch:38 step:35934 [D loss: 0.633763, acc.: 67.19%] [G loss: 1.599937]\n",
      "epoch:38 step:35935 [D loss: 0.575166, acc.: 71.09%] [G loss: 1.363813]\n",
      "epoch:38 step:35936 [D loss: 0.521033, acc.: 70.31%] [G loss: 1.675930]\n",
      "epoch:38 step:35937 [D loss: 0.524995, acc.: 75.78%] [G loss: 1.304893]\n",
      "epoch:38 step:35938 [D loss: 0.520733, acc.: 74.22%] [G loss: 1.601728]\n",
      "epoch:38 step:35939 [D loss: 0.417261, acc.: 84.38%] [G loss: 2.135510]\n",
      "epoch:38 step:35940 [D loss: 0.461327, acc.: 79.69%] [G loss: 2.121053]\n",
      "epoch:38 step:35941 [D loss: 0.455596, acc.: 78.12%] [G loss: 1.554683]\n",
      "epoch:38 step:35942 [D loss: 0.410688, acc.: 85.94%] [G loss: 1.543453]\n",
      "epoch:38 step:35943 [D loss: 0.452374, acc.: 80.47%] [G loss: 1.711041]\n",
      "epoch:38 step:35944 [D loss: 0.465899, acc.: 78.12%] [G loss: 1.408780]\n",
      "epoch:38 step:35945 [D loss: 0.410666, acc.: 84.38%] [G loss: 1.427254]\n",
      "epoch:38 step:35946 [D loss: 0.438113, acc.: 80.47%] [G loss: 1.508963]\n",
      "epoch:38 step:35947 [D loss: 0.665858, acc.: 62.50%] [G loss: 1.445016]\n",
      "epoch:38 step:35948 [D loss: 0.568450, acc.: 64.84%] [G loss: 1.456319]\n",
      "epoch:38 step:35949 [D loss: 0.508586, acc.: 71.09%] [G loss: 1.565678]\n",
      "epoch:38 step:35950 [D loss: 0.345730, acc.: 89.06%] [G loss: 1.730861]\n",
      "epoch:38 step:35951 [D loss: 0.527423, acc.: 75.78%] [G loss: 1.518014]\n",
      "epoch:38 step:35952 [D loss: 0.584580, acc.: 68.75%] [G loss: 1.812318]\n",
      "epoch:38 step:35953 [D loss: 0.663775, acc.: 66.41%] [G loss: 0.890380]\n",
      "epoch:38 step:35954 [D loss: 0.695806, acc.: 65.62%] [G loss: 1.731815]\n",
      "epoch:38 step:35955 [D loss: 0.548990, acc.: 71.88%] [G loss: 1.915538]\n",
      "epoch:38 step:35956 [D loss: 0.546360, acc.: 71.88%] [G loss: 2.199291]\n",
      "epoch:38 step:35957 [D loss: 0.516361, acc.: 73.44%] [G loss: 1.268903]\n",
      "epoch:38 step:35958 [D loss: 0.587106, acc.: 67.97%] [G loss: 1.499443]\n",
      "epoch:38 step:35959 [D loss: 0.393057, acc.: 88.28%] [G loss: 1.380977]\n",
      "epoch:38 step:35960 [D loss: 0.443558, acc.: 78.91%] [G loss: 1.418015]\n",
      "epoch:38 step:35961 [D loss: 0.414547, acc.: 81.25%] [G loss: 1.203693]\n",
      "epoch:38 step:35962 [D loss: 0.535731, acc.: 71.09%] [G loss: 1.351346]\n",
      "epoch:38 step:35963 [D loss: 0.459670, acc.: 79.69%] [G loss: 1.942500]\n",
      "epoch:38 step:35964 [D loss: 0.671866, acc.: 67.19%] [G loss: 1.336921]\n",
      "epoch:38 step:35965 [D loss: 0.479768, acc.: 77.34%] [G loss: 1.876697]\n",
      "epoch:38 step:35966 [D loss: 0.366932, acc.: 84.38%] [G loss: 2.284353]\n",
      "epoch:38 step:35967 [D loss: 0.564595, acc.: 73.44%] [G loss: 1.338854]\n",
      "epoch:38 step:35968 [D loss: 0.564276, acc.: 74.22%] [G loss: 1.440921]\n",
      "epoch:38 step:35969 [D loss: 0.403487, acc.: 86.72%] [G loss: 1.384385]\n",
      "epoch:38 step:35970 [D loss: 0.608795, acc.: 67.97%] [G loss: 1.842925]\n",
      "epoch:38 step:35971 [D loss: 0.726620, acc.: 54.69%] [G loss: 1.108540]\n",
      "epoch:38 step:35972 [D loss: 0.588919, acc.: 70.31%] [G loss: 1.503804]\n",
      "epoch:38 step:35973 [D loss: 0.398265, acc.: 84.38%] [G loss: 1.616293]\n",
      "epoch:38 step:35974 [D loss: 0.617998, acc.: 66.41%] [G loss: 1.355421]\n",
      "epoch:38 step:35975 [D loss: 0.625133, acc.: 61.72%] [G loss: 1.946185]\n",
      "epoch:38 step:35976 [D loss: 0.453049, acc.: 82.03%] [G loss: 1.615718]\n",
      "epoch:38 step:35977 [D loss: 0.516204, acc.: 74.22%] [G loss: 1.751529]\n",
      "epoch:38 step:35978 [D loss: 0.374191, acc.: 84.38%] [G loss: 1.976849]\n",
      "epoch:38 step:35979 [D loss: 0.676031, acc.: 63.28%] [G loss: 1.250064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35980 [D loss: 0.557484, acc.: 67.19%] [G loss: 2.088224]\n",
      "epoch:38 step:35981 [D loss: 0.703580, acc.: 58.59%] [G loss: 0.838813]\n",
      "epoch:38 step:35982 [D loss: 0.499878, acc.: 76.56%] [G loss: 1.525378]\n",
      "epoch:38 step:35983 [D loss: 0.582472, acc.: 67.97%] [G loss: 1.340033]\n",
      "epoch:38 step:35984 [D loss: 0.379952, acc.: 82.81%] [G loss: 1.612898]\n",
      "epoch:38 step:35985 [D loss: 0.366538, acc.: 85.16%] [G loss: 2.251797]\n",
      "epoch:38 step:35986 [D loss: 0.555046, acc.: 72.66%] [G loss: 1.306989]\n",
      "epoch:38 step:35987 [D loss: 0.417010, acc.: 80.47%] [G loss: 1.638192]\n",
      "epoch:38 step:35988 [D loss: 0.555295, acc.: 71.09%] [G loss: 1.327684]\n",
      "epoch:38 step:35989 [D loss: 0.396057, acc.: 82.81%] [G loss: 1.769616]\n",
      "epoch:38 step:35990 [D loss: 0.563625, acc.: 70.31%] [G loss: 1.625363]\n",
      "epoch:38 step:35991 [D loss: 0.640733, acc.: 66.41%] [G loss: 1.212153]\n",
      "epoch:38 step:35992 [D loss: 0.537356, acc.: 75.78%] [G loss: 1.202945]\n",
      "epoch:38 step:35993 [D loss: 0.384376, acc.: 85.94%] [G loss: 1.708866]\n",
      "epoch:38 step:35994 [D loss: 0.440630, acc.: 83.59%] [G loss: 1.747649]\n",
      "epoch:38 step:35995 [D loss: 0.382040, acc.: 86.72%] [G loss: 2.204422]\n",
      "epoch:38 step:35996 [D loss: 0.597450, acc.: 64.84%] [G loss: 1.221339]\n",
      "epoch:38 step:35997 [D loss: 0.724617, acc.: 62.50%] [G loss: 1.510928]\n",
      "epoch:38 step:35998 [D loss: 0.450792, acc.: 81.25%] [G loss: 1.954394]\n",
      "epoch:38 step:35999 [D loss: 0.467160, acc.: 78.91%] [G loss: 1.405071]\n",
      "epoch:38 step:36000 [D loss: 0.556011, acc.: 73.44%] [G loss: 1.065351]\n",
      "##############\n",
      "[2.71961959 1.95369749 1.79708167 3.0236056  0.67934901 6.04779813\n",
      " 2.22945343 2.63510772 3.78884354 7.14868929]\n",
      "##########\n",
      "epoch:38 step:36001 [D loss: 0.431771, acc.: 81.25%] [G loss: 1.442870]\n",
      "epoch:38 step:36002 [D loss: 0.602294, acc.: 68.75%] [G loss: 1.592736]\n",
      "epoch:38 step:36003 [D loss: 0.619622, acc.: 68.75%] [G loss: 1.182959]\n",
      "epoch:38 step:36004 [D loss: 0.621526, acc.: 64.84%] [G loss: 1.484603]\n",
      "epoch:38 step:36005 [D loss: 0.523333, acc.: 73.44%] [G loss: 1.481414]\n",
      "epoch:38 step:36006 [D loss: 0.531874, acc.: 70.31%] [G loss: 1.951396]\n",
      "epoch:38 step:36007 [D loss: 0.599580, acc.: 70.31%] [G loss: 1.864230]\n",
      "epoch:38 step:36008 [D loss: 0.560129, acc.: 74.22%] [G loss: 1.119091]\n",
      "epoch:38 step:36009 [D loss: 0.388371, acc.: 85.16%] [G loss: 1.612429]\n",
      "epoch:38 step:36010 [D loss: 0.550517, acc.: 68.75%] [G loss: 1.616183]\n",
      "epoch:38 step:36011 [D loss: 0.566521, acc.: 70.31%] [G loss: 1.305558]\n",
      "epoch:38 step:36012 [D loss: 0.418799, acc.: 85.16%] [G loss: 1.709389]\n",
      "epoch:38 step:36013 [D loss: 0.417965, acc.: 79.69%] [G loss: 1.498346]\n",
      "epoch:38 step:36014 [D loss: 0.433657, acc.: 81.25%] [G loss: 1.599622]\n",
      "epoch:38 step:36015 [D loss: 0.553751, acc.: 69.53%] [G loss: 1.970242]\n",
      "epoch:38 step:36016 [D loss: 0.822370, acc.: 56.25%] [G loss: 1.319598]\n",
      "epoch:38 step:36017 [D loss: 0.480216, acc.: 75.00%] [G loss: 1.768997]\n",
      "epoch:38 step:36018 [D loss: 0.495214, acc.: 79.69%] [G loss: 1.048167]\n",
      "epoch:38 step:36019 [D loss: 0.496402, acc.: 76.56%] [G loss: 1.379546]\n",
      "epoch:38 step:36020 [D loss: 0.531877, acc.: 74.22%] [G loss: 1.933884]\n",
      "epoch:38 step:36021 [D loss: 0.477432, acc.: 79.69%] [G loss: 1.522107]\n",
      "epoch:38 step:36022 [D loss: 0.903534, acc.: 47.66%] [G loss: 1.275066]\n",
      "epoch:38 step:36023 [D loss: 0.485629, acc.: 76.56%] [G loss: 2.066079]\n",
      "epoch:38 step:36024 [D loss: 0.685066, acc.: 61.72%] [G loss: 1.367763]\n",
      "epoch:38 step:36025 [D loss: 0.445273, acc.: 76.56%] [G loss: 1.435882]\n",
      "epoch:38 step:36026 [D loss: 0.493204, acc.: 72.66%] [G loss: 1.626652]\n",
      "epoch:38 step:36027 [D loss: 0.443386, acc.: 78.12%] [G loss: 1.686686]\n",
      "epoch:38 step:36028 [D loss: 0.630650, acc.: 64.06%] [G loss: 1.216187]\n",
      "epoch:38 step:36029 [D loss: 0.439229, acc.: 80.47%] [G loss: 1.397320]\n",
      "epoch:38 step:36030 [D loss: 0.434966, acc.: 79.69%] [G loss: 1.725871]\n",
      "epoch:38 step:36031 [D loss: 0.875877, acc.: 47.66%] [G loss: 0.969698]\n",
      "epoch:38 step:36032 [D loss: 0.563793, acc.: 70.31%] [G loss: 2.196692]\n",
      "epoch:38 step:36033 [D loss: 0.328634, acc.: 89.84%] [G loss: 1.519014]\n",
      "epoch:38 step:36034 [D loss: 0.616311, acc.: 65.62%] [G loss: 1.158613]\n",
      "epoch:38 step:36035 [D loss: 0.655199, acc.: 65.62%] [G loss: 1.581330]\n",
      "epoch:38 step:36036 [D loss: 0.331703, acc.: 89.84%] [G loss: 1.821707]\n",
      "epoch:38 step:36037 [D loss: 0.361740, acc.: 87.50%] [G loss: 2.008569]\n",
      "epoch:38 step:36038 [D loss: 0.547844, acc.: 73.44%] [G loss: 1.563411]\n",
      "epoch:38 step:36039 [D loss: 0.537493, acc.: 71.09%] [G loss: 1.862805]\n",
      "epoch:38 step:36040 [D loss: 0.564653, acc.: 72.66%] [G loss: 1.111810]\n",
      "epoch:38 step:36041 [D loss: 0.708211, acc.: 60.94%] [G loss: 1.422174]\n",
      "epoch:38 step:36042 [D loss: 0.377031, acc.: 82.81%] [G loss: 1.798762]\n",
      "epoch:38 step:36043 [D loss: 0.589877, acc.: 67.19%] [G loss: 1.549287]\n",
      "epoch:38 step:36044 [D loss: 0.628537, acc.: 65.62%] [G loss: 1.270104]\n",
      "epoch:38 step:36045 [D loss: 0.418145, acc.: 82.03%] [G loss: 1.465719]\n",
      "epoch:38 step:36046 [D loss: 0.507146, acc.: 75.78%] [G loss: 1.538090]\n",
      "epoch:38 step:36047 [D loss: 0.457923, acc.: 79.69%] [G loss: 1.412935]\n",
      "epoch:38 step:36048 [D loss: 0.360733, acc.: 87.50%] [G loss: 1.610223]\n",
      "epoch:38 step:36049 [D loss: 0.493768, acc.: 72.66%] [G loss: 1.218076]\n",
      "epoch:38 step:36050 [D loss: 0.605129, acc.: 67.97%] [G loss: 1.528972]\n",
      "epoch:38 step:36051 [D loss: 0.596044, acc.: 69.53%] [G loss: 1.486279]\n",
      "epoch:38 step:36052 [D loss: 0.505635, acc.: 75.78%] [G loss: 1.752786]\n",
      "epoch:38 step:36053 [D loss: 0.458642, acc.: 79.69%] [G loss: 1.552802]\n",
      "epoch:38 step:36054 [D loss: 0.604516, acc.: 65.62%] [G loss: 1.190289]\n",
      "epoch:38 step:36055 [D loss: 0.421308, acc.: 83.59%] [G loss: 1.939713]\n",
      "epoch:38 step:36056 [D loss: 0.496379, acc.: 75.00%] [G loss: 1.500856]\n",
      "epoch:38 step:36057 [D loss: 0.485016, acc.: 79.69%] [G loss: 1.402859]\n",
      "epoch:38 step:36058 [D loss: 0.579361, acc.: 64.84%] [G loss: 1.231115]\n",
      "epoch:38 step:36059 [D loss: 0.556127, acc.: 72.66%] [G loss: 1.509155]\n",
      "epoch:38 step:36060 [D loss: 0.460674, acc.: 81.25%] [G loss: 1.459252]\n",
      "epoch:38 step:36061 [D loss: 0.510631, acc.: 74.22%] [G loss: 2.086624]\n",
      "epoch:38 step:36062 [D loss: 0.874650, acc.: 53.12%] [G loss: 1.497602]\n",
      "epoch:38 step:36063 [D loss: 0.454617, acc.: 78.91%] [G loss: 1.600536]\n",
      "epoch:38 step:36064 [D loss: 0.484593, acc.: 80.47%] [G loss: 1.843351]\n",
      "epoch:38 step:36065 [D loss: 0.303517, acc.: 89.06%] [G loss: 1.592376]\n",
      "epoch:38 step:36066 [D loss: 0.541844, acc.: 72.66%] [G loss: 1.585716]\n",
      "epoch:38 step:36067 [D loss: 0.665812, acc.: 60.94%] [G loss: 1.485384]\n",
      "epoch:38 step:36068 [D loss: 0.780690, acc.: 50.00%] [G loss: 1.665389]\n",
      "epoch:38 step:36069 [D loss: 0.460103, acc.: 79.69%] [G loss: 1.196930]\n",
      "epoch:38 step:36070 [D loss: 0.473306, acc.: 76.56%] [G loss: 1.086650]\n",
      "epoch:38 step:36071 [D loss: 0.446356, acc.: 82.03%] [G loss: 1.355755]\n",
      "epoch:38 step:36072 [D loss: 0.407756, acc.: 84.38%] [G loss: 1.456086]\n",
      "epoch:38 step:36073 [D loss: 0.345870, acc.: 89.06%] [G loss: 1.262029]\n",
      "epoch:38 step:36074 [D loss: 0.600241, acc.: 66.41%] [G loss: 1.626022]\n",
      "epoch:38 step:36075 [D loss: 0.478977, acc.: 79.69%] [G loss: 2.045544]\n",
      "epoch:38 step:36076 [D loss: 0.447730, acc.: 78.91%] [G loss: 1.991632]\n",
      "epoch:38 step:36077 [D loss: 0.479931, acc.: 79.69%] [G loss: 1.312668]\n",
      "epoch:38 step:36078 [D loss: 0.362551, acc.: 89.06%] [G loss: 1.863882]\n",
      "epoch:38 step:36079 [D loss: 0.577850, acc.: 67.19%] [G loss: 1.655566]\n",
      "epoch:38 step:36080 [D loss: 0.595844, acc.: 67.97%] [G loss: 1.859429]\n",
      "epoch:38 step:36081 [D loss: 0.290188, acc.: 92.19%] [G loss: 1.860401]\n",
      "epoch:38 step:36082 [D loss: 0.628719, acc.: 64.84%] [G loss: 1.621583]\n",
      "epoch:38 step:36083 [D loss: 0.341295, acc.: 89.06%] [G loss: 2.037776]\n",
      "epoch:38 step:36084 [D loss: 0.383808, acc.: 85.16%] [G loss: 1.663700]\n",
      "epoch:38 step:36085 [D loss: 0.771589, acc.: 53.12%] [G loss: 0.967302]\n",
      "epoch:38 step:36086 [D loss: 0.568372, acc.: 69.53%] [G loss: 1.348016]\n",
      "epoch:38 step:36087 [D loss: 0.565348, acc.: 69.53%] [G loss: 1.483118]\n",
      "epoch:38 step:36088 [D loss: 0.598721, acc.: 64.84%] [G loss: 1.368407]\n",
      "epoch:38 step:36089 [D loss: 0.491179, acc.: 78.12%] [G loss: 1.491040]\n",
      "epoch:38 step:36090 [D loss: 0.725632, acc.: 59.38%] [G loss: 1.466480]\n",
      "epoch:38 step:36091 [D loss: 0.488170, acc.: 76.56%] [G loss: 1.737047]\n",
      "epoch:38 step:36092 [D loss: 0.416267, acc.: 85.16%] [G loss: 1.584853]\n",
      "epoch:38 step:36093 [D loss: 0.341347, acc.: 90.62%] [G loss: 1.732303]\n",
      "epoch:38 step:36094 [D loss: 0.405450, acc.: 82.03%] [G loss: 2.051757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36095 [D loss: 0.714811, acc.: 56.25%] [G loss: 1.954851]\n",
      "epoch:38 step:36096 [D loss: 0.673536, acc.: 61.72%] [G loss: 1.924623]\n",
      "epoch:38 step:36097 [D loss: 0.470040, acc.: 75.00%] [G loss: 1.401649]\n",
      "epoch:38 step:36098 [D loss: 0.750662, acc.: 50.78%] [G loss: 1.755847]\n",
      "epoch:38 step:36099 [D loss: 0.485095, acc.: 77.34%] [G loss: 1.326607]\n",
      "epoch:38 step:36100 [D loss: 0.697265, acc.: 62.50%] [G loss: 1.712312]\n",
      "epoch:38 step:36101 [D loss: 0.565716, acc.: 70.31%] [G loss: 1.764509]\n",
      "epoch:38 step:36102 [D loss: 0.663764, acc.: 64.06%] [G loss: 1.235412]\n",
      "epoch:38 step:36103 [D loss: 0.409373, acc.: 86.72%] [G loss: 1.121833]\n",
      "epoch:38 step:36104 [D loss: 0.565950, acc.: 68.75%] [G loss: 1.556943]\n",
      "epoch:38 step:36105 [D loss: 0.311111, acc.: 89.84%] [G loss: 1.453081]\n",
      "epoch:38 step:36106 [D loss: 0.599959, acc.: 70.31%] [G loss: 1.524196]\n",
      "epoch:38 step:36107 [D loss: 0.413264, acc.: 80.47%] [G loss: 1.645018]\n",
      "epoch:38 step:36108 [D loss: 0.460589, acc.: 81.25%] [G loss: 1.679138]\n",
      "epoch:38 step:36109 [D loss: 0.325742, acc.: 92.97%] [G loss: 1.651295]\n",
      "epoch:38 step:36110 [D loss: 0.556575, acc.: 70.31%] [G loss: 1.601737]\n",
      "epoch:38 step:36111 [D loss: 0.439650, acc.: 78.12%] [G loss: 1.754209]\n",
      "epoch:38 step:36112 [D loss: 0.352884, acc.: 88.28%] [G loss: 1.734340]\n",
      "epoch:38 step:36113 [D loss: 0.593222, acc.: 70.31%] [G loss: 1.316252]\n",
      "epoch:38 step:36114 [D loss: 0.669570, acc.: 60.94%] [G loss: 1.579984]\n",
      "epoch:38 step:36115 [D loss: 0.414262, acc.: 83.59%] [G loss: 1.578115]\n",
      "epoch:38 step:36116 [D loss: 0.438970, acc.: 78.12%] [G loss: 1.596434]\n",
      "epoch:38 step:36117 [D loss: 0.342636, acc.: 90.62%] [G loss: 1.836040]\n",
      "epoch:38 step:36118 [D loss: 0.570554, acc.: 64.84%] [G loss: 1.639700]\n",
      "epoch:38 step:36119 [D loss: 0.348773, acc.: 85.94%] [G loss: 1.551875]\n",
      "epoch:38 step:36120 [D loss: 0.362439, acc.: 84.38%] [G loss: 1.525160]\n",
      "epoch:38 step:36121 [D loss: 0.618614, acc.: 67.19%] [G loss: 1.414433]\n",
      "epoch:38 step:36122 [D loss: 0.372210, acc.: 84.38%] [G loss: 1.567952]\n",
      "epoch:38 step:36123 [D loss: 0.593802, acc.: 67.97%] [G loss: 1.468801]\n",
      "epoch:38 step:36124 [D loss: 0.443588, acc.: 79.69%] [G loss: 1.585884]\n",
      "epoch:38 step:36125 [D loss: 0.465259, acc.: 81.25%] [G loss: 1.933245]\n",
      "epoch:38 step:36126 [D loss: 0.579069, acc.: 71.09%] [G loss: 1.498129]\n",
      "epoch:38 step:36127 [D loss: 0.432903, acc.: 80.47%] [G loss: 1.472003]\n",
      "epoch:38 step:36128 [D loss: 0.704662, acc.: 59.38%] [G loss: 1.368527]\n",
      "epoch:38 step:36129 [D loss: 0.345059, acc.: 87.50%] [G loss: 1.344532]\n",
      "epoch:38 step:36130 [D loss: 0.467935, acc.: 80.47%] [G loss: 1.365446]\n",
      "epoch:38 step:36131 [D loss: 0.440889, acc.: 78.91%] [G loss: 1.661849]\n",
      "epoch:38 step:36132 [D loss: 0.595818, acc.: 68.75%] [G loss: 1.205616]\n",
      "epoch:38 step:36133 [D loss: 0.739519, acc.: 57.81%] [G loss: 1.723594]\n",
      "epoch:38 step:36134 [D loss: 0.378657, acc.: 89.84%] [G loss: 1.776937]\n",
      "epoch:38 step:36135 [D loss: 0.458871, acc.: 77.34%] [G loss: 1.756308]\n",
      "epoch:38 step:36136 [D loss: 0.797621, acc.: 49.22%] [G loss: 1.022024]\n",
      "epoch:38 step:36137 [D loss: 0.351724, acc.: 85.94%] [G loss: 1.285753]\n",
      "epoch:38 step:36138 [D loss: 0.725103, acc.: 56.25%] [G loss: 1.427385]\n",
      "epoch:38 step:36139 [D loss: 0.649401, acc.: 68.75%] [G loss: 1.273355]\n",
      "epoch:38 step:36140 [D loss: 0.432823, acc.: 81.25%] [G loss: 1.349076]\n",
      "epoch:38 step:36141 [D loss: 0.523744, acc.: 74.22%] [G loss: 1.307878]\n",
      "epoch:38 step:36142 [D loss: 0.893940, acc.: 47.66%] [G loss: 1.037241]\n",
      "epoch:38 step:36143 [D loss: 0.607943, acc.: 63.28%] [G loss: 1.420604]\n",
      "epoch:38 step:36144 [D loss: 0.602219, acc.: 69.53%] [G loss: 1.791654]\n",
      "epoch:38 step:36145 [D loss: 0.623109, acc.: 63.28%] [G loss: 1.984241]\n",
      "epoch:38 step:36146 [D loss: 0.459293, acc.: 78.12%] [G loss: 2.157653]\n",
      "epoch:38 step:36147 [D loss: 0.717985, acc.: 55.47%] [G loss: 1.888292]\n",
      "epoch:38 step:36148 [D loss: 0.775196, acc.: 53.12%] [G loss: 1.420115]\n",
      "epoch:38 step:36149 [D loss: 0.281075, acc.: 91.41%] [G loss: 1.781730]\n",
      "epoch:38 step:36150 [D loss: 0.364606, acc.: 84.38%] [G loss: 2.163276]\n",
      "epoch:38 step:36151 [D loss: 0.520808, acc.: 74.22%] [G loss: 1.887713]\n",
      "epoch:38 step:36152 [D loss: 0.370759, acc.: 86.72%] [G loss: 1.507178]\n",
      "epoch:38 step:36153 [D loss: 0.654087, acc.: 60.94%] [G loss: 1.114319]\n",
      "epoch:38 step:36154 [D loss: 0.636841, acc.: 62.50%] [G loss: 1.677800]\n",
      "epoch:38 step:36155 [D loss: 0.467789, acc.: 75.78%] [G loss: 1.713105]\n",
      "epoch:38 step:36156 [D loss: 0.336330, acc.: 84.38%] [G loss: 1.805175]\n",
      "epoch:38 step:36157 [D loss: 0.668460, acc.: 57.81%] [G loss: 1.335773]\n",
      "epoch:38 step:36158 [D loss: 0.509321, acc.: 72.66%] [G loss: 1.297807]\n",
      "epoch:38 step:36159 [D loss: 0.371378, acc.: 85.16%] [G loss: 1.800765]\n",
      "epoch:38 step:36160 [D loss: 0.520132, acc.: 74.22%] [G loss: 1.863467]\n",
      "epoch:38 step:36161 [D loss: 0.459176, acc.: 78.12%] [G loss: 1.306759]\n",
      "epoch:38 step:36162 [D loss: 0.432041, acc.: 82.81%] [G loss: 1.667995]\n",
      "epoch:38 step:36163 [D loss: 0.527265, acc.: 71.88%] [G loss: 1.365949]\n",
      "epoch:38 step:36164 [D loss: 0.592581, acc.: 67.97%] [G loss: 1.272258]\n",
      "epoch:38 step:36165 [D loss: 0.652639, acc.: 64.06%] [G loss: 1.034680]\n",
      "epoch:38 step:36166 [D loss: 0.633782, acc.: 61.72%] [G loss: 1.894848]\n",
      "epoch:38 step:36167 [D loss: 0.672775, acc.: 61.72%] [G loss: 1.226603]\n",
      "epoch:38 step:36168 [D loss: 0.488871, acc.: 75.00%] [G loss: 2.036290]\n",
      "epoch:38 step:36169 [D loss: 0.365745, acc.: 85.94%] [G loss: 2.189837]\n",
      "epoch:38 step:36170 [D loss: 0.502421, acc.: 72.66%] [G loss: 1.665908]\n",
      "epoch:38 step:36171 [D loss: 0.430253, acc.: 81.25%] [G loss: 2.083156]\n",
      "epoch:38 step:36172 [D loss: 0.340622, acc.: 89.06%] [G loss: 1.570596]\n",
      "epoch:38 step:36173 [D loss: 0.371660, acc.: 85.16%] [G loss: 1.560676]\n",
      "epoch:38 step:36174 [D loss: 0.645301, acc.: 67.19%] [G loss: 1.263139]\n",
      "epoch:38 step:36175 [D loss: 0.494056, acc.: 77.34%] [G loss: 1.857660]\n",
      "epoch:38 step:36176 [D loss: 0.620473, acc.: 67.19%] [G loss: 1.488346]\n",
      "epoch:38 step:36177 [D loss: 0.367277, acc.: 86.72%] [G loss: 1.521336]\n",
      "epoch:38 step:36178 [D loss: 0.313763, acc.: 91.41%] [G loss: 1.840252]\n",
      "epoch:38 step:36179 [D loss: 0.674140, acc.: 60.94%] [G loss: 1.535537]\n",
      "epoch:38 step:36180 [D loss: 0.761069, acc.: 54.69%] [G loss: 1.334658]\n",
      "epoch:38 step:36181 [D loss: 0.399323, acc.: 82.03%] [G loss: 1.465631]\n",
      "epoch:38 step:36182 [D loss: 0.470839, acc.: 77.34%] [G loss: 1.707198]\n",
      "epoch:38 step:36183 [D loss: 0.508355, acc.: 73.44%] [G loss: 1.295269]\n",
      "epoch:38 step:36184 [D loss: 0.507333, acc.: 76.56%] [G loss: 1.758025]\n",
      "epoch:38 step:36185 [D loss: 0.306954, acc.: 89.84%] [G loss: 1.229531]\n",
      "epoch:38 step:36186 [D loss: 0.486957, acc.: 72.66%] [G loss: 1.119816]\n",
      "epoch:38 step:36187 [D loss: 0.674044, acc.: 62.50%] [G loss: 1.154287]\n",
      "epoch:38 step:36188 [D loss: 0.555464, acc.: 74.22%] [G loss: 1.709291]\n",
      "epoch:38 step:36189 [D loss: 0.769972, acc.: 59.38%] [G loss: 1.424150]\n",
      "epoch:38 step:36190 [D loss: 0.544295, acc.: 70.31%] [G loss: 1.596774]\n",
      "epoch:38 step:36191 [D loss: 0.399722, acc.: 85.16%] [G loss: 2.236408]\n",
      "epoch:38 step:36192 [D loss: 0.361845, acc.: 89.84%] [G loss: 2.260782]\n",
      "epoch:38 step:36193 [D loss: 0.490715, acc.: 75.78%] [G loss: 1.520765]\n",
      "epoch:38 step:36194 [D loss: 0.304981, acc.: 92.19%] [G loss: 1.426115]\n",
      "epoch:38 step:36195 [D loss: 0.660716, acc.: 62.50%] [G loss: 1.578188]\n",
      "epoch:38 step:36196 [D loss: 0.445957, acc.: 83.59%] [G loss: 1.398708]\n",
      "epoch:38 step:36197 [D loss: 0.595680, acc.: 66.41%] [G loss: 1.329431]\n",
      "epoch:38 step:36198 [D loss: 0.415970, acc.: 82.03%] [G loss: 1.701803]\n",
      "epoch:38 step:36199 [D loss: 0.635482, acc.: 67.97%] [G loss: 1.509436]\n",
      "epoch:38 step:36200 [D loss: 0.419472, acc.: 82.81%] [G loss: 1.577015]\n",
      "##############\n",
      "[2.70479764 2.06351164 2.14499422 3.52508976 0.7687888  6.33471155\n",
      " 2.27741913 2.75982156 4.02033663 6.38029678]\n",
      "##########\n",
      "epoch:38 step:36201 [D loss: 0.588402, acc.: 64.84%] [G loss: 1.868419]\n",
      "epoch:38 step:36202 [D loss: 0.449316, acc.: 80.47%] [G loss: 1.400810]\n",
      "epoch:38 step:36203 [D loss: 0.591278, acc.: 67.97%] [G loss: 1.632197]\n",
      "epoch:38 step:36204 [D loss: 0.472387, acc.: 78.12%] [G loss: 1.383202]\n",
      "epoch:38 step:36205 [D loss: 0.480733, acc.: 80.47%] [G loss: 2.339868]\n",
      "epoch:38 step:36206 [D loss: 0.513819, acc.: 75.78%] [G loss: 1.278153]\n",
      "epoch:38 step:36207 [D loss: 0.593269, acc.: 68.75%] [G loss: 2.257910]\n",
      "epoch:38 step:36208 [D loss: 0.314685, acc.: 92.19%] [G loss: 1.641524]\n",
      "epoch:38 step:36209 [D loss: 0.583644, acc.: 67.97%] [G loss: 1.740218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36210 [D loss: 0.645719, acc.: 66.41%] [G loss: 1.235363]\n",
      "epoch:38 step:36211 [D loss: 0.454453, acc.: 77.34%] [G loss: 1.576966]\n",
      "epoch:38 step:36212 [D loss: 0.649798, acc.: 66.41%] [G loss: 1.366173]\n",
      "epoch:38 step:36213 [D loss: 0.480908, acc.: 78.12%] [G loss: 1.723445]\n",
      "epoch:38 step:36214 [D loss: 0.730389, acc.: 59.38%] [G loss: 1.713331]\n",
      "epoch:38 step:36215 [D loss: 0.618055, acc.: 65.62%] [G loss: 1.925661]\n",
      "epoch:38 step:36216 [D loss: 0.488799, acc.: 80.47%] [G loss: 1.616112]\n",
      "epoch:38 step:36217 [D loss: 0.428847, acc.: 82.03%] [G loss: 1.512574]\n",
      "epoch:38 step:36218 [D loss: 0.675855, acc.: 60.94%] [G loss: 1.267372]\n",
      "epoch:38 step:36219 [D loss: 0.482520, acc.: 74.22%] [G loss: 1.484559]\n",
      "epoch:38 step:36220 [D loss: 0.490710, acc.: 73.44%] [G loss: 1.379423]\n",
      "epoch:38 step:36221 [D loss: 0.481919, acc.: 78.91%] [G loss: 1.277889]\n",
      "epoch:38 step:36222 [D loss: 0.385135, acc.: 83.59%] [G loss: 1.469167]\n",
      "epoch:38 step:36223 [D loss: 0.533193, acc.: 70.31%] [G loss: 1.473648]\n",
      "epoch:38 step:36224 [D loss: 0.389959, acc.: 86.72%] [G loss: 1.812038]\n",
      "epoch:38 step:36225 [D loss: 0.363334, acc.: 85.16%] [G loss: 2.194445]\n",
      "epoch:38 step:36226 [D loss: 0.598036, acc.: 65.62%] [G loss: 1.586273]\n",
      "epoch:38 step:36227 [D loss: 0.486030, acc.: 78.91%] [G loss: 1.636238]\n",
      "epoch:38 step:36228 [D loss: 0.567833, acc.: 68.75%] [G loss: 1.298942]\n",
      "epoch:38 step:36229 [D loss: 0.396212, acc.: 84.38%] [G loss: 1.154943]\n",
      "epoch:38 step:36230 [D loss: 0.493409, acc.: 70.31%] [G loss: 1.663470]\n",
      "epoch:38 step:36231 [D loss: 0.492823, acc.: 76.56%] [G loss: 1.646688]\n",
      "epoch:38 step:36232 [D loss: 0.494603, acc.: 76.56%] [G loss: 1.719744]\n",
      "epoch:38 step:36233 [D loss: 0.517426, acc.: 75.78%] [G loss: 1.759687]\n",
      "epoch:38 step:36234 [D loss: 0.481813, acc.: 76.56%] [G loss: 1.376915]\n",
      "epoch:38 step:36235 [D loss: 0.516430, acc.: 74.22%] [G loss: 2.047806]\n",
      "epoch:38 step:36236 [D loss: 0.560164, acc.: 75.78%] [G loss: 1.199872]\n",
      "epoch:38 step:36237 [D loss: 0.412167, acc.: 85.94%] [G loss: 1.112793]\n",
      "epoch:38 step:36238 [D loss: 0.211702, acc.: 96.09%] [G loss: 1.935618]\n",
      "epoch:38 step:36239 [D loss: 0.609379, acc.: 66.41%] [G loss: 1.542223]\n",
      "epoch:38 step:36240 [D loss: 0.389600, acc.: 85.16%] [G loss: 1.665170]\n",
      "epoch:38 step:36241 [D loss: 0.394585, acc.: 85.16%] [G loss: 1.524299]\n",
      "epoch:38 step:36242 [D loss: 0.487513, acc.: 78.12%] [G loss: 1.643814]\n",
      "epoch:38 step:36243 [D loss: 0.559634, acc.: 72.66%] [G loss: 1.764855]\n",
      "epoch:38 step:36244 [D loss: 0.821670, acc.: 51.56%] [G loss: 1.519262]\n",
      "epoch:38 step:36245 [D loss: 0.554202, acc.: 70.31%] [G loss: 2.213835]\n",
      "epoch:38 step:36246 [D loss: 0.444240, acc.: 82.03%] [G loss: 1.437769]\n",
      "epoch:38 step:36247 [D loss: 0.603651, acc.: 70.31%] [G loss: 1.645243]\n",
      "epoch:38 step:36248 [D loss: 0.474531, acc.: 78.12%] [G loss: 1.563038]\n",
      "epoch:38 step:36249 [D loss: 0.630301, acc.: 65.62%] [G loss: 1.359514]\n",
      "epoch:38 step:36250 [D loss: 0.448275, acc.: 78.12%] [G loss: 1.404014]\n",
      "epoch:38 step:36251 [D loss: 0.388366, acc.: 86.72%] [G loss: 1.611530]\n",
      "epoch:38 step:36252 [D loss: 0.769203, acc.: 50.00%] [G loss: 1.195521]\n",
      "epoch:38 step:36253 [D loss: 0.484710, acc.: 81.25%] [G loss: 1.350568]\n",
      "epoch:38 step:36254 [D loss: 0.436622, acc.: 78.91%] [G loss: 1.917553]\n",
      "epoch:38 step:36255 [D loss: 0.342802, acc.: 88.28%] [G loss: 1.223518]\n",
      "epoch:38 step:36256 [D loss: 0.589676, acc.: 70.31%] [G loss: 1.810436]\n",
      "epoch:38 step:36257 [D loss: 0.385824, acc.: 83.59%] [G loss: 1.557113]\n",
      "epoch:38 step:36258 [D loss: 0.473394, acc.: 76.56%] [G loss: 1.618117]\n",
      "epoch:38 step:36259 [D loss: 0.774991, acc.: 57.03%] [G loss: 1.006326]\n",
      "epoch:38 step:36260 [D loss: 0.480561, acc.: 81.25%] [G loss: 1.835940]\n",
      "epoch:38 step:36261 [D loss: 0.463900, acc.: 78.91%] [G loss: 1.764564]\n",
      "epoch:38 step:36262 [D loss: 0.433612, acc.: 84.38%] [G loss: 1.460018]\n",
      "epoch:38 step:36263 [D loss: 0.414300, acc.: 82.81%] [G loss: 1.680968]\n",
      "epoch:38 step:36264 [D loss: 0.618277, acc.: 67.97%] [G loss: 1.110499]\n",
      "epoch:38 step:36265 [D loss: 0.501097, acc.: 77.34%] [G loss: 1.346958]\n",
      "epoch:38 step:36266 [D loss: 0.630882, acc.: 67.19%] [G loss: 1.307669]\n",
      "epoch:38 step:36267 [D loss: 0.386777, acc.: 82.81%] [G loss: 1.227083]\n",
      "epoch:38 step:36268 [D loss: 0.552620, acc.: 72.66%] [G loss: 1.963956]\n",
      "epoch:38 step:36269 [D loss: 0.494576, acc.: 78.12%] [G loss: 1.920755]\n",
      "epoch:38 step:36270 [D loss: 0.901776, acc.: 40.62%] [G loss: 1.159139]\n",
      "epoch:38 step:36271 [D loss: 0.591620, acc.: 71.09%] [G loss: 2.420848]\n",
      "epoch:38 step:36272 [D loss: 0.420475, acc.: 80.47%] [G loss: 1.483529]\n",
      "epoch:38 step:36273 [D loss: 0.607036, acc.: 69.53%] [G loss: 1.702189]\n",
      "epoch:38 step:36274 [D loss: 0.458977, acc.: 77.34%] [G loss: 2.349353]\n",
      "epoch:38 step:36275 [D loss: 0.301460, acc.: 92.97%] [G loss: 1.956848]\n",
      "epoch:38 step:36276 [D loss: 0.496348, acc.: 76.56%] [G loss: 1.872798]\n",
      "epoch:38 step:36277 [D loss: 0.453231, acc.: 78.12%] [G loss: 1.622152]\n",
      "epoch:38 step:36278 [D loss: 0.603841, acc.: 69.53%] [G loss: 1.935950]\n",
      "epoch:38 step:36279 [D loss: 0.641699, acc.: 66.41%] [G loss: 1.866909]\n",
      "epoch:38 step:36280 [D loss: 0.571783, acc.: 64.84%] [G loss: 1.360401]\n",
      "epoch:38 step:36281 [D loss: 0.533202, acc.: 74.22%] [G loss: 1.677485]\n",
      "epoch:38 step:36282 [D loss: 0.363320, acc.: 88.28%] [G loss: 1.451796]\n",
      "epoch:38 step:36283 [D loss: 0.421889, acc.: 81.25%] [G loss: 1.911047]\n",
      "epoch:38 step:36284 [D loss: 0.561546, acc.: 74.22%] [G loss: 1.126846]\n",
      "epoch:38 step:36285 [D loss: 0.451836, acc.: 82.81%] [G loss: 1.029853]\n",
      "epoch:38 step:36286 [D loss: 0.623375, acc.: 65.62%] [G loss: 1.600515]\n",
      "epoch:38 step:36287 [D loss: 0.568406, acc.: 71.88%] [G loss: 1.785420]\n",
      "epoch:38 step:36288 [D loss: 0.413006, acc.: 80.47%] [G loss: 1.246639]\n",
      "epoch:38 step:36289 [D loss: 0.795149, acc.: 54.69%] [G loss: 0.901631]\n",
      "epoch:38 step:36290 [D loss: 0.556282, acc.: 73.44%] [G loss: 1.714797]\n",
      "epoch:38 step:36291 [D loss: 0.409895, acc.: 82.03%] [G loss: 1.618976]\n",
      "epoch:38 step:36292 [D loss: 0.362887, acc.: 87.50%] [G loss: 1.525820]\n",
      "epoch:38 step:36293 [D loss: 0.608318, acc.: 69.53%] [G loss: 1.966867]\n",
      "epoch:38 step:36294 [D loss: 0.561255, acc.: 67.97%] [G loss: 1.477510]\n",
      "epoch:38 step:36295 [D loss: 0.892941, acc.: 47.66%] [G loss: 0.964408]\n",
      "epoch:38 step:36296 [D loss: 0.420961, acc.: 81.25%] [G loss: 1.916274]\n",
      "epoch:38 step:36297 [D loss: 0.516497, acc.: 76.56%] [G loss: 1.707221]\n",
      "epoch:38 step:36298 [D loss: 0.579736, acc.: 68.75%] [G loss: 1.053915]\n",
      "epoch:38 step:36299 [D loss: 0.475950, acc.: 78.91%] [G loss: 1.276135]\n",
      "epoch:38 step:36300 [D loss: 0.504734, acc.: 78.12%] [G loss: 1.676892]\n",
      "epoch:38 step:36301 [D loss: 0.372626, acc.: 88.28%] [G loss: 1.504843]\n",
      "epoch:38 step:36302 [D loss: 0.477924, acc.: 74.22%] [G loss: 1.096821]\n",
      "epoch:38 step:36303 [D loss: 0.431709, acc.: 82.81%] [G loss: 1.705331]\n",
      "epoch:38 step:36304 [D loss: 0.682972, acc.: 59.38%] [G loss: 1.222990]\n",
      "epoch:38 step:36305 [D loss: 0.364943, acc.: 85.94%] [G loss: 2.015193]\n",
      "epoch:38 step:36306 [D loss: 0.434867, acc.: 81.25%] [G loss: 1.144413]\n",
      "epoch:38 step:36307 [D loss: 0.345412, acc.: 90.62%] [G loss: 1.669838]\n",
      "epoch:38 step:36308 [D loss: 0.551331, acc.: 71.88%] [G loss: 1.819925]\n",
      "epoch:38 step:36309 [D loss: 0.353204, acc.: 85.16%] [G loss: 1.829267]\n",
      "epoch:38 step:36310 [D loss: 0.776378, acc.: 57.81%] [G loss: 1.382393]\n",
      "epoch:38 step:36311 [D loss: 0.510358, acc.: 72.66%] [G loss: 1.649549]\n",
      "epoch:38 step:36312 [D loss: 0.660093, acc.: 59.38%] [G loss: 1.171467]\n",
      "epoch:38 step:36313 [D loss: 0.365618, acc.: 85.16%] [G loss: 2.196768]\n",
      "epoch:38 step:36314 [D loss: 0.513955, acc.: 80.47%] [G loss: 1.525791]\n",
      "epoch:38 step:36315 [D loss: 0.592925, acc.: 67.97%] [G loss: 1.324650]\n",
      "epoch:38 step:36316 [D loss: 0.640277, acc.: 64.84%] [G loss: 1.338456]\n",
      "epoch:38 step:36317 [D loss: 0.417231, acc.: 82.03%] [G loss: 1.953366]\n",
      "epoch:38 step:36318 [D loss: 0.853722, acc.: 48.44%] [G loss: 1.289160]\n",
      "epoch:38 step:36319 [D loss: 0.526364, acc.: 73.44%] [G loss: 1.687245]\n",
      "epoch:38 step:36320 [D loss: 0.519716, acc.: 68.75%] [G loss: 1.531525]\n",
      "epoch:38 step:36321 [D loss: 0.380468, acc.: 84.38%] [G loss: 1.642733]\n",
      "epoch:38 step:36322 [D loss: 0.398280, acc.: 84.38%] [G loss: 1.622370]\n",
      "epoch:38 step:36323 [D loss: 0.492130, acc.: 76.56%] [G loss: 1.745230]\n",
      "epoch:38 step:36324 [D loss: 0.429083, acc.: 79.69%] [G loss: 1.555536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36325 [D loss: 0.675074, acc.: 62.50%] [G loss: 1.161744]\n",
      "epoch:38 step:36326 [D loss: 0.485127, acc.: 78.12%] [G loss: 1.778810]\n",
      "epoch:38 step:36327 [D loss: 0.447520, acc.: 82.03%] [G loss: 1.575345]\n",
      "epoch:38 step:36328 [D loss: 0.477641, acc.: 74.22%] [G loss: 1.336597]\n",
      "epoch:38 step:36329 [D loss: 0.547640, acc.: 67.97%] [G loss: 1.462811]\n",
      "epoch:38 step:36330 [D loss: 0.415690, acc.: 82.03%] [G loss: 1.668365]\n",
      "epoch:38 step:36331 [D loss: 0.484937, acc.: 72.66%] [G loss: 1.856565]\n",
      "epoch:38 step:36332 [D loss: 0.660047, acc.: 61.72%] [G loss: 1.136285]\n",
      "epoch:38 step:36333 [D loss: 0.552459, acc.: 70.31%] [G loss: 1.136487]\n",
      "epoch:38 step:36334 [D loss: 0.627686, acc.: 67.97%] [G loss: 1.987088]\n",
      "epoch:38 step:36335 [D loss: 0.438789, acc.: 82.03%] [G loss: 1.838118]\n",
      "epoch:38 step:36336 [D loss: 0.369479, acc.: 86.72%] [G loss: 2.040763]\n",
      "epoch:38 step:36337 [D loss: 0.643333, acc.: 68.75%] [G loss: 0.933826]\n",
      "epoch:38 step:36338 [D loss: 0.894227, acc.: 48.44%] [G loss: 1.122574]\n",
      "epoch:38 step:36339 [D loss: 0.373760, acc.: 87.50%] [G loss: 2.372760]\n",
      "epoch:38 step:36340 [D loss: 0.239159, acc.: 92.97%] [G loss: 2.089435]\n",
      "epoch:38 step:36341 [D loss: 0.444525, acc.: 78.91%] [G loss: 1.582966]\n",
      "epoch:38 step:36342 [D loss: 0.484354, acc.: 73.44%] [G loss: 1.921238]\n",
      "epoch:38 step:36343 [D loss: 0.606069, acc.: 64.84%] [G loss: 1.966157]\n",
      "epoch:38 step:36344 [D loss: 0.570344, acc.: 70.31%] [G loss: 1.817812]\n",
      "epoch:38 step:36345 [D loss: 0.774012, acc.: 54.69%] [G loss: 1.345827]\n",
      "epoch:38 step:36346 [D loss: 0.486975, acc.: 78.12%] [G loss: 1.730717]\n",
      "epoch:38 step:36347 [D loss: 0.431053, acc.: 81.25%] [G loss: 1.506838]\n",
      "epoch:38 step:36348 [D loss: 0.460167, acc.: 78.12%] [G loss: 2.003552]\n",
      "epoch:38 step:36349 [D loss: 0.476429, acc.: 78.91%] [G loss: 1.684293]\n",
      "epoch:38 step:36350 [D loss: 0.428950, acc.: 82.81%] [G loss: 1.322515]\n",
      "epoch:38 step:36351 [D loss: 0.564912, acc.: 71.88%] [G loss: 0.850057]\n",
      "epoch:38 step:36352 [D loss: 0.618242, acc.: 65.62%] [G loss: 1.653271]\n",
      "epoch:38 step:36353 [D loss: 0.461935, acc.: 81.25%] [G loss: 1.734048]\n",
      "epoch:38 step:36354 [D loss: 0.518626, acc.: 75.78%] [G loss: 1.353559]\n",
      "epoch:38 step:36355 [D loss: 0.565848, acc.: 73.44%] [G loss: 1.874879]\n",
      "epoch:38 step:36356 [D loss: 0.446630, acc.: 79.69%] [G loss: 2.018911]\n",
      "epoch:38 step:36357 [D loss: 0.644791, acc.: 64.84%] [G loss: 1.436789]\n",
      "epoch:38 step:36358 [D loss: 0.656576, acc.: 58.59%] [G loss: 1.352815]\n",
      "epoch:38 step:36359 [D loss: 0.354328, acc.: 89.84%] [G loss: 1.706716]\n",
      "epoch:38 step:36360 [D loss: 0.607796, acc.: 68.75%] [G loss: 1.434357]\n",
      "epoch:38 step:36361 [D loss: 0.650354, acc.: 67.19%] [G loss: 1.848581]\n",
      "epoch:38 step:36362 [D loss: 0.565788, acc.: 66.41%] [G loss: 1.732957]\n",
      "epoch:38 step:36363 [D loss: 0.421248, acc.: 79.69%] [G loss: 1.747175]\n",
      "epoch:38 step:36364 [D loss: 0.462928, acc.: 79.69%] [G loss: 1.930309]\n",
      "epoch:38 step:36365 [D loss: 0.490723, acc.: 76.56%] [G loss: 1.568342]\n",
      "epoch:38 step:36366 [D loss: 0.378224, acc.: 80.47%] [G loss: 1.459001]\n",
      "epoch:38 step:36367 [D loss: 0.438066, acc.: 81.25%] [G loss: 1.406571]\n",
      "epoch:38 step:36368 [D loss: 0.494596, acc.: 78.12%] [G loss: 1.322237]\n",
      "epoch:38 step:36369 [D loss: 0.528282, acc.: 76.56%] [G loss: 1.694195]\n",
      "epoch:38 step:36370 [D loss: 0.519641, acc.: 72.66%] [G loss: 1.422077]\n",
      "epoch:38 step:36371 [D loss: 0.451185, acc.: 79.69%] [G loss: 2.243356]\n",
      "epoch:38 step:36372 [D loss: 0.667862, acc.: 57.03%] [G loss: 1.691206]\n",
      "epoch:38 step:36373 [D loss: 0.386765, acc.: 88.28%] [G loss: 1.956956]\n",
      "epoch:38 step:36374 [D loss: 0.542423, acc.: 73.44%] [G loss: 2.056363]\n",
      "epoch:38 step:36375 [D loss: 0.438343, acc.: 83.59%] [G loss: 1.148921]\n",
      "epoch:38 step:36376 [D loss: 0.650548, acc.: 64.06%] [G loss: 1.262317]\n",
      "epoch:38 step:36377 [D loss: 0.489104, acc.: 75.78%] [G loss: 1.781927]\n",
      "epoch:38 step:36378 [D loss: 0.443396, acc.: 81.25%] [G loss: 1.551073]\n",
      "epoch:38 step:36379 [D loss: 0.607161, acc.: 65.62%] [G loss: 1.602512]\n",
      "epoch:38 step:36380 [D loss: 0.647971, acc.: 64.06%] [G loss: 1.559248]\n",
      "epoch:38 step:36381 [D loss: 0.630248, acc.: 67.19%] [G loss: 1.217195]\n",
      "epoch:38 step:36382 [D loss: 0.493219, acc.: 76.56%] [G loss: 1.972456]\n",
      "epoch:38 step:36383 [D loss: 0.637472, acc.: 64.06%] [G loss: 1.987085]\n",
      "epoch:38 step:36384 [D loss: 0.385825, acc.: 84.38%] [G loss: 1.811925]\n",
      "epoch:38 step:36385 [D loss: 0.533452, acc.: 75.78%] [G loss: 1.580518]\n",
      "epoch:38 step:36386 [D loss: 0.488553, acc.: 79.69%] [G loss: 1.251626]\n",
      "epoch:38 step:36387 [D loss: 0.345192, acc.: 87.50%] [G loss: 2.042256]\n",
      "epoch:38 step:36388 [D loss: 0.544037, acc.: 73.44%] [G loss: 1.419466]\n",
      "epoch:38 step:36389 [D loss: 0.462710, acc.: 76.56%] [G loss: 1.644922]\n",
      "epoch:38 step:36390 [D loss: 0.545691, acc.: 73.44%] [G loss: 1.788758]\n",
      "epoch:38 step:36391 [D loss: 0.698997, acc.: 57.03%] [G loss: 1.502754]\n",
      "epoch:38 step:36392 [D loss: 0.443836, acc.: 82.03%] [G loss: 1.681460]\n",
      "epoch:38 step:36393 [D loss: 0.287358, acc.: 89.84%] [G loss: 1.940279]\n",
      "epoch:38 step:36394 [D loss: 0.554633, acc.: 77.34%] [G loss: 1.532074]\n",
      "epoch:38 step:36395 [D loss: 0.690967, acc.: 60.16%] [G loss: 1.043219]\n",
      "epoch:38 step:36396 [D loss: 0.594455, acc.: 67.97%] [G loss: 1.406888]\n",
      "epoch:38 step:36397 [D loss: 0.509964, acc.: 68.75%] [G loss: 1.296908]\n",
      "epoch:38 step:36398 [D loss: 0.594746, acc.: 71.88%] [G loss: 1.052359]\n",
      "epoch:38 step:36399 [D loss: 0.662597, acc.: 64.06%] [G loss: 1.299095]\n",
      "epoch:38 step:36400 [D loss: 0.451696, acc.: 78.91%] [G loss: 2.073314]\n",
      "##############\n",
      "[2.63726533 1.96884837 1.86055088 2.95285953 0.89583906 6.19083402\n",
      " 2.17882803 2.52073885 3.8649377  8.14868929]\n",
      "##########\n",
      "epoch:38 step:36401 [D loss: 0.579599, acc.: 73.44%] [G loss: 1.709589]\n",
      "epoch:38 step:36402 [D loss: 0.463720, acc.: 75.00%] [G loss: 1.395329]\n",
      "epoch:38 step:36403 [D loss: 0.434218, acc.: 77.34%] [G loss: 1.561705]\n",
      "epoch:38 step:36404 [D loss: 0.639959, acc.: 64.84%] [G loss: 1.765683]\n",
      "epoch:38 step:36405 [D loss: 0.436882, acc.: 74.22%] [G loss: 1.321225]\n",
      "epoch:38 step:36406 [D loss: 0.515258, acc.: 74.22%] [G loss: 1.797567]\n",
      "epoch:38 step:36407 [D loss: 0.717549, acc.: 61.72%] [G loss: 1.568137]\n",
      "epoch:38 step:36408 [D loss: 0.581960, acc.: 75.78%] [G loss: 1.157270]\n",
      "epoch:38 step:36409 [D loss: 0.526359, acc.: 75.00%] [G loss: 1.415832]\n",
      "epoch:38 step:36410 [D loss: 0.287200, acc.: 88.28%] [G loss: 1.867602]\n",
      "epoch:38 step:36411 [D loss: 0.508305, acc.: 71.09%] [G loss: 1.608482]\n",
      "epoch:38 step:36412 [D loss: 0.532292, acc.: 71.09%] [G loss: 1.544492]\n",
      "epoch:38 step:36413 [D loss: 0.484793, acc.: 78.12%] [G loss: 1.613467]\n",
      "epoch:38 step:36414 [D loss: 0.342184, acc.: 91.41%] [G loss: 1.443830]\n",
      "epoch:38 step:36415 [D loss: 0.583698, acc.: 68.75%] [G loss: 1.496159]\n",
      "epoch:38 step:36416 [D loss: 0.523437, acc.: 77.34%] [G loss: 1.484402]\n",
      "epoch:38 step:36417 [D loss: 0.744621, acc.: 53.12%] [G loss: 1.755128]\n",
      "epoch:38 step:36418 [D loss: 0.619230, acc.: 67.19%] [G loss: 1.491014]\n",
      "epoch:38 step:36419 [D loss: 0.381536, acc.: 87.50%] [G loss: 1.608326]\n",
      "epoch:38 step:36420 [D loss: 0.618860, acc.: 64.84%] [G loss: 1.183971]\n",
      "epoch:38 step:36421 [D loss: 0.584643, acc.: 70.31%] [G loss: 1.398782]\n",
      "epoch:38 step:36422 [D loss: 0.496466, acc.: 72.66%] [G loss: 1.844285]\n",
      "epoch:38 step:36423 [D loss: 0.616248, acc.: 63.28%] [G loss: 1.475827]\n",
      "epoch:38 step:36424 [D loss: 0.409450, acc.: 80.47%] [G loss: 1.348774]\n",
      "epoch:38 step:36425 [D loss: 0.672363, acc.: 60.94%] [G loss: 1.815747]\n",
      "epoch:38 step:36426 [D loss: 0.417058, acc.: 84.38%] [G loss: 1.413487]\n",
      "epoch:38 step:36427 [D loss: 0.547961, acc.: 73.44%] [G loss: 1.718334]\n",
      "epoch:38 step:36428 [D loss: 0.450489, acc.: 85.16%] [G loss: 1.137726]\n",
      "epoch:38 step:36429 [D loss: 0.543217, acc.: 73.44%] [G loss: 1.574464]\n",
      "epoch:38 step:36430 [D loss: 0.541727, acc.: 69.53%] [G loss: 1.679716]\n",
      "epoch:38 step:36431 [D loss: 0.515579, acc.: 74.22%] [G loss: 1.790252]\n",
      "epoch:38 step:36432 [D loss: 0.490385, acc.: 79.69%] [G loss: 1.724204]\n",
      "epoch:38 step:36433 [D loss: 0.643528, acc.: 67.19%] [G loss: 1.292625]\n",
      "epoch:38 step:36434 [D loss: 0.690368, acc.: 64.06%] [G loss: 1.458364]\n",
      "epoch:38 step:36435 [D loss: 0.592119, acc.: 66.41%] [G loss: 1.342675]\n",
      "epoch:38 step:36436 [D loss: 0.436940, acc.: 78.91%] [G loss: 1.641115]\n",
      "epoch:38 step:36437 [D loss: 0.281532, acc.: 92.19%] [G loss: 2.475411]\n",
      "epoch:38 step:36438 [D loss: 0.634477, acc.: 60.94%] [G loss: 1.508601]\n",
      "epoch:38 step:36439 [D loss: 0.501510, acc.: 72.66%] [G loss: 1.095701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36440 [D loss: 0.643601, acc.: 66.41%] [G loss: 0.934207]\n",
      "epoch:38 step:36441 [D loss: 0.380231, acc.: 87.50%] [G loss: 1.890620]\n",
      "epoch:38 step:36442 [D loss: 0.473985, acc.: 80.47%] [G loss: 1.113927]\n",
      "epoch:38 step:36443 [D loss: 0.692122, acc.: 63.28%] [G loss: 1.353628]\n",
      "epoch:38 step:36444 [D loss: 0.493019, acc.: 77.34%] [G loss: 2.214142]\n",
      "epoch:38 step:36445 [D loss: 0.580250, acc.: 66.41%] [G loss: 1.964357]\n",
      "epoch:38 step:36446 [D loss: 0.580392, acc.: 69.53%] [G loss: 1.621490]\n",
      "epoch:38 step:36447 [D loss: 0.467655, acc.: 78.12%] [G loss: 1.157462]\n",
      "epoch:38 step:36448 [D loss: 0.540551, acc.: 74.22%] [G loss: 1.765502]\n",
      "epoch:38 step:36449 [D loss: 0.551752, acc.: 66.41%] [G loss: 1.771535]\n",
      "epoch:38 step:36450 [D loss: 0.422819, acc.: 82.03%] [G loss: 1.388785]\n",
      "epoch:38 step:36451 [D loss: 0.513410, acc.: 71.09%] [G loss: 1.270517]\n",
      "epoch:38 step:36452 [D loss: 0.350820, acc.: 87.50%] [G loss: 1.302739]\n",
      "epoch:38 step:36453 [D loss: 0.595821, acc.: 71.88%] [G loss: 1.971664]\n",
      "epoch:38 step:36454 [D loss: 0.706856, acc.: 63.28%] [G loss: 1.881890]\n",
      "epoch:38 step:36455 [D loss: 0.572094, acc.: 73.44%] [G loss: 1.762963]\n",
      "epoch:38 step:36456 [D loss: 0.344106, acc.: 86.72%] [G loss: 1.630249]\n",
      "epoch:38 step:36457 [D loss: 0.362316, acc.: 82.81%] [G loss: 1.840561]\n",
      "epoch:38 step:36458 [D loss: 0.464579, acc.: 78.91%] [G loss: 1.678030]\n",
      "epoch:38 step:36459 [D loss: 0.786849, acc.: 52.34%] [G loss: 1.092295]\n",
      "epoch:38 step:36460 [D loss: 0.750009, acc.: 53.91%] [G loss: 1.194129]\n",
      "epoch:38 step:36461 [D loss: 0.600818, acc.: 68.75%] [G loss: 1.367674]\n",
      "epoch:38 step:36462 [D loss: 0.504363, acc.: 77.34%] [G loss: 1.414532]\n",
      "epoch:38 step:36463 [D loss: 0.489613, acc.: 77.34%] [G loss: 1.854080]\n",
      "epoch:38 step:36464 [D loss: 0.571435, acc.: 74.22%] [G loss: 1.788544]\n",
      "epoch:38 step:36465 [D loss: 0.768232, acc.: 52.34%] [G loss: 1.490927]\n",
      "epoch:38 step:36466 [D loss: 0.551563, acc.: 68.75%] [G loss: 1.734367]\n",
      "epoch:38 step:36467 [D loss: 0.578274, acc.: 76.56%] [G loss: 1.780494]\n",
      "epoch:38 step:36468 [D loss: 0.742720, acc.: 56.25%] [G loss: 1.519003]\n",
      "epoch:38 step:36469 [D loss: 0.434957, acc.: 85.16%] [G loss: 1.957533]\n",
      "epoch:38 step:36470 [D loss: 0.461394, acc.: 82.03%] [G loss: 1.739053]\n",
      "epoch:38 step:36471 [D loss: 0.352539, acc.: 85.94%] [G loss: 1.535622]\n",
      "epoch:38 step:36472 [D loss: 0.464560, acc.: 79.69%] [G loss: 1.656108]\n",
      "epoch:38 step:36473 [D loss: 0.631807, acc.: 67.97%] [G loss: 1.362926]\n",
      "epoch:38 step:36474 [D loss: 0.511288, acc.: 70.31%] [G loss: 1.806085]\n",
      "epoch:38 step:36475 [D loss: 0.377128, acc.: 83.59%] [G loss: 1.625772]\n",
      "epoch:38 step:36476 [D loss: 0.506018, acc.: 76.56%] [G loss: 1.838898]\n",
      "epoch:38 step:36477 [D loss: 0.607132, acc.: 64.84%] [G loss: 1.223418]\n",
      "epoch:38 step:36478 [D loss: 0.611447, acc.: 70.31%] [G loss: 1.329184]\n",
      "epoch:38 step:36479 [D loss: 0.433203, acc.: 82.03%] [G loss: 2.110504]\n",
      "epoch:38 step:36480 [D loss: 0.442597, acc.: 82.03%] [G loss: 2.282417]\n",
      "epoch:38 step:36481 [D loss: 0.700390, acc.: 61.72%] [G loss: 1.905523]\n",
      "epoch:38 step:36482 [D loss: 0.502684, acc.: 77.34%] [G loss: 1.254021]\n",
      "epoch:38 step:36483 [D loss: 0.431508, acc.: 83.59%] [G loss: 1.301091]\n",
      "epoch:38 step:36484 [D loss: 0.552681, acc.: 69.53%] [G loss: 1.766204]\n",
      "epoch:38 step:36485 [D loss: 0.671791, acc.: 57.81%] [G loss: 1.380236]\n",
      "epoch:38 step:36486 [D loss: 0.772063, acc.: 52.34%] [G loss: 1.590345]\n",
      "epoch:38 step:36487 [D loss: 0.717437, acc.: 63.28%] [G loss: 1.824796]\n",
      "epoch:38 step:36488 [D loss: 0.506824, acc.: 75.78%] [G loss: 1.901170]\n",
      "epoch:38 step:36489 [D loss: 0.648342, acc.: 64.06%] [G loss: 1.823167]\n",
      "epoch:38 step:36490 [D loss: 0.449828, acc.: 83.59%] [G loss: 1.122021]\n",
      "epoch:38 step:36491 [D loss: 0.511132, acc.: 78.12%] [G loss: 1.741778]\n",
      "epoch:38 step:36492 [D loss: 0.591799, acc.: 66.41%] [G loss: 1.410205]\n",
      "epoch:38 step:36493 [D loss: 0.446593, acc.: 81.25%] [G loss: 1.784287]\n",
      "epoch:38 step:36494 [D loss: 0.586866, acc.: 70.31%] [G loss: 1.497914]\n",
      "epoch:38 step:36495 [D loss: 0.382864, acc.: 82.03%] [G loss: 1.260795]\n",
      "epoch:38 step:36496 [D loss: 0.638515, acc.: 62.50%] [G loss: 1.526869]\n",
      "epoch:38 step:36497 [D loss: 0.654872, acc.: 62.50%] [G loss: 1.511575]\n",
      "epoch:38 step:36498 [D loss: 0.427802, acc.: 79.69%] [G loss: 1.833502]\n",
      "epoch:38 step:36499 [D loss: 0.593175, acc.: 70.31%] [G loss: 1.626411]\n",
      "epoch:38 step:36500 [D loss: 0.518665, acc.: 75.00%] [G loss: 1.706551]\n",
      "epoch:38 step:36501 [D loss: 0.547954, acc.: 71.88%] [G loss: 1.578608]\n",
      "epoch:38 step:36502 [D loss: 0.526790, acc.: 67.97%] [G loss: 1.587091]\n",
      "epoch:38 step:36503 [D loss: 0.546077, acc.: 75.78%] [G loss: 1.497715]\n",
      "epoch:38 step:36504 [D loss: 0.314839, acc.: 92.19%] [G loss: 1.520149]\n",
      "epoch:38 step:36505 [D loss: 0.413135, acc.: 84.38%] [G loss: 1.615036]\n",
      "epoch:38 step:36506 [D loss: 0.463360, acc.: 76.56%] [G loss: 1.321737]\n",
      "epoch:38 step:36507 [D loss: 0.721297, acc.: 56.25%] [G loss: 1.470914]\n",
      "epoch:38 step:36508 [D loss: 0.385083, acc.: 87.50%] [G loss: 1.730143]\n",
      "epoch:38 step:36509 [D loss: 0.514031, acc.: 78.12%] [G loss: 1.906057]\n",
      "epoch:38 step:36510 [D loss: 0.568296, acc.: 72.66%] [G loss: 2.049421]\n",
      "epoch:38 step:36511 [D loss: 0.430824, acc.: 78.12%] [G loss: 1.850119]\n",
      "epoch:38 step:36512 [D loss: 0.788258, acc.: 50.78%] [G loss: 1.338602]\n",
      "epoch:38 step:36513 [D loss: 0.749611, acc.: 57.81%] [G loss: 1.146708]\n",
      "epoch:38 step:36514 [D loss: 0.668003, acc.: 60.94%] [G loss: 1.251130]\n",
      "epoch:38 step:36515 [D loss: 0.627703, acc.: 67.97%] [G loss: 1.527336]\n",
      "epoch:38 step:36516 [D loss: 0.367725, acc.: 86.72%] [G loss: 2.255999]\n",
      "epoch:38 step:36517 [D loss: 0.394022, acc.: 84.38%] [G loss: 1.091691]\n",
      "epoch:38 step:36518 [D loss: 0.543288, acc.: 71.09%] [G loss: 1.888924]\n",
      "epoch:38 step:36519 [D loss: 0.427188, acc.: 80.47%] [G loss: 1.340504]\n",
      "epoch:38 step:36520 [D loss: 0.652375, acc.: 63.28%] [G loss: 0.912003]\n",
      "epoch:38 step:36521 [D loss: 0.445075, acc.: 81.25%] [G loss: 1.769116]\n",
      "epoch:38 step:36522 [D loss: 0.675818, acc.: 61.72%] [G loss: 1.662414]\n",
      "epoch:38 step:36523 [D loss: 0.606853, acc.: 66.41%] [G loss: 1.592972]\n",
      "epoch:38 step:36524 [D loss: 0.550850, acc.: 71.88%] [G loss: 1.560929]\n",
      "epoch:38 step:36525 [D loss: 0.538496, acc.: 71.88%] [G loss: 1.721556]\n",
      "epoch:38 step:36526 [D loss: 0.582117, acc.: 69.53%] [G loss: 1.414919]\n",
      "epoch:38 step:36527 [D loss: 0.365944, acc.: 88.28%] [G loss: 1.966837]\n",
      "epoch:38 step:36528 [D loss: 0.340848, acc.: 89.06%] [G loss: 1.431945]\n",
      "epoch:38 step:36529 [D loss: 0.489608, acc.: 81.25%] [G loss: 1.549193]\n",
      "epoch:38 step:36530 [D loss: 0.653968, acc.: 62.50%] [G loss: 1.063296]\n",
      "epoch:38 step:36531 [D loss: 0.428287, acc.: 80.47%] [G loss: 1.584092]\n",
      "epoch:38 step:36532 [D loss: 0.608125, acc.: 62.50%] [G loss: 1.206801]\n",
      "epoch:38 step:36533 [D loss: 0.879362, acc.: 46.09%] [G loss: 1.603151]\n",
      "epoch:38 step:36534 [D loss: 0.523186, acc.: 75.78%] [G loss: 1.408235]\n",
      "epoch:38 step:36535 [D loss: 0.717508, acc.: 61.72%] [G loss: 1.286157]\n",
      "epoch:38 step:36536 [D loss: 0.573684, acc.: 68.75%] [G loss: 2.176999]\n",
      "epoch:38 step:36537 [D loss: 0.440262, acc.: 78.12%] [G loss: 1.254084]\n",
      "epoch:38 step:36538 [D loss: 0.658804, acc.: 67.97%] [G loss: 1.579676]\n",
      "epoch:38 step:36539 [D loss: 0.718320, acc.: 54.69%] [G loss: 1.641949]\n",
      "epoch:38 step:36540 [D loss: 0.519562, acc.: 73.44%] [G loss: 2.133227]\n",
      "epoch:38 step:36541 [D loss: 0.423546, acc.: 83.59%] [G loss: 1.637503]\n",
      "epoch:38 step:36542 [D loss: 0.461592, acc.: 75.78%] [G loss: 1.656741]\n",
      "epoch:38 step:36543 [D loss: 0.529382, acc.: 73.44%] [G loss: 1.811698]\n",
      "epoch:39 step:36544 [D loss: 0.716726, acc.: 58.59%] [G loss: 1.484015]\n",
      "epoch:39 step:36545 [D loss: 0.349848, acc.: 89.06%] [G loss: 1.615542]\n",
      "epoch:39 step:36546 [D loss: 0.770301, acc.: 57.81%] [G loss: 1.378724]\n",
      "epoch:39 step:36547 [D loss: 0.473170, acc.: 78.12%] [G loss: 1.198445]\n",
      "epoch:39 step:36548 [D loss: 0.491898, acc.: 75.78%] [G loss: 2.004968]\n",
      "epoch:39 step:36549 [D loss: 0.757230, acc.: 59.38%] [G loss: 1.527221]\n",
      "epoch:39 step:36550 [D loss: 0.568657, acc.: 75.00%] [G loss: 1.270697]\n",
      "epoch:39 step:36551 [D loss: 0.338745, acc.: 88.28%] [G loss: 2.215946]\n",
      "epoch:39 step:36552 [D loss: 0.431225, acc.: 82.81%] [G loss: 1.393503]\n",
      "epoch:39 step:36553 [D loss: 0.465620, acc.: 79.69%] [G loss: 1.316881]\n",
      "epoch:39 step:36554 [D loss: 0.581862, acc.: 68.75%] [G loss: 1.736705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36555 [D loss: 0.755531, acc.: 54.69%] [G loss: 1.080014]\n",
      "epoch:39 step:36556 [D loss: 0.634751, acc.: 62.50%] [G loss: 1.165677]\n",
      "epoch:39 step:36557 [D loss: 0.530984, acc.: 71.09%] [G loss: 1.514046]\n",
      "epoch:39 step:36558 [D loss: 0.490441, acc.: 78.91%] [G loss: 1.923584]\n",
      "epoch:39 step:36559 [D loss: 0.365844, acc.: 85.16%] [G loss: 2.315096]\n",
      "epoch:39 step:36560 [D loss: 0.559244, acc.: 69.53%] [G loss: 2.269938]\n",
      "epoch:39 step:36561 [D loss: 0.399366, acc.: 83.59%] [G loss: 1.264766]\n",
      "epoch:39 step:36562 [D loss: 0.696950, acc.: 64.06%] [G loss: 1.114382]\n",
      "epoch:39 step:36563 [D loss: 0.430504, acc.: 80.47%] [G loss: 1.737742]\n",
      "epoch:39 step:36564 [D loss: 0.588431, acc.: 67.97%] [G loss: 1.268113]\n",
      "epoch:39 step:36565 [D loss: 0.527828, acc.: 71.09%] [G loss: 1.947105]\n",
      "epoch:39 step:36566 [D loss: 0.554134, acc.: 71.88%] [G loss: 1.968933]\n",
      "epoch:39 step:36567 [D loss: 0.525827, acc.: 75.78%] [G loss: 1.624256]\n",
      "epoch:39 step:36568 [D loss: 0.396282, acc.: 85.94%] [G loss: 1.716010]\n",
      "epoch:39 step:36569 [D loss: 0.599861, acc.: 67.97%] [G loss: 1.415647]\n",
      "epoch:39 step:36570 [D loss: 0.643260, acc.: 62.50%] [G loss: 1.053467]\n",
      "epoch:39 step:36571 [D loss: 0.413534, acc.: 82.81%] [G loss: 1.984923]\n",
      "epoch:39 step:36572 [D loss: 0.660169, acc.: 64.84%] [G loss: 1.179462]\n",
      "epoch:39 step:36573 [D loss: 0.458390, acc.: 79.69%] [G loss: 1.581214]\n",
      "epoch:39 step:36574 [D loss: 0.419723, acc.: 81.25%] [G loss: 1.261617]\n",
      "epoch:39 step:36575 [D loss: 0.471179, acc.: 78.91%] [G loss: 1.659492]\n",
      "epoch:39 step:36576 [D loss: 0.466136, acc.: 78.12%] [G loss: 1.665750]\n",
      "epoch:39 step:36577 [D loss: 0.434198, acc.: 80.47%] [G loss: 1.283522]\n",
      "epoch:39 step:36578 [D loss: 0.526367, acc.: 75.00%] [G loss: 2.274770]\n",
      "epoch:39 step:36579 [D loss: 0.403233, acc.: 84.38%] [G loss: 1.422760]\n",
      "epoch:39 step:36580 [D loss: 0.422411, acc.: 82.81%] [G loss: 2.304229]\n",
      "epoch:39 step:36581 [D loss: 0.600074, acc.: 70.31%] [G loss: 1.766249]\n",
      "epoch:39 step:36582 [D loss: 0.478330, acc.: 76.56%] [G loss: 1.715934]\n",
      "epoch:39 step:36583 [D loss: 0.565428, acc.: 75.78%] [G loss: 1.202982]\n",
      "epoch:39 step:36584 [D loss: 0.616892, acc.: 67.19%] [G loss: 1.347665]\n",
      "epoch:39 step:36585 [D loss: 0.521632, acc.: 73.44%] [G loss: 1.271176]\n",
      "epoch:39 step:36586 [D loss: 0.641187, acc.: 64.06%] [G loss: 1.439019]\n",
      "epoch:39 step:36587 [D loss: 0.454977, acc.: 80.47%] [G loss: 1.473024]\n",
      "epoch:39 step:36588 [D loss: 0.471389, acc.: 81.25%] [G loss: 1.674258]\n",
      "epoch:39 step:36589 [D loss: 0.647397, acc.: 67.19%] [G loss: 1.004039]\n",
      "epoch:39 step:36590 [D loss: 0.880720, acc.: 50.78%] [G loss: 1.214431]\n",
      "epoch:39 step:36591 [D loss: 0.515386, acc.: 78.91%] [G loss: 1.884056]\n",
      "epoch:39 step:36592 [D loss: 0.560706, acc.: 71.09%] [G loss: 1.740224]\n",
      "epoch:39 step:36593 [D loss: 0.489569, acc.: 78.12%] [G loss: 2.102943]\n",
      "epoch:39 step:36594 [D loss: 0.621605, acc.: 71.88%] [G loss: 1.883166]\n",
      "epoch:39 step:36595 [D loss: 0.394940, acc.: 86.72%] [G loss: 1.542409]\n",
      "epoch:39 step:36596 [D loss: 0.261507, acc.: 92.97%] [G loss: 2.088920]\n",
      "epoch:39 step:36597 [D loss: 0.431236, acc.: 81.25%] [G loss: 1.428339]\n",
      "epoch:39 step:36598 [D loss: 0.618021, acc.: 60.94%] [G loss: 1.463274]\n",
      "epoch:39 step:36599 [D loss: 0.568778, acc.: 71.88%] [G loss: 1.269484]\n",
      "epoch:39 step:36600 [D loss: 0.562410, acc.: 74.22%] [G loss: 1.823369]\n",
      "##############\n",
      "[2.6043221  2.09371298 1.5963527  2.50859874 0.82555953 5.75817447\n",
      " 2.17093236 2.42153636 3.88916081 4.33699705]\n",
      "##########\n",
      "epoch:39 step:36601 [D loss: 0.559850, acc.: 75.00%] [G loss: 1.546233]\n",
      "epoch:39 step:36602 [D loss: 0.248226, acc.: 92.97%] [G loss: 2.106805]\n",
      "epoch:39 step:36603 [D loss: 0.349585, acc.: 87.50%] [G loss: 1.248497]\n",
      "epoch:39 step:36604 [D loss: 0.546416, acc.: 71.88%] [G loss: 1.728148]\n",
      "epoch:39 step:36605 [D loss: 0.434591, acc.: 80.47%] [G loss: 1.851549]\n",
      "epoch:39 step:36606 [D loss: 0.566797, acc.: 67.97%] [G loss: 1.727388]\n",
      "epoch:39 step:36607 [D loss: 0.483153, acc.: 77.34%] [G loss: 2.019180]\n",
      "epoch:39 step:36608 [D loss: 0.295211, acc.: 91.41%] [G loss: 1.479904]\n",
      "epoch:39 step:36609 [D loss: 0.287650, acc.: 88.28%] [G loss: 1.458771]\n",
      "epoch:39 step:36610 [D loss: 0.524195, acc.: 74.22%] [G loss: 1.477630]\n",
      "epoch:39 step:36611 [D loss: 0.478045, acc.: 75.78%] [G loss: 1.631537]\n",
      "epoch:39 step:36612 [D loss: 0.397425, acc.: 79.69%] [G loss: 1.622391]\n",
      "epoch:39 step:36613 [D loss: 0.330736, acc.: 89.84%] [G loss: 1.811279]\n",
      "epoch:39 step:36614 [D loss: 0.612875, acc.: 68.75%] [G loss: 0.993658]\n",
      "epoch:39 step:36615 [D loss: 0.465995, acc.: 80.47%] [G loss: 1.518748]\n",
      "epoch:39 step:36616 [D loss: 0.423304, acc.: 83.59%] [G loss: 2.065886]\n",
      "epoch:39 step:36617 [D loss: 0.314449, acc.: 87.50%] [G loss: 1.525642]\n",
      "epoch:39 step:36618 [D loss: 0.368859, acc.: 85.16%] [G loss: 2.092797]\n",
      "epoch:39 step:36619 [D loss: 0.520557, acc.: 71.09%] [G loss: 1.984926]\n",
      "epoch:39 step:36620 [D loss: 0.427984, acc.: 82.81%] [G loss: 1.468628]\n",
      "epoch:39 step:36621 [D loss: 0.564002, acc.: 74.22%] [G loss: 1.458436]\n",
      "epoch:39 step:36622 [D loss: 0.527608, acc.: 73.44%] [G loss: 1.307100]\n",
      "epoch:39 step:36623 [D loss: 0.514667, acc.: 71.88%] [G loss: 1.721593]\n",
      "epoch:39 step:36624 [D loss: 0.923358, acc.: 39.84%] [G loss: 1.017342]\n",
      "epoch:39 step:36625 [D loss: 0.584524, acc.: 67.19%] [G loss: 1.600850]\n",
      "epoch:39 step:36626 [D loss: 0.464945, acc.: 75.00%] [G loss: 2.219081]\n",
      "epoch:39 step:36627 [D loss: 0.449187, acc.: 80.47%] [G loss: 2.266025]\n",
      "epoch:39 step:36628 [D loss: 0.562275, acc.: 66.41%] [G loss: 1.146916]\n",
      "epoch:39 step:36629 [D loss: 0.756648, acc.: 61.72%] [G loss: 1.612556]\n",
      "epoch:39 step:36630 [D loss: 0.442641, acc.: 82.03%] [G loss: 1.793185]\n",
      "epoch:39 step:36631 [D loss: 0.403875, acc.: 81.25%] [G loss: 1.573114]\n",
      "epoch:39 step:36632 [D loss: 0.434109, acc.: 78.91%] [G loss: 1.450598]\n",
      "epoch:39 step:36633 [D loss: 0.571309, acc.: 69.53%] [G loss: 1.501688]\n",
      "epoch:39 step:36634 [D loss: 0.454697, acc.: 80.47%] [G loss: 1.933854]\n",
      "epoch:39 step:36635 [D loss: 0.420021, acc.: 82.03%] [G loss: 1.065889]\n",
      "epoch:39 step:36636 [D loss: 0.473337, acc.: 75.78%] [G loss: 1.224205]\n",
      "epoch:39 step:36637 [D loss: 0.413980, acc.: 80.47%] [G loss: 1.907435]\n",
      "epoch:39 step:36638 [D loss: 0.713954, acc.: 59.38%] [G loss: 1.736936]\n",
      "epoch:39 step:36639 [D loss: 0.462748, acc.: 78.12%] [G loss: 1.249759]\n",
      "epoch:39 step:36640 [D loss: 0.396383, acc.: 83.59%] [G loss: 1.741756]\n",
      "epoch:39 step:36641 [D loss: 0.292822, acc.: 89.84%] [G loss: 1.949311]\n",
      "epoch:39 step:36642 [D loss: 0.442724, acc.: 82.03%] [G loss: 1.785277]\n",
      "epoch:39 step:36643 [D loss: 0.476817, acc.: 77.34%] [G loss: 1.518862]\n",
      "epoch:39 step:36644 [D loss: 0.604133, acc.: 68.75%] [G loss: 1.265041]\n",
      "epoch:39 step:36645 [D loss: 0.701489, acc.: 57.03%] [G loss: 1.544948]\n",
      "epoch:39 step:36646 [D loss: 0.455266, acc.: 78.12%] [G loss: 1.449099]\n",
      "epoch:39 step:36647 [D loss: 0.542124, acc.: 73.44%] [G loss: 1.938327]\n",
      "epoch:39 step:36648 [D loss: 0.546676, acc.: 72.66%] [G loss: 1.551717]\n",
      "epoch:39 step:36649 [D loss: 0.556931, acc.: 71.88%] [G loss: 1.948992]\n",
      "epoch:39 step:36650 [D loss: 0.518299, acc.: 74.22%] [G loss: 1.688491]\n",
      "epoch:39 step:36651 [D loss: 0.333146, acc.: 89.84%] [G loss: 1.733711]\n",
      "epoch:39 step:36652 [D loss: 0.327458, acc.: 89.84%] [G loss: 1.830901]\n",
      "epoch:39 step:36653 [D loss: 0.461042, acc.: 81.25%] [G loss: 2.002946]\n",
      "epoch:39 step:36654 [D loss: 0.467048, acc.: 79.69%] [G loss: 1.178875]\n",
      "epoch:39 step:36655 [D loss: 0.485668, acc.: 71.88%] [G loss: 1.576097]\n",
      "epoch:39 step:36656 [D loss: 0.446566, acc.: 82.03%] [G loss: 1.912104]\n",
      "epoch:39 step:36657 [D loss: 0.453001, acc.: 79.69%] [G loss: 1.913612]\n",
      "epoch:39 step:36658 [D loss: 0.496497, acc.: 78.12%] [G loss: 1.650595]\n",
      "epoch:39 step:36659 [D loss: 0.558773, acc.: 67.97%] [G loss: 1.179906]\n",
      "epoch:39 step:36660 [D loss: 0.619985, acc.: 68.75%] [G loss: 1.580831]\n",
      "epoch:39 step:36661 [D loss: 0.365634, acc.: 85.94%] [G loss: 1.521636]\n",
      "epoch:39 step:36662 [D loss: 0.267484, acc.: 95.31%] [G loss: 1.665951]\n",
      "epoch:39 step:36663 [D loss: 0.482298, acc.: 79.69%] [G loss: 1.131056]\n",
      "epoch:39 step:36664 [D loss: 0.563572, acc.: 66.41%] [G loss: 1.506610]\n",
      "epoch:39 step:36665 [D loss: 0.371773, acc.: 80.47%] [G loss: 1.692705]\n",
      "epoch:39 step:36666 [D loss: 0.675118, acc.: 62.50%] [G loss: 1.134417]\n",
      "epoch:39 step:36667 [D loss: 0.527814, acc.: 73.44%] [G loss: 1.142918]\n",
      "epoch:39 step:36668 [D loss: 1.140579, acc.: 31.25%] [G loss: 1.463510]\n",
      "epoch:39 step:36669 [D loss: 0.434849, acc.: 80.47%] [G loss: 1.450925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36670 [D loss: 0.494428, acc.: 78.91%] [G loss: 1.831860]\n",
      "epoch:39 step:36671 [D loss: 0.537170, acc.: 73.44%] [G loss: 1.856063]\n",
      "epoch:39 step:36672 [D loss: 0.377203, acc.: 87.50%] [G loss: 1.646347]\n",
      "epoch:39 step:36673 [D loss: 0.413032, acc.: 85.94%] [G loss: 1.577160]\n",
      "epoch:39 step:36674 [D loss: 0.415340, acc.: 84.38%] [G loss: 1.898003]\n",
      "epoch:39 step:36675 [D loss: 0.483751, acc.: 76.56%] [G loss: 1.634705]\n",
      "epoch:39 step:36676 [D loss: 0.502800, acc.: 75.00%] [G loss: 1.536427]\n",
      "epoch:39 step:36677 [D loss: 0.607006, acc.: 65.62%] [G loss: 1.267756]\n",
      "epoch:39 step:36678 [D loss: 0.359960, acc.: 89.06%] [G loss: 1.095916]\n",
      "epoch:39 step:36679 [D loss: 1.013366, acc.: 31.25%] [G loss: 1.099952]\n",
      "epoch:39 step:36680 [D loss: 0.560668, acc.: 71.88%] [G loss: 1.394115]\n",
      "epoch:39 step:36681 [D loss: 0.469493, acc.: 78.91%] [G loss: 2.082990]\n",
      "epoch:39 step:36682 [D loss: 0.570622, acc.: 67.19%] [G loss: 1.426190]\n",
      "epoch:39 step:36683 [D loss: 0.530868, acc.: 71.09%] [G loss: 2.069980]\n",
      "epoch:39 step:36684 [D loss: 0.719019, acc.: 60.94%] [G loss: 1.686404]\n",
      "epoch:39 step:36685 [D loss: 0.444184, acc.: 82.81%] [G loss: 1.459539]\n",
      "epoch:39 step:36686 [D loss: 0.582541, acc.: 68.75%] [G loss: 1.667470]\n",
      "epoch:39 step:36687 [D loss: 0.830856, acc.: 51.56%] [G loss: 1.090517]\n",
      "epoch:39 step:36688 [D loss: 0.270601, acc.: 91.41%] [G loss: 2.235353]\n",
      "epoch:39 step:36689 [D loss: 0.582170, acc.: 64.06%] [G loss: 1.178028]\n",
      "epoch:39 step:36690 [D loss: 0.478581, acc.: 79.69%] [G loss: 1.532996]\n",
      "epoch:39 step:36691 [D loss: 0.218127, acc.: 92.97%] [G loss: 1.453933]\n",
      "epoch:39 step:36692 [D loss: 0.508385, acc.: 76.56%] [G loss: 1.146068]\n",
      "epoch:39 step:36693 [D loss: 0.528285, acc.: 73.44%] [G loss: 1.476720]\n",
      "epoch:39 step:36694 [D loss: 0.442937, acc.: 76.56%] [G loss: 1.466511]\n",
      "epoch:39 step:36695 [D loss: 1.015917, acc.: 39.06%] [G loss: 1.445953]\n",
      "epoch:39 step:36696 [D loss: 0.377629, acc.: 86.72%] [G loss: 1.891740]\n",
      "epoch:39 step:36697 [D loss: 0.485796, acc.: 77.34%] [G loss: 1.274601]\n",
      "epoch:39 step:36698 [D loss: 0.607865, acc.: 69.53%] [G loss: 1.879415]\n",
      "epoch:39 step:36699 [D loss: 0.529618, acc.: 71.09%] [G loss: 1.486654]\n",
      "epoch:39 step:36700 [D loss: 0.571983, acc.: 70.31%] [G loss: 1.847774]\n",
      "epoch:39 step:36701 [D loss: 0.504986, acc.: 72.66%] [G loss: 1.512038]\n",
      "epoch:39 step:36702 [D loss: 0.381443, acc.: 87.50%] [G loss: 1.363774]\n",
      "epoch:39 step:36703 [D loss: 0.500649, acc.: 74.22%] [G loss: 1.042537]\n",
      "epoch:39 step:36704 [D loss: 0.691257, acc.: 60.16%] [G loss: 1.487962]\n",
      "epoch:39 step:36705 [D loss: 0.649850, acc.: 64.06%] [G loss: 1.143384]\n",
      "epoch:39 step:36706 [D loss: 0.450080, acc.: 77.34%] [G loss: 1.969680]\n",
      "epoch:39 step:36707 [D loss: 0.435367, acc.: 81.25%] [G loss: 1.310503]\n",
      "epoch:39 step:36708 [D loss: 0.412520, acc.: 82.81%] [G loss: 1.279703]\n",
      "epoch:39 step:36709 [D loss: 0.523301, acc.: 74.22%] [G loss: 1.802479]\n",
      "epoch:39 step:36710 [D loss: 0.437935, acc.: 82.03%] [G loss: 1.368265]\n",
      "epoch:39 step:36711 [D loss: 0.659240, acc.: 67.97%] [G loss: 1.332501]\n",
      "epoch:39 step:36712 [D loss: 0.491607, acc.: 77.34%] [G loss: 1.519103]\n",
      "epoch:39 step:36713 [D loss: 0.329727, acc.: 89.06%] [G loss: 1.830963]\n",
      "epoch:39 step:36714 [D loss: 0.390156, acc.: 86.72%] [G loss: 2.042604]\n",
      "epoch:39 step:36715 [D loss: 0.287128, acc.: 92.97%] [G loss: 2.229338]\n",
      "epoch:39 step:36716 [D loss: 0.741881, acc.: 59.38%] [G loss: 1.007920]\n",
      "epoch:39 step:36717 [D loss: 0.508424, acc.: 71.88%] [G loss: 1.621659]\n",
      "epoch:39 step:36718 [D loss: 0.737054, acc.: 61.72%] [G loss: 1.259160]\n",
      "epoch:39 step:36719 [D loss: 0.570900, acc.: 71.88%] [G loss: 1.695330]\n",
      "epoch:39 step:36720 [D loss: 0.523779, acc.: 71.09%] [G loss: 1.881624]\n",
      "epoch:39 step:36721 [D loss: 0.557332, acc.: 74.22%] [G loss: 1.416718]\n",
      "epoch:39 step:36722 [D loss: 0.539269, acc.: 73.44%] [G loss: 1.456566]\n",
      "epoch:39 step:36723 [D loss: 0.418728, acc.: 85.16%] [G loss: 1.774282]\n",
      "epoch:39 step:36724 [D loss: 0.353762, acc.: 83.59%] [G loss: 1.446872]\n",
      "epoch:39 step:36725 [D loss: 0.512190, acc.: 75.78%] [G loss: 1.596723]\n",
      "epoch:39 step:36726 [D loss: 0.564540, acc.: 70.31%] [G loss: 1.784697]\n",
      "epoch:39 step:36727 [D loss: 0.712910, acc.: 62.50%] [G loss: 1.482474]\n",
      "epoch:39 step:36728 [D loss: 0.709078, acc.: 61.72%] [G loss: 1.250669]\n",
      "epoch:39 step:36729 [D loss: 0.556876, acc.: 71.09%] [G loss: 1.739866]\n",
      "epoch:39 step:36730 [D loss: 0.416294, acc.: 82.81%] [G loss: 2.008778]\n",
      "epoch:39 step:36731 [D loss: 0.501912, acc.: 77.34%] [G loss: 1.631565]\n",
      "epoch:39 step:36732 [D loss: 0.418628, acc.: 79.69%] [G loss: 1.832170]\n",
      "epoch:39 step:36733 [D loss: 0.684369, acc.: 60.94%] [G loss: 1.640610]\n",
      "epoch:39 step:36734 [D loss: 0.493163, acc.: 75.00%] [G loss: 1.555782]\n",
      "epoch:39 step:36735 [D loss: 0.608591, acc.: 67.19%] [G loss: 1.541161]\n",
      "epoch:39 step:36736 [D loss: 0.576928, acc.: 68.75%] [G loss: 1.928394]\n",
      "epoch:39 step:36737 [D loss: 0.673892, acc.: 63.28%] [G loss: 1.004632]\n",
      "epoch:39 step:36738 [D loss: 0.640520, acc.: 61.72%] [G loss: 1.569154]\n",
      "epoch:39 step:36739 [D loss: 0.499603, acc.: 73.44%] [G loss: 1.207534]\n",
      "epoch:39 step:36740 [D loss: 0.538539, acc.: 75.00%] [G loss: 1.493501]\n",
      "epoch:39 step:36741 [D loss: 0.426787, acc.: 82.03%] [G loss: 1.741465]\n",
      "epoch:39 step:36742 [D loss: 0.542759, acc.: 75.00%] [G loss: 1.641446]\n",
      "epoch:39 step:36743 [D loss: 0.309866, acc.: 91.41%] [G loss: 1.870090]\n",
      "epoch:39 step:36744 [D loss: 0.463955, acc.: 75.00%] [G loss: 1.636783]\n",
      "epoch:39 step:36745 [D loss: 0.678825, acc.: 62.50%] [G loss: 1.633672]\n",
      "epoch:39 step:36746 [D loss: 0.450099, acc.: 81.25%] [G loss: 1.486558]\n",
      "epoch:39 step:36747 [D loss: 0.351772, acc.: 88.28%] [G loss: 2.155533]\n",
      "epoch:39 step:36748 [D loss: 0.430693, acc.: 79.69%] [G loss: 1.850845]\n",
      "epoch:39 step:36749 [D loss: 0.593519, acc.: 66.41%] [G loss: 1.488409]\n",
      "epoch:39 step:36750 [D loss: 0.658689, acc.: 63.28%] [G loss: 1.241814]\n",
      "epoch:39 step:36751 [D loss: 0.451802, acc.: 80.47%] [G loss: 1.734631]\n",
      "epoch:39 step:36752 [D loss: 0.401416, acc.: 79.69%] [G loss: 2.070831]\n",
      "epoch:39 step:36753 [D loss: 0.409197, acc.: 82.03%] [G loss: 1.402560]\n",
      "epoch:39 step:36754 [D loss: 0.382014, acc.: 81.25%] [G loss: 1.365989]\n",
      "epoch:39 step:36755 [D loss: 0.396211, acc.: 83.59%] [G loss: 1.952362]\n",
      "epoch:39 step:36756 [D loss: 0.448944, acc.: 78.12%] [G loss: 1.437527]\n",
      "epoch:39 step:36757 [D loss: 0.511132, acc.: 75.00%] [G loss: 1.852210]\n",
      "epoch:39 step:36758 [D loss: 0.372737, acc.: 87.50%] [G loss: 1.256719]\n",
      "epoch:39 step:36759 [D loss: 0.532809, acc.: 71.88%] [G loss: 1.673248]\n",
      "epoch:39 step:36760 [D loss: 0.442553, acc.: 78.91%] [G loss: 1.609227]\n",
      "epoch:39 step:36761 [D loss: 0.482169, acc.: 79.69%] [G loss: 1.671381]\n",
      "epoch:39 step:36762 [D loss: 0.786433, acc.: 54.69%] [G loss: 0.955219]\n",
      "epoch:39 step:36763 [D loss: 0.742043, acc.: 56.25%] [G loss: 1.237138]\n",
      "epoch:39 step:36764 [D loss: 0.394182, acc.: 87.50%] [G loss: 1.395090]\n",
      "epoch:39 step:36765 [D loss: 0.526336, acc.: 76.56%] [G loss: 1.029434]\n",
      "epoch:39 step:36766 [D loss: 0.501407, acc.: 79.69%] [G loss: 1.415481]\n",
      "epoch:39 step:36767 [D loss: 0.459562, acc.: 78.12%] [G loss: 1.143049]\n",
      "epoch:39 step:36768 [D loss: 0.460141, acc.: 77.34%] [G loss: 1.093508]\n",
      "epoch:39 step:36769 [D loss: 0.501338, acc.: 75.78%] [G loss: 2.060450]\n",
      "epoch:39 step:36770 [D loss: 0.692686, acc.: 59.38%] [G loss: 1.863790]\n",
      "epoch:39 step:36771 [D loss: 0.480668, acc.: 75.00%] [G loss: 1.430180]\n",
      "epoch:39 step:36772 [D loss: 0.679315, acc.: 60.94%] [G loss: 1.280288]\n",
      "epoch:39 step:36773 [D loss: 0.411917, acc.: 83.59%] [G loss: 1.662941]\n",
      "epoch:39 step:36774 [D loss: 0.604967, acc.: 66.41%] [G loss: 1.317722]\n",
      "epoch:39 step:36775 [D loss: 0.417508, acc.: 81.25%] [G loss: 1.585902]\n",
      "epoch:39 step:36776 [D loss: 0.392952, acc.: 82.03%] [G loss: 2.266090]\n",
      "epoch:39 step:36777 [D loss: 0.561660, acc.: 78.12%] [G loss: 1.260454]\n",
      "epoch:39 step:36778 [D loss: 0.441305, acc.: 83.59%] [G loss: 1.607942]\n",
      "epoch:39 step:36779 [D loss: 0.357908, acc.: 85.94%] [G loss: 1.409545]\n",
      "epoch:39 step:36780 [D loss: 0.586650, acc.: 67.97%] [G loss: 1.309277]\n",
      "epoch:39 step:36781 [D loss: 0.728162, acc.: 59.38%] [G loss: 1.649423]\n",
      "epoch:39 step:36782 [D loss: 0.594336, acc.: 69.53%] [G loss: 1.300305]\n",
      "epoch:39 step:36783 [D loss: 0.732497, acc.: 55.47%] [G loss: 1.180987]\n",
      "epoch:39 step:36784 [D loss: 0.642288, acc.: 62.50%] [G loss: 1.789724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36785 [D loss: 0.530761, acc.: 75.00%] [G loss: 1.877424]\n",
      "epoch:39 step:36786 [D loss: 0.644874, acc.: 65.62%] [G loss: 1.306251]\n",
      "epoch:39 step:36787 [D loss: 0.486249, acc.: 77.34%] [G loss: 1.007122]\n",
      "epoch:39 step:36788 [D loss: 0.425453, acc.: 80.47%] [G loss: 1.253099]\n",
      "epoch:39 step:36789 [D loss: 0.438500, acc.: 78.91%] [G loss: 1.369039]\n",
      "epoch:39 step:36790 [D loss: 0.511369, acc.: 73.44%] [G loss: 1.643860]\n",
      "epoch:39 step:36791 [D loss: 0.572997, acc.: 70.31%] [G loss: 1.411530]\n",
      "epoch:39 step:36792 [D loss: 0.520809, acc.: 76.56%] [G loss: 1.323607]\n",
      "epoch:39 step:36793 [D loss: 0.478781, acc.: 79.69%] [G loss: 1.918739]\n",
      "epoch:39 step:36794 [D loss: 0.390992, acc.: 83.59%] [G loss: 1.723991]\n",
      "epoch:39 step:36795 [D loss: 0.492703, acc.: 78.91%] [G loss: 1.556947]\n",
      "epoch:39 step:36796 [D loss: 0.357355, acc.: 85.94%] [G loss: 1.412130]\n",
      "epoch:39 step:36797 [D loss: 0.467380, acc.: 75.78%] [G loss: 1.961758]\n",
      "epoch:39 step:36798 [D loss: 0.322211, acc.: 88.28%] [G loss: 1.477864]\n",
      "epoch:39 step:36799 [D loss: 0.508558, acc.: 74.22%] [G loss: 1.530630]\n",
      "epoch:39 step:36800 [D loss: 0.442053, acc.: 77.34%] [G loss: 1.595123]\n",
      "##############\n",
      "[2.73447739 2.0220661  2.16376616 3.07746713 1.27321118 6.25785026\n",
      " 2.43069492 2.69049759 3.93024388 5.58289804]\n",
      "##########\n",
      "epoch:39 step:36801 [D loss: 0.647711, acc.: 64.84%] [G loss: 1.400448]\n",
      "epoch:39 step:36802 [D loss: 0.539378, acc.: 71.09%] [G loss: 1.858103]\n",
      "epoch:39 step:36803 [D loss: 0.417499, acc.: 78.91%] [G loss: 1.484574]\n",
      "epoch:39 step:36804 [D loss: 0.451185, acc.: 76.56%] [G loss: 1.625890]\n",
      "epoch:39 step:36805 [D loss: 0.510347, acc.: 71.09%] [G loss: 1.853372]\n",
      "epoch:39 step:36806 [D loss: 0.604516, acc.: 68.75%] [G loss: 2.081611]\n",
      "epoch:39 step:36807 [D loss: 0.454143, acc.: 80.47%] [G loss: 1.311118]\n",
      "epoch:39 step:36808 [D loss: 0.501273, acc.: 72.66%] [G loss: 1.601640]\n",
      "epoch:39 step:36809 [D loss: 0.482660, acc.: 78.12%] [G loss: 1.708367]\n",
      "epoch:39 step:36810 [D loss: 0.615987, acc.: 67.19%] [G loss: 1.570141]\n",
      "epoch:39 step:36811 [D loss: 0.601429, acc.: 72.66%] [G loss: 1.562050]\n",
      "epoch:39 step:36812 [D loss: 0.346790, acc.: 88.28%] [G loss: 1.902449]\n",
      "epoch:39 step:36813 [D loss: 0.342638, acc.: 88.28%] [G loss: 2.067630]\n",
      "epoch:39 step:36814 [D loss: 0.617899, acc.: 68.75%] [G loss: 1.395283]\n",
      "epoch:39 step:36815 [D loss: 0.495774, acc.: 76.56%] [G loss: 1.391654]\n",
      "epoch:39 step:36816 [D loss: 0.400217, acc.: 83.59%] [G loss: 1.560585]\n",
      "epoch:39 step:36817 [D loss: 0.730772, acc.: 61.72%] [G loss: 1.692154]\n",
      "epoch:39 step:36818 [D loss: 0.573967, acc.: 68.75%] [G loss: 1.398843]\n",
      "epoch:39 step:36819 [D loss: 0.741427, acc.: 57.81%] [G loss: 1.653304]\n",
      "epoch:39 step:36820 [D loss: 0.343618, acc.: 89.06%] [G loss: 1.990742]\n",
      "epoch:39 step:36821 [D loss: 0.593714, acc.: 69.53%] [G loss: 2.012742]\n",
      "epoch:39 step:36822 [D loss: 0.576627, acc.: 69.53%] [G loss: 1.456450]\n",
      "epoch:39 step:36823 [D loss: 0.573673, acc.: 70.31%] [G loss: 1.391098]\n",
      "epoch:39 step:36824 [D loss: 0.326324, acc.: 87.50%] [G loss: 2.045110]\n",
      "epoch:39 step:36825 [D loss: 0.503943, acc.: 71.88%] [G loss: 1.479568]\n",
      "epoch:39 step:36826 [D loss: 0.511887, acc.: 77.34%] [G loss: 1.170003]\n",
      "epoch:39 step:36827 [D loss: 0.819101, acc.: 57.81%] [G loss: 1.264536]\n",
      "epoch:39 step:36828 [D loss: 0.432193, acc.: 80.47%] [G loss: 1.816449]\n",
      "epoch:39 step:36829 [D loss: 0.554182, acc.: 71.88%] [G loss: 1.846992]\n",
      "epoch:39 step:36830 [D loss: 0.572562, acc.: 71.88%] [G loss: 1.102114]\n",
      "epoch:39 step:36831 [D loss: 0.406469, acc.: 80.47%] [G loss: 1.337922]\n",
      "epoch:39 step:36832 [D loss: 0.468853, acc.: 72.66%] [G loss: 1.488115]\n",
      "epoch:39 step:36833 [D loss: 0.444597, acc.: 79.69%] [G loss: 1.507159]\n",
      "epoch:39 step:36834 [D loss: 0.526593, acc.: 75.00%] [G loss: 1.651581]\n",
      "epoch:39 step:36835 [D loss: 0.460477, acc.: 78.91%] [G loss: 1.568162]\n",
      "epoch:39 step:36836 [D loss: 0.532859, acc.: 76.56%] [G loss: 1.593967]\n",
      "epoch:39 step:36837 [D loss: 0.652027, acc.: 66.41%] [G loss: 1.123345]\n",
      "epoch:39 step:36838 [D loss: 0.482397, acc.: 81.25%] [G loss: 1.778468]\n",
      "epoch:39 step:36839 [D loss: 0.341205, acc.: 87.50%] [G loss: 1.513640]\n",
      "epoch:39 step:36840 [D loss: 0.571444, acc.: 68.75%] [G loss: 1.679421]\n",
      "epoch:39 step:36841 [D loss: 0.538370, acc.: 74.22%] [G loss: 1.160819]\n",
      "epoch:39 step:36842 [D loss: 0.606339, acc.: 68.75%] [G loss: 1.382850]\n",
      "epoch:39 step:36843 [D loss: 0.613020, acc.: 68.75%] [G loss: 1.388365]\n",
      "epoch:39 step:36844 [D loss: 0.660452, acc.: 62.50%] [G loss: 1.152987]\n",
      "epoch:39 step:36845 [D loss: 0.466939, acc.: 79.69%] [G loss: 2.164037]\n",
      "epoch:39 step:36846 [D loss: 1.093396, acc.: 32.81%] [G loss: 1.408687]\n",
      "epoch:39 step:36847 [D loss: 0.536565, acc.: 73.44%] [G loss: 1.615263]\n",
      "epoch:39 step:36848 [D loss: 0.644905, acc.: 67.97%] [G loss: 1.189358]\n",
      "epoch:39 step:36849 [D loss: 0.724121, acc.: 55.47%] [G loss: 1.456558]\n",
      "epoch:39 step:36850 [D loss: 0.373550, acc.: 83.59%] [G loss: 1.412886]\n",
      "epoch:39 step:36851 [D loss: 0.443419, acc.: 79.69%] [G loss: 1.669396]\n",
      "epoch:39 step:36852 [D loss: 0.535982, acc.: 73.44%] [G loss: 1.343153]\n",
      "epoch:39 step:36853 [D loss: 0.522231, acc.: 72.66%] [G loss: 1.733424]\n",
      "epoch:39 step:36854 [D loss: 0.567828, acc.: 70.31%] [G loss: 0.992627]\n",
      "epoch:39 step:36855 [D loss: 0.446102, acc.: 84.38%] [G loss: 1.627833]\n",
      "epoch:39 step:36856 [D loss: 0.476240, acc.: 77.34%] [G loss: 1.195217]\n",
      "epoch:39 step:36857 [D loss: 0.503125, acc.: 74.22%] [G loss: 1.721196]\n",
      "epoch:39 step:36858 [D loss: 0.554235, acc.: 71.09%] [G loss: 1.445633]\n",
      "epoch:39 step:36859 [D loss: 0.471592, acc.: 78.12%] [G loss: 1.692239]\n",
      "epoch:39 step:36860 [D loss: 0.463552, acc.: 78.91%] [G loss: 2.084198]\n",
      "epoch:39 step:36861 [D loss: 0.454267, acc.: 78.91%] [G loss: 1.181612]\n",
      "epoch:39 step:36862 [D loss: 0.494401, acc.: 75.78%] [G loss: 1.812847]\n",
      "epoch:39 step:36863 [D loss: 0.644222, acc.: 65.62%] [G loss: 1.483387]\n",
      "epoch:39 step:36864 [D loss: 0.931012, acc.: 42.19%] [G loss: 1.512011]\n",
      "epoch:39 step:36865 [D loss: 0.375994, acc.: 89.06%] [G loss: 2.256933]\n",
      "epoch:39 step:36866 [D loss: 0.413531, acc.: 82.81%] [G loss: 1.712013]\n",
      "epoch:39 step:36867 [D loss: 0.412414, acc.: 82.81%] [G loss: 1.505457]\n",
      "epoch:39 step:36868 [D loss: 0.474001, acc.: 77.34%] [G loss: 1.562268]\n",
      "epoch:39 step:36869 [D loss: 0.814522, acc.: 47.66%] [G loss: 0.909324]\n",
      "epoch:39 step:36870 [D loss: 0.287007, acc.: 92.19%] [G loss: 2.539793]\n",
      "epoch:39 step:36871 [D loss: 0.406874, acc.: 80.47%] [G loss: 1.557500]\n",
      "epoch:39 step:36872 [D loss: 0.449609, acc.: 77.34%] [G loss: 1.227182]\n",
      "epoch:39 step:36873 [D loss: 0.483886, acc.: 76.56%] [G loss: 1.176299]\n",
      "epoch:39 step:36874 [D loss: 0.419004, acc.: 83.59%] [G loss: 1.602899]\n",
      "epoch:39 step:36875 [D loss: 0.465498, acc.: 80.47%] [G loss: 1.927730]\n",
      "epoch:39 step:36876 [D loss: 0.442470, acc.: 79.69%] [G loss: 2.180432]\n",
      "epoch:39 step:36877 [D loss: 0.533691, acc.: 74.22%] [G loss: 1.537845]\n",
      "epoch:39 step:36878 [D loss: 0.408335, acc.: 82.81%] [G loss: 1.551476]\n",
      "epoch:39 step:36879 [D loss: 0.390202, acc.: 82.81%] [G loss: 1.738769]\n",
      "epoch:39 step:36880 [D loss: 0.566000, acc.: 70.31%] [G loss: 1.383293]\n",
      "epoch:39 step:36881 [D loss: 0.535800, acc.: 74.22%] [G loss: 1.480234]\n",
      "epoch:39 step:36882 [D loss: 0.370504, acc.: 83.59%] [G loss: 1.628475]\n",
      "epoch:39 step:36883 [D loss: 0.531234, acc.: 71.88%] [G loss: 1.804693]\n",
      "epoch:39 step:36884 [D loss: 0.754794, acc.: 57.81%] [G loss: 1.426153]\n",
      "epoch:39 step:36885 [D loss: 0.429378, acc.: 81.25%] [G loss: 1.466107]\n",
      "epoch:39 step:36886 [D loss: 0.663849, acc.: 63.28%] [G loss: 1.365523]\n",
      "epoch:39 step:36887 [D loss: 0.360395, acc.: 82.03%] [G loss: 1.473656]\n",
      "epoch:39 step:36888 [D loss: 0.435075, acc.: 81.25%] [G loss: 1.812958]\n",
      "epoch:39 step:36889 [D loss: 0.471871, acc.: 75.78%] [G loss: 0.963472]\n",
      "epoch:39 step:36890 [D loss: 0.648879, acc.: 66.41%] [G loss: 1.400641]\n",
      "epoch:39 step:36891 [D loss: 0.552863, acc.: 72.66%] [G loss: 1.798866]\n",
      "epoch:39 step:36892 [D loss: 0.445569, acc.: 81.25%] [G loss: 2.032962]\n",
      "epoch:39 step:36893 [D loss: 0.468759, acc.: 77.34%] [G loss: 1.744190]\n",
      "epoch:39 step:36894 [D loss: 0.548725, acc.: 71.09%] [G loss: 1.578206]\n",
      "epoch:39 step:36895 [D loss: 0.598739, acc.: 68.75%] [G loss: 1.573600]\n",
      "epoch:39 step:36896 [D loss: 0.390786, acc.: 85.16%] [G loss: 1.700556]\n",
      "epoch:39 step:36897 [D loss: 0.564350, acc.: 71.88%] [G loss: 1.736485]\n",
      "epoch:39 step:36898 [D loss: 0.561348, acc.: 70.31%] [G loss: 1.548607]\n",
      "epoch:39 step:36899 [D loss: 0.458851, acc.: 79.69%] [G loss: 1.741990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36900 [D loss: 0.685848, acc.: 64.06%] [G loss: 1.544000]\n",
      "epoch:39 step:36901 [D loss: 0.580330, acc.: 69.53%] [G loss: 1.201898]\n",
      "epoch:39 step:36902 [D loss: 0.448459, acc.: 81.25%] [G loss: 1.198482]\n",
      "epoch:39 step:36903 [D loss: 0.559262, acc.: 67.19%] [G loss: 1.591637]\n",
      "epoch:39 step:36904 [D loss: 0.593404, acc.: 67.19%] [G loss: 2.174761]\n",
      "epoch:39 step:36905 [D loss: 0.425223, acc.: 83.59%] [G loss: 2.178509]\n",
      "epoch:39 step:36906 [D loss: 0.339890, acc.: 87.50%] [G loss: 1.948200]\n",
      "epoch:39 step:36907 [D loss: 0.534987, acc.: 75.00%] [G loss: 1.500133]\n",
      "epoch:39 step:36908 [D loss: 0.426838, acc.: 82.03%] [G loss: 1.995348]\n",
      "epoch:39 step:36909 [D loss: 0.500990, acc.: 78.12%] [G loss: 1.884348]\n",
      "epoch:39 step:36910 [D loss: 0.507804, acc.: 76.56%] [G loss: 1.760418]\n",
      "epoch:39 step:36911 [D loss: 0.555841, acc.: 74.22%] [G loss: 1.678677]\n",
      "epoch:39 step:36912 [D loss: 0.616040, acc.: 68.75%] [G loss: 1.487804]\n",
      "epoch:39 step:36913 [D loss: 0.539120, acc.: 71.88%] [G loss: 1.320913]\n",
      "epoch:39 step:36914 [D loss: 0.420660, acc.: 85.16%] [G loss: 1.954250]\n",
      "epoch:39 step:36915 [D loss: 0.702409, acc.: 61.72%] [G loss: 1.301435]\n",
      "epoch:39 step:36916 [D loss: 0.506917, acc.: 75.00%] [G loss: 1.600759]\n",
      "epoch:39 step:36917 [D loss: 0.735172, acc.: 58.59%] [G loss: 1.073459]\n",
      "epoch:39 step:36918 [D loss: 0.531264, acc.: 77.34%] [G loss: 1.808642]\n",
      "epoch:39 step:36919 [D loss: 0.583017, acc.: 72.66%] [G loss: 1.850683]\n",
      "epoch:39 step:36920 [D loss: 0.539005, acc.: 70.31%] [G loss: 1.773546]\n",
      "epoch:39 step:36921 [D loss: 0.373567, acc.: 87.50%] [G loss: 0.984902]\n",
      "epoch:39 step:36922 [D loss: 0.729258, acc.: 59.38%] [G loss: 1.079711]\n",
      "epoch:39 step:36923 [D loss: 0.544013, acc.: 71.88%] [G loss: 1.093963]\n",
      "epoch:39 step:36924 [D loss: 0.480424, acc.: 74.22%] [G loss: 1.442665]\n",
      "epoch:39 step:36925 [D loss: 0.411808, acc.: 82.03%] [G loss: 2.329980]\n",
      "epoch:39 step:36926 [D loss: 0.417116, acc.: 82.81%] [G loss: 1.557581]\n",
      "epoch:39 step:36927 [D loss: 0.454643, acc.: 80.47%] [G loss: 1.465602]\n",
      "epoch:39 step:36928 [D loss: 0.456558, acc.: 78.12%] [G loss: 1.515811]\n",
      "epoch:39 step:36929 [D loss: 0.566409, acc.: 65.62%] [G loss: 1.298982]\n",
      "epoch:39 step:36930 [D loss: 0.531173, acc.: 73.44%] [G loss: 1.465977]\n",
      "epoch:39 step:36931 [D loss: 0.651539, acc.: 61.72%] [G loss: 1.600486]\n",
      "epoch:39 step:36932 [D loss: 0.488919, acc.: 75.78%] [G loss: 2.039438]\n",
      "epoch:39 step:36933 [D loss: 0.677981, acc.: 61.72%] [G loss: 1.522818]\n",
      "epoch:39 step:36934 [D loss: 0.639909, acc.: 66.41%] [G loss: 1.182325]\n",
      "epoch:39 step:36935 [D loss: 0.512551, acc.: 77.34%] [G loss: 1.408818]\n",
      "epoch:39 step:36936 [D loss: 0.481688, acc.: 76.56%] [G loss: 1.101691]\n",
      "epoch:39 step:36937 [D loss: 0.610834, acc.: 69.53%] [G loss: 1.614121]\n",
      "epoch:39 step:36938 [D loss: 0.478598, acc.: 75.78%] [G loss: 1.203380]\n",
      "epoch:39 step:36939 [D loss: 0.556498, acc.: 70.31%] [G loss: 1.905616]\n",
      "epoch:39 step:36940 [D loss: 0.688628, acc.: 57.81%] [G loss: 1.482939]\n",
      "epoch:39 step:36941 [D loss: 0.553271, acc.: 72.66%] [G loss: 1.701890]\n",
      "epoch:39 step:36942 [D loss: 0.418760, acc.: 83.59%] [G loss: 1.647016]\n",
      "epoch:39 step:36943 [D loss: 0.474673, acc.: 77.34%] [G loss: 1.035024]\n",
      "epoch:39 step:36944 [D loss: 0.575272, acc.: 71.09%] [G loss: 1.585740]\n",
      "epoch:39 step:36945 [D loss: 0.345570, acc.: 88.28%] [G loss: 1.354897]\n",
      "epoch:39 step:36946 [D loss: 0.425163, acc.: 83.59%] [G loss: 1.584304]\n",
      "epoch:39 step:36947 [D loss: 0.493765, acc.: 77.34%] [G loss: 1.590238]\n",
      "epoch:39 step:36948 [D loss: 0.468197, acc.: 79.69%] [G loss: 1.578846]\n",
      "epoch:39 step:36949 [D loss: 0.431429, acc.: 81.25%] [G loss: 1.908654]\n",
      "epoch:39 step:36950 [D loss: 0.415322, acc.: 80.47%] [G loss: 1.579412]\n",
      "epoch:39 step:36951 [D loss: 0.357575, acc.: 86.72%] [G loss: 1.657806]\n",
      "epoch:39 step:36952 [D loss: 0.646368, acc.: 63.28%] [G loss: 1.213489]\n",
      "epoch:39 step:36953 [D loss: 0.445122, acc.: 81.25%] [G loss: 1.426293]\n",
      "epoch:39 step:36954 [D loss: 0.300724, acc.: 91.41%] [G loss: 1.960579]\n",
      "epoch:39 step:36955 [D loss: 0.605333, acc.: 65.62%] [G loss: 1.431253]\n",
      "epoch:39 step:36956 [D loss: 0.614270, acc.: 66.41%] [G loss: 0.774060]\n",
      "epoch:39 step:36957 [D loss: 0.591113, acc.: 64.84%] [G loss: 1.372141]\n",
      "epoch:39 step:36958 [D loss: 0.429820, acc.: 84.38%] [G loss: 1.250269]\n",
      "epoch:39 step:36959 [D loss: 0.417547, acc.: 82.81%] [G loss: 1.281046]\n",
      "epoch:39 step:36960 [D loss: 0.606565, acc.: 67.19%] [G loss: 1.338186]\n",
      "epoch:39 step:36961 [D loss: 0.488948, acc.: 77.34%] [G loss: 1.021184]\n",
      "epoch:39 step:36962 [D loss: 0.481197, acc.: 76.56%] [G loss: 1.400777]\n",
      "epoch:39 step:36963 [D loss: 0.377227, acc.: 85.94%] [G loss: 1.854686]\n",
      "epoch:39 step:36964 [D loss: 0.496899, acc.: 76.56%] [G loss: 1.465161]\n",
      "epoch:39 step:36965 [D loss: 0.569061, acc.: 74.22%] [G loss: 1.648827]\n",
      "epoch:39 step:36966 [D loss: 0.460592, acc.: 77.34%] [G loss: 1.653043]\n",
      "epoch:39 step:36967 [D loss: 0.560779, acc.: 72.66%] [G loss: 1.410301]\n",
      "epoch:39 step:36968 [D loss: 0.670251, acc.: 62.50%] [G loss: 1.232143]\n",
      "epoch:39 step:36969 [D loss: 0.449612, acc.: 83.59%] [G loss: 2.227200]\n",
      "epoch:39 step:36970 [D loss: 0.438348, acc.: 80.47%] [G loss: 1.576401]\n",
      "epoch:39 step:36971 [D loss: 0.389240, acc.: 87.50%] [G loss: 1.477483]\n",
      "epoch:39 step:36972 [D loss: 0.384412, acc.: 82.03%] [G loss: 1.200212]\n",
      "epoch:39 step:36973 [D loss: 0.706142, acc.: 63.28%] [G loss: 1.107099]\n",
      "epoch:39 step:36974 [D loss: 0.717142, acc.: 60.94%] [G loss: 2.157332]\n",
      "epoch:39 step:36975 [D loss: 0.523508, acc.: 71.88%] [G loss: 2.133285]\n",
      "epoch:39 step:36976 [D loss: 0.495964, acc.: 75.00%] [G loss: 1.949070]\n",
      "epoch:39 step:36977 [D loss: 0.667690, acc.: 59.38%] [G loss: 1.365603]\n",
      "epoch:39 step:36978 [D loss: 0.635641, acc.: 65.62%] [G loss: 1.618384]\n",
      "epoch:39 step:36979 [D loss: 0.423894, acc.: 81.25%] [G loss: 1.261677]\n",
      "epoch:39 step:36980 [D loss: 0.538633, acc.: 75.78%] [G loss: 0.993769]\n",
      "epoch:39 step:36981 [D loss: 0.413502, acc.: 87.50%] [G loss: 1.538624]\n",
      "epoch:39 step:36982 [D loss: 0.471405, acc.: 74.22%] [G loss: 1.727869]\n",
      "epoch:39 step:36983 [D loss: 0.479995, acc.: 76.56%] [G loss: 1.208492]\n",
      "epoch:39 step:36984 [D loss: 0.450070, acc.: 83.59%] [G loss: 2.088043]\n",
      "epoch:39 step:36985 [D loss: 0.501916, acc.: 74.22%] [G loss: 1.724981]\n",
      "epoch:39 step:36986 [D loss: 0.337028, acc.: 89.84%] [G loss: 1.750097]\n",
      "epoch:39 step:36987 [D loss: 0.471290, acc.: 78.91%] [G loss: 1.710358]\n",
      "epoch:39 step:36988 [D loss: 0.644560, acc.: 68.75%] [G loss: 1.422540]\n",
      "epoch:39 step:36989 [D loss: 0.557650, acc.: 73.44%] [G loss: 1.335318]\n",
      "epoch:39 step:36990 [D loss: 0.385867, acc.: 85.94%] [G loss: 1.445792]\n",
      "epoch:39 step:36991 [D loss: 0.523455, acc.: 71.88%] [G loss: 1.780887]\n",
      "epoch:39 step:36992 [D loss: 0.506568, acc.: 76.56%] [G loss: 1.213879]\n",
      "epoch:39 step:36993 [D loss: 0.768608, acc.: 53.12%] [G loss: 1.569131]\n",
      "epoch:39 step:36994 [D loss: 0.534221, acc.: 71.09%] [G loss: 1.741037]\n",
      "epoch:39 step:36995 [D loss: 0.527049, acc.: 73.44%] [G loss: 1.621018]\n",
      "epoch:39 step:36996 [D loss: 0.269530, acc.: 92.19%] [G loss: 1.920039]\n",
      "epoch:39 step:36997 [D loss: 0.655193, acc.: 64.84%] [G loss: 1.771527]\n",
      "epoch:39 step:36998 [D loss: 0.363022, acc.: 88.28%] [G loss: 1.698133]\n",
      "epoch:39 step:36999 [D loss: 0.406909, acc.: 81.25%] [G loss: 1.217794]\n",
      "epoch:39 step:37000 [D loss: 0.608934, acc.: 60.94%] [G loss: 1.543030]\n",
      "##############\n",
      "[2.71471855 2.21859858 2.03469607 2.72094477 0.78771839 6.12126867\n",
      " 2.11699244 2.61448869 3.87766352 4.83683636]\n",
      "##########\n",
      "epoch:39 step:37001 [D loss: 0.416447, acc.: 78.91%] [G loss: 1.767417]\n",
      "epoch:39 step:37002 [D loss: 0.379327, acc.: 87.50%] [G loss: 1.602323]\n",
      "epoch:39 step:37003 [D loss: 0.394101, acc.: 84.38%] [G loss: 1.741755]\n",
      "epoch:39 step:37004 [D loss: 0.418899, acc.: 82.81%] [G loss: 1.562424]\n",
      "epoch:39 step:37005 [D loss: 0.473286, acc.: 80.47%] [G loss: 1.859138]\n",
      "epoch:39 step:37006 [D loss: 0.766018, acc.: 59.38%] [G loss: 1.160278]\n",
      "epoch:39 step:37007 [D loss: 0.731016, acc.: 58.59%] [G loss: 1.487536]\n",
      "epoch:39 step:37008 [D loss: 0.458770, acc.: 81.25%] [G loss: 1.371353]\n",
      "epoch:39 step:37009 [D loss: 0.541820, acc.: 71.09%] [G loss: 1.188438]\n",
      "epoch:39 step:37010 [D loss: 0.569548, acc.: 68.75%] [G loss: 1.377522]\n",
      "epoch:39 step:37011 [D loss: 0.697788, acc.: 64.06%] [G loss: 2.044652]\n",
      "epoch:39 step:37012 [D loss: 0.548572, acc.: 72.66%] [G loss: 1.355671]\n",
      "epoch:39 step:37013 [D loss: 0.913326, acc.: 42.19%] [G loss: 1.440584]\n",
      "epoch:39 step:37014 [D loss: 0.607436, acc.: 69.53%] [G loss: 1.315816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37015 [D loss: 0.640383, acc.: 70.31%] [G loss: 1.561657]\n",
      "epoch:39 step:37016 [D loss: 0.395142, acc.: 85.94%] [G loss: 1.897826]\n",
      "epoch:39 step:37017 [D loss: 0.616814, acc.: 58.59%] [G loss: 1.673984]\n",
      "epoch:39 step:37018 [D loss: 0.371229, acc.: 89.84%] [G loss: 1.415376]\n",
      "epoch:39 step:37019 [D loss: 0.407767, acc.: 84.38%] [G loss: 1.546313]\n",
      "epoch:39 step:37020 [D loss: 0.765074, acc.: 53.12%] [G loss: 1.407418]\n",
      "epoch:39 step:37021 [D loss: 0.479014, acc.: 78.12%] [G loss: 2.183782]\n",
      "epoch:39 step:37022 [D loss: 0.400261, acc.: 85.16%] [G loss: 1.390155]\n",
      "epoch:39 step:37023 [D loss: 0.460024, acc.: 80.47%] [G loss: 1.650183]\n",
      "epoch:39 step:37024 [D loss: 0.599158, acc.: 67.97%] [G loss: 1.571798]\n",
      "epoch:39 step:37025 [D loss: 0.461354, acc.: 78.91%] [G loss: 0.969702]\n",
      "epoch:39 step:37026 [D loss: 0.630423, acc.: 59.38%] [G loss: 1.187783]\n",
      "epoch:39 step:37027 [D loss: 0.461679, acc.: 77.34%] [G loss: 1.431484]\n",
      "epoch:39 step:37028 [D loss: 0.333607, acc.: 87.50%] [G loss: 2.287793]\n",
      "epoch:39 step:37029 [D loss: 0.254429, acc.: 93.75%] [G loss: 1.400020]\n",
      "epoch:39 step:37030 [D loss: 0.348334, acc.: 88.28%] [G loss: 1.801795]\n",
      "epoch:39 step:37031 [D loss: 0.477435, acc.: 79.69%] [G loss: 1.196818]\n",
      "epoch:39 step:37032 [D loss: 0.442435, acc.: 78.91%] [G loss: 1.428729]\n",
      "epoch:39 step:37033 [D loss: 0.400675, acc.: 82.81%] [G loss: 1.248750]\n",
      "epoch:39 step:37034 [D loss: 0.509621, acc.: 79.69%] [G loss: 1.517481]\n",
      "epoch:39 step:37035 [D loss: 0.468992, acc.: 77.34%] [G loss: 1.691852]\n",
      "epoch:39 step:37036 [D loss: 0.421148, acc.: 82.03%] [G loss: 1.494914]\n",
      "epoch:39 step:37037 [D loss: 0.335644, acc.: 91.41%] [G loss: 1.425761]\n",
      "epoch:39 step:37038 [D loss: 0.440983, acc.: 82.03%] [G loss: 1.643960]\n",
      "epoch:39 step:37039 [D loss: 0.655679, acc.: 65.62%] [G loss: 1.922511]\n",
      "epoch:39 step:37040 [D loss: 0.484547, acc.: 78.91%] [G loss: 2.144336]\n",
      "epoch:39 step:37041 [D loss: 0.500532, acc.: 77.34%] [G loss: 1.689113]\n",
      "epoch:39 step:37042 [D loss: 0.394194, acc.: 82.81%] [G loss: 1.939502]\n",
      "epoch:39 step:37043 [D loss: 0.558375, acc.: 71.88%] [G loss: 2.131742]\n",
      "epoch:39 step:37044 [D loss: 0.424413, acc.: 79.69%] [G loss: 1.579642]\n",
      "epoch:39 step:37045 [D loss: 0.608340, acc.: 69.53%] [G loss: 2.022381]\n",
      "epoch:39 step:37046 [D loss: 0.642050, acc.: 63.28%] [G loss: 1.629834]\n",
      "epoch:39 step:37047 [D loss: 0.706069, acc.: 59.38%] [G loss: 2.221477]\n",
      "epoch:39 step:37048 [D loss: 0.335998, acc.: 89.84%] [G loss: 1.867400]\n",
      "epoch:39 step:37049 [D loss: 0.633570, acc.: 63.28%] [G loss: 1.574311]\n",
      "epoch:39 step:37050 [D loss: 0.833886, acc.: 46.09%] [G loss: 1.237163]\n",
      "epoch:39 step:37051 [D loss: 0.526977, acc.: 75.78%] [G loss: 1.373334]\n",
      "epoch:39 step:37052 [D loss: 0.594405, acc.: 67.19%] [G loss: 1.074775]\n",
      "epoch:39 step:37053 [D loss: 0.569305, acc.: 71.88%] [G loss: 1.403493]\n",
      "epoch:39 step:37054 [D loss: 0.397990, acc.: 82.03%] [G loss: 2.183270]\n",
      "epoch:39 step:37055 [D loss: 0.601623, acc.: 64.84%] [G loss: 1.541298]\n",
      "epoch:39 step:37056 [D loss: 0.374501, acc.: 85.94%] [G loss: 2.275357]\n",
      "epoch:39 step:37057 [D loss: 0.573350, acc.: 68.75%] [G loss: 1.691054]\n",
      "epoch:39 step:37058 [D loss: 0.728951, acc.: 54.69%] [G loss: 2.397647]\n",
      "epoch:39 step:37059 [D loss: 0.420239, acc.: 78.12%] [G loss: 1.446229]\n",
      "epoch:39 step:37060 [D loss: 0.442020, acc.: 81.25%] [G loss: 0.884015]\n",
      "epoch:39 step:37061 [D loss: 0.440615, acc.: 78.91%] [G loss: 1.543619]\n",
      "epoch:39 step:37062 [D loss: 0.430789, acc.: 82.81%] [G loss: 2.189151]\n",
      "epoch:39 step:37063 [D loss: 0.427199, acc.: 82.81%] [G loss: 1.542309]\n",
      "epoch:39 step:37064 [D loss: 0.641477, acc.: 64.84%] [G loss: 0.962850]\n",
      "epoch:39 step:37065 [D loss: 0.583866, acc.: 70.31%] [G loss: 1.542723]\n",
      "epoch:39 step:37066 [D loss: 0.596371, acc.: 70.31%] [G loss: 1.863894]\n",
      "epoch:39 step:37067 [D loss: 0.471280, acc.: 76.56%] [G loss: 1.433516]\n",
      "epoch:39 step:37068 [D loss: 0.352602, acc.: 88.28%] [G loss: 2.099458]\n",
      "epoch:39 step:37069 [D loss: 0.583921, acc.: 71.88%] [G loss: 1.940327]\n",
      "epoch:39 step:37070 [D loss: 0.426658, acc.: 81.25%] [G loss: 0.926073]\n",
      "epoch:39 step:37071 [D loss: 0.559717, acc.: 69.53%] [G loss: 1.201490]\n",
      "epoch:39 step:37072 [D loss: 0.514249, acc.: 71.09%] [G loss: 1.585437]\n",
      "epoch:39 step:37073 [D loss: 0.617512, acc.: 66.41%] [G loss: 1.349258]\n",
      "epoch:39 step:37074 [D loss: 0.642423, acc.: 70.31%] [G loss: 1.450369]\n",
      "epoch:39 step:37075 [D loss: 0.578304, acc.: 71.09%] [G loss: 1.291613]\n",
      "epoch:39 step:37076 [D loss: 0.454572, acc.: 79.69%] [G loss: 1.866045]\n",
      "epoch:39 step:37077 [D loss: 0.630633, acc.: 70.31%] [G loss: 1.531279]\n",
      "epoch:39 step:37078 [D loss: 0.522012, acc.: 75.78%] [G loss: 2.098296]\n",
      "epoch:39 step:37079 [D loss: 0.614716, acc.: 67.19%] [G loss: 1.781764]\n",
      "epoch:39 step:37080 [D loss: 0.473524, acc.: 75.00%] [G loss: 1.506453]\n",
      "epoch:39 step:37081 [D loss: 0.476184, acc.: 78.12%] [G loss: 0.768813]\n",
      "epoch:39 step:37082 [D loss: 0.680314, acc.: 66.41%] [G loss: 1.386961]\n",
      "epoch:39 step:37083 [D loss: 0.398235, acc.: 82.03%] [G loss: 1.353551]\n",
      "epoch:39 step:37084 [D loss: 0.569653, acc.: 70.31%] [G loss: 1.608889]\n",
      "epoch:39 step:37085 [D loss: 0.480076, acc.: 73.44%] [G loss: 1.878626]\n",
      "epoch:39 step:37086 [D loss: 0.317085, acc.: 89.84%] [G loss: 1.772250]\n",
      "epoch:39 step:37087 [D loss: 0.467647, acc.: 82.81%] [G loss: 1.590787]\n",
      "epoch:39 step:37088 [D loss: 0.475869, acc.: 78.91%] [G loss: 1.607481]\n",
      "epoch:39 step:37089 [D loss: 0.607092, acc.: 70.31%] [G loss: 1.863046]\n",
      "epoch:39 step:37090 [D loss: 0.478774, acc.: 78.91%] [G loss: 1.578940]\n",
      "epoch:39 step:37091 [D loss: 0.677042, acc.: 60.94%] [G loss: 1.773715]\n",
      "epoch:39 step:37092 [D loss: 0.512356, acc.: 74.22%] [G loss: 1.376745]\n",
      "epoch:39 step:37093 [D loss: 0.276206, acc.: 89.84%] [G loss: 1.313249]\n",
      "epoch:39 step:37094 [D loss: 0.407830, acc.: 86.72%] [G loss: 1.621783]\n",
      "epoch:39 step:37095 [D loss: 0.474162, acc.: 79.69%] [G loss: 1.507633]\n",
      "epoch:39 step:37096 [D loss: 0.480958, acc.: 80.47%] [G loss: 1.254830]\n",
      "epoch:39 step:37097 [D loss: 0.681957, acc.: 58.59%] [G loss: 1.578031]\n",
      "epoch:39 step:37098 [D loss: 0.689980, acc.: 57.81%] [G loss: 1.699453]\n",
      "epoch:39 step:37099 [D loss: 0.763099, acc.: 57.81%] [G loss: 1.201974]\n",
      "epoch:39 step:37100 [D loss: 0.582929, acc.: 70.31%] [G loss: 1.202963]\n",
      "epoch:39 step:37101 [D loss: 0.653898, acc.: 64.84%] [G loss: 1.487175]\n",
      "epoch:39 step:37102 [D loss: 0.646190, acc.: 67.97%] [G loss: 1.343717]\n",
      "epoch:39 step:37103 [D loss: 0.336860, acc.: 91.41%] [G loss: 1.321934]\n",
      "epoch:39 step:37104 [D loss: 0.697467, acc.: 58.59%] [G loss: 1.858848]\n",
      "epoch:39 step:37105 [D loss: 0.697830, acc.: 58.59%] [G loss: 1.185841]\n",
      "epoch:39 step:37106 [D loss: 0.325398, acc.: 86.72%] [G loss: 1.712014]\n",
      "epoch:39 step:37107 [D loss: 0.560306, acc.: 68.75%] [G loss: 1.975568]\n",
      "epoch:39 step:37108 [D loss: 0.539516, acc.: 71.88%] [G loss: 1.690332]\n",
      "epoch:39 step:37109 [D loss: 0.291891, acc.: 92.19%] [G loss: 1.618129]\n",
      "epoch:39 step:37110 [D loss: 0.573128, acc.: 73.44%] [G loss: 1.408440]\n",
      "epoch:39 step:37111 [D loss: 0.654040, acc.: 57.03%] [G loss: 0.917661]\n",
      "epoch:39 step:37112 [D loss: 0.289515, acc.: 90.62%] [G loss: 1.682655]\n",
      "epoch:39 step:37113 [D loss: 0.439781, acc.: 78.91%] [G loss: 1.463827]\n",
      "epoch:39 step:37114 [D loss: 0.594227, acc.: 66.41%] [G loss: 1.351228]\n",
      "epoch:39 step:37115 [D loss: 0.370595, acc.: 82.03%] [G loss: 1.851432]\n",
      "epoch:39 step:37116 [D loss: 0.717640, acc.: 60.16%] [G loss: 1.109896]\n",
      "epoch:39 step:37117 [D loss: 0.407532, acc.: 81.25%] [G loss: 1.322726]\n",
      "epoch:39 step:37118 [D loss: 0.472433, acc.: 80.47%] [G loss: 1.482104]\n",
      "epoch:39 step:37119 [D loss: 0.383808, acc.: 84.38%] [G loss: 1.714409]\n",
      "epoch:39 step:37120 [D loss: 0.416893, acc.: 82.81%] [G loss: 1.353243]\n",
      "epoch:39 step:37121 [D loss: 0.551617, acc.: 74.22%] [G loss: 1.081882]\n",
      "epoch:39 step:37122 [D loss: 0.474373, acc.: 72.66%] [G loss: 1.106919]\n",
      "epoch:39 step:37123 [D loss: 0.606815, acc.: 63.28%] [G loss: 1.058515]\n",
      "epoch:39 step:37124 [D loss: 0.526074, acc.: 78.12%] [G loss: 1.005419]\n",
      "epoch:39 step:37125 [D loss: 0.555042, acc.: 65.62%] [G loss: 1.539623]\n",
      "epoch:39 step:37126 [D loss: 0.440343, acc.: 82.03%] [G loss: 1.495707]\n",
      "epoch:39 step:37127 [D loss: 0.315332, acc.: 89.06%] [G loss: 2.059351]\n",
      "epoch:39 step:37128 [D loss: 0.638197, acc.: 65.62%] [G loss: 1.150589]\n",
      "epoch:39 step:37129 [D loss: 0.435282, acc.: 86.72%] [G loss: 1.379215]\n",
      "epoch:39 step:37130 [D loss: 0.605712, acc.: 68.75%] [G loss: 1.384832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37131 [D loss: 0.833844, acc.: 50.00%] [G loss: 1.735230]\n",
      "epoch:39 step:37132 [D loss: 0.424228, acc.: 79.69%] [G loss: 1.735153]\n",
      "epoch:39 step:37133 [D loss: 0.527990, acc.: 69.53%] [G loss: 1.305272]\n",
      "epoch:39 step:37134 [D loss: 0.594800, acc.: 66.41%] [G loss: 1.460369]\n",
      "epoch:39 step:37135 [D loss: 0.476764, acc.: 78.91%] [G loss: 1.556097]\n",
      "epoch:39 step:37136 [D loss: 0.665349, acc.: 64.06%] [G loss: 1.770960]\n",
      "epoch:39 step:37137 [D loss: 0.468082, acc.: 78.91%] [G loss: 1.458035]\n",
      "epoch:39 step:37138 [D loss: 0.405588, acc.: 86.72%] [G loss: 1.310797]\n",
      "epoch:39 step:37139 [D loss: 0.477354, acc.: 75.78%] [G loss: 1.664857]\n",
      "epoch:39 step:37140 [D loss: 0.587402, acc.: 70.31%] [G loss: 2.251184]\n",
      "epoch:39 step:37141 [D loss: 0.649052, acc.: 57.81%] [G loss: 1.461180]\n",
      "epoch:39 step:37142 [D loss: 0.416469, acc.: 81.25%] [G loss: 1.467730]\n",
      "epoch:39 step:37143 [D loss: 0.615809, acc.: 70.31%] [G loss: 1.337894]\n",
      "epoch:39 step:37144 [D loss: 0.398314, acc.: 84.38%] [G loss: 1.931572]\n",
      "epoch:39 step:37145 [D loss: 0.472718, acc.: 78.12%] [G loss: 0.947907]\n",
      "epoch:39 step:37146 [D loss: 0.349130, acc.: 87.50%] [G loss: 1.141051]\n",
      "epoch:39 step:37147 [D loss: 0.680951, acc.: 66.41%] [G loss: 1.100168]\n",
      "epoch:39 step:37148 [D loss: 0.643150, acc.: 61.72%] [G loss: 1.268131]\n",
      "epoch:39 step:37149 [D loss: 0.645330, acc.: 61.72%] [G loss: 0.982514]\n",
      "epoch:39 step:37150 [D loss: 0.458255, acc.: 78.12%] [G loss: 1.767216]\n",
      "epoch:39 step:37151 [D loss: 0.420299, acc.: 83.59%] [G loss: 1.051864]\n",
      "epoch:39 step:37152 [D loss: 0.495622, acc.: 76.56%] [G loss: 1.407183]\n",
      "epoch:39 step:37153 [D loss: 0.344124, acc.: 88.28%] [G loss: 1.845224]\n",
      "epoch:39 step:37154 [D loss: 0.550044, acc.: 72.66%] [G loss: 1.678747]\n",
      "epoch:39 step:37155 [D loss: 0.609902, acc.: 67.19%] [G loss: 1.333767]\n",
      "epoch:39 step:37156 [D loss: 0.455194, acc.: 78.12%] [G loss: 1.260900]\n",
      "epoch:39 step:37157 [D loss: 0.360746, acc.: 85.16%] [G loss: 1.498433]\n",
      "epoch:39 step:37158 [D loss: 0.397728, acc.: 85.94%] [G loss: 1.726656]\n",
      "epoch:39 step:37159 [D loss: 0.479875, acc.: 78.91%] [G loss: 1.738168]\n",
      "epoch:39 step:37160 [D loss: 0.362986, acc.: 85.16%] [G loss: 1.882484]\n",
      "epoch:39 step:37161 [D loss: 0.705022, acc.: 61.72%] [G loss: 1.134873]\n",
      "epoch:39 step:37162 [D loss: 0.523427, acc.: 71.88%] [G loss: 1.204815]\n",
      "epoch:39 step:37163 [D loss: 0.606897, acc.: 64.84%] [G loss: 2.003259]\n",
      "epoch:39 step:37164 [D loss: 0.573400, acc.: 73.44%] [G loss: 1.273489]\n",
      "epoch:39 step:37165 [D loss: 0.356026, acc.: 88.28%] [G loss: 1.456177]\n",
      "epoch:39 step:37166 [D loss: 0.595188, acc.: 65.62%] [G loss: 1.627278]\n",
      "epoch:39 step:37167 [D loss: 0.401818, acc.: 82.03%] [G loss: 1.750817]\n",
      "epoch:39 step:37168 [D loss: 0.730617, acc.: 57.81%] [G loss: 1.377495]\n",
      "epoch:39 step:37169 [D loss: 0.337234, acc.: 87.50%] [G loss: 1.975704]\n",
      "epoch:39 step:37170 [D loss: 0.496669, acc.: 73.44%] [G loss: 1.810899]\n",
      "epoch:39 step:37171 [D loss: 0.570051, acc.: 67.19%] [G loss: 1.375708]\n",
      "epoch:39 step:37172 [D loss: 0.538910, acc.: 78.91%] [G loss: 1.895220]\n",
      "epoch:39 step:37173 [D loss: 0.556191, acc.: 70.31%] [G loss: 1.426917]\n",
      "epoch:39 step:37174 [D loss: 0.525739, acc.: 71.88%] [G loss: 1.163961]\n",
      "epoch:39 step:37175 [D loss: 0.474214, acc.: 78.91%] [G loss: 2.208800]\n",
      "epoch:39 step:37176 [D loss: 0.401964, acc.: 82.03%] [G loss: 1.647658]\n",
      "epoch:39 step:37177 [D loss: 0.646352, acc.: 69.53%] [G loss: 1.429953]\n",
      "epoch:39 step:37178 [D loss: 0.441838, acc.: 81.25%] [G loss: 1.980031]\n",
      "epoch:39 step:37179 [D loss: 0.468352, acc.: 78.12%] [G loss: 1.976387]\n",
      "epoch:39 step:37180 [D loss: 0.545230, acc.: 71.09%] [G loss: 1.922635]\n",
      "epoch:39 step:37181 [D loss: 0.688069, acc.: 60.16%] [G loss: 1.288807]\n",
      "epoch:39 step:37182 [D loss: 0.622216, acc.: 67.19%] [G loss: 1.583141]\n",
      "epoch:39 step:37183 [D loss: 0.562338, acc.: 71.09%] [G loss: 1.559549]\n",
      "epoch:39 step:37184 [D loss: 0.657174, acc.: 64.06%] [G loss: 2.278720]\n",
      "epoch:39 step:37185 [D loss: 0.801218, acc.: 53.12%] [G loss: 1.315313]\n",
      "epoch:39 step:37186 [D loss: 0.642402, acc.: 67.19%] [G loss: 2.032255]\n",
      "epoch:39 step:37187 [D loss: 0.470865, acc.: 80.47%] [G loss: 1.635091]\n",
      "epoch:39 step:37188 [D loss: 0.516082, acc.: 74.22%] [G loss: 1.606376]\n",
      "epoch:39 step:37189 [D loss: 0.491109, acc.: 78.12%] [G loss: 1.726725]\n",
      "epoch:39 step:37190 [D loss: 0.729550, acc.: 59.38%] [G loss: 1.292201]\n",
      "epoch:39 step:37191 [D loss: 0.311755, acc.: 89.06%] [G loss: 1.289515]\n",
      "epoch:39 step:37192 [D loss: 0.378105, acc.: 85.94%] [G loss: 1.337519]\n",
      "epoch:39 step:37193 [D loss: 0.390851, acc.: 85.94%] [G loss: 1.375037]\n",
      "epoch:39 step:37194 [D loss: 0.219251, acc.: 92.19%] [G loss: 1.645817]\n",
      "epoch:39 step:37195 [D loss: 0.596638, acc.: 68.75%] [G loss: 1.424987]\n",
      "epoch:39 step:37196 [D loss: 0.381197, acc.: 84.38%] [G loss: 1.533015]\n",
      "epoch:39 step:37197 [D loss: 0.792906, acc.: 54.69%] [G loss: 2.074599]\n",
      "epoch:39 step:37198 [D loss: 0.782294, acc.: 58.59%] [G loss: 1.544874]\n",
      "epoch:39 step:37199 [D loss: 0.260742, acc.: 95.31%] [G loss: 1.863337]\n",
      "epoch:39 step:37200 [D loss: 0.455611, acc.: 79.69%] [G loss: 1.250547]\n",
      "##############\n",
      "[2.68521567 2.176929   1.8907909  2.95264638 1.19903603 5.90321305\n",
      " 2.23827968 2.83052931 3.94317666 5.58644042]\n",
      "##########\n",
      "epoch:39 step:37201 [D loss: 0.492281, acc.: 78.12%] [G loss: 1.817800]\n",
      "epoch:39 step:37202 [D loss: 0.285654, acc.: 89.84%] [G loss: 1.191906]\n",
      "epoch:39 step:37203 [D loss: 0.625759, acc.: 65.62%] [G loss: 1.757627]\n",
      "epoch:39 step:37204 [D loss: 0.635182, acc.: 67.97%] [G loss: 1.641768]\n",
      "epoch:39 step:37205 [D loss: 0.542108, acc.: 74.22%] [G loss: 1.237490]\n",
      "epoch:39 step:37206 [D loss: 0.471508, acc.: 78.91%] [G loss: 2.007215]\n",
      "epoch:39 step:37207 [D loss: 0.684453, acc.: 54.69%] [G loss: 1.345842]\n",
      "epoch:39 step:37208 [D loss: 0.493024, acc.: 71.88%] [G loss: 1.385512]\n",
      "epoch:39 step:37209 [D loss: 0.279804, acc.: 92.19%] [G loss: 1.614185]\n",
      "epoch:39 step:37210 [D loss: 0.475428, acc.: 78.12%] [G loss: 0.996737]\n",
      "epoch:39 step:37211 [D loss: 0.726875, acc.: 58.59%] [G loss: 1.275447]\n",
      "epoch:39 step:37212 [D loss: 0.426757, acc.: 82.81%] [G loss: 1.883901]\n",
      "epoch:39 step:37213 [D loss: 0.397047, acc.: 84.38%] [G loss: 1.777720]\n",
      "epoch:39 step:37214 [D loss: 0.546022, acc.: 73.44%] [G loss: 1.535985]\n",
      "epoch:39 step:37215 [D loss: 0.415452, acc.: 84.38%] [G loss: 1.178160]\n",
      "epoch:39 step:37216 [D loss: 0.587580, acc.: 67.19%] [G loss: 1.350163]\n",
      "epoch:39 step:37217 [D loss: 0.600327, acc.: 66.41%] [G loss: 1.610187]\n",
      "epoch:39 step:37218 [D loss: 0.466051, acc.: 78.91%] [G loss: 1.303322]\n",
      "epoch:39 step:37219 [D loss: 0.563184, acc.: 69.53%] [G loss: 1.704546]\n",
      "epoch:39 step:37220 [D loss: 0.438688, acc.: 76.56%] [G loss: 1.681696]\n",
      "epoch:39 step:37221 [D loss: 0.595317, acc.: 68.75%] [G loss: 1.113208]\n",
      "epoch:39 step:37222 [D loss: 0.525755, acc.: 78.12%] [G loss: 1.209036]\n",
      "epoch:39 step:37223 [D loss: 0.606858, acc.: 67.19%] [G loss: 2.020090]\n",
      "epoch:39 step:37224 [D loss: 0.451958, acc.: 78.12%] [G loss: 2.251111]\n",
      "epoch:39 step:37225 [D loss: 0.436942, acc.: 79.69%] [G loss: 1.567143]\n",
      "epoch:39 step:37226 [D loss: 0.503253, acc.: 75.00%] [G loss: 1.203887]\n",
      "epoch:39 step:37227 [D loss: 0.400880, acc.: 84.38%] [G loss: 1.664047]\n",
      "epoch:39 step:37228 [D loss: 0.637221, acc.: 67.97%] [G loss: 1.356261]\n",
      "epoch:39 step:37229 [D loss: 0.350969, acc.: 89.84%] [G loss: 1.153183]\n",
      "epoch:39 step:37230 [D loss: 0.429525, acc.: 83.59%] [G loss: 2.119738]\n",
      "epoch:39 step:37231 [D loss: 0.468486, acc.: 78.91%] [G loss: 1.931729]\n",
      "epoch:39 step:37232 [D loss: 0.746616, acc.: 55.47%] [G loss: 1.610554]\n",
      "epoch:39 step:37233 [D loss: 0.426242, acc.: 81.25%] [G loss: 1.691018]\n",
      "epoch:39 step:37234 [D loss: 0.498111, acc.: 73.44%] [G loss: 1.704394]\n",
      "epoch:39 step:37235 [D loss: 0.469040, acc.: 77.34%] [G loss: 1.519363]\n",
      "epoch:39 step:37236 [D loss: 0.611021, acc.: 61.72%] [G loss: 1.085298]\n",
      "epoch:39 step:37237 [D loss: 0.551005, acc.: 69.53%] [G loss: 1.399319]\n",
      "epoch:39 step:37238 [D loss: 0.572308, acc.: 67.19%] [G loss: 1.062822]\n",
      "epoch:39 step:37239 [D loss: 0.635449, acc.: 59.38%] [G loss: 2.121069]\n",
      "epoch:39 step:37240 [D loss: 0.417581, acc.: 79.69%] [G loss: 1.740603]\n",
      "epoch:39 step:37241 [D loss: 0.571207, acc.: 71.88%] [G loss: 1.962839]\n",
      "epoch:39 step:37242 [D loss: 0.590691, acc.: 71.09%] [G loss: 1.577668]\n",
      "epoch:39 step:37243 [D loss: 0.548104, acc.: 75.00%] [G loss: 1.252198]\n",
      "epoch:39 step:37244 [D loss: 0.461958, acc.: 78.12%] [G loss: 1.590155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37245 [D loss: 0.295822, acc.: 86.72%] [G loss: 1.932544]\n",
      "epoch:39 step:37246 [D loss: 0.592523, acc.: 71.88%] [G loss: 1.558492]\n",
      "epoch:39 step:37247 [D loss: 0.646823, acc.: 63.28%] [G loss: 1.471736]\n",
      "epoch:39 step:37248 [D loss: 0.378458, acc.: 82.81%] [G loss: 1.464034]\n",
      "epoch:39 step:37249 [D loss: 0.411010, acc.: 80.47%] [G loss: 1.562377]\n",
      "epoch:39 step:37250 [D loss: 0.391942, acc.: 83.59%] [G loss: 1.645561]\n",
      "epoch:39 step:37251 [D loss: 0.555283, acc.: 67.97%] [G loss: 2.166753]\n",
      "epoch:39 step:37252 [D loss: 0.670113, acc.: 60.94%] [G loss: 1.808284]\n",
      "epoch:39 step:37253 [D loss: 0.601612, acc.: 67.97%] [G loss: 1.328098]\n",
      "epoch:39 step:37254 [D loss: 0.420784, acc.: 82.03%] [G loss: 1.497415]\n",
      "epoch:39 step:37255 [D loss: 0.743246, acc.: 55.47%] [G loss: 1.544561]\n",
      "epoch:39 step:37256 [D loss: 0.611006, acc.: 67.19%] [G loss: 1.961744]\n",
      "epoch:39 step:37257 [D loss: 0.655334, acc.: 63.28%] [G loss: 1.932378]\n",
      "epoch:39 step:37258 [D loss: 0.629850, acc.: 67.97%] [G loss: 1.573828]\n",
      "epoch:39 step:37259 [D loss: 0.531317, acc.: 70.31%] [G loss: 1.838425]\n",
      "epoch:39 step:37260 [D loss: 0.606332, acc.: 64.84%] [G loss: 1.344758]\n",
      "epoch:39 step:37261 [D loss: 0.290687, acc.: 93.75%] [G loss: 2.373790]\n",
      "epoch:39 step:37262 [D loss: 0.617346, acc.: 63.28%] [G loss: 1.576244]\n",
      "epoch:39 step:37263 [D loss: 0.719094, acc.: 59.38%] [G loss: 1.147330]\n",
      "epoch:39 step:37264 [D loss: 0.333570, acc.: 90.62%] [G loss: 2.033984]\n",
      "epoch:39 step:37265 [D loss: 0.547240, acc.: 71.88%] [G loss: 1.785380]\n",
      "epoch:39 step:37266 [D loss: 0.441744, acc.: 82.03%] [G loss: 2.079358]\n",
      "epoch:39 step:37267 [D loss: 0.398683, acc.: 84.38%] [G loss: 1.906825]\n",
      "epoch:39 step:37268 [D loss: 0.479212, acc.: 78.91%] [G loss: 2.136536]\n",
      "epoch:39 step:37269 [D loss: 0.774369, acc.: 54.69%] [G loss: 1.771402]\n",
      "epoch:39 step:37270 [D loss: 0.357676, acc.: 87.50%] [G loss: 1.435164]\n",
      "epoch:39 step:37271 [D loss: 0.434219, acc.: 81.25%] [G loss: 1.509234]\n",
      "epoch:39 step:37272 [D loss: 0.565095, acc.: 73.44%] [G loss: 1.086067]\n",
      "epoch:39 step:37273 [D loss: 0.480066, acc.: 75.00%] [G loss: 1.625479]\n",
      "epoch:39 step:37274 [D loss: 0.606027, acc.: 67.97%] [G loss: 1.449668]\n",
      "epoch:39 step:37275 [D loss: 0.350043, acc.: 90.62%] [G loss: 2.064260]\n",
      "epoch:39 step:37276 [D loss: 0.488818, acc.: 75.78%] [G loss: 1.579883]\n",
      "epoch:39 step:37277 [D loss: 0.438243, acc.: 80.47%] [G loss: 1.947904]\n",
      "epoch:39 step:37278 [D loss: 0.447970, acc.: 81.25%] [G loss: 1.486508]\n",
      "epoch:39 step:37279 [D loss: 0.732273, acc.: 60.16%] [G loss: 1.863191]\n",
      "epoch:39 step:37280 [D loss: 0.450164, acc.: 82.81%] [G loss: 1.614601]\n",
      "epoch:39 step:37281 [D loss: 0.934578, acc.: 42.97%] [G loss: 1.401293]\n",
      "epoch:39 step:37282 [D loss: 0.366366, acc.: 85.94%] [G loss: 1.226802]\n",
      "epoch:39 step:37283 [D loss: 0.497614, acc.: 79.69%] [G loss: 1.451438]\n",
      "epoch:39 step:37284 [D loss: 0.616437, acc.: 64.84%] [G loss: 1.348505]\n",
      "epoch:39 step:37285 [D loss: 0.435619, acc.: 82.03%] [G loss: 1.674339]\n",
      "epoch:39 step:37286 [D loss: 0.473111, acc.: 81.25%] [G loss: 1.772079]\n",
      "epoch:39 step:37287 [D loss: 0.446204, acc.: 84.38%] [G loss: 1.707640]\n",
      "epoch:39 step:37288 [D loss: 0.440081, acc.: 76.56%] [G loss: 1.425554]\n",
      "epoch:39 step:37289 [D loss: 0.385771, acc.: 86.72%] [G loss: 1.603186]\n",
      "epoch:39 step:37290 [D loss: 0.458842, acc.: 78.91%] [G loss: 1.706072]\n",
      "epoch:39 step:37291 [D loss: 0.360745, acc.: 83.59%] [G loss: 2.038904]\n",
      "epoch:39 step:37292 [D loss: 0.509127, acc.: 70.31%] [G loss: 1.302268]\n",
      "epoch:39 step:37293 [D loss: 0.347879, acc.: 89.84%] [G loss: 1.973024]\n",
      "epoch:39 step:37294 [D loss: 0.592845, acc.: 68.75%] [G loss: 1.805944]\n",
      "epoch:39 step:37295 [D loss: 0.540533, acc.: 75.00%] [G loss: 1.866084]\n",
      "epoch:39 step:37296 [D loss: 0.461312, acc.: 80.47%] [G loss: 1.199253]\n",
      "epoch:39 step:37297 [D loss: 0.540211, acc.: 76.56%] [G loss: 1.689679]\n",
      "epoch:39 step:37298 [D loss: 0.602684, acc.: 68.75%] [G loss: 1.231303]\n",
      "epoch:39 step:37299 [D loss: 0.575088, acc.: 69.53%] [G loss: 1.547131]\n",
      "epoch:39 step:37300 [D loss: 0.511658, acc.: 75.78%] [G loss: 1.302445]\n",
      "epoch:39 step:37301 [D loss: 0.507631, acc.: 74.22%] [G loss: 1.769103]\n",
      "epoch:39 step:37302 [D loss: 0.409901, acc.: 84.38%] [G loss: 1.718686]\n",
      "epoch:39 step:37303 [D loss: 0.606449, acc.: 64.06%] [G loss: 1.186132]\n",
      "epoch:39 step:37304 [D loss: 0.601220, acc.: 69.53%] [G loss: 1.319154]\n",
      "epoch:39 step:37305 [D loss: 0.619444, acc.: 64.06%] [G loss: 1.281987]\n",
      "epoch:39 step:37306 [D loss: 0.248900, acc.: 96.09%] [G loss: 1.323287]\n",
      "epoch:39 step:37307 [D loss: 0.568045, acc.: 72.66%] [G loss: 1.326138]\n",
      "epoch:39 step:37308 [D loss: 0.541470, acc.: 69.53%] [G loss: 1.741397]\n",
      "epoch:39 step:37309 [D loss: 0.547906, acc.: 73.44%] [G loss: 1.556063]\n",
      "epoch:39 step:37310 [D loss: 0.439106, acc.: 80.47%] [G loss: 1.459158]\n",
      "epoch:39 step:37311 [D loss: 0.556935, acc.: 71.88%] [G loss: 1.624987]\n",
      "epoch:39 step:37312 [D loss: 0.376055, acc.: 87.50%] [G loss: 1.532479]\n",
      "epoch:39 step:37313 [D loss: 0.752342, acc.: 53.91%] [G loss: 1.447406]\n",
      "epoch:39 step:37314 [D loss: 0.617976, acc.: 63.28%] [G loss: 1.500378]\n",
      "epoch:39 step:37315 [D loss: 0.520407, acc.: 76.56%] [G loss: 1.509194]\n",
      "epoch:39 step:37316 [D loss: 0.550545, acc.: 71.88%] [G loss: 1.879863]\n",
      "epoch:39 step:37317 [D loss: 0.577364, acc.: 71.09%] [G loss: 2.096007]\n",
      "epoch:39 step:37318 [D loss: 0.686468, acc.: 66.41%] [G loss: 1.987157]\n",
      "epoch:39 step:37319 [D loss: 0.566850, acc.: 71.88%] [G loss: 2.224630]\n",
      "epoch:39 step:37320 [D loss: 0.569724, acc.: 71.88%] [G loss: 1.392563]\n",
      "epoch:39 step:37321 [D loss: 0.573858, acc.: 72.66%] [G loss: 1.245825]\n",
      "epoch:39 step:37322 [D loss: 0.423050, acc.: 82.03%] [G loss: 1.460435]\n",
      "epoch:39 step:37323 [D loss: 0.685544, acc.: 65.62%] [G loss: 0.983092]\n",
      "epoch:39 step:37324 [D loss: 0.352038, acc.: 87.50%] [G loss: 1.928668]\n",
      "epoch:39 step:37325 [D loss: 0.602674, acc.: 67.97%] [G loss: 1.628178]\n",
      "epoch:39 step:37326 [D loss: 0.499224, acc.: 75.00%] [G loss: 1.479959]\n",
      "epoch:39 step:37327 [D loss: 0.292291, acc.: 91.41%] [G loss: 2.410715]\n",
      "epoch:39 step:37328 [D loss: 0.318010, acc.: 90.62%] [G loss: 1.815389]\n",
      "epoch:39 step:37329 [D loss: 0.575867, acc.: 71.09%] [G loss: 1.563292]\n",
      "epoch:39 step:37330 [D loss: 0.468509, acc.: 77.34%] [G loss: 1.823610]\n",
      "epoch:39 step:37331 [D loss: 0.608478, acc.: 71.88%] [G loss: 1.768551]\n",
      "epoch:39 step:37332 [D loss: 0.580381, acc.: 69.53%] [G loss: 1.264505]\n",
      "epoch:39 step:37333 [D loss: 0.592914, acc.: 68.75%] [G loss: 1.535022]\n",
      "epoch:39 step:37334 [D loss: 0.685093, acc.: 64.84%] [G loss: 1.717906]\n",
      "epoch:39 step:37335 [D loss: 0.494260, acc.: 75.78%] [G loss: 1.757934]\n",
      "epoch:39 step:37336 [D loss: 0.480757, acc.: 79.69%] [G loss: 1.166071]\n",
      "epoch:39 step:37337 [D loss: 0.336859, acc.: 91.41%] [G loss: 2.313869]\n",
      "epoch:39 step:37338 [D loss: 0.462114, acc.: 78.91%] [G loss: 1.521301]\n",
      "epoch:39 step:37339 [D loss: 0.566598, acc.: 74.22%] [G loss: 1.365060]\n",
      "epoch:39 step:37340 [D loss: 0.433096, acc.: 82.03%] [G loss: 1.948201]\n",
      "epoch:39 step:37341 [D loss: 0.466796, acc.: 76.56%] [G loss: 1.816933]\n",
      "epoch:39 step:37342 [D loss: 0.533674, acc.: 72.66%] [G loss: 1.789110]\n",
      "epoch:39 step:37343 [D loss: 0.479519, acc.: 75.78%] [G loss: 1.673302]\n",
      "epoch:39 step:37344 [D loss: 0.650813, acc.: 64.84%] [G loss: 1.866695]\n",
      "epoch:39 step:37345 [D loss: 0.409086, acc.: 84.38%] [G loss: 1.572861]\n",
      "epoch:39 step:37346 [D loss: 0.626907, acc.: 64.06%] [G loss: 1.195228]\n",
      "epoch:39 step:37347 [D loss: 0.445292, acc.: 82.81%] [G loss: 1.151520]\n",
      "epoch:39 step:37348 [D loss: 0.696531, acc.: 62.50%] [G loss: 0.941185]\n",
      "epoch:39 step:37349 [D loss: 0.541124, acc.: 74.22%] [G loss: 1.916241]\n",
      "epoch:39 step:37350 [D loss: 0.755335, acc.: 60.94%] [G loss: 1.995663]\n",
      "epoch:39 step:37351 [D loss: 0.425139, acc.: 82.81%] [G loss: 2.388193]\n",
      "epoch:39 step:37352 [D loss: 0.528463, acc.: 71.09%] [G loss: 1.895330]\n",
      "epoch:39 step:37353 [D loss: 0.419036, acc.: 80.47%] [G loss: 2.233235]\n",
      "epoch:39 step:37354 [D loss: 0.577569, acc.: 69.53%] [G loss: 1.734839]\n",
      "epoch:39 step:37355 [D loss: 0.621994, acc.: 68.75%] [G loss: 1.584704]\n",
      "epoch:39 step:37356 [D loss: 0.634225, acc.: 63.28%] [G loss: 1.093999]\n",
      "epoch:39 step:37357 [D loss: 0.603331, acc.: 63.28%] [G loss: 1.575808]\n",
      "epoch:39 step:37358 [D loss: 0.475202, acc.: 76.56%] [G loss: 2.199090]\n",
      "epoch:39 step:37359 [D loss: 0.524746, acc.: 76.56%] [G loss: 2.117006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37360 [D loss: 0.565242, acc.: 68.75%] [G loss: 1.883466]\n",
      "epoch:39 step:37361 [D loss: 0.468737, acc.: 77.34%] [G loss: 1.365231]\n",
      "epoch:39 step:37362 [D loss: 0.449409, acc.: 82.03%] [G loss: 1.032136]\n",
      "epoch:39 step:37363 [D loss: 0.661996, acc.: 61.72%] [G loss: 1.411644]\n",
      "epoch:39 step:37364 [D loss: 0.603538, acc.: 64.06%] [G loss: 1.124521]\n",
      "epoch:39 step:37365 [D loss: 0.621184, acc.: 67.19%] [G loss: 1.267666]\n",
      "epoch:39 step:37366 [D loss: 0.439787, acc.: 78.91%] [G loss: 2.184372]\n",
      "epoch:39 step:37367 [D loss: 0.472971, acc.: 78.91%] [G loss: 1.758645]\n",
      "epoch:39 step:37368 [D loss: 0.423298, acc.: 83.59%] [G loss: 1.597846]\n",
      "epoch:39 step:37369 [D loss: 0.477967, acc.: 76.56%] [G loss: 2.008663]\n",
      "epoch:39 step:37370 [D loss: 0.449578, acc.: 78.12%] [G loss: 1.768026]\n",
      "epoch:39 step:37371 [D loss: 0.797803, acc.: 53.91%] [G loss: 1.815733]\n",
      "epoch:39 step:37372 [D loss: 0.759980, acc.: 54.69%] [G loss: 1.228346]\n",
      "epoch:39 step:37373 [D loss: 0.317719, acc.: 92.19%] [G loss: 1.320811]\n",
      "epoch:39 step:37374 [D loss: 0.464716, acc.: 77.34%] [G loss: 1.208934]\n",
      "epoch:39 step:37375 [D loss: 0.466638, acc.: 75.00%] [G loss: 1.371308]\n",
      "epoch:39 step:37376 [D loss: 0.484111, acc.: 76.56%] [G loss: 1.386948]\n",
      "epoch:39 step:37377 [D loss: 0.436159, acc.: 80.47%] [G loss: 1.696156]\n",
      "epoch:39 step:37378 [D loss: 0.429800, acc.: 81.25%] [G loss: 1.356741]\n",
      "epoch:39 step:37379 [D loss: 0.529127, acc.: 76.56%] [G loss: 1.218408]\n",
      "epoch:39 step:37380 [D loss: 0.655401, acc.: 66.41%] [G loss: 1.117696]\n",
      "epoch:39 step:37381 [D loss: 0.402481, acc.: 87.50%] [G loss: 1.355484]\n",
      "epoch:39 step:37382 [D loss: 0.596233, acc.: 68.75%] [G loss: 1.534298]\n",
      "epoch:39 step:37383 [D loss: 0.482390, acc.: 78.12%] [G loss: 1.881743]\n",
      "epoch:39 step:37384 [D loss: 0.606725, acc.: 70.31%] [G loss: 1.454769]\n",
      "epoch:39 step:37385 [D loss: 0.603194, acc.: 67.19%] [G loss: 1.573522]\n",
      "epoch:39 step:37386 [D loss: 0.405925, acc.: 85.16%] [G loss: 1.657764]\n",
      "epoch:39 step:37387 [D loss: 0.882122, acc.: 50.78%] [G loss: 1.679250]\n",
      "epoch:39 step:37388 [D loss: 0.454354, acc.: 78.91%] [G loss: 1.612826]\n",
      "epoch:39 step:37389 [D loss: 0.699929, acc.: 59.38%] [G loss: 1.436301]\n",
      "epoch:39 step:37390 [D loss: 0.341249, acc.: 88.28%] [G loss: 1.932573]\n",
      "epoch:39 step:37391 [D loss: 0.566903, acc.: 71.09%] [G loss: 1.278023]\n",
      "epoch:39 step:37392 [D loss: 0.414579, acc.: 82.81%] [G loss: 1.962215]\n",
      "epoch:39 step:37393 [D loss: 0.328312, acc.: 87.50%] [G loss: 1.945477]\n",
      "epoch:39 step:37394 [D loss: 0.455330, acc.: 77.34%] [G loss: 1.879836]\n",
      "epoch:39 step:37395 [D loss: 0.272063, acc.: 91.41%] [G loss: 2.735831]\n",
      "epoch:39 step:37396 [D loss: 0.440697, acc.: 80.47%] [G loss: 1.748093]\n",
      "epoch:39 step:37397 [D loss: 0.589928, acc.: 67.19%] [G loss: 1.678259]\n",
      "epoch:39 step:37398 [D loss: 0.516555, acc.: 71.88%] [G loss: 1.466116]\n",
      "epoch:39 step:37399 [D loss: 0.394432, acc.: 82.03%] [G loss: 2.000803]\n",
      "epoch:39 step:37400 [D loss: 0.372384, acc.: 85.16%] [G loss: 1.882453]\n",
      "##############\n",
      "[2.83620546 2.03170009 1.9012286  2.59331756 0.84268459 5.63513944\n",
      " 2.39522102 2.4801264  3.98936612 5.42866113]\n",
      "##########\n",
      "epoch:39 step:37401 [D loss: 0.413598, acc.: 80.47%] [G loss: 1.778723]\n",
      "epoch:39 step:37402 [D loss: 0.581680, acc.: 72.66%] [G loss: 2.292998]\n",
      "epoch:39 step:37403 [D loss: 0.680866, acc.: 62.50%] [G loss: 1.326854]\n",
      "epoch:39 step:37404 [D loss: 0.397316, acc.: 81.25%] [G loss: 2.113221]\n",
      "epoch:39 step:37405 [D loss: 0.707821, acc.: 59.38%] [G loss: 1.557743]\n",
      "epoch:39 step:37406 [D loss: 0.442202, acc.: 79.69%] [G loss: 1.511882]\n",
      "epoch:39 step:37407 [D loss: 0.356750, acc.: 87.50%] [G loss: 1.523505]\n",
      "epoch:39 step:37408 [D loss: 0.459922, acc.: 80.47%] [G loss: 1.346570]\n",
      "epoch:39 step:37409 [D loss: 0.446259, acc.: 79.69%] [G loss: 1.661606]\n",
      "epoch:39 step:37410 [D loss: 0.454494, acc.: 78.91%] [G loss: 1.578562]\n",
      "epoch:39 step:37411 [D loss: 0.416541, acc.: 82.03%] [G loss: 1.754941]\n",
      "epoch:39 step:37412 [D loss: 0.460114, acc.: 75.00%] [G loss: 1.717782]\n",
      "epoch:39 step:37413 [D loss: 0.457580, acc.: 81.25%] [G loss: 1.868932]\n",
      "epoch:39 step:37414 [D loss: 0.579591, acc.: 73.44%] [G loss: 1.079298]\n",
      "epoch:39 step:37415 [D loss: 0.449303, acc.: 79.69%] [G loss: 1.472964]\n",
      "epoch:39 step:37416 [D loss: 0.308792, acc.: 91.41%] [G loss: 1.712068]\n",
      "epoch:39 step:37417 [D loss: 0.533229, acc.: 69.53%] [G loss: 1.424371]\n",
      "epoch:39 step:37418 [D loss: 0.506878, acc.: 76.56%] [G loss: 1.129791]\n",
      "epoch:39 step:37419 [D loss: 0.361274, acc.: 91.41%] [G loss: 1.418571]\n",
      "epoch:39 step:37420 [D loss: 0.505357, acc.: 73.44%] [G loss: 1.184829]\n",
      "epoch:39 step:37421 [D loss: 0.426657, acc.: 82.03%] [G loss: 1.665086]\n",
      "epoch:39 step:37422 [D loss: 0.708040, acc.: 63.28%] [G loss: 2.242915]\n",
      "epoch:39 step:37423 [D loss: 0.612389, acc.: 64.84%] [G loss: 1.339689]\n",
      "epoch:39 step:37424 [D loss: 0.593031, acc.: 70.31%] [G loss: 2.035521]\n",
      "epoch:39 step:37425 [D loss: 0.382265, acc.: 85.16%] [G loss: 2.443691]\n",
      "epoch:39 step:37426 [D loss: 0.556909, acc.: 71.88%] [G loss: 1.178113]\n",
      "epoch:39 step:37427 [D loss: 0.401510, acc.: 81.25%] [G loss: 1.539676]\n",
      "epoch:39 step:37428 [D loss: 0.785045, acc.: 57.81%] [G loss: 0.928681]\n",
      "epoch:39 step:37429 [D loss: 0.601198, acc.: 68.75%] [G loss: 2.115545]\n",
      "epoch:39 step:37430 [D loss: 0.441848, acc.: 78.12%] [G loss: 2.292163]\n",
      "epoch:39 step:37431 [D loss: 0.516225, acc.: 75.00%] [G loss: 2.073181]\n",
      "epoch:39 step:37432 [D loss: 0.739209, acc.: 58.59%] [G loss: 1.885884]\n",
      "epoch:39 step:37433 [D loss: 0.538155, acc.: 77.34%] [G loss: 1.097508]\n",
      "epoch:39 step:37434 [D loss: 0.444378, acc.: 81.25%] [G loss: 1.929471]\n",
      "epoch:39 step:37435 [D loss: 0.444172, acc.: 78.91%] [G loss: 1.403199]\n",
      "epoch:39 step:37436 [D loss: 0.454225, acc.: 83.59%] [G loss: 1.745046]\n",
      "epoch:39 step:37437 [D loss: 0.588934, acc.: 67.19%] [G loss: 1.922444]\n",
      "epoch:39 step:37438 [D loss: 0.431326, acc.: 82.03%] [G loss: 1.490076]\n",
      "epoch:39 step:37439 [D loss: 0.494613, acc.: 72.66%] [G loss: 1.837018]\n",
      "epoch:39 step:37440 [D loss: 0.477457, acc.: 77.34%] [G loss: 1.926334]\n",
      "epoch:39 step:37441 [D loss: 0.402030, acc.: 81.25%] [G loss: 1.465142]\n",
      "epoch:39 step:37442 [D loss: 0.600531, acc.: 67.97%] [G loss: 1.381836]\n",
      "epoch:39 step:37443 [D loss: 0.565952, acc.: 68.75%] [G loss: 1.324840]\n",
      "epoch:39 step:37444 [D loss: 0.676914, acc.: 62.50%] [G loss: 1.500631]\n",
      "epoch:39 step:37445 [D loss: 0.431186, acc.: 79.69%] [G loss: 0.847429]\n",
      "epoch:39 step:37446 [D loss: 0.573906, acc.: 72.66%] [G loss: 1.155288]\n",
      "epoch:39 step:37447 [D loss: 0.493337, acc.: 77.34%] [G loss: 1.297433]\n",
      "epoch:39 step:37448 [D loss: 0.734956, acc.: 59.38%] [G loss: 1.361238]\n",
      "epoch:39 step:37449 [D loss: 0.624045, acc.: 66.41%] [G loss: 1.492850]\n",
      "epoch:39 step:37450 [D loss: 0.387244, acc.: 86.72%] [G loss: 1.834339]\n",
      "epoch:39 step:37451 [D loss: 0.664706, acc.: 67.19%] [G loss: 1.935565]\n",
      "epoch:39 step:37452 [D loss: 0.474008, acc.: 80.47%] [G loss: 1.665987]\n",
      "epoch:39 step:37453 [D loss: 0.529824, acc.: 72.66%] [G loss: 1.950887]\n",
      "epoch:39 step:37454 [D loss: 0.711864, acc.: 54.69%] [G loss: 2.102866]\n",
      "epoch:39 step:37455 [D loss: 0.701471, acc.: 58.59%] [G loss: 1.475424]\n",
      "epoch:39 step:37456 [D loss: 0.505672, acc.: 77.34%] [G loss: 1.840942]\n",
      "epoch:39 step:37457 [D loss: 0.566910, acc.: 71.09%] [G loss: 1.188767]\n",
      "epoch:39 step:37458 [D loss: 0.557171, acc.: 69.53%] [G loss: 1.701493]\n",
      "epoch:39 step:37459 [D loss: 0.445704, acc.: 79.69%] [G loss: 1.724960]\n",
      "epoch:39 step:37460 [D loss: 0.471439, acc.: 76.56%] [G loss: 1.318968]\n",
      "epoch:39 step:37461 [D loss: 0.596129, acc.: 71.09%] [G loss: 1.622650]\n",
      "epoch:39 step:37462 [D loss: 0.584049, acc.: 66.41%] [G loss: 1.658147]\n",
      "epoch:39 step:37463 [D loss: 0.435670, acc.: 79.69%] [G loss: 2.045441]\n",
      "epoch:39 step:37464 [D loss: 0.850773, acc.: 48.44%] [G loss: 0.928270]\n",
      "epoch:39 step:37465 [D loss: 0.575901, acc.: 68.75%] [G loss: 1.792050]\n",
      "epoch:39 step:37466 [D loss: 0.387078, acc.: 85.94%] [G loss: 1.593455]\n",
      "epoch:39 step:37467 [D loss: 0.527201, acc.: 76.56%] [G loss: 1.426557]\n",
      "epoch:39 step:37468 [D loss: 0.663443, acc.: 65.62%] [G loss: 1.394866]\n",
      "epoch:39 step:37469 [D loss: 0.555686, acc.: 69.53%] [G loss: 2.056265]\n",
      "epoch:39 step:37470 [D loss: 0.795999, acc.: 47.66%] [G loss: 1.448600]\n",
      "epoch:39 step:37471 [D loss: 0.390332, acc.: 83.59%] [G loss: 1.325761]\n",
      "epoch:39 step:37472 [D loss: 0.521490, acc.: 70.31%] [G loss: 1.688795]\n",
      "epoch:39 step:37473 [D loss: 0.278682, acc.: 93.75%] [G loss: 1.518468]\n",
      "epoch:39 step:37474 [D loss: 0.501864, acc.: 71.88%] [G loss: 1.386119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37475 [D loss: 0.338060, acc.: 88.28%] [G loss: 1.732601]\n",
      "epoch:39 step:37476 [D loss: 0.551471, acc.: 71.09%] [G loss: 1.335128]\n",
      "epoch:39 step:37477 [D loss: 0.562218, acc.: 75.78%] [G loss: 1.649644]\n",
      "epoch:39 step:37478 [D loss: 0.425856, acc.: 80.47%] [G loss: 1.436218]\n",
      "epoch:39 step:37479 [D loss: 0.480611, acc.: 76.56%] [G loss: 1.705756]\n",
      "epoch:39 step:37480 [D loss: 0.584994, acc.: 73.44%] [G loss: 1.306676]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('dcgan')):\n",
    "    os.mkdir('saved_models_{}'.format('dcgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('dcgan'), mode='w')\n",
    "import cv2\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "        # y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (\n",
    "                epoch, global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % save_interval == 0:\n",
    "                    s = self.sample_images(global_step, X_test,y_test, sampleSize)\n",
    "    def sample_images(self, epoch,x_test,y_test,sample_num):\n",
    "        r, c = sample_num//10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=40, batch_size=64, save_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
