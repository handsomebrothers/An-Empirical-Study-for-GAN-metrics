{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import util\n",
    "import tensorflow as tf\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "def EuclideanDistances(A, B):\n",
    "\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(),ED.shape[0]*ED.shape[1])\n",
    "def cal_distance_image_real(images,labels):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "def cal_distance_image_fake(images):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    labels = tf.Session().run(tf.argmax(y_logits, 1))\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D loss: 0.973047, acc: 38.28%] [G loss: 4.520699]\n",
      "epoch:0 step:2 [D loss: 0.382832, acc: 78.12%] [G loss: 5.751928]\n",
      "epoch:0 step:3 [D loss: 0.168323, acc: 96.88%] [G loss: 5.958460]\n",
      "epoch:0 step:4 [D loss: 0.130671, acc: 98.44%] [G loss: 7.055146]\n",
      "epoch:0 step:5 [D loss: 0.059513, acc: 100.00%] [G loss: 7.713448]\n",
      "epoch:0 step:6 [D loss: 0.056840, acc: 100.00%] [G loss: 9.108557]\n",
      "epoch:0 step:7 [D loss: 0.022003, acc: 100.00%] [G loss: 9.786201]\n",
      "epoch:0 step:8 [D loss: 0.031620, acc: 100.00%] [G loss: 9.486759]\n",
      "epoch:0 step:9 [D loss: 0.022756, acc: 100.00%] [G loss: 10.570301]\n",
      "epoch:0 step:10 [D loss: 0.016624, acc: 100.00%] [G loss: 11.038242]\n",
      "epoch:0 step:11 [D loss: 0.010671, acc: 100.00%] [G loss: 11.177032]\n",
      "epoch:0 step:12 [D loss: 0.014732, acc: 100.00%] [G loss: 11.370243]\n",
      "epoch:0 step:13 [D loss: 0.019275, acc: 100.00%] [G loss: 12.202085]\n",
      "epoch:0 step:14 [D loss: 0.012388, acc: 100.00%] [G loss: 13.099105]\n",
      "epoch:0 step:15 [D loss: 0.008708, acc: 100.00%] [G loss: 12.837545]\n",
      "epoch:0 step:16 [D loss: 0.008914, acc: 100.00%] [G loss: 13.473640]\n",
      "epoch:0 step:17 [D loss: 0.010524, acc: 100.00%] [G loss: 13.624830]\n",
      "epoch:0 step:18 [D loss: 0.005849, acc: 100.00%] [G loss: 13.386875]\n",
      "epoch:0 step:19 [D loss: 0.035307, acc: 98.44%] [G loss: 15.027649]\n",
      "epoch:0 step:20 [D loss: 0.021578, acc: 99.22%] [G loss: 16.303038]\n",
      "epoch:0 step:21 [D loss: 0.009685, acc: 100.00%] [G loss: 16.423691]\n",
      "epoch:0 step:22 [D loss: 0.008756, acc: 100.00%] [G loss: 16.312683]\n",
      "epoch:0 step:23 [D loss: 0.013560, acc: 100.00%] [G loss: 16.714451]\n",
      "epoch:0 step:24 [D loss: 0.005423, acc: 100.00%] [G loss: 16.534378]\n",
      "epoch:0 step:25 [D loss: 0.005817, acc: 100.00%] [G loss: 16.546516]\n",
      "epoch:0 step:26 [D loss: 0.007586, acc: 100.00%] [G loss: 17.751097]\n",
      "epoch:0 step:27 [D loss: 0.004597, acc: 100.00%] [G loss: 17.585533]\n",
      "epoch:0 step:28 [D loss: 0.013577, acc: 99.22%] [G loss: 18.164381]\n",
      "epoch:0 step:29 [D loss: 0.006817, acc: 100.00%] [G loss: 18.415094]\n",
      "epoch:0 step:30 [D loss: 0.021905, acc: 99.22%] [G loss: 19.788799]\n",
      "epoch:0 step:31 [D loss: 0.038363, acc: 99.22%] [G loss: 20.545279]\n",
      "epoch:0 step:32 [D loss: 0.014894, acc: 100.00%] [G loss: 19.944256]\n",
      "epoch:0 step:33 [D loss: 0.014816, acc: 100.00%] [G loss: 20.405930]\n",
      "epoch:0 step:34 [D loss: 0.004993, acc: 100.00%] [G loss: 20.476631]\n",
      "epoch:0 step:35 [D loss: 0.005940, acc: 100.00%] [G loss: 21.329712]\n",
      "epoch:0 step:36 [D loss: 0.005277, acc: 100.00%] [G loss: 20.471710]\n",
      "epoch:0 step:37 [D loss: 0.013556, acc: 99.22%] [G loss: 20.695738]\n",
      "epoch:0 step:38 [D loss: 0.012014, acc: 99.22%] [G loss: 21.097359]\n",
      "epoch:0 step:39 [D loss: 0.004283, acc: 100.00%] [G loss: 21.874575]\n",
      "epoch:0 step:40 [D loss: 0.004047, acc: 100.00%] [G loss: 21.171427]\n",
      "epoch:0 step:41 [D loss: 0.022060, acc: 99.22%] [G loss: 21.563637]\n",
      "epoch:0 step:42 [D loss: 0.010506, acc: 100.00%] [G loss: 21.403482]\n",
      "epoch:0 step:43 [D loss: 0.036643, acc: 99.22%] [G loss: 21.153435]\n",
      "epoch:0 step:44 [D loss: 0.022712, acc: 100.00%] [G loss: 21.346693]\n",
      "epoch:0 step:45 [D loss: 0.018595, acc: 99.22%] [G loss: 22.427071]\n",
      "epoch:0 step:46 [D loss: 0.010053, acc: 100.00%] [G loss: 22.124441]\n",
      "epoch:0 step:47 [D loss: 0.004759, acc: 100.00%] [G loss: 22.724758]\n",
      "epoch:0 step:48 [D loss: 0.005440, acc: 100.00%] [G loss: 22.450573]\n",
      "epoch:0 step:49 [D loss: 0.128117, acc: 99.22%] [G loss: 22.043522]\n",
      "epoch:0 step:50 [D loss: 0.002821, acc: 100.00%] [G loss: 22.456533]\n",
      "epoch:0 step:51 [D loss: 0.106754, acc: 98.44%] [G loss: 22.179056]\n",
      "epoch:0 step:52 [D loss: 0.019351, acc: 99.22%] [G loss: 22.084129]\n",
      "epoch:0 step:53 [D loss: 0.211460, acc: 96.88%] [G loss: 22.244745]\n",
      "epoch:0 step:54 [D loss: 0.011188, acc: 100.00%] [G loss: 22.591728]\n",
      "epoch:0 step:55 [D loss: 0.009810, acc: 100.00%] [G loss: 21.727415]\n",
      "epoch:0 step:56 [D loss: 0.387503, acc: 92.19%] [G loss: 21.832636]\n",
      "epoch:0 step:57 [D loss: 0.070962, acc: 97.66%] [G loss: 22.404064]\n",
      "epoch:0 step:58 [D loss: 0.353280, acc: 94.53%] [G loss: 21.797073]\n",
      "epoch:0 step:59 [D loss: 0.023591, acc: 99.22%] [G loss: 22.869041]\n",
      "epoch:0 step:60 [D loss: 0.059098, acc: 96.88%] [G loss: 23.670235]\n",
      "epoch:0 step:61 [D loss: 0.016635, acc: 100.00%] [G loss: 24.338551]\n",
      "epoch:0 step:62 [D loss: 0.028000, acc: 98.44%] [G loss: 24.629925]\n",
      "epoch:0 step:63 [D loss: 0.004512, acc: 100.00%] [G loss: 24.585079]\n",
      "epoch:0 step:64 [D loss: 0.107569, acc: 98.44%] [G loss: 24.157848]\n",
      "epoch:0 step:65 [D loss: 0.067543, acc: 95.31%] [G loss: 24.465034]\n",
      "epoch:0 step:66 [D loss: 0.058578, acc: 96.88%] [G loss: 24.662518]\n",
      "epoch:0 step:67 [D loss: 0.131113, acc: 95.31%] [G loss: 24.498035]\n",
      "epoch:0 step:68 [D loss: 0.136635, acc: 94.53%] [G loss: 24.398376]\n",
      "epoch:0 step:69 [D loss: 0.105532, acc: 96.88%] [G loss: 22.213612]\n",
      "epoch:0 step:70 [D loss: 0.907477, acc: 82.03%] [G loss: 23.614511]\n",
      "epoch:0 step:71 [D loss: 0.307543, acc: 90.62%] [G loss: 21.683233]\n",
      "epoch:0 step:72 [D loss: 0.311131, acc: 86.72%] [G loss: 22.897957]\n",
      "epoch:0 step:73 [D loss: 0.562666, acc: 82.03%] [G loss: 22.964832]\n",
      "epoch:0 step:74 [D loss: 0.399234, acc: 88.28%] [G loss: 19.341179]\n",
      "epoch:0 step:75 [D loss: 0.608805, acc: 85.16%] [G loss: 21.852741]\n",
      "epoch:0 step:76 [D loss: 0.288379, acc: 92.19%] [G loss: 18.114491]\n",
      "epoch:0 step:77 [D loss: 0.932839, acc: 77.34%] [G loss: 23.626587]\n",
      "epoch:0 step:78 [D loss: 0.371705, acc: 89.06%] [G loss: 19.199783]\n",
      "epoch:0 step:79 [D loss: 0.425874, acc: 82.03%] [G loss: 18.412003]\n",
      "epoch:0 step:80 [D loss: 0.326040, acc: 89.84%] [G loss: 17.668930]\n",
      "epoch:0 step:81 [D loss: 0.119865, acc: 96.09%] [G loss: 15.300668]\n",
      "epoch:0 step:82 [D loss: 0.492058, acc: 82.81%] [G loss: 20.670477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:83 [D loss: 0.468622, acc: 83.59%] [G loss: 13.275326]\n",
      "epoch:0 step:84 [D loss: 0.426609, acc: 89.06%] [G loss: 15.853758]\n",
      "epoch:0 step:85 [D loss: 0.283986, acc: 87.50%] [G loss: 14.888668]\n",
      "epoch:0 step:86 [D loss: 0.327465, acc: 85.16%] [G loss: 14.879415]\n",
      "epoch:0 step:87 [D loss: 0.303412, acc: 85.16%] [G loss: 16.639004]\n",
      "epoch:0 step:88 [D loss: 0.432522, acc: 82.81%] [G loss: 14.719597]\n",
      "epoch:0 step:89 [D loss: 0.254990, acc: 88.28%] [G loss: 12.733261]\n",
      "epoch:0 step:90 [D loss: 0.438938, acc: 82.03%] [G loss: 17.461018]\n",
      "epoch:0 step:91 [D loss: 0.481008, acc: 83.59%] [G loss: 12.678728]\n",
      "epoch:0 step:92 [D loss: 0.479376, acc: 82.81%] [G loss: 12.122629]\n",
      "epoch:0 step:93 [D loss: 0.226798, acc: 90.62%] [G loss: 11.484596]\n",
      "epoch:0 step:94 [D loss: 0.433246, acc: 80.47%] [G loss: 12.704674]\n",
      "epoch:0 step:95 [D loss: 0.230730, acc: 90.62%] [G loss: 10.656030]\n",
      "epoch:0 step:96 [D loss: 0.413457, acc: 85.16%] [G loss: 13.527205]\n",
      "epoch:0 step:97 [D loss: 0.204907, acc: 89.06%] [G loss: 11.792305]\n",
      "epoch:0 step:98 [D loss: 0.233572, acc: 89.84%] [G loss: 8.679334]\n",
      "epoch:0 step:99 [D loss: 0.701690, acc: 71.09%] [G loss: 14.803843]\n",
      "epoch:0 step:100 [D loss: 0.446654, acc: 81.25%] [G loss: 11.354578]\n",
      "epoch:0 step:101 [D loss: 0.164305, acc: 90.62%] [G loss: 8.873877]\n",
      "epoch:0 step:102 [D loss: 0.673814, acc: 72.66%] [G loss: 12.329989]\n",
      "epoch:0 step:103 [D loss: 0.310127, acc: 83.59%] [G loss: 10.121011]\n",
      "epoch:0 step:104 [D loss: 0.233351, acc: 91.41%] [G loss: 9.563692]\n",
      "epoch:0 step:105 [D loss: 0.342447, acc: 83.59%] [G loss: 10.986092]\n",
      "epoch:0 step:106 [D loss: 0.208623, acc: 90.62%] [G loss: 9.393820]\n",
      "epoch:0 step:107 [D loss: 0.231499, acc: 89.06%] [G loss: 9.002417]\n",
      "epoch:0 step:108 [D loss: 0.766207, acc: 66.41%] [G loss: 12.039093]\n",
      "epoch:0 step:109 [D loss: 0.279565, acc: 84.38%] [G loss: 10.335964]\n",
      "epoch:0 step:110 [D loss: 0.269770, acc: 86.72%] [G loss: 9.138083]\n",
      "epoch:0 step:111 [D loss: 0.210506, acc: 92.19%] [G loss: 9.628716]\n",
      "epoch:0 step:112 [D loss: 0.245544, acc: 89.84%] [G loss: 9.914695]\n",
      "epoch:0 step:113 [D loss: 0.327346, acc: 82.81%] [G loss: 9.642319]\n",
      "epoch:0 step:114 [D loss: 0.286521, acc: 82.81%] [G loss: 9.672435]\n",
      "epoch:0 step:115 [D loss: 0.308105, acc: 87.50%] [G loss: 10.936054]\n",
      "epoch:0 step:116 [D loss: 0.302295, acc: 82.03%] [G loss: 9.910788]\n",
      "epoch:0 step:117 [D loss: 0.328797, acc: 85.16%] [G loss: 11.013437]\n",
      "epoch:0 step:118 [D loss: 0.273506, acc: 87.50%] [G loss: 9.579954]\n",
      "epoch:0 step:119 [D loss: 0.314796, acc: 82.03%] [G loss: 10.129650]\n",
      "epoch:0 step:120 [D loss: 0.467445, acc: 80.47%] [G loss: 11.433917]\n",
      "epoch:0 step:121 [D loss: 0.266932, acc: 86.72%] [G loss: 9.221251]\n",
      "epoch:0 step:122 [D loss: 0.288513, acc: 82.81%] [G loss: 8.750650]\n",
      "epoch:0 step:123 [D loss: 0.408028, acc: 73.44%] [G loss: 10.300427]\n",
      "epoch:0 step:124 [D loss: 0.302711, acc: 82.81%] [G loss: 9.172595]\n",
      "epoch:0 step:125 [D loss: 0.255017, acc: 88.28%] [G loss: 8.889642]\n",
      "epoch:0 step:126 [D loss: 0.342903, acc: 85.16%] [G loss: 9.598181]\n",
      "epoch:0 step:127 [D loss: 0.252857, acc: 86.72%] [G loss: 9.525571]\n",
      "epoch:0 step:128 [D loss: 0.351630, acc: 78.12%] [G loss: 10.288317]\n",
      "epoch:0 step:129 [D loss: 0.199249, acc: 92.19%] [G loss: 9.906759]\n",
      "epoch:0 step:130 [D loss: 0.237382, acc: 86.72%] [G loss: 8.941101]\n",
      "epoch:0 step:131 [D loss: 0.366031, acc: 83.59%] [G loss: 10.684956]\n",
      "epoch:0 step:132 [D loss: 0.219537, acc: 90.62%] [G loss: 9.606783]\n",
      "epoch:0 step:133 [D loss: 0.226204, acc: 87.50%] [G loss: 8.083262]\n",
      "epoch:0 step:134 [D loss: 0.339308, acc: 85.16%] [G loss: 9.711060]\n",
      "epoch:0 step:135 [D loss: 0.280746, acc: 87.50%] [G loss: 9.151829]\n",
      "epoch:0 step:136 [D loss: 0.247885, acc: 87.50%] [G loss: 8.849869]\n",
      "epoch:0 step:137 [D loss: 0.312384, acc: 83.59%] [G loss: 10.371673]\n",
      "epoch:0 step:138 [D loss: 0.265681, acc: 83.59%] [G loss: 8.928268]\n",
      "epoch:0 step:139 [D loss: 0.278851, acc: 86.72%] [G loss: 9.671208]\n",
      "epoch:0 step:140 [D loss: 0.188601, acc: 91.41%] [G loss: 9.554667]\n",
      "epoch:0 step:141 [D loss: 0.224027, acc: 87.50%] [G loss: 9.629154]\n",
      "epoch:0 step:142 [D loss: 0.204439, acc: 89.06%] [G loss: 9.165400]\n",
      "epoch:0 step:143 [D loss: 0.237598, acc: 89.06%] [G loss: 8.918875]\n",
      "epoch:0 step:144 [D loss: 0.291476, acc: 86.72%] [G loss: 8.996419]\n",
      "epoch:0 step:145 [D loss: 0.157825, acc: 93.75%] [G loss: 9.077290]\n",
      "epoch:0 step:146 [D loss: 0.363925, acc: 81.25%] [G loss: 9.847841]\n",
      "epoch:0 step:147 [D loss: 0.221443, acc: 86.72%] [G loss: 8.548538]\n",
      "epoch:0 step:148 [D loss: 0.299730, acc: 83.59%] [G loss: 9.150439]\n",
      "epoch:0 step:149 [D loss: 0.197645, acc: 91.41%] [G loss: 8.752890]\n",
      "epoch:0 step:150 [D loss: 0.203401, acc: 92.97%] [G loss: 9.119798]\n",
      "epoch:0 step:151 [D loss: 0.242754, acc: 86.72%] [G loss: 9.705566]\n",
      "epoch:0 step:152 [D loss: 0.162686, acc: 92.97%] [G loss: 8.343996]\n",
      "epoch:0 step:153 [D loss: 0.319627, acc: 85.16%] [G loss: 10.878968]\n",
      "epoch:0 step:154 [D loss: 0.312259, acc: 82.81%] [G loss: 8.952422]\n",
      "epoch:0 step:155 [D loss: 0.171795, acc: 94.53%] [G loss: 9.013024]\n",
      "epoch:0 step:156 [D loss: 0.271295, acc: 85.16%] [G loss: 8.777897]\n",
      "epoch:0 step:157 [D loss: 0.191040, acc: 93.75%] [G loss: 8.730873]\n",
      "epoch:0 step:158 [D loss: 0.194837, acc: 90.62%] [G loss: 8.807102]\n",
      "epoch:0 step:159 [D loss: 0.143205, acc: 95.31%] [G loss: 9.208687]\n",
      "epoch:0 step:160 [D loss: 0.528865, acc: 78.12%] [G loss: 10.440907]\n",
      "epoch:0 step:161 [D loss: 0.244076, acc: 85.94%] [G loss: 9.686456]\n",
      "epoch:0 step:162 [D loss: 0.174780, acc: 92.19%] [G loss: 8.393624]\n",
      "epoch:0 step:163 [D loss: 0.175217, acc: 93.75%] [G loss: 8.948776]\n",
      "epoch:0 step:164 [D loss: 0.172955, acc: 92.19%] [G loss: 8.067470]\n",
      "epoch:0 step:165 [D loss: 0.170350, acc: 93.75%] [G loss: 8.104391]\n",
      "epoch:0 step:166 [D loss: 0.134035, acc: 96.09%] [G loss: 7.886502]\n",
      "epoch:0 step:167 [D loss: 0.157490, acc: 92.97%] [G loss: 8.816225]\n",
      "epoch:0 step:168 [D loss: 0.175807, acc: 93.75%] [G loss: 8.079025]\n",
      "epoch:0 step:169 [D loss: 0.149713, acc: 93.75%] [G loss: 7.729086]\n",
      "epoch:0 step:170 [D loss: 0.141351, acc: 92.97%] [G loss: 8.329639]\n",
      "epoch:0 step:171 [D loss: 0.412095, acc: 78.91%] [G loss: 7.637304]\n",
      "epoch:0 step:172 [D loss: 0.277077, acc: 86.72%] [G loss: 7.925327]\n",
      "epoch:0 step:173 [D loss: 0.368921, acc: 87.50%] [G loss: 7.850442]\n",
      "epoch:0 step:174 [D loss: 0.272462, acc: 85.94%] [G loss: 7.959256]\n",
      "epoch:0 step:175 [D loss: 0.171129, acc: 90.62%] [G loss: 7.565870]\n",
      "epoch:0 step:176 [D loss: 0.234061, acc: 89.06%] [G loss: 8.084179]\n",
      "epoch:0 step:177 [D loss: 0.163707, acc: 92.19%] [G loss: 7.609595]\n",
      "epoch:0 step:178 [D loss: 0.216164, acc: 91.41%] [G loss: 7.889562]\n",
      "epoch:0 step:179 [D loss: 0.213732, acc: 89.06%] [G loss: 7.655578]\n",
      "epoch:0 step:180 [D loss: 0.203325, acc: 89.06%] [G loss: 7.393798]\n",
      "epoch:0 step:181 [D loss: 0.188996, acc: 91.41%] [G loss: 6.307847]\n",
      "epoch:0 step:182 [D loss: 0.159000, acc: 94.53%] [G loss: 6.998101]\n",
      "epoch:0 step:183 [D loss: 0.223594, acc: 89.06%] [G loss: 6.643106]\n",
      "epoch:0 step:184 [D loss: 0.229842, acc: 89.84%] [G loss: 7.030650]\n",
      "epoch:0 step:185 [D loss: 0.275092, acc: 83.59%] [G loss: 7.097170]\n",
      "epoch:0 step:186 [D loss: 0.196815, acc: 88.28%] [G loss: 6.511296]\n",
      "epoch:0 step:187 [D loss: 0.290119, acc: 85.16%] [G loss: 7.271074]\n",
      "epoch:0 step:188 [D loss: 0.356492, acc: 84.38%] [G loss: 7.014540]\n",
      "epoch:0 step:189 [D loss: 0.171771, acc: 92.97%] [G loss: 5.967176]\n",
      "epoch:0 step:190 [D loss: 0.262658, acc: 87.50%] [G loss: 6.373307]\n",
      "epoch:0 step:191 [D loss: 0.355331, acc: 85.94%] [G loss: 6.597060]\n",
      "epoch:0 step:192 [D loss: 0.191698, acc: 92.19%] [G loss: 5.892699]\n",
      "epoch:0 step:193 [D loss: 0.312745, acc: 85.94%] [G loss: 6.791875]\n",
      "epoch:0 step:194 [D loss: 0.261306, acc: 90.62%] [G loss: 6.025212]\n",
      "epoch:0 step:195 [D loss: 0.323330, acc: 85.16%] [G loss: 5.514601]\n",
      "epoch:0 step:196 [D loss: 0.255225, acc: 90.62%] [G loss: 5.986300]\n",
      "epoch:0 step:197 [D loss: 0.427907, acc: 82.03%] [G loss: 6.460925]\n",
      "epoch:0 step:198 [D loss: 0.195105, acc: 93.75%] [G loss: 5.804911]\n",
      "epoch:0 step:199 [D loss: 0.233070, acc: 90.62%] [G loss: 6.340249]\n",
      "epoch:0 step:200 [D loss: 0.144711, acc: 97.66%] [G loss: 5.265257]\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[ 7.1299072   7.59974359 13.25367035 10.80824     8.84389209 11.98333983\n",
      " 11.27914699  9.82118836 10.00487991  9.29102348]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.180940, acc: 93.75%] [G loss: 6.005892]\n",
      "epoch:0 step:202 [D loss: 0.552187, acc: 75.78%] [G loss: 7.067804]\n",
      "epoch:0 step:203 [D loss: 0.213903, acc: 88.28%] [G loss: 5.527696]\n",
      "epoch:0 step:204 [D loss: 0.187607, acc: 96.09%] [G loss: 4.904421]\n",
      "epoch:0 step:205 [D loss: 0.332821, acc: 79.69%] [G loss: 6.451403]\n",
      "epoch:0 step:206 [D loss: 0.197755, acc: 92.19%] [G loss: 6.173525]\n",
      "epoch:0 step:207 [D loss: 0.486503, acc: 74.22%] [G loss: 6.836219]\n",
      "epoch:0 step:208 [D loss: 0.170131, acc: 92.97%] [G loss: 5.150594]\n",
      "epoch:0 step:209 [D loss: 0.344044, acc: 88.28%] [G loss: 5.528923]\n",
      "epoch:0 step:210 [D loss: 0.303972, acc: 87.50%] [G loss: 6.065314]\n",
      "epoch:0 step:211 [D loss: 0.173801, acc: 95.31%] [G loss: 4.721954]\n",
      "epoch:0 step:212 [D loss: 0.475361, acc: 80.47%] [G loss: 6.364856]\n",
      "epoch:0 step:213 [D loss: 0.186712, acc: 92.19%] [G loss: 5.214032]\n",
      "epoch:0 step:214 [D loss: 0.240598, acc: 88.28%] [G loss: 4.384759]\n",
      "epoch:0 step:215 [D loss: 0.370744, acc: 81.25%] [G loss: 5.694351]\n",
      "epoch:0 step:216 [D loss: 0.209100, acc: 92.97%] [G loss: 4.766530]\n",
      "epoch:0 step:217 [D loss: 0.281807, acc: 89.84%] [G loss: 5.385602]\n",
      "epoch:0 step:218 [D loss: 0.289653, acc: 85.16%] [G loss: 5.522013]\n",
      "epoch:0 step:219 [D loss: 0.271286, acc: 91.41%] [G loss: 5.663085]\n",
      "epoch:0 step:220 [D loss: 0.638741, acc: 65.62%] [G loss: 6.241220]\n",
      "epoch:0 step:221 [D loss: 0.213317, acc: 88.28%] [G loss: 4.747396]\n",
      "epoch:0 step:222 [D loss: 0.467531, acc: 78.12%] [G loss: 4.780963]\n",
      "epoch:0 step:223 [D loss: 0.212354, acc: 91.41%] [G loss: 4.304336]\n",
      "epoch:0 step:224 [D loss: 0.443332, acc: 80.47%] [G loss: 4.631795]\n",
      "epoch:0 step:225 [D loss: 0.198249, acc: 92.97%] [G loss: 4.250162]\n",
      "epoch:0 step:226 [D loss: 0.376950, acc: 84.38%] [G loss: 4.753475]\n",
      "epoch:0 step:227 [D loss: 0.174089, acc: 95.31%] [G loss: 4.050828]\n",
      "epoch:0 step:228 [D loss: 0.325542, acc: 85.16%] [G loss: 4.633333]\n",
      "epoch:0 step:229 [D loss: 0.341772, acc: 79.69%] [G loss: 4.940414]\n",
      "epoch:0 step:230 [D loss: 0.297990, acc: 84.38%] [G loss: 4.253112]\n",
      "epoch:0 step:231 [D loss: 0.311027, acc: 86.72%] [G loss: 5.508184]\n",
      "epoch:0 step:232 [D loss: 0.257967, acc: 87.50%] [G loss: 4.326159]\n",
      "epoch:0 step:233 [D loss: 0.926634, acc: 57.03%] [G loss: 5.948120]\n",
      "epoch:0 step:234 [D loss: 0.214390, acc: 89.84%] [G loss: 4.079551]\n",
      "epoch:0 step:235 [D loss: 0.388489, acc: 83.59%] [G loss: 4.769648]\n",
      "epoch:0 step:236 [D loss: 0.362293, acc: 82.81%] [G loss: 4.260192]\n",
      "epoch:0 step:237 [D loss: 0.350781, acc: 83.59%] [G loss: 4.440302]\n",
      "epoch:0 step:238 [D loss: 0.203321, acc: 94.53%] [G loss: 4.185511]\n",
      "epoch:0 step:239 [D loss: 0.383098, acc: 81.25%] [G loss: 4.707598]\n",
      "epoch:0 step:240 [D loss: 0.227786, acc: 89.84%] [G loss: 3.966774]\n",
      "epoch:0 step:241 [D loss: 0.519290, acc: 78.91%] [G loss: 5.239852]\n",
      "epoch:0 step:242 [D loss: 0.230046, acc: 94.53%] [G loss: 4.285448]\n",
      "epoch:0 step:243 [D loss: 0.513996, acc: 73.44%] [G loss: 4.889506]\n",
      "epoch:0 step:244 [D loss: 0.248534, acc: 89.84%] [G loss: 3.467355]\n",
      "epoch:0 step:245 [D loss: 0.744259, acc: 55.47%] [G loss: 5.226544]\n",
      "epoch:0 step:246 [D loss: 0.255744, acc: 89.84%] [G loss: 3.473569]\n",
      "epoch:0 step:247 [D loss: 0.412105, acc: 84.38%] [G loss: 3.999044]\n",
      "epoch:0 step:248 [D loss: 0.371028, acc: 78.91%] [G loss: 4.387859]\n",
      "epoch:0 step:249 [D loss: 0.345952, acc: 82.81%] [G loss: 3.767557]\n",
      "epoch:0 step:250 [D loss: 0.472127, acc: 75.00%] [G loss: 4.292660]\n",
      "epoch:0 step:251 [D loss: 0.305702, acc: 90.62%] [G loss: 3.735307]\n",
      "epoch:0 step:252 [D loss: 0.340035, acc: 86.72%] [G loss: 4.088286]\n",
      "epoch:0 step:253 [D loss: 0.341949, acc: 85.16%] [G loss: 3.936298]\n",
      "epoch:0 step:254 [D loss: 0.319880, acc: 89.06%] [G loss: 4.421329]\n",
      "epoch:0 step:255 [D loss: 0.302001, acc: 88.28%] [G loss: 4.676705]\n",
      "epoch:0 step:256 [D loss: 0.434079, acc: 85.16%] [G loss: 4.987272]\n",
      "epoch:0 step:257 [D loss: 0.247839, acc: 92.19%] [G loss: 4.223714]\n",
      "epoch:0 step:258 [D loss: 0.239633, acc: 94.53%] [G loss: 5.073573]\n",
      "epoch:0 step:259 [D loss: 0.303956, acc: 89.06%] [G loss: 4.962846]\n",
      "epoch:0 step:260 [D loss: 0.501270, acc: 75.78%] [G loss: 5.561473]\n",
      "epoch:0 step:261 [D loss: 0.297787, acc: 85.16%] [G loss: 4.437157]\n",
      "epoch:0 step:262 [D loss: 0.346917, acc: 88.28%] [G loss: 4.655299]\n",
      "epoch:0 step:263 [D loss: 0.401497, acc: 78.91%] [G loss: 3.934218]\n",
      "epoch:0 step:264 [D loss: 0.379995, acc: 81.25%] [G loss: 3.920842]\n",
      "epoch:0 step:265 [D loss: 0.534919, acc: 75.00%] [G loss: 4.928367]\n",
      "epoch:0 step:266 [D loss: 0.371375, acc: 85.16%] [G loss: 4.302769]\n",
      "epoch:0 step:267 [D loss: 0.355327, acc: 85.94%] [G loss: 4.399334]\n",
      "epoch:0 step:268 [D loss: 0.553827, acc: 74.22%] [G loss: 5.324073]\n",
      "epoch:0 step:269 [D loss: 0.266661, acc: 92.19%] [G loss: 4.053823]\n",
      "epoch:0 step:270 [D loss: 0.255966, acc: 92.19%] [G loss: 4.057791]\n",
      "epoch:0 step:271 [D loss: 0.480185, acc: 71.88%] [G loss: 4.713815]\n",
      "epoch:0 step:272 [D loss: 0.231797, acc: 93.75%] [G loss: 3.971140]\n",
      "epoch:0 step:273 [D loss: 0.480506, acc: 78.12%] [G loss: 4.743878]\n",
      "epoch:0 step:274 [D loss: 0.179802, acc: 96.09%] [G loss: 4.096755]\n",
      "epoch:0 step:275 [D loss: 0.416711, acc: 82.03%] [G loss: 4.584257]\n",
      "epoch:0 step:276 [D loss: 0.242030, acc: 92.19%] [G loss: 4.006960]\n",
      "epoch:0 step:277 [D loss: 0.414141, acc: 82.03%] [G loss: 3.732273]\n",
      "epoch:0 step:278 [D loss: 0.621369, acc: 60.94%] [G loss: 4.432673]\n",
      "epoch:0 step:279 [D loss: 0.356663, acc: 85.94%] [G loss: 4.172441]\n",
      "epoch:0 step:280 [D loss: 0.288449, acc: 89.06%] [G loss: 4.468979]\n",
      "epoch:0 step:281 [D loss: 0.499135, acc: 75.00%] [G loss: 4.810649]\n",
      "epoch:0 step:282 [D loss: 0.461929, acc: 79.69%] [G loss: 4.654928]\n",
      "epoch:0 step:283 [D loss: 0.280940, acc: 89.06%] [G loss: 4.320482]\n",
      "epoch:0 step:284 [D loss: 0.296877, acc: 89.84%] [G loss: 3.993135]\n",
      "epoch:0 step:285 [D loss: 0.444313, acc: 75.78%] [G loss: 4.715594]\n",
      "epoch:0 step:286 [D loss: 0.318632, acc: 85.94%] [G loss: 4.595687]\n",
      "epoch:0 step:287 [D loss: 0.341308, acc: 89.84%] [G loss: 4.306698]\n",
      "epoch:0 step:288 [D loss: 0.304194, acc: 89.84%] [G loss: 4.256601]\n",
      "epoch:0 step:289 [D loss: 0.326789, acc: 89.06%] [G loss: 5.082211]\n",
      "epoch:0 step:290 [D loss: 0.419185, acc: 75.00%] [G loss: 4.112349]\n",
      "epoch:0 step:291 [D loss: 0.434108, acc: 77.34%] [G loss: 4.356338]\n",
      "epoch:0 step:292 [D loss: 0.283966, acc: 88.28%] [G loss: 4.341313]\n",
      "epoch:0 step:293 [D loss: 0.521680, acc: 76.56%] [G loss: 4.190940]\n",
      "epoch:0 step:294 [D loss: 0.428960, acc: 80.47%] [G loss: 4.194246]\n",
      "epoch:0 step:295 [D loss: 0.360003, acc: 87.50%] [G loss: 4.289586]\n",
      "epoch:0 step:296 [D loss: 0.339835, acc: 89.06%] [G loss: 4.594195]\n",
      "epoch:0 step:297 [D loss: 0.226285, acc: 95.31%] [G loss: 5.021538]\n",
      "epoch:0 step:298 [D loss: 0.312938, acc: 86.72%] [G loss: 5.115788]\n",
      "epoch:0 step:299 [D loss: 0.249194, acc: 93.75%] [G loss: 4.407506]\n",
      "epoch:0 step:300 [D loss: 0.355351, acc: 88.28%] [G loss: 3.871855]\n",
      "epoch:0 step:301 [D loss: 0.638723, acc: 60.16%] [G loss: 4.816919]\n",
      "epoch:0 step:302 [D loss: 0.290723, acc: 91.41%] [G loss: 4.288733]\n",
      "epoch:0 step:303 [D loss: 0.272994, acc: 90.62%] [G loss: 3.867440]\n",
      "epoch:0 step:304 [D loss: 0.496022, acc: 71.88%] [G loss: 4.896512]\n",
      "epoch:0 step:305 [D loss: 0.199083, acc: 99.22%] [G loss: 4.728192]\n",
      "epoch:0 step:306 [D loss: 0.323165, acc: 87.50%] [G loss: 5.012362]\n",
      "epoch:0 step:307 [D loss: 0.371877, acc: 85.16%] [G loss: 5.095424]\n",
      "epoch:0 step:308 [D loss: 0.291797, acc: 90.62%] [G loss: 4.414013]\n",
      "epoch:0 step:309 [D loss: 0.302716, acc: 92.97%] [G loss: 4.481657]\n",
      "epoch:0 step:310 [D loss: 0.282169, acc: 91.41%] [G loss: 4.830855]\n",
      "epoch:0 step:311 [D loss: 0.499384, acc: 73.44%] [G loss: 4.580112]\n",
      "epoch:0 step:312 [D loss: 0.341166, acc: 90.62%] [G loss: 4.632000]\n",
      "epoch:0 step:313 [D loss: 0.302148, acc: 91.41%] [G loss: 4.413693]\n",
      "epoch:0 step:314 [D loss: 0.316382, acc: 88.28%] [G loss: 4.686574]\n",
      "epoch:0 step:315 [D loss: 0.269107, acc: 93.75%] [G loss: 4.951068]\n",
      "epoch:0 step:316 [D loss: 0.332520, acc: 89.06%] [G loss: 4.735087]\n",
      "epoch:0 step:317 [D loss: 0.250898, acc: 92.19%] [G loss: 4.045864]\n",
      "epoch:0 step:318 [D loss: 0.805440, acc: 51.56%] [G loss: 4.616203]\n",
      "epoch:0 step:319 [D loss: 0.224413, acc: 93.75%] [G loss: 3.832283]\n",
      "epoch:0 step:320 [D loss: 0.487529, acc: 78.12%] [G loss: 4.822593]\n",
      "epoch:0 step:321 [D loss: 0.331180, acc: 89.84%] [G loss: 4.373996]\n",
      "epoch:0 step:322 [D loss: 0.449540, acc: 80.47%] [G loss: 4.557936]\n",
      "epoch:0 step:323 [D loss: 0.346882, acc: 85.94%] [G loss: 3.685196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:324 [D loss: 0.421119, acc: 82.03%] [G loss: 4.410597]\n",
      "epoch:0 step:325 [D loss: 0.288865, acc: 91.41%] [G loss: 4.378838]\n",
      "epoch:0 step:326 [D loss: 0.522106, acc: 74.22%] [G loss: 4.342999]\n",
      "epoch:0 step:327 [D loss: 0.217289, acc: 92.97%] [G loss: 3.877490]\n",
      "epoch:0 step:328 [D loss: 0.599822, acc: 63.28%] [G loss: 5.617480]\n",
      "epoch:0 step:329 [D loss: 0.261669, acc: 91.41%] [G loss: 3.976883]\n",
      "epoch:0 step:330 [D loss: 0.443512, acc: 82.81%] [G loss: 4.571805]\n",
      "epoch:0 step:331 [D loss: 0.241081, acc: 94.53%] [G loss: 3.698300]\n",
      "epoch:0 step:332 [D loss: 0.516601, acc: 75.00%] [G loss: 4.290234]\n",
      "epoch:0 step:333 [D loss: 0.295089, acc: 88.28%] [G loss: 3.912292]\n",
      "epoch:0 step:334 [D loss: 0.485350, acc: 75.78%] [G loss: 3.970228]\n",
      "epoch:0 step:335 [D loss: 0.288798, acc: 89.84%] [G loss: 4.584590]\n",
      "epoch:0 step:336 [D loss: 0.351034, acc: 90.62%] [G loss: 4.446581]\n",
      "epoch:0 step:337 [D loss: 0.429524, acc: 78.91%] [G loss: 4.139856]\n",
      "epoch:0 step:338 [D loss: 0.421664, acc: 85.16%] [G loss: 4.799430]\n",
      "epoch:0 step:339 [D loss: 0.252415, acc: 89.06%] [G loss: 4.084157]\n",
      "epoch:0 step:340 [D loss: 0.549837, acc: 75.00%] [G loss: 5.387125]\n",
      "epoch:0 step:341 [D loss: 0.756759, acc: 58.59%] [G loss: 4.692045]\n",
      "epoch:0 step:342 [D loss: 0.237053, acc: 94.53%] [G loss: 3.603293]\n",
      "epoch:0 step:343 [D loss: 0.513549, acc: 75.78%] [G loss: 4.358178]\n",
      "epoch:0 step:344 [D loss: 0.237906, acc: 93.75%] [G loss: 3.881640]\n",
      "epoch:0 step:345 [D loss: 0.498940, acc: 75.78%] [G loss: 4.919838]\n",
      "epoch:0 step:346 [D loss: 0.265078, acc: 91.41%] [G loss: 3.849494]\n",
      "epoch:0 step:347 [D loss: 0.294321, acc: 92.19%] [G loss: 4.431061]\n",
      "epoch:0 step:348 [D loss: 0.552585, acc: 65.62%] [G loss: 4.145980]\n",
      "epoch:0 step:349 [D loss: 0.348479, acc: 89.06%] [G loss: 3.990824]\n",
      "epoch:0 step:350 [D loss: 0.325202, acc: 90.62%] [G loss: 4.366452]\n",
      "epoch:0 step:351 [D loss: 0.349363, acc: 90.62%] [G loss: 4.228278]\n",
      "epoch:0 step:352 [D loss: 0.286841, acc: 93.75%] [G loss: 3.949559]\n",
      "epoch:0 step:353 [D loss: 0.348502, acc: 84.38%] [G loss: 4.056452]\n",
      "epoch:0 step:354 [D loss: 0.523198, acc: 81.25%] [G loss: 4.175258]\n",
      "epoch:0 step:355 [D loss: 0.393438, acc: 84.38%] [G loss: 4.174816]\n",
      "epoch:0 step:356 [D loss: 0.300445, acc: 90.62%] [G loss: 3.745975]\n",
      "epoch:0 step:357 [D loss: 0.374278, acc: 86.72%] [G loss: 4.155738]\n",
      "epoch:0 step:358 [D loss: 0.361902, acc: 86.72%] [G loss: 3.914948]\n",
      "epoch:0 step:359 [D loss: 0.328101, acc: 89.06%] [G loss: 4.149777]\n",
      "epoch:0 step:360 [D loss: 0.400795, acc: 83.59%] [G loss: 4.531789]\n",
      "epoch:0 step:361 [D loss: 0.256708, acc: 95.31%] [G loss: 4.346413]\n",
      "epoch:0 step:362 [D loss: 0.405564, acc: 84.38%] [G loss: 4.120550]\n",
      "epoch:0 step:363 [D loss: 0.322393, acc: 91.41%] [G loss: 4.189042]\n",
      "epoch:0 step:364 [D loss: 0.386447, acc: 84.38%] [G loss: 4.363609]\n",
      "epoch:0 step:365 [D loss: 0.441060, acc: 80.47%] [G loss: 4.043430]\n",
      "epoch:0 step:366 [D loss: 0.462567, acc: 79.69%] [G loss: 4.295508]\n",
      "epoch:0 step:367 [D loss: 0.342243, acc: 85.94%] [G loss: 3.779611]\n",
      "epoch:0 step:368 [D loss: 0.584196, acc: 71.09%] [G loss: 4.347900]\n",
      "epoch:0 step:369 [D loss: 0.411664, acc: 80.47%] [G loss: 3.599886]\n",
      "epoch:0 step:370 [D loss: 0.433389, acc: 82.81%] [G loss: 3.741944]\n",
      "epoch:0 step:371 [D loss: 0.464954, acc: 75.00%] [G loss: 4.141118]\n",
      "epoch:0 step:372 [D loss: 0.397827, acc: 82.03%] [G loss: 3.940475]\n",
      "epoch:0 step:373 [D loss: 0.409926, acc: 85.94%] [G loss: 4.197321]\n",
      "epoch:0 step:374 [D loss: 0.287760, acc: 92.97%] [G loss: 3.778805]\n",
      "epoch:0 step:375 [D loss: 0.374334, acc: 87.50%] [G loss: 4.648270]\n",
      "epoch:0 step:376 [D loss: 0.389340, acc: 84.38%] [G loss: 4.385015]\n",
      "epoch:0 step:377 [D loss: 0.320854, acc: 90.62%] [G loss: 4.120728]\n",
      "epoch:0 step:378 [D loss: 0.422223, acc: 82.81%] [G loss: 4.563015]\n",
      "epoch:0 step:379 [D loss: 0.349173, acc: 87.50%] [G loss: 4.387026]\n",
      "epoch:0 step:380 [D loss: 0.381611, acc: 84.38%] [G loss: 4.808279]\n",
      "epoch:0 step:381 [D loss: 0.333539, acc: 88.28%] [G loss: 4.079756]\n",
      "epoch:0 step:382 [D loss: 0.499502, acc: 80.47%] [G loss: 4.231042]\n",
      "epoch:0 step:383 [D loss: 0.399214, acc: 82.03%] [G loss: 3.928502]\n",
      "epoch:0 step:384 [D loss: 0.500650, acc: 73.44%] [G loss: 4.080810]\n",
      "epoch:0 step:385 [D loss: 0.310148, acc: 89.06%] [G loss: 4.030476]\n",
      "epoch:0 step:386 [D loss: 0.450901, acc: 76.56%] [G loss: 4.808188]\n",
      "epoch:0 step:387 [D loss: 0.278773, acc: 92.97%] [G loss: 4.104627]\n",
      "epoch:0 step:388 [D loss: 0.230114, acc: 93.75%] [G loss: 4.773262]\n",
      "epoch:0 step:389 [D loss: 0.387658, acc: 88.28%] [G loss: 4.603734]\n",
      "epoch:0 step:390 [D loss: 0.321420, acc: 87.50%] [G loss: 4.069571]\n",
      "epoch:0 step:391 [D loss: 0.500022, acc: 77.34%] [G loss: 4.129525]\n",
      "epoch:0 step:392 [D loss: 0.327924, acc: 86.72%] [G loss: 4.355884]\n",
      "epoch:0 step:393 [D loss: 0.489757, acc: 76.56%] [G loss: 4.555671]\n",
      "epoch:0 step:394 [D loss: 0.255486, acc: 93.75%] [G loss: 3.846034]\n",
      "epoch:0 step:395 [D loss: 0.358793, acc: 87.50%] [G loss: 4.302451]\n",
      "epoch:0 step:396 [D loss: 0.436248, acc: 82.03%] [G loss: 4.345730]\n",
      "epoch:0 step:397 [D loss: 0.274923, acc: 92.97%] [G loss: 4.887336]\n",
      "epoch:0 step:398 [D loss: 0.326599, acc: 90.62%] [G loss: 4.763956]\n",
      "epoch:0 step:399 [D loss: 0.299394, acc: 91.41%] [G loss: 4.947048]\n",
      "epoch:0 step:400 [D loss: 0.316460, acc: 89.84%] [G loss: 5.242456]\n",
      "##############\n",
      "[ 6.31304937  6.59974359 10.85015     9.09141799  8.20154141 11.98333983\n",
      " 11.27914699  9.11663518  9.45918933  8.46541785]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.302307, acc: 92.19%] [G loss: 5.020438]\n",
      "epoch:0 step:402 [D loss: 0.393255, acc: 80.47%] [G loss: 4.875654]\n",
      "epoch:0 step:403 [D loss: 0.389547, acc: 82.81%] [G loss: 4.408715]\n",
      "epoch:0 step:404 [D loss: 0.515581, acc: 73.44%] [G loss: 4.397092]\n",
      "epoch:0 step:405 [D loss: 0.354772, acc: 85.16%] [G loss: 4.904795]\n",
      "epoch:0 step:406 [D loss: 0.405465, acc: 84.38%] [G loss: 4.847355]\n",
      "epoch:0 step:407 [D loss: 0.352627, acc: 85.16%] [G loss: 4.983388]\n",
      "epoch:0 step:408 [D loss: 0.357621, acc: 85.94%] [G loss: 4.295085]\n",
      "epoch:0 step:409 [D loss: 0.384910, acc: 83.59%] [G loss: 4.802837]\n",
      "epoch:0 step:410 [D loss: 0.320308, acc: 86.72%] [G loss: 4.484184]\n",
      "epoch:0 step:411 [D loss: 0.433053, acc: 84.38%] [G loss: 3.947269]\n",
      "epoch:0 step:412 [D loss: 0.348243, acc: 82.81%] [G loss: 4.641829]\n",
      "epoch:0 step:413 [D loss: 0.378811, acc: 82.03%] [G loss: 4.070267]\n",
      "epoch:0 step:414 [D loss: 0.469206, acc: 82.03%] [G loss: 4.648675]\n",
      "epoch:0 step:415 [D loss: 0.449174, acc: 77.34%] [G loss: 4.363323]\n",
      "epoch:0 step:416 [D loss: 0.379051, acc: 86.72%] [G loss: 4.528010]\n",
      "epoch:0 step:417 [D loss: 0.433659, acc: 81.25%] [G loss: 4.516208]\n",
      "epoch:0 step:418 [D loss: 0.425907, acc: 81.25%] [G loss: 4.411388]\n",
      "epoch:0 step:419 [D loss: 0.381861, acc: 83.59%] [G loss: 4.463561]\n",
      "epoch:0 step:420 [D loss: 0.445063, acc: 82.03%] [G loss: 4.253725]\n",
      "epoch:0 step:421 [D loss: 0.592417, acc: 69.53%] [G loss: 4.209140]\n",
      "epoch:0 step:422 [D loss: 0.302157, acc: 92.19%] [G loss: 4.150683]\n",
      "epoch:0 step:423 [D loss: 0.477873, acc: 78.12%] [G loss: 4.092762]\n",
      "epoch:0 step:424 [D loss: 0.413867, acc: 83.59%] [G loss: 3.868150]\n",
      "epoch:0 step:425 [D loss: 0.491208, acc: 72.66%] [G loss: 4.058931]\n",
      "epoch:0 step:426 [D loss: 0.373660, acc: 82.03%] [G loss: 4.455474]\n",
      "epoch:0 step:427 [D loss: 0.359003, acc: 86.72%] [G loss: 4.380014]\n",
      "epoch:0 step:428 [D loss: 0.321912, acc: 85.94%] [G loss: 4.235054]\n",
      "epoch:0 step:429 [D loss: 0.390311, acc: 81.25%] [G loss: 4.421433]\n",
      "epoch:0 step:430 [D loss: 0.460617, acc: 80.47%] [G loss: 4.252702]\n",
      "epoch:0 step:431 [D loss: 0.557906, acc: 74.22%] [G loss: 4.241664]\n",
      "epoch:0 step:432 [D loss: 0.404564, acc: 84.38%] [G loss: 4.085173]\n",
      "epoch:0 step:433 [D loss: 0.549990, acc: 75.78%] [G loss: 3.928662]\n",
      "epoch:0 step:434 [D loss: 0.323265, acc: 89.84%] [G loss: 3.836577]\n",
      "epoch:0 step:435 [D loss: 0.457553, acc: 78.12%] [G loss: 3.695475]\n",
      "epoch:0 step:436 [D loss: 0.419499, acc: 76.56%] [G loss: 4.447702]\n",
      "epoch:0 step:437 [D loss: 0.542072, acc: 71.88%] [G loss: 4.747056]\n",
      "epoch:0 step:438 [D loss: 0.335025, acc: 85.16%] [G loss: 4.313469]\n",
      "epoch:0 step:439 [D loss: 0.488633, acc: 81.25%] [G loss: 4.726922]\n",
      "epoch:0 step:440 [D loss: 0.316656, acc: 87.50%] [G loss: 4.138724]\n",
      "epoch:0 step:441 [D loss: 0.407686, acc: 78.91%] [G loss: 3.836340]\n",
      "epoch:0 step:442 [D loss: 0.557891, acc: 74.22%] [G loss: 4.430835]\n",
      "epoch:0 step:443 [D loss: 0.370108, acc: 86.72%] [G loss: 4.380800]\n",
      "epoch:0 step:444 [D loss: 0.343522, acc: 89.06%] [G loss: 4.457023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:445 [D loss: 0.382835, acc: 85.16%] [G loss: 4.226974]\n",
      "epoch:0 step:446 [D loss: 0.503469, acc: 73.44%] [G loss: 3.935754]\n",
      "epoch:0 step:447 [D loss: 0.350972, acc: 90.62%] [G loss: 4.344460]\n",
      "epoch:0 step:448 [D loss: 0.520063, acc: 73.44%] [G loss: 4.157565]\n",
      "epoch:0 step:449 [D loss: 0.318261, acc: 89.84%] [G loss: 4.322023]\n",
      "epoch:0 step:450 [D loss: 0.313123, acc: 87.50%] [G loss: 4.573184]\n",
      "epoch:0 step:451 [D loss: 0.541551, acc: 74.22%] [G loss: 4.108429]\n",
      "epoch:0 step:452 [D loss: 0.446274, acc: 77.34%] [G loss: 4.438254]\n",
      "epoch:0 step:453 [D loss: 0.430661, acc: 83.59%] [G loss: 3.999035]\n",
      "epoch:0 step:454 [D loss: 0.549417, acc: 70.31%] [G loss: 4.105881]\n",
      "epoch:0 step:455 [D loss: 0.340521, acc: 89.06%] [G loss: 3.996634]\n",
      "epoch:0 step:456 [D loss: 0.526692, acc: 72.66%] [G loss: 3.727266]\n",
      "epoch:0 step:457 [D loss: 0.417185, acc: 79.69%] [G loss: 4.077831]\n",
      "epoch:0 step:458 [D loss: 0.459851, acc: 81.25%] [G loss: 4.071331]\n",
      "epoch:0 step:459 [D loss: 0.314928, acc: 88.28%] [G loss: 4.476820]\n",
      "epoch:0 step:460 [D loss: 0.452805, acc: 83.59%] [G loss: 3.997739]\n",
      "epoch:0 step:461 [D loss: 0.450649, acc: 79.69%] [G loss: 4.062102]\n",
      "epoch:0 step:462 [D loss: 0.562671, acc: 72.66%] [G loss: 3.726231]\n",
      "epoch:0 step:463 [D loss: 0.506836, acc: 75.00%] [G loss: 3.626659]\n",
      "epoch:0 step:464 [D loss: 0.469223, acc: 80.47%] [G loss: 3.972616]\n",
      "epoch:0 step:465 [D loss: 0.552280, acc: 69.53%] [G loss: 4.069972]\n",
      "epoch:0 step:466 [D loss: 0.400073, acc: 83.59%] [G loss: 4.147194]\n",
      "epoch:0 step:467 [D loss: 0.551091, acc: 72.66%] [G loss: 3.882861]\n",
      "epoch:0 step:468 [D loss: 0.412199, acc: 84.38%] [G loss: 4.205094]\n",
      "epoch:0 step:469 [D loss: 0.376304, acc: 85.16%] [G loss: 4.057441]\n",
      "epoch:0 step:470 [D loss: 0.381907, acc: 83.59%] [G loss: 4.421728]\n",
      "epoch:0 step:471 [D loss: 0.369911, acc: 83.59%] [G loss: 4.884264]\n",
      "epoch:0 step:472 [D loss: 0.445016, acc: 77.34%] [G loss: 3.839157]\n",
      "epoch:0 step:473 [D loss: 0.349824, acc: 87.50%] [G loss: 3.848266]\n",
      "epoch:0 step:474 [D loss: 0.389975, acc: 82.81%] [G loss: 4.291330]\n",
      "epoch:0 step:475 [D loss: 0.439735, acc: 82.03%] [G loss: 4.264153]\n",
      "epoch:0 step:476 [D loss: 0.666516, acc: 65.62%] [G loss: 3.812073]\n",
      "epoch:0 step:477 [D loss: 0.542524, acc: 75.00%] [G loss: 3.520262]\n",
      "epoch:0 step:478 [D loss: 0.510981, acc: 71.09%] [G loss: 3.504070]\n",
      "epoch:0 step:479 [D loss: 0.529445, acc: 72.66%] [G loss: 4.312421]\n",
      "epoch:0 step:480 [D loss: 0.358535, acc: 86.72%] [G loss: 3.737628]\n",
      "epoch:0 step:481 [D loss: 0.482056, acc: 80.47%] [G loss: 3.983654]\n",
      "epoch:0 step:482 [D loss: 0.650760, acc: 63.28%] [G loss: 3.460455]\n",
      "epoch:0 step:483 [D loss: 0.461765, acc: 80.47%] [G loss: 3.560734]\n",
      "epoch:0 step:484 [D loss: 0.401611, acc: 82.81%] [G loss: 3.784519]\n",
      "epoch:0 step:485 [D loss: 0.446198, acc: 80.47%] [G loss: 3.854221]\n",
      "epoch:0 step:486 [D loss: 0.621307, acc: 64.84%] [G loss: 3.486659]\n",
      "epoch:0 step:487 [D loss: 0.508263, acc: 75.00%] [G loss: 3.666374]\n",
      "epoch:0 step:488 [D loss: 0.552651, acc: 71.88%] [G loss: 3.894417]\n",
      "epoch:0 step:489 [D loss: 0.490841, acc: 74.22%] [G loss: 4.113380]\n",
      "epoch:0 step:490 [D loss: 0.397323, acc: 84.38%] [G loss: 3.864539]\n",
      "epoch:0 step:491 [D loss: 0.505129, acc: 71.88%] [G loss: 3.688882]\n",
      "epoch:0 step:492 [D loss: 0.526252, acc: 71.88%] [G loss: 3.653326]\n",
      "epoch:0 step:493 [D loss: 0.534785, acc: 73.44%] [G loss: 3.449738]\n",
      "epoch:0 step:494 [D loss: 0.492923, acc: 78.12%] [G loss: 3.652688]\n",
      "epoch:0 step:495 [D loss: 0.520535, acc: 72.66%] [G loss: 3.413455]\n",
      "epoch:0 step:496 [D loss: 0.472711, acc: 75.78%] [G loss: 3.678318]\n",
      "epoch:0 step:497 [D loss: 0.452553, acc: 82.03%] [G loss: 3.665873]\n",
      "epoch:0 step:498 [D loss: 0.445874, acc: 83.59%] [G loss: 3.796684]\n",
      "epoch:0 step:499 [D loss: 0.367550, acc: 85.16%] [G loss: 3.859791]\n",
      "epoch:0 step:500 [D loss: 0.573360, acc: 71.09%] [G loss: 4.039070]\n",
      "epoch:0 step:501 [D loss: 0.399762, acc: 83.59%] [G loss: 3.513007]\n",
      "epoch:0 step:502 [D loss: 0.475844, acc: 72.66%] [G loss: 3.784021]\n",
      "epoch:0 step:503 [D loss: 0.463040, acc: 83.59%] [G loss: 4.036569]\n",
      "epoch:0 step:504 [D loss: 0.413584, acc: 83.59%] [G loss: 4.097076]\n",
      "epoch:0 step:505 [D loss: 0.416724, acc: 82.03%] [G loss: 3.805220]\n",
      "epoch:0 step:506 [D loss: 0.424910, acc: 80.47%] [G loss: 4.536714]\n",
      "epoch:0 step:507 [D loss: 0.473854, acc: 78.91%] [G loss: 3.941942]\n",
      "epoch:0 step:508 [D loss: 0.395495, acc: 81.25%] [G loss: 3.738753]\n",
      "epoch:0 step:509 [D loss: 0.683688, acc: 64.06%] [G loss: 3.427023]\n",
      "epoch:0 step:510 [D loss: 0.578706, acc: 71.09%] [G loss: 3.478921]\n",
      "epoch:0 step:511 [D loss: 0.558227, acc: 71.88%] [G loss: 3.508294]\n",
      "epoch:0 step:512 [D loss: 0.523682, acc: 76.56%] [G loss: 3.377963]\n",
      "epoch:0 step:513 [D loss: 0.471361, acc: 78.91%] [G loss: 3.562565]\n",
      "epoch:0 step:514 [D loss: 0.485934, acc: 77.34%] [G loss: 3.579294]\n",
      "epoch:0 step:515 [D loss: 0.524455, acc: 75.78%] [G loss: 3.415788]\n",
      "epoch:0 step:516 [D loss: 0.473468, acc: 78.12%] [G loss: 3.883925]\n",
      "epoch:0 step:517 [D loss: 0.603764, acc: 69.53%] [G loss: 3.418791]\n",
      "epoch:0 step:518 [D loss: 0.598508, acc: 66.41%] [G loss: 3.694097]\n",
      "epoch:0 step:519 [D loss: 0.378603, acc: 84.38%] [G loss: 3.585423]\n",
      "epoch:0 step:520 [D loss: 0.511151, acc: 69.53%] [G loss: 3.292916]\n",
      "epoch:0 step:521 [D loss: 0.645526, acc: 71.88%] [G loss: 3.378751]\n",
      "epoch:0 step:522 [D loss: 0.405539, acc: 83.59%] [G loss: 3.293084]\n",
      "epoch:0 step:523 [D loss: 0.497872, acc: 78.12%] [G loss: 3.831069]\n",
      "epoch:0 step:524 [D loss: 0.462648, acc: 78.91%] [G loss: 3.219634]\n",
      "epoch:0 step:525 [D loss: 0.458751, acc: 79.69%] [G loss: 3.339720]\n",
      "epoch:0 step:526 [D loss: 0.523298, acc: 74.22%] [G loss: 3.277876]\n",
      "epoch:0 step:527 [D loss: 0.515866, acc: 72.66%] [G loss: 3.299473]\n",
      "epoch:0 step:528 [D loss: 0.411092, acc: 85.94%] [G loss: 3.540674]\n",
      "epoch:0 step:529 [D loss: 0.430124, acc: 81.25%] [G loss: 3.460016]\n",
      "epoch:0 step:530 [D loss: 0.365792, acc: 83.59%] [G loss: 3.828005]\n",
      "epoch:0 step:531 [D loss: 0.838143, acc: 51.56%] [G loss: 3.299426]\n",
      "epoch:0 step:532 [D loss: 0.419103, acc: 82.81%] [G loss: 3.229169]\n",
      "epoch:0 step:533 [D loss: 0.596364, acc: 70.31%] [G loss: 3.867484]\n",
      "epoch:0 step:534 [D loss: 0.487684, acc: 75.00%] [G loss: 3.670896]\n",
      "epoch:0 step:535 [D loss: 0.668151, acc: 62.50%] [G loss: 3.332223]\n",
      "epoch:0 step:536 [D loss: 0.457770, acc: 78.91%] [G loss: 3.041796]\n",
      "epoch:0 step:537 [D loss: 0.460784, acc: 75.78%] [G loss: 3.466325]\n",
      "epoch:0 step:538 [D loss: 0.559538, acc: 75.00%] [G loss: 3.675053]\n",
      "epoch:0 step:539 [D loss: 0.543501, acc: 71.88%] [G loss: 3.241309]\n",
      "epoch:0 step:540 [D loss: 0.484762, acc: 80.47%] [G loss: 3.053020]\n",
      "epoch:0 step:541 [D loss: 0.488023, acc: 73.44%] [G loss: 3.201368]\n",
      "epoch:0 step:542 [D loss: 0.557018, acc: 71.09%] [G loss: 3.412808]\n",
      "epoch:0 step:543 [D loss: 0.435083, acc: 80.47%] [G loss: 3.556303]\n",
      "epoch:0 step:544 [D loss: 0.442425, acc: 79.69%] [G loss: 3.454805]\n",
      "epoch:0 step:545 [D loss: 0.459370, acc: 81.25%] [G loss: 3.481799]\n",
      "epoch:0 step:546 [D loss: 0.500142, acc: 75.00%] [G loss: 3.425618]\n",
      "epoch:0 step:547 [D loss: 0.388487, acc: 85.16%] [G loss: 3.277713]\n",
      "epoch:0 step:548 [D loss: 0.499090, acc: 74.22%] [G loss: 3.797662]\n",
      "epoch:0 step:549 [D loss: 0.502823, acc: 75.78%] [G loss: 3.244862]\n",
      "epoch:0 step:550 [D loss: 0.440871, acc: 78.91%] [G loss: 3.615498]\n",
      "epoch:0 step:551 [D loss: 0.539232, acc: 75.78%] [G loss: 3.386788]\n",
      "epoch:0 step:552 [D loss: 0.667303, acc: 65.62%] [G loss: 3.746944]\n",
      "epoch:0 step:553 [D loss: 0.601858, acc: 67.19%] [G loss: 3.397562]\n",
      "epoch:0 step:554 [D loss: 0.366140, acc: 87.50%] [G loss: 3.587492]\n",
      "epoch:0 step:555 [D loss: 0.560844, acc: 74.22%] [G loss: 3.533503]\n",
      "epoch:0 step:556 [D loss: 0.554009, acc: 74.22%] [G loss: 3.183404]\n",
      "epoch:0 step:557 [D loss: 0.534186, acc: 78.12%] [G loss: 3.142393]\n",
      "epoch:0 step:558 [D loss: 0.423586, acc: 82.81%] [G loss: 3.138253]\n",
      "epoch:0 step:559 [D loss: 0.562207, acc: 71.09%] [G loss: 3.036233]\n",
      "epoch:0 step:560 [D loss: 0.605738, acc: 69.53%] [G loss: 3.246592]\n",
      "epoch:0 step:561 [D loss: 0.530825, acc: 77.34%] [G loss: 2.948828]\n",
      "epoch:0 step:562 [D loss: 0.539242, acc: 77.34%] [G loss: 2.976394]\n",
      "epoch:0 step:563 [D loss: 0.449629, acc: 82.81%] [G loss: 3.094359]\n",
      "epoch:0 step:564 [D loss: 0.550800, acc: 67.19%] [G loss: 3.391269]\n",
      "epoch:0 step:565 [D loss: 0.661386, acc: 67.19%] [G loss: 3.511513]\n",
      "epoch:0 step:566 [D loss: 0.549799, acc: 70.31%] [G loss: 3.808694]\n",
      "epoch:0 step:567 [D loss: 0.417853, acc: 79.69%] [G loss: 3.521965]\n",
      "epoch:0 step:568 [D loss: 0.560676, acc: 74.22%] [G loss: 3.323461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:569 [D loss: 0.647479, acc: 66.41%] [G loss: 2.819487]\n",
      "epoch:0 step:570 [D loss: 0.511484, acc: 71.88%] [G loss: 2.906719]\n",
      "epoch:0 step:571 [D loss: 0.615256, acc: 67.97%] [G loss: 2.882071]\n",
      "epoch:0 step:572 [D loss: 0.509152, acc: 75.78%] [G loss: 3.609683]\n",
      "epoch:0 step:573 [D loss: 0.494136, acc: 77.34%] [G loss: 3.434148]\n",
      "epoch:0 step:574 [D loss: 0.618700, acc: 69.53%] [G loss: 3.028605]\n",
      "epoch:0 step:575 [D loss: 0.391867, acc: 84.38%] [G loss: 3.563564]\n",
      "epoch:0 step:576 [D loss: 0.659145, acc: 57.81%] [G loss: 3.019524]\n",
      "epoch:0 step:577 [D loss: 0.504800, acc: 75.00%] [G loss: 2.867197]\n",
      "epoch:0 step:578 [D loss: 0.643003, acc: 65.62%] [G loss: 2.801581]\n",
      "epoch:0 step:579 [D loss: 0.520549, acc: 70.31%] [G loss: 2.772907]\n",
      "epoch:0 step:580 [D loss: 0.585865, acc: 72.66%] [G loss: 3.243404]\n",
      "epoch:0 step:581 [D loss: 0.491369, acc: 76.56%] [G loss: 3.442729]\n",
      "epoch:0 step:582 [D loss: 0.363984, acc: 91.41%] [G loss: 3.766476]\n",
      "epoch:0 step:583 [D loss: 0.620248, acc: 64.84%] [G loss: 3.008011]\n",
      "epoch:0 step:584 [D loss: 0.550973, acc: 70.31%] [G loss: 3.030252]\n",
      "epoch:0 step:585 [D loss: 0.551071, acc: 75.78%] [G loss: 2.891210]\n",
      "epoch:0 step:586 [D loss: 0.483904, acc: 74.22%] [G loss: 2.982562]\n",
      "epoch:0 step:587 [D loss: 0.527483, acc: 70.31%] [G loss: 2.870331]\n",
      "epoch:0 step:588 [D loss: 0.384845, acc: 87.50%] [G loss: 3.136909]\n",
      "epoch:0 step:589 [D loss: 0.530083, acc: 75.78%] [G loss: 3.253557]\n",
      "epoch:0 step:590 [D loss: 0.580719, acc: 70.31%] [G loss: 2.987949]\n",
      "epoch:0 step:591 [D loss: 0.652746, acc: 62.50%] [G loss: 3.102960]\n",
      "epoch:0 step:592 [D loss: 0.536334, acc: 71.88%] [G loss: 2.989505]\n",
      "epoch:0 step:593 [D loss: 0.590400, acc: 71.09%] [G loss: 3.004290]\n",
      "epoch:0 step:594 [D loss: 0.625678, acc: 61.72%] [G loss: 2.812495]\n",
      "epoch:0 step:595 [D loss: 0.559547, acc: 71.09%] [G loss: 3.065142]\n",
      "epoch:0 step:596 [D loss: 0.610882, acc: 64.84%] [G loss: 3.277926]\n",
      "epoch:0 step:597 [D loss: 0.507717, acc: 78.91%] [G loss: 2.827698]\n",
      "epoch:0 step:598 [D loss: 0.592071, acc: 67.97%] [G loss: 3.213595]\n",
      "epoch:0 step:599 [D loss: 0.548506, acc: 71.09%] [G loss: 3.058251]\n",
      "epoch:0 step:600 [D loss: 0.518910, acc: 75.78%] [G loss: 2.996710]\n",
      "##############\n",
      "[4.88887111 3.92876534 9.06075217 7.61991903 6.47969336 8.31114083\n",
      " 7.47117306 7.40584341 7.98207781 6.25636498]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.562833, acc: 70.31%] [G loss: 3.087231]\n",
      "epoch:0 step:602 [D loss: 0.419707, acc: 82.81%] [G loss: 3.132207]\n",
      "epoch:0 step:603 [D loss: 0.487068, acc: 74.22%] [G loss: 3.382232]\n",
      "epoch:0 step:604 [D loss: 0.663416, acc: 65.62%] [G loss: 3.112471]\n",
      "epoch:0 step:605 [D loss: 0.519597, acc: 76.56%] [G loss: 3.129666]\n",
      "epoch:0 step:606 [D loss: 0.497286, acc: 75.00%] [G loss: 3.064682]\n",
      "epoch:0 step:607 [D loss: 0.596520, acc: 69.53%] [G loss: 2.970133]\n",
      "epoch:0 step:608 [D loss: 0.465474, acc: 78.12%] [G loss: 3.078996]\n",
      "epoch:0 step:609 [D loss: 0.402302, acc: 85.16%] [G loss: 3.264347]\n",
      "epoch:0 step:610 [D loss: 0.548703, acc: 70.31%] [G loss: 3.052078]\n",
      "epoch:0 step:611 [D loss: 0.495002, acc: 78.12%] [G loss: 3.309033]\n",
      "epoch:0 step:612 [D loss: 0.535717, acc: 73.44%] [G loss: 3.124661]\n",
      "epoch:0 step:613 [D loss: 0.420052, acc: 84.38%] [G loss: 3.017415]\n",
      "epoch:0 step:614 [D loss: 0.472870, acc: 82.81%] [G loss: 3.396437]\n",
      "epoch:0 step:615 [D loss: 0.510812, acc: 74.22%] [G loss: 2.989054]\n",
      "epoch:0 step:616 [D loss: 0.581788, acc: 67.97%] [G loss: 3.113990]\n",
      "epoch:0 step:617 [D loss: 0.414417, acc: 79.69%] [G loss: 3.526974]\n",
      "epoch:0 step:618 [D loss: 0.578866, acc: 70.31%] [G loss: 3.441174]\n",
      "epoch:0 step:619 [D loss: 0.480670, acc: 78.12%] [G loss: 3.440607]\n",
      "epoch:0 step:620 [D loss: 0.526760, acc: 75.78%] [G loss: 3.450456]\n",
      "epoch:0 step:621 [D loss: 0.667393, acc: 67.19%] [G loss: 3.150350]\n",
      "epoch:0 step:622 [D loss: 0.545787, acc: 78.91%] [G loss: 3.155411]\n",
      "epoch:0 step:623 [D loss: 0.515331, acc: 76.56%] [G loss: 3.244640]\n",
      "epoch:0 step:624 [D loss: 0.453963, acc: 80.47%] [G loss: 3.082749]\n",
      "epoch:0 step:625 [D loss: 0.543937, acc: 68.75%] [G loss: 3.131258]\n",
      "epoch:0 step:626 [D loss: 0.674720, acc: 65.62%] [G loss: 3.115650]\n",
      "epoch:0 step:627 [D loss: 0.458458, acc: 77.34%] [G loss: 3.415069]\n",
      "epoch:0 step:628 [D loss: 0.492944, acc: 76.56%] [G loss: 3.043401]\n",
      "epoch:0 step:629 [D loss: 0.572416, acc: 70.31%] [G loss: 3.175915]\n",
      "epoch:0 step:630 [D loss: 0.586744, acc: 69.53%] [G loss: 2.740331]\n",
      "epoch:0 step:631 [D loss: 0.458384, acc: 81.25%] [G loss: 3.181755]\n",
      "epoch:0 step:632 [D loss: 0.629546, acc: 69.53%] [G loss: 3.237857]\n",
      "epoch:0 step:633 [D loss: 0.535514, acc: 75.00%] [G loss: 3.179837]\n",
      "epoch:0 step:634 [D loss: 0.585003, acc: 69.53%] [G loss: 3.468169]\n",
      "epoch:0 step:635 [D loss: 0.631214, acc: 59.38%] [G loss: 3.156938]\n",
      "epoch:0 step:636 [D loss: 0.650901, acc: 63.28%] [G loss: 2.873174]\n",
      "epoch:0 step:637 [D loss: 0.523571, acc: 80.47%] [G loss: 3.063734]\n",
      "epoch:0 step:638 [D loss: 0.523877, acc: 70.31%] [G loss: 3.131732]\n",
      "epoch:0 step:639 [D loss: 0.519140, acc: 76.56%] [G loss: 3.116246]\n",
      "epoch:0 step:640 [D loss: 0.515777, acc: 72.66%] [G loss: 2.732850]\n",
      "epoch:0 step:641 [D loss: 0.466402, acc: 79.69%] [G loss: 3.077962]\n",
      "epoch:0 step:642 [D loss: 0.480943, acc: 74.22%] [G loss: 2.977317]\n",
      "epoch:0 step:643 [D loss: 0.570058, acc: 72.66%] [G loss: 3.048541]\n",
      "epoch:0 step:644 [D loss: 0.654342, acc: 62.50%] [G loss: 2.898949]\n",
      "epoch:0 step:645 [D loss: 0.575364, acc: 71.88%] [G loss: 3.026769]\n",
      "epoch:0 step:646 [D loss: 0.472367, acc: 79.69%] [G loss: 3.451017]\n",
      "epoch:0 step:647 [D loss: 0.586476, acc: 64.06%] [G loss: 3.034273]\n",
      "epoch:0 step:648 [D loss: 0.369761, acc: 84.38%] [G loss: 3.221772]\n",
      "epoch:0 step:649 [D loss: 0.603097, acc: 67.19%] [G loss: 3.512792]\n",
      "epoch:0 step:650 [D loss: 0.519733, acc: 75.78%] [G loss: 3.387973]\n",
      "epoch:0 step:651 [D loss: 0.511637, acc: 72.66%] [G loss: 3.189585]\n",
      "epoch:0 step:652 [D loss: 0.542739, acc: 72.66%] [G loss: 2.984623]\n",
      "epoch:0 step:653 [D loss: 0.533570, acc: 72.66%] [G loss: 2.942942]\n",
      "epoch:0 step:654 [D loss: 0.586259, acc: 71.09%] [G loss: 3.008332]\n",
      "epoch:0 step:655 [D loss: 0.531441, acc: 78.12%] [G loss: 3.003803]\n",
      "epoch:0 step:656 [D loss: 0.540947, acc: 74.22%] [G loss: 3.207266]\n",
      "epoch:0 step:657 [D loss: 0.547400, acc: 69.53%] [G loss: 3.301547]\n",
      "epoch:0 step:658 [D loss: 0.485237, acc: 76.56%] [G loss: 3.124624]\n",
      "epoch:0 step:659 [D loss: 0.442788, acc: 80.47%] [G loss: 3.566347]\n",
      "epoch:0 step:660 [D loss: 0.607770, acc: 62.50%] [G loss: 3.430478]\n",
      "epoch:0 step:661 [D loss: 0.590593, acc: 69.53%] [G loss: 3.377416]\n",
      "epoch:0 step:662 [D loss: 0.635997, acc: 69.53%] [G loss: 2.823793]\n",
      "epoch:0 step:663 [D loss: 0.544392, acc: 71.09%] [G loss: 2.933446]\n",
      "epoch:0 step:664 [D loss: 0.458144, acc: 82.03%] [G loss: 2.896666]\n",
      "epoch:0 step:665 [D loss: 0.542494, acc: 73.44%] [G loss: 3.192928]\n",
      "epoch:0 step:666 [D loss: 0.490384, acc: 75.78%] [G loss: 3.054397]\n",
      "epoch:0 step:667 [D loss: 0.529557, acc: 78.12%] [G loss: 3.065347]\n",
      "epoch:0 step:668 [D loss: 0.634439, acc: 64.06%] [G loss: 2.917754]\n",
      "epoch:0 step:669 [D loss: 0.555887, acc: 71.09%] [G loss: 2.968683]\n",
      "epoch:0 step:670 [D loss: 0.617495, acc: 67.97%] [G loss: 2.941982]\n",
      "epoch:0 step:671 [D loss: 0.540677, acc: 72.66%] [G loss: 2.779759]\n",
      "epoch:0 step:672 [D loss: 0.552160, acc: 75.00%] [G loss: 2.703218]\n",
      "epoch:0 step:673 [D loss: 0.567548, acc: 66.41%] [G loss: 2.964015]\n",
      "epoch:0 step:674 [D loss: 0.541223, acc: 69.53%] [G loss: 2.882932]\n",
      "epoch:0 step:675 [D loss: 0.543674, acc: 70.31%] [G loss: 2.894577]\n",
      "epoch:0 step:676 [D loss: 0.527402, acc: 73.44%] [G loss: 2.924656]\n",
      "epoch:0 step:677 [D loss: 0.454884, acc: 78.12%] [G loss: 2.760261]\n",
      "epoch:0 step:678 [D loss: 0.536704, acc: 78.91%] [G loss: 2.905727]\n",
      "epoch:0 step:679 [D loss: 0.528409, acc: 76.56%] [G loss: 2.954126]\n",
      "epoch:0 step:680 [D loss: 0.463596, acc: 78.12%] [G loss: 3.122141]\n",
      "epoch:0 step:681 [D loss: 0.628894, acc: 64.84%] [G loss: 2.822072]\n",
      "epoch:0 step:682 [D loss: 0.527513, acc: 75.78%] [G loss: 2.697742]\n",
      "epoch:0 step:683 [D loss: 0.748705, acc: 57.81%] [G loss: 2.762958]\n",
      "epoch:0 step:684 [D loss: 0.530141, acc: 75.00%] [G loss: 3.064049]\n",
      "epoch:0 step:685 [D loss: 0.507083, acc: 78.12%] [G loss: 3.087123]\n",
      "epoch:0 step:686 [D loss: 0.548443, acc: 72.66%] [G loss: 2.801503]\n",
      "epoch:0 step:687 [D loss: 0.676034, acc: 60.94%] [G loss: 3.038454]\n",
      "epoch:0 step:688 [D loss: 0.494544, acc: 76.56%] [G loss: 3.125066]\n",
      "epoch:0 step:689 [D loss: 0.482618, acc: 75.00%] [G loss: 3.043512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:690 [D loss: 0.579891, acc: 68.75%] [G loss: 3.063110]\n",
      "epoch:0 step:691 [D loss: 0.502222, acc: 77.34%] [G loss: 2.967839]\n",
      "epoch:0 step:692 [D loss: 0.569896, acc: 71.09%] [G loss: 3.351584]\n",
      "epoch:0 step:693 [D loss: 0.574343, acc: 70.31%] [G loss: 3.007631]\n",
      "epoch:0 step:694 [D loss: 0.541693, acc: 70.31%] [G loss: 2.857778]\n",
      "epoch:0 step:695 [D loss: 0.517942, acc: 75.00%] [G loss: 2.862981]\n",
      "epoch:0 step:696 [D loss: 0.531243, acc: 75.78%] [G loss: 3.130961]\n",
      "epoch:0 step:697 [D loss: 0.568307, acc: 72.66%] [G loss: 3.151494]\n",
      "epoch:0 step:698 [D loss: 0.579812, acc: 67.19%] [G loss: 2.912859]\n",
      "epoch:0 step:699 [D loss: 0.524550, acc: 79.69%] [G loss: 3.064049]\n",
      "epoch:0 step:700 [D loss: 0.524367, acc: 75.78%] [G loss: 3.369188]\n",
      "epoch:0 step:701 [D loss: 0.538128, acc: 75.78%] [G loss: 2.975641]\n",
      "epoch:0 step:702 [D loss: 0.593623, acc: 65.62%] [G loss: 2.904482]\n",
      "epoch:0 step:703 [D loss: 0.503637, acc: 75.00%] [G loss: 2.984683]\n",
      "epoch:0 step:704 [D loss: 0.610411, acc: 69.53%] [G loss: 2.897516]\n",
      "epoch:0 step:705 [D loss: 0.530409, acc: 75.78%] [G loss: 3.164982]\n",
      "epoch:0 step:706 [D loss: 0.598471, acc: 66.41%] [G loss: 3.159410]\n",
      "epoch:0 step:707 [D loss: 0.406927, acc: 80.47%] [G loss: 3.481886]\n",
      "epoch:0 step:708 [D loss: 0.501400, acc: 74.22%] [G loss: 3.023548]\n",
      "epoch:0 step:709 [D loss: 0.462928, acc: 78.12%] [G loss: 3.268157]\n",
      "epoch:0 step:710 [D loss: 0.809020, acc: 54.69%] [G loss: 2.757678]\n",
      "epoch:0 step:711 [D loss: 0.607487, acc: 73.44%] [G loss: 2.715562]\n",
      "epoch:0 step:712 [D loss: 0.624764, acc: 67.97%] [G loss: 2.894413]\n",
      "epoch:0 step:713 [D loss: 0.583717, acc: 67.97%] [G loss: 3.080179]\n",
      "epoch:0 step:714 [D loss: 0.558089, acc: 72.66%] [G loss: 2.794002]\n",
      "epoch:0 step:715 [D loss: 0.648555, acc: 67.97%] [G loss: 2.639745]\n",
      "epoch:0 step:716 [D loss: 0.570804, acc: 73.44%] [G loss: 2.879014]\n",
      "epoch:0 step:717 [D loss: 0.551722, acc: 77.34%] [G loss: 2.632980]\n",
      "epoch:0 step:718 [D loss: 0.603760, acc: 67.19%] [G loss: 3.030456]\n",
      "epoch:0 step:719 [D loss: 0.553349, acc: 70.31%] [G loss: 2.833148]\n",
      "epoch:0 step:720 [D loss: 0.560402, acc: 71.09%] [G loss: 2.947780]\n",
      "epoch:0 step:721 [D loss: 0.575703, acc: 71.09%] [G loss: 2.843493]\n",
      "epoch:0 step:722 [D loss: 0.513591, acc: 79.69%] [G loss: 3.141847]\n",
      "epoch:0 step:723 [D loss: 0.600798, acc: 68.75%] [G loss: 2.876523]\n",
      "epoch:0 step:724 [D loss: 0.575023, acc: 69.53%] [G loss: 2.870945]\n",
      "epoch:0 step:725 [D loss: 0.611341, acc: 64.06%] [G loss: 2.987210]\n",
      "epoch:0 step:726 [D loss: 0.597353, acc: 66.41%] [G loss: 3.043994]\n",
      "epoch:0 step:727 [D loss: 0.506423, acc: 78.12%] [G loss: 2.643739]\n",
      "epoch:0 step:728 [D loss: 0.658670, acc: 64.06%] [G loss: 2.962123]\n",
      "epoch:0 step:729 [D loss: 0.571693, acc: 69.53%] [G loss: 2.973330]\n",
      "epoch:0 step:730 [D loss: 0.549885, acc: 73.44%] [G loss: 3.068756]\n",
      "epoch:0 step:731 [D loss: 0.578105, acc: 68.75%] [G loss: 2.647295]\n",
      "epoch:0 step:732 [D loss: 0.479451, acc: 78.12%] [G loss: 3.155212]\n",
      "epoch:0 step:733 [D loss: 0.501080, acc: 77.34%] [G loss: 3.180295]\n",
      "epoch:0 step:734 [D loss: 0.562556, acc: 72.66%] [G loss: 2.900545]\n",
      "epoch:0 step:735 [D loss: 0.551314, acc: 71.09%] [G loss: 3.029062]\n",
      "epoch:0 step:736 [D loss: 0.618852, acc: 63.28%] [G loss: 2.948134]\n",
      "epoch:0 step:737 [D loss: 0.540353, acc: 74.22%] [G loss: 2.746396]\n",
      "epoch:0 step:738 [D loss: 0.564825, acc: 73.44%] [G loss: 2.938213]\n",
      "epoch:0 step:739 [D loss: 0.639076, acc: 65.62%] [G loss: 2.492744]\n",
      "epoch:0 step:740 [D loss: 0.571468, acc: 68.75%] [G loss: 2.801763]\n",
      "epoch:0 step:741 [D loss: 0.626026, acc: 64.06%] [G loss: 3.069735]\n",
      "epoch:0 step:742 [D loss: 0.608653, acc: 62.50%] [G loss: 2.926059]\n",
      "epoch:0 step:743 [D loss: 0.490532, acc: 75.00%] [G loss: 2.774030]\n",
      "epoch:0 step:744 [D loss: 0.576553, acc: 66.41%] [G loss: 2.629306]\n",
      "epoch:0 step:745 [D loss: 0.514124, acc: 71.09%] [G loss: 3.308770]\n",
      "epoch:0 step:746 [D loss: 0.512066, acc: 72.66%] [G loss: 3.194391]\n",
      "epoch:0 step:747 [D loss: 0.564393, acc: 68.75%] [G loss: 3.064985]\n",
      "epoch:0 step:748 [D loss: 0.533722, acc: 75.00%] [G loss: 2.862513]\n",
      "epoch:0 step:749 [D loss: 0.541629, acc: 70.31%] [G loss: 3.143038]\n",
      "epoch:0 step:750 [D loss: 0.625330, acc: 64.06%] [G loss: 3.034143]\n",
      "epoch:0 step:751 [D loss: 0.549792, acc: 67.97%] [G loss: 3.022664]\n",
      "epoch:0 step:752 [D loss: 0.409003, acc: 82.03%] [G loss: 3.038903]\n",
      "epoch:0 step:753 [D loss: 0.570221, acc: 70.31%] [G loss: 3.121510]\n",
      "epoch:0 step:754 [D loss: 0.454152, acc: 79.69%] [G loss: 3.380438]\n",
      "epoch:0 step:755 [D loss: 0.454264, acc: 82.03%] [G loss: 3.090720]\n",
      "epoch:0 step:756 [D loss: 0.542319, acc: 72.66%] [G loss: 3.324195]\n",
      "epoch:0 step:757 [D loss: 0.537579, acc: 71.88%] [G loss: 3.004147]\n",
      "epoch:0 step:758 [D loss: 0.525970, acc: 69.53%] [G loss: 2.929664]\n",
      "epoch:0 step:759 [D loss: 0.545406, acc: 71.88%] [G loss: 3.050026]\n",
      "epoch:0 step:760 [D loss: 0.459008, acc: 85.16%] [G loss: 2.915361]\n",
      "epoch:0 step:761 [D loss: 0.572169, acc: 66.41%] [G loss: 2.977646]\n",
      "epoch:0 step:762 [D loss: 0.539137, acc: 72.66%] [G loss: 2.757037]\n",
      "epoch:0 step:763 [D loss: 0.486875, acc: 78.12%] [G loss: 3.090908]\n",
      "epoch:0 step:764 [D loss: 0.422858, acc: 78.91%] [G loss: 2.924693]\n",
      "epoch:0 step:765 [D loss: 0.651534, acc: 63.28%] [G loss: 2.694787]\n",
      "epoch:0 step:766 [D loss: 0.530166, acc: 71.88%] [G loss: 2.942588]\n",
      "epoch:0 step:767 [D loss: 0.620947, acc: 60.16%] [G loss: 2.862144]\n",
      "epoch:0 step:768 [D loss: 0.571017, acc: 75.00%] [G loss: 3.254659]\n",
      "epoch:0 step:769 [D loss: 0.578925, acc: 68.75%] [G loss: 3.214869]\n",
      "epoch:0 step:770 [D loss: 0.478161, acc: 76.56%] [G loss: 3.095966]\n",
      "epoch:0 step:771 [D loss: 0.535744, acc: 77.34%] [G loss: 2.980507]\n",
      "epoch:0 step:772 [D loss: 0.576584, acc: 69.53%] [G loss: 3.037589]\n",
      "epoch:0 step:773 [D loss: 0.545385, acc: 71.09%] [G loss: 3.072918]\n",
      "epoch:0 step:774 [D loss: 0.575563, acc: 69.53%] [G loss: 3.107343]\n",
      "epoch:0 step:775 [D loss: 0.470905, acc: 81.25%] [G loss: 3.215168]\n",
      "epoch:0 step:776 [D loss: 0.459898, acc: 81.25%] [G loss: 3.054041]\n",
      "epoch:0 step:777 [D loss: 0.626533, acc: 62.50%] [G loss: 2.918075]\n",
      "epoch:0 step:778 [D loss: 0.533466, acc: 70.31%] [G loss: 3.288825]\n",
      "epoch:0 step:779 [D loss: 0.658279, acc: 62.50%] [G loss: 2.926105]\n",
      "epoch:0 step:780 [D loss: 0.554190, acc: 72.66%] [G loss: 2.790467]\n",
      "epoch:0 step:781 [D loss: 0.404732, acc: 86.72%] [G loss: 3.079336]\n",
      "epoch:0 step:782 [D loss: 0.485369, acc: 75.78%] [G loss: 3.487435]\n",
      "epoch:0 step:783 [D loss: 0.508041, acc: 75.78%] [G loss: 3.036878]\n",
      "epoch:0 step:784 [D loss: 0.758377, acc: 57.03%] [G loss: 2.595865]\n",
      "epoch:0 step:785 [D loss: 0.567800, acc: 64.84%] [G loss: 2.765683]\n",
      "epoch:0 step:786 [D loss: 0.544847, acc: 75.78%] [G loss: 2.920382]\n",
      "epoch:0 step:787 [D loss: 0.588534, acc: 67.19%] [G loss: 2.811073]\n",
      "epoch:0 step:788 [D loss: 0.529128, acc: 72.66%] [G loss: 2.700256]\n",
      "epoch:0 step:789 [D loss: 0.515769, acc: 72.66%] [G loss: 3.024345]\n",
      "epoch:0 step:790 [D loss: 0.561546, acc: 73.44%] [G loss: 2.796307]\n",
      "epoch:0 step:791 [D loss: 0.485476, acc: 76.56%] [G loss: 3.314715]\n",
      "epoch:0 step:792 [D loss: 0.507936, acc: 76.56%] [G loss: 3.156388]\n",
      "epoch:0 step:793 [D loss: 0.517445, acc: 72.66%] [G loss: 3.044369]\n",
      "epoch:0 step:794 [D loss: 0.676490, acc: 65.62%] [G loss: 2.994535]\n",
      "epoch:0 step:795 [D loss: 0.554152, acc: 72.66%] [G loss: 3.079500]\n",
      "epoch:0 step:796 [D loss: 0.542615, acc: 74.22%] [G loss: 3.314006]\n",
      "epoch:0 step:797 [D loss: 0.561277, acc: 67.97%] [G loss: 3.333787]\n",
      "epoch:0 step:798 [D loss: 0.587176, acc: 66.41%] [G loss: 2.937201]\n",
      "epoch:0 step:799 [D loss: 0.537228, acc: 77.34%] [G loss: 2.834206]\n",
      "epoch:0 step:800 [D loss: 0.663365, acc: 60.94%] [G loss: 2.926249]\n",
      "##############\n",
      "[4.26975525 3.59214847 8.47845113 7.04870611 5.67587712 7.50610607\n",
      " 6.6174164  6.84350278 7.05014461 5.41347409]\n",
      "##########\n",
      "epoch:0 step:801 [D loss: 0.483855, acc: 80.47%] [G loss: 3.250423]\n",
      "epoch:0 step:802 [D loss: 0.454896, acc: 75.78%] [G loss: 3.040602]\n",
      "epoch:0 step:803 [D loss: 0.536990, acc: 75.00%] [G loss: 3.086420]\n",
      "epoch:0 step:804 [D loss: 0.564070, acc: 74.22%] [G loss: 3.093723]\n",
      "epoch:0 step:805 [D loss: 0.460307, acc: 81.25%] [G loss: 3.024092]\n",
      "epoch:0 step:806 [D loss: 0.555875, acc: 70.31%] [G loss: 3.154457]\n",
      "epoch:0 step:807 [D loss: 0.464068, acc: 78.91%] [G loss: 3.276168]\n",
      "epoch:0 step:808 [D loss: 0.621576, acc: 63.28%] [G loss: 2.794631]\n",
      "epoch:0 step:809 [D loss: 0.610735, acc: 63.28%] [G loss: 2.899551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:810 [D loss: 0.516635, acc: 74.22%] [G loss: 2.985244]\n",
      "epoch:0 step:811 [D loss: 0.565498, acc: 68.75%] [G loss: 3.051154]\n",
      "epoch:0 step:812 [D loss: 0.522160, acc: 75.78%] [G loss: 2.958978]\n",
      "epoch:0 step:813 [D loss: 0.563401, acc: 70.31%] [G loss: 2.806603]\n",
      "epoch:0 step:814 [D loss: 0.548361, acc: 73.44%] [G loss: 2.707112]\n",
      "epoch:0 step:815 [D loss: 0.617830, acc: 65.62%] [G loss: 2.902928]\n",
      "epoch:0 step:816 [D loss: 0.591451, acc: 69.53%] [G loss: 3.035055]\n",
      "epoch:0 step:817 [D loss: 0.497606, acc: 75.78%] [G loss: 2.922482]\n",
      "epoch:0 step:818 [D loss: 0.613463, acc: 69.53%] [G loss: 2.696489]\n",
      "epoch:0 step:819 [D loss: 0.532947, acc: 74.22%] [G loss: 3.169182]\n",
      "epoch:0 step:820 [D loss: 0.558122, acc: 69.53%] [G loss: 3.012432]\n",
      "epoch:0 step:821 [D loss: 0.576836, acc: 75.78%] [G loss: 3.011184]\n",
      "epoch:0 step:822 [D loss: 0.423884, acc: 80.47%] [G loss: 3.252447]\n",
      "epoch:0 step:823 [D loss: 0.489807, acc: 79.69%] [G loss: 2.999928]\n",
      "epoch:0 step:824 [D loss: 0.571676, acc: 69.53%] [G loss: 2.998122]\n",
      "epoch:0 step:825 [D loss: 0.630160, acc: 68.75%] [G loss: 2.857041]\n",
      "epoch:0 step:826 [D loss: 0.442639, acc: 82.03%] [G loss: 3.460880]\n",
      "epoch:0 step:827 [D loss: 0.601080, acc: 66.41%] [G loss: 2.959786]\n",
      "epoch:0 step:828 [D loss: 0.592044, acc: 69.53%] [G loss: 3.180265]\n",
      "epoch:0 step:829 [D loss: 0.496250, acc: 73.44%] [G loss: 3.376820]\n",
      "epoch:0 step:830 [D loss: 0.534751, acc: 77.34%] [G loss: 3.049896]\n",
      "epoch:0 step:831 [D loss: 0.501388, acc: 75.78%] [G loss: 3.232860]\n",
      "epoch:0 step:832 [D loss: 0.496843, acc: 74.22%] [G loss: 3.403955]\n",
      "epoch:0 step:833 [D loss: 0.602934, acc: 67.19%] [G loss: 2.799790]\n",
      "epoch:0 step:834 [D loss: 0.543722, acc: 72.66%] [G loss: 2.965101]\n",
      "epoch:0 step:835 [D loss: 0.592640, acc: 69.53%] [G loss: 2.918932]\n",
      "epoch:0 step:836 [D loss: 0.534955, acc: 71.09%] [G loss: 3.149543]\n",
      "epoch:0 step:837 [D loss: 0.530877, acc: 74.22%] [G loss: 3.210001]\n",
      "epoch:0 step:838 [D loss: 0.471559, acc: 78.91%] [G loss: 2.979659]\n",
      "epoch:0 step:839 [D loss: 0.572312, acc: 71.88%] [G loss: 2.825692]\n",
      "epoch:0 step:840 [D loss: 0.558224, acc: 68.75%] [G loss: 2.638625]\n",
      "epoch:0 step:841 [D loss: 0.519761, acc: 74.22%] [G loss: 2.799817]\n",
      "epoch:0 step:842 [D loss: 0.591040, acc: 66.41%] [G loss: 3.146737]\n",
      "epoch:0 step:843 [D loss: 0.614507, acc: 67.19%] [G loss: 3.187553]\n",
      "epoch:0 step:844 [D loss: 0.566840, acc: 71.09%] [G loss: 2.915051]\n",
      "epoch:0 step:845 [D loss: 0.576162, acc: 67.97%] [G loss: 3.030813]\n",
      "epoch:0 step:846 [D loss: 0.525704, acc: 77.34%] [G loss: 2.990781]\n",
      "epoch:0 step:847 [D loss: 0.560560, acc: 71.88%] [G loss: 2.962718]\n",
      "epoch:0 step:848 [D loss: 0.521244, acc: 73.44%] [G loss: 2.893833]\n",
      "epoch:0 step:849 [D loss: 0.570241, acc: 72.66%] [G loss: 2.619923]\n",
      "epoch:0 step:850 [D loss: 0.597833, acc: 73.44%] [G loss: 2.951786]\n",
      "epoch:0 step:851 [D loss: 0.490466, acc: 75.78%] [G loss: 2.853559]\n",
      "epoch:0 step:852 [D loss: 0.545004, acc: 68.75%] [G loss: 3.174865]\n",
      "epoch:0 step:853 [D loss: 0.371708, acc: 87.50%] [G loss: 3.447599]\n",
      "epoch:0 step:854 [D loss: 0.499872, acc: 75.00%] [G loss: 3.295876]\n",
      "epoch:0 step:855 [D loss: 0.517141, acc: 68.75%] [G loss: 3.198120]\n",
      "epoch:0 step:856 [D loss: 0.524158, acc: 71.09%] [G loss: 3.143352]\n",
      "epoch:0 step:857 [D loss: 0.463928, acc: 77.34%] [G loss: 3.241915]\n",
      "epoch:0 step:858 [D loss: 0.673340, acc: 60.94%] [G loss: 3.222854]\n",
      "epoch:0 step:859 [D loss: 0.555315, acc: 63.28%] [G loss: 2.989328]\n",
      "epoch:0 step:860 [D loss: 0.566375, acc: 68.75%] [G loss: 3.233874]\n",
      "epoch:0 step:861 [D loss: 0.655635, acc: 62.50%] [G loss: 3.099309]\n",
      "epoch:0 step:862 [D loss: 0.591806, acc: 69.53%] [G loss: 2.877677]\n",
      "epoch:0 step:863 [D loss: 0.518371, acc: 75.00%] [G loss: 2.971761]\n",
      "epoch:0 step:864 [D loss: 0.565301, acc: 67.19%] [G loss: 3.029642]\n",
      "epoch:0 step:865 [D loss: 0.525065, acc: 72.66%] [G loss: 3.181450]\n",
      "epoch:0 step:866 [D loss: 0.552488, acc: 74.22%] [G loss: 2.865399]\n",
      "epoch:0 step:867 [D loss: 0.656490, acc: 61.72%] [G loss: 2.950151]\n",
      "epoch:0 step:868 [D loss: 0.645589, acc: 65.62%] [G loss: 3.050994]\n",
      "epoch:0 step:869 [D loss: 0.518102, acc: 75.78%] [G loss: 3.184618]\n",
      "epoch:0 step:870 [D loss: 0.527209, acc: 71.88%] [G loss: 2.907898]\n",
      "epoch:0 step:871 [D loss: 0.585400, acc: 74.22%] [G loss: 2.979582]\n",
      "epoch:0 step:872 [D loss: 0.546543, acc: 73.44%] [G loss: 2.891336]\n",
      "epoch:0 step:873 [D loss: 0.568178, acc: 70.31%] [G loss: 2.809511]\n",
      "epoch:0 step:874 [D loss: 0.618118, acc: 65.62%] [G loss: 2.858081]\n",
      "epoch:0 step:875 [D loss: 0.512369, acc: 75.78%] [G loss: 2.818744]\n",
      "epoch:0 step:876 [D loss: 0.632701, acc: 64.84%] [G loss: 2.833995]\n",
      "epoch:0 step:877 [D loss: 0.623294, acc: 66.41%] [G loss: 2.778706]\n",
      "epoch:0 step:878 [D loss: 0.665094, acc: 64.06%] [G loss: 2.735130]\n",
      "epoch:0 step:879 [D loss: 0.575534, acc: 67.19%] [G loss: 2.846930]\n",
      "epoch:0 step:880 [D loss: 0.627896, acc: 67.97%] [G loss: 2.547148]\n",
      "epoch:0 step:881 [D loss: 0.520607, acc: 71.09%] [G loss: 2.756293]\n",
      "epoch:0 step:882 [D loss: 0.657579, acc: 63.28%] [G loss: 2.808148]\n",
      "epoch:0 step:883 [D loss: 0.542551, acc: 74.22%] [G loss: 2.855663]\n",
      "epoch:0 step:884 [D loss: 0.530796, acc: 72.66%] [G loss: 2.936184]\n",
      "epoch:0 step:885 [D loss: 0.466060, acc: 73.44%] [G loss: 3.095543]\n",
      "epoch:0 step:886 [D loss: 0.441267, acc: 78.91%] [G loss: 3.281216]\n",
      "epoch:0 step:887 [D loss: 0.585852, acc: 71.09%] [G loss: 3.004481]\n",
      "epoch:0 step:888 [D loss: 0.524686, acc: 71.88%] [G loss: 2.962266]\n",
      "epoch:0 step:889 [D loss: 0.472989, acc: 79.69%] [G loss: 3.354978]\n",
      "epoch:0 step:890 [D loss: 0.572713, acc: 67.97%] [G loss: 3.087480]\n",
      "epoch:0 step:891 [D loss: 0.603434, acc: 68.75%] [G loss: 3.041750]\n",
      "epoch:0 step:892 [D loss: 0.759518, acc: 51.56%] [G loss: 2.525463]\n",
      "epoch:0 step:893 [D loss: 0.597604, acc: 73.44%] [G loss: 2.731272]\n",
      "epoch:0 step:894 [D loss: 0.507534, acc: 76.56%] [G loss: 3.122483]\n",
      "epoch:0 step:895 [D loss: 0.556327, acc: 67.97%] [G loss: 2.863643]\n",
      "epoch:0 step:896 [D loss: 0.544588, acc: 71.09%] [G loss: 2.723117]\n",
      "epoch:0 step:897 [D loss: 0.518217, acc: 75.00%] [G loss: 2.875060]\n",
      "epoch:0 step:898 [D loss: 0.533982, acc: 74.22%] [G loss: 2.906259]\n",
      "epoch:0 step:899 [D loss: 0.588731, acc: 73.44%] [G loss: 3.316517]\n",
      "epoch:0 step:900 [D loss: 0.623036, acc: 65.62%] [G loss: 3.128091]\n",
      "epoch:0 step:901 [D loss: 0.613448, acc: 69.53%] [G loss: 2.911942]\n",
      "epoch:0 step:902 [D loss: 0.512863, acc: 74.22%] [G loss: 2.677333]\n",
      "epoch:0 step:903 [D loss: 0.638292, acc: 64.84%] [G loss: 2.656228]\n",
      "epoch:0 step:904 [D loss: 0.551911, acc: 66.41%] [G loss: 3.019075]\n",
      "epoch:0 step:905 [D loss: 0.520288, acc: 71.88%] [G loss: 2.784109]\n",
      "epoch:0 step:906 [D loss: 0.609277, acc: 67.19%] [G loss: 2.956502]\n",
      "epoch:0 step:907 [D loss: 0.613922, acc: 60.94%] [G loss: 2.954683]\n",
      "epoch:0 step:908 [D loss: 0.574811, acc: 73.44%] [G loss: 2.887172]\n",
      "epoch:0 step:909 [D loss: 0.514713, acc: 76.56%] [G loss: 2.730629]\n",
      "epoch:0 step:910 [D loss: 0.515609, acc: 70.31%] [G loss: 2.854605]\n",
      "epoch:0 step:911 [D loss: 0.646649, acc: 66.41%] [G loss: 2.691701]\n",
      "epoch:0 step:912 [D loss: 0.581023, acc: 69.53%] [G loss: 3.158992]\n",
      "epoch:0 step:913 [D loss: 0.569247, acc: 68.75%] [G loss: 2.942156]\n",
      "epoch:0 step:914 [D loss: 0.504483, acc: 74.22%] [G loss: 3.361082]\n",
      "epoch:0 step:915 [D loss: 0.602794, acc: 67.19%] [G loss: 2.888485]\n",
      "epoch:0 step:916 [D loss: 0.579600, acc: 67.19%] [G loss: 2.913716]\n",
      "epoch:0 step:917 [D loss: 0.512503, acc: 76.56%] [G loss: 2.945023]\n",
      "epoch:0 step:918 [D loss: 0.535992, acc: 72.66%] [G loss: 3.069433]\n",
      "epoch:0 step:919 [D loss: 0.539503, acc: 68.75%] [G loss: 2.904309]\n",
      "epoch:0 step:920 [D loss: 0.865842, acc: 50.78%] [G loss: 2.834877]\n",
      "epoch:0 step:921 [D loss: 0.486212, acc: 79.69%] [G loss: 3.004199]\n",
      "epoch:0 step:922 [D loss: 0.565975, acc: 68.75%] [G loss: 2.804473]\n",
      "epoch:0 step:923 [D loss: 0.508517, acc: 76.56%] [G loss: 3.272276]\n",
      "epoch:0 step:924 [D loss: 0.498913, acc: 77.34%] [G loss: 2.977298]\n",
      "epoch:0 step:925 [D loss: 0.537936, acc: 67.19%] [G loss: 3.285006]\n",
      "epoch:0 step:926 [D loss: 0.501469, acc: 76.56%] [G loss: 2.960249]\n",
      "epoch:0 step:927 [D loss: 0.540642, acc: 75.78%] [G loss: 3.092128]\n",
      "epoch:0 step:928 [D loss: 0.610895, acc: 69.53%] [G loss: 3.573386]\n",
      "epoch:0 step:929 [D loss: 0.549254, acc: 70.31%] [G loss: 3.719661]\n",
      "epoch:0 step:930 [D loss: 0.459206, acc: 80.47%] [G loss: 3.200485]\n",
      "epoch:0 step:931 [D loss: 0.716998, acc: 58.59%] [G loss: 3.017218]\n",
      "epoch:0 step:932 [D loss: 0.674345, acc: 59.38%] [G loss: 3.079582]\n",
      "epoch:0 step:933 [D loss: 0.578043, acc: 69.53%] [G loss: 2.797032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:934 [D loss: 0.484923, acc: 78.91%] [G loss: 3.265918]\n",
      "epoch:0 step:935 [D loss: 0.479060, acc: 79.69%] [G loss: 3.165102]\n",
      "epoch:0 step:936 [D loss: 0.498743, acc: 77.34%] [G loss: 3.804025]\n",
      "epoch:0 step:937 [D loss: 0.670751, acc: 66.41%] [G loss: 3.574789]\n",
      "epoch:1 step:938 [D loss: 0.549360, acc: 72.66%] [G loss: 2.952007]\n",
      "epoch:1 step:939 [D loss: 0.546807, acc: 67.97%] [G loss: 2.885852]\n",
      "epoch:1 step:940 [D loss: 0.679765, acc: 61.72%] [G loss: 2.935273]\n",
      "epoch:1 step:941 [D loss: 0.504823, acc: 72.66%] [G loss: 2.915657]\n",
      "epoch:1 step:942 [D loss: 0.500807, acc: 75.78%] [G loss: 2.935955]\n",
      "epoch:1 step:943 [D loss: 0.543413, acc: 73.44%] [G loss: 2.754761]\n",
      "epoch:1 step:944 [D loss: 0.662080, acc: 66.41%] [G loss: 2.856697]\n",
      "epoch:1 step:945 [D loss: 0.603587, acc: 65.62%] [G loss: 2.847888]\n",
      "epoch:1 step:946 [D loss: 0.548930, acc: 66.41%] [G loss: 2.890121]\n",
      "epoch:1 step:947 [D loss: 0.646523, acc: 67.19%] [G loss: 3.024542]\n",
      "epoch:1 step:948 [D loss: 0.515722, acc: 73.44%] [G loss: 2.827213]\n",
      "epoch:1 step:949 [D loss: 0.671491, acc: 60.16%] [G loss: 3.002364]\n",
      "epoch:1 step:950 [D loss: 0.589746, acc: 67.19%] [G loss: 2.923244]\n",
      "epoch:1 step:951 [D loss: 0.549626, acc: 70.31%] [G loss: 3.055120]\n",
      "epoch:1 step:952 [D loss: 0.571104, acc: 71.88%] [G loss: 3.211592]\n",
      "epoch:1 step:953 [D loss: 0.598836, acc: 70.31%] [G loss: 2.794502]\n",
      "epoch:1 step:954 [D loss: 0.605498, acc: 65.62%] [G loss: 2.572140]\n",
      "epoch:1 step:955 [D loss: 0.550010, acc: 76.56%] [G loss: 2.676649]\n",
      "epoch:1 step:956 [D loss: 0.530701, acc: 75.00%] [G loss: 2.684757]\n",
      "epoch:1 step:957 [D loss: 0.717385, acc: 55.47%] [G loss: 2.912003]\n",
      "epoch:1 step:958 [D loss: 0.465257, acc: 79.69%] [G loss: 2.646432]\n",
      "epoch:1 step:959 [D loss: 0.589093, acc: 68.75%] [G loss: 2.943730]\n",
      "epoch:1 step:960 [D loss: 0.716138, acc: 61.72%] [G loss: 2.945797]\n",
      "epoch:1 step:961 [D loss: 0.583394, acc: 69.53%] [G loss: 2.792800]\n",
      "epoch:1 step:962 [D loss: 0.575611, acc: 73.44%] [G loss: 2.833489]\n",
      "epoch:1 step:963 [D loss: 0.587298, acc: 67.19%] [G loss: 2.644420]\n",
      "epoch:1 step:964 [D loss: 0.607751, acc: 61.72%] [G loss: 2.695803]\n",
      "epoch:1 step:965 [D loss: 0.629218, acc: 65.62%] [G loss: 2.422489]\n",
      "epoch:1 step:966 [D loss: 0.615981, acc: 68.75%] [G loss: 2.749263]\n",
      "epoch:1 step:967 [D loss: 0.687074, acc: 61.72%] [G loss: 2.578664]\n",
      "epoch:1 step:968 [D loss: 0.620839, acc: 67.97%] [G loss: 2.647285]\n",
      "epoch:1 step:969 [D loss: 0.694418, acc: 60.94%] [G loss: 2.799014]\n",
      "epoch:1 step:970 [D loss: 0.540036, acc: 72.66%] [G loss: 2.556518]\n",
      "epoch:1 step:971 [D loss: 0.560418, acc: 75.00%] [G loss: 2.846310]\n",
      "epoch:1 step:972 [D loss: 0.637989, acc: 64.06%] [G loss: 2.721513]\n",
      "epoch:1 step:973 [D loss: 0.597493, acc: 67.97%] [G loss: 2.618829]\n",
      "epoch:1 step:974 [D loss: 0.623816, acc: 65.62%] [G loss: 2.606939]\n",
      "epoch:1 step:975 [D loss: 0.627690, acc: 65.62%] [G loss: 2.682301]\n",
      "epoch:1 step:976 [D loss: 0.562162, acc: 75.00%] [G loss: 2.994025]\n",
      "epoch:1 step:977 [D loss: 0.519328, acc: 75.78%] [G loss: 2.804775]\n",
      "epoch:1 step:978 [D loss: 0.589546, acc: 68.75%] [G loss: 2.567498]\n",
      "epoch:1 step:979 [D loss: 0.572313, acc: 67.97%] [G loss: 2.436634]\n",
      "epoch:1 step:980 [D loss: 0.530172, acc: 75.78%] [G loss: 2.749342]\n",
      "epoch:1 step:981 [D loss: 0.626538, acc: 66.41%] [G loss: 2.831817]\n",
      "epoch:1 step:982 [D loss: 0.552560, acc: 69.53%] [G loss: 2.675855]\n",
      "epoch:1 step:983 [D loss: 0.628285, acc: 65.62%] [G loss: 2.903653]\n",
      "epoch:1 step:984 [D loss: 0.607055, acc: 60.94%] [G loss: 2.658896]\n",
      "epoch:1 step:985 [D loss: 0.594966, acc: 65.62%] [G loss: 2.498538]\n",
      "epoch:1 step:986 [D loss: 0.641230, acc: 65.62%] [G loss: 2.633116]\n",
      "epoch:1 step:987 [D loss: 0.704117, acc: 57.81%] [G loss: 2.679389]\n",
      "epoch:1 step:988 [D loss: 0.550443, acc: 68.75%] [G loss: 2.817139]\n",
      "epoch:1 step:989 [D loss: 0.655915, acc: 61.72%] [G loss: 2.639137]\n",
      "epoch:1 step:990 [D loss: 0.572562, acc: 70.31%] [G loss: 2.648430]\n",
      "epoch:1 step:991 [D loss: 0.673391, acc: 67.97%] [G loss: 2.782225]\n",
      "epoch:1 step:992 [D loss: 0.549928, acc: 71.88%] [G loss: 2.870202]\n",
      "epoch:1 step:993 [D loss: 0.543256, acc: 75.00%] [G loss: 2.853508]\n",
      "epoch:1 step:994 [D loss: 0.535115, acc: 72.66%] [G loss: 2.755103]\n",
      "epoch:1 step:995 [D loss: 0.625142, acc: 66.41%] [G loss: 2.629977]\n",
      "epoch:1 step:996 [D loss: 0.518080, acc: 74.22%] [G loss: 2.738323]\n",
      "epoch:1 step:997 [D loss: 0.552767, acc: 75.00%] [G loss: 2.764228]\n",
      "epoch:1 step:998 [D loss: 0.562438, acc: 73.44%] [G loss: 2.859410]\n",
      "epoch:1 step:999 [D loss: 0.689596, acc: 57.03%] [G loss: 2.684517]\n",
      "epoch:1 step:1000 [D loss: 0.604357, acc: 67.19%] [G loss: 2.644358]\n",
      "##############\n",
      "[3.83495781 2.95593839 8.05420898 6.56358201 5.38817805 7.31579691\n",
      " 6.35941368 6.41387077 6.65235407 4.93251409]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.660572, acc: 60.94%] [G loss: 2.593652]\n",
      "epoch:1 step:1002 [D loss: 0.623228, acc: 66.41%] [G loss: 2.630051]\n",
      "epoch:1 step:1003 [D loss: 0.618706, acc: 66.41%] [G loss: 2.364586]\n",
      "epoch:1 step:1004 [D loss: 0.649469, acc: 64.06%] [G loss: 2.851099]\n",
      "epoch:1 step:1005 [D loss: 0.532713, acc: 72.66%] [G loss: 2.653896]\n",
      "epoch:1 step:1006 [D loss: 0.594222, acc: 71.88%] [G loss: 2.543013]\n",
      "epoch:1 step:1007 [D loss: 0.602282, acc: 67.19%] [G loss: 2.389251]\n",
      "epoch:1 step:1008 [D loss: 0.617547, acc: 64.06%] [G loss: 2.743808]\n",
      "epoch:1 step:1009 [D loss: 0.601772, acc: 67.97%] [G loss: 2.663258]\n",
      "epoch:1 step:1010 [D loss: 0.578218, acc: 70.31%] [G loss: 2.683291]\n",
      "epoch:1 step:1011 [D loss: 0.563198, acc: 70.31%] [G loss: 3.074967]\n",
      "epoch:1 step:1012 [D loss: 0.566283, acc: 71.88%] [G loss: 3.074096]\n",
      "epoch:1 step:1013 [D loss: 0.598795, acc: 67.97%] [G loss: 3.006528]\n",
      "epoch:1 step:1014 [D loss: 0.479420, acc: 79.69%] [G loss: 3.034153]\n",
      "epoch:1 step:1015 [D loss: 0.547652, acc: 71.09%] [G loss: 3.017895]\n",
      "epoch:1 step:1016 [D loss: 0.571939, acc: 66.41%] [G loss: 3.027318]\n",
      "epoch:1 step:1017 [D loss: 0.531821, acc: 75.78%] [G loss: 3.041126]\n",
      "epoch:1 step:1018 [D loss: 0.709398, acc: 64.06%] [G loss: 2.719244]\n",
      "epoch:1 step:1019 [D loss: 0.551824, acc: 75.78%] [G loss: 2.887354]\n",
      "epoch:1 step:1020 [D loss: 0.569400, acc: 71.88%] [G loss: 2.712038]\n",
      "epoch:1 step:1021 [D loss: 0.614212, acc: 63.28%] [G loss: 2.465239]\n",
      "epoch:1 step:1022 [D loss: 0.568937, acc: 67.97%] [G loss: 2.488662]\n",
      "epoch:1 step:1023 [D loss: 0.577465, acc: 67.97%] [G loss: 2.859800]\n",
      "epoch:1 step:1024 [D loss: 0.622662, acc: 65.62%] [G loss: 2.672246]\n",
      "epoch:1 step:1025 [D loss: 0.607881, acc: 62.50%] [G loss: 2.749431]\n",
      "epoch:1 step:1026 [D loss: 0.620362, acc: 67.19%] [G loss: 3.058335]\n",
      "epoch:1 step:1027 [D loss: 0.579839, acc: 66.41%] [G loss: 2.757983]\n",
      "epoch:1 step:1028 [D loss: 0.544728, acc: 72.66%] [G loss: 2.659646]\n",
      "epoch:1 step:1029 [D loss: 0.552928, acc: 75.00%] [G loss: 2.725699]\n",
      "epoch:1 step:1030 [D loss: 0.524786, acc: 68.75%] [G loss: 2.864818]\n",
      "epoch:1 step:1031 [D loss: 0.547585, acc: 74.22%] [G loss: 2.797476]\n",
      "epoch:1 step:1032 [D loss: 0.646286, acc: 62.50%] [G loss: 2.805595]\n",
      "epoch:1 step:1033 [D loss: 0.550840, acc: 77.34%] [G loss: 3.076568]\n",
      "epoch:1 step:1034 [D loss: 0.524819, acc: 73.44%] [G loss: 2.993512]\n",
      "epoch:1 step:1035 [D loss: 0.505614, acc: 72.66%] [G loss: 3.161266]\n",
      "epoch:1 step:1036 [D loss: 0.588403, acc: 67.97%] [G loss: 2.924965]\n",
      "epoch:1 step:1037 [D loss: 0.609649, acc: 64.84%] [G loss: 3.108519]\n",
      "epoch:1 step:1038 [D loss: 0.554108, acc: 72.66%] [G loss: 2.828354]\n",
      "epoch:1 step:1039 [D loss: 0.609769, acc: 67.19%] [G loss: 2.771788]\n",
      "epoch:1 step:1040 [D loss: 0.487380, acc: 76.56%] [G loss: 2.991991]\n",
      "epoch:1 step:1041 [D loss: 0.508477, acc: 76.56%] [G loss: 2.652933]\n",
      "epoch:1 step:1042 [D loss: 0.699916, acc: 57.03%] [G loss: 2.518199]\n",
      "epoch:1 step:1043 [D loss: 0.691558, acc: 58.59%] [G loss: 2.365182]\n",
      "epoch:1 step:1044 [D loss: 0.658927, acc: 56.25%] [G loss: 2.448871]\n",
      "epoch:1 step:1045 [D loss: 0.645706, acc: 65.62%] [G loss: 2.320852]\n",
      "epoch:1 step:1046 [D loss: 0.685684, acc: 59.38%] [G loss: 2.577806]\n",
      "epoch:1 step:1047 [D loss: 0.513518, acc: 71.09%] [G loss: 2.914061]\n",
      "epoch:1 step:1048 [D loss: 0.577949, acc: 77.34%] [G loss: 2.744883]\n",
      "epoch:1 step:1049 [D loss: 0.626997, acc: 63.28%] [G loss: 2.941316]\n",
      "epoch:1 step:1050 [D loss: 0.552853, acc: 71.88%] [G loss: 2.425468]\n",
      "epoch:1 step:1051 [D loss: 0.677084, acc: 55.47%] [G loss: 2.445582]\n",
      "epoch:1 step:1052 [D loss: 0.589880, acc: 67.19%] [G loss: 2.646647]\n",
      "epoch:1 step:1053 [D loss: 0.574273, acc: 71.09%] [G loss: 2.694226]\n",
      "epoch:1 step:1054 [D loss: 0.582183, acc: 65.62%] [G loss: 2.858392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1055 [D loss: 0.620359, acc: 69.53%] [G loss: 2.927879]\n",
      "epoch:1 step:1056 [D loss: 0.547811, acc: 69.53%] [G loss: 2.831987]\n",
      "epoch:1 step:1057 [D loss: 0.672707, acc: 67.19%] [G loss: 2.900389]\n",
      "epoch:1 step:1058 [D loss: 0.657422, acc: 61.72%] [G loss: 2.613523]\n",
      "epoch:1 step:1059 [D loss: 0.632065, acc: 60.94%] [G loss: 2.568769]\n",
      "epoch:1 step:1060 [D loss: 0.489161, acc: 79.69%] [G loss: 2.591898]\n",
      "epoch:1 step:1061 [D loss: 0.661762, acc: 68.75%] [G loss: 2.774558]\n",
      "epoch:1 step:1062 [D loss: 0.460299, acc: 78.12%] [G loss: 2.833766]\n",
      "epoch:1 step:1063 [D loss: 0.571319, acc: 67.19%] [G loss: 2.786225]\n",
      "epoch:1 step:1064 [D loss: 0.540542, acc: 75.78%] [G loss: 2.787284]\n",
      "epoch:1 step:1065 [D loss: 0.538756, acc: 76.56%] [G loss: 2.725171]\n",
      "epoch:1 step:1066 [D loss: 0.669583, acc: 58.59%] [G loss: 2.508441]\n",
      "epoch:1 step:1067 [D loss: 0.633821, acc: 64.84%] [G loss: 2.623513]\n",
      "epoch:1 step:1068 [D loss: 0.501321, acc: 74.22%] [G loss: 2.772592]\n",
      "epoch:1 step:1069 [D loss: 0.569984, acc: 72.66%] [G loss: 2.735395]\n",
      "epoch:1 step:1070 [D loss: 0.546092, acc: 73.44%] [G loss: 2.851815]\n",
      "epoch:1 step:1071 [D loss: 0.637463, acc: 67.97%] [G loss: 3.512854]\n",
      "epoch:1 step:1072 [D loss: 0.484201, acc: 75.78%] [G loss: 3.251854]\n",
      "epoch:1 step:1073 [D loss: 0.589309, acc: 67.97%] [G loss: 2.813342]\n",
      "epoch:1 step:1074 [D loss: 0.671233, acc: 60.16%] [G loss: 2.674798]\n",
      "epoch:1 step:1075 [D loss: 0.511559, acc: 80.47%] [G loss: 2.757077]\n",
      "epoch:1 step:1076 [D loss: 0.618477, acc: 64.06%] [G loss: 2.852503]\n",
      "epoch:1 step:1077 [D loss: 0.634562, acc: 65.62%] [G loss: 3.078717]\n",
      "epoch:1 step:1078 [D loss: 0.513566, acc: 73.44%] [G loss: 2.933612]\n",
      "epoch:1 step:1079 [D loss: 0.530254, acc: 70.31%] [G loss: 2.630107]\n",
      "epoch:1 step:1080 [D loss: 0.596621, acc: 65.62%] [G loss: 2.575533]\n",
      "epoch:1 step:1081 [D loss: 0.688827, acc: 57.81%] [G loss: 2.740372]\n",
      "epoch:1 step:1082 [D loss: 0.596784, acc: 65.62%] [G loss: 2.807192]\n",
      "epoch:1 step:1083 [D loss: 0.576599, acc: 67.19%] [G loss: 2.700431]\n",
      "epoch:1 step:1084 [D loss: 0.513365, acc: 75.00%] [G loss: 2.524832]\n",
      "epoch:1 step:1085 [D loss: 0.579222, acc: 68.75%] [G loss: 2.876085]\n",
      "epoch:1 step:1086 [D loss: 0.480383, acc: 76.56%] [G loss: 2.990019]\n",
      "epoch:1 step:1087 [D loss: 0.638324, acc: 60.94%] [G loss: 3.014309]\n",
      "epoch:1 step:1088 [D loss: 0.562396, acc: 64.84%] [G loss: 3.225585]\n",
      "epoch:1 step:1089 [D loss: 0.582095, acc: 67.97%] [G loss: 3.151122]\n",
      "epoch:1 step:1090 [D loss: 0.663188, acc: 62.50%] [G loss: 2.558042]\n",
      "epoch:1 step:1091 [D loss: 0.611402, acc: 67.97%] [G loss: 2.740165]\n",
      "epoch:1 step:1092 [D loss: 0.529552, acc: 75.78%] [G loss: 2.920175]\n",
      "epoch:1 step:1093 [D loss: 0.674604, acc: 60.94%] [G loss: 2.534980]\n",
      "epoch:1 step:1094 [D loss: 0.495203, acc: 76.56%] [G loss: 2.614633]\n",
      "epoch:1 step:1095 [D loss: 0.681035, acc: 59.38%] [G loss: 2.625280]\n",
      "epoch:1 step:1096 [D loss: 0.562190, acc: 69.53%] [G loss: 2.535916]\n",
      "epoch:1 step:1097 [D loss: 0.574448, acc: 72.66%] [G loss: 2.822888]\n",
      "epoch:1 step:1098 [D loss: 0.645695, acc: 64.84%] [G loss: 3.037874]\n",
      "epoch:1 step:1099 [D loss: 0.500004, acc: 72.66%] [G loss: 3.144795]\n",
      "epoch:1 step:1100 [D loss: 0.601962, acc: 65.62%] [G loss: 2.535149]\n",
      "epoch:1 step:1101 [D loss: 0.565839, acc: 67.97%] [G loss: 2.888632]\n",
      "epoch:1 step:1102 [D loss: 0.548713, acc: 75.00%] [G loss: 2.822218]\n",
      "epoch:1 step:1103 [D loss: 0.528921, acc: 74.22%] [G loss: 2.790489]\n",
      "epoch:1 step:1104 [D loss: 0.563003, acc: 72.66%] [G loss: 2.748329]\n",
      "epoch:1 step:1105 [D loss: 0.544433, acc: 72.66%] [G loss: 2.908091]\n",
      "epoch:1 step:1106 [D loss: 0.480782, acc: 78.91%] [G loss: 2.547644]\n",
      "epoch:1 step:1107 [D loss: 0.592208, acc: 68.75%] [G loss: 2.734525]\n",
      "epoch:1 step:1108 [D loss: 0.550833, acc: 71.09%] [G loss: 2.929978]\n",
      "epoch:1 step:1109 [D loss: 0.583898, acc: 70.31%] [G loss: 2.939768]\n",
      "epoch:1 step:1110 [D loss: 0.587428, acc: 68.75%] [G loss: 2.753809]\n",
      "epoch:1 step:1111 [D loss: 0.594455, acc: 69.53%] [G loss: 2.705723]\n",
      "epoch:1 step:1112 [D loss: 0.570704, acc: 70.31%] [G loss: 2.774522]\n",
      "epoch:1 step:1113 [D loss: 0.586477, acc: 64.06%] [G loss: 2.791348]\n",
      "epoch:1 step:1114 [D loss: 0.543070, acc: 75.00%] [G loss: 2.526098]\n",
      "epoch:1 step:1115 [D loss: 0.502424, acc: 74.22%] [G loss: 2.881616]\n",
      "epoch:1 step:1116 [D loss: 0.586766, acc: 67.19%] [G loss: 2.847471]\n",
      "epoch:1 step:1117 [D loss: 0.590102, acc: 71.09%] [G loss: 2.596274]\n",
      "epoch:1 step:1118 [D loss: 0.540299, acc: 75.00%] [G loss: 2.774160]\n",
      "epoch:1 step:1119 [D loss: 0.639443, acc: 63.28%] [G loss: 2.580828]\n",
      "epoch:1 step:1120 [D loss: 0.567840, acc: 68.75%] [G loss: 2.717224]\n",
      "epoch:1 step:1121 [D loss: 0.498625, acc: 77.34%] [G loss: 2.658683]\n",
      "epoch:1 step:1122 [D loss: 0.744137, acc: 59.38%] [G loss: 2.581824]\n",
      "epoch:1 step:1123 [D loss: 0.594519, acc: 67.97%] [G loss: 2.686279]\n",
      "epoch:1 step:1124 [D loss: 0.715098, acc: 57.03%] [G loss: 2.704363]\n",
      "epoch:1 step:1125 [D loss: 0.551060, acc: 70.31%] [G loss: 2.752210]\n",
      "epoch:1 step:1126 [D loss: 0.564469, acc: 71.09%] [G loss: 2.727932]\n",
      "epoch:1 step:1127 [D loss: 0.607538, acc: 69.53%] [G loss: 2.796098]\n",
      "epoch:1 step:1128 [D loss: 0.628244, acc: 64.84%] [G loss: 2.481498]\n",
      "epoch:1 step:1129 [D loss: 0.599142, acc: 67.97%] [G loss: 2.509146]\n",
      "epoch:1 step:1130 [D loss: 0.572141, acc: 68.75%] [G loss: 2.568707]\n",
      "epoch:1 step:1131 [D loss: 0.471712, acc: 74.22%] [G loss: 3.207151]\n",
      "epoch:1 step:1132 [D loss: 0.526428, acc: 71.88%] [G loss: 2.852453]\n",
      "epoch:1 step:1133 [D loss: 0.624951, acc: 62.50%] [G loss: 2.560875]\n",
      "epoch:1 step:1134 [D loss: 0.648759, acc: 63.28%] [G loss: 2.867982]\n",
      "epoch:1 step:1135 [D loss: 0.572137, acc: 72.66%] [G loss: 2.862047]\n",
      "epoch:1 step:1136 [D loss: 0.593557, acc: 64.84%] [G loss: 2.691495]\n",
      "epoch:1 step:1137 [D loss: 0.680770, acc: 62.50%] [G loss: 2.598694]\n",
      "epoch:1 step:1138 [D loss: 0.565058, acc: 71.88%] [G loss: 2.636909]\n",
      "epoch:1 step:1139 [D loss: 0.553992, acc: 73.44%] [G loss: 2.410626]\n",
      "epoch:1 step:1140 [D loss: 0.689573, acc: 58.59%] [G loss: 2.562730]\n",
      "epoch:1 step:1141 [D loss: 0.557441, acc: 69.53%] [G loss: 3.057986]\n",
      "epoch:1 step:1142 [D loss: 0.546031, acc: 75.78%] [G loss: 2.929845]\n",
      "epoch:1 step:1143 [D loss: 0.528251, acc: 71.09%] [G loss: 2.714823]\n",
      "epoch:1 step:1144 [D loss: 0.473170, acc: 75.78%] [G loss: 3.036709]\n",
      "epoch:1 step:1145 [D loss: 0.516574, acc: 76.56%] [G loss: 3.142333]\n",
      "epoch:1 step:1146 [D loss: 0.584380, acc: 72.66%] [G loss: 2.888497]\n",
      "epoch:1 step:1147 [D loss: 0.580222, acc: 69.53%] [G loss: 2.702291]\n",
      "epoch:1 step:1148 [D loss: 0.601396, acc: 66.41%] [G loss: 2.689306]\n",
      "epoch:1 step:1149 [D loss: 0.508245, acc: 75.00%] [G loss: 2.750955]\n",
      "epoch:1 step:1150 [D loss: 0.556208, acc: 67.19%] [G loss: 2.815211]\n",
      "epoch:1 step:1151 [D loss: 0.604997, acc: 64.84%] [G loss: 2.589816]\n",
      "epoch:1 step:1152 [D loss: 0.643557, acc: 64.84%] [G loss: 2.473366]\n",
      "epoch:1 step:1153 [D loss: 0.565436, acc: 70.31%] [G loss: 2.478128]\n",
      "epoch:1 step:1154 [D loss: 0.612132, acc: 71.09%] [G loss: 2.815132]\n",
      "epoch:1 step:1155 [D loss: 0.632268, acc: 67.97%] [G loss: 2.553469]\n",
      "epoch:1 step:1156 [D loss: 0.658766, acc: 63.28%] [G loss: 2.697671]\n",
      "epoch:1 step:1157 [D loss: 0.595850, acc: 67.19%] [G loss: 2.853071]\n",
      "epoch:1 step:1158 [D loss: 0.622661, acc: 64.84%] [G loss: 2.666574]\n",
      "epoch:1 step:1159 [D loss: 0.530423, acc: 72.66%] [G loss: 2.919855]\n",
      "epoch:1 step:1160 [D loss: 0.587893, acc: 69.53%] [G loss: 2.790386]\n",
      "epoch:1 step:1161 [D loss: 0.594175, acc: 69.53%] [G loss: 2.776116]\n",
      "epoch:1 step:1162 [D loss: 0.628500, acc: 60.94%] [G loss: 2.714437]\n",
      "epoch:1 step:1163 [D loss: 0.660337, acc: 67.97%] [G loss: 2.661500]\n",
      "epoch:1 step:1164 [D loss: 0.639251, acc: 64.06%] [G loss: 2.666475]\n",
      "epoch:1 step:1165 [D loss: 0.629300, acc: 67.97%] [G loss: 2.417629]\n",
      "epoch:1 step:1166 [D loss: 0.631762, acc: 66.41%] [G loss: 2.760159]\n",
      "epoch:1 step:1167 [D loss: 0.538012, acc: 78.12%] [G loss: 2.740224]\n",
      "epoch:1 step:1168 [D loss: 0.580509, acc: 67.19%] [G loss: 2.895742]\n",
      "epoch:1 step:1169 [D loss: 0.455809, acc: 79.69%] [G loss: 3.228182]\n",
      "epoch:1 step:1170 [D loss: 0.646669, acc: 66.41%] [G loss: 2.897985]\n",
      "epoch:1 step:1171 [D loss: 0.673383, acc: 61.72%] [G loss: 2.790048]\n",
      "epoch:1 step:1172 [D loss: 0.571757, acc: 68.75%] [G loss: 2.596732]\n",
      "epoch:1 step:1173 [D loss: 0.511773, acc: 75.78%] [G loss: 2.582888]\n",
      "epoch:1 step:1174 [D loss: 0.610853, acc: 67.97%] [G loss: 2.580245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1175 [D loss: 0.598570, acc: 65.62%] [G loss: 2.474642]\n",
      "epoch:1 step:1176 [D loss: 0.606836, acc: 65.62%] [G loss: 2.334166]\n",
      "epoch:1 step:1177 [D loss: 0.596642, acc: 69.53%] [G loss: 2.734010]\n",
      "epoch:1 step:1178 [D loss: 0.607489, acc: 65.62%] [G loss: 2.672279]\n",
      "epoch:1 step:1179 [D loss: 0.654004, acc: 64.06%] [G loss: 2.711282]\n",
      "epoch:1 step:1180 [D loss: 0.571576, acc: 69.53%] [G loss: 2.671242]\n",
      "epoch:1 step:1181 [D loss: 0.633869, acc: 67.97%] [G loss: 2.616117]\n",
      "epoch:1 step:1182 [D loss: 0.624395, acc: 64.06%] [G loss: 2.732100]\n",
      "epoch:1 step:1183 [D loss: 0.626276, acc: 63.28%] [G loss: 2.414073]\n",
      "epoch:1 step:1184 [D loss: 0.638150, acc: 64.06%] [G loss: 2.500195]\n",
      "epoch:1 step:1185 [D loss: 0.652821, acc: 60.16%] [G loss: 2.356114]\n",
      "epoch:1 step:1186 [D loss: 0.535087, acc: 75.00%] [G loss: 2.683843]\n",
      "epoch:1 step:1187 [D loss: 0.672968, acc: 60.16%] [G loss: 2.953221]\n",
      "epoch:1 step:1188 [D loss: 0.538939, acc: 74.22%] [G loss: 2.559220]\n",
      "epoch:1 step:1189 [D loss: 0.634821, acc: 66.41%] [G loss: 2.513572]\n",
      "epoch:1 step:1190 [D loss: 0.583948, acc: 66.41%] [G loss: 2.465084]\n",
      "epoch:1 step:1191 [D loss: 0.617968, acc: 67.19%] [G loss: 2.532619]\n",
      "epoch:1 step:1192 [D loss: 0.546381, acc: 74.22%] [G loss: 2.678082]\n",
      "epoch:1 step:1193 [D loss: 0.591295, acc: 67.19%] [G loss: 2.711301]\n",
      "epoch:1 step:1194 [D loss: 0.554570, acc: 71.88%] [G loss: 2.608429]\n",
      "epoch:1 step:1195 [D loss: 0.575523, acc: 67.97%] [G loss: 2.788624]\n",
      "epoch:1 step:1196 [D loss: 0.573690, acc: 71.88%] [G loss: 2.741351]\n",
      "epoch:1 step:1197 [D loss: 0.607916, acc: 64.06%] [G loss: 2.506188]\n",
      "epoch:1 step:1198 [D loss: 0.593287, acc: 65.62%] [G loss: 3.011294]\n",
      "epoch:1 step:1199 [D loss: 0.593419, acc: 64.84%] [G loss: 2.639391]\n",
      "epoch:1 step:1200 [D loss: 0.770296, acc: 52.34%] [G loss: 2.346871]\n",
      "##############\n",
      "[3.60654741 2.79068691 7.98409295 6.28257957 5.11866514 6.81227408\n",
      " 5.87725841 6.20674743 6.43334426 4.62516794]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.617934, acc: 64.84%] [G loss: 2.607870]\n",
      "epoch:1 step:1202 [D loss: 0.621211, acc: 64.06%] [G loss: 2.565536]\n",
      "epoch:1 step:1203 [D loss: 0.523286, acc: 75.00%] [G loss: 2.382122]\n",
      "epoch:1 step:1204 [D loss: 0.687079, acc: 60.94%] [G loss: 2.326805]\n",
      "epoch:1 step:1205 [D loss: 0.686047, acc: 57.81%] [G loss: 2.346192]\n",
      "epoch:1 step:1206 [D loss: 0.687098, acc: 60.16%] [G loss: 2.203372]\n",
      "epoch:1 step:1207 [D loss: 0.540479, acc: 68.75%] [G loss: 2.389553]\n",
      "epoch:1 step:1208 [D loss: 0.535858, acc: 74.22%] [G loss: 2.464336]\n",
      "epoch:1 step:1209 [D loss: 0.605201, acc: 64.06%] [G loss: 2.528325]\n",
      "epoch:1 step:1210 [D loss: 0.546235, acc: 73.44%] [G loss: 2.900265]\n",
      "epoch:1 step:1211 [D loss: 0.620779, acc: 64.06%] [G loss: 2.641039]\n",
      "epoch:1 step:1212 [D loss: 0.598688, acc: 64.84%] [G loss: 2.525245]\n",
      "epoch:1 step:1213 [D loss: 0.640094, acc: 60.16%] [G loss: 2.418942]\n",
      "epoch:1 step:1214 [D loss: 0.626802, acc: 60.94%] [G loss: 2.614924]\n",
      "epoch:1 step:1215 [D loss: 0.628903, acc: 67.19%] [G loss: 2.677741]\n",
      "epoch:1 step:1216 [D loss: 0.573604, acc: 67.97%] [G loss: 2.633206]\n",
      "epoch:1 step:1217 [D loss: 0.494680, acc: 78.12%] [G loss: 2.607435]\n",
      "epoch:1 step:1218 [D loss: 0.648438, acc: 67.19%] [G loss: 2.495430]\n",
      "epoch:1 step:1219 [D loss: 0.686535, acc: 59.38%] [G loss: 2.352042]\n",
      "epoch:1 step:1220 [D loss: 0.636407, acc: 68.75%] [G loss: 2.457456]\n",
      "epoch:1 step:1221 [D loss: 0.534797, acc: 71.09%] [G loss: 2.754230]\n",
      "epoch:1 step:1222 [D loss: 0.466488, acc: 77.34%] [G loss: 3.017002]\n",
      "epoch:1 step:1223 [D loss: 0.629254, acc: 62.50%] [G loss: 2.697081]\n",
      "epoch:1 step:1224 [D loss: 0.566622, acc: 67.97%] [G loss: 2.677075]\n",
      "epoch:1 step:1225 [D loss: 0.608879, acc: 65.62%] [G loss: 2.621068]\n",
      "epoch:1 step:1226 [D loss: 0.575823, acc: 64.84%] [G loss: 2.802773]\n",
      "epoch:1 step:1227 [D loss: 0.661359, acc: 64.84%] [G loss: 2.695404]\n",
      "epoch:1 step:1228 [D loss: 0.535842, acc: 75.00%] [G loss: 2.518812]\n",
      "epoch:1 step:1229 [D loss: 0.595638, acc: 67.19%] [G loss: 2.736082]\n",
      "epoch:1 step:1230 [D loss: 0.544587, acc: 73.44%] [G loss: 2.624055]\n",
      "epoch:1 step:1231 [D loss: 0.577043, acc: 68.75%] [G loss: 2.442249]\n",
      "epoch:1 step:1232 [D loss: 0.672686, acc: 60.16%] [G loss: 2.527082]\n",
      "epoch:1 step:1233 [D loss: 0.562442, acc: 70.31%] [G loss: 2.705358]\n",
      "epoch:1 step:1234 [D loss: 0.645755, acc: 67.97%] [G loss: 2.390772]\n",
      "epoch:1 step:1235 [D loss: 0.618955, acc: 63.28%] [G loss: 2.795705]\n",
      "epoch:1 step:1236 [D loss: 0.577162, acc: 67.19%] [G loss: 2.496299]\n",
      "epoch:1 step:1237 [D loss: 0.544967, acc: 75.00%] [G loss: 2.671649]\n",
      "epoch:1 step:1238 [D loss: 0.713165, acc: 57.03%] [G loss: 2.455772]\n",
      "epoch:1 step:1239 [D loss: 0.561665, acc: 71.09%] [G loss: 2.761876]\n",
      "epoch:1 step:1240 [D loss: 0.589858, acc: 67.97%] [G loss: 2.560173]\n",
      "epoch:1 step:1241 [D loss: 0.492025, acc: 77.34%] [G loss: 2.981987]\n",
      "epoch:1 step:1242 [D loss: 0.573615, acc: 71.88%] [G loss: 2.719709]\n",
      "epoch:1 step:1243 [D loss: 0.590888, acc: 68.75%] [G loss: 2.720159]\n",
      "epoch:1 step:1244 [D loss: 0.533276, acc: 73.44%] [G loss: 3.001946]\n",
      "epoch:1 step:1245 [D loss: 0.582541, acc: 71.88%] [G loss: 2.607972]\n",
      "epoch:1 step:1246 [D loss: 0.561600, acc: 74.22%] [G loss: 2.642177]\n",
      "epoch:1 step:1247 [D loss: 0.512328, acc: 75.00%] [G loss: 2.782002]\n",
      "epoch:1 step:1248 [D loss: 0.545644, acc: 77.34%] [G loss: 2.650050]\n",
      "epoch:1 step:1249 [D loss: 0.452988, acc: 82.03%] [G loss: 3.036960]\n",
      "epoch:1 step:1250 [D loss: 0.538812, acc: 72.66%] [G loss: 3.017179]\n",
      "epoch:1 step:1251 [D loss: 0.512490, acc: 72.66%] [G loss: 3.366646]\n",
      "epoch:1 step:1252 [D loss: 0.543074, acc: 71.09%] [G loss: 3.195114]\n",
      "epoch:1 step:1253 [D loss: 0.659580, acc: 63.28%] [G loss: 2.454373]\n",
      "epoch:1 step:1254 [D loss: 0.568092, acc: 70.31%] [G loss: 2.644532]\n",
      "epoch:1 step:1255 [D loss: 0.646403, acc: 65.62%] [G loss: 2.315032]\n",
      "epoch:1 step:1256 [D loss: 0.522870, acc: 75.78%] [G loss: 2.827022]\n",
      "epoch:1 step:1257 [D loss: 0.609298, acc: 67.19%] [G loss: 2.684672]\n",
      "epoch:1 step:1258 [D loss: 0.622445, acc: 71.09%] [G loss: 2.728822]\n",
      "epoch:1 step:1259 [D loss: 0.555064, acc: 75.00%] [G loss: 2.519514]\n",
      "epoch:1 step:1260 [D loss: 0.584071, acc: 65.62%] [G loss: 2.744343]\n",
      "epoch:1 step:1261 [D loss: 0.652350, acc: 67.19%] [G loss: 2.965290]\n",
      "epoch:1 step:1262 [D loss: 0.535259, acc: 70.31%] [G loss: 2.905560]\n",
      "epoch:1 step:1263 [D loss: 0.648436, acc: 60.16%] [G loss: 2.691855]\n",
      "epoch:1 step:1264 [D loss: 0.582418, acc: 67.19%] [G loss: 2.788324]\n",
      "epoch:1 step:1265 [D loss: 0.594453, acc: 66.41%] [G loss: 2.734973]\n",
      "epoch:1 step:1266 [D loss: 0.584587, acc: 67.97%] [G loss: 2.593686]\n",
      "epoch:1 step:1267 [D loss: 0.608619, acc: 67.19%] [G loss: 2.571843]\n",
      "epoch:1 step:1268 [D loss: 0.558375, acc: 72.66%] [G loss: 2.636981]\n",
      "epoch:1 step:1269 [D loss: 0.533900, acc: 77.34%] [G loss: 2.737847]\n",
      "epoch:1 step:1270 [D loss: 0.555576, acc: 68.75%] [G loss: 2.837480]\n",
      "epoch:1 step:1271 [D loss: 0.611722, acc: 67.97%] [G loss: 2.777116]\n",
      "epoch:1 step:1272 [D loss: 0.527638, acc: 69.53%] [G loss: 2.971677]\n",
      "epoch:1 step:1273 [D loss: 0.485416, acc: 82.03%] [G loss: 3.107971]\n",
      "epoch:1 step:1274 [D loss: 0.548800, acc: 72.66%] [G loss: 2.857773]\n",
      "epoch:1 step:1275 [D loss: 0.644481, acc: 63.28%] [G loss: 2.568552]\n",
      "epoch:1 step:1276 [D loss: 0.580931, acc: 70.31%] [G loss: 2.757907]\n",
      "epoch:1 step:1277 [D loss: 0.532510, acc: 74.22%] [G loss: 2.608335]\n",
      "epoch:1 step:1278 [D loss: 0.775374, acc: 54.69%] [G loss: 2.918186]\n",
      "epoch:1 step:1279 [D loss: 0.509011, acc: 76.56%] [G loss: 2.634985]\n",
      "epoch:1 step:1280 [D loss: 0.424402, acc: 84.38%] [G loss: 3.098078]\n",
      "epoch:1 step:1281 [D loss: 0.415566, acc: 80.47%] [G loss: 3.390659]\n",
      "epoch:1 step:1282 [D loss: 0.552200, acc: 69.53%] [G loss: 2.917104]\n",
      "epoch:1 step:1283 [D loss: 0.460260, acc: 80.47%] [G loss: 3.217209]\n",
      "epoch:1 step:1284 [D loss: 0.488310, acc: 77.34%] [G loss: 3.340868]\n",
      "epoch:1 step:1285 [D loss: 0.575897, acc: 74.22%] [G loss: 3.299563]\n",
      "epoch:1 step:1286 [D loss: 0.658748, acc: 63.28%] [G loss: 2.533408]\n",
      "epoch:1 step:1287 [D loss: 0.561748, acc: 72.66%] [G loss: 2.929687]\n",
      "epoch:1 step:1288 [D loss: 0.498116, acc: 77.34%] [G loss: 2.859820]\n",
      "epoch:1 step:1289 [D loss: 0.592567, acc: 65.62%] [G loss: 2.859382]\n",
      "epoch:1 step:1290 [D loss: 0.577327, acc: 69.53%] [G loss: 3.155419]\n",
      "epoch:1 step:1291 [D loss: 0.579864, acc: 67.97%] [G loss: 3.032595]\n",
      "epoch:1 step:1292 [D loss: 0.527162, acc: 71.09%] [G loss: 2.852334]\n",
      "epoch:1 step:1293 [D loss: 0.543269, acc: 73.44%] [G loss: 2.913392]\n",
      "epoch:1 step:1294 [D loss: 0.651758, acc: 59.38%] [G loss: 2.835853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1295 [D loss: 0.450289, acc: 78.91%] [G loss: 2.892880]\n",
      "epoch:1 step:1296 [D loss: 0.455973, acc: 79.69%] [G loss: 3.248524]\n",
      "epoch:1 step:1297 [D loss: 0.541609, acc: 75.78%] [G loss: 2.951606]\n",
      "epoch:1 step:1298 [D loss: 0.591506, acc: 72.66%] [G loss: 2.915802]\n",
      "epoch:1 step:1299 [D loss: 0.674531, acc: 59.38%] [G loss: 2.865931]\n",
      "epoch:1 step:1300 [D loss: 0.565137, acc: 66.41%] [G loss: 2.861081]\n",
      "epoch:1 step:1301 [D loss: 0.534850, acc: 72.66%] [G loss: 2.940967]\n",
      "epoch:1 step:1302 [D loss: 0.576014, acc: 67.97%] [G loss: 2.758098]\n",
      "epoch:1 step:1303 [D loss: 0.505833, acc: 78.12%] [G loss: 3.333090]\n",
      "epoch:1 step:1304 [D loss: 0.567591, acc: 66.41%] [G loss: 2.970803]\n",
      "epoch:1 step:1305 [D loss: 0.668139, acc: 62.50%] [G loss: 3.062988]\n",
      "epoch:1 step:1306 [D loss: 0.618925, acc: 64.84%] [G loss: 2.834418]\n",
      "epoch:1 step:1307 [D loss: 0.572847, acc: 71.88%] [G loss: 2.526513]\n",
      "epoch:1 step:1308 [D loss: 0.615022, acc: 65.62%] [G loss: 2.546606]\n",
      "epoch:1 step:1309 [D loss: 0.563505, acc: 69.53%] [G loss: 2.564158]\n",
      "epoch:1 step:1310 [D loss: 0.593353, acc: 67.97%] [G loss: 2.762649]\n",
      "epoch:1 step:1311 [D loss: 0.569848, acc: 66.41%] [G loss: 2.670246]\n",
      "epoch:1 step:1312 [D loss: 0.637132, acc: 70.31%] [G loss: 2.630127]\n",
      "epoch:1 step:1313 [D loss: 0.651648, acc: 65.62%] [G loss: 2.842703]\n",
      "epoch:1 step:1314 [D loss: 0.589245, acc: 65.62%] [G loss: 2.794033]\n",
      "epoch:1 step:1315 [D loss: 0.539683, acc: 71.88%] [G loss: 2.927670]\n",
      "epoch:1 step:1316 [D loss: 0.568631, acc: 67.97%] [G loss: 2.795422]\n",
      "epoch:1 step:1317 [D loss: 0.590259, acc: 71.09%] [G loss: 3.029247]\n",
      "epoch:1 step:1318 [D loss: 0.555182, acc: 71.88%] [G loss: 3.215282]\n",
      "epoch:1 step:1319 [D loss: 0.524181, acc: 72.66%] [G loss: 2.956246]\n",
      "epoch:1 step:1320 [D loss: 0.536307, acc: 72.66%] [G loss: 3.028554]\n",
      "epoch:1 step:1321 [D loss: 0.547797, acc: 71.88%] [G loss: 3.122753]\n",
      "epoch:1 step:1322 [D loss: 0.550478, acc: 68.75%] [G loss: 2.990959]\n",
      "epoch:1 step:1323 [D loss: 0.517766, acc: 71.88%] [G loss: 2.699856]\n",
      "epoch:1 step:1324 [D loss: 0.578236, acc: 68.75%] [G loss: 2.888844]\n",
      "epoch:1 step:1325 [D loss: 0.513803, acc: 73.44%] [G loss: 3.097821]\n",
      "epoch:1 step:1326 [D loss: 0.591073, acc: 68.75%] [G loss: 2.729107]\n",
      "epoch:1 step:1327 [D loss: 0.666532, acc: 59.38%] [G loss: 2.501654]\n",
      "epoch:1 step:1328 [D loss: 0.568300, acc: 64.84%] [G loss: 2.755010]\n",
      "epoch:1 step:1329 [D loss: 0.551004, acc: 72.66%] [G loss: 3.005044]\n",
      "epoch:1 step:1330 [D loss: 0.543313, acc: 75.00%] [G loss: 2.743541]\n",
      "epoch:1 step:1331 [D loss: 0.548125, acc: 69.53%] [G loss: 2.682691]\n",
      "epoch:1 step:1332 [D loss: 0.555541, acc: 71.88%] [G loss: 2.749239]\n",
      "epoch:1 step:1333 [D loss: 0.674094, acc: 64.84%] [G loss: 2.737349]\n",
      "epoch:1 step:1334 [D loss: 0.468816, acc: 78.12%] [G loss: 3.130056]\n",
      "epoch:1 step:1335 [D loss: 0.564547, acc: 72.66%] [G loss: 3.560302]\n",
      "epoch:1 step:1336 [D loss: 0.455518, acc: 79.69%] [G loss: 2.945724]\n",
      "epoch:1 step:1337 [D loss: 0.521786, acc: 76.56%] [G loss: 3.032450]\n",
      "epoch:1 step:1338 [D loss: 0.525349, acc: 75.00%] [G loss: 2.892183]\n",
      "epoch:1 step:1339 [D loss: 0.542610, acc: 72.66%] [G loss: 3.320763]\n",
      "epoch:1 step:1340 [D loss: 0.545607, acc: 73.44%] [G loss: 3.052477]\n",
      "epoch:1 step:1341 [D loss: 0.670213, acc: 63.28%] [G loss: 2.860220]\n",
      "epoch:1 step:1342 [D loss: 0.524038, acc: 73.44%] [G loss: 3.004086]\n",
      "epoch:1 step:1343 [D loss: 0.557498, acc: 68.75%] [G loss: 3.008033]\n",
      "epoch:1 step:1344 [D loss: 0.602907, acc: 63.28%] [G loss: 2.834021]\n",
      "epoch:1 step:1345 [D loss: 0.524569, acc: 76.56%] [G loss: 2.981653]\n",
      "epoch:1 step:1346 [D loss: 0.519125, acc: 71.88%] [G loss: 3.061476]\n",
      "epoch:1 step:1347 [D loss: 0.618086, acc: 61.72%] [G loss: 2.842728]\n",
      "epoch:1 step:1348 [D loss: 0.582636, acc: 70.31%] [G loss: 2.708792]\n",
      "epoch:1 step:1349 [D loss: 0.625705, acc: 65.62%] [G loss: 2.604414]\n",
      "epoch:1 step:1350 [D loss: 0.550126, acc: 70.31%] [G loss: 3.028869]\n",
      "epoch:1 step:1351 [D loss: 0.504606, acc: 74.22%] [G loss: 2.792501]\n",
      "epoch:1 step:1352 [D loss: 0.603329, acc: 64.06%] [G loss: 2.503200]\n",
      "epoch:1 step:1353 [D loss: 0.579373, acc: 67.19%] [G loss: 2.523028]\n",
      "epoch:1 step:1354 [D loss: 0.622244, acc: 64.06%] [G loss: 2.738701]\n",
      "epoch:1 step:1355 [D loss: 0.587190, acc: 67.97%] [G loss: 2.757792]\n",
      "epoch:1 step:1356 [D loss: 0.610417, acc: 72.66%] [G loss: 2.692457]\n",
      "epoch:1 step:1357 [D loss: 0.585734, acc: 64.84%] [G loss: 2.762944]\n",
      "epoch:1 step:1358 [D loss: 0.556233, acc: 71.09%] [G loss: 2.727857]\n",
      "epoch:1 step:1359 [D loss: 0.671469, acc: 62.50%] [G loss: 2.583205]\n",
      "epoch:1 step:1360 [D loss: 0.461706, acc: 80.47%] [G loss: 2.883263]\n",
      "epoch:1 step:1361 [D loss: 0.613182, acc: 71.09%] [G loss: 2.971379]\n",
      "epoch:1 step:1362 [D loss: 0.500118, acc: 78.12%] [G loss: 2.962323]\n",
      "epoch:1 step:1363 [D loss: 0.519571, acc: 76.56%] [G loss: 3.158602]\n",
      "epoch:1 step:1364 [D loss: 0.499579, acc: 71.88%] [G loss: 3.053758]\n",
      "epoch:1 step:1365 [D loss: 0.511225, acc: 75.00%] [G loss: 3.204650]\n",
      "epoch:1 step:1366 [D loss: 0.483334, acc: 78.91%] [G loss: 2.900617]\n",
      "epoch:1 step:1367 [D loss: 0.467145, acc: 74.22%] [G loss: 3.093278]\n",
      "epoch:1 step:1368 [D loss: 0.625323, acc: 64.84%] [G loss: 3.015618]\n",
      "epoch:1 step:1369 [D loss: 0.603212, acc: 67.19%] [G loss: 2.884875]\n",
      "epoch:1 step:1370 [D loss: 0.615197, acc: 68.75%] [G loss: 2.909731]\n",
      "epoch:1 step:1371 [D loss: 0.606032, acc: 64.06%] [G loss: 2.761521]\n",
      "epoch:1 step:1372 [D loss: 0.514899, acc: 77.34%] [G loss: 2.822219]\n",
      "epoch:1 step:1373 [D loss: 0.512156, acc: 78.12%] [G loss: 3.048786]\n",
      "epoch:1 step:1374 [D loss: 0.714631, acc: 57.03%] [G loss: 2.679059]\n",
      "epoch:1 step:1375 [D loss: 0.566667, acc: 71.09%] [G loss: 2.754820]\n",
      "epoch:1 step:1376 [D loss: 0.585940, acc: 74.22%] [G loss: 2.959450]\n",
      "epoch:1 step:1377 [D loss: 0.559995, acc: 71.88%] [G loss: 3.093727]\n",
      "epoch:1 step:1378 [D loss: 0.644892, acc: 60.94%] [G loss: 2.629790]\n",
      "epoch:1 step:1379 [D loss: 0.535994, acc: 71.88%] [G loss: 2.982821]\n",
      "epoch:1 step:1380 [D loss: 0.595272, acc: 70.31%] [G loss: 2.963726]\n",
      "epoch:1 step:1381 [D loss: 0.594706, acc: 67.19%] [G loss: 2.674989]\n",
      "epoch:1 step:1382 [D loss: 0.684780, acc: 64.06%] [G loss: 2.855772]\n",
      "epoch:1 step:1383 [D loss: 0.510545, acc: 75.00%] [G loss: 2.936436]\n",
      "epoch:1 step:1384 [D loss: 0.493472, acc: 76.56%] [G loss: 2.803495]\n",
      "epoch:1 step:1385 [D loss: 0.590048, acc: 68.75%] [G loss: 2.607524]\n",
      "epoch:1 step:1386 [D loss: 0.577180, acc: 66.41%] [G loss: 2.526444]\n",
      "epoch:1 step:1387 [D loss: 0.537436, acc: 75.00%] [G loss: 2.691284]\n",
      "epoch:1 step:1388 [D loss: 0.515324, acc: 72.66%] [G loss: 2.944829]\n",
      "epoch:1 step:1389 [D loss: 0.582025, acc: 73.44%] [G loss: 2.898562]\n",
      "epoch:1 step:1390 [D loss: 0.515138, acc: 78.91%] [G loss: 2.931130]\n",
      "epoch:1 step:1391 [D loss: 0.591461, acc: 71.09%] [G loss: 2.768089]\n",
      "epoch:1 step:1392 [D loss: 0.518599, acc: 71.09%] [G loss: 2.737675]\n",
      "epoch:1 step:1393 [D loss: 0.641688, acc: 65.62%] [G loss: 2.658000]\n",
      "epoch:1 step:1394 [D loss: 0.487068, acc: 75.78%] [G loss: 2.744248]\n",
      "epoch:1 step:1395 [D loss: 0.682340, acc: 64.84%] [G loss: 3.011297]\n",
      "epoch:1 step:1396 [D loss: 0.604013, acc: 75.00%] [G loss: 2.807897]\n",
      "epoch:1 step:1397 [D loss: 0.554246, acc: 74.22%] [G loss: 2.920048]\n",
      "epoch:1 step:1398 [D loss: 0.537420, acc: 77.34%] [G loss: 2.915690]\n",
      "epoch:1 step:1399 [D loss: 0.539327, acc: 68.75%] [G loss: 2.899097]\n",
      "epoch:1 step:1400 [D loss: 0.511478, acc: 76.56%] [G loss: 2.719214]\n",
      "##############\n",
      "[3.47014981 2.42165669 7.50856243 6.12465655 4.86376892 6.38021839\n",
      " 5.50349466 5.73499734 6.06996443 4.40266728]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.612915, acc: 62.50%] [G loss: 2.731516]\n",
      "epoch:1 step:1402 [D loss: 0.598665, acc: 67.97%] [G loss: 2.622151]\n",
      "epoch:1 step:1403 [D loss: 0.532323, acc: 72.66%] [G loss: 2.698652]\n",
      "epoch:1 step:1404 [D loss: 0.587319, acc: 64.84%] [G loss: 2.755732]\n",
      "epoch:1 step:1405 [D loss: 0.520872, acc: 75.78%] [G loss: 2.857932]\n",
      "epoch:1 step:1406 [D loss: 0.565262, acc: 69.53%] [G loss: 3.005215]\n",
      "epoch:1 step:1407 [D loss: 0.610690, acc: 66.41%] [G loss: 2.600066]\n",
      "epoch:1 step:1408 [D loss: 0.587139, acc: 67.19%] [G loss: 3.059261]\n",
      "epoch:1 step:1409 [D loss: 0.588426, acc: 66.41%] [G loss: 2.979800]\n",
      "epoch:1 step:1410 [D loss: 0.588911, acc: 66.41%] [G loss: 3.005899]\n",
      "epoch:1 step:1411 [D loss: 0.568311, acc: 71.09%] [G loss: 2.856103]\n",
      "epoch:1 step:1412 [D loss: 0.534097, acc: 71.88%] [G loss: 2.812258]\n",
      "epoch:1 step:1413 [D loss: 0.513599, acc: 72.66%] [G loss: 2.930826]\n",
      "epoch:1 step:1414 [D loss: 0.588883, acc: 71.09%] [G loss: 2.644936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1415 [D loss: 0.596408, acc: 65.62%] [G loss: 2.983038]\n",
      "epoch:1 step:1416 [D loss: 0.641619, acc: 65.62%] [G loss: 2.678391]\n",
      "epoch:1 step:1417 [D loss: 0.510436, acc: 75.00%] [G loss: 2.564846]\n",
      "epoch:1 step:1418 [D loss: 0.613397, acc: 67.97%] [G loss: 2.857302]\n",
      "epoch:1 step:1419 [D loss: 0.578253, acc: 71.88%] [G loss: 2.525707]\n",
      "epoch:1 step:1420 [D loss: 0.663155, acc: 63.28%] [G loss: 2.955472]\n",
      "epoch:1 step:1421 [D loss: 0.509845, acc: 75.78%] [G loss: 2.733293]\n",
      "epoch:1 step:1422 [D loss: 0.548086, acc: 66.41%] [G loss: 2.879985]\n",
      "epoch:1 step:1423 [D loss: 0.587575, acc: 70.31%] [G loss: 2.674743]\n",
      "epoch:1 step:1424 [D loss: 0.564183, acc: 70.31%] [G loss: 2.570474]\n",
      "epoch:1 step:1425 [D loss: 0.524076, acc: 76.56%] [G loss: 2.977115]\n",
      "epoch:1 step:1426 [D loss: 0.639405, acc: 68.75%] [G loss: 2.999805]\n",
      "epoch:1 step:1427 [D loss: 0.578637, acc: 70.31%] [G loss: 2.804325]\n",
      "epoch:1 step:1428 [D loss: 0.529132, acc: 76.56%] [G loss: 2.962457]\n",
      "epoch:1 step:1429 [D loss: 0.693521, acc: 60.94%] [G loss: 2.629833]\n",
      "epoch:1 step:1430 [D loss: 0.542795, acc: 72.66%] [G loss: 3.117869]\n",
      "epoch:1 step:1431 [D loss: 0.526573, acc: 72.66%] [G loss: 2.747251]\n",
      "epoch:1 step:1432 [D loss: 0.585619, acc: 62.50%] [G loss: 2.687723]\n",
      "epoch:1 step:1433 [D loss: 0.611940, acc: 64.06%] [G loss: 2.582617]\n",
      "epoch:1 step:1434 [D loss: 0.599408, acc: 63.28%] [G loss: 3.154781]\n",
      "epoch:1 step:1435 [D loss: 0.485033, acc: 77.34%] [G loss: 3.305221]\n",
      "epoch:1 step:1436 [D loss: 0.432476, acc: 82.81%] [G loss: 2.984523]\n",
      "epoch:1 step:1437 [D loss: 0.669628, acc: 65.62%] [G loss: 2.782973]\n",
      "epoch:1 step:1438 [D loss: 0.638024, acc: 65.62%] [G loss: 2.666405]\n",
      "epoch:1 step:1439 [D loss: 0.662789, acc: 64.06%] [G loss: 2.534909]\n",
      "epoch:1 step:1440 [D loss: 0.562223, acc: 66.41%] [G loss: 2.896081]\n",
      "epoch:1 step:1441 [D loss: 0.522972, acc: 75.78%] [G loss: 2.947603]\n",
      "epoch:1 step:1442 [D loss: 0.598481, acc: 71.09%] [G loss: 2.820624]\n",
      "epoch:1 step:1443 [D loss: 0.625695, acc: 65.62%] [G loss: 2.746055]\n",
      "epoch:1 step:1444 [D loss: 0.576171, acc: 70.31%] [G loss: 2.632361]\n",
      "epoch:1 step:1445 [D loss: 0.525564, acc: 72.66%] [G loss: 2.855429]\n",
      "epoch:1 step:1446 [D loss: 0.733537, acc: 60.16%] [G loss: 2.695800]\n",
      "epoch:1 step:1447 [D loss: 0.517775, acc: 76.56%] [G loss: 2.604179]\n",
      "epoch:1 step:1448 [D loss: 0.655912, acc: 60.16%] [G loss: 2.548393]\n",
      "epoch:1 step:1449 [D loss: 0.559774, acc: 68.75%] [G loss: 2.736805]\n",
      "epoch:1 step:1450 [D loss: 0.532601, acc: 71.88%] [G loss: 2.710248]\n",
      "epoch:1 step:1451 [D loss: 0.575090, acc: 71.88%] [G loss: 3.134076]\n",
      "epoch:1 step:1452 [D loss: 0.563337, acc: 68.75%] [G loss: 2.828342]\n",
      "epoch:1 step:1453 [D loss: 0.564032, acc: 69.53%] [G loss: 2.718597]\n",
      "epoch:1 step:1454 [D loss: 0.629113, acc: 64.84%] [G loss: 2.628915]\n",
      "epoch:1 step:1455 [D loss: 0.627200, acc: 62.50%] [G loss: 2.745706]\n",
      "epoch:1 step:1456 [D loss: 0.621388, acc: 66.41%] [G loss: 2.643450]\n",
      "epoch:1 step:1457 [D loss: 0.611121, acc: 68.75%] [G loss: 2.623206]\n",
      "epoch:1 step:1458 [D loss: 0.553177, acc: 70.31%] [G loss: 2.696944]\n",
      "epoch:1 step:1459 [D loss: 0.566521, acc: 72.66%] [G loss: 2.841689]\n",
      "epoch:1 step:1460 [D loss: 0.572516, acc: 69.53%] [G loss: 2.989380]\n",
      "epoch:1 step:1461 [D loss: 0.632617, acc: 61.72%] [G loss: 2.650246]\n",
      "epoch:1 step:1462 [D loss: 0.570192, acc: 68.75%] [G loss: 2.544537]\n",
      "epoch:1 step:1463 [D loss: 0.607380, acc: 67.97%] [G loss: 2.790636]\n",
      "epoch:1 step:1464 [D loss: 0.584771, acc: 66.41%] [G loss: 2.736926]\n",
      "epoch:1 step:1465 [D loss: 0.700841, acc: 59.38%] [G loss: 2.516679]\n",
      "epoch:1 step:1466 [D loss: 0.602576, acc: 65.62%] [G loss: 2.522046]\n",
      "epoch:1 step:1467 [D loss: 0.531890, acc: 73.44%] [G loss: 2.578641]\n",
      "epoch:1 step:1468 [D loss: 0.558646, acc: 68.75%] [G loss: 2.559507]\n",
      "epoch:1 step:1469 [D loss: 0.607189, acc: 70.31%] [G loss: 2.748582]\n",
      "epoch:1 step:1470 [D loss: 0.564634, acc: 67.19%] [G loss: 2.726198]\n",
      "epoch:1 step:1471 [D loss: 0.489000, acc: 77.34%] [G loss: 2.838745]\n",
      "epoch:1 step:1472 [D loss: 0.548476, acc: 71.09%] [G loss: 2.735308]\n",
      "epoch:1 step:1473 [D loss: 0.577175, acc: 73.44%] [G loss: 2.717455]\n",
      "epoch:1 step:1474 [D loss: 0.548367, acc: 71.09%] [G loss: 2.975538]\n",
      "epoch:1 step:1475 [D loss: 0.709011, acc: 57.81%] [G loss: 2.973799]\n",
      "epoch:1 step:1476 [D loss: 0.613647, acc: 64.06%] [G loss: 2.522635]\n",
      "epoch:1 step:1477 [D loss: 0.606000, acc: 70.31%] [G loss: 2.770083]\n",
      "epoch:1 step:1478 [D loss: 0.551797, acc: 73.44%] [G loss: 3.266181]\n",
      "epoch:1 step:1479 [D loss: 0.627697, acc: 65.62%] [G loss: 2.622601]\n",
      "epoch:1 step:1480 [D loss: 0.611378, acc: 67.19%] [G loss: 2.599158]\n",
      "epoch:1 step:1481 [D loss: 0.526085, acc: 78.12%] [G loss: 2.573208]\n",
      "epoch:1 step:1482 [D loss: 0.532724, acc: 72.66%] [G loss: 2.553937]\n",
      "epoch:1 step:1483 [D loss: 0.593739, acc: 69.53%] [G loss: 2.839931]\n",
      "epoch:1 step:1484 [D loss: 0.528934, acc: 76.56%] [G loss: 3.092022]\n",
      "epoch:1 step:1485 [D loss: 0.576357, acc: 70.31%] [G loss: 2.866070]\n",
      "epoch:1 step:1486 [D loss: 0.531647, acc: 74.22%] [G loss: 2.860564]\n",
      "epoch:1 step:1487 [D loss: 0.581260, acc: 71.88%] [G loss: 2.747517]\n",
      "epoch:1 step:1488 [D loss: 0.509565, acc: 72.66%] [G loss: 2.726962]\n",
      "epoch:1 step:1489 [D loss: 0.581017, acc: 67.19%] [G loss: 2.667657]\n",
      "epoch:1 step:1490 [D loss: 0.566345, acc: 68.75%] [G loss: 2.669179]\n",
      "epoch:1 step:1491 [D loss: 0.646084, acc: 67.97%] [G loss: 2.763524]\n",
      "epoch:1 step:1492 [D loss: 0.508464, acc: 74.22%] [G loss: 3.038133]\n",
      "epoch:1 step:1493 [D loss: 0.546875, acc: 71.09%] [G loss: 2.647366]\n",
      "epoch:1 step:1494 [D loss: 0.645315, acc: 63.28%] [G loss: 3.064290]\n",
      "epoch:1 step:1495 [D loss: 0.470278, acc: 74.22%] [G loss: 3.239102]\n",
      "epoch:1 step:1496 [D loss: 0.585141, acc: 67.19%] [G loss: 2.784474]\n",
      "epoch:1 step:1497 [D loss: 0.590956, acc: 67.97%] [G loss: 2.628377]\n",
      "epoch:1 step:1498 [D loss: 0.501103, acc: 71.88%] [G loss: 2.746998]\n",
      "epoch:1 step:1499 [D loss: 0.623681, acc: 65.62%] [G loss: 2.723694]\n",
      "epoch:1 step:1500 [D loss: 0.606026, acc: 66.41%] [G loss: 2.680963]\n",
      "epoch:1 step:1501 [D loss: 0.533598, acc: 71.09%] [G loss: 2.571826]\n",
      "epoch:1 step:1502 [D loss: 0.582597, acc: 68.75%] [G loss: 2.738856]\n",
      "epoch:1 step:1503 [D loss: 0.594917, acc: 66.41%] [G loss: 2.866532]\n",
      "epoch:1 step:1504 [D loss: 0.491026, acc: 78.12%] [G loss: 2.800291]\n",
      "epoch:1 step:1505 [D loss: 0.641211, acc: 64.84%] [G loss: 2.989386]\n",
      "epoch:1 step:1506 [D loss: 0.625682, acc: 68.75%] [G loss: 2.883532]\n",
      "epoch:1 step:1507 [D loss: 0.647802, acc: 64.06%] [G loss: 2.881648]\n",
      "epoch:1 step:1508 [D loss: 0.601819, acc: 67.97%] [G loss: 2.930056]\n",
      "epoch:1 step:1509 [D loss: 0.557362, acc: 69.53%] [G loss: 2.690497]\n",
      "epoch:1 step:1510 [D loss: 0.537600, acc: 71.09%] [G loss: 2.514722]\n",
      "epoch:1 step:1511 [D loss: 0.563862, acc: 69.53%] [G loss: 2.914003]\n",
      "epoch:1 step:1512 [D loss: 0.464601, acc: 78.91%] [G loss: 2.879824]\n",
      "epoch:1 step:1513 [D loss: 0.517794, acc: 74.22%] [G loss: 2.784542]\n",
      "epoch:1 step:1514 [D loss: 0.686161, acc: 62.50%] [G loss: 2.739824]\n",
      "epoch:1 step:1515 [D loss: 0.541053, acc: 67.97%] [G loss: 2.699501]\n",
      "epoch:1 step:1516 [D loss: 0.523618, acc: 74.22%] [G loss: 3.037040]\n",
      "epoch:1 step:1517 [D loss: 0.618104, acc: 64.84%] [G loss: 2.706153]\n",
      "epoch:1 step:1518 [D loss: 0.593293, acc: 70.31%] [G loss: 2.781445]\n",
      "epoch:1 step:1519 [D loss: 0.567747, acc: 70.31%] [G loss: 2.853072]\n",
      "epoch:1 step:1520 [D loss: 0.602987, acc: 68.75%] [G loss: 2.598116]\n",
      "epoch:1 step:1521 [D loss: 0.615366, acc: 64.06%] [G loss: 2.576205]\n",
      "epoch:1 step:1522 [D loss: 0.620852, acc: 67.19%] [G loss: 2.592204]\n",
      "epoch:1 step:1523 [D loss: 0.622688, acc: 67.19%] [G loss: 2.596325]\n",
      "epoch:1 step:1524 [D loss: 0.563330, acc: 70.31%] [G loss: 2.593563]\n",
      "epoch:1 step:1525 [D loss: 0.586206, acc: 75.00%] [G loss: 2.733485]\n",
      "epoch:1 step:1526 [D loss: 0.568341, acc: 71.09%] [G loss: 2.782534]\n",
      "epoch:1 step:1527 [D loss: 0.531601, acc: 67.97%] [G loss: 2.935454]\n",
      "epoch:1 step:1528 [D loss: 0.598696, acc: 65.62%] [G loss: 2.885155]\n",
      "epoch:1 step:1529 [D loss: 0.556916, acc: 67.19%] [G loss: 2.961927]\n",
      "epoch:1 step:1530 [D loss: 0.582406, acc: 71.09%] [G loss: 2.543493]\n",
      "epoch:1 step:1531 [D loss: 0.626297, acc: 65.62%] [G loss: 2.569329]\n",
      "epoch:1 step:1532 [D loss: 0.577471, acc: 71.88%] [G loss: 2.680207]\n",
      "epoch:1 step:1533 [D loss: 0.645513, acc: 65.62%] [G loss: 2.609166]\n",
      "epoch:1 step:1534 [D loss: 0.565367, acc: 69.53%] [G loss: 2.748070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1535 [D loss: 0.528944, acc: 75.00%] [G loss: 2.866580]\n",
      "epoch:1 step:1536 [D loss: 0.649969, acc: 64.84%] [G loss: 2.672448]\n",
      "epoch:1 step:1537 [D loss: 0.627106, acc: 60.16%] [G loss: 2.517125]\n",
      "epoch:1 step:1538 [D loss: 0.594010, acc: 71.09%] [G loss: 2.994218]\n",
      "epoch:1 step:1539 [D loss: 0.503834, acc: 75.00%] [G loss: 2.798513]\n",
      "epoch:1 step:1540 [D loss: 0.617572, acc: 67.19%] [G loss: 2.837260]\n",
      "epoch:1 step:1541 [D loss: 0.634382, acc: 67.19%] [G loss: 2.608338]\n",
      "epoch:1 step:1542 [D loss: 0.605689, acc: 65.62%] [G loss: 2.728184]\n",
      "epoch:1 step:1543 [D loss: 0.510756, acc: 71.09%] [G loss: 2.656188]\n",
      "epoch:1 step:1544 [D loss: 0.587968, acc: 69.53%] [G loss: 3.100373]\n",
      "epoch:1 step:1545 [D loss: 0.583577, acc: 66.41%] [G loss: 2.887707]\n",
      "epoch:1 step:1546 [D loss: 0.552337, acc: 70.31%] [G loss: 2.774130]\n",
      "epoch:1 step:1547 [D loss: 0.526393, acc: 74.22%] [G loss: 2.982139]\n",
      "epoch:1 step:1548 [D loss: 0.483599, acc: 79.69%] [G loss: 3.247521]\n",
      "epoch:1 step:1549 [D loss: 0.508410, acc: 75.00%] [G loss: 3.109828]\n",
      "epoch:1 step:1550 [D loss: 0.603124, acc: 70.31%] [G loss: 2.824480]\n",
      "epoch:1 step:1551 [D loss: 0.598456, acc: 69.53%] [G loss: 2.974586]\n",
      "epoch:1 step:1552 [D loss: 0.546179, acc: 72.66%] [G loss: 2.862232]\n",
      "epoch:1 step:1553 [D loss: 0.563980, acc: 71.88%] [G loss: 3.110154]\n",
      "epoch:1 step:1554 [D loss: 0.679498, acc: 58.59%] [G loss: 2.680033]\n",
      "epoch:1 step:1555 [D loss: 0.651149, acc: 71.09%] [G loss: 2.781818]\n",
      "epoch:1 step:1556 [D loss: 0.511186, acc: 76.56%] [G loss: 2.956198]\n",
      "epoch:1 step:1557 [D loss: 0.562142, acc: 74.22%] [G loss: 2.898761]\n",
      "epoch:1 step:1558 [D loss: 0.588282, acc: 69.53%] [G loss: 2.906430]\n",
      "epoch:1 step:1559 [D loss: 0.672698, acc: 60.16%] [G loss: 2.409250]\n",
      "epoch:1 step:1560 [D loss: 0.506299, acc: 76.56%] [G loss: 2.932723]\n",
      "epoch:1 step:1561 [D loss: 0.554704, acc: 76.56%] [G loss: 2.670624]\n",
      "epoch:1 step:1562 [D loss: 0.648937, acc: 64.06%] [G loss: 2.685123]\n",
      "epoch:1 step:1563 [D loss: 0.496919, acc: 75.78%] [G loss: 2.746810]\n",
      "epoch:1 step:1564 [D loss: 0.604726, acc: 64.06%] [G loss: 2.846543]\n",
      "epoch:1 step:1565 [D loss: 0.524072, acc: 75.78%] [G loss: 2.788072]\n",
      "epoch:1 step:1566 [D loss: 0.606002, acc: 67.97%] [G loss: 2.746864]\n",
      "epoch:1 step:1567 [D loss: 0.498037, acc: 76.56%] [G loss: 2.675347]\n",
      "epoch:1 step:1568 [D loss: 0.548719, acc: 67.97%] [G loss: 3.005269]\n",
      "epoch:1 step:1569 [D loss: 0.550569, acc: 69.53%] [G loss: 2.886854]\n",
      "epoch:1 step:1570 [D loss: 0.517072, acc: 75.00%] [G loss: 2.780173]\n",
      "epoch:1 step:1571 [D loss: 0.602734, acc: 66.41%] [G loss: 2.924527]\n",
      "epoch:1 step:1572 [D loss: 0.511254, acc: 79.69%] [G loss: 2.767946]\n",
      "epoch:1 step:1573 [D loss: 0.582134, acc: 68.75%] [G loss: 2.756423]\n",
      "epoch:1 step:1574 [D loss: 0.478638, acc: 81.25%] [G loss: 2.693654]\n",
      "epoch:1 step:1575 [D loss: 0.523119, acc: 72.66%] [G loss: 2.611860]\n",
      "epoch:1 step:1576 [D loss: 0.516425, acc: 74.22%] [G loss: 3.263789]\n",
      "epoch:1 step:1577 [D loss: 0.536386, acc: 70.31%] [G loss: 3.091987]\n",
      "epoch:1 step:1578 [D loss: 0.552343, acc: 69.53%] [G loss: 3.201456]\n",
      "epoch:1 step:1579 [D loss: 0.541803, acc: 74.22%] [G loss: 2.705832]\n",
      "epoch:1 step:1580 [D loss: 0.601453, acc: 68.75%] [G loss: 2.816165]\n",
      "epoch:1 step:1581 [D loss: 0.664620, acc: 62.50%] [G loss: 2.894764]\n",
      "epoch:1 step:1582 [D loss: 0.554742, acc: 68.75%] [G loss: 2.762901]\n",
      "epoch:1 step:1583 [D loss: 0.580651, acc: 67.97%] [G loss: 2.748938]\n",
      "epoch:1 step:1584 [D loss: 0.581587, acc: 70.31%] [G loss: 3.085060]\n",
      "epoch:1 step:1585 [D loss: 0.430077, acc: 78.91%] [G loss: 3.560874]\n",
      "epoch:1 step:1586 [D loss: 0.612848, acc: 68.75%] [G loss: 3.096992]\n",
      "epoch:1 step:1587 [D loss: 0.520320, acc: 75.00%] [G loss: 3.265569]\n",
      "epoch:1 step:1588 [D loss: 0.587605, acc: 71.09%] [G loss: 3.140964]\n",
      "epoch:1 step:1589 [D loss: 0.706185, acc: 58.59%] [G loss: 2.494777]\n",
      "epoch:1 step:1590 [D loss: 0.606898, acc: 71.88%] [G loss: 2.756927]\n",
      "epoch:1 step:1591 [D loss: 0.528107, acc: 75.00%] [G loss: 3.118796]\n",
      "epoch:1 step:1592 [D loss: 0.641049, acc: 71.88%] [G loss: 2.819236]\n",
      "epoch:1 step:1593 [D loss: 0.552580, acc: 73.44%] [G loss: 2.635566]\n",
      "epoch:1 step:1594 [D loss: 0.548701, acc: 71.09%] [G loss: 2.588114]\n",
      "epoch:1 step:1595 [D loss: 0.607602, acc: 68.75%] [G loss: 2.599685]\n",
      "epoch:1 step:1596 [D loss: 0.565097, acc: 69.53%] [G loss: 2.730533]\n",
      "epoch:1 step:1597 [D loss: 0.563416, acc: 71.88%] [G loss: 2.838370]\n",
      "epoch:1 step:1598 [D loss: 0.560867, acc: 71.09%] [G loss: 2.965775]\n",
      "epoch:1 step:1599 [D loss: 0.560573, acc: 67.19%] [G loss: 2.700625]\n",
      "epoch:1 step:1600 [D loss: 0.562862, acc: 67.97%] [G loss: 2.877080]\n",
      "##############\n",
      "[3.48402876 2.37993119 7.41395036 5.90787058 4.99211874 6.44660905\n",
      " 5.74221971 5.80127343 6.07488795 4.29065038]\n",
      "##########\n",
      "epoch:1 step:1601 [D loss: 0.603149, acc: 67.19%] [G loss: 2.984576]\n",
      "epoch:1 step:1602 [D loss: 0.530456, acc: 75.00%] [G loss: 3.106319]\n",
      "epoch:1 step:1603 [D loss: 0.479867, acc: 78.12%] [G loss: 2.919865]\n",
      "epoch:1 step:1604 [D loss: 0.502963, acc: 75.00%] [G loss: 2.731781]\n",
      "epoch:1 step:1605 [D loss: 0.541628, acc: 78.12%] [G loss: 2.687473]\n",
      "epoch:1 step:1606 [D loss: 0.547639, acc: 71.88%] [G loss: 2.810958]\n",
      "epoch:1 step:1607 [D loss: 0.551919, acc: 71.88%] [G loss: 3.045929]\n",
      "epoch:1 step:1608 [D loss: 0.568668, acc: 67.97%] [G loss: 3.024810]\n",
      "epoch:1 step:1609 [D loss: 0.609203, acc: 69.53%] [G loss: 2.827122]\n",
      "epoch:1 step:1610 [D loss: 0.631238, acc: 65.62%] [G loss: 3.066263]\n",
      "epoch:1 step:1611 [D loss: 0.563357, acc: 68.75%] [G loss: 2.954386]\n",
      "epoch:1 step:1612 [D loss: 0.566341, acc: 69.53%] [G loss: 2.777722]\n",
      "epoch:1 step:1613 [D loss: 0.683200, acc: 64.84%] [G loss: 2.693849]\n",
      "epoch:1 step:1614 [D loss: 0.453364, acc: 82.81%] [G loss: 3.196128]\n",
      "epoch:1 step:1615 [D loss: 0.529530, acc: 70.31%] [G loss: 3.086549]\n",
      "epoch:1 step:1616 [D loss: 0.499774, acc: 75.78%] [G loss: 2.784988]\n",
      "epoch:1 step:1617 [D loss: 0.503444, acc: 78.12%] [G loss: 3.120414]\n",
      "epoch:1 step:1618 [D loss: 0.570453, acc: 71.09%] [G loss: 3.075110]\n",
      "epoch:1 step:1619 [D loss: 0.501659, acc: 73.44%] [G loss: 3.049050]\n",
      "epoch:1 step:1620 [D loss: 0.564842, acc: 71.88%] [G loss: 2.950916]\n",
      "epoch:1 step:1621 [D loss: 0.552032, acc: 71.88%] [G loss: 3.012567]\n",
      "epoch:1 step:1622 [D loss: 0.506191, acc: 76.56%] [G loss: 3.016900]\n",
      "epoch:1 step:1623 [D loss: 0.515940, acc: 71.09%] [G loss: 2.952172]\n",
      "epoch:1 step:1624 [D loss: 0.620468, acc: 61.72%] [G loss: 2.913702]\n",
      "epoch:1 step:1625 [D loss: 0.606707, acc: 74.22%] [G loss: 2.715810]\n",
      "epoch:1 step:1626 [D loss: 0.552029, acc: 72.66%] [G loss: 2.964961]\n",
      "epoch:1 step:1627 [D loss: 0.604902, acc: 61.72%] [G loss: 3.116827]\n",
      "epoch:1 step:1628 [D loss: 0.571495, acc: 73.44%] [G loss: 2.972143]\n",
      "epoch:1 step:1629 [D loss: 0.598476, acc: 62.50%] [G loss: 2.598654]\n",
      "epoch:1 step:1630 [D loss: 0.574897, acc: 65.62%] [G loss: 2.815638]\n",
      "epoch:1 step:1631 [D loss: 0.500572, acc: 71.88%] [G loss: 2.987697]\n",
      "epoch:1 step:1632 [D loss: 0.524421, acc: 72.66%] [G loss: 2.911604]\n",
      "epoch:1 step:1633 [D loss: 0.574099, acc: 71.88%] [G loss: 2.888352]\n",
      "epoch:1 step:1634 [D loss: 0.554112, acc: 76.56%] [G loss: 2.750661]\n",
      "epoch:1 step:1635 [D loss: 0.590517, acc: 66.41%] [G loss: 2.884375]\n",
      "epoch:1 step:1636 [D loss: 0.526202, acc: 73.44%] [G loss: 2.992884]\n",
      "epoch:1 step:1637 [D loss: 0.637994, acc: 62.50%] [G loss: 2.894937]\n",
      "epoch:1 step:1638 [D loss: 0.639694, acc: 64.84%] [G loss: 2.608439]\n",
      "epoch:1 step:1639 [D loss: 0.557418, acc: 67.97%] [G loss: 2.958514]\n",
      "epoch:1 step:1640 [D loss: 0.562726, acc: 73.44%] [G loss: 2.776444]\n",
      "epoch:1 step:1641 [D loss: 0.630637, acc: 60.94%] [G loss: 2.814257]\n",
      "epoch:1 step:1642 [D loss: 0.580451, acc: 69.53%] [G loss: 3.017123]\n",
      "epoch:1 step:1643 [D loss: 0.517079, acc: 77.34%] [G loss: 2.967571]\n",
      "epoch:1 step:1644 [D loss: 0.475450, acc: 75.78%] [G loss: 3.746316]\n",
      "epoch:1 step:1645 [D loss: 0.524493, acc: 74.22%] [G loss: 3.198562]\n",
      "epoch:1 step:1646 [D loss: 0.543238, acc: 75.00%] [G loss: 3.143024]\n",
      "epoch:1 step:1647 [D loss: 0.635649, acc: 58.59%] [G loss: 2.684902]\n",
      "epoch:1 step:1648 [D loss: 0.619872, acc: 61.72%] [G loss: 2.602457]\n",
      "epoch:1 step:1649 [D loss: 0.562801, acc: 71.88%] [G loss: 2.850367]\n",
      "epoch:1 step:1650 [D loss: 0.529898, acc: 77.34%] [G loss: 3.257704]\n",
      "epoch:1 step:1651 [D loss: 0.493609, acc: 75.00%] [G loss: 3.182690]\n",
      "epoch:1 step:1652 [D loss: 0.486831, acc: 78.91%] [G loss: 2.919433]\n",
      "epoch:1 step:1653 [D loss: 0.585992, acc: 65.62%] [G loss: 2.869730]\n",
      "epoch:1 step:1654 [D loss: 0.609966, acc: 62.50%] [G loss: 2.886502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1655 [D loss: 0.639843, acc: 64.84%] [G loss: 2.804917]\n",
      "epoch:1 step:1656 [D loss: 0.578630, acc: 69.53%] [G loss: 2.902017]\n",
      "epoch:1 step:1657 [D loss: 0.563862, acc: 67.97%] [G loss: 2.640648]\n",
      "epoch:1 step:1658 [D loss: 0.641623, acc: 67.19%] [G loss: 2.890029]\n",
      "epoch:1 step:1659 [D loss: 0.544593, acc: 70.31%] [G loss: 2.978888]\n",
      "epoch:1 step:1660 [D loss: 0.586593, acc: 71.88%] [G loss: 2.812194]\n",
      "epoch:1 step:1661 [D loss: 0.566960, acc: 73.44%] [G loss: 2.861646]\n",
      "epoch:1 step:1662 [D loss: 0.621406, acc: 65.62%] [G loss: 2.876711]\n",
      "epoch:1 step:1663 [D loss: 0.586292, acc: 75.78%] [G loss: 2.593724]\n",
      "epoch:1 step:1664 [D loss: 0.551923, acc: 72.66%] [G loss: 2.737514]\n",
      "epoch:1 step:1665 [D loss: 0.542920, acc: 72.66%] [G loss: 2.750569]\n",
      "epoch:1 step:1666 [D loss: 0.514424, acc: 78.12%] [G loss: 2.778358]\n",
      "epoch:1 step:1667 [D loss: 0.567917, acc: 66.41%] [G loss: 3.052420]\n",
      "epoch:1 step:1668 [D loss: 0.543399, acc: 68.75%] [G loss: 2.886925]\n",
      "epoch:1 step:1669 [D loss: 0.465394, acc: 76.56%] [G loss: 3.151172]\n",
      "epoch:1 step:1670 [D loss: 0.555070, acc: 73.44%] [G loss: 2.744125]\n",
      "epoch:1 step:1671 [D loss: 0.633475, acc: 61.72%] [G loss: 3.182213]\n",
      "epoch:1 step:1672 [D loss: 0.574112, acc: 68.75%] [G loss: 2.604623]\n",
      "epoch:1 step:1673 [D loss: 0.505873, acc: 75.00%] [G loss: 3.058819]\n",
      "epoch:1 step:1674 [D loss: 0.533900, acc: 77.34%] [G loss: 2.528446]\n",
      "epoch:1 step:1675 [D loss: 0.543520, acc: 70.31%] [G loss: 2.818982]\n",
      "epoch:1 step:1676 [D loss: 0.617414, acc: 68.75%] [G loss: 2.802500]\n",
      "epoch:1 step:1677 [D loss: 0.521232, acc: 74.22%] [G loss: 2.793479]\n",
      "epoch:1 step:1678 [D loss: 0.461051, acc: 78.91%] [G loss: 2.952194]\n",
      "epoch:1 step:1679 [D loss: 0.560557, acc: 71.88%] [G loss: 2.946661]\n",
      "epoch:1 step:1680 [D loss: 0.601201, acc: 68.75%] [G loss: 2.821908]\n",
      "epoch:1 step:1681 [D loss: 0.505881, acc: 76.56%] [G loss: 2.765207]\n",
      "epoch:1 step:1682 [D loss: 0.651007, acc: 70.31%] [G loss: 3.122746]\n",
      "epoch:1 step:1683 [D loss: 0.451424, acc: 76.56%] [G loss: 3.526083]\n",
      "epoch:1 step:1684 [D loss: 0.507951, acc: 72.66%] [G loss: 3.355632]\n",
      "epoch:1 step:1685 [D loss: 0.513673, acc: 75.78%] [G loss: 3.090949]\n",
      "epoch:1 step:1686 [D loss: 0.439300, acc: 83.59%] [G loss: 3.013575]\n",
      "epoch:1 step:1687 [D loss: 0.494320, acc: 75.00%] [G loss: 3.351458]\n",
      "epoch:1 step:1688 [D loss: 0.504262, acc: 71.09%] [G loss: 3.246964]\n",
      "epoch:1 step:1689 [D loss: 0.479335, acc: 82.03%] [G loss: 3.533946]\n",
      "epoch:1 step:1690 [D loss: 0.518226, acc: 76.56%] [G loss: 3.050744]\n",
      "epoch:1 step:1691 [D loss: 0.474291, acc: 76.56%] [G loss: 3.358994]\n",
      "epoch:1 step:1692 [D loss: 0.424184, acc: 82.03%] [G loss: 3.755005]\n",
      "epoch:1 step:1693 [D loss: 0.515067, acc: 75.78%] [G loss: 3.074209]\n",
      "epoch:1 step:1694 [D loss: 0.496480, acc: 73.44%] [G loss: 3.501324]\n",
      "epoch:1 step:1695 [D loss: 0.480127, acc: 79.69%] [G loss: 3.218338]\n",
      "epoch:1 step:1696 [D loss: 0.453613, acc: 81.25%] [G loss: 3.163029]\n",
      "epoch:1 step:1697 [D loss: 0.586517, acc: 69.53%] [G loss: 3.079357]\n",
      "epoch:1 step:1698 [D loss: 0.593135, acc: 71.09%] [G loss: 3.035595]\n",
      "epoch:1 step:1699 [D loss: 0.513134, acc: 76.56%] [G loss: 3.206191]\n",
      "epoch:1 step:1700 [D loss: 0.621931, acc: 64.84%] [G loss: 2.974020]\n",
      "epoch:1 step:1701 [D loss: 0.536450, acc: 70.31%] [G loss: 3.006593]\n",
      "epoch:1 step:1702 [D loss: 0.585717, acc: 71.88%] [G loss: 2.946528]\n",
      "epoch:1 step:1703 [D loss: 0.648427, acc: 61.72%] [G loss: 2.753303]\n",
      "epoch:1 step:1704 [D loss: 0.727234, acc: 64.84%] [G loss: 2.836788]\n",
      "epoch:1 step:1705 [D loss: 0.629993, acc: 63.28%] [G loss: 2.605944]\n",
      "epoch:1 step:1706 [D loss: 0.563710, acc: 75.00%] [G loss: 2.865529]\n",
      "epoch:1 step:1707 [D loss: 0.619111, acc: 65.62%] [G loss: 2.917786]\n",
      "epoch:1 step:1708 [D loss: 0.555176, acc: 70.31%] [G loss: 2.905128]\n",
      "epoch:1 step:1709 [D loss: 0.552042, acc: 71.09%] [G loss: 2.895117]\n",
      "epoch:1 step:1710 [D loss: 0.503741, acc: 77.34%] [G loss: 2.765871]\n",
      "epoch:1 step:1711 [D loss: 0.545715, acc: 66.41%] [G loss: 2.781337]\n",
      "epoch:1 step:1712 [D loss: 0.522926, acc: 71.88%] [G loss: 3.095030]\n",
      "epoch:1 step:1713 [D loss: 0.487292, acc: 75.00%] [G loss: 2.940410]\n",
      "epoch:1 step:1714 [D loss: 0.503880, acc: 73.44%] [G loss: 2.914695]\n",
      "epoch:1 step:1715 [D loss: 0.548212, acc: 72.66%] [G loss: 2.806410]\n",
      "epoch:1 step:1716 [D loss: 0.657340, acc: 63.28%] [G loss: 2.778590]\n",
      "epoch:1 step:1717 [D loss: 0.548368, acc: 72.66%] [G loss: 3.252383]\n",
      "epoch:1 step:1718 [D loss: 0.556719, acc: 71.88%] [G loss: 3.193051]\n",
      "epoch:1 step:1719 [D loss: 0.521988, acc: 73.44%] [G loss: 2.855162]\n",
      "epoch:1 step:1720 [D loss: 0.513314, acc: 75.00%] [G loss: 3.314364]\n",
      "epoch:1 step:1721 [D loss: 0.552161, acc: 73.44%] [G loss: 3.028351]\n",
      "epoch:1 step:1722 [D loss: 0.517204, acc: 75.78%] [G loss: 3.185782]\n",
      "epoch:1 step:1723 [D loss: 0.571592, acc: 72.66%] [G loss: 3.351513]\n",
      "epoch:1 step:1724 [D loss: 0.480076, acc: 71.88%] [G loss: 3.338069]\n",
      "epoch:1 step:1725 [D loss: 0.576295, acc: 68.75%] [G loss: 2.526942]\n",
      "epoch:1 step:1726 [D loss: 0.553774, acc: 71.09%] [G loss: 2.864954]\n",
      "epoch:1 step:1727 [D loss: 0.588631, acc: 73.44%] [G loss: 3.275738]\n",
      "epoch:1 step:1728 [D loss: 0.540620, acc: 67.97%] [G loss: 3.352145]\n",
      "epoch:1 step:1729 [D loss: 0.431033, acc: 82.03%] [G loss: 4.025176]\n",
      "epoch:1 step:1730 [D loss: 0.504337, acc: 75.78%] [G loss: 3.257246]\n",
      "epoch:1 step:1731 [D loss: 0.519758, acc: 75.78%] [G loss: 3.324841]\n",
      "epoch:1 step:1732 [D loss: 0.549876, acc: 74.22%] [G loss: 3.452960]\n",
      "epoch:1 step:1733 [D loss: 0.498077, acc: 75.78%] [G loss: 2.823256]\n",
      "epoch:1 step:1734 [D loss: 0.587355, acc: 71.88%] [G loss: 2.986369]\n",
      "epoch:1 step:1735 [D loss: 0.496464, acc: 72.66%] [G loss: 3.396383]\n",
      "epoch:1 step:1736 [D loss: 0.491802, acc: 76.56%] [G loss: 3.349313]\n",
      "epoch:1 step:1737 [D loss: 0.506416, acc: 82.03%] [G loss: 2.927437]\n",
      "epoch:1 step:1738 [D loss: 0.438945, acc: 80.47%] [G loss: 3.637409]\n",
      "epoch:1 step:1739 [D loss: 0.575031, acc: 70.31%] [G loss: 3.339030]\n",
      "epoch:1 step:1740 [D loss: 0.468458, acc: 80.47%] [G loss: 3.273048]\n",
      "epoch:1 step:1741 [D loss: 0.468893, acc: 77.34%] [G loss: 3.398569]\n",
      "epoch:1 step:1742 [D loss: 0.479789, acc: 75.00%] [G loss: 3.392486]\n",
      "epoch:1 step:1743 [D loss: 0.410087, acc: 82.81%] [G loss: 3.727690]\n",
      "epoch:1 step:1744 [D loss: 0.427725, acc: 80.47%] [G loss: 3.468074]\n",
      "epoch:1 step:1745 [D loss: 0.629688, acc: 65.62%] [G loss: 2.624929]\n",
      "epoch:1 step:1746 [D loss: 0.619049, acc: 68.75%] [G loss: 2.874530]\n",
      "epoch:1 step:1747 [D loss: 0.559027, acc: 71.09%] [G loss: 2.921859]\n",
      "epoch:1 step:1748 [D loss: 0.574674, acc: 71.09%] [G loss: 2.852343]\n",
      "epoch:1 step:1749 [D loss: 0.655759, acc: 62.50%] [G loss: 2.857258]\n",
      "epoch:1 step:1750 [D loss: 0.468765, acc: 79.69%] [G loss: 2.780777]\n",
      "epoch:1 step:1751 [D loss: 0.589486, acc: 68.75%] [G loss: 3.112968]\n",
      "epoch:1 step:1752 [D loss: 0.533107, acc: 75.78%] [G loss: 2.950909]\n",
      "epoch:1 step:1753 [D loss: 0.501973, acc: 75.00%] [G loss: 3.054605]\n",
      "epoch:1 step:1754 [D loss: 0.602868, acc: 63.28%] [G loss: 3.613843]\n",
      "epoch:1 step:1755 [D loss: 0.646247, acc: 62.50%] [G loss: 3.208365]\n",
      "epoch:1 step:1756 [D loss: 0.543498, acc: 73.44%] [G loss: 3.138627]\n",
      "epoch:1 step:1757 [D loss: 0.582125, acc: 72.66%] [G loss: 3.067444]\n",
      "epoch:1 step:1758 [D loss: 0.514440, acc: 73.44%] [G loss: 3.399740]\n",
      "epoch:1 step:1759 [D loss: 0.443752, acc: 78.12%] [G loss: 3.115721]\n",
      "epoch:1 step:1760 [D loss: 0.412140, acc: 80.47%] [G loss: 3.617184]\n",
      "epoch:1 step:1761 [D loss: 0.590580, acc: 67.97%] [G loss: 3.229446]\n",
      "epoch:1 step:1762 [D loss: 0.495371, acc: 71.88%] [G loss: 3.187137]\n",
      "epoch:1 step:1763 [D loss: 0.526034, acc: 75.00%] [G loss: 3.428231]\n",
      "epoch:1 step:1764 [D loss: 0.585405, acc: 67.19%] [G loss: 3.177771]\n",
      "epoch:1 step:1765 [D loss: 0.667254, acc: 64.84%] [G loss: 2.771631]\n",
      "epoch:1 step:1766 [D loss: 0.487983, acc: 75.78%] [G loss: 3.242066]\n",
      "epoch:1 step:1767 [D loss: 0.514197, acc: 73.44%] [G loss: 3.097821]\n",
      "epoch:1 step:1768 [D loss: 0.571147, acc: 70.31%] [G loss: 2.953353]\n",
      "epoch:1 step:1769 [D loss: 0.508680, acc: 76.56%] [G loss: 3.364110]\n",
      "epoch:1 step:1770 [D loss: 0.558484, acc: 71.88%] [G loss: 3.098730]\n",
      "epoch:1 step:1771 [D loss: 0.544610, acc: 72.66%] [G loss: 3.412923]\n",
      "epoch:1 step:1772 [D loss: 0.438961, acc: 77.34%] [G loss: 3.650337]\n",
      "epoch:1 step:1773 [D loss: 0.545047, acc: 73.44%] [G loss: 3.373984]\n",
      "epoch:1 step:1774 [D loss: 0.531802, acc: 73.44%] [G loss: 3.257321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1775 [D loss: 0.473817, acc: 77.34%] [G loss: 3.080709]\n",
      "epoch:1 step:1776 [D loss: 0.548582, acc: 67.97%] [G loss: 2.674620]\n",
      "epoch:1 step:1777 [D loss: 0.511581, acc: 71.88%] [G loss: 2.991784]\n",
      "epoch:1 step:1778 [D loss: 0.516805, acc: 71.09%] [G loss: 3.443310]\n",
      "epoch:1 step:1779 [D loss: 0.478897, acc: 77.34%] [G loss: 3.662909]\n",
      "epoch:1 step:1780 [D loss: 0.539570, acc: 67.19%] [G loss: 3.285512]\n",
      "epoch:1 step:1781 [D loss: 0.562866, acc: 69.53%] [G loss: 2.668996]\n",
      "epoch:1 step:1782 [D loss: 0.685869, acc: 62.50%] [G loss: 2.939474]\n",
      "epoch:1 step:1783 [D loss: 0.520957, acc: 75.00%] [G loss: 3.017161]\n",
      "epoch:1 step:1784 [D loss: 0.459273, acc: 78.12%] [G loss: 3.163569]\n",
      "epoch:1 step:1785 [D loss: 0.466429, acc: 79.69%] [G loss: 3.166628]\n",
      "epoch:1 step:1786 [D loss: 0.507440, acc: 78.12%] [G loss: 2.993311]\n",
      "epoch:1 step:1787 [D loss: 0.625945, acc: 61.72%] [G loss: 3.241450]\n",
      "epoch:1 step:1788 [D loss: 0.490917, acc: 78.91%] [G loss: 3.084465]\n",
      "epoch:1 step:1789 [D loss: 0.533266, acc: 75.00%] [G loss: 3.656602]\n",
      "epoch:1 step:1790 [D loss: 0.487847, acc: 72.66%] [G loss: 3.612259]\n",
      "epoch:1 step:1791 [D loss: 0.466140, acc: 76.56%] [G loss: 3.398920]\n",
      "epoch:1 step:1792 [D loss: 0.471003, acc: 77.34%] [G loss: 3.303924]\n",
      "epoch:1 step:1793 [D loss: 0.520052, acc: 71.88%] [G loss: 3.352301]\n",
      "epoch:1 step:1794 [D loss: 0.492135, acc: 76.56%] [G loss: 3.294888]\n",
      "epoch:1 step:1795 [D loss: 0.576063, acc: 69.53%] [G loss: 3.020874]\n",
      "epoch:1 step:1796 [D loss: 0.664763, acc: 62.50%] [G loss: 2.994739]\n",
      "epoch:1 step:1797 [D loss: 0.499791, acc: 75.78%] [G loss: 3.217454]\n",
      "epoch:1 step:1798 [D loss: 0.610596, acc: 66.41%] [G loss: 2.984750]\n",
      "epoch:1 step:1799 [D loss: 0.566037, acc: 71.88%] [G loss: 3.039341]\n",
      "epoch:1 step:1800 [D loss: 0.507657, acc: 74.22%] [G loss: 2.644437]\n",
      "##############\n",
      "[3.42242017 2.31174138 7.41901129 5.86791097 4.78705539 6.50582679\n",
      " 5.52472798 5.68930415 5.84700432 4.35131762]\n",
      "##########\n",
      "epoch:1 step:1801 [D loss: 0.525430, acc: 70.31%] [G loss: 2.953042]\n",
      "epoch:1 step:1802 [D loss: 0.505321, acc: 76.56%] [G loss: 2.977830]\n",
      "epoch:1 step:1803 [D loss: 0.588490, acc: 73.44%] [G loss: 3.020969]\n",
      "epoch:1 step:1804 [D loss: 0.555027, acc: 71.09%] [G loss: 2.979887]\n",
      "epoch:1 step:1805 [D loss: 0.606447, acc: 68.75%] [G loss: 2.919415]\n",
      "epoch:1 step:1806 [D loss: 0.547169, acc: 71.88%] [G loss: 2.981695]\n",
      "epoch:1 step:1807 [D loss: 0.522904, acc: 66.41%] [G loss: 3.008780]\n",
      "epoch:1 step:1808 [D loss: 0.602411, acc: 67.97%] [G loss: 3.331228]\n",
      "epoch:1 step:1809 [D loss: 0.514754, acc: 75.78%] [G loss: 3.192931]\n",
      "epoch:1 step:1810 [D loss: 0.535671, acc: 73.44%] [G loss: 3.040552]\n",
      "epoch:1 step:1811 [D loss: 0.578216, acc: 72.66%] [G loss: 3.413206]\n",
      "epoch:1 step:1812 [D loss: 0.542325, acc: 72.66%] [G loss: 3.381683]\n",
      "epoch:1 step:1813 [D loss: 0.573510, acc: 65.62%] [G loss: 2.935792]\n",
      "epoch:1 step:1814 [D loss: 0.512867, acc: 74.22%] [G loss: 3.414888]\n",
      "epoch:1 step:1815 [D loss: 0.694709, acc: 64.84%] [G loss: 3.117673]\n",
      "epoch:1 step:1816 [D loss: 0.555822, acc: 70.31%] [G loss: 3.068318]\n",
      "epoch:1 step:1817 [D loss: 0.610719, acc: 67.97%] [G loss: 3.004825]\n",
      "epoch:1 step:1818 [D loss: 0.504909, acc: 75.00%] [G loss: 3.228999]\n",
      "epoch:1 step:1819 [D loss: 0.501109, acc: 77.34%] [G loss: 3.085245]\n",
      "epoch:1 step:1820 [D loss: 0.574562, acc: 69.53%] [G loss: 3.027661]\n",
      "epoch:1 step:1821 [D loss: 0.555590, acc: 71.88%] [G loss: 2.907418]\n",
      "epoch:1 step:1822 [D loss: 0.462261, acc: 77.34%] [G loss: 3.416848]\n",
      "epoch:1 step:1823 [D loss: 0.542243, acc: 69.53%] [G loss: 3.324309]\n",
      "epoch:1 step:1824 [D loss: 0.603895, acc: 69.53%] [G loss: 3.428929]\n",
      "epoch:1 step:1825 [D loss: 0.446810, acc: 78.91%] [G loss: 3.603946]\n",
      "epoch:1 step:1826 [D loss: 0.500495, acc: 76.56%] [G loss: 3.413378]\n",
      "epoch:1 step:1827 [D loss: 0.469518, acc: 76.56%] [G loss: 3.698677]\n",
      "epoch:1 step:1828 [D loss: 0.694577, acc: 60.16%] [G loss: 3.250419]\n",
      "epoch:1 step:1829 [D loss: 0.632705, acc: 59.38%] [G loss: 3.016942]\n",
      "epoch:1 step:1830 [D loss: 0.667265, acc: 64.84%] [G loss: 2.745480]\n",
      "epoch:1 step:1831 [D loss: 0.556241, acc: 71.88%] [G loss: 2.759395]\n",
      "epoch:1 step:1832 [D loss: 0.563275, acc: 70.31%] [G loss: 2.957635]\n",
      "epoch:1 step:1833 [D loss: 0.526725, acc: 80.47%] [G loss: 3.078987]\n",
      "epoch:1 step:1834 [D loss: 0.438337, acc: 79.69%] [G loss: 3.171820]\n",
      "epoch:1 step:1835 [D loss: 0.544278, acc: 71.09%] [G loss: 3.331859]\n",
      "epoch:1 step:1836 [D loss: 0.592230, acc: 71.09%] [G loss: 3.161873]\n",
      "epoch:1 step:1837 [D loss: 0.556583, acc: 71.88%] [G loss: 3.175062]\n",
      "epoch:1 step:1838 [D loss: 0.491482, acc: 75.00%] [G loss: 3.367782]\n",
      "epoch:1 step:1839 [D loss: 0.640688, acc: 64.06%] [G loss: 2.598810]\n",
      "epoch:1 step:1840 [D loss: 0.600787, acc: 70.31%] [G loss: 2.872887]\n",
      "epoch:1 step:1841 [D loss: 0.463819, acc: 78.91%] [G loss: 3.693283]\n",
      "epoch:1 step:1842 [D loss: 0.553562, acc: 71.88%] [G loss: 3.138655]\n",
      "epoch:1 step:1843 [D loss: 0.611442, acc: 67.19%] [G loss: 3.148962]\n",
      "epoch:1 step:1844 [D loss: 0.707604, acc: 61.72%] [G loss: 3.057467]\n",
      "epoch:1 step:1845 [D loss: 0.569224, acc: 68.75%] [G loss: 2.868770]\n",
      "epoch:1 step:1846 [D loss: 0.636793, acc: 67.19%] [G loss: 3.039951]\n",
      "epoch:1 step:1847 [D loss: 0.455286, acc: 78.12%] [G loss: 3.261142]\n",
      "epoch:1 step:1848 [D loss: 0.556154, acc: 72.66%] [G loss: 3.231048]\n",
      "epoch:1 step:1849 [D loss: 0.438515, acc: 79.69%] [G loss: 3.654351]\n",
      "epoch:1 step:1850 [D loss: 0.563135, acc: 67.97%] [G loss: 2.859755]\n",
      "epoch:1 step:1851 [D loss: 0.574026, acc: 69.53%] [G loss: 3.414406]\n",
      "epoch:1 step:1852 [D loss: 0.585166, acc: 67.97%] [G loss: 3.064914]\n",
      "epoch:1 step:1853 [D loss: 0.575939, acc: 68.75%] [G loss: 3.058686]\n",
      "epoch:1 step:1854 [D loss: 0.583895, acc: 70.31%] [G loss: 3.028417]\n",
      "epoch:1 step:1855 [D loss: 0.446375, acc: 79.69%] [G loss: 3.203162]\n",
      "epoch:1 step:1856 [D loss: 0.601355, acc: 70.31%] [G loss: 3.162681]\n",
      "epoch:1 step:1857 [D loss: 0.874765, acc: 55.47%] [G loss: 2.935092]\n",
      "epoch:1 step:1858 [D loss: 0.472386, acc: 78.12%] [G loss: 3.588001]\n",
      "epoch:1 step:1859 [D loss: 0.604769, acc: 67.19%] [G loss: 3.344741]\n",
      "epoch:1 step:1860 [D loss: 0.478984, acc: 77.34%] [G loss: 3.233471]\n",
      "epoch:1 step:1861 [D loss: 0.539124, acc: 73.44%] [G loss: 3.067578]\n",
      "epoch:1 step:1862 [D loss: 0.470929, acc: 78.12%] [G loss: 3.589123]\n",
      "epoch:1 step:1863 [D loss: 0.489620, acc: 78.91%] [G loss: 3.396763]\n",
      "epoch:1 step:1864 [D loss: 0.459467, acc: 75.78%] [G loss: 3.532732]\n",
      "epoch:1 step:1865 [D loss: 0.680430, acc: 64.84%] [G loss: 3.189572]\n",
      "epoch:1 step:1866 [D loss: 0.554757, acc: 71.88%] [G loss: 4.144125]\n",
      "epoch:1 step:1867 [D loss: 0.423572, acc: 78.12%] [G loss: 4.246093]\n",
      "epoch:1 step:1868 [D loss: 0.638433, acc: 67.19%] [G loss: 3.114764]\n",
      "epoch:1 step:1869 [D loss: 0.550896, acc: 75.78%] [G loss: 3.171517]\n",
      "epoch:1 step:1870 [D loss: 0.584825, acc: 71.09%] [G loss: 3.203561]\n",
      "epoch:1 step:1871 [D loss: 0.627160, acc: 66.41%] [G loss: 3.243968]\n",
      "epoch:1 step:1872 [D loss: 0.474564, acc: 76.56%] [G loss: 3.687519]\n",
      "epoch:1 step:1873 [D loss: 0.412134, acc: 79.69%] [G loss: 3.844349]\n",
      "epoch:1 step:1874 [D loss: 0.619914, acc: 67.19%] [G loss: 3.808197]\n",
      "epoch:2 step:1875 [D loss: 0.610107, acc: 64.84%] [G loss: 3.335849]\n",
      "epoch:2 step:1876 [D loss: 0.522985, acc: 72.66%] [G loss: 3.287485]\n",
      "epoch:2 step:1877 [D loss: 0.673118, acc: 64.06%] [G loss: 2.967465]\n",
      "epoch:2 step:1878 [D loss: 0.464149, acc: 79.69%] [G loss: 3.267502]\n",
      "epoch:2 step:1879 [D loss: 0.501604, acc: 77.34%] [G loss: 2.898830]\n",
      "epoch:2 step:1880 [D loss: 0.503646, acc: 75.78%] [G loss: 3.015473]\n",
      "epoch:2 step:1881 [D loss: 0.564269, acc: 71.09%] [G loss: 3.215878]\n",
      "epoch:2 step:1882 [D loss: 0.597947, acc: 70.31%] [G loss: 2.922246]\n",
      "epoch:2 step:1883 [D loss: 0.563822, acc: 71.09%] [G loss: 2.887471]\n",
      "epoch:2 step:1884 [D loss: 0.645372, acc: 64.06%] [G loss: 3.176127]\n",
      "epoch:2 step:1885 [D loss: 0.625311, acc: 67.97%] [G loss: 2.828724]\n",
      "epoch:2 step:1886 [D loss: 0.574540, acc: 70.31%] [G loss: 3.173269]\n",
      "epoch:2 step:1887 [D loss: 0.560782, acc: 70.31%] [G loss: 3.232758]\n",
      "epoch:2 step:1888 [D loss: 0.506274, acc: 71.88%] [G loss: 3.254650]\n",
      "epoch:2 step:1889 [D loss: 0.565573, acc: 70.31%] [G loss: 2.890686]\n",
      "epoch:2 step:1890 [D loss: 0.611558, acc: 70.31%] [G loss: 2.793919]\n",
      "epoch:2 step:1891 [D loss: 0.637312, acc: 65.62%] [G loss: 2.900950]\n",
      "epoch:2 step:1892 [D loss: 0.607051, acc: 67.19%] [G loss: 2.645710]\n",
      "epoch:2 step:1893 [D loss: 0.548532, acc: 73.44%] [G loss: 2.865402]\n",
      "epoch:2 step:1894 [D loss: 0.695973, acc: 55.47%] [G loss: 3.015108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1895 [D loss: 0.493086, acc: 74.22%] [G loss: 2.632151]\n",
      "epoch:2 step:1896 [D loss: 0.525689, acc: 74.22%] [G loss: 2.895377]\n",
      "epoch:2 step:1897 [D loss: 0.626542, acc: 63.28%] [G loss: 3.246542]\n",
      "epoch:2 step:1898 [D loss: 0.502543, acc: 72.66%] [G loss: 3.455450]\n",
      "epoch:2 step:1899 [D loss: 0.591038, acc: 68.75%] [G loss: 2.960682]\n",
      "epoch:2 step:1900 [D loss: 0.591026, acc: 70.31%] [G loss: 2.909547]\n",
      "epoch:2 step:1901 [D loss: 0.540514, acc: 67.19%] [G loss: 2.915960]\n",
      "epoch:2 step:1902 [D loss: 0.557590, acc: 72.66%] [G loss: 2.790760]\n",
      "epoch:2 step:1903 [D loss: 0.605439, acc: 67.97%] [G loss: 2.875766]\n",
      "epoch:2 step:1904 [D loss: 0.607723, acc: 66.41%] [G loss: 2.839579]\n",
      "epoch:2 step:1905 [D loss: 0.661052, acc: 63.28%] [G loss: 2.775164]\n",
      "epoch:2 step:1906 [D loss: 0.569017, acc: 74.22%] [G loss: 2.747834]\n",
      "epoch:2 step:1907 [D loss: 0.428739, acc: 80.47%] [G loss: 3.306880]\n",
      "epoch:2 step:1908 [D loss: 0.476199, acc: 78.12%] [G loss: 3.349324]\n",
      "epoch:2 step:1909 [D loss: 0.567293, acc: 70.31%] [G loss: 3.085303]\n",
      "epoch:2 step:1910 [D loss: 0.533444, acc: 73.44%] [G loss: 2.832858]\n",
      "epoch:2 step:1911 [D loss: 0.492594, acc: 70.31%] [G loss: 3.007620]\n",
      "epoch:2 step:1912 [D loss: 0.641960, acc: 61.72%] [G loss: 2.833585]\n",
      "epoch:2 step:1913 [D loss: 0.556768, acc: 67.19%] [G loss: 3.079763]\n",
      "epoch:2 step:1914 [D loss: 0.557643, acc: 69.53%] [G loss: 2.823404]\n",
      "epoch:2 step:1915 [D loss: 0.579160, acc: 70.31%] [G loss: 2.775885]\n",
      "epoch:2 step:1916 [D loss: 0.554474, acc: 73.44%] [G loss: 3.125733]\n",
      "epoch:2 step:1917 [D loss: 0.592150, acc: 66.41%] [G loss: 2.999054]\n",
      "epoch:2 step:1918 [D loss: 0.546618, acc: 70.31%] [G loss: 3.061748]\n",
      "epoch:2 step:1919 [D loss: 0.548553, acc: 71.09%] [G loss: 3.041442]\n",
      "epoch:2 step:1920 [D loss: 0.668492, acc: 64.06%] [G loss: 2.864391]\n",
      "epoch:2 step:1921 [D loss: 0.571158, acc: 71.09%] [G loss: 3.045403]\n",
      "epoch:2 step:1922 [D loss: 0.581113, acc: 69.53%] [G loss: 2.608845]\n",
      "epoch:2 step:1923 [D loss: 0.584669, acc: 70.31%] [G loss: 2.941112]\n",
      "epoch:2 step:1924 [D loss: 0.484975, acc: 80.47%] [G loss: 2.770059]\n",
      "epoch:2 step:1925 [D loss: 0.509465, acc: 75.78%] [G loss: 3.028206]\n",
      "epoch:2 step:1926 [D loss: 0.494643, acc: 76.56%] [G loss: 3.107585]\n",
      "epoch:2 step:1927 [D loss: 0.629328, acc: 71.09%] [G loss: 3.059886]\n",
      "epoch:2 step:1928 [D loss: 0.487157, acc: 73.44%] [G loss: 3.260488]\n",
      "epoch:2 step:1929 [D loss: 0.497997, acc: 76.56%] [G loss: 3.172418]\n",
      "epoch:2 step:1930 [D loss: 0.617956, acc: 67.97%] [G loss: 3.146625]\n",
      "epoch:2 step:1931 [D loss: 0.506541, acc: 71.09%] [G loss: 3.434739]\n",
      "epoch:2 step:1932 [D loss: 0.487267, acc: 78.12%] [G loss: 3.540080]\n",
      "epoch:2 step:1933 [D loss: 0.566554, acc: 67.97%] [G loss: 3.428212]\n",
      "epoch:2 step:1934 [D loss: 0.572295, acc: 67.19%] [G loss: 3.158888]\n",
      "epoch:2 step:1935 [D loss: 0.607820, acc: 69.53%] [G loss: 3.518300]\n",
      "epoch:2 step:1936 [D loss: 0.437210, acc: 78.12%] [G loss: 2.935667]\n",
      "epoch:2 step:1937 [D loss: 0.541314, acc: 69.53%] [G loss: 3.066245]\n",
      "epoch:2 step:1938 [D loss: 0.593729, acc: 66.41%] [G loss: 2.513191]\n",
      "epoch:2 step:1939 [D loss: 0.443168, acc: 82.03%] [G loss: 3.082872]\n",
      "epoch:2 step:1940 [D loss: 0.536515, acc: 71.09%] [G loss: 3.120562]\n",
      "epoch:2 step:1941 [D loss: 0.538629, acc: 71.88%] [G loss: 3.165061]\n",
      "epoch:2 step:1942 [D loss: 0.515605, acc: 75.00%] [G loss: 3.050872]\n",
      "epoch:2 step:1943 [D loss: 0.561639, acc: 71.88%] [G loss: 2.918445]\n",
      "epoch:2 step:1944 [D loss: 0.522892, acc: 75.00%] [G loss: 3.074441]\n",
      "epoch:2 step:1945 [D loss: 0.515798, acc: 73.44%] [G loss: 2.951903]\n",
      "epoch:2 step:1946 [D loss: 0.567093, acc: 74.22%] [G loss: 3.265791]\n",
      "epoch:2 step:1947 [D loss: 0.527178, acc: 75.00%] [G loss: 2.863130]\n",
      "epoch:2 step:1948 [D loss: 0.520642, acc: 71.88%] [G loss: 3.018237]\n",
      "epoch:2 step:1949 [D loss: 0.520826, acc: 75.78%] [G loss: 3.410968]\n",
      "epoch:2 step:1950 [D loss: 0.509532, acc: 75.78%] [G loss: 3.533263]\n",
      "epoch:2 step:1951 [D loss: 0.429393, acc: 84.38%] [G loss: 3.543008]\n",
      "epoch:2 step:1952 [D loss: 0.625430, acc: 68.75%] [G loss: 3.147876]\n",
      "epoch:2 step:1953 [D loss: 0.598871, acc: 67.19%] [G loss: 2.783508]\n",
      "epoch:2 step:1954 [D loss: 0.606881, acc: 67.97%] [G loss: 2.810790]\n",
      "epoch:2 step:1955 [D loss: 0.477737, acc: 75.78%] [G loss: 3.007645]\n",
      "epoch:2 step:1956 [D loss: 0.533950, acc: 76.56%] [G loss: 3.572377]\n",
      "epoch:2 step:1957 [D loss: 0.508043, acc: 76.56%] [G loss: 3.350101]\n",
      "epoch:2 step:1958 [D loss: 0.562029, acc: 65.62%] [G loss: 3.281999]\n",
      "epoch:2 step:1959 [D loss: 0.502860, acc: 75.00%] [G loss: 3.431188]\n",
      "epoch:2 step:1960 [D loss: 0.529501, acc: 71.09%] [G loss: 3.304631]\n",
      "epoch:2 step:1961 [D loss: 0.597858, acc: 71.88%] [G loss: 3.434142]\n",
      "epoch:2 step:1962 [D loss: 0.404001, acc: 84.38%] [G loss: 3.454827]\n",
      "epoch:2 step:1963 [D loss: 0.530843, acc: 69.53%] [G loss: 3.075378]\n",
      "epoch:2 step:1964 [D loss: 0.517047, acc: 71.88%] [G loss: 3.357885]\n",
      "epoch:2 step:1965 [D loss: 0.596400, acc: 67.19%] [G loss: 3.015520]\n",
      "epoch:2 step:1966 [D loss: 0.535791, acc: 70.31%] [G loss: 3.175278]\n",
      "epoch:2 step:1967 [D loss: 0.461658, acc: 71.09%] [G loss: 3.420603]\n",
      "epoch:2 step:1968 [D loss: 0.661557, acc: 66.41%] [G loss: 3.417154]\n",
      "epoch:2 step:1969 [D loss: 0.564817, acc: 73.44%] [G loss: 3.157689]\n",
      "epoch:2 step:1970 [D loss: 0.560833, acc: 67.97%] [G loss: 3.395520]\n",
      "epoch:2 step:1971 [D loss: 0.588778, acc: 67.97%] [G loss: 3.036709]\n",
      "epoch:2 step:1972 [D loss: 0.603395, acc: 67.97%] [G loss: 3.146778]\n",
      "epoch:2 step:1973 [D loss: 0.521704, acc: 77.34%] [G loss: 3.028282]\n",
      "epoch:2 step:1974 [D loss: 0.492101, acc: 75.78%] [G loss: 3.140603]\n",
      "epoch:2 step:1975 [D loss: 0.634621, acc: 65.62%] [G loss: 3.061593]\n",
      "epoch:2 step:1976 [D loss: 0.625586, acc: 65.62%] [G loss: 3.039883]\n",
      "epoch:2 step:1977 [D loss: 0.489302, acc: 75.78%] [G loss: 3.082832]\n",
      "epoch:2 step:1978 [D loss: 0.468874, acc: 80.47%] [G loss: 3.106808]\n",
      "epoch:2 step:1979 [D loss: 0.581048, acc: 67.97%] [G loss: 2.838547]\n",
      "epoch:2 step:1980 [D loss: 0.697400, acc: 59.38%] [G loss: 2.540249]\n",
      "epoch:2 step:1981 [D loss: 0.647496, acc: 65.62%] [G loss: 2.730972]\n",
      "epoch:2 step:1982 [D loss: 0.657354, acc: 67.97%] [G loss: 2.964129]\n",
      "epoch:2 step:1983 [D loss: 0.612020, acc: 65.62%] [G loss: 2.977480]\n",
      "epoch:2 step:1984 [D loss: 0.493816, acc: 75.00%] [G loss: 3.072652]\n",
      "epoch:2 step:1985 [D loss: 0.568190, acc: 73.44%] [G loss: 2.710886]\n",
      "epoch:2 step:1986 [D loss: 0.606946, acc: 73.44%] [G loss: 2.996414]\n",
      "epoch:2 step:1987 [D loss: 0.644701, acc: 64.06%] [G loss: 2.909995]\n",
      "epoch:2 step:1988 [D loss: 0.618103, acc: 69.53%] [G loss: 2.709053]\n",
      "epoch:2 step:1989 [D loss: 0.720552, acc: 61.72%] [G loss: 2.564781]\n",
      "epoch:2 step:1990 [D loss: 0.687646, acc: 57.81%] [G loss: 2.556959]\n",
      "epoch:2 step:1991 [D loss: 0.575138, acc: 69.53%] [G loss: 3.184493]\n",
      "epoch:2 step:1992 [D loss: 0.581607, acc: 64.06%] [G loss: 3.114948]\n",
      "epoch:2 step:1993 [D loss: 0.529746, acc: 78.91%] [G loss: 2.975120]\n",
      "epoch:2 step:1994 [D loss: 0.627024, acc: 65.62%] [G loss: 2.756680]\n",
      "epoch:2 step:1995 [D loss: 0.594152, acc: 66.41%] [G loss: 2.949562]\n",
      "epoch:2 step:1996 [D loss: 0.663998, acc: 68.75%] [G loss: 2.630963]\n",
      "epoch:2 step:1997 [D loss: 0.609972, acc: 68.75%] [G loss: 2.926759]\n",
      "epoch:2 step:1998 [D loss: 0.552020, acc: 71.09%] [G loss: 2.875778]\n",
      "epoch:2 step:1999 [D loss: 0.566658, acc: 73.44%] [G loss: 2.717253]\n",
      "epoch:2 step:2000 [D loss: 0.560562, acc: 75.00%] [G loss: 2.773502]\n",
      "##############\n",
      "[3.27350025 2.70436304 7.31086262 5.91954516 4.70938564 6.46269195\n",
      " 5.44212533 5.57531194 5.85414197 4.31232952]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.539053, acc: 75.00%] [G loss: 3.405256]\n",
      "epoch:2 step:2002 [D loss: 0.656084, acc: 63.28%] [G loss: 3.213813]\n",
      "epoch:2 step:2003 [D loss: 0.639847, acc: 66.41%] [G loss: 2.786304]\n",
      "epoch:2 step:2004 [D loss: 0.534998, acc: 73.44%] [G loss: 2.888062]\n",
      "epoch:2 step:2005 [D loss: 0.556960, acc: 72.66%] [G loss: 2.849811]\n",
      "epoch:2 step:2006 [D loss: 0.541056, acc: 75.78%] [G loss: 2.976839]\n",
      "epoch:2 step:2007 [D loss: 0.575486, acc: 70.31%] [G loss: 2.839879]\n",
      "epoch:2 step:2008 [D loss: 0.534765, acc: 71.09%] [G loss: 3.251637]\n",
      "epoch:2 step:2009 [D loss: 0.453858, acc: 78.91%] [G loss: 3.401654]\n",
      "epoch:2 step:2010 [D loss: 0.535277, acc: 73.44%] [G loss: 2.841553]\n",
      "epoch:2 step:2011 [D loss: 0.646811, acc: 64.84%] [G loss: 2.873569]\n",
      "epoch:2 step:2012 [D loss: 0.535024, acc: 76.56%] [G loss: 2.872683]\n",
      "epoch:2 step:2013 [D loss: 0.639471, acc: 67.97%] [G loss: 2.684463]\n",
      "epoch:2 step:2014 [D loss: 0.445948, acc: 85.94%] [G loss: 2.723431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2015 [D loss: 0.431320, acc: 79.69%] [G loss: 3.417747]\n",
      "epoch:2 step:2016 [D loss: 0.503156, acc: 76.56%] [G loss: 3.084079]\n",
      "epoch:2 step:2017 [D loss: 0.579890, acc: 74.22%] [G loss: 2.991775]\n",
      "epoch:2 step:2018 [D loss: 0.551544, acc: 71.88%] [G loss: 2.981893]\n",
      "epoch:2 step:2019 [D loss: 0.587200, acc: 67.19%] [G loss: 2.866460]\n",
      "epoch:2 step:2020 [D loss: 0.573415, acc: 70.31%] [G loss: 3.067621]\n",
      "epoch:2 step:2021 [D loss: 0.609607, acc: 70.31%] [G loss: 2.659802]\n",
      "epoch:2 step:2022 [D loss: 0.633062, acc: 64.06%] [G loss: 3.114788]\n",
      "epoch:2 step:2023 [D loss: 0.482063, acc: 78.91%] [G loss: 3.338910]\n",
      "epoch:2 step:2024 [D loss: 0.470168, acc: 78.12%] [G loss: 3.046442]\n",
      "epoch:2 step:2025 [D loss: 0.427512, acc: 79.69%] [G loss: 3.801768]\n",
      "epoch:2 step:2026 [D loss: 0.480885, acc: 75.78%] [G loss: 3.640799]\n",
      "epoch:2 step:2027 [D loss: 0.645309, acc: 62.50%] [G loss: 2.853961]\n",
      "epoch:2 step:2028 [D loss: 0.533946, acc: 73.44%] [G loss: 3.014391]\n",
      "epoch:2 step:2029 [D loss: 0.554214, acc: 73.44%] [G loss: 3.678599]\n",
      "epoch:2 step:2030 [D loss: 0.549355, acc: 67.97%] [G loss: 2.995150]\n",
      "epoch:2 step:2031 [D loss: 0.446548, acc: 78.12%] [G loss: 2.742555]\n",
      "epoch:2 step:2032 [D loss: 0.574698, acc: 73.44%] [G loss: 2.869141]\n",
      "epoch:2 step:2033 [D loss: 0.511443, acc: 73.44%] [G loss: 2.707819]\n",
      "epoch:2 step:2034 [D loss: 0.589622, acc: 66.41%] [G loss: 2.747164]\n",
      "epoch:2 step:2035 [D loss: 0.595474, acc: 70.31%] [G loss: 3.047400]\n",
      "epoch:2 step:2036 [D loss: 0.542876, acc: 72.66%] [G loss: 3.466443]\n",
      "epoch:2 step:2037 [D loss: 0.542597, acc: 73.44%] [G loss: 3.556960]\n",
      "epoch:2 step:2038 [D loss: 0.598181, acc: 70.31%] [G loss: 3.114661]\n",
      "epoch:2 step:2039 [D loss: 0.518337, acc: 76.56%] [G loss: 3.203961]\n",
      "epoch:2 step:2040 [D loss: 0.510215, acc: 74.22%] [G loss: 2.920618]\n",
      "epoch:2 step:2041 [D loss: 0.608459, acc: 61.72%] [G loss: 3.087540]\n",
      "epoch:2 step:2042 [D loss: 0.518091, acc: 73.44%] [G loss: 3.011662]\n",
      "epoch:2 step:2043 [D loss: 0.567938, acc: 73.44%] [G loss: 2.756507]\n",
      "epoch:2 step:2044 [D loss: 0.552089, acc: 71.88%] [G loss: 3.192567]\n",
      "epoch:2 step:2045 [D loss: 0.499732, acc: 74.22%] [G loss: 2.843935]\n",
      "epoch:2 step:2046 [D loss: 0.476770, acc: 77.34%] [G loss: 2.949133]\n",
      "epoch:2 step:2047 [D loss: 0.639998, acc: 66.41%] [G loss: 3.221963]\n",
      "epoch:2 step:2048 [D loss: 0.500776, acc: 78.12%] [G loss: 3.251792]\n",
      "epoch:2 step:2049 [D loss: 0.456968, acc: 78.12%] [G loss: 3.173199]\n",
      "epoch:2 step:2050 [D loss: 0.579200, acc: 67.19%] [G loss: 3.153376]\n",
      "epoch:2 step:2051 [D loss: 0.623230, acc: 67.19%] [G loss: 3.036406]\n",
      "epoch:2 step:2052 [D loss: 0.529407, acc: 71.09%] [G loss: 3.163611]\n",
      "epoch:2 step:2053 [D loss: 0.594065, acc: 69.53%] [G loss: 2.832774]\n",
      "epoch:2 step:2054 [D loss: 0.608022, acc: 64.06%] [G loss: 2.757821]\n",
      "epoch:2 step:2055 [D loss: 0.617886, acc: 62.50%] [G loss: 3.027841]\n",
      "epoch:2 step:2056 [D loss: 0.568122, acc: 70.31%] [G loss: 3.116701]\n",
      "epoch:2 step:2057 [D loss: 0.469724, acc: 79.69%] [G loss: 3.226598]\n",
      "epoch:2 step:2058 [D loss: 0.612192, acc: 72.66%] [G loss: 3.036131]\n",
      "epoch:2 step:2059 [D loss: 0.522425, acc: 74.22%] [G loss: 2.837654]\n",
      "epoch:2 step:2060 [D loss: 0.589924, acc: 67.97%] [G loss: 2.878550]\n",
      "epoch:2 step:2061 [D loss: 0.613260, acc: 64.06%] [G loss: 2.900629]\n",
      "epoch:2 step:2062 [D loss: 0.562839, acc: 72.66%] [G loss: 2.751762]\n",
      "epoch:2 step:2063 [D loss: 0.563547, acc: 73.44%] [G loss: 2.938731]\n",
      "epoch:2 step:2064 [D loss: 0.518069, acc: 71.09%] [G loss: 2.897612]\n",
      "epoch:2 step:2065 [D loss: 0.475724, acc: 81.25%] [G loss: 3.178956]\n",
      "epoch:2 step:2066 [D loss: 0.620462, acc: 67.19%] [G loss: 3.063958]\n",
      "epoch:2 step:2067 [D loss: 0.505127, acc: 73.44%] [G loss: 3.133582]\n",
      "epoch:2 step:2068 [D loss: 0.532919, acc: 76.56%] [G loss: 3.507199]\n",
      "epoch:2 step:2069 [D loss: 0.499313, acc: 76.56%] [G loss: 3.139913]\n",
      "epoch:2 step:2070 [D loss: 0.519707, acc: 75.00%] [G loss: 2.794852]\n",
      "epoch:2 step:2071 [D loss: 0.524011, acc: 72.66%] [G loss: 3.236485]\n",
      "epoch:2 step:2072 [D loss: 0.619244, acc: 64.06%] [G loss: 2.867355]\n",
      "epoch:2 step:2073 [D loss: 0.503985, acc: 73.44%] [G loss: 2.803020]\n",
      "epoch:2 step:2074 [D loss: 0.637885, acc: 65.62%] [G loss: 3.109884]\n",
      "epoch:2 step:2075 [D loss: 0.553467, acc: 69.53%] [G loss: 2.905206]\n",
      "epoch:2 step:2076 [D loss: 0.517023, acc: 73.44%] [G loss: 2.835124]\n",
      "epoch:2 step:2077 [D loss: 0.646305, acc: 66.41%] [G loss: 3.133955]\n",
      "epoch:2 step:2078 [D loss: 0.546237, acc: 72.66%] [G loss: 3.053419]\n",
      "epoch:2 step:2079 [D loss: 0.613683, acc: 66.41%] [G loss: 2.797981]\n",
      "epoch:2 step:2080 [D loss: 0.607789, acc: 65.62%] [G loss: 2.788112]\n",
      "epoch:2 step:2081 [D loss: 0.509338, acc: 73.44%] [G loss: 3.362378]\n",
      "epoch:2 step:2082 [D loss: 0.484927, acc: 80.47%] [G loss: 3.185321]\n",
      "epoch:2 step:2083 [D loss: 0.529755, acc: 72.66%] [G loss: 2.810798]\n",
      "epoch:2 step:2084 [D loss: 0.599314, acc: 70.31%] [G loss: 3.060983]\n",
      "epoch:2 step:2085 [D loss: 0.520962, acc: 73.44%] [G loss: 3.161139]\n",
      "epoch:2 step:2086 [D loss: 0.491934, acc: 80.47%] [G loss: 3.219258]\n",
      "epoch:2 step:2087 [D loss: 0.628491, acc: 71.09%] [G loss: 3.047118]\n",
      "epoch:2 step:2088 [D loss: 0.595817, acc: 67.19%] [G loss: 2.561301]\n",
      "epoch:2 step:2089 [D loss: 0.650544, acc: 61.72%] [G loss: 2.511061]\n",
      "epoch:2 step:2090 [D loss: 0.579268, acc: 70.31%] [G loss: 2.904794]\n",
      "epoch:2 step:2091 [D loss: 0.530044, acc: 74.22%] [G loss: 3.021906]\n",
      "epoch:2 step:2092 [D loss: 0.535824, acc: 74.22%] [G loss: 2.972068]\n",
      "epoch:2 step:2093 [D loss: 0.630184, acc: 67.97%] [G loss: 2.902423]\n",
      "epoch:2 step:2094 [D loss: 0.648508, acc: 64.84%] [G loss: 3.193985]\n",
      "epoch:2 step:2095 [D loss: 0.629216, acc: 65.62%] [G loss: 3.096368]\n",
      "epoch:2 step:2096 [D loss: 0.514015, acc: 76.56%] [G loss: 2.953747]\n",
      "epoch:2 step:2097 [D loss: 0.573398, acc: 70.31%] [G loss: 3.086919]\n",
      "epoch:2 step:2098 [D loss: 0.543769, acc: 73.44%] [G loss: 2.936624]\n",
      "epoch:2 step:2099 [D loss: 0.580086, acc: 70.31%] [G loss: 2.752990]\n",
      "epoch:2 step:2100 [D loss: 0.563781, acc: 70.31%] [G loss: 3.014627]\n",
      "epoch:2 step:2101 [D loss: 0.556163, acc: 71.09%] [G loss: 2.835421]\n",
      "epoch:2 step:2102 [D loss: 0.580724, acc: 67.19%] [G loss: 2.679282]\n",
      "epoch:2 step:2103 [D loss: 0.613289, acc: 64.06%] [G loss: 2.617076]\n",
      "epoch:2 step:2104 [D loss: 0.430831, acc: 79.69%] [G loss: 3.197294]\n",
      "epoch:2 step:2105 [D loss: 0.495359, acc: 75.00%] [G loss: 3.265195]\n",
      "epoch:2 step:2106 [D loss: 0.419947, acc: 80.47%] [G loss: 3.568694]\n",
      "epoch:2 step:2107 [D loss: 0.550326, acc: 71.09%] [G loss: 3.213848]\n",
      "epoch:2 step:2108 [D loss: 0.604586, acc: 64.06%] [G loss: 3.570181]\n",
      "epoch:2 step:2109 [D loss: 0.518243, acc: 71.09%] [G loss: 3.529771]\n",
      "epoch:2 step:2110 [D loss: 0.482493, acc: 73.44%] [G loss: 2.824029]\n",
      "epoch:2 step:2111 [D loss: 0.628900, acc: 67.97%] [G loss: 3.023199]\n",
      "epoch:2 step:2112 [D loss: 0.582096, acc: 71.09%] [G loss: 2.677050]\n",
      "epoch:2 step:2113 [D loss: 0.575404, acc: 71.88%] [G loss: 2.933921]\n",
      "epoch:2 step:2114 [D loss: 0.523071, acc: 71.88%] [G loss: 3.166602]\n",
      "epoch:2 step:2115 [D loss: 0.654563, acc: 61.72%] [G loss: 2.670947]\n",
      "epoch:2 step:2116 [D loss: 0.618402, acc: 63.28%] [G loss: 2.979316]\n",
      "epoch:2 step:2117 [D loss: 0.495970, acc: 75.78%] [G loss: 3.018271]\n",
      "epoch:2 step:2118 [D loss: 0.550793, acc: 68.75%] [G loss: 2.702360]\n",
      "epoch:2 step:2119 [D loss: 0.584536, acc: 70.31%] [G loss: 2.830610]\n",
      "epoch:2 step:2120 [D loss: 0.588448, acc: 66.41%] [G loss: 2.956783]\n",
      "epoch:2 step:2121 [D loss: 0.626397, acc: 67.19%] [G loss: 2.698341]\n",
      "epoch:2 step:2122 [D loss: 0.576337, acc: 64.84%] [G loss: 2.694712]\n",
      "epoch:2 step:2123 [D loss: 0.529938, acc: 74.22%] [G loss: 3.279000]\n",
      "epoch:2 step:2124 [D loss: 0.628122, acc: 67.19%] [G loss: 3.030300]\n",
      "epoch:2 step:2125 [D loss: 0.593641, acc: 70.31%] [G loss: 2.890988]\n",
      "epoch:2 step:2126 [D loss: 0.668913, acc: 59.38%] [G loss: 2.707897]\n",
      "epoch:2 step:2127 [D loss: 0.582902, acc: 70.31%] [G loss: 2.625134]\n",
      "epoch:2 step:2128 [D loss: 0.516804, acc: 75.78%] [G loss: 3.103150]\n",
      "epoch:2 step:2129 [D loss: 0.564963, acc: 70.31%] [G loss: 2.842798]\n",
      "epoch:2 step:2130 [D loss: 0.504202, acc: 75.00%] [G loss: 3.168200]\n",
      "epoch:2 step:2131 [D loss: 0.587207, acc: 69.53%] [G loss: 2.551749]\n",
      "epoch:2 step:2132 [D loss: 0.464582, acc: 77.34%] [G loss: 3.092984]\n",
      "epoch:2 step:2133 [D loss: 0.499402, acc: 76.56%] [G loss: 3.233699]\n",
      "epoch:2 step:2134 [D loss: 0.472032, acc: 79.69%] [G loss: 2.931677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2135 [D loss: 0.527528, acc: 74.22%] [G loss: 3.499139]\n",
      "epoch:2 step:2136 [D loss: 0.537383, acc: 72.66%] [G loss: 3.195703]\n",
      "epoch:2 step:2137 [D loss: 0.645410, acc: 63.28%] [G loss: 2.542767]\n",
      "epoch:2 step:2138 [D loss: 0.554819, acc: 73.44%] [G loss: 2.990252]\n",
      "epoch:2 step:2139 [D loss: 0.600665, acc: 67.97%] [G loss: 3.389957]\n",
      "epoch:2 step:2140 [D loss: 0.495963, acc: 75.00%] [G loss: 2.931787]\n",
      "epoch:2 step:2141 [D loss: 0.592144, acc: 67.19%] [G loss: 3.080217]\n",
      "epoch:2 step:2142 [D loss: 0.596956, acc: 71.88%] [G loss: 2.722537]\n",
      "epoch:2 step:2143 [D loss: 0.668911, acc: 63.28%] [G loss: 2.749070]\n",
      "epoch:2 step:2144 [D loss: 0.540484, acc: 67.97%] [G loss: 3.054064]\n",
      "epoch:2 step:2145 [D loss: 0.558656, acc: 70.31%] [G loss: 3.144181]\n",
      "epoch:2 step:2146 [D loss: 0.568134, acc: 70.31%] [G loss: 2.827495]\n",
      "epoch:2 step:2147 [D loss: 0.499475, acc: 75.00%] [G loss: 3.169158]\n",
      "epoch:2 step:2148 [D loss: 0.645778, acc: 61.72%] [G loss: 2.795378]\n",
      "epoch:2 step:2149 [D loss: 0.608946, acc: 67.97%] [G loss: 2.827082]\n",
      "epoch:2 step:2150 [D loss: 0.615637, acc: 64.06%] [G loss: 2.693885]\n",
      "epoch:2 step:2151 [D loss: 0.624519, acc: 64.06%] [G loss: 2.713698]\n",
      "epoch:2 step:2152 [D loss: 0.579273, acc: 71.88%] [G loss: 2.767339]\n",
      "epoch:2 step:2153 [D loss: 0.637205, acc: 67.19%] [G loss: 3.209746]\n",
      "epoch:2 step:2154 [D loss: 0.528010, acc: 73.44%] [G loss: 3.090254]\n",
      "epoch:2 step:2155 [D loss: 0.631661, acc: 60.94%] [G loss: 3.210928]\n",
      "epoch:2 step:2156 [D loss: 0.554704, acc: 67.97%] [G loss: 2.716689]\n",
      "epoch:2 step:2157 [D loss: 0.502471, acc: 78.12%] [G loss: 2.977765]\n",
      "epoch:2 step:2158 [D loss: 0.520944, acc: 76.56%] [G loss: 2.975070]\n",
      "epoch:2 step:2159 [D loss: 0.500592, acc: 72.66%] [G loss: 3.371299]\n",
      "epoch:2 step:2160 [D loss: 0.627706, acc: 64.84%] [G loss: 3.187121]\n",
      "epoch:2 step:2161 [D loss: 0.648278, acc: 70.31%] [G loss: 2.797164]\n",
      "epoch:2 step:2162 [D loss: 0.657108, acc: 63.28%] [G loss: 2.584949]\n",
      "epoch:2 step:2163 [D loss: 0.557781, acc: 71.09%] [G loss: 2.566576]\n",
      "epoch:2 step:2164 [D loss: 0.517758, acc: 73.44%] [G loss: 3.001776]\n",
      "epoch:2 step:2165 [D loss: 0.602192, acc: 72.66%] [G loss: 3.030451]\n",
      "epoch:2 step:2166 [D loss: 0.601932, acc: 69.53%] [G loss: 3.186594]\n",
      "epoch:2 step:2167 [D loss: 0.525273, acc: 73.44%] [G loss: 3.040057]\n",
      "epoch:2 step:2168 [D loss: 0.625877, acc: 67.19%] [G loss: 2.805814]\n",
      "epoch:2 step:2169 [D loss: 0.514482, acc: 70.31%] [G loss: 2.844079]\n",
      "epoch:2 step:2170 [D loss: 0.516147, acc: 74.22%] [G loss: 2.963813]\n",
      "epoch:2 step:2171 [D loss: 0.491772, acc: 76.56%] [G loss: 2.800189]\n",
      "epoch:2 step:2172 [D loss: 0.602236, acc: 66.41%] [G loss: 2.973602]\n",
      "epoch:2 step:2173 [D loss: 0.606537, acc: 67.97%] [G loss: 2.926369]\n",
      "epoch:2 step:2174 [D loss: 0.558946, acc: 73.44%] [G loss: 3.019255]\n",
      "epoch:2 step:2175 [D loss: 0.599070, acc: 70.31%] [G loss: 2.750269]\n",
      "epoch:2 step:2176 [D loss: 0.509391, acc: 76.56%] [G loss: 2.740806]\n",
      "epoch:2 step:2177 [D loss: 0.601127, acc: 67.19%] [G loss: 2.859736]\n",
      "epoch:2 step:2178 [D loss: 0.514864, acc: 76.56%] [G loss: 3.199882]\n",
      "epoch:2 step:2179 [D loss: 0.603280, acc: 67.97%] [G loss: 2.717325]\n",
      "epoch:2 step:2180 [D loss: 0.604048, acc: 67.19%] [G loss: 2.707270]\n",
      "epoch:2 step:2181 [D loss: 0.486474, acc: 77.34%] [G loss: 2.852991]\n",
      "epoch:2 step:2182 [D loss: 0.603888, acc: 68.75%] [G loss: 3.005511]\n",
      "epoch:2 step:2183 [D loss: 0.502312, acc: 75.00%] [G loss: 3.178102]\n",
      "epoch:2 step:2184 [D loss: 0.513408, acc: 75.78%] [G loss: 3.506345]\n",
      "epoch:2 step:2185 [D loss: 0.540225, acc: 69.53%] [G loss: 2.906574]\n",
      "epoch:2 step:2186 [D loss: 0.499792, acc: 74.22%] [G loss: 3.449224]\n",
      "epoch:2 step:2187 [D loss: 0.577639, acc: 72.66%] [G loss: 3.475049]\n",
      "epoch:2 step:2188 [D loss: 0.488380, acc: 73.44%] [G loss: 3.927447]\n",
      "epoch:2 step:2189 [D loss: 0.456683, acc: 78.91%] [G loss: 3.759951]\n",
      "epoch:2 step:2190 [D loss: 0.628327, acc: 71.09%] [G loss: 2.819918]\n",
      "epoch:2 step:2191 [D loss: 0.581196, acc: 71.09%] [G loss: 2.903515]\n",
      "epoch:2 step:2192 [D loss: 0.496835, acc: 73.44%] [G loss: 2.790857]\n",
      "epoch:2 step:2193 [D loss: 0.533321, acc: 71.88%] [G loss: 2.778941]\n",
      "epoch:2 step:2194 [D loss: 0.529111, acc: 73.44%] [G loss: 2.975618]\n",
      "epoch:2 step:2195 [D loss: 0.546006, acc: 75.00%] [G loss: 3.110010]\n",
      "epoch:2 step:2196 [D loss: 0.520306, acc: 72.66%] [G loss: 3.034145]\n",
      "epoch:2 step:2197 [D loss: 0.583713, acc: 66.41%] [G loss: 3.276968]\n",
      "epoch:2 step:2198 [D loss: 0.536307, acc: 71.09%] [G loss: 3.083663]\n",
      "epoch:2 step:2199 [D loss: 0.560507, acc: 70.31%] [G loss: 3.375195]\n",
      "epoch:2 step:2200 [D loss: 0.506266, acc: 73.44%] [G loss: 3.153772]\n",
      "##############\n",
      "[3.23375536 2.06092087 7.25744034 5.60476233 4.56970977 6.31584358\n",
      " 5.39984914 5.66548852 5.65461463 4.10142515]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.611404, acc: 70.31%] [G loss: 2.881414]\n",
      "epoch:2 step:2202 [D loss: 0.547008, acc: 73.44%] [G loss: 3.138885]\n",
      "epoch:2 step:2203 [D loss: 0.495306, acc: 80.47%] [G loss: 2.896906]\n",
      "epoch:2 step:2204 [D loss: 0.487415, acc: 75.00%] [G loss: 3.309328]\n",
      "epoch:2 step:2205 [D loss: 0.526212, acc: 71.09%] [G loss: 3.067194]\n",
      "epoch:2 step:2206 [D loss: 0.609639, acc: 64.06%] [G loss: 2.606503]\n",
      "epoch:2 step:2207 [D loss: 0.577861, acc: 74.22%] [G loss: 3.234617]\n",
      "epoch:2 step:2208 [D loss: 0.545935, acc: 75.78%] [G loss: 3.053771]\n",
      "epoch:2 step:2209 [D loss: 0.526350, acc: 72.66%] [G loss: 3.073503]\n",
      "epoch:2 step:2210 [D loss: 0.489293, acc: 75.00%] [G loss: 3.391866]\n",
      "epoch:2 step:2211 [D loss: 0.570264, acc: 75.78%] [G loss: 3.075484]\n",
      "epoch:2 step:2212 [D loss: 0.610772, acc: 65.62%] [G loss: 2.875157]\n",
      "epoch:2 step:2213 [D loss: 0.614917, acc: 67.97%] [G loss: 2.880084]\n",
      "epoch:2 step:2214 [D loss: 0.582697, acc: 68.75%] [G loss: 2.752393]\n",
      "epoch:2 step:2215 [D loss: 0.482942, acc: 76.56%] [G loss: 2.861656]\n",
      "epoch:2 step:2216 [D loss: 0.578709, acc: 67.19%] [G loss: 3.368373]\n",
      "epoch:2 step:2217 [D loss: 0.378276, acc: 87.50%] [G loss: 3.423241]\n",
      "epoch:2 step:2218 [D loss: 0.499895, acc: 74.22%] [G loss: 3.338142]\n",
      "epoch:2 step:2219 [D loss: 0.541660, acc: 73.44%] [G loss: 3.426675]\n",
      "epoch:2 step:2220 [D loss: 0.498039, acc: 80.47%] [G loss: 3.513294]\n",
      "epoch:2 step:2221 [D loss: 0.398252, acc: 81.25%] [G loss: 3.770584]\n",
      "epoch:2 step:2222 [D loss: 0.639107, acc: 67.97%] [G loss: 2.969267]\n",
      "epoch:2 step:2223 [D loss: 0.773666, acc: 53.12%] [G loss: 2.708781]\n",
      "epoch:2 step:2224 [D loss: 0.587104, acc: 72.66%] [G loss: 3.257450]\n",
      "epoch:2 step:2225 [D loss: 0.523509, acc: 75.00%] [G loss: 3.322721]\n",
      "epoch:2 step:2226 [D loss: 0.636904, acc: 71.88%] [G loss: 2.651143]\n",
      "epoch:2 step:2227 [D loss: 0.636286, acc: 65.62%] [G loss: 2.620667]\n",
      "epoch:2 step:2228 [D loss: 0.444986, acc: 80.47%] [G loss: 3.137924]\n",
      "epoch:2 step:2229 [D loss: 0.540754, acc: 71.09%] [G loss: 2.984499]\n",
      "epoch:2 step:2230 [D loss: 0.603030, acc: 69.53%] [G loss: 2.545014]\n",
      "epoch:2 step:2231 [D loss: 0.469226, acc: 80.47%] [G loss: 3.280378]\n",
      "epoch:2 step:2232 [D loss: 0.513357, acc: 74.22%] [G loss: 3.104330]\n",
      "epoch:2 step:2233 [D loss: 0.489022, acc: 74.22%] [G loss: 3.030900]\n",
      "epoch:2 step:2234 [D loss: 0.555675, acc: 69.53%] [G loss: 2.929871]\n",
      "epoch:2 step:2235 [D loss: 0.633895, acc: 65.62%] [G loss: 3.193779]\n",
      "epoch:2 step:2236 [D loss: 0.575364, acc: 68.75%] [G loss: 2.999256]\n",
      "epoch:2 step:2237 [D loss: 0.621176, acc: 71.09%] [G loss: 3.179519]\n",
      "epoch:2 step:2238 [D loss: 0.517506, acc: 78.12%] [G loss: 2.844672]\n",
      "epoch:2 step:2239 [D loss: 0.533394, acc: 73.44%] [G loss: 3.130708]\n",
      "epoch:2 step:2240 [D loss: 0.490110, acc: 79.69%] [G loss: 3.711916]\n",
      "epoch:2 step:2241 [D loss: 0.518276, acc: 75.00%] [G loss: 2.990464]\n",
      "epoch:2 step:2242 [D loss: 0.523718, acc: 74.22%] [G loss: 3.605093]\n",
      "epoch:2 step:2243 [D loss: 0.564274, acc: 67.97%] [G loss: 2.899959]\n",
      "epoch:2 step:2244 [D loss: 0.711463, acc: 61.72%] [G loss: 2.660110]\n",
      "epoch:2 step:2245 [D loss: 0.531350, acc: 76.56%] [G loss: 2.895052]\n",
      "epoch:2 step:2246 [D loss: 0.525892, acc: 78.12%] [G loss: 3.039457]\n",
      "epoch:2 step:2247 [D loss: 0.599978, acc: 70.31%] [G loss: 2.693629]\n",
      "epoch:2 step:2248 [D loss: 0.504593, acc: 75.78%] [G loss: 3.231412]\n",
      "epoch:2 step:2249 [D loss: 0.630881, acc: 71.09%] [G loss: 3.050131]\n",
      "epoch:2 step:2250 [D loss: 0.553876, acc: 72.66%] [G loss: 2.750326]\n",
      "epoch:2 step:2251 [D loss: 0.552189, acc: 71.09%] [G loss: 2.731570]\n",
      "epoch:2 step:2252 [D loss: 0.500518, acc: 75.00%] [G loss: 3.080592]\n",
      "epoch:2 step:2253 [D loss: 0.570455, acc: 69.53%] [G loss: 3.004680]\n",
      "epoch:2 step:2254 [D loss: 0.522981, acc: 71.09%] [G loss: 3.111362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2255 [D loss: 0.455322, acc: 80.47%] [G loss: 3.184481]\n",
      "epoch:2 step:2256 [D loss: 0.635822, acc: 68.75%] [G loss: 2.819853]\n",
      "epoch:2 step:2257 [D loss: 0.622419, acc: 64.84%] [G loss: 3.007139]\n",
      "epoch:2 step:2258 [D loss: 0.566307, acc: 75.00%] [G loss: 2.904209]\n",
      "epoch:2 step:2259 [D loss: 0.610698, acc: 67.19%] [G loss: 3.345018]\n",
      "epoch:2 step:2260 [D loss: 0.583936, acc: 70.31%] [G loss: 2.827802]\n",
      "epoch:2 step:2261 [D loss: 0.570563, acc: 69.53%] [G loss: 2.665799]\n",
      "epoch:2 step:2262 [D loss: 0.604068, acc: 67.19%] [G loss: 2.850008]\n",
      "epoch:2 step:2263 [D loss: 0.601023, acc: 64.84%] [G loss: 2.698846]\n",
      "epoch:2 step:2264 [D loss: 0.570519, acc: 71.88%] [G loss: 3.169053]\n",
      "epoch:2 step:2265 [D loss: 0.593764, acc: 70.31%] [G loss: 2.817730]\n",
      "epoch:2 step:2266 [D loss: 0.490069, acc: 73.44%] [G loss: 3.076925]\n",
      "epoch:2 step:2267 [D loss: 0.499144, acc: 76.56%] [G loss: 3.191894]\n",
      "epoch:2 step:2268 [D loss: 0.512482, acc: 75.00%] [G loss: 2.940907]\n",
      "epoch:2 step:2269 [D loss: 0.543450, acc: 71.88%] [G loss: 3.115308]\n",
      "epoch:2 step:2270 [D loss: 0.487732, acc: 78.12%] [G loss: 2.978086]\n",
      "epoch:2 step:2271 [D loss: 0.492768, acc: 78.12%] [G loss: 3.600949]\n",
      "epoch:2 step:2272 [D loss: 0.454567, acc: 83.59%] [G loss: 3.724745]\n",
      "epoch:2 step:2273 [D loss: 0.450886, acc: 78.12%] [G loss: 3.547980]\n",
      "epoch:2 step:2274 [D loss: 0.654642, acc: 64.84%] [G loss: 3.023514]\n",
      "epoch:2 step:2275 [D loss: 0.607324, acc: 64.84%] [G loss: 3.220290]\n",
      "epoch:2 step:2276 [D loss: 0.485924, acc: 75.78%] [G loss: 3.064204]\n",
      "epoch:2 step:2277 [D loss: 0.520149, acc: 75.00%] [G loss: 3.546877]\n",
      "epoch:2 step:2278 [D loss: 0.641935, acc: 63.28%] [G loss: 3.371009]\n",
      "epoch:2 step:2279 [D loss: 0.645229, acc: 66.41%] [G loss: 3.079805]\n",
      "epoch:2 step:2280 [D loss: 0.504208, acc: 73.44%] [G loss: 3.214142]\n",
      "epoch:2 step:2281 [D loss: 0.569788, acc: 66.41%] [G loss: 2.842563]\n",
      "epoch:2 step:2282 [D loss: 0.547693, acc: 71.88%] [G loss: 3.150162]\n",
      "epoch:2 step:2283 [D loss: 0.578421, acc: 69.53%] [G loss: 3.002543]\n",
      "epoch:2 step:2284 [D loss: 0.521014, acc: 71.88%] [G loss: 2.889806]\n",
      "epoch:2 step:2285 [D loss: 0.551342, acc: 71.88%] [G loss: 2.844130]\n",
      "epoch:2 step:2286 [D loss: 0.553628, acc: 68.75%] [G loss: 2.765156]\n",
      "epoch:2 step:2287 [D loss: 0.538550, acc: 71.09%] [G loss: 3.248280]\n",
      "epoch:2 step:2288 [D loss: 0.491322, acc: 74.22%] [G loss: 3.014662]\n",
      "epoch:2 step:2289 [D loss: 0.679673, acc: 58.59%] [G loss: 2.766908]\n",
      "epoch:2 step:2290 [D loss: 0.575807, acc: 67.97%] [G loss: 2.700148]\n",
      "epoch:2 step:2291 [D loss: 0.636717, acc: 62.50%] [G loss: 2.905738]\n",
      "epoch:2 step:2292 [D loss: 0.613897, acc: 72.66%] [G loss: 2.923750]\n",
      "epoch:2 step:2293 [D loss: 0.572760, acc: 71.88%] [G loss: 2.909649]\n",
      "epoch:2 step:2294 [D loss: 0.521145, acc: 75.00%] [G loss: 2.689543]\n",
      "epoch:2 step:2295 [D loss: 0.542169, acc: 71.09%] [G loss: 3.155648]\n",
      "epoch:2 step:2296 [D loss: 0.626712, acc: 66.41%] [G loss: 3.182102]\n",
      "epoch:2 step:2297 [D loss: 0.496948, acc: 75.78%] [G loss: 3.250672]\n",
      "epoch:2 step:2298 [D loss: 0.606411, acc: 71.88%] [G loss: 3.060539]\n",
      "epoch:2 step:2299 [D loss: 0.593400, acc: 71.88%] [G loss: 2.954095]\n",
      "epoch:2 step:2300 [D loss: 0.505305, acc: 75.78%] [G loss: 3.577241]\n",
      "epoch:2 step:2301 [D loss: 0.568040, acc: 72.66%] [G loss: 3.282497]\n",
      "epoch:2 step:2302 [D loss: 0.509136, acc: 75.78%] [G loss: 3.563059]\n",
      "epoch:2 step:2303 [D loss: 0.539664, acc: 73.44%] [G loss: 3.391992]\n",
      "epoch:2 step:2304 [D loss: 0.535994, acc: 71.88%] [G loss: 3.348003]\n",
      "epoch:2 step:2305 [D loss: 0.586106, acc: 70.31%] [G loss: 3.298892]\n",
      "epoch:2 step:2306 [D loss: 0.592547, acc: 67.97%] [G loss: 2.862303]\n",
      "epoch:2 step:2307 [D loss: 0.605190, acc: 69.53%] [G loss: 2.780043]\n",
      "epoch:2 step:2308 [D loss: 0.522181, acc: 73.44%] [G loss: 2.958723]\n",
      "epoch:2 step:2309 [D loss: 0.605471, acc: 66.41%] [G loss: 3.181473]\n",
      "epoch:2 step:2310 [D loss: 0.561702, acc: 69.53%] [G loss: 3.136630]\n",
      "epoch:2 step:2311 [D loss: 0.696859, acc: 63.28%] [G loss: 2.903832]\n",
      "epoch:2 step:2312 [D loss: 0.677032, acc: 66.41%] [G loss: 3.094731]\n",
      "epoch:2 step:2313 [D loss: 0.651244, acc: 66.41%] [G loss: 2.962332]\n",
      "epoch:2 step:2314 [D loss: 0.536984, acc: 72.66%] [G loss: 2.916799]\n",
      "epoch:2 step:2315 [D loss: 0.550403, acc: 68.75%] [G loss: 2.942364]\n",
      "epoch:2 step:2316 [D loss: 0.564230, acc: 71.09%] [G loss: 3.092079]\n",
      "epoch:2 step:2317 [D loss: 0.600964, acc: 68.75%] [G loss: 2.802027]\n",
      "epoch:2 step:2318 [D loss: 0.566513, acc: 73.44%] [G loss: 2.755458]\n",
      "epoch:2 step:2319 [D loss: 0.570948, acc: 68.75%] [G loss: 2.749474]\n",
      "epoch:2 step:2320 [D loss: 0.594484, acc: 68.75%] [G loss: 2.713560]\n",
      "epoch:2 step:2321 [D loss: 0.586999, acc: 67.97%] [G loss: 3.036170]\n",
      "epoch:2 step:2322 [D loss: 0.599003, acc: 67.97%] [G loss: 2.918454]\n",
      "epoch:2 step:2323 [D loss: 0.578066, acc: 67.97%] [G loss: 2.644554]\n",
      "epoch:2 step:2324 [D loss: 0.623684, acc: 69.53%] [G loss: 2.723759]\n",
      "epoch:2 step:2325 [D loss: 0.540836, acc: 73.44%] [G loss: 3.032755]\n",
      "epoch:2 step:2326 [D loss: 0.488683, acc: 78.91%] [G loss: 3.215320]\n",
      "epoch:2 step:2327 [D loss: 0.607280, acc: 66.41%] [G loss: 2.769001]\n",
      "epoch:2 step:2328 [D loss: 0.608564, acc: 67.97%] [G loss: 2.679397]\n",
      "epoch:2 step:2329 [D loss: 0.522701, acc: 73.44%] [G loss: 2.850873]\n",
      "epoch:2 step:2330 [D loss: 0.656026, acc: 64.06%] [G loss: 2.804765]\n",
      "epoch:2 step:2331 [D loss: 0.605215, acc: 66.41%] [G loss: 2.845196]\n",
      "epoch:2 step:2332 [D loss: 0.559726, acc: 69.53%] [G loss: 2.892418]\n",
      "epoch:2 step:2333 [D loss: 0.555212, acc: 74.22%] [G loss: 2.719666]\n",
      "epoch:2 step:2334 [D loss: 0.512150, acc: 77.34%] [G loss: 3.206005]\n",
      "epoch:2 step:2335 [D loss: 0.444127, acc: 82.03%] [G loss: 2.930769]\n",
      "epoch:2 step:2336 [D loss: 0.492547, acc: 74.22%] [G loss: 2.967892]\n",
      "epoch:2 step:2337 [D loss: 0.633051, acc: 67.19%] [G loss: 2.853563]\n",
      "epoch:2 step:2338 [D loss: 0.561135, acc: 70.31%] [G loss: 2.704803]\n",
      "epoch:2 step:2339 [D loss: 0.665950, acc: 62.50%] [G loss: 2.589097]\n",
      "epoch:2 step:2340 [D loss: 0.546205, acc: 74.22%] [G loss: 3.007905]\n",
      "epoch:2 step:2341 [D loss: 0.728956, acc: 61.72%] [G loss: 2.761065]\n",
      "epoch:2 step:2342 [D loss: 0.576077, acc: 73.44%] [G loss: 2.793988]\n",
      "epoch:2 step:2343 [D loss: 0.653374, acc: 64.84%] [G loss: 2.523915]\n",
      "epoch:2 step:2344 [D loss: 0.529995, acc: 72.66%] [G loss: 2.827931]\n",
      "epoch:2 step:2345 [D loss: 0.464793, acc: 79.69%] [G loss: 3.143994]\n",
      "epoch:2 step:2346 [D loss: 0.498360, acc: 75.00%] [G loss: 3.456806]\n",
      "epoch:2 step:2347 [D loss: 0.629632, acc: 65.62%] [G loss: 2.905192]\n",
      "epoch:2 step:2348 [D loss: 0.527425, acc: 71.09%] [G loss: 3.277225]\n",
      "epoch:2 step:2349 [D loss: 0.533639, acc: 73.44%] [G loss: 3.206815]\n",
      "epoch:2 step:2350 [D loss: 0.548062, acc: 75.00%] [G loss: 2.566020]\n",
      "epoch:2 step:2351 [D loss: 0.656380, acc: 61.72%] [G loss: 2.829020]\n",
      "epoch:2 step:2352 [D loss: 0.607482, acc: 71.09%] [G loss: 2.919701]\n",
      "epoch:2 step:2353 [D loss: 0.652003, acc: 61.72%] [G loss: 2.629875]\n",
      "epoch:2 step:2354 [D loss: 0.563049, acc: 71.09%] [G loss: 2.553661]\n",
      "epoch:2 step:2355 [D loss: 0.549770, acc: 71.88%] [G loss: 3.015458]\n",
      "epoch:2 step:2356 [D loss: 0.597438, acc: 70.31%] [G loss: 2.724840]\n",
      "epoch:2 step:2357 [D loss: 0.549196, acc: 68.75%] [G loss: 2.967045]\n",
      "epoch:2 step:2358 [D loss: 0.492687, acc: 70.31%] [G loss: 3.025451]\n",
      "epoch:2 step:2359 [D loss: 0.443096, acc: 79.69%] [G loss: 3.187902]\n",
      "epoch:2 step:2360 [D loss: 0.579377, acc: 70.31%] [G loss: 2.820828]\n",
      "epoch:2 step:2361 [D loss: 0.535039, acc: 73.44%] [G loss: 2.940680]\n",
      "epoch:2 step:2362 [D loss: 0.459576, acc: 78.12%] [G loss: 3.230881]\n",
      "epoch:2 step:2363 [D loss: 0.556220, acc: 72.66%] [G loss: 3.026940]\n",
      "epoch:2 step:2364 [D loss: 0.630334, acc: 66.41%] [G loss: 3.059089]\n",
      "epoch:2 step:2365 [D loss: 0.522468, acc: 76.56%] [G loss: 3.121061]\n",
      "epoch:2 step:2366 [D loss: 0.583988, acc: 67.97%] [G loss: 2.559548]\n",
      "epoch:2 step:2367 [D loss: 0.577225, acc: 67.97%] [G loss: 2.734289]\n",
      "epoch:2 step:2368 [D loss: 0.519799, acc: 75.00%] [G loss: 3.185575]\n",
      "epoch:2 step:2369 [D loss: 0.607188, acc: 67.97%] [G loss: 3.110219]\n",
      "epoch:2 step:2370 [D loss: 0.551517, acc: 73.44%] [G loss: 2.689565]\n",
      "epoch:2 step:2371 [D loss: 0.569556, acc: 70.31%] [G loss: 3.289825]\n",
      "epoch:2 step:2372 [D loss: 0.509289, acc: 72.66%] [G loss: 3.607891]\n",
      "epoch:2 step:2373 [D loss: 0.474591, acc: 78.12%] [G loss: 3.705912]\n",
      "epoch:2 step:2374 [D loss: 0.814512, acc: 55.47%] [G loss: 2.754121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2375 [D loss: 0.612687, acc: 67.97%] [G loss: 2.711335]\n",
      "epoch:2 step:2376 [D loss: 0.582902, acc: 71.09%] [G loss: 2.589499]\n",
      "epoch:2 step:2377 [D loss: 0.619686, acc: 65.62%] [G loss: 2.951128]\n",
      "epoch:2 step:2378 [D loss: 0.490180, acc: 77.34%] [G loss: 3.117719]\n",
      "epoch:2 step:2379 [D loss: 0.611200, acc: 67.19%] [G loss: 2.692251]\n",
      "epoch:2 step:2380 [D loss: 0.534725, acc: 74.22%] [G loss: 2.869937]\n",
      "epoch:2 step:2381 [D loss: 0.558298, acc: 69.53%] [G loss: 2.757704]\n",
      "epoch:2 step:2382 [D loss: 0.500044, acc: 76.56%] [G loss: 3.077695]\n",
      "epoch:2 step:2383 [D loss: 0.619213, acc: 65.62%] [G loss: 2.940516]\n",
      "epoch:2 step:2384 [D loss: 0.683312, acc: 66.41%] [G loss: 2.440696]\n",
      "epoch:2 step:2385 [D loss: 0.610355, acc: 66.41%] [G loss: 2.970963]\n",
      "epoch:2 step:2386 [D loss: 0.592926, acc: 70.31%] [G loss: 2.688303]\n",
      "epoch:2 step:2387 [D loss: 0.506389, acc: 72.66%] [G loss: 2.758039]\n",
      "epoch:2 step:2388 [D loss: 0.504763, acc: 77.34%] [G loss: 3.057192]\n",
      "epoch:2 step:2389 [D loss: 0.561065, acc: 67.97%] [G loss: 3.453827]\n",
      "epoch:2 step:2390 [D loss: 0.502521, acc: 70.31%] [G loss: 2.869973]\n",
      "epoch:2 step:2391 [D loss: 0.622947, acc: 67.19%] [G loss: 2.970757]\n",
      "epoch:2 step:2392 [D loss: 0.465080, acc: 82.03%] [G loss: 2.949937]\n",
      "epoch:2 step:2393 [D loss: 0.507168, acc: 74.22%] [G loss: 3.003716]\n",
      "epoch:2 step:2394 [D loss: 0.543090, acc: 71.88%] [G loss: 3.097945]\n",
      "epoch:2 step:2395 [D loss: 0.481037, acc: 77.34%] [G loss: 2.995101]\n",
      "epoch:2 step:2396 [D loss: 0.508232, acc: 73.44%] [G loss: 3.064617]\n",
      "epoch:2 step:2397 [D loss: 0.566554, acc: 72.66%] [G loss: 3.278320]\n",
      "epoch:2 step:2398 [D loss: 0.598398, acc: 67.19%] [G loss: 3.074455]\n",
      "epoch:2 step:2399 [D loss: 0.608912, acc: 67.19%] [G loss: 2.564737]\n",
      "epoch:2 step:2400 [D loss: 0.521101, acc: 72.66%] [G loss: 2.833597]\n",
      "##############\n",
      "[3.19017317 2.06529809 7.22251826 5.83891528 4.59561362 6.33247967\n",
      " 5.33210948 5.58773099 5.63785835 4.18657623]\n",
      "##########\n",
      "epoch:2 step:2401 [D loss: 0.575389, acc: 67.97%] [G loss: 3.089794]\n",
      "epoch:2 step:2402 [D loss: 0.597909, acc: 64.84%] [G loss: 2.821651]\n",
      "epoch:2 step:2403 [D loss: 0.585241, acc: 69.53%] [G loss: 2.850551]\n",
      "epoch:2 step:2404 [D loss: 0.589130, acc: 72.66%] [G loss: 3.005093]\n",
      "epoch:2 step:2405 [D loss: 0.602155, acc: 67.19%] [G loss: 2.788733]\n",
      "epoch:2 step:2406 [D loss: 0.470448, acc: 78.12%] [G loss: 2.964348]\n",
      "epoch:2 step:2407 [D loss: 0.543153, acc: 71.88%] [G loss: 2.922551]\n",
      "epoch:2 step:2408 [D loss: 0.506064, acc: 76.56%] [G loss: 2.894628]\n",
      "epoch:2 step:2409 [D loss: 0.558654, acc: 71.88%] [G loss: 2.745552]\n",
      "epoch:2 step:2410 [D loss: 0.500653, acc: 76.56%] [G loss: 3.246672]\n",
      "epoch:2 step:2411 [D loss: 0.606070, acc: 69.53%] [G loss: 2.918553]\n",
      "epoch:2 step:2412 [D loss: 0.674047, acc: 60.94%] [G loss: 2.948550]\n",
      "epoch:2 step:2413 [D loss: 0.614496, acc: 65.62%] [G loss: 2.619399]\n",
      "epoch:2 step:2414 [D loss: 0.496038, acc: 73.44%] [G loss: 2.950572]\n",
      "epoch:2 step:2415 [D loss: 0.495013, acc: 76.56%] [G loss: 3.061757]\n",
      "epoch:2 step:2416 [D loss: 0.609381, acc: 70.31%] [G loss: 3.170529]\n",
      "epoch:2 step:2417 [D loss: 0.692073, acc: 57.81%] [G loss: 2.702343]\n",
      "epoch:2 step:2418 [D loss: 0.537849, acc: 71.88%] [G loss: 2.932777]\n",
      "epoch:2 step:2419 [D loss: 0.557743, acc: 70.31%] [G loss: 2.869942]\n",
      "epoch:2 step:2420 [D loss: 0.495134, acc: 73.44%] [G loss: 3.271080]\n",
      "epoch:2 step:2421 [D loss: 0.590779, acc: 68.75%] [G loss: 3.049393]\n",
      "epoch:2 step:2422 [D loss: 0.477451, acc: 77.34%] [G loss: 2.903754]\n",
      "epoch:2 step:2423 [D loss: 0.604703, acc: 67.19%] [G loss: 2.997188]\n",
      "epoch:2 step:2424 [D loss: 0.586889, acc: 70.31%] [G loss: 2.888743]\n",
      "epoch:2 step:2425 [D loss: 0.630928, acc: 64.06%] [G loss: 3.025801]\n",
      "epoch:2 step:2426 [D loss: 0.589772, acc: 68.75%] [G loss: 2.848824]\n",
      "epoch:2 step:2427 [D loss: 0.604227, acc: 66.41%] [G loss: 3.107316]\n",
      "epoch:2 step:2428 [D loss: 0.497092, acc: 75.78%] [G loss: 3.548866]\n",
      "epoch:2 step:2429 [D loss: 0.539877, acc: 71.09%] [G loss: 3.145811]\n",
      "epoch:2 step:2430 [D loss: 0.476135, acc: 82.81%] [G loss: 3.256993]\n",
      "epoch:2 step:2431 [D loss: 0.586210, acc: 74.22%] [G loss: 2.915498]\n",
      "epoch:2 step:2432 [D loss: 0.469215, acc: 73.44%] [G loss: 3.224390]\n",
      "epoch:2 step:2433 [D loss: 0.712664, acc: 56.25%] [G loss: 2.857741]\n",
      "epoch:2 step:2434 [D loss: 0.572565, acc: 70.31%] [G loss: 2.830067]\n",
      "epoch:2 step:2435 [D loss: 0.516252, acc: 75.78%] [G loss: 2.841594]\n",
      "epoch:2 step:2436 [D loss: 0.589431, acc: 63.28%] [G loss: 2.690485]\n",
      "epoch:2 step:2437 [D loss: 0.573441, acc: 67.19%] [G loss: 2.685424]\n",
      "epoch:2 step:2438 [D loss: 0.597911, acc: 67.19%] [G loss: 3.103983]\n",
      "epoch:2 step:2439 [D loss: 0.676660, acc: 61.72%] [G loss: 2.739012]\n",
      "epoch:2 step:2440 [D loss: 0.622085, acc: 70.31%] [G loss: 2.906574]\n",
      "epoch:2 step:2441 [D loss: 0.583733, acc: 71.09%] [G loss: 3.419412]\n",
      "epoch:2 step:2442 [D loss: 0.509081, acc: 75.00%] [G loss: 2.890109]\n",
      "epoch:2 step:2443 [D loss: 0.579985, acc: 67.97%] [G loss: 3.026065]\n",
      "epoch:2 step:2444 [D loss: 0.525800, acc: 73.44%] [G loss: 3.065100]\n",
      "epoch:2 step:2445 [D loss: 0.555259, acc: 71.09%] [G loss: 2.964816]\n",
      "epoch:2 step:2446 [D loss: 0.546578, acc: 76.56%] [G loss: 2.976579]\n",
      "epoch:2 step:2447 [D loss: 0.562791, acc: 69.53%] [G loss: 2.564579]\n",
      "epoch:2 step:2448 [D loss: 0.566410, acc: 72.66%] [G loss: 3.033335]\n",
      "epoch:2 step:2449 [D loss: 0.463722, acc: 78.91%] [G loss: 2.919918]\n",
      "epoch:2 step:2450 [D loss: 0.635834, acc: 67.19%] [G loss: 2.670373]\n",
      "epoch:2 step:2451 [D loss: 0.557832, acc: 74.22%] [G loss: 2.768691]\n",
      "epoch:2 step:2452 [D loss: 0.562541, acc: 70.31%] [G loss: 2.973825]\n",
      "epoch:2 step:2453 [D loss: 0.554557, acc: 70.31%] [G loss: 2.996069]\n",
      "epoch:2 step:2454 [D loss: 0.653829, acc: 64.84%] [G loss: 3.054996]\n",
      "epoch:2 step:2455 [D loss: 0.530155, acc: 73.44%] [G loss: 2.868612]\n",
      "epoch:2 step:2456 [D loss: 0.491353, acc: 79.69%] [G loss: 3.523272]\n",
      "epoch:2 step:2457 [D loss: 0.490953, acc: 74.22%] [G loss: 3.035740]\n",
      "epoch:2 step:2458 [D loss: 0.594534, acc: 61.72%] [G loss: 2.846924]\n",
      "epoch:2 step:2459 [D loss: 0.552449, acc: 77.34%] [G loss: 2.981664]\n",
      "epoch:2 step:2460 [D loss: 0.554522, acc: 70.31%] [G loss: 2.989431]\n",
      "epoch:2 step:2461 [D loss: 0.633056, acc: 69.53%] [G loss: 2.600828]\n",
      "epoch:2 step:2462 [D loss: 0.621056, acc: 67.19%] [G loss: 2.820666]\n",
      "epoch:2 step:2463 [D loss: 0.554914, acc: 70.31%] [G loss: 2.947700]\n",
      "epoch:2 step:2464 [D loss: 0.525993, acc: 73.44%] [G loss: 2.679282]\n",
      "epoch:2 step:2465 [D loss: 0.568962, acc: 71.88%] [G loss: 2.982413]\n",
      "epoch:2 step:2466 [D loss: 0.484130, acc: 78.91%] [G loss: 3.167352]\n",
      "epoch:2 step:2467 [D loss: 0.632824, acc: 64.06%] [G loss: 2.835362]\n",
      "epoch:2 step:2468 [D loss: 0.614288, acc: 66.41%] [G loss: 2.703475]\n",
      "epoch:2 step:2469 [D loss: 0.593693, acc: 69.53%] [G loss: 2.631635]\n",
      "epoch:2 step:2470 [D loss: 0.584005, acc: 71.09%] [G loss: 3.224704]\n",
      "epoch:2 step:2471 [D loss: 0.569842, acc: 69.53%] [G loss: 3.195858]\n",
      "epoch:2 step:2472 [D loss: 0.576481, acc: 71.09%] [G loss: 2.805424]\n",
      "epoch:2 step:2473 [D loss: 0.528296, acc: 75.78%] [G loss: 2.707865]\n",
      "epoch:2 step:2474 [D loss: 0.609212, acc: 70.31%] [G loss: 2.914493]\n",
      "epoch:2 step:2475 [D loss: 0.621633, acc: 63.28%] [G loss: 2.821104]\n",
      "epoch:2 step:2476 [D loss: 0.544613, acc: 74.22%] [G loss: 2.905155]\n",
      "epoch:2 step:2477 [D loss: 0.505018, acc: 75.00%] [G loss: 3.304999]\n",
      "epoch:2 step:2478 [D loss: 0.549808, acc: 74.22%] [G loss: 2.962723]\n",
      "epoch:2 step:2479 [D loss: 0.515891, acc: 72.66%] [G loss: 3.068562]\n",
      "epoch:2 step:2480 [D loss: 0.565469, acc: 67.19%] [G loss: 2.948322]\n",
      "epoch:2 step:2481 [D loss: 0.517387, acc: 72.66%] [G loss: 3.198910]\n",
      "epoch:2 step:2482 [D loss: 0.538042, acc: 75.00%] [G loss: 3.252939]\n",
      "epoch:2 step:2483 [D loss: 0.494732, acc: 77.34%] [G loss: 3.091990]\n",
      "epoch:2 step:2484 [D loss: 0.520974, acc: 76.56%] [G loss: 3.251595]\n",
      "epoch:2 step:2485 [D loss: 0.507854, acc: 72.66%] [G loss: 3.612299]\n",
      "epoch:2 step:2486 [D loss: 0.538287, acc: 70.31%] [G loss: 3.174999]\n",
      "epoch:2 step:2487 [D loss: 0.471803, acc: 82.03%] [G loss: 3.206841]\n",
      "epoch:2 step:2488 [D loss: 0.487574, acc: 79.69%] [G loss: 3.111716]\n",
      "epoch:2 step:2489 [D loss: 0.707418, acc: 64.84%] [G loss: 2.762919]\n",
      "epoch:2 step:2490 [D loss: 0.537005, acc: 73.44%] [G loss: 3.072063]\n",
      "epoch:2 step:2491 [D loss: 0.653666, acc: 67.97%] [G loss: 2.951653]\n",
      "epoch:2 step:2492 [D loss: 0.526227, acc: 75.00%] [G loss: 3.248703]\n",
      "epoch:2 step:2493 [D loss: 0.578443, acc: 68.75%] [G loss: 2.902772]\n",
      "epoch:2 step:2494 [D loss: 0.519906, acc: 72.66%] [G loss: 3.111789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2495 [D loss: 0.595916, acc: 71.09%] [G loss: 2.888467]\n",
      "epoch:2 step:2496 [D loss: 0.647593, acc: 65.62%] [G loss: 2.566045]\n",
      "epoch:2 step:2497 [D loss: 0.553987, acc: 70.31%] [G loss: 2.755040]\n",
      "epoch:2 step:2498 [D loss: 0.568492, acc: 67.97%] [G loss: 2.492802]\n",
      "epoch:2 step:2499 [D loss: 0.594995, acc: 65.62%] [G loss: 3.026451]\n",
      "epoch:2 step:2500 [D loss: 0.475538, acc: 78.91%] [G loss: 2.876140]\n",
      "epoch:2 step:2501 [D loss: 0.531795, acc: 67.97%] [G loss: 3.256371]\n",
      "epoch:2 step:2502 [D loss: 0.446608, acc: 78.91%] [G loss: 3.139063]\n",
      "epoch:2 step:2503 [D loss: 0.451372, acc: 80.47%] [G loss: 2.951614]\n",
      "epoch:2 step:2504 [D loss: 0.528027, acc: 70.31%] [G loss: 3.183714]\n",
      "epoch:2 step:2505 [D loss: 0.420387, acc: 81.25%] [G loss: 3.415038]\n",
      "epoch:2 step:2506 [D loss: 0.510455, acc: 74.22%] [G loss: 3.796660]\n",
      "epoch:2 step:2507 [D loss: 0.480640, acc: 78.12%] [G loss: 3.584196]\n",
      "epoch:2 step:2508 [D loss: 0.529189, acc: 73.44%] [G loss: 3.304065]\n",
      "epoch:2 step:2509 [D loss: 0.543460, acc: 74.22%] [G loss: 3.096725]\n",
      "epoch:2 step:2510 [D loss: 0.647109, acc: 64.06%] [G loss: 2.892243]\n",
      "epoch:2 step:2511 [D loss: 0.538063, acc: 74.22%] [G loss: 2.943010]\n",
      "epoch:2 step:2512 [D loss: 0.602566, acc: 71.09%] [G loss: 3.129131]\n",
      "epoch:2 step:2513 [D loss: 0.488002, acc: 79.69%] [G loss: 3.510059]\n",
      "epoch:2 step:2514 [D loss: 0.386643, acc: 84.38%] [G loss: 3.277743]\n",
      "epoch:2 step:2515 [D loss: 0.450219, acc: 80.47%] [G loss: 3.667518]\n",
      "epoch:2 step:2516 [D loss: 0.603987, acc: 72.66%] [G loss: 3.444176]\n",
      "epoch:2 step:2517 [D loss: 0.497845, acc: 75.00%] [G loss: 3.085623]\n",
      "epoch:2 step:2518 [D loss: 0.577722, acc: 70.31%] [G loss: 3.184163]\n",
      "epoch:2 step:2519 [D loss: 0.568390, acc: 67.97%] [G loss: 3.508409]\n",
      "epoch:2 step:2520 [D loss: 0.620926, acc: 64.06%] [G loss: 3.283045]\n",
      "epoch:2 step:2521 [D loss: 0.594324, acc: 72.66%] [G loss: 3.217274]\n",
      "epoch:2 step:2522 [D loss: 0.520619, acc: 75.78%] [G loss: 3.109714]\n",
      "epoch:2 step:2523 [D loss: 0.616100, acc: 71.88%] [G loss: 2.969781]\n",
      "epoch:2 step:2524 [D loss: 0.552888, acc: 68.75%] [G loss: 3.102957]\n",
      "epoch:2 step:2525 [D loss: 0.517205, acc: 75.78%] [G loss: 3.112839]\n",
      "epoch:2 step:2526 [D loss: 0.559549, acc: 72.66%] [G loss: 2.952379]\n",
      "epoch:2 step:2527 [D loss: 0.621182, acc: 63.28%] [G loss: 2.908153]\n",
      "epoch:2 step:2528 [D loss: 0.501176, acc: 75.00%] [G loss: 3.147447]\n",
      "epoch:2 step:2529 [D loss: 0.661626, acc: 67.19%] [G loss: 2.935458]\n",
      "epoch:2 step:2530 [D loss: 0.530874, acc: 72.66%] [G loss: 3.020618]\n",
      "epoch:2 step:2531 [D loss: 0.575307, acc: 66.41%] [G loss: 2.797529]\n",
      "epoch:2 step:2532 [D loss: 0.541055, acc: 67.97%] [G loss: 2.727922]\n",
      "epoch:2 step:2533 [D loss: 0.557486, acc: 74.22%] [G loss: 3.080440]\n",
      "epoch:2 step:2534 [D loss: 0.532545, acc: 75.00%] [G loss: 3.339229]\n",
      "epoch:2 step:2535 [D loss: 0.481425, acc: 72.66%] [G loss: 3.468443]\n",
      "epoch:2 step:2536 [D loss: 0.555447, acc: 72.66%] [G loss: 3.247490]\n",
      "epoch:2 step:2537 [D loss: 0.608690, acc: 69.53%] [G loss: 3.080363]\n",
      "epoch:2 step:2538 [D loss: 0.575588, acc: 73.44%] [G loss: 3.176799]\n",
      "epoch:2 step:2539 [D loss: 0.484614, acc: 77.34%] [G loss: 3.117612]\n",
      "epoch:2 step:2540 [D loss: 0.628665, acc: 64.06%] [G loss: 3.002486]\n",
      "epoch:2 step:2541 [D loss: 0.617997, acc: 66.41%] [G loss: 2.750858]\n",
      "epoch:2 step:2542 [D loss: 0.572069, acc: 69.53%] [G loss: 2.718245]\n",
      "epoch:2 step:2543 [D loss: 0.450641, acc: 76.56%] [G loss: 3.017772]\n",
      "epoch:2 step:2544 [D loss: 0.511804, acc: 73.44%] [G loss: 3.100352]\n",
      "epoch:2 step:2545 [D loss: 0.577461, acc: 71.09%] [G loss: 3.275466]\n",
      "epoch:2 step:2546 [D loss: 0.591390, acc: 71.88%] [G loss: 3.070911]\n",
      "epoch:2 step:2547 [D loss: 0.533832, acc: 76.56%] [G loss: 3.117925]\n",
      "epoch:2 step:2548 [D loss: 0.643836, acc: 65.62%] [G loss: 2.924408]\n",
      "epoch:2 step:2549 [D loss: 0.553562, acc: 66.41%] [G loss: 3.151809]\n",
      "epoch:2 step:2550 [D loss: 0.519361, acc: 74.22%] [G loss: 2.957120]\n",
      "epoch:2 step:2551 [D loss: 0.553299, acc: 71.88%] [G loss: 3.022948]\n",
      "epoch:2 step:2552 [D loss: 0.521679, acc: 74.22%] [G loss: 3.310791]\n",
      "epoch:2 step:2553 [D loss: 0.473933, acc: 76.56%] [G loss: 3.163471]\n",
      "epoch:2 step:2554 [D loss: 0.472249, acc: 76.56%] [G loss: 3.366537]\n",
      "epoch:2 step:2555 [D loss: 0.507473, acc: 76.56%] [G loss: 3.120542]\n",
      "epoch:2 step:2556 [D loss: 0.453596, acc: 76.56%] [G loss: 3.020529]\n",
      "epoch:2 step:2557 [D loss: 0.534491, acc: 71.09%] [G loss: 2.982836]\n",
      "epoch:2 step:2558 [D loss: 0.602906, acc: 67.97%] [G loss: 3.102467]\n",
      "epoch:2 step:2559 [D loss: 0.572041, acc: 72.66%] [G loss: 3.100190]\n",
      "epoch:2 step:2560 [D loss: 0.505802, acc: 75.78%] [G loss: 2.838731]\n",
      "epoch:2 step:2561 [D loss: 0.646949, acc: 68.75%] [G loss: 2.985498]\n",
      "epoch:2 step:2562 [D loss: 0.680566, acc: 61.72%] [G loss: 2.869613]\n",
      "epoch:2 step:2563 [D loss: 0.519836, acc: 75.78%] [G loss: 3.229111]\n",
      "epoch:2 step:2564 [D loss: 0.557533, acc: 72.66%] [G loss: 2.985437]\n",
      "epoch:2 step:2565 [D loss: 0.482301, acc: 80.47%] [G loss: 3.046086]\n",
      "epoch:2 step:2566 [D loss: 0.610219, acc: 67.97%] [G loss: 3.144872]\n",
      "epoch:2 step:2567 [D loss: 0.606671, acc: 71.09%] [G loss: 3.021394]\n",
      "epoch:2 step:2568 [D loss: 0.492809, acc: 75.00%] [G loss: 3.212177]\n",
      "epoch:2 step:2569 [D loss: 0.547266, acc: 73.44%] [G loss: 3.091240]\n",
      "epoch:2 step:2570 [D loss: 0.563934, acc: 71.09%] [G loss: 2.790956]\n",
      "epoch:2 step:2571 [D loss: 0.538240, acc: 72.66%] [G loss: 3.148593]\n",
      "epoch:2 step:2572 [D loss: 0.542716, acc: 75.00%] [G loss: 3.141813]\n",
      "epoch:2 step:2573 [D loss: 0.446374, acc: 77.34%] [G loss: 3.258575]\n",
      "epoch:2 step:2574 [D loss: 0.615696, acc: 68.75%] [G loss: 3.414666]\n",
      "epoch:2 step:2575 [D loss: 0.573382, acc: 67.97%] [G loss: 2.962586]\n",
      "epoch:2 step:2576 [D loss: 0.540020, acc: 71.88%] [G loss: 2.887156]\n",
      "epoch:2 step:2577 [D loss: 0.676987, acc: 61.72%] [G loss: 2.921957]\n",
      "epoch:2 step:2578 [D loss: 0.586676, acc: 68.75%] [G loss: 2.790842]\n",
      "epoch:2 step:2579 [D loss: 0.557176, acc: 67.97%] [G loss: 3.104713]\n",
      "epoch:2 step:2580 [D loss: 0.533873, acc: 76.56%] [G loss: 3.285444]\n",
      "epoch:2 step:2581 [D loss: 0.452554, acc: 79.69%] [G loss: 4.010708]\n",
      "epoch:2 step:2582 [D loss: 0.495724, acc: 75.78%] [G loss: 3.380137]\n",
      "epoch:2 step:2583 [D loss: 0.508953, acc: 73.44%] [G loss: 3.493446]\n",
      "epoch:2 step:2584 [D loss: 0.559746, acc: 67.97%] [G loss: 2.676230]\n",
      "epoch:2 step:2585 [D loss: 0.598659, acc: 69.53%] [G loss: 2.695890]\n",
      "epoch:2 step:2586 [D loss: 0.557442, acc: 65.62%] [G loss: 2.955312]\n",
      "epoch:2 step:2587 [D loss: 0.530365, acc: 75.78%] [G loss: 3.250145]\n",
      "epoch:2 step:2588 [D loss: 0.522647, acc: 75.00%] [G loss: 3.517287]\n",
      "epoch:2 step:2589 [D loss: 0.528548, acc: 75.78%] [G loss: 2.988546]\n",
      "epoch:2 step:2590 [D loss: 0.524903, acc: 71.88%] [G loss: 2.861465]\n",
      "epoch:2 step:2591 [D loss: 0.560223, acc: 70.31%] [G loss: 2.838742]\n",
      "epoch:2 step:2592 [D loss: 0.585399, acc: 64.84%] [G loss: 3.060984]\n",
      "epoch:2 step:2593 [D loss: 0.573760, acc: 67.19%] [G loss: 2.935079]\n",
      "epoch:2 step:2594 [D loss: 0.666652, acc: 64.84%] [G loss: 2.804497]\n",
      "epoch:2 step:2595 [D loss: 0.586339, acc: 69.53%] [G loss: 3.292927]\n",
      "epoch:2 step:2596 [D loss: 0.569356, acc: 67.97%] [G loss: 3.009097]\n",
      "epoch:2 step:2597 [D loss: 0.613793, acc: 66.41%] [G loss: 2.821453]\n",
      "epoch:2 step:2598 [D loss: 0.621378, acc: 66.41%] [G loss: 2.933616]\n",
      "epoch:2 step:2599 [D loss: 0.618654, acc: 63.28%] [G loss: 3.267266]\n",
      "epoch:2 step:2600 [D loss: 0.604336, acc: 72.66%] [G loss: 2.858425]\n",
      "##############\n",
      "[3.25717412 2.01059616 7.05815185 5.90647408 4.63700974 6.2421647\n",
      " 5.52018915 5.48993913 5.54532879 4.1709237 ]\n",
      "##########\n",
      "epoch:2 step:2601 [D loss: 0.551452, acc: 74.22%] [G loss: 3.050177]\n",
      "epoch:2 step:2602 [D loss: 0.638531, acc: 67.19%] [G loss: 3.071267]\n",
      "epoch:2 step:2603 [D loss: 0.535248, acc: 80.47%] [G loss: 3.090491]\n",
      "epoch:2 step:2604 [D loss: 0.525232, acc: 73.44%] [G loss: 2.940348]\n",
      "epoch:2 step:2605 [D loss: 0.556161, acc: 70.31%] [G loss: 2.947153]\n",
      "epoch:2 step:2606 [D loss: 0.519365, acc: 75.78%] [G loss: 3.395611]\n",
      "epoch:2 step:2607 [D loss: 0.511033, acc: 74.22%] [G loss: 3.303711]\n",
      "epoch:2 step:2608 [D loss: 0.545412, acc: 70.31%] [G loss: 3.380550]\n",
      "epoch:2 step:2609 [D loss: 0.622631, acc: 66.41%] [G loss: 2.697049]\n",
      "epoch:2 step:2610 [D loss: 0.493179, acc: 76.56%] [G loss: 3.133596]\n",
      "epoch:2 step:2611 [D loss: 0.514963, acc: 74.22%] [G loss: 3.321543]\n",
      "epoch:2 step:2612 [D loss: 0.517414, acc: 73.44%] [G loss: 3.023115]\n",
      "epoch:2 step:2613 [D loss: 0.586721, acc: 71.09%] [G loss: 3.060990]\n",
      "epoch:2 step:2614 [D loss: 0.567474, acc: 73.44%] [G loss: 2.929879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2615 [D loss: 0.658666, acc: 66.41%] [G loss: 2.973718]\n",
      "epoch:2 step:2616 [D loss: 0.590456, acc: 67.97%] [G loss: 2.862670]\n",
      "epoch:2 step:2617 [D loss: 0.483146, acc: 74.22%] [G loss: 2.999720]\n",
      "epoch:2 step:2618 [D loss: 0.703514, acc: 61.72%] [G loss: 2.683597]\n",
      "epoch:2 step:2619 [D loss: 0.533019, acc: 76.56%] [G loss: 3.423700]\n",
      "epoch:2 step:2620 [D loss: 0.584079, acc: 72.66%] [G loss: 3.291907]\n",
      "epoch:2 step:2621 [D loss: 0.446423, acc: 81.25%] [G loss: 3.456507]\n",
      "epoch:2 step:2622 [D loss: 0.639505, acc: 64.06%] [G loss: 2.902913]\n",
      "epoch:2 step:2623 [D loss: 0.582603, acc: 74.22%] [G loss: 2.876362]\n",
      "epoch:2 step:2624 [D loss: 0.532633, acc: 70.31%] [G loss: 3.590163]\n",
      "epoch:2 step:2625 [D loss: 0.570689, acc: 70.31%] [G loss: 2.748862]\n",
      "epoch:2 step:2626 [D loss: 0.532528, acc: 70.31%] [G loss: 3.143939]\n",
      "epoch:2 step:2627 [D loss: 0.485570, acc: 79.69%] [G loss: 3.087707]\n",
      "epoch:2 step:2628 [D loss: 0.564109, acc: 75.00%] [G loss: 3.279240]\n",
      "epoch:2 step:2629 [D loss: 0.448159, acc: 82.03%] [G loss: 3.222154]\n",
      "epoch:2 step:2630 [D loss: 0.549803, acc: 75.00%] [G loss: 3.012773]\n",
      "epoch:2 step:2631 [D loss: 0.522503, acc: 75.00%] [G loss: 3.098420]\n",
      "epoch:2 step:2632 [D loss: 0.527994, acc: 71.88%] [G loss: 3.084292]\n",
      "epoch:2 step:2633 [D loss: 0.494045, acc: 78.12%] [G loss: 2.870573]\n",
      "epoch:2 step:2634 [D loss: 0.571891, acc: 71.88%] [G loss: 3.252088]\n",
      "epoch:2 step:2635 [D loss: 0.512952, acc: 78.91%] [G loss: 3.106910]\n",
      "epoch:2 step:2636 [D loss: 0.520657, acc: 69.53%] [G loss: 2.926340]\n",
      "epoch:2 step:2637 [D loss: 0.641194, acc: 62.50%] [G loss: 3.316835]\n",
      "epoch:2 step:2638 [D loss: 0.551693, acc: 67.97%] [G loss: 3.544001]\n",
      "epoch:2 step:2639 [D loss: 0.685144, acc: 59.38%] [G loss: 3.359852]\n",
      "epoch:2 step:2640 [D loss: 0.682217, acc: 66.41%] [G loss: 2.692245]\n",
      "epoch:2 step:2641 [D loss: 0.612511, acc: 63.28%] [G loss: 3.104957]\n",
      "epoch:2 step:2642 [D loss: 0.559747, acc: 71.09%] [G loss: 2.842408]\n",
      "epoch:2 step:2643 [D loss: 0.506905, acc: 74.22%] [G loss: 2.931834]\n",
      "epoch:2 step:2644 [D loss: 0.613963, acc: 61.72%] [G loss: 3.045889]\n",
      "epoch:2 step:2645 [D loss: 0.627361, acc: 66.41%] [G loss: 2.883905]\n",
      "epoch:2 step:2646 [D loss: 0.592211, acc: 64.84%] [G loss: 2.728755]\n",
      "epoch:2 step:2647 [D loss: 0.571957, acc: 70.31%] [G loss: 3.104438]\n",
      "epoch:2 step:2648 [D loss: 0.555268, acc: 68.75%] [G loss: 2.896884]\n",
      "epoch:2 step:2649 [D loss: 0.579168, acc: 67.19%] [G loss: 2.900334]\n",
      "epoch:2 step:2650 [D loss: 0.521623, acc: 71.88%] [G loss: 2.917358]\n",
      "epoch:2 step:2651 [D loss: 0.550218, acc: 73.44%] [G loss: 2.623926]\n",
      "epoch:2 step:2652 [D loss: 0.525606, acc: 75.00%] [G loss: 3.082628]\n",
      "epoch:2 step:2653 [D loss: 0.502086, acc: 81.25%] [G loss: 3.193267]\n",
      "epoch:2 step:2654 [D loss: 0.555295, acc: 70.31%] [G loss: 2.973363]\n",
      "epoch:2 step:2655 [D loss: 0.483702, acc: 76.56%] [G loss: 2.826192]\n",
      "epoch:2 step:2656 [D loss: 0.567550, acc: 68.75%] [G loss: 3.064687]\n",
      "epoch:2 step:2657 [D loss: 0.526411, acc: 72.66%] [G loss: 3.089471]\n",
      "epoch:2 step:2658 [D loss: 0.579631, acc: 72.66%] [G loss: 3.121228]\n",
      "epoch:2 step:2659 [D loss: 0.521832, acc: 74.22%] [G loss: 2.870288]\n",
      "epoch:2 step:2660 [D loss: 0.562055, acc: 70.31%] [G loss: 2.750690]\n",
      "epoch:2 step:2661 [D loss: 0.489547, acc: 76.56%] [G loss: 3.107923]\n",
      "epoch:2 step:2662 [D loss: 0.621866, acc: 61.72%] [G loss: 2.658364]\n",
      "epoch:2 step:2663 [D loss: 0.604594, acc: 71.09%] [G loss: 2.872063]\n",
      "epoch:2 step:2664 [D loss: 0.584250, acc: 71.09%] [G loss: 3.384354]\n",
      "epoch:2 step:2665 [D loss: 0.526834, acc: 67.97%] [G loss: 3.105127]\n",
      "epoch:2 step:2666 [D loss: 0.449004, acc: 80.47%] [G loss: 3.334677]\n",
      "epoch:2 step:2667 [D loss: 0.643260, acc: 66.41%] [G loss: 2.569488]\n",
      "epoch:2 step:2668 [D loss: 0.595492, acc: 68.75%] [G loss: 2.744461]\n",
      "epoch:2 step:2669 [D loss: 0.559442, acc: 70.31%] [G loss: 3.736112]\n",
      "epoch:2 step:2670 [D loss: 0.543174, acc: 72.66%] [G loss: 3.058201]\n",
      "epoch:2 step:2671 [D loss: 0.640171, acc: 70.31%] [G loss: 2.779415]\n",
      "epoch:2 step:2672 [D loss: 0.561024, acc: 71.88%] [G loss: 2.820233]\n",
      "epoch:2 step:2673 [D loss: 0.588955, acc: 71.88%] [G loss: 2.904019]\n",
      "epoch:2 step:2674 [D loss: 0.589931, acc: 67.97%] [G loss: 2.919636]\n",
      "epoch:2 step:2675 [D loss: 0.582326, acc: 67.97%] [G loss: 3.008681]\n",
      "epoch:2 step:2676 [D loss: 0.636162, acc: 65.62%] [G loss: 2.755620]\n",
      "epoch:2 step:2677 [D loss: 0.505855, acc: 78.12%] [G loss: 3.004133]\n",
      "epoch:2 step:2678 [D loss: 0.538677, acc: 71.09%] [G loss: 3.291018]\n",
      "epoch:2 step:2679 [D loss: 0.491848, acc: 78.12%] [G loss: 3.516954]\n",
      "epoch:2 step:2680 [D loss: 0.560527, acc: 71.09%] [G loss: 3.728889]\n",
      "epoch:2 step:2681 [D loss: 0.536802, acc: 73.44%] [G loss: 3.363968]\n",
      "epoch:2 step:2682 [D loss: 0.574499, acc: 71.88%] [G loss: 3.093419]\n",
      "epoch:2 step:2683 [D loss: 0.622363, acc: 67.19%] [G loss: 2.971314]\n",
      "epoch:2 step:2684 [D loss: 0.543238, acc: 73.44%] [G loss: 2.584733]\n",
      "epoch:2 step:2685 [D loss: 0.561394, acc: 71.09%] [G loss: 2.679620]\n",
      "epoch:2 step:2686 [D loss: 0.575942, acc: 68.75%] [G loss: 2.981820]\n",
      "epoch:2 step:2687 [D loss: 0.623196, acc: 71.88%] [G loss: 2.673342]\n",
      "epoch:2 step:2688 [D loss: 0.564541, acc: 70.31%] [G loss: 2.886973]\n",
      "epoch:2 step:2689 [D loss: 0.525827, acc: 71.09%] [G loss: 2.908826]\n",
      "epoch:2 step:2690 [D loss: 0.648666, acc: 65.62%] [G loss: 2.911777]\n",
      "epoch:2 step:2691 [D loss: 0.531012, acc: 70.31%] [G loss: 2.769107]\n",
      "epoch:2 step:2692 [D loss: 0.499833, acc: 75.00%] [G loss: 3.117716]\n",
      "epoch:2 step:2693 [D loss: 0.592816, acc: 64.84%] [G loss: 2.819311]\n",
      "epoch:2 step:2694 [D loss: 0.577320, acc: 65.62%] [G loss: 3.221627]\n",
      "epoch:2 step:2695 [D loss: 0.634356, acc: 69.53%] [G loss: 3.125573]\n",
      "epoch:2 step:2696 [D loss: 0.473252, acc: 75.78%] [G loss: 3.139195]\n",
      "epoch:2 step:2697 [D loss: 0.457680, acc: 75.00%] [G loss: 3.071970]\n",
      "epoch:2 step:2698 [D loss: 0.580230, acc: 71.88%] [G loss: 2.956298]\n",
      "epoch:2 step:2699 [D loss: 0.497826, acc: 75.00%] [G loss: 3.240272]\n",
      "epoch:2 step:2700 [D loss: 0.545717, acc: 66.41%] [G loss: 2.990671]\n",
      "epoch:2 step:2701 [D loss: 0.669227, acc: 66.41%] [G loss: 2.599461]\n",
      "epoch:2 step:2702 [D loss: 0.574025, acc: 68.75%] [G loss: 2.774268]\n",
      "epoch:2 step:2703 [D loss: 0.585933, acc: 70.31%] [G loss: 2.887483]\n",
      "epoch:2 step:2704 [D loss: 0.544365, acc: 67.97%] [G loss: 3.037530]\n",
      "epoch:2 step:2705 [D loss: 0.564526, acc: 72.66%] [G loss: 3.176745]\n",
      "epoch:2 step:2706 [D loss: 0.547045, acc: 71.88%] [G loss: 2.920143]\n",
      "epoch:2 step:2707 [D loss: 0.572159, acc: 67.19%] [G loss: 3.208959]\n",
      "epoch:2 step:2708 [D loss: 0.478026, acc: 76.56%] [G loss: 3.320308]\n",
      "epoch:2 step:2709 [D loss: 0.589060, acc: 68.75%] [G loss: 3.209877]\n",
      "epoch:2 step:2710 [D loss: 0.623403, acc: 67.19%] [G loss: 2.884094]\n",
      "epoch:2 step:2711 [D loss: 0.602190, acc: 74.22%] [G loss: 2.920559]\n",
      "epoch:2 step:2712 [D loss: 0.493173, acc: 75.00%] [G loss: 3.158916]\n",
      "epoch:2 step:2713 [D loss: 0.589544, acc: 67.19%] [G loss: 2.901586]\n",
      "epoch:2 step:2714 [D loss: 0.565284, acc: 67.19%] [G loss: 2.568665]\n",
      "epoch:2 step:2715 [D loss: 0.534620, acc: 73.44%] [G loss: 3.113738]\n",
      "epoch:2 step:2716 [D loss: 0.504332, acc: 75.00%] [G loss: 2.999575]\n",
      "epoch:2 step:2717 [D loss: 0.483302, acc: 78.91%] [G loss: 3.250229]\n",
      "epoch:2 step:2718 [D loss: 0.665801, acc: 63.28%] [G loss: 2.869861]\n",
      "epoch:2 step:2719 [D loss: 0.539749, acc: 75.78%] [G loss: 3.120493]\n",
      "epoch:2 step:2720 [D loss: 0.524952, acc: 69.53%] [G loss: 2.945365]\n",
      "epoch:2 step:2721 [D loss: 0.552364, acc: 72.66%] [G loss: 3.310814]\n",
      "epoch:2 step:2722 [D loss: 0.615211, acc: 69.53%] [G loss: 3.032768]\n",
      "epoch:2 step:2723 [D loss: 0.589781, acc: 66.41%] [G loss: 2.593879]\n",
      "epoch:2 step:2724 [D loss: 0.565407, acc: 71.09%] [G loss: 2.895627]\n",
      "epoch:2 step:2725 [D loss: 0.588360, acc: 73.44%] [G loss: 2.820271]\n",
      "epoch:2 step:2726 [D loss: 0.504970, acc: 75.78%] [G loss: 3.188290]\n",
      "epoch:2 step:2727 [D loss: 0.450636, acc: 83.59%] [G loss: 3.434382]\n",
      "epoch:2 step:2728 [D loss: 0.485515, acc: 78.12%] [G loss: 3.423751]\n",
      "epoch:2 step:2729 [D loss: 0.577630, acc: 70.31%] [G loss: 3.087250]\n",
      "epoch:2 step:2730 [D loss: 0.578489, acc: 68.75%] [G loss: 3.043173]\n",
      "epoch:2 step:2731 [D loss: 0.492300, acc: 77.34%] [G loss: 3.321517]\n",
      "epoch:2 step:2732 [D loss: 0.640073, acc: 64.06%] [G loss: 3.166404]\n",
      "epoch:2 step:2733 [D loss: 0.663446, acc: 70.31%] [G loss: 2.978076]\n",
      "epoch:2 step:2734 [D loss: 0.540033, acc: 73.44%] [G loss: 3.342227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2735 [D loss: 0.635969, acc: 68.75%] [G loss: 2.650201]\n",
      "epoch:2 step:2736 [D loss: 0.540228, acc: 75.00%] [G loss: 3.060189]\n",
      "epoch:2 step:2737 [D loss: 0.556107, acc: 72.66%] [G loss: 2.728908]\n",
      "epoch:2 step:2738 [D loss: 0.615956, acc: 72.66%] [G loss: 2.660530]\n",
      "epoch:2 step:2739 [D loss: 0.582703, acc: 73.44%] [G loss: 2.883961]\n",
      "epoch:2 step:2740 [D loss: 0.528385, acc: 77.34%] [G loss: 3.280368]\n",
      "epoch:2 step:2741 [D loss: 0.561344, acc: 71.88%] [G loss: 2.814486]\n",
      "epoch:2 step:2742 [D loss: 0.580249, acc: 67.19%] [G loss: 2.845646]\n",
      "epoch:2 step:2743 [D loss: 0.502103, acc: 78.12%] [G loss: 3.290698]\n",
      "epoch:2 step:2744 [D loss: 0.484962, acc: 76.56%] [G loss: 3.242358]\n",
      "epoch:2 step:2745 [D loss: 0.605217, acc: 66.41%] [G loss: 3.188038]\n",
      "epoch:2 step:2746 [D loss: 0.427041, acc: 85.16%] [G loss: 3.259410]\n",
      "epoch:2 step:2747 [D loss: 0.574993, acc: 67.19%] [G loss: 3.109682]\n",
      "epoch:2 step:2748 [D loss: 0.604951, acc: 67.97%] [G loss: 3.160244]\n",
      "epoch:2 step:2749 [D loss: 0.419065, acc: 84.38%] [G loss: 3.363600]\n",
      "epoch:2 step:2750 [D loss: 0.592503, acc: 71.09%] [G loss: 2.895877]\n",
      "epoch:2 step:2751 [D loss: 0.557241, acc: 72.66%] [G loss: 3.133612]\n",
      "epoch:2 step:2752 [D loss: 0.598276, acc: 67.97%] [G loss: 2.895530]\n",
      "epoch:2 step:2753 [D loss: 0.565299, acc: 70.31%] [G loss: 2.852023]\n",
      "epoch:2 step:2754 [D loss: 0.637398, acc: 59.38%] [G loss: 2.598000]\n",
      "epoch:2 step:2755 [D loss: 0.550668, acc: 71.88%] [G loss: 2.912620]\n",
      "epoch:2 step:2756 [D loss: 0.534205, acc: 72.66%] [G loss: 3.012124]\n",
      "epoch:2 step:2757 [D loss: 0.590061, acc: 67.19%] [G loss: 2.904985]\n",
      "epoch:2 step:2758 [D loss: 0.539752, acc: 76.56%] [G loss: 3.370037]\n",
      "epoch:2 step:2759 [D loss: 0.531704, acc: 74.22%] [G loss: 3.443970]\n",
      "epoch:2 step:2760 [D loss: 0.520079, acc: 76.56%] [G loss: 3.201219]\n",
      "epoch:2 step:2761 [D loss: 0.449024, acc: 78.12%] [G loss: 3.272916]\n",
      "epoch:2 step:2762 [D loss: 0.491622, acc: 74.22%] [G loss: 3.404145]\n",
      "epoch:2 step:2763 [D loss: 0.489966, acc: 79.69%] [G loss: 3.340043]\n",
      "epoch:2 step:2764 [D loss: 0.480201, acc: 78.12%] [G loss: 3.767918]\n",
      "epoch:2 step:2765 [D loss: 0.599999, acc: 64.06%] [G loss: 2.992624]\n",
      "epoch:2 step:2766 [D loss: 0.657487, acc: 64.84%] [G loss: 2.749048]\n",
      "epoch:2 step:2767 [D loss: 0.530090, acc: 74.22%] [G loss: 3.061824]\n",
      "epoch:2 step:2768 [D loss: 0.586174, acc: 73.44%] [G loss: 2.946676]\n",
      "epoch:2 step:2769 [D loss: 0.479006, acc: 80.47%] [G loss: 3.263172]\n",
      "epoch:2 step:2770 [D loss: 0.624565, acc: 66.41%] [G loss: 2.928059]\n",
      "epoch:2 step:2771 [D loss: 0.509097, acc: 77.34%] [G loss: 3.265407]\n",
      "epoch:2 step:2772 [D loss: 0.510676, acc: 73.44%] [G loss: 3.479660]\n",
      "epoch:2 step:2773 [D loss: 0.546698, acc: 75.78%] [G loss: 3.044119]\n",
      "epoch:2 step:2774 [D loss: 0.614987, acc: 71.88%] [G loss: 3.186723]\n",
      "epoch:2 step:2775 [D loss: 0.566247, acc: 70.31%] [G loss: 3.576935]\n",
      "epoch:2 step:2776 [D loss: 0.553557, acc: 70.31%] [G loss: 2.819419]\n",
      "epoch:2 step:2777 [D loss: 0.577429, acc: 67.97%] [G loss: 2.907163]\n",
      "epoch:2 step:2778 [D loss: 0.572446, acc: 68.75%] [G loss: 3.332455]\n",
      "epoch:2 step:2779 [D loss: 0.546790, acc: 72.66%] [G loss: 3.031629]\n",
      "epoch:2 step:2780 [D loss: 0.481965, acc: 74.22%] [G loss: 3.497891]\n",
      "epoch:2 step:2781 [D loss: 0.589381, acc: 73.44%] [G loss: 2.787411]\n",
      "epoch:2 step:2782 [D loss: 0.563851, acc: 70.31%] [G loss: 2.915796]\n",
      "epoch:2 step:2783 [D loss: 0.486301, acc: 78.12%] [G loss: 3.339316]\n",
      "epoch:2 step:2784 [D loss: 0.452149, acc: 82.03%] [G loss: 3.608834]\n",
      "epoch:2 step:2785 [D loss: 0.508545, acc: 74.22%] [G loss: 3.305130]\n",
      "epoch:2 step:2786 [D loss: 0.507424, acc: 75.00%] [G loss: 3.402617]\n",
      "epoch:2 step:2787 [D loss: 0.592938, acc: 69.53%] [G loss: 2.757627]\n",
      "epoch:2 step:2788 [D loss: 0.569371, acc: 70.31%] [G loss: 3.225569]\n",
      "epoch:2 step:2789 [D loss: 0.692753, acc: 63.28%] [G loss: 2.719791]\n",
      "epoch:2 step:2790 [D loss: 0.544015, acc: 72.66%] [G loss: 3.139738]\n",
      "epoch:2 step:2791 [D loss: 0.570272, acc: 71.09%] [G loss: 2.912055]\n",
      "epoch:2 step:2792 [D loss: 0.572119, acc: 69.53%] [G loss: 3.047613]\n",
      "epoch:2 step:2793 [D loss: 0.543242, acc: 70.31%] [G loss: 3.127783]\n",
      "epoch:2 step:2794 [D loss: 0.580437, acc: 70.31%] [G loss: 3.379124]\n",
      "epoch:2 step:2795 [D loss: 0.399056, acc: 82.03%] [G loss: 3.750289]\n",
      "epoch:2 step:2796 [D loss: 0.586736, acc: 62.50%] [G loss: 3.591594]\n",
      "epoch:2 step:2797 [D loss: 0.448422, acc: 78.91%] [G loss: 3.639441]\n",
      "epoch:2 step:2798 [D loss: 0.446025, acc: 85.16%] [G loss: 3.312806]\n",
      "epoch:2 step:2799 [D loss: 0.526198, acc: 73.44%] [G loss: 3.807097]\n",
      "epoch:2 step:2800 [D loss: 0.462874, acc: 77.34%] [G loss: 3.328332]\n",
      "##############\n",
      "[3.12859361 2.03157484 7.0608139  5.7185821  4.24366928 6.43389188\n",
      " 5.5677131  5.61195034 5.59153518 4.1709356 ]\n",
      "##########\n",
      "epoch:2 step:2801 [D loss: 0.617021, acc: 67.97%] [G loss: 3.042845]\n",
      "epoch:2 step:2802 [D loss: 0.889007, acc: 53.12%] [G loss: 2.839293]\n",
      "epoch:2 step:2803 [D loss: 0.404380, acc: 80.47%] [G loss: 3.358218]\n",
      "epoch:2 step:2804 [D loss: 0.480983, acc: 82.03%] [G loss: 3.333769]\n",
      "epoch:2 step:2805 [D loss: 0.569145, acc: 72.66%] [G loss: 2.966977]\n",
      "epoch:2 step:2806 [D loss: 0.520348, acc: 73.44%] [G loss: 2.777290]\n",
      "epoch:2 step:2807 [D loss: 0.598084, acc: 70.31%] [G loss: 3.196131]\n",
      "epoch:2 step:2808 [D loss: 0.414282, acc: 83.59%] [G loss: 3.075169]\n",
      "epoch:2 step:2809 [D loss: 0.563162, acc: 73.44%] [G loss: 2.965425]\n",
      "epoch:2 step:2810 [D loss: 0.365304, acc: 83.59%] [G loss: 3.533294]\n",
      "epoch:2 step:2811 [D loss: 0.635132, acc: 68.75%] [G loss: 3.694457]\n",
      "epoch:3 step:2812 [D loss: 0.555446, acc: 72.66%] [G loss: 3.226692]\n",
      "epoch:3 step:2813 [D loss: 0.517439, acc: 75.78%] [G loss: 3.550077]\n",
      "epoch:3 step:2814 [D loss: 0.570312, acc: 69.53%] [G loss: 2.898251]\n",
      "epoch:3 step:2815 [D loss: 0.489317, acc: 73.44%] [G loss: 3.241378]\n",
      "epoch:3 step:2816 [D loss: 0.566186, acc: 71.09%] [G loss: 3.413764]\n",
      "epoch:3 step:2817 [D loss: 0.518620, acc: 71.88%] [G loss: 3.013582]\n",
      "epoch:3 step:2818 [D loss: 0.529274, acc: 76.56%] [G loss: 3.220813]\n",
      "epoch:3 step:2819 [D loss: 0.511774, acc: 71.88%] [G loss: 3.133727]\n",
      "epoch:3 step:2820 [D loss: 0.498053, acc: 79.69%] [G loss: 3.345656]\n",
      "epoch:3 step:2821 [D loss: 0.528202, acc: 72.66%] [G loss: 3.414509]\n",
      "epoch:3 step:2822 [D loss: 0.524499, acc: 76.56%] [G loss: 3.280337]\n",
      "epoch:3 step:2823 [D loss: 0.642570, acc: 67.19%] [G loss: 3.124533]\n",
      "epoch:3 step:2824 [D loss: 0.566653, acc: 75.78%] [G loss: 2.907577]\n",
      "epoch:3 step:2825 [D loss: 0.517436, acc: 75.78%] [G loss: 3.034534]\n",
      "epoch:3 step:2826 [D loss: 0.507591, acc: 78.91%] [G loss: 3.220932]\n",
      "epoch:3 step:2827 [D loss: 0.532523, acc: 75.00%] [G loss: 3.184662]\n",
      "epoch:3 step:2828 [D loss: 0.640642, acc: 63.28%] [G loss: 3.062571]\n",
      "epoch:3 step:2829 [D loss: 0.626691, acc: 66.41%] [G loss: 2.583744]\n",
      "epoch:3 step:2830 [D loss: 0.631501, acc: 64.06%] [G loss: 3.030971]\n",
      "epoch:3 step:2831 [D loss: 0.698914, acc: 57.81%] [G loss: 2.810580]\n",
      "epoch:3 step:2832 [D loss: 0.621856, acc: 63.28%] [G loss: 2.878521]\n",
      "epoch:3 step:2833 [D loss: 0.508760, acc: 78.12%] [G loss: 2.901927]\n",
      "epoch:3 step:2834 [D loss: 0.553643, acc: 74.22%] [G loss: 3.207433]\n",
      "epoch:3 step:2835 [D loss: 0.506621, acc: 75.00%] [G loss: 3.279850]\n",
      "epoch:3 step:2836 [D loss: 0.572988, acc: 71.09%] [G loss: 2.929437]\n",
      "epoch:3 step:2837 [D loss: 0.526413, acc: 72.66%] [G loss: 2.697412]\n",
      "epoch:3 step:2838 [D loss: 0.590125, acc: 67.19%] [G loss: 2.847404]\n",
      "epoch:3 step:2839 [D loss: 0.593302, acc: 70.31%] [G loss: 2.924271]\n",
      "epoch:3 step:2840 [D loss: 0.537121, acc: 77.34%] [G loss: 3.268721]\n",
      "epoch:3 step:2841 [D loss: 0.576932, acc: 64.84%] [G loss: 2.665603]\n",
      "epoch:3 step:2842 [D loss: 0.641140, acc: 61.72%] [G loss: 2.947702]\n",
      "epoch:3 step:2843 [D loss: 0.650614, acc: 65.62%] [G loss: 2.895130]\n",
      "epoch:3 step:2844 [D loss: 0.521337, acc: 74.22%] [G loss: 2.919019]\n",
      "epoch:3 step:2845 [D loss: 0.409470, acc: 82.03%] [G loss: 3.243703]\n",
      "epoch:3 step:2846 [D loss: 0.556387, acc: 69.53%] [G loss: 2.952950]\n",
      "epoch:3 step:2847 [D loss: 0.583037, acc: 74.22%] [G loss: 3.187764]\n",
      "epoch:3 step:2848 [D loss: 0.533185, acc: 75.00%] [G loss: 3.198014]\n",
      "epoch:3 step:2849 [D loss: 0.560361, acc: 71.88%] [G loss: 3.098824]\n",
      "epoch:3 step:2850 [D loss: 0.458410, acc: 79.69%] [G loss: 3.451688]\n",
      "epoch:3 step:2851 [D loss: 0.517368, acc: 75.00%] [G loss: 3.209056]\n",
      "epoch:3 step:2852 [D loss: 0.578671, acc: 70.31%] [G loss: 2.911928]\n",
      "epoch:3 step:2853 [D loss: 0.467226, acc: 76.56%] [G loss: 2.964135]\n",
      "epoch:3 step:2854 [D loss: 0.503539, acc: 73.44%] [G loss: 3.366680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2855 [D loss: 0.588712, acc: 67.19%] [G loss: 3.234606]\n",
      "epoch:3 step:2856 [D loss: 0.587377, acc: 71.09%] [G loss: 2.822502]\n",
      "epoch:3 step:2857 [D loss: 0.599332, acc: 67.97%] [G loss: 2.715954]\n",
      "epoch:3 step:2858 [D loss: 0.530696, acc: 76.56%] [G loss: 3.322652]\n",
      "epoch:3 step:2859 [D loss: 0.517594, acc: 75.78%] [G loss: 3.373617]\n",
      "epoch:3 step:2860 [D loss: 0.551677, acc: 75.00%] [G loss: 3.512340]\n",
      "epoch:3 step:2861 [D loss: 0.474901, acc: 74.22%] [G loss: 3.429052]\n",
      "epoch:3 step:2862 [D loss: 0.555277, acc: 72.66%] [G loss: 3.422387]\n",
      "epoch:3 step:2863 [D loss: 0.591062, acc: 71.09%] [G loss: 2.913152]\n",
      "epoch:3 step:2864 [D loss: 0.528190, acc: 75.78%] [G loss: 3.117842]\n",
      "epoch:3 step:2865 [D loss: 0.595318, acc: 64.06%] [G loss: 3.016302]\n",
      "epoch:3 step:2866 [D loss: 0.416350, acc: 82.81%] [G loss: 3.496290]\n",
      "epoch:3 step:2867 [D loss: 0.556270, acc: 72.66%] [G loss: 2.905948]\n",
      "epoch:3 step:2868 [D loss: 0.495041, acc: 75.00%] [G loss: 2.997443]\n",
      "epoch:3 step:2869 [D loss: 0.564116, acc: 72.66%] [G loss: 2.949496]\n",
      "epoch:3 step:2870 [D loss: 0.620685, acc: 67.19%] [G loss: 3.020648]\n",
      "epoch:3 step:2871 [D loss: 0.567709, acc: 68.75%] [G loss: 2.982552]\n",
      "epoch:3 step:2872 [D loss: 0.472989, acc: 78.91%] [G loss: 3.585439]\n",
      "epoch:3 step:2873 [D loss: 0.615642, acc: 64.84%] [G loss: 2.975495]\n",
      "epoch:3 step:2874 [D loss: 0.484545, acc: 79.69%] [G loss: 3.001732]\n",
      "epoch:3 step:2875 [D loss: 0.533164, acc: 75.78%] [G loss: 3.218635]\n",
      "epoch:3 step:2876 [D loss: 0.604978, acc: 71.88%] [G loss: 3.159133]\n",
      "epoch:3 step:2877 [D loss: 0.560534, acc: 69.53%] [G loss: 2.908516]\n",
      "epoch:3 step:2878 [D loss: 0.442364, acc: 78.91%] [G loss: 3.111911]\n",
      "epoch:3 step:2879 [D loss: 0.685479, acc: 60.94%] [G loss: 2.781571]\n",
      "epoch:3 step:2880 [D loss: 0.473517, acc: 75.00%] [G loss: 3.007005]\n",
      "epoch:3 step:2881 [D loss: 0.506953, acc: 78.12%] [G loss: 2.992929]\n",
      "epoch:3 step:2882 [D loss: 0.638936, acc: 64.06%] [G loss: 3.061658]\n",
      "epoch:3 step:2883 [D loss: 0.591346, acc: 64.06%] [G loss: 3.283602]\n",
      "epoch:3 step:2884 [D loss: 0.508720, acc: 68.75%] [G loss: 2.893183]\n",
      "epoch:3 step:2885 [D loss: 0.537013, acc: 71.88%] [G loss: 2.835025]\n",
      "epoch:3 step:2886 [D loss: 0.475321, acc: 79.69%] [G loss: 3.946407]\n",
      "epoch:3 step:2887 [D loss: 0.572609, acc: 68.75%] [G loss: 3.327330]\n",
      "epoch:3 step:2888 [D loss: 0.434886, acc: 79.69%] [G loss: 3.952458]\n",
      "epoch:3 step:2889 [D loss: 0.587973, acc: 70.31%] [G loss: 3.659712]\n",
      "epoch:3 step:2890 [D loss: 0.477422, acc: 78.12%] [G loss: 3.233896]\n",
      "epoch:3 step:2891 [D loss: 0.629335, acc: 67.19%] [G loss: 3.034356]\n",
      "epoch:3 step:2892 [D loss: 0.527323, acc: 78.12%] [G loss: 3.073779]\n",
      "epoch:3 step:2893 [D loss: 0.505827, acc: 77.34%] [G loss: 3.033766]\n",
      "epoch:3 step:2894 [D loss: 0.496651, acc: 76.56%] [G loss: 3.041095]\n",
      "epoch:3 step:2895 [D loss: 0.630489, acc: 64.84%] [G loss: 3.161331]\n",
      "epoch:3 step:2896 [D loss: 0.672267, acc: 73.44%] [G loss: 2.751777]\n",
      "epoch:3 step:2897 [D loss: 0.567277, acc: 73.44%] [G loss: 3.286210]\n",
      "epoch:3 step:2898 [D loss: 0.474552, acc: 80.47%] [G loss: 3.602993]\n",
      "epoch:3 step:2899 [D loss: 0.544787, acc: 75.00%] [G loss: 3.142600]\n",
      "epoch:3 step:2900 [D loss: 0.542982, acc: 75.00%] [G loss: 2.881611]\n",
      "epoch:3 step:2901 [D loss: 0.495349, acc: 73.44%] [G loss: 3.426342]\n",
      "epoch:3 step:2902 [D loss: 0.513663, acc: 74.22%] [G loss: 2.928219]\n",
      "epoch:3 step:2903 [D loss: 0.572505, acc: 69.53%] [G loss: 3.025952]\n",
      "epoch:3 step:2904 [D loss: 0.533116, acc: 71.88%] [G loss: 3.046466]\n",
      "epoch:3 step:2905 [D loss: 0.581507, acc: 71.88%] [G loss: 2.999705]\n",
      "epoch:3 step:2906 [D loss: 0.566602, acc: 70.31%] [G loss: 2.855589]\n",
      "epoch:3 step:2907 [D loss: 0.548693, acc: 72.66%] [G loss: 3.215318]\n",
      "epoch:3 step:2908 [D loss: 0.491130, acc: 76.56%] [G loss: 3.264719]\n",
      "epoch:3 step:2909 [D loss: 0.610216, acc: 71.09%] [G loss: 3.233129]\n",
      "epoch:3 step:2910 [D loss: 0.480421, acc: 76.56%] [G loss: 2.678731]\n",
      "epoch:3 step:2911 [D loss: 0.579518, acc: 67.19%] [G loss: 3.191102]\n",
      "epoch:3 step:2912 [D loss: 0.531409, acc: 75.78%] [G loss: 2.954915]\n",
      "epoch:3 step:2913 [D loss: 0.552881, acc: 71.09%] [G loss: 2.829610]\n",
      "epoch:3 step:2914 [D loss: 0.477018, acc: 75.78%] [G loss: 3.884575]\n",
      "epoch:3 step:2915 [D loss: 0.444758, acc: 84.38%] [G loss: 3.357315]\n",
      "epoch:3 step:2916 [D loss: 0.457547, acc: 79.69%] [G loss: 3.553010]\n",
      "epoch:3 step:2917 [D loss: 0.594091, acc: 67.19%] [G loss: 2.990497]\n",
      "epoch:3 step:2918 [D loss: 0.639830, acc: 65.62%] [G loss: 2.636215]\n",
      "epoch:3 step:2919 [D loss: 0.458705, acc: 78.12%] [G loss: 3.063423]\n",
      "epoch:3 step:2920 [D loss: 0.640383, acc: 69.53%] [G loss: 3.001944]\n",
      "epoch:3 step:2921 [D loss: 0.555101, acc: 71.88%] [G loss: 3.273503]\n",
      "epoch:3 step:2922 [D loss: 0.590301, acc: 70.31%] [G loss: 3.177111]\n",
      "epoch:3 step:2923 [D loss: 0.677687, acc: 62.50%] [G loss: 3.047110]\n",
      "epoch:3 step:2924 [D loss: 0.740985, acc: 60.94%] [G loss: 2.648354]\n",
      "epoch:3 step:2925 [D loss: 0.519923, acc: 74.22%] [G loss: 2.874964]\n",
      "epoch:3 step:2926 [D loss: 0.676878, acc: 61.72%] [G loss: 2.452663]\n",
      "epoch:3 step:2927 [D loss: 0.565276, acc: 70.31%] [G loss: 2.998275]\n",
      "epoch:3 step:2928 [D loss: 0.515186, acc: 75.78%] [G loss: 2.939437]\n",
      "epoch:3 step:2929 [D loss: 0.552124, acc: 69.53%] [G loss: 3.321813]\n",
      "epoch:3 step:2930 [D loss: 0.536941, acc: 74.22%] [G loss: 2.843122]\n",
      "epoch:3 step:2931 [D loss: 0.619966, acc: 71.09%] [G loss: 3.017666]\n",
      "epoch:3 step:2932 [D loss: 0.692145, acc: 64.06%] [G loss: 2.974296]\n",
      "epoch:3 step:2933 [D loss: 0.553798, acc: 68.75%] [G loss: 2.990318]\n",
      "epoch:3 step:2934 [D loss: 0.654537, acc: 64.84%] [G loss: 2.731687]\n",
      "epoch:3 step:2935 [D loss: 0.513926, acc: 75.00%] [G loss: 2.864138]\n",
      "epoch:3 step:2936 [D loss: 0.547330, acc: 72.66%] [G loss: 2.943624]\n",
      "epoch:3 step:2937 [D loss: 0.582387, acc: 71.09%] [G loss: 2.886395]\n",
      "epoch:3 step:2938 [D loss: 0.506596, acc: 77.34%] [G loss: 3.010091]\n",
      "epoch:3 step:2939 [D loss: 0.546936, acc: 70.31%] [G loss: 3.028328]\n",
      "epoch:3 step:2940 [D loss: 0.572234, acc: 74.22%] [G loss: 2.534491]\n",
      "epoch:3 step:2941 [D loss: 0.550769, acc: 73.44%] [G loss: 2.863222]\n",
      "epoch:3 step:2942 [D loss: 0.574584, acc: 71.88%] [G loss: 3.063458]\n",
      "epoch:3 step:2943 [D loss: 0.537531, acc: 75.00%] [G loss: 2.831358]\n",
      "epoch:3 step:2944 [D loss: 0.607012, acc: 67.97%] [G loss: 2.710814]\n",
      "epoch:3 step:2945 [D loss: 0.563260, acc: 70.31%] [G loss: 3.312914]\n",
      "epoch:3 step:2946 [D loss: 0.494521, acc: 77.34%] [G loss: 3.173646]\n",
      "epoch:3 step:2947 [D loss: 0.553414, acc: 67.97%] [G loss: 2.933666]\n",
      "epoch:3 step:2948 [D loss: 0.635469, acc: 64.84%] [G loss: 2.522815]\n",
      "epoch:3 step:2949 [D loss: 0.575539, acc: 74.22%] [G loss: 2.981762]\n",
      "epoch:3 step:2950 [D loss: 0.622921, acc: 66.41%] [G loss: 2.862831]\n",
      "epoch:3 step:2951 [D loss: 0.520068, acc: 70.31%] [G loss: 2.894000]\n",
      "epoch:3 step:2952 [D loss: 0.469809, acc: 79.69%] [G loss: 3.296036]\n",
      "epoch:3 step:2953 [D loss: 0.527254, acc: 72.66%] [G loss: 2.807436]\n",
      "epoch:3 step:2954 [D loss: 0.565173, acc: 67.97%] [G loss: 2.910982]\n",
      "epoch:3 step:2955 [D loss: 0.569014, acc: 69.53%] [G loss: 2.818375]\n",
      "epoch:3 step:2956 [D loss: 0.658679, acc: 64.06%] [G loss: 2.828727]\n",
      "epoch:3 step:2957 [D loss: 0.581594, acc: 69.53%] [G loss: 3.019373]\n",
      "epoch:3 step:2958 [D loss: 0.589048, acc: 72.66%] [G loss: 2.983994]\n",
      "epoch:3 step:2959 [D loss: 0.541136, acc: 73.44%] [G loss: 2.545054]\n",
      "epoch:3 step:2960 [D loss: 0.591799, acc: 66.41%] [G loss: 2.723573]\n",
      "epoch:3 step:2961 [D loss: 0.659299, acc: 64.84%] [G loss: 3.208474]\n",
      "epoch:3 step:2962 [D loss: 0.563493, acc: 75.78%] [G loss: 3.210363]\n",
      "epoch:3 step:2963 [D loss: 0.485663, acc: 75.78%] [G loss: 3.327110]\n",
      "epoch:3 step:2964 [D loss: 0.596117, acc: 68.75%] [G loss: 2.680502]\n",
      "epoch:3 step:2965 [D loss: 0.529900, acc: 72.66%] [G loss: 2.880807]\n",
      "epoch:3 step:2966 [D loss: 0.623631, acc: 65.62%] [G loss: 3.308468]\n",
      "epoch:3 step:2967 [D loss: 0.447664, acc: 79.69%] [G loss: 3.204144]\n",
      "epoch:3 step:2968 [D loss: 0.611592, acc: 64.84%] [G loss: 2.729888]\n",
      "epoch:3 step:2969 [D loss: 0.574898, acc: 72.66%] [G loss: 3.019217]\n",
      "epoch:3 step:2970 [D loss: 0.507465, acc: 74.22%] [G loss: 3.128585]\n",
      "epoch:3 step:2971 [D loss: 0.701133, acc: 61.72%] [G loss: 2.457680]\n",
      "epoch:3 step:2972 [D loss: 0.568734, acc: 68.75%] [G loss: 2.733397]\n",
      "epoch:3 step:2973 [D loss: 0.536861, acc: 72.66%] [G loss: 3.220417]\n",
      "epoch:3 step:2974 [D loss: 0.408311, acc: 83.59%] [G loss: 2.855560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2975 [D loss: 0.485426, acc: 77.34%] [G loss: 3.210066]\n",
      "epoch:3 step:2976 [D loss: 0.551033, acc: 71.09%] [G loss: 2.960959]\n",
      "epoch:3 step:2977 [D loss: 0.547619, acc: 73.44%] [G loss: 2.863249]\n",
      "epoch:3 step:2978 [D loss: 0.538200, acc: 76.56%] [G loss: 3.414271]\n",
      "epoch:3 step:2979 [D loss: 0.569107, acc: 70.31%] [G loss: 3.118490]\n",
      "epoch:3 step:2980 [D loss: 0.541464, acc: 74.22%] [G loss: 2.714705]\n",
      "epoch:3 step:2981 [D loss: 0.512645, acc: 78.91%] [G loss: 2.796669]\n",
      "epoch:3 step:2982 [D loss: 0.479188, acc: 78.12%] [G loss: 3.162887]\n",
      "epoch:3 step:2983 [D loss: 0.680360, acc: 60.16%] [G loss: 3.006399]\n",
      "epoch:3 step:2984 [D loss: 0.555057, acc: 74.22%] [G loss: 2.996905]\n",
      "epoch:3 step:2985 [D loss: 0.516423, acc: 73.44%] [G loss: 3.156248]\n",
      "epoch:3 step:2986 [D loss: 0.536362, acc: 75.00%] [G loss: 2.924439]\n",
      "epoch:3 step:2987 [D loss: 0.550623, acc: 73.44%] [G loss: 3.184671]\n",
      "epoch:3 step:2988 [D loss: 0.492120, acc: 73.44%] [G loss: 3.239636]\n",
      "epoch:3 step:2989 [D loss: 0.475786, acc: 76.56%] [G loss: 3.428493]\n",
      "epoch:3 step:2990 [D loss: 0.608015, acc: 68.75%] [G loss: 3.031059]\n",
      "epoch:3 step:2991 [D loss: 0.598978, acc: 68.75%] [G loss: 3.201743]\n",
      "epoch:3 step:2992 [D loss: 0.526011, acc: 71.88%] [G loss: 3.271672]\n",
      "epoch:3 step:2993 [D loss: 0.601110, acc: 68.75%] [G loss: 2.806831]\n",
      "epoch:3 step:2994 [D loss: 0.566720, acc: 68.75%] [G loss: 2.935757]\n",
      "epoch:3 step:2995 [D loss: 0.633232, acc: 63.28%] [G loss: 3.059521]\n",
      "epoch:3 step:2996 [D loss: 0.524291, acc: 73.44%] [G loss: 3.059392]\n",
      "epoch:3 step:2997 [D loss: 0.562946, acc: 67.97%] [G loss: 2.960273]\n",
      "epoch:3 step:2998 [D loss: 0.616398, acc: 67.97%] [G loss: 3.028859]\n",
      "epoch:3 step:2999 [D loss: 0.589427, acc: 71.88%] [G loss: 2.977775]\n",
      "epoch:3 step:3000 [D loss: 0.482635, acc: 76.56%] [G loss: 2.808621]\n",
      "##############\n",
      "[3.03977058 1.9077346  7.09184781 5.59195218 4.53542465 6.15574464\n",
      " 5.24271674 5.36170021 5.57618541 4.06187123]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.543557, acc: 74.22%] [G loss: 3.232732]\n",
      "epoch:3 step:3002 [D loss: 0.538583, acc: 71.88%] [G loss: 3.335568]\n",
      "epoch:3 step:3003 [D loss: 0.556332, acc: 72.66%] [G loss: 3.293494]\n",
      "epoch:3 step:3004 [D loss: 0.579266, acc: 71.09%] [G loss: 3.037544]\n",
      "epoch:3 step:3005 [D loss: 0.497044, acc: 75.00%] [G loss: 3.180570]\n",
      "epoch:3 step:3006 [D loss: 0.498022, acc: 75.00%] [G loss: 2.896426]\n",
      "epoch:3 step:3007 [D loss: 0.557995, acc: 70.31%] [G loss: 2.812787]\n",
      "epoch:3 step:3008 [D loss: 0.545572, acc: 75.00%] [G loss: 3.067987]\n",
      "epoch:3 step:3009 [D loss: 0.507672, acc: 74.22%] [G loss: 3.061010]\n",
      "epoch:3 step:3010 [D loss: 0.573646, acc: 72.66%] [G loss: 2.965889]\n",
      "epoch:3 step:3011 [D loss: 0.551922, acc: 74.22%] [G loss: 2.804729]\n",
      "epoch:3 step:3012 [D loss: 0.620018, acc: 64.84%] [G loss: 2.924698]\n",
      "epoch:3 step:3013 [D loss: 0.550633, acc: 71.88%] [G loss: 3.150991]\n",
      "epoch:3 step:3014 [D loss: 0.565061, acc: 70.31%] [G loss: 2.778874]\n",
      "epoch:3 step:3015 [D loss: 0.562306, acc: 71.09%] [G loss: 2.872406]\n",
      "epoch:3 step:3016 [D loss: 0.510090, acc: 77.34%] [G loss: 3.185588]\n",
      "epoch:3 step:3017 [D loss: 0.510390, acc: 77.34%] [G loss: 3.012573]\n",
      "epoch:3 step:3018 [D loss: 0.512414, acc: 74.22%] [G loss: 3.253424]\n",
      "epoch:3 step:3019 [D loss: 0.471201, acc: 78.12%] [G loss: 3.401928]\n",
      "epoch:3 step:3020 [D loss: 0.448864, acc: 82.03%] [G loss: 3.176757]\n",
      "epoch:3 step:3021 [D loss: 0.511225, acc: 72.66%] [G loss: 3.273593]\n",
      "epoch:3 step:3022 [D loss: 0.622975, acc: 71.88%] [G loss: 2.980495]\n",
      "epoch:3 step:3023 [D loss: 0.575265, acc: 71.88%] [G loss: 3.153862]\n",
      "epoch:3 step:3024 [D loss: 0.631854, acc: 69.53%] [G loss: 2.915163]\n",
      "epoch:3 step:3025 [D loss: 0.807827, acc: 55.47%] [G loss: 2.745408]\n",
      "epoch:3 step:3026 [D loss: 0.654094, acc: 64.84%] [G loss: 2.459467]\n",
      "epoch:3 step:3027 [D loss: 0.562773, acc: 71.09%] [G loss: 2.795067]\n",
      "epoch:3 step:3028 [D loss: 0.503951, acc: 77.34%] [G loss: 3.106949]\n",
      "epoch:3 step:3029 [D loss: 0.551171, acc: 71.09%] [G loss: 2.683795]\n",
      "epoch:3 step:3030 [D loss: 0.517163, acc: 71.09%] [G loss: 3.095271]\n",
      "epoch:3 step:3031 [D loss: 0.601986, acc: 70.31%] [G loss: 3.116792]\n",
      "epoch:3 step:3032 [D loss: 0.608325, acc: 65.62%] [G loss: 3.041517]\n",
      "epoch:3 step:3033 [D loss: 0.535421, acc: 76.56%] [G loss: 2.998324]\n",
      "epoch:3 step:3034 [D loss: 0.601985, acc: 64.84%] [G loss: 2.811349]\n",
      "epoch:3 step:3035 [D loss: 0.590233, acc: 69.53%] [G loss: 2.628578]\n",
      "epoch:3 step:3036 [D loss: 0.630470, acc: 64.84%] [G loss: 2.655186]\n",
      "epoch:3 step:3037 [D loss: 0.527924, acc: 72.66%] [G loss: 2.724775]\n",
      "epoch:3 step:3038 [D loss: 0.600837, acc: 65.62%] [G loss: 2.690323]\n",
      "epoch:3 step:3039 [D loss: 0.604900, acc: 68.75%] [G loss: 2.738053]\n",
      "epoch:3 step:3040 [D loss: 0.657311, acc: 64.84%] [G loss: 2.971954]\n",
      "epoch:3 step:3041 [D loss: 0.632733, acc: 65.62%] [G loss: 2.907238]\n",
      "epoch:3 step:3042 [D loss: 0.481218, acc: 75.00%] [G loss: 2.940337]\n",
      "epoch:3 step:3043 [D loss: 0.488203, acc: 76.56%] [G loss: 3.528157]\n",
      "epoch:3 step:3044 [D loss: 0.668777, acc: 64.06%] [G loss: 3.070234]\n",
      "epoch:3 step:3045 [D loss: 0.583442, acc: 70.31%] [G loss: 3.081045]\n",
      "epoch:3 step:3046 [D loss: 0.571409, acc: 71.09%] [G loss: 3.276676]\n",
      "epoch:3 step:3047 [D loss: 0.560001, acc: 73.44%] [G loss: 3.062170]\n",
      "epoch:3 step:3048 [D loss: 0.641504, acc: 61.72%] [G loss: 2.814639]\n",
      "epoch:3 step:3049 [D loss: 0.542892, acc: 73.44%] [G loss: 2.912988]\n",
      "epoch:3 step:3050 [D loss: 0.669483, acc: 59.38%] [G loss: 2.967141]\n",
      "epoch:3 step:3051 [D loss: 0.536643, acc: 73.44%] [G loss: 2.673252]\n",
      "epoch:3 step:3052 [D loss: 0.540863, acc: 69.53%] [G loss: 2.647325]\n",
      "epoch:3 step:3053 [D loss: 0.570657, acc: 67.97%] [G loss: 2.636841]\n",
      "epoch:3 step:3054 [D loss: 0.529574, acc: 70.31%] [G loss: 2.672693]\n",
      "epoch:3 step:3055 [D loss: 0.566903, acc: 69.53%] [G loss: 2.879211]\n",
      "epoch:3 step:3056 [D loss: 0.517601, acc: 72.66%] [G loss: 2.872942]\n",
      "epoch:3 step:3057 [D loss: 0.575737, acc: 75.00%] [G loss: 2.610102]\n",
      "epoch:3 step:3058 [D loss: 0.621024, acc: 63.28%] [G loss: 2.680649]\n",
      "epoch:3 step:3059 [D loss: 0.533465, acc: 77.34%] [G loss: 2.565664]\n",
      "epoch:3 step:3060 [D loss: 0.636838, acc: 60.94%] [G loss: 2.732270]\n",
      "epoch:3 step:3061 [D loss: 0.674828, acc: 66.41%] [G loss: 2.785098]\n",
      "epoch:3 step:3062 [D loss: 0.599965, acc: 72.66%] [G loss: 2.914754]\n",
      "epoch:3 step:3063 [D loss: 0.611447, acc: 70.31%] [G loss: 2.665286]\n",
      "epoch:3 step:3064 [D loss: 0.545289, acc: 71.09%] [G loss: 2.732050]\n",
      "epoch:3 step:3065 [D loss: 0.493883, acc: 76.56%] [G loss: 3.020919]\n",
      "epoch:3 step:3066 [D loss: 0.514831, acc: 77.34%] [G loss: 3.191783]\n",
      "epoch:3 step:3067 [D loss: 0.557403, acc: 69.53%] [G loss: 3.038427]\n",
      "epoch:3 step:3068 [D loss: 0.538651, acc: 75.00%] [G loss: 2.902567]\n",
      "epoch:3 step:3069 [D loss: 0.521151, acc: 73.44%] [G loss: 3.129678]\n",
      "epoch:3 step:3070 [D loss: 0.479615, acc: 78.91%] [G loss: 2.966177]\n",
      "epoch:3 step:3071 [D loss: 0.622460, acc: 66.41%] [G loss: 2.900907]\n",
      "epoch:3 step:3072 [D loss: 0.516353, acc: 75.00%] [G loss: 3.334533]\n",
      "epoch:3 step:3073 [D loss: 0.578921, acc: 74.22%] [G loss: 3.207793]\n",
      "epoch:3 step:3074 [D loss: 0.623096, acc: 64.84%] [G loss: 2.760639]\n",
      "epoch:3 step:3075 [D loss: 0.504827, acc: 78.91%] [G loss: 3.003633]\n",
      "epoch:3 step:3076 [D loss: 0.554234, acc: 71.88%] [G loss: 2.787244]\n",
      "epoch:3 step:3077 [D loss: 0.531145, acc: 75.00%] [G loss: 3.188766]\n",
      "epoch:3 step:3078 [D loss: 0.541980, acc: 76.56%] [G loss: 2.819454]\n",
      "epoch:3 step:3079 [D loss: 0.520781, acc: 71.88%] [G loss: 2.876735]\n",
      "epoch:3 step:3080 [D loss: 0.541268, acc: 72.66%] [G loss: 3.089225]\n",
      "epoch:3 step:3081 [D loss: 0.537619, acc: 79.69%] [G loss: 2.911336]\n",
      "epoch:3 step:3082 [D loss: 0.570946, acc: 71.88%] [G loss: 2.929171]\n",
      "epoch:3 step:3083 [D loss: 0.506899, acc: 74.22%] [G loss: 2.818308]\n",
      "epoch:3 step:3084 [D loss: 0.500415, acc: 78.12%] [G loss: 3.308008]\n",
      "epoch:3 step:3085 [D loss: 0.630844, acc: 67.97%] [G loss: 2.671790]\n",
      "epoch:3 step:3086 [D loss: 0.645026, acc: 63.28%] [G loss: 2.652622]\n",
      "epoch:3 step:3087 [D loss: 0.619352, acc: 68.75%] [G loss: 2.532092]\n",
      "epoch:3 step:3088 [D loss: 0.567996, acc: 70.31%] [G loss: 2.791544]\n",
      "epoch:3 step:3089 [D loss: 0.600039, acc: 71.09%] [G loss: 2.700298]\n",
      "epoch:3 step:3090 [D loss: 0.495423, acc: 74.22%] [G loss: 3.078307]\n",
      "epoch:3 step:3091 [D loss: 0.546584, acc: 71.09%] [G loss: 3.047831]\n",
      "epoch:3 step:3092 [D loss: 0.585856, acc: 71.09%] [G loss: 2.917263]\n",
      "epoch:3 step:3093 [D loss: 0.630879, acc: 63.28%] [G loss: 2.822502]\n",
      "epoch:3 step:3094 [D loss: 0.553630, acc: 70.31%] [G loss: 3.199889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3095 [D loss: 0.554733, acc: 67.97%] [G loss: 2.694035]\n",
      "epoch:3 step:3096 [D loss: 0.487425, acc: 75.78%] [G loss: 2.604190]\n",
      "epoch:3 step:3097 [D loss: 0.529822, acc: 70.31%] [G loss: 3.027729]\n",
      "epoch:3 step:3098 [D loss: 0.550155, acc: 67.97%] [G loss: 2.969564]\n",
      "epoch:3 step:3099 [D loss: 0.594599, acc: 64.84%] [G loss: 2.863103]\n",
      "epoch:3 step:3100 [D loss: 0.512983, acc: 73.44%] [G loss: 3.200721]\n",
      "epoch:3 step:3101 [D loss: 0.595248, acc: 67.97%] [G loss: 3.128222]\n",
      "epoch:3 step:3102 [D loss: 0.654801, acc: 67.19%] [G loss: 3.023721]\n",
      "epoch:3 step:3103 [D loss: 0.620865, acc: 66.41%] [G loss: 2.838953]\n",
      "epoch:3 step:3104 [D loss: 0.724583, acc: 55.47%] [G loss: 2.891222]\n",
      "epoch:3 step:3105 [D loss: 0.612302, acc: 63.28%] [G loss: 2.674919]\n",
      "epoch:3 step:3106 [D loss: 0.489421, acc: 80.47%] [G loss: 2.982700]\n",
      "epoch:3 step:3107 [D loss: 0.555172, acc: 72.66%] [G loss: 3.132876]\n",
      "epoch:3 step:3108 [D loss: 0.580613, acc: 65.62%] [G loss: 2.923188]\n",
      "epoch:3 step:3109 [D loss: 0.633314, acc: 61.72%] [G loss: 2.891743]\n",
      "epoch:3 step:3110 [D loss: 0.646962, acc: 64.84%] [G loss: 2.935503]\n",
      "epoch:3 step:3111 [D loss: 0.556468, acc: 77.34%] [G loss: 3.290335]\n",
      "epoch:3 step:3112 [D loss: 0.626073, acc: 64.06%] [G loss: 2.683783]\n",
      "epoch:3 step:3113 [D loss: 0.595141, acc: 67.97%] [G loss: 2.785595]\n",
      "epoch:3 step:3114 [D loss: 0.538181, acc: 71.88%] [G loss: 2.881556]\n",
      "epoch:3 step:3115 [D loss: 0.506323, acc: 75.00%] [G loss: 3.170115]\n",
      "epoch:3 step:3116 [D loss: 0.541320, acc: 74.22%] [G loss: 2.952992]\n",
      "epoch:3 step:3117 [D loss: 0.590806, acc: 67.19%] [G loss: 3.121892]\n",
      "epoch:3 step:3118 [D loss: 0.565104, acc: 67.97%] [G loss: 3.113502]\n",
      "epoch:3 step:3119 [D loss: 0.537183, acc: 74.22%] [G loss: 3.064789]\n",
      "epoch:3 step:3120 [D loss: 0.495534, acc: 76.56%] [G loss: 3.423396]\n",
      "epoch:3 step:3121 [D loss: 0.598855, acc: 70.31%] [G loss: 3.074167]\n",
      "epoch:3 step:3122 [D loss: 0.554729, acc: 67.19%] [G loss: 3.048457]\n",
      "epoch:3 step:3123 [D loss: 0.432073, acc: 84.38%] [G loss: 3.850675]\n",
      "epoch:3 step:3124 [D loss: 0.514714, acc: 74.22%] [G loss: 3.701666]\n",
      "epoch:3 step:3125 [D loss: 0.470045, acc: 80.47%] [G loss: 4.034056]\n",
      "epoch:3 step:3126 [D loss: 0.417060, acc: 79.69%] [G loss: 3.475032]\n",
      "epoch:3 step:3127 [D loss: 0.678464, acc: 64.06%] [G loss: 2.804941]\n",
      "epoch:3 step:3128 [D loss: 0.511238, acc: 77.34%] [G loss: 3.003927]\n",
      "epoch:3 step:3129 [D loss: 0.587650, acc: 69.53%] [G loss: 3.004822]\n",
      "epoch:3 step:3130 [D loss: 0.473543, acc: 78.91%] [G loss: 3.339663]\n",
      "epoch:3 step:3131 [D loss: 0.496383, acc: 75.78%] [G loss: 3.110797]\n",
      "epoch:3 step:3132 [D loss: 0.594544, acc: 68.75%] [G loss: 2.923982]\n",
      "epoch:3 step:3133 [D loss: 0.539203, acc: 74.22%] [G loss: 2.796200]\n",
      "epoch:3 step:3134 [D loss: 0.576625, acc: 71.88%] [G loss: 2.744009]\n",
      "epoch:3 step:3135 [D loss: 0.538715, acc: 72.66%] [G loss: 2.910726]\n",
      "epoch:3 step:3136 [D loss: 0.497195, acc: 75.00%] [G loss: 2.954983]\n",
      "epoch:3 step:3137 [D loss: 0.550812, acc: 69.53%] [G loss: 2.926099]\n",
      "epoch:3 step:3138 [D loss: 0.612094, acc: 65.62%] [G loss: 2.822846]\n",
      "epoch:3 step:3139 [D loss: 0.581608, acc: 70.31%] [G loss: 2.948160]\n",
      "epoch:3 step:3140 [D loss: 0.655059, acc: 59.38%] [G loss: 2.806176]\n",
      "epoch:3 step:3141 [D loss: 0.579765, acc: 68.75%] [G loss: 2.885968]\n",
      "epoch:3 step:3142 [D loss: 0.545024, acc: 77.34%] [G loss: 2.857359]\n",
      "epoch:3 step:3143 [D loss: 0.538544, acc: 71.88%] [G loss: 2.917373]\n",
      "epoch:3 step:3144 [D loss: 0.547086, acc: 70.31%] [G loss: 3.432895]\n",
      "epoch:3 step:3145 [D loss: 0.635426, acc: 64.06%] [G loss: 3.004003]\n",
      "epoch:3 step:3146 [D loss: 0.550922, acc: 74.22%] [G loss: 3.137648]\n",
      "epoch:3 step:3147 [D loss: 0.583452, acc: 70.31%] [G loss: 2.778061]\n",
      "epoch:3 step:3148 [D loss: 0.524199, acc: 71.09%] [G loss: 3.174847]\n",
      "epoch:3 step:3149 [D loss: 0.593925, acc: 66.41%] [G loss: 2.755819]\n",
      "epoch:3 step:3150 [D loss: 0.531762, acc: 75.00%] [G loss: 2.719249]\n",
      "epoch:3 step:3151 [D loss: 0.587631, acc: 67.97%] [G loss: 3.121153]\n",
      "epoch:3 step:3152 [D loss: 0.535750, acc: 73.44%] [G loss: 2.970123]\n",
      "epoch:3 step:3153 [D loss: 0.614258, acc: 66.41%] [G loss: 3.177083]\n",
      "epoch:3 step:3154 [D loss: 0.592437, acc: 67.97%] [G loss: 3.078239]\n",
      "epoch:3 step:3155 [D loss: 0.537054, acc: 75.78%] [G loss: 3.438504]\n",
      "epoch:3 step:3156 [D loss: 0.571276, acc: 65.62%] [G loss: 3.017569]\n",
      "epoch:3 step:3157 [D loss: 0.628040, acc: 67.19%] [G loss: 3.347951]\n",
      "epoch:3 step:3158 [D loss: 0.576094, acc: 67.97%] [G loss: 3.175809]\n",
      "epoch:3 step:3159 [D loss: 0.663920, acc: 63.28%] [G loss: 2.603086]\n",
      "epoch:3 step:3160 [D loss: 0.731533, acc: 59.38%] [G loss: 2.420862]\n",
      "epoch:3 step:3161 [D loss: 0.504157, acc: 78.91%] [G loss: 2.988033]\n",
      "epoch:3 step:3162 [D loss: 0.542130, acc: 72.66%] [G loss: 3.141117]\n",
      "epoch:3 step:3163 [D loss: 0.676338, acc: 64.84%] [G loss: 2.675453]\n",
      "epoch:3 step:3164 [D loss: 0.664993, acc: 55.47%] [G loss: 2.878150]\n",
      "epoch:3 step:3165 [D loss: 0.547257, acc: 67.19%] [G loss: 2.808063]\n",
      "epoch:3 step:3166 [D loss: 0.513263, acc: 75.00%] [G loss: 2.919320]\n",
      "epoch:3 step:3167 [D loss: 0.580651, acc: 69.53%] [G loss: 2.699789]\n",
      "epoch:3 step:3168 [D loss: 0.573371, acc: 68.75%] [G loss: 3.111373]\n",
      "epoch:3 step:3169 [D loss: 0.506909, acc: 73.44%] [G loss: 3.050359]\n",
      "epoch:3 step:3170 [D loss: 0.502005, acc: 73.44%] [G loss: 3.056662]\n",
      "epoch:3 step:3171 [D loss: 0.574104, acc: 67.97%] [G loss: 2.883388]\n",
      "epoch:3 step:3172 [D loss: 0.617741, acc: 67.97%] [G loss: 2.687371]\n",
      "epoch:3 step:3173 [D loss: 0.611337, acc: 67.19%] [G loss: 3.032971]\n",
      "epoch:3 step:3174 [D loss: 0.579346, acc: 72.66%] [G loss: 2.856834]\n",
      "epoch:3 step:3175 [D loss: 0.548575, acc: 73.44%] [G loss: 2.903309]\n",
      "epoch:3 step:3176 [D loss: 0.492190, acc: 78.12%] [G loss: 3.250470]\n",
      "epoch:3 step:3177 [D loss: 0.470411, acc: 77.34%] [G loss: 3.124480]\n",
      "epoch:3 step:3178 [D loss: 0.561522, acc: 73.44%] [G loss: 3.453944]\n",
      "epoch:3 step:3179 [D loss: 0.452555, acc: 82.03%] [G loss: 3.291325]\n",
      "epoch:3 step:3180 [D loss: 0.561296, acc: 71.09%] [G loss: 2.870811]\n",
      "epoch:3 step:3181 [D loss: 0.612019, acc: 66.41%] [G loss: 2.948617]\n",
      "epoch:3 step:3182 [D loss: 0.623663, acc: 71.09%] [G loss: 2.907501]\n",
      "epoch:3 step:3183 [D loss: 0.530888, acc: 78.12%] [G loss: 2.762945]\n",
      "epoch:3 step:3184 [D loss: 0.531658, acc: 72.66%] [G loss: 2.831061]\n",
      "epoch:3 step:3185 [D loss: 0.557550, acc: 68.75%] [G loss: 3.182714]\n",
      "epoch:3 step:3186 [D loss: 0.611666, acc: 60.94%] [G loss: 3.020220]\n",
      "epoch:3 step:3187 [D loss: 0.733342, acc: 60.16%] [G loss: 2.610919]\n",
      "epoch:3 step:3188 [D loss: 0.654608, acc: 61.72%] [G loss: 2.572008]\n",
      "epoch:3 step:3189 [D loss: 0.500456, acc: 81.25%] [G loss: 2.902975]\n",
      "epoch:3 step:3190 [D loss: 0.560612, acc: 72.66%] [G loss: 2.982826]\n",
      "epoch:3 step:3191 [D loss: 0.555111, acc: 70.31%] [G loss: 2.717317]\n",
      "epoch:3 step:3192 [D loss: 0.452807, acc: 78.91%] [G loss: 3.340652]\n",
      "epoch:3 step:3193 [D loss: 0.553611, acc: 74.22%] [G loss: 2.732500]\n",
      "epoch:3 step:3194 [D loss: 0.606461, acc: 69.53%] [G loss: 2.967426]\n",
      "epoch:3 step:3195 [D loss: 0.572428, acc: 69.53%] [G loss: 2.964662]\n",
      "epoch:3 step:3196 [D loss: 0.619488, acc: 70.31%] [G loss: 3.013748]\n",
      "epoch:3 step:3197 [D loss: 0.613924, acc: 63.28%] [G loss: 2.497660]\n",
      "epoch:3 step:3198 [D loss: 0.568049, acc: 69.53%] [G loss: 2.915478]\n",
      "epoch:3 step:3199 [D loss: 0.567569, acc: 70.31%] [G loss: 3.119407]\n",
      "epoch:3 step:3200 [D loss: 0.521017, acc: 74.22%] [G loss: 2.760264]\n",
      "##############\n",
      "[3.06215049 2.01196877 7.08345768 5.68044082 4.32212989 6.48881075\n",
      " 5.30462312 5.49121939 5.63683703 4.18873558]\n",
      "##########\n",
      "epoch:3 step:3201 [D loss: 0.607186, acc: 64.84%] [G loss: 3.091259]\n",
      "epoch:3 step:3202 [D loss: 0.671240, acc: 60.94%] [G loss: 2.651548]\n",
      "epoch:3 step:3203 [D loss: 0.496840, acc: 75.78%] [G loss: 2.861164]\n",
      "epoch:3 step:3204 [D loss: 0.618526, acc: 67.97%] [G loss: 2.526802]\n",
      "epoch:3 step:3205 [D loss: 0.551388, acc: 72.66%] [G loss: 2.550901]\n",
      "epoch:3 step:3206 [D loss: 0.571208, acc: 69.53%] [G loss: 2.770331]\n",
      "epoch:3 step:3207 [D loss: 0.552008, acc: 71.88%] [G loss: 2.767549]\n",
      "epoch:3 step:3208 [D loss: 0.493960, acc: 75.00%] [G loss: 3.181455]\n",
      "epoch:3 step:3209 [D loss: 0.502632, acc: 75.78%] [G loss: 3.072735]\n",
      "epoch:3 step:3210 [D loss: 0.481556, acc: 79.69%] [G loss: 3.218865]\n",
      "epoch:3 step:3211 [D loss: 0.587091, acc: 74.22%] [G loss: 2.839530]\n",
      "epoch:3 step:3212 [D loss: 0.558053, acc: 75.00%] [G loss: 2.946764]\n",
      "epoch:3 step:3213 [D loss: 0.484439, acc: 74.22%] [G loss: 3.368641]\n",
      "epoch:3 step:3214 [D loss: 0.504697, acc: 78.91%] [G loss: 3.117045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3215 [D loss: 0.720992, acc: 58.59%] [G loss: 3.109299]\n",
      "epoch:3 step:3216 [D loss: 0.467729, acc: 75.78%] [G loss: 3.098104]\n",
      "epoch:3 step:3217 [D loss: 0.566207, acc: 66.41%] [G loss: 3.177715]\n",
      "epoch:3 step:3218 [D loss: 0.568464, acc: 68.75%] [G loss: 2.906419]\n",
      "epoch:3 step:3219 [D loss: 0.516669, acc: 74.22%] [G loss: 3.116915]\n",
      "epoch:3 step:3220 [D loss: 0.573880, acc: 70.31%] [G loss: 2.850059]\n",
      "epoch:3 step:3221 [D loss: 0.598665, acc: 71.88%] [G loss: 2.768763]\n",
      "epoch:3 step:3222 [D loss: 0.581180, acc: 68.75%] [G loss: 2.702224]\n",
      "epoch:3 step:3223 [D loss: 0.600256, acc: 69.53%] [G loss: 2.593822]\n",
      "epoch:3 step:3224 [D loss: 0.652849, acc: 59.38%] [G loss: 2.845107]\n",
      "epoch:3 step:3225 [D loss: 0.599720, acc: 68.75%] [G loss: 3.028250]\n",
      "epoch:3 step:3226 [D loss: 0.633617, acc: 62.50%] [G loss: 2.717993]\n",
      "epoch:3 step:3227 [D loss: 0.491804, acc: 78.12%] [G loss: 2.922570]\n",
      "epoch:3 step:3228 [D loss: 0.579633, acc: 72.66%] [G loss: 2.672498]\n",
      "epoch:3 step:3229 [D loss: 0.627722, acc: 66.41%] [G loss: 2.692776]\n",
      "epoch:3 step:3230 [D loss: 0.666239, acc: 61.72%] [G loss: 2.926661]\n",
      "epoch:3 step:3231 [D loss: 0.595720, acc: 69.53%] [G loss: 2.661974]\n",
      "epoch:3 step:3232 [D loss: 0.486777, acc: 82.03%] [G loss: 2.660168]\n",
      "epoch:3 step:3233 [D loss: 0.457866, acc: 82.03%] [G loss: 3.200528]\n",
      "epoch:3 step:3234 [D loss: 0.507224, acc: 75.00%] [G loss: 3.291478]\n",
      "epoch:3 step:3235 [D loss: 0.580527, acc: 67.97%] [G loss: 2.941410]\n",
      "epoch:3 step:3236 [D loss: 0.522837, acc: 71.88%] [G loss: 3.330888]\n",
      "epoch:3 step:3237 [D loss: 0.644051, acc: 70.31%] [G loss: 3.112137]\n",
      "epoch:3 step:3238 [D loss: 0.521658, acc: 75.00%] [G loss: 3.159228]\n",
      "epoch:3 step:3239 [D loss: 0.479316, acc: 79.69%] [G loss: 3.477300]\n",
      "epoch:3 step:3240 [D loss: 0.507904, acc: 75.00%] [G loss: 3.661852]\n",
      "epoch:3 step:3241 [D loss: 0.519499, acc: 79.69%] [G loss: 3.676979]\n",
      "epoch:3 step:3242 [D loss: 0.591297, acc: 66.41%] [G loss: 3.160889]\n",
      "epoch:3 step:3243 [D loss: 0.573356, acc: 69.53%] [G loss: 3.048198]\n",
      "epoch:3 step:3244 [D loss: 0.593063, acc: 67.97%] [G loss: 2.968338]\n",
      "epoch:3 step:3245 [D loss: 0.553750, acc: 73.44%] [G loss: 2.932042]\n",
      "epoch:3 step:3246 [D loss: 0.582142, acc: 69.53%] [G loss: 2.827935]\n",
      "epoch:3 step:3247 [D loss: 0.512310, acc: 80.47%] [G loss: 2.791235]\n",
      "epoch:3 step:3248 [D loss: 0.677196, acc: 66.41%] [G loss: 2.580754]\n",
      "epoch:3 step:3249 [D loss: 0.644645, acc: 63.28%] [G loss: 2.959338]\n",
      "epoch:3 step:3250 [D loss: 0.616951, acc: 67.19%] [G loss: 2.883628]\n",
      "epoch:3 step:3251 [D loss: 0.585806, acc: 74.22%] [G loss: 3.130157]\n",
      "epoch:3 step:3252 [D loss: 0.540074, acc: 71.09%] [G loss: 2.899116]\n",
      "epoch:3 step:3253 [D loss: 0.634483, acc: 69.53%] [G loss: 2.797094]\n",
      "epoch:3 step:3254 [D loss: 0.598007, acc: 69.53%] [G loss: 2.776596]\n",
      "epoch:3 step:3255 [D loss: 0.587145, acc: 67.97%] [G loss: 3.141669]\n",
      "epoch:3 step:3256 [D loss: 0.580729, acc: 70.31%] [G loss: 2.623601]\n",
      "epoch:3 step:3257 [D loss: 0.549867, acc: 68.75%] [G loss: 2.816395]\n",
      "epoch:3 step:3258 [D loss: 0.515035, acc: 78.91%] [G loss: 2.910594]\n",
      "epoch:3 step:3259 [D loss: 0.616441, acc: 65.62%] [G loss: 2.561582]\n",
      "epoch:3 step:3260 [D loss: 0.578191, acc: 70.31%] [G loss: 2.635259]\n",
      "epoch:3 step:3261 [D loss: 0.549203, acc: 72.66%] [G loss: 2.843853]\n",
      "epoch:3 step:3262 [D loss: 0.552925, acc: 72.66%] [G loss: 3.291237]\n",
      "epoch:3 step:3263 [D loss: 0.561285, acc: 71.88%] [G loss: 2.986228]\n",
      "epoch:3 step:3264 [D loss: 0.551456, acc: 77.34%] [G loss: 2.935786]\n",
      "epoch:3 step:3265 [D loss: 0.522284, acc: 74.22%] [G loss: 3.141585]\n",
      "epoch:3 step:3266 [D loss: 0.576100, acc: 66.41%] [G loss: 2.925252]\n",
      "epoch:3 step:3267 [D loss: 0.678153, acc: 64.84%] [G loss: 2.656258]\n",
      "epoch:3 step:3268 [D loss: 0.562010, acc: 72.66%] [G loss: 2.903811]\n",
      "epoch:3 step:3269 [D loss: 0.644635, acc: 64.06%] [G loss: 2.877723]\n",
      "epoch:3 step:3270 [D loss: 0.643011, acc: 67.19%] [G loss: 2.654685]\n",
      "epoch:3 step:3271 [D loss: 0.541413, acc: 72.66%] [G loss: 2.926733]\n",
      "epoch:3 step:3272 [D loss: 0.542124, acc: 75.78%] [G loss: 2.847185]\n",
      "epoch:3 step:3273 [D loss: 0.524285, acc: 74.22%] [G loss: 3.198365]\n",
      "epoch:3 step:3274 [D loss: 0.595483, acc: 65.62%] [G loss: 2.676838]\n",
      "epoch:3 step:3275 [D loss: 0.596309, acc: 64.84%] [G loss: 2.691212]\n",
      "epoch:3 step:3276 [D loss: 0.606292, acc: 63.28%] [G loss: 2.457951]\n",
      "epoch:3 step:3277 [D loss: 0.603460, acc: 67.19%] [G loss: 3.104708]\n",
      "epoch:3 step:3278 [D loss: 0.475794, acc: 78.12%] [G loss: 2.656234]\n",
      "epoch:3 step:3279 [D loss: 0.607697, acc: 69.53%] [G loss: 2.935959]\n",
      "epoch:3 step:3280 [D loss: 0.632933, acc: 67.19%] [G loss: 2.626480]\n",
      "epoch:3 step:3281 [D loss: 0.543999, acc: 71.09%] [G loss: 2.859399]\n",
      "epoch:3 step:3282 [D loss: 0.507650, acc: 79.69%] [G loss: 3.853170]\n",
      "epoch:3 step:3283 [D loss: 0.495633, acc: 78.12%] [G loss: 3.515549]\n",
      "epoch:3 step:3284 [D loss: 0.671336, acc: 62.50%] [G loss: 3.046423]\n",
      "epoch:3 step:3285 [D loss: 0.522319, acc: 74.22%] [G loss: 3.452492]\n",
      "epoch:3 step:3286 [D loss: 0.540230, acc: 72.66%] [G loss: 3.403089]\n",
      "epoch:3 step:3287 [D loss: 0.584291, acc: 67.19%] [G loss: 3.226020]\n",
      "epoch:3 step:3288 [D loss: 0.693143, acc: 61.72%] [G loss: 2.422398]\n",
      "epoch:3 step:3289 [D loss: 0.714103, acc: 65.62%] [G loss: 2.729611]\n",
      "epoch:3 step:3290 [D loss: 0.580099, acc: 65.62%] [G loss: 2.918030]\n",
      "epoch:3 step:3291 [D loss: 0.626650, acc: 66.41%] [G loss: 2.610841]\n",
      "epoch:3 step:3292 [D loss: 0.573785, acc: 67.19%] [G loss: 2.841002]\n",
      "epoch:3 step:3293 [D loss: 0.716186, acc: 61.72%] [G loss: 2.433211]\n",
      "epoch:3 step:3294 [D loss: 0.695106, acc: 58.59%] [G loss: 2.595161]\n",
      "epoch:3 step:3295 [D loss: 0.511778, acc: 77.34%] [G loss: 2.895968]\n",
      "epoch:3 step:3296 [D loss: 0.678242, acc: 61.72%] [G loss: 2.657436]\n",
      "epoch:3 step:3297 [D loss: 0.569408, acc: 77.34%] [G loss: 2.309688]\n",
      "epoch:3 step:3298 [D loss: 0.543635, acc: 68.75%] [G loss: 2.730819]\n",
      "epoch:3 step:3299 [D loss: 0.502999, acc: 77.34%] [G loss: 3.068720]\n",
      "epoch:3 step:3300 [D loss: 0.662356, acc: 66.41%] [G loss: 3.082253]\n",
      "epoch:3 step:3301 [D loss: 0.566360, acc: 71.09%] [G loss: 2.898416]\n",
      "epoch:3 step:3302 [D loss: 0.694393, acc: 64.06%] [G loss: 2.553055]\n",
      "epoch:3 step:3303 [D loss: 0.557771, acc: 70.31%] [G loss: 2.751746]\n",
      "epoch:3 step:3304 [D loss: 0.639876, acc: 60.94%] [G loss: 2.380383]\n",
      "epoch:3 step:3305 [D loss: 0.489529, acc: 76.56%] [G loss: 2.871847]\n",
      "epoch:3 step:3306 [D loss: 0.572287, acc: 64.84%] [G loss: 2.926318]\n",
      "epoch:3 step:3307 [D loss: 0.631888, acc: 65.62%] [G loss: 2.457912]\n",
      "epoch:3 step:3308 [D loss: 0.540906, acc: 69.53%] [G loss: 3.136797]\n",
      "epoch:3 step:3309 [D loss: 0.634749, acc: 67.97%] [G loss: 2.813709]\n",
      "epoch:3 step:3310 [D loss: 0.433277, acc: 80.47%] [G loss: 3.063064]\n",
      "epoch:3 step:3311 [D loss: 0.735254, acc: 63.28%] [G loss: 2.323494]\n",
      "epoch:3 step:3312 [D loss: 0.729127, acc: 57.81%] [G loss: 2.355361]\n",
      "epoch:3 step:3313 [D loss: 0.540680, acc: 70.31%] [G loss: 2.539855]\n",
      "epoch:3 step:3314 [D loss: 0.541538, acc: 69.53%] [G loss: 2.892974]\n",
      "epoch:3 step:3315 [D loss: 0.508967, acc: 76.56%] [G loss: 3.084715]\n",
      "epoch:3 step:3316 [D loss: 0.574603, acc: 69.53%] [G loss: 2.456754]\n",
      "epoch:3 step:3317 [D loss: 0.599818, acc: 67.97%] [G loss: 2.546603]\n",
      "epoch:3 step:3318 [D loss: 0.601154, acc: 73.44%] [G loss: 2.588372]\n",
      "epoch:3 step:3319 [D loss: 0.507640, acc: 77.34%] [G loss: 2.952474]\n",
      "epoch:3 step:3320 [D loss: 0.578668, acc: 68.75%] [G loss: 3.039356]\n",
      "epoch:3 step:3321 [D loss: 0.644935, acc: 67.19%] [G loss: 2.475271]\n",
      "epoch:3 step:3322 [D loss: 0.547732, acc: 72.66%] [G loss: 2.903278]\n",
      "epoch:3 step:3323 [D loss: 0.568220, acc: 73.44%] [G loss: 2.995317]\n",
      "epoch:3 step:3324 [D loss: 0.515590, acc: 75.78%] [G loss: 2.689410]\n",
      "epoch:3 step:3325 [D loss: 0.531551, acc: 73.44%] [G loss: 3.030375]\n",
      "epoch:3 step:3326 [D loss: 0.575109, acc: 73.44%] [G loss: 2.848609]\n",
      "epoch:3 step:3327 [D loss: 0.505892, acc: 78.12%] [G loss: 2.955714]\n",
      "epoch:3 step:3328 [D loss: 0.628904, acc: 64.84%] [G loss: 2.716073]\n",
      "epoch:3 step:3329 [D loss: 0.556850, acc: 70.31%] [G loss: 2.938318]\n",
      "epoch:3 step:3330 [D loss: 0.534496, acc: 71.88%] [G loss: 3.023400]\n",
      "epoch:3 step:3331 [D loss: 0.579378, acc: 70.31%] [G loss: 2.594704]\n",
      "epoch:3 step:3332 [D loss: 0.519501, acc: 78.91%] [G loss: 3.037931]\n",
      "epoch:3 step:3333 [D loss: 0.583378, acc: 69.53%] [G loss: 2.624892]\n",
      "epoch:3 step:3334 [D loss: 0.533335, acc: 70.31%] [G loss: 3.089756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3335 [D loss: 0.559033, acc: 71.09%] [G loss: 3.075288]\n",
      "epoch:3 step:3336 [D loss: 0.619725, acc: 65.62%] [G loss: 2.996579]\n",
      "epoch:3 step:3337 [D loss: 0.639087, acc: 63.28%] [G loss: 3.197011]\n",
      "epoch:3 step:3338 [D loss: 0.600713, acc: 67.19%] [G loss: 2.881351]\n",
      "epoch:3 step:3339 [D loss: 0.612601, acc: 63.28%] [G loss: 2.936587]\n",
      "epoch:3 step:3340 [D loss: 0.579637, acc: 69.53%] [G loss: 2.639961]\n",
      "epoch:3 step:3341 [D loss: 0.498131, acc: 73.44%] [G loss: 3.016784]\n",
      "epoch:3 step:3342 [D loss: 0.557745, acc: 73.44%] [G loss: 2.521619]\n",
      "epoch:3 step:3343 [D loss: 0.621249, acc: 66.41%] [G loss: 2.704660]\n",
      "epoch:3 step:3344 [D loss: 0.471826, acc: 78.12%] [G loss: 2.874866]\n",
      "epoch:3 step:3345 [D loss: 0.549755, acc: 77.34%] [G loss: 3.209402]\n",
      "epoch:3 step:3346 [D loss: 0.576038, acc: 68.75%] [G loss: 2.871402]\n",
      "epoch:3 step:3347 [D loss: 0.637358, acc: 67.19%] [G loss: 2.884393]\n",
      "epoch:3 step:3348 [D loss: 0.517045, acc: 71.09%] [G loss: 2.901738]\n",
      "epoch:3 step:3349 [D loss: 0.586355, acc: 69.53%] [G loss: 2.701725]\n",
      "epoch:3 step:3350 [D loss: 0.560436, acc: 70.31%] [G loss: 2.679666]\n",
      "epoch:3 step:3351 [D loss: 0.584509, acc: 67.97%] [G loss: 2.889593]\n",
      "epoch:3 step:3352 [D loss: 0.560982, acc: 68.75%] [G loss: 2.788482]\n",
      "epoch:3 step:3353 [D loss: 0.570430, acc: 67.19%] [G loss: 2.764507]\n",
      "epoch:3 step:3354 [D loss: 0.515171, acc: 78.12%] [G loss: 2.778802]\n",
      "epoch:3 step:3355 [D loss: 0.548447, acc: 73.44%] [G loss: 3.082031]\n",
      "epoch:3 step:3356 [D loss: 0.560576, acc: 70.31%] [G loss: 2.895193]\n",
      "epoch:3 step:3357 [D loss: 0.522874, acc: 78.12%] [G loss: 3.268685]\n",
      "epoch:3 step:3358 [D loss: 0.536359, acc: 75.00%] [G loss: 3.363092]\n",
      "epoch:3 step:3359 [D loss: 0.579870, acc: 71.09%] [G loss: 2.798722]\n",
      "epoch:3 step:3360 [D loss: 0.514748, acc: 76.56%] [G loss: 3.110436]\n",
      "epoch:3 step:3361 [D loss: 0.498988, acc: 71.88%] [G loss: 2.795150]\n",
      "epoch:3 step:3362 [D loss: 0.534077, acc: 72.66%] [G loss: 2.926169]\n",
      "epoch:3 step:3363 [D loss: 0.472875, acc: 77.34%] [G loss: 3.457959]\n",
      "epoch:3 step:3364 [D loss: 0.533642, acc: 74.22%] [G loss: 3.297005]\n",
      "epoch:3 step:3365 [D loss: 0.524923, acc: 71.09%] [G loss: 3.177565]\n",
      "epoch:3 step:3366 [D loss: 0.498687, acc: 78.12%] [G loss: 2.984284]\n",
      "epoch:3 step:3367 [D loss: 0.470829, acc: 77.34%] [G loss: 2.839986]\n",
      "epoch:3 step:3368 [D loss: 0.553206, acc: 74.22%] [G loss: 3.385043]\n",
      "epoch:3 step:3369 [D loss: 0.503771, acc: 73.44%] [G loss: 3.171568]\n",
      "epoch:3 step:3370 [D loss: 0.600198, acc: 70.31%] [G loss: 2.927493]\n",
      "epoch:3 step:3371 [D loss: 0.588193, acc: 66.41%] [G loss: 2.874629]\n",
      "epoch:3 step:3372 [D loss: 0.502301, acc: 75.00%] [G loss: 2.955360]\n",
      "epoch:3 step:3373 [D loss: 0.692548, acc: 59.38%] [G loss: 2.698509]\n",
      "epoch:3 step:3374 [D loss: 0.547470, acc: 72.66%] [G loss: 2.670050]\n",
      "epoch:3 step:3375 [D loss: 0.572059, acc: 71.88%] [G loss: 2.861714]\n",
      "epoch:3 step:3376 [D loss: 0.514803, acc: 75.00%] [G loss: 2.993566]\n",
      "epoch:3 step:3377 [D loss: 0.607538, acc: 66.41%] [G loss: 2.876450]\n",
      "epoch:3 step:3378 [D loss: 0.590365, acc: 70.31%] [G loss: 2.873583]\n",
      "epoch:3 step:3379 [D loss: 0.548836, acc: 70.31%] [G loss: 3.337518]\n",
      "epoch:3 step:3380 [D loss: 0.548884, acc: 73.44%] [G loss: 2.915226]\n",
      "epoch:3 step:3381 [D loss: 0.582462, acc: 70.31%] [G loss: 3.248290]\n",
      "epoch:3 step:3382 [D loss: 0.610415, acc: 64.84%] [G loss: 2.978468]\n",
      "epoch:3 step:3383 [D loss: 0.565218, acc: 71.88%] [G loss: 2.338321]\n",
      "epoch:3 step:3384 [D loss: 0.705492, acc: 63.28%] [G loss: 2.583709]\n",
      "epoch:3 step:3385 [D loss: 0.522287, acc: 72.66%] [G loss: 2.606192]\n",
      "epoch:3 step:3386 [D loss: 0.542460, acc: 75.00%] [G loss: 2.842681]\n",
      "epoch:3 step:3387 [D loss: 0.610167, acc: 72.66%] [G loss: 2.766058]\n",
      "epoch:3 step:3388 [D loss: 0.629560, acc: 69.53%] [G loss: 3.006480]\n",
      "epoch:3 step:3389 [D loss: 0.589410, acc: 67.97%] [G loss: 2.705306]\n",
      "epoch:3 step:3390 [D loss: 0.619577, acc: 64.06%] [G loss: 2.667802]\n",
      "epoch:3 step:3391 [D loss: 0.553048, acc: 71.88%] [G loss: 2.609015]\n",
      "epoch:3 step:3392 [D loss: 0.512928, acc: 72.66%] [G loss: 2.910789]\n",
      "epoch:3 step:3393 [D loss: 0.560337, acc: 67.97%] [G loss: 3.078477]\n",
      "epoch:3 step:3394 [D loss: 0.519780, acc: 72.66%] [G loss: 2.905229]\n",
      "epoch:3 step:3395 [D loss: 0.525862, acc: 69.53%] [G loss: 2.751006]\n",
      "epoch:3 step:3396 [D loss: 0.533596, acc: 75.00%] [G loss: 2.730246]\n",
      "epoch:3 step:3397 [D loss: 0.655997, acc: 65.62%] [G loss: 2.678919]\n",
      "epoch:3 step:3398 [D loss: 0.593238, acc: 65.62%] [G loss: 3.023654]\n",
      "epoch:3 step:3399 [D loss: 0.609186, acc: 65.62%] [G loss: 2.715106]\n",
      "epoch:3 step:3400 [D loss: 0.579327, acc: 64.06%] [G loss: 3.016814]\n",
      "##############\n",
      "[3.19398927 2.18534455 6.88109905 5.51885061 4.55546929 6.31203127\n",
      " 5.62587158 5.34710161 5.58363135 4.13801916]\n",
      "##########\n",
      "epoch:3 step:3401 [D loss: 0.615102, acc: 68.75%] [G loss: 2.774916]\n",
      "epoch:3 step:3402 [D loss: 0.534976, acc: 74.22%] [G loss: 2.958776]\n",
      "epoch:3 step:3403 [D loss: 0.510161, acc: 75.00%] [G loss: 3.200576]\n",
      "epoch:3 step:3404 [D loss: 0.580633, acc: 70.31%] [G loss: 2.845793]\n",
      "epoch:3 step:3405 [D loss: 0.526971, acc: 72.66%] [G loss: 2.862023]\n",
      "epoch:3 step:3406 [D loss: 0.590708, acc: 71.09%] [G loss: 2.838039]\n",
      "epoch:3 step:3407 [D loss: 0.588024, acc: 61.72%] [G loss: 2.721457]\n",
      "epoch:3 step:3408 [D loss: 0.597067, acc: 72.66%] [G loss: 2.883864]\n",
      "epoch:3 step:3409 [D loss: 0.544549, acc: 76.56%] [G loss: 3.026755]\n",
      "epoch:3 step:3410 [D loss: 0.533139, acc: 69.53%] [G loss: 2.828607]\n",
      "epoch:3 step:3411 [D loss: 0.687505, acc: 64.06%] [G loss: 2.804580]\n",
      "epoch:3 step:3412 [D loss: 0.580819, acc: 68.75%] [G loss: 3.101888]\n",
      "epoch:3 step:3413 [D loss: 0.531205, acc: 74.22%] [G loss: 3.175246]\n",
      "epoch:3 step:3414 [D loss: 0.530644, acc: 70.31%] [G loss: 3.229704]\n",
      "epoch:3 step:3415 [D loss: 0.597732, acc: 69.53%] [G loss: 2.667818]\n",
      "epoch:3 step:3416 [D loss: 0.604230, acc: 63.28%] [G loss: 2.973834]\n",
      "epoch:3 step:3417 [D loss: 0.597281, acc: 70.31%] [G loss: 2.798158]\n",
      "epoch:3 step:3418 [D loss: 0.591939, acc: 67.19%] [G loss: 2.827758]\n",
      "epoch:3 step:3419 [D loss: 0.490491, acc: 81.25%] [G loss: 2.716827]\n",
      "epoch:3 step:3420 [D loss: 0.553897, acc: 71.88%] [G loss: 3.143383]\n",
      "epoch:3 step:3421 [D loss: 0.590702, acc: 70.31%] [G loss: 2.765326]\n",
      "epoch:3 step:3422 [D loss: 0.588474, acc: 74.22%] [G loss: 3.023037]\n",
      "epoch:3 step:3423 [D loss: 0.600776, acc: 70.31%] [G loss: 3.023983]\n",
      "epoch:3 step:3424 [D loss: 0.506726, acc: 70.31%] [G loss: 3.147281]\n",
      "epoch:3 step:3425 [D loss: 0.519075, acc: 76.56%] [G loss: 2.880307]\n",
      "epoch:3 step:3426 [D loss: 0.513120, acc: 75.78%] [G loss: 2.748766]\n",
      "epoch:3 step:3427 [D loss: 0.520730, acc: 71.09%] [G loss: 2.682812]\n",
      "epoch:3 step:3428 [D loss: 0.584404, acc: 71.09%] [G loss: 2.800832]\n",
      "epoch:3 step:3429 [D loss: 0.582966, acc: 71.09%] [G loss: 2.852242]\n",
      "epoch:3 step:3430 [D loss: 0.599272, acc: 73.44%] [G loss: 3.170061]\n",
      "epoch:3 step:3431 [D loss: 0.560746, acc: 71.88%] [G loss: 2.872998]\n",
      "epoch:3 step:3432 [D loss: 0.630565, acc: 67.97%] [G loss: 2.548807]\n",
      "epoch:3 step:3433 [D loss: 0.747181, acc: 59.38%] [G loss: 2.230204]\n",
      "epoch:3 step:3434 [D loss: 0.574157, acc: 70.31%] [G loss: 2.493491]\n",
      "epoch:3 step:3435 [D loss: 0.571648, acc: 75.00%] [G loss: 2.372353]\n",
      "epoch:3 step:3436 [D loss: 0.551686, acc: 69.53%] [G loss: 2.684883]\n",
      "epoch:3 step:3437 [D loss: 0.572771, acc: 71.88%] [G loss: 2.894433]\n",
      "epoch:3 step:3438 [D loss: 0.592400, acc: 65.62%] [G loss: 2.396216]\n",
      "epoch:3 step:3439 [D loss: 0.546542, acc: 73.44%] [G loss: 2.996722]\n",
      "epoch:3 step:3440 [D loss: 0.563819, acc: 68.75%] [G loss: 2.767638]\n",
      "epoch:3 step:3441 [D loss: 0.473142, acc: 75.00%] [G loss: 3.056074]\n",
      "epoch:3 step:3442 [D loss: 0.557661, acc: 72.66%] [G loss: 3.096201]\n",
      "epoch:3 step:3443 [D loss: 0.518037, acc: 71.09%] [G loss: 2.776127]\n",
      "epoch:3 step:3444 [D loss: 0.543359, acc: 69.53%] [G loss: 3.032907]\n",
      "epoch:3 step:3445 [D loss: 0.509276, acc: 75.00%] [G loss: 3.042870]\n",
      "epoch:3 step:3446 [D loss: 0.517442, acc: 71.88%] [G loss: 2.865118]\n",
      "epoch:3 step:3447 [D loss: 0.633680, acc: 64.84%] [G loss: 2.974797]\n",
      "epoch:3 step:3448 [D loss: 0.582020, acc: 66.41%] [G loss: 2.848652]\n",
      "epoch:3 step:3449 [D loss: 0.496801, acc: 78.12%] [G loss: 3.019146]\n",
      "epoch:3 step:3450 [D loss: 0.460965, acc: 76.56%] [G loss: 3.430978]\n",
      "epoch:3 step:3451 [D loss: 0.489382, acc: 76.56%] [G loss: 3.411986]\n",
      "epoch:3 step:3452 [D loss: 0.504030, acc: 80.47%] [G loss: 3.237631]\n",
      "epoch:3 step:3453 [D loss: 0.474218, acc: 77.34%] [G loss: 3.184391]\n",
      "epoch:3 step:3454 [D loss: 0.564681, acc: 72.66%] [G loss: 3.147303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3455 [D loss: 0.606779, acc: 68.75%] [G loss: 2.826112]\n",
      "epoch:3 step:3456 [D loss: 0.506472, acc: 75.00%] [G loss: 2.994835]\n",
      "epoch:3 step:3457 [D loss: 0.536454, acc: 71.88%] [G loss: 3.401535]\n",
      "epoch:3 step:3458 [D loss: 0.569756, acc: 70.31%] [G loss: 3.025183]\n",
      "epoch:3 step:3459 [D loss: 0.531235, acc: 74.22%] [G loss: 3.740618]\n",
      "epoch:3 step:3460 [D loss: 0.463576, acc: 79.69%] [G loss: 3.190431]\n",
      "epoch:3 step:3461 [D loss: 0.497998, acc: 75.78%] [G loss: 3.051929]\n",
      "epoch:3 step:3462 [D loss: 0.612814, acc: 71.09%] [G loss: 2.895986]\n",
      "epoch:3 step:3463 [D loss: 0.533986, acc: 71.88%] [G loss: 2.896486]\n",
      "epoch:3 step:3464 [D loss: 0.610638, acc: 71.88%] [G loss: 2.835158]\n",
      "epoch:3 step:3465 [D loss: 0.521952, acc: 74.22%] [G loss: 3.368584]\n",
      "epoch:3 step:3466 [D loss: 0.549109, acc: 71.88%] [G loss: 3.261419]\n",
      "epoch:3 step:3467 [D loss: 0.608399, acc: 71.88%] [G loss: 3.081017]\n",
      "epoch:3 step:3468 [D loss: 0.482425, acc: 78.91%] [G loss: 2.821377]\n",
      "epoch:3 step:3469 [D loss: 0.682881, acc: 62.50%] [G loss: 2.268602]\n",
      "epoch:3 step:3470 [D loss: 0.521716, acc: 75.00%] [G loss: 3.305315]\n",
      "epoch:3 step:3471 [D loss: 0.535082, acc: 71.09%] [G loss: 3.304987]\n",
      "epoch:3 step:3472 [D loss: 0.529240, acc: 75.78%] [G loss: 2.885779]\n",
      "epoch:3 step:3473 [D loss: 0.427500, acc: 80.47%] [G loss: 3.308953]\n",
      "epoch:3 step:3474 [D loss: 0.530008, acc: 71.09%] [G loss: 3.460258]\n",
      "epoch:3 step:3475 [D loss: 0.625538, acc: 67.19%] [G loss: 3.229891]\n",
      "epoch:3 step:3476 [D loss: 0.490241, acc: 78.12%] [G loss: 3.460802]\n",
      "epoch:3 step:3477 [D loss: 0.576840, acc: 70.31%] [G loss: 2.965708]\n",
      "epoch:3 step:3478 [D loss: 0.582444, acc: 71.88%] [G loss: 2.957743]\n",
      "epoch:3 step:3479 [D loss: 0.552282, acc: 68.75%] [G loss: 2.843205]\n",
      "epoch:3 step:3480 [D loss: 0.636654, acc: 62.50%] [G loss: 2.674473]\n",
      "epoch:3 step:3481 [D loss: 0.512974, acc: 69.53%] [G loss: 3.195128]\n",
      "epoch:3 step:3482 [D loss: 0.592935, acc: 69.53%] [G loss: 2.719057]\n",
      "epoch:3 step:3483 [D loss: 0.602274, acc: 71.88%] [G loss: 2.929287]\n",
      "epoch:3 step:3484 [D loss: 0.639246, acc: 60.94%] [G loss: 2.784974]\n",
      "epoch:3 step:3485 [D loss: 0.523385, acc: 75.78%] [G loss: 2.859758]\n",
      "epoch:3 step:3486 [D loss: 0.545806, acc: 72.66%] [G loss: 2.898480]\n",
      "epoch:3 step:3487 [D loss: 0.705480, acc: 62.50%] [G loss: 2.854851]\n",
      "epoch:3 step:3488 [D loss: 0.548085, acc: 71.09%] [G loss: 3.038516]\n",
      "epoch:3 step:3489 [D loss: 0.546375, acc: 71.09%] [G loss: 3.195148]\n",
      "epoch:3 step:3490 [D loss: 0.617275, acc: 65.62%] [G loss: 2.635524]\n",
      "epoch:3 step:3491 [D loss: 0.515596, acc: 78.91%] [G loss: 3.380881]\n",
      "epoch:3 step:3492 [D loss: 0.485985, acc: 75.78%] [G loss: 2.844020]\n",
      "epoch:3 step:3493 [D loss: 0.553068, acc: 70.31%] [G loss: 3.070212]\n",
      "epoch:3 step:3494 [D loss: 0.552059, acc: 72.66%] [G loss: 3.071590]\n",
      "epoch:3 step:3495 [D loss: 0.541969, acc: 77.34%] [G loss: 2.730915]\n",
      "epoch:3 step:3496 [D loss: 0.583647, acc: 66.41%] [G loss: 2.758205]\n",
      "epoch:3 step:3497 [D loss: 0.521172, acc: 73.44%] [G loss: 2.811431]\n",
      "epoch:3 step:3498 [D loss: 0.576911, acc: 71.09%] [G loss: 2.574384]\n",
      "epoch:3 step:3499 [D loss: 0.606836, acc: 67.97%] [G loss: 2.933022]\n",
      "epoch:3 step:3500 [D loss: 0.545222, acc: 75.78%] [G loss: 3.076087]\n",
      "epoch:3 step:3501 [D loss: 0.565197, acc: 71.09%] [G loss: 3.011420]\n",
      "epoch:3 step:3502 [D loss: 0.627501, acc: 71.09%] [G loss: 2.981379]\n",
      "epoch:3 step:3503 [D loss: 0.677294, acc: 67.97%] [G loss: 2.799812]\n",
      "epoch:3 step:3504 [D loss: 0.487578, acc: 77.34%] [G loss: 3.189420]\n",
      "epoch:3 step:3505 [D loss: 0.532063, acc: 71.09%] [G loss: 3.086647]\n",
      "epoch:3 step:3506 [D loss: 0.595454, acc: 71.09%] [G loss: 3.484370]\n",
      "epoch:3 step:3507 [D loss: 0.554735, acc: 71.88%] [G loss: 3.011338]\n",
      "epoch:3 step:3508 [D loss: 0.518130, acc: 71.09%] [G loss: 3.053347]\n",
      "epoch:3 step:3509 [D loss: 0.572385, acc: 69.53%] [G loss: 2.822088]\n",
      "epoch:3 step:3510 [D loss: 0.474764, acc: 73.44%] [G loss: 3.130196]\n",
      "epoch:3 step:3511 [D loss: 0.569896, acc: 71.09%] [G loss: 2.945273]\n",
      "epoch:3 step:3512 [D loss: 0.560339, acc: 71.09%] [G loss: 3.089406]\n",
      "epoch:3 step:3513 [D loss: 0.612076, acc: 64.06%] [G loss: 3.120677]\n",
      "epoch:3 step:3514 [D loss: 0.701753, acc: 61.72%] [G loss: 2.557629]\n",
      "epoch:3 step:3515 [D loss: 0.598112, acc: 65.62%] [G loss: 2.741652]\n",
      "epoch:3 step:3516 [D loss: 0.488312, acc: 82.03%] [G loss: 2.775238]\n",
      "epoch:3 step:3517 [D loss: 0.570299, acc: 67.97%] [G loss: 2.577424]\n",
      "epoch:3 step:3518 [D loss: 0.469141, acc: 76.56%] [G loss: 3.730085]\n",
      "epoch:3 step:3519 [D loss: 0.527981, acc: 71.88%] [G loss: 3.615397]\n",
      "epoch:3 step:3520 [D loss: 0.519609, acc: 71.88%] [G loss: 3.324771]\n",
      "epoch:3 step:3521 [D loss: 0.690351, acc: 60.94%] [G loss: 2.726179]\n",
      "epoch:3 step:3522 [D loss: 0.753167, acc: 57.03%] [G loss: 2.366636]\n",
      "epoch:3 step:3523 [D loss: 0.489907, acc: 79.69%] [G loss: 3.138764]\n",
      "epoch:3 step:3524 [D loss: 0.635279, acc: 71.88%] [G loss: 3.027239]\n",
      "epoch:3 step:3525 [D loss: 0.587716, acc: 66.41%] [G loss: 3.039847]\n",
      "epoch:3 step:3526 [D loss: 0.529654, acc: 74.22%] [G loss: 2.835871]\n",
      "epoch:3 step:3527 [D loss: 0.636032, acc: 60.16%] [G loss: 2.679096]\n",
      "epoch:3 step:3528 [D loss: 0.590075, acc: 71.09%] [G loss: 2.619183]\n",
      "epoch:3 step:3529 [D loss: 0.574414, acc: 68.75%] [G loss: 2.899211]\n",
      "epoch:3 step:3530 [D loss: 0.498484, acc: 77.34%] [G loss: 2.712876]\n",
      "epoch:3 step:3531 [D loss: 0.658152, acc: 58.59%] [G loss: 2.699014]\n",
      "epoch:3 step:3532 [D loss: 0.570072, acc: 64.84%] [G loss: 2.797686]\n",
      "epoch:3 step:3533 [D loss: 0.572022, acc: 71.88%] [G loss: 3.390232]\n",
      "epoch:3 step:3534 [D loss: 0.645670, acc: 63.28%] [G loss: 2.557059]\n",
      "epoch:3 step:3535 [D loss: 0.502729, acc: 75.00%] [G loss: 2.943400]\n",
      "epoch:3 step:3536 [D loss: 0.489542, acc: 76.56%] [G loss: 2.819512]\n",
      "epoch:3 step:3537 [D loss: 0.605340, acc: 68.75%] [G loss: 2.834412]\n",
      "epoch:3 step:3538 [D loss: 0.609544, acc: 67.19%] [G loss: 3.022526]\n",
      "epoch:3 step:3539 [D loss: 0.522817, acc: 73.44%] [G loss: 2.909465]\n",
      "epoch:3 step:3540 [D loss: 0.611520, acc: 64.06%] [G loss: 2.400403]\n",
      "epoch:3 step:3541 [D loss: 0.527533, acc: 70.31%] [G loss: 2.992036]\n",
      "epoch:3 step:3542 [D loss: 0.645639, acc: 63.28%] [G loss: 2.584510]\n",
      "epoch:3 step:3543 [D loss: 0.555655, acc: 71.88%] [G loss: 3.014636]\n",
      "epoch:3 step:3544 [D loss: 0.541411, acc: 68.75%] [G loss: 3.077445]\n",
      "epoch:3 step:3545 [D loss: 0.494417, acc: 79.69%] [G loss: 2.853451]\n",
      "epoch:3 step:3546 [D loss: 0.627060, acc: 61.72%] [G loss: 2.803915]\n",
      "epoch:3 step:3547 [D loss: 0.474058, acc: 81.25%] [G loss: 3.338577]\n",
      "epoch:3 step:3548 [D loss: 0.538436, acc: 78.12%] [G loss: 2.575002]\n",
      "epoch:3 step:3549 [D loss: 0.604956, acc: 62.50%] [G loss: 2.581571]\n",
      "epoch:3 step:3550 [D loss: 0.543054, acc: 70.31%] [G loss: 2.659537]\n",
      "epoch:3 step:3551 [D loss: 0.600673, acc: 65.62%] [G loss: 2.846559]\n",
      "epoch:3 step:3552 [D loss: 0.611555, acc: 68.75%] [G loss: 2.902640]\n",
      "epoch:3 step:3553 [D loss: 0.615729, acc: 65.62%] [G loss: 2.607361]\n",
      "epoch:3 step:3554 [D loss: 0.509631, acc: 76.56%] [G loss: 2.980885]\n",
      "epoch:3 step:3555 [D loss: 0.536504, acc: 73.44%] [G loss: 2.754270]\n",
      "epoch:3 step:3556 [D loss: 0.571307, acc: 71.09%] [G loss: 2.564012]\n",
      "epoch:3 step:3557 [D loss: 0.442430, acc: 83.59%] [G loss: 3.015972]\n",
      "epoch:3 step:3558 [D loss: 0.460175, acc: 84.38%] [G loss: 3.201464]\n",
      "epoch:3 step:3559 [D loss: 0.540340, acc: 71.88%] [G loss: 2.806637]\n",
      "epoch:3 step:3560 [D loss: 0.589521, acc: 67.97%] [G loss: 2.879720]\n",
      "epoch:3 step:3561 [D loss: 0.574665, acc: 66.41%] [G loss: 2.912834]\n",
      "epoch:3 step:3562 [D loss: 0.569219, acc: 67.19%] [G loss: 2.704057]\n",
      "epoch:3 step:3563 [D loss: 0.547678, acc: 68.75%] [G loss: 2.854838]\n",
      "epoch:3 step:3564 [D loss: 0.466519, acc: 81.25%] [G loss: 2.762542]\n",
      "epoch:3 step:3565 [D loss: 0.558833, acc: 75.78%] [G loss: 2.831701]\n",
      "epoch:3 step:3566 [D loss: 0.552530, acc: 71.09%] [G loss: 3.061375]\n",
      "epoch:3 step:3567 [D loss: 0.516442, acc: 76.56%] [G loss: 2.764103]\n",
      "epoch:3 step:3568 [D loss: 0.487603, acc: 79.69%] [G loss: 3.155910]\n",
      "epoch:3 step:3569 [D loss: 0.537000, acc: 71.88%] [G loss: 2.681266]\n",
      "epoch:3 step:3570 [D loss: 0.557482, acc: 67.19%] [G loss: 2.999555]\n",
      "epoch:3 step:3571 [D loss: 0.507778, acc: 77.34%] [G loss: 3.031069]\n",
      "epoch:3 step:3572 [D loss: 0.612218, acc: 67.97%] [G loss: 2.726094]\n",
      "epoch:3 step:3573 [D loss: 0.536594, acc: 74.22%] [G loss: 2.654456]\n",
      "epoch:3 step:3574 [D loss: 0.494679, acc: 73.44%] [G loss: 2.961241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3575 [D loss: 0.629442, acc: 67.97%] [G loss: 3.346871]\n",
      "epoch:3 step:3576 [D loss: 0.663096, acc: 65.62%] [G loss: 2.759125]\n",
      "epoch:3 step:3577 [D loss: 0.519827, acc: 70.31%] [G loss: 2.601219]\n",
      "epoch:3 step:3578 [D loss: 0.518660, acc: 74.22%] [G loss: 2.748913]\n",
      "epoch:3 step:3579 [D loss: 0.630208, acc: 62.50%] [G loss: 3.159067]\n",
      "epoch:3 step:3580 [D loss: 0.508070, acc: 74.22%] [G loss: 3.142934]\n",
      "epoch:3 step:3581 [D loss: 0.517433, acc: 70.31%] [G loss: 2.724733]\n",
      "epoch:3 step:3582 [D loss: 0.556913, acc: 75.00%] [G loss: 2.774293]\n",
      "epoch:3 step:3583 [D loss: 0.494497, acc: 75.00%] [G loss: 3.249430]\n",
      "epoch:3 step:3584 [D loss: 0.641551, acc: 63.28%] [G loss: 2.956521]\n",
      "epoch:3 step:3585 [D loss: 0.504547, acc: 73.44%] [G loss: 3.356995]\n",
      "epoch:3 step:3586 [D loss: 0.548088, acc: 70.31%] [G loss: 3.665563]\n",
      "epoch:3 step:3587 [D loss: 0.607497, acc: 66.41%] [G loss: 3.031408]\n",
      "epoch:3 step:3588 [D loss: 0.496692, acc: 75.78%] [G loss: 3.071409]\n",
      "epoch:3 step:3589 [D loss: 0.634690, acc: 66.41%] [G loss: 2.631326]\n",
      "epoch:3 step:3590 [D loss: 0.558138, acc: 72.66%] [G loss: 3.199069]\n",
      "epoch:3 step:3591 [D loss: 0.588509, acc: 67.97%] [G loss: 3.367555]\n",
      "epoch:3 step:3592 [D loss: 0.537667, acc: 70.31%] [G loss: 3.283112]\n",
      "epoch:3 step:3593 [D loss: 0.600815, acc: 71.09%] [G loss: 2.798918]\n",
      "epoch:3 step:3594 [D loss: 0.610794, acc: 66.41%] [G loss: 3.247751]\n",
      "epoch:3 step:3595 [D loss: 0.530840, acc: 71.09%] [G loss: 2.923611]\n",
      "epoch:3 step:3596 [D loss: 0.678533, acc: 59.38%] [G loss: 3.104650]\n",
      "epoch:3 step:3597 [D loss: 0.541512, acc: 69.53%] [G loss: 3.072022]\n",
      "epoch:3 step:3598 [D loss: 0.592975, acc: 66.41%] [G loss: 2.832684]\n",
      "epoch:3 step:3599 [D loss: 0.663939, acc: 64.06%] [G loss: 2.689435]\n",
      "epoch:3 step:3600 [D loss: 0.574648, acc: 72.66%] [G loss: 2.694952]\n",
      "##############\n",
      "[2.99881858 1.9451475  6.89843705 5.46884472 4.22324923 6.1614186\n",
      " 5.19247421 5.19704146 5.69999392 4.01875564]\n",
      "##########\n",
      "epoch:3 step:3601 [D loss: 0.572748, acc: 70.31%] [G loss: 2.732981]\n",
      "epoch:3 step:3602 [D loss: 0.558797, acc: 69.53%] [G loss: 3.036312]\n",
      "epoch:3 step:3603 [D loss: 0.528933, acc: 68.75%] [G loss: 3.355177]\n",
      "epoch:3 step:3604 [D loss: 0.593000, acc: 67.19%] [G loss: 2.848783]\n",
      "epoch:3 step:3605 [D loss: 0.599655, acc: 71.88%] [G loss: 2.601783]\n",
      "epoch:3 step:3606 [D loss: 0.551161, acc: 69.53%] [G loss: 3.106097]\n",
      "epoch:3 step:3607 [D loss: 0.503343, acc: 74.22%] [G loss: 2.955360]\n",
      "epoch:3 step:3608 [D loss: 0.648331, acc: 66.41%] [G loss: 2.637373]\n",
      "epoch:3 step:3609 [D loss: 0.586757, acc: 64.06%] [G loss: 3.148704]\n",
      "epoch:3 step:3610 [D loss: 0.572463, acc: 71.09%] [G loss: 2.640637]\n",
      "epoch:3 step:3611 [D loss: 0.632967, acc: 68.75%] [G loss: 3.093063]\n",
      "epoch:3 step:3612 [D loss: 0.529855, acc: 75.00%] [G loss: 3.206256]\n",
      "epoch:3 step:3613 [D loss: 0.537935, acc: 75.78%] [G loss: 3.200575]\n",
      "epoch:3 step:3614 [D loss: 0.532897, acc: 76.56%] [G loss: 3.001732]\n",
      "epoch:3 step:3615 [D loss: 0.550393, acc: 73.44%] [G loss: 2.902985]\n",
      "epoch:3 step:3616 [D loss: 0.505857, acc: 76.56%] [G loss: 3.381760]\n",
      "epoch:3 step:3617 [D loss: 0.551604, acc: 75.00%] [G loss: 3.180757]\n",
      "epoch:3 step:3618 [D loss: 0.562164, acc: 78.91%] [G loss: 3.029738]\n",
      "epoch:3 step:3619 [D loss: 0.655089, acc: 67.19%] [G loss: 2.702523]\n",
      "epoch:3 step:3620 [D loss: 0.643302, acc: 64.84%] [G loss: 2.668975]\n",
      "epoch:3 step:3621 [D loss: 0.518275, acc: 75.78%] [G loss: 2.993736]\n",
      "epoch:3 step:3622 [D loss: 0.598716, acc: 68.75%] [G loss: 2.984147]\n",
      "epoch:3 step:3623 [D loss: 0.581927, acc: 73.44%] [G loss: 2.900046]\n",
      "epoch:3 step:3624 [D loss: 0.590559, acc: 75.78%] [G loss: 2.635673]\n",
      "epoch:3 step:3625 [D loss: 0.523520, acc: 75.78%] [G loss: 3.180659]\n",
      "epoch:3 step:3626 [D loss: 0.533519, acc: 71.88%] [G loss: 3.280938]\n",
      "epoch:3 step:3627 [D loss: 0.733589, acc: 60.16%] [G loss: 3.066274]\n",
      "epoch:3 step:3628 [D loss: 0.603947, acc: 63.28%] [G loss: 2.730927]\n",
      "epoch:3 step:3629 [D loss: 0.569975, acc: 72.66%] [G loss: 2.351977]\n",
      "epoch:3 step:3630 [D loss: 0.586358, acc: 74.22%] [G loss: 2.973142]\n",
      "epoch:3 step:3631 [D loss: 0.661615, acc: 59.38%] [G loss: 2.714587]\n",
      "epoch:3 step:3632 [D loss: 0.547024, acc: 71.09%] [G loss: 2.897290]\n",
      "epoch:3 step:3633 [D loss: 0.474683, acc: 79.69%] [G loss: 2.983356]\n",
      "epoch:3 step:3634 [D loss: 0.555941, acc: 72.66%] [G loss: 2.924907]\n",
      "epoch:3 step:3635 [D loss: 0.624303, acc: 65.62%] [G loss: 2.588796]\n",
      "epoch:3 step:3636 [D loss: 0.532599, acc: 69.53%] [G loss: 2.848459]\n",
      "epoch:3 step:3637 [D loss: 0.584843, acc: 74.22%] [G loss: 2.821356]\n",
      "epoch:3 step:3638 [D loss: 0.682938, acc: 59.38%] [G loss: 2.507787]\n",
      "epoch:3 step:3639 [D loss: 0.596003, acc: 64.84%] [G loss: 2.731865]\n",
      "epoch:3 step:3640 [D loss: 0.569154, acc: 69.53%] [G loss: 2.734961]\n",
      "epoch:3 step:3641 [D loss: 0.518232, acc: 75.00%] [G loss: 2.748226]\n",
      "epoch:3 step:3642 [D loss: 0.606683, acc: 67.19%] [G loss: 3.108094]\n",
      "epoch:3 step:3643 [D loss: 0.530771, acc: 72.66%] [G loss: 2.764074]\n",
      "epoch:3 step:3644 [D loss: 0.556521, acc: 75.78%] [G loss: 2.882765]\n",
      "epoch:3 step:3645 [D loss: 0.595626, acc: 71.88%] [G loss: 3.055501]\n",
      "epoch:3 step:3646 [D loss: 0.578571, acc: 71.88%] [G loss: 2.744603]\n",
      "epoch:3 step:3647 [D loss: 0.603259, acc: 70.31%] [G loss: 2.631285]\n",
      "epoch:3 step:3648 [D loss: 0.531943, acc: 75.00%] [G loss: 3.006431]\n",
      "epoch:3 step:3649 [D loss: 0.607785, acc: 67.19%] [G loss: 2.883781]\n",
      "epoch:3 step:3650 [D loss: 0.598688, acc: 68.75%] [G loss: 2.730434]\n",
      "epoch:3 step:3651 [D loss: 0.606233, acc: 68.75%] [G loss: 2.631695]\n",
      "epoch:3 step:3652 [D loss: 0.478124, acc: 74.22%] [G loss: 2.922727]\n",
      "epoch:3 step:3653 [D loss: 0.556101, acc: 72.66%] [G loss: 2.890570]\n",
      "epoch:3 step:3654 [D loss: 0.609342, acc: 67.19%] [G loss: 2.612660]\n",
      "epoch:3 step:3655 [D loss: 0.624452, acc: 63.28%] [G loss: 2.419102]\n",
      "epoch:3 step:3656 [D loss: 0.621573, acc: 62.50%] [G loss: 3.001750]\n",
      "epoch:3 step:3657 [D loss: 0.602679, acc: 70.31%] [G loss: 2.445060]\n",
      "epoch:3 step:3658 [D loss: 0.524405, acc: 75.00%] [G loss: 2.687861]\n",
      "epoch:3 step:3659 [D loss: 0.549907, acc: 70.31%] [G loss: 3.108035]\n",
      "epoch:3 step:3660 [D loss: 0.659619, acc: 62.50%] [G loss: 2.490086]\n",
      "epoch:3 step:3661 [D loss: 0.587595, acc: 66.41%] [G loss: 2.950954]\n",
      "epoch:3 step:3662 [D loss: 0.605807, acc: 61.72%] [G loss: 2.639824]\n",
      "epoch:3 step:3663 [D loss: 0.584757, acc: 71.88%] [G loss: 2.954868]\n",
      "epoch:3 step:3664 [D loss: 0.531044, acc: 73.44%] [G loss: 3.274994]\n",
      "epoch:3 step:3665 [D loss: 0.474115, acc: 81.25%] [G loss: 3.124986]\n",
      "epoch:3 step:3666 [D loss: 0.650167, acc: 63.28%] [G loss: 2.880100]\n",
      "epoch:3 step:3667 [D loss: 0.563095, acc: 70.31%] [G loss: 3.349789]\n",
      "epoch:3 step:3668 [D loss: 0.590339, acc: 65.62%] [G loss: 2.918828]\n",
      "epoch:3 step:3669 [D loss: 0.676690, acc: 64.84%] [G loss: 2.901883]\n",
      "epoch:3 step:3670 [D loss: 0.562732, acc: 67.19%] [G loss: 2.867720]\n",
      "epoch:3 step:3671 [D loss: 0.546655, acc: 71.09%] [G loss: 3.003206]\n",
      "epoch:3 step:3672 [D loss: 0.597944, acc: 65.62%] [G loss: 2.750761]\n",
      "epoch:3 step:3673 [D loss: 0.626323, acc: 61.72%] [G loss: 2.432312]\n",
      "epoch:3 step:3674 [D loss: 0.479831, acc: 79.69%] [G loss: 2.750580]\n",
      "epoch:3 step:3675 [D loss: 0.516349, acc: 69.53%] [G loss: 2.479198]\n",
      "epoch:3 step:3676 [D loss: 0.501404, acc: 73.44%] [G loss: 2.982158]\n",
      "epoch:3 step:3677 [D loss: 0.554413, acc: 71.88%] [G loss: 2.996317]\n",
      "epoch:3 step:3678 [D loss: 0.585443, acc: 67.19%] [G loss: 2.562573]\n",
      "epoch:3 step:3679 [D loss: 0.570663, acc: 70.31%] [G loss: 2.828898]\n",
      "epoch:3 step:3680 [D loss: 0.512199, acc: 71.88%] [G loss: 3.133507]\n",
      "epoch:3 step:3681 [D loss: 0.573179, acc: 71.09%] [G loss: 2.767595]\n",
      "epoch:3 step:3682 [D loss: 0.559678, acc: 74.22%] [G loss: 2.980880]\n",
      "epoch:3 step:3683 [D loss: 0.471333, acc: 74.22%] [G loss: 2.954573]\n",
      "epoch:3 step:3684 [D loss: 0.646401, acc: 67.19%] [G loss: 2.487476]\n",
      "epoch:3 step:3685 [D loss: 0.550425, acc: 69.53%] [G loss: 2.785555]\n",
      "epoch:3 step:3686 [D loss: 0.506578, acc: 74.22%] [G loss: 3.141220]\n",
      "epoch:3 step:3687 [D loss: 0.666214, acc: 66.41%] [G loss: 2.612109]\n",
      "epoch:3 step:3688 [D loss: 0.479164, acc: 75.00%] [G loss: 3.033467]\n",
      "epoch:3 step:3689 [D loss: 0.633595, acc: 64.06%] [G loss: 2.786051]\n",
      "epoch:3 step:3690 [D loss: 0.513069, acc: 73.44%] [G loss: 2.919005]\n",
      "epoch:3 step:3691 [D loss: 0.556688, acc: 68.75%] [G loss: 2.847637]\n",
      "epoch:3 step:3692 [D loss: 0.487339, acc: 73.44%] [G loss: 2.823877]\n",
      "epoch:3 step:3693 [D loss: 0.588254, acc: 65.62%] [G loss: 2.626809]\n",
      "epoch:3 step:3694 [D loss: 0.548283, acc: 69.53%] [G loss: 2.981343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3695 [D loss: 0.548853, acc: 71.88%] [G loss: 2.918338]\n",
      "epoch:3 step:3696 [D loss: 0.485295, acc: 75.78%] [G loss: 2.991697]\n",
      "epoch:3 step:3697 [D loss: 0.496125, acc: 77.34%] [G loss: 3.281117]\n",
      "epoch:3 step:3698 [D loss: 0.544731, acc: 73.44%] [G loss: 3.343088]\n",
      "epoch:3 step:3699 [D loss: 0.497906, acc: 71.88%] [G loss: 3.505960]\n",
      "epoch:3 step:3700 [D loss: 0.537352, acc: 74.22%] [G loss: 3.133885]\n",
      "epoch:3 step:3701 [D loss: 0.497612, acc: 76.56%] [G loss: 3.567826]\n",
      "epoch:3 step:3702 [D loss: 0.640686, acc: 65.62%] [G loss: 3.015803]\n",
      "epoch:3 step:3703 [D loss: 0.593766, acc: 69.53%] [G loss: 2.888732]\n",
      "epoch:3 step:3704 [D loss: 0.574609, acc: 71.09%] [G loss: 2.894954]\n",
      "epoch:3 step:3705 [D loss: 0.472031, acc: 81.25%] [G loss: 2.903103]\n",
      "epoch:3 step:3706 [D loss: 0.579317, acc: 67.19%] [G loss: 2.755074]\n",
      "epoch:3 step:3707 [D loss: 0.625803, acc: 65.62%] [G loss: 2.906274]\n",
      "epoch:3 step:3708 [D loss: 0.585925, acc: 67.19%] [G loss: 2.554608]\n",
      "epoch:3 step:3709 [D loss: 0.482945, acc: 82.03%] [G loss: 3.831361]\n",
      "epoch:3 step:3710 [D loss: 0.530224, acc: 71.09%] [G loss: 2.842794]\n",
      "epoch:3 step:3711 [D loss: 0.625276, acc: 68.75%] [G loss: 3.036190]\n",
      "epoch:3 step:3712 [D loss: 0.564212, acc: 73.44%] [G loss: 2.804575]\n",
      "epoch:3 step:3713 [D loss: 0.578705, acc: 71.88%] [G loss: 2.915392]\n",
      "epoch:3 step:3714 [D loss: 0.577186, acc: 65.62%] [G loss: 2.935390]\n",
      "epoch:3 step:3715 [D loss: 0.520110, acc: 75.00%] [G loss: 3.231595]\n",
      "epoch:3 step:3716 [D loss: 0.578164, acc: 71.88%] [G loss: 2.931217]\n",
      "epoch:3 step:3717 [D loss: 0.638718, acc: 67.97%] [G loss: 3.324266]\n",
      "epoch:3 step:3718 [D loss: 0.573691, acc: 73.44%] [G loss: 2.810490]\n",
      "epoch:3 step:3719 [D loss: 0.547446, acc: 71.88%] [G loss: 2.956819]\n",
      "epoch:3 step:3720 [D loss: 0.520309, acc: 74.22%] [G loss: 3.158916]\n",
      "epoch:3 step:3721 [D loss: 0.482542, acc: 75.00%] [G loss: 3.490342]\n",
      "epoch:3 step:3722 [D loss: 0.545075, acc: 75.78%] [G loss: 3.025587]\n",
      "epoch:3 step:3723 [D loss: 0.467606, acc: 78.12%] [G loss: 3.534774]\n",
      "epoch:3 step:3724 [D loss: 0.609254, acc: 64.84%] [G loss: 2.943933]\n",
      "epoch:3 step:3725 [D loss: 0.592770, acc: 67.97%] [G loss: 2.721140]\n",
      "epoch:3 step:3726 [D loss: 0.582642, acc: 71.09%] [G loss: 2.832901]\n",
      "epoch:3 step:3727 [D loss: 0.583830, acc: 68.75%] [G loss: 2.886747]\n",
      "epoch:3 step:3728 [D loss: 0.617755, acc: 67.19%] [G loss: 2.962699]\n",
      "epoch:3 step:3729 [D loss: 0.607150, acc: 64.84%] [G loss: 2.698242]\n",
      "epoch:3 step:3730 [D loss: 0.562626, acc: 68.75%] [G loss: 3.206395]\n",
      "epoch:3 step:3731 [D loss: 0.713546, acc: 61.72%] [G loss: 3.111434]\n",
      "epoch:3 step:3732 [D loss: 0.535128, acc: 70.31%] [G loss: 3.003048]\n",
      "epoch:3 step:3733 [D loss: 0.574683, acc: 70.31%] [G loss: 2.614667]\n",
      "epoch:3 step:3734 [D loss: 0.486897, acc: 72.66%] [G loss: 3.068935]\n",
      "epoch:3 step:3735 [D loss: 0.569211, acc: 65.62%] [G loss: 2.954310]\n",
      "epoch:3 step:3736 [D loss: 0.582318, acc: 70.31%] [G loss: 3.264238]\n",
      "epoch:3 step:3737 [D loss: 0.474492, acc: 79.69%] [G loss: 3.218809]\n",
      "epoch:3 step:3738 [D loss: 0.550072, acc: 70.31%] [G loss: 2.865969]\n",
      "epoch:3 step:3739 [D loss: 0.748515, acc: 62.50%] [G loss: 2.831254]\n",
      "epoch:3 step:3740 [D loss: 0.571206, acc: 71.88%] [G loss: 3.202834]\n",
      "epoch:3 step:3741 [D loss: 0.550846, acc: 75.78%] [G loss: 3.565254]\n",
      "epoch:3 step:3742 [D loss: 0.637517, acc: 65.62%] [G loss: 3.342846]\n",
      "epoch:3 step:3743 [D loss: 0.578137, acc: 68.75%] [G loss: 2.915409]\n",
      "epoch:3 step:3744 [D loss: 0.566326, acc: 71.09%] [G loss: 2.806623]\n",
      "epoch:3 step:3745 [D loss: 0.570476, acc: 74.22%] [G loss: 3.065314]\n",
      "epoch:3 step:3746 [D loss: 0.543914, acc: 73.44%] [G loss: 3.203000]\n",
      "epoch:3 step:3747 [D loss: 0.473256, acc: 78.12%] [G loss: 3.591340]\n",
      "epoch:3 step:3748 [D loss: 0.442242, acc: 78.12%] [G loss: 3.890758]\n",
      "epoch:4 step:3749 [D loss: 0.700635, acc: 60.94%] [G loss: 2.949686]\n",
      "epoch:4 step:3750 [D loss: 0.517211, acc: 73.44%] [G loss: 2.919260]\n",
      "epoch:4 step:3751 [D loss: 0.610695, acc: 62.50%] [G loss: 2.852153]\n",
      "epoch:4 step:3752 [D loss: 0.562117, acc: 70.31%] [G loss: 3.114681]\n",
      "epoch:4 step:3753 [D loss: 0.571819, acc: 68.75%] [G loss: 2.832335]\n",
      "epoch:4 step:3754 [D loss: 0.577672, acc: 68.75%] [G loss: 3.029322]\n",
      "epoch:4 step:3755 [D loss: 0.548353, acc: 71.09%] [G loss: 3.042704]\n",
      "epoch:4 step:3756 [D loss: 0.558154, acc: 68.75%] [G loss: 2.972715]\n",
      "epoch:4 step:3757 [D loss: 0.523347, acc: 75.78%] [G loss: 2.755391]\n",
      "epoch:4 step:3758 [D loss: 0.543768, acc: 70.31%] [G loss: 2.842571]\n",
      "epoch:4 step:3759 [D loss: 0.581351, acc: 64.84%] [G loss: 3.040853]\n",
      "epoch:4 step:3760 [D loss: 0.582790, acc: 67.19%] [G loss: 2.737156]\n",
      "epoch:4 step:3761 [D loss: 0.523443, acc: 80.47%] [G loss: 2.816587]\n",
      "epoch:4 step:3762 [D loss: 0.621752, acc: 67.19%] [G loss: 2.643639]\n",
      "epoch:4 step:3763 [D loss: 0.497329, acc: 75.78%] [G loss: 3.207448]\n",
      "epoch:4 step:3764 [D loss: 0.405748, acc: 84.38%] [G loss: 3.086169]\n",
      "epoch:4 step:3765 [D loss: 0.598314, acc: 66.41%] [G loss: 2.753087]\n",
      "epoch:4 step:3766 [D loss: 0.604656, acc: 66.41%] [G loss: 2.766181]\n",
      "epoch:4 step:3767 [D loss: 0.605730, acc: 67.19%] [G loss: 3.023060]\n",
      "epoch:4 step:3768 [D loss: 0.680212, acc: 60.94%] [G loss: 2.626314]\n",
      "epoch:4 step:3769 [D loss: 0.584727, acc: 71.88%] [G loss: 2.697546]\n",
      "epoch:4 step:3770 [D loss: 0.636543, acc: 68.75%] [G loss: 2.726931]\n",
      "epoch:4 step:3771 [D loss: 0.679579, acc: 60.16%] [G loss: 2.968520]\n",
      "epoch:4 step:3772 [D loss: 0.513290, acc: 76.56%] [G loss: 3.084850]\n",
      "epoch:4 step:3773 [D loss: 0.529551, acc: 77.34%] [G loss: 2.711263]\n",
      "epoch:4 step:3774 [D loss: 0.580765, acc: 64.06%] [G loss: 2.644554]\n",
      "epoch:4 step:3775 [D loss: 0.603046, acc: 67.19%] [G loss: 2.838630]\n",
      "epoch:4 step:3776 [D loss: 0.523738, acc: 72.66%] [G loss: 2.842700]\n",
      "epoch:4 step:3777 [D loss: 0.522184, acc: 71.09%] [G loss: 2.912043]\n",
      "epoch:4 step:3778 [D loss: 0.534428, acc: 77.34%] [G loss: 2.720741]\n",
      "epoch:4 step:3779 [D loss: 0.576654, acc: 68.75%] [G loss: 2.499627]\n",
      "epoch:4 step:3780 [D loss: 0.633392, acc: 61.72%] [G loss: 2.719046]\n",
      "epoch:4 step:3781 [D loss: 0.548012, acc: 71.88%] [G loss: 2.882057]\n",
      "epoch:4 step:3782 [D loss: 0.591560, acc: 68.75%] [G loss: 2.796768]\n",
      "epoch:4 step:3783 [D loss: 0.578692, acc: 68.75%] [G loss: 3.002633]\n",
      "epoch:4 step:3784 [D loss: 0.576030, acc: 68.75%] [G loss: 3.082757]\n",
      "epoch:4 step:3785 [D loss: 0.524929, acc: 75.00%] [G loss: 2.863581]\n",
      "epoch:4 step:3786 [D loss: 0.612650, acc: 67.19%] [G loss: 2.860070]\n",
      "epoch:4 step:3787 [D loss: 0.509330, acc: 75.78%] [G loss: 3.104387]\n",
      "epoch:4 step:3788 [D loss: 0.494252, acc: 80.47%] [G loss: 3.279350]\n",
      "epoch:4 step:3789 [D loss: 0.524708, acc: 72.66%] [G loss: 2.863454]\n",
      "epoch:4 step:3790 [D loss: 0.497481, acc: 75.00%] [G loss: 3.062048]\n",
      "epoch:4 step:3791 [D loss: 0.518260, acc: 76.56%] [G loss: 3.121295]\n",
      "epoch:4 step:3792 [D loss: 0.633488, acc: 64.84%] [G loss: 2.709082]\n",
      "epoch:4 step:3793 [D loss: 0.577565, acc: 70.31%] [G loss: 2.745045]\n",
      "epoch:4 step:3794 [D loss: 0.610958, acc: 66.41%] [G loss: 2.756876]\n",
      "epoch:4 step:3795 [D loss: 0.531435, acc: 68.75%] [G loss: 2.977235]\n",
      "epoch:4 step:3796 [D loss: 0.554517, acc: 67.97%] [G loss: 3.011719]\n",
      "epoch:4 step:3797 [D loss: 0.563722, acc: 75.78%] [G loss: 2.817456]\n",
      "epoch:4 step:3798 [D loss: 0.524954, acc: 73.44%] [G loss: 3.078264]\n",
      "epoch:4 step:3799 [D loss: 0.574760, acc: 69.53%] [G loss: 2.915616]\n",
      "epoch:4 step:3800 [D loss: 0.530827, acc: 72.66%] [G loss: 3.108238]\n",
      "##############\n",
      "[3.05558333 1.6861201  6.84951239 5.42741026 4.12526632 6.05540087\n",
      " 5.31905868 5.15417598 5.56393224 3.7261095 ]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.709619, acc: 66.41%] [G loss: 2.995263]\n",
      "epoch:4 step:3802 [D loss: 0.510668, acc: 77.34%] [G loss: 2.705928]\n",
      "epoch:4 step:3803 [D loss: 0.676802, acc: 64.84%] [G loss: 3.009740]\n",
      "epoch:4 step:3804 [D loss: 0.566389, acc: 71.09%] [G loss: 2.721029]\n",
      "epoch:4 step:3805 [D loss: 0.544239, acc: 73.44%] [G loss: 2.726122]\n",
      "epoch:4 step:3806 [D loss: 0.567606, acc: 70.31%] [G loss: 2.729243]\n",
      "epoch:4 step:3807 [D loss: 0.543117, acc: 72.66%] [G loss: 3.011157]\n",
      "epoch:4 step:3808 [D loss: 0.539410, acc: 69.53%] [G loss: 2.973837]\n",
      "epoch:4 step:3809 [D loss: 0.525218, acc: 73.44%] [G loss: 3.350448]\n",
      "epoch:4 step:3810 [D loss: 0.670851, acc: 62.50%] [G loss: 2.709831]\n",
      "epoch:4 step:3811 [D loss: 0.582786, acc: 71.88%] [G loss: 2.734063]\n",
      "epoch:4 step:3812 [D loss: 0.619263, acc: 67.19%] [G loss: 2.823713]\n",
      "epoch:4 step:3813 [D loss: 0.601613, acc: 71.88%] [G loss: 2.459054]\n",
      "epoch:4 step:3814 [D loss: 0.599145, acc: 74.22%] [G loss: 2.620379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3815 [D loss: 0.500710, acc: 75.00%] [G loss: 2.617779]\n",
      "epoch:4 step:3816 [D loss: 0.521130, acc: 74.22%] [G loss: 2.953894]\n",
      "epoch:4 step:3817 [D loss: 0.612033, acc: 67.19%] [G loss: 3.086939]\n",
      "epoch:4 step:3818 [D loss: 0.471842, acc: 76.56%] [G loss: 3.206718]\n",
      "epoch:4 step:3819 [D loss: 0.553420, acc: 72.66%] [G loss: 3.092609]\n",
      "epoch:4 step:3820 [D loss: 0.583187, acc: 68.75%] [G loss: 3.144812]\n",
      "epoch:4 step:3821 [D loss: 0.513331, acc: 73.44%] [G loss: 2.728014]\n",
      "epoch:4 step:3822 [D loss: 0.648809, acc: 65.62%] [G loss: 2.872654]\n",
      "epoch:4 step:3823 [D loss: 0.447031, acc: 81.25%] [G loss: 3.426645]\n",
      "epoch:4 step:3824 [D loss: 0.539966, acc: 69.53%] [G loss: 3.156816]\n",
      "epoch:4 step:3825 [D loss: 0.476759, acc: 77.34%] [G loss: 3.492375]\n",
      "epoch:4 step:3826 [D loss: 0.613679, acc: 64.84%] [G loss: 2.967948]\n",
      "epoch:4 step:3827 [D loss: 0.562766, acc: 74.22%] [G loss: 3.023835]\n",
      "epoch:4 step:3828 [D loss: 0.600071, acc: 75.00%] [G loss: 2.453360]\n",
      "epoch:4 step:3829 [D loss: 0.613040, acc: 68.75%] [G loss: 2.720490]\n",
      "epoch:4 step:3830 [D loss: 0.508206, acc: 75.00%] [G loss: 3.226367]\n",
      "epoch:4 step:3831 [D loss: 0.575785, acc: 71.09%] [G loss: 2.914619]\n",
      "epoch:4 step:3832 [D loss: 0.598777, acc: 68.75%] [G loss: 2.820215]\n",
      "epoch:4 step:3833 [D loss: 0.595213, acc: 67.19%] [G loss: 3.095295]\n",
      "epoch:4 step:3834 [D loss: 0.659836, acc: 70.31%] [G loss: 2.601720]\n",
      "epoch:4 step:3835 [D loss: 0.558184, acc: 72.66%] [G loss: 2.910309]\n",
      "epoch:4 step:3836 [D loss: 0.551983, acc: 74.22%] [G loss: 2.839663]\n",
      "epoch:4 step:3837 [D loss: 0.554901, acc: 71.09%] [G loss: 2.872291]\n",
      "epoch:4 step:3838 [D loss: 0.609936, acc: 68.75%] [G loss: 2.853918]\n",
      "epoch:4 step:3839 [D loss: 0.575766, acc: 75.00%] [G loss: 3.062332]\n",
      "epoch:4 step:3840 [D loss: 0.621017, acc: 70.31%] [G loss: 3.074529]\n",
      "epoch:4 step:3841 [D loss: 0.577512, acc: 70.31%] [G loss: 2.773942]\n",
      "epoch:4 step:3842 [D loss: 0.645689, acc: 64.06%] [G loss: 2.739789]\n",
      "epoch:4 step:3843 [D loss: 0.608343, acc: 71.88%] [G loss: 2.746585]\n",
      "epoch:4 step:3844 [D loss: 0.513059, acc: 77.34%] [G loss: 2.939857]\n",
      "epoch:4 step:3845 [D loss: 0.541863, acc: 72.66%] [G loss: 2.616266]\n",
      "epoch:4 step:3846 [D loss: 0.523861, acc: 75.78%] [G loss: 2.813924]\n",
      "epoch:4 step:3847 [D loss: 0.565503, acc: 74.22%] [G loss: 2.823265]\n",
      "epoch:4 step:3848 [D loss: 0.638769, acc: 65.62%] [G loss: 3.042170]\n",
      "epoch:4 step:3849 [D loss: 0.525908, acc: 71.09%] [G loss: 2.954437]\n",
      "epoch:4 step:3850 [D loss: 0.583894, acc: 67.19%] [G loss: 2.515064]\n",
      "epoch:4 step:3851 [D loss: 0.447673, acc: 75.00%] [G loss: 3.127458]\n",
      "epoch:4 step:3852 [D loss: 0.563213, acc: 69.53%] [G loss: 2.791834]\n",
      "epoch:4 step:3853 [D loss: 0.576589, acc: 68.75%] [G loss: 2.867482]\n",
      "epoch:4 step:3854 [D loss: 0.559223, acc: 71.09%] [G loss: 2.815875]\n",
      "epoch:4 step:3855 [D loss: 0.651709, acc: 64.84%] [G loss: 2.545309]\n",
      "epoch:4 step:3856 [D loss: 0.686424, acc: 57.81%] [G loss: 2.683565]\n",
      "epoch:4 step:3857 [D loss: 0.644608, acc: 64.06%] [G loss: 2.555165]\n",
      "epoch:4 step:3858 [D loss: 0.533312, acc: 71.88%] [G loss: 2.788654]\n",
      "epoch:4 step:3859 [D loss: 0.650156, acc: 64.84%] [G loss: 2.764469]\n",
      "epoch:4 step:3860 [D loss: 0.429802, acc: 83.59%] [G loss: 3.039770]\n",
      "epoch:4 step:3861 [D loss: 0.580057, acc: 64.84%] [G loss: 2.815622]\n",
      "epoch:4 step:3862 [D loss: 0.553110, acc: 68.75%] [G loss: 2.764916]\n",
      "epoch:4 step:3863 [D loss: 0.652709, acc: 62.50%] [G loss: 2.731023]\n",
      "epoch:4 step:3864 [D loss: 0.586936, acc: 67.19%] [G loss: 2.660536]\n",
      "epoch:4 step:3865 [D loss: 0.498872, acc: 76.56%] [G loss: 2.848372]\n",
      "epoch:4 step:3866 [D loss: 0.627945, acc: 64.06%] [G loss: 3.132470]\n",
      "epoch:4 step:3867 [D loss: 0.474120, acc: 78.91%] [G loss: 3.212501]\n",
      "epoch:4 step:3868 [D loss: 0.563135, acc: 73.44%] [G loss: 2.609654]\n",
      "epoch:4 step:3869 [D loss: 0.649571, acc: 62.50%] [G loss: 2.738978]\n",
      "epoch:4 step:3870 [D loss: 0.582135, acc: 64.84%] [G loss: 2.891591]\n",
      "epoch:4 step:3871 [D loss: 0.573620, acc: 70.31%] [G loss: 2.845129]\n",
      "epoch:4 step:3872 [D loss: 0.712316, acc: 58.59%] [G loss: 2.432774]\n",
      "epoch:4 step:3873 [D loss: 0.619679, acc: 71.09%] [G loss: 2.522436]\n",
      "epoch:4 step:3874 [D loss: 0.588714, acc: 67.19%] [G loss: 2.342018]\n",
      "epoch:4 step:3875 [D loss: 0.612887, acc: 69.53%] [G loss: 2.179826]\n",
      "epoch:4 step:3876 [D loss: 0.618253, acc: 66.41%] [G loss: 2.507961]\n",
      "epoch:4 step:3877 [D loss: 0.617208, acc: 64.06%] [G loss: 2.322515]\n",
      "epoch:4 step:3878 [D loss: 0.529927, acc: 75.00%] [G loss: 2.936768]\n",
      "epoch:4 step:3879 [D loss: 0.540131, acc: 74.22%] [G loss: 2.705521]\n",
      "epoch:4 step:3880 [D loss: 0.592764, acc: 67.97%] [G loss: 2.823712]\n",
      "epoch:4 step:3881 [D loss: 0.642284, acc: 64.06%] [G loss: 2.674874]\n",
      "epoch:4 step:3882 [D loss: 0.606282, acc: 69.53%] [G loss: 2.461537]\n",
      "epoch:4 step:3883 [D loss: 0.608779, acc: 66.41%] [G loss: 2.498042]\n",
      "epoch:4 step:3884 [D loss: 0.604869, acc: 67.19%] [G loss: 2.684820]\n",
      "epoch:4 step:3885 [D loss: 0.604184, acc: 65.62%] [G loss: 2.361642]\n",
      "epoch:4 step:3886 [D loss: 0.576932, acc: 75.78%] [G loss: 2.534297]\n",
      "epoch:4 step:3887 [D loss: 0.588499, acc: 67.19%] [G loss: 2.337077]\n",
      "epoch:4 step:3888 [D loss: 0.545471, acc: 74.22%] [G loss: 2.384605]\n",
      "epoch:4 step:3889 [D loss: 0.540528, acc: 71.88%] [G loss: 2.786803]\n",
      "epoch:4 step:3890 [D loss: 0.574413, acc: 68.75%] [G loss: 2.953344]\n",
      "epoch:4 step:3891 [D loss: 0.599014, acc: 68.75%] [G loss: 2.778826]\n",
      "epoch:4 step:3892 [D loss: 0.578641, acc: 73.44%] [G loss: 2.771004]\n",
      "epoch:4 step:3893 [D loss: 0.557205, acc: 68.75%] [G loss: 3.025388]\n",
      "epoch:4 step:3894 [D loss: 0.532884, acc: 73.44%] [G loss: 2.641742]\n",
      "epoch:4 step:3895 [D loss: 0.616588, acc: 71.88%] [G loss: 2.680457]\n",
      "epoch:4 step:3896 [D loss: 0.596567, acc: 68.75%] [G loss: 2.605213]\n",
      "epoch:4 step:3897 [D loss: 0.565628, acc: 71.88%] [G loss: 3.084911]\n",
      "epoch:4 step:3898 [D loss: 0.673366, acc: 61.72%] [G loss: 2.804326]\n",
      "epoch:4 step:3899 [D loss: 0.592331, acc: 68.75%] [G loss: 3.543197]\n",
      "epoch:4 step:3900 [D loss: 0.601614, acc: 64.84%] [G loss: 3.091939]\n",
      "epoch:4 step:3901 [D loss: 0.575773, acc: 79.69%] [G loss: 2.425601]\n",
      "epoch:4 step:3902 [D loss: 0.574871, acc: 69.53%] [G loss: 2.775088]\n",
      "epoch:4 step:3903 [D loss: 0.548845, acc: 75.00%] [G loss: 2.950848]\n",
      "epoch:4 step:3904 [D loss: 0.529413, acc: 69.53%] [G loss: 2.920183]\n",
      "epoch:4 step:3905 [D loss: 0.647164, acc: 66.41%] [G loss: 2.413501]\n",
      "epoch:4 step:3906 [D loss: 0.538464, acc: 75.78%] [G loss: 2.691171]\n",
      "epoch:4 step:3907 [D loss: 0.596790, acc: 67.97%] [G loss: 3.157696]\n",
      "epoch:4 step:3908 [D loss: 0.650686, acc: 59.38%] [G loss: 2.478129]\n",
      "epoch:4 step:3909 [D loss: 0.736590, acc: 61.72%] [G loss: 2.515908]\n",
      "epoch:4 step:3910 [D loss: 0.545477, acc: 74.22%] [G loss: 2.654539]\n",
      "epoch:4 step:3911 [D loss: 0.597051, acc: 72.66%] [G loss: 2.988926]\n",
      "epoch:4 step:3912 [D loss: 0.598157, acc: 66.41%] [G loss: 2.296388]\n",
      "epoch:4 step:3913 [D loss: 0.643725, acc: 71.09%] [G loss: 2.749057]\n",
      "epoch:4 step:3914 [D loss: 0.627117, acc: 64.06%] [G loss: 2.697627]\n",
      "epoch:4 step:3915 [D loss: 0.560026, acc: 68.75%] [G loss: 2.667258]\n",
      "epoch:4 step:3916 [D loss: 0.670658, acc: 64.06%] [G loss: 2.455739]\n",
      "epoch:4 step:3917 [D loss: 0.545978, acc: 68.75%] [G loss: 2.595176]\n",
      "epoch:4 step:3918 [D loss: 0.539024, acc: 75.00%] [G loss: 2.566375]\n",
      "epoch:4 step:3919 [D loss: 0.510228, acc: 77.34%] [G loss: 2.889813]\n",
      "epoch:4 step:3920 [D loss: 0.518524, acc: 74.22%] [G loss: 2.484061]\n",
      "epoch:4 step:3921 [D loss: 0.546765, acc: 75.78%] [G loss: 2.825005]\n",
      "epoch:4 step:3922 [D loss: 0.580074, acc: 68.75%] [G loss: 2.704302]\n",
      "epoch:4 step:3923 [D loss: 0.525348, acc: 74.22%] [G loss: 2.783488]\n",
      "epoch:4 step:3924 [D loss: 0.488513, acc: 79.69%] [G loss: 2.984473]\n",
      "epoch:4 step:3925 [D loss: 0.533379, acc: 75.78%] [G loss: 2.922606]\n",
      "epoch:4 step:3926 [D loss: 0.536745, acc: 71.88%] [G loss: 3.011635]\n",
      "epoch:4 step:3927 [D loss: 0.571487, acc: 71.09%] [G loss: 2.733045]\n",
      "epoch:4 step:3928 [D loss: 0.581863, acc: 71.88%] [G loss: 2.829302]\n",
      "epoch:4 step:3929 [D loss: 0.634480, acc: 64.06%] [G loss: 2.958123]\n",
      "epoch:4 step:3930 [D loss: 0.620776, acc: 66.41%] [G loss: 2.724087]\n",
      "epoch:4 step:3931 [D loss: 0.636437, acc: 66.41%] [G loss: 2.616527]\n",
      "epoch:4 step:3932 [D loss: 0.639563, acc: 68.75%] [G loss: 2.483247]\n",
      "epoch:4 step:3933 [D loss: 0.609213, acc: 63.28%] [G loss: 2.628709]\n",
      "epoch:4 step:3934 [D loss: 0.578918, acc: 69.53%] [G loss: 2.730956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3935 [D loss: 0.670691, acc: 57.81%] [G loss: 2.589754]\n",
      "epoch:4 step:3936 [D loss: 0.564400, acc: 70.31%] [G loss: 2.730604]\n",
      "epoch:4 step:3937 [D loss: 0.541600, acc: 73.44%] [G loss: 2.986779]\n",
      "epoch:4 step:3938 [D loss: 0.621285, acc: 71.88%] [G loss: 2.826709]\n",
      "epoch:4 step:3939 [D loss: 0.477915, acc: 78.12%] [G loss: 2.877520]\n",
      "epoch:4 step:3940 [D loss: 0.512673, acc: 79.69%] [G loss: 2.516441]\n",
      "epoch:4 step:3941 [D loss: 0.592807, acc: 69.53%] [G loss: 2.900428]\n",
      "epoch:4 step:3942 [D loss: 0.534075, acc: 71.88%] [G loss: 3.087049]\n",
      "epoch:4 step:3943 [D loss: 0.546993, acc: 73.44%] [G loss: 2.623339]\n",
      "epoch:4 step:3944 [D loss: 0.638565, acc: 64.06%] [G loss: 2.763064]\n",
      "epoch:4 step:3945 [D loss: 0.571904, acc: 67.97%] [G loss: 3.276920]\n",
      "epoch:4 step:3946 [D loss: 0.539842, acc: 77.34%] [G loss: 2.960805]\n",
      "epoch:4 step:3947 [D loss: 0.596879, acc: 69.53%] [G loss: 2.691851]\n",
      "epoch:4 step:3948 [D loss: 0.624947, acc: 64.84%] [G loss: 2.707407]\n",
      "epoch:4 step:3949 [D loss: 0.659343, acc: 60.16%] [G loss: 2.724597]\n",
      "epoch:4 step:3950 [D loss: 0.555490, acc: 71.88%] [G loss: 2.512868]\n",
      "epoch:4 step:3951 [D loss: 0.677826, acc: 57.81%] [G loss: 2.613081]\n",
      "epoch:4 step:3952 [D loss: 0.543560, acc: 75.78%] [G loss: 2.849895]\n",
      "epoch:4 step:3953 [D loss: 0.568743, acc: 71.09%] [G loss: 2.925744]\n",
      "epoch:4 step:3954 [D loss: 0.476420, acc: 78.91%] [G loss: 3.016576]\n",
      "epoch:4 step:3955 [D loss: 0.534790, acc: 75.00%] [G loss: 3.005833]\n",
      "epoch:4 step:3956 [D loss: 0.460263, acc: 77.34%] [G loss: 3.357821]\n",
      "epoch:4 step:3957 [D loss: 0.496488, acc: 77.34%] [G loss: 3.109644]\n",
      "epoch:4 step:3958 [D loss: 0.628696, acc: 62.50%] [G loss: 2.833518]\n",
      "epoch:4 step:3959 [D loss: 0.671535, acc: 62.50%] [G loss: 2.475207]\n",
      "epoch:4 step:3960 [D loss: 0.580206, acc: 73.44%] [G loss: 2.832031]\n",
      "epoch:4 step:3961 [D loss: 0.585782, acc: 67.97%] [G loss: 2.863166]\n",
      "epoch:4 step:3962 [D loss: 0.606356, acc: 66.41%] [G loss: 2.429300]\n",
      "epoch:4 step:3963 [D loss: 0.673859, acc: 59.38%] [G loss: 2.305372]\n",
      "epoch:4 step:3964 [D loss: 0.575686, acc: 71.09%] [G loss: 2.632126]\n",
      "epoch:4 step:3965 [D loss: 0.504109, acc: 75.00%] [G loss: 3.205777]\n",
      "epoch:4 step:3966 [D loss: 0.635975, acc: 68.75%] [G loss: 2.665997]\n",
      "epoch:4 step:3967 [D loss: 0.582094, acc: 67.19%] [G loss: 2.846009]\n",
      "epoch:4 step:3968 [D loss: 0.720485, acc: 57.03%] [G loss: 2.677687]\n",
      "epoch:4 step:3969 [D loss: 0.572846, acc: 66.41%] [G loss: 2.557982]\n",
      "epoch:4 step:3970 [D loss: 0.590116, acc: 71.09%] [G loss: 2.877720]\n",
      "epoch:4 step:3971 [D loss: 0.518832, acc: 75.00%] [G loss: 2.846287]\n",
      "epoch:4 step:3972 [D loss: 0.568950, acc: 69.53%] [G loss: 2.706846]\n",
      "epoch:4 step:3973 [D loss: 0.777433, acc: 53.12%] [G loss: 2.828163]\n",
      "epoch:4 step:3974 [D loss: 0.583954, acc: 70.31%] [G loss: 2.694148]\n",
      "epoch:4 step:3975 [D loss: 0.576422, acc: 65.62%] [G loss: 2.359983]\n",
      "epoch:4 step:3976 [D loss: 0.605591, acc: 64.84%] [G loss: 2.359908]\n",
      "epoch:4 step:3977 [D loss: 0.569260, acc: 71.88%] [G loss: 3.013705]\n",
      "epoch:4 step:3978 [D loss: 0.581295, acc: 65.62%] [G loss: 2.956276]\n",
      "epoch:4 step:3979 [D loss: 0.477360, acc: 78.12%] [G loss: 3.015195]\n",
      "epoch:4 step:3980 [D loss: 0.488669, acc: 76.56%] [G loss: 3.200473]\n",
      "epoch:4 step:3981 [D loss: 0.655982, acc: 71.09%] [G loss: 2.725171]\n",
      "epoch:4 step:3982 [D loss: 0.628587, acc: 67.97%] [G loss: 2.923111]\n",
      "epoch:4 step:3983 [D loss: 0.589210, acc: 70.31%] [G loss: 2.681606]\n",
      "epoch:4 step:3984 [D loss: 0.517224, acc: 78.91%] [G loss: 2.596617]\n",
      "epoch:4 step:3985 [D loss: 0.620646, acc: 64.84%] [G loss: 2.594660]\n",
      "epoch:4 step:3986 [D loss: 0.647838, acc: 67.19%] [G loss: 2.784576]\n",
      "epoch:4 step:3987 [D loss: 0.559376, acc: 71.88%] [G loss: 2.899931]\n",
      "epoch:4 step:3988 [D loss: 0.541979, acc: 69.53%] [G loss: 2.807745]\n",
      "epoch:4 step:3989 [D loss: 0.591823, acc: 65.62%] [G loss: 2.745717]\n",
      "epoch:4 step:3990 [D loss: 0.585986, acc: 66.41%] [G loss: 2.740593]\n",
      "epoch:4 step:3991 [D loss: 0.633458, acc: 66.41%] [G loss: 2.789127]\n",
      "epoch:4 step:3992 [D loss: 0.613548, acc: 65.62%] [G loss: 2.663418]\n",
      "epoch:4 step:3993 [D loss: 0.534147, acc: 71.88%] [G loss: 2.561056]\n",
      "epoch:4 step:3994 [D loss: 0.587446, acc: 68.75%] [G loss: 2.863778]\n",
      "epoch:4 step:3995 [D loss: 0.709082, acc: 57.81%] [G loss: 2.475482]\n",
      "epoch:4 step:3996 [D loss: 0.552647, acc: 70.31%] [G loss: 2.660938]\n",
      "epoch:4 step:3997 [D loss: 0.677640, acc: 64.06%] [G loss: 2.651553]\n",
      "epoch:4 step:3998 [D loss: 0.678233, acc: 63.28%] [G loss: 2.311593]\n",
      "epoch:4 step:3999 [D loss: 0.618752, acc: 68.75%] [G loss: 2.273547]\n",
      "epoch:4 step:4000 [D loss: 0.589120, acc: 67.97%] [G loss: 2.514769]\n",
      "##############\n",
      "[3.01702715 1.63743563 6.97140092 5.54047398 4.33597881 6.30534315\n",
      " 5.03247897 5.4011296  5.60715389 3.9118864 ]\n",
      "##########\n",
      "epoch:4 step:4001 [D loss: 0.539587, acc: 75.00%] [G loss: 2.782869]\n",
      "epoch:4 step:4002 [D loss: 0.525175, acc: 77.34%] [G loss: 2.939940]\n",
      "epoch:4 step:4003 [D loss: 0.598421, acc: 68.75%] [G loss: 2.601559]\n",
      "epoch:4 step:4004 [D loss: 0.582509, acc: 68.75%] [G loss: 2.596784]\n",
      "epoch:4 step:4005 [D loss: 0.620622, acc: 64.84%] [G loss: 2.366920]\n",
      "epoch:4 step:4006 [D loss: 0.470064, acc: 78.91%] [G loss: 2.919497]\n",
      "epoch:4 step:4007 [D loss: 0.498907, acc: 78.91%] [G loss: 3.013568]\n",
      "epoch:4 step:4008 [D loss: 0.648578, acc: 65.62%] [G loss: 2.657129]\n",
      "epoch:4 step:4009 [D loss: 0.608265, acc: 67.97%] [G loss: 2.979726]\n",
      "epoch:4 step:4010 [D loss: 0.541120, acc: 74.22%] [G loss: 3.131767]\n",
      "epoch:4 step:4011 [D loss: 0.608089, acc: 67.19%] [G loss: 2.450298]\n",
      "epoch:4 step:4012 [D loss: 0.575163, acc: 71.09%] [G loss: 2.817788]\n",
      "epoch:4 step:4013 [D loss: 0.622044, acc: 66.41%] [G loss: 2.826468]\n",
      "epoch:4 step:4014 [D loss: 0.571811, acc: 67.97%] [G loss: 3.076373]\n",
      "epoch:4 step:4015 [D loss: 0.589429, acc: 71.09%] [G loss: 2.688553]\n",
      "epoch:4 step:4016 [D loss: 0.573187, acc: 62.50%] [G loss: 2.658210]\n",
      "epoch:4 step:4017 [D loss: 0.565295, acc: 67.19%] [G loss: 2.766125]\n",
      "epoch:4 step:4018 [D loss: 0.509013, acc: 72.66%] [G loss: 2.849217]\n",
      "epoch:4 step:4019 [D loss: 0.552377, acc: 68.75%] [G loss: 2.850474]\n",
      "epoch:4 step:4020 [D loss: 0.545234, acc: 67.97%] [G loss: 3.027932]\n",
      "epoch:4 step:4021 [D loss: 0.479655, acc: 78.91%] [G loss: 3.163393]\n",
      "epoch:4 step:4022 [D loss: 0.638768, acc: 65.62%] [G loss: 2.744169]\n",
      "epoch:4 step:4023 [D loss: 0.578901, acc: 65.62%] [G loss: 2.772193]\n",
      "epoch:4 step:4024 [D loss: 0.593371, acc: 67.19%] [G loss: 2.725135]\n",
      "epoch:4 step:4025 [D loss: 0.617631, acc: 69.53%] [G loss: 2.975667]\n",
      "epoch:4 step:4026 [D loss: 0.647205, acc: 67.97%] [G loss: 2.961656]\n",
      "epoch:4 step:4027 [D loss: 0.562486, acc: 69.53%] [G loss: 2.888877]\n",
      "epoch:4 step:4028 [D loss: 0.489920, acc: 80.47%] [G loss: 3.298138]\n",
      "epoch:4 step:4029 [D loss: 0.585393, acc: 67.97%] [G loss: 3.070966]\n",
      "epoch:4 step:4030 [D loss: 0.637460, acc: 68.75%] [G loss: 2.619821]\n",
      "epoch:4 step:4031 [D loss: 0.524436, acc: 72.66%] [G loss: 2.774691]\n",
      "epoch:4 step:4032 [D loss: 0.459234, acc: 78.12%] [G loss: 3.034514]\n",
      "epoch:4 step:4033 [D loss: 0.524498, acc: 72.66%] [G loss: 3.014032]\n",
      "epoch:4 step:4034 [D loss: 0.562370, acc: 74.22%] [G loss: 3.060376]\n",
      "epoch:4 step:4035 [D loss: 0.637691, acc: 64.84%] [G loss: 2.865145]\n",
      "epoch:4 step:4036 [D loss: 0.608329, acc: 72.66%] [G loss: 2.642313]\n",
      "epoch:4 step:4037 [D loss: 0.581087, acc: 69.53%] [G loss: 2.965511]\n",
      "epoch:4 step:4038 [D loss: 0.573682, acc: 64.06%] [G loss: 2.433851]\n",
      "epoch:4 step:4039 [D loss: 0.540338, acc: 70.31%] [G loss: 2.857375]\n",
      "epoch:4 step:4040 [D loss: 0.591275, acc: 67.19%] [G loss: 2.593499]\n",
      "epoch:4 step:4041 [D loss: 0.548619, acc: 72.66%] [G loss: 2.999284]\n",
      "epoch:4 step:4042 [D loss: 0.550681, acc: 76.56%] [G loss: 2.711433]\n",
      "epoch:4 step:4043 [D loss: 0.542031, acc: 73.44%] [G loss: 2.955457]\n",
      "epoch:4 step:4044 [D loss: 0.569120, acc: 80.47%] [G loss: 2.889669]\n",
      "epoch:4 step:4045 [D loss: 0.580670, acc: 67.97%] [G loss: 2.883947]\n",
      "epoch:4 step:4046 [D loss: 0.512547, acc: 75.78%] [G loss: 2.804598]\n",
      "epoch:4 step:4047 [D loss: 0.625082, acc: 67.97%] [G loss: 3.162650]\n",
      "epoch:4 step:4048 [D loss: 0.521371, acc: 71.88%] [G loss: 2.974217]\n",
      "epoch:4 step:4049 [D loss: 0.667016, acc: 63.28%] [G loss: 2.722729]\n",
      "epoch:4 step:4050 [D loss: 0.597280, acc: 65.62%] [G loss: 2.359829]\n",
      "epoch:4 step:4051 [D loss: 0.622297, acc: 69.53%] [G loss: 2.743480]\n",
      "epoch:4 step:4052 [D loss: 0.560191, acc: 73.44%] [G loss: 3.225336]\n",
      "epoch:4 step:4053 [D loss: 0.594365, acc: 66.41%] [G loss: 2.606774]\n",
      "epoch:4 step:4054 [D loss: 0.683869, acc: 60.16%] [G loss: 2.507493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4055 [D loss: 0.527512, acc: 73.44%] [G loss: 2.913499]\n",
      "epoch:4 step:4056 [D loss: 0.512412, acc: 72.66%] [G loss: 2.642652]\n",
      "epoch:4 step:4057 [D loss: 0.475157, acc: 82.03%] [G loss: 2.863199]\n",
      "epoch:4 step:4058 [D loss: 0.602634, acc: 65.62%] [G loss: 3.004264]\n",
      "epoch:4 step:4059 [D loss: 0.601893, acc: 67.19%] [G loss: 2.925969]\n",
      "epoch:4 step:4060 [D loss: 0.438525, acc: 79.69%] [G loss: 3.472711]\n",
      "epoch:4 step:4061 [D loss: 0.435056, acc: 79.69%] [G loss: 3.113216]\n",
      "epoch:4 step:4062 [D loss: 0.446724, acc: 80.47%] [G loss: 3.572718]\n",
      "epoch:4 step:4063 [D loss: 0.521803, acc: 76.56%] [G loss: 3.187735]\n",
      "epoch:4 step:4064 [D loss: 0.744284, acc: 66.41%] [G loss: 2.351546]\n",
      "epoch:4 step:4065 [D loss: 0.645919, acc: 66.41%] [G loss: 2.508307]\n",
      "epoch:4 step:4066 [D loss: 0.568067, acc: 68.75%] [G loss: 2.784141]\n",
      "epoch:4 step:4067 [D loss: 0.588236, acc: 71.88%] [G loss: 2.608482]\n",
      "epoch:4 step:4068 [D loss: 0.564140, acc: 65.62%] [G loss: 2.732114]\n",
      "epoch:4 step:4069 [D loss: 0.531291, acc: 73.44%] [G loss: 2.889337]\n",
      "epoch:4 step:4070 [D loss: 0.545741, acc: 74.22%] [G loss: 2.947746]\n",
      "epoch:4 step:4071 [D loss: 0.645937, acc: 62.50%] [G loss: 2.665406]\n",
      "epoch:4 step:4072 [D loss: 0.579146, acc: 67.97%] [G loss: 2.345848]\n",
      "epoch:4 step:4073 [D loss: 0.557588, acc: 71.88%] [G loss: 2.587386]\n",
      "epoch:4 step:4074 [D loss: 0.553902, acc: 75.00%] [G loss: 2.824696]\n",
      "epoch:4 step:4075 [D loss: 0.556953, acc: 71.88%] [G loss: 2.513837]\n",
      "epoch:4 step:4076 [D loss: 0.680324, acc: 67.19%] [G loss: 2.550173]\n",
      "epoch:4 step:4077 [D loss: 0.561906, acc: 71.88%] [G loss: 2.670803]\n",
      "epoch:4 step:4078 [D loss: 0.598396, acc: 67.19%] [G loss: 2.531248]\n",
      "epoch:4 step:4079 [D loss: 0.559572, acc: 67.97%] [G loss: 2.651002]\n",
      "epoch:4 step:4080 [D loss: 0.643766, acc: 67.19%] [G loss: 2.639472]\n",
      "epoch:4 step:4081 [D loss: 0.627639, acc: 65.62%] [G loss: 2.737010]\n",
      "epoch:4 step:4082 [D loss: 0.633949, acc: 63.28%] [G loss: 2.401289]\n",
      "epoch:4 step:4083 [D loss: 0.575494, acc: 65.62%] [G loss: 3.186540]\n",
      "epoch:4 step:4084 [D loss: 0.553273, acc: 71.09%] [G loss: 2.951688]\n",
      "epoch:4 step:4085 [D loss: 0.576311, acc: 70.31%] [G loss: 2.421334]\n",
      "epoch:4 step:4086 [D loss: 0.596883, acc: 69.53%] [G loss: 2.820714]\n",
      "epoch:4 step:4087 [D loss: 0.511269, acc: 77.34%] [G loss: 2.907355]\n",
      "epoch:4 step:4088 [D loss: 0.551280, acc: 70.31%] [G loss: 2.772407]\n",
      "epoch:4 step:4089 [D loss: 0.686166, acc: 54.69%] [G loss: 2.727560]\n",
      "epoch:4 step:4090 [D loss: 0.589660, acc: 71.88%] [G loss: 2.550602]\n",
      "epoch:4 step:4091 [D loss: 0.595163, acc: 69.53%] [G loss: 2.989646]\n",
      "epoch:4 step:4092 [D loss: 0.600160, acc: 71.09%] [G loss: 2.955341]\n",
      "epoch:4 step:4093 [D loss: 0.612225, acc: 69.53%] [G loss: 2.733639]\n",
      "epoch:4 step:4094 [D loss: 0.548170, acc: 71.09%] [G loss: 2.853679]\n",
      "epoch:4 step:4095 [D loss: 0.542497, acc: 68.75%] [G loss: 3.018920]\n",
      "epoch:4 step:4096 [D loss: 0.629726, acc: 60.94%] [G loss: 2.821450]\n",
      "epoch:4 step:4097 [D loss: 0.608517, acc: 67.97%] [G loss: 2.626267]\n",
      "epoch:4 step:4098 [D loss: 0.590333, acc: 70.31%] [G loss: 2.759235]\n",
      "epoch:4 step:4099 [D loss: 0.525047, acc: 71.88%] [G loss: 2.873106]\n",
      "epoch:4 step:4100 [D loss: 0.619160, acc: 65.62%] [G loss: 2.672061]\n",
      "epoch:4 step:4101 [D loss: 0.636714, acc: 60.94%] [G loss: 2.746034]\n",
      "epoch:4 step:4102 [D loss: 0.485087, acc: 77.34%] [G loss: 2.938683]\n",
      "epoch:4 step:4103 [D loss: 0.591983, acc: 65.62%] [G loss: 2.890542]\n",
      "epoch:4 step:4104 [D loss: 0.583750, acc: 71.88%] [G loss: 2.604246]\n",
      "epoch:4 step:4105 [D loss: 0.567051, acc: 69.53%] [G loss: 2.996510]\n",
      "epoch:4 step:4106 [D loss: 0.541323, acc: 75.00%] [G loss: 3.232531]\n",
      "epoch:4 step:4107 [D loss: 0.555313, acc: 75.78%] [G loss: 2.953919]\n",
      "epoch:4 step:4108 [D loss: 0.568790, acc: 71.09%] [G loss: 3.049940]\n",
      "epoch:4 step:4109 [D loss: 0.528036, acc: 72.66%] [G loss: 2.795383]\n",
      "epoch:4 step:4110 [D loss: 0.552019, acc: 67.97%] [G loss: 2.720928]\n",
      "epoch:4 step:4111 [D loss: 0.541631, acc: 73.44%] [G loss: 2.954922]\n",
      "epoch:4 step:4112 [D loss: 0.528960, acc: 71.88%] [G loss: 2.593019]\n",
      "epoch:4 step:4113 [D loss: 0.545379, acc: 67.97%] [G loss: 2.873964]\n",
      "epoch:4 step:4114 [D loss: 0.575237, acc: 70.31%] [G loss: 3.075060]\n",
      "epoch:4 step:4115 [D loss: 0.576325, acc: 70.31%] [G loss: 2.790112]\n",
      "epoch:4 step:4116 [D loss: 0.486899, acc: 79.69%] [G loss: 2.901519]\n",
      "epoch:4 step:4117 [D loss: 0.646455, acc: 62.50%] [G loss: 2.830077]\n",
      "epoch:4 step:4118 [D loss: 0.568228, acc: 66.41%] [G loss: 2.914195]\n",
      "epoch:4 step:4119 [D loss: 0.585571, acc: 74.22%] [G loss: 3.024140]\n",
      "epoch:4 step:4120 [D loss: 0.672295, acc: 60.16%] [G loss: 3.137151]\n",
      "epoch:4 step:4121 [D loss: 0.632519, acc: 67.97%] [G loss: 2.529318]\n",
      "epoch:4 step:4122 [D loss: 0.563636, acc: 70.31%] [G loss: 3.154399]\n",
      "epoch:4 step:4123 [D loss: 0.617127, acc: 68.75%] [G loss: 2.480354]\n",
      "epoch:4 step:4124 [D loss: 0.620334, acc: 69.53%] [G loss: 2.196096]\n",
      "epoch:4 step:4125 [D loss: 0.678670, acc: 67.19%] [G loss: 2.542691]\n",
      "epoch:4 step:4126 [D loss: 0.554310, acc: 71.09%] [G loss: 2.809195]\n",
      "epoch:4 step:4127 [D loss: 0.522777, acc: 74.22%] [G loss: 2.840733]\n",
      "epoch:4 step:4128 [D loss: 0.587361, acc: 68.75%] [G loss: 2.882532]\n",
      "epoch:4 step:4129 [D loss: 0.534504, acc: 67.97%] [G loss: 2.913205]\n",
      "epoch:4 step:4130 [D loss: 0.606151, acc: 66.41%] [G loss: 2.750719]\n",
      "epoch:4 step:4131 [D loss: 0.626662, acc: 65.62%] [G loss: 2.521471]\n",
      "epoch:4 step:4132 [D loss: 0.576282, acc: 67.97%] [G loss: 2.876622]\n",
      "epoch:4 step:4133 [D loss: 0.563743, acc: 70.31%] [G loss: 3.098813]\n",
      "epoch:4 step:4134 [D loss: 0.715721, acc: 56.25%] [G loss: 2.468759]\n",
      "epoch:4 step:4135 [D loss: 0.587234, acc: 65.62%] [G loss: 2.509929]\n",
      "epoch:4 step:4136 [D loss: 0.628865, acc: 70.31%] [G loss: 2.701946]\n",
      "epoch:4 step:4137 [D loss: 0.579527, acc: 67.97%] [G loss: 2.433252]\n",
      "epoch:4 step:4138 [D loss: 0.584680, acc: 67.97%] [G loss: 2.833607]\n",
      "epoch:4 step:4139 [D loss: 0.681350, acc: 63.28%] [G loss: 2.320690]\n",
      "epoch:4 step:4140 [D loss: 0.522365, acc: 72.66%] [G loss: 3.197521]\n",
      "epoch:4 step:4141 [D loss: 0.574916, acc: 67.97%] [G loss: 2.616800]\n",
      "epoch:4 step:4142 [D loss: 0.570161, acc: 67.97%] [G loss: 2.498030]\n",
      "epoch:4 step:4143 [D loss: 0.573521, acc: 74.22%] [G loss: 2.964318]\n",
      "epoch:4 step:4144 [D loss: 0.670628, acc: 63.28%] [G loss: 2.363182]\n",
      "epoch:4 step:4145 [D loss: 0.585204, acc: 66.41%] [G loss: 2.691647]\n",
      "epoch:4 step:4146 [D loss: 0.501580, acc: 73.44%] [G loss: 3.004966]\n",
      "epoch:4 step:4147 [D loss: 0.527863, acc: 75.78%] [G loss: 2.882852]\n",
      "epoch:4 step:4148 [D loss: 0.598308, acc: 67.19%] [G loss: 2.851215]\n",
      "epoch:4 step:4149 [D loss: 0.632997, acc: 64.06%] [G loss: 2.868834]\n",
      "epoch:4 step:4150 [D loss: 0.536051, acc: 72.66%] [G loss: 2.902385]\n",
      "epoch:4 step:4151 [D loss: 0.681165, acc: 60.94%] [G loss: 3.013027]\n",
      "epoch:4 step:4152 [D loss: 0.515492, acc: 77.34%] [G loss: 2.942525]\n",
      "epoch:4 step:4153 [D loss: 0.520013, acc: 78.91%] [G loss: 3.087980]\n",
      "epoch:4 step:4154 [D loss: 0.537151, acc: 74.22%] [G loss: 2.795578]\n",
      "epoch:4 step:4155 [D loss: 0.719355, acc: 63.28%] [G loss: 2.673748]\n",
      "epoch:4 step:4156 [D loss: 0.622308, acc: 68.75%] [G loss: 2.522601]\n",
      "epoch:4 step:4157 [D loss: 0.587290, acc: 68.75%] [G loss: 2.832426]\n",
      "epoch:4 step:4158 [D loss: 0.597642, acc: 67.19%] [G loss: 2.563107]\n",
      "epoch:4 step:4159 [D loss: 0.568646, acc: 69.53%] [G loss: 2.600052]\n",
      "epoch:4 step:4160 [D loss: 0.717596, acc: 60.94%] [G loss: 2.311347]\n",
      "epoch:4 step:4161 [D loss: 0.618819, acc: 68.75%] [G loss: 2.600931]\n",
      "epoch:4 step:4162 [D loss: 0.558354, acc: 71.09%] [G loss: 2.356689]\n",
      "epoch:4 step:4163 [D loss: 0.590220, acc: 68.75%] [G loss: 2.533615]\n",
      "epoch:4 step:4164 [D loss: 0.653463, acc: 61.72%] [G loss: 2.629569]\n",
      "epoch:4 step:4165 [D loss: 0.644864, acc: 64.06%] [G loss: 2.599304]\n",
      "epoch:4 step:4166 [D loss: 0.613761, acc: 63.28%] [G loss: 2.845154]\n",
      "epoch:4 step:4167 [D loss: 0.552570, acc: 71.09%] [G loss: 2.763716]\n",
      "epoch:4 step:4168 [D loss: 0.594602, acc: 67.19%] [G loss: 2.767540]\n",
      "epoch:4 step:4169 [D loss: 0.556077, acc: 67.19%] [G loss: 2.539516]\n",
      "epoch:4 step:4170 [D loss: 0.532255, acc: 79.69%] [G loss: 2.901119]\n",
      "epoch:4 step:4171 [D loss: 0.568544, acc: 72.66%] [G loss: 2.889256]\n",
      "epoch:4 step:4172 [D loss: 0.533624, acc: 73.44%] [G loss: 2.849720]\n",
      "epoch:4 step:4173 [D loss: 0.580903, acc: 72.66%] [G loss: 3.044837]\n",
      "epoch:4 step:4174 [D loss: 0.458712, acc: 84.38%] [G loss: 2.980978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4175 [D loss: 0.553192, acc: 71.88%] [G loss: 3.093995]\n",
      "epoch:4 step:4176 [D loss: 0.484543, acc: 78.12%] [G loss: 3.110878]\n",
      "epoch:4 step:4177 [D loss: 0.512566, acc: 71.09%] [G loss: 3.133225]\n",
      "epoch:4 step:4178 [D loss: 0.575683, acc: 69.53%] [G loss: 3.316132]\n",
      "epoch:4 step:4179 [D loss: 0.637276, acc: 62.50%] [G loss: 2.812722]\n",
      "epoch:4 step:4180 [D loss: 0.638968, acc: 67.19%] [G loss: 2.760216]\n",
      "epoch:4 step:4181 [D loss: 0.601836, acc: 70.31%] [G loss: 2.779590]\n",
      "epoch:4 step:4182 [D loss: 0.577052, acc: 73.44%] [G loss: 2.472145]\n",
      "epoch:4 step:4183 [D loss: 0.648482, acc: 64.06%] [G loss: 2.519166]\n",
      "epoch:4 step:4184 [D loss: 0.617156, acc: 69.53%] [G loss: 2.334628]\n",
      "epoch:4 step:4185 [D loss: 0.634601, acc: 67.19%] [G loss: 2.124198]\n",
      "epoch:4 step:4186 [D loss: 0.594952, acc: 67.19%] [G loss: 2.599236]\n",
      "epoch:4 step:4187 [D loss: 0.604812, acc: 66.41%] [G loss: 2.442284]\n",
      "epoch:4 step:4188 [D loss: 0.621660, acc: 67.97%] [G loss: 2.850214]\n",
      "epoch:4 step:4189 [D loss: 0.569293, acc: 71.09%] [G loss: 2.506579]\n",
      "epoch:4 step:4190 [D loss: 0.651345, acc: 58.59%] [G loss: 2.338463]\n",
      "epoch:4 step:4191 [D loss: 0.591664, acc: 71.09%] [G loss: 2.491001]\n",
      "epoch:4 step:4192 [D loss: 0.656390, acc: 69.53%] [G loss: 2.314000]\n",
      "epoch:4 step:4193 [D loss: 0.517284, acc: 76.56%] [G loss: 2.501957]\n",
      "epoch:4 step:4194 [D loss: 0.555619, acc: 67.97%] [G loss: 2.551054]\n",
      "epoch:4 step:4195 [D loss: 0.544349, acc: 70.31%] [G loss: 2.718261]\n",
      "epoch:4 step:4196 [D loss: 0.639654, acc: 68.75%] [G loss: 2.606319]\n",
      "epoch:4 step:4197 [D loss: 0.647686, acc: 62.50%] [G loss: 2.611132]\n",
      "epoch:4 step:4198 [D loss: 0.509520, acc: 74.22%] [G loss: 2.730587]\n",
      "epoch:4 step:4199 [D loss: 0.641379, acc: 68.75%] [G loss: 3.037401]\n",
      "epoch:4 step:4200 [D loss: 0.615717, acc: 66.41%] [G loss: 2.825630]\n",
      "##############\n",
      "[2.93666258 1.54193889 6.74983369 5.40794147 4.25104882 6.19206755\n",
      " 5.22657267 5.25651518 5.36354143 3.85041135]\n",
      "##########\n",
      "epoch:4 step:4201 [D loss: 0.605212, acc: 71.88%] [G loss: 2.624414]\n",
      "epoch:4 step:4202 [D loss: 0.617446, acc: 68.75%] [G loss: 2.335243]\n",
      "epoch:4 step:4203 [D loss: 0.575361, acc: 68.75%] [G loss: 2.679687]\n",
      "epoch:4 step:4204 [D loss: 0.631498, acc: 65.62%] [G loss: 2.453756]\n",
      "epoch:4 step:4205 [D loss: 0.571991, acc: 67.19%] [G loss: 2.608285]\n",
      "epoch:4 step:4206 [D loss: 0.567254, acc: 67.19%] [G loss: 2.564628]\n",
      "epoch:4 step:4207 [D loss: 0.677447, acc: 59.38%] [G loss: 2.421592]\n",
      "epoch:4 step:4208 [D loss: 0.626293, acc: 62.50%] [G loss: 2.588140]\n",
      "epoch:4 step:4209 [D loss: 0.621557, acc: 64.84%] [G loss: 2.278331]\n",
      "epoch:4 step:4210 [D loss: 0.590213, acc: 69.53%] [G loss: 2.630629]\n",
      "epoch:4 step:4211 [D loss: 0.588100, acc: 65.62%] [G loss: 2.610155]\n",
      "epoch:4 step:4212 [D loss: 0.531113, acc: 78.91%] [G loss: 2.606012]\n",
      "epoch:4 step:4213 [D loss: 0.637964, acc: 60.94%] [G loss: 2.550139]\n",
      "epoch:4 step:4214 [D loss: 0.531091, acc: 73.44%] [G loss: 2.945389]\n",
      "epoch:4 step:4215 [D loss: 0.627150, acc: 65.62%] [G loss: 2.599859]\n",
      "epoch:4 step:4216 [D loss: 0.629090, acc: 64.84%] [G loss: 2.759380]\n",
      "epoch:4 step:4217 [D loss: 0.631843, acc: 63.28%] [G loss: 2.783164]\n",
      "epoch:4 step:4218 [D loss: 0.519381, acc: 77.34%] [G loss: 2.838378]\n",
      "epoch:4 step:4219 [D loss: 0.436871, acc: 83.59%] [G loss: 3.268620]\n",
      "epoch:4 step:4220 [D loss: 0.595676, acc: 65.62%] [G loss: 3.206375]\n",
      "epoch:4 step:4221 [D loss: 0.551919, acc: 71.09%] [G loss: 2.615352]\n",
      "epoch:4 step:4222 [D loss: 0.583297, acc: 68.75%] [G loss: 3.074636]\n",
      "epoch:4 step:4223 [D loss: 0.569573, acc: 71.09%] [G loss: 2.897284]\n",
      "epoch:4 step:4224 [D loss: 0.500403, acc: 74.22%] [G loss: 2.995397]\n",
      "epoch:4 step:4225 [D loss: 0.684152, acc: 60.94%] [G loss: 2.255782]\n",
      "epoch:4 step:4226 [D loss: 0.653895, acc: 58.59%] [G loss: 2.608237]\n",
      "epoch:4 step:4227 [D loss: 0.562269, acc: 72.66%] [G loss: 2.996084]\n",
      "epoch:4 step:4228 [D loss: 0.642581, acc: 64.84%] [G loss: 2.828036]\n",
      "epoch:4 step:4229 [D loss: 0.575936, acc: 72.66%] [G loss: 2.819347]\n",
      "epoch:4 step:4230 [D loss: 0.589325, acc: 66.41%] [G loss: 2.564837]\n",
      "epoch:4 step:4231 [D loss: 0.598878, acc: 70.31%] [G loss: 2.835221]\n",
      "epoch:4 step:4232 [D loss: 0.518090, acc: 76.56%] [G loss: 2.846431]\n",
      "epoch:4 step:4233 [D loss: 0.737101, acc: 60.94%] [G loss: 2.609574]\n",
      "epoch:4 step:4234 [D loss: 0.548412, acc: 71.09%] [G loss: 2.580278]\n",
      "epoch:4 step:4235 [D loss: 0.551885, acc: 74.22%] [G loss: 2.483427]\n",
      "epoch:4 step:4236 [D loss: 0.647341, acc: 67.19%] [G loss: 2.740337]\n",
      "epoch:4 step:4237 [D loss: 0.574348, acc: 72.66%] [G loss: 2.768015]\n",
      "epoch:4 step:4238 [D loss: 0.563589, acc: 73.44%] [G loss: 2.902230]\n",
      "epoch:4 step:4239 [D loss: 0.512426, acc: 76.56%] [G loss: 2.962381]\n",
      "epoch:4 step:4240 [D loss: 0.521492, acc: 75.00%] [G loss: 2.636173]\n",
      "epoch:4 step:4241 [D loss: 0.609230, acc: 72.66%] [G loss: 2.779025]\n",
      "epoch:4 step:4242 [D loss: 0.494259, acc: 78.91%] [G loss: 3.333170]\n",
      "epoch:4 step:4243 [D loss: 0.612014, acc: 67.97%] [G loss: 2.817858]\n",
      "epoch:4 step:4244 [D loss: 0.636401, acc: 64.06%] [G loss: 2.777432]\n",
      "epoch:4 step:4245 [D loss: 0.568902, acc: 70.31%] [G loss: 2.942113]\n",
      "epoch:4 step:4246 [D loss: 0.514544, acc: 72.66%] [G loss: 3.130525]\n",
      "epoch:4 step:4247 [D loss: 0.546991, acc: 70.31%] [G loss: 2.923411]\n",
      "epoch:4 step:4248 [D loss: 0.712017, acc: 62.50%] [G loss: 2.672938]\n",
      "epoch:4 step:4249 [D loss: 0.645624, acc: 64.84%] [G loss: 2.582670]\n",
      "epoch:4 step:4250 [D loss: 0.684128, acc: 61.72%] [G loss: 2.580190]\n",
      "epoch:4 step:4251 [D loss: 0.590676, acc: 68.75%] [G loss: 2.660036]\n",
      "epoch:4 step:4252 [D loss: 0.535065, acc: 71.88%] [G loss: 2.763515]\n",
      "epoch:4 step:4253 [D loss: 0.598986, acc: 67.97%] [G loss: 2.577862]\n",
      "epoch:4 step:4254 [D loss: 0.627979, acc: 67.19%] [G loss: 2.491736]\n",
      "epoch:4 step:4255 [D loss: 0.633546, acc: 70.31%] [G loss: 2.950013]\n",
      "epoch:4 step:4256 [D loss: 0.548639, acc: 74.22%] [G loss: 2.933044]\n",
      "epoch:4 step:4257 [D loss: 0.632634, acc: 67.19%] [G loss: 2.689249]\n",
      "epoch:4 step:4258 [D loss: 0.652490, acc: 60.94%] [G loss: 2.274349]\n",
      "epoch:4 step:4259 [D loss: 0.635577, acc: 66.41%] [G loss: 2.487510]\n",
      "epoch:4 step:4260 [D loss: 0.621306, acc: 67.97%] [G loss: 2.663615]\n",
      "epoch:4 step:4261 [D loss: 0.577073, acc: 70.31%] [G loss: 2.590312]\n",
      "epoch:4 step:4262 [D loss: 0.584196, acc: 69.53%] [G loss: 2.653761]\n",
      "epoch:4 step:4263 [D loss: 0.546282, acc: 70.31%] [G loss: 2.636236]\n",
      "epoch:4 step:4264 [D loss: 0.562542, acc: 71.09%] [G loss: 2.823036]\n",
      "epoch:4 step:4265 [D loss: 0.579857, acc: 71.88%] [G loss: 2.699269]\n",
      "epoch:4 step:4266 [D loss: 0.611900, acc: 67.97%] [G loss: 2.660146]\n",
      "epoch:4 step:4267 [D loss: 0.491589, acc: 75.00%] [G loss: 2.758118]\n",
      "epoch:4 step:4268 [D loss: 0.512489, acc: 78.91%] [G loss: 2.812698]\n",
      "epoch:4 step:4269 [D loss: 0.482164, acc: 81.25%] [G loss: 2.842412]\n",
      "epoch:4 step:4270 [D loss: 0.564033, acc: 73.44%] [G loss: 2.957344]\n",
      "epoch:4 step:4271 [D loss: 0.507711, acc: 78.12%] [G loss: 3.196881]\n",
      "epoch:4 step:4272 [D loss: 0.582206, acc: 71.09%] [G loss: 2.726604]\n",
      "epoch:4 step:4273 [D loss: 0.648506, acc: 64.06%] [G loss: 2.662069]\n",
      "epoch:4 step:4274 [D loss: 0.601422, acc: 66.41%] [G loss: 2.965179]\n",
      "epoch:4 step:4275 [D loss: 0.589862, acc: 71.88%] [G loss: 2.716571]\n",
      "epoch:4 step:4276 [D loss: 0.609169, acc: 67.19%] [G loss: 2.665294]\n",
      "epoch:4 step:4277 [D loss: 0.615781, acc: 64.84%] [G loss: 2.504552]\n",
      "epoch:4 step:4278 [D loss: 0.537986, acc: 75.00%] [G loss: 2.703501]\n",
      "epoch:4 step:4279 [D loss: 0.656499, acc: 64.06%] [G loss: 2.783988]\n",
      "epoch:4 step:4280 [D loss: 0.518269, acc: 75.78%] [G loss: 2.846065]\n",
      "epoch:4 step:4281 [D loss: 0.612770, acc: 67.19%] [G loss: 2.890650]\n",
      "epoch:4 step:4282 [D loss: 0.491644, acc: 82.81%] [G loss: 2.985882]\n",
      "epoch:4 step:4283 [D loss: 0.574623, acc: 70.31%] [G loss: 2.716025]\n",
      "epoch:4 step:4284 [D loss: 0.543831, acc: 72.66%] [G loss: 3.006818]\n",
      "epoch:4 step:4285 [D loss: 0.594170, acc: 69.53%] [G loss: 2.564985]\n",
      "epoch:4 step:4286 [D loss: 0.641162, acc: 64.06%] [G loss: 2.602686]\n",
      "epoch:4 step:4287 [D loss: 0.439351, acc: 77.34%] [G loss: 2.819687]\n",
      "epoch:4 step:4288 [D loss: 0.597250, acc: 64.84%] [G loss: 2.602812]\n",
      "epoch:4 step:4289 [D loss: 0.587925, acc: 65.62%] [G loss: 2.857440]\n",
      "epoch:4 step:4290 [D loss: 0.642269, acc: 64.06%] [G loss: 2.684819]\n",
      "epoch:4 step:4291 [D loss: 0.589823, acc: 69.53%] [G loss: 2.622179]\n",
      "epoch:4 step:4292 [D loss: 0.549213, acc: 71.88%] [G loss: 2.794494]\n",
      "epoch:4 step:4293 [D loss: 0.676726, acc: 60.94%] [G loss: 2.859583]\n",
      "epoch:4 step:4294 [D loss: 0.513208, acc: 76.56%] [G loss: 3.294745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4295 [D loss: 0.539716, acc: 72.66%] [G loss: 3.076424]\n",
      "epoch:4 step:4296 [D loss: 0.576574, acc: 75.00%] [G loss: 2.784855]\n",
      "epoch:4 step:4297 [D loss: 0.534988, acc: 75.00%] [G loss: 3.086192]\n",
      "epoch:4 step:4298 [D loss: 0.518399, acc: 72.66%] [G loss: 2.794426]\n",
      "epoch:4 step:4299 [D loss: 0.597705, acc: 70.31%] [G loss: 2.784179]\n",
      "epoch:4 step:4300 [D loss: 0.617446, acc: 63.28%] [G loss: 2.743065]\n",
      "epoch:4 step:4301 [D loss: 0.547085, acc: 71.88%] [G loss: 3.011581]\n",
      "epoch:4 step:4302 [D loss: 0.490635, acc: 76.56%] [G loss: 3.198834]\n",
      "epoch:4 step:4303 [D loss: 0.575774, acc: 75.78%] [G loss: 2.968646]\n",
      "epoch:4 step:4304 [D loss: 0.496369, acc: 75.78%] [G loss: 3.308520]\n",
      "epoch:4 step:4305 [D loss: 0.549162, acc: 73.44%] [G loss: 3.233522]\n",
      "epoch:4 step:4306 [D loss: 0.594580, acc: 70.31%] [G loss: 3.105458]\n",
      "epoch:4 step:4307 [D loss: 0.738375, acc: 62.50%] [G loss: 2.661813]\n",
      "epoch:4 step:4308 [D loss: 0.539781, acc: 74.22%] [G loss: 2.689015]\n",
      "epoch:4 step:4309 [D loss: 0.545714, acc: 70.31%] [G loss: 2.752808]\n",
      "epoch:4 step:4310 [D loss: 0.574684, acc: 72.66%] [G loss: 2.737086]\n",
      "epoch:4 step:4311 [D loss: 0.656090, acc: 60.94%] [G loss: 2.598577]\n",
      "epoch:4 step:4312 [D loss: 0.616475, acc: 65.62%] [G loss: 3.117666]\n",
      "epoch:4 step:4313 [D loss: 0.549125, acc: 70.31%] [G loss: 2.667810]\n",
      "epoch:4 step:4314 [D loss: 0.667454, acc: 61.72%] [G loss: 2.453506]\n",
      "epoch:4 step:4315 [D loss: 0.586495, acc: 69.53%] [G loss: 2.793725]\n",
      "epoch:4 step:4316 [D loss: 0.596222, acc: 64.84%] [G loss: 2.788818]\n",
      "epoch:4 step:4317 [D loss: 0.581974, acc: 67.97%] [G loss: 2.691991]\n",
      "epoch:4 step:4318 [D loss: 0.597029, acc: 70.31%] [G loss: 2.967841]\n",
      "epoch:4 step:4319 [D loss: 0.550921, acc: 71.88%] [G loss: 2.894449]\n",
      "epoch:4 step:4320 [D loss: 0.542521, acc: 71.09%] [G loss: 2.357964]\n",
      "epoch:4 step:4321 [D loss: 0.639784, acc: 67.97%] [G loss: 2.537001]\n",
      "epoch:4 step:4322 [D loss: 0.532497, acc: 71.09%] [G loss: 2.696335]\n",
      "epoch:4 step:4323 [D loss: 0.595925, acc: 69.53%] [G loss: 2.698286]\n",
      "epoch:4 step:4324 [D loss: 0.676232, acc: 60.94%] [G loss: 2.444642]\n",
      "epoch:4 step:4325 [D loss: 0.561557, acc: 72.66%] [G loss: 2.740526]\n",
      "epoch:4 step:4326 [D loss: 0.595506, acc: 71.88%] [G loss: 2.674349]\n",
      "epoch:4 step:4327 [D loss: 0.584370, acc: 69.53%] [G loss: 2.629916]\n",
      "epoch:4 step:4328 [D loss: 0.562130, acc: 73.44%] [G loss: 2.734046]\n",
      "epoch:4 step:4329 [D loss: 0.633532, acc: 63.28%] [G loss: 2.490902]\n",
      "epoch:4 step:4330 [D loss: 0.548096, acc: 71.88%] [G loss: 2.675237]\n",
      "epoch:4 step:4331 [D loss: 0.646600, acc: 63.28%] [G loss: 2.930201]\n",
      "epoch:4 step:4332 [D loss: 0.625348, acc: 64.84%] [G loss: 2.632281]\n",
      "epoch:4 step:4333 [D loss: 0.628038, acc: 68.75%] [G loss: 2.847992]\n",
      "epoch:4 step:4334 [D loss: 0.655392, acc: 67.19%] [G loss: 2.638258]\n",
      "epoch:4 step:4335 [D loss: 0.578313, acc: 69.53%] [G loss: 2.917370]\n",
      "epoch:4 step:4336 [D loss: 0.582613, acc: 67.19%] [G loss: 2.745390]\n",
      "epoch:4 step:4337 [D loss: 0.522061, acc: 74.22%] [G loss: 3.039340]\n",
      "epoch:4 step:4338 [D loss: 0.612292, acc: 64.84%] [G loss: 2.547254]\n",
      "epoch:4 step:4339 [D loss: 0.638657, acc: 60.94%] [G loss: 2.494966]\n",
      "epoch:4 step:4340 [D loss: 0.491845, acc: 76.56%] [G loss: 3.048017]\n",
      "epoch:4 step:4341 [D loss: 0.636698, acc: 67.19%] [G loss: 2.554773]\n",
      "epoch:4 step:4342 [D loss: 0.578092, acc: 71.88%] [G loss: 2.506040]\n",
      "epoch:4 step:4343 [D loss: 0.544308, acc: 70.31%] [G loss: 2.724447]\n",
      "epoch:4 step:4344 [D loss: 0.532462, acc: 73.44%] [G loss: 2.626779]\n",
      "epoch:4 step:4345 [D loss: 0.608050, acc: 65.62%] [G loss: 2.599623]\n",
      "epoch:4 step:4346 [D loss: 0.482927, acc: 78.91%] [G loss: 3.127624]\n",
      "epoch:4 step:4347 [D loss: 0.634902, acc: 64.84%] [G loss: 2.514974]\n",
      "epoch:4 step:4348 [D loss: 0.603920, acc: 67.97%] [G loss: 2.685915]\n",
      "epoch:4 step:4349 [D loss: 0.596435, acc: 72.66%] [G loss: 2.452006]\n",
      "epoch:4 step:4350 [D loss: 0.587751, acc: 67.19%] [G loss: 3.104450]\n",
      "epoch:4 step:4351 [D loss: 0.518919, acc: 73.44%] [G loss: 3.373580]\n",
      "epoch:4 step:4352 [D loss: 0.586089, acc: 68.75%] [G loss: 2.445057]\n",
      "epoch:4 step:4353 [D loss: 0.546324, acc: 70.31%] [G loss: 3.069266]\n",
      "epoch:4 step:4354 [D loss: 0.530183, acc: 70.31%] [G loss: 2.603632]\n",
      "epoch:4 step:4355 [D loss: 0.628070, acc: 64.06%] [G loss: 2.477366]\n",
      "epoch:4 step:4356 [D loss: 0.591179, acc: 72.66%] [G loss: 2.577335]\n",
      "epoch:4 step:4357 [D loss: 0.582821, acc: 66.41%] [G loss: 2.903421]\n",
      "epoch:4 step:4358 [D loss: 0.677520, acc: 60.94%] [G loss: 2.639612]\n",
      "epoch:4 step:4359 [D loss: 0.520693, acc: 75.00%] [G loss: 2.843102]\n",
      "epoch:4 step:4360 [D loss: 0.607572, acc: 64.06%] [G loss: 2.778739]\n",
      "epoch:4 step:4361 [D loss: 0.579421, acc: 71.09%] [G loss: 2.697789]\n",
      "epoch:4 step:4362 [D loss: 0.596626, acc: 72.66%] [G loss: 2.424757]\n",
      "epoch:4 step:4363 [D loss: 0.626243, acc: 64.84%] [G loss: 2.285923]\n",
      "epoch:4 step:4364 [D loss: 0.636054, acc: 61.72%] [G loss: 2.490372]\n",
      "epoch:4 step:4365 [D loss: 0.599315, acc: 67.19%] [G loss: 2.748666]\n",
      "epoch:4 step:4366 [D loss: 0.613631, acc: 65.62%] [G loss: 2.696727]\n",
      "epoch:4 step:4367 [D loss: 0.586965, acc: 68.75%] [G loss: 2.852200]\n",
      "epoch:4 step:4368 [D loss: 0.606945, acc: 67.19%] [G loss: 2.520111]\n",
      "epoch:4 step:4369 [D loss: 0.550972, acc: 73.44%] [G loss: 2.495875]\n",
      "epoch:4 step:4370 [D loss: 0.677602, acc: 61.72%] [G loss: 2.378348]\n",
      "epoch:4 step:4371 [D loss: 0.624838, acc: 65.62%] [G loss: 2.487683]\n",
      "epoch:4 step:4372 [D loss: 0.534809, acc: 74.22%] [G loss: 2.611525]\n",
      "epoch:4 step:4373 [D loss: 0.560639, acc: 71.88%] [G loss: 2.760699]\n",
      "epoch:4 step:4374 [D loss: 0.617895, acc: 67.97%] [G loss: 2.781570]\n",
      "epoch:4 step:4375 [D loss: 0.561662, acc: 71.09%] [G loss: 2.597878]\n",
      "epoch:4 step:4376 [D loss: 0.547543, acc: 74.22%] [G loss: 2.492871]\n",
      "epoch:4 step:4377 [D loss: 0.585340, acc: 67.97%] [G loss: 2.709521]\n",
      "epoch:4 step:4378 [D loss: 0.500484, acc: 77.34%] [G loss: 2.972097]\n",
      "epoch:4 step:4379 [D loss: 0.597050, acc: 76.56%] [G loss: 2.788420]\n",
      "epoch:4 step:4380 [D loss: 0.560195, acc: 74.22%] [G loss: 2.834329]\n",
      "epoch:4 step:4381 [D loss: 0.599989, acc: 68.75%] [G loss: 2.815690]\n",
      "epoch:4 step:4382 [D loss: 0.561590, acc: 67.97%] [G loss: 2.684894]\n",
      "epoch:4 step:4383 [D loss: 0.539789, acc: 73.44%] [G loss: 2.921887]\n",
      "epoch:4 step:4384 [D loss: 0.573098, acc: 69.53%] [G loss: 2.660246]\n",
      "epoch:4 step:4385 [D loss: 0.530093, acc: 71.09%] [G loss: 3.150230]\n",
      "epoch:4 step:4386 [D loss: 0.510662, acc: 78.91%] [G loss: 2.770766]\n",
      "epoch:4 step:4387 [D loss: 0.468584, acc: 78.12%] [G loss: 3.291721]\n",
      "epoch:4 step:4388 [D loss: 0.521316, acc: 76.56%] [G loss: 2.939622]\n",
      "epoch:4 step:4389 [D loss: 0.481452, acc: 78.91%] [G loss: 2.957482]\n",
      "epoch:4 step:4390 [D loss: 0.512888, acc: 75.78%] [G loss: 3.194174]\n",
      "epoch:4 step:4391 [D loss: 0.520950, acc: 73.44%] [G loss: 2.842404]\n",
      "epoch:4 step:4392 [D loss: 0.589786, acc: 71.09%] [G loss: 3.041754]\n",
      "epoch:4 step:4393 [D loss: 0.591025, acc: 67.97%] [G loss: 2.776188]\n",
      "epoch:4 step:4394 [D loss: 0.565955, acc: 66.41%] [G loss: 2.812254]\n",
      "epoch:4 step:4395 [D loss: 0.569884, acc: 67.19%] [G loss: 2.977239]\n",
      "epoch:4 step:4396 [D loss: 0.514229, acc: 79.69%] [G loss: 3.444494]\n",
      "epoch:4 step:4397 [D loss: 0.577562, acc: 68.75%] [G loss: 3.012110]\n",
      "epoch:4 step:4398 [D loss: 0.523706, acc: 69.53%] [G loss: 3.066350]\n",
      "epoch:4 step:4399 [D loss: 0.655336, acc: 62.50%] [G loss: 3.027504]\n",
      "epoch:4 step:4400 [D loss: 0.655079, acc: 60.94%] [G loss: 2.625903]\n",
      "##############\n",
      "[2.9770003  1.5941403  6.92648587 5.42086888 4.28786286 6.18476064\n",
      " 5.30907761 5.0356365  5.4285419  3.84244024]\n",
      "##########\n",
      "epoch:4 step:4401 [D loss: 0.600734, acc: 69.53%] [G loss: 2.893976]\n",
      "epoch:4 step:4402 [D loss: 0.539025, acc: 70.31%] [G loss: 2.734052]\n",
      "epoch:4 step:4403 [D loss: 0.545149, acc: 73.44%] [G loss: 2.904124]\n",
      "epoch:4 step:4404 [D loss: 0.580724, acc: 69.53%] [G loss: 2.684780]\n",
      "epoch:4 step:4405 [D loss: 0.611344, acc: 69.53%] [G loss: 2.702354]\n",
      "epoch:4 step:4406 [D loss: 0.604503, acc: 69.53%] [G loss: 2.555278]\n",
      "epoch:4 step:4407 [D loss: 0.583572, acc: 66.41%] [G loss: 2.380184]\n",
      "epoch:4 step:4408 [D loss: 0.501642, acc: 75.78%] [G loss: 3.183362]\n",
      "epoch:4 step:4409 [D loss: 0.576175, acc: 70.31%] [G loss: 2.719599]\n",
      "epoch:4 step:4410 [D loss: 0.565236, acc: 71.09%] [G loss: 2.902330]\n",
      "epoch:4 step:4411 [D loss: 0.568437, acc: 75.00%] [G loss: 2.935361]\n",
      "epoch:4 step:4412 [D loss: 0.551516, acc: 73.44%] [G loss: 2.884701]\n",
      "epoch:4 step:4413 [D loss: 0.528615, acc: 74.22%] [G loss: 2.793955]\n",
      "epoch:4 step:4414 [D loss: 0.643007, acc: 68.75%] [G loss: 2.748442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4415 [D loss: 0.606431, acc: 64.84%] [G loss: 2.531098]\n",
      "epoch:4 step:4416 [D loss: 0.636201, acc: 65.62%] [G loss: 2.649337]\n",
      "epoch:4 step:4417 [D loss: 0.622972, acc: 65.62%] [G loss: 2.345359]\n",
      "epoch:4 step:4418 [D loss: 0.568411, acc: 65.62%] [G loss: 2.518062]\n",
      "epoch:4 step:4419 [D loss: 0.566837, acc: 69.53%] [G loss: 2.774222]\n",
      "epoch:4 step:4420 [D loss: 0.610990, acc: 63.28%] [G loss: 2.435396]\n",
      "epoch:4 step:4421 [D loss: 0.548438, acc: 67.97%] [G loss: 2.581131]\n",
      "epoch:4 step:4422 [D loss: 0.634982, acc: 67.97%] [G loss: 2.569125]\n",
      "epoch:4 step:4423 [D loss: 0.591436, acc: 67.97%] [G loss: 2.825706]\n",
      "epoch:4 step:4424 [D loss: 0.573175, acc: 67.19%] [G loss: 2.625539]\n",
      "epoch:4 step:4425 [D loss: 0.416494, acc: 83.59%] [G loss: 3.016810]\n",
      "epoch:4 step:4426 [D loss: 0.546734, acc: 71.09%] [G loss: 2.990520]\n",
      "epoch:4 step:4427 [D loss: 0.592945, acc: 64.84%] [G loss: 2.496013]\n",
      "epoch:4 step:4428 [D loss: 0.542101, acc: 74.22%] [G loss: 2.616995]\n",
      "epoch:4 step:4429 [D loss: 0.564503, acc: 67.97%] [G loss: 2.793194]\n",
      "epoch:4 step:4430 [D loss: 0.573475, acc: 74.22%] [G loss: 2.720604]\n",
      "epoch:4 step:4431 [D loss: 0.661818, acc: 63.28%] [G loss: 2.692648]\n",
      "epoch:4 step:4432 [D loss: 0.582463, acc: 68.75%] [G loss: 2.885604]\n",
      "epoch:4 step:4433 [D loss: 0.615534, acc: 65.62%] [G loss: 2.837713]\n",
      "epoch:4 step:4434 [D loss: 0.515696, acc: 78.91%] [G loss: 2.708201]\n",
      "epoch:4 step:4435 [D loss: 0.590587, acc: 73.44%] [G loss: 2.300333]\n",
      "epoch:4 step:4436 [D loss: 0.629933, acc: 66.41%] [G loss: 2.549667]\n",
      "epoch:4 step:4437 [D loss: 0.545918, acc: 74.22%] [G loss: 2.796532]\n",
      "epoch:4 step:4438 [D loss: 0.504253, acc: 72.66%] [G loss: 2.899333]\n",
      "epoch:4 step:4439 [D loss: 0.531075, acc: 73.44%] [G loss: 2.914284]\n",
      "epoch:4 step:4440 [D loss: 0.587209, acc: 65.62%] [G loss: 2.773422]\n",
      "epoch:4 step:4441 [D loss: 0.545440, acc: 79.69%] [G loss: 3.064616]\n",
      "epoch:4 step:4442 [D loss: 0.575501, acc: 70.31%] [G loss: 2.743119]\n",
      "epoch:4 step:4443 [D loss: 0.540064, acc: 71.09%] [G loss: 2.941812]\n",
      "epoch:4 step:4444 [D loss: 0.572052, acc: 70.31%] [G loss: 2.713426]\n",
      "epoch:4 step:4445 [D loss: 0.585694, acc: 69.53%] [G loss: 2.930279]\n",
      "epoch:4 step:4446 [D loss: 0.596482, acc: 67.97%] [G loss: 2.785920]\n",
      "epoch:4 step:4447 [D loss: 0.596333, acc: 67.97%] [G loss: 2.897408]\n",
      "epoch:4 step:4448 [D loss: 0.636200, acc: 66.41%] [G loss: 2.840334]\n",
      "epoch:4 step:4449 [D loss: 0.616794, acc: 68.75%] [G loss: 2.672341]\n",
      "epoch:4 step:4450 [D loss: 0.627596, acc: 66.41%] [G loss: 2.613072]\n",
      "epoch:4 step:4451 [D loss: 0.625520, acc: 71.09%] [G loss: 2.471288]\n",
      "epoch:4 step:4452 [D loss: 0.624354, acc: 66.41%] [G loss: 2.784178]\n",
      "epoch:4 step:4453 [D loss: 0.573546, acc: 70.31%] [G loss: 2.828759]\n",
      "epoch:4 step:4454 [D loss: 0.662809, acc: 59.38%] [G loss: 2.844809]\n",
      "epoch:4 step:4455 [D loss: 0.474540, acc: 75.78%] [G loss: 3.360759]\n",
      "epoch:4 step:4456 [D loss: 0.597474, acc: 67.97%] [G loss: 2.816425]\n",
      "epoch:4 step:4457 [D loss: 0.468836, acc: 80.47%] [G loss: 2.777231]\n",
      "epoch:4 step:4458 [D loss: 0.665471, acc: 64.84%] [G loss: 2.594590]\n",
      "epoch:4 step:4459 [D loss: 0.628859, acc: 66.41%] [G loss: 2.871271]\n",
      "epoch:4 step:4460 [D loss: 0.524918, acc: 74.22%] [G loss: 2.714513]\n",
      "epoch:4 step:4461 [D loss: 0.615758, acc: 62.50%] [G loss: 2.928507]\n",
      "epoch:4 step:4462 [D loss: 0.499827, acc: 75.78%] [G loss: 2.904028]\n",
      "epoch:4 step:4463 [D loss: 0.586046, acc: 65.62%] [G loss: 2.756316]\n",
      "epoch:4 step:4464 [D loss: 0.660168, acc: 61.72%] [G loss: 2.514625]\n",
      "epoch:4 step:4465 [D loss: 0.580353, acc: 73.44%] [G loss: 2.456922]\n",
      "epoch:4 step:4466 [D loss: 0.511621, acc: 73.44%] [G loss: 2.995544]\n",
      "epoch:4 step:4467 [D loss: 0.517100, acc: 81.25%] [G loss: 2.771784]\n",
      "epoch:4 step:4468 [D loss: 0.610148, acc: 67.19%] [G loss: 2.547562]\n",
      "epoch:4 step:4469 [D loss: 0.636226, acc: 64.84%] [G loss: 2.721488]\n",
      "epoch:4 step:4470 [D loss: 0.594431, acc: 69.53%] [G loss: 2.983355]\n",
      "epoch:4 step:4471 [D loss: 0.632757, acc: 69.53%] [G loss: 2.699047]\n",
      "epoch:4 step:4472 [D loss: 0.567470, acc: 67.19%] [G loss: 2.872380]\n",
      "epoch:4 step:4473 [D loss: 0.595585, acc: 66.41%] [G loss: 2.959999]\n",
      "epoch:4 step:4474 [D loss: 0.696845, acc: 64.06%] [G loss: 2.297663]\n",
      "epoch:4 step:4475 [D loss: 0.592837, acc: 67.19%] [G loss: 2.762250]\n",
      "epoch:4 step:4476 [D loss: 0.583955, acc: 68.75%] [G loss: 3.080902]\n",
      "epoch:4 step:4477 [D loss: 0.646480, acc: 63.28%] [G loss: 2.496330]\n",
      "epoch:4 step:4478 [D loss: 0.612981, acc: 66.41%] [G loss: 2.363087]\n",
      "epoch:4 step:4479 [D loss: 0.589785, acc: 66.41%] [G loss: 2.429822]\n",
      "epoch:4 step:4480 [D loss: 0.639908, acc: 63.28%] [G loss: 2.586237]\n",
      "epoch:4 step:4481 [D loss: 0.593760, acc: 71.88%] [G loss: 2.669055]\n",
      "epoch:4 step:4482 [D loss: 0.562156, acc: 66.41%] [G loss: 2.546163]\n",
      "epoch:4 step:4483 [D loss: 0.618831, acc: 67.97%] [G loss: 2.356947]\n",
      "epoch:4 step:4484 [D loss: 0.502046, acc: 71.88%] [G loss: 3.016647]\n",
      "epoch:4 step:4485 [D loss: 0.601651, acc: 68.75%] [G loss: 2.847738]\n",
      "epoch:4 step:4486 [D loss: 0.564203, acc: 70.31%] [G loss: 2.714580]\n",
      "epoch:4 step:4487 [D loss: 0.657817, acc: 63.28%] [G loss: 2.437946]\n",
      "epoch:4 step:4488 [D loss: 0.622339, acc: 64.06%] [G loss: 2.509965]\n",
      "epoch:4 step:4489 [D loss: 0.634799, acc: 66.41%] [G loss: 2.580570]\n",
      "epoch:4 step:4490 [D loss: 0.648256, acc: 67.97%] [G loss: 2.332253]\n",
      "epoch:4 step:4491 [D loss: 0.621593, acc: 60.94%] [G loss: 2.518558]\n",
      "epoch:4 step:4492 [D loss: 0.586084, acc: 72.66%] [G loss: 2.487714]\n",
      "epoch:4 step:4493 [D loss: 0.627685, acc: 64.84%] [G loss: 2.502265]\n",
      "epoch:4 step:4494 [D loss: 0.562007, acc: 71.09%] [G loss: 2.893888]\n",
      "epoch:4 step:4495 [D loss: 0.565038, acc: 68.75%] [G loss: 2.681260]\n",
      "epoch:4 step:4496 [D loss: 0.520452, acc: 78.12%] [G loss: 2.677830]\n",
      "epoch:4 step:4497 [D loss: 0.642557, acc: 62.50%] [G loss: 2.379105]\n",
      "epoch:4 step:4498 [D loss: 0.600012, acc: 67.97%] [G loss: 2.577413]\n",
      "epoch:4 step:4499 [D loss: 0.651390, acc: 56.25%] [G loss: 2.587534]\n",
      "epoch:4 step:4500 [D loss: 0.590234, acc: 68.75%] [G loss: 2.939521]\n",
      "epoch:4 step:4501 [D loss: 0.572827, acc: 75.00%] [G loss: 2.498646]\n",
      "epoch:4 step:4502 [D loss: 0.536295, acc: 74.22%] [G loss: 2.852584]\n",
      "epoch:4 step:4503 [D loss: 0.644476, acc: 59.38%] [G loss: 2.543419]\n",
      "epoch:4 step:4504 [D loss: 0.508278, acc: 78.12%] [G loss: 2.856154]\n",
      "epoch:4 step:4505 [D loss: 0.561825, acc: 64.84%] [G loss: 2.722686]\n",
      "epoch:4 step:4506 [D loss: 0.603468, acc: 68.75%] [G loss: 2.534652]\n",
      "epoch:4 step:4507 [D loss: 0.605063, acc: 66.41%] [G loss: 2.553681]\n",
      "epoch:4 step:4508 [D loss: 0.540693, acc: 71.09%] [G loss: 2.662753]\n",
      "epoch:4 step:4509 [D loss: 0.578081, acc: 75.78%] [G loss: 2.817010]\n",
      "epoch:4 step:4510 [D loss: 0.548181, acc: 71.09%] [G loss: 2.266638]\n",
      "epoch:4 step:4511 [D loss: 0.603025, acc: 67.19%] [G loss: 2.811411]\n",
      "epoch:4 step:4512 [D loss: 0.578915, acc: 70.31%] [G loss: 2.682337]\n",
      "epoch:4 step:4513 [D loss: 0.709912, acc: 56.25%] [G loss: 2.581771]\n",
      "epoch:4 step:4514 [D loss: 0.639827, acc: 64.06%] [G loss: 2.237819]\n",
      "epoch:4 step:4515 [D loss: 0.543876, acc: 71.09%] [G loss: 2.556123]\n",
      "epoch:4 step:4516 [D loss: 0.629852, acc: 61.72%] [G loss: 2.620882]\n",
      "epoch:4 step:4517 [D loss: 0.515352, acc: 72.66%] [G loss: 3.103621]\n",
      "epoch:4 step:4518 [D loss: 0.691325, acc: 61.72%] [G loss: 2.403690]\n",
      "epoch:4 step:4519 [D loss: 0.509534, acc: 71.88%] [G loss: 2.713787]\n",
      "epoch:4 step:4520 [D loss: 0.577948, acc: 64.84%] [G loss: 2.730271]\n",
      "epoch:4 step:4521 [D loss: 0.544993, acc: 71.88%] [G loss: 2.766221]\n",
      "epoch:4 step:4522 [D loss: 0.659495, acc: 63.28%] [G loss: 2.714266]\n",
      "epoch:4 step:4523 [D loss: 0.515169, acc: 75.78%] [G loss: 2.734219]\n",
      "epoch:4 step:4524 [D loss: 0.555715, acc: 67.97%] [G loss: 2.585990]\n",
      "epoch:4 step:4525 [D loss: 0.503431, acc: 77.34%] [G loss: 2.698732]\n",
      "epoch:4 step:4526 [D loss: 0.612217, acc: 69.53%] [G loss: 2.615496]\n",
      "epoch:4 step:4527 [D loss: 0.576581, acc: 71.09%] [G loss: 3.009665]\n",
      "epoch:4 step:4528 [D loss: 0.568480, acc: 69.53%] [G loss: 2.846489]\n",
      "epoch:4 step:4529 [D loss: 0.463342, acc: 79.69%] [G loss: 3.168674]\n",
      "epoch:4 step:4530 [D loss: 0.585321, acc: 75.78%] [G loss: 2.659152]\n",
      "epoch:4 step:4531 [D loss: 0.605323, acc: 64.06%] [G loss: 2.615809]\n",
      "epoch:4 step:4532 [D loss: 0.636057, acc: 68.75%] [G loss: 2.300668]\n",
      "epoch:4 step:4533 [D loss: 0.485964, acc: 82.03%] [G loss: 2.760697]\n",
      "epoch:4 step:4534 [D loss: 0.531214, acc: 79.69%] [G loss: 2.986778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4535 [D loss: 0.566976, acc: 76.56%] [G loss: 2.966139]\n",
      "epoch:4 step:4536 [D loss: 0.743100, acc: 57.03%] [G loss: 2.385134]\n",
      "epoch:4 step:4537 [D loss: 0.614085, acc: 61.72%] [G loss: 2.785835]\n",
      "epoch:4 step:4538 [D loss: 0.582835, acc: 69.53%] [G loss: 2.677850]\n",
      "epoch:4 step:4539 [D loss: 0.554697, acc: 73.44%] [G loss: 2.666432]\n",
      "epoch:4 step:4540 [D loss: 0.505580, acc: 74.22%] [G loss: 3.106707]\n",
      "epoch:4 step:4541 [D loss: 0.624824, acc: 67.97%] [G loss: 2.604853]\n",
      "epoch:4 step:4542 [D loss: 0.690667, acc: 57.81%] [G loss: 2.366718]\n",
      "epoch:4 step:4543 [D loss: 0.588754, acc: 71.09%] [G loss: 2.637250]\n",
      "epoch:4 step:4544 [D loss: 0.598755, acc: 67.19%] [G loss: 2.882257]\n",
      "epoch:4 step:4545 [D loss: 0.605876, acc: 62.50%] [G loss: 2.312997]\n",
      "epoch:4 step:4546 [D loss: 0.527711, acc: 71.88%] [G loss: 2.558363]\n",
      "epoch:4 step:4547 [D loss: 0.687448, acc: 65.62%] [G loss: 2.567083]\n",
      "epoch:4 step:4548 [D loss: 0.553256, acc: 76.56%] [G loss: 2.571578]\n",
      "epoch:4 step:4549 [D loss: 0.568756, acc: 69.53%] [G loss: 2.437822]\n",
      "epoch:4 step:4550 [D loss: 0.586906, acc: 75.00%] [G loss: 2.918703]\n",
      "epoch:4 step:4551 [D loss: 0.482845, acc: 78.91%] [G loss: 2.676753]\n",
      "epoch:4 step:4552 [D loss: 0.517161, acc: 78.12%] [G loss: 2.752769]\n",
      "epoch:4 step:4553 [D loss: 0.620609, acc: 71.09%] [G loss: 2.902443]\n",
      "epoch:4 step:4554 [D loss: 0.530015, acc: 71.88%] [G loss: 3.092841]\n",
      "epoch:4 step:4555 [D loss: 0.554781, acc: 72.66%] [G loss: 2.839656]\n",
      "epoch:4 step:4556 [D loss: 0.551324, acc: 73.44%] [G loss: 2.898513]\n",
      "epoch:4 step:4557 [D loss: 0.573332, acc: 69.53%] [G loss: 3.091990]\n",
      "epoch:4 step:4558 [D loss: 0.616874, acc: 70.31%] [G loss: 2.755749]\n",
      "epoch:4 step:4559 [D loss: 0.615344, acc: 67.97%] [G loss: 2.592613]\n",
      "epoch:4 step:4560 [D loss: 0.631125, acc: 63.28%] [G loss: 2.751896]\n",
      "epoch:4 step:4561 [D loss: 0.564184, acc: 70.31%] [G loss: 2.642410]\n",
      "epoch:4 step:4562 [D loss: 0.561551, acc: 75.00%] [G loss: 2.779642]\n",
      "epoch:4 step:4563 [D loss: 0.521666, acc: 72.66%] [G loss: 2.961540]\n",
      "epoch:4 step:4564 [D loss: 0.569813, acc: 71.09%] [G loss: 3.105251]\n",
      "epoch:4 step:4565 [D loss: 0.618758, acc: 64.84%] [G loss: 2.521316]\n",
      "epoch:4 step:4566 [D loss: 0.631844, acc: 64.06%] [G loss: 2.617642]\n",
      "epoch:4 step:4567 [D loss: 0.537367, acc: 72.66%] [G loss: 3.006873]\n",
      "epoch:4 step:4568 [D loss: 0.696236, acc: 62.50%] [G loss: 2.337227]\n",
      "epoch:4 step:4569 [D loss: 0.565090, acc: 71.88%] [G loss: 2.871312]\n",
      "epoch:4 step:4570 [D loss: 0.479033, acc: 78.91%] [G loss: 3.059468]\n",
      "epoch:4 step:4571 [D loss: 0.601562, acc: 62.50%] [G loss: 2.665121]\n",
      "epoch:4 step:4572 [D loss: 0.634018, acc: 66.41%] [G loss: 2.565428]\n",
      "epoch:4 step:4573 [D loss: 0.571505, acc: 70.31%] [G loss: 2.592870]\n",
      "epoch:4 step:4574 [D loss: 0.594053, acc: 65.62%] [G loss: 2.550543]\n",
      "epoch:4 step:4575 [D loss: 0.656696, acc: 65.62%] [G loss: 2.497293]\n",
      "epoch:4 step:4576 [D loss: 0.641569, acc: 64.06%] [G loss: 2.231917]\n",
      "epoch:4 step:4577 [D loss: 0.653535, acc: 60.94%] [G loss: 2.619543]\n",
      "epoch:4 step:4578 [D loss: 0.610038, acc: 66.41%] [G loss: 2.451643]\n",
      "epoch:4 step:4579 [D loss: 0.556112, acc: 64.84%] [G loss: 2.643839]\n",
      "epoch:4 step:4580 [D loss: 0.645236, acc: 62.50%] [G loss: 2.346908]\n",
      "epoch:4 step:4581 [D loss: 0.566869, acc: 69.53%] [G loss: 2.547369]\n",
      "epoch:4 step:4582 [D loss: 0.605086, acc: 69.53%] [G loss: 2.550291]\n",
      "epoch:4 step:4583 [D loss: 0.608003, acc: 68.75%] [G loss: 2.395317]\n",
      "epoch:4 step:4584 [D loss: 0.584798, acc: 65.62%] [G loss: 2.345067]\n",
      "epoch:4 step:4585 [D loss: 0.559632, acc: 72.66%] [G loss: 2.879944]\n",
      "epoch:4 step:4586 [D loss: 0.570066, acc: 67.97%] [G loss: 2.729412]\n",
      "epoch:4 step:4587 [D loss: 0.604998, acc: 67.19%] [G loss: 2.470781]\n",
      "epoch:4 step:4588 [D loss: 0.547102, acc: 67.97%] [G loss: 2.322829]\n",
      "epoch:4 step:4589 [D loss: 0.528387, acc: 73.44%] [G loss: 2.767235]\n",
      "epoch:4 step:4590 [D loss: 0.498403, acc: 78.91%] [G loss: 2.608491]\n",
      "epoch:4 step:4591 [D loss: 0.707582, acc: 64.84%] [G loss: 2.758102]\n",
      "epoch:4 step:4592 [D loss: 0.643889, acc: 60.94%] [G loss: 2.431325]\n",
      "epoch:4 step:4593 [D loss: 0.515165, acc: 77.34%] [G loss: 2.663437]\n",
      "epoch:4 step:4594 [D loss: 0.616106, acc: 68.75%] [G loss: 2.520477]\n",
      "epoch:4 step:4595 [D loss: 0.552379, acc: 75.00%] [G loss: 2.598081]\n",
      "epoch:4 step:4596 [D loss: 0.516463, acc: 77.34%] [G loss: 2.847308]\n",
      "epoch:4 step:4597 [D loss: 0.564152, acc: 66.41%] [G loss: 2.514475]\n",
      "epoch:4 step:4598 [D loss: 0.658510, acc: 60.16%] [G loss: 2.370432]\n",
      "epoch:4 step:4599 [D loss: 0.635033, acc: 66.41%] [G loss: 2.318182]\n",
      "epoch:4 step:4600 [D loss: 0.594225, acc: 70.31%] [G loss: 2.492111]\n",
      "##############\n",
      "[2.94569874 1.55729554 6.80032054 5.38567942 4.21262778 6.25760761\n",
      " 5.07930943 4.98199187 5.41744074 3.59369922]\n",
      "##########\n",
      "epoch:4 step:4601 [D loss: 0.526499, acc: 74.22%] [G loss: 2.953113]\n",
      "epoch:4 step:4602 [D loss: 0.493486, acc: 75.00%] [G loss: 2.930124]\n",
      "epoch:4 step:4603 [D loss: 0.525632, acc: 75.00%] [G loss: 2.597437]\n",
      "epoch:4 step:4604 [D loss: 0.612792, acc: 68.75%] [G loss: 2.501365]\n",
      "epoch:4 step:4605 [D loss: 0.560610, acc: 72.66%] [G loss: 2.839002]\n",
      "epoch:4 step:4606 [D loss: 0.642112, acc: 65.62%] [G loss: 2.756673]\n",
      "epoch:4 step:4607 [D loss: 0.560902, acc: 71.09%] [G loss: 2.647868]\n",
      "epoch:4 step:4608 [D loss: 0.507357, acc: 75.78%] [G loss: 3.007997]\n",
      "epoch:4 step:4609 [D loss: 0.573577, acc: 70.31%] [G loss: 2.375990]\n",
      "epoch:4 step:4610 [D loss: 0.555757, acc: 78.12%] [G loss: 2.662105]\n",
      "epoch:4 step:4611 [D loss: 0.574053, acc: 71.88%] [G loss: 2.918578]\n",
      "epoch:4 step:4612 [D loss: 0.546916, acc: 70.31%] [G loss: 2.744425]\n",
      "epoch:4 step:4613 [D loss: 0.585913, acc: 69.53%] [G loss: 2.774909]\n",
      "epoch:4 step:4614 [D loss: 0.550179, acc: 72.66%] [G loss: 2.589856]\n",
      "epoch:4 step:4615 [D loss: 0.578185, acc: 69.53%] [G loss: 2.480715]\n",
      "epoch:4 step:4616 [D loss: 0.581663, acc: 69.53%] [G loss: 2.888786]\n",
      "epoch:4 step:4617 [D loss: 0.643796, acc: 66.41%] [G loss: 2.470198]\n",
      "epoch:4 step:4618 [D loss: 0.551773, acc: 68.75%] [G loss: 2.718081]\n",
      "epoch:4 step:4619 [D loss: 0.605199, acc: 69.53%] [G loss: 2.806694]\n",
      "epoch:4 step:4620 [D loss: 0.549384, acc: 71.88%] [G loss: 2.767906]\n",
      "epoch:4 step:4621 [D loss: 0.601557, acc: 65.62%] [G loss: 2.886443]\n",
      "epoch:4 step:4622 [D loss: 0.558750, acc: 68.75%] [G loss: 2.487956]\n",
      "epoch:4 step:4623 [D loss: 0.571077, acc: 71.88%] [G loss: 2.759441]\n",
      "epoch:4 step:4624 [D loss: 0.498214, acc: 75.00%] [G loss: 2.816524]\n",
      "epoch:4 step:4625 [D loss: 0.595528, acc: 65.62%] [G loss: 2.820648]\n",
      "epoch:4 step:4626 [D loss: 0.606140, acc: 67.97%] [G loss: 2.643224]\n",
      "epoch:4 step:4627 [D loss: 0.567675, acc: 72.66%] [G loss: 2.822131]\n",
      "epoch:4 step:4628 [D loss: 0.580550, acc: 75.00%] [G loss: 2.636874]\n",
      "epoch:4 step:4629 [D loss: 0.559359, acc: 71.88%] [G loss: 2.544311]\n",
      "epoch:4 step:4630 [D loss: 0.545108, acc: 73.44%] [G loss: 2.877164]\n",
      "epoch:4 step:4631 [D loss: 0.604563, acc: 63.28%] [G loss: 2.709535]\n",
      "epoch:4 step:4632 [D loss: 0.513837, acc: 76.56%] [G loss: 2.952634]\n",
      "epoch:4 step:4633 [D loss: 0.508183, acc: 74.22%] [G loss: 3.084669]\n",
      "epoch:4 step:4634 [D loss: 0.534315, acc: 72.66%] [G loss: 3.139486]\n",
      "epoch:4 step:4635 [D loss: 0.490624, acc: 77.34%] [G loss: 2.995926]\n",
      "epoch:4 step:4636 [D loss: 0.531321, acc: 76.56%] [G loss: 3.297924]\n",
      "epoch:4 step:4637 [D loss: 0.625524, acc: 70.31%] [G loss: 2.919942]\n",
      "epoch:4 step:4638 [D loss: 0.472826, acc: 76.56%] [G loss: 2.829009]\n",
      "epoch:4 step:4639 [D loss: 0.614451, acc: 67.19%] [G loss: 2.472621]\n",
      "epoch:4 step:4640 [D loss: 0.658199, acc: 65.62%] [G loss: 2.703886]\n",
      "epoch:4 step:4641 [D loss: 0.591727, acc: 67.19%] [G loss: 2.797856]\n",
      "epoch:4 step:4642 [D loss: 0.542821, acc: 72.66%] [G loss: 2.858158]\n",
      "epoch:4 step:4643 [D loss: 0.598433, acc: 68.75%] [G loss: 2.764642]\n",
      "epoch:4 step:4644 [D loss: 0.569148, acc: 68.75%] [G loss: 2.738718]\n",
      "epoch:4 step:4645 [D loss: 0.590571, acc: 68.75%] [G loss: 2.635187]\n",
      "epoch:4 step:4646 [D loss: 0.522707, acc: 77.34%] [G loss: 3.367571]\n",
      "epoch:4 step:4647 [D loss: 0.631338, acc: 66.41%] [G loss: 2.820629]\n",
      "epoch:4 step:4648 [D loss: 0.565172, acc: 65.62%] [G loss: 2.936955]\n",
      "epoch:4 step:4649 [D loss: 0.587009, acc: 71.09%] [G loss: 2.626906]\n",
      "epoch:4 step:4650 [D loss: 0.656569, acc: 61.72%] [G loss: 2.246076]\n",
      "epoch:4 step:4651 [D loss: 0.625843, acc: 64.84%] [G loss: 2.434264]\n",
      "epoch:4 step:4652 [D loss: 0.531098, acc: 76.56%] [G loss: 2.907178]\n",
      "epoch:4 step:4653 [D loss: 0.565610, acc: 68.75%] [G loss: 2.720530]\n",
      "epoch:4 step:4654 [D loss: 0.629110, acc: 69.53%] [G loss: 2.839817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4655 [D loss: 0.591488, acc: 71.09%] [G loss: 2.599044]\n",
      "epoch:4 step:4656 [D loss: 0.661187, acc: 61.72%] [G loss: 2.373981]\n",
      "epoch:4 step:4657 [D loss: 0.613842, acc: 68.75%] [G loss: 2.431166]\n",
      "epoch:4 step:4658 [D loss: 0.618156, acc: 63.28%] [G loss: 2.600830]\n",
      "epoch:4 step:4659 [D loss: 0.569976, acc: 67.97%] [G loss: 2.762483]\n",
      "epoch:4 step:4660 [D loss: 0.615952, acc: 71.09%] [G loss: 3.263689]\n",
      "epoch:4 step:4661 [D loss: 0.708453, acc: 59.38%] [G loss: 2.380828]\n",
      "epoch:4 step:4662 [D loss: 0.598805, acc: 65.62%] [G loss: 2.589970]\n",
      "epoch:4 step:4663 [D loss: 0.567003, acc: 69.53%] [G loss: 2.386392]\n",
      "epoch:4 step:4664 [D loss: 0.595118, acc: 63.28%] [G loss: 2.481169]\n",
      "epoch:4 step:4665 [D loss: 0.607953, acc: 62.50%] [G loss: 2.381667]\n",
      "epoch:4 step:4666 [D loss: 0.551941, acc: 71.09%] [G loss: 2.904330]\n",
      "epoch:4 step:4667 [D loss: 0.606479, acc: 66.41%] [G loss: 2.799793]\n",
      "epoch:4 step:4668 [D loss: 0.576150, acc: 71.88%] [G loss: 2.814414]\n",
      "epoch:4 step:4669 [D loss: 0.484893, acc: 75.78%] [G loss: 2.839287]\n",
      "epoch:4 step:4670 [D loss: 0.566819, acc: 71.88%] [G loss: 2.433098]\n",
      "epoch:4 step:4671 [D loss: 0.466181, acc: 82.81%] [G loss: 3.081330]\n",
      "epoch:4 step:4672 [D loss: 0.512036, acc: 78.12%] [G loss: 3.101470]\n",
      "epoch:4 step:4673 [D loss: 0.522340, acc: 75.78%] [G loss: 3.021676]\n",
      "epoch:4 step:4674 [D loss: 0.635911, acc: 64.84%] [G loss: 2.671769]\n",
      "epoch:4 step:4675 [D loss: 0.499939, acc: 71.88%] [G loss: 2.570586]\n",
      "epoch:4 step:4676 [D loss: 0.884947, acc: 49.22%] [G loss: 2.981148]\n",
      "epoch:4 step:4677 [D loss: 0.665942, acc: 60.16%] [G loss: 3.081394]\n",
      "epoch:4 step:4678 [D loss: 0.587305, acc: 67.97%] [G loss: 2.746188]\n",
      "epoch:4 step:4679 [D loss: 0.575483, acc: 68.75%] [G loss: 2.765724]\n",
      "epoch:4 step:4680 [D loss: 0.522454, acc: 77.34%] [G loss: 2.561452]\n",
      "epoch:4 step:4681 [D loss: 0.522463, acc: 74.22%] [G loss: 2.428251]\n",
      "epoch:4 step:4682 [D loss: 0.601963, acc: 69.53%] [G loss: 2.658688]\n",
      "epoch:4 step:4683 [D loss: 0.604862, acc: 67.97%] [G loss: 2.739751]\n",
      "epoch:4 step:4684 [D loss: 0.509799, acc: 71.09%] [G loss: 2.670964]\n",
      "epoch:4 step:4685 [D loss: 0.516985, acc: 75.00%] [G loss: 3.488244]\n",
      "epoch:5 step:4686 [D loss: 0.546445, acc: 70.31%] [G loss: 2.812775]\n",
      "epoch:5 step:4687 [D loss: 0.574800, acc: 69.53%] [G loss: 2.842224]\n",
      "epoch:5 step:4688 [D loss: 0.620540, acc: 67.97%] [G loss: 2.534445]\n",
      "epoch:5 step:4689 [D loss: 0.540742, acc: 71.09%] [G loss: 2.705623]\n",
      "epoch:5 step:4690 [D loss: 0.562162, acc: 75.00%] [G loss: 2.947540]\n",
      "epoch:5 step:4691 [D loss: 0.588757, acc: 71.88%] [G loss: 2.768816]\n",
      "epoch:5 step:4692 [D loss: 0.607843, acc: 74.22%] [G loss: 2.790813]\n",
      "epoch:5 step:4693 [D loss: 0.528599, acc: 76.56%] [G loss: 2.943048]\n",
      "epoch:5 step:4694 [D loss: 0.591605, acc: 70.31%] [G loss: 2.964211]\n",
      "epoch:5 step:4695 [D loss: 0.566011, acc: 67.97%] [G loss: 3.096437]\n",
      "epoch:5 step:4696 [D loss: 0.630179, acc: 72.66%] [G loss: 2.584688]\n",
      "epoch:5 step:4697 [D loss: 0.545193, acc: 69.53%] [G loss: 2.641788]\n",
      "epoch:5 step:4698 [D loss: 0.493681, acc: 78.12%] [G loss: 2.690217]\n",
      "epoch:5 step:4699 [D loss: 0.575777, acc: 67.19%] [G loss: 2.520100]\n",
      "epoch:5 step:4700 [D loss: 0.511771, acc: 78.12%] [G loss: 3.128242]\n",
      "epoch:5 step:4701 [D loss: 0.508461, acc: 72.66%] [G loss: 2.931117]\n",
      "epoch:5 step:4702 [D loss: 0.656480, acc: 66.41%] [G loss: 2.659152]\n",
      "epoch:5 step:4703 [D loss: 0.611883, acc: 69.53%] [G loss: 2.405647]\n",
      "epoch:5 step:4704 [D loss: 0.590146, acc: 67.97%] [G loss: 2.550438]\n",
      "epoch:5 step:4705 [D loss: 0.620391, acc: 66.41%] [G loss: 2.404277]\n",
      "epoch:5 step:4706 [D loss: 0.708842, acc: 60.94%] [G loss: 2.443778]\n",
      "epoch:5 step:4707 [D loss: 0.654979, acc: 65.62%] [G loss: 2.647114]\n",
      "epoch:5 step:4708 [D loss: 0.640562, acc: 66.41%] [G loss: 2.762747]\n",
      "epoch:5 step:4709 [D loss: 0.573682, acc: 65.62%] [G loss: 2.755339]\n",
      "epoch:5 step:4710 [D loss: 0.500506, acc: 75.78%] [G loss: 2.800705]\n",
      "epoch:5 step:4711 [D loss: 0.564365, acc: 71.88%] [G loss: 2.582519]\n",
      "epoch:5 step:4712 [D loss: 0.639810, acc: 65.62%] [G loss: 2.466944]\n",
      "epoch:5 step:4713 [D loss: 0.606101, acc: 71.09%] [G loss: 2.590215]\n",
      "epoch:5 step:4714 [D loss: 0.549398, acc: 66.41%] [G loss: 2.631399]\n",
      "epoch:5 step:4715 [D loss: 0.667909, acc: 62.50%] [G loss: 2.301194]\n",
      "epoch:5 step:4716 [D loss: 0.631005, acc: 63.28%] [G loss: 2.312445]\n",
      "epoch:5 step:4717 [D loss: 0.588956, acc: 62.50%] [G loss: 2.478216]\n",
      "epoch:5 step:4718 [D loss: 0.621098, acc: 63.28%] [G loss: 2.406069]\n",
      "epoch:5 step:4719 [D loss: 0.517072, acc: 75.78%] [G loss: 2.432130]\n",
      "epoch:5 step:4720 [D loss: 0.526574, acc: 78.91%] [G loss: 2.622451]\n",
      "epoch:5 step:4721 [D loss: 0.533035, acc: 73.44%] [G loss: 2.764439]\n",
      "epoch:5 step:4722 [D loss: 0.478273, acc: 78.91%] [G loss: 3.032688]\n",
      "epoch:5 step:4723 [D loss: 0.649242, acc: 63.28%] [G loss: 2.817275]\n",
      "epoch:5 step:4724 [D loss: 0.560507, acc: 75.00%] [G loss: 3.029859]\n",
      "epoch:5 step:4725 [D loss: 0.560869, acc: 69.53%] [G loss: 3.095771]\n",
      "epoch:5 step:4726 [D loss: 0.541950, acc: 71.88%] [G loss: 2.487537]\n",
      "epoch:5 step:4727 [D loss: 0.552271, acc: 68.75%] [G loss: 2.720186]\n",
      "epoch:5 step:4728 [D loss: 0.573165, acc: 74.22%] [G loss: 2.789518]\n",
      "epoch:5 step:4729 [D loss: 0.631663, acc: 69.53%] [G loss: 2.343963]\n",
      "epoch:5 step:4730 [D loss: 0.725681, acc: 57.81%] [G loss: 2.395620]\n",
      "epoch:5 step:4731 [D loss: 0.612767, acc: 65.62%] [G loss: 2.392400]\n",
      "epoch:5 step:4732 [D loss: 0.528779, acc: 71.88%] [G loss: 2.674652]\n",
      "epoch:5 step:4733 [D loss: 0.581230, acc: 69.53%] [G loss: 2.678705]\n",
      "epoch:5 step:4734 [D loss: 0.560978, acc: 70.31%] [G loss: 2.694977]\n",
      "epoch:5 step:4735 [D loss: 0.617929, acc: 64.84%] [G loss: 2.694444]\n",
      "epoch:5 step:4736 [D loss: 0.612231, acc: 69.53%] [G loss: 2.393046]\n",
      "epoch:5 step:4737 [D loss: 0.582656, acc: 66.41%] [G loss: 2.816727]\n",
      "epoch:5 step:4738 [D loss: 0.592363, acc: 65.62%] [G loss: 2.569912]\n",
      "epoch:5 step:4739 [D loss: 0.590577, acc: 73.44%] [G loss: 2.617728]\n",
      "epoch:5 step:4740 [D loss: 0.469950, acc: 78.91%] [G loss: 2.854282]\n",
      "epoch:5 step:4741 [D loss: 0.633699, acc: 63.28%] [G loss: 2.624684]\n",
      "epoch:5 step:4742 [D loss: 0.573901, acc: 72.66%] [G loss: 2.565086]\n",
      "epoch:5 step:4743 [D loss: 0.545610, acc: 76.56%] [G loss: 2.651774]\n",
      "epoch:5 step:4744 [D loss: 0.604599, acc: 67.97%] [G loss: 2.680501]\n",
      "epoch:5 step:4745 [D loss: 0.540865, acc: 75.00%] [G loss: 2.695718]\n",
      "epoch:5 step:4746 [D loss: 0.615186, acc: 71.09%] [G loss: 2.691077]\n",
      "epoch:5 step:4747 [D loss: 0.646302, acc: 61.72%] [G loss: 2.416644]\n",
      "epoch:5 step:4748 [D loss: 0.584193, acc: 67.97%] [G loss: 2.541457]\n",
      "epoch:5 step:4749 [D loss: 0.617224, acc: 71.09%] [G loss: 2.477883]\n",
      "epoch:5 step:4750 [D loss: 0.592939, acc: 70.31%] [G loss: 2.667247]\n",
      "epoch:5 step:4751 [D loss: 0.698921, acc: 67.97%] [G loss: 2.346165]\n",
      "epoch:5 step:4752 [D loss: 0.570977, acc: 71.88%] [G loss: 2.671042]\n",
      "epoch:5 step:4753 [D loss: 0.534361, acc: 71.09%] [G loss: 2.631646]\n",
      "epoch:5 step:4754 [D loss: 0.634022, acc: 60.16%] [G loss: 2.422843]\n",
      "epoch:5 step:4755 [D loss: 0.521334, acc: 74.22%] [G loss: 2.836898]\n",
      "epoch:5 step:4756 [D loss: 0.605147, acc: 62.50%] [G loss: 2.651418]\n",
      "epoch:5 step:4757 [D loss: 0.528707, acc: 75.00%] [G loss: 2.755927]\n",
      "epoch:5 step:4758 [D loss: 0.645951, acc: 67.19%] [G loss: 2.436492]\n",
      "epoch:5 step:4759 [D loss: 0.624544, acc: 68.75%] [G loss: 2.741885]\n",
      "epoch:5 step:4760 [D loss: 0.460040, acc: 80.47%] [G loss: 2.961505]\n",
      "epoch:5 step:4761 [D loss: 0.570544, acc: 66.41%] [G loss: 2.790967]\n",
      "epoch:5 step:4762 [D loss: 0.571631, acc: 75.78%] [G loss: 2.797801]\n",
      "epoch:5 step:4763 [D loss: 0.629245, acc: 66.41%] [G loss: 2.501602]\n",
      "epoch:5 step:4764 [D loss: 0.565430, acc: 67.19%] [G loss: 2.437799]\n",
      "epoch:5 step:4765 [D loss: 0.590944, acc: 70.31%] [G loss: 2.488580]\n",
      "epoch:5 step:4766 [D loss: 0.655716, acc: 63.28%] [G loss: 2.509837]\n",
      "epoch:5 step:4767 [D loss: 0.521916, acc: 73.44%] [G loss: 2.780118]\n",
      "epoch:5 step:4768 [D loss: 0.502465, acc: 77.34%] [G loss: 2.774449]\n",
      "epoch:5 step:4769 [D loss: 0.563341, acc: 71.88%] [G loss: 2.645895]\n",
      "epoch:5 step:4770 [D loss: 0.665558, acc: 60.16%] [G loss: 2.450310]\n",
      "epoch:5 step:4771 [D loss: 0.516999, acc: 76.56%] [G loss: 2.615576]\n",
      "epoch:5 step:4772 [D loss: 0.619253, acc: 67.97%] [G loss: 2.648135]\n",
      "epoch:5 step:4773 [D loss: 0.508394, acc: 75.00%] [G loss: 2.673806]\n",
      "epoch:5 step:4774 [D loss: 0.580469, acc: 71.88%] [G loss: 2.529611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4775 [D loss: 0.548757, acc: 76.56%] [G loss: 2.686374]\n",
      "epoch:5 step:4776 [D loss: 0.607364, acc: 68.75%] [G loss: 2.820264]\n",
      "epoch:5 step:4777 [D loss: 0.490265, acc: 75.78%] [G loss: 2.985996]\n",
      "epoch:5 step:4778 [D loss: 0.539487, acc: 70.31%] [G loss: 2.757215]\n",
      "epoch:5 step:4779 [D loss: 0.581075, acc: 74.22%] [G loss: 2.591755]\n",
      "epoch:5 step:4780 [D loss: 0.673098, acc: 59.38%] [G loss: 2.541662]\n",
      "epoch:5 step:4781 [D loss: 0.557262, acc: 73.44%] [G loss: 2.946260]\n",
      "epoch:5 step:4782 [D loss: 0.599104, acc: 69.53%] [G loss: 2.550230]\n",
      "epoch:5 step:4783 [D loss: 0.630860, acc: 65.62%] [G loss: 2.655532]\n",
      "epoch:5 step:4784 [D loss: 0.595087, acc: 70.31%] [G loss: 2.341703]\n",
      "epoch:5 step:4785 [D loss: 0.558745, acc: 72.66%] [G loss: 2.502725]\n",
      "epoch:5 step:4786 [D loss: 0.530403, acc: 75.00%] [G loss: 2.751148]\n",
      "epoch:5 step:4787 [D loss: 0.610614, acc: 68.75%] [G loss: 2.758219]\n",
      "epoch:5 step:4788 [D loss: 0.591616, acc: 70.31%] [G loss: 2.946487]\n",
      "epoch:5 step:4789 [D loss: 0.542643, acc: 75.78%] [G loss: 2.682594]\n",
      "epoch:5 step:4790 [D loss: 0.575194, acc: 70.31%] [G loss: 2.844318]\n",
      "epoch:5 step:4791 [D loss: 0.566129, acc: 68.75%] [G loss: 2.637217]\n",
      "epoch:5 step:4792 [D loss: 0.516523, acc: 70.31%] [G loss: 2.625480]\n",
      "epoch:5 step:4793 [D loss: 0.584042, acc: 67.97%] [G loss: 2.453305]\n",
      "epoch:5 step:4794 [D loss: 0.633690, acc: 65.62%] [G loss: 2.474255]\n",
      "epoch:5 step:4795 [D loss: 0.583197, acc: 68.75%] [G loss: 2.294481]\n",
      "epoch:5 step:4796 [D loss: 0.533915, acc: 71.88%] [G loss: 2.886623]\n",
      "epoch:5 step:4797 [D loss: 0.574853, acc: 72.66%] [G loss: 2.500364]\n",
      "epoch:5 step:4798 [D loss: 0.652750, acc: 60.16%] [G loss: 2.800646]\n",
      "epoch:5 step:4799 [D loss: 0.510521, acc: 74.22%] [G loss: 2.619181]\n",
      "epoch:5 step:4800 [D loss: 0.592014, acc: 63.28%] [G loss: 2.752833]\n",
      "##############\n",
      "[2.67980378 1.63030637 6.67318104 5.26049859 4.13657606 6.24844999\n",
      " 4.98382525 5.11332015 5.32811295 3.77214636]\n",
      "##########\n",
      "epoch:5 step:4801 [D loss: 0.578424, acc: 72.66%] [G loss: 2.682616]\n",
      "epoch:5 step:4802 [D loss: 0.485847, acc: 78.91%] [G loss: 2.839019]\n",
      "epoch:5 step:4803 [D loss: 0.644763, acc: 67.97%] [G loss: 2.782335]\n",
      "epoch:5 step:4804 [D loss: 0.506137, acc: 78.91%] [G loss: 3.385438]\n",
      "epoch:5 step:4805 [D loss: 0.639099, acc: 62.50%] [G loss: 2.541316]\n",
      "epoch:5 step:4806 [D loss: 0.685743, acc: 61.72%] [G loss: 2.401337]\n",
      "epoch:5 step:4807 [D loss: 0.608808, acc: 73.44%] [G loss: 2.759534]\n",
      "epoch:5 step:4808 [D loss: 0.659438, acc: 63.28%] [G loss: 2.695008]\n",
      "epoch:5 step:4809 [D loss: 0.659980, acc: 64.84%] [G loss: 2.281653]\n",
      "epoch:5 step:4810 [D loss: 0.559676, acc: 68.75%] [G loss: 2.263799]\n",
      "epoch:5 step:4811 [D loss: 0.590068, acc: 67.97%] [G loss: 2.542765]\n",
      "epoch:5 step:4812 [D loss: 0.542958, acc: 74.22%] [G loss: 2.455370]\n",
      "epoch:5 step:4813 [D loss: 0.586118, acc: 68.75%] [G loss: 2.683745]\n",
      "epoch:5 step:4814 [D loss: 0.622461, acc: 65.62%] [G loss: 2.503271]\n",
      "epoch:5 step:4815 [D loss: 0.594618, acc: 66.41%] [G loss: 2.705348]\n",
      "epoch:5 step:4816 [D loss: 0.603744, acc: 69.53%] [G loss: 2.708224]\n",
      "epoch:5 step:4817 [D loss: 0.571553, acc: 73.44%] [G loss: 2.675589]\n",
      "epoch:5 step:4818 [D loss: 0.649037, acc: 59.38%] [G loss: 2.243865]\n",
      "epoch:5 step:4819 [D loss: 0.619762, acc: 68.75%] [G loss: 2.642180]\n",
      "epoch:5 step:4820 [D loss: 0.566098, acc: 69.53%] [G loss: 2.433179]\n",
      "epoch:5 step:4821 [D loss: 0.574718, acc: 74.22%] [G loss: 2.254989]\n",
      "epoch:5 step:4822 [D loss: 0.563261, acc: 71.09%] [G loss: 2.303737]\n",
      "epoch:5 step:4823 [D loss: 0.601337, acc: 66.41%] [G loss: 2.609344]\n",
      "epoch:5 step:4824 [D loss: 0.638005, acc: 63.28%] [G loss: 2.601191]\n",
      "epoch:5 step:4825 [D loss: 0.575968, acc: 72.66%] [G loss: 2.352646]\n",
      "epoch:5 step:4826 [D loss: 0.516839, acc: 73.44%] [G loss: 2.895323]\n",
      "epoch:5 step:4827 [D loss: 0.502203, acc: 77.34%] [G loss: 2.590491]\n",
      "epoch:5 step:4828 [D loss: 0.539115, acc: 71.09%] [G loss: 2.828052]\n",
      "epoch:5 step:4829 [D loss: 0.583794, acc: 68.75%] [G loss: 2.747087]\n",
      "epoch:5 step:4830 [D loss: 0.616542, acc: 70.31%] [G loss: 2.648625]\n",
      "epoch:5 step:4831 [D loss: 0.582983, acc: 64.84%] [G loss: 2.375576]\n",
      "epoch:5 step:4832 [D loss: 0.638444, acc: 64.06%] [G loss: 2.393477]\n",
      "epoch:5 step:4833 [D loss: 0.637326, acc: 63.28%] [G loss: 2.420934]\n",
      "epoch:5 step:4834 [D loss: 0.609007, acc: 66.41%] [G loss: 2.764838]\n",
      "epoch:5 step:4835 [D loss: 0.556086, acc: 69.53%] [G loss: 2.573215]\n",
      "epoch:5 step:4836 [D loss: 0.535213, acc: 68.75%] [G loss: 2.693119]\n",
      "epoch:5 step:4837 [D loss: 0.588871, acc: 74.22%] [G loss: 3.091792]\n",
      "epoch:5 step:4838 [D loss: 0.600022, acc: 64.06%] [G loss: 2.409758]\n",
      "epoch:5 step:4839 [D loss: 0.530938, acc: 75.00%] [G loss: 2.560030]\n",
      "epoch:5 step:4840 [D loss: 0.536877, acc: 74.22%] [G loss: 2.917220]\n",
      "epoch:5 step:4841 [D loss: 0.558943, acc: 69.53%] [G loss: 2.834413]\n",
      "epoch:5 step:4842 [D loss: 0.557860, acc: 72.66%] [G loss: 2.425894]\n",
      "epoch:5 step:4843 [D loss: 0.613928, acc: 64.06%] [G loss: 2.723542]\n",
      "epoch:5 step:4844 [D loss: 0.569234, acc: 68.75%] [G loss: 2.713860]\n",
      "epoch:5 step:4845 [D loss: 0.707673, acc: 55.47%] [G loss: 2.126973]\n",
      "epoch:5 step:4846 [D loss: 0.585735, acc: 74.22%] [G loss: 2.310954]\n",
      "epoch:5 step:4847 [D loss: 0.542889, acc: 73.44%] [G loss: 2.595442]\n",
      "epoch:5 step:4848 [D loss: 0.593430, acc: 70.31%] [G loss: 2.512118]\n",
      "epoch:5 step:4849 [D loss: 0.614547, acc: 64.06%] [G loss: 2.374416]\n",
      "epoch:5 step:4850 [D loss: 0.561706, acc: 71.88%] [G loss: 2.386863]\n",
      "epoch:5 step:4851 [D loss: 0.576173, acc: 71.88%] [G loss: 2.637852]\n",
      "epoch:5 step:4852 [D loss: 0.536739, acc: 70.31%] [G loss: 2.569591]\n",
      "epoch:5 step:4853 [D loss: 0.578210, acc: 69.53%] [G loss: 2.395628]\n",
      "epoch:5 step:4854 [D loss: 0.620563, acc: 67.19%] [G loss: 2.365673]\n",
      "epoch:5 step:4855 [D loss: 0.623589, acc: 60.16%] [G loss: 2.692481]\n",
      "epoch:5 step:4856 [D loss: 0.524362, acc: 75.78%] [G loss: 2.821094]\n",
      "epoch:5 step:4857 [D loss: 0.545312, acc: 71.09%] [G loss: 2.561127]\n",
      "epoch:5 step:4858 [D loss: 0.608596, acc: 64.84%] [G loss: 2.701652]\n",
      "epoch:5 step:4859 [D loss: 0.558607, acc: 72.66%] [G loss: 2.918015]\n",
      "epoch:5 step:4860 [D loss: 0.662401, acc: 64.06%] [G loss: 2.849323]\n",
      "epoch:5 step:4861 [D loss: 0.571999, acc: 71.09%] [G loss: 2.584706]\n",
      "epoch:5 step:4862 [D loss: 0.650392, acc: 61.72%] [G loss: 2.436806]\n",
      "epoch:5 step:4863 [D loss: 0.583687, acc: 73.44%] [G loss: 2.474548]\n",
      "epoch:5 step:4864 [D loss: 0.611885, acc: 67.19%] [G loss: 2.352427]\n",
      "epoch:5 step:4865 [D loss: 0.633750, acc: 62.50%] [G loss: 2.567534]\n",
      "epoch:5 step:4866 [D loss: 0.628657, acc: 61.72%] [G loss: 2.378880]\n",
      "epoch:5 step:4867 [D loss: 0.639290, acc: 67.97%] [G loss: 2.488948]\n",
      "epoch:5 step:4868 [D loss: 0.638420, acc: 61.72%] [G loss: 2.338001]\n",
      "epoch:5 step:4869 [D loss: 0.511089, acc: 78.12%] [G loss: 2.654209]\n",
      "epoch:5 step:4870 [D loss: 0.636252, acc: 60.94%] [G loss: 2.292065]\n",
      "epoch:5 step:4871 [D loss: 0.573650, acc: 68.75%] [G loss: 2.425546]\n",
      "epoch:5 step:4872 [D loss: 0.596339, acc: 70.31%] [G loss: 2.431939]\n",
      "epoch:5 step:4873 [D loss: 0.622496, acc: 60.94%] [G loss: 2.564893]\n",
      "epoch:5 step:4874 [D loss: 0.583303, acc: 67.97%] [G loss: 2.802069]\n",
      "epoch:5 step:4875 [D loss: 0.505460, acc: 74.22%] [G loss: 2.830538]\n",
      "epoch:5 step:4876 [D loss: 0.550885, acc: 71.09%] [G loss: 2.626477]\n",
      "epoch:5 step:4877 [D loss: 0.544296, acc: 77.34%] [G loss: 2.686703]\n",
      "epoch:5 step:4878 [D loss: 0.591220, acc: 67.97%] [G loss: 2.669896]\n",
      "epoch:5 step:4879 [D loss: 0.473213, acc: 78.12%] [G loss: 3.127099]\n",
      "epoch:5 step:4880 [D loss: 0.556493, acc: 72.66%] [G loss: 2.929330]\n",
      "epoch:5 step:4881 [D loss: 0.581721, acc: 71.09%] [G loss: 2.653516]\n",
      "epoch:5 step:4882 [D loss: 0.582643, acc: 71.88%] [G loss: 2.916254]\n",
      "epoch:5 step:4883 [D loss: 0.441955, acc: 82.03%] [G loss: 3.144132]\n",
      "epoch:5 step:4884 [D loss: 0.556956, acc: 76.56%] [G loss: 2.702826]\n",
      "epoch:5 step:4885 [D loss: 0.595014, acc: 67.19%] [G loss: 2.424825]\n",
      "epoch:5 step:4886 [D loss: 0.557226, acc: 70.31%] [G loss: 2.905990]\n",
      "epoch:5 step:4887 [D loss: 0.607037, acc: 67.19%] [G loss: 2.678465]\n",
      "epoch:5 step:4888 [D loss: 0.620852, acc: 62.50%] [G loss: 2.458529]\n",
      "epoch:5 step:4889 [D loss: 0.581456, acc: 72.66%] [G loss: 2.680232]\n",
      "epoch:5 step:4890 [D loss: 0.570489, acc: 70.31%] [G loss: 2.719080]\n",
      "epoch:5 step:4891 [D loss: 0.500865, acc: 78.12%] [G loss: 3.091814]\n",
      "epoch:5 step:4892 [D loss: 0.521916, acc: 71.88%] [G loss: 3.174164]\n",
      "epoch:5 step:4893 [D loss: 0.500276, acc: 78.91%] [G loss: 3.021248]\n",
      "epoch:5 step:4894 [D loss: 0.512869, acc: 75.78%] [G loss: 3.064406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4895 [D loss: 0.654075, acc: 63.28%] [G loss: 2.586872]\n",
      "epoch:5 step:4896 [D loss: 0.645329, acc: 61.72%] [G loss: 2.527667]\n",
      "epoch:5 step:4897 [D loss: 0.614804, acc: 67.97%] [G loss: 2.791186]\n",
      "epoch:5 step:4898 [D loss: 0.609455, acc: 69.53%] [G loss: 2.813628]\n",
      "epoch:5 step:4899 [D loss: 0.623144, acc: 61.72%] [G loss: 2.323212]\n",
      "epoch:5 step:4900 [D loss: 0.557118, acc: 71.88%] [G loss: 2.414355]\n",
      "epoch:5 step:4901 [D loss: 0.611092, acc: 64.84%] [G loss: 2.799920]\n",
      "epoch:5 step:4902 [D loss: 0.454154, acc: 77.34%] [G loss: 2.582766]\n",
      "epoch:5 step:4903 [D loss: 0.575472, acc: 67.19%] [G loss: 2.801913]\n",
      "epoch:5 step:4904 [D loss: 0.588187, acc: 72.66%] [G loss: 2.615443]\n",
      "epoch:5 step:4905 [D loss: 0.631941, acc: 67.19%] [G loss: 2.559149]\n",
      "epoch:5 step:4906 [D loss: 0.548770, acc: 75.78%] [G loss: 2.671686]\n",
      "epoch:5 step:4907 [D loss: 0.560189, acc: 70.31%] [G loss: 2.574873]\n",
      "epoch:5 step:4908 [D loss: 0.531724, acc: 74.22%] [G loss: 2.874229]\n",
      "epoch:5 step:4909 [D loss: 0.634876, acc: 64.84%] [G loss: 2.755833]\n",
      "epoch:5 step:4910 [D loss: 0.718740, acc: 58.59%] [G loss: 2.355637]\n",
      "epoch:5 step:4911 [D loss: 0.646475, acc: 62.50%] [G loss: 2.459677]\n",
      "epoch:5 step:4912 [D loss: 0.658530, acc: 66.41%] [G loss: 2.138876]\n",
      "epoch:5 step:4913 [D loss: 0.699445, acc: 61.72%] [G loss: 2.194033]\n",
      "epoch:5 step:4914 [D loss: 0.578061, acc: 68.75%] [G loss: 2.264379]\n",
      "epoch:5 step:4915 [D loss: 0.546003, acc: 71.88%] [G loss: 2.784001]\n",
      "epoch:5 step:4916 [D loss: 0.595100, acc: 71.09%] [G loss: 2.910790]\n",
      "epoch:5 step:4917 [D loss: 0.526924, acc: 74.22%] [G loss: 3.063833]\n",
      "epoch:5 step:4918 [D loss: 0.743745, acc: 57.03%] [G loss: 2.700571]\n",
      "epoch:5 step:4919 [D loss: 0.642597, acc: 62.50%] [G loss: 2.386355]\n",
      "epoch:5 step:4920 [D loss: 0.585642, acc: 67.97%] [G loss: 2.496541]\n",
      "epoch:5 step:4921 [D loss: 0.587752, acc: 72.66%] [G loss: 2.447037]\n",
      "epoch:5 step:4922 [D loss: 0.693269, acc: 58.59%] [G loss: 2.463409]\n",
      "epoch:5 step:4923 [D loss: 0.598411, acc: 67.19%] [G loss: 2.665400]\n",
      "epoch:5 step:4924 [D loss: 0.566544, acc: 65.62%] [G loss: 2.438509]\n",
      "epoch:5 step:4925 [D loss: 0.622824, acc: 65.62%] [G loss: 2.533453]\n",
      "epoch:5 step:4926 [D loss: 0.548074, acc: 70.31%] [G loss: 2.622109]\n",
      "epoch:5 step:4927 [D loss: 0.531343, acc: 78.91%] [G loss: 2.638620]\n",
      "epoch:5 step:4928 [D loss: 0.656659, acc: 64.06%] [G loss: 2.505770]\n",
      "epoch:5 step:4929 [D loss: 0.574865, acc: 71.88%] [G loss: 2.759752]\n",
      "epoch:5 step:4930 [D loss: 0.543778, acc: 75.00%] [G loss: 2.664249]\n",
      "epoch:5 step:4931 [D loss: 0.548754, acc: 71.09%] [G loss: 2.583451]\n",
      "epoch:5 step:4932 [D loss: 0.613323, acc: 64.06%] [G loss: 2.562531]\n",
      "epoch:5 step:4933 [D loss: 0.653531, acc: 67.19%] [G loss: 2.337733]\n",
      "epoch:5 step:4934 [D loss: 0.602840, acc: 67.19%] [G loss: 2.462433]\n",
      "epoch:5 step:4935 [D loss: 0.590934, acc: 70.31%] [G loss: 2.587799]\n",
      "epoch:5 step:4936 [D loss: 0.639860, acc: 66.41%] [G loss: 2.436901]\n",
      "epoch:5 step:4937 [D loss: 0.640467, acc: 63.28%] [G loss: 2.441032]\n",
      "epoch:5 step:4938 [D loss: 0.612229, acc: 64.06%] [G loss: 2.578001]\n",
      "epoch:5 step:4939 [D loss: 0.623507, acc: 64.84%] [G loss: 2.708239]\n",
      "epoch:5 step:4940 [D loss: 0.586148, acc: 70.31%] [G loss: 2.583537]\n",
      "epoch:5 step:4941 [D loss: 0.599645, acc: 69.53%] [G loss: 2.543369]\n",
      "epoch:5 step:4942 [D loss: 0.549286, acc: 70.31%] [G loss: 2.400857]\n",
      "epoch:5 step:4943 [D loss: 0.655194, acc: 61.72%] [G loss: 2.677777]\n",
      "epoch:5 step:4944 [D loss: 0.536726, acc: 75.78%] [G loss: 2.602761]\n",
      "epoch:5 step:4945 [D loss: 0.566503, acc: 70.31%] [G loss: 2.511760]\n",
      "epoch:5 step:4946 [D loss: 0.556660, acc: 71.88%] [G loss: 2.878556]\n",
      "epoch:5 step:4947 [D loss: 0.583674, acc: 65.62%] [G loss: 2.828212]\n",
      "epoch:5 step:4948 [D loss: 0.558289, acc: 71.09%] [G loss: 2.763819]\n",
      "epoch:5 step:4949 [D loss: 0.493669, acc: 75.78%] [G loss: 2.817713]\n",
      "epoch:5 step:4950 [D loss: 0.632652, acc: 67.19%] [G loss: 2.770285]\n",
      "epoch:5 step:4951 [D loss: 0.641679, acc: 67.19%] [G loss: 2.767884]\n",
      "epoch:5 step:4952 [D loss: 0.679270, acc: 65.62%] [G loss: 2.176998]\n",
      "epoch:5 step:4953 [D loss: 0.639920, acc: 66.41%] [G loss: 2.667321]\n",
      "epoch:5 step:4954 [D loss: 0.595646, acc: 70.31%] [G loss: 2.598252]\n",
      "epoch:5 step:4955 [D loss: 0.590140, acc: 67.19%] [G loss: 2.664569]\n",
      "epoch:5 step:4956 [D loss: 0.571891, acc: 75.00%] [G loss: 2.516454]\n",
      "epoch:5 step:4957 [D loss: 0.530019, acc: 72.66%] [G loss: 2.848812]\n",
      "epoch:5 step:4958 [D loss: 0.594775, acc: 64.84%] [G loss: 2.621773]\n",
      "epoch:5 step:4959 [D loss: 0.563064, acc: 70.31%] [G loss: 2.680148]\n",
      "epoch:5 step:4960 [D loss: 0.655877, acc: 64.84%] [G loss: 2.558948]\n",
      "epoch:5 step:4961 [D loss: 0.605160, acc: 64.06%] [G loss: 2.582304]\n",
      "epoch:5 step:4962 [D loss: 0.593692, acc: 71.09%] [G loss: 2.618368]\n",
      "epoch:5 step:4963 [D loss: 0.588258, acc: 74.22%] [G loss: 2.573780]\n",
      "epoch:5 step:4964 [D loss: 0.542894, acc: 69.53%] [G loss: 2.870227]\n",
      "epoch:5 step:4965 [D loss: 0.546559, acc: 68.75%] [G loss: 2.690794]\n",
      "epoch:5 step:4966 [D loss: 0.587546, acc: 67.19%] [G loss: 2.478544]\n",
      "epoch:5 step:4967 [D loss: 0.588283, acc: 66.41%] [G loss: 2.877268]\n",
      "epoch:5 step:4968 [D loss: 0.521357, acc: 76.56%] [G loss: 2.621942]\n",
      "epoch:5 step:4969 [D loss: 0.530249, acc: 72.66%] [G loss: 2.932607]\n",
      "epoch:5 step:4970 [D loss: 0.546361, acc: 71.09%] [G loss: 3.130846]\n",
      "epoch:5 step:4971 [D loss: 0.469885, acc: 76.56%] [G loss: 2.892467]\n",
      "epoch:5 step:4972 [D loss: 0.628629, acc: 71.88%] [G loss: 2.670286]\n",
      "epoch:5 step:4973 [D loss: 0.683467, acc: 57.03%] [G loss: 2.301109]\n",
      "epoch:5 step:4974 [D loss: 0.542376, acc: 76.56%] [G loss: 2.587329]\n",
      "epoch:5 step:4975 [D loss: 0.594617, acc: 67.97%] [G loss: 2.484906]\n",
      "epoch:5 step:4976 [D loss: 0.575886, acc: 66.41%] [G loss: 2.463165]\n",
      "epoch:5 step:4977 [D loss: 0.588088, acc: 70.31%] [G loss: 2.688407]\n",
      "epoch:5 step:4978 [D loss: 0.569541, acc: 66.41%] [G loss: 2.622243]\n",
      "epoch:5 step:4979 [D loss: 0.593321, acc: 64.84%] [G loss: 2.406855]\n",
      "epoch:5 step:4980 [D loss: 0.561349, acc: 68.75%] [G loss: 2.866636]\n",
      "epoch:5 step:4981 [D loss: 0.607181, acc: 70.31%] [G loss: 2.455780]\n",
      "epoch:5 step:4982 [D loss: 0.598571, acc: 65.62%] [G loss: 2.497583]\n",
      "epoch:5 step:4983 [D loss: 0.537885, acc: 75.00%] [G loss: 2.812652]\n",
      "epoch:5 step:4984 [D loss: 0.509306, acc: 78.91%] [G loss: 2.751101]\n",
      "epoch:5 step:4985 [D loss: 0.580493, acc: 69.53%] [G loss: 2.606391]\n",
      "epoch:5 step:4986 [D loss: 0.651584, acc: 61.72%] [G loss: 2.119016]\n",
      "epoch:5 step:4987 [D loss: 0.613553, acc: 66.41%] [G loss: 2.493672]\n",
      "epoch:5 step:4988 [D loss: 0.632318, acc: 63.28%] [G loss: 2.595791]\n",
      "epoch:5 step:4989 [D loss: 0.621331, acc: 71.88%] [G loss: 2.905844]\n",
      "epoch:5 step:4990 [D loss: 0.572686, acc: 69.53%] [G loss: 2.657224]\n",
      "epoch:5 step:4991 [D loss: 0.626352, acc: 61.72%] [G loss: 2.458154]\n",
      "epoch:5 step:4992 [D loss: 0.580292, acc: 68.75%] [G loss: 2.608990]\n",
      "epoch:5 step:4993 [D loss: 0.564542, acc: 74.22%] [G loss: 2.589873]\n",
      "epoch:5 step:4994 [D loss: 0.636302, acc: 70.31%] [G loss: 2.703759]\n",
      "epoch:5 step:4995 [D loss: 0.609861, acc: 68.75%] [G loss: 2.659422]\n",
      "epoch:5 step:4996 [D loss: 0.555273, acc: 71.09%] [G loss: 2.648844]\n",
      "epoch:5 step:4997 [D loss: 0.555997, acc: 71.88%] [G loss: 3.242015]\n",
      "epoch:5 step:4998 [D loss: 0.537929, acc: 68.75%] [G loss: 3.129372]\n",
      "epoch:5 step:4999 [D loss: 0.513757, acc: 75.78%] [G loss: 3.135589]\n",
      "epoch:5 step:5000 [D loss: 0.560850, acc: 74.22%] [G loss: 3.137315]\n",
      "##############\n",
      "[2.893141   1.84994812 6.98142629 5.13427067 4.01153146 6.06043218\n",
      " 5.13273938 5.09617981 5.26820422 3.8753264 ]\n",
      "##########\n",
      "epoch:5 step:5001 [D loss: 0.662197, acc: 64.84%] [G loss: 2.528866]\n",
      "epoch:5 step:5002 [D loss: 0.606007, acc: 72.66%] [G loss: 2.248083]\n",
      "epoch:5 step:5003 [D loss: 0.582410, acc: 71.09%] [G loss: 2.574727]\n",
      "epoch:5 step:5004 [D loss: 0.600971, acc: 67.19%] [G loss: 2.290531]\n",
      "epoch:5 step:5005 [D loss: 0.622913, acc: 66.41%] [G loss: 2.485544]\n",
      "epoch:5 step:5006 [D loss: 0.628147, acc: 69.53%] [G loss: 2.687337]\n",
      "epoch:5 step:5007 [D loss: 0.585281, acc: 72.66%] [G loss: 2.517464]\n",
      "epoch:5 step:5008 [D loss: 0.627778, acc: 67.97%] [G loss: 2.425777]\n",
      "epoch:5 step:5009 [D loss: 0.588559, acc: 69.53%] [G loss: 2.436806]\n",
      "epoch:5 step:5010 [D loss: 0.578879, acc: 67.19%] [G loss: 2.715393]\n",
      "epoch:5 step:5011 [D loss: 0.543897, acc: 71.09%] [G loss: 2.496813]\n",
      "epoch:5 step:5012 [D loss: 0.626667, acc: 67.19%] [G loss: 2.503887]\n",
      "epoch:5 step:5013 [D loss: 0.602156, acc: 64.84%] [G loss: 2.497939]\n",
      "epoch:5 step:5014 [D loss: 0.613509, acc: 67.19%] [G loss: 2.549314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5015 [D loss: 0.527707, acc: 72.66%] [G loss: 2.791371]\n",
      "epoch:5 step:5016 [D loss: 0.602062, acc: 68.75%] [G loss: 2.609961]\n",
      "epoch:5 step:5017 [D loss: 0.555043, acc: 71.09%] [G loss: 3.029438]\n",
      "epoch:5 step:5018 [D loss: 0.580263, acc: 68.75%] [G loss: 2.719193]\n",
      "epoch:5 step:5019 [D loss: 0.647950, acc: 67.97%] [G loss: 2.785253]\n",
      "epoch:5 step:5020 [D loss: 0.518483, acc: 72.66%] [G loss: 2.980956]\n",
      "epoch:5 step:5021 [D loss: 0.512800, acc: 71.09%] [G loss: 3.159401]\n",
      "epoch:5 step:5022 [D loss: 0.635382, acc: 70.31%] [G loss: 2.840024]\n",
      "epoch:5 step:5023 [D loss: 0.571784, acc: 67.97%] [G loss: 2.707273]\n",
      "epoch:5 step:5024 [D loss: 0.646417, acc: 65.62%] [G loss: 2.583606]\n",
      "epoch:5 step:5025 [D loss: 0.616797, acc: 71.09%] [G loss: 2.517255]\n",
      "epoch:5 step:5026 [D loss: 0.642198, acc: 63.28%] [G loss: 2.307242]\n",
      "epoch:5 step:5027 [D loss: 0.697108, acc: 59.38%] [G loss: 2.382515]\n",
      "epoch:5 step:5028 [D loss: 0.661234, acc: 66.41%] [G loss: 2.600533]\n",
      "epoch:5 step:5029 [D loss: 0.587457, acc: 68.75%] [G loss: 2.511413]\n",
      "epoch:5 step:5030 [D loss: 0.500410, acc: 78.12%] [G loss: 2.888785]\n",
      "epoch:5 step:5031 [D loss: 0.504128, acc: 76.56%] [G loss: 3.200855]\n",
      "epoch:5 step:5032 [D loss: 0.529300, acc: 67.97%] [G loss: 3.391752]\n",
      "epoch:5 step:5033 [D loss: 0.631621, acc: 63.28%] [G loss: 2.435695]\n",
      "epoch:5 step:5034 [D loss: 0.763578, acc: 53.91%] [G loss: 2.306821]\n",
      "epoch:5 step:5035 [D loss: 0.552444, acc: 71.09%] [G loss: 2.371049]\n",
      "epoch:5 step:5036 [D loss: 0.592926, acc: 67.19%] [G loss: 2.500566]\n",
      "epoch:5 step:5037 [D loss: 0.672438, acc: 58.59%] [G loss: 2.431414]\n",
      "epoch:5 step:5038 [D loss: 0.614593, acc: 64.06%] [G loss: 2.832573]\n",
      "epoch:5 step:5039 [D loss: 0.568344, acc: 71.09%] [G loss: 2.822704]\n",
      "epoch:5 step:5040 [D loss: 0.520389, acc: 76.56%] [G loss: 2.556635]\n",
      "epoch:5 step:5041 [D loss: 0.704506, acc: 55.47%] [G loss: 2.211274]\n",
      "epoch:5 step:5042 [D loss: 0.557967, acc: 67.19%] [G loss: 2.562889]\n",
      "epoch:5 step:5043 [D loss: 0.514614, acc: 75.78%] [G loss: 2.665879]\n",
      "epoch:5 step:5044 [D loss: 0.548260, acc: 70.31%] [G loss: 2.800464]\n",
      "epoch:5 step:5045 [D loss: 0.503639, acc: 75.78%] [G loss: 2.463218]\n",
      "epoch:5 step:5046 [D loss: 0.554136, acc: 68.75%] [G loss: 2.453121]\n",
      "epoch:5 step:5047 [D loss: 0.582132, acc: 68.75%] [G loss: 2.481069]\n",
      "epoch:5 step:5048 [D loss: 0.592995, acc: 69.53%] [G loss: 2.519801]\n",
      "epoch:5 step:5049 [D loss: 0.546729, acc: 71.88%] [G loss: 2.613487]\n",
      "epoch:5 step:5050 [D loss: 0.584136, acc: 64.84%] [G loss: 2.983508]\n",
      "epoch:5 step:5051 [D loss: 0.541803, acc: 71.09%] [G loss: 2.799416]\n",
      "epoch:5 step:5052 [D loss: 0.549122, acc: 71.88%] [G loss: 2.877147]\n",
      "epoch:5 step:5053 [D loss: 0.550063, acc: 74.22%] [G loss: 2.829746]\n",
      "epoch:5 step:5054 [D loss: 0.596026, acc: 63.28%] [G loss: 2.664409]\n",
      "epoch:5 step:5055 [D loss: 0.599513, acc: 71.88%] [G loss: 2.791241]\n",
      "epoch:5 step:5056 [D loss: 0.606191, acc: 64.06%] [G loss: 3.129825]\n",
      "epoch:5 step:5057 [D loss: 0.542796, acc: 76.56%] [G loss: 2.697235]\n",
      "epoch:5 step:5058 [D loss: 0.640250, acc: 66.41%] [G loss: 2.389285]\n",
      "epoch:5 step:5059 [D loss: 0.518184, acc: 72.66%] [G loss: 2.981324]\n",
      "epoch:5 step:5060 [D loss: 0.657013, acc: 62.50%] [G loss: 2.572461]\n",
      "epoch:5 step:5061 [D loss: 0.728269, acc: 60.16%] [G loss: 2.260581]\n",
      "epoch:5 step:5062 [D loss: 0.701400, acc: 64.84%] [G loss: 2.149966]\n",
      "epoch:5 step:5063 [D loss: 0.597910, acc: 68.75%] [G loss: 2.428502]\n",
      "epoch:5 step:5064 [D loss: 0.625491, acc: 68.75%] [G loss: 2.504829]\n",
      "epoch:5 step:5065 [D loss: 0.579967, acc: 72.66%] [G loss: 2.515592]\n",
      "epoch:5 step:5066 [D loss: 0.514435, acc: 75.78%] [G loss: 2.771013]\n",
      "epoch:5 step:5067 [D loss: 0.607090, acc: 64.84%] [G loss: 2.398212]\n",
      "epoch:5 step:5068 [D loss: 0.670096, acc: 60.16%] [G loss: 2.484111]\n",
      "epoch:5 step:5069 [D loss: 0.587906, acc: 73.44%] [G loss: 2.456971]\n",
      "epoch:5 step:5070 [D loss: 0.531739, acc: 75.78%] [G loss: 2.627032]\n",
      "epoch:5 step:5071 [D loss: 0.639827, acc: 66.41%] [G loss: 2.295723]\n",
      "epoch:5 step:5072 [D loss: 0.657692, acc: 62.50%] [G loss: 2.376050]\n",
      "epoch:5 step:5073 [D loss: 0.549219, acc: 70.31%] [G loss: 2.628686]\n",
      "epoch:5 step:5074 [D loss: 0.547585, acc: 74.22%] [G loss: 2.593455]\n",
      "epoch:5 step:5075 [D loss: 0.548938, acc: 71.88%] [G loss: 2.655331]\n",
      "epoch:5 step:5076 [D loss: 0.695827, acc: 55.47%] [G loss: 2.267503]\n",
      "epoch:5 step:5077 [D loss: 0.600268, acc: 67.97%] [G loss: 2.710989]\n",
      "epoch:5 step:5078 [D loss: 0.668906, acc: 60.16%] [G loss: 2.253223]\n",
      "epoch:5 step:5079 [D loss: 0.616584, acc: 60.94%] [G loss: 2.457533]\n",
      "epoch:5 step:5080 [D loss: 0.639186, acc: 63.28%] [G loss: 2.504042]\n",
      "epoch:5 step:5081 [D loss: 0.647632, acc: 60.16%] [G loss: 2.418022]\n",
      "epoch:5 step:5082 [D loss: 0.566499, acc: 70.31%] [G loss: 2.434784]\n",
      "epoch:5 step:5083 [D loss: 0.553696, acc: 74.22%] [G loss: 2.560305]\n",
      "epoch:5 step:5084 [D loss: 0.514288, acc: 77.34%] [G loss: 2.966736]\n",
      "epoch:5 step:5085 [D loss: 0.614934, acc: 64.06%] [G loss: 2.336916]\n",
      "epoch:5 step:5086 [D loss: 0.652157, acc: 63.28%] [G loss: 2.397487]\n",
      "epoch:5 step:5087 [D loss: 0.532504, acc: 75.00%] [G loss: 2.773280]\n",
      "epoch:5 step:5088 [D loss: 0.588869, acc: 65.62%] [G loss: 2.611097]\n",
      "epoch:5 step:5089 [D loss: 0.580966, acc: 67.97%] [G loss: 2.524306]\n",
      "epoch:5 step:5090 [D loss: 0.562869, acc: 72.66%] [G loss: 2.507734]\n",
      "epoch:5 step:5091 [D loss: 0.568900, acc: 69.53%] [G loss: 2.653894]\n",
      "epoch:5 step:5092 [D loss: 0.582798, acc: 67.97%] [G loss: 2.676788]\n",
      "epoch:5 step:5093 [D loss: 0.601713, acc: 67.97%] [G loss: 2.592046]\n",
      "epoch:5 step:5094 [D loss: 0.549890, acc: 71.88%] [G loss: 2.221523]\n",
      "epoch:5 step:5095 [D loss: 0.643203, acc: 62.50%] [G loss: 2.430962]\n",
      "epoch:5 step:5096 [D loss: 0.638563, acc: 60.16%] [G loss: 2.281816]\n",
      "epoch:5 step:5097 [D loss: 0.553402, acc: 72.66%] [G loss: 2.318369]\n",
      "epoch:5 step:5098 [D loss: 0.570131, acc: 74.22%] [G loss: 2.656862]\n",
      "epoch:5 step:5099 [D loss: 0.651408, acc: 61.72%] [G loss: 2.433652]\n",
      "epoch:5 step:5100 [D loss: 0.518652, acc: 73.44%] [G loss: 2.666584]\n",
      "epoch:5 step:5101 [D loss: 0.680782, acc: 57.03%] [G loss: 2.527385]\n",
      "epoch:5 step:5102 [D loss: 0.678869, acc: 60.16%] [G loss: 2.264698]\n",
      "epoch:5 step:5103 [D loss: 0.633442, acc: 66.41%] [G loss: 2.575032]\n",
      "epoch:5 step:5104 [D loss: 0.670070, acc: 64.06%] [G loss: 2.884063]\n",
      "epoch:5 step:5105 [D loss: 0.658586, acc: 59.38%] [G loss: 2.408782]\n",
      "epoch:5 step:5106 [D loss: 0.614764, acc: 64.06%] [G loss: 2.362564]\n",
      "epoch:5 step:5107 [D loss: 0.613073, acc: 65.62%] [G loss: 2.770863]\n",
      "epoch:5 step:5108 [D loss: 0.636308, acc: 70.31%] [G loss: 2.670169]\n",
      "epoch:5 step:5109 [D loss: 0.566993, acc: 72.66%] [G loss: 2.444448]\n",
      "epoch:5 step:5110 [D loss: 0.620557, acc: 67.97%] [G loss: 2.471872]\n",
      "epoch:5 step:5111 [D loss: 0.581892, acc: 69.53%] [G loss: 2.864574]\n",
      "epoch:5 step:5112 [D loss: 0.525846, acc: 75.78%] [G loss: 2.988490]\n",
      "epoch:5 step:5113 [D loss: 0.476163, acc: 77.34%] [G loss: 3.356540]\n",
      "epoch:5 step:5114 [D loss: 0.581267, acc: 70.31%] [G loss: 2.695971]\n",
      "epoch:5 step:5115 [D loss: 0.582357, acc: 64.84%] [G loss: 3.216144]\n",
      "epoch:5 step:5116 [D loss: 0.577993, acc: 66.41%] [G loss: 2.705192]\n",
      "epoch:5 step:5117 [D loss: 0.704106, acc: 51.56%] [G loss: 2.320231]\n",
      "epoch:5 step:5118 [D loss: 0.600966, acc: 66.41%] [G loss: 2.598805]\n",
      "epoch:5 step:5119 [D loss: 0.591721, acc: 74.22%] [G loss: 2.583988]\n",
      "epoch:5 step:5120 [D loss: 0.644822, acc: 64.84%] [G loss: 2.499875]\n",
      "epoch:5 step:5121 [D loss: 0.634480, acc: 68.75%] [G loss: 2.371119]\n",
      "epoch:5 step:5122 [D loss: 0.656875, acc: 59.38%] [G loss: 2.319639]\n",
      "epoch:5 step:5123 [D loss: 0.600600, acc: 65.62%] [G loss: 2.518648]\n",
      "epoch:5 step:5124 [D loss: 0.549751, acc: 71.88%] [G loss: 2.565110]\n",
      "epoch:5 step:5125 [D loss: 0.641861, acc: 64.84%] [G loss: 2.534285]\n",
      "epoch:5 step:5126 [D loss: 0.620267, acc: 69.53%] [G loss: 2.521131]\n",
      "epoch:5 step:5127 [D loss: 0.608872, acc: 64.84%] [G loss: 2.225065]\n",
      "epoch:5 step:5128 [D loss: 0.544610, acc: 71.88%] [G loss: 2.384995]\n",
      "epoch:5 step:5129 [D loss: 0.590481, acc: 68.75%] [G loss: 2.508329]\n",
      "epoch:5 step:5130 [D loss: 0.671521, acc: 60.94%] [G loss: 2.504582]\n",
      "epoch:5 step:5131 [D loss: 0.559629, acc: 70.31%] [G loss: 2.299289]\n",
      "epoch:5 step:5132 [D loss: 0.703262, acc: 65.62%] [G loss: 2.467659]\n",
      "epoch:5 step:5133 [D loss: 0.665595, acc: 60.16%] [G loss: 2.612207]\n",
      "epoch:5 step:5134 [D loss: 0.573704, acc: 67.97%] [G loss: 2.445445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5135 [D loss: 0.529236, acc: 74.22%] [G loss: 2.745739]\n",
      "epoch:5 step:5136 [D loss: 0.608882, acc: 67.19%] [G loss: 2.759343]\n",
      "epoch:5 step:5137 [D loss: 0.586805, acc: 66.41%] [G loss: 2.690966]\n",
      "epoch:5 step:5138 [D loss: 0.535243, acc: 74.22%] [G loss: 2.921726]\n",
      "epoch:5 step:5139 [D loss: 0.555812, acc: 72.66%] [G loss: 2.622219]\n",
      "epoch:5 step:5140 [D loss: 0.667696, acc: 67.19%] [G loss: 2.664970]\n",
      "epoch:5 step:5141 [D loss: 0.697699, acc: 58.59%] [G loss: 2.382726]\n",
      "epoch:5 step:5142 [D loss: 0.559434, acc: 73.44%] [G loss: 2.530067]\n",
      "epoch:5 step:5143 [D loss: 0.643770, acc: 60.94%] [G loss: 2.449804]\n",
      "epoch:5 step:5144 [D loss: 0.676216, acc: 59.38%] [G loss: 2.286302]\n",
      "epoch:5 step:5145 [D loss: 0.552770, acc: 68.75%] [G loss: 2.446201]\n",
      "epoch:5 step:5146 [D loss: 0.604041, acc: 70.31%] [G loss: 2.228575]\n",
      "epoch:5 step:5147 [D loss: 0.593502, acc: 70.31%] [G loss: 2.409559]\n",
      "epoch:5 step:5148 [D loss: 0.624744, acc: 64.84%] [G loss: 2.468292]\n",
      "epoch:5 step:5149 [D loss: 0.595477, acc: 68.75%] [G loss: 2.420407]\n",
      "epoch:5 step:5150 [D loss: 0.600830, acc: 64.84%] [G loss: 2.394141]\n",
      "epoch:5 step:5151 [D loss: 0.582007, acc: 66.41%] [G loss: 2.552619]\n",
      "epoch:5 step:5152 [D loss: 0.652557, acc: 64.06%] [G loss: 2.556983]\n",
      "epoch:5 step:5153 [D loss: 0.522035, acc: 73.44%] [G loss: 2.471967]\n",
      "epoch:5 step:5154 [D loss: 0.594320, acc: 71.88%] [G loss: 2.391620]\n",
      "epoch:5 step:5155 [D loss: 0.572950, acc: 75.00%] [G loss: 2.788604]\n",
      "epoch:5 step:5156 [D loss: 0.491602, acc: 78.12%] [G loss: 3.475371]\n",
      "epoch:5 step:5157 [D loss: 0.528410, acc: 69.53%] [G loss: 2.866458]\n",
      "epoch:5 step:5158 [D loss: 0.602237, acc: 68.75%] [G loss: 2.743192]\n",
      "epoch:5 step:5159 [D loss: 0.556007, acc: 71.88%] [G loss: 2.650784]\n",
      "epoch:5 step:5160 [D loss: 0.514007, acc: 76.56%] [G loss: 3.048195]\n",
      "epoch:5 step:5161 [D loss: 0.558344, acc: 71.09%] [G loss: 2.902179]\n",
      "epoch:5 step:5162 [D loss: 0.688099, acc: 61.72%] [G loss: 2.312898]\n",
      "epoch:5 step:5163 [D loss: 0.609053, acc: 67.97%] [G loss: 2.302372]\n",
      "epoch:5 step:5164 [D loss: 0.574528, acc: 74.22%] [G loss: 2.972054]\n",
      "epoch:5 step:5165 [D loss: 0.612975, acc: 64.84%] [G loss: 2.558037]\n",
      "epoch:5 step:5166 [D loss: 0.578705, acc: 73.44%] [G loss: 2.828273]\n",
      "epoch:5 step:5167 [D loss: 0.609342, acc: 68.75%] [G loss: 2.327711]\n",
      "epoch:5 step:5168 [D loss: 0.608154, acc: 65.62%] [G loss: 2.741465]\n",
      "epoch:5 step:5169 [D loss: 0.618395, acc: 64.06%] [G loss: 2.487495]\n",
      "epoch:5 step:5170 [D loss: 0.563514, acc: 74.22%] [G loss: 2.713867]\n",
      "epoch:5 step:5171 [D loss: 0.682914, acc: 62.50%] [G loss: 2.434053]\n",
      "epoch:5 step:5172 [D loss: 0.657849, acc: 60.94%] [G loss: 2.647169]\n",
      "epoch:5 step:5173 [D loss: 0.567421, acc: 75.78%] [G loss: 2.337434]\n",
      "epoch:5 step:5174 [D loss: 0.673155, acc: 62.50%] [G loss: 2.540375]\n",
      "epoch:5 step:5175 [D loss: 0.624768, acc: 66.41%] [G loss: 2.493462]\n",
      "epoch:5 step:5176 [D loss: 0.610343, acc: 70.31%] [G loss: 2.723144]\n",
      "epoch:5 step:5177 [D loss: 0.575424, acc: 69.53%] [G loss: 2.391879]\n",
      "epoch:5 step:5178 [D loss: 0.626025, acc: 64.84%] [G loss: 2.447461]\n",
      "epoch:5 step:5179 [D loss: 0.560153, acc: 71.88%] [G loss: 2.344720]\n",
      "epoch:5 step:5180 [D loss: 0.605850, acc: 65.62%] [G loss: 2.693224]\n",
      "epoch:5 step:5181 [D loss: 0.641500, acc: 70.31%] [G loss: 2.379703]\n",
      "epoch:5 step:5182 [D loss: 0.567144, acc: 76.56%] [G loss: 2.978939]\n",
      "epoch:5 step:5183 [D loss: 0.536113, acc: 74.22%] [G loss: 2.936335]\n",
      "epoch:5 step:5184 [D loss: 0.486412, acc: 79.69%] [G loss: 3.259016]\n",
      "epoch:5 step:5185 [D loss: 0.699512, acc: 62.50%] [G loss: 2.274274]\n",
      "epoch:5 step:5186 [D loss: 0.767570, acc: 53.91%] [G loss: 2.289136]\n",
      "epoch:5 step:5187 [D loss: 0.623990, acc: 66.41%] [G loss: 2.023582]\n",
      "epoch:5 step:5188 [D loss: 0.599734, acc: 64.06%] [G loss: 2.591216]\n",
      "epoch:5 step:5189 [D loss: 0.565543, acc: 71.88%] [G loss: 2.992801]\n",
      "epoch:5 step:5190 [D loss: 0.592154, acc: 67.97%] [G loss: 2.439902]\n",
      "epoch:5 step:5191 [D loss: 0.673560, acc: 64.84%] [G loss: 2.500436]\n",
      "epoch:5 step:5192 [D loss: 0.646591, acc: 64.06%] [G loss: 2.730949]\n",
      "epoch:5 step:5193 [D loss: 0.629909, acc: 67.97%] [G loss: 2.449741]\n",
      "epoch:5 step:5194 [D loss: 0.608549, acc: 64.84%] [G loss: 2.441035]\n",
      "epoch:5 step:5195 [D loss: 0.656538, acc: 55.47%] [G loss: 2.190036]\n",
      "epoch:5 step:5196 [D loss: 0.655837, acc: 60.94%] [G loss: 2.229664]\n",
      "epoch:5 step:5197 [D loss: 0.570345, acc: 67.97%] [G loss: 2.478415]\n",
      "epoch:5 step:5198 [D loss: 0.628925, acc: 66.41%] [G loss: 2.416731]\n",
      "epoch:5 step:5199 [D loss: 0.608873, acc: 70.31%] [G loss: 2.420888]\n",
      "epoch:5 step:5200 [D loss: 0.612270, acc: 66.41%] [G loss: 2.584680]\n",
      "##############\n",
      "[2.7791379  1.43399806 6.56624157 5.4010024  4.14104899 6.11730206\n",
      " 5.11578648 5.19050362 5.28054223 3.8090145 ]\n",
      "##########\n",
      "epoch:5 step:5201 [D loss: 0.578552, acc: 75.78%] [G loss: 2.271893]\n",
      "epoch:5 step:5202 [D loss: 0.620339, acc: 70.31%] [G loss: 2.587046]\n",
      "epoch:5 step:5203 [D loss: 0.639925, acc: 65.62%] [G loss: 2.423941]\n",
      "epoch:5 step:5204 [D loss: 0.568980, acc: 77.34%] [G loss: 2.284369]\n",
      "epoch:5 step:5205 [D loss: 0.635804, acc: 63.28%] [G loss: 2.394527]\n",
      "epoch:5 step:5206 [D loss: 0.563750, acc: 72.66%] [G loss: 2.513140]\n",
      "epoch:5 step:5207 [D loss: 0.549138, acc: 72.66%] [G loss: 2.519723]\n",
      "epoch:5 step:5208 [D loss: 0.556553, acc: 66.41%] [G loss: 2.558578]\n",
      "epoch:5 step:5209 [D loss: 0.607516, acc: 70.31%] [G loss: 2.733165]\n",
      "epoch:5 step:5210 [D loss: 0.590895, acc: 67.97%] [G loss: 2.356494]\n",
      "epoch:5 step:5211 [D loss: 0.579965, acc: 71.09%] [G loss: 2.788882]\n",
      "epoch:5 step:5212 [D loss: 0.669838, acc: 61.72%] [G loss: 2.356170]\n",
      "epoch:5 step:5213 [D loss: 0.609888, acc: 68.75%] [G loss: 2.294371]\n",
      "epoch:5 step:5214 [D loss: 0.730983, acc: 61.72%] [G loss: 2.234941]\n",
      "epoch:5 step:5215 [D loss: 0.577187, acc: 69.53%] [G loss: 2.502320]\n",
      "epoch:5 step:5216 [D loss: 0.680888, acc: 60.16%] [G loss: 2.198703]\n",
      "epoch:5 step:5217 [D loss: 0.589150, acc: 69.53%] [G loss: 2.599494]\n",
      "epoch:5 step:5218 [D loss: 0.556214, acc: 70.31%] [G loss: 2.612431]\n",
      "epoch:5 step:5219 [D loss: 0.536101, acc: 68.75%] [G loss: 2.514208]\n",
      "epoch:5 step:5220 [D loss: 0.667120, acc: 64.84%] [G loss: 2.243934]\n",
      "epoch:5 step:5221 [D loss: 0.559620, acc: 74.22%] [G loss: 2.709761]\n",
      "epoch:5 step:5222 [D loss: 0.585738, acc: 69.53%] [G loss: 2.671839]\n",
      "epoch:5 step:5223 [D loss: 0.719831, acc: 60.16%] [G loss: 2.180520]\n",
      "epoch:5 step:5224 [D loss: 0.540773, acc: 73.44%] [G loss: 2.412689]\n",
      "epoch:5 step:5225 [D loss: 0.615597, acc: 68.75%] [G loss: 2.443554]\n",
      "epoch:5 step:5226 [D loss: 0.586963, acc: 67.97%] [G loss: 2.563082]\n",
      "epoch:5 step:5227 [D loss: 0.623117, acc: 63.28%] [G loss: 2.368357]\n",
      "epoch:5 step:5228 [D loss: 0.602585, acc: 69.53%] [G loss: 2.226394]\n",
      "epoch:5 step:5229 [D loss: 0.626956, acc: 67.19%] [G loss: 2.555724]\n",
      "epoch:5 step:5230 [D loss: 0.568225, acc: 68.75%] [G loss: 2.463885]\n",
      "epoch:5 step:5231 [D loss: 0.589388, acc: 72.66%] [G loss: 2.509265]\n",
      "epoch:5 step:5232 [D loss: 0.553030, acc: 69.53%] [G loss: 2.515985]\n",
      "epoch:5 step:5233 [D loss: 0.608204, acc: 66.41%] [G loss: 2.550769]\n",
      "epoch:5 step:5234 [D loss: 0.580511, acc: 70.31%] [G loss: 2.603122]\n",
      "epoch:5 step:5235 [D loss: 0.601804, acc: 64.84%] [G loss: 2.648882]\n",
      "epoch:5 step:5236 [D loss: 0.530345, acc: 74.22%] [G loss: 2.577031]\n",
      "epoch:5 step:5237 [D loss: 0.535584, acc: 79.69%] [G loss: 2.861487]\n",
      "epoch:5 step:5238 [D loss: 0.571822, acc: 67.97%] [G loss: 2.598758]\n",
      "epoch:5 step:5239 [D loss: 0.558984, acc: 75.00%] [G loss: 2.874472]\n",
      "epoch:5 step:5240 [D loss: 0.685607, acc: 60.94%] [G loss: 2.512947]\n",
      "epoch:5 step:5241 [D loss: 0.536372, acc: 75.00%] [G loss: 2.663841]\n",
      "epoch:5 step:5242 [D loss: 0.601828, acc: 71.09%] [G loss: 2.609420]\n",
      "epoch:5 step:5243 [D loss: 0.523446, acc: 71.88%] [G loss: 2.764638]\n",
      "epoch:5 step:5244 [D loss: 0.578070, acc: 67.19%] [G loss: 2.177498]\n",
      "epoch:5 step:5245 [D loss: 0.638556, acc: 67.97%] [G loss: 2.273050]\n",
      "epoch:5 step:5246 [D loss: 0.627324, acc: 67.19%] [G loss: 2.551120]\n",
      "epoch:5 step:5247 [D loss: 0.646533, acc: 63.28%] [G loss: 2.428281]\n",
      "epoch:5 step:5248 [D loss: 0.602023, acc: 68.75%] [G loss: 2.395315]\n",
      "epoch:5 step:5249 [D loss: 0.549771, acc: 69.53%] [G loss: 2.779092]\n",
      "epoch:5 step:5250 [D loss: 0.575603, acc: 71.09%] [G loss: 2.505072]\n",
      "epoch:5 step:5251 [D loss: 0.593043, acc: 66.41%] [G loss: 2.322920]\n",
      "epoch:5 step:5252 [D loss: 0.557934, acc: 71.09%] [G loss: 2.766503]\n",
      "epoch:5 step:5253 [D loss: 0.550471, acc: 74.22%] [G loss: 2.724037]\n",
      "epoch:5 step:5254 [D loss: 0.621130, acc: 67.97%] [G loss: 2.528433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5255 [D loss: 0.600878, acc: 70.31%] [G loss: 2.681691]\n",
      "epoch:5 step:5256 [D loss: 0.652019, acc: 66.41%] [G loss: 2.294533]\n",
      "epoch:5 step:5257 [D loss: 0.563532, acc: 67.19%] [G loss: 2.265588]\n",
      "epoch:5 step:5258 [D loss: 0.587062, acc: 70.31%] [G loss: 2.264926]\n",
      "epoch:5 step:5259 [D loss: 0.501998, acc: 77.34%] [G loss: 2.591681]\n",
      "epoch:5 step:5260 [D loss: 0.594933, acc: 67.97%] [G loss: 2.587250]\n",
      "epoch:5 step:5261 [D loss: 0.656668, acc: 60.94%] [G loss: 2.133158]\n",
      "epoch:5 step:5262 [D loss: 0.568231, acc: 66.41%] [G loss: 2.403938]\n",
      "epoch:5 step:5263 [D loss: 0.620222, acc: 67.19%] [G loss: 2.487865]\n",
      "epoch:5 step:5264 [D loss: 0.539052, acc: 78.12%] [G loss: 2.293371]\n",
      "epoch:5 step:5265 [D loss: 0.588829, acc: 66.41%] [G loss: 2.446043]\n",
      "epoch:5 step:5266 [D loss: 0.605744, acc: 68.75%] [G loss: 2.283071]\n",
      "epoch:5 step:5267 [D loss: 0.638652, acc: 62.50%] [G loss: 2.749355]\n",
      "epoch:5 step:5268 [D loss: 0.681522, acc: 64.84%] [G loss: 2.523090]\n",
      "epoch:5 step:5269 [D loss: 0.623198, acc: 67.19%] [G loss: 2.334603]\n",
      "epoch:5 step:5270 [D loss: 0.604535, acc: 69.53%] [G loss: 2.278351]\n",
      "epoch:5 step:5271 [D loss: 0.536597, acc: 74.22%] [G loss: 2.628106]\n",
      "epoch:5 step:5272 [D loss: 0.628300, acc: 64.84%] [G loss: 2.575662]\n",
      "epoch:5 step:5273 [D loss: 0.621774, acc: 65.62%] [G loss: 2.325756]\n",
      "epoch:5 step:5274 [D loss: 0.540577, acc: 67.97%] [G loss: 2.841739]\n",
      "epoch:5 step:5275 [D loss: 0.683209, acc: 58.59%] [G loss: 2.359781]\n",
      "epoch:5 step:5276 [D loss: 0.560609, acc: 71.88%] [G loss: 2.623398]\n",
      "epoch:5 step:5277 [D loss: 0.558037, acc: 74.22%] [G loss: 2.548357]\n",
      "epoch:5 step:5278 [D loss: 0.675125, acc: 59.38%] [G loss: 2.230803]\n",
      "epoch:5 step:5279 [D loss: 0.642671, acc: 61.72%] [G loss: 2.132285]\n",
      "epoch:5 step:5280 [D loss: 0.623290, acc: 72.66%] [G loss: 2.355386]\n",
      "epoch:5 step:5281 [D loss: 0.735743, acc: 57.81%] [G loss: 2.358033]\n",
      "epoch:5 step:5282 [D loss: 0.639257, acc: 64.06%] [G loss: 2.214632]\n",
      "epoch:5 step:5283 [D loss: 0.568027, acc: 71.88%] [G loss: 2.384597]\n",
      "epoch:5 step:5284 [D loss: 0.696814, acc: 59.38%] [G loss: 2.275952]\n",
      "epoch:5 step:5285 [D loss: 0.664135, acc: 66.41%] [G loss: 2.519134]\n",
      "epoch:5 step:5286 [D loss: 0.637020, acc: 71.09%] [G loss: 2.269162]\n",
      "epoch:5 step:5287 [D loss: 0.576999, acc: 70.31%] [G loss: 2.648314]\n",
      "epoch:5 step:5288 [D loss: 0.589516, acc: 67.19%] [G loss: 2.893027]\n",
      "epoch:5 step:5289 [D loss: 0.586684, acc: 68.75%] [G loss: 2.419857]\n",
      "epoch:5 step:5290 [D loss: 0.607534, acc: 64.84%] [G loss: 2.707097]\n",
      "epoch:5 step:5291 [D loss: 0.587698, acc: 66.41%] [G loss: 2.371117]\n",
      "epoch:5 step:5292 [D loss: 0.578264, acc: 66.41%] [G loss: 2.496144]\n",
      "epoch:5 step:5293 [D loss: 0.559632, acc: 71.09%] [G loss: 2.534053]\n",
      "epoch:5 step:5294 [D loss: 0.610681, acc: 69.53%] [G loss: 2.450674]\n",
      "epoch:5 step:5295 [D loss: 0.597639, acc: 70.31%] [G loss: 2.283916]\n",
      "epoch:5 step:5296 [D loss: 0.687224, acc: 64.84%] [G loss: 2.255354]\n",
      "epoch:5 step:5297 [D loss: 0.643793, acc: 67.97%] [G loss: 2.323514]\n",
      "epoch:5 step:5298 [D loss: 0.590086, acc: 67.19%] [G loss: 2.318144]\n",
      "epoch:5 step:5299 [D loss: 0.688407, acc: 57.03%] [G loss: 2.415701]\n",
      "epoch:5 step:5300 [D loss: 0.632929, acc: 60.16%] [G loss: 2.164140]\n",
      "epoch:5 step:5301 [D loss: 0.609054, acc: 67.97%] [G loss: 2.444726]\n",
      "epoch:5 step:5302 [D loss: 0.601055, acc: 64.84%] [G loss: 2.310220]\n",
      "epoch:5 step:5303 [D loss: 0.655427, acc: 60.16%] [G loss: 2.286728]\n",
      "epoch:5 step:5304 [D loss: 0.637557, acc: 64.84%] [G loss: 2.456106]\n",
      "epoch:5 step:5305 [D loss: 0.564023, acc: 73.44%] [G loss: 2.284052]\n",
      "epoch:5 step:5306 [D loss: 0.623766, acc: 60.94%] [G loss: 2.283521]\n",
      "epoch:5 step:5307 [D loss: 0.606667, acc: 64.84%] [G loss: 2.212082]\n",
      "epoch:5 step:5308 [D loss: 0.543908, acc: 73.44%] [G loss: 2.283376]\n",
      "epoch:5 step:5309 [D loss: 0.613016, acc: 64.84%] [G loss: 2.508670]\n",
      "epoch:5 step:5310 [D loss: 0.584781, acc: 75.00%] [G loss: 2.559049]\n",
      "epoch:5 step:5311 [D loss: 0.604780, acc: 69.53%] [G loss: 2.409686]\n",
      "epoch:5 step:5312 [D loss: 0.664820, acc: 59.38%] [G loss: 2.316473]\n",
      "epoch:5 step:5313 [D loss: 0.687278, acc: 64.06%] [G loss: 2.246672]\n",
      "epoch:5 step:5314 [D loss: 0.619843, acc: 67.19%] [G loss: 2.422276]\n",
      "epoch:5 step:5315 [D loss: 0.487096, acc: 81.25%] [G loss: 2.656508]\n",
      "epoch:5 step:5316 [D loss: 0.535123, acc: 74.22%] [G loss: 2.672976]\n",
      "epoch:5 step:5317 [D loss: 0.541029, acc: 74.22%] [G loss: 2.673975]\n",
      "epoch:5 step:5318 [D loss: 0.555118, acc: 77.34%] [G loss: 2.436212]\n",
      "epoch:5 step:5319 [D loss: 0.603568, acc: 67.19%] [G loss: 2.837555]\n",
      "epoch:5 step:5320 [D loss: 0.591133, acc: 71.88%] [G loss: 2.592421]\n",
      "epoch:5 step:5321 [D loss: 0.628390, acc: 67.19%] [G loss: 2.375067]\n",
      "epoch:5 step:5322 [D loss: 0.524127, acc: 75.00%] [G loss: 2.574362]\n",
      "epoch:5 step:5323 [D loss: 0.519431, acc: 71.88%] [G loss: 2.477307]\n",
      "epoch:5 step:5324 [D loss: 0.561797, acc: 71.09%] [G loss: 3.044511]\n",
      "epoch:5 step:5325 [D loss: 0.566829, acc: 70.31%] [G loss: 2.745320]\n",
      "epoch:5 step:5326 [D loss: 0.601900, acc: 72.66%] [G loss: 2.877989]\n",
      "epoch:5 step:5327 [D loss: 0.584743, acc: 71.88%] [G loss: 2.811127]\n",
      "epoch:5 step:5328 [D loss: 0.595070, acc: 67.19%] [G loss: 2.577209]\n",
      "epoch:5 step:5329 [D loss: 0.537847, acc: 73.44%] [G loss: 2.790154]\n",
      "epoch:5 step:5330 [D loss: 0.632880, acc: 68.75%] [G loss: 2.518875]\n",
      "epoch:5 step:5331 [D loss: 0.598775, acc: 71.09%] [G loss: 2.612318]\n",
      "epoch:5 step:5332 [D loss: 0.609990, acc: 67.97%] [G loss: 2.873673]\n",
      "epoch:5 step:5333 [D loss: 0.512376, acc: 77.34%] [G loss: 3.095152]\n",
      "epoch:5 step:5334 [D loss: 0.523740, acc: 75.78%] [G loss: 2.810536]\n",
      "epoch:5 step:5335 [D loss: 0.586582, acc: 70.31%] [G loss: 2.622591]\n",
      "epoch:5 step:5336 [D loss: 0.609237, acc: 67.97%] [G loss: 2.656971]\n",
      "epoch:5 step:5337 [D loss: 0.550943, acc: 68.75%] [G loss: 2.598523]\n",
      "epoch:5 step:5338 [D loss: 0.639676, acc: 64.06%] [G loss: 2.686829]\n",
      "epoch:5 step:5339 [D loss: 0.486419, acc: 78.91%] [G loss: 2.744239]\n",
      "epoch:5 step:5340 [D loss: 0.584923, acc: 67.19%] [G loss: 2.732617]\n",
      "epoch:5 step:5341 [D loss: 0.589423, acc: 71.09%] [G loss: 2.708204]\n",
      "epoch:5 step:5342 [D loss: 0.609330, acc: 61.72%] [G loss: 2.388075]\n",
      "epoch:5 step:5343 [D loss: 0.668043, acc: 60.94%] [G loss: 2.220310]\n",
      "epoch:5 step:5344 [D loss: 0.578638, acc: 71.09%] [G loss: 2.560170]\n",
      "epoch:5 step:5345 [D loss: 0.606178, acc: 66.41%] [G loss: 2.541366]\n",
      "epoch:5 step:5346 [D loss: 0.610995, acc: 71.88%] [G loss: 2.534024]\n",
      "epoch:5 step:5347 [D loss: 0.725034, acc: 58.59%] [G loss: 2.588301]\n",
      "epoch:5 step:5348 [D loss: 0.509462, acc: 76.56%] [G loss: 2.979629]\n",
      "epoch:5 step:5349 [D loss: 0.626301, acc: 68.75%] [G loss: 2.579945]\n",
      "epoch:5 step:5350 [D loss: 0.614183, acc: 66.41%] [G loss: 2.506143]\n",
      "epoch:5 step:5351 [D loss: 0.667049, acc: 67.19%] [G loss: 2.218918]\n",
      "epoch:5 step:5352 [D loss: 0.597841, acc: 67.19%] [G loss: 2.397994]\n",
      "epoch:5 step:5353 [D loss: 0.629671, acc: 67.97%] [G loss: 2.580698]\n",
      "epoch:5 step:5354 [D loss: 0.627608, acc: 68.75%] [G loss: 2.383164]\n",
      "epoch:5 step:5355 [D loss: 0.638990, acc: 65.62%] [G loss: 2.477927]\n",
      "epoch:5 step:5356 [D loss: 0.605901, acc: 65.62%] [G loss: 2.217846]\n",
      "epoch:5 step:5357 [D loss: 0.672057, acc: 60.94%] [G loss: 2.470460]\n",
      "epoch:5 step:5358 [D loss: 0.613024, acc: 62.50%] [G loss: 2.356769]\n",
      "epoch:5 step:5359 [D loss: 0.579928, acc: 70.31%] [G loss: 2.411462]\n",
      "epoch:5 step:5360 [D loss: 0.592095, acc: 67.97%] [G loss: 2.586879]\n",
      "epoch:5 step:5361 [D loss: 0.613988, acc: 67.19%] [G loss: 2.491868]\n",
      "epoch:5 step:5362 [D loss: 0.622550, acc: 64.84%] [G loss: 2.654751]\n",
      "epoch:5 step:5363 [D loss: 0.564367, acc: 71.09%] [G loss: 2.524768]\n",
      "epoch:5 step:5364 [D loss: 0.647809, acc: 64.84%] [G loss: 2.317981]\n",
      "epoch:5 step:5365 [D loss: 0.626039, acc: 67.97%] [G loss: 2.576858]\n",
      "epoch:5 step:5366 [D loss: 0.659178, acc: 62.50%] [G loss: 2.379314]\n",
      "epoch:5 step:5367 [D loss: 0.651632, acc: 61.72%] [G loss: 2.141671]\n",
      "epoch:5 step:5368 [D loss: 0.638042, acc: 61.72%] [G loss: 2.304228]\n",
      "epoch:5 step:5369 [D loss: 0.620121, acc: 64.06%] [G loss: 2.491075]\n",
      "epoch:5 step:5370 [D loss: 0.608579, acc: 67.97%] [G loss: 2.150643]\n",
      "epoch:5 step:5371 [D loss: 0.652712, acc: 61.72%] [G loss: 2.292733]\n",
      "epoch:5 step:5372 [D loss: 0.677336, acc: 65.62%] [G loss: 2.137646]\n",
      "epoch:5 step:5373 [D loss: 0.593059, acc: 66.41%] [G loss: 2.111372]\n",
      "epoch:5 step:5374 [D loss: 0.542135, acc: 75.00%] [G loss: 2.645233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5375 [D loss: 0.619266, acc: 72.66%] [G loss: 2.603168]\n",
      "epoch:5 step:5376 [D loss: 0.579893, acc: 68.75%] [G loss: 2.609685]\n",
      "epoch:5 step:5377 [D loss: 0.593353, acc: 66.41%] [G loss: 2.558719]\n",
      "epoch:5 step:5378 [D loss: 0.559358, acc: 68.75%] [G loss: 2.701903]\n",
      "epoch:5 step:5379 [D loss: 0.581578, acc: 70.31%] [G loss: 2.656873]\n",
      "epoch:5 step:5380 [D loss: 0.565779, acc: 71.09%] [G loss: 2.219168]\n",
      "epoch:5 step:5381 [D loss: 0.616488, acc: 64.84%] [G loss: 2.441067]\n",
      "epoch:5 step:5382 [D loss: 0.621839, acc: 62.50%] [G loss: 2.392303]\n",
      "epoch:5 step:5383 [D loss: 0.652562, acc: 62.50%] [G loss: 2.296933]\n",
      "epoch:5 step:5384 [D loss: 0.544414, acc: 67.97%] [G loss: 2.786151]\n",
      "epoch:5 step:5385 [D loss: 0.533117, acc: 71.88%] [G loss: 2.624133]\n",
      "epoch:5 step:5386 [D loss: 0.558963, acc: 70.31%] [G loss: 2.407177]\n",
      "epoch:5 step:5387 [D loss: 0.671383, acc: 63.28%] [G loss: 2.518610]\n",
      "epoch:5 step:5388 [D loss: 0.612054, acc: 66.41%] [G loss: 2.527956]\n",
      "epoch:5 step:5389 [D loss: 0.615518, acc: 64.84%] [G loss: 2.395222]\n",
      "epoch:5 step:5390 [D loss: 0.669675, acc: 62.50%] [G loss: 2.532180]\n",
      "epoch:5 step:5391 [D loss: 0.585841, acc: 71.88%] [G loss: 2.394081]\n",
      "epoch:5 step:5392 [D loss: 0.446840, acc: 82.03%] [G loss: 2.941931]\n",
      "epoch:5 step:5393 [D loss: 0.553214, acc: 75.00%] [G loss: 2.958348]\n",
      "epoch:5 step:5394 [D loss: 0.523265, acc: 75.00%] [G loss: 2.995612]\n",
      "epoch:5 step:5395 [D loss: 0.661877, acc: 61.72%] [G loss: 2.291078]\n",
      "epoch:5 step:5396 [D loss: 0.634311, acc: 70.31%] [G loss: 2.308284]\n",
      "epoch:5 step:5397 [D loss: 0.627416, acc: 64.84%] [G loss: 2.478585]\n",
      "epoch:5 step:5398 [D loss: 0.564096, acc: 69.53%] [G loss: 2.524155]\n",
      "epoch:5 step:5399 [D loss: 0.540245, acc: 67.19%] [G loss: 2.474965]\n",
      "epoch:5 step:5400 [D loss: 0.595998, acc: 66.41%] [G loss: 2.518538]\n",
      "##############\n",
      "[2.96159096 1.69133894 6.89919219 5.30983698 4.12337327 5.92987081\n",
      " 5.24081948 5.01863981 5.30168607 3.76565431]\n",
      "##########\n",
      "epoch:5 step:5401 [D loss: 0.574744, acc: 68.75%] [G loss: 2.621557]\n",
      "epoch:5 step:5402 [D loss: 0.726193, acc: 50.00%] [G loss: 2.460641]\n",
      "epoch:5 step:5403 [D loss: 0.620602, acc: 65.62%] [G loss: 2.457139]\n",
      "epoch:5 step:5404 [D loss: 0.594050, acc: 69.53%] [G loss: 2.513860]\n",
      "epoch:5 step:5405 [D loss: 0.675095, acc: 64.06%] [G loss: 2.334178]\n",
      "epoch:5 step:5406 [D loss: 0.581690, acc: 73.44%] [G loss: 2.359550]\n",
      "epoch:5 step:5407 [D loss: 0.589735, acc: 67.19%] [G loss: 2.379737]\n",
      "epoch:5 step:5408 [D loss: 0.683763, acc: 60.94%] [G loss: 2.052404]\n",
      "epoch:5 step:5409 [D loss: 0.547600, acc: 73.44%] [G loss: 2.635160]\n",
      "epoch:5 step:5410 [D loss: 0.494994, acc: 78.91%] [G loss: 2.562753]\n",
      "epoch:5 step:5411 [D loss: 0.529015, acc: 71.88%] [G loss: 2.847077]\n",
      "epoch:5 step:5412 [D loss: 0.593463, acc: 69.53%] [G loss: 2.284109]\n",
      "epoch:5 step:5413 [D loss: 0.576581, acc: 71.09%] [G loss: 2.639047]\n",
      "epoch:5 step:5414 [D loss: 0.652351, acc: 65.62%] [G loss: 2.316979]\n",
      "epoch:5 step:5415 [D loss: 0.605469, acc: 67.19%] [G loss: 2.536013]\n",
      "epoch:5 step:5416 [D loss: 0.606697, acc: 67.19%] [G loss: 2.192051]\n",
      "epoch:5 step:5417 [D loss: 0.588783, acc: 73.44%] [G loss: 2.410349]\n",
      "epoch:5 step:5418 [D loss: 0.539750, acc: 74.22%] [G loss: 2.595369]\n",
      "epoch:5 step:5419 [D loss: 0.542319, acc: 74.22%] [G loss: 2.510626]\n",
      "epoch:5 step:5420 [D loss: 0.612141, acc: 65.62%] [G loss: 2.446658]\n",
      "epoch:5 step:5421 [D loss: 0.593042, acc: 70.31%] [G loss: 2.671326]\n",
      "epoch:5 step:5422 [D loss: 0.511013, acc: 75.78%] [G loss: 2.360122]\n",
      "epoch:5 step:5423 [D loss: 0.677165, acc: 60.94%] [G loss: 2.095685]\n",
      "epoch:5 step:5424 [D loss: 0.643954, acc: 63.28%] [G loss: 2.372293]\n",
      "epoch:5 step:5425 [D loss: 0.600072, acc: 72.66%] [G loss: 2.340590]\n",
      "epoch:5 step:5426 [D loss: 0.596777, acc: 66.41%] [G loss: 2.518608]\n",
      "epoch:5 step:5427 [D loss: 0.673027, acc: 62.50%] [G loss: 2.463283]\n",
      "epoch:5 step:5428 [D loss: 0.567384, acc: 75.00%] [G loss: 2.532236]\n",
      "epoch:5 step:5429 [D loss: 0.613920, acc: 67.97%] [G loss: 2.404453]\n",
      "epoch:5 step:5430 [D loss: 0.609584, acc: 67.97%] [G loss: 2.468004]\n",
      "epoch:5 step:5431 [D loss: 0.570847, acc: 68.75%] [G loss: 2.888749]\n",
      "epoch:5 step:5432 [D loss: 0.563971, acc: 73.44%] [G loss: 2.759663]\n",
      "epoch:5 step:5433 [D loss: 0.549103, acc: 69.53%] [G loss: 2.443634]\n",
      "epoch:5 step:5434 [D loss: 0.540369, acc: 72.66%] [G loss: 2.409247]\n",
      "epoch:5 step:5435 [D loss: 0.570534, acc: 67.19%] [G loss: 2.664615]\n",
      "epoch:5 step:5436 [D loss: 0.597540, acc: 67.97%] [G loss: 2.415612]\n",
      "epoch:5 step:5437 [D loss: 0.595889, acc: 69.53%] [G loss: 2.506350]\n",
      "epoch:5 step:5438 [D loss: 0.639839, acc: 64.84%] [G loss: 2.444805]\n",
      "epoch:5 step:5439 [D loss: 0.562517, acc: 72.66%] [G loss: 2.409948]\n",
      "epoch:5 step:5440 [D loss: 0.594462, acc: 61.72%] [G loss: 2.437722]\n",
      "epoch:5 step:5441 [D loss: 0.716747, acc: 59.38%] [G loss: 2.396807]\n",
      "epoch:5 step:5442 [D loss: 0.530316, acc: 71.88%] [G loss: 2.796520]\n",
      "epoch:5 step:5443 [D loss: 0.654763, acc: 66.41%] [G loss: 2.448043]\n",
      "epoch:5 step:5444 [D loss: 0.601626, acc: 64.84%] [G loss: 2.236310]\n",
      "epoch:5 step:5445 [D loss: 0.550457, acc: 71.09%] [G loss: 2.485431]\n",
      "epoch:5 step:5446 [D loss: 0.567789, acc: 71.09%] [G loss: 2.433562]\n",
      "epoch:5 step:5447 [D loss: 0.633479, acc: 66.41%] [G loss: 2.160011]\n",
      "epoch:5 step:5448 [D loss: 0.593953, acc: 68.75%] [G loss: 2.436336]\n",
      "epoch:5 step:5449 [D loss: 0.557206, acc: 69.53%] [G loss: 2.673316]\n",
      "epoch:5 step:5450 [D loss: 0.697310, acc: 56.25%] [G loss: 2.335800]\n",
      "epoch:5 step:5451 [D loss: 0.608245, acc: 65.62%] [G loss: 2.401613]\n",
      "epoch:5 step:5452 [D loss: 0.551538, acc: 71.09%] [G loss: 2.432654]\n",
      "epoch:5 step:5453 [D loss: 0.662356, acc: 63.28%] [G loss: 2.374704]\n",
      "epoch:5 step:5454 [D loss: 0.464847, acc: 78.12%] [G loss: 2.776706]\n",
      "epoch:5 step:5455 [D loss: 0.550788, acc: 72.66%] [G loss: 2.353394]\n",
      "epoch:5 step:5456 [D loss: 0.584270, acc: 72.66%] [G loss: 2.320926]\n",
      "epoch:5 step:5457 [D loss: 0.628757, acc: 66.41%] [G loss: 2.661630]\n",
      "epoch:5 step:5458 [D loss: 0.694500, acc: 62.50%] [G loss: 2.387707]\n",
      "epoch:5 step:5459 [D loss: 0.548782, acc: 72.66%] [G loss: 2.782904]\n",
      "epoch:5 step:5460 [D loss: 0.546837, acc: 71.09%] [G loss: 2.823597]\n",
      "epoch:5 step:5461 [D loss: 0.596657, acc: 67.19%] [G loss: 2.722657]\n",
      "epoch:5 step:5462 [D loss: 0.512294, acc: 77.34%] [G loss: 2.634413]\n",
      "epoch:5 step:5463 [D loss: 0.663065, acc: 59.38%] [G loss: 2.566589]\n",
      "epoch:5 step:5464 [D loss: 0.560266, acc: 71.88%] [G loss: 2.554443]\n",
      "epoch:5 step:5465 [D loss: 0.556360, acc: 72.66%] [G loss: 2.826289]\n",
      "epoch:5 step:5466 [D loss: 0.595376, acc: 70.31%] [G loss: 2.894082]\n",
      "epoch:5 step:5467 [D loss: 0.586569, acc: 65.62%] [G loss: 2.730302]\n",
      "epoch:5 step:5468 [D loss: 0.581972, acc: 65.62%] [G loss: 2.831440]\n",
      "epoch:5 step:5469 [D loss: 0.626187, acc: 70.31%] [G loss: 2.235542]\n",
      "epoch:5 step:5470 [D loss: 0.628430, acc: 63.28%] [G loss: 2.571589]\n",
      "epoch:5 step:5471 [D loss: 0.572229, acc: 72.66%] [G loss: 2.489062]\n",
      "epoch:5 step:5472 [D loss: 0.573185, acc: 72.66%] [G loss: 2.505312]\n",
      "epoch:5 step:5473 [D loss: 0.674600, acc: 60.94%] [G loss: 2.247643]\n",
      "epoch:5 step:5474 [D loss: 0.656446, acc: 65.62%] [G loss: 2.329587]\n",
      "epoch:5 step:5475 [D loss: 0.577432, acc: 74.22%] [G loss: 2.540792]\n",
      "epoch:5 step:5476 [D loss: 0.571133, acc: 69.53%] [G loss: 2.989882]\n",
      "epoch:5 step:5477 [D loss: 0.455769, acc: 82.81%] [G loss: 2.955113]\n",
      "epoch:5 step:5478 [D loss: 0.643727, acc: 65.62%] [G loss: 2.450228]\n",
      "epoch:5 step:5479 [D loss: 0.671464, acc: 62.50%] [G loss: 2.270380]\n",
      "epoch:5 step:5480 [D loss: 0.533026, acc: 68.75%] [G loss: 2.757075]\n",
      "epoch:5 step:5481 [D loss: 0.540576, acc: 72.66%] [G loss: 2.544471]\n",
      "epoch:5 step:5482 [D loss: 0.711993, acc: 61.72%] [G loss: 2.104663]\n",
      "epoch:5 step:5483 [D loss: 0.564231, acc: 74.22%] [G loss: 2.588084]\n",
      "epoch:5 step:5484 [D loss: 0.664328, acc: 59.38%] [G loss: 2.065310]\n",
      "epoch:5 step:5485 [D loss: 0.634063, acc: 62.50%] [G loss: 2.197791]\n",
      "epoch:5 step:5486 [D loss: 0.647436, acc: 67.19%] [G loss: 2.318242]\n",
      "epoch:5 step:5487 [D loss: 0.602591, acc: 67.19%] [G loss: 2.658939]\n",
      "epoch:5 step:5488 [D loss: 0.578303, acc: 69.53%] [G loss: 2.408269]\n",
      "epoch:5 step:5489 [D loss: 0.679015, acc: 60.94%] [G loss: 2.466675]\n",
      "epoch:5 step:5490 [D loss: 0.545444, acc: 76.56%] [G loss: 2.478913]\n",
      "epoch:5 step:5491 [D loss: 0.483868, acc: 77.34%] [G loss: 2.744042]\n",
      "epoch:5 step:5492 [D loss: 0.652237, acc: 60.94%] [G loss: 2.614524]\n",
      "epoch:5 step:5493 [D loss: 0.584663, acc: 69.53%] [G loss: 2.867254]\n",
      "epoch:5 step:5494 [D loss: 0.589672, acc: 71.09%] [G loss: 2.839451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5495 [D loss: 0.552772, acc: 75.00%] [G loss: 2.440028]\n",
      "epoch:5 step:5496 [D loss: 0.558052, acc: 75.00%] [G loss: 2.236151]\n",
      "epoch:5 step:5497 [D loss: 0.600047, acc: 66.41%] [G loss: 2.523147]\n",
      "epoch:5 step:5498 [D loss: 0.591710, acc: 68.75%] [G loss: 2.609037]\n",
      "epoch:5 step:5499 [D loss: 0.679813, acc: 59.38%] [G loss: 2.725285]\n",
      "epoch:5 step:5500 [D loss: 0.537681, acc: 75.00%] [G loss: 2.874226]\n",
      "epoch:5 step:5501 [D loss: 0.572137, acc: 72.66%] [G loss: 2.815039]\n",
      "epoch:5 step:5502 [D loss: 0.532191, acc: 74.22%] [G loss: 2.814463]\n",
      "epoch:5 step:5503 [D loss: 0.614357, acc: 63.28%] [G loss: 2.585126]\n",
      "epoch:5 step:5504 [D loss: 0.623992, acc: 64.06%] [G loss: 2.438096]\n",
      "epoch:5 step:5505 [D loss: 0.642771, acc: 60.94%] [G loss: 2.498858]\n",
      "epoch:5 step:5506 [D loss: 0.630298, acc: 67.19%] [G loss: 2.342815]\n",
      "epoch:5 step:5507 [D loss: 0.525874, acc: 75.00%] [G loss: 2.711022]\n",
      "epoch:5 step:5508 [D loss: 0.561618, acc: 69.53%] [G loss: 2.565711]\n",
      "epoch:5 step:5509 [D loss: 0.708264, acc: 54.69%] [G loss: 2.302567]\n",
      "epoch:5 step:5510 [D loss: 0.605078, acc: 71.88%] [G loss: 2.481395]\n",
      "epoch:5 step:5511 [D loss: 0.642618, acc: 65.62%] [G loss: 2.235562]\n",
      "epoch:5 step:5512 [D loss: 0.736322, acc: 60.94%] [G loss: 2.355024]\n",
      "epoch:5 step:5513 [D loss: 0.671971, acc: 60.94%] [G loss: 2.149541]\n",
      "epoch:5 step:5514 [D loss: 0.611531, acc: 71.09%] [G loss: 2.342603]\n",
      "epoch:5 step:5515 [D loss: 0.617761, acc: 68.75%] [G loss: 2.186568]\n",
      "epoch:5 step:5516 [D loss: 0.576990, acc: 70.31%] [G loss: 2.522549]\n",
      "epoch:5 step:5517 [D loss: 0.580739, acc: 71.09%] [G loss: 2.317450]\n",
      "epoch:5 step:5518 [D loss: 0.587540, acc: 66.41%] [G loss: 2.554701]\n",
      "epoch:5 step:5519 [D loss: 0.551221, acc: 71.09%] [G loss: 2.508720]\n",
      "epoch:5 step:5520 [D loss: 0.585155, acc: 71.09%] [G loss: 2.330940]\n",
      "epoch:5 step:5521 [D loss: 0.637515, acc: 63.28%] [G loss: 2.412846]\n",
      "epoch:5 step:5522 [D loss: 0.566002, acc: 67.97%] [G loss: 2.497232]\n",
      "epoch:5 step:5523 [D loss: 0.604901, acc: 61.72%] [G loss: 2.275057]\n",
      "epoch:5 step:5524 [D loss: 0.581826, acc: 71.09%] [G loss: 2.372362]\n",
      "epoch:5 step:5525 [D loss: 0.586793, acc: 64.84%] [G loss: 2.579865]\n",
      "epoch:5 step:5526 [D loss: 0.588934, acc: 74.22%] [G loss: 2.395701]\n",
      "epoch:5 step:5527 [D loss: 0.607353, acc: 71.09%] [G loss: 2.680465]\n",
      "epoch:5 step:5528 [D loss: 0.621707, acc: 68.75%] [G loss: 2.561328]\n",
      "epoch:5 step:5529 [D loss: 0.617362, acc: 67.19%] [G loss: 2.568265]\n",
      "epoch:5 step:5530 [D loss: 0.626969, acc: 59.38%] [G loss: 2.626955]\n",
      "epoch:5 step:5531 [D loss: 0.526154, acc: 74.22%] [G loss: 2.496842]\n",
      "epoch:5 step:5532 [D loss: 0.569279, acc: 73.44%] [G loss: 2.502863]\n",
      "epoch:5 step:5533 [D loss: 0.628172, acc: 64.84%] [G loss: 2.341521]\n",
      "epoch:5 step:5534 [D loss: 0.641083, acc: 62.50%] [G loss: 2.387373]\n",
      "epoch:5 step:5535 [D loss: 0.611143, acc: 60.94%] [G loss: 2.495464]\n",
      "epoch:5 step:5536 [D loss: 0.635716, acc: 64.06%] [G loss: 2.579138]\n",
      "epoch:5 step:5537 [D loss: 0.552211, acc: 72.66%] [G loss: 2.477356]\n",
      "epoch:5 step:5538 [D loss: 0.572278, acc: 70.31%] [G loss: 2.659532]\n",
      "epoch:5 step:5539 [D loss: 0.619968, acc: 61.72%] [G loss: 2.289320]\n",
      "epoch:5 step:5540 [D loss: 0.541698, acc: 74.22%] [G loss: 2.348920]\n",
      "epoch:5 step:5541 [D loss: 0.685715, acc: 67.19%] [G loss: 2.220466]\n",
      "epoch:5 step:5542 [D loss: 0.579101, acc: 67.19%] [G loss: 2.416538]\n",
      "epoch:5 step:5543 [D loss: 0.723848, acc: 62.50%] [G loss: 2.377832]\n",
      "epoch:5 step:5544 [D loss: 0.533956, acc: 74.22%] [G loss: 2.580343]\n",
      "epoch:5 step:5545 [D loss: 0.573581, acc: 70.31%] [G loss: 2.544295]\n",
      "epoch:5 step:5546 [D loss: 0.687085, acc: 58.59%] [G loss: 2.277975]\n",
      "epoch:5 step:5547 [D loss: 0.546753, acc: 67.19%] [G loss: 2.263702]\n",
      "epoch:5 step:5548 [D loss: 0.589484, acc: 69.53%] [G loss: 2.560247]\n",
      "epoch:5 step:5549 [D loss: 0.655313, acc: 64.06%] [G loss: 2.255512]\n",
      "epoch:5 step:5550 [D loss: 0.653703, acc: 64.84%] [G loss: 2.289097]\n",
      "epoch:5 step:5551 [D loss: 0.501058, acc: 79.69%] [G loss: 2.659738]\n",
      "epoch:5 step:5552 [D loss: 0.642023, acc: 62.50%] [G loss: 2.052328]\n",
      "epoch:5 step:5553 [D loss: 0.581052, acc: 67.97%] [G loss: 2.236531]\n",
      "epoch:5 step:5554 [D loss: 0.621502, acc: 66.41%] [G loss: 2.194313]\n",
      "epoch:5 step:5555 [D loss: 0.612581, acc: 64.06%] [G loss: 2.285016]\n",
      "epoch:5 step:5556 [D loss: 0.550663, acc: 76.56%] [G loss: 2.362979]\n",
      "epoch:5 step:5557 [D loss: 0.579382, acc: 71.88%] [G loss: 2.371706]\n",
      "epoch:5 step:5558 [D loss: 0.662278, acc: 61.72%] [G loss: 2.427139]\n",
      "epoch:5 step:5559 [D loss: 0.607824, acc: 64.84%] [G loss: 2.597217]\n",
      "epoch:5 step:5560 [D loss: 0.573295, acc: 67.97%] [G loss: 2.227974]\n",
      "epoch:5 step:5561 [D loss: 0.611450, acc: 67.19%] [G loss: 2.230060]\n",
      "epoch:5 step:5562 [D loss: 0.573774, acc: 72.66%] [G loss: 2.542274]\n",
      "epoch:5 step:5563 [D loss: 0.684861, acc: 62.50%] [G loss: 2.171202]\n",
      "epoch:5 step:5564 [D loss: 0.552672, acc: 74.22%] [G loss: 2.510777]\n",
      "epoch:5 step:5565 [D loss: 0.628078, acc: 68.75%] [G loss: 2.237035]\n",
      "epoch:5 step:5566 [D loss: 0.670818, acc: 60.16%] [G loss: 2.183117]\n",
      "epoch:5 step:5567 [D loss: 0.550688, acc: 75.00%] [G loss: 2.435620]\n",
      "epoch:5 step:5568 [D loss: 0.625851, acc: 61.72%] [G loss: 2.445161]\n",
      "epoch:5 step:5569 [D loss: 0.517328, acc: 76.56%] [G loss: 3.094151]\n",
      "epoch:5 step:5570 [D loss: 0.631507, acc: 66.41%] [G loss: 2.622213]\n",
      "epoch:5 step:5571 [D loss: 0.641696, acc: 67.97%] [G loss: 2.597472]\n",
      "epoch:5 step:5572 [D loss: 0.588247, acc: 69.53%] [G loss: 2.541989]\n",
      "epoch:5 step:5573 [D loss: 0.554565, acc: 71.09%] [G loss: 2.611154]\n",
      "epoch:5 step:5574 [D loss: 0.612194, acc: 64.84%] [G loss: 2.458334]\n",
      "epoch:5 step:5575 [D loss: 0.553188, acc: 70.31%] [G loss: 2.582504]\n",
      "epoch:5 step:5576 [D loss: 0.625597, acc: 70.31%] [G loss: 2.557300]\n",
      "epoch:5 step:5577 [D loss: 0.671422, acc: 66.41%] [G loss: 2.458910]\n",
      "epoch:5 step:5578 [D loss: 0.568502, acc: 67.97%] [G loss: 2.568907]\n",
      "epoch:5 step:5579 [D loss: 0.646440, acc: 66.41%] [G loss: 2.292004]\n",
      "epoch:5 step:5580 [D loss: 0.540163, acc: 75.00%] [G loss: 2.566190]\n",
      "epoch:5 step:5581 [D loss: 0.569790, acc: 71.88%] [G loss: 2.473600]\n",
      "epoch:5 step:5582 [D loss: 0.602425, acc: 67.19%] [G loss: 2.264207]\n",
      "epoch:5 step:5583 [D loss: 0.505602, acc: 73.44%] [G loss: 2.917248]\n",
      "epoch:5 step:5584 [D loss: 0.607207, acc: 64.84%] [G loss: 2.623305]\n",
      "epoch:5 step:5585 [D loss: 0.634287, acc: 67.19%] [G loss: 2.784228]\n",
      "epoch:5 step:5586 [D loss: 0.517015, acc: 73.44%] [G loss: 2.539781]\n",
      "epoch:5 step:5587 [D loss: 0.609272, acc: 67.19%] [G loss: 2.254222]\n",
      "epoch:5 step:5588 [D loss: 0.631461, acc: 64.84%] [G loss: 2.453054]\n",
      "epoch:5 step:5589 [D loss: 0.588925, acc: 67.19%] [G loss: 2.700093]\n",
      "epoch:5 step:5590 [D loss: 0.610324, acc: 64.84%] [G loss: 2.486766]\n",
      "epoch:5 step:5591 [D loss: 0.585895, acc: 73.44%] [G loss: 2.714996]\n",
      "epoch:5 step:5592 [D loss: 0.585161, acc: 72.66%] [G loss: 2.369719]\n",
      "epoch:5 step:5593 [D loss: 0.629713, acc: 61.72%] [G loss: 2.521127]\n",
      "epoch:5 step:5594 [D loss: 0.590055, acc: 68.75%] [G loss: 2.901955]\n",
      "epoch:5 step:5595 [D loss: 0.623658, acc: 64.84%] [G loss: 2.731193]\n",
      "epoch:5 step:5596 [D loss: 0.600229, acc: 70.31%] [G loss: 2.719436]\n",
      "epoch:5 step:5597 [D loss: 0.529586, acc: 75.78%] [G loss: 2.984509]\n",
      "epoch:5 step:5598 [D loss: 0.625256, acc: 67.19%] [G loss: 2.828900]\n",
      "epoch:5 step:5599 [D loss: 0.593879, acc: 71.09%] [G loss: 2.353383]\n",
      "epoch:5 step:5600 [D loss: 0.642539, acc: 61.72%] [G loss: 2.336720]\n",
      "##############\n",
      "[2.77250728 1.75648723 6.71555287 5.34941067 4.08782398 5.92335028\n",
      " 5.04576698 5.08220236 5.2502359  3.76159351]\n",
      "##########\n",
      "epoch:5 step:5601 [D loss: 0.578292, acc: 68.75%] [G loss: 2.253671]\n",
      "epoch:5 step:5602 [D loss: 0.644420, acc: 62.50%] [G loss: 2.421595]\n",
      "epoch:5 step:5603 [D loss: 0.567340, acc: 73.44%] [G loss: 2.683810]\n",
      "epoch:5 step:5604 [D loss: 0.530446, acc: 75.00%] [G loss: 2.947363]\n",
      "epoch:5 step:5605 [D loss: 0.678119, acc: 65.62%] [G loss: 2.354341]\n",
      "epoch:5 step:5606 [D loss: 0.594940, acc: 64.84%] [G loss: 2.756485]\n",
      "epoch:5 step:5607 [D loss: 0.578282, acc: 67.19%] [G loss: 2.541531]\n",
      "epoch:5 step:5608 [D loss: 0.590506, acc: 71.09%] [G loss: 2.788564]\n",
      "epoch:5 step:5609 [D loss: 0.506356, acc: 78.91%] [G loss: 2.879492]\n",
      "epoch:5 step:5610 [D loss: 0.630356, acc: 64.84%] [G loss: 2.828043]\n",
      "epoch:5 step:5611 [D loss: 0.586167, acc: 69.53%] [G loss: 2.948979]\n",
      "epoch:5 step:5612 [D loss: 0.509320, acc: 71.88%] [G loss: 2.819123]\n",
      "epoch:5 step:5613 [D loss: 0.736207, acc: 64.06%] [G loss: 2.785981]\n",
      "epoch:5 step:5614 [D loss: 0.625590, acc: 67.97%] [G loss: 2.847494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5615 [D loss: 0.522919, acc: 75.78%] [G loss: 2.537688]\n",
      "epoch:5 step:5616 [D loss: 0.585392, acc: 70.31%] [G loss: 2.607338]\n",
      "epoch:5 step:5617 [D loss: 0.595226, acc: 68.75%] [G loss: 2.453119]\n",
      "epoch:5 step:5618 [D loss: 0.599110, acc: 75.00%] [G loss: 2.452645]\n",
      "epoch:5 step:5619 [D loss: 0.601174, acc: 67.97%] [G loss: 2.746073]\n",
      "epoch:5 step:5620 [D loss: 0.619444, acc: 67.97%] [G loss: 2.396105]\n",
      "epoch:5 step:5621 [D loss: 0.586220, acc: 68.75%] [G loss: 2.503708]\n",
      "epoch:5 step:5622 [D loss: 0.556637, acc: 75.00%] [G loss: 3.217060]\n",
      "epoch:6 step:5623 [D loss: 0.627684, acc: 68.75%] [G loss: 2.487098]\n",
      "epoch:6 step:5624 [D loss: 0.576325, acc: 69.53%] [G loss: 2.782788]\n",
      "epoch:6 step:5625 [D loss: 0.665894, acc: 64.06%] [G loss: 2.598411]\n",
      "epoch:6 step:5626 [D loss: 0.594644, acc: 69.53%] [G loss: 2.632045]\n",
      "epoch:6 step:5627 [D loss: 0.631295, acc: 61.72%] [G loss: 2.415915]\n",
      "epoch:6 step:5628 [D loss: 0.583864, acc: 69.53%] [G loss: 2.635290]\n",
      "epoch:6 step:5629 [D loss: 0.581485, acc: 67.19%] [G loss: 2.853942]\n",
      "epoch:6 step:5630 [D loss: 0.609761, acc: 65.62%] [G loss: 2.513563]\n",
      "epoch:6 step:5631 [D loss: 0.620892, acc: 64.84%] [G loss: 2.556476]\n",
      "epoch:6 step:5632 [D loss: 0.532221, acc: 71.09%] [G loss: 2.786645]\n",
      "epoch:6 step:5633 [D loss: 0.630377, acc: 67.19%] [G loss: 2.519565]\n",
      "epoch:6 step:5634 [D loss: 0.623745, acc: 70.31%] [G loss: 2.374641]\n",
      "epoch:6 step:5635 [D loss: 0.601089, acc: 68.75%] [G loss: 2.625520]\n",
      "epoch:6 step:5636 [D loss: 0.661998, acc: 66.41%] [G loss: 2.490246]\n",
      "epoch:6 step:5637 [D loss: 0.602648, acc: 65.62%] [G loss: 2.675306]\n",
      "epoch:6 step:5638 [D loss: 0.508876, acc: 75.78%] [G loss: 2.497794]\n",
      "epoch:6 step:5639 [D loss: 0.589787, acc: 66.41%] [G loss: 2.424565]\n",
      "epoch:6 step:5640 [D loss: 0.674971, acc: 64.06%] [G loss: 2.278693]\n",
      "epoch:6 step:5641 [D loss: 0.629556, acc: 62.50%] [G loss: 2.305215]\n",
      "epoch:6 step:5642 [D loss: 0.706199, acc: 58.59%] [G loss: 2.185359]\n",
      "epoch:6 step:5643 [D loss: 0.614741, acc: 64.84%] [G loss: 2.461657]\n",
      "epoch:6 step:5644 [D loss: 0.579571, acc: 65.62%] [G loss: 2.585917]\n",
      "epoch:6 step:5645 [D loss: 0.588015, acc: 64.06%] [G loss: 2.403607]\n",
      "epoch:6 step:5646 [D loss: 0.529315, acc: 78.91%] [G loss: 2.550935]\n",
      "epoch:6 step:5647 [D loss: 0.541699, acc: 71.09%] [G loss: 2.723157]\n",
      "epoch:6 step:5648 [D loss: 0.568202, acc: 68.75%] [G loss: 2.461741]\n",
      "epoch:6 step:5649 [D loss: 0.567680, acc: 75.00%] [G loss: 2.740385]\n",
      "epoch:6 step:5650 [D loss: 0.668056, acc: 64.06%] [G loss: 2.477304]\n",
      "epoch:6 step:5651 [D loss: 0.601201, acc: 68.75%] [G loss: 2.641426]\n",
      "epoch:6 step:5652 [D loss: 0.615103, acc: 67.19%] [G loss: 2.220477]\n",
      "epoch:6 step:5653 [D loss: 0.593566, acc: 67.19%] [G loss: 2.225384]\n",
      "epoch:6 step:5654 [D loss: 0.611120, acc: 66.41%] [G loss: 2.348186]\n",
      "epoch:6 step:5655 [D loss: 0.613062, acc: 70.31%] [G loss: 2.370077]\n",
      "epoch:6 step:5656 [D loss: 0.578060, acc: 68.75%] [G loss: 2.386541]\n",
      "epoch:6 step:5657 [D loss: 0.691756, acc: 61.72%] [G loss: 2.368132]\n",
      "epoch:6 step:5658 [D loss: 0.591770, acc: 71.09%] [G loss: 2.354305]\n",
      "epoch:6 step:5659 [D loss: 0.537786, acc: 74.22%] [G loss: 2.474985]\n",
      "epoch:6 step:5660 [D loss: 0.603582, acc: 65.62%] [G loss: 2.528757]\n",
      "epoch:6 step:5661 [D loss: 0.527764, acc: 73.44%] [G loss: 2.504131]\n",
      "epoch:6 step:5662 [D loss: 0.517981, acc: 72.66%] [G loss: 2.588373]\n",
      "epoch:6 step:5663 [D loss: 0.559730, acc: 68.75%] [G loss: 2.232146]\n",
      "epoch:6 step:5664 [D loss: 0.613495, acc: 69.53%] [G loss: 2.615129]\n",
      "epoch:6 step:5665 [D loss: 0.529387, acc: 73.44%] [G loss: 2.766823]\n",
      "epoch:6 step:5666 [D loss: 0.675943, acc: 57.81%] [G loss: 2.216322]\n",
      "epoch:6 step:5667 [D loss: 0.573895, acc: 70.31%] [G loss: 2.381835]\n",
      "epoch:6 step:5668 [D loss: 0.547051, acc: 71.88%] [G loss: 2.615747]\n",
      "epoch:6 step:5669 [D loss: 0.563023, acc: 74.22%] [G loss: 2.423005]\n",
      "epoch:6 step:5670 [D loss: 0.543889, acc: 71.09%] [G loss: 2.615083]\n",
      "epoch:6 step:5671 [D loss: 0.614456, acc: 64.06%] [G loss: 2.343220]\n",
      "epoch:6 step:5672 [D loss: 0.606631, acc: 65.62%] [G loss: 2.467370]\n",
      "epoch:6 step:5673 [D loss: 0.599486, acc: 64.84%] [G loss: 2.434852]\n",
      "epoch:6 step:5674 [D loss: 0.564710, acc: 68.75%] [G loss: 2.406043]\n",
      "epoch:6 step:5675 [D loss: 0.670207, acc: 66.41%] [G loss: 2.487579]\n",
      "epoch:6 step:5676 [D loss: 0.590904, acc: 67.19%] [G loss: 2.570747]\n",
      "epoch:6 step:5677 [D loss: 0.621760, acc: 64.84%] [G loss: 2.694760]\n",
      "epoch:6 step:5678 [D loss: 0.593376, acc: 74.22%] [G loss: 2.532239]\n",
      "epoch:6 step:5679 [D loss: 0.591263, acc: 67.97%] [G loss: 2.531822]\n",
      "epoch:6 step:5680 [D loss: 0.575136, acc: 67.97%] [G loss: 2.423066]\n",
      "epoch:6 step:5681 [D loss: 0.630798, acc: 66.41%] [G loss: 2.227903]\n",
      "epoch:6 step:5682 [D loss: 0.584839, acc: 68.75%] [G loss: 2.689014]\n",
      "epoch:6 step:5683 [D loss: 0.530603, acc: 73.44%] [G loss: 2.534081]\n",
      "epoch:6 step:5684 [D loss: 0.627248, acc: 62.50%] [G loss: 2.387140]\n",
      "epoch:6 step:5685 [D loss: 0.639624, acc: 67.19%] [G loss: 2.516361]\n",
      "epoch:6 step:5686 [D loss: 0.589253, acc: 67.97%] [G loss: 2.361751]\n",
      "epoch:6 step:5687 [D loss: 0.608720, acc: 69.53%] [G loss: 2.314927]\n",
      "epoch:6 step:5688 [D loss: 0.683892, acc: 60.16%] [G loss: 2.308327]\n",
      "epoch:6 step:5689 [D loss: 0.605504, acc: 62.50%] [G loss: 2.281064]\n",
      "epoch:6 step:5690 [D loss: 0.579102, acc: 69.53%] [G loss: 2.562838]\n",
      "epoch:6 step:5691 [D loss: 0.602736, acc: 68.75%] [G loss: 2.546051]\n",
      "epoch:6 step:5692 [D loss: 0.619785, acc: 61.72%] [G loss: 2.486502]\n",
      "epoch:6 step:5693 [D loss: 0.582613, acc: 66.41%] [G loss: 2.340568]\n",
      "epoch:6 step:5694 [D loss: 0.584034, acc: 73.44%] [G loss: 2.685749]\n",
      "epoch:6 step:5695 [D loss: 0.616892, acc: 64.06%] [G loss: 2.421000]\n",
      "epoch:6 step:5696 [D loss: 0.585850, acc: 67.19%] [G loss: 2.491173]\n",
      "epoch:6 step:5697 [D loss: 0.543394, acc: 71.88%] [G loss: 2.626268]\n",
      "epoch:6 step:5698 [D loss: 0.609153, acc: 65.62%] [G loss: 2.767563]\n",
      "epoch:6 step:5699 [D loss: 0.483736, acc: 77.34%] [G loss: 3.023003]\n",
      "epoch:6 step:5700 [D loss: 0.592877, acc: 66.41%] [G loss: 2.649916]\n",
      "epoch:6 step:5701 [D loss: 0.625809, acc: 68.75%] [G loss: 2.668933]\n",
      "epoch:6 step:5702 [D loss: 0.655503, acc: 61.72%] [G loss: 2.320971]\n",
      "epoch:6 step:5703 [D loss: 0.605887, acc: 69.53%] [G loss: 2.379216]\n",
      "epoch:6 step:5704 [D loss: 0.564964, acc: 72.66%] [G loss: 2.724414]\n",
      "epoch:6 step:5705 [D loss: 0.500851, acc: 78.12%] [G loss: 2.362601]\n",
      "epoch:6 step:5706 [D loss: 0.659797, acc: 58.59%] [G loss: 2.471039]\n",
      "epoch:6 step:5707 [D loss: 0.604112, acc: 70.31%] [G loss: 2.438008]\n",
      "epoch:6 step:5708 [D loss: 0.613413, acc: 64.06%] [G loss: 2.231700]\n",
      "epoch:6 step:5709 [D loss: 0.624689, acc: 69.53%] [G loss: 2.409801]\n",
      "epoch:6 step:5710 [D loss: 0.651795, acc: 55.47%] [G loss: 2.327554]\n",
      "epoch:6 step:5711 [D loss: 0.560332, acc: 69.53%] [G loss: 2.347104]\n",
      "epoch:6 step:5712 [D loss: 0.584937, acc: 70.31%] [G loss: 2.355966]\n",
      "epoch:6 step:5713 [D loss: 0.652465, acc: 60.94%] [G loss: 2.375874]\n",
      "epoch:6 step:5714 [D loss: 0.521092, acc: 78.12%] [G loss: 2.726118]\n",
      "epoch:6 step:5715 [D loss: 0.573570, acc: 67.19%] [G loss: 2.633261]\n",
      "epoch:6 step:5716 [D loss: 0.607451, acc: 63.28%] [G loss: 2.515200]\n",
      "epoch:6 step:5717 [D loss: 0.674170, acc: 60.94%] [G loss: 2.277678]\n",
      "epoch:6 step:5718 [D loss: 0.586383, acc: 75.00%] [G loss: 2.385911]\n",
      "epoch:6 step:5719 [D loss: 0.587278, acc: 71.09%] [G loss: 2.415004]\n",
      "epoch:6 step:5720 [D loss: 0.688083, acc: 64.84%] [G loss: 2.358164]\n",
      "epoch:6 step:5721 [D loss: 0.607676, acc: 64.06%] [G loss: 2.252811]\n",
      "epoch:6 step:5722 [D loss: 0.554705, acc: 70.31%] [G loss: 2.474304]\n",
      "epoch:6 step:5723 [D loss: 0.618066, acc: 64.06%] [G loss: 2.419132]\n",
      "epoch:6 step:5724 [D loss: 0.648551, acc: 58.59%] [G loss: 2.200888]\n",
      "epoch:6 step:5725 [D loss: 0.508939, acc: 76.56%] [G loss: 2.823234]\n",
      "epoch:6 step:5726 [D loss: 0.636197, acc: 65.62%] [G loss: 2.429128]\n",
      "epoch:6 step:5727 [D loss: 0.524925, acc: 73.44%] [G loss: 2.642186]\n",
      "epoch:6 step:5728 [D loss: 0.643081, acc: 64.84%] [G loss: 2.532774]\n",
      "epoch:6 step:5729 [D loss: 0.635747, acc: 67.19%] [G loss: 2.325084]\n",
      "epoch:6 step:5730 [D loss: 0.691703, acc: 56.25%] [G loss: 2.371492]\n",
      "epoch:6 step:5731 [D loss: 0.662864, acc: 62.50%] [G loss: 2.292045]\n",
      "epoch:6 step:5732 [D loss: 0.685569, acc: 58.59%] [G loss: 2.379980]\n",
      "epoch:6 step:5733 [D loss: 0.568906, acc: 75.00%] [G loss: 2.658095]\n",
      "epoch:6 step:5734 [D loss: 0.631837, acc: 63.28%] [G loss: 2.432743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5735 [D loss: 0.667575, acc: 61.72%] [G loss: 2.382576]\n",
      "epoch:6 step:5736 [D loss: 0.651378, acc: 66.41%] [G loss: 2.542113]\n",
      "epoch:6 step:5737 [D loss: 0.600520, acc: 68.75%] [G loss: 2.560447]\n",
      "epoch:6 step:5738 [D loss: 0.666457, acc: 61.72%] [G loss: 2.403778]\n",
      "epoch:6 step:5739 [D loss: 0.566916, acc: 76.56%] [G loss: 2.583012]\n",
      "epoch:6 step:5740 [D loss: 0.645981, acc: 60.94%] [G loss: 2.545406]\n",
      "epoch:6 step:5741 [D loss: 0.511160, acc: 76.56%] [G loss: 2.915575]\n",
      "epoch:6 step:5742 [D loss: 0.512886, acc: 77.34%] [G loss: 2.748636]\n",
      "epoch:6 step:5743 [D loss: 0.605378, acc: 64.84%] [G loss: 2.556950]\n",
      "epoch:6 step:5744 [D loss: 0.601999, acc: 67.97%] [G loss: 2.494992]\n",
      "epoch:6 step:5745 [D loss: 0.628254, acc: 63.28%] [G loss: 2.276325]\n",
      "epoch:6 step:5746 [D loss: 0.638962, acc: 65.62%] [G loss: 2.383474]\n",
      "epoch:6 step:5747 [D loss: 0.608764, acc: 67.97%] [G loss: 2.213941]\n",
      "epoch:6 step:5748 [D loss: 0.615034, acc: 67.19%] [G loss: 2.428221]\n",
      "epoch:6 step:5749 [D loss: 0.575327, acc: 70.31%] [G loss: 2.507123]\n",
      "epoch:6 step:5750 [D loss: 0.604597, acc: 68.75%] [G loss: 2.391524]\n",
      "epoch:6 step:5751 [D loss: 0.609256, acc: 68.75%] [G loss: 2.324044]\n",
      "epoch:6 step:5752 [D loss: 0.671382, acc: 58.59%] [G loss: 2.414699]\n",
      "epoch:6 step:5753 [D loss: 0.576854, acc: 69.53%] [G loss: 2.722640]\n",
      "epoch:6 step:5754 [D loss: 0.486407, acc: 79.69%] [G loss: 2.369343]\n",
      "epoch:6 step:5755 [D loss: 0.671937, acc: 63.28%] [G loss: 2.191864]\n",
      "epoch:6 step:5756 [D loss: 0.593466, acc: 72.66%] [G loss: 2.353070]\n",
      "epoch:6 step:5757 [D loss: 0.643557, acc: 64.06%] [G loss: 2.312115]\n",
      "epoch:6 step:5758 [D loss: 0.602931, acc: 65.62%] [G loss: 2.412723]\n",
      "epoch:6 step:5759 [D loss: 0.575909, acc: 72.66%] [G loss: 2.344895]\n",
      "epoch:6 step:5760 [D loss: 0.536990, acc: 71.09%] [G loss: 2.520355]\n",
      "epoch:6 step:5761 [D loss: 0.671233, acc: 56.25%] [G loss: 2.506572]\n",
      "epoch:6 step:5762 [D loss: 0.576391, acc: 67.97%] [G loss: 2.172098]\n",
      "epoch:6 step:5763 [D loss: 0.677942, acc: 53.91%] [G loss: 2.265234]\n",
      "epoch:6 step:5764 [D loss: 0.661225, acc: 62.50%] [G loss: 2.231876]\n",
      "epoch:6 step:5765 [D loss: 0.606160, acc: 69.53%] [G loss: 2.443738]\n",
      "epoch:6 step:5766 [D loss: 0.540043, acc: 72.66%] [G loss: 2.523296]\n",
      "epoch:6 step:5767 [D loss: 0.582459, acc: 69.53%] [G loss: 2.421842]\n",
      "epoch:6 step:5768 [D loss: 0.639725, acc: 64.84%] [G loss: 2.233493]\n",
      "epoch:6 step:5769 [D loss: 0.717370, acc: 59.38%] [G loss: 2.186235]\n",
      "epoch:6 step:5770 [D loss: 0.605105, acc: 61.72%] [G loss: 2.420103]\n",
      "epoch:6 step:5771 [D loss: 0.580461, acc: 74.22%] [G loss: 2.411966]\n",
      "epoch:6 step:5772 [D loss: 0.660175, acc: 63.28%] [G loss: 2.427728]\n",
      "epoch:6 step:5773 [D loss: 0.652020, acc: 61.72%] [G loss: 2.992773]\n",
      "epoch:6 step:5774 [D loss: 0.556326, acc: 72.66%] [G loss: 2.424929]\n",
      "epoch:6 step:5775 [D loss: 0.561634, acc: 67.97%] [G loss: 2.332618]\n",
      "epoch:6 step:5776 [D loss: 0.521216, acc: 75.00%] [G loss: 2.513955]\n",
      "epoch:6 step:5777 [D loss: 0.581510, acc: 70.31%] [G loss: 2.715185]\n",
      "epoch:6 step:5778 [D loss: 0.564074, acc: 70.31%] [G loss: 2.513062]\n",
      "epoch:6 step:5779 [D loss: 0.684262, acc: 53.91%] [G loss: 2.200814]\n",
      "epoch:6 step:5780 [D loss: 0.569600, acc: 72.66%] [G loss: 2.572237]\n",
      "epoch:6 step:5781 [D loss: 0.589530, acc: 67.97%] [G loss: 2.761852]\n",
      "epoch:6 step:5782 [D loss: 0.689787, acc: 59.38%] [G loss: 2.207176]\n",
      "epoch:6 step:5783 [D loss: 0.615458, acc: 69.53%] [G loss: 2.291780]\n",
      "epoch:6 step:5784 [D loss: 0.582852, acc: 67.19%] [G loss: 2.573694]\n",
      "epoch:6 step:5785 [D loss: 0.602741, acc: 66.41%] [G loss: 2.425276]\n",
      "epoch:6 step:5786 [D loss: 0.618573, acc: 67.19%] [G loss: 2.080435]\n",
      "epoch:6 step:5787 [D loss: 0.643074, acc: 66.41%] [G loss: 2.368785]\n",
      "epoch:6 step:5788 [D loss: 0.520718, acc: 73.44%] [G loss: 2.450772]\n",
      "epoch:6 step:5789 [D loss: 0.627929, acc: 66.41%] [G loss: 2.554182]\n",
      "epoch:6 step:5790 [D loss: 0.543206, acc: 70.31%] [G loss: 2.760821]\n",
      "epoch:6 step:5791 [D loss: 0.606892, acc: 68.75%] [G loss: 2.456943]\n",
      "epoch:6 step:5792 [D loss: 0.649238, acc: 63.28%] [G loss: 2.327851]\n",
      "epoch:6 step:5793 [D loss: 0.576716, acc: 67.97%] [G loss: 2.476593]\n",
      "epoch:6 step:5794 [D loss: 0.550041, acc: 74.22%] [G loss: 2.559357]\n",
      "epoch:6 step:5795 [D loss: 0.627478, acc: 64.06%] [G loss: 2.254488]\n",
      "epoch:6 step:5796 [D loss: 0.630223, acc: 67.19%] [G loss: 2.334195]\n",
      "epoch:6 step:5797 [D loss: 0.595806, acc: 61.72%] [G loss: 2.543264]\n",
      "epoch:6 step:5798 [D loss: 0.614316, acc: 60.16%] [G loss: 2.536120]\n",
      "epoch:6 step:5799 [D loss: 0.618876, acc: 68.75%] [G loss: 2.232197]\n",
      "epoch:6 step:5800 [D loss: 0.543261, acc: 71.88%] [G loss: 2.230231]\n",
      "##############\n",
      "[2.74986547 1.59504687 6.75144811 5.22570778 4.09005791 5.86888658\n",
      " 5.03262799 4.9572756  5.19080127 3.64714659]\n",
      "##########\n",
      "epoch:6 step:5801 [D loss: 0.606023, acc: 64.06%] [G loss: 2.227372]\n",
      "epoch:6 step:5802 [D loss: 0.674561, acc: 58.59%] [G loss: 2.479314]\n",
      "epoch:6 step:5803 [D loss: 0.618697, acc: 64.84%] [G loss: 2.322728]\n",
      "epoch:6 step:5804 [D loss: 0.683016, acc: 64.06%] [G loss: 2.260002]\n",
      "epoch:6 step:5805 [D loss: 0.616985, acc: 64.84%] [G loss: 2.308129]\n",
      "epoch:6 step:5806 [D loss: 0.619001, acc: 67.97%] [G loss: 2.254800]\n",
      "epoch:6 step:5807 [D loss: 0.690206, acc: 58.59%] [G loss: 2.212276]\n",
      "epoch:6 step:5808 [D loss: 0.562820, acc: 72.66%] [G loss: 2.369920]\n",
      "epoch:6 step:5809 [D loss: 0.636726, acc: 63.28%] [G loss: 2.086044]\n",
      "epoch:6 step:5810 [D loss: 0.602572, acc: 67.19%] [G loss: 2.302668]\n",
      "epoch:6 step:5811 [D loss: 0.627118, acc: 68.75%] [G loss: 2.311278]\n",
      "epoch:6 step:5812 [D loss: 0.562865, acc: 71.09%] [G loss: 2.537433]\n",
      "epoch:6 step:5813 [D loss: 0.628463, acc: 65.62%] [G loss: 2.405921]\n",
      "epoch:6 step:5814 [D loss: 0.553358, acc: 68.75%] [G loss: 2.814187]\n",
      "epoch:6 step:5815 [D loss: 0.534184, acc: 71.88%] [G loss: 2.786748]\n",
      "epoch:6 step:5816 [D loss: 0.506317, acc: 75.78%] [G loss: 2.803826]\n",
      "epoch:6 step:5817 [D loss: 0.589550, acc: 67.97%] [G loss: 2.742173]\n",
      "epoch:6 step:5818 [D loss: 0.624255, acc: 70.31%] [G loss: 2.221791]\n",
      "epoch:6 step:5819 [D loss: 0.595167, acc: 69.53%] [G loss: 2.599673]\n",
      "epoch:6 step:5820 [D loss: 0.539851, acc: 73.44%] [G loss: 2.710062]\n",
      "epoch:6 step:5821 [D loss: 0.619802, acc: 65.62%] [G loss: 2.183829]\n",
      "epoch:6 step:5822 [D loss: 0.642744, acc: 61.72%] [G loss: 2.142076]\n",
      "epoch:6 step:5823 [D loss: 0.594823, acc: 67.97%] [G loss: 2.335042]\n",
      "epoch:6 step:5824 [D loss: 0.629662, acc: 65.62%] [G loss: 2.564650]\n",
      "epoch:6 step:5825 [D loss: 0.606736, acc: 67.97%] [G loss: 2.158309]\n",
      "epoch:6 step:5826 [D loss: 0.620636, acc: 66.41%] [G loss: 2.122747]\n",
      "epoch:6 step:5827 [D loss: 0.644942, acc: 66.41%] [G loss: 2.537035]\n",
      "epoch:6 step:5828 [D loss: 0.550607, acc: 74.22%] [G loss: 2.700380]\n",
      "epoch:6 step:5829 [D loss: 0.572486, acc: 70.31%] [G loss: 2.752303]\n",
      "epoch:6 step:5830 [D loss: 0.463782, acc: 78.12%] [G loss: 2.875496]\n",
      "epoch:6 step:5831 [D loss: 0.547377, acc: 73.44%] [G loss: 2.983094]\n",
      "epoch:6 step:5832 [D loss: 0.674686, acc: 58.59%] [G loss: 2.263114]\n",
      "epoch:6 step:5833 [D loss: 0.624500, acc: 63.28%] [G loss: 2.248451]\n",
      "epoch:6 step:5834 [D loss: 0.695096, acc: 62.50%] [G loss: 2.311269]\n",
      "epoch:6 step:5835 [D loss: 0.575761, acc: 71.09%] [G loss: 2.268370]\n",
      "epoch:6 step:5836 [D loss: 0.620520, acc: 65.62%] [G loss: 2.268796]\n",
      "epoch:6 step:5837 [D loss: 0.595228, acc: 64.06%] [G loss: 2.217483]\n",
      "epoch:6 step:5838 [D loss: 0.666956, acc: 65.62%] [G loss: 2.333942]\n",
      "epoch:6 step:5839 [D loss: 0.611421, acc: 62.50%] [G loss: 2.661837]\n",
      "epoch:6 step:5840 [D loss: 0.542501, acc: 72.66%] [G loss: 2.687522]\n",
      "epoch:6 step:5841 [D loss: 0.651408, acc: 65.62%] [G loss: 2.522728]\n",
      "epoch:6 step:5842 [D loss: 0.695431, acc: 51.56%] [G loss: 2.368654]\n",
      "epoch:6 step:5843 [D loss: 0.557338, acc: 71.88%] [G loss: 2.448034]\n",
      "epoch:6 step:5844 [D loss: 0.601591, acc: 69.53%] [G loss: 2.413028]\n",
      "epoch:6 step:5845 [D loss: 0.533334, acc: 75.78%] [G loss: 2.439710]\n",
      "epoch:6 step:5846 [D loss: 0.610055, acc: 65.62%] [G loss: 2.166445]\n",
      "epoch:6 step:5847 [D loss: 0.729073, acc: 59.38%] [G loss: 2.283873]\n",
      "epoch:6 step:5848 [D loss: 0.709490, acc: 63.28%] [G loss: 2.164186]\n",
      "epoch:6 step:5849 [D loss: 0.609754, acc: 69.53%] [G loss: 2.125029]\n",
      "epoch:6 step:5850 [D loss: 0.604858, acc: 67.97%] [G loss: 2.206615]\n",
      "epoch:6 step:5851 [D loss: 0.481518, acc: 79.69%] [G loss: 2.263628]\n",
      "epoch:6 step:5852 [D loss: 0.543053, acc: 68.75%] [G loss: 2.600994]\n",
      "epoch:6 step:5853 [D loss: 0.573807, acc: 72.66%] [G loss: 2.716809]\n",
      "epoch:6 step:5854 [D loss: 0.555654, acc: 73.44%] [G loss: 2.852943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5855 [D loss: 0.586773, acc: 66.41%] [G loss: 2.690951]\n",
      "epoch:6 step:5856 [D loss: 0.599797, acc: 64.84%] [G loss: 2.324324]\n",
      "epoch:6 step:5857 [D loss: 0.681812, acc: 63.28%] [G loss: 2.354569]\n",
      "epoch:6 step:5858 [D loss: 0.640406, acc: 65.62%] [G loss: 2.307909]\n",
      "epoch:6 step:5859 [D loss: 0.570453, acc: 67.19%] [G loss: 2.311257]\n",
      "epoch:6 step:5860 [D loss: 0.630472, acc: 73.44%] [G loss: 2.595775]\n",
      "epoch:6 step:5861 [D loss: 0.551258, acc: 75.78%] [G loss: 2.120803]\n",
      "epoch:6 step:5862 [D loss: 0.565656, acc: 67.19%] [G loss: 2.700816]\n",
      "epoch:6 step:5863 [D loss: 0.598896, acc: 70.31%] [G loss: 2.411372]\n",
      "epoch:6 step:5864 [D loss: 0.600936, acc: 68.75%] [G loss: 2.429801]\n",
      "epoch:6 step:5865 [D loss: 0.597346, acc: 64.06%] [G loss: 2.387855]\n",
      "epoch:6 step:5866 [D loss: 0.558216, acc: 75.00%] [G loss: 2.623876]\n",
      "epoch:6 step:5867 [D loss: 0.620035, acc: 64.84%] [G loss: 2.469647]\n",
      "epoch:6 step:5868 [D loss: 0.690522, acc: 60.16%] [G loss: 2.424832]\n",
      "epoch:6 step:5869 [D loss: 0.603897, acc: 64.06%] [G loss: 2.309774]\n",
      "epoch:6 step:5870 [D loss: 0.547787, acc: 75.00%] [G loss: 2.515029]\n",
      "epoch:6 step:5871 [D loss: 0.642955, acc: 64.84%] [G loss: 2.348047]\n",
      "epoch:6 step:5872 [D loss: 0.666880, acc: 60.16%] [G loss: 2.309298]\n",
      "epoch:6 step:5873 [D loss: 0.726907, acc: 56.25%] [G loss: 2.292792]\n",
      "epoch:6 step:5874 [D loss: 0.642272, acc: 60.94%] [G loss: 2.200727]\n",
      "epoch:6 step:5875 [D loss: 0.583659, acc: 68.75%] [G loss: 2.249919]\n",
      "epoch:6 step:5876 [D loss: 0.632418, acc: 66.41%] [G loss: 2.180583]\n",
      "epoch:6 step:5877 [D loss: 0.603024, acc: 69.53%] [G loss: 2.356696]\n",
      "epoch:6 step:5878 [D loss: 0.667872, acc: 57.81%] [G loss: 2.236281]\n",
      "epoch:6 step:5879 [D loss: 0.618893, acc: 65.62%] [G loss: 2.228994]\n",
      "epoch:6 step:5880 [D loss: 0.589105, acc: 65.62%] [G loss: 2.288638]\n",
      "epoch:6 step:5881 [D loss: 0.570047, acc: 70.31%] [G loss: 2.476517]\n",
      "epoch:6 step:5882 [D loss: 0.632841, acc: 61.72%] [G loss: 2.310679]\n",
      "epoch:6 step:5883 [D loss: 0.567074, acc: 71.09%] [G loss: 2.546975]\n",
      "epoch:6 step:5884 [D loss: 0.577664, acc: 74.22%] [G loss: 2.512958]\n",
      "epoch:6 step:5885 [D loss: 0.630718, acc: 66.41%] [G loss: 2.495555]\n",
      "epoch:6 step:5886 [D loss: 0.489853, acc: 79.69%] [G loss: 2.587866]\n",
      "epoch:6 step:5887 [D loss: 0.638927, acc: 73.44%] [G loss: 2.430839]\n",
      "epoch:6 step:5888 [D loss: 0.569607, acc: 68.75%] [G loss: 2.447164]\n",
      "epoch:6 step:5889 [D loss: 0.600728, acc: 70.31%] [G loss: 2.610409]\n",
      "epoch:6 step:5890 [D loss: 0.632229, acc: 71.09%] [G loss: 2.618260]\n",
      "epoch:6 step:5891 [D loss: 0.598506, acc: 67.19%] [G loss: 2.383522]\n",
      "epoch:6 step:5892 [D loss: 0.659670, acc: 64.84%] [G loss: 2.310826]\n",
      "epoch:6 step:5893 [D loss: 0.620923, acc: 70.31%] [G loss: 2.599804]\n",
      "epoch:6 step:5894 [D loss: 0.668618, acc: 61.72%] [G loss: 2.390550]\n",
      "epoch:6 step:5895 [D loss: 0.582092, acc: 67.19%] [G loss: 2.429894]\n",
      "epoch:6 step:5896 [D loss: 0.574270, acc: 69.53%] [G loss: 2.528013]\n",
      "epoch:6 step:5897 [D loss: 0.592793, acc: 70.31%] [G loss: 2.212186]\n",
      "epoch:6 step:5898 [D loss: 0.618739, acc: 66.41%] [G loss: 2.305297]\n",
      "epoch:6 step:5899 [D loss: 0.660747, acc: 65.62%] [G loss: 2.358964]\n",
      "epoch:6 step:5900 [D loss: 0.662243, acc: 62.50%] [G loss: 2.326747]\n",
      "epoch:6 step:5901 [D loss: 0.650377, acc: 63.28%] [G loss: 2.451568]\n",
      "epoch:6 step:5902 [D loss: 0.593032, acc: 66.41%] [G loss: 2.479338]\n",
      "epoch:6 step:5903 [D loss: 0.617064, acc: 67.97%] [G loss: 2.117931]\n",
      "epoch:6 step:5904 [D loss: 0.632938, acc: 64.06%] [G loss: 2.362427]\n",
      "epoch:6 step:5905 [D loss: 0.584516, acc: 71.88%] [G loss: 2.722905]\n",
      "epoch:6 step:5906 [D loss: 0.523294, acc: 76.56%] [G loss: 2.671125]\n",
      "epoch:6 step:5907 [D loss: 0.602042, acc: 68.75%] [G loss: 2.394799]\n",
      "epoch:6 step:5908 [D loss: 0.526240, acc: 72.66%] [G loss: 2.723499]\n",
      "epoch:6 step:5909 [D loss: 0.550470, acc: 70.31%] [G loss: 2.237531]\n",
      "epoch:6 step:5910 [D loss: 0.667411, acc: 58.59%] [G loss: 2.417424]\n",
      "epoch:6 step:5911 [D loss: 0.574617, acc: 70.31%] [G loss: 2.468386]\n",
      "epoch:6 step:5912 [D loss: 0.555134, acc: 71.88%] [G loss: 2.910639]\n",
      "epoch:6 step:5913 [D loss: 0.646723, acc: 60.94%] [G loss: 2.380448]\n",
      "epoch:6 step:5914 [D loss: 0.657993, acc: 59.38%] [G loss: 2.351646]\n",
      "epoch:6 step:5915 [D loss: 0.650303, acc: 60.94%] [G loss: 2.227314]\n",
      "epoch:6 step:5916 [D loss: 0.561658, acc: 73.44%] [G loss: 2.375216]\n",
      "epoch:6 step:5917 [D loss: 0.552007, acc: 71.09%] [G loss: 2.357563]\n",
      "epoch:6 step:5918 [D loss: 0.590154, acc: 64.84%] [G loss: 2.556809]\n",
      "epoch:6 step:5919 [D loss: 0.642175, acc: 62.50%] [G loss: 2.268503]\n",
      "epoch:6 step:5920 [D loss: 0.585249, acc: 68.75%] [G loss: 2.449733]\n",
      "epoch:6 step:5921 [D loss: 0.595117, acc: 66.41%] [G loss: 2.397100]\n",
      "epoch:6 step:5922 [D loss: 0.689309, acc: 61.72%] [G loss: 2.774824]\n",
      "epoch:6 step:5923 [D loss: 0.654531, acc: 62.50%] [G loss: 2.173214]\n",
      "epoch:6 step:5924 [D loss: 0.594937, acc: 69.53%] [G loss: 2.452602]\n",
      "epoch:6 step:5925 [D loss: 0.609398, acc: 67.97%] [G loss: 2.338871]\n",
      "epoch:6 step:5926 [D loss: 0.617197, acc: 69.53%] [G loss: 2.255734]\n",
      "epoch:6 step:5927 [D loss: 0.551036, acc: 74.22%] [G loss: 2.357548]\n",
      "epoch:6 step:5928 [D loss: 0.548437, acc: 72.66%] [G loss: 2.358023]\n",
      "epoch:6 step:5929 [D loss: 0.636798, acc: 62.50%] [G loss: 2.357208]\n",
      "epoch:6 step:5930 [D loss: 0.605971, acc: 67.19%] [G loss: 2.320816]\n",
      "epoch:6 step:5931 [D loss: 0.602178, acc: 71.09%] [G loss: 2.498870]\n",
      "epoch:6 step:5932 [D loss: 0.571743, acc: 69.53%] [G loss: 2.254470]\n",
      "epoch:6 step:5933 [D loss: 0.573388, acc: 72.66%] [G loss: 2.365755]\n",
      "epoch:6 step:5934 [D loss: 0.518422, acc: 75.78%] [G loss: 2.731241]\n",
      "epoch:6 step:5935 [D loss: 0.567471, acc: 67.97%] [G loss: 2.860146]\n",
      "epoch:6 step:5936 [D loss: 0.524095, acc: 78.12%] [G loss: 2.960949]\n",
      "epoch:6 step:5937 [D loss: 0.628992, acc: 69.53%] [G loss: 2.771284]\n",
      "epoch:6 step:5938 [D loss: 0.692996, acc: 61.72%] [G loss: 2.189312]\n",
      "epoch:6 step:5939 [D loss: 0.635819, acc: 61.72%] [G loss: 2.298451]\n",
      "epoch:6 step:5940 [D loss: 0.558856, acc: 65.62%] [G loss: 2.513430]\n",
      "epoch:6 step:5941 [D loss: 0.550644, acc: 75.78%] [G loss: 2.365915]\n",
      "epoch:6 step:5942 [D loss: 0.542989, acc: 75.00%] [G loss: 2.546067]\n",
      "epoch:6 step:5943 [D loss: 0.568231, acc: 71.88%] [G loss: 2.559309]\n",
      "epoch:6 step:5944 [D loss: 0.518627, acc: 75.00%] [G loss: 2.359673]\n",
      "epoch:6 step:5945 [D loss: 0.731139, acc: 56.25%] [G loss: 2.414460]\n",
      "epoch:6 step:5946 [D loss: 0.688672, acc: 57.81%] [G loss: 2.233075]\n",
      "epoch:6 step:5947 [D loss: 0.699222, acc: 61.72%] [G loss: 2.302351]\n",
      "epoch:6 step:5948 [D loss: 0.615233, acc: 66.41%] [G loss: 2.107864]\n",
      "epoch:6 step:5949 [D loss: 0.590554, acc: 71.88%] [G loss: 2.304859]\n",
      "epoch:6 step:5950 [D loss: 0.707414, acc: 60.16%] [G loss: 2.236690]\n",
      "epoch:6 step:5951 [D loss: 0.582974, acc: 67.19%] [G loss: 2.416892]\n",
      "epoch:6 step:5952 [D loss: 0.617365, acc: 67.19%] [G loss: 2.596969]\n",
      "epoch:6 step:5953 [D loss: 0.603877, acc: 71.09%] [G loss: 2.502536]\n",
      "epoch:6 step:5954 [D loss: 0.607408, acc: 68.75%] [G loss: 2.554648]\n",
      "epoch:6 step:5955 [D loss: 0.582278, acc: 67.97%] [G loss: 2.801346]\n",
      "epoch:6 step:5956 [D loss: 0.672009, acc: 61.72%] [G loss: 2.694956]\n",
      "epoch:6 step:5957 [D loss: 0.549107, acc: 71.09%] [G loss: 2.763381]\n",
      "epoch:6 step:5958 [D loss: 0.542327, acc: 75.00%] [G loss: 2.559639]\n",
      "epoch:6 step:5959 [D loss: 0.590764, acc: 71.09%] [G loss: 2.451930]\n",
      "epoch:6 step:5960 [D loss: 0.568810, acc: 71.09%] [G loss: 2.576877]\n",
      "epoch:6 step:5961 [D loss: 0.607902, acc: 67.97%] [G loss: 2.332354]\n",
      "epoch:6 step:5962 [D loss: 0.612528, acc: 70.31%] [G loss: 2.408085]\n",
      "epoch:6 step:5963 [D loss: 0.726699, acc: 60.16%] [G loss: 2.250053]\n",
      "epoch:6 step:5964 [D loss: 0.689350, acc: 53.12%] [G loss: 2.186574]\n",
      "epoch:6 step:5965 [D loss: 0.597985, acc: 65.62%] [G loss: 2.399790]\n",
      "epoch:6 step:5966 [D loss: 0.609776, acc: 67.97%] [G loss: 2.416358]\n",
      "epoch:6 step:5967 [D loss: 0.491195, acc: 75.00%] [G loss: 2.638596]\n",
      "epoch:6 step:5968 [D loss: 0.526591, acc: 74.22%] [G loss: 3.071536]\n",
      "epoch:6 step:5969 [D loss: 0.509707, acc: 77.34%] [G loss: 3.076385]\n",
      "epoch:6 step:5970 [D loss: 0.655167, acc: 64.06%] [G loss: 2.297257]\n",
      "epoch:6 step:5971 [D loss: 0.772003, acc: 49.22%] [G loss: 1.987141]\n",
      "epoch:6 step:5972 [D loss: 0.618017, acc: 67.19%] [G loss: 2.490920]\n",
      "epoch:6 step:5973 [D loss: 0.514296, acc: 75.00%] [G loss: 2.345379]\n",
      "epoch:6 step:5974 [D loss: 0.662102, acc: 62.50%] [G loss: 2.375645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5975 [D loss: 0.638263, acc: 64.84%] [G loss: 2.314407]\n",
      "epoch:6 step:5976 [D loss: 0.583634, acc: 68.75%] [G loss: 2.143324]\n",
      "epoch:6 step:5977 [D loss: 0.718767, acc: 56.25%] [G loss: 2.162573]\n",
      "epoch:6 step:5978 [D loss: 0.609142, acc: 70.31%] [G loss: 2.098857]\n",
      "epoch:6 step:5979 [D loss: 0.564962, acc: 74.22%] [G loss: 2.404643]\n",
      "epoch:6 step:5980 [D loss: 0.604641, acc: 65.62%] [G loss: 2.596124]\n",
      "epoch:6 step:5981 [D loss: 0.533323, acc: 72.66%] [G loss: 2.708731]\n",
      "epoch:6 step:5982 [D loss: 0.561735, acc: 69.53%] [G loss: 2.467216]\n",
      "epoch:6 step:5983 [D loss: 0.652180, acc: 67.97%] [G loss: 2.173874]\n",
      "epoch:6 step:5984 [D loss: 0.581110, acc: 67.19%] [G loss: 2.481641]\n",
      "epoch:6 step:5985 [D loss: 0.615081, acc: 67.19%] [G loss: 2.319243]\n",
      "epoch:6 step:5986 [D loss: 0.559782, acc: 77.34%] [G loss: 2.421745]\n",
      "epoch:6 step:5987 [D loss: 0.585253, acc: 70.31%] [G loss: 2.346317]\n",
      "epoch:6 step:5988 [D loss: 0.577180, acc: 71.09%] [G loss: 2.504038]\n",
      "epoch:6 step:5989 [D loss: 0.611068, acc: 67.97%] [G loss: 2.673285]\n",
      "epoch:6 step:5990 [D loss: 0.528426, acc: 70.31%] [G loss: 2.399879]\n",
      "epoch:6 step:5991 [D loss: 0.574110, acc: 71.09%] [G loss: 2.579009]\n",
      "epoch:6 step:5992 [D loss: 0.499251, acc: 72.66%] [G loss: 2.485421]\n",
      "epoch:6 step:5993 [D loss: 0.592930, acc: 68.75%] [G loss: 2.758085]\n",
      "epoch:6 step:5994 [D loss: 0.597908, acc: 65.62%] [G loss: 2.761104]\n",
      "epoch:6 step:5995 [D loss: 0.695484, acc: 60.16%] [G loss: 2.220252]\n",
      "epoch:6 step:5996 [D loss: 0.549963, acc: 75.00%] [G loss: 2.646460]\n",
      "epoch:6 step:5997 [D loss: 0.665300, acc: 61.72%] [G loss: 2.220316]\n",
      "epoch:6 step:5998 [D loss: 0.705145, acc: 64.06%] [G loss: 2.139631]\n",
      "epoch:6 step:5999 [D loss: 0.726957, acc: 59.38%] [G loss: 2.065548]\n",
      "epoch:6 step:6000 [D loss: 0.715539, acc: 63.28%] [G loss: 2.241950]\n",
      "##############\n",
      "[2.63397491 1.63367136 6.63283603 5.05933657 4.08421952 6.11207125\n",
      " 4.95986327 5.08809674 5.22919961 3.60148368]\n",
      "##########\n",
      "epoch:6 step:6001 [D loss: 0.604947, acc: 67.97%] [G loss: 2.178570]\n",
      "epoch:6 step:6002 [D loss: 0.565934, acc: 73.44%] [G loss: 2.205214]\n",
      "epoch:6 step:6003 [D loss: 0.573709, acc: 71.09%] [G loss: 2.359252]\n",
      "epoch:6 step:6004 [D loss: 0.658834, acc: 58.59%] [G loss: 2.310015]\n",
      "epoch:6 step:6005 [D loss: 0.600428, acc: 68.75%] [G loss: 2.081463]\n",
      "epoch:6 step:6006 [D loss: 0.602193, acc: 64.84%] [G loss: 2.286697]\n",
      "epoch:6 step:6007 [D loss: 0.639465, acc: 64.06%] [G loss: 2.364031]\n",
      "epoch:6 step:6008 [D loss: 0.643406, acc: 64.06%] [G loss: 2.373874]\n",
      "epoch:6 step:6009 [D loss: 0.625777, acc: 62.50%] [G loss: 2.186236]\n",
      "epoch:6 step:6010 [D loss: 0.590796, acc: 67.19%] [G loss: 2.497560]\n",
      "epoch:6 step:6011 [D loss: 0.577611, acc: 67.97%] [G loss: 2.198016]\n",
      "epoch:6 step:6012 [D loss: 0.682283, acc: 60.94%] [G loss: 2.434423]\n",
      "epoch:6 step:6013 [D loss: 0.598048, acc: 63.28%] [G loss: 2.279801]\n",
      "epoch:6 step:6014 [D loss: 0.559886, acc: 71.09%] [G loss: 2.323060]\n",
      "epoch:6 step:6015 [D loss: 0.621148, acc: 67.19%] [G loss: 2.419497]\n",
      "epoch:6 step:6016 [D loss: 0.661290, acc: 62.50%] [G loss: 2.326134]\n",
      "epoch:6 step:6017 [D loss: 0.537721, acc: 74.22%] [G loss: 2.556561]\n",
      "epoch:6 step:6018 [D loss: 0.687738, acc: 55.47%] [G loss: 2.201514]\n",
      "epoch:6 step:6019 [D loss: 0.624719, acc: 60.94%] [G loss: 2.324094]\n",
      "epoch:6 step:6020 [D loss: 0.562658, acc: 74.22%] [G loss: 2.498258]\n",
      "epoch:6 step:6021 [D loss: 0.605738, acc: 68.75%] [G loss: 2.522797]\n",
      "epoch:6 step:6022 [D loss: 0.676320, acc: 57.81%] [G loss: 2.375110]\n",
      "epoch:6 step:6023 [D loss: 0.622962, acc: 64.06%] [G loss: 2.186238]\n",
      "epoch:6 step:6024 [D loss: 0.537057, acc: 73.44%] [G loss: 2.622483]\n",
      "epoch:6 step:6025 [D loss: 0.629180, acc: 64.06%] [G loss: 2.448685]\n",
      "epoch:6 step:6026 [D loss: 0.541110, acc: 75.00%] [G loss: 2.607945]\n",
      "epoch:6 step:6027 [D loss: 0.575779, acc: 70.31%] [G loss: 2.697901]\n",
      "epoch:6 step:6028 [D loss: 0.580276, acc: 69.53%] [G loss: 2.650726]\n",
      "epoch:6 step:6029 [D loss: 0.667636, acc: 66.41%] [G loss: 2.230112]\n",
      "epoch:6 step:6030 [D loss: 0.627923, acc: 64.84%] [G loss: 2.198876]\n",
      "epoch:6 step:6031 [D loss: 0.596631, acc: 67.19%] [G loss: 2.456107]\n",
      "epoch:6 step:6032 [D loss: 0.629847, acc: 63.28%] [G loss: 2.102686]\n",
      "epoch:6 step:6033 [D loss: 0.648705, acc: 62.50%] [G loss: 2.497470]\n",
      "epoch:6 step:6034 [D loss: 0.574319, acc: 73.44%] [G loss: 2.224276]\n",
      "epoch:6 step:6035 [D loss: 0.588564, acc: 73.44%] [G loss: 2.417431]\n",
      "epoch:6 step:6036 [D loss: 0.697284, acc: 64.06%] [G loss: 2.207551]\n",
      "epoch:6 step:6037 [D loss: 0.654814, acc: 62.50%] [G loss: 2.310163]\n",
      "epoch:6 step:6038 [D loss: 0.578010, acc: 69.53%] [G loss: 2.443752]\n",
      "epoch:6 step:6039 [D loss: 0.684974, acc: 64.84%] [G loss: 2.331691]\n",
      "epoch:6 step:6040 [D loss: 0.671258, acc: 59.38%] [G loss: 2.404881]\n",
      "epoch:6 step:6041 [D loss: 0.629673, acc: 71.88%] [G loss: 2.246864]\n",
      "epoch:6 step:6042 [D loss: 0.701421, acc: 56.25%] [G loss: 2.121475]\n",
      "epoch:6 step:6043 [D loss: 0.604566, acc: 69.53%] [G loss: 2.203368]\n",
      "epoch:6 step:6044 [D loss: 0.678474, acc: 61.72%] [G loss: 2.223646]\n",
      "epoch:6 step:6045 [D loss: 0.627416, acc: 70.31%] [G loss: 2.430055]\n",
      "epoch:6 step:6046 [D loss: 0.645402, acc: 64.06%] [G loss: 2.301173]\n",
      "epoch:6 step:6047 [D loss: 0.592308, acc: 67.19%] [G loss: 2.577075]\n",
      "epoch:6 step:6048 [D loss: 0.593731, acc: 68.75%] [G loss: 2.535797]\n",
      "epoch:6 step:6049 [D loss: 0.598064, acc: 62.50%] [G loss: 2.464707]\n",
      "epoch:6 step:6050 [D loss: 0.544378, acc: 78.12%] [G loss: 2.895483]\n",
      "epoch:6 step:6051 [D loss: 0.568260, acc: 70.31%] [G loss: 2.577882]\n",
      "epoch:6 step:6052 [D loss: 0.550513, acc: 71.88%] [G loss: 3.119286]\n",
      "epoch:6 step:6053 [D loss: 0.582847, acc: 71.09%] [G loss: 2.431550]\n",
      "epoch:6 step:6054 [D loss: 0.692824, acc: 57.81%] [G loss: 2.357507]\n",
      "epoch:6 step:6055 [D loss: 0.656010, acc: 63.28%] [G loss: 2.239194]\n",
      "epoch:6 step:6056 [D loss: 0.521818, acc: 76.56%] [G loss: 2.524242]\n",
      "epoch:6 step:6057 [D loss: 0.569287, acc: 63.28%] [G loss: 2.582473]\n",
      "epoch:6 step:6058 [D loss: 0.625341, acc: 71.09%] [G loss: 2.291440]\n",
      "epoch:6 step:6059 [D loss: 0.689011, acc: 57.03%] [G loss: 2.021712]\n",
      "epoch:6 step:6060 [D loss: 0.683423, acc: 56.25%] [G loss: 2.122897]\n",
      "epoch:6 step:6061 [D loss: 0.643853, acc: 64.06%] [G loss: 2.211767]\n",
      "epoch:6 step:6062 [D loss: 0.614305, acc: 64.84%] [G loss: 2.336127]\n",
      "epoch:6 step:6063 [D loss: 0.662400, acc: 66.41%] [G loss: 2.369103]\n",
      "epoch:6 step:6064 [D loss: 0.707918, acc: 58.59%] [G loss: 2.204048]\n",
      "epoch:6 step:6065 [D loss: 0.595083, acc: 70.31%] [G loss: 2.083539]\n",
      "epoch:6 step:6066 [D loss: 0.604151, acc: 71.09%] [G loss: 2.274053]\n",
      "epoch:6 step:6067 [D loss: 0.662791, acc: 63.28%] [G loss: 2.318175]\n",
      "epoch:6 step:6068 [D loss: 0.626003, acc: 64.06%] [G loss: 2.182922]\n",
      "epoch:6 step:6069 [D loss: 0.539044, acc: 74.22%] [G loss: 2.394727]\n",
      "epoch:6 step:6070 [D loss: 0.682810, acc: 62.50%] [G loss: 2.187706]\n",
      "epoch:6 step:6071 [D loss: 0.619854, acc: 67.97%] [G loss: 2.459498]\n",
      "epoch:6 step:6072 [D loss: 0.522845, acc: 75.00%] [G loss: 2.503540]\n",
      "epoch:6 step:6073 [D loss: 0.520998, acc: 73.44%] [G loss: 2.697514]\n",
      "epoch:6 step:6074 [D loss: 0.618289, acc: 67.97%] [G loss: 2.728146]\n",
      "epoch:6 step:6075 [D loss: 0.610531, acc: 69.53%] [G loss: 2.318008]\n",
      "epoch:6 step:6076 [D loss: 0.587438, acc: 65.62%] [G loss: 2.286424]\n",
      "epoch:6 step:6077 [D loss: 0.582138, acc: 67.97%] [G loss: 2.249877]\n",
      "epoch:6 step:6078 [D loss: 0.593410, acc: 66.41%] [G loss: 2.281063]\n",
      "epoch:6 step:6079 [D loss: 0.602242, acc: 64.06%] [G loss: 2.467787]\n",
      "epoch:6 step:6080 [D loss: 0.655144, acc: 64.84%] [G loss: 2.320258]\n",
      "epoch:6 step:6081 [D loss: 0.665938, acc: 66.41%] [G loss: 2.077673]\n",
      "epoch:6 step:6082 [D loss: 0.622380, acc: 63.28%] [G loss: 2.051747]\n",
      "epoch:6 step:6083 [D loss: 0.625237, acc: 58.59%] [G loss: 2.179759]\n",
      "epoch:6 step:6084 [D loss: 0.581023, acc: 69.53%] [G loss: 2.344721]\n",
      "epoch:6 step:6085 [D loss: 0.606908, acc: 64.84%] [G loss: 2.226534]\n",
      "epoch:6 step:6086 [D loss: 0.551985, acc: 71.88%] [G loss: 2.376635]\n",
      "epoch:6 step:6087 [D loss: 0.684844, acc: 56.25%] [G loss: 2.015145]\n",
      "epoch:6 step:6088 [D loss: 0.593437, acc: 67.97%] [G loss: 2.397831]\n",
      "epoch:6 step:6089 [D loss: 0.617343, acc: 70.31%] [G loss: 2.323324]\n",
      "epoch:6 step:6090 [D loss: 0.652916, acc: 61.72%] [G loss: 2.515231]\n",
      "epoch:6 step:6091 [D loss: 0.594143, acc: 69.53%] [G loss: 2.357573]\n",
      "epoch:6 step:6092 [D loss: 0.589063, acc: 69.53%] [G loss: 2.652172]\n",
      "epoch:6 step:6093 [D loss: 0.507146, acc: 75.78%] [G loss: 3.055457]\n",
      "epoch:6 step:6094 [D loss: 0.612248, acc: 67.97%] [G loss: 2.791666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6095 [D loss: 0.672956, acc: 60.16%] [G loss: 2.347829]\n",
      "epoch:6 step:6096 [D loss: 0.538771, acc: 73.44%] [G loss: 2.729446]\n",
      "epoch:6 step:6097 [D loss: 0.614373, acc: 66.41%] [G loss: 2.339760]\n",
      "epoch:6 step:6098 [D loss: 0.631277, acc: 63.28%] [G loss: 2.335950]\n",
      "epoch:6 step:6099 [D loss: 0.641084, acc: 64.84%] [G loss: 2.466097]\n",
      "epoch:6 step:6100 [D loss: 0.668024, acc: 59.38%] [G loss: 2.128421]\n",
      "epoch:6 step:6101 [D loss: 0.645195, acc: 67.19%] [G loss: 2.353987]\n",
      "epoch:6 step:6102 [D loss: 0.665505, acc: 61.72%] [G loss: 2.239318]\n",
      "epoch:6 step:6103 [D loss: 0.616516, acc: 65.62%] [G loss: 2.410880]\n",
      "epoch:6 step:6104 [D loss: 0.665239, acc: 65.62%] [G loss: 2.038005]\n",
      "epoch:6 step:6105 [D loss: 0.614410, acc: 69.53%] [G loss: 2.334356]\n",
      "epoch:6 step:6106 [D loss: 0.532326, acc: 75.78%] [G loss: 2.412811]\n",
      "epoch:6 step:6107 [D loss: 0.577564, acc: 67.19%] [G loss: 2.425738]\n",
      "epoch:6 step:6108 [D loss: 0.740563, acc: 54.69%] [G loss: 2.287208]\n",
      "epoch:6 step:6109 [D loss: 0.595812, acc: 69.53%] [G loss: 2.239770]\n",
      "epoch:6 step:6110 [D loss: 0.622961, acc: 67.97%] [G loss: 2.341215]\n",
      "epoch:6 step:6111 [D loss: 0.576241, acc: 69.53%] [G loss: 2.330848]\n",
      "epoch:6 step:6112 [D loss: 0.633124, acc: 64.06%] [G loss: 2.407927]\n",
      "epoch:6 step:6113 [D loss: 0.567092, acc: 70.31%] [G loss: 2.621197]\n",
      "epoch:6 step:6114 [D loss: 0.679601, acc: 60.94%] [G loss: 2.238504]\n",
      "epoch:6 step:6115 [D loss: 0.620900, acc: 68.75%] [G loss: 2.079592]\n",
      "epoch:6 step:6116 [D loss: 0.533499, acc: 72.66%] [G loss: 2.249330]\n",
      "epoch:6 step:6117 [D loss: 0.577361, acc: 71.88%] [G loss: 2.284403]\n",
      "epoch:6 step:6118 [D loss: 0.618622, acc: 65.62%] [G loss: 2.177104]\n",
      "epoch:6 step:6119 [D loss: 0.629311, acc: 64.06%] [G loss: 2.592496]\n",
      "epoch:6 step:6120 [D loss: 0.552485, acc: 71.88%] [G loss: 2.852926]\n",
      "epoch:6 step:6121 [D loss: 0.624521, acc: 66.41%] [G loss: 2.550416]\n",
      "epoch:6 step:6122 [D loss: 0.694363, acc: 60.94%] [G loss: 2.101509]\n",
      "epoch:6 step:6123 [D loss: 0.716588, acc: 58.59%] [G loss: 2.123489]\n",
      "epoch:6 step:6124 [D loss: 0.613551, acc: 66.41%] [G loss: 1.928870]\n",
      "epoch:6 step:6125 [D loss: 0.622437, acc: 64.06%] [G loss: 2.619566]\n",
      "epoch:6 step:6126 [D loss: 0.514697, acc: 77.34%] [G loss: 2.696489]\n",
      "epoch:6 step:6127 [D loss: 0.564125, acc: 70.31%] [G loss: 2.413396]\n",
      "epoch:6 step:6128 [D loss: 0.690078, acc: 57.03%] [G loss: 2.118501]\n",
      "epoch:6 step:6129 [D loss: 0.717312, acc: 60.16%] [G loss: 2.460926]\n",
      "epoch:6 step:6130 [D loss: 0.534657, acc: 78.12%] [G loss: 2.628152]\n",
      "epoch:6 step:6131 [D loss: 0.583208, acc: 70.31%] [G loss: 2.585473]\n",
      "epoch:6 step:6132 [D loss: 0.571701, acc: 71.88%] [G loss: 2.393342]\n",
      "epoch:6 step:6133 [D loss: 0.639581, acc: 60.16%] [G loss: 2.293040]\n",
      "epoch:6 step:6134 [D loss: 0.657599, acc: 60.16%] [G loss: 2.304318]\n",
      "epoch:6 step:6135 [D loss: 0.591838, acc: 65.62%] [G loss: 2.209305]\n",
      "epoch:6 step:6136 [D loss: 0.558264, acc: 71.88%] [G loss: 2.300249]\n",
      "epoch:6 step:6137 [D loss: 0.608840, acc: 66.41%] [G loss: 2.389176]\n",
      "epoch:6 step:6138 [D loss: 0.582354, acc: 67.97%] [G loss: 2.245472]\n",
      "epoch:6 step:6139 [D loss: 0.554901, acc: 71.09%] [G loss: 2.342945]\n",
      "epoch:6 step:6140 [D loss: 0.638345, acc: 62.50%] [G loss: 2.471130]\n",
      "epoch:6 step:6141 [D loss: 0.611308, acc: 67.97%] [G loss: 2.358978]\n",
      "epoch:6 step:6142 [D loss: 0.612755, acc: 66.41%] [G loss: 2.320140]\n",
      "epoch:6 step:6143 [D loss: 0.537944, acc: 70.31%] [G loss: 2.350472]\n",
      "epoch:6 step:6144 [D loss: 0.670403, acc: 63.28%] [G loss: 2.536244]\n",
      "epoch:6 step:6145 [D loss: 0.578507, acc: 70.31%] [G loss: 2.662497]\n",
      "epoch:6 step:6146 [D loss: 0.669874, acc: 58.59%] [G loss: 2.247572]\n",
      "epoch:6 step:6147 [D loss: 0.577266, acc: 66.41%] [G loss: 2.345151]\n",
      "epoch:6 step:6148 [D loss: 0.637619, acc: 67.19%] [G loss: 2.357019]\n",
      "epoch:6 step:6149 [D loss: 0.640678, acc: 63.28%] [G loss: 2.262582]\n",
      "epoch:6 step:6150 [D loss: 0.581781, acc: 70.31%] [G loss: 2.283526]\n",
      "epoch:6 step:6151 [D loss: 0.741399, acc: 59.38%] [G loss: 2.162208]\n",
      "epoch:6 step:6152 [D loss: 0.626326, acc: 67.97%] [G loss: 2.187044]\n",
      "epoch:6 step:6153 [D loss: 0.640889, acc: 67.19%] [G loss: 2.067588]\n",
      "epoch:6 step:6154 [D loss: 0.561959, acc: 74.22%] [G loss: 2.357430]\n",
      "epoch:6 step:6155 [D loss: 0.608669, acc: 67.97%] [G loss: 2.550989]\n",
      "epoch:6 step:6156 [D loss: 0.573891, acc: 64.06%] [G loss: 2.581325]\n",
      "epoch:6 step:6157 [D loss: 0.607920, acc: 70.31%] [G loss: 2.406347]\n",
      "epoch:6 step:6158 [D loss: 0.519218, acc: 81.25%] [G loss: 2.621049]\n",
      "epoch:6 step:6159 [D loss: 0.715494, acc: 59.38%] [G loss: 2.304329]\n",
      "epoch:6 step:6160 [D loss: 0.628600, acc: 62.50%] [G loss: 2.133026]\n",
      "epoch:6 step:6161 [D loss: 0.664860, acc: 64.84%] [G loss: 2.048304]\n",
      "epoch:6 step:6162 [D loss: 0.617920, acc: 62.50%] [G loss: 2.441172]\n",
      "epoch:6 step:6163 [D loss: 0.611082, acc: 64.84%] [G loss: 2.195258]\n",
      "epoch:6 step:6164 [D loss: 0.659051, acc: 61.72%] [G loss: 2.280927]\n",
      "epoch:6 step:6165 [D loss: 0.652427, acc: 60.16%] [G loss: 2.108753]\n",
      "epoch:6 step:6166 [D loss: 0.652761, acc: 63.28%] [G loss: 2.101417]\n",
      "epoch:6 step:6167 [D loss: 0.606049, acc: 71.88%] [G loss: 2.291868]\n",
      "epoch:6 step:6168 [D loss: 0.589561, acc: 69.53%] [G loss: 2.425668]\n",
      "epoch:6 step:6169 [D loss: 0.578246, acc: 68.75%] [G loss: 2.349928]\n",
      "epoch:6 step:6170 [D loss: 0.674119, acc: 60.16%] [G loss: 2.372378]\n",
      "epoch:6 step:6171 [D loss: 0.575569, acc: 65.62%] [G loss: 2.484760]\n",
      "epoch:6 step:6172 [D loss: 0.594000, acc: 70.31%] [G loss: 2.298323]\n",
      "epoch:6 step:6173 [D loss: 0.588868, acc: 68.75%] [G loss: 2.623939]\n",
      "epoch:6 step:6174 [D loss: 0.611598, acc: 68.75%] [G loss: 2.423576]\n",
      "epoch:6 step:6175 [D loss: 0.566755, acc: 68.75%] [G loss: 2.406933]\n",
      "epoch:6 step:6176 [D loss: 0.516517, acc: 76.56%] [G loss: 2.930325]\n",
      "epoch:6 step:6177 [D loss: 0.582449, acc: 67.97%] [G loss: 2.371408]\n",
      "epoch:6 step:6178 [D loss: 0.616706, acc: 64.84%] [G loss: 2.456823]\n",
      "epoch:6 step:6179 [D loss: 0.573625, acc: 71.09%] [G loss: 2.412139]\n",
      "epoch:6 step:6180 [D loss: 0.564708, acc: 70.31%] [G loss: 2.420238]\n",
      "epoch:6 step:6181 [D loss: 0.712251, acc: 60.94%] [G loss: 2.309540]\n",
      "epoch:6 step:6182 [D loss: 0.623475, acc: 64.06%] [G loss: 2.041992]\n",
      "epoch:6 step:6183 [D loss: 0.657780, acc: 64.06%] [G loss: 2.311252]\n",
      "epoch:6 step:6184 [D loss: 0.604461, acc: 63.28%] [G loss: 2.316615]\n",
      "epoch:6 step:6185 [D loss: 0.646585, acc: 60.94%] [G loss: 2.209865]\n",
      "epoch:6 step:6186 [D loss: 0.567607, acc: 69.53%] [G loss: 2.434552]\n",
      "epoch:6 step:6187 [D loss: 0.591376, acc: 70.31%] [G loss: 2.253053]\n",
      "epoch:6 step:6188 [D loss: 0.644678, acc: 62.50%] [G loss: 2.177997]\n",
      "epoch:6 step:6189 [D loss: 0.576785, acc: 66.41%] [G loss: 2.439011]\n",
      "epoch:6 step:6190 [D loss: 0.602902, acc: 64.84%] [G loss: 2.496680]\n",
      "epoch:6 step:6191 [D loss: 0.678167, acc: 62.50%] [G loss: 2.202269]\n",
      "epoch:6 step:6192 [D loss: 0.680621, acc: 56.25%] [G loss: 2.430551]\n",
      "epoch:6 step:6193 [D loss: 0.627517, acc: 71.88%] [G loss: 2.173578]\n",
      "epoch:6 step:6194 [D loss: 0.606638, acc: 60.94%] [G loss: 2.105574]\n",
      "epoch:6 step:6195 [D loss: 0.651144, acc: 62.50%] [G loss: 2.255361]\n",
      "epoch:6 step:6196 [D loss: 0.577852, acc: 69.53%] [G loss: 2.341318]\n",
      "epoch:6 step:6197 [D loss: 0.591681, acc: 69.53%] [G loss: 2.452207]\n",
      "epoch:6 step:6198 [D loss: 0.631077, acc: 64.84%] [G loss: 2.010343]\n",
      "epoch:6 step:6199 [D loss: 0.660190, acc: 64.84%] [G loss: 2.390561]\n",
      "epoch:6 step:6200 [D loss: 0.615870, acc: 69.53%] [G loss: 2.049176]\n",
      "##############\n",
      "[2.582008   1.44547187 6.49819873 5.13284581 4.11940801 5.93252189\n",
      " 4.8683169  4.88404152 5.20545271 3.65827887]\n",
      "##########\n",
      "epoch:6 step:6201 [D loss: 0.681133, acc: 59.38%] [G loss: 2.341737]\n",
      "epoch:6 step:6202 [D loss: 0.641548, acc: 62.50%] [G loss: 2.095927]\n",
      "epoch:6 step:6203 [D loss: 0.598641, acc: 64.06%] [G loss: 2.399166]\n",
      "epoch:6 step:6204 [D loss: 0.625131, acc: 64.06%] [G loss: 2.311799]\n",
      "epoch:6 step:6205 [D loss: 0.641502, acc: 66.41%] [G loss: 2.077381]\n",
      "epoch:6 step:6206 [D loss: 0.636186, acc: 66.41%] [G loss: 2.066452]\n",
      "epoch:6 step:6207 [D loss: 0.579262, acc: 70.31%] [G loss: 2.001763]\n",
      "epoch:6 step:6208 [D loss: 0.618522, acc: 64.06%] [G loss: 2.319473]\n",
      "epoch:6 step:6209 [D loss: 0.661632, acc: 58.59%] [G loss: 2.221045]\n",
      "epoch:6 step:6210 [D loss: 0.623890, acc: 68.75%] [G loss: 2.213565]\n",
      "epoch:6 step:6211 [D loss: 0.543255, acc: 74.22%] [G loss: 2.468250]\n",
      "epoch:6 step:6212 [D loss: 0.598497, acc: 70.31%] [G loss: 2.248641]\n",
      "epoch:6 step:6213 [D loss: 0.693292, acc: 63.28%] [G loss: 2.170052]\n",
      "epoch:6 step:6214 [D loss: 0.605349, acc: 69.53%] [G loss: 2.304245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6215 [D loss: 0.591346, acc: 65.62%] [G loss: 2.163027]\n",
      "epoch:6 step:6216 [D loss: 0.646126, acc: 59.38%] [G loss: 2.147276]\n",
      "epoch:6 step:6217 [D loss: 0.726333, acc: 57.81%] [G loss: 2.281115]\n",
      "epoch:6 step:6218 [D loss: 0.597895, acc: 67.97%] [G loss: 2.202671]\n",
      "epoch:6 step:6219 [D loss: 0.629671, acc: 67.19%] [G loss: 2.260570]\n",
      "epoch:6 step:6220 [D loss: 0.591074, acc: 68.75%] [G loss: 2.305359]\n",
      "epoch:6 step:6221 [D loss: 0.699267, acc: 59.38%] [G loss: 2.184572]\n",
      "epoch:6 step:6222 [D loss: 0.656973, acc: 57.03%] [G loss: 2.243113]\n",
      "epoch:6 step:6223 [D loss: 0.553679, acc: 69.53%] [G loss: 2.422799]\n",
      "epoch:6 step:6224 [D loss: 0.599702, acc: 70.31%] [G loss: 2.438706]\n",
      "epoch:6 step:6225 [D loss: 0.567466, acc: 62.50%] [G loss: 2.446970]\n",
      "epoch:6 step:6226 [D loss: 0.629821, acc: 67.97%] [G loss: 2.034817]\n",
      "epoch:6 step:6227 [D loss: 0.613453, acc: 71.88%] [G loss: 2.548349]\n",
      "epoch:6 step:6228 [D loss: 0.665298, acc: 58.59%] [G loss: 2.224319]\n",
      "epoch:6 step:6229 [D loss: 0.647821, acc: 65.62%] [G loss: 2.116513]\n",
      "epoch:6 step:6230 [D loss: 0.591361, acc: 64.06%] [G loss: 2.302279]\n",
      "epoch:6 step:6231 [D loss: 0.542338, acc: 74.22%] [G loss: 2.308875]\n",
      "epoch:6 step:6232 [D loss: 0.622072, acc: 71.09%] [G loss: 2.350938]\n",
      "epoch:6 step:6233 [D loss: 0.703989, acc: 60.16%] [G loss: 2.125845]\n",
      "epoch:6 step:6234 [D loss: 0.600645, acc: 68.75%] [G loss: 2.227423]\n",
      "epoch:6 step:6235 [D loss: 0.538667, acc: 76.56%] [G loss: 2.529073]\n",
      "epoch:6 step:6236 [D loss: 0.622389, acc: 64.84%] [G loss: 2.108748]\n",
      "epoch:6 step:6237 [D loss: 0.624767, acc: 58.59%] [G loss: 2.252972]\n",
      "epoch:6 step:6238 [D loss: 0.585670, acc: 68.75%] [G loss: 2.168174]\n",
      "epoch:6 step:6239 [D loss: 0.635763, acc: 64.84%] [G loss: 2.228459]\n",
      "epoch:6 step:6240 [D loss: 0.586747, acc: 68.75%] [G loss: 2.185905]\n",
      "epoch:6 step:6241 [D loss: 0.604829, acc: 71.88%] [G loss: 2.257778]\n",
      "epoch:6 step:6242 [D loss: 0.674759, acc: 63.28%] [G loss: 2.462427]\n",
      "epoch:6 step:6243 [D loss: 0.602429, acc: 65.62%] [G loss: 2.210829]\n",
      "epoch:6 step:6244 [D loss: 0.672334, acc: 66.41%] [G loss: 2.216067]\n",
      "epoch:6 step:6245 [D loss: 0.655960, acc: 59.38%] [G loss: 2.258046]\n",
      "epoch:6 step:6246 [D loss: 0.626956, acc: 64.06%] [G loss: 2.249274]\n",
      "epoch:6 step:6247 [D loss: 0.672855, acc: 57.03%] [G loss: 2.121812]\n",
      "epoch:6 step:6248 [D loss: 0.591703, acc: 66.41%] [G loss: 2.436574]\n",
      "epoch:6 step:6249 [D loss: 0.613185, acc: 68.75%] [G loss: 2.216023]\n",
      "epoch:6 step:6250 [D loss: 0.681209, acc: 53.91%] [G loss: 2.128325]\n",
      "epoch:6 step:6251 [D loss: 0.598245, acc: 68.75%] [G loss: 2.346016]\n",
      "epoch:6 step:6252 [D loss: 0.590033, acc: 70.31%] [G loss: 2.295059]\n",
      "epoch:6 step:6253 [D loss: 0.579149, acc: 71.09%] [G loss: 2.404673]\n",
      "epoch:6 step:6254 [D loss: 0.570973, acc: 69.53%] [G loss: 2.282893]\n",
      "epoch:6 step:6255 [D loss: 0.630913, acc: 65.62%] [G loss: 2.134583]\n",
      "epoch:6 step:6256 [D loss: 0.551196, acc: 70.31%] [G loss: 2.460558]\n",
      "epoch:6 step:6257 [D loss: 0.577463, acc: 73.44%] [G loss: 2.317571]\n",
      "epoch:6 step:6258 [D loss: 0.552429, acc: 74.22%] [G loss: 2.516590]\n",
      "epoch:6 step:6259 [D loss: 0.537105, acc: 72.66%] [G loss: 2.393127]\n",
      "epoch:6 step:6260 [D loss: 0.622872, acc: 66.41%] [G loss: 2.449720]\n",
      "epoch:6 step:6261 [D loss: 0.589620, acc: 65.62%] [G loss: 2.495931]\n",
      "epoch:6 step:6262 [D loss: 0.585297, acc: 65.62%] [G loss: 2.222135]\n",
      "epoch:6 step:6263 [D loss: 0.556029, acc: 69.53%] [G loss: 2.796536]\n",
      "epoch:6 step:6264 [D loss: 0.555305, acc: 69.53%] [G loss: 2.598001]\n",
      "epoch:6 step:6265 [D loss: 0.594819, acc: 67.19%] [G loss: 2.255144]\n",
      "epoch:6 step:6266 [D loss: 0.591644, acc: 71.88%] [G loss: 2.448753]\n",
      "epoch:6 step:6267 [D loss: 0.624218, acc: 66.41%] [G loss: 2.348074]\n",
      "epoch:6 step:6268 [D loss: 0.523369, acc: 73.44%] [G loss: 2.529217]\n",
      "epoch:6 step:6269 [D loss: 0.619735, acc: 67.19%] [G loss: 2.589594]\n",
      "epoch:6 step:6270 [D loss: 0.510322, acc: 73.44%] [G loss: 3.092066]\n",
      "epoch:6 step:6271 [D loss: 0.544773, acc: 73.44%] [G loss: 2.574050]\n",
      "epoch:6 step:6272 [D loss: 0.487408, acc: 78.91%] [G loss: 2.829974]\n",
      "epoch:6 step:6273 [D loss: 0.590943, acc: 70.31%] [G loss: 2.537530]\n",
      "epoch:6 step:6274 [D loss: 0.638920, acc: 60.94%] [G loss: 2.173755]\n",
      "epoch:6 step:6275 [D loss: 0.611648, acc: 69.53%] [G loss: 2.296333]\n",
      "epoch:6 step:6276 [D loss: 0.521055, acc: 75.00%] [G loss: 2.536167]\n",
      "epoch:6 step:6277 [D loss: 0.611603, acc: 67.19%] [G loss: 2.370907]\n",
      "epoch:6 step:6278 [D loss: 0.572892, acc: 70.31%] [G loss: 2.239351]\n",
      "epoch:6 step:6279 [D loss: 0.590124, acc: 71.09%] [G loss: 2.491786]\n",
      "epoch:6 step:6280 [D loss: 0.626759, acc: 64.84%] [G loss: 2.103798]\n",
      "epoch:6 step:6281 [D loss: 0.668405, acc: 62.50%] [G loss: 2.526258]\n",
      "epoch:6 step:6282 [D loss: 0.597528, acc: 70.31%] [G loss: 2.301110]\n",
      "epoch:6 step:6283 [D loss: 0.609832, acc: 69.53%] [G loss: 2.464990]\n",
      "epoch:6 step:6284 [D loss: 0.604417, acc: 71.88%] [G loss: 2.333559]\n",
      "epoch:6 step:6285 [D loss: 0.630709, acc: 62.50%] [G loss: 2.499639]\n",
      "epoch:6 step:6286 [D loss: 0.618799, acc: 71.88%] [G loss: 2.459460]\n",
      "epoch:6 step:6287 [D loss: 0.589489, acc: 67.97%] [G loss: 2.447766]\n",
      "epoch:6 step:6288 [D loss: 0.616049, acc: 66.41%] [G loss: 2.253266]\n",
      "epoch:6 step:6289 [D loss: 0.682409, acc: 59.38%] [G loss: 2.300204]\n",
      "epoch:6 step:6290 [D loss: 0.602837, acc: 65.62%] [G loss: 2.407074]\n",
      "epoch:6 step:6291 [D loss: 0.664551, acc: 59.38%] [G loss: 1.879777]\n",
      "epoch:6 step:6292 [D loss: 0.533434, acc: 74.22%] [G loss: 2.320091]\n",
      "epoch:6 step:6293 [D loss: 0.565996, acc: 70.31%] [G loss: 2.292233]\n",
      "epoch:6 step:6294 [D loss: 0.676028, acc: 60.94%] [G loss: 2.140826]\n",
      "epoch:6 step:6295 [D loss: 0.644532, acc: 61.72%] [G loss: 2.218729]\n",
      "epoch:6 step:6296 [D loss: 0.694235, acc: 59.38%] [G loss: 2.341326]\n",
      "epoch:6 step:6297 [D loss: 0.576734, acc: 70.31%] [G loss: 2.280215]\n",
      "epoch:6 step:6298 [D loss: 0.640761, acc: 63.28%] [G loss: 2.198398]\n",
      "epoch:6 step:6299 [D loss: 0.549585, acc: 69.53%] [G loss: 2.319349]\n",
      "epoch:6 step:6300 [D loss: 0.665467, acc: 63.28%] [G loss: 2.076307]\n",
      "epoch:6 step:6301 [D loss: 0.626737, acc: 66.41%] [G loss: 2.414445]\n",
      "epoch:6 step:6302 [D loss: 0.651138, acc: 67.97%] [G loss: 2.172858]\n",
      "epoch:6 step:6303 [D loss: 0.574444, acc: 71.09%] [G loss: 2.328269]\n",
      "epoch:6 step:6304 [D loss: 0.583285, acc: 71.09%] [G loss: 2.247658]\n",
      "epoch:6 step:6305 [D loss: 0.617681, acc: 63.28%] [G loss: 2.275656]\n",
      "epoch:6 step:6306 [D loss: 0.640833, acc: 65.62%] [G loss: 2.267027]\n",
      "epoch:6 step:6307 [D loss: 0.621737, acc: 64.84%] [G loss: 2.431519]\n",
      "epoch:6 step:6308 [D loss: 0.610573, acc: 66.41%] [G loss: 2.105372]\n",
      "epoch:6 step:6309 [D loss: 0.573322, acc: 70.31%] [G loss: 2.364382]\n",
      "epoch:6 step:6310 [D loss: 0.592867, acc: 72.66%] [G loss: 2.228015]\n",
      "epoch:6 step:6311 [D loss: 0.538960, acc: 71.88%] [G loss: 2.230450]\n",
      "epoch:6 step:6312 [D loss: 0.578713, acc: 69.53%] [G loss: 2.353639]\n",
      "epoch:6 step:6313 [D loss: 0.635405, acc: 67.19%] [G loss: 2.410512]\n",
      "epoch:6 step:6314 [D loss: 0.599073, acc: 67.97%] [G loss: 2.536895]\n",
      "epoch:6 step:6315 [D loss: 0.557401, acc: 73.44%] [G loss: 2.842389]\n",
      "epoch:6 step:6316 [D loss: 0.580530, acc: 67.97%] [G loss: 2.742335]\n",
      "epoch:6 step:6317 [D loss: 0.590553, acc: 71.09%] [G loss: 2.538525]\n",
      "epoch:6 step:6318 [D loss: 0.624163, acc: 62.50%] [G loss: 2.297992]\n",
      "epoch:6 step:6319 [D loss: 0.630237, acc: 63.28%] [G loss: 2.270109]\n",
      "epoch:6 step:6320 [D loss: 0.646684, acc: 62.50%] [G loss: 2.290300]\n",
      "epoch:6 step:6321 [D loss: 0.646262, acc: 60.16%] [G loss: 2.368954]\n",
      "epoch:6 step:6322 [D loss: 0.634306, acc: 64.06%] [G loss: 2.303292]\n",
      "epoch:6 step:6323 [D loss: 0.605698, acc: 67.97%] [G loss: 2.420842]\n",
      "epoch:6 step:6324 [D loss: 0.625328, acc: 64.84%] [G loss: 2.220939]\n",
      "epoch:6 step:6325 [D loss: 0.728528, acc: 54.69%] [G loss: 2.260784]\n",
      "epoch:6 step:6326 [D loss: 0.645135, acc: 61.72%] [G loss: 2.266820]\n",
      "epoch:6 step:6327 [D loss: 0.718384, acc: 59.38%] [G loss: 2.447736]\n",
      "epoch:6 step:6328 [D loss: 0.628574, acc: 58.59%] [G loss: 2.299098]\n",
      "epoch:6 step:6329 [D loss: 0.593923, acc: 66.41%] [G loss: 2.794343]\n",
      "epoch:6 step:6330 [D loss: 0.604370, acc: 68.75%] [G loss: 2.699042]\n",
      "epoch:6 step:6331 [D loss: 0.548653, acc: 78.91%] [G loss: 2.616083]\n",
      "epoch:6 step:6332 [D loss: 0.631755, acc: 67.97%] [G loss: 2.176301]\n",
      "epoch:6 step:6333 [D loss: 0.670680, acc: 62.50%] [G loss: 2.184152]\n",
      "epoch:6 step:6334 [D loss: 0.597739, acc: 71.09%] [G loss: 2.383403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6335 [D loss: 0.631816, acc: 65.62%] [G loss: 2.036338]\n",
      "epoch:6 step:6336 [D loss: 0.591902, acc: 70.31%] [G loss: 2.224604]\n",
      "epoch:6 step:6337 [D loss: 0.599235, acc: 69.53%] [G loss: 2.234019]\n",
      "epoch:6 step:6338 [D loss: 0.656601, acc: 63.28%] [G loss: 2.160069]\n",
      "epoch:6 step:6339 [D loss: 0.675199, acc: 54.69%] [G loss: 2.262826]\n",
      "epoch:6 step:6340 [D loss: 0.579803, acc: 69.53%] [G loss: 2.240093]\n",
      "epoch:6 step:6341 [D loss: 0.561070, acc: 72.66%] [G loss: 2.329125]\n",
      "epoch:6 step:6342 [D loss: 0.584876, acc: 71.09%] [G loss: 2.272768]\n",
      "epoch:6 step:6343 [D loss: 0.616256, acc: 63.28%] [G loss: 2.443627]\n",
      "epoch:6 step:6344 [D loss: 0.658957, acc: 64.06%] [G loss: 2.544249]\n",
      "epoch:6 step:6345 [D loss: 0.594910, acc: 69.53%] [G loss: 2.330006]\n",
      "epoch:6 step:6346 [D loss: 0.612775, acc: 64.84%] [G loss: 2.412082]\n",
      "epoch:6 step:6347 [D loss: 0.582571, acc: 68.75%] [G loss: 2.496435]\n",
      "epoch:6 step:6348 [D loss: 0.573531, acc: 68.75%] [G loss: 2.531991]\n",
      "epoch:6 step:6349 [D loss: 0.540646, acc: 71.09%] [G loss: 2.235207]\n",
      "epoch:6 step:6350 [D loss: 0.565414, acc: 71.88%] [G loss: 2.468912]\n",
      "epoch:6 step:6351 [D loss: 0.552611, acc: 67.97%] [G loss: 2.295574]\n",
      "epoch:6 step:6352 [D loss: 0.617951, acc: 64.06%] [G loss: 2.228112]\n",
      "epoch:6 step:6353 [D loss: 0.609196, acc: 68.75%] [G loss: 2.149143]\n",
      "epoch:6 step:6354 [D loss: 0.593050, acc: 69.53%] [G loss: 2.285687]\n",
      "epoch:6 step:6355 [D loss: 0.577594, acc: 69.53%] [G loss: 2.460211]\n",
      "epoch:6 step:6356 [D loss: 0.565300, acc: 73.44%] [G loss: 2.348176]\n",
      "epoch:6 step:6357 [D loss: 0.637881, acc: 64.06%] [G loss: 2.171328]\n",
      "epoch:6 step:6358 [D loss: 0.503950, acc: 78.12%] [G loss: 2.305776]\n",
      "epoch:6 step:6359 [D loss: 0.604256, acc: 64.06%] [G loss: 2.048130]\n",
      "epoch:6 step:6360 [D loss: 0.630737, acc: 63.28%] [G loss: 2.231352]\n",
      "epoch:6 step:6361 [D loss: 0.687036, acc: 61.72%] [G loss: 2.074655]\n",
      "epoch:6 step:6362 [D loss: 0.639066, acc: 69.53%] [G loss: 2.461758]\n",
      "epoch:6 step:6363 [D loss: 0.661421, acc: 60.94%] [G loss: 2.091956]\n",
      "epoch:6 step:6364 [D loss: 0.640769, acc: 62.50%] [G loss: 2.340891]\n",
      "epoch:6 step:6365 [D loss: 0.660133, acc: 63.28%] [G loss: 2.126141]\n",
      "epoch:6 step:6366 [D loss: 0.701845, acc: 57.81%] [G loss: 2.052324]\n",
      "epoch:6 step:6367 [D loss: 0.715081, acc: 50.78%] [G loss: 2.150567]\n",
      "epoch:6 step:6368 [D loss: 0.622881, acc: 67.19%] [G loss: 2.319927]\n",
      "epoch:6 step:6369 [D loss: 0.558467, acc: 76.56%] [G loss: 2.098135]\n",
      "epoch:6 step:6370 [D loss: 0.662348, acc: 61.72%] [G loss: 2.156328]\n",
      "epoch:6 step:6371 [D loss: 0.647714, acc: 65.62%] [G loss: 2.134661]\n",
      "epoch:6 step:6372 [D loss: 0.578933, acc: 67.97%] [G loss: 2.176651]\n",
      "epoch:6 step:6373 [D loss: 0.597554, acc: 64.84%] [G loss: 2.006487]\n",
      "epoch:6 step:6374 [D loss: 0.611003, acc: 66.41%] [G loss: 2.167175]\n",
      "epoch:6 step:6375 [D loss: 0.595539, acc: 69.53%] [G loss: 2.012736]\n",
      "epoch:6 step:6376 [D loss: 0.572525, acc: 67.19%] [G loss: 2.267852]\n",
      "epoch:6 step:6377 [D loss: 0.612116, acc: 61.72%] [G loss: 2.328286]\n",
      "epoch:6 step:6378 [D loss: 0.598469, acc: 67.97%] [G loss: 2.226911]\n",
      "epoch:6 step:6379 [D loss: 0.632640, acc: 64.84%] [G loss: 2.346161]\n",
      "epoch:6 step:6380 [D loss: 0.624827, acc: 62.50%] [G loss: 2.193172]\n",
      "epoch:6 step:6381 [D loss: 0.627675, acc: 61.72%] [G loss: 2.024824]\n",
      "epoch:6 step:6382 [D loss: 0.586962, acc: 68.75%] [G loss: 2.147462]\n",
      "epoch:6 step:6383 [D loss: 0.635577, acc: 64.84%] [G loss: 2.408142]\n",
      "epoch:6 step:6384 [D loss: 0.573119, acc: 71.09%] [G loss: 2.080909]\n",
      "epoch:6 step:6385 [D loss: 0.656166, acc: 64.06%] [G loss: 2.123983]\n",
      "epoch:6 step:6386 [D loss: 0.658195, acc: 60.94%] [G loss: 2.280824]\n",
      "epoch:6 step:6387 [D loss: 0.650928, acc: 63.28%] [G loss: 2.267912]\n",
      "epoch:6 step:6388 [D loss: 0.636025, acc: 67.19%] [G loss: 2.136532]\n",
      "epoch:6 step:6389 [D loss: 0.618511, acc: 69.53%] [G loss: 2.279124]\n",
      "epoch:6 step:6390 [D loss: 0.602436, acc: 64.06%] [G loss: 2.247072]\n",
      "epoch:6 step:6391 [D loss: 0.553369, acc: 67.97%] [G loss: 2.590768]\n",
      "epoch:6 step:6392 [D loss: 0.621282, acc: 66.41%] [G loss: 2.122269]\n",
      "epoch:6 step:6393 [D loss: 0.588948, acc: 71.09%] [G loss: 2.181363]\n",
      "epoch:6 step:6394 [D loss: 0.619114, acc: 65.62%] [G loss: 2.545599]\n",
      "epoch:6 step:6395 [D loss: 0.632911, acc: 64.84%] [G loss: 2.205139]\n",
      "epoch:6 step:6396 [D loss: 0.591535, acc: 66.41%] [G loss: 2.550570]\n",
      "epoch:6 step:6397 [D loss: 0.552834, acc: 71.88%] [G loss: 2.656690]\n",
      "epoch:6 step:6398 [D loss: 0.573673, acc: 65.62%] [G loss: 2.481091]\n",
      "epoch:6 step:6399 [D loss: 0.544528, acc: 73.44%] [G loss: 2.655452]\n",
      "epoch:6 step:6400 [D loss: 0.588966, acc: 71.09%] [G loss: 2.237054]\n",
      "##############\n",
      "[2.71914924 1.59114417 6.41475244 4.92252123 3.85220778 5.8584766\n",
      " 4.74963127 4.89838961 4.94738329 3.62657585]\n",
      "##########\n",
      "epoch:6 step:6401 [D loss: 0.579517, acc: 67.97%] [G loss: 2.523733]\n",
      "epoch:6 step:6402 [D loss: 0.614330, acc: 73.44%] [G loss: 2.665750]\n",
      "epoch:6 step:6403 [D loss: 0.562398, acc: 67.97%] [G loss: 2.627279]\n",
      "epoch:6 step:6404 [D loss: 0.481891, acc: 83.59%] [G loss: 2.752766]\n",
      "epoch:6 step:6405 [D loss: 0.569035, acc: 70.31%] [G loss: 2.614654]\n",
      "epoch:6 step:6406 [D loss: 0.717877, acc: 56.25%] [G loss: 2.122682]\n",
      "epoch:6 step:6407 [D loss: 0.582953, acc: 70.31%] [G loss: 2.378178]\n",
      "epoch:6 step:6408 [D loss: 0.574408, acc: 70.31%] [G loss: 2.461253]\n",
      "epoch:6 step:6409 [D loss: 0.614438, acc: 63.28%] [G loss: 2.476024]\n",
      "epoch:6 step:6410 [D loss: 0.658954, acc: 60.94%] [G loss: 2.153767]\n",
      "epoch:6 step:6411 [D loss: 0.657303, acc: 63.28%] [G loss: 2.151149]\n",
      "epoch:6 step:6412 [D loss: 0.575006, acc: 74.22%] [G loss: 2.501869]\n",
      "epoch:6 step:6413 [D loss: 0.571996, acc: 70.31%] [G loss: 2.518406]\n",
      "epoch:6 step:6414 [D loss: 0.521808, acc: 72.66%] [G loss: 2.680231]\n",
      "epoch:6 step:6415 [D loss: 0.677504, acc: 60.16%] [G loss: 2.269740]\n",
      "epoch:6 step:6416 [D loss: 0.660378, acc: 60.16%] [G loss: 2.243741]\n",
      "epoch:6 step:6417 [D loss: 0.613089, acc: 69.53%] [G loss: 2.347442]\n",
      "epoch:6 step:6418 [D loss: 0.621257, acc: 65.62%] [G loss: 2.312612]\n",
      "epoch:6 step:6419 [D loss: 0.576953, acc: 68.75%] [G loss: 2.125949]\n",
      "epoch:6 step:6420 [D loss: 0.670361, acc: 60.16%] [G loss: 2.235767]\n",
      "epoch:6 step:6421 [D loss: 0.669081, acc: 62.50%] [G loss: 2.128325]\n",
      "epoch:6 step:6422 [D loss: 0.623962, acc: 65.62%] [G loss: 2.298874]\n",
      "epoch:6 step:6423 [D loss: 0.627727, acc: 64.84%] [G loss: 2.326490]\n",
      "epoch:6 step:6424 [D loss: 0.596952, acc: 67.19%] [G loss: 2.193274]\n",
      "epoch:6 step:6425 [D loss: 0.586697, acc: 74.22%] [G loss: 2.287002]\n",
      "epoch:6 step:6426 [D loss: 0.581539, acc: 66.41%] [G loss: 2.573385]\n",
      "epoch:6 step:6427 [D loss: 0.592753, acc: 67.97%] [G loss: 2.419299]\n",
      "epoch:6 step:6428 [D loss: 0.544394, acc: 71.88%] [G loss: 2.441434]\n",
      "epoch:6 step:6429 [D loss: 0.604315, acc: 65.62%] [G loss: 2.452352]\n",
      "epoch:6 step:6430 [D loss: 0.635657, acc: 63.28%] [G loss: 2.540997]\n",
      "epoch:6 step:6431 [D loss: 0.568034, acc: 74.22%] [G loss: 2.607908]\n",
      "epoch:6 step:6432 [D loss: 0.612873, acc: 65.62%] [G loss: 2.302477]\n",
      "epoch:6 step:6433 [D loss: 0.697064, acc: 61.72%] [G loss: 2.207891]\n",
      "epoch:6 step:6434 [D loss: 0.628730, acc: 64.06%] [G loss: 2.124408]\n",
      "epoch:6 step:6435 [D loss: 0.647187, acc: 60.94%] [G loss: 2.533852]\n",
      "epoch:6 step:6436 [D loss: 0.623685, acc: 68.75%] [G loss: 2.364238]\n",
      "epoch:6 step:6437 [D loss: 0.625646, acc: 69.53%] [G loss: 2.402636]\n",
      "epoch:6 step:6438 [D loss: 0.582005, acc: 67.19%] [G loss: 2.807280]\n",
      "epoch:6 step:6439 [D loss: 0.628120, acc: 65.62%] [G loss: 2.331797]\n",
      "epoch:6 step:6440 [D loss: 0.644041, acc: 64.06%] [G loss: 2.098795]\n",
      "epoch:6 step:6441 [D loss: 0.658502, acc: 61.72%] [G loss: 2.407617]\n",
      "epoch:6 step:6442 [D loss: 0.697025, acc: 57.81%] [G loss: 2.115343]\n",
      "epoch:6 step:6443 [D loss: 0.617032, acc: 62.50%] [G loss: 2.303030]\n",
      "epoch:6 step:6444 [D loss: 0.587763, acc: 76.56%] [G loss: 2.450447]\n",
      "epoch:6 step:6445 [D loss: 0.610623, acc: 62.50%] [G loss: 2.379014]\n",
      "epoch:6 step:6446 [D loss: 0.695794, acc: 62.50%] [G loss: 2.298198]\n",
      "epoch:6 step:6447 [D loss: 0.624332, acc: 68.75%] [G loss: 2.152535]\n",
      "epoch:6 step:6448 [D loss: 0.600071, acc: 70.31%] [G loss: 2.001605]\n",
      "epoch:6 step:6449 [D loss: 0.645182, acc: 56.25%] [G loss: 2.170378]\n",
      "epoch:6 step:6450 [D loss: 0.715457, acc: 55.47%] [G loss: 2.033341]\n",
      "epoch:6 step:6451 [D loss: 0.586609, acc: 75.00%] [G loss: 2.244375]\n",
      "epoch:6 step:6452 [D loss: 0.618403, acc: 64.06%] [G loss: 2.192166]\n",
      "epoch:6 step:6453 [D loss: 0.577189, acc: 73.44%] [G loss: 2.330461]\n",
      "epoch:6 step:6454 [D loss: 0.597699, acc: 66.41%] [G loss: 2.143036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6455 [D loss: 0.548752, acc: 71.88%] [G loss: 2.488267]\n",
      "epoch:6 step:6456 [D loss: 0.605055, acc: 62.50%] [G loss: 2.582281]\n",
      "epoch:6 step:6457 [D loss: 0.602407, acc: 70.31%] [G loss: 2.277490]\n",
      "epoch:6 step:6458 [D loss: 0.733117, acc: 53.12%] [G loss: 2.275733]\n",
      "epoch:6 step:6459 [D loss: 0.626195, acc: 66.41%] [G loss: 2.205197]\n",
      "epoch:6 step:6460 [D loss: 0.609809, acc: 67.19%] [G loss: 2.429768]\n",
      "epoch:6 step:6461 [D loss: 0.550261, acc: 70.31%] [G loss: 2.389375]\n",
      "epoch:6 step:6462 [D loss: 0.611807, acc: 67.97%] [G loss: 2.436565]\n",
      "epoch:6 step:6463 [D loss: 0.565396, acc: 71.88%] [G loss: 2.391957]\n",
      "epoch:6 step:6464 [D loss: 0.562606, acc: 66.41%] [G loss: 2.301736]\n",
      "epoch:6 step:6465 [D loss: 0.633831, acc: 71.09%] [G loss: 2.352163]\n",
      "epoch:6 step:6466 [D loss: 0.654801, acc: 60.94%] [G loss: 2.343974]\n",
      "epoch:6 step:6467 [D loss: 0.575420, acc: 67.97%] [G loss: 2.438006]\n",
      "epoch:6 step:6468 [D loss: 0.593879, acc: 64.84%] [G loss: 2.356978]\n",
      "epoch:6 step:6469 [D loss: 0.653454, acc: 60.94%] [G loss: 2.385610]\n",
      "epoch:6 step:6470 [D loss: 0.574764, acc: 75.00%] [G loss: 2.568190]\n",
      "epoch:6 step:6471 [D loss: 0.633707, acc: 59.38%] [G loss: 2.206960]\n",
      "epoch:6 step:6472 [D loss: 0.642922, acc: 69.53%] [G loss: 2.498372]\n",
      "epoch:6 step:6473 [D loss: 0.647184, acc: 67.97%] [G loss: 2.134663]\n",
      "epoch:6 step:6474 [D loss: 0.609618, acc: 64.06%] [G loss: 2.162879]\n",
      "epoch:6 step:6475 [D loss: 0.571279, acc: 71.09%] [G loss: 2.274003]\n",
      "epoch:6 step:6476 [D loss: 0.654570, acc: 64.06%] [G loss: 2.385180]\n",
      "epoch:6 step:6477 [D loss: 0.676540, acc: 64.06%] [G loss: 2.120771]\n",
      "epoch:6 step:6478 [D loss: 0.704206, acc: 60.16%] [G loss: 2.066659]\n",
      "epoch:6 step:6479 [D loss: 0.556380, acc: 75.00%] [G loss: 2.126145]\n",
      "epoch:6 step:6480 [D loss: 0.713391, acc: 61.72%] [G loss: 1.988679]\n",
      "epoch:6 step:6481 [D loss: 0.625956, acc: 67.97%] [G loss: 2.257532]\n",
      "epoch:6 step:6482 [D loss: 0.553558, acc: 76.56%] [G loss: 2.179982]\n",
      "epoch:6 step:6483 [D loss: 0.621262, acc: 62.50%] [G loss: 2.138329]\n",
      "epoch:6 step:6484 [D loss: 0.603277, acc: 68.75%] [G loss: 2.535846]\n",
      "epoch:6 step:6485 [D loss: 0.554895, acc: 70.31%] [G loss: 2.291330]\n",
      "epoch:6 step:6486 [D loss: 0.585605, acc: 74.22%] [G loss: 2.320869]\n",
      "epoch:6 step:6487 [D loss: 0.590590, acc: 67.97%] [G loss: 2.368858]\n",
      "epoch:6 step:6488 [D loss: 0.533270, acc: 71.88%] [G loss: 2.364504]\n",
      "epoch:6 step:6489 [D loss: 0.704640, acc: 60.16%] [G loss: 2.174684]\n",
      "epoch:6 step:6490 [D loss: 0.620083, acc: 69.53%] [G loss: 2.302198]\n",
      "epoch:6 step:6491 [D loss: 0.587310, acc: 66.41%] [G loss: 2.098549]\n",
      "epoch:6 step:6492 [D loss: 0.664057, acc: 66.41%] [G loss: 2.355245]\n",
      "epoch:6 step:6493 [D loss: 0.570750, acc: 69.53%] [G loss: 2.280958]\n",
      "epoch:6 step:6494 [D loss: 0.587282, acc: 67.19%] [G loss: 2.072211]\n",
      "epoch:6 step:6495 [D loss: 0.657829, acc: 62.50%] [G loss: 2.029960]\n",
      "epoch:6 step:6496 [D loss: 0.717909, acc: 63.28%] [G loss: 2.066614]\n",
      "epoch:6 step:6497 [D loss: 0.594142, acc: 69.53%] [G loss: 2.249044]\n",
      "epoch:6 step:6498 [D loss: 0.547886, acc: 71.09%] [G loss: 2.382256]\n",
      "epoch:6 step:6499 [D loss: 0.670066, acc: 65.62%] [G loss: 2.119220]\n",
      "epoch:6 step:6500 [D loss: 0.713539, acc: 57.81%] [G loss: 2.126658]\n",
      "epoch:6 step:6501 [D loss: 0.620965, acc: 63.28%] [G loss: 2.187835]\n",
      "epoch:6 step:6502 [D loss: 0.613667, acc: 63.28%] [G loss: 2.178446]\n",
      "epoch:6 step:6503 [D loss: 0.652601, acc: 66.41%] [G loss: 2.234270]\n",
      "epoch:6 step:6504 [D loss: 0.569192, acc: 75.00%] [G loss: 2.165869]\n",
      "epoch:6 step:6505 [D loss: 0.589793, acc: 65.62%] [G loss: 2.380053]\n",
      "epoch:6 step:6506 [D loss: 0.516357, acc: 75.78%] [G loss: 2.614379]\n",
      "epoch:6 step:6507 [D loss: 0.599796, acc: 69.53%] [G loss: 2.502580]\n",
      "epoch:6 step:6508 [D loss: 0.588135, acc: 71.88%] [G loss: 2.633312]\n",
      "epoch:6 step:6509 [D loss: 0.541931, acc: 75.78%] [G loss: 2.504107]\n",
      "epoch:6 step:6510 [D loss: 0.599023, acc: 68.75%] [G loss: 2.523915]\n",
      "epoch:6 step:6511 [D loss: 0.643776, acc: 56.25%] [G loss: 2.305306]\n",
      "epoch:6 step:6512 [D loss: 0.629602, acc: 67.19%] [G loss: 2.570642]\n",
      "epoch:6 step:6513 [D loss: 0.581053, acc: 61.72%] [G loss: 2.347327]\n",
      "epoch:6 step:6514 [D loss: 0.702667, acc: 57.81%] [G loss: 2.366799]\n",
      "epoch:6 step:6515 [D loss: 0.542687, acc: 77.34%] [G loss: 2.562719]\n",
      "epoch:6 step:6516 [D loss: 0.635261, acc: 66.41%] [G loss: 2.298936]\n",
      "epoch:6 step:6517 [D loss: 0.606788, acc: 63.28%] [G loss: 2.351965]\n",
      "epoch:6 step:6518 [D loss: 0.634893, acc: 62.50%] [G loss: 2.425262]\n",
      "epoch:6 step:6519 [D loss: 0.584188, acc: 69.53%] [G loss: 2.425111]\n",
      "epoch:6 step:6520 [D loss: 0.652404, acc: 63.28%] [G loss: 2.403093]\n",
      "epoch:6 step:6521 [D loss: 0.594636, acc: 65.62%] [G loss: 2.297401]\n",
      "epoch:6 step:6522 [D loss: 0.595202, acc: 66.41%] [G loss: 2.311514]\n",
      "epoch:6 step:6523 [D loss: 0.546799, acc: 71.88%] [G loss: 2.198305]\n",
      "epoch:6 step:6524 [D loss: 0.670136, acc: 63.28%] [G loss: 2.350969]\n",
      "epoch:6 step:6525 [D loss: 0.652225, acc: 64.84%] [G loss: 2.512712]\n",
      "epoch:6 step:6526 [D loss: 0.586430, acc: 71.09%] [G loss: 2.375061]\n",
      "epoch:6 step:6527 [D loss: 0.591254, acc: 68.75%] [G loss: 2.262359]\n",
      "epoch:6 step:6528 [D loss: 0.548836, acc: 67.97%] [G loss: 2.598899]\n",
      "epoch:6 step:6529 [D loss: 0.576888, acc: 71.09%] [G loss: 2.367046]\n",
      "epoch:6 step:6530 [D loss: 0.545561, acc: 70.31%] [G loss: 2.448462]\n",
      "epoch:6 step:6531 [D loss: 0.521649, acc: 80.47%] [G loss: 2.388471]\n",
      "epoch:6 step:6532 [D loss: 0.592451, acc: 67.19%] [G loss: 2.447759]\n",
      "epoch:6 step:6533 [D loss: 0.562161, acc: 71.09%] [G loss: 2.586318]\n",
      "epoch:6 step:6534 [D loss: 0.577787, acc: 70.31%] [G loss: 2.599974]\n",
      "epoch:6 step:6535 [D loss: 0.642749, acc: 63.28%] [G loss: 2.550549]\n",
      "epoch:6 step:6536 [D loss: 0.664008, acc: 67.97%] [G loss: 2.303147]\n",
      "epoch:6 step:6537 [D loss: 0.635835, acc: 62.50%] [G loss: 2.338876]\n",
      "epoch:6 step:6538 [D loss: 0.595753, acc: 66.41%] [G loss: 2.312648]\n",
      "epoch:6 step:6539 [D loss: 0.568025, acc: 71.09%] [G loss: 2.552312]\n",
      "epoch:6 step:6540 [D loss: 0.582398, acc: 70.31%] [G loss: 2.509828]\n",
      "epoch:6 step:6541 [D loss: 0.531783, acc: 72.66%] [G loss: 2.709855]\n",
      "epoch:6 step:6542 [D loss: 0.716575, acc: 58.59%] [G loss: 2.415337]\n",
      "epoch:6 step:6543 [D loss: 0.637866, acc: 61.72%] [G loss: 2.708482]\n",
      "epoch:6 step:6544 [D loss: 0.612611, acc: 63.28%] [G loss: 2.294749]\n",
      "epoch:6 step:6545 [D loss: 0.549746, acc: 73.44%] [G loss: 2.243751]\n",
      "epoch:6 step:6546 [D loss: 0.535158, acc: 77.34%] [G loss: 2.856346]\n",
      "epoch:6 step:6547 [D loss: 0.709937, acc: 54.69%] [G loss: 2.370458]\n",
      "epoch:6 step:6548 [D loss: 0.630899, acc: 66.41%] [G loss: 2.406356]\n",
      "epoch:6 step:6549 [D loss: 0.659375, acc: 64.06%] [G loss: 2.431084]\n",
      "epoch:6 step:6550 [D loss: 0.732356, acc: 58.59%] [G loss: 2.083598]\n",
      "epoch:6 step:6551 [D loss: 0.666066, acc: 55.47%] [G loss: 2.593214]\n",
      "epoch:6 step:6552 [D loss: 0.570558, acc: 72.66%] [G loss: 2.369833]\n",
      "epoch:6 step:6553 [D loss: 0.608536, acc: 67.19%] [G loss: 2.392413]\n",
      "epoch:6 step:6554 [D loss: 0.621323, acc: 67.19%] [G loss: 2.254265]\n",
      "epoch:6 step:6555 [D loss: 0.623454, acc: 63.28%] [G loss: 2.210273]\n",
      "epoch:6 step:6556 [D loss: 0.645917, acc: 66.41%] [G loss: 2.483056]\n",
      "epoch:6 step:6557 [D loss: 0.637941, acc: 60.94%] [G loss: 2.343582]\n",
      "epoch:6 step:6558 [D loss: 0.480079, acc: 77.34%] [G loss: 2.637741]\n",
      "epoch:6 step:6559 [D loss: 0.551886, acc: 71.88%] [G loss: 2.899326]\n",
      "epoch:7 step:6560 [D loss: 0.566866, acc: 68.75%] [G loss: 2.415096]\n",
      "epoch:7 step:6561 [D loss: 0.613001, acc: 65.62%] [G loss: 2.366857]\n",
      "epoch:7 step:6562 [D loss: 0.663267, acc: 60.16%] [G loss: 2.532979]\n",
      "epoch:7 step:6563 [D loss: 0.616444, acc: 70.31%] [G loss: 2.232835]\n",
      "epoch:7 step:6564 [D loss: 0.670653, acc: 60.16%] [G loss: 2.148796]\n",
      "epoch:7 step:6565 [D loss: 0.599427, acc: 68.75%] [G loss: 2.348359]\n",
      "epoch:7 step:6566 [D loss: 0.567035, acc: 67.19%] [G loss: 2.354750]\n",
      "epoch:7 step:6567 [D loss: 0.574123, acc: 71.88%] [G loss: 2.701086]\n",
      "epoch:7 step:6568 [D loss: 0.648732, acc: 67.19%] [G loss: 2.531280]\n",
      "epoch:7 step:6569 [D loss: 0.652370, acc: 64.06%] [G loss: 2.462509]\n",
      "epoch:7 step:6570 [D loss: 0.574369, acc: 71.88%] [G loss: 2.297665]\n",
      "epoch:7 step:6571 [D loss: 0.601078, acc: 71.09%] [G loss: 2.373757]\n",
      "epoch:7 step:6572 [D loss: 0.596323, acc: 67.19%] [G loss: 2.431287]\n",
      "epoch:7 step:6573 [D loss: 0.632419, acc: 67.97%] [G loss: 2.332878]\n",
      "epoch:7 step:6574 [D loss: 0.579596, acc: 68.75%] [G loss: 2.329486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6575 [D loss: 0.585381, acc: 71.09%] [G loss: 2.386691]\n",
      "epoch:7 step:6576 [D loss: 0.630644, acc: 62.50%] [G loss: 2.336377]\n",
      "epoch:7 step:6577 [D loss: 0.659402, acc: 63.28%] [G loss: 2.197514]\n",
      "epoch:7 step:6578 [D loss: 0.570895, acc: 71.88%] [G loss: 2.205304]\n",
      "epoch:7 step:6579 [D loss: 0.698866, acc: 56.25%] [G loss: 2.104758]\n",
      "epoch:7 step:6580 [D loss: 0.587648, acc: 66.41%] [G loss: 1.941774]\n",
      "epoch:7 step:6581 [D loss: 0.588102, acc: 67.97%] [G loss: 2.221888]\n",
      "epoch:7 step:6582 [D loss: 0.578707, acc: 68.75%] [G loss: 2.387975]\n",
      "epoch:7 step:6583 [D loss: 0.633885, acc: 66.41%] [G loss: 2.296159]\n",
      "epoch:7 step:6584 [D loss: 0.558628, acc: 69.53%] [G loss: 2.290401]\n",
      "epoch:7 step:6585 [D loss: 0.628382, acc: 65.62%] [G loss: 2.160970]\n",
      "epoch:7 step:6586 [D loss: 0.614770, acc: 67.19%] [G loss: 2.125055]\n",
      "epoch:7 step:6587 [D loss: 0.553250, acc: 68.75%] [G loss: 2.245011]\n",
      "epoch:7 step:6588 [D loss: 0.572092, acc: 67.19%] [G loss: 2.281456]\n",
      "epoch:7 step:6589 [D loss: 0.689755, acc: 61.72%] [G loss: 2.168263]\n",
      "epoch:7 step:6590 [D loss: 0.629621, acc: 59.38%] [G loss: 1.887673]\n",
      "epoch:7 step:6591 [D loss: 0.692690, acc: 57.03%] [G loss: 2.200737]\n",
      "epoch:7 step:6592 [D loss: 0.618803, acc: 69.53%] [G loss: 2.138682]\n",
      "epoch:7 step:6593 [D loss: 0.605632, acc: 69.53%] [G loss: 2.119445]\n",
      "epoch:7 step:6594 [D loss: 0.593162, acc: 68.75%] [G loss: 2.224301]\n",
      "epoch:7 step:6595 [D loss: 0.570572, acc: 71.88%] [G loss: 2.376367]\n",
      "epoch:7 step:6596 [D loss: 0.561065, acc: 70.31%] [G loss: 2.289691]\n",
      "epoch:7 step:6597 [D loss: 0.587241, acc: 67.97%] [G loss: 2.566831]\n",
      "epoch:7 step:6598 [D loss: 0.577649, acc: 70.31%] [G loss: 2.295069]\n",
      "epoch:7 step:6599 [D loss: 0.532188, acc: 73.44%] [G loss: 2.478932]\n",
      "epoch:7 step:6600 [D loss: 0.635071, acc: 63.28%] [G loss: 2.311954]\n",
      "##############\n",
      "[2.58510131 1.64757309 6.64416238 4.93679528 3.89382395 5.93694249\n",
      " 4.68890778 5.00759856 4.96751727 3.63503776]\n",
      "##########\n",
      "epoch:7 step:6601 [D loss: 0.578143, acc: 71.09%] [G loss: 2.357593]\n",
      "epoch:7 step:6602 [D loss: 0.648670, acc: 64.84%] [G loss: 2.465002]\n",
      "epoch:7 step:6603 [D loss: 0.655416, acc: 59.38%] [G loss: 2.075680]\n",
      "epoch:7 step:6604 [D loss: 0.638314, acc: 62.50%] [G loss: 2.146842]\n",
      "epoch:7 step:6605 [D loss: 0.604657, acc: 66.41%] [G loss: 2.129180]\n",
      "epoch:7 step:6606 [D loss: 0.535505, acc: 71.88%] [G loss: 2.267058]\n",
      "epoch:7 step:6607 [D loss: 0.590667, acc: 70.31%] [G loss: 2.289831]\n",
      "epoch:7 step:6608 [D loss: 0.535653, acc: 70.31%] [G loss: 2.425435]\n",
      "epoch:7 step:6609 [D loss: 0.596307, acc: 70.31%] [G loss: 2.213589]\n",
      "epoch:7 step:6610 [D loss: 0.598373, acc: 65.62%] [G loss: 2.501238]\n",
      "epoch:7 step:6611 [D loss: 0.595729, acc: 67.97%] [G loss: 2.301126]\n",
      "epoch:7 step:6612 [D loss: 0.548385, acc: 71.09%] [G loss: 2.697870]\n",
      "epoch:7 step:6613 [D loss: 0.544870, acc: 72.66%] [G loss: 2.348119]\n",
      "epoch:7 step:6614 [D loss: 0.637653, acc: 65.62%] [G loss: 2.560257]\n",
      "epoch:7 step:6615 [D loss: 0.582230, acc: 68.75%] [G loss: 2.468394]\n",
      "epoch:7 step:6616 [D loss: 0.685881, acc: 63.28%] [G loss: 1.998547]\n",
      "epoch:7 step:6617 [D loss: 0.580449, acc: 69.53%] [G loss: 2.534821]\n",
      "epoch:7 step:6618 [D loss: 0.661218, acc: 61.72%] [G loss: 2.247750]\n",
      "epoch:7 step:6619 [D loss: 0.578793, acc: 68.75%] [G loss: 2.335400]\n",
      "epoch:7 step:6620 [D loss: 0.598096, acc: 68.75%] [G loss: 2.225085]\n",
      "epoch:7 step:6621 [D loss: 0.676658, acc: 64.84%] [G loss: 2.362674]\n",
      "epoch:7 step:6622 [D loss: 0.673642, acc: 64.84%] [G loss: 2.293865]\n",
      "epoch:7 step:6623 [D loss: 0.603947, acc: 67.19%] [G loss: 2.381737]\n",
      "epoch:7 step:6624 [D loss: 0.619855, acc: 64.06%] [G loss: 2.219162]\n",
      "epoch:7 step:6625 [D loss: 0.647453, acc: 67.97%] [G loss: 2.294739]\n",
      "epoch:7 step:6626 [D loss: 0.541367, acc: 75.78%] [G loss: 2.274108]\n",
      "epoch:7 step:6627 [D loss: 0.539759, acc: 74.22%] [G loss: 2.607198]\n",
      "epoch:7 step:6628 [D loss: 0.643779, acc: 59.38%] [G loss: 2.565962]\n",
      "epoch:7 step:6629 [D loss: 0.590476, acc: 65.62%] [G loss: 2.403475]\n",
      "epoch:7 step:6630 [D loss: 0.624942, acc: 65.62%] [G loss: 2.333050]\n",
      "epoch:7 step:6631 [D loss: 0.512663, acc: 76.56%] [G loss: 2.499885]\n",
      "epoch:7 step:6632 [D loss: 0.640743, acc: 64.06%] [G loss: 2.417846]\n",
      "epoch:7 step:6633 [D loss: 0.672196, acc: 57.81%] [G loss: 2.618216]\n",
      "epoch:7 step:6634 [D loss: 0.559900, acc: 70.31%] [G loss: 2.634935]\n",
      "epoch:7 step:6635 [D loss: 0.573883, acc: 66.41%] [G loss: 2.541891]\n",
      "epoch:7 step:6636 [D loss: 0.522147, acc: 73.44%] [G loss: 2.972335]\n",
      "epoch:7 step:6637 [D loss: 0.589950, acc: 69.53%] [G loss: 2.463870]\n",
      "epoch:7 step:6638 [D loss: 0.624711, acc: 65.62%] [G loss: 2.279284]\n",
      "epoch:7 step:6639 [D loss: 0.745840, acc: 52.34%] [G loss: 2.233629]\n",
      "epoch:7 step:6640 [D loss: 0.660287, acc: 63.28%] [G loss: 2.102443]\n",
      "epoch:7 step:6641 [D loss: 0.588336, acc: 67.97%] [G loss: 2.267128]\n",
      "epoch:7 step:6642 [D loss: 0.520105, acc: 77.34%] [G loss: 2.557114]\n",
      "epoch:7 step:6643 [D loss: 0.659628, acc: 58.59%] [G loss: 2.178691]\n",
      "epoch:7 step:6644 [D loss: 0.654766, acc: 62.50%] [G loss: 2.280333]\n",
      "epoch:7 step:6645 [D loss: 0.689787, acc: 60.94%] [G loss: 2.096575]\n",
      "epoch:7 step:6646 [D loss: 0.601756, acc: 67.19%] [G loss: 2.356161]\n",
      "epoch:7 step:6647 [D loss: 0.585093, acc: 71.88%] [G loss: 2.451840]\n",
      "epoch:7 step:6648 [D loss: 0.529022, acc: 78.12%] [G loss: 2.270865]\n",
      "epoch:7 step:6649 [D loss: 0.646013, acc: 62.50%] [G loss: 2.313993]\n",
      "epoch:7 step:6650 [D loss: 0.614015, acc: 69.53%] [G loss: 2.216320]\n",
      "epoch:7 step:6651 [D loss: 0.599098, acc: 63.28%] [G loss: 2.440001]\n",
      "epoch:7 step:6652 [D loss: 0.523670, acc: 75.78%] [G loss: 2.352808]\n",
      "epoch:7 step:6653 [D loss: 0.564914, acc: 70.31%] [G loss: 2.506086]\n",
      "epoch:7 step:6654 [D loss: 0.631491, acc: 63.28%] [G loss: 2.073857]\n",
      "epoch:7 step:6655 [D loss: 0.586285, acc: 70.31%] [G loss: 2.317665]\n",
      "epoch:7 step:6656 [D loss: 0.653834, acc: 58.59%] [G loss: 2.257883]\n",
      "epoch:7 step:6657 [D loss: 0.690808, acc: 61.72%] [G loss: 2.278866]\n",
      "epoch:7 step:6658 [D loss: 0.654424, acc: 64.84%] [G loss: 2.221822]\n",
      "epoch:7 step:6659 [D loss: 0.618471, acc: 66.41%] [G loss: 2.402556]\n",
      "epoch:7 step:6660 [D loss: 0.577341, acc: 74.22%] [G loss: 2.444087]\n",
      "epoch:7 step:6661 [D loss: 0.593058, acc: 67.97%] [G loss: 2.291260]\n",
      "epoch:7 step:6662 [D loss: 0.599910, acc: 64.84%] [G loss: 2.468891]\n",
      "epoch:7 step:6663 [D loss: 0.576728, acc: 68.75%] [G loss: 2.591218]\n",
      "epoch:7 step:6664 [D loss: 0.638108, acc: 62.50%] [G loss: 2.404391]\n",
      "epoch:7 step:6665 [D loss: 0.624324, acc: 66.41%] [G loss: 2.217764]\n",
      "epoch:7 step:6666 [D loss: 0.536547, acc: 71.09%] [G loss: 2.285671]\n",
      "epoch:7 step:6667 [D loss: 0.669094, acc: 64.06%] [G loss: 2.090003]\n",
      "epoch:7 step:6668 [D loss: 0.588067, acc: 73.44%] [G loss: 2.323596]\n",
      "epoch:7 step:6669 [D loss: 0.645256, acc: 64.84%] [G loss: 2.179239]\n",
      "epoch:7 step:6670 [D loss: 0.582055, acc: 70.31%] [G loss: 2.456984]\n",
      "epoch:7 step:6671 [D loss: 0.602888, acc: 67.97%] [G loss: 2.523959]\n",
      "epoch:7 step:6672 [D loss: 0.562680, acc: 74.22%] [G loss: 2.174973]\n",
      "epoch:7 step:6673 [D loss: 0.648036, acc: 63.28%] [G loss: 2.442348]\n",
      "epoch:7 step:6674 [D loss: 0.614965, acc: 65.62%] [G loss: 2.443153]\n",
      "epoch:7 step:6675 [D loss: 0.619449, acc: 63.28%] [G loss: 2.481441]\n",
      "epoch:7 step:6676 [D loss: 0.549636, acc: 74.22%] [G loss: 2.610673]\n",
      "epoch:7 step:6677 [D loss: 0.638193, acc: 64.84%] [G loss: 2.218064]\n",
      "epoch:7 step:6678 [D loss: 0.580979, acc: 72.66%] [G loss: 2.646695]\n",
      "epoch:7 step:6679 [D loss: 0.594631, acc: 67.19%] [G loss: 2.615621]\n",
      "epoch:7 step:6680 [D loss: 0.603348, acc: 67.19%] [G loss: 2.627259]\n",
      "epoch:7 step:6681 [D loss: 0.539122, acc: 75.00%] [G loss: 2.827808]\n",
      "epoch:7 step:6682 [D loss: 0.667810, acc: 66.41%] [G loss: 2.367914]\n",
      "epoch:7 step:6683 [D loss: 0.645901, acc: 65.62%] [G loss: 2.201901]\n",
      "epoch:7 step:6684 [D loss: 0.599482, acc: 67.19%] [G loss: 2.000711]\n",
      "epoch:7 step:6685 [D loss: 0.616563, acc: 62.50%] [G loss: 2.321581]\n",
      "epoch:7 step:6686 [D loss: 0.653876, acc: 57.03%] [G loss: 2.166677]\n",
      "epoch:7 step:6687 [D loss: 0.579317, acc: 71.88%] [G loss: 2.198893]\n",
      "epoch:7 step:6688 [D loss: 0.617188, acc: 63.28%] [G loss: 2.344552]\n",
      "epoch:7 step:6689 [D loss: 0.620399, acc: 66.41%] [G loss: 2.620455]\n",
      "epoch:7 step:6690 [D loss: 0.573157, acc: 74.22%] [G loss: 2.467233]\n",
      "epoch:7 step:6691 [D loss: 0.670830, acc: 64.84%] [G loss: 2.366020]\n",
      "epoch:7 step:6692 [D loss: 0.693415, acc: 56.25%] [G loss: 2.224352]\n",
      "epoch:7 step:6693 [D loss: 0.599121, acc: 64.84%] [G loss: 2.066382]\n",
      "epoch:7 step:6694 [D loss: 0.578573, acc: 70.31%] [G loss: 2.247600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6695 [D loss: 0.680582, acc: 63.28%] [G loss: 2.211691]\n",
      "epoch:7 step:6696 [D loss: 0.656031, acc: 68.75%] [G loss: 2.123223]\n",
      "epoch:7 step:6697 [D loss: 0.644440, acc: 61.72%] [G loss: 2.161436]\n",
      "epoch:7 step:6698 [D loss: 0.671578, acc: 63.28%] [G loss: 2.146780]\n",
      "epoch:7 step:6699 [D loss: 0.607914, acc: 62.50%] [G loss: 2.044722]\n",
      "epoch:7 step:6700 [D loss: 0.584610, acc: 67.97%] [G loss: 2.223765]\n",
      "epoch:7 step:6701 [D loss: 0.603421, acc: 66.41%] [G loss: 2.311571]\n",
      "epoch:7 step:6702 [D loss: 0.646256, acc: 62.50%] [G loss: 2.158132]\n",
      "epoch:7 step:6703 [D loss: 0.622391, acc: 70.31%] [G loss: 2.295396]\n",
      "epoch:7 step:6704 [D loss: 0.583457, acc: 67.19%] [G loss: 2.507596]\n",
      "epoch:7 step:6705 [D loss: 0.603623, acc: 67.19%] [G loss: 2.101471]\n",
      "epoch:7 step:6706 [D loss: 0.647459, acc: 63.28%] [G loss: 2.032780]\n",
      "epoch:7 step:6707 [D loss: 0.697661, acc: 60.94%] [G loss: 2.146756]\n",
      "epoch:7 step:6708 [D loss: 0.575464, acc: 68.75%] [G loss: 2.396975]\n",
      "epoch:7 step:6709 [D loss: 0.603086, acc: 67.97%] [G loss: 2.439513]\n",
      "epoch:7 step:6710 [D loss: 0.664747, acc: 59.38%] [G loss: 2.533015]\n",
      "epoch:7 step:6711 [D loss: 0.527908, acc: 72.66%] [G loss: 2.536489]\n",
      "epoch:7 step:6712 [D loss: 0.674156, acc: 60.16%] [G loss: 2.191832]\n",
      "epoch:7 step:6713 [D loss: 0.617007, acc: 64.06%] [G loss: 2.167577]\n",
      "epoch:7 step:6714 [D loss: 0.589374, acc: 67.97%] [G loss: 2.431659]\n",
      "epoch:7 step:6715 [D loss: 0.575137, acc: 68.75%] [G loss: 2.429531]\n",
      "epoch:7 step:6716 [D loss: 0.642441, acc: 60.94%] [G loss: 2.091084]\n",
      "epoch:7 step:6717 [D loss: 0.588441, acc: 69.53%] [G loss: 2.451728]\n",
      "epoch:7 step:6718 [D loss: 0.650538, acc: 64.84%] [G loss: 2.413317]\n",
      "epoch:7 step:6719 [D loss: 0.679081, acc: 60.16%] [G loss: 2.068820]\n",
      "epoch:7 step:6720 [D loss: 0.653144, acc: 65.62%] [G loss: 2.265451]\n",
      "epoch:7 step:6721 [D loss: 0.566177, acc: 71.88%] [G loss: 2.265029]\n",
      "epoch:7 step:6722 [D loss: 0.489603, acc: 75.78%] [G loss: 2.212818]\n",
      "epoch:7 step:6723 [D loss: 0.621849, acc: 65.62%] [G loss: 2.138617]\n",
      "epoch:7 step:6724 [D loss: 0.567120, acc: 68.75%] [G loss: 2.140421]\n",
      "epoch:7 step:6725 [D loss: 0.613061, acc: 64.06%] [G loss: 2.208236]\n",
      "epoch:7 step:6726 [D loss: 0.626491, acc: 65.62%] [G loss: 2.190228]\n",
      "epoch:7 step:6727 [D loss: 0.611501, acc: 64.84%] [G loss: 2.478054]\n",
      "epoch:7 step:6728 [D loss: 0.575459, acc: 71.09%] [G loss: 2.114821]\n",
      "epoch:7 step:6729 [D loss: 0.630319, acc: 64.84%] [G loss: 2.162852]\n",
      "epoch:7 step:6730 [D loss: 0.602459, acc: 67.19%] [G loss: 2.377440]\n",
      "epoch:7 step:6731 [D loss: 0.629032, acc: 61.72%] [G loss: 2.081077]\n",
      "epoch:7 step:6732 [D loss: 0.617570, acc: 64.06%] [G loss: 2.125423]\n",
      "epoch:7 step:6733 [D loss: 0.661896, acc: 60.16%] [G loss: 2.391962]\n",
      "epoch:7 step:6734 [D loss: 0.619969, acc: 62.50%] [G loss: 2.246125]\n",
      "epoch:7 step:6735 [D loss: 0.600618, acc: 68.75%] [G loss: 2.505874]\n",
      "epoch:7 step:6736 [D loss: 0.629412, acc: 67.97%] [G loss: 2.231219]\n",
      "epoch:7 step:6737 [D loss: 0.598969, acc: 66.41%] [G loss: 2.424930]\n",
      "epoch:7 step:6738 [D loss: 0.618324, acc: 67.19%] [G loss: 2.356824]\n",
      "epoch:7 step:6739 [D loss: 0.668060, acc: 61.72%] [G loss: 2.103496]\n",
      "epoch:7 step:6740 [D loss: 0.650200, acc: 66.41%] [G loss: 2.253913]\n",
      "epoch:7 step:6741 [D loss: 0.564689, acc: 71.88%] [G loss: 2.384286]\n",
      "epoch:7 step:6742 [D loss: 0.637043, acc: 64.06%] [G loss: 2.385304]\n",
      "epoch:7 step:6743 [D loss: 0.626675, acc: 60.16%] [G loss: 2.320787]\n",
      "epoch:7 step:6744 [D loss: 0.659734, acc: 69.53%] [G loss: 2.210350]\n",
      "epoch:7 step:6745 [D loss: 0.685151, acc: 55.47%] [G loss: 2.089571]\n",
      "epoch:7 step:6746 [D loss: 0.706529, acc: 55.47%] [G loss: 2.235966]\n",
      "epoch:7 step:6747 [D loss: 0.607834, acc: 67.19%] [G loss: 2.134573]\n",
      "epoch:7 step:6748 [D loss: 0.606420, acc: 67.97%] [G loss: 2.215401]\n",
      "epoch:7 step:6749 [D loss: 0.569169, acc: 68.75%] [G loss: 2.325753]\n",
      "epoch:7 step:6750 [D loss: 0.612272, acc: 71.09%] [G loss: 2.293006]\n",
      "epoch:7 step:6751 [D loss: 0.564529, acc: 71.09%] [G loss: 2.379829]\n",
      "epoch:7 step:6752 [D loss: 0.621133, acc: 68.75%] [G loss: 2.459428]\n",
      "epoch:7 step:6753 [D loss: 0.531843, acc: 74.22%] [G loss: 2.628565]\n",
      "epoch:7 step:6754 [D loss: 0.623285, acc: 62.50%] [G loss: 2.302941]\n",
      "epoch:7 step:6755 [D loss: 0.686217, acc: 60.94%] [G loss: 2.336997]\n",
      "epoch:7 step:6756 [D loss: 0.546663, acc: 75.00%] [G loss: 2.205497]\n",
      "epoch:7 step:6757 [D loss: 0.618299, acc: 67.19%] [G loss: 2.635222]\n",
      "epoch:7 step:6758 [D loss: 0.596186, acc: 67.19%] [G loss: 2.272997]\n",
      "epoch:7 step:6759 [D loss: 0.610013, acc: 65.62%] [G loss: 2.133779]\n",
      "epoch:7 step:6760 [D loss: 0.625723, acc: 61.72%] [G loss: 2.117224]\n",
      "epoch:7 step:6761 [D loss: 0.616986, acc: 67.19%] [G loss: 2.318059]\n",
      "epoch:7 step:6762 [D loss: 0.651080, acc: 63.28%] [G loss: 2.012631]\n",
      "epoch:7 step:6763 [D loss: 0.637838, acc: 64.84%] [G loss: 2.097553]\n",
      "epoch:7 step:6764 [D loss: 0.690545, acc: 61.72%] [G loss: 2.363575]\n",
      "epoch:7 step:6765 [D loss: 0.517889, acc: 79.69%] [G loss: 2.671978]\n",
      "epoch:7 step:6766 [D loss: 0.548391, acc: 73.44%] [G loss: 2.593513]\n",
      "epoch:7 step:6767 [D loss: 0.563067, acc: 71.09%] [G loss: 2.798753]\n",
      "epoch:7 step:6768 [D loss: 0.537227, acc: 75.00%] [G loss: 2.671118]\n",
      "epoch:7 step:6769 [D loss: 0.614369, acc: 67.19%] [G loss: 2.274929]\n",
      "epoch:7 step:6770 [D loss: 0.625605, acc: 60.94%] [G loss: 2.045960]\n",
      "epoch:7 step:6771 [D loss: 0.643171, acc: 60.16%] [G loss: 2.110598]\n",
      "epoch:7 step:6772 [D loss: 0.667076, acc: 61.72%] [G loss: 2.255991]\n",
      "epoch:7 step:6773 [D loss: 0.637166, acc: 60.16%] [G loss: 2.196672]\n",
      "epoch:7 step:6774 [D loss: 0.633277, acc: 66.41%] [G loss: 2.115235]\n",
      "epoch:7 step:6775 [D loss: 0.579247, acc: 71.09%] [G loss: 2.446959]\n",
      "epoch:7 step:6776 [D loss: 0.530769, acc: 76.56%] [G loss: 2.525913]\n",
      "epoch:7 step:6777 [D loss: 0.506626, acc: 77.34%] [G loss: 2.669474]\n",
      "epoch:7 step:6778 [D loss: 0.578240, acc: 74.22%] [G loss: 2.363308]\n",
      "epoch:7 step:6779 [D loss: 0.635955, acc: 63.28%] [G loss: 2.331057]\n",
      "epoch:7 step:6780 [D loss: 0.607320, acc: 64.84%] [G loss: 2.448829]\n",
      "epoch:7 step:6781 [D loss: 0.626509, acc: 71.88%] [G loss: 2.335267]\n",
      "epoch:7 step:6782 [D loss: 0.632710, acc: 61.72%] [G loss: 2.372080]\n",
      "epoch:7 step:6783 [D loss: 0.616968, acc: 67.97%] [G loss: 2.219041]\n",
      "epoch:7 step:6784 [D loss: 0.703933, acc: 53.91%] [G loss: 2.247156]\n",
      "epoch:7 step:6785 [D loss: 0.620767, acc: 64.06%] [G loss: 2.074463]\n",
      "epoch:7 step:6786 [D loss: 0.643501, acc: 61.72%] [G loss: 2.125904]\n",
      "epoch:7 step:6787 [D loss: 0.715341, acc: 58.59%] [G loss: 2.030502]\n",
      "epoch:7 step:6788 [D loss: 0.555247, acc: 71.09%] [G loss: 2.263558]\n",
      "epoch:7 step:6789 [D loss: 0.608265, acc: 71.88%] [G loss: 2.459678]\n",
      "epoch:7 step:6790 [D loss: 0.558726, acc: 68.75%] [G loss: 2.821713]\n",
      "epoch:7 step:6791 [D loss: 0.538882, acc: 71.88%] [G loss: 2.823527]\n",
      "epoch:7 step:6792 [D loss: 0.652339, acc: 61.72%] [G loss: 2.370584]\n",
      "epoch:7 step:6793 [D loss: 0.599628, acc: 67.19%] [G loss: 2.357057]\n",
      "epoch:7 step:6794 [D loss: 0.687477, acc: 62.50%] [G loss: 2.343626]\n",
      "epoch:7 step:6795 [D loss: 0.668938, acc: 66.41%] [G loss: 2.289274]\n",
      "epoch:7 step:6796 [D loss: 0.599478, acc: 67.19%] [G loss: 2.223096]\n",
      "epoch:7 step:6797 [D loss: 0.578107, acc: 70.31%] [G loss: 2.343052]\n",
      "epoch:7 step:6798 [D loss: 0.635278, acc: 60.16%] [G loss: 2.171598]\n",
      "epoch:7 step:6799 [D loss: 0.620028, acc: 64.84%] [G loss: 2.226690]\n",
      "epoch:7 step:6800 [D loss: 0.635680, acc: 60.94%] [G loss: 2.153893]\n",
      "##############\n",
      "[2.64953637 1.54542396 6.74849788 5.22240142 3.90776426 5.83938845\n",
      " 4.81621495 4.97839533 5.22865493 3.6539949 ]\n",
      "##########\n",
      "epoch:7 step:6801 [D loss: 0.562135, acc: 70.31%] [G loss: 2.432833]\n",
      "epoch:7 step:6802 [D loss: 0.543232, acc: 71.09%] [G loss: 2.385692]\n",
      "epoch:7 step:6803 [D loss: 0.629058, acc: 63.28%] [G loss: 2.274411]\n",
      "epoch:7 step:6804 [D loss: 0.588998, acc: 65.62%] [G loss: 2.233040]\n",
      "epoch:7 step:6805 [D loss: 0.660337, acc: 60.94%] [G loss: 2.313706]\n",
      "epoch:7 step:6806 [D loss: 0.665813, acc: 62.50%] [G loss: 2.081615]\n",
      "epoch:7 step:6807 [D loss: 0.660059, acc: 60.16%] [G loss: 2.359059]\n",
      "epoch:7 step:6808 [D loss: 0.617831, acc: 66.41%] [G loss: 2.063542]\n",
      "epoch:7 step:6809 [D loss: 0.699175, acc: 56.25%] [G loss: 2.044761]\n",
      "epoch:7 step:6810 [D loss: 0.651934, acc: 64.06%] [G loss: 2.217954]\n",
      "epoch:7 step:6811 [D loss: 0.655703, acc: 63.28%] [G loss: 2.029596]\n",
      "epoch:7 step:6812 [D loss: 0.619771, acc: 67.19%] [G loss: 2.056559]\n",
      "epoch:7 step:6813 [D loss: 0.637593, acc: 58.59%] [G loss: 2.062294]\n",
      "epoch:7 step:6814 [D loss: 0.524827, acc: 75.00%] [G loss: 2.076474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6815 [D loss: 0.633465, acc: 66.41%] [G loss: 2.140390]\n",
      "epoch:7 step:6816 [D loss: 0.589297, acc: 67.19%] [G loss: 2.348690]\n",
      "epoch:7 step:6817 [D loss: 0.583355, acc: 65.62%] [G loss: 2.211752]\n",
      "epoch:7 step:6818 [D loss: 0.592404, acc: 63.28%] [G loss: 2.307440]\n",
      "epoch:7 step:6819 [D loss: 0.636836, acc: 64.06%] [G loss: 2.252638]\n",
      "epoch:7 step:6820 [D loss: 0.622689, acc: 63.28%] [G loss: 2.450024]\n",
      "epoch:7 step:6821 [D loss: 0.587022, acc: 69.53%] [G loss: 2.364779]\n",
      "epoch:7 step:6822 [D loss: 0.651475, acc: 64.06%] [G loss: 2.191981]\n",
      "epoch:7 step:6823 [D loss: 0.597312, acc: 67.19%] [G loss: 2.448360]\n",
      "epoch:7 step:6824 [D loss: 0.638045, acc: 66.41%] [G loss: 2.201523]\n",
      "epoch:7 step:6825 [D loss: 0.636979, acc: 69.53%] [G loss: 2.400604]\n",
      "epoch:7 step:6826 [D loss: 0.612853, acc: 65.62%] [G loss: 2.185796]\n",
      "epoch:7 step:6827 [D loss: 0.538241, acc: 76.56%] [G loss: 2.155290]\n",
      "epoch:7 step:6828 [D loss: 0.637940, acc: 62.50%] [G loss: 2.232732]\n",
      "epoch:7 step:6829 [D loss: 0.563055, acc: 67.97%] [G loss: 2.365573]\n",
      "epoch:7 step:6830 [D loss: 0.621814, acc: 67.97%] [G loss: 2.327449]\n",
      "epoch:7 step:6831 [D loss: 0.562618, acc: 75.00%] [G loss: 2.401258]\n",
      "epoch:7 step:6832 [D loss: 0.628749, acc: 61.72%] [G loss: 2.201319]\n",
      "epoch:7 step:6833 [D loss: 0.612535, acc: 64.06%] [G loss: 2.184742]\n",
      "epoch:7 step:6834 [D loss: 0.583778, acc: 66.41%] [G loss: 2.303823]\n",
      "epoch:7 step:6835 [D loss: 0.675753, acc: 64.06%] [G loss: 2.349254]\n",
      "epoch:7 step:6836 [D loss: 0.636310, acc: 61.72%] [G loss: 2.294699]\n",
      "epoch:7 step:6837 [D loss: 0.611827, acc: 64.06%] [G loss: 2.220704]\n",
      "epoch:7 step:6838 [D loss: 0.577539, acc: 72.66%] [G loss: 2.258189]\n",
      "epoch:7 step:6839 [D loss: 0.604346, acc: 69.53%] [G loss: 2.542760]\n",
      "epoch:7 step:6840 [D loss: 0.614899, acc: 67.19%] [G loss: 2.047001]\n",
      "epoch:7 step:6841 [D loss: 0.663057, acc: 62.50%] [G loss: 2.285758]\n",
      "epoch:7 step:6842 [D loss: 0.562387, acc: 69.53%] [G loss: 2.310103]\n",
      "epoch:7 step:6843 [D loss: 0.622432, acc: 64.06%] [G loss: 2.608610]\n",
      "epoch:7 step:6844 [D loss: 0.585746, acc: 65.62%] [G loss: 2.332390]\n",
      "epoch:7 step:6845 [D loss: 0.574923, acc: 73.44%] [G loss: 2.644264]\n",
      "epoch:7 step:6846 [D loss: 0.656397, acc: 62.50%] [G loss: 2.423933]\n",
      "epoch:7 step:6847 [D loss: 0.633693, acc: 66.41%] [G loss: 2.174630]\n",
      "epoch:7 step:6848 [D loss: 0.649169, acc: 64.84%] [G loss: 2.085475]\n",
      "epoch:7 step:6849 [D loss: 0.587134, acc: 65.62%] [G loss: 2.339468]\n",
      "epoch:7 step:6850 [D loss: 0.674711, acc: 66.41%] [G loss: 2.227331]\n",
      "epoch:7 step:6851 [D loss: 0.638112, acc: 62.50%] [G loss: 2.193885]\n",
      "epoch:7 step:6852 [D loss: 0.623117, acc: 67.97%] [G loss: 2.421561]\n",
      "epoch:7 step:6853 [D loss: 0.627440, acc: 62.50%] [G loss: 2.041382]\n",
      "epoch:7 step:6854 [D loss: 0.572525, acc: 70.31%] [G loss: 2.399064]\n",
      "epoch:7 step:6855 [D loss: 0.567656, acc: 63.28%] [G loss: 2.502527]\n",
      "epoch:7 step:6856 [D loss: 0.611593, acc: 64.84%] [G loss: 2.205893]\n",
      "epoch:7 step:6857 [D loss: 0.538294, acc: 75.00%] [G loss: 2.366005]\n",
      "epoch:7 step:6858 [D loss: 0.623579, acc: 70.31%] [G loss: 2.456047]\n",
      "epoch:7 step:6859 [D loss: 0.595644, acc: 65.62%] [G loss: 2.418121]\n",
      "epoch:7 step:6860 [D loss: 0.657021, acc: 57.03%] [G loss: 2.110277]\n",
      "epoch:7 step:6861 [D loss: 0.651613, acc: 60.94%] [G loss: 2.202566]\n",
      "epoch:7 step:6862 [D loss: 0.608365, acc: 65.62%] [G loss: 2.108430]\n",
      "epoch:7 step:6863 [D loss: 0.585198, acc: 66.41%] [G loss: 2.197539]\n",
      "epoch:7 step:6864 [D loss: 0.642091, acc: 63.28%] [G loss: 2.197082]\n",
      "epoch:7 step:6865 [D loss: 0.665421, acc: 64.84%] [G loss: 1.994604]\n",
      "epoch:7 step:6866 [D loss: 0.642875, acc: 60.94%] [G loss: 2.359545]\n",
      "epoch:7 step:6867 [D loss: 0.566878, acc: 69.53%] [G loss: 2.208374]\n",
      "epoch:7 step:6868 [D loss: 0.570406, acc: 69.53%] [G loss: 2.327663]\n",
      "epoch:7 step:6869 [D loss: 0.634962, acc: 64.06%] [G loss: 2.121359]\n",
      "epoch:7 step:6870 [D loss: 0.576851, acc: 68.75%] [G loss: 2.229692]\n",
      "epoch:7 step:6871 [D loss: 0.458590, acc: 79.69%] [G loss: 2.678676]\n",
      "epoch:7 step:6872 [D loss: 0.489385, acc: 78.91%] [G loss: 2.611339]\n",
      "epoch:7 step:6873 [D loss: 0.554716, acc: 69.53%] [G loss: 2.983043]\n",
      "epoch:7 step:6874 [D loss: 0.595126, acc: 77.34%] [G loss: 2.614545]\n",
      "epoch:7 step:6875 [D loss: 0.611788, acc: 65.62%] [G loss: 2.046551]\n",
      "epoch:7 step:6876 [D loss: 0.621778, acc: 64.84%] [G loss: 2.192526]\n",
      "epoch:7 step:6877 [D loss: 0.604038, acc: 71.88%] [G loss: 2.403843]\n",
      "epoch:7 step:6878 [D loss: 0.655985, acc: 60.94%] [G loss: 2.067289]\n",
      "epoch:7 step:6879 [D loss: 0.582627, acc: 65.62%] [G loss: 2.354678]\n",
      "epoch:7 step:6880 [D loss: 0.581756, acc: 69.53%] [G loss: 2.556381]\n",
      "epoch:7 step:6881 [D loss: 0.577890, acc: 71.09%] [G loss: 2.329405]\n",
      "epoch:7 step:6882 [D loss: 0.581889, acc: 71.09%] [G loss: 2.231197]\n",
      "epoch:7 step:6883 [D loss: 0.588785, acc: 72.66%] [G loss: 2.161852]\n",
      "epoch:7 step:6884 [D loss: 0.642706, acc: 62.50%] [G loss: 2.401542]\n",
      "epoch:7 step:6885 [D loss: 0.742754, acc: 55.47%] [G loss: 2.206817]\n",
      "epoch:7 step:6886 [D loss: 0.645092, acc: 67.19%] [G loss: 2.278494]\n",
      "epoch:7 step:6887 [D loss: 0.623796, acc: 60.94%] [G loss: 2.234531]\n",
      "epoch:7 step:6888 [D loss: 0.584531, acc: 65.62%] [G loss: 2.543016]\n",
      "epoch:7 step:6889 [D loss: 0.677464, acc: 58.59%] [G loss: 2.320516]\n",
      "epoch:7 step:6890 [D loss: 0.566090, acc: 65.62%] [G loss: 2.453692]\n",
      "epoch:7 step:6891 [D loss: 0.599096, acc: 71.88%] [G loss: 2.454602]\n",
      "epoch:7 step:6892 [D loss: 0.569941, acc: 70.31%] [G loss: 2.421977]\n",
      "epoch:7 step:6893 [D loss: 0.699082, acc: 62.50%] [G loss: 2.343287]\n",
      "epoch:7 step:6894 [D loss: 0.681842, acc: 62.50%] [G loss: 2.185989]\n",
      "epoch:7 step:6895 [D loss: 0.556855, acc: 74.22%] [G loss: 2.421843]\n",
      "epoch:7 step:6896 [D loss: 0.561395, acc: 75.00%] [G loss: 2.540273]\n",
      "epoch:7 step:6897 [D loss: 0.501769, acc: 76.56%] [G loss: 2.573140]\n",
      "epoch:7 step:6898 [D loss: 0.574307, acc: 68.75%] [G loss: 2.463393]\n",
      "epoch:7 step:6899 [D loss: 0.581499, acc: 70.31%] [G loss: 2.368437]\n",
      "epoch:7 step:6900 [D loss: 0.664023, acc: 64.06%] [G loss: 2.103647]\n",
      "epoch:7 step:6901 [D loss: 0.636088, acc: 66.41%] [G loss: 2.285688]\n",
      "epoch:7 step:6902 [D loss: 0.581106, acc: 70.31%] [G loss: 2.637009]\n",
      "epoch:7 step:6903 [D loss: 0.662031, acc: 67.19%] [G loss: 2.489264]\n",
      "epoch:7 step:6904 [D loss: 0.531945, acc: 75.00%] [G loss: 2.763318]\n",
      "epoch:7 step:6905 [D loss: 0.504767, acc: 73.44%] [G loss: 2.883139]\n",
      "epoch:7 step:6906 [D loss: 0.582218, acc: 70.31%] [G loss: 2.683530]\n",
      "epoch:7 step:6907 [D loss: 0.778825, acc: 57.81%] [G loss: 2.087015]\n",
      "epoch:7 step:6908 [D loss: 0.706299, acc: 57.81%] [G loss: 1.915398]\n",
      "epoch:7 step:6909 [D loss: 0.613784, acc: 66.41%] [G loss: 2.305296]\n",
      "epoch:7 step:6910 [D loss: 0.584198, acc: 67.97%] [G loss: 2.369019]\n",
      "epoch:7 step:6911 [D loss: 0.697888, acc: 63.28%] [G loss: 2.354552]\n",
      "epoch:7 step:6912 [D loss: 0.593292, acc: 65.62%] [G loss: 2.568303]\n",
      "epoch:7 step:6913 [D loss: 0.498237, acc: 77.34%] [G loss: 2.584704]\n",
      "epoch:7 step:6914 [D loss: 0.639026, acc: 63.28%] [G loss: 2.169862]\n",
      "epoch:7 step:6915 [D loss: 0.706004, acc: 60.94%] [G loss: 2.126452]\n",
      "epoch:7 step:6916 [D loss: 0.540539, acc: 67.97%] [G loss: 2.521185]\n",
      "epoch:7 step:6917 [D loss: 0.632557, acc: 67.97%] [G loss: 2.503516]\n",
      "epoch:7 step:6918 [D loss: 0.580849, acc: 68.75%] [G loss: 2.303720]\n",
      "epoch:7 step:6919 [D loss: 0.547723, acc: 70.31%] [G loss: 2.146649]\n",
      "epoch:7 step:6920 [D loss: 0.661737, acc: 60.94%] [G loss: 2.262664]\n",
      "epoch:7 step:6921 [D loss: 0.668993, acc: 62.50%] [G loss: 2.023312]\n",
      "epoch:7 step:6922 [D loss: 0.626730, acc: 67.19%] [G loss: 2.126594]\n",
      "epoch:7 step:6923 [D loss: 0.571974, acc: 67.97%] [G loss: 2.425882]\n",
      "epoch:7 step:6924 [D loss: 0.605463, acc: 63.28%] [G loss: 2.097183]\n",
      "epoch:7 step:6925 [D loss: 0.610938, acc: 68.75%] [G loss: 2.382254]\n",
      "epoch:7 step:6926 [D loss: 0.617867, acc: 67.97%] [G loss: 2.309589]\n",
      "epoch:7 step:6927 [D loss: 0.603030, acc: 68.75%] [G loss: 2.273790]\n",
      "epoch:7 step:6928 [D loss: 0.730009, acc: 60.94%] [G loss: 2.297529]\n",
      "epoch:7 step:6929 [D loss: 0.598349, acc: 68.75%] [G loss: 2.130752]\n",
      "epoch:7 step:6930 [D loss: 0.585804, acc: 71.88%] [G loss: 2.325296]\n",
      "epoch:7 step:6931 [D loss: 0.602818, acc: 68.75%] [G loss: 2.390910]\n",
      "epoch:7 step:6932 [D loss: 0.765563, acc: 52.34%] [G loss: 1.974785]\n",
      "epoch:7 step:6933 [D loss: 0.539151, acc: 76.56%] [G loss: 2.486012]\n",
      "epoch:7 step:6934 [D loss: 0.663634, acc: 61.72%] [G loss: 2.064582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6935 [D loss: 0.695221, acc: 58.59%] [G loss: 2.081551]\n",
      "epoch:7 step:6936 [D loss: 0.723790, acc: 57.81%] [G loss: 2.087511]\n",
      "epoch:7 step:6937 [D loss: 0.623924, acc: 67.19%] [G loss: 2.178674]\n",
      "epoch:7 step:6938 [D loss: 0.585040, acc: 67.97%] [G loss: 2.268649]\n",
      "epoch:7 step:6939 [D loss: 0.678766, acc: 62.50%] [G loss: 2.335798]\n",
      "epoch:7 step:6940 [D loss: 0.611211, acc: 70.31%] [G loss: 2.418126]\n",
      "epoch:7 step:6941 [D loss: 0.661286, acc: 63.28%] [G loss: 2.310150]\n",
      "epoch:7 step:6942 [D loss: 0.658329, acc: 64.84%] [G loss: 1.950065]\n",
      "epoch:7 step:6943 [D loss: 0.590116, acc: 67.97%] [G loss: 2.336935]\n",
      "epoch:7 step:6944 [D loss: 0.625561, acc: 67.97%] [G loss: 2.202361]\n",
      "epoch:7 step:6945 [D loss: 0.582732, acc: 71.09%] [G loss: 2.162358]\n",
      "epoch:7 step:6946 [D loss: 0.652594, acc: 63.28%] [G loss: 2.188041]\n",
      "epoch:7 step:6947 [D loss: 0.633703, acc: 67.97%] [G loss: 2.091137]\n",
      "epoch:7 step:6948 [D loss: 0.567624, acc: 69.53%] [G loss: 2.036873]\n",
      "epoch:7 step:6949 [D loss: 0.590729, acc: 67.19%] [G loss: 2.019709]\n",
      "epoch:7 step:6950 [D loss: 0.744599, acc: 57.81%] [G loss: 1.995648]\n",
      "epoch:7 step:6951 [D loss: 0.627697, acc: 64.06%] [G loss: 2.314231]\n",
      "epoch:7 step:6952 [D loss: 0.656346, acc: 62.50%] [G loss: 2.247339]\n",
      "epoch:7 step:6953 [D loss: 0.612144, acc: 66.41%] [G loss: 2.158248]\n",
      "epoch:7 step:6954 [D loss: 0.649397, acc: 60.16%] [G loss: 2.146453]\n",
      "epoch:7 step:6955 [D loss: 0.674290, acc: 57.81%] [G loss: 2.181147]\n",
      "epoch:7 step:6956 [D loss: 0.616575, acc: 64.06%] [G loss: 2.027007]\n",
      "epoch:7 step:6957 [D loss: 0.614825, acc: 60.94%] [G loss: 2.039568]\n",
      "epoch:7 step:6958 [D loss: 0.574402, acc: 67.97%] [G loss: 2.194691]\n",
      "epoch:7 step:6959 [D loss: 0.627609, acc: 66.41%] [G loss: 2.327430]\n",
      "epoch:7 step:6960 [D loss: 0.695585, acc: 62.50%] [G loss: 2.103683]\n",
      "epoch:7 step:6961 [D loss: 0.509134, acc: 75.78%] [G loss: 2.422124]\n",
      "epoch:7 step:6962 [D loss: 0.563767, acc: 72.66%] [G loss: 2.378692]\n",
      "epoch:7 step:6963 [D loss: 0.654853, acc: 60.94%] [G loss: 2.208217]\n",
      "epoch:7 step:6964 [D loss: 0.532012, acc: 73.44%] [G loss: 2.560830]\n",
      "epoch:7 step:6965 [D loss: 0.574821, acc: 68.75%] [G loss: 2.488088]\n",
      "epoch:7 step:6966 [D loss: 0.719459, acc: 64.84%] [G loss: 2.014787]\n",
      "epoch:7 step:6967 [D loss: 0.605624, acc: 61.72%] [G loss: 2.174749]\n",
      "epoch:7 step:6968 [D loss: 0.693435, acc: 58.59%] [G loss: 2.116750]\n",
      "epoch:7 step:6969 [D loss: 0.658941, acc: 66.41%] [G loss: 2.236550]\n",
      "epoch:7 step:6970 [D loss: 0.605525, acc: 64.84%] [G loss: 2.454009]\n",
      "epoch:7 step:6971 [D loss: 0.544567, acc: 75.00%] [G loss: 2.241972]\n",
      "epoch:7 step:6972 [D loss: 0.616897, acc: 66.41%] [G loss: 2.464059]\n",
      "epoch:7 step:6973 [D loss: 0.628855, acc: 67.97%] [G loss: 2.315270]\n",
      "epoch:7 step:6974 [D loss: 0.615989, acc: 66.41%] [G loss: 2.372308]\n",
      "epoch:7 step:6975 [D loss: 0.587406, acc: 70.31%] [G loss: 2.264587]\n",
      "epoch:7 step:6976 [D loss: 0.711010, acc: 60.16%] [G loss: 2.067238]\n",
      "epoch:7 step:6977 [D loss: 0.681831, acc: 61.72%] [G loss: 2.268963]\n",
      "epoch:7 step:6978 [D loss: 0.652555, acc: 66.41%] [G loss: 2.273944]\n",
      "epoch:7 step:6979 [D loss: 0.635683, acc: 60.94%] [G loss: 2.144713]\n",
      "epoch:7 step:6980 [D loss: 0.635279, acc: 64.84%] [G loss: 2.077777]\n",
      "epoch:7 step:6981 [D loss: 0.708204, acc: 56.25%] [G loss: 2.120511]\n",
      "epoch:7 step:6982 [D loss: 0.631091, acc: 64.84%] [G loss: 2.062142]\n",
      "epoch:7 step:6983 [D loss: 0.576971, acc: 74.22%] [G loss: 2.321845]\n",
      "epoch:7 step:6984 [D loss: 0.591882, acc: 70.31%] [G loss: 2.185364]\n",
      "epoch:7 step:6985 [D loss: 0.634998, acc: 67.19%] [G loss: 2.425768]\n",
      "epoch:7 step:6986 [D loss: 0.586796, acc: 67.97%] [G loss: 2.258414]\n",
      "epoch:7 step:6987 [D loss: 0.518167, acc: 75.78%] [G loss: 2.528204]\n",
      "epoch:7 step:6988 [D loss: 0.633707, acc: 64.84%] [G loss: 2.587845]\n",
      "epoch:7 step:6989 [D loss: 0.525196, acc: 74.22%] [G loss: 2.659182]\n",
      "epoch:7 step:6990 [D loss: 0.681410, acc: 61.72%] [G loss: 2.319361]\n",
      "epoch:7 step:6991 [D loss: 0.634940, acc: 71.09%] [G loss: 2.467978]\n",
      "epoch:7 step:6992 [D loss: 0.611046, acc: 67.19%] [G loss: 2.270186]\n",
      "epoch:7 step:6993 [D loss: 0.604683, acc: 68.75%] [G loss: 2.508434]\n",
      "epoch:7 step:6994 [D loss: 0.604128, acc: 65.62%] [G loss: 2.106346]\n",
      "epoch:7 step:6995 [D loss: 0.569804, acc: 70.31%] [G loss: 2.080247]\n",
      "epoch:7 step:6996 [D loss: 0.695443, acc: 61.72%] [G loss: 2.123010]\n",
      "epoch:7 step:6997 [D loss: 0.750388, acc: 53.91%] [G loss: 2.134107]\n",
      "epoch:7 step:6998 [D loss: 0.687912, acc: 56.25%] [G loss: 1.973082]\n",
      "epoch:7 step:6999 [D loss: 0.646506, acc: 60.16%] [G loss: 2.177503]\n",
      "epoch:7 step:7000 [D loss: 0.608855, acc: 64.06%] [G loss: 2.064248]\n",
      "##############\n",
      "[2.76508504 1.67197078 6.75068004 5.15033285 3.96750863 5.81306301\n",
      " 4.84778262 4.86131638 4.99553603 3.73065538]\n",
      "##########\n",
      "epoch:7 step:7001 [D loss: 0.589924, acc: 67.97%] [G loss: 2.081306]\n",
      "epoch:7 step:7002 [D loss: 0.646561, acc: 64.06%] [G loss: 2.211074]\n",
      "epoch:7 step:7003 [D loss: 0.634776, acc: 66.41%] [G loss: 2.062343]\n",
      "epoch:7 step:7004 [D loss: 0.595837, acc: 72.66%] [G loss: 2.030202]\n",
      "epoch:7 step:7005 [D loss: 0.588394, acc: 71.88%] [G loss: 2.081275]\n",
      "epoch:7 step:7006 [D loss: 0.617104, acc: 64.06%] [G loss: 2.167277]\n",
      "epoch:7 step:7007 [D loss: 0.641045, acc: 61.72%] [G loss: 1.899283]\n",
      "epoch:7 step:7008 [D loss: 0.573261, acc: 70.31%] [G loss: 2.116155]\n",
      "epoch:7 step:7009 [D loss: 0.633330, acc: 60.16%] [G loss: 2.297213]\n",
      "epoch:7 step:7010 [D loss: 0.587807, acc: 70.31%] [G loss: 2.241837]\n",
      "epoch:7 step:7011 [D loss: 0.632696, acc: 68.75%] [G loss: 2.333716]\n",
      "epoch:7 step:7012 [D loss: 0.606602, acc: 67.97%] [G loss: 2.403670]\n",
      "epoch:7 step:7013 [D loss: 0.594725, acc: 74.22%] [G loss: 2.386696]\n",
      "epoch:7 step:7014 [D loss: 0.679476, acc: 57.81%] [G loss: 2.375288]\n",
      "epoch:7 step:7015 [D loss: 0.578850, acc: 71.09%] [G loss: 2.307742]\n",
      "epoch:7 step:7016 [D loss: 0.660463, acc: 60.94%] [G loss: 2.169349]\n",
      "epoch:7 step:7017 [D loss: 0.639238, acc: 64.84%] [G loss: 2.077989]\n",
      "epoch:7 step:7018 [D loss: 0.705941, acc: 57.03%] [G loss: 2.070345]\n",
      "epoch:7 step:7019 [D loss: 0.618345, acc: 70.31%] [G loss: 2.111306]\n",
      "epoch:7 step:7020 [D loss: 0.608074, acc: 65.62%] [G loss: 2.077542]\n",
      "epoch:7 step:7021 [D loss: 0.560332, acc: 70.31%] [G loss: 2.170395]\n",
      "epoch:7 step:7022 [D loss: 0.617156, acc: 67.19%] [G loss: 2.337354]\n",
      "epoch:7 step:7023 [D loss: 0.621899, acc: 69.53%] [G loss: 2.393368]\n",
      "epoch:7 step:7024 [D loss: 0.622389, acc: 67.19%] [G loss: 2.115794]\n",
      "epoch:7 step:7025 [D loss: 0.684381, acc: 70.31%] [G loss: 2.192672]\n",
      "epoch:7 step:7026 [D loss: 0.561544, acc: 71.88%] [G loss: 2.195462]\n",
      "epoch:7 step:7027 [D loss: 0.648030, acc: 67.97%] [G loss: 2.122218]\n",
      "epoch:7 step:7028 [D loss: 0.649909, acc: 59.38%] [G loss: 2.250518]\n",
      "epoch:7 step:7029 [D loss: 0.596830, acc: 67.19%] [G loss: 2.459229]\n",
      "epoch:7 step:7030 [D loss: 0.500664, acc: 73.44%] [G loss: 2.933949]\n",
      "epoch:7 step:7031 [D loss: 0.597406, acc: 69.53%] [G loss: 2.819555]\n",
      "epoch:7 step:7032 [D loss: 0.649469, acc: 57.81%] [G loss: 2.038757]\n",
      "epoch:7 step:7033 [D loss: 0.568638, acc: 71.09%] [G loss: 2.294008]\n",
      "epoch:7 step:7034 [D loss: 0.625810, acc: 67.19%] [G loss: 2.429541]\n",
      "epoch:7 step:7035 [D loss: 0.701610, acc: 59.38%] [G loss: 2.253323]\n",
      "epoch:7 step:7036 [D loss: 0.674997, acc: 64.84%] [G loss: 1.970356]\n",
      "epoch:7 step:7037 [D loss: 0.608054, acc: 65.62%] [G loss: 2.034551]\n",
      "epoch:7 step:7038 [D loss: 0.548083, acc: 71.09%] [G loss: 2.296645]\n",
      "epoch:7 step:7039 [D loss: 0.646561, acc: 62.50%] [G loss: 2.295934]\n",
      "epoch:7 step:7040 [D loss: 0.607553, acc: 67.19%] [G loss: 2.358251]\n",
      "epoch:7 step:7041 [D loss: 0.690119, acc: 55.47%] [G loss: 1.853683]\n",
      "epoch:7 step:7042 [D loss: 0.606621, acc: 60.94%] [G loss: 2.162778]\n",
      "epoch:7 step:7043 [D loss: 0.634371, acc: 64.06%] [G loss: 2.307432]\n",
      "epoch:7 step:7044 [D loss: 0.649136, acc: 56.25%] [G loss: 2.255281]\n",
      "epoch:7 step:7045 [D loss: 0.647388, acc: 64.06%] [G loss: 2.132452]\n",
      "epoch:7 step:7046 [D loss: 0.657164, acc: 57.81%] [G loss: 2.048767]\n",
      "epoch:7 step:7047 [D loss: 0.577202, acc: 69.53%] [G loss: 2.358480]\n",
      "epoch:7 step:7048 [D loss: 0.614297, acc: 68.75%] [G loss: 2.320369]\n",
      "epoch:7 step:7049 [D loss: 0.618842, acc: 67.19%] [G loss: 2.302375]\n",
      "epoch:7 step:7050 [D loss: 0.606758, acc: 70.31%] [G loss: 2.281117]\n",
      "epoch:7 step:7051 [D loss: 0.689925, acc: 55.47%] [G loss: 2.032688]\n",
      "epoch:7 step:7052 [D loss: 0.621741, acc: 64.84%] [G loss: 2.055671]\n",
      "epoch:7 step:7053 [D loss: 0.649715, acc: 66.41%] [G loss: 2.268901]\n",
      "epoch:7 step:7054 [D loss: 0.551081, acc: 71.09%] [G loss: 2.376118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7055 [D loss: 0.646876, acc: 64.06%] [G loss: 2.094720]\n",
      "epoch:7 step:7056 [D loss: 0.551112, acc: 73.44%] [G loss: 2.480609]\n",
      "epoch:7 step:7057 [D loss: 0.521458, acc: 75.78%] [G loss: 2.381868]\n",
      "epoch:7 step:7058 [D loss: 0.629095, acc: 65.62%] [G loss: 2.323380]\n",
      "epoch:7 step:7059 [D loss: 0.595310, acc: 67.97%] [G loss: 1.971038]\n",
      "epoch:7 step:7060 [D loss: 0.797979, acc: 50.00%] [G loss: 2.148128]\n",
      "epoch:7 step:7061 [D loss: 0.717602, acc: 53.91%] [G loss: 1.947979]\n",
      "epoch:7 step:7062 [D loss: 0.608268, acc: 65.62%] [G loss: 2.300145]\n",
      "epoch:7 step:7063 [D loss: 0.574169, acc: 67.19%] [G loss: 2.424316]\n",
      "epoch:7 step:7064 [D loss: 0.626011, acc: 63.28%] [G loss: 2.019040]\n",
      "epoch:7 step:7065 [D loss: 0.593913, acc: 64.06%] [G loss: 1.959167]\n",
      "epoch:7 step:7066 [D loss: 0.642789, acc: 55.47%] [G loss: 2.222613]\n",
      "epoch:7 step:7067 [D loss: 0.540716, acc: 75.78%] [G loss: 2.374716]\n",
      "epoch:7 step:7068 [D loss: 0.613940, acc: 64.84%] [G loss: 2.406798]\n",
      "epoch:7 step:7069 [D loss: 0.685739, acc: 57.81%] [G loss: 2.212410]\n",
      "epoch:7 step:7070 [D loss: 0.629912, acc: 62.50%] [G loss: 1.982884]\n",
      "epoch:7 step:7071 [D loss: 0.649223, acc: 63.28%] [G loss: 2.134419]\n",
      "epoch:7 step:7072 [D loss: 0.570892, acc: 67.19%] [G loss: 2.130334]\n",
      "epoch:7 step:7073 [D loss: 0.659491, acc: 63.28%] [G loss: 2.039514]\n",
      "epoch:7 step:7074 [D loss: 0.605264, acc: 65.62%] [G loss: 2.086162]\n",
      "epoch:7 step:7075 [D loss: 0.599930, acc: 66.41%] [G loss: 2.271656]\n",
      "epoch:7 step:7076 [D loss: 0.581067, acc: 70.31%] [G loss: 2.425123]\n",
      "epoch:7 step:7077 [D loss: 0.614783, acc: 68.75%] [G loss: 2.298980]\n",
      "epoch:7 step:7078 [D loss: 0.638545, acc: 68.75%] [G loss: 2.185974]\n",
      "epoch:7 step:7079 [D loss: 0.618207, acc: 70.31%] [G loss: 2.336172]\n",
      "epoch:7 step:7080 [D loss: 0.633436, acc: 63.28%] [G loss: 2.272191]\n",
      "epoch:7 step:7081 [D loss: 0.527782, acc: 75.00%] [G loss: 2.246896]\n",
      "epoch:7 step:7082 [D loss: 0.547225, acc: 69.53%] [G loss: 2.328637]\n",
      "epoch:7 step:7083 [D loss: 0.579990, acc: 67.19%] [G loss: 2.140225]\n",
      "epoch:7 step:7084 [D loss: 0.620455, acc: 67.97%] [G loss: 2.475152]\n",
      "epoch:7 step:7085 [D loss: 0.564575, acc: 69.53%] [G loss: 2.307048]\n",
      "epoch:7 step:7086 [D loss: 0.640907, acc: 66.41%] [G loss: 2.338579]\n",
      "epoch:7 step:7087 [D loss: 0.760391, acc: 57.03%] [G loss: 2.037609]\n",
      "epoch:7 step:7088 [D loss: 0.663548, acc: 60.16%] [G loss: 2.328768]\n",
      "epoch:7 step:7089 [D loss: 0.526689, acc: 76.56%] [G loss: 2.303307]\n",
      "epoch:7 step:7090 [D loss: 0.738383, acc: 57.03%] [G loss: 2.074668]\n",
      "epoch:7 step:7091 [D loss: 0.565432, acc: 71.09%] [G loss: 2.048856]\n",
      "epoch:7 step:7092 [D loss: 0.697641, acc: 53.12%] [G loss: 2.082657]\n",
      "epoch:7 step:7093 [D loss: 0.573270, acc: 67.97%] [G loss: 2.350570]\n",
      "epoch:7 step:7094 [D loss: 0.623638, acc: 60.16%] [G loss: 2.219687]\n",
      "epoch:7 step:7095 [D loss: 0.572711, acc: 68.75%] [G loss: 2.253496]\n",
      "epoch:7 step:7096 [D loss: 0.668430, acc: 58.59%] [G loss: 2.198696]\n",
      "epoch:7 step:7097 [D loss: 0.641340, acc: 63.28%] [G loss: 2.138794]\n",
      "epoch:7 step:7098 [D loss: 0.660001, acc: 58.59%] [G loss: 2.042681]\n",
      "epoch:7 step:7099 [D loss: 0.610502, acc: 68.75%] [G loss: 2.052199]\n",
      "epoch:7 step:7100 [D loss: 0.548467, acc: 71.09%] [G loss: 2.114810]\n",
      "epoch:7 step:7101 [D loss: 0.668519, acc: 63.28%] [G loss: 2.034532]\n",
      "epoch:7 step:7102 [D loss: 0.709117, acc: 59.38%] [G loss: 2.044017]\n",
      "epoch:7 step:7103 [D loss: 0.577255, acc: 69.53%] [G loss: 1.931060]\n",
      "epoch:7 step:7104 [D loss: 0.559381, acc: 68.75%] [G loss: 2.227820]\n",
      "epoch:7 step:7105 [D loss: 0.591309, acc: 70.31%] [G loss: 2.370915]\n",
      "epoch:7 step:7106 [D loss: 0.546817, acc: 73.44%] [G loss: 2.400290]\n",
      "epoch:7 step:7107 [D loss: 0.551014, acc: 71.09%] [G loss: 2.355882]\n",
      "epoch:7 step:7108 [D loss: 0.635669, acc: 66.41%] [G loss: 2.479495]\n",
      "epoch:7 step:7109 [D loss: 0.575119, acc: 71.09%] [G loss: 2.425329]\n",
      "epoch:7 step:7110 [D loss: 0.507362, acc: 77.34%] [G loss: 2.781503]\n",
      "epoch:7 step:7111 [D loss: 0.553305, acc: 74.22%] [G loss: 2.402962]\n",
      "epoch:7 step:7112 [D loss: 0.559182, acc: 75.78%] [G loss: 2.213524]\n",
      "epoch:7 step:7113 [D loss: 0.561782, acc: 69.53%] [G loss: 2.863906]\n",
      "epoch:7 step:7114 [D loss: 0.611002, acc: 64.84%] [G loss: 2.667089]\n",
      "epoch:7 step:7115 [D loss: 0.580960, acc: 69.53%] [G loss: 2.672675]\n",
      "epoch:7 step:7116 [D loss: 0.566057, acc: 74.22%] [G loss: 2.344470]\n",
      "epoch:7 step:7117 [D loss: 0.567340, acc: 75.78%] [G loss: 2.632417]\n",
      "epoch:7 step:7118 [D loss: 0.652171, acc: 66.41%] [G loss: 2.240708]\n",
      "epoch:7 step:7119 [D loss: 0.618239, acc: 65.62%] [G loss: 2.227945]\n",
      "epoch:7 step:7120 [D loss: 0.619348, acc: 64.06%] [G loss: 2.298907]\n",
      "epoch:7 step:7121 [D loss: 0.662396, acc: 62.50%] [G loss: 2.056141]\n",
      "epoch:7 step:7122 [D loss: 0.629276, acc: 63.28%] [G loss: 2.235945]\n",
      "epoch:7 step:7123 [D loss: 0.594613, acc: 71.88%] [G loss: 2.302281]\n",
      "epoch:7 step:7124 [D loss: 0.589942, acc: 67.19%] [G loss: 2.345692]\n",
      "epoch:7 step:7125 [D loss: 0.650832, acc: 64.06%] [G loss: 2.234615]\n",
      "epoch:7 step:7126 [D loss: 0.653942, acc: 59.38%] [G loss: 2.106663]\n",
      "epoch:7 step:7127 [D loss: 0.618820, acc: 67.19%] [G loss: 2.084063]\n",
      "epoch:7 step:7128 [D loss: 0.660004, acc: 61.72%] [G loss: 2.173922]\n",
      "epoch:7 step:7129 [D loss: 0.567477, acc: 73.44%] [G loss: 2.195008]\n",
      "epoch:7 step:7130 [D loss: 0.636880, acc: 64.84%] [G loss: 2.338538]\n",
      "epoch:7 step:7131 [D loss: 0.688658, acc: 60.94%] [G loss: 1.872912]\n",
      "epoch:7 step:7132 [D loss: 0.623338, acc: 70.31%] [G loss: 1.921033]\n",
      "epoch:7 step:7133 [D loss: 0.555618, acc: 71.09%] [G loss: 2.233876]\n",
      "epoch:7 step:7134 [D loss: 0.641090, acc: 68.75%] [G loss: 2.060962]\n",
      "epoch:7 step:7135 [D loss: 0.735201, acc: 49.22%] [G loss: 2.057870]\n",
      "epoch:7 step:7136 [D loss: 0.673793, acc: 59.38%] [G loss: 2.168552]\n",
      "epoch:7 step:7137 [D loss: 0.653157, acc: 63.28%] [G loss: 1.905355]\n",
      "epoch:7 step:7138 [D loss: 0.681708, acc: 60.16%] [G loss: 2.041430]\n",
      "epoch:7 step:7139 [D loss: 0.626705, acc: 67.97%] [G loss: 2.183146]\n",
      "epoch:7 step:7140 [D loss: 0.579134, acc: 69.53%] [G loss: 2.223022]\n",
      "epoch:7 step:7141 [D loss: 0.592072, acc: 70.31%] [G loss: 2.032900]\n",
      "epoch:7 step:7142 [D loss: 0.634477, acc: 64.84%] [G loss: 2.126323]\n",
      "epoch:7 step:7143 [D loss: 0.628575, acc: 64.84%] [G loss: 2.025302]\n",
      "epoch:7 step:7144 [D loss: 0.628777, acc: 64.06%] [G loss: 2.172920]\n",
      "epoch:7 step:7145 [D loss: 0.626610, acc: 61.72%] [G loss: 2.368759]\n",
      "epoch:7 step:7146 [D loss: 0.578528, acc: 67.97%] [G loss: 2.199679]\n",
      "epoch:7 step:7147 [D loss: 0.610251, acc: 67.97%] [G loss: 2.232144]\n",
      "epoch:7 step:7148 [D loss: 0.634917, acc: 71.09%] [G loss: 2.314481]\n",
      "epoch:7 step:7149 [D loss: 0.620345, acc: 65.62%] [G loss: 2.021084]\n",
      "epoch:7 step:7150 [D loss: 0.603835, acc: 67.97%] [G loss: 2.187831]\n",
      "epoch:7 step:7151 [D loss: 0.627898, acc: 59.38%] [G loss: 2.216501]\n",
      "epoch:7 step:7152 [D loss: 0.732374, acc: 51.56%] [G loss: 2.147517]\n",
      "epoch:7 step:7153 [D loss: 0.653308, acc: 62.50%] [G loss: 2.198100]\n",
      "epoch:7 step:7154 [D loss: 0.668973, acc: 67.19%] [G loss: 2.202282]\n",
      "epoch:7 step:7155 [D loss: 0.556837, acc: 70.31%] [G loss: 2.152205]\n",
      "epoch:7 step:7156 [D loss: 0.647891, acc: 59.38%] [G loss: 2.209298]\n",
      "epoch:7 step:7157 [D loss: 0.588533, acc: 66.41%] [G loss: 2.448791]\n",
      "epoch:7 step:7158 [D loss: 0.579690, acc: 67.97%] [G loss: 2.344824]\n",
      "epoch:7 step:7159 [D loss: 0.698335, acc: 60.94%] [G loss: 2.274109]\n",
      "epoch:7 step:7160 [D loss: 0.583600, acc: 71.09%] [G loss: 2.294446]\n",
      "epoch:7 step:7161 [D loss: 0.605504, acc: 69.53%] [G loss: 2.347003]\n",
      "epoch:7 step:7162 [D loss: 0.582679, acc: 68.75%] [G loss: 2.256962]\n",
      "epoch:7 step:7163 [D loss: 0.636739, acc: 61.72%] [G loss: 2.331627]\n",
      "epoch:7 step:7164 [D loss: 0.686426, acc: 55.47%] [G loss: 2.402219]\n",
      "epoch:7 step:7165 [D loss: 0.573114, acc: 71.09%] [G loss: 2.094440]\n",
      "epoch:7 step:7166 [D loss: 0.592873, acc: 68.75%] [G loss: 2.071940]\n",
      "epoch:7 step:7167 [D loss: 0.625355, acc: 67.19%] [G loss: 2.245192]\n",
      "epoch:7 step:7168 [D loss: 0.615560, acc: 66.41%] [G loss: 2.259701]\n",
      "epoch:7 step:7169 [D loss: 0.594932, acc: 69.53%] [G loss: 2.115782]\n",
      "epoch:7 step:7170 [D loss: 0.631401, acc: 67.19%] [G loss: 2.415917]\n",
      "epoch:7 step:7171 [D loss: 0.620149, acc: 66.41%] [G loss: 2.262955]\n",
      "epoch:7 step:7172 [D loss: 0.580322, acc: 66.41%] [G loss: 2.410561]\n",
      "epoch:7 step:7173 [D loss: 0.692371, acc: 57.03%] [G loss: 2.248266]\n",
      "epoch:7 step:7174 [D loss: 0.653263, acc: 59.38%] [G loss: 2.112722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7175 [D loss: 0.635242, acc: 64.84%] [G loss: 2.053060]\n",
      "epoch:7 step:7176 [D loss: 0.596194, acc: 72.66%] [G loss: 2.094052]\n",
      "epoch:7 step:7177 [D loss: 0.654085, acc: 64.06%] [G loss: 2.153339]\n",
      "epoch:7 step:7178 [D loss: 0.643316, acc: 60.16%] [G loss: 2.162644]\n",
      "epoch:7 step:7179 [D loss: 0.599590, acc: 67.19%] [G loss: 2.393251]\n",
      "epoch:7 step:7180 [D loss: 0.606965, acc: 67.19%] [G loss: 2.040189]\n",
      "epoch:7 step:7181 [D loss: 0.635143, acc: 63.28%] [G loss: 2.421599]\n",
      "epoch:7 step:7182 [D loss: 0.671445, acc: 57.81%] [G loss: 2.242651]\n",
      "epoch:7 step:7183 [D loss: 0.572533, acc: 70.31%] [G loss: 2.183273]\n",
      "epoch:7 step:7184 [D loss: 0.615410, acc: 64.84%] [G loss: 2.273407]\n",
      "epoch:7 step:7185 [D loss: 0.621265, acc: 66.41%] [G loss: 2.314723]\n",
      "epoch:7 step:7186 [D loss: 0.596644, acc: 71.09%] [G loss: 2.055503]\n",
      "epoch:7 step:7187 [D loss: 0.612814, acc: 67.19%] [G loss: 2.201375]\n",
      "epoch:7 step:7188 [D loss: 0.586824, acc: 68.75%] [G loss: 2.388410]\n",
      "epoch:7 step:7189 [D loss: 0.629483, acc: 66.41%] [G loss: 2.308491]\n",
      "epoch:7 step:7190 [D loss: 0.585786, acc: 68.75%] [G loss: 2.260446]\n",
      "epoch:7 step:7191 [D loss: 0.602968, acc: 71.09%] [G loss: 2.272609]\n",
      "epoch:7 step:7192 [D loss: 0.572068, acc: 71.09%] [G loss: 2.327987]\n",
      "epoch:7 step:7193 [D loss: 0.611018, acc: 66.41%] [G loss: 2.415520]\n",
      "epoch:7 step:7194 [D loss: 0.611829, acc: 70.31%] [G loss: 2.253370]\n",
      "epoch:7 step:7195 [D loss: 0.623899, acc: 61.72%] [G loss: 2.217929]\n",
      "epoch:7 step:7196 [D loss: 0.601667, acc: 71.09%] [G loss: 2.481631]\n",
      "epoch:7 step:7197 [D loss: 0.614240, acc: 60.94%] [G loss: 2.398693]\n",
      "epoch:7 step:7198 [D loss: 0.574947, acc: 73.44%] [G loss: 2.515123]\n",
      "epoch:7 step:7199 [D loss: 0.595199, acc: 68.75%] [G loss: 2.460252]\n",
      "epoch:7 step:7200 [D loss: 0.566342, acc: 73.44%] [G loss: 2.440361]\n",
      "##############\n",
      "[2.57047505 1.55796217 6.6587083  5.14184564 4.00313983 6.04927411\n",
      " 4.62838231 4.96859888 5.11407967 3.70343397]\n",
      "##########\n",
      "epoch:7 step:7201 [D loss: 0.550136, acc: 71.88%] [G loss: 2.455487]\n",
      "epoch:7 step:7202 [D loss: 0.690577, acc: 60.94%] [G loss: 2.117794]\n",
      "epoch:7 step:7203 [D loss: 0.603073, acc: 64.84%] [G loss: 2.377163]\n",
      "epoch:7 step:7204 [D loss: 0.618164, acc: 66.41%] [G loss: 2.135798]\n",
      "epoch:7 step:7205 [D loss: 0.672361, acc: 60.94%] [G loss: 2.189278]\n",
      "epoch:7 step:7206 [D loss: 0.597267, acc: 68.75%] [G loss: 2.437508]\n",
      "epoch:7 step:7207 [D loss: 0.556624, acc: 73.44%] [G loss: 2.459187]\n",
      "epoch:7 step:7208 [D loss: 0.517048, acc: 75.00%] [G loss: 2.554404]\n",
      "epoch:7 step:7209 [D loss: 0.549536, acc: 67.19%] [G loss: 2.697459]\n",
      "epoch:7 step:7210 [D loss: 0.618599, acc: 67.97%] [G loss: 2.078643]\n",
      "epoch:7 step:7211 [D loss: 0.695312, acc: 57.03%] [G loss: 2.075078]\n",
      "epoch:7 step:7212 [D loss: 0.614335, acc: 68.75%] [G loss: 2.284152]\n",
      "epoch:7 step:7213 [D loss: 0.539719, acc: 73.44%] [G loss: 2.382570]\n",
      "epoch:7 step:7214 [D loss: 0.598938, acc: 66.41%] [G loss: 2.366797]\n",
      "epoch:7 step:7215 [D loss: 0.617598, acc: 69.53%] [G loss: 2.425316]\n",
      "epoch:7 step:7216 [D loss: 0.624475, acc: 63.28%] [G loss: 2.225682]\n",
      "epoch:7 step:7217 [D loss: 0.711864, acc: 58.59%] [G loss: 1.977664]\n",
      "epoch:7 step:7218 [D loss: 0.615809, acc: 66.41%] [G loss: 2.116201]\n",
      "epoch:7 step:7219 [D loss: 0.588411, acc: 67.97%] [G loss: 2.110067]\n",
      "epoch:7 step:7220 [D loss: 0.648784, acc: 64.06%] [G loss: 2.196270]\n",
      "epoch:7 step:7221 [D loss: 0.643526, acc: 63.28%] [G loss: 2.245186]\n",
      "epoch:7 step:7222 [D loss: 0.595680, acc: 69.53%] [G loss: 2.270849]\n",
      "epoch:7 step:7223 [D loss: 0.591893, acc: 74.22%] [G loss: 2.226796]\n",
      "epoch:7 step:7224 [D loss: 0.648770, acc: 60.16%] [G loss: 2.259818]\n",
      "epoch:7 step:7225 [D loss: 0.646313, acc: 63.28%] [G loss: 2.182330]\n",
      "epoch:7 step:7226 [D loss: 0.643540, acc: 61.72%] [G loss: 2.095836]\n",
      "epoch:7 step:7227 [D loss: 0.612061, acc: 64.06%] [G loss: 2.423838]\n",
      "epoch:7 step:7228 [D loss: 0.653812, acc: 64.84%] [G loss: 2.004964]\n",
      "epoch:7 step:7229 [D loss: 0.649491, acc: 67.19%] [G loss: 2.116178]\n",
      "epoch:7 step:7230 [D loss: 0.586396, acc: 67.97%] [G loss: 2.033741]\n",
      "epoch:7 step:7231 [D loss: 0.606993, acc: 67.19%] [G loss: 2.348738]\n",
      "epoch:7 step:7232 [D loss: 0.648733, acc: 63.28%] [G loss: 2.455791]\n",
      "epoch:7 step:7233 [D loss: 0.629313, acc: 66.41%] [G loss: 2.142270]\n",
      "epoch:7 step:7234 [D loss: 0.689442, acc: 56.25%] [G loss: 2.021593]\n",
      "epoch:7 step:7235 [D loss: 0.611839, acc: 66.41%] [G loss: 2.263494]\n",
      "epoch:7 step:7236 [D loss: 0.609603, acc: 65.62%] [G loss: 2.284942]\n",
      "epoch:7 step:7237 [D loss: 0.624961, acc: 67.19%] [G loss: 2.267584]\n",
      "epoch:7 step:7238 [D loss: 0.631796, acc: 66.41%] [G loss: 2.381101]\n",
      "epoch:7 step:7239 [D loss: 0.622411, acc: 64.06%] [G loss: 2.360426]\n",
      "epoch:7 step:7240 [D loss: 0.585006, acc: 72.66%] [G loss: 2.204560]\n",
      "epoch:7 step:7241 [D loss: 0.657553, acc: 62.50%] [G loss: 2.252578]\n",
      "epoch:7 step:7242 [D loss: 0.666848, acc: 58.59%] [G loss: 2.041095]\n",
      "epoch:7 step:7243 [D loss: 0.584732, acc: 67.97%] [G loss: 2.214882]\n",
      "epoch:7 step:7244 [D loss: 0.604410, acc: 68.75%] [G loss: 2.227248]\n",
      "epoch:7 step:7245 [D loss: 0.635505, acc: 64.84%] [G loss: 2.211337]\n",
      "epoch:7 step:7246 [D loss: 0.629267, acc: 64.84%] [G loss: 2.118867]\n",
      "epoch:7 step:7247 [D loss: 0.662259, acc: 56.25%] [G loss: 2.336310]\n",
      "epoch:7 step:7248 [D loss: 0.571068, acc: 75.78%] [G loss: 2.488828]\n",
      "epoch:7 step:7249 [D loss: 0.539909, acc: 76.56%] [G loss: 2.681148]\n",
      "epoch:7 step:7250 [D loss: 0.621742, acc: 69.53%] [G loss: 2.455548]\n",
      "epoch:7 step:7251 [D loss: 0.590080, acc: 70.31%] [G loss: 2.828611]\n",
      "epoch:7 step:7252 [D loss: 0.583124, acc: 70.31%] [G loss: 2.560274]\n",
      "epoch:7 step:7253 [D loss: 0.562824, acc: 68.75%] [G loss: 2.668678]\n",
      "epoch:7 step:7254 [D loss: 0.548380, acc: 74.22%] [G loss: 2.432314]\n",
      "epoch:7 step:7255 [D loss: 0.555788, acc: 70.31%] [G loss: 2.190889]\n",
      "epoch:7 step:7256 [D loss: 0.661562, acc: 63.28%] [G loss: 2.243818]\n",
      "epoch:7 step:7257 [D loss: 0.676180, acc: 57.81%] [G loss: 2.170789]\n",
      "epoch:7 step:7258 [D loss: 0.546826, acc: 71.09%] [G loss: 2.718986]\n",
      "epoch:7 step:7259 [D loss: 0.582997, acc: 70.31%] [G loss: 2.394471]\n",
      "epoch:7 step:7260 [D loss: 0.617203, acc: 64.06%] [G loss: 2.447376]\n",
      "epoch:7 step:7261 [D loss: 0.701303, acc: 58.59%] [G loss: 2.033519]\n",
      "epoch:7 step:7262 [D loss: 0.662628, acc: 63.28%] [G loss: 2.167947]\n",
      "epoch:7 step:7263 [D loss: 0.666230, acc: 57.81%] [G loss: 2.168951]\n",
      "epoch:7 step:7264 [D loss: 0.612856, acc: 64.84%] [G loss: 2.176167]\n",
      "epoch:7 step:7265 [D loss: 0.598779, acc: 65.62%] [G loss: 2.262392]\n",
      "epoch:7 step:7266 [D loss: 0.572373, acc: 71.09%] [G loss: 2.513722]\n",
      "epoch:7 step:7267 [D loss: 0.578693, acc: 64.06%] [G loss: 2.404557]\n",
      "epoch:7 step:7268 [D loss: 0.595514, acc: 67.97%] [G loss: 2.334021]\n",
      "epoch:7 step:7269 [D loss: 0.607506, acc: 62.50%] [G loss: 2.111385]\n",
      "epoch:7 step:7270 [D loss: 0.652292, acc: 63.28%] [G loss: 2.260150]\n",
      "epoch:7 step:7271 [D loss: 0.582733, acc: 71.09%] [G loss: 2.276105]\n",
      "epoch:7 step:7272 [D loss: 0.612445, acc: 67.97%] [G loss: 2.119754]\n",
      "epoch:7 step:7273 [D loss: 0.573819, acc: 70.31%] [G loss: 2.412976]\n",
      "epoch:7 step:7274 [D loss: 0.627904, acc: 61.72%] [G loss: 2.162230]\n",
      "epoch:7 step:7275 [D loss: 0.625757, acc: 64.06%] [G loss: 2.006645]\n",
      "epoch:7 step:7276 [D loss: 0.618048, acc: 63.28%] [G loss: 2.065166]\n",
      "epoch:7 step:7277 [D loss: 0.650733, acc: 57.81%] [G loss: 2.074079]\n",
      "epoch:7 step:7278 [D loss: 0.636914, acc: 63.28%] [G loss: 2.414383]\n",
      "epoch:7 step:7279 [D loss: 0.641360, acc: 62.50%] [G loss: 2.083383]\n",
      "epoch:7 step:7280 [D loss: 0.570883, acc: 71.09%] [G loss: 2.463089]\n",
      "epoch:7 step:7281 [D loss: 0.661709, acc: 58.59%] [G loss: 2.145899]\n",
      "epoch:7 step:7282 [D loss: 0.642327, acc: 59.38%] [G loss: 2.077918]\n",
      "epoch:7 step:7283 [D loss: 0.580650, acc: 69.53%] [G loss: 2.200313]\n",
      "epoch:7 step:7284 [D loss: 0.732230, acc: 57.03%] [G loss: 2.294562]\n",
      "epoch:7 step:7285 [D loss: 0.611037, acc: 68.75%] [G loss: 2.316414]\n",
      "epoch:7 step:7286 [D loss: 0.654168, acc: 67.97%] [G loss: 2.034763]\n",
      "epoch:7 step:7287 [D loss: 0.640637, acc: 60.94%] [G loss: 2.324292]\n",
      "epoch:7 step:7288 [D loss: 0.663227, acc: 60.16%] [G loss: 2.090636]\n",
      "epoch:7 step:7289 [D loss: 0.588950, acc: 64.06%] [G loss: 2.182406]\n",
      "epoch:7 step:7290 [D loss: 0.630130, acc: 62.50%] [G loss: 2.146885]\n",
      "epoch:7 step:7291 [D loss: 0.609170, acc: 67.97%] [G loss: 2.214614]\n",
      "epoch:7 step:7292 [D loss: 0.625453, acc: 62.50%] [G loss: 2.145945]\n",
      "epoch:7 step:7293 [D loss: 0.639622, acc: 67.19%] [G loss: 1.937411]\n",
      "epoch:7 step:7294 [D loss: 0.609372, acc: 71.09%] [G loss: 2.278887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7295 [D loss: 0.574900, acc: 71.09%] [G loss: 2.255728]\n",
      "epoch:7 step:7296 [D loss: 0.655297, acc: 64.06%] [G loss: 2.033992]\n",
      "epoch:7 step:7297 [D loss: 0.635903, acc: 60.16%] [G loss: 2.165833]\n",
      "epoch:7 step:7298 [D loss: 0.644025, acc: 63.28%] [G loss: 1.949562]\n",
      "epoch:7 step:7299 [D loss: 0.580945, acc: 72.66%] [G loss: 2.050530]\n",
      "epoch:7 step:7300 [D loss: 0.612106, acc: 62.50%] [G loss: 2.349076]\n",
      "epoch:7 step:7301 [D loss: 0.627348, acc: 63.28%] [G loss: 2.186524]\n",
      "epoch:7 step:7302 [D loss: 0.601497, acc: 64.84%] [G loss: 2.382297]\n",
      "epoch:7 step:7303 [D loss: 0.674696, acc: 67.19%] [G loss: 2.010005]\n",
      "epoch:7 step:7304 [D loss: 0.573462, acc: 74.22%] [G loss: 2.311927]\n",
      "epoch:7 step:7305 [D loss: 0.606896, acc: 71.09%] [G loss: 2.475312]\n",
      "epoch:7 step:7306 [D loss: 0.594106, acc: 69.53%] [G loss: 2.483553]\n",
      "epoch:7 step:7307 [D loss: 0.633154, acc: 60.94%] [G loss: 2.263274]\n",
      "epoch:7 step:7308 [D loss: 0.612767, acc: 62.50%] [G loss: 1.995922]\n",
      "epoch:7 step:7309 [D loss: 0.659448, acc: 58.59%] [G loss: 2.290523]\n",
      "epoch:7 step:7310 [D loss: 0.669137, acc: 64.06%] [G loss: 2.064997]\n",
      "epoch:7 step:7311 [D loss: 0.652194, acc: 63.28%] [G loss: 2.121705]\n",
      "epoch:7 step:7312 [D loss: 0.687973, acc: 67.19%] [G loss: 2.045032]\n",
      "epoch:7 step:7313 [D loss: 0.556401, acc: 72.66%] [G loss: 2.194146]\n",
      "epoch:7 step:7314 [D loss: 0.612641, acc: 66.41%] [G loss: 1.991320]\n",
      "epoch:7 step:7315 [D loss: 0.602079, acc: 66.41%] [G loss: 2.177436]\n",
      "epoch:7 step:7316 [D loss: 0.628300, acc: 58.59%] [G loss: 2.076617]\n",
      "epoch:7 step:7317 [D loss: 0.632101, acc: 64.06%] [G loss: 2.064306]\n",
      "epoch:7 step:7318 [D loss: 0.652747, acc: 60.16%] [G loss: 2.138531]\n",
      "epoch:7 step:7319 [D loss: 0.556972, acc: 74.22%] [G loss: 2.143897]\n",
      "epoch:7 step:7320 [D loss: 0.551854, acc: 75.78%] [G loss: 2.148260]\n",
      "epoch:7 step:7321 [D loss: 0.616445, acc: 70.31%] [G loss: 2.182567]\n",
      "epoch:7 step:7322 [D loss: 0.585579, acc: 72.66%] [G loss: 2.214646]\n",
      "epoch:7 step:7323 [D loss: 0.651512, acc: 60.16%] [G loss: 2.181094]\n",
      "epoch:7 step:7324 [D loss: 0.655908, acc: 64.06%] [G loss: 2.109587]\n",
      "epoch:7 step:7325 [D loss: 0.662785, acc: 60.94%] [G loss: 2.067292]\n",
      "epoch:7 step:7326 [D loss: 0.581497, acc: 66.41%] [G loss: 2.162969]\n",
      "epoch:7 step:7327 [D loss: 0.660127, acc: 62.50%] [G loss: 1.932783]\n",
      "epoch:7 step:7328 [D loss: 0.589748, acc: 69.53%] [G loss: 2.345015]\n",
      "epoch:7 step:7329 [D loss: 0.632143, acc: 64.84%] [G loss: 2.183540]\n",
      "epoch:7 step:7330 [D loss: 0.585183, acc: 69.53%] [G loss: 2.034624]\n",
      "epoch:7 step:7331 [D loss: 0.646207, acc: 62.50%] [G loss: 2.301275]\n",
      "epoch:7 step:7332 [D loss: 0.668223, acc: 62.50%] [G loss: 2.084430]\n",
      "epoch:7 step:7333 [D loss: 0.681202, acc: 59.38%] [G loss: 2.162328]\n",
      "epoch:7 step:7334 [D loss: 0.564366, acc: 70.31%] [G loss: 2.224903]\n",
      "epoch:7 step:7335 [D loss: 0.615937, acc: 66.41%] [G loss: 2.364924]\n",
      "epoch:7 step:7336 [D loss: 0.550000, acc: 71.88%] [G loss: 2.477551]\n",
      "epoch:7 step:7337 [D loss: 0.604497, acc: 63.28%] [G loss: 2.169324]\n",
      "epoch:7 step:7338 [D loss: 0.614787, acc: 65.62%] [G loss: 2.119011]\n",
      "epoch:7 step:7339 [D loss: 0.599282, acc: 65.62%] [G loss: 2.336739]\n",
      "epoch:7 step:7340 [D loss: 0.521540, acc: 72.66%] [G loss: 2.419170]\n",
      "epoch:7 step:7341 [D loss: 0.548554, acc: 72.66%] [G loss: 2.402731]\n",
      "epoch:7 step:7342 [D loss: 0.634833, acc: 61.72%] [G loss: 2.337344]\n",
      "epoch:7 step:7343 [D loss: 0.659650, acc: 62.50%] [G loss: 2.234239]\n",
      "epoch:7 step:7344 [D loss: 0.647658, acc: 66.41%] [G loss: 2.357954]\n",
      "epoch:7 step:7345 [D loss: 0.526523, acc: 74.22%] [G loss: 2.657768]\n",
      "epoch:7 step:7346 [D loss: 0.689837, acc: 56.25%] [G loss: 2.235106]\n",
      "epoch:7 step:7347 [D loss: 0.575939, acc: 71.88%] [G loss: 2.167169]\n",
      "epoch:7 step:7348 [D loss: 0.646055, acc: 60.16%] [G loss: 2.073182]\n",
      "epoch:7 step:7349 [D loss: 0.667876, acc: 61.72%] [G loss: 2.274608]\n",
      "epoch:7 step:7350 [D loss: 0.608650, acc: 65.62%] [G loss: 2.067406]\n",
      "epoch:7 step:7351 [D loss: 0.588398, acc: 71.09%] [G loss: 2.552674]\n",
      "epoch:7 step:7352 [D loss: 0.651013, acc: 60.94%] [G loss: 2.151600]\n",
      "epoch:7 step:7353 [D loss: 0.692592, acc: 58.59%] [G loss: 1.998937]\n",
      "epoch:7 step:7354 [D loss: 0.610027, acc: 71.09%] [G loss: 2.118809]\n",
      "epoch:7 step:7355 [D loss: 0.629927, acc: 64.84%] [G loss: 2.216964]\n",
      "epoch:7 step:7356 [D loss: 0.705005, acc: 54.69%] [G loss: 2.130472]\n",
      "epoch:7 step:7357 [D loss: 0.669694, acc: 60.94%] [G loss: 2.204046]\n",
      "epoch:7 step:7358 [D loss: 0.648454, acc: 62.50%] [G loss: 1.876488]\n",
      "epoch:7 step:7359 [D loss: 0.683261, acc: 60.94%] [G loss: 1.967070]\n",
      "epoch:7 step:7360 [D loss: 0.676079, acc: 59.38%] [G loss: 2.170177]\n",
      "epoch:7 step:7361 [D loss: 0.617595, acc: 70.31%] [G loss: 2.204513]\n",
      "epoch:7 step:7362 [D loss: 0.649624, acc: 67.19%] [G loss: 2.126817]\n",
      "epoch:7 step:7363 [D loss: 0.650894, acc: 61.72%] [G loss: 2.170518]\n",
      "epoch:7 step:7364 [D loss: 0.606737, acc: 67.19%] [G loss: 2.457996]\n",
      "epoch:7 step:7365 [D loss: 0.622165, acc: 63.28%] [G loss: 2.517549]\n",
      "epoch:7 step:7366 [D loss: 0.587642, acc: 72.66%] [G loss: 2.210526]\n",
      "epoch:7 step:7367 [D loss: 0.641759, acc: 67.19%] [G loss: 2.209507]\n",
      "epoch:7 step:7368 [D loss: 0.627678, acc: 66.41%] [G loss: 2.406128]\n",
      "epoch:7 step:7369 [D loss: 0.551915, acc: 71.09%] [G loss: 2.145268]\n",
      "epoch:7 step:7370 [D loss: 0.586746, acc: 69.53%] [G loss: 2.152878]\n",
      "epoch:7 step:7371 [D loss: 0.658045, acc: 63.28%] [G loss: 1.835220]\n",
      "epoch:7 step:7372 [D loss: 0.663409, acc: 60.16%] [G loss: 2.198391]\n",
      "epoch:7 step:7373 [D loss: 0.626833, acc: 68.75%] [G loss: 2.299407]\n",
      "epoch:7 step:7374 [D loss: 0.557128, acc: 72.66%] [G loss: 2.448759]\n",
      "epoch:7 step:7375 [D loss: 0.626619, acc: 68.75%] [G loss: 2.426216]\n",
      "epoch:7 step:7376 [D loss: 0.602316, acc: 67.19%] [G loss: 2.268809]\n",
      "epoch:7 step:7377 [D loss: 0.617074, acc: 64.06%] [G loss: 2.136695]\n",
      "epoch:7 step:7378 [D loss: 0.633581, acc: 64.06%] [G loss: 2.087944]\n",
      "epoch:7 step:7379 [D loss: 0.686772, acc: 60.94%] [G loss: 2.081370]\n",
      "epoch:7 step:7380 [D loss: 0.637243, acc: 67.19%] [G loss: 2.146507]\n",
      "epoch:7 step:7381 [D loss: 0.589896, acc: 66.41%] [G loss: 2.284363]\n",
      "epoch:7 step:7382 [D loss: 0.575089, acc: 70.31%] [G loss: 2.366605]\n",
      "epoch:7 step:7383 [D loss: 0.653961, acc: 60.94%] [G loss: 2.146713]\n",
      "epoch:7 step:7384 [D loss: 0.598124, acc: 67.19%] [G loss: 2.564480]\n",
      "epoch:7 step:7385 [D loss: 0.642600, acc: 62.50%] [G loss: 1.975114]\n",
      "epoch:7 step:7386 [D loss: 0.697502, acc: 60.16%] [G loss: 1.861700]\n",
      "epoch:7 step:7387 [D loss: 0.647820, acc: 57.81%] [G loss: 1.915119]\n",
      "epoch:7 step:7388 [D loss: 0.662283, acc: 59.38%] [G loss: 2.007087]\n",
      "epoch:7 step:7389 [D loss: 0.602179, acc: 68.75%] [G loss: 2.088844]\n",
      "epoch:7 step:7390 [D loss: 0.556583, acc: 70.31%] [G loss: 2.396962]\n",
      "epoch:7 step:7391 [D loss: 0.629129, acc: 65.62%] [G loss: 2.164722]\n",
      "epoch:7 step:7392 [D loss: 0.593797, acc: 69.53%] [G loss: 2.191314]\n",
      "epoch:7 step:7393 [D loss: 0.528021, acc: 76.56%] [G loss: 2.355041]\n",
      "epoch:7 step:7394 [D loss: 0.604813, acc: 64.06%] [G loss: 2.049217]\n",
      "epoch:7 step:7395 [D loss: 0.567739, acc: 72.66%] [G loss: 2.115538]\n",
      "epoch:7 step:7396 [D loss: 0.608482, acc: 66.41%] [G loss: 2.438488]\n",
      "epoch:7 step:7397 [D loss: 0.682556, acc: 57.81%] [G loss: 2.330259]\n",
      "epoch:7 step:7398 [D loss: 0.653591, acc: 59.38%] [G loss: 2.147960]\n",
      "epoch:7 step:7399 [D loss: 0.601051, acc: 69.53%] [G loss: 2.266984]\n",
      "epoch:7 step:7400 [D loss: 0.591559, acc: 66.41%] [G loss: 2.469064]\n",
      "##############\n",
      "[2.56380696 1.41261176 6.7143103  5.17022489 4.07013112 5.96130007\n",
      " 4.75807842 4.84136839 5.10892003 3.78509455]\n",
      "##########\n",
      "epoch:7 step:7401 [D loss: 0.607707, acc: 67.19%] [G loss: 2.336082]\n",
      "epoch:7 step:7402 [D loss: 0.637852, acc: 62.50%] [G loss: 2.296437]\n",
      "epoch:7 step:7403 [D loss: 0.557972, acc: 73.44%] [G loss: 2.362506]\n",
      "epoch:7 step:7404 [D loss: 0.663368, acc: 59.38%] [G loss: 2.492456]\n",
      "epoch:7 step:7405 [D loss: 0.645720, acc: 64.06%] [G loss: 2.069836]\n",
      "epoch:7 step:7406 [D loss: 0.636694, acc: 63.28%] [G loss: 2.135813]\n",
      "epoch:7 step:7407 [D loss: 0.535926, acc: 74.22%] [G loss: 2.222120]\n",
      "epoch:7 step:7408 [D loss: 0.598648, acc: 65.62%] [G loss: 2.287825]\n",
      "epoch:7 step:7409 [D loss: 0.626483, acc: 60.94%] [G loss: 2.387335]\n",
      "epoch:7 step:7410 [D loss: 0.691866, acc: 57.81%] [G loss: 2.112164]\n",
      "epoch:7 step:7411 [D loss: 0.665164, acc: 63.28%] [G loss: 2.339737]\n",
      "epoch:7 step:7412 [D loss: 0.581704, acc: 66.41%] [G loss: 2.399458]\n",
      "epoch:7 step:7413 [D loss: 0.594212, acc: 64.84%] [G loss: 2.213732]\n",
      "epoch:7 step:7414 [D loss: 0.587375, acc: 66.41%] [G loss: 2.256839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7415 [D loss: 0.698239, acc: 57.03%] [G loss: 2.052385]\n",
      "epoch:7 step:7416 [D loss: 0.552915, acc: 73.44%] [G loss: 2.395952]\n",
      "epoch:7 step:7417 [D loss: 0.691963, acc: 57.03%] [G loss: 1.971929]\n",
      "epoch:7 step:7418 [D loss: 0.629244, acc: 65.62%] [G loss: 2.280628]\n",
      "epoch:7 step:7419 [D loss: 0.604910, acc: 66.41%] [G loss: 2.271142]\n",
      "epoch:7 step:7420 [D loss: 0.605489, acc: 67.19%] [G loss: 2.127514]\n",
      "epoch:7 step:7421 [D loss: 0.585991, acc: 67.19%] [G loss: 2.154901]\n",
      "epoch:7 step:7422 [D loss: 0.573596, acc: 71.88%] [G loss: 2.362515]\n",
      "epoch:7 step:7423 [D loss: 0.734554, acc: 56.25%] [G loss: 2.286737]\n",
      "epoch:7 step:7424 [D loss: 0.592631, acc: 66.41%] [G loss: 2.149515]\n",
      "epoch:7 step:7425 [D loss: 0.587042, acc: 71.09%] [G loss: 2.444679]\n",
      "epoch:7 step:7426 [D loss: 0.657930, acc: 56.25%] [G loss: 2.195068]\n",
      "epoch:7 step:7427 [D loss: 0.594844, acc: 69.53%] [G loss: 2.148325]\n",
      "epoch:7 step:7428 [D loss: 0.697465, acc: 65.62%] [G loss: 2.233072]\n",
      "epoch:7 step:7429 [D loss: 0.618780, acc: 66.41%] [G loss: 2.268579]\n",
      "epoch:7 step:7430 [D loss: 0.669738, acc: 55.47%] [G loss: 2.051758]\n",
      "epoch:7 step:7431 [D loss: 0.614220, acc: 68.75%] [G loss: 2.163481]\n",
      "epoch:7 step:7432 [D loss: 0.625192, acc: 67.19%] [G loss: 2.116734]\n",
      "epoch:7 step:7433 [D loss: 0.659260, acc: 59.38%] [G loss: 2.263139]\n",
      "epoch:7 step:7434 [D loss: 0.605402, acc: 67.97%] [G loss: 2.317310]\n",
      "epoch:7 step:7435 [D loss: 0.660327, acc: 63.28%] [G loss: 2.158727]\n",
      "epoch:7 step:7436 [D loss: 0.604739, acc: 69.53%] [G loss: 2.167524]\n",
      "epoch:7 step:7437 [D loss: 0.675049, acc: 61.72%] [G loss: 2.310387]\n",
      "epoch:7 step:7438 [D loss: 0.625019, acc: 64.84%] [G loss: 2.159290]\n",
      "epoch:7 step:7439 [D loss: 0.651549, acc: 57.81%] [G loss: 2.109458]\n",
      "epoch:7 step:7440 [D loss: 0.641543, acc: 66.41%] [G loss: 2.059635]\n",
      "epoch:7 step:7441 [D loss: 0.622525, acc: 65.62%] [G loss: 2.074355]\n",
      "epoch:7 step:7442 [D loss: 0.626160, acc: 67.19%] [G loss: 2.129984]\n",
      "epoch:7 step:7443 [D loss: 0.591080, acc: 64.84%] [G loss: 2.469453]\n",
      "epoch:7 step:7444 [D loss: 0.621578, acc: 61.72%] [G loss: 2.262251]\n",
      "epoch:7 step:7445 [D loss: 0.543121, acc: 69.53%] [G loss: 2.610226]\n",
      "epoch:7 step:7446 [D loss: 0.612386, acc: 61.72%] [G loss: 2.178990]\n",
      "epoch:7 step:7447 [D loss: 0.538663, acc: 75.00%] [G loss: 2.310852]\n",
      "epoch:7 step:7448 [D loss: 0.592333, acc: 67.97%] [G loss: 2.444910]\n",
      "epoch:7 step:7449 [D loss: 0.553467, acc: 71.88%] [G loss: 2.415154]\n",
      "epoch:7 step:7450 [D loss: 0.789017, acc: 56.25%] [G loss: 2.123975]\n",
      "epoch:7 step:7451 [D loss: 0.660837, acc: 62.50%] [G loss: 2.010119]\n",
      "epoch:7 step:7452 [D loss: 0.572994, acc: 66.41%] [G loss: 2.112050]\n",
      "epoch:7 step:7453 [D loss: 0.649209, acc: 60.94%] [G loss: 2.171566]\n",
      "epoch:7 step:7454 [D loss: 0.633108, acc: 60.16%] [G loss: 2.378797]\n",
      "epoch:7 step:7455 [D loss: 0.618701, acc: 69.53%] [G loss: 2.157435]\n",
      "epoch:7 step:7456 [D loss: 0.610435, acc: 68.75%] [G loss: 2.243014]\n",
      "epoch:7 step:7457 [D loss: 0.609431, acc: 66.41%] [G loss: 2.428550]\n",
      "epoch:7 step:7458 [D loss: 0.619428, acc: 67.97%] [G loss: 2.398364]\n",
      "epoch:7 step:7459 [D loss: 0.642350, acc: 65.62%] [G loss: 2.133861]\n",
      "epoch:7 step:7460 [D loss: 0.573879, acc: 71.09%] [G loss: 2.265367]\n",
      "epoch:7 step:7461 [D loss: 0.616616, acc: 68.75%] [G loss: 2.223242]\n",
      "epoch:7 step:7462 [D loss: 0.686467, acc: 62.50%] [G loss: 2.130881]\n",
      "epoch:7 step:7463 [D loss: 0.595052, acc: 63.28%] [G loss: 2.159651]\n",
      "epoch:7 step:7464 [D loss: 0.617796, acc: 66.41%] [G loss: 2.234222]\n",
      "epoch:7 step:7465 [D loss: 0.643534, acc: 64.06%] [G loss: 2.301188]\n",
      "epoch:7 step:7466 [D loss: 0.619947, acc: 64.84%] [G loss: 2.294618]\n",
      "epoch:7 step:7467 [D loss: 0.636699, acc: 62.50%] [G loss: 2.309598]\n",
      "epoch:7 step:7468 [D loss: 0.635130, acc: 70.31%] [G loss: 2.483413]\n",
      "epoch:7 step:7469 [D loss: 0.638981, acc: 64.06%] [G loss: 2.372236]\n",
      "epoch:7 step:7470 [D loss: 0.582686, acc: 69.53%] [G loss: 2.606040]\n",
      "epoch:7 step:7471 [D loss: 0.585997, acc: 67.97%] [G loss: 2.782429]\n",
      "epoch:7 step:7472 [D loss: 0.601524, acc: 65.62%] [G loss: 2.590707]\n",
      "epoch:7 step:7473 [D loss: 0.663940, acc: 59.38%] [G loss: 2.242693]\n",
      "epoch:7 step:7474 [D loss: 0.647009, acc: 60.94%] [G loss: 2.281179]\n",
      "epoch:7 step:7475 [D loss: 0.640191, acc: 62.50%] [G loss: 2.287224]\n",
      "epoch:7 step:7476 [D loss: 0.648676, acc: 66.41%] [G loss: 2.358483]\n",
      "epoch:7 step:7477 [D loss: 0.571422, acc: 74.22%] [G loss: 2.452601]\n",
      "epoch:7 step:7478 [D loss: 0.561495, acc: 67.97%] [G loss: 2.357401]\n",
      "epoch:7 step:7479 [D loss: 0.713788, acc: 56.25%] [G loss: 2.112180]\n",
      "epoch:7 step:7480 [D loss: 0.627167, acc: 59.38%] [G loss: 2.185515]\n",
      "epoch:7 step:7481 [D loss: 0.587767, acc: 71.09%] [G loss: 2.288477]\n",
      "epoch:7 step:7482 [D loss: 0.517754, acc: 74.22%] [G loss: 2.348104]\n",
      "epoch:7 step:7483 [D loss: 0.536674, acc: 79.69%] [G loss: 2.527446]\n",
      "epoch:7 step:7484 [D loss: 0.573586, acc: 71.88%] [G loss: 2.440277]\n",
      "epoch:7 step:7485 [D loss: 0.586560, acc: 67.97%] [G loss: 2.634555]\n",
      "epoch:7 step:7486 [D loss: 0.580599, acc: 71.88%] [G loss: 2.557870]\n",
      "epoch:7 step:7487 [D loss: 0.856712, acc: 52.34%] [G loss: 2.096632]\n",
      "epoch:7 step:7488 [D loss: 0.671830, acc: 62.50%] [G loss: 2.323285]\n",
      "epoch:7 step:7489 [D loss: 0.573151, acc: 73.44%] [G loss: 2.380295]\n",
      "epoch:7 step:7490 [D loss: 0.619657, acc: 62.50%] [G loss: 2.296069]\n",
      "epoch:7 step:7491 [D loss: 0.650205, acc: 66.41%] [G loss: 2.086960]\n",
      "epoch:7 step:7492 [D loss: 0.741193, acc: 54.69%] [G loss: 2.112588]\n",
      "epoch:7 step:7493 [D loss: 0.589546, acc: 70.31%] [G loss: 2.189372]\n",
      "epoch:7 step:7494 [D loss: 0.624545, acc: 63.28%] [G loss: 2.352746]\n",
      "epoch:7 step:7495 [D loss: 0.527836, acc: 75.78%] [G loss: 2.451389]\n",
      "epoch:7 step:7496 [D loss: 0.565374, acc: 71.88%] [G loss: 2.833672]\n",
      "epoch:8 step:7497 [D loss: 0.644403, acc: 63.28%] [G loss: 2.315241]\n",
      "epoch:8 step:7498 [D loss: 0.644322, acc: 65.62%] [G loss: 2.392836]\n",
      "epoch:8 step:7499 [D loss: 0.625736, acc: 64.84%] [G loss: 2.039341]\n",
      "epoch:8 step:7500 [D loss: 0.612079, acc: 67.97%] [G loss: 2.175903]\n",
      "epoch:8 step:7501 [D loss: 0.626314, acc: 63.28%] [G loss: 2.279455]\n",
      "epoch:8 step:7502 [D loss: 0.605389, acc: 66.41%] [G loss: 2.241174]\n",
      "epoch:8 step:7503 [D loss: 0.562584, acc: 70.31%] [G loss: 2.312981]\n",
      "epoch:8 step:7504 [D loss: 0.567364, acc: 71.09%] [G loss: 2.415646]\n",
      "epoch:8 step:7505 [D loss: 0.624158, acc: 65.62%] [G loss: 2.442209]\n",
      "epoch:8 step:7506 [D loss: 0.643154, acc: 67.19%] [G loss: 2.477091]\n",
      "epoch:8 step:7507 [D loss: 0.620354, acc: 68.75%] [G loss: 2.277299]\n",
      "epoch:8 step:7508 [D loss: 0.647373, acc: 65.62%] [G loss: 2.212595]\n",
      "epoch:8 step:7509 [D loss: 0.606503, acc: 64.84%] [G loss: 2.191501]\n",
      "epoch:8 step:7510 [D loss: 0.574737, acc: 67.97%] [G loss: 2.318750]\n",
      "epoch:8 step:7511 [D loss: 0.571187, acc: 74.22%] [G loss: 2.638418]\n",
      "epoch:8 step:7512 [D loss: 0.516424, acc: 73.44%] [G loss: 2.191244]\n",
      "epoch:8 step:7513 [D loss: 0.712744, acc: 58.59%] [G loss: 2.240875]\n",
      "epoch:8 step:7514 [D loss: 0.650095, acc: 64.84%] [G loss: 2.143867]\n",
      "epoch:8 step:7515 [D loss: 0.685766, acc: 63.28%] [G loss: 2.132094]\n",
      "epoch:8 step:7516 [D loss: 0.661261, acc: 64.06%] [G loss: 1.998136]\n",
      "epoch:8 step:7517 [D loss: 0.590477, acc: 64.06%] [G loss: 1.986809]\n",
      "epoch:8 step:7518 [D loss: 0.631482, acc: 63.28%] [G loss: 2.263585]\n",
      "epoch:8 step:7519 [D loss: 0.636738, acc: 63.28%] [G loss: 2.364771]\n",
      "epoch:8 step:7520 [D loss: 0.537030, acc: 74.22%] [G loss: 2.227145]\n",
      "epoch:8 step:7521 [D loss: 0.569691, acc: 73.44%] [G loss: 2.489225]\n",
      "epoch:8 step:7522 [D loss: 0.645427, acc: 57.03%] [G loss: 2.023605]\n",
      "epoch:8 step:7523 [D loss: 0.628799, acc: 60.16%] [G loss: 2.255485]\n",
      "epoch:8 step:7524 [D loss: 0.605994, acc: 67.19%] [G loss: 2.263514]\n",
      "epoch:8 step:7525 [D loss: 0.594068, acc: 64.06%] [G loss: 2.158594]\n",
      "epoch:8 step:7526 [D loss: 0.658627, acc: 60.16%] [G loss: 2.187543]\n",
      "epoch:8 step:7527 [D loss: 0.673961, acc: 63.28%] [G loss: 1.944407]\n",
      "epoch:8 step:7528 [D loss: 0.677953, acc: 60.16%] [G loss: 1.969957]\n",
      "epoch:8 step:7529 [D loss: 0.622391, acc: 65.62%] [G loss: 1.993113]\n",
      "epoch:8 step:7530 [D loss: 0.640171, acc: 67.97%] [G loss: 1.992174]\n",
      "epoch:8 step:7531 [D loss: 0.660733, acc: 64.06%] [G loss: 2.052914]\n",
      "epoch:8 step:7532 [D loss: 0.552377, acc: 76.56%] [G loss: 2.269010]\n",
      "epoch:8 step:7533 [D loss: 0.626736, acc: 62.50%] [G loss: 2.214526]\n",
      "epoch:8 step:7534 [D loss: 0.641466, acc: 64.84%] [G loss: 2.259762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7535 [D loss: 0.608255, acc: 69.53%] [G loss: 2.239033]\n",
      "epoch:8 step:7536 [D loss: 0.534934, acc: 71.09%] [G loss: 2.513872]\n",
      "epoch:8 step:7537 [D loss: 0.633889, acc: 64.84%] [G loss: 2.074077]\n",
      "epoch:8 step:7538 [D loss: 0.573810, acc: 72.66%] [G loss: 2.309700]\n",
      "epoch:8 step:7539 [D loss: 0.617661, acc: 69.53%] [G loss: 2.230871]\n",
      "epoch:8 step:7540 [D loss: 0.660320, acc: 63.28%] [G loss: 2.009122]\n",
      "epoch:8 step:7541 [D loss: 0.608307, acc: 67.97%] [G loss: 2.108891]\n",
      "epoch:8 step:7542 [D loss: 0.680076, acc: 59.38%] [G loss: 2.264804]\n",
      "epoch:8 step:7543 [D loss: 0.608340, acc: 67.19%] [G loss: 2.263831]\n",
      "epoch:8 step:7544 [D loss: 0.524062, acc: 74.22%] [G loss: 2.311619]\n",
      "epoch:8 step:7545 [D loss: 0.604114, acc: 68.75%] [G loss: 2.374774]\n",
      "epoch:8 step:7546 [D loss: 0.619176, acc: 67.97%] [G loss: 2.386832]\n",
      "epoch:8 step:7547 [D loss: 0.646324, acc: 64.06%] [G loss: 2.059084]\n",
      "epoch:8 step:7548 [D loss: 0.583869, acc: 73.44%] [G loss: 2.213541]\n",
      "epoch:8 step:7549 [D loss: 0.615087, acc: 67.97%] [G loss: 2.395852]\n",
      "epoch:8 step:7550 [D loss: 0.646571, acc: 63.28%] [G loss: 2.150167]\n",
      "epoch:8 step:7551 [D loss: 0.575727, acc: 71.09%] [G loss: 2.304725]\n",
      "epoch:8 step:7552 [D loss: 0.646954, acc: 64.84%] [G loss: 2.513041]\n",
      "epoch:8 step:7553 [D loss: 0.659694, acc: 64.84%] [G loss: 2.054865]\n",
      "epoch:8 step:7554 [D loss: 0.660967, acc: 55.47%] [G loss: 2.228237]\n",
      "epoch:8 step:7555 [D loss: 0.660667, acc: 60.94%] [G loss: 2.065599]\n",
      "epoch:8 step:7556 [D loss: 0.641046, acc: 60.94%] [G loss: 2.082034]\n",
      "epoch:8 step:7557 [D loss: 0.674341, acc: 62.50%] [G loss: 1.942874]\n",
      "epoch:8 step:7558 [D loss: 0.587337, acc: 67.97%] [G loss: 2.128051]\n",
      "epoch:8 step:7559 [D loss: 0.588835, acc: 65.62%] [G loss: 2.033611]\n",
      "epoch:8 step:7560 [D loss: 0.610298, acc: 64.84%] [G loss: 2.483421]\n",
      "epoch:8 step:7561 [D loss: 0.657359, acc: 65.62%] [G loss: 2.177743]\n",
      "epoch:8 step:7562 [D loss: 0.543684, acc: 72.66%] [G loss: 2.246204]\n",
      "epoch:8 step:7563 [D loss: 0.673597, acc: 57.81%] [G loss: 2.108796]\n",
      "epoch:8 step:7564 [D loss: 0.572358, acc: 68.75%] [G loss: 2.336219]\n",
      "epoch:8 step:7565 [D loss: 0.613801, acc: 66.41%] [G loss: 2.169783]\n",
      "epoch:8 step:7566 [D loss: 0.549160, acc: 76.56%] [G loss: 2.545067]\n",
      "epoch:8 step:7567 [D loss: 0.610839, acc: 64.84%] [G loss: 2.326417]\n",
      "epoch:8 step:7568 [D loss: 0.561679, acc: 67.19%] [G loss: 2.370275]\n",
      "epoch:8 step:7569 [D loss: 0.599969, acc: 64.84%] [G loss: 2.210703]\n",
      "epoch:8 step:7570 [D loss: 0.575455, acc: 66.41%] [G loss: 2.323682]\n",
      "epoch:8 step:7571 [D loss: 0.620234, acc: 67.97%] [G loss: 2.515245]\n",
      "epoch:8 step:7572 [D loss: 0.597665, acc: 68.75%] [G loss: 2.514900]\n",
      "epoch:8 step:7573 [D loss: 0.534667, acc: 70.31%] [G loss: 2.508935]\n",
      "epoch:8 step:7574 [D loss: 0.675362, acc: 54.69%] [G loss: 2.246420]\n",
      "epoch:8 step:7575 [D loss: 0.682864, acc: 59.38%] [G loss: 2.170644]\n",
      "epoch:8 step:7576 [D loss: 0.637835, acc: 64.06%] [G loss: 2.082294]\n",
      "epoch:8 step:7577 [D loss: 0.695913, acc: 58.59%] [G loss: 2.027344]\n",
      "epoch:8 step:7578 [D loss: 0.555405, acc: 72.66%] [G loss: 2.363456]\n",
      "epoch:8 step:7579 [D loss: 0.590676, acc: 68.75%] [G loss: 2.361710]\n",
      "epoch:8 step:7580 [D loss: 0.643107, acc: 65.62%] [G loss: 2.453627]\n",
      "epoch:8 step:7581 [D loss: 0.679904, acc: 59.38%] [G loss: 2.085842]\n",
      "epoch:8 step:7582 [D loss: 0.643485, acc: 62.50%] [G loss: 2.137411]\n",
      "epoch:8 step:7583 [D loss: 0.656889, acc: 61.72%] [G loss: 2.030136]\n",
      "epoch:8 step:7584 [D loss: 0.634805, acc: 63.28%] [G loss: 2.022501]\n",
      "epoch:8 step:7585 [D loss: 0.670123, acc: 61.72%] [G loss: 2.158130]\n",
      "epoch:8 step:7586 [D loss: 0.608926, acc: 62.50%] [G loss: 2.157773]\n",
      "epoch:8 step:7587 [D loss: 0.594592, acc: 71.09%] [G loss: 2.097359]\n",
      "epoch:8 step:7588 [D loss: 0.604793, acc: 71.88%] [G loss: 2.216644]\n",
      "epoch:8 step:7589 [D loss: 0.637416, acc: 64.06%] [G loss: 2.250238]\n",
      "epoch:8 step:7590 [D loss: 0.670472, acc: 63.28%] [G loss: 2.319190]\n",
      "epoch:8 step:7591 [D loss: 0.648527, acc: 62.50%] [G loss: 2.107875]\n",
      "epoch:8 step:7592 [D loss: 0.600268, acc: 69.53%] [G loss: 2.144016]\n",
      "epoch:8 step:7593 [D loss: 0.662955, acc: 60.94%] [G loss: 2.265826]\n",
      "epoch:8 step:7594 [D loss: 0.553762, acc: 72.66%] [G loss: 2.040050]\n",
      "epoch:8 step:7595 [D loss: 0.633367, acc: 61.72%] [G loss: 2.145561]\n",
      "epoch:8 step:7596 [D loss: 0.619187, acc: 64.06%] [G loss: 2.230207]\n",
      "epoch:8 step:7597 [D loss: 0.596580, acc: 62.50%] [G loss: 2.193324]\n",
      "epoch:8 step:7598 [D loss: 0.638881, acc: 60.16%] [G loss: 2.317078]\n",
      "epoch:8 step:7599 [D loss: 0.592713, acc: 69.53%] [G loss: 2.376949]\n",
      "epoch:8 step:7600 [D loss: 0.600746, acc: 68.75%] [G loss: 2.204496]\n",
      "##############\n",
      "[2.66565911 1.45284153 6.39343555 4.97835921 3.99156487 5.83964018\n",
      " 4.55877368 4.82675688 5.00035236 3.75167334]\n",
      "##########\n",
      "epoch:8 step:7601 [D loss: 0.633850, acc: 65.62%] [G loss: 2.144355]\n",
      "epoch:8 step:7602 [D loss: 0.612693, acc: 66.41%] [G loss: 2.454523]\n",
      "epoch:8 step:7603 [D loss: 0.604736, acc: 61.72%] [G loss: 2.244338]\n",
      "epoch:8 step:7604 [D loss: 0.663780, acc: 64.06%] [G loss: 2.090753]\n",
      "epoch:8 step:7605 [D loss: 0.652951, acc: 61.72%] [G loss: 2.202176]\n",
      "epoch:8 step:7606 [D loss: 0.653495, acc: 59.38%] [G loss: 2.106352]\n",
      "epoch:8 step:7607 [D loss: 0.622787, acc: 61.72%] [G loss: 2.148425]\n",
      "epoch:8 step:7608 [D loss: 0.573103, acc: 73.44%] [G loss: 2.319217]\n",
      "epoch:8 step:7609 [D loss: 0.621131, acc: 67.97%] [G loss: 2.217872]\n",
      "epoch:8 step:7610 [D loss: 0.616414, acc: 71.09%] [G loss: 2.519545]\n",
      "epoch:8 step:7611 [D loss: 0.674666, acc: 62.50%] [G loss: 2.108875]\n",
      "epoch:8 step:7612 [D loss: 0.592487, acc: 65.62%] [G loss: 2.296691]\n",
      "epoch:8 step:7613 [D loss: 0.630486, acc: 65.62%] [G loss: 2.262158]\n",
      "epoch:8 step:7614 [D loss: 0.667113, acc: 64.84%] [G loss: 2.051070]\n",
      "epoch:8 step:7615 [D loss: 0.600684, acc: 71.09%] [G loss: 2.252399]\n",
      "epoch:8 step:7616 [D loss: 0.622822, acc: 62.50%] [G loss: 2.396854]\n",
      "epoch:8 step:7617 [D loss: 0.674752, acc: 60.94%] [G loss: 2.334362]\n",
      "epoch:8 step:7618 [D loss: 0.606260, acc: 66.41%] [G loss: 2.144954]\n",
      "epoch:8 step:7619 [D loss: 0.604332, acc: 70.31%] [G loss: 2.186338]\n",
      "epoch:8 step:7620 [D loss: 0.587888, acc: 68.75%] [G loss: 2.251787]\n",
      "epoch:8 step:7621 [D loss: 0.597921, acc: 69.53%] [G loss: 2.213588]\n",
      "epoch:8 step:7622 [D loss: 0.605985, acc: 67.19%] [G loss: 2.212861]\n",
      "epoch:8 step:7623 [D loss: 0.621377, acc: 61.72%] [G loss: 2.005647]\n",
      "epoch:8 step:7624 [D loss: 0.592486, acc: 69.53%] [G loss: 2.276130]\n",
      "epoch:8 step:7625 [D loss: 0.654309, acc: 63.28%] [G loss: 1.887417]\n",
      "epoch:8 step:7626 [D loss: 0.572845, acc: 68.75%] [G loss: 2.402808]\n",
      "epoch:8 step:7627 [D loss: 0.652563, acc: 67.19%] [G loss: 2.187443]\n",
      "epoch:8 step:7628 [D loss: 0.702363, acc: 57.81%] [G loss: 2.278912]\n",
      "epoch:8 step:7629 [D loss: 0.691965, acc: 56.25%] [G loss: 1.986433]\n",
      "epoch:8 step:7630 [D loss: 0.612233, acc: 64.84%] [G loss: 2.105719]\n",
      "epoch:8 step:7631 [D loss: 0.628491, acc: 63.28%] [G loss: 1.939743]\n",
      "epoch:8 step:7632 [D loss: 0.634093, acc: 62.50%] [G loss: 2.007422]\n",
      "epoch:8 step:7633 [D loss: 0.650180, acc: 63.28%] [G loss: 2.071072]\n",
      "epoch:8 step:7634 [D loss: 0.644991, acc: 60.94%] [G loss: 2.235014]\n",
      "epoch:8 step:7635 [D loss: 0.646909, acc: 64.84%] [G loss: 2.139261]\n",
      "epoch:8 step:7636 [D loss: 0.593969, acc: 70.31%] [G loss: 2.138523]\n",
      "epoch:8 step:7637 [D loss: 0.600333, acc: 68.75%] [G loss: 2.223326]\n",
      "epoch:8 step:7638 [D loss: 0.623022, acc: 64.84%] [G loss: 2.071875]\n",
      "epoch:8 step:7639 [D loss: 0.665465, acc: 65.62%] [G loss: 2.128806]\n",
      "epoch:8 step:7640 [D loss: 0.603297, acc: 66.41%] [G loss: 2.043118]\n",
      "epoch:8 step:7641 [D loss: 0.589288, acc: 71.09%] [G loss: 2.224818]\n",
      "epoch:8 step:7642 [D loss: 0.641167, acc: 61.72%] [G loss: 2.045010]\n",
      "epoch:8 step:7643 [D loss: 0.617824, acc: 64.84%] [G loss: 2.061292]\n",
      "epoch:8 step:7644 [D loss: 0.664575, acc: 59.38%] [G loss: 1.962289]\n",
      "epoch:8 step:7645 [D loss: 0.558745, acc: 73.44%] [G loss: 2.281309]\n",
      "epoch:8 step:7646 [D loss: 0.579933, acc: 69.53%] [G loss: 2.256974]\n",
      "epoch:8 step:7647 [D loss: 0.563349, acc: 71.88%] [G loss: 2.432457]\n",
      "epoch:8 step:7648 [D loss: 0.593656, acc: 67.19%] [G loss: 2.401384]\n",
      "epoch:8 step:7649 [D loss: 0.619744, acc: 64.06%] [G loss: 2.213249]\n",
      "epoch:8 step:7650 [D loss: 0.624012, acc: 63.28%] [G loss: 2.237964]\n",
      "epoch:8 step:7651 [D loss: 0.628268, acc: 66.41%] [G loss: 2.428178]\n",
      "epoch:8 step:7652 [D loss: 0.563140, acc: 68.75%] [G loss: 2.258640]\n",
      "epoch:8 step:7653 [D loss: 0.611929, acc: 65.62%] [G loss: 2.177379]\n",
      "epoch:8 step:7654 [D loss: 0.602439, acc: 63.28%] [G loss: 2.434864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7655 [D loss: 0.590312, acc: 65.62%] [G loss: 2.465737]\n",
      "epoch:8 step:7656 [D loss: 0.685682, acc: 58.59%] [G loss: 2.136142]\n",
      "epoch:8 step:7657 [D loss: 0.572544, acc: 67.19%] [G loss: 2.350882]\n",
      "epoch:8 step:7658 [D loss: 0.601985, acc: 68.75%] [G loss: 2.155291]\n",
      "epoch:8 step:7659 [D loss: 0.622688, acc: 65.62%] [G loss: 2.246118]\n",
      "epoch:8 step:7660 [D loss: 0.629640, acc: 65.62%] [G loss: 2.268239]\n",
      "epoch:8 step:7661 [D loss: 0.624128, acc: 70.31%] [G loss: 2.212660]\n",
      "epoch:8 step:7662 [D loss: 0.561722, acc: 72.66%] [G loss: 2.297857]\n",
      "epoch:8 step:7663 [D loss: 0.634637, acc: 65.62%] [G loss: 2.144490]\n",
      "epoch:8 step:7664 [D loss: 0.639221, acc: 63.28%] [G loss: 2.342661]\n",
      "epoch:8 step:7665 [D loss: 0.619967, acc: 64.84%] [G loss: 2.021000]\n",
      "epoch:8 step:7666 [D loss: 0.645145, acc: 60.94%] [G loss: 2.283239]\n",
      "epoch:8 step:7667 [D loss: 0.545036, acc: 78.12%] [G loss: 2.344371]\n",
      "epoch:8 step:7668 [D loss: 0.646842, acc: 61.72%] [G loss: 2.305279]\n",
      "epoch:8 step:7669 [D loss: 0.611127, acc: 66.41%] [G loss: 2.151493]\n",
      "epoch:8 step:7670 [D loss: 0.650179, acc: 59.38%] [G loss: 2.213437]\n",
      "epoch:8 step:7671 [D loss: 0.651975, acc: 59.38%] [G loss: 2.111409]\n",
      "epoch:8 step:7672 [D loss: 0.612707, acc: 64.84%] [G loss: 2.161357]\n",
      "epoch:8 step:7673 [D loss: 0.633272, acc: 62.50%] [G loss: 2.258549]\n",
      "epoch:8 step:7674 [D loss: 0.657556, acc: 61.72%] [G loss: 2.146265]\n",
      "epoch:8 step:7675 [D loss: 0.673795, acc: 60.16%] [G loss: 2.202113]\n",
      "epoch:8 step:7676 [D loss: 0.579538, acc: 70.31%] [G loss: 2.121013]\n",
      "epoch:8 step:7677 [D loss: 0.657612, acc: 60.94%] [G loss: 2.265438]\n",
      "epoch:8 step:7678 [D loss: 0.607492, acc: 71.09%] [G loss: 2.071803]\n",
      "epoch:8 step:7679 [D loss: 0.628215, acc: 66.41%] [G loss: 2.213578]\n",
      "epoch:8 step:7680 [D loss: 0.665508, acc: 61.72%] [G loss: 2.122416]\n",
      "epoch:8 step:7681 [D loss: 0.620295, acc: 68.75%] [G loss: 2.190155]\n",
      "epoch:8 step:7682 [D loss: 0.675946, acc: 57.81%] [G loss: 2.079369]\n",
      "epoch:8 step:7683 [D loss: 0.627570, acc: 64.06%] [G loss: 2.202661]\n",
      "epoch:8 step:7684 [D loss: 0.617484, acc: 64.06%] [G loss: 2.253486]\n",
      "epoch:8 step:7685 [D loss: 0.622423, acc: 66.41%] [G loss: 2.191490]\n",
      "epoch:8 step:7686 [D loss: 0.630907, acc: 64.84%] [G loss: 2.160242]\n",
      "epoch:8 step:7687 [D loss: 0.571367, acc: 71.09%] [G loss: 2.161032]\n",
      "epoch:8 step:7688 [D loss: 0.597672, acc: 67.97%] [G loss: 2.326548]\n",
      "epoch:8 step:7689 [D loss: 0.613917, acc: 64.06%] [G loss: 2.350260]\n",
      "epoch:8 step:7690 [D loss: 0.546896, acc: 73.44%] [G loss: 2.933531]\n",
      "epoch:8 step:7691 [D loss: 0.597540, acc: 70.31%] [G loss: 2.366519]\n",
      "epoch:8 step:7692 [D loss: 0.665788, acc: 64.84%] [G loss: 2.118392]\n",
      "epoch:8 step:7693 [D loss: 0.589679, acc: 72.66%] [G loss: 2.418547]\n",
      "epoch:8 step:7694 [D loss: 0.611198, acc: 67.19%] [G loss: 2.397576]\n",
      "epoch:8 step:7695 [D loss: 0.598087, acc: 64.84%] [G loss: 2.237864]\n",
      "epoch:8 step:7696 [D loss: 0.737323, acc: 54.69%] [G loss: 1.896805]\n",
      "epoch:8 step:7697 [D loss: 0.615938, acc: 67.19%] [G loss: 2.128439]\n",
      "epoch:8 step:7698 [D loss: 0.581441, acc: 70.31%] [G loss: 2.183893]\n",
      "epoch:8 step:7699 [D loss: 0.716926, acc: 49.22%] [G loss: 2.198600]\n",
      "epoch:8 step:7700 [D loss: 0.632062, acc: 68.75%] [G loss: 1.969166]\n",
      "epoch:8 step:7701 [D loss: 0.595693, acc: 66.41%] [G loss: 2.166044]\n",
      "epoch:8 step:7702 [D loss: 0.544461, acc: 74.22%] [G loss: 2.556562]\n",
      "epoch:8 step:7703 [D loss: 0.594641, acc: 68.75%] [G loss: 2.658628]\n",
      "epoch:8 step:7704 [D loss: 0.502964, acc: 79.69%] [G loss: 2.536916]\n",
      "epoch:8 step:7705 [D loss: 0.604690, acc: 67.19%] [G loss: 2.441418]\n",
      "epoch:8 step:7706 [D loss: 0.617253, acc: 67.19%] [G loss: 2.061867]\n",
      "epoch:8 step:7707 [D loss: 0.646761, acc: 64.06%] [G loss: 2.002650]\n",
      "epoch:8 step:7708 [D loss: 0.641798, acc: 67.19%] [G loss: 2.321502]\n",
      "epoch:8 step:7709 [D loss: 0.672369, acc: 58.59%] [G loss: 1.989031]\n",
      "epoch:8 step:7710 [D loss: 0.715020, acc: 60.16%] [G loss: 2.101208]\n",
      "epoch:8 step:7711 [D loss: 0.656185, acc: 59.38%] [G loss: 1.908017]\n",
      "epoch:8 step:7712 [D loss: 0.637443, acc: 64.06%] [G loss: 2.256374]\n",
      "epoch:8 step:7713 [D loss: 0.617795, acc: 67.19%] [G loss: 2.298201]\n",
      "epoch:8 step:7714 [D loss: 0.561777, acc: 70.31%] [G loss: 2.144783]\n",
      "epoch:8 step:7715 [D loss: 0.616659, acc: 67.19%] [G loss: 2.672572]\n",
      "epoch:8 step:7716 [D loss: 0.659856, acc: 61.72%] [G loss: 1.982931]\n",
      "epoch:8 step:7717 [D loss: 0.682199, acc: 59.38%] [G loss: 2.454410]\n",
      "epoch:8 step:7718 [D loss: 0.601207, acc: 69.53%] [G loss: 2.340360]\n",
      "epoch:8 step:7719 [D loss: 0.658836, acc: 66.41%] [G loss: 2.365026]\n",
      "epoch:8 step:7720 [D loss: 0.681989, acc: 60.94%] [G loss: 2.052587]\n",
      "epoch:8 step:7721 [D loss: 0.651219, acc: 65.62%] [G loss: 2.102529]\n",
      "epoch:8 step:7722 [D loss: 0.637304, acc: 66.41%] [G loss: 2.055624]\n",
      "epoch:8 step:7723 [D loss: 0.697971, acc: 57.03%] [G loss: 1.894878]\n",
      "epoch:8 step:7724 [D loss: 0.683722, acc: 59.38%] [G loss: 2.053458]\n",
      "epoch:8 step:7725 [D loss: 0.634345, acc: 67.19%] [G loss: 2.071622]\n",
      "epoch:8 step:7726 [D loss: 0.587722, acc: 65.62%] [G loss: 2.151413]\n",
      "epoch:8 step:7727 [D loss: 0.574225, acc: 70.31%] [G loss: 2.523951]\n",
      "epoch:8 step:7728 [D loss: 0.530150, acc: 75.78%] [G loss: 2.840706]\n",
      "epoch:8 step:7729 [D loss: 0.676578, acc: 60.16%] [G loss: 1.949566]\n",
      "epoch:8 step:7730 [D loss: 0.648726, acc: 62.50%] [G loss: 2.290308]\n",
      "epoch:8 step:7731 [D loss: 0.606594, acc: 64.84%] [G loss: 2.087860]\n",
      "epoch:8 step:7732 [D loss: 0.558235, acc: 74.22%] [G loss: 2.053409]\n",
      "epoch:8 step:7733 [D loss: 0.657353, acc: 64.84%] [G loss: 1.983388]\n",
      "epoch:8 step:7734 [D loss: 0.562107, acc: 68.75%] [G loss: 2.161579]\n",
      "epoch:8 step:7735 [D loss: 0.601462, acc: 67.19%] [G loss: 1.982498]\n",
      "epoch:8 step:7736 [D loss: 0.693665, acc: 56.25%] [G loss: 2.067980]\n",
      "epoch:8 step:7737 [D loss: 0.624435, acc: 62.50%] [G loss: 2.028886]\n",
      "epoch:8 step:7738 [D loss: 0.616309, acc: 65.62%] [G loss: 2.269831]\n",
      "epoch:8 step:7739 [D loss: 0.605888, acc: 62.50%] [G loss: 2.335043]\n",
      "epoch:8 step:7740 [D loss: 0.552762, acc: 74.22%] [G loss: 2.287711]\n",
      "epoch:8 step:7741 [D loss: 0.639101, acc: 70.31%] [G loss: 2.183663]\n",
      "epoch:8 step:7742 [D loss: 0.617437, acc: 68.75%] [G loss: 2.318902]\n",
      "epoch:8 step:7743 [D loss: 0.656717, acc: 64.84%] [G loss: 2.175210]\n",
      "epoch:8 step:7744 [D loss: 0.672307, acc: 63.28%] [G loss: 2.274255]\n",
      "epoch:8 step:7745 [D loss: 0.676176, acc: 61.72%] [G loss: 2.049380]\n",
      "epoch:8 step:7746 [D loss: 0.711790, acc: 50.78%] [G loss: 1.969038]\n",
      "epoch:8 step:7747 [D loss: 0.678765, acc: 59.38%] [G loss: 2.130648]\n",
      "epoch:8 step:7748 [D loss: 0.616354, acc: 64.06%] [G loss: 1.982845]\n",
      "epoch:8 step:7749 [D loss: 0.626629, acc: 65.62%] [G loss: 2.060175]\n",
      "epoch:8 step:7750 [D loss: 0.577381, acc: 69.53%] [G loss: 2.036676]\n",
      "epoch:8 step:7751 [D loss: 0.580406, acc: 67.19%] [G loss: 2.063898]\n",
      "epoch:8 step:7752 [D loss: 0.589950, acc: 73.44%] [G loss: 2.105755]\n",
      "epoch:8 step:7753 [D loss: 0.649419, acc: 64.84%] [G loss: 1.893664]\n",
      "epoch:8 step:7754 [D loss: 0.641499, acc: 64.06%] [G loss: 2.093863]\n",
      "epoch:8 step:7755 [D loss: 0.637595, acc: 66.41%] [G loss: 2.200219]\n",
      "epoch:8 step:7756 [D loss: 0.647320, acc: 67.97%] [G loss: 2.224455]\n",
      "epoch:8 step:7757 [D loss: 0.673579, acc: 60.16%] [G loss: 2.175462]\n",
      "epoch:8 step:7758 [D loss: 0.599498, acc: 68.75%] [G loss: 2.047461]\n",
      "epoch:8 step:7759 [D loss: 0.657983, acc: 60.94%] [G loss: 2.189645]\n",
      "epoch:8 step:7760 [D loss: 0.603372, acc: 67.97%] [G loss: 2.348861]\n",
      "epoch:8 step:7761 [D loss: 0.633480, acc: 65.62%] [G loss: 2.055936]\n",
      "epoch:8 step:7762 [D loss: 0.600848, acc: 65.62%] [G loss: 2.167500]\n",
      "epoch:8 step:7763 [D loss: 0.598737, acc: 70.31%] [G loss: 2.106661]\n",
      "epoch:8 step:7764 [D loss: 0.646556, acc: 67.97%] [G loss: 2.106516]\n",
      "epoch:8 step:7765 [D loss: 0.571889, acc: 71.88%] [G loss: 2.045884]\n",
      "epoch:8 step:7766 [D loss: 0.607944, acc: 60.94%] [G loss: 2.194847]\n",
      "epoch:8 step:7767 [D loss: 0.656701, acc: 63.28%] [G loss: 2.231284]\n",
      "epoch:8 step:7768 [D loss: 0.634094, acc: 68.75%] [G loss: 2.182852]\n",
      "epoch:8 step:7769 [D loss: 0.644746, acc: 64.84%] [G loss: 2.130194]\n",
      "epoch:8 step:7770 [D loss: 0.634081, acc: 67.97%] [G loss: 2.259281]\n",
      "epoch:8 step:7771 [D loss: 0.653512, acc: 65.62%] [G loss: 2.132947]\n",
      "epoch:8 step:7772 [D loss: 0.613289, acc: 64.06%] [G loss: 2.090952]\n",
      "epoch:8 step:7773 [D loss: 0.581893, acc: 65.62%] [G loss: 2.127249]\n",
      "epoch:8 step:7774 [D loss: 0.664794, acc: 64.06%] [G loss: 2.107667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7775 [D loss: 0.577474, acc: 69.53%] [G loss: 2.232826]\n",
      "epoch:8 step:7776 [D loss: 0.582490, acc: 68.75%] [G loss: 2.526927]\n",
      "epoch:8 step:7777 [D loss: 0.659737, acc: 60.94%] [G loss: 2.137482]\n",
      "epoch:8 step:7778 [D loss: 0.627633, acc: 62.50%] [G loss: 2.261871]\n",
      "epoch:8 step:7779 [D loss: 0.602161, acc: 68.75%] [G loss: 2.167207]\n",
      "epoch:8 step:7780 [D loss: 0.558170, acc: 72.66%] [G loss: 2.142000]\n",
      "epoch:8 step:7781 [D loss: 0.590305, acc: 66.41%] [G loss: 2.304597]\n",
      "epoch:8 step:7782 [D loss: 0.672760, acc: 59.38%] [G loss: 2.313740]\n",
      "epoch:8 step:7783 [D loss: 0.623644, acc: 64.84%] [G loss: 2.115491]\n",
      "epoch:8 step:7784 [D loss: 0.633784, acc: 67.97%] [G loss: 2.206221]\n",
      "epoch:8 step:7785 [D loss: 0.631318, acc: 63.28%] [G loss: 2.114993]\n",
      "epoch:8 step:7786 [D loss: 0.703282, acc: 60.16%] [G loss: 2.216861]\n",
      "epoch:8 step:7787 [D loss: 0.645952, acc: 61.72%] [G loss: 2.127342]\n",
      "epoch:8 step:7788 [D loss: 0.631515, acc: 68.75%] [G loss: 1.999883]\n",
      "epoch:8 step:7789 [D loss: 0.549834, acc: 74.22%] [G loss: 2.198038]\n",
      "epoch:8 step:7790 [D loss: 0.642892, acc: 63.28%] [G loss: 2.149798]\n",
      "epoch:8 step:7791 [D loss: 0.675781, acc: 60.16%] [G loss: 2.272674]\n",
      "epoch:8 step:7792 [D loss: 0.623386, acc: 63.28%] [G loss: 2.120723]\n",
      "epoch:8 step:7793 [D loss: 0.580234, acc: 72.66%] [G loss: 2.091105]\n",
      "epoch:8 step:7794 [D loss: 0.620996, acc: 65.62%] [G loss: 2.353676]\n",
      "epoch:8 step:7795 [D loss: 0.582971, acc: 71.09%] [G loss: 2.215260]\n",
      "epoch:8 step:7796 [D loss: 0.634733, acc: 67.19%] [G loss: 2.314220]\n",
      "epoch:8 step:7797 [D loss: 0.669266, acc: 59.38%] [G loss: 2.007818]\n",
      "epoch:8 step:7798 [D loss: 0.636453, acc: 62.50%] [G loss: 2.255083]\n",
      "epoch:8 step:7799 [D loss: 0.688565, acc: 56.25%] [G loss: 2.252793]\n",
      "epoch:8 step:7800 [D loss: 0.613254, acc: 68.75%] [G loss: 2.005181]\n",
      "##############\n",
      "[2.57330266 1.36191186 6.46358012 5.08827059 3.99975624 5.81060469\n",
      " 4.66313809 4.925363   5.05927761 3.63367327]\n",
      "##########\n",
      "epoch:8 step:7801 [D loss: 0.633058, acc: 67.97%] [G loss: 2.016838]\n",
      "epoch:8 step:7802 [D loss: 0.645872, acc: 65.62%] [G loss: 2.016281]\n",
      "epoch:8 step:7803 [D loss: 0.611525, acc: 66.41%] [G loss: 2.147578]\n",
      "epoch:8 step:7804 [D loss: 0.593046, acc: 70.31%] [G loss: 1.875605]\n",
      "epoch:8 step:7805 [D loss: 0.592799, acc: 68.75%] [G loss: 2.049364]\n",
      "epoch:8 step:7806 [D loss: 0.642027, acc: 63.28%] [G loss: 2.102204]\n",
      "epoch:8 step:7807 [D loss: 0.597918, acc: 68.75%] [G loss: 2.242953]\n",
      "epoch:8 step:7808 [D loss: 0.564351, acc: 67.19%] [G loss: 2.516464]\n",
      "epoch:8 step:7809 [D loss: 0.554160, acc: 68.75%] [G loss: 2.401295]\n",
      "epoch:8 step:7810 [D loss: 0.527651, acc: 75.00%] [G loss: 2.496551]\n",
      "epoch:8 step:7811 [D loss: 0.583005, acc: 65.62%] [G loss: 2.518966]\n",
      "epoch:8 step:7812 [D loss: 0.693860, acc: 54.69%] [G loss: 1.990102]\n",
      "epoch:8 step:7813 [D loss: 0.661784, acc: 57.03%] [G loss: 1.951349]\n",
      "epoch:8 step:7814 [D loss: 0.652259, acc: 61.72%] [G loss: 2.186506]\n",
      "epoch:8 step:7815 [D loss: 0.657760, acc: 63.28%] [G loss: 2.132826]\n",
      "epoch:8 step:7816 [D loss: 0.659947, acc: 60.16%] [G loss: 2.067044]\n",
      "epoch:8 step:7817 [D loss: 0.605964, acc: 67.19%] [G loss: 2.393861]\n",
      "epoch:8 step:7818 [D loss: 0.655749, acc: 64.06%] [G loss: 2.020541]\n",
      "epoch:8 step:7819 [D loss: 0.707023, acc: 54.69%] [G loss: 1.930969]\n",
      "epoch:8 step:7820 [D loss: 0.585731, acc: 69.53%] [G loss: 1.999182]\n",
      "epoch:8 step:7821 [D loss: 0.571352, acc: 69.53%] [G loss: 2.038501]\n",
      "epoch:8 step:7822 [D loss: 0.620674, acc: 63.28%] [G loss: 2.227059]\n",
      "epoch:8 step:7823 [D loss: 0.644025, acc: 64.06%] [G loss: 2.297658]\n",
      "epoch:8 step:7824 [D loss: 0.660292, acc: 62.50%] [G loss: 2.083419]\n",
      "epoch:8 step:7825 [D loss: 0.611046, acc: 64.06%] [G loss: 2.237061]\n",
      "epoch:8 step:7826 [D loss: 0.642051, acc: 62.50%] [G loss: 2.173604]\n",
      "epoch:8 step:7827 [D loss: 0.602925, acc: 67.19%] [G loss: 2.164676]\n",
      "epoch:8 step:7828 [D loss: 0.567698, acc: 70.31%] [G loss: 2.357028]\n",
      "epoch:8 step:7829 [D loss: 0.597449, acc: 65.62%] [G loss: 2.345203]\n",
      "epoch:8 step:7830 [D loss: 0.620015, acc: 69.53%] [G loss: 2.385543]\n",
      "epoch:8 step:7831 [D loss: 0.616494, acc: 66.41%] [G loss: 2.422051]\n",
      "epoch:8 step:7832 [D loss: 0.576740, acc: 67.97%] [G loss: 2.194598]\n",
      "epoch:8 step:7833 [D loss: 0.576966, acc: 71.09%] [G loss: 2.354311]\n",
      "epoch:8 step:7834 [D loss: 0.622596, acc: 64.84%] [G loss: 2.183501]\n",
      "epoch:8 step:7835 [D loss: 0.578253, acc: 67.97%] [G loss: 2.455287]\n",
      "epoch:8 step:7836 [D loss: 0.610387, acc: 69.53%] [G loss: 2.147451]\n",
      "epoch:8 step:7837 [D loss: 0.665713, acc: 62.50%] [G loss: 2.149513]\n",
      "epoch:8 step:7838 [D loss: 0.677452, acc: 55.47%] [G loss: 2.183571]\n",
      "epoch:8 step:7839 [D loss: 0.675089, acc: 62.50%] [G loss: 2.148660]\n",
      "epoch:8 step:7840 [D loss: 0.629220, acc: 66.41%] [G loss: 2.133380]\n",
      "epoch:8 step:7841 [D loss: 0.574882, acc: 73.44%] [G loss: 2.497728]\n",
      "epoch:8 step:7842 [D loss: 0.538292, acc: 75.00%] [G loss: 2.574473]\n",
      "epoch:8 step:7843 [D loss: 0.549613, acc: 73.44%] [G loss: 2.335922]\n",
      "epoch:8 step:7844 [D loss: 0.742930, acc: 56.25%] [G loss: 2.099617]\n",
      "epoch:8 step:7845 [D loss: 0.692398, acc: 56.25%] [G loss: 1.811609]\n",
      "epoch:8 step:7846 [D loss: 0.657272, acc: 63.28%] [G loss: 2.065621]\n",
      "epoch:8 step:7847 [D loss: 0.668602, acc: 63.28%] [G loss: 1.924688]\n",
      "epoch:8 step:7848 [D loss: 0.652378, acc: 62.50%] [G loss: 2.254478]\n",
      "epoch:8 step:7849 [D loss: 0.611063, acc: 63.28%] [G loss: 2.329859]\n",
      "epoch:8 step:7850 [D loss: 0.633225, acc: 62.50%] [G loss: 2.449504]\n",
      "epoch:8 step:7851 [D loss: 0.590765, acc: 67.19%] [G loss: 2.150197]\n",
      "epoch:8 step:7852 [D loss: 0.728080, acc: 53.91%] [G loss: 1.981766]\n",
      "epoch:8 step:7853 [D loss: 0.548833, acc: 73.44%] [G loss: 2.148033]\n",
      "epoch:8 step:7854 [D loss: 0.608791, acc: 69.53%] [G loss: 2.466710]\n",
      "epoch:8 step:7855 [D loss: 0.579806, acc: 70.31%] [G loss: 2.379383]\n",
      "epoch:8 step:7856 [D loss: 0.570573, acc: 70.31%] [G loss: 2.156550]\n",
      "epoch:8 step:7857 [D loss: 0.622575, acc: 62.50%] [G loss: 2.164065]\n",
      "epoch:8 step:7858 [D loss: 0.644450, acc: 63.28%] [G loss: 2.150699]\n",
      "epoch:8 step:7859 [D loss: 0.606148, acc: 65.62%] [G loss: 2.198850]\n",
      "epoch:8 step:7860 [D loss: 0.565634, acc: 67.19%] [G loss: 2.139268]\n",
      "epoch:8 step:7861 [D loss: 0.583134, acc: 71.09%] [G loss: 2.199817]\n",
      "epoch:8 step:7862 [D loss: 0.576602, acc: 67.97%] [G loss: 1.986837]\n",
      "epoch:8 step:7863 [D loss: 0.635932, acc: 64.84%] [G loss: 2.148560]\n",
      "epoch:8 step:7864 [D loss: 0.560200, acc: 75.00%] [G loss: 2.217441]\n",
      "epoch:8 step:7865 [D loss: 0.628542, acc: 62.50%] [G loss: 2.201329]\n",
      "epoch:8 step:7866 [D loss: 0.641112, acc: 60.16%] [G loss: 2.136086]\n",
      "epoch:8 step:7867 [D loss: 0.582198, acc: 71.09%] [G loss: 2.372909]\n",
      "epoch:8 step:7868 [D loss: 0.630159, acc: 63.28%] [G loss: 2.361396]\n",
      "epoch:8 step:7869 [D loss: 0.611235, acc: 60.94%] [G loss: 2.032724]\n",
      "epoch:8 step:7870 [D loss: 0.581718, acc: 67.97%] [G loss: 2.306724]\n",
      "epoch:8 step:7871 [D loss: 0.636866, acc: 61.72%] [G loss: 2.215860]\n",
      "epoch:8 step:7872 [D loss: 0.706195, acc: 52.34%] [G loss: 1.916627]\n",
      "epoch:8 step:7873 [D loss: 0.692674, acc: 61.72%] [G loss: 1.924306]\n",
      "epoch:8 step:7874 [D loss: 0.652802, acc: 60.94%] [G loss: 2.037922]\n",
      "epoch:8 step:7875 [D loss: 0.611226, acc: 69.53%] [G loss: 2.156786]\n",
      "epoch:8 step:7876 [D loss: 0.682925, acc: 63.28%] [G loss: 2.191867]\n",
      "epoch:8 step:7877 [D loss: 0.552909, acc: 67.97%] [G loss: 2.363465]\n",
      "epoch:8 step:7878 [D loss: 0.629127, acc: 69.53%] [G loss: 2.136682]\n",
      "epoch:8 step:7879 [D loss: 0.637481, acc: 66.41%] [G loss: 2.011590]\n",
      "epoch:8 step:7880 [D loss: 0.599428, acc: 67.97%] [G loss: 2.268909]\n",
      "epoch:8 step:7881 [D loss: 0.639805, acc: 65.62%] [G loss: 2.307967]\n",
      "epoch:8 step:7882 [D loss: 0.645998, acc: 62.50%] [G loss: 1.925376]\n",
      "epoch:8 step:7883 [D loss: 0.689510, acc: 59.38%] [G loss: 2.298454]\n",
      "epoch:8 step:7884 [D loss: 0.731200, acc: 58.59%] [G loss: 2.028532]\n",
      "epoch:8 step:7885 [D loss: 0.693471, acc: 57.81%] [G loss: 2.021352]\n",
      "epoch:8 step:7886 [D loss: 0.621291, acc: 66.41%] [G loss: 2.320962]\n",
      "epoch:8 step:7887 [D loss: 0.718584, acc: 62.50%] [G loss: 1.979770]\n",
      "epoch:8 step:7888 [D loss: 0.582360, acc: 67.97%] [G loss: 2.161026]\n",
      "epoch:8 step:7889 [D loss: 0.634022, acc: 62.50%] [G loss: 2.112927]\n",
      "epoch:8 step:7890 [D loss: 0.608562, acc: 67.19%] [G loss: 2.339368]\n",
      "epoch:8 step:7891 [D loss: 0.590216, acc: 64.84%] [G loss: 2.108434]\n",
      "epoch:8 step:7892 [D loss: 0.655834, acc: 60.16%] [G loss: 2.041093]\n",
      "epoch:8 step:7893 [D loss: 0.654971, acc: 57.81%] [G loss: 2.198523]\n",
      "epoch:8 step:7894 [D loss: 0.669370, acc: 65.62%] [G loss: 2.046526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7895 [D loss: 0.614992, acc: 63.28%] [G loss: 2.103816]\n",
      "epoch:8 step:7896 [D loss: 0.644870, acc: 71.88%] [G loss: 2.247108]\n",
      "epoch:8 step:7897 [D loss: 0.635831, acc: 67.19%] [G loss: 2.128798]\n",
      "epoch:8 step:7898 [D loss: 0.611600, acc: 61.72%] [G loss: 2.246531]\n",
      "epoch:8 step:7899 [D loss: 0.593706, acc: 70.31%] [G loss: 2.247851]\n",
      "epoch:8 step:7900 [D loss: 0.610976, acc: 66.41%] [G loss: 2.245063]\n",
      "epoch:8 step:7901 [D loss: 0.568866, acc: 70.31%] [G loss: 2.346363]\n",
      "epoch:8 step:7902 [D loss: 0.587480, acc: 71.09%] [G loss: 2.358707]\n",
      "epoch:8 step:7903 [D loss: 0.599445, acc: 63.28%] [G loss: 2.163633]\n",
      "epoch:8 step:7904 [D loss: 0.653418, acc: 64.84%] [G loss: 2.382144]\n",
      "epoch:8 step:7905 [D loss: 0.584003, acc: 64.06%] [G loss: 2.235190]\n",
      "epoch:8 step:7906 [D loss: 0.598747, acc: 64.06%] [G loss: 2.331800]\n",
      "epoch:8 step:7907 [D loss: 0.630032, acc: 64.06%] [G loss: 2.125889]\n",
      "epoch:8 step:7908 [D loss: 0.601473, acc: 72.66%] [G loss: 2.171626]\n",
      "epoch:8 step:7909 [D loss: 0.665839, acc: 57.03%] [G loss: 2.210952]\n",
      "epoch:8 step:7910 [D loss: 0.586362, acc: 71.88%] [G loss: 2.357688]\n",
      "epoch:8 step:7911 [D loss: 0.593097, acc: 67.19%] [G loss: 2.158931]\n",
      "epoch:8 step:7912 [D loss: 0.610335, acc: 67.19%] [G loss: 2.234648]\n",
      "epoch:8 step:7913 [D loss: 0.684472, acc: 59.38%] [G loss: 2.161513]\n",
      "epoch:8 step:7914 [D loss: 0.666652, acc: 59.38%] [G loss: 1.986199]\n",
      "epoch:8 step:7915 [D loss: 0.679395, acc: 63.28%] [G loss: 2.218574]\n",
      "epoch:8 step:7916 [D loss: 0.673442, acc: 61.72%] [G loss: 2.245518]\n",
      "epoch:8 step:7917 [D loss: 0.650189, acc: 66.41%] [G loss: 2.074374]\n",
      "epoch:8 step:7918 [D loss: 0.706414, acc: 57.81%] [G loss: 2.129146]\n",
      "epoch:8 step:7919 [D loss: 0.618495, acc: 68.75%] [G loss: 2.000803]\n",
      "epoch:8 step:7920 [D loss: 0.644329, acc: 64.84%] [G loss: 2.136330]\n",
      "epoch:8 step:7921 [D loss: 0.688888, acc: 57.03%] [G loss: 2.205622]\n",
      "epoch:8 step:7922 [D loss: 0.665286, acc: 66.41%] [G loss: 2.153910]\n",
      "epoch:8 step:7923 [D loss: 0.588208, acc: 70.31%] [G loss: 2.297928]\n",
      "epoch:8 step:7924 [D loss: 0.498531, acc: 75.78%] [G loss: 2.495379]\n",
      "epoch:8 step:7925 [D loss: 0.598987, acc: 65.62%] [G loss: 2.499084]\n",
      "epoch:8 step:7926 [D loss: 0.526551, acc: 74.22%] [G loss: 2.570526]\n",
      "epoch:8 step:7927 [D loss: 0.583953, acc: 72.66%] [G loss: 2.325053]\n",
      "epoch:8 step:7928 [D loss: 0.667804, acc: 64.06%] [G loss: 2.207607]\n",
      "epoch:8 step:7929 [D loss: 0.601015, acc: 70.31%] [G loss: 2.110377]\n",
      "epoch:8 step:7930 [D loss: 0.651118, acc: 65.62%] [G loss: 2.163991]\n",
      "epoch:8 step:7931 [D loss: 0.674381, acc: 59.38%] [G loss: 2.137060]\n",
      "epoch:8 step:7932 [D loss: 0.613229, acc: 64.84%] [G loss: 2.376303]\n",
      "epoch:8 step:7933 [D loss: 0.670485, acc: 57.81%] [G loss: 2.108662]\n",
      "epoch:8 step:7934 [D loss: 0.709903, acc: 60.16%] [G loss: 2.075372]\n",
      "epoch:8 step:7935 [D loss: 0.644916, acc: 62.50%] [G loss: 1.932564]\n",
      "epoch:8 step:7936 [D loss: 0.641981, acc: 63.28%] [G loss: 2.177824]\n",
      "epoch:8 step:7937 [D loss: 0.673171, acc: 56.25%] [G loss: 1.930824]\n",
      "epoch:8 step:7938 [D loss: 0.647376, acc: 60.94%] [G loss: 2.135376]\n",
      "epoch:8 step:7939 [D loss: 0.741011, acc: 53.12%] [G loss: 2.028512]\n",
      "epoch:8 step:7940 [D loss: 0.613764, acc: 63.28%] [G loss: 1.984979]\n",
      "epoch:8 step:7941 [D loss: 0.632482, acc: 66.41%] [G loss: 2.046170]\n",
      "epoch:8 step:7942 [D loss: 0.637415, acc: 65.62%] [G loss: 1.981466]\n",
      "epoch:8 step:7943 [D loss: 0.588763, acc: 69.53%] [G loss: 2.012079]\n",
      "epoch:8 step:7944 [D loss: 0.640957, acc: 60.94%] [G loss: 1.927483]\n",
      "epoch:8 step:7945 [D loss: 0.640894, acc: 60.94%] [G loss: 2.207676]\n",
      "epoch:8 step:7946 [D loss: 0.661121, acc: 57.03%] [G loss: 2.099995]\n",
      "epoch:8 step:7947 [D loss: 0.535895, acc: 80.47%] [G loss: 2.272320]\n",
      "epoch:8 step:7948 [D loss: 0.588682, acc: 68.75%] [G loss: 2.150807]\n",
      "epoch:8 step:7949 [D loss: 0.604610, acc: 67.19%] [G loss: 2.314933]\n",
      "epoch:8 step:7950 [D loss: 0.591748, acc: 70.31%] [G loss: 2.168923]\n",
      "epoch:8 step:7951 [D loss: 0.615282, acc: 68.75%] [G loss: 2.125493]\n",
      "epoch:8 step:7952 [D loss: 0.604517, acc: 68.75%] [G loss: 1.970637]\n",
      "epoch:8 step:7953 [D loss: 0.566856, acc: 70.31%] [G loss: 2.166529]\n",
      "epoch:8 step:7954 [D loss: 0.637603, acc: 67.97%] [G loss: 2.072480]\n",
      "epoch:8 step:7955 [D loss: 0.686848, acc: 57.81%] [G loss: 1.999706]\n",
      "epoch:8 step:7956 [D loss: 0.615004, acc: 64.06%] [G loss: 1.999717]\n",
      "epoch:8 step:7957 [D loss: 0.650429, acc: 62.50%] [G loss: 2.032397]\n",
      "epoch:8 step:7958 [D loss: 0.558350, acc: 70.31%] [G loss: 2.016014]\n",
      "epoch:8 step:7959 [D loss: 0.638691, acc: 68.75%] [G loss: 2.157866]\n",
      "epoch:8 step:7960 [D loss: 0.613465, acc: 64.84%] [G loss: 2.195834]\n",
      "epoch:8 step:7961 [D loss: 0.618288, acc: 68.75%] [G loss: 2.019435]\n",
      "epoch:8 step:7962 [D loss: 0.602335, acc: 69.53%] [G loss: 2.107576]\n",
      "epoch:8 step:7963 [D loss: 0.583124, acc: 74.22%] [G loss: 2.095034]\n",
      "epoch:8 step:7964 [D loss: 0.616454, acc: 69.53%] [G loss: 2.377840]\n",
      "epoch:8 step:7965 [D loss: 0.619234, acc: 64.06%] [G loss: 2.332754]\n",
      "epoch:8 step:7966 [D loss: 0.686754, acc: 62.50%] [G loss: 2.269425]\n",
      "epoch:8 step:7967 [D loss: 0.581910, acc: 68.75%] [G loss: 2.734274]\n",
      "epoch:8 step:7968 [D loss: 0.607977, acc: 66.41%] [G loss: 2.617445]\n",
      "epoch:8 step:7969 [D loss: 0.668542, acc: 61.72%] [G loss: 2.117279]\n",
      "epoch:8 step:7970 [D loss: 0.609240, acc: 65.62%] [G loss: 2.278765]\n",
      "epoch:8 step:7971 [D loss: 0.663395, acc: 60.94%] [G loss: 2.462590]\n",
      "epoch:8 step:7972 [D loss: 0.624363, acc: 64.84%] [G loss: 2.188494]\n",
      "epoch:8 step:7973 [D loss: 0.695791, acc: 50.00%] [G loss: 1.961333]\n",
      "epoch:8 step:7974 [D loss: 0.669984, acc: 62.50%] [G loss: 1.920794]\n",
      "epoch:8 step:7975 [D loss: 0.609757, acc: 67.97%] [G loss: 2.207755]\n",
      "epoch:8 step:7976 [D loss: 0.651344, acc: 54.69%] [G loss: 2.196481]\n",
      "epoch:8 step:7977 [D loss: 0.595371, acc: 70.31%] [G loss: 2.398316]\n",
      "epoch:8 step:7978 [D loss: 0.638804, acc: 63.28%] [G loss: 2.018921]\n",
      "epoch:8 step:7979 [D loss: 0.689833, acc: 61.72%] [G loss: 2.012224]\n",
      "epoch:8 step:7980 [D loss: 0.670637, acc: 66.41%] [G loss: 2.167497]\n",
      "epoch:8 step:7981 [D loss: 0.602312, acc: 67.97%] [G loss: 2.121563]\n",
      "epoch:8 step:7982 [D loss: 0.693485, acc: 59.38%] [G loss: 1.952387]\n",
      "epoch:8 step:7983 [D loss: 0.597447, acc: 67.97%] [G loss: 2.142678]\n",
      "epoch:8 step:7984 [D loss: 0.605784, acc: 71.88%] [G loss: 2.362012]\n",
      "epoch:8 step:7985 [D loss: 0.593459, acc: 69.53%] [G loss: 2.073036]\n",
      "epoch:8 step:7986 [D loss: 0.651668, acc: 64.84%] [G loss: 2.219953]\n",
      "epoch:8 step:7987 [D loss: 0.592533, acc: 67.97%] [G loss: 2.303457]\n",
      "epoch:8 step:7988 [D loss: 0.678331, acc: 60.16%] [G loss: 2.057994]\n",
      "epoch:8 step:7989 [D loss: 0.628129, acc: 66.41%] [G loss: 2.013701]\n",
      "epoch:8 step:7990 [D loss: 0.580618, acc: 67.97%] [G loss: 2.404757]\n",
      "epoch:8 step:7991 [D loss: 0.581825, acc: 65.62%] [G loss: 2.380805]\n",
      "epoch:8 step:7992 [D loss: 0.581080, acc: 70.31%] [G loss: 2.202039]\n",
      "epoch:8 step:7993 [D loss: 0.595231, acc: 70.31%] [G loss: 2.332525]\n",
      "epoch:8 step:7994 [D loss: 0.554379, acc: 70.31%] [G loss: 2.336571]\n",
      "epoch:8 step:7995 [D loss: 0.621081, acc: 67.97%] [G loss: 2.414638]\n",
      "epoch:8 step:7996 [D loss: 0.725843, acc: 59.38%] [G loss: 1.974311]\n",
      "epoch:8 step:7997 [D loss: 0.738308, acc: 53.12%] [G loss: 1.993890]\n",
      "epoch:8 step:7998 [D loss: 0.681583, acc: 56.25%] [G loss: 2.054235]\n",
      "epoch:8 step:7999 [D loss: 0.577406, acc: 73.44%] [G loss: 2.148568]\n",
      "epoch:8 step:8000 [D loss: 0.606256, acc: 65.62%] [G loss: 2.264560]\n",
      "##############\n",
      "[2.66237581 1.46192956 6.54314392 5.06552322 4.10748685 5.75362004\n",
      " 4.56963956 4.91637433 4.97282099 3.6081522 ]\n",
      "##########\n",
      "epoch:8 step:8001 [D loss: 0.650139, acc: 59.38%] [G loss: 2.107484]\n",
      "epoch:8 step:8002 [D loss: 0.641669, acc: 62.50%] [G loss: 1.926552]\n",
      "epoch:8 step:8003 [D loss: 0.688182, acc: 59.38%] [G loss: 2.172187]\n",
      "epoch:8 step:8004 [D loss: 0.577715, acc: 72.66%] [G loss: 2.282988]\n",
      "epoch:8 step:8005 [D loss: 0.660018, acc: 56.25%] [G loss: 1.959732]\n",
      "epoch:8 step:8006 [D loss: 0.655550, acc: 62.50%] [G loss: 2.022507]\n",
      "epoch:8 step:8007 [D loss: 0.702887, acc: 54.69%] [G loss: 1.840188]\n",
      "epoch:8 step:8008 [D loss: 0.703399, acc: 57.03%] [G loss: 1.965582]\n",
      "epoch:8 step:8009 [D loss: 0.632728, acc: 67.19%] [G loss: 2.099357]\n",
      "epoch:8 step:8010 [D loss: 0.640813, acc: 59.38%] [G loss: 2.132104]\n",
      "epoch:8 step:8011 [D loss: 0.634365, acc: 67.19%] [G loss: 2.265424]\n",
      "epoch:8 step:8012 [D loss: 0.585053, acc: 69.53%] [G loss: 2.148403]\n",
      "epoch:8 step:8013 [D loss: 0.545029, acc: 70.31%] [G loss: 2.243182]\n",
      "epoch:8 step:8014 [D loss: 0.571106, acc: 67.97%] [G loss: 2.201801]\n",
      "epoch:8 step:8015 [D loss: 0.655003, acc: 62.50%] [G loss: 2.236704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8016 [D loss: 0.523385, acc: 73.44%] [G loss: 2.293653]\n",
      "epoch:8 step:8017 [D loss: 0.587296, acc: 70.31%] [G loss: 2.094800]\n",
      "epoch:8 step:8018 [D loss: 0.617806, acc: 60.94%] [G loss: 2.164711]\n",
      "epoch:8 step:8019 [D loss: 0.580469, acc: 71.09%] [G loss: 2.250672]\n",
      "epoch:8 step:8020 [D loss: 0.652991, acc: 57.03%] [G loss: 2.251686]\n",
      "epoch:8 step:8021 [D loss: 0.629564, acc: 67.97%] [G loss: 2.063746]\n",
      "epoch:8 step:8022 [D loss: 0.571248, acc: 68.75%] [G loss: 2.100894]\n",
      "epoch:8 step:8023 [D loss: 0.696362, acc: 57.03%] [G loss: 2.166343]\n",
      "epoch:8 step:8024 [D loss: 0.640740, acc: 67.19%] [G loss: 1.976246]\n",
      "epoch:8 step:8025 [D loss: 0.670993, acc: 62.50%] [G loss: 2.115932]\n",
      "epoch:8 step:8026 [D loss: 0.668641, acc: 60.94%] [G loss: 2.137546]\n",
      "epoch:8 step:8027 [D loss: 0.621139, acc: 65.62%] [G loss: 2.050185]\n",
      "epoch:8 step:8028 [D loss: 0.582911, acc: 68.75%] [G loss: 2.320239]\n",
      "epoch:8 step:8029 [D loss: 0.622540, acc: 61.72%] [G loss: 2.206248]\n",
      "epoch:8 step:8030 [D loss: 0.619645, acc: 65.62%] [G loss: 2.328929]\n",
      "epoch:8 step:8031 [D loss: 0.595631, acc: 66.41%] [G loss: 2.041308]\n",
      "epoch:8 step:8032 [D loss: 0.536910, acc: 72.66%] [G loss: 2.187861]\n",
      "epoch:8 step:8033 [D loss: 0.656675, acc: 57.81%] [G loss: 2.077138]\n",
      "epoch:8 step:8034 [D loss: 0.641533, acc: 62.50%] [G loss: 2.107307]\n",
      "epoch:8 step:8035 [D loss: 0.655107, acc: 62.50%] [G loss: 1.955365]\n",
      "epoch:8 step:8036 [D loss: 0.622866, acc: 68.75%] [G loss: 2.210819]\n",
      "epoch:8 step:8037 [D loss: 0.580221, acc: 71.09%] [G loss: 2.073573]\n",
      "epoch:8 step:8038 [D loss: 0.665172, acc: 63.28%] [G loss: 2.083114]\n",
      "epoch:8 step:8039 [D loss: 0.647941, acc: 65.62%] [G loss: 1.989934]\n",
      "epoch:8 step:8040 [D loss: 0.656998, acc: 66.41%] [G loss: 2.075766]\n",
      "epoch:8 step:8041 [D loss: 0.597430, acc: 74.22%] [G loss: 2.489785]\n",
      "epoch:8 step:8042 [D loss: 0.630967, acc: 62.50%] [G loss: 2.146959]\n",
      "epoch:8 step:8043 [D loss: 0.561347, acc: 71.09%] [G loss: 2.408178]\n",
      "epoch:8 step:8044 [D loss: 0.613323, acc: 68.75%] [G loss: 2.180404]\n",
      "epoch:8 step:8045 [D loss: 0.580969, acc: 74.22%] [G loss: 2.385460]\n",
      "epoch:8 step:8046 [D loss: 0.580379, acc: 73.44%] [G loss: 2.331055]\n",
      "epoch:8 step:8047 [D loss: 0.558202, acc: 71.88%] [G loss: 2.605612]\n",
      "epoch:8 step:8048 [D loss: 0.547003, acc: 71.88%] [G loss: 2.457002]\n",
      "epoch:8 step:8049 [D loss: 0.608421, acc: 69.53%] [G loss: 2.100802]\n",
      "epoch:8 step:8050 [D loss: 0.565911, acc: 71.09%] [G loss: 2.508558]\n",
      "epoch:8 step:8051 [D loss: 0.526088, acc: 75.00%] [G loss: 2.549643]\n",
      "epoch:8 step:8052 [D loss: 0.568574, acc: 71.88%] [G loss: 2.271939]\n",
      "epoch:8 step:8053 [D loss: 0.618572, acc: 63.28%] [G loss: 2.537558]\n",
      "epoch:8 step:8054 [D loss: 0.545811, acc: 78.91%] [G loss: 2.494797]\n",
      "epoch:8 step:8055 [D loss: 0.654532, acc: 60.94%] [G loss: 2.095737]\n",
      "epoch:8 step:8056 [D loss: 0.697990, acc: 59.38%] [G loss: 2.169218]\n",
      "epoch:8 step:8057 [D loss: 0.618335, acc: 67.19%] [G loss: 2.247778]\n",
      "epoch:8 step:8058 [D loss: 0.653185, acc: 64.06%] [G loss: 1.937560]\n",
      "epoch:8 step:8059 [D loss: 0.600141, acc: 67.19%] [G loss: 2.195614]\n",
      "epoch:8 step:8060 [D loss: 0.515395, acc: 75.00%] [G loss: 2.276785]\n",
      "epoch:8 step:8061 [D loss: 0.704573, acc: 58.59%] [G loss: 1.982516]\n",
      "epoch:8 step:8062 [D loss: 0.694980, acc: 59.38%] [G loss: 2.173585]\n",
      "epoch:8 step:8063 [D loss: 0.633274, acc: 67.97%] [G loss: 2.054682]\n",
      "epoch:8 step:8064 [D loss: 0.593274, acc: 69.53%] [G loss: 2.033166]\n",
      "epoch:8 step:8065 [D loss: 0.622194, acc: 64.06%] [G loss: 2.192351]\n",
      "epoch:8 step:8066 [D loss: 0.578753, acc: 67.97%] [G loss: 2.088858]\n",
      "epoch:8 step:8067 [D loss: 0.627313, acc: 60.94%] [G loss: 2.069135]\n",
      "epoch:8 step:8068 [D loss: 0.625218, acc: 69.53%] [G loss: 2.118534]\n",
      "epoch:8 step:8069 [D loss: 0.663012, acc: 60.94%] [G loss: 2.116764]\n",
      "epoch:8 step:8070 [D loss: 0.601650, acc: 60.16%] [G loss: 2.169855]\n",
      "epoch:8 step:8071 [D loss: 0.565457, acc: 75.78%] [G loss: 2.224822]\n",
      "epoch:8 step:8072 [D loss: 0.662356, acc: 58.59%] [G loss: 1.895306]\n",
      "epoch:8 step:8073 [D loss: 0.694223, acc: 57.03%] [G loss: 2.057420]\n",
      "epoch:8 step:8074 [D loss: 0.631237, acc: 66.41%] [G loss: 2.063013]\n",
      "epoch:8 step:8075 [D loss: 0.606170, acc: 69.53%] [G loss: 1.943559]\n",
      "epoch:8 step:8076 [D loss: 0.651233, acc: 62.50%] [G loss: 2.121034]\n",
      "epoch:8 step:8077 [D loss: 0.616443, acc: 69.53%] [G loss: 2.254612]\n",
      "epoch:8 step:8078 [D loss: 0.545563, acc: 75.78%] [G loss: 2.084500]\n",
      "epoch:8 step:8079 [D loss: 0.645836, acc: 65.62%] [G loss: 2.194456]\n",
      "epoch:8 step:8080 [D loss: 0.647887, acc: 56.25%] [G loss: 2.135256]\n",
      "epoch:8 step:8081 [D loss: 0.644366, acc: 64.06%] [G loss: 2.040004]\n",
      "epoch:8 step:8082 [D loss: 0.593825, acc: 66.41%] [G loss: 2.233786]\n",
      "epoch:8 step:8083 [D loss: 0.581465, acc: 67.97%] [G loss: 2.091201]\n",
      "epoch:8 step:8084 [D loss: 0.602803, acc: 66.41%] [G loss: 2.107286]\n",
      "epoch:8 step:8085 [D loss: 0.588892, acc: 69.53%] [G loss: 2.263823]\n",
      "epoch:8 step:8086 [D loss: 0.626549, acc: 67.19%] [G loss: 2.151526]\n",
      "epoch:8 step:8087 [D loss: 0.592333, acc: 69.53%] [G loss: 2.392344]\n",
      "epoch:8 step:8088 [D loss: 0.629880, acc: 66.41%] [G loss: 2.196371]\n",
      "epoch:8 step:8089 [D loss: 0.653027, acc: 63.28%] [G loss: 2.157535]\n",
      "epoch:8 step:8090 [D loss: 0.680039, acc: 64.06%] [G loss: 1.971649]\n",
      "epoch:8 step:8091 [D loss: 0.652023, acc: 59.38%] [G loss: 2.043613]\n",
      "epoch:8 step:8092 [D loss: 0.646732, acc: 65.62%] [G loss: 2.134068]\n",
      "epoch:8 step:8093 [D loss: 0.699630, acc: 60.16%] [G loss: 2.124006]\n",
      "epoch:8 step:8094 [D loss: 0.577947, acc: 71.88%] [G loss: 2.156673]\n",
      "epoch:8 step:8095 [D loss: 0.636680, acc: 59.38%] [G loss: 2.003795]\n",
      "epoch:8 step:8096 [D loss: 0.666089, acc: 62.50%] [G loss: 2.115538]\n",
      "epoch:8 step:8097 [D loss: 0.587068, acc: 70.31%] [G loss: 2.186175]\n",
      "epoch:8 step:8098 [D loss: 0.597392, acc: 66.41%] [G loss: 2.348983]\n",
      "epoch:8 step:8099 [D loss: 0.599224, acc: 67.97%] [G loss: 2.373942]\n",
      "epoch:8 step:8100 [D loss: 0.665398, acc: 60.16%] [G loss: 1.926518]\n",
      "epoch:8 step:8101 [D loss: 0.621836, acc: 67.19%] [G loss: 2.255677]\n",
      "epoch:8 step:8102 [D loss: 0.698272, acc: 56.25%] [G loss: 2.012648]\n",
      "epoch:8 step:8103 [D loss: 0.652592, acc: 60.16%] [G loss: 2.094136]\n",
      "epoch:8 step:8104 [D loss: 0.651522, acc: 60.16%] [G loss: 2.120360]\n",
      "epoch:8 step:8105 [D loss: 0.662273, acc: 61.72%] [G loss: 2.171299]\n",
      "epoch:8 step:8106 [D loss: 0.661692, acc: 63.28%] [G loss: 2.066099]\n",
      "epoch:8 step:8107 [D loss: 0.627777, acc: 63.28%] [G loss: 1.942883]\n",
      "epoch:8 step:8108 [D loss: 0.637397, acc: 60.94%] [G loss: 2.149356]\n",
      "epoch:8 step:8109 [D loss: 0.561642, acc: 75.00%] [G loss: 2.091666]\n",
      "epoch:8 step:8110 [D loss: 0.623304, acc: 65.62%] [G loss: 2.105126]\n",
      "epoch:8 step:8111 [D loss: 0.645588, acc: 64.06%] [G loss: 1.915079]\n",
      "epoch:8 step:8112 [D loss: 0.719316, acc: 54.69%] [G loss: 2.037620]\n",
      "epoch:8 step:8113 [D loss: 0.686064, acc: 59.38%] [G loss: 2.022839]\n",
      "epoch:8 step:8114 [D loss: 0.555952, acc: 71.09%] [G loss: 2.135458]\n",
      "epoch:8 step:8115 [D loss: 0.643391, acc: 60.94%] [G loss: 1.958429]\n",
      "epoch:8 step:8116 [D loss: 0.664810, acc: 59.38%] [G loss: 1.966091]\n",
      "epoch:8 step:8117 [D loss: 0.628276, acc: 65.62%] [G loss: 1.897943]\n",
      "epoch:8 step:8118 [D loss: 0.611039, acc: 64.84%] [G loss: 1.949360]\n",
      "epoch:8 step:8119 [D loss: 0.639975, acc: 63.28%] [G loss: 2.210192]\n",
      "epoch:8 step:8120 [D loss: 0.596284, acc: 68.75%] [G loss: 2.185539]\n",
      "epoch:8 step:8121 [D loss: 0.652728, acc: 63.28%] [G loss: 2.049548]\n",
      "epoch:8 step:8122 [D loss: 0.653767, acc: 64.06%] [G loss: 2.101623]\n",
      "epoch:8 step:8123 [D loss: 0.605464, acc: 71.09%] [G loss: 2.106097]\n",
      "epoch:8 step:8124 [D loss: 0.589509, acc: 68.75%] [G loss: 2.236735]\n",
      "epoch:8 step:8125 [D loss: 0.675364, acc: 62.50%] [G loss: 2.172727]\n",
      "epoch:8 step:8126 [D loss: 0.632309, acc: 63.28%] [G loss: 2.277066]\n",
      "epoch:8 step:8127 [D loss: 0.598531, acc: 66.41%] [G loss: 2.183983]\n",
      "epoch:8 step:8128 [D loss: 0.549612, acc: 73.44%] [G loss: 2.374718]\n",
      "epoch:8 step:8129 [D loss: 0.606163, acc: 69.53%] [G loss: 2.142556]\n",
      "epoch:8 step:8130 [D loss: 0.565773, acc: 68.75%] [G loss: 2.365037]\n",
      "epoch:8 step:8131 [D loss: 0.584743, acc: 71.09%] [G loss: 2.585231]\n",
      "epoch:8 step:8132 [D loss: 0.592304, acc: 68.75%] [G loss: 2.250957]\n",
      "epoch:8 step:8133 [D loss: 0.638133, acc: 64.06%] [G loss: 2.314864]\n",
      "epoch:8 step:8134 [D loss: 0.611831, acc: 70.31%] [G loss: 2.349782]\n",
      "epoch:8 step:8135 [D loss: 0.567160, acc: 76.56%] [G loss: 2.178826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8136 [D loss: 0.601260, acc: 68.75%] [G loss: 2.229671]\n",
      "epoch:8 step:8137 [D loss: 0.623691, acc: 64.06%] [G loss: 2.428273]\n",
      "epoch:8 step:8138 [D loss: 0.554032, acc: 70.31%] [G loss: 2.390988]\n",
      "epoch:8 step:8139 [D loss: 0.621258, acc: 63.28%] [G loss: 2.144485]\n",
      "epoch:8 step:8140 [D loss: 0.579732, acc: 65.62%] [G loss: 2.271752]\n",
      "epoch:8 step:8141 [D loss: 0.614490, acc: 63.28%] [G loss: 2.322118]\n",
      "epoch:8 step:8142 [D loss: 0.582631, acc: 69.53%] [G loss: 2.222146]\n",
      "epoch:8 step:8143 [D loss: 0.605730, acc: 68.75%] [G loss: 2.381945]\n",
      "epoch:8 step:8144 [D loss: 0.578033, acc: 68.75%] [G loss: 2.530851]\n",
      "epoch:8 step:8145 [D loss: 0.528298, acc: 73.44%] [G loss: 2.478570]\n",
      "epoch:8 step:8146 [D loss: 0.552184, acc: 71.88%] [G loss: 2.548196]\n",
      "epoch:8 step:8147 [D loss: 0.659203, acc: 63.28%] [G loss: 2.289755]\n",
      "epoch:8 step:8148 [D loss: 0.606181, acc: 68.75%] [G loss: 2.179820]\n",
      "epoch:8 step:8149 [D loss: 0.665095, acc: 61.72%] [G loss: 2.413461]\n",
      "epoch:8 step:8150 [D loss: 0.570712, acc: 69.53%] [G loss: 2.179968]\n",
      "epoch:8 step:8151 [D loss: 0.630508, acc: 64.84%] [G loss: 2.222308]\n",
      "epoch:8 step:8152 [D loss: 0.655459, acc: 60.16%] [G loss: 2.266644]\n",
      "epoch:8 step:8153 [D loss: 0.693854, acc: 57.81%] [G loss: 2.136502]\n",
      "epoch:8 step:8154 [D loss: 0.679562, acc: 61.72%] [G loss: 2.085387]\n",
      "epoch:8 step:8155 [D loss: 0.671045, acc: 57.03%] [G loss: 2.124898]\n",
      "epoch:8 step:8156 [D loss: 0.605185, acc: 64.84%] [G loss: 2.142256]\n",
      "epoch:8 step:8157 [D loss: 0.619399, acc: 67.97%] [G loss: 2.085886]\n",
      "epoch:8 step:8158 [D loss: 0.640509, acc: 66.41%] [G loss: 2.157839]\n",
      "epoch:8 step:8159 [D loss: 0.617313, acc: 63.28%] [G loss: 2.340082]\n",
      "epoch:8 step:8160 [D loss: 0.668500, acc: 61.72%] [G loss: 2.204101]\n",
      "epoch:8 step:8161 [D loss: 0.651545, acc: 62.50%] [G loss: 2.200170]\n",
      "epoch:8 step:8162 [D loss: 0.578701, acc: 70.31%] [G loss: 2.071579]\n",
      "epoch:8 step:8163 [D loss: 0.666166, acc: 61.72%] [G loss: 2.046141]\n",
      "epoch:8 step:8164 [D loss: 0.691831, acc: 56.25%] [G loss: 2.125495]\n",
      "epoch:8 step:8165 [D loss: 0.619889, acc: 66.41%] [G loss: 1.960546]\n",
      "epoch:8 step:8166 [D loss: 0.591794, acc: 71.09%] [G loss: 2.165415]\n",
      "epoch:8 step:8167 [D loss: 0.641274, acc: 64.06%] [G loss: 2.008512]\n",
      "epoch:8 step:8168 [D loss: 0.615217, acc: 67.19%] [G loss: 2.101637]\n",
      "epoch:8 step:8169 [D loss: 0.640126, acc: 64.06%] [G loss: 1.985755]\n",
      "epoch:8 step:8170 [D loss: 0.620702, acc: 70.31%] [G loss: 2.025587]\n",
      "epoch:8 step:8171 [D loss: 0.644466, acc: 66.41%] [G loss: 2.056852]\n",
      "epoch:8 step:8172 [D loss: 0.631368, acc: 65.62%] [G loss: 2.122774]\n",
      "epoch:8 step:8173 [D loss: 0.604779, acc: 65.62%] [G loss: 2.230243]\n",
      "epoch:8 step:8174 [D loss: 0.689727, acc: 60.16%] [G loss: 2.186561]\n",
      "epoch:8 step:8175 [D loss: 0.632635, acc: 61.72%] [G loss: 2.351983]\n",
      "epoch:8 step:8176 [D loss: 0.689771, acc: 59.38%] [G loss: 2.085318]\n",
      "epoch:8 step:8177 [D loss: 0.669624, acc: 61.72%] [G loss: 2.196300]\n",
      "epoch:8 step:8178 [D loss: 0.655114, acc: 60.16%] [G loss: 2.108048]\n",
      "epoch:8 step:8179 [D loss: 0.620215, acc: 65.62%] [G loss: 1.973902]\n",
      "epoch:8 step:8180 [D loss: 0.612216, acc: 67.97%] [G loss: 1.872179]\n",
      "epoch:8 step:8181 [D loss: 0.635204, acc: 62.50%] [G loss: 2.026839]\n",
      "epoch:8 step:8182 [D loss: 0.623259, acc: 65.62%] [G loss: 2.341161]\n",
      "epoch:8 step:8183 [D loss: 0.648677, acc: 64.06%] [G loss: 2.075497]\n",
      "epoch:8 step:8184 [D loss: 0.615505, acc: 64.84%] [G loss: 2.108260]\n",
      "epoch:8 step:8185 [D loss: 0.638858, acc: 63.28%] [G loss: 2.141394]\n",
      "epoch:8 step:8186 [D loss: 0.644540, acc: 65.62%] [G loss: 2.273027]\n",
      "epoch:8 step:8187 [D loss: 0.647401, acc: 60.94%] [G loss: 2.406540]\n",
      "epoch:8 step:8188 [D loss: 0.554753, acc: 73.44%] [G loss: 2.363199]\n",
      "epoch:8 step:8189 [D loss: 0.606791, acc: 73.44%] [G loss: 2.333155]\n",
      "epoch:8 step:8190 [D loss: 0.653337, acc: 66.41%] [G loss: 2.317293]\n",
      "epoch:8 step:8191 [D loss: 0.564599, acc: 66.41%] [G loss: 2.314713]\n",
      "epoch:8 step:8192 [D loss: 0.718141, acc: 57.81%] [G loss: 1.957594]\n",
      "epoch:8 step:8193 [D loss: 0.560149, acc: 71.09%] [G loss: 2.076840]\n",
      "epoch:8 step:8194 [D loss: 0.660824, acc: 59.38%] [G loss: 2.108580]\n",
      "epoch:8 step:8195 [D loss: 0.620134, acc: 66.41%] [G loss: 2.257831]\n",
      "epoch:8 step:8196 [D loss: 0.599831, acc: 67.19%] [G loss: 2.174281]\n",
      "epoch:8 step:8197 [D loss: 0.585504, acc: 71.88%] [G loss: 2.207151]\n",
      "epoch:8 step:8198 [D loss: 0.650984, acc: 60.94%] [G loss: 2.066912]\n",
      "epoch:8 step:8199 [D loss: 0.615528, acc: 67.97%] [G loss: 1.976227]\n",
      "epoch:8 step:8200 [D loss: 0.623140, acc: 67.19%] [G loss: 2.151150]\n",
      "##############\n",
      "[2.53196867 1.42858846 6.42426809 4.90820467 3.84455226 5.62437684\n",
      " 4.47210016 4.72165846 4.88533549 3.53633766]\n",
      "##########\n",
      "epoch:8 step:8201 [D loss: 0.573965, acc: 71.09%] [G loss: 2.122243]\n",
      "epoch:8 step:8202 [D loss: 0.565192, acc: 71.09%] [G loss: 2.295555]\n",
      "epoch:8 step:8203 [D loss: 0.624421, acc: 66.41%] [G loss: 2.520254]\n",
      "epoch:8 step:8204 [D loss: 0.562347, acc: 74.22%] [G loss: 2.434691]\n",
      "epoch:8 step:8205 [D loss: 0.613343, acc: 60.94%] [G loss: 2.446643]\n",
      "epoch:8 step:8206 [D loss: 0.653045, acc: 56.25%] [G loss: 2.027262]\n",
      "epoch:8 step:8207 [D loss: 0.674839, acc: 60.94%] [G loss: 2.044717]\n",
      "epoch:8 step:8208 [D loss: 0.573986, acc: 70.31%] [G loss: 2.159456]\n",
      "epoch:8 step:8209 [D loss: 0.623236, acc: 64.84%] [G loss: 2.117331]\n",
      "epoch:8 step:8210 [D loss: 0.574693, acc: 73.44%] [G loss: 2.137842]\n",
      "epoch:8 step:8211 [D loss: 0.641137, acc: 64.84%] [G loss: 2.119307]\n",
      "epoch:8 step:8212 [D loss: 0.672228, acc: 60.94%] [G loss: 1.991902]\n",
      "epoch:8 step:8213 [D loss: 0.636748, acc: 62.50%] [G loss: 2.045511]\n",
      "epoch:8 step:8214 [D loss: 0.589482, acc: 67.97%] [G loss: 2.228067]\n",
      "epoch:8 step:8215 [D loss: 0.596498, acc: 69.53%] [G loss: 2.386298]\n",
      "epoch:8 step:8216 [D loss: 0.622703, acc: 66.41%] [G loss: 2.085013]\n",
      "epoch:8 step:8217 [D loss: 0.652276, acc: 62.50%] [G loss: 2.281552]\n",
      "epoch:8 step:8218 [D loss: 0.655817, acc: 62.50%] [G loss: 2.238858]\n",
      "epoch:8 step:8219 [D loss: 0.632527, acc: 65.62%] [G loss: 2.064576]\n",
      "epoch:8 step:8220 [D loss: 0.537670, acc: 71.88%] [G loss: 2.450825]\n",
      "epoch:8 step:8221 [D loss: 0.594542, acc: 67.97%] [G loss: 2.265271]\n",
      "epoch:8 step:8222 [D loss: 0.642145, acc: 61.72%] [G loss: 2.333384]\n",
      "epoch:8 step:8223 [D loss: 0.607107, acc: 61.72%] [G loss: 2.090507]\n",
      "epoch:8 step:8224 [D loss: 0.609338, acc: 66.41%] [G loss: 2.222718]\n",
      "epoch:8 step:8225 [D loss: 0.609836, acc: 69.53%] [G loss: 2.369961]\n",
      "epoch:8 step:8226 [D loss: 0.589405, acc: 69.53%] [G loss: 2.190158]\n",
      "epoch:8 step:8227 [D loss: 0.627372, acc: 68.75%] [G loss: 2.236317]\n",
      "epoch:8 step:8228 [D loss: 0.661451, acc: 61.72%] [G loss: 2.268337]\n",
      "epoch:8 step:8229 [D loss: 0.584542, acc: 67.19%] [G loss: 2.244629]\n",
      "epoch:8 step:8230 [D loss: 0.692829, acc: 60.16%] [G loss: 2.047426]\n",
      "epoch:8 step:8231 [D loss: 0.648350, acc: 64.06%] [G loss: 2.061927]\n",
      "epoch:8 step:8232 [D loss: 0.626545, acc: 67.97%] [G loss: 2.149666]\n",
      "epoch:8 step:8233 [D loss: 0.641531, acc: 61.72%] [G loss: 1.985510]\n",
      "epoch:8 step:8234 [D loss: 0.641657, acc: 63.28%] [G loss: 1.994362]\n",
      "epoch:8 step:8235 [D loss: 0.637540, acc: 64.06%] [G loss: 2.114050]\n",
      "epoch:8 step:8236 [D loss: 0.647272, acc: 64.06%] [G loss: 2.109803]\n",
      "epoch:8 step:8237 [D loss: 0.634968, acc: 64.06%] [G loss: 2.127960]\n",
      "epoch:8 step:8238 [D loss: 0.680371, acc: 56.25%] [G loss: 1.995451]\n",
      "epoch:8 step:8239 [D loss: 0.623515, acc: 68.75%] [G loss: 2.134219]\n",
      "epoch:8 step:8240 [D loss: 0.638995, acc: 66.41%] [G loss: 2.072246]\n",
      "epoch:8 step:8241 [D loss: 0.658768, acc: 60.16%] [G loss: 2.115528]\n",
      "epoch:8 step:8242 [D loss: 0.576551, acc: 67.19%] [G loss: 2.276511]\n",
      "epoch:8 step:8243 [D loss: 0.677778, acc: 58.59%] [G loss: 2.245676]\n",
      "epoch:8 step:8244 [D loss: 0.600313, acc: 64.84%] [G loss: 2.017135]\n",
      "epoch:8 step:8245 [D loss: 0.688446, acc: 53.12%] [G loss: 2.108298]\n",
      "epoch:8 step:8246 [D loss: 0.668645, acc: 57.81%] [G loss: 1.950515]\n",
      "epoch:8 step:8247 [D loss: 0.652844, acc: 62.50%] [G loss: 1.931425]\n",
      "epoch:8 step:8248 [D loss: 0.663769, acc: 60.94%] [G loss: 2.116778]\n",
      "epoch:8 step:8249 [D loss: 0.636195, acc: 65.62%] [G loss: 2.042391]\n",
      "epoch:8 step:8250 [D loss: 0.651042, acc: 63.28%] [G loss: 2.143960]\n",
      "epoch:8 step:8251 [D loss: 0.643060, acc: 63.28%] [G loss: 2.172991]\n",
      "epoch:8 step:8252 [D loss: 0.621435, acc: 62.50%] [G loss: 2.115876]\n",
      "epoch:8 step:8253 [D loss: 0.663793, acc: 57.81%] [G loss: 2.108976]\n",
      "epoch:8 step:8254 [D loss: 0.657140, acc: 63.28%] [G loss: 2.103026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8255 [D loss: 0.646679, acc: 61.72%] [G loss: 2.086960]\n",
      "epoch:8 step:8256 [D loss: 0.594348, acc: 70.31%] [G loss: 2.217750]\n",
      "epoch:8 step:8257 [D loss: 0.637242, acc: 64.84%] [G loss: 2.026824]\n",
      "epoch:8 step:8258 [D loss: 0.674245, acc: 55.47%] [G loss: 1.926692]\n",
      "epoch:8 step:8259 [D loss: 0.609081, acc: 67.97%] [G loss: 2.061408]\n",
      "epoch:8 step:8260 [D loss: 0.592476, acc: 65.62%] [G loss: 2.110795]\n",
      "epoch:8 step:8261 [D loss: 0.756218, acc: 53.91%] [G loss: 1.814705]\n",
      "epoch:8 step:8262 [D loss: 0.651577, acc: 60.94%] [G loss: 1.857361]\n",
      "epoch:8 step:8263 [D loss: 0.604026, acc: 65.62%] [G loss: 2.020084]\n",
      "epoch:8 step:8264 [D loss: 0.642514, acc: 60.94%] [G loss: 2.030996]\n",
      "epoch:8 step:8265 [D loss: 0.609307, acc: 66.41%] [G loss: 2.211046]\n",
      "epoch:8 step:8266 [D loss: 0.609066, acc: 65.62%] [G loss: 1.965024]\n",
      "epoch:8 step:8267 [D loss: 0.621581, acc: 66.41%] [G loss: 2.039915]\n",
      "epoch:8 step:8268 [D loss: 0.659953, acc: 60.94%] [G loss: 2.314282]\n",
      "epoch:8 step:8269 [D loss: 0.638781, acc: 62.50%] [G loss: 1.988613]\n",
      "epoch:8 step:8270 [D loss: 0.580220, acc: 72.66%] [G loss: 2.384556]\n",
      "epoch:8 step:8271 [D loss: 0.590096, acc: 68.75%] [G loss: 2.532645]\n",
      "epoch:8 step:8272 [D loss: 0.656218, acc: 60.16%] [G loss: 2.019018]\n",
      "epoch:8 step:8273 [D loss: 0.605965, acc: 65.62%] [G loss: 2.191816]\n",
      "epoch:8 step:8274 [D loss: 0.610503, acc: 65.62%] [G loss: 2.081012]\n",
      "epoch:8 step:8275 [D loss: 0.692370, acc: 55.47%] [G loss: 2.052356]\n",
      "epoch:8 step:8276 [D loss: 0.640059, acc: 67.97%] [G loss: 2.101532]\n",
      "epoch:8 step:8277 [D loss: 0.595961, acc: 71.09%] [G loss: 2.214857]\n",
      "epoch:8 step:8278 [D loss: 0.588418, acc: 66.41%] [G loss: 2.225070]\n",
      "epoch:8 step:8279 [D loss: 0.598527, acc: 69.53%] [G loss: 2.202454]\n",
      "epoch:8 step:8280 [D loss: 0.654782, acc: 64.06%] [G loss: 2.071029]\n",
      "epoch:8 step:8281 [D loss: 0.660342, acc: 60.94%] [G loss: 2.119108]\n",
      "epoch:8 step:8282 [D loss: 0.536912, acc: 74.22%] [G loss: 2.335835]\n",
      "epoch:8 step:8283 [D loss: 0.640851, acc: 64.06%] [G loss: 2.161882]\n",
      "epoch:8 step:8284 [D loss: 0.665608, acc: 65.62%] [G loss: 2.090286]\n",
      "epoch:8 step:8285 [D loss: 0.656089, acc: 62.50%] [G loss: 2.007071]\n",
      "epoch:8 step:8286 [D loss: 0.605903, acc: 71.09%] [G loss: 2.016979]\n",
      "epoch:8 step:8287 [D loss: 0.653580, acc: 58.59%] [G loss: 1.976331]\n",
      "epoch:8 step:8288 [D loss: 0.547726, acc: 73.44%] [G loss: 2.432717]\n",
      "epoch:8 step:8289 [D loss: 0.684600, acc: 58.59%] [G loss: 2.258678]\n",
      "epoch:8 step:8290 [D loss: 0.646432, acc: 58.59%] [G loss: 2.142981]\n",
      "epoch:8 step:8291 [D loss: 0.572320, acc: 65.62%] [G loss: 2.374306]\n",
      "epoch:8 step:8292 [D loss: 0.632957, acc: 66.41%] [G loss: 2.133673]\n",
      "epoch:8 step:8293 [D loss: 0.695214, acc: 52.34%] [G loss: 1.951674]\n",
      "epoch:8 step:8294 [D loss: 0.607828, acc: 64.84%] [G loss: 2.115358]\n",
      "epoch:8 step:8295 [D loss: 0.681923, acc: 59.38%] [G loss: 2.048681]\n",
      "epoch:8 step:8296 [D loss: 0.671069, acc: 61.72%] [G loss: 1.967105]\n",
      "epoch:8 step:8297 [D loss: 0.622066, acc: 63.28%] [G loss: 2.022506]\n",
      "epoch:8 step:8298 [D loss: 0.668141, acc: 54.69%] [G loss: 1.941462]\n",
      "epoch:8 step:8299 [D loss: 0.603698, acc: 67.19%] [G loss: 1.995821]\n",
      "epoch:8 step:8300 [D loss: 0.651886, acc: 61.72%] [G loss: 1.865012]\n",
      "epoch:8 step:8301 [D loss: 0.568255, acc: 69.53%] [G loss: 2.041238]\n",
      "epoch:8 step:8302 [D loss: 0.586718, acc: 72.66%] [G loss: 2.238755]\n",
      "epoch:8 step:8303 [D loss: 0.543939, acc: 75.78%] [G loss: 2.277721]\n",
      "epoch:8 step:8304 [D loss: 0.626683, acc: 58.59%] [G loss: 2.082361]\n",
      "epoch:8 step:8305 [D loss: 0.643704, acc: 66.41%] [G loss: 2.437374]\n",
      "epoch:8 step:8306 [D loss: 0.638821, acc: 64.06%] [G loss: 2.067982]\n",
      "epoch:8 step:8307 [D loss: 0.548882, acc: 74.22%] [G loss: 2.099910]\n",
      "epoch:8 step:8308 [D loss: 0.698725, acc: 56.25%] [G loss: 1.982015]\n",
      "epoch:8 step:8309 [D loss: 0.607317, acc: 66.41%] [G loss: 2.201636]\n",
      "epoch:8 step:8310 [D loss: 0.590123, acc: 67.19%] [G loss: 2.208394]\n",
      "epoch:8 step:8311 [D loss: 0.619745, acc: 68.75%] [G loss: 2.485410]\n",
      "epoch:8 step:8312 [D loss: 0.536700, acc: 72.66%] [G loss: 2.233608]\n",
      "epoch:8 step:8313 [D loss: 0.607914, acc: 68.75%] [G loss: 2.136692]\n",
      "epoch:8 step:8314 [D loss: 0.642874, acc: 64.06%] [G loss: 1.991665]\n",
      "epoch:8 step:8315 [D loss: 0.647883, acc: 60.94%] [G loss: 2.104922]\n",
      "epoch:8 step:8316 [D loss: 0.690723, acc: 58.59%] [G loss: 2.029871]\n",
      "epoch:8 step:8317 [D loss: 0.626774, acc: 62.50%] [G loss: 2.079129]\n",
      "epoch:8 step:8318 [D loss: 0.577430, acc: 64.84%] [G loss: 2.218309]\n",
      "epoch:8 step:8319 [D loss: 0.577860, acc: 70.31%] [G loss: 2.120189]\n",
      "epoch:8 step:8320 [D loss: 0.680042, acc: 59.38%] [G loss: 1.994416]\n",
      "epoch:8 step:8321 [D loss: 0.623514, acc: 64.06%] [G loss: 2.155552]\n",
      "epoch:8 step:8322 [D loss: 0.691178, acc: 58.59%] [G loss: 1.973347]\n",
      "epoch:8 step:8323 [D loss: 0.665239, acc: 57.03%] [G loss: 1.828330]\n",
      "epoch:8 step:8324 [D loss: 0.631329, acc: 61.72%] [G loss: 2.053727]\n",
      "epoch:8 step:8325 [D loss: 0.640717, acc: 63.28%] [G loss: 2.085623]\n",
      "epoch:8 step:8326 [D loss: 0.622673, acc: 72.66%] [G loss: 2.016423]\n",
      "epoch:8 step:8327 [D loss: 0.680124, acc: 60.16%] [G loss: 2.404596]\n",
      "epoch:8 step:8328 [D loss: 0.608693, acc: 68.75%] [G loss: 2.080795]\n",
      "epoch:8 step:8329 [D loss: 0.589862, acc: 69.53%] [G loss: 2.202104]\n",
      "epoch:8 step:8330 [D loss: 0.580499, acc: 71.88%] [G loss: 2.127100]\n",
      "epoch:8 step:8331 [D loss: 0.604782, acc: 67.19%] [G loss: 2.039488]\n",
      "epoch:8 step:8332 [D loss: 0.656986, acc: 59.38%] [G loss: 2.042155]\n",
      "epoch:8 step:8333 [D loss: 0.586359, acc: 68.75%] [G loss: 2.098572]\n",
      "epoch:8 step:8334 [D loss: 0.578249, acc: 67.97%] [G loss: 2.135576]\n",
      "epoch:8 step:8335 [D loss: 0.671957, acc: 59.38%] [G loss: 2.109000]\n",
      "epoch:8 step:8336 [D loss: 0.591260, acc: 66.41%] [G loss: 2.090788]\n",
      "epoch:8 step:8337 [D loss: 0.569754, acc: 71.09%] [G loss: 2.052421]\n",
      "epoch:8 step:8338 [D loss: 0.597818, acc: 66.41%] [G loss: 2.131391]\n",
      "epoch:8 step:8339 [D loss: 0.533146, acc: 72.66%] [G loss: 2.311350]\n",
      "epoch:8 step:8340 [D loss: 0.547573, acc: 71.88%] [G loss: 2.299311]\n",
      "epoch:8 step:8341 [D loss: 0.589906, acc: 67.19%] [G loss: 2.364570]\n",
      "epoch:8 step:8342 [D loss: 0.573343, acc: 72.66%] [G loss: 2.331309]\n",
      "epoch:8 step:8343 [D loss: 0.620374, acc: 69.53%] [G loss: 2.352104]\n",
      "epoch:8 step:8344 [D loss: 0.662630, acc: 62.50%] [G loss: 2.215862]\n",
      "epoch:8 step:8345 [D loss: 0.605281, acc: 67.97%] [G loss: 2.113419]\n",
      "epoch:8 step:8346 [D loss: 0.717079, acc: 53.91%] [G loss: 1.978202]\n",
      "epoch:8 step:8347 [D loss: 0.660456, acc: 59.38%] [G loss: 1.855941]\n",
      "epoch:8 step:8348 [D loss: 0.587147, acc: 71.88%] [G loss: 2.179362]\n",
      "epoch:8 step:8349 [D loss: 0.609852, acc: 66.41%] [G loss: 2.288351]\n",
      "epoch:8 step:8350 [D loss: 0.652165, acc: 64.06%] [G loss: 2.232517]\n",
      "epoch:8 step:8351 [D loss: 0.672645, acc: 62.50%] [G loss: 2.053429]\n",
      "epoch:8 step:8352 [D loss: 0.688274, acc: 59.38%] [G loss: 1.920604]\n",
      "epoch:8 step:8353 [D loss: 0.669226, acc: 63.28%] [G loss: 2.057480]\n",
      "epoch:8 step:8354 [D loss: 0.647222, acc: 62.50%] [G loss: 2.001822]\n",
      "epoch:8 step:8355 [D loss: 0.690914, acc: 58.59%] [G loss: 1.983113]\n",
      "epoch:8 step:8356 [D loss: 0.581743, acc: 74.22%] [G loss: 2.159178]\n",
      "epoch:8 step:8357 [D loss: 0.608470, acc: 69.53%] [G loss: 2.030337]\n",
      "epoch:8 step:8358 [D loss: 0.660911, acc: 60.16%] [G loss: 2.106679]\n",
      "epoch:8 step:8359 [D loss: 0.620955, acc: 66.41%] [G loss: 2.184157]\n",
      "epoch:8 step:8360 [D loss: 0.595061, acc: 65.62%] [G loss: 2.154431]\n",
      "epoch:8 step:8361 [D loss: 0.601346, acc: 67.97%] [G loss: 2.084811]\n",
      "epoch:8 step:8362 [D loss: 0.654430, acc: 60.16%] [G loss: 2.209980]\n",
      "epoch:8 step:8363 [D loss: 0.633528, acc: 60.94%] [G loss: 2.006262]\n",
      "epoch:8 step:8364 [D loss: 0.642972, acc: 61.72%] [G loss: 2.005881]\n",
      "epoch:8 step:8365 [D loss: 0.647357, acc: 66.41%] [G loss: 1.972249]\n",
      "epoch:8 step:8366 [D loss: 0.592472, acc: 71.09%] [G loss: 2.108696]\n",
      "epoch:8 step:8367 [D loss: 0.617218, acc: 66.41%] [G loss: 2.059772]\n",
      "epoch:8 step:8368 [D loss: 0.611437, acc: 67.19%] [G loss: 2.251152]\n",
      "epoch:8 step:8369 [D loss: 0.716421, acc: 53.91%] [G loss: 1.987634]\n",
      "epoch:8 step:8370 [D loss: 0.651085, acc: 62.50%] [G loss: 1.929307]\n",
      "epoch:8 step:8371 [D loss: 0.587707, acc: 72.66%] [G loss: 2.070609]\n",
      "epoch:8 step:8372 [D loss: 0.608659, acc: 67.19%] [G loss: 2.055177]\n",
      "epoch:8 step:8373 [D loss: 0.618115, acc: 71.88%] [G loss: 2.162404]\n",
      "epoch:8 step:8374 [D loss: 0.694558, acc: 55.47%] [G loss: 2.146078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8375 [D loss: 0.667137, acc: 64.06%] [G loss: 2.054370]\n",
      "epoch:8 step:8376 [D loss: 0.656040, acc: 58.59%] [G loss: 1.959563]\n",
      "epoch:8 step:8377 [D loss: 0.602868, acc: 67.97%] [G loss: 2.101318]\n",
      "epoch:8 step:8378 [D loss: 0.617931, acc: 67.19%] [G loss: 2.027176]\n",
      "epoch:8 step:8379 [D loss: 0.607245, acc: 66.41%] [G loss: 2.096132]\n",
      "epoch:8 step:8380 [D loss: 0.581263, acc: 69.53%] [G loss: 2.363688]\n",
      "epoch:8 step:8381 [D loss: 0.609913, acc: 67.97%] [G loss: 2.095984]\n",
      "epoch:8 step:8382 [D loss: 0.681340, acc: 56.25%] [G loss: 2.459687]\n",
      "epoch:8 step:8383 [D loss: 0.550225, acc: 74.22%] [G loss: 2.200195]\n",
      "epoch:8 step:8384 [D loss: 0.656078, acc: 63.28%] [G loss: 2.302557]\n",
      "epoch:8 step:8385 [D loss: 0.597683, acc: 64.06%] [G loss: 2.131274]\n",
      "epoch:8 step:8386 [D loss: 0.591684, acc: 65.62%] [G loss: 2.469217]\n",
      "epoch:8 step:8387 [D loss: 0.664386, acc: 62.50%] [G loss: 2.064612]\n",
      "epoch:8 step:8388 [D loss: 0.670874, acc: 57.03%] [G loss: 1.938266]\n",
      "epoch:8 step:8389 [D loss: 0.633367, acc: 67.19%] [G loss: 2.126815]\n",
      "epoch:8 step:8390 [D loss: 0.586214, acc: 68.75%] [G loss: 2.120388]\n",
      "epoch:8 step:8391 [D loss: 0.617966, acc: 64.84%] [G loss: 2.063033]\n",
      "epoch:8 step:8392 [D loss: 0.625704, acc: 61.72%] [G loss: 2.048309]\n",
      "epoch:8 step:8393 [D loss: 0.627617, acc: 66.41%] [G loss: 2.176795]\n",
      "epoch:8 step:8394 [D loss: 0.651490, acc: 59.38%] [G loss: 2.064572]\n",
      "epoch:8 step:8395 [D loss: 0.553791, acc: 72.66%] [G loss: 2.341413]\n",
      "epoch:8 step:8396 [D loss: 0.611987, acc: 69.53%] [G loss: 2.260831]\n",
      "epoch:8 step:8397 [D loss: 0.593473, acc: 65.62%] [G loss: 2.086705]\n",
      "epoch:8 step:8398 [D loss: 0.656576, acc: 57.81%] [G loss: 2.136957]\n",
      "epoch:8 step:8399 [D loss: 0.675312, acc: 61.72%] [G loss: 2.177617]\n",
      "epoch:8 step:8400 [D loss: 0.767624, acc: 53.91%] [G loss: 2.169441]\n",
      "##############\n",
      "[2.4928195  1.24230194 6.39815004 4.84829784 3.90818791 5.78339874\n",
      " 4.50344397 4.90194922 4.74381421 3.4931896 ]\n",
      "##########\n",
      "epoch:8 step:8401 [D loss: 0.586043, acc: 70.31%] [G loss: 2.043919]\n",
      "epoch:8 step:8402 [D loss: 0.610885, acc: 69.53%] [G loss: 2.283897]\n",
      "epoch:8 step:8403 [D loss: 0.594269, acc: 69.53%] [G loss: 2.180063]\n",
      "epoch:8 step:8404 [D loss: 0.629051, acc: 64.06%] [G loss: 2.140651]\n",
      "epoch:8 step:8405 [D loss: 0.564734, acc: 69.53%] [G loss: 2.337214]\n",
      "epoch:8 step:8406 [D loss: 0.636840, acc: 60.94%] [G loss: 2.124151]\n",
      "epoch:8 step:8407 [D loss: 0.617125, acc: 66.41%] [G loss: 2.162125]\n",
      "epoch:8 step:8408 [D loss: 0.587379, acc: 66.41%] [G loss: 2.542367]\n",
      "epoch:8 step:8409 [D loss: 0.670207, acc: 59.38%] [G loss: 2.112596]\n",
      "epoch:8 step:8410 [D loss: 0.644749, acc: 64.84%] [G loss: 2.032915]\n",
      "epoch:8 step:8411 [D loss: 0.585166, acc: 67.97%] [G loss: 2.016425]\n",
      "epoch:8 step:8412 [D loss: 0.620626, acc: 69.53%] [G loss: 2.160675]\n",
      "epoch:8 step:8413 [D loss: 0.622988, acc: 65.62%] [G loss: 2.242907]\n",
      "epoch:8 step:8414 [D loss: 0.652655, acc: 68.75%] [G loss: 2.287439]\n",
      "epoch:8 step:8415 [D loss: 0.557589, acc: 73.44%] [G loss: 2.345981]\n",
      "epoch:8 step:8416 [D loss: 0.738015, acc: 53.91%] [G loss: 2.101991]\n",
      "epoch:8 step:8417 [D loss: 0.647384, acc: 60.94%] [G loss: 2.150390]\n",
      "epoch:8 step:8418 [D loss: 0.619317, acc: 67.97%] [G loss: 2.194774]\n",
      "epoch:8 step:8419 [D loss: 0.576581, acc: 71.88%] [G loss: 2.267773]\n",
      "epoch:8 step:8420 [D loss: 0.518313, acc: 75.00%] [G loss: 2.464751]\n",
      "epoch:8 step:8421 [D loss: 0.548556, acc: 73.44%] [G loss: 2.431569]\n",
      "epoch:8 step:8422 [D loss: 0.624371, acc: 64.84%] [G loss: 2.362742]\n",
      "epoch:8 step:8423 [D loss: 0.598364, acc: 70.31%] [G loss: 2.330146]\n",
      "epoch:8 step:8424 [D loss: 0.694200, acc: 59.38%] [G loss: 1.959843]\n",
      "epoch:8 step:8425 [D loss: 0.665911, acc: 59.38%] [G loss: 2.251750]\n",
      "epoch:8 step:8426 [D loss: 0.598811, acc: 66.41%] [G loss: 2.170562]\n",
      "epoch:8 step:8427 [D loss: 0.600394, acc: 64.84%] [G loss: 2.215028]\n",
      "epoch:8 step:8428 [D loss: 0.608752, acc: 67.19%] [G loss: 2.005199]\n",
      "epoch:8 step:8429 [D loss: 0.589405, acc: 67.97%] [G loss: 2.383474]\n",
      "epoch:8 step:8430 [D loss: 0.613745, acc: 71.09%] [G loss: 2.195894]\n",
      "epoch:8 step:8431 [D loss: 0.614918, acc: 67.97%] [G loss: 2.234378]\n",
      "epoch:8 step:8432 [D loss: 0.551987, acc: 67.97%] [G loss: 2.372196]\n",
      "epoch:8 step:8433 [D loss: 0.563405, acc: 71.88%] [G loss: 2.944455]\n",
      "epoch:9 step:8434 [D loss: 0.614489, acc: 66.41%] [G loss: 2.210040]\n",
      "epoch:9 step:8435 [D loss: 0.597773, acc: 67.19%] [G loss: 2.411722]\n",
      "epoch:9 step:8436 [D loss: 0.607790, acc: 67.19%] [G loss: 2.252557]\n",
      "epoch:9 step:8437 [D loss: 0.572159, acc: 72.66%] [G loss: 2.216758]\n",
      "epoch:9 step:8438 [D loss: 0.617694, acc: 67.19%] [G loss: 2.082005]\n",
      "epoch:9 step:8439 [D loss: 0.637308, acc: 60.16%] [G loss: 1.958292]\n",
      "epoch:9 step:8440 [D loss: 0.601153, acc: 69.53%] [G loss: 2.118944]\n",
      "epoch:9 step:8441 [D loss: 0.580033, acc: 73.44%] [G loss: 2.190809]\n",
      "epoch:9 step:8442 [D loss: 0.615318, acc: 67.19%] [G loss: 2.209345]\n",
      "epoch:9 step:8443 [D loss: 0.634263, acc: 65.62%] [G loss: 2.346716]\n",
      "epoch:9 step:8444 [D loss: 0.639120, acc: 62.50%] [G loss: 2.175379]\n",
      "epoch:9 step:8445 [D loss: 0.583426, acc: 67.97%] [G loss: 2.319048]\n",
      "epoch:9 step:8446 [D loss: 0.594375, acc: 69.53%] [G loss: 2.143909]\n",
      "epoch:9 step:8447 [D loss: 0.623108, acc: 65.62%] [G loss: 2.062730]\n",
      "epoch:9 step:8448 [D loss: 0.645765, acc: 65.62%] [G loss: 2.299245]\n",
      "epoch:9 step:8449 [D loss: 0.620869, acc: 69.53%] [G loss: 2.408302]\n",
      "epoch:9 step:8450 [D loss: 0.634428, acc: 65.62%] [G loss: 2.206893]\n",
      "epoch:9 step:8451 [D loss: 0.726921, acc: 58.59%] [G loss: 2.007571]\n",
      "epoch:9 step:8452 [D loss: 0.630296, acc: 63.28%] [G loss: 2.120330]\n",
      "epoch:9 step:8453 [D loss: 0.742302, acc: 57.03%] [G loss: 1.933907]\n",
      "epoch:9 step:8454 [D loss: 0.586076, acc: 67.19%] [G loss: 2.207511]\n",
      "epoch:9 step:8455 [D loss: 0.583273, acc: 69.53%] [G loss: 2.179850]\n",
      "epoch:9 step:8456 [D loss: 0.597942, acc: 72.66%] [G loss: 2.401920]\n",
      "epoch:9 step:8457 [D loss: 0.595555, acc: 68.75%] [G loss: 2.282579]\n",
      "epoch:9 step:8458 [D loss: 0.599044, acc: 68.75%] [G loss: 2.240830]\n",
      "epoch:9 step:8459 [D loss: 0.641788, acc: 66.41%] [G loss: 1.943485]\n",
      "epoch:9 step:8460 [D loss: 0.657902, acc: 63.28%] [G loss: 2.121662]\n",
      "epoch:9 step:8461 [D loss: 0.612391, acc: 65.62%] [G loss: 2.126526]\n",
      "epoch:9 step:8462 [D loss: 0.562843, acc: 75.00%] [G loss: 2.143259]\n",
      "epoch:9 step:8463 [D loss: 0.628928, acc: 67.97%] [G loss: 2.021162]\n",
      "epoch:9 step:8464 [D loss: 0.663807, acc: 57.81%] [G loss: 2.032300]\n",
      "epoch:9 step:8465 [D loss: 0.623281, acc: 64.06%] [G loss: 1.937655]\n",
      "epoch:9 step:8466 [D loss: 0.715426, acc: 57.81%] [G loss: 1.847655]\n",
      "epoch:9 step:8467 [D loss: 0.617158, acc: 69.53%] [G loss: 2.086317]\n",
      "epoch:9 step:8468 [D loss: 0.675797, acc: 57.81%] [G loss: 2.061238]\n",
      "epoch:9 step:8469 [D loss: 0.593757, acc: 60.16%] [G loss: 2.299624]\n",
      "epoch:9 step:8470 [D loss: 0.609288, acc: 68.75%] [G loss: 2.068538]\n",
      "epoch:9 step:8471 [D loss: 0.638157, acc: 64.06%] [G loss: 2.090250]\n",
      "epoch:9 step:8472 [D loss: 0.646949, acc: 64.06%] [G loss: 2.237639]\n",
      "epoch:9 step:8473 [D loss: 0.562701, acc: 68.75%] [G loss: 2.345003]\n",
      "epoch:9 step:8474 [D loss: 0.623828, acc: 66.41%] [G loss: 1.975428]\n",
      "epoch:9 step:8475 [D loss: 0.569248, acc: 73.44%] [G loss: 2.295891]\n",
      "epoch:9 step:8476 [D loss: 0.642245, acc: 63.28%] [G loss: 2.158372]\n",
      "epoch:9 step:8477 [D loss: 0.656568, acc: 64.84%] [G loss: 2.095912]\n",
      "epoch:9 step:8478 [D loss: 0.591841, acc: 65.62%] [G loss: 2.375581]\n",
      "epoch:9 step:8479 [D loss: 0.629448, acc: 64.84%] [G loss: 2.024082]\n",
      "epoch:9 step:8480 [D loss: 0.632922, acc: 70.31%] [G loss: 2.187379]\n",
      "epoch:9 step:8481 [D loss: 0.632596, acc: 69.53%] [G loss: 2.225642]\n",
      "epoch:9 step:8482 [D loss: 0.567838, acc: 73.44%] [G loss: 2.270051]\n",
      "epoch:9 step:8483 [D loss: 0.616978, acc: 64.06%] [G loss: 2.210774]\n",
      "epoch:9 step:8484 [D loss: 0.664367, acc: 67.19%] [G loss: 2.124273]\n",
      "epoch:9 step:8485 [D loss: 0.665651, acc: 60.94%] [G loss: 2.241886]\n",
      "epoch:9 step:8486 [D loss: 0.512735, acc: 78.12%] [G loss: 2.478159]\n",
      "epoch:9 step:8487 [D loss: 0.590207, acc: 65.62%] [G loss: 2.137096]\n",
      "epoch:9 step:8488 [D loss: 0.574474, acc: 73.44%] [G loss: 2.326377]\n",
      "epoch:9 step:8489 [D loss: 0.588344, acc: 67.19%] [G loss: 2.450433]\n",
      "epoch:9 step:8490 [D loss: 0.629123, acc: 63.28%] [G loss: 2.216152]\n",
      "epoch:9 step:8491 [D loss: 0.677764, acc: 64.06%] [G loss: 2.244485]\n",
      "epoch:9 step:8492 [D loss: 0.657625, acc: 66.41%] [G loss: 2.080266]\n",
      "epoch:9 step:8493 [D loss: 0.583484, acc: 68.75%] [G loss: 2.051486]\n",
      "epoch:9 step:8494 [D loss: 0.634581, acc: 67.19%] [G loss: 2.089701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8495 [D loss: 0.658170, acc: 60.94%] [G loss: 2.057418]\n",
      "epoch:9 step:8496 [D loss: 0.642441, acc: 59.38%] [G loss: 2.082315]\n",
      "epoch:9 step:8497 [D loss: 0.715486, acc: 60.94%] [G loss: 2.071307]\n",
      "epoch:9 step:8498 [D loss: 0.621468, acc: 64.06%] [G loss: 1.970392]\n",
      "epoch:9 step:8499 [D loss: 0.614455, acc: 64.06%] [G loss: 2.026417]\n",
      "epoch:9 step:8500 [D loss: 0.654929, acc: 60.94%] [G loss: 2.066690]\n",
      "epoch:9 step:8501 [D loss: 0.557620, acc: 67.19%] [G loss: 2.101652]\n",
      "epoch:9 step:8502 [D loss: 0.692164, acc: 65.62%] [G loss: 2.438185]\n",
      "epoch:9 step:8503 [D loss: 0.559772, acc: 71.09%] [G loss: 2.215659]\n",
      "epoch:9 step:8504 [D loss: 0.635508, acc: 64.06%] [G loss: 2.035444]\n",
      "epoch:9 step:8505 [D loss: 0.624443, acc: 66.41%] [G loss: 1.984197]\n",
      "epoch:9 step:8506 [D loss: 0.686431, acc: 60.16%] [G loss: 2.064713]\n",
      "epoch:9 step:8507 [D loss: 0.580798, acc: 65.62%] [G loss: 2.239644]\n",
      "epoch:9 step:8508 [D loss: 0.632827, acc: 67.19%] [G loss: 2.506283]\n",
      "epoch:9 step:8509 [D loss: 0.542351, acc: 75.00%] [G loss: 2.484576]\n",
      "epoch:9 step:8510 [D loss: 0.547023, acc: 70.31%] [G loss: 2.588213]\n",
      "epoch:9 step:8511 [D loss: 0.665569, acc: 57.81%] [G loss: 2.192255]\n",
      "epoch:9 step:8512 [D loss: 0.645772, acc: 64.84%] [G loss: 2.152958]\n",
      "epoch:9 step:8513 [D loss: 0.683517, acc: 57.81%] [G loss: 1.941396]\n",
      "epoch:9 step:8514 [D loss: 0.648653, acc: 62.50%] [G loss: 1.915867]\n",
      "epoch:9 step:8515 [D loss: 0.710861, acc: 55.47%] [G loss: 1.924504]\n",
      "epoch:9 step:8516 [D loss: 0.618522, acc: 64.84%] [G loss: 2.142491]\n",
      "epoch:9 step:8517 [D loss: 0.657490, acc: 60.16%] [G loss: 2.116687]\n",
      "epoch:9 step:8518 [D loss: 0.699721, acc: 57.03%] [G loss: 2.016956]\n",
      "epoch:9 step:8519 [D loss: 0.653651, acc: 65.62%] [G loss: 1.950423]\n",
      "epoch:9 step:8520 [D loss: 0.625029, acc: 67.97%] [G loss: 1.846169]\n",
      "epoch:9 step:8521 [D loss: 0.610428, acc: 71.88%] [G loss: 2.068278]\n",
      "epoch:9 step:8522 [D loss: 0.657110, acc: 58.59%] [G loss: 1.953738]\n",
      "epoch:9 step:8523 [D loss: 0.635192, acc: 66.41%] [G loss: 2.252844]\n",
      "epoch:9 step:8524 [D loss: 0.606205, acc: 65.62%] [G loss: 2.078945]\n",
      "epoch:9 step:8525 [D loss: 0.711244, acc: 59.38%] [G loss: 2.087266]\n",
      "epoch:9 step:8526 [D loss: 0.609309, acc: 66.41%] [G loss: 2.011733]\n",
      "epoch:9 step:8527 [D loss: 0.623320, acc: 64.06%] [G loss: 2.242170]\n",
      "epoch:9 step:8528 [D loss: 0.672249, acc: 61.72%] [G loss: 1.900961]\n",
      "epoch:9 step:8529 [D loss: 0.612859, acc: 68.75%] [G loss: 2.015443]\n",
      "epoch:9 step:8530 [D loss: 0.588666, acc: 66.41%] [G loss: 2.163060]\n",
      "epoch:9 step:8531 [D loss: 0.660557, acc: 62.50%] [G loss: 2.072442]\n",
      "epoch:9 step:8532 [D loss: 0.604914, acc: 69.53%] [G loss: 1.962696]\n",
      "epoch:9 step:8533 [D loss: 0.596134, acc: 66.41%] [G loss: 2.207942]\n",
      "epoch:9 step:8534 [D loss: 0.603040, acc: 71.09%] [G loss: 2.175714]\n",
      "epoch:9 step:8535 [D loss: 0.639615, acc: 62.50%] [G loss: 2.170465]\n",
      "epoch:9 step:8536 [D loss: 0.582895, acc: 74.22%] [G loss: 2.143252]\n",
      "epoch:9 step:8537 [D loss: 0.572258, acc: 69.53%] [G loss: 2.181546]\n",
      "epoch:9 step:8538 [D loss: 0.630108, acc: 64.06%] [G loss: 2.126216]\n",
      "epoch:9 step:8539 [D loss: 0.579296, acc: 67.97%] [G loss: 2.337818]\n",
      "epoch:9 step:8540 [D loss: 0.679406, acc: 55.47%] [G loss: 2.118291]\n",
      "epoch:9 step:8541 [D loss: 0.678315, acc: 61.72%] [G loss: 1.921328]\n",
      "epoch:9 step:8542 [D loss: 0.618940, acc: 65.62%] [G loss: 2.029117]\n",
      "epoch:9 step:8543 [D loss: 0.641212, acc: 63.28%] [G loss: 1.984300]\n",
      "epoch:9 step:8544 [D loss: 0.619871, acc: 65.62%] [G loss: 2.184758]\n",
      "epoch:9 step:8545 [D loss: 0.624695, acc: 65.62%] [G loss: 2.235190]\n",
      "epoch:9 step:8546 [D loss: 0.685091, acc: 54.69%] [G loss: 2.058018]\n",
      "epoch:9 step:8547 [D loss: 0.577254, acc: 75.00%] [G loss: 2.401218]\n",
      "epoch:9 step:8548 [D loss: 0.621376, acc: 67.97%] [G loss: 2.313197]\n",
      "epoch:9 step:8549 [D loss: 0.617791, acc: 65.62%] [G loss: 2.245252]\n",
      "epoch:9 step:8550 [D loss: 0.635149, acc: 62.50%] [G loss: 2.211436]\n",
      "epoch:9 step:8551 [D loss: 0.629385, acc: 65.62%] [G loss: 2.139945]\n",
      "epoch:9 step:8552 [D loss: 0.566865, acc: 73.44%] [G loss: 2.441235]\n",
      "epoch:9 step:8553 [D loss: 0.641462, acc: 67.97%] [G loss: 2.450223]\n",
      "epoch:9 step:8554 [D loss: 0.687434, acc: 58.59%] [G loss: 2.065345]\n",
      "epoch:9 step:8555 [D loss: 0.627213, acc: 64.06%] [G loss: 2.330546]\n",
      "epoch:9 step:8556 [D loss: 0.615957, acc: 66.41%] [G loss: 2.196256]\n",
      "epoch:9 step:8557 [D loss: 0.655876, acc: 60.94%] [G loss: 2.103270]\n",
      "epoch:9 step:8558 [D loss: 0.681763, acc: 61.72%] [G loss: 2.025527]\n",
      "epoch:9 step:8559 [D loss: 0.600404, acc: 65.62%] [G loss: 2.088113]\n",
      "epoch:9 step:8560 [D loss: 0.685663, acc: 52.34%] [G loss: 2.078408]\n",
      "epoch:9 step:8561 [D loss: 0.656823, acc: 67.97%] [G loss: 2.164691]\n",
      "epoch:9 step:8562 [D loss: 0.618131, acc: 64.06%] [G loss: 2.077410]\n",
      "epoch:9 step:8563 [D loss: 0.616360, acc: 69.53%] [G loss: 2.013770]\n",
      "epoch:9 step:8564 [D loss: 0.650264, acc: 62.50%] [G loss: 2.223983]\n",
      "epoch:9 step:8565 [D loss: 0.643205, acc: 62.50%] [G loss: 2.060225]\n",
      "epoch:9 step:8566 [D loss: 0.683795, acc: 58.59%] [G loss: 2.163117]\n",
      "epoch:9 step:8567 [D loss: 0.633850, acc: 60.16%] [G loss: 2.045085]\n",
      "epoch:9 step:8568 [D loss: 0.609688, acc: 68.75%] [G loss: 2.173110]\n",
      "epoch:9 step:8569 [D loss: 0.684026, acc: 57.03%] [G loss: 1.959455]\n",
      "epoch:9 step:8570 [D loss: 0.565147, acc: 75.00%] [G loss: 2.024164]\n",
      "epoch:9 step:8571 [D loss: 0.654252, acc: 60.16%] [G loss: 2.078261]\n",
      "epoch:9 step:8572 [D loss: 0.634976, acc: 68.75%] [G loss: 2.173722]\n",
      "epoch:9 step:8573 [D loss: 0.600827, acc: 68.75%] [G loss: 2.172907]\n",
      "epoch:9 step:8574 [D loss: 0.610373, acc: 63.28%] [G loss: 2.107701]\n",
      "epoch:9 step:8575 [D loss: 0.597396, acc: 67.97%] [G loss: 2.135740]\n",
      "epoch:9 step:8576 [D loss: 0.595343, acc: 64.84%] [G loss: 2.170140]\n",
      "epoch:9 step:8577 [D loss: 0.565476, acc: 73.44%] [G loss: 2.294168]\n",
      "epoch:9 step:8578 [D loss: 0.580402, acc: 72.66%] [G loss: 2.215137]\n",
      "epoch:9 step:8579 [D loss: 0.639872, acc: 63.28%] [G loss: 2.241333]\n",
      "epoch:9 step:8580 [D loss: 0.620274, acc: 66.41%] [G loss: 2.024015]\n",
      "epoch:9 step:8581 [D loss: 0.639502, acc: 65.62%] [G loss: 2.147351]\n",
      "epoch:9 step:8582 [D loss: 0.616893, acc: 65.62%] [G loss: 2.425065]\n",
      "epoch:9 step:8583 [D loss: 0.572980, acc: 71.88%] [G loss: 2.191652]\n",
      "epoch:9 step:8584 [D loss: 0.578252, acc: 68.75%] [G loss: 2.623658]\n",
      "epoch:9 step:8585 [D loss: 0.603527, acc: 66.41%] [G loss: 2.370716]\n",
      "epoch:9 step:8586 [D loss: 0.606440, acc: 64.06%] [G loss: 2.349953]\n",
      "epoch:9 step:8587 [D loss: 0.648598, acc: 66.41%] [G loss: 2.394554]\n",
      "epoch:9 step:8588 [D loss: 0.663452, acc: 63.28%] [G loss: 2.384165]\n",
      "epoch:9 step:8589 [D loss: 0.581297, acc: 71.09%] [G loss: 2.325817]\n",
      "epoch:9 step:8590 [D loss: 0.661825, acc: 64.06%] [G loss: 2.174633]\n",
      "epoch:9 step:8591 [D loss: 0.666852, acc: 63.28%] [G loss: 2.023673]\n",
      "epoch:9 step:8592 [D loss: 0.621516, acc: 63.28%] [G loss: 2.064919]\n",
      "epoch:9 step:8593 [D loss: 0.731923, acc: 57.81%] [G loss: 1.891652]\n",
      "epoch:9 step:8594 [D loss: 0.617361, acc: 67.19%] [G loss: 2.174147]\n",
      "epoch:9 step:8595 [D loss: 0.620427, acc: 63.28%] [G loss: 2.150565]\n",
      "epoch:9 step:8596 [D loss: 0.614763, acc: 65.62%] [G loss: 2.201936]\n",
      "epoch:9 step:8597 [D loss: 0.703109, acc: 55.47%] [G loss: 1.947943]\n",
      "epoch:9 step:8598 [D loss: 0.625308, acc: 64.06%] [G loss: 2.098378]\n",
      "epoch:9 step:8599 [D loss: 0.609917, acc: 64.06%] [G loss: 1.960533]\n",
      "epoch:9 step:8600 [D loss: 0.608677, acc: 69.53%] [G loss: 2.146607]\n",
      "##############\n",
      "[2.55597083 1.08145754 6.38898471 4.58525016 3.85875726 5.64763541\n",
      " 4.73439604 4.78784156 4.84638967 3.71034512]\n",
      "##########\n",
      "epoch:9 step:8601 [D loss: 0.644637, acc: 68.75%] [G loss: 2.178066]\n",
      "epoch:9 step:8602 [D loss: 0.600471, acc: 64.06%] [G loss: 2.021948]\n",
      "epoch:9 step:8603 [D loss: 0.628426, acc: 64.84%] [G loss: 2.116520]\n",
      "epoch:9 step:8604 [D loss: 0.603043, acc: 68.75%] [G loss: 2.202687]\n",
      "epoch:9 step:8605 [D loss: 0.583355, acc: 68.75%] [G loss: 2.090914]\n",
      "epoch:9 step:8606 [D loss: 0.575157, acc: 71.88%] [G loss: 2.066500]\n",
      "epoch:9 step:8607 [D loss: 0.670584, acc: 54.69%] [G loss: 2.040529]\n",
      "epoch:9 step:8608 [D loss: 0.606446, acc: 70.31%] [G loss: 2.068157]\n",
      "epoch:9 step:8609 [D loss: 0.653696, acc: 62.50%] [G loss: 2.151152]\n",
      "epoch:9 step:8610 [D loss: 0.619404, acc: 65.62%] [G loss: 2.001361]\n",
      "epoch:9 step:8611 [D loss: 0.601217, acc: 66.41%] [G loss: 2.074892]\n",
      "epoch:9 step:8612 [D loss: 0.667210, acc: 59.38%] [G loss: 2.146297]\n",
      "epoch:9 step:8613 [D loss: 0.628524, acc: 67.97%] [G loss: 2.143941]\n",
      "epoch:9 step:8614 [D loss: 0.687615, acc: 57.81%] [G loss: 2.017375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8615 [D loss: 0.578912, acc: 64.06%] [G loss: 1.980134]\n",
      "epoch:9 step:8616 [D loss: 0.625744, acc: 65.62%] [G loss: 2.170532]\n",
      "epoch:9 step:8617 [D loss: 0.652954, acc: 60.94%] [G loss: 2.161329]\n",
      "epoch:9 step:8618 [D loss: 0.598731, acc: 71.88%] [G loss: 2.320733]\n",
      "epoch:9 step:8619 [D loss: 0.620249, acc: 62.50%] [G loss: 2.167311]\n",
      "epoch:9 step:8620 [D loss: 0.633605, acc: 65.62%] [G loss: 1.890731]\n",
      "epoch:9 step:8621 [D loss: 0.639090, acc: 60.94%] [G loss: 2.170945]\n",
      "epoch:9 step:8622 [D loss: 0.660220, acc: 58.59%] [G loss: 2.026471]\n",
      "epoch:9 step:8623 [D loss: 0.629975, acc: 62.50%] [G loss: 2.170576]\n",
      "epoch:9 step:8624 [D loss: 0.588421, acc: 70.31%] [G loss: 2.155751]\n",
      "epoch:9 step:8625 [D loss: 0.614266, acc: 68.75%] [G loss: 2.246400]\n",
      "epoch:9 step:8626 [D loss: 0.573271, acc: 68.75%] [G loss: 2.276550]\n",
      "epoch:9 step:8627 [D loss: 0.561394, acc: 71.88%] [G loss: 2.487161]\n",
      "epoch:9 step:8628 [D loss: 0.627638, acc: 63.28%] [G loss: 2.096121]\n",
      "epoch:9 step:8629 [D loss: 0.710390, acc: 59.38%] [G loss: 2.103085]\n",
      "epoch:9 step:8630 [D loss: 0.637138, acc: 66.41%] [G loss: 2.282515]\n",
      "epoch:9 step:8631 [D loss: 0.613433, acc: 66.41%] [G loss: 2.198806]\n",
      "epoch:9 step:8632 [D loss: 0.672883, acc: 60.16%] [G loss: 2.156941]\n",
      "epoch:9 step:8633 [D loss: 0.704074, acc: 56.25%] [G loss: 1.885706]\n",
      "epoch:9 step:8634 [D loss: 0.532462, acc: 76.56%] [G loss: 2.254122]\n",
      "epoch:9 step:8635 [D loss: 0.569354, acc: 71.88%] [G loss: 2.146485]\n",
      "epoch:9 step:8636 [D loss: 0.674945, acc: 61.72%] [G loss: 1.822632]\n",
      "epoch:9 step:8637 [D loss: 0.609905, acc: 66.41%] [G loss: 1.950480]\n",
      "epoch:9 step:8638 [D loss: 0.645139, acc: 57.81%] [G loss: 2.006649]\n",
      "epoch:9 step:8639 [D loss: 0.622229, acc: 60.94%] [G loss: 2.228956]\n",
      "epoch:9 step:8640 [D loss: 0.643448, acc: 62.50%] [G loss: 2.278572]\n",
      "epoch:9 step:8641 [D loss: 0.597060, acc: 71.09%] [G loss: 2.577977]\n",
      "epoch:9 step:8642 [D loss: 0.574359, acc: 70.31%] [G loss: 2.283517]\n",
      "epoch:9 step:8643 [D loss: 0.631776, acc: 64.84%] [G loss: 2.046690]\n",
      "epoch:9 step:8644 [D loss: 0.770023, acc: 52.34%] [G loss: 2.089982]\n",
      "epoch:9 step:8645 [D loss: 0.641480, acc: 64.06%] [G loss: 2.092571]\n",
      "epoch:9 step:8646 [D loss: 0.637687, acc: 67.19%] [G loss: 1.894636]\n",
      "epoch:9 step:8647 [D loss: 0.631874, acc: 64.06%] [G loss: 1.920985]\n",
      "epoch:9 step:8648 [D loss: 0.606624, acc: 69.53%] [G loss: 1.981485]\n",
      "epoch:9 step:8649 [D loss: 0.619381, acc: 65.62%] [G loss: 2.002434]\n",
      "epoch:9 step:8650 [D loss: 0.602718, acc: 68.75%] [G loss: 2.052181]\n",
      "epoch:9 step:8651 [D loss: 0.642952, acc: 64.84%] [G loss: 2.340854]\n",
      "epoch:9 step:8652 [D loss: 0.624443, acc: 61.72%] [G loss: 2.338670]\n",
      "epoch:9 step:8653 [D loss: 0.737638, acc: 54.69%] [G loss: 1.759386]\n",
      "epoch:9 step:8654 [D loss: 0.655321, acc: 61.72%] [G loss: 2.185565]\n",
      "epoch:9 step:8655 [D loss: 0.595030, acc: 64.84%] [G loss: 2.175946]\n",
      "epoch:9 step:8656 [D loss: 0.689150, acc: 58.59%] [G loss: 2.168804]\n",
      "epoch:9 step:8657 [D loss: 0.607373, acc: 71.09%] [G loss: 1.959513]\n",
      "epoch:9 step:8658 [D loss: 0.614721, acc: 65.62%] [G loss: 2.128621]\n",
      "epoch:9 step:8659 [D loss: 0.647563, acc: 64.84%] [G loss: 2.059119]\n",
      "epoch:9 step:8660 [D loss: 0.641339, acc: 61.72%] [G loss: 1.877691]\n",
      "epoch:9 step:8661 [D loss: 0.640594, acc: 62.50%] [G loss: 1.922980]\n",
      "epoch:9 step:8662 [D loss: 0.618470, acc: 67.19%] [G loss: 1.993132]\n",
      "epoch:9 step:8663 [D loss: 0.559480, acc: 69.53%] [G loss: 2.332371]\n",
      "epoch:9 step:8664 [D loss: 0.584298, acc: 73.44%] [G loss: 2.493948]\n",
      "epoch:9 step:8665 [D loss: 0.589257, acc: 64.84%] [G loss: 2.413382]\n",
      "epoch:9 step:8666 [D loss: 0.637080, acc: 61.72%] [G loss: 2.108837]\n",
      "epoch:9 step:8667 [D loss: 0.643897, acc: 60.94%] [G loss: 2.122412]\n",
      "epoch:9 step:8668 [D loss: 0.613553, acc: 67.97%] [G loss: 2.053185]\n",
      "epoch:9 step:8669 [D loss: 0.618797, acc: 64.84%] [G loss: 2.163106]\n",
      "epoch:9 step:8670 [D loss: 0.638921, acc: 60.16%] [G loss: 2.115975]\n",
      "epoch:9 step:8671 [D loss: 0.654363, acc: 59.38%] [G loss: 1.957042]\n",
      "epoch:9 step:8672 [D loss: 0.558340, acc: 74.22%] [G loss: 2.047375]\n",
      "epoch:9 step:8673 [D loss: 0.603747, acc: 67.19%] [G loss: 2.422270]\n",
      "epoch:9 step:8674 [D loss: 0.652686, acc: 60.94%] [G loss: 2.114670]\n",
      "epoch:9 step:8675 [D loss: 0.622961, acc: 63.28%] [G loss: 2.255135]\n",
      "epoch:9 step:8676 [D loss: 0.565702, acc: 75.78%] [G loss: 2.073417]\n",
      "epoch:9 step:8677 [D loss: 0.641477, acc: 65.62%] [G loss: 2.344096]\n",
      "epoch:9 step:8678 [D loss: 0.602738, acc: 68.75%] [G loss: 2.264095]\n",
      "epoch:9 step:8679 [D loss: 0.605280, acc: 69.53%] [G loss: 2.128668]\n",
      "epoch:9 step:8680 [D loss: 0.626358, acc: 65.62%] [G loss: 2.326610]\n",
      "epoch:9 step:8681 [D loss: 0.590826, acc: 69.53%] [G loss: 2.341349]\n",
      "epoch:9 step:8682 [D loss: 0.729068, acc: 50.78%] [G loss: 2.140435]\n",
      "epoch:9 step:8683 [D loss: 0.645925, acc: 66.41%] [G loss: 2.137622]\n",
      "epoch:9 step:8684 [D loss: 0.692975, acc: 63.28%] [G loss: 2.047986]\n",
      "epoch:9 step:8685 [D loss: 0.648671, acc: 63.28%] [G loss: 1.850632]\n",
      "epoch:9 step:8686 [D loss: 0.602868, acc: 67.97%] [G loss: 2.348284]\n",
      "epoch:9 step:8687 [D loss: 0.626739, acc: 62.50%] [G loss: 2.174331]\n",
      "epoch:9 step:8688 [D loss: 0.683528, acc: 54.69%] [G loss: 2.189485]\n",
      "epoch:9 step:8689 [D loss: 0.640515, acc: 64.06%] [G loss: 2.131927]\n",
      "epoch:9 step:8690 [D loss: 0.688842, acc: 59.38%] [G loss: 2.027608]\n",
      "epoch:9 step:8691 [D loss: 0.633877, acc: 67.19%] [G loss: 2.055932]\n",
      "epoch:9 step:8692 [D loss: 0.568685, acc: 68.75%] [G loss: 2.100637]\n",
      "epoch:9 step:8693 [D loss: 0.651373, acc: 60.94%] [G loss: 2.100962]\n",
      "epoch:9 step:8694 [D loss: 0.664090, acc: 63.28%] [G loss: 2.134141]\n",
      "epoch:9 step:8695 [D loss: 0.542969, acc: 75.78%] [G loss: 2.129258]\n",
      "epoch:9 step:8696 [D loss: 0.702331, acc: 55.47%] [G loss: 2.081784]\n",
      "epoch:9 step:8697 [D loss: 0.609461, acc: 67.19%] [G loss: 2.341015]\n",
      "epoch:9 step:8698 [D loss: 0.679045, acc: 62.50%] [G loss: 2.042955]\n",
      "epoch:9 step:8699 [D loss: 0.631735, acc: 60.94%] [G loss: 2.127783]\n",
      "epoch:9 step:8700 [D loss: 0.585792, acc: 68.75%] [G loss: 2.252435]\n",
      "epoch:9 step:8701 [D loss: 0.586050, acc: 68.75%] [G loss: 2.025257]\n",
      "epoch:9 step:8702 [D loss: 0.642334, acc: 63.28%] [G loss: 2.294140]\n",
      "epoch:9 step:8703 [D loss: 0.591582, acc: 63.28%] [G loss: 2.260245]\n",
      "epoch:9 step:8704 [D loss: 0.652231, acc: 61.72%] [G loss: 2.152894]\n",
      "epoch:9 step:8705 [D loss: 0.650542, acc: 61.72%] [G loss: 2.271994]\n",
      "epoch:9 step:8706 [D loss: 0.608228, acc: 67.19%] [G loss: 2.117522]\n",
      "epoch:9 step:8707 [D loss: 0.592364, acc: 70.31%] [G loss: 2.029409]\n",
      "epoch:9 step:8708 [D loss: 0.698246, acc: 60.16%] [G loss: 2.104607]\n",
      "epoch:9 step:8709 [D loss: 0.624146, acc: 64.84%] [G loss: 2.044041]\n",
      "epoch:9 step:8710 [D loss: 0.625243, acc: 67.19%] [G loss: 2.158007]\n",
      "epoch:9 step:8711 [D loss: 0.618107, acc: 66.41%] [G loss: 2.109595]\n",
      "epoch:9 step:8712 [D loss: 0.693273, acc: 57.03%] [G loss: 2.082253]\n",
      "epoch:9 step:8713 [D loss: 0.565788, acc: 74.22%] [G loss: 2.111817]\n",
      "epoch:9 step:8714 [D loss: 0.674909, acc: 59.38%] [G loss: 1.797525]\n",
      "epoch:9 step:8715 [D loss: 0.611877, acc: 69.53%] [G loss: 2.144789]\n",
      "epoch:9 step:8716 [D loss: 0.649514, acc: 62.50%] [G loss: 2.238931]\n",
      "epoch:9 step:8717 [D loss: 0.592424, acc: 71.88%] [G loss: 2.050855]\n",
      "epoch:9 step:8718 [D loss: 0.621651, acc: 64.84%] [G loss: 2.165264]\n",
      "epoch:9 step:8719 [D loss: 0.595159, acc: 67.19%] [G loss: 2.390742]\n",
      "epoch:9 step:8720 [D loss: 0.634124, acc: 67.19%] [G loss: 2.027015]\n",
      "epoch:9 step:8721 [D loss: 0.660992, acc: 61.72%] [G loss: 2.059337]\n",
      "epoch:9 step:8722 [D loss: 0.632068, acc: 63.28%] [G loss: 2.088507]\n",
      "epoch:9 step:8723 [D loss: 0.602221, acc: 69.53%] [G loss: 2.026022]\n",
      "epoch:9 step:8724 [D loss: 0.654365, acc: 61.72%] [G loss: 2.101534]\n",
      "epoch:9 step:8725 [D loss: 0.674273, acc: 60.94%] [G loss: 1.955595]\n",
      "epoch:9 step:8726 [D loss: 0.586227, acc: 69.53%] [G loss: 2.169031]\n",
      "epoch:9 step:8727 [D loss: 0.611006, acc: 69.53%] [G loss: 2.008449]\n",
      "epoch:9 step:8728 [D loss: 0.677545, acc: 61.72%] [G loss: 2.286082]\n",
      "epoch:9 step:8729 [D loss: 0.597385, acc: 71.88%] [G loss: 2.240816]\n",
      "epoch:9 step:8730 [D loss: 0.602245, acc: 72.66%] [G loss: 2.121545]\n",
      "epoch:9 step:8731 [D loss: 0.564631, acc: 72.66%] [G loss: 2.226050]\n",
      "epoch:9 step:8732 [D loss: 0.595853, acc: 71.88%] [G loss: 2.081297]\n",
      "epoch:9 step:8733 [D loss: 0.628201, acc: 65.62%] [G loss: 2.254967]\n",
      "epoch:9 step:8734 [D loss: 0.612498, acc: 65.62%] [G loss: 2.044028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8735 [D loss: 0.656273, acc: 62.50%] [G loss: 1.939994]\n",
      "epoch:9 step:8736 [D loss: 0.635689, acc: 62.50%] [G loss: 2.029292]\n",
      "epoch:9 step:8737 [D loss: 0.635193, acc: 64.84%] [G loss: 2.031582]\n",
      "epoch:9 step:8738 [D loss: 0.572752, acc: 73.44%] [G loss: 2.123222]\n",
      "epoch:9 step:8739 [D loss: 0.603310, acc: 67.97%] [G loss: 2.090760]\n",
      "epoch:9 step:8740 [D loss: 0.588662, acc: 67.97%] [G loss: 2.148746]\n",
      "epoch:9 step:8741 [D loss: 0.652730, acc: 64.06%] [G loss: 2.025119]\n",
      "epoch:9 step:8742 [D loss: 0.656224, acc: 60.94%] [G loss: 2.249461]\n",
      "epoch:9 step:8743 [D loss: 0.628994, acc: 70.31%] [G loss: 2.064593]\n",
      "epoch:9 step:8744 [D loss: 0.611046, acc: 63.28%] [G loss: 2.097875]\n",
      "epoch:9 step:8745 [D loss: 0.501890, acc: 82.81%] [G loss: 2.743254]\n",
      "epoch:9 step:8746 [D loss: 0.638302, acc: 61.72%] [G loss: 2.465575]\n",
      "epoch:9 step:8747 [D loss: 0.519406, acc: 71.88%] [G loss: 2.408642]\n",
      "epoch:9 step:8748 [D loss: 0.523374, acc: 71.88%] [G loss: 2.505383]\n",
      "epoch:9 step:8749 [D loss: 0.738692, acc: 59.38%] [G loss: 2.163222]\n",
      "epoch:9 step:8750 [D loss: 0.625551, acc: 65.62%] [G loss: 2.161552]\n",
      "epoch:9 step:8751 [D loss: 0.608932, acc: 64.84%] [G loss: 2.177326]\n",
      "epoch:9 step:8752 [D loss: 0.630628, acc: 61.72%] [G loss: 2.036089]\n",
      "epoch:9 step:8753 [D loss: 0.663997, acc: 58.59%] [G loss: 2.160686]\n",
      "epoch:9 step:8754 [D loss: 0.604176, acc: 68.75%] [G loss: 2.154918]\n",
      "epoch:9 step:8755 [D loss: 0.633202, acc: 63.28%] [G loss: 1.997097]\n",
      "epoch:9 step:8756 [D loss: 0.643039, acc: 60.16%] [G loss: 1.956872]\n",
      "epoch:9 step:8757 [D loss: 0.623517, acc: 64.06%] [G loss: 1.877826]\n",
      "epoch:9 step:8758 [D loss: 0.636524, acc: 62.50%] [G loss: 2.057275]\n",
      "epoch:9 step:8759 [D loss: 0.639926, acc: 61.72%] [G loss: 2.042420]\n",
      "epoch:9 step:8760 [D loss: 0.630518, acc: 65.62%] [G loss: 2.087306]\n",
      "epoch:9 step:8761 [D loss: 0.667525, acc: 57.81%] [G loss: 2.039156]\n",
      "epoch:9 step:8762 [D loss: 0.611528, acc: 71.09%] [G loss: 2.058618]\n",
      "epoch:9 step:8763 [D loss: 0.638709, acc: 65.62%] [G loss: 2.192209]\n",
      "epoch:9 step:8764 [D loss: 0.610260, acc: 63.28%] [G loss: 2.071729]\n",
      "epoch:9 step:8765 [D loss: 0.608934, acc: 66.41%] [G loss: 2.310224]\n",
      "epoch:9 step:8766 [D loss: 0.562046, acc: 69.53%] [G loss: 2.300587]\n",
      "epoch:9 step:8767 [D loss: 0.683107, acc: 58.59%] [G loss: 2.354576]\n",
      "epoch:9 step:8768 [D loss: 0.597983, acc: 65.62%] [G loss: 2.401975]\n",
      "epoch:9 step:8769 [D loss: 0.615008, acc: 61.72%] [G loss: 2.293880]\n",
      "epoch:9 step:8770 [D loss: 0.629920, acc: 66.41%] [G loss: 2.188059]\n",
      "epoch:9 step:8771 [D loss: 0.613907, acc: 67.97%] [G loss: 2.191794]\n",
      "epoch:9 step:8772 [D loss: 0.598014, acc: 70.31%] [G loss: 2.109686]\n",
      "epoch:9 step:8773 [D loss: 0.627029, acc: 66.41%] [G loss: 2.263443]\n",
      "epoch:9 step:8774 [D loss: 0.750113, acc: 54.69%] [G loss: 2.061013]\n",
      "epoch:9 step:8775 [D loss: 0.684393, acc: 57.03%] [G loss: 1.939719]\n",
      "epoch:9 step:8776 [D loss: 0.655707, acc: 62.50%] [G loss: 2.089735]\n",
      "epoch:9 step:8777 [D loss: 0.559404, acc: 71.09%] [G loss: 2.044544]\n",
      "epoch:9 step:8778 [D loss: 0.569263, acc: 70.31%] [G loss: 2.436169]\n",
      "epoch:9 step:8779 [D loss: 0.569719, acc: 71.88%] [G loss: 2.707794]\n",
      "epoch:9 step:8780 [D loss: 0.584366, acc: 61.72%] [G loss: 2.773117]\n",
      "epoch:9 step:8781 [D loss: 0.664553, acc: 62.50%] [G loss: 2.056996]\n",
      "epoch:9 step:8782 [D loss: 0.675263, acc: 60.94%] [G loss: 1.830693]\n",
      "epoch:9 step:8783 [D loss: 0.634264, acc: 63.28%] [G loss: 1.994814]\n",
      "epoch:9 step:8784 [D loss: 0.641947, acc: 65.62%] [G loss: 2.119172]\n",
      "epoch:9 step:8785 [D loss: 0.655038, acc: 62.50%] [G loss: 1.922306]\n",
      "epoch:9 step:8786 [D loss: 0.628658, acc: 65.62%] [G loss: 2.149632]\n",
      "epoch:9 step:8787 [D loss: 0.626731, acc: 65.62%] [G loss: 2.331218]\n",
      "epoch:9 step:8788 [D loss: 0.673993, acc: 62.50%] [G loss: 1.958835]\n",
      "epoch:9 step:8789 [D loss: 0.687503, acc: 59.38%] [G loss: 1.981426]\n",
      "epoch:9 step:8790 [D loss: 0.611650, acc: 65.62%] [G loss: 2.162388]\n",
      "epoch:9 step:8791 [D loss: 0.535271, acc: 76.56%] [G loss: 2.351971]\n",
      "epoch:9 step:8792 [D loss: 0.583608, acc: 70.31%] [G loss: 2.326026]\n",
      "epoch:9 step:8793 [D loss: 0.651932, acc: 61.72%] [G loss: 2.165883]\n",
      "epoch:9 step:8794 [D loss: 0.637402, acc: 63.28%] [G loss: 2.092442]\n",
      "epoch:9 step:8795 [D loss: 0.626440, acc: 67.97%] [G loss: 2.091867]\n",
      "epoch:9 step:8796 [D loss: 0.643724, acc: 57.03%] [G loss: 2.207739]\n",
      "epoch:9 step:8797 [D loss: 0.631845, acc: 67.19%] [G loss: 1.977582]\n",
      "epoch:9 step:8798 [D loss: 0.636237, acc: 64.06%] [G loss: 2.181224]\n",
      "epoch:9 step:8799 [D loss: 0.639817, acc: 64.06%] [G loss: 2.047633]\n",
      "epoch:9 step:8800 [D loss: 0.583119, acc: 70.31%] [G loss: 2.317333]\n",
      "##############\n",
      "[2.4735405  1.44415614 6.57222221 4.85716014 3.79400267 5.65482224\n",
      " 4.55068445 4.75352556 4.90790203 3.67196198]\n",
      "##########\n",
      "epoch:9 step:8801 [D loss: 0.616974, acc: 64.06%] [G loss: 2.180171]\n",
      "epoch:9 step:8802 [D loss: 0.615646, acc: 67.97%] [G loss: 2.170943]\n",
      "epoch:9 step:8803 [D loss: 0.689383, acc: 57.03%] [G loss: 2.056413]\n",
      "epoch:9 step:8804 [D loss: 0.629438, acc: 64.84%] [G loss: 2.093368]\n",
      "epoch:9 step:8805 [D loss: 0.583528, acc: 67.97%] [G loss: 2.037146]\n",
      "epoch:9 step:8806 [D loss: 0.689978, acc: 58.59%] [G loss: 1.785126]\n",
      "epoch:9 step:8807 [D loss: 0.605967, acc: 71.88%] [G loss: 2.448710]\n",
      "epoch:9 step:8808 [D loss: 0.621499, acc: 65.62%] [G loss: 2.057568]\n",
      "epoch:9 step:8809 [D loss: 0.682828, acc: 58.59%] [G loss: 1.947401]\n",
      "epoch:9 step:8810 [D loss: 0.706170, acc: 51.56%] [G loss: 1.886247]\n",
      "epoch:9 step:8811 [D loss: 0.563412, acc: 70.31%] [G loss: 1.993801]\n",
      "epoch:9 step:8812 [D loss: 0.627780, acc: 64.06%] [G loss: 2.165178]\n",
      "epoch:9 step:8813 [D loss: 0.630784, acc: 68.75%] [G loss: 2.160604]\n",
      "epoch:9 step:8814 [D loss: 0.598009, acc: 61.72%] [G loss: 2.080460]\n",
      "epoch:9 step:8815 [D loss: 0.591197, acc: 67.19%] [G loss: 2.050587]\n",
      "epoch:9 step:8816 [D loss: 0.630067, acc: 65.62%] [G loss: 2.020843]\n",
      "epoch:9 step:8817 [D loss: 0.604697, acc: 72.66%] [G loss: 1.980823]\n",
      "epoch:9 step:8818 [D loss: 0.639839, acc: 64.84%] [G loss: 2.263832]\n",
      "epoch:9 step:8819 [D loss: 0.712672, acc: 50.78%] [G loss: 1.912503]\n",
      "epoch:9 step:8820 [D loss: 0.629448, acc: 64.06%] [G loss: 1.989026]\n",
      "epoch:9 step:8821 [D loss: 0.638739, acc: 62.50%] [G loss: 2.187121]\n",
      "epoch:9 step:8822 [D loss: 0.664207, acc: 60.16%] [G loss: 2.033549]\n",
      "epoch:9 step:8823 [D loss: 0.681533, acc: 58.59%] [G loss: 1.978108]\n",
      "epoch:9 step:8824 [D loss: 0.692700, acc: 56.25%] [G loss: 2.015773]\n",
      "epoch:9 step:8825 [D loss: 0.645747, acc: 65.62%] [G loss: 2.166915]\n",
      "epoch:9 step:8826 [D loss: 0.609822, acc: 61.72%] [G loss: 1.979543]\n",
      "epoch:9 step:8827 [D loss: 0.616838, acc: 67.97%] [G loss: 2.159248]\n",
      "epoch:9 step:8828 [D loss: 0.667404, acc: 60.94%] [G loss: 2.026025]\n",
      "epoch:9 step:8829 [D loss: 0.683860, acc: 62.50%] [G loss: 1.977034]\n",
      "epoch:9 step:8830 [D loss: 0.652444, acc: 60.16%] [G loss: 1.907826]\n",
      "epoch:9 step:8831 [D loss: 0.583711, acc: 67.97%] [G loss: 2.135115]\n",
      "epoch:9 step:8832 [D loss: 0.597131, acc: 66.41%] [G loss: 2.198044]\n",
      "epoch:9 step:8833 [D loss: 0.672310, acc: 60.94%] [G loss: 2.111997]\n",
      "epoch:9 step:8834 [D loss: 0.658726, acc: 59.38%] [G loss: 2.233532]\n",
      "epoch:9 step:8835 [D loss: 0.566547, acc: 72.66%] [G loss: 2.161604]\n",
      "epoch:9 step:8836 [D loss: 0.619461, acc: 65.62%] [G loss: 1.929826]\n",
      "epoch:9 step:8837 [D loss: 0.658549, acc: 62.50%] [G loss: 2.154540]\n",
      "epoch:9 step:8838 [D loss: 0.578908, acc: 71.09%] [G loss: 2.200512]\n",
      "epoch:9 step:8839 [D loss: 0.596649, acc: 64.84%] [G loss: 2.332876]\n",
      "epoch:9 step:8840 [D loss: 0.732194, acc: 57.03%] [G loss: 2.106008]\n",
      "epoch:9 step:8841 [D loss: 0.617267, acc: 67.19%] [G loss: 2.067385]\n",
      "epoch:9 step:8842 [D loss: 0.619501, acc: 64.84%] [G loss: 2.057652]\n",
      "epoch:9 step:8843 [D loss: 0.567624, acc: 71.09%] [G loss: 2.308883]\n",
      "epoch:9 step:8844 [D loss: 0.698033, acc: 55.47%] [G loss: 2.009300]\n",
      "epoch:9 step:8845 [D loss: 0.661493, acc: 64.84%] [G loss: 1.937234]\n",
      "epoch:9 step:8846 [D loss: 0.677776, acc: 58.59%] [G loss: 2.189636]\n",
      "epoch:9 step:8847 [D loss: 0.640423, acc: 65.62%] [G loss: 2.111436]\n",
      "epoch:9 step:8848 [D loss: 0.622869, acc: 68.75%] [G loss: 2.154536]\n",
      "epoch:9 step:8849 [D loss: 0.676877, acc: 55.47%] [G loss: 2.200109]\n",
      "epoch:9 step:8850 [D loss: 0.700013, acc: 64.84%] [G loss: 1.989548]\n",
      "epoch:9 step:8851 [D loss: 0.640455, acc: 66.41%] [G loss: 2.147508]\n",
      "epoch:9 step:8852 [D loss: 0.578512, acc: 67.19%] [G loss: 2.197757]\n",
      "epoch:9 step:8853 [D loss: 0.661036, acc: 59.38%] [G loss: 2.020587]\n",
      "epoch:9 step:8854 [D loss: 0.641854, acc: 59.38%] [G loss: 2.155827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8855 [D loss: 0.636123, acc: 63.28%] [G loss: 2.151606]\n",
      "epoch:9 step:8856 [D loss: 0.702906, acc: 55.47%] [G loss: 1.962341]\n",
      "epoch:9 step:8857 [D loss: 0.600241, acc: 67.19%] [G loss: 2.123684]\n",
      "epoch:9 step:8858 [D loss: 0.642781, acc: 60.16%] [G loss: 2.211546]\n",
      "epoch:9 step:8859 [D loss: 0.628366, acc: 65.62%] [G loss: 2.152298]\n",
      "epoch:9 step:8860 [D loss: 0.615226, acc: 64.84%] [G loss: 2.103421]\n",
      "epoch:9 step:8861 [D loss: 0.532053, acc: 73.44%] [G loss: 2.429389]\n",
      "epoch:9 step:8862 [D loss: 0.569560, acc: 70.31%] [G loss: 2.649211]\n",
      "epoch:9 step:8863 [D loss: 0.576801, acc: 70.31%] [G loss: 2.604896]\n",
      "epoch:9 step:8864 [D loss: 0.680915, acc: 66.41%] [G loss: 2.070663]\n",
      "epoch:9 step:8865 [D loss: 0.704724, acc: 51.56%] [G loss: 1.998313]\n",
      "epoch:9 step:8866 [D loss: 0.681024, acc: 65.62%] [G loss: 2.014193]\n",
      "epoch:9 step:8867 [D loss: 0.600551, acc: 69.53%] [G loss: 2.163775]\n",
      "epoch:9 step:8868 [D loss: 0.648480, acc: 60.94%] [G loss: 2.023736]\n",
      "epoch:9 step:8869 [D loss: 0.585242, acc: 71.09%] [G loss: 2.175891]\n",
      "epoch:9 step:8870 [D loss: 0.714030, acc: 57.81%] [G loss: 1.687209]\n",
      "epoch:9 step:8871 [D loss: 0.627945, acc: 67.97%] [G loss: 1.970958]\n",
      "epoch:9 step:8872 [D loss: 0.642809, acc: 64.06%] [G loss: 1.877786]\n",
      "epoch:9 step:8873 [D loss: 0.659054, acc: 57.81%] [G loss: 1.886463]\n",
      "epoch:9 step:8874 [D loss: 0.696643, acc: 58.59%] [G loss: 1.910647]\n",
      "epoch:9 step:8875 [D loss: 0.673724, acc: 63.28%] [G loss: 2.019347]\n",
      "epoch:9 step:8876 [D loss: 0.614761, acc: 64.84%] [G loss: 1.873715]\n",
      "epoch:9 step:8877 [D loss: 0.629880, acc: 64.06%] [G loss: 1.959561]\n",
      "epoch:9 step:8878 [D loss: 0.609603, acc: 65.62%] [G loss: 2.022923]\n",
      "epoch:9 step:8879 [D loss: 0.642284, acc: 61.72%] [G loss: 1.858799]\n",
      "epoch:9 step:8880 [D loss: 0.664956, acc: 64.06%] [G loss: 1.947319]\n",
      "epoch:9 step:8881 [D loss: 0.641759, acc: 63.28%] [G loss: 1.916674]\n",
      "epoch:9 step:8882 [D loss: 0.592587, acc: 67.97%] [G loss: 2.103011]\n",
      "epoch:9 step:8883 [D loss: 0.635798, acc: 64.06%] [G loss: 1.934587]\n",
      "epoch:9 step:8884 [D loss: 0.601755, acc: 70.31%] [G loss: 2.025255]\n",
      "epoch:9 step:8885 [D loss: 0.647628, acc: 68.75%] [G loss: 1.905325]\n",
      "epoch:9 step:8886 [D loss: 0.558667, acc: 68.75%] [G loss: 2.199806]\n",
      "epoch:9 step:8887 [D loss: 0.617267, acc: 66.41%] [G loss: 2.107505]\n",
      "epoch:9 step:8888 [D loss: 0.576677, acc: 72.66%] [G loss: 2.070951]\n",
      "epoch:9 step:8889 [D loss: 0.695601, acc: 61.72%] [G loss: 2.046720]\n",
      "epoch:9 step:8890 [D loss: 0.602753, acc: 66.41%] [G loss: 2.038106]\n",
      "epoch:9 step:8891 [D loss: 0.632026, acc: 65.62%] [G loss: 1.856573]\n",
      "epoch:9 step:8892 [D loss: 0.667334, acc: 57.81%] [G loss: 2.044280]\n",
      "epoch:9 step:8893 [D loss: 0.614378, acc: 66.41%] [G loss: 2.061962]\n",
      "epoch:9 step:8894 [D loss: 0.611399, acc: 67.97%] [G loss: 2.068012]\n",
      "epoch:9 step:8895 [D loss: 0.615552, acc: 67.19%] [G loss: 2.022484]\n",
      "epoch:9 step:8896 [D loss: 0.697233, acc: 54.69%] [G loss: 2.058985]\n",
      "epoch:9 step:8897 [D loss: 0.604508, acc: 72.66%] [G loss: 2.176924]\n",
      "epoch:9 step:8898 [D loss: 0.597974, acc: 63.28%] [G loss: 2.120018]\n",
      "epoch:9 step:8899 [D loss: 0.713757, acc: 57.03%] [G loss: 2.176433]\n",
      "epoch:9 step:8900 [D loss: 0.624303, acc: 65.62%] [G loss: 2.062529]\n",
      "epoch:9 step:8901 [D loss: 0.663946, acc: 59.38%] [G loss: 2.338446]\n",
      "epoch:9 step:8902 [D loss: 0.604578, acc: 61.72%] [G loss: 2.263552]\n",
      "epoch:9 step:8903 [D loss: 0.623482, acc: 67.97%] [G loss: 2.209459]\n",
      "epoch:9 step:8904 [D loss: 0.536812, acc: 73.44%] [G loss: 2.693182]\n",
      "epoch:9 step:8905 [D loss: 0.570607, acc: 72.66%] [G loss: 2.417186]\n",
      "epoch:9 step:8906 [D loss: 0.632130, acc: 65.62%] [G loss: 2.037358]\n",
      "epoch:9 step:8907 [D loss: 0.570040, acc: 70.31%] [G loss: 2.305003]\n",
      "epoch:9 step:8908 [D loss: 0.616264, acc: 64.06%] [G loss: 2.233148]\n",
      "epoch:9 step:8909 [D loss: 0.672334, acc: 60.16%] [G loss: 2.127675]\n",
      "epoch:9 step:8910 [D loss: 0.653324, acc: 62.50%] [G loss: 1.971644]\n",
      "epoch:9 step:8911 [D loss: 0.680206, acc: 57.81%] [G loss: 1.985809]\n",
      "epoch:9 step:8912 [D loss: 0.591918, acc: 67.19%] [G loss: 2.193371]\n",
      "epoch:9 step:8913 [D loss: 0.581777, acc: 69.53%] [G loss: 2.004039]\n",
      "epoch:9 step:8914 [D loss: 0.615206, acc: 67.97%] [G loss: 2.117809]\n",
      "epoch:9 step:8915 [D loss: 0.624459, acc: 63.28%] [G loss: 1.909948]\n",
      "epoch:9 step:8916 [D loss: 0.722428, acc: 54.69%] [G loss: 2.144919]\n",
      "epoch:9 step:8917 [D loss: 0.642010, acc: 65.62%] [G loss: 2.202596]\n",
      "epoch:9 step:8918 [D loss: 0.640758, acc: 64.84%] [G loss: 2.056416]\n",
      "epoch:9 step:8919 [D loss: 0.614161, acc: 67.97%] [G loss: 2.122353]\n",
      "epoch:9 step:8920 [D loss: 0.691523, acc: 57.81%] [G loss: 2.019131]\n",
      "epoch:9 step:8921 [D loss: 0.591587, acc: 75.78%] [G loss: 2.300624]\n",
      "epoch:9 step:8922 [D loss: 0.562352, acc: 70.31%] [G loss: 2.031894]\n",
      "epoch:9 step:8923 [D loss: 0.661999, acc: 64.84%] [G loss: 2.063039]\n",
      "epoch:9 step:8924 [D loss: 0.675935, acc: 61.72%] [G loss: 2.063704]\n",
      "epoch:9 step:8925 [D loss: 0.609691, acc: 64.06%] [G loss: 1.982509]\n",
      "epoch:9 step:8926 [D loss: 0.660760, acc: 62.50%] [G loss: 1.946058]\n",
      "epoch:9 step:8927 [D loss: 0.621515, acc: 68.75%] [G loss: 2.043211]\n",
      "epoch:9 step:8928 [D loss: 0.599409, acc: 68.75%] [G loss: 2.276381]\n",
      "epoch:9 step:8929 [D loss: 0.641976, acc: 64.84%] [G loss: 1.994579]\n",
      "epoch:9 step:8930 [D loss: 0.638415, acc: 63.28%] [G loss: 2.169598]\n",
      "epoch:9 step:8931 [D loss: 0.584486, acc: 70.31%] [G loss: 2.358924]\n",
      "epoch:9 step:8932 [D loss: 0.560537, acc: 67.97%] [G loss: 2.198797]\n",
      "epoch:9 step:8933 [D loss: 0.662189, acc: 64.84%] [G loss: 1.996503]\n",
      "epoch:9 step:8934 [D loss: 0.681810, acc: 55.47%] [G loss: 1.979697]\n",
      "epoch:9 step:8935 [D loss: 0.690335, acc: 53.12%] [G loss: 1.925235]\n",
      "epoch:9 step:8936 [D loss: 0.580514, acc: 73.44%] [G loss: 2.035209]\n",
      "epoch:9 step:8937 [D loss: 0.588850, acc: 71.09%] [G loss: 2.350997]\n",
      "epoch:9 step:8938 [D loss: 0.604040, acc: 64.06%] [G loss: 2.176914]\n",
      "epoch:9 step:8939 [D loss: 0.650123, acc: 60.94%] [G loss: 2.169467]\n",
      "epoch:9 step:8940 [D loss: 0.634086, acc: 63.28%] [G loss: 2.135554]\n",
      "epoch:9 step:8941 [D loss: 0.551391, acc: 74.22%] [G loss: 2.208720]\n",
      "epoch:9 step:8942 [D loss: 0.571745, acc: 75.00%] [G loss: 2.146751]\n",
      "epoch:9 step:8943 [D loss: 0.680685, acc: 57.03%] [G loss: 1.890056]\n",
      "epoch:9 step:8944 [D loss: 0.734160, acc: 53.12%] [G loss: 1.933751]\n",
      "epoch:9 step:8945 [D loss: 0.656520, acc: 64.06%] [G loss: 1.931777]\n",
      "epoch:9 step:8946 [D loss: 0.599894, acc: 71.09%] [G loss: 2.066901]\n",
      "epoch:9 step:8947 [D loss: 0.617617, acc: 68.75%] [G loss: 2.224520]\n",
      "epoch:9 step:8948 [D loss: 0.650375, acc: 65.62%] [G loss: 2.049263]\n",
      "epoch:9 step:8949 [D loss: 0.605466, acc: 67.97%] [G loss: 2.047586]\n",
      "epoch:9 step:8950 [D loss: 0.619779, acc: 66.41%] [G loss: 2.263395]\n",
      "epoch:9 step:8951 [D loss: 0.701813, acc: 62.50%] [G loss: 2.123226]\n",
      "epoch:9 step:8952 [D loss: 0.624085, acc: 61.72%] [G loss: 2.115711]\n",
      "epoch:9 step:8953 [D loss: 0.587254, acc: 67.97%] [G loss: 2.195977]\n",
      "epoch:9 step:8954 [D loss: 0.537208, acc: 77.34%] [G loss: 2.151699]\n",
      "epoch:9 step:8955 [D loss: 0.555206, acc: 72.66%] [G loss: 2.183180]\n",
      "epoch:9 step:8956 [D loss: 0.636628, acc: 63.28%] [G loss: 2.075461]\n",
      "epoch:9 step:8957 [D loss: 0.620133, acc: 64.84%] [G loss: 2.212835]\n",
      "epoch:9 step:8958 [D loss: 0.675791, acc: 59.38%] [G loss: 2.066886]\n",
      "epoch:9 step:8959 [D loss: 0.585355, acc: 70.31%] [G loss: 1.983637]\n",
      "epoch:9 step:8960 [D loss: 0.667558, acc: 60.16%] [G loss: 2.069184]\n",
      "epoch:9 step:8961 [D loss: 0.701222, acc: 53.12%] [G loss: 1.857201]\n",
      "epoch:9 step:8962 [D loss: 0.696407, acc: 59.38%] [G loss: 1.897906]\n",
      "epoch:9 step:8963 [D loss: 0.632215, acc: 64.06%] [G loss: 2.013202]\n",
      "epoch:9 step:8964 [D loss: 0.630263, acc: 62.50%] [G loss: 2.206742]\n",
      "epoch:9 step:8965 [D loss: 0.578449, acc: 75.78%] [G loss: 2.121410]\n",
      "epoch:9 step:8966 [D loss: 0.643342, acc: 66.41%] [G loss: 2.166182]\n",
      "epoch:9 step:8967 [D loss: 0.637560, acc: 64.06%] [G loss: 2.033199]\n",
      "epoch:9 step:8968 [D loss: 0.560699, acc: 75.78%] [G loss: 2.048942]\n",
      "epoch:9 step:8969 [D loss: 0.626577, acc: 64.06%] [G loss: 2.246141]\n",
      "epoch:9 step:8970 [D loss: 0.683930, acc: 59.38%] [G loss: 1.892008]\n",
      "epoch:9 step:8971 [D loss: 0.670122, acc: 57.03%] [G loss: 1.972132]\n",
      "epoch:9 step:8972 [D loss: 0.617234, acc: 67.19%] [G loss: 2.232531]\n",
      "epoch:9 step:8973 [D loss: 0.680805, acc: 52.34%] [G loss: 1.975635]\n",
      "epoch:9 step:8974 [D loss: 0.622195, acc: 65.62%] [G loss: 2.074605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8975 [D loss: 0.621876, acc: 66.41%] [G loss: 1.994762]\n",
      "epoch:9 step:8976 [D loss: 0.670221, acc: 59.38%] [G loss: 1.974887]\n",
      "epoch:9 step:8977 [D loss: 0.654800, acc: 60.94%] [G loss: 2.034452]\n",
      "epoch:9 step:8978 [D loss: 0.682708, acc: 59.38%] [G loss: 2.143133]\n",
      "epoch:9 step:8979 [D loss: 0.606799, acc: 67.97%] [G loss: 2.147821]\n",
      "epoch:9 step:8980 [D loss: 0.656636, acc: 58.59%] [G loss: 2.193018]\n",
      "epoch:9 step:8981 [D loss: 0.625996, acc: 63.28%] [G loss: 2.143811]\n",
      "epoch:9 step:8982 [D loss: 0.630806, acc: 64.06%] [G loss: 2.153150]\n",
      "epoch:9 step:8983 [D loss: 0.612345, acc: 64.84%] [G loss: 2.205001]\n",
      "epoch:9 step:8984 [D loss: 0.586851, acc: 72.66%] [G loss: 2.104609]\n",
      "epoch:9 step:8985 [D loss: 0.604011, acc: 67.19%] [G loss: 2.116717]\n",
      "epoch:9 step:8986 [D loss: 0.706439, acc: 57.81%] [G loss: 1.886529]\n",
      "epoch:9 step:8987 [D loss: 0.591389, acc: 69.53%] [G loss: 2.367972]\n",
      "epoch:9 step:8988 [D loss: 0.601513, acc: 68.75%] [G loss: 2.191961]\n",
      "epoch:9 step:8989 [D loss: 0.601959, acc: 66.41%] [G loss: 2.039672]\n",
      "epoch:9 step:8990 [D loss: 0.658049, acc: 57.81%] [G loss: 2.025994]\n",
      "epoch:9 step:8991 [D loss: 0.564113, acc: 72.66%] [G loss: 2.250818]\n",
      "epoch:9 step:8992 [D loss: 0.701691, acc: 57.81%] [G loss: 1.965503]\n",
      "epoch:9 step:8993 [D loss: 0.638163, acc: 66.41%] [G loss: 2.016836]\n",
      "epoch:9 step:8994 [D loss: 0.599294, acc: 60.94%] [G loss: 1.956607]\n",
      "epoch:9 step:8995 [D loss: 0.628683, acc: 65.62%] [G loss: 2.086845]\n",
      "epoch:9 step:8996 [D loss: 0.633694, acc: 60.94%] [G loss: 2.088419]\n",
      "epoch:9 step:8997 [D loss: 0.574523, acc: 71.88%] [G loss: 2.239164]\n",
      "epoch:9 step:8998 [D loss: 0.662962, acc: 59.38%] [G loss: 1.929619]\n",
      "epoch:9 step:8999 [D loss: 0.756654, acc: 53.12%] [G loss: 1.880490]\n",
      "epoch:9 step:9000 [D loss: 0.612687, acc: 67.19%] [G loss: 2.067336]\n",
      "##############\n",
      "[2.55479218 1.40673241 6.43095821 5.03785789 3.94171104 5.85076545\n",
      " 4.6640196  4.87932834 4.73298267 3.61262284]\n",
      "##########\n",
      "epoch:9 step:9001 [D loss: 0.598464, acc: 67.97%] [G loss: 2.102826]\n",
      "epoch:9 step:9002 [D loss: 0.677635, acc: 63.28%] [G loss: 2.040664]\n",
      "epoch:9 step:9003 [D loss: 0.632774, acc: 66.41%] [G loss: 1.989939]\n",
      "epoch:9 step:9004 [D loss: 0.605645, acc: 67.97%] [G loss: 2.066023]\n",
      "epoch:9 step:9005 [D loss: 0.673320, acc: 60.94%] [G loss: 1.974693]\n",
      "epoch:9 step:9006 [D loss: 0.609936, acc: 68.75%] [G loss: 1.919898]\n",
      "epoch:9 step:9007 [D loss: 0.669507, acc: 58.59%] [G loss: 2.158421]\n",
      "epoch:9 step:9008 [D loss: 0.659125, acc: 60.94%] [G loss: 1.980500]\n",
      "epoch:9 step:9009 [D loss: 0.669653, acc: 66.41%] [G loss: 1.844225]\n",
      "epoch:9 step:9010 [D loss: 0.681645, acc: 59.38%] [G loss: 1.963088]\n",
      "epoch:9 step:9011 [D loss: 0.634673, acc: 61.72%] [G loss: 1.948382]\n",
      "epoch:9 step:9012 [D loss: 0.650425, acc: 63.28%] [G loss: 1.819239]\n",
      "epoch:9 step:9013 [D loss: 0.646115, acc: 63.28%] [G loss: 2.041817]\n",
      "epoch:9 step:9014 [D loss: 0.584216, acc: 75.78%] [G loss: 2.063487]\n",
      "epoch:9 step:9015 [D loss: 0.584416, acc: 71.09%] [G loss: 2.024347]\n",
      "epoch:9 step:9016 [D loss: 0.676239, acc: 60.16%] [G loss: 1.917747]\n",
      "epoch:9 step:9017 [D loss: 0.670572, acc: 58.59%] [G loss: 1.980446]\n",
      "epoch:9 step:9018 [D loss: 0.680401, acc: 61.72%] [G loss: 1.867308]\n",
      "epoch:9 step:9019 [D loss: 0.655991, acc: 60.94%] [G loss: 1.993252]\n",
      "epoch:9 step:9020 [D loss: 0.658595, acc: 62.50%] [G loss: 2.090677]\n",
      "epoch:9 step:9021 [D loss: 0.631159, acc: 66.41%] [G loss: 2.072338]\n",
      "epoch:9 step:9022 [D loss: 0.606166, acc: 66.41%] [G loss: 2.140903]\n",
      "epoch:9 step:9023 [D loss: 0.612560, acc: 70.31%] [G loss: 1.959582]\n",
      "epoch:9 step:9024 [D loss: 0.561530, acc: 65.62%] [G loss: 2.146180]\n",
      "epoch:9 step:9025 [D loss: 0.646271, acc: 58.59%] [G loss: 2.111043]\n",
      "epoch:9 step:9026 [D loss: 0.624907, acc: 66.41%] [G loss: 2.014916]\n",
      "epoch:9 step:9027 [D loss: 0.642053, acc: 66.41%] [G loss: 1.908326]\n",
      "epoch:9 step:9028 [D loss: 0.590029, acc: 71.09%] [G loss: 1.984768]\n",
      "epoch:9 step:9029 [D loss: 0.621724, acc: 63.28%] [G loss: 2.033485]\n",
      "epoch:9 step:9030 [D loss: 0.678570, acc: 57.81%] [G loss: 2.049345]\n",
      "epoch:9 step:9031 [D loss: 0.682463, acc: 58.59%] [G loss: 2.053895]\n",
      "epoch:9 step:9032 [D loss: 0.662465, acc: 55.47%] [G loss: 2.087690]\n",
      "epoch:9 step:9033 [D loss: 0.713579, acc: 54.69%] [G loss: 2.011744]\n",
      "epoch:9 step:9034 [D loss: 0.688576, acc: 58.59%] [G loss: 2.073959]\n",
      "epoch:9 step:9035 [D loss: 0.609937, acc: 64.84%] [G loss: 2.099144]\n",
      "epoch:9 step:9036 [D loss: 0.647906, acc: 57.81%] [G loss: 1.982289]\n",
      "epoch:9 step:9037 [D loss: 0.659397, acc: 64.06%] [G loss: 1.920448]\n",
      "epoch:9 step:9038 [D loss: 0.644676, acc: 61.72%] [G loss: 2.224719]\n",
      "epoch:9 step:9039 [D loss: 0.667867, acc: 57.03%] [G loss: 1.900489]\n",
      "epoch:9 step:9040 [D loss: 0.681169, acc: 62.50%] [G loss: 1.958553]\n",
      "epoch:9 step:9041 [D loss: 0.653980, acc: 64.06%] [G loss: 2.100112]\n",
      "epoch:9 step:9042 [D loss: 0.583754, acc: 66.41%] [G loss: 2.020993]\n",
      "epoch:9 step:9043 [D loss: 0.619442, acc: 71.09%] [G loss: 1.883299]\n",
      "epoch:9 step:9044 [D loss: 0.676178, acc: 54.69%] [G loss: 1.933737]\n",
      "epoch:9 step:9045 [D loss: 0.598513, acc: 71.09%] [G loss: 2.002629]\n",
      "epoch:9 step:9046 [D loss: 0.596024, acc: 65.62%] [G loss: 2.026099]\n",
      "epoch:9 step:9047 [D loss: 0.625470, acc: 61.72%] [G loss: 2.038537]\n",
      "epoch:9 step:9048 [D loss: 0.679568, acc: 54.69%] [G loss: 1.747357]\n",
      "epoch:9 step:9049 [D loss: 0.677800, acc: 54.69%] [G loss: 1.959700]\n",
      "epoch:9 step:9050 [D loss: 0.641499, acc: 62.50%] [G loss: 2.018400]\n",
      "epoch:9 step:9051 [D loss: 0.613227, acc: 67.97%] [G loss: 1.919203]\n",
      "epoch:9 step:9052 [D loss: 0.574077, acc: 71.09%] [G loss: 2.064366]\n",
      "epoch:9 step:9053 [D loss: 0.622857, acc: 65.62%] [G loss: 2.192390]\n",
      "epoch:9 step:9054 [D loss: 0.686884, acc: 57.03%] [G loss: 1.990816]\n",
      "epoch:9 step:9055 [D loss: 0.666392, acc: 59.38%] [G loss: 1.950801]\n",
      "epoch:9 step:9056 [D loss: 0.633811, acc: 63.28%] [G loss: 1.994326]\n",
      "epoch:9 step:9057 [D loss: 0.587731, acc: 69.53%] [G loss: 2.085863]\n",
      "epoch:9 step:9058 [D loss: 0.599963, acc: 64.84%] [G loss: 2.044589]\n",
      "epoch:9 step:9059 [D loss: 0.580184, acc: 70.31%] [G loss: 2.085096]\n",
      "epoch:9 step:9060 [D loss: 0.642069, acc: 65.62%] [G loss: 2.066358]\n",
      "epoch:9 step:9061 [D loss: 0.648773, acc: 60.16%] [G loss: 2.130171]\n",
      "epoch:9 step:9062 [D loss: 0.619653, acc: 60.94%] [G loss: 2.024023]\n",
      "epoch:9 step:9063 [D loss: 0.616561, acc: 67.19%] [G loss: 2.166618]\n",
      "epoch:9 step:9064 [D loss: 0.627719, acc: 64.06%] [G loss: 2.278406]\n",
      "epoch:9 step:9065 [D loss: 0.638285, acc: 65.62%] [G loss: 2.041344]\n",
      "epoch:9 step:9066 [D loss: 0.631698, acc: 63.28%] [G loss: 2.118067]\n",
      "epoch:9 step:9067 [D loss: 0.557929, acc: 69.53%] [G loss: 2.310643]\n",
      "epoch:9 step:9068 [D loss: 0.615674, acc: 71.88%] [G loss: 2.169158]\n",
      "epoch:9 step:9069 [D loss: 0.631984, acc: 62.50%] [G loss: 2.118333]\n",
      "epoch:9 step:9070 [D loss: 0.585442, acc: 66.41%] [G loss: 2.217097]\n",
      "epoch:9 step:9071 [D loss: 0.622955, acc: 66.41%] [G loss: 2.194603]\n",
      "epoch:9 step:9072 [D loss: 0.613253, acc: 64.06%] [G loss: 2.263376]\n",
      "epoch:9 step:9073 [D loss: 0.658853, acc: 63.28%] [G loss: 2.087151]\n",
      "epoch:9 step:9074 [D loss: 0.623214, acc: 58.59%] [G loss: 2.390574]\n",
      "epoch:9 step:9075 [D loss: 0.553401, acc: 73.44%] [G loss: 2.450038]\n",
      "epoch:9 step:9076 [D loss: 0.635007, acc: 61.72%] [G loss: 2.073730]\n",
      "epoch:9 step:9077 [D loss: 0.601121, acc: 67.19%] [G loss: 2.091434]\n",
      "epoch:9 step:9078 [D loss: 0.606279, acc: 71.88%] [G loss: 2.344351]\n",
      "epoch:9 step:9079 [D loss: 0.699843, acc: 66.41%] [G loss: 2.305070]\n",
      "epoch:9 step:9080 [D loss: 0.566054, acc: 71.88%] [G loss: 2.389701]\n",
      "epoch:9 step:9081 [D loss: 0.560598, acc: 66.41%] [G loss: 2.519203]\n",
      "epoch:9 step:9082 [D loss: 0.654049, acc: 56.25%] [G loss: 2.176580]\n",
      "epoch:9 step:9083 [D loss: 0.607812, acc: 71.09%] [G loss: 2.384032]\n",
      "epoch:9 step:9084 [D loss: 0.632927, acc: 63.28%] [G loss: 2.226711]\n",
      "epoch:9 step:9085 [D loss: 0.645511, acc: 62.50%] [G loss: 2.027210]\n",
      "epoch:9 step:9086 [D loss: 0.625922, acc: 64.84%] [G loss: 2.064125]\n",
      "epoch:9 step:9087 [D loss: 0.632049, acc: 61.72%] [G loss: 2.159436]\n",
      "epoch:9 step:9088 [D loss: 0.645892, acc: 61.72%] [G loss: 2.168264]\n",
      "epoch:9 step:9089 [D loss: 0.670369, acc: 60.16%] [G loss: 2.010574]\n",
      "epoch:9 step:9090 [D loss: 0.642159, acc: 60.94%] [G loss: 1.961597]\n",
      "epoch:9 step:9091 [D loss: 0.638483, acc: 64.06%] [G loss: 1.913063]\n",
      "epoch:9 step:9092 [D loss: 0.668598, acc: 62.50%] [G loss: 1.985742]\n",
      "epoch:9 step:9093 [D loss: 0.638408, acc: 62.50%] [G loss: 2.143256]\n",
      "epoch:9 step:9094 [D loss: 0.568275, acc: 73.44%] [G loss: 2.105492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9095 [D loss: 0.594746, acc: 67.97%] [G loss: 2.098718]\n",
      "epoch:9 step:9096 [D loss: 0.598926, acc: 65.62%] [G loss: 2.138676]\n",
      "epoch:9 step:9097 [D loss: 0.656292, acc: 67.19%] [G loss: 2.103975]\n",
      "epoch:9 step:9098 [D loss: 0.637726, acc: 63.28%] [G loss: 2.237235]\n",
      "epoch:9 step:9099 [D loss: 0.595723, acc: 69.53%] [G loss: 2.086026]\n",
      "epoch:9 step:9100 [D loss: 0.640159, acc: 64.84%] [G loss: 1.996714]\n",
      "epoch:9 step:9101 [D loss: 0.599324, acc: 66.41%] [G loss: 2.084147]\n",
      "epoch:9 step:9102 [D loss: 0.646971, acc: 62.50%] [G loss: 1.883257]\n",
      "epoch:9 step:9103 [D loss: 0.658764, acc: 61.72%] [G loss: 1.886322]\n",
      "epoch:9 step:9104 [D loss: 0.628663, acc: 64.84%] [G loss: 2.156273]\n",
      "epoch:9 step:9105 [D loss: 0.637007, acc: 65.62%] [G loss: 2.040312]\n",
      "epoch:9 step:9106 [D loss: 0.649147, acc: 67.97%] [G loss: 2.121908]\n",
      "epoch:9 step:9107 [D loss: 0.671127, acc: 59.38%] [G loss: 2.115671]\n",
      "epoch:9 step:9108 [D loss: 0.617667, acc: 67.97%] [G loss: 2.068844]\n",
      "epoch:9 step:9109 [D loss: 0.592314, acc: 67.97%] [G loss: 2.176297]\n",
      "epoch:9 step:9110 [D loss: 0.595118, acc: 67.97%] [G loss: 2.223819]\n",
      "epoch:9 step:9111 [D loss: 0.652916, acc: 62.50%] [G loss: 2.066618]\n",
      "epoch:9 step:9112 [D loss: 0.608114, acc: 65.62%] [G loss: 1.991959]\n",
      "epoch:9 step:9113 [D loss: 0.602698, acc: 72.66%] [G loss: 2.135625]\n",
      "epoch:9 step:9114 [D loss: 0.582526, acc: 70.31%] [G loss: 2.114693]\n",
      "epoch:9 step:9115 [D loss: 0.635172, acc: 60.94%] [G loss: 1.946265]\n",
      "epoch:9 step:9116 [D loss: 0.659199, acc: 67.19%] [G loss: 2.171299]\n",
      "epoch:9 step:9117 [D loss: 0.632791, acc: 66.41%] [G loss: 2.044464]\n",
      "epoch:9 step:9118 [D loss: 0.621547, acc: 69.53%] [G loss: 2.203267]\n",
      "epoch:9 step:9119 [D loss: 0.615250, acc: 66.41%] [G loss: 2.162509]\n",
      "epoch:9 step:9120 [D loss: 0.636456, acc: 67.19%] [G loss: 2.130022]\n",
      "epoch:9 step:9121 [D loss: 0.633358, acc: 64.06%] [G loss: 2.073411]\n",
      "epoch:9 step:9122 [D loss: 0.633817, acc: 64.84%] [G loss: 2.037609]\n",
      "epoch:9 step:9123 [D loss: 0.655263, acc: 60.94%] [G loss: 2.113896]\n",
      "epoch:9 step:9124 [D loss: 0.600985, acc: 64.84%] [G loss: 2.322551]\n",
      "epoch:9 step:9125 [D loss: 0.591987, acc: 68.75%] [G loss: 2.252973]\n",
      "epoch:9 step:9126 [D loss: 0.574971, acc: 67.97%] [G loss: 2.304201]\n",
      "epoch:9 step:9127 [D loss: 0.597407, acc: 68.75%] [G loss: 2.384792]\n",
      "epoch:9 step:9128 [D loss: 0.599234, acc: 65.62%] [G loss: 2.338390]\n",
      "epoch:9 step:9129 [D loss: 0.681245, acc: 58.59%] [G loss: 2.085445]\n",
      "epoch:9 step:9130 [D loss: 0.634880, acc: 62.50%] [G loss: 2.149441]\n",
      "epoch:9 step:9131 [D loss: 0.619123, acc: 66.41%] [G loss: 2.007877]\n",
      "epoch:9 step:9132 [D loss: 0.641608, acc: 64.84%] [G loss: 2.208596]\n",
      "epoch:9 step:9133 [D loss: 0.656512, acc: 57.03%] [G loss: 2.126569]\n",
      "epoch:9 step:9134 [D loss: 0.616735, acc: 65.62%] [G loss: 2.183780]\n",
      "epoch:9 step:9135 [D loss: 0.737444, acc: 54.69%] [G loss: 2.042343]\n",
      "epoch:9 step:9136 [D loss: 0.633231, acc: 64.84%] [G loss: 1.989879]\n",
      "epoch:9 step:9137 [D loss: 0.655836, acc: 60.16%] [G loss: 1.880114]\n",
      "epoch:9 step:9138 [D loss: 0.570379, acc: 74.22%] [G loss: 2.090883]\n",
      "epoch:9 step:9139 [D loss: 0.629558, acc: 60.94%] [G loss: 2.007373]\n",
      "epoch:9 step:9140 [D loss: 0.547231, acc: 73.44%] [G loss: 2.483834]\n",
      "epoch:9 step:9141 [D loss: 0.641859, acc: 65.62%] [G loss: 2.161154]\n",
      "epoch:9 step:9142 [D loss: 0.565291, acc: 73.44%] [G loss: 2.214113]\n",
      "epoch:9 step:9143 [D loss: 0.711772, acc: 53.91%] [G loss: 1.905529]\n",
      "epoch:9 step:9144 [D loss: 0.610863, acc: 64.06%] [G loss: 2.092204]\n",
      "epoch:9 step:9145 [D loss: 0.650800, acc: 62.50%] [G loss: 2.094881]\n",
      "epoch:9 step:9146 [D loss: 0.616122, acc: 67.19%] [G loss: 2.168918]\n",
      "epoch:9 step:9147 [D loss: 0.617628, acc: 64.84%] [G loss: 2.163665]\n",
      "epoch:9 step:9148 [D loss: 0.632857, acc: 67.97%] [G loss: 2.072957]\n",
      "epoch:9 step:9149 [D loss: 0.656343, acc: 61.72%] [G loss: 2.154490]\n",
      "epoch:9 step:9150 [D loss: 0.620900, acc: 62.50%] [G loss: 2.107472]\n",
      "epoch:9 step:9151 [D loss: 0.646691, acc: 65.62%] [G loss: 2.099693]\n",
      "epoch:9 step:9152 [D loss: 0.609915, acc: 67.97%] [G loss: 2.231169]\n",
      "epoch:9 step:9153 [D loss: 0.653756, acc: 64.84%] [G loss: 2.155335]\n",
      "epoch:9 step:9154 [D loss: 0.609047, acc: 67.97%] [G loss: 2.085201]\n",
      "epoch:9 step:9155 [D loss: 0.682554, acc: 55.47%] [G loss: 2.084997]\n",
      "epoch:9 step:9156 [D loss: 0.679700, acc: 60.16%] [G loss: 1.896383]\n",
      "epoch:9 step:9157 [D loss: 0.607309, acc: 64.84%] [G loss: 2.003781]\n",
      "epoch:9 step:9158 [D loss: 0.595084, acc: 73.44%] [G loss: 2.144124]\n",
      "epoch:9 step:9159 [D loss: 0.629905, acc: 64.84%] [G loss: 2.198047]\n",
      "epoch:9 step:9160 [D loss: 0.648039, acc: 60.94%] [G loss: 1.994950]\n",
      "epoch:9 step:9161 [D loss: 0.629744, acc: 63.28%] [G loss: 2.217647]\n",
      "epoch:9 step:9162 [D loss: 0.610363, acc: 66.41%] [G loss: 2.015883]\n",
      "epoch:9 step:9163 [D loss: 0.629724, acc: 67.19%] [G loss: 1.963942]\n",
      "epoch:9 step:9164 [D loss: 0.602444, acc: 67.97%] [G loss: 2.029545]\n",
      "epoch:9 step:9165 [D loss: 0.610796, acc: 67.97%] [G loss: 2.143296]\n",
      "epoch:9 step:9166 [D loss: 0.539183, acc: 69.53%] [G loss: 2.208575]\n",
      "epoch:9 step:9167 [D loss: 0.644814, acc: 67.19%] [G loss: 1.948330]\n",
      "epoch:9 step:9168 [D loss: 0.685938, acc: 59.38%] [G loss: 2.118832]\n",
      "epoch:9 step:9169 [D loss: 0.555713, acc: 79.69%] [G loss: 2.275624]\n",
      "epoch:9 step:9170 [D loss: 0.634320, acc: 64.84%] [G loss: 2.018745]\n",
      "epoch:9 step:9171 [D loss: 0.663355, acc: 65.62%] [G loss: 2.180315]\n",
      "epoch:9 step:9172 [D loss: 0.660039, acc: 60.16%] [G loss: 2.094689]\n",
      "epoch:9 step:9173 [D loss: 0.628931, acc: 61.72%] [G loss: 2.071688]\n",
      "epoch:9 step:9174 [D loss: 0.574151, acc: 67.19%] [G loss: 1.973873]\n",
      "epoch:9 step:9175 [D loss: 0.704233, acc: 60.16%] [G loss: 2.112373]\n",
      "epoch:9 step:9176 [D loss: 0.682110, acc: 58.59%] [G loss: 1.964436]\n",
      "epoch:9 step:9177 [D loss: 0.630926, acc: 65.62%] [G loss: 2.041282]\n",
      "epoch:9 step:9178 [D loss: 0.686935, acc: 57.03%] [G loss: 2.021369]\n",
      "epoch:9 step:9179 [D loss: 0.617732, acc: 65.62%] [G loss: 2.087137]\n",
      "epoch:9 step:9180 [D loss: 0.610627, acc: 70.31%] [G loss: 2.063021]\n",
      "epoch:9 step:9181 [D loss: 0.590417, acc: 73.44%] [G loss: 1.996341]\n",
      "epoch:9 step:9182 [D loss: 0.653583, acc: 66.41%] [G loss: 1.914479]\n",
      "epoch:9 step:9183 [D loss: 0.719976, acc: 55.47%] [G loss: 1.868669]\n",
      "epoch:9 step:9184 [D loss: 0.610799, acc: 64.84%] [G loss: 2.067155]\n",
      "epoch:9 step:9185 [D loss: 0.682711, acc: 56.25%] [G loss: 1.936949]\n",
      "epoch:9 step:9186 [D loss: 0.605452, acc: 65.62%] [G loss: 1.986542]\n",
      "epoch:9 step:9187 [D loss: 0.555631, acc: 73.44%] [G loss: 2.218113]\n",
      "epoch:9 step:9188 [D loss: 0.628312, acc: 64.84%] [G loss: 2.164817]\n",
      "epoch:9 step:9189 [D loss: 0.598473, acc: 67.19%] [G loss: 2.272202]\n",
      "epoch:9 step:9190 [D loss: 0.583930, acc: 64.84%] [G loss: 1.981633]\n",
      "epoch:9 step:9191 [D loss: 0.635841, acc: 70.31%] [G loss: 2.037163]\n",
      "epoch:9 step:9192 [D loss: 0.675732, acc: 57.81%] [G loss: 2.034782]\n",
      "epoch:9 step:9193 [D loss: 0.667096, acc: 62.50%] [G loss: 2.025803]\n",
      "epoch:9 step:9194 [D loss: 0.678850, acc: 64.06%] [G loss: 1.938928]\n",
      "epoch:9 step:9195 [D loss: 0.608621, acc: 66.41%] [G loss: 1.945860]\n",
      "epoch:9 step:9196 [D loss: 0.592920, acc: 69.53%] [G loss: 2.130561]\n",
      "epoch:9 step:9197 [D loss: 0.634594, acc: 64.06%] [G loss: 2.057713]\n",
      "epoch:9 step:9198 [D loss: 0.725709, acc: 50.78%] [G loss: 1.832746]\n",
      "epoch:9 step:9199 [D loss: 0.676823, acc: 57.81%] [G loss: 1.870703]\n",
      "epoch:9 step:9200 [D loss: 0.652485, acc: 60.94%] [G loss: 2.119574]\n",
      "##############\n",
      "[2.45191137 1.15936625 6.51145308 5.09096676 3.74910934 5.85968895\n",
      " 4.29758039 4.8520179  4.63615522 3.47483842]\n",
      "##########\n",
      "epoch:9 step:9201 [D loss: 0.639243, acc: 62.50%] [G loss: 1.967388]\n",
      "epoch:9 step:9202 [D loss: 0.584174, acc: 64.84%] [G loss: 2.244541]\n",
      "epoch:9 step:9203 [D loss: 0.693701, acc: 59.38%] [G loss: 1.934706]\n",
      "epoch:9 step:9204 [D loss: 0.612920, acc: 67.19%] [G loss: 1.952862]\n",
      "epoch:9 step:9205 [D loss: 0.616036, acc: 62.50%] [G loss: 2.137202]\n",
      "epoch:9 step:9206 [D loss: 0.654887, acc: 60.94%] [G loss: 2.027805]\n",
      "epoch:9 step:9207 [D loss: 0.563886, acc: 74.22%] [G loss: 2.239074]\n",
      "epoch:9 step:9208 [D loss: 0.574359, acc: 67.19%] [G loss: 2.497698]\n",
      "epoch:9 step:9209 [D loss: 0.668902, acc: 58.59%] [G loss: 2.102755]\n",
      "epoch:9 step:9210 [D loss: 0.630044, acc: 69.53%] [G loss: 1.967199]\n",
      "epoch:9 step:9211 [D loss: 0.650601, acc: 62.50%] [G loss: 2.064412]\n",
      "epoch:9 step:9212 [D loss: 0.624279, acc: 64.06%] [G loss: 2.055088]\n",
      "epoch:9 step:9213 [D loss: 0.630721, acc: 67.97%] [G loss: 2.133316]\n",
      "epoch:9 step:9214 [D loss: 0.586091, acc: 75.00%] [G loss: 2.483324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9215 [D loss: 0.603683, acc: 69.53%] [G loss: 2.327964]\n",
      "epoch:9 step:9216 [D loss: 0.643108, acc: 60.94%] [G loss: 2.294675]\n",
      "epoch:9 step:9217 [D loss: 0.639096, acc: 66.41%] [G loss: 1.890047]\n",
      "epoch:9 step:9218 [D loss: 0.598977, acc: 71.09%] [G loss: 2.013632]\n",
      "epoch:9 step:9219 [D loss: 0.593014, acc: 66.41%] [G loss: 2.366487]\n",
      "epoch:9 step:9220 [D loss: 0.607586, acc: 67.19%] [G loss: 2.119753]\n",
      "epoch:9 step:9221 [D loss: 0.605629, acc: 65.62%] [G loss: 2.011726]\n",
      "epoch:9 step:9222 [D loss: 0.611512, acc: 67.97%] [G loss: 2.043231]\n",
      "epoch:9 step:9223 [D loss: 0.609395, acc: 66.41%] [G loss: 2.173720]\n",
      "epoch:9 step:9224 [D loss: 0.617603, acc: 69.53%] [G loss: 2.112875]\n",
      "epoch:9 step:9225 [D loss: 0.587058, acc: 72.66%] [G loss: 2.286517]\n",
      "epoch:9 step:9226 [D loss: 0.612919, acc: 65.62%] [G loss: 2.296773]\n",
      "epoch:9 step:9227 [D loss: 0.709636, acc: 57.81%] [G loss: 2.051801]\n",
      "epoch:9 step:9228 [D loss: 0.613856, acc: 66.41%] [G loss: 2.083226]\n",
      "epoch:9 step:9229 [D loss: 0.602784, acc: 71.09%] [G loss: 2.289604]\n",
      "epoch:9 step:9230 [D loss: 0.632136, acc: 68.75%] [G loss: 1.927556]\n",
      "epoch:9 step:9231 [D loss: 0.615045, acc: 63.28%] [G loss: 2.285438]\n",
      "epoch:9 step:9232 [D loss: 0.607930, acc: 59.38%] [G loss: 2.149345]\n",
      "epoch:9 step:9233 [D loss: 0.718987, acc: 57.81%] [G loss: 2.045833]\n",
      "epoch:9 step:9234 [D loss: 0.702997, acc: 58.59%] [G loss: 1.940517]\n",
      "epoch:9 step:9235 [D loss: 0.635650, acc: 61.72%] [G loss: 2.182355]\n",
      "epoch:9 step:9236 [D loss: 0.646294, acc: 62.50%] [G loss: 1.992004]\n",
      "epoch:9 step:9237 [D loss: 0.603887, acc: 64.84%] [G loss: 2.120997]\n",
      "epoch:9 step:9238 [D loss: 0.523916, acc: 78.12%] [G loss: 2.587179]\n",
      "epoch:9 step:9239 [D loss: 0.612037, acc: 68.75%] [G loss: 2.288912]\n",
      "epoch:9 step:9240 [D loss: 0.576951, acc: 71.09%] [G loss: 2.267250]\n",
      "epoch:9 step:9241 [D loss: 0.608654, acc: 62.50%] [G loss: 2.104193]\n",
      "epoch:9 step:9242 [D loss: 0.654376, acc: 62.50%] [G loss: 2.194573]\n",
      "epoch:9 step:9243 [D loss: 0.597750, acc: 68.75%] [G loss: 2.052642]\n",
      "epoch:9 step:9244 [D loss: 0.583547, acc: 74.22%] [G loss: 2.081763]\n",
      "epoch:9 step:9245 [D loss: 0.702219, acc: 60.16%] [G loss: 2.089774]\n",
      "epoch:9 step:9246 [D loss: 0.600828, acc: 67.97%] [G loss: 2.103725]\n",
      "epoch:9 step:9247 [D loss: 0.592326, acc: 68.75%] [G loss: 2.234805]\n",
      "epoch:9 step:9248 [D loss: 0.600129, acc: 66.41%] [G loss: 2.531110]\n",
      "epoch:9 step:9249 [D loss: 0.598733, acc: 69.53%] [G loss: 2.352963]\n",
      "epoch:9 step:9250 [D loss: 0.645219, acc: 62.50%] [G loss: 1.993757]\n",
      "epoch:9 step:9251 [D loss: 0.602844, acc: 70.31%] [G loss: 2.062895]\n",
      "epoch:9 step:9252 [D loss: 0.676788, acc: 60.16%] [G loss: 2.082570]\n",
      "epoch:9 step:9253 [D loss: 0.634533, acc: 59.38%] [G loss: 2.063796]\n",
      "epoch:9 step:9254 [D loss: 0.574300, acc: 71.88%] [G loss: 2.139937]\n",
      "epoch:9 step:9255 [D loss: 0.648387, acc: 62.50%] [G loss: 2.226832]\n",
      "epoch:9 step:9256 [D loss: 0.596620, acc: 64.84%] [G loss: 2.117190]\n",
      "epoch:9 step:9257 [D loss: 0.592556, acc: 71.09%] [G loss: 2.204257]\n",
      "epoch:9 step:9258 [D loss: 0.618908, acc: 65.62%] [G loss: 2.040067]\n",
      "epoch:9 step:9259 [D loss: 0.665170, acc: 57.81%] [G loss: 2.488057]\n",
      "epoch:9 step:9260 [D loss: 0.655315, acc: 65.62%] [G loss: 1.960434]\n",
      "epoch:9 step:9261 [D loss: 0.657499, acc: 65.62%] [G loss: 2.082466]\n",
      "epoch:9 step:9262 [D loss: 0.687850, acc: 56.25%] [G loss: 1.942151]\n",
      "epoch:9 step:9263 [D loss: 0.591828, acc: 67.19%] [G loss: 2.169298]\n",
      "epoch:9 step:9264 [D loss: 0.631893, acc: 64.06%] [G loss: 2.122094]\n",
      "epoch:9 step:9265 [D loss: 0.602461, acc: 65.62%] [G loss: 1.972664]\n",
      "epoch:9 step:9266 [D loss: 0.653207, acc: 65.62%] [G loss: 2.278105]\n",
      "epoch:9 step:9267 [D loss: 0.577900, acc: 66.41%] [G loss: 2.208352]\n",
      "epoch:9 step:9268 [D loss: 0.601902, acc: 70.31%] [G loss: 2.076662]\n",
      "epoch:9 step:9269 [D loss: 0.636027, acc: 64.84%] [G loss: 1.980309]\n",
      "epoch:9 step:9270 [D loss: 0.597356, acc: 74.22%] [G loss: 2.166003]\n",
      "epoch:9 step:9271 [D loss: 0.640831, acc: 65.62%] [G loss: 2.239648]\n",
      "epoch:9 step:9272 [D loss: 0.579833, acc: 67.19%] [G loss: 2.157485]\n",
      "epoch:9 step:9273 [D loss: 0.599843, acc: 68.75%] [G loss: 2.127284]\n",
      "epoch:9 step:9274 [D loss: 0.578087, acc: 69.53%] [G loss: 2.294166]\n",
      "epoch:9 step:9275 [D loss: 0.636779, acc: 65.62%] [G loss: 2.122874]\n",
      "epoch:9 step:9276 [D loss: 0.698432, acc: 60.94%] [G loss: 2.211778]\n",
      "epoch:9 step:9277 [D loss: 0.625283, acc: 61.72%] [G loss: 2.083728]\n",
      "epoch:9 step:9278 [D loss: 0.583596, acc: 69.53%] [G loss: 2.288062]\n",
      "epoch:9 step:9279 [D loss: 0.603302, acc: 65.62%] [G loss: 2.119171]\n",
      "epoch:9 step:9280 [D loss: 0.636622, acc: 63.28%] [G loss: 2.110325]\n",
      "epoch:9 step:9281 [D loss: 0.642868, acc: 60.94%] [G loss: 2.153385]\n",
      "epoch:9 step:9282 [D loss: 0.662979, acc: 63.28%] [G loss: 2.355216]\n",
      "epoch:9 step:9283 [D loss: 0.690655, acc: 57.03%] [G loss: 2.057048]\n",
      "epoch:9 step:9284 [D loss: 0.623768, acc: 67.97%] [G loss: 2.067350]\n",
      "epoch:9 step:9285 [D loss: 0.640396, acc: 60.16%] [G loss: 2.210064]\n",
      "epoch:9 step:9286 [D loss: 0.670554, acc: 57.81%] [G loss: 2.069820]\n",
      "epoch:9 step:9287 [D loss: 0.618257, acc: 67.97%] [G loss: 2.040721]\n",
      "epoch:9 step:9288 [D loss: 0.672116, acc: 58.59%] [G loss: 1.893392]\n",
      "epoch:9 step:9289 [D loss: 0.630196, acc: 59.38%] [G loss: 1.917158]\n",
      "epoch:9 step:9290 [D loss: 0.607692, acc: 64.06%] [G loss: 2.268641]\n",
      "epoch:9 step:9291 [D loss: 0.746700, acc: 49.22%] [G loss: 1.889088]\n",
      "epoch:9 step:9292 [D loss: 0.673756, acc: 52.34%] [G loss: 1.844726]\n",
      "epoch:9 step:9293 [D loss: 0.645907, acc: 59.38%] [G loss: 2.013768]\n",
      "epoch:9 step:9294 [D loss: 0.669327, acc: 61.72%] [G loss: 2.011688]\n",
      "epoch:9 step:9295 [D loss: 0.678836, acc: 59.38%] [G loss: 1.869835]\n",
      "epoch:9 step:9296 [D loss: 0.623702, acc: 61.72%] [G loss: 2.044062]\n",
      "epoch:9 step:9297 [D loss: 0.606756, acc: 66.41%] [G loss: 2.059253]\n",
      "epoch:9 step:9298 [D loss: 0.715663, acc: 60.16%] [G loss: 2.016376]\n",
      "epoch:9 step:9299 [D loss: 0.631587, acc: 63.28%] [G loss: 1.925349]\n",
      "epoch:9 step:9300 [D loss: 0.644452, acc: 64.06%] [G loss: 2.054502]\n",
      "epoch:9 step:9301 [D loss: 0.667699, acc: 65.62%] [G loss: 2.092185]\n",
      "epoch:9 step:9302 [D loss: 0.671727, acc: 59.38%] [G loss: 1.943718]\n",
      "epoch:9 step:9303 [D loss: 0.612235, acc: 67.97%] [G loss: 2.198569]\n",
      "epoch:9 step:9304 [D loss: 0.620374, acc: 65.62%] [G loss: 1.965364]\n",
      "epoch:9 step:9305 [D loss: 0.642522, acc: 60.94%] [G loss: 2.073500]\n",
      "epoch:9 step:9306 [D loss: 0.679561, acc: 58.59%] [G loss: 1.905344]\n",
      "epoch:9 step:9307 [D loss: 0.681430, acc: 58.59%] [G loss: 1.914103]\n",
      "epoch:9 step:9308 [D loss: 0.616863, acc: 64.06%] [G loss: 2.002668]\n",
      "epoch:9 step:9309 [D loss: 0.611075, acc: 65.62%] [G loss: 2.206645]\n",
      "epoch:9 step:9310 [D loss: 0.661653, acc: 59.38%] [G loss: 2.020088]\n",
      "epoch:9 step:9311 [D loss: 0.656128, acc: 59.38%] [G loss: 2.027023]\n",
      "epoch:9 step:9312 [D loss: 0.642471, acc: 60.16%] [G loss: 1.962559]\n",
      "epoch:9 step:9313 [D loss: 0.620508, acc: 64.06%] [G loss: 1.913848]\n",
      "epoch:9 step:9314 [D loss: 0.688396, acc: 54.69%] [G loss: 2.017227]\n",
      "epoch:9 step:9315 [D loss: 0.644339, acc: 58.59%] [G loss: 2.114527]\n",
      "epoch:9 step:9316 [D loss: 0.594036, acc: 67.97%] [G loss: 1.891537]\n",
      "epoch:9 step:9317 [D loss: 0.547799, acc: 73.44%] [G loss: 2.363315]\n",
      "epoch:9 step:9318 [D loss: 0.622815, acc: 59.38%] [G loss: 2.188196]\n",
      "epoch:9 step:9319 [D loss: 0.591547, acc: 65.62%] [G loss: 2.312456]\n",
      "epoch:9 step:9320 [D loss: 0.646093, acc: 57.81%] [G loss: 2.122125]\n",
      "epoch:9 step:9321 [D loss: 0.634045, acc: 58.59%] [G loss: 2.116535]\n",
      "epoch:9 step:9322 [D loss: 0.608851, acc: 66.41%] [G loss: 2.198006]\n",
      "epoch:9 step:9323 [D loss: 0.682135, acc: 57.03%] [G loss: 1.963813]\n",
      "epoch:9 step:9324 [D loss: 0.623906, acc: 67.97%] [G loss: 2.138257]\n",
      "epoch:9 step:9325 [D loss: 0.658155, acc: 60.94%] [G loss: 1.857340]\n",
      "epoch:9 step:9326 [D loss: 0.640860, acc: 64.84%] [G loss: 1.966944]\n",
      "epoch:9 step:9327 [D loss: 0.661504, acc: 63.28%] [G loss: 2.095586]\n",
      "epoch:9 step:9328 [D loss: 0.587015, acc: 71.88%] [G loss: 2.029970]\n",
      "epoch:9 step:9329 [D loss: 0.646387, acc: 63.28%] [G loss: 2.004806]\n",
      "epoch:9 step:9330 [D loss: 0.582230, acc: 72.66%] [G loss: 1.987294]\n",
      "epoch:9 step:9331 [D loss: 0.632218, acc: 64.06%] [G loss: 2.224236]\n",
      "epoch:9 step:9332 [D loss: 0.662763, acc: 62.50%] [G loss: 2.314759]\n",
      "epoch:9 step:9333 [D loss: 0.605721, acc: 65.62%] [G loss: 2.105238]\n",
      "epoch:9 step:9334 [D loss: 0.701299, acc: 54.69%] [G loss: 2.040961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9335 [D loss: 0.649678, acc: 61.72%] [G loss: 1.877399]\n",
      "epoch:9 step:9336 [D loss: 0.587277, acc: 73.44%] [G loss: 2.142307]\n",
      "epoch:9 step:9337 [D loss: 0.641137, acc: 60.94%] [G loss: 2.338934]\n",
      "epoch:9 step:9338 [D loss: 0.594246, acc: 69.53%] [G loss: 2.122522]\n",
      "epoch:9 step:9339 [D loss: 0.601402, acc: 67.97%] [G loss: 2.324695]\n",
      "epoch:9 step:9340 [D loss: 0.637151, acc: 63.28%] [G loss: 2.257048]\n",
      "epoch:9 step:9341 [D loss: 0.613222, acc: 67.97%] [G loss: 2.159533]\n",
      "epoch:9 step:9342 [D loss: 0.568905, acc: 73.44%] [G loss: 2.153206]\n",
      "epoch:9 step:9343 [D loss: 0.596887, acc: 70.31%] [G loss: 2.327065]\n",
      "epoch:9 step:9344 [D loss: 0.537213, acc: 80.47%] [G loss: 2.393095]\n",
      "epoch:9 step:9345 [D loss: 0.545640, acc: 75.00%] [G loss: 2.350154]\n",
      "epoch:9 step:9346 [D loss: 0.620588, acc: 64.84%] [G loss: 2.326159]\n",
      "epoch:9 step:9347 [D loss: 0.651416, acc: 64.06%] [G loss: 2.099623]\n",
      "epoch:9 step:9348 [D loss: 0.641081, acc: 62.50%] [G loss: 2.296574]\n",
      "epoch:9 step:9349 [D loss: 0.600011, acc: 71.09%] [G loss: 2.239703]\n",
      "epoch:9 step:9350 [D loss: 0.661220, acc: 57.81%] [G loss: 2.272987]\n",
      "epoch:9 step:9351 [D loss: 0.576283, acc: 71.88%] [G loss: 2.175118]\n",
      "epoch:9 step:9352 [D loss: 0.601507, acc: 65.62%] [G loss: 2.540577]\n",
      "epoch:9 step:9353 [D loss: 0.728990, acc: 52.34%] [G loss: 2.014215]\n",
      "epoch:9 step:9354 [D loss: 0.606826, acc: 67.97%] [G loss: 2.113518]\n",
      "epoch:9 step:9355 [D loss: 0.607938, acc: 60.94%] [G loss: 2.130724]\n",
      "epoch:9 step:9356 [D loss: 0.594347, acc: 68.75%] [G loss: 2.273345]\n",
      "epoch:9 step:9357 [D loss: 0.565398, acc: 74.22%] [G loss: 2.449830]\n",
      "epoch:9 step:9358 [D loss: 0.550058, acc: 72.66%] [G loss: 2.591487]\n",
      "epoch:9 step:9359 [D loss: 0.571829, acc: 72.66%] [G loss: 2.607489]\n",
      "epoch:9 step:9360 [D loss: 0.694481, acc: 65.62%] [G loss: 2.399975]\n",
      "epoch:9 step:9361 [D loss: 0.826017, acc: 51.56%] [G loss: 1.927634]\n",
      "epoch:9 step:9362 [D loss: 0.747060, acc: 48.44%] [G loss: 2.057270]\n",
      "epoch:9 step:9363 [D loss: 0.577091, acc: 68.75%] [G loss: 2.102657]\n",
      "epoch:9 step:9364 [D loss: 0.609987, acc: 67.19%] [G loss: 2.148860]\n",
      "epoch:9 step:9365 [D loss: 0.618743, acc: 67.97%] [G loss: 2.038870]\n",
      "epoch:9 step:9366 [D loss: 0.661526, acc: 61.72%] [G loss: 2.112651]\n",
      "epoch:9 step:9367 [D loss: 0.662794, acc: 59.38%] [G loss: 2.076084]\n",
      "epoch:9 step:9368 [D loss: 0.616534, acc: 67.97%] [G loss: 2.170232]\n",
      "epoch:9 step:9369 [D loss: 0.607385, acc: 63.28%] [G loss: 2.234353]\n",
      "epoch:9 step:9370 [D loss: 0.537866, acc: 76.56%] [G loss: 2.897970]\n",
      "epoch:10 step:9371 [D loss: 0.578783, acc: 71.09%] [G loss: 2.145927]\n",
      "epoch:10 step:9372 [D loss: 0.585223, acc: 66.41%] [G loss: 2.049227]\n",
      "epoch:10 step:9373 [D loss: 0.598425, acc: 66.41%] [G loss: 2.241123]\n",
      "epoch:10 step:9374 [D loss: 0.579031, acc: 66.41%] [G loss: 2.444272]\n",
      "epoch:10 step:9375 [D loss: 0.677148, acc: 63.28%] [G loss: 2.127537]\n",
      "epoch:10 step:9376 [D loss: 0.561814, acc: 68.75%] [G loss: 2.269961]\n",
      "epoch:10 step:9377 [D loss: 0.598703, acc: 63.28%] [G loss: 2.217318]\n",
      "epoch:10 step:9378 [D loss: 0.646498, acc: 67.19%] [G loss: 2.118439]\n",
      "epoch:10 step:9379 [D loss: 0.580588, acc: 67.97%] [G loss: 2.406151]\n",
      "epoch:10 step:9380 [D loss: 0.643238, acc: 66.41%] [G loss: 2.181668]\n",
      "epoch:10 step:9381 [D loss: 0.582744, acc: 67.97%] [G loss: 2.122944]\n",
      "epoch:10 step:9382 [D loss: 0.643126, acc: 62.50%] [G loss: 2.059436]\n",
      "epoch:10 step:9383 [D loss: 0.672239, acc: 56.25%] [G loss: 1.967154]\n",
      "epoch:10 step:9384 [D loss: 0.617428, acc: 66.41%] [G loss: 1.927906]\n",
      "epoch:10 step:9385 [D loss: 0.546287, acc: 71.09%] [G loss: 2.395637]\n",
      "epoch:10 step:9386 [D loss: 0.624446, acc: 62.50%] [G loss: 2.347836]\n",
      "epoch:10 step:9387 [D loss: 0.680370, acc: 59.38%] [G loss: 2.135629]\n",
      "epoch:10 step:9388 [D loss: 0.676006, acc: 58.59%] [G loss: 2.025482]\n",
      "epoch:10 step:9389 [D loss: 0.650151, acc: 66.41%] [G loss: 1.929055]\n",
      "epoch:10 step:9390 [D loss: 0.667347, acc: 60.94%] [G loss: 1.814958]\n",
      "epoch:10 step:9391 [D loss: 0.655905, acc: 62.50%] [G loss: 2.010951]\n",
      "epoch:10 step:9392 [D loss: 0.575672, acc: 70.31%] [G loss: 1.883242]\n",
      "epoch:10 step:9393 [D loss: 0.612610, acc: 66.41%] [G loss: 2.272621]\n",
      "epoch:10 step:9394 [D loss: 0.566921, acc: 74.22%] [G loss: 2.252287]\n",
      "epoch:10 step:9395 [D loss: 0.566456, acc: 71.09%] [G loss: 2.132050]\n",
      "epoch:10 step:9396 [D loss: 0.553235, acc: 74.22%] [G loss: 2.128971]\n",
      "epoch:10 step:9397 [D loss: 0.651614, acc: 62.50%] [G loss: 2.096742]\n",
      "epoch:10 step:9398 [D loss: 0.613791, acc: 71.09%] [G loss: 2.155955]\n",
      "epoch:10 step:9399 [D loss: 0.573612, acc: 70.31%] [G loss: 2.074852]\n",
      "epoch:10 step:9400 [D loss: 0.648677, acc: 67.19%] [G loss: 2.172395]\n",
      "##############\n",
      "[2.39759892 1.48594389 6.16426759 4.84530052 3.77803889 5.66409439\n",
      " 4.46826376 4.81267449 4.71523357 3.51010399]\n",
      "##########\n",
      "epoch:10 step:9401 [D loss: 0.645631, acc: 58.59%] [G loss: 2.076208]\n",
      "epoch:10 step:9402 [D loss: 0.607602, acc: 66.41%] [G loss: 1.920613]\n",
      "epoch:10 step:9403 [D loss: 0.608429, acc: 67.19%] [G loss: 1.967780]\n",
      "epoch:10 step:9404 [D loss: 0.610946, acc: 67.97%] [G loss: 2.029873]\n",
      "epoch:10 step:9405 [D loss: 0.608620, acc: 67.97%] [G loss: 2.081805]\n",
      "epoch:10 step:9406 [D loss: 0.593196, acc: 69.53%] [G loss: 2.349917]\n",
      "epoch:10 step:9407 [D loss: 0.622135, acc: 64.84%] [G loss: 2.313374]\n",
      "epoch:10 step:9408 [D loss: 0.623914, acc: 67.97%] [G loss: 2.279840]\n",
      "epoch:10 step:9409 [D loss: 0.602070, acc: 67.97%] [G loss: 2.160684]\n",
      "epoch:10 step:9410 [D loss: 0.579041, acc: 68.75%] [G loss: 2.171891]\n",
      "epoch:10 step:9411 [D loss: 0.666347, acc: 61.72%] [G loss: 1.852481]\n",
      "epoch:10 step:9412 [D loss: 0.614474, acc: 71.09%] [G loss: 2.135757]\n",
      "epoch:10 step:9413 [D loss: 0.608626, acc: 65.62%] [G loss: 2.150531]\n",
      "epoch:10 step:9414 [D loss: 0.671235, acc: 63.28%] [G loss: 2.164271]\n",
      "epoch:10 step:9415 [D loss: 0.600207, acc: 68.75%] [G loss: 2.363807]\n",
      "epoch:10 step:9416 [D loss: 0.601234, acc: 64.06%] [G loss: 2.156082]\n",
      "epoch:10 step:9417 [D loss: 0.625609, acc: 66.41%] [G loss: 2.093256]\n",
      "epoch:10 step:9418 [D loss: 0.584097, acc: 73.44%] [G loss: 2.151783]\n",
      "epoch:10 step:9419 [D loss: 0.651495, acc: 66.41%] [G loss: 2.010731]\n",
      "epoch:10 step:9420 [D loss: 0.651718, acc: 64.84%] [G loss: 2.306993]\n",
      "epoch:10 step:9421 [D loss: 0.647396, acc: 61.72%] [G loss: 2.227134]\n",
      "epoch:10 step:9422 [D loss: 0.623348, acc: 67.19%] [G loss: 2.017830]\n",
      "epoch:10 step:9423 [D loss: 0.615881, acc: 64.06%] [G loss: 2.306458]\n",
      "epoch:10 step:9424 [D loss: 0.589029, acc: 68.75%] [G loss: 2.261117]\n",
      "epoch:10 step:9425 [D loss: 0.618460, acc: 67.19%] [G loss: 2.030099]\n",
      "epoch:10 step:9426 [D loss: 0.560407, acc: 73.44%] [G loss: 2.222372]\n",
      "epoch:10 step:9427 [D loss: 0.609733, acc: 66.41%] [G loss: 2.059679]\n",
      "epoch:10 step:9428 [D loss: 0.583818, acc: 67.19%] [G loss: 2.205199]\n",
      "epoch:10 step:9429 [D loss: 0.629461, acc: 63.28%] [G loss: 2.308996]\n",
      "epoch:10 step:9430 [D loss: 0.633061, acc: 62.50%] [G loss: 2.274135]\n",
      "epoch:10 step:9431 [D loss: 0.700647, acc: 60.16%] [G loss: 2.209391]\n",
      "epoch:10 step:9432 [D loss: 0.647199, acc: 61.72%] [G loss: 2.111839]\n",
      "epoch:10 step:9433 [D loss: 0.610584, acc: 63.28%] [G loss: 2.152443]\n",
      "epoch:10 step:9434 [D loss: 0.652259, acc: 61.72%] [G loss: 2.002388]\n",
      "epoch:10 step:9435 [D loss: 0.665703, acc: 58.59%] [G loss: 2.061768]\n",
      "epoch:10 step:9436 [D loss: 0.597594, acc: 69.53%] [G loss: 1.917048]\n",
      "epoch:10 step:9437 [D loss: 0.611354, acc: 65.62%] [G loss: 1.977586]\n",
      "epoch:10 step:9438 [D loss: 0.646537, acc: 60.16%] [G loss: 2.180706]\n",
      "epoch:10 step:9439 [D loss: 0.596111, acc: 69.53%] [G loss: 2.084944]\n",
      "epoch:10 step:9440 [D loss: 0.638277, acc: 64.84%] [G loss: 2.263392]\n",
      "epoch:10 step:9441 [D loss: 0.639721, acc: 64.06%] [G loss: 2.056569]\n",
      "epoch:10 step:9442 [D loss: 0.607947, acc: 68.75%] [G loss: 2.136926]\n",
      "epoch:10 step:9443 [D loss: 0.600610, acc: 67.97%] [G loss: 2.022005]\n",
      "epoch:10 step:9444 [D loss: 0.588166, acc: 68.75%] [G loss: 2.243121]\n",
      "epoch:10 step:9445 [D loss: 0.635124, acc: 66.41%] [G loss: 2.424740]\n",
      "epoch:10 step:9446 [D loss: 0.602223, acc: 64.84%] [G loss: 2.327573]\n",
      "epoch:10 step:9447 [D loss: 0.605193, acc: 67.19%] [G loss: 2.298646]\n",
      "epoch:10 step:9448 [D loss: 0.633383, acc: 66.41%] [G loss: 1.937263]\n",
      "epoch:10 step:9449 [D loss: 0.624181, acc: 65.62%] [G loss: 2.077303]\n",
      "epoch:10 step:9450 [D loss: 0.687365, acc: 50.00%] [G loss: 1.889340]\n",
      "epoch:10 step:9451 [D loss: 0.641409, acc: 64.06%] [G loss: 1.804809]\n",
      "epoch:10 step:9452 [D loss: 0.632473, acc: 61.72%] [G loss: 1.988497]\n",
      "epoch:10 step:9453 [D loss: 0.613613, acc: 69.53%] [G loss: 2.110588]\n",
      "epoch:10 step:9454 [D loss: 0.659879, acc: 59.38%] [G loss: 2.068964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9455 [D loss: 0.693627, acc: 59.38%] [G loss: 1.895951]\n",
      "epoch:10 step:9456 [D loss: 0.702602, acc: 57.03%] [G loss: 1.908656]\n",
      "epoch:10 step:9457 [D loss: 0.643765, acc: 61.72%] [G loss: 1.821711]\n",
      "epoch:10 step:9458 [D loss: 0.658934, acc: 60.94%] [G loss: 2.126501]\n",
      "epoch:10 step:9459 [D loss: 0.602290, acc: 68.75%] [G loss: 2.001699]\n",
      "epoch:10 step:9460 [D loss: 0.648251, acc: 64.84%] [G loss: 2.175851]\n",
      "epoch:10 step:9461 [D loss: 0.603270, acc: 68.75%] [G loss: 1.986078]\n",
      "epoch:10 step:9462 [D loss: 0.608213, acc: 65.62%] [G loss: 2.223189]\n",
      "epoch:10 step:9463 [D loss: 0.638303, acc: 61.72%] [G loss: 2.181829]\n",
      "epoch:10 step:9464 [D loss: 0.558248, acc: 78.91%] [G loss: 2.063911]\n",
      "epoch:10 step:9465 [D loss: 0.611205, acc: 70.31%] [G loss: 1.933607]\n",
      "epoch:10 step:9466 [D loss: 0.631313, acc: 65.62%] [G loss: 2.049226]\n",
      "epoch:10 step:9467 [D loss: 0.616524, acc: 65.62%] [G loss: 2.192133]\n",
      "epoch:10 step:9468 [D loss: 0.677149, acc: 61.72%] [G loss: 1.886655]\n",
      "epoch:10 step:9469 [D loss: 0.624875, acc: 66.41%] [G loss: 2.113700]\n",
      "epoch:10 step:9470 [D loss: 0.616227, acc: 64.84%] [G loss: 2.097211]\n",
      "epoch:10 step:9471 [D loss: 0.661688, acc: 64.84%] [G loss: 2.012680]\n",
      "epoch:10 step:9472 [D loss: 0.640331, acc: 64.84%] [G loss: 1.910791]\n",
      "epoch:10 step:9473 [D loss: 0.551765, acc: 74.22%] [G loss: 2.141579]\n",
      "epoch:10 step:9474 [D loss: 0.611393, acc: 71.09%] [G loss: 1.987873]\n",
      "epoch:10 step:9475 [D loss: 0.700169, acc: 54.69%] [G loss: 1.843835]\n",
      "epoch:10 step:9476 [D loss: 0.668792, acc: 58.59%] [G loss: 1.997069]\n",
      "epoch:10 step:9477 [D loss: 0.619648, acc: 68.75%] [G loss: 1.991086]\n",
      "epoch:10 step:9478 [D loss: 0.683133, acc: 60.16%] [G loss: 2.076211]\n",
      "epoch:10 step:9479 [D loss: 0.634343, acc: 69.53%] [G loss: 2.132443]\n",
      "epoch:10 step:9480 [D loss: 0.643546, acc: 59.38%] [G loss: 2.167774]\n",
      "epoch:10 step:9481 [D loss: 0.614218, acc: 69.53%] [G loss: 2.208461]\n",
      "epoch:10 step:9482 [D loss: 0.598720, acc: 69.53%] [G loss: 2.206889]\n",
      "epoch:10 step:9483 [D loss: 0.660281, acc: 57.81%] [G loss: 2.112156]\n",
      "epoch:10 step:9484 [D loss: 0.653489, acc: 62.50%] [G loss: 2.083739]\n",
      "epoch:10 step:9485 [D loss: 0.626066, acc: 67.19%] [G loss: 2.241598]\n",
      "epoch:10 step:9486 [D loss: 0.642219, acc: 63.28%] [G loss: 2.170861]\n",
      "epoch:10 step:9487 [D loss: 0.650471, acc: 66.41%] [G loss: 2.308752]\n",
      "epoch:10 step:9488 [D loss: 0.579508, acc: 68.75%] [G loss: 2.173214]\n",
      "epoch:10 step:9489 [D loss: 0.611445, acc: 67.19%] [G loss: 2.300469]\n",
      "epoch:10 step:9490 [D loss: 0.698576, acc: 56.25%] [G loss: 2.201677]\n",
      "epoch:10 step:9491 [D loss: 0.623351, acc: 67.19%] [G loss: 2.198384]\n",
      "epoch:10 step:9492 [D loss: 0.628571, acc: 67.97%] [G loss: 2.175366]\n",
      "epoch:10 step:9493 [D loss: 0.635875, acc: 62.50%] [G loss: 2.323116]\n",
      "epoch:10 step:9494 [D loss: 0.641226, acc: 59.38%] [G loss: 2.045789]\n",
      "epoch:10 step:9495 [D loss: 0.658989, acc: 60.94%] [G loss: 2.138421]\n",
      "epoch:10 step:9496 [D loss: 0.609290, acc: 64.84%] [G loss: 2.067276]\n",
      "epoch:10 step:9497 [D loss: 0.683936, acc: 60.16%] [G loss: 1.996617]\n",
      "epoch:10 step:9498 [D loss: 0.635128, acc: 64.06%] [G loss: 2.192914]\n",
      "epoch:10 step:9499 [D loss: 0.670949, acc: 57.03%] [G loss: 2.111467]\n",
      "epoch:10 step:9500 [D loss: 0.658030, acc: 57.81%] [G loss: 1.999591]\n",
      "epoch:10 step:9501 [D loss: 0.608141, acc: 68.75%] [G loss: 2.141915]\n",
      "epoch:10 step:9502 [D loss: 0.601635, acc: 67.19%] [G loss: 2.211676]\n",
      "epoch:10 step:9503 [D loss: 0.660435, acc: 63.28%] [G loss: 1.952523]\n",
      "epoch:10 step:9504 [D loss: 0.646719, acc: 66.41%] [G loss: 1.935498]\n",
      "epoch:10 step:9505 [D loss: 0.598357, acc: 64.84%] [G loss: 2.201806]\n",
      "epoch:10 step:9506 [D loss: 0.656631, acc: 60.94%] [G loss: 1.851490]\n",
      "epoch:10 step:9507 [D loss: 0.610252, acc: 63.28%] [G loss: 1.959034]\n",
      "epoch:10 step:9508 [D loss: 0.688220, acc: 53.12%] [G loss: 1.968964]\n",
      "epoch:10 step:9509 [D loss: 0.576276, acc: 69.53%] [G loss: 2.220414]\n",
      "epoch:10 step:9510 [D loss: 0.619053, acc: 66.41%] [G loss: 2.057375]\n",
      "epoch:10 step:9511 [D loss: 0.583085, acc: 71.09%] [G loss: 2.259180]\n",
      "epoch:10 step:9512 [D loss: 0.615869, acc: 65.62%] [G loss: 1.954024]\n",
      "epoch:10 step:9513 [D loss: 0.667526, acc: 64.84%] [G loss: 2.121518]\n",
      "epoch:10 step:9514 [D loss: 0.699886, acc: 59.38%] [G loss: 2.379111]\n",
      "epoch:10 step:9515 [D loss: 0.651589, acc: 64.84%] [G loss: 2.233596]\n",
      "epoch:10 step:9516 [D loss: 0.673708, acc: 64.84%] [G loss: 2.102463]\n",
      "epoch:10 step:9517 [D loss: 0.667841, acc: 58.59%] [G loss: 2.364906]\n",
      "epoch:10 step:9518 [D loss: 0.644356, acc: 63.28%] [G loss: 2.102199]\n",
      "epoch:10 step:9519 [D loss: 0.634766, acc: 65.62%] [G loss: 2.212694]\n",
      "epoch:10 step:9520 [D loss: 0.624176, acc: 67.97%] [G loss: 2.119837]\n",
      "epoch:10 step:9521 [D loss: 0.648167, acc: 64.84%] [G loss: 2.227630]\n",
      "epoch:10 step:9522 [D loss: 0.594910, acc: 70.31%] [G loss: 2.183515]\n",
      "epoch:10 step:9523 [D loss: 0.666231, acc: 58.59%] [G loss: 1.880878]\n",
      "epoch:10 step:9524 [D loss: 0.611938, acc: 66.41%] [G loss: 2.258875]\n",
      "epoch:10 step:9525 [D loss: 0.615776, acc: 62.50%] [G loss: 2.129611]\n",
      "epoch:10 step:9526 [D loss: 0.610605, acc: 65.62%] [G loss: 2.103064]\n",
      "epoch:10 step:9527 [D loss: 0.606368, acc: 70.31%] [G loss: 2.048215]\n",
      "epoch:10 step:9528 [D loss: 0.581224, acc: 75.78%] [G loss: 1.957254]\n",
      "epoch:10 step:9529 [D loss: 0.613628, acc: 62.50%] [G loss: 2.272952]\n",
      "epoch:10 step:9530 [D loss: 0.635277, acc: 63.28%] [G loss: 2.044310]\n",
      "epoch:10 step:9531 [D loss: 0.669370, acc: 60.94%] [G loss: 2.181769]\n",
      "epoch:10 step:9532 [D loss: 0.664158, acc: 63.28%] [G loss: 2.125786]\n",
      "epoch:10 step:9533 [D loss: 0.683824, acc: 58.59%] [G loss: 1.998429]\n",
      "epoch:10 step:9534 [D loss: 0.650737, acc: 60.16%] [G loss: 1.900314]\n",
      "epoch:10 step:9535 [D loss: 0.660388, acc: 62.50%] [G loss: 2.031321]\n",
      "epoch:10 step:9536 [D loss: 0.616454, acc: 70.31%] [G loss: 1.972119]\n",
      "epoch:10 step:9537 [D loss: 0.622678, acc: 70.31%] [G loss: 2.077162]\n",
      "epoch:10 step:9538 [D loss: 0.593929, acc: 66.41%] [G loss: 2.015167]\n",
      "epoch:10 step:9539 [D loss: 0.648404, acc: 64.84%] [G loss: 2.009559]\n",
      "epoch:10 step:9540 [D loss: 0.630359, acc: 63.28%] [G loss: 2.024392]\n",
      "epoch:10 step:9541 [D loss: 0.569650, acc: 72.66%] [G loss: 2.066380]\n",
      "epoch:10 step:9542 [D loss: 0.622370, acc: 67.97%] [G loss: 2.003990]\n",
      "epoch:10 step:9543 [D loss: 0.649677, acc: 60.16%] [G loss: 2.174816]\n",
      "epoch:10 step:9544 [D loss: 0.675401, acc: 60.16%] [G loss: 2.035551]\n",
      "epoch:10 step:9545 [D loss: 0.618856, acc: 64.84%] [G loss: 2.162995]\n",
      "epoch:10 step:9546 [D loss: 0.683781, acc: 60.16%] [G loss: 1.948047]\n",
      "epoch:10 step:9547 [D loss: 0.685803, acc: 57.03%] [G loss: 1.909338]\n",
      "epoch:10 step:9548 [D loss: 0.598870, acc: 62.50%] [G loss: 1.968720]\n",
      "epoch:10 step:9549 [D loss: 0.635532, acc: 64.06%] [G loss: 1.941804]\n",
      "epoch:10 step:9550 [D loss: 0.613715, acc: 65.62%] [G loss: 2.000606]\n",
      "epoch:10 step:9551 [D loss: 0.655308, acc: 63.28%] [G loss: 1.847313]\n",
      "epoch:10 step:9552 [D loss: 0.636375, acc: 66.41%] [G loss: 1.888297]\n",
      "epoch:10 step:9553 [D loss: 0.718977, acc: 51.56%] [G loss: 2.002308]\n",
      "epoch:10 step:9554 [D loss: 0.676785, acc: 60.94%] [G loss: 1.986077]\n",
      "epoch:10 step:9555 [D loss: 0.645074, acc: 61.72%] [G loss: 2.037867]\n",
      "epoch:10 step:9556 [D loss: 0.602907, acc: 67.19%] [G loss: 2.150205]\n",
      "epoch:10 step:9557 [D loss: 0.620021, acc: 62.50%] [G loss: 1.958699]\n",
      "epoch:10 step:9558 [D loss: 0.654925, acc: 61.72%] [G loss: 1.997639]\n",
      "epoch:10 step:9559 [D loss: 0.665981, acc: 59.38%] [G loss: 1.957942]\n",
      "epoch:10 step:9560 [D loss: 0.579168, acc: 67.97%] [G loss: 2.064176]\n",
      "epoch:10 step:9561 [D loss: 0.599712, acc: 69.53%] [G loss: 2.068762]\n",
      "epoch:10 step:9562 [D loss: 0.682100, acc: 60.94%] [G loss: 2.250125]\n",
      "epoch:10 step:9563 [D loss: 0.698927, acc: 58.59%] [G loss: 2.092392]\n",
      "epoch:10 step:9564 [D loss: 0.563756, acc: 71.88%] [G loss: 2.173771]\n",
      "epoch:10 step:9565 [D loss: 0.629771, acc: 64.84%] [G loss: 1.978249]\n",
      "epoch:10 step:9566 [D loss: 0.651522, acc: 62.50%] [G loss: 2.086333]\n",
      "epoch:10 step:9567 [D loss: 0.625127, acc: 63.28%] [G loss: 2.157285]\n",
      "epoch:10 step:9568 [D loss: 0.608890, acc: 64.06%] [G loss: 2.107857]\n",
      "epoch:10 step:9569 [D loss: 0.612424, acc: 60.16%] [G loss: 2.185484]\n",
      "epoch:10 step:9570 [D loss: 0.659595, acc: 63.28%] [G loss: 1.919015]\n",
      "epoch:10 step:9571 [D loss: 0.651316, acc: 58.59%] [G loss: 2.037317]\n",
      "epoch:10 step:9572 [D loss: 0.628693, acc: 60.16%] [G loss: 1.992306]\n",
      "epoch:10 step:9573 [D loss: 0.646097, acc: 61.72%] [G loss: 1.929212]\n",
      "epoch:10 step:9574 [D loss: 0.660667, acc: 57.81%] [G loss: 2.152398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9575 [D loss: 0.638539, acc: 64.84%] [G loss: 1.924293]\n",
      "epoch:10 step:9576 [D loss: 0.591962, acc: 70.31%] [G loss: 2.185365]\n",
      "epoch:10 step:9577 [D loss: 0.593022, acc: 68.75%] [G loss: 2.265947]\n",
      "epoch:10 step:9578 [D loss: 0.521002, acc: 77.34%] [G loss: 2.329745]\n",
      "epoch:10 step:9579 [D loss: 0.581515, acc: 67.19%] [G loss: 2.320456]\n",
      "epoch:10 step:9580 [D loss: 0.588848, acc: 70.31%] [G loss: 2.058467]\n",
      "epoch:10 step:9581 [D loss: 0.698540, acc: 51.56%] [G loss: 2.053858]\n",
      "epoch:10 step:9582 [D loss: 0.672123, acc: 53.91%] [G loss: 2.049392]\n",
      "epoch:10 step:9583 [D loss: 0.654970, acc: 60.16%] [G loss: 1.908231]\n",
      "epoch:10 step:9584 [D loss: 0.724536, acc: 53.12%] [G loss: 2.015000]\n",
      "epoch:10 step:9585 [D loss: 0.639080, acc: 62.50%] [G loss: 1.967612]\n",
      "epoch:10 step:9586 [D loss: 0.608236, acc: 64.84%] [G loss: 2.216966]\n",
      "epoch:10 step:9587 [D loss: 0.584809, acc: 67.97%] [G loss: 2.118050]\n",
      "epoch:10 step:9588 [D loss: 0.532685, acc: 77.34%] [G loss: 2.546585]\n",
      "epoch:10 step:9589 [D loss: 0.590278, acc: 71.09%] [G loss: 2.415647]\n",
      "epoch:10 step:9590 [D loss: 0.704245, acc: 59.38%] [G loss: 2.117406]\n",
      "epoch:10 step:9591 [D loss: 0.607549, acc: 62.50%] [G loss: 2.278880]\n",
      "epoch:10 step:9592 [D loss: 0.663263, acc: 64.06%] [G loss: 2.279890]\n",
      "epoch:10 step:9593 [D loss: 0.588872, acc: 66.41%] [G loss: 1.978376]\n",
      "epoch:10 step:9594 [D loss: 0.637202, acc: 64.84%] [G loss: 2.000525]\n",
      "epoch:10 step:9595 [D loss: 0.610054, acc: 68.75%] [G loss: 2.145742]\n",
      "epoch:10 step:9596 [D loss: 0.641761, acc: 64.84%] [G loss: 2.034716]\n",
      "epoch:10 step:9597 [D loss: 0.696949, acc: 55.47%] [G loss: 1.881489]\n",
      "epoch:10 step:9598 [D loss: 0.663147, acc: 61.72%] [G loss: 1.747452]\n",
      "epoch:10 step:9599 [D loss: 0.629182, acc: 63.28%] [G loss: 2.089107]\n",
      "epoch:10 step:9600 [D loss: 0.594151, acc: 68.75%] [G loss: 2.249578]\n",
      "##############\n",
      "[2.36388313 1.29615811 6.45286456 5.10037054 3.97976603 5.98975952\n",
      " 4.41363803 4.75680366 4.8881713  3.5289822 ]\n",
      "##########\n",
      "epoch:10 step:9601 [D loss: 0.553964, acc: 70.31%] [G loss: 2.409855]\n",
      "epoch:10 step:9602 [D loss: 0.581519, acc: 69.53%] [G loss: 2.447418]\n",
      "epoch:10 step:9603 [D loss: 0.708828, acc: 57.81%] [G loss: 2.016435]\n",
      "epoch:10 step:9604 [D loss: 0.627261, acc: 65.62%] [G loss: 2.052998]\n",
      "epoch:10 step:9605 [D loss: 0.642994, acc: 58.59%] [G loss: 1.923648]\n",
      "epoch:10 step:9606 [D loss: 0.619723, acc: 70.31%] [G loss: 2.086216]\n",
      "epoch:10 step:9607 [D loss: 0.632591, acc: 67.97%] [G loss: 2.141881]\n",
      "epoch:10 step:9608 [D loss: 0.664346, acc: 63.28%] [G loss: 2.035405]\n",
      "epoch:10 step:9609 [D loss: 0.583108, acc: 69.53%] [G loss: 2.201397]\n",
      "epoch:10 step:9610 [D loss: 0.605823, acc: 67.19%] [G loss: 2.150823]\n",
      "epoch:10 step:9611 [D loss: 0.663053, acc: 58.59%] [G loss: 2.109549]\n",
      "epoch:10 step:9612 [D loss: 0.614948, acc: 65.62%] [G loss: 2.210192]\n",
      "epoch:10 step:9613 [D loss: 0.593501, acc: 65.62%] [G loss: 2.128076]\n",
      "epoch:10 step:9614 [D loss: 0.730796, acc: 53.91%] [G loss: 2.003350]\n",
      "epoch:10 step:9615 [D loss: 0.596961, acc: 67.19%] [G loss: 2.089814]\n",
      "epoch:10 step:9616 [D loss: 0.579266, acc: 73.44%] [G loss: 2.204504]\n",
      "epoch:10 step:9617 [D loss: 0.623318, acc: 60.16%] [G loss: 2.107186]\n",
      "epoch:10 step:9618 [D loss: 0.655363, acc: 64.06%] [G loss: 2.039699]\n",
      "epoch:10 step:9619 [D loss: 0.635265, acc: 64.06%] [G loss: 2.095712]\n",
      "epoch:10 step:9620 [D loss: 0.663534, acc: 58.59%] [G loss: 1.918218]\n",
      "epoch:10 step:9621 [D loss: 0.680963, acc: 53.91%] [G loss: 2.004683]\n",
      "epoch:10 step:9622 [D loss: 0.689707, acc: 60.94%] [G loss: 1.874696]\n",
      "epoch:10 step:9623 [D loss: 0.608452, acc: 64.06%] [G loss: 2.121295]\n",
      "epoch:10 step:9624 [D loss: 0.678982, acc: 59.38%] [G loss: 1.918328]\n",
      "epoch:10 step:9625 [D loss: 0.648341, acc: 56.25%] [G loss: 1.930571]\n",
      "epoch:10 step:9626 [D loss: 0.647105, acc: 66.41%] [G loss: 2.075542]\n",
      "epoch:10 step:9627 [D loss: 0.683068, acc: 53.91%] [G loss: 1.856796]\n",
      "epoch:10 step:9628 [D loss: 0.658763, acc: 64.84%] [G loss: 1.909205]\n",
      "epoch:10 step:9629 [D loss: 0.655693, acc: 64.06%] [G loss: 2.028820]\n",
      "epoch:10 step:9630 [D loss: 0.606038, acc: 68.75%] [G loss: 2.045707]\n",
      "epoch:10 step:9631 [D loss: 0.640418, acc: 63.28%] [G loss: 1.972710]\n",
      "epoch:10 step:9632 [D loss: 0.608690, acc: 65.62%] [G loss: 2.122370]\n",
      "epoch:10 step:9633 [D loss: 0.633717, acc: 66.41%] [G loss: 1.930018]\n",
      "epoch:10 step:9634 [D loss: 0.619929, acc: 72.66%] [G loss: 2.156222]\n",
      "epoch:10 step:9635 [D loss: 0.686380, acc: 54.69%] [G loss: 1.950163]\n",
      "epoch:10 step:9636 [D loss: 0.612612, acc: 67.97%] [G loss: 2.103174]\n",
      "epoch:10 step:9637 [D loss: 0.594839, acc: 66.41%] [G loss: 2.038741]\n",
      "epoch:10 step:9638 [D loss: 0.654514, acc: 63.28%] [G loss: 1.998939]\n",
      "epoch:10 step:9639 [D loss: 0.704060, acc: 57.03%] [G loss: 2.102773]\n",
      "epoch:10 step:9640 [D loss: 0.602099, acc: 64.06%] [G loss: 2.354738]\n",
      "epoch:10 step:9641 [D loss: 0.603810, acc: 66.41%] [G loss: 2.159777]\n",
      "epoch:10 step:9642 [D loss: 0.589323, acc: 71.88%] [G loss: 2.106183]\n",
      "epoch:10 step:9643 [D loss: 0.626505, acc: 65.62%] [G loss: 2.063981]\n",
      "epoch:10 step:9644 [D loss: 0.626619, acc: 68.75%] [G loss: 2.170259]\n",
      "epoch:10 step:9645 [D loss: 0.598113, acc: 67.19%] [G loss: 1.942443]\n",
      "epoch:10 step:9646 [D loss: 0.636528, acc: 63.28%] [G loss: 2.135060]\n",
      "epoch:10 step:9647 [D loss: 0.660035, acc: 57.03%] [G loss: 1.917302]\n",
      "epoch:10 step:9648 [D loss: 0.635178, acc: 61.72%] [G loss: 1.865581]\n",
      "epoch:10 step:9649 [D loss: 0.673023, acc: 60.16%] [G loss: 2.002067]\n",
      "epoch:10 step:9650 [D loss: 0.577598, acc: 67.19%] [G loss: 2.067682]\n",
      "epoch:10 step:9651 [D loss: 0.645428, acc: 61.72%] [G loss: 1.841616]\n",
      "epoch:10 step:9652 [D loss: 0.626301, acc: 60.16%] [G loss: 2.039585]\n",
      "epoch:10 step:9653 [D loss: 0.552084, acc: 70.31%] [G loss: 2.178911]\n",
      "epoch:10 step:9654 [D loss: 0.619464, acc: 66.41%] [G loss: 2.143533]\n",
      "epoch:10 step:9655 [D loss: 0.626518, acc: 65.62%] [G loss: 1.996461]\n",
      "epoch:10 step:9656 [D loss: 0.588157, acc: 69.53%] [G loss: 2.393471]\n",
      "epoch:10 step:9657 [D loss: 0.629326, acc: 60.16%] [G loss: 2.025460]\n",
      "epoch:10 step:9658 [D loss: 0.621039, acc: 70.31%] [G loss: 1.993128]\n",
      "epoch:10 step:9659 [D loss: 0.677462, acc: 57.81%] [G loss: 1.875455]\n",
      "epoch:10 step:9660 [D loss: 0.614981, acc: 69.53%] [G loss: 1.978838]\n",
      "epoch:10 step:9661 [D loss: 0.661973, acc: 59.38%] [G loss: 1.950493]\n",
      "epoch:10 step:9662 [D loss: 0.668437, acc: 58.59%] [G loss: 2.005996]\n",
      "epoch:10 step:9663 [D loss: 0.638684, acc: 62.50%] [G loss: 1.999789]\n",
      "epoch:10 step:9664 [D loss: 0.639798, acc: 64.06%] [G loss: 1.951155]\n",
      "epoch:10 step:9665 [D loss: 0.633709, acc: 61.72%] [G loss: 2.114633]\n",
      "epoch:10 step:9666 [D loss: 0.566561, acc: 70.31%] [G loss: 2.256822]\n",
      "epoch:10 step:9667 [D loss: 0.591797, acc: 67.19%] [G loss: 1.958824]\n",
      "epoch:10 step:9668 [D loss: 0.590423, acc: 70.31%] [G loss: 2.182993]\n",
      "epoch:10 step:9669 [D loss: 0.597043, acc: 64.84%] [G loss: 2.270193]\n",
      "epoch:10 step:9670 [D loss: 0.608553, acc: 70.31%] [G loss: 2.254020]\n",
      "epoch:10 step:9671 [D loss: 0.659425, acc: 59.38%] [G loss: 2.041776]\n",
      "epoch:10 step:9672 [D loss: 0.611235, acc: 61.72%] [G loss: 2.210329]\n",
      "epoch:10 step:9673 [D loss: 0.548918, acc: 76.56%] [G loss: 2.052954]\n",
      "epoch:10 step:9674 [D loss: 0.625608, acc: 63.28%] [G loss: 2.100877]\n",
      "epoch:10 step:9675 [D loss: 0.685838, acc: 53.91%] [G loss: 2.038347]\n",
      "epoch:10 step:9676 [D loss: 0.584338, acc: 67.97%] [G loss: 2.256057]\n",
      "epoch:10 step:9677 [D loss: 0.642452, acc: 66.41%] [G loss: 2.075644]\n",
      "epoch:10 step:9678 [D loss: 0.649521, acc: 64.06%] [G loss: 1.856034]\n",
      "epoch:10 step:9679 [D loss: 0.581122, acc: 75.00%] [G loss: 2.154027]\n",
      "epoch:10 step:9680 [D loss: 0.710824, acc: 57.81%] [G loss: 1.870653]\n",
      "epoch:10 step:9681 [D loss: 0.595609, acc: 67.19%] [G loss: 2.118281]\n",
      "epoch:10 step:9682 [D loss: 0.586480, acc: 69.53%] [G loss: 2.483330]\n",
      "epoch:10 step:9683 [D loss: 0.542273, acc: 69.53%] [G loss: 2.283083]\n",
      "epoch:10 step:9684 [D loss: 0.537204, acc: 71.88%] [G loss: 2.386579]\n",
      "epoch:10 step:9685 [D loss: 0.624084, acc: 61.72%] [G loss: 2.286891]\n",
      "epoch:10 step:9686 [D loss: 0.653754, acc: 65.62%] [G loss: 2.049014]\n",
      "epoch:10 step:9687 [D loss: 0.701943, acc: 54.69%] [G loss: 1.952455]\n",
      "epoch:10 step:9688 [D loss: 0.637023, acc: 62.50%] [G loss: 2.043989]\n",
      "epoch:10 step:9689 [D loss: 0.571043, acc: 69.53%] [G loss: 2.002870]\n",
      "epoch:10 step:9690 [D loss: 0.615639, acc: 64.06%] [G loss: 2.045364]\n",
      "epoch:10 step:9691 [D loss: 0.623226, acc: 67.97%] [G loss: 2.142354]\n",
      "epoch:10 step:9692 [D loss: 0.614025, acc: 66.41%] [G loss: 2.249818]\n",
      "epoch:10 step:9693 [D loss: 0.669088, acc: 57.03%] [G loss: 2.128993]\n",
      "epoch:10 step:9694 [D loss: 0.629811, acc: 67.97%] [G loss: 1.954179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9695 [D loss: 0.626706, acc: 62.50%] [G loss: 2.113374]\n",
      "epoch:10 step:9696 [D loss: 0.631479, acc: 57.03%] [G loss: 1.931867]\n",
      "epoch:10 step:9697 [D loss: 0.680463, acc: 61.72%] [G loss: 1.977238]\n",
      "epoch:10 step:9698 [D loss: 0.628267, acc: 63.28%] [G loss: 1.953332]\n",
      "epoch:10 step:9699 [D loss: 0.631447, acc: 63.28%] [G loss: 2.256380]\n",
      "epoch:10 step:9700 [D loss: 0.617308, acc: 66.41%] [G loss: 2.044043]\n",
      "epoch:10 step:9701 [D loss: 0.622743, acc: 69.53%] [G loss: 2.319776]\n",
      "epoch:10 step:9702 [D loss: 0.586563, acc: 66.41%] [G loss: 2.276558]\n",
      "epoch:10 step:9703 [D loss: 0.606016, acc: 70.31%] [G loss: 2.401817]\n",
      "epoch:10 step:9704 [D loss: 0.678612, acc: 58.59%] [G loss: 2.249809]\n",
      "epoch:10 step:9705 [D loss: 0.585521, acc: 67.97%] [G loss: 2.468786]\n",
      "epoch:10 step:9706 [D loss: 0.560170, acc: 73.44%] [G loss: 2.233623]\n",
      "epoch:10 step:9707 [D loss: 0.603881, acc: 63.28%] [G loss: 2.267720]\n",
      "epoch:10 step:9708 [D loss: 0.596374, acc: 67.19%] [G loss: 2.170646]\n",
      "epoch:10 step:9709 [D loss: 0.630690, acc: 61.72%] [G loss: 2.267143]\n",
      "epoch:10 step:9710 [D loss: 0.675827, acc: 67.19%] [G loss: 2.182395]\n",
      "epoch:10 step:9711 [D loss: 0.682840, acc: 58.59%] [G loss: 1.931908]\n",
      "epoch:10 step:9712 [D loss: 0.662803, acc: 60.94%] [G loss: 1.902512]\n",
      "epoch:10 step:9713 [D loss: 0.628382, acc: 64.84%] [G loss: 1.989924]\n",
      "epoch:10 step:9714 [D loss: 0.663938, acc: 63.28%] [G loss: 2.099050]\n",
      "epoch:10 step:9715 [D loss: 0.564859, acc: 72.66%] [G loss: 2.422201]\n",
      "epoch:10 step:9716 [D loss: 0.612905, acc: 70.31%] [G loss: 2.344984]\n",
      "epoch:10 step:9717 [D loss: 0.539811, acc: 75.78%] [G loss: 2.502504]\n",
      "epoch:10 step:9718 [D loss: 0.652408, acc: 61.72%] [G loss: 1.969181]\n",
      "epoch:10 step:9719 [D loss: 0.646727, acc: 62.50%] [G loss: 1.853956]\n",
      "epoch:10 step:9720 [D loss: 0.667199, acc: 58.59%] [G loss: 2.036410]\n",
      "epoch:10 step:9721 [D loss: 0.697578, acc: 52.34%] [G loss: 1.959155]\n",
      "epoch:10 step:9722 [D loss: 0.588247, acc: 67.97%] [G loss: 1.986901]\n",
      "epoch:10 step:9723 [D loss: 0.616055, acc: 66.41%] [G loss: 2.078066]\n",
      "epoch:10 step:9724 [D loss: 0.582805, acc: 67.97%] [G loss: 2.340189]\n",
      "epoch:10 step:9725 [D loss: 0.670649, acc: 60.16%] [G loss: 1.923671]\n",
      "epoch:10 step:9726 [D loss: 0.681821, acc: 61.72%] [G loss: 1.803900]\n",
      "epoch:10 step:9727 [D loss: 0.595338, acc: 68.75%] [G loss: 2.300015]\n",
      "epoch:10 step:9728 [D loss: 0.544777, acc: 73.44%] [G loss: 2.217072]\n",
      "epoch:10 step:9729 [D loss: 0.555415, acc: 71.88%] [G loss: 2.180969]\n",
      "epoch:10 step:9730 [D loss: 0.628302, acc: 66.41%] [G loss: 2.124768]\n",
      "epoch:10 step:9731 [D loss: 0.610430, acc: 70.31%] [G loss: 2.086448]\n",
      "epoch:10 step:9732 [D loss: 0.664202, acc: 60.94%] [G loss: 2.034000]\n",
      "epoch:10 step:9733 [D loss: 0.697178, acc: 60.16%] [G loss: 2.093565]\n",
      "epoch:10 step:9734 [D loss: 0.625166, acc: 64.06%] [G loss: 2.058726]\n",
      "epoch:10 step:9735 [D loss: 0.637183, acc: 63.28%] [G loss: 2.105824]\n",
      "epoch:10 step:9736 [D loss: 0.612827, acc: 67.19%] [G loss: 2.263005]\n",
      "epoch:10 step:9737 [D loss: 0.632206, acc: 60.94%] [G loss: 1.983274]\n",
      "epoch:10 step:9738 [D loss: 0.625607, acc: 64.84%] [G loss: 2.091438]\n",
      "epoch:10 step:9739 [D loss: 0.648419, acc: 60.16%] [G loss: 1.938787]\n",
      "epoch:10 step:9740 [D loss: 0.637946, acc: 69.53%] [G loss: 1.977880]\n",
      "epoch:10 step:9741 [D loss: 0.603815, acc: 67.97%] [G loss: 2.091455]\n",
      "epoch:10 step:9742 [D loss: 0.638355, acc: 60.16%] [G loss: 1.976977]\n",
      "epoch:10 step:9743 [D loss: 0.694554, acc: 58.59%] [G loss: 1.797313]\n",
      "epoch:10 step:9744 [D loss: 0.632974, acc: 61.72%] [G loss: 2.163830]\n",
      "epoch:10 step:9745 [D loss: 0.675763, acc: 57.81%] [G loss: 1.838227]\n",
      "epoch:10 step:9746 [D loss: 0.697988, acc: 53.91%] [G loss: 1.857025]\n",
      "epoch:10 step:9747 [D loss: 0.752241, acc: 47.66%] [G loss: 1.890882]\n",
      "epoch:10 step:9748 [D loss: 0.642252, acc: 64.06%] [G loss: 2.045176]\n",
      "epoch:10 step:9749 [D loss: 0.657925, acc: 58.59%] [G loss: 2.070561]\n",
      "epoch:10 step:9750 [D loss: 0.562675, acc: 71.09%] [G loss: 2.147504]\n",
      "epoch:10 step:9751 [D loss: 0.598296, acc: 67.19%] [G loss: 2.098932]\n",
      "epoch:10 step:9752 [D loss: 0.668390, acc: 65.62%] [G loss: 1.893003]\n",
      "epoch:10 step:9753 [D loss: 0.674252, acc: 60.16%] [G loss: 1.910717]\n",
      "epoch:10 step:9754 [D loss: 0.624780, acc: 65.62%] [G loss: 2.023214]\n",
      "epoch:10 step:9755 [D loss: 0.631730, acc: 64.06%] [G loss: 2.061882]\n",
      "epoch:10 step:9756 [D loss: 0.626878, acc: 61.72%] [G loss: 1.764889]\n",
      "epoch:10 step:9757 [D loss: 0.610462, acc: 65.62%] [G loss: 1.858702]\n",
      "epoch:10 step:9758 [D loss: 0.659719, acc: 62.50%] [G loss: 2.007328]\n",
      "epoch:10 step:9759 [D loss: 0.637631, acc: 65.62%] [G loss: 2.173011]\n",
      "epoch:10 step:9760 [D loss: 0.673556, acc: 58.59%] [G loss: 2.173583]\n",
      "epoch:10 step:9761 [D loss: 0.677746, acc: 63.28%] [G loss: 1.906331]\n",
      "epoch:10 step:9762 [D loss: 0.638302, acc: 66.41%] [G loss: 2.018440]\n",
      "epoch:10 step:9763 [D loss: 0.578449, acc: 65.62%] [G loss: 2.158267]\n",
      "epoch:10 step:9764 [D loss: 0.643439, acc: 59.38%] [G loss: 1.943056]\n",
      "epoch:10 step:9765 [D loss: 0.612508, acc: 67.97%] [G loss: 1.960522]\n",
      "epoch:10 step:9766 [D loss: 0.742260, acc: 50.78%] [G loss: 1.934390]\n",
      "epoch:10 step:9767 [D loss: 0.686845, acc: 58.59%] [G loss: 1.973657]\n",
      "epoch:10 step:9768 [D loss: 0.605127, acc: 69.53%] [G loss: 2.117851]\n",
      "epoch:10 step:9769 [D loss: 0.664878, acc: 60.16%] [G loss: 2.279531]\n",
      "epoch:10 step:9770 [D loss: 0.701340, acc: 60.16%] [G loss: 1.974726]\n",
      "epoch:10 step:9771 [D loss: 0.594936, acc: 69.53%] [G loss: 2.071594]\n",
      "epoch:10 step:9772 [D loss: 0.664850, acc: 61.72%] [G loss: 2.028634]\n",
      "epoch:10 step:9773 [D loss: 0.655105, acc: 59.38%] [G loss: 2.100617]\n",
      "epoch:10 step:9774 [D loss: 0.666262, acc: 56.25%] [G loss: 2.133410]\n",
      "epoch:10 step:9775 [D loss: 0.600371, acc: 69.53%] [G loss: 2.130083]\n",
      "epoch:10 step:9776 [D loss: 0.571733, acc: 72.66%] [G loss: 2.238737]\n",
      "epoch:10 step:9777 [D loss: 0.687017, acc: 60.16%] [G loss: 1.979231]\n",
      "epoch:10 step:9778 [D loss: 0.624733, acc: 68.75%] [G loss: 1.906458]\n",
      "epoch:10 step:9779 [D loss: 0.637137, acc: 62.50%] [G loss: 1.991892]\n",
      "epoch:10 step:9780 [D loss: 0.687431, acc: 57.81%] [G loss: 1.986738]\n",
      "epoch:10 step:9781 [D loss: 0.649257, acc: 60.94%] [G loss: 1.879607]\n",
      "epoch:10 step:9782 [D loss: 0.569473, acc: 77.34%] [G loss: 2.212545]\n",
      "epoch:10 step:9783 [D loss: 0.655699, acc: 59.38%] [G loss: 1.943118]\n",
      "epoch:10 step:9784 [D loss: 0.583881, acc: 71.09%] [G loss: 2.060387]\n",
      "epoch:10 step:9785 [D loss: 0.613168, acc: 67.19%] [G loss: 2.168434]\n",
      "epoch:10 step:9786 [D loss: 0.594680, acc: 69.53%] [G loss: 2.091185]\n",
      "epoch:10 step:9787 [D loss: 0.688547, acc: 59.38%] [G loss: 2.177448]\n",
      "epoch:10 step:9788 [D loss: 0.647492, acc: 62.50%] [G loss: 1.950460]\n",
      "epoch:10 step:9789 [D loss: 0.651180, acc: 65.62%] [G loss: 2.014215]\n",
      "epoch:10 step:9790 [D loss: 0.628178, acc: 64.84%] [G loss: 1.961239]\n",
      "epoch:10 step:9791 [D loss: 0.653576, acc: 58.59%] [G loss: 1.826610]\n",
      "epoch:10 step:9792 [D loss: 0.615860, acc: 64.84%] [G loss: 2.209687]\n",
      "epoch:10 step:9793 [D loss: 0.669743, acc: 59.38%] [G loss: 1.926343]\n",
      "epoch:10 step:9794 [D loss: 0.584589, acc: 64.06%] [G loss: 2.069217]\n",
      "epoch:10 step:9795 [D loss: 0.630700, acc: 61.72%] [G loss: 2.170758]\n",
      "epoch:10 step:9796 [D loss: 0.631230, acc: 64.84%] [G loss: 2.031599]\n",
      "epoch:10 step:9797 [D loss: 0.716216, acc: 61.72%] [G loss: 2.248385]\n",
      "epoch:10 step:9798 [D loss: 0.630476, acc: 64.84%] [G loss: 2.364101]\n",
      "epoch:10 step:9799 [D loss: 0.563431, acc: 70.31%] [G loss: 2.297426]\n",
      "epoch:10 step:9800 [D loss: 0.556299, acc: 69.53%] [G loss: 2.349600]\n",
      "##############\n",
      "[2.4850798  1.31795645 6.21064076 4.8934243  3.71573073 5.70910003\n",
      " 4.45966619 4.63539296 4.67455514 3.67514681]\n",
      "##########\n",
      "epoch:10 step:9801 [D loss: 0.637111, acc: 64.84%] [G loss: 2.177372]\n",
      "epoch:10 step:9802 [D loss: 0.687725, acc: 57.03%] [G loss: 1.929005]\n",
      "epoch:10 step:9803 [D loss: 0.621957, acc: 64.06%] [G loss: 2.110292]\n",
      "epoch:10 step:9804 [D loss: 0.582986, acc: 71.09%] [G loss: 2.186857]\n",
      "epoch:10 step:9805 [D loss: 0.661509, acc: 60.94%] [G loss: 2.128809]\n",
      "epoch:10 step:9806 [D loss: 0.610292, acc: 67.19%] [G loss: 2.343578]\n",
      "epoch:10 step:9807 [D loss: 0.719078, acc: 54.69%] [G loss: 2.008720]\n",
      "epoch:10 step:9808 [D loss: 0.717394, acc: 52.34%] [G loss: 2.000021]\n",
      "epoch:10 step:9809 [D loss: 0.643952, acc: 64.06%] [G loss: 1.983773]\n",
      "epoch:10 step:9810 [D loss: 0.688083, acc: 54.69%] [G loss: 1.949442]\n",
      "epoch:10 step:9811 [D loss: 0.656715, acc: 58.59%] [G loss: 1.871127]\n",
      "epoch:10 step:9812 [D loss: 0.626846, acc: 64.84%] [G loss: 1.986111]\n",
      "epoch:10 step:9813 [D loss: 0.582811, acc: 74.22%] [G loss: 2.007330]\n",
      "epoch:10 step:9814 [D loss: 0.636585, acc: 60.16%] [G loss: 1.960468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9815 [D loss: 0.621911, acc: 67.97%] [G loss: 1.981725]\n",
      "epoch:10 step:9816 [D loss: 0.641493, acc: 59.38%] [G loss: 1.940977]\n",
      "epoch:10 step:9817 [D loss: 0.621566, acc: 64.84%] [G loss: 2.136314]\n",
      "epoch:10 step:9818 [D loss: 0.721336, acc: 56.25%] [G loss: 1.864768]\n",
      "epoch:10 step:9819 [D loss: 0.702632, acc: 57.81%] [G loss: 1.922086]\n",
      "epoch:10 step:9820 [D loss: 0.571901, acc: 75.00%] [G loss: 1.907115]\n",
      "epoch:10 step:9821 [D loss: 0.584787, acc: 69.53%] [G loss: 2.181612]\n",
      "epoch:10 step:9822 [D loss: 0.602136, acc: 71.88%] [G loss: 1.953161]\n",
      "epoch:10 step:9823 [D loss: 0.640152, acc: 62.50%] [G loss: 2.230842]\n",
      "epoch:10 step:9824 [D loss: 0.686220, acc: 57.81%] [G loss: 1.967124]\n",
      "epoch:10 step:9825 [D loss: 0.620805, acc: 62.50%] [G loss: 2.035239]\n",
      "epoch:10 step:9826 [D loss: 0.643618, acc: 64.84%] [G loss: 2.080367]\n",
      "epoch:10 step:9827 [D loss: 0.673332, acc: 63.28%] [G loss: 2.002345]\n",
      "epoch:10 step:9828 [D loss: 0.682503, acc: 56.25%] [G loss: 1.909212]\n",
      "epoch:10 step:9829 [D loss: 0.639932, acc: 65.62%] [G loss: 1.861259]\n",
      "epoch:10 step:9830 [D loss: 0.658347, acc: 63.28%] [G loss: 1.880377]\n",
      "epoch:10 step:9831 [D loss: 0.662729, acc: 57.81%] [G loss: 1.986819]\n",
      "epoch:10 step:9832 [D loss: 0.621619, acc: 63.28%] [G loss: 2.077011]\n",
      "epoch:10 step:9833 [D loss: 0.672553, acc: 62.50%] [G loss: 1.966258]\n",
      "epoch:10 step:9834 [D loss: 0.641595, acc: 64.84%] [G loss: 2.004856]\n",
      "epoch:10 step:9835 [D loss: 0.681940, acc: 60.16%] [G loss: 1.920701]\n",
      "epoch:10 step:9836 [D loss: 0.658663, acc: 63.28%] [G loss: 2.073833]\n",
      "epoch:10 step:9837 [D loss: 0.638934, acc: 67.19%] [G loss: 1.910629]\n",
      "epoch:10 step:9838 [D loss: 0.556531, acc: 71.88%] [G loss: 2.256634]\n",
      "epoch:10 step:9839 [D loss: 0.614563, acc: 67.97%] [G loss: 2.152939]\n",
      "epoch:10 step:9840 [D loss: 0.603744, acc: 70.31%] [G loss: 2.190557]\n",
      "epoch:10 step:9841 [D loss: 0.571627, acc: 63.28%] [G loss: 2.563426]\n",
      "epoch:10 step:9842 [D loss: 0.586092, acc: 67.97%] [G loss: 2.217159]\n",
      "epoch:10 step:9843 [D loss: 0.679387, acc: 57.03%] [G loss: 1.957441]\n",
      "epoch:10 step:9844 [D loss: 0.642081, acc: 64.84%] [G loss: 1.830613]\n",
      "epoch:10 step:9845 [D loss: 0.646099, acc: 63.28%] [G loss: 2.129202]\n",
      "epoch:10 step:9846 [D loss: 0.667904, acc: 67.19%] [G loss: 2.059850]\n",
      "epoch:10 step:9847 [D loss: 0.642788, acc: 60.94%] [G loss: 1.894783]\n",
      "epoch:10 step:9848 [D loss: 0.692944, acc: 59.38%] [G loss: 1.810086]\n",
      "epoch:10 step:9849 [D loss: 0.590936, acc: 69.53%] [G loss: 2.006453]\n",
      "epoch:10 step:9850 [D loss: 0.627151, acc: 60.94%] [G loss: 2.128379]\n",
      "epoch:10 step:9851 [D loss: 0.615459, acc: 67.97%] [G loss: 2.289658]\n",
      "epoch:10 step:9852 [D loss: 0.665267, acc: 55.47%] [G loss: 1.852061]\n",
      "epoch:10 step:9853 [D loss: 0.670194, acc: 59.38%] [G loss: 2.051406]\n",
      "epoch:10 step:9854 [D loss: 0.594646, acc: 62.50%] [G loss: 2.165215]\n",
      "epoch:10 step:9855 [D loss: 0.594662, acc: 71.88%] [G loss: 2.094217]\n",
      "epoch:10 step:9856 [D loss: 0.690170, acc: 62.50%] [G loss: 2.004246]\n",
      "epoch:10 step:9857 [D loss: 0.627716, acc: 61.72%] [G loss: 1.982724]\n",
      "epoch:10 step:9858 [D loss: 0.583349, acc: 69.53%] [G loss: 2.224994]\n",
      "epoch:10 step:9859 [D loss: 0.670531, acc: 60.16%] [G loss: 1.926000]\n",
      "epoch:10 step:9860 [D loss: 0.616435, acc: 64.84%] [G loss: 1.990712]\n",
      "epoch:10 step:9861 [D loss: 0.659569, acc: 62.50%] [G loss: 2.099493]\n",
      "epoch:10 step:9862 [D loss: 0.676552, acc: 59.38%] [G loss: 1.975796]\n",
      "epoch:10 step:9863 [D loss: 0.599019, acc: 68.75%] [G loss: 2.085423]\n",
      "epoch:10 step:9864 [D loss: 0.591574, acc: 64.84%] [G loss: 2.318709]\n",
      "epoch:10 step:9865 [D loss: 0.616060, acc: 71.09%] [G loss: 2.323216]\n",
      "epoch:10 step:9866 [D loss: 0.604686, acc: 66.41%] [G loss: 2.250843]\n",
      "epoch:10 step:9867 [D loss: 0.604888, acc: 69.53%] [G loss: 2.369694]\n",
      "epoch:10 step:9868 [D loss: 0.578978, acc: 71.88%] [G loss: 2.560293]\n",
      "epoch:10 step:9869 [D loss: 0.566482, acc: 75.00%] [G loss: 2.360954]\n",
      "epoch:10 step:9870 [D loss: 0.739087, acc: 51.56%] [G loss: 1.808968]\n",
      "epoch:10 step:9871 [D loss: 0.714365, acc: 58.59%] [G loss: 2.041431]\n",
      "epoch:10 step:9872 [D loss: 0.698137, acc: 57.03%] [G loss: 1.890884]\n",
      "epoch:10 step:9873 [D loss: 0.650074, acc: 66.41%] [G loss: 1.923605]\n",
      "epoch:10 step:9874 [D loss: 0.606059, acc: 67.97%] [G loss: 2.246113]\n",
      "epoch:10 step:9875 [D loss: 0.589360, acc: 67.19%] [G loss: 2.094182]\n",
      "epoch:10 step:9876 [D loss: 0.644719, acc: 61.72%] [G loss: 1.950710]\n",
      "epoch:10 step:9877 [D loss: 0.667656, acc: 62.50%] [G loss: 1.884558]\n",
      "epoch:10 step:9878 [D loss: 0.603274, acc: 74.22%] [G loss: 2.009169]\n",
      "epoch:10 step:9879 [D loss: 0.641121, acc: 65.62%] [G loss: 2.069036]\n",
      "epoch:10 step:9880 [D loss: 0.700714, acc: 62.50%] [G loss: 1.955718]\n",
      "epoch:10 step:9881 [D loss: 0.628405, acc: 65.62%] [G loss: 1.857014]\n",
      "epoch:10 step:9882 [D loss: 0.618587, acc: 65.62%] [G loss: 2.158576]\n",
      "epoch:10 step:9883 [D loss: 0.702402, acc: 59.38%] [G loss: 1.954720]\n",
      "epoch:10 step:9884 [D loss: 0.597313, acc: 68.75%] [G loss: 2.116584]\n",
      "epoch:10 step:9885 [D loss: 0.593840, acc: 68.75%] [G loss: 2.102725]\n",
      "epoch:10 step:9886 [D loss: 0.612264, acc: 67.19%] [G loss: 1.898071]\n",
      "epoch:10 step:9887 [D loss: 0.613333, acc: 67.19%] [G loss: 2.218944]\n",
      "epoch:10 step:9888 [D loss: 0.601646, acc: 67.97%] [G loss: 1.980913]\n",
      "epoch:10 step:9889 [D loss: 0.605312, acc: 66.41%] [G loss: 1.984080]\n",
      "epoch:10 step:9890 [D loss: 0.636664, acc: 62.50%] [G loss: 2.002598]\n",
      "epoch:10 step:9891 [D loss: 0.616844, acc: 63.28%] [G loss: 2.044654]\n",
      "epoch:10 step:9892 [D loss: 0.588436, acc: 68.75%] [G loss: 2.250357]\n",
      "epoch:10 step:9893 [D loss: 0.631155, acc: 67.19%] [G loss: 2.094044]\n",
      "epoch:10 step:9894 [D loss: 0.581768, acc: 71.88%] [G loss: 2.179462]\n",
      "epoch:10 step:9895 [D loss: 0.635680, acc: 62.50%] [G loss: 2.105089]\n",
      "epoch:10 step:9896 [D loss: 0.649715, acc: 64.06%] [G loss: 1.993680]\n",
      "epoch:10 step:9897 [D loss: 0.634278, acc: 62.50%] [G loss: 1.871104]\n",
      "epoch:10 step:9898 [D loss: 0.674244, acc: 60.94%] [G loss: 1.856668]\n",
      "epoch:10 step:9899 [D loss: 0.675465, acc: 59.38%] [G loss: 1.872930]\n",
      "epoch:10 step:9900 [D loss: 0.572910, acc: 73.44%] [G loss: 1.934294]\n",
      "epoch:10 step:9901 [D loss: 0.689029, acc: 57.81%] [G loss: 1.880648]\n",
      "epoch:10 step:9902 [D loss: 0.651007, acc: 63.28%] [G loss: 2.067794]\n",
      "epoch:10 step:9903 [D loss: 0.632960, acc: 63.28%] [G loss: 2.085549]\n",
      "epoch:10 step:9904 [D loss: 0.593794, acc: 70.31%] [G loss: 1.958360]\n",
      "epoch:10 step:9905 [D loss: 0.682497, acc: 56.25%] [G loss: 2.018667]\n",
      "epoch:10 step:9906 [D loss: 0.618128, acc: 68.75%] [G loss: 2.220220]\n",
      "epoch:10 step:9907 [D loss: 0.656013, acc: 61.72%] [G loss: 2.026866]\n",
      "epoch:10 step:9908 [D loss: 0.702299, acc: 55.47%] [G loss: 2.042253]\n",
      "epoch:10 step:9909 [D loss: 0.694394, acc: 55.47%] [G loss: 1.898526]\n",
      "epoch:10 step:9910 [D loss: 0.664075, acc: 59.38%] [G loss: 1.921815]\n",
      "epoch:10 step:9911 [D loss: 0.608579, acc: 69.53%] [G loss: 1.932509]\n",
      "epoch:10 step:9912 [D loss: 0.712514, acc: 52.34%] [G loss: 1.859413]\n",
      "epoch:10 step:9913 [D loss: 0.624513, acc: 60.16%] [G loss: 1.892118]\n",
      "epoch:10 step:9914 [D loss: 0.666269, acc: 63.28%] [G loss: 1.979841]\n",
      "epoch:10 step:9915 [D loss: 0.629520, acc: 62.50%] [G loss: 2.127900]\n",
      "epoch:10 step:9916 [D loss: 0.667062, acc: 62.50%] [G loss: 2.053190]\n",
      "epoch:10 step:9917 [D loss: 0.577055, acc: 74.22%] [G loss: 2.134655]\n",
      "epoch:10 step:9918 [D loss: 0.621736, acc: 61.72%] [G loss: 2.110798]\n",
      "epoch:10 step:9919 [D loss: 0.572501, acc: 69.53%] [G loss: 2.289358]\n",
      "epoch:10 step:9920 [D loss: 0.605248, acc: 64.84%] [G loss: 2.181740]\n",
      "epoch:10 step:9921 [D loss: 0.558673, acc: 74.22%] [G loss: 2.372450]\n",
      "epoch:10 step:9922 [D loss: 0.608270, acc: 66.41%] [G loss: 2.113963]\n",
      "epoch:10 step:9923 [D loss: 0.661915, acc: 58.59%] [G loss: 1.962682]\n",
      "epoch:10 step:9924 [D loss: 0.555470, acc: 72.66%] [G loss: 2.407559]\n",
      "epoch:10 step:9925 [D loss: 0.618821, acc: 65.62%] [G loss: 2.072015]\n",
      "epoch:10 step:9926 [D loss: 0.585544, acc: 70.31%] [G loss: 2.231512]\n",
      "epoch:10 step:9927 [D loss: 0.598553, acc: 64.06%] [G loss: 2.166140]\n",
      "epoch:10 step:9928 [D loss: 0.573548, acc: 72.66%] [G loss: 2.212520]\n",
      "epoch:10 step:9929 [D loss: 0.717637, acc: 58.59%] [G loss: 1.910603]\n",
      "epoch:10 step:9930 [D loss: 0.666895, acc: 61.72%] [G loss: 1.956468]\n",
      "epoch:10 step:9931 [D loss: 0.665107, acc: 58.59%] [G loss: 2.069655]\n",
      "epoch:10 step:9932 [D loss: 0.618678, acc: 68.75%] [G loss: 1.919873]\n",
      "epoch:10 step:9933 [D loss: 0.645593, acc: 64.84%] [G loss: 1.951490]\n",
      "epoch:10 step:9934 [D loss: 0.604250, acc: 64.84%] [G loss: 2.209123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9935 [D loss: 0.613236, acc: 65.62%] [G loss: 1.883836]\n",
      "epoch:10 step:9936 [D loss: 0.708397, acc: 55.47%] [G loss: 2.028650]\n",
      "epoch:10 step:9937 [D loss: 0.660946, acc: 65.62%] [G loss: 2.042710]\n",
      "epoch:10 step:9938 [D loss: 0.618940, acc: 66.41%] [G loss: 2.207905]\n",
      "epoch:10 step:9939 [D loss: 0.627438, acc: 61.72%] [G loss: 1.950606]\n",
      "epoch:10 step:9940 [D loss: 0.625663, acc: 66.41%] [G loss: 2.064560]\n",
      "epoch:10 step:9941 [D loss: 0.666515, acc: 60.16%] [G loss: 1.928032]\n",
      "epoch:10 step:9942 [D loss: 0.665056, acc: 61.72%] [G loss: 2.030224]\n",
      "epoch:10 step:9943 [D loss: 0.623453, acc: 64.84%] [G loss: 1.999151]\n",
      "epoch:10 step:9944 [D loss: 0.628099, acc: 66.41%] [G loss: 2.137254]\n",
      "epoch:10 step:9945 [D loss: 0.635444, acc: 61.72%] [G loss: 2.048528]\n",
      "epoch:10 step:9946 [D loss: 0.677755, acc: 58.59%] [G loss: 1.981666]\n",
      "epoch:10 step:9947 [D loss: 0.606286, acc: 65.62%] [G loss: 1.797531]\n",
      "epoch:10 step:9948 [D loss: 0.634130, acc: 60.94%] [G loss: 1.943400]\n",
      "epoch:10 step:9949 [D loss: 0.684435, acc: 60.16%] [G loss: 1.951164]\n",
      "epoch:10 step:9950 [D loss: 0.612737, acc: 64.06%] [G loss: 2.102120]\n",
      "epoch:10 step:9951 [D loss: 0.610413, acc: 70.31%] [G loss: 2.048112]\n",
      "epoch:10 step:9952 [D loss: 0.599817, acc: 71.09%] [G loss: 2.044621]\n",
      "epoch:10 step:9953 [D loss: 0.626481, acc: 65.62%] [G loss: 1.867652]\n",
      "epoch:10 step:9954 [D loss: 0.618026, acc: 62.50%] [G loss: 1.995629]\n",
      "epoch:10 step:9955 [D loss: 0.645945, acc: 61.72%] [G loss: 1.987047]\n",
      "epoch:10 step:9956 [D loss: 0.648856, acc: 61.72%] [G loss: 1.923849]\n",
      "epoch:10 step:9957 [D loss: 0.662186, acc: 59.38%] [G loss: 2.076967]\n",
      "epoch:10 step:9958 [D loss: 0.640458, acc: 61.72%] [G loss: 1.906723]\n",
      "epoch:10 step:9959 [D loss: 0.604178, acc: 67.97%] [G loss: 2.040504]\n",
      "epoch:10 step:9960 [D loss: 0.600093, acc: 74.22%] [G loss: 2.023464]\n",
      "epoch:10 step:9961 [D loss: 0.622015, acc: 64.06%] [G loss: 2.123550]\n",
      "epoch:10 step:9962 [D loss: 0.647897, acc: 63.28%] [G loss: 2.068233]\n",
      "epoch:10 step:9963 [D loss: 0.675049, acc: 60.16%] [G loss: 1.979365]\n",
      "epoch:10 step:9964 [D loss: 0.644406, acc: 65.62%] [G loss: 2.042519]\n",
      "epoch:10 step:9965 [D loss: 0.645078, acc: 62.50%] [G loss: 1.965133]\n",
      "epoch:10 step:9966 [D loss: 0.648838, acc: 55.47%] [G loss: 1.953049]\n",
      "epoch:10 step:9967 [D loss: 0.664368, acc: 60.16%] [G loss: 1.935815]\n",
      "epoch:10 step:9968 [D loss: 0.656184, acc: 63.28%] [G loss: 2.015729]\n",
      "epoch:10 step:9969 [D loss: 0.679609, acc: 57.03%] [G loss: 1.915526]\n",
      "epoch:10 step:9970 [D loss: 0.582174, acc: 69.53%] [G loss: 2.130239]\n",
      "epoch:10 step:9971 [D loss: 0.751279, acc: 49.22%] [G loss: 1.928001]\n",
      "epoch:10 step:9972 [D loss: 0.653297, acc: 61.72%] [G loss: 1.838585]\n",
      "epoch:10 step:9973 [D loss: 0.631520, acc: 67.19%] [G loss: 2.097197]\n",
      "epoch:10 step:9974 [D loss: 0.692214, acc: 60.16%] [G loss: 2.015492]\n",
      "epoch:10 step:9975 [D loss: 0.628760, acc: 67.97%] [G loss: 2.209388]\n",
      "epoch:10 step:9976 [D loss: 0.642171, acc: 60.16%] [G loss: 1.956463]\n",
      "epoch:10 step:9977 [D loss: 0.624234, acc: 63.28%] [G loss: 2.036066]\n",
      "epoch:10 step:9978 [D loss: 0.603017, acc: 67.97%] [G loss: 1.989641]\n",
      "epoch:10 step:9979 [D loss: 0.653447, acc: 63.28%] [G loss: 2.050760]\n",
      "epoch:10 step:9980 [D loss: 0.657707, acc: 65.62%] [G loss: 2.061785]\n",
      "epoch:10 step:9981 [D loss: 0.634934, acc: 61.72%] [G loss: 1.893131]\n",
      "epoch:10 step:9982 [D loss: 0.616810, acc: 64.84%] [G loss: 1.972813]\n",
      "epoch:10 step:9983 [D loss: 0.613211, acc: 65.62%] [G loss: 2.204081]\n",
      "epoch:10 step:9984 [D loss: 0.625007, acc: 68.75%] [G loss: 1.893955]\n",
      "epoch:10 step:9985 [D loss: 0.644145, acc: 63.28%] [G loss: 1.892862]\n",
      "epoch:10 step:9986 [D loss: 0.698483, acc: 60.16%] [G loss: 1.842068]\n",
      "epoch:10 step:9987 [D loss: 0.610087, acc: 67.19%] [G loss: 1.830073]\n",
      "epoch:10 step:9988 [D loss: 0.557463, acc: 73.44%] [G loss: 2.061087]\n",
      "epoch:10 step:9989 [D loss: 0.580176, acc: 72.66%] [G loss: 1.945959]\n",
      "epoch:10 step:9990 [D loss: 0.664873, acc: 60.94%] [G loss: 2.179621]\n",
      "epoch:10 step:9991 [D loss: 0.630145, acc: 67.97%] [G loss: 1.950996]\n",
      "epoch:10 step:9992 [D loss: 0.681107, acc: 59.38%] [G loss: 2.015318]\n",
      "epoch:10 step:9993 [D loss: 0.628137, acc: 68.75%] [G loss: 2.097733]\n",
      "epoch:10 step:9994 [D loss: 0.623735, acc: 64.84%] [G loss: 1.951221]\n",
      "epoch:10 step:9995 [D loss: 0.648416, acc: 61.72%] [G loss: 1.966650]\n",
      "epoch:10 step:9996 [D loss: 0.632288, acc: 64.06%] [G loss: 2.015440]\n",
      "epoch:10 step:9997 [D loss: 0.685471, acc: 57.81%] [G loss: 2.012643]\n",
      "epoch:10 step:9998 [D loss: 0.631835, acc: 64.84%] [G loss: 1.698058]\n",
      "epoch:10 step:9999 [D loss: 0.597819, acc: 64.84%] [G loss: 2.054935]\n",
      "epoch:10 step:10000 [D loss: 0.613698, acc: 65.62%] [G loss: 2.247492]\n",
      "##############\n",
      "[2.55635177 1.42531    6.60615588 4.84685537 3.77923562 5.71557602\n",
      " 4.53905481 4.70174064 4.83132852 3.77120699]\n",
      "##########\n",
      "epoch:10 step:10001 [D loss: 0.631789, acc: 63.28%] [G loss: 2.183018]\n",
      "epoch:10 step:10002 [D loss: 0.572941, acc: 71.88%] [G loss: 2.158900]\n",
      "epoch:10 step:10003 [D loss: 0.666967, acc: 64.06%] [G loss: 1.916250]\n",
      "epoch:10 step:10004 [D loss: 0.584433, acc: 73.44%] [G loss: 2.043157]\n",
      "epoch:10 step:10005 [D loss: 0.610053, acc: 64.84%] [G loss: 2.139700]\n",
      "epoch:10 step:10006 [D loss: 0.632831, acc: 65.62%] [G loss: 2.005776]\n",
      "epoch:10 step:10007 [D loss: 0.624444, acc: 61.72%] [G loss: 2.303277]\n",
      "epoch:10 step:10008 [D loss: 0.602767, acc: 66.41%] [G loss: 2.161111]\n",
      "epoch:10 step:10009 [D loss: 0.628960, acc: 61.72%] [G loss: 2.199805]\n",
      "epoch:10 step:10010 [D loss: 0.674963, acc: 60.94%] [G loss: 2.066523]\n",
      "epoch:10 step:10011 [D loss: 0.655235, acc: 60.94%] [G loss: 2.063079]\n",
      "epoch:10 step:10012 [D loss: 0.579100, acc: 66.41%] [G loss: 2.296504]\n",
      "epoch:10 step:10013 [D loss: 0.614948, acc: 62.50%] [G loss: 2.443026]\n",
      "epoch:10 step:10014 [D loss: 0.603126, acc: 64.06%] [G loss: 2.168636]\n",
      "epoch:10 step:10015 [D loss: 0.622971, acc: 66.41%] [G loss: 1.986937]\n",
      "epoch:10 step:10016 [D loss: 0.622266, acc: 63.28%] [G loss: 2.342552]\n",
      "epoch:10 step:10017 [D loss: 0.583050, acc: 74.22%] [G loss: 2.189929]\n",
      "epoch:10 step:10018 [D loss: 0.584523, acc: 71.09%] [G loss: 2.689955]\n",
      "epoch:10 step:10019 [D loss: 0.572725, acc: 68.75%] [G loss: 2.286117]\n",
      "epoch:10 step:10020 [D loss: 0.581201, acc: 67.97%] [G loss: 2.316695]\n",
      "epoch:10 step:10021 [D loss: 0.600823, acc: 63.28%] [G loss: 2.053575]\n",
      "epoch:10 step:10022 [D loss: 0.704727, acc: 55.47%] [G loss: 2.116709]\n",
      "epoch:10 step:10023 [D loss: 0.664384, acc: 60.16%] [G loss: 2.038539]\n",
      "epoch:10 step:10024 [D loss: 0.631683, acc: 62.50%] [G loss: 2.123725]\n",
      "epoch:10 step:10025 [D loss: 0.633999, acc: 66.41%] [G loss: 2.009118]\n",
      "epoch:10 step:10026 [D loss: 0.651823, acc: 61.72%] [G loss: 1.910369]\n",
      "epoch:10 step:10027 [D loss: 0.626183, acc: 66.41%] [G loss: 2.040244]\n",
      "epoch:10 step:10028 [D loss: 0.711595, acc: 52.34%] [G loss: 1.894939]\n",
      "epoch:10 step:10029 [D loss: 0.671889, acc: 60.94%] [G loss: 1.760244]\n",
      "epoch:10 step:10030 [D loss: 0.612587, acc: 64.84%] [G loss: 2.084867]\n",
      "epoch:10 step:10031 [D loss: 0.652305, acc: 65.62%] [G loss: 2.038841]\n",
      "epoch:10 step:10032 [D loss: 0.631928, acc: 63.28%] [G loss: 2.298709]\n",
      "epoch:10 step:10033 [D loss: 0.663816, acc: 60.94%] [G loss: 2.141753]\n",
      "epoch:10 step:10034 [D loss: 0.645757, acc: 63.28%] [G loss: 1.933464]\n",
      "epoch:10 step:10035 [D loss: 0.703577, acc: 53.91%] [G loss: 2.060369]\n",
      "epoch:10 step:10036 [D loss: 0.653978, acc: 63.28%] [G loss: 1.925428]\n",
      "epoch:10 step:10037 [D loss: 0.747505, acc: 45.31%] [G loss: 1.955516]\n",
      "epoch:10 step:10038 [D loss: 0.654248, acc: 60.94%] [G loss: 2.029872]\n",
      "epoch:10 step:10039 [D loss: 0.613674, acc: 66.41%] [G loss: 1.985840]\n",
      "epoch:10 step:10040 [D loss: 0.678392, acc: 50.00%] [G loss: 1.979223]\n",
      "epoch:10 step:10041 [D loss: 0.618453, acc: 64.06%] [G loss: 2.017188]\n",
      "epoch:10 step:10042 [D loss: 0.641860, acc: 60.16%] [G loss: 1.809242]\n",
      "epoch:10 step:10043 [D loss: 0.671536, acc: 61.72%] [G loss: 2.163575]\n",
      "epoch:10 step:10044 [D loss: 0.642626, acc: 67.19%] [G loss: 1.936448]\n",
      "epoch:10 step:10045 [D loss: 0.622730, acc: 64.06%] [G loss: 1.860279]\n",
      "epoch:10 step:10046 [D loss: 0.643210, acc: 60.94%] [G loss: 1.804276]\n",
      "epoch:10 step:10047 [D loss: 0.601129, acc: 69.53%] [G loss: 2.146682]\n",
      "epoch:10 step:10048 [D loss: 0.626602, acc: 68.75%] [G loss: 2.028887]\n",
      "epoch:10 step:10049 [D loss: 0.661389, acc: 63.28%] [G loss: 2.013783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10050 [D loss: 0.598416, acc: 69.53%] [G loss: 2.091201]\n",
      "epoch:10 step:10051 [D loss: 0.613541, acc: 67.97%] [G loss: 1.967273]\n",
      "epoch:10 step:10052 [D loss: 0.640452, acc: 62.50%] [G loss: 1.973256]\n",
      "epoch:10 step:10053 [D loss: 0.631852, acc: 68.75%] [G loss: 2.118788]\n",
      "epoch:10 step:10054 [D loss: 0.644494, acc: 67.19%] [G loss: 2.106756]\n",
      "epoch:10 step:10055 [D loss: 0.590140, acc: 66.41%] [G loss: 2.013970]\n",
      "epoch:10 step:10056 [D loss: 0.645472, acc: 67.19%] [G loss: 2.032558]\n",
      "epoch:10 step:10057 [D loss: 0.647426, acc: 63.28%] [G loss: 2.015585]\n",
      "epoch:10 step:10058 [D loss: 0.618718, acc: 68.75%] [G loss: 1.949248]\n",
      "epoch:10 step:10059 [D loss: 0.589415, acc: 67.97%] [G loss: 2.140063]\n",
      "epoch:10 step:10060 [D loss: 0.609423, acc: 64.84%] [G loss: 2.102070]\n",
      "epoch:10 step:10061 [D loss: 0.656521, acc: 61.72%] [G loss: 2.257375]\n",
      "epoch:10 step:10062 [D loss: 0.674177, acc: 65.62%] [G loss: 2.160346]\n",
      "epoch:10 step:10063 [D loss: 0.585279, acc: 67.97%] [G loss: 2.329018]\n",
      "epoch:10 step:10064 [D loss: 0.572106, acc: 67.97%] [G loss: 2.212827]\n",
      "epoch:10 step:10065 [D loss: 0.604617, acc: 65.62%] [G loss: 2.363588]\n",
      "epoch:10 step:10066 [D loss: 0.665312, acc: 59.38%] [G loss: 1.877173]\n",
      "epoch:10 step:10067 [D loss: 0.588322, acc: 69.53%] [G loss: 2.093411]\n",
      "epoch:10 step:10068 [D loss: 0.598303, acc: 68.75%] [G loss: 1.905787]\n",
      "epoch:10 step:10069 [D loss: 0.598840, acc: 70.31%] [G loss: 2.093722]\n",
      "epoch:10 step:10070 [D loss: 0.648840, acc: 64.84%] [G loss: 1.960423]\n",
      "epoch:10 step:10071 [D loss: 0.654484, acc: 59.38%] [G loss: 2.132974]\n",
      "epoch:10 step:10072 [D loss: 0.654940, acc: 57.03%] [G loss: 2.002484]\n",
      "epoch:10 step:10073 [D loss: 0.687320, acc: 58.59%] [G loss: 2.051710]\n",
      "epoch:10 step:10074 [D loss: 0.651528, acc: 66.41%] [G loss: 1.867627]\n",
      "epoch:10 step:10075 [D loss: 0.656327, acc: 64.06%] [G loss: 1.997867]\n",
      "epoch:10 step:10076 [D loss: 0.653945, acc: 60.94%] [G loss: 2.073549]\n",
      "epoch:10 step:10077 [D loss: 0.571006, acc: 71.88%] [G loss: 2.460820]\n",
      "epoch:10 step:10078 [D loss: 0.566694, acc: 73.44%] [G loss: 2.208499]\n",
      "epoch:10 step:10079 [D loss: 0.617854, acc: 67.97%] [G loss: 2.164625]\n",
      "epoch:10 step:10080 [D loss: 0.597441, acc: 69.53%] [G loss: 1.922081]\n",
      "epoch:10 step:10081 [D loss: 0.672835, acc: 60.94%] [G loss: 2.064840]\n",
      "epoch:10 step:10082 [D loss: 0.566703, acc: 67.97%] [G loss: 2.080007]\n",
      "epoch:10 step:10083 [D loss: 0.650836, acc: 64.06%] [G loss: 1.858766]\n",
      "epoch:10 step:10084 [D loss: 0.608649, acc: 65.62%] [G loss: 1.954419]\n",
      "epoch:10 step:10085 [D loss: 0.631590, acc: 61.72%] [G loss: 2.004709]\n",
      "epoch:10 step:10086 [D loss: 0.685144, acc: 60.94%] [G loss: 1.944951]\n",
      "epoch:10 step:10087 [D loss: 0.667198, acc: 54.69%] [G loss: 1.783557]\n",
      "epoch:10 step:10088 [D loss: 0.634904, acc: 62.50%] [G loss: 1.891829]\n",
      "epoch:10 step:10089 [D loss: 0.610621, acc: 70.31%] [G loss: 2.134416]\n",
      "epoch:10 step:10090 [D loss: 0.669933, acc: 64.06%] [G loss: 1.948887]\n",
      "epoch:10 step:10091 [D loss: 0.617851, acc: 67.19%] [G loss: 2.019491]\n",
      "epoch:10 step:10092 [D loss: 0.725484, acc: 55.47%] [G loss: 2.021473]\n",
      "epoch:10 step:10093 [D loss: 0.651503, acc: 60.16%] [G loss: 1.869439]\n",
      "epoch:10 step:10094 [D loss: 0.618483, acc: 69.53%] [G loss: 1.979129]\n",
      "epoch:10 step:10095 [D loss: 0.632469, acc: 66.41%] [G loss: 2.254796]\n",
      "epoch:10 step:10096 [D loss: 0.679568, acc: 57.81%] [G loss: 2.109529]\n",
      "epoch:10 step:10097 [D loss: 0.643460, acc: 66.41%] [G loss: 1.947502]\n",
      "epoch:10 step:10098 [D loss: 0.633183, acc: 63.28%] [G loss: 2.120185]\n",
      "epoch:10 step:10099 [D loss: 0.625153, acc: 60.16%] [G loss: 2.080917]\n",
      "epoch:10 step:10100 [D loss: 0.601514, acc: 66.41%] [G loss: 2.112584]\n",
      "epoch:10 step:10101 [D loss: 0.648423, acc: 64.06%] [G loss: 2.072443]\n",
      "epoch:10 step:10102 [D loss: 0.558270, acc: 69.53%] [G loss: 2.044291]\n",
      "epoch:10 step:10103 [D loss: 0.568910, acc: 73.44%] [G loss: 2.065217]\n",
      "epoch:10 step:10104 [D loss: 0.679652, acc: 60.94%] [G loss: 1.975068]\n",
      "epoch:10 step:10105 [D loss: 0.667285, acc: 63.28%] [G loss: 1.973514]\n",
      "epoch:10 step:10106 [D loss: 0.563878, acc: 70.31%] [G loss: 2.152805]\n",
      "epoch:10 step:10107 [D loss: 0.601880, acc: 63.28%] [G loss: 1.979737]\n",
      "epoch:10 step:10108 [D loss: 0.604337, acc: 68.75%] [G loss: 1.948466]\n",
      "epoch:10 step:10109 [D loss: 0.627395, acc: 57.03%] [G loss: 2.093133]\n",
      "epoch:10 step:10110 [D loss: 0.589933, acc: 67.97%] [G loss: 2.053490]\n",
      "epoch:10 step:10111 [D loss: 0.675907, acc: 63.28%] [G loss: 2.014775]\n",
      "epoch:10 step:10112 [D loss: 0.677333, acc: 59.38%] [G loss: 2.119693]\n",
      "epoch:10 step:10113 [D loss: 0.642223, acc: 63.28%] [G loss: 2.178373]\n",
      "epoch:10 step:10114 [D loss: 0.654437, acc: 66.41%] [G loss: 1.860942]\n",
      "epoch:10 step:10115 [D loss: 0.640765, acc: 60.16%] [G loss: 1.940198]\n",
      "epoch:10 step:10116 [D loss: 0.627512, acc: 64.06%] [G loss: 2.223179]\n",
      "epoch:10 step:10117 [D loss: 0.650982, acc: 60.94%] [G loss: 2.110693]\n",
      "epoch:10 step:10118 [D loss: 0.628354, acc: 67.19%] [G loss: 2.077331]\n",
      "epoch:10 step:10119 [D loss: 0.626152, acc: 64.84%] [G loss: 1.826016]\n",
      "epoch:10 step:10120 [D loss: 0.663597, acc: 62.50%] [G loss: 2.036067]\n",
      "epoch:10 step:10121 [D loss: 0.608532, acc: 68.75%] [G loss: 2.073816]\n",
      "epoch:10 step:10122 [D loss: 0.603808, acc: 66.41%] [G loss: 2.131590]\n",
      "epoch:10 step:10123 [D loss: 0.628865, acc: 59.38%] [G loss: 2.114949]\n",
      "epoch:10 step:10124 [D loss: 0.627999, acc: 61.72%] [G loss: 2.213284]\n",
      "epoch:10 step:10125 [D loss: 0.640948, acc: 66.41%] [G loss: 2.072296]\n",
      "epoch:10 step:10126 [D loss: 0.554713, acc: 72.66%] [G loss: 2.159680]\n",
      "epoch:10 step:10127 [D loss: 0.623258, acc: 64.06%] [G loss: 1.992016]\n",
      "epoch:10 step:10128 [D loss: 0.609275, acc: 68.75%] [G loss: 2.168534]\n",
      "epoch:10 step:10129 [D loss: 0.669574, acc: 56.25%] [G loss: 1.778837]\n",
      "epoch:10 step:10130 [D loss: 0.564445, acc: 76.56%] [G loss: 2.058008]\n",
      "epoch:10 step:10131 [D loss: 0.644218, acc: 64.06%] [G loss: 2.088046]\n",
      "epoch:10 step:10132 [D loss: 0.641205, acc: 61.72%] [G loss: 1.921689]\n",
      "epoch:10 step:10133 [D loss: 0.635681, acc: 63.28%] [G loss: 2.021462]\n",
      "epoch:10 step:10134 [D loss: 0.622207, acc: 64.84%] [G loss: 2.156467]\n",
      "epoch:10 step:10135 [D loss: 0.687995, acc: 59.38%] [G loss: 1.772323]\n",
      "epoch:10 step:10136 [D loss: 0.699024, acc: 63.28%] [G loss: 1.984216]\n",
      "epoch:10 step:10137 [D loss: 0.687482, acc: 62.50%] [G loss: 2.017457]\n",
      "epoch:10 step:10138 [D loss: 0.664061, acc: 61.72%] [G loss: 1.919284]\n",
      "epoch:10 step:10139 [D loss: 0.619387, acc: 64.06%] [G loss: 1.929330]\n",
      "epoch:10 step:10140 [D loss: 0.617356, acc: 64.84%] [G loss: 2.092869]\n",
      "epoch:10 step:10141 [D loss: 0.674814, acc: 57.81%] [G loss: 1.952736]\n",
      "epoch:10 step:10142 [D loss: 0.575138, acc: 75.00%] [G loss: 1.864538]\n",
      "epoch:10 step:10143 [D loss: 0.641965, acc: 61.72%] [G loss: 2.040964]\n",
      "epoch:10 step:10144 [D loss: 0.601271, acc: 67.97%] [G loss: 2.358757]\n",
      "epoch:10 step:10145 [D loss: 0.598203, acc: 71.88%] [G loss: 2.466193]\n",
      "epoch:10 step:10146 [D loss: 0.540684, acc: 72.66%] [G loss: 2.245697]\n",
      "epoch:10 step:10147 [D loss: 0.603804, acc: 61.72%] [G loss: 2.186424]\n",
      "epoch:10 step:10148 [D loss: 0.639215, acc: 65.62%] [G loss: 2.224803]\n",
      "epoch:10 step:10149 [D loss: 0.659217, acc: 63.28%] [G loss: 2.018255]\n",
      "epoch:10 step:10150 [D loss: 0.631587, acc: 64.84%] [G loss: 2.390897]\n",
      "epoch:10 step:10151 [D loss: 0.571832, acc: 69.53%] [G loss: 2.181340]\n",
      "epoch:10 step:10152 [D loss: 0.555847, acc: 67.97%] [G loss: 2.372693]\n",
      "epoch:10 step:10153 [D loss: 0.688691, acc: 60.16%] [G loss: 2.184985]\n",
      "epoch:10 step:10154 [D loss: 0.717490, acc: 59.38%] [G loss: 2.000760]\n",
      "epoch:10 step:10155 [D loss: 0.629701, acc: 66.41%] [G loss: 1.967228]\n",
      "epoch:10 step:10156 [D loss: 0.582971, acc: 71.88%] [G loss: 2.177313]\n",
      "epoch:10 step:10157 [D loss: 0.707158, acc: 52.34%] [G loss: 1.955425]\n",
      "epoch:10 step:10158 [D loss: 0.683025, acc: 57.03%] [G loss: 1.933475]\n",
      "epoch:10 step:10159 [D loss: 0.626925, acc: 61.72%] [G loss: 2.011053]\n",
      "epoch:10 step:10160 [D loss: 0.627455, acc: 63.28%] [G loss: 2.085804]\n",
      "epoch:10 step:10161 [D loss: 0.655384, acc: 65.62%] [G loss: 1.865394]\n",
      "epoch:10 step:10162 [D loss: 0.638645, acc: 60.94%] [G loss: 2.161839]\n",
      "epoch:10 step:10163 [D loss: 0.629633, acc: 64.84%] [G loss: 2.091001]\n",
      "epoch:10 step:10164 [D loss: 0.656237, acc: 63.28%] [G loss: 2.013234]\n",
      "epoch:10 step:10165 [D loss: 0.643819, acc: 64.84%] [G loss: 2.025394]\n",
      "epoch:10 step:10166 [D loss: 0.639611, acc: 64.84%] [G loss: 1.924328]\n",
      "epoch:10 step:10167 [D loss: 0.652870, acc: 65.62%] [G loss: 1.940282]\n",
      "epoch:10 step:10168 [D loss: 0.623974, acc: 61.72%] [G loss: 2.054487]\n",
      "epoch:10 step:10169 [D loss: 0.671784, acc: 57.03%] [G loss: 2.083544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10170 [D loss: 0.675379, acc: 60.94%] [G loss: 1.880692]\n",
      "epoch:10 step:10171 [D loss: 0.668091, acc: 57.81%] [G loss: 1.822809]\n",
      "epoch:10 step:10172 [D loss: 0.653338, acc: 60.94%] [G loss: 1.945541]\n",
      "epoch:10 step:10173 [D loss: 0.651462, acc: 61.72%] [G loss: 1.996763]\n",
      "epoch:10 step:10174 [D loss: 0.617734, acc: 67.19%] [G loss: 1.948481]\n",
      "epoch:10 step:10175 [D loss: 0.589738, acc: 66.41%] [G loss: 2.012085]\n",
      "epoch:10 step:10176 [D loss: 0.643236, acc: 67.19%] [G loss: 1.979732]\n",
      "epoch:10 step:10177 [D loss: 0.603859, acc: 67.19%] [G loss: 2.202838]\n",
      "epoch:10 step:10178 [D loss: 0.653657, acc: 62.50%] [G loss: 2.217905]\n",
      "epoch:10 step:10179 [D loss: 0.611925, acc: 67.97%] [G loss: 2.070554]\n",
      "epoch:10 step:10180 [D loss: 0.639724, acc: 62.50%] [G loss: 1.954274]\n",
      "epoch:10 step:10181 [D loss: 0.603544, acc: 66.41%] [G loss: 2.008225]\n",
      "epoch:10 step:10182 [D loss: 0.721266, acc: 51.56%] [G loss: 1.909143]\n",
      "epoch:10 step:10183 [D loss: 0.678925, acc: 57.81%] [G loss: 1.903941]\n",
      "epoch:10 step:10184 [D loss: 0.646969, acc: 64.06%] [G loss: 2.075904]\n",
      "epoch:10 step:10185 [D loss: 0.559561, acc: 75.00%] [G loss: 2.357799]\n",
      "epoch:10 step:10186 [D loss: 0.548487, acc: 71.88%] [G loss: 2.218164]\n",
      "epoch:10 step:10187 [D loss: 0.590677, acc: 70.31%] [G loss: 2.089065]\n",
      "epoch:10 step:10188 [D loss: 0.677923, acc: 57.81%] [G loss: 1.807327]\n",
      "epoch:10 step:10189 [D loss: 0.636592, acc: 64.06%] [G loss: 2.068574]\n",
      "epoch:10 step:10190 [D loss: 0.715694, acc: 55.47%] [G loss: 1.945655]\n",
      "epoch:10 step:10191 [D loss: 0.602260, acc: 71.09%] [G loss: 2.213593]\n",
      "epoch:10 step:10192 [D loss: 0.631083, acc: 68.75%] [G loss: 2.130876]\n",
      "epoch:10 step:10193 [D loss: 0.612599, acc: 58.59%] [G loss: 2.167843]\n",
      "epoch:10 step:10194 [D loss: 0.668835, acc: 61.72%] [G loss: 2.003153]\n",
      "epoch:10 step:10195 [D loss: 0.602248, acc: 71.09%] [G loss: 2.158968]\n",
      "epoch:10 step:10196 [D loss: 0.637981, acc: 58.59%] [G loss: 2.006742]\n",
      "epoch:10 step:10197 [D loss: 0.668169, acc: 60.94%] [G loss: 1.841453]\n",
      "epoch:10 step:10198 [D loss: 0.676286, acc: 53.12%] [G loss: 1.851640]\n",
      "epoch:10 step:10199 [D loss: 0.682490, acc: 60.16%] [G loss: 1.958294]\n",
      "epoch:10 step:10200 [D loss: 0.577425, acc: 66.41%] [G loss: 2.034465]\n",
      "##############\n",
      "[2.41386482 1.46786898 6.52091143 4.87997267 3.9546764  5.84182812\n",
      " 4.40546222 4.79585155 4.93133914 3.52584186]\n",
      "##########\n",
      "epoch:10 step:10201 [D loss: 0.632256, acc: 61.72%] [G loss: 2.047663]\n",
      "epoch:10 step:10202 [D loss: 0.631114, acc: 64.06%] [G loss: 2.017085]\n",
      "epoch:10 step:10203 [D loss: 0.611949, acc: 66.41%] [G loss: 2.245609]\n",
      "epoch:10 step:10204 [D loss: 0.663219, acc: 61.72%] [G loss: 2.020199]\n",
      "epoch:10 step:10205 [D loss: 0.607590, acc: 67.19%] [G loss: 1.959997]\n",
      "epoch:10 step:10206 [D loss: 0.675475, acc: 60.16%] [G loss: 2.031859]\n",
      "epoch:10 step:10207 [D loss: 0.590494, acc: 67.19%] [G loss: 2.268615]\n",
      "epoch:10 step:10208 [D loss: 0.604668, acc: 67.19%] [G loss: 2.000572]\n",
      "epoch:10 step:10209 [D loss: 0.637325, acc: 60.94%] [G loss: 2.038318]\n",
      "epoch:10 step:10210 [D loss: 0.656703, acc: 64.84%] [G loss: 2.154361]\n",
      "epoch:10 step:10211 [D loss: 0.614438, acc: 68.75%] [G loss: 2.010534]\n",
      "epoch:10 step:10212 [D loss: 0.543372, acc: 75.78%] [G loss: 2.274652]\n",
      "epoch:10 step:10213 [D loss: 0.666896, acc: 62.50%] [G loss: 2.184606]\n",
      "epoch:10 step:10214 [D loss: 0.644536, acc: 62.50%] [G loss: 2.155546]\n",
      "epoch:10 step:10215 [D loss: 0.631857, acc: 65.62%] [G loss: 1.993172]\n",
      "epoch:10 step:10216 [D loss: 0.716466, acc: 62.50%] [G loss: 2.001834]\n",
      "epoch:10 step:10217 [D loss: 0.591728, acc: 68.75%] [G loss: 2.154561]\n",
      "epoch:10 step:10218 [D loss: 0.609991, acc: 66.41%] [G loss: 2.084783]\n",
      "epoch:10 step:10219 [D loss: 0.640438, acc: 63.28%] [G loss: 1.987771]\n",
      "epoch:10 step:10220 [D loss: 0.586158, acc: 67.97%] [G loss: 2.215743]\n",
      "epoch:10 step:10221 [D loss: 0.706378, acc: 56.25%] [G loss: 2.120355]\n",
      "epoch:10 step:10222 [D loss: 0.588947, acc: 64.06%] [G loss: 2.009400]\n",
      "epoch:10 step:10223 [D loss: 0.619060, acc: 60.94%] [G loss: 2.170166]\n",
      "epoch:10 step:10224 [D loss: 0.631511, acc: 62.50%] [G loss: 1.933632]\n",
      "epoch:10 step:10225 [D loss: 0.671304, acc: 54.69%] [G loss: 1.857802]\n",
      "epoch:10 step:10226 [D loss: 0.700464, acc: 56.25%] [G loss: 1.826954]\n",
      "epoch:10 step:10227 [D loss: 0.665379, acc: 60.94%] [G loss: 2.051833]\n",
      "epoch:10 step:10228 [D loss: 0.722788, acc: 57.81%] [G loss: 1.851535]\n",
      "epoch:10 step:10229 [D loss: 0.668257, acc: 61.72%] [G loss: 1.956090]\n",
      "epoch:10 step:10230 [D loss: 0.595125, acc: 71.88%] [G loss: 2.095179]\n",
      "epoch:10 step:10231 [D loss: 0.670736, acc: 63.28%] [G loss: 1.934414]\n",
      "epoch:10 step:10232 [D loss: 0.678506, acc: 63.28%] [G loss: 2.107053]\n",
      "epoch:10 step:10233 [D loss: 0.628220, acc: 60.16%] [G loss: 2.018408]\n",
      "epoch:10 step:10234 [D loss: 0.650318, acc: 60.94%] [G loss: 1.998140]\n",
      "epoch:10 step:10235 [D loss: 0.591563, acc: 71.09%] [G loss: 1.995732]\n",
      "epoch:10 step:10236 [D loss: 0.620934, acc: 64.84%] [G loss: 2.057740]\n",
      "epoch:10 step:10237 [D loss: 0.623194, acc: 67.19%] [G loss: 1.972199]\n",
      "epoch:10 step:10238 [D loss: 0.684873, acc: 62.50%] [G loss: 1.944099]\n",
      "epoch:10 step:10239 [D loss: 0.634783, acc: 62.50%] [G loss: 1.951898]\n",
      "epoch:10 step:10240 [D loss: 0.617509, acc: 63.28%] [G loss: 2.068665]\n",
      "epoch:10 step:10241 [D loss: 0.689015, acc: 58.59%] [G loss: 2.066122]\n",
      "epoch:10 step:10242 [D loss: 0.651928, acc: 70.31%] [G loss: 1.965850]\n",
      "epoch:10 step:10243 [D loss: 0.647716, acc: 63.28%] [G loss: 1.804295]\n",
      "epoch:10 step:10244 [D loss: 0.655701, acc: 61.72%] [G loss: 2.038277]\n",
      "epoch:10 step:10245 [D loss: 0.641388, acc: 64.84%] [G loss: 1.968948]\n",
      "epoch:10 step:10246 [D loss: 0.612551, acc: 70.31%] [G loss: 2.067150]\n",
      "epoch:10 step:10247 [D loss: 0.644516, acc: 64.06%] [G loss: 2.083383]\n",
      "epoch:10 step:10248 [D loss: 0.640949, acc: 64.06%] [G loss: 1.895034]\n",
      "epoch:10 step:10249 [D loss: 0.678667, acc: 55.47%] [G loss: 1.960856]\n",
      "epoch:10 step:10250 [D loss: 0.615876, acc: 69.53%] [G loss: 1.995024]\n",
      "epoch:10 step:10251 [D loss: 0.632478, acc: 67.97%] [G loss: 1.940046]\n",
      "epoch:10 step:10252 [D loss: 0.626150, acc: 67.97%] [G loss: 1.986981]\n",
      "epoch:10 step:10253 [D loss: 0.640141, acc: 67.19%] [G loss: 2.061600]\n",
      "epoch:10 step:10254 [D loss: 0.560400, acc: 69.53%] [G loss: 2.244996]\n",
      "epoch:10 step:10255 [D loss: 0.667954, acc: 59.38%] [G loss: 2.010245]\n",
      "epoch:10 step:10256 [D loss: 0.615928, acc: 67.19%] [G loss: 2.306055]\n",
      "epoch:10 step:10257 [D loss: 0.629766, acc: 64.84%] [G loss: 2.066185]\n",
      "epoch:10 step:10258 [D loss: 0.661114, acc: 64.84%] [G loss: 2.094808]\n",
      "epoch:10 step:10259 [D loss: 0.625135, acc: 63.28%] [G loss: 2.094163]\n",
      "epoch:10 step:10260 [D loss: 0.601480, acc: 67.97%] [G loss: 2.185337]\n",
      "epoch:10 step:10261 [D loss: 0.692316, acc: 57.81%] [G loss: 1.955067]\n",
      "epoch:10 step:10262 [D loss: 0.692187, acc: 57.81%] [G loss: 1.830895]\n",
      "epoch:10 step:10263 [D loss: 0.726583, acc: 55.47%] [G loss: 1.839023]\n",
      "epoch:10 step:10264 [D loss: 0.647673, acc: 68.75%] [G loss: 2.025838]\n",
      "epoch:10 step:10265 [D loss: 0.690455, acc: 56.25%] [G loss: 1.996166]\n",
      "epoch:10 step:10266 [D loss: 0.659985, acc: 60.16%] [G loss: 1.884839]\n",
      "epoch:10 step:10267 [D loss: 0.635427, acc: 66.41%] [G loss: 2.025936]\n",
      "epoch:10 step:10268 [D loss: 0.690813, acc: 57.03%] [G loss: 1.924235]\n",
      "epoch:10 step:10269 [D loss: 0.658533, acc: 59.38%] [G loss: 2.053751]\n",
      "epoch:10 step:10270 [D loss: 0.620496, acc: 63.28%] [G loss: 1.932282]\n",
      "epoch:10 step:10271 [D loss: 0.599820, acc: 71.09%] [G loss: 1.993830]\n",
      "epoch:10 step:10272 [D loss: 0.688468, acc: 59.38%] [G loss: 1.990696]\n",
      "epoch:10 step:10273 [D loss: 0.648424, acc: 60.16%] [G loss: 2.061894]\n",
      "epoch:10 step:10274 [D loss: 0.646804, acc: 60.94%] [G loss: 1.970176]\n",
      "epoch:10 step:10275 [D loss: 0.638925, acc: 62.50%] [G loss: 1.924865]\n",
      "epoch:10 step:10276 [D loss: 0.617053, acc: 62.50%] [G loss: 1.830906]\n",
      "epoch:10 step:10277 [D loss: 0.619359, acc: 67.97%] [G loss: 2.231332]\n",
      "epoch:10 step:10278 [D loss: 0.708367, acc: 57.03%] [G loss: 1.925087]\n",
      "epoch:10 step:10279 [D loss: 0.639664, acc: 61.72%] [G loss: 2.000488]\n",
      "epoch:10 step:10280 [D loss: 0.641741, acc: 60.94%] [G loss: 2.067719]\n",
      "epoch:10 step:10281 [D loss: 0.644023, acc: 63.28%] [G loss: 2.168585]\n",
      "epoch:10 step:10282 [D loss: 0.626200, acc: 65.62%] [G loss: 2.265326]\n",
      "epoch:10 step:10283 [D loss: 0.629597, acc: 61.72%] [G loss: 2.117442]\n",
      "epoch:10 step:10284 [D loss: 0.592593, acc: 68.75%] [G loss: 2.147388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10285 [D loss: 0.684049, acc: 53.12%] [G loss: 2.050488]\n",
      "epoch:10 step:10286 [D loss: 0.696515, acc: 58.59%] [G loss: 1.909590]\n",
      "epoch:10 step:10287 [D loss: 0.640276, acc: 58.59%] [G loss: 2.037801]\n",
      "epoch:10 step:10288 [D loss: 0.563157, acc: 73.44%] [G loss: 2.173973]\n",
      "epoch:10 step:10289 [D loss: 0.633370, acc: 64.06%] [G loss: 2.111009]\n",
      "epoch:10 step:10290 [D loss: 0.712851, acc: 53.12%] [G loss: 1.830163]\n",
      "epoch:10 step:10291 [D loss: 0.580605, acc: 67.97%] [G loss: 1.917099]\n",
      "epoch:10 step:10292 [D loss: 0.616929, acc: 66.41%] [G loss: 2.060783]\n",
      "epoch:10 step:10293 [D loss: 0.558842, acc: 75.78%] [G loss: 2.360923]\n",
      "epoch:10 step:10294 [D loss: 0.572094, acc: 71.88%] [G loss: 2.224856]\n",
      "epoch:10 step:10295 [D loss: 0.569003, acc: 71.09%] [G loss: 2.435092]\n",
      "epoch:10 step:10296 [D loss: 0.586643, acc: 69.53%] [G loss: 2.442326]\n",
      "epoch:10 step:10297 [D loss: 0.588840, acc: 67.97%] [G loss: 2.244579]\n",
      "epoch:10 step:10298 [D loss: 0.724410, acc: 54.69%] [G loss: 1.877191]\n",
      "epoch:10 step:10299 [D loss: 0.682631, acc: 58.59%] [G loss: 2.304229]\n",
      "epoch:10 step:10300 [D loss: 0.593355, acc: 67.97%] [G loss: 2.106634]\n",
      "epoch:10 step:10301 [D loss: 0.665788, acc: 60.94%] [G loss: 2.032815]\n",
      "epoch:10 step:10302 [D loss: 0.604588, acc: 65.62%] [G loss: 1.956234]\n",
      "epoch:10 step:10303 [D loss: 0.595360, acc: 72.66%] [G loss: 2.226765]\n",
      "epoch:10 step:10304 [D loss: 0.574485, acc: 76.56%] [G loss: 2.348242]\n",
      "epoch:10 step:10305 [D loss: 0.619384, acc: 63.28%] [G loss: 2.086698]\n",
      "epoch:10 step:10306 [D loss: 0.625130, acc: 65.62%] [G loss: 2.198697]\n",
      "epoch:10 step:10307 [D loss: 0.609677, acc: 67.19%] [G loss: 2.423297]\n",
      "epoch:11 step:10308 [D loss: 0.601724, acc: 71.88%] [G loss: 2.122306]\n",
      "epoch:11 step:10309 [D loss: 0.618008, acc: 64.06%] [G loss: 2.159379]\n",
      "epoch:11 step:10310 [D loss: 0.697244, acc: 60.16%] [G loss: 2.182498]\n",
      "epoch:11 step:10311 [D loss: 0.609663, acc: 66.41%] [G loss: 2.035766]\n",
      "epoch:11 step:10312 [D loss: 0.629391, acc: 65.62%] [G loss: 2.006157]\n",
      "epoch:11 step:10313 [D loss: 0.673739, acc: 57.81%] [G loss: 2.096325]\n",
      "epoch:11 step:10314 [D loss: 0.600437, acc: 71.88%] [G loss: 1.989885]\n",
      "epoch:11 step:10315 [D loss: 0.594752, acc: 67.97%] [G loss: 2.108804]\n",
      "epoch:11 step:10316 [D loss: 0.603844, acc: 72.66%] [G loss: 2.142702]\n",
      "epoch:11 step:10317 [D loss: 0.636520, acc: 67.97%] [G loss: 2.059164]\n",
      "epoch:11 step:10318 [D loss: 0.672559, acc: 60.16%] [G loss: 2.030720]\n",
      "epoch:11 step:10319 [D loss: 0.593001, acc: 71.09%] [G loss: 1.982179]\n",
      "epoch:11 step:10320 [D loss: 0.558658, acc: 75.00%] [G loss: 2.236821]\n",
      "epoch:11 step:10321 [D loss: 0.605992, acc: 68.75%] [G loss: 2.155943]\n",
      "epoch:11 step:10322 [D loss: 0.582491, acc: 62.50%] [G loss: 2.341213]\n",
      "epoch:11 step:10323 [D loss: 0.618726, acc: 64.84%] [G loss: 2.307684]\n",
      "epoch:11 step:10324 [D loss: 0.636877, acc: 65.62%] [G loss: 1.976800]\n",
      "epoch:11 step:10325 [D loss: 0.694579, acc: 57.03%] [G loss: 2.031315]\n",
      "epoch:11 step:10326 [D loss: 0.665056, acc: 60.16%] [G loss: 1.997080]\n",
      "epoch:11 step:10327 [D loss: 0.687095, acc: 55.47%] [G loss: 1.791430]\n",
      "epoch:11 step:10328 [D loss: 0.657490, acc: 60.16%] [G loss: 2.018490]\n",
      "epoch:11 step:10329 [D loss: 0.613182, acc: 67.19%] [G loss: 1.947113]\n",
      "epoch:11 step:10330 [D loss: 0.666878, acc: 60.94%] [G loss: 2.068818]\n",
      "epoch:11 step:10331 [D loss: 0.612054, acc: 64.06%] [G loss: 2.134029]\n",
      "epoch:11 step:10332 [D loss: 0.588033, acc: 74.22%] [G loss: 2.202027]\n",
      "epoch:11 step:10333 [D loss: 0.631464, acc: 62.50%] [G loss: 1.903309]\n",
      "epoch:11 step:10334 [D loss: 0.682098, acc: 59.38%] [G loss: 1.954850]\n",
      "epoch:11 step:10335 [D loss: 0.606061, acc: 62.50%] [G loss: 2.053328]\n",
      "epoch:11 step:10336 [D loss: 0.617497, acc: 68.75%] [G loss: 2.121494]\n",
      "epoch:11 step:10337 [D loss: 0.639589, acc: 64.84%] [G loss: 2.057271]\n",
      "epoch:11 step:10338 [D loss: 0.654140, acc: 57.03%] [G loss: 1.949581]\n",
      "epoch:11 step:10339 [D loss: 0.648705, acc: 69.53%] [G loss: 1.772014]\n",
      "epoch:11 step:10340 [D loss: 0.642836, acc: 64.06%] [G loss: 2.039517]\n",
      "epoch:11 step:10341 [D loss: 0.634225, acc: 67.19%] [G loss: 1.996526]\n",
      "epoch:11 step:10342 [D loss: 0.654780, acc: 63.28%] [G loss: 1.985501]\n",
      "epoch:11 step:10343 [D loss: 0.576715, acc: 75.00%] [G loss: 2.048941]\n",
      "epoch:11 step:10344 [D loss: 0.627707, acc: 64.06%] [G loss: 2.186132]\n",
      "epoch:11 step:10345 [D loss: 0.581462, acc: 70.31%] [G loss: 2.107022]\n",
      "epoch:11 step:10346 [D loss: 0.609929, acc: 67.19%] [G loss: 2.174448]\n",
      "epoch:11 step:10347 [D loss: 0.556627, acc: 73.44%] [G loss: 2.579243]\n",
      "epoch:11 step:10348 [D loss: 0.625397, acc: 65.62%] [G loss: 2.166744]\n",
      "epoch:11 step:10349 [D loss: 0.558217, acc: 71.88%] [G loss: 2.270390]\n",
      "epoch:11 step:10350 [D loss: 0.585433, acc: 71.09%] [G loss: 2.242698]\n",
      "epoch:11 step:10351 [D loss: 0.624288, acc: 57.81%] [G loss: 2.094136]\n",
      "epoch:11 step:10352 [D loss: 0.598230, acc: 66.41%] [G loss: 2.244711]\n",
      "epoch:11 step:10353 [D loss: 0.577510, acc: 71.09%] [G loss: 2.141816]\n",
      "epoch:11 step:10354 [D loss: 0.641319, acc: 63.28%] [G loss: 2.382517]\n",
      "epoch:11 step:10355 [D loss: 0.591036, acc: 70.31%] [G loss: 2.300909]\n",
      "epoch:11 step:10356 [D loss: 0.636461, acc: 66.41%] [G loss: 2.110155]\n",
      "epoch:11 step:10357 [D loss: 0.566941, acc: 72.66%] [G loss: 2.085030]\n",
      "epoch:11 step:10358 [D loss: 0.746902, acc: 57.81%] [G loss: 2.154452]\n",
      "epoch:11 step:10359 [D loss: 0.629628, acc: 65.62%] [G loss: 2.068148]\n",
      "epoch:11 step:10360 [D loss: 0.644314, acc: 58.59%] [G loss: 2.080505]\n",
      "epoch:11 step:10361 [D loss: 0.609958, acc: 67.19%] [G loss: 2.389031]\n",
      "epoch:11 step:10362 [D loss: 0.558136, acc: 71.88%] [G loss: 2.289134]\n",
      "epoch:11 step:10363 [D loss: 0.606634, acc: 67.97%] [G loss: 2.143049]\n",
      "epoch:11 step:10364 [D loss: 0.676056, acc: 60.16%] [G loss: 1.934945]\n",
      "epoch:11 step:10365 [D loss: 0.679160, acc: 61.72%] [G loss: 2.027553]\n",
      "epoch:11 step:10366 [D loss: 0.640398, acc: 61.72%] [G loss: 2.031552]\n",
      "epoch:11 step:10367 [D loss: 0.639220, acc: 64.06%] [G loss: 2.213819]\n",
      "epoch:11 step:10368 [D loss: 0.605289, acc: 70.31%] [G loss: 2.044068]\n",
      "epoch:11 step:10369 [D loss: 0.642966, acc: 67.19%] [G loss: 2.173676]\n",
      "epoch:11 step:10370 [D loss: 0.614759, acc: 67.19%] [G loss: 2.010677]\n",
      "epoch:11 step:10371 [D loss: 0.570671, acc: 71.09%] [G loss: 2.214429]\n",
      "epoch:11 step:10372 [D loss: 0.677413, acc: 59.38%] [G loss: 2.089008]\n",
      "epoch:11 step:10373 [D loss: 0.635315, acc: 64.06%] [G loss: 2.133674]\n",
      "epoch:11 step:10374 [D loss: 0.618307, acc: 65.62%] [G loss: 1.885611]\n",
      "epoch:11 step:10375 [D loss: 0.654979, acc: 66.41%] [G loss: 1.929196]\n",
      "epoch:11 step:10376 [D loss: 0.655949, acc: 56.25%] [G loss: 2.091382]\n",
      "epoch:11 step:10377 [D loss: 0.606927, acc: 64.06%] [G loss: 2.190324]\n",
      "epoch:11 step:10378 [D loss: 0.637807, acc: 62.50%] [G loss: 2.041315]\n",
      "epoch:11 step:10379 [D loss: 0.616722, acc: 67.97%] [G loss: 2.175048]\n",
      "epoch:11 step:10380 [D loss: 0.624821, acc: 64.06%] [G loss: 2.000632]\n",
      "epoch:11 step:10381 [D loss: 0.586538, acc: 64.06%] [G loss: 2.162136]\n",
      "epoch:11 step:10382 [D loss: 0.652541, acc: 62.50%] [G loss: 2.274881]\n",
      "epoch:11 step:10383 [D loss: 0.653585, acc: 64.06%] [G loss: 2.048698]\n",
      "epoch:11 step:10384 [D loss: 0.574981, acc: 71.09%] [G loss: 2.223490]\n",
      "epoch:11 step:10385 [D loss: 0.681228, acc: 64.84%] [G loss: 1.950587]\n",
      "epoch:11 step:10386 [D loss: 0.669398, acc: 53.91%] [G loss: 1.944712]\n",
      "epoch:11 step:10387 [D loss: 0.667274, acc: 58.59%] [G loss: 1.812141]\n",
      "epoch:11 step:10388 [D loss: 0.697018, acc: 54.69%] [G loss: 1.916785]\n",
      "epoch:11 step:10389 [D loss: 0.570905, acc: 70.31%] [G loss: 1.870476]\n",
      "epoch:11 step:10390 [D loss: 0.625822, acc: 59.38%] [G loss: 2.055872]\n",
      "epoch:11 step:10391 [D loss: 0.633721, acc: 64.84%] [G loss: 2.065838]\n",
      "epoch:11 step:10392 [D loss: 0.617562, acc: 67.97%] [G loss: 1.912777]\n",
      "epoch:11 step:10393 [D loss: 0.634688, acc: 64.06%] [G loss: 1.982061]\n",
      "epoch:11 step:10394 [D loss: 0.699802, acc: 56.25%] [G loss: 1.904105]\n",
      "epoch:11 step:10395 [D loss: 0.655048, acc: 66.41%] [G loss: 1.930638]\n",
      "epoch:11 step:10396 [D loss: 0.602709, acc: 66.41%] [G loss: 2.061187]\n",
      "epoch:11 step:10397 [D loss: 0.703674, acc: 50.00%] [G loss: 1.924563]\n",
      "epoch:11 step:10398 [D loss: 0.676675, acc: 59.38%] [G loss: 1.791502]\n",
      "epoch:11 step:10399 [D loss: 0.635055, acc: 64.84%] [G loss: 1.975040]\n",
      "epoch:11 step:10400 [D loss: 0.638050, acc: 66.41%] [G loss: 2.042447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.48074419 1.24400178 6.20879208 4.91660016 3.81795768 5.87103581\n",
      " 4.59680786 4.86476504 4.6788192  3.43308328]\n",
      "##########\n",
      "epoch:11 step:10401 [D loss: 0.589862, acc: 67.19%] [G loss: 2.012202]\n",
      "epoch:11 step:10402 [D loss: 0.630873, acc: 67.97%] [G loss: 1.930425]\n",
      "epoch:11 step:10403 [D loss: 0.609148, acc: 65.62%] [G loss: 2.121826]\n",
      "epoch:11 step:10404 [D loss: 0.576311, acc: 67.97%] [G loss: 1.959890]\n",
      "epoch:11 step:10405 [D loss: 0.622785, acc: 67.97%] [G loss: 1.970707]\n",
      "epoch:11 step:10406 [D loss: 0.624451, acc: 67.19%] [G loss: 1.995237]\n",
      "epoch:11 step:10407 [D loss: 0.540834, acc: 75.78%] [G loss: 2.102049]\n",
      "epoch:11 step:10408 [D loss: 0.621491, acc: 66.41%] [G loss: 1.969643]\n",
      "epoch:11 step:10409 [D loss: 0.656851, acc: 60.94%] [G loss: 2.032053]\n",
      "epoch:11 step:10410 [D loss: 0.583555, acc: 66.41%] [G loss: 2.157100]\n",
      "epoch:11 step:10411 [D loss: 0.641755, acc: 64.06%] [G loss: 1.954298]\n",
      "epoch:11 step:10412 [D loss: 0.702159, acc: 57.81%] [G loss: 2.051828]\n",
      "epoch:11 step:10413 [D loss: 0.611635, acc: 64.06%] [G loss: 2.184499]\n",
      "epoch:11 step:10414 [D loss: 0.662758, acc: 60.16%] [G loss: 1.961871]\n",
      "epoch:11 step:10415 [D loss: 0.614503, acc: 64.06%] [G loss: 2.093440]\n",
      "epoch:11 step:10416 [D loss: 0.622499, acc: 69.53%] [G loss: 1.920471]\n",
      "epoch:11 step:10417 [D loss: 0.650239, acc: 64.06%] [G loss: 1.822770]\n",
      "epoch:11 step:10418 [D loss: 0.662426, acc: 62.50%] [G loss: 1.988750]\n",
      "epoch:11 step:10419 [D loss: 0.655296, acc: 58.59%] [G loss: 2.001091]\n",
      "epoch:11 step:10420 [D loss: 0.601938, acc: 66.41%] [G loss: 2.056934]\n",
      "epoch:11 step:10421 [D loss: 0.651009, acc: 61.72%] [G loss: 1.957319]\n",
      "epoch:11 step:10422 [D loss: 0.606804, acc: 67.97%] [G loss: 2.060005]\n",
      "epoch:11 step:10423 [D loss: 0.664388, acc: 57.03%] [G loss: 2.021911]\n",
      "epoch:11 step:10424 [D loss: 0.671887, acc: 58.59%] [G loss: 2.168416]\n",
      "epoch:11 step:10425 [D loss: 0.623078, acc: 67.97%] [G loss: 2.101890]\n",
      "epoch:11 step:10426 [D loss: 0.551856, acc: 73.44%] [G loss: 2.165497]\n",
      "epoch:11 step:10427 [D loss: 0.701238, acc: 57.81%] [G loss: 1.930452]\n",
      "epoch:11 step:10428 [D loss: 0.613596, acc: 67.19%] [G loss: 2.071650]\n",
      "epoch:11 step:10429 [D loss: 0.582309, acc: 71.09%] [G loss: 2.243241]\n",
      "epoch:11 step:10430 [D loss: 0.633804, acc: 62.50%] [G loss: 2.175849]\n",
      "epoch:11 step:10431 [D loss: 0.642522, acc: 64.06%] [G loss: 2.121998]\n",
      "epoch:11 step:10432 [D loss: 0.631082, acc: 64.84%] [G loss: 1.855594]\n",
      "epoch:11 step:10433 [D loss: 0.586548, acc: 70.31%] [G loss: 2.075201]\n",
      "epoch:11 step:10434 [D loss: 0.688831, acc: 53.91%] [G loss: 1.945982]\n",
      "epoch:11 step:10435 [D loss: 0.629214, acc: 63.28%] [G loss: 1.966374]\n",
      "epoch:11 step:10436 [D loss: 0.639397, acc: 59.38%] [G loss: 1.870915]\n",
      "epoch:11 step:10437 [D loss: 0.646357, acc: 60.94%] [G loss: 2.047983]\n",
      "epoch:11 step:10438 [D loss: 0.601760, acc: 65.62%] [G loss: 2.150562]\n",
      "epoch:11 step:10439 [D loss: 0.591281, acc: 68.75%] [G loss: 2.070983]\n",
      "epoch:11 step:10440 [D loss: 0.640752, acc: 64.06%] [G loss: 1.996713]\n",
      "epoch:11 step:10441 [D loss: 0.649916, acc: 60.16%] [G loss: 1.919038]\n",
      "epoch:11 step:10442 [D loss: 0.646528, acc: 61.72%] [G loss: 1.984465]\n",
      "epoch:11 step:10443 [D loss: 0.643608, acc: 63.28%] [G loss: 1.938252]\n",
      "epoch:11 step:10444 [D loss: 0.653864, acc: 64.06%] [G loss: 1.918934]\n",
      "epoch:11 step:10445 [D loss: 0.647349, acc: 61.72%] [G loss: 2.035350]\n",
      "epoch:11 step:10446 [D loss: 0.558887, acc: 74.22%] [G loss: 2.221343]\n",
      "epoch:11 step:10447 [D loss: 0.656330, acc: 62.50%] [G loss: 1.920947]\n",
      "epoch:11 step:10448 [D loss: 0.630855, acc: 67.97%] [G loss: 1.933462]\n",
      "epoch:11 step:10449 [D loss: 0.614679, acc: 67.97%] [G loss: 1.974398]\n",
      "epoch:11 step:10450 [D loss: 0.619004, acc: 68.75%] [G loss: 1.965497]\n",
      "epoch:11 step:10451 [D loss: 0.612207, acc: 70.31%] [G loss: 2.044873]\n",
      "epoch:11 step:10452 [D loss: 0.597527, acc: 70.31%] [G loss: 1.972803]\n",
      "epoch:11 step:10453 [D loss: 0.636588, acc: 61.72%] [G loss: 2.185464]\n",
      "epoch:11 step:10454 [D loss: 0.647922, acc: 65.62%] [G loss: 1.988042]\n",
      "epoch:11 step:10455 [D loss: 0.616599, acc: 64.84%] [G loss: 1.875535]\n",
      "epoch:11 step:10456 [D loss: 0.610666, acc: 66.41%] [G loss: 2.046759]\n",
      "epoch:11 step:10457 [D loss: 0.659964, acc: 62.50%] [G loss: 2.092463]\n",
      "epoch:11 step:10458 [D loss: 0.585343, acc: 67.97%] [G loss: 2.070047]\n",
      "epoch:11 step:10459 [D loss: 0.625357, acc: 64.84%] [G loss: 2.133164]\n",
      "epoch:11 step:10460 [D loss: 0.627353, acc: 67.19%] [G loss: 1.902622]\n",
      "epoch:11 step:10461 [D loss: 0.576208, acc: 67.19%] [G loss: 2.087293]\n",
      "epoch:11 step:10462 [D loss: 0.611730, acc: 66.41%] [G loss: 2.316794]\n",
      "epoch:11 step:10463 [D loss: 0.581411, acc: 71.09%] [G loss: 2.114786]\n",
      "epoch:11 step:10464 [D loss: 0.658130, acc: 60.16%] [G loss: 1.927460]\n",
      "epoch:11 step:10465 [D loss: 0.674986, acc: 56.25%] [G loss: 2.045281]\n",
      "epoch:11 step:10466 [D loss: 0.602019, acc: 67.19%] [G loss: 1.996066]\n",
      "epoch:11 step:10467 [D loss: 0.696387, acc: 54.69%] [G loss: 1.869747]\n",
      "epoch:11 step:10468 [D loss: 0.552147, acc: 72.66%] [G loss: 2.177896]\n",
      "epoch:11 step:10469 [D loss: 0.638273, acc: 67.97%] [G loss: 2.080273]\n",
      "epoch:11 step:10470 [D loss: 0.630143, acc: 67.97%] [G loss: 2.133603]\n",
      "epoch:11 step:10471 [D loss: 0.635506, acc: 64.06%] [G loss: 1.800688]\n",
      "epoch:11 step:10472 [D loss: 0.602097, acc: 71.09%] [G loss: 2.111761]\n",
      "epoch:11 step:10473 [D loss: 0.634758, acc: 66.41%] [G loss: 2.083978]\n",
      "epoch:11 step:10474 [D loss: 0.696721, acc: 57.81%] [G loss: 2.035727]\n",
      "epoch:11 step:10475 [D loss: 0.599726, acc: 69.53%] [G loss: 2.070241]\n",
      "epoch:11 step:10476 [D loss: 0.647283, acc: 65.62%] [G loss: 1.910372]\n",
      "epoch:11 step:10477 [D loss: 0.652241, acc: 63.28%] [G loss: 1.776827]\n",
      "epoch:11 step:10478 [D loss: 0.632120, acc: 62.50%] [G loss: 2.150175]\n",
      "epoch:11 step:10479 [D loss: 0.701146, acc: 51.56%] [G loss: 2.119499]\n",
      "epoch:11 step:10480 [D loss: 0.629515, acc: 68.75%] [G loss: 1.958748]\n",
      "epoch:11 step:10481 [D loss: 0.680020, acc: 56.25%] [G loss: 1.892943]\n",
      "epoch:11 step:10482 [D loss: 0.658607, acc: 64.06%] [G loss: 1.948004]\n",
      "epoch:11 step:10483 [D loss: 0.670935, acc: 58.59%] [G loss: 1.904899]\n",
      "epoch:11 step:10484 [D loss: 0.676077, acc: 57.03%] [G loss: 1.940086]\n",
      "epoch:11 step:10485 [D loss: 0.657366, acc: 59.38%] [G loss: 2.061857]\n",
      "epoch:11 step:10486 [D loss: 0.644944, acc: 63.28%] [G loss: 1.900910]\n",
      "epoch:11 step:10487 [D loss: 0.658443, acc: 58.59%] [G loss: 1.937071]\n",
      "epoch:11 step:10488 [D loss: 0.645339, acc: 66.41%] [G loss: 1.876338]\n",
      "epoch:11 step:10489 [D loss: 0.666077, acc: 60.94%] [G loss: 1.914926]\n",
      "epoch:11 step:10490 [D loss: 0.715141, acc: 57.03%] [G loss: 2.007851]\n",
      "epoch:11 step:10491 [D loss: 0.654104, acc: 61.72%] [G loss: 2.024261]\n",
      "epoch:11 step:10492 [D loss: 0.624393, acc: 69.53%] [G loss: 1.996796]\n",
      "epoch:11 step:10493 [D loss: 0.631048, acc: 68.75%] [G loss: 1.920335]\n",
      "epoch:11 step:10494 [D loss: 0.645408, acc: 63.28%] [G loss: 1.920339]\n",
      "epoch:11 step:10495 [D loss: 0.666767, acc: 60.16%] [G loss: 1.951397]\n",
      "epoch:11 step:10496 [D loss: 0.623311, acc: 64.06%] [G loss: 1.856758]\n",
      "epoch:11 step:10497 [D loss: 0.636782, acc: 64.06%] [G loss: 2.017536]\n",
      "epoch:11 step:10498 [D loss: 0.589162, acc: 67.97%] [G loss: 2.039509]\n",
      "epoch:11 step:10499 [D loss: 0.643995, acc: 64.84%] [G loss: 2.249121]\n",
      "epoch:11 step:10500 [D loss: 0.631196, acc: 62.50%] [G loss: 2.093083]\n",
      "epoch:11 step:10501 [D loss: 0.568900, acc: 68.75%] [G loss: 2.097861]\n",
      "epoch:11 step:10502 [D loss: 0.665099, acc: 63.28%] [G loss: 1.961927]\n",
      "epoch:11 step:10503 [D loss: 0.651431, acc: 60.94%] [G loss: 1.904922]\n",
      "epoch:11 step:10504 [D loss: 0.626751, acc: 68.75%] [G loss: 2.069580]\n",
      "epoch:11 step:10505 [D loss: 0.587266, acc: 72.66%] [G loss: 2.051846]\n",
      "epoch:11 step:10506 [D loss: 0.614575, acc: 69.53%] [G loss: 2.095021]\n",
      "epoch:11 step:10507 [D loss: 0.746263, acc: 51.56%] [G loss: 1.864628]\n",
      "epoch:11 step:10508 [D loss: 0.637565, acc: 64.84%] [G loss: 1.903704]\n",
      "epoch:11 step:10509 [D loss: 0.677986, acc: 62.50%] [G loss: 2.090709]\n",
      "epoch:11 step:10510 [D loss: 0.614185, acc: 67.19%] [G loss: 2.050913]\n",
      "epoch:11 step:10511 [D loss: 0.640919, acc: 60.94%] [G loss: 1.913359]\n",
      "epoch:11 step:10512 [D loss: 0.666895, acc: 60.94%] [G loss: 1.768827]\n",
      "epoch:11 step:10513 [D loss: 0.575678, acc: 72.66%] [G loss: 2.159014]\n",
      "epoch:11 step:10514 [D loss: 0.589411, acc: 70.31%] [G loss: 2.226713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10515 [D loss: 0.578845, acc: 71.88%] [G loss: 2.407062]\n",
      "epoch:11 step:10516 [D loss: 0.548605, acc: 72.66%] [G loss: 2.384274]\n",
      "epoch:11 step:10517 [D loss: 0.643270, acc: 63.28%] [G loss: 1.950860]\n",
      "epoch:11 step:10518 [D loss: 0.696275, acc: 55.47%] [G loss: 1.908893]\n",
      "epoch:11 step:10519 [D loss: 0.621554, acc: 67.97%] [G loss: 1.956612]\n",
      "epoch:11 step:10520 [D loss: 0.692515, acc: 55.47%] [G loss: 1.850819]\n",
      "epoch:11 step:10521 [D loss: 0.653024, acc: 61.72%] [G loss: 1.930134]\n",
      "epoch:11 step:10522 [D loss: 0.656826, acc: 64.06%] [G loss: 1.970630]\n",
      "epoch:11 step:10523 [D loss: 0.589768, acc: 72.66%] [G loss: 2.151839]\n",
      "epoch:11 step:10524 [D loss: 0.647964, acc: 60.16%] [G loss: 1.980589]\n",
      "epoch:11 step:10525 [D loss: 0.581757, acc: 70.31%] [G loss: 2.059959]\n",
      "epoch:11 step:10526 [D loss: 0.596058, acc: 68.75%] [G loss: 2.354263]\n",
      "epoch:11 step:10527 [D loss: 0.713859, acc: 53.12%] [G loss: 1.854024]\n",
      "epoch:11 step:10528 [D loss: 0.662703, acc: 60.94%] [G loss: 2.055605]\n",
      "epoch:11 step:10529 [D loss: 0.626263, acc: 64.06%] [G loss: 1.977730]\n",
      "epoch:11 step:10530 [D loss: 0.624774, acc: 62.50%] [G loss: 2.163821]\n",
      "epoch:11 step:10531 [D loss: 0.637474, acc: 57.81%] [G loss: 2.202920]\n",
      "epoch:11 step:10532 [D loss: 0.632468, acc: 64.06%] [G loss: 1.962498]\n",
      "epoch:11 step:10533 [D loss: 0.705633, acc: 57.81%] [G loss: 1.955401]\n",
      "epoch:11 step:10534 [D loss: 0.610299, acc: 67.19%] [G loss: 1.983778]\n",
      "epoch:11 step:10535 [D loss: 0.622965, acc: 64.06%] [G loss: 1.862997]\n",
      "epoch:11 step:10536 [D loss: 0.602035, acc: 70.31%] [G loss: 2.262826]\n",
      "epoch:11 step:10537 [D loss: 0.571754, acc: 71.88%] [G loss: 2.411078]\n",
      "epoch:11 step:10538 [D loss: 0.538550, acc: 71.09%] [G loss: 2.319180]\n",
      "epoch:11 step:10539 [D loss: 0.605177, acc: 65.62%] [G loss: 2.422703]\n",
      "epoch:11 step:10540 [D loss: 0.655074, acc: 62.50%] [G loss: 1.964905]\n",
      "epoch:11 step:10541 [D loss: 0.640205, acc: 60.94%] [G loss: 2.081232]\n",
      "epoch:11 step:10542 [D loss: 0.647817, acc: 63.28%] [G loss: 1.973148]\n",
      "epoch:11 step:10543 [D loss: 0.643572, acc: 66.41%] [G loss: 2.090785]\n",
      "epoch:11 step:10544 [D loss: 0.698745, acc: 56.25%] [G loss: 1.998810]\n",
      "epoch:11 step:10545 [D loss: 0.570736, acc: 72.66%] [G loss: 2.017780]\n",
      "epoch:11 step:10546 [D loss: 0.646578, acc: 63.28%] [G loss: 1.998934]\n",
      "epoch:11 step:10547 [D loss: 0.608842, acc: 64.84%] [G loss: 2.191987]\n",
      "epoch:11 step:10548 [D loss: 0.611012, acc: 66.41%] [G loss: 1.989620]\n",
      "epoch:11 step:10549 [D loss: 0.608696, acc: 65.62%] [G loss: 2.099821]\n",
      "epoch:11 step:10550 [D loss: 0.614595, acc: 66.41%] [G loss: 2.196350]\n",
      "epoch:11 step:10551 [D loss: 0.669615, acc: 61.72%] [G loss: 2.309623]\n",
      "epoch:11 step:10552 [D loss: 0.606560, acc: 67.97%] [G loss: 2.239230]\n",
      "epoch:11 step:10553 [D loss: 0.671121, acc: 57.03%] [G loss: 2.053288]\n",
      "epoch:11 step:10554 [D loss: 0.643733, acc: 60.94%] [G loss: 2.254506]\n",
      "epoch:11 step:10555 [D loss: 0.583764, acc: 71.88%] [G loss: 2.234200]\n",
      "epoch:11 step:10556 [D loss: 0.668833, acc: 62.50%] [G loss: 1.898005]\n",
      "epoch:11 step:10557 [D loss: 0.684092, acc: 57.03%] [G loss: 1.846826]\n",
      "epoch:11 step:10558 [D loss: 0.649303, acc: 64.84%] [G loss: 1.904674]\n",
      "epoch:11 step:10559 [D loss: 0.699806, acc: 53.91%] [G loss: 1.779031]\n",
      "epoch:11 step:10560 [D loss: 0.600687, acc: 61.72%] [G loss: 1.950931]\n",
      "epoch:11 step:10561 [D loss: 0.665676, acc: 58.59%] [G loss: 1.866871]\n",
      "epoch:11 step:10562 [D loss: 0.630556, acc: 60.94%] [G loss: 2.016263]\n",
      "epoch:11 step:10563 [D loss: 0.656979, acc: 61.72%] [G loss: 2.026130]\n",
      "epoch:11 step:10564 [D loss: 0.627164, acc: 60.94%] [G loss: 1.910861]\n",
      "epoch:11 step:10565 [D loss: 0.622002, acc: 64.84%] [G loss: 2.040949]\n",
      "epoch:11 step:10566 [D loss: 0.560864, acc: 67.19%] [G loss: 2.081229]\n",
      "epoch:11 step:10567 [D loss: 0.637776, acc: 64.84%] [G loss: 2.036730]\n",
      "epoch:11 step:10568 [D loss: 0.646912, acc: 73.44%] [G loss: 2.204548]\n",
      "epoch:11 step:10569 [D loss: 0.553514, acc: 71.09%] [G loss: 2.473613]\n",
      "epoch:11 step:10570 [D loss: 0.680594, acc: 62.50%] [G loss: 2.200844]\n",
      "epoch:11 step:10571 [D loss: 0.587355, acc: 72.66%] [G loss: 2.254495]\n",
      "epoch:11 step:10572 [D loss: 0.638271, acc: 64.84%] [G loss: 2.042752]\n",
      "epoch:11 step:10573 [D loss: 0.625655, acc: 62.50%] [G loss: 2.067620]\n",
      "epoch:11 step:10574 [D loss: 0.603865, acc: 64.06%] [G loss: 1.883642]\n",
      "epoch:11 step:10575 [D loss: 0.591732, acc: 71.09%] [G loss: 2.026734]\n",
      "epoch:11 step:10576 [D loss: 0.650545, acc: 62.50%] [G loss: 2.168277]\n",
      "epoch:11 step:10577 [D loss: 0.608119, acc: 69.53%] [G loss: 2.170028]\n",
      "epoch:11 step:10578 [D loss: 0.676718, acc: 60.16%] [G loss: 2.222063]\n",
      "epoch:11 step:10579 [D loss: 0.594504, acc: 67.97%] [G loss: 2.041280]\n",
      "epoch:11 step:10580 [D loss: 0.624707, acc: 65.62%] [G loss: 2.195891]\n",
      "epoch:11 step:10581 [D loss: 0.620709, acc: 65.62%] [G loss: 2.117660]\n",
      "epoch:11 step:10582 [D loss: 0.590329, acc: 65.62%] [G loss: 2.092804]\n",
      "epoch:11 step:10583 [D loss: 0.641203, acc: 62.50%] [G loss: 2.096687]\n",
      "epoch:11 step:10584 [D loss: 0.609431, acc: 63.28%] [G loss: 2.128314]\n",
      "epoch:11 step:10585 [D loss: 0.640216, acc: 64.84%] [G loss: 1.991552]\n",
      "epoch:11 step:10586 [D loss: 0.662636, acc: 59.38%] [G loss: 2.097924]\n",
      "epoch:11 step:10587 [D loss: 0.598952, acc: 70.31%] [G loss: 2.067070]\n",
      "epoch:11 step:10588 [D loss: 0.672855, acc: 56.25%] [G loss: 1.871317]\n",
      "epoch:11 step:10589 [D loss: 0.579400, acc: 70.31%] [G loss: 1.983001]\n",
      "epoch:11 step:10590 [D loss: 0.587841, acc: 71.88%] [G loss: 2.014677]\n",
      "epoch:11 step:10591 [D loss: 0.661863, acc: 58.59%] [G loss: 2.243200]\n",
      "epoch:11 step:10592 [D loss: 0.612458, acc: 67.19%] [G loss: 2.158408]\n",
      "epoch:11 step:10593 [D loss: 0.643615, acc: 66.41%] [G loss: 2.209097]\n",
      "epoch:11 step:10594 [D loss: 0.636067, acc: 64.06%] [G loss: 2.109534]\n",
      "epoch:11 step:10595 [D loss: 0.657189, acc: 59.38%] [G loss: 1.912773]\n",
      "epoch:11 step:10596 [D loss: 0.637367, acc: 64.84%] [G loss: 1.970565]\n",
      "epoch:11 step:10597 [D loss: 0.705064, acc: 58.59%] [G loss: 1.881411]\n",
      "epoch:11 step:10598 [D loss: 0.657890, acc: 64.06%] [G loss: 1.936900]\n",
      "epoch:11 step:10599 [D loss: 0.678255, acc: 60.94%] [G loss: 1.818148]\n",
      "epoch:11 step:10600 [D loss: 0.563244, acc: 74.22%] [G loss: 2.161995]\n",
      "##############\n",
      "[2.47359131 1.28790127 6.52679979 4.82524579 3.74711038 5.57874053\n",
      " 4.57445515 4.7379303  4.83793139 3.49841068]\n",
      "##########\n",
      "epoch:11 step:10601 [D loss: 0.648484, acc: 64.06%] [G loss: 2.069019]\n",
      "epoch:11 step:10602 [D loss: 0.619982, acc: 64.06%] [G loss: 2.036770]\n",
      "epoch:11 step:10603 [D loss: 0.684837, acc: 56.25%] [G loss: 2.004082]\n",
      "epoch:11 step:10604 [D loss: 0.714697, acc: 55.47%] [G loss: 1.859277]\n",
      "epoch:11 step:10605 [D loss: 0.562888, acc: 71.88%] [G loss: 1.956841]\n",
      "epoch:11 step:10606 [D loss: 0.623707, acc: 60.16%] [G loss: 2.276684]\n",
      "epoch:11 step:10607 [D loss: 0.543310, acc: 75.00%] [G loss: 2.127217]\n",
      "epoch:11 step:10608 [D loss: 0.685625, acc: 60.16%] [G loss: 1.875404]\n",
      "epoch:11 step:10609 [D loss: 0.619059, acc: 65.62%] [G loss: 2.129804]\n",
      "epoch:11 step:10610 [D loss: 0.625469, acc: 66.41%] [G loss: 1.944551]\n",
      "epoch:11 step:10611 [D loss: 0.689611, acc: 54.69%] [G loss: 1.897243]\n",
      "epoch:11 step:10612 [D loss: 0.642196, acc: 62.50%] [G loss: 2.124290]\n",
      "epoch:11 step:10613 [D loss: 0.639649, acc: 67.19%] [G loss: 1.896305]\n",
      "epoch:11 step:10614 [D loss: 0.576836, acc: 75.00%] [G loss: 2.046338]\n",
      "epoch:11 step:10615 [D loss: 0.659426, acc: 62.50%] [G loss: 2.001934]\n",
      "epoch:11 step:10616 [D loss: 0.552821, acc: 71.09%] [G loss: 2.206010]\n",
      "epoch:11 step:10617 [D loss: 0.628320, acc: 64.06%] [G loss: 2.164759]\n",
      "epoch:11 step:10618 [D loss: 0.600451, acc: 67.97%] [G loss: 2.060584]\n",
      "epoch:11 step:10619 [D loss: 0.524067, acc: 72.66%] [G loss: 2.532612]\n",
      "epoch:11 step:10620 [D loss: 0.520661, acc: 76.56%] [G loss: 2.427966]\n",
      "epoch:11 step:10621 [D loss: 0.521211, acc: 75.78%] [G loss: 2.497468]\n",
      "epoch:11 step:10622 [D loss: 0.615384, acc: 71.09%] [G loss: 2.545143]\n",
      "epoch:11 step:10623 [D loss: 0.700463, acc: 57.81%] [G loss: 1.791955]\n",
      "epoch:11 step:10624 [D loss: 0.712233, acc: 50.78%] [G loss: 1.970053]\n",
      "epoch:11 step:10625 [D loss: 0.670639, acc: 58.59%] [G loss: 2.129331]\n",
      "epoch:11 step:10626 [D loss: 0.646790, acc: 62.50%] [G loss: 2.059466]\n",
      "epoch:11 step:10627 [D loss: 0.639335, acc: 63.28%] [G loss: 2.088130]\n",
      "epoch:11 step:10628 [D loss: 0.598360, acc: 64.06%] [G loss: 2.237039]\n",
      "epoch:11 step:10629 [D loss: 0.602354, acc: 67.19%] [G loss: 1.979799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10630 [D loss: 0.688819, acc: 62.50%] [G loss: 2.006162]\n",
      "epoch:11 step:10631 [D loss: 0.635474, acc: 63.28%] [G loss: 2.075881]\n",
      "epoch:11 step:10632 [D loss: 0.609055, acc: 64.06%] [G loss: 2.203802]\n",
      "epoch:11 step:10633 [D loss: 0.657320, acc: 64.84%] [G loss: 1.933465]\n",
      "epoch:11 step:10634 [D loss: 0.579118, acc: 67.19%] [G loss: 2.090604]\n",
      "epoch:11 step:10635 [D loss: 0.649941, acc: 65.62%] [G loss: 2.254614]\n",
      "epoch:11 step:10636 [D loss: 0.594005, acc: 65.62%] [G loss: 2.084645]\n",
      "epoch:11 step:10637 [D loss: 0.627966, acc: 61.72%] [G loss: 2.246669]\n",
      "epoch:11 step:10638 [D loss: 0.603345, acc: 68.75%] [G loss: 2.173718]\n",
      "epoch:11 step:10639 [D loss: 0.609374, acc: 68.75%] [G loss: 2.129334]\n",
      "epoch:11 step:10640 [D loss: 0.603305, acc: 67.19%] [G loss: 2.205493]\n",
      "epoch:11 step:10641 [D loss: 0.693622, acc: 60.94%] [G loss: 2.186999]\n",
      "epoch:11 step:10642 [D loss: 0.611377, acc: 64.84%] [G loss: 2.027832]\n",
      "epoch:11 step:10643 [D loss: 0.623431, acc: 61.72%] [G loss: 1.975337]\n",
      "epoch:11 step:10644 [D loss: 0.588709, acc: 66.41%] [G loss: 1.998934]\n",
      "epoch:11 step:10645 [D loss: 0.573276, acc: 71.09%] [G loss: 2.295717]\n",
      "epoch:11 step:10646 [D loss: 0.570239, acc: 73.44%] [G loss: 2.117912]\n",
      "epoch:11 step:10647 [D loss: 0.688406, acc: 55.47%] [G loss: 2.121749]\n",
      "epoch:11 step:10648 [D loss: 0.637669, acc: 61.72%] [G loss: 2.159566]\n",
      "epoch:11 step:10649 [D loss: 0.666264, acc: 53.91%] [G loss: 2.082843]\n",
      "epoch:11 step:10650 [D loss: 0.613957, acc: 69.53%] [G loss: 2.125669]\n",
      "epoch:11 step:10651 [D loss: 0.607612, acc: 62.50%] [G loss: 2.246185]\n",
      "epoch:11 step:10652 [D loss: 0.620525, acc: 61.72%] [G loss: 2.310837]\n",
      "epoch:11 step:10653 [D loss: 0.554601, acc: 68.75%] [G loss: 2.449475]\n",
      "epoch:11 step:10654 [D loss: 0.552167, acc: 74.22%] [G loss: 2.241209]\n",
      "epoch:11 step:10655 [D loss: 0.699771, acc: 60.94%] [G loss: 1.960814]\n",
      "epoch:11 step:10656 [D loss: 0.765849, acc: 53.12%] [G loss: 1.870444]\n",
      "epoch:11 step:10657 [D loss: 0.669665, acc: 57.03%] [G loss: 2.004011]\n",
      "epoch:11 step:10658 [D loss: 0.640350, acc: 64.06%] [G loss: 1.906194]\n",
      "epoch:11 step:10659 [D loss: 0.696191, acc: 58.59%] [G loss: 1.999509]\n",
      "epoch:11 step:10660 [D loss: 0.672544, acc: 60.94%] [G loss: 2.099534]\n",
      "epoch:11 step:10661 [D loss: 0.615713, acc: 60.16%] [G loss: 2.290284]\n",
      "epoch:11 step:10662 [D loss: 0.656134, acc: 62.50%] [G loss: 2.058121]\n",
      "epoch:11 step:10663 [D loss: 0.654553, acc: 62.50%] [G loss: 1.938620]\n",
      "epoch:11 step:10664 [D loss: 0.652860, acc: 61.72%] [G loss: 2.383302]\n",
      "epoch:11 step:10665 [D loss: 0.581026, acc: 66.41%] [G loss: 2.072674]\n",
      "epoch:11 step:10666 [D loss: 0.655149, acc: 62.50%] [G loss: 2.249454]\n",
      "epoch:11 step:10667 [D loss: 0.640166, acc: 63.28%] [G loss: 2.222700]\n",
      "epoch:11 step:10668 [D loss: 0.603665, acc: 66.41%] [G loss: 1.994489]\n",
      "epoch:11 step:10669 [D loss: 0.625185, acc: 66.41%] [G loss: 2.118373]\n",
      "epoch:11 step:10670 [D loss: 0.660159, acc: 62.50%] [G loss: 1.968394]\n",
      "epoch:11 step:10671 [D loss: 0.631892, acc: 61.72%] [G loss: 1.957221]\n",
      "epoch:11 step:10672 [D loss: 0.643133, acc: 61.72%] [G loss: 2.007288]\n",
      "epoch:11 step:10673 [D loss: 0.660067, acc: 57.03%] [G loss: 2.021947]\n",
      "epoch:11 step:10674 [D loss: 0.647237, acc: 59.38%] [G loss: 2.062011]\n",
      "epoch:11 step:10675 [D loss: 0.686845, acc: 57.81%] [G loss: 1.943512]\n",
      "epoch:11 step:10676 [D loss: 0.608257, acc: 67.97%] [G loss: 2.092014]\n",
      "epoch:11 step:10677 [D loss: 0.614173, acc: 67.97%] [G loss: 2.207016]\n",
      "epoch:11 step:10678 [D loss: 0.628327, acc: 64.06%] [G loss: 2.182187]\n",
      "epoch:11 step:10679 [D loss: 0.631438, acc: 64.84%] [G loss: 2.086509]\n",
      "epoch:11 step:10680 [D loss: 0.721693, acc: 57.81%] [G loss: 1.812722]\n",
      "epoch:11 step:10681 [D loss: 0.631140, acc: 60.16%] [G loss: 1.936349]\n",
      "epoch:11 step:10682 [D loss: 0.641444, acc: 59.38%] [G loss: 1.755814]\n",
      "epoch:11 step:10683 [D loss: 0.696923, acc: 62.50%] [G loss: 1.954022]\n",
      "epoch:11 step:10684 [D loss: 0.719408, acc: 58.59%] [G loss: 1.860264]\n",
      "epoch:11 step:10685 [D loss: 0.632597, acc: 67.19%] [G loss: 1.944605]\n",
      "epoch:11 step:10686 [D loss: 0.650894, acc: 65.62%] [G loss: 2.237405]\n",
      "epoch:11 step:10687 [D loss: 0.631789, acc: 63.28%] [G loss: 2.065277]\n",
      "epoch:11 step:10688 [D loss: 0.585990, acc: 73.44%] [G loss: 2.071622]\n",
      "epoch:11 step:10689 [D loss: 0.651090, acc: 61.72%] [G loss: 2.000983]\n",
      "epoch:11 step:10690 [D loss: 0.616371, acc: 62.50%] [G loss: 2.011746]\n",
      "epoch:11 step:10691 [D loss: 0.623713, acc: 64.06%] [G loss: 1.940595]\n",
      "epoch:11 step:10692 [D loss: 0.638582, acc: 60.16%] [G loss: 1.988477]\n",
      "epoch:11 step:10693 [D loss: 0.645126, acc: 62.50%] [G loss: 1.897523]\n",
      "epoch:11 step:10694 [D loss: 0.624338, acc: 64.06%] [G loss: 1.987567]\n",
      "epoch:11 step:10695 [D loss: 0.663745, acc: 62.50%] [G loss: 1.947054]\n",
      "epoch:11 step:10696 [D loss: 0.642881, acc: 62.50%] [G loss: 1.954451]\n",
      "epoch:11 step:10697 [D loss: 0.627705, acc: 61.72%] [G loss: 1.876578]\n",
      "epoch:11 step:10698 [D loss: 0.680317, acc: 62.50%] [G loss: 1.877946]\n",
      "epoch:11 step:10699 [D loss: 0.576613, acc: 72.66%] [G loss: 1.977172]\n",
      "epoch:11 step:10700 [D loss: 0.643160, acc: 61.72%] [G loss: 1.936792]\n",
      "epoch:11 step:10701 [D loss: 0.663105, acc: 60.16%] [G loss: 2.101202]\n",
      "epoch:11 step:10702 [D loss: 0.598214, acc: 64.06%] [G loss: 1.982688]\n",
      "epoch:11 step:10703 [D loss: 0.699344, acc: 54.69%] [G loss: 1.882662]\n",
      "epoch:11 step:10704 [D loss: 0.645375, acc: 60.94%] [G loss: 1.809465]\n",
      "epoch:11 step:10705 [D loss: 0.592809, acc: 67.19%] [G loss: 1.940362]\n",
      "epoch:11 step:10706 [D loss: 0.617118, acc: 69.53%] [G loss: 1.921025]\n",
      "epoch:11 step:10707 [D loss: 0.662987, acc: 61.72%] [G loss: 1.859737]\n",
      "epoch:11 step:10708 [D loss: 0.648047, acc: 61.72%] [G loss: 1.937208]\n",
      "epoch:11 step:10709 [D loss: 0.583731, acc: 71.09%] [G loss: 2.091555]\n",
      "epoch:11 step:10710 [D loss: 0.565777, acc: 69.53%] [G loss: 1.947527]\n",
      "epoch:11 step:10711 [D loss: 0.611089, acc: 64.84%] [G loss: 2.001425]\n",
      "epoch:11 step:10712 [D loss: 0.648019, acc: 67.19%] [G loss: 2.057287]\n",
      "epoch:11 step:10713 [D loss: 0.598797, acc: 67.97%] [G loss: 2.309973]\n",
      "epoch:11 step:10714 [D loss: 0.597272, acc: 69.53%] [G loss: 2.037751]\n",
      "epoch:11 step:10715 [D loss: 0.657611, acc: 57.81%] [G loss: 1.907176]\n",
      "epoch:11 step:10716 [D loss: 0.653001, acc: 66.41%] [G loss: 2.229179]\n",
      "epoch:11 step:10717 [D loss: 0.677398, acc: 59.38%] [G loss: 2.076069]\n",
      "epoch:11 step:10718 [D loss: 0.702252, acc: 58.59%] [G loss: 2.116324]\n",
      "epoch:11 step:10719 [D loss: 0.635503, acc: 66.41%] [G loss: 2.061168]\n",
      "epoch:11 step:10720 [D loss: 0.616740, acc: 63.28%] [G loss: 2.243920]\n",
      "epoch:11 step:10721 [D loss: 0.600555, acc: 70.31%] [G loss: 2.055314]\n",
      "epoch:11 step:10722 [D loss: 0.554059, acc: 74.22%] [G loss: 2.154575]\n",
      "epoch:11 step:10723 [D loss: 0.673447, acc: 65.62%] [G loss: 2.052589]\n",
      "epoch:11 step:10724 [D loss: 0.699969, acc: 60.94%] [G loss: 1.943819]\n",
      "epoch:11 step:10725 [D loss: 0.678902, acc: 57.81%] [G loss: 2.031381]\n",
      "epoch:11 step:10726 [D loss: 0.734745, acc: 54.69%] [G loss: 1.883514]\n",
      "epoch:11 step:10727 [D loss: 0.607828, acc: 67.97%] [G loss: 1.930025]\n",
      "epoch:11 step:10728 [D loss: 0.653395, acc: 64.06%] [G loss: 2.000793]\n",
      "epoch:11 step:10729 [D loss: 0.641667, acc: 62.50%] [G loss: 1.958063]\n",
      "epoch:11 step:10730 [D loss: 0.669696, acc: 59.38%] [G loss: 2.008841]\n",
      "epoch:11 step:10731 [D loss: 0.639992, acc: 65.62%] [G loss: 2.136087]\n",
      "epoch:11 step:10732 [D loss: 0.633709, acc: 62.50%] [G loss: 1.821487]\n",
      "epoch:11 step:10733 [D loss: 0.671575, acc: 63.28%] [G loss: 1.890883]\n",
      "epoch:11 step:10734 [D loss: 0.654486, acc: 63.28%] [G loss: 2.148881]\n",
      "epoch:11 step:10735 [D loss: 0.590081, acc: 67.19%] [G loss: 2.056159]\n",
      "epoch:11 step:10736 [D loss: 0.584207, acc: 68.75%] [G loss: 2.249541]\n",
      "epoch:11 step:10737 [D loss: 0.541181, acc: 77.34%] [G loss: 2.383050]\n",
      "epoch:11 step:10738 [D loss: 0.566667, acc: 66.41%] [G loss: 2.189413]\n",
      "epoch:11 step:10739 [D loss: 0.646368, acc: 60.94%] [G loss: 1.947156]\n",
      "epoch:11 step:10740 [D loss: 0.617097, acc: 65.62%] [G loss: 1.933672]\n",
      "epoch:11 step:10741 [D loss: 0.632354, acc: 63.28%] [G loss: 2.047283]\n",
      "epoch:11 step:10742 [D loss: 0.676233, acc: 65.62%] [G loss: 2.009051]\n",
      "epoch:11 step:10743 [D loss: 0.606536, acc: 66.41%] [G loss: 2.002485]\n",
      "epoch:11 step:10744 [D loss: 0.627721, acc: 61.72%] [G loss: 1.882254]\n",
      "epoch:11 step:10745 [D loss: 0.645499, acc: 60.16%] [G loss: 1.883358]\n",
      "epoch:11 step:10746 [D loss: 0.618139, acc: 65.62%] [G loss: 2.158120]\n",
      "epoch:11 step:10747 [D loss: 0.653109, acc: 61.72%] [G loss: 2.095409]\n",
      "epoch:11 step:10748 [D loss: 0.641565, acc: 62.50%] [G loss: 2.046072]\n",
      "epoch:11 step:10749 [D loss: 0.675757, acc: 61.72%] [G loss: 1.963383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10750 [D loss: 0.689262, acc: 56.25%] [G loss: 1.831852]\n",
      "epoch:11 step:10751 [D loss: 0.622889, acc: 65.62%] [G loss: 2.024903]\n",
      "epoch:11 step:10752 [D loss: 0.645208, acc: 60.16%] [G loss: 1.991809]\n",
      "epoch:11 step:10753 [D loss: 0.617262, acc: 58.59%] [G loss: 1.951130]\n",
      "epoch:11 step:10754 [D loss: 0.642474, acc: 63.28%] [G loss: 2.178587]\n",
      "epoch:11 step:10755 [D loss: 0.645741, acc: 64.06%] [G loss: 1.923460]\n",
      "epoch:11 step:10756 [D loss: 0.647580, acc: 66.41%] [G loss: 1.891934]\n",
      "epoch:11 step:10757 [D loss: 0.628375, acc: 63.28%] [G loss: 1.918030]\n",
      "epoch:11 step:10758 [D loss: 0.649729, acc: 66.41%] [G loss: 1.927712]\n",
      "epoch:11 step:10759 [D loss: 0.648256, acc: 62.50%] [G loss: 1.999921]\n",
      "epoch:11 step:10760 [D loss: 0.648458, acc: 66.41%] [G loss: 1.951678]\n",
      "epoch:11 step:10761 [D loss: 0.639229, acc: 65.62%] [G loss: 2.034718]\n",
      "epoch:11 step:10762 [D loss: 0.612389, acc: 68.75%] [G loss: 2.058102]\n",
      "epoch:11 step:10763 [D loss: 0.651490, acc: 63.28%] [G loss: 2.001548]\n",
      "epoch:11 step:10764 [D loss: 0.688038, acc: 59.38%] [G loss: 1.991875]\n",
      "epoch:11 step:10765 [D loss: 0.651606, acc: 60.94%] [G loss: 1.981679]\n",
      "epoch:11 step:10766 [D loss: 0.690280, acc: 54.69%] [G loss: 1.880414]\n",
      "epoch:11 step:10767 [D loss: 0.707931, acc: 55.47%] [G loss: 1.901551]\n",
      "epoch:11 step:10768 [D loss: 0.647015, acc: 62.50%] [G loss: 1.970558]\n",
      "epoch:11 step:10769 [D loss: 0.689430, acc: 52.34%] [G loss: 1.836046]\n",
      "epoch:11 step:10770 [D loss: 0.623437, acc: 67.97%] [G loss: 2.112808]\n",
      "epoch:11 step:10771 [D loss: 0.636508, acc: 64.06%] [G loss: 2.000222]\n",
      "epoch:11 step:10772 [D loss: 0.657266, acc: 61.72%] [G loss: 1.943716]\n",
      "epoch:11 step:10773 [D loss: 0.689901, acc: 63.28%] [G loss: 1.761597]\n",
      "epoch:11 step:10774 [D loss: 0.683506, acc: 56.25%] [G loss: 1.996984]\n",
      "epoch:11 step:10775 [D loss: 0.712179, acc: 55.47%] [G loss: 1.866777]\n",
      "epoch:11 step:10776 [D loss: 0.617615, acc: 63.28%] [G loss: 1.945247]\n",
      "epoch:11 step:10777 [D loss: 0.643354, acc: 64.84%] [G loss: 2.010964]\n",
      "epoch:11 step:10778 [D loss: 0.605821, acc: 71.88%] [G loss: 2.268855]\n",
      "epoch:11 step:10779 [D loss: 0.564135, acc: 74.22%] [G loss: 2.415619]\n",
      "epoch:11 step:10780 [D loss: 0.672682, acc: 60.16%] [G loss: 1.921615]\n",
      "epoch:11 step:10781 [D loss: 0.679839, acc: 60.94%] [G loss: 1.945655]\n",
      "epoch:11 step:10782 [D loss: 0.607940, acc: 70.31%] [G loss: 1.977381]\n",
      "epoch:11 step:10783 [D loss: 0.635071, acc: 67.19%] [G loss: 2.030802]\n",
      "epoch:11 step:10784 [D loss: 0.642873, acc: 66.41%] [G loss: 1.934016]\n",
      "epoch:11 step:10785 [D loss: 0.664900, acc: 62.50%] [G loss: 1.990726]\n",
      "epoch:11 step:10786 [D loss: 0.607992, acc: 71.88%] [G loss: 2.066843]\n",
      "epoch:11 step:10787 [D loss: 0.670348, acc: 58.59%] [G loss: 1.919230]\n",
      "epoch:11 step:10788 [D loss: 0.620833, acc: 64.06%] [G loss: 2.088881]\n",
      "epoch:11 step:10789 [D loss: 0.680906, acc: 63.28%] [G loss: 1.962072]\n",
      "epoch:11 step:10790 [D loss: 0.635590, acc: 62.50%] [G loss: 1.950681]\n",
      "epoch:11 step:10791 [D loss: 0.635625, acc: 65.62%] [G loss: 1.990492]\n",
      "epoch:11 step:10792 [D loss: 0.649018, acc: 58.59%] [G loss: 1.956192]\n",
      "epoch:11 step:10793 [D loss: 0.665385, acc: 60.94%] [G loss: 2.011255]\n",
      "epoch:11 step:10794 [D loss: 0.633498, acc: 64.84%] [G loss: 2.083561]\n",
      "epoch:11 step:10795 [D loss: 0.609297, acc: 67.19%] [G loss: 2.015315]\n",
      "epoch:11 step:10796 [D loss: 0.683882, acc: 57.81%] [G loss: 1.849055]\n",
      "epoch:11 step:10797 [D loss: 0.692268, acc: 60.94%] [G loss: 1.834931]\n",
      "epoch:11 step:10798 [D loss: 0.618786, acc: 66.41%] [G loss: 2.168927]\n",
      "epoch:11 step:10799 [D loss: 0.634468, acc: 60.16%] [G loss: 2.011745]\n",
      "epoch:11 step:10800 [D loss: 0.590486, acc: 65.62%] [G loss: 1.948364]\n",
      "##############\n",
      "[2.35937498 1.34393085 6.23072599 4.99339412 3.81111646 5.72118614\n",
      " 4.39480927 4.63629234 4.77569909 3.33043032]\n",
      "##########\n",
      "epoch:11 step:10801 [D loss: 0.584052, acc: 68.75%] [G loss: 1.973797]\n",
      "epoch:11 step:10802 [D loss: 0.626038, acc: 68.75%] [G loss: 2.193230]\n",
      "epoch:11 step:10803 [D loss: 0.659558, acc: 61.72%] [G loss: 1.997649]\n",
      "epoch:11 step:10804 [D loss: 0.599688, acc: 68.75%] [G loss: 2.323670]\n",
      "epoch:11 step:10805 [D loss: 0.605346, acc: 67.97%] [G loss: 2.190513]\n",
      "epoch:11 step:10806 [D loss: 0.579313, acc: 68.75%] [G loss: 2.213019]\n",
      "epoch:11 step:10807 [D loss: 0.719305, acc: 53.91%] [G loss: 1.835587]\n",
      "epoch:11 step:10808 [D loss: 0.687846, acc: 56.25%] [G loss: 1.911964]\n",
      "epoch:11 step:10809 [D loss: 0.684553, acc: 57.03%] [G loss: 1.792845]\n",
      "epoch:11 step:10810 [D loss: 0.638428, acc: 61.72%] [G loss: 2.002321]\n",
      "epoch:11 step:10811 [D loss: 0.620631, acc: 63.28%] [G loss: 2.273648]\n",
      "epoch:11 step:10812 [D loss: 0.565666, acc: 74.22%] [G loss: 2.150759]\n",
      "epoch:11 step:10813 [D loss: 0.695353, acc: 57.81%] [G loss: 1.996564]\n",
      "epoch:11 step:10814 [D loss: 0.650863, acc: 60.16%] [G loss: 1.963373]\n",
      "epoch:11 step:10815 [D loss: 0.545329, acc: 75.78%] [G loss: 2.184077]\n",
      "epoch:11 step:10816 [D loss: 0.646674, acc: 63.28%] [G loss: 2.112831]\n",
      "epoch:11 step:10817 [D loss: 0.655480, acc: 61.72%] [G loss: 1.923830]\n",
      "epoch:11 step:10818 [D loss: 0.698278, acc: 53.12%] [G loss: 1.877099]\n",
      "epoch:11 step:10819 [D loss: 0.641551, acc: 58.59%] [G loss: 2.079117]\n",
      "epoch:11 step:10820 [D loss: 0.648814, acc: 60.94%] [G loss: 2.011893]\n",
      "epoch:11 step:10821 [D loss: 0.667341, acc: 55.47%] [G loss: 1.977375]\n",
      "epoch:11 step:10822 [D loss: 0.592654, acc: 68.75%] [G loss: 1.853881]\n",
      "epoch:11 step:10823 [D loss: 0.594056, acc: 68.75%] [G loss: 2.005185]\n",
      "epoch:11 step:10824 [D loss: 0.575576, acc: 72.66%] [G loss: 2.166706]\n",
      "epoch:11 step:10825 [D loss: 0.598201, acc: 73.44%] [G loss: 2.025365]\n",
      "epoch:11 step:10826 [D loss: 0.605848, acc: 68.75%] [G loss: 2.010153]\n",
      "epoch:11 step:10827 [D loss: 0.607241, acc: 70.31%] [G loss: 2.174387]\n",
      "epoch:11 step:10828 [D loss: 0.604026, acc: 64.84%] [G loss: 2.061346]\n",
      "epoch:11 step:10829 [D loss: 0.575330, acc: 73.44%] [G loss: 2.052760]\n",
      "epoch:11 step:10830 [D loss: 0.598523, acc: 67.19%] [G loss: 2.108480]\n",
      "epoch:11 step:10831 [D loss: 0.633261, acc: 64.84%] [G loss: 2.012221]\n",
      "epoch:11 step:10832 [D loss: 0.657990, acc: 60.94%] [G loss: 2.131836]\n",
      "epoch:11 step:10833 [D loss: 0.657895, acc: 57.81%] [G loss: 1.970468]\n",
      "epoch:11 step:10834 [D loss: 0.683522, acc: 62.50%] [G loss: 1.875937]\n",
      "epoch:11 step:10835 [D loss: 0.673428, acc: 59.38%] [G loss: 1.881429]\n",
      "epoch:11 step:10836 [D loss: 0.644594, acc: 57.81%] [G loss: 1.855332]\n",
      "epoch:11 step:10837 [D loss: 0.595475, acc: 72.66%] [G loss: 2.008060]\n",
      "epoch:11 step:10838 [D loss: 0.648047, acc: 60.94%] [G loss: 1.957186]\n",
      "epoch:11 step:10839 [D loss: 0.599427, acc: 66.41%] [G loss: 2.183109]\n",
      "epoch:11 step:10840 [D loss: 0.605460, acc: 67.97%] [G loss: 1.998954]\n",
      "epoch:11 step:10841 [D loss: 0.575079, acc: 72.66%] [G loss: 2.278505]\n",
      "epoch:11 step:10842 [D loss: 0.625635, acc: 62.50%] [G loss: 1.821788]\n",
      "epoch:11 step:10843 [D loss: 0.569690, acc: 74.22%] [G loss: 2.215502]\n",
      "epoch:11 step:10844 [D loss: 0.701590, acc: 57.03%] [G loss: 1.977069]\n",
      "epoch:11 step:10845 [D loss: 0.683718, acc: 57.03%] [G loss: 1.888361]\n",
      "epoch:11 step:10846 [D loss: 0.670262, acc: 61.72%] [G loss: 1.949032]\n",
      "epoch:11 step:10847 [D loss: 0.680317, acc: 58.59%] [G loss: 1.854763]\n",
      "epoch:11 step:10848 [D loss: 0.612030, acc: 69.53%] [G loss: 2.053324]\n",
      "epoch:11 step:10849 [D loss: 0.669391, acc: 59.38%] [G loss: 1.895819]\n",
      "epoch:11 step:10850 [D loss: 0.606594, acc: 72.66%] [G loss: 1.965052]\n",
      "epoch:11 step:10851 [D loss: 0.650602, acc: 61.72%] [G loss: 1.995617]\n",
      "epoch:11 step:10852 [D loss: 0.552731, acc: 73.44%] [G loss: 2.076751]\n",
      "epoch:11 step:10853 [D loss: 0.623061, acc: 64.84%] [G loss: 2.003676]\n",
      "epoch:11 step:10854 [D loss: 0.619728, acc: 64.84%] [G loss: 2.213765]\n",
      "epoch:11 step:10855 [D loss: 0.596418, acc: 67.97%] [G loss: 2.037375]\n",
      "epoch:11 step:10856 [D loss: 0.618342, acc: 65.62%] [G loss: 2.079308]\n",
      "epoch:11 step:10857 [D loss: 0.620012, acc: 62.50%] [G loss: 2.102661]\n",
      "epoch:11 step:10858 [D loss: 0.539426, acc: 73.44%] [G loss: 2.278908]\n",
      "epoch:11 step:10859 [D loss: 0.611142, acc: 62.50%] [G loss: 2.155775]\n",
      "epoch:11 step:10860 [D loss: 0.603586, acc: 67.97%] [G loss: 1.930999]\n",
      "epoch:11 step:10861 [D loss: 0.587738, acc: 69.53%] [G loss: 2.326474]\n",
      "epoch:11 step:10862 [D loss: 0.624135, acc: 71.09%] [G loss: 2.230415]\n",
      "epoch:11 step:10863 [D loss: 0.522380, acc: 74.22%] [G loss: 2.515690]\n",
      "epoch:11 step:10864 [D loss: 0.646448, acc: 61.72%] [G loss: 2.245357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10865 [D loss: 0.562576, acc: 69.53%] [G loss: 2.322887]\n",
      "epoch:11 step:10866 [D loss: 0.610468, acc: 71.09%] [G loss: 2.041488]\n",
      "epoch:11 step:10867 [D loss: 0.651038, acc: 63.28%] [G loss: 1.930765]\n",
      "epoch:11 step:10868 [D loss: 0.686727, acc: 59.38%] [G loss: 1.978264]\n",
      "epoch:11 step:10869 [D loss: 0.637282, acc: 65.62%] [G loss: 2.001184]\n",
      "epoch:11 step:10870 [D loss: 0.636857, acc: 68.75%] [G loss: 1.888182]\n",
      "epoch:11 step:10871 [D loss: 0.652894, acc: 60.16%] [G loss: 2.181620]\n",
      "epoch:11 step:10872 [D loss: 0.683111, acc: 59.38%] [G loss: 2.042068]\n",
      "epoch:11 step:10873 [D loss: 0.675869, acc: 60.16%] [G loss: 1.891965]\n",
      "epoch:11 step:10874 [D loss: 0.688645, acc: 58.59%] [G loss: 2.047014]\n",
      "epoch:11 step:10875 [D loss: 0.654464, acc: 61.72%] [G loss: 2.032987]\n",
      "epoch:11 step:10876 [D loss: 0.677962, acc: 56.25%] [G loss: 2.087885]\n",
      "epoch:11 step:10877 [D loss: 0.608507, acc: 67.19%] [G loss: 1.841774]\n",
      "epoch:11 step:10878 [D loss: 0.667390, acc: 60.94%] [G loss: 1.967113]\n",
      "epoch:11 step:10879 [D loss: 0.643963, acc: 60.94%] [G loss: 2.017670]\n",
      "epoch:11 step:10880 [D loss: 0.640528, acc: 64.84%] [G loss: 1.915451]\n",
      "epoch:11 step:10881 [D loss: 0.611978, acc: 70.31%] [G loss: 2.076021]\n",
      "epoch:11 step:10882 [D loss: 0.612783, acc: 70.31%] [G loss: 2.100222]\n",
      "epoch:11 step:10883 [D loss: 0.608123, acc: 64.84%] [G loss: 1.998791]\n",
      "epoch:11 step:10884 [D loss: 0.693375, acc: 61.72%] [G loss: 1.890382]\n",
      "epoch:11 step:10885 [D loss: 0.666112, acc: 62.50%] [G loss: 1.920190]\n",
      "epoch:11 step:10886 [D loss: 0.667435, acc: 57.03%] [G loss: 1.785199]\n",
      "epoch:11 step:10887 [D loss: 0.644672, acc: 63.28%] [G loss: 1.811160]\n",
      "epoch:11 step:10888 [D loss: 0.626901, acc: 64.84%] [G loss: 1.896526]\n",
      "epoch:11 step:10889 [D loss: 0.699411, acc: 54.69%] [G loss: 2.059697]\n",
      "epoch:11 step:10890 [D loss: 0.642767, acc: 67.19%] [G loss: 1.975175]\n",
      "epoch:11 step:10891 [D loss: 0.609307, acc: 67.19%] [G loss: 1.909082]\n",
      "epoch:11 step:10892 [D loss: 0.677028, acc: 61.72%] [G loss: 1.975993]\n",
      "epoch:11 step:10893 [D loss: 0.648929, acc: 62.50%] [G loss: 1.882953]\n",
      "epoch:11 step:10894 [D loss: 0.608269, acc: 67.97%] [G loss: 2.178946]\n",
      "epoch:11 step:10895 [D loss: 0.638208, acc: 68.75%] [G loss: 1.967938]\n",
      "epoch:11 step:10896 [D loss: 0.604871, acc: 66.41%] [G loss: 2.089087]\n",
      "epoch:11 step:10897 [D loss: 0.677200, acc: 60.16%] [G loss: 2.151184]\n",
      "epoch:11 step:10898 [D loss: 0.638584, acc: 61.72%] [G loss: 1.980258]\n",
      "epoch:11 step:10899 [D loss: 0.587372, acc: 71.88%] [G loss: 2.167846]\n",
      "epoch:11 step:10900 [D loss: 0.653793, acc: 60.16%] [G loss: 2.108605]\n",
      "epoch:11 step:10901 [D loss: 0.693111, acc: 55.47%] [G loss: 1.972462]\n",
      "epoch:11 step:10902 [D loss: 0.645360, acc: 62.50%] [G loss: 2.000035]\n",
      "epoch:11 step:10903 [D loss: 0.703455, acc: 62.50%] [G loss: 1.970683]\n",
      "epoch:11 step:10904 [D loss: 0.679404, acc: 60.16%] [G loss: 1.973508]\n",
      "epoch:11 step:10905 [D loss: 0.610043, acc: 68.75%] [G loss: 2.132119]\n",
      "epoch:11 step:10906 [D loss: 0.641916, acc: 62.50%] [G loss: 2.050165]\n",
      "epoch:11 step:10907 [D loss: 0.678832, acc: 60.94%] [G loss: 1.983715]\n",
      "epoch:11 step:10908 [D loss: 0.650745, acc: 65.62%] [G loss: 1.988441]\n",
      "epoch:11 step:10909 [D loss: 0.665028, acc: 55.47%] [G loss: 2.088876]\n",
      "epoch:11 step:10910 [D loss: 0.660243, acc: 61.72%] [G loss: 2.041720]\n",
      "epoch:11 step:10911 [D loss: 0.628660, acc: 63.28%] [G loss: 2.095204]\n",
      "epoch:11 step:10912 [D loss: 0.640252, acc: 62.50%] [G loss: 2.078874]\n",
      "epoch:11 step:10913 [D loss: 0.637115, acc: 63.28%] [G loss: 1.921025]\n",
      "epoch:11 step:10914 [D loss: 0.581565, acc: 71.09%] [G loss: 2.107996]\n",
      "epoch:11 step:10915 [D loss: 0.686418, acc: 57.81%] [G loss: 2.058392]\n",
      "epoch:11 step:10916 [D loss: 0.652515, acc: 61.72%] [G loss: 1.965561]\n",
      "epoch:11 step:10917 [D loss: 0.683364, acc: 57.03%] [G loss: 1.678298]\n",
      "epoch:11 step:10918 [D loss: 0.670171, acc: 56.25%] [G loss: 1.902085]\n",
      "epoch:11 step:10919 [D loss: 0.635167, acc: 59.38%] [G loss: 1.980364]\n",
      "epoch:11 step:10920 [D loss: 0.579825, acc: 69.53%] [G loss: 2.039776]\n",
      "epoch:11 step:10921 [D loss: 0.644974, acc: 62.50%] [G loss: 1.952812]\n",
      "epoch:11 step:10922 [D loss: 0.688874, acc: 57.81%] [G loss: 1.880418]\n",
      "epoch:11 step:10923 [D loss: 0.609223, acc: 64.06%] [G loss: 1.900792]\n",
      "epoch:11 step:10924 [D loss: 0.655391, acc: 60.16%] [G loss: 1.898997]\n",
      "epoch:11 step:10925 [D loss: 0.650777, acc: 60.94%] [G loss: 1.861709]\n",
      "epoch:11 step:10926 [D loss: 0.684192, acc: 59.38%] [G loss: 1.931761]\n",
      "epoch:11 step:10927 [D loss: 0.628938, acc: 67.19%] [G loss: 1.973336]\n",
      "epoch:11 step:10928 [D loss: 0.616208, acc: 63.28%] [G loss: 2.046179]\n",
      "epoch:11 step:10929 [D loss: 0.642443, acc: 59.38%] [G loss: 1.929281]\n",
      "epoch:11 step:10930 [D loss: 0.669503, acc: 59.38%] [G loss: 1.872133]\n",
      "epoch:11 step:10931 [D loss: 0.634709, acc: 67.97%] [G loss: 2.123759]\n",
      "epoch:11 step:10932 [D loss: 0.646878, acc: 61.72%] [G loss: 1.791394]\n",
      "epoch:11 step:10933 [D loss: 0.636494, acc: 65.62%] [G loss: 2.033976]\n",
      "epoch:11 step:10934 [D loss: 0.602759, acc: 68.75%] [G loss: 2.017100]\n",
      "epoch:11 step:10935 [D loss: 0.629935, acc: 65.62%] [G loss: 1.969025]\n",
      "epoch:11 step:10936 [D loss: 0.649668, acc: 64.06%] [G loss: 2.132190]\n",
      "epoch:11 step:10937 [D loss: 0.617510, acc: 69.53%] [G loss: 2.114123]\n",
      "epoch:11 step:10938 [D loss: 0.577046, acc: 70.31%] [G loss: 2.117320]\n",
      "epoch:11 step:10939 [D loss: 0.631201, acc: 66.41%] [G loss: 1.846093]\n",
      "epoch:11 step:10940 [D loss: 0.674712, acc: 57.81%] [G loss: 1.882894]\n",
      "epoch:11 step:10941 [D loss: 0.550559, acc: 73.44%] [G loss: 2.061193]\n",
      "epoch:11 step:10942 [D loss: 0.613097, acc: 63.28%] [G loss: 1.973779]\n",
      "epoch:11 step:10943 [D loss: 0.671312, acc: 61.72%] [G loss: 1.961928]\n",
      "epoch:11 step:10944 [D loss: 0.591320, acc: 66.41%] [G loss: 2.263250]\n",
      "epoch:11 step:10945 [D loss: 0.645121, acc: 64.06%] [G loss: 1.973503]\n",
      "epoch:11 step:10946 [D loss: 0.620535, acc: 65.62%] [G loss: 2.029833]\n",
      "epoch:11 step:10947 [D loss: 0.623617, acc: 64.84%] [G loss: 1.937385]\n",
      "epoch:11 step:10948 [D loss: 0.592366, acc: 66.41%] [G loss: 2.257518]\n",
      "epoch:11 step:10949 [D loss: 0.587963, acc: 71.88%] [G loss: 2.237024]\n",
      "epoch:11 step:10950 [D loss: 0.623315, acc: 64.84%] [G loss: 2.245917]\n",
      "epoch:11 step:10951 [D loss: 0.635525, acc: 67.19%] [G loss: 2.040576]\n",
      "epoch:11 step:10952 [D loss: 0.633983, acc: 62.50%] [G loss: 1.963912]\n",
      "epoch:11 step:10953 [D loss: 0.683441, acc: 57.03%] [G loss: 2.221062]\n",
      "epoch:11 step:10954 [D loss: 0.627190, acc: 61.72%] [G loss: 2.325240]\n",
      "epoch:11 step:10955 [D loss: 0.594646, acc: 65.62%] [G loss: 2.492679]\n",
      "epoch:11 step:10956 [D loss: 0.588746, acc: 67.97%] [G loss: 2.309001]\n",
      "epoch:11 step:10957 [D loss: 0.675381, acc: 57.81%] [G loss: 2.233779]\n",
      "epoch:11 step:10958 [D loss: 0.616288, acc: 70.31%] [G loss: 2.074498]\n",
      "epoch:11 step:10959 [D loss: 0.595802, acc: 66.41%] [G loss: 2.059462]\n",
      "epoch:11 step:10960 [D loss: 0.623261, acc: 64.84%] [G loss: 1.907903]\n",
      "epoch:11 step:10961 [D loss: 0.617705, acc: 67.97%] [G loss: 2.274479]\n",
      "epoch:11 step:10962 [D loss: 0.673378, acc: 60.16%] [G loss: 1.969315]\n",
      "epoch:11 step:10963 [D loss: 0.615011, acc: 62.50%] [G loss: 2.015044]\n",
      "epoch:11 step:10964 [D loss: 0.659359, acc: 63.28%] [G loss: 2.018555]\n",
      "epoch:11 step:10965 [D loss: 0.642030, acc: 63.28%] [G loss: 1.842971]\n",
      "epoch:11 step:10966 [D loss: 0.634392, acc: 61.72%] [G loss: 1.941305]\n",
      "epoch:11 step:10967 [D loss: 0.632041, acc: 69.53%] [G loss: 2.028346]\n",
      "epoch:11 step:10968 [D loss: 0.623314, acc: 65.62%] [G loss: 1.897199]\n",
      "epoch:11 step:10969 [D loss: 0.574711, acc: 71.88%] [G loss: 2.073363]\n",
      "epoch:11 step:10970 [D loss: 0.603322, acc: 67.19%] [G loss: 2.010414]\n",
      "epoch:11 step:10971 [D loss: 0.642872, acc: 61.72%] [G loss: 2.000276]\n",
      "epoch:11 step:10972 [D loss: 0.630737, acc: 65.62%] [G loss: 1.910376]\n",
      "epoch:11 step:10973 [D loss: 0.635797, acc: 61.72%] [G loss: 2.001332]\n",
      "epoch:11 step:10974 [D loss: 0.656213, acc: 62.50%] [G loss: 1.946588]\n",
      "epoch:11 step:10975 [D loss: 0.669942, acc: 61.72%] [G loss: 2.041055]\n",
      "epoch:11 step:10976 [D loss: 0.619485, acc: 65.62%] [G loss: 1.932122]\n",
      "epoch:11 step:10977 [D loss: 0.623146, acc: 65.62%] [G loss: 2.004824]\n",
      "epoch:11 step:10978 [D loss: 0.676292, acc: 58.59%] [G loss: 1.995232]\n",
      "epoch:11 step:10979 [D loss: 0.675598, acc: 64.06%] [G loss: 1.953558]\n",
      "epoch:11 step:10980 [D loss: 0.644442, acc: 60.94%] [G loss: 1.950399]\n",
      "epoch:11 step:10981 [D loss: 0.622863, acc: 66.41%] [G loss: 1.936296]\n",
      "epoch:11 step:10982 [D loss: 0.690459, acc: 59.38%] [G loss: 1.938017]\n",
      "epoch:11 step:10983 [D loss: 0.643333, acc: 56.25%] [G loss: 2.040184]\n",
      "epoch:11 step:10984 [D loss: 0.608971, acc: 68.75%] [G loss: 1.975969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10985 [D loss: 0.628190, acc: 64.84%] [G loss: 1.880096]\n",
      "epoch:11 step:10986 [D loss: 0.628402, acc: 62.50%] [G loss: 2.058426]\n",
      "epoch:11 step:10987 [D loss: 0.609517, acc: 68.75%] [G loss: 2.020185]\n",
      "epoch:11 step:10988 [D loss: 0.615642, acc: 71.09%] [G loss: 1.969421]\n",
      "epoch:11 step:10989 [D loss: 0.685004, acc: 60.16%] [G loss: 1.982022]\n",
      "epoch:11 step:10990 [D loss: 0.664722, acc: 61.72%] [G loss: 1.912441]\n",
      "epoch:11 step:10991 [D loss: 0.695776, acc: 60.94%] [G loss: 1.847884]\n",
      "epoch:11 step:10992 [D loss: 0.607442, acc: 63.28%] [G loss: 1.960665]\n",
      "epoch:11 step:10993 [D loss: 0.620040, acc: 62.50%] [G loss: 1.996526]\n",
      "epoch:11 step:10994 [D loss: 0.647007, acc: 62.50%] [G loss: 2.095234]\n",
      "epoch:11 step:10995 [D loss: 0.622946, acc: 64.84%] [G loss: 2.021417]\n",
      "epoch:11 step:10996 [D loss: 0.619976, acc: 64.06%] [G loss: 2.031847]\n",
      "epoch:11 step:10997 [D loss: 0.657653, acc: 61.72%] [G loss: 2.074959]\n",
      "epoch:11 step:10998 [D loss: 0.570260, acc: 71.09%] [G loss: 2.076014]\n",
      "epoch:11 step:10999 [D loss: 0.576625, acc: 68.75%] [G loss: 2.180101]\n",
      "epoch:11 step:11000 [D loss: 0.614987, acc: 67.97%] [G loss: 2.177429]\n",
      "##############\n",
      "[2.18666374 1.47148927 6.39448337 4.93251647 3.89191588 5.85167\n",
      " 4.4706573  4.83331439 4.79951429 3.56732915]\n",
      "##########\n",
      "epoch:11 step:11001 [D loss: 0.669959, acc: 58.59%] [G loss: 2.147570]\n",
      "epoch:11 step:11002 [D loss: 0.581424, acc: 67.97%] [G loss: 2.019172]\n",
      "epoch:11 step:11003 [D loss: 0.721564, acc: 57.81%] [G loss: 2.050956]\n",
      "epoch:11 step:11004 [D loss: 0.676506, acc: 62.50%] [G loss: 1.943241]\n",
      "epoch:11 step:11005 [D loss: 0.641828, acc: 64.06%] [G loss: 1.848858]\n",
      "epoch:11 step:11006 [D loss: 0.631685, acc: 65.62%] [G loss: 2.097114]\n",
      "epoch:11 step:11007 [D loss: 0.618234, acc: 64.06%] [G loss: 1.980909]\n",
      "epoch:11 step:11008 [D loss: 0.637942, acc: 66.41%] [G loss: 2.031116]\n",
      "epoch:11 step:11009 [D loss: 0.664195, acc: 60.16%] [G loss: 1.890172]\n",
      "epoch:11 step:11010 [D loss: 0.650371, acc: 61.72%] [G loss: 1.841570]\n",
      "epoch:11 step:11011 [D loss: 0.607015, acc: 67.19%] [G loss: 1.983610]\n",
      "epoch:11 step:11012 [D loss: 0.665790, acc: 63.28%] [G loss: 1.970114]\n",
      "epoch:11 step:11013 [D loss: 0.650835, acc: 60.16%] [G loss: 2.131190]\n",
      "epoch:11 step:11014 [D loss: 0.618254, acc: 68.75%] [G loss: 2.082503]\n",
      "epoch:11 step:11015 [D loss: 0.616901, acc: 68.75%] [G loss: 2.018412]\n",
      "epoch:11 step:11016 [D loss: 0.607223, acc: 66.41%] [G loss: 2.159920]\n",
      "epoch:11 step:11017 [D loss: 0.653336, acc: 60.94%] [G loss: 1.867655]\n",
      "epoch:11 step:11018 [D loss: 0.622234, acc: 61.72%] [G loss: 2.014100]\n",
      "epoch:11 step:11019 [D loss: 0.535573, acc: 79.69%] [G loss: 2.139982]\n",
      "epoch:11 step:11020 [D loss: 0.578209, acc: 71.09%] [G loss: 1.973589]\n",
      "epoch:11 step:11021 [D loss: 0.642052, acc: 60.94%] [G loss: 2.252234]\n",
      "epoch:11 step:11022 [D loss: 0.761978, acc: 54.69%] [G loss: 1.956247]\n",
      "epoch:11 step:11023 [D loss: 0.685429, acc: 56.25%] [G loss: 1.852555]\n",
      "epoch:11 step:11024 [D loss: 0.673220, acc: 53.91%] [G loss: 1.876547]\n",
      "epoch:11 step:11025 [D loss: 0.690180, acc: 60.94%] [G loss: 1.835466]\n",
      "epoch:11 step:11026 [D loss: 0.626484, acc: 65.62%] [G loss: 2.046519]\n",
      "epoch:11 step:11027 [D loss: 0.613039, acc: 66.41%] [G loss: 2.087915]\n",
      "epoch:11 step:11028 [D loss: 0.625127, acc: 64.84%] [G loss: 1.985742]\n",
      "epoch:11 step:11029 [D loss: 0.631852, acc: 60.94%] [G loss: 1.929030]\n",
      "epoch:11 step:11030 [D loss: 0.658707, acc: 64.84%] [G loss: 1.824255]\n",
      "epoch:11 step:11031 [D loss: 0.609709, acc: 67.19%] [G loss: 2.016919]\n",
      "epoch:11 step:11032 [D loss: 0.715431, acc: 58.59%] [G loss: 2.102016]\n",
      "epoch:11 step:11033 [D loss: 0.606193, acc: 63.28%] [G loss: 1.988426]\n",
      "epoch:11 step:11034 [D loss: 0.619017, acc: 67.19%] [G loss: 1.992538]\n",
      "epoch:11 step:11035 [D loss: 0.661596, acc: 61.72%] [G loss: 2.070662]\n",
      "epoch:11 step:11036 [D loss: 0.641523, acc: 58.59%] [G loss: 1.933593]\n",
      "epoch:11 step:11037 [D loss: 0.612487, acc: 64.06%] [G loss: 1.951821]\n",
      "epoch:11 step:11038 [D loss: 0.662229, acc: 66.41%] [G loss: 1.950660]\n",
      "epoch:11 step:11039 [D loss: 0.669712, acc: 63.28%] [G loss: 1.976975]\n",
      "epoch:11 step:11040 [D loss: 0.626143, acc: 66.41%] [G loss: 1.963486]\n",
      "epoch:11 step:11041 [D loss: 0.643616, acc: 64.84%] [G loss: 2.030510]\n",
      "epoch:11 step:11042 [D loss: 0.589124, acc: 67.97%] [G loss: 2.351977]\n",
      "epoch:11 step:11043 [D loss: 0.653119, acc: 60.94%] [G loss: 1.936275]\n",
      "epoch:11 step:11044 [D loss: 0.620760, acc: 63.28%] [G loss: 1.891537]\n",
      "epoch:11 step:11045 [D loss: 0.660811, acc: 62.50%] [G loss: 1.725964]\n",
      "epoch:11 step:11046 [D loss: 0.636478, acc: 58.59%] [G loss: 1.872207]\n",
      "epoch:11 step:11047 [D loss: 0.599071, acc: 67.19%] [G loss: 2.036422]\n",
      "epoch:11 step:11048 [D loss: 0.683729, acc: 64.84%] [G loss: 1.802776]\n",
      "epoch:11 step:11049 [D loss: 0.700687, acc: 54.69%] [G loss: 1.813207]\n",
      "epoch:11 step:11050 [D loss: 0.664954, acc: 57.81%] [G loss: 1.920738]\n",
      "epoch:11 step:11051 [D loss: 0.623923, acc: 60.94%] [G loss: 1.914511]\n",
      "epoch:11 step:11052 [D loss: 0.671431, acc: 58.59%] [G loss: 1.875013]\n",
      "epoch:11 step:11053 [D loss: 0.619029, acc: 60.16%] [G loss: 2.052680]\n",
      "epoch:11 step:11054 [D loss: 0.589116, acc: 70.31%] [G loss: 2.026039]\n",
      "epoch:11 step:11055 [D loss: 0.609395, acc: 66.41%] [G loss: 1.938783]\n",
      "epoch:11 step:11056 [D loss: 0.662622, acc: 62.50%] [G loss: 1.882048]\n",
      "epoch:11 step:11057 [D loss: 0.673635, acc: 57.03%] [G loss: 1.766346]\n",
      "epoch:11 step:11058 [D loss: 0.654528, acc: 64.06%] [G loss: 1.845612]\n",
      "epoch:11 step:11059 [D loss: 0.597102, acc: 72.66%] [G loss: 1.801702]\n",
      "epoch:11 step:11060 [D loss: 0.644205, acc: 61.72%] [G loss: 1.907868]\n",
      "epoch:11 step:11061 [D loss: 0.581162, acc: 67.97%] [G loss: 2.063832]\n",
      "epoch:11 step:11062 [D loss: 0.644879, acc: 59.38%] [G loss: 1.965928]\n",
      "epoch:11 step:11063 [D loss: 0.579288, acc: 75.00%] [G loss: 1.884333]\n",
      "epoch:11 step:11064 [D loss: 0.608819, acc: 64.84%] [G loss: 2.033412]\n",
      "epoch:11 step:11065 [D loss: 0.683831, acc: 56.25%] [G loss: 1.961714]\n",
      "epoch:11 step:11066 [D loss: 0.665424, acc: 64.84%] [G loss: 1.868083]\n",
      "epoch:11 step:11067 [D loss: 0.574280, acc: 67.19%] [G loss: 1.765594]\n",
      "epoch:11 step:11068 [D loss: 0.635709, acc: 61.72%] [G loss: 1.918084]\n",
      "epoch:11 step:11069 [D loss: 0.640437, acc: 63.28%] [G loss: 1.857087]\n",
      "epoch:11 step:11070 [D loss: 0.646412, acc: 58.59%] [G loss: 2.160275]\n",
      "epoch:11 step:11071 [D loss: 0.624650, acc: 67.19%] [G loss: 1.925530]\n",
      "epoch:11 step:11072 [D loss: 0.746271, acc: 55.47%] [G loss: 1.727415]\n",
      "epoch:11 step:11073 [D loss: 0.710718, acc: 51.56%] [G loss: 1.896104]\n",
      "epoch:11 step:11074 [D loss: 0.587226, acc: 68.75%] [G loss: 2.107584]\n",
      "epoch:11 step:11075 [D loss: 0.710753, acc: 57.03%] [G loss: 1.792681]\n",
      "epoch:11 step:11076 [D loss: 0.692181, acc: 58.59%] [G loss: 2.016870]\n",
      "epoch:11 step:11077 [D loss: 0.644263, acc: 62.50%] [G loss: 2.138188]\n",
      "epoch:11 step:11078 [D loss: 0.696978, acc: 57.03%] [G loss: 1.970599]\n",
      "epoch:11 step:11079 [D loss: 0.615041, acc: 61.72%] [G loss: 1.806166]\n",
      "epoch:11 step:11080 [D loss: 0.690515, acc: 58.59%] [G loss: 1.992649]\n",
      "epoch:11 step:11081 [D loss: 0.610256, acc: 68.75%] [G loss: 2.050308]\n",
      "epoch:11 step:11082 [D loss: 0.611579, acc: 65.62%] [G loss: 2.177359]\n",
      "epoch:11 step:11083 [D loss: 0.570671, acc: 70.31%] [G loss: 2.201612]\n",
      "epoch:11 step:11084 [D loss: 0.618917, acc: 64.06%] [G loss: 1.977765]\n",
      "epoch:11 step:11085 [D loss: 0.638922, acc: 65.62%] [G loss: 2.024988]\n",
      "epoch:11 step:11086 [D loss: 0.667562, acc: 59.38%] [G loss: 1.942507]\n",
      "epoch:11 step:11087 [D loss: 0.622387, acc: 66.41%] [G loss: 1.993207]\n",
      "epoch:11 step:11088 [D loss: 0.609636, acc: 67.97%] [G loss: 2.288858]\n",
      "epoch:11 step:11089 [D loss: 0.624354, acc: 63.28%] [G loss: 2.252952]\n",
      "epoch:11 step:11090 [D loss: 0.689627, acc: 57.03%] [G loss: 1.906907]\n",
      "epoch:11 step:11091 [D loss: 0.655004, acc: 64.84%] [G loss: 1.910876]\n",
      "epoch:11 step:11092 [D loss: 0.653562, acc: 61.72%] [G loss: 1.817892]\n",
      "epoch:11 step:11093 [D loss: 0.603242, acc: 67.19%] [G loss: 2.132724]\n",
      "epoch:11 step:11094 [D loss: 0.652684, acc: 64.84%] [G loss: 1.961923]\n",
      "epoch:11 step:11095 [D loss: 0.673072, acc: 61.72%] [G loss: 1.865808]\n",
      "epoch:11 step:11096 [D loss: 0.649015, acc: 64.06%] [G loss: 1.840972]\n",
      "epoch:11 step:11097 [D loss: 0.600825, acc: 72.66%] [G loss: 2.121740]\n",
      "epoch:11 step:11098 [D loss: 0.662466, acc: 61.72%] [G loss: 1.940764]\n",
      "epoch:11 step:11099 [D loss: 0.555068, acc: 71.88%] [G loss: 2.093450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11100 [D loss: 0.618902, acc: 71.09%] [G loss: 1.883141]\n",
      "epoch:11 step:11101 [D loss: 0.655807, acc: 60.16%] [G loss: 1.872074]\n",
      "epoch:11 step:11102 [D loss: 0.650569, acc: 65.62%] [G loss: 1.921960]\n",
      "epoch:11 step:11103 [D loss: 0.549655, acc: 73.44%] [G loss: 1.975044]\n",
      "epoch:11 step:11104 [D loss: 0.603134, acc: 66.41%] [G loss: 2.003067]\n",
      "epoch:11 step:11105 [D loss: 0.620818, acc: 70.31%] [G loss: 2.077496]\n",
      "epoch:11 step:11106 [D loss: 0.660196, acc: 61.72%] [G loss: 1.882386]\n",
      "epoch:11 step:11107 [D loss: 0.691365, acc: 59.38%] [G loss: 1.810042]\n",
      "epoch:11 step:11108 [D loss: 0.714697, acc: 52.34%] [G loss: 1.931787]\n",
      "epoch:11 step:11109 [D loss: 0.672293, acc: 61.72%] [G loss: 2.036158]\n",
      "epoch:11 step:11110 [D loss: 0.627131, acc: 64.06%] [G loss: 1.948138]\n",
      "epoch:11 step:11111 [D loss: 0.647474, acc: 63.28%] [G loss: 2.136935]\n",
      "epoch:11 step:11112 [D loss: 0.599221, acc: 62.50%] [G loss: 2.006582]\n",
      "epoch:11 step:11113 [D loss: 0.549439, acc: 73.44%] [G loss: 2.152828]\n",
      "epoch:11 step:11114 [D loss: 0.601668, acc: 64.06%] [G loss: 2.153445]\n",
      "epoch:11 step:11115 [D loss: 0.632122, acc: 60.94%] [G loss: 1.857212]\n",
      "epoch:11 step:11116 [D loss: 0.635600, acc: 62.50%] [G loss: 1.889115]\n",
      "epoch:11 step:11117 [D loss: 0.607107, acc: 63.28%] [G loss: 1.899462]\n",
      "epoch:11 step:11118 [D loss: 0.651964, acc: 63.28%] [G loss: 1.902370]\n",
      "epoch:11 step:11119 [D loss: 0.692851, acc: 51.56%] [G loss: 1.922345]\n",
      "epoch:11 step:11120 [D loss: 0.637007, acc: 60.94%] [G loss: 1.928508]\n",
      "epoch:11 step:11121 [D loss: 0.631403, acc: 64.06%] [G loss: 2.055210]\n",
      "epoch:11 step:11122 [D loss: 0.548749, acc: 71.88%] [G loss: 2.483029]\n",
      "epoch:11 step:11123 [D loss: 0.549711, acc: 73.44%] [G loss: 2.188761]\n",
      "epoch:11 step:11124 [D loss: 0.650784, acc: 66.41%] [G loss: 2.104282]\n",
      "epoch:11 step:11125 [D loss: 0.681913, acc: 64.06%] [G loss: 1.934754]\n",
      "epoch:11 step:11126 [D loss: 0.624523, acc: 67.97%] [G loss: 1.913426]\n",
      "epoch:11 step:11127 [D loss: 0.701315, acc: 57.03%] [G loss: 1.912648]\n",
      "epoch:11 step:11128 [D loss: 0.676941, acc: 61.72%] [G loss: 1.907514]\n",
      "epoch:11 step:11129 [D loss: 0.659617, acc: 59.38%] [G loss: 2.002070]\n",
      "epoch:11 step:11130 [D loss: 0.641296, acc: 61.72%] [G loss: 2.035706]\n",
      "epoch:11 step:11131 [D loss: 0.709256, acc: 53.91%] [G loss: 1.946475]\n",
      "epoch:11 step:11132 [D loss: 0.620977, acc: 65.62%] [G loss: 2.015146]\n",
      "epoch:11 step:11133 [D loss: 0.619939, acc: 64.84%] [G loss: 2.064950]\n",
      "epoch:11 step:11134 [D loss: 0.657129, acc: 57.81%] [G loss: 1.771093]\n",
      "epoch:11 step:11135 [D loss: 0.693057, acc: 54.69%] [G loss: 1.791608]\n",
      "epoch:11 step:11136 [D loss: 0.715326, acc: 50.78%] [G loss: 1.714001]\n",
      "epoch:11 step:11137 [D loss: 0.690053, acc: 59.38%] [G loss: 1.825995]\n",
      "epoch:11 step:11138 [D loss: 0.654073, acc: 63.28%] [G loss: 1.981972]\n",
      "epoch:11 step:11139 [D loss: 0.652410, acc: 59.38%] [G loss: 1.953233]\n",
      "epoch:11 step:11140 [D loss: 0.666356, acc: 62.50%] [G loss: 1.990179]\n",
      "epoch:11 step:11141 [D loss: 0.586869, acc: 67.19%] [G loss: 1.965437]\n",
      "epoch:11 step:11142 [D loss: 0.589687, acc: 71.09%] [G loss: 1.876709]\n",
      "epoch:11 step:11143 [D loss: 0.633294, acc: 63.28%] [G loss: 1.996091]\n",
      "epoch:11 step:11144 [D loss: 0.630191, acc: 63.28%] [G loss: 1.939752]\n",
      "epoch:11 step:11145 [D loss: 0.598989, acc: 70.31%] [G loss: 1.969490]\n",
      "epoch:11 step:11146 [D loss: 0.595891, acc: 71.09%] [G loss: 1.954887]\n",
      "epoch:11 step:11147 [D loss: 0.569188, acc: 74.22%] [G loss: 2.034170]\n",
      "epoch:11 step:11148 [D loss: 0.612229, acc: 64.84%] [G loss: 2.107105]\n",
      "epoch:11 step:11149 [D loss: 0.585822, acc: 71.09%] [G loss: 2.117280]\n",
      "epoch:11 step:11150 [D loss: 0.636361, acc: 62.50%] [G loss: 2.059988]\n",
      "epoch:11 step:11151 [D loss: 0.658646, acc: 64.06%] [G loss: 2.020105]\n",
      "epoch:11 step:11152 [D loss: 0.591428, acc: 71.09%] [G loss: 2.064070]\n",
      "epoch:11 step:11153 [D loss: 0.607961, acc: 66.41%] [G loss: 1.948234]\n",
      "epoch:11 step:11154 [D loss: 0.632536, acc: 64.06%] [G loss: 1.964431]\n",
      "epoch:11 step:11155 [D loss: 0.615817, acc: 67.19%] [G loss: 1.989620]\n",
      "epoch:11 step:11156 [D loss: 0.625396, acc: 64.06%] [G loss: 2.107617]\n",
      "epoch:11 step:11157 [D loss: 0.663803, acc: 56.25%] [G loss: 1.993043]\n",
      "epoch:11 step:11158 [D loss: 0.665507, acc: 64.84%] [G loss: 1.746569]\n",
      "epoch:11 step:11159 [D loss: 0.653835, acc: 60.16%] [G loss: 1.979962]\n",
      "epoch:11 step:11160 [D loss: 0.646806, acc: 60.16%] [G loss: 2.009959]\n",
      "epoch:11 step:11161 [D loss: 0.652120, acc: 62.50%] [G loss: 1.910296]\n",
      "epoch:11 step:11162 [D loss: 0.651011, acc: 59.38%] [G loss: 2.092142]\n",
      "epoch:11 step:11163 [D loss: 0.647727, acc: 60.94%] [G loss: 1.903487]\n",
      "epoch:11 step:11164 [D loss: 0.642873, acc: 60.16%] [G loss: 2.015874]\n",
      "epoch:11 step:11165 [D loss: 0.683474, acc: 59.38%] [G loss: 1.847837]\n",
      "epoch:11 step:11166 [D loss: 0.646850, acc: 64.84%] [G loss: 1.816151]\n",
      "epoch:11 step:11167 [D loss: 0.630596, acc: 67.19%] [G loss: 2.167262]\n",
      "epoch:11 step:11168 [D loss: 0.648177, acc: 63.28%] [G loss: 1.814243]\n",
      "epoch:11 step:11169 [D loss: 0.624911, acc: 64.84%] [G loss: 1.875636]\n",
      "epoch:11 step:11170 [D loss: 0.599465, acc: 68.75%] [G loss: 2.041563]\n",
      "epoch:11 step:11171 [D loss: 0.614519, acc: 64.84%] [G loss: 1.922501]\n",
      "epoch:11 step:11172 [D loss: 0.706607, acc: 60.94%] [G loss: 1.943019]\n",
      "epoch:11 step:11173 [D loss: 0.646889, acc: 63.28%] [G loss: 1.938695]\n",
      "epoch:11 step:11174 [D loss: 0.646692, acc: 67.19%] [G loss: 1.844579]\n",
      "epoch:11 step:11175 [D loss: 0.625451, acc: 64.06%] [G loss: 1.995808]\n",
      "epoch:11 step:11176 [D loss: 0.659304, acc: 62.50%] [G loss: 1.888143]\n",
      "epoch:11 step:11177 [D loss: 0.657032, acc: 60.16%] [G loss: 1.820611]\n",
      "epoch:11 step:11178 [D loss: 0.681818, acc: 62.50%] [G loss: 2.224979]\n",
      "epoch:11 step:11179 [D loss: 0.631979, acc: 64.06%] [G loss: 2.041611]\n",
      "epoch:11 step:11180 [D loss: 0.631313, acc: 64.06%] [G loss: 1.804006]\n",
      "epoch:11 step:11181 [D loss: 0.617756, acc: 64.06%] [G loss: 1.825262]\n",
      "epoch:11 step:11182 [D loss: 0.633826, acc: 60.94%] [G loss: 2.023623]\n",
      "epoch:11 step:11183 [D loss: 0.648621, acc: 61.72%] [G loss: 2.067351]\n",
      "epoch:11 step:11184 [D loss: 0.659183, acc: 58.59%] [G loss: 2.106680]\n",
      "epoch:11 step:11185 [D loss: 0.651532, acc: 64.06%] [G loss: 2.069076]\n",
      "epoch:11 step:11186 [D loss: 0.648416, acc: 61.72%] [G loss: 1.800813]\n",
      "epoch:11 step:11187 [D loss: 0.664717, acc: 60.94%] [G loss: 1.921553]\n",
      "epoch:11 step:11188 [D loss: 0.644181, acc: 63.28%] [G loss: 1.979958]\n",
      "epoch:11 step:11189 [D loss: 0.630852, acc: 62.50%] [G loss: 1.981646]\n",
      "epoch:11 step:11190 [D loss: 0.590355, acc: 67.97%] [G loss: 1.907647]\n",
      "epoch:11 step:11191 [D loss: 0.616173, acc: 59.38%] [G loss: 2.176803]\n",
      "epoch:11 step:11192 [D loss: 0.627927, acc: 64.84%] [G loss: 1.978834]\n",
      "epoch:11 step:11193 [D loss: 0.607479, acc: 67.19%] [G loss: 2.137586]\n",
      "epoch:11 step:11194 [D loss: 0.641826, acc: 63.28%] [G loss: 2.036592]\n",
      "epoch:11 step:11195 [D loss: 0.635354, acc: 63.28%] [G loss: 2.044862]\n",
      "epoch:11 step:11196 [D loss: 0.631212, acc: 64.06%] [G loss: 1.913379]\n",
      "epoch:11 step:11197 [D loss: 0.591053, acc: 68.75%] [G loss: 2.213848]\n",
      "epoch:11 step:11198 [D loss: 0.654129, acc: 65.62%] [G loss: 1.927345]\n",
      "epoch:11 step:11199 [D loss: 0.676328, acc: 60.16%] [G loss: 1.951197]\n",
      "epoch:11 step:11200 [D loss: 0.597660, acc: 70.31%] [G loss: 2.120903]\n",
      "##############\n",
      "[2.55380917 1.45207582 6.13797959 4.88841259 3.64957311 5.79624322\n",
      " 4.35382648 4.72688594 4.46169018 3.56954848]\n",
      "##########\n",
      "epoch:11 step:11201 [D loss: 0.635099, acc: 60.94%] [G loss: 2.101371]\n",
      "epoch:11 step:11202 [D loss: 0.657357, acc: 64.84%] [G loss: 2.284198]\n",
      "epoch:11 step:11203 [D loss: 0.641443, acc: 63.28%] [G loss: 1.997020]\n",
      "epoch:11 step:11204 [D loss: 0.593690, acc: 67.97%] [G loss: 1.993491]\n",
      "epoch:11 step:11205 [D loss: 0.613833, acc: 67.97%] [G loss: 1.967998]\n",
      "epoch:11 step:11206 [D loss: 0.635322, acc: 63.28%] [G loss: 2.289654]\n",
      "epoch:11 step:11207 [D loss: 0.611566, acc: 67.97%] [G loss: 1.988054]\n",
      "epoch:11 step:11208 [D loss: 0.614226, acc: 67.97%] [G loss: 2.077248]\n",
      "epoch:11 step:11209 [D loss: 0.616253, acc: 63.28%] [G loss: 1.950471]\n",
      "epoch:11 step:11210 [D loss: 0.655433, acc: 57.03%] [G loss: 2.063146]\n",
      "epoch:11 step:11211 [D loss: 0.595536, acc: 64.06%] [G loss: 2.132405]\n",
      "epoch:11 step:11212 [D loss: 0.597347, acc: 66.41%] [G loss: 2.060258]\n",
      "epoch:11 step:11213 [D loss: 0.646370, acc: 63.28%] [G loss: 2.013780]\n",
      "epoch:11 step:11214 [D loss: 0.645970, acc: 63.28%] [G loss: 2.273871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11215 [D loss: 0.622645, acc: 64.06%] [G loss: 2.075446]\n",
      "epoch:11 step:11216 [D loss: 0.614170, acc: 65.62%] [G loss: 2.155155]\n",
      "epoch:11 step:11217 [D loss: 0.619560, acc: 63.28%] [G loss: 2.068891]\n",
      "epoch:11 step:11218 [D loss: 0.607152, acc: 65.62%] [G loss: 2.040401]\n",
      "epoch:11 step:11219 [D loss: 0.623793, acc: 63.28%] [G loss: 2.217326]\n",
      "epoch:11 step:11220 [D loss: 0.677917, acc: 61.72%] [G loss: 2.258564]\n",
      "epoch:11 step:11221 [D loss: 0.642840, acc: 66.41%] [G loss: 2.108767]\n",
      "epoch:11 step:11222 [D loss: 0.635525, acc: 64.84%] [G loss: 2.093004]\n",
      "epoch:11 step:11223 [D loss: 0.605360, acc: 66.41%] [G loss: 1.925015]\n",
      "epoch:11 step:11224 [D loss: 0.699705, acc: 57.81%] [G loss: 2.016929]\n",
      "epoch:11 step:11225 [D loss: 0.577756, acc: 70.31%] [G loss: 2.248969]\n",
      "epoch:11 step:11226 [D loss: 0.566342, acc: 73.44%] [G loss: 2.236855]\n",
      "epoch:11 step:11227 [D loss: 0.715729, acc: 53.91%] [G loss: 1.969115]\n",
      "epoch:11 step:11228 [D loss: 0.555156, acc: 77.34%] [G loss: 2.294542]\n",
      "epoch:11 step:11229 [D loss: 0.635792, acc: 61.72%] [G loss: 1.981265]\n",
      "epoch:11 step:11230 [D loss: 0.567166, acc: 69.53%] [G loss: 2.085241]\n",
      "epoch:11 step:11231 [D loss: 0.632577, acc: 63.28%] [G loss: 2.279911]\n",
      "epoch:11 step:11232 [D loss: 0.569923, acc: 69.53%] [G loss: 2.264617]\n",
      "epoch:11 step:11233 [D loss: 0.614416, acc: 66.41%] [G loss: 2.269327]\n",
      "epoch:11 step:11234 [D loss: 0.640772, acc: 60.16%] [G loss: 2.263807]\n",
      "epoch:11 step:11235 [D loss: 0.841687, acc: 50.00%] [G loss: 2.058986]\n",
      "epoch:11 step:11236 [D loss: 0.698579, acc: 57.81%] [G loss: 2.065908]\n",
      "epoch:11 step:11237 [D loss: 0.585098, acc: 71.09%] [G loss: 2.011553]\n",
      "epoch:11 step:11238 [D loss: 0.665388, acc: 59.38%] [G loss: 1.975463]\n",
      "epoch:11 step:11239 [D loss: 0.680838, acc: 63.28%] [G loss: 1.913911]\n",
      "epoch:11 step:11240 [D loss: 0.640794, acc: 64.84%] [G loss: 1.972777]\n",
      "epoch:11 step:11241 [D loss: 0.575049, acc: 71.09%] [G loss: 2.182113]\n",
      "epoch:11 step:11242 [D loss: 0.595470, acc: 65.62%] [G loss: 2.011542]\n",
      "epoch:11 step:11243 [D loss: 0.589313, acc: 72.66%] [G loss: 2.044737]\n",
      "epoch:11 step:11244 [D loss: 0.544038, acc: 74.22%] [G loss: 2.576193]\n",
      "epoch:12 step:11245 [D loss: 0.661972, acc: 60.94%] [G loss: 2.000266]\n",
      "epoch:12 step:11246 [D loss: 0.594114, acc: 63.28%] [G loss: 2.024595]\n",
      "epoch:12 step:11247 [D loss: 0.669233, acc: 57.81%] [G loss: 2.154174]\n",
      "epoch:12 step:11248 [D loss: 0.599082, acc: 66.41%] [G loss: 2.146024]\n",
      "epoch:12 step:11249 [D loss: 0.619672, acc: 67.19%] [G loss: 1.966386]\n",
      "epoch:12 step:11250 [D loss: 0.649035, acc: 64.84%] [G loss: 1.982078]\n",
      "epoch:12 step:11251 [D loss: 0.569250, acc: 72.66%] [G loss: 2.039061]\n",
      "epoch:12 step:11252 [D loss: 0.680199, acc: 59.38%] [G loss: 2.099975]\n",
      "epoch:12 step:11253 [D loss: 0.565964, acc: 73.44%] [G loss: 2.208314]\n",
      "epoch:12 step:11254 [D loss: 0.580398, acc: 71.88%] [G loss: 2.115715]\n",
      "epoch:12 step:11255 [D loss: 0.616025, acc: 66.41%] [G loss: 2.107782]\n",
      "epoch:12 step:11256 [D loss: 0.607409, acc: 68.75%] [G loss: 2.134854]\n",
      "epoch:12 step:11257 [D loss: 0.575135, acc: 67.19%] [G loss: 2.070088]\n",
      "epoch:12 step:11258 [D loss: 0.670237, acc: 60.94%] [G loss: 2.221214]\n",
      "epoch:12 step:11259 [D loss: 0.563153, acc: 71.09%] [G loss: 2.136674]\n",
      "epoch:12 step:11260 [D loss: 0.592487, acc: 67.19%] [G loss: 2.223339]\n",
      "epoch:12 step:11261 [D loss: 0.655760, acc: 63.28%] [G loss: 1.935467]\n",
      "epoch:12 step:11262 [D loss: 0.693118, acc: 55.47%] [G loss: 2.159141]\n",
      "epoch:12 step:11263 [D loss: 0.638481, acc: 63.28%] [G loss: 1.878328]\n",
      "epoch:12 step:11264 [D loss: 0.623424, acc: 67.97%] [G loss: 1.967322]\n",
      "epoch:12 step:11265 [D loss: 0.592834, acc: 64.84%] [G loss: 2.145751]\n",
      "epoch:12 step:11266 [D loss: 0.646770, acc: 63.28%] [G loss: 2.213535]\n",
      "epoch:12 step:11267 [D loss: 0.660985, acc: 61.72%] [G loss: 2.193844]\n",
      "epoch:12 step:11268 [D loss: 0.638586, acc: 60.16%] [G loss: 2.297024]\n",
      "epoch:12 step:11269 [D loss: 0.584117, acc: 69.53%] [G loss: 2.255164]\n",
      "epoch:12 step:11270 [D loss: 0.647580, acc: 64.06%] [G loss: 1.971829]\n",
      "epoch:12 step:11271 [D loss: 0.700848, acc: 57.81%] [G loss: 2.071820]\n",
      "epoch:12 step:11272 [D loss: 0.647542, acc: 62.50%] [G loss: 1.926733]\n",
      "epoch:12 step:11273 [D loss: 0.611445, acc: 67.97%] [G loss: 2.067056]\n",
      "epoch:12 step:11274 [D loss: 0.635154, acc: 63.28%] [G loss: 2.084127]\n",
      "epoch:12 step:11275 [D loss: 0.691941, acc: 58.59%] [G loss: 1.956797]\n",
      "epoch:12 step:11276 [D loss: 0.700713, acc: 56.25%] [G loss: 1.886471]\n",
      "epoch:12 step:11277 [D loss: 0.667831, acc: 63.28%] [G loss: 1.889588]\n",
      "epoch:12 step:11278 [D loss: 0.690230, acc: 55.47%] [G loss: 1.887190]\n",
      "epoch:12 step:11279 [D loss: 0.573730, acc: 69.53%] [G loss: 1.871332]\n",
      "epoch:12 step:11280 [D loss: 0.650545, acc: 63.28%] [G loss: 2.138449]\n",
      "epoch:12 step:11281 [D loss: 0.567732, acc: 69.53%] [G loss: 1.996576]\n",
      "epoch:12 step:11282 [D loss: 0.662568, acc: 61.72%] [G loss: 1.908626]\n",
      "epoch:12 step:11283 [D loss: 0.632593, acc: 67.97%] [G loss: 2.058620]\n",
      "epoch:12 step:11284 [D loss: 0.582017, acc: 71.88%] [G loss: 2.176192]\n",
      "epoch:12 step:11285 [D loss: 0.624584, acc: 64.06%] [G loss: 1.960837]\n",
      "epoch:12 step:11286 [D loss: 0.604784, acc: 67.19%] [G loss: 2.175910]\n",
      "epoch:12 step:11287 [D loss: 0.599436, acc: 67.97%] [G loss: 2.067022]\n",
      "epoch:12 step:11288 [D loss: 0.700623, acc: 63.28%] [G loss: 1.915397]\n",
      "epoch:12 step:11289 [D loss: 0.617549, acc: 61.72%] [G loss: 2.090572]\n",
      "epoch:12 step:11290 [D loss: 0.694249, acc: 54.69%] [G loss: 2.008711]\n",
      "epoch:12 step:11291 [D loss: 0.574477, acc: 64.06%] [G loss: 2.170277]\n",
      "epoch:12 step:11292 [D loss: 0.591636, acc: 70.31%] [G loss: 2.143614]\n",
      "epoch:12 step:11293 [D loss: 0.665233, acc: 59.38%] [G loss: 1.995725]\n",
      "epoch:12 step:11294 [D loss: 0.586439, acc: 71.88%] [G loss: 2.118813]\n",
      "epoch:12 step:11295 [D loss: 0.668197, acc: 60.94%] [G loss: 2.004185]\n",
      "epoch:12 step:11296 [D loss: 0.678515, acc: 57.03%] [G loss: 2.028180]\n",
      "epoch:12 step:11297 [D loss: 0.671514, acc: 58.59%] [G loss: 2.058579]\n",
      "epoch:12 step:11298 [D loss: 0.576924, acc: 67.97%] [G loss: 2.013931]\n",
      "epoch:12 step:11299 [D loss: 0.575354, acc: 71.09%] [G loss: 2.107493]\n",
      "epoch:12 step:11300 [D loss: 0.686443, acc: 57.81%] [G loss: 2.222405]\n",
      "epoch:12 step:11301 [D loss: 0.602629, acc: 67.19%] [G loss: 2.067429]\n",
      "epoch:12 step:11302 [D loss: 0.630831, acc: 67.19%] [G loss: 2.158113]\n",
      "epoch:12 step:11303 [D loss: 0.647616, acc: 63.28%] [G loss: 2.106579]\n",
      "epoch:12 step:11304 [D loss: 0.651949, acc: 53.91%] [G loss: 1.890939]\n",
      "epoch:12 step:11305 [D loss: 0.720987, acc: 55.47%] [G loss: 1.928296]\n",
      "epoch:12 step:11306 [D loss: 0.650908, acc: 64.84%] [G loss: 2.108362]\n",
      "epoch:12 step:11307 [D loss: 0.641151, acc: 65.62%] [G loss: 2.190567]\n",
      "epoch:12 step:11308 [D loss: 0.669981, acc: 59.38%] [G loss: 2.123572]\n",
      "epoch:12 step:11309 [D loss: 0.704095, acc: 57.81%] [G loss: 1.891006]\n",
      "epoch:12 step:11310 [D loss: 0.617986, acc: 70.31%] [G loss: 2.021158]\n",
      "epoch:12 step:11311 [D loss: 0.631163, acc: 60.94%] [G loss: 1.916472]\n",
      "epoch:12 step:11312 [D loss: 0.623655, acc: 67.19%] [G loss: 1.947227]\n",
      "epoch:12 step:11313 [D loss: 0.644795, acc: 60.94%] [G loss: 2.078804]\n",
      "epoch:12 step:11314 [D loss: 0.599717, acc: 67.97%] [G loss: 1.868041]\n",
      "epoch:12 step:11315 [D loss: 0.650378, acc: 62.50%] [G loss: 2.047360]\n",
      "epoch:12 step:11316 [D loss: 0.683363, acc: 58.59%] [G loss: 2.083422]\n",
      "epoch:12 step:11317 [D loss: 0.666928, acc: 58.59%] [G loss: 1.892967]\n",
      "epoch:12 step:11318 [D loss: 0.615878, acc: 63.28%] [G loss: 2.055418]\n",
      "epoch:12 step:11319 [D loss: 0.647224, acc: 60.94%] [G loss: 2.196082]\n",
      "epoch:12 step:11320 [D loss: 0.663912, acc: 62.50%] [G loss: 2.091321]\n",
      "epoch:12 step:11321 [D loss: 0.596090, acc: 69.53%] [G loss: 2.084970]\n",
      "epoch:12 step:11322 [D loss: 0.670540, acc: 58.59%] [G loss: 1.797581]\n",
      "epoch:12 step:11323 [D loss: 0.656366, acc: 64.84%] [G loss: 1.924835]\n",
      "epoch:12 step:11324 [D loss: 0.692838, acc: 57.81%] [G loss: 1.906817]\n",
      "epoch:12 step:11325 [D loss: 0.670710, acc: 57.81%] [G loss: 1.894511]\n",
      "epoch:12 step:11326 [D loss: 0.646977, acc: 60.94%] [G loss: 1.935359]\n",
      "epoch:12 step:11327 [D loss: 0.615861, acc: 62.50%] [G loss: 2.109921]\n",
      "epoch:12 step:11328 [D loss: 0.592248, acc: 67.97%] [G loss: 2.087048]\n",
      "epoch:12 step:11329 [D loss: 0.647283, acc: 67.19%] [G loss: 2.018952]\n",
      "epoch:12 step:11330 [D loss: 0.687606, acc: 64.06%] [G loss: 1.947490]\n",
      "epoch:12 step:11331 [D loss: 0.637001, acc: 63.28%] [G loss: 1.849766]\n",
      "epoch:12 step:11332 [D loss: 0.577201, acc: 74.22%] [G loss: 1.981783]\n",
      "epoch:12 step:11333 [D loss: 0.668598, acc: 59.38%] [G loss: 2.091096]\n",
      "epoch:12 step:11334 [D loss: 0.669938, acc: 63.28%] [G loss: 1.949712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11335 [D loss: 0.679339, acc: 56.25%] [G loss: 1.977804]\n",
      "epoch:12 step:11336 [D loss: 0.578706, acc: 69.53%] [G loss: 2.156232]\n",
      "epoch:12 step:11337 [D loss: 0.628982, acc: 64.84%] [G loss: 1.957169]\n",
      "epoch:12 step:11338 [D loss: 0.634400, acc: 61.72%] [G loss: 2.046997]\n",
      "epoch:12 step:11339 [D loss: 0.642396, acc: 69.53%] [G loss: 1.846848]\n",
      "epoch:12 step:11340 [D loss: 0.605301, acc: 71.09%] [G loss: 1.953211]\n",
      "epoch:12 step:11341 [D loss: 0.634090, acc: 64.06%] [G loss: 2.049424]\n",
      "epoch:12 step:11342 [D loss: 0.663344, acc: 57.81%] [G loss: 1.909318]\n",
      "epoch:12 step:11343 [D loss: 0.620456, acc: 72.66%] [G loss: 1.898770]\n",
      "epoch:12 step:11344 [D loss: 0.598958, acc: 67.97%] [G loss: 2.020992]\n",
      "epoch:12 step:11345 [D loss: 0.616811, acc: 63.28%] [G loss: 2.079341]\n",
      "epoch:12 step:11346 [D loss: 0.635773, acc: 63.28%] [G loss: 1.974575]\n",
      "epoch:12 step:11347 [D loss: 0.589395, acc: 65.62%] [G loss: 2.093277]\n",
      "epoch:12 step:11348 [D loss: 0.697433, acc: 55.47%] [G loss: 1.996664]\n",
      "epoch:12 step:11349 [D loss: 0.612730, acc: 64.06%] [G loss: 2.007926]\n",
      "epoch:12 step:11350 [D loss: 0.660014, acc: 60.16%] [G loss: 2.214398]\n",
      "epoch:12 step:11351 [D loss: 0.695448, acc: 61.72%] [G loss: 1.910199]\n",
      "epoch:12 step:11352 [D loss: 0.678295, acc: 60.16%] [G loss: 2.071571]\n",
      "epoch:12 step:11353 [D loss: 0.629732, acc: 64.06%] [G loss: 2.035934]\n",
      "epoch:12 step:11354 [D loss: 0.676030, acc: 57.81%] [G loss: 1.967814]\n",
      "epoch:12 step:11355 [D loss: 0.606840, acc: 67.97%] [G loss: 1.995207]\n",
      "epoch:12 step:11356 [D loss: 0.635686, acc: 63.28%] [G loss: 1.949808]\n",
      "epoch:12 step:11357 [D loss: 0.716606, acc: 54.69%] [G loss: 1.911991]\n",
      "epoch:12 step:11358 [D loss: 0.698167, acc: 58.59%] [G loss: 1.934071]\n",
      "epoch:12 step:11359 [D loss: 0.655093, acc: 69.53%] [G loss: 2.232058]\n",
      "epoch:12 step:11360 [D loss: 0.594782, acc: 68.75%] [G loss: 2.071843]\n",
      "epoch:12 step:11361 [D loss: 0.603483, acc: 67.97%] [G loss: 2.167947]\n",
      "epoch:12 step:11362 [D loss: 0.604109, acc: 64.84%] [G loss: 2.004863]\n",
      "epoch:12 step:11363 [D loss: 0.584522, acc: 68.75%] [G loss: 2.129660]\n",
      "epoch:12 step:11364 [D loss: 0.664073, acc: 62.50%] [G loss: 2.105517]\n",
      "epoch:12 step:11365 [D loss: 0.613562, acc: 67.19%] [G loss: 2.070168]\n",
      "epoch:12 step:11366 [D loss: 0.696013, acc: 60.94%] [G loss: 2.172808]\n",
      "epoch:12 step:11367 [D loss: 0.648675, acc: 57.03%] [G loss: 2.033898]\n",
      "epoch:12 step:11368 [D loss: 0.633627, acc: 61.72%] [G loss: 2.030260]\n",
      "epoch:12 step:11369 [D loss: 0.663905, acc: 61.72%] [G loss: 1.832694]\n",
      "epoch:12 step:11370 [D loss: 0.628965, acc: 65.62%] [G loss: 2.048561]\n",
      "epoch:12 step:11371 [D loss: 0.681385, acc: 55.47%] [G loss: 1.917730]\n",
      "epoch:12 step:11372 [D loss: 0.614590, acc: 67.19%] [G loss: 1.891540]\n",
      "epoch:12 step:11373 [D loss: 0.621950, acc: 63.28%] [G loss: 1.930485]\n",
      "epoch:12 step:11374 [D loss: 0.613800, acc: 64.84%] [G loss: 2.092246]\n",
      "epoch:12 step:11375 [D loss: 0.621737, acc: 61.72%] [G loss: 2.007488]\n",
      "epoch:12 step:11376 [D loss: 0.615608, acc: 66.41%] [G loss: 2.024179]\n",
      "epoch:12 step:11377 [D loss: 0.673754, acc: 53.12%] [G loss: 1.762330]\n",
      "epoch:12 step:11378 [D loss: 0.643687, acc: 61.72%] [G loss: 1.801682]\n",
      "epoch:12 step:11379 [D loss: 0.626793, acc: 64.84%] [G loss: 1.985735]\n",
      "epoch:12 step:11380 [D loss: 0.645398, acc: 63.28%] [G loss: 1.933679]\n",
      "epoch:12 step:11381 [D loss: 0.645999, acc: 65.62%] [G loss: 1.974245]\n",
      "epoch:12 step:11382 [D loss: 0.604489, acc: 64.06%] [G loss: 1.946001]\n",
      "epoch:12 step:11383 [D loss: 0.618492, acc: 63.28%] [G loss: 1.917364]\n",
      "epoch:12 step:11384 [D loss: 0.660081, acc: 59.38%] [G loss: 2.171906]\n",
      "epoch:12 step:11385 [D loss: 0.627323, acc: 64.06%] [G loss: 1.851903]\n",
      "epoch:12 step:11386 [D loss: 0.627011, acc: 67.97%] [G loss: 1.984828]\n",
      "epoch:12 step:11387 [D loss: 0.696863, acc: 58.59%] [G loss: 1.974945]\n",
      "epoch:12 step:11388 [D loss: 0.622497, acc: 71.09%] [G loss: 2.119388]\n",
      "epoch:12 step:11389 [D loss: 0.615153, acc: 66.41%] [G loss: 2.013621]\n",
      "epoch:12 step:11390 [D loss: 0.582700, acc: 71.88%] [G loss: 2.069476]\n",
      "epoch:12 step:11391 [D loss: 0.653557, acc: 61.72%] [G loss: 2.043706]\n",
      "epoch:12 step:11392 [D loss: 0.638925, acc: 65.62%] [G loss: 2.034769]\n",
      "epoch:12 step:11393 [D loss: 0.674627, acc: 57.81%] [G loss: 2.020415]\n",
      "epoch:12 step:11394 [D loss: 0.564131, acc: 70.31%] [G loss: 2.247864]\n",
      "epoch:12 step:11395 [D loss: 0.589293, acc: 75.78%] [G loss: 2.224543]\n",
      "epoch:12 step:11396 [D loss: 0.571353, acc: 70.31%] [G loss: 2.197747]\n",
      "epoch:12 step:11397 [D loss: 0.572271, acc: 72.66%] [G loss: 1.998442]\n",
      "epoch:12 step:11398 [D loss: 0.633620, acc: 65.62%] [G loss: 2.150356]\n",
      "epoch:12 step:11399 [D loss: 0.586041, acc: 71.88%] [G loss: 2.187848]\n",
      "epoch:12 step:11400 [D loss: 0.615279, acc: 60.94%] [G loss: 2.022322]\n",
      "##############\n",
      "[2.59363978 1.38322579 6.45651877 4.81308156 3.81403238 5.69387446\n",
      " 4.45171987 4.75516662 4.71058328 3.71837693]\n",
      "##########\n",
      "epoch:12 step:11401 [D loss: 0.642031, acc: 63.28%] [G loss: 1.863573]\n",
      "epoch:12 step:11402 [D loss: 0.623425, acc: 65.62%] [G loss: 1.950530]\n",
      "epoch:12 step:11403 [D loss: 0.638916, acc: 66.41%] [G loss: 1.888200]\n",
      "epoch:12 step:11404 [D loss: 0.665403, acc: 60.94%] [G loss: 1.829351]\n",
      "epoch:12 step:11405 [D loss: 0.617160, acc: 67.97%] [G loss: 1.972020]\n",
      "epoch:12 step:11406 [D loss: 0.629940, acc: 64.06%] [G loss: 1.950606]\n",
      "epoch:12 step:11407 [D loss: 0.641879, acc: 60.94%] [G loss: 2.015705]\n",
      "epoch:12 step:11408 [D loss: 0.609480, acc: 67.97%] [G loss: 1.894915]\n",
      "epoch:12 step:11409 [D loss: 0.648187, acc: 63.28%] [G loss: 2.155691]\n",
      "epoch:12 step:11410 [D loss: 0.639123, acc: 64.84%] [G loss: 1.919511]\n",
      "epoch:12 step:11411 [D loss: 0.641949, acc: 63.28%] [G loss: 1.922776]\n",
      "epoch:12 step:11412 [D loss: 0.654686, acc: 58.59%] [G loss: 2.137455]\n",
      "epoch:12 step:11413 [D loss: 0.676127, acc: 58.59%] [G loss: 1.824451]\n",
      "epoch:12 step:11414 [D loss: 0.664100, acc: 61.72%] [G loss: 1.998163]\n",
      "epoch:12 step:11415 [D loss: 0.565765, acc: 72.66%] [G loss: 2.057151]\n",
      "epoch:12 step:11416 [D loss: 0.663565, acc: 60.16%] [G loss: 1.983675]\n",
      "epoch:12 step:11417 [D loss: 0.621893, acc: 67.97%] [G loss: 1.968768]\n",
      "epoch:12 step:11418 [D loss: 0.627314, acc: 66.41%] [G loss: 1.956314]\n",
      "epoch:12 step:11419 [D loss: 0.631710, acc: 63.28%] [G loss: 2.048547]\n",
      "epoch:12 step:11420 [D loss: 0.653509, acc: 63.28%] [G loss: 1.941736]\n",
      "epoch:12 step:11421 [D loss: 0.659667, acc: 64.84%] [G loss: 1.847185]\n",
      "epoch:12 step:11422 [D loss: 0.654602, acc: 58.59%] [G loss: 1.909352]\n",
      "epoch:12 step:11423 [D loss: 0.670660, acc: 59.38%] [G loss: 1.817368]\n",
      "epoch:12 step:11424 [D loss: 0.696765, acc: 58.59%] [G loss: 1.788961]\n",
      "epoch:12 step:11425 [D loss: 0.675420, acc: 59.38%] [G loss: 1.882637]\n",
      "epoch:12 step:11426 [D loss: 0.673776, acc: 63.28%] [G loss: 1.989680]\n",
      "epoch:12 step:11427 [D loss: 0.647111, acc: 62.50%] [G loss: 1.971521]\n",
      "epoch:12 step:11428 [D loss: 0.629289, acc: 64.06%] [G loss: 2.088035]\n",
      "epoch:12 step:11429 [D loss: 0.638806, acc: 65.62%] [G loss: 1.985224]\n",
      "epoch:12 step:11430 [D loss: 0.638223, acc: 58.59%] [G loss: 1.925590]\n",
      "epoch:12 step:11431 [D loss: 0.605862, acc: 67.97%] [G loss: 1.992133]\n",
      "epoch:12 step:11432 [D loss: 0.630729, acc: 60.94%] [G loss: 1.995484]\n",
      "epoch:12 step:11433 [D loss: 0.677486, acc: 58.59%] [G loss: 1.823121]\n",
      "epoch:12 step:11434 [D loss: 0.610965, acc: 65.62%] [G loss: 2.099611]\n",
      "epoch:12 step:11435 [D loss: 0.620587, acc: 67.19%] [G loss: 2.047437]\n",
      "epoch:12 step:11436 [D loss: 0.590998, acc: 67.97%] [G loss: 2.153034]\n",
      "epoch:12 step:11437 [D loss: 0.617196, acc: 59.38%] [G loss: 2.045450]\n",
      "epoch:12 step:11438 [D loss: 0.634459, acc: 65.62%] [G loss: 2.095178]\n",
      "epoch:12 step:11439 [D loss: 0.607912, acc: 71.88%] [G loss: 2.069926]\n",
      "epoch:12 step:11440 [D loss: 0.645826, acc: 60.94%] [G loss: 1.853951]\n",
      "epoch:12 step:11441 [D loss: 0.602302, acc: 71.09%] [G loss: 2.272558]\n",
      "epoch:12 step:11442 [D loss: 0.629840, acc: 66.41%] [G loss: 2.109011]\n",
      "epoch:12 step:11443 [D loss: 0.679328, acc: 57.81%] [G loss: 2.012345]\n",
      "epoch:12 step:11444 [D loss: 0.716405, acc: 54.69%] [G loss: 1.789509]\n",
      "epoch:12 step:11445 [D loss: 0.691926, acc: 53.12%] [G loss: 1.966481]\n",
      "epoch:12 step:11446 [D loss: 0.624522, acc: 61.72%] [G loss: 1.822775]\n",
      "epoch:12 step:11447 [D loss: 0.644029, acc: 61.72%] [G loss: 1.810564]\n",
      "epoch:12 step:11448 [D loss: 0.606421, acc: 68.75%] [G loss: 1.864021]\n",
      "epoch:12 step:11449 [D loss: 0.638700, acc: 59.38%] [G loss: 1.859195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11450 [D loss: 0.692421, acc: 60.16%] [G loss: 2.086216]\n",
      "epoch:12 step:11451 [D loss: 0.592449, acc: 68.75%] [G loss: 2.125254]\n",
      "epoch:12 step:11452 [D loss: 0.515721, acc: 78.12%] [G loss: 2.394917]\n",
      "epoch:12 step:11453 [D loss: 0.584852, acc: 68.75%] [G loss: 2.309687]\n",
      "epoch:12 step:11454 [D loss: 0.636066, acc: 67.19%] [G loss: 2.010430]\n",
      "epoch:12 step:11455 [D loss: 0.708897, acc: 56.25%] [G loss: 1.898113]\n",
      "epoch:12 step:11456 [D loss: 0.624756, acc: 64.84%] [G loss: 2.052975]\n",
      "epoch:12 step:11457 [D loss: 0.690533, acc: 56.25%] [G loss: 1.925856]\n",
      "epoch:12 step:11458 [D loss: 0.671373, acc: 59.38%] [G loss: 1.859411]\n",
      "epoch:12 step:11459 [D loss: 0.731210, acc: 53.91%] [G loss: 1.773784]\n",
      "epoch:12 step:11460 [D loss: 0.633866, acc: 62.50%] [G loss: 2.055829]\n",
      "epoch:12 step:11461 [D loss: 0.599799, acc: 66.41%] [G loss: 2.013480]\n",
      "epoch:12 step:11462 [D loss: 0.603421, acc: 67.97%] [G loss: 2.028301]\n",
      "epoch:12 step:11463 [D loss: 0.576876, acc: 69.53%] [G loss: 2.314852]\n",
      "epoch:12 step:11464 [D loss: 0.687101, acc: 60.16%] [G loss: 1.860744]\n",
      "epoch:12 step:11465 [D loss: 0.658167, acc: 60.16%] [G loss: 2.077220]\n",
      "epoch:12 step:11466 [D loss: 0.624504, acc: 60.16%] [G loss: 2.149191]\n",
      "epoch:12 step:11467 [D loss: 0.601729, acc: 67.97%] [G loss: 2.055865]\n",
      "epoch:12 step:11468 [D loss: 0.601006, acc: 71.09%] [G loss: 2.028202]\n",
      "epoch:12 step:11469 [D loss: 0.669351, acc: 63.28%] [G loss: 1.889794]\n",
      "epoch:12 step:11470 [D loss: 0.650314, acc: 64.84%] [G loss: 1.971689]\n",
      "epoch:12 step:11471 [D loss: 0.673555, acc: 61.72%] [G loss: 1.979330]\n",
      "epoch:12 step:11472 [D loss: 0.661005, acc: 57.03%] [G loss: 1.899833]\n",
      "epoch:12 step:11473 [D loss: 0.567089, acc: 72.66%] [G loss: 2.202024]\n",
      "epoch:12 step:11474 [D loss: 0.611239, acc: 67.19%] [G loss: 2.235316]\n",
      "epoch:12 step:11475 [D loss: 0.539364, acc: 72.66%] [G loss: 2.418029]\n",
      "epoch:12 step:11476 [D loss: 0.531498, acc: 78.12%] [G loss: 2.473099]\n",
      "epoch:12 step:11477 [D loss: 0.704345, acc: 56.25%] [G loss: 1.943988]\n",
      "epoch:12 step:11478 [D loss: 0.639570, acc: 67.97%] [G loss: 2.094986]\n",
      "epoch:12 step:11479 [D loss: 0.658350, acc: 65.62%] [G loss: 1.942322]\n",
      "epoch:12 step:11480 [D loss: 0.642911, acc: 60.16%] [G loss: 2.038066]\n",
      "epoch:12 step:11481 [D loss: 0.625462, acc: 65.62%] [G loss: 1.957923]\n",
      "epoch:12 step:11482 [D loss: 0.554837, acc: 71.88%] [G loss: 2.127057]\n",
      "epoch:12 step:11483 [D loss: 0.674414, acc: 61.72%] [G loss: 1.871957]\n",
      "epoch:12 step:11484 [D loss: 0.632665, acc: 62.50%] [G loss: 1.995741]\n",
      "epoch:12 step:11485 [D loss: 0.636596, acc: 71.88%] [G loss: 2.188056]\n",
      "epoch:12 step:11486 [D loss: 0.575507, acc: 71.88%] [G loss: 1.967746]\n",
      "epoch:12 step:11487 [D loss: 0.615021, acc: 66.41%] [G loss: 2.110505]\n",
      "epoch:12 step:11488 [D loss: 0.607837, acc: 68.75%] [G loss: 2.160745]\n",
      "epoch:12 step:11489 [D loss: 0.626884, acc: 64.84%] [G loss: 2.065374]\n",
      "epoch:12 step:11490 [D loss: 0.642052, acc: 60.94%] [G loss: 2.222058]\n",
      "epoch:12 step:11491 [D loss: 0.610424, acc: 69.53%] [G loss: 2.149820]\n",
      "epoch:12 step:11492 [D loss: 0.628240, acc: 67.19%] [G loss: 2.250670]\n",
      "epoch:12 step:11493 [D loss: 0.702437, acc: 53.12%] [G loss: 1.943586]\n",
      "epoch:12 step:11494 [D loss: 0.720972, acc: 57.81%] [G loss: 1.977646]\n",
      "epoch:12 step:11495 [D loss: 0.698294, acc: 56.25%] [G loss: 1.918994]\n",
      "epoch:12 step:11496 [D loss: 0.629281, acc: 64.06%] [G loss: 1.815292]\n",
      "epoch:12 step:11497 [D loss: 0.587163, acc: 68.75%] [G loss: 2.037713]\n",
      "epoch:12 step:11498 [D loss: 0.617691, acc: 64.84%] [G loss: 2.165201]\n",
      "epoch:12 step:11499 [D loss: 0.633687, acc: 62.50%] [G loss: 1.956404]\n",
      "epoch:12 step:11500 [D loss: 0.663854, acc: 60.94%] [G loss: 1.907515]\n",
      "epoch:12 step:11501 [D loss: 0.694034, acc: 55.47%] [G loss: 1.953362]\n",
      "epoch:12 step:11502 [D loss: 0.598177, acc: 70.31%] [G loss: 2.013083]\n",
      "epoch:12 step:11503 [D loss: 0.585925, acc: 71.09%] [G loss: 1.997250]\n",
      "epoch:12 step:11504 [D loss: 0.695756, acc: 59.38%] [G loss: 2.013260]\n",
      "epoch:12 step:11505 [D loss: 0.590722, acc: 70.31%] [G loss: 2.181929]\n",
      "epoch:12 step:11506 [D loss: 0.632500, acc: 59.38%] [G loss: 2.005751]\n",
      "epoch:12 step:11507 [D loss: 0.619762, acc: 63.28%] [G loss: 2.211244]\n",
      "epoch:12 step:11508 [D loss: 0.617634, acc: 64.84%] [G loss: 2.246103]\n",
      "epoch:12 step:11509 [D loss: 0.671654, acc: 60.16%] [G loss: 1.979913]\n",
      "epoch:12 step:11510 [D loss: 0.663895, acc: 57.03%] [G loss: 1.905596]\n",
      "epoch:12 step:11511 [D loss: 0.592845, acc: 70.31%] [G loss: 2.043900]\n",
      "epoch:12 step:11512 [D loss: 0.648330, acc: 62.50%] [G loss: 1.936541]\n",
      "epoch:12 step:11513 [D loss: 0.632132, acc: 67.97%] [G loss: 2.049887]\n",
      "epoch:12 step:11514 [D loss: 0.631903, acc: 61.72%] [G loss: 2.120526]\n",
      "epoch:12 step:11515 [D loss: 0.636926, acc: 60.16%] [G loss: 2.016990]\n",
      "epoch:12 step:11516 [D loss: 0.612399, acc: 67.19%] [G loss: 2.084095]\n",
      "epoch:12 step:11517 [D loss: 0.633923, acc: 64.06%] [G loss: 1.901167]\n",
      "epoch:12 step:11518 [D loss: 0.515998, acc: 78.91%] [G loss: 2.278111]\n",
      "epoch:12 step:11519 [D loss: 0.592602, acc: 67.97%] [G loss: 2.103137]\n",
      "epoch:12 step:11520 [D loss: 0.642111, acc: 61.72%] [G loss: 2.203995]\n",
      "epoch:12 step:11521 [D loss: 0.654708, acc: 65.62%] [G loss: 2.018140]\n",
      "epoch:12 step:11522 [D loss: 0.670633, acc: 60.16%] [G loss: 1.956304]\n",
      "epoch:12 step:11523 [D loss: 0.634142, acc: 65.62%] [G loss: 2.086641]\n",
      "epoch:12 step:11524 [D loss: 0.626490, acc: 61.72%] [G loss: 2.174743]\n",
      "epoch:12 step:11525 [D loss: 0.759817, acc: 51.56%] [G loss: 1.844175]\n",
      "epoch:12 step:11526 [D loss: 0.711706, acc: 57.03%] [G loss: 1.949873]\n",
      "epoch:12 step:11527 [D loss: 0.626048, acc: 61.72%] [G loss: 1.964488]\n",
      "epoch:12 step:11528 [D loss: 0.708232, acc: 52.34%] [G loss: 1.897231]\n",
      "epoch:12 step:11529 [D loss: 0.656357, acc: 59.38%] [G loss: 2.066572]\n",
      "epoch:12 step:11530 [D loss: 0.593515, acc: 68.75%] [G loss: 2.097436]\n",
      "epoch:12 step:11531 [D loss: 0.635473, acc: 62.50%] [G loss: 1.839074]\n",
      "epoch:12 step:11532 [D loss: 0.639101, acc: 61.72%] [G loss: 1.979574]\n",
      "epoch:12 step:11533 [D loss: 0.657988, acc: 61.72%] [G loss: 1.967810]\n",
      "epoch:12 step:11534 [D loss: 0.668214, acc: 60.94%] [G loss: 1.838429]\n",
      "epoch:12 step:11535 [D loss: 0.665701, acc: 59.38%] [G loss: 1.982062]\n",
      "epoch:12 step:11536 [D loss: 0.626635, acc: 60.16%] [G loss: 2.117855]\n",
      "epoch:12 step:11537 [D loss: 0.656675, acc: 60.16%] [G loss: 2.070058]\n",
      "epoch:12 step:11538 [D loss: 0.711592, acc: 57.81%] [G loss: 1.969267]\n",
      "epoch:12 step:11539 [D loss: 0.647413, acc: 60.94%] [G loss: 1.975951]\n",
      "epoch:12 step:11540 [D loss: 0.609671, acc: 67.19%] [G loss: 2.008039]\n",
      "epoch:12 step:11541 [D loss: 0.633689, acc: 60.16%] [G loss: 1.830371]\n",
      "epoch:12 step:11542 [D loss: 0.611791, acc: 60.16%] [G loss: 2.030994]\n",
      "epoch:12 step:11543 [D loss: 0.636083, acc: 66.41%] [G loss: 2.081868]\n",
      "epoch:12 step:11544 [D loss: 0.571867, acc: 77.34%] [G loss: 2.086689]\n",
      "epoch:12 step:11545 [D loss: 0.668280, acc: 57.03%] [G loss: 1.929921]\n",
      "epoch:12 step:11546 [D loss: 0.645871, acc: 59.38%] [G loss: 2.103550]\n",
      "epoch:12 step:11547 [D loss: 0.671322, acc: 61.72%] [G loss: 1.856967]\n",
      "epoch:12 step:11548 [D loss: 0.661826, acc: 65.62%] [G loss: 1.976992]\n",
      "epoch:12 step:11549 [D loss: 0.616370, acc: 67.19%] [G loss: 1.990642]\n",
      "epoch:12 step:11550 [D loss: 0.645351, acc: 61.72%] [G loss: 2.030178]\n",
      "epoch:12 step:11551 [D loss: 0.607402, acc: 63.28%] [G loss: 1.969775]\n",
      "epoch:12 step:11552 [D loss: 0.616604, acc: 64.06%] [G loss: 1.869469]\n",
      "epoch:12 step:11553 [D loss: 0.630726, acc: 68.75%] [G loss: 2.063367]\n",
      "epoch:12 step:11554 [D loss: 0.669662, acc: 61.72%] [G loss: 1.934202]\n",
      "epoch:12 step:11555 [D loss: 0.653831, acc: 64.06%] [G loss: 2.035798]\n",
      "epoch:12 step:11556 [D loss: 0.602857, acc: 63.28%] [G loss: 2.305784]\n",
      "epoch:12 step:11557 [D loss: 0.602282, acc: 71.09%] [G loss: 2.152265]\n",
      "epoch:12 step:11558 [D loss: 0.591492, acc: 70.31%] [G loss: 2.269511]\n",
      "epoch:12 step:11559 [D loss: 0.538910, acc: 75.00%] [G loss: 2.515167]\n",
      "epoch:12 step:11560 [D loss: 0.622617, acc: 66.41%] [G loss: 2.014003]\n",
      "epoch:12 step:11561 [D loss: 0.702775, acc: 56.25%] [G loss: 1.938403]\n",
      "epoch:12 step:11562 [D loss: 0.671104, acc: 59.38%] [G loss: 2.032725]\n",
      "epoch:12 step:11563 [D loss: 0.597960, acc: 69.53%] [G loss: 1.919934]\n",
      "epoch:12 step:11564 [D loss: 0.665546, acc: 63.28%] [G loss: 1.977682]\n",
      "epoch:12 step:11565 [D loss: 0.606358, acc: 68.75%] [G loss: 2.113663]\n",
      "epoch:12 step:11566 [D loss: 0.605077, acc: 66.41%] [G loss: 2.036550]\n",
      "epoch:12 step:11567 [D loss: 0.710073, acc: 50.00%] [G loss: 1.964770]\n",
      "epoch:12 step:11568 [D loss: 0.648933, acc: 64.06%] [G loss: 1.905886]\n",
      "epoch:12 step:11569 [D loss: 0.586375, acc: 68.75%] [G loss: 2.045858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11570 [D loss: 0.591646, acc: 71.09%] [G loss: 1.932364]\n",
      "epoch:12 step:11571 [D loss: 0.612003, acc: 73.44%] [G loss: 1.941970]\n",
      "epoch:12 step:11572 [D loss: 0.701997, acc: 53.12%] [G loss: 2.022623]\n",
      "epoch:12 step:11573 [D loss: 0.654073, acc: 58.59%] [G loss: 2.038625]\n",
      "epoch:12 step:11574 [D loss: 0.614535, acc: 62.50%] [G loss: 2.134503]\n",
      "epoch:12 step:11575 [D loss: 0.611663, acc: 66.41%] [G loss: 1.985547]\n",
      "epoch:12 step:11576 [D loss: 0.634283, acc: 66.41%] [G loss: 2.152276]\n",
      "epoch:12 step:11577 [D loss: 0.620377, acc: 62.50%] [G loss: 1.921571]\n",
      "epoch:12 step:11578 [D loss: 0.642098, acc: 63.28%] [G loss: 2.161172]\n",
      "epoch:12 step:11579 [D loss: 0.599700, acc: 66.41%] [G loss: 2.333409]\n",
      "epoch:12 step:11580 [D loss: 0.656693, acc: 58.59%] [G loss: 2.023801]\n",
      "epoch:12 step:11581 [D loss: 0.659231, acc: 64.06%] [G loss: 2.031102]\n",
      "epoch:12 step:11582 [D loss: 0.642178, acc: 62.50%] [G loss: 2.021823]\n",
      "epoch:12 step:11583 [D loss: 0.631968, acc: 66.41%] [G loss: 2.069954]\n",
      "epoch:12 step:11584 [D loss: 0.581352, acc: 70.31%] [G loss: 2.034347]\n",
      "epoch:12 step:11585 [D loss: 0.674423, acc: 60.16%] [G loss: 1.918588]\n",
      "epoch:12 step:11586 [D loss: 0.682182, acc: 59.38%] [G loss: 1.874670]\n",
      "epoch:12 step:11587 [D loss: 0.638674, acc: 64.06%] [G loss: 1.963988]\n",
      "epoch:12 step:11588 [D loss: 0.654916, acc: 64.84%] [G loss: 1.906870]\n",
      "epoch:12 step:11589 [D loss: 0.604321, acc: 67.97%] [G loss: 2.323524]\n",
      "epoch:12 step:11590 [D loss: 0.620562, acc: 62.50%] [G loss: 2.152520]\n",
      "epoch:12 step:11591 [D loss: 0.566721, acc: 71.88%] [G loss: 2.275321]\n",
      "epoch:12 step:11592 [D loss: 0.618409, acc: 68.75%] [G loss: 2.046622]\n",
      "epoch:12 step:11593 [D loss: 0.688059, acc: 59.38%] [G loss: 1.920042]\n",
      "epoch:12 step:11594 [D loss: 0.647234, acc: 64.06%] [G loss: 2.177424]\n",
      "epoch:12 step:11595 [D loss: 0.682362, acc: 56.25%] [G loss: 2.016536]\n",
      "epoch:12 step:11596 [D loss: 0.677348, acc: 57.81%] [G loss: 1.889045]\n",
      "epoch:12 step:11597 [D loss: 0.567703, acc: 70.31%] [G loss: 2.006745]\n",
      "epoch:12 step:11598 [D loss: 0.612735, acc: 65.62%] [G loss: 2.066437]\n",
      "epoch:12 step:11599 [D loss: 0.663346, acc: 60.94%] [G loss: 2.003395]\n",
      "epoch:12 step:11600 [D loss: 0.679097, acc: 60.94%] [G loss: 1.925590]\n",
      "##############\n",
      "[2.54149953 1.44392702 6.44888771 4.85974331 3.65003533 5.60313983\n",
      " 4.38621891 4.86403863 4.64952343 3.34454461]\n",
      "##########\n",
      "epoch:12 step:11601 [D loss: 0.654384, acc: 59.38%] [G loss: 1.970994]\n",
      "epoch:12 step:11602 [D loss: 0.590411, acc: 65.62%] [G loss: 2.090171]\n",
      "epoch:12 step:11603 [D loss: 0.581512, acc: 70.31%] [G loss: 2.226509]\n",
      "epoch:12 step:11604 [D loss: 0.664026, acc: 60.16%] [G loss: 2.031702]\n",
      "epoch:12 step:11605 [D loss: 0.659953, acc: 60.94%] [G loss: 1.915083]\n",
      "epoch:12 step:11606 [D loss: 0.683485, acc: 57.81%] [G loss: 1.839575]\n",
      "epoch:12 step:11607 [D loss: 0.610841, acc: 62.50%] [G loss: 2.011817]\n",
      "epoch:12 step:11608 [D loss: 0.652813, acc: 57.81%] [G loss: 2.059373]\n",
      "epoch:12 step:11609 [D loss: 0.643552, acc: 62.50%] [G loss: 2.031847]\n",
      "epoch:12 step:11610 [D loss: 0.677288, acc: 58.59%] [G loss: 1.980731]\n",
      "epoch:12 step:11611 [D loss: 0.621843, acc: 65.62%] [G loss: 1.972550]\n",
      "epoch:12 step:11612 [D loss: 0.637820, acc: 60.16%] [G loss: 1.907183]\n",
      "epoch:12 step:11613 [D loss: 0.660581, acc: 64.06%] [G loss: 2.040752]\n",
      "epoch:12 step:11614 [D loss: 0.632777, acc: 64.06%] [G loss: 1.896691]\n",
      "epoch:12 step:11615 [D loss: 0.601397, acc: 64.06%] [G loss: 2.030039]\n",
      "epoch:12 step:11616 [D loss: 0.634405, acc: 60.94%] [G loss: 1.943995]\n",
      "epoch:12 step:11617 [D loss: 0.639450, acc: 59.38%] [G loss: 1.838238]\n",
      "epoch:12 step:11618 [D loss: 0.563772, acc: 69.53%] [G loss: 2.290658]\n",
      "epoch:12 step:11619 [D loss: 0.688522, acc: 57.03%] [G loss: 1.930407]\n",
      "epoch:12 step:11620 [D loss: 0.677948, acc: 60.94%] [G loss: 1.860532]\n",
      "epoch:12 step:11621 [D loss: 0.725351, acc: 51.56%] [G loss: 1.859055]\n",
      "epoch:12 step:11622 [D loss: 0.613287, acc: 70.31%] [G loss: 1.929401]\n",
      "epoch:12 step:11623 [D loss: 0.586072, acc: 70.31%] [G loss: 1.999165]\n",
      "epoch:12 step:11624 [D loss: 0.623041, acc: 64.84%] [G loss: 2.069058]\n",
      "epoch:12 step:11625 [D loss: 0.568111, acc: 75.00%] [G loss: 2.001705]\n",
      "epoch:12 step:11626 [D loss: 0.660655, acc: 59.38%] [G loss: 1.947607]\n",
      "epoch:12 step:11627 [D loss: 0.662795, acc: 60.16%] [G loss: 2.004923]\n",
      "epoch:12 step:11628 [D loss: 0.611782, acc: 66.41%] [G loss: 2.079651]\n",
      "epoch:12 step:11629 [D loss: 0.680513, acc: 58.59%] [G loss: 2.149012]\n",
      "epoch:12 step:11630 [D loss: 0.676732, acc: 57.81%] [G loss: 1.857827]\n",
      "epoch:12 step:11631 [D loss: 0.678979, acc: 62.50%] [G loss: 2.066154]\n",
      "epoch:12 step:11632 [D loss: 0.687223, acc: 67.97%] [G loss: 1.796486]\n",
      "epoch:12 step:11633 [D loss: 0.680903, acc: 60.94%] [G loss: 1.882725]\n",
      "epoch:12 step:11634 [D loss: 0.634242, acc: 63.28%] [G loss: 1.928512]\n",
      "epoch:12 step:11635 [D loss: 0.667480, acc: 60.94%] [G loss: 2.020115]\n",
      "epoch:12 step:11636 [D loss: 0.690708, acc: 60.16%] [G loss: 1.869791]\n",
      "epoch:12 step:11637 [D loss: 0.637139, acc: 66.41%] [G loss: 1.865200]\n",
      "epoch:12 step:11638 [D loss: 0.665925, acc: 58.59%] [G loss: 1.939771]\n",
      "epoch:12 step:11639 [D loss: 0.598461, acc: 64.84%] [G loss: 1.964847]\n",
      "epoch:12 step:11640 [D loss: 0.635269, acc: 62.50%] [G loss: 1.991962]\n",
      "epoch:12 step:11641 [D loss: 0.647823, acc: 61.72%] [G loss: 1.734947]\n",
      "epoch:12 step:11642 [D loss: 0.619170, acc: 67.97%] [G loss: 2.014998]\n",
      "epoch:12 step:11643 [D loss: 0.594275, acc: 70.31%] [G loss: 2.075469]\n",
      "epoch:12 step:11644 [D loss: 0.709826, acc: 50.78%] [G loss: 2.075796]\n",
      "epoch:12 step:11645 [D loss: 0.658227, acc: 64.06%] [G loss: 1.946592]\n",
      "epoch:12 step:11646 [D loss: 0.621728, acc: 66.41%] [G loss: 2.045046]\n",
      "epoch:12 step:11647 [D loss: 0.652569, acc: 63.28%] [G loss: 1.963705]\n",
      "epoch:12 step:11648 [D loss: 0.665344, acc: 60.16%] [G loss: 1.985337]\n",
      "epoch:12 step:11649 [D loss: 0.605849, acc: 71.09%] [G loss: 2.115155]\n",
      "epoch:12 step:11650 [D loss: 0.578758, acc: 67.97%] [G loss: 2.244573]\n",
      "epoch:12 step:11651 [D loss: 0.667066, acc: 58.59%] [G loss: 1.974667]\n",
      "epoch:12 step:11652 [D loss: 0.656855, acc: 63.28%] [G loss: 1.798190]\n",
      "epoch:12 step:11653 [D loss: 0.670800, acc: 58.59%] [G loss: 1.928170]\n",
      "epoch:12 step:11654 [D loss: 0.618212, acc: 64.06%] [G loss: 1.969286]\n",
      "epoch:12 step:11655 [D loss: 0.700884, acc: 58.59%] [G loss: 2.023195]\n",
      "epoch:12 step:11656 [D loss: 0.719939, acc: 52.34%] [G loss: 1.979233]\n",
      "epoch:12 step:11657 [D loss: 0.599566, acc: 65.62%] [G loss: 2.182653]\n",
      "epoch:12 step:11658 [D loss: 0.665893, acc: 64.06%] [G loss: 2.063790]\n",
      "epoch:12 step:11659 [D loss: 0.621952, acc: 65.62%] [G loss: 2.175625]\n",
      "epoch:12 step:11660 [D loss: 0.630983, acc: 65.62%] [G loss: 2.026978]\n",
      "epoch:12 step:11661 [D loss: 0.674275, acc: 62.50%] [G loss: 1.970806]\n",
      "epoch:12 step:11662 [D loss: 0.673268, acc: 61.72%] [G loss: 1.870892]\n",
      "epoch:12 step:11663 [D loss: 0.705707, acc: 53.12%] [G loss: 1.845165]\n",
      "epoch:12 step:11664 [D loss: 0.655097, acc: 60.16%] [G loss: 1.804005]\n",
      "epoch:12 step:11665 [D loss: 0.661407, acc: 61.72%] [G loss: 1.722540]\n",
      "epoch:12 step:11666 [D loss: 0.655451, acc: 63.28%] [G loss: 1.835281]\n",
      "epoch:12 step:11667 [D loss: 0.612220, acc: 68.75%] [G loss: 1.869447]\n",
      "epoch:12 step:11668 [D loss: 0.649494, acc: 63.28%] [G loss: 1.892165]\n",
      "epoch:12 step:11669 [D loss: 0.653458, acc: 62.50%] [G loss: 1.975186]\n",
      "epoch:12 step:11670 [D loss: 0.566589, acc: 72.66%] [G loss: 1.966187]\n",
      "epoch:12 step:11671 [D loss: 0.642386, acc: 64.06%] [G loss: 2.116281]\n",
      "epoch:12 step:11672 [D loss: 0.599117, acc: 71.09%] [G loss: 2.156597]\n",
      "epoch:12 step:11673 [D loss: 0.672109, acc: 60.16%] [G loss: 2.166233]\n",
      "epoch:12 step:11674 [D loss: 0.595740, acc: 74.22%] [G loss: 2.196742]\n",
      "epoch:12 step:11675 [D loss: 0.570965, acc: 68.75%] [G loss: 2.041134]\n",
      "epoch:12 step:11676 [D loss: 0.686499, acc: 59.38%] [G loss: 1.907481]\n",
      "epoch:12 step:11677 [D loss: 0.631801, acc: 63.28%] [G loss: 2.060382]\n",
      "epoch:12 step:11678 [D loss: 0.601622, acc: 62.50%] [G loss: 2.102846]\n",
      "epoch:12 step:11679 [D loss: 0.671710, acc: 63.28%] [G loss: 2.092029]\n",
      "epoch:12 step:11680 [D loss: 0.643425, acc: 66.41%] [G loss: 2.055784]\n",
      "epoch:12 step:11681 [D loss: 0.746568, acc: 47.66%] [G loss: 1.963897]\n",
      "epoch:12 step:11682 [D loss: 0.671081, acc: 57.03%] [G loss: 1.819577]\n",
      "epoch:12 step:11683 [D loss: 0.666709, acc: 57.03%] [G loss: 1.927675]\n",
      "epoch:12 step:11684 [D loss: 0.639635, acc: 60.94%] [G loss: 1.862583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11685 [D loss: 0.645136, acc: 57.03%] [G loss: 2.001817]\n",
      "epoch:12 step:11686 [D loss: 0.678372, acc: 61.72%] [G loss: 1.872369]\n",
      "epoch:12 step:11687 [D loss: 0.633443, acc: 62.50%] [G loss: 1.878660]\n",
      "epoch:12 step:11688 [D loss: 0.723723, acc: 56.25%] [G loss: 1.707180]\n",
      "epoch:12 step:11689 [D loss: 0.647412, acc: 64.06%] [G loss: 1.940773]\n",
      "epoch:12 step:11690 [D loss: 0.647423, acc: 60.94%] [G loss: 1.872836]\n",
      "epoch:12 step:11691 [D loss: 0.601987, acc: 64.84%] [G loss: 2.021170]\n",
      "epoch:12 step:11692 [D loss: 0.665307, acc: 61.72%] [G loss: 1.893656]\n",
      "epoch:12 step:11693 [D loss: 0.621693, acc: 66.41%] [G loss: 2.119493]\n",
      "epoch:12 step:11694 [D loss: 0.622974, acc: 64.06%] [G loss: 1.925746]\n",
      "epoch:12 step:11695 [D loss: 0.636489, acc: 59.38%] [G loss: 1.919054]\n",
      "epoch:12 step:11696 [D loss: 0.633265, acc: 63.28%] [G loss: 1.998958]\n",
      "epoch:12 step:11697 [D loss: 0.607886, acc: 65.62%] [G loss: 1.934585]\n",
      "epoch:12 step:11698 [D loss: 0.663652, acc: 60.94%] [G loss: 1.939703]\n",
      "epoch:12 step:11699 [D loss: 0.644717, acc: 57.03%] [G loss: 1.896690]\n",
      "epoch:12 step:11700 [D loss: 0.639050, acc: 62.50%] [G loss: 2.051702]\n",
      "epoch:12 step:11701 [D loss: 0.618377, acc: 66.41%] [G loss: 2.003733]\n",
      "epoch:12 step:11702 [D loss: 0.735491, acc: 52.34%] [G loss: 1.942556]\n",
      "epoch:12 step:11703 [D loss: 0.678901, acc: 60.94%] [G loss: 1.890064]\n",
      "epoch:12 step:11704 [D loss: 0.707132, acc: 53.91%] [G loss: 1.829992]\n",
      "epoch:12 step:11705 [D loss: 0.604236, acc: 65.62%] [G loss: 1.924100]\n",
      "epoch:12 step:11706 [D loss: 0.640796, acc: 64.84%] [G loss: 1.887976]\n",
      "epoch:12 step:11707 [D loss: 0.629285, acc: 66.41%] [G loss: 1.959533]\n",
      "epoch:12 step:11708 [D loss: 0.623122, acc: 66.41%] [G loss: 2.029456]\n",
      "epoch:12 step:11709 [D loss: 0.735128, acc: 46.09%] [G loss: 1.948822]\n",
      "epoch:12 step:11710 [D loss: 0.661556, acc: 57.81%] [G loss: 1.832737]\n",
      "epoch:12 step:11711 [D loss: 0.619349, acc: 67.97%] [G loss: 2.035111]\n",
      "epoch:12 step:11712 [D loss: 0.623681, acc: 66.41%] [G loss: 1.861192]\n",
      "epoch:12 step:11713 [D loss: 0.652818, acc: 62.50%] [G loss: 2.257160]\n",
      "epoch:12 step:11714 [D loss: 0.624129, acc: 69.53%] [G loss: 2.248898]\n",
      "epoch:12 step:11715 [D loss: 0.658135, acc: 56.25%] [G loss: 2.068410]\n",
      "epoch:12 step:11716 [D loss: 0.654441, acc: 60.16%] [G loss: 1.971752]\n",
      "epoch:12 step:11717 [D loss: 0.646606, acc: 64.06%] [G loss: 1.865216]\n",
      "epoch:12 step:11718 [D loss: 0.654478, acc: 58.59%] [G loss: 1.894804]\n",
      "epoch:12 step:11719 [D loss: 0.617076, acc: 67.97%] [G loss: 1.923292]\n",
      "epoch:12 step:11720 [D loss: 0.660725, acc: 58.59%] [G loss: 1.914776]\n",
      "epoch:12 step:11721 [D loss: 0.693865, acc: 53.12%] [G loss: 1.775265]\n",
      "epoch:12 step:11722 [D loss: 0.600308, acc: 72.66%] [G loss: 1.786315]\n",
      "epoch:12 step:11723 [D loss: 0.613354, acc: 64.84%] [G loss: 2.007559]\n",
      "epoch:12 step:11724 [D loss: 0.588543, acc: 69.53%] [G loss: 2.128292]\n",
      "epoch:12 step:11725 [D loss: 0.562989, acc: 71.88%] [G loss: 2.180169]\n",
      "epoch:12 step:11726 [D loss: 0.718106, acc: 55.47%] [G loss: 1.755917]\n",
      "epoch:12 step:11727 [D loss: 0.664945, acc: 60.94%] [G loss: 1.854204]\n",
      "epoch:12 step:11728 [D loss: 0.673896, acc: 62.50%] [G loss: 1.923225]\n",
      "epoch:12 step:11729 [D loss: 0.680419, acc: 55.47%] [G loss: 1.802702]\n",
      "epoch:12 step:11730 [D loss: 0.637605, acc: 62.50%] [G loss: 1.930459]\n",
      "epoch:12 step:11731 [D loss: 0.663088, acc: 57.03%] [G loss: 1.974503]\n",
      "epoch:12 step:11732 [D loss: 0.598827, acc: 67.97%] [G loss: 2.142895]\n",
      "epoch:12 step:11733 [D loss: 0.698642, acc: 56.25%] [G loss: 1.814193]\n",
      "epoch:12 step:11734 [D loss: 0.647812, acc: 64.06%] [G loss: 1.917774]\n",
      "epoch:12 step:11735 [D loss: 0.645343, acc: 60.94%] [G loss: 2.048847]\n",
      "epoch:12 step:11736 [D loss: 0.677051, acc: 53.12%] [G loss: 1.860239]\n",
      "epoch:12 step:11737 [D loss: 0.626284, acc: 69.53%] [G loss: 1.977151]\n",
      "epoch:12 step:11738 [D loss: 0.581604, acc: 67.19%] [G loss: 2.237965]\n",
      "epoch:12 step:11739 [D loss: 0.583107, acc: 71.09%] [G loss: 2.198837]\n",
      "epoch:12 step:11740 [D loss: 0.650858, acc: 66.41%] [G loss: 1.996503]\n",
      "epoch:12 step:11741 [D loss: 0.586379, acc: 70.31%] [G loss: 2.077619]\n",
      "epoch:12 step:11742 [D loss: 0.607618, acc: 71.09%] [G loss: 2.113380]\n",
      "epoch:12 step:11743 [D loss: 0.581685, acc: 67.97%] [G loss: 2.047612]\n",
      "epoch:12 step:11744 [D loss: 0.702174, acc: 55.47%] [G loss: 1.698623]\n",
      "epoch:12 step:11745 [D loss: 0.704188, acc: 57.03%] [G loss: 1.797257]\n",
      "epoch:12 step:11746 [D loss: 0.711822, acc: 53.12%] [G loss: 1.784963]\n",
      "epoch:12 step:11747 [D loss: 0.592768, acc: 67.19%] [G loss: 1.927322]\n",
      "epoch:12 step:11748 [D loss: 0.648346, acc: 63.28%] [G loss: 2.000099]\n",
      "epoch:12 step:11749 [D loss: 0.621199, acc: 64.84%] [G loss: 2.094229]\n",
      "epoch:12 step:11750 [D loss: 0.676539, acc: 60.16%] [G loss: 1.750963]\n",
      "epoch:12 step:11751 [D loss: 0.641180, acc: 60.16%] [G loss: 1.856419]\n",
      "epoch:12 step:11752 [D loss: 0.592419, acc: 71.09%] [G loss: 2.141423]\n",
      "epoch:12 step:11753 [D loss: 0.658805, acc: 59.38%] [G loss: 2.140800]\n",
      "epoch:12 step:11754 [D loss: 0.673092, acc: 53.91%] [G loss: 1.890124]\n",
      "epoch:12 step:11755 [D loss: 0.666647, acc: 60.16%] [G loss: 1.804268]\n",
      "epoch:12 step:11756 [D loss: 0.637892, acc: 61.72%] [G loss: 1.859559]\n",
      "epoch:12 step:11757 [D loss: 0.618794, acc: 64.06%] [G loss: 1.892246]\n",
      "epoch:12 step:11758 [D loss: 0.669292, acc: 59.38%] [G loss: 1.920874]\n",
      "epoch:12 step:11759 [D loss: 0.593294, acc: 70.31%] [G loss: 1.936485]\n",
      "epoch:12 step:11760 [D loss: 0.562856, acc: 71.88%] [G loss: 2.011245]\n",
      "epoch:12 step:11761 [D loss: 0.627436, acc: 66.41%] [G loss: 2.027720]\n",
      "epoch:12 step:11762 [D loss: 0.661970, acc: 60.94%] [G loss: 1.993901]\n",
      "epoch:12 step:11763 [D loss: 0.587377, acc: 65.62%] [G loss: 2.084473]\n",
      "epoch:12 step:11764 [D loss: 0.600090, acc: 72.66%] [G loss: 2.049987]\n",
      "epoch:12 step:11765 [D loss: 0.608261, acc: 74.22%] [G loss: 2.189534]\n",
      "epoch:12 step:11766 [D loss: 0.647793, acc: 64.06%] [G loss: 2.073639]\n",
      "epoch:12 step:11767 [D loss: 0.598989, acc: 67.97%] [G loss: 2.126312]\n",
      "epoch:12 step:11768 [D loss: 0.621566, acc: 67.19%] [G loss: 2.116627]\n",
      "epoch:12 step:11769 [D loss: 0.634768, acc: 64.84%] [G loss: 2.034079]\n",
      "epoch:12 step:11770 [D loss: 0.705758, acc: 59.38%] [G loss: 2.025908]\n",
      "epoch:12 step:11771 [D loss: 0.605717, acc: 70.31%] [G loss: 1.851720]\n",
      "epoch:12 step:11772 [D loss: 0.670003, acc: 60.16%] [G loss: 1.792880]\n",
      "epoch:12 step:11773 [D loss: 0.702561, acc: 57.03%] [G loss: 1.732786]\n",
      "epoch:12 step:11774 [D loss: 0.683926, acc: 59.38%] [G loss: 1.887059]\n",
      "epoch:12 step:11775 [D loss: 0.619091, acc: 64.06%] [G loss: 2.031246]\n",
      "epoch:12 step:11776 [D loss: 0.646133, acc: 60.16%] [G loss: 2.062621]\n",
      "epoch:12 step:11777 [D loss: 0.620353, acc: 64.06%] [G loss: 1.878829]\n",
      "epoch:12 step:11778 [D loss: 0.636926, acc: 62.50%] [G loss: 2.100598]\n",
      "epoch:12 step:11779 [D loss: 0.663858, acc: 60.94%] [G loss: 1.890620]\n",
      "epoch:12 step:11780 [D loss: 0.585900, acc: 69.53%] [G loss: 2.270473]\n",
      "epoch:12 step:11781 [D loss: 0.647073, acc: 61.72%] [G loss: 1.827462]\n",
      "epoch:12 step:11782 [D loss: 0.645476, acc: 65.62%] [G loss: 1.781473]\n",
      "epoch:12 step:11783 [D loss: 0.678896, acc: 58.59%] [G loss: 1.893491]\n",
      "epoch:12 step:11784 [D loss: 0.667876, acc: 56.25%] [G loss: 1.869800]\n",
      "epoch:12 step:11785 [D loss: 0.668373, acc: 56.25%] [G loss: 1.877048]\n",
      "epoch:12 step:11786 [D loss: 0.656469, acc: 55.47%] [G loss: 1.793911]\n",
      "epoch:12 step:11787 [D loss: 0.685104, acc: 58.59%] [G loss: 1.835125]\n",
      "epoch:12 step:11788 [D loss: 0.650511, acc: 57.81%] [G loss: 1.765421]\n",
      "epoch:12 step:11789 [D loss: 0.638757, acc: 57.81%] [G loss: 2.049424]\n",
      "epoch:12 step:11790 [D loss: 0.612766, acc: 70.31%] [G loss: 2.042638]\n",
      "epoch:12 step:11791 [D loss: 0.595790, acc: 64.06%] [G loss: 2.087883]\n",
      "epoch:12 step:11792 [D loss: 0.613636, acc: 68.75%] [G loss: 2.068396]\n",
      "epoch:12 step:11793 [D loss: 0.591424, acc: 64.06%] [G loss: 2.144559]\n",
      "epoch:12 step:11794 [D loss: 0.661804, acc: 59.38%] [G loss: 2.113488]\n",
      "epoch:12 step:11795 [D loss: 0.603612, acc: 71.09%] [G loss: 2.026408]\n",
      "epoch:12 step:11796 [D loss: 0.631111, acc: 64.06%] [G loss: 2.089586]\n",
      "epoch:12 step:11797 [D loss: 0.616815, acc: 71.09%] [G loss: 1.837774]\n",
      "epoch:12 step:11798 [D loss: 0.560837, acc: 72.66%] [G loss: 2.185772]\n",
      "epoch:12 step:11799 [D loss: 0.580870, acc: 71.09%] [G loss: 1.902663]\n",
      "epoch:12 step:11800 [D loss: 0.632781, acc: 62.50%] [G loss: 2.263734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.52659435 1.71326831 6.41173075 4.85984044 3.80325109 5.72086303\n",
      " 4.57728842 4.72026697 4.83846404 3.68966326]\n",
      "##########\n",
      "epoch:12 step:11801 [D loss: 0.639959, acc: 64.06%] [G loss: 2.331431]\n",
      "epoch:12 step:11802 [D loss: 0.697643, acc: 53.91%] [G loss: 2.005144]\n",
      "epoch:12 step:11803 [D loss: 0.720757, acc: 54.69%] [G loss: 1.778002]\n",
      "epoch:12 step:11804 [D loss: 0.647914, acc: 64.06%] [G loss: 1.892727]\n",
      "epoch:12 step:11805 [D loss: 0.609439, acc: 65.62%] [G loss: 2.054511]\n",
      "epoch:12 step:11806 [D loss: 0.647499, acc: 67.97%] [G loss: 1.998671]\n",
      "epoch:12 step:11807 [D loss: 0.633473, acc: 63.28%] [G loss: 2.014423]\n",
      "epoch:12 step:11808 [D loss: 0.660006, acc: 60.16%] [G loss: 2.135725]\n",
      "epoch:12 step:11809 [D loss: 0.631531, acc: 65.62%] [G loss: 1.984988]\n",
      "epoch:12 step:11810 [D loss: 0.658480, acc: 60.94%] [G loss: 1.841609]\n",
      "epoch:12 step:11811 [D loss: 0.618808, acc: 67.19%] [G loss: 2.043974]\n",
      "epoch:12 step:11812 [D loss: 0.655634, acc: 54.69%] [G loss: 1.869893]\n",
      "epoch:12 step:11813 [D loss: 0.661685, acc: 56.25%] [G loss: 1.930806]\n",
      "epoch:12 step:11814 [D loss: 0.609513, acc: 64.84%] [G loss: 1.857556]\n",
      "epoch:12 step:11815 [D loss: 0.601491, acc: 68.75%] [G loss: 1.936710]\n",
      "epoch:12 step:11816 [D loss: 0.675938, acc: 60.16%] [G loss: 1.879531]\n",
      "epoch:12 step:11817 [D loss: 0.640078, acc: 61.72%] [G loss: 1.947252]\n",
      "epoch:12 step:11818 [D loss: 0.623744, acc: 71.09%] [G loss: 1.985413]\n",
      "epoch:12 step:11819 [D loss: 0.644835, acc: 63.28%] [G loss: 1.817064]\n",
      "epoch:12 step:11820 [D loss: 0.642561, acc: 59.38%] [G loss: 1.802141]\n",
      "epoch:12 step:11821 [D loss: 0.669126, acc: 57.81%] [G loss: 1.835307]\n",
      "epoch:12 step:11822 [D loss: 0.661686, acc: 54.69%] [G loss: 1.988078]\n",
      "epoch:12 step:11823 [D loss: 0.673520, acc: 55.47%] [G loss: 1.871865]\n",
      "epoch:12 step:11824 [D loss: 0.595722, acc: 67.97%] [G loss: 1.957560]\n",
      "epoch:12 step:11825 [D loss: 0.653660, acc: 60.94%] [G loss: 1.878212]\n",
      "epoch:12 step:11826 [D loss: 0.653390, acc: 63.28%] [G loss: 1.892358]\n",
      "epoch:12 step:11827 [D loss: 0.633255, acc: 66.41%] [G loss: 2.045648]\n",
      "epoch:12 step:11828 [D loss: 0.677849, acc: 57.03%] [G loss: 1.936500]\n",
      "epoch:12 step:11829 [D loss: 0.666206, acc: 55.47%] [G loss: 1.939553]\n",
      "epoch:12 step:11830 [D loss: 0.611422, acc: 63.28%] [G loss: 1.939304]\n",
      "epoch:12 step:11831 [D loss: 0.688133, acc: 58.59%] [G loss: 1.925074]\n",
      "epoch:12 step:11832 [D loss: 0.675823, acc: 58.59%] [G loss: 1.945256]\n",
      "epoch:12 step:11833 [D loss: 0.631810, acc: 65.62%] [G loss: 1.892550]\n",
      "epoch:12 step:11834 [D loss: 0.608625, acc: 68.75%] [G loss: 2.131176]\n",
      "epoch:12 step:11835 [D loss: 0.619110, acc: 64.06%] [G loss: 1.984819]\n",
      "epoch:12 step:11836 [D loss: 0.653545, acc: 64.84%] [G loss: 2.074319]\n",
      "epoch:12 step:11837 [D loss: 0.607699, acc: 67.97%] [G loss: 1.921006]\n",
      "epoch:12 step:11838 [D loss: 0.633379, acc: 62.50%] [G loss: 1.937019]\n",
      "epoch:12 step:11839 [D loss: 0.714074, acc: 60.16%] [G loss: 2.045321]\n",
      "epoch:12 step:11840 [D loss: 0.639298, acc: 65.62%] [G loss: 1.964469]\n",
      "epoch:12 step:11841 [D loss: 0.721743, acc: 52.34%] [G loss: 1.889317]\n",
      "epoch:12 step:11842 [D loss: 0.619619, acc: 70.31%] [G loss: 1.931064]\n",
      "epoch:12 step:11843 [D loss: 0.631040, acc: 68.75%] [G loss: 1.905073]\n",
      "epoch:12 step:11844 [D loss: 0.629828, acc: 63.28%] [G loss: 1.950534]\n",
      "epoch:12 step:11845 [D loss: 0.591690, acc: 66.41%] [G loss: 1.996368]\n",
      "epoch:12 step:11846 [D loss: 0.575004, acc: 67.97%] [G loss: 2.108657]\n",
      "epoch:12 step:11847 [D loss: 0.655973, acc: 57.81%] [G loss: 2.153361]\n",
      "epoch:12 step:11848 [D loss: 0.618445, acc: 61.72%] [G loss: 1.865672]\n",
      "epoch:12 step:11849 [D loss: 0.671272, acc: 64.06%] [G loss: 2.033698]\n",
      "epoch:12 step:11850 [D loss: 0.695148, acc: 59.38%] [G loss: 1.829311]\n",
      "epoch:12 step:11851 [D loss: 0.648224, acc: 57.03%] [G loss: 1.905496]\n",
      "epoch:12 step:11852 [D loss: 0.628509, acc: 63.28%] [G loss: 1.941475]\n",
      "epoch:12 step:11853 [D loss: 0.591952, acc: 74.22%] [G loss: 2.029250]\n",
      "epoch:12 step:11854 [D loss: 0.620777, acc: 62.50%] [G loss: 1.852720]\n",
      "epoch:12 step:11855 [D loss: 0.666692, acc: 67.19%] [G loss: 1.798900]\n",
      "epoch:12 step:11856 [D loss: 0.669162, acc: 60.94%] [G loss: 1.863023]\n",
      "epoch:12 step:11857 [D loss: 0.634527, acc: 65.62%] [G loss: 2.092384]\n",
      "epoch:12 step:11858 [D loss: 0.640481, acc: 58.59%] [G loss: 1.854271]\n",
      "epoch:12 step:11859 [D loss: 0.678064, acc: 57.03%] [G loss: 1.898303]\n",
      "epoch:12 step:11860 [D loss: 0.611104, acc: 66.41%] [G loss: 1.897723]\n",
      "epoch:12 step:11861 [D loss: 0.640974, acc: 60.16%] [G loss: 1.800602]\n",
      "epoch:12 step:11862 [D loss: 0.626935, acc: 64.06%] [G loss: 1.929639]\n",
      "epoch:12 step:11863 [D loss: 0.658844, acc: 57.81%] [G loss: 1.903005]\n",
      "epoch:12 step:11864 [D loss: 0.631817, acc: 67.19%] [G loss: 1.988944]\n",
      "epoch:12 step:11865 [D loss: 0.688762, acc: 64.84%] [G loss: 1.966652]\n",
      "epoch:12 step:11866 [D loss: 0.661209, acc: 57.81%] [G loss: 1.933181]\n",
      "epoch:12 step:11867 [D loss: 0.662013, acc: 63.28%] [G loss: 1.931385]\n",
      "epoch:12 step:11868 [D loss: 0.579047, acc: 71.88%] [G loss: 2.038571]\n",
      "epoch:12 step:11869 [D loss: 0.650390, acc: 60.94%] [G loss: 1.952814]\n",
      "epoch:12 step:11870 [D loss: 0.616310, acc: 69.53%] [G loss: 1.871028]\n",
      "epoch:12 step:11871 [D loss: 0.631172, acc: 63.28%] [G loss: 2.042188]\n",
      "epoch:12 step:11872 [D loss: 0.661283, acc: 64.06%] [G loss: 1.923922]\n",
      "epoch:12 step:11873 [D loss: 0.610280, acc: 66.41%] [G loss: 1.941892]\n",
      "epoch:12 step:11874 [D loss: 0.630250, acc: 63.28%] [G loss: 2.008433]\n",
      "epoch:12 step:11875 [D loss: 0.627641, acc: 64.84%] [G loss: 2.089311]\n",
      "epoch:12 step:11876 [D loss: 0.633763, acc: 64.84%] [G loss: 1.974017]\n",
      "epoch:12 step:11877 [D loss: 0.627269, acc: 63.28%] [G loss: 1.941770]\n",
      "epoch:12 step:11878 [D loss: 0.613856, acc: 69.53%] [G loss: 2.067287]\n",
      "epoch:12 step:11879 [D loss: 0.545581, acc: 72.66%] [G loss: 2.155653]\n",
      "epoch:12 step:11880 [D loss: 0.666866, acc: 60.94%] [G loss: 1.944183]\n",
      "epoch:12 step:11881 [D loss: 0.622000, acc: 61.72%] [G loss: 2.048998]\n",
      "epoch:12 step:11882 [D loss: 0.680672, acc: 62.50%] [G loss: 1.830549]\n",
      "epoch:12 step:11883 [D loss: 0.648049, acc: 60.94%] [G loss: 1.860062]\n",
      "epoch:12 step:11884 [D loss: 0.636048, acc: 61.72%] [G loss: 1.898037]\n",
      "epoch:12 step:11885 [D loss: 0.588109, acc: 69.53%] [G loss: 2.021331]\n",
      "epoch:12 step:11886 [D loss: 0.637436, acc: 68.75%] [G loss: 2.154890]\n",
      "epoch:12 step:11887 [D loss: 0.625902, acc: 64.06%] [G loss: 2.019313]\n",
      "epoch:12 step:11888 [D loss: 0.597508, acc: 60.94%] [G loss: 2.135190]\n",
      "epoch:12 step:11889 [D loss: 0.594811, acc: 70.31%] [G loss: 2.158010]\n",
      "epoch:12 step:11890 [D loss: 0.660634, acc: 62.50%] [G loss: 2.002490]\n",
      "epoch:12 step:11891 [D loss: 0.613707, acc: 65.62%] [G loss: 2.147531]\n",
      "epoch:12 step:11892 [D loss: 0.577280, acc: 64.06%] [G loss: 2.720531]\n",
      "epoch:12 step:11893 [D loss: 0.591407, acc: 67.97%] [G loss: 2.156324]\n",
      "epoch:12 step:11894 [D loss: 0.574360, acc: 65.62%] [G loss: 2.034978]\n",
      "epoch:12 step:11895 [D loss: 0.609943, acc: 66.41%] [G loss: 2.110532]\n",
      "epoch:12 step:11896 [D loss: 0.611737, acc: 67.19%] [G loss: 2.097211]\n",
      "epoch:12 step:11897 [D loss: 0.599078, acc: 65.62%] [G loss: 2.021811]\n",
      "epoch:12 step:11898 [D loss: 0.591748, acc: 74.22%] [G loss: 2.080454]\n",
      "epoch:12 step:11899 [D loss: 0.639503, acc: 58.59%] [G loss: 2.028240]\n",
      "epoch:12 step:11900 [D loss: 0.613117, acc: 64.84%] [G loss: 1.980005]\n",
      "epoch:12 step:11901 [D loss: 0.640727, acc: 60.94%] [G loss: 1.904009]\n",
      "epoch:12 step:11902 [D loss: 0.672905, acc: 61.72%] [G loss: 1.934357]\n",
      "epoch:12 step:11903 [D loss: 0.614127, acc: 64.84%] [G loss: 2.035943]\n",
      "epoch:12 step:11904 [D loss: 0.552354, acc: 71.88%] [G loss: 1.828820]\n",
      "epoch:12 step:11905 [D loss: 0.623906, acc: 68.75%] [G loss: 1.991163]\n",
      "epoch:12 step:11906 [D loss: 0.621788, acc: 67.19%] [G loss: 1.828214]\n",
      "epoch:12 step:11907 [D loss: 0.650034, acc: 60.16%] [G loss: 1.982893]\n",
      "epoch:12 step:11908 [D loss: 0.615480, acc: 67.19%] [G loss: 1.944271]\n",
      "epoch:12 step:11909 [D loss: 0.694155, acc: 60.94%] [G loss: 1.970629]\n",
      "epoch:12 step:11910 [D loss: 0.667780, acc: 57.03%] [G loss: 1.989201]\n",
      "epoch:12 step:11911 [D loss: 0.651665, acc: 60.94%] [G loss: 1.881190]\n",
      "epoch:12 step:11912 [D loss: 0.645633, acc: 62.50%] [G loss: 1.941908]\n",
      "epoch:12 step:11913 [D loss: 0.598240, acc: 72.66%] [G loss: 1.972237]\n",
      "epoch:12 step:11914 [D loss: 0.657879, acc: 55.47%] [G loss: 1.939476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11915 [D loss: 0.691889, acc: 59.38%] [G loss: 1.901620]\n",
      "epoch:12 step:11916 [D loss: 0.599291, acc: 70.31%] [G loss: 2.061041]\n",
      "epoch:12 step:11917 [D loss: 0.645990, acc: 63.28%] [G loss: 2.062480]\n",
      "epoch:12 step:11918 [D loss: 0.633699, acc: 62.50%] [G loss: 1.896286]\n",
      "epoch:12 step:11919 [D loss: 0.636438, acc: 62.50%] [G loss: 1.854642]\n",
      "epoch:12 step:11920 [D loss: 0.656716, acc: 63.28%] [G loss: 1.826113]\n",
      "epoch:12 step:11921 [D loss: 0.573584, acc: 71.09%] [G loss: 1.858220]\n",
      "epoch:12 step:11922 [D loss: 0.665525, acc: 58.59%] [G loss: 2.026692]\n",
      "epoch:12 step:11923 [D loss: 0.609096, acc: 71.88%] [G loss: 1.891649]\n",
      "epoch:12 step:11924 [D loss: 0.599842, acc: 67.19%] [G loss: 2.041457]\n",
      "epoch:12 step:11925 [D loss: 0.637655, acc: 66.41%] [G loss: 2.081496]\n",
      "epoch:12 step:11926 [D loss: 0.620519, acc: 61.72%] [G loss: 2.100642]\n",
      "epoch:12 step:11927 [D loss: 0.677889, acc: 57.03%] [G loss: 1.901561]\n",
      "epoch:12 step:11928 [D loss: 0.617286, acc: 67.19%] [G loss: 1.922802]\n",
      "epoch:12 step:11929 [D loss: 0.655064, acc: 54.69%] [G loss: 2.074410]\n",
      "epoch:12 step:11930 [D loss: 0.626391, acc: 64.06%] [G loss: 2.086192]\n",
      "epoch:12 step:11931 [D loss: 0.673602, acc: 57.81%] [G loss: 1.963568]\n",
      "epoch:12 step:11932 [D loss: 0.626229, acc: 71.09%] [G loss: 1.882074]\n",
      "epoch:12 step:11933 [D loss: 0.632979, acc: 61.72%] [G loss: 2.149338]\n",
      "epoch:12 step:11934 [D loss: 0.610910, acc: 66.41%] [G loss: 2.016184]\n",
      "epoch:12 step:11935 [D loss: 0.574282, acc: 66.41%] [G loss: 2.249073]\n",
      "epoch:12 step:11936 [D loss: 0.624587, acc: 67.97%] [G loss: 2.010541]\n",
      "epoch:12 step:11937 [D loss: 0.581588, acc: 67.19%] [G loss: 2.225218]\n",
      "epoch:12 step:11938 [D loss: 0.558465, acc: 74.22%] [G loss: 2.420764]\n",
      "epoch:12 step:11939 [D loss: 0.624528, acc: 64.06%] [G loss: 2.148043]\n",
      "epoch:12 step:11940 [D loss: 0.646010, acc: 60.94%] [G loss: 2.031209]\n",
      "epoch:12 step:11941 [D loss: 0.678959, acc: 59.38%] [G loss: 2.038829]\n",
      "epoch:12 step:11942 [D loss: 0.626546, acc: 63.28%] [G loss: 1.878560]\n",
      "epoch:12 step:11943 [D loss: 0.637099, acc: 63.28%] [G loss: 2.165458]\n",
      "epoch:12 step:11944 [D loss: 0.626839, acc: 60.94%] [G loss: 2.178582]\n",
      "epoch:12 step:11945 [D loss: 0.610191, acc: 63.28%] [G loss: 2.026297]\n",
      "epoch:12 step:11946 [D loss: 0.681192, acc: 64.06%] [G loss: 1.951666]\n",
      "epoch:12 step:11947 [D loss: 0.720967, acc: 54.69%] [G loss: 2.019922]\n",
      "epoch:12 step:11948 [D loss: 0.593246, acc: 64.84%] [G loss: 1.938506]\n",
      "epoch:12 step:11949 [D loss: 0.657660, acc: 64.06%] [G loss: 1.867486]\n",
      "epoch:12 step:11950 [D loss: 0.636129, acc: 66.41%] [G loss: 2.029421]\n",
      "epoch:12 step:11951 [D loss: 0.628249, acc: 64.84%] [G loss: 2.124458]\n",
      "epoch:12 step:11952 [D loss: 0.589698, acc: 66.41%] [G loss: 2.013608]\n",
      "epoch:12 step:11953 [D loss: 0.602422, acc: 64.06%] [G loss: 2.307839]\n",
      "epoch:12 step:11954 [D loss: 0.698761, acc: 56.25%] [G loss: 2.078461]\n",
      "epoch:12 step:11955 [D loss: 0.657158, acc: 65.62%] [G loss: 2.067470]\n",
      "epoch:12 step:11956 [D loss: 0.617896, acc: 67.19%] [G loss: 2.202373]\n",
      "epoch:12 step:11957 [D loss: 0.651862, acc: 65.62%] [G loss: 1.958727]\n",
      "epoch:12 step:11958 [D loss: 0.727819, acc: 60.94%] [G loss: 1.946555]\n",
      "epoch:12 step:11959 [D loss: 0.620276, acc: 67.97%] [G loss: 1.849217]\n",
      "epoch:12 step:11960 [D loss: 0.676351, acc: 58.59%] [G loss: 1.953399]\n",
      "epoch:12 step:11961 [D loss: 0.625195, acc: 64.06%] [G loss: 2.056729]\n",
      "epoch:12 step:11962 [D loss: 0.681434, acc: 59.38%] [G loss: 2.097827]\n",
      "epoch:12 step:11963 [D loss: 0.631930, acc: 64.06%] [G loss: 2.026091]\n",
      "epoch:12 step:11964 [D loss: 0.673926, acc: 61.72%] [G loss: 1.941909]\n",
      "epoch:12 step:11965 [D loss: 0.632744, acc: 67.19%] [G loss: 1.987872]\n",
      "epoch:12 step:11966 [D loss: 0.701178, acc: 59.38%] [G loss: 1.811983]\n",
      "epoch:12 step:11967 [D loss: 0.628079, acc: 60.94%] [G loss: 1.914039]\n",
      "epoch:12 step:11968 [D loss: 0.597829, acc: 68.75%] [G loss: 2.052154]\n",
      "epoch:12 step:11969 [D loss: 0.623126, acc: 64.84%] [G loss: 2.058197]\n",
      "epoch:12 step:11970 [D loss: 0.668959, acc: 61.72%] [G loss: 1.963494]\n",
      "epoch:12 step:11971 [D loss: 0.659054, acc: 57.81%] [G loss: 2.007933]\n",
      "epoch:12 step:11972 [D loss: 0.653151, acc: 67.19%] [G loss: 2.067071]\n",
      "epoch:12 step:11973 [D loss: 0.598260, acc: 62.50%] [G loss: 1.989970]\n",
      "epoch:12 step:11974 [D loss: 0.645765, acc: 63.28%] [G loss: 1.911922]\n",
      "epoch:12 step:11975 [D loss: 0.654573, acc: 64.06%] [G loss: 2.008137]\n",
      "epoch:12 step:11976 [D loss: 0.632600, acc: 62.50%] [G loss: 1.949522]\n",
      "epoch:12 step:11977 [D loss: 0.653225, acc: 63.28%] [G loss: 1.946449]\n",
      "epoch:12 step:11978 [D loss: 0.631702, acc: 63.28%] [G loss: 1.946902]\n",
      "epoch:12 step:11979 [D loss: 0.606005, acc: 65.62%] [G loss: 2.055990]\n",
      "epoch:12 step:11980 [D loss: 0.638231, acc: 63.28%] [G loss: 2.031265]\n",
      "epoch:12 step:11981 [D loss: 0.611844, acc: 67.19%] [G loss: 2.021158]\n",
      "epoch:12 step:11982 [D loss: 0.643794, acc: 58.59%] [G loss: 1.906306]\n",
      "epoch:12 step:11983 [D loss: 0.626841, acc: 67.97%] [G loss: 2.096548]\n",
      "epoch:12 step:11984 [D loss: 0.631880, acc: 68.75%] [G loss: 1.934796]\n",
      "epoch:12 step:11985 [D loss: 0.597126, acc: 70.31%] [G loss: 1.880459]\n",
      "epoch:12 step:11986 [D loss: 0.672982, acc: 53.91%] [G loss: 1.863364]\n",
      "epoch:12 step:11987 [D loss: 0.682814, acc: 55.47%] [G loss: 1.843506]\n",
      "epoch:12 step:11988 [D loss: 0.673410, acc: 59.38%] [G loss: 1.834314]\n",
      "epoch:12 step:11989 [D loss: 0.648498, acc: 62.50%] [G loss: 2.000431]\n",
      "epoch:12 step:11990 [D loss: 0.622607, acc: 61.72%] [G loss: 1.999418]\n",
      "epoch:12 step:11991 [D loss: 0.651981, acc: 62.50%] [G loss: 2.001222]\n",
      "epoch:12 step:11992 [D loss: 0.672494, acc: 54.69%] [G loss: 1.912852]\n",
      "epoch:12 step:11993 [D loss: 0.622593, acc: 64.06%] [G loss: 1.817453]\n",
      "epoch:12 step:11994 [D loss: 0.634375, acc: 67.19%] [G loss: 1.917681]\n",
      "epoch:12 step:11995 [D loss: 0.609059, acc: 68.75%] [G loss: 1.938361]\n",
      "epoch:12 step:11996 [D loss: 0.643595, acc: 61.72%] [G loss: 1.731514]\n",
      "epoch:12 step:11997 [D loss: 0.648657, acc: 61.72%] [G loss: 1.940246]\n",
      "epoch:12 step:11998 [D loss: 0.643671, acc: 63.28%] [G loss: 2.096780]\n",
      "epoch:12 step:11999 [D loss: 0.641938, acc: 66.41%] [G loss: 1.948171]\n",
      "epoch:12 step:12000 [D loss: 0.608611, acc: 68.75%] [G loss: 1.940715]\n",
      "##############\n",
      "[2.41542563 1.56725638 6.03924688 4.82507956 3.76016252 5.69086983\n",
      " 4.36030497 4.79502847 4.76446013 3.57682298]\n",
      "##########\n",
      "epoch:12 step:12001 [D loss: 0.629723, acc: 62.50%] [G loss: 2.048841]\n",
      "epoch:12 step:12002 [D loss: 0.677918, acc: 61.72%] [G loss: 1.824369]\n",
      "epoch:12 step:12003 [D loss: 0.637287, acc: 62.50%] [G loss: 1.901835]\n",
      "epoch:12 step:12004 [D loss: 0.672412, acc: 62.50%] [G loss: 1.833312]\n",
      "epoch:12 step:12005 [D loss: 0.643996, acc: 61.72%] [G loss: 1.921792]\n",
      "epoch:12 step:12006 [D loss: 0.663702, acc: 58.59%] [G loss: 1.747408]\n",
      "epoch:12 step:12007 [D loss: 0.567679, acc: 66.41%] [G loss: 2.091476]\n",
      "epoch:12 step:12008 [D loss: 0.701534, acc: 57.03%] [G loss: 1.944937]\n",
      "epoch:12 step:12009 [D loss: 0.719468, acc: 55.47%] [G loss: 1.752612]\n",
      "epoch:12 step:12010 [D loss: 0.728493, acc: 51.56%] [G loss: 1.771552]\n",
      "epoch:12 step:12011 [D loss: 0.658034, acc: 60.16%] [G loss: 1.939228]\n",
      "epoch:12 step:12012 [D loss: 0.641579, acc: 67.19%] [G loss: 1.992411]\n",
      "epoch:12 step:12013 [D loss: 0.621864, acc: 57.03%] [G loss: 2.056000]\n",
      "epoch:12 step:12014 [D loss: 0.664575, acc: 60.16%] [G loss: 2.011083]\n",
      "epoch:12 step:12015 [D loss: 0.665854, acc: 59.38%] [G loss: 2.036901]\n",
      "epoch:12 step:12016 [D loss: 0.663958, acc: 63.28%] [G loss: 2.011952]\n",
      "epoch:12 step:12017 [D loss: 0.629360, acc: 61.72%] [G loss: 1.932422]\n",
      "epoch:12 step:12018 [D loss: 0.556976, acc: 71.88%] [G loss: 2.145564]\n",
      "epoch:12 step:12019 [D loss: 0.582517, acc: 67.97%] [G loss: 2.321628]\n",
      "epoch:12 step:12020 [D loss: 0.615944, acc: 65.62%] [G loss: 2.026707]\n",
      "epoch:12 step:12021 [D loss: 0.588389, acc: 72.66%] [G loss: 2.014296]\n",
      "epoch:12 step:12022 [D loss: 0.625085, acc: 63.28%] [G loss: 2.141407]\n",
      "epoch:12 step:12023 [D loss: 0.645588, acc: 65.62%] [G loss: 1.926680]\n",
      "epoch:12 step:12024 [D loss: 0.631874, acc: 69.53%] [G loss: 1.999499]\n",
      "epoch:12 step:12025 [D loss: 0.589234, acc: 67.19%] [G loss: 2.228814]\n",
      "epoch:12 step:12026 [D loss: 0.589343, acc: 72.66%] [G loss: 2.266500]\n",
      "epoch:12 step:12027 [D loss: 0.698310, acc: 64.06%] [G loss: 2.101094]\n",
      "epoch:12 step:12028 [D loss: 0.663472, acc: 57.03%] [G loss: 1.847371]\n",
      "epoch:12 step:12029 [D loss: 0.706923, acc: 55.47%] [G loss: 1.945350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12030 [D loss: 0.593893, acc: 70.31%] [G loss: 2.084980]\n",
      "epoch:12 step:12031 [D loss: 0.704719, acc: 57.03%] [G loss: 1.939888]\n",
      "epoch:12 step:12032 [D loss: 0.688819, acc: 54.69%] [G loss: 1.867392]\n",
      "epoch:12 step:12033 [D loss: 0.666445, acc: 60.16%] [G loss: 1.943412]\n",
      "epoch:12 step:12034 [D loss: 0.635628, acc: 65.62%] [G loss: 1.900393]\n",
      "epoch:12 step:12035 [D loss: 0.661398, acc: 55.47%] [G loss: 1.990279]\n",
      "epoch:12 step:12036 [D loss: 0.606763, acc: 64.06%] [G loss: 2.160284]\n",
      "epoch:12 step:12037 [D loss: 0.618068, acc: 64.06%] [G loss: 1.998336]\n",
      "epoch:12 step:12038 [D loss: 0.664100, acc: 60.16%] [G loss: 1.906578]\n",
      "epoch:12 step:12039 [D loss: 0.648156, acc: 61.72%] [G loss: 1.806850]\n",
      "epoch:12 step:12040 [D loss: 0.666861, acc: 62.50%] [G loss: 1.929680]\n",
      "epoch:12 step:12041 [D loss: 0.641689, acc: 61.72%] [G loss: 1.915131]\n",
      "epoch:12 step:12042 [D loss: 0.620678, acc: 67.97%] [G loss: 1.891866]\n",
      "epoch:12 step:12043 [D loss: 0.637757, acc: 62.50%] [G loss: 1.942305]\n",
      "epoch:12 step:12044 [D loss: 0.694593, acc: 57.81%] [G loss: 1.704987]\n",
      "epoch:12 step:12045 [D loss: 0.633322, acc: 64.06%] [G loss: 1.836634]\n",
      "epoch:12 step:12046 [D loss: 0.623203, acc: 65.62%] [G loss: 1.848396]\n",
      "epoch:12 step:12047 [D loss: 0.638810, acc: 65.62%] [G loss: 1.904115]\n",
      "epoch:12 step:12048 [D loss: 0.619281, acc: 68.75%] [G loss: 1.957208]\n",
      "epoch:12 step:12049 [D loss: 0.614754, acc: 69.53%] [G loss: 1.966305]\n",
      "epoch:12 step:12050 [D loss: 0.628135, acc: 57.81%] [G loss: 1.997042]\n",
      "epoch:12 step:12051 [D loss: 0.577960, acc: 71.88%] [G loss: 2.208393]\n",
      "epoch:12 step:12052 [D loss: 0.633215, acc: 60.94%] [G loss: 2.069689]\n",
      "epoch:12 step:12053 [D loss: 0.590198, acc: 73.44%] [G loss: 2.030222]\n",
      "epoch:12 step:12054 [D loss: 0.653955, acc: 63.28%] [G loss: 2.079700]\n",
      "epoch:12 step:12055 [D loss: 0.618406, acc: 69.53%] [G loss: 1.951148]\n",
      "epoch:12 step:12056 [D loss: 0.668633, acc: 59.38%] [G loss: 1.928717]\n",
      "epoch:12 step:12057 [D loss: 0.627796, acc: 62.50%] [G loss: 2.047212]\n",
      "epoch:12 step:12058 [D loss: 0.662387, acc: 61.72%] [G loss: 1.836704]\n",
      "epoch:12 step:12059 [D loss: 0.643724, acc: 65.62%] [G loss: 2.183184]\n",
      "epoch:12 step:12060 [D loss: 0.559625, acc: 71.88%] [G loss: 2.263265]\n",
      "epoch:12 step:12061 [D loss: 0.640886, acc: 61.72%] [G loss: 1.904538]\n",
      "epoch:12 step:12062 [D loss: 0.660588, acc: 61.72%] [G loss: 1.837539]\n",
      "epoch:12 step:12063 [D loss: 0.634708, acc: 62.50%] [G loss: 1.926060]\n",
      "epoch:12 step:12064 [D loss: 0.704376, acc: 60.16%] [G loss: 1.864545]\n",
      "epoch:12 step:12065 [D loss: 0.615120, acc: 62.50%] [G loss: 2.027972]\n",
      "epoch:12 step:12066 [D loss: 0.635455, acc: 61.72%] [G loss: 2.138751]\n",
      "epoch:12 step:12067 [D loss: 0.625317, acc: 66.41%] [G loss: 2.022572]\n",
      "epoch:12 step:12068 [D loss: 0.669337, acc: 63.28%] [G loss: 1.815828]\n",
      "epoch:12 step:12069 [D loss: 0.634943, acc: 62.50%] [G loss: 2.032573]\n",
      "epoch:12 step:12070 [D loss: 0.664734, acc: 60.16%] [G loss: 1.881433]\n",
      "epoch:12 step:12071 [D loss: 0.728062, acc: 45.31%] [G loss: 1.776994]\n",
      "epoch:12 step:12072 [D loss: 0.650585, acc: 60.16%] [G loss: 1.891891]\n",
      "epoch:12 step:12073 [D loss: 0.689042, acc: 60.16%] [G loss: 1.825001]\n",
      "epoch:12 step:12074 [D loss: 0.712501, acc: 53.12%] [G loss: 1.871545]\n",
      "epoch:12 step:12075 [D loss: 0.661607, acc: 64.84%] [G loss: 1.954372]\n",
      "epoch:12 step:12076 [D loss: 0.619623, acc: 62.50%] [G loss: 1.884277]\n",
      "epoch:12 step:12077 [D loss: 0.624278, acc: 63.28%] [G loss: 1.910763]\n",
      "epoch:12 step:12078 [D loss: 0.609924, acc: 65.62%] [G loss: 1.886581]\n",
      "epoch:12 step:12079 [D loss: 0.596003, acc: 75.00%] [G loss: 1.902855]\n",
      "epoch:12 step:12080 [D loss: 0.628706, acc: 64.84%] [G loss: 1.966495]\n",
      "epoch:12 step:12081 [D loss: 0.621967, acc: 65.62%] [G loss: 2.023063]\n",
      "epoch:12 step:12082 [D loss: 0.638092, acc: 65.62%] [G loss: 2.131361]\n",
      "epoch:12 step:12083 [D loss: 0.647577, acc: 60.16%] [G loss: 2.065844]\n",
      "epoch:12 step:12084 [D loss: 0.648074, acc: 66.41%] [G loss: 2.024868]\n",
      "epoch:12 step:12085 [D loss: 0.600906, acc: 67.19%] [G loss: 2.030740]\n",
      "epoch:12 step:12086 [D loss: 0.646653, acc: 64.06%] [G loss: 2.061279]\n",
      "epoch:12 step:12087 [D loss: 0.666825, acc: 56.25%] [G loss: 2.080912]\n",
      "epoch:12 step:12088 [D loss: 0.627351, acc: 65.62%] [G loss: 1.986450]\n",
      "epoch:12 step:12089 [D loss: 0.586232, acc: 71.09%] [G loss: 2.049062]\n",
      "epoch:12 step:12090 [D loss: 0.655546, acc: 60.94%] [G loss: 1.878877]\n",
      "epoch:12 step:12091 [D loss: 0.675262, acc: 61.72%] [G loss: 1.938964]\n",
      "epoch:12 step:12092 [D loss: 0.562845, acc: 71.09%] [G loss: 1.960991]\n",
      "epoch:12 step:12093 [D loss: 0.628437, acc: 63.28%] [G loss: 2.103254]\n",
      "epoch:12 step:12094 [D loss: 0.627796, acc: 67.19%] [G loss: 1.944118]\n",
      "epoch:12 step:12095 [D loss: 0.579597, acc: 61.72%] [G loss: 2.229003]\n",
      "epoch:12 step:12096 [D loss: 0.587612, acc: 67.19%] [G loss: 2.179655]\n",
      "epoch:12 step:12097 [D loss: 0.615565, acc: 69.53%] [G loss: 2.080155]\n",
      "epoch:12 step:12098 [D loss: 0.589364, acc: 72.66%] [G loss: 1.948477]\n",
      "epoch:12 step:12099 [D loss: 0.697893, acc: 55.47%] [G loss: 1.993943]\n",
      "epoch:12 step:12100 [D loss: 0.666638, acc: 60.94%] [G loss: 1.912196]\n",
      "epoch:12 step:12101 [D loss: 0.673412, acc: 62.50%] [G loss: 1.929331]\n",
      "epoch:12 step:12102 [D loss: 0.682733, acc: 52.34%] [G loss: 1.994687]\n",
      "epoch:12 step:12103 [D loss: 0.671459, acc: 57.03%] [G loss: 1.840701]\n",
      "epoch:12 step:12104 [D loss: 0.626765, acc: 64.06%] [G loss: 1.974739]\n",
      "epoch:12 step:12105 [D loss: 0.645888, acc: 63.28%] [G loss: 1.867920]\n",
      "epoch:12 step:12106 [D loss: 0.659834, acc: 61.72%] [G loss: 1.906317]\n",
      "epoch:12 step:12107 [D loss: 0.587014, acc: 69.53%] [G loss: 1.864109]\n",
      "epoch:12 step:12108 [D loss: 0.685386, acc: 57.81%] [G loss: 1.993565]\n",
      "epoch:12 step:12109 [D loss: 0.662334, acc: 57.03%] [G loss: 1.841632]\n",
      "epoch:12 step:12110 [D loss: 0.642586, acc: 60.16%] [G loss: 1.906025]\n",
      "epoch:12 step:12111 [D loss: 0.632851, acc: 63.28%] [G loss: 1.976101]\n",
      "epoch:12 step:12112 [D loss: 0.640018, acc: 67.97%] [G loss: 1.833848]\n",
      "epoch:12 step:12113 [D loss: 0.612705, acc: 63.28%] [G loss: 1.837841]\n",
      "epoch:12 step:12114 [D loss: 0.687148, acc: 57.03%] [G loss: 2.110233]\n",
      "epoch:12 step:12115 [D loss: 0.646193, acc: 58.59%] [G loss: 1.891878]\n",
      "epoch:12 step:12116 [D loss: 0.642420, acc: 65.62%] [G loss: 1.920377]\n",
      "epoch:12 step:12117 [D loss: 0.664820, acc: 58.59%] [G loss: 1.947690]\n",
      "epoch:12 step:12118 [D loss: 0.676209, acc: 57.81%] [G loss: 1.858174]\n",
      "epoch:12 step:12119 [D loss: 0.629853, acc: 67.19%] [G loss: 2.081961]\n",
      "epoch:12 step:12120 [D loss: 0.648441, acc: 62.50%] [G loss: 1.936629]\n",
      "epoch:12 step:12121 [D loss: 0.649210, acc: 64.06%] [G loss: 1.917343]\n",
      "epoch:12 step:12122 [D loss: 0.659969, acc: 57.81%] [G loss: 1.830379]\n",
      "epoch:12 step:12123 [D loss: 0.632684, acc: 68.75%] [G loss: 1.999807]\n",
      "epoch:12 step:12124 [D loss: 0.657546, acc: 60.94%] [G loss: 1.945458]\n",
      "epoch:12 step:12125 [D loss: 0.609119, acc: 63.28%] [G loss: 2.018687]\n",
      "epoch:12 step:12126 [D loss: 0.637281, acc: 67.97%] [G loss: 1.950288]\n",
      "epoch:12 step:12127 [D loss: 0.655308, acc: 58.59%] [G loss: 2.003021]\n",
      "epoch:12 step:12128 [D loss: 0.601059, acc: 66.41%] [G loss: 2.042487]\n",
      "epoch:12 step:12129 [D loss: 0.620341, acc: 64.84%] [G loss: 1.904317]\n",
      "epoch:12 step:12130 [D loss: 0.658849, acc: 60.16%] [G loss: 2.117887]\n",
      "epoch:12 step:12131 [D loss: 0.637672, acc: 66.41%] [G loss: 1.977962]\n",
      "epoch:12 step:12132 [D loss: 0.589319, acc: 70.31%] [G loss: 2.067570]\n",
      "epoch:12 step:12133 [D loss: 0.642804, acc: 64.06%] [G loss: 1.956911]\n",
      "epoch:12 step:12134 [D loss: 0.605736, acc: 67.97%] [G loss: 2.055097]\n",
      "epoch:12 step:12135 [D loss: 0.655058, acc: 62.50%] [G loss: 1.802817]\n",
      "epoch:12 step:12136 [D loss: 0.696288, acc: 59.38%] [G loss: 1.862455]\n",
      "epoch:12 step:12137 [D loss: 0.613049, acc: 68.75%] [G loss: 2.023336]\n",
      "epoch:12 step:12138 [D loss: 0.629256, acc: 60.94%] [G loss: 2.133046]\n",
      "epoch:12 step:12139 [D loss: 0.644262, acc: 67.19%] [G loss: 1.952107]\n",
      "epoch:12 step:12140 [D loss: 0.670757, acc: 67.19%] [G loss: 2.028475]\n",
      "epoch:12 step:12141 [D loss: 0.600262, acc: 68.75%] [G loss: 2.054368]\n",
      "epoch:12 step:12142 [D loss: 0.673767, acc: 65.62%] [G loss: 1.918039]\n",
      "epoch:12 step:12143 [D loss: 0.639716, acc: 65.62%] [G loss: 2.047926]\n",
      "epoch:12 step:12144 [D loss: 0.601809, acc: 67.97%] [G loss: 2.176710]\n",
      "epoch:12 step:12145 [D loss: 0.585240, acc: 70.31%] [G loss: 2.082935]\n",
      "epoch:12 step:12146 [D loss: 0.651548, acc: 64.84%] [G loss: 2.011090]\n",
      "epoch:12 step:12147 [D loss: 0.626496, acc: 63.28%] [G loss: 1.955094]\n",
      "epoch:12 step:12148 [D loss: 0.674815, acc: 58.59%] [G loss: 1.944670]\n",
      "epoch:12 step:12149 [D loss: 0.657505, acc: 57.81%] [G loss: 1.969273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12150 [D loss: 0.681411, acc: 56.25%] [G loss: 1.996079]\n",
      "epoch:12 step:12151 [D loss: 0.633096, acc: 62.50%] [G loss: 2.072813]\n",
      "epoch:12 step:12152 [D loss: 0.672569, acc: 57.03%] [G loss: 1.959650]\n",
      "epoch:12 step:12153 [D loss: 0.573161, acc: 75.78%] [G loss: 2.196652]\n",
      "epoch:12 step:12154 [D loss: 0.633041, acc: 63.28%] [G loss: 2.032422]\n",
      "epoch:12 step:12155 [D loss: 0.621560, acc: 64.06%] [G loss: 2.161300]\n",
      "epoch:12 step:12156 [D loss: 0.618461, acc: 67.19%] [G loss: 2.211547]\n",
      "epoch:12 step:12157 [D loss: 0.673580, acc: 57.03%] [G loss: 2.052870]\n",
      "epoch:12 step:12158 [D loss: 0.684274, acc: 56.25%] [G loss: 1.947977]\n",
      "epoch:12 step:12159 [D loss: 0.575300, acc: 69.53%] [G loss: 2.089118]\n",
      "epoch:12 step:12160 [D loss: 0.620865, acc: 62.50%] [G loss: 2.005180]\n",
      "epoch:12 step:12161 [D loss: 0.662949, acc: 57.81%] [G loss: 1.881322]\n",
      "epoch:12 step:12162 [D loss: 0.596367, acc: 72.66%] [G loss: 1.989287]\n",
      "epoch:12 step:12163 [D loss: 0.597789, acc: 70.31%] [G loss: 2.163469]\n",
      "epoch:12 step:12164 [D loss: 0.687107, acc: 57.81%] [G loss: 1.899825]\n",
      "epoch:12 step:12165 [D loss: 0.642916, acc: 62.50%] [G loss: 1.997154]\n",
      "epoch:12 step:12166 [D loss: 0.626328, acc: 69.53%] [G loss: 2.071908]\n",
      "epoch:12 step:12167 [D loss: 0.595646, acc: 63.28%] [G loss: 2.108490]\n",
      "epoch:12 step:12168 [D loss: 0.531169, acc: 76.56%] [G loss: 2.340690]\n",
      "epoch:12 step:12169 [D loss: 0.543062, acc: 73.44%] [G loss: 2.264777]\n",
      "epoch:12 step:12170 [D loss: 0.568056, acc: 67.19%] [G loss: 2.404194]\n",
      "epoch:12 step:12171 [D loss: 0.649871, acc: 62.50%] [G loss: 2.106678]\n",
      "epoch:12 step:12172 [D loss: 0.691082, acc: 57.03%] [G loss: 1.918819]\n",
      "epoch:12 step:12173 [D loss: 0.773378, acc: 45.31%] [G loss: 2.024565]\n",
      "epoch:12 step:12174 [D loss: 0.566608, acc: 69.53%] [G loss: 2.265387]\n",
      "epoch:12 step:12175 [D loss: 0.612140, acc: 67.19%] [G loss: 2.125565]\n",
      "epoch:12 step:12176 [D loss: 0.587894, acc: 73.44%] [G loss: 2.135923]\n",
      "epoch:12 step:12177 [D loss: 0.611013, acc: 71.09%] [G loss: 2.067167]\n",
      "epoch:12 step:12178 [D loss: 0.651593, acc: 63.28%] [G loss: 2.039020]\n",
      "epoch:12 step:12179 [D loss: 0.610793, acc: 63.28%] [G loss: 2.190202]\n",
      "epoch:12 step:12180 [D loss: 0.622704, acc: 67.97%] [G loss: 2.158277]\n",
      "epoch:12 step:12181 [D loss: 0.596799, acc: 71.09%] [G loss: 2.568982]\n",
      "epoch:13 step:12182 [D loss: 0.589299, acc: 67.19%] [G loss: 2.176054]\n",
      "epoch:13 step:12183 [D loss: 0.673624, acc: 64.84%] [G loss: 2.097906]\n",
      "epoch:13 step:12184 [D loss: 0.650503, acc: 62.50%] [G loss: 2.002992]\n",
      "epoch:13 step:12185 [D loss: 0.628935, acc: 60.16%] [G loss: 2.140118]\n",
      "epoch:13 step:12186 [D loss: 0.585243, acc: 73.44%] [G loss: 2.032722]\n",
      "epoch:13 step:12187 [D loss: 0.671637, acc: 59.38%] [G loss: 1.918082]\n",
      "epoch:13 step:12188 [D loss: 0.638600, acc: 64.06%] [G loss: 1.985734]\n",
      "epoch:13 step:12189 [D loss: 0.627011, acc: 63.28%] [G loss: 2.097291]\n",
      "epoch:13 step:12190 [D loss: 0.645217, acc: 64.84%] [G loss: 2.285269]\n",
      "epoch:13 step:12191 [D loss: 0.640171, acc: 67.19%] [G loss: 2.079622]\n",
      "epoch:13 step:12192 [D loss: 0.587374, acc: 67.97%] [G loss: 2.009821]\n",
      "epoch:13 step:12193 [D loss: 0.637355, acc: 58.59%] [G loss: 2.060905]\n",
      "epoch:13 step:12194 [D loss: 0.626803, acc: 69.53%] [G loss: 2.109280]\n",
      "epoch:13 step:12195 [D loss: 0.639114, acc: 60.94%] [G loss: 2.026860]\n",
      "epoch:13 step:12196 [D loss: 0.592509, acc: 66.41%] [G loss: 2.295873]\n",
      "epoch:13 step:12197 [D loss: 0.686307, acc: 60.94%] [G loss: 2.223131]\n",
      "epoch:13 step:12198 [D loss: 0.690958, acc: 57.81%] [G loss: 1.948041]\n",
      "epoch:13 step:12199 [D loss: 0.631228, acc: 62.50%] [G loss: 1.964480]\n",
      "epoch:13 step:12200 [D loss: 0.655288, acc: 67.97%] [G loss: 2.009586]\n",
      "##############\n",
      "[2.43039101 1.41857727 6.22008441 4.7873427  3.57066172 5.85141717\n",
      " 4.41569773 4.89369918 4.64525442 3.67387956]\n",
      "##########\n",
      "epoch:13 step:12201 [D loss: 0.703382, acc: 61.72%] [G loss: 1.701828]\n",
      "epoch:13 step:12202 [D loss: 0.624641, acc: 64.06%] [G loss: 1.975072]\n",
      "epoch:13 step:12203 [D loss: 0.679582, acc: 60.94%] [G loss: 1.810948]\n",
      "epoch:13 step:12204 [D loss: 0.606856, acc: 67.97%] [G loss: 2.014397]\n",
      "epoch:13 step:12205 [D loss: 0.631377, acc: 60.94%] [G loss: 1.917254]\n",
      "epoch:13 step:12206 [D loss: 0.596769, acc: 69.53%] [G loss: 2.234156]\n",
      "epoch:13 step:12207 [D loss: 0.590799, acc: 64.06%] [G loss: 1.788178]\n",
      "epoch:13 step:12208 [D loss: 0.668215, acc: 64.06%] [G loss: 1.806762]\n",
      "epoch:13 step:12209 [D loss: 0.631953, acc: 61.72%] [G loss: 1.873293]\n",
      "epoch:13 step:12210 [D loss: 0.575626, acc: 68.75%] [G loss: 1.943935]\n",
      "epoch:13 step:12211 [D loss: 0.649642, acc: 61.72%] [G loss: 1.960732]\n",
      "epoch:13 step:12212 [D loss: 0.691372, acc: 57.81%] [G loss: 1.853726]\n",
      "epoch:13 step:12213 [D loss: 0.663155, acc: 60.94%] [G loss: 1.901490]\n",
      "epoch:13 step:12214 [D loss: 0.633721, acc: 60.94%] [G loss: 2.036835]\n",
      "epoch:13 step:12215 [D loss: 0.648298, acc: 63.28%] [G loss: 2.048580]\n",
      "epoch:13 step:12216 [D loss: 0.667078, acc: 59.38%] [G loss: 1.870819]\n",
      "epoch:13 step:12217 [D loss: 0.615346, acc: 67.97%] [G loss: 1.921923]\n",
      "epoch:13 step:12218 [D loss: 0.596641, acc: 70.31%] [G loss: 2.090556]\n",
      "epoch:13 step:12219 [D loss: 0.681593, acc: 57.81%] [G loss: 2.131478]\n",
      "epoch:13 step:12220 [D loss: 0.629283, acc: 67.19%] [G loss: 2.222414]\n",
      "epoch:13 step:12221 [D loss: 0.615046, acc: 67.19%] [G loss: 2.229236]\n",
      "epoch:13 step:12222 [D loss: 0.666325, acc: 60.94%] [G loss: 1.841807]\n",
      "epoch:13 step:12223 [D loss: 0.590475, acc: 71.88%] [G loss: 2.216348]\n",
      "epoch:13 step:12224 [D loss: 0.578797, acc: 67.97%] [G loss: 1.975642]\n",
      "epoch:13 step:12225 [D loss: 0.658330, acc: 60.94%] [G loss: 1.975676]\n",
      "epoch:13 step:12226 [D loss: 0.672061, acc: 60.16%] [G loss: 1.978085]\n",
      "epoch:13 step:12227 [D loss: 0.617033, acc: 67.97%] [G loss: 1.878154]\n",
      "epoch:13 step:12228 [D loss: 0.645501, acc: 64.84%] [G loss: 2.200518]\n",
      "epoch:13 step:12229 [D loss: 0.593833, acc: 68.75%] [G loss: 2.047373]\n",
      "epoch:13 step:12230 [D loss: 0.571728, acc: 67.97%] [G loss: 2.135207]\n",
      "epoch:13 step:12231 [D loss: 0.598012, acc: 68.75%] [G loss: 2.099652]\n",
      "epoch:13 step:12232 [D loss: 0.628584, acc: 64.84%] [G loss: 1.998034]\n",
      "epoch:13 step:12233 [D loss: 0.621143, acc: 65.62%] [G loss: 1.969902]\n",
      "epoch:13 step:12234 [D loss: 0.659581, acc: 63.28%] [G loss: 2.136264]\n",
      "epoch:13 step:12235 [D loss: 0.624521, acc: 63.28%] [G loss: 2.209463]\n",
      "epoch:13 step:12236 [D loss: 0.583628, acc: 69.53%] [G loss: 2.219524]\n",
      "epoch:13 step:12237 [D loss: 0.608827, acc: 67.19%] [G loss: 2.139027]\n",
      "epoch:13 step:12238 [D loss: 0.621277, acc: 65.62%] [G loss: 2.149755]\n",
      "epoch:13 step:12239 [D loss: 0.614402, acc: 71.09%] [G loss: 2.123415]\n",
      "epoch:13 step:12240 [D loss: 0.680033, acc: 57.81%] [G loss: 1.931501]\n",
      "epoch:13 step:12241 [D loss: 0.580270, acc: 72.66%] [G loss: 2.046438]\n",
      "epoch:13 step:12242 [D loss: 0.656635, acc: 58.59%] [G loss: 1.999164]\n",
      "epoch:13 step:12243 [D loss: 0.615578, acc: 66.41%] [G loss: 2.152984]\n",
      "epoch:13 step:12244 [D loss: 0.672891, acc: 64.06%] [G loss: 1.977033]\n",
      "epoch:13 step:12245 [D loss: 0.671707, acc: 60.16%] [G loss: 1.964165]\n",
      "epoch:13 step:12246 [D loss: 0.635552, acc: 63.28%] [G loss: 1.980803]\n",
      "epoch:13 step:12247 [D loss: 0.630597, acc: 61.72%] [G loss: 2.096479]\n",
      "epoch:13 step:12248 [D loss: 0.613796, acc: 64.84%] [G loss: 1.968240]\n",
      "epoch:13 step:12249 [D loss: 0.630131, acc: 63.28%] [G loss: 1.972507]\n",
      "epoch:13 step:12250 [D loss: 0.607290, acc: 71.88%] [G loss: 2.064345]\n",
      "epoch:13 step:12251 [D loss: 0.585418, acc: 67.97%] [G loss: 2.129374]\n",
      "epoch:13 step:12252 [D loss: 0.648047, acc: 61.72%] [G loss: 1.992345]\n",
      "epoch:13 step:12253 [D loss: 0.613722, acc: 58.59%] [G loss: 1.960305]\n",
      "epoch:13 step:12254 [D loss: 0.643165, acc: 66.41%] [G loss: 1.942330]\n",
      "epoch:13 step:12255 [D loss: 0.566350, acc: 71.88%] [G loss: 2.146234]\n",
      "epoch:13 step:12256 [D loss: 0.667086, acc: 70.31%] [G loss: 2.342491]\n",
      "epoch:13 step:12257 [D loss: 0.548768, acc: 65.62%] [G loss: 2.331464]\n",
      "epoch:13 step:12258 [D loss: 0.603836, acc: 67.19%] [G loss: 2.202508]\n",
      "epoch:13 step:12259 [D loss: 0.681603, acc: 59.38%] [G loss: 1.998708]\n",
      "epoch:13 step:12260 [D loss: 0.665426, acc: 60.16%] [G loss: 1.886277]\n",
      "epoch:13 step:12261 [D loss: 0.597996, acc: 69.53%] [G loss: 1.912112]\n",
      "epoch:13 step:12262 [D loss: 0.659659, acc: 64.06%] [G loss: 1.881570]\n",
      "epoch:13 step:12263 [D loss: 0.647113, acc: 66.41%] [G loss: 2.044844]\n",
      "epoch:13 step:12264 [D loss: 0.640597, acc: 64.06%] [G loss: 2.060693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12265 [D loss: 0.672070, acc: 61.72%] [G loss: 2.016649]\n",
      "epoch:13 step:12266 [D loss: 0.626664, acc: 67.19%] [G loss: 1.913316]\n",
      "epoch:13 step:12267 [D loss: 0.691213, acc: 61.72%] [G loss: 1.808417]\n",
      "epoch:13 step:12268 [D loss: 0.655812, acc: 64.06%] [G loss: 1.832880]\n",
      "epoch:13 step:12269 [D loss: 0.637394, acc: 70.31%] [G loss: 1.965566]\n",
      "epoch:13 step:12270 [D loss: 0.618481, acc: 68.75%] [G loss: 1.962713]\n",
      "epoch:13 step:12271 [D loss: 0.652884, acc: 62.50%] [G loss: 1.904572]\n",
      "epoch:13 step:12272 [D loss: 0.648214, acc: 60.94%] [G loss: 1.982043]\n",
      "epoch:13 step:12273 [D loss: 0.621521, acc: 72.66%] [G loss: 2.131564]\n",
      "epoch:13 step:12274 [D loss: 0.635484, acc: 64.06%] [G loss: 2.005786]\n",
      "epoch:13 step:12275 [D loss: 0.588518, acc: 66.41%] [G loss: 2.099299]\n",
      "epoch:13 step:12276 [D loss: 0.601143, acc: 68.75%] [G loss: 1.968423]\n",
      "epoch:13 step:12277 [D loss: 0.641962, acc: 66.41%] [G loss: 1.978644]\n",
      "epoch:13 step:12278 [D loss: 0.580996, acc: 75.00%] [G loss: 1.957855]\n",
      "epoch:13 step:12279 [D loss: 0.684526, acc: 58.59%] [G loss: 1.888985]\n",
      "epoch:13 step:12280 [D loss: 0.625508, acc: 66.41%] [G loss: 1.990286]\n",
      "epoch:13 step:12281 [D loss: 0.615063, acc: 64.84%] [G loss: 2.050149]\n",
      "epoch:13 step:12282 [D loss: 0.642875, acc: 66.41%] [G loss: 2.088790]\n",
      "epoch:13 step:12283 [D loss: 0.651396, acc: 63.28%] [G loss: 1.957543]\n",
      "epoch:13 step:12284 [D loss: 0.619932, acc: 66.41%] [G loss: 2.150545]\n",
      "epoch:13 step:12285 [D loss: 0.691545, acc: 58.59%] [G loss: 1.985259]\n",
      "epoch:13 step:12286 [D loss: 0.664810, acc: 58.59%] [G loss: 1.966467]\n",
      "epoch:13 step:12287 [D loss: 0.597079, acc: 67.19%] [G loss: 2.088789]\n",
      "epoch:13 step:12288 [D loss: 0.635413, acc: 65.62%] [G loss: 2.011735]\n",
      "epoch:13 step:12289 [D loss: 0.696283, acc: 60.94%] [G loss: 1.919372]\n",
      "epoch:13 step:12290 [D loss: 0.704068, acc: 57.81%] [G loss: 1.985128]\n",
      "epoch:13 step:12291 [D loss: 0.590279, acc: 65.62%] [G loss: 1.890146]\n",
      "epoch:13 step:12292 [D loss: 0.651989, acc: 59.38%] [G loss: 2.116607]\n",
      "epoch:13 step:12293 [D loss: 0.603745, acc: 68.75%] [G loss: 2.081198]\n",
      "epoch:13 step:12294 [D loss: 0.647033, acc: 63.28%] [G loss: 1.924398]\n",
      "epoch:13 step:12295 [D loss: 0.626404, acc: 64.84%] [G loss: 1.939695]\n",
      "epoch:13 step:12296 [D loss: 0.622309, acc: 65.62%] [G loss: 2.243648]\n",
      "epoch:13 step:12297 [D loss: 0.624276, acc: 67.19%] [G loss: 2.182352]\n",
      "epoch:13 step:12298 [D loss: 0.616201, acc: 64.84%] [G loss: 2.226424]\n",
      "epoch:13 step:12299 [D loss: 0.579024, acc: 71.88%] [G loss: 2.110089]\n",
      "epoch:13 step:12300 [D loss: 0.607467, acc: 68.75%] [G loss: 2.304427]\n",
      "epoch:13 step:12301 [D loss: 0.670380, acc: 65.62%] [G loss: 2.036994]\n",
      "epoch:13 step:12302 [D loss: 0.638339, acc: 56.25%] [G loss: 2.129581]\n",
      "epoch:13 step:12303 [D loss: 0.595151, acc: 68.75%] [G loss: 2.291708]\n",
      "epoch:13 step:12304 [D loss: 0.695393, acc: 58.59%] [G loss: 1.972937]\n",
      "epoch:13 step:12305 [D loss: 0.634522, acc: 66.41%] [G loss: 1.927156]\n",
      "epoch:13 step:12306 [D loss: 0.630625, acc: 66.41%] [G loss: 1.971355]\n",
      "epoch:13 step:12307 [D loss: 0.644216, acc: 63.28%] [G loss: 1.807332]\n",
      "epoch:13 step:12308 [D loss: 0.658396, acc: 60.16%] [G loss: 2.017407]\n",
      "epoch:13 step:12309 [D loss: 0.656562, acc: 58.59%] [G loss: 2.037694]\n",
      "epoch:13 step:12310 [D loss: 0.691704, acc: 55.47%] [G loss: 1.974292]\n",
      "epoch:13 step:12311 [D loss: 0.614532, acc: 60.94%] [G loss: 1.923461]\n",
      "epoch:13 step:12312 [D loss: 0.672685, acc: 60.94%] [G loss: 1.957340]\n",
      "epoch:13 step:12313 [D loss: 0.626615, acc: 66.41%] [G loss: 1.982996]\n",
      "epoch:13 step:12314 [D loss: 0.705852, acc: 55.47%] [G loss: 1.889083]\n",
      "epoch:13 step:12315 [D loss: 0.643729, acc: 63.28%] [G loss: 1.907796]\n",
      "epoch:13 step:12316 [D loss: 0.644613, acc: 65.62%] [G loss: 1.959825]\n",
      "epoch:13 step:12317 [D loss: 0.643462, acc: 65.62%] [G loss: 1.897899]\n",
      "epoch:13 step:12318 [D loss: 0.626101, acc: 69.53%] [G loss: 1.965085]\n",
      "epoch:13 step:12319 [D loss: 0.682117, acc: 57.81%] [G loss: 1.920490]\n",
      "epoch:13 step:12320 [D loss: 0.625888, acc: 69.53%] [G loss: 2.022074]\n",
      "epoch:13 step:12321 [D loss: 0.651920, acc: 61.72%] [G loss: 1.950991]\n",
      "epoch:13 step:12322 [D loss: 0.638024, acc: 67.97%] [G loss: 1.856653]\n",
      "epoch:13 step:12323 [D loss: 0.657604, acc: 65.62%] [G loss: 1.855320]\n",
      "epoch:13 step:12324 [D loss: 0.653053, acc: 63.28%] [G loss: 1.814656]\n",
      "epoch:13 step:12325 [D loss: 0.584653, acc: 69.53%] [G loss: 1.910725]\n",
      "epoch:13 step:12326 [D loss: 0.687248, acc: 60.94%] [G loss: 1.989955]\n",
      "epoch:13 step:12327 [D loss: 0.621908, acc: 64.84%] [G loss: 2.027659]\n",
      "epoch:13 step:12328 [D loss: 0.666814, acc: 61.72%] [G loss: 2.061234]\n",
      "epoch:13 step:12329 [D loss: 0.669437, acc: 56.25%] [G loss: 1.996847]\n",
      "epoch:13 step:12330 [D loss: 0.682131, acc: 55.47%] [G loss: 1.946306]\n",
      "epoch:13 step:12331 [D loss: 0.610655, acc: 71.88%] [G loss: 1.958532]\n",
      "epoch:13 step:12332 [D loss: 0.629553, acc: 67.97%] [G loss: 2.060145]\n",
      "epoch:13 step:12333 [D loss: 0.592332, acc: 71.88%] [G loss: 1.889030]\n",
      "epoch:13 step:12334 [D loss: 0.655403, acc: 60.94%] [G loss: 1.878431]\n",
      "epoch:13 step:12335 [D loss: 0.583182, acc: 75.00%] [G loss: 2.213261]\n",
      "epoch:13 step:12336 [D loss: 0.590363, acc: 71.09%] [G loss: 2.273911]\n",
      "epoch:13 step:12337 [D loss: 0.694494, acc: 57.81%] [G loss: 2.086489]\n",
      "epoch:13 step:12338 [D loss: 0.627180, acc: 64.06%] [G loss: 1.984736]\n",
      "epoch:13 step:12339 [D loss: 0.658896, acc: 60.94%] [G loss: 1.834795]\n",
      "epoch:13 step:12340 [D loss: 0.635978, acc: 65.62%] [G loss: 1.901290]\n",
      "epoch:13 step:12341 [D loss: 0.718871, acc: 53.91%] [G loss: 1.801134]\n",
      "epoch:13 step:12342 [D loss: 0.659593, acc: 61.72%] [G loss: 1.874701]\n",
      "epoch:13 step:12343 [D loss: 0.606974, acc: 73.44%] [G loss: 1.869802]\n",
      "epoch:13 step:12344 [D loss: 0.658390, acc: 61.72%] [G loss: 1.851959]\n",
      "epoch:13 step:12345 [D loss: 0.627316, acc: 65.62%] [G loss: 1.808025]\n",
      "epoch:13 step:12346 [D loss: 0.604998, acc: 65.62%] [G loss: 2.106412]\n",
      "epoch:13 step:12347 [D loss: 0.633958, acc: 63.28%] [G loss: 1.908766]\n",
      "epoch:13 step:12348 [D loss: 0.584449, acc: 72.66%] [G loss: 2.092704]\n",
      "epoch:13 step:12349 [D loss: 0.628674, acc: 61.72%] [G loss: 2.152941]\n",
      "epoch:13 step:12350 [D loss: 0.587865, acc: 69.53%] [G loss: 1.993210]\n",
      "epoch:13 step:12351 [D loss: 0.641075, acc: 63.28%] [G loss: 1.922909]\n",
      "epoch:13 step:12352 [D loss: 0.624767, acc: 64.84%] [G loss: 2.090147]\n",
      "epoch:13 step:12353 [D loss: 0.642091, acc: 63.28%] [G loss: 1.896750]\n",
      "epoch:13 step:12354 [D loss: 0.642902, acc: 64.84%] [G loss: 1.910533]\n",
      "epoch:13 step:12355 [D loss: 0.617699, acc: 69.53%] [G loss: 1.981570]\n",
      "epoch:13 step:12356 [D loss: 0.616405, acc: 65.62%] [G loss: 2.029103]\n",
      "epoch:13 step:12357 [D loss: 0.647453, acc: 64.84%] [G loss: 1.923643]\n",
      "epoch:13 step:12358 [D loss: 0.748878, acc: 53.12%] [G loss: 1.836121]\n",
      "epoch:13 step:12359 [D loss: 0.675346, acc: 63.28%] [G loss: 1.980654]\n",
      "epoch:13 step:12360 [D loss: 0.692093, acc: 57.03%] [G loss: 1.831387]\n",
      "epoch:13 step:12361 [D loss: 0.662587, acc: 60.94%] [G loss: 1.800416]\n",
      "epoch:13 step:12362 [D loss: 0.691842, acc: 57.81%] [G loss: 1.907881]\n",
      "epoch:13 step:12363 [D loss: 0.633928, acc: 61.72%] [G loss: 2.064747]\n",
      "epoch:13 step:12364 [D loss: 0.685068, acc: 57.81%] [G loss: 1.983129]\n",
      "epoch:13 step:12365 [D loss: 0.638105, acc: 66.41%] [G loss: 1.997699]\n",
      "epoch:13 step:12366 [D loss: 0.648005, acc: 59.38%] [G loss: 2.048425]\n",
      "epoch:13 step:12367 [D loss: 0.669670, acc: 57.03%] [G loss: 2.073367]\n",
      "epoch:13 step:12368 [D loss: 0.650422, acc: 63.28%] [G loss: 2.046657]\n",
      "epoch:13 step:12369 [D loss: 0.670999, acc: 50.00%] [G loss: 1.938163]\n",
      "epoch:13 step:12370 [D loss: 0.715386, acc: 58.59%] [G loss: 1.857761]\n",
      "epoch:13 step:12371 [D loss: 0.611036, acc: 67.97%] [G loss: 1.879751]\n",
      "epoch:13 step:12372 [D loss: 0.623184, acc: 67.19%] [G loss: 1.956693]\n",
      "epoch:13 step:12373 [D loss: 0.638802, acc: 63.28%] [G loss: 2.043964]\n",
      "epoch:13 step:12374 [D loss: 0.624241, acc: 64.06%] [G loss: 1.908035]\n",
      "epoch:13 step:12375 [D loss: 0.606562, acc: 71.88%] [G loss: 2.129225]\n",
      "epoch:13 step:12376 [D loss: 0.653890, acc: 60.16%] [G loss: 2.022775]\n",
      "epoch:13 step:12377 [D loss: 0.618658, acc: 60.16%] [G loss: 1.908733]\n",
      "epoch:13 step:12378 [D loss: 0.617085, acc: 64.06%] [G loss: 2.036935]\n",
      "epoch:13 step:12379 [D loss: 0.571484, acc: 69.53%] [G loss: 1.945630]\n",
      "epoch:13 step:12380 [D loss: 0.624707, acc: 64.06%] [G loss: 2.117151]\n",
      "epoch:13 step:12381 [D loss: 0.691291, acc: 58.59%] [G loss: 1.865090]\n",
      "epoch:13 step:12382 [D loss: 0.634588, acc: 71.09%] [G loss: 1.979603]\n",
      "epoch:13 step:12383 [D loss: 0.594261, acc: 66.41%] [G loss: 2.135041]\n",
      "epoch:13 step:12384 [D loss: 0.634412, acc: 61.72%] [G loss: 2.055474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12385 [D loss: 0.647719, acc: 63.28%] [G loss: 1.876683]\n",
      "epoch:13 step:12386 [D loss: 0.625103, acc: 64.06%] [G loss: 1.873324]\n",
      "epoch:13 step:12387 [D loss: 0.641937, acc: 64.06%] [G loss: 2.121213]\n",
      "epoch:13 step:12388 [D loss: 0.560939, acc: 71.88%] [G loss: 2.271191]\n",
      "epoch:13 step:12389 [D loss: 0.577655, acc: 69.53%] [G loss: 2.362291]\n",
      "epoch:13 step:12390 [D loss: 0.592228, acc: 67.97%] [G loss: 2.205169]\n",
      "epoch:13 step:12391 [D loss: 0.594106, acc: 69.53%] [G loss: 2.069874]\n",
      "epoch:13 step:12392 [D loss: 0.738626, acc: 49.22%] [G loss: 1.892823]\n",
      "epoch:13 step:12393 [D loss: 0.626061, acc: 68.75%] [G loss: 1.908238]\n",
      "epoch:13 step:12394 [D loss: 0.619208, acc: 69.53%] [G loss: 1.954398]\n",
      "epoch:13 step:12395 [D loss: 0.707532, acc: 55.47%] [G loss: 1.837036]\n",
      "epoch:13 step:12396 [D loss: 0.726733, acc: 51.56%] [G loss: 1.833438]\n",
      "epoch:13 step:12397 [D loss: 0.625168, acc: 67.19%] [G loss: 2.099853]\n",
      "epoch:13 step:12398 [D loss: 0.580898, acc: 69.53%] [G loss: 2.192402]\n",
      "epoch:13 step:12399 [D loss: 0.570039, acc: 67.19%] [G loss: 2.228684]\n",
      "epoch:13 step:12400 [D loss: 0.536383, acc: 75.00%] [G loss: 2.489714]\n",
      "##############\n",
      "[2.4788474  1.33493794 6.17628462 4.83862309 3.88534947 5.82053029\n",
      " 4.50140439 4.74623622 4.53640578 3.56600799]\n",
      "##########\n",
      "epoch:13 step:12401 [D loss: 0.745134, acc: 50.78%] [G loss: 1.718238]\n",
      "epoch:13 step:12402 [D loss: 0.644743, acc: 58.59%] [G loss: 2.117893]\n",
      "epoch:13 step:12403 [D loss: 0.641651, acc: 60.94%] [G loss: 2.011166]\n",
      "epoch:13 step:12404 [D loss: 0.607969, acc: 69.53%] [G loss: 1.970531]\n",
      "epoch:13 step:12405 [D loss: 0.635788, acc: 64.06%] [G loss: 2.070015]\n",
      "epoch:13 step:12406 [D loss: 0.662289, acc: 60.16%] [G loss: 1.974170]\n",
      "epoch:13 step:12407 [D loss: 0.617634, acc: 62.50%] [G loss: 1.971558]\n",
      "epoch:13 step:12408 [D loss: 0.640908, acc: 62.50%] [G loss: 1.952543]\n",
      "epoch:13 step:12409 [D loss: 0.642365, acc: 64.84%] [G loss: 1.912869]\n",
      "epoch:13 step:12410 [D loss: 0.653468, acc: 64.06%] [G loss: 2.202485]\n",
      "epoch:13 step:12411 [D loss: 0.598036, acc: 64.06%] [G loss: 2.173517]\n",
      "epoch:13 step:12412 [D loss: 0.590511, acc: 67.19%] [G loss: 2.306516]\n",
      "epoch:13 step:12413 [D loss: 0.520056, acc: 77.34%] [G loss: 2.446412]\n",
      "epoch:13 step:12414 [D loss: 0.620378, acc: 67.19%] [G loss: 1.979175]\n",
      "epoch:13 step:12415 [D loss: 0.620119, acc: 69.53%] [G loss: 1.940388]\n",
      "epoch:13 step:12416 [D loss: 0.664646, acc: 64.06%] [G loss: 2.067039]\n",
      "epoch:13 step:12417 [D loss: 0.649252, acc: 54.69%] [G loss: 1.901169]\n",
      "epoch:13 step:12418 [D loss: 0.618819, acc: 63.28%] [G loss: 2.060133]\n",
      "epoch:13 step:12419 [D loss: 0.622246, acc: 62.50%] [G loss: 2.064661]\n",
      "epoch:13 step:12420 [D loss: 0.620682, acc: 63.28%] [G loss: 2.112124]\n",
      "epoch:13 step:12421 [D loss: 0.603844, acc: 69.53%] [G loss: 2.028626]\n",
      "epoch:13 step:12422 [D loss: 0.670398, acc: 57.81%] [G loss: 1.916988]\n",
      "epoch:13 step:12423 [D loss: 0.588949, acc: 67.19%] [G loss: 2.198280]\n",
      "epoch:13 step:12424 [D loss: 0.647566, acc: 61.72%] [G loss: 2.075546]\n",
      "epoch:13 step:12425 [D loss: 0.628118, acc: 66.41%] [G loss: 2.107011]\n",
      "epoch:13 step:12426 [D loss: 0.627009, acc: 62.50%] [G loss: 2.070324]\n",
      "epoch:13 step:12427 [D loss: 0.650924, acc: 65.62%] [G loss: 2.028426]\n",
      "epoch:13 step:12428 [D loss: 0.634234, acc: 64.06%] [G loss: 2.019790]\n",
      "epoch:13 step:12429 [D loss: 0.633902, acc: 64.06%] [G loss: 2.099167]\n",
      "epoch:13 step:12430 [D loss: 0.683550, acc: 58.59%] [G loss: 2.005019]\n",
      "epoch:13 step:12431 [D loss: 0.627012, acc: 64.06%] [G loss: 1.896760]\n",
      "epoch:13 step:12432 [D loss: 0.721789, acc: 57.03%] [G loss: 1.897879]\n",
      "epoch:13 step:12433 [D loss: 0.653796, acc: 60.94%] [G loss: 1.784012]\n",
      "epoch:13 step:12434 [D loss: 0.555564, acc: 69.53%] [G loss: 1.945884]\n",
      "epoch:13 step:12435 [D loss: 0.619799, acc: 61.72%] [G loss: 1.889030]\n",
      "epoch:13 step:12436 [D loss: 0.656229, acc: 59.38%] [G loss: 1.831463]\n",
      "epoch:13 step:12437 [D loss: 0.647496, acc: 62.50%] [G loss: 1.934212]\n",
      "epoch:13 step:12438 [D loss: 0.699514, acc: 56.25%] [G loss: 1.911578]\n",
      "epoch:13 step:12439 [D loss: 0.661003, acc: 57.81%] [G loss: 1.867010]\n",
      "epoch:13 step:12440 [D loss: 0.607164, acc: 67.19%] [G loss: 2.051578]\n",
      "epoch:13 step:12441 [D loss: 0.664465, acc: 54.69%] [G loss: 1.931955]\n",
      "epoch:13 step:12442 [D loss: 0.616140, acc: 67.19%] [G loss: 1.957871]\n",
      "epoch:13 step:12443 [D loss: 0.629617, acc: 70.31%] [G loss: 2.135192]\n",
      "epoch:13 step:12444 [D loss: 0.667597, acc: 59.38%] [G loss: 1.968438]\n",
      "epoch:13 step:12445 [D loss: 0.592269, acc: 69.53%] [G loss: 2.043519]\n",
      "epoch:13 step:12446 [D loss: 0.630441, acc: 64.06%] [G loss: 1.946511]\n",
      "epoch:13 step:12447 [D loss: 0.636061, acc: 64.06%] [G loss: 1.939560]\n",
      "epoch:13 step:12448 [D loss: 0.663146, acc: 59.38%] [G loss: 1.903023]\n",
      "epoch:13 step:12449 [D loss: 0.651961, acc: 59.38%] [G loss: 2.008224]\n",
      "epoch:13 step:12450 [D loss: 0.631325, acc: 63.28%] [G loss: 2.162957]\n",
      "epoch:13 step:12451 [D loss: 0.629411, acc: 63.28%] [G loss: 2.106357]\n",
      "epoch:13 step:12452 [D loss: 0.589787, acc: 69.53%] [G loss: 2.167918]\n",
      "epoch:13 step:12453 [D loss: 0.585460, acc: 67.19%] [G loss: 2.090236]\n",
      "epoch:13 step:12454 [D loss: 0.531334, acc: 72.66%] [G loss: 2.103878]\n",
      "epoch:13 step:12455 [D loss: 0.548334, acc: 76.56%] [G loss: 2.175169]\n",
      "epoch:13 step:12456 [D loss: 0.611452, acc: 61.72%] [G loss: 1.918047]\n",
      "epoch:13 step:12457 [D loss: 0.666267, acc: 65.62%] [G loss: 2.182357]\n",
      "epoch:13 step:12458 [D loss: 0.659716, acc: 66.41%] [G loss: 1.986190]\n",
      "epoch:13 step:12459 [D loss: 0.656631, acc: 62.50%] [G loss: 2.096277]\n",
      "epoch:13 step:12460 [D loss: 0.635647, acc: 67.97%] [G loss: 2.001356]\n",
      "epoch:13 step:12461 [D loss: 0.650447, acc: 64.06%] [G loss: 2.064893]\n",
      "epoch:13 step:12462 [D loss: 0.688782, acc: 57.81%] [G loss: 1.924424]\n",
      "epoch:13 step:12463 [D loss: 0.688390, acc: 58.59%] [G loss: 1.968123]\n",
      "epoch:13 step:12464 [D loss: 0.619738, acc: 62.50%] [G loss: 1.975140]\n",
      "epoch:13 step:12465 [D loss: 0.655102, acc: 61.72%] [G loss: 1.959524]\n",
      "epoch:13 step:12466 [D loss: 0.617174, acc: 64.84%] [G loss: 2.028501]\n",
      "epoch:13 step:12467 [D loss: 0.602963, acc: 68.75%] [G loss: 2.216304]\n",
      "epoch:13 step:12468 [D loss: 0.691363, acc: 57.03%] [G loss: 1.966946]\n",
      "epoch:13 step:12469 [D loss: 0.654380, acc: 60.16%] [G loss: 1.807767]\n",
      "epoch:13 step:12470 [D loss: 0.672806, acc: 59.38%] [G loss: 1.955284]\n",
      "epoch:13 step:12471 [D loss: 0.632915, acc: 65.62%] [G loss: 1.932078]\n",
      "epoch:13 step:12472 [D loss: 0.641463, acc: 64.06%] [G loss: 2.053207]\n",
      "epoch:13 step:12473 [D loss: 0.627013, acc: 65.62%] [G loss: 1.922158]\n",
      "epoch:13 step:12474 [D loss: 0.625449, acc: 62.50%] [G loss: 2.128226]\n",
      "epoch:13 step:12475 [D loss: 0.646537, acc: 60.16%] [G loss: 1.921214]\n",
      "epoch:13 step:12476 [D loss: 0.600825, acc: 67.19%] [G loss: 2.046314]\n",
      "epoch:13 step:12477 [D loss: 0.627851, acc: 66.41%] [G loss: 2.002544]\n",
      "epoch:13 step:12478 [D loss: 0.617684, acc: 66.41%] [G loss: 1.943949]\n",
      "epoch:13 step:12479 [D loss: 0.605408, acc: 69.53%] [G loss: 2.220056]\n",
      "epoch:13 step:12480 [D loss: 0.598874, acc: 67.19%] [G loss: 2.174257]\n",
      "epoch:13 step:12481 [D loss: 0.656087, acc: 62.50%] [G loss: 2.122811]\n",
      "epoch:13 step:12482 [D loss: 0.700663, acc: 60.94%] [G loss: 1.831811]\n",
      "epoch:13 step:12483 [D loss: 0.607886, acc: 63.28%] [G loss: 2.080841]\n",
      "epoch:13 step:12484 [D loss: 0.670287, acc: 57.03%] [G loss: 1.965017]\n",
      "epoch:13 step:12485 [D loss: 0.631940, acc: 61.72%] [G loss: 1.978565]\n",
      "epoch:13 step:12486 [D loss: 0.683899, acc: 60.94%] [G loss: 1.859490]\n",
      "epoch:13 step:12487 [D loss: 0.625673, acc: 64.06%] [G loss: 1.855015]\n",
      "epoch:13 step:12488 [D loss: 0.648785, acc: 60.94%] [G loss: 1.953095]\n",
      "epoch:13 step:12489 [D loss: 0.650121, acc: 59.38%] [G loss: 1.861840]\n",
      "epoch:13 step:12490 [D loss: 0.591845, acc: 66.41%] [G loss: 1.974362]\n",
      "epoch:13 step:12491 [D loss: 0.674859, acc: 54.69%] [G loss: 2.080975]\n",
      "epoch:13 step:12492 [D loss: 0.613917, acc: 69.53%] [G loss: 1.892972]\n",
      "epoch:13 step:12493 [D loss: 0.551180, acc: 77.34%] [G loss: 2.440570]\n",
      "epoch:13 step:12494 [D loss: 0.579812, acc: 67.97%] [G loss: 2.417217]\n",
      "epoch:13 step:12495 [D loss: 0.619015, acc: 63.28%] [G loss: 2.275105]\n",
      "epoch:13 step:12496 [D loss: 0.535047, acc: 75.78%] [G loss: 2.316859]\n",
      "epoch:13 step:12497 [D loss: 0.680490, acc: 55.47%] [G loss: 1.795371]\n",
      "epoch:13 step:12498 [D loss: 0.654711, acc: 59.38%] [G loss: 1.939952]\n",
      "epoch:13 step:12499 [D loss: 0.647510, acc: 57.81%] [G loss: 2.129369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12500 [D loss: 0.695521, acc: 57.81%] [G loss: 2.001691]\n",
      "epoch:13 step:12501 [D loss: 0.584723, acc: 71.09%] [G loss: 1.909784]\n",
      "epoch:13 step:12502 [D loss: 0.659166, acc: 64.06%] [G loss: 2.092938]\n",
      "epoch:13 step:12503 [D loss: 0.640843, acc: 60.94%] [G loss: 2.119828]\n",
      "epoch:13 step:12504 [D loss: 0.638133, acc: 58.59%] [G loss: 1.727599]\n",
      "epoch:13 step:12505 [D loss: 0.658468, acc: 57.03%] [G loss: 1.789015]\n",
      "epoch:13 step:12506 [D loss: 0.655896, acc: 54.69%] [G loss: 1.959719]\n",
      "epoch:13 step:12507 [D loss: 0.621832, acc: 67.19%] [G loss: 1.960824]\n",
      "epoch:13 step:12508 [D loss: 0.640461, acc: 64.06%] [G loss: 2.014663]\n",
      "epoch:13 step:12509 [D loss: 0.669830, acc: 63.28%] [G loss: 1.872688]\n",
      "epoch:13 step:12510 [D loss: 0.641863, acc: 59.38%] [G loss: 2.058950]\n",
      "epoch:13 step:12511 [D loss: 0.599734, acc: 69.53%] [G loss: 2.111826]\n",
      "epoch:13 step:12512 [D loss: 0.594412, acc: 71.88%] [G loss: 1.922098]\n",
      "epoch:13 step:12513 [D loss: 0.627436, acc: 63.28%] [G loss: 2.043757]\n",
      "epoch:13 step:12514 [D loss: 0.648695, acc: 62.50%] [G loss: 1.927548]\n",
      "epoch:13 step:12515 [D loss: 0.676729, acc: 59.38%] [G loss: 2.043423]\n",
      "epoch:13 step:12516 [D loss: 0.668964, acc: 60.94%] [G loss: 2.006077]\n",
      "epoch:13 step:12517 [D loss: 0.652205, acc: 65.62%] [G loss: 2.043378]\n",
      "epoch:13 step:12518 [D loss: 0.551097, acc: 71.88%] [G loss: 2.165713]\n",
      "epoch:13 step:12519 [D loss: 0.658046, acc: 60.94%] [G loss: 2.167562]\n",
      "epoch:13 step:12520 [D loss: 0.624122, acc: 65.62%] [G loss: 2.005617]\n",
      "epoch:13 step:12521 [D loss: 0.647030, acc: 64.84%] [G loss: 1.926259]\n",
      "epoch:13 step:12522 [D loss: 0.667673, acc: 57.81%] [G loss: 1.843109]\n",
      "epoch:13 step:12523 [D loss: 0.677335, acc: 58.59%] [G loss: 1.986742]\n",
      "epoch:13 step:12524 [D loss: 0.649265, acc: 60.94%] [G loss: 2.092297]\n",
      "epoch:13 step:12525 [D loss: 0.650418, acc: 64.84%] [G loss: 2.026895]\n",
      "epoch:13 step:12526 [D loss: 0.601144, acc: 63.28%] [G loss: 2.386487]\n",
      "epoch:13 step:12527 [D loss: 0.582997, acc: 71.88%] [G loss: 2.248411]\n",
      "epoch:13 step:12528 [D loss: 0.586676, acc: 67.97%] [G loss: 2.394904]\n",
      "epoch:13 step:12529 [D loss: 0.673344, acc: 57.81%] [G loss: 1.877882]\n",
      "epoch:13 step:12530 [D loss: 0.723058, acc: 53.12%] [G loss: 1.881286]\n",
      "epoch:13 step:12531 [D loss: 0.608917, acc: 64.06%] [G loss: 2.122530]\n",
      "epoch:13 step:12532 [D loss: 0.620355, acc: 60.94%] [G loss: 1.855523]\n",
      "epoch:13 step:12533 [D loss: 0.672017, acc: 59.38%] [G loss: 1.867245]\n",
      "epoch:13 step:12534 [D loss: 0.658031, acc: 64.06%] [G loss: 1.929835]\n",
      "epoch:13 step:12535 [D loss: 0.675890, acc: 59.38%] [G loss: 2.098958]\n",
      "epoch:13 step:12536 [D loss: 0.700574, acc: 58.59%] [G loss: 2.029260]\n",
      "epoch:13 step:12537 [D loss: 0.685493, acc: 59.38%] [G loss: 1.904470]\n",
      "epoch:13 step:12538 [D loss: 0.583780, acc: 70.31%] [G loss: 2.064504]\n",
      "epoch:13 step:12539 [D loss: 0.577638, acc: 71.88%] [G loss: 1.930527]\n",
      "epoch:13 step:12540 [D loss: 0.598264, acc: 67.19%] [G loss: 2.087153]\n",
      "epoch:13 step:12541 [D loss: 0.605349, acc: 66.41%] [G loss: 1.943985]\n",
      "epoch:13 step:12542 [D loss: 0.645658, acc: 63.28%] [G loss: 1.984875]\n",
      "epoch:13 step:12543 [D loss: 0.686113, acc: 57.03%] [G loss: 1.862387]\n",
      "epoch:13 step:12544 [D loss: 0.593731, acc: 66.41%] [G loss: 2.025743]\n",
      "epoch:13 step:12545 [D loss: 0.670065, acc: 58.59%] [G loss: 1.960493]\n",
      "epoch:13 step:12546 [D loss: 0.654269, acc: 63.28%] [G loss: 1.906288]\n",
      "epoch:13 step:12547 [D loss: 0.619718, acc: 69.53%] [G loss: 2.040449]\n",
      "epoch:13 step:12548 [D loss: 0.656380, acc: 60.94%] [G loss: 1.855307]\n",
      "epoch:13 step:12549 [D loss: 0.585961, acc: 71.88%] [G loss: 2.044633]\n",
      "epoch:13 step:12550 [D loss: 0.633348, acc: 64.84%] [G loss: 2.040449]\n",
      "epoch:13 step:12551 [D loss: 0.584276, acc: 69.53%] [G loss: 2.124623]\n",
      "epoch:13 step:12552 [D loss: 0.610312, acc: 67.19%] [G loss: 2.178431]\n",
      "epoch:13 step:12553 [D loss: 0.690698, acc: 60.16%] [G loss: 1.965152]\n",
      "epoch:13 step:12554 [D loss: 0.688022, acc: 60.16%] [G loss: 1.838480]\n",
      "epoch:13 step:12555 [D loss: 0.603536, acc: 65.62%] [G loss: 2.100669]\n",
      "epoch:13 step:12556 [D loss: 0.644587, acc: 62.50%] [G loss: 1.930672]\n",
      "epoch:13 step:12557 [D loss: 0.682246, acc: 61.72%] [G loss: 1.834237]\n",
      "epoch:13 step:12558 [D loss: 0.696796, acc: 57.81%] [G loss: 1.888197]\n",
      "epoch:13 step:12559 [D loss: 0.667693, acc: 59.38%] [G loss: 1.930745]\n",
      "epoch:13 step:12560 [D loss: 0.604374, acc: 69.53%] [G loss: 1.994787]\n",
      "epoch:13 step:12561 [D loss: 0.631314, acc: 61.72%] [G loss: 1.854113]\n",
      "epoch:13 step:12562 [D loss: 0.610446, acc: 67.19%] [G loss: 2.027439]\n",
      "epoch:13 step:12563 [D loss: 0.632046, acc: 64.06%] [G loss: 2.024605]\n",
      "epoch:13 step:12564 [D loss: 0.675514, acc: 59.38%] [G loss: 2.072574]\n",
      "epoch:13 step:12565 [D loss: 0.602507, acc: 67.19%] [G loss: 2.077690]\n",
      "epoch:13 step:12566 [D loss: 0.575153, acc: 68.75%] [G loss: 2.173727]\n",
      "epoch:13 step:12567 [D loss: 0.696093, acc: 53.91%] [G loss: 1.893815]\n",
      "epoch:13 step:12568 [D loss: 0.668403, acc: 56.25%] [G loss: 1.945809]\n",
      "epoch:13 step:12569 [D loss: 0.578025, acc: 67.97%] [G loss: 2.059263]\n",
      "epoch:13 step:12570 [D loss: 0.625621, acc: 64.06%] [G loss: 1.897737]\n",
      "epoch:13 step:12571 [D loss: 0.629823, acc: 65.62%] [G loss: 1.919969]\n",
      "epoch:13 step:12572 [D loss: 0.635818, acc: 60.94%] [G loss: 1.971023]\n",
      "epoch:13 step:12573 [D loss: 0.630270, acc: 66.41%] [G loss: 2.029541]\n",
      "epoch:13 step:12574 [D loss: 0.642300, acc: 60.16%] [G loss: 1.923549]\n",
      "epoch:13 step:12575 [D loss: 0.662137, acc: 60.16%] [G loss: 1.952343]\n",
      "epoch:13 step:12576 [D loss: 0.570417, acc: 67.97%] [G loss: 2.032613]\n",
      "epoch:13 step:12577 [D loss: 0.648196, acc: 64.84%] [G loss: 2.016829]\n",
      "epoch:13 step:12578 [D loss: 0.644601, acc: 67.19%] [G loss: 1.967218]\n",
      "epoch:13 step:12579 [D loss: 0.621521, acc: 68.75%] [G loss: 2.145764]\n",
      "epoch:13 step:12580 [D loss: 0.600693, acc: 70.31%] [G loss: 1.906618]\n",
      "epoch:13 step:12581 [D loss: 0.682338, acc: 54.69%] [G loss: 1.987944]\n",
      "epoch:13 step:12582 [D loss: 0.689422, acc: 57.81%] [G loss: 1.842428]\n",
      "epoch:13 step:12583 [D loss: 0.636750, acc: 65.62%] [G loss: 2.068576]\n",
      "epoch:13 step:12584 [D loss: 0.584207, acc: 71.88%] [G loss: 2.040181]\n",
      "epoch:13 step:12585 [D loss: 0.623944, acc: 67.19%] [G loss: 2.089634]\n",
      "epoch:13 step:12586 [D loss: 0.630505, acc: 65.62%] [G loss: 2.220315]\n",
      "epoch:13 step:12587 [D loss: 0.612981, acc: 68.75%] [G loss: 2.253300]\n",
      "epoch:13 step:12588 [D loss: 0.641593, acc: 67.19%] [G loss: 2.028499]\n",
      "epoch:13 step:12589 [D loss: 0.688175, acc: 57.03%] [G loss: 1.958674]\n",
      "epoch:13 step:12590 [D loss: 0.684133, acc: 57.81%] [G loss: 2.135246]\n",
      "epoch:13 step:12591 [D loss: 0.693721, acc: 58.59%] [G loss: 1.791282]\n",
      "epoch:13 step:12592 [D loss: 0.625570, acc: 65.62%] [G loss: 2.009762]\n",
      "epoch:13 step:12593 [D loss: 0.650380, acc: 60.94%] [G loss: 2.004535]\n",
      "epoch:13 step:12594 [D loss: 0.645283, acc: 65.62%] [G loss: 2.042796]\n",
      "epoch:13 step:12595 [D loss: 0.655072, acc: 61.72%] [G loss: 1.906873]\n",
      "epoch:13 step:12596 [D loss: 0.595151, acc: 67.97%] [G loss: 1.950232]\n",
      "epoch:13 step:12597 [D loss: 0.637466, acc: 64.84%] [G loss: 2.030776]\n",
      "epoch:13 step:12598 [D loss: 0.629611, acc: 64.06%] [G loss: 2.047166]\n",
      "epoch:13 step:12599 [D loss: 0.713401, acc: 51.56%] [G loss: 2.005100]\n",
      "epoch:13 step:12600 [D loss: 0.697193, acc: 56.25%] [G loss: 1.885540]\n",
      "##############\n",
      "[2.39084008 1.41770956 6.19335405 4.7249728  3.60442564 5.63118371\n",
      " 4.25003853 4.64620932 4.60426451 3.47038039]\n",
      "##########\n",
      "epoch:13 step:12601 [D loss: 0.641991, acc: 63.28%] [G loss: 1.954032]\n",
      "epoch:13 step:12602 [D loss: 0.642132, acc: 64.06%] [G loss: 1.941030]\n",
      "epoch:13 step:12603 [D loss: 0.706423, acc: 56.25%] [G loss: 1.828119]\n",
      "epoch:13 step:12604 [D loss: 0.675269, acc: 58.59%] [G loss: 1.951806]\n",
      "epoch:13 step:12605 [D loss: 0.617828, acc: 60.94%] [G loss: 2.001518]\n",
      "epoch:13 step:12606 [D loss: 0.658380, acc: 59.38%] [G loss: 2.002577]\n",
      "epoch:13 step:12607 [D loss: 0.582624, acc: 71.09%] [G loss: 1.991494]\n",
      "epoch:13 step:12608 [D loss: 0.566753, acc: 73.44%] [G loss: 2.125121]\n",
      "epoch:13 step:12609 [D loss: 0.564630, acc: 75.78%] [G loss: 2.239774]\n",
      "epoch:13 step:12610 [D loss: 0.618570, acc: 71.09%] [G loss: 2.094806]\n",
      "epoch:13 step:12611 [D loss: 0.577359, acc: 67.97%] [G loss: 2.222997]\n",
      "epoch:13 step:12612 [D loss: 0.639851, acc: 66.41%] [G loss: 2.051903]\n",
      "epoch:13 step:12613 [D loss: 0.707056, acc: 59.38%] [G loss: 1.841249]\n",
      "epoch:13 step:12614 [D loss: 0.672369, acc: 58.59%] [G loss: 1.794771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12615 [D loss: 0.598777, acc: 67.19%] [G loss: 2.127179]\n",
      "epoch:13 step:12616 [D loss: 0.729098, acc: 56.25%] [G loss: 1.951717]\n",
      "epoch:13 step:12617 [D loss: 0.654321, acc: 57.81%] [G loss: 2.038812]\n",
      "epoch:13 step:12618 [D loss: 0.739806, acc: 50.00%] [G loss: 1.822177]\n",
      "epoch:13 step:12619 [D loss: 0.667392, acc: 63.28%] [G loss: 1.763013]\n",
      "epoch:13 step:12620 [D loss: 0.658221, acc: 60.94%] [G loss: 1.924165]\n",
      "epoch:13 step:12621 [D loss: 0.597424, acc: 63.28%] [G loss: 1.774108]\n",
      "epoch:13 step:12622 [D loss: 0.663076, acc: 57.81%] [G loss: 1.774177]\n",
      "epoch:13 step:12623 [D loss: 0.643011, acc: 61.72%] [G loss: 1.909404]\n",
      "epoch:13 step:12624 [D loss: 0.618925, acc: 65.62%] [G loss: 1.817345]\n",
      "epoch:13 step:12625 [D loss: 0.630867, acc: 61.72%] [G loss: 1.964180]\n",
      "epoch:13 step:12626 [D loss: 0.668521, acc: 60.16%] [G loss: 2.073298]\n",
      "epoch:13 step:12627 [D loss: 0.653350, acc: 62.50%] [G loss: 1.692224]\n",
      "epoch:13 step:12628 [D loss: 0.627520, acc: 64.84%] [G loss: 1.947865]\n",
      "epoch:13 step:12629 [D loss: 0.710662, acc: 56.25%] [G loss: 1.757624]\n",
      "epoch:13 step:12630 [D loss: 0.647238, acc: 64.84%] [G loss: 1.854897]\n",
      "epoch:13 step:12631 [D loss: 0.604515, acc: 68.75%] [G loss: 1.993834]\n",
      "epoch:13 step:12632 [D loss: 0.662176, acc: 60.16%] [G loss: 1.965621]\n",
      "epoch:13 step:12633 [D loss: 0.645935, acc: 60.94%] [G loss: 1.942032]\n",
      "epoch:13 step:12634 [D loss: 0.624853, acc: 64.84%] [G loss: 1.877338]\n",
      "epoch:13 step:12635 [D loss: 0.679157, acc: 58.59%] [G loss: 1.831796]\n",
      "epoch:13 step:12636 [D loss: 0.640092, acc: 65.62%] [G loss: 1.993289]\n",
      "epoch:13 step:12637 [D loss: 0.646361, acc: 64.06%] [G loss: 1.926356]\n",
      "epoch:13 step:12638 [D loss: 0.617939, acc: 65.62%] [G loss: 2.001082]\n",
      "epoch:13 step:12639 [D loss: 0.674047, acc: 56.25%] [G loss: 1.787214]\n",
      "epoch:13 step:12640 [D loss: 0.663909, acc: 64.06%] [G loss: 1.837120]\n",
      "epoch:13 step:12641 [D loss: 0.631747, acc: 60.16%] [G loss: 1.839181]\n",
      "epoch:13 step:12642 [D loss: 0.643079, acc: 64.84%] [G loss: 1.978453]\n",
      "epoch:13 step:12643 [D loss: 0.692100, acc: 58.59%] [G loss: 1.979274]\n",
      "epoch:13 step:12644 [D loss: 0.637247, acc: 65.62%] [G loss: 1.971788]\n",
      "epoch:13 step:12645 [D loss: 0.627643, acc: 65.62%] [G loss: 1.957517]\n",
      "epoch:13 step:12646 [D loss: 0.642542, acc: 63.28%] [G loss: 1.820302]\n",
      "epoch:13 step:12647 [D loss: 0.662973, acc: 57.81%] [G loss: 2.017481]\n",
      "epoch:13 step:12648 [D loss: 0.604870, acc: 69.53%] [G loss: 2.009537]\n",
      "epoch:13 step:12649 [D loss: 0.634630, acc: 65.62%] [G loss: 1.975009]\n",
      "epoch:13 step:12650 [D loss: 0.624981, acc: 66.41%] [G loss: 2.215932]\n",
      "epoch:13 step:12651 [D loss: 0.634941, acc: 67.97%] [G loss: 2.087032]\n",
      "epoch:13 step:12652 [D loss: 0.619947, acc: 63.28%] [G loss: 2.320666]\n",
      "epoch:13 step:12653 [D loss: 0.587928, acc: 69.53%] [G loss: 2.136791]\n",
      "epoch:13 step:12654 [D loss: 0.669455, acc: 60.94%] [G loss: 1.969793]\n",
      "epoch:13 step:12655 [D loss: 0.666909, acc: 59.38%] [G loss: 1.900343]\n",
      "epoch:13 step:12656 [D loss: 0.644315, acc: 63.28%] [G loss: 1.995240]\n",
      "epoch:13 step:12657 [D loss: 0.627712, acc: 62.50%] [G loss: 2.142504]\n",
      "epoch:13 step:12658 [D loss: 0.653711, acc: 65.62%] [G loss: 1.887914]\n",
      "epoch:13 step:12659 [D loss: 0.690864, acc: 60.94%] [G loss: 1.735682]\n",
      "epoch:13 step:12660 [D loss: 0.610926, acc: 66.41%] [G loss: 1.960429]\n",
      "epoch:13 step:12661 [D loss: 0.636231, acc: 64.06%] [G loss: 2.017516]\n",
      "epoch:13 step:12662 [D loss: 0.583207, acc: 71.09%] [G loss: 2.149264]\n",
      "epoch:13 step:12663 [D loss: 0.678435, acc: 63.28%] [G loss: 1.761515]\n",
      "epoch:13 step:12664 [D loss: 0.643839, acc: 58.59%] [G loss: 1.771846]\n",
      "epoch:13 step:12665 [D loss: 0.621782, acc: 64.06%] [G loss: 2.114004]\n",
      "epoch:13 step:12666 [D loss: 0.691257, acc: 56.25%] [G loss: 1.889111]\n",
      "epoch:13 step:12667 [D loss: 0.663885, acc: 60.16%] [G loss: 1.928967]\n",
      "epoch:13 step:12668 [D loss: 0.646611, acc: 62.50%] [G loss: 1.945391]\n",
      "epoch:13 step:12669 [D loss: 0.667880, acc: 60.94%] [G loss: 1.937548]\n",
      "epoch:13 step:12670 [D loss: 0.697695, acc: 60.94%] [G loss: 1.765931]\n",
      "epoch:13 step:12671 [D loss: 0.633617, acc: 65.62%] [G loss: 1.828476]\n",
      "epoch:13 step:12672 [D loss: 0.647857, acc: 64.84%] [G loss: 1.954907]\n",
      "epoch:13 step:12673 [D loss: 0.681830, acc: 60.94%] [G loss: 1.838933]\n",
      "epoch:13 step:12674 [D loss: 0.650967, acc: 61.72%] [G loss: 1.861927]\n",
      "epoch:13 step:12675 [D loss: 0.641836, acc: 65.62%] [G loss: 1.886745]\n",
      "epoch:13 step:12676 [D loss: 0.640282, acc: 61.72%] [G loss: 2.143130]\n",
      "epoch:13 step:12677 [D loss: 0.628481, acc: 65.62%] [G loss: 1.830525]\n",
      "epoch:13 step:12678 [D loss: 0.641856, acc: 66.41%] [G loss: 1.873381]\n",
      "epoch:13 step:12679 [D loss: 0.612440, acc: 67.19%] [G loss: 2.032931]\n",
      "epoch:13 step:12680 [D loss: 0.601884, acc: 69.53%] [G loss: 2.006220]\n",
      "epoch:13 step:12681 [D loss: 0.685171, acc: 57.03%] [G loss: 1.771598]\n",
      "epoch:13 step:12682 [D loss: 0.689382, acc: 57.03%] [G loss: 1.867499]\n",
      "epoch:13 step:12683 [D loss: 0.654602, acc: 57.03%] [G loss: 1.767130]\n",
      "epoch:13 step:12684 [D loss: 0.685348, acc: 55.47%] [G loss: 1.822391]\n",
      "epoch:13 step:12685 [D loss: 0.645413, acc: 64.06%] [G loss: 1.922061]\n",
      "epoch:13 step:12686 [D loss: 0.651075, acc: 60.94%] [G loss: 1.817360]\n",
      "epoch:13 step:12687 [D loss: 0.697765, acc: 56.25%] [G loss: 1.637869]\n",
      "epoch:13 step:12688 [D loss: 0.659099, acc: 58.59%] [G loss: 1.863018]\n",
      "epoch:13 step:12689 [D loss: 0.606588, acc: 68.75%] [G loss: 1.997405]\n",
      "epoch:13 step:12690 [D loss: 0.661522, acc: 59.38%] [G loss: 1.729443]\n",
      "epoch:13 step:12691 [D loss: 0.668165, acc: 59.38%] [G loss: 2.003371]\n",
      "epoch:13 step:12692 [D loss: 0.660477, acc: 60.94%] [G loss: 1.744361]\n",
      "epoch:13 step:12693 [D loss: 0.633127, acc: 64.84%] [G loss: 1.894802]\n",
      "epoch:13 step:12694 [D loss: 0.641608, acc: 64.06%] [G loss: 1.925555]\n",
      "epoch:13 step:12695 [D loss: 0.659288, acc: 60.94%] [G loss: 1.890442]\n",
      "epoch:13 step:12696 [D loss: 0.617488, acc: 69.53%] [G loss: 2.009327]\n",
      "epoch:13 step:12697 [D loss: 0.625493, acc: 68.75%] [G loss: 1.895569]\n",
      "epoch:13 step:12698 [D loss: 0.677497, acc: 54.69%] [G loss: 2.122600]\n",
      "epoch:13 step:12699 [D loss: 0.667721, acc: 58.59%] [G loss: 1.836258]\n",
      "epoch:13 step:12700 [D loss: 0.651919, acc: 63.28%] [G loss: 1.884573]\n",
      "epoch:13 step:12701 [D loss: 0.653937, acc: 64.06%] [G loss: 1.891943]\n",
      "epoch:13 step:12702 [D loss: 0.594151, acc: 67.19%] [G loss: 2.007177]\n",
      "epoch:13 step:12703 [D loss: 0.642759, acc: 57.81%] [G loss: 2.158673]\n",
      "epoch:13 step:12704 [D loss: 0.607580, acc: 64.84%] [G loss: 1.977336]\n",
      "epoch:13 step:12705 [D loss: 0.656155, acc: 61.72%] [G loss: 1.971090]\n",
      "epoch:13 step:12706 [D loss: 0.655600, acc: 62.50%] [G loss: 1.809789]\n",
      "epoch:13 step:12707 [D loss: 0.656569, acc: 65.62%] [G loss: 1.962883]\n",
      "epoch:13 step:12708 [D loss: 0.637755, acc: 65.62%] [G loss: 1.896661]\n",
      "epoch:13 step:12709 [D loss: 0.713835, acc: 50.78%] [G loss: 1.799054]\n",
      "epoch:13 step:12710 [D loss: 0.678446, acc: 54.69%] [G loss: 1.736611]\n",
      "epoch:13 step:12711 [D loss: 0.647852, acc: 67.97%] [G loss: 1.852307]\n",
      "epoch:13 step:12712 [D loss: 0.642523, acc: 60.16%] [G loss: 1.867505]\n",
      "epoch:13 step:12713 [D loss: 0.652726, acc: 63.28%] [G loss: 1.946293]\n",
      "epoch:13 step:12714 [D loss: 0.668506, acc: 59.38%] [G loss: 1.979857]\n",
      "epoch:13 step:12715 [D loss: 0.617666, acc: 66.41%] [G loss: 1.989934]\n",
      "epoch:13 step:12716 [D loss: 0.616613, acc: 65.62%] [G loss: 1.825481]\n",
      "epoch:13 step:12717 [D loss: 0.638434, acc: 64.06%] [G loss: 2.038698]\n",
      "epoch:13 step:12718 [D loss: 0.658450, acc: 62.50%] [G loss: 1.890877]\n",
      "epoch:13 step:12719 [D loss: 0.729314, acc: 53.91%] [G loss: 1.781920]\n",
      "epoch:13 step:12720 [D loss: 0.621157, acc: 68.75%] [G loss: 1.988469]\n",
      "epoch:13 step:12721 [D loss: 0.649819, acc: 65.62%] [G loss: 1.799612]\n",
      "epoch:13 step:12722 [D loss: 0.655095, acc: 60.94%] [G loss: 1.951507]\n",
      "epoch:13 step:12723 [D loss: 0.722513, acc: 51.56%] [G loss: 1.767507]\n",
      "epoch:13 step:12724 [D loss: 0.654914, acc: 62.50%] [G loss: 1.842591]\n",
      "epoch:13 step:12725 [D loss: 0.683078, acc: 57.81%] [G loss: 1.918448]\n",
      "epoch:13 step:12726 [D loss: 0.638417, acc: 60.16%] [G loss: 1.958164]\n",
      "epoch:13 step:12727 [D loss: 0.636389, acc: 67.19%] [G loss: 1.973804]\n",
      "epoch:13 step:12728 [D loss: 0.632791, acc: 65.62%] [G loss: 2.090899]\n",
      "epoch:13 step:12729 [D loss: 0.634698, acc: 67.19%] [G loss: 2.116987]\n",
      "epoch:13 step:12730 [D loss: 0.621736, acc: 64.84%] [G loss: 1.968276]\n",
      "epoch:13 step:12731 [D loss: 0.611617, acc: 71.88%] [G loss: 2.005258]\n",
      "epoch:13 step:12732 [D loss: 0.610427, acc: 64.84%] [G loss: 1.969619]\n",
      "epoch:13 step:12733 [D loss: 0.602326, acc: 66.41%] [G loss: 1.998734]\n",
      "epoch:13 step:12734 [D loss: 0.690362, acc: 59.38%] [G loss: 1.789423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12735 [D loss: 0.575629, acc: 69.53%] [G loss: 2.208250]\n",
      "epoch:13 step:12736 [D loss: 0.639651, acc: 65.62%] [G loss: 2.084241]\n",
      "epoch:13 step:12737 [D loss: 0.607491, acc: 67.19%] [G loss: 2.008740]\n",
      "epoch:13 step:12738 [D loss: 0.567600, acc: 71.88%] [G loss: 1.977333]\n",
      "epoch:13 step:12739 [D loss: 0.596228, acc: 72.66%] [G loss: 1.961443]\n",
      "epoch:13 step:12740 [D loss: 0.694654, acc: 57.03%] [G loss: 1.932730]\n",
      "epoch:13 step:12741 [D loss: 0.649571, acc: 59.38%] [G loss: 1.834822]\n",
      "epoch:13 step:12742 [D loss: 0.646703, acc: 62.50%] [G loss: 2.066116]\n",
      "epoch:13 step:12743 [D loss: 0.708706, acc: 58.59%] [G loss: 1.866096]\n",
      "epoch:13 step:12744 [D loss: 0.679287, acc: 61.72%] [G loss: 1.860269]\n",
      "epoch:13 step:12745 [D loss: 0.659199, acc: 59.38%] [G loss: 2.097871]\n",
      "epoch:13 step:12746 [D loss: 0.671083, acc: 61.72%] [G loss: 2.090849]\n",
      "epoch:13 step:12747 [D loss: 0.699738, acc: 55.47%] [G loss: 1.906348]\n",
      "epoch:13 step:12748 [D loss: 0.660894, acc: 58.59%] [G loss: 1.942171]\n",
      "epoch:13 step:12749 [D loss: 0.660424, acc: 59.38%] [G loss: 1.934147]\n",
      "epoch:13 step:12750 [D loss: 0.654663, acc: 63.28%] [G loss: 1.992512]\n",
      "epoch:13 step:12751 [D loss: 0.603412, acc: 66.41%] [G loss: 1.976638]\n",
      "epoch:13 step:12752 [D loss: 0.662362, acc: 59.38%] [G loss: 1.912086]\n",
      "epoch:13 step:12753 [D loss: 0.647804, acc: 61.72%] [G loss: 1.846684]\n",
      "epoch:13 step:12754 [D loss: 0.638247, acc: 64.06%] [G loss: 1.815075]\n",
      "epoch:13 step:12755 [D loss: 0.600601, acc: 71.88%] [G loss: 1.909662]\n",
      "epoch:13 step:12756 [D loss: 0.622756, acc: 65.62%] [G loss: 1.978211]\n",
      "epoch:13 step:12757 [D loss: 0.665431, acc: 64.06%] [G loss: 1.824107]\n",
      "epoch:13 step:12758 [D loss: 0.661127, acc: 60.94%] [G loss: 1.728139]\n",
      "epoch:13 step:12759 [D loss: 0.630546, acc: 64.06%] [G loss: 1.847361]\n",
      "epoch:13 step:12760 [D loss: 0.640075, acc: 65.62%] [G loss: 1.960954]\n",
      "epoch:13 step:12761 [D loss: 0.660828, acc: 60.16%] [G loss: 1.917233]\n",
      "epoch:13 step:12762 [D loss: 0.602463, acc: 70.31%] [G loss: 1.886624]\n",
      "epoch:13 step:12763 [D loss: 0.571582, acc: 74.22%] [G loss: 2.045496]\n",
      "epoch:13 step:12764 [D loss: 0.633090, acc: 68.75%] [G loss: 1.873798]\n",
      "epoch:13 step:12765 [D loss: 0.696249, acc: 60.16%] [G loss: 1.804350]\n",
      "epoch:13 step:12766 [D loss: 0.639020, acc: 59.38%] [G loss: 1.847401]\n",
      "epoch:13 step:12767 [D loss: 0.734960, acc: 49.22%] [G loss: 1.973961]\n",
      "epoch:13 step:12768 [D loss: 0.632451, acc: 61.72%] [G loss: 1.912671]\n",
      "epoch:13 step:12769 [D loss: 0.640889, acc: 64.84%] [G loss: 1.915982]\n",
      "epoch:13 step:12770 [D loss: 0.570804, acc: 73.44%] [G loss: 2.064364]\n",
      "epoch:13 step:12771 [D loss: 0.634169, acc: 64.84%] [G loss: 2.035800]\n",
      "epoch:13 step:12772 [D loss: 0.717131, acc: 55.47%] [G loss: 1.818806]\n",
      "epoch:13 step:12773 [D loss: 0.637606, acc: 64.06%] [G loss: 1.934814]\n",
      "epoch:13 step:12774 [D loss: 0.678301, acc: 55.47%] [G loss: 1.891595]\n",
      "epoch:13 step:12775 [D loss: 0.588510, acc: 69.53%] [G loss: 1.882491]\n",
      "epoch:13 step:12776 [D loss: 0.677718, acc: 58.59%] [G loss: 2.075363]\n",
      "epoch:13 step:12777 [D loss: 0.613415, acc: 69.53%] [G loss: 1.977275]\n",
      "epoch:13 step:12778 [D loss: 0.700484, acc: 57.81%] [G loss: 1.755845]\n",
      "epoch:13 step:12779 [D loss: 0.646519, acc: 63.28%] [G loss: 1.710079]\n",
      "epoch:13 step:12780 [D loss: 0.672556, acc: 61.72%] [G loss: 1.837868]\n",
      "epoch:13 step:12781 [D loss: 0.682830, acc: 58.59%] [G loss: 1.843686]\n",
      "epoch:13 step:12782 [D loss: 0.673408, acc: 60.16%] [G loss: 1.973604]\n",
      "epoch:13 step:12783 [D loss: 0.625180, acc: 63.28%] [G loss: 1.880945]\n",
      "epoch:13 step:12784 [D loss: 0.584698, acc: 74.22%] [G loss: 2.044039]\n",
      "epoch:13 step:12785 [D loss: 0.608422, acc: 67.19%] [G loss: 1.895271]\n",
      "epoch:13 step:12786 [D loss: 0.650976, acc: 63.28%] [G loss: 2.052375]\n",
      "epoch:13 step:12787 [D loss: 0.648156, acc: 63.28%] [G loss: 1.799160]\n",
      "epoch:13 step:12788 [D loss: 0.698010, acc: 54.69%] [G loss: 1.952377]\n",
      "epoch:13 step:12789 [D loss: 0.649247, acc: 60.94%] [G loss: 1.888901]\n",
      "epoch:13 step:12790 [D loss: 0.596946, acc: 64.06%] [G loss: 1.981966]\n",
      "epoch:13 step:12791 [D loss: 0.670249, acc: 60.16%] [G loss: 1.848936]\n",
      "epoch:13 step:12792 [D loss: 0.629463, acc: 63.28%] [G loss: 1.870153]\n",
      "epoch:13 step:12793 [D loss: 0.614377, acc: 66.41%] [G loss: 1.897406]\n",
      "epoch:13 step:12794 [D loss: 0.637376, acc: 64.84%] [G loss: 1.967692]\n",
      "epoch:13 step:12795 [D loss: 0.667576, acc: 57.81%] [G loss: 1.850842]\n",
      "epoch:13 step:12796 [D loss: 0.658520, acc: 59.38%] [G loss: 1.893240]\n",
      "epoch:13 step:12797 [D loss: 0.653294, acc: 60.94%] [G loss: 1.942549]\n",
      "epoch:13 step:12798 [D loss: 0.644984, acc: 66.41%] [G loss: 1.783983]\n",
      "epoch:13 step:12799 [D loss: 0.673492, acc: 57.81%] [G loss: 1.902193]\n",
      "epoch:13 step:12800 [D loss: 0.666984, acc: 57.03%] [G loss: 1.834161]\n",
      "##############\n",
      "[2.55074986 1.24793589 6.11446037 4.95650851 3.69538326 5.85838065\n",
      " 4.36156276 4.82232724 4.76171403 3.72410044]\n",
      "##########\n",
      "epoch:13 step:12801 [D loss: 0.625203, acc: 66.41%] [G loss: 1.856451]\n",
      "epoch:13 step:12802 [D loss: 0.663329, acc: 60.16%] [G loss: 1.973035]\n",
      "epoch:13 step:12803 [D loss: 0.622686, acc: 66.41%] [G loss: 2.005813]\n",
      "epoch:13 step:12804 [D loss: 0.659229, acc: 61.72%] [G loss: 1.923386]\n",
      "epoch:13 step:12805 [D loss: 0.647589, acc: 60.94%] [G loss: 2.110512]\n",
      "epoch:13 step:12806 [D loss: 0.625607, acc: 67.19%] [G loss: 1.798736]\n",
      "epoch:13 step:12807 [D loss: 0.644113, acc: 57.81%] [G loss: 1.912245]\n",
      "epoch:13 step:12808 [D loss: 0.583811, acc: 72.66%] [G loss: 1.912710]\n",
      "epoch:13 step:12809 [D loss: 0.693526, acc: 58.59%] [G loss: 1.825976]\n",
      "epoch:13 step:12810 [D loss: 0.637552, acc: 59.38%] [G loss: 1.855756]\n",
      "epoch:13 step:12811 [D loss: 0.675564, acc: 57.03%] [G loss: 1.907444]\n",
      "epoch:13 step:12812 [D loss: 0.610119, acc: 67.19%] [G loss: 2.028473]\n",
      "epoch:13 step:12813 [D loss: 0.552943, acc: 67.97%] [G loss: 2.010656]\n",
      "epoch:13 step:12814 [D loss: 0.624213, acc: 64.06%] [G loss: 1.838357]\n",
      "epoch:13 step:12815 [D loss: 0.559225, acc: 72.66%] [G loss: 2.028509]\n",
      "epoch:13 step:12816 [D loss: 0.633512, acc: 67.19%] [G loss: 1.922050]\n",
      "epoch:13 step:12817 [D loss: 0.604448, acc: 67.97%] [G loss: 1.907792]\n",
      "epoch:13 step:12818 [D loss: 0.602668, acc: 66.41%] [G loss: 2.019947]\n",
      "epoch:13 step:12819 [D loss: 0.668045, acc: 59.38%] [G loss: 2.052110]\n",
      "epoch:13 step:12820 [D loss: 0.613625, acc: 67.97%] [G loss: 1.941827]\n",
      "epoch:13 step:12821 [D loss: 0.624147, acc: 64.06%] [G loss: 1.971873]\n",
      "epoch:13 step:12822 [D loss: 0.627948, acc: 62.50%] [G loss: 2.035862]\n",
      "epoch:13 step:12823 [D loss: 0.600588, acc: 69.53%] [G loss: 2.061356]\n",
      "epoch:13 step:12824 [D loss: 0.636370, acc: 62.50%] [G loss: 1.866426]\n",
      "epoch:13 step:12825 [D loss: 0.655050, acc: 65.62%] [G loss: 2.129117]\n",
      "epoch:13 step:12826 [D loss: 0.585153, acc: 69.53%] [G loss: 2.150284]\n",
      "epoch:13 step:12827 [D loss: 0.598594, acc: 68.75%] [G loss: 1.932310]\n",
      "epoch:13 step:12828 [D loss: 0.610987, acc: 67.19%] [G loss: 2.093819]\n",
      "epoch:13 step:12829 [D loss: 0.601552, acc: 67.19%] [G loss: 2.440754]\n",
      "epoch:13 step:12830 [D loss: 0.555285, acc: 71.88%] [G loss: 2.325479]\n",
      "epoch:13 step:12831 [D loss: 0.593046, acc: 67.97%] [G loss: 2.299721]\n",
      "epoch:13 step:12832 [D loss: 0.629121, acc: 64.06%] [G loss: 1.948730]\n",
      "epoch:13 step:12833 [D loss: 0.689507, acc: 59.38%] [G loss: 2.074153]\n",
      "epoch:13 step:12834 [D loss: 0.643903, acc: 66.41%] [G loss: 2.004963]\n",
      "epoch:13 step:12835 [D loss: 0.679255, acc: 57.03%] [G loss: 2.119747]\n",
      "epoch:13 step:12836 [D loss: 0.687000, acc: 55.47%] [G loss: 2.001092]\n",
      "epoch:13 step:12837 [D loss: 0.657513, acc: 62.50%] [G loss: 1.930126]\n",
      "epoch:13 step:12838 [D loss: 0.672557, acc: 60.94%] [G loss: 1.730034]\n",
      "epoch:13 step:12839 [D loss: 0.632841, acc: 65.62%] [G loss: 1.791276]\n",
      "epoch:13 step:12840 [D loss: 0.647326, acc: 64.06%] [G loss: 1.778944]\n",
      "epoch:13 step:12841 [D loss: 0.663035, acc: 60.94%] [G loss: 1.883162]\n",
      "epoch:13 step:12842 [D loss: 0.578452, acc: 69.53%] [G loss: 1.905961]\n",
      "epoch:13 step:12843 [D loss: 0.636549, acc: 65.62%] [G loss: 1.908150]\n",
      "epoch:13 step:12844 [D loss: 0.661335, acc: 60.94%] [G loss: 1.950634]\n",
      "epoch:13 step:12845 [D loss: 0.655469, acc: 60.94%] [G loss: 1.893305]\n",
      "epoch:13 step:12846 [D loss: 0.694378, acc: 57.03%] [G loss: 1.968855]\n",
      "epoch:13 step:12847 [D loss: 0.628165, acc: 67.19%] [G loss: 1.939287]\n",
      "epoch:13 step:12848 [D loss: 0.676017, acc: 55.47%] [G loss: 1.726174]\n",
      "epoch:13 step:12849 [D loss: 0.605272, acc: 67.19%] [G loss: 1.895367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12850 [D loss: 0.693036, acc: 58.59%] [G loss: 1.724193]\n",
      "epoch:13 step:12851 [D loss: 0.626109, acc: 64.06%] [G loss: 1.843394]\n",
      "epoch:13 step:12852 [D loss: 0.708077, acc: 63.28%] [G loss: 1.859532]\n",
      "epoch:13 step:12853 [D loss: 0.634652, acc: 66.41%] [G loss: 1.847284]\n",
      "epoch:13 step:12854 [D loss: 0.701607, acc: 55.47%] [G loss: 1.811564]\n",
      "epoch:13 step:12855 [D loss: 0.657819, acc: 59.38%] [G loss: 1.812840]\n",
      "epoch:13 step:12856 [D loss: 0.667444, acc: 61.72%] [G loss: 1.775193]\n",
      "epoch:13 step:12857 [D loss: 0.692782, acc: 56.25%] [G loss: 1.838532]\n",
      "epoch:13 step:12858 [D loss: 0.621239, acc: 63.28%] [G loss: 1.927182]\n",
      "epoch:13 step:12859 [D loss: 0.611483, acc: 65.62%] [G loss: 1.897417]\n",
      "epoch:13 step:12860 [D loss: 0.654712, acc: 60.16%] [G loss: 1.928745]\n",
      "epoch:13 step:12861 [D loss: 0.580825, acc: 72.66%] [G loss: 1.891768]\n",
      "epoch:13 step:12862 [D loss: 0.626067, acc: 67.97%] [G loss: 2.036497]\n",
      "epoch:13 step:12863 [D loss: 0.643808, acc: 67.19%] [G loss: 1.943759]\n",
      "epoch:13 step:12864 [D loss: 0.627556, acc: 62.50%] [G loss: 2.054272]\n",
      "epoch:13 step:12865 [D loss: 0.655276, acc: 60.94%] [G loss: 1.957182]\n",
      "epoch:13 step:12866 [D loss: 0.635760, acc: 65.62%] [G loss: 1.962234]\n",
      "epoch:13 step:12867 [D loss: 0.607370, acc: 64.84%] [G loss: 1.973420]\n",
      "epoch:13 step:12868 [D loss: 0.615908, acc: 64.84%] [G loss: 1.955097]\n",
      "epoch:13 step:12869 [D loss: 0.653954, acc: 62.50%] [G loss: 1.845250]\n",
      "epoch:13 step:12870 [D loss: 0.607585, acc: 67.97%] [G loss: 2.059437]\n",
      "epoch:13 step:12871 [D loss: 0.620252, acc: 65.62%] [G loss: 1.847538]\n",
      "epoch:13 step:12872 [D loss: 0.644141, acc: 63.28%] [G loss: 1.946409]\n",
      "epoch:13 step:12873 [D loss: 0.643318, acc: 57.81%] [G loss: 2.023512]\n",
      "epoch:13 step:12874 [D loss: 0.587484, acc: 69.53%] [G loss: 2.039062]\n",
      "epoch:13 step:12875 [D loss: 0.650949, acc: 64.06%] [G loss: 2.140271]\n",
      "epoch:13 step:12876 [D loss: 0.606447, acc: 69.53%] [G loss: 1.860058]\n",
      "epoch:13 step:12877 [D loss: 0.649235, acc: 66.41%] [G loss: 1.876015]\n",
      "epoch:13 step:12878 [D loss: 0.659384, acc: 60.94%] [G loss: 1.996345]\n",
      "epoch:13 step:12879 [D loss: 0.662675, acc: 60.94%] [G loss: 1.906023]\n",
      "epoch:13 step:12880 [D loss: 0.632914, acc: 64.84%] [G loss: 2.143367]\n",
      "epoch:13 step:12881 [D loss: 0.653667, acc: 65.62%] [G loss: 1.907592]\n",
      "epoch:13 step:12882 [D loss: 0.643270, acc: 64.06%] [G loss: 1.830765]\n",
      "epoch:13 step:12883 [D loss: 0.691693, acc: 56.25%] [G loss: 1.780305]\n",
      "epoch:13 step:12884 [D loss: 0.672477, acc: 59.38%] [G loss: 1.696140]\n",
      "epoch:13 step:12885 [D loss: 0.674902, acc: 59.38%] [G loss: 1.817921]\n",
      "epoch:13 step:12886 [D loss: 0.640856, acc: 65.62%] [G loss: 1.821910]\n",
      "epoch:13 step:12887 [D loss: 0.623401, acc: 60.16%] [G loss: 2.024593]\n",
      "epoch:13 step:12888 [D loss: 0.573583, acc: 71.09%] [G loss: 2.085774]\n",
      "epoch:13 step:12889 [D loss: 0.690930, acc: 58.59%] [G loss: 2.008006]\n",
      "epoch:13 step:12890 [D loss: 0.594728, acc: 67.19%] [G loss: 2.087238]\n",
      "epoch:13 step:12891 [D loss: 0.679440, acc: 59.38%] [G loss: 1.870777]\n",
      "epoch:13 step:12892 [D loss: 0.651272, acc: 63.28%] [G loss: 1.920852]\n",
      "epoch:13 step:12893 [D loss: 0.631484, acc: 63.28%] [G loss: 2.029467]\n",
      "epoch:13 step:12894 [D loss: 0.645008, acc: 67.97%] [G loss: 1.971571]\n",
      "epoch:13 step:12895 [D loss: 0.658727, acc: 63.28%] [G loss: 1.911511]\n",
      "epoch:13 step:12896 [D loss: 0.638141, acc: 62.50%] [G loss: 1.973849]\n",
      "epoch:13 step:12897 [D loss: 0.664917, acc: 65.62%] [G loss: 1.909410]\n",
      "epoch:13 step:12898 [D loss: 0.622523, acc: 64.84%] [G loss: 1.863031]\n",
      "epoch:13 step:12899 [D loss: 0.636761, acc: 64.84%] [G loss: 2.057597]\n",
      "epoch:13 step:12900 [D loss: 0.639135, acc: 64.84%] [G loss: 1.997329]\n",
      "epoch:13 step:12901 [D loss: 0.588095, acc: 66.41%] [G loss: 2.046797]\n",
      "epoch:13 step:12902 [D loss: 0.645528, acc: 63.28%] [G loss: 1.961959]\n",
      "epoch:13 step:12903 [D loss: 0.666751, acc: 56.25%] [G loss: 1.852914]\n",
      "epoch:13 step:12904 [D loss: 0.603111, acc: 65.62%] [G loss: 1.824254]\n",
      "epoch:13 step:12905 [D loss: 0.625641, acc: 62.50%] [G loss: 1.835176]\n",
      "epoch:13 step:12906 [D loss: 0.645291, acc: 59.38%] [G loss: 1.865430]\n",
      "epoch:13 step:12907 [D loss: 0.686657, acc: 63.28%] [G loss: 2.010101]\n",
      "epoch:13 step:12908 [D loss: 0.636513, acc: 67.97%] [G loss: 2.042947]\n",
      "epoch:13 step:12909 [D loss: 0.657326, acc: 61.72%] [G loss: 2.018257]\n",
      "epoch:13 step:12910 [D loss: 0.649510, acc: 64.06%] [G loss: 1.826658]\n",
      "epoch:13 step:12911 [D loss: 0.635293, acc: 60.16%] [G loss: 1.922047]\n",
      "epoch:13 step:12912 [D loss: 0.650632, acc: 59.38%] [G loss: 1.921719]\n",
      "epoch:13 step:12913 [D loss: 0.686717, acc: 57.03%] [G loss: 2.029732]\n",
      "epoch:13 step:12914 [D loss: 0.619017, acc: 64.84%] [G loss: 1.909072]\n",
      "epoch:13 step:12915 [D loss: 0.657453, acc: 64.06%] [G loss: 1.842140]\n",
      "epoch:13 step:12916 [D loss: 0.649148, acc: 59.38%] [G loss: 1.923534]\n",
      "epoch:13 step:12917 [D loss: 0.611336, acc: 67.19%] [G loss: 2.040410]\n",
      "epoch:13 step:12918 [D loss: 0.595718, acc: 63.28%] [G loss: 1.892202]\n",
      "epoch:13 step:12919 [D loss: 0.638420, acc: 67.97%] [G loss: 1.970746]\n",
      "epoch:13 step:12920 [D loss: 0.665856, acc: 59.38%] [G loss: 1.943043]\n",
      "epoch:13 step:12921 [D loss: 0.693105, acc: 50.78%] [G loss: 1.922181]\n",
      "epoch:13 step:12922 [D loss: 0.648761, acc: 62.50%] [G loss: 1.835027]\n",
      "epoch:13 step:12923 [D loss: 0.676439, acc: 57.03%] [G loss: 1.899928]\n",
      "epoch:13 step:12924 [D loss: 0.657477, acc: 57.81%] [G loss: 1.836394]\n",
      "epoch:13 step:12925 [D loss: 0.671142, acc: 62.50%] [G loss: 1.851401]\n",
      "epoch:13 step:12926 [D loss: 0.670049, acc: 57.81%] [G loss: 1.890989]\n",
      "epoch:13 step:12927 [D loss: 0.604854, acc: 64.84%] [G loss: 2.018306]\n",
      "epoch:13 step:12928 [D loss: 0.631300, acc: 67.19%] [G loss: 1.935371]\n",
      "epoch:13 step:12929 [D loss: 0.607859, acc: 68.75%] [G loss: 1.936652]\n",
      "epoch:13 step:12930 [D loss: 0.678838, acc: 57.81%] [G loss: 1.897444]\n",
      "epoch:13 step:12931 [D loss: 0.627973, acc: 65.62%] [G loss: 1.982838]\n",
      "epoch:13 step:12932 [D loss: 0.627145, acc: 62.50%] [G loss: 1.823551]\n",
      "epoch:13 step:12933 [D loss: 0.664520, acc: 61.72%] [G loss: 1.818979]\n",
      "epoch:13 step:12934 [D loss: 0.632445, acc: 63.28%] [G loss: 1.910318]\n",
      "epoch:13 step:12935 [D loss: 0.597294, acc: 68.75%] [G loss: 2.076669]\n",
      "epoch:13 step:12936 [D loss: 0.624319, acc: 67.19%] [G loss: 1.918383]\n",
      "epoch:13 step:12937 [D loss: 0.647559, acc: 66.41%] [G loss: 1.894742]\n",
      "epoch:13 step:12938 [D loss: 0.679677, acc: 60.94%] [G loss: 1.935442]\n",
      "epoch:13 step:12939 [D loss: 0.641993, acc: 64.06%] [G loss: 1.819633]\n",
      "epoch:13 step:12940 [D loss: 0.650340, acc: 61.72%] [G loss: 1.960992]\n",
      "epoch:13 step:12941 [D loss: 0.659833, acc: 67.97%] [G loss: 1.931706]\n",
      "epoch:13 step:12942 [D loss: 0.664686, acc: 63.28%] [G loss: 1.829313]\n",
      "epoch:13 step:12943 [D loss: 0.657504, acc: 60.16%] [G loss: 1.872370]\n",
      "epoch:13 step:12944 [D loss: 0.625963, acc: 60.94%] [G loss: 1.984615]\n",
      "epoch:13 step:12945 [D loss: 0.647242, acc: 62.50%] [G loss: 1.834847]\n",
      "epoch:13 step:12946 [D loss: 0.758095, acc: 48.44%] [G loss: 1.664624]\n",
      "epoch:13 step:12947 [D loss: 0.664754, acc: 62.50%] [G loss: 1.728508]\n",
      "epoch:13 step:12948 [D loss: 0.653130, acc: 61.72%] [G loss: 1.834275]\n",
      "epoch:13 step:12949 [D loss: 0.669664, acc: 59.38%] [G loss: 1.773788]\n",
      "epoch:13 step:12950 [D loss: 0.650118, acc: 61.72%] [G loss: 1.930295]\n",
      "epoch:13 step:12951 [D loss: 0.625574, acc: 63.28%] [G loss: 1.839222]\n",
      "epoch:13 step:12952 [D loss: 0.640260, acc: 61.72%] [G loss: 1.787669]\n",
      "epoch:13 step:12953 [D loss: 0.641600, acc: 62.50%] [G loss: 2.027168]\n",
      "epoch:13 step:12954 [D loss: 0.663618, acc: 61.72%] [G loss: 1.938508]\n",
      "epoch:13 step:12955 [D loss: 0.602885, acc: 67.97%] [G loss: 2.157912]\n",
      "epoch:13 step:12956 [D loss: 0.565436, acc: 75.78%] [G loss: 2.036949]\n",
      "epoch:13 step:12957 [D loss: 0.584216, acc: 66.41%] [G loss: 1.919031]\n",
      "epoch:13 step:12958 [D loss: 0.632861, acc: 62.50%] [G loss: 1.994639]\n",
      "epoch:13 step:12959 [D loss: 0.603778, acc: 65.62%] [G loss: 1.987122]\n",
      "epoch:13 step:12960 [D loss: 0.666874, acc: 60.16%] [G loss: 1.983020]\n",
      "epoch:13 step:12961 [D loss: 0.666262, acc: 62.50%] [G loss: 1.987016]\n",
      "epoch:13 step:12962 [D loss: 0.637692, acc: 66.41%] [G loss: 2.003499]\n",
      "epoch:13 step:12963 [D loss: 0.621251, acc: 62.50%] [G loss: 1.969919]\n",
      "epoch:13 step:12964 [D loss: 0.759388, acc: 48.44%] [G loss: 1.812734]\n",
      "epoch:13 step:12965 [D loss: 0.676076, acc: 57.81%] [G loss: 1.809765]\n",
      "epoch:13 step:12966 [D loss: 0.676505, acc: 57.03%] [G loss: 1.815900]\n",
      "epoch:13 step:12967 [D loss: 0.651052, acc: 62.50%] [G loss: 1.976049]\n",
      "epoch:13 step:12968 [D loss: 0.699044, acc: 53.12%] [G loss: 1.966181]\n",
      "epoch:13 step:12969 [D loss: 0.628007, acc: 64.06%] [G loss: 1.724339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12970 [D loss: 0.658592, acc: 61.72%] [G loss: 1.860718]\n",
      "epoch:13 step:12971 [D loss: 0.651559, acc: 57.81%] [G loss: 1.937324]\n",
      "epoch:13 step:12972 [D loss: 0.676692, acc: 59.38%] [G loss: 1.924399]\n",
      "epoch:13 step:12973 [D loss: 0.611927, acc: 64.06%] [G loss: 1.937699]\n",
      "epoch:13 step:12974 [D loss: 0.650268, acc: 62.50%] [G loss: 2.034435]\n",
      "epoch:13 step:12975 [D loss: 0.626729, acc: 62.50%] [G loss: 1.820900]\n",
      "epoch:13 step:12976 [D loss: 0.676336, acc: 57.81%] [G loss: 1.833273]\n",
      "epoch:13 step:12977 [D loss: 0.658600, acc: 63.28%] [G loss: 1.951290]\n",
      "epoch:13 step:12978 [D loss: 0.699100, acc: 60.94%] [G loss: 1.757639]\n",
      "epoch:13 step:12979 [D loss: 0.617633, acc: 66.41%] [G loss: 1.894784]\n",
      "epoch:13 step:12980 [D loss: 0.631204, acc: 67.97%] [G loss: 1.860656]\n",
      "epoch:13 step:12981 [D loss: 0.661781, acc: 66.41%] [G loss: 1.761692]\n",
      "epoch:13 step:12982 [D loss: 0.701058, acc: 54.69%] [G loss: 1.721818]\n",
      "epoch:13 step:12983 [D loss: 0.649770, acc: 60.94%] [G loss: 1.833119]\n",
      "epoch:13 step:12984 [D loss: 0.615438, acc: 67.19%] [G loss: 1.795836]\n",
      "epoch:13 step:12985 [D loss: 0.599309, acc: 67.97%] [G loss: 1.926326]\n",
      "epoch:13 step:12986 [D loss: 0.634749, acc: 64.06%] [G loss: 1.919014]\n",
      "epoch:13 step:12987 [D loss: 0.639683, acc: 61.72%] [G loss: 1.860514]\n",
      "epoch:13 step:12988 [D loss: 0.632951, acc: 68.75%] [G loss: 1.913826]\n",
      "epoch:13 step:12989 [D loss: 0.668715, acc: 61.72%] [G loss: 1.963465]\n",
      "epoch:13 step:12990 [D loss: 0.668095, acc: 60.94%] [G loss: 1.881303]\n",
      "epoch:13 step:12991 [D loss: 0.675617, acc: 61.72%] [G loss: 1.862258]\n",
      "epoch:13 step:12992 [D loss: 0.652852, acc: 64.06%] [G loss: 1.915599]\n",
      "epoch:13 step:12993 [D loss: 0.645752, acc: 58.59%] [G loss: 1.812599]\n",
      "epoch:13 step:12994 [D loss: 0.649127, acc: 65.62%] [G loss: 1.848288]\n",
      "epoch:13 step:12995 [D loss: 0.665096, acc: 58.59%] [G loss: 1.981031]\n",
      "epoch:13 step:12996 [D loss: 0.601236, acc: 65.62%] [G loss: 2.215086]\n",
      "epoch:13 step:12997 [D loss: 0.565858, acc: 68.75%] [G loss: 2.018461]\n",
      "epoch:13 step:12998 [D loss: 0.696231, acc: 57.81%] [G loss: 1.934640]\n",
      "epoch:13 step:12999 [D loss: 0.673325, acc: 60.16%] [G loss: 1.849964]\n",
      "epoch:13 step:13000 [D loss: 0.641419, acc: 66.41%] [G loss: 1.920611]\n",
      "##############\n",
      "[2.37992579 1.48868835 6.2103074  4.79708187 3.74837267 5.68333437\n",
      " 4.49068441 4.64523302 4.7428516  3.29153672]\n",
      "##########\n",
      "epoch:13 step:13001 [D loss: 0.668229, acc: 57.81%] [G loss: 1.720914]\n",
      "epoch:13 step:13002 [D loss: 0.608847, acc: 68.75%] [G loss: 1.960549]\n",
      "epoch:13 step:13003 [D loss: 0.618554, acc: 67.19%] [G loss: 1.969447]\n",
      "epoch:13 step:13004 [D loss: 0.622752, acc: 65.62%] [G loss: 2.057983]\n",
      "epoch:13 step:13005 [D loss: 0.623580, acc: 66.41%] [G loss: 1.877542]\n",
      "epoch:13 step:13006 [D loss: 0.661508, acc: 62.50%] [G loss: 1.863740]\n",
      "epoch:13 step:13007 [D loss: 0.626099, acc: 64.84%] [G loss: 2.002319]\n",
      "epoch:13 step:13008 [D loss: 0.654412, acc: 60.94%] [G loss: 1.916873]\n",
      "epoch:13 step:13009 [D loss: 0.698175, acc: 57.81%] [G loss: 1.920665]\n",
      "epoch:13 step:13010 [D loss: 0.706108, acc: 54.69%] [G loss: 1.922346]\n",
      "epoch:13 step:13011 [D loss: 0.617201, acc: 62.50%] [G loss: 1.872678]\n",
      "epoch:13 step:13012 [D loss: 0.698185, acc: 53.91%] [G loss: 1.966715]\n",
      "epoch:13 step:13013 [D loss: 0.622719, acc: 63.28%] [G loss: 1.960599]\n",
      "epoch:13 step:13014 [D loss: 0.624982, acc: 64.84%] [G loss: 2.065838]\n",
      "epoch:13 step:13015 [D loss: 0.670909, acc: 60.16%] [G loss: 2.000598]\n",
      "epoch:13 step:13016 [D loss: 0.631777, acc: 66.41%] [G loss: 2.010011]\n",
      "epoch:13 step:13017 [D loss: 0.615074, acc: 62.50%] [G loss: 1.821185]\n",
      "epoch:13 step:13018 [D loss: 0.635173, acc: 59.38%] [G loss: 2.017375]\n",
      "epoch:13 step:13019 [D loss: 0.642091, acc: 64.84%] [G loss: 2.019386]\n",
      "epoch:13 step:13020 [D loss: 0.655579, acc: 60.94%] [G loss: 1.941343]\n",
      "epoch:13 step:13021 [D loss: 0.581327, acc: 67.97%] [G loss: 2.088107]\n",
      "epoch:13 step:13022 [D loss: 0.601183, acc: 69.53%] [G loss: 2.078372]\n",
      "epoch:13 step:13023 [D loss: 0.617975, acc: 64.06%] [G loss: 2.002767]\n",
      "epoch:13 step:13024 [D loss: 0.659064, acc: 59.38%] [G loss: 2.031027]\n",
      "epoch:13 step:13025 [D loss: 0.604286, acc: 64.84%] [G loss: 2.053778]\n",
      "epoch:13 step:13026 [D loss: 0.620749, acc: 66.41%] [G loss: 1.968015]\n",
      "epoch:13 step:13027 [D loss: 0.635839, acc: 65.62%] [G loss: 2.019310]\n",
      "epoch:13 step:13028 [D loss: 0.600060, acc: 68.75%] [G loss: 1.944823]\n",
      "epoch:13 step:13029 [D loss: 0.636943, acc: 63.28%] [G loss: 1.844013]\n",
      "epoch:13 step:13030 [D loss: 0.595727, acc: 71.88%] [G loss: 1.953832]\n",
      "epoch:13 step:13031 [D loss: 0.706759, acc: 51.56%] [G loss: 1.802338]\n",
      "epoch:13 step:13032 [D loss: 0.660299, acc: 61.72%] [G loss: 1.842794]\n",
      "epoch:13 step:13033 [D loss: 0.601079, acc: 65.62%] [G loss: 1.941522]\n",
      "epoch:13 step:13034 [D loss: 0.668022, acc: 61.72%] [G loss: 1.910018]\n",
      "epoch:13 step:13035 [D loss: 0.682350, acc: 60.94%] [G loss: 1.823107]\n",
      "epoch:13 step:13036 [D loss: 0.615336, acc: 64.84%] [G loss: 1.816754]\n",
      "epoch:13 step:13037 [D loss: 0.707333, acc: 45.31%] [G loss: 1.823155]\n",
      "epoch:13 step:13038 [D loss: 0.645560, acc: 64.06%] [G loss: 2.052771]\n",
      "epoch:13 step:13039 [D loss: 0.671229, acc: 56.25%] [G loss: 1.936109]\n",
      "epoch:13 step:13040 [D loss: 0.646678, acc: 63.28%] [G loss: 1.808260]\n",
      "epoch:13 step:13041 [D loss: 0.604969, acc: 68.75%] [G loss: 2.039805]\n",
      "epoch:13 step:13042 [D loss: 0.639286, acc: 61.72%] [G loss: 1.837713]\n",
      "epoch:13 step:13043 [D loss: 0.699857, acc: 61.72%] [G loss: 1.793676]\n",
      "epoch:13 step:13044 [D loss: 0.590450, acc: 68.75%] [G loss: 1.976985]\n",
      "epoch:13 step:13045 [D loss: 0.682097, acc: 53.91%] [G loss: 1.835462]\n",
      "epoch:13 step:13046 [D loss: 0.581195, acc: 71.09%] [G loss: 1.969079]\n",
      "epoch:13 step:13047 [D loss: 0.666204, acc: 59.38%] [G loss: 1.889241]\n",
      "epoch:13 step:13048 [D loss: 0.710111, acc: 57.81%] [G loss: 1.961595]\n",
      "epoch:13 step:13049 [D loss: 0.576327, acc: 71.09%] [G loss: 2.137007]\n",
      "epoch:13 step:13050 [D loss: 0.656110, acc: 64.06%] [G loss: 1.810525]\n",
      "epoch:13 step:13051 [D loss: 0.658071, acc: 60.94%] [G loss: 1.900970]\n",
      "epoch:13 step:13052 [D loss: 0.638533, acc: 66.41%] [G loss: 1.904263]\n",
      "epoch:13 step:13053 [D loss: 0.631177, acc: 67.19%] [G loss: 1.762894]\n",
      "epoch:13 step:13054 [D loss: 0.663728, acc: 57.81%] [G loss: 1.790996]\n",
      "epoch:13 step:13055 [D loss: 0.686352, acc: 56.25%] [G loss: 1.895941]\n",
      "epoch:13 step:13056 [D loss: 0.582607, acc: 75.00%] [G loss: 2.005348]\n",
      "epoch:13 step:13057 [D loss: 0.642386, acc: 68.75%] [G loss: 1.960112]\n",
      "epoch:13 step:13058 [D loss: 0.612330, acc: 65.62%] [G loss: 1.899960]\n",
      "epoch:13 step:13059 [D loss: 0.633842, acc: 62.50%] [G loss: 1.787583]\n",
      "epoch:13 step:13060 [D loss: 0.670897, acc: 61.72%] [G loss: 1.866961]\n",
      "epoch:13 step:13061 [D loss: 0.662370, acc: 59.38%] [G loss: 1.888246]\n",
      "epoch:13 step:13062 [D loss: 0.600690, acc: 65.62%] [G loss: 1.939626]\n",
      "epoch:13 step:13063 [D loss: 0.641245, acc: 60.16%] [G loss: 1.952907]\n",
      "epoch:13 step:13064 [D loss: 0.694845, acc: 59.38%] [G loss: 2.042020]\n",
      "epoch:13 step:13065 [D loss: 0.575354, acc: 69.53%] [G loss: 2.033874]\n",
      "epoch:13 step:13066 [D loss: 0.646675, acc: 60.94%] [G loss: 2.061933]\n",
      "epoch:13 step:13067 [D loss: 0.600292, acc: 64.06%] [G loss: 2.015257]\n",
      "epoch:13 step:13068 [D loss: 0.627856, acc: 62.50%] [G loss: 1.960137]\n",
      "epoch:13 step:13069 [D loss: 0.646088, acc: 64.84%] [G loss: 1.945473]\n",
      "epoch:13 step:13070 [D loss: 0.700358, acc: 56.25%] [G loss: 1.939689]\n",
      "epoch:13 step:13071 [D loss: 0.691413, acc: 63.28%] [G loss: 2.055754]\n",
      "epoch:13 step:13072 [D loss: 0.656493, acc: 62.50%] [G loss: 1.828726]\n",
      "epoch:13 step:13073 [D loss: 0.657785, acc: 57.81%] [G loss: 1.942258]\n",
      "epoch:13 step:13074 [D loss: 0.677395, acc: 59.38%] [G loss: 1.885191]\n",
      "epoch:13 step:13075 [D loss: 0.593225, acc: 67.97%] [G loss: 2.050111]\n",
      "epoch:13 step:13076 [D loss: 0.668773, acc: 59.38%] [G loss: 1.949854]\n",
      "epoch:13 step:13077 [D loss: 0.656415, acc: 61.72%] [G loss: 1.943665]\n",
      "epoch:13 step:13078 [D loss: 0.629591, acc: 61.72%] [G loss: 2.006601]\n",
      "epoch:13 step:13079 [D loss: 0.646299, acc: 64.84%] [G loss: 2.065063]\n",
      "epoch:13 step:13080 [D loss: 0.608873, acc: 68.75%] [G loss: 1.988835]\n",
      "epoch:13 step:13081 [D loss: 0.651521, acc: 62.50%] [G loss: 2.104419]\n",
      "epoch:13 step:13082 [D loss: 0.631853, acc: 67.97%] [G loss: 1.861771]\n",
      "epoch:13 step:13083 [D loss: 0.621070, acc: 62.50%] [G loss: 1.945931]\n",
      "epoch:13 step:13084 [D loss: 0.589021, acc: 69.53%] [G loss: 1.915319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13085 [D loss: 0.636403, acc: 62.50%] [G loss: 2.030813]\n",
      "epoch:13 step:13086 [D loss: 0.603892, acc: 66.41%] [G loss: 2.057894]\n",
      "epoch:13 step:13087 [D loss: 0.693909, acc: 51.56%] [G loss: 1.945715]\n",
      "epoch:13 step:13088 [D loss: 0.576036, acc: 64.84%] [G loss: 1.922941]\n",
      "epoch:13 step:13089 [D loss: 0.645768, acc: 63.28%] [G loss: 2.011465]\n",
      "epoch:13 step:13090 [D loss: 0.567466, acc: 73.44%] [G loss: 2.041279]\n",
      "epoch:13 step:13091 [D loss: 0.631351, acc: 60.94%] [G loss: 2.068806]\n",
      "epoch:13 step:13092 [D loss: 0.598863, acc: 73.44%] [G loss: 2.041190]\n",
      "epoch:13 step:13093 [D loss: 0.611428, acc: 64.84%] [G loss: 2.500620]\n",
      "epoch:13 step:13094 [D loss: 0.636123, acc: 57.81%] [G loss: 2.122870]\n",
      "epoch:13 step:13095 [D loss: 0.605134, acc: 66.41%] [G loss: 1.983595]\n",
      "epoch:13 step:13096 [D loss: 0.668638, acc: 56.25%] [G loss: 2.071223]\n",
      "epoch:13 step:13097 [D loss: 0.644805, acc: 65.62%] [G loss: 2.068619]\n",
      "epoch:13 step:13098 [D loss: 0.684417, acc: 57.03%] [G loss: 1.931034]\n",
      "epoch:13 step:13099 [D loss: 0.633746, acc: 68.75%] [G loss: 2.156798]\n",
      "epoch:13 step:13100 [D loss: 0.583979, acc: 70.31%] [G loss: 2.225901]\n",
      "epoch:13 step:13101 [D loss: 0.747722, acc: 50.78%] [G loss: 1.814822]\n",
      "epoch:13 step:13102 [D loss: 0.618418, acc: 65.62%] [G loss: 1.940128]\n",
      "epoch:13 step:13103 [D loss: 0.623302, acc: 67.19%] [G loss: 2.053480]\n",
      "epoch:13 step:13104 [D loss: 0.603564, acc: 70.31%] [G loss: 2.259914]\n",
      "epoch:13 step:13105 [D loss: 0.587860, acc: 71.09%] [G loss: 2.294808]\n",
      "epoch:13 step:13106 [D loss: 0.569290, acc: 71.88%] [G loss: 2.156764]\n",
      "epoch:13 step:13107 [D loss: 0.601802, acc: 66.41%] [G loss: 2.340954]\n",
      "epoch:13 step:13108 [D loss: 0.663023, acc: 65.62%] [G loss: 2.220090]\n",
      "epoch:13 step:13109 [D loss: 0.727287, acc: 57.81%] [G loss: 1.774319]\n",
      "epoch:13 step:13110 [D loss: 0.756045, acc: 47.66%] [G loss: 1.955652]\n",
      "epoch:13 step:13111 [D loss: 0.620912, acc: 67.97%] [G loss: 2.056487]\n",
      "epoch:13 step:13112 [D loss: 0.614811, acc: 64.06%] [G loss: 2.021229]\n",
      "epoch:13 step:13113 [D loss: 0.663866, acc: 62.50%] [G loss: 2.020571]\n",
      "epoch:13 step:13114 [D loss: 0.605317, acc: 67.19%] [G loss: 1.939203]\n",
      "epoch:13 step:13115 [D loss: 0.622300, acc: 64.84%] [G loss: 1.940599]\n",
      "epoch:13 step:13116 [D loss: 0.623547, acc: 64.84%] [G loss: 1.862377]\n",
      "epoch:13 step:13117 [D loss: 0.604325, acc: 65.62%] [G loss: 2.177933]\n",
      "epoch:13 step:13118 [D loss: 0.596175, acc: 70.31%] [G loss: 2.384107]\n",
      "epoch:14 step:13119 [D loss: 0.691723, acc: 56.25%] [G loss: 1.847827]\n",
      "epoch:14 step:13120 [D loss: 0.687156, acc: 61.72%] [G loss: 2.042933]\n",
      "epoch:14 step:13121 [D loss: 0.654844, acc: 61.72%] [G loss: 1.874152]\n",
      "epoch:14 step:13122 [D loss: 0.618026, acc: 66.41%] [G loss: 1.889152]\n",
      "epoch:14 step:13123 [D loss: 0.669337, acc: 57.03%] [G loss: 1.856762]\n",
      "epoch:14 step:13124 [D loss: 0.624761, acc: 65.62%] [G loss: 1.961602]\n",
      "epoch:14 step:13125 [D loss: 0.634463, acc: 62.50%] [G loss: 1.978824]\n",
      "epoch:14 step:13126 [D loss: 0.737270, acc: 57.03%] [G loss: 1.874079]\n",
      "epoch:14 step:13127 [D loss: 0.636824, acc: 63.28%] [G loss: 1.912358]\n",
      "epoch:14 step:13128 [D loss: 0.605479, acc: 64.84%] [G loss: 2.007668]\n",
      "epoch:14 step:13129 [D loss: 0.676821, acc: 59.38%] [G loss: 2.014142]\n",
      "epoch:14 step:13130 [D loss: 0.698497, acc: 55.47%] [G loss: 1.943972]\n",
      "epoch:14 step:13131 [D loss: 0.651982, acc: 63.28%] [G loss: 2.088921]\n",
      "epoch:14 step:13132 [D loss: 0.669767, acc: 61.72%] [G loss: 1.930123]\n",
      "epoch:14 step:13133 [D loss: 0.603676, acc: 71.09%] [G loss: 2.226211]\n",
      "epoch:14 step:13134 [D loss: 0.621636, acc: 63.28%] [G loss: 2.070164]\n",
      "epoch:14 step:13135 [D loss: 0.610149, acc: 67.19%] [G loss: 1.959076]\n",
      "epoch:14 step:13136 [D loss: 0.716642, acc: 52.34%] [G loss: 1.779933]\n",
      "epoch:14 step:13137 [D loss: 0.592370, acc: 64.84%] [G loss: 1.829922]\n",
      "epoch:14 step:13138 [D loss: 0.675055, acc: 54.69%] [G loss: 1.696793]\n",
      "epoch:14 step:13139 [D loss: 0.654560, acc: 62.50%] [G loss: 1.882465]\n",
      "epoch:14 step:13140 [D loss: 0.656104, acc: 60.94%] [G loss: 1.841991]\n",
      "epoch:14 step:13141 [D loss: 0.613808, acc: 69.53%] [G loss: 2.137252]\n",
      "epoch:14 step:13142 [D loss: 0.626164, acc: 67.97%] [G loss: 1.990591]\n",
      "epoch:14 step:13143 [D loss: 0.587767, acc: 69.53%] [G loss: 2.004456]\n",
      "epoch:14 step:13144 [D loss: 0.621889, acc: 66.41%] [G loss: 1.906113]\n",
      "epoch:14 step:13145 [D loss: 0.680927, acc: 57.03%] [G loss: 1.951178]\n",
      "epoch:14 step:13146 [D loss: 0.671536, acc: 63.28%] [G loss: 1.820226]\n",
      "epoch:14 step:13147 [D loss: 0.599816, acc: 67.19%] [G loss: 1.767833]\n",
      "epoch:14 step:13148 [D loss: 0.639863, acc: 62.50%] [G loss: 1.837041]\n",
      "epoch:14 step:13149 [D loss: 0.661419, acc: 57.81%] [G loss: 1.863700]\n",
      "epoch:14 step:13150 [D loss: 0.690759, acc: 56.25%] [G loss: 1.774429]\n",
      "epoch:14 step:13151 [D loss: 0.645006, acc: 63.28%] [G loss: 1.852358]\n",
      "epoch:14 step:13152 [D loss: 0.601533, acc: 66.41%] [G loss: 1.849088]\n",
      "epoch:14 step:13153 [D loss: 0.649540, acc: 64.84%] [G loss: 1.835545]\n",
      "epoch:14 step:13154 [D loss: 0.602861, acc: 68.75%] [G loss: 2.040514]\n",
      "epoch:14 step:13155 [D loss: 0.614888, acc: 71.09%] [G loss: 2.050337]\n",
      "epoch:14 step:13156 [D loss: 0.647429, acc: 60.16%] [G loss: 2.043245]\n",
      "epoch:14 step:13157 [D loss: 0.634681, acc: 64.84%] [G loss: 1.918364]\n",
      "epoch:14 step:13158 [D loss: 0.570536, acc: 70.31%] [G loss: 2.132364]\n",
      "epoch:14 step:13159 [D loss: 0.680482, acc: 59.38%] [G loss: 1.897640]\n",
      "epoch:14 step:13160 [D loss: 0.603851, acc: 69.53%] [G loss: 1.909992]\n",
      "epoch:14 step:13161 [D loss: 0.606043, acc: 69.53%] [G loss: 1.935902]\n",
      "epoch:14 step:13162 [D loss: 0.659055, acc: 60.16%] [G loss: 1.926652]\n",
      "epoch:14 step:13163 [D loss: 0.672721, acc: 59.38%] [G loss: 1.987479]\n",
      "epoch:14 step:13164 [D loss: 0.631678, acc: 64.06%] [G loss: 1.795743]\n",
      "epoch:14 step:13165 [D loss: 0.589595, acc: 72.66%] [G loss: 1.899876]\n",
      "epoch:14 step:13166 [D loss: 0.646605, acc: 61.72%] [G loss: 2.040931]\n",
      "epoch:14 step:13167 [D loss: 0.587653, acc: 69.53%] [G loss: 1.935351]\n",
      "epoch:14 step:13168 [D loss: 0.641695, acc: 67.97%] [G loss: 2.152853]\n",
      "epoch:14 step:13169 [D loss: 0.652563, acc: 60.94%] [G loss: 1.969754]\n",
      "epoch:14 step:13170 [D loss: 0.714586, acc: 57.03%] [G loss: 1.937914]\n",
      "epoch:14 step:13171 [D loss: 0.632714, acc: 64.06%] [G loss: 2.071982]\n",
      "epoch:14 step:13172 [D loss: 0.666968, acc: 60.94%] [G loss: 2.082249]\n",
      "epoch:14 step:13173 [D loss: 0.631842, acc: 61.72%] [G loss: 1.935089]\n",
      "epoch:14 step:13174 [D loss: 0.670196, acc: 62.50%] [G loss: 2.058009]\n",
      "epoch:14 step:13175 [D loss: 0.633380, acc: 61.72%] [G loss: 1.929985]\n",
      "epoch:14 step:13176 [D loss: 0.594276, acc: 68.75%] [G loss: 1.914299]\n",
      "epoch:14 step:13177 [D loss: 0.660681, acc: 59.38%] [G loss: 1.850108]\n",
      "epoch:14 step:13178 [D loss: 0.683857, acc: 55.47%] [G loss: 1.840506]\n",
      "epoch:14 step:13179 [D loss: 0.701684, acc: 56.25%] [G loss: 1.935122]\n",
      "epoch:14 step:13180 [D loss: 0.660722, acc: 65.62%] [G loss: 2.012314]\n",
      "epoch:14 step:13181 [D loss: 0.659886, acc: 61.72%] [G loss: 1.836905]\n",
      "epoch:14 step:13182 [D loss: 0.587079, acc: 67.97%] [G loss: 1.899608]\n",
      "epoch:14 step:13183 [D loss: 0.660628, acc: 60.16%] [G loss: 1.800872]\n",
      "epoch:14 step:13184 [D loss: 0.670217, acc: 64.06%] [G loss: 1.920393]\n",
      "epoch:14 step:13185 [D loss: 0.642766, acc: 64.84%] [G loss: 1.806624]\n",
      "epoch:14 step:13186 [D loss: 0.651168, acc: 63.28%] [G loss: 1.966403]\n",
      "epoch:14 step:13187 [D loss: 0.597911, acc: 69.53%] [G loss: 2.135685]\n",
      "epoch:14 step:13188 [D loss: 0.588138, acc: 69.53%] [G loss: 1.964366]\n",
      "epoch:14 step:13189 [D loss: 0.660184, acc: 64.06%] [G loss: 1.859974]\n",
      "epoch:14 step:13190 [D loss: 0.662638, acc: 61.72%] [G loss: 1.963052]\n",
      "epoch:14 step:13191 [D loss: 0.677710, acc: 58.59%] [G loss: 1.850401]\n",
      "epoch:14 step:13192 [D loss: 0.686004, acc: 62.50%] [G loss: 2.125861]\n",
      "epoch:14 step:13193 [D loss: 0.589475, acc: 71.88%] [G loss: 2.011669]\n",
      "epoch:14 step:13194 [D loss: 0.596694, acc: 67.97%] [G loss: 2.122929]\n",
      "epoch:14 step:13195 [D loss: 0.604892, acc: 69.53%] [G loss: 2.096739]\n",
      "epoch:14 step:13196 [D loss: 0.641495, acc: 61.72%] [G loss: 1.818131]\n",
      "epoch:14 step:13197 [D loss: 0.643140, acc: 61.72%] [G loss: 1.836510]\n",
      "epoch:14 step:13198 [D loss: 0.679433, acc: 60.94%] [G loss: 1.807751]\n",
      "epoch:14 step:13199 [D loss: 0.706071, acc: 57.81%] [G loss: 1.783753]\n",
      "epoch:14 step:13200 [D loss: 0.632778, acc: 65.62%] [G loss: 1.823313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.46152388 1.35778819 6.22303192 4.74102308 3.67840553 5.73270002\n",
      " 4.3895518  4.65029327 4.65158601 3.57091277]\n",
      "##########\n",
      "epoch:14 step:13201 [D loss: 0.659695, acc: 60.94%] [G loss: 1.845741]\n",
      "epoch:14 step:13202 [D loss: 0.656725, acc: 60.16%] [G loss: 1.911160]\n",
      "epoch:14 step:13203 [D loss: 0.689900, acc: 56.25%] [G loss: 1.786094]\n",
      "epoch:14 step:13204 [D loss: 0.684580, acc: 57.81%] [G loss: 1.861854]\n",
      "epoch:14 step:13205 [D loss: 0.577443, acc: 69.53%] [G loss: 1.895132]\n",
      "epoch:14 step:13206 [D loss: 0.608132, acc: 70.31%] [G loss: 1.789088]\n",
      "epoch:14 step:13207 [D loss: 0.604430, acc: 71.88%] [G loss: 1.996060]\n",
      "epoch:14 step:13208 [D loss: 0.655999, acc: 63.28%] [G loss: 1.958516]\n",
      "epoch:14 step:13209 [D loss: 0.644277, acc: 61.72%] [G loss: 2.028176]\n",
      "epoch:14 step:13210 [D loss: 0.591819, acc: 70.31%] [G loss: 2.057317]\n",
      "epoch:14 step:13211 [D loss: 0.614010, acc: 66.41%] [G loss: 2.215189]\n",
      "epoch:14 step:13212 [D loss: 0.649934, acc: 62.50%] [G loss: 1.931519]\n",
      "epoch:14 step:13213 [D loss: 0.656252, acc: 64.84%] [G loss: 1.899075]\n",
      "epoch:14 step:13214 [D loss: 0.649217, acc: 57.81%] [G loss: 1.999165]\n",
      "epoch:14 step:13215 [D loss: 0.630654, acc: 64.06%] [G loss: 1.999962]\n",
      "epoch:14 step:13216 [D loss: 0.647238, acc: 60.94%] [G loss: 1.908032]\n",
      "epoch:14 step:13217 [D loss: 0.633794, acc: 63.28%] [G loss: 1.967780]\n",
      "epoch:14 step:13218 [D loss: 0.598012, acc: 69.53%] [G loss: 2.076189]\n",
      "epoch:14 step:13219 [D loss: 0.578675, acc: 71.09%] [G loss: 2.110064]\n",
      "epoch:14 step:13220 [D loss: 0.660384, acc: 60.94%] [G loss: 1.961519]\n",
      "epoch:14 step:13221 [D loss: 0.653115, acc: 62.50%] [G loss: 2.010804]\n",
      "epoch:14 step:13222 [D loss: 0.641225, acc: 61.72%] [G loss: 1.936908]\n",
      "epoch:14 step:13223 [D loss: 0.643399, acc: 60.94%] [G loss: 1.924661]\n",
      "epoch:14 step:13224 [D loss: 0.563781, acc: 70.31%] [G loss: 2.024724]\n",
      "epoch:14 step:13225 [D loss: 0.617843, acc: 65.62%] [G loss: 1.995880]\n",
      "epoch:14 step:13226 [D loss: 0.646537, acc: 60.94%] [G loss: 1.872865]\n",
      "epoch:14 step:13227 [D loss: 0.605576, acc: 66.41%] [G loss: 1.989991]\n",
      "epoch:14 step:13228 [D loss: 0.682975, acc: 57.81%] [G loss: 1.922451]\n",
      "epoch:14 step:13229 [D loss: 0.623968, acc: 65.62%] [G loss: 1.946909]\n",
      "epoch:14 step:13230 [D loss: 0.632436, acc: 61.72%] [G loss: 1.944667]\n",
      "epoch:14 step:13231 [D loss: 0.701035, acc: 57.03%] [G loss: 1.867637]\n",
      "epoch:14 step:13232 [D loss: 0.723138, acc: 53.91%] [G loss: 1.906732]\n",
      "epoch:14 step:13233 [D loss: 0.599495, acc: 69.53%] [G loss: 2.062286]\n",
      "epoch:14 step:13234 [D loss: 0.594012, acc: 68.75%] [G loss: 2.217282]\n",
      "epoch:14 step:13235 [D loss: 0.613120, acc: 67.19%] [G loss: 2.380427]\n",
      "epoch:14 step:13236 [D loss: 0.648396, acc: 65.62%] [G loss: 1.949193]\n",
      "epoch:14 step:13237 [D loss: 0.596648, acc: 67.97%] [G loss: 2.377149]\n",
      "epoch:14 step:13238 [D loss: 0.672105, acc: 64.84%] [G loss: 2.104068]\n",
      "epoch:14 step:13239 [D loss: 0.631419, acc: 65.62%] [G loss: 1.925429]\n",
      "epoch:14 step:13240 [D loss: 0.676112, acc: 60.16%] [G loss: 1.964503]\n",
      "epoch:14 step:13241 [D loss: 0.714816, acc: 56.25%] [G loss: 1.895881]\n",
      "epoch:14 step:13242 [D loss: 0.663221, acc: 61.72%] [G loss: 1.830969]\n",
      "epoch:14 step:13243 [D loss: 0.713839, acc: 54.69%] [G loss: 1.837063]\n",
      "epoch:14 step:13244 [D loss: 0.606751, acc: 67.19%] [G loss: 1.904092]\n",
      "epoch:14 step:13245 [D loss: 0.641709, acc: 60.16%] [G loss: 1.887838]\n",
      "epoch:14 step:13246 [D loss: 0.662731, acc: 57.81%] [G loss: 1.877123]\n",
      "epoch:14 step:13247 [D loss: 0.653040, acc: 64.06%] [G loss: 1.743581]\n",
      "epoch:14 step:13248 [D loss: 0.636316, acc: 64.06%] [G loss: 1.976728]\n",
      "epoch:14 step:13249 [D loss: 0.620693, acc: 66.41%] [G loss: 2.169456]\n",
      "epoch:14 step:13250 [D loss: 0.630000, acc: 67.19%] [G loss: 1.918567]\n",
      "epoch:14 step:13251 [D loss: 0.756749, acc: 49.22%] [G loss: 1.826478]\n",
      "epoch:14 step:13252 [D loss: 0.645566, acc: 60.94%] [G loss: 1.898473]\n",
      "epoch:14 step:13253 [D loss: 0.602133, acc: 67.19%] [G loss: 1.889314]\n",
      "epoch:14 step:13254 [D loss: 0.641920, acc: 65.62%] [G loss: 1.883731]\n",
      "epoch:14 step:13255 [D loss: 0.684327, acc: 61.72%] [G loss: 1.935726]\n",
      "epoch:14 step:13256 [D loss: 0.670670, acc: 60.94%] [G loss: 1.872322]\n",
      "epoch:14 step:13257 [D loss: 0.691453, acc: 57.03%] [G loss: 1.893100]\n",
      "epoch:14 step:13258 [D loss: 0.624593, acc: 67.97%] [G loss: 1.883155]\n",
      "epoch:14 step:13259 [D loss: 0.650208, acc: 58.59%] [G loss: 1.900725]\n",
      "epoch:14 step:13260 [D loss: 0.603413, acc: 67.97%] [G loss: 1.919090]\n",
      "epoch:14 step:13261 [D loss: 0.680856, acc: 53.91%] [G loss: 1.941700]\n",
      "epoch:14 step:13262 [D loss: 0.671670, acc: 64.84%] [G loss: 2.105502]\n",
      "epoch:14 step:13263 [D loss: 0.648146, acc: 63.28%] [G loss: 1.907705]\n",
      "epoch:14 step:13264 [D loss: 0.593943, acc: 69.53%] [G loss: 1.990962]\n",
      "epoch:14 step:13265 [D loss: 0.662655, acc: 60.94%] [G loss: 1.987013]\n",
      "epoch:14 step:13266 [D loss: 0.669410, acc: 59.38%] [G loss: 1.808233]\n",
      "epoch:14 step:13267 [D loss: 0.666620, acc: 58.59%] [G loss: 1.932428]\n",
      "epoch:14 step:13268 [D loss: 0.622339, acc: 64.06%] [G loss: 1.959254]\n",
      "epoch:14 step:13269 [D loss: 0.664013, acc: 62.50%] [G loss: 2.037362]\n",
      "epoch:14 step:13270 [D loss: 0.653555, acc: 63.28%] [G loss: 1.930441]\n",
      "epoch:14 step:13271 [D loss: 0.604032, acc: 66.41%] [G loss: 1.875935]\n",
      "epoch:14 step:13272 [D loss: 0.651609, acc: 60.16%] [G loss: 1.836296]\n",
      "epoch:14 step:13273 [D loss: 0.663690, acc: 59.38%] [G loss: 1.975288]\n",
      "epoch:14 step:13274 [D loss: 0.731760, acc: 58.59%] [G loss: 2.073286]\n",
      "epoch:14 step:13275 [D loss: 0.651063, acc: 65.62%] [G loss: 1.970963]\n",
      "epoch:14 step:13276 [D loss: 0.650554, acc: 64.84%] [G loss: 1.859211]\n",
      "epoch:14 step:13277 [D loss: 0.620888, acc: 64.06%] [G loss: 1.990023]\n",
      "epoch:14 step:13278 [D loss: 0.677058, acc: 57.03%] [G loss: 1.914097]\n",
      "epoch:14 step:13279 [D loss: 0.648303, acc: 62.50%] [G loss: 2.079061]\n",
      "epoch:14 step:13280 [D loss: 0.611316, acc: 67.19%] [G loss: 2.049776]\n",
      "epoch:14 step:13281 [D loss: 0.646700, acc: 64.84%] [G loss: 1.954854]\n",
      "epoch:14 step:13282 [D loss: 0.630858, acc: 60.16%] [G loss: 1.880454]\n",
      "epoch:14 step:13283 [D loss: 0.673491, acc: 60.16%] [G loss: 1.798648]\n",
      "epoch:14 step:13284 [D loss: 0.604115, acc: 65.62%] [G loss: 1.770045]\n",
      "epoch:14 step:13285 [D loss: 0.675294, acc: 56.25%] [G loss: 1.958370]\n",
      "epoch:14 step:13286 [D loss: 0.651312, acc: 60.94%] [G loss: 1.922828]\n",
      "epoch:14 step:13287 [D loss: 0.627734, acc: 64.06%] [G loss: 1.927563]\n",
      "epoch:14 step:13288 [D loss: 0.653038, acc: 67.19%] [G loss: 1.882521]\n",
      "epoch:14 step:13289 [D loss: 0.613899, acc: 64.84%] [G loss: 2.000735]\n",
      "epoch:14 step:13290 [D loss: 0.624215, acc: 70.31%] [G loss: 1.842342]\n",
      "epoch:14 step:13291 [D loss: 0.613857, acc: 65.62%] [G loss: 1.875939]\n",
      "epoch:14 step:13292 [D loss: 0.658101, acc: 59.38%] [G loss: 1.838429]\n",
      "epoch:14 step:13293 [D loss: 0.694300, acc: 52.34%] [G loss: 1.987660]\n",
      "epoch:14 step:13294 [D loss: 0.626326, acc: 60.16%] [G loss: 1.885915]\n",
      "epoch:14 step:13295 [D loss: 0.613137, acc: 64.06%] [G loss: 1.829884]\n",
      "epoch:14 step:13296 [D loss: 0.652336, acc: 59.38%] [G loss: 1.972806]\n",
      "epoch:14 step:13297 [D loss: 0.670447, acc: 60.16%] [G loss: 1.834226]\n",
      "epoch:14 step:13298 [D loss: 0.634089, acc: 65.62%] [G loss: 1.916803]\n",
      "epoch:14 step:13299 [D loss: 0.674190, acc: 59.38%] [G loss: 1.968312]\n",
      "epoch:14 step:13300 [D loss: 0.686302, acc: 58.59%] [G loss: 1.752831]\n",
      "epoch:14 step:13301 [D loss: 0.600097, acc: 63.28%] [G loss: 1.804675]\n",
      "epoch:14 step:13302 [D loss: 0.590291, acc: 67.97%] [G loss: 1.896319]\n",
      "epoch:14 step:13303 [D loss: 0.641746, acc: 62.50%] [G loss: 2.033233]\n",
      "epoch:14 step:13304 [D loss: 0.659402, acc: 57.03%] [G loss: 1.918834]\n",
      "epoch:14 step:13305 [D loss: 0.674037, acc: 60.94%] [G loss: 1.901088]\n",
      "epoch:14 step:13306 [D loss: 0.658007, acc: 55.47%] [G loss: 1.969939]\n",
      "epoch:14 step:13307 [D loss: 0.635465, acc: 62.50%] [G loss: 1.788043]\n",
      "epoch:14 step:13308 [D loss: 0.642333, acc: 64.06%] [G loss: 1.841911]\n",
      "epoch:14 step:13309 [D loss: 0.681461, acc: 55.47%] [G loss: 1.817497]\n",
      "epoch:14 step:13310 [D loss: 0.637268, acc: 66.41%] [G loss: 1.962681]\n",
      "epoch:14 step:13311 [D loss: 0.641628, acc: 61.72%] [G loss: 1.991140]\n",
      "epoch:14 step:13312 [D loss: 0.563618, acc: 74.22%] [G loss: 2.130519]\n",
      "epoch:14 step:13313 [D loss: 0.652170, acc: 60.94%] [G loss: 1.950252]\n",
      "epoch:14 step:13314 [D loss: 0.684556, acc: 55.47%] [G loss: 1.944502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13315 [D loss: 0.635084, acc: 63.28%] [G loss: 1.925693]\n",
      "epoch:14 step:13316 [D loss: 0.641843, acc: 64.84%] [G loss: 2.178484]\n",
      "epoch:14 step:13317 [D loss: 0.602486, acc: 66.41%] [G loss: 2.037781]\n",
      "epoch:14 step:13318 [D loss: 0.748329, acc: 50.78%] [G loss: 1.815479]\n",
      "epoch:14 step:13319 [D loss: 0.655381, acc: 61.72%] [G loss: 1.759911]\n",
      "epoch:14 step:13320 [D loss: 0.648457, acc: 60.16%] [G loss: 1.731525]\n",
      "epoch:14 step:13321 [D loss: 0.643008, acc: 58.59%] [G loss: 1.913798]\n",
      "epoch:14 step:13322 [D loss: 0.605901, acc: 69.53%] [G loss: 1.949200]\n",
      "epoch:14 step:13323 [D loss: 0.651393, acc: 60.94%] [G loss: 1.942015]\n",
      "epoch:14 step:13324 [D loss: 0.601930, acc: 64.06%] [G loss: 1.948976]\n",
      "epoch:14 step:13325 [D loss: 0.574089, acc: 73.44%] [G loss: 2.274937]\n",
      "epoch:14 step:13326 [D loss: 0.670650, acc: 63.28%] [G loss: 2.252570]\n",
      "epoch:14 step:13327 [D loss: 0.571266, acc: 72.66%] [G loss: 2.117907]\n",
      "epoch:14 step:13328 [D loss: 0.693326, acc: 60.94%] [G loss: 1.985296]\n",
      "epoch:14 step:13329 [D loss: 0.635050, acc: 66.41%] [G loss: 1.832986]\n",
      "epoch:14 step:13330 [D loss: 0.697591, acc: 62.50%] [G loss: 1.862707]\n",
      "epoch:14 step:13331 [D loss: 0.617770, acc: 64.84%] [G loss: 1.922872]\n",
      "epoch:14 step:13332 [D loss: 0.668595, acc: 63.28%] [G loss: 1.732033]\n",
      "epoch:14 step:13333 [D loss: 0.658937, acc: 60.94%] [G loss: 1.962846]\n",
      "epoch:14 step:13334 [D loss: 0.628779, acc: 64.06%] [G loss: 1.989948]\n",
      "epoch:14 step:13335 [D loss: 0.562849, acc: 68.75%] [G loss: 2.105063]\n",
      "epoch:14 step:13336 [D loss: 0.637784, acc: 62.50%] [G loss: 2.066041]\n",
      "epoch:14 step:13337 [D loss: 0.589172, acc: 71.88%] [G loss: 2.140364]\n",
      "epoch:14 step:13338 [D loss: 0.708435, acc: 56.25%] [G loss: 1.856736]\n",
      "epoch:14 step:13339 [D loss: 0.621796, acc: 64.84%] [G loss: 2.007159]\n",
      "epoch:14 step:13340 [D loss: 0.642998, acc: 64.06%] [G loss: 1.974612]\n",
      "epoch:14 step:13341 [D loss: 0.591460, acc: 67.19%] [G loss: 2.071273]\n",
      "epoch:14 step:13342 [D loss: 0.596866, acc: 67.19%] [G loss: 1.941797]\n",
      "epoch:14 step:13343 [D loss: 0.715984, acc: 53.12%] [G loss: 1.957693]\n",
      "epoch:14 step:13344 [D loss: 0.630062, acc: 60.94%] [G loss: 1.858097]\n",
      "epoch:14 step:13345 [D loss: 0.603618, acc: 72.66%] [G loss: 1.850901]\n",
      "epoch:14 step:13346 [D loss: 0.650552, acc: 63.28%] [G loss: 1.814140]\n",
      "epoch:14 step:13347 [D loss: 0.664750, acc: 62.50%] [G loss: 2.081206]\n",
      "epoch:14 step:13348 [D loss: 0.569062, acc: 68.75%] [G loss: 2.171408]\n",
      "epoch:14 step:13349 [D loss: 0.570045, acc: 69.53%] [G loss: 2.425173]\n",
      "epoch:14 step:13350 [D loss: 0.550576, acc: 72.66%] [G loss: 2.353588]\n",
      "epoch:14 step:13351 [D loss: 0.613208, acc: 67.19%] [G loss: 1.917127]\n",
      "epoch:14 step:13352 [D loss: 0.603832, acc: 68.75%] [G loss: 1.947398]\n",
      "epoch:14 step:13353 [D loss: 0.643589, acc: 60.94%] [G loss: 1.926513]\n",
      "epoch:14 step:13354 [D loss: 0.594003, acc: 67.97%] [G loss: 1.934930]\n",
      "epoch:14 step:13355 [D loss: 0.653981, acc: 64.06%] [G loss: 1.988379]\n",
      "epoch:14 step:13356 [D loss: 0.608764, acc: 68.75%] [G loss: 1.931654]\n",
      "epoch:14 step:13357 [D loss: 0.676299, acc: 55.47%] [G loss: 2.141256]\n",
      "epoch:14 step:13358 [D loss: 0.716161, acc: 57.03%] [G loss: 1.930589]\n",
      "epoch:14 step:13359 [D loss: 0.629113, acc: 63.28%] [G loss: 2.053114]\n",
      "epoch:14 step:13360 [D loss: 0.664619, acc: 58.59%] [G loss: 2.013426]\n",
      "epoch:14 step:13361 [D loss: 0.600663, acc: 63.28%] [G loss: 1.988244]\n",
      "epoch:14 step:13362 [D loss: 0.587281, acc: 70.31%] [G loss: 2.021248]\n",
      "epoch:14 step:13363 [D loss: 0.677993, acc: 57.03%] [G loss: 2.066102]\n",
      "epoch:14 step:13364 [D loss: 0.637592, acc: 61.72%] [G loss: 1.936008]\n",
      "epoch:14 step:13365 [D loss: 0.629579, acc: 62.50%] [G loss: 1.949356]\n",
      "epoch:14 step:13366 [D loss: 0.614723, acc: 71.09%] [G loss: 2.020080]\n",
      "epoch:14 step:13367 [D loss: 0.692538, acc: 54.69%] [G loss: 1.923582]\n",
      "epoch:14 step:13368 [D loss: 0.727683, acc: 55.47%] [G loss: 1.704054]\n",
      "epoch:14 step:13369 [D loss: 0.693650, acc: 57.81%] [G loss: 1.826082]\n",
      "epoch:14 step:13370 [D loss: 0.668329, acc: 56.25%] [G loss: 1.807770]\n",
      "epoch:14 step:13371 [D loss: 0.623532, acc: 67.97%] [G loss: 1.938407]\n",
      "epoch:14 step:13372 [D loss: 0.657028, acc: 60.94%] [G loss: 1.682842]\n",
      "epoch:14 step:13373 [D loss: 0.707084, acc: 53.91%] [G loss: 1.830960]\n",
      "epoch:14 step:13374 [D loss: 0.625485, acc: 66.41%] [G loss: 1.732297]\n",
      "epoch:14 step:13375 [D loss: 0.693340, acc: 56.25%] [G loss: 1.848609]\n",
      "epoch:14 step:13376 [D loss: 0.666956, acc: 62.50%] [G loss: 1.886044]\n",
      "epoch:14 step:13377 [D loss: 0.677624, acc: 57.81%] [G loss: 1.790003]\n",
      "epoch:14 step:13378 [D loss: 0.589016, acc: 72.66%] [G loss: 1.946976]\n",
      "epoch:14 step:13379 [D loss: 0.758555, acc: 49.22%] [G loss: 1.917696]\n",
      "epoch:14 step:13380 [D loss: 0.653193, acc: 60.94%] [G loss: 1.902035]\n",
      "epoch:14 step:13381 [D loss: 0.658004, acc: 61.72%] [G loss: 1.814762]\n",
      "epoch:14 step:13382 [D loss: 0.600411, acc: 68.75%] [G loss: 2.022551]\n",
      "epoch:14 step:13383 [D loss: 0.683755, acc: 64.84%] [G loss: 1.711130]\n",
      "epoch:14 step:13384 [D loss: 0.592261, acc: 67.97%] [G loss: 2.000174]\n",
      "epoch:14 step:13385 [D loss: 0.674185, acc: 67.19%] [G loss: 1.863160]\n",
      "epoch:14 step:13386 [D loss: 0.643019, acc: 61.72%] [G loss: 1.925289]\n",
      "epoch:14 step:13387 [D loss: 0.632675, acc: 64.84%] [G loss: 1.890516]\n",
      "epoch:14 step:13388 [D loss: 0.614160, acc: 69.53%] [G loss: 1.934971]\n",
      "epoch:14 step:13389 [D loss: 0.622209, acc: 64.84%] [G loss: 1.995781]\n",
      "epoch:14 step:13390 [D loss: 0.642049, acc: 66.41%] [G loss: 1.999348]\n",
      "epoch:14 step:13391 [D loss: 0.658980, acc: 57.81%] [G loss: 1.966814]\n",
      "epoch:14 step:13392 [D loss: 0.595297, acc: 69.53%] [G loss: 2.099063]\n",
      "epoch:14 step:13393 [D loss: 0.628388, acc: 64.06%] [G loss: 2.063941]\n",
      "epoch:14 step:13394 [D loss: 0.606643, acc: 67.97%] [G loss: 2.269460]\n",
      "epoch:14 step:13395 [D loss: 0.680998, acc: 60.16%] [G loss: 1.940792]\n",
      "epoch:14 step:13396 [D loss: 0.652426, acc: 67.19%] [G loss: 2.019481]\n",
      "epoch:14 step:13397 [D loss: 0.620280, acc: 67.19%] [G loss: 1.910495]\n",
      "epoch:14 step:13398 [D loss: 0.604620, acc: 67.19%] [G loss: 2.018164]\n",
      "epoch:14 step:13399 [D loss: 0.681747, acc: 60.16%] [G loss: 1.865818]\n",
      "epoch:14 step:13400 [D loss: 0.654404, acc: 60.16%] [G loss: 1.906100]\n",
      "##############\n",
      "[2.40092517 1.48436317 6.30693646 4.73121007 3.65394381 5.8231355\n",
      " 4.30208047 4.63279096 4.70253764 3.44508275]\n",
      "##########\n",
      "epoch:14 step:13401 [D loss: 0.549338, acc: 71.09%] [G loss: 1.964758]\n",
      "epoch:14 step:13402 [D loss: 0.580231, acc: 71.88%] [G loss: 1.931040]\n",
      "epoch:14 step:13403 [D loss: 0.677155, acc: 60.16%] [G loss: 1.868854]\n",
      "epoch:14 step:13404 [D loss: 0.644512, acc: 63.28%] [G loss: 2.005563]\n",
      "epoch:14 step:13405 [D loss: 0.681511, acc: 61.72%] [G loss: 1.987935]\n",
      "epoch:14 step:13406 [D loss: 0.621125, acc: 68.75%] [G loss: 1.917641]\n",
      "epoch:14 step:13407 [D loss: 0.631117, acc: 60.94%] [G loss: 1.931758]\n",
      "epoch:14 step:13408 [D loss: 0.724398, acc: 47.66%] [G loss: 1.852616]\n",
      "epoch:14 step:13409 [D loss: 0.678201, acc: 59.38%] [G loss: 1.919261]\n",
      "epoch:14 step:13410 [D loss: 0.622581, acc: 64.06%] [G loss: 1.799708]\n",
      "epoch:14 step:13411 [D loss: 0.618203, acc: 67.97%] [G loss: 2.037200]\n",
      "epoch:14 step:13412 [D loss: 0.670985, acc: 57.81%] [G loss: 1.900675]\n",
      "epoch:14 step:13413 [D loss: 0.629579, acc: 66.41%] [G loss: 1.985382]\n",
      "epoch:14 step:13414 [D loss: 0.630141, acc: 66.41%] [G loss: 2.075939]\n",
      "epoch:14 step:13415 [D loss: 0.666815, acc: 55.47%] [G loss: 1.836735]\n",
      "epoch:14 step:13416 [D loss: 0.616964, acc: 68.75%] [G loss: 2.202410]\n",
      "epoch:14 step:13417 [D loss: 0.588254, acc: 66.41%] [G loss: 1.960050]\n",
      "epoch:14 step:13418 [D loss: 0.642139, acc: 59.38%] [G loss: 2.146664]\n",
      "epoch:14 step:13419 [D loss: 0.680505, acc: 53.91%] [G loss: 1.839547]\n",
      "epoch:14 step:13420 [D loss: 0.682018, acc: 59.38%] [G loss: 1.981634]\n",
      "epoch:14 step:13421 [D loss: 0.654818, acc: 57.03%] [G loss: 1.991257]\n",
      "epoch:14 step:13422 [D loss: 0.687747, acc: 53.91%] [G loss: 1.883617]\n",
      "epoch:14 step:13423 [D loss: 0.622184, acc: 64.84%] [G loss: 1.820794]\n",
      "epoch:14 step:13424 [D loss: 0.683904, acc: 57.81%] [G loss: 1.830698]\n",
      "epoch:14 step:13425 [D loss: 0.608549, acc: 65.62%] [G loss: 1.833985]\n",
      "epoch:14 step:13426 [D loss: 0.645143, acc: 60.94%] [G loss: 1.876127]\n",
      "epoch:14 step:13427 [D loss: 0.605471, acc: 69.53%] [G loss: 2.028719]\n",
      "epoch:14 step:13428 [D loss: 0.617366, acc: 66.41%] [G loss: 1.930892]\n",
      "epoch:14 step:13429 [D loss: 0.588012, acc: 71.88%] [G loss: 1.909572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13430 [D loss: 0.572455, acc: 72.66%] [G loss: 2.126246]\n",
      "epoch:14 step:13431 [D loss: 0.610069, acc: 71.09%] [G loss: 2.097042]\n",
      "epoch:14 step:13432 [D loss: 0.572243, acc: 65.62%] [G loss: 2.305783]\n",
      "epoch:14 step:13433 [D loss: 0.627643, acc: 64.06%] [G loss: 2.465199]\n",
      "epoch:14 step:13434 [D loss: 0.708544, acc: 55.47%] [G loss: 1.834059]\n",
      "epoch:14 step:13435 [D loss: 0.679114, acc: 58.59%] [G loss: 1.701797]\n",
      "epoch:14 step:13436 [D loss: 0.667189, acc: 59.38%] [G loss: 2.026466]\n",
      "epoch:14 step:13437 [D loss: 0.649782, acc: 60.16%] [G loss: 1.778757]\n",
      "epoch:14 step:13438 [D loss: 0.640995, acc: 60.94%] [G loss: 1.934528]\n",
      "epoch:14 step:13439 [D loss: 0.595131, acc: 71.09%] [G loss: 1.953824]\n",
      "epoch:14 step:13440 [D loss: 0.682955, acc: 57.03%] [G loss: 1.927216]\n",
      "epoch:14 step:13441 [D loss: 0.672554, acc: 61.72%] [G loss: 1.889247]\n",
      "epoch:14 step:13442 [D loss: 0.620379, acc: 64.84%] [G loss: 1.712654]\n",
      "epoch:14 step:13443 [D loss: 0.641157, acc: 63.28%] [G loss: 1.995850]\n",
      "epoch:14 step:13444 [D loss: 0.668818, acc: 64.84%] [G loss: 1.935248]\n",
      "epoch:14 step:13445 [D loss: 0.642296, acc: 59.38%] [G loss: 1.937721]\n",
      "epoch:14 step:13446 [D loss: 0.639588, acc: 56.25%] [G loss: 2.045088]\n",
      "epoch:14 step:13447 [D loss: 0.627825, acc: 64.06%] [G loss: 1.869332]\n",
      "epoch:14 step:13448 [D loss: 0.641939, acc: 61.72%] [G loss: 1.966747]\n",
      "epoch:14 step:13449 [D loss: 0.676037, acc: 57.03%] [G loss: 1.898611]\n",
      "epoch:14 step:13450 [D loss: 0.617037, acc: 60.94%] [G loss: 2.125373]\n",
      "epoch:14 step:13451 [D loss: 0.597587, acc: 69.53%] [G loss: 2.176360]\n",
      "epoch:14 step:13452 [D loss: 0.659746, acc: 62.50%] [G loss: 2.275140]\n",
      "epoch:14 step:13453 [D loss: 0.606490, acc: 69.53%] [G loss: 2.076775]\n",
      "epoch:14 step:13454 [D loss: 0.630942, acc: 64.84%] [G loss: 1.929149]\n",
      "epoch:14 step:13455 [D loss: 0.613607, acc: 66.41%] [G loss: 1.926555]\n",
      "epoch:14 step:13456 [D loss: 0.587317, acc: 71.09%] [G loss: 2.160084]\n",
      "epoch:14 step:13457 [D loss: 0.576528, acc: 67.97%] [G loss: 2.013305]\n",
      "epoch:14 step:13458 [D loss: 0.627477, acc: 64.84%] [G loss: 2.093234]\n",
      "epoch:14 step:13459 [D loss: 0.712274, acc: 58.59%] [G loss: 1.813706]\n",
      "epoch:14 step:13460 [D loss: 0.688173, acc: 56.25%] [G loss: 1.836076]\n",
      "epoch:14 step:13461 [D loss: 0.663862, acc: 59.38%] [G loss: 2.030389]\n",
      "epoch:14 step:13462 [D loss: 0.633859, acc: 61.72%] [G loss: 1.924980]\n",
      "epoch:14 step:13463 [D loss: 0.608564, acc: 69.53%] [G loss: 2.369204]\n",
      "epoch:14 step:13464 [D loss: 0.578104, acc: 69.53%] [G loss: 2.228378]\n",
      "epoch:14 step:13465 [D loss: 0.589488, acc: 67.97%] [G loss: 2.516765]\n",
      "epoch:14 step:13466 [D loss: 0.637387, acc: 60.94%] [G loss: 1.848472]\n",
      "epoch:14 step:13467 [D loss: 0.732117, acc: 49.22%] [G loss: 1.743962]\n",
      "epoch:14 step:13468 [D loss: 0.650757, acc: 64.06%] [G loss: 1.932387]\n",
      "epoch:14 step:13469 [D loss: 0.663485, acc: 63.28%] [G loss: 1.903687]\n",
      "epoch:14 step:13470 [D loss: 0.662235, acc: 57.03%] [G loss: 1.772860]\n",
      "epoch:14 step:13471 [D loss: 0.617091, acc: 69.53%] [G loss: 1.919062]\n",
      "epoch:14 step:13472 [D loss: 0.598725, acc: 67.19%] [G loss: 2.064440]\n",
      "epoch:14 step:13473 [D loss: 0.662608, acc: 60.94%] [G loss: 1.936606]\n",
      "epoch:14 step:13474 [D loss: 0.658646, acc: 59.38%] [G loss: 1.786140]\n",
      "epoch:14 step:13475 [D loss: 0.635199, acc: 60.94%] [G loss: 1.980036]\n",
      "epoch:14 step:13476 [D loss: 0.580749, acc: 68.75%] [G loss: 1.842639]\n",
      "epoch:14 step:13477 [D loss: 0.599462, acc: 67.19%] [G loss: 2.013688]\n",
      "epoch:14 step:13478 [D loss: 0.633731, acc: 64.84%] [G loss: 2.015787]\n",
      "epoch:14 step:13479 [D loss: 0.648967, acc: 69.53%] [G loss: 1.975713]\n",
      "epoch:14 step:13480 [D loss: 0.681341, acc: 63.28%] [G loss: 1.881449]\n",
      "epoch:14 step:13481 [D loss: 0.626267, acc: 63.28%] [G loss: 1.947502]\n",
      "epoch:14 step:13482 [D loss: 0.666262, acc: 63.28%] [G loss: 1.988224]\n",
      "epoch:14 step:13483 [D loss: 0.594150, acc: 64.84%] [G loss: 2.037499]\n",
      "epoch:14 step:13484 [D loss: 0.608016, acc: 68.75%] [G loss: 2.006676]\n",
      "epoch:14 step:13485 [D loss: 0.652310, acc: 60.16%] [G loss: 2.103160]\n",
      "epoch:14 step:13486 [D loss: 0.660955, acc: 62.50%] [G loss: 2.071471]\n",
      "epoch:14 step:13487 [D loss: 0.700086, acc: 57.81%] [G loss: 1.983541]\n",
      "epoch:14 step:13488 [D loss: 0.622477, acc: 62.50%] [G loss: 1.959851]\n",
      "epoch:14 step:13489 [D loss: 0.616548, acc: 67.97%] [G loss: 2.073308]\n",
      "epoch:14 step:13490 [D loss: 0.646099, acc: 58.59%] [G loss: 1.935268]\n",
      "epoch:14 step:13491 [D loss: 0.617549, acc: 67.19%] [G loss: 1.899564]\n",
      "epoch:14 step:13492 [D loss: 0.628243, acc: 68.75%] [G loss: 1.932349]\n",
      "epoch:14 step:13493 [D loss: 0.617970, acc: 68.75%] [G loss: 2.071608]\n",
      "epoch:14 step:13494 [D loss: 0.625769, acc: 67.97%] [G loss: 1.890877]\n",
      "epoch:14 step:13495 [D loss: 0.650120, acc: 59.38%] [G loss: 1.800926]\n",
      "epoch:14 step:13496 [D loss: 0.608010, acc: 68.75%] [G loss: 1.826450]\n",
      "epoch:14 step:13497 [D loss: 0.645350, acc: 60.16%] [G loss: 1.898694]\n",
      "epoch:14 step:13498 [D loss: 0.645564, acc: 61.72%] [G loss: 2.039246]\n",
      "epoch:14 step:13499 [D loss: 0.599056, acc: 69.53%] [G loss: 2.266957]\n",
      "epoch:14 step:13500 [D loss: 0.682189, acc: 56.25%] [G loss: 2.056792]\n",
      "epoch:14 step:13501 [D loss: 0.635229, acc: 66.41%] [G loss: 1.922945]\n",
      "epoch:14 step:13502 [D loss: 0.626858, acc: 64.84%] [G loss: 2.028312]\n",
      "epoch:14 step:13503 [D loss: 0.659485, acc: 60.16%] [G loss: 2.024810]\n",
      "epoch:14 step:13504 [D loss: 0.719203, acc: 55.47%] [G loss: 1.862840]\n",
      "epoch:14 step:13505 [D loss: 0.679913, acc: 58.59%] [G loss: 1.808913]\n",
      "epoch:14 step:13506 [D loss: 0.614121, acc: 65.62%] [G loss: 1.867223]\n",
      "epoch:14 step:13507 [D loss: 0.645155, acc: 62.50%] [G loss: 2.026178]\n",
      "epoch:14 step:13508 [D loss: 0.637494, acc: 64.84%] [G loss: 2.038622]\n",
      "epoch:14 step:13509 [D loss: 0.669975, acc: 57.81%] [G loss: 1.816370]\n",
      "epoch:14 step:13510 [D loss: 0.636550, acc: 64.84%] [G loss: 1.990004]\n",
      "epoch:14 step:13511 [D loss: 0.617243, acc: 64.84%] [G loss: 1.870565]\n",
      "epoch:14 step:13512 [D loss: 0.629765, acc: 63.28%] [G loss: 1.958620]\n",
      "epoch:14 step:13513 [D loss: 0.614028, acc: 61.72%] [G loss: 2.060606]\n",
      "epoch:14 step:13514 [D loss: 0.680737, acc: 56.25%] [G loss: 1.893085]\n",
      "epoch:14 step:13515 [D loss: 0.675435, acc: 60.94%] [G loss: 1.849918]\n",
      "epoch:14 step:13516 [D loss: 0.612951, acc: 68.75%] [G loss: 2.008214]\n",
      "epoch:14 step:13517 [D loss: 0.642714, acc: 62.50%] [G loss: 1.940640]\n",
      "epoch:14 step:13518 [D loss: 0.645960, acc: 63.28%] [G loss: 1.993938]\n",
      "epoch:14 step:13519 [D loss: 0.613459, acc: 67.97%] [G loss: 1.968887]\n",
      "epoch:14 step:13520 [D loss: 0.648450, acc: 66.41%] [G loss: 1.986817]\n",
      "epoch:14 step:13521 [D loss: 0.643116, acc: 61.72%] [G loss: 2.061645]\n",
      "epoch:14 step:13522 [D loss: 0.633680, acc: 64.06%] [G loss: 1.986415]\n",
      "epoch:14 step:13523 [D loss: 0.615908, acc: 60.16%] [G loss: 2.056256]\n",
      "epoch:14 step:13524 [D loss: 0.643433, acc: 60.94%] [G loss: 2.004414]\n",
      "epoch:14 step:13525 [D loss: 0.670792, acc: 58.59%] [G loss: 1.965208]\n",
      "epoch:14 step:13526 [D loss: 0.719155, acc: 58.59%] [G loss: 1.777974]\n",
      "epoch:14 step:13527 [D loss: 0.603843, acc: 64.84%] [G loss: 1.913458]\n",
      "epoch:14 step:13528 [D loss: 0.609906, acc: 67.19%] [G loss: 1.889922]\n",
      "epoch:14 step:13529 [D loss: 0.717501, acc: 52.34%] [G loss: 1.850398]\n",
      "epoch:14 step:13530 [D loss: 0.615614, acc: 67.19%] [G loss: 1.942985]\n",
      "epoch:14 step:13531 [D loss: 0.676836, acc: 59.38%] [G loss: 2.038885]\n",
      "epoch:14 step:13532 [D loss: 0.626497, acc: 60.94%] [G loss: 1.869717]\n",
      "epoch:14 step:13533 [D loss: 0.651534, acc: 61.72%] [G loss: 1.991600]\n",
      "epoch:14 step:13534 [D loss: 0.639729, acc: 64.84%] [G loss: 1.990759]\n",
      "epoch:14 step:13535 [D loss: 0.641827, acc: 64.06%] [G loss: 1.876729]\n",
      "epoch:14 step:13536 [D loss: 0.707889, acc: 54.69%] [G loss: 1.789312]\n",
      "epoch:14 step:13537 [D loss: 0.647750, acc: 64.06%] [G loss: 1.886998]\n",
      "epoch:14 step:13538 [D loss: 0.652504, acc: 57.81%] [G loss: 1.839622]\n",
      "epoch:14 step:13539 [D loss: 0.632297, acc: 65.62%] [G loss: 1.927389]\n",
      "epoch:14 step:13540 [D loss: 0.676802, acc: 60.94%] [G loss: 1.924015]\n",
      "epoch:14 step:13541 [D loss: 0.659285, acc: 58.59%] [G loss: 1.894235]\n",
      "epoch:14 step:13542 [D loss: 0.666265, acc: 59.38%] [G loss: 1.865757]\n",
      "epoch:14 step:13543 [D loss: 0.688380, acc: 59.38%] [G loss: 2.047371]\n",
      "epoch:14 step:13544 [D loss: 0.629830, acc: 57.81%] [G loss: 1.887686]\n",
      "epoch:14 step:13545 [D loss: 0.634400, acc: 67.97%] [G loss: 1.959459]\n",
      "epoch:14 step:13546 [D loss: 0.572032, acc: 65.62%] [G loss: 2.158268]\n",
      "epoch:14 step:13547 [D loss: 0.612907, acc: 67.97%] [G loss: 2.275819]\n",
      "epoch:14 step:13548 [D loss: 0.542319, acc: 71.09%] [G loss: 2.240851]\n",
      "epoch:14 step:13549 [D loss: 0.595187, acc: 71.09%] [G loss: 2.044762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13550 [D loss: 0.661999, acc: 60.94%] [G loss: 1.918895]\n",
      "epoch:14 step:13551 [D loss: 0.625489, acc: 67.19%] [G loss: 1.937817]\n",
      "epoch:14 step:13552 [D loss: 0.578317, acc: 75.00%] [G loss: 2.181951]\n",
      "epoch:14 step:13553 [D loss: 0.630113, acc: 64.84%] [G loss: 2.009947]\n",
      "epoch:14 step:13554 [D loss: 0.617789, acc: 70.31%] [G loss: 2.109807]\n",
      "epoch:14 step:13555 [D loss: 0.746651, acc: 50.00%] [G loss: 1.756022]\n",
      "epoch:14 step:13556 [D loss: 0.648175, acc: 57.03%] [G loss: 1.850233]\n",
      "epoch:14 step:13557 [D loss: 0.670365, acc: 60.16%] [G loss: 1.828166]\n",
      "epoch:14 step:13558 [D loss: 0.646887, acc: 64.06%] [G loss: 1.838114]\n",
      "epoch:14 step:13559 [D loss: 0.671260, acc: 61.72%] [G loss: 1.854184]\n",
      "epoch:14 step:13560 [D loss: 0.656523, acc: 64.06%] [G loss: 1.882261]\n",
      "epoch:14 step:13561 [D loss: 0.624559, acc: 66.41%] [G loss: 1.968164]\n",
      "epoch:14 step:13562 [D loss: 0.679126, acc: 60.16%] [G loss: 1.785968]\n",
      "epoch:14 step:13563 [D loss: 0.598883, acc: 68.75%] [G loss: 1.972701]\n",
      "epoch:14 step:13564 [D loss: 0.647955, acc: 64.84%] [G loss: 1.944099]\n",
      "epoch:14 step:13565 [D loss: 0.604486, acc: 68.75%] [G loss: 2.089104]\n",
      "epoch:14 step:13566 [D loss: 0.670417, acc: 53.91%] [G loss: 1.796320]\n",
      "epoch:14 step:13567 [D loss: 0.680210, acc: 58.59%] [G loss: 1.896734]\n",
      "epoch:14 step:13568 [D loss: 0.627030, acc: 64.84%] [G loss: 1.774348]\n",
      "epoch:14 step:13569 [D loss: 0.622631, acc: 60.94%] [G loss: 1.975140]\n",
      "epoch:14 step:13570 [D loss: 0.632553, acc: 62.50%] [G loss: 1.993872]\n",
      "epoch:14 step:13571 [D loss: 0.653918, acc: 65.62%] [G loss: 1.988632]\n",
      "epoch:14 step:13572 [D loss: 0.690349, acc: 57.03%] [G loss: 1.961916]\n",
      "epoch:14 step:13573 [D loss: 0.649613, acc: 63.28%] [G loss: 1.896165]\n",
      "epoch:14 step:13574 [D loss: 0.637311, acc: 65.62%] [G loss: 2.094963]\n",
      "epoch:14 step:13575 [D loss: 0.623472, acc: 67.97%] [G loss: 2.061172]\n",
      "epoch:14 step:13576 [D loss: 0.594047, acc: 66.41%] [G loss: 1.843315]\n",
      "epoch:14 step:13577 [D loss: 0.663762, acc: 58.59%] [G loss: 1.866901]\n",
      "epoch:14 step:13578 [D loss: 0.620473, acc: 67.19%] [G loss: 1.909404]\n",
      "epoch:14 step:13579 [D loss: 0.665046, acc: 60.94%] [G loss: 2.103526]\n",
      "epoch:14 step:13580 [D loss: 0.660702, acc: 58.59%] [G loss: 1.965601]\n",
      "epoch:14 step:13581 [D loss: 0.630535, acc: 60.16%] [G loss: 1.783153]\n",
      "epoch:14 step:13582 [D loss: 0.651112, acc: 63.28%] [G loss: 1.885677]\n",
      "epoch:14 step:13583 [D loss: 0.613818, acc: 68.75%] [G loss: 1.979481]\n",
      "epoch:14 step:13584 [D loss: 0.665436, acc: 59.38%] [G loss: 2.045259]\n",
      "epoch:14 step:13585 [D loss: 0.621068, acc: 66.41%] [G loss: 2.000836]\n",
      "epoch:14 step:13586 [D loss: 0.644901, acc: 65.62%] [G loss: 2.020348]\n",
      "epoch:14 step:13587 [D loss: 0.657886, acc: 64.84%] [G loss: 2.036267]\n",
      "epoch:14 step:13588 [D loss: 0.584333, acc: 75.78%] [G loss: 2.075363]\n",
      "epoch:14 step:13589 [D loss: 0.575346, acc: 68.75%] [G loss: 2.300582]\n",
      "epoch:14 step:13590 [D loss: 0.682589, acc: 59.38%] [G loss: 2.298660]\n",
      "epoch:14 step:13591 [D loss: 0.644020, acc: 65.62%] [G loss: 1.841266]\n",
      "epoch:14 step:13592 [D loss: 0.629098, acc: 64.06%] [G loss: 1.962848]\n",
      "epoch:14 step:13593 [D loss: 0.678129, acc: 61.72%] [G loss: 1.994883]\n",
      "epoch:14 step:13594 [D loss: 0.671324, acc: 62.50%] [G loss: 2.086419]\n",
      "epoch:14 step:13595 [D loss: 0.657900, acc: 61.72%] [G loss: 1.917461]\n",
      "epoch:14 step:13596 [D loss: 0.663327, acc: 62.50%] [G loss: 1.880436]\n",
      "epoch:14 step:13597 [D loss: 0.612333, acc: 70.31%] [G loss: 1.987034]\n",
      "epoch:14 step:13598 [D loss: 0.636519, acc: 62.50%] [G loss: 1.984944]\n",
      "epoch:14 step:13599 [D loss: 0.570747, acc: 69.53%] [G loss: 2.249944]\n",
      "epoch:14 step:13600 [D loss: 0.706968, acc: 55.47%] [G loss: 1.695825]\n",
      "##############\n",
      "[2.5094507  1.61721055 6.19623947 4.78502784 3.87076611 5.72988013\n",
      " 4.48455211 4.7739015  4.64103643 3.89379561]\n",
      "##########\n",
      "epoch:14 step:13601 [D loss: 0.670428, acc: 60.16%] [G loss: 1.738503]\n",
      "epoch:14 step:13602 [D loss: 0.623752, acc: 61.72%] [G loss: 1.998007]\n",
      "epoch:14 step:13603 [D loss: 0.637607, acc: 67.97%] [G loss: 1.820037]\n",
      "epoch:14 step:13604 [D loss: 0.633946, acc: 65.62%] [G loss: 1.907893]\n",
      "epoch:14 step:13605 [D loss: 0.658268, acc: 64.06%] [G loss: 1.950583]\n",
      "epoch:14 step:13606 [D loss: 0.607016, acc: 71.09%] [G loss: 2.014653]\n",
      "epoch:14 step:13607 [D loss: 0.733934, acc: 57.81%] [G loss: 1.824419]\n",
      "epoch:14 step:13608 [D loss: 0.678015, acc: 59.38%] [G loss: 1.823663]\n",
      "epoch:14 step:13609 [D loss: 0.605741, acc: 65.62%] [G loss: 1.883747]\n",
      "epoch:14 step:13610 [D loss: 0.645425, acc: 59.38%] [G loss: 1.986326]\n",
      "epoch:14 step:13611 [D loss: 0.620770, acc: 64.84%] [G loss: 1.937905]\n",
      "epoch:14 step:13612 [D loss: 0.611361, acc: 67.19%] [G loss: 2.298536]\n",
      "epoch:14 step:13613 [D loss: 0.611268, acc: 68.75%] [G loss: 2.109520]\n",
      "epoch:14 step:13614 [D loss: 0.622393, acc: 65.62%] [G loss: 2.023370]\n",
      "epoch:14 step:13615 [D loss: 0.625425, acc: 67.19%] [G loss: 2.167739]\n",
      "epoch:14 step:13616 [D loss: 0.632332, acc: 59.38%] [G loss: 2.124485]\n",
      "epoch:14 step:13617 [D loss: 0.632707, acc: 65.62%] [G loss: 2.074350]\n",
      "epoch:14 step:13618 [D loss: 0.663203, acc: 66.41%] [G loss: 1.799184]\n",
      "epoch:14 step:13619 [D loss: 0.668139, acc: 60.94%] [G loss: 1.844501]\n",
      "epoch:14 step:13620 [D loss: 0.688586, acc: 60.16%] [G loss: 1.781615]\n",
      "epoch:14 step:13621 [D loss: 0.664884, acc: 60.16%] [G loss: 1.964232]\n",
      "epoch:14 step:13622 [D loss: 0.620637, acc: 71.09%] [G loss: 2.094867]\n",
      "epoch:14 step:13623 [D loss: 0.604682, acc: 66.41%] [G loss: 1.883369]\n",
      "epoch:14 step:13624 [D loss: 0.655783, acc: 60.94%] [G loss: 1.837268]\n",
      "epoch:14 step:13625 [D loss: 0.664714, acc: 59.38%] [G loss: 2.034143]\n",
      "epoch:14 step:13626 [D loss: 0.649829, acc: 61.72%] [G loss: 2.011800]\n",
      "epoch:14 step:13627 [D loss: 0.624039, acc: 65.62%] [G loss: 1.930550]\n",
      "epoch:14 step:13628 [D loss: 0.631418, acc: 62.50%] [G loss: 1.902341]\n",
      "epoch:14 step:13629 [D loss: 0.692214, acc: 55.47%] [G loss: 1.868116]\n",
      "epoch:14 step:13630 [D loss: 0.648047, acc: 66.41%] [G loss: 1.924361]\n",
      "epoch:14 step:13631 [D loss: 0.656989, acc: 64.84%] [G loss: 1.937892]\n",
      "epoch:14 step:13632 [D loss: 0.642454, acc: 67.19%] [G loss: 1.928824]\n",
      "epoch:14 step:13633 [D loss: 0.680002, acc: 58.59%] [G loss: 1.986004]\n",
      "epoch:14 step:13634 [D loss: 0.591049, acc: 71.09%] [G loss: 2.185102]\n",
      "epoch:14 step:13635 [D loss: 0.616322, acc: 66.41%] [G loss: 2.003881]\n",
      "epoch:14 step:13636 [D loss: 0.637508, acc: 63.28%] [G loss: 2.002969]\n",
      "epoch:14 step:13637 [D loss: 0.657042, acc: 57.03%] [G loss: 1.974387]\n",
      "epoch:14 step:13638 [D loss: 0.624134, acc: 67.97%] [G loss: 2.035063]\n",
      "epoch:14 step:13639 [D loss: 0.612752, acc: 64.06%] [G loss: 1.888120]\n",
      "epoch:14 step:13640 [D loss: 0.593748, acc: 65.62%] [G loss: 2.041980]\n",
      "epoch:14 step:13641 [D loss: 0.597280, acc: 67.97%] [G loss: 1.980973]\n",
      "epoch:14 step:13642 [D loss: 0.646374, acc: 62.50%] [G loss: 1.917778]\n",
      "epoch:14 step:13643 [D loss: 0.646586, acc: 67.19%] [G loss: 1.973476]\n",
      "epoch:14 step:13644 [D loss: 0.670174, acc: 60.16%] [G loss: 1.758895]\n",
      "epoch:14 step:13645 [D loss: 0.645848, acc: 62.50%] [G loss: 2.000634]\n",
      "epoch:14 step:13646 [D loss: 0.720809, acc: 50.78%] [G loss: 1.841397]\n",
      "epoch:14 step:13647 [D loss: 0.713409, acc: 63.28%] [G loss: 1.793621]\n",
      "epoch:14 step:13648 [D loss: 0.621525, acc: 64.84%] [G loss: 1.723688]\n",
      "epoch:14 step:13649 [D loss: 0.626058, acc: 59.38%] [G loss: 1.872419]\n",
      "epoch:14 step:13650 [D loss: 0.618110, acc: 64.06%] [G loss: 1.902005]\n",
      "epoch:14 step:13651 [D loss: 0.646785, acc: 60.94%] [G loss: 1.943806]\n",
      "epoch:14 step:13652 [D loss: 0.592939, acc: 72.66%] [G loss: 2.178830]\n",
      "epoch:14 step:13653 [D loss: 0.642318, acc: 62.50%] [G loss: 1.851104]\n",
      "epoch:14 step:13654 [D loss: 0.629430, acc: 71.88%] [G loss: 2.040897]\n",
      "epoch:14 step:13655 [D loss: 0.614676, acc: 61.72%] [G loss: 1.829737]\n",
      "epoch:14 step:13656 [D loss: 0.624854, acc: 66.41%] [G loss: 1.913789]\n",
      "epoch:14 step:13657 [D loss: 0.654104, acc: 62.50%] [G loss: 1.875548]\n",
      "epoch:14 step:13658 [D loss: 0.620785, acc: 64.06%] [G loss: 2.055572]\n",
      "epoch:14 step:13659 [D loss: 0.601729, acc: 64.84%] [G loss: 1.952588]\n",
      "epoch:14 step:13660 [D loss: 0.645269, acc: 62.50%] [G loss: 1.889231]\n",
      "epoch:14 step:13661 [D loss: 0.594909, acc: 67.19%] [G loss: 1.888461]\n",
      "epoch:14 step:13662 [D loss: 0.636878, acc: 63.28%] [G loss: 1.945345]\n",
      "epoch:14 step:13663 [D loss: 0.681033, acc: 60.16%] [G loss: 1.949571]\n",
      "epoch:14 step:13664 [D loss: 0.660001, acc: 64.06%] [G loss: 2.018621]\n",
      "epoch:14 step:13665 [D loss: 0.611465, acc: 68.75%] [G loss: 1.928469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13666 [D loss: 0.660042, acc: 62.50%] [G loss: 2.062131]\n",
      "epoch:14 step:13667 [D loss: 0.627067, acc: 67.97%] [G loss: 2.209517]\n",
      "epoch:14 step:13668 [D loss: 0.669011, acc: 62.50%] [G loss: 2.003428]\n",
      "epoch:14 step:13669 [D loss: 0.627610, acc: 60.94%] [G loss: 2.012715]\n",
      "epoch:14 step:13670 [D loss: 0.592757, acc: 69.53%] [G loss: 2.067333]\n",
      "epoch:14 step:13671 [D loss: 0.682566, acc: 60.16%] [G loss: 1.866192]\n",
      "epoch:14 step:13672 [D loss: 0.600286, acc: 60.94%] [G loss: 2.034760]\n",
      "epoch:14 step:13673 [D loss: 0.636873, acc: 68.75%] [G loss: 1.962681]\n",
      "epoch:14 step:13674 [D loss: 0.636554, acc: 64.06%] [G loss: 2.110382]\n",
      "epoch:14 step:13675 [D loss: 0.641786, acc: 61.72%] [G loss: 2.155885]\n",
      "epoch:14 step:13676 [D loss: 0.603416, acc: 64.84%] [G loss: 2.102071]\n",
      "epoch:14 step:13677 [D loss: 0.746270, acc: 52.34%] [G loss: 1.919479]\n",
      "epoch:14 step:13678 [D loss: 0.599588, acc: 68.75%] [G loss: 1.958447]\n",
      "epoch:14 step:13679 [D loss: 0.657204, acc: 61.72%] [G loss: 1.936788]\n",
      "epoch:14 step:13680 [D loss: 0.663857, acc: 64.06%] [G loss: 1.994528]\n",
      "epoch:14 step:13681 [D loss: 0.651383, acc: 63.28%] [G loss: 1.881480]\n",
      "epoch:14 step:13682 [D loss: 0.628810, acc: 57.81%] [G loss: 2.235982]\n",
      "epoch:14 step:13683 [D loss: 0.655527, acc: 61.72%] [G loss: 1.904876]\n",
      "epoch:14 step:13684 [D loss: 0.748116, acc: 52.34%] [G loss: 1.820691]\n",
      "epoch:14 step:13685 [D loss: 0.649945, acc: 56.25%] [G loss: 1.846086]\n",
      "epoch:14 step:13686 [D loss: 0.637463, acc: 61.72%] [G loss: 1.827524]\n",
      "epoch:14 step:13687 [D loss: 0.644578, acc: 61.72%] [G loss: 1.948714]\n",
      "epoch:14 step:13688 [D loss: 0.653305, acc: 64.06%] [G loss: 1.889282]\n",
      "epoch:14 step:13689 [D loss: 0.647847, acc: 66.41%] [G loss: 1.999172]\n",
      "epoch:14 step:13690 [D loss: 0.632006, acc: 67.97%] [G loss: 1.713251]\n",
      "epoch:14 step:13691 [D loss: 0.661056, acc: 54.69%] [G loss: 1.719965]\n",
      "epoch:14 step:13692 [D loss: 0.664068, acc: 59.38%] [G loss: 1.841778]\n",
      "epoch:14 step:13693 [D loss: 0.646247, acc: 62.50%] [G loss: 1.760276]\n",
      "epoch:14 step:13694 [D loss: 0.686356, acc: 55.47%] [G loss: 1.723972]\n",
      "epoch:14 step:13695 [D loss: 0.630573, acc: 67.19%] [G loss: 1.827559]\n",
      "epoch:14 step:13696 [D loss: 0.673003, acc: 58.59%] [G loss: 1.749515]\n",
      "epoch:14 step:13697 [D loss: 0.663222, acc: 60.16%] [G loss: 1.950672]\n",
      "epoch:14 step:13698 [D loss: 0.655034, acc: 60.94%] [G loss: 1.798450]\n",
      "epoch:14 step:13699 [D loss: 0.638859, acc: 67.97%] [G loss: 1.696763]\n",
      "epoch:14 step:13700 [D loss: 0.650157, acc: 59.38%] [G loss: 1.888030]\n",
      "epoch:14 step:13701 [D loss: 0.638164, acc: 62.50%] [G loss: 1.965680]\n",
      "epoch:14 step:13702 [D loss: 0.650408, acc: 59.38%] [G loss: 1.886687]\n",
      "epoch:14 step:13703 [D loss: 0.677643, acc: 55.47%] [G loss: 1.859221]\n",
      "epoch:14 step:13704 [D loss: 0.679947, acc: 61.72%] [G loss: 1.798945]\n",
      "epoch:14 step:13705 [D loss: 0.588524, acc: 66.41%] [G loss: 1.800987]\n",
      "epoch:14 step:13706 [D loss: 0.642748, acc: 63.28%] [G loss: 1.862811]\n",
      "epoch:14 step:13707 [D loss: 0.613019, acc: 67.19%] [G loss: 2.012251]\n",
      "epoch:14 step:13708 [D loss: 0.591899, acc: 67.97%] [G loss: 1.931740]\n",
      "epoch:14 step:13709 [D loss: 0.663548, acc: 56.25%] [G loss: 1.902116]\n",
      "epoch:14 step:13710 [D loss: 0.612745, acc: 66.41%] [G loss: 1.972924]\n",
      "epoch:14 step:13711 [D loss: 0.636347, acc: 64.84%] [G loss: 1.911917]\n",
      "epoch:14 step:13712 [D loss: 0.656595, acc: 62.50%] [G loss: 1.884462]\n",
      "epoch:14 step:13713 [D loss: 0.624981, acc: 62.50%] [G loss: 1.951789]\n",
      "epoch:14 step:13714 [D loss: 0.628338, acc: 62.50%] [G loss: 1.820346]\n",
      "epoch:14 step:13715 [D loss: 0.643316, acc: 59.38%] [G loss: 1.935328]\n",
      "epoch:14 step:13716 [D loss: 0.606573, acc: 67.97%] [G loss: 2.001332]\n",
      "epoch:14 step:13717 [D loss: 0.728281, acc: 57.03%] [G loss: 1.867302]\n",
      "epoch:14 step:13718 [D loss: 0.680777, acc: 59.38%] [G loss: 2.038329]\n",
      "epoch:14 step:13719 [D loss: 0.661536, acc: 58.59%] [G loss: 2.011768]\n",
      "epoch:14 step:13720 [D loss: 0.640997, acc: 67.19%] [G loss: 1.977119]\n",
      "epoch:14 step:13721 [D loss: 0.664478, acc: 59.38%] [G loss: 1.967664]\n",
      "epoch:14 step:13722 [D loss: 0.649422, acc: 58.59%] [G loss: 1.862604]\n",
      "epoch:14 step:13723 [D loss: 0.596684, acc: 67.97%] [G loss: 2.032026]\n",
      "epoch:14 step:13724 [D loss: 0.660110, acc: 64.06%] [G loss: 1.884985]\n",
      "epoch:14 step:13725 [D loss: 0.640155, acc: 61.72%] [G loss: 1.901528]\n",
      "epoch:14 step:13726 [D loss: 0.611952, acc: 64.84%] [G loss: 1.978215]\n",
      "epoch:14 step:13727 [D loss: 0.649843, acc: 64.06%] [G loss: 1.972564]\n",
      "epoch:14 step:13728 [D loss: 0.628934, acc: 61.72%] [G loss: 1.870134]\n",
      "epoch:14 step:13729 [D loss: 0.637428, acc: 64.06%] [G loss: 1.886741]\n",
      "epoch:14 step:13730 [D loss: 0.727297, acc: 55.47%] [G loss: 1.802999]\n",
      "epoch:14 step:13731 [D loss: 0.645606, acc: 61.72%] [G loss: 1.943563]\n",
      "epoch:14 step:13732 [D loss: 0.712269, acc: 53.12%] [G loss: 1.891863]\n",
      "epoch:14 step:13733 [D loss: 0.669203, acc: 57.03%] [G loss: 1.742920]\n",
      "epoch:14 step:13734 [D loss: 0.656273, acc: 57.81%] [G loss: 1.921988]\n",
      "epoch:14 step:13735 [D loss: 0.672560, acc: 59.38%] [G loss: 1.806134]\n",
      "epoch:14 step:13736 [D loss: 0.624680, acc: 66.41%] [G loss: 2.035899]\n",
      "epoch:14 step:13737 [D loss: 0.664049, acc: 62.50%] [G loss: 1.804550]\n",
      "epoch:14 step:13738 [D loss: 0.631001, acc: 60.94%] [G loss: 1.888851]\n",
      "epoch:14 step:13739 [D loss: 0.626148, acc: 62.50%] [G loss: 1.779827]\n",
      "epoch:14 step:13740 [D loss: 0.677718, acc: 55.47%] [G loss: 1.893468]\n",
      "epoch:14 step:13741 [D loss: 0.670521, acc: 57.81%] [G loss: 1.914576]\n",
      "epoch:14 step:13742 [D loss: 0.623785, acc: 67.97%] [G loss: 2.042958]\n",
      "epoch:14 step:13743 [D loss: 0.642174, acc: 62.50%] [G loss: 1.879767]\n",
      "epoch:14 step:13744 [D loss: 0.704404, acc: 53.12%] [G loss: 1.885049]\n",
      "epoch:14 step:13745 [D loss: 0.650548, acc: 59.38%] [G loss: 1.860389]\n",
      "epoch:14 step:13746 [D loss: 0.650669, acc: 64.06%] [G loss: 1.755141]\n",
      "epoch:14 step:13747 [D loss: 0.618506, acc: 66.41%] [G loss: 1.999903]\n",
      "epoch:14 step:13748 [D loss: 0.618877, acc: 66.41%] [G loss: 2.030954]\n",
      "epoch:14 step:13749 [D loss: 0.593655, acc: 67.19%] [G loss: 2.014134]\n",
      "epoch:14 step:13750 [D loss: 0.619527, acc: 64.06%] [G loss: 1.996011]\n",
      "epoch:14 step:13751 [D loss: 0.644213, acc: 55.47%] [G loss: 1.865646]\n",
      "epoch:14 step:13752 [D loss: 0.606095, acc: 65.62%] [G loss: 2.101412]\n",
      "epoch:14 step:13753 [D loss: 0.615379, acc: 68.75%] [G loss: 1.980747]\n",
      "epoch:14 step:13754 [D loss: 0.673768, acc: 60.16%] [G loss: 1.910603]\n",
      "epoch:14 step:13755 [D loss: 0.603079, acc: 69.53%] [G loss: 2.090489]\n",
      "epoch:14 step:13756 [D loss: 0.627250, acc: 64.84%] [G loss: 2.018167]\n",
      "epoch:14 step:13757 [D loss: 0.667515, acc: 60.16%] [G loss: 2.041897]\n",
      "epoch:14 step:13758 [D loss: 0.634422, acc: 65.62%] [G loss: 1.982386]\n",
      "epoch:14 step:13759 [D loss: 0.619118, acc: 63.28%] [G loss: 2.000842]\n",
      "epoch:14 step:13760 [D loss: 0.583011, acc: 68.75%] [G loss: 1.998607]\n",
      "epoch:14 step:13761 [D loss: 0.641882, acc: 62.50%] [G loss: 1.898282]\n",
      "epoch:14 step:13762 [D loss: 0.605463, acc: 65.62%] [G loss: 1.959478]\n",
      "epoch:14 step:13763 [D loss: 0.701177, acc: 57.81%] [G loss: 1.951023]\n",
      "epoch:14 step:13764 [D loss: 0.683985, acc: 51.56%] [G loss: 1.971095]\n",
      "epoch:14 step:13765 [D loss: 0.590840, acc: 67.19%] [G loss: 2.203080]\n",
      "epoch:14 step:13766 [D loss: 0.570644, acc: 73.44%] [G loss: 2.636389]\n",
      "epoch:14 step:13767 [D loss: 0.649617, acc: 62.50%] [G loss: 2.143008]\n",
      "epoch:14 step:13768 [D loss: 0.658161, acc: 58.59%] [G loss: 2.083268]\n",
      "epoch:14 step:13769 [D loss: 0.602009, acc: 65.62%] [G loss: 2.057971]\n",
      "epoch:14 step:13770 [D loss: 0.644766, acc: 64.06%] [G loss: 1.979839]\n",
      "epoch:14 step:13771 [D loss: 0.621609, acc: 65.62%] [G loss: 2.069264]\n",
      "epoch:14 step:13772 [D loss: 0.602135, acc: 67.19%] [G loss: 2.104532]\n",
      "epoch:14 step:13773 [D loss: 0.694071, acc: 53.91%] [G loss: 1.937820]\n",
      "epoch:14 step:13774 [D loss: 0.668754, acc: 56.25%] [G loss: 1.958482]\n",
      "epoch:14 step:13775 [D loss: 0.695563, acc: 59.38%] [G loss: 1.840744]\n",
      "epoch:14 step:13776 [D loss: 0.644468, acc: 60.16%] [G loss: 1.839203]\n",
      "epoch:14 step:13777 [D loss: 0.682457, acc: 59.38%] [G loss: 1.778539]\n",
      "epoch:14 step:13778 [D loss: 0.648294, acc: 63.28%] [G loss: 1.920521]\n",
      "epoch:14 step:13779 [D loss: 0.648579, acc: 60.16%] [G loss: 1.919847]\n",
      "epoch:14 step:13780 [D loss: 0.649434, acc: 60.94%] [G loss: 1.839253]\n",
      "epoch:14 step:13781 [D loss: 0.627074, acc: 64.06%] [G loss: 2.021135]\n",
      "epoch:14 step:13782 [D loss: 0.735590, acc: 53.12%] [G loss: 1.882654]\n",
      "epoch:14 step:13783 [D loss: 0.681023, acc: 57.03%] [G loss: 1.859767]\n",
      "epoch:14 step:13784 [D loss: 0.634154, acc: 64.06%] [G loss: 1.936975]\n",
      "epoch:14 step:13785 [D loss: 0.671076, acc: 60.94%] [G loss: 1.808911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13786 [D loss: 0.620749, acc: 69.53%] [G loss: 2.030495]\n",
      "epoch:14 step:13787 [D loss: 0.688971, acc: 59.38%] [G loss: 1.890351]\n",
      "epoch:14 step:13788 [D loss: 0.675415, acc: 59.38%] [G loss: 1.888505]\n",
      "epoch:14 step:13789 [D loss: 0.620246, acc: 61.72%] [G loss: 1.771814]\n",
      "epoch:14 step:13790 [D loss: 0.671672, acc: 62.50%] [G loss: 1.883504]\n",
      "epoch:14 step:13791 [D loss: 0.653496, acc: 57.03%] [G loss: 1.881459]\n",
      "epoch:14 step:13792 [D loss: 0.648677, acc: 62.50%] [G loss: 1.900168]\n",
      "epoch:14 step:13793 [D loss: 0.660843, acc: 61.72%] [G loss: 1.868295]\n",
      "epoch:14 step:13794 [D loss: 0.625595, acc: 68.75%] [G loss: 1.936017]\n",
      "epoch:14 step:13795 [D loss: 0.639924, acc: 63.28%] [G loss: 1.868621]\n",
      "epoch:14 step:13796 [D loss: 0.651940, acc: 51.56%] [G loss: 1.872537]\n",
      "epoch:14 step:13797 [D loss: 0.595450, acc: 71.09%] [G loss: 2.055248]\n",
      "epoch:14 step:13798 [D loss: 0.639648, acc: 62.50%] [G loss: 2.024367]\n",
      "epoch:14 step:13799 [D loss: 0.590780, acc: 65.62%] [G loss: 2.014114]\n",
      "epoch:14 step:13800 [D loss: 0.621550, acc: 61.72%] [G loss: 2.040624]\n",
      "##############\n",
      "[2.27942193 1.2387048  6.28487216 4.81438873 3.71924861 5.64401745\n",
      " 4.35783529 4.74813434 4.58589321 3.48743299]\n",
      "##########\n",
      "epoch:14 step:13801 [D loss: 0.635671, acc: 64.84%] [G loss: 2.085040]\n",
      "epoch:14 step:13802 [D loss: 0.635338, acc: 64.84%] [G loss: 1.884038]\n",
      "epoch:14 step:13803 [D loss: 0.640733, acc: 63.28%] [G loss: 2.027679]\n",
      "epoch:14 step:13804 [D loss: 0.632782, acc: 65.62%] [G loss: 2.035943]\n",
      "epoch:14 step:13805 [D loss: 0.676271, acc: 60.16%] [G loss: 1.828478]\n",
      "epoch:14 step:13806 [D loss: 0.638805, acc: 65.62%] [G loss: 1.795773]\n",
      "epoch:14 step:13807 [D loss: 0.602023, acc: 65.62%] [G loss: 1.980316]\n",
      "epoch:14 step:13808 [D loss: 0.631781, acc: 64.84%] [G loss: 1.952249]\n",
      "epoch:14 step:13809 [D loss: 0.612842, acc: 70.31%] [G loss: 1.985731]\n",
      "epoch:14 step:13810 [D loss: 0.571001, acc: 77.34%] [G loss: 2.008244]\n",
      "epoch:14 step:13811 [D loss: 0.563250, acc: 72.66%] [G loss: 2.069917]\n",
      "epoch:14 step:13812 [D loss: 0.586706, acc: 71.88%] [G loss: 2.380595]\n",
      "epoch:14 step:13813 [D loss: 0.626198, acc: 66.41%] [G loss: 1.995458]\n",
      "epoch:14 step:13814 [D loss: 0.653925, acc: 65.62%] [G loss: 2.054398]\n",
      "epoch:14 step:13815 [D loss: 0.642168, acc: 61.72%] [G loss: 2.026499]\n",
      "epoch:14 step:13816 [D loss: 0.615282, acc: 64.06%] [G loss: 2.000210]\n",
      "epoch:14 step:13817 [D loss: 0.681709, acc: 59.38%] [G loss: 2.016012]\n",
      "epoch:14 step:13818 [D loss: 0.650578, acc: 57.81%] [G loss: 2.009019]\n",
      "epoch:14 step:13819 [D loss: 0.608882, acc: 69.53%] [G loss: 2.021176]\n",
      "epoch:14 step:13820 [D loss: 0.737456, acc: 53.12%] [G loss: 1.801164]\n",
      "epoch:14 step:13821 [D loss: 0.682463, acc: 60.94%] [G loss: 1.750063]\n",
      "epoch:14 step:13822 [D loss: 0.721704, acc: 53.12%] [G loss: 1.740465]\n",
      "epoch:14 step:13823 [D loss: 0.677083, acc: 57.03%] [G loss: 1.777182]\n",
      "epoch:14 step:13824 [D loss: 0.706409, acc: 56.25%] [G loss: 2.056477]\n",
      "epoch:14 step:13825 [D loss: 0.604911, acc: 66.41%] [G loss: 1.951961]\n",
      "epoch:14 step:13826 [D loss: 0.670886, acc: 60.94%] [G loss: 2.075207]\n",
      "epoch:14 step:13827 [D loss: 0.595343, acc: 68.75%] [G loss: 2.090701]\n",
      "epoch:14 step:13828 [D loss: 0.691695, acc: 57.03%] [G loss: 1.721530]\n",
      "epoch:14 step:13829 [D loss: 0.643180, acc: 62.50%] [G loss: 1.974498]\n",
      "epoch:14 step:13830 [D loss: 0.617997, acc: 62.50%] [G loss: 2.163514]\n",
      "epoch:14 step:13831 [D loss: 0.653152, acc: 62.50%] [G loss: 1.939498]\n",
      "epoch:14 step:13832 [D loss: 0.615028, acc: 64.84%] [G loss: 1.872615]\n",
      "epoch:14 step:13833 [D loss: 0.630063, acc: 65.62%] [G loss: 1.976964]\n",
      "epoch:14 step:13834 [D loss: 0.687329, acc: 57.03%] [G loss: 1.823106]\n",
      "epoch:14 step:13835 [D loss: 0.635187, acc: 64.84%] [G loss: 1.794439]\n",
      "epoch:14 step:13836 [D loss: 0.679255, acc: 60.94%] [G loss: 1.983020]\n",
      "epoch:14 step:13837 [D loss: 0.590106, acc: 69.53%] [G loss: 2.093809]\n",
      "epoch:14 step:13838 [D loss: 0.658593, acc: 64.06%] [G loss: 1.954165]\n",
      "epoch:14 step:13839 [D loss: 0.643072, acc: 67.19%] [G loss: 1.928130]\n",
      "epoch:14 step:13840 [D loss: 0.651928, acc: 61.72%] [G loss: 1.881543]\n",
      "epoch:14 step:13841 [D loss: 0.616228, acc: 63.28%] [G loss: 1.872155]\n",
      "epoch:14 step:13842 [D loss: 0.663722, acc: 60.94%] [G loss: 1.942526]\n",
      "epoch:14 step:13843 [D loss: 0.628665, acc: 64.84%] [G loss: 2.025216]\n",
      "epoch:14 step:13844 [D loss: 0.605935, acc: 72.66%] [G loss: 1.902759]\n",
      "epoch:14 step:13845 [D loss: 0.621668, acc: 63.28%] [G loss: 1.964774]\n",
      "epoch:14 step:13846 [D loss: 0.653919, acc: 64.06%] [G loss: 1.911896]\n",
      "epoch:14 step:13847 [D loss: 0.632169, acc: 64.84%] [G loss: 1.874463]\n",
      "epoch:14 step:13848 [D loss: 0.668814, acc: 53.91%] [G loss: 1.721294]\n",
      "epoch:14 step:13849 [D loss: 0.654146, acc: 60.16%] [G loss: 1.965139]\n",
      "epoch:14 step:13850 [D loss: 0.636054, acc: 64.06%] [G loss: 1.795523]\n",
      "epoch:14 step:13851 [D loss: 0.631564, acc: 65.62%] [G loss: 1.988437]\n",
      "epoch:14 step:13852 [D loss: 0.665106, acc: 60.94%] [G loss: 1.750690]\n",
      "epoch:14 step:13853 [D loss: 0.643497, acc: 66.41%] [G loss: 1.959096]\n",
      "epoch:14 step:13854 [D loss: 0.568207, acc: 67.97%] [G loss: 1.832096]\n",
      "epoch:14 step:13855 [D loss: 0.672313, acc: 60.16%] [G loss: 1.999648]\n",
      "epoch:14 step:13856 [D loss: 0.687539, acc: 57.03%] [G loss: 1.760088]\n",
      "epoch:14 step:13857 [D loss: 0.707884, acc: 53.91%] [G loss: 1.841055]\n",
      "epoch:14 step:13858 [D loss: 0.621545, acc: 66.41%] [G loss: 1.908374]\n",
      "epoch:14 step:13859 [D loss: 0.712917, acc: 58.59%] [G loss: 1.781224]\n",
      "epoch:14 step:13860 [D loss: 0.629755, acc: 57.03%] [G loss: 1.693633]\n",
      "epoch:14 step:13861 [D loss: 0.630613, acc: 65.62%] [G loss: 1.874263]\n",
      "epoch:14 step:13862 [D loss: 0.656842, acc: 62.50%] [G loss: 1.665551]\n",
      "epoch:14 step:13863 [D loss: 0.654686, acc: 57.81%] [G loss: 1.743967]\n",
      "epoch:14 step:13864 [D loss: 0.632822, acc: 61.72%] [G loss: 2.192149]\n",
      "epoch:14 step:13865 [D loss: 0.588630, acc: 75.00%] [G loss: 1.895593]\n",
      "epoch:14 step:13866 [D loss: 0.681037, acc: 59.38%] [G loss: 1.894603]\n",
      "epoch:14 step:13867 [D loss: 0.736872, acc: 51.56%] [G loss: 1.861371]\n",
      "epoch:14 step:13868 [D loss: 0.631642, acc: 67.19%] [G loss: 1.808137]\n",
      "epoch:14 step:13869 [D loss: 0.622091, acc: 64.84%] [G loss: 1.854202]\n",
      "epoch:14 step:13870 [D loss: 0.658881, acc: 58.59%] [G loss: 1.836823]\n",
      "epoch:14 step:13871 [D loss: 0.623045, acc: 70.31%] [G loss: 1.876102]\n",
      "epoch:14 step:13872 [D loss: 0.633155, acc: 67.97%] [G loss: 1.898986]\n",
      "epoch:14 step:13873 [D loss: 0.608902, acc: 65.62%] [G loss: 1.979610]\n",
      "epoch:14 step:13874 [D loss: 0.601643, acc: 63.28%] [G loss: 1.911659]\n",
      "epoch:14 step:13875 [D loss: 0.621890, acc: 68.75%] [G loss: 2.041504]\n",
      "epoch:14 step:13876 [D loss: 0.680695, acc: 60.94%] [G loss: 1.808272]\n",
      "epoch:14 step:13877 [D loss: 0.601564, acc: 71.09%] [G loss: 1.808520]\n",
      "epoch:14 step:13878 [D loss: 0.648489, acc: 60.16%] [G loss: 1.898029]\n",
      "epoch:14 step:13879 [D loss: 0.662174, acc: 60.94%] [G loss: 1.865002]\n",
      "epoch:14 step:13880 [D loss: 0.685893, acc: 56.25%] [G loss: 1.827567]\n",
      "epoch:14 step:13881 [D loss: 0.611244, acc: 68.75%] [G loss: 2.005321]\n",
      "epoch:14 step:13882 [D loss: 0.660747, acc: 59.38%] [G loss: 1.729193]\n",
      "epoch:14 step:13883 [D loss: 0.688703, acc: 58.59%] [G loss: 1.626373]\n",
      "epoch:14 step:13884 [D loss: 0.674244, acc: 58.59%] [G loss: 1.801031]\n",
      "epoch:14 step:13885 [D loss: 0.656367, acc: 70.31%] [G loss: 1.958921]\n",
      "epoch:14 step:13886 [D loss: 0.658600, acc: 64.06%] [G loss: 1.771230]\n",
      "epoch:14 step:13887 [D loss: 0.658843, acc: 62.50%] [G loss: 1.949353]\n",
      "epoch:14 step:13888 [D loss: 0.626083, acc: 66.41%] [G loss: 1.913228]\n",
      "epoch:14 step:13889 [D loss: 0.592783, acc: 71.09%] [G loss: 1.868920]\n",
      "epoch:14 step:13890 [D loss: 0.644672, acc: 57.03%] [G loss: 1.913065]\n",
      "epoch:14 step:13891 [D loss: 0.627337, acc: 61.72%] [G loss: 1.808546]\n",
      "epoch:14 step:13892 [D loss: 0.667740, acc: 62.50%] [G loss: 1.901291]\n",
      "epoch:14 step:13893 [D loss: 0.630113, acc: 60.94%] [G loss: 1.974944]\n",
      "epoch:14 step:13894 [D loss: 0.632546, acc: 61.72%] [G loss: 1.979722]\n",
      "epoch:14 step:13895 [D loss: 0.613793, acc: 63.28%] [G loss: 1.912545]\n",
      "epoch:14 step:13896 [D loss: 0.583818, acc: 71.09%] [G loss: 2.036474]\n",
      "epoch:14 step:13897 [D loss: 0.656742, acc: 65.62%] [G loss: 1.909918]\n",
      "epoch:14 step:13898 [D loss: 0.611428, acc: 64.84%] [G loss: 1.990050]\n",
      "epoch:14 step:13899 [D loss: 0.616644, acc: 72.66%] [G loss: 2.081784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13900 [D loss: 0.646653, acc: 60.94%] [G loss: 2.115284]\n",
      "epoch:14 step:13901 [D loss: 0.669203, acc: 53.91%] [G loss: 1.882338]\n",
      "epoch:14 step:13902 [D loss: 0.711051, acc: 54.69%] [G loss: 1.813136]\n",
      "epoch:14 step:13903 [D loss: 0.641630, acc: 64.84%] [G loss: 1.914764]\n",
      "epoch:14 step:13904 [D loss: 0.618968, acc: 65.62%] [G loss: 1.974639]\n",
      "epoch:14 step:13905 [D loss: 0.667760, acc: 57.03%] [G loss: 1.854883]\n",
      "epoch:14 step:13906 [D loss: 0.651700, acc: 64.06%] [G loss: 1.775850]\n",
      "epoch:14 step:13907 [D loss: 0.660682, acc: 60.94%] [G loss: 1.871979]\n",
      "epoch:14 step:13908 [D loss: 0.633817, acc: 67.19%] [G loss: 1.959742]\n",
      "epoch:14 step:13909 [D loss: 0.603494, acc: 67.97%] [G loss: 2.105109]\n",
      "epoch:14 step:13910 [D loss: 0.568299, acc: 75.00%] [G loss: 2.099940]\n",
      "epoch:14 step:13911 [D loss: 0.655907, acc: 58.59%] [G loss: 2.001644]\n",
      "epoch:14 step:13912 [D loss: 0.699314, acc: 57.03%] [G loss: 1.823916]\n",
      "epoch:14 step:13913 [D loss: 0.617869, acc: 67.97%] [G loss: 1.969776]\n",
      "epoch:14 step:13914 [D loss: 0.665528, acc: 59.38%] [G loss: 1.978029]\n",
      "epoch:14 step:13915 [D loss: 0.633277, acc: 65.62%] [G loss: 1.760486]\n",
      "epoch:14 step:13916 [D loss: 0.695439, acc: 57.03%] [G loss: 1.997663]\n",
      "epoch:14 step:13917 [D loss: 0.637132, acc: 62.50%] [G loss: 1.833742]\n",
      "epoch:14 step:13918 [D loss: 0.694012, acc: 53.91%] [G loss: 1.657002]\n",
      "epoch:14 step:13919 [D loss: 0.714039, acc: 56.25%] [G loss: 1.704714]\n",
      "epoch:14 step:13920 [D loss: 0.594788, acc: 67.19%] [G loss: 1.944483]\n",
      "epoch:14 step:13921 [D loss: 0.597231, acc: 69.53%] [G loss: 1.907539]\n",
      "epoch:14 step:13922 [D loss: 0.617302, acc: 62.50%] [G loss: 1.993478]\n",
      "epoch:14 step:13923 [D loss: 0.648296, acc: 58.59%] [G loss: 2.067972]\n",
      "epoch:14 step:13924 [D loss: 0.595024, acc: 67.97%] [G loss: 2.156983]\n",
      "epoch:14 step:13925 [D loss: 0.625468, acc: 63.28%] [G loss: 2.021395]\n",
      "epoch:14 step:13926 [D loss: 0.644929, acc: 60.94%] [G loss: 2.041801]\n",
      "epoch:14 step:13927 [D loss: 0.675879, acc: 55.47%] [G loss: 1.915971]\n",
      "epoch:14 step:13928 [D loss: 0.633886, acc: 67.97%] [G loss: 1.923864]\n",
      "epoch:14 step:13929 [D loss: 0.610371, acc: 64.84%] [G loss: 1.726837]\n",
      "epoch:14 step:13930 [D loss: 0.665270, acc: 57.03%] [G loss: 1.802704]\n",
      "epoch:14 step:13931 [D loss: 0.686092, acc: 56.25%] [G loss: 2.092253]\n",
      "epoch:14 step:13932 [D loss: 0.629446, acc: 68.75%] [G loss: 1.855220]\n",
      "epoch:14 step:13933 [D loss: 0.636960, acc: 67.19%] [G loss: 2.133313]\n",
      "epoch:14 step:13934 [D loss: 0.628446, acc: 63.28%] [G loss: 2.053628]\n",
      "epoch:14 step:13935 [D loss: 0.641082, acc: 64.84%] [G loss: 1.752685]\n",
      "epoch:14 step:13936 [D loss: 0.663731, acc: 53.91%] [G loss: 1.820704]\n",
      "epoch:14 step:13937 [D loss: 0.646222, acc: 62.50%] [G loss: 1.834888]\n",
      "epoch:14 step:13938 [D loss: 0.734138, acc: 50.78%] [G loss: 1.780526]\n",
      "epoch:14 step:13939 [D loss: 0.652798, acc: 57.03%] [G loss: 1.888653]\n",
      "epoch:14 step:13940 [D loss: 0.626495, acc: 64.06%] [G loss: 1.924486]\n",
      "epoch:14 step:13941 [D loss: 0.665884, acc: 59.38%] [G loss: 1.879036]\n",
      "epoch:14 step:13942 [D loss: 0.649194, acc: 58.59%] [G loss: 1.972493]\n",
      "epoch:14 step:13943 [D loss: 0.588468, acc: 71.88%] [G loss: 2.015869]\n",
      "epoch:14 step:13944 [D loss: 0.632211, acc: 67.97%] [G loss: 1.962163]\n",
      "epoch:14 step:13945 [D loss: 0.662889, acc: 66.41%] [G loss: 1.821480]\n",
      "epoch:14 step:13946 [D loss: 0.680841, acc: 56.25%] [G loss: 1.842337]\n",
      "epoch:14 step:13947 [D loss: 0.677456, acc: 58.59%] [G loss: 1.800742]\n",
      "epoch:14 step:13948 [D loss: 0.633451, acc: 65.62%] [G loss: 1.857197]\n",
      "epoch:14 step:13949 [D loss: 0.662338, acc: 60.16%] [G loss: 1.936684]\n",
      "epoch:14 step:13950 [D loss: 0.650678, acc: 64.84%] [G loss: 1.860890]\n",
      "epoch:14 step:13951 [D loss: 0.626409, acc: 62.50%] [G loss: 1.985572]\n",
      "epoch:14 step:13952 [D loss: 0.646370, acc: 64.84%] [G loss: 1.917679]\n",
      "epoch:14 step:13953 [D loss: 0.615855, acc: 67.97%] [G loss: 1.970737]\n",
      "epoch:14 step:13954 [D loss: 0.660872, acc: 63.28%] [G loss: 1.938800]\n",
      "epoch:14 step:13955 [D loss: 0.639853, acc: 64.84%] [G loss: 1.986896]\n",
      "epoch:14 step:13956 [D loss: 0.574655, acc: 72.66%] [G loss: 1.992554]\n",
      "epoch:14 step:13957 [D loss: 0.613343, acc: 66.41%] [G loss: 1.849269]\n",
      "epoch:14 step:13958 [D loss: 0.580859, acc: 71.09%] [G loss: 2.025798]\n",
      "epoch:14 step:13959 [D loss: 0.602648, acc: 65.62%] [G loss: 2.024318]\n",
      "epoch:14 step:13960 [D loss: 0.612813, acc: 61.72%] [G loss: 1.967564]\n",
      "epoch:14 step:13961 [D loss: 0.635971, acc: 61.72%] [G loss: 2.083126]\n",
      "epoch:14 step:13962 [D loss: 0.617220, acc: 66.41%] [G loss: 2.031204]\n",
      "epoch:14 step:13963 [D loss: 0.688333, acc: 60.16%] [G loss: 1.906645]\n",
      "epoch:14 step:13964 [D loss: 0.652778, acc: 62.50%] [G loss: 1.825664]\n",
      "epoch:14 step:13965 [D loss: 0.644303, acc: 59.38%] [G loss: 1.884128]\n",
      "epoch:14 step:13966 [D loss: 0.645580, acc: 62.50%] [G loss: 1.927010]\n",
      "epoch:14 step:13967 [D loss: 0.636325, acc: 69.53%] [G loss: 1.998781]\n",
      "epoch:14 step:13968 [D loss: 0.705089, acc: 57.81%] [G loss: 1.734622]\n",
      "epoch:14 step:13969 [D loss: 0.693073, acc: 53.12%] [G loss: 1.798564]\n",
      "epoch:14 step:13970 [D loss: 0.716447, acc: 53.12%] [G loss: 1.954953]\n",
      "epoch:14 step:13971 [D loss: 0.639174, acc: 63.28%] [G loss: 1.915591]\n",
      "epoch:14 step:13972 [D loss: 0.636962, acc: 66.41%] [G loss: 1.927917]\n",
      "epoch:14 step:13973 [D loss: 0.626284, acc: 60.16%] [G loss: 1.901870]\n",
      "epoch:14 step:13974 [D loss: 0.700804, acc: 53.12%] [G loss: 1.730568]\n",
      "epoch:14 step:13975 [D loss: 0.632816, acc: 63.28%] [G loss: 1.922331]\n",
      "epoch:14 step:13976 [D loss: 0.704160, acc: 57.03%] [G loss: 1.804779]\n",
      "epoch:14 step:13977 [D loss: 0.682811, acc: 61.72%] [G loss: 1.832798]\n",
      "epoch:14 step:13978 [D loss: 0.625051, acc: 64.06%] [G loss: 1.940735]\n",
      "epoch:14 step:13979 [D loss: 0.640659, acc: 62.50%] [G loss: 1.827499]\n",
      "epoch:14 step:13980 [D loss: 0.697050, acc: 58.59%] [G loss: 1.913779]\n",
      "epoch:14 step:13981 [D loss: 0.659409, acc: 57.81%] [G loss: 1.944615]\n",
      "epoch:14 step:13982 [D loss: 0.641559, acc: 67.19%] [G loss: 1.843415]\n",
      "epoch:14 step:13983 [D loss: 0.680375, acc: 63.28%] [G loss: 1.766874]\n",
      "epoch:14 step:13984 [D loss: 0.640858, acc: 66.41%] [G loss: 1.950406]\n",
      "epoch:14 step:13985 [D loss: 0.668178, acc: 59.38%] [G loss: 1.796098]\n",
      "epoch:14 step:13986 [D loss: 0.678780, acc: 55.47%] [G loss: 1.891629]\n",
      "epoch:14 step:13987 [D loss: 0.653743, acc: 62.50%] [G loss: 1.914839]\n",
      "epoch:14 step:13988 [D loss: 0.665889, acc: 58.59%] [G loss: 1.903883]\n",
      "epoch:14 step:13989 [D loss: 0.659392, acc: 59.38%] [G loss: 1.883188]\n",
      "epoch:14 step:13990 [D loss: 0.656566, acc: 58.59%] [G loss: 1.864995]\n",
      "epoch:14 step:13991 [D loss: 0.690627, acc: 53.12%] [G loss: 1.818482]\n",
      "epoch:14 step:13992 [D loss: 0.634660, acc: 66.41%] [G loss: 1.820559]\n",
      "epoch:14 step:13993 [D loss: 0.654906, acc: 55.47%] [G loss: 1.935777]\n",
      "epoch:14 step:13994 [D loss: 0.632660, acc: 66.41%] [G loss: 1.969409]\n",
      "epoch:14 step:13995 [D loss: 0.705190, acc: 56.25%] [G loss: 1.833233]\n",
      "epoch:14 step:13996 [D loss: 0.667749, acc: 55.47%] [G loss: 1.860347]\n",
      "epoch:14 step:13997 [D loss: 0.623942, acc: 64.06%] [G loss: 1.741840]\n",
      "epoch:14 step:13998 [D loss: 0.605161, acc: 69.53%] [G loss: 1.873703]\n",
      "epoch:14 step:13999 [D loss: 0.655805, acc: 58.59%] [G loss: 1.901210]\n",
      "epoch:14 step:14000 [D loss: 0.618710, acc: 65.62%] [G loss: 1.928592]\n",
      "##############\n",
      "[2.61753961 1.08159238 6.34520115 4.77973903 3.65179907 5.49961726\n",
      " 4.38555686 4.49714187 4.62423968 3.49276385]\n",
      "##########\n",
      "epoch:14 step:14001 [D loss: 0.637963, acc: 62.50%] [G loss: 1.876582]\n",
      "epoch:14 step:14002 [D loss: 0.605827, acc: 67.19%] [G loss: 1.971506]\n",
      "epoch:14 step:14003 [D loss: 0.656512, acc: 54.69%] [G loss: 1.908030]\n",
      "epoch:14 step:14004 [D loss: 0.648459, acc: 62.50%] [G loss: 2.009221]\n",
      "epoch:14 step:14005 [D loss: 0.613012, acc: 67.19%] [G loss: 1.964799]\n",
      "epoch:14 step:14006 [D loss: 0.620387, acc: 70.31%] [G loss: 1.881533]\n",
      "epoch:14 step:14007 [D loss: 0.624477, acc: 64.06%] [G loss: 1.958398]\n",
      "epoch:14 step:14008 [D loss: 0.643493, acc: 64.84%] [G loss: 1.957681]\n",
      "epoch:14 step:14009 [D loss: 0.666305, acc: 62.50%] [G loss: 1.864071]\n",
      "epoch:14 step:14010 [D loss: 0.638431, acc: 65.62%] [G loss: 1.883395]\n",
      "epoch:14 step:14011 [D loss: 0.613195, acc: 60.94%] [G loss: 1.942248]\n",
      "epoch:14 step:14012 [D loss: 0.657801, acc: 59.38%] [G loss: 1.919091]\n",
      "epoch:14 step:14013 [D loss: 0.663507, acc: 61.72%] [G loss: 1.815695]\n",
      "epoch:14 step:14014 [D loss: 0.661851, acc: 53.12%] [G loss: 1.736974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:14015 [D loss: 0.596680, acc: 68.75%] [G loss: 1.931645]\n",
      "epoch:14 step:14016 [D loss: 0.691123, acc: 54.69%] [G loss: 1.932744]\n",
      "epoch:14 step:14017 [D loss: 0.647295, acc: 60.94%] [G loss: 2.085574]\n",
      "epoch:14 step:14018 [D loss: 0.629081, acc: 64.06%] [G loss: 1.987522]\n",
      "epoch:14 step:14019 [D loss: 0.640859, acc: 60.94%] [G loss: 1.869713]\n",
      "epoch:14 step:14020 [D loss: 0.680153, acc: 57.81%] [G loss: 1.916963]\n",
      "epoch:14 step:14021 [D loss: 0.643777, acc: 64.06%] [G loss: 1.864520]\n",
      "epoch:14 step:14022 [D loss: 0.629170, acc: 64.84%] [G loss: 1.962550]\n",
      "epoch:14 step:14023 [D loss: 0.601599, acc: 66.41%] [G loss: 1.843086]\n",
      "epoch:14 step:14024 [D loss: 0.644036, acc: 60.94%] [G loss: 2.094003]\n",
      "epoch:14 step:14025 [D loss: 0.617425, acc: 65.62%] [G loss: 2.072334]\n",
      "epoch:14 step:14026 [D loss: 0.570825, acc: 73.44%] [G loss: 2.128650]\n",
      "epoch:14 step:14027 [D loss: 0.663297, acc: 61.72%] [G loss: 2.009927]\n",
      "epoch:14 step:14028 [D loss: 0.650521, acc: 60.16%] [G loss: 1.977623]\n",
      "epoch:14 step:14029 [D loss: 0.629219, acc: 64.84%] [G loss: 1.958698]\n",
      "epoch:14 step:14030 [D loss: 0.614326, acc: 64.84%] [G loss: 2.281692]\n",
      "epoch:14 step:14031 [D loss: 0.636105, acc: 56.25%] [G loss: 2.045302]\n",
      "epoch:14 step:14032 [D loss: 0.699203, acc: 54.69%] [G loss: 1.988108]\n",
      "epoch:14 step:14033 [D loss: 0.690065, acc: 57.81%] [G loss: 1.917510]\n",
      "epoch:14 step:14034 [D loss: 0.603401, acc: 66.41%] [G loss: 1.995335]\n",
      "epoch:14 step:14035 [D loss: 0.634874, acc: 61.72%] [G loss: 1.982301]\n",
      "epoch:14 step:14036 [D loss: 0.585221, acc: 71.88%] [G loss: 2.095980]\n",
      "epoch:14 step:14037 [D loss: 0.554261, acc: 75.78%] [G loss: 2.414852]\n",
      "epoch:14 step:14038 [D loss: 0.670705, acc: 60.16%] [G loss: 1.898623]\n",
      "epoch:14 step:14039 [D loss: 0.638424, acc: 64.84%] [G loss: 1.981533]\n",
      "epoch:14 step:14040 [D loss: 0.650471, acc: 64.06%] [G loss: 2.027723]\n",
      "epoch:14 step:14041 [D loss: 0.602547, acc: 68.75%] [G loss: 2.033643]\n",
      "epoch:14 step:14042 [D loss: 0.614978, acc: 67.19%] [G loss: 2.045418]\n",
      "epoch:14 step:14043 [D loss: 0.571764, acc: 69.53%] [G loss: 2.137729]\n",
      "epoch:14 step:14044 [D loss: 0.598219, acc: 66.41%] [G loss: 2.365269]\n",
      "epoch:14 step:14045 [D loss: 0.681433, acc: 62.50%] [G loss: 2.313180]\n",
      "epoch:14 step:14046 [D loss: 0.787725, acc: 46.09%] [G loss: 1.965306]\n",
      "epoch:14 step:14047 [D loss: 0.706763, acc: 55.47%] [G loss: 2.030550]\n",
      "epoch:14 step:14048 [D loss: 0.641010, acc: 60.94%] [G loss: 1.988159]\n",
      "epoch:14 step:14049 [D loss: 0.617746, acc: 66.41%] [G loss: 1.970671]\n",
      "epoch:14 step:14050 [D loss: 0.652425, acc: 60.16%] [G loss: 1.864222]\n",
      "epoch:14 step:14051 [D loss: 0.620421, acc: 61.72%] [G loss: 2.072115]\n",
      "epoch:14 step:14052 [D loss: 0.588676, acc: 69.53%] [G loss: 1.986426]\n",
      "epoch:14 step:14053 [D loss: 0.585245, acc: 70.31%] [G loss: 1.956468]\n",
      "epoch:14 step:14054 [D loss: 0.622182, acc: 65.62%] [G loss: 2.279156]\n",
      "epoch:14 step:14055 [D loss: 0.657508, acc: 64.06%] [G loss: 2.321762]\n",
      "epoch:15 step:14056 [D loss: 0.643842, acc: 65.62%] [G loss: 1.944669]\n",
      "epoch:15 step:14057 [D loss: 0.701003, acc: 58.59%] [G loss: 1.955004]\n",
      "epoch:15 step:14058 [D loss: 0.687932, acc: 53.91%] [G loss: 2.102486]\n",
      "epoch:15 step:14059 [D loss: 0.623644, acc: 69.53%] [G loss: 1.809193]\n",
      "epoch:15 step:14060 [D loss: 0.669667, acc: 60.94%] [G loss: 1.896485]\n",
      "epoch:15 step:14061 [D loss: 0.619284, acc: 64.06%] [G loss: 2.063781]\n",
      "epoch:15 step:14062 [D loss: 0.584098, acc: 67.19%] [G loss: 2.011235]\n",
      "epoch:15 step:14063 [D loss: 0.743490, acc: 56.25%] [G loss: 1.944735]\n",
      "epoch:15 step:14064 [D loss: 0.583917, acc: 70.31%] [G loss: 1.953176]\n",
      "epoch:15 step:14065 [D loss: 0.627478, acc: 62.50%] [G loss: 2.060577]\n",
      "epoch:15 step:14066 [D loss: 0.669261, acc: 63.28%] [G loss: 1.991283]\n",
      "epoch:15 step:14067 [D loss: 0.659464, acc: 60.16%] [G loss: 1.836030]\n",
      "epoch:15 step:14068 [D loss: 0.621092, acc: 62.50%] [G loss: 1.860151]\n",
      "epoch:15 step:14069 [D loss: 0.589686, acc: 65.62%] [G loss: 1.882123]\n",
      "epoch:15 step:14070 [D loss: 0.661532, acc: 60.94%] [G loss: 2.146315]\n",
      "epoch:15 step:14071 [D loss: 0.619282, acc: 66.41%] [G loss: 1.995800]\n",
      "epoch:15 step:14072 [D loss: 0.655566, acc: 62.50%] [G loss: 1.916876]\n",
      "epoch:15 step:14073 [D loss: 0.634602, acc: 59.38%] [G loss: 1.827093]\n",
      "epoch:15 step:14074 [D loss: 0.683057, acc: 57.03%] [G loss: 1.861275]\n",
      "epoch:15 step:14075 [D loss: 0.694753, acc: 55.47%] [G loss: 1.731122]\n",
      "epoch:15 step:14076 [D loss: 0.638227, acc: 67.19%] [G loss: 1.929846]\n",
      "epoch:15 step:14077 [D loss: 0.662929, acc: 60.16%] [G loss: 1.844334]\n",
      "epoch:15 step:14078 [D loss: 0.591777, acc: 69.53%] [G loss: 2.098590]\n",
      "epoch:15 step:14079 [D loss: 0.669115, acc: 62.50%] [G loss: 1.889599]\n",
      "epoch:15 step:14080 [D loss: 0.596528, acc: 67.97%] [G loss: 2.085275]\n",
      "epoch:15 step:14081 [D loss: 0.668921, acc: 58.59%] [G loss: 1.842462]\n",
      "epoch:15 step:14082 [D loss: 0.663785, acc: 62.50%] [G loss: 1.944023]\n",
      "epoch:15 step:14083 [D loss: 0.590912, acc: 74.22%] [G loss: 1.906697]\n",
      "epoch:15 step:14084 [D loss: 0.633482, acc: 63.28%] [G loss: 1.950888]\n",
      "epoch:15 step:14085 [D loss: 0.639381, acc: 62.50%] [G loss: 1.882516]\n",
      "epoch:15 step:14086 [D loss: 0.669684, acc: 60.16%] [G loss: 1.826036]\n",
      "epoch:15 step:14087 [D loss: 0.662278, acc: 64.06%] [G loss: 1.717759]\n",
      "epoch:15 step:14088 [D loss: 0.640459, acc: 60.94%] [G loss: 1.872971]\n",
      "epoch:15 step:14089 [D loss: 0.670479, acc: 60.16%] [G loss: 1.785939]\n",
      "epoch:15 step:14090 [D loss: 0.617978, acc: 66.41%] [G loss: 1.838862]\n",
      "epoch:15 step:14091 [D loss: 0.618532, acc: 66.41%] [G loss: 2.025715]\n",
      "epoch:15 step:14092 [D loss: 0.676163, acc: 63.28%] [G loss: 1.904488]\n",
      "epoch:15 step:14093 [D loss: 0.620306, acc: 63.28%] [G loss: 1.940746]\n",
      "epoch:15 step:14094 [D loss: 0.608339, acc: 63.28%] [G loss: 2.081510]\n",
      "epoch:15 step:14095 [D loss: 0.603679, acc: 65.62%] [G loss: 2.081209]\n",
      "epoch:15 step:14096 [D loss: 0.656542, acc: 67.19%] [G loss: 1.897794]\n",
      "epoch:15 step:14097 [D loss: 0.594840, acc: 64.06%] [G loss: 1.993127]\n",
      "epoch:15 step:14098 [D loss: 0.638332, acc: 64.06%] [G loss: 1.960517]\n",
      "epoch:15 step:14099 [D loss: 0.645192, acc: 62.50%] [G loss: 1.846576]\n",
      "epoch:15 step:14100 [D loss: 0.592517, acc: 67.97%] [G loss: 2.087338]\n",
      "epoch:15 step:14101 [D loss: 0.659398, acc: 63.28%] [G loss: 1.863336]\n",
      "epoch:15 step:14102 [D loss: 0.613486, acc: 67.97%] [G loss: 2.071263]\n",
      "epoch:15 step:14103 [D loss: 0.659215, acc: 58.59%] [G loss: 1.922336]\n",
      "epoch:15 step:14104 [D loss: 0.679207, acc: 54.69%] [G loss: 2.025046]\n",
      "epoch:15 step:14105 [D loss: 0.696167, acc: 61.72%] [G loss: 1.922717]\n",
      "epoch:15 step:14106 [D loss: 0.683224, acc: 60.94%] [G loss: 1.881342]\n",
      "epoch:15 step:14107 [D loss: 0.677455, acc: 58.59%] [G loss: 1.877155]\n",
      "epoch:15 step:14108 [D loss: 0.624403, acc: 64.84%] [G loss: 1.878944]\n",
      "epoch:15 step:14109 [D loss: 0.655722, acc: 61.72%] [G loss: 1.939443]\n",
      "epoch:15 step:14110 [D loss: 0.625406, acc: 61.72%] [G loss: 2.025218]\n",
      "epoch:15 step:14111 [D loss: 0.639012, acc: 67.97%] [G loss: 2.103835]\n",
      "epoch:15 step:14112 [D loss: 0.658607, acc: 66.41%] [G loss: 1.894060]\n",
      "epoch:15 step:14113 [D loss: 0.677944, acc: 57.81%] [G loss: 2.051143]\n",
      "epoch:15 step:14114 [D loss: 0.636863, acc: 59.38%] [G loss: 1.854746]\n",
      "epoch:15 step:14115 [D loss: 0.634565, acc: 62.50%] [G loss: 1.913367]\n",
      "epoch:15 step:14116 [D loss: 0.701968, acc: 58.59%] [G loss: 1.794585]\n",
      "epoch:15 step:14117 [D loss: 0.669034, acc: 61.72%] [G loss: 1.789630]\n",
      "epoch:15 step:14118 [D loss: 0.625408, acc: 66.41%] [G loss: 1.919542]\n",
      "epoch:15 step:14119 [D loss: 0.668642, acc: 60.16%] [G loss: 2.026720]\n",
      "epoch:15 step:14120 [D loss: 0.681598, acc: 63.28%] [G loss: 1.814333]\n",
      "epoch:15 step:14121 [D loss: 0.649204, acc: 54.69%] [G loss: 1.900673]\n",
      "epoch:15 step:14122 [D loss: 0.681024, acc: 63.28%] [G loss: 1.938816]\n",
      "epoch:15 step:14123 [D loss: 0.622772, acc: 67.19%] [G loss: 1.932106]\n",
      "epoch:15 step:14124 [D loss: 0.581005, acc: 71.09%] [G loss: 1.995076]\n",
      "epoch:15 step:14125 [D loss: 0.664837, acc: 61.72%] [G loss: 1.991847]\n",
      "epoch:15 step:14126 [D loss: 0.658363, acc: 59.38%] [G loss: 1.871799]\n",
      "epoch:15 step:14127 [D loss: 0.644574, acc: 64.84%] [G loss: 1.862134]\n",
      "epoch:15 step:14128 [D loss: 0.673515, acc: 54.69%] [G loss: 1.807317]\n",
      "epoch:15 step:14129 [D loss: 0.618439, acc: 66.41%] [G loss: 2.128491]\n",
      "epoch:15 step:14130 [D loss: 0.653511, acc: 60.16%] [G loss: 2.003388]\n",
      "epoch:15 step:14131 [D loss: 0.632162, acc: 70.31%] [G loss: 2.188229]\n",
      "epoch:15 step:14132 [D loss: 0.573706, acc: 67.97%] [G loss: 2.032199]\n",
      "epoch:15 step:14133 [D loss: 0.675826, acc: 54.69%] [G loss: 1.914875]\n",
      "epoch:15 step:14134 [D loss: 0.652332, acc: 62.50%] [G loss: 1.784408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14135 [D loss: 0.654868, acc: 64.06%] [G loss: 1.731155]\n",
      "epoch:15 step:14136 [D loss: 0.674679, acc: 59.38%] [G loss: 1.785312]\n",
      "epoch:15 step:14137 [D loss: 0.659819, acc: 61.72%] [G loss: 1.898638]\n",
      "epoch:15 step:14138 [D loss: 0.643550, acc: 64.84%] [G loss: 1.896234]\n",
      "epoch:15 step:14139 [D loss: 0.602191, acc: 74.22%] [G loss: 1.973438]\n",
      "epoch:15 step:14140 [D loss: 0.643864, acc: 57.03%] [G loss: 1.763648]\n",
      "epoch:15 step:14141 [D loss: 0.660642, acc: 55.47%] [G loss: 1.762437]\n",
      "epoch:15 step:14142 [D loss: 0.614245, acc: 65.62%] [G loss: 1.834104]\n",
      "epoch:15 step:14143 [D loss: 0.655003, acc: 62.50%] [G loss: 2.000610]\n",
      "epoch:15 step:14144 [D loss: 0.664389, acc: 57.03%] [G loss: 1.852899]\n",
      "epoch:15 step:14145 [D loss: 0.653393, acc: 63.28%] [G loss: 1.909259]\n",
      "epoch:15 step:14146 [D loss: 0.662663, acc: 57.81%] [G loss: 1.936212]\n",
      "epoch:15 step:14147 [D loss: 0.621614, acc: 71.09%] [G loss: 1.982375]\n",
      "epoch:15 step:14148 [D loss: 0.646440, acc: 60.16%] [G loss: 1.917822]\n",
      "epoch:15 step:14149 [D loss: 0.633316, acc: 61.72%] [G loss: 1.827145]\n",
      "epoch:15 step:14150 [D loss: 0.674087, acc: 59.38%] [G loss: 1.878031]\n",
      "epoch:15 step:14151 [D loss: 0.618559, acc: 66.41%] [G loss: 1.927966]\n",
      "epoch:15 step:14152 [D loss: 0.640782, acc: 59.38%] [G loss: 1.903037]\n",
      "epoch:15 step:14153 [D loss: 0.674366, acc: 63.28%] [G loss: 1.901515]\n",
      "epoch:15 step:14154 [D loss: 0.629581, acc: 64.84%] [G loss: 1.801829]\n",
      "epoch:15 step:14155 [D loss: 0.617826, acc: 64.06%] [G loss: 2.043689]\n",
      "epoch:15 step:14156 [D loss: 0.642018, acc: 65.62%] [G loss: 1.765168]\n",
      "epoch:15 step:14157 [D loss: 0.626416, acc: 64.84%] [G loss: 1.949017]\n",
      "epoch:15 step:14158 [D loss: 0.599403, acc: 68.75%] [G loss: 1.997470]\n",
      "epoch:15 step:14159 [D loss: 0.683674, acc: 57.81%] [G loss: 1.820481]\n",
      "epoch:15 step:14160 [D loss: 0.650398, acc: 59.38%] [G loss: 1.894069]\n",
      "epoch:15 step:14161 [D loss: 0.653453, acc: 58.59%] [G loss: 2.035177]\n",
      "epoch:15 step:14162 [D loss: 0.651664, acc: 64.84%] [G loss: 2.117209]\n",
      "epoch:15 step:14163 [D loss: 0.741303, acc: 54.69%] [G loss: 1.707764]\n",
      "epoch:15 step:14164 [D loss: 0.607670, acc: 71.09%] [G loss: 1.851177]\n",
      "epoch:15 step:14165 [D loss: 0.610123, acc: 66.41%] [G loss: 1.875336]\n",
      "epoch:15 step:14166 [D loss: 0.607505, acc: 67.97%] [G loss: 2.126464]\n",
      "epoch:15 step:14167 [D loss: 0.635167, acc: 62.50%] [G loss: 2.023658]\n",
      "epoch:15 step:14168 [D loss: 0.594617, acc: 73.44%] [G loss: 2.153316]\n",
      "epoch:15 step:14169 [D loss: 0.646751, acc: 60.16%] [G loss: 2.110932]\n",
      "epoch:15 step:14170 [D loss: 0.623609, acc: 62.50%] [G loss: 2.120439]\n",
      "epoch:15 step:14171 [D loss: 0.623603, acc: 65.62%] [G loss: 1.955438]\n",
      "epoch:15 step:14172 [D loss: 0.571323, acc: 67.97%] [G loss: 2.064250]\n",
      "epoch:15 step:14173 [D loss: 0.620749, acc: 63.28%] [G loss: 1.885175]\n",
      "epoch:15 step:14174 [D loss: 0.630395, acc: 64.06%] [G loss: 2.148284]\n",
      "epoch:15 step:14175 [D loss: 0.679047, acc: 58.59%] [G loss: 1.897015]\n",
      "epoch:15 step:14176 [D loss: 0.676055, acc: 60.94%] [G loss: 1.884860]\n",
      "epoch:15 step:14177 [D loss: 0.627995, acc: 67.97%] [G loss: 2.083382]\n",
      "epoch:15 step:14178 [D loss: 0.682050, acc: 56.25%] [G loss: 1.868500]\n",
      "epoch:15 step:14179 [D loss: 0.667410, acc: 60.16%] [G loss: 2.048359]\n",
      "epoch:15 step:14180 [D loss: 0.670147, acc: 58.59%] [G loss: 1.735548]\n",
      "epoch:15 step:14181 [D loss: 0.644778, acc: 62.50%] [G loss: 1.862274]\n",
      "epoch:15 step:14182 [D loss: 0.587973, acc: 70.31%] [G loss: 2.046285]\n",
      "epoch:15 step:14183 [D loss: 0.601235, acc: 66.41%] [G loss: 1.981351]\n",
      "epoch:15 step:14184 [D loss: 0.643567, acc: 59.38%] [G loss: 1.923540]\n",
      "epoch:15 step:14185 [D loss: 0.656033, acc: 64.84%] [G loss: 2.004940]\n",
      "epoch:15 step:14186 [D loss: 0.631912, acc: 59.38%] [G loss: 1.962211]\n",
      "epoch:15 step:14187 [D loss: 0.615705, acc: 66.41%] [G loss: 1.943963]\n",
      "epoch:15 step:14188 [D loss: 0.659454, acc: 60.94%] [G loss: 1.900451]\n",
      "epoch:15 step:14189 [D loss: 0.639758, acc: 66.41%] [G loss: 1.835252]\n",
      "epoch:15 step:14190 [D loss: 0.651665, acc: 64.06%] [G loss: 1.982022]\n",
      "epoch:15 step:14191 [D loss: 0.640391, acc: 67.19%] [G loss: 1.800740]\n",
      "epoch:15 step:14192 [D loss: 0.624811, acc: 63.28%] [G loss: 1.896738]\n",
      "epoch:15 step:14193 [D loss: 0.654447, acc: 56.25%] [G loss: 1.932541]\n",
      "epoch:15 step:14194 [D loss: 0.646762, acc: 61.72%] [G loss: 1.982801]\n",
      "epoch:15 step:14195 [D loss: 0.658680, acc: 57.81%] [G loss: 1.894233]\n",
      "epoch:15 step:14196 [D loss: 0.669558, acc: 62.50%] [G loss: 1.930776]\n",
      "epoch:15 step:14197 [D loss: 0.626541, acc: 63.28%] [G loss: 1.922151]\n",
      "epoch:15 step:14198 [D loss: 0.661676, acc: 57.03%] [G loss: 1.764597]\n",
      "epoch:15 step:14199 [D loss: 0.599128, acc: 68.75%] [G loss: 2.014650]\n",
      "epoch:15 step:14200 [D loss: 0.648574, acc: 63.28%] [G loss: 1.918701]\n",
      "##############\n",
      "[2.38073632 1.36013155 6.41817016 4.72015985 3.58246599 5.54006005\n",
      " 4.44219896 4.69464356 4.54602679 3.52816541]\n",
      "##########\n",
      "epoch:15 step:14201 [D loss: 0.650819, acc: 64.06%] [G loss: 1.864508]\n",
      "epoch:15 step:14202 [D loss: 0.667692, acc: 64.06%] [G loss: 1.958144]\n",
      "epoch:15 step:14203 [D loss: 0.676877, acc: 57.81%] [G loss: 1.919107]\n",
      "epoch:15 step:14204 [D loss: 0.616067, acc: 66.41%] [G loss: 2.008193]\n",
      "epoch:15 step:14205 [D loss: 0.641197, acc: 65.62%] [G loss: 1.930430]\n",
      "epoch:15 step:14206 [D loss: 0.618733, acc: 65.62%] [G loss: 2.008151]\n",
      "epoch:15 step:14207 [D loss: 0.617234, acc: 64.06%] [G loss: 1.982120]\n",
      "epoch:15 step:14208 [D loss: 0.654437, acc: 58.59%] [G loss: 1.897197]\n",
      "epoch:15 step:14209 [D loss: 0.681611, acc: 58.59%] [G loss: 2.016996]\n",
      "epoch:15 step:14210 [D loss: 0.616735, acc: 64.06%] [G loss: 1.963860]\n",
      "epoch:15 step:14211 [D loss: 0.637291, acc: 57.03%] [G loss: 2.065674]\n",
      "epoch:15 step:14212 [D loss: 0.635301, acc: 67.97%] [G loss: 1.859492]\n",
      "epoch:15 step:14213 [D loss: 0.642666, acc: 66.41%] [G loss: 1.841608]\n",
      "epoch:15 step:14214 [D loss: 0.654240, acc: 64.06%] [G loss: 1.880051]\n",
      "epoch:15 step:14215 [D loss: 0.697308, acc: 60.94%] [G loss: 1.853904]\n",
      "epoch:15 step:14216 [D loss: 0.689477, acc: 56.25%] [G loss: 1.902049]\n",
      "epoch:15 step:14217 [D loss: 0.626180, acc: 68.75%] [G loss: 1.775372]\n",
      "epoch:15 step:14218 [D loss: 0.666789, acc: 59.38%] [G loss: 1.867675]\n",
      "epoch:15 step:14219 [D loss: 0.622367, acc: 67.97%] [G loss: 1.794816]\n",
      "epoch:15 step:14220 [D loss: 0.654109, acc: 59.38%] [G loss: 1.847699]\n",
      "epoch:15 step:14221 [D loss: 0.597865, acc: 69.53%] [G loss: 1.829538]\n",
      "epoch:15 step:14222 [D loss: 0.636843, acc: 60.16%] [G loss: 1.934901]\n",
      "epoch:15 step:14223 [D loss: 0.648231, acc: 63.28%] [G loss: 2.075468]\n",
      "epoch:15 step:14224 [D loss: 0.653634, acc: 64.06%] [G loss: 1.926054]\n",
      "epoch:15 step:14225 [D loss: 0.651599, acc: 59.38%] [G loss: 1.838295]\n",
      "epoch:15 step:14226 [D loss: 0.623819, acc: 63.28%] [G loss: 1.970803]\n",
      "epoch:15 step:14227 [D loss: 0.616864, acc: 64.84%] [G loss: 1.756982]\n",
      "epoch:15 step:14228 [D loss: 0.620184, acc: 67.97%] [G loss: 1.952126]\n",
      "epoch:15 step:14229 [D loss: 0.655687, acc: 61.72%] [G loss: 1.815816]\n",
      "epoch:15 step:14230 [D loss: 0.651027, acc: 60.94%] [G loss: 1.826971]\n",
      "epoch:15 step:14231 [D loss: 0.623011, acc: 66.41%] [G loss: 1.883962]\n",
      "epoch:15 step:14232 [D loss: 0.680305, acc: 58.59%] [G loss: 1.931822]\n",
      "epoch:15 step:14233 [D loss: 0.638724, acc: 65.62%] [G loss: 1.881095]\n",
      "epoch:15 step:14234 [D loss: 0.673547, acc: 60.16%] [G loss: 1.810271]\n",
      "epoch:15 step:14235 [D loss: 0.686589, acc: 60.16%] [G loss: 1.786207]\n",
      "epoch:15 step:14236 [D loss: 0.676724, acc: 57.03%] [G loss: 1.801970]\n",
      "epoch:15 step:14237 [D loss: 0.705420, acc: 53.12%] [G loss: 1.880375]\n",
      "epoch:15 step:14238 [D loss: 0.693846, acc: 57.81%] [G loss: 1.883653]\n",
      "epoch:15 step:14239 [D loss: 0.643948, acc: 67.97%] [G loss: 1.844666]\n",
      "epoch:15 step:14240 [D loss: 0.655430, acc: 59.38%] [G loss: 1.943842]\n",
      "epoch:15 step:14241 [D loss: 0.624264, acc: 64.06%] [G loss: 1.813210]\n",
      "epoch:15 step:14242 [D loss: 0.683190, acc: 58.59%] [G loss: 1.834229]\n",
      "epoch:15 step:14243 [D loss: 0.636113, acc: 60.16%] [G loss: 1.710809]\n",
      "epoch:15 step:14244 [D loss: 0.648703, acc: 57.03%] [G loss: 1.864006]\n",
      "epoch:15 step:14245 [D loss: 0.619784, acc: 65.62%] [G loss: 2.073033]\n",
      "epoch:15 step:14246 [D loss: 0.616679, acc: 64.06%] [G loss: 1.849143]\n",
      "epoch:15 step:14247 [D loss: 0.625336, acc: 63.28%] [G loss: 1.937818]\n",
      "epoch:15 step:14248 [D loss: 0.683282, acc: 58.59%] [G loss: 1.881639]\n",
      "epoch:15 step:14249 [D loss: 0.608625, acc: 67.97%] [G loss: 1.967558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14250 [D loss: 0.658960, acc: 62.50%] [G loss: 1.874950]\n",
      "epoch:15 step:14251 [D loss: 0.686869, acc: 57.81%] [G loss: 1.779453]\n",
      "epoch:15 step:14252 [D loss: 0.599773, acc: 66.41%] [G loss: 1.933818]\n",
      "epoch:15 step:14253 [D loss: 0.585801, acc: 71.88%] [G loss: 1.890375]\n",
      "epoch:15 step:14254 [D loss: 0.659083, acc: 57.03%] [G loss: 1.989521]\n",
      "epoch:15 step:14255 [D loss: 0.668531, acc: 62.50%] [G loss: 1.877177]\n",
      "epoch:15 step:14256 [D loss: 0.693157, acc: 54.69%] [G loss: 1.767741]\n",
      "epoch:15 step:14257 [D loss: 0.655455, acc: 59.38%] [G loss: 1.810824]\n",
      "epoch:15 step:14258 [D loss: 0.636470, acc: 60.16%] [G loss: 1.760158]\n",
      "epoch:15 step:14259 [D loss: 0.648636, acc: 64.06%] [G loss: 1.735070]\n",
      "epoch:15 step:14260 [D loss: 0.665186, acc: 59.38%] [G loss: 1.870770]\n",
      "epoch:15 step:14261 [D loss: 0.654127, acc: 63.28%] [G loss: 2.006679]\n",
      "epoch:15 step:14262 [D loss: 0.643035, acc: 65.62%] [G loss: 2.035312]\n",
      "epoch:15 step:14263 [D loss: 0.566044, acc: 70.31%] [G loss: 2.310448]\n",
      "epoch:15 step:14264 [D loss: 0.583997, acc: 71.88%] [G loss: 2.224979]\n",
      "epoch:15 step:14265 [D loss: 0.692993, acc: 53.91%] [G loss: 1.908479]\n",
      "epoch:15 step:14266 [D loss: 0.664031, acc: 58.59%] [G loss: 1.825378]\n",
      "epoch:15 step:14267 [D loss: 0.671476, acc: 59.38%] [G loss: 1.969486]\n",
      "epoch:15 step:14268 [D loss: 0.668270, acc: 58.59%] [G loss: 1.851140]\n",
      "epoch:15 step:14269 [D loss: 0.720174, acc: 53.91%] [G loss: 1.810310]\n",
      "epoch:15 step:14270 [D loss: 0.684214, acc: 57.03%] [G loss: 1.874505]\n",
      "epoch:15 step:14271 [D loss: 0.636538, acc: 61.72%] [G loss: 1.908541]\n",
      "epoch:15 step:14272 [D loss: 0.599972, acc: 67.97%] [G loss: 1.895101]\n",
      "epoch:15 step:14273 [D loss: 0.613723, acc: 65.62%] [G loss: 2.185235]\n",
      "epoch:15 step:14274 [D loss: 0.608751, acc: 69.53%] [G loss: 2.071125]\n",
      "epoch:15 step:14275 [D loss: 0.707133, acc: 55.47%] [G loss: 1.714445]\n",
      "epoch:15 step:14276 [D loss: 0.637546, acc: 66.41%] [G loss: 2.015291]\n",
      "epoch:15 step:14277 [D loss: 0.648164, acc: 63.28%] [G loss: 1.943525]\n",
      "epoch:15 step:14278 [D loss: 0.622906, acc: 64.84%] [G loss: 1.885679]\n",
      "epoch:15 step:14279 [D loss: 0.634920, acc: 67.19%] [G loss: 1.924894]\n",
      "epoch:15 step:14280 [D loss: 0.715229, acc: 60.16%] [G loss: 1.882021]\n",
      "epoch:15 step:14281 [D loss: 0.610763, acc: 67.97%] [G loss: 1.723940]\n",
      "epoch:15 step:14282 [D loss: 0.667384, acc: 64.84%] [G loss: 1.860563]\n",
      "epoch:15 step:14283 [D loss: 0.663790, acc: 61.72%] [G loss: 1.804872]\n",
      "epoch:15 step:14284 [D loss: 0.587258, acc: 74.22%] [G loss: 1.961477]\n",
      "epoch:15 step:14285 [D loss: 0.614180, acc: 64.06%] [G loss: 2.168218]\n",
      "epoch:15 step:14286 [D loss: 0.579382, acc: 72.66%] [G loss: 2.297327]\n",
      "epoch:15 step:14287 [D loss: 0.648397, acc: 62.50%] [G loss: 2.323995]\n",
      "epoch:15 step:14288 [D loss: 0.658131, acc: 61.72%] [G loss: 1.829812]\n",
      "epoch:15 step:14289 [D loss: 0.666952, acc: 59.38%] [G loss: 1.679563]\n",
      "epoch:15 step:14290 [D loss: 0.651920, acc: 59.38%] [G loss: 1.795728]\n",
      "epoch:15 step:14291 [D loss: 0.636460, acc: 64.06%] [G loss: 1.939556]\n",
      "epoch:15 step:14292 [D loss: 0.630459, acc: 67.97%] [G loss: 1.888409]\n",
      "epoch:15 step:14293 [D loss: 0.642328, acc: 64.84%] [G loss: 1.840091]\n",
      "epoch:15 step:14294 [D loss: 0.677899, acc: 64.06%] [G loss: 1.902035]\n",
      "epoch:15 step:14295 [D loss: 0.681767, acc: 55.47%] [G loss: 1.809844]\n",
      "epoch:15 step:14296 [D loss: 0.643300, acc: 60.94%] [G loss: 1.942410]\n",
      "epoch:15 step:14297 [D loss: 0.610201, acc: 65.62%] [G loss: 1.915501]\n",
      "epoch:15 step:14298 [D loss: 0.647173, acc: 63.28%] [G loss: 1.794429]\n",
      "epoch:15 step:14299 [D loss: 0.639324, acc: 61.72%] [G loss: 1.912427]\n",
      "epoch:15 step:14300 [D loss: 0.662586, acc: 61.72%] [G loss: 1.965585]\n",
      "epoch:15 step:14301 [D loss: 0.624306, acc: 68.75%] [G loss: 1.941742]\n",
      "epoch:15 step:14302 [D loss: 0.620957, acc: 64.06%] [G loss: 1.950736]\n",
      "epoch:15 step:14303 [D loss: 0.615751, acc: 67.97%] [G loss: 1.974253]\n",
      "epoch:15 step:14304 [D loss: 0.667237, acc: 56.25%] [G loss: 1.892802]\n",
      "epoch:15 step:14305 [D loss: 0.741024, acc: 46.09%] [G loss: 1.735547]\n",
      "epoch:15 step:14306 [D loss: 0.699714, acc: 55.47%] [G loss: 1.779521]\n",
      "epoch:15 step:14307 [D loss: 0.625657, acc: 62.50%] [G loss: 1.971539]\n",
      "epoch:15 step:14308 [D loss: 0.671218, acc: 55.47%] [G loss: 1.875054]\n",
      "epoch:15 step:14309 [D loss: 0.685361, acc: 53.91%] [G loss: 1.785677]\n",
      "epoch:15 step:14310 [D loss: 0.681347, acc: 63.28%] [G loss: 1.820477]\n",
      "epoch:15 step:14311 [D loss: 0.642077, acc: 60.94%] [G loss: 1.872423]\n",
      "epoch:15 step:14312 [D loss: 0.631911, acc: 67.19%] [G loss: 1.839576]\n",
      "epoch:15 step:14313 [D loss: 0.621690, acc: 63.28%] [G loss: 1.803954]\n",
      "epoch:15 step:14314 [D loss: 0.623799, acc: 65.62%] [G loss: 1.952429]\n",
      "epoch:15 step:14315 [D loss: 0.624589, acc: 62.50%] [G loss: 1.865368]\n",
      "epoch:15 step:14316 [D loss: 0.657268, acc: 64.84%] [G loss: 1.892086]\n",
      "epoch:15 step:14317 [D loss: 0.578458, acc: 71.09%] [G loss: 1.989970]\n",
      "epoch:15 step:14318 [D loss: 0.675700, acc: 58.59%] [G loss: 1.858036]\n",
      "epoch:15 step:14319 [D loss: 0.648515, acc: 61.72%] [G loss: 1.970615]\n",
      "epoch:15 step:14320 [D loss: 0.655876, acc: 60.16%] [G loss: 1.826382]\n",
      "epoch:15 step:14321 [D loss: 0.647207, acc: 60.94%] [G loss: 1.945590]\n",
      "epoch:15 step:14322 [D loss: 0.642539, acc: 64.06%] [G loss: 1.857670]\n",
      "epoch:15 step:14323 [D loss: 0.684329, acc: 57.81%] [G loss: 1.791370]\n",
      "epoch:15 step:14324 [D loss: 0.665986, acc: 62.50%] [G loss: 1.855362]\n",
      "epoch:15 step:14325 [D loss: 0.643075, acc: 66.41%] [G loss: 2.022927]\n",
      "epoch:15 step:14326 [D loss: 0.635710, acc: 66.41%] [G loss: 2.069157]\n",
      "epoch:15 step:14327 [D loss: 0.687024, acc: 62.50%] [G loss: 2.024893]\n",
      "epoch:15 step:14328 [D loss: 0.641437, acc: 62.50%] [G loss: 1.845077]\n",
      "epoch:15 step:14329 [D loss: 0.612884, acc: 67.19%] [G loss: 2.035171]\n",
      "epoch:15 step:14330 [D loss: 0.661117, acc: 58.59%] [G loss: 2.012937]\n",
      "epoch:15 step:14331 [D loss: 0.611159, acc: 64.84%] [G loss: 2.183869]\n",
      "epoch:15 step:14332 [D loss: 0.655828, acc: 59.38%] [G loss: 1.904476]\n",
      "epoch:15 step:14333 [D loss: 0.664828, acc: 57.81%] [G loss: 1.958271]\n",
      "epoch:15 step:14334 [D loss: 0.653890, acc: 59.38%] [G loss: 1.867592]\n",
      "epoch:15 step:14335 [D loss: 0.678300, acc: 60.94%] [G loss: 1.870486]\n",
      "epoch:15 step:14336 [D loss: 0.675673, acc: 61.72%] [G loss: 1.842643]\n",
      "epoch:15 step:14337 [D loss: 0.638930, acc: 65.62%] [G loss: 1.881541]\n",
      "epoch:15 step:14338 [D loss: 0.621075, acc: 64.06%] [G loss: 1.851304]\n",
      "epoch:15 step:14339 [D loss: 0.639028, acc: 64.06%] [G loss: 1.840020]\n",
      "epoch:15 step:14340 [D loss: 0.685706, acc: 56.25%] [G loss: 1.864893]\n",
      "epoch:15 step:14341 [D loss: 0.616895, acc: 68.75%] [G loss: 2.073523]\n",
      "epoch:15 step:14342 [D loss: 0.616643, acc: 61.72%] [G loss: 1.971729]\n",
      "epoch:15 step:14343 [D loss: 0.651948, acc: 67.19%] [G loss: 2.004805]\n",
      "epoch:15 step:14344 [D loss: 0.619688, acc: 70.31%] [G loss: 1.950812]\n",
      "epoch:15 step:14345 [D loss: 0.609022, acc: 68.75%] [G loss: 1.948043]\n",
      "epoch:15 step:14346 [D loss: 0.635996, acc: 67.97%] [G loss: 1.939177]\n",
      "epoch:15 step:14347 [D loss: 0.703937, acc: 55.47%] [G loss: 1.767929]\n",
      "epoch:15 step:14348 [D loss: 0.675140, acc: 58.59%] [G loss: 1.993478]\n",
      "epoch:15 step:14349 [D loss: 0.687093, acc: 58.59%] [G loss: 1.821906]\n",
      "epoch:15 step:14350 [D loss: 0.652766, acc: 57.03%] [G loss: 1.865877]\n",
      "epoch:15 step:14351 [D loss: 0.585913, acc: 71.88%] [G loss: 2.016517]\n",
      "epoch:15 step:14352 [D loss: 0.663359, acc: 61.72%] [G loss: 1.869616]\n",
      "epoch:15 step:14353 [D loss: 0.610075, acc: 69.53%] [G loss: 2.076641]\n",
      "epoch:15 step:14354 [D loss: 0.615142, acc: 67.19%] [G loss: 1.926443]\n",
      "epoch:15 step:14355 [D loss: 0.565885, acc: 72.66%] [G loss: 1.956749]\n",
      "epoch:15 step:14356 [D loss: 0.683090, acc: 61.72%] [G loss: 1.846910]\n",
      "epoch:15 step:14357 [D loss: 0.602308, acc: 67.19%] [G loss: 1.954391]\n",
      "epoch:15 step:14358 [D loss: 0.643431, acc: 60.94%] [G loss: 2.004421]\n",
      "epoch:15 step:14359 [D loss: 0.684005, acc: 59.38%] [G loss: 1.843607]\n",
      "epoch:15 step:14360 [D loss: 0.609093, acc: 65.62%] [G loss: 1.915749]\n",
      "epoch:15 step:14361 [D loss: 0.620788, acc: 62.50%] [G loss: 1.834598]\n",
      "epoch:15 step:14362 [D loss: 0.711537, acc: 53.91%] [G loss: 1.877053]\n",
      "epoch:15 step:14363 [D loss: 0.605342, acc: 71.09%] [G loss: 1.913423]\n",
      "epoch:15 step:14364 [D loss: 0.616252, acc: 65.62%] [G loss: 1.898726]\n",
      "epoch:15 step:14365 [D loss: 0.666036, acc: 60.16%] [G loss: 1.893995]\n",
      "epoch:15 step:14366 [D loss: 0.655147, acc: 60.16%] [G loss: 1.916270]\n",
      "epoch:15 step:14367 [D loss: 0.636020, acc: 64.06%] [G loss: 2.206917]\n",
      "epoch:15 step:14368 [D loss: 0.608178, acc: 67.97%] [G loss: 2.212204]\n",
      "epoch:15 step:14369 [D loss: 0.583145, acc: 68.75%] [G loss: 2.358216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14370 [D loss: 0.592368, acc: 66.41%] [G loss: 2.365299]\n",
      "epoch:15 step:14371 [D loss: 0.688244, acc: 60.16%] [G loss: 1.767493]\n",
      "epoch:15 step:14372 [D loss: 0.693497, acc: 55.47%] [G loss: 1.933004]\n",
      "epoch:15 step:14373 [D loss: 0.655162, acc: 57.81%] [G loss: 1.958773]\n",
      "epoch:15 step:14374 [D loss: 0.625741, acc: 64.84%] [G loss: 1.861817]\n",
      "epoch:15 step:14375 [D loss: 0.651289, acc: 62.50%] [G loss: 1.971752]\n",
      "epoch:15 step:14376 [D loss: 0.595579, acc: 71.88%] [G loss: 1.986378]\n",
      "epoch:15 step:14377 [D loss: 0.650235, acc: 58.59%] [G loss: 2.031999]\n",
      "epoch:15 step:14378 [D loss: 0.645812, acc: 62.50%] [G loss: 1.740856]\n",
      "epoch:15 step:14379 [D loss: 0.665873, acc: 62.50%] [G loss: 1.882714]\n",
      "epoch:15 step:14380 [D loss: 0.586873, acc: 71.09%] [G loss: 1.968915]\n",
      "epoch:15 step:14381 [D loss: 0.683027, acc: 57.03%] [G loss: 1.949788]\n",
      "epoch:15 step:14382 [D loss: 0.736181, acc: 52.34%] [G loss: 1.861324]\n",
      "epoch:15 step:14383 [D loss: 0.679223, acc: 56.25%] [G loss: 1.806760]\n",
      "epoch:15 step:14384 [D loss: 0.670676, acc: 58.59%] [G loss: 1.922583]\n",
      "epoch:15 step:14385 [D loss: 0.686446, acc: 60.94%] [G loss: 2.067288]\n",
      "epoch:15 step:14386 [D loss: 0.636215, acc: 60.16%] [G loss: 1.847486]\n",
      "epoch:15 step:14387 [D loss: 0.596269, acc: 72.66%] [G loss: 1.897121]\n",
      "epoch:15 step:14388 [D loss: 0.650986, acc: 63.28%] [G loss: 1.954081]\n",
      "epoch:15 step:14389 [D loss: 0.687676, acc: 55.47%] [G loss: 2.023446]\n",
      "epoch:15 step:14390 [D loss: 0.620894, acc: 60.94%] [G loss: 2.025787]\n",
      "epoch:15 step:14391 [D loss: 0.641879, acc: 61.72%] [G loss: 2.186988]\n",
      "epoch:15 step:14392 [D loss: 0.659571, acc: 64.84%] [G loss: 1.987737]\n",
      "epoch:15 step:14393 [D loss: 0.632648, acc: 64.84%] [G loss: 1.973224]\n",
      "epoch:15 step:14394 [D loss: 0.629656, acc: 67.97%] [G loss: 2.051422]\n",
      "epoch:15 step:14395 [D loss: 0.600869, acc: 71.09%] [G loss: 1.985881]\n",
      "epoch:15 step:14396 [D loss: 0.702999, acc: 55.47%] [G loss: 1.891107]\n",
      "epoch:15 step:14397 [D loss: 0.683252, acc: 59.38%] [G loss: 1.870162]\n",
      "epoch:15 step:14398 [D loss: 0.690837, acc: 57.81%] [G loss: 2.014166]\n",
      "epoch:15 step:14399 [D loss: 0.661845, acc: 60.94%] [G loss: 1.890747]\n",
      "epoch:15 step:14400 [D loss: 0.631065, acc: 63.28%] [G loss: 2.156358]\n",
      "##############\n",
      "[2.56895407 1.38002126 6.36664216 4.85540385 3.73858716 5.56447615\n",
      " 4.23807242 4.72726665 4.47705268 3.5215351 ]\n",
      "##########\n",
      "epoch:15 step:14401 [D loss: 0.589406, acc: 72.66%] [G loss: 2.137919]\n",
      "epoch:15 step:14402 [D loss: 0.555225, acc: 72.66%] [G loss: 2.245567]\n",
      "epoch:15 step:14403 [D loss: 0.658981, acc: 57.81%] [G loss: 1.958015]\n",
      "epoch:15 step:14404 [D loss: 0.733485, acc: 53.12%] [G loss: 1.738625]\n",
      "epoch:15 step:14405 [D loss: 0.657331, acc: 62.50%] [G loss: 1.821526]\n",
      "epoch:15 step:14406 [D loss: 0.645963, acc: 64.06%] [G loss: 1.890769]\n",
      "epoch:15 step:14407 [D loss: 0.684073, acc: 54.69%] [G loss: 1.883322]\n",
      "epoch:15 step:14408 [D loss: 0.641257, acc: 61.72%] [G loss: 1.882645]\n",
      "epoch:15 step:14409 [D loss: 0.628437, acc: 64.84%] [G loss: 1.993413]\n",
      "epoch:15 step:14410 [D loss: 0.653947, acc: 63.28%] [G loss: 1.787596]\n",
      "epoch:15 step:14411 [D loss: 0.673648, acc: 56.25%] [G loss: 1.903813]\n",
      "epoch:15 step:14412 [D loss: 0.640559, acc: 65.62%] [G loss: 1.894522]\n",
      "epoch:15 step:14413 [D loss: 0.616850, acc: 66.41%] [G loss: 2.048286]\n",
      "epoch:15 step:14414 [D loss: 0.619218, acc: 71.09%] [G loss: 2.101914]\n",
      "epoch:15 step:14415 [D loss: 0.641091, acc: 61.72%] [G loss: 1.932625]\n",
      "epoch:15 step:14416 [D loss: 0.675483, acc: 58.59%] [G loss: 1.830364]\n",
      "epoch:15 step:14417 [D loss: 0.616070, acc: 60.94%] [G loss: 1.915685]\n",
      "epoch:15 step:14418 [D loss: 0.635712, acc: 64.84%] [G loss: 1.922730]\n",
      "epoch:15 step:14419 [D loss: 0.599009, acc: 68.75%] [G loss: 1.954056]\n",
      "epoch:15 step:14420 [D loss: 0.630674, acc: 69.53%] [G loss: 1.974848]\n",
      "epoch:15 step:14421 [D loss: 0.619922, acc: 64.06%] [G loss: 2.038160]\n",
      "epoch:15 step:14422 [D loss: 0.651162, acc: 57.81%] [G loss: 1.916121]\n",
      "epoch:15 step:14423 [D loss: 0.586477, acc: 72.66%] [G loss: 1.959895]\n",
      "epoch:15 step:14424 [D loss: 0.660597, acc: 64.06%] [G loss: 1.807405]\n",
      "epoch:15 step:14425 [D loss: 0.625892, acc: 65.62%] [G loss: 1.992426]\n",
      "epoch:15 step:14426 [D loss: 0.617518, acc: 67.19%] [G loss: 2.135290]\n",
      "epoch:15 step:14427 [D loss: 0.641131, acc: 66.41%] [G loss: 1.910705]\n",
      "epoch:15 step:14428 [D loss: 0.696464, acc: 56.25%] [G loss: 1.773633]\n",
      "epoch:15 step:14429 [D loss: 0.667353, acc: 61.72%] [G loss: 1.976409]\n",
      "epoch:15 step:14430 [D loss: 0.642520, acc: 59.38%] [G loss: 1.747566]\n",
      "epoch:15 step:14431 [D loss: 0.706708, acc: 55.47%] [G loss: 1.846453]\n",
      "epoch:15 step:14432 [D loss: 0.648350, acc: 64.06%] [G loss: 1.866892]\n",
      "epoch:15 step:14433 [D loss: 0.630780, acc: 65.62%] [G loss: 1.971665]\n",
      "epoch:15 step:14434 [D loss: 0.667230, acc: 63.28%] [G loss: 1.961055]\n",
      "epoch:15 step:14435 [D loss: 0.633791, acc: 64.06%] [G loss: 2.000458]\n",
      "epoch:15 step:14436 [D loss: 0.621617, acc: 67.97%] [G loss: 2.059265]\n",
      "epoch:15 step:14437 [D loss: 0.660293, acc: 57.81%] [G loss: 1.768634]\n",
      "epoch:15 step:14438 [D loss: 0.611865, acc: 67.19%] [G loss: 2.031913]\n",
      "epoch:15 step:14439 [D loss: 0.638464, acc: 63.28%] [G loss: 1.950697]\n",
      "epoch:15 step:14440 [D loss: 0.611670, acc: 69.53%] [G loss: 1.891620]\n",
      "epoch:15 step:14441 [D loss: 0.676138, acc: 59.38%] [G loss: 1.763144]\n",
      "epoch:15 step:14442 [D loss: 0.664675, acc: 59.38%] [G loss: 1.757296]\n",
      "epoch:15 step:14443 [D loss: 0.685073, acc: 59.38%] [G loss: 1.786414]\n",
      "epoch:15 step:14444 [D loss: 0.602456, acc: 63.28%] [G loss: 1.884869]\n",
      "epoch:15 step:14445 [D loss: 0.623520, acc: 62.50%] [G loss: 1.875998]\n",
      "epoch:15 step:14446 [D loss: 0.695958, acc: 59.38%] [G loss: 1.779854]\n",
      "epoch:15 step:14447 [D loss: 0.638512, acc: 64.06%] [G loss: 1.881258]\n",
      "epoch:15 step:14448 [D loss: 0.637509, acc: 63.28%] [G loss: 1.869590]\n",
      "epoch:15 step:14449 [D loss: 0.630915, acc: 63.28%] [G loss: 1.810541]\n",
      "epoch:15 step:14450 [D loss: 0.644919, acc: 64.06%] [G loss: 1.923954]\n",
      "epoch:15 step:14451 [D loss: 0.681836, acc: 53.91%] [G loss: 1.875814]\n",
      "epoch:15 step:14452 [D loss: 0.652429, acc: 57.81%] [G loss: 1.699636]\n",
      "epoch:15 step:14453 [D loss: 0.623402, acc: 69.53%] [G loss: 1.889029]\n",
      "epoch:15 step:14454 [D loss: 0.626239, acc: 65.62%] [G loss: 2.010523]\n",
      "epoch:15 step:14455 [D loss: 0.645279, acc: 59.38%] [G loss: 1.890054]\n",
      "epoch:15 step:14456 [D loss: 0.654942, acc: 62.50%] [G loss: 1.865667]\n",
      "epoch:15 step:14457 [D loss: 0.603925, acc: 67.97%] [G loss: 1.975601]\n",
      "epoch:15 step:14458 [D loss: 0.576415, acc: 69.53%] [G loss: 2.106148]\n",
      "epoch:15 step:14459 [D loss: 0.641118, acc: 63.28%] [G loss: 2.036585]\n",
      "epoch:15 step:14460 [D loss: 0.608624, acc: 71.09%] [G loss: 2.010758]\n",
      "epoch:15 step:14461 [D loss: 0.608660, acc: 68.75%] [G loss: 2.189075]\n",
      "epoch:15 step:14462 [D loss: 0.668428, acc: 54.69%] [G loss: 1.828886]\n",
      "epoch:15 step:14463 [D loss: 0.666582, acc: 57.81%] [G loss: 1.958936]\n",
      "epoch:15 step:14464 [D loss: 0.682546, acc: 54.69%] [G loss: 1.943654]\n",
      "epoch:15 step:14465 [D loss: 0.678035, acc: 60.94%] [G loss: 1.961628]\n",
      "epoch:15 step:14466 [D loss: 0.612693, acc: 67.97%] [G loss: 1.926436]\n",
      "epoch:15 step:14467 [D loss: 0.642020, acc: 67.97%] [G loss: 1.857281]\n",
      "epoch:15 step:14468 [D loss: 0.659690, acc: 60.94%] [G loss: 2.012847]\n",
      "epoch:15 step:14469 [D loss: 0.586022, acc: 71.09%] [G loss: 2.033446]\n",
      "epoch:15 step:14470 [D loss: 0.748492, acc: 62.50%] [G loss: 1.973481]\n",
      "epoch:15 step:14471 [D loss: 0.616073, acc: 59.38%] [G loss: 1.945091]\n",
      "epoch:15 step:14472 [D loss: 0.619784, acc: 64.06%] [G loss: 1.867449]\n",
      "epoch:15 step:14473 [D loss: 0.691887, acc: 57.81%] [G loss: 1.794377]\n",
      "epoch:15 step:14474 [D loss: 0.650147, acc: 58.59%] [G loss: 1.998594]\n",
      "epoch:15 step:14475 [D loss: 0.658626, acc: 63.28%] [G loss: 1.954704]\n",
      "epoch:15 step:14476 [D loss: 0.629472, acc: 66.41%] [G loss: 1.885945]\n",
      "epoch:15 step:14477 [D loss: 0.683681, acc: 57.03%] [G loss: 1.888552]\n",
      "epoch:15 step:14478 [D loss: 0.658658, acc: 62.50%] [G loss: 1.826913]\n",
      "epoch:15 step:14479 [D loss: 0.628891, acc: 61.72%] [G loss: 1.950640]\n",
      "epoch:15 step:14480 [D loss: 0.663856, acc: 62.50%] [G loss: 1.840876]\n",
      "epoch:15 step:14481 [D loss: 0.691165, acc: 55.47%] [G loss: 1.859766]\n",
      "epoch:15 step:14482 [D loss: 0.616974, acc: 66.41%] [G loss: 2.026084]\n",
      "epoch:15 step:14483 [D loss: 0.574878, acc: 70.31%] [G loss: 2.057056]\n",
      "epoch:15 step:14484 [D loss: 0.628607, acc: 64.06%] [G loss: 2.106507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14485 [D loss: 0.588868, acc: 67.19%] [G loss: 2.148149]\n",
      "epoch:15 step:14486 [D loss: 0.716109, acc: 59.38%] [G loss: 1.827235]\n",
      "epoch:15 step:14487 [D loss: 0.659128, acc: 61.72%] [G loss: 1.756972]\n",
      "epoch:15 step:14488 [D loss: 0.691136, acc: 54.69%] [G loss: 1.816476]\n",
      "epoch:15 step:14489 [D loss: 0.573495, acc: 71.09%] [G loss: 2.059151]\n",
      "epoch:15 step:14490 [D loss: 0.657652, acc: 62.50%] [G loss: 1.824358]\n",
      "epoch:15 step:14491 [D loss: 0.587620, acc: 67.19%] [G loss: 2.110857]\n",
      "epoch:15 step:14492 [D loss: 0.711324, acc: 54.69%] [G loss: 1.778894]\n",
      "epoch:15 step:14493 [D loss: 0.684380, acc: 56.25%] [G loss: 1.780510]\n",
      "epoch:15 step:14494 [D loss: 0.680529, acc: 57.03%] [G loss: 1.695720]\n",
      "epoch:15 step:14495 [D loss: 0.665710, acc: 62.50%] [G loss: 1.707802]\n",
      "epoch:15 step:14496 [D loss: 0.686424, acc: 57.03%] [G loss: 1.828493]\n",
      "epoch:15 step:14497 [D loss: 0.671171, acc: 57.03%] [G loss: 1.773794]\n",
      "epoch:15 step:14498 [D loss: 0.665878, acc: 60.94%] [G loss: 1.784332]\n",
      "epoch:15 step:14499 [D loss: 0.671755, acc: 59.38%] [G loss: 1.837237]\n",
      "epoch:15 step:14500 [D loss: 0.641366, acc: 62.50%] [G loss: 1.843579]\n",
      "epoch:15 step:14501 [D loss: 0.649806, acc: 64.06%] [G loss: 1.830463]\n",
      "epoch:15 step:14502 [D loss: 0.614990, acc: 64.84%] [G loss: 1.870163]\n",
      "epoch:15 step:14503 [D loss: 0.697886, acc: 58.59%] [G loss: 1.717219]\n",
      "epoch:15 step:14504 [D loss: 0.646905, acc: 63.28%] [G loss: 1.808650]\n",
      "epoch:15 step:14505 [D loss: 0.619726, acc: 70.31%] [G loss: 1.842861]\n",
      "epoch:15 step:14506 [D loss: 0.590207, acc: 73.44%] [G loss: 2.017256]\n",
      "epoch:15 step:14507 [D loss: 0.612624, acc: 64.84%] [G loss: 2.025319]\n",
      "epoch:15 step:14508 [D loss: 0.589857, acc: 70.31%] [G loss: 1.971129]\n",
      "epoch:15 step:14509 [D loss: 0.686010, acc: 56.25%] [G loss: 1.904619]\n",
      "epoch:15 step:14510 [D loss: 0.617550, acc: 64.84%] [G loss: 1.993591]\n",
      "epoch:15 step:14511 [D loss: 0.644875, acc: 63.28%] [G loss: 1.944560]\n",
      "epoch:15 step:14512 [D loss: 0.621058, acc: 63.28%] [G loss: 2.035802]\n",
      "epoch:15 step:14513 [D loss: 0.722883, acc: 50.78%] [G loss: 1.940832]\n",
      "epoch:15 step:14514 [D loss: 0.664907, acc: 60.16%] [G loss: 1.916468]\n",
      "epoch:15 step:14515 [D loss: 0.670615, acc: 57.03%] [G loss: 1.826443]\n",
      "epoch:15 step:14516 [D loss: 0.700857, acc: 56.25%] [G loss: 1.924764]\n",
      "epoch:15 step:14517 [D loss: 0.683503, acc: 59.38%] [G loss: 1.802986]\n",
      "epoch:15 step:14518 [D loss: 0.638691, acc: 64.06%] [G loss: 1.860416]\n",
      "epoch:15 step:14519 [D loss: 0.692445, acc: 56.25%] [G loss: 1.901112]\n",
      "epoch:15 step:14520 [D loss: 0.683312, acc: 61.72%] [G loss: 1.924876]\n",
      "epoch:15 step:14521 [D loss: 0.673056, acc: 53.91%] [G loss: 1.836612]\n",
      "epoch:15 step:14522 [D loss: 0.677737, acc: 64.06%] [G loss: 1.790371]\n",
      "epoch:15 step:14523 [D loss: 0.644562, acc: 68.75%] [G loss: 1.993695]\n",
      "epoch:15 step:14524 [D loss: 0.574047, acc: 73.44%] [G loss: 1.985746]\n",
      "epoch:15 step:14525 [D loss: 0.613092, acc: 66.41%] [G loss: 2.062160]\n",
      "epoch:15 step:14526 [D loss: 0.561829, acc: 70.31%] [G loss: 2.192903]\n",
      "epoch:15 step:14527 [D loss: 0.634638, acc: 60.94%] [G loss: 1.996166]\n",
      "epoch:15 step:14528 [D loss: 0.696000, acc: 53.91%] [G loss: 1.775243]\n",
      "epoch:15 step:14529 [D loss: 0.648825, acc: 63.28%] [G loss: 1.789604]\n",
      "epoch:15 step:14530 [D loss: 0.660294, acc: 63.28%] [G loss: 1.836787]\n",
      "epoch:15 step:14531 [D loss: 0.600514, acc: 65.62%] [G loss: 2.008796]\n",
      "epoch:15 step:14532 [D loss: 0.654369, acc: 57.81%] [G loss: 1.755796]\n",
      "epoch:15 step:14533 [D loss: 0.656940, acc: 61.72%] [G loss: 1.848153]\n",
      "epoch:15 step:14534 [D loss: 0.641061, acc: 71.09%] [G loss: 1.882016]\n",
      "epoch:15 step:14535 [D loss: 0.662663, acc: 64.06%] [G loss: 2.209683]\n",
      "epoch:15 step:14536 [D loss: 0.620322, acc: 64.84%] [G loss: 2.086987]\n",
      "epoch:15 step:14537 [D loss: 0.685272, acc: 54.69%] [G loss: 1.767905]\n",
      "epoch:15 step:14538 [D loss: 0.654693, acc: 63.28%] [G loss: 1.888594]\n",
      "epoch:15 step:14539 [D loss: 0.639455, acc: 62.50%] [G loss: 1.927125]\n",
      "epoch:15 step:14540 [D loss: 0.664825, acc: 57.81%] [G loss: 1.942688]\n",
      "epoch:15 step:14541 [D loss: 0.690251, acc: 57.81%] [G loss: 1.826179]\n",
      "epoch:15 step:14542 [D loss: 0.608887, acc: 72.66%] [G loss: 1.856124]\n",
      "epoch:15 step:14543 [D loss: 0.648300, acc: 60.94%] [G loss: 2.019489]\n",
      "epoch:15 step:14544 [D loss: 0.613241, acc: 71.88%] [G loss: 1.764261]\n",
      "epoch:15 step:14545 [D loss: 0.650359, acc: 67.19%] [G loss: 1.891084]\n",
      "epoch:15 step:14546 [D loss: 0.638297, acc: 59.38%] [G loss: 1.883241]\n",
      "epoch:15 step:14547 [D loss: 0.644359, acc: 63.28%] [G loss: 1.843994]\n",
      "epoch:15 step:14548 [D loss: 0.639700, acc: 65.62%] [G loss: 1.808668]\n",
      "epoch:15 step:14549 [D loss: 0.592300, acc: 67.97%] [G loss: 1.877529]\n",
      "epoch:15 step:14550 [D loss: 0.596017, acc: 71.09%] [G loss: 2.173553]\n",
      "epoch:15 step:14551 [D loss: 0.631301, acc: 67.97%] [G loss: 1.918072]\n",
      "epoch:15 step:14552 [D loss: 0.619961, acc: 64.84%] [G loss: 1.998615]\n",
      "epoch:15 step:14553 [D loss: 0.640821, acc: 65.62%] [G loss: 1.898141]\n",
      "epoch:15 step:14554 [D loss: 0.590175, acc: 71.09%] [G loss: 2.018432]\n",
      "epoch:15 step:14555 [D loss: 0.677803, acc: 54.69%] [G loss: 1.747734]\n",
      "epoch:15 step:14556 [D loss: 0.666835, acc: 57.81%] [G loss: 1.864491]\n",
      "epoch:15 step:14557 [D loss: 0.699089, acc: 58.59%] [G loss: 1.766493]\n",
      "epoch:15 step:14558 [D loss: 0.634129, acc: 61.72%] [G loss: 1.827214]\n",
      "epoch:15 step:14559 [D loss: 0.671129, acc: 59.38%] [G loss: 1.953227]\n",
      "epoch:15 step:14560 [D loss: 0.704350, acc: 56.25%] [G loss: 1.911716]\n",
      "epoch:15 step:14561 [D loss: 0.659027, acc: 60.94%] [G loss: 1.767652]\n",
      "epoch:15 step:14562 [D loss: 0.718716, acc: 49.22%] [G loss: 1.752133]\n",
      "epoch:15 step:14563 [D loss: 0.600701, acc: 67.97%] [G loss: 2.014341]\n",
      "epoch:15 step:14564 [D loss: 0.654078, acc: 62.50%] [G loss: 1.921635]\n",
      "epoch:15 step:14565 [D loss: 0.636423, acc: 60.16%] [G loss: 1.823328]\n",
      "epoch:15 step:14566 [D loss: 0.643885, acc: 66.41%] [G loss: 1.805645]\n",
      "epoch:15 step:14567 [D loss: 0.647488, acc: 65.62%] [G loss: 1.927570]\n",
      "epoch:15 step:14568 [D loss: 0.632537, acc: 62.50%] [G loss: 1.834722]\n",
      "epoch:15 step:14569 [D loss: 0.624838, acc: 61.72%] [G loss: 1.890888]\n",
      "epoch:15 step:14570 [D loss: 0.631944, acc: 66.41%] [G loss: 2.074544]\n",
      "epoch:15 step:14571 [D loss: 0.604424, acc: 70.31%] [G loss: 1.950646]\n",
      "epoch:15 step:14572 [D loss: 0.647133, acc: 64.06%] [G loss: 2.111790]\n",
      "epoch:15 step:14573 [D loss: 0.670217, acc: 60.94%] [G loss: 1.802599]\n",
      "epoch:15 step:14574 [D loss: 0.670690, acc: 60.16%] [G loss: 1.816360]\n",
      "epoch:15 step:14575 [D loss: 0.610637, acc: 65.62%] [G loss: 1.933402]\n",
      "epoch:15 step:14576 [D loss: 0.635871, acc: 64.06%] [G loss: 2.117088]\n",
      "epoch:15 step:14577 [D loss: 0.620022, acc: 61.72%] [G loss: 1.954111]\n",
      "epoch:15 step:14578 [D loss: 0.657372, acc: 58.59%] [G loss: 2.002625]\n",
      "epoch:15 step:14579 [D loss: 0.615816, acc: 64.06%] [G loss: 1.926189]\n",
      "epoch:15 step:14580 [D loss: 0.685028, acc: 63.28%] [G loss: 1.905971]\n",
      "epoch:15 step:14581 [D loss: 0.653810, acc: 57.81%] [G loss: 1.824389]\n",
      "epoch:15 step:14582 [D loss: 0.666824, acc: 60.16%] [G loss: 1.816166]\n",
      "epoch:15 step:14583 [D loss: 0.695563, acc: 55.47%] [G loss: 1.714578]\n",
      "epoch:15 step:14584 [D loss: 0.687455, acc: 60.94%] [G loss: 1.803192]\n",
      "epoch:15 step:14585 [D loss: 0.619369, acc: 67.19%] [G loss: 1.876486]\n",
      "epoch:15 step:14586 [D loss: 0.708966, acc: 47.66%] [G loss: 1.752466]\n",
      "epoch:15 step:14587 [D loss: 0.616714, acc: 65.62%] [G loss: 2.020959]\n",
      "epoch:15 step:14588 [D loss: 0.662488, acc: 59.38%] [G loss: 1.973063]\n",
      "epoch:15 step:14589 [D loss: 0.593075, acc: 69.53%] [G loss: 1.969383]\n",
      "epoch:15 step:14590 [D loss: 0.662256, acc: 57.03%] [G loss: 1.771734]\n",
      "epoch:15 step:14591 [D loss: 0.660212, acc: 60.94%] [G loss: 2.009602]\n",
      "epoch:15 step:14592 [D loss: 0.630801, acc: 65.62%] [G loss: 1.800580]\n",
      "epoch:15 step:14593 [D loss: 0.708352, acc: 53.12%] [G loss: 1.725272]\n",
      "epoch:15 step:14594 [D loss: 0.633072, acc: 67.19%] [G loss: 1.946502]\n",
      "epoch:15 step:14595 [D loss: 0.670692, acc: 60.16%] [G loss: 1.906827]\n",
      "epoch:15 step:14596 [D loss: 0.630070, acc: 67.19%] [G loss: 1.842593]\n",
      "epoch:15 step:14597 [D loss: 0.672916, acc: 61.72%] [G loss: 1.843025]\n",
      "epoch:15 step:14598 [D loss: 0.675094, acc: 62.50%] [G loss: 1.744609]\n",
      "epoch:15 step:14599 [D loss: 0.611840, acc: 63.28%] [G loss: 1.790697]\n",
      "epoch:15 step:14600 [D loss: 0.656470, acc: 62.50%] [G loss: 2.001947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.51106618 1.58708773 6.36107032 4.91104361 3.77149535 5.69302357\n",
      " 4.48497385 4.65867705 4.79184438 3.56035728]\n",
      "##########\n",
      "epoch:15 step:14601 [D loss: 0.638566, acc: 64.84%] [G loss: 1.782959]\n",
      "epoch:15 step:14602 [D loss: 0.621162, acc: 66.41%] [G loss: 1.873941]\n",
      "epoch:15 step:14603 [D loss: 0.602019, acc: 67.19%] [G loss: 1.920833]\n",
      "epoch:15 step:14604 [D loss: 0.596881, acc: 66.41%] [G loss: 1.846850]\n",
      "epoch:15 step:14605 [D loss: 0.657894, acc: 60.16%] [G loss: 1.886026]\n",
      "epoch:15 step:14606 [D loss: 0.619307, acc: 69.53%] [G loss: 1.870960]\n",
      "epoch:15 step:14607 [D loss: 0.597580, acc: 67.97%] [G loss: 1.996846]\n",
      "epoch:15 step:14608 [D loss: 0.646392, acc: 57.81%] [G loss: 1.875442]\n",
      "epoch:15 step:14609 [D loss: 0.593503, acc: 68.75%] [G loss: 2.085886]\n",
      "epoch:15 step:14610 [D loss: 0.601228, acc: 64.84%] [G loss: 2.025863]\n",
      "epoch:15 step:14611 [D loss: 0.613124, acc: 64.06%] [G loss: 2.010860]\n",
      "epoch:15 step:14612 [D loss: 0.674139, acc: 58.59%] [G loss: 1.892743]\n",
      "epoch:15 step:14613 [D loss: 0.632585, acc: 62.50%] [G loss: 1.942792]\n",
      "epoch:15 step:14614 [D loss: 0.713516, acc: 56.25%] [G loss: 1.708591]\n",
      "epoch:15 step:14615 [D loss: 0.613321, acc: 68.75%] [G loss: 1.828586]\n",
      "epoch:15 step:14616 [D loss: 0.609548, acc: 66.41%] [G loss: 1.932785]\n",
      "epoch:15 step:14617 [D loss: 0.619465, acc: 68.75%] [G loss: 1.838576]\n",
      "epoch:15 step:14618 [D loss: 0.591234, acc: 68.75%] [G loss: 2.038898]\n",
      "epoch:15 step:14619 [D loss: 0.601164, acc: 65.62%] [G loss: 2.205820]\n",
      "epoch:15 step:14620 [D loss: 0.694393, acc: 57.03%] [G loss: 1.788909]\n",
      "epoch:15 step:14621 [D loss: 0.717885, acc: 50.78%] [G loss: 1.726756]\n",
      "epoch:15 step:14622 [D loss: 0.651877, acc: 60.16%] [G loss: 1.911024]\n",
      "epoch:15 step:14623 [D loss: 0.677263, acc: 56.25%] [G loss: 1.943440]\n",
      "epoch:15 step:14624 [D loss: 0.639195, acc: 64.06%] [G loss: 1.974845]\n",
      "epoch:15 step:14625 [D loss: 0.683508, acc: 57.81%] [G loss: 1.881906]\n",
      "epoch:15 step:14626 [D loss: 0.621702, acc: 63.28%] [G loss: 1.834595]\n",
      "epoch:15 step:14627 [D loss: 0.643916, acc: 67.19%] [G loss: 1.784017]\n",
      "epoch:15 step:14628 [D loss: 0.638826, acc: 60.94%] [G loss: 1.893234]\n",
      "epoch:15 step:14629 [D loss: 0.624698, acc: 65.62%] [G loss: 1.940862]\n",
      "epoch:15 step:14630 [D loss: 0.661460, acc: 62.50%] [G loss: 1.913837]\n",
      "epoch:15 step:14631 [D loss: 0.692079, acc: 59.38%] [G loss: 1.771411]\n",
      "epoch:15 step:14632 [D loss: 0.665263, acc: 55.47%] [G loss: 1.775398]\n",
      "epoch:15 step:14633 [D loss: 0.624268, acc: 70.31%] [G loss: 1.859552]\n",
      "epoch:15 step:14634 [D loss: 0.686081, acc: 57.81%] [G loss: 1.820603]\n",
      "epoch:15 step:14635 [D loss: 0.671580, acc: 60.94%] [G loss: 2.049251]\n",
      "epoch:15 step:14636 [D loss: 0.669934, acc: 57.03%] [G loss: 1.886144]\n",
      "epoch:15 step:14637 [D loss: 0.622419, acc: 66.41%] [G loss: 1.950718]\n",
      "epoch:15 step:14638 [D loss: 0.625371, acc: 66.41%] [G loss: 1.962445]\n",
      "epoch:15 step:14639 [D loss: 0.669615, acc: 62.50%] [G loss: 1.738720]\n",
      "epoch:15 step:14640 [D loss: 0.656290, acc: 56.25%] [G loss: 1.904543]\n",
      "epoch:15 step:14641 [D loss: 0.743046, acc: 42.97%] [G loss: 1.822291]\n",
      "epoch:15 step:14642 [D loss: 0.694440, acc: 57.81%] [G loss: 1.765580]\n",
      "epoch:15 step:14643 [D loss: 0.641889, acc: 64.84%] [G loss: 1.912265]\n",
      "epoch:15 step:14644 [D loss: 0.684256, acc: 57.81%] [G loss: 1.909669]\n",
      "epoch:15 step:14645 [D loss: 0.655715, acc: 61.72%] [G loss: 1.821297]\n",
      "epoch:15 step:14646 [D loss: 0.613431, acc: 65.62%] [G loss: 1.946955]\n",
      "epoch:15 step:14647 [D loss: 0.603260, acc: 64.84%] [G loss: 1.862434]\n",
      "epoch:15 step:14648 [D loss: 0.647887, acc: 59.38%] [G loss: 1.901562]\n",
      "epoch:15 step:14649 [D loss: 0.690705, acc: 59.38%] [G loss: 1.876043]\n",
      "epoch:15 step:14650 [D loss: 0.611317, acc: 68.75%] [G loss: 1.838349]\n",
      "epoch:15 step:14651 [D loss: 0.677683, acc: 61.72%] [G loss: 1.788951]\n",
      "epoch:15 step:14652 [D loss: 0.689133, acc: 60.94%] [G loss: 1.801614]\n",
      "epoch:15 step:14653 [D loss: 0.682432, acc: 52.34%] [G loss: 1.764485]\n",
      "epoch:15 step:14654 [D loss: 0.631224, acc: 60.94%] [G loss: 1.809010]\n",
      "epoch:15 step:14655 [D loss: 0.653192, acc: 61.72%] [G loss: 1.752990]\n",
      "epoch:15 step:14656 [D loss: 0.685817, acc: 59.38%] [G loss: 1.871225]\n",
      "epoch:15 step:14657 [D loss: 0.603292, acc: 68.75%] [G loss: 1.858366]\n",
      "epoch:15 step:14658 [D loss: 0.611318, acc: 69.53%] [G loss: 1.913431]\n",
      "epoch:15 step:14659 [D loss: 0.684535, acc: 55.47%] [G loss: 1.898445]\n",
      "epoch:15 step:14660 [D loss: 0.630459, acc: 65.62%] [G loss: 1.889060]\n",
      "epoch:15 step:14661 [D loss: 0.632036, acc: 61.72%] [G loss: 1.797257]\n",
      "epoch:15 step:14662 [D loss: 0.638600, acc: 60.16%] [G loss: 1.851861]\n",
      "epoch:15 step:14663 [D loss: 0.648293, acc: 57.03%] [G loss: 1.835715]\n",
      "epoch:15 step:14664 [D loss: 0.657523, acc: 57.81%] [G loss: 1.858990]\n",
      "epoch:15 step:14665 [D loss: 0.604134, acc: 69.53%] [G loss: 1.869770]\n",
      "epoch:15 step:14666 [D loss: 0.664151, acc: 61.72%] [G loss: 1.742484]\n",
      "epoch:15 step:14667 [D loss: 0.635782, acc: 66.41%] [G loss: 1.810269]\n",
      "epoch:15 step:14668 [D loss: 0.614356, acc: 70.31%] [G loss: 1.942504]\n",
      "epoch:15 step:14669 [D loss: 0.678183, acc: 58.59%] [G loss: 1.734394]\n",
      "epoch:15 step:14670 [D loss: 0.669180, acc: 59.38%] [G loss: 1.904685]\n",
      "epoch:15 step:14671 [D loss: 0.679070, acc: 54.69%] [G loss: 1.765029]\n",
      "epoch:15 step:14672 [D loss: 0.677930, acc: 56.25%] [G loss: 1.836135]\n",
      "epoch:15 step:14673 [D loss: 0.643328, acc: 61.72%] [G loss: 1.829456]\n",
      "epoch:15 step:14674 [D loss: 0.656353, acc: 61.72%] [G loss: 1.827664]\n",
      "epoch:15 step:14675 [D loss: 0.629067, acc: 63.28%] [G loss: 1.782740]\n",
      "epoch:15 step:14676 [D loss: 0.580805, acc: 73.44%] [G loss: 1.776590]\n",
      "epoch:15 step:14677 [D loss: 0.637374, acc: 64.84%] [G loss: 1.998347]\n",
      "epoch:15 step:14678 [D loss: 0.653176, acc: 63.28%] [G loss: 1.753094]\n",
      "epoch:15 step:14679 [D loss: 0.591550, acc: 69.53%] [G loss: 2.040006]\n",
      "epoch:15 step:14680 [D loss: 0.654671, acc: 60.94%] [G loss: 1.836754]\n",
      "epoch:15 step:14681 [D loss: 0.637237, acc: 63.28%] [G loss: 1.945487]\n",
      "epoch:15 step:14682 [D loss: 0.669069, acc: 59.38%] [G loss: 1.904387]\n",
      "epoch:15 step:14683 [D loss: 0.649042, acc: 63.28%] [G loss: 1.793001]\n",
      "epoch:15 step:14684 [D loss: 0.618783, acc: 64.84%] [G loss: 1.980400]\n",
      "epoch:15 step:14685 [D loss: 0.650613, acc: 60.94%] [G loss: 1.923637]\n",
      "epoch:15 step:14686 [D loss: 0.626753, acc: 65.62%] [G loss: 2.036653]\n",
      "epoch:15 step:14687 [D loss: 0.610516, acc: 69.53%] [G loss: 1.935383]\n",
      "epoch:15 step:14688 [D loss: 0.601412, acc: 66.41%] [G loss: 1.885588]\n",
      "epoch:15 step:14689 [D loss: 0.647078, acc: 64.84%] [G loss: 1.888941]\n",
      "epoch:15 step:14690 [D loss: 0.613906, acc: 59.38%] [G loss: 1.986058]\n",
      "epoch:15 step:14691 [D loss: 0.627977, acc: 64.06%] [G loss: 1.975052]\n",
      "epoch:15 step:14692 [D loss: 0.655905, acc: 64.84%] [G loss: 2.010037]\n",
      "epoch:15 step:14693 [D loss: 0.652412, acc: 61.72%] [G loss: 1.991650]\n",
      "epoch:15 step:14694 [D loss: 0.702729, acc: 54.69%] [G loss: 1.924190]\n",
      "epoch:15 step:14695 [D loss: 0.697268, acc: 58.59%] [G loss: 2.017475]\n",
      "epoch:15 step:14696 [D loss: 0.597899, acc: 64.84%] [G loss: 1.938870]\n",
      "epoch:15 step:14697 [D loss: 0.604938, acc: 68.75%] [G loss: 2.063947]\n",
      "epoch:15 step:14698 [D loss: 0.630383, acc: 66.41%] [G loss: 1.829853]\n",
      "epoch:15 step:14699 [D loss: 0.661687, acc: 60.94%] [G loss: 1.900491]\n",
      "epoch:15 step:14700 [D loss: 0.638865, acc: 62.50%] [G loss: 1.894815]\n",
      "epoch:15 step:14701 [D loss: 0.664688, acc: 59.38%] [G loss: 1.967864]\n",
      "epoch:15 step:14702 [D loss: 0.627599, acc: 67.19%] [G loss: 2.110566]\n",
      "epoch:15 step:14703 [D loss: 0.590500, acc: 65.62%] [G loss: 2.223100]\n",
      "epoch:15 step:14704 [D loss: 0.619400, acc: 65.62%] [G loss: 1.937460]\n",
      "epoch:15 step:14705 [D loss: 0.636121, acc: 61.72%] [G loss: 2.054403]\n",
      "epoch:15 step:14706 [D loss: 0.674098, acc: 59.38%] [G loss: 1.875508]\n",
      "epoch:15 step:14707 [D loss: 0.619513, acc: 67.97%] [G loss: 1.886086]\n",
      "epoch:15 step:14708 [D loss: 0.629057, acc: 67.19%] [G loss: 1.955675]\n",
      "epoch:15 step:14709 [D loss: 0.611315, acc: 64.06%] [G loss: 2.106004]\n",
      "epoch:15 step:14710 [D loss: 0.648348, acc: 64.06%] [G loss: 1.898331]\n",
      "epoch:15 step:14711 [D loss: 0.662107, acc: 58.59%] [G loss: 1.805867]\n",
      "epoch:15 step:14712 [D loss: 0.638981, acc: 67.19%] [G loss: 1.800410]\n",
      "epoch:15 step:14713 [D loss: 0.676016, acc: 57.81%] [G loss: 1.853266]\n",
      "epoch:15 step:14714 [D loss: 0.660642, acc: 63.28%] [G loss: 1.936743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14715 [D loss: 0.641677, acc: 66.41%] [G loss: 1.940207]\n",
      "epoch:15 step:14716 [D loss: 0.670595, acc: 60.16%] [G loss: 1.819258]\n",
      "epoch:15 step:14717 [D loss: 0.616814, acc: 68.75%] [G loss: 1.987071]\n",
      "epoch:15 step:14718 [D loss: 0.558113, acc: 75.00%] [G loss: 1.908094]\n",
      "epoch:15 step:14719 [D loss: 0.694612, acc: 59.38%] [G loss: 1.801670]\n",
      "epoch:15 step:14720 [D loss: 0.620837, acc: 64.06%] [G loss: 2.026244]\n",
      "epoch:15 step:14721 [D loss: 0.675474, acc: 60.94%] [G loss: 1.732698]\n",
      "epoch:15 step:14722 [D loss: 0.685298, acc: 57.03%] [G loss: 1.824059]\n",
      "epoch:15 step:14723 [D loss: 0.643946, acc: 60.16%] [G loss: 1.910604]\n",
      "epoch:15 step:14724 [D loss: 0.654792, acc: 64.84%] [G loss: 1.788191]\n",
      "epoch:15 step:14725 [D loss: 0.653536, acc: 65.62%] [G loss: 1.831173]\n",
      "epoch:15 step:14726 [D loss: 0.687316, acc: 56.25%] [G loss: 1.773353]\n",
      "epoch:15 step:14727 [D loss: 0.661892, acc: 60.16%] [G loss: 1.858881]\n",
      "epoch:15 step:14728 [D loss: 0.670874, acc: 58.59%] [G loss: 1.831822]\n",
      "epoch:15 step:14729 [D loss: 0.682317, acc: 60.94%] [G loss: 1.871512]\n",
      "epoch:15 step:14730 [D loss: 0.668294, acc: 60.94%] [G loss: 1.900132]\n",
      "epoch:15 step:14731 [D loss: 0.641182, acc: 61.72%] [G loss: 1.904559]\n",
      "epoch:15 step:14732 [D loss: 0.581583, acc: 69.53%] [G loss: 1.814149]\n",
      "epoch:15 step:14733 [D loss: 0.645372, acc: 67.97%] [G loss: 1.869824]\n",
      "epoch:15 step:14734 [D loss: 0.576944, acc: 70.31%] [G loss: 1.905607]\n",
      "epoch:15 step:14735 [D loss: 0.619562, acc: 60.16%] [G loss: 1.901961]\n",
      "epoch:15 step:14736 [D loss: 0.591677, acc: 70.31%] [G loss: 1.943623]\n",
      "epoch:15 step:14737 [D loss: 0.697554, acc: 54.69%] [G loss: 1.931286]\n",
      "epoch:15 step:14738 [D loss: 0.713689, acc: 60.94%] [G loss: 1.931319]\n",
      "epoch:15 step:14739 [D loss: 0.650231, acc: 63.28%] [G loss: 1.843809]\n",
      "epoch:15 step:14740 [D loss: 0.662629, acc: 58.59%] [G loss: 1.821507]\n",
      "epoch:15 step:14741 [D loss: 0.686914, acc: 59.38%] [G loss: 1.781279]\n",
      "epoch:15 step:14742 [D loss: 0.647684, acc: 64.06%] [G loss: 1.814140]\n",
      "epoch:15 step:14743 [D loss: 0.618354, acc: 64.84%] [G loss: 1.942942]\n",
      "epoch:15 step:14744 [D loss: 0.573798, acc: 72.66%] [G loss: 1.970842]\n",
      "epoch:15 step:14745 [D loss: 0.649163, acc: 64.84%] [G loss: 1.937562]\n",
      "epoch:15 step:14746 [D loss: 0.632902, acc: 65.62%] [G loss: 1.867854]\n",
      "epoch:15 step:14747 [D loss: 0.651795, acc: 61.72%] [G loss: 2.012146]\n",
      "epoch:15 step:14748 [D loss: 0.616570, acc: 67.97%] [G loss: 2.034093]\n",
      "epoch:15 step:14749 [D loss: 0.596639, acc: 66.41%] [G loss: 2.174302]\n",
      "epoch:15 step:14750 [D loss: 0.638581, acc: 65.62%] [G loss: 1.970225]\n",
      "epoch:15 step:14751 [D loss: 0.589889, acc: 71.09%] [G loss: 1.902783]\n",
      "epoch:15 step:14752 [D loss: 0.625471, acc: 64.84%] [G loss: 1.909145]\n",
      "epoch:15 step:14753 [D loss: 0.625355, acc: 69.53%] [G loss: 1.900445]\n",
      "epoch:15 step:14754 [D loss: 0.651314, acc: 61.72%] [G loss: 2.023829]\n",
      "epoch:15 step:14755 [D loss: 0.662457, acc: 58.59%] [G loss: 1.993082]\n",
      "epoch:15 step:14756 [D loss: 0.605862, acc: 67.97%] [G loss: 2.002167]\n",
      "epoch:15 step:14757 [D loss: 0.671044, acc: 60.16%] [G loss: 1.904208]\n",
      "epoch:15 step:14758 [D loss: 0.673397, acc: 60.16%] [G loss: 1.811790]\n",
      "epoch:15 step:14759 [D loss: 0.715161, acc: 56.25%] [G loss: 1.848293]\n",
      "epoch:15 step:14760 [D loss: 0.705494, acc: 56.25%] [G loss: 1.841985]\n",
      "epoch:15 step:14761 [D loss: 0.610591, acc: 63.28%] [G loss: 1.935239]\n",
      "epoch:15 step:14762 [D loss: 0.606276, acc: 67.19%] [G loss: 1.951456]\n",
      "epoch:15 step:14763 [D loss: 0.586551, acc: 68.75%] [G loss: 2.117537]\n",
      "epoch:15 step:14764 [D loss: 0.657946, acc: 57.03%] [G loss: 1.980193]\n",
      "epoch:15 step:14765 [D loss: 0.679689, acc: 59.38%] [G loss: 1.903825]\n",
      "epoch:15 step:14766 [D loss: 0.612245, acc: 69.53%] [G loss: 2.123302]\n",
      "epoch:15 step:14767 [D loss: 0.580200, acc: 71.88%] [G loss: 2.037138]\n",
      "epoch:15 step:14768 [D loss: 0.686541, acc: 57.81%] [G loss: 1.718727]\n",
      "epoch:15 step:14769 [D loss: 0.658964, acc: 62.50%] [G loss: 1.872839]\n",
      "epoch:15 step:14770 [D loss: 0.610809, acc: 66.41%] [G loss: 1.955600]\n",
      "epoch:15 step:14771 [D loss: 0.698434, acc: 57.03%] [G loss: 1.890740]\n",
      "epoch:15 step:14772 [D loss: 0.597270, acc: 64.84%] [G loss: 1.799686]\n",
      "epoch:15 step:14773 [D loss: 0.712608, acc: 55.47%] [G loss: 1.830587]\n",
      "epoch:15 step:14774 [D loss: 0.588872, acc: 75.00%] [G loss: 1.958815]\n",
      "epoch:15 step:14775 [D loss: 0.658608, acc: 57.81%] [G loss: 1.929133]\n",
      "epoch:15 step:14776 [D loss: 0.694547, acc: 60.16%] [G loss: 1.975275]\n",
      "epoch:15 step:14777 [D loss: 0.676477, acc: 53.91%] [G loss: 1.744793]\n",
      "epoch:15 step:14778 [D loss: 0.639629, acc: 60.94%] [G loss: 1.948418]\n",
      "epoch:15 step:14779 [D loss: 0.645279, acc: 68.75%] [G loss: 2.057405]\n",
      "epoch:15 step:14780 [D loss: 0.633855, acc: 62.50%] [G loss: 1.903039]\n",
      "epoch:15 step:14781 [D loss: 0.677298, acc: 57.81%] [G loss: 1.911812]\n",
      "epoch:15 step:14782 [D loss: 0.680737, acc: 57.81%] [G loss: 1.913932]\n",
      "epoch:15 step:14783 [D loss: 0.665919, acc: 62.50%] [G loss: 1.906972]\n",
      "epoch:15 step:14784 [D loss: 0.706743, acc: 54.69%] [G loss: 1.717564]\n",
      "epoch:15 step:14785 [D loss: 0.637782, acc: 63.28%] [G loss: 1.849255]\n",
      "epoch:15 step:14786 [D loss: 0.641504, acc: 68.75%] [G loss: 1.854215]\n",
      "epoch:15 step:14787 [D loss: 0.668278, acc: 57.81%] [G loss: 1.876174]\n",
      "epoch:15 step:14788 [D loss: 0.632504, acc: 60.16%] [G loss: 1.883264]\n",
      "epoch:15 step:14789 [D loss: 0.693499, acc: 57.03%] [G loss: 1.839662]\n",
      "epoch:15 step:14790 [D loss: 0.635217, acc: 60.16%] [G loss: 1.928306]\n",
      "epoch:15 step:14791 [D loss: 0.664361, acc: 60.16%] [G loss: 1.829046]\n",
      "epoch:15 step:14792 [D loss: 0.663636, acc: 60.94%] [G loss: 1.783899]\n",
      "epoch:15 step:14793 [D loss: 0.657169, acc: 61.72%] [G loss: 1.778668]\n",
      "epoch:15 step:14794 [D loss: 0.696894, acc: 57.03%] [G loss: 1.789293]\n",
      "epoch:15 step:14795 [D loss: 0.642340, acc: 59.38%] [G loss: 1.799347]\n",
      "epoch:15 step:14796 [D loss: 0.632673, acc: 65.62%] [G loss: 1.768983]\n",
      "epoch:15 step:14797 [D loss: 0.659226, acc: 63.28%] [G loss: 1.808848]\n",
      "epoch:15 step:14798 [D loss: 0.659612, acc: 62.50%] [G loss: 1.800637]\n",
      "epoch:15 step:14799 [D loss: 0.667390, acc: 60.16%] [G loss: 1.853888]\n",
      "epoch:15 step:14800 [D loss: 0.656120, acc: 60.94%] [G loss: 1.927289]\n",
      "##############\n",
      "[2.35309968 1.64768625 6.23697811 4.59175613 3.82304992 5.37810839\n",
      " 4.47017067 4.82463589 4.62781543 3.7677893 ]\n",
      "##########\n",
      "epoch:15 step:14801 [D loss: 0.596587, acc: 68.75%] [G loss: 1.982230]\n",
      "epoch:15 step:14802 [D loss: 0.644539, acc: 63.28%] [G loss: 1.994992]\n",
      "epoch:15 step:14803 [D loss: 0.620355, acc: 69.53%] [G loss: 1.943579]\n",
      "epoch:15 step:14804 [D loss: 0.658252, acc: 57.81%] [G loss: 1.885067]\n",
      "epoch:15 step:14805 [D loss: 0.644805, acc: 60.16%] [G loss: 1.880899]\n",
      "epoch:15 step:14806 [D loss: 0.641335, acc: 65.62%] [G loss: 1.816272]\n",
      "epoch:15 step:14807 [D loss: 0.675705, acc: 57.81%] [G loss: 1.859928]\n",
      "epoch:15 step:14808 [D loss: 0.635798, acc: 64.06%] [G loss: 2.011459]\n",
      "epoch:15 step:14809 [D loss: 0.641467, acc: 60.16%] [G loss: 1.905414]\n",
      "epoch:15 step:14810 [D loss: 0.642513, acc: 65.62%] [G loss: 1.829107]\n",
      "epoch:15 step:14811 [D loss: 0.627165, acc: 62.50%] [G loss: 1.777024]\n",
      "epoch:15 step:14812 [D loss: 0.640111, acc: 58.59%] [G loss: 1.833595]\n",
      "epoch:15 step:14813 [D loss: 0.657595, acc: 62.50%] [G loss: 1.821980]\n",
      "epoch:15 step:14814 [D loss: 0.658212, acc: 59.38%] [G loss: 1.795062]\n",
      "epoch:15 step:14815 [D loss: 0.640296, acc: 63.28%] [G loss: 1.788026]\n",
      "epoch:15 step:14816 [D loss: 0.635494, acc: 63.28%] [G loss: 1.857433]\n",
      "epoch:15 step:14817 [D loss: 0.641788, acc: 69.53%] [G loss: 1.836855]\n",
      "epoch:15 step:14818 [D loss: 0.624498, acc: 65.62%] [G loss: 1.960800]\n",
      "epoch:15 step:14819 [D loss: 0.605493, acc: 71.88%] [G loss: 1.996465]\n",
      "epoch:15 step:14820 [D loss: 0.735645, acc: 57.03%] [G loss: 1.704875]\n",
      "epoch:15 step:14821 [D loss: 0.679861, acc: 60.94%] [G loss: 1.695537]\n",
      "epoch:15 step:14822 [D loss: 0.666752, acc: 58.59%] [G loss: 1.713909]\n",
      "epoch:15 step:14823 [D loss: 0.671761, acc: 57.03%] [G loss: 1.840529]\n",
      "epoch:15 step:14824 [D loss: 0.648912, acc: 61.72%] [G loss: 1.915291]\n",
      "epoch:15 step:14825 [D loss: 0.675724, acc: 59.38%] [G loss: 1.869520]\n",
      "epoch:15 step:14826 [D loss: 0.656868, acc: 64.06%] [G loss: 1.892028]\n",
      "epoch:15 step:14827 [D loss: 0.655005, acc: 63.28%] [G loss: 1.898837]\n",
      "epoch:15 step:14828 [D loss: 0.636212, acc: 62.50%] [G loss: 1.904145]\n",
      "epoch:15 step:14829 [D loss: 0.638090, acc: 60.16%] [G loss: 2.058609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14830 [D loss: 0.592176, acc: 65.62%] [G loss: 2.069213]\n",
      "epoch:15 step:14831 [D loss: 0.636189, acc: 60.16%] [G loss: 1.934164]\n",
      "epoch:15 step:14832 [D loss: 0.659566, acc: 58.59%] [G loss: 1.976976]\n",
      "epoch:15 step:14833 [D loss: 0.609569, acc: 71.88%] [G loss: 1.933887]\n",
      "epoch:15 step:14834 [D loss: 0.654426, acc: 61.72%] [G loss: 1.798374]\n",
      "epoch:15 step:14835 [D loss: 0.631493, acc: 61.72%] [G loss: 1.851343]\n",
      "epoch:15 step:14836 [D loss: 0.604621, acc: 64.84%] [G loss: 2.149171]\n",
      "epoch:15 step:14837 [D loss: 0.578531, acc: 70.31%] [G loss: 2.099111]\n",
      "epoch:15 step:14838 [D loss: 0.648068, acc: 65.62%] [G loss: 1.989854]\n",
      "epoch:15 step:14839 [D loss: 0.650173, acc: 59.38%] [G loss: 1.909205]\n",
      "epoch:15 step:14840 [D loss: 0.643839, acc: 58.59%] [G loss: 1.925417]\n",
      "epoch:15 step:14841 [D loss: 0.642603, acc: 61.72%] [G loss: 2.174445]\n",
      "epoch:15 step:14842 [D loss: 0.679531, acc: 56.25%] [G loss: 1.947034]\n",
      "epoch:15 step:14843 [D loss: 0.658252, acc: 62.50%] [G loss: 1.766151]\n",
      "epoch:15 step:14844 [D loss: 0.673443, acc: 61.72%] [G loss: 1.971525]\n",
      "epoch:15 step:14845 [D loss: 0.605703, acc: 67.97%] [G loss: 1.954228]\n",
      "epoch:15 step:14846 [D loss: 0.701017, acc: 59.38%] [G loss: 1.751848]\n",
      "epoch:15 step:14847 [D loss: 0.561253, acc: 72.66%] [G loss: 1.981137]\n",
      "epoch:15 step:14848 [D loss: 0.569897, acc: 69.53%] [G loss: 2.122089]\n",
      "epoch:15 step:14849 [D loss: 0.731232, acc: 54.69%] [G loss: 1.876261]\n",
      "epoch:15 step:14850 [D loss: 0.662739, acc: 55.47%] [G loss: 1.849845]\n",
      "epoch:15 step:14851 [D loss: 0.645043, acc: 63.28%] [G loss: 1.865507]\n",
      "epoch:15 step:14852 [D loss: 0.683056, acc: 55.47%] [G loss: 1.746161]\n",
      "epoch:15 step:14853 [D loss: 0.629462, acc: 64.06%] [G loss: 1.841716]\n",
      "epoch:15 step:14854 [D loss: 0.716776, acc: 53.91%] [G loss: 1.901098]\n",
      "epoch:15 step:14855 [D loss: 0.697269, acc: 53.91%] [G loss: 1.678926]\n",
      "epoch:15 step:14856 [D loss: 0.711628, acc: 57.03%] [G loss: 1.836056]\n",
      "epoch:15 step:14857 [D loss: 0.643731, acc: 64.84%] [G loss: 1.898510]\n",
      "epoch:15 step:14858 [D loss: 0.639979, acc: 65.62%] [G loss: 1.967162]\n",
      "epoch:15 step:14859 [D loss: 0.634799, acc: 68.75%] [G loss: 1.920876]\n",
      "epoch:15 step:14860 [D loss: 0.643644, acc: 60.94%] [G loss: 1.946033]\n",
      "epoch:15 step:14861 [D loss: 0.627754, acc: 66.41%] [G loss: 2.000355]\n",
      "epoch:15 step:14862 [D loss: 0.647069, acc: 60.16%] [G loss: 1.904968]\n",
      "epoch:15 step:14863 [D loss: 0.623548, acc: 66.41%] [G loss: 1.995694]\n",
      "epoch:15 step:14864 [D loss: 0.668602, acc: 60.16%] [G loss: 1.828079]\n",
      "epoch:15 step:14865 [D loss: 0.634258, acc: 66.41%] [G loss: 1.885001]\n",
      "epoch:15 step:14866 [D loss: 0.631141, acc: 66.41%] [G loss: 1.695478]\n",
      "epoch:15 step:14867 [D loss: 0.691940, acc: 55.47%] [G loss: 1.731526]\n",
      "epoch:15 step:14868 [D loss: 0.647910, acc: 56.25%] [G loss: 1.742494]\n",
      "epoch:15 step:14869 [D loss: 0.673719, acc: 59.38%] [G loss: 2.007291]\n",
      "epoch:15 step:14870 [D loss: 0.600272, acc: 70.31%] [G loss: 2.107429]\n",
      "epoch:15 step:14871 [D loss: 0.621128, acc: 68.75%] [G loss: 1.980151]\n",
      "epoch:15 step:14872 [D loss: 0.640967, acc: 61.72%] [G loss: 1.847416]\n",
      "epoch:15 step:14873 [D loss: 0.646377, acc: 64.84%] [G loss: 1.735452]\n",
      "epoch:15 step:14874 [D loss: 0.706151, acc: 53.12%] [G loss: 1.817974]\n",
      "epoch:15 step:14875 [D loss: 0.701887, acc: 51.56%] [G loss: 1.710528]\n",
      "epoch:15 step:14876 [D loss: 0.682903, acc: 59.38%] [G loss: 1.794635]\n",
      "epoch:15 step:14877 [D loss: 0.589165, acc: 64.84%] [G loss: 2.174532]\n",
      "epoch:15 step:14878 [D loss: 0.622577, acc: 64.84%] [G loss: 2.009827]\n",
      "epoch:15 step:14879 [D loss: 0.644117, acc: 64.06%] [G loss: 2.028108]\n",
      "epoch:15 step:14880 [D loss: 0.595247, acc: 69.53%] [G loss: 2.001776]\n",
      "epoch:15 step:14881 [D loss: 0.624242, acc: 66.41%] [G loss: 1.983994]\n",
      "epoch:15 step:14882 [D loss: 0.750780, acc: 58.59%] [G loss: 1.822029]\n",
      "epoch:15 step:14883 [D loss: 0.683998, acc: 58.59%] [G loss: 1.734470]\n",
      "epoch:15 step:14884 [D loss: 0.643693, acc: 67.19%] [G loss: 1.774188]\n",
      "epoch:15 step:14885 [D loss: 0.683503, acc: 59.38%] [G loss: 1.741789]\n",
      "epoch:15 step:14886 [D loss: 0.627310, acc: 65.62%] [G loss: 1.825133]\n",
      "epoch:15 step:14887 [D loss: 0.693762, acc: 57.03%] [G loss: 1.889894]\n",
      "epoch:15 step:14888 [D loss: 0.618910, acc: 62.50%] [G loss: 1.882899]\n",
      "epoch:15 step:14889 [D loss: 0.647036, acc: 64.84%] [G loss: 1.871481]\n",
      "epoch:15 step:14890 [D loss: 0.614996, acc: 67.19%] [G loss: 1.887441]\n",
      "epoch:15 step:14891 [D loss: 0.655473, acc: 60.94%] [G loss: 1.872793]\n",
      "epoch:15 step:14892 [D loss: 0.629192, acc: 60.16%] [G loss: 1.844057]\n",
      "epoch:15 step:14893 [D loss: 0.647080, acc: 61.72%] [G loss: 1.851035]\n",
      "epoch:15 step:14894 [D loss: 0.647664, acc: 64.84%] [G loss: 1.982238]\n",
      "epoch:15 step:14895 [D loss: 0.600600, acc: 66.41%] [G loss: 2.001701]\n",
      "epoch:15 step:14896 [D loss: 0.634555, acc: 69.53%] [G loss: 1.906508]\n",
      "epoch:15 step:14897 [D loss: 0.585202, acc: 69.53%] [G loss: 1.899180]\n",
      "epoch:15 step:14898 [D loss: 0.752722, acc: 52.34%] [G loss: 1.825834]\n",
      "epoch:15 step:14899 [D loss: 0.688703, acc: 58.59%] [G loss: 1.990617]\n",
      "epoch:15 step:14900 [D loss: 0.591725, acc: 71.88%] [G loss: 1.885019]\n",
      "epoch:15 step:14901 [D loss: 0.680008, acc: 57.03%] [G loss: 1.889619]\n",
      "epoch:15 step:14902 [D loss: 0.623183, acc: 68.75%] [G loss: 1.910204]\n",
      "epoch:15 step:14903 [D loss: 0.625620, acc: 67.97%] [G loss: 1.906910]\n",
      "epoch:15 step:14904 [D loss: 0.656060, acc: 62.50%] [G loss: 1.942579]\n",
      "epoch:15 step:14905 [D loss: 0.681494, acc: 57.81%] [G loss: 1.909621]\n",
      "epoch:15 step:14906 [D loss: 0.653858, acc: 60.94%] [G loss: 1.827806]\n",
      "epoch:15 step:14907 [D loss: 0.689700, acc: 55.47%] [G loss: 1.834715]\n",
      "epoch:15 step:14908 [D loss: 0.628516, acc: 62.50%] [G loss: 1.848311]\n",
      "epoch:15 step:14909 [D loss: 0.678057, acc: 56.25%] [G loss: 1.838228]\n",
      "epoch:15 step:14910 [D loss: 0.683884, acc: 57.03%] [G loss: 1.686530]\n",
      "epoch:15 step:14911 [D loss: 0.686049, acc: 58.59%] [G loss: 1.793531]\n",
      "epoch:15 step:14912 [D loss: 0.642195, acc: 63.28%] [G loss: 1.815750]\n",
      "epoch:15 step:14913 [D loss: 0.731286, acc: 46.09%] [G loss: 1.663868]\n",
      "epoch:15 step:14914 [D loss: 0.683268, acc: 61.72%] [G loss: 1.730657]\n",
      "epoch:15 step:14915 [D loss: 0.613563, acc: 70.31%] [G loss: 1.938657]\n",
      "epoch:15 step:14916 [D loss: 0.653049, acc: 59.38%] [G loss: 1.788565]\n",
      "epoch:15 step:14917 [D loss: 0.670718, acc: 60.16%] [G loss: 1.722196]\n",
      "epoch:15 step:14918 [D loss: 0.655325, acc: 60.94%] [G loss: 1.837431]\n",
      "epoch:15 step:14919 [D loss: 0.637896, acc: 61.72%] [G loss: 1.840582]\n",
      "epoch:15 step:14920 [D loss: 0.677270, acc: 58.59%] [G loss: 1.739649]\n",
      "epoch:15 step:14921 [D loss: 0.627734, acc: 67.19%] [G loss: 1.885882]\n",
      "epoch:15 step:14922 [D loss: 0.673224, acc: 57.81%] [G loss: 1.771963]\n",
      "epoch:15 step:14923 [D loss: 0.649746, acc: 60.16%] [G loss: 1.909106]\n",
      "epoch:15 step:14924 [D loss: 0.709024, acc: 53.91%] [G loss: 1.744217]\n",
      "epoch:15 step:14925 [D loss: 0.653569, acc: 61.72%] [G loss: 1.809537]\n",
      "epoch:15 step:14926 [D loss: 0.642828, acc: 65.62%] [G loss: 1.848255]\n",
      "epoch:15 step:14927 [D loss: 0.662707, acc: 57.03%] [G loss: 1.793641]\n",
      "epoch:15 step:14928 [D loss: 0.681053, acc: 60.94%] [G loss: 1.723176]\n",
      "epoch:15 step:14929 [D loss: 0.654490, acc: 66.41%] [G loss: 1.746459]\n",
      "epoch:15 step:14930 [D loss: 0.697320, acc: 57.81%] [G loss: 1.799418]\n",
      "epoch:15 step:14931 [D loss: 0.632473, acc: 66.41%] [G loss: 1.775659]\n",
      "epoch:15 step:14932 [D loss: 0.636513, acc: 63.28%] [G loss: 1.860342]\n",
      "epoch:15 step:14933 [D loss: 0.633682, acc: 66.41%] [G loss: 1.888307]\n",
      "epoch:15 step:14934 [D loss: 0.671131, acc: 60.16%] [G loss: 1.851610]\n",
      "epoch:15 step:14935 [D loss: 0.636819, acc: 62.50%] [G loss: 1.864740]\n",
      "epoch:15 step:14936 [D loss: 0.611083, acc: 68.75%] [G loss: 1.811989]\n",
      "epoch:15 step:14937 [D loss: 0.619778, acc: 64.84%] [G loss: 2.090092]\n",
      "epoch:15 step:14938 [D loss: 0.630051, acc: 62.50%] [G loss: 1.922163]\n",
      "epoch:15 step:14939 [D loss: 0.591981, acc: 68.75%] [G loss: 1.956889]\n",
      "epoch:15 step:14940 [D loss: 0.631319, acc: 71.88%] [G loss: 1.912221]\n",
      "epoch:15 step:14941 [D loss: 0.699096, acc: 53.91%] [G loss: 1.932819]\n",
      "epoch:15 step:14942 [D loss: 0.643207, acc: 66.41%] [G loss: 1.986839]\n",
      "epoch:15 step:14943 [D loss: 0.627054, acc: 64.06%] [G loss: 1.920709]\n",
      "epoch:15 step:14944 [D loss: 0.687679, acc: 56.25%] [G loss: 1.847916]\n",
      "epoch:15 step:14945 [D loss: 0.619304, acc: 69.53%] [G loss: 1.933466]\n",
      "epoch:15 step:14946 [D loss: 0.683536, acc: 55.47%] [G loss: 1.863477]\n",
      "epoch:15 step:14947 [D loss: 0.687701, acc: 60.16%] [G loss: 1.803668]\n",
      "epoch:15 step:14948 [D loss: 0.625817, acc: 60.94%] [G loss: 1.924500]\n",
      "epoch:15 step:14949 [D loss: 0.619753, acc: 67.19%] [G loss: 2.018073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14950 [D loss: 0.617564, acc: 64.06%] [G loss: 1.860858]\n",
      "epoch:15 step:14951 [D loss: 0.679937, acc: 62.50%] [G loss: 1.798630]\n",
      "epoch:15 step:14952 [D loss: 0.616674, acc: 64.84%] [G loss: 1.929164]\n",
      "epoch:15 step:14953 [D loss: 0.630538, acc: 62.50%] [G loss: 1.813741]\n",
      "epoch:15 step:14954 [D loss: 0.595683, acc: 65.62%] [G loss: 1.965821]\n",
      "epoch:15 step:14955 [D loss: 0.636208, acc: 62.50%] [G loss: 2.020631]\n",
      "epoch:15 step:14956 [D loss: 0.612058, acc: 67.97%] [G loss: 1.943193]\n",
      "epoch:15 step:14957 [D loss: 0.642295, acc: 62.50%] [G loss: 1.798387]\n",
      "epoch:15 step:14958 [D loss: 0.660301, acc: 60.16%] [G loss: 1.794640]\n",
      "epoch:15 step:14959 [D loss: 0.692666, acc: 53.12%] [G loss: 1.877905]\n",
      "epoch:15 step:14960 [D loss: 0.642466, acc: 65.62%] [G loss: 1.925911]\n",
      "epoch:15 step:14961 [D loss: 0.678541, acc: 57.03%] [G loss: 2.017502]\n",
      "epoch:15 step:14962 [D loss: 0.642742, acc: 73.44%] [G loss: 1.844245]\n",
      "epoch:15 step:14963 [D loss: 0.625545, acc: 65.62%] [G loss: 1.946316]\n",
      "epoch:15 step:14964 [D loss: 0.635726, acc: 64.06%] [G loss: 2.114581]\n",
      "epoch:15 step:14965 [D loss: 0.641118, acc: 57.03%] [G loss: 1.891208]\n",
      "epoch:15 step:14966 [D loss: 0.686679, acc: 54.69%] [G loss: 1.907212]\n",
      "epoch:15 step:14967 [D loss: 0.676787, acc: 60.16%] [G loss: 2.117980]\n",
      "epoch:15 step:14968 [D loss: 0.630085, acc: 62.50%] [G loss: 2.077616]\n",
      "epoch:15 step:14969 [D loss: 0.665712, acc: 58.59%] [G loss: 1.884418]\n",
      "epoch:15 step:14970 [D loss: 0.641181, acc: 61.72%] [G loss: 2.007773]\n",
      "epoch:15 step:14971 [D loss: 0.654926, acc: 59.38%] [G loss: 1.832263]\n",
      "epoch:15 step:14972 [D loss: 0.634013, acc: 59.38%] [G loss: 1.882164]\n",
      "epoch:15 step:14973 [D loss: 0.564005, acc: 74.22%] [G loss: 2.171105]\n",
      "epoch:15 step:14974 [D loss: 0.582862, acc: 70.31%] [G loss: 2.173602]\n",
      "epoch:15 step:14975 [D loss: 0.775316, acc: 43.75%] [G loss: 1.824918]\n",
      "epoch:15 step:14976 [D loss: 0.615263, acc: 64.06%] [G loss: 1.981188]\n",
      "epoch:15 step:14977 [D loss: 0.618030, acc: 65.62%] [G loss: 1.888301]\n",
      "epoch:15 step:14978 [D loss: 0.579300, acc: 67.19%] [G loss: 2.060163]\n",
      "epoch:15 step:14979 [D loss: 0.602730, acc: 71.09%] [G loss: 2.240950]\n",
      "epoch:15 step:14980 [D loss: 0.578258, acc: 77.34%] [G loss: 2.193777]\n",
      "epoch:15 step:14981 [D loss: 0.618017, acc: 65.62%] [G loss: 2.195874]\n",
      "epoch:15 step:14982 [D loss: 0.656701, acc: 64.84%] [G loss: 1.969759]\n",
      "epoch:15 step:14983 [D loss: 0.829357, acc: 55.47%] [G loss: 1.744670]\n",
      "epoch:15 step:14984 [D loss: 0.764627, acc: 44.53%] [G loss: 1.831964]\n",
      "epoch:15 step:14985 [D loss: 0.640085, acc: 63.28%] [G loss: 1.934190]\n",
      "epoch:15 step:14986 [D loss: 0.621572, acc: 67.97%] [G loss: 1.941048]\n",
      "epoch:15 step:14987 [D loss: 0.624468, acc: 66.41%] [G loss: 1.847485]\n",
      "epoch:15 step:14988 [D loss: 0.635316, acc: 59.38%] [G loss: 1.814356]\n",
      "epoch:15 step:14989 [D loss: 0.651199, acc: 64.84%] [G loss: 2.005284]\n",
      "epoch:15 step:14990 [D loss: 0.659268, acc: 64.06%] [G loss: 1.984122]\n",
      "epoch:15 step:14991 [D loss: 0.637562, acc: 70.31%] [G loss: 1.932947]\n",
      "epoch:15 step:14992 [D loss: 0.627700, acc: 65.62%] [G loss: 2.281090]\n",
      "epoch:16 step:14993 [D loss: 0.678143, acc: 59.38%] [G loss: 1.847663]\n",
      "epoch:16 step:14994 [D loss: 0.643820, acc: 64.06%] [G loss: 1.894951]\n",
      "epoch:16 step:14995 [D loss: 0.669718, acc: 53.91%] [G loss: 1.848149]\n",
      "epoch:16 step:14996 [D loss: 0.663129, acc: 58.59%] [G loss: 1.770601]\n",
      "epoch:16 step:14997 [D loss: 0.654053, acc: 64.84%] [G loss: 1.803014]\n",
      "epoch:16 step:14998 [D loss: 0.628138, acc: 66.41%] [G loss: 1.908896]\n",
      "epoch:16 step:14999 [D loss: 0.620056, acc: 64.84%] [G loss: 1.964193]\n",
      "epoch:16 step:15000 [D loss: 0.661232, acc: 63.28%] [G loss: 1.762034]\n",
      "##############\n",
      "[2.37583526 1.76759578 6.36017266 4.65789255 3.69351136 5.6362192\n",
      " 4.39927184 4.75747448 4.75673906 3.74937632]\n",
      "##########\n",
      "epoch:16 step:15001 [D loss: 0.632552, acc: 64.84%] [G loss: 2.119261]\n",
      "epoch:16 step:15002 [D loss: 0.599290, acc: 71.09%] [G loss: 1.911561]\n",
      "epoch:16 step:15003 [D loss: 0.586711, acc: 73.44%] [G loss: 2.015064]\n",
      "epoch:16 step:15004 [D loss: 0.646933, acc: 60.16%] [G loss: 1.891914]\n",
      "epoch:16 step:15005 [D loss: 0.651487, acc: 63.28%] [G loss: 1.941814]\n",
      "epoch:16 step:15006 [D loss: 0.676568, acc: 60.94%] [G loss: 1.862416]\n",
      "epoch:16 step:15007 [D loss: 0.570808, acc: 74.22%] [G loss: 2.061481]\n",
      "epoch:16 step:15008 [D loss: 0.584595, acc: 67.97%] [G loss: 2.035286]\n",
      "epoch:16 step:15009 [D loss: 0.635348, acc: 60.94%] [G loss: 1.805693]\n",
      "epoch:16 step:15010 [D loss: 0.644251, acc: 60.94%] [G loss: 1.825518]\n",
      "epoch:16 step:15011 [D loss: 0.599948, acc: 64.84%] [G loss: 2.144103]\n",
      "epoch:16 step:15012 [D loss: 0.704585, acc: 53.91%] [G loss: 1.866809]\n",
      "epoch:16 step:15013 [D loss: 0.639300, acc: 60.16%] [G loss: 1.850602]\n",
      "epoch:16 step:15014 [D loss: 0.628085, acc: 64.84%] [G loss: 1.951396]\n",
      "epoch:16 step:15015 [D loss: 0.590766, acc: 71.09%] [G loss: 2.016115]\n",
      "epoch:16 step:15016 [D loss: 0.635748, acc: 65.62%] [G loss: 1.900823]\n",
      "epoch:16 step:15017 [D loss: 0.612587, acc: 68.75%] [G loss: 2.028184]\n",
      "epoch:16 step:15018 [D loss: 0.649924, acc: 58.59%] [G loss: 1.901143]\n",
      "epoch:16 step:15019 [D loss: 0.687747, acc: 59.38%] [G loss: 1.813835]\n",
      "epoch:16 step:15020 [D loss: 0.659613, acc: 64.84%] [G loss: 1.861932]\n",
      "epoch:16 step:15021 [D loss: 0.594547, acc: 67.19%] [G loss: 1.915329]\n",
      "epoch:16 step:15022 [D loss: 0.630519, acc: 60.94%] [G loss: 1.936721]\n",
      "epoch:16 step:15023 [D loss: 0.651479, acc: 67.19%] [G loss: 1.838640]\n",
      "epoch:16 step:15024 [D loss: 0.655919, acc: 60.94%] [G loss: 1.814200]\n",
      "epoch:16 step:15025 [D loss: 0.604874, acc: 64.84%] [G loss: 2.097869]\n",
      "epoch:16 step:15026 [D loss: 0.603581, acc: 63.28%] [G loss: 1.848874]\n",
      "epoch:16 step:15027 [D loss: 0.664430, acc: 60.94%] [G loss: 1.857952]\n",
      "epoch:16 step:15028 [D loss: 0.612433, acc: 62.50%] [G loss: 1.916708]\n",
      "epoch:16 step:15029 [D loss: 0.642750, acc: 62.50%] [G loss: 1.955793]\n",
      "epoch:16 step:15030 [D loss: 0.672410, acc: 60.16%] [G loss: 1.866609]\n",
      "epoch:16 step:15031 [D loss: 0.638641, acc: 62.50%] [G loss: 2.023973]\n",
      "epoch:16 step:15032 [D loss: 0.644118, acc: 60.16%] [G loss: 2.057969]\n",
      "epoch:16 step:15033 [D loss: 0.714957, acc: 54.69%] [G loss: 1.885786]\n",
      "epoch:16 step:15034 [D loss: 0.542836, acc: 77.34%] [G loss: 1.955388]\n",
      "epoch:16 step:15035 [D loss: 0.653887, acc: 64.84%] [G loss: 1.886734]\n",
      "epoch:16 step:15036 [D loss: 0.631101, acc: 65.62%] [G loss: 1.812613]\n",
      "epoch:16 step:15037 [D loss: 0.653392, acc: 60.16%] [G loss: 1.963909]\n",
      "epoch:16 step:15038 [D loss: 0.611693, acc: 65.62%] [G loss: 1.940905]\n",
      "epoch:16 step:15039 [D loss: 0.621698, acc: 64.06%] [G loss: 1.878150]\n",
      "epoch:16 step:15040 [D loss: 0.631547, acc: 64.06%] [G loss: 1.801285]\n",
      "epoch:16 step:15041 [D loss: 0.659099, acc: 62.50%] [G loss: 1.920496]\n",
      "epoch:16 step:15042 [D loss: 0.623203, acc: 61.72%] [G loss: 1.930760]\n",
      "epoch:16 step:15043 [D loss: 0.724107, acc: 53.12%] [G loss: 1.870549]\n",
      "epoch:16 step:15044 [D loss: 0.629438, acc: 68.75%] [G loss: 2.023417]\n",
      "epoch:16 step:15045 [D loss: 0.650971, acc: 67.97%] [G loss: 1.990357]\n",
      "epoch:16 step:15046 [D loss: 0.585750, acc: 66.41%] [G loss: 1.943149]\n",
      "epoch:16 step:15047 [D loss: 0.578334, acc: 71.88%] [G loss: 1.975770]\n",
      "epoch:16 step:15048 [D loss: 0.626711, acc: 60.16%] [G loss: 2.091846]\n",
      "epoch:16 step:15049 [D loss: 0.642768, acc: 64.06%] [G loss: 1.909967]\n",
      "epoch:16 step:15050 [D loss: 0.654954, acc: 60.16%] [G loss: 2.011437]\n",
      "epoch:16 step:15051 [D loss: 0.660548, acc: 64.84%] [G loss: 1.925372]\n",
      "epoch:16 step:15052 [D loss: 0.660232, acc: 62.50%] [G loss: 1.895595]\n",
      "epoch:16 step:15053 [D loss: 0.631195, acc: 60.16%] [G loss: 1.855723]\n",
      "epoch:16 step:15054 [D loss: 0.664152, acc: 58.59%] [G loss: 1.994473]\n",
      "epoch:16 step:15055 [D loss: 0.615279, acc: 69.53%] [G loss: 1.846974]\n",
      "epoch:16 step:15056 [D loss: 0.625241, acc: 67.19%] [G loss: 1.903632]\n",
      "epoch:16 step:15057 [D loss: 0.675942, acc: 58.59%] [G loss: 1.838934]\n",
      "epoch:16 step:15058 [D loss: 0.616017, acc: 68.75%] [G loss: 2.012633]\n",
      "epoch:16 step:15059 [D loss: 0.567989, acc: 67.97%] [G loss: 1.893720]\n",
      "epoch:16 step:15060 [D loss: 0.627108, acc: 67.19%] [G loss: 1.922119]\n",
      "epoch:16 step:15061 [D loss: 0.564584, acc: 71.09%] [G loss: 2.024371]\n",
      "epoch:16 step:15062 [D loss: 0.639559, acc: 64.06%] [G loss: 2.019862]\n",
      "epoch:16 step:15063 [D loss: 0.619875, acc: 64.84%] [G loss: 1.999236]\n",
      "epoch:16 step:15064 [D loss: 0.629849, acc: 65.62%] [G loss: 2.085058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15065 [D loss: 0.662067, acc: 60.94%] [G loss: 1.885590]\n",
      "epoch:16 step:15066 [D loss: 0.655217, acc: 56.25%] [G loss: 2.075373]\n",
      "epoch:16 step:15067 [D loss: 0.609296, acc: 66.41%] [G loss: 2.137058]\n",
      "epoch:16 step:15068 [D loss: 0.645710, acc: 66.41%] [G loss: 2.024980]\n",
      "epoch:16 step:15069 [D loss: 0.614518, acc: 65.62%] [G loss: 2.259381]\n",
      "epoch:16 step:15070 [D loss: 0.676439, acc: 55.47%] [G loss: 1.895976]\n",
      "epoch:16 step:15071 [D loss: 0.655999, acc: 66.41%] [G loss: 1.948571]\n",
      "epoch:16 step:15072 [D loss: 0.682341, acc: 55.47%] [G loss: 1.835368]\n",
      "epoch:16 step:15073 [D loss: 0.732896, acc: 45.31%] [G loss: 1.736851]\n",
      "epoch:16 step:15074 [D loss: 0.656968, acc: 65.62%] [G loss: 1.905232]\n",
      "epoch:16 step:15075 [D loss: 0.609451, acc: 69.53%] [G loss: 1.856718]\n",
      "epoch:16 step:15076 [D loss: 0.682005, acc: 56.25%] [G loss: 1.825468]\n",
      "epoch:16 step:15077 [D loss: 0.681162, acc: 57.03%] [G loss: 1.706110]\n",
      "epoch:16 step:15078 [D loss: 0.657681, acc: 65.62%] [G loss: 1.779539]\n",
      "epoch:16 step:15079 [D loss: 0.628449, acc: 67.97%] [G loss: 1.829842]\n",
      "epoch:16 step:15080 [D loss: 0.649110, acc: 63.28%] [G loss: 1.815339]\n",
      "epoch:16 step:15081 [D loss: 0.668849, acc: 60.16%] [G loss: 1.947331]\n",
      "epoch:16 step:15082 [D loss: 0.612978, acc: 71.88%] [G loss: 1.908943]\n",
      "epoch:16 step:15083 [D loss: 0.643820, acc: 64.84%] [G loss: 1.950944]\n",
      "epoch:16 step:15084 [D loss: 0.616882, acc: 62.50%] [G loss: 2.093604]\n",
      "epoch:16 step:15085 [D loss: 0.604834, acc: 67.19%] [G loss: 2.053360]\n",
      "epoch:16 step:15086 [D loss: 0.653081, acc: 64.06%] [G loss: 1.922468]\n",
      "epoch:16 step:15087 [D loss: 0.682834, acc: 59.38%] [G loss: 1.752943]\n",
      "epoch:16 step:15088 [D loss: 0.625136, acc: 64.06%] [G loss: 1.831453]\n",
      "epoch:16 step:15089 [D loss: 0.630415, acc: 63.28%] [G loss: 1.952065]\n",
      "epoch:16 step:15090 [D loss: 0.677856, acc: 60.16%] [G loss: 1.938417]\n",
      "epoch:16 step:15091 [D loss: 0.660319, acc: 62.50%] [G loss: 1.929917]\n",
      "epoch:16 step:15092 [D loss: 0.594766, acc: 68.75%] [G loss: 1.984728]\n",
      "epoch:16 step:15093 [D loss: 0.651194, acc: 67.19%] [G loss: 1.915673]\n",
      "epoch:16 step:15094 [D loss: 0.628611, acc: 68.75%] [G loss: 1.931551]\n",
      "epoch:16 step:15095 [D loss: 0.655674, acc: 58.59%] [G loss: 1.973802]\n",
      "epoch:16 step:15096 [D loss: 0.683194, acc: 59.38%] [G loss: 1.908687]\n",
      "epoch:16 step:15097 [D loss: 0.610071, acc: 68.75%] [G loss: 1.820719]\n",
      "epoch:16 step:15098 [D loss: 0.667440, acc: 59.38%] [G loss: 1.947757]\n",
      "epoch:16 step:15099 [D loss: 0.652032, acc: 60.94%] [G loss: 2.049516]\n",
      "epoch:16 step:15100 [D loss: 0.676619, acc: 58.59%] [G loss: 1.762971]\n",
      "epoch:16 step:15101 [D loss: 0.679168, acc: 53.91%] [G loss: 1.847510]\n",
      "epoch:16 step:15102 [D loss: 0.651512, acc: 61.72%] [G loss: 1.705398]\n",
      "epoch:16 step:15103 [D loss: 0.603161, acc: 66.41%] [G loss: 1.875221]\n",
      "epoch:16 step:15104 [D loss: 0.593886, acc: 71.09%] [G loss: 1.959310]\n",
      "epoch:16 step:15105 [D loss: 0.666057, acc: 62.50%] [G loss: 2.048964]\n",
      "epoch:16 step:15106 [D loss: 0.592778, acc: 73.44%] [G loss: 2.108889]\n",
      "epoch:16 step:15107 [D loss: 0.588755, acc: 67.19%] [G loss: 2.260732]\n",
      "epoch:16 step:15108 [D loss: 0.662049, acc: 60.94%] [G loss: 2.142700]\n",
      "epoch:16 step:15109 [D loss: 0.647547, acc: 62.50%] [G loss: 2.133297]\n",
      "epoch:16 step:15110 [D loss: 0.688740, acc: 59.38%] [G loss: 1.877794]\n",
      "epoch:16 step:15111 [D loss: 0.593521, acc: 69.53%] [G loss: 2.047255]\n",
      "epoch:16 step:15112 [D loss: 0.652561, acc: 56.25%] [G loss: 1.949308]\n",
      "epoch:16 step:15113 [D loss: 0.668843, acc: 58.59%] [G loss: 1.905910]\n",
      "epoch:16 step:15114 [D loss: 0.646116, acc: 63.28%] [G loss: 2.056388]\n",
      "epoch:16 step:15115 [D loss: 0.643943, acc: 62.50%] [G loss: 1.883862]\n",
      "epoch:16 step:15116 [D loss: 0.669271, acc: 63.28%] [G loss: 1.931834]\n",
      "epoch:16 step:15117 [D loss: 0.718914, acc: 49.22%] [G loss: 1.759685]\n",
      "epoch:16 step:15118 [D loss: 0.610247, acc: 64.84%] [G loss: 2.015785]\n",
      "epoch:16 step:15119 [D loss: 0.650058, acc: 55.47%] [G loss: 2.056371]\n",
      "epoch:16 step:15120 [D loss: 0.663295, acc: 54.69%] [G loss: 2.016411]\n",
      "epoch:16 step:15121 [D loss: 0.646550, acc: 61.72%] [G loss: 1.749020]\n",
      "epoch:16 step:15122 [D loss: 0.641845, acc: 62.50%] [G loss: 1.964251]\n",
      "epoch:16 step:15123 [D loss: 0.655256, acc: 61.72%] [G loss: 2.100980]\n",
      "epoch:16 step:15124 [D loss: 0.688558, acc: 53.12%] [G loss: 2.004088]\n",
      "epoch:16 step:15125 [D loss: 0.666780, acc: 57.81%] [G loss: 1.839221]\n",
      "epoch:16 step:15126 [D loss: 0.667910, acc: 58.59%] [G loss: 1.784144]\n",
      "epoch:16 step:15127 [D loss: 0.625233, acc: 68.75%] [G loss: 1.840981]\n",
      "epoch:16 step:15128 [D loss: 0.649400, acc: 59.38%] [G loss: 1.822206]\n",
      "epoch:16 step:15129 [D loss: 0.614657, acc: 69.53%] [G loss: 1.894066]\n",
      "epoch:16 step:15130 [D loss: 0.644578, acc: 61.72%] [G loss: 1.787901]\n",
      "epoch:16 step:15131 [D loss: 0.651149, acc: 60.94%] [G loss: 1.967505]\n",
      "epoch:16 step:15132 [D loss: 0.620964, acc: 64.84%] [G loss: 1.926384]\n",
      "epoch:16 step:15133 [D loss: 0.662746, acc: 57.03%] [G loss: 1.890268]\n",
      "epoch:16 step:15134 [D loss: 0.645333, acc: 66.41%] [G loss: 1.831001]\n",
      "epoch:16 step:15135 [D loss: 0.680631, acc: 60.16%] [G loss: 1.913886]\n",
      "epoch:16 step:15136 [D loss: 0.655254, acc: 61.72%] [G loss: 1.813267]\n",
      "epoch:16 step:15137 [D loss: 0.631867, acc: 65.62%] [G loss: 1.975852]\n",
      "epoch:16 step:15138 [D loss: 0.619632, acc: 65.62%] [G loss: 1.904297]\n",
      "epoch:16 step:15139 [D loss: 0.690595, acc: 60.16%] [G loss: 1.701894]\n",
      "epoch:16 step:15140 [D loss: 0.696876, acc: 53.12%] [G loss: 1.852424]\n",
      "epoch:16 step:15141 [D loss: 0.599917, acc: 72.66%] [G loss: 1.830564]\n",
      "epoch:16 step:15142 [D loss: 0.603688, acc: 66.41%] [G loss: 1.816629]\n",
      "epoch:16 step:15143 [D loss: 0.644394, acc: 63.28%] [G loss: 1.914587]\n",
      "epoch:16 step:15144 [D loss: 0.688652, acc: 59.38%] [G loss: 2.009295]\n",
      "epoch:16 step:15145 [D loss: 0.620708, acc: 65.62%] [G loss: 1.773421]\n",
      "epoch:16 step:15146 [D loss: 0.620854, acc: 64.84%] [G loss: 2.034869]\n",
      "epoch:16 step:15147 [D loss: 0.640662, acc: 62.50%] [G loss: 2.019810]\n",
      "epoch:16 step:15148 [D loss: 0.657084, acc: 58.59%] [G loss: 1.900939]\n",
      "epoch:16 step:15149 [D loss: 0.628404, acc: 64.06%] [G loss: 1.902838]\n",
      "epoch:16 step:15150 [D loss: 0.636960, acc: 61.72%] [G loss: 1.923608]\n",
      "epoch:16 step:15151 [D loss: 0.601600, acc: 71.09%] [G loss: 2.033325]\n",
      "epoch:16 step:15152 [D loss: 0.733447, acc: 53.12%] [G loss: 1.719543]\n",
      "epoch:16 step:15153 [D loss: 0.664131, acc: 61.72%] [G loss: 1.811954]\n",
      "epoch:16 step:15154 [D loss: 0.619981, acc: 70.31%] [G loss: 1.912234]\n",
      "epoch:16 step:15155 [D loss: 0.635244, acc: 64.84%] [G loss: 1.926018]\n",
      "epoch:16 step:15156 [D loss: 0.610089, acc: 66.41%] [G loss: 1.865798]\n",
      "epoch:16 step:15157 [D loss: 0.653403, acc: 61.72%] [G loss: 1.934930]\n",
      "epoch:16 step:15158 [D loss: 0.633644, acc: 63.28%] [G loss: 1.779852]\n",
      "epoch:16 step:15159 [D loss: 0.653961, acc: 60.94%] [G loss: 2.036054]\n",
      "epoch:16 step:15160 [D loss: 0.612909, acc: 67.97%] [G loss: 1.860795]\n",
      "epoch:16 step:15161 [D loss: 0.647496, acc: 58.59%] [G loss: 1.886307]\n",
      "epoch:16 step:15162 [D loss: 0.684198, acc: 57.03%] [G loss: 1.941951]\n",
      "epoch:16 step:15163 [D loss: 0.647165, acc: 63.28%] [G loss: 1.883081]\n",
      "epoch:16 step:15164 [D loss: 0.649273, acc: 59.38%] [G loss: 2.004396]\n",
      "epoch:16 step:15165 [D loss: 0.678927, acc: 57.81%] [G loss: 1.840330]\n",
      "epoch:16 step:15166 [D loss: 0.681419, acc: 60.94%] [G loss: 1.818057]\n",
      "epoch:16 step:15167 [D loss: 0.632872, acc: 60.94%] [G loss: 1.911036]\n",
      "epoch:16 step:15168 [D loss: 0.652350, acc: 60.16%] [G loss: 1.804209]\n",
      "epoch:16 step:15169 [D loss: 0.657183, acc: 60.16%] [G loss: 1.932739]\n",
      "epoch:16 step:15170 [D loss: 0.665342, acc: 54.69%] [G loss: 1.862002]\n",
      "epoch:16 step:15171 [D loss: 0.676028, acc: 66.41%] [G loss: 1.847779]\n",
      "epoch:16 step:15172 [D loss: 0.646902, acc: 63.28%] [G loss: 1.763311]\n",
      "epoch:16 step:15173 [D loss: 0.702897, acc: 51.56%] [G loss: 1.822298]\n",
      "epoch:16 step:15174 [D loss: 0.667183, acc: 64.06%] [G loss: 1.900644]\n",
      "epoch:16 step:15175 [D loss: 0.634815, acc: 64.06%] [G loss: 1.759393]\n",
      "epoch:16 step:15176 [D loss: 0.656101, acc: 62.50%] [G loss: 1.753030]\n",
      "epoch:16 step:15177 [D loss: 0.637663, acc: 64.06%] [G loss: 1.883500]\n",
      "epoch:16 step:15178 [D loss: 0.645657, acc: 62.50%] [G loss: 1.942999]\n",
      "epoch:16 step:15179 [D loss: 0.613984, acc: 67.97%] [G loss: 2.059096]\n",
      "epoch:16 step:15180 [D loss: 0.626310, acc: 66.41%] [G loss: 1.968663]\n",
      "epoch:16 step:15181 [D loss: 0.582891, acc: 70.31%] [G loss: 1.962972]\n",
      "epoch:16 step:15182 [D loss: 0.631619, acc: 62.50%] [G loss: 1.960030]\n",
      "epoch:16 step:15183 [D loss: 0.660983, acc: 59.38%] [G loss: 1.915256]\n",
      "epoch:16 step:15184 [D loss: 0.610171, acc: 64.06%] [G loss: 1.908598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15185 [D loss: 0.616173, acc: 60.16%] [G loss: 2.064458]\n",
      "epoch:16 step:15186 [D loss: 0.600240, acc: 67.19%] [G loss: 2.120690]\n",
      "epoch:16 step:15187 [D loss: 0.590023, acc: 68.75%] [G loss: 2.143278]\n",
      "epoch:16 step:15188 [D loss: 0.732767, acc: 53.91%] [G loss: 1.979652]\n",
      "epoch:16 step:15189 [D loss: 0.609071, acc: 67.19%] [G loss: 1.874661]\n",
      "epoch:16 step:15190 [D loss: 0.627212, acc: 64.84%] [G loss: 1.943603]\n",
      "epoch:16 step:15191 [D loss: 0.642906, acc: 60.16%] [G loss: 1.898269]\n",
      "epoch:16 step:15192 [D loss: 0.719094, acc: 57.03%] [G loss: 1.813139]\n",
      "epoch:16 step:15193 [D loss: 0.610409, acc: 64.06%] [G loss: 1.961357]\n",
      "epoch:16 step:15194 [D loss: 0.634948, acc: 64.06%] [G loss: 1.892216]\n",
      "epoch:16 step:15195 [D loss: 0.640142, acc: 64.06%] [G loss: 1.937666]\n",
      "epoch:16 step:15196 [D loss: 0.708973, acc: 53.91%] [G loss: 1.884674]\n",
      "epoch:16 step:15197 [D loss: 0.656572, acc: 62.50%] [G loss: 1.808065]\n",
      "epoch:16 step:15198 [D loss: 0.636827, acc: 65.62%] [G loss: 1.890066]\n",
      "epoch:16 step:15199 [D loss: 0.621917, acc: 67.97%] [G loss: 2.045937]\n",
      "epoch:16 step:15200 [D loss: 0.593467, acc: 71.09%] [G loss: 2.214993]\n",
      "##############\n",
      "[2.48488563 1.42692076 6.17875891 4.69318755 3.67156073 5.49246976\n",
      " 4.2170083  4.55111377 4.53426059 3.81248622]\n",
      "##########\n",
      "epoch:16 step:15201 [D loss: 0.617065, acc: 68.75%] [G loss: 2.281927]\n",
      "epoch:16 step:15202 [D loss: 0.631546, acc: 67.19%] [G loss: 1.932090]\n",
      "epoch:16 step:15203 [D loss: 0.694086, acc: 53.12%] [G loss: 1.836891]\n",
      "epoch:16 step:15204 [D loss: 0.643742, acc: 64.84%] [G loss: 1.859604]\n",
      "epoch:16 step:15205 [D loss: 0.641089, acc: 65.62%] [G loss: 1.897561]\n",
      "epoch:16 step:15206 [D loss: 0.642180, acc: 64.06%] [G loss: 1.800205]\n",
      "epoch:16 step:15207 [D loss: 0.680397, acc: 60.16%] [G loss: 1.792383]\n",
      "epoch:16 step:15208 [D loss: 0.604786, acc: 62.50%] [G loss: 1.898920]\n",
      "epoch:16 step:15209 [D loss: 0.647262, acc: 67.97%] [G loss: 1.928702]\n",
      "epoch:16 step:15210 [D loss: 0.601502, acc: 72.66%] [G loss: 2.147542]\n",
      "epoch:16 step:15211 [D loss: 0.584318, acc: 68.75%] [G loss: 2.098571]\n",
      "epoch:16 step:15212 [D loss: 0.708845, acc: 52.34%] [G loss: 1.715522]\n",
      "epoch:16 step:15213 [D loss: 0.628006, acc: 64.06%] [G loss: 1.926458]\n",
      "epoch:16 step:15214 [D loss: 0.658513, acc: 64.06%] [G loss: 1.889983]\n",
      "epoch:16 step:15215 [D loss: 0.638790, acc: 64.06%] [G loss: 1.937561]\n",
      "epoch:16 step:15216 [D loss: 0.612627, acc: 63.28%] [G loss: 1.837080]\n",
      "epoch:16 step:15217 [D loss: 0.659578, acc: 53.91%] [G loss: 1.931729]\n",
      "epoch:16 step:15218 [D loss: 0.649157, acc: 62.50%] [G loss: 1.906111]\n",
      "epoch:16 step:15219 [D loss: 0.621475, acc: 62.50%] [G loss: 1.848859]\n",
      "epoch:16 step:15220 [D loss: 0.664237, acc: 57.81%] [G loss: 1.818382]\n",
      "epoch:16 step:15221 [D loss: 0.612436, acc: 69.53%] [G loss: 2.073635]\n",
      "epoch:16 step:15222 [D loss: 0.582441, acc: 71.88%] [G loss: 2.116358]\n",
      "epoch:16 step:15223 [D loss: 0.609389, acc: 69.53%] [G loss: 2.242165]\n",
      "epoch:16 step:15224 [D loss: 0.594963, acc: 65.62%] [G loss: 2.220277]\n",
      "epoch:16 step:15225 [D loss: 0.685447, acc: 60.16%] [G loss: 1.828331]\n",
      "epoch:16 step:15226 [D loss: 0.644841, acc: 62.50%] [G loss: 2.088768]\n",
      "epoch:16 step:15227 [D loss: 0.666878, acc: 56.25%] [G loss: 1.917471]\n",
      "epoch:16 step:15228 [D loss: 0.632846, acc: 62.50%] [G loss: 1.950644]\n",
      "epoch:16 step:15229 [D loss: 0.648861, acc: 60.94%] [G loss: 2.030267]\n",
      "epoch:16 step:15230 [D loss: 0.655001, acc: 60.94%] [G loss: 2.109275]\n",
      "epoch:16 step:15231 [D loss: 0.659407, acc: 60.16%] [G loss: 1.971612]\n",
      "epoch:16 step:15232 [D loss: 0.627566, acc: 62.50%] [G loss: 2.008246]\n",
      "epoch:16 step:15233 [D loss: 0.620988, acc: 68.75%] [G loss: 2.025572]\n",
      "epoch:16 step:15234 [D loss: 0.645378, acc: 64.06%] [G loss: 2.025823]\n",
      "epoch:16 step:15235 [D loss: 0.639943, acc: 59.38%] [G loss: 1.900287]\n",
      "epoch:16 step:15236 [D loss: 0.607882, acc: 68.75%] [G loss: 1.873415]\n",
      "epoch:16 step:15237 [D loss: 0.643389, acc: 66.41%] [G loss: 1.874551]\n",
      "epoch:16 step:15238 [D loss: 0.646132, acc: 64.06%] [G loss: 2.123104]\n",
      "epoch:16 step:15239 [D loss: 0.664832, acc: 59.38%] [G loss: 2.000228]\n",
      "epoch:16 step:15240 [D loss: 0.651811, acc: 62.50%] [G loss: 2.146667]\n",
      "epoch:16 step:15241 [D loss: 0.673050, acc: 58.59%] [G loss: 1.774933]\n",
      "epoch:16 step:15242 [D loss: 0.727539, acc: 53.91%] [G loss: 1.764529]\n",
      "epoch:16 step:15243 [D loss: 0.671909, acc: 56.25%] [G loss: 1.822027]\n",
      "epoch:16 step:15244 [D loss: 0.714525, acc: 53.12%] [G loss: 1.747343]\n",
      "epoch:16 step:15245 [D loss: 0.654299, acc: 53.91%] [G loss: 1.836411]\n",
      "epoch:16 step:15246 [D loss: 0.655268, acc: 60.94%] [G loss: 1.799774]\n",
      "epoch:16 step:15247 [D loss: 0.658382, acc: 57.81%] [G loss: 1.800848]\n",
      "epoch:16 step:15248 [D loss: 0.681387, acc: 54.69%] [G loss: 1.795568]\n",
      "epoch:16 step:15249 [D loss: 0.647303, acc: 63.28%] [G loss: 1.938734]\n",
      "epoch:16 step:15250 [D loss: 0.689347, acc: 58.59%] [G loss: 1.827799]\n",
      "epoch:16 step:15251 [D loss: 0.615302, acc: 64.84%] [G loss: 1.886632]\n",
      "epoch:16 step:15252 [D loss: 0.643436, acc: 60.16%] [G loss: 1.838569]\n",
      "epoch:16 step:15253 [D loss: 0.670037, acc: 61.72%] [G loss: 1.903704]\n",
      "epoch:16 step:15254 [D loss: 0.616894, acc: 66.41%] [G loss: 1.929236]\n",
      "epoch:16 step:15255 [D loss: 0.661394, acc: 61.72%] [G loss: 1.820549]\n",
      "epoch:16 step:15256 [D loss: 0.617055, acc: 65.62%] [G loss: 2.009928]\n",
      "epoch:16 step:15257 [D loss: 0.653287, acc: 61.72%] [G loss: 1.839915]\n",
      "epoch:16 step:15258 [D loss: 0.645567, acc: 63.28%] [G loss: 1.874988]\n",
      "epoch:16 step:15259 [D loss: 0.658256, acc: 59.38%] [G loss: 1.873088]\n",
      "epoch:16 step:15260 [D loss: 0.682213, acc: 60.16%] [G loss: 1.787284]\n",
      "epoch:16 step:15261 [D loss: 0.604098, acc: 66.41%] [G loss: 1.971561]\n",
      "epoch:16 step:15262 [D loss: 0.582447, acc: 70.31%] [G loss: 1.898188]\n",
      "epoch:16 step:15263 [D loss: 0.628551, acc: 67.19%] [G loss: 1.977979]\n",
      "epoch:16 step:15264 [D loss: 0.630941, acc: 62.50%] [G loss: 1.978106]\n",
      "epoch:16 step:15265 [D loss: 0.650292, acc: 64.84%] [G loss: 1.834292]\n",
      "epoch:16 step:15266 [D loss: 0.638706, acc: 61.72%] [G loss: 2.275273]\n",
      "epoch:16 step:15267 [D loss: 0.622277, acc: 64.84%] [G loss: 2.098008]\n",
      "epoch:16 step:15268 [D loss: 0.609663, acc: 61.72%] [G loss: 2.218642]\n",
      "epoch:16 step:15269 [D loss: 0.667106, acc: 57.03%] [G loss: 2.014744]\n",
      "epoch:16 step:15270 [D loss: 0.695635, acc: 60.16%] [G loss: 1.728572]\n",
      "epoch:16 step:15271 [D loss: 0.660497, acc: 61.72%] [G loss: 1.980889]\n",
      "epoch:16 step:15272 [D loss: 0.642286, acc: 55.47%] [G loss: 1.838424]\n",
      "epoch:16 step:15273 [D loss: 0.673667, acc: 57.81%] [G loss: 1.763391]\n",
      "epoch:16 step:15274 [D loss: 0.650052, acc: 64.06%] [G loss: 2.026799]\n",
      "epoch:16 step:15275 [D loss: 0.643427, acc: 65.62%] [G loss: 1.873885]\n",
      "epoch:16 step:15276 [D loss: 0.605658, acc: 64.06%] [G loss: 1.853739]\n",
      "epoch:16 step:15277 [D loss: 0.617569, acc: 69.53%] [G loss: 1.907770]\n",
      "epoch:16 step:15278 [D loss: 0.654410, acc: 59.38%] [G loss: 1.858031]\n",
      "epoch:16 step:15279 [D loss: 0.662840, acc: 59.38%] [G loss: 1.913186]\n",
      "epoch:16 step:15280 [D loss: 0.627927, acc: 68.75%] [G loss: 1.731181]\n",
      "epoch:16 step:15281 [D loss: 0.657362, acc: 56.25%] [G loss: 1.825788]\n",
      "epoch:16 step:15282 [D loss: 0.648394, acc: 60.16%] [G loss: 1.843669]\n",
      "epoch:16 step:15283 [D loss: 0.708803, acc: 63.28%] [G loss: 1.885817]\n",
      "epoch:16 step:15284 [D loss: 0.639106, acc: 62.50%] [G loss: 1.890856]\n",
      "epoch:16 step:15285 [D loss: 0.579071, acc: 69.53%] [G loss: 2.034961]\n",
      "epoch:16 step:15286 [D loss: 0.630231, acc: 67.19%] [G loss: 1.863557]\n",
      "epoch:16 step:15287 [D loss: 0.621840, acc: 61.72%] [G loss: 1.864428]\n",
      "epoch:16 step:15288 [D loss: 0.617316, acc: 64.84%] [G loss: 1.968048]\n",
      "epoch:16 step:15289 [D loss: 0.625132, acc: 63.28%] [G loss: 1.912783]\n",
      "epoch:16 step:15290 [D loss: 0.645411, acc: 64.84%] [G loss: 1.971552]\n",
      "epoch:16 step:15291 [D loss: 0.584007, acc: 64.84%] [G loss: 1.838885]\n",
      "epoch:16 step:15292 [D loss: 0.630488, acc: 64.84%] [G loss: 1.980252]\n",
      "epoch:16 step:15293 [D loss: 0.700642, acc: 53.91%] [G loss: 1.879511]\n",
      "epoch:16 step:15294 [D loss: 0.606124, acc: 68.75%] [G loss: 2.007208]\n",
      "epoch:16 step:15295 [D loss: 0.622918, acc: 64.06%] [G loss: 2.001804]\n",
      "epoch:16 step:15296 [D loss: 0.631533, acc: 64.84%] [G loss: 1.971019]\n",
      "epoch:16 step:15297 [D loss: 0.664346, acc: 60.94%] [G loss: 1.815240]\n",
      "epoch:16 step:15298 [D loss: 0.663135, acc: 62.50%] [G loss: 1.912710]\n",
      "epoch:16 step:15299 [D loss: 0.645911, acc: 61.72%] [G loss: 1.871185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15300 [D loss: 0.623436, acc: 65.62%] [G loss: 1.928190]\n",
      "epoch:16 step:15301 [D loss: 0.694863, acc: 54.69%] [G loss: 2.000744]\n",
      "epoch:16 step:15302 [D loss: 0.583932, acc: 72.66%] [G loss: 1.916777]\n",
      "epoch:16 step:15303 [D loss: 0.590216, acc: 70.31%] [G loss: 2.054406]\n",
      "epoch:16 step:15304 [D loss: 0.663968, acc: 64.84%] [G loss: 2.128532]\n",
      "epoch:16 step:15305 [D loss: 0.608483, acc: 69.53%] [G loss: 2.208354]\n",
      "epoch:16 step:15306 [D loss: 0.607137, acc: 69.53%] [G loss: 2.188841]\n",
      "epoch:16 step:15307 [D loss: 0.629081, acc: 60.16%] [G loss: 2.098231]\n",
      "epoch:16 step:15308 [D loss: 0.724739, acc: 60.16%] [G loss: 1.794079]\n",
      "epoch:16 step:15309 [D loss: 0.658092, acc: 62.50%] [G loss: 1.733303]\n",
      "epoch:16 step:15310 [D loss: 0.654434, acc: 62.50%] [G loss: 1.785029]\n",
      "epoch:16 step:15311 [D loss: 0.636379, acc: 63.28%] [G loss: 1.724828]\n",
      "epoch:16 step:15312 [D loss: 0.667558, acc: 58.59%] [G loss: 1.900877]\n",
      "epoch:16 step:15313 [D loss: 0.627354, acc: 64.84%] [G loss: 1.981965]\n",
      "epoch:16 step:15314 [D loss: 0.695556, acc: 57.03%] [G loss: 2.053975]\n",
      "epoch:16 step:15315 [D loss: 0.645457, acc: 58.59%] [G loss: 1.759738]\n",
      "epoch:16 step:15316 [D loss: 0.620580, acc: 60.16%] [G loss: 1.736719]\n",
      "epoch:16 step:15317 [D loss: 0.729213, acc: 48.44%] [G loss: 1.804528]\n",
      "epoch:16 step:15318 [D loss: 0.655858, acc: 61.72%] [G loss: 1.841865]\n",
      "epoch:16 step:15319 [D loss: 0.653917, acc: 67.19%] [G loss: 1.818881]\n",
      "epoch:16 step:15320 [D loss: 0.703331, acc: 52.34%] [G loss: 1.840045]\n",
      "epoch:16 step:15321 [D loss: 0.629260, acc: 66.41%] [G loss: 1.728755]\n",
      "epoch:16 step:15322 [D loss: 0.670726, acc: 59.38%] [G loss: 1.959554]\n",
      "epoch:16 step:15323 [D loss: 0.635667, acc: 64.06%] [G loss: 1.948646]\n",
      "epoch:16 step:15324 [D loss: 0.655419, acc: 57.03%] [G loss: 1.928215]\n",
      "epoch:16 step:15325 [D loss: 0.607580, acc: 71.09%] [G loss: 1.914921]\n",
      "epoch:16 step:15326 [D loss: 0.651717, acc: 60.94%] [G loss: 1.929587]\n",
      "epoch:16 step:15327 [D loss: 0.629824, acc: 60.16%] [G loss: 1.958054]\n",
      "epoch:16 step:15328 [D loss: 0.636571, acc: 64.06%] [G loss: 1.903971]\n",
      "epoch:16 step:15329 [D loss: 0.660928, acc: 60.16%] [G loss: 2.041129]\n",
      "epoch:16 step:15330 [D loss: 0.636526, acc: 70.31%] [G loss: 1.907644]\n",
      "epoch:16 step:15331 [D loss: 0.622480, acc: 65.62%] [G loss: 1.867302]\n",
      "epoch:16 step:15332 [D loss: 0.685797, acc: 59.38%] [G loss: 1.809092]\n",
      "epoch:16 step:15333 [D loss: 0.676575, acc: 63.28%] [G loss: 1.744299]\n",
      "epoch:16 step:15334 [D loss: 0.659630, acc: 60.94%] [G loss: 1.805438]\n",
      "epoch:16 step:15335 [D loss: 0.672736, acc: 57.81%] [G loss: 1.922748]\n",
      "epoch:16 step:15336 [D loss: 0.603923, acc: 71.88%] [G loss: 1.878097]\n",
      "epoch:16 step:15337 [D loss: 0.635832, acc: 64.84%] [G loss: 2.125969]\n",
      "epoch:16 step:15338 [D loss: 0.596250, acc: 66.41%] [G loss: 2.131367]\n",
      "epoch:16 step:15339 [D loss: 0.599798, acc: 63.28%] [G loss: 2.205986]\n",
      "epoch:16 step:15340 [D loss: 0.686945, acc: 55.47%] [G loss: 1.894140]\n",
      "epoch:16 step:15341 [D loss: 0.691688, acc: 50.78%] [G loss: 1.725365]\n",
      "epoch:16 step:15342 [D loss: 0.680085, acc: 60.94%] [G loss: 1.971516]\n",
      "epoch:16 step:15343 [D loss: 0.645630, acc: 64.84%] [G loss: 1.713908]\n",
      "epoch:16 step:15344 [D loss: 0.656860, acc: 60.16%] [G loss: 1.891634]\n",
      "epoch:16 step:15345 [D loss: 0.651643, acc: 60.94%] [G loss: 1.945984]\n",
      "epoch:16 step:15346 [D loss: 0.583375, acc: 67.97%] [G loss: 2.007258]\n",
      "epoch:16 step:15347 [D loss: 0.646988, acc: 61.72%] [G loss: 1.953679]\n",
      "epoch:16 step:15348 [D loss: 0.665278, acc: 59.38%] [G loss: 1.730671]\n",
      "epoch:16 step:15349 [D loss: 0.637010, acc: 59.38%] [G loss: 2.157732]\n",
      "epoch:16 step:15350 [D loss: 0.615099, acc: 63.28%] [G loss: 2.019462]\n",
      "epoch:16 step:15351 [D loss: 0.635075, acc: 64.84%] [G loss: 2.007635]\n",
      "epoch:16 step:15352 [D loss: 0.619246, acc: 64.84%] [G loss: 1.984589]\n",
      "epoch:16 step:15353 [D loss: 0.642775, acc: 64.06%] [G loss: 1.927689]\n",
      "epoch:16 step:15354 [D loss: 0.690138, acc: 53.91%] [G loss: 1.827999]\n",
      "epoch:16 step:15355 [D loss: 0.649295, acc: 59.38%] [G loss: 1.928853]\n",
      "epoch:16 step:15356 [D loss: 0.661603, acc: 64.06%] [G loss: 1.987956]\n",
      "epoch:16 step:15357 [D loss: 0.665548, acc: 64.84%] [G loss: 1.917479]\n",
      "epoch:16 step:15358 [D loss: 0.612733, acc: 67.19%] [G loss: 1.972059]\n",
      "epoch:16 step:15359 [D loss: 0.636501, acc: 64.06%] [G loss: 1.945132]\n",
      "epoch:16 step:15360 [D loss: 0.626641, acc: 67.19%] [G loss: 1.940252]\n",
      "epoch:16 step:15361 [D loss: 0.660154, acc: 61.72%] [G loss: 1.945358]\n",
      "epoch:16 step:15362 [D loss: 0.649127, acc: 64.06%] [G loss: 1.912197]\n",
      "epoch:16 step:15363 [D loss: 0.636624, acc: 57.03%] [G loss: 2.000282]\n",
      "epoch:16 step:15364 [D loss: 0.631035, acc: 64.84%] [G loss: 2.005786]\n",
      "epoch:16 step:15365 [D loss: 0.648483, acc: 60.16%] [G loss: 1.691982]\n",
      "epoch:16 step:15366 [D loss: 0.612650, acc: 65.62%] [G loss: 2.018215]\n",
      "epoch:16 step:15367 [D loss: 0.665668, acc: 58.59%] [G loss: 1.953281]\n",
      "epoch:16 step:15368 [D loss: 0.669343, acc: 57.81%] [G loss: 1.827082]\n",
      "epoch:16 step:15369 [D loss: 0.689550, acc: 51.56%] [G loss: 1.632846]\n",
      "epoch:16 step:15370 [D loss: 0.656622, acc: 60.94%] [G loss: 1.743102]\n",
      "epoch:16 step:15371 [D loss: 0.650782, acc: 62.50%] [G loss: 2.012381]\n",
      "epoch:16 step:15372 [D loss: 0.688205, acc: 58.59%] [G loss: 2.080821]\n",
      "epoch:16 step:15373 [D loss: 0.599005, acc: 67.97%] [G loss: 2.053314]\n",
      "epoch:16 step:15374 [D loss: 0.646000, acc: 60.16%] [G loss: 1.806371]\n",
      "epoch:16 step:15375 [D loss: 0.629201, acc: 64.06%] [G loss: 1.879755]\n",
      "epoch:16 step:15376 [D loss: 0.616153, acc: 64.06%] [G loss: 1.899703]\n",
      "epoch:16 step:15377 [D loss: 0.630770, acc: 65.62%] [G loss: 1.895555]\n",
      "epoch:16 step:15378 [D loss: 0.652057, acc: 64.06%] [G loss: 1.839969]\n",
      "epoch:16 step:15379 [D loss: 0.648506, acc: 63.28%] [G loss: 1.888134]\n",
      "epoch:16 step:15380 [D loss: 0.628419, acc: 67.97%] [G loss: 1.693456]\n",
      "epoch:16 step:15381 [D loss: 0.616465, acc: 68.75%] [G loss: 2.012209]\n",
      "epoch:16 step:15382 [D loss: 0.715022, acc: 60.94%] [G loss: 1.963391]\n",
      "epoch:16 step:15383 [D loss: 0.661622, acc: 62.50%] [G loss: 1.743996]\n",
      "epoch:16 step:15384 [D loss: 0.643267, acc: 64.84%] [G loss: 1.950856]\n",
      "epoch:16 step:15385 [D loss: 0.638694, acc: 62.50%] [G loss: 1.811794]\n",
      "epoch:16 step:15386 [D loss: 0.627417, acc: 62.50%] [G loss: 1.916247]\n",
      "epoch:16 step:15387 [D loss: 0.621293, acc: 66.41%] [G loss: 1.926713]\n",
      "epoch:16 step:15388 [D loss: 0.705274, acc: 56.25%] [G loss: 1.831580]\n",
      "epoch:16 step:15389 [D loss: 0.642176, acc: 63.28%] [G loss: 1.835511]\n",
      "epoch:16 step:15390 [D loss: 0.608608, acc: 66.41%] [G loss: 1.924817]\n",
      "epoch:16 step:15391 [D loss: 0.648115, acc: 63.28%] [G loss: 1.934782]\n",
      "epoch:16 step:15392 [D loss: 0.615971, acc: 64.84%] [G loss: 1.786411]\n",
      "epoch:16 step:15393 [D loss: 0.661546, acc: 63.28%] [G loss: 1.889031]\n",
      "epoch:16 step:15394 [D loss: 0.671154, acc: 58.59%] [G loss: 1.995343]\n",
      "epoch:16 step:15395 [D loss: 0.651275, acc: 65.62%] [G loss: 1.872465]\n",
      "epoch:16 step:15396 [D loss: 0.622423, acc: 68.75%] [G loss: 1.954133]\n",
      "epoch:16 step:15397 [D loss: 0.639868, acc: 63.28%] [G loss: 1.957504]\n",
      "epoch:16 step:15398 [D loss: 0.599880, acc: 63.28%] [G loss: 2.041575]\n",
      "epoch:16 step:15399 [D loss: 0.660408, acc: 62.50%] [G loss: 1.890847]\n",
      "epoch:16 step:15400 [D loss: 0.682807, acc: 57.81%] [G loss: 1.874518]\n",
      "##############\n",
      "[2.566384   1.29916274 6.28847324 4.75636164 3.54854493 5.76341246\n",
      " 4.32005203 4.66979965 4.70725368 3.75988456]\n",
      "##########\n",
      "epoch:16 step:15401 [D loss: 0.659202, acc: 61.72%] [G loss: 1.873816]\n",
      "epoch:16 step:15402 [D loss: 0.657731, acc: 60.94%] [G loss: 1.882886]\n",
      "epoch:16 step:15403 [D loss: 0.666532, acc: 58.59%] [G loss: 1.814704]\n",
      "epoch:16 step:15404 [D loss: 0.659660, acc: 63.28%] [G loss: 1.886905]\n",
      "epoch:16 step:15405 [D loss: 0.650497, acc: 61.72%] [G loss: 2.048434]\n",
      "epoch:16 step:15406 [D loss: 0.658766, acc: 59.38%] [G loss: 1.932800]\n",
      "epoch:16 step:15407 [D loss: 0.659507, acc: 59.38%] [G loss: 1.996581]\n",
      "epoch:16 step:15408 [D loss: 0.653442, acc: 62.50%] [G loss: 2.067225]\n",
      "epoch:16 step:15409 [D loss: 0.635676, acc: 63.28%] [G loss: 1.903456]\n",
      "epoch:16 step:15410 [D loss: 0.696332, acc: 57.81%] [G loss: 1.750706]\n",
      "epoch:16 step:15411 [D loss: 0.672027, acc: 58.59%] [G loss: 2.032558]\n",
      "epoch:16 step:15412 [D loss: 0.647114, acc: 66.41%] [G loss: 1.958357]\n",
      "epoch:16 step:15413 [D loss: 0.649974, acc: 64.06%] [G loss: 1.835606]\n",
      "epoch:16 step:15414 [D loss: 0.678446, acc: 57.03%] [G loss: 1.881818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15415 [D loss: 0.655299, acc: 61.72%] [G loss: 1.916769]\n",
      "epoch:16 step:15416 [D loss: 0.661613, acc: 60.16%] [G loss: 1.727419]\n",
      "epoch:16 step:15417 [D loss: 0.643321, acc: 65.62%] [G loss: 1.945928]\n",
      "epoch:16 step:15418 [D loss: 0.616565, acc: 65.62%] [G loss: 1.992315]\n",
      "epoch:16 step:15419 [D loss: 0.641473, acc: 58.59%] [G loss: 1.905756]\n",
      "epoch:16 step:15420 [D loss: 0.607972, acc: 67.19%] [G loss: 2.031981]\n",
      "epoch:16 step:15421 [D loss: 0.595022, acc: 67.19%] [G loss: 2.244600]\n",
      "epoch:16 step:15422 [D loss: 0.603370, acc: 70.31%] [G loss: 1.984511]\n",
      "epoch:16 step:15423 [D loss: 0.673957, acc: 60.94%] [G loss: 1.834107]\n",
      "epoch:16 step:15424 [D loss: 0.658937, acc: 59.38%] [G loss: 1.849338]\n",
      "epoch:16 step:15425 [D loss: 0.690067, acc: 52.34%] [G loss: 1.791120]\n",
      "epoch:16 step:15426 [D loss: 0.649312, acc: 61.72%] [G loss: 1.901066]\n",
      "epoch:16 step:15427 [D loss: 0.639678, acc: 62.50%] [G loss: 1.960532]\n",
      "epoch:16 step:15428 [D loss: 0.636552, acc: 62.50%] [G loss: 1.875862]\n",
      "epoch:16 step:15429 [D loss: 0.675893, acc: 56.25%] [G loss: 1.817668]\n",
      "epoch:16 step:15430 [D loss: 0.661354, acc: 60.16%] [G loss: 1.795177]\n",
      "epoch:16 step:15431 [D loss: 0.675324, acc: 62.50%] [G loss: 1.834421]\n",
      "epoch:16 step:15432 [D loss: 0.649489, acc: 56.25%] [G loss: 1.784058]\n",
      "epoch:16 step:15433 [D loss: 0.665250, acc: 59.38%] [G loss: 1.816801]\n",
      "epoch:16 step:15434 [D loss: 0.657851, acc: 60.16%] [G loss: 1.870689]\n",
      "epoch:16 step:15435 [D loss: 0.671871, acc: 60.16%] [G loss: 1.715754]\n",
      "epoch:16 step:15436 [D loss: 0.630756, acc: 64.84%] [G loss: 1.803134]\n",
      "epoch:16 step:15437 [D loss: 0.620201, acc: 62.50%] [G loss: 1.896321]\n",
      "epoch:16 step:15438 [D loss: 0.680736, acc: 60.16%] [G loss: 1.735187]\n",
      "epoch:16 step:15439 [D loss: 0.638835, acc: 60.94%] [G loss: 1.971464]\n",
      "epoch:16 step:15440 [D loss: 0.689932, acc: 54.69%] [G loss: 1.719282]\n",
      "epoch:16 step:15441 [D loss: 0.623824, acc: 66.41%] [G loss: 1.789633]\n",
      "epoch:16 step:15442 [D loss: 0.638926, acc: 61.72%] [G loss: 1.929822]\n",
      "epoch:16 step:15443 [D loss: 0.644705, acc: 67.19%] [G loss: 1.803166]\n",
      "epoch:16 step:15444 [D loss: 0.647729, acc: 63.28%] [G loss: 1.803437]\n",
      "epoch:16 step:15445 [D loss: 0.602475, acc: 74.22%] [G loss: 1.963477]\n",
      "epoch:16 step:15446 [D loss: 0.673115, acc: 58.59%] [G loss: 1.852790]\n",
      "epoch:16 step:15447 [D loss: 0.711025, acc: 57.03%] [G loss: 1.766050]\n",
      "epoch:16 step:15448 [D loss: 0.662936, acc: 60.16%] [G loss: 1.820414]\n",
      "epoch:16 step:15449 [D loss: 0.595689, acc: 68.75%] [G loss: 1.984651]\n",
      "epoch:16 step:15450 [D loss: 0.684333, acc: 60.16%] [G loss: 1.833207]\n",
      "epoch:16 step:15451 [D loss: 0.637761, acc: 66.41%] [G loss: 1.851256]\n",
      "epoch:16 step:15452 [D loss: 0.614363, acc: 67.97%] [G loss: 1.967197]\n",
      "epoch:16 step:15453 [D loss: 0.646995, acc: 58.59%] [G loss: 1.911659]\n",
      "epoch:16 step:15454 [D loss: 0.592874, acc: 68.75%] [G loss: 1.884588]\n",
      "epoch:16 step:15455 [D loss: 0.582833, acc: 71.09%] [G loss: 1.992317]\n",
      "epoch:16 step:15456 [D loss: 0.604743, acc: 64.84%] [G loss: 1.987751]\n",
      "epoch:16 step:15457 [D loss: 0.674290, acc: 53.12%] [G loss: 1.928138]\n",
      "epoch:16 step:15458 [D loss: 0.617098, acc: 69.53%] [G loss: 1.945268]\n",
      "epoch:16 step:15459 [D loss: 0.650865, acc: 61.72%] [G loss: 1.935786]\n",
      "epoch:16 step:15460 [D loss: 0.616961, acc: 64.06%] [G loss: 2.040270]\n",
      "epoch:16 step:15461 [D loss: 0.635206, acc: 61.72%] [G loss: 1.919738]\n",
      "epoch:16 step:15462 [D loss: 0.636877, acc: 66.41%] [G loss: 2.103176]\n",
      "epoch:16 step:15463 [D loss: 0.657421, acc: 61.72%] [G loss: 2.210159]\n",
      "epoch:16 step:15464 [D loss: 0.660898, acc: 56.25%] [G loss: 2.209430]\n",
      "epoch:16 step:15465 [D loss: 0.693311, acc: 57.81%] [G loss: 1.796141]\n",
      "epoch:16 step:15466 [D loss: 0.631578, acc: 59.38%] [G loss: 1.937153]\n",
      "epoch:16 step:15467 [D loss: 0.670769, acc: 61.72%] [G loss: 1.921821]\n",
      "epoch:16 step:15468 [D loss: 0.654130, acc: 58.59%] [G loss: 1.923272]\n",
      "epoch:16 step:15469 [D loss: 0.677067, acc: 53.91%] [G loss: 1.828953]\n",
      "epoch:16 step:15470 [D loss: 0.633640, acc: 64.06%] [G loss: 1.831173]\n",
      "epoch:16 step:15471 [D loss: 0.683957, acc: 60.16%] [G loss: 1.932126]\n",
      "epoch:16 step:15472 [D loss: 0.603941, acc: 67.97%] [G loss: 2.038402]\n",
      "epoch:16 step:15473 [D loss: 0.577324, acc: 69.53%] [G loss: 2.012902]\n",
      "epoch:16 step:15474 [D loss: 0.608617, acc: 71.09%] [G loss: 1.759786]\n",
      "epoch:16 step:15475 [D loss: 0.682954, acc: 57.81%] [G loss: 1.923593]\n",
      "epoch:16 step:15476 [D loss: 0.575140, acc: 74.22%] [G loss: 1.992720]\n",
      "epoch:16 step:15477 [D loss: 0.640004, acc: 62.50%] [G loss: 1.942042]\n",
      "epoch:16 step:15478 [D loss: 0.660305, acc: 56.25%] [G loss: 1.907583]\n",
      "epoch:16 step:15479 [D loss: 0.659948, acc: 62.50%] [G loss: 1.870286]\n",
      "epoch:16 step:15480 [D loss: 0.642162, acc: 64.06%] [G loss: 2.067872]\n",
      "epoch:16 step:15481 [D loss: 0.653088, acc: 62.50%] [G loss: 1.829234]\n",
      "epoch:16 step:15482 [D loss: 0.671239, acc: 60.94%] [G loss: 1.916672]\n",
      "epoch:16 step:15483 [D loss: 0.628743, acc: 66.41%] [G loss: 1.919780]\n",
      "epoch:16 step:15484 [D loss: 0.699847, acc: 57.81%] [G loss: 1.909559]\n",
      "epoch:16 step:15485 [D loss: 0.654926, acc: 64.06%] [G loss: 1.807376]\n",
      "epoch:16 step:15486 [D loss: 0.659716, acc: 60.94%] [G loss: 2.062163]\n",
      "epoch:16 step:15487 [D loss: 0.592564, acc: 71.09%] [G loss: 1.988132]\n",
      "epoch:16 step:15488 [D loss: 0.656390, acc: 60.94%] [G loss: 1.866244]\n",
      "epoch:16 step:15489 [D loss: 0.629083, acc: 64.06%] [G loss: 2.012826]\n",
      "epoch:16 step:15490 [D loss: 0.597655, acc: 70.31%] [G loss: 1.987985]\n",
      "epoch:16 step:15491 [D loss: 0.671463, acc: 60.16%] [G loss: 2.123181]\n",
      "epoch:16 step:15492 [D loss: 0.667252, acc: 61.72%] [G loss: 1.774243]\n",
      "epoch:16 step:15493 [D loss: 0.729199, acc: 51.56%] [G loss: 1.676018]\n",
      "epoch:16 step:15494 [D loss: 0.688080, acc: 57.81%] [G loss: 1.800058]\n",
      "epoch:16 step:15495 [D loss: 0.678146, acc: 57.81%] [G loss: 1.932426]\n",
      "epoch:16 step:15496 [D loss: 0.595864, acc: 64.06%] [G loss: 2.201541]\n",
      "epoch:16 step:15497 [D loss: 0.703813, acc: 55.47%] [G loss: 1.869604]\n",
      "epoch:16 step:15498 [D loss: 0.634514, acc: 65.62%] [G loss: 1.900452]\n",
      "epoch:16 step:15499 [D loss: 0.689431, acc: 54.69%] [G loss: 1.897685]\n",
      "epoch:16 step:15500 [D loss: 0.606542, acc: 67.19%] [G loss: 2.078195]\n",
      "epoch:16 step:15501 [D loss: 0.619850, acc: 64.84%] [G loss: 1.936284]\n",
      "epoch:16 step:15502 [D loss: 0.685658, acc: 57.81%] [G loss: 1.817289]\n",
      "epoch:16 step:15503 [D loss: 0.663668, acc: 59.38%] [G loss: 1.778546]\n",
      "epoch:16 step:15504 [D loss: 0.655620, acc: 59.38%] [G loss: 1.863641]\n",
      "epoch:16 step:15505 [D loss: 0.612026, acc: 67.19%] [G loss: 1.799070]\n",
      "epoch:16 step:15506 [D loss: 0.634709, acc: 60.94%] [G loss: 1.883457]\n",
      "epoch:16 step:15507 [D loss: 0.606782, acc: 69.53%] [G loss: 1.997541]\n",
      "epoch:16 step:15508 [D loss: 0.564140, acc: 72.66%] [G loss: 1.878000]\n",
      "epoch:16 step:15509 [D loss: 0.659207, acc: 57.81%] [G loss: 1.896361]\n",
      "epoch:16 step:15510 [D loss: 0.624334, acc: 67.97%] [G loss: 1.856595]\n",
      "epoch:16 step:15511 [D loss: 0.653118, acc: 60.94%] [G loss: 2.000154]\n",
      "epoch:16 step:15512 [D loss: 0.616041, acc: 67.19%] [G loss: 1.946392]\n",
      "epoch:16 step:15513 [D loss: 0.619276, acc: 65.62%] [G loss: 2.100249]\n",
      "epoch:16 step:15514 [D loss: 0.607342, acc: 67.97%] [G loss: 2.140710]\n",
      "epoch:16 step:15515 [D loss: 0.683128, acc: 62.50%] [G loss: 1.941329]\n",
      "epoch:16 step:15516 [D loss: 0.643699, acc: 60.94%] [G loss: 1.915353]\n",
      "epoch:16 step:15517 [D loss: 0.685202, acc: 57.03%] [G loss: 1.758418]\n",
      "epoch:16 step:15518 [D loss: 0.617118, acc: 71.09%] [G loss: 1.946980]\n",
      "epoch:16 step:15519 [D loss: 0.655601, acc: 59.38%] [G loss: 1.906703]\n",
      "epoch:16 step:15520 [D loss: 0.690492, acc: 53.12%] [G loss: 1.782762]\n",
      "epoch:16 step:15521 [D loss: 0.640448, acc: 63.28%] [G loss: 1.740083]\n",
      "epoch:16 step:15522 [D loss: 0.696942, acc: 57.03%] [G loss: 1.679884]\n",
      "epoch:16 step:15523 [D loss: 0.721922, acc: 58.59%] [G loss: 1.841300]\n",
      "epoch:16 step:15524 [D loss: 0.642611, acc: 64.06%] [G loss: 1.914394]\n",
      "epoch:16 step:15525 [D loss: 0.669418, acc: 60.94%] [G loss: 1.818796]\n",
      "epoch:16 step:15526 [D loss: 0.623978, acc: 71.09%] [G loss: 1.935213]\n",
      "epoch:16 step:15527 [D loss: 0.681996, acc: 54.69%] [G loss: 1.861899]\n",
      "epoch:16 step:15528 [D loss: 0.644214, acc: 60.94%] [G loss: 1.845944]\n",
      "epoch:16 step:15529 [D loss: 0.638559, acc: 60.16%] [G loss: 1.809590]\n",
      "epoch:16 step:15530 [D loss: 0.670012, acc: 60.16%] [G loss: 1.735005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15531 [D loss: 0.626930, acc: 64.84%] [G loss: 1.887340]\n",
      "epoch:16 step:15532 [D loss: 0.704104, acc: 53.91%] [G loss: 1.653352]\n",
      "epoch:16 step:15533 [D loss: 0.633771, acc: 62.50%] [G loss: 1.844083]\n",
      "epoch:16 step:15534 [D loss: 0.642035, acc: 63.28%] [G loss: 1.923956]\n",
      "epoch:16 step:15535 [D loss: 0.633258, acc: 62.50%] [G loss: 1.973170]\n",
      "epoch:16 step:15536 [D loss: 0.651662, acc: 65.62%] [G loss: 1.877655]\n",
      "epoch:16 step:15537 [D loss: 0.658613, acc: 61.72%] [G loss: 1.935779]\n",
      "epoch:16 step:15538 [D loss: 0.629175, acc: 60.94%] [G loss: 1.976366]\n",
      "epoch:16 step:15539 [D loss: 0.639137, acc: 68.75%] [G loss: 2.006219]\n",
      "epoch:16 step:15540 [D loss: 0.587828, acc: 72.66%] [G loss: 1.964310]\n",
      "epoch:16 step:15541 [D loss: 0.669914, acc: 64.84%] [G loss: 2.009675]\n",
      "epoch:16 step:15542 [D loss: 0.617355, acc: 65.62%] [G loss: 1.926117]\n",
      "epoch:16 step:15543 [D loss: 0.620207, acc: 67.19%] [G loss: 1.873874]\n",
      "epoch:16 step:15544 [D loss: 0.671051, acc: 57.03%] [G loss: 1.871670]\n",
      "epoch:16 step:15545 [D loss: 0.695107, acc: 57.03%] [G loss: 1.932964]\n",
      "epoch:16 step:15546 [D loss: 0.566867, acc: 71.09%] [G loss: 2.111123]\n",
      "epoch:16 step:15547 [D loss: 0.656338, acc: 62.50%] [G loss: 1.998495]\n",
      "epoch:16 step:15548 [D loss: 0.572189, acc: 75.00%] [G loss: 2.178774]\n",
      "epoch:16 step:15549 [D loss: 0.574066, acc: 72.66%] [G loss: 2.105469]\n",
      "epoch:16 step:15550 [D loss: 0.587837, acc: 69.53%] [G loss: 2.136620]\n",
      "epoch:16 step:15551 [D loss: 0.677729, acc: 60.16%] [G loss: 1.893480]\n",
      "epoch:16 step:15552 [D loss: 0.643294, acc: 60.94%] [G loss: 1.889498]\n",
      "epoch:16 step:15553 [D loss: 0.654225, acc: 63.28%] [G loss: 1.849064]\n",
      "epoch:16 step:15554 [D loss: 0.671296, acc: 63.28%] [G loss: 1.911993]\n",
      "epoch:16 step:15555 [D loss: 0.600013, acc: 66.41%] [G loss: 1.904642]\n",
      "epoch:16 step:15556 [D loss: 0.667610, acc: 62.50%] [G loss: 1.842955]\n",
      "epoch:16 step:15557 [D loss: 0.662918, acc: 56.25%] [G loss: 1.960270]\n",
      "epoch:16 step:15558 [D loss: 0.653124, acc: 60.94%] [G loss: 1.779012]\n",
      "epoch:16 step:15559 [D loss: 0.658100, acc: 61.72%] [G loss: 1.771181]\n",
      "epoch:16 step:15560 [D loss: 0.653727, acc: 62.50%] [G loss: 1.886193]\n",
      "epoch:16 step:15561 [D loss: 0.614813, acc: 71.09%] [G loss: 1.920288]\n",
      "epoch:16 step:15562 [D loss: 0.662391, acc: 60.94%] [G loss: 1.882272]\n",
      "epoch:16 step:15563 [D loss: 0.667021, acc: 58.59%] [G loss: 1.898853]\n",
      "epoch:16 step:15564 [D loss: 0.649692, acc: 58.59%] [G loss: 1.826242]\n",
      "epoch:16 step:15565 [D loss: 0.682106, acc: 55.47%] [G loss: 1.823475]\n",
      "epoch:16 step:15566 [D loss: 0.654803, acc: 60.16%] [G loss: 1.878610]\n",
      "epoch:16 step:15567 [D loss: 0.685310, acc: 60.16%] [G loss: 1.820320]\n",
      "epoch:16 step:15568 [D loss: 0.652437, acc: 60.16%] [G loss: 1.751809]\n",
      "epoch:16 step:15569 [D loss: 0.629249, acc: 67.19%] [G loss: 1.721868]\n",
      "epoch:16 step:15570 [D loss: 0.623335, acc: 67.19%] [G loss: 1.861032]\n",
      "epoch:16 step:15571 [D loss: 0.648205, acc: 64.84%] [G loss: 1.887862]\n",
      "epoch:16 step:15572 [D loss: 0.659049, acc: 58.59%] [G loss: 1.849112]\n",
      "epoch:16 step:15573 [D loss: 0.645667, acc: 60.94%] [G loss: 1.886330]\n",
      "epoch:16 step:15574 [D loss: 0.616907, acc: 64.84%] [G loss: 1.900466]\n",
      "epoch:16 step:15575 [D loss: 0.677696, acc: 57.81%] [G loss: 1.896118]\n",
      "epoch:16 step:15576 [D loss: 0.635331, acc: 59.38%] [G loss: 1.803277]\n",
      "epoch:16 step:15577 [D loss: 0.646215, acc: 60.16%] [G loss: 1.894471]\n",
      "epoch:16 step:15578 [D loss: 0.651705, acc: 61.72%] [G loss: 1.796501]\n",
      "epoch:16 step:15579 [D loss: 0.610628, acc: 67.19%] [G loss: 1.930169]\n",
      "epoch:16 step:15580 [D loss: 0.664370, acc: 56.25%] [G loss: 2.012653]\n",
      "epoch:16 step:15581 [D loss: 0.623885, acc: 68.75%] [G loss: 1.930789]\n",
      "epoch:16 step:15582 [D loss: 0.642854, acc: 61.72%] [G loss: 1.910157]\n",
      "epoch:16 step:15583 [D loss: 0.595936, acc: 71.09%] [G loss: 1.847582]\n",
      "epoch:16 step:15584 [D loss: 0.669121, acc: 56.25%] [G loss: 1.902331]\n",
      "epoch:16 step:15585 [D loss: 0.623465, acc: 66.41%] [G loss: 1.976378]\n",
      "epoch:16 step:15586 [D loss: 0.646081, acc: 65.62%] [G loss: 1.880203]\n",
      "epoch:16 step:15587 [D loss: 0.606060, acc: 71.09%] [G loss: 1.985873]\n",
      "epoch:16 step:15588 [D loss: 0.647531, acc: 62.50%] [G loss: 1.710097]\n",
      "epoch:16 step:15589 [D loss: 0.696019, acc: 59.38%] [G loss: 1.809990]\n",
      "epoch:16 step:15590 [D loss: 0.651097, acc: 61.72%] [G loss: 1.875722]\n",
      "epoch:16 step:15591 [D loss: 0.601244, acc: 65.62%] [G loss: 1.876333]\n",
      "epoch:16 step:15592 [D loss: 0.655749, acc: 64.06%] [G loss: 1.896776]\n",
      "epoch:16 step:15593 [D loss: 0.621252, acc: 63.28%] [G loss: 1.833033]\n",
      "epoch:16 step:15594 [D loss: 0.606978, acc: 68.75%] [G loss: 2.066406]\n",
      "epoch:16 step:15595 [D loss: 0.666079, acc: 57.81%] [G loss: 2.048116]\n",
      "epoch:16 step:15596 [D loss: 0.650334, acc: 66.41%] [G loss: 2.048801]\n",
      "epoch:16 step:15597 [D loss: 0.627674, acc: 66.41%] [G loss: 1.924636]\n",
      "epoch:16 step:15598 [D loss: 0.645473, acc: 57.81%] [G loss: 1.922303]\n",
      "epoch:16 step:15599 [D loss: 0.613667, acc: 66.41%] [G loss: 1.871566]\n",
      "epoch:16 step:15600 [D loss: 0.651354, acc: 57.81%] [G loss: 2.005155]\n",
      "##############\n",
      "[2.51549988 1.67803385 6.3588973  4.86584418 3.70410345 5.81021257\n",
      " 4.36210797 4.76756477 4.5701403  3.66603795]\n",
      "##########\n",
      "epoch:16 step:15601 [D loss: 0.600348, acc: 67.19%] [G loss: 2.017285]\n",
      "epoch:16 step:15602 [D loss: 0.631270, acc: 64.06%] [G loss: 1.995649]\n",
      "epoch:16 step:15603 [D loss: 0.671026, acc: 57.81%] [G loss: 1.881523]\n",
      "epoch:16 step:15604 [D loss: 0.685618, acc: 56.25%] [G loss: 1.905397]\n",
      "epoch:16 step:15605 [D loss: 0.662814, acc: 57.03%] [G loss: 1.938074]\n",
      "epoch:16 step:15606 [D loss: 0.699449, acc: 53.12%] [G loss: 1.789162]\n",
      "epoch:16 step:15607 [D loss: 0.713294, acc: 54.69%] [G loss: 1.873797]\n",
      "epoch:16 step:15608 [D loss: 0.612904, acc: 65.62%] [G loss: 1.845452]\n",
      "epoch:16 step:15609 [D loss: 0.677065, acc: 63.28%] [G loss: 1.884274]\n",
      "epoch:16 step:15610 [D loss: 0.644400, acc: 67.97%] [G loss: 1.821672]\n",
      "epoch:16 step:15611 [D loss: 0.641817, acc: 63.28%] [G loss: 1.822328]\n",
      "epoch:16 step:15612 [D loss: 0.696223, acc: 57.81%] [G loss: 1.973211]\n",
      "epoch:16 step:15613 [D loss: 0.656789, acc: 65.62%] [G loss: 1.837576]\n",
      "epoch:16 step:15614 [D loss: 0.650333, acc: 62.50%] [G loss: 1.852764]\n",
      "epoch:16 step:15615 [D loss: 0.646515, acc: 63.28%] [G loss: 1.965729]\n",
      "epoch:16 step:15616 [D loss: 0.608563, acc: 65.62%] [G loss: 1.926330]\n",
      "epoch:16 step:15617 [D loss: 0.657572, acc: 64.06%] [G loss: 1.819368]\n",
      "epoch:16 step:15618 [D loss: 0.672710, acc: 57.81%] [G loss: 1.876435]\n",
      "epoch:16 step:15619 [D loss: 0.678264, acc: 62.50%] [G loss: 1.923055]\n",
      "epoch:16 step:15620 [D loss: 0.641161, acc: 57.81%] [G loss: 1.709868]\n",
      "epoch:16 step:15621 [D loss: 0.669097, acc: 57.81%] [G loss: 1.892623]\n",
      "epoch:16 step:15622 [D loss: 0.624749, acc: 67.19%] [G loss: 1.957294]\n",
      "epoch:16 step:15623 [D loss: 0.653200, acc: 67.19%] [G loss: 1.990310]\n",
      "epoch:16 step:15624 [D loss: 0.606675, acc: 70.31%] [G loss: 1.968182]\n",
      "epoch:16 step:15625 [D loss: 0.670185, acc: 57.81%] [G loss: 1.988106]\n",
      "epoch:16 step:15626 [D loss: 0.602461, acc: 66.41%] [G loss: 1.999279]\n",
      "epoch:16 step:15627 [D loss: 0.629219, acc: 64.06%] [G loss: 1.973589]\n",
      "epoch:16 step:15628 [D loss: 0.621095, acc: 62.50%] [G loss: 1.961000]\n",
      "epoch:16 step:15629 [D loss: 0.641740, acc: 61.72%] [G loss: 2.024633]\n",
      "epoch:16 step:15630 [D loss: 0.635052, acc: 62.50%] [G loss: 2.009324]\n",
      "epoch:16 step:15631 [D loss: 0.652832, acc: 61.72%] [G loss: 1.867462]\n",
      "epoch:16 step:15632 [D loss: 0.613889, acc: 63.28%] [G loss: 1.900759]\n",
      "epoch:16 step:15633 [D loss: 0.631054, acc: 60.16%] [G loss: 1.924273]\n",
      "epoch:16 step:15634 [D loss: 0.648552, acc: 59.38%] [G loss: 2.002824]\n",
      "epoch:16 step:15635 [D loss: 0.671659, acc: 57.03%] [G loss: 1.891290]\n",
      "epoch:16 step:15636 [D loss: 0.592049, acc: 65.62%] [G loss: 1.850210]\n",
      "epoch:16 step:15637 [D loss: 0.673424, acc: 61.72%] [G loss: 1.972513]\n",
      "epoch:16 step:15638 [D loss: 0.587972, acc: 71.88%] [G loss: 2.004629]\n",
      "epoch:16 step:15639 [D loss: 0.597299, acc: 66.41%] [G loss: 2.099032]\n",
      "epoch:16 step:15640 [D loss: 0.589763, acc: 70.31%] [G loss: 2.205883]\n",
      "epoch:16 step:15641 [D loss: 0.685382, acc: 58.59%] [G loss: 2.195087]\n",
      "epoch:16 step:15642 [D loss: 0.645200, acc: 61.72%] [G loss: 1.993022]\n",
      "epoch:16 step:15643 [D loss: 0.596707, acc: 67.19%] [G loss: 2.066728]\n",
      "epoch:16 step:15644 [D loss: 0.638805, acc: 64.06%] [G loss: 1.988273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15645 [D loss: 0.641028, acc: 64.06%] [G loss: 2.018588]\n",
      "epoch:16 step:15646 [D loss: 0.674309, acc: 60.16%] [G loss: 1.864248]\n",
      "epoch:16 step:15647 [D loss: 0.644954, acc: 61.72%] [G loss: 1.758027]\n",
      "epoch:16 step:15648 [D loss: 0.661152, acc: 60.16%] [G loss: 2.017870]\n",
      "epoch:16 step:15649 [D loss: 0.748625, acc: 46.88%] [G loss: 1.781530]\n",
      "epoch:16 step:15650 [D loss: 0.659055, acc: 58.59%] [G loss: 1.732293]\n",
      "epoch:16 step:15651 [D loss: 0.634064, acc: 62.50%] [G loss: 1.793094]\n",
      "epoch:16 step:15652 [D loss: 0.646345, acc: 60.94%] [G loss: 1.897042]\n",
      "epoch:16 step:15653 [D loss: 0.666656, acc: 58.59%] [G loss: 1.952663]\n",
      "epoch:16 step:15654 [D loss: 0.620363, acc: 69.53%] [G loss: 1.853462]\n",
      "epoch:16 step:15655 [D loss: 0.608981, acc: 66.41%] [G loss: 1.882838]\n",
      "epoch:16 step:15656 [D loss: 0.700800, acc: 53.12%] [G loss: 1.825416]\n",
      "epoch:16 step:15657 [D loss: 0.658393, acc: 58.59%] [G loss: 1.926416]\n",
      "epoch:16 step:15658 [D loss: 0.694970, acc: 52.34%] [G loss: 1.888573]\n",
      "epoch:16 step:15659 [D loss: 0.624363, acc: 65.62%] [G loss: 1.798913]\n",
      "epoch:16 step:15660 [D loss: 0.651928, acc: 60.16%] [G loss: 1.826347]\n",
      "epoch:16 step:15661 [D loss: 0.647499, acc: 61.72%] [G loss: 1.777379]\n",
      "epoch:16 step:15662 [D loss: 0.637214, acc: 61.72%] [G loss: 1.788090]\n",
      "epoch:16 step:15663 [D loss: 0.629553, acc: 67.19%] [G loss: 1.820567]\n",
      "epoch:16 step:15664 [D loss: 0.650400, acc: 58.59%] [G loss: 1.994614]\n",
      "epoch:16 step:15665 [D loss: 0.635645, acc: 64.84%] [G loss: 1.961684]\n",
      "epoch:16 step:15666 [D loss: 0.682235, acc: 57.81%] [G loss: 1.959966]\n",
      "epoch:16 step:15667 [D loss: 0.654690, acc: 60.94%] [G loss: 1.856045]\n",
      "epoch:16 step:15668 [D loss: 0.641738, acc: 64.06%] [G loss: 1.781232]\n",
      "epoch:16 step:15669 [D loss: 0.699849, acc: 58.59%] [G loss: 1.922179]\n",
      "epoch:16 step:15670 [D loss: 0.627146, acc: 62.50%] [G loss: 1.925681]\n",
      "epoch:16 step:15671 [D loss: 0.609370, acc: 68.75%] [G loss: 2.033540]\n",
      "epoch:16 step:15672 [D loss: 0.618309, acc: 65.62%] [G loss: 1.991731]\n",
      "epoch:16 step:15673 [D loss: 0.622115, acc: 65.62%] [G loss: 2.089534]\n",
      "epoch:16 step:15674 [D loss: 0.630555, acc: 61.72%] [G loss: 1.944902]\n",
      "epoch:16 step:15675 [D loss: 0.666427, acc: 57.03%] [G loss: 1.855168]\n",
      "epoch:16 step:15676 [D loss: 0.642615, acc: 65.62%] [G loss: 1.883765]\n",
      "epoch:16 step:15677 [D loss: 0.587029, acc: 71.88%] [G loss: 1.925807]\n",
      "epoch:16 step:15678 [D loss: 0.618958, acc: 68.75%] [G loss: 1.868698]\n",
      "epoch:16 step:15679 [D loss: 0.639950, acc: 60.94%] [G loss: 1.997856]\n",
      "epoch:16 step:15680 [D loss: 0.709243, acc: 53.12%] [G loss: 1.943686]\n",
      "epoch:16 step:15681 [D loss: 0.654240, acc: 65.62%] [G loss: 1.939198]\n",
      "epoch:16 step:15682 [D loss: 0.580810, acc: 66.41%] [G loss: 1.979739]\n",
      "epoch:16 step:15683 [D loss: 0.614545, acc: 59.38%] [G loss: 2.106291]\n",
      "epoch:16 step:15684 [D loss: 0.625272, acc: 64.06%] [G loss: 2.076136]\n",
      "epoch:16 step:15685 [D loss: 0.591603, acc: 69.53%] [G loss: 1.948787]\n",
      "epoch:16 step:15686 [D loss: 0.573169, acc: 71.09%] [G loss: 2.295413]\n",
      "epoch:16 step:15687 [D loss: 0.595302, acc: 64.06%] [G loss: 2.034275]\n",
      "epoch:16 step:15688 [D loss: 0.642480, acc: 60.94%] [G loss: 1.854966]\n",
      "epoch:16 step:15689 [D loss: 0.623213, acc: 67.97%] [G loss: 1.985534]\n",
      "epoch:16 step:15690 [D loss: 0.686379, acc: 57.03%] [G loss: 1.954260]\n",
      "epoch:16 step:15691 [D loss: 0.621946, acc: 69.53%] [G loss: 2.044013]\n",
      "epoch:16 step:15692 [D loss: 0.674089, acc: 60.94%] [G loss: 1.882254]\n",
      "epoch:16 step:15693 [D loss: 0.673915, acc: 59.38%] [G loss: 1.850809]\n",
      "epoch:16 step:15694 [D loss: 0.639653, acc: 64.06%] [G loss: 1.846027]\n",
      "epoch:16 step:15695 [D loss: 0.666202, acc: 58.59%] [G loss: 1.760783]\n",
      "epoch:16 step:15696 [D loss: 0.675193, acc: 56.25%] [G loss: 1.811605]\n",
      "epoch:16 step:15697 [D loss: 0.658951, acc: 57.81%] [G loss: 1.929111]\n",
      "epoch:16 step:15698 [D loss: 0.647465, acc: 60.94%] [G loss: 1.961188]\n",
      "epoch:16 step:15699 [D loss: 0.633806, acc: 61.72%] [G loss: 2.072166]\n",
      "epoch:16 step:15700 [D loss: 0.592290, acc: 67.97%] [G loss: 2.201932]\n",
      "epoch:16 step:15701 [D loss: 0.599520, acc: 68.75%] [G loss: 2.046832]\n",
      "epoch:16 step:15702 [D loss: 0.659329, acc: 57.03%] [G loss: 1.934379]\n",
      "epoch:16 step:15703 [D loss: 0.643119, acc: 60.16%] [G loss: 1.904550]\n",
      "epoch:16 step:15704 [D loss: 0.617857, acc: 67.19%] [G loss: 1.926867]\n",
      "epoch:16 step:15705 [D loss: 0.653106, acc: 59.38%] [G loss: 1.859710]\n",
      "epoch:16 step:15706 [D loss: 0.618907, acc: 69.53%] [G loss: 2.003897]\n",
      "epoch:16 step:15707 [D loss: 0.616581, acc: 67.19%] [G loss: 1.797136]\n",
      "epoch:16 step:15708 [D loss: 0.684118, acc: 53.91%] [G loss: 1.965862]\n",
      "epoch:16 step:15709 [D loss: 0.676763, acc: 53.91%] [G loss: 1.790455]\n",
      "epoch:16 step:15710 [D loss: 0.671476, acc: 53.12%] [G loss: 1.878303]\n",
      "epoch:16 step:15711 [D loss: 0.627960, acc: 60.16%] [G loss: 2.006804]\n",
      "epoch:16 step:15712 [D loss: 0.703552, acc: 51.56%] [G loss: 2.040182]\n",
      "epoch:16 step:15713 [D loss: 0.607726, acc: 69.53%] [G loss: 1.890373]\n",
      "epoch:16 step:15714 [D loss: 0.641237, acc: 64.84%] [G loss: 1.804859]\n",
      "epoch:16 step:15715 [D loss: 0.652012, acc: 60.94%] [G loss: 1.828415]\n",
      "epoch:16 step:15716 [D loss: 0.617679, acc: 63.28%] [G loss: 2.073942]\n",
      "epoch:16 step:15717 [D loss: 0.629190, acc: 66.41%] [G loss: 1.993545]\n",
      "epoch:16 step:15718 [D loss: 0.669953, acc: 57.03%] [G loss: 2.020438]\n",
      "epoch:16 step:15719 [D loss: 0.653950, acc: 59.38%] [G loss: 1.991544]\n",
      "epoch:16 step:15720 [D loss: 0.637818, acc: 58.59%] [G loss: 2.029883]\n",
      "epoch:16 step:15721 [D loss: 0.673018, acc: 52.34%] [G loss: 1.844993]\n",
      "epoch:16 step:15722 [D loss: 0.648022, acc: 64.06%] [G loss: 1.884861]\n",
      "epoch:16 step:15723 [D loss: 0.622149, acc: 64.84%] [G loss: 1.864266]\n",
      "epoch:16 step:15724 [D loss: 0.611824, acc: 70.31%] [G loss: 1.948689]\n",
      "epoch:16 step:15725 [D loss: 0.626354, acc: 63.28%] [G loss: 1.971572]\n",
      "epoch:16 step:15726 [D loss: 0.663106, acc: 60.16%] [G loss: 1.896330]\n",
      "epoch:16 step:15727 [D loss: 0.674196, acc: 60.94%] [G loss: 1.939898]\n",
      "epoch:16 step:15728 [D loss: 0.608146, acc: 70.31%] [G loss: 1.971279]\n",
      "epoch:16 step:15729 [D loss: 0.598782, acc: 74.22%] [G loss: 1.765098]\n",
      "epoch:16 step:15730 [D loss: 0.688008, acc: 56.25%] [G loss: 1.806368]\n",
      "epoch:16 step:15731 [D loss: 0.700518, acc: 53.12%] [G loss: 1.900695]\n",
      "epoch:16 step:15732 [D loss: 0.658773, acc: 61.72%] [G loss: 1.915673]\n",
      "epoch:16 step:15733 [D loss: 0.740895, acc: 53.12%] [G loss: 1.751909]\n",
      "epoch:16 step:15734 [D loss: 0.637668, acc: 63.28%] [G loss: 1.847811]\n",
      "epoch:16 step:15735 [D loss: 0.700687, acc: 55.47%] [G loss: 1.858985]\n",
      "epoch:16 step:15736 [D loss: 0.680756, acc: 60.94%] [G loss: 1.837581]\n",
      "epoch:16 step:15737 [D loss: 0.646393, acc: 63.28%] [G loss: 1.855322]\n",
      "epoch:16 step:15738 [D loss: 0.659962, acc: 60.94%] [G loss: 2.012996]\n",
      "epoch:16 step:15739 [D loss: 0.598995, acc: 67.97%] [G loss: 1.954295]\n",
      "epoch:16 step:15740 [D loss: 0.627722, acc: 65.62%] [G loss: 1.788124]\n",
      "epoch:16 step:15741 [D loss: 0.656061, acc: 60.16%] [G loss: 1.691229]\n",
      "epoch:16 step:15742 [D loss: 0.686299, acc: 58.59%] [G loss: 1.907866]\n",
      "epoch:16 step:15743 [D loss: 0.666262, acc: 59.38%] [G loss: 1.815828]\n",
      "epoch:16 step:15744 [D loss: 0.672291, acc: 59.38%] [G loss: 1.832083]\n",
      "epoch:16 step:15745 [D loss: 0.621534, acc: 65.62%] [G loss: 1.996696]\n",
      "epoch:16 step:15746 [D loss: 0.651638, acc: 62.50%] [G loss: 1.870302]\n",
      "epoch:16 step:15747 [D loss: 0.622487, acc: 64.84%] [G loss: 1.973730]\n",
      "epoch:16 step:15748 [D loss: 0.599615, acc: 68.75%] [G loss: 1.842018]\n",
      "epoch:16 step:15749 [D loss: 0.613501, acc: 64.06%] [G loss: 1.952415]\n",
      "epoch:16 step:15750 [D loss: 0.671704, acc: 53.91%] [G loss: 1.717840]\n",
      "epoch:16 step:15751 [D loss: 0.674666, acc: 60.16%] [G loss: 1.781317]\n",
      "epoch:16 step:15752 [D loss: 0.635126, acc: 61.72%] [G loss: 1.830998]\n",
      "epoch:16 step:15753 [D loss: 0.667175, acc: 56.25%] [G loss: 1.742746]\n",
      "epoch:16 step:15754 [D loss: 0.656979, acc: 63.28%] [G loss: 1.858676]\n",
      "epoch:16 step:15755 [D loss: 0.618336, acc: 70.31%] [G loss: 1.958797]\n",
      "epoch:16 step:15756 [D loss: 0.611090, acc: 71.09%] [G loss: 1.788431]\n",
      "epoch:16 step:15757 [D loss: 0.653022, acc: 57.81%] [G loss: 1.769298]\n",
      "epoch:16 step:15758 [D loss: 0.725766, acc: 53.91%] [G loss: 1.729699]\n",
      "epoch:16 step:15759 [D loss: 0.688869, acc: 53.12%] [G loss: 1.760572]\n",
      "epoch:16 step:15760 [D loss: 0.661536, acc: 64.06%] [G loss: 1.784522]\n",
      "epoch:16 step:15761 [D loss: 0.637906, acc: 60.94%] [G loss: 1.910339]\n",
      "epoch:16 step:15762 [D loss: 0.649204, acc: 60.94%] [G loss: 1.908149]\n",
      "epoch:16 step:15763 [D loss: 0.661427, acc: 58.59%] [G loss: 1.813919]\n",
      "epoch:16 step:15764 [D loss: 0.680230, acc: 60.94%] [G loss: 1.976531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15765 [D loss: 0.615387, acc: 71.88%] [G loss: 1.923208]\n",
      "epoch:16 step:15766 [D loss: 0.672424, acc: 66.41%] [G loss: 2.072792]\n",
      "epoch:16 step:15767 [D loss: 0.619022, acc: 62.50%] [G loss: 2.019991]\n",
      "epoch:16 step:15768 [D loss: 0.688290, acc: 53.91%] [G loss: 1.929461]\n",
      "epoch:16 step:15769 [D loss: 0.711823, acc: 53.91%] [G loss: 1.790743]\n",
      "epoch:16 step:15770 [D loss: 0.673990, acc: 56.25%] [G loss: 1.936445]\n",
      "epoch:16 step:15771 [D loss: 0.728159, acc: 53.91%] [G loss: 1.847695]\n",
      "epoch:16 step:15772 [D loss: 0.654487, acc: 63.28%] [G loss: 1.849306]\n",
      "epoch:16 step:15773 [D loss: 0.606320, acc: 68.75%] [G loss: 2.002623]\n",
      "epoch:16 step:15774 [D loss: 0.607811, acc: 64.84%] [G loss: 2.069554]\n",
      "epoch:16 step:15775 [D loss: 0.692916, acc: 57.03%] [G loss: 1.874395]\n",
      "epoch:16 step:15776 [D loss: 0.725156, acc: 53.91%] [G loss: 1.757649]\n",
      "epoch:16 step:15777 [D loss: 0.660020, acc: 64.84%] [G loss: 1.820378]\n",
      "epoch:16 step:15778 [D loss: 0.610615, acc: 65.62%] [G loss: 1.912239]\n",
      "epoch:16 step:15779 [D loss: 0.638182, acc: 59.38%] [G loss: 1.898358]\n",
      "epoch:16 step:15780 [D loss: 0.697257, acc: 55.47%] [G loss: 1.718406]\n",
      "epoch:16 step:15781 [D loss: 0.621849, acc: 63.28%] [G loss: 1.841115]\n",
      "epoch:16 step:15782 [D loss: 0.640302, acc: 64.84%] [G loss: 1.915012]\n",
      "epoch:16 step:15783 [D loss: 0.639139, acc: 63.28%] [G loss: 1.911562]\n",
      "epoch:16 step:15784 [D loss: 0.637514, acc: 66.41%] [G loss: 2.003780]\n",
      "epoch:16 step:15785 [D loss: 0.634890, acc: 67.19%] [G loss: 1.847951]\n",
      "epoch:16 step:15786 [D loss: 0.672140, acc: 58.59%] [G loss: 1.763171]\n",
      "epoch:16 step:15787 [D loss: 0.653994, acc: 60.94%] [G loss: 1.830850]\n",
      "epoch:16 step:15788 [D loss: 0.668103, acc: 54.69%] [G loss: 1.848079]\n",
      "epoch:16 step:15789 [D loss: 0.687855, acc: 57.03%] [G loss: 1.839863]\n",
      "epoch:16 step:15790 [D loss: 0.621872, acc: 62.50%] [G loss: 1.842659]\n",
      "epoch:16 step:15791 [D loss: 0.687132, acc: 59.38%] [G loss: 1.759069]\n",
      "epoch:16 step:15792 [D loss: 0.697524, acc: 60.94%] [G loss: 1.703426]\n",
      "epoch:16 step:15793 [D loss: 0.685013, acc: 57.03%] [G loss: 1.674862]\n",
      "epoch:16 step:15794 [D loss: 0.650017, acc: 62.50%] [G loss: 1.873854]\n",
      "epoch:16 step:15795 [D loss: 0.619404, acc: 73.44%] [G loss: 1.790408]\n",
      "epoch:16 step:15796 [D loss: 0.615225, acc: 66.41%] [G loss: 1.883963]\n",
      "epoch:16 step:15797 [D loss: 0.654836, acc: 60.94%] [G loss: 1.982966]\n",
      "epoch:16 step:15798 [D loss: 0.608821, acc: 67.19%] [G loss: 1.861582]\n",
      "epoch:16 step:15799 [D loss: 0.616665, acc: 58.59%] [G loss: 2.036381]\n",
      "epoch:16 step:15800 [D loss: 0.577573, acc: 72.66%] [G loss: 1.889773]\n",
      "##############\n",
      "[2.5197401  1.58574702 6.44113109 4.78097004 3.82409675 5.54072683\n",
      " 4.49314128 4.65931391 4.81665127 3.81728845]\n",
      "##########\n",
      "epoch:16 step:15801 [D loss: 0.653822, acc: 61.72%] [G loss: 1.878038]\n",
      "epoch:16 step:15802 [D loss: 0.598675, acc: 67.97%] [G loss: 1.950360]\n",
      "epoch:16 step:15803 [D loss: 0.631868, acc: 65.62%] [G loss: 1.940278]\n",
      "epoch:16 step:15804 [D loss: 0.649847, acc: 62.50%] [G loss: 1.759884]\n",
      "epoch:16 step:15805 [D loss: 0.635571, acc: 64.84%] [G loss: 1.849089]\n",
      "epoch:16 step:15806 [D loss: 0.592501, acc: 66.41%] [G loss: 2.129958]\n",
      "epoch:16 step:15807 [D loss: 0.631135, acc: 64.84%] [G loss: 2.160137]\n",
      "epoch:16 step:15808 [D loss: 0.643446, acc: 64.84%] [G loss: 1.916892]\n",
      "epoch:16 step:15809 [D loss: 0.666778, acc: 57.81%] [G loss: 1.774798]\n",
      "epoch:16 step:15810 [D loss: 0.693567, acc: 59.38%] [G loss: 1.714537]\n",
      "epoch:16 step:15811 [D loss: 0.657199, acc: 66.41%] [G loss: 1.964280]\n",
      "epoch:16 step:15812 [D loss: 0.708914, acc: 59.38%] [G loss: 1.766159]\n",
      "epoch:16 step:15813 [D loss: 0.656829, acc: 59.38%] [G loss: 1.935939]\n",
      "epoch:16 step:15814 [D loss: 0.615889, acc: 61.72%] [G loss: 1.870935]\n",
      "epoch:16 step:15815 [D loss: 0.571976, acc: 74.22%] [G loss: 2.057485]\n",
      "epoch:16 step:15816 [D loss: 0.640983, acc: 66.41%] [G loss: 1.855202]\n",
      "epoch:16 step:15817 [D loss: 0.650214, acc: 65.62%] [G loss: 2.026712]\n",
      "epoch:16 step:15818 [D loss: 0.640764, acc: 63.28%] [G loss: 1.973555]\n",
      "epoch:16 step:15819 [D loss: 0.676918, acc: 61.72%] [G loss: 1.813290]\n",
      "epoch:16 step:15820 [D loss: 0.655227, acc: 60.16%] [G loss: 1.736603]\n",
      "epoch:16 step:15821 [D loss: 0.648888, acc: 60.94%] [G loss: 1.864176]\n",
      "epoch:16 step:15822 [D loss: 0.620711, acc: 62.50%] [G loss: 2.013731]\n",
      "epoch:16 step:15823 [D loss: 0.652020, acc: 62.50%] [G loss: 1.953107]\n",
      "epoch:16 step:15824 [D loss: 0.646612, acc: 62.50%] [G loss: 1.986674]\n",
      "epoch:16 step:15825 [D loss: 0.613278, acc: 67.97%] [G loss: 2.011514]\n",
      "epoch:16 step:15826 [D loss: 0.647896, acc: 65.62%] [G loss: 1.986720]\n",
      "epoch:16 step:15827 [D loss: 0.610850, acc: 68.75%] [G loss: 2.063053]\n",
      "epoch:16 step:15828 [D loss: 0.649498, acc: 58.59%] [G loss: 1.901386]\n",
      "epoch:16 step:15829 [D loss: 0.577750, acc: 71.09%] [G loss: 1.963642]\n",
      "epoch:16 step:15830 [D loss: 0.614276, acc: 66.41%] [G loss: 2.008931]\n",
      "epoch:16 step:15831 [D loss: 0.633626, acc: 60.16%] [G loss: 1.932586]\n",
      "epoch:16 step:15832 [D loss: 0.616569, acc: 62.50%] [G loss: 1.944613]\n",
      "epoch:16 step:15833 [D loss: 0.595577, acc: 71.88%] [G loss: 2.101565]\n",
      "epoch:16 step:15834 [D loss: 0.599929, acc: 69.53%] [G loss: 2.074860]\n",
      "epoch:16 step:15835 [D loss: 0.610878, acc: 65.62%] [G loss: 1.875664]\n",
      "epoch:16 step:15836 [D loss: 0.625609, acc: 64.84%] [G loss: 1.994696]\n",
      "epoch:16 step:15837 [D loss: 0.600338, acc: 67.97%] [G loss: 1.934031]\n",
      "epoch:16 step:15838 [D loss: 0.652849, acc: 59.38%] [G loss: 1.901648]\n",
      "epoch:16 step:15839 [D loss: 0.662154, acc: 60.94%] [G loss: 1.981723]\n",
      "epoch:16 step:15840 [D loss: 0.694837, acc: 58.59%] [G loss: 1.992999]\n",
      "epoch:16 step:15841 [D loss: 0.578855, acc: 71.09%] [G loss: 1.988417]\n",
      "epoch:16 step:15842 [D loss: 0.663082, acc: 67.97%] [G loss: 1.795329]\n",
      "epoch:16 step:15843 [D loss: 0.669840, acc: 55.47%] [G loss: 1.741644]\n",
      "epoch:16 step:15844 [D loss: 0.626294, acc: 64.06%] [G loss: 1.907692]\n",
      "epoch:16 step:15845 [D loss: 0.670488, acc: 60.16%] [G loss: 1.787052]\n",
      "epoch:16 step:15846 [D loss: 0.645497, acc: 68.75%] [G loss: 1.936759]\n",
      "epoch:16 step:15847 [D loss: 0.642058, acc: 62.50%] [G loss: 1.779556]\n",
      "epoch:16 step:15848 [D loss: 0.654485, acc: 64.84%] [G loss: 1.879243]\n",
      "epoch:16 step:15849 [D loss: 0.709254, acc: 55.47%] [G loss: 1.889431]\n",
      "epoch:16 step:15850 [D loss: 0.635124, acc: 62.50%] [G loss: 1.821390]\n",
      "epoch:16 step:15851 [D loss: 0.706379, acc: 55.47%] [G loss: 1.727753]\n",
      "epoch:16 step:15852 [D loss: 0.593257, acc: 70.31%] [G loss: 1.915085]\n",
      "epoch:16 step:15853 [D loss: 0.646940, acc: 60.94%] [G loss: 1.723951]\n",
      "epoch:16 step:15854 [D loss: 0.629654, acc: 66.41%] [G loss: 1.811201]\n",
      "epoch:16 step:15855 [D loss: 0.647889, acc: 63.28%] [G loss: 1.774956]\n",
      "epoch:16 step:15856 [D loss: 0.691180, acc: 57.81%] [G loss: 1.825576]\n",
      "epoch:16 step:15857 [D loss: 0.669870, acc: 59.38%] [G loss: 1.806142]\n",
      "epoch:16 step:15858 [D loss: 0.637714, acc: 69.53%] [G loss: 1.975223]\n",
      "epoch:16 step:15859 [D loss: 0.709971, acc: 52.34%] [G loss: 1.833097]\n",
      "epoch:16 step:15860 [D loss: 0.633459, acc: 64.84%] [G loss: 1.865011]\n",
      "epoch:16 step:15861 [D loss: 0.658133, acc: 60.16%] [G loss: 1.885181]\n",
      "epoch:16 step:15862 [D loss: 0.673484, acc: 60.16%] [G loss: 1.716425]\n",
      "epoch:16 step:15863 [D loss: 0.642632, acc: 67.19%] [G loss: 1.833618]\n",
      "epoch:16 step:15864 [D loss: 0.642849, acc: 67.19%] [G loss: 1.900985]\n",
      "epoch:16 step:15865 [D loss: 0.648420, acc: 64.84%] [G loss: 1.800202]\n",
      "epoch:16 step:15866 [D loss: 0.679446, acc: 59.38%] [G loss: 1.857919]\n",
      "epoch:16 step:15867 [D loss: 0.638467, acc: 64.06%] [G loss: 2.076798]\n",
      "epoch:16 step:15868 [D loss: 0.665014, acc: 57.81%] [G loss: 1.901840]\n",
      "epoch:16 step:15869 [D loss: 0.627233, acc: 64.06%] [G loss: 1.793968]\n",
      "epoch:16 step:15870 [D loss: 0.705319, acc: 55.47%] [G loss: 1.933377]\n",
      "epoch:16 step:15871 [D loss: 0.638494, acc: 63.28%] [G loss: 1.859573]\n",
      "epoch:16 step:15872 [D loss: 0.632978, acc: 64.06%] [G loss: 1.741441]\n",
      "epoch:16 step:15873 [D loss: 0.650544, acc: 58.59%] [G loss: 1.952544]\n",
      "epoch:16 step:15874 [D loss: 0.634265, acc: 60.94%] [G loss: 1.830778]\n",
      "epoch:16 step:15875 [D loss: 0.636463, acc: 64.06%] [G loss: 1.867569]\n",
      "epoch:16 step:15876 [D loss: 0.651943, acc: 60.16%] [G loss: 1.887735]\n",
      "epoch:16 step:15877 [D loss: 0.616400, acc: 67.97%] [G loss: 1.862024]\n",
      "epoch:16 step:15878 [D loss: 0.563166, acc: 74.22%] [G loss: 1.920544]\n",
      "epoch:16 step:15879 [D loss: 0.691463, acc: 60.94%] [G loss: 1.950622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15880 [D loss: 0.666461, acc: 61.72%] [G loss: 1.917985]\n",
      "epoch:16 step:15881 [D loss: 0.649847, acc: 59.38%] [G loss: 1.813482]\n",
      "epoch:16 step:15882 [D loss: 0.630595, acc: 60.16%] [G loss: 1.869606]\n",
      "epoch:16 step:15883 [D loss: 0.632824, acc: 61.72%] [G loss: 1.951004]\n",
      "epoch:16 step:15884 [D loss: 0.694604, acc: 49.22%] [G loss: 1.786401]\n",
      "epoch:16 step:15885 [D loss: 0.679021, acc: 60.94%] [G loss: 1.808624]\n",
      "epoch:16 step:15886 [D loss: 0.605715, acc: 63.28%] [G loss: 1.947939]\n",
      "epoch:16 step:15887 [D loss: 0.656654, acc: 62.50%] [G loss: 1.827770]\n",
      "epoch:16 step:15888 [D loss: 0.661698, acc: 58.59%] [G loss: 2.038491]\n",
      "epoch:16 step:15889 [D loss: 0.598328, acc: 71.09%] [G loss: 1.863782]\n",
      "epoch:16 step:15890 [D loss: 0.612323, acc: 65.62%] [G loss: 1.987520]\n",
      "epoch:16 step:15891 [D loss: 0.633913, acc: 65.62%] [G loss: 2.208825]\n",
      "epoch:16 step:15892 [D loss: 0.601610, acc: 72.66%] [G loss: 2.075056]\n",
      "epoch:16 step:15893 [D loss: 0.650049, acc: 61.72%] [G loss: 1.946519]\n",
      "epoch:16 step:15894 [D loss: 0.705098, acc: 60.16%] [G loss: 1.825650]\n",
      "epoch:16 step:15895 [D loss: 0.696798, acc: 51.56%] [G loss: 1.891351]\n",
      "epoch:16 step:15896 [D loss: 0.657893, acc: 63.28%] [G loss: 1.850996]\n",
      "epoch:16 step:15897 [D loss: 0.661613, acc: 60.16%] [G loss: 1.939278]\n",
      "epoch:16 step:15898 [D loss: 0.652362, acc: 61.72%] [G loss: 1.889974]\n",
      "epoch:16 step:15899 [D loss: 0.629048, acc: 69.53%] [G loss: 2.003633]\n",
      "epoch:16 step:15900 [D loss: 0.596560, acc: 71.88%] [G loss: 1.953767]\n",
      "epoch:16 step:15901 [D loss: 0.608031, acc: 65.62%] [G loss: 2.134313]\n",
      "epoch:16 step:15902 [D loss: 0.622373, acc: 60.94%] [G loss: 2.086927]\n",
      "epoch:16 step:15903 [D loss: 0.648221, acc: 61.72%] [G loss: 1.962277]\n",
      "epoch:16 step:15904 [D loss: 0.642810, acc: 64.84%] [G loss: 2.077893]\n",
      "epoch:16 step:15905 [D loss: 0.698610, acc: 54.69%] [G loss: 2.076314]\n",
      "epoch:16 step:15906 [D loss: 0.632544, acc: 64.06%] [G loss: 1.813347]\n",
      "epoch:16 step:15907 [D loss: 0.630801, acc: 67.97%] [G loss: 2.036538]\n",
      "epoch:16 step:15908 [D loss: 0.658225, acc: 61.72%] [G loss: 1.929297]\n",
      "epoch:16 step:15909 [D loss: 0.633757, acc: 60.94%] [G loss: 2.033947]\n",
      "epoch:16 step:15910 [D loss: 0.565211, acc: 73.44%] [G loss: 1.936687]\n",
      "epoch:16 step:15911 [D loss: 0.580676, acc: 68.75%] [G loss: 2.219342]\n",
      "epoch:16 step:15912 [D loss: 0.746415, acc: 47.66%] [G loss: 1.834115]\n",
      "epoch:16 step:15913 [D loss: 0.677104, acc: 57.81%] [G loss: 1.934548]\n",
      "epoch:16 step:15914 [D loss: 0.703226, acc: 57.81%] [G loss: 1.880914]\n",
      "epoch:16 step:15915 [D loss: 0.653272, acc: 60.94%] [G loss: 1.997928]\n",
      "epoch:16 step:15916 [D loss: 0.546886, acc: 80.47%] [G loss: 2.249470]\n",
      "epoch:16 step:15917 [D loss: 0.623644, acc: 67.19%] [G loss: 1.958404]\n",
      "epoch:16 step:15918 [D loss: 0.610193, acc: 64.84%] [G loss: 2.115693]\n",
      "epoch:16 step:15919 [D loss: 0.631138, acc: 64.06%] [G loss: 2.361962]\n",
      "epoch:16 step:15920 [D loss: 0.717602, acc: 55.47%] [G loss: 1.936010]\n",
      "epoch:16 step:15921 [D loss: 0.737074, acc: 51.56%] [G loss: 2.027589]\n",
      "epoch:16 step:15922 [D loss: 0.565869, acc: 71.09%] [G loss: 2.146516]\n",
      "epoch:16 step:15923 [D loss: 0.629302, acc: 63.28%] [G loss: 2.081388]\n",
      "epoch:16 step:15924 [D loss: 0.604998, acc: 68.75%] [G loss: 2.016057]\n",
      "epoch:16 step:15925 [D loss: 0.618893, acc: 67.97%] [G loss: 1.976636]\n",
      "epoch:16 step:15926 [D loss: 0.629217, acc: 61.72%] [G loss: 2.117492]\n",
      "epoch:16 step:15927 [D loss: 0.597260, acc: 65.62%] [G loss: 2.026477]\n",
      "epoch:16 step:15928 [D loss: 0.586805, acc: 71.88%] [G loss: 2.103894]\n",
      "epoch:16 step:15929 [D loss: 0.556771, acc: 71.88%] [G loss: 2.470202]\n",
      "epoch:17 step:15930 [D loss: 0.667598, acc: 57.03%] [G loss: 1.863001]\n",
      "epoch:17 step:15931 [D loss: 0.695228, acc: 58.59%] [G loss: 1.927861]\n",
      "epoch:17 step:15932 [D loss: 0.682338, acc: 57.03%] [G loss: 2.068396]\n",
      "epoch:17 step:15933 [D loss: 0.664269, acc: 53.12%] [G loss: 1.948818]\n",
      "epoch:17 step:15934 [D loss: 0.649867, acc: 58.59%] [G loss: 1.940537]\n",
      "epoch:17 step:15935 [D loss: 0.650492, acc: 64.06%] [G loss: 1.910687]\n",
      "epoch:17 step:15936 [D loss: 0.636593, acc: 62.50%] [G loss: 1.866589]\n",
      "epoch:17 step:15937 [D loss: 0.637458, acc: 62.50%] [G loss: 1.870303]\n",
      "epoch:17 step:15938 [D loss: 0.639092, acc: 63.28%] [G loss: 2.012249]\n",
      "epoch:17 step:15939 [D loss: 0.651925, acc: 60.94%] [G loss: 2.182437]\n",
      "epoch:17 step:15940 [D loss: 0.590526, acc: 67.97%] [G loss: 2.009799]\n",
      "epoch:17 step:15941 [D loss: 0.597982, acc: 67.19%] [G loss: 2.014969]\n",
      "epoch:17 step:15942 [D loss: 0.702002, acc: 57.03%] [G loss: 1.942945]\n",
      "epoch:17 step:15943 [D loss: 0.581145, acc: 75.78%] [G loss: 2.034304]\n",
      "epoch:17 step:15944 [D loss: 0.556298, acc: 72.66%] [G loss: 2.212893]\n",
      "epoch:17 step:15945 [D loss: 0.584158, acc: 72.66%] [G loss: 2.085164]\n",
      "epoch:17 step:15946 [D loss: 0.622568, acc: 64.06%] [G loss: 1.997971]\n",
      "epoch:17 step:15947 [D loss: 0.644792, acc: 58.59%] [G loss: 1.956394]\n",
      "epoch:17 step:15948 [D loss: 0.696116, acc: 57.81%] [G loss: 1.969204]\n",
      "epoch:17 step:15949 [D loss: 0.730868, acc: 57.03%] [G loss: 1.770515]\n",
      "epoch:17 step:15950 [D loss: 0.634833, acc: 61.72%] [G loss: 1.802341]\n",
      "epoch:17 step:15951 [D loss: 0.638863, acc: 64.06%] [G loss: 2.015651]\n",
      "epoch:17 step:15952 [D loss: 0.608538, acc: 71.88%] [G loss: 2.020267]\n",
      "epoch:17 step:15953 [D loss: 0.628422, acc: 66.41%] [G loss: 2.047987]\n",
      "epoch:17 step:15954 [D loss: 0.641040, acc: 60.94%] [G loss: 1.901420]\n",
      "epoch:17 step:15955 [D loss: 0.671736, acc: 61.72%] [G loss: 1.783497]\n",
      "epoch:17 step:15956 [D loss: 0.610978, acc: 66.41%] [G loss: 1.814355]\n",
      "epoch:17 step:15957 [D loss: 0.658272, acc: 59.38%] [G loss: 1.833738]\n",
      "epoch:17 step:15958 [D loss: 0.620619, acc: 68.75%] [G loss: 1.872388]\n",
      "epoch:17 step:15959 [D loss: 0.703995, acc: 55.47%] [G loss: 1.972705]\n",
      "epoch:17 step:15960 [D loss: 0.676679, acc: 58.59%] [G loss: 1.793354]\n",
      "epoch:17 step:15961 [D loss: 0.644912, acc: 60.94%] [G loss: 1.855061]\n",
      "epoch:17 step:15962 [D loss: 0.618957, acc: 62.50%] [G loss: 1.924798]\n",
      "epoch:17 step:15963 [D loss: 0.624831, acc: 66.41%] [G loss: 1.868452]\n",
      "epoch:17 step:15964 [D loss: 0.653993, acc: 63.28%] [G loss: 1.797554]\n",
      "epoch:17 step:15965 [D loss: 0.634614, acc: 65.62%] [G loss: 2.080178]\n",
      "epoch:17 step:15966 [D loss: 0.680869, acc: 57.81%] [G loss: 2.032618]\n",
      "epoch:17 step:15967 [D loss: 0.657164, acc: 60.94%] [G loss: 1.929358]\n",
      "epoch:17 step:15968 [D loss: 0.633522, acc: 61.72%] [G loss: 2.053885]\n",
      "epoch:17 step:15969 [D loss: 0.566622, acc: 67.97%] [G loss: 2.412220]\n",
      "epoch:17 step:15970 [D loss: 0.640385, acc: 60.16%] [G loss: 1.924972]\n",
      "epoch:17 step:15971 [D loss: 0.598175, acc: 67.19%] [G loss: 1.896093]\n",
      "epoch:17 step:15972 [D loss: 0.662506, acc: 57.03%] [G loss: 1.914196]\n",
      "epoch:17 step:15973 [D loss: 0.680553, acc: 62.50%] [G loss: 1.732473]\n",
      "epoch:17 step:15974 [D loss: 0.636546, acc: 66.41%] [G loss: 2.055073]\n",
      "epoch:17 step:15975 [D loss: 0.668880, acc: 63.28%] [G loss: 1.677866]\n",
      "epoch:17 step:15976 [D loss: 0.603767, acc: 65.62%] [G loss: 2.012261]\n",
      "epoch:17 step:15977 [D loss: 0.632672, acc: 61.72%] [G loss: 1.993553]\n",
      "epoch:17 step:15978 [D loss: 0.643979, acc: 62.50%] [G loss: 1.874866]\n",
      "epoch:17 step:15979 [D loss: 0.631042, acc: 62.50%] [G loss: 1.862407]\n",
      "epoch:17 step:15980 [D loss: 0.686692, acc: 57.81%] [G loss: 1.883467]\n",
      "epoch:17 step:15981 [D loss: 0.668787, acc: 60.94%] [G loss: 1.811697]\n",
      "epoch:17 step:15982 [D loss: 0.650002, acc: 64.06%] [G loss: 1.913238]\n",
      "epoch:17 step:15983 [D loss: 0.616460, acc: 67.19%] [G loss: 2.016113]\n",
      "epoch:17 step:15984 [D loss: 0.658687, acc: 61.72%] [G loss: 1.968303]\n",
      "epoch:17 step:15985 [D loss: 0.609529, acc: 67.19%] [G loss: 1.972380]\n",
      "epoch:17 step:15986 [D loss: 0.624492, acc: 64.84%] [G loss: 1.907798]\n",
      "epoch:17 step:15987 [D loss: 0.627923, acc: 61.72%] [G loss: 1.987890]\n",
      "epoch:17 step:15988 [D loss: 0.719201, acc: 60.16%] [G loss: 1.974334]\n",
      "epoch:17 step:15989 [D loss: 0.619017, acc: 63.28%] [G loss: 1.846600]\n",
      "epoch:17 step:15990 [D loss: 0.665815, acc: 54.69%] [G loss: 1.900863]\n",
      "epoch:17 step:15991 [D loss: 0.612083, acc: 70.31%] [G loss: 2.068739]\n",
      "epoch:17 step:15992 [D loss: 0.703629, acc: 51.56%] [G loss: 1.901533]\n",
      "epoch:17 step:15993 [D loss: 0.701588, acc: 61.72%] [G loss: 2.009186]\n",
      "epoch:17 step:15994 [D loss: 0.650476, acc: 56.25%] [G loss: 1.819176]\n",
      "epoch:17 step:15995 [D loss: 0.613819, acc: 67.19%] [G loss: 1.821634]\n",
      "epoch:17 step:15996 [D loss: 0.653676, acc: 58.59%] [G loss: 1.864902]\n",
      "epoch:17 step:15997 [D loss: 0.609792, acc: 63.28%] [G loss: 1.944005]\n",
      "epoch:17 step:15998 [D loss: 0.643396, acc: 62.50%] [G loss: 2.108739]\n",
      "epoch:17 step:15999 [D loss: 0.612496, acc: 67.19%] [G loss: 1.991202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16000 [D loss: 0.625281, acc: 64.84%] [G loss: 1.918538]\n",
      "##############\n",
      "[2.44195277 1.68270259 6.2322318  4.73578334 3.62018031 5.57033001\n",
      " 4.3923215  4.82745398 4.59859452 3.50245376]\n",
      "##########\n",
      "epoch:17 step:16001 [D loss: 0.724369, acc: 50.00%] [G loss: 1.948526]\n",
      "epoch:17 step:16002 [D loss: 0.640820, acc: 63.28%] [G loss: 1.865804]\n",
      "epoch:17 step:16003 [D loss: 0.647890, acc: 67.19%] [G loss: 1.989675]\n",
      "epoch:17 step:16004 [D loss: 0.623290, acc: 64.06%] [G loss: 1.978004]\n",
      "epoch:17 step:16005 [D loss: 0.633559, acc: 67.19%] [G loss: 1.980595]\n",
      "epoch:17 step:16006 [D loss: 0.621035, acc: 64.84%] [G loss: 2.151093]\n",
      "epoch:17 step:16007 [D loss: 0.618865, acc: 64.06%] [G loss: 1.862738]\n",
      "epoch:17 step:16008 [D loss: 0.660661, acc: 66.41%] [G loss: 1.740328]\n",
      "epoch:17 step:16009 [D loss: 0.627320, acc: 68.75%] [G loss: 1.805678]\n",
      "epoch:17 step:16010 [D loss: 0.684411, acc: 52.34%] [G loss: 1.778040]\n",
      "epoch:17 step:16011 [D loss: 0.671889, acc: 57.03%] [G loss: 1.842723]\n",
      "epoch:17 step:16012 [D loss: 0.637058, acc: 62.50%] [G loss: 1.868722]\n",
      "epoch:17 step:16013 [D loss: 0.653127, acc: 57.81%] [G loss: 1.901684]\n",
      "epoch:17 step:16014 [D loss: 0.589752, acc: 67.19%] [G loss: 1.938371]\n",
      "epoch:17 step:16015 [D loss: 0.656804, acc: 64.06%] [G loss: 1.888438]\n",
      "epoch:17 step:16016 [D loss: 0.632269, acc: 65.62%] [G loss: 1.941757]\n",
      "epoch:17 step:16017 [D loss: 0.597270, acc: 67.19%] [G loss: 1.838146]\n",
      "epoch:17 step:16018 [D loss: 0.604918, acc: 67.97%] [G loss: 2.008567]\n",
      "epoch:17 step:16019 [D loss: 0.608395, acc: 69.53%] [G loss: 1.966792]\n",
      "epoch:17 step:16020 [D loss: 0.647052, acc: 64.06%] [G loss: 1.918351]\n",
      "epoch:17 step:16021 [D loss: 0.632083, acc: 67.19%] [G loss: 1.968328]\n",
      "epoch:17 step:16022 [D loss: 0.603484, acc: 63.28%] [G loss: 2.077191]\n",
      "epoch:17 step:16023 [D loss: 0.641636, acc: 65.62%] [G loss: 2.019942]\n",
      "epoch:17 step:16024 [D loss: 0.638597, acc: 64.84%] [G loss: 1.777682]\n",
      "epoch:17 step:16025 [D loss: 0.671111, acc: 62.50%] [G loss: 1.843301]\n",
      "epoch:17 step:16026 [D loss: 0.637715, acc: 62.50%] [G loss: 1.967002]\n",
      "epoch:17 step:16027 [D loss: 0.649838, acc: 60.16%] [G loss: 1.829988]\n",
      "epoch:17 step:16028 [D loss: 0.643755, acc: 64.06%] [G loss: 1.785263]\n",
      "epoch:17 step:16029 [D loss: 0.595182, acc: 68.75%] [G loss: 1.798485]\n",
      "epoch:17 step:16030 [D loss: 0.614535, acc: 64.06%] [G loss: 2.045882]\n",
      "epoch:17 step:16031 [D loss: 0.617631, acc: 64.06%] [G loss: 1.911156]\n",
      "epoch:17 step:16032 [D loss: 0.584859, acc: 73.44%] [G loss: 2.018044]\n",
      "epoch:17 step:16033 [D loss: 0.707232, acc: 50.78%] [G loss: 1.829160]\n",
      "epoch:17 step:16034 [D loss: 0.700976, acc: 54.69%] [G loss: 1.908037]\n",
      "epoch:17 step:16035 [D loss: 0.644138, acc: 64.06%] [G loss: 2.086669]\n",
      "epoch:17 step:16036 [D loss: 0.635524, acc: 67.97%] [G loss: 2.107715]\n",
      "epoch:17 step:16037 [D loss: 0.690370, acc: 54.69%] [G loss: 1.792532]\n",
      "epoch:17 step:16038 [D loss: 0.694699, acc: 56.25%] [G loss: 1.754263]\n",
      "epoch:17 step:16039 [D loss: 0.656946, acc: 59.38%] [G loss: 1.862696]\n",
      "epoch:17 step:16040 [D loss: 0.616777, acc: 65.62%] [G loss: 2.082501]\n",
      "epoch:17 step:16041 [D loss: 0.669685, acc: 57.03%] [G loss: 1.976301]\n",
      "epoch:17 step:16042 [D loss: 0.624323, acc: 64.84%] [G loss: 1.965220]\n",
      "epoch:17 step:16043 [D loss: 0.623504, acc: 63.28%] [G loss: 2.016661]\n",
      "epoch:17 step:16044 [D loss: 0.626214, acc: 66.41%] [G loss: 2.211448]\n",
      "epoch:17 step:16045 [D loss: 0.605296, acc: 63.28%] [G loss: 2.076660]\n",
      "epoch:17 step:16046 [D loss: 0.593817, acc: 70.31%] [G loss: 2.219897]\n",
      "epoch:17 step:16047 [D loss: 0.654720, acc: 63.28%] [G loss: 1.866631]\n",
      "epoch:17 step:16048 [D loss: 0.625421, acc: 65.62%] [G loss: 2.035779]\n",
      "epoch:17 step:16049 [D loss: 0.653757, acc: 60.94%] [G loss: 1.971755]\n",
      "epoch:17 step:16050 [D loss: 0.717428, acc: 62.50%] [G loss: 1.915440]\n",
      "epoch:17 step:16051 [D loss: 0.604861, acc: 65.62%] [G loss: 2.070819]\n",
      "epoch:17 step:16052 [D loss: 0.641135, acc: 64.84%] [G loss: 1.925778]\n",
      "epoch:17 step:16053 [D loss: 0.666593, acc: 55.47%] [G loss: 1.984982]\n",
      "epoch:17 step:16054 [D loss: 0.698169, acc: 53.12%] [G loss: 1.686146]\n",
      "epoch:17 step:16055 [D loss: 0.649456, acc: 66.41%] [G loss: 1.984898]\n",
      "epoch:17 step:16056 [D loss: 0.672922, acc: 57.03%] [G loss: 1.792671]\n",
      "epoch:17 step:16057 [D loss: 0.612011, acc: 64.84%] [G loss: 1.905174]\n",
      "epoch:17 step:16058 [D loss: 0.660141, acc: 59.38%] [G loss: 1.859920]\n",
      "epoch:17 step:16059 [D loss: 0.620197, acc: 67.19%] [G loss: 1.918191]\n",
      "epoch:17 step:16060 [D loss: 0.672027, acc: 56.25%] [G loss: 2.043028]\n",
      "epoch:17 step:16061 [D loss: 0.625923, acc: 64.06%] [G loss: 1.860652]\n",
      "epoch:17 step:16062 [D loss: 0.700876, acc: 53.12%] [G loss: 1.817116]\n",
      "epoch:17 step:16063 [D loss: 0.689920, acc: 60.16%] [G loss: 1.786197]\n",
      "epoch:17 step:16064 [D loss: 0.660169, acc: 58.59%] [G loss: 1.880578]\n",
      "epoch:17 step:16065 [D loss: 0.681161, acc: 57.03%] [G loss: 1.810869]\n",
      "epoch:17 step:16066 [D loss: 0.679564, acc: 58.59%] [G loss: 1.799965]\n",
      "epoch:17 step:16067 [D loss: 0.638444, acc: 63.28%] [G loss: 1.883489]\n",
      "epoch:17 step:16068 [D loss: 0.608724, acc: 66.41%] [G loss: 1.907645]\n",
      "epoch:17 step:16069 [D loss: 0.649374, acc: 65.62%] [G loss: 1.752674]\n",
      "epoch:17 step:16070 [D loss: 0.661623, acc: 59.38%] [G loss: 1.730453]\n",
      "epoch:17 step:16071 [D loss: 0.663975, acc: 61.72%] [G loss: 1.823081]\n",
      "epoch:17 step:16072 [D loss: 0.624459, acc: 67.19%] [G loss: 1.885253]\n",
      "epoch:17 step:16073 [D loss: 0.660820, acc: 61.72%] [G loss: 1.731844]\n",
      "epoch:17 step:16074 [D loss: 0.655014, acc: 63.28%] [G loss: 1.850518]\n",
      "epoch:17 step:16075 [D loss: 0.590460, acc: 65.62%] [G loss: 1.835802]\n",
      "epoch:17 step:16076 [D loss: 0.665078, acc: 57.81%] [G loss: 1.824441]\n",
      "epoch:17 step:16077 [D loss: 0.645088, acc: 59.38%] [G loss: 1.734733]\n",
      "epoch:17 step:16078 [D loss: 0.648922, acc: 58.59%] [G loss: 1.786760]\n",
      "epoch:17 step:16079 [D loss: 0.666318, acc: 62.50%] [G loss: 1.991651]\n",
      "epoch:17 step:16080 [D loss: 0.673503, acc: 60.94%] [G loss: 1.966241]\n",
      "epoch:17 step:16081 [D loss: 0.631334, acc: 64.06%] [G loss: 1.993937]\n",
      "epoch:17 step:16082 [D loss: 0.620660, acc: 64.06%] [G loss: 1.863897]\n",
      "epoch:17 step:16083 [D loss: 0.699025, acc: 56.25%] [G loss: 1.970718]\n",
      "epoch:17 step:16084 [D loss: 0.641239, acc: 62.50%] [G loss: 1.907297]\n",
      "epoch:17 step:16085 [D loss: 0.612780, acc: 65.62%] [G loss: 1.919116]\n",
      "epoch:17 step:16086 [D loss: 0.635537, acc: 67.19%] [G loss: 1.871763]\n",
      "epoch:17 step:16087 [D loss: 0.619808, acc: 66.41%] [G loss: 1.760219]\n",
      "epoch:17 step:16088 [D loss: 0.644883, acc: 63.28%] [G loss: 1.721819]\n",
      "epoch:17 step:16089 [D loss: 0.684738, acc: 60.16%] [G loss: 1.809882]\n",
      "epoch:17 step:16090 [D loss: 0.621744, acc: 65.62%] [G loss: 1.961798]\n",
      "epoch:17 step:16091 [D loss: 0.680152, acc: 60.16%] [G loss: 1.802783]\n",
      "epoch:17 step:16092 [D loss: 0.656481, acc: 64.06%] [G loss: 1.929011]\n",
      "epoch:17 step:16093 [D loss: 0.652231, acc: 64.06%] [G loss: 1.818244]\n",
      "epoch:17 step:16094 [D loss: 0.654737, acc: 58.59%] [G loss: 1.804103]\n",
      "epoch:17 step:16095 [D loss: 0.603718, acc: 65.62%] [G loss: 1.773744]\n",
      "epoch:17 step:16096 [D loss: 0.649614, acc: 59.38%] [G loss: 1.793962]\n",
      "epoch:17 step:16097 [D loss: 0.667828, acc: 57.81%] [G loss: 1.719559]\n",
      "epoch:17 step:16098 [D loss: 0.670815, acc: 62.50%] [G loss: 1.852020]\n",
      "epoch:17 step:16099 [D loss: 0.675768, acc: 63.28%] [G loss: 1.942031]\n",
      "epoch:17 step:16100 [D loss: 0.609122, acc: 64.84%] [G loss: 1.978149]\n",
      "epoch:17 step:16101 [D loss: 0.641382, acc: 66.41%] [G loss: 1.897078]\n",
      "epoch:17 step:16102 [D loss: 0.648047, acc: 60.94%] [G loss: 1.785395]\n",
      "epoch:17 step:16103 [D loss: 0.636664, acc: 64.84%] [G loss: 1.794347]\n",
      "epoch:17 step:16104 [D loss: 0.661796, acc: 56.25%] [G loss: 1.739882]\n",
      "epoch:17 step:16105 [D loss: 0.621611, acc: 63.28%] [G loss: 1.786741]\n",
      "epoch:17 step:16106 [D loss: 0.673820, acc: 59.38%] [G loss: 1.759587]\n",
      "epoch:17 step:16107 [D loss: 0.646344, acc: 60.16%] [G loss: 1.882733]\n",
      "epoch:17 step:16108 [D loss: 0.633882, acc: 62.50%] [G loss: 1.761660]\n",
      "epoch:17 step:16109 [D loss: 0.660429, acc: 59.38%] [G loss: 1.795771]\n",
      "epoch:17 step:16110 [D loss: 0.683114, acc: 60.16%] [G loss: 1.782976]\n",
      "epoch:17 step:16111 [D loss: 0.689310, acc: 61.72%] [G loss: 2.022780]\n",
      "epoch:17 step:16112 [D loss: 0.643211, acc: 63.28%] [G loss: 1.734662]\n",
      "epoch:17 step:16113 [D loss: 0.654000, acc: 61.72%] [G loss: 1.864177]\n",
      "epoch:17 step:16114 [D loss: 0.666945, acc: 61.72%] [G loss: 1.908921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16115 [D loss: 0.675397, acc: 62.50%] [G loss: 1.965311]\n",
      "epoch:17 step:16116 [D loss: 0.669446, acc: 57.81%] [G loss: 1.816270]\n",
      "epoch:17 step:16117 [D loss: 0.669286, acc: 56.25%] [G loss: 1.951113]\n",
      "epoch:17 step:16118 [D loss: 0.641007, acc: 66.41%] [G loss: 1.908664]\n",
      "epoch:17 step:16119 [D loss: 0.637266, acc: 63.28%] [G loss: 1.914533]\n",
      "epoch:17 step:16120 [D loss: 0.653017, acc: 61.72%] [G loss: 1.913830]\n",
      "epoch:17 step:16121 [D loss: 0.662173, acc: 60.16%] [G loss: 2.085048]\n",
      "epoch:17 step:16122 [D loss: 0.650698, acc: 64.06%] [G loss: 1.863024]\n",
      "epoch:17 step:16123 [D loss: 0.632773, acc: 61.72%] [G loss: 1.947721]\n",
      "epoch:17 step:16124 [D loss: 0.763528, acc: 46.88%] [G loss: 1.960153]\n",
      "epoch:17 step:16125 [D loss: 0.645180, acc: 66.41%] [G loss: 1.900345]\n",
      "epoch:17 step:16126 [D loss: 0.593604, acc: 65.62%] [G loss: 2.098183]\n",
      "epoch:17 step:16127 [D loss: 0.628889, acc: 69.53%] [G loss: 1.833797]\n",
      "epoch:17 step:16128 [D loss: 0.652393, acc: 64.06%] [G loss: 1.798755]\n",
      "epoch:17 step:16129 [D loss: 0.653139, acc: 60.94%] [G loss: 1.816448]\n",
      "epoch:17 step:16130 [D loss: 0.641419, acc: 64.06%] [G loss: 1.890998]\n",
      "epoch:17 step:16131 [D loss: 0.632973, acc: 67.97%] [G loss: 1.982362]\n",
      "epoch:17 step:16132 [D loss: 0.665518, acc: 56.25%] [G loss: 1.941225]\n",
      "epoch:17 step:16133 [D loss: 0.626465, acc: 64.84%] [G loss: 1.941407]\n",
      "epoch:17 step:16134 [D loss: 0.643253, acc: 66.41%] [G loss: 1.766045]\n",
      "epoch:17 step:16135 [D loss: 0.627517, acc: 67.19%] [G loss: 1.918762]\n",
      "epoch:17 step:16136 [D loss: 0.617644, acc: 64.06%] [G loss: 2.111180]\n",
      "epoch:17 step:16137 [D loss: 0.554434, acc: 71.09%] [G loss: 2.276692]\n",
      "epoch:17 step:16138 [D loss: 0.560346, acc: 70.31%] [G loss: 2.166876]\n",
      "epoch:17 step:16139 [D loss: 0.659426, acc: 58.59%] [G loss: 1.828961]\n",
      "epoch:17 step:16140 [D loss: 0.677508, acc: 59.38%] [G loss: 1.766730]\n",
      "epoch:17 step:16141 [D loss: 0.639708, acc: 65.62%] [G loss: 1.800647]\n",
      "epoch:17 step:16142 [D loss: 0.643739, acc: 62.50%] [G loss: 1.934030]\n",
      "epoch:17 step:16143 [D loss: 0.676877, acc: 54.69%] [G loss: 1.857550]\n",
      "epoch:17 step:16144 [D loss: 0.660710, acc: 64.84%] [G loss: 1.851236]\n",
      "epoch:17 step:16145 [D loss: 0.625038, acc: 67.19%] [G loss: 2.219717]\n",
      "epoch:17 step:16146 [D loss: 0.593177, acc: 65.62%] [G loss: 2.048748]\n",
      "epoch:17 step:16147 [D loss: 0.567115, acc: 75.78%] [G loss: 2.213675]\n",
      "epoch:17 step:16148 [D loss: 0.589147, acc: 70.31%] [G loss: 2.297772]\n",
      "epoch:17 step:16149 [D loss: 0.743378, acc: 50.78%] [G loss: 1.702637]\n",
      "epoch:17 step:16150 [D loss: 0.637248, acc: 62.50%] [G loss: 2.047376]\n",
      "epoch:17 step:16151 [D loss: 0.640994, acc: 60.94%] [G loss: 1.964800]\n",
      "epoch:17 step:16152 [D loss: 0.643251, acc: 64.06%] [G loss: 1.930904]\n",
      "epoch:17 step:16153 [D loss: 0.633403, acc: 64.06%] [G loss: 1.872497]\n",
      "epoch:17 step:16154 [D loss: 0.678643, acc: 60.16%] [G loss: 1.865423]\n",
      "epoch:17 step:16155 [D loss: 0.625555, acc: 62.50%] [G loss: 1.964924]\n",
      "epoch:17 step:16156 [D loss: 0.646146, acc: 61.72%] [G loss: 1.857165]\n",
      "epoch:17 step:16157 [D loss: 0.674903, acc: 60.16%] [G loss: 1.791764]\n",
      "epoch:17 step:16158 [D loss: 0.584235, acc: 64.06%] [G loss: 2.012637]\n",
      "epoch:17 step:16159 [D loss: 0.589456, acc: 72.66%] [G loss: 2.189392]\n",
      "epoch:17 step:16160 [D loss: 0.537173, acc: 74.22%] [G loss: 2.433386]\n",
      "epoch:17 step:16161 [D loss: 0.579220, acc: 71.09%] [G loss: 2.294845]\n",
      "epoch:17 step:16162 [D loss: 0.712443, acc: 57.81%] [G loss: 1.813613]\n",
      "epoch:17 step:16163 [D loss: 0.653645, acc: 60.94%] [G loss: 1.901068]\n",
      "epoch:17 step:16164 [D loss: 0.694865, acc: 54.69%] [G loss: 1.821541]\n",
      "epoch:17 step:16165 [D loss: 0.655074, acc: 69.53%] [G loss: 1.867182]\n",
      "epoch:17 step:16166 [D loss: 0.690408, acc: 51.56%] [G loss: 1.877294]\n",
      "epoch:17 step:16167 [D loss: 0.646701, acc: 65.62%] [G loss: 1.963086]\n",
      "epoch:17 step:16168 [D loss: 0.671191, acc: 62.50%] [G loss: 1.908253]\n",
      "epoch:17 step:16169 [D loss: 0.680890, acc: 57.03%] [G loss: 1.846863]\n",
      "epoch:17 step:16170 [D loss: 0.662881, acc: 58.59%] [G loss: 1.859074]\n",
      "epoch:17 step:16171 [D loss: 0.593637, acc: 64.06%] [G loss: 2.008730]\n",
      "epoch:17 step:16172 [D loss: 0.690571, acc: 56.25%] [G loss: 2.000686]\n",
      "epoch:17 step:16173 [D loss: 0.593429, acc: 69.53%] [G loss: 2.018384]\n",
      "epoch:17 step:16174 [D loss: 0.587713, acc: 67.19%] [G loss: 2.086846]\n",
      "epoch:17 step:16175 [D loss: 0.660500, acc: 57.03%] [G loss: 2.034427]\n",
      "epoch:17 step:16176 [D loss: 0.614896, acc: 65.62%] [G loss: 2.070737]\n",
      "epoch:17 step:16177 [D loss: 0.667329, acc: 62.50%] [G loss: 2.089494]\n",
      "epoch:17 step:16178 [D loss: 0.670314, acc: 57.81%] [G loss: 1.690621]\n",
      "epoch:17 step:16179 [D loss: 0.699303, acc: 55.47%] [G loss: 1.699464]\n",
      "epoch:17 step:16180 [D loss: 0.718209, acc: 53.12%] [G loss: 1.821193]\n",
      "epoch:17 step:16181 [D loss: 0.650935, acc: 59.38%] [G loss: 1.721171]\n",
      "epoch:17 step:16182 [D loss: 0.646702, acc: 59.38%] [G loss: 1.856326]\n",
      "epoch:17 step:16183 [D loss: 0.708333, acc: 56.25%] [G loss: 1.857299]\n",
      "epoch:17 step:16184 [D loss: 0.608321, acc: 66.41%] [G loss: 1.781718]\n",
      "epoch:17 step:16185 [D loss: 0.614428, acc: 68.75%] [G loss: 1.872302]\n",
      "epoch:17 step:16186 [D loss: 0.680940, acc: 60.94%] [G loss: 1.795777]\n",
      "epoch:17 step:16187 [D loss: 0.627982, acc: 67.97%] [G loss: 1.942853]\n",
      "epoch:17 step:16188 [D loss: 0.632861, acc: 64.06%] [G loss: 1.845279]\n",
      "epoch:17 step:16189 [D loss: 0.630265, acc: 64.06%] [G loss: 1.867639]\n",
      "epoch:17 step:16190 [D loss: 0.629352, acc: 61.72%] [G loss: 2.046745]\n",
      "epoch:17 step:16191 [D loss: 0.656506, acc: 65.62%] [G loss: 1.919979]\n",
      "epoch:17 step:16192 [D loss: 0.673571, acc: 57.81%] [G loss: 1.870578]\n",
      "epoch:17 step:16193 [D loss: 0.617487, acc: 65.62%] [G loss: 2.079146]\n",
      "epoch:17 step:16194 [D loss: 0.622884, acc: 64.84%] [G loss: 1.902917]\n",
      "epoch:17 step:16195 [D loss: 0.664197, acc: 60.94%] [G loss: 1.884310]\n",
      "epoch:17 step:16196 [D loss: 0.628839, acc: 64.84%] [G loss: 1.791914]\n",
      "epoch:17 step:16197 [D loss: 0.631678, acc: 65.62%] [G loss: 1.884817]\n",
      "epoch:17 step:16198 [D loss: 0.628693, acc: 59.38%] [G loss: 2.014613]\n",
      "epoch:17 step:16199 [D loss: 0.648069, acc: 64.84%] [G loss: 1.932027]\n",
      "epoch:17 step:16200 [D loss: 0.576405, acc: 75.00%] [G loss: 2.073194]\n",
      "##############\n",
      "[2.51286658 1.36335658 6.25027581 4.64997741 3.75982031 5.74148969\n",
      " 4.43179868 4.79514668 4.60831728 3.66023111]\n",
      "##########\n",
      "epoch:17 step:16201 [D loss: 0.636334, acc: 67.97%] [G loss: 1.924012]\n",
      "epoch:17 step:16202 [D loss: 0.642007, acc: 66.41%] [G loss: 1.980693]\n",
      "epoch:17 step:16203 [D loss: 0.528374, acc: 78.12%] [G loss: 2.046622]\n",
      "epoch:17 step:16204 [D loss: 0.647818, acc: 60.94%] [G loss: 2.146446]\n",
      "epoch:17 step:16205 [D loss: 0.616862, acc: 67.19%] [G loss: 2.141247]\n",
      "epoch:17 step:16206 [D loss: 0.637438, acc: 68.75%] [G loss: 1.821263]\n",
      "epoch:17 step:16207 [D loss: 0.663608, acc: 52.34%] [G loss: 1.915146]\n",
      "epoch:17 step:16208 [D loss: 0.637359, acc: 65.62%] [G loss: 1.881355]\n",
      "epoch:17 step:16209 [D loss: 0.690029, acc: 53.12%] [G loss: 1.957675]\n",
      "epoch:17 step:16210 [D loss: 0.672751, acc: 60.16%] [G loss: 1.753143]\n",
      "epoch:17 step:16211 [D loss: 0.637360, acc: 67.97%] [G loss: 1.840373]\n",
      "epoch:17 step:16212 [D loss: 0.641356, acc: 63.28%] [G loss: 1.922619]\n",
      "epoch:17 step:16213 [D loss: 0.680893, acc: 58.59%] [G loss: 1.920583]\n",
      "epoch:17 step:16214 [D loss: 0.603704, acc: 67.19%] [G loss: 1.831468]\n",
      "epoch:17 step:16215 [D loss: 0.618136, acc: 71.09%] [G loss: 1.927203]\n",
      "epoch:17 step:16216 [D loss: 0.659688, acc: 61.72%] [G loss: 1.848525]\n",
      "epoch:17 step:16217 [D loss: 0.679230, acc: 60.16%] [G loss: 1.869416]\n",
      "epoch:17 step:16218 [D loss: 0.634660, acc: 65.62%] [G loss: 1.964951]\n",
      "epoch:17 step:16219 [D loss: 0.651446, acc: 60.16%] [G loss: 1.920464]\n",
      "epoch:17 step:16220 [D loss: 0.603449, acc: 60.94%] [G loss: 2.008303]\n",
      "epoch:17 step:16221 [D loss: 0.637925, acc: 69.53%] [G loss: 1.812542]\n",
      "epoch:17 step:16222 [D loss: 0.604041, acc: 66.41%] [G loss: 2.035171]\n",
      "epoch:17 step:16223 [D loss: 0.671410, acc: 60.94%] [G loss: 1.813805]\n",
      "epoch:17 step:16224 [D loss: 0.628519, acc: 61.72%] [G loss: 1.946933]\n",
      "epoch:17 step:16225 [D loss: 0.601470, acc: 68.75%] [G loss: 1.898814]\n",
      "epoch:17 step:16226 [D loss: 0.629386, acc: 67.19%] [G loss: 1.985420]\n",
      "epoch:17 step:16227 [D loss: 0.632415, acc: 67.19%] [G loss: 2.023438]\n",
      "epoch:17 step:16228 [D loss: 0.650341, acc: 64.06%] [G loss: 2.005531]\n",
      "epoch:17 step:16229 [D loss: 0.646787, acc: 64.84%] [G loss: 2.072010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16230 [D loss: 0.696640, acc: 57.81%] [G loss: 1.751720]\n",
      "epoch:17 step:16231 [D loss: 0.661674, acc: 60.16%] [G loss: 2.010763]\n",
      "epoch:17 step:16232 [D loss: 0.638035, acc: 61.72%] [G loss: 1.910820]\n",
      "epoch:17 step:16233 [D loss: 0.640839, acc: 59.38%] [G loss: 1.789612]\n",
      "epoch:17 step:16234 [D loss: 0.583959, acc: 70.31%] [G loss: 1.832491]\n",
      "epoch:17 step:16235 [D loss: 0.644640, acc: 59.38%] [G loss: 1.951625]\n",
      "epoch:17 step:16236 [D loss: 0.652882, acc: 63.28%] [G loss: 1.934985]\n",
      "epoch:17 step:16237 [D loss: 0.667012, acc: 64.06%] [G loss: 1.798395]\n",
      "epoch:17 step:16238 [D loss: 0.672079, acc: 56.25%] [G loss: 1.963662]\n",
      "epoch:17 step:16239 [D loss: 0.681363, acc: 57.03%] [G loss: 1.898174]\n",
      "epoch:17 step:16240 [D loss: 0.656295, acc: 63.28%] [G loss: 1.718085]\n",
      "epoch:17 step:16241 [D loss: 0.612994, acc: 70.31%] [G loss: 2.066204]\n",
      "epoch:17 step:16242 [D loss: 0.563778, acc: 76.56%] [G loss: 2.104027]\n",
      "epoch:17 step:16243 [D loss: 0.556644, acc: 75.00%] [G loss: 2.090197]\n",
      "epoch:17 step:16244 [D loss: 0.558027, acc: 74.22%] [G loss: 2.207342]\n",
      "epoch:17 step:16245 [D loss: 0.692342, acc: 57.81%] [G loss: 1.868730]\n",
      "epoch:17 step:16246 [D loss: 0.673309, acc: 56.25%] [G loss: 1.726949]\n",
      "epoch:17 step:16247 [D loss: 0.606566, acc: 65.62%] [G loss: 1.957251]\n",
      "epoch:17 step:16248 [D loss: 0.629706, acc: 63.28%] [G loss: 1.856506]\n",
      "epoch:17 step:16249 [D loss: 0.603834, acc: 68.75%] [G loss: 1.897334]\n",
      "epoch:17 step:16250 [D loss: 0.662208, acc: 62.50%] [G loss: 1.970899]\n",
      "epoch:17 step:16251 [D loss: 0.669957, acc: 65.62%] [G loss: 1.894957]\n",
      "epoch:17 step:16252 [D loss: 0.624239, acc: 64.84%] [G loss: 1.818956]\n",
      "epoch:17 step:16253 [D loss: 0.681345, acc: 57.03%] [G loss: 1.859609]\n",
      "epoch:17 step:16254 [D loss: 0.650246, acc: 60.94%] [G loss: 2.078570]\n",
      "epoch:17 step:16255 [D loss: 0.666555, acc: 63.28%] [G loss: 1.935855]\n",
      "epoch:17 step:16256 [D loss: 0.624215, acc: 64.06%] [G loss: 1.883478]\n",
      "epoch:17 step:16257 [D loss: 0.644821, acc: 62.50%] [G loss: 2.046868]\n",
      "epoch:17 step:16258 [D loss: 0.678433, acc: 53.91%] [G loss: 1.977096]\n",
      "epoch:17 step:16259 [D loss: 0.675556, acc: 57.03%] [G loss: 2.042174]\n",
      "epoch:17 step:16260 [D loss: 0.623291, acc: 67.19%] [G loss: 2.142320]\n",
      "epoch:17 step:16261 [D loss: 0.614223, acc: 64.84%] [G loss: 2.020809]\n",
      "epoch:17 step:16262 [D loss: 0.633515, acc: 62.50%] [G loss: 2.022327]\n",
      "epoch:17 step:16263 [D loss: 0.645070, acc: 61.72%] [G loss: 1.961184]\n",
      "epoch:17 step:16264 [D loss: 0.653169, acc: 60.16%] [G loss: 1.920190]\n",
      "epoch:17 step:16265 [D loss: 0.625957, acc: 68.75%] [G loss: 2.189165]\n",
      "epoch:17 step:16266 [D loss: 0.642929, acc: 60.94%] [G loss: 1.959473]\n",
      "epoch:17 step:16267 [D loss: 0.655032, acc: 61.72%] [G loss: 2.038290]\n",
      "epoch:17 step:16268 [D loss: 0.614620, acc: 69.53%] [G loss: 1.924386]\n",
      "epoch:17 step:16269 [D loss: 0.603998, acc: 63.28%] [G loss: 1.973603]\n",
      "epoch:17 step:16270 [D loss: 0.727024, acc: 53.91%] [G loss: 1.769610]\n",
      "epoch:17 step:16271 [D loss: 0.674901, acc: 57.81%] [G loss: 1.900987]\n",
      "epoch:17 step:16272 [D loss: 0.692339, acc: 60.16%] [G loss: 1.766920]\n",
      "epoch:17 step:16273 [D loss: 0.615244, acc: 66.41%] [G loss: 1.882309]\n",
      "epoch:17 step:16274 [D loss: 0.638524, acc: 57.81%] [G loss: 2.125715]\n",
      "epoch:17 step:16275 [D loss: 0.637085, acc: 65.62%] [G loss: 2.052802]\n",
      "epoch:17 step:16276 [D loss: 0.595635, acc: 67.97%] [G loss: 2.078899]\n",
      "epoch:17 step:16277 [D loss: 0.699878, acc: 55.47%] [G loss: 1.874215]\n",
      "epoch:17 step:16278 [D loss: 0.728026, acc: 56.25%] [G loss: 1.826628]\n",
      "epoch:17 step:16279 [D loss: 0.642816, acc: 66.41%] [G loss: 1.806374]\n",
      "epoch:17 step:16280 [D loss: 0.658776, acc: 60.16%] [G loss: 1.813506]\n",
      "epoch:17 step:16281 [D loss: 0.665331, acc: 59.38%] [G loss: 1.782754]\n",
      "epoch:17 step:16282 [D loss: 0.623881, acc: 68.75%] [G loss: 1.931715]\n",
      "epoch:17 step:16283 [D loss: 0.598648, acc: 68.75%] [G loss: 2.104394]\n",
      "epoch:17 step:16284 [D loss: 0.659644, acc: 56.25%] [G loss: 1.756305]\n",
      "epoch:17 step:16285 [D loss: 0.704651, acc: 55.47%] [G loss: 1.759027]\n",
      "epoch:17 step:16286 [D loss: 0.614847, acc: 65.62%] [G loss: 1.791927]\n",
      "epoch:17 step:16287 [D loss: 0.607101, acc: 61.72%] [G loss: 1.895135]\n",
      "epoch:17 step:16288 [D loss: 0.648226, acc: 67.97%] [G loss: 1.937497]\n",
      "epoch:17 step:16289 [D loss: 0.641024, acc: 66.41%] [G loss: 1.878055]\n",
      "epoch:17 step:16290 [D loss: 0.628085, acc: 60.94%] [G loss: 1.800695]\n",
      "epoch:17 step:16291 [D loss: 0.632272, acc: 62.50%] [G loss: 1.844257]\n",
      "epoch:17 step:16292 [D loss: 0.653751, acc: 60.94%] [G loss: 1.991967]\n",
      "epoch:17 step:16293 [D loss: 0.629714, acc: 61.72%] [G loss: 1.985019]\n",
      "epoch:17 step:16294 [D loss: 0.596789, acc: 68.75%] [G loss: 1.812156]\n",
      "epoch:17 step:16295 [D loss: 0.630278, acc: 61.72%] [G loss: 2.011957]\n",
      "epoch:17 step:16296 [D loss: 0.637922, acc: 67.19%] [G loss: 1.935508]\n",
      "epoch:17 step:16297 [D loss: 0.648440, acc: 64.06%] [G loss: 1.831696]\n",
      "epoch:17 step:16298 [D loss: 0.612112, acc: 66.41%] [G loss: 1.873573]\n",
      "epoch:17 step:16299 [D loss: 0.611845, acc: 69.53%] [G loss: 1.938068]\n",
      "epoch:17 step:16300 [D loss: 0.645313, acc: 63.28%] [G loss: 2.072529]\n",
      "epoch:17 step:16301 [D loss: 0.587823, acc: 71.09%] [G loss: 1.905170]\n",
      "epoch:17 step:16302 [D loss: 0.720516, acc: 57.03%] [G loss: 1.801677]\n",
      "epoch:17 step:16303 [D loss: 0.649827, acc: 59.38%] [G loss: 1.989842]\n",
      "epoch:17 step:16304 [D loss: 0.686802, acc: 54.69%] [G loss: 1.776559]\n",
      "epoch:17 step:16305 [D loss: 0.695859, acc: 59.38%] [G loss: 1.906867]\n",
      "epoch:17 step:16306 [D loss: 0.723546, acc: 50.78%] [G loss: 1.655540]\n",
      "epoch:17 step:16307 [D loss: 0.669326, acc: 61.72%] [G loss: 1.903427]\n",
      "epoch:17 step:16308 [D loss: 0.682886, acc: 58.59%] [G loss: 1.777207]\n",
      "epoch:17 step:16309 [D loss: 0.636077, acc: 60.94%] [G loss: 1.977782]\n",
      "epoch:17 step:16310 [D loss: 0.614746, acc: 68.75%] [G loss: 1.901142]\n",
      "epoch:17 step:16311 [D loss: 0.636875, acc: 60.94%] [G loss: 1.956035]\n",
      "epoch:17 step:16312 [D loss: 0.629039, acc: 60.94%] [G loss: 2.015841]\n",
      "epoch:17 step:16313 [D loss: 0.656265, acc: 60.94%] [G loss: 2.038301]\n",
      "epoch:17 step:16314 [D loss: 0.629314, acc: 65.62%] [G loss: 2.146864]\n",
      "epoch:17 step:16315 [D loss: 0.659038, acc: 57.03%] [G loss: 1.640592]\n",
      "epoch:17 step:16316 [D loss: 0.675547, acc: 60.16%] [G loss: 1.740077]\n",
      "epoch:17 step:16317 [D loss: 0.700570, acc: 54.69%] [G loss: 1.726737]\n",
      "epoch:17 step:16318 [D loss: 0.663639, acc: 62.50%] [G loss: 1.788633]\n",
      "epoch:17 step:16319 [D loss: 0.622895, acc: 67.19%] [G loss: 1.916883]\n",
      "epoch:17 step:16320 [D loss: 0.640630, acc: 63.28%] [G loss: 1.823833]\n",
      "epoch:17 step:16321 [D loss: 0.615302, acc: 64.06%] [G loss: 1.724032]\n",
      "epoch:17 step:16322 [D loss: 0.626715, acc: 63.28%] [G loss: 1.820351]\n",
      "epoch:17 step:16323 [D loss: 0.672575, acc: 53.91%] [G loss: 1.872863]\n",
      "epoch:17 step:16324 [D loss: 0.663649, acc: 57.03%] [G loss: 1.855459]\n",
      "epoch:17 step:16325 [D loss: 0.681451, acc: 53.91%] [G loss: 1.807945]\n",
      "epoch:17 step:16326 [D loss: 0.672404, acc: 61.72%] [G loss: 1.658344]\n",
      "epoch:17 step:16327 [D loss: 0.584238, acc: 73.44%] [G loss: 1.875165]\n",
      "epoch:17 step:16328 [D loss: 0.628991, acc: 63.28%] [G loss: 1.839927]\n",
      "epoch:17 step:16329 [D loss: 0.738975, acc: 55.47%] [G loss: 1.846644]\n",
      "epoch:17 step:16330 [D loss: 0.634102, acc: 62.50%] [G loss: 1.960149]\n",
      "epoch:17 step:16331 [D loss: 0.654343, acc: 59.38%] [G loss: 1.790441]\n",
      "epoch:17 step:16332 [D loss: 0.653809, acc: 59.38%] [G loss: 1.965950]\n",
      "epoch:17 step:16333 [D loss: 0.609591, acc: 61.72%] [G loss: 2.031313]\n",
      "epoch:17 step:16334 [D loss: 0.584553, acc: 69.53%] [G loss: 2.079314]\n",
      "epoch:17 step:16335 [D loss: 0.644384, acc: 58.59%] [G loss: 2.021054]\n",
      "epoch:17 step:16336 [D loss: 0.677129, acc: 59.38%] [G loss: 1.846460]\n",
      "epoch:17 step:16337 [D loss: 0.691082, acc: 58.59%] [G loss: 1.856431]\n",
      "epoch:17 step:16338 [D loss: 0.636545, acc: 60.94%] [G loss: 1.924165]\n",
      "epoch:17 step:16339 [D loss: 0.620520, acc: 62.50%] [G loss: 1.903889]\n",
      "epoch:17 step:16340 [D loss: 0.696432, acc: 54.69%] [G loss: 1.838344]\n",
      "epoch:17 step:16341 [D loss: 0.669187, acc: 60.16%] [G loss: 1.842405]\n",
      "epoch:17 step:16342 [D loss: 0.626760, acc: 67.19%] [G loss: 2.077628]\n",
      "epoch:17 step:16343 [D loss: 0.649537, acc: 66.41%] [G loss: 1.875881]\n",
      "epoch:17 step:16344 [D loss: 0.592170, acc: 67.19%] [G loss: 1.973898]\n",
      "epoch:17 step:16345 [D loss: 0.608432, acc: 70.31%] [G loss: 2.083428]\n",
      "epoch:17 step:16346 [D loss: 0.634029, acc: 67.19%] [G loss: 1.909268]\n",
      "epoch:17 step:16347 [D loss: 0.676095, acc: 60.16%] [G loss: 1.810855]\n",
      "epoch:17 step:16348 [D loss: 0.619377, acc: 66.41%] [G loss: 1.874835]\n",
      "epoch:17 step:16349 [D loss: 0.712662, acc: 52.34%] [G loss: 1.900295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16350 [D loss: 0.667467, acc: 60.16%] [G loss: 1.859869]\n",
      "epoch:17 step:16351 [D loss: 0.712686, acc: 57.03%] [G loss: 1.870896]\n",
      "epoch:17 step:16352 [D loss: 0.650992, acc: 64.84%] [G loss: 1.702013]\n",
      "epoch:17 step:16353 [D loss: 0.613166, acc: 64.84%] [G loss: 1.812842]\n",
      "epoch:17 step:16354 [D loss: 0.658715, acc: 58.59%] [G loss: 1.908757]\n",
      "epoch:17 step:16355 [D loss: 0.608961, acc: 69.53%] [G loss: 1.948472]\n",
      "epoch:17 step:16356 [D loss: 0.633582, acc: 65.62%] [G loss: 1.930028]\n",
      "epoch:17 step:16357 [D loss: 0.617206, acc: 64.06%] [G loss: 1.938687]\n",
      "epoch:17 step:16358 [D loss: 0.583152, acc: 71.88%] [G loss: 2.044780]\n",
      "epoch:17 step:16359 [D loss: 0.618500, acc: 64.84%] [G loss: 2.242481]\n",
      "epoch:17 step:16360 [D loss: 0.615527, acc: 68.75%] [G loss: 2.041426]\n",
      "epoch:17 step:16361 [D loss: 0.711019, acc: 62.50%] [G loss: 1.876515]\n",
      "epoch:17 step:16362 [D loss: 0.644805, acc: 64.84%] [G loss: 1.826931]\n",
      "epoch:17 step:16363 [D loss: 0.611435, acc: 64.06%] [G loss: 2.093575]\n",
      "epoch:17 step:16364 [D loss: 0.691300, acc: 55.47%] [G loss: 1.931167]\n",
      "epoch:17 step:16365 [D loss: 0.657308, acc: 60.16%] [G loss: 1.794498]\n",
      "epoch:17 step:16366 [D loss: 0.668512, acc: 62.50%] [G loss: 1.764616]\n",
      "epoch:17 step:16367 [D loss: 0.662408, acc: 60.16%] [G loss: 1.811344]\n",
      "epoch:17 step:16368 [D loss: 0.636771, acc: 63.28%] [G loss: 1.763115]\n",
      "epoch:17 step:16369 [D loss: 0.634865, acc: 64.06%] [G loss: 2.020736]\n",
      "epoch:17 step:16370 [D loss: 0.672646, acc: 53.91%] [G loss: 1.746325]\n",
      "epoch:17 step:16371 [D loss: 0.656481, acc: 60.16%] [G loss: 1.954413]\n",
      "epoch:17 step:16372 [D loss: 0.672174, acc: 57.03%] [G loss: 1.818479]\n",
      "epoch:17 step:16373 [D loss: 0.700626, acc: 54.69%] [G loss: 1.813086]\n",
      "epoch:17 step:16374 [D loss: 0.660297, acc: 64.06%] [G loss: 1.883405]\n",
      "epoch:17 step:16375 [D loss: 0.670907, acc: 55.47%] [G loss: 1.803711]\n",
      "epoch:17 step:16376 [D loss: 0.654383, acc: 59.38%] [G loss: 1.832379]\n",
      "epoch:17 step:16377 [D loss: 0.667455, acc: 60.16%] [G loss: 1.743213]\n",
      "epoch:17 step:16378 [D loss: 0.650805, acc: 56.25%] [G loss: 1.807206]\n",
      "epoch:17 step:16379 [D loss: 0.630149, acc: 62.50%] [G loss: 1.848555]\n",
      "epoch:17 step:16380 [D loss: 0.666902, acc: 60.94%] [G loss: 1.802727]\n",
      "epoch:17 step:16381 [D loss: 0.660478, acc: 62.50%] [G loss: 1.777774]\n",
      "epoch:17 step:16382 [D loss: 0.651632, acc: 64.06%] [G loss: 1.792779]\n",
      "epoch:17 step:16383 [D loss: 0.655573, acc: 64.06%] [G loss: 1.779689]\n",
      "epoch:17 step:16384 [D loss: 0.640613, acc: 67.19%] [G loss: 1.806278]\n",
      "epoch:17 step:16385 [D loss: 0.641353, acc: 63.28%] [G loss: 1.972065]\n",
      "epoch:17 step:16386 [D loss: 0.666789, acc: 61.72%] [G loss: 1.875123]\n",
      "epoch:17 step:16387 [D loss: 0.700884, acc: 58.59%] [G loss: 1.748677]\n",
      "epoch:17 step:16388 [D loss: 0.692186, acc: 57.81%] [G loss: 1.758209]\n",
      "epoch:17 step:16389 [D loss: 0.680478, acc: 57.81%] [G loss: 1.723208]\n",
      "epoch:17 step:16390 [D loss: 0.665527, acc: 64.06%] [G loss: 1.720367]\n",
      "epoch:17 step:16391 [D loss: 0.656828, acc: 57.81%] [G loss: 1.854835]\n",
      "epoch:17 step:16392 [D loss: 0.602615, acc: 67.19%] [G loss: 2.032873]\n",
      "epoch:17 step:16393 [D loss: 0.638904, acc: 65.62%] [G loss: 1.811025]\n",
      "epoch:17 step:16394 [D loss: 0.706263, acc: 57.81%] [G loss: 1.786120]\n",
      "epoch:17 step:16395 [D loss: 0.606272, acc: 71.88%] [G loss: 1.802463]\n",
      "epoch:17 step:16396 [D loss: 0.610454, acc: 67.97%] [G loss: 1.812494]\n",
      "epoch:17 step:16397 [D loss: 0.656617, acc: 60.94%] [G loss: 1.912256]\n",
      "epoch:17 step:16398 [D loss: 0.601953, acc: 67.19%] [G loss: 2.128919]\n",
      "epoch:17 step:16399 [D loss: 0.631723, acc: 62.50%] [G loss: 2.006598]\n",
      "epoch:17 step:16400 [D loss: 0.576402, acc: 69.53%] [G loss: 2.291413]\n",
      "##############\n",
      "[2.4064167  1.44171544 6.38098287 4.78125631 3.69639905 5.58193776\n",
      " 4.30618335 4.56711046 4.66052578 3.79302596]\n",
      "##########\n",
      "epoch:17 step:16401 [D loss: 0.657648, acc: 61.72%] [G loss: 2.041986]\n",
      "epoch:17 step:16402 [D loss: 0.671824, acc: 59.38%] [G loss: 1.796347]\n",
      "epoch:17 step:16403 [D loss: 0.608967, acc: 67.97%] [G loss: 1.805470]\n",
      "epoch:17 step:16404 [D loss: 0.644530, acc: 60.94%] [G loss: 1.977702]\n",
      "epoch:17 step:16405 [D loss: 0.615426, acc: 67.97%] [G loss: 2.101404]\n",
      "epoch:17 step:16406 [D loss: 0.699852, acc: 54.69%] [G loss: 1.666389]\n",
      "epoch:17 step:16407 [D loss: 0.662439, acc: 60.94%] [G loss: 1.745944]\n",
      "epoch:17 step:16408 [D loss: 0.646811, acc: 67.97%] [G loss: 1.888091]\n",
      "epoch:17 step:16409 [D loss: 0.619894, acc: 64.84%] [G loss: 1.882715]\n",
      "epoch:17 step:16410 [D loss: 0.620532, acc: 67.19%] [G loss: 1.991443]\n",
      "epoch:17 step:16411 [D loss: 0.705923, acc: 56.25%] [G loss: 1.749786]\n",
      "epoch:17 step:16412 [D loss: 0.652767, acc: 61.72%] [G loss: 1.809049]\n",
      "epoch:17 step:16413 [D loss: 0.627697, acc: 65.62%] [G loss: 1.873954]\n",
      "epoch:17 step:16414 [D loss: 0.660089, acc: 64.84%] [G loss: 1.818607]\n",
      "epoch:17 step:16415 [D loss: 0.644162, acc: 66.41%] [G loss: 1.801871]\n",
      "epoch:17 step:16416 [D loss: 0.662660, acc: 67.97%] [G loss: 1.806231]\n",
      "epoch:17 step:16417 [D loss: 0.630134, acc: 67.97%] [G loss: 1.962696]\n",
      "epoch:17 step:16418 [D loss: 0.656773, acc: 54.69%] [G loss: 1.923684]\n",
      "epoch:17 step:16419 [D loss: 0.633743, acc: 66.41%] [G loss: 1.819491]\n",
      "epoch:17 step:16420 [D loss: 0.630444, acc: 60.94%] [G loss: 2.043379]\n",
      "epoch:17 step:16421 [D loss: 0.616291, acc: 64.84%] [G loss: 1.854732]\n",
      "epoch:17 step:16422 [D loss: 0.628924, acc: 64.84%] [G loss: 2.004946]\n",
      "epoch:17 step:16423 [D loss: 0.693978, acc: 53.91%] [G loss: 2.046322]\n",
      "epoch:17 step:16424 [D loss: 0.610499, acc: 68.75%] [G loss: 2.036581]\n",
      "epoch:17 step:16425 [D loss: 0.616536, acc: 64.84%] [G loss: 1.940474]\n",
      "epoch:17 step:16426 [D loss: 0.629975, acc: 72.66%] [G loss: 1.951026]\n",
      "epoch:17 step:16427 [D loss: 0.574666, acc: 73.44%] [G loss: 2.113593]\n",
      "epoch:17 step:16428 [D loss: 0.566791, acc: 73.44%] [G loss: 2.297028]\n",
      "epoch:17 step:16429 [D loss: 0.705194, acc: 57.03%] [G loss: 1.766247]\n",
      "epoch:17 step:16430 [D loss: 0.738829, acc: 52.34%] [G loss: 1.764667]\n",
      "epoch:17 step:16431 [D loss: 0.701156, acc: 54.69%] [G loss: 1.711275]\n",
      "epoch:17 step:16432 [D loss: 0.672527, acc: 60.94%] [G loss: 1.850909]\n",
      "epoch:17 step:16433 [D loss: 0.642691, acc: 64.84%] [G loss: 1.813384]\n",
      "epoch:17 step:16434 [D loss: 0.636719, acc: 66.41%] [G loss: 1.789628]\n",
      "epoch:17 step:16435 [D loss: 0.690761, acc: 57.03%] [G loss: 1.708943]\n",
      "epoch:17 step:16436 [D loss: 0.708889, acc: 54.69%] [G loss: 1.805740]\n",
      "epoch:17 step:16437 [D loss: 0.624243, acc: 63.28%] [G loss: 1.895390]\n",
      "epoch:17 step:16438 [D loss: 0.654405, acc: 62.50%] [G loss: 1.765332]\n",
      "epoch:17 step:16439 [D loss: 0.666628, acc: 57.81%] [G loss: 1.846205]\n",
      "epoch:17 step:16440 [D loss: 0.680008, acc: 56.25%] [G loss: 1.809670]\n",
      "epoch:17 step:16441 [D loss: 0.661252, acc: 64.06%] [G loss: 1.910101]\n",
      "epoch:17 step:16442 [D loss: 0.625747, acc: 63.28%] [G loss: 1.852103]\n",
      "epoch:17 step:16443 [D loss: 0.637742, acc: 60.16%] [G loss: 1.773311]\n",
      "epoch:17 step:16444 [D loss: 0.604347, acc: 71.88%] [G loss: 1.932398]\n",
      "epoch:17 step:16445 [D loss: 0.589377, acc: 68.75%] [G loss: 1.870335]\n",
      "epoch:17 step:16446 [D loss: 0.611817, acc: 63.28%] [G loss: 2.025457]\n",
      "epoch:17 step:16447 [D loss: 0.689362, acc: 59.38%] [G loss: 1.831994]\n",
      "epoch:17 step:16448 [D loss: 0.647416, acc: 67.97%] [G loss: 1.831010]\n",
      "epoch:17 step:16449 [D loss: 0.612740, acc: 69.53%] [G loss: 1.918941]\n",
      "epoch:17 step:16450 [D loss: 0.657630, acc: 59.38%] [G loss: 2.031565]\n",
      "epoch:17 step:16451 [D loss: 0.624309, acc: 70.31%] [G loss: 2.032544]\n",
      "epoch:17 step:16452 [D loss: 0.627312, acc: 58.59%] [G loss: 1.936229]\n",
      "epoch:17 step:16453 [D loss: 0.698159, acc: 61.72%] [G loss: 1.857627]\n",
      "epoch:17 step:16454 [D loss: 0.656982, acc: 60.16%] [G loss: 1.694315]\n",
      "epoch:17 step:16455 [D loss: 0.675944, acc: 54.69%] [G loss: 1.870619]\n",
      "epoch:17 step:16456 [D loss: 0.643793, acc: 65.62%] [G loss: 1.824294]\n",
      "epoch:17 step:16457 [D loss: 0.651927, acc: 59.38%] [G loss: 1.772041]\n",
      "epoch:17 step:16458 [D loss: 0.635573, acc: 62.50%] [G loss: 1.793213]\n",
      "epoch:17 step:16459 [D loss: 0.696483, acc: 57.03%] [G loss: 1.635254]\n",
      "epoch:17 step:16460 [D loss: 0.673260, acc: 53.12%] [G loss: 1.876940]\n",
      "epoch:17 step:16461 [D loss: 0.614003, acc: 68.75%] [G loss: 1.853299]\n",
      "epoch:17 step:16462 [D loss: 0.600775, acc: 67.19%] [G loss: 1.891752]\n",
      "epoch:17 step:16463 [D loss: 0.664478, acc: 57.81%] [G loss: 1.823618]\n",
      "epoch:17 step:16464 [D loss: 0.671701, acc: 58.59%] [G loss: 1.779965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16465 [D loss: 0.616247, acc: 68.75%] [G loss: 1.990652]\n",
      "epoch:17 step:16466 [D loss: 0.669914, acc: 59.38%] [G loss: 1.801930]\n",
      "epoch:17 step:16467 [D loss: 0.647834, acc: 64.84%] [G loss: 1.768294]\n",
      "epoch:17 step:16468 [D loss: 0.671015, acc: 59.38%] [G loss: 1.976929]\n",
      "epoch:17 step:16469 [D loss: 0.640278, acc: 63.28%] [G loss: 1.850200]\n",
      "epoch:17 step:16470 [D loss: 0.698156, acc: 57.81%] [G loss: 1.726394]\n",
      "epoch:17 step:16471 [D loss: 0.678172, acc: 64.84%] [G loss: 1.714703]\n",
      "epoch:17 step:16472 [D loss: 0.698003, acc: 60.16%] [G loss: 1.879510]\n",
      "epoch:17 step:16473 [D loss: 0.645834, acc: 62.50%] [G loss: 1.762054]\n",
      "epoch:17 step:16474 [D loss: 0.644079, acc: 60.94%] [G loss: 1.879042]\n",
      "epoch:17 step:16475 [D loss: 0.654857, acc: 60.16%] [G loss: 1.750054]\n",
      "epoch:17 step:16476 [D loss: 0.600105, acc: 67.19%] [G loss: 1.845612]\n",
      "epoch:17 step:16477 [D loss: 0.624309, acc: 63.28%] [G loss: 1.856583]\n",
      "epoch:17 step:16478 [D loss: 0.585737, acc: 71.09%] [G loss: 2.012695]\n",
      "epoch:17 step:16479 [D loss: 0.619192, acc: 68.75%] [G loss: 2.028476]\n",
      "epoch:17 step:16480 [D loss: 0.673718, acc: 63.28%] [G loss: 2.085614]\n",
      "epoch:17 step:16481 [D loss: 0.612151, acc: 64.84%] [G loss: 1.895240]\n",
      "epoch:17 step:16482 [D loss: 0.685551, acc: 58.59%] [G loss: 1.787853]\n",
      "epoch:17 step:16483 [D loss: 0.593193, acc: 70.31%] [G loss: 2.103510]\n",
      "epoch:17 step:16484 [D loss: 0.581788, acc: 69.53%] [G loss: 2.004599]\n",
      "epoch:17 step:16485 [D loss: 0.567952, acc: 72.66%] [G loss: 2.120137]\n",
      "epoch:17 step:16486 [D loss: 0.586746, acc: 67.97%] [G loss: 2.089117]\n",
      "epoch:17 step:16487 [D loss: 0.612298, acc: 65.62%] [G loss: 2.013426]\n",
      "epoch:17 step:16488 [D loss: 0.707250, acc: 52.34%] [G loss: 1.881453]\n",
      "epoch:17 step:16489 [D loss: 0.644344, acc: 64.84%] [G loss: 1.750602]\n",
      "epoch:17 step:16490 [D loss: 0.656023, acc: 64.84%] [G loss: 1.981290]\n",
      "epoch:17 step:16491 [D loss: 0.629027, acc: 64.06%] [G loss: 1.900821]\n",
      "epoch:17 step:16492 [D loss: 0.634014, acc: 66.41%] [G loss: 2.006023]\n",
      "epoch:17 step:16493 [D loss: 0.593775, acc: 71.88%] [G loss: 2.178180]\n",
      "epoch:17 step:16494 [D loss: 0.661746, acc: 60.94%] [G loss: 1.828285]\n",
      "epoch:17 step:16495 [D loss: 0.715856, acc: 50.78%] [G loss: 1.859023]\n",
      "epoch:17 step:16496 [D loss: 0.646680, acc: 64.84%] [G loss: 1.843989]\n",
      "epoch:17 step:16497 [D loss: 0.671351, acc: 61.72%] [G loss: 1.901736]\n",
      "epoch:17 step:16498 [D loss: 0.694065, acc: 57.81%] [G loss: 1.802011]\n",
      "epoch:17 step:16499 [D loss: 0.631511, acc: 64.06%] [G loss: 1.924055]\n",
      "epoch:17 step:16500 [D loss: 0.656521, acc: 61.72%] [G loss: 1.846826]\n",
      "epoch:17 step:16501 [D loss: 0.687063, acc: 56.25%] [G loss: 1.676463]\n",
      "epoch:17 step:16502 [D loss: 0.672189, acc: 60.94%] [G loss: 1.734027]\n",
      "epoch:17 step:16503 [D loss: 0.614009, acc: 67.97%] [G loss: 1.889981]\n",
      "epoch:17 step:16504 [D loss: 0.608866, acc: 65.62%] [G loss: 1.861162]\n",
      "epoch:17 step:16505 [D loss: 0.619764, acc: 67.97%] [G loss: 1.729158]\n",
      "epoch:17 step:16506 [D loss: 0.666856, acc: 60.16%] [G loss: 1.783059]\n",
      "epoch:17 step:16507 [D loss: 0.686877, acc: 57.81%] [G loss: 1.916873]\n",
      "epoch:17 step:16508 [D loss: 0.628611, acc: 65.62%] [G loss: 1.886589]\n",
      "epoch:17 step:16509 [D loss: 0.639629, acc: 64.84%] [G loss: 1.879128]\n",
      "epoch:17 step:16510 [D loss: 0.680118, acc: 57.81%] [G loss: 1.915883]\n",
      "epoch:17 step:16511 [D loss: 0.610349, acc: 64.06%] [G loss: 2.047544]\n",
      "epoch:17 step:16512 [D loss: 0.666041, acc: 64.84%] [G loss: 1.941666]\n",
      "epoch:17 step:16513 [D loss: 0.684317, acc: 60.16%] [G loss: 1.811733]\n",
      "epoch:17 step:16514 [D loss: 0.636583, acc: 63.28%] [G loss: 1.967323]\n",
      "epoch:17 step:16515 [D loss: 0.704762, acc: 60.94%] [G loss: 1.972829]\n",
      "epoch:17 step:16516 [D loss: 0.602071, acc: 66.41%] [G loss: 1.950524]\n",
      "epoch:17 step:16517 [D loss: 0.633220, acc: 62.50%] [G loss: 1.944792]\n",
      "epoch:17 step:16518 [D loss: 0.602865, acc: 71.09%] [G loss: 1.973699]\n",
      "epoch:17 step:16519 [D loss: 0.677792, acc: 61.72%] [G loss: 1.822895]\n",
      "epoch:17 step:16520 [D loss: 0.648344, acc: 63.28%] [G loss: 1.921964]\n",
      "epoch:17 step:16521 [D loss: 0.676805, acc: 57.81%] [G loss: 1.884099]\n",
      "epoch:17 step:16522 [D loss: 0.621210, acc: 63.28%] [G loss: 1.919568]\n",
      "epoch:17 step:16523 [D loss: 0.615455, acc: 67.97%] [G loss: 1.839418]\n",
      "epoch:17 step:16524 [D loss: 0.651654, acc: 63.28%] [G loss: 1.915864]\n",
      "epoch:17 step:16525 [D loss: 0.700870, acc: 58.59%] [G loss: 1.747578]\n",
      "epoch:17 step:16526 [D loss: 0.666819, acc: 63.28%] [G loss: 1.920783]\n",
      "epoch:17 step:16527 [D loss: 0.629546, acc: 66.41%] [G loss: 1.886222]\n",
      "epoch:17 step:16528 [D loss: 0.624334, acc: 67.19%] [G loss: 1.964900]\n",
      "epoch:17 step:16529 [D loss: 0.666578, acc: 59.38%] [G loss: 1.927687]\n",
      "epoch:17 step:16530 [D loss: 0.732935, acc: 57.03%] [G loss: 1.861684]\n",
      "epoch:17 step:16531 [D loss: 0.620684, acc: 67.97%] [G loss: 2.030159]\n",
      "epoch:17 step:16532 [D loss: 0.654491, acc: 62.50%] [G loss: 1.846403]\n",
      "epoch:17 step:16533 [D loss: 0.634784, acc: 62.50%] [G loss: 1.935945]\n",
      "epoch:17 step:16534 [D loss: 0.583927, acc: 67.97%] [G loss: 1.877300]\n",
      "epoch:17 step:16535 [D loss: 0.658576, acc: 55.47%] [G loss: 1.768855]\n",
      "epoch:17 step:16536 [D loss: 0.644220, acc: 61.72%] [G loss: 1.801777]\n",
      "epoch:17 step:16537 [D loss: 0.599457, acc: 69.53%] [G loss: 2.010805]\n",
      "epoch:17 step:16538 [D loss: 0.647322, acc: 62.50%] [G loss: 2.022479]\n",
      "epoch:17 step:16539 [D loss: 0.685694, acc: 58.59%] [G loss: 1.923336]\n",
      "epoch:17 step:16540 [D loss: 0.643483, acc: 58.59%] [G loss: 1.876464]\n",
      "epoch:17 step:16541 [D loss: 0.659316, acc: 64.84%] [G loss: 1.869587]\n",
      "epoch:17 step:16542 [D loss: 0.606199, acc: 64.84%] [G loss: 2.130865]\n",
      "epoch:17 step:16543 [D loss: 0.658040, acc: 57.81%] [G loss: 1.827701]\n",
      "epoch:17 step:16544 [D loss: 0.621516, acc: 61.72%] [G loss: 2.001855]\n",
      "epoch:17 step:16545 [D loss: 0.659948, acc: 61.72%] [G loss: 1.882441]\n",
      "epoch:17 step:16546 [D loss: 0.651652, acc: 58.59%] [G loss: 1.916478]\n",
      "epoch:17 step:16547 [D loss: 0.678807, acc: 58.59%] [G loss: 1.919182]\n",
      "epoch:17 step:16548 [D loss: 0.632380, acc: 67.19%] [G loss: 1.880268]\n",
      "epoch:17 step:16549 [D loss: 0.691749, acc: 51.56%] [G loss: 1.833166]\n",
      "epoch:17 step:16550 [D loss: 0.621933, acc: 64.06%] [G loss: 1.881143]\n",
      "epoch:17 step:16551 [D loss: 0.648738, acc: 62.50%] [G loss: 1.922858]\n",
      "epoch:17 step:16552 [D loss: 0.610992, acc: 64.84%] [G loss: 1.864557]\n",
      "epoch:17 step:16553 [D loss: 0.587434, acc: 66.41%] [G loss: 2.026339]\n",
      "epoch:17 step:16554 [D loss: 0.713253, acc: 51.56%] [G loss: 1.848620]\n",
      "epoch:17 step:16555 [D loss: 0.684772, acc: 51.56%] [G loss: 1.777027]\n",
      "epoch:17 step:16556 [D loss: 0.682654, acc: 56.25%] [G loss: 1.814786]\n",
      "epoch:17 step:16557 [D loss: 0.675989, acc: 62.50%] [G loss: 1.728402]\n",
      "epoch:17 step:16558 [D loss: 0.614851, acc: 64.06%] [G loss: 1.838422]\n",
      "epoch:17 step:16559 [D loss: 0.626893, acc: 70.31%] [G loss: 1.923173]\n",
      "epoch:17 step:16560 [D loss: 0.649333, acc: 66.41%] [G loss: 1.940365]\n",
      "epoch:17 step:16561 [D loss: 0.568595, acc: 73.44%] [G loss: 2.056868]\n",
      "epoch:17 step:16562 [D loss: 0.595208, acc: 66.41%] [G loss: 1.860790]\n",
      "epoch:17 step:16563 [D loss: 0.621919, acc: 68.75%] [G loss: 2.044780]\n",
      "epoch:17 step:16564 [D loss: 0.678050, acc: 60.94%] [G loss: 1.889077]\n",
      "epoch:17 step:16565 [D loss: 0.626338, acc: 61.72%] [G loss: 1.908045]\n",
      "epoch:17 step:16566 [D loss: 0.623008, acc: 66.41%] [G loss: 2.038867]\n",
      "epoch:17 step:16567 [D loss: 0.587669, acc: 67.19%] [G loss: 2.078216]\n",
      "epoch:17 step:16568 [D loss: 0.663120, acc: 57.03%] [G loss: 1.851751]\n",
      "epoch:17 step:16569 [D loss: 0.581548, acc: 66.41%] [G loss: 1.989059]\n",
      "epoch:17 step:16570 [D loss: 0.595565, acc: 71.09%] [G loss: 1.948954]\n",
      "epoch:17 step:16571 [D loss: 0.633932, acc: 62.50%] [G loss: 2.055989]\n",
      "epoch:17 step:16572 [D loss: 0.674760, acc: 63.28%] [G loss: 1.903775]\n",
      "epoch:17 step:16573 [D loss: 0.623842, acc: 61.72%] [G loss: 2.049153]\n",
      "epoch:17 step:16574 [D loss: 0.649532, acc: 64.06%] [G loss: 1.932333]\n",
      "epoch:17 step:16575 [D loss: 0.658497, acc: 60.94%] [G loss: 2.011155]\n",
      "epoch:17 step:16576 [D loss: 0.621014, acc: 67.97%] [G loss: 2.126365]\n",
      "epoch:17 step:16577 [D loss: 0.601452, acc: 64.84%] [G loss: 2.194178]\n",
      "epoch:17 step:16578 [D loss: 0.604090, acc: 69.53%] [G loss: 2.098275]\n",
      "epoch:17 step:16579 [D loss: 0.646499, acc: 64.84%] [G loss: 1.991954]\n",
      "epoch:17 step:16580 [D loss: 0.614873, acc: 63.28%] [G loss: 1.819990]\n",
      "epoch:17 step:16581 [D loss: 0.641306, acc: 66.41%] [G loss: 1.821428]\n",
      "epoch:17 step:16582 [D loss: 0.645875, acc: 64.06%] [G loss: 1.928221]\n",
      "epoch:17 step:16583 [D loss: 0.588737, acc: 72.66%] [G loss: 2.195455]\n",
      "epoch:17 step:16584 [D loss: 0.623390, acc: 69.53%] [G loss: 1.994699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16585 [D loss: 0.693852, acc: 62.50%] [G loss: 1.954704]\n",
      "epoch:17 step:16586 [D loss: 0.690440, acc: 58.59%] [G loss: 1.785717]\n",
      "epoch:17 step:16587 [D loss: 0.675811, acc: 58.59%] [G loss: 1.796730]\n",
      "epoch:17 step:16588 [D loss: 0.674366, acc: 57.03%] [G loss: 1.834843]\n",
      "epoch:17 step:16589 [D loss: 0.625011, acc: 66.41%] [G loss: 1.931244]\n",
      "epoch:17 step:16590 [D loss: 0.629763, acc: 67.19%] [G loss: 1.950072]\n",
      "epoch:17 step:16591 [D loss: 0.671641, acc: 57.81%] [G loss: 1.853264]\n",
      "epoch:17 step:16592 [D loss: 0.635133, acc: 63.28%] [G loss: 1.954380]\n",
      "epoch:17 step:16593 [D loss: 0.620490, acc: 61.72%] [G loss: 1.908610]\n",
      "epoch:17 step:16594 [D loss: 0.695457, acc: 53.12%] [G loss: 1.825377]\n",
      "epoch:17 step:16595 [D loss: 0.689016, acc: 60.94%] [G loss: 1.811126]\n",
      "epoch:17 step:16596 [D loss: 0.611775, acc: 64.84%] [G loss: 1.812741]\n",
      "epoch:17 step:16597 [D loss: 0.631008, acc: 64.06%] [G loss: 2.009757]\n",
      "epoch:17 step:16598 [D loss: 0.622074, acc: 64.84%] [G loss: 1.818860]\n",
      "epoch:17 step:16599 [D loss: 0.679683, acc: 60.16%] [G loss: 1.847345]\n",
      "epoch:17 step:16600 [D loss: 0.688170, acc: 58.59%] [G loss: 1.785964]\n",
      "##############\n",
      "[2.57415959 1.31916437 6.16557769 4.84202721 3.61629202 5.67866176\n",
      " 4.51878801 4.65007404 4.59864179 3.77862686]\n",
      "##########\n",
      "epoch:17 step:16601 [D loss: 0.595789, acc: 70.31%] [G loss: 1.887595]\n",
      "epoch:17 step:16602 [D loss: 0.632731, acc: 60.16%] [G loss: 1.906076]\n",
      "epoch:17 step:16603 [D loss: 0.653429, acc: 63.28%] [G loss: 1.926185]\n",
      "epoch:17 step:16604 [D loss: 0.656103, acc: 62.50%] [G loss: 1.761468]\n",
      "epoch:17 step:16605 [D loss: 0.625775, acc: 62.50%] [G loss: 1.848328]\n",
      "epoch:17 step:16606 [D loss: 0.566792, acc: 75.78%] [G loss: 1.994393]\n",
      "epoch:17 step:16607 [D loss: 0.648967, acc: 64.84%] [G loss: 1.821375]\n",
      "epoch:17 step:16608 [D loss: 0.564289, acc: 72.66%] [G loss: 1.983984]\n",
      "epoch:17 step:16609 [D loss: 0.566903, acc: 68.75%] [G loss: 1.929007]\n",
      "epoch:17 step:16610 [D loss: 0.676677, acc: 61.72%] [G loss: 1.924239]\n",
      "epoch:17 step:16611 [D loss: 0.652497, acc: 59.38%] [G loss: 1.970460]\n",
      "epoch:17 step:16612 [D loss: 0.731919, acc: 50.78%] [G loss: 1.707183]\n",
      "epoch:17 step:16613 [D loss: 0.617169, acc: 60.16%] [G loss: 1.912991]\n",
      "epoch:17 step:16614 [D loss: 0.610236, acc: 67.97%] [G loss: 2.001429]\n",
      "epoch:17 step:16615 [D loss: 0.643450, acc: 64.84%] [G loss: 1.782059]\n",
      "epoch:17 step:16616 [D loss: 0.696773, acc: 55.47%] [G loss: 1.910953]\n",
      "epoch:17 step:16617 [D loss: 0.611193, acc: 69.53%] [G loss: 1.835764]\n",
      "epoch:17 step:16618 [D loss: 0.674230, acc: 55.47%] [G loss: 1.961225]\n",
      "epoch:17 step:16619 [D loss: 0.638181, acc: 62.50%] [G loss: 1.885712]\n",
      "epoch:17 step:16620 [D loss: 0.667546, acc: 61.72%] [G loss: 1.906644]\n",
      "epoch:17 step:16621 [D loss: 0.646150, acc: 56.25%] [G loss: 1.914456]\n",
      "epoch:17 step:16622 [D loss: 0.672357, acc: 59.38%] [G loss: 1.938504]\n",
      "epoch:17 step:16623 [D loss: 0.570581, acc: 67.19%] [G loss: 2.153708]\n",
      "epoch:17 step:16624 [D loss: 0.644973, acc: 64.84%] [G loss: 2.024878]\n",
      "epoch:17 step:16625 [D loss: 0.657465, acc: 57.81%] [G loss: 1.880545]\n",
      "epoch:17 step:16626 [D loss: 0.645001, acc: 60.94%] [G loss: 1.798808]\n",
      "epoch:17 step:16627 [D loss: 0.651259, acc: 60.94%] [G loss: 1.946975]\n",
      "epoch:17 step:16628 [D loss: 0.635113, acc: 60.94%] [G loss: 1.984946]\n",
      "epoch:17 step:16629 [D loss: 0.660697, acc: 59.38%] [G loss: 1.932358]\n",
      "epoch:17 step:16630 [D loss: 0.656997, acc: 57.81%] [G loss: 1.951556]\n",
      "epoch:17 step:16631 [D loss: 0.659105, acc: 60.94%] [G loss: 1.811718]\n",
      "epoch:17 step:16632 [D loss: 0.657580, acc: 57.81%] [G loss: 1.831902]\n",
      "epoch:17 step:16633 [D loss: 0.714983, acc: 50.78%] [G loss: 1.693776]\n",
      "epoch:17 step:16634 [D loss: 0.625663, acc: 66.41%] [G loss: 1.839621]\n",
      "epoch:17 step:16635 [D loss: 0.670110, acc: 63.28%] [G loss: 1.938675]\n",
      "epoch:17 step:16636 [D loss: 0.643652, acc: 60.16%] [G loss: 1.941655]\n",
      "epoch:17 step:16637 [D loss: 0.617444, acc: 65.62%] [G loss: 2.000171]\n",
      "epoch:17 step:16638 [D loss: 0.631061, acc: 64.06%] [G loss: 2.016940]\n",
      "epoch:17 step:16639 [D loss: 0.678213, acc: 57.81%] [G loss: 1.776762]\n",
      "epoch:17 step:16640 [D loss: 0.674513, acc: 56.25%] [G loss: 1.871798]\n",
      "epoch:17 step:16641 [D loss: 0.611443, acc: 60.94%] [G loss: 2.084012]\n",
      "epoch:17 step:16642 [D loss: 0.677895, acc: 59.38%] [G loss: 1.896304]\n",
      "epoch:17 step:16643 [D loss: 0.589492, acc: 66.41%] [G loss: 1.827783]\n",
      "epoch:17 step:16644 [D loss: 0.662378, acc: 61.72%] [G loss: 1.775394]\n",
      "epoch:17 step:16645 [D loss: 0.700810, acc: 57.03%] [G loss: 1.868980]\n",
      "epoch:17 step:16646 [D loss: 0.651648, acc: 58.59%] [G loss: 1.776252]\n",
      "epoch:17 step:16647 [D loss: 0.662702, acc: 63.28%] [G loss: 1.774219]\n",
      "epoch:17 step:16648 [D loss: 0.613939, acc: 67.19%] [G loss: 1.949008]\n",
      "epoch:17 step:16649 [D loss: 0.679663, acc: 61.72%] [G loss: 1.957245]\n",
      "epoch:17 step:16650 [D loss: 0.601039, acc: 70.31%] [G loss: 1.874328]\n",
      "epoch:17 step:16651 [D loss: 0.677869, acc: 55.47%] [G loss: 1.846832]\n",
      "epoch:17 step:16652 [D loss: 0.648957, acc: 62.50%] [G loss: 1.714485]\n",
      "epoch:17 step:16653 [D loss: 0.637008, acc: 64.06%] [G loss: 1.735275]\n",
      "epoch:17 step:16654 [D loss: 0.636123, acc: 60.16%] [G loss: 1.873556]\n",
      "epoch:17 step:16655 [D loss: 0.601860, acc: 67.19%] [G loss: 1.902271]\n",
      "epoch:17 step:16656 [D loss: 0.632946, acc: 62.50%] [G loss: 1.796092]\n",
      "epoch:17 step:16657 [D loss: 0.663971, acc: 55.47%] [G loss: 2.068738]\n",
      "epoch:17 step:16658 [D loss: 0.642925, acc: 60.94%] [G loss: 2.032821]\n",
      "epoch:17 step:16659 [D loss: 0.656467, acc: 60.94%] [G loss: 1.990651]\n",
      "epoch:17 step:16660 [D loss: 0.663798, acc: 58.59%] [G loss: 1.862430]\n",
      "epoch:17 step:16661 [D loss: 0.674018, acc: 64.84%] [G loss: 1.847453]\n",
      "epoch:17 step:16662 [D loss: 0.611711, acc: 67.97%] [G loss: 1.847962]\n",
      "epoch:17 step:16663 [D loss: 0.666825, acc: 60.16%] [G loss: 1.798673]\n",
      "epoch:17 step:16664 [D loss: 0.663617, acc: 61.72%] [G loss: 1.980730]\n",
      "epoch:17 step:16665 [D loss: 0.641973, acc: 64.84%] [G loss: 1.865191]\n",
      "epoch:17 step:16666 [D loss: 0.648931, acc: 60.94%] [G loss: 1.910271]\n",
      "epoch:17 step:16667 [D loss: 0.628880, acc: 63.28%] [G loss: 1.819487]\n",
      "epoch:17 step:16668 [D loss: 0.680777, acc: 57.81%] [G loss: 1.857062]\n",
      "epoch:17 step:16669 [D loss: 0.649464, acc: 60.16%] [G loss: 1.917450]\n",
      "epoch:17 step:16670 [D loss: 0.678164, acc: 50.00%] [G loss: 1.916793]\n",
      "epoch:17 step:16671 [D loss: 0.651110, acc: 60.94%] [G loss: 1.910997]\n",
      "epoch:17 step:16672 [D loss: 0.638653, acc: 64.84%] [G loss: 1.778868]\n",
      "epoch:17 step:16673 [D loss: 0.620761, acc: 64.06%] [G loss: 1.974490]\n",
      "epoch:17 step:16674 [D loss: 0.666759, acc: 54.69%] [G loss: 1.750523]\n",
      "epoch:17 step:16675 [D loss: 0.605418, acc: 73.44%] [G loss: 1.939940]\n",
      "epoch:17 step:16676 [D loss: 0.613586, acc: 66.41%] [G loss: 2.011467]\n",
      "epoch:17 step:16677 [D loss: 0.616682, acc: 65.62%] [G loss: 1.827252]\n",
      "epoch:17 step:16678 [D loss: 0.654642, acc: 65.62%] [G loss: 1.760406]\n",
      "epoch:17 step:16679 [D loss: 0.657777, acc: 64.06%] [G loss: 1.782384]\n",
      "epoch:17 step:16680 [D loss: 0.638307, acc: 63.28%] [G loss: 1.857641]\n",
      "epoch:17 step:16681 [D loss: 0.696620, acc: 60.16%] [G loss: 1.773318]\n",
      "epoch:17 step:16682 [D loss: 0.617205, acc: 69.53%] [G loss: 1.856969]\n",
      "epoch:17 step:16683 [D loss: 0.640379, acc: 60.94%] [G loss: 1.825639]\n",
      "epoch:17 step:16684 [D loss: 0.682987, acc: 56.25%] [G loss: 1.849257]\n",
      "epoch:17 step:16685 [D loss: 0.646585, acc: 63.28%] [G loss: 1.954020]\n",
      "epoch:17 step:16686 [D loss: 0.646672, acc: 64.06%] [G loss: 1.983140]\n",
      "epoch:17 step:16687 [D loss: 0.661890, acc: 61.72%] [G loss: 1.724890]\n",
      "epoch:17 step:16688 [D loss: 0.666115, acc: 60.94%] [G loss: 1.839402]\n",
      "epoch:17 step:16689 [D loss: 0.635988, acc: 63.28%] [G loss: 1.845737]\n",
      "epoch:17 step:16690 [D loss: 0.658636, acc: 61.72%] [G loss: 1.973865]\n",
      "epoch:17 step:16691 [D loss: 0.668232, acc: 60.94%] [G loss: 1.712779]\n",
      "epoch:17 step:16692 [D loss: 0.624967, acc: 65.62%] [G loss: 1.947578]\n",
      "epoch:17 step:16693 [D loss: 0.638688, acc: 61.72%] [G loss: 1.816092]\n",
      "epoch:17 step:16694 [D loss: 0.716012, acc: 50.00%] [G loss: 1.746580]\n",
      "epoch:17 step:16695 [D loss: 0.693962, acc: 51.56%] [G loss: 1.674793]\n",
      "epoch:17 step:16696 [D loss: 0.669105, acc: 57.81%] [G loss: 1.736181]\n",
      "epoch:17 step:16697 [D loss: 0.683994, acc: 60.94%] [G loss: 1.866079]\n",
      "epoch:17 step:16698 [D loss: 0.629445, acc: 60.16%] [G loss: 1.939172]\n",
      "epoch:17 step:16699 [D loss: 0.648106, acc: 63.28%] [G loss: 1.929851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16700 [D loss: 0.665307, acc: 62.50%] [G loss: 1.844487]\n",
      "epoch:17 step:16701 [D loss: 0.672624, acc: 59.38%] [G loss: 1.868167]\n",
      "epoch:17 step:16702 [D loss: 0.644020, acc: 64.06%] [G loss: 1.973971]\n",
      "epoch:17 step:16703 [D loss: 0.596850, acc: 67.97%] [G loss: 2.138894]\n",
      "epoch:17 step:16704 [D loss: 0.609646, acc: 60.16%] [G loss: 2.045289]\n",
      "epoch:17 step:16705 [D loss: 0.630100, acc: 63.28%] [G loss: 1.880490]\n",
      "epoch:17 step:16706 [D loss: 0.674734, acc: 60.94%] [G loss: 1.795930]\n",
      "epoch:17 step:16707 [D loss: 0.629334, acc: 64.06%] [G loss: 1.885981]\n",
      "epoch:17 step:16708 [D loss: 0.636253, acc: 64.84%] [G loss: 1.944224]\n",
      "epoch:17 step:16709 [D loss: 0.630113, acc: 70.31%] [G loss: 2.111926]\n",
      "epoch:17 step:16710 [D loss: 0.612882, acc: 69.53%] [G loss: 2.001276]\n",
      "epoch:17 step:16711 [D loss: 0.589364, acc: 71.09%] [G loss: 2.162109]\n",
      "epoch:17 step:16712 [D loss: 0.660268, acc: 67.97%] [G loss: 1.823150]\n",
      "epoch:17 step:16713 [D loss: 0.701825, acc: 53.91%] [G loss: 1.839239]\n",
      "epoch:17 step:16714 [D loss: 0.634295, acc: 63.28%] [G loss: 1.919697]\n",
      "epoch:17 step:16715 [D loss: 0.649675, acc: 66.41%] [G loss: 2.178962]\n",
      "epoch:17 step:16716 [D loss: 0.714830, acc: 47.66%] [G loss: 1.839340]\n",
      "epoch:17 step:16717 [D loss: 0.666058, acc: 60.94%] [G loss: 1.813988]\n",
      "epoch:17 step:16718 [D loss: 0.648704, acc: 61.72%] [G loss: 1.948925]\n",
      "epoch:17 step:16719 [D loss: 0.678963, acc: 60.94%] [G loss: 1.903076]\n",
      "epoch:17 step:16720 [D loss: 0.699755, acc: 62.50%] [G loss: 1.860880]\n",
      "epoch:17 step:16721 [D loss: 0.624744, acc: 67.19%] [G loss: 1.960542]\n",
      "epoch:17 step:16722 [D loss: 0.662459, acc: 60.16%] [G loss: 1.982773]\n",
      "epoch:17 step:16723 [D loss: 0.664970, acc: 53.91%] [G loss: 1.787633]\n",
      "epoch:17 step:16724 [D loss: 0.676240, acc: 60.94%] [G loss: 1.758227]\n",
      "epoch:17 step:16725 [D loss: 0.667630, acc: 57.81%] [G loss: 1.869676]\n",
      "epoch:17 step:16726 [D loss: 0.666058, acc: 59.38%] [G loss: 1.810946]\n",
      "epoch:17 step:16727 [D loss: 0.671316, acc: 60.94%] [G loss: 1.901102]\n",
      "epoch:17 step:16728 [D loss: 0.628435, acc: 67.19%] [G loss: 1.811788]\n",
      "epoch:17 step:16729 [D loss: 0.710406, acc: 53.91%] [G loss: 1.781984]\n",
      "epoch:17 step:16730 [D loss: 0.684422, acc: 61.72%] [G loss: 1.767474]\n",
      "epoch:17 step:16731 [D loss: 0.661412, acc: 61.72%] [G loss: 1.833957]\n",
      "epoch:17 step:16732 [D loss: 0.626647, acc: 64.84%] [G loss: 1.788804]\n",
      "epoch:17 step:16733 [D loss: 0.637844, acc: 61.72%] [G loss: 1.910480]\n",
      "epoch:17 step:16734 [D loss: 0.619917, acc: 66.41%] [G loss: 2.108387]\n",
      "epoch:17 step:16735 [D loss: 0.637556, acc: 63.28%] [G loss: 1.899065]\n",
      "epoch:17 step:16736 [D loss: 0.607552, acc: 69.53%] [G loss: 1.918813]\n",
      "epoch:17 step:16737 [D loss: 0.712206, acc: 56.25%] [G loss: 1.839531]\n",
      "epoch:17 step:16738 [D loss: 0.641980, acc: 64.84%] [G loss: 1.901980]\n",
      "epoch:17 step:16739 [D loss: 0.606604, acc: 66.41%] [G loss: 1.923960]\n",
      "epoch:17 step:16740 [D loss: 0.622884, acc: 66.41%] [G loss: 1.800551]\n",
      "epoch:17 step:16741 [D loss: 0.643300, acc: 62.50%] [G loss: 1.887944]\n",
      "epoch:17 step:16742 [D loss: 0.616535, acc: 66.41%] [G loss: 1.924049]\n",
      "epoch:17 step:16743 [D loss: 0.695317, acc: 58.59%] [G loss: 1.947317]\n",
      "epoch:17 step:16744 [D loss: 0.616349, acc: 67.19%] [G loss: 2.314071]\n",
      "epoch:17 step:16745 [D loss: 0.597092, acc: 69.53%] [G loss: 2.117853]\n",
      "epoch:17 step:16746 [D loss: 0.673924, acc: 60.16%] [G loss: 1.969787]\n",
      "epoch:17 step:16747 [D loss: 0.657838, acc: 60.94%] [G loss: 1.830118]\n",
      "epoch:17 step:16748 [D loss: 0.634334, acc: 61.72%] [G loss: 1.815530]\n",
      "epoch:17 step:16749 [D loss: 0.663316, acc: 63.28%] [G loss: 1.703931]\n",
      "epoch:17 step:16750 [D loss: 0.592429, acc: 68.75%] [G loss: 1.931303]\n",
      "epoch:17 step:16751 [D loss: 0.658364, acc: 60.16%] [G loss: 1.835136]\n",
      "epoch:17 step:16752 [D loss: 0.623456, acc: 63.28%] [G loss: 1.872923]\n",
      "epoch:17 step:16753 [D loss: 0.633598, acc: 66.41%] [G loss: 2.025373]\n",
      "epoch:17 step:16754 [D loss: 0.697354, acc: 60.16%] [G loss: 1.966482]\n",
      "epoch:17 step:16755 [D loss: 0.685742, acc: 55.47%] [G loss: 1.950608]\n",
      "epoch:17 step:16756 [D loss: 0.644625, acc: 64.06%] [G loss: 1.741891]\n",
      "epoch:17 step:16757 [D loss: 0.697816, acc: 50.78%] [G loss: 1.743341]\n",
      "epoch:17 step:16758 [D loss: 0.669877, acc: 60.16%] [G loss: 1.826286]\n",
      "epoch:17 step:16759 [D loss: 0.652375, acc: 58.59%] [G loss: 1.732770]\n",
      "epoch:17 step:16760 [D loss: 0.649511, acc: 62.50%] [G loss: 2.018746]\n",
      "epoch:17 step:16761 [D loss: 0.646143, acc: 59.38%] [G loss: 1.851172]\n",
      "epoch:17 step:16762 [D loss: 0.588666, acc: 69.53%] [G loss: 1.850429]\n",
      "epoch:17 step:16763 [D loss: 0.642311, acc: 58.59%] [G loss: 1.937371]\n",
      "epoch:17 step:16764 [D loss: 0.605274, acc: 69.53%] [G loss: 1.852453]\n",
      "epoch:17 step:16765 [D loss: 0.664938, acc: 61.72%] [G loss: 1.770583]\n",
      "epoch:17 step:16766 [D loss: 0.631760, acc: 64.06%] [G loss: 1.956995]\n",
      "epoch:17 step:16767 [D loss: 0.659805, acc: 60.16%] [G loss: 1.890880]\n",
      "epoch:17 step:16768 [D loss: 0.666853, acc: 57.03%] [G loss: 1.886980]\n",
      "epoch:17 step:16769 [D loss: 0.648335, acc: 66.41%] [G loss: 1.822813]\n",
      "epoch:17 step:16770 [D loss: 0.588039, acc: 71.09%] [G loss: 1.961193]\n",
      "epoch:17 step:16771 [D loss: 0.684061, acc: 57.03%] [G loss: 1.963098]\n",
      "epoch:17 step:16772 [D loss: 0.668672, acc: 59.38%] [G loss: 1.864838]\n",
      "epoch:17 step:16773 [D loss: 0.668008, acc: 58.59%] [G loss: 2.003028]\n",
      "epoch:17 step:16774 [D loss: 0.632268, acc: 67.19%] [G loss: 2.063889]\n",
      "epoch:17 step:16775 [D loss: 0.676968, acc: 59.38%] [G loss: 1.946734]\n",
      "epoch:17 step:16776 [D loss: 0.630734, acc: 65.62%] [G loss: 1.883643]\n",
      "epoch:17 step:16777 [D loss: 0.642169, acc: 60.94%] [G loss: 1.850482]\n",
      "epoch:17 step:16778 [D loss: 0.651220, acc: 62.50%] [G loss: 1.915228]\n",
      "epoch:17 step:16779 [D loss: 0.634269, acc: 65.62%] [G loss: 1.952986]\n",
      "epoch:17 step:16780 [D loss: 0.665068, acc: 56.25%] [G loss: 1.970372]\n",
      "epoch:17 step:16781 [D loss: 0.607492, acc: 71.09%] [G loss: 2.081712]\n",
      "epoch:17 step:16782 [D loss: 0.629980, acc: 64.84%] [G loss: 1.945469]\n",
      "epoch:17 step:16783 [D loss: 0.677245, acc: 61.72%] [G loss: 1.817743]\n",
      "epoch:17 step:16784 [D loss: 0.630409, acc: 63.28%] [G loss: 1.893740]\n",
      "epoch:17 step:16785 [D loss: 0.680075, acc: 57.81%] [G loss: 1.806577]\n",
      "epoch:17 step:16786 [D loss: 0.631468, acc: 70.31%] [G loss: 1.959377]\n",
      "epoch:17 step:16787 [D loss: 0.675850, acc: 54.69%] [G loss: 1.811261]\n",
      "epoch:17 step:16788 [D loss: 0.732410, acc: 50.78%] [G loss: 1.844275]\n",
      "epoch:17 step:16789 [D loss: 0.625065, acc: 64.84%] [G loss: 1.839691]\n",
      "epoch:17 step:16790 [D loss: 0.629492, acc: 67.19%] [G loss: 1.834952]\n",
      "epoch:17 step:16791 [D loss: 0.633503, acc: 61.72%] [G loss: 1.780051]\n",
      "epoch:17 step:16792 [D loss: 0.649350, acc: 61.72%] [G loss: 1.817449]\n",
      "epoch:17 step:16793 [D loss: 0.632468, acc: 64.06%] [G loss: 1.754497]\n",
      "epoch:17 step:16794 [D loss: 0.623673, acc: 67.97%] [G loss: 1.795550]\n",
      "epoch:17 step:16795 [D loss: 0.608156, acc: 72.66%] [G loss: 1.861294]\n",
      "epoch:17 step:16796 [D loss: 0.695143, acc: 59.38%] [G loss: 1.720250]\n",
      "epoch:17 step:16797 [D loss: 0.632656, acc: 60.94%] [G loss: 1.922306]\n",
      "epoch:17 step:16798 [D loss: 0.669051, acc: 61.72%] [G loss: 1.782417]\n",
      "epoch:17 step:16799 [D loss: 0.650038, acc: 61.72%] [G loss: 1.943618]\n",
      "epoch:17 step:16800 [D loss: 0.633791, acc: 61.72%] [G loss: 1.905637]\n",
      "##############\n",
      "[2.48661718 1.39963111 6.19453911 4.69606883 3.55187403 5.49926479\n",
      " 4.32344255 4.68691235 4.55162226 3.68245662]\n",
      "##########\n",
      "epoch:17 step:16801 [D loss: 0.618284, acc: 61.72%] [G loss: 1.931431]\n",
      "epoch:17 step:16802 [D loss: 0.714809, acc: 52.34%] [G loss: 1.773547]\n",
      "epoch:17 step:16803 [D loss: 0.669072, acc: 61.72%] [G loss: 1.753019]\n",
      "epoch:17 step:16804 [D loss: 0.626683, acc: 66.41%] [G loss: 1.888167]\n",
      "epoch:17 step:16805 [D loss: 0.664542, acc: 63.28%] [G loss: 1.855775]\n",
      "epoch:17 step:16806 [D loss: 0.663364, acc: 60.94%] [G loss: 1.918105]\n",
      "epoch:17 step:16807 [D loss: 0.681849, acc: 57.03%] [G loss: 1.886822]\n",
      "epoch:17 step:16808 [D loss: 0.649608, acc: 58.59%] [G loss: 1.844951]\n",
      "epoch:17 step:16809 [D loss: 0.606746, acc: 67.19%] [G loss: 1.836865]\n",
      "epoch:17 step:16810 [D loss: 0.657293, acc: 64.06%] [G loss: 1.785399]\n",
      "epoch:17 step:16811 [D loss: 0.631502, acc: 60.94%] [G loss: 2.039193]\n",
      "epoch:17 step:16812 [D loss: 0.639510, acc: 58.59%] [G loss: 1.821070]\n",
      "epoch:17 step:16813 [D loss: 0.601125, acc: 71.09%] [G loss: 1.952172]\n",
      "epoch:17 step:16814 [D loss: 0.663282, acc: 60.94%] [G loss: 1.784894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16815 [D loss: 0.652654, acc: 63.28%] [G loss: 2.070204]\n",
      "epoch:17 step:16816 [D loss: 0.651701, acc: 60.16%] [G loss: 1.785653]\n",
      "epoch:17 step:16817 [D loss: 0.642168, acc: 60.94%] [G loss: 1.872124]\n",
      "epoch:17 step:16818 [D loss: 0.603089, acc: 67.19%] [G loss: 1.957808]\n",
      "epoch:17 step:16819 [D loss: 0.571816, acc: 74.22%] [G loss: 2.047913]\n",
      "epoch:17 step:16820 [D loss: 0.696576, acc: 57.03%] [G loss: 1.842213]\n",
      "epoch:17 step:16821 [D loss: 0.677818, acc: 56.25%] [G loss: 1.858382]\n",
      "epoch:17 step:16822 [D loss: 0.661255, acc: 60.94%] [G loss: 1.839692]\n",
      "epoch:17 step:16823 [D loss: 0.641800, acc: 60.94%] [G loss: 2.008134]\n",
      "epoch:17 step:16824 [D loss: 0.653661, acc: 62.50%] [G loss: 1.826856]\n",
      "epoch:17 step:16825 [D loss: 0.632043, acc: 62.50%] [G loss: 1.842159]\n",
      "epoch:17 step:16826 [D loss: 0.610774, acc: 70.31%] [G loss: 1.963037]\n",
      "epoch:17 step:16827 [D loss: 0.641313, acc: 64.84%] [G loss: 1.894897]\n",
      "epoch:17 step:16828 [D loss: 0.614933, acc: 66.41%] [G loss: 1.997268]\n",
      "epoch:17 step:16829 [D loss: 0.669260, acc: 60.94%] [G loss: 1.943625]\n",
      "epoch:17 step:16830 [D loss: 0.633935, acc: 64.06%] [G loss: 1.830189]\n",
      "epoch:17 step:16831 [D loss: 0.693968, acc: 54.69%] [G loss: 1.759379]\n",
      "epoch:17 step:16832 [D loss: 0.647941, acc: 61.72%] [G loss: 1.953288]\n",
      "epoch:17 step:16833 [D loss: 0.644458, acc: 64.06%] [G loss: 1.958191]\n",
      "epoch:17 step:16834 [D loss: 0.609231, acc: 68.75%] [G loss: 1.940090]\n",
      "epoch:17 step:16835 [D loss: 0.638891, acc: 62.50%] [G loss: 1.916447]\n",
      "epoch:17 step:16836 [D loss: 0.619015, acc: 66.41%] [G loss: 1.984763]\n",
      "epoch:17 step:16837 [D loss: 0.652828, acc: 65.62%] [G loss: 1.942886]\n",
      "epoch:17 step:16838 [D loss: 0.632597, acc: 71.88%] [G loss: 2.048399]\n",
      "epoch:17 step:16839 [D loss: 0.628369, acc: 64.06%] [G loss: 1.879553]\n",
      "epoch:17 step:16840 [D loss: 0.597114, acc: 65.62%] [G loss: 2.163385]\n",
      "epoch:17 step:16841 [D loss: 0.623727, acc: 70.31%] [G loss: 2.210863]\n",
      "epoch:17 step:16842 [D loss: 0.685043, acc: 55.47%] [G loss: 1.976410]\n",
      "epoch:17 step:16843 [D loss: 0.700736, acc: 58.59%] [G loss: 1.933846]\n",
      "epoch:17 step:16844 [D loss: 0.640966, acc: 64.84%] [G loss: 1.937637]\n",
      "epoch:17 step:16845 [D loss: 0.647998, acc: 64.06%] [G loss: 2.066473]\n",
      "epoch:17 step:16846 [D loss: 0.754532, acc: 53.91%] [G loss: 1.955557]\n",
      "epoch:17 step:16847 [D loss: 0.584978, acc: 70.31%] [G loss: 1.995376]\n",
      "epoch:17 step:16848 [D loss: 0.644060, acc: 67.97%] [G loss: 2.040490]\n",
      "epoch:17 step:16849 [D loss: 0.724454, acc: 53.12%] [G loss: 1.920515]\n",
      "epoch:17 step:16850 [D loss: 0.625244, acc: 60.94%] [G loss: 2.040802]\n",
      "epoch:17 step:16851 [D loss: 0.579143, acc: 70.31%] [G loss: 2.014660]\n",
      "epoch:17 step:16852 [D loss: 0.617074, acc: 70.31%] [G loss: 2.033368]\n",
      "epoch:17 step:16853 [D loss: 0.598895, acc: 70.31%] [G loss: 2.235970]\n",
      "epoch:17 step:16854 [D loss: 0.638104, acc: 61.72%] [G loss: 2.150734]\n",
      "epoch:17 step:16855 [D loss: 0.552244, acc: 75.00%] [G loss: 2.129029]\n",
      "epoch:17 step:16856 [D loss: 0.606256, acc: 73.44%] [G loss: 2.059253]\n",
      "epoch:17 step:16857 [D loss: 0.790819, acc: 46.09%] [G loss: 1.787589]\n",
      "epoch:17 step:16858 [D loss: 0.741161, acc: 50.78%] [G loss: 1.862254]\n",
      "epoch:17 step:16859 [D loss: 0.598224, acc: 67.19%] [G loss: 2.023498]\n",
      "epoch:17 step:16860 [D loss: 0.671160, acc: 60.16%] [G loss: 2.038824]\n",
      "epoch:17 step:16861 [D loss: 0.643825, acc: 66.41%] [G loss: 1.979936]\n",
      "epoch:17 step:16862 [D loss: 0.632405, acc: 66.41%] [G loss: 1.937639]\n",
      "epoch:17 step:16863 [D loss: 0.633140, acc: 58.59%] [G loss: 2.075134]\n",
      "epoch:17 step:16864 [D loss: 0.621635, acc: 66.41%] [G loss: 1.887238]\n",
      "epoch:17 step:16865 [D loss: 0.619295, acc: 64.84%] [G loss: 2.108305]\n",
      "epoch:17 step:16866 [D loss: 0.552595, acc: 73.44%] [G loss: 2.330854]\n",
      "epoch:18 step:16867 [D loss: 0.731854, acc: 56.25%] [G loss: 1.858620]\n",
      "epoch:18 step:16868 [D loss: 0.651219, acc: 60.94%] [G loss: 1.933642]\n",
      "epoch:18 step:16869 [D loss: 0.647979, acc: 60.94%] [G loss: 2.011197]\n",
      "epoch:18 step:16870 [D loss: 0.643564, acc: 65.62%] [G loss: 1.999552]\n",
      "epoch:18 step:16871 [D loss: 0.626144, acc: 65.62%] [G loss: 1.867044]\n",
      "epoch:18 step:16872 [D loss: 0.586096, acc: 69.53%] [G loss: 2.004562]\n",
      "epoch:18 step:16873 [D loss: 0.644735, acc: 61.72%] [G loss: 2.059600]\n",
      "epoch:18 step:16874 [D loss: 0.633183, acc: 65.62%] [G loss: 1.965419]\n",
      "epoch:18 step:16875 [D loss: 0.626070, acc: 60.94%] [G loss: 2.279366]\n",
      "epoch:18 step:16876 [D loss: 0.596795, acc: 72.66%] [G loss: 2.201977]\n",
      "epoch:18 step:16877 [D loss: 0.592173, acc: 70.31%] [G loss: 2.007256]\n",
      "epoch:18 step:16878 [D loss: 0.727406, acc: 53.12%] [G loss: 1.971614]\n",
      "epoch:18 step:16879 [D loss: 0.627672, acc: 64.06%] [G loss: 1.903382]\n",
      "epoch:18 step:16880 [D loss: 0.687401, acc: 57.81%] [G loss: 1.965178]\n",
      "epoch:18 step:16881 [D loss: 0.605801, acc: 68.75%] [G loss: 2.119071]\n",
      "epoch:18 step:16882 [D loss: 0.620304, acc: 64.06%] [G loss: 2.020957]\n",
      "epoch:18 step:16883 [D loss: 0.643100, acc: 60.94%] [G loss: 1.918068]\n",
      "epoch:18 step:16884 [D loss: 0.660571, acc: 64.06%] [G loss: 1.947084]\n",
      "epoch:18 step:16885 [D loss: 0.611247, acc: 72.66%] [G loss: 1.894075]\n",
      "epoch:18 step:16886 [D loss: 0.711565, acc: 50.78%] [G loss: 1.694533]\n",
      "epoch:18 step:16887 [D loss: 0.683814, acc: 59.38%] [G loss: 1.632801]\n",
      "epoch:18 step:16888 [D loss: 0.625536, acc: 69.53%] [G loss: 1.841173]\n",
      "epoch:18 step:16889 [D loss: 0.634765, acc: 64.06%] [G loss: 1.927818]\n",
      "epoch:18 step:16890 [D loss: 0.625742, acc: 64.84%] [G loss: 1.952886]\n",
      "epoch:18 step:16891 [D loss: 0.616788, acc: 64.06%] [G loss: 2.028802]\n",
      "epoch:18 step:16892 [D loss: 0.627538, acc: 65.62%] [G loss: 1.782580]\n",
      "epoch:18 step:16893 [D loss: 0.690499, acc: 59.38%] [G loss: 1.792874]\n",
      "epoch:18 step:16894 [D loss: 0.635187, acc: 59.38%] [G loss: 1.883010]\n",
      "epoch:18 step:16895 [D loss: 0.618791, acc: 65.62%] [G loss: 2.080151]\n",
      "epoch:18 step:16896 [D loss: 0.640860, acc: 64.84%] [G loss: 1.921601]\n",
      "epoch:18 step:16897 [D loss: 0.701521, acc: 54.69%] [G loss: 1.741565]\n",
      "epoch:18 step:16898 [D loss: 0.655413, acc: 59.38%] [G loss: 1.733751]\n",
      "epoch:18 step:16899 [D loss: 0.625952, acc: 62.50%] [G loss: 1.919809]\n",
      "epoch:18 step:16900 [D loss: 0.644989, acc: 60.94%] [G loss: 1.809361]\n",
      "epoch:18 step:16901 [D loss: 0.643238, acc: 63.28%] [G loss: 1.924483]\n",
      "epoch:18 step:16902 [D loss: 0.596044, acc: 67.19%] [G loss: 1.960121]\n",
      "epoch:18 step:16903 [D loss: 0.638981, acc: 60.94%] [G loss: 1.961704]\n",
      "epoch:18 step:16904 [D loss: 0.705066, acc: 59.38%] [G loss: 1.942340]\n",
      "epoch:18 step:16905 [D loss: 0.664899, acc: 61.72%] [G loss: 2.004804]\n",
      "epoch:18 step:16906 [D loss: 0.621640, acc: 67.97%] [G loss: 2.026538]\n",
      "epoch:18 step:16907 [D loss: 0.696010, acc: 56.25%] [G loss: 1.855832]\n",
      "epoch:18 step:16908 [D loss: 0.589805, acc: 70.31%] [G loss: 2.025512]\n",
      "epoch:18 step:16909 [D loss: 0.634974, acc: 67.97%] [G loss: 1.897150]\n",
      "epoch:18 step:16910 [D loss: 0.694772, acc: 60.94%] [G loss: 1.775313]\n",
      "epoch:18 step:16911 [D loss: 0.668635, acc: 62.50%] [G loss: 1.856508]\n",
      "epoch:18 step:16912 [D loss: 0.699496, acc: 60.16%] [G loss: 1.851806]\n",
      "epoch:18 step:16913 [D loss: 0.629083, acc: 60.16%] [G loss: 1.826187]\n",
      "epoch:18 step:16914 [D loss: 0.620675, acc: 65.62%] [G loss: 1.833155]\n",
      "epoch:18 step:16915 [D loss: 0.663897, acc: 59.38%] [G loss: 1.928739]\n",
      "epoch:18 step:16916 [D loss: 0.643422, acc: 64.84%] [G loss: 1.790682]\n",
      "epoch:18 step:16917 [D loss: 0.685853, acc: 62.50%] [G loss: 1.773447]\n",
      "epoch:18 step:16918 [D loss: 0.610800, acc: 71.09%] [G loss: 1.891532]\n",
      "epoch:18 step:16919 [D loss: 0.639042, acc: 67.19%] [G loss: 1.916816]\n",
      "epoch:18 step:16920 [D loss: 0.616759, acc: 67.19%] [G loss: 2.013089]\n",
      "epoch:18 step:16921 [D loss: 0.625565, acc: 67.19%] [G loss: 2.058484]\n",
      "epoch:18 step:16922 [D loss: 0.573322, acc: 78.12%] [G loss: 1.887601]\n",
      "epoch:18 step:16923 [D loss: 0.657797, acc: 63.28%] [G loss: 1.873631]\n",
      "epoch:18 step:16924 [D loss: 0.663714, acc: 55.47%] [G loss: 1.824627]\n",
      "epoch:18 step:16925 [D loss: 0.661680, acc: 60.16%] [G loss: 1.844429]\n",
      "epoch:18 step:16926 [D loss: 0.652062, acc: 60.94%] [G loss: 1.865173]\n",
      "epoch:18 step:16927 [D loss: 0.639160, acc: 61.72%] [G loss: 1.733528]\n",
      "epoch:18 step:16928 [D loss: 0.625251, acc: 61.72%] [G loss: 1.806303]\n",
      "epoch:18 step:16929 [D loss: 0.600977, acc: 70.31%] [G loss: 1.996062]\n",
      "epoch:18 step:16930 [D loss: 0.716328, acc: 57.03%] [G loss: 1.940910]\n",
      "epoch:18 step:16931 [D loss: 0.636694, acc: 67.97%] [G loss: 1.922159]\n",
      "epoch:18 step:16932 [D loss: 0.627406, acc: 64.06%] [G loss: 1.940747]\n",
      "epoch:18 step:16933 [D loss: 0.625939, acc: 63.28%] [G loss: 1.976863]\n",
      "epoch:18 step:16934 [D loss: 0.665208, acc: 63.28%] [G loss: 1.901757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16935 [D loss: 0.700096, acc: 55.47%] [G loss: 1.888131]\n",
      "epoch:18 step:16936 [D loss: 0.621260, acc: 64.06%] [G loss: 1.862359]\n",
      "epoch:18 step:16937 [D loss: 0.680026, acc: 62.50%] [G loss: 1.762347]\n",
      "epoch:18 step:16938 [D loss: 0.660945, acc: 62.50%] [G loss: 1.908414]\n",
      "epoch:18 step:16939 [D loss: 0.667177, acc: 63.28%] [G loss: 1.817832]\n",
      "epoch:18 step:16940 [D loss: 0.632116, acc: 66.41%] [G loss: 1.962483]\n",
      "epoch:18 step:16941 [D loss: 0.605072, acc: 67.19%] [G loss: 2.101838]\n",
      "epoch:18 step:16942 [D loss: 0.634239, acc: 64.84%] [G loss: 2.067085]\n",
      "epoch:18 step:16943 [D loss: 0.619736, acc: 65.62%] [G loss: 2.209252]\n",
      "epoch:18 step:16944 [D loss: 0.663923, acc: 58.59%] [G loss: 2.032764]\n",
      "epoch:18 step:16945 [D loss: 0.632601, acc: 60.94%] [G loss: 1.843838]\n",
      "epoch:18 step:16946 [D loss: 0.684600, acc: 60.16%] [G loss: 1.740193]\n",
      "epoch:18 step:16947 [D loss: 0.733380, acc: 54.69%] [G loss: 1.749533]\n",
      "epoch:18 step:16948 [D loss: 0.616076, acc: 69.53%] [G loss: 1.930835]\n",
      "epoch:18 step:16949 [D loss: 0.630684, acc: 60.16%] [G loss: 1.847374]\n",
      "epoch:18 step:16950 [D loss: 0.636283, acc: 68.75%] [G loss: 1.946333]\n",
      "epoch:18 step:16951 [D loss: 0.594095, acc: 72.66%] [G loss: 1.846147]\n",
      "epoch:18 step:16952 [D loss: 0.667418, acc: 63.28%] [G loss: 1.940683]\n",
      "epoch:18 step:16953 [D loss: 0.612179, acc: 64.84%] [G loss: 1.884160]\n",
      "epoch:18 step:16954 [D loss: 0.671601, acc: 54.69%] [G loss: 1.850962]\n",
      "epoch:18 step:16955 [D loss: 0.669630, acc: 60.16%] [G loss: 1.927456]\n",
      "epoch:18 step:16956 [D loss: 0.644968, acc: 61.72%] [G loss: 1.858813]\n",
      "epoch:18 step:16957 [D loss: 0.629639, acc: 63.28%] [G loss: 1.863886]\n",
      "epoch:18 step:16958 [D loss: 0.679633, acc: 57.81%] [G loss: 1.887352]\n",
      "epoch:18 step:16959 [D loss: 0.627140, acc: 63.28%] [G loss: 2.026220]\n",
      "epoch:18 step:16960 [D loss: 0.629330, acc: 63.28%] [G loss: 1.983241]\n",
      "epoch:18 step:16961 [D loss: 0.636341, acc: 60.94%] [G loss: 1.765813]\n",
      "epoch:18 step:16962 [D loss: 0.613601, acc: 65.62%] [G loss: 1.967882]\n",
      "epoch:18 step:16963 [D loss: 0.626561, acc: 66.41%] [G loss: 1.953328]\n",
      "epoch:18 step:16964 [D loss: 0.660773, acc: 63.28%] [G loss: 1.752814]\n",
      "epoch:18 step:16965 [D loss: 0.669413, acc: 59.38%] [G loss: 1.878986]\n",
      "epoch:18 step:16966 [D loss: 0.634804, acc: 66.41%] [G loss: 1.964323]\n",
      "epoch:18 step:16967 [D loss: 0.616253, acc: 64.06%] [G loss: 1.992814]\n",
      "epoch:18 step:16968 [D loss: 0.649459, acc: 59.38%] [G loss: 1.856416]\n",
      "epoch:18 step:16969 [D loss: 0.634534, acc: 71.09%] [G loss: 2.034989]\n",
      "epoch:18 step:16970 [D loss: 0.680702, acc: 56.25%] [G loss: 1.810834]\n",
      "epoch:18 step:16971 [D loss: 0.640645, acc: 60.94%] [G loss: 1.805017]\n",
      "epoch:18 step:16972 [D loss: 0.620152, acc: 67.19%] [G loss: 2.070890]\n",
      "epoch:18 step:16973 [D loss: 0.604718, acc: 67.97%] [G loss: 1.983170]\n",
      "epoch:18 step:16974 [D loss: 0.700691, acc: 53.12%] [G loss: 1.812265]\n",
      "epoch:18 step:16975 [D loss: 0.648842, acc: 61.72%] [G loss: 1.862922]\n",
      "epoch:18 step:16976 [D loss: 0.647945, acc: 60.94%] [G loss: 1.818240]\n",
      "epoch:18 step:16977 [D loss: 0.643837, acc: 66.41%] [G loss: 1.903232]\n",
      "epoch:18 step:16978 [D loss: 0.627213, acc: 62.50%] [G loss: 1.891290]\n",
      "epoch:18 step:16979 [D loss: 0.657385, acc: 64.84%] [G loss: 1.920996]\n",
      "epoch:18 step:16980 [D loss: 0.594505, acc: 67.97%] [G loss: 2.099816]\n",
      "epoch:18 step:16981 [D loss: 0.600432, acc: 71.09%] [G loss: 2.106635]\n",
      "epoch:18 step:16982 [D loss: 0.622353, acc: 63.28%] [G loss: 2.146104]\n",
      "epoch:18 step:16983 [D loss: 0.594798, acc: 69.53%] [G loss: 2.191446]\n",
      "epoch:18 step:16984 [D loss: 0.647257, acc: 64.84%] [G loss: 1.969306]\n",
      "epoch:18 step:16985 [D loss: 0.623622, acc: 59.38%] [G loss: 2.303219]\n",
      "epoch:18 step:16986 [D loss: 0.649586, acc: 63.28%] [G loss: 1.952450]\n",
      "epoch:18 step:16987 [D loss: 0.652293, acc: 62.50%] [G loss: 1.913842]\n",
      "epoch:18 step:16988 [D loss: 0.648199, acc: 65.62%] [G loss: 2.087406]\n",
      "epoch:18 step:16989 [D loss: 0.634582, acc: 67.97%] [G loss: 2.124066]\n",
      "epoch:18 step:16990 [D loss: 0.706481, acc: 52.34%] [G loss: 1.962809]\n",
      "epoch:18 step:16991 [D loss: 0.700179, acc: 58.59%] [G loss: 1.737626]\n",
      "epoch:18 step:16992 [D loss: 0.638501, acc: 65.62%] [G loss: 1.862953]\n",
      "epoch:18 step:16993 [D loss: 0.687795, acc: 54.69%] [G loss: 1.781906]\n",
      "epoch:18 step:16994 [D loss: 0.615004, acc: 63.28%] [G loss: 1.701501]\n",
      "epoch:18 step:16995 [D loss: 0.628194, acc: 65.62%] [G loss: 1.731746]\n",
      "epoch:18 step:16996 [D loss: 0.547587, acc: 74.22%] [G loss: 1.969023]\n",
      "epoch:18 step:16997 [D loss: 0.623095, acc: 64.06%] [G loss: 2.036371]\n",
      "epoch:18 step:16998 [D loss: 0.629011, acc: 63.28%] [G loss: 1.942224]\n",
      "epoch:18 step:16999 [D loss: 0.702245, acc: 58.59%] [G loss: 1.749881]\n",
      "epoch:18 step:17000 [D loss: 0.704930, acc: 57.03%] [G loss: 1.769850]\n",
      "##############\n",
      "[2.51540175 1.61571263 6.05190499 4.78816812 3.55329559 5.8948382\n",
      " 4.49291907 4.59322822 4.47698938 3.65128134]\n",
      "##########\n",
      "epoch:18 step:17001 [D loss: 0.681183, acc: 62.50%] [G loss: 1.748769]\n",
      "epoch:18 step:17002 [D loss: 0.665570, acc: 59.38%] [G loss: 1.827870]\n",
      "epoch:18 step:17003 [D loss: 0.613946, acc: 65.62%] [G loss: 1.756038]\n",
      "epoch:18 step:17004 [D loss: 0.616090, acc: 67.19%] [G loss: 1.712088]\n",
      "epoch:18 step:17005 [D loss: 0.647460, acc: 59.38%] [G loss: 1.887168]\n",
      "epoch:18 step:17006 [D loss: 0.642933, acc: 60.94%] [G loss: 1.792690]\n",
      "epoch:18 step:17007 [D loss: 0.674708, acc: 62.50%] [G loss: 1.859918]\n",
      "epoch:18 step:17008 [D loss: 0.603740, acc: 71.09%] [G loss: 1.921571]\n",
      "epoch:18 step:17009 [D loss: 0.711632, acc: 53.12%] [G loss: 1.729954]\n",
      "epoch:18 step:17010 [D loss: 0.624550, acc: 64.84%] [G loss: 1.786346]\n",
      "epoch:18 step:17011 [D loss: 0.662556, acc: 61.72%] [G loss: 1.767235]\n",
      "epoch:18 step:17012 [D loss: 0.659020, acc: 59.38%] [G loss: 1.862564]\n",
      "epoch:18 step:17013 [D loss: 0.654295, acc: 60.94%] [G loss: 1.760493]\n",
      "epoch:18 step:17014 [D loss: 0.679100, acc: 58.59%] [G loss: 1.909086]\n",
      "epoch:18 step:17015 [D loss: 0.630263, acc: 69.53%] [G loss: 1.952235]\n",
      "epoch:18 step:17016 [D loss: 0.637149, acc: 64.06%] [G loss: 1.935532]\n",
      "epoch:18 step:17017 [D loss: 0.666909, acc: 58.59%] [G loss: 2.093550]\n",
      "epoch:18 step:17018 [D loss: 0.604731, acc: 64.84%] [G loss: 1.991553]\n",
      "epoch:18 step:17019 [D loss: 0.671358, acc: 57.81%] [G loss: 1.794295]\n",
      "epoch:18 step:17020 [D loss: 0.615350, acc: 64.06%] [G loss: 1.865299]\n",
      "epoch:18 step:17021 [D loss: 0.569372, acc: 75.00%] [G loss: 2.052270]\n",
      "epoch:18 step:17022 [D loss: 0.643779, acc: 60.16%] [G loss: 1.945523]\n",
      "epoch:18 step:17023 [D loss: 0.608296, acc: 64.06%] [G loss: 1.896580]\n",
      "epoch:18 step:17024 [D loss: 0.731313, acc: 58.59%] [G loss: 1.857460]\n",
      "epoch:18 step:17025 [D loss: 0.609970, acc: 67.19%] [G loss: 1.750949]\n",
      "epoch:18 step:17026 [D loss: 0.682009, acc: 58.59%] [G loss: 1.864877]\n",
      "epoch:18 step:17027 [D loss: 0.658504, acc: 61.72%] [G loss: 1.905565]\n",
      "epoch:18 step:17028 [D loss: 0.631394, acc: 68.75%] [G loss: 1.991394]\n",
      "epoch:18 step:17029 [D loss: 0.664057, acc: 63.28%] [G loss: 1.824609]\n",
      "epoch:18 step:17030 [D loss: 0.700163, acc: 54.69%] [G loss: 1.852007]\n",
      "epoch:18 step:17031 [D loss: 0.661953, acc: 60.94%] [G loss: 1.959466]\n",
      "epoch:18 step:17032 [D loss: 0.677791, acc: 51.56%] [G loss: 1.859732]\n",
      "epoch:18 step:17033 [D loss: 0.664611, acc: 58.59%] [G loss: 1.921028]\n",
      "epoch:18 step:17034 [D loss: 0.669242, acc: 60.16%] [G loss: 1.966148]\n",
      "epoch:18 step:17035 [D loss: 0.656920, acc: 60.16%] [G loss: 1.905267]\n",
      "epoch:18 step:17036 [D loss: 0.624939, acc: 67.19%] [G loss: 1.839860]\n",
      "epoch:18 step:17037 [D loss: 0.653375, acc: 58.59%] [G loss: 1.897263]\n",
      "epoch:18 step:17038 [D loss: 0.642878, acc: 60.94%] [G loss: 1.831565]\n",
      "epoch:18 step:17039 [D loss: 0.691286, acc: 54.69%] [G loss: 1.878834]\n",
      "epoch:18 step:17040 [D loss: 0.642073, acc: 60.94%] [G loss: 1.724052]\n",
      "epoch:18 step:17041 [D loss: 0.660658, acc: 62.50%] [G loss: 1.744817]\n",
      "epoch:18 step:17042 [D loss: 0.670084, acc: 56.25%] [G loss: 1.762007]\n",
      "epoch:18 step:17043 [D loss: 0.656735, acc: 64.84%] [G loss: 1.944663]\n",
      "epoch:18 step:17044 [D loss: 0.658257, acc: 62.50%] [G loss: 1.862881]\n",
      "epoch:18 step:17045 [D loss: 0.616509, acc: 64.06%] [G loss: 1.857724]\n",
      "epoch:18 step:17046 [D loss: 0.682238, acc: 57.03%] [G loss: 1.822438]\n",
      "epoch:18 step:17047 [D loss: 0.666424, acc: 59.38%] [G loss: 1.872085]\n",
      "epoch:18 step:17048 [D loss: 0.649359, acc: 60.94%] [G loss: 1.770175]\n",
      "epoch:18 step:17049 [D loss: 0.691388, acc: 55.47%] [G loss: 1.827727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17050 [D loss: 0.630049, acc: 69.53%] [G loss: 1.895474]\n",
      "epoch:18 step:17051 [D loss: 0.712426, acc: 58.59%] [G loss: 1.849029]\n",
      "epoch:18 step:17052 [D loss: 0.688644, acc: 53.91%] [G loss: 1.737958]\n",
      "epoch:18 step:17053 [D loss: 0.661333, acc: 60.94%] [G loss: 1.973457]\n",
      "epoch:18 step:17054 [D loss: 0.648408, acc: 61.72%] [G loss: 1.870718]\n",
      "epoch:18 step:17055 [D loss: 0.626957, acc: 65.62%] [G loss: 1.834094]\n",
      "epoch:18 step:17056 [D loss: 0.631756, acc: 64.84%] [G loss: 1.885178]\n",
      "epoch:18 step:17057 [D loss: 0.633962, acc: 67.19%] [G loss: 1.723863]\n",
      "epoch:18 step:17058 [D loss: 0.640632, acc: 66.41%] [G loss: 1.980031]\n",
      "epoch:18 step:17059 [D loss: 0.663314, acc: 60.94%] [G loss: 1.930386]\n",
      "epoch:18 step:17060 [D loss: 0.572739, acc: 68.75%] [G loss: 1.986148]\n",
      "epoch:18 step:17061 [D loss: 0.640233, acc: 67.97%] [G loss: 1.939719]\n",
      "epoch:18 step:17062 [D loss: 0.684762, acc: 60.16%] [G loss: 1.880909]\n",
      "epoch:18 step:17063 [D loss: 0.608843, acc: 65.62%] [G loss: 2.023784]\n",
      "epoch:18 step:17064 [D loss: 0.622244, acc: 64.06%] [G loss: 1.986342]\n",
      "epoch:18 step:17065 [D loss: 0.638541, acc: 59.38%] [G loss: 1.981810]\n",
      "epoch:18 step:17066 [D loss: 0.673807, acc: 62.50%] [G loss: 1.747692]\n",
      "epoch:18 step:17067 [D loss: 0.650262, acc: 61.72%] [G loss: 1.830066]\n",
      "epoch:18 step:17068 [D loss: 0.694873, acc: 52.34%] [G loss: 1.903656]\n",
      "epoch:18 step:17069 [D loss: 0.644997, acc: 65.62%] [G loss: 1.922665]\n",
      "epoch:18 step:17070 [D loss: 0.625779, acc: 70.31%] [G loss: 1.855327]\n",
      "epoch:18 step:17071 [D loss: 0.608846, acc: 64.84%] [G loss: 1.855634]\n",
      "epoch:18 step:17072 [D loss: 0.622183, acc: 67.19%] [G loss: 1.953620]\n",
      "epoch:18 step:17073 [D loss: 0.583045, acc: 71.09%] [G loss: 2.212832]\n",
      "epoch:18 step:17074 [D loss: 0.606920, acc: 67.19%] [G loss: 2.092339]\n",
      "epoch:18 step:17075 [D loss: 0.608617, acc: 67.19%] [G loss: 2.022995]\n",
      "epoch:18 step:17076 [D loss: 0.648815, acc: 61.72%] [G loss: 1.895594]\n",
      "epoch:18 step:17077 [D loss: 0.647068, acc: 63.28%] [G loss: 1.886782]\n",
      "epoch:18 step:17078 [D loss: 0.655603, acc: 64.84%] [G loss: 1.816799]\n",
      "epoch:18 step:17079 [D loss: 0.674267, acc: 60.16%] [G loss: 1.845524]\n",
      "epoch:18 step:17080 [D loss: 0.597807, acc: 69.53%] [G loss: 1.796345]\n",
      "epoch:18 step:17081 [D loss: 0.688629, acc: 54.69%] [G loss: 1.873851]\n",
      "epoch:18 step:17082 [D loss: 0.651873, acc: 60.16%] [G loss: 1.989028]\n",
      "epoch:18 step:17083 [D loss: 0.593981, acc: 69.53%] [G loss: 2.063848]\n",
      "epoch:18 step:17084 [D loss: 0.607797, acc: 67.19%] [G loss: 1.881745]\n",
      "epoch:18 step:17085 [D loss: 0.621561, acc: 62.50%] [G loss: 2.055136]\n",
      "epoch:18 step:17086 [D loss: 0.705528, acc: 50.78%] [G loss: 1.705871]\n",
      "epoch:18 step:17087 [D loss: 0.634328, acc: 63.28%] [G loss: 1.941646]\n",
      "epoch:18 step:17088 [D loss: 0.699499, acc: 51.56%] [G loss: 1.849504]\n",
      "epoch:18 step:17089 [D loss: 0.624075, acc: 60.94%] [G loss: 1.864422]\n",
      "epoch:18 step:17090 [D loss: 0.627181, acc: 61.72%] [G loss: 1.827966]\n",
      "epoch:18 step:17091 [D loss: 0.654042, acc: 61.72%] [G loss: 1.855965]\n",
      "epoch:18 step:17092 [D loss: 0.635725, acc: 66.41%] [G loss: 1.819830]\n",
      "epoch:18 step:17093 [D loss: 0.703995, acc: 55.47%] [G loss: 1.813013]\n",
      "epoch:18 step:17094 [D loss: 0.643770, acc: 62.50%] [G loss: 1.802105]\n",
      "epoch:18 step:17095 [D loss: 0.621732, acc: 68.75%] [G loss: 2.026159]\n",
      "epoch:18 step:17096 [D loss: 0.588812, acc: 67.97%] [G loss: 2.127632]\n",
      "epoch:18 step:17097 [D loss: 0.553662, acc: 72.66%] [G loss: 2.276210]\n",
      "epoch:18 step:17098 [D loss: 0.557542, acc: 74.22%] [G loss: 2.254472]\n",
      "epoch:18 step:17099 [D loss: 0.634529, acc: 64.06%] [G loss: 1.804149]\n",
      "epoch:18 step:17100 [D loss: 0.662398, acc: 55.47%] [G loss: 1.971493]\n",
      "epoch:18 step:17101 [D loss: 0.731622, acc: 57.03%] [G loss: 1.719625]\n",
      "epoch:18 step:17102 [D loss: 0.643320, acc: 61.72%] [G loss: 1.942025]\n",
      "epoch:18 step:17103 [D loss: 0.641546, acc: 58.59%] [G loss: 1.974262]\n",
      "epoch:18 step:17104 [D loss: 0.659301, acc: 57.81%] [G loss: 1.969497]\n",
      "epoch:18 step:17105 [D loss: 0.604552, acc: 67.19%] [G loss: 1.836906]\n",
      "epoch:18 step:17106 [D loss: 0.633454, acc: 59.38%] [G loss: 1.876346]\n",
      "epoch:18 step:17107 [D loss: 0.631531, acc: 65.62%] [G loss: 1.808623]\n",
      "epoch:18 step:17108 [D loss: 0.598495, acc: 68.75%] [G loss: 2.056462]\n",
      "epoch:18 step:17109 [D loss: 0.624526, acc: 64.06%] [G loss: 1.805315]\n",
      "epoch:18 step:17110 [D loss: 0.617892, acc: 64.84%] [G loss: 1.943949]\n",
      "epoch:18 step:17111 [D loss: 0.585954, acc: 68.75%] [G loss: 1.936188]\n",
      "epoch:18 step:17112 [D loss: 0.610305, acc: 63.28%] [G loss: 1.991585]\n",
      "epoch:18 step:17113 [D loss: 0.623346, acc: 63.28%] [G loss: 2.161318]\n",
      "epoch:18 step:17114 [D loss: 0.614982, acc: 69.53%] [G loss: 2.160777]\n",
      "epoch:18 step:17115 [D loss: 0.692516, acc: 53.91%] [G loss: 1.825360]\n",
      "epoch:18 step:17116 [D loss: 0.691578, acc: 59.38%] [G loss: 1.834153]\n",
      "epoch:18 step:17117 [D loss: 0.683183, acc: 58.59%] [G loss: 1.754868]\n",
      "epoch:18 step:17118 [D loss: 0.668971, acc: 58.59%] [G loss: 1.800035]\n",
      "epoch:18 step:17119 [D loss: 0.662765, acc: 56.25%] [G loss: 1.886198]\n",
      "epoch:18 step:17120 [D loss: 0.698116, acc: 60.94%] [G loss: 1.835041]\n",
      "epoch:18 step:17121 [D loss: 0.647772, acc: 64.84%] [G loss: 1.772673]\n",
      "epoch:18 step:17122 [D loss: 0.629319, acc: 64.84%] [G loss: 2.021234]\n",
      "epoch:18 step:17123 [D loss: 0.659805, acc: 63.28%] [G loss: 1.764012]\n",
      "epoch:18 step:17124 [D loss: 0.650345, acc: 65.62%] [G loss: 1.898861]\n",
      "epoch:18 step:17125 [D loss: 0.680170, acc: 59.38%] [G loss: 1.999711]\n",
      "epoch:18 step:17126 [D loss: 0.587044, acc: 69.53%] [G loss: 1.946436]\n",
      "epoch:18 step:17127 [D loss: 0.656610, acc: 59.38%] [G loss: 1.868780]\n",
      "epoch:18 step:17128 [D loss: 0.612902, acc: 68.75%] [G loss: 2.012056]\n",
      "epoch:18 step:17129 [D loss: 0.649489, acc: 60.94%] [G loss: 2.049073]\n",
      "epoch:18 step:17130 [D loss: 0.622170, acc: 63.28%] [G loss: 2.117027]\n",
      "epoch:18 step:17131 [D loss: 0.662763, acc: 60.94%] [G loss: 1.970222]\n",
      "epoch:18 step:17132 [D loss: 0.601162, acc: 69.53%] [G loss: 1.877067]\n",
      "epoch:18 step:17133 [D loss: 0.647632, acc: 62.50%] [G loss: 1.883660]\n",
      "epoch:18 step:17134 [D loss: 0.609547, acc: 70.31%] [G loss: 1.987837]\n",
      "epoch:18 step:17135 [D loss: 0.707494, acc: 60.94%] [G loss: 1.917691]\n",
      "epoch:18 step:17136 [D loss: 0.610689, acc: 67.97%] [G loss: 1.909384]\n",
      "epoch:18 step:17137 [D loss: 0.624654, acc: 67.19%] [G loss: 2.075515]\n",
      "epoch:18 step:17138 [D loss: 0.575232, acc: 74.22%] [G loss: 1.929375]\n",
      "epoch:18 step:17139 [D loss: 0.629809, acc: 65.62%] [G loss: 2.036402]\n",
      "epoch:18 step:17140 [D loss: 0.613044, acc: 66.41%] [G loss: 2.045372]\n",
      "epoch:18 step:17141 [D loss: 0.624160, acc: 67.19%] [G loss: 2.025392]\n",
      "epoch:18 step:17142 [D loss: 0.599324, acc: 70.31%] [G loss: 2.247469]\n",
      "epoch:18 step:17143 [D loss: 0.621695, acc: 64.06%] [G loss: 1.864639]\n",
      "epoch:18 step:17144 [D loss: 0.604576, acc: 63.28%] [G loss: 1.933048]\n",
      "epoch:18 step:17145 [D loss: 0.608391, acc: 67.97%] [G loss: 1.986683]\n",
      "epoch:18 step:17146 [D loss: 0.637967, acc: 61.72%] [G loss: 1.863696]\n",
      "epoch:18 step:17147 [D loss: 0.667560, acc: 61.72%] [G loss: 2.009379]\n",
      "epoch:18 step:17148 [D loss: 0.721857, acc: 53.12%] [G loss: 1.842450]\n",
      "epoch:18 step:17149 [D loss: 0.686431, acc: 59.38%] [G loss: 2.049980]\n",
      "epoch:18 step:17150 [D loss: 0.694483, acc: 53.91%] [G loss: 1.750271]\n",
      "epoch:18 step:17151 [D loss: 0.646410, acc: 59.38%] [G loss: 1.890419]\n",
      "epoch:18 step:17152 [D loss: 0.644436, acc: 64.84%] [G loss: 1.848579]\n",
      "epoch:18 step:17153 [D loss: 0.676850, acc: 58.59%] [G loss: 1.916693]\n",
      "epoch:18 step:17154 [D loss: 0.638204, acc: 67.97%] [G loss: 2.025372]\n",
      "epoch:18 step:17155 [D loss: 0.683304, acc: 56.25%] [G loss: 1.873505]\n",
      "epoch:18 step:17156 [D loss: 0.657683, acc: 60.16%] [G loss: 1.923563]\n",
      "epoch:18 step:17157 [D loss: 0.589690, acc: 67.19%] [G loss: 1.944621]\n",
      "epoch:18 step:17158 [D loss: 0.613343, acc: 67.97%] [G loss: 1.864437]\n",
      "epoch:18 step:17159 [D loss: 0.617582, acc: 65.62%] [G loss: 1.975211]\n",
      "epoch:18 step:17160 [D loss: 0.653618, acc: 60.94%] [G loss: 1.933606]\n",
      "epoch:18 step:17161 [D loss: 0.645356, acc: 59.38%] [G loss: 1.935552]\n",
      "epoch:18 step:17162 [D loss: 0.623818, acc: 63.28%] [G loss: 2.095442]\n",
      "epoch:18 step:17163 [D loss: 0.617073, acc: 64.06%] [G loss: 1.959650]\n",
      "epoch:18 step:17164 [D loss: 0.621990, acc: 66.41%] [G loss: 2.019818]\n",
      "epoch:18 step:17165 [D loss: 0.633047, acc: 62.50%] [G loss: 2.056737]\n",
      "epoch:18 step:17166 [D loss: 0.653771, acc: 65.62%] [G loss: 1.955638]\n",
      "epoch:18 step:17167 [D loss: 0.678451, acc: 62.50%] [G loss: 1.840526]\n",
      "epoch:18 step:17168 [D loss: 0.677185, acc: 60.16%] [G loss: 2.068650]\n",
      "epoch:18 step:17169 [D loss: 0.654697, acc: 64.84%] [G loss: 2.014964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17170 [D loss: 0.696387, acc: 53.12%] [G loss: 1.848177]\n",
      "epoch:18 step:17171 [D loss: 0.671006, acc: 60.94%] [G loss: 1.828160]\n",
      "epoch:18 step:17172 [D loss: 0.621625, acc: 64.84%] [G loss: 1.897842]\n",
      "epoch:18 step:17173 [D loss: 0.667620, acc: 58.59%] [G loss: 1.901431]\n",
      "epoch:18 step:17174 [D loss: 0.646096, acc: 60.16%] [G loss: 1.800927]\n",
      "epoch:18 step:17175 [D loss: 0.621179, acc: 66.41%] [G loss: 1.876877]\n",
      "epoch:18 step:17176 [D loss: 0.586705, acc: 70.31%] [G loss: 1.796022]\n",
      "epoch:18 step:17177 [D loss: 0.677865, acc: 55.47%] [G loss: 1.744034]\n",
      "epoch:18 step:17178 [D loss: 0.572762, acc: 74.22%] [G loss: 2.080662]\n",
      "epoch:18 step:17179 [D loss: 0.602682, acc: 71.09%] [G loss: 2.040894]\n",
      "epoch:18 step:17180 [D loss: 0.555020, acc: 71.88%] [G loss: 2.123970]\n",
      "epoch:18 step:17181 [D loss: 0.674444, acc: 64.84%] [G loss: 2.136173]\n",
      "epoch:18 step:17182 [D loss: 0.699338, acc: 55.47%] [G loss: 1.778151]\n",
      "epoch:18 step:17183 [D loss: 0.701421, acc: 60.16%] [G loss: 1.856075]\n",
      "epoch:18 step:17184 [D loss: 0.620973, acc: 66.41%] [G loss: 1.975961]\n",
      "epoch:18 step:17185 [D loss: 0.621075, acc: 67.19%] [G loss: 1.825082]\n",
      "epoch:18 step:17186 [D loss: 0.649259, acc: 60.94%] [G loss: 1.867988]\n",
      "epoch:18 step:17187 [D loss: 0.688276, acc: 53.91%] [G loss: 2.143497]\n",
      "epoch:18 step:17188 [D loss: 0.655150, acc: 64.06%] [G loss: 1.800777]\n",
      "epoch:18 step:17189 [D loss: 0.684582, acc: 60.16%] [G loss: 1.837834]\n",
      "epoch:18 step:17190 [D loss: 0.666828, acc: 68.75%] [G loss: 1.789642]\n",
      "epoch:18 step:17191 [D loss: 0.646877, acc: 57.81%] [G loss: 1.910327]\n",
      "epoch:18 step:17192 [D loss: 0.619251, acc: 64.06%] [G loss: 1.877994]\n",
      "epoch:18 step:17193 [D loss: 0.672256, acc: 57.81%] [G loss: 1.798210]\n",
      "epoch:18 step:17194 [D loss: 0.625271, acc: 64.84%] [G loss: 1.881758]\n",
      "epoch:18 step:17195 [D loss: 0.671536, acc: 59.38%] [G loss: 1.844605]\n",
      "epoch:18 step:17196 [D loss: 0.610159, acc: 64.06%] [G loss: 1.893203]\n",
      "epoch:18 step:17197 [D loss: 0.602200, acc: 65.62%] [G loss: 1.973616]\n",
      "epoch:18 step:17198 [D loss: 0.603175, acc: 61.72%] [G loss: 2.128269]\n",
      "epoch:18 step:17199 [D loss: 0.663836, acc: 57.03%] [G loss: 1.942080]\n",
      "epoch:18 step:17200 [D loss: 0.696845, acc: 53.91%] [G loss: 1.851529]\n",
      "##############\n",
      "[2.54311655 1.38255555 6.23393085 4.72855397 3.91984773 5.62593012\n",
      " 4.20565902 4.84039413 4.64320324 3.65366534]\n",
      "##########\n",
      "epoch:18 step:17201 [D loss: 0.591744, acc: 67.97%] [G loss: 2.013919]\n",
      "epoch:18 step:17202 [D loss: 0.638482, acc: 66.41%] [G loss: 2.082479]\n",
      "epoch:18 step:17203 [D loss: 0.616246, acc: 63.28%] [G loss: 1.969637]\n",
      "epoch:18 step:17204 [D loss: 0.637159, acc: 67.19%] [G loss: 2.052203]\n",
      "epoch:18 step:17205 [D loss: 0.586034, acc: 68.75%] [G loss: 2.052245]\n",
      "epoch:18 step:17206 [D loss: 0.651589, acc: 64.06%] [G loss: 1.859100]\n",
      "epoch:18 step:17207 [D loss: 0.694067, acc: 57.03%] [G loss: 1.899376]\n",
      "epoch:18 step:17208 [D loss: 0.663134, acc: 64.84%] [G loss: 1.901719]\n",
      "epoch:18 step:17209 [D loss: 0.690122, acc: 59.38%] [G loss: 1.928859]\n",
      "epoch:18 step:17210 [D loss: 0.600871, acc: 68.75%] [G loss: 2.075779]\n",
      "epoch:18 step:17211 [D loss: 0.645034, acc: 63.28%] [G loss: 2.190606]\n",
      "epoch:18 step:17212 [D loss: 0.651356, acc: 62.50%] [G loss: 2.037912]\n",
      "epoch:18 step:17213 [D loss: 0.563162, acc: 71.09%] [G loss: 2.404734]\n",
      "epoch:18 step:17214 [D loss: 0.674223, acc: 58.59%] [G loss: 1.940995]\n",
      "epoch:18 step:17215 [D loss: 0.667605, acc: 57.03%] [G loss: 1.896451]\n",
      "epoch:18 step:17216 [D loss: 0.628734, acc: 64.06%] [G loss: 1.874573]\n",
      "epoch:18 step:17217 [D loss: 0.706446, acc: 57.03%] [G loss: 1.855044]\n",
      "epoch:18 step:17218 [D loss: 0.701512, acc: 60.94%] [G loss: 1.820925]\n",
      "epoch:18 step:17219 [D loss: 0.688424, acc: 61.72%] [G loss: 1.856958]\n",
      "epoch:18 step:17220 [D loss: 0.587469, acc: 65.62%] [G loss: 2.122782]\n",
      "epoch:18 step:17221 [D loss: 0.719920, acc: 58.59%] [G loss: 1.922642]\n",
      "epoch:18 step:17222 [D loss: 0.663072, acc: 59.38%] [G loss: 1.975172]\n",
      "epoch:18 step:17223 [D loss: 0.628057, acc: 61.72%] [G loss: 1.981883]\n",
      "epoch:18 step:17224 [D loss: 0.603614, acc: 70.31%] [G loss: 2.099145]\n",
      "epoch:18 step:17225 [D loss: 0.614963, acc: 61.72%] [G loss: 1.978011]\n",
      "epoch:18 step:17226 [D loss: 0.610871, acc: 65.62%] [G loss: 2.192867]\n",
      "epoch:18 step:17227 [D loss: 0.618400, acc: 63.28%] [G loss: 1.997712]\n",
      "epoch:18 step:17228 [D loss: 0.702757, acc: 55.47%] [G loss: 1.956467]\n",
      "epoch:18 step:17229 [D loss: 0.652752, acc: 56.25%] [G loss: 2.016723]\n",
      "epoch:18 step:17230 [D loss: 0.626918, acc: 61.72%] [G loss: 1.972815]\n",
      "epoch:18 step:17231 [D loss: 0.680048, acc: 63.28%] [G loss: 1.892905]\n",
      "epoch:18 step:17232 [D loss: 0.612317, acc: 65.62%] [G loss: 1.847956]\n",
      "epoch:18 step:17233 [D loss: 0.649192, acc: 59.38%] [G loss: 2.057811]\n",
      "epoch:18 step:17234 [D loss: 0.660791, acc: 60.94%] [G loss: 1.862970]\n",
      "epoch:18 step:17235 [D loss: 0.681082, acc: 60.16%] [G loss: 1.827780]\n",
      "epoch:18 step:17236 [D loss: 0.640706, acc: 62.50%] [G loss: 1.859355]\n",
      "epoch:18 step:17237 [D loss: 0.647681, acc: 63.28%] [G loss: 1.945950]\n",
      "epoch:18 step:17238 [D loss: 0.637433, acc: 67.97%] [G loss: 1.864686]\n",
      "epoch:18 step:17239 [D loss: 0.667294, acc: 58.59%] [G loss: 1.748818]\n",
      "epoch:18 step:17240 [D loss: 0.682682, acc: 59.38%] [G loss: 1.908858]\n",
      "epoch:18 step:17241 [D loss: 0.658224, acc: 60.16%] [G loss: 1.785595]\n",
      "epoch:18 step:17242 [D loss: 0.704880, acc: 55.47%] [G loss: 1.758352]\n",
      "epoch:18 step:17243 [D loss: 0.649414, acc: 57.81%] [G loss: 1.878369]\n",
      "epoch:18 step:17244 [D loss: 0.660923, acc: 61.72%] [G loss: 1.740627]\n",
      "epoch:18 step:17245 [D loss: 0.660200, acc: 55.47%] [G loss: 1.846934]\n",
      "epoch:18 step:17246 [D loss: 0.685132, acc: 58.59%] [G loss: 1.894176]\n",
      "epoch:18 step:17247 [D loss: 0.603459, acc: 68.75%] [G loss: 2.002829]\n",
      "epoch:18 step:17248 [D loss: 0.653216, acc: 61.72%] [G loss: 1.960688]\n",
      "epoch:18 step:17249 [D loss: 0.676074, acc: 57.03%] [G loss: 1.932657]\n",
      "epoch:18 step:17250 [D loss: 0.654234, acc: 64.84%] [G loss: 1.872327]\n",
      "epoch:18 step:17251 [D loss: 0.620377, acc: 60.94%] [G loss: 1.878574]\n",
      "epoch:18 step:17252 [D loss: 0.696264, acc: 55.47%] [G loss: 1.779073]\n",
      "epoch:18 step:17253 [D loss: 0.638181, acc: 59.38%] [G loss: 1.813496]\n",
      "epoch:18 step:17254 [D loss: 0.643838, acc: 65.62%] [G loss: 1.796562]\n",
      "epoch:18 step:17255 [D loss: 0.579302, acc: 73.44%] [G loss: 1.863930]\n",
      "epoch:18 step:17256 [D loss: 0.675354, acc: 60.16%] [G loss: 1.814939]\n",
      "epoch:18 step:17257 [D loss: 0.632166, acc: 65.62%] [G loss: 1.910451]\n",
      "epoch:18 step:17258 [D loss: 0.662603, acc: 64.06%] [G loss: 1.847248]\n",
      "epoch:18 step:17259 [D loss: 0.644925, acc: 62.50%] [G loss: 1.806787]\n",
      "epoch:18 step:17260 [D loss: 0.627494, acc: 65.62%] [G loss: 1.892553]\n",
      "epoch:18 step:17261 [D loss: 0.621030, acc: 61.72%] [G loss: 1.770299]\n",
      "epoch:18 step:17262 [D loss: 0.696455, acc: 57.03%] [G loss: 1.866486]\n",
      "epoch:18 step:17263 [D loss: 0.620449, acc: 61.72%] [G loss: 1.720169]\n",
      "epoch:18 step:17264 [D loss: 0.617418, acc: 64.06%] [G loss: 1.941657]\n",
      "epoch:18 step:17265 [D loss: 0.635507, acc: 69.53%] [G loss: 1.857942]\n",
      "epoch:18 step:17266 [D loss: 0.646287, acc: 62.50%] [G loss: 1.982083]\n",
      "epoch:18 step:17267 [D loss: 0.674884, acc: 60.16%] [G loss: 1.925702]\n",
      "epoch:18 step:17268 [D loss: 0.634089, acc: 66.41%] [G loss: 2.074428]\n",
      "epoch:18 step:17269 [D loss: 0.663348, acc: 65.62%] [G loss: 1.998746]\n",
      "epoch:18 step:17270 [D loss: 0.608113, acc: 71.09%] [G loss: 2.101165]\n",
      "epoch:18 step:17271 [D loss: 0.622751, acc: 68.75%] [G loss: 2.004275]\n",
      "epoch:18 step:17272 [D loss: 0.609191, acc: 64.06%] [G loss: 2.170845]\n",
      "epoch:18 step:17273 [D loss: 0.664628, acc: 60.94%] [G loss: 1.914085]\n",
      "epoch:18 step:17274 [D loss: 0.733701, acc: 59.38%] [G loss: 1.748196]\n",
      "epoch:18 step:17275 [D loss: 0.647968, acc: 64.84%] [G loss: 1.778487]\n",
      "epoch:18 step:17276 [D loss: 0.637591, acc: 62.50%] [G loss: 1.824430]\n",
      "epoch:18 step:17277 [D loss: 0.674487, acc: 59.38%] [G loss: 1.671069]\n",
      "epoch:18 step:17278 [D loss: 0.672651, acc: 57.81%] [G loss: 1.848948]\n",
      "epoch:18 step:17279 [D loss: 0.663078, acc: 58.59%] [G loss: 1.957842]\n",
      "epoch:18 step:17280 [D loss: 0.621398, acc: 64.84%] [G loss: 1.904003]\n",
      "epoch:18 step:17281 [D loss: 0.663482, acc: 55.47%] [G loss: 1.983426]\n",
      "epoch:18 step:17282 [D loss: 0.655993, acc: 61.72%] [G loss: 1.937335]\n",
      "epoch:18 step:17283 [D loss: 0.623763, acc: 64.06%] [G loss: 1.950060]\n",
      "epoch:18 step:17284 [D loss: 0.696050, acc: 54.69%] [G loss: 1.886650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17285 [D loss: 0.628035, acc: 61.72%] [G loss: 1.966271]\n",
      "epoch:18 step:17286 [D loss: 0.640860, acc: 60.94%] [G loss: 1.892362]\n",
      "epoch:18 step:17287 [D loss: 0.646882, acc: 65.62%] [G loss: 1.832821]\n",
      "epoch:18 step:17288 [D loss: 0.660326, acc: 59.38%] [G loss: 1.779627]\n",
      "epoch:18 step:17289 [D loss: 0.680912, acc: 59.38%] [G loss: 1.809350]\n",
      "epoch:18 step:17290 [D loss: 0.643345, acc: 62.50%] [G loss: 1.812633]\n",
      "epoch:18 step:17291 [D loss: 0.680390, acc: 55.47%] [G loss: 1.984076]\n",
      "epoch:18 step:17292 [D loss: 0.666275, acc: 57.03%] [G loss: 1.913615]\n",
      "epoch:18 step:17293 [D loss: 0.658551, acc: 57.81%] [G loss: 2.119133]\n",
      "epoch:18 step:17294 [D loss: 0.603801, acc: 67.19%] [G loss: 2.073377]\n",
      "epoch:18 step:17295 [D loss: 0.548240, acc: 76.56%] [G loss: 2.176646]\n",
      "epoch:18 step:17296 [D loss: 0.616022, acc: 62.50%] [G loss: 2.248063]\n",
      "epoch:18 step:17297 [D loss: 0.625380, acc: 69.53%] [G loss: 2.012787]\n",
      "epoch:18 step:17298 [D loss: 0.684887, acc: 57.03%] [G loss: 1.822331]\n",
      "epoch:18 step:17299 [D loss: 0.631775, acc: 60.16%] [G loss: 1.845452]\n",
      "epoch:18 step:17300 [D loss: 0.599483, acc: 69.53%] [G loss: 2.020527]\n",
      "epoch:18 step:17301 [D loss: 0.640240, acc: 64.84%] [G loss: 1.997599]\n",
      "epoch:18 step:17302 [D loss: 0.592464, acc: 68.75%] [G loss: 2.009975]\n",
      "epoch:18 step:17303 [D loss: 0.698809, acc: 60.16%] [G loss: 1.766822]\n",
      "epoch:18 step:17304 [D loss: 0.693769, acc: 53.12%] [G loss: 1.709458]\n",
      "epoch:18 step:17305 [D loss: 0.662405, acc: 57.81%] [G loss: 1.779230]\n",
      "epoch:18 step:17306 [D loss: 0.689834, acc: 55.47%] [G loss: 1.815795]\n",
      "epoch:18 step:17307 [D loss: 0.644516, acc: 60.94%] [G loss: 1.832078]\n",
      "epoch:18 step:17308 [D loss: 0.679231, acc: 59.38%] [G loss: 1.926138]\n",
      "epoch:18 step:17309 [D loss: 0.618179, acc: 64.06%] [G loss: 1.855452]\n",
      "epoch:18 step:17310 [D loss: 0.638073, acc: 63.28%] [G loss: 1.890791]\n",
      "epoch:18 step:17311 [D loss: 0.636767, acc: 63.28%] [G loss: 1.983881]\n",
      "epoch:18 step:17312 [D loss: 0.671768, acc: 60.16%] [G loss: 1.810814]\n",
      "epoch:18 step:17313 [D loss: 0.626121, acc: 65.62%] [G loss: 1.968410]\n",
      "epoch:18 step:17314 [D loss: 0.638476, acc: 66.41%] [G loss: 1.772409]\n",
      "epoch:18 step:17315 [D loss: 0.655691, acc: 58.59%] [G loss: 1.965459]\n",
      "epoch:18 step:17316 [D loss: 0.632236, acc: 61.72%] [G loss: 1.924771]\n",
      "epoch:18 step:17317 [D loss: 0.614567, acc: 64.84%] [G loss: 2.055648]\n",
      "epoch:18 step:17318 [D loss: 0.639463, acc: 62.50%] [G loss: 1.993726]\n",
      "epoch:18 step:17319 [D loss: 0.641552, acc: 67.97%] [G loss: 1.945978]\n",
      "epoch:18 step:17320 [D loss: 0.636290, acc: 61.72%] [G loss: 1.954283]\n",
      "epoch:18 step:17321 [D loss: 0.638632, acc: 57.03%] [G loss: 1.998720]\n",
      "epoch:18 step:17322 [D loss: 0.615836, acc: 67.19%] [G loss: 1.891921]\n",
      "epoch:18 step:17323 [D loss: 0.602404, acc: 67.19%] [G loss: 2.144907]\n",
      "epoch:18 step:17324 [D loss: 0.653274, acc: 60.94%] [G loss: 1.903496]\n",
      "epoch:18 step:17325 [D loss: 0.626658, acc: 65.62%] [G loss: 1.919688]\n",
      "epoch:18 step:17326 [D loss: 0.667161, acc: 61.72%] [G loss: 1.787053]\n",
      "epoch:18 step:17327 [D loss: 0.662923, acc: 64.06%] [G loss: 1.975749]\n",
      "epoch:18 step:17328 [D loss: 0.695364, acc: 54.69%] [G loss: 1.964443]\n",
      "epoch:18 step:17329 [D loss: 0.643406, acc: 66.41%] [G loss: 2.053723]\n",
      "epoch:18 step:17330 [D loss: 0.639196, acc: 64.84%] [G loss: 1.894660]\n",
      "epoch:18 step:17331 [D loss: 0.624334, acc: 64.84%] [G loss: 1.929941]\n",
      "epoch:18 step:17332 [D loss: 0.626470, acc: 64.06%] [G loss: 1.881296]\n",
      "epoch:18 step:17333 [D loss: 0.701389, acc: 59.38%] [G loss: 1.898201]\n",
      "epoch:18 step:17334 [D loss: 0.631621, acc: 64.06%] [G loss: 1.961430]\n",
      "epoch:18 step:17335 [D loss: 0.652658, acc: 65.62%] [G loss: 1.907329]\n",
      "epoch:18 step:17336 [D loss: 0.574603, acc: 71.88%] [G loss: 2.066257]\n",
      "epoch:18 step:17337 [D loss: 0.653785, acc: 65.62%] [G loss: 2.173636]\n",
      "epoch:18 step:17338 [D loss: 0.601273, acc: 67.97%] [G loss: 2.081693]\n",
      "epoch:18 step:17339 [D loss: 0.660719, acc: 60.16%] [G loss: 1.998181]\n",
      "epoch:18 step:17340 [D loss: 0.620393, acc: 68.75%] [G loss: 1.877357]\n",
      "epoch:18 step:17341 [D loss: 0.698510, acc: 53.91%] [G loss: 1.940723]\n",
      "epoch:18 step:17342 [D loss: 0.684275, acc: 57.81%] [G loss: 2.088763]\n",
      "epoch:18 step:17343 [D loss: 0.698216, acc: 59.38%] [G loss: 1.743449]\n",
      "epoch:18 step:17344 [D loss: 0.638842, acc: 61.72%] [G loss: 1.945862]\n",
      "epoch:18 step:17345 [D loss: 0.691211, acc: 55.47%] [G loss: 1.959687]\n",
      "epoch:18 step:17346 [D loss: 0.635306, acc: 61.72%] [G loss: 2.000968]\n",
      "epoch:18 step:17347 [D loss: 0.620190, acc: 65.62%] [G loss: 1.922584]\n",
      "epoch:18 step:17348 [D loss: 0.635571, acc: 61.72%] [G loss: 1.730758]\n",
      "epoch:18 step:17349 [D loss: 0.663998, acc: 57.81%] [G loss: 1.877107]\n",
      "epoch:18 step:17350 [D loss: 0.627196, acc: 62.50%] [G loss: 1.979660]\n",
      "epoch:18 step:17351 [D loss: 0.656592, acc: 60.16%] [G loss: 1.788529]\n",
      "epoch:18 step:17352 [D loss: 0.636286, acc: 64.06%] [G loss: 1.784521]\n",
      "epoch:18 step:17353 [D loss: 0.653226, acc: 57.81%] [G loss: 1.910950]\n",
      "epoch:18 step:17354 [D loss: 0.619723, acc: 64.06%] [G loss: 1.966574]\n",
      "epoch:18 step:17355 [D loss: 0.687442, acc: 51.56%] [G loss: 1.932108]\n",
      "epoch:18 step:17356 [D loss: 0.595165, acc: 65.62%] [G loss: 1.974195]\n",
      "epoch:18 step:17357 [D loss: 0.659908, acc: 57.81%] [G loss: 1.899854]\n",
      "epoch:18 step:17358 [D loss: 0.675475, acc: 60.16%] [G loss: 1.738604]\n",
      "epoch:18 step:17359 [D loss: 0.600596, acc: 67.19%] [G loss: 1.950134]\n",
      "epoch:18 step:17360 [D loss: 0.654683, acc: 60.94%] [G loss: 1.901718]\n",
      "epoch:18 step:17361 [D loss: 0.543556, acc: 75.00%] [G loss: 2.109546]\n",
      "epoch:18 step:17362 [D loss: 0.669492, acc: 58.59%] [G loss: 1.993425]\n",
      "epoch:18 step:17363 [D loss: 0.628212, acc: 71.09%] [G loss: 1.832403]\n",
      "epoch:18 step:17364 [D loss: 0.563012, acc: 70.31%] [G loss: 1.932058]\n",
      "epoch:18 step:17365 [D loss: 0.586140, acc: 66.41%] [G loss: 2.114311]\n",
      "epoch:18 step:17366 [D loss: 0.680054, acc: 56.25%] [G loss: 1.906572]\n",
      "epoch:18 step:17367 [D loss: 0.695570, acc: 57.03%] [G loss: 1.863536]\n",
      "epoch:18 step:17368 [D loss: 0.700549, acc: 53.91%] [G loss: 1.896689]\n",
      "epoch:18 step:17369 [D loss: 0.619658, acc: 65.62%] [G loss: 1.970474]\n",
      "epoch:18 step:17370 [D loss: 0.610522, acc: 70.31%] [G loss: 1.904570]\n",
      "epoch:18 step:17371 [D loss: 0.601514, acc: 62.50%] [G loss: 1.844456]\n",
      "epoch:18 step:17372 [D loss: 0.645095, acc: 67.19%] [G loss: 1.918199]\n",
      "epoch:18 step:17373 [D loss: 0.684418, acc: 57.03%] [G loss: 2.026441]\n",
      "epoch:18 step:17374 [D loss: 0.703469, acc: 59.38%] [G loss: 2.076474]\n",
      "epoch:18 step:17375 [D loss: 0.651609, acc: 60.94%] [G loss: 1.857271]\n",
      "epoch:18 step:17376 [D loss: 0.623324, acc: 65.62%] [G loss: 1.811752]\n",
      "epoch:18 step:17377 [D loss: 0.695914, acc: 55.47%] [G loss: 1.743465]\n",
      "epoch:18 step:17378 [D loss: 0.642602, acc: 62.50%] [G loss: 1.924902]\n",
      "epoch:18 step:17379 [D loss: 0.695572, acc: 55.47%] [G loss: 1.904917]\n",
      "epoch:18 step:17380 [D loss: 0.636452, acc: 64.06%] [G loss: 1.802631]\n",
      "epoch:18 step:17381 [D loss: 0.645723, acc: 63.28%] [G loss: 1.892831]\n",
      "epoch:18 step:17382 [D loss: 0.615852, acc: 71.09%] [G loss: 2.046988]\n",
      "epoch:18 step:17383 [D loss: 0.606675, acc: 66.41%] [G loss: 1.926414]\n",
      "epoch:18 step:17384 [D loss: 0.674379, acc: 61.72%] [G loss: 1.824286]\n",
      "epoch:18 step:17385 [D loss: 0.631059, acc: 63.28%] [G loss: 1.808069]\n",
      "epoch:18 step:17386 [D loss: 0.622804, acc: 64.84%] [G loss: 1.902833]\n",
      "epoch:18 step:17387 [D loss: 0.671761, acc: 60.16%] [G loss: 1.888545]\n",
      "epoch:18 step:17388 [D loss: 0.630086, acc: 67.97%] [G loss: 1.872866]\n",
      "epoch:18 step:17389 [D loss: 0.652004, acc: 61.72%] [G loss: 2.047712]\n",
      "epoch:18 step:17390 [D loss: 0.626143, acc: 62.50%] [G loss: 1.803802]\n",
      "epoch:18 step:17391 [D loss: 0.626017, acc: 64.84%] [G loss: 1.795895]\n",
      "epoch:18 step:17392 [D loss: 0.666169, acc: 62.50%] [G loss: 1.893729]\n",
      "epoch:18 step:17393 [D loss: 0.649395, acc: 60.94%] [G loss: 1.834998]\n",
      "epoch:18 step:17394 [D loss: 0.653871, acc: 66.41%] [G loss: 1.813059]\n",
      "epoch:18 step:17395 [D loss: 0.729843, acc: 52.34%] [G loss: 1.716053]\n",
      "epoch:18 step:17396 [D loss: 0.698792, acc: 55.47%] [G loss: 1.787579]\n",
      "epoch:18 step:17397 [D loss: 0.635923, acc: 58.59%] [G loss: 1.810580]\n",
      "epoch:18 step:17398 [D loss: 0.629042, acc: 65.62%] [G loss: 2.026510]\n",
      "epoch:18 step:17399 [D loss: 0.659501, acc: 60.16%] [G loss: 2.010144]\n",
      "epoch:18 step:17400 [D loss: 0.612554, acc: 67.19%] [G loss: 2.126658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.5750422  1.41566861 6.13237736 4.9470194  3.75577607 5.67636087\n",
      " 4.28388824 4.43462413 4.67202993 3.5345791 ]\n",
      "##########\n",
      "epoch:18 step:17401 [D loss: 0.639294, acc: 70.31%] [G loss: 1.779999]\n",
      "epoch:18 step:17402 [D loss: 0.589954, acc: 73.44%] [G loss: 2.145112]\n",
      "epoch:18 step:17403 [D loss: 0.651405, acc: 65.62%] [G loss: 1.882034]\n",
      "epoch:18 step:17404 [D loss: 0.604225, acc: 69.53%] [G loss: 2.032863]\n",
      "epoch:18 step:17405 [D loss: 0.644446, acc: 62.50%] [G loss: 1.851271]\n",
      "epoch:18 step:17406 [D loss: 0.652043, acc: 64.06%] [G loss: 1.824576]\n",
      "epoch:18 step:17407 [D loss: 0.617575, acc: 63.28%] [G loss: 1.951128]\n",
      "epoch:18 step:17408 [D loss: 0.669346, acc: 60.94%] [G loss: 1.882468]\n",
      "epoch:18 step:17409 [D loss: 0.657070, acc: 60.94%] [G loss: 1.961853]\n",
      "epoch:18 step:17410 [D loss: 0.654830, acc: 59.38%] [G loss: 1.959154]\n",
      "epoch:18 step:17411 [D loss: 0.667641, acc: 66.41%] [G loss: 2.101066]\n",
      "epoch:18 step:17412 [D loss: 0.675608, acc: 64.84%] [G loss: 1.946878]\n",
      "epoch:18 step:17413 [D loss: 0.671264, acc: 64.06%] [G loss: 1.955561]\n",
      "epoch:18 step:17414 [D loss: 0.658646, acc: 67.19%] [G loss: 1.940744]\n",
      "epoch:18 step:17415 [D loss: 0.626173, acc: 64.06%] [G loss: 1.864686]\n",
      "epoch:18 step:17416 [D loss: 0.578741, acc: 68.75%] [G loss: 2.027483]\n",
      "epoch:18 step:17417 [D loss: 0.583057, acc: 64.06%] [G loss: 2.129077]\n",
      "epoch:18 step:17418 [D loss: 0.612488, acc: 64.06%] [G loss: 2.084644]\n",
      "epoch:18 step:17419 [D loss: 0.693937, acc: 53.91%] [G loss: 1.867686]\n",
      "epoch:18 step:17420 [D loss: 0.598319, acc: 67.19%] [G loss: 2.058517]\n",
      "epoch:18 step:17421 [D loss: 0.654409, acc: 66.41%] [G loss: 1.965852]\n",
      "epoch:18 step:17422 [D loss: 0.646071, acc: 60.94%] [G loss: 2.012044]\n",
      "epoch:18 step:17423 [D loss: 0.580764, acc: 73.44%] [G loss: 2.009689]\n",
      "epoch:18 step:17424 [D loss: 0.679122, acc: 60.94%] [G loss: 2.016416]\n",
      "epoch:18 step:17425 [D loss: 0.657652, acc: 58.59%] [G loss: 1.815777]\n",
      "epoch:18 step:17426 [D loss: 0.611039, acc: 64.84%] [G loss: 1.910879]\n",
      "epoch:18 step:17427 [D loss: 0.688051, acc: 55.47%] [G loss: 1.889511]\n",
      "epoch:18 step:17428 [D loss: 0.629536, acc: 66.41%] [G loss: 1.864076]\n",
      "epoch:18 step:17429 [D loss: 0.642858, acc: 62.50%] [G loss: 1.885632]\n",
      "epoch:18 step:17430 [D loss: 0.632918, acc: 63.28%] [G loss: 2.007862]\n",
      "epoch:18 step:17431 [D loss: 0.710922, acc: 53.12%] [G loss: 1.776029]\n",
      "epoch:18 step:17432 [D loss: 0.693282, acc: 57.81%] [G loss: 1.834907]\n",
      "epoch:18 step:17433 [D loss: 0.649843, acc: 66.41%] [G loss: 1.936127]\n",
      "epoch:18 step:17434 [D loss: 0.675656, acc: 56.25%] [G loss: 1.932512]\n",
      "epoch:18 step:17435 [D loss: 0.635416, acc: 62.50%] [G loss: 1.880839]\n",
      "epoch:18 step:17436 [D loss: 0.639806, acc: 67.19%] [G loss: 1.909391]\n",
      "epoch:18 step:17437 [D loss: 0.656818, acc: 59.38%] [G loss: 1.902292]\n",
      "epoch:18 step:17438 [D loss: 0.688729, acc: 55.47%] [G loss: 1.781048]\n",
      "epoch:18 step:17439 [D loss: 0.624822, acc: 61.72%] [G loss: 1.819644]\n",
      "epoch:18 step:17440 [D loss: 0.670339, acc: 62.50%] [G loss: 1.866614]\n",
      "epoch:18 step:17441 [D loss: 0.615178, acc: 68.75%] [G loss: 1.827351]\n",
      "epoch:18 step:17442 [D loss: 0.630121, acc: 64.06%] [G loss: 1.773531]\n",
      "epoch:18 step:17443 [D loss: 0.686300, acc: 55.47%] [G loss: 1.737972]\n",
      "epoch:18 step:17444 [D loss: 0.626151, acc: 65.62%] [G loss: 1.845866]\n",
      "epoch:18 step:17445 [D loss: 0.644036, acc: 60.94%] [G loss: 1.871271]\n",
      "epoch:18 step:17446 [D loss: 0.644336, acc: 61.72%] [G loss: 1.914287]\n",
      "epoch:18 step:17447 [D loss: 0.640049, acc: 62.50%] [G loss: 1.990388]\n",
      "epoch:18 step:17448 [D loss: 0.625574, acc: 60.94%] [G loss: 1.844381]\n",
      "epoch:18 step:17449 [D loss: 0.642532, acc: 62.50%] [G loss: 1.748322]\n",
      "epoch:18 step:17450 [D loss: 0.662566, acc: 58.59%] [G loss: 1.807625]\n",
      "epoch:18 step:17451 [D loss: 0.620901, acc: 62.50%] [G loss: 1.866068]\n",
      "epoch:18 step:17452 [D loss: 0.644603, acc: 60.16%] [G loss: 1.823184]\n",
      "epoch:18 step:17453 [D loss: 0.661813, acc: 60.16%] [G loss: 1.846430]\n",
      "epoch:18 step:17454 [D loss: 0.647412, acc: 60.94%] [G loss: 1.902187]\n",
      "epoch:18 step:17455 [D loss: 0.644572, acc: 56.25%] [G loss: 1.965637]\n",
      "epoch:18 step:17456 [D loss: 0.656123, acc: 58.59%] [G loss: 1.894762]\n",
      "epoch:18 step:17457 [D loss: 0.612564, acc: 67.19%] [G loss: 1.875692]\n",
      "epoch:18 step:17458 [D loss: 0.657040, acc: 60.94%] [G loss: 1.902375]\n",
      "epoch:18 step:17459 [D loss: 0.666010, acc: 60.94%] [G loss: 1.817819]\n",
      "epoch:18 step:17460 [D loss: 0.582989, acc: 73.44%] [G loss: 1.827944]\n",
      "epoch:18 step:17461 [D loss: 0.607028, acc: 66.41%] [G loss: 1.831848]\n",
      "epoch:18 step:17462 [D loss: 0.664580, acc: 58.59%] [G loss: 1.807986]\n",
      "epoch:18 step:17463 [D loss: 0.692121, acc: 60.94%] [G loss: 1.833195]\n",
      "epoch:18 step:17464 [D loss: 0.654319, acc: 57.81%] [G loss: 1.867563]\n",
      "epoch:18 step:17465 [D loss: 0.620531, acc: 61.72%] [G loss: 1.948999]\n",
      "epoch:18 step:17466 [D loss: 0.642734, acc: 63.28%] [G loss: 1.836630]\n",
      "epoch:18 step:17467 [D loss: 0.661734, acc: 66.41%] [G loss: 1.952131]\n",
      "epoch:18 step:17468 [D loss: 0.682597, acc: 60.16%] [G loss: 1.908506]\n",
      "epoch:18 step:17469 [D loss: 0.579075, acc: 67.19%] [G loss: 1.884519]\n",
      "epoch:18 step:17470 [D loss: 0.609591, acc: 67.97%] [G loss: 1.968959]\n",
      "epoch:18 step:17471 [D loss: 0.595945, acc: 64.84%] [G loss: 2.023497]\n",
      "epoch:18 step:17472 [D loss: 0.654715, acc: 60.16%] [G loss: 1.900720]\n",
      "epoch:18 step:17473 [D loss: 0.633217, acc: 61.72%] [G loss: 1.934298]\n",
      "epoch:18 step:17474 [D loss: 0.674261, acc: 59.38%] [G loss: 1.932837]\n",
      "epoch:18 step:17475 [D loss: 0.622144, acc: 63.28%] [G loss: 1.816831]\n",
      "epoch:18 step:17476 [D loss: 0.611311, acc: 68.75%] [G loss: 1.927543]\n",
      "epoch:18 step:17477 [D loss: 0.666856, acc: 57.81%] [G loss: 1.713514]\n",
      "epoch:18 step:17478 [D loss: 0.678552, acc: 53.91%] [G loss: 1.897788]\n",
      "epoch:18 step:17479 [D loss: 0.642765, acc: 57.81%] [G loss: 1.884415]\n",
      "epoch:18 step:17480 [D loss: 0.737022, acc: 48.44%] [G loss: 1.776014]\n",
      "epoch:18 step:17481 [D loss: 0.630905, acc: 58.59%] [G loss: 1.790983]\n",
      "epoch:18 step:17482 [D loss: 0.673421, acc: 56.25%] [G loss: 1.858426]\n",
      "epoch:18 step:17483 [D loss: 0.660603, acc: 59.38%] [G loss: 1.855505]\n",
      "epoch:18 step:17484 [D loss: 0.662346, acc: 60.16%] [G loss: 1.776659]\n",
      "epoch:18 step:17485 [D loss: 0.638240, acc: 60.94%] [G loss: 1.882922]\n",
      "epoch:18 step:17486 [D loss: 0.634296, acc: 65.62%] [G loss: 1.926194]\n",
      "epoch:18 step:17487 [D loss: 0.663348, acc: 62.50%] [G loss: 1.789152]\n",
      "epoch:18 step:17488 [D loss: 0.705345, acc: 57.03%] [G loss: 1.879357]\n",
      "epoch:18 step:17489 [D loss: 0.677432, acc: 58.59%] [G loss: 1.931294]\n",
      "epoch:18 step:17490 [D loss: 0.655810, acc: 61.72%] [G loss: 2.011069]\n",
      "epoch:18 step:17491 [D loss: 0.696930, acc: 53.91%] [G loss: 1.790912]\n",
      "epoch:18 step:17492 [D loss: 0.651685, acc: 64.06%] [G loss: 1.700102]\n",
      "epoch:18 step:17493 [D loss: 0.635884, acc: 61.72%] [G loss: 1.874367]\n",
      "epoch:18 step:17494 [D loss: 0.717660, acc: 56.25%] [G loss: 1.809467]\n",
      "epoch:18 step:17495 [D loss: 0.630696, acc: 64.84%] [G loss: 2.006205]\n",
      "epoch:18 step:17496 [D loss: 0.674584, acc: 59.38%] [G loss: 1.937328]\n",
      "epoch:18 step:17497 [D loss: 0.616913, acc: 66.41%] [G loss: 1.921540]\n",
      "epoch:18 step:17498 [D loss: 0.600154, acc: 68.75%] [G loss: 1.939984]\n",
      "epoch:18 step:17499 [D loss: 0.616467, acc: 64.84%] [G loss: 1.937093]\n",
      "epoch:18 step:17500 [D loss: 0.627302, acc: 69.53%] [G loss: 1.884466]\n",
      "epoch:18 step:17501 [D loss: 0.651762, acc: 63.28%] [G loss: 2.014728]\n",
      "epoch:18 step:17502 [D loss: 0.606929, acc: 64.84%] [G loss: 1.942486]\n",
      "epoch:18 step:17503 [D loss: 0.640162, acc: 66.41%] [G loss: 2.128010]\n",
      "epoch:18 step:17504 [D loss: 0.638245, acc: 64.84%] [G loss: 2.005483]\n",
      "epoch:18 step:17505 [D loss: 0.568048, acc: 71.88%] [G loss: 1.922499]\n",
      "epoch:18 step:17506 [D loss: 0.650650, acc: 61.72%] [G loss: 1.914651]\n",
      "epoch:18 step:17507 [D loss: 0.570588, acc: 69.53%] [G loss: 1.840844]\n",
      "epoch:18 step:17508 [D loss: 0.616055, acc: 65.62%] [G loss: 1.956202]\n",
      "epoch:18 step:17509 [D loss: 0.621874, acc: 69.53%] [G loss: 1.816638]\n",
      "epoch:18 step:17510 [D loss: 0.615718, acc: 69.53%] [G loss: 2.073734]\n",
      "epoch:18 step:17511 [D loss: 0.623920, acc: 67.97%] [G loss: 2.013817]\n",
      "epoch:18 step:17512 [D loss: 0.649483, acc: 64.84%] [G loss: 1.998224]\n",
      "epoch:18 step:17513 [D loss: 0.598267, acc: 70.31%] [G loss: 2.219146]\n",
      "epoch:18 step:17514 [D loss: 0.583013, acc: 71.09%] [G loss: 2.309528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17515 [D loss: 0.572597, acc: 67.97%] [G loss: 2.344126]\n",
      "epoch:18 step:17516 [D loss: 0.665657, acc: 58.59%] [G loss: 2.230254]\n",
      "epoch:18 step:17517 [D loss: 0.620211, acc: 65.62%] [G loss: 1.986013]\n",
      "epoch:18 step:17518 [D loss: 0.655544, acc: 63.28%] [G loss: 2.015831]\n",
      "epoch:18 step:17519 [D loss: 0.658000, acc: 58.59%] [G loss: 1.913441]\n",
      "epoch:18 step:17520 [D loss: 0.628533, acc: 64.84%] [G loss: 2.074398]\n",
      "epoch:18 step:17521 [D loss: 0.674672, acc: 59.38%] [G loss: 1.971759]\n",
      "epoch:18 step:17522 [D loss: 0.656077, acc: 60.16%] [G loss: 1.749163]\n",
      "epoch:18 step:17523 [D loss: 0.684300, acc: 57.81%] [G loss: 1.808650]\n",
      "epoch:18 step:17524 [D loss: 0.665716, acc: 60.94%] [G loss: 1.806626]\n",
      "epoch:18 step:17525 [D loss: 0.660442, acc: 60.16%] [G loss: 1.800903]\n",
      "epoch:18 step:17526 [D loss: 0.625518, acc: 67.19%] [G loss: 1.949224]\n",
      "epoch:18 step:17527 [D loss: 0.626866, acc: 64.06%] [G loss: 1.954261]\n",
      "epoch:18 step:17528 [D loss: 0.678693, acc: 59.38%] [G loss: 1.801506]\n",
      "epoch:18 step:17529 [D loss: 0.604417, acc: 64.06%] [G loss: 1.902230]\n",
      "epoch:18 step:17530 [D loss: 0.630485, acc: 61.72%] [G loss: 1.844061]\n",
      "epoch:18 step:17531 [D loss: 0.624584, acc: 67.19%] [G loss: 1.954628]\n",
      "epoch:18 step:17532 [D loss: 0.679748, acc: 57.81%] [G loss: 1.845305]\n",
      "epoch:18 step:17533 [D loss: 0.624413, acc: 64.06%] [G loss: 1.903595]\n",
      "epoch:18 step:17534 [D loss: 0.619301, acc: 65.62%] [G loss: 1.982133]\n",
      "epoch:18 step:17535 [D loss: 0.660710, acc: 57.81%] [G loss: 1.982316]\n",
      "epoch:18 step:17536 [D loss: 0.623041, acc: 62.50%] [G loss: 1.830161]\n",
      "epoch:18 step:17537 [D loss: 0.634572, acc: 62.50%] [G loss: 1.897569]\n",
      "epoch:18 step:17538 [D loss: 0.733328, acc: 46.88%] [G loss: 1.834152]\n",
      "epoch:18 step:17539 [D loss: 0.638140, acc: 64.84%] [G loss: 1.809946]\n",
      "epoch:18 step:17540 [D loss: 0.634737, acc: 61.72%] [G loss: 1.757465]\n",
      "epoch:18 step:17541 [D loss: 0.596905, acc: 68.75%] [G loss: 1.828600]\n",
      "epoch:18 step:17542 [D loss: 0.637532, acc: 60.94%] [G loss: 1.885103]\n",
      "epoch:18 step:17543 [D loss: 0.669434, acc: 65.62%] [G loss: 1.969341]\n",
      "epoch:18 step:17544 [D loss: 0.619560, acc: 66.41%] [G loss: 1.811708]\n",
      "epoch:18 step:17545 [D loss: 0.588579, acc: 71.88%] [G loss: 2.082214]\n",
      "epoch:18 step:17546 [D loss: 0.695802, acc: 61.72%] [G loss: 1.826021]\n",
      "epoch:18 step:17547 [D loss: 0.590755, acc: 65.62%] [G loss: 1.940208]\n",
      "epoch:18 step:17548 [D loss: 0.675266, acc: 59.38%] [G loss: 1.786107]\n",
      "epoch:18 step:17549 [D loss: 0.620529, acc: 65.62%] [G loss: 1.896436]\n",
      "epoch:18 step:17550 [D loss: 0.636346, acc: 64.84%] [G loss: 1.863318]\n",
      "epoch:18 step:17551 [D loss: 0.608666, acc: 64.84%] [G loss: 1.857591]\n",
      "epoch:18 step:17552 [D loss: 0.658561, acc: 65.62%] [G loss: 1.846336]\n",
      "epoch:18 step:17553 [D loss: 0.703659, acc: 53.91%] [G loss: 1.754494]\n",
      "epoch:18 step:17554 [D loss: 0.636015, acc: 61.72%] [G loss: 1.882117]\n",
      "epoch:18 step:17555 [D loss: 0.604680, acc: 68.75%] [G loss: 1.942478]\n",
      "epoch:18 step:17556 [D loss: 0.635539, acc: 60.16%] [G loss: 1.995438]\n",
      "epoch:18 step:17557 [D loss: 0.613414, acc: 70.31%] [G loss: 2.007381]\n",
      "epoch:18 step:17558 [D loss: 0.618653, acc: 63.28%] [G loss: 2.088635]\n",
      "epoch:18 step:17559 [D loss: 0.629898, acc: 66.41%] [G loss: 2.097248]\n",
      "epoch:18 step:17560 [D loss: 0.606437, acc: 68.75%] [G loss: 2.241903]\n",
      "epoch:18 step:17561 [D loss: 0.593095, acc: 64.84%] [G loss: 2.243120]\n",
      "epoch:18 step:17562 [D loss: 0.689824, acc: 58.59%] [G loss: 2.057836]\n",
      "epoch:18 step:17563 [D loss: 0.655639, acc: 59.38%] [G loss: 1.930655]\n",
      "epoch:18 step:17564 [D loss: 0.691972, acc: 60.94%] [G loss: 1.832483]\n",
      "epoch:18 step:17565 [D loss: 0.658682, acc: 64.84%] [G loss: 1.871035]\n",
      "epoch:18 step:17566 [D loss: 0.674364, acc: 53.12%] [G loss: 1.807020]\n",
      "epoch:18 step:17567 [D loss: 0.640991, acc: 60.16%] [G loss: 1.918253]\n",
      "epoch:18 step:17568 [D loss: 0.696710, acc: 55.47%] [G loss: 1.995755]\n",
      "epoch:18 step:17569 [D loss: 0.618153, acc: 58.59%] [G loss: 1.935481]\n",
      "epoch:18 step:17570 [D loss: 0.638811, acc: 64.06%] [G loss: 1.898262]\n",
      "epoch:18 step:17571 [D loss: 0.634880, acc: 63.28%] [G loss: 1.956733]\n",
      "epoch:18 step:17572 [D loss: 0.638501, acc: 67.97%] [G loss: 1.948233]\n",
      "epoch:18 step:17573 [D loss: 0.665789, acc: 60.94%] [G loss: 2.007535]\n",
      "epoch:18 step:17574 [D loss: 0.602856, acc: 67.97%] [G loss: 1.972004]\n",
      "epoch:18 step:17575 [D loss: 0.578351, acc: 70.31%] [G loss: 1.993183]\n",
      "epoch:18 step:17576 [D loss: 0.609595, acc: 67.97%] [G loss: 2.046371]\n",
      "epoch:18 step:17577 [D loss: 0.657100, acc: 60.94%] [G loss: 2.126574]\n",
      "epoch:18 step:17578 [D loss: 0.628315, acc: 66.41%] [G loss: 2.015026]\n",
      "epoch:18 step:17579 [D loss: 0.692756, acc: 59.38%] [G loss: 1.711590]\n",
      "epoch:18 step:17580 [D loss: 0.630734, acc: 59.38%] [G loss: 1.886058]\n",
      "epoch:18 step:17581 [D loss: 0.662821, acc: 57.81%] [G loss: 1.943531]\n",
      "epoch:18 step:17582 [D loss: 0.606994, acc: 68.75%] [G loss: 2.061252]\n",
      "epoch:18 step:17583 [D loss: 0.653717, acc: 58.59%] [G loss: 1.917684]\n",
      "epoch:18 step:17584 [D loss: 0.621991, acc: 69.53%] [G loss: 1.933428]\n",
      "epoch:18 step:17585 [D loss: 0.625935, acc: 61.72%] [G loss: 2.092528]\n",
      "epoch:18 step:17586 [D loss: 0.638065, acc: 63.28%] [G loss: 1.961583]\n",
      "epoch:18 step:17587 [D loss: 0.686419, acc: 60.94%] [G loss: 2.116350]\n",
      "epoch:18 step:17588 [D loss: 0.704090, acc: 57.03%] [G loss: 1.953826]\n",
      "epoch:18 step:17589 [D loss: 0.613731, acc: 61.72%] [G loss: 2.097695]\n",
      "epoch:18 step:17590 [D loss: 0.662572, acc: 62.50%] [G loss: 1.982557]\n",
      "epoch:18 step:17591 [D loss: 0.622443, acc: 65.62%] [G loss: 1.901028]\n",
      "epoch:18 step:17592 [D loss: 0.613152, acc: 68.75%] [G loss: 1.918801]\n",
      "epoch:18 step:17593 [D loss: 0.675544, acc: 57.81%] [G loss: 1.762438]\n",
      "epoch:18 step:17594 [D loss: 0.646318, acc: 64.06%] [G loss: 1.839412]\n",
      "epoch:18 step:17595 [D loss: 0.690151, acc: 53.91%] [G loss: 1.960753]\n",
      "epoch:18 step:17596 [D loss: 0.610811, acc: 66.41%] [G loss: 1.940668]\n",
      "epoch:18 step:17597 [D loss: 0.637406, acc: 64.06%] [G loss: 1.821832]\n",
      "epoch:18 step:17598 [D loss: 0.619705, acc: 68.75%] [G loss: 1.890435]\n",
      "epoch:18 step:17599 [D loss: 0.647187, acc: 64.84%] [G loss: 1.928283]\n",
      "epoch:18 step:17600 [D loss: 0.636650, acc: 66.41%] [G loss: 1.848391]\n",
      "##############\n",
      "[2.52421577 1.55499582 6.24515992 4.81949073 3.79986689 5.4729062\n",
      " 4.47596262 4.54159348 4.77468381 3.5104729 ]\n",
      "##########\n",
      "epoch:18 step:17601 [D loss: 0.677840, acc: 54.69%] [G loss: 1.902659]\n",
      "epoch:18 step:17602 [D loss: 0.609032, acc: 68.75%] [G loss: 1.925838]\n",
      "epoch:18 step:17603 [D loss: 0.599409, acc: 69.53%] [G loss: 2.028206]\n",
      "epoch:18 step:17604 [D loss: 0.666108, acc: 61.72%] [G loss: 1.902222]\n",
      "epoch:18 step:17605 [D loss: 0.658796, acc: 61.72%] [G loss: 1.945974]\n",
      "epoch:18 step:17606 [D loss: 0.662817, acc: 64.06%] [G loss: 1.881892]\n",
      "epoch:18 step:17607 [D loss: 0.623385, acc: 66.41%] [G loss: 1.911086]\n",
      "epoch:18 step:17608 [D loss: 0.636488, acc: 60.94%] [G loss: 1.940341]\n",
      "epoch:18 step:17609 [D loss: 0.587428, acc: 69.53%] [G loss: 1.853618]\n",
      "epoch:18 step:17610 [D loss: 0.617225, acc: 67.19%] [G loss: 2.044497]\n",
      "epoch:18 step:17611 [D loss: 0.631871, acc: 67.19%] [G loss: 1.899208]\n",
      "epoch:18 step:17612 [D loss: 0.640577, acc: 60.94%] [G loss: 1.993968]\n",
      "epoch:18 step:17613 [D loss: 0.581871, acc: 67.19%] [G loss: 2.067924]\n",
      "epoch:18 step:17614 [D loss: 0.633271, acc: 63.28%] [G loss: 1.885936]\n",
      "epoch:18 step:17615 [D loss: 0.691146, acc: 60.94%] [G loss: 1.811518]\n",
      "epoch:18 step:17616 [D loss: 0.621748, acc: 67.19%] [G loss: 1.987581]\n",
      "epoch:18 step:17617 [D loss: 0.710250, acc: 53.91%] [G loss: 1.727064]\n",
      "epoch:18 step:17618 [D loss: 0.711859, acc: 51.56%] [G loss: 1.676169]\n",
      "epoch:18 step:17619 [D loss: 0.668626, acc: 59.38%] [G loss: 1.856876]\n",
      "epoch:18 step:17620 [D loss: 0.630121, acc: 63.28%] [G loss: 1.866204]\n",
      "epoch:18 step:17621 [D loss: 0.700775, acc: 58.59%] [G loss: 1.778259]\n",
      "epoch:18 step:17622 [D loss: 0.629001, acc: 63.28%] [G loss: 1.904815]\n",
      "epoch:18 step:17623 [D loss: 0.632645, acc: 67.19%] [G loss: 1.879215]\n",
      "epoch:18 step:17624 [D loss: 0.666376, acc: 64.84%] [G loss: 1.853059]\n",
      "epoch:18 step:17625 [D loss: 0.710048, acc: 50.00%] [G loss: 1.792221]\n",
      "epoch:18 step:17626 [D loss: 0.660828, acc: 64.84%] [G loss: 1.759606]\n",
      "epoch:18 step:17627 [D loss: 0.663802, acc: 63.28%] [G loss: 1.866615]\n",
      "epoch:18 step:17628 [D loss: 0.656778, acc: 60.94%] [G loss: 1.837450]\n",
      "epoch:18 step:17629 [D loss: 0.676852, acc: 61.72%] [G loss: 1.761989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17630 [D loss: 0.643679, acc: 62.50%] [G loss: 1.863181]\n",
      "epoch:18 step:17631 [D loss: 0.725278, acc: 52.34%] [G loss: 1.599755]\n",
      "epoch:18 step:17632 [D loss: 0.674713, acc: 54.69%] [G loss: 1.769211]\n",
      "epoch:18 step:17633 [D loss: 0.697913, acc: 50.00%] [G loss: 1.728191]\n",
      "epoch:18 step:17634 [D loss: 0.679660, acc: 57.03%] [G loss: 1.961984]\n",
      "epoch:18 step:17635 [D loss: 0.693936, acc: 53.12%] [G loss: 1.894528]\n",
      "epoch:18 step:17636 [D loss: 0.626368, acc: 66.41%] [G loss: 1.943204]\n",
      "epoch:18 step:17637 [D loss: 0.681085, acc: 59.38%] [G loss: 1.891846]\n",
      "epoch:18 step:17638 [D loss: 0.654732, acc: 62.50%] [G loss: 1.885251]\n",
      "epoch:18 step:17639 [D loss: 0.676363, acc: 57.81%] [G loss: 1.827790]\n",
      "epoch:18 step:17640 [D loss: 0.656092, acc: 64.06%] [G loss: 1.995160]\n",
      "epoch:18 step:17641 [D loss: 0.608801, acc: 65.62%] [G loss: 2.114111]\n",
      "epoch:18 step:17642 [D loss: 0.659448, acc: 65.62%] [G loss: 1.913497]\n",
      "epoch:18 step:17643 [D loss: 0.702402, acc: 57.03%] [G loss: 1.933500]\n",
      "epoch:18 step:17644 [D loss: 0.581268, acc: 67.19%] [G loss: 2.002515]\n",
      "epoch:18 step:17645 [D loss: 0.661458, acc: 64.84%] [G loss: 1.755605]\n",
      "epoch:18 step:17646 [D loss: 0.601929, acc: 64.84%] [G loss: 1.988630]\n",
      "epoch:18 step:17647 [D loss: 0.602581, acc: 66.41%] [G loss: 1.905080]\n",
      "epoch:18 step:17648 [D loss: 0.619681, acc: 65.62%] [G loss: 1.989676]\n",
      "epoch:18 step:17649 [D loss: 0.662332, acc: 61.72%] [G loss: 1.811082]\n",
      "epoch:18 step:17650 [D loss: 0.656642, acc: 60.94%] [G loss: 1.853807]\n",
      "epoch:18 step:17651 [D loss: 0.664827, acc: 62.50%] [G loss: 1.966727]\n",
      "epoch:18 step:17652 [D loss: 0.651098, acc: 66.41%] [G loss: 2.042087]\n",
      "epoch:18 step:17653 [D loss: 0.643227, acc: 66.41%] [G loss: 1.821812]\n",
      "epoch:18 step:17654 [D loss: 0.649929, acc: 57.03%] [G loss: 1.769689]\n",
      "epoch:18 step:17655 [D loss: 0.696935, acc: 54.69%] [G loss: 1.805630]\n",
      "epoch:18 step:17656 [D loss: 0.597490, acc: 67.19%] [G loss: 1.893399]\n",
      "epoch:18 step:17657 [D loss: 0.635046, acc: 67.19%] [G loss: 1.957770]\n",
      "epoch:18 step:17658 [D loss: 0.603278, acc: 66.41%] [G loss: 1.969001]\n",
      "epoch:18 step:17659 [D loss: 0.596511, acc: 72.66%] [G loss: 1.909830]\n",
      "epoch:18 step:17660 [D loss: 0.675449, acc: 54.69%] [G loss: 1.901845]\n",
      "epoch:18 step:17661 [D loss: 0.649207, acc: 64.06%] [G loss: 1.977750]\n",
      "epoch:18 step:17662 [D loss: 0.672872, acc: 58.59%] [G loss: 1.905627]\n",
      "epoch:18 step:17663 [D loss: 0.688899, acc: 61.72%] [G loss: 1.838403]\n",
      "epoch:18 step:17664 [D loss: 0.650301, acc: 63.28%] [G loss: 1.909171]\n",
      "epoch:18 step:17665 [D loss: 0.647970, acc: 63.28%] [G loss: 1.981739]\n",
      "epoch:18 step:17666 [D loss: 0.757434, acc: 55.47%] [G loss: 1.805438]\n",
      "epoch:18 step:17667 [D loss: 0.684995, acc: 60.94%] [G loss: 1.735132]\n",
      "epoch:18 step:17668 [D loss: 0.643686, acc: 67.97%] [G loss: 1.834723]\n",
      "epoch:18 step:17669 [D loss: 0.597938, acc: 71.88%] [G loss: 1.877339]\n",
      "epoch:18 step:17670 [D loss: 0.630560, acc: 63.28%] [G loss: 1.880475]\n",
      "epoch:18 step:17671 [D loss: 0.644753, acc: 66.41%] [G loss: 1.922529]\n",
      "epoch:18 step:17672 [D loss: 0.591261, acc: 71.09%] [G loss: 1.933677]\n",
      "epoch:18 step:17673 [D loss: 0.663613, acc: 67.97%] [G loss: 2.019271]\n",
      "epoch:18 step:17674 [D loss: 0.636985, acc: 61.72%] [G loss: 1.848449]\n",
      "epoch:18 step:17675 [D loss: 0.663996, acc: 58.59%] [G loss: 1.921687]\n",
      "epoch:18 step:17676 [D loss: 0.652720, acc: 62.50%] [G loss: 1.983516]\n",
      "epoch:18 step:17677 [D loss: 0.635263, acc: 64.06%] [G loss: 1.804049]\n",
      "epoch:18 step:17678 [D loss: 0.682787, acc: 63.28%] [G loss: 1.834691]\n",
      "epoch:18 step:17679 [D loss: 0.692560, acc: 55.47%] [G loss: 1.930853]\n",
      "epoch:18 step:17680 [D loss: 0.624968, acc: 70.31%] [G loss: 1.945192]\n",
      "epoch:18 step:17681 [D loss: 0.605356, acc: 72.66%] [G loss: 2.207153]\n",
      "epoch:18 step:17682 [D loss: 0.583309, acc: 67.97%] [G loss: 2.053805]\n",
      "epoch:18 step:17683 [D loss: 0.665034, acc: 56.25%] [G loss: 1.885559]\n",
      "epoch:18 step:17684 [D loss: 0.723469, acc: 59.38%] [G loss: 1.711328]\n",
      "epoch:18 step:17685 [D loss: 0.627139, acc: 65.62%] [G loss: 1.824824]\n",
      "epoch:18 step:17686 [D loss: 0.723041, acc: 48.44%] [G loss: 1.739099]\n",
      "epoch:18 step:17687 [D loss: 0.680482, acc: 57.81%] [G loss: 1.722186]\n",
      "epoch:18 step:17688 [D loss: 0.662237, acc: 62.50%] [G loss: 1.826020]\n",
      "epoch:18 step:17689 [D loss: 0.606007, acc: 69.53%] [G loss: 1.841306]\n",
      "epoch:18 step:17690 [D loss: 0.613506, acc: 68.75%] [G loss: 1.834971]\n",
      "epoch:18 step:17691 [D loss: 0.555338, acc: 75.78%] [G loss: 1.942464]\n",
      "epoch:18 step:17692 [D loss: 0.653745, acc: 61.72%] [G loss: 1.929270]\n",
      "epoch:18 step:17693 [D loss: 0.707361, acc: 53.91%] [G loss: 1.825113]\n",
      "epoch:18 step:17694 [D loss: 0.663630, acc: 60.16%] [G loss: 1.718237]\n",
      "epoch:18 step:17695 [D loss: 0.649341, acc: 61.72%] [G loss: 1.780355]\n",
      "epoch:18 step:17696 [D loss: 0.642439, acc: 61.72%] [G loss: 1.742046]\n",
      "epoch:18 step:17697 [D loss: 0.595683, acc: 66.41%] [G loss: 1.990067]\n",
      "epoch:18 step:17698 [D loss: 0.602763, acc: 66.41%] [G loss: 1.887116]\n",
      "epoch:18 step:17699 [D loss: 0.607134, acc: 67.97%] [G loss: 2.154399]\n",
      "epoch:18 step:17700 [D loss: 0.654828, acc: 60.16%] [G loss: 1.855093]\n",
      "epoch:18 step:17701 [D loss: 0.623588, acc: 70.31%] [G loss: 1.951145]\n",
      "epoch:18 step:17702 [D loss: 0.608267, acc: 64.84%] [G loss: 1.836107]\n",
      "epoch:18 step:17703 [D loss: 0.622753, acc: 60.16%] [G loss: 1.940857]\n",
      "epoch:18 step:17704 [D loss: 0.627616, acc: 68.75%] [G loss: 2.028522]\n",
      "epoch:18 step:17705 [D loss: 0.627319, acc: 67.97%] [G loss: 1.901648]\n",
      "epoch:18 step:17706 [D loss: 0.616879, acc: 65.62%] [G loss: 1.880965]\n",
      "epoch:18 step:17707 [D loss: 0.644268, acc: 62.50%] [G loss: 1.886769]\n",
      "epoch:18 step:17708 [D loss: 0.681837, acc: 60.16%] [G loss: 2.091628]\n",
      "epoch:18 step:17709 [D loss: 0.625684, acc: 67.97%] [G loss: 1.924072]\n",
      "epoch:18 step:17710 [D loss: 0.632839, acc: 60.94%] [G loss: 1.985779]\n",
      "epoch:18 step:17711 [D loss: 0.649295, acc: 63.28%] [G loss: 1.968393]\n",
      "epoch:18 step:17712 [D loss: 0.683701, acc: 57.03%] [G loss: 1.811756]\n",
      "epoch:18 step:17713 [D loss: 0.634309, acc: 60.16%] [G loss: 1.881042]\n",
      "epoch:18 step:17714 [D loss: 0.646501, acc: 63.28%] [G loss: 1.856827]\n",
      "epoch:18 step:17715 [D loss: 0.619569, acc: 60.94%] [G loss: 2.020058]\n",
      "epoch:18 step:17716 [D loss: 0.655900, acc: 56.25%] [G loss: 1.901831]\n",
      "epoch:18 step:17717 [D loss: 0.601276, acc: 69.53%] [G loss: 2.077970]\n",
      "epoch:18 step:17718 [D loss: 0.643729, acc: 66.41%] [G loss: 2.185977]\n",
      "epoch:18 step:17719 [D loss: 0.625453, acc: 63.28%] [G loss: 1.954353]\n",
      "epoch:18 step:17720 [D loss: 0.637764, acc: 66.41%] [G loss: 1.930609]\n",
      "epoch:18 step:17721 [D loss: 0.678456, acc: 60.16%] [G loss: 1.880871]\n",
      "epoch:18 step:17722 [D loss: 0.652139, acc: 63.28%] [G loss: 1.760558]\n",
      "epoch:18 step:17723 [D loss: 0.673347, acc: 58.59%] [G loss: 1.972355]\n",
      "epoch:18 step:17724 [D loss: 0.716925, acc: 52.34%] [G loss: 1.888783]\n",
      "epoch:18 step:17725 [D loss: 0.663272, acc: 64.84%] [G loss: 1.842838]\n",
      "epoch:18 step:17726 [D loss: 0.667259, acc: 61.72%] [G loss: 1.856595]\n",
      "epoch:18 step:17727 [D loss: 0.662214, acc: 64.84%] [G loss: 1.763176]\n",
      "epoch:18 step:17728 [D loss: 0.676929, acc: 63.28%] [G loss: 1.801111]\n",
      "epoch:18 step:17729 [D loss: 0.676668, acc: 60.16%] [G loss: 1.841127]\n",
      "epoch:18 step:17730 [D loss: 0.667408, acc: 59.38%] [G loss: 1.854205]\n",
      "epoch:18 step:17731 [D loss: 0.661108, acc: 58.59%] [G loss: 1.868993]\n",
      "epoch:18 step:17732 [D loss: 0.679488, acc: 55.47%] [G loss: 1.807922]\n",
      "epoch:18 step:17733 [D loss: 0.657012, acc: 66.41%] [G loss: 1.687383]\n",
      "epoch:18 step:17734 [D loss: 0.629660, acc: 67.19%] [G loss: 1.823349]\n",
      "epoch:18 step:17735 [D loss: 0.691712, acc: 55.47%] [G loss: 1.760195]\n",
      "epoch:18 step:17736 [D loss: 0.679555, acc: 57.03%] [G loss: 1.844531]\n",
      "epoch:18 step:17737 [D loss: 0.652003, acc: 61.72%] [G loss: 1.910589]\n",
      "epoch:18 step:17738 [D loss: 0.606305, acc: 66.41%] [G loss: 1.937317]\n",
      "epoch:18 step:17739 [D loss: 0.663670, acc: 60.16%] [G loss: 1.729087]\n",
      "epoch:18 step:17740 [D loss: 0.680657, acc: 59.38%] [G loss: 1.805258]\n",
      "epoch:18 step:17741 [D loss: 0.628309, acc: 67.97%] [G loss: 1.858583]\n",
      "epoch:18 step:17742 [D loss: 0.633838, acc: 60.94%] [G loss: 1.792412]\n",
      "epoch:18 step:17743 [D loss: 0.647490, acc: 60.16%] [G loss: 1.828465]\n",
      "epoch:18 step:17744 [D loss: 0.610956, acc: 63.28%] [G loss: 1.936145]\n",
      "epoch:18 step:17745 [D loss: 0.630976, acc: 64.84%] [G loss: 1.836097]\n",
      "epoch:18 step:17746 [D loss: 0.665244, acc: 63.28%] [G loss: 1.867811]\n",
      "epoch:18 step:17747 [D loss: 0.651981, acc: 61.72%] [G loss: 1.907824]\n",
      "epoch:18 step:17748 [D loss: 0.646502, acc: 64.06%] [G loss: 1.924299]\n",
      "epoch:18 step:17749 [D loss: 0.617071, acc: 64.84%] [G loss: 1.834265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17750 [D loss: 0.590376, acc: 66.41%] [G loss: 1.896389]\n",
      "epoch:18 step:17751 [D loss: 0.645892, acc: 55.47%] [G loss: 1.956749]\n",
      "epoch:18 step:17752 [D loss: 0.604387, acc: 68.75%] [G loss: 2.200793]\n",
      "epoch:18 step:17753 [D loss: 0.670074, acc: 64.84%] [G loss: 1.828743]\n",
      "epoch:18 step:17754 [D loss: 0.641064, acc: 65.62%] [G loss: 1.861257]\n",
      "epoch:18 step:17755 [D loss: 0.641507, acc: 66.41%] [G loss: 1.926617]\n",
      "epoch:18 step:17756 [D loss: 0.667240, acc: 57.03%] [G loss: 1.911682]\n",
      "epoch:18 step:17757 [D loss: 0.690501, acc: 55.47%] [G loss: 1.792419]\n",
      "epoch:18 step:17758 [D loss: 0.634137, acc: 67.97%] [G loss: 1.911168]\n",
      "epoch:18 step:17759 [D loss: 0.682202, acc: 57.81%] [G loss: 1.839190]\n",
      "epoch:18 step:17760 [D loss: 0.601334, acc: 64.84%] [G loss: 1.981675]\n",
      "epoch:18 step:17761 [D loss: 0.634266, acc: 60.94%] [G loss: 1.965502]\n",
      "epoch:18 step:17762 [D loss: 0.669176, acc: 66.41%] [G loss: 1.849127]\n",
      "epoch:18 step:17763 [D loss: 0.654432, acc: 59.38%] [G loss: 1.950908]\n",
      "epoch:18 step:17764 [D loss: 0.636899, acc: 67.19%] [G loss: 1.967621]\n",
      "epoch:18 step:17765 [D loss: 0.612837, acc: 64.06%] [G loss: 2.033370]\n",
      "epoch:18 step:17766 [D loss: 0.649402, acc: 64.84%] [G loss: 1.946752]\n",
      "epoch:18 step:17767 [D loss: 0.601341, acc: 66.41%] [G loss: 1.905486]\n",
      "epoch:18 step:17768 [D loss: 0.631776, acc: 63.28%] [G loss: 1.896754]\n",
      "epoch:18 step:17769 [D loss: 0.656617, acc: 59.38%] [G loss: 1.851162]\n",
      "epoch:18 step:17770 [D loss: 0.672008, acc: 60.94%] [G loss: 1.936804]\n",
      "epoch:18 step:17771 [D loss: 0.661052, acc: 59.38%] [G loss: 1.915948]\n",
      "epoch:18 step:17772 [D loss: 0.623027, acc: 71.09%] [G loss: 1.971392]\n",
      "epoch:18 step:17773 [D loss: 0.616118, acc: 67.97%] [G loss: 1.932277]\n",
      "epoch:18 step:17774 [D loss: 0.631503, acc: 64.06%] [G loss: 2.000509]\n",
      "epoch:18 step:17775 [D loss: 0.620944, acc: 67.19%] [G loss: 1.933790]\n",
      "epoch:18 step:17776 [D loss: 0.631820, acc: 64.06%] [G loss: 1.923552]\n",
      "epoch:18 step:17777 [D loss: 0.659392, acc: 64.84%] [G loss: 2.203083]\n",
      "epoch:18 step:17778 [D loss: 0.604886, acc: 68.75%] [G loss: 2.104731]\n",
      "epoch:18 step:17779 [D loss: 0.693966, acc: 53.91%] [G loss: 1.900390]\n",
      "epoch:18 step:17780 [D loss: 0.659209, acc: 59.38%] [G loss: 1.846728]\n",
      "epoch:18 step:17781 [D loss: 0.609848, acc: 69.53%] [G loss: 1.910361]\n",
      "epoch:18 step:17782 [D loss: 0.622466, acc: 64.06%] [G loss: 2.015438]\n",
      "epoch:18 step:17783 [D loss: 0.628460, acc: 65.62%] [G loss: 2.045802]\n",
      "epoch:18 step:17784 [D loss: 0.610489, acc: 66.41%] [G loss: 2.177672]\n",
      "epoch:18 step:17785 [D loss: 0.596901, acc: 67.97%] [G loss: 2.252423]\n",
      "epoch:18 step:17786 [D loss: 0.741656, acc: 48.44%] [G loss: 1.891804]\n",
      "epoch:18 step:17787 [D loss: 0.626959, acc: 66.41%] [G loss: 2.060344]\n",
      "epoch:18 step:17788 [D loss: 0.673513, acc: 60.16%] [G loss: 1.960967]\n",
      "epoch:18 step:17789 [D loss: 0.624058, acc: 65.62%] [G loss: 2.208587]\n",
      "epoch:18 step:17790 [D loss: 0.578339, acc: 75.00%] [G loss: 2.187864]\n",
      "epoch:18 step:17791 [D loss: 0.587484, acc: 69.53%] [G loss: 2.250873]\n",
      "epoch:18 step:17792 [D loss: 0.614753, acc: 67.19%] [G loss: 2.214649]\n",
      "epoch:18 step:17793 [D loss: 0.613618, acc: 66.41%] [G loss: 2.192053]\n",
      "epoch:18 step:17794 [D loss: 0.786207, acc: 53.12%] [G loss: 1.772917]\n",
      "epoch:18 step:17795 [D loss: 0.726807, acc: 50.78%] [G loss: 1.902335]\n",
      "epoch:18 step:17796 [D loss: 0.595161, acc: 75.00%] [G loss: 2.006731]\n",
      "epoch:18 step:17797 [D loss: 0.576321, acc: 75.00%] [G loss: 2.083814]\n",
      "epoch:18 step:17798 [D loss: 0.604068, acc: 64.84%] [G loss: 2.056816]\n",
      "epoch:18 step:17799 [D loss: 0.651852, acc: 60.94%] [G loss: 1.934368]\n",
      "epoch:18 step:17800 [D loss: 0.621301, acc: 61.72%] [G loss: 1.969042]\n",
      "##############\n",
      "[2.40159554 1.51639651 6.30415931 4.7857019  3.87372224 5.63879563\n",
      " 4.40356471 4.64855359 4.64703564 3.47458517]\n",
      "##########\n",
      "epoch:18 step:17801 [D loss: 0.577718, acc: 67.19%] [G loss: 2.105811]\n",
      "epoch:18 step:17802 [D loss: 0.559978, acc: 73.44%] [G loss: 2.050259]\n",
      "epoch:18 step:17803 [D loss: 0.613348, acc: 73.44%] [G loss: 2.550314]\n",
      "epoch:19 step:17804 [D loss: 0.751607, acc: 54.69%] [G loss: 1.979872]\n",
      "epoch:19 step:17805 [D loss: 0.631954, acc: 66.41%] [G loss: 2.021941]\n",
      "epoch:19 step:17806 [D loss: 0.667464, acc: 61.72%] [G loss: 1.956831]\n",
      "epoch:19 step:17807 [D loss: 0.686210, acc: 53.12%] [G loss: 1.903975]\n",
      "epoch:19 step:17808 [D loss: 0.613156, acc: 62.50%] [G loss: 1.844792]\n",
      "epoch:19 step:17809 [D loss: 0.608186, acc: 68.75%] [G loss: 2.028716]\n",
      "epoch:19 step:17810 [D loss: 0.637976, acc: 61.72%] [G loss: 1.915276]\n",
      "epoch:19 step:17811 [D loss: 0.656359, acc: 60.16%] [G loss: 1.844763]\n",
      "epoch:19 step:17812 [D loss: 0.592802, acc: 71.09%] [G loss: 2.158516]\n",
      "epoch:19 step:17813 [D loss: 0.609418, acc: 61.72%] [G loss: 2.122575]\n",
      "epoch:19 step:17814 [D loss: 0.708227, acc: 57.81%] [G loss: 1.878536]\n",
      "epoch:19 step:17815 [D loss: 0.639701, acc: 62.50%] [G loss: 1.872044]\n",
      "epoch:19 step:17816 [D loss: 0.643528, acc: 64.06%] [G loss: 1.749555]\n",
      "epoch:19 step:17817 [D loss: 0.617655, acc: 64.06%] [G loss: 1.904235]\n",
      "epoch:19 step:17818 [D loss: 0.559834, acc: 71.09%] [G loss: 2.146026]\n",
      "epoch:19 step:17819 [D loss: 0.582270, acc: 66.41%] [G loss: 2.115032]\n",
      "epoch:19 step:17820 [D loss: 0.633592, acc: 61.72%] [G loss: 1.956606]\n",
      "epoch:19 step:17821 [D loss: 0.631698, acc: 65.62%] [G loss: 1.893858]\n",
      "epoch:19 step:17822 [D loss: 0.720830, acc: 47.66%] [G loss: 1.986551]\n",
      "epoch:19 step:17823 [D loss: 0.680529, acc: 57.03%] [G loss: 1.793612]\n",
      "epoch:19 step:17824 [D loss: 0.677911, acc: 57.81%] [G loss: 1.922756]\n",
      "epoch:19 step:17825 [D loss: 0.641638, acc: 62.50%] [G loss: 1.839513]\n",
      "epoch:19 step:17826 [D loss: 0.628104, acc: 64.06%] [G loss: 1.956464]\n",
      "epoch:19 step:17827 [D loss: 0.598687, acc: 68.75%] [G loss: 1.919450]\n",
      "epoch:19 step:17828 [D loss: 0.624838, acc: 60.94%] [G loss: 2.021285]\n",
      "epoch:19 step:17829 [D loss: 0.646340, acc: 62.50%] [G loss: 1.816284]\n",
      "epoch:19 step:17830 [D loss: 0.701797, acc: 53.91%] [G loss: 1.780312]\n",
      "epoch:19 step:17831 [D loss: 0.655259, acc: 57.03%] [G loss: 1.893585]\n",
      "epoch:19 step:17832 [D loss: 0.670786, acc: 59.38%] [G loss: 1.846601]\n",
      "epoch:19 step:17833 [D loss: 0.636932, acc: 63.28%] [G loss: 1.980398]\n",
      "epoch:19 step:17834 [D loss: 0.695667, acc: 60.16%] [G loss: 1.694255]\n",
      "epoch:19 step:17835 [D loss: 0.646501, acc: 62.50%] [G loss: 1.728045]\n",
      "epoch:19 step:17836 [D loss: 0.660559, acc: 60.94%] [G loss: 1.789310]\n",
      "epoch:19 step:17837 [D loss: 0.676677, acc: 64.06%] [G loss: 1.822205]\n",
      "epoch:19 step:17838 [D loss: 0.629939, acc: 63.28%] [G loss: 1.806109]\n",
      "epoch:19 step:17839 [D loss: 0.674390, acc: 57.03%] [G loss: 1.925112]\n",
      "epoch:19 step:17840 [D loss: 0.652332, acc: 60.94%] [G loss: 1.902142]\n",
      "epoch:19 step:17841 [D loss: 0.644632, acc: 62.50%] [G loss: 2.005777]\n",
      "epoch:19 step:17842 [D loss: 0.628416, acc: 66.41%] [G loss: 1.851684]\n",
      "epoch:19 step:17843 [D loss: 0.652874, acc: 59.38%] [G loss: 2.019259]\n",
      "epoch:19 step:17844 [D loss: 0.618979, acc: 69.53%] [G loss: 1.911678]\n",
      "epoch:19 step:17845 [D loss: 0.631838, acc: 61.72%] [G loss: 1.975736]\n",
      "epoch:19 step:17846 [D loss: 0.642569, acc: 64.06%] [G loss: 1.887515]\n",
      "epoch:19 step:17847 [D loss: 0.656792, acc: 61.72%] [G loss: 1.844369]\n",
      "epoch:19 step:17848 [D loss: 0.691440, acc: 55.47%] [G loss: 2.018317]\n",
      "epoch:19 step:17849 [D loss: 0.659623, acc: 61.72%] [G loss: 1.806606]\n",
      "epoch:19 step:17850 [D loss: 0.629937, acc: 67.19%] [G loss: 2.010521]\n",
      "epoch:19 step:17851 [D loss: 0.623020, acc: 67.97%] [G loss: 2.035768]\n",
      "epoch:19 step:17852 [D loss: 0.568143, acc: 73.44%] [G loss: 1.976470]\n",
      "epoch:19 step:17853 [D loss: 0.653148, acc: 63.28%] [G loss: 1.951399]\n",
      "epoch:19 step:17854 [D loss: 0.644170, acc: 60.16%] [G loss: 1.808079]\n",
      "epoch:19 step:17855 [D loss: 0.606542, acc: 69.53%] [G loss: 1.888245]\n",
      "epoch:19 step:17856 [D loss: 0.672030, acc: 61.72%] [G loss: 1.975950]\n",
      "epoch:19 step:17857 [D loss: 0.633016, acc: 62.50%] [G loss: 2.002009]\n",
      "epoch:19 step:17858 [D loss: 0.604085, acc: 69.53%] [G loss: 1.985714]\n",
      "epoch:19 step:17859 [D loss: 0.633114, acc: 61.72%] [G loss: 1.947271]\n",
      "epoch:19 step:17860 [D loss: 0.686537, acc: 60.94%] [G loss: 1.903489]\n",
      "epoch:19 step:17861 [D loss: 0.641665, acc: 65.62%] [G loss: 1.859896]\n",
      "epoch:19 step:17862 [D loss: 0.663507, acc: 62.50%] [G loss: 2.037710]\n",
      "epoch:19 step:17863 [D loss: 0.636774, acc: 60.94%] [G loss: 1.800111]\n",
      "epoch:19 step:17864 [D loss: 0.651115, acc: 60.16%] [G loss: 1.887399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17865 [D loss: 0.634578, acc: 64.84%] [G loss: 1.930184]\n",
      "epoch:19 step:17866 [D loss: 0.648810, acc: 59.38%] [G loss: 1.748061]\n",
      "epoch:19 step:17867 [D loss: 0.592953, acc: 67.97%] [G loss: 1.972369]\n",
      "epoch:19 step:17868 [D loss: 0.670556, acc: 60.94%] [G loss: 1.954963]\n",
      "epoch:19 step:17869 [D loss: 0.661387, acc: 60.94%] [G loss: 1.964469]\n",
      "epoch:19 step:17870 [D loss: 0.617048, acc: 70.31%] [G loss: 1.791066]\n",
      "epoch:19 step:17871 [D loss: 0.660570, acc: 61.72%] [G loss: 2.025425]\n",
      "epoch:19 step:17872 [D loss: 0.604570, acc: 65.62%] [G loss: 2.011009]\n",
      "epoch:19 step:17873 [D loss: 0.629860, acc: 63.28%] [G loss: 1.993968]\n",
      "epoch:19 step:17874 [D loss: 0.652157, acc: 60.94%] [G loss: 1.831460]\n",
      "epoch:19 step:17875 [D loss: 0.644890, acc: 65.62%] [G loss: 1.854390]\n",
      "epoch:19 step:17876 [D loss: 0.679353, acc: 58.59%] [G loss: 1.805774]\n",
      "epoch:19 step:17877 [D loss: 0.657302, acc: 58.59%] [G loss: 1.893219]\n",
      "epoch:19 step:17878 [D loss: 0.617433, acc: 64.84%] [G loss: 1.997837]\n",
      "epoch:19 step:17879 [D loss: 0.603138, acc: 71.09%] [G loss: 2.025301]\n",
      "epoch:19 step:17880 [D loss: 0.582760, acc: 66.41%] [G loss: 1.991486]\n",
      "epoch:19 step:17881 [D loss: 0.698279, acc: 57.03%] [G loss: 1.799989]\n",
      "epoch:19 step:17882 [D loss: 0.709250, acc: 57.81%] [G loss: 1.891941]\n",
      "epoch:19 step:17883 [D loss: 0.686731, acc: 60.16%] [G loss: 1.857555]\n",
      "epoch:19 step:17884 [D loss: 0.674968, acc: 52.34%] [G loss: 1.746473]\n",
      "epoch:19 step:17885 [D loss: 0.652439, acc: 58.59%] [G loss: 1.810493]\n",
      "epoch:19 step:17886 [D loss: 0.583103, acc: 67.19%] [G loss: 1.972215]\n",
      "epoch:19 step:17887 [D loss: 0.676418, acc: 59.38%] [G loss: 1.908542]\n",
      "epoch:19 step:17888 [D loss: 0.608642, acc: 66.41%] [G loss: 1.756432]\n",
      "epoch:19 step:17889 [D loss: 0.673732, acc: 61.72%] [G loss: 1.690197]\n",
      "epoch:19 step:17890 [D loss: 0.619928, acc: 69.53%] [G loss: 1.832987]\n",
      "epoch:19 step:17891 [D loss: 0.622944, acc: 66.41%] [G loss: 1.884073]\n",
      "epoch:19 step:17892 [D loss: 0.661541, acc: 63.28%] [G loss: 1.848818]\n",
      "epoch:19 step:17893 [D loss: 0.640689, acc: 63.28%] [G loss: 1.965465]\n",
      "epoch:19 step:17894 [D loss: 0.642706, acc: 62.50%] [G loss: 1.816218]\n",
      "epoch:19 step:17895 [D loss: 0.657851, acc: 64.84%] [G loss: 2.026974]\n",
      "epoch:19 step:17896 [D loss: 0.633393, acc: 61.72%] [G loss: 1.887093]\n",
      "epoch:19 step:17897 [D loss: 0.629419, acc: 63.28%] [G loss: 1.872298]\n",
      "epoch:19 step:17898 [D loss: 0.629484, acc: 64.06%] [G loss: 1.835350]\n",
      "epoch:19 step:17899 [D loss: 0.623057, acc: 65.62%] [G loss: 1.822852]\n",
      "epoch:19 step:17900 [D loss: 0.611068, acc: 63.28%] [G loss: 1.987137]\n",
      "epoch:19 step:17901 [D loss: 0.640155, acc: 66.41%] [G loss: 1.796269]\n",
      "epoch:19 step:17902 [D loss: 0.632060, acc: 58.59%] [G loss: 1.894088]\n",
      "epoch:19 step:17903 [D loss: 0.621499, acc: 65.62%] [G loss: 1.918897]\n",
      "epoch:19 step:17904 [D loss: 0.652896, acc: 59.38%] [G loss: 1.856868]\n",
      "epoch:19 step:17905 [D loss: 0.690734, acc: 60.94%] [G loss: 1.831925]\n",
      "epoch:19 step:17906 [D loss: 0.605764, acc: 72.66%] [G loss: 1.954745]\n",
      "epoch:19 step:17907 [D loss: 0.619674, acc: 63.28%] [G loss: 1.866274]\n",
      "epoch:19 step:17908 [D loss: 0.670184, acc: 59.38%] [G loss: 1.897795]\n",
      "epoch:19 step:17909 [D loss: 0.613425, acc: 70.31%] [G loss: 2.087730]\n",
      "epoch:19 step:17910 [D loss: 0.634136, acc: 64.06%] [G loss: 2.262030]\n",
      "epoch:19 step:17911 [D loss: 0.644145, acc: 64.06%] [G loss: 1.928591]\n",
      "epoch:19 step:17912 [D loss: 0.666186, acc: 56.25%] [G loss: 1.795334]\n",
      "epoch:19 step:17913 [D loss: 0.687659, acc: 61.72%] [G loss: 1.919693]\n",
      "epoch:19 step:17914 [D loss: 0.643502, acc: 67.19%] [G loss: 1.867309]\n",
      "epoch:19 step:17915 [D loss: 0.649673, acc: 62.50%] [G loss: 1.946533]\n",
      "epoch:19 step:17916 [D loss: 0.626057, acc: 64.06%] [G loss: 1.925996]\n",
      "epoch:19 step:17917 [D loss: 0.621726, acc: 66.41%] [G loss: 2.015595]\n",
      "epoch:19 step:17918 [D loss: 0.631628, acc: 61.72%] [G loss: 2.098856]\n",
      "epoch:19 step:17919 [D loss: 0.555464, acc: 71.09%] [G loss: 2.290892]\n",
      "epoch:19 step:17920 [D loss: 0.660776, acc: 64.06%] [G loss: 2.068125]\n",
      "epoch:19 step:17921 [D loss: 0.598031, acc: 65.62%] [G loss: 1.998778]\n",
      "epoch:19 step:17922 [D loss: 0.632718, acc: 70.31%] [G loss: 2.185749]\n",
      "epoch:19 step:17923 [D loss: 0.648066, acc: 61.72%] [G loss: 1.949716]\n",
      "epoch:19 step:17924 [D loss: 0.633180, acc: 59.38%] [G loss: 1.910533]\n",
      "epoch:19 step:17925 [D loss: 0.684724, acc: 60.16%] [G loss: 2.004820]\n",
      "epoch:19 step:17926 [D loss: 0.648548, acc: 66.41%] [G loss: 2.069502]\n",
      "epoch:19 step:17927 [D loss: 0.688975, acc: 60.16%] [G loss: 1.888614]\n",
      "epoch:19 step:17928 [D loss: 0.690984, acc: 57.81%] [G loss: 1.875650]\n",
      "epoch:19 step:17929 [D loss: 0.672940, acc: 63.28%] [G loss: 2.034935]\n",
      "epoch:19 step:17930 [D loss: 0.679438, acc: 60.16%] [G loss: 1.850801]\n",
      "epoch:19 step:17931 [D loss: 0.608552, acc: 67.97%] [G loss: 1.912757]\n",
      "epoch:19 step:17932 [D loss: 0.633772, acc: 63.28%] [G loss: 1.838012]\n",
      "epoch:19 step:17933 [D loss: 0.622649, acc: 64.84%] [G loss: 1.873505]\n",
      "epoch:19 step:17934 [D loss: 0.585723, acc: 70.31%] [G loss: 2.069267]\n",
      "epoch:19 step:17935 [D loss: 0.628907, acc: 58.59%] [G loss: 1.823152]\n",
      "epoch:19 step:17936 [D loss: 0.712751, acc: 50.78%] [G loss: 1.829862]\n",
      "epoch:19 step:17937 [D loss: 0.674814, acc: 56.25%] [G loss: 1.837301]\n",
      "epoch:19 step:17938 [D loss: 0.639342, acc: 63.28%] [G loss: 1.781206]\n",
      "epoch:19 step:17939 [D loss: 0.668456, acc: 54.69%] [G loss: 1.907732]\n",
      "epoch:19 step:17940 [D loss: 0.629359, acc: 65.62%] [G loss: 1.901511]\n",
      "epoch:19 step:17941 [D loss: 0.686494, acc: 55.47%] [G loss: 1.777688]\n",
      "epoch:19 step:17942 [D loss: 0.650247, acc: 63.28%] [G loss: 1.786202]\n",
      "epoch:19 step:17943 [D loss: 0.650728, acc: 62.50%] [G loss: 1.816458]\n",
      "epoch:19 step:17944 [D loss: 0.659986, acc: 62.50%] [G loss: 1.797538]\n",
      "epoch:19 step:17945 [D loss: 0.641932, acc: 59.38%] [G loss: 1.848322]\n",
      "epoch:19 step:17946 [D loss: 0.701832, acc: 53.91%] [G loss: 1.748576]\n",
      "epoch:19 step:17947 [D loss: 0.677017, acc: 59.38%] [G loss: 1.904377]\n",
      "epoch:19 step:17948 [D loss: 0.665891, acc: 57.81%] [G loss: 1.895630]\n",
      "epoch:19 step:17949 [D loss: 0.589219, acc: 64.84%] [G loss: 1.952383]\n",
      "epoch:19 step:17950 [D loss: 0.625757, acc: 67.19%] [G loss: 1.877807]\n",
      "epoch:19 step:17951 [D loss: 0.677851, acc: 55.47%] [G loss: 1.764629]\n",
      "epoch:19 step:17952 [D loss: 0.611152, acc: 70.31%] [G loss: 1.960653]\n",
      "epoch:19 step:17953 [D loss: 0.646552, acc: 60.94%] [G loss: 1.962132]\n",
      "epoch:19 step:17954 [D loss: 0.590065, acc: 65.62%] [G loss: 1.912144]\n",
      "epoch:19 step:17955 [D loss: 0.617294, acc: 59.38%] [G loss: 1.969551]\n",
      "epoch:19 step:17956 [D loss: 0.667439, acc: 60.94%] [G loss: 1.896578]\n",
      "epoch:19 step:17957 [D loss: 0.673875, acc: 58.59%] [G loss: 1.899270]\n",
      "epoch:19 step:17958 [D loss: 0.645441, acc: 62.50%] [G loss: 1.938724]\n",
      "epoch:19 step:17959 [D loss: 0.621936, acc: 68.75%] [G loss: 1.928376]\n",
      "epoch:19 step:17960 [D loss: 0.649365, acc: 64.84%] [G loss: 1.840138]\n",
      "epoch:19 step:17961 [D loss: 0.630006, acc: 63.28%] [G loss: 1.783890]\n",
      "epoch:19 step:17962 [D loss: 0.655711, acc: 63.28%] [G loss: 1.896762]\n",
      "epoch:19 step:17963 [D loss: 0.695492, acc: 59.38%] [G loss: 1.797205]\n",
      "epoch:19 step:17964 [D loss: 0.634535, acc: 67.19%] [G loss: 1.953458]\n",
      "epoch:19 step:17965 [D loss: 0.655755, acc: 61.72%] [G loss: 1.936698]\n",
      "epoch:19 step:17966 [D loss: 0.598540, acc: 71.09%] [G loss: 1.906229]\n",
      "epoch:19 step:17967 [D loss: 0.666749, acc: 62.50%] [G loss: 1.820840]\n",
      "epoch:19 step:17968 [D loss: 0.639228, acc: 60.94%] [G loss: 1.894983]\n",
      "epoch:19 step:17969 [D loss: 0.673448, acc: 57.81%] [G loss: 1.795937]\n",
      "epoch:19 step:17970 [D loss: 0.701096, acc: 60.94%] [G loss: 1.841063]\n",
      "epoch:19 step:17971 [D loss: 0.668154, acc: 60.16%] [G loss: 1.832890]\n",
      "epoch:19 step:17972 [D loss: 0.704862, acc: 54.69%] [G loss: 1.823785]\n",
      "epoch:19 step:17973 [D loss: 0.692035, acc: 57.81%] [G loss: 1.840756]\n",
      "epoch:19 step:17974 [D loss: 0.638598, acc: 64.06%] [G loss: 1.927837]\n",
      "epoch:19 step:17975 [D loss: 0.649745, acc: 60.16%] [G loss: 1.814268]\n",
      "epoch:19 step:17976 [D loss: 0.679860, acc: 60.16%] [G loss: 1.847173]\n",
      "epoch:19 step:17977 [D loss: 0.729447, acc: 55.47%] [G loss: 1.765084]\n",
      "epoch:19 step:17978 [D loss: 0.658862, acc: 59.38%] [G loss: 1.732630]\n",
      "epoch:19 step:17979 [D loss: 0.673693, acc: 57.81%] [G loss: 1.696836]\n",
      "epoch:19 step:17980 [D loss: 0.604708, acc: 66.41%] [G loss: 1.833435]\n",
      "epoch:19 step:17981 [D loss: 0.681984, acc: 60.94%] [G loss: 1.800701]\n",
      "epoch:19 step:17982 [D loss: 0.625731, acc: 64.84%] [G loss: 1.769279]\n",
      "epoch:19 step:17983 [D loss: 0.672585, acc: 59.38%] [G loss: 1.830353]\n",
      "epoch:19 step:17984 [D loss: 0.666569, acc: 60.16%] [G loss: 1.821201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17985 [D loss: 0.623817, acc: 64.06%] [G loss: 1.800253]\n",
      "epoch:19 step:17986 [D loss: 0.696555, acc: 53.12%] [G loss: 1.690193]\n",
      "epoch:19 step:17987 [D loss: 0.645460, acc: 59.38%] [G loss: 1.915451]\n",
      "epoch:19 step:17988 [D loss: 0.596560, acc: 74.22%] [G loss: 1.950262]\n",
      "epoch:19 step:17989 [D loss: 0.677878, acc: 59.38%] [G loss: 1.916715]\n",
      "epoch:19 step:17990 [D loss: 0.668983, acc: 57.81%] [G loss: 1.853395]\n",
      "epoch:19 step:17991 [D loss: 0.687845, acc: 57.03%] [G loss: 1.977365]\n",
      "epoch:19 step:17992 [D loss: 0.678369, acc: 62.50%] [G loss: 1.904281]\n",
      "epoch:19 step:17993 [D loss: 0.642800, acc: 64.84%] [G loss: 1.896896]\n",
      "epoch:19 step:17994 [D loss: 0.647475, acc: 64.84%] [G loss: 1.951932]\n",
      "epoch:19 step:17995 [D loss: 0.598389, acc: 66.41%] [G loss: 1.918308]\n",
      "epoch:19 step:17996 [D loss: 0.630259, acc: 61.72%] [G loss: 2.008029]\n",
      "epoch:19 step:17997 [D loss: 0.603083, acc: 71.09%] [G loss: 1.927019]\n",
      "epoch:19 step:17998 [D loss: 0.621462, acc: 66.41%] [G loss: 1.913114]\n",
      "epoch:19 step:17999 [D loss: 0.659516, acc: 62.50%] [G loss: 1.807885]\n",
      "epoch:19 step:18000 [D loss: 0.634057, acc: 64.84%] [G loss: 1.821571]\n",
      "##############\n",
      "[2.39518123 1.38438036 6.12676598 4.69428298 3.67548109 5.74664702\n",
      " 4.34557201 4.76425465 4.5032376  3.82981357]\n",
      "##########\n",
      "epoch:19 step:18001 [D loss: 0.637821, acc: 60.94%] [G loss: 1.854597]\n",
      "epoch:19 step:18002 [D loss: 0.630058, acc: 63.28%] [G loss: 1.898894]\n",
      "epoch:19 step:18003 [D loss: 0.659635, acc: 60.16%] [G loss: 1.725440]\n",
      "epoch:19 step:18004 [D loss: 0.666090, acc: 62.50%] [G loss: 2.056497]\n",
      "epoch:19 step:18005 [D loss: 0.694252, acc: 56.25%] [G loss: 1.850457]\n",
      "epoch:19 step:18006 [D loss: 0.674250, acc: 53.91%] [G loss: 1.771965]\n",
      "epoch:19 step:18007 [D loss: 0.619476, acc: 63.28%] [G loss: 1.872582]\n",
      "epoch:19 step:18008 [D loss: 0.656491, acc: 64.06%] [G loss: 1.827832]\n",
      "epoch:19 step:18009 [D loss: 0.607316, acc: 67.19%] [G loss: 1.940470]\n",
      "epoch:19 step:18010 [D loss: 0.596188, acc: 63.28%] [G loss: 2.092100]\n",
      "epoch:19 step:18011 [D loss: 0.621134, acc: 61.72%] [G loss: 2.116418]\n",
      "epoch:19 step:18012 [D loss: 0.593870, acc: 67.19%] [G loss: 2.262840]\n",
      "epoch:19 step:18013 [D loss: 0.672049, acc: 58.59%] [G loss: 1.911221]\n",
      "epoch:19 step:18014 [D loss: 0.621253, acc: 64.06%] [G loss: 1.808364]\n",
      "epoch:19 step:18015 [D loss: 0.678286, acc: 52.34%] [G loss: 1.816907]\n",
      "epoch:19 step:18016 [D loss: 0.629749, acc: 66.41%] [G loss: 1.785944]\n",
      "epoch:19 step:18017 [D loss: 0.657346, acc: 64.84%] [G loss: 1.859678]\n",
      "epoch:19 step:18018 [D loss: 0.678913, acc: 57.03%] [G loss: 1.902023]\n",
      "epoch:19 step:18019 [D loss: 0.625382, acc: 64.06%] [G loss: 2.209285]\n",
      "epoch:19 step:18020 [D loss: 0.588725, acc: 68.75%] [G loss: 1.947707]\n",
      "epoch:19 step:18021 [D loss: 0.585126, acc: 67.97%] [G loss: 2.157080]\n",
      "epoch:19 step:18022 [D loss: 0.635438, acc: 64.84%] [G loss: 2.299236]\n",
      "epoch:19 step:18023 [D loss: 0.648018, acc: 58.59%] [G loss: 1.865502]\n",
      "epoch:19 step:18024 [D loss: 0.649293, acc: 64.06%] [G loss: 1.842433]\n",
      "epoch:19 step:18025 [D loss: 0.638844, acc: 60.94%] [G loss: 2.062232]\n",
      "epoch:19 step:18026 [D loss: 0.643693, acc: 58.59%] [G loss: 1.927453]\n",
      "epoch:19 step:18027 [D loss: 0.688530, acc: 60.16%] [G loss: 1.814217]\n",
      "epoch:19 step:18028 [D loss: 0.650576, acc: 60.94%] [G loss: 1.926873]\n",
      "epoch:19 step:18029 [D loss: 0.664835, acc: 57.03%] [G loss: 1.961614]\n",
      "epoch:19 step:18030 [D loss: 0.634027, acc: 67.19%] [G loss: 1.948028]\n",
      "epoch:19 step:18031 [D loss: 0.690440, acc: 58.59%] [G loss: 1.905851]\n",
      "epoch:19 step:18032 [D loss: 0.577315, acc: 71.88%] [G loss: 2.063297]\n",
      "epoch:19 step:18033 [D loss: 0.596284, acc: 66.41%] [G loss: 2.150738]\n",
      "epoch:19 step:18034 [D loss: 0.640450, acc: 62.50%] [G loss: 2.276854]\n",
      "epoch:19 step:18035 [D loss: 0.602964, acc: 70.31%] [G loss: 2.438946]\n",
      "epoch:19 step:18036 [D loss: 0.670452, acc: 63.28%] [G loss: 1.874554]\n",
      "epoch:19 step:18037 [D loss: 0.660307, acc: 59.38%] [G loss: 1.887841]\n",
      "epoch:19 step:18038 [D loss: 0.664438, acc: 60.94%] [G loss: 1.817923]\n",
      "epoch:19 step:18039 [D loss: 0.651817, acc: 62.50%] [G loss: 1.822618]\n",
      "epoch:19 step:18040 [D loss: 0.666821, acc: 57.81%] [G loss: 1.828909]\n",
      "epoch:19 step:18041 [D loss: 0.641569, acc: 60.16%] [G loss: 1.815304]\n",
      "epoch:19 step:18042 [D loss: 0.646981, acc: 64.06%] [G loss: 2.000125]\n",
      "epoch:19 step:18043 [D loss: 0.638909, acc: 61.72%] [G loss: 1.953084]\n",
      "epoch:19 step:18044 [D loss: 0.596065, acc: 71.88%] [G loss: 1.941872]\n",
      "epoch:19 step:18045 [D loss: 0.578107, acc: 66.41%] [G loss: 2.062711]\n",
      "epoch:19 step:18046 [D loss: 0.657904, acc: 58.59%] [G loss: 1.986440]\n",
      "epoch:19 step:18047 [D loss: 0.631004, acc: 67.19%] [G loss: 2.057629]\n",
      "epoch:19 step:18048 [D loss: 0.592654, acc: 67.97%] [G loss: 1.948313]\n",
      "epoch:19 step:18049 [D loss: 0.710356, acc: 51.56%] [G loss: 2.048100]\n",
      "epoch:19 step:18050 [D loss: 0.668687, acc: 60.94%] [G loss: 1.943538]\n",
      "epoch:19 step:18051 [D loss: 0.629802, acc: 61.72%] [G loss: 1.985379]\n",
      "epoch:19 step:18052 [D loss: 0.693156, acc: 56.25%] [G loss: 1.912909]\n",
      "epoch:19 step:18053 [D loss: 0.711457, acc: 58.59%] [G loss: 1.687022]\n",
      "epoch:19 step:18054 [D loss: 0.729141, acc: 54.69%] [G loss: 1.807918]\n",
      "epoch:19 step:18055 [D loss: 0.681277, acc: 55.47%] [G loss: 1.814709]\n",
      "epoch:19 step:18056 [D loss: 0.686117, acc: 54.69%] [G loss: 1.901177]\n",
      "epoch:19 step:18057 [D loss: 0.623138, acc: 64.06%] [G loss: 1.908532]\n",
      "epoch:19 step:18058 [D loss: 0.648349, acc: 60.94%] [G loss: 1.861840]\n",
      "epoch:19 step:18059 [D loss: 0.656004, acc: 61.72%] [G loss: 1.914514]\n",
      "epoch:19 step:18060 [D loss: 0.690868, acc: 55.47%] [G loss: 1.753826]\n",
      "epoch:19 step:18061 [D loss: 0.704179, acc: 57.81%] [G loss: 1.836184]\n",
      "epoch:19 step:18062 [D loss: 0.641707, acc: 61.72%] [G loss: 1.765529]\n",
      "epoch:19 step:18063 [D loss: 0.627631, acc: 64.84%] [G loss: 1.801820]\n",
      "epoch:19 step:18064 [D loss: 0.644763, acc: 62.50%] [G loss: 1.865027]\n",
      "epoch:19 step:18065 [D loss: 0.619673, acc: 60.16%] [G loss: 1.895797]\n",
      "epoch:19 step:18066 [D loss: 0.666306, acc: 60.94%] [G loss: 1.933104]\n",
      "epoch:19 step:18067 [D loss: 0.636941, acc: 60.16%] [G loss: 2.058043]\n",
      "epoch:19 step:18068 [D loss: 0.603740, acc: 67.97%] [G loss: 1.911856]\n",
      "epoch:19 step:18069 [D loss: 0.643466, acc: 67.19%] [G loss: 1.901369]\n",
      "epoch:19 step:18070 [D loss: 0.641702, acc: 58.59%] [G loss: 1.779984]\n",
      "epoch:19 step:18071 [D loss: 0.724706, acc: 54.69%] [G loss: 1.899447]\n",
      "epoch:19 step:18072 [D loss: 0.618357, acc: 69.53%] [G loss: 1.861377]\n",
      "epoch:19 step:18073 [D loss: 0.656042, acc: 59.38%] [G loss: 1.860669]\n",
      "epoch:19 step:18074 [D loss: 0.588537, acc: 70.31%] [G loss: 2.016918]\n",
      "epoch:19 step:18075 [D loss: 0.631831, acc: 64.84%] [G loss: 1.811686]\n",
      "epoch:19 step:18076 [D loss: 0.644935, acc: 61.72%] [G loss: 1.906891]\n",
      "epoch:19 step:18077 [D loss: 0.612936, acc: 67.19%] [G loss: 1.998529]\n",
      "epoch:19 step:18078 [D loss: 0.628831, acc: 63.28%] [G loss: 1.990463]\n",
      "epoch:19 step:18079 [D loss: 0.610650, acc: 67.97%] [G loss: 2.163893]\n",
      "epoch:19 step:18080 [D loss: 0.641271, acc: 61.72%] [G loss: 1.932185]\n",
      "epoch:19 step:18081 [D loss: 0.643979, acc: 63.28%] [G loss: 1.804519]\n",
      "epoch:19 step:18082 [D loss: 0.638961, acc: 64.06%] [G loss: 1.921242]\n",
      "epoch:19 step:18083 [D loss: 0.623053, acc: 63.28%] [G loss: 1.883509]\n",
      "epoch:19 step:18084 [D loss: 0.674122, acc: 53.91%] [G loss: 1.797915]\n",
      "epoch:19 step:18085 [D loss: 0.671753, acc: 63.28%] [G loss: 1.775561]\n",
      "epoch:19 step:18086 [D loss: 0.678166, acc: 57.03%] [G loss: 1.801788]\n",
      "epoch:19 step:18087 [D loss: 0.652448, acc: 60.16%] [G loss: 1.946618]\n",
      "epoch:19 step:18088 [D loss: 0.678892, acc: 57.03%] [G loss: 1.793046]\n",
      "epoch:19 step:18089 [D loss: 0.624243, acc: 65.62%] [G loss: 2.031027]\n",
      "epoch:19 step:18090 [D loss: 0.637897, acc: 64.06%] [G loss: 1.985033]\n",
      "epoch:19 step:18091 [D loss: 0.673281, acc: 60.94%] [G loss: 1.785118]\n",
      "epoch:19 step:18092 [D loss: 0.619707, acc: 67.97%] [G loss: 1.954446]\n",
      "epoch:19 step:18093 [D loss: 0.603394, acc: 69.53%] [G loss: 1.851524]\n",
      "epoch:19 step:18094 [D loss: 0.659707, acc: 58.59%] [G loss: 1.932047]\n",
      "epoch:19 step:18095 [D loss: 0.642299, acc: 64.06%] [G loss: 1.851873]\n",
      "epoch:19 step:18096 [D loss: 0.603199, acc: 67.19%] [G loss: 2.128992]\n",
      "epoch:19 step:18097 [D loss: 0.664297, acc: 60.94%] [G loss: 1.842751]\n",
      "epoch:19 step:18098 [D loss: 0.722274, acc: 52.34%] [G loss: 1.837427]\n",
      "epoch:19 step:18099 [D loss: 0.624997, acc: 64.84%] [G loss: 2.105210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18100 [D loss: 0.654545, acc: 59.38%] [G loss: 1.898607]\n",
      "epoch:19 step:18101 [D loss: 0.640374, acc: 66.41%] [G loss: 1.890968]\n",
      "epoch:19 step:18102 [D loss: 0.634227, acc: 69.53%] [G loss: 2.039066]\n",
      "epoch:19 step:18103 [D loss: 0.637023, acc: 64.84%] [G loss: 1.981930]\n",
      "epoch:19 step:18104 [D loss: 0.649057, acc: 65.62%] [G loss: 1.872052]\n",
      "epoch:19 step:18105 [D loss: 0.636261, acc: 58.59%] [G loss: 1.988995]\n",
      "epoch:19 step:18106 [D loss: 0.643906, acc: 64.06%] [G loss: 1.850971]\n",
      "epoch:19 step:18107 [D loss: 0.665840, acc: 61.72%] [G loss: 1.748978]\n",
      "epoch:19 step:18108 [D loss: 0.646977, acc: 69.53%] [G loss: 1.863777]\n",
      "epoch:19 step:18109 [D loss: 0.587043, acc: 71.88%] [G loss: 1.908595]\n",
      "epoch:19 step:18110 [D loss: 0.649827, acc: 66.41%] [G loss: 1.882270]\n",
      "epoch:19 step:18111 [D loss: 0.702542, acc: 52.34%] [G loss: 1.875491]\n",
      "epoch:19 step:18112 [D loss: 0.580290, acc: 68.75%] [G loss: 1.863822]\n",
      "epoch:19 step:18113 [D loss: 0.631138, acc: 63.28%] [G loss: 1.762730]\n",
      "epoch:19 step:18114 [D loss: 0.605652, acc: 63.28%] [G loss: 1.807229]\n",
      "epoch:19 step:18115 [D loss: 0.585416, acc: 74.22%] [G loss: 2.047562]\n",
      "epoch:19 step:18116 [D loss: 0.609362, acc: 61.72%] [G loss: 1.995146]\n",
      "epoch:19 step:18117 [D loss: 0.565052, acc: 70.31%] [G loss: 1.975320]\n",
      "epoch:19 step:18118 [D loss: 0.604663, acc: 64.06%] [G loss: 2.157144]\n",
      "epoch:19 step:18119 [D loss: 0.651593, acc: 62.50%] [G loss: 1.701326]\n",
      "epoch:19 step:18120 [D loss: 0.670650, acc: 57.81%] [G loss: 1.725259]\n",
      "epoch:19 step:18121 [D loss: 0.643088, acc: 64.84%] [G loss: 1.996584]\n",
      "epoch:19 step:18122 [D loss: 0.620207, acc: 69.53%] [G loss: 1.902126]\n",
      "epoch:19 step:18123 [D loss: 0.724645, acc: 51.56%] [G loss: 1.861132]\n",
      "epoch:19 step:18124 [D loss: 0.639678, acc: 63.28%] [G loss: 2.081287]\n",
      "epoch:19 step:18125 [D loss: 0.670904, acc: 57.81%] [G loss: 1.756570]\n",
      "epoch:19 step:18126 [D loss: 0.639943, acc: 58.59%] [G loss: 1.748444]\n",
      "epoch:19 step:18127 [D loss: 0.664298, acc: 61.72%] [G loss: 1.744303]\n",
      "epoch:19 step:18128 [D loss: 0.670104, acc: 60.16%] [G loss: 1.824855]\n",
      "epoch:19 step:18129 [D loss: 0.647026, acc: 66.41%] [G loss: 1.785228]\n",
      "epoch:19 step:18130 [D loss: 0.651543, acc: 62.50%] [G loss: 1.927221]\n",
      "epoch:19 step:18131 [D loss: 0.641468, acc: 62.50%] [G loss: 1.993042]\n",
      "epoch:19 step:18132 [D loss: 0.653067, acc: 60.94%] [G loss: 1.973549]\n",
      "epoch:19 step:18133 [D loss: 0.631435, acc: 64.06%] [G loss: 1.926479]\n",
      "epoch:19 step:18134 [D loss: 0.645505, acc: 64.06%] [G loss: 2.050005]\n",
      "epoch:19 step:18135 [D loss: 0.577270, acc: 72.66%] [G loss: 2.105969]\n",
      "epoch:19 step:18136 [D loss: 0.655668, acc: 64.06%] [G loss: 1.876065]\n",
      "epoch:19 step:18137 [D loss: 0.668570, acc: 64.84%] [G loss: 1.994789]\n",
      "epoch:19 step:18138 [D loss: 0.614798, acc: 63.28%] [G loss: 2.249086]\n",
      "epoch:19 step:18139 [D loss: 0.636147, acc: 65.62%] [G loss: 1.885979]\n",
      "epoch:19 step:18140 [D loss: 0.663888, acc: 65.62%] [G loss: 2.083495]\n",
      "epoch:19 step:18141 [D loss: 0.605466, acc: 70.31%] [G loss: 2.066980]\n",
      "epoch:19 step:18142 [D loss: 0.600030, acc: 67.97%] [G loss: 2.018775]\n",
      "epoch:19 step:18143 [D loss: 0.605737, acc: 65.62%] [G loss: 1.995114]\n",
      "epoch:19 step:18144 [D loss: 0.729220, acc: 56.25%] [G loss: 1.747294]\n",
      "epoch:19 step:18145 [D loss: 0.689638, acc: 52.34%] [G loss: 1.738329]\n",
      "epoch:19 step:18146 [D loss: 0.625679, acc: 66.41%] [G loss: 1.958884]\n",
      "epoch:19 step:18147 [D loss: 0.695882, acc: 62.50%] [G loss: 1.874223]\n",
      "epoch:19 step:18148 [D loss: 0.613775, acc: 63.28%] [G loss: 2.027231]\n",
      "epoch:19 step:18149 [D loss: 0.590401, acc: 69.53%] [G loss: 2.121495]\n",
      "epoch:19 step:18150 [D loss: 0.607350, acc: 64.84%] [G loss: 2.106428]\n",
      "epoch:19 step:18151 [D loss: 0.660351, acc: 61.72%] [G loss: 1.971429]\n",
      "epoch:19 step:18152 [D loss: 0.683879, acc: 55.47%] [G loss: 1.755097]\n",
      "epoch:19 step:18153 [D loss: 0.640518, acc: 63.28%] [G loss: 1.930042]\n",
      "epoch:19 step:18154 [D loss: 0.601654, acc: 65.62%] [G loss: 1.804904]\n",
      "epoch:19 step:18155 [D loss: 0.721505, acc: 50.00%] [G loss: 1.873753]\n",
      "epoch:19 step:18156 [D loss: 0.656637, acc: 60.16%] [G loss: 1.878095]\n",
      "epoch:19 step:18157 [D loss: 0.624084, acc: 62.50%] [G loss: 2.018619]\n",
      "epoch:19 step:18158 [D loss: 0.736092, acc: 48.44%] [G loss: 1.722696]\n",
      "epoch:19 step:18159 [D loss: 0.653198, acc: 61.72%] [G loss: 1.894494]\n",
      "epoch:19 step:18160 [D loss: 0.608572, acc: 65.62%] [G loss: 1.807235]\n",
      "epoch:19 step:18161 [D loss: 0.599749, acc: 65.62%] [G loss: 2.175960]\n",
      "epoch:19 step:18162 [D loss: 0.644889, acc: 67.97%] [G loss: 1.956112]\n",
      "epoch:19 step:18163 [D loss: 0.659174, acc: 58.59%] [G loss: 1.866811]\n",
      "epoch:19 step:18164 [D loss: 0.653543, acc: 66.41%] [G loss: 1.778912]\n",
      "epoch:19 step:18165 [D loss: 0.647278, acc: 60.16%] [G loss: 1.779023]\n",
      "epoch:19 step:18166 [D loss: 0.611422, acc: 64.84%] [G loss: 1.979951]\n",
      "epoch:19 step:18167 [D loss: 0.592270, acc: 69.53%] [G loss: 1.943530]\n",
      "epoch:19 step:18168 [D loss: 0.654627, acc: 67.97%] [G loss: 1.919352]\n",
      "epoch:19 step:18169 [D loss: 0.646196, acc: 61.72%] [G loss: 2.001750]\n",
      "epoch:19 step:18170 [D loss: 0.641743, acc: 61.72%] [G loss: 1.883282]\n",
      "epoch:19 step:18171 [D loss: 0.611586, acc: 66.41%] [G loss: 1.920497]\n",
      "epoch:19 step:18172 [D loss: 0.628572, acc: 66.41%] [G loss: 1.922444]\n",
      "epoch:19 step:18173 [D loss: 0.632894, acc: 62.50%] [G loss: 2.177531]\n",
      "epoch:19 step:18174 [D loss: 0.559426, acc: 74.22%] [G loss: 2.209622]\n",
      "epoch:19 step:18175 [D loss: 0.612676, acc: 62.50%] [G loss: 2.050549]\n",
      "epoch:19 step:18176 [D loss: 0.713432, acc: 52.34%] [G loss: 1.847511]\n",
      "epoch:19 step:18177 [D loss: 0.685094, acc: 60.16%] [G loss: 2.002112]\n",
      "epoch:19 step:18178 [D loss: 0.635652, acc: 61.72%] [G loss: 1.840044]\n",
      "epoch:19 step:18179 [D loss: 0.669743, acc: 57.81%] [G loss: 1.875366]\n",
      "epoch:19 step:18180 [D loss: 0.692026, acc: 55.47%] [G loss: 1.793381]\n",
      "epoch:19 step:18181 [D loss: 0.689015, acc: 53.12%] [G loss: 1.708541]\n",
      "epoch:19 step:18182 [D loss: 0.622993, acc: 68.75%] [G loss: 1.883218]\n",
      "epoch:19 step:18183 [D loss: 0.636595, acc: 57.81%] [G loss: 1.857925]\n",
      "epoch:19 step:18184 [D loss: 0.609923, acc: 65.62%] [G loss: 1.910419]\n",
      "epoch:19 step:18185 [D loss: 0.637429, acc: 60.94%] [G loss: 1.922666]\n",
      "epoch:19 step:18186 [D loss: 0.612489, acc: 74.22%] [G loss: 1.849589]\n",
      "epoch:19 step:18187 [D loss: 0.662335, acc: 56.25%] [G loss: 1.882545]\n",
      "epoch:19 step:18188 [D loss: 0.643122, acc: 64.84%] [G loss: 2.052975]\n",
      "epoch:19 step:18189 [D loss: 0.661221, acc: 62.50%] [G loss: 1.935600]\n",
      "epoch:19 step:18190 [D loss: 0.674663, acc: 53.91%] [G loss: 1.679689]\n",
      "epoch:19 step:18191 [D loss: 0.625012, acc: 64.06%] [G loss: 1.850501]\n",
      "epoch:19 step:18192 [D loss: 0.666998, acc: 57.03%] [G loss: 1.748405]\n",
      "epoch:19 step:18193 [D loss: 0.628557, acc: 61.72%] [G loss: 1.850161]\n",
      "epoch:19 step:18194 [D loss: 0.684479, acc: 57.81%] [G loss: 1.762176]\n",
      "epoch:19 step:18195 [D loss: 0.661308, acc: 66.41%] [G loss: 1.885456]\n",
      "epoch:19 step:18196 [D loss: 0.659142, acc: 60.94%] [G loss: 1.840984]\n",
      "epoch:19 step:18197 [D loss: 0.663304, acc: 60.16%] [G loss: 1.876035]\n",
      "epoch:19 step:18198 [D loss: 0.634125, acc: 61.72%] [G loss: 1.872630]\n",
      "epoch:19 step:18199 [D loss: 0.720050, acc: 57.81%] [G loss: 1.938220]\n",
      "epoch:19 step:18200 [D loss: 0.650828, acc: 60.16%] [G loss: 1.754777]\n",
      "##############\n",
      "[2.67101759 1.28052636 6.29768866 4.74261725 3.68650489 5.64210268\n",
      " 4.24898791 4.88034023 4.61635469 3.65816967]\n",
      "##########\n",
      "epoch:19 step:18201 [D loss: 0.655915, acc: 62.50%] [G loss: 1.953262]\n",
      "epoch:19 step:18202 [D loss: 0.653633, acc: 60.94%] [G loss: 1.813262]\n",
      "epoch:19 step:18203 [D loss: 0.655549, acc: 59.38%] [G loss: 1.857274]\n",
      "epoch:19 step:18204 [D loss: 0.643294, acc: 54.69%] [G loss: 1.891805]\n",
      "epoch:19 step:18205 [D loss: 0.674890, acc: 60.94%] [G loss: 1.992670]\n",
      "epoch:19 step:18206 [D loss: 0.615614, acc: 65.62%] [G loss: 2.042964]\n",
      "epoch:19 step:18207 [D loss: 0.613583, acc: 67.97%] [G loss: 1.863872]\n",
      "epoch:19 step:18208 [D loss: 0.609196, acc: 67.19%] [G loss: 2.063965]\n",
      "epoch:19 step:18209 [D loss: 0.638597, acc: 67.19%] [G loss: 2.036160]\n",
      "epoch:19 step:18210 [D loss: 0.630170, acc: 66.41%] [G loss: 1.863499]\n",
      "epoch:19 step:18211 [D loss: 0.666230, acc: 57.03%] [G loss: 1.838446]\n",
      "epoch:19 step:18212 [D loss: 0.694653, acc: 55.47%] [G loss: 1.824362]\n",
      "epoch:19 step:18213 [D loss: 0.595373, acc: 70.31%] [G loss: 1.934009]\n",
      "epoch:19 step:18214 [D loss: 0.696129, acc: 54.69%] [G loss: 1.952754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18215 [D loss: 0.683747, acc: 60.94%] [G loss: 1.893074]\n",
      "epoch:19 step:18216 [D loss: 0.669007, acc: 58.59%] [G loss: 1.984350]\n",
      "epoch:19 step:18217 [D loss: 0.655826, acc: 56.25%] [G loss: 1.840627]\n",
      "epoch:19 step:18218 [D loss: 0.673547, acc: 52.34%] [G loss: 1.814117]\n",
      "epoch:19 step:18219 [D loss: 0.607011, acc: 66.41%] [G loss: 1.977607]\n",
      "epoch:19 step:18220 [D loss: 0.684631, acc: 56.25%] [G loss: 1.975615]\n",
      "epoch:19 step:18221 [D loss: 0.660225, acc: 60.16%] [G loss: 1.927692]\n",
      "epoch:19 step:18222 [D loss: 0.700188, acc: 56.25%] [G loss: 1.816230]\n",
      "epoch:19 step:18223 [D loss: 0.663558, acc: 60.94%] [G loss: 1.834443]\n",
      "epoch:19 step:18224 [D loss: 0.649228, acc: 64.84%] [G loss: 1.776180]\n",
      "epoch:19 step:18225 [D loss: 0.718190, acc: 51.56%] [G loss: 1.711009]\n",
      "epoch:19 step:18226 [D loss: 0.669192, acc: 56.25%] [G loss: 1.784144]\n",
      "epoch:19 step:18227 [D loss: 0.670481, acc: 60.16%] [G loss: 1.877704]\n",
      "epoch:19 step:18228 [D loss: 0.686606, acc: 54.69%] [G loss: 1.794213]\n",
      "epoch:19 step:18229 [D loss: 0.652185, acc: 60.16%] [G loss: 1.838630]\n",
      "epoch:19 step:18230 [D loss: 0.626265, acc: 67.19%] [G loss: 1.871272]\n",
      "epoch:19 step:18231 [D loss: 0.578823, acc: 69.53%] [G loss: 2.044413]\n",
      "epoch:19 step:18232 [D loss: 0.672963, acc: 64.06%] [G loss: 2.128014]\n",
      "epoch:19 step:18233 [D loss: 0.622947, acc: 65.62%] [G loss: 2.093715]\n",
      "epoch:19 step:18234 [D loss: 0.645498, acc: 60.94%] [G loss: 1.840305]\n",
      "epoch:19 step:18235 [D loss: 0.694884, acc: 56.25%] [G loss: 1.850682]\n",
      "epoch:19 step:18236 [D loss: 0.684911, acc: 54.69%] [G loss: 1.757503]\n",
      "epoch:19 step:18237 [D loss: 0.628400, acc: 64.06%] [G loss: 1.992116]\n",
      "epoch:19 step:18238 [D loss: 0.623937, acc: 64.06%] [G loss: 1.849599]\n",
      "epoch:19 step:18239 [D loss: 0.624922, acc: 61.72%] [G loss: 1.954858]\n",
      "epoch:19 step:18240 [D loss: 0.756576, acc: 46.09%] [G loss: 1.813363]\n",
      "epoch:19 step:18241 [D loss: 0.668322, acc: 63.28%] [G loss: 1.782509]\n",
      "epoch:19 step:18242 [D loss: 0.639396, acc: 63.28%] [G loss: 1.818007]\n",
      "epoch:19 step:18243 [D loss: 0.624699, acc: 64.84%] [G loss: 1.765833]\n",
      "epoch:19 step:18244 [D loss: 0.644946, acc: 64.06%] [G loss: 1.955604]\n",
      "epoch:19 step:18245 [D loss: 0.624828, acc: 67.97%] [G loss: 1.855439]\n",
      "epoch:19 step:18246 [D loss: 0.704536, acc: 57.81%] [G loss: 1.784494]\n",
      "epoch:19 step:18247 [D loss: 0.673570, acc: 60.16%] [G loss: 1.740320]\n",
      "epoch:19 step:18248 [D loss: 0.612233, acc: 67.19%] [G loss: 1.992785]\n",
      "epoch:19 step:18249 [D loss: 0.642365, acc: 60.94%] [G loss: 1.789650]\n",
      "epoch:19 step:18250 [D loss: 0.660296, acc: 59.38%] [G loss: 1.875789]\n",
      "epoch:19 step:18251 [D loss: 0.665459, acc: 64.84%] [G loss: 1.774128]\n",
      "epoch:19 step:18252 [D loss: 0.662992, acc: 57.03%] [G loss: 1.774958]\n",
      "epoch:19 step:18253 [D loss: 0.636134, acc: 61.72%] [G loss: 1.813550]\n",
      "epoch:19 step:18254 [D loss: 0.613103, acc: 66.41%] [G loss: 1.988788]\n",
      "epoch:19 step:18255 [D loss: 0.648123, acc: 63.28%] [G loss: 1.825374]\n",
      "epoch:19 step:18256 [D loss: 0.670186, acc: 65.62%] [G loss: 1.832667]\n",
      "epoch:19 step:18257 [D loss: 0.662512, acc: 61.72%] [G loss: 1.975477]\n",
      "epoch:19 step:18258 [D loss: 0.630414, acc: 64.84%] [G loss: 1.954015]\n",
      "epoch:19 step:18259 [D loss: 0.664747, acc: 59.38%] [G loss: 1.927313]\n",
      "epoch:19 step:18260 [D loss: 0.579659, acc: 67.19%] [G loss: 2.099990]\n",
      "epoch:19 step:18261 [D loss: 0.653666, acc: 66.41%] [G loss: 1.845256]\n",
      "epoch:19 step:18262 [D loss: 0.663759, acc: 58.59%] [G loss: 1.831385]\n",
      "epoch:19 step:18263 [D loss: 0.730599, acc: 50.78%] [G loss: 1.825756]\n",
      "epoch:19 step:18264 [D loss: 0.606485, acc: 69.53%] [G loss: 1.939504]\n",
      "epoch:19 step:18265 [D loss: 0.627873, acc: 65.62%] [G loss: 1.855631]\n",
      "epoch:19 step:18266 [D loss: 0.650231, acc: 62.50%] [G loss: 1.933035]\n",
      "epoch:19 step:18267 [D loss: 0.635044, acc: 66.41%] [G loss: 1.842154]\n",
      "epoch:19 step:18268 [D loss: 0.673781, acc: 54.69%] [G loss: 1.881209]\n",
      "epoch:19 step:18269 [D loss: 0.642784, acc: 64.06%] [G loss: 1.939047]\n",
      "epoch:19 step:18270 [D loss: 0.612840, acc: 68.75%] [G loss: 1.898972]\n",
      "epoch:19 step:18271 [D loss: 0.622761, acc: 65.62%] [G loss: 1.967035]\n",
      "epoch:19 step:18272 [D loss: 0.633340, acc: 65.62%] [G loss: 2.008912]\n",
      "epoch:19 step:18273 [D loss: 0.610468, acc: 70.31%] [G loss: 2.182237]\n",
      "epoch:19 step:18274 [D loss: 0.522785, acc: 77.34%] [G loss: 2.376185]\n",
      "epoch:19 step:18275 [D loss: 0.645670, acc: 64.84%] [G loss: 2.122715]\n",
      "epoch:19 step:18276 [D loss: 0.726299, acc: 53.12%] [G loss: 1.835328]\n",
      "epoch:19 step:18277 [D loss: 0.648356, acc: 66.41%] [G loss: 1.819169]\n",
      "epoch:19 step:18278 [D loss: 0.653986, acc: 62.50%] [G loss: 1.871284]\n",
      "epoch:19 step:18279 [D loss: 0.679880, acc: 57.03%] [G loss: 1.820137]\n",
      "epoch:19 step:18280 [D loss: 0.692202, acc: 53.91%] [G loss: 1.833958]\n",
      "epoch:19 step:18281 [D loss: 0.677276, acc: 57.81%] [G loss: 1.798022]\n",
      "epoch:19 step:18282 [D loss: 0.636947, acc: 61.72%] [G loss: 2.019936]\n",
      "epoch:19 step:18283 [D loss: 0.661398, acc: 59.38%] [G loss: 1.939850]\n",
      "epoch:19 step:18284 [D loss: 0.578985, acc: 69.53%] [G loss: 2.142958]\n",
      "epoch:19 step:18285 [D loss: 0.666158, acc: 64.84%] [G loss: 1.674980]\n",
      "epoch:19 step:18286 [D loss: 0.691252, acc: 53.91%] [G loss: 1.750760]\n",
      "epoch:19 step:18287 [D loss: 0.608973, acc: 65.62%] [G loss: 1.917237]\n",
      "epoch:19 step:18288 [D loss: 0.639020, acc: 62.50%] [G loss: 1.957256]\n",
      "epoch:19 step:18289 [D loss: 0.624802, acc: 64.84%] [G loss: 2.012128]\n",
      "epoch:19 step:18290 [D loss: 0.648636, acc: 59.38%] [G loss: 1.914138]\n",
      "epoch:19 step:18291 [D loss: 0.636362, acc: 60.94%] [G loss: 2.092141]\n",
      "epoch:19 step:18292 [D loss: 0.657931, acc: 57.03%] [G loss: 1.859297]\n",
      "epoch:19 step:18293 [D loss: 0.641332, acc: 67.19%] [G loss: 1.871186]\n",
      "epoch:19 step:18294 [D loss: 0.664172, acc: 59.38%] [G loss: 1.910759]\n",
      "epoch:19 step:18295 [D loss: 0.656347, acc: 61.72%] [G loss: 1.813896]\n",
      "epoch:19 step:18296 [D loss: 0.654967, acc: 60.94%] [G loss: 1.826097]\n",
      "epoch:19 step:18297 [D loss: 0.607784, acc: 63.28%] [G loss: 2.037019]\n",
      "epoch:19 step:18298 [D loss: 0.688031, acc: 57.03%] [G loss: 1.943579]\n",
      "epoch:19 step:18299 [D loss: 0.685258, acc: 53.91%] [G loss: 1.828704]\n",
      "epoch:19 step:18300 [D loss: 0.632345, acc: 65.62%] [G loss: 1.812253]\n",
      "epoch:19 step:18301 [D loss: 0.627956, acc: 64.84%] [G loss: 2.031347]\n",
      "epoch:19 step:18302 [D loss: 0.623581, acc: 67.19%] [G loss: 1.893872]\n",
      "epoch:19 step:18303 [D loss: 0.657461, acc: 57.03%] [G loss: 1.805711]\n",
      "epoch:19 step:18304 [D loss: 0.697468, acc: 55.47%] [G loss: 1.772961]\n",
      "epoch:19 step:18305 [D loss: 0.658129, acc: 64.06%] [G loss: 1.678987]\n",
      "epoch:19 step:18306 [D loss: 0.687597, acc: 63.28%] [G loss: 1.774776]\n",
      "epoch:19 step:18307 [D loss: 0.632270, acc: 61.72%] [G loss: 1.889378]\n",
      "epoch:19 step:18308 [D loss: 0.702036, acc: 52.34%] [G loss: 1.855139]\n",
      "epoch:19 step:18309 [D loss: 0.651713, acc: 62.50%] [G loss: 1.806062]\n",
      "epoch:19 step:18310 [D loss: 0.636463, acc: 61.72%] [G loss: 1.854859]\n",
      "epoch:19 step:18311 [D loss: 0.585084, acc: 67.97%] [G loss: 2.068191]\n",
      "epoch:19 step:18312 [D loss: 0.703220, acc: 53.91%] [G loss: 1.796016]\n",
      "epoch:19 step:18313 [D loss: 0.681061, acc: 52.34%] [G loss: 1.872643]\n",
      "epoch:19 step:18314 [D loss: 0.679857, acc: 54.69%] [G loss: 1.725262]\n",
      "epoch:19 step:18315 [D loss: 0.666139, acc: 57.81%] [G loss: 1.804091]\n",
      "epoch:19 step:18316 [D loss: 0.666396, acc: 60.94%] [G loss: 1.860839]\n",
      "epoch:19 step:18317 [D loss: 0.658979, acc: 57.03%] [G loss: 1.812710]\n",
      "epoch:19 step:18318 [D loss: 0.623825, acc: 57.81%] [G loss: 1.815482]\n",
      "epoch:19 step:18319 [D loss: 0.609159, acc: 65.62%] [G loss: 2.000651]\n",
      "epoch:19 step:18320 [D loss: 0.648425, acc: 60.94%] [G loss: 1.917342]\n",
      "epoch:19 step:18321 [D loss: 0.623974, acc: 64.06%] [G loss: 1.793478]\n",
      "epoch:19 step:18322 [D loss: 0.656904, acc: 60.16%] [G loss: 1.922661]\n",
      "epoch:19 step:18323 [D loss: 0.626572, acc: 63.28%] [G loss: 1.842168]\n",
      "epoch:19 step:18324 [D loss: 0.600314, acc: 67.97%] [G loss: 1.868359]\n",
      "epoch:19 step:18325 [D loss: 0.632617, acc: 65.62%] [G loss: 1.867337]\n",
      "epoch:19 step:18326 [D loss: 0.602373, acc: 66.41%] [G loss: 1.940051]\n",
      "epoch:19 step:18327 [D loss: 0.612161, acc: 68.75%] [G loss: 1.940252]\n",
      "epoch:19 step:18328 [D loss: 0.639285, acc: 63.28%] [G loss: 1.922387]\n",
      "epoch:19 step:18329 [D loss: 0.663692, acc: 55.47%] [G loss: 1.930393]\n",
      "epoch:19 step:18330 [D loss: 0.644644, acc: 60.16%] [G loss: 1.872248]\n",
      "epoch:19 step:18331 [D loss: 0.702486, acc: 58.59%] [G loss: 1.668195]\n",
      "epoch:19 step:18332 [D loss: 0.683863, acc: 60.16%] [G loss: 1.771576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18333 [D loss: 0.670238, acc: 59.38%] [G loss: 1.621151]\n",
      "epoch:19 step:18334 [D loss: 0.680508, acc: 54.69%] [G loss: 1.777494]\n",
      "epoch:19 step:18335 [D loss: 0.611767, acc: 68.75%] [G loss: 1.995746]\n",
      "epoch:19 step:18336 [D loss: 0.632850, acc: 67.19%] [G loss: 1.991849]\n",
      "epoch:19 step:18337 [D loss: 0.611439, acc: 66.41%] [G loss: 1.862829]\n",
      "epoch:19 step:18338 [D loss: 0.649713, acc: 64.84%] [G loss: 1.741207]\n",
      "epoch:19 step:18339 [D loss: 0.596503, acc: 72.66%] [G loss: 1.997151]\n",
      "epoch:19 step:18340 [D loss: 0.711847, acc: 52.34%] [G loss: 1.881135]\n",
      "epoch:19 step:18341 [D loss: 0.692001, acc: 59.38%] [G loss: 1.820854]\n",
      "epoch:19 step:18342 [D loss: 0.669368, acc: 55.47%] [G loss: 1.874623]\n",
      "epoch:19 step:18343 [D loss: 0.658316, acc: 62.50%] [G loss: 1.750145]\n",
      "epoch:19 step:18344 [D loss: 0.662911, acc: 58.59%] [G loss: 1.895338]\n",
      "epoch:19 step:18345 [D loss: 0.713814, acc: 54.69%] [G loss: 1.756466]\n",
      "epoch:19 step:18346 [D loss: 0.620624, acc: 65.62%] [G loss: 1.889426]\n",
      "epoch:19 step:18347 [D loss: 0.669132, acc: 61.72%] [G loss: 1.792216]\n",
      "epoch:19 step:18348 [D loss: 0.638387, acc: 70.31%] [G loss: 1.850932]\n",
      "epoch:19 step:18349 [D loss: 0.607564, acc: 75.78%] [G loss: 1.905305]\n",
      "epoch:19 step:18350 [D loss: 0.632460, acc: 68.75%] [G loss: 1.945394]\n",
      "epoch:19 step:18351 [D loss: 0.666953, acc: 60.94%] [G loss: 1.833853]\n",
      "epoch:19 step:18352 [D loss: 0.645831, acc: 64.06%] [G loss: 2.037839]\n",
      "epoch:19 step:18353 [D loss: 0.613992, acc: 64.84%] [G loss: 1.973902]\n",
      "epoch:19 step:18354 [D loss: 0.622295, acc: 69.53%] [G loss: 2.055735]\n",
      "epoch:19 step:18355 [D loss: 0.613842, acc: 65.62%] [G loss: 2.041660]\n",
      "epoch:19 step:18356 [D loss: 0.660443, acc: 59.38%] [G loss: 1.806059]\n",
      "epoch:19 step:18357 [D loss: 0.626413, acc: 60.94%] [G loss: 1.970780]\n",
      "epoch:19 step:18358 [D loss: 0.630248, acc: 64.84%] [G loss: 1.948484]\n",
      "epoch:19 step:18359 [D loss: 0.594250, acc: 68.75%] [G loss: 2.209288]\n",
      "epoch:19 step:18360 [D loss: 0.561368, acc: 73.44%] [G loss: 2.124774]\n",
      "epoch:19 step:18361 [D loss: 0.618336, acc: 65.62%] [G loss: 2.073212]\n",
      "epoch:19 step:18362 [D loss: 0.665478, acc: 60.94%] [G loss: 1.793787]\n",
      "epoch:19 step:18363 [D loss: 0.637477, acc: 64.84%] [G loss: 1.977841]\n",
      "epoch:19 step:18364 [D loss: 0.645690, acc: 64.06%] [G loss: 1.948723]\n",
      "epoch:19 step:18365 [D loss: 0.616365, acc: 68.75%] [G loss: 1.925908]\n",
      "epoch:19 step:18366 [D loss: 0.610906, acc: 67.97%] [G loss: 2.026866]\n",
      "epoch:19 step:18367 [D loss: 0.605462, acc: 66.41%] [G loss: 1.994833]\n",
      "epoch:19 step:18368 [D loss: 0.622903, acc: 66.41%] [G loss: 1.960730]\n",
      "epoch:19 step:18369 [D loss: 0.735717, acc: 51.56%] [G loss: 1.730023]\n",
      "epoch:19 step:18370 [D loss: 0.678397, acc: 57.81%] [G loss: 1.891365]\n",
      "epoch:19 step:18371 [D loss: 0.648421, acc: 60.94%] [G loss: 1.961505]\n",
      "epoch:19 step:18372 [D loss: 0.595704, acc: 66.41%] [G loss: 1.922498]\n",
      "epoch:19 step:18373 [D loss: 0.627281, acc: 64.06%] [G loss: 1.943214]\n",
      "epoch:19 step:18374 [D loss: 0.621674, acc: 67.97%] [G loss: 1.936682]\n",
      "epoch:19 step:18375 [D loss: 0.693177, acc: 57.03%] [G loss: 1.877423]\n",
      "epoch:19 step:18376 [D loss: 0.687712, acc: 59.38%] [G loss: 1.737612]\n",
      "epoch:19 step:18377 [D loss: 0.614126, acc: 67.19%] [G loss: 1.919497]\n",
      "epoch:19 step:18378 [D loss: 0.649359, acc: 60.16%] [G loss: 1.877807]\n",
      "epoch:19 step:18379 [D loss: 0.631485, acc: 64.84%] [G loss: 1.861895]\n",
      "epoch:19 step:18380 [D loss: 0.679951, acc: 61.72%] [G loss: 1.799732]\n",
      "epoch:19 step:18381 [D loss: 0.630697, acc: 60.94%] [G loss: 1.871003]\n",
      "epoch:19 step:18382 [D loss: 0.662526, acc: 59.38%] [G loss: 1.882125]\n",
      "epoch:19 step:18383 [D loss: 0.677022, acc: 56.25%] [G loss: 1.751476]\n",
      "epoch:19 step:18384 [D loss: 0.654407, acc: 68.75%] [G loss: 1.930948]\n",
      "epoch:19 step:18385 [D loss: 0.652617, acc: 65.62%] [G loss: 1.807343]\n",
      "epoch:19 step:18386 [D loss: 0.671115, acc: 57.03%] [G loss: 1.851297]\n",
      "epoch:19 step:18387 [D loss: 0.626577, acc: 64.06%] [G loss: 1.806830]\n",
      "epoch:19 step:18388 [D loss: 0.642802, acc: 61.72%] [G loss: 1.957405]\n",
      "epoch:19 step:18389 [D loss: 0.656278, acc: 60.94%] [G loss: 1.833873]\n",
      "epoch:19 step:18390 [D loss: 0.654191, acc: 63.28%] [G loss: 1.927316]\n",
      "epoch:19 step:18391 [D loss: 0.634825, acc: 66.41%] [G loss: 2.031001]\n",
      "epoch:19 step:18392 [D loss: 0.607240, acc: 69.53%] [G loss: 1.825420]\n",
      "epoch:19 step:18393 [D loss: 0.702273, acc: 57.81%] [G loss: 1.886399]\n",
      "epoch:19 step:18394 [D loss: 0.657010, acc: 58.59%] [G loss: 2.031112]\n",
      "epoch:19 step:18395 [D loss: 0.630605, acc: 68.75%] [G loss: 1.927141]\n",
      "epoch:19 step:18396 [D loss: 0.630789, acc: 64.84%] [G loss: 1.857811]\n",
      "epoch:19 step:18397 [D loss: 0.617741, acc: 67.19%] [G loss: 1.946360]\n",
      "epoch:19 step:18398 [D loss: 0.614179, acc: 67.19%] [G loss: 1.920920]\n",
      "epoch:19 step:18399 [D loss: 0.667293, acc: 59.38%] [G loss: 1.842922]\n",
      "epoch:19 step:18400 [D loss: 0.687863, acc: 54.69%] [G loss: 1.864087]\n",
      "##############\n",
      "[2.49249818 1.46244402 6.01316498 4.70886132 3.81243108 5.59571578\n",
      " 4.33725447 4.84362231 4.53152202 3.73003771]\n",
      "##########\n",
      "epoch:19 step:18401 [D loss: 0.606272, acc: 70.31%] [G loss: 1.700176]\n",
      "epoch:19 step:18402 [D loss: 0.647893, acc: 62.50%] [G loss: 1.883036]\n",
      "epoch:19 step:18403 [D loss: 0.651388, acc: 59.38%] [G loss: 1.874556]\n",
      "epoch:19 step:18404 [D loss: 0.625559, acc: 66.41%] [G loss: 1.794399]\n",
      "epoch:19 step:18405 [D loss: 0.606607, acc: 64.06%] [G loss: 1.989020]\n",
      "epoch:19 step:18406 [D loss: 0.628544, acc: 69.53%] [G loss: 2.035731]\n",
      "epoch:19 step:18407 [D loss: 0.634204, acc: 62.50%] [G loss: 2.072471]\n",
      "epoch:19 step:18408 [D loss: 0.592755, acc: 70.31%] [G loss: 1.938459]\n",
      "epoch:19 step:18409 [D loss: 0.670521, acc: 64.06%] [G loss: 1.908297]\n",
      "epoch:19 step:18410 [D loss: 0.621054, acc: 66.41%] [G loss: 1.977406]\n",
      "epoch:19 step:18411 [D loss: 0.630443, acc: 64.84%] [G loss: 1.831750]\n",
      "epoch:19 step:18412 [D loss: 0.654059, acc: 57.81%] [G loss: 1.768082]\n",
      "epoch:19 step:18413 [D loss: 0.643573, acc: 61.72%] [G loss: 1.768431]\n",
      "epoch:19 step:18414 [D loss: 0.666189, acc: 56.25%] [G loss: 1.828467]\n",
      "epoch:19 step:18415 [D loss: 0.626571, acc: 67.19%] [G loss: 1.870260]\n",
      "epoch:19 step:18416 [D loss: 0.631456, acc: 57.03%] [G loss: 1.891954]\n",
      "epoch:19 step:18417 [D loss: 0.656488, acc: 60.94%] [G loss: 1.858645]\n",
      "epoch:19 step:18418 [D loss: 0.628835, acc: 67.97%] [G loss: 1.752826]\n",
      "epoch:19 step:18419 [D loss: 0.606130, acc: 64.06%] [G loss: 1.959107]\n",
      "epoch:19 step:18420 [D loss: 0.642646, acc: 61.72%] [G loss: 1.847182]\n",
      "epoch:19 step:18421 [D loss: 0.646305, acc: 62.50%] [G loss: 1.917546]\n",
      "epoch:19 step:18422 [D loss: 0.682162, acc: 56.25%] [G loss: 1.770023]\n",
      "epoch:19 step:18423 [D loss: 0.693841, acc: 54.69%] [G loss: 1.818538]\n",
      "epoch:19 step:18424 [D loss: 0.592034, acc: 71.09%] [G loss: 1.861139]\n",
      "epoch:19 step:18425 [D loss: 0.677346, acc: 58.59%] [G loss: 1.955738]\n",
      "epoch:19 step:18426 [D loss: 0.642302, acc: 62.50%] [G loss: 1.875314]\n",
      "epoch:19 step:18427 [D loss: 0.616632, acc: 72.66%] [G loss: 2.113029]\n",
      "epoch:19 step:18428 [D loss: 0.699708, acc: 48.44%] [G loss: 1.794066]\n",
      "epoch:19 step:18429 [D loss: 0.647341, acc: 60.94%] [G loss: 1.964216]\n",
      "epoch:19 step:18430 [D loss: 0.630643, acc: 65.62%] [G loss: 1.922882]\n",
      "epoch:19 step:18431 [D loss: 0.677968, acc: 60.94%] [G loss: 1.865068]\n",
      "epoch:19 step:18432 [D loss: 0.585038, acc: 71.09%] [G loss: 2.002944]\n",
      "epoch:19 step:18433 [D loss: 0.626191, acc: 64.84%] [G loss: 1.967204]\n",
      "epoch:19 step:18434 [D loss: 0.650597, acc: 56.25%] [G loss: 2.029394]\n",
      "epoch:19 step:18435 [D loss: 0.608607, acc: 69.53%] [G loss: 2.192096]\n",
      "epoch:19 step:18436 [D loss: 0.675617, acc: 61.72%] [G loss: 2.022916]\n",
      "epoch:19 step:18437 [D loss: 0.613096, acc: 66.41%] [G loss: 2.051061]\n",
      "epoch:19 step:18438 [D loss: 0.654011, acc: 60.16%] [G loss: 2.027063]\n",
      "epoch:19 step:18439 [D loss: 0.680339, acc: 57.81%] [G loss: 1.875341]\n",
      "epoch:19 step:18440 [D loss: 0.632545, acc: 66.41%] [G loss: 1.981176]\n",
      "epoch:19 step:18441 [D loss: 0.636846, acc: 61.72%] [G loss: 1.902432]\n",
      "epoch:19 step:18442 [D loss: 0.675440, acc: 53.91%] [G loss: 1.891675]\n",
      "epoch:19 step:18443 [D loss: 0.673736, acc: 59.38%] [G loss: 1.904111]\n",
      "epoch:19 step:18444 [D loss: 0.660864, acc: 58.59%] [G loss: 1.961626]\n",
      "epoch:19 step:18445 [D loss: 0.617782, acc: 64.84%] [G loss: 1.888281]\n",
      "epoch:19 step:18446 [D loss: 0.676960, acc: 58.59%] [G loss: 2.004299]\n",
      "epoch:19 step:18447 [D loss: 0.639361, acc: 62.50%] [G loss: 1.948211]\n",
      "epoch:19 step:18448 [D loss: 0.659090, acc: 58.59%] [G loss: 1.957702]\n",
      "epoch:19 step:18449 [D loss: 0.601186, acc: 68.75%] [G loss: 1.899295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18450 [D loss: 0.602018, acc: 70.31%] [G loss: 2.066629]\n",
      "epoch:19 step:18451 [D loss: 0.615621, acc: 62.50%] [G loss: 2.131761]\n",
      "epoch:19 step:18452 [D loss: 0.599022, acc: 64.84%] [G loss: 2.221566]\n",
      "epoch:19 step:18453 [D loss: 0.603992, acc: 65.62%] [G loss: 2.290842]\n",
      "epoch:19 step:18454 [D loss: 0.633386, acc: 61.72%] [G loss: 1.947338]\n",
      "epoch:19 step:18455 [D loss: 0.628542, acc: 65.62%] [G loss: 1.937907]\n",
      "epoch:19 step:18456 [D loss: 0.648294, acc: 66.41%] [G loss: 1.823618]\n",
      "epoch:19 step:18457 [D loss: 0.644745, acc: 64.06%] [G loss: 1.946810]\n",
      "epoch:19 step:18458 [D loss: 0.651540, acc: 61.72%] [G loss: 1.810503]\n",
      "epoch:19 step:18459 [D loss: 0.699723, acc: 60.16%] [G loss: 1.863000]\n",
      "epoch:19 step:18460 [D loss: 0.691280, acc: 55.47%] [G loss: 1.848232]\n",
      "epoch:19 step:18461 [D loss: 0.658856, acc: 63.28%] [G loss: 1.737227]\n",
      "epoch:19 step:18462 [D loss: 0.676821, acc: 59.38%] [G loss: 1.760707]\n",
      "epoch:19 step:18463 [D loss: 0.649943, acc: 60.94%] [G loss: 1.788548]\n",
      "epoch:19 step:18464 [D loss: 0.676079, acc: 65.62%] [G loss: 1.829272]\n",
      "epoch:19 step:18465 [D loss: 0.639644, acc: 66.41%] [G loss: 1.788155]\n",
      "epoch:19 step:18466 [D loss: 0.674195, acc: 66.41%] [G loss: 1.841470]\n",
      "epoch:19 step:18467 [D loss: 0.662504, acc: 64.06%] [G loss: 1.910302]\n",
      "epoch:19 step:18468 [D loss: 0.615975, acc: 67.19%] [G loss: 1.793748]\n",
      "epoch:19 step:18469 [D loss: 0.684191, acc: 54.69%] [G loss: 1.765210]\n",
      "epoch:19 step:18470 [D loss: 0.624096, acc: 64.84%] [G loss: 1.739674]\n",
      "epoch:19 step:18471 [D loss: 0.611664, acc: 64.84%] [G loss: 1.974700]\n",
      "epoch:19 step:18472 [D loss: 0.686509, acc: 53.91%] [G loss: 1.809016]\n",
      "epoch:19 step:18473 [D loss: 0.668825, acc: 56.25%] [G loss: 1.946486]\n",
      "epoch:19 step:18474 [D loss: 0.626309, acc: 62.50%] [G loss: 1.766490]\n",
      "epoch:19 step:18475 [D loss: 0.648443, acc: 59.38%] [G loss: 1.817409]\n",
      "epoch:19 step:18476 [D loss: 0.682857, acc: 53.12%] [G loss: 1.929495]\n",
      "epoch:19 step:18477 [D loss: 0.636723, acc: 65.62%] [G loss: 1.844523]\n",
      "epoch:19 step:18478 [D loss: 0.714750, acc: 57.03%] [G loss: 1.728795]\n",
      "epoch:19 step:18479 [D loss: 0.631096, acc: 59.38%] [G loss: 1.919396]\n",
      "epoch:19 step:18480 [D loss: 0.689802, acc: 55.47%] [G loss: 1.874046]\n",
      "epoch:19 step:18481 [D loss: 0.630854, acc: 63.28%] [G loss: 1.923828]\n",
      "epoch:19 step:18482 [D loss: 0.667432, acc: 59.38%] [G loss: 1.867780]\n",
      "epoch:19 step:18483 [D loss: 0.611454, acc: 67.97%] [G loss: 1.958206]\n",
      "epoch:19 step:18484 [D loss: 0.622252, acc: 64.84%] [G loss: 2.099833]\n",
      "epoch:19 step:18485 [D loss: 0.672248, acc: 60.16%] [G loss: 1.887178]\n",
      "epoch:19 step:18486 [D loss: 0.640643, acc: 64.84%] [G loss: 1.828781]\n",
      "epoch:19 step:18487 [D loss: 0.620091, acc: 69.53%] [G loss: 1.806800]\n",
      "epoch:19 step:18488 [D loss: 0.632263, acc: 60.94%] [G loss: 2.017451]\n",
      "epoch:19 step:18489 [D loss: 0.652935, acc: 65.62%] [G loss: 1.803901]\n",
      "epoch:19 step:18490 [D loss: 0.625304, acc: 60.16%] [G loss: 1.822056]\n",
      "epoch:19 step:18491 [D loss: 0.633918, acc: 62.50%] [G loss: 1.957321]\n",
      "epoch:19 step:18492 [D loss: 0.611566, acc: 64.84%] [G loss: 1.985695]\n",
      "epoch:19 step:18493 [D loss: 0.659086, acc: 60.16%] [G loss: 2.003934]\n",
      "epoch:19 step:18494 [D loss: 0.632268, acc: 64.84%] [G loss: 1.947199]\n",
      "epoch:19 step:18495 [D loss: 0.652931, acc: 62.50%] [G loss: 2.007965]\n",
      "epoch:19 step:18496 [D loss: 0.620465, acc: 65.62%] [G loss: 2.139876]\n",
      "epoch:19 step:18497 [D loss: 0.624137, acc: 66.41%] [G loss: 2.039207]\n",
      "epoch:19 step:18498 [D loss: 0.649933, acc: 59.38%] [G loss: 1.861695]\n",
      "epoch:19 step:18499 [D loss: 0.642215, acc: 64.06%] [G loss: 1.827514]\n",
      "epoch:19 step:18500 [D loss: 0.661865, acc: 58.59%] [G loss: 1.873678]\n",
      "epoch:19 step:18501 [D loss: 0.656738, acc: 54.69%] [G loss: 1.904842]\n",
      "epoch:19 step:18502 [D loss: 0.624607, acc: 64.84%] [G loss: 1.918266]\n",
      "epoch:19 step:18503 [D loss: 0.699766, acc: 53.12%] [G loss: 1.926733]\n",
      "epoch:19 step:18504 [D loss: 0.634455, acc: 60.16%] [G loss: 1.802544]\n",
      "epoch:19 step:18505 [D loss: 0.629599, acc: 65.62%] [G loss: 1.862329]\n",
      "epoch:19 step:18506 [D loss: 0.663534, acc: 54.69%] [G loss: 1.815950]\n",
      "epoch:19 step:18507 [D loss: 0.718318, acc: 61.72%] [G loss: 1.760464]\n",
      "epoch:19 step:18508 [D loss: 0.628016, acc: 67.19%] [G loss: 1.787795]\n",
      "epoch:19 step:18509 [D loss: 0.619499, acc: 62.50%] [G loss: 1.868851]\n",
      "epoch:19 step:18510 [D loss: 0.633453, acc: 67.19%] [G loss: 2.055130]\n",
      "epoch:19 step:18511 [D loss: 0.585316, acc: 69.53%] [G loss: 1.958256]\n",
      "epoch:19 step:18512 [D loss: 0.599101, acc: 68.75%] [G loss: 2.091723]\n",
      "epoch:19 step:18513 [D loss: 0.689329, acc: 62.50%] [G loss: 1.880910]\n",
      "epoch:19 step:18514 [D loss: 0.622664, acc: 69.53%] [G loss: 1.851985]\n",
      "epoch:19 step:18515 [D loss: 0.689670, acc: 63.28%] [G loss: 2.072614]\n",
      "epoch:19 step:18516 [D loss: 0.660357, acc: 60.94%] [G loss: 1.903526]\n",
      "epoch:19 step:18517 [D loss: 0.636370, acc: 69.53%] [G loss: 2.037301]\n",
      "epoch:19 step:18518 [D loss: 0.681923, acc: 59.38%] [G loss: 1.840348]\n",
      "epoch:19 step:18519 [D loss: 0.668789, acc: 66.41%] [G loss: 1.740718]\n",
      "epoch:19 step:18520 [D loss: 0.666703, acc: 61.72%] [G loss: 1.864100]\n",
      "epoch:19 step:18521 [D loss: 0.662171, acc: 62.50%] [G loss: 1.973548]\n",
      "epoch:19 step:18522 [D loss: 0.628335, acc: 64.06%] [G loss: 1.999555]\n",
      "epoch:19 step:18523 [D loss: 0.639982, acc: 63.28%] [G loss: 1.999085]\n",
      "epoch:19 step:18524 [D loss: 0.612899, acc: 68.75%] [G loss: 2.018477]\n",
      "epoch:19 step:18525 [D loss: 0.685514, acc: 58.59%] [G loss: 1.902306]\n",
      "epoch:19 step:18526 [D loss: 0.663994, acc: 58.59%] [G loss: 1.942326]\n",
      "epoch:19 step:18527 [D loss: 0.656700, acc: 63.28%] [G loss: 1.943485]\n",
      "epoch:19 step:18528 [D loss: 0.671221, acc: 60.16%] [G loss: 1.843742]\n",
      "epoch:19 step:18529 [D loss: 0.663126, acc: 64.84%] [G loss: 1.919530]\n",
      "epoch:19 step:18530 [D loss: 0.669854, acc: 57.81%] [G loss: 1.731847]\n",
      "epoch:19 step:18531 [D loss: 0.626006, acc: 64.84%] [G loss: 1.899303]\n",
      "epoch:19 step:18532 [D loss: 0.684333, acc: 57.03%] [G loss: 1.800608]\n",
      "epoch:19 step:18533 [D loss: 0.620547, acc: 66.41%] [G loss: 1.826653]\n",
      "epoch:19 step:18534 [D loss: 0.619780, acc: 67.97%] [G loss: 1.855587]\n",
      "epoch:19 step:18535 [D loss: 0.674727, acc: 53.12%] [G loss: 1.841368]\n",
      "epoch:19 step:18536 [D loss: 0.632587, acc: 64.84%] [G loss: 1.900403]\n",
      "epoch:19 step:18537 [D loss: 0.643789, acc: 67.97%] [G loss: 1.881756]\n",
      "epoch:19 step:18538 [D loss: 0.660491, acc: 57.81%] [G loss: 2.011487]\n",
      "epoch:19 step:18539 [D loss: 0.665475, acc: 58.59%] [G loss: 1.748452]\n",
      "epoch:19 step:18540 [D loss: 0.611352, acc: 67.97%] [G loss: 1.899145]\n",
      "epoch:19 step:18541 [D loss: 0.648589, acc: 61.72%] [G loss: 1.796901]\n",
      "epoch:19 step:18542 [D loss: 0.698103, acc: 58.59%] [G loss: 1.801224]\n",
      "epoch:19 step:18543 [D loss: 0.633013, acc: 67.19%] [G loss: 1.971867]\n",
      "epoch:19 step:18544 [D loss: 0.652739, acc: 60.94%] [G loss: 1.834817]\n",
      "epoch:19 step:18545 [D loss: 0.691436, acc: 54.69%] [G loss: 1.899499]\n",
      "epoch:19 step:18546 [D loss: 0.677002, acc: 58.59%] [G loss: 1.696755]\n",
      "epoch:19 step:18547 [D loss: 0.637368, acc: 60.94%] [G loss: 1.811845]\n",
      "epoch:19 step:18548 [D loss: 0.661247, acc: 61.72%] [G loss: 1.796810]\n",
      "epoch:19 step:18549 [D loss: 0.610637, acc: 69.53%] [G loss: 1.975553]\n",
      "epoch:19 step:18550 [D loss: 0.603625, acc: 71.09%] [G loss: 1.973295]\n",
      "epoch:19 step:18551 [D loss: 0.627264, acc: 63.28%] [G loss: 1.873503]\n",
      "epoch:19 step:18552 [D loss: 0.620786, acc: 64.84%] [G loss: 1.833394]\n",
      "epoch:19 step:18553 [D loss: 0.700352, acc: 57.81%] [G loss: 1.759564]\n",
      "epoch:19 step:18554 [D loss: 0.624815, acc: 63.28%] [G loss: 1.897690]\n",
      "epoch:19 step:18555 [D loss: 0.685515, acc: 63.28%] [G loss: 1.774759]\n",
      "epoch:19 step:18556 [D loss: 0.655514, acc: 55.47%] [G loss: 1.842914]\n",
      "epoch:19 step:18557 [D loss: 0.625242, acc: 68.75%] [G loss: 1.932198]\n",
      "epoch:19 step:18558 [D loss: 0.630645, acc: 64.84%] [G loss: 1.858373]\n",
      "epoch:19 step:18559 [D loss: 0.653451, acc: 64.06%] [G loss: 1.897312]\n",
      "epoch:19 step:18560 [D loss: 0.646795, acc: 60.94%] [G loss: 1.875097]\n",
      "epoch:19 step:18561 [D loss: 0.659680, acc: 60.16%] [G loss: 1.844092]\n",
      "epoch:19 step:18562 [D loss: 0.660859, acc: 61.72%] [G loss: 1.810619]\n",
      "epoch:19 step:18563 [D loss: 0.629986, acc: 65.62%] [G loss: 1.853530]\n",
      "epoch:19 step:18564 [D loss: 0.645911, acc: 60.94%] [G loss: 1.823349]\n",
      "epoch:19 step:18565 [D loss: 0.613271, acc: 68.75%] [G loss: 1.833396]\n",
      "epoch:19 step:18566 [D loss: 0.660678, acc: 56.25%] [G loss: 1.894056]\n",
      "epoch:19 step:18567 [D loss: 0.646589, acc: 61.72%] [G loss: 1.922758]\n",
      "epoch:19 step:18568 [D loss: 0.754265, acc: 53.91%] [G loss: 1.749194]\n",
      "epoch:19 step:18569 [D loss: 0.656958, acc: 57.81%] [G loss: 1.799278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18570 [D loss: 0.657684, acc: 60.94%] [G loss: 1.753937]\n",
      "epoch:19 step:18571 [D loss: 0.646367, acc: 67.97%] [G loss: 1.907283]\n",
      "epoch:19 step:18572 [D loss: 0.608122, acc: 69.53%] [G loss: 1.952397]\n",
      "epoch:19 step:18573 [D loss: 0.655665, acc: 59.38%] [G loss: 1.861706]\n",
      "epoch:19 step:18574 [D loss: 0.625567, acc: 64.84%] [G loss: 1.843972]\n",
      "epoch:19 step:18575 [D loss: 0.604487, acc: 68.75%] [G loss: 1.909009]\n",
      "epoch:19 step:18576 [D loss: 0.646332, acc: 66.41%] [G loss: 1.761755]\n",
      "epoch:19 step:18577 [D loss: 0.595105, acc: 67.97%] [G loss: 1.911275]\n",
      "epoch:19 step:18578 [D loss: 0.643092, acc: 64.06%] [G loss: 2.070055]\n",
      "epoch:19 step:18579 [D loss: 0.685462, acc: 57.81%] [G loss: 1.874892]\n",
      "epoch:19 step:18580 [D loss: 0.668276, acc: 59.38%] [G loss: 1.848115]\n",
      "epoch:19 step:18581 [D loss: 0.620185, acc: 67.97%] [G loss: 2.071896]\n",
      "epoch:19 step:18582 [D loss: 0.696000, acc: 61.72%] [G loss: 1.796084]\n",
      "epoch:19 step:18583 [D loss: 0.669678, acc: 61.72%] [G loss: 1.909482]\n",
      "epoch:19 step:18584 [D loss: 0.649292, acc: 63.28%] [G loss: 1.876278]\n",
      "epoch:19 step:18585 [D loss: 0.629378, acc: 61.72%] [G loss: 1.924652]\n",
      "epoch:19 step:18586 [D loss: 0.626454, acc: 67.97%] [G loss: 1.897330]\n",
      "epoch:19 step:18587 [D loss: 0.729503, acc: 51.56%] [G loss: 1.732930]\n",
      "epoch:19 step:18588 [D loss: 0.664687, acc: 63.28%] [G loss: 1.867854]\n",
      "epoch:19 step:18589 [D loss: 0.574602, acc: 72.66%] [G loss: 2.048196]\n",
      "epoch:19 step:18590 [D loss: 0.724684, acc: 53.91%] [G loss: 1.866490]\n",
      "epoch:19 step:18591 [D loss: 0.658307, acc: 61.72%] [G loss: 1.765978]\n",
      "epoch:19 step:18592 [D loss: 0.673436, acc: 58.59%] [G loss: 1.935253]\n",
      "epoch:19 step:18593 [D loss: 0.666149, acc: 60.16%] [G loss: 1.925565]\n",
      "epoch:19 step:18594 [D loss: 0.658155, acc: 62.50%] [G loss: 1.805355]\n",
      "epoch:19 step:18595 [D loss: 0.622452, acc: 67.97%] [G loss: 1.970063]\n",
      "epoch:19 step:18596 [D loss: 0.643576, acc: 61.72%] [G loss: 1.869866]\n",
      "epoch:19 step:18597 [D loss: 0.653049, acc: 58.59%] [G loss: 1.801282]\n",
      "epoch:19 step:18598 [D loss: 0.706657, acc: 54.69%] [G loss: 1.770716]\n",
      "epoch:19 step:18599 [D loss: 0.648436, acc: 61.72%] [G loss: 1.856646]\n",
      "epoch:19 step:18600 [D loss: 0.634339, acc: 64.06%] [G loss: 1.878368]\n",
      "##############\n",
      "[2.55487027 1.5213165  6.37396579 4.83459924 3.5275759  5.50105803\n",
      " 4.4341913  4.62661587 4.62409304 3.60802156]\n",
      "##########\n",
      "epoch:19 step:18601 [D loss: 0.652414, acc: 59.38%] [G loss: 1.855975]\n",
      "epoch:19 step:18602 [D loss: 0.656883, acc: 62.50%] [G loss: 1.882039]\n",
      "epoch:19 step:18603 [D loss: 0.698691, acc: 53.12%] [G loss: 1.703890]\n",
      "epoch:19 step:18604 [D loss: 0.705038, acc: 51.56%] [G loss: 1.671324]\n",
      "epoch:19 step:18605 [D loss: 0.673962, acc: 56.25%] [G loss: 1.931237]\n",
      "epoch:19 step:18606 [D loss: 0.650701, acc: 65.62%] [G loss: 1.779748]\n",
      "epoch:19 step:18607 [D loss: 0.587552, acc: 68.75%] [G loss: 1.901755]\n",
      "epoch:19 step:18608 [D loss: 0.674231, acc: 56.25%] [G loss: 1.810909]\n",
      "epoch:19 step:18609 [D loss: 0.640865, acc: 61.72%] [G loss: 1.814420]\n",
      "epoch:19 step:18610 [D loss: 0.612588, acc: 66.41%] [G loss: 1.980146]\n",
      "epoch:19 step:18611 [D loss: 0.625706, acc: 63.28%] [G loss: 1.859062]\n",
      "epoch:19 step:18612 [D loss: 0.649976, acc: 60.94%] [G loss: 1.950045]\n",
      "epoch:19 step:18613 [D loss: 0.604191, acc: 65.62%] [G loss: 1.747851]\n",
      "epoch:19 step:18614 [D loss: 0.627204, acc: 68.75%] [G loss: 1.897187]\n",
      "epoch:19 step:18615 [D loss: 0.639961, acc: 62.50%] [G loss: 1.846789]\n",
      "epoch:19 step:18616 [D loss: 0.654921, acc: 60.94%] [G loss: 1.827152]\n",
      "epoch:19 step:18617 [D loss: 0.685885, acc: 56.25%] [G loss: 1.894204]\n",
      "epoch:19 step:18618 [D loss: 0.642251, acc: 63.28%] [G loss: 1.978333]\n",
      "epoch:19 step:18619 [D loss: 0.654239, acc: 60.16%] [G loss: 1.992481]\n",
      "epoch:19 step:18620 [D loss: 0.678757, acc: 55.47%] [G loss: 1.803789]\n",
      "epoch:19 step:18621 [D loss: 0.691608, acc: 50.00%] [G loss: 1.754625]\n",
      "epoch:19 step:18622 [D loss: 0.628905, acc: 65.62%] [G loss: 1.877995]\n",
      "epoch:19 step:18623 [D loss: 0.689127, acc: 50.78%] [G loss: 1.704397]\n",
      "epoch:19 step:18624 [D loss: 0.616094, acc: 64.84%] [G loss: 1.889448]\n",
      "epoch:19 step:18625 [D loss: 0.619898, acc: 64.84%] [G loss: 1.798181]\n",
      "epoch:19 step:18626 [D loss: 0.593705, acc: 73.44%] [G loss: 1.904481]\n",
      "epoch:19 step:18627 [D loss: 0.648704, acc: 67.19%] [G loss: 1.730177]\n",
      "epoch:19 step:18628 [D loss: 0.641802, acc: 63.28%] [G loss: 1.920608]\n",
      "epoch:19 step:18629 [D loss: 0.685257, acc: 54.69%] [G loss: 1.774507]\n",
      "epoch:19 step:18630 [D loss: 0.669450, acc: 61.72%] [G loss: 1.761072]\n",
      "epoch:19 step:18631 [D loss: 0.673954, acc: 56.25%] [G loss: 1.645709]\n",
      "epoch:19 step:18632 [D loss: 0.641233, acc: 58.59%] [G loss: 1.762522]\n",
      "epoch:19 step:18633 [D loss: 0.684812, acc: 61.72%] [G loss: 1.789665]\n",
      "epoch:19 step:18634 [D loss: 0.652511, acc: 57.81%] [G loss: 1.884243]\n",
      "epoch:19 step:18635 [D loss: 0.656711, acc: 62.50%] [G loss: 1.858937]\n",
      "epoch:19 step:18636 [D loss: 0.612358, acc: 65.62%] [G loss: 1.907047]\n",
      "epoch:19 step:18637 [D loss: 0.636503, acc: 64.84%] [G loss: 1.772571]\n",
      "epoch:19 step:18638 [D loss: 0.609223, acc: 67.19%] [G loss: 1.943628]\n",
      "epoch:19 step:18639 [D loss: 0.624788, acc: 63.28%] [G loss: 1.818673]\n",
      "epoch:19 step:18640 [D loss: 0.646880, acc: 64.06%] [G loss: 1.755682]\n",
      "epoch:19 step:18641 [D loss: 0.652798, acc: 64.06%] [G loss: 1.876423]\n",
      "epoch:19 step:18642 [D loss: 0.677542, acc: 57.03%] [G loss: 1.917351]\n",
      "epoch:19 step:18643 [D loss: 0.617237, acc: 65.62%] [G loss: 1.887010]\n",
      "epoch:19 step:18644 [D loss: 0.635856, acc: 62.50%] [G loss: 2.012963]\n",
      "epoch:19 step:18645 [D loss: 0.615438, acc: 67.97%] [G loss: 1.922199]\n",
      "epoch:19 step:18646 [D loss: 0.710335, acc: 60.16%] [G loss: 1.939793]\n",
      "epoch:19 step:18647 [D loss: 0.685596, acc: 55.47%] [G loss: 1.858810]\n",
      "epoch:19 step:18648 [D loss: 0.684178, acc: 64.84%] [G loss: 1.762152]\n",
      "epoch:19 step:18649 [D loss: 0.678022, acc: 57.03%] [G loss: 1.838300]\n",
      "epoch:19 step:18650 [D loss: 0.639217, acc: 65.62%] [G loss: 1.816841]\n",
      "epoch:19 step:18651 [D loss: 0.646667, acc: 64.84%] [G loss: 1.998561]\n",
      "epoch:19 step:18652 [D loss: 0.627589, acc: 64.06%] [G loss: 1.850884]\n",
      "epoch:19 step:18653 [D loss: 0.737998, acc: 50.78%] [G loss: 1.705696]\n",
      "epoch:19 step:18654 [D loss: 0.648613, acc: 67.19%] [G loss: 1.805851]\n",
      "epoch:19 step:18655 [D loss: 0.646206, acc: 63.28%] [G loss: 1.800429]\n",
      "epoch:19 step:18656 [D loss: 0.640306, acc: 60.94%] [G loss: 1.776898]\n",
      "epoch:19 step:18657 [D loss: 0.645672, acc: 64.84%] [G loss: 1.763307]\n",
      "epoch:19 step:18658 [D loss: 0.634001, acc: 62.50%] [G loss: 1.737781]\n",
      "epoch:19 step:18659 [D loss: 0.655189, acc: 63.28%] [G loss: 1.811736]\n",
      "epoch:19 step:18660 [D loss: 0.633065, acc: 65.62%] [G loss: 1.915444]\n",
      "epoch:19 step:18661 [D loss: 0.725746, acc: 51.56%] [G loss: 1.711777]\n",
      "epoch:19 step:18662 [D loss: 0.694302, acc: 55.47%] [G loss: 1.730410]\n",
      "epoch:19 step:18663 [D loss: 0.624138, acc: 63.28%] [G loss: 1.996181]\n",
      "epoch:19 step:18664 [D loss: 0.673617, acc: 60.16%] [G loss: 1.707636]\n",
      "epoch:19 step:18665 [D loss: 0.650552, acc: 61.72%] [G loss: 1.721011]\n",
      "epoch:19 step:18666 [D loss: 0.670265, acc: 54.69%] [G loss: 1.829599]\n",
      "epoch:19 step:18667 [D loss: 0.644894, acc: 57.03%] [G loss: 1.870708]\n",
      "epoch:19 step:18668 [D loss: 0.666473, acc: 57.81%] [G loss: 1.891875]\n",
      "epoch:19 step:18669 [D loss: 0.657539, acc: 64.06%] [G loss: 1.808274]\n",
      "epoch:19 step:18670 [D loss: 0.663398, acc: 58.59%] [G loss: 1.822196]\n",
      "epoch:19 step:18671 [D loss: 0.622422, acc: 59.38%] [G loss: 1.939374]\n",
      "epoch:19 step:18672 [D loss: 0.690830, acc: 53.12%] [G loss: 1.716317]\n",
      "epoch:19 step:18673 [D loss: 0.672493, acc: 57.03%] [G loss: 1.741828]\n",
      "epoch:19 step:18674 [D loss: 0.691792, acc: 53.12%] [G loss: 1.774631]\n",
      "epoch:19 step:18675 [D loss: 0.623954, acc: 67.97%] [G loss: 1.729087]\n",
      "epoch:19 step:18676 [D loss: 0.627950, acc: 64.84%] [G loss: 1.769143]\n",
      "epoch:19 step:18677 [D loss: 0.608834, acc: 70.31%] [G loss: 1.856139]\n",
      "epoch:19 step:18678 [D loss: 0.623961, acc: 60.94%] [G loss: 1.924655]\n",
      "epoch:19 step:18679 [D loss: 0.664274, acc: 65.62%] [G loss: 1.912698]\n",
      "epoch:19 step:18680 [D loss: 0.605879, acc: 66.41%] [G loss: 1.784146]\n",
      "epoch:19 step:18681 [D loss: 0.654066, acc: 60.16%] [G loss: 1.943198]\n",
      "epoch:19 step:18682 [D loss: 0.657577, acc: 57.03%] [G loss: 1.687707]\n",
      "epoch:19 step:18683 [D loss: 0.666063, acc: 61.72%] [G loss: 1.814496]\n",
      "epoch:19 step:18684 [D loss: 0.624646, acc: 65.62%] [G loss: 1.766708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18685 [D loss: 0.599872, acc: 69.53%] [G loss: 1.732893]\n",
      "epoch:19 step:18686 [D loss: 0.640312, acc: 71.09%] [G loss: 1.842901]\n",
      "epoch:19 step:18687 [D loss: 0.606879, acc: 68.75%] [G loss: 2.082105]\n",
      "epoch:19 step:18688 [D loss: 0.643030, acc: 65.62%] [G loss: 1.852701]\n",
      "epoch:19 step:18689 [D loss: 0.570116, acc: 72.66%] [G loss: 2.074931]\n",
      "epoch:19 step:18690 [D loss: 0.683230, acc: 60.94%] [G loss: 1.947556]\n",
      "epoch:19 step:18691 [D loss: 0.625626, acc: 62.50%] [G loss: 1.934034]\n",
      "epoch:19 step:18692 [D loss: 0.635873, acc: 62.50%] [G loss: 1.954812]\n",
      "epoch:19 step:18693 [D loss: 0.613363, acc: 67.97%] [G loss: 1.931737]\n",
      "epoch:19 step:18694 [D loss: 0.686045, acc: 58.59%] [G loss: 1.801044]\n",
      "epoch:19 step:18695 [D loss: 0.697632, acc: 50.78%] [G loss: 1.819734]\n",
      "epoch:19 step:18696 [D loss: 0.646967, acc: 63.28%] [G loss: 1.848493]\n",
      "epoch:19 step:18697 [D loss: 0.584721, acc: 71.88%] [G loss: 2.025602]\n",
      "epoch:19 step:18698 [D loss: 0.655598, acc: 64.84%] [G loss: 1.927742]\n",
      "epoch:19 step:18699 [D loss: 0.651274, acc: 65.62%] [G loss: 1.785607]\n",
      "epoch:19 step:18700 [D loss: 0.631412, acc: 60.94%] [G loss: 1.890153]\n",
      "epoch:19 step:18701 [D loss: 0.621997, acc: 64.06%] [G loss: 2.014374]\n",
      "epoch:19 step:18702 [D loss: 0.636829, acc: 65.62%] [G loss: 2.018223]\n",
      "epoch:19 step:18703 [D loss: 0.626839, acc: 64.84%] [G loss: 1.984751]\n",
      "epoch:19 step:18704 [D loss: 0.606328, acc: 67.97%] [G loss: 2.089185]\n",
      "epoch:19 step:18705 [D loss: 0.620329, acc: 65.62%] [G loss: 1.880666]\n",
      "epoch:19 step:18706 [D loss: 0.622847, acc: 64.84%] [G loss: 1.897581]\n",
      "epoch:19 step:18707 [D loss: 0.637348, acc: 63.28%] [G loss: 1.927191]\n",
      "epoch:19 step:18708 [D loss: 0.601798, acc: 64.84%] [G loss: 1.923272]\n",
      "epoch:19 step:18709 [D loss: 0.702690, acc: 56.25%] [G loss: 2.038936]\n",
      "epoch:19 step:18710 [D loss: 0.642022, acc: 60.94%] [G loss: 1.861870]\n",
      "epoch:19 step:18711 [D loss: 0.621985, acc: 67.19%] [G loss: 1.962149]\n",
      "epoch:19 step:18712 [D loss: 0.631627, acc: 62.50%] [G loss: 1.944493]\n",
      "epoch:19 step:18713 [D loss: 0.655388, acc: 61.72%] [G loss: 1.902592]\n",
      "epoch:19 step:18714 [D loss: 0.634929, acc: 58.59%] [G loss: 2.105533]\n",
      "epoch:19 step:18715 [D loss: 0.595014, acc: 67.97%] [G loss: 2.080529]\n",
      "epoch:19 step:18716 [D loss: 0.690595, acc: 53.91%] [G loss: 1.830712]\n",
      "epoch:19 step:18717 [D loss: 0.678174, acc: 55.47%] [G loss: 1.803778]\n",
      "epoch:19 step:18718 [D loss: 0.758918, acc: 52.34%] [G loss: 1.893549]\n",
      "epoch:19 step:18719 [D loss: 0.638370, acc: 64.84%] [G loss: 1.900851]\n",
      "epoch:19 step:18720 [D loss: 0.666959, acc: 56.25%] [G loss: 1.922791]\n",
      "epoch:19 step:18721 [D loss: 0.581121, acc: 68.75%] [G loss: 2.040421]\n",
      "epoch:19 step:18722 [D loss: 0.583559, acc: 73.44%] [G loss: 2.063387]\n",
      "epoch:19 step:18723 [D loss: 0.726685, acc: 56.25%] [G loss: 1.865698]\n",
      "epoch:19 step:18724 [D loss: 0.595255, acc: 69.53%] [G loss: 1.918928]\n",
      "epoch:19 step:18725 [D loss: 0.594909, acc: 70.31%] [G loss: 1.931136]\n",
      "epoch:19 step:18726 [D loss: 0.639218, acc: 64.06%] [G loss: 2.072059]\n",
      "epoch:19 step:18727 [D loss: 0.608733, acc: 66.41%] [G loss: 2.057824]\n",
      "epoch:19 step:18728 [D loss: 0.556569, acc: 75.00%] [G loss: 2.064370]\n",
      "epoch:19 step:18729 [D loss: 0.648918, acc: 62.50%] [G loss: 1.935479]\n",
      "epoch:19 step:18730 [D loss: 0.694065, acc: 60.94%] [G loss: 2.056331]\n",
      "epoch:19 step:18731 [D loss: 0.743503, acc: 47.66%] [G loss: 1.768385]\n",
      "epoch:19 step:18732 [D loss: 0.744491, acc: 44.53%] [G loss: 1.887139]\n",
      "epoch:19 step:18733 [D loss: 0.622350, acc: 67.19%] [G loss: 2.040895]\n",
      "epoch:19 step:18734 [D loss: 0.672988, acc: 54.69%] [G loss: 1.945011]\n",
      "epoch:19 step:18735 [D loss: 0.665703, acc: 60.16%] [G loss: 1.794093]\n",
      "epoch:19 step:18736 [D loss: 0.561885, acc: 72.66%] [G loss: 2.041952]\n",
      "epoch:19 step:18737 [D loss: 0.626907, acc: 66.41%] [G loss: 2.007629]\n",
      "epoch:19 step:18738 [D loss: 0.627299, acc: 67.97%] [G loss: 2.027955]\n",
      "epoch:19 step:18739 [D loss: 0.581020, acc: 71.09%] [G loss: 2.110105]\n",
      "epoch:19 step:18740 [D loss: 0.625066, acc: 71.88%] [G loss: 2.588715]\n",
      "epoch:20 step:18741 [D loss: 0.732219, acc: 53.91%] [G loss: 1.859642]\n",
      "epoch:20 step:18742 [D loss: 0.700753, acc: 53.91%] [G loss: 1.761094]\n",
      "epoch:20 step:18743 [D loss: 0.653029, acc: 60.94%] [G loss: 1.851434]\n",
      "epoch:20 step:18744 [D loss: 0.654026, acc: 63.28%] [G loss: 1.847664]\n",
      "epoch:20 step:18745 [D loss: 0.633753, acc: 61.72%] [G loss: 2.056527]\n",
      "epoch:20 step:18746 [D loss: 0.641754, acc: 64.84%] [G loss: 1.877683]\n",
      "epoch:20 step:18747 [D loss: 0.654795, acc: 58.59%] [G loss: 1.956140]\n",
      "epoch:20 step:18748 [D loss: 0.675321, acc: 55.47%] [G loss: 1.990111]\n",
      "epoch:20 step:18749 [D loss: 0.621216, acc: 64.84%] [G loss: 1.933800]\n",
      "epoch:20 step:18750 [D loss: 0.655880, acc: 61.72%] [G loss: 1.947633]\n",
      "epoch:20 step:18751 [D loss: 0.600650, acc: 69.53%] [G loss: 1.938677]\n",
      "epoch:20 step:18752 [D loss: 0.618329, acc: 68.75%] [G loss: 1.870506]\n",
      "epoch:20 step:18753 [D loss: 0.623815, acc: 63.28%] [G loss: 1.999583]\n",
      "epoch:20 step:18754 [D loss: 0.649861, acc: 59.38%] [G loss: 1.888166]\n",
      "epoch:20 step:18755 [D loss: 0.659491, acc: 69.53%] [G loss: 1.958568]\n",
      "epoch:20 step:18756 [D loss: 0.570859, acc: 74.22%] [G loss: 2.084918]\n",
      "epoch:20 step:18757 [D loss: 0.630767, acc: 60.16%] [G loss: 1.826545]\n",
      "epoch:20 step:18758 [D loss: 0.664773, acc: 58.59%] [G loss: 1.877708]\n",
      "epoch:20 step:18759 [D loss: 0.691406, acc: 62.50%] [G loss: 1.910793]\n",
      "epoch:20 step:18760 [D loss: 0.723338, acc: 52.34%] [G loss: 1.759287]\n",
      "epoch:20 step:18761 [D loss: 0.652099, acc: 65.62%] [G loss: 1.922897]\n",
      "epoch:20 step:18762 [D loss: 0.656877, acc: 60.94%] [G loss: 1.920604]\n",
      "epoch:20 step:18763 [D loss: 0.599759, acc: 67.19%] [G loss: 2.091800]\n",
      "epoch:20 step:18764 [D loss: 0.655336, acc: 69.53%] [G loss: 1.989525]\n",
      "epoch:20 step:18765 [D loss: 0.626661, acc: 60.94%] [G loss: 1.980409]\n",
      "epoch:20 step:18766 [D loss: 0.595141, acc: 68.75%] [G loss: 1.940618]\n",
      "epoch:20 step:18767 [D loss: 0.687317, acc: 56.25%] [G loss: 1.763739]\n",
      "epoch:20 step:18768 [D loss: 0.659606, acc: 64.84%] [G loss: 1.791846]\n",
      "epoch:20 step:18769 [D loss: 0.640213, acc: 64.84%] [G loss: 1.881975]\n",
      "epoch:20 step:18770 [D loss: 0.680919, acc: 60.16%] [G loss: 1.857001]\n",
      "epoch:20 step:18771 [D loss: 0.657097, acc: 62.50%] [G loss: 1.746204]\n",
      "epoch:20 step:18772 [D loss: 0.677520, acc: 53.12%] [G loss: 1.716614]\n",
      "epoch:20 step:18773 [D loss: 0.691685, acc: 64.06%] [G loss: 1.785329]\n",
      "epoch:20 step:18774 [D loss: 0.668734, acc: 63.28%] [G loss: 1.864906]\n",
      "epoch:20 step:18775 [D loss: 0.613764, acc: 67.97%] [G loss: 1.736145]\n",
      "epoch:20 step:18776 [D loss: 0.585178, acc: 71.88%] [G loss: 1.952127]\n",
      "epoch:20 step:18777 [D loss: 0.613525, acc: 66.41%] [G loss: 1.968018]\n",
      "epoch:20 step:18778 [D loss: 0.651557, acc: 60.94%] [G loss: 1.935323]\n",
      "epoch:20 step:18779 [D loss: 0.616209, acc: 66.41%] [G loss: 2.023083]\n",
      "epoch:20 step:18780 [D loss: 0.579305, acc: 67.97%] [G loss: 2.250692]\n",
      "epoch:20 step:18781 [D loss: 0.670408, acc: 56.25%] [G loss: 1.844639]\n",
      "epoch:20 step:18782 [D loss: 0.615017, acc: 66.41%] [G loss: 1.893742]\n",
      "epoch:20 step:18783 [D loss: 0.694302, acc: 58.59%] [G loss: 1.850028]\n",
      "epoch:20 step:18784 [D loss: 0.610678, acc: 66.41%] [G loss: 1.836519]\n",
      "epoch:20 step:18785 [D loss: 0.675159, acc: 53.91%] [G loss: 1.782376]\n",
      "epoch:20 step:18786 [D loss: 0.695289, acc: 56.25%] [G loss: 1.812277]\n",
      "epoch:20 step:18787 [D loss: 0.551928, acc: 77.34%] [G loss: 1.878030]\n",
      "epoch:20 step:18788 [D loss: 0.604695, acc: 64.84%] [G loss: 1.868310]\n",
      "epoch:20 step:18789 [D loss: 0.638202, acc: 61.72%] [G loss: 1.961059]\n",
      "epoch:20 step:18790 [D loss: 0.590875, acc: 70.31%] [G loss: 1.906842]\n",
      "epoch:20 step:18791 [D loss: 0.622377, acc: 69.53%] [G loss: 1.928581]\n",
      "epoch:20 step:18792 [D loss: 0.687221, acc: 58.59%] [G loss: 1.935244]\n",
      "epoch:20 step:18793 [D loss: 0.665640, acc: 67.19%] [G loss: 2.044818]\n",
      "epoch:20 step:18794 [D loss: 0.624214, acc: 67.19%] [G loss: 1.939492]\n",
      "epoch:20 step:18795 [D loss: 0.638541, acc: 63.28%] [G loss: 1.958575]\n",
      "epoch:20 step:18796 [D loss: 0.654809, acc: 63.28%] [G loss: 1.967375]\n",
      "epoch:20 step:18797 [D loss: 0.609648, acc: 70.31%] [G loss: 1.932741]\n",
      "epoch:20 step:18798 [D loss: 0.637248, acc: 64.84%] [G loss: 1.984954]\n",
      "epoch:20 step:18799 [D loss: 0.630403, acc: 65.62%] [G loss: 1.859954]\n",
      "epoch:20 step:18800 [D loss: 0.682053, acc: 58.59%] [G loss: 1.887084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.38980447 1.61550715 6.13259353 4.63715064 3.72127234 5.71484295\n",
      " 4.34637156 4.72469736 4.55916684 3.70316019]\n",
      "##########\n",
      "epoch:20 step:18801 [D loss: 0.628474, acc: 67.97%] [G loss: 1.904230]\n",
      "epoch:20 step:18802 [D loss: 0.635242, acc: 66.41%] [G loss: 2.014160]\n",
      "epoch:20 step:18803 [D loss: 0.650324, acc: 58.59%] [G loss: 1.897378]\n",
      "epoch:20 step:18804 [D loss: 0.642484, acc: 60.16%] [G loss: 1.828579]\n",
      "epoch:20 step:18805 [D loss: 0.708286, acc: 60.94%] [G loss: 1.969793]\n",
      "epoch:20 step:18806 [D loss: 0.618689, acc: 67.19%] [G loss: 1.889131]\n",
      "epoch:20 step:18807 [D loss: 0.657551, acc: 61.72%] [G loss: 2.003059]\n",
      "epoch:20 step:18808 [D loss: 0.646103, acc: 66.41%] [G loss: 1.932550]\n",
      "epoch:20 step:18809 [D loss: 0.626851, acc: 64.06%] [G loss: 2.093492]\n",
      "epoch:20 step:18810 [D loss: 0.637451, acc: 63.28%] [G loss: 1.962507]\n",
      "epoch:20 step:18811 [D loss: 0.628983, acc: 64.84%] [G loss: 1.813606]\n",
      "epoch:20 step:18812 [D loss: 0.650046, acc: 66.41%] [G loss: 1.808769]\n",
      "epoch:20 step:18813 [D loss: 0.695816, acc: 55.47%] [G loss: 1.859725]\n",
      "epoch:20 step:18814 [D loss: 0.638242, acc: 62.50%] [G loss: 1.981175]\n",
      "epoch:20 step:18815 [D loss: 0.605476, acc: 66.41%] [G loss: 1.936806]\n",
      "epoch:20 step:18816 [D loss: 0.647625, acc: 66.41%] [G loss: 2.089557]\n",
      "epoch:20 step:18817 [D loss: 0.619599, acc: 64.84%] [G loss: 2.185269]\n",
      "epoch:20 step:18818 [D loss: 0.644395, acc: 66.41%] [G loss: 1.896560]\n",
      "epoch:20 step:18819 [D loss: 0.639973, acc: 64.06%] [G loss: 1.818048]\n",
      "epoch:20 step:18820 [D loss: 0.677859, acc: 58.59%] [G loss: 1.831388]\n",
      "epoch:20 step:18821 [D loss: 0.664686, acc: 53.91%] [G loss: 1.842576]\n",
      "epoch:20 step:18822 [D loss: 0.634429, acc: 62.50%] [G loss: 1.834811]\n",
      "epoch:20 step:18823 [D loss: 0.681251, acc: 60.16%] [G loss: 1.986638]\n",
      "epoch:20 step:18824 [D loss: 0.607395, acc: 65.62%] [G loss: 1.957834]\n",
      "epoch:20 step:18825 [D loss: 0.650604, acc: 60.94%] [G loss: 1.898450]\n",
      "epoch:20 step:18826 [D loss: 0.640668, acc: 59.38%] [G loss: 1.791373]\n",
      "epoch:20 step:18827 [D loss: 0.626051, acc: 62.50%] [G loss: 1.957342]\n",
      "epoch:20 step:18828 [D loss: 0.679618, acc: 59.38%] [G loss: 1.807442]\n",
      "epoch:20 step:18829 [D loss: 0.630610, acc: 66.41%] [G loss: 1.824584]\n",
      "epoch:20 step:18830 [D loss: 0.643576, acc: 69.53%] [G loss: 1.729567]\n",
      "epoch:20 step:18831 [D loss: 0.642577, acc: 64.06%] [G loss: 1.833769]\n",
      "epoch:20 step:18832 [D loss: 0.610898, acc: 64.84%] [G loss: 1.959865]\n",
      "epoch:20 step:18833 [D loss: 0.611912, acc: 64.84%] [G loss: 1.945069]\n",
      "epoch:20 step:18834 [D loss: 0.620592, acc: 67.19%] [G loss: 1.838591]\n",
      "epoch:20 step:18835 [D loss: 0.655722, acc: 57.81%] [G loss: 1.928499]\n",
      "epoch:20 step:18836 [D loss: 0.638271, acc: 68.75%] [G loss: 1.946303]\n",
      "epoch:20 step:18837 [D loss: 0.633730, acc: 71.09%] [G loss: 1.981289]\n",
      "epoch:20 step:18838 [D loss: 0.654679, acc: 63.28%] [G loss: 1.824744]\n",
      "epoch:20 step:18839 [D loss: 0.624429, acc: 66.41%] [G loss: 1.877052]\n",
      "epoch:20 step:18840 [D loss: 0.615556, acc: 71.09%] [G loss: 2.021383]\n",
      "epoch:20 step:18841 [D loss: 0.613069, acc: 64.84%] [G loss: 1.927272]\n",
      "epoch:20 step:18842 [D loss: 0.637784, acc: 60.94%] [G loss: 1.910400]\n",
      "epoch:20 step:18843 [D loss: 0.640315, acc: 63.28%] [G loss: 1.938138]\n",
      "epoch:20 step:18844 [D loss: 0.586750, acc: 71.09%] [G loss: 1.886365]\n",
      "epoch:20 step:18845 [D loss: 0.666161, acc: 64.06%] [G loss: 1.973422]\n",
      "epoch:20 step:18846 [D loss: 0.642551, acc: 61.72%] [G loss: 2.009338]\n",
      "epoch:20 step:18847 [D loss: 0.585203, acc: 67.19%] [G loss: 2.148722]\n",
      "epoch:20 step:18848 [D loss: 0.663490, acc: 58.59%] [G loss: 1.825003]\n",
      "epoch:20 step:18849 [D loss: 0.648592, acc: 61.72%] [G loss: 1.943369]\n",
      "epoch:20 step:18850 [D loss: 0.682446, acc: 60.16%] [G loss: 1.827956]\n",
      "epoch:20 step:18851 [D loss: 0.603789, acc: 68.75%] [G loss: 2.006867]\n",
      "epoch:20 step:18852 [D loss: 0.583239, acc: 69.53%] [G loss: 1.877156]\n",
      "epoch:20 step:18853 [D loss: 0.636813, acc: 65.62%] [G loss: 2.056262]\n",
      "epoch:20 step:18854 [D loss: 0.606128, acc: 66.41%] [G loss: 2.083361]\n",
      "epoch:20 step:18855 [D loss: 0.659573, acc: 65.62%] [G loss: 2.092155]\n",
      "epoch:20 step:18856 [D loss: 0.633891, acc: 66.41%] [G loss: 2.086992]\n",
      "epoch:20 step:18857 [D loss: 0.627916, acc: 63.28%] [G loss: 2.178060]\n",
      "epoch:20 step:18858 [D loss: 0.617233, acc: 68.75%] [G loss: 2.219817]\n",
      "epoch:20 step:18859 [D loss: 0.625635, acc: 61.72%] [G loss: 2.283539]\n",
      "epoch:20 step:18860 [D loss: 0.710334, acc: 56.25%] [G loss: 1.896190]\n",
      "epoch:20 step:18861 [D loss: 0.662392, acc: 63.28%] [G loss: 1.997617]\n",
      "epoch:20 step:18862 [D loss: 0.670908, acc: 60.16%] [G loss: 2.081937]\n",
      "epoch:20 step:18863 [D loss: 0.703371, acc: 59.38%] [G loss: 1.984307]\n",
      "epoch:20 step:18864 [D loss: 0.688236, acc: 60.94%] [G loss: 1.914602]\n",
      "epoch:20 step:18865 [D loss: 0.690070, acc: 56.25%] [G loss: 1.782119]\n",
      "epoch:20 step:18866 [D loss: 0.717111, acc: 50.00%] [G loss: 1.822615]\n",
      "epoch:20 step:18867 [D loss: 0.650366, acc: 63.28%] [G loss: 1.826948]\n",
      "epoch:20 step:18868 [D loss: 0.661964, acc: 53.91%] [G loss: 1.780787]\n",
      "epoch:20 step:18869 [D loss: 0.637179, acc: 65.62%] [G loss: 1.795974]\n",
      "epoch:20 step:18870 [D loss: 0.638337, acc: 60.16%] [G loss: 1.968015]\n",
      "epoch:20 step:18871 [D loss: 0.645753, acc: 60.94%] [G loss: 2.014935]\n",
      "epoch:20 step:18872 [D loss: 0.639697, acc: 62.50%] [G loss: 1.936032]\n",
      "epoch:20 step:18873 [D loss: 0.671633, acc: 58.59%] [G loss: 1.856606]\n",
      "epoch:20 step:18874 [D loss: 0.659091, acc: 57.81%] [G loss: 1.770257]\n",
      "epoch:20 step:18875 [D loss: 0.644475, acc: 64.06%] [G loss: 1.824467]\n",
      "epoch:20 step:18876 [D loss: 0.696350, acc: 50.00%] [G loss: 1.840508]\n",
      "epoch:20 step:18877 [D loss: 0.699480, acc: 57.81%] [G loss: 1.869786]\n",
      "epoch:20 step:18878 [D loss: 0.699841, acc: 58.59%] [G loss: 1.774371]\n",
      "epoch:20 step:18879 [D loss: 0.619143, acc: 67.97%] [G loss: 1.943264]\n",
      "epoch:20 step:18880 [D loss: 0.641045, acc: 62.50%] [G loss: 1.809494]\n",
      "epoch:20 step:18881 [D loss: 0.634254, acc: 68.75%] [G loss: 1.804248]\n",
      "epoch:20 step:18882 [D loss: 0.660776, acc: 56.25%] [G loss: 1.701941]\n",
      "epoch:20 step:18883 [D loss: 0.652129, acc: 58.59%] [G loss: 1.848400]\n",
      "epoch:20 step:18884 [D loss: 0.677837, acc: 60.16%] [G loss: 1.961793]\n",
      "epoch:20 step:18885 [D loss: 0.650859, acc: 60.16%] [G loss: 2.056213]\n",
      "epoch:20 step:18886 [D loss: 0.608347, acc: 72.66%] [G loss: 1.994618]\n",
      "epoch:20 step:18887 [D loss: 0.666420, acc: 60.16%] [G loss: 1.872477]\n",
      "epoch:20 step:18888 [D loss: 0.694723, acc: 60.16%] [G loss: 1.792744]\n",
      "epoch:20 step:18889 [D loss: 0.634197, acc: 64.06%] [G loss: 1.870411]\n",
      "epoch:20 step:18890 [D loss: 0.630387, acc: 68.75%] [G loss: 1.923377]\n",
      "epoch:20 step:18891 [D loss: 0.665536, acc: 60.16%] [G loss: 1.949296]\n",
      "epoch:20 step:18892 [D loss: 0.646298, acc: 63.28%] [G loss: 1.909996]\n",
      "epoch:20 step:18893 [D loss: 0.697781, acc: 55.47%] [G loss: 1.809087]\n",
      "epoch:20 step:18894 [D loss: 0.670798, acc: 60.94%] [G loss: 1.928381]\n",
      "epoch:20 step:18895 [D loss: 0.619219, acc: 64.06%] [G loss: 1.842597]\n",
      "epoch:20 step:18896 [D loss: 0.586374, acc: 72.66%] [G loss: 1.908910]\n",
      "epoch:20 step:18897 [D loss: 0.676990, acc: 62.50%] [G loss: 1.854050]\n",
      "epoch:20 step:18898 [D loss: 0.647389, acc: 65.62%] [G loss: 1.847911]\n",
      "epoch:20 step:18899 [D loss: 0.637882, acc: 61.72%] [G loss: 1.950301]\n",
      "epoch:20 step:18900 [D loss: 0.696457, acc: 57.81%] [G loss: 1.780956]\n",
      "epoch:20 step:18901 [D loss: 0.655304, acc: 64.84%] [G loss: 1.857861]\n",
      "epoch:20 step:18902 [D loss: 0.633075, acc: 69.53%] [G loss: 1.836349]\n",
      "epoch:20 step:18903 [D loss: 0.637219, acc: 58.59%] [G loss: 1.825820]\n",
      "epoch:20 step:18904 [D loss: 0.675430, acc: 60.16%] [G loss: 1.757912]\n",
      "epoch:20 step:18905 [D loss: 0.607896, acc: 67.19%] [G loss: 1.908750]\n",
      "epoch:20 step:18906 [D loss: 0.655668, acc: 64.06%] [G loss: 1.930006]\n",
      "epoch:20 step:18907 [D loss: 0.684157, acc: 57.03%] [G loss: 1.861658]\n",
      "epoch:20 step:18908 [D loss: 0.603672, acc: 69.53%] [G loss: 1.831395]\n",
      "epoch:20 step:18909 [D loss: 0.652224, acc: 64.06%] [G loss: 1.782877]\n",
      "epoch:20 step:18910 [D loss: 0.640323, acc: 59.38%] [G loss: 1.939613]\n",
      "epoch:20 step:18911 [D loss: 0.615617, acc: 64.84%] [G loss: 1.841730]\n",
      "epoch:20 step:18912 [D loss: 0.677837, acc: 60.16%] [G loss: 1.776790]\n",
      "epoch:20 step:18913 [D loss: 0.659584, acc: 56.25%] [G loss: 1.871985]\n",
      "epoch:20 step:18914 [D loss: 0.695956, acc: 54.69%] [G loss: 1.908498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18915 [D loss: 0.663067, acc: 61.72%] [G loss: 1.833958]\n",
      "epoch:20 step:18916 [D loss: 0.640914, acc: 61.72%] [G loss: 1.720542]\n",
      "epoch:20 step:18917 [D loss: 0.685837, acc: 60.16%] [G loss: 1.819746]\n",
      "epoch:20 step:18918 [D loss: 0.609169, acc: 72.66%] [G loss: 1.726480]\n",
      "epoch:20 step:18919 [D loss: 0.642470, acc: 59.38%] [G loss: 1.751427]\n",
      "epoch:20 step:18920 [D loss: 0.649880, acc: 64.06%] [G loss: 1.803503]\n",
      "epoch:20 step:18921 [D loss: 0.680252, acc: 63.28%] [G loss: 1.938757]\n",
      "epoch:20 step:18922 [D loss: 0.638715, acc: 66.41%] [G loss: 1.837037]\n",
      "epoch:20 step:18923 [D loss: 0.659594, acc: 60.16%] [G loss: 1.771394]\n",
      "epoch:20 step:18924 [D loss: 0.697096, acc: 53.91%] [G loss: 1.859495]\n",
      "epoch:20 step:18925 [D loss: 0.673539, acc: 55.47%] [G loss: 1.909634]\n",
      "epoch:20 step:18926 [D loss: 0.650122, acc: 65.62%] [G loss: 1.860066]\n",
      "epoch:20 step:18927 [D loss: 0.645447, acc: 58.59%] [G loss: 1.856436]\n",
      "epoch:20 step:18928 [D loss: 0.637504, acc: 65.62%] [G loss: 1.856661]\n",
      "epoch:20 step:18929 [D loss: 0.645779, acc: 62.50%] [G loss: 1.855991]\n",
      "epoch:20 step:18930 [D loss: 0.666757, acc: 58.59%] [G loss: 1.890493]\n",
      "epoch:20 step:18931 [D loss: 0.650653, acc: 66.41%] [G loss: 1.849737]\n",
      "epoch:20 step:18932 [D loss: 0.672366, acc: 60.94%] [G loss: 1.926200]\n",
      "epoch:20 step:18933 [D loss: 0.673904, acc: 56.25%] [G loss: 1.833266]\n",
      "epoch:20 step:18934 [D loss: 0.610542, acc: 63.28%] [G loss: 1.912277]\n",
      "epoch:20 step:18935 [D loss: 0.673170, acc: 59.38%] [G loss: 1.817832]\n",
      "epoch:20 step:18936 [D loss: 0.638464, acc: 64.84%] [G loss: 1.906322]\n",
      "epoch:20 step:18937 [D loss: 0.679236, acc: 60.94%] [G loss: 1.986112]\n",
      "epoch:20 step:18938 [D loss: 0.616653, acc: 68.75%] [G loss: 1.973379]\n",
      "epoch:20 step:18939 [D loss: 0.677730, acc: 60.94%] [G loss: 1.828633]\n",
      "epoch:20 step:18940 [D loss: 0.680385, acc: 61.72%] [G loss: 1.729921]\n",
      "epoch:20 step:18941 [D loss: 0.650008, acc: 61.72%] [G loss: 1.977200]\n",
      "epoch:20 step:18942 [D loss: 0.651628, acc: 61.72%] [G loss: 1.958994]\n",
      "epoch:20 step:18943 [D loss: 0.669881, acc: 57.03%] [G loss: 1.750182]\n",
      "epoch:20 step:18944 [D loss: 0.673370, acc: 57.03%] [G loss: 1.837339]\n",
      "epoch:20 step:18945 [D loss: 0.636241, acc: 65.62%] [G loss: 1.783480]\n",
      "epoch:20 step:18946 [D loss: 0.619676, acc: 65.62%] [G loss: 2.046911]\n",
      "epoch:20 step:18947 [D loss: 0.632278, acc: 64.06%] [G loss: 2.034375]\n",
      "epoch:20 step:18948 [D loss: 0.609586, acc: 63.28%] [G loss: 1.988598]\n",
      "epoch:20 step:18949 [D loss: 0.592954, acc: 71.09%] [G loss: 2.076809]\n",
      "epoch:20 step:18950 [D loss: 0.677504, acc: 57.03%] [G loss: 1.727481]\n",
      "epoch:20 step:18951 [D loss: 0.677640, acc: 57.03%] [G loss: 1.700284]\n",
      "epoch:20 step:18952 [D loss: 0.646354, acc: 61.72%] [G loss: 1.861720]\n",
      "epoch:20 step:18953 [D loss: 0.680794, acc: 57.03%] [G loss: 1.833056]\n",
      "epoch:20 step:18954 [D loss: 0.633496, acc: 60.94%] [G loss: 1.773832]\n",
      "epoch:20 step:18955 [D loss: 0.669613, acc: 57.81%] [G loss: 1.834614]\n",
      "epoch:20 step:18956 [D loss: 0.684600, acc: 64.06%] [G loss: 1.833597]\n",
      "epoch:20 step:18957 [D loss: 0.662701, acc: 64.06%] [G loss: 1.945284]\n",
      "epoch:20 step:18958 [D loss: 0.627686, acc: 69.53%] [G loss: 1.934608]\n",
      "epoch:20 step:18959 [D loss: 0.632139, acc: 66.41%] [G loss: 2.160078]\n",
      "epoch:20 step:18960 [D loss: 0.716215, acc: 53.91%] [G loss: 1.935690]\n",
      "epoch:20 step:18961 [D loss: 0.640630, acc: 60.16%] [G loss: 1.957693]\n",
      "epoch:20 step:18962 [D loss: 0.648865, acc: 60.94%] [G loss: 1.954931]\n",
      "epoch:20 step:18963 [D loss: 0.696261, acc: 56.25%] [G loss: 1.811962]\n",
      "epoch:20 step:18964 [D loss: 0.662188, acc: 58.59%] [G loss: 1.864845]\n",
      "epoch:20 step:18965 [D loss: 0.636914, acc: 65.62%] [G loss: 1.833035]\n",
      "epoch:20 step:18966 [D loss: 0.667257, acc: 57.81%] [G loss: 1.872608]\n",
      "epoch:20 step:18967 [D loss: 0.607338, acc: 61.72%] [G loss: 1.972500]\n",
      "epoch:20 step:18968 [D loss: 0.669482, acc: 60.16%] [G loss: 1.743708]\n",
      "epoch:20 step:18969 [D loss: 0.601987, acc: 71.88%] [G loss: 2.010775]\n",
      "epoch:20 step:18970 [D loss: 0.596428, acc: 67.19%] [G loss: 2.194016]\n",
      "epoch:20 step:18971 [D loss: 0.607018, acc: 66.41%] [G loss: 2.308052]\n",
      "epoch:20 step:18972 [D loss: 0.569470, acc: 67.19%] [G loss: 2.262918]\n",
      "epoch:20 step:18973 [D loss: 0.651335, acc: 60.16%] [G loss: 1.834361]\n",
      "epoch:20 step:18974 [D loss: 0.627602, acc: 66.41%] [G loss: 1.873835]\n",
      "epoch:20 step:18975 [D loss: 0.695013, acc: 53.12%] [G loss: 1.773274]\n",
      "epoch:20 step:18976 [D loss: 0.631417, acc: 65.62%] [G loss: 1.957748]\n",
      "epoch:20 step:18977 [D loss: 0.643951, acc: 64.84%] [G loss: 1.913770]\n",
      "epoch:20 step:18978 [D loss: 0.624841, acc: 64.06%] [G loss: 2.021848]\n",
      "epoch:20 step:18979 [D loss: 0.688723, acc: 58.59%] [G loss: 1.911855]\n",
      "epoch:20 step:18980 [D loss: 0.582376, acc: 73.44%] [G loss: 1.956398]\n",
      "epoch:20 step:18981 [D loss: 0.644593, acc: 67.97%] [G loss: 2.031659]\n",
      "epoch:20 step:18982 [D loss: 0.605954, acc: 67.19%] [G loss: 2.033880]\n",
      "epoch:20 step:18983 [D loss: 0.661306, acc: 61.72%] [G loss: 1.923815]\n",
      "epoch:20 step:18984 [D loss: 0.627198, acc: 60.94%] [G loss: 1.873959]\n",
      "epoch:20 step:18985 [D loss: 0.629972, acc: 60.94%] [G loss: 1.881537]\n",
      "epoch:20 step:18986 [D loss: 0.664873, acc: 56.25%] [G loss: 1.948351]\n",
      "epoch:20 step:18987 [D loss: 0.656300, acc: 61.72%] [G loss: 1.975769]\n",
      "epoch:20 step:18988 [D loss: 0.637547, acc: 65.62%] [G loss: 2.074393]\n",
      "epoch:20 step:18989 [D loss: 0.625101, acc: 64.06%] [G loss: 1.913851]\n",
      "epoch:20 step:18990 [D loss: 0.739968, acc: 44.53%] [G loss: 1.666489]\n",
      "epoch:20 step:18991 [D loss: 0.658293, acc: 60.94%] [G loss: 1.690733]\n",
      "epoch:20 step:18992 [D loss: 0.684200, acc: 59.38%] [G loss: 1.739742]\n",
      "epoch:20 step:18993 [D loss: 0.667690, acc: 57.81%] [G loss: 1.774049]\n",
      "epoch:20 step:18994 [D loss: 0.654825, acc: 63.28%] [G loss: 1.710400]\n",
      "epoch:20 step:18995 [D loss: 0.643635, acc: 65.62%] [G loss: 1.912193]\n",
      "epoch:20 step:18996 [D loss: 0.664471, acc: 64.06%] [G loss: 1.793163]\n",
      "epoch:20 step:18997 [D loss: 0.677801, acc: 57.81%] [G loss: 1.754761]\n",
      "epoch:20 step:18998 [D loss: 0.645508, acc: 64.84%] [G loss: 1.814945]\n",
      "epoch:20 step:18999 [D loss: 0.662947, acc: 58.59%] [G loss: 1.812569]\n",
      "epoch:20 step:19000 [D loss: 0.696238, acc: 53.12%] [G loss: 1.862020]\n",
      "##############\n",
      "[2.5105982  1.32530367 6.29358374 4.68383555 3.71978551 5.64870061\n",
      " 4.40879424 4.76937243 4.71940241 3.58650077]\n",
      "##########\n",
      "epoch:20 step:19001 [D loss: 0.634188, acc: 57.81%] [G loss: 1.945985]\n",
      "epoch:20 step:19002 [D loss: 0.602907, acc: 67.97%] [G loss: 1.902527]\n",
      "epoch:20 step:19003 [D loss: 0.631600, acc: 65.62%] [G loss: 1.934589]\n",
      "epoch:20 step:19004 [D loss: 0.669507, acc: 58.59%] [G loss: 1.816213]\n",
      "epoch:20 step:19005 [D loss: 0.640078, acc: 60.16%] [G loss: 1.806432]\n",
      "epoch:20 step:19006 [D loss: 0.637668, acc: 63.28%] [G loss: 1.762455]\n",
      "epoch:20 step:19007 [D loss: 0.628545, acc: 64.06%] [G loss: 1.758711]\n",
      "epoch:20 step:19008 [D loss: 0.643502, acc: 62.50%] [G loss: 1.766358]\n",
      "epoch:20 step:19009 [D loss: 0.632046, acc: 66.41%] [G loss: 1.984533]\n",
      "epoch:20 step:19010 [D loss: 0.629307, acc: 65.62%] [G loss: 1.982015]\n",
      "epoch:20 step:19011 [D loss: 0.659953, acc: 58.59%] [G loss: 1.767786]\n",
      "epoch:20 step:19012 [D loss: 0.612695, acc: 65.62%] [G loss: 1.853989]\n",
      "epoch:20 step:19013 [D loss: 0.668384, acc: 61.72%] [G loss: 1.809064]\n",
      "epoch:20 step:19014 [D loss: 0.634004, acc: 67.19%] [G loss: 2.000642]\n",
      "epoch:20 step:19015 [D loss: 0.598443, acc: 68.75%] [G loss: 2.074578]\n",
      "epoch:20 step:19016 [D loss: 0.614464, acc: 65.62%] [G loss: 2.082691]\n",
      "epoch:20 step:19017 [D loss: 0.659608, acc: 62.50%] [G loss: 1.926636]\n",
      "epoch:20 step:19018 [D loss: 0.677637, acc: 60.94%] [G loss: 1.940210]\n",
      "epoch:20 step:19019 [D loss: 0.702137, acc: 55.47%] [G loss: 1.826998]\n",
      "epoch:20 step:19020 [D loss: 0.656054, acc: 56.25%] [G loss: 1.963183]\n",
      "epoch:20 step:19021 [D loss: 0.646002, acc: 61.72%] [G loss: 1.805220]\n",
      "epoch:20 step:19022 [D loss: 0.652869, acc: 60.94%] [G loss: 1.891417]\n",
      "epoch:20 step:19023 [D loss: 0.679077, acc: 57.81%] [G loss: 2.002672]\n",
      "epoch:20 step:19024 [D loss: 0.652764, acc: 67.19%] [G loss: 1.859655]\n",
      "epoch:20 step:19025 [D loss: 0.650237, acc: 51.56%] [G loss: 1.828905]\n",
      "epoch:20 step:19026 [D loss: 0.655293, acc: 66.41%] [G loss: 1.944651]\n",
      "epoch:20 step:19027 [D loss: 0.673047, acc: 55.47%] [G loss: 1.837519]\n",
      "epoch:20 step:19028 [D loss: 0.718059, acc: 52.34%] [G loss: 1.938322]\n",
      "epoch:20 step:19029 [D loss: 0.594222, acc: 64.84%] [G loss: 1.855903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19030 [D loss: 0.658609, acc: 58.59%] [G loss: 1.914919]\n",
      "epoch:20 step:19031 [D loss: 0.676372, acc: 57.81%] [G loss: 1.900375]\n",
      "epoch:20 step:19032 [D loss: 0.599827, acc: 71.88%] [G loss: 1.856988]\n",
      "epoch:20 step:19033 [D loss: 0.652474, acc: 60.94%] [G loss: 1.991935]\n",
      "epoch:20 step:19034 [D loss: 0.650052, acc: 60.94%] [G loss: 1.850281]\n",
      "epoch:20 step:19035 [D loss: 0.676676, acc: 58.59%] [G loss: 2.014571]\n",
      "epoch:20 step:19036 [D loss: 0.616028, acc: 64.84%] [G loss: 1.996510]\n",
      "epoch:20 step:19037 [D loss: 0.705301, acc: 56.25%] [G loss: 1.860336]\n",
      "epoch:20 step:19038 [D loss: 0.583964, acc: 69.53%] [G loss: 2.010220]\n",
      "epoch:20 step:19039 [D loss: 0.638074, acc: 60.94%] [G loss: 2.019342]\n",
      "epoch:20 step:19040 [D loss: 0.622984, acc: 65.62%] [G loss: 1.865071]\n",
      "epoch:20 step:19041 [D loss: 0.705826, acc: 54.69%] [G loss: 1.907461]\n",
      "epoch:20 step:19042 [D loss: 0.580315, acc: 78.12%] [G loss: 1.929307]\n",
      "epoch:20 step:19043 [D loss: 0.690100, acc: 55.47%] [G loss: 1.746674]\n",
      "epoch:20 step:19044 [D loss: 0.634959, acc: 63.28%] [G loss: 1.838080]\n",
      "epoch:20 step:19045 [D loss: 0.692672, acc: 56.25%] [G loss: 1.773568]\n",
      "epoch:20 step:19046 [D loss: 0.664557, acc: 57.03%] [G loss: 1.711586]\n",
      "epoch:20 step:19047 [D loss: 0.688148, acc: 56.25%] [G loss: 1.895454]\n",
      "epoch:20 step:19048 [D loss: 0.613824, acc: 64.84%] [G loss: 1.733308]\n",
      "epoch:20 step:19049 [D loss: 0.610684, acc: 66.41%] [G loss: 1.890891]\n",
      "epoch:20 step:19050 [D loss: 0.648234, acc: 62.50%] [G loss: 1.874969]\n",
      "epoch:20 step:19051 [D loss: 0.646283, acc: 64.06%] [G loss: 1.902184]\n",
      "epoch:20 step:19052 [D loss: 0.643808, acc: 64.84%] [G loss: 2.264173]\n",
      "epoch:20 step:19053 [D loss: 0.603226, acc: 68.75%] [G loss: 2.206352]\n",
      "epoch:20 step:19054 [D loss: 0.603694, acc: 69.53%] [G loss: 2.139768]\n",
      "epoch:20 step:19055 [D loss: 0.594157, acc: 67.19%] [G loss: 2.179105]\n",
      "epoch:20 step:19056 [D loss: 0.678958, acc: 61.72%] [G loss: 1.827134]\n",
      "epoch:20 step:19057 [D loss: 0.736452, acc: 53.91%] [G loss: 1.749374]\n",
      "epoch:20 step:19058 [D loss: 0.623497, acc: 61.72%] [G loss: 1.779706]\n",
      "epoch:20 step:19059 [D loss: 0.666731, acc: 63.28%] [G loss: 1.882041]\n",
      "epoch:20 step:19060 [D loss: 0.709796, acc: 53.12%] [G loss: 1.849531]\n",
      "epoch:20 step:19061 [D loss: 0.652162, acc: 62.50%] [G loss: 1.946508]\n",
      "epoch:20 step:19062 [D loss: 0.663130, acc: 63.28%] [G loss: 1.759166]\n",
      "epoch:20 step:19063 [D loss: 0.685084, acc: 59.38%] [G loss: 1.737979]\n",
      "epoch:20 step:19064 [D loss: 0.656451, acc: 60.16%] [G loss: 1.766332]\n",
      "epoch:20 step:19065 [D loss: 0.662926, acc: 59.38%] [G loss: 1.780071]\n",
      "epoch:20 step:19066 [D loss: 0.674173, acc: 61.72%] [G loss: 1.808334]\n",
      "epoch:20 step:19067 [D loss: 0.646694, acc: 63.28%] [G loss: 1.906710]\n",
      "epoch:20 step:19068 [D loss: 0.637352, acc: 64.06%] [G loss: 1.955690]\n",
      "epoch:20 step:19069 [D loss: 0.610011, acc: 67.19%] [G loss: 1.786374]\n",
      "epoch:20 step:19070 [D loss: 0.665081, acc: 60.16%] [G loss: 2.019197]\n",
      "epoch:20 step:19071 [D loss: 0.663499, acc: 64.06%] [G loss: 1.808691]\n",
      "epoch:20 step:19072 [D loss: 0.626475, acc: 62.50%] [G loss: 1.969884]\n",
      "epoch:20 step:19073 [D loss: 0.656069, acc: 60.94%] [G loss: 1.978803]\n",
      "epoch:20 step:19074 [D loss: 0.671656, acc: 58.59%] [G loss: 1.908760]\n",
      "epoch:20 step:19075 [D loss: 0.644582, acc: 63.28%] [G loss: 1.886597]\n",
      "epoch:20 step:19076 [D loss: 0.681669, acc: 61.72%] [G loss: 2.005016]\n",
      "epoch:20 step:19077 [D loss: 0.644702, acc: 60.16%] [G loss: 1.965846]\n",
      "epoch:20 step:19078 [D loss: 0.589781, acc: 70.31%] [G loss: 2.010360]\n",
      "epoch:20 step:19079 [D loss: 0.640651, acc: 63.28%] [G loss: 1.868544]\n",
      "epoch:20 step:19080 [D loss: 0.610376, acc: 67.97%] [G loss: 1.956571]\n",
      "epoch:20 step:19081 [D loss: 0.659256, acc: 61.72%] [G loss: 1.868919]\n",
      "epoch:20 step:19082 [D loss: 0.653982, acc: 62.50%] [G loss: 1.872135]\n",
      "epoch:20 step:19083 [D loss: 0.624400, acc: 64.84%] [G loss: 1.908335]\n",
      "epoch:20 step:19084 [D loss: 0.640961, acc: 61.72%] [G loss: 1.957654]\n",
      "epoch:20 step:19085 [D loss: 0.575977, acc: 69.53%] [G loss: 2.069924]\n",
      "epoch:20 step:19086 [D loss: 0.617178, acc: 62.50%] [G loss: 2.278190]\n",
      "epoch:20 step:19087 [D loss: 0.667789, acc: 60.16%] [G loss: 2.103922]\n",
      "epoch:20 step:19088 [D loss: 0.633304, acc: 67.97%] [G loss: 1.830927]\n",
      "epoch:20 step:19089 [D loss: 0.687128, acc: 53.91%] [G loss: 1.749711]\n",
      "epoch:20 step:19090 [D loss: 0.613831, acc: 65.62%] [G loss: 1.925226]\n",
      "epoch:20 step:19091 [D loss: 0.615639, acc: 67.19%] [G loss: 1.828836]\n",
      "epoch:20 step:19092 [D loss: 0.650910, acc: 63.28%] [G loss: 1.731864]\n",
      "epoch:20 step:19093 [D loss: 0.651549, acc: 60.16%] [G loss: 2.100720]\n",
      "epoch:20 step:19094 [D loss: 0.605696, acc: 64.06%] [G loss: 2.124269]\n",
      "epoch:20 step:19095 [D loss: 0.668047, acc: 59.38%] [G loss: 1.896154]\n",
      "epoch:20 step:19096 [D loss: 0.661919, acc: 62.50%] [G loss: 1.794016]\n",
      "epoch:20 step:19097 [D loss: 0.663645, acc: 58.59%] [G loss: 2.030390]\n",
      "epoch:20 step:19098 [D loss: 0.618147, acc: 67.19%] [G loss: 1.924662]\n",
      "epoch:20 step:19099 [D loss: 0.615220, acc: 64.84%] [G loss: 1.923530]\n",
      "epoch:20 step:19100 [D loss: 0.620020, acc: 65.62%] [G loss: 1.977001]\n",
      "epoch:20 step:19101 [D loss: 0.639686, acc: 63.28%] [G loss: 1.832243]\n",
      "epoch:20 step:19102 [D loss: 0.638561, acc: 64.84%] [G loss: 1.896534]\n",
      "epoch:20 step:19103 [D loss: 0.644483, acc: 66.41%] [G loss: 1.795521]\n",
      "epoch:20 step:19104 [D loss: 0.630464, acc: 68.75%] [G loss: 1.921157]\n",
      "epoch:20 step:19105 [D loss: 0.623828, acc: 67.19%] [G loss: 1.968141]\n",
      "epoch:20 step:19106 [D loss: 0.640461, acc: 57.03%] [G loss: 1.843261]\n",
      "epoch:20 step:19107 [D loss: 0.647659, acc: 60.16%] [G loss: 1.826420]\n",
      "epoch:20 step:19108 [D loss: 0.596428, acc: 71.09%] [G loss: 1.799365]\n",
      "epoch:20 step:19109 [D loss: 0.626936, acc: 62.50%] [G loss: 1.966632]\n",
      "epoch:20 step:19110 [D loss: 0.571531, acc: 75.00%] [G loss: 2.159002]\n",
      "epoch:20 step:19111 [D loss: 0.615675, acc: 67.19%] [G loss: 2.009509]\n",
      "epoch:20 step:19112 [D loss: 0.684210, acc: 59.38%] [G loss: 1.771631]\n",
      "epoch:20 step:19113 [D loss: 0.666230, acc: 64.06%] [G loss: 1.926447]\n",
      "epoch:20 step:19114 [D loss: 0.625129, acc: 71.09%] [G loss: 1.953171]\n",
      "epoch:20 step:19115 [D loss: 0.649148, acc: 60.16%] [G loss: 1.923962]\n",
      "epoch:20 step:19116 [D loss: 0.660543, acc: 63.28%] [G loss: 1.896181]\n",
      "epoch:20 step:19117 [D loss: 0.712218, acc: 55.47%] [G loss: 1.741869]\n",
      "epoch:20 step:19118 [D loss: 0.657533, acc: 58.59%] [G loss: 1.908455]\n",
      "epoch:20 step:19119 [D loss: 0.569378, acc: 70.31%] [G loss: 1.923060]\n",
      "epoch:20 step:19120 [D loss: 0.672407, acc: 58.59%] [G loss: 1.937672]\n",
      "epoch:20 step:19121 [D loss: 0.646283, acc: 60.94%] [G loss: 1.985191]\n",
      "epoch:20 step:19122 [D loss: 0.642123, acc: 61.72%] [G loss: 1.889682]\n",
      "epoch:20 step:19123 [D loss: 0.658046, acc: 57.81%] [G loss: 2.056291]\n",
      "epoch:20 step:19124 [D loss: 0.649521, acc: 60.94%] [G loss: 2.076191]\n",
      "epoch:20 step:19125 [D loss: 0.611937, acc: 69.53%] [G loss: 2.158795]\n",
      "epoch:20 step:19126 [D loss: 0.714639, acc: 53.12%] [G loss: 1.705976]\n",
      "epoch:20 step:19127 [D loss: 0.679851, acc: 58.59%] [G loss: 1.818687]\n",
      "epoch:20 step:19128 [D loss: 0.641701, acc: 60.94%] [G loss: 1.967913]\n",
      "epoch:20 step:19129 [D loss: 0.665638, acc: 59.38%] [G loss: 1.801303]\n",
      "epoch:20 step:19130 [D loss: 0.644050, acc: 60.94%] [G loss: 1.821173]\n",
      "epoch:20 step:19131 [D loss: 0.640778, acc: 66.41%] [G loss: 1.774049]\n",
      "epoch:20 step:19132 [D loss: 0.699982, acc: 59.38%] [G loss: 1.927029]\n",
      "epoch:20 step:19133 [D loss: 0.628713, acc: 65.62%] [G loss: 1.789975]\n",
      "epoch:20 step:19134 [D loss: 0.658557, acc: 58.59%] [G loss: 1.917153]\n",
      "epoch:20 step:19135 [D loss: 0.674032, acc: 56.25%] [G loss: 1.881698]\n",
      "epoch:20 step:19136 [D loss: 0.766134, acc: 43.75%] [G loss: 1.788311]\n",
      "epoch:20 step:19137 [D loss: 0.608160, acc: 65.62%] [G loss: 1.744766]\n",
      "epoch:20 step:19138 [D loss: 0.655294, acc: 59.38%] [G loss: 1.811156]\n",
      "epoch:20 step:19139 [D loss: 0.694445, acc: 60.94%] [G loss: 1.728734]\n",
      "epoch:20 step:19140 [D loss: 0.660597, acc: 60.16%] [G loss: 1.890709]\n",
      "epoch:20 step:19141 [D loss: 0.676001, acc: 60.94%] [G loss: 1.861334]\n",
      "epoch:20 step:19142 [D loss: 0.650658, acc: 64.06%] [G loss: 1.852846]\n",
      "epoch:20 step:19143 [D loss: 0.640390, acc: 62.50%] [G loss: 1.770376]\n",
      "epoch:20 step:19144 [D loss: 0.669678, acc: 57.81%] [G loss: 1.871722]\n",
      "epoch:20 step:19145 [D loss: 0.607330, acc: 71.88%] [G loss: 2.036182]\n",
      "epoch:20 step:19146 [D loss: 0.612725, acc: 64.84%] [G loss: 2.071430]\n",
      "epoch:20 step:19147 [D loss: 0.630106, acc: 64.84%] [G loss: 1.832690]\n",
      "epoch:20 step:19148 [D loss: 0.654770, acc: 61.72%] [G loss: 1.821652]\n",
      "epoch:20 step:19149 [D loss: 0.622531, acc: 62.50%] [G loss: 2.004533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19150 [D loss: 0.648734, acc: 62.50%] [G loss: 1.820146]\n",
      "epoch:20 step:19151 [D loss: 0.620843, acc: 65.62%] [G loss: 2.075811]\n",
      "epoch:20 step:19152 [D loss: 0.597077, acc: 67.97%] [G loss: 1.996513]\n",
      "epoch:20 step:19153 [D loss: 0.662563, acc: 56.25%] [G loss: 1.865184]\n",
      "epoch:20 step:19154 [D loss: 0.654698, acc: 65.62%] [G loss: 2.023814]\n",
      "epoch:20 step:19155 [D loss: 0.577228, acc: 72.66%] [G loss: 1.889125]\n",
      "epoch:20 step:19156 [D loss: 0.610762, acc: 62.50%] [G loss: 2.070629]\n",
      "epoch:20 step:19157 [D loss: 0.657375, acc: 60.16%] [G loss: 1.992571]\n",
      "epoch:20 step:19158 [D loss: 0.682380, acc: 54.69%] [G loss: 1.768286]\n",
      "epoch:20 step:19159 [D loss: 0.673864, acc: 57.81%] [G loss: 1.991947]\n",
      "epoch:20 step:19160 [D loss: 0.652642, acc: 62.50%] [G loss: 1.887683]\n",
      "epoch:20 step:19161 [D loss: 0.636973, acc: 65.62%] [G loss: 1.787770]\n",
      "epoch:20 step:19162 [D loss: 0.663265, acc: 60.16%] [G loss: 1.785039]\n",
      "epoch:20 step:19163 [D loss: 0.670538, acc: 62.50%] [G loss: 1.805453]\n",
      "epoch:20 step:19164 [D loss: 0.766670, acc: 46.09%] [G loss: 1.753707]\n",
      "epoch:20 step:19165 [D loss: 0.668272, acc: 57.81%] [G loss: 1.854856]\n",
      "epoch:20 step:19166 [D loss: 0.630655, acc: 67.97%] [G loss: 1.842647]\n",
      "epoch:20 step:19167 [D loss: 0.658589, acc: 61.72%] [G loss: 2.092308]\n",
      "epoch:20 step:19168 [D loss: 0.553339, acc: 71.88%] [G loss: 2.019972]\n",
      "epoch:20 step:19169 [D loss: 0.575263, acc: 67.19%] [G loss: 2.138467]\n",
      "epoch:20 step:19170 [D loss: 0.609153, acc: 67.97%] [G loss: 2.046639]\n",
      "epoch:20 step:19171 [D loss: 0.655382, acc: 64.84%] [G loss: 1.932040]\n",
      "epoch:20 step:19172 [D loss: 0.637714, acc: 62.50%] [G loss: 1.733368]\n",
      "epoch:20 step:19173 [D loss: 0.691103, acc: 56.25%] [G loss: 1.901178]\n",
      "epoch:20 step:19174 [D loss: 0.610038, acc: 64.84%] [G loss: 1.915381]\n",
      "epoch:20 step:19175 [D loss: 0.649939, acc: 62.50%] [G loss: 2.027863]\n",
      "epoch:20 step:19176 [D loss: 0.654830, acc: 59.38%] [G loss: 1.904926]\n",
      "epoch:20 step:19177 [D loss: 0.751580, acc: 55.47%] [G loss: 1.784819]\n",
      "epoch:20 step:19178 [D loss: 0.697918, acc: 55.47%] [G loss: 1.660086]\n",
      "epoch:20 step:19179 [D loss: 0.687667, acc: 57.03%] [G loss: 1.835020]\n",
      "epoch:20 step:19180 [D loss: 0.646333, acc: 59.38%] [G loss: 1.799114]\n",
      "epoch:20 step:19181 [D loss: 0.665758, acc: 59.38%] [G loss: 1.834166]\n",
      "epoch:20 step:19182 [D loss: 0.706420, acc: 50.78%] [G loss: 1.894162]\n",
      "epoch:20 step:19183 [D loss: 0.672286, acc: 54.69%] [G loss: 1.640487]\n",
      "epoch:20 step:19184 [D loss: 0.673958, acc: 57.81%] [G loss: 1.790691]\n",
      "epoch:20 step:19185 [D loss: 0.632417, acc: 64.84%] [G loss: 1.811215]\n",
      "epoch:20 step:19186 [D loss: 0.649807, acc: 66.41%] [G loss: 1.839051]\n",
      "epoch:20 step:19187 [D loss: 0.628307, acc: 68.75%] [G loss: 1.873190]\n",
      "epoch:20 step:19188 [D loss: 0.669844, acc: 61.72%] [G loss: 1.687750]\n",
      "epoch:20 step:19189 [D loss: 0.660259, acc: 63.28%] [G loss: 1.765877]\n",
      "epoch:20 step:19190 [D loss: 0.647982, acc: 64.06%] [G loss: 1.727988]\n",
      "epoch:20 step:19191 [D loss: 0.637684, acc: 61.72%] [G loss: 1.727821]\n",
      "epoch:20 step:19192 [D loss: 0.623492, acc: 67.19%] [G loss: 1.802530]\n",
      "epoch:20 step:19193 [D loss: 0.674280, acc: 56.25%] [G loss: 1.862337]\n",
      "epoch:20 step:19194 [D loss: 0.641598, acc: 64.06%] [G loss: 1.824404]\n",
      "epoch:20 step:19195 [D loss: 0.662233, acc: 63.28%] [G loss: 1.865785]\n",
      "epoch:20 step:19196 [D loss: 0.649387, acc: 64.06%] [G loss: 1.821317]\n",
      "epoch:20 step:19197 [D loss: 0.643529, acc: 60.16%] [G loss: 1.893119]\n",
      "epoch:20 step:19198 [D loss: 0.650979, acc: 62.50%] [G loss: 1.920590]\n",
      "epoch:20 step:19199 [D loss: 0.718469, acc: 56.25%] [G loss: 1.697002]\n",
      "epoch:20 step:19200 [D loss: 0.650108, acc: 63.28%] [G loss: 1.695691]\n",
      "##############\n",
      "[2.48749871 1.55924529 6.05224353 4.80449397 3.77158217 5.69532943\n",
      " 4.33098747 4.76609461 4.61617812 3.49481605]\n",
      "##########\n",
      "epoch:20 step:19201 [D loss: 0.626275, acc: 66.41%] [G loss: 1.853870]\n",
      "epoch:20 step:19202 [D loss: 0.655692, acc: 64.84%] [G loss: 1.840997]\n",
      "epoch:20 step:19203 [D loss: 0.641111, acc: 63.28%] [G loss: 1.922756]\n",
      "epoch:20 step:19204 [D loss: 0.645468, acc: 58.59%] [G loss: 1.922104]\n",
      "epoch:20 step:19205 [D loss: 0.620241, acc: 62.50%] [G loss: 1.921949]\n",
      "epoch:20 step:19206 [D loss: 0.673288, acc: 61.72%] [G loss: 2.009728]\n",
      "epoch:20 step:19207 [D loss: 0.679034, acc: 59.38%] [G loss: 1.993525]\n",
      "epoch:20 step:19208 [D loss: 0.616210, acc: 69.53%] [G loss: 1.903974]\n",
      "epoch:20 step:19209 [D loss: 0.654828, acc: 63.28%] [G loss: 2.126781]\n",
      "epoch:20 step:19210 [D loss: 0.645263, acc: 62.50%] [G loss: 1.906689]\n",
      "epoch:20 step:19211 [D loss: 0.609981, acc: 66.41%] [G loss: 2.204404]\n",
      "epoch:20 step:19212 [D loss: 0.652813, acc: 60.94%] [G loss: 2.010636]\n",
      "epoch:20 step:19213 [D loss: 0.732143, acc: 50.00%] [G loss: 1.682080]\n",
      "epoch:20 step:19214 [D loss: 0.623030, acc: 62.50%] [G loss: 1.965651]\n",
      "epoch:20 step:19215 [D loss: 0.619416, acc: 69.53%] [G loss: 1.815728]\n",
      "epoch:20 step:19216 [D loss: 0.629646, acc: 62.50%] [G loss: 1.887679]\n",
      "epoch:20 step:19217 [D loss: 0.666321, acc: 60.94%] [G loss: 1.751679]\n",
      "epoch:20 step:19218 [D loss: 0.635566, acc: 60.16%] [G loss: 1.718328]\n",
      "epoch:20 step:19219 [D loss: 0.644459, acc: 58.59%] [G loss: 1.893631]\n",
      "epoch:20 step:19220 [D loss: 0.616422, acc: 66.41%] [G loss: 1.896969]\n",
      "epoch:20 step:19221 [D loss: 0.636176, acc: 64.84%] [G loss: 2.115559]\n",
      "epoch:20 step:19222 [D loss: 0.724001, acc: 54.69%] [G loss: 1.884758]\n",
      "epoch:20 step:19223 [D loss: 0.671339, acc: 61.72%] [G loss: 1.867641]\n",
      "epoch:20 step:19224 [D loss: 0.616886, acc: 69.53%] [G loss: 1.889633]\n",
      "epoch:20 step:19225 [D loss: 0.597698, acc: 64.84%] [G loss: 1.927941]\n",
      "epoch:20 step:19226 [D loss: 0.634207, acc: 63.28%] [G loss: 1.778005]\n",
      "epoch:20 step:19227 [D loss: 0.692404, acc: 61.72%] [G loss: 1.930957]\n",
      "epoch:20 step:19228 [D loss: 0.621192, acc: 67.97%] [G loss: 2.012268]\n",
      "epoch:20 step:19229 [D loss: 0.631804, acc: 64.84%] [G loss: 1.898037]\n",
      "epoch:20 step:19230 [D loss: 0.641059, acc: 63.28%] [G loss: 2.068380]\n",
      "epoch:20 step:19231 [D loss: 0.661301, acc: 60.94%] [G loss: 1.891230]\n",
      "epoch:20 step:19232 [D loss: 0.664062, acc: 57.03%] [G loss: 1.861083]\n",
      "epoch:20 step:19233 [D loss: 0.651215, acc: 64.06%] [G loss: 2.025733]\n",
      "epoch:20 step:19234 [D loss: 0.634635, acc: 62.50%] [G loss: 1.975404]\n",
      "epoch:20 step:19235 [D loss: 0.600900, acc: 67.97%] [G loss: 1.916034]\n",
      "epoch:20 step:19236 [D loss: 0.645694, acc: 68.75%] [G loss: 1.937509]\n",
      "epoch:20 step:19237 [D loss: 0.633763, acc: 62.50%] [G loss: 2.014688]\n",
      "epoch:20 step:19238 [D loss: 0.647518, acc: 65.62%] [G loss: 2.063826]\n",
      "epoch:20 step:19239 [D loss: 0.603411, acc: 67.19%] [G loss: 2.099383]\n",
      "epoch:20 step:19240 [D loss: 0.688126, acc: 57.81%] [G loss: 1.822563]\n",
      "epoch:20 step:19241 [D loss: 0.688210, acc: 55.47%] [G loss: 1.815152]\n",
      "epoch:20 step:19242 [D loss: 0.685451, acc: 50.00%] [G loss: 1.740956]\n",
      "epoch:20 step:19243 [D loss: 0.619929, acc: 65.62%] [G loss: 1.799889]\n",
      "epoch:20 step:19244 [D loss: 0.658877, acc: 61.72%] [G loss: 2.002077]\n",
      "epoch:20 step:19245 [D loss: 0.654807, acc: 68.75%] [G loss: 1.848914]\n",
      "epoch:20 step:19246 [D loss: 0.681442, acc: 59.38%] [G loss: 1.735179]\n",
      "epoch:20 step:19247 [D loss: 0.639192, acc: 63.28%] [G loss: 1.751459]\n",
      "epoch:20 step:19248 [D loss: 0.640744, acc: 60.94%] [G loss: 2.051860]\n",
      "epoch:20 step:19249 [D loss: 0.649054, acc: 64.84%] [G loss: 1.852947]\n",
      "epoch:20 step:19250 [D loss: 0.677679, acc: 57.81%] [G loss: 1.854353]\n",
      "epoch:20 step:19251 [D loss: 0.632910, acc: 63.28%] [G loss: 1.817710]\n",
      "epoch:20 step:19252 [D loss: 0.666912, acc: 60.94%] [G loss: 1.971476]\n",
      "epoch:20 step:19253 [D loss: 0.652983, acc: 60.16%] [G loss: 1.741762]\n",
      "epoch:20 step:19254 [D loss: 0.609909, acc: 71.88%] [G loss: 1.961729]\n",
      "epoch:20 step:19255 [D loss: 0.613419, acc: 67.19%] [G loss: 2.053162]\n",
      "epoch:20 step:19256 [D loss: 0.601964, acc: 68.75%] [G loss: 2.038706]\n",
      "epoch:20 step:19257 [D loss: 0.666207, acc: 61.72%] [G loss: 1.847125]\n",
      "epoch:20 step:19258 [D loss: 0.664923, acc: 58.59%] [G loss: 1.865112]\n",
      "epoch:20 step:19259 [D loss: 0.616866, acc: 60.94%] [G loss: 1.868086]\n",
      "epoch:20 step:19260 [D loss: 0.582251, acc: 71.09%] [G loss: 1.946818]\n",
      "epoch:20 step:19261 [D loss: 0.613248, acc: 64.06%] [G loss: 1.943985]\n",
      "epoch:20 step:19262 [D loss: 0.588090, acc: 67.19%] [G loss: 2.104535]\n",
      "epoch:20 step:19263 [D loss: 0.636386, acc: 61.72%] [G loss: 2.056382]\n",
      "epoch:20 step:19264 [D loss: 0.602391, acc: 66.41%] [G loss: 1.943182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19265 [D loss: 0.672473, acc: 59.38%] [G loss: 1.989637]\n",
      "epoch:20 step:19266 [D loss: 0.587277, acc: 74.22%] [G loss: 1.927693]\n",
      "epoch:20 step:19267 [D loss: 0.652138, acc: 60.94%] [G loss: 1.928495]\n",
      "epoch:20 step:19268 [D loss: 0.659524, acc: 60.16%] [G loss: 1.772275]\n",
      "epoch:20 step:19269 [D loss: 0.654591, acc: 60.94%] [G loss: 1.763104]\n",
      "epoch:20 step:19270 [D loss: 0.600374, acc: 70.31%] [G loss: 1.728261]\n",
      "epoch:20 step:19271 [D loss: 0.673067, acc: 59.38%] [G loss: 1.843474]\n",
      "epoch:20 step:19272 [D loss: 0.589991, acc: 67.19%] [G loss: 1.910224]\n",
      "epoch:20 step:19273 [D loss: 0.635699, acc: 62.50%] [G loss: 1.927294]\n",
      "epoch:20 step:19274 [D loss: 0.662670, acc: 59.38%] [G loss: 1.923369]\n",
      "epoch:20 step:19275 [D loss: 0.653980, acc: 63.28%] [G loss: 1.848526]\n",
      "epoch:20 step:19276 [D loss: 0.621422, acc: 66.41%] [G loss: 2.012583]\n",
      "epoch:20 step:19277 [D loss: 0.665247, acc: 61.72%] [G loss: 1.924549]\n",
      "epoch:20 step:19278 [D loss: 0.687155, acc: 60.94%] [G loss: 1.754839]\n",
      "epoch:20 step:19279 [D loss: 0.654458, acc: 59.38%] [G loss: 1.948163]\n",
      "epoch:20 step:19280 [D loss: 0.689228, acc: 57.03%] [G loss: 1.883324]\n",
      "epoch:20 step:19281 [D loss: 0.664190, acc: 61.72%] [G loss: 1.918094]\n",
      "epoch:20 step:19282 [D loss: 0.660268, acc: 61.72%] [G loss: 1.871048]\n",
      "epoch:20 step:19283 [D loss: 0.654628, acc: 58.59%] [G loss: 2.001811]\n",
      "epoch:20 step:19284 [D loss: 0.657435, acc: 65.62%] [G loss: 1.836190]\n",
      "epoch:20 step:19285 [D loss: 0.651825, acc: 60.16%] [G loss: 1.914482]\n",
      "epoch:20 step:19286 [D loss: 0.623235, acc: 61.72%] [G loss: 1.791032]\n",
      "epoch:20 step:19287 [D loss: 0.647387, acc: 64.84%] [G loss: 1.801479]\n",
      "epoch:20 step:19288 [D loss: 0.600828, acc: 68.75%] [G loss: 2.068536]\n",
      "epoch:20 step:19289 [D loss: 0.625447, acc: 68.75%] [G loss: 1.990116]\n",
      "epoch:20 step:19290 [D loss: 0.584230, acc: 67.19%] [G loss: 1.979893]\n",
      "epoch:20 step:19291 [D loss: 0.638055, acc: 57.03%] [G loss: 1.928721]\n",
      "epoch:20 step:19292 [D loss: 0.635649, acc: 67.97%] [G loss: 2.111276]\n",
      "epoch:20 step:19293 [D loss: 0.680470, acc: 61.72%] [G loss: 1.822651]\n",
      "epoch:20 step:19294 [D loss: 0.576087, acc: 69.53%] [G loss: 2.182151]\n",
      "epoch:20 step:19295 [D loss: 0.638383, acc: 59.38%] [G loss: 2.009927]\n",
      "epoch:20 step:19296 [D loss: 0.601729, acc: 69.53%] [G loss: 2.292789]\n",
      "epoch:20 step:19297 [D loss: 0.623305, acc: 67.97%] [G loss: 2.078345]\n",
      "epoch:20 step:19298 [D loss: 0.674491, acc: 62.50%] [G loss: 1.973616]\n",
      "epoch:20 step:19299 [D loss: 0.714014, acc: 51.56%] [G loss: 1.888853]\n",
      "epoch:20 step:19300 [D loss: 0.660832, acc: 61.72%] [G loss: 1.810125]\n",
      "epoch:20 step:19301 [D loss: 0.639420, acc: 61.72%] [G loss: 1.955497]\n",
      "epoch:20 step:19302 [D loss: 0.693805, acc: 60.16%] [G loss: 2.010009]\n",
      "epoch:20 step:19303 [D loss: 0.668948, acc: 59.38%] [G loss: 1.886273]\n",
      "epoch:20 step:19304 [D loss: 0.593005, acc: 67.19%] [G loss: 2.032348]\n",
      "epoch:20 step:19305 [D loss: 0.669980, acc: 60.16%] [G loss: 1.876894]\n",
      "epoch:20 step:19306 [D loss: 0.728610, acc: 50.78%] [G loss: 1.693543]\n",
      "epoch:20 step:19307 [D loss: 0.664381, acc: 57.81%] [G loss: 1.868520]\n",
      "epoch:20 step:19308 [D loss: 0.697882, acc: 59.38%] [G loss: 1.848834]\n",
      "epoch:20 step:19309 [D loss: 0.670545, acc: 54.69%] [G loss: 1.804211]\n",
      "epoch:20 step:19310 [D loss: 0.660077, acc: 59.38%] [G loss: 1.917462]\n",
      "epoch:20 step:19311 [D loss: 0.628368, acc: 62.50%] [G loss: 1.777911]\n",
      "epoch:20 step:19312 [D loss: 0.637417, acc: 64.84%] [G loss: 1.876863]\n",
      "epoch:20 step:19313 [D loss: 0.679419, acc: 60.94%] [G loss: 1.717314]\n",
      "epoch:20 step:19314 [D loss: 0.631743, acc: 66.41%] [G loss: 1.950596]\n",
      "epoch:20 step:19315 [D loss: 0.650030, acc: 62.50%] [G loss: 1.927940]\n",
      "epoch:20 step:19316 [D loss: 0.671934, acc: 56.25%] [G loss: 1.881413]\n",
      "epoch:20 step:19317 [D loss: 0.638063, acc: 60.94%] [G loss: 1.775566]\n",
      "epoch:20 step:19318 [D loss: 0.626143, acc: 60.16%] [G loss: 1.978100]\n",
      "epoch:20 step:19319 [D loss: 0.667556, acc: 60.94%] [G loss: 1.881794]\n",
      "epoch:20 step:19320 [D loss: 0.663124, acc: 62.50%] [G loss: 1.873616]\n",
      "epoch:20 step:19321 [D loss: 0.657445, acc: 60.94%] [G loss: 1.820258]\n",
      "epoch:20 step:19322 [D loss: 0.647822, acc: 63.28%] [G loss: 1.796008]\n",
      "epoch:20 step:19323 [D loss: 0.613126, acc: 68.75%] [G loss: 1.866179]\n",
      "epoch:20 step:19324 [D loss: 0.663567, acc: 62.50%] [G loss: 1.894935]\n",
      "epoch:20 step:19325 [D loss: 0.634884, acc: 64.84%] [G loss: 1.867815]\n",
      "epoch:20 step:19326 [D loss: 0.679147, acc: 61.72%] [G loss: 1.858104]\n",
      "epoch:20 step:19327 [D loss: 0.689932, acc: 54.69%] [G loss: 1.913300]\n",
      "epoch:20 step:19328 [D loss: 0.671807, acc: 62.50%] [G loss: 2.080283]\n",
      "epoch:20 step:19329 [D loss: 0.634463, acc: 67.97%] [G loss: 1.849164]\n",
      "epoch:20 step:19330 [D loss: 0.645273, acc: 60.16%] [G loss: 1.750433]\n",
      "epoch:20 step:19331 [D loss: 0.621407, acc: 67.97%] [G loss: 1.815679]\n",
      "epoch:20 step:19332 [D loss: 0.679626, acc: 61.72%] [G loss: 2.062637]\n",
      "epoch:20 step:19333 [D loss: 0.640687, acc: 57.81%] [G loss: 1.919343]\n",
      "epoch:20 step:19334 [D loss: 0.647581, acc: 60.94%] [G loss: 1.857191]\n",
      "epoch:20 step:19335 [D loss: 0.624730, acc: 66.41%] [G loss: 1.871635]\n",
      "epoch:20 step:19336 [D loss: 0.678179, acc: 60.16%] [G loss: 1.791506]\n",
      "epoch:20 step:19337 [D loss: 0.643922, acc: 62.50%] [G loss: 1.808561]\n",
      "epoch:20 step:19338 [D loss: 0.634683, acc: 63.28%] [G loss: 1.849272]\n",
      "epoch:20 step:19339 [D loss: 0.678636, acc: 56.25%] [G loss: 1.933041]\n",
      "epoch:20 step:19340 [D loss: 0.642105, acc: 67.19%] [G loss: 1.773587]\n",
      "epoch:20 step:19341 [D loss: 0.693302, acc: 53.91%] [G loss: 1.811948]\n",
      "epoch:20 step:19342 [D loss: 0.650728, acc: 58.59%] [G loss: 1.704533]\n",
      "epoch:20 step:19343 [D loss: 0.606805, acc: 64.06%] [G loss: 2.072533]\n",
      "epoch:20 step:19344 [D loss: 0.637987, acc: 62.50%] [G loss: 1.961984]\n",
      "epoch:20 step:19345 [D loss: 0.648938, acc: 61.72%] [G loss: 1.787544]\n",
      "epoch:20 step:19346 [D loss: 0.640666, acc: 57.81%] [G loss: 1.779351]\n",
      "epoch:20 step:19347 [D loss: 0.660035, acc: 58.59%] [G loss: 1.969212]\n",
      "epoch:20 step:19348 [D loss: 0.588751, acc: 71.09%] [G loss: 1.992412]\n",
      "epoch:20 step:19349 [D loss: 0.615572, acc: 70.31%] [G loss: 1.804230]\n",
      "epoch:20 step:19350 [D loss: 0.684010, acc: 56.25%] [G loss: 1.767597]\n",
      "epoch:20 step:19351 [D loss: 0.686343, acc: 57.03%] [G loss: 1.776749]\n",
      "epoch:20 step:19352 [D loss: 0.685410, acc: 60.94%] [G loss: 1.952459]\n",
      "epoch:20 step:19353 [D loss: 0.642404, acc: 60.94%] [G loss: 1.934544]\n",
      "epoch:20 step:19354 [D loss: 0.669047, acc: 59.38%] [G loss: 1.703882]\n",
      "epoch:20 step:19355 [D loss: 0.691615, acc: 52.34%] [G loss: 1.708338]\n",
      "epoch:20 step:19356 [D loss: 0.635945, acc: 70.31%] [G loss: 1.877559]\n",
      "epoch:20 step:19357 [D loss: 0.609936, acc: 70.31%] [G loss: 1.774906]\n",
      "epoch:20 step:19358 [D loss: 0.616242, acc: 64.84%] [G loss: 1.863005]\n",
      "epoch:20 step:19359 [D loss: 0.639496, acc: 69.53%] [G loss: 1.852920]\n",
      "epoch:20 step:19360 [D loss: 0.665739, acc: 56.25%] [G loss: 1.835097]\n",
      "epoch:20 step:19361 [D loss: 0.661823, acc: 60.94%] [G loss: 1.859167]\n",
      "epoch:20 step:19362 [D loss: 0.634744, acc: 63.28%] [G loss: 1.894982]\n",
      "epoch:20 step:19363 [D loss: 0.666991, acc: 58.59%] [G loss: 1.923527]\n",
      "epoch:20 step:19364 [D loss: 0.647406, acc: 63.28%] [G loss: 2.075233]\n",
      "epoch:20 step:19365 [D loss: 0.685736, acc: 60.16%] [G loss: 1.741011]\n",
      "epoch:20 step:19366 [D loss: 0.631239, acc: 62.50%] [G loss: 1.847045]\n",
      "epoch:20 step:19367 [D loss: 0.639705, acc: 67.19%] [G loss: 1.816250]\n",
      "epoch:20 step:19368 [D loss: 0.651166, acc: 64.06%] [G loss: 1.819567]\n",
      "epoch:20 step:19369 [D loss: 0.670393, acc: 57.81%] [G loss: 1.904651]\n",
      "epoch:20 step:19370 [D loss: 0.605598, acc: 66.41%] [G loss: 1.926029]\n",
      "epoch:20 step:19371 [D loss: 0.686146, acc: 55.47%] [G loss: 1.882147]\n",
      "epoch:20 step:19372 [D loss: 0.662500, acc: 70.31%] [G loss: 2.017641]\n",
      "epoch:20 step:19373 [D loss: 0.626643, acc: 67.19%] [G loss: 1.978086]\n",
      "epoch:20 step:19374 [D loss: 0.670175, acc: 60.94%] [G loss: 1.884668]\n",
      "epoch:20 step:19375 [D loss: 0.643690, acc: 60.94%] [G loss: 1.915130]\n",
      "epoch:20 step:19376 [D loss: 0.621474, acc: 64.06%] [G loss: 1.780171]\n",
      "epoch:20 step:19377 [D loss: 0.642299, acc: 66.41%] [G loss: 1.949491]\n",
      "epoch:20 step:19378 [D loss: 0.662509, acc: 65.62%] [G loss: 1.902354]\n",
      "epoch:20 step:19379 [D loss: 0.671153, acc: 59.38%] [G loss: 1.894879]\n",
      "epoch:20 step:19380 [D loss: 0.651289, acc: 66.41%] [G loss: 1.861558]\n",
      "epoch:20 step:19381 [D loss: 0.641490, acc: 60.16%] [G loss: 1.873097]\n",
      "epoch:20 step:19382 [D loss: 0.625133, acc: 64.84%] [G loss: 2.055273]\n",
      "epoch:20 step:19383 [D loss: 0.642427, acc: 59.38%] [G loss: 2.084495]\n",
      "epoch:20 step:19384 [D loss: 0.601855, acc: 64.06%] [G loss: 1.937931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19385 [D loss: 0.657162, acc: 60.16%] [G loss: 1.940576]\n",
      "epoch:20 step:19386 [D loss: 0.639909, acc: 64.06%] [G loss: 1.978763]\n",
      "epoch:20 step:19387 [D loss: 0.617711, acc: 67.97%] [G loss: 2.094788]\n",
      "epoch:20 step:19388 [D loss: 0.590978, acc: 64.06%] [G loss: 2.274558]\n",
      "epoch:20 step:19389 [D loss: 0.540165, acc: 76.56%] [G loss: 2.140439]\n",
      "epoch:20 step:19390 [D loss: 0.589439, acc: 70.31%] [G loss: 2.071829]\n",
      "epoch:20 step:19391 [D loss: 0.652448, acc: 64.06%] [G loss: 1.938980]\n",
      "epoch:20 step:19392 [D loss: 0.630584, acc: 65.62%] [G loss: 1.905710]\n",
      "epoch:20 step:19393 [D loss: 0.587183, acc: 70.31%] [G loss: 2.007599]\n",
      "epoch:20 step:19394 [D loss: 0.612275, acc: 67.19%] [G loss: 1.984247]\n",
      "epoch:20 step:19395 [D loss: 0.691505, acc: 54.69%] [G loss: 1.840008]\n",
      "epoch:20 step:19396 [D loss: 0.690387, acc: 60.94%] [G loss: 1.822507]\n",
      "epoch:20 step:19397 [D loss: 0.656668, acc: 57.03%] [G loss: 1.794054]\n",
      "epoch:20 step:19398 [D loss: 0.664771, acc: 57.03%] [G loss: 1.860500]\n",
      "epoch:20 step:19399 [D loss: 0.627261, acc: 58.59%] [G loss: 1.919547]\n",
      "epoch:20 step:19400 [D loss: 0.625528, acc: 68.75%] [G loss: 1.839028]\n",
      "##############\n",
      "[2.49375847 1.68071685 5.98695609 4.80585162 3.47393724 5.80224361\n",
      " 4.39220177 4.42953741 4.58185422 3.72282903]\n",
      "##########\n",
      "epoch:20 step:19401 [D loss: 0.660199, acc: 58.59%] [G loss: 1.801079]\n",
      "epoch:20 step:19402 [D loss: 0.629857, acc: 66.41%] [G loss: 1.919648]\n",
      "epoch:20 step:19403 [D loss: 0.648014, acc: 60.16%] [G loss: 1.876472]\n",
      "epoch:20 step:19404 [D loss: 0.682980, acc: 57.81%] [G loss: 1.896223]\n",
      "epoch:20 step:19405 [D loss: 0.682132, acc: 56.25%] [G loss: 1.919882]\n",
      "epoch:20 step:19406 [D loss: 0.692813, acc: 58.59%] [G loss: 1.734278]\n",
      "epoch:20 step:19407 [D loss: 0.695485, acc: 51.56%] [G loss: 1.771665]\n",
      "epoch:20 step:19408 [D loss: 0.642026, acc: 59.38%] [G loss: 1.881084]\n",
      "epoch:20 step:19409 [D loss: 0.598639, acc: 73.44%] [G loss: 1.874565]\n",
      "epoch:20 step:19410 [D loss: 0.715467, acc: 52.34%] [G loss: 1.808533]\n",
      "epoch:20 step:19411 [D loss: 0.659756, acc: 64.06%] [G loss: 1.783418]\n",
      "epoch:20 step:19412 [D loss: 0.657460, acc: 59.38%] [G loss: 1.863525]\n",
      "epoch:20 step:19413 [D loss: 0.624535, acc: 70.31%] [G loss: 1.787108]\n",
      "epoch:20 step:19414 [D loss: 0.624665, acc: 65.62%] [G loss: 1.784858]\n",
      "epoch:20 step:19415 [D loss: 0.714525, acc: 57.81%] [G loss: 1.866263]\n",
      "epoch:20 step:19416 [D loss: 0.660442, acc: 63.28%] [G loss: 1.792074]\n",
      "epoch:20 step:19417 [D loss: 0.630073, acc: 62.50%] [G loss: 1.862812]\n",
      "epoch:20 step:19418 [D loss: 0.638136, acc: 61.72%] [G loss: 1.864583]\n",
      "epoch:20 step:19419 [D loss: 0.670331, acc: 52.34%] [G loss: 1.888224]\n",
      "epoch:20 step:19420 [D loss: 0.640546, acc: 61.72%] [G loss: 1.989333]\n",
      "epoch:20 step:19421 [D loss: 0.714861, acc: 56.25%] [G loss: 1.871422]\n",
      "epoch:20 step:19422 [D loss: 0.674957, acc: 60.16%] [G loss: 1.821680]\n",
      "epoch:20 step:19423 [D loss: 0.654215, acc: 62.50%] [G loss: 1.861328]\n",
      "epoch:20 step:19424 [D loss: 0.641456, acc: 61.72%] [G loss: 1.780186]\n",
      "epoch:20 step:19425 [D loss: 0.614578, acc: 64.84%] [G loss: 1.904053]\n",
      "epoch:20 step:19426 [D loss: 0.609557, acc: 64.06%] [G loss: 1.811351]\n",
      "epoch:20 step:19427 [D loss: 0.629004, acc: 63.28%] [G loss: 1.939140]\n",
      "epoch:20 step:19428 [D loss: 0.672560, acc: 58.59%] [G loss: 1.915889]\n",
      "epoch:20 step:19429 [D loss: 0.636825, acc: 62.50%] [G loss: 1.839669]\n",
      "epoch:20 step:19430 [D loss: 0.618313, acc: 63.28%] [G loss: 1.984089]\n",
      "epoch:20 step:19431 [D loss: 0.632523, acc: 62.50%] [G loss: 1.987234]\n",
      "epoch:20 step:19432 [D loss: 0.650303, acc: 64.06%] [G loss: 2.174837]\n",
      "epoch:20 step:19433 [D loss: 0.624811, acc: 63.28%] [G loss: 2.034773]\n",
      "epoch:20 step:19434 [D loss: 0.612141, acc: 62.50%] [G loss: 2.100623]\n",
      "epoch:20 step:19435 [D loss: 0.627774, acc: 67.19%] [G loss: 1.994684]\n",
      "epoch:20 step:19436 [D loss: 0.632377, acc: 65.62%] [G loss: 1.845764]\n",
      "epoch:20 step:19437 [D loss: 0.683818, acc: 57.03%] [G loss: 1.834957]\n",
      "epoch:20 step:19438 [D loss: 0.671433, acc: 59.38%] [G loss: 1.934740]\n",
      "epoch:20 step:19439 [D loss: 0.643876, acc: 66.41%] [G loss: 1.977385]\n",
      "epoch:20 step:19440 [D loss: 0.630126, acc: 68.75%] [G loss: 1.823767]\n",
      "epoch:20 step:19441 [D loss: 0.723716, acc: 55.47%] [G loss: 1.942751]\n",
      "epoch:20 step:19442 [D loss: 0.690730, acc: 52.34%] [G loss: 1.837609]\n",
      "epoch:20 step:19443 [D loss: 0.645212, acc: 63.28%] [G loss: 1.859725]\n",
      "epoch:20 step:19444 [D loss: 0.673296, acc: 54.69%] [G loss: 1.861649]\n",
      "epoch:20 step:19445 [D loss: 0.684880, acc: 57.03%] [G loss: 1.877524]\n",
      "epoch:20 step:19446 [D loss: 0.572923, acc: 75.78%] [G loss: 1.962333]\n",
      "epoch:20 step:19447 [D loss: 0.627073, acc: 66.41%] [G loss: 1.939537]\n",
      "epoch:20 step:19448 [D loss: 0.660988, acc: 59.38%] [G loss: 1.896962]\n",
      "epoch:20 step:19449 [D loss: 0.593723, acc: 66.41%] [G loss: 2.121409]\n",
      "epoch:20 step:19450 [D loss: 0.709047, acc: 53.12%] [G loss: 1.735459]\n",
      "epoch:20 step:19451 [D loss: 0.620517, acc: 61.72%] [G loss: 1.922961]\n",
      "epoch:20 step:19452 [D loss: 0.634397, acc: 65.62%] [G loss: 1.882515]\n",
      "epoch:20 step:19453 [D loss: 0.610812, acc: 69.53%] [G loss: 1.915197]\n",
      "epoch:20 step:19454 [D loss: 0.656002, acc: 64.06%] [G loss: 2.045273]\n",
      "epoch:20 step:19455 [D loss: 0.637560, acc: 66.41%] [G loss: 2.042314]\n",
      "epoch:20 step:19456 [D loss: 0.640884, acc: 63.28%] [G loss: 1.805298]\n",
      "epoch:20 step:19457 [D loss: 0.637955, acc: 62.50%] [G loss: 1.915621]\n",
      "epoch:20 step:19458 [D loss: 0.673033, acc: 61.72%] [G loss: 1.844373]\n",
      "epoch:20 step:19459 [D loss: 0.604763, acc: 67.97%] [G loss: 1.962252]\n",
      "epoch:20 step:19460 [D loss: 0.643646, acc: 62.50%] [G loss: 1.894355]\n",
      "epoch:20 step:19461 [D loss: 0.631849, acc: 58.59%] [G loss: 1.983145]\n",
      "epoch:20 step:19462 [D loss: 0.638003, acc: 64.06%] [G loss: 1.863636]\n",
      "epoch:20 step:19463 [D loss: 0.638177, acc: 67.97%] [G loss: 1.964509]\n",
      "epoch:20 step:19464 [D loss: 0.569212, acc: 77.34%] [G loss: 1.881725]\n",
      "epoch:20 step:19465 [D loss: 0.668433, acc: 61.72%] [G loss: 1.896754]\n",
      "epoch:20 step:19466 [D loss: 0.619939, acc: 61.72%] [G loss: 1.989908]\n",
      "epoch:20 step:19467 [D loss: 0.691371, acc: 55.47%] [G loss: 1.851675]\n",
      "epoch:20 step:19468 [D loss: 0.631327, acc: 64.06%] [G loss: 1.898204]\n",
      "epoch:20 step:19469 [D loss: 0.660546, acc: 61.72%] [G loss: 1.980866]\n",
      "epoch:20 step:19470 [D loss: 0.647944, acc: 61.72%] [G loss: 1.879224]\n",
      "epoch:20 step:19471 [D loss: 0.642430, acc: 60.16%] [G loss: 2.032317]\n",
      "epoch:20 step:19472 [D loss: 0.598570, acc: 73.44%] [G loss: 1.987619]\n",
      "epoch:20 step:19473 [D loss: 0.614931, acc: 68.75%] [G loss: 1.964699]\n",
      "epoch:20 step:19474 [D loss: 0.665863, acc: 58.59%] [G loss: 1.938170]\n",
      "epoch:20 step:19475 [D loss: 0.642229, acc: 62.50%] [G loss: 1.859888]\n",
      "epoch:20 step:19476 [D loss: 0.663693, acc: 63.28%] [G loss: 1.892229]\n",
      "epoch:20 step:19477 [D loss: 0.599401, acc: 71.88%] [G loss: 1.962934]\n",
      "epoch:20 step:19478 [D loss: 0.654737, acc: 64.84%] [G loss: 1.902698]\n",
      "epoch:20 step:19479 [D loss: 0.626583, acc: 64.06%] [G loss: 1.961016]\n",
      "epoch:20 step:19480 [D loss: 0.617772, acc: 66.41%] [G loss: 1.918329]\n",
      "epoch:20 step:19481 [D loss: 0.669745, acc: 60.94%] [G loss: 1.960696]\n",
      "epoch:20 step:19482 [D loss: 0.669174, acc: 60.16%] [G loss: 1.935113]\n",
      "epoch:20 step:19483 [D loss: 0.635943, acc: 64.84%] [G loss: 1.975135]\n",
      "epoch:20 step:19484 [D loss: 0.634004, acc: 57.81%] [G loss: 1.876883]\n",
      "epoch:20 step:19485 [D loss: 0.661763, acc: 61.72%] [G loss: 1.920441]\n",
      "epoch:20 step:19486 [D loss: 0.623560, acc: 59.38%] [G loss: 2.014486]\n",
      "epoch:20 step:19487 [D loss: 0.600518, acc: 65.62%] [G loss: 2.030988]\n",
      "epoch:20 step:19488 [D loss: 0.659591, acc: 64.06%] [G loss: 1.916682]\n",
      "epoch:20 step:19489 [D loss: 0.692581, acc: 57.03%] [G loss: 1.806523]\n",
      "epoch:20 step:19490 [D loss: 0.639230, acc: 63.28%] [G loss: 1.741735]\n",
      "epoch:20 step:19491 [D loss: 0.641866, acc: 64.06%] [G loss: 1.963933]\n",
      "epoch:20 step:19492 [D loss: 0.669571, acc: 57.03%] [G loss: 1.832592]\n",
      "epoch:20 step:19493 [D loss: 0.604881, acc: 67.97%] [G loss: 1.903812]\n",
      "epoch:20 step:19494 [D loss: 0.610039, acc: 64.84%] [G loss: 1.875747]\n",
      "epoch:20 step:19495 [D loss: 0.650660, acc: 60.94%] [G loss: 1.827415]\n",
      "epoch:20 step:19496 [D loss: 0.630280, acc: 68.75%] [G loss: 2.078347]\n",
      "epoch:20 step:19497 [D loss: 0.715169, acc: 50.78%] [G loss: 1.907777]\n",
      "epoch:20 step:19498 [D loss: 0.714895, acc: 57.03%] [G loss: 1.851496]\n",
      "epoch:20 step:19499 [D loss: 0.613272, acc: 69.53%] [G loss: 1.891574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19500 [D loss: 0.666801, acc: 60.16%] [G loss: 1.877984]\n",
      "epoch:20 step:19501 [D loss: 0.655022, acc: 60.16%] [G loss: 1.979326]\n",
      "epoch:20 step:19502 [D loss: 0.666538, acc: 57.81%] [G loss: 1.829399]\n",
      "epoch:20 step:19503 [D loss: 0.676370, acc: 54.69%] [G loss: 1.813348]\n",
      "epoch:20 step:19504 [D loss: 0.634658, acc: 63.28%] [G loss: 1.895180]\n",
      "epoch:20 step:19505 [D loss: 0.720599, acc: 56.25%] [G loss: 1.681531]\n",
      "epoch:20 step:19506 [D loss: 0.694267, acc: 55.47%] [G loss: 1.682316]\n",
      "epoch:20 step:19507 [D loss: 0.643940, acc: 66.41%] [G loss: 1.757424]\n",
      "epoch:20 step:19508 [D loss: 0.689592, acc: 57.03%] [G loss: 1.746537]\n",
      "epoch:20 step:19509 [D loss: 0.649673, acc: 55.47%] [G loss: 1.915645]\n",
      "epoch:20 step:19510 [D loss: 0.668970, acc: 56.25%] [G loss: 1.846992]\n",
      "epoch:20 step:19511 [D loss: 0.627084, acc: 66.41%] [G loss: 1.859082]\n",
      "epoch:20 step:19512 [D loss: 0.664515, acc: 63.28%] [G loss: 1.790889]\n",
      "epoch:20 step:19513 [D loss: 0.637130, acc: 64.06%] [G loss: 1.875993]\n",
      "epoch:20 step:19514 [D loss: 0.633790, acc: 67.19%] [G loss: 2.030529]\n",
      "epoch:20 step:19515 [D loss: 0.715371, acc: 59.38%] [G loss: 1.992576]\n",
      "epoch:20 step:19516 [D loss: 0.619827, acc: 63.28%] [G loss: 1.961554]\n",
      "epoch:20 step:19517 [D loss: 0.615454, acc: 63.28%] [G loss: 1.750069]\n",
      "epoch:20 step:19518 [D loss: 0.671989, acc: 64.06%] [G loss: 1.943223]\n",
      "epoch:20 step:19519 [D loss: 0.665817, acc: 60.94%] [G loss: 1.771527]\n",
      "epoch:20 step:19520 [D loss: 0.619758, acc: 67.97%] [G loss: 1.927933]\n",
      "epoch:20 step:19521 [D loss: 0.557024, acc: 73.44%] [G loss: 1.982874]\n",
      "epoch:20 step:19522 [D loss: 0.620397, acc: 60.16%] [G loss: 2.049351]\n",
      "epoch:20 step:19523 [D loss: 0.621162, acc: 67.19%] [G loss: 1.983626]\n",
      "epoch:20 step:19524 [D loss: 0.684819, acc: 57.81%] [G loss: 1.823293]\n",
      "epoch:20 step:19525 [D loss: 0.635325, acc: 66.41%] [G loss: 1.994313]\n",
      "epoch:20 step:19526 [D loss: 0.632923, acc: 69.53%] [G loss: 1.835989]\n",
      "epoch:20 step:19527 [D loss: 0.696414, acc: 56.25%] [G loss: 1.955948]\n",
      "epoch:20 step:19528 [D loss: 0.657764, acc: 59.38%] [G loss: 1.697121]\n",
      "epoch:20 step:19529 [D loss: 0.688141, acc: 60.16%] [G loss: 1.837947]\n",
      "epoch:20 step:19530 [D loss: 0.652322, acc: 61.72%] [G loss: 1.896040]\n",
      "epoch:20 step:19531 [D loss: 0.683856, acc: 60.16%] [G loss: 1.789532]\n",
      "epoch:20 step:19532 [D loss: 0.653954, acc: 62.50%] [G loss: 2.021643]\n",
      "epoch:20 step:19533 [D loss: 0.662427, acc: 58.59%] [G loss: 1.908682]\n",
      "epoch:20 step:19534 [D loss: 0.670508, acc: 50.78%] [G loss: 1.783142]\n",
      "epoch:20 step:19535 [D loss: 0.651229, acc: 63.28%] [G loss: 1.869509]\n",
      "epoch:20 step:19536 [D loss: 0.644029, acc: 63.28%] [G loss: 1.957030]\n",
      "epoch:20 step:19537 [D loss: 0.637770, acc: 64.84%] [G loss: 1.989358]\n",
      "epoch:20 step:19538 [D loss: 0.673953, acc: 61.72%] [G loss: 1.879868]\n",
      "epoch:20 step:19539 [D loss: 0.663856, acc: 59.38%] [G loss: 1.833244]\n",
      "epoch:20 step:19540 [D loss: 0.727476, acc: 45.31%] [G loss: 1.720506]\n",
      "epoch:20 step:19541 [D loss: 0.659904, acc: 59.38%] [G loss: 1.858740]\n",
      "epoch:20 step:19542 [D loss: 0.648188, acc: 63.28%] [G loss: 1.946519]\n",
      "epoch:20 step:19543 [D loss: 0.647064, acc: 60.94%] [G loss: 1.958979]\n",
      "epoch:20 step:19544 [D loss: 0.589900, acc: 64.84%] [G loss: 1.849865]\n",
      "epoch:20 step:19545 [D loss: 0.610233, acc: 71.88%] [G loss: 1.964440]\n",
      "epoch:20 step:19546 [D loss: 0.651295, acc: 64.84%] [G loss: 1.945296]\n",
      "epoch:20 step:19547 [D loss: 0.637731, acc: 66.41%] [G loss: 1.918151]\n",
      "epoch:20 step:19548 [D loss: 0.633276, acc: 64.06%] [G loss: 1.928856]\n",
      "epoch:20 step:19549 [D loss: 0.631741, acc: 61.72%] [G loss: 1.952588]\n",
      "epoch:20 step:19550 [D loss: 0.636288, acc: 62.50%] [G loss: 1.827749]\n",
      "epoch:20 step:19551 [D loss: 0.621166, acc: 65.62%] [G loss: 1.856283]\n",
      "epoch:20 step:19552 [D loss: 0.637193, acc: 67.19%] [G loss: 1.820811]\n",
      "epoch:20 step:19553 [D loss: 0.652268, acc: 62.50%] [G loss: 1.848814]\n",
      "epoch:20 step:19554 [D loss: 0.612620, acc: 65.62%] [G loss: 1.979242]\n",
      "epoch:20 step:19555 [D loss: 0.652163, acc: 61.72%] [G loss: 1.964944]\n",
      "epoch:20 step:19556 [D loss: 0.610976, acc: 64.84%] [G loss: 2.001613]\n",
      "epoch:20 step:19557 [D loss: 0.638554, acc: 63.28%] [G loss: 1.889723]\n",
      "epoch:20 step:19558 [D loss: 0.669484, acc: 61.72%] [G loss: 1.749221]\n",
      "epoch:20 step:19559 [D loss: 0.624692, acc: 61.72%] [G loss: 1.935654]\n",
      "epoch:20 step:19560 [D loss: 0.699759, acc: 54.69%] [G loss: 1.807261]\n",
      "epoch:20 step:19561 [D loss: 0.626670, acc: 63.28%] [G loss: 1.839175]\n",
      "epoch:20 step:19562 [D loss: 0.627011, acc: 67.19%] [G loss: 1.890873]\n",
      "epoch:20 step:19563 [D loss: 0.597654, acc: 72.66%] [G loss: 2.088978]\n",
      "epoch:20 step:19564 [D loss: 0.663089, acc: 60.94%] [G loss: 1.831985]\n",
      "epoch:20 step:19565 [D loss: 0.607205, acc: 68.75%] [G loss: 2.054298]\n",
      "epoch:20 step:19566 [D loss: 0.637462, acc: 61.72%] [G loss: 1.883919]\n",
      "epoch:20 step:19567 [D loss: 0.669412, acc: 55.47%] [G loss: 1.701091]\n",
      "epoch:20 step:19568 [D loss: 0.672880, acc: 55.47%] [G loss: 1.887918]\n",
      "epoch:20 step:19569 [D loss: 0.620696, acc: 70.31%] [G loss: 1.775913]\n",
      "epoch:20 step:19570 [D loss: 0.676858, acc: 60.16%] [G loss: 1.853021]\n",
      "epoch:20 step:19571 [D loss: 0.653699, acc: 67.97%] [G loss: 2.007897]\n",
      "epoch:20 step:19572 [D loss: 0.636210, acc: 65.62%] [G loss: 1.850115]\n",
      "epoch:20 step:19573 [D loss: 0.656265, acc: 64.84%] [G loss: 1.970385]\n",
      "epoch:20 step:19574 [D loss: 0.676798, acc: 58.59%] [G loss: 1.877462]\n",
      "epoch:20 step:19575 [D loss: 0.630948, acc: 63.28%] [G loss: 1.872061]\n",
      "epoch:20 step:19576 [D loss: 0.666897, acc: 60.16%] [G loss: 1.904405]\n",
      "epoch:20 step:19577 [D loss: 0.630250, acc: 62.50%] [G loss: 1.928518]\n",
      "epoch:20 step:19578 [D loss: 0.610146, acc: 65.62%] [G loss: 1.841309]\n",
      "epoch:20 step:19579 [D loss: 0.601118, acc: 68.75%] [G loss: 1.844956]\n",
      "epoch:20 step:19580 [D loss: 0.640582, acc: 62.50%] [G loss: 1.937615]\n",
      "epoch:20 step:19581 [D loss: 0.639663, acc: 66.41%] [G loss: 1.946723]\n",
      "epoch:20 step:19582 [D loss: 0.637223, acc: 62.50%] [G loss: 1.952807]\n",
      "epoch:20 step:19583 [D loss: 0.659461, acc: 64.06%] [G loss: 2.036109]\n",
      "epoch:20 step:19584 [D loss: 0.647781, acc: 64.84%] [G loss: 1.976527]\n",
      "epoch:20 step:19585 [D loss: 0.636143, acc: 61.72%] [G loss: 1.843741]\n",
      "epoch:20 step:19586 [D loss: 0.700028, acc: 64.06%] [G loss: 1.753149]\n",
      "epoch:20 step:19587 [D loss: 0.631881, acc: 60.94%] [G loss: 1.993727]\n",
      "epoch:20 step:19588 [D loss: 0.638550, acc: 63.28%] [G loss: 1.911047]\n",
      "epoch:20 step:19589 [D loss: 0.603875, acc: 66.41%] [G loss: 2.066008]\n",
      "epoch:20 step:19590 [D loss: 0.664591, acc: 60.94%] [G loss: 1.921417]\n",
      "epoch:20 step:19591 [D loss: 0.652812, acc: 64.06%] [G loss: 1.878559]\n",
      "epoch:20 step:19592 [D loss: 0.727001, acc: 55.47%] [G loss: 1.977270]\n",
      "epoch:20 step:19593 [D loss: 0.670889, acc: 62.50%] [G loss: 1.856109]\n",
      "epoch:20 step:19594 [D loss: 0.612624, acc: 64.84%] [G loss: 1.838188]\n",
      "epoch:20 step:19595 [D loss: 0.666185, acc: 59.38%] [G loss: 1.781431]\n",
      "epoch:20 step:19596 [D loss: 0.719006, acc: 53.91%] [G loss: 1.764158]\n",
      "epoch:20 step:19597 [D loss: 0.669742, acc: 64.84%] [G loss: 1.832780]\n",
      "epoch:20 step:19598 [D loss: 0.663506, acc: 57.03%] [G loss: 1.833188]\n",
      "epoch:20 step:19599 [D loss: 0.642300, acc: 60.94%] [G loss: 1.864604]\n",
      "epoch:20 step:19600 [D loss: 0.601695, acc: 67.19%] [G loss: 1.910287]\n",
      "##############\n",
      "[2.53994676 1.43195099 6.11113129 4.91134149 3.68374589 5.58673526\n",
      " 4.30947467 4.46683624 4.51525783 3.61084667]\n",
      "##########\n",
      "epoch:20 step:19601 [D loss: 0.644620, acc: 59.38%] [G loss: 1.872734]\n",
      "epoch:20 step:19602 [D loss: 0.626669, acc: 65.62%] [G loss: 1.829364]\n",
      "epoch:20 step:19603 [D loss: 0.649055, acc: 64.06%] [G loss: 1.900385]\n",
      "epoch:20 step:19604 [D loss: 0.658774, acc: 62.50%] [G loss: 1.927570]\n",
      "epoch:20 step:19605 [D loss: 0.621385, acc: 63.28%] [G loss: 1.809463]\n",
      "epoch:20 step:19606 [D loss: 0.651318, acc: 59.38%] [G loss: 1.901829]\n",
      "epoch:20 step:19607 [D loss: 0.680443, acc: 61.72%] [G loss: 1.802651]\n",
      "epoch:20 step:19608 [D loss: 0.627356, acc: 67.97%] [G loss: 1.933219]\n",
      "epoch:20 step:19609 [D loss: 0.702283, acc: 50.00%] [G loss: 1.804302]\n",
      "epoch:20 step:19610 [D loss: 0.665809, acc: 57.81%] [G loss: 1.843910]\n",
      "epoch:20 step:19611 [D loss: 0.679298, acc: 53.91%] [G loss: 1.784247]\n",
      "epoch:20 step:19612 [D loss: 0.633892, acc: 67.19%] [G loss: 1.764395]\n",
      "epoch:20 step:19613 [D loss: 0.627361, acc: 65.62%] [G loss: 1.678362]\n",
      "epoch:20 step:19614 [D loss: 0.643078, acc: 61.72%] [G loss: 1.883382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19615 [D loss: 0.619639, acc: 65.62%] [G loss: 1.952236]\n",
      "epoch:20 step:19616 [D loss: 0.687132, acc: 56.25%] [G loss: 1.943065]\n",
      "epoch:20 step:19617 [D loss: 0.662149, acc: 57.81%] [G loss: 1.874514]\n",
      "epoch:20 step:19618 [D loss: 0.617823, acc: 61.72%] [G loss: 1.782994]\n",
      "epoch:20 step:19619 [D loss: 0.635138, acc: 68.75%] [G loss: 1.911368]\n",
      "epoch:20 step:19620 [D loss: 0.655246, acc: 64.84%] [G loss: 1.808662]\n",
      "epoch:20 step:19621 [D loss: 0.611142, acc: 65.62%] [G loss: 2.037548]\n",
      "epoch:20 step:19622 [D loss: 0.676371, acc: 58.59%] [G loss: 1.845801]\n",
      "epoch:20 step:19623 [D loss: 0.699389, acc: 50.00%] [G loss: 1.744615]\n",
      "epoch:20 step:19624 [D loss: 0.635454, acc: 67.19%] [G loss: 1.905590]\n",
      "epoch:20 step:19625 [D loss: 0.604467, acc: 71.09%] [G loss: 2.002878]\n",
      "epoch:20 step:19626 [D loss: 0.627934, acc: 60.94%] [G loss: 1.976688]\n",
      "epoch:20 step:19627 [D loss: 0.655251, acc: 56.25%] [G loss: 1.922671]\n",
      "epoch:20 step:19628 [D loss: 0.684298, acc: 53.91%] [G loss: 1.991946]\n",
      "epoch:20 step:19629 [D loss: 0.595463, acc: 71.09%] [G loss: 1.849653]\n",
      "epoch:20 step:19630 [D loss: 0.642262, acc: 64.06%] [G loss: 1.971861]\n",
      "epoch:20 step:19631 [D loss: 0.653124, acc: 53.91%] [G loss: 1.813629]\n",
      "epoch:20 step:19632 [D loss: 0.709582, acc: 60.94%] [G loss: 1.886811]\n",
      "epoch:20 step:19633 [D loss: 0.620571, acc: 66.41%] [G loss: 2.019188]\n",
      "epoch:20 step:19634 [D loss: 0.645291, acc: 62.50%] [G loss: 1.977540]\n",
      "epoch:20 step:19635 [D loss: 0.673304, acc: 60.16%] [G loss: 1.778142]\n",
      "epoch:20 step:19636 [D loss: 0.677911, acc: 56.25%] [G loss: 1.823864]\n",
      "epoch:20 step:19637 [D loss: 0.654754, acc: 57.03%] [G loss: 1.905098]\n",
      "epoch:20 step:19638 [D loss: 0.681851, acc: 58.59%] [G loss: 1.788877]\n",
      "epoch:20 step:19639 [D loss: 0.630757, acc: 66.41%] [G loss: 1.977625]\n",
      "epoch:20 step:19640 [D loss: 0.643127, acc: 62.50%] [G loss: 2.014374]\n",
      "epoch:20 step:19641 [D loss: 0.586729, acc: 73.44%] [G loss: 1.921589]\n",
      "epoch:20 step:19642 [D loss: 0.629425, acc: 68.75%] [G loss: 1.880298]\n",
      "epoch:20 step:19643 [D loss: 0.648884, acc: 62.50%] [G loss: 1.741957]\n",
      "epoch:20 step:19644 [D loss: 0.676973, acc: 57.03%] [G loss: 1.974665]\n",
      "epoch:20 step:19645 [D loss: 0.607132, acc: 64.84%] [G loss: 1.988627]\n",
      "epoch:20 step:19646 [D loss: 0.646363, acc: 60.16%] [G loss: 2.037070]\n",
      "epoch:20 step:19647 [D loss: 0.631468, acc: 63.28%] [G loss: 1.934643]\n",
      "epoch:20 step:19648 [D loss: 0.648427, acc: 64.84%] [G loss: 1.936126]\n",
      "epoch:20 step:19649 [D loss: 0.633773, acc: 62.50%] [G loss: 1.822469]\n",
      "epoch:20 step:19650 [D loss: 0.609783, acc: 67.97%] [G loss: 1.928400]\n",
      "epoch:20 step:19651 [D loss: 0.682694, acc: 57.03%] [G loss: 2.042214]\n",
      "epoch:20 step:19652 [D loss: 0.635990, acc: 61.72%] [G loss: 1.984032]\n",
      "epoch:20 step:19653 [D loss: 0.723674, acc: 53.91%] [G loss: 1.951809]\n",
      "epoch:20 step:19654 [D loss: 0.704710, acc: 54.69%] [G loss: 1.924503]\n",
      "epoch:20 step:19655 [D loss: 0.630659, acc: 63.28%] [G loss: 1.841738]\n",
      "epoch:20 step:19656 [D loss: 0.663378, acc: 57.03%] [G loss: 1.873966]\n",
      "epoch:20 step:19657 [D loss: 0.660408, acc: 60.16%] [G loss: 1.776912]\n",
      "epoch:20 step:19658 [D loss: 0.588896, acc: 71.88%] [G loss: 1.872814]\n",
      "epoch:20 step:19659 [D loss: 0.585007, acc: 73.44%] [G loss: 2.183305]\n",
      "epoch:20 step:19660 [D loss: 0.730812, acc: 54.69%] [G loss: 1.873024]\n",
      "epoch:20 step:19661 [D loss: 0.631139, acc: 61.72%] [G loss: 1.921159]\n",
      "epoch:20 step:19662 [D loss: 0.628900, acc: 65.62%] [G loss: 1.973665]\n",
      "epoch:20 step:19663 [D loss: 0.585830, acc: 68.75%] [G loss: 2.209350]\n",
      "epoch:20 step:19664 [D loss: 0.580628, acc: 73.44%] [G loss: 2.121079]\n",
      "epoch:20 step:19665 [D loss: 0.595146, acc: 74.22%] [G loss: 2.107487]\n",
      "epoch:20 step:19666 [D loss: 0.614515, acc: 64.84%] [G loss: 1.983411]\n",
      "epoch:20 step:19667 [D loss: 0.647707, acc: 64.06%] [G loss: 1.922176]\n",
      "epoch:20 step:19668 [D loss: 0.730523, acc: 56.25%] [G loss: 1.843398]\n",
      "epoch:20 step:19669 [D loss: 0.757296, acc: 50.00%] [G loss: 1.826419]\n",
      "epoch:20 step:19670 [D loss: 0.665703, acc: 60.16%] [G loss: 1.925991]\n",
      "epoch:20 step:19671 [D loss: 0.620719, acc: 67.97%] [G loss: 2.003250]\n",
      "epoch:20 step:19672 [D loss: 0.632917, acc: 63.28%] [G loss: 1.847875]\n",
      "epoch:20 step:19673 [D loss: 0.656443, acc: 61.72%] [G loss: 1.995369]\n",
      "epoch:20 step:19674 [D loss: 0.609016, acc: 66.41%] [G loss: 1.972413]\n",
      "epoch:20 step:19675 [D loss: 0.581991, acc: 71.88%] [G loss: 2.099194]\n",
      "epoch:20 step:19676 [D loss: 0.596858, acc: 71.88%] [G loss: 1.975084]\n",
      "epoch:20 step:19677 [D loss: 0.597816, acc: 76.56%] [G loss: 2.470268]\n",
      "epoch:21 step:19678 [D loss: 0.676660, acc: 57.81%] [G loss: 1.872623]\n",
      "epoch:21 step:19679 [D loss: 0.680563, acc: 51.56%] [G loss: 1.851595]\n",
      "epoch:21 step:19680 [D loss: 0.672906, acc: 56.25%] [G loss: 2.054114]\n",
      "epoch:21 step:19681 [D loss: 0.650103, acc: 63.28%] [G loss: 1.978674]\n",
      "epoch:21 step:19682 [D loss: 0.652038, acc: 58.59%] [G loss: 1.891042]\n",
      "epoch:21 step:19683 [D loss: 0.641070, acc: 63.28%] [G loss: 1.969349]\n",
      "epoch:21 step:19684 [D loss: 0.633778, acc: 66.41%] [G loss: 1.934798]\n",
      "epoch:21 step:19685 [D loss: 0.651014, acc: 63.28%] [G loss: 2.039883]\n",
      "epoch:21 step:19686 [D loss: 0.569312, acc: 73.44%] [G loss: 1.960043]\n",
      "epoch:21 step:19687 [D loss: 0.656334, acc: 59.38%] [G loss: 1.888441]\n",
      "epoch:21 step:19688 [D loss: 0.623037, acc: 64.84%] [G loss: 1.871325]\n",
      "epoch:21 step:19689 [D loss: 0.606732, acc: 64.06%] [G loss: 2.040812]\n",
      "epoch:21 step:19690 [D loss: 0.583847, acc: 67.97%] [G loss: 1.958140]\n",
      "epoch:21 step:19691 [D loss: 0.613882, acc: 66.41%] [G loss: 1.979594]\n",
      "epoch:21 step:19692 [D loss: 0.619962, acc: 63.28%] [G loss: 2.022568]\n",
      "epoch:21 step:19693 [D loss: 0.614266, acc: 72.66%] [G loss: 2.091381]\n",
      "epoch:21 step:19694 [D loss: 0.661908, acc: 60.94%] [G loss: 1.968930]\n",
      "epoch:21 step:19695 [D loss: 0.715105, acc: 53.91%] [G loss: 1.946326]\n",
      "epoch:21 step:19696 [D loss: 0.598854, acc: 71.88%] [G loss: 1.955486]\n",
      "epoch:21 step:19697 [D loss: 0.720324, acc: 54.69%] [G loss: 1.716475]\n",
      "epoch:21 step:19698 [D loss: 0.684579, acc: 53.91%] [G loss: 1.751297]\n",
      "epoch:21 step:19699 [D loss: 0.641191, acc: 63.28%] [G loss: 1.821592]\n",
      "epoch:21 step:19700 [D loss: 0.672036, acc: 60.16%] [G loss: 1.925622]\n",
      "epoch:21 step:19701 [D loss: 0.628511, acc: 64.84%] [G loss: 1.880478]\n",
      "epoch:21 step:19702 [D loss: 0.646096, acc: 60.16%] [G loss: 1.772146]\n",
      "epoch:21 step:19703 [D loss: 0.648817, acc: 70.31%] [G loss: 1.825755]\n",
      "epoch:21 step:19704 [D loss: 0.677531, acc: 61.72%] [G loss: 1.795019]\n",
      "epoch:21 step:19705 [D loss: 0.675799, acc: 56.25%] [G loss: 1.801182]\n",
      "epoch:21 step:19706 [D loss: 0.613274, acc: 65.62%] [G loss: 1.857278]\n",
      "epoch:21 step:19707 [D loss: 0.639516, acc: 60.16%] [G loss: 1.903111]\n",
      "epoch:21 step:19708 [D loss: 0.697858, acc: 53.12%] [G loss: 1.751117]\n",
      "epoch:21 step:19709 [D loss: 0.726212, acc: 52.34%] [G loss: 1.797326]\n",
      "epoch:21 step:19710 [D loss: 0.639810, acc: 61.72%] [G loss: 1.751168]\n",
      "epoch:21 step:19711 [D loss: 0.667425, acc: 59.38%] [G loss: 1.757261]\n",
      "epoch:21 step:19712 [D loss: 0.690908, acc: 59.38%] [G loss: 1.634811]\n",
      "epoch:21 step:19713 [D loss: 0.593629, acc: 67.97%] [G loss: 1.778532]\n",
      "epoch:21 step:19714 [D loss: 0.581836, acc: 69.53%] [G loss: 1.858053]\n",
      "epoch:21 step:19715 [D loss: 0.657863, acc: 56.25%] [G loss: 1.796303]\n",
      "epoch:21 step:19716 [D loss: 0.648566, acc: 67.19%] [G loss: 1.851135]\n",
      "epoch:21 step:19717 [D loss: 0.616545, acc: 62.50%] [G loss: 1.952260]\n",
      "epoch:21 step:19718 [D loss: 0.658958, acc: 58.59%] [G loss: 1.859723]\n",
      "epoch:21 step:19719 [D loss: 0.598448, acc: 67.19%] [G loss: 1.953636]\n",
      "epoch:21 step:19720 [D loss: 0.649381, acc: 64.84%] [G loss: 1.870047]\n",
      "epoch:21 step:19721 [D loss: 0.684769, acc: 60.16%] [G loss: 1.810274]\n",
      "epoch:21 step:19722 [D loss: 0.645480, acc: 61.72%] [G loss: 1.952544]\n",
      "epoch:21 step:19723 [D loss: 0.695225, acc: 57.03%] [G loss: 1.886564]\n",
      "epoch:21 step:19724 [D loss: 0.606939, acc: 68.75%] [G loss: 1.957432]\n",
      "epoch:21 step:19725 [D loss: 0.627277, acc: 71.09%] [G loss: 1.895007]\n",
      "epoch:21 step:19726 [D loss: 0.591355, acc: 64.84%] [G loss: 1.966113]\n",
      "epoch:21 step:19727 [D loss: 0.595268, acc: 67.19%] [G loss: 1.896324]\n",
      "epoch:21 step:19728 [D loss: 0.630543, acc: 63.28%] [G loss: 1.912776]\n",
      "epoch:21 step:19729 [D loss: 0.650577, acc: 64.06%] [G loss: 2.092360]\n",
      "epoch:21 step:19730 [D loss: 0.616103, acc: 60.16%] [G loss: 1.921686]\n",
      "epoch:21 step:19731 [D loss: 0.617818, acc: 68.75%] [G loss: 1.902501]\n",
      "epoch:21 step:19732 [D loss: 0.610903, acc: 69.53%] [G loss: 2.062112]\n",
      "epoch:21 step:19733 [D loss: 0.632762, acc: 62.50%] [G loss: 2.034782]\n",
      "epoch:21 step:19734 [D loss: 0.646079, acc: 62.50%] [G loss: 1.977680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19735 [D loss: 0.647511, acc: 59.38%] [G loss: 1.922455]\n",
      "epoch:21 step:19736 [D loss: 0.663374, acc: 64.06%] [G loss: 2.002339]\n",
      "epoch:21 step:19737 [D loss: 0.687521, acc: 58.59%] [G loss: 1.825224]\n",
      "epoch:21 step:19738 [D loss: 0.643409, acc: 62.50%] [G loss: 1.859779]\n",
      "epoch:21 step:19739 [D loss: 0.619933, acc: 69.53%] [G loss: 1.999896]\n",
      "epoch:21 step:19740 [D loss: 0.601970, acc: 69.53%] [G loss: 1.858902]\n",
      "epoch:21 step:19741 [D loss: 0.645814, acc: 62.50%] [G loss: 1.946808]\n",
      "epoch:21 step:19742 [D loss: 0.662649, acc: 61.72%] [G loss: 2.024944]\n",
      "epoch:21 step:19743 [D loss: 0.657948, acc: 59.38%] [G loss: 1.791073]\n",
      "epoch:21 step:19744 [D loss: 0.642012, acc: 65.62%] [G loss: 1.795200]\n",
      "epoch:21 step:19745 [D loss: 0.619546, acc: 62.50%] [G loss: 1.994715]\n",
      "epoch:21 step:19746 [D loss: 0.642296, acc: 60.94%] [G loss: 2.023170]\n",
      "epoch:21 step:19747 [D loss: 0.627940, acc: 63.28%] [G loss: 1.844565]\n",
      "epoch:21 step:19748 [D loss: 0.667767, acc: 62.50%] [G loss: 1.791659]\n",
      "epoch:21 step:19749 [D loss: 0.600673, acc: 62.50%] [G loss: 1.881482]\n",
      "epoch:21 step:19750 [D loss: 0.655535, acc: 66.41%] [G loss: 1.840957]\n",
      "epoch:21 step:19751 [D loss: 0.650111, acc: 60.16%] [G loss: 1.905619]\n",
      "epoch:21 step:19752 [D loss: 0.632129, acc: 64.06%] [G loss: 2.032008]\n",
      "epoch:21 step:19753 [D loss: 0.595342, acc: 66.41%] [G loss: 2.042893]\n",
      "epoch:21 step:19754 [D loss: 0.580290, acc: 70.31%] [G loss: 2.125017]\n",
      "epoch:21 step:19755 [D loss: 0.705755, acc: 54.69%] [G loss: 1.826168]\n",
      "epoch:21 step:19756 [D loss: 0.633101, acc: 62.50%] [G loss: 1.880837]\n",
      "epoch:21 step:19757 [D loss: 0.667069, acc: 60.16%] [G loss: 1.774539]\n",
      "epoch:21 step:19758 [D loss: 0.666709, acc: 56.25%] [G loss: 1.837773]\n",
      "epoch:21 step:19759 [D loss: 0.705726, acc: 58.59%] [G loss: 1.749416]\n",
      "epoch:21 step:19760 [D loss: 0.642663, acc: 66.41%] [G loss: 1.978938]\n",
      "epoch:21 step:19761 [D loss: 0.704900, acc: 55.47%] [G loss: 1.823509]\n",
      "epoch:21 step:19762 [D loss: 0.711669, acc: 55.47%] [G loss: 1.746318]\n",
      "epoch:21 step:19763 [D loss: 0.683818, acc: 56.25%] [G loss: 1.712582]\n",
      "epoch:21 step:19764 [D loss: 0.640321, acc: 62.50%] [G loss: 1.766609]\n",
      "epoch:21 step:19765 [D loss: 0.628588, acc: 65.62%] [G loss: 1.791683]\n",
      "epoch:21 step:19766 [D loss: 0.673999, acc: 59.38%] [G loss: 1.962983]\n",
      "epoch:21 step:19767 [D loss: 0.625293, acc: 65.62%] [G loss: 1.860339]\n",
      "epoch:21 step:19768 [D loss: 0.637099, acc: 63.28%] [G loss: 1.917364]\n",
      "epoch:21 step:19769 [D loss: 0.677345, acc: 60.16%] [G loss: 1.897248]\n",
      "epoch:21 step:19770 [D loss: 0.664389, acc: 57.03%] [G loss: 2.044703]\n",
      "epoch:21 step:19771 [D loss: 0.624459, acc: 67.19%] [G loss: 1.813990]\n",
      "epoch:21 step:19772 [D loss: 0.642620, acc: 60.94%] [G loss: 1.826526]\n",
      "epoch:21 step:19773 [D loss: 0.646235, acc: 62.50%] [G loss: 1.879153]\n",
      "epoch:21 step:19774 [D loss: 0.616283, acc: 66.41%] [G loss: 1.827785]\n",
      "epoch:21 step:19775 [D loss: 0.682377, acc: 57.03%] [G loss: 1.775997]\n",
      "epoch:21 step:19776 [D loss: 0.631377, acc: 65.62%] [G loss: 1.801467]\n",
      "epoch:21 step:19777 [D loss: 0.585041, acc: 70.31%] [G loss: 1.887545]\n",
      "epoch:21 step:19778 [D loss: 0.580750, acc: 73.44%] [G loss: 1.839242]\n",
      "epoch:21 step:19779 [D loss: 0.649513, acc: 63.28%] [G loss: 1.916334]\n",
      "epoch:21 step:19780 [D loss: 0.678578, acc: 57.03%] [G loss: 1.953412]\n",
      "epoch:21 step:19781 [D loss: 0.627633, acc: 62.50%] [G loss: 1.902688]\n",
      "epoch:21 step:19782 [D loss: 0.666004, acc: 61.72%] [G loss: 1.859377]\n",
      "epoch:21 step:19783 [D loss: 0.614912, acc: 65.62%] [G loss: 2.024992]\n",
      "epoch:21 step:19784 [D loss: 0.577221, acc: 72.66%] [G loss: 2.116336]\n",
      "epoch:21 step:19785 [D loss: 0.698737, acc: 59.38%] [G loss: 1.778622]\n",
      "epoch:21 step:19786 [D loss: 0.621991, acc: 63.28%] [G loss: 1.797586]\n",
      "epoch:21 step:19787 [D loss: 0.647997, acc: 64.84%] [G loss: 1.701935]\n",
      "epoch:21 step:19788 [D loss: 0.659125, acc: 60.16%] [G loss: 2.000696]\n",
      "epoch:21 step:19789 [D loss: 0.604847, acc: 67.19%] [G loss: 1.979580]\n",
      "epoch:21 step:19790 [D loss: 0.586483, acc: 71.09%] [G loss: 2.011533]\n",
      "epoch:21 step:19791 [D loss: 0.643772, acc: 64.06%] [G loss: 1.929002]\n",
      "epoch:21 step:19792 [D loss: 0.598974, acc: 64.84%] [G loss: 2.148383]\n",
      "epoch:21 step:19793 [D loss: 0.601212, acc: 66.41%] [G loss: 2.114651]\n",
      "epoch:21 step:19794 [D loss: 0.590980, acc: 70.31%] [G loss: 2.198184]\n",
      "epoch:21 step:19795 [D loss: 0.644577, acc: 65.62%] [G loss: 2.177125]\n",
      "epoch:21 step:19796 [D loss: 0.571858, acc: 74.22%] [G loss: 2.175230]\n",
      "epoch:21 step:19797 [D loss: 0.683730, acc: 61.72%] [G loss: 1.918432]\n",
      "epoch:21 step:19798 [D loss: 0.630079, acc: 66.41%] [G loss: 1.768644]\n",
      "epoch:21 step:19799 [D loss: 0.616482, acc: 69.53%] [G loss: 2.049800]\n",
      "epoch:21 step:19800 [D loss: 0.688049, acc: 58.59%] [G loss: 1.862499]\n",
      "##############\n",
      "[2.3503466  1.37412325 6.25055462 4.89277729 3.56476595 5.62052082\n",
      " 4.27735603 4.58568512 4.443826   3.53991593]\n",
      "##########\n",
      "epoch:21 step:19801 [D loss: 0.702585, acc: 55.47%] [G loss: 1.924663]\n",
      "epoch:21 step:19802 [D loss: 0.631647, acc: 64.06%] [G loss: 1.777079]\n",
      "epoch:21 step:19803 [D loss: 0.644984, acc: 62.50%] [G loss: 1.904449]\n",
      "epoch:21 step:19804 [D loss: 0.678874, acc: 57.03%] [G loss: 1.863954]\n",
      "epoch:21 step:19805 [D loss: 0.660197, acc: 58.59%] [G loss: 1.957513]\n",
      "epoch:21 step:19806 [D loss: 0.644436, acc: 64.06%] [G loss: 1.769293]\n",
      "epoch:21 step:19807 [D loss: 0.631720, acc: 62.50%] [G loss: 1.879604]\n",
      "epoch:21 step:19808 [D loss: 0.631648, acc: 65.62%] [G loss: 1.880909]\n",
      "epoch:21 step:19809 [D loss: 0.671860, acc: 59.38%] [G loss: 1.742228]\n",
      "epoch:21 step:19810 [D loss: 0.691719, acc: 55.47%] [G loss: 1.915212]\n",
      "epoch:21 step:19811 [D loss: 0.685831, acc: 55.47%] [G loss: 1.680441]\n",
      "epoch:21 step:19812 [D loss: 0.651826, acc: 59.38%] [G loss: 1.849604]\n",
      "epoch:21 step:19813 [D loss: 0.640469, acc: 58.59%] [G loss: 1.773233]\n",
      "epoch:21 step:19814 [D loss: 0.673338, acc: 60.94%] [G loss: 1.870053]\n",
      "epoch:21 step:19815 [D loss: 0.661786, acc: 53.12%] [G loss: 1.772213]\n",
      "epoch:21 step:19816 [D loss: 0.632669, acc: 60.94%] [G loss: 1.861068]\n",
      "epoch:21 step:19817 [D loss: 0.687734, acc: 51.56%] [G loss: 1.852798]\n",
      "epoch:21 step:19818 [D loss: 0.624765, acc: 63.28%] [G loss: 1.801209]\n",
      "epoch:21 step:19819 [D loss: 0.668116, acc: 60.94%] [G loss: 1.726071]\n",
      "epoch:21 step:19820 [D loss: 0.641434, acc: 64.06%] [G loss: 1.775311]\n",
      "epoch:21 step:19821 [D loss: 0.665900, acc: 58.59%] [G loss: 1.918211]\n",
      "epoch:21 step:19822 [D loss: 0.694500, acc: 52.34%] [G loss: 1.978152]\n",
      "epoch:21 step:19823 [D loss: 0.637889, acc: 61.72%] [G loss: 1.914029]\n",
      "epoch:21 step:19824 [D loss: 0.631245, acc: 65.62%] [G loss: 1.751203]\n",
      "epoch:21 step:19825 [D loss: 0.726614, acc: 52.34%] [G loss: 1.794713]\n",
      "epoch:21 step:19826 [D loss: 0.591649, acc: 75.00%] [G loss: 1.937091]\n",
      "epoch:21 step:19827 [D loss: 0.670185, acc: 58.59%] [G loss: 1.798942]\n",
      "epoch:21 step:19828 [D loss: 0.607005, acc: 66.41%] [G loss: 1.898067]\n",
      "epoch:21 step:19829 [D loss: 0.631425, acc: 65.62%] [G loss: 1.889562]\n",
      "epoch:21 step:19830 [D loss: 0.624310, acc: 66.41%] [G loss: 1.795968]\n",
      "epoch:21 step:19831 [D loss: 0.663833, acc: 63.28%] [G loss: 1.972492]\n",
      "epoch:21 step:19832 [D loss: 0.610627, acc: 67.97%] [G loss: 1.911801]\n",
      "epoch:21 step:19833 [D loss: 0.643735, acc: 64.84%] [G loss: 1.972429]\n",
      "epoch:21 step:19834 [D loss: 0.666883, acc: 59.38%] [G loss: 1.860236]\n",
      "epoch:21 step:19835 [D loss: 0.618628, acc: 67.97%] [G loss: 1.903437]\n",
      "epoch:21 step:19836 [D loss: 0.653097, acc: 61.72%] [G loss: 1.930954]\n",
      "epoch:21 step:19837 [D loss: 0.676774, acc: 57.81%] [G loss: 1.717046]\n",
      "epoch:21 step:19838 [D loss: 0.662970, acc: 59.38%] [G loss: 1.839675]\n",
      "epoch:21 step:19839 [D loss: 0.629695, acc: 65.62%] [G loss: 1.836276]\n",
      "epoch:21 step:19840 [D loss: 0.649688, acc: 62.50%] [G loss: 1.874613]\n",
      "epoch:21 step:19841 [D loss: 0.680870, acc: 51.56%] [G loss: 1.741265]\n",
      "epoch:21 step:19842 [D loss: 0.653315, acc: 61.72%] [G loss: 1.911462]\n",
      "epoch:21 step:19843 [D loss: 0.648200, acc: 62.50%] [G loss: 1.983170]\n",
      "epoch:21 step:19844 [D loss: 0.638923, acc: 60.16%] [G loss: 1.919506]\n",
      "epoch:21 step:19845 [D loss: 0.640055, acc: 61.72%] [G loss: 1.820903]\n",
      "epoch:21 step:19846 [D loss: 0.694776, acc: 57.03%] [G loss: 1.759054]\n",
      "epoch:21 step:19847 [D loss: 0.603734, acc: 65.62%] [G loss: 1.891728]\n",
      "epoch:21 step:19848 [D loss: 0.656322, acc: 67.19%] [G loss: 1.831250]\n",
      "epoch:21 step:19849 [D loss: 0.626454, acc: 64.06%] [G loss: 1.948972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19850 [D loss: 0.698909, acc: 53.12%] [G loss: 1.821050]\n",
      "epoch:21 step:19851 [D loss: 0.661282, acc: 53.91%] [G loss: 1.805870]\n",
      "epoch:21 step:19852 [D loss: 0.655057, acc: 61.72%] [G loss: 1.760813]\n",
      "epoch:21 step:19853 [D loss: 0.680876, acc: 55.47%] [G loss: 1.762443]\n",
      "epoch:21 step:19854 [D loss: 0.678850, acc: 54.69%] [G loss: 1.783623]\n",
      "epoch:21 step:19855 [D loss: 0.657585, acc: 60.16%] [G loss: 1.717525]\n",
      "epoch:21 step:19856 [D loss: 0.663808, acc: 57.81%] [G loss: 1.796720]\n",
      "epoch:21 step:19857 [D loss: 0.628692, acc: 67.97%] [G loss: 1.781611]\n",
      "epoch:21 step:19858 [D loss: 0.675041, acc: 56.25%] [G loss: 1.803350]\n",
      "epoch:21 step:19859 [D loss: 0.648980, acc: 66.41%] [G loss: 1.887513]\n",
      "epoch:21 step:19860 [D loss: 0.652188, acc: 61.72%] [G loss: 1.803458]\n",
      "epoch:21 step:19861 [D loss: 0.654892, acc: 61.72%] [G loss: 1.837448]\n",
      "epoch:21 step:19862 [D loss: 0.686117, acc: 57.03%] [G loss: 1.835889]\n",
      "epoch:21 step:19863 [D loss: 0.611637, acc: 65.62%] [G loss: 1.804676]\n",
      "epoch:21 step:19864 [D loss: 0.667548, acc: 60.16%] [G loss: 1.777883]\n",
      "epoch:21 step:19865 [D loss: 0.630114, acc: 64.84%] [G loss: 1.917492]\n",
      "epoch:21 step:19866 [D loss: 0.674486, acc: 56.25%] [G loss: 1.787522]\n",
      "epoch:21 step:19867 [D loss: 0.667884, acc: 58.59%] [G loss: 1.951527]\n",
      "epoch:21 step:19868 [D loss: 0.669228, acc: 62.50%] [G loss: 1.832438]\n",
      "epoch:21 step:19869 [D loss: 0.566089, acc: 72.66%] [G loss: 2.013176]\n",
      "epoch:21 step:19870 [D loss: 0.638012, acc: 61.72%] [G loss: 1.825854]\n",
      "epoch:21 step:19871 [D loss: 0.618346, acc: 68.75%] [G loss: 2.042793]\n",
      "epoch:21 step:19872 [D loss: 0.610787, acc: 66.41%] [G loss: 1.890824]\n",
      "epoch:21 step:19873 [D loss: 0.682246, acc: 53.12%] [G loss: 1.826868]\n",
      "epoch:21 step:19874 [D loss: 0.658129, acc: 58.59%] [G loss: 1.876359]\n",
      "epoch:21 step:19875 [D loss: 0.590020, acc: 71.88%] [G loss: 1.985816]\n",
      "epoch:21 step:19876 [D loss: 0.641682, acc: 62.50%] [G loss: 1.817898]\n",
      "epoch:21 step:19877 [D loss: 0.687921, acc: 60.94%] [G loss: 1.766736]\n",
      "epoch:21 step:19878 [D loss: 0.614940, acc: 66.41%] [G loss: 2.035647]\n",
      "epoch:21 step:19879 [D loss: 0.661318, acc: 64.06%] [G loss: 1.853522]\n",
      "epoch:21 step:19880 [D loss: 0.665813, acc: 62.50%] [G loss: 1.821826]\n",
      "epoch:21 step:19881 [D loss: 0.702471, acc: 53.91%] [G loss: 1.850831]\n",
      "epoch:21 step:19882 [D loss: 0.608812, acc: 61.72%] [G loss: 1.798476]\n",
      "epoch:21 step:19883 [D loss: 0.637666, acc: 61.72%] [G loss: 1.936323]\n",
      "epoch:21 step:19884 [D loss: 0.585092, acc: 69.53%] [G loss: 2.033322]\n",
      "epoch:21 step:19885 [D loss: 0.602314, acc: 64.84%] [G loss: 2.072721]\n",
      "epoch:21 step:19886 [D loss: 0.571622, acc: 70.31%] [G loss: 2.035368]\n",
      "epoch:21 step:19887 [D loss: 0.706368, acc: 59.38%] [G loss: 1.896781]\n",
      "epoch:21 step:19888 [D loss: 0.673653, acc: 56.25%] [G loss: 1.818154]\n",
      "epoch:21 step:19889 [D loss: 0.650976, acc: 60.16%] [G loss: 1.780157]\n",
      "epoch:21 step:19890 [D loss: 0.708939, acc: 55.47%] [G loss: 1.707674]\n",
      "epoch:21 step:19891 [D loss: 0.691003, acc: 58.59%] [G loss: 1.836691]\n",
      "epoch:21 step:19892 [D loss: 0.678374, acc: 60.16%] [G loss: 1.804352]\n",
      "epoch:21 step:19893 [D loss: 0.596064, acc: 70.31%] [G loss: 1.969223]\n",
      "epoch:21 step:19894 [D loss: 0.601585, acc: 69.53%] [G loss: 1.813349]\n",
      "epoch:21 step:19895 [D loss: 0.605274, acc: 67.97%] [G loss: 1.955582]\n",
      "epoch:21 step:19896 [D loss: 0.636829, acc: 62.50%] [G loss: 2.109834]\n",
      "epoch:21 step:19897 [D loss: 0.662315, acc: 60.16%] [G loss: 1.836051]\n",
      "epoch:21 step:19898 [D loss: 0.614335, acc: 67.19%] [G loss: 1.964213]\n",
      "epoch:21 step:19899 [D loss: 0.644057, acc: 62.50%] [G loss: 1.943576]\n",
      "epoch:21 step:19900 [D loss: 0.657880, acc: 57.03%] [G loss: 1.830447]\n",
      "epoch:21 step:19901 [D loss: 0.635865, acc: 60.94%] [G loss: 1.875881]\n",
      "epoch:21 step:19902 [D loss: 0.688894, acc: 60.94%] [G loss: 1.848238]\n",
      "epoch:21 step:19903 [D loss: 0.657991, acc: 59.38%] [G loss: 1.923962]\n",
      "epoch:21 step:19904 [D loss: 0.632002, acc: 64.06%] [G loss: 1.860833]\n",
      "epoch:21 step:19905 [D loss: 0.674161, acc: 58.59%] [G loss: 1.775768]\n",
      "epoch:21 step:19906 [D loss: 0.621927, acc: 65.62%] [G loss: 2.013425]\n",
      "epoch:21 step:19907 [D loss: 0.592249, acc: 66.41%] [G loss: 2.119570]\n",
      "epoch:21 step:19908 [D loss: 0.582482, acc: 73.44%] [G loss: 2.292387]\n",
      "epoch:21 step:19909 [D loss: 0.548534, acc: 75.78%] [G loss: 2.324989]\n",
      "epoch:21 step:19910 [D loss: 0.685941, acc: 57.03%] [G loss: 2.041279]\n",
      "epoch:21 step:19911 [D loss: 0.648954, acc: 59.38%] [G loss: 1.889266]\n",
      "epoch:21 step:19912 [D loss: 0.626748, acc: 61.72%] [G loss: 1.845304]\n",
      "epoch:21 step:19913 [D loss: 0.640520, acc: 64.84%] [G loss: 1.936764]\n",
      "epoch:21 step:19914 [D loss: 0.681630, acc: 61.72%] [G loss: 1.985493]\n",
      "epoch:21 step:19915 [D loss: 0.607848, acc: 62.50%] [G loss: 1.943133]\n",
      "epoch:21 step:19916 [D loss: 0.662126, acc: 57.81%] [G loss: 1.929722]\n",
      "epoch:21 step:19917 [D loss: 0.605874, acc: 67.19%] [G loss: 2.020357]\n",
      "epoch:21 step:19918 [D loss: 0.638146, acc: 65.62%] [G loss: 1.925113]\n",
      "epoch:21 step:19919 [D loss: 0.618779, acc: 67.97%] [G loss: 2.001120]\n",
      "epoch:21 step:19920 [D loss: 0.651864, acc: 59.38%] [G loss: 1.857421]\n",
      "epoch:21 step:19921 [D loss: 0.644112, acc: 64.84%] [G loss: 1.895738]\n",
      "epoch:21 step:19922 [D loss: 0.587735, acc: 75.78%] [G loss: 1.874754]\n",
      "epoch:21 step:19923 [D loss: 0.675078, acc: 59.38%] [G loss: 1.901295]\n",
      "epoch:21 step:19924 [D loss: 0.659953, acc: 58.59%] [G loss: 1.990756]\n",
      "epoch:21 step:19925 [D loss: 0.648994, acc: 60.16%] [G loss: 1.941604]\n",
      "epoch:21 step:19926 [D loss: 0.703901, acc: 52.34%] [G loss: 1.784694]\n",
      "epoch:21 step:19927 [D loss: 0.723780, acc: 51.56%] [G loss: 1.727916]\n",
      "epoch:21 step:19928 [D loss: 0.704341, acc: 46.88%] [G loss: 1.688396]\n",
      "epoch:21 step:19929 [D loss: 0.677322, acc: 57.81%] [G loss: 1.809893]\n",
      "epoch:21 step:19930 [D loss: 0.683962, acc: 61.72%] [G loss: 1.862530]\n",
      "epoch:21 step:19931 [D loss: 0.688782, acc: 55.47%] [G loss: 1.763915]\n",
      "epoch:21 step:19932 [D loss: 0.631531, acc: 64.84%] [G loss: 1.787472]\n",
      "epoch:21 step:19933 [D loss: 0.615735, acc: 67.19%] [G loss: 1.932602]\n",
      "epoch:21 step:19934 [D loss: 0.659147, acc: 62.50%] [G loss: 1.819269]\n",
      "epoch:21 step:19935 [D loss: 0.674161, acc: 60.16%] [G loss: 1.820841]\n",
      "epoch:21 step:19936 [D loss: 0.687997, acc: 57.03%] [G loss: 1.825609]\n",
      "epoch:21 step:19937 [D loss: 0.659220, acc: 59.38%] [G loss: 1.756647]\n",
      "epoch:21 step:19938 [D loss: 0.667391, acc: 58.59%] [G loss: 1.919140]\n",
      "epoch:21 step:19939 [D loss: 0.639210, acc: 65.62%] [G loss: 1.884352]\n",
      "epoch:21 step:19940 [D loss: 0.643204, acc: 58.59%] [G loss: 1.839852]\n",
      "epoch:21 step:19941 [D loss: 0.618929, acc: 63.28%] [G loss: 2.029694]\n",
      "epoch:21 step:19942 [D loss: 0.632764, acc: 64.84%] [G loss: 1.827105]\n",
      "epoch:21 step:19943 [D loss: 0.634998, acc: 60.94%] [G loss: 1.800068]\n",
      "epoch:21 step:19944 [D loss: 0.650879, acc: 60.16%] [G loss: 1.885333]\n",
      "epoch:21 step:19945 [D loss: 0.671323, acc: 58.59%] [G loss: 1.839094]\n",
      "epoch:21 step:19946 [D loss: 0.588527, acc: 69.53%] [G loss: 2.115297]\n",
      "epoch:21 step:19947 [D loss: 0.659845, acc: 59.38%] [G loss: 1.989784]\n",
      "epoch:21 step:19948 [D loss: 0.626905, acc: 59.38%] [G loss: 1.955480]\n",
      "epoch:21 step:19949 [D loss: 0.638031, acc: 64.84%] [G loss: 1.946458]\n",
      "epoch:21 step:19950 [D loss: 0.643538, acc: 60.16%] [G loss: 1.837477]\n",
      "epoch:21 step:19951 [D loss: 0.586428, acc: 67.97%] [G loss: 2.001302]\n",
      "epoch:21 step:19952 [D loss: 0.612493, acc: 62.50%] [G loss: 1.972566]\n",
      "epoch:21 step:19953 [D loss: 0.565580, acc: 70.31%] [G loss: 2.201436]\n",
      "epoch:21 step:19954 [D loss: 0.658991, acc: 55.47%] [G loss: 1.773989]\n",
      "epoch:21 step:19955 [D loss: 0.688907, acc: 57.81%] [G loss: 1.932916]\n",
      "epoch:21 step:19956 [D loss: 0.677949, acc: 60.16%] [G loss: 1.861733]\n",
      "epoch:21 step:19957 [D loss: 0.609046, acc: 66.41%] [G loss: 1.896369]\n",
      "epoch:21 step:19958 [D loss: 0.684846, acc: 58.59%] [G loss: 1.852800]\n",
      "epoch:21 step:19959 [D loss: 0.647566, acc: 64.06%] [G loss: 1.868305]\n",
      "epoch:21 step:19960 [D loss: 0.646792, acc: 56.25%] [G loss: 1.899471]\n",
      "epoch:21 step:19961 [D loss: 0.678154, acc: 59.38%] [G loss: 1.714795]\n",
      "epoch:21 step:19962 [D loss: 0.663259, acc: 57.03%] [G loss: 1.898701]\n",
      "epoch:21 step:19963 [D loss: 0.623477, acc: 68.75%] [G loss: 1.963392]\n",
      "epoch:21 step:19964 [D loss: 0.669612, acc: 57.03%] [G loss: 1.791310]\n",
      "epoch:21 step:19965 [D loss: 0.666287, acc: 58.59%] [G loss: 1.835847]\n",
      "epoch:21 step:19966 [D loss: 0.663634, acc: 60.94%] [G loss: 1.962619]\n",
      "epoch:21 step:19967 [D loss: 0.655487, acc: 60.16%] [G loss: 1.795881]\n",
      "epoch:21 step:19968 [D loss: 0.672376, acc: 59.38%] [G loss: 1.777722]\n",
      "epoch:21 step:19969 [D loss: 0.663307, acc: 60.16%] [G loss: 1.800625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19970 [D loss: 0.644687, acc: 60.94%] [G loss: 1.975971]\n",
      "epoch:21 step:19971 [D loss: 0.629759, acc: 62.50%] [G loss: 1.901580]\n",
      "epoch:21 step:19972 [D loss: 0.631966, acc: 64.06%] [G loss: 1.920410]\n",
      "epoch:21 step:19973 [D loss: 0.620409, acc: 62.50%] [G loss: 1.912002]\n",
      "epoch:21 step:19974 [D loss: 0.615319, acc: 67.97%] [G loss: 1.833086]\n",
      "epoch:21 step:19975 [D loss: 0.612152, acc: 67.97%] [G loss: 2.067842]\n",
      "epoch:21 step:19976 [D loss: 0.651431, acc: 62.50%] [G loss: 2.242184]\n",
      "epoch:21 step:19977 [D loss: 0.652617, acc: 57.81%] [G loss: 1.926694]\n",
      "epoch:21 step:19978 [D loss: 0.701800, acc: 54.69%] [G loss: 1.773734]\n",
      "epoch:21 step:19979 [D loss: 0.655968, acc: 57.81%] [G loss: 1.784865]\n",
      "epoch:21 step:19980 [D loss: 0.709926, acc: 57.03%] [G loss: 1.769421]\n",
      "epoch:21 step:19981 [D loss: 0.620322, acc: 70.31%] [G loss: 1.944088]\n",
      "epoch:21 step:19982 [D loss: 0.680769, acc: 56.25%] [G loss: 1.867309]\n",
      "epoch:21 step:19983 [D loss: 0.614321, acc: 64.84%] [G loss: 1.887958]\n",
      "epoch:21 step:19984 [D loss: 0.703538, acc: 54.69%] [G loss: 1.788079]\n",
      "epoch:21 step:19985 [D loss: 0.733870, acc: 48.44%] [G loss: 1.867889]\n",
      "epoch:21 step:19986 [D loss: 0.617092, acc: 64.06%] [G loss: 1.831737]\n",
      "epoch:21 step:19987 [D loss: 0.659977, acc: 60.94%] [G loss: 1.823781]\n",
      "epoch:21 step:19988 [D loss: 0.604070, acc: 68.75%] [G loss: 1.753702]\n",
      "epoch:21 step:19989 [D loss: 0.583300, acc: 71.09%] [G loss: 2.208885]\n",
      "epoch:21 step:19990 [D loss: 0.604467, acc: 65.62%] [G loss: 2.149802]\n",
      "epoch:21 step:19991 [D loss: 0.558020, acc: 72.66%] [G loss: 2.231224]\n",
      "epoch:21 step:19992 [D loss: 0.590607, acc: 67.19%] [G loss: 2.164868]\n",
      "epoch:21 step:19993 [D loss: 0.708178, acc: 53.91%] [G loss: 1.868388]\n",
      "epoch:21 step:19994 [D loss: 0.668556, acc: 59.38%] [G loss: 1.854866]\n",
      "epoch:21 step:19995 [D loss: 0.637789, acc: 64.84%] [G loss: 1.911342]\n",
      "epoch:21 step:19996 [D loss: 0.629545, acc: 60.94%] [G loss: 1.851325]\n",
      "epoch:21 step:19997 [D loss: 0.645790, acc: 60.94%] [G loss: 2.024023]\n",
      "epoch:21 step:19998 [D loss: 0.678649, acc: 56.25%] [G loss: 2.026051]\n",
      "epoch:21 step:19999 [D loss: 0.673512, acc: 64.84%] [G loss: 1.815181]\n",
      "epoch:21 step:20000 [D loss: 0.657021, acc: 58.59%] [G loss: 1.733291]\n",
      "##############\n",
      "[2.42551398 1.32347099 6.28049551 4.75807496 3.49223415 5.38067122\n",
      " 4.327053   4.6935864  4.45502165 3.62039871]\n",
      "##########\n",
      "epoch:21 step:20001 [D loss: 0.664434, acc: 57.81%] [G loss: 1.824332]\n",
      "epoch:21 step:20002 [D loss: 0.654959, acc: 58.59%] [G loss: 1.786167]\n",
      "epoch:21 step:20003 [D loss: 0.657550, acc: 62.50%] [G loss: 1.897202]\n",
      "epoch:21 step:20004 [D loss: 0.677606, acc: 63.28%] [G loss: 1.837668]\n",
      "epoch:21 step:20005 [D loss: 0.608740, acc: 62.50%] [G loss: 1.888715]\n",
      "epoch:21 step:20006 [D loss: 0.623596, acc: 66.41%] [G loss: 1.981561]\n",
      "epoch:21 step:20007 [D loss: 0.647742, acc: 64.06%] [G loss: 1.865744]\n",
      "epoch:21 step:20008 [D loss: 0.631388, acc: 60.94%] [G loss: 2.005471]\n",
      "epoch:21 step:20009 [D loss: 0.621970, acc: 65.62%] [G loss: 2.036709]\n",
      "epoch:21 step:20010 [D loss: 0.655263, acc: 67.97%] [G loss: 1.999318]\n",
      "epoch:21 step:20011 [D loss: 0.691533, acc: 60.16%] [G loss: 1.978223]\n",
      "epoch:21 step:20012 [D loss: 0.630566, acc: 69.53%] [G loss: 2.027551]\n",
      "epoch:21 step:20013 [D loss: 0.653159, acc: 61.72%] [G loss: 1.906490]\n",
      "epoch:21 step:20014 [D loss: 0.564768, acc: 71.09%] [G loss: 2.096744]\n",
      "epoch:21 step:20015 [D loss: 0.646336, acc: 59.38%] [G loss: 2.035195]\n",
      "epoch:21 step:20016 [D loss: 0.623500, acc: 67.19%] [G loss: 1.857831]\n",
      "epoch:21 step:20017 [D loss: 0.595349, acc: 66.41%] [G loss: 1.960920]\n",
      "epoch:21 step:20018 [D loss: 0.704330, acc: 53.91%] [G loss: 1.851943]\n",
      "epoch:21 step:20019 [D loss: 0.698362, acc: 50.78%] [G loss: 1.630417]\n",
      "epoch:21 step:20020 [D loss: 0.696882, acc: 55.47%] [G loss: 1.872870]\n",
      "epoch:21 step:20021 [D loss: 0.692409, acc: 54.69%] [G loss: 1.809304]\n",
      "epoch:21 step:20022 [D loss: 0.669238, acc: 61.72%] [G loss: 2.057029]\n",
      "epoch:21 step:20023 [D loss: 0.634810, acc: 65.62%] [G loss: 2.011664]\n",
      "epoch:21 step:20024 [D loss: 0.546035, acc: 74.22%] [G loss: 2.115318]\n",
      "epoch:21 step:20025 [D loss: 0.698343, acc: 57.81%] [G loss: 1.787041]\n",
      "epoch:21 step:20026 [D loss: 0.699515, acc: 57.03%] [G loss: 1.813513]\n",
      "epoch:21 step:20027 [D loss: 0.669018, acc: 59.38%] [G loss: 1.843052]\n",
      "epoch:21 step:20028 [D loss: 0.680207, acc: 61.72%] [G loss: 1.932962]\n",
      "epoch:21 step:20029 [D loss: 0.647932, acc: 67.19%] [G loss: 1.828719]\n",
      "epoch:21 step:20030 [D loss: 0.658168, acc: 61.72%] [G loss: 1.943455]\n",
      "epoch:21 step:20031 [D loss: 0.614441, acc: 67.97%] [G loss: 2.170753]\n",
      "epoch:21 step:20032 [D loss: 0.702712, acc: 53.91%] [G loss: 1.698189]\n",
      "epoch:21 step:20033 [D loss: 0.697824, acc: 53.12%] [G loss: 1.752635]\n",
      "epoch:21 step:20034 [D loss: 0.641122, acc: 62.50%] [G loss: 1.917731]\n",
      "epoch:21 step:20035 [D loss: 0.660840, acc: 64.06%] [G loss: 1.851575]\n",
      "epoch:21 step:20036 [D loss: 0.633136, acc: 64.06%] [G loss: 1.997021]\n",
      "epoch:21 step:20037 [D loss: 0.639966, acc: 61.72%] [G loss: 1.985822]\n",
      "epoch:21 step:20038 [D loss: 0.623146, acc: 62.50%] [G loss: 1.840757]\n",
      "epoch:21 step:20039 [D loss: 0.636503, acc: 62.50%] [G loss: 1.848902]\n",
      "epoch:21 step:20040 [D loss: 0.640220, acc: 59.38%] [G loss: 1.850188]\n",
      "epoch:21 step:20041 [D loss: 0.656142, acc: 65.62%] [G loss: 1.982693]\n",
      "epoch:21 step:20042 [D loss: 0.639623, acc: 65.62%] [G loss: 1.801414]\n",
      "epoch:21 step:20043 [D loss: 0.611069, acc: 67.19%] [G loss: 1.872609]\n",
      "epoch:21 step:20044 [D loss: 0.664076, acc: 64.06%] [G loss: 1.964079]\n",
      "epoch:21 step:20045 [D loss: 0.618955, acc: 68.75%] [G loss: 1.948097]\n",
      "epoch:21 step:20046 [D loss: 0.627580, acc: 64.84%] [G loss: 1.869003]\n",
      "epoch:21 step:20047 [D loss: 0.624923, acc: 67.19%] [G loss: 1.991597]\n",
      "epoch:21 step:20048 [D loss: 0.620211, acc: 66.41%] [G loss: 1.955896]\n",
      "epoch:21 step:20049 [D loss: 0.671664, acc: 63.28%] [G loss: 1.954339]\n",
      "epoch:21 step:20050 [D loss: 0.738075, acc: 52.34%] [G loss: 1.831631]\n",
      "epoch:21 step:20051 [D loss: 0.593293, acc: 71.09%] [G loss: 1.964823]\n",
      "epoch:21 step:20052 [D loss: 0.687076, acc: 59.38%] [G loss: 1.840391]\n",
      "epoch:21 step:20053 [D loss: 0.682128, acc: 53.91%] [G loss: 1.989288]\n",
      "epoch:21 step:20054 [D loss: 0.652651, acc: 59.38%] [G loss: 1.798400]\n",
      "epoch:21 step:20055 [D loss: 0.656577, acc: 60.16%] [G loss: 1.842904]\n",
      "epoch:21 step:20056 [D loss: 0.658173, acc: 56.25%] [G loss: 1.898724]\n",
      "epoch:21 step:20057 [D loss: 0.637340, acc: 64.84%] [G loss: 1.893460]\n",
      "epoch:21 step:20058 [D loss: 0.635827, acc: 63.28%] [G loss: 1.957068]\n",
      "epoch:21 step:20059 [D loss: 0.639007, acc: 61.72%] [G loss: 1.749905]\n",
      "epoch:21 step:20060 [D loss: 0.626362, acc: 60.16%] [G loss: 1.824477]\n",
      "epoch:21 step:20061 [D loss: 0.637834, acc: 67.19%] [G loss: 1.852997]\n",
      "epoch:21 step:20062 [D loss: 0.633330, acc: 60.94%] [G loss: 1.948905]\n",
      "epoch:21 step:20063 [D loss: 0.661130, acc: 59.38%] [G loss: 1.762331]\n",
      "epoch:21 step:20064 [D loss: 0.688766, acc: 51.56%] [G loss: 1.804339]\n",
      "epoch:21 step:20065 [D loss: 0.655818, acc: 63.28%] [G loss: 1.750647]\n",
      "epoch:21 step:20066 [D loss: 0.645459, acc: 57.81%] [G loss: 1.772734]\n",
      "epoch:21 step:20067 [D loss: 0.644311, acc: 58.59%] [G loss: 1.904706]\n",
      "epoch:21 step:20068 [D loss: 0.675284, acc: 60.16%] [G loss: 1.857796]\n",
      "epoch:21 step:20069 [D loss: 0.621833, acc: 66.41%] [G loss: 1.813459]\n",
      "epoch:21 step:20070 [D loss: 0.652561, acc: 64.06%] [G loss: 1.865286]\n",
      "epoch:21 step:20071 [D loss: 0.693590, acc: 56.25%] [G loss: 1.776962]\n",
      "epoch:21 step:20072 [D loss: 0.598728, acc: 70.31%] [G loss: 1.985792]\n",
      "epoch:21 step:20073 [D loss: 0.657905, acc: 64.06%] [G loss: 1.905752]\n",
      "epoch:21 step:20074 [D loss: 0.674356, acc: 57.03%] [G loss: 1.719911]\n",
      "epoch:21 step:20075 [D loss: 0.626896, acc: 61.72%] [G loss: 1.934660]\n",
      "epoch:21 step:20076 [D loss: 0.666817, acc: 63.28%] [G loss: 1.751567]\n",
      "epoch:21 step:20077 [D loss: 0.598074, acc: 67.19%] [G loss: 1.872571]\n",
      "epoch:21 step:20078 [D loss: 0.656118, acc: 60.94%] [G loss: 1.866860]\n",
      "epoch:21 step:20079 [D loss: 0.647222, acc: 61.72%] [G loss: 1.827065]\n",
      "epoch:21 step:20080 [D loss: 0.612890, acc: 65.62%] [G loss: 1.881707]\n",
      "epoch:21 step:20081 [D loss: 0.666962, acc: 64.84%] [G loss: 2.032948]\n",
      "epoch:21 step:20082 [D loss: 0.638019, acc: 64.84%] [G loss: 2.125768]\n",
      "epoch:21 step:20083 [D loss: 0.641831, acc: 59.38%] [G loss: 1.983986]\n",
      "epoch:21 step:20084 [D loss: 0.648856, acc: 63.28%] [G loss: 1.942784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20085 [D loss: 0.713630, acc: 50.78%] [G loss: 1.809743]\n",
      "epoch:21 step:20086 [D loss: 0.638118, acc: 64.84%] [G loss: 1.787391]\n",
      "epoch:21 step:20087 [D loss: 0.643155, acc: 62.50%] [G loss: 1.818870]\n",
      "epoch:21 step:20088 [D loss: 0.685133, acc: 52.34%] [G loss: 1.840050]\n",
      "epoch:21 step:20089 [D loss: 0.637275, acc: 61.72%] [G loss: 1.857105]\n",
      "epoch:21 step:20090 [D loss: 0.682097, acc: 61.72%] [G loss: 1.770622]\n",
      "epoch:21 step:20091 [D loss: 0.641215, acc: 64.06%] [G loss: 1.898969]\n",
      "epoch:21 step:20092 [D loss: 0.612547, acc: 65.62%] [G loss: 1.971386]\n",
      "epoch:21 step:20093 [D loss: 0.618191, acc: 63.28%] [G loss: 2.004949]\n",
      "epoch:21 step:20094 [D loss: 0.655131, acc: 57.81%] [G loss: 1.924984]\n",
      "epoch:21 step:20095 [D loss: 0.689634, acc: 53.91%] [G loss: 1.858778]\n",
      "epoch:21 step:20096 [D loss: 0.589471, acc: 73.44%] [G loss: 1.890022]\n",
      "epoch:21 step:20097 [D loss: 0.660929, acc: 56.25%] [G loss: 1.926774]\n",
      "epoch:21 step:20098 [D loss: 0.677327, acc: 54.69%] [G loss: 1.971448]\n",
      "epoch:21 step:20099 [D loss: 0.701542, acc: 57.03%] [G loss: 1.915097]\n",
      "epoch:21 step:20100 [D loss: 0.627987, acc: 64.84%] [G loss: 1.815518]\n",
      "epoch:21 step:20101 [D loss: 0.685245, acc: 55.47%] [G loss: 1.839036]\n",
      "epoch:21 step:20102 [D loss: 0.657634, acc: 57.81%] [G loss: 1.916348]\n",
      "epoch:21 step:20103 [D loss: 0.670076, acc: 64.84%] [G loss: 1.925806]\n",
      "epoch:21 step:20104 [D loss: 0.687496, acc: 57.81%] [G loss: 1.926618]\n",
      "epoch:21 step:20105 [D loss: 0.585789, acc: 73.44%] [G loss: 2.135438]\n",
      "epoch:21 step:20106 [D loss: 0.577206, acc: 73.44%] [G loss: 2.121091]\n",
      "epoch:21 step:20107 [D loss: 0.601588, acc: 68.75%] [G loss: 2.174974]\n",
      "epoch:21 step:20108 [D loss: 0.662901, acc: 61.72%] [G loss: 1.988375]\n",
      "epoch:21 step:20109 [D loss: 0.675707, acc: 58.59%] [G loss: 1.765841]\n",
      "epoch:21 step:20110 [D loss: 0.667704, acc: 61.72%] [G loss: 1.921074]\n",
      "epoch:21 step:20111 [D loss: 0.618720, acc: 66.41%] [G loss: 1.989208]\n",
      "epoch:21 step:20112 [D loss: 0.635707, acc: 57.81%] [G loss: 1.991889]\n",
      "epoch:21 step:20113 [D loss: 0.598713, acc: 71.88%] [G loss: 1.990651]\n",
      "epoch:21 step:20114 [D loss: 0.662565, acc: 60.16%] [G loss: 1.748879]\n",
      "epoch:21 step:20115 [D loss: 0.632971, acc: 64.06%] [G loss: 1.787493]\n",
      "epoch:21 step:20116 [D loss: 0.643261, acc: 62.50%] [G loss: 1.771163]\n",
      "epoch:21 step:20117 [D loss: 0.656688, acc: 58.59%] [G loss: 1.871759]\n",
      "epoch:21 step:20118 [D loss: 0.676297, acc: 57.03%] [G loss: 1.871071]\n",
      "epoch:21 step:20119 [D loss: 0.666363, acc: 57.81%] [G loss: 1.701828]\n",
      "epoch:21 step:20120 [D loss: 0.641338, acc: 69.53%] [G loss: 1.961091]\n",
      "epoch:21 step:20121 [D loss: 0.661727, acc: 64.84%] [G loss: 1.748137]\n",
      "epoch:21 step:20122 [D loss: 0.661586, acc: 60.16%] [G loss: 1.975745]\n",
      "epoch:21 step:20123 [D loss: 0.688175, acc: 54.69%] [G loss: 1.898837]\n",
      "epoch:21 step:20124 [D loss: 0.618807, acc: 72.66%] [G loss: 1.928767]\n",
      "epoch:21 step:20125 [D loss: 0.631846, acc: 60.94%] [G loss: 1.841153]\n",
      "epoch:21 step:20126 [D loss: 0.693899, acc: 60.16%] [G loss: 1.867658]\n",
      "epoch:21 step:20127 [D loss: 0.719865, acc: 53.12%] [G loss: 1.743741]\n",
      "epoch:21 step:20128 [D loss: 0.632662, acc: 65.62%] [G loss: 1.851545]\n",
      "epoch:21 step:20129 [D loss: 0.621308, acc: 67.19%] [G loss: 1.874400]\n",
      "epoch:21 step:20130 [D loss: 0.672233, acc: 60.16%] [G loss: 1.858788]\n",
      "epoch:21 step:20131 [D loss: 0.655274, acc: 60.16%] [G loss: 1.843668]\n",
      "epoch:21 step:20132 [D loss: 0.665906, acc: 57.03%] [G loss: 1.820800]\n",
      "epoch:21 step:20133 [D loss: 0.673859, acc: 59.38%] [G loss: 1.932838]\n",
      "epoch:21 step:20134 [D loss: 0.644537, acc: 62.50%] [G loss: 1.852690]\n",
      "epoch:21 step:20135 [D loss: 0.691692, acc: 56.25%] [G loss: 1.781917]\n",
      "epoch:21 step:20136 [D loss: 0.653128, acc: 62.50%] [G loss: 1.755937]\n",
      "epoch:21 step:20137 [D loss: 0.646835, acc: 64.06%] [G loss: 1.810045]\n",
      "epoch:21 step:20138 [D loss: 0.638743, acc: 63.28%] [G loss: 1.947805]\n",
      "epoch:21 step:20139 [D loss: 0.646206, acc: 60.94%] [G loss: 1.914764]\n",
      "epoch:21 step:20140 [D loss: 0.583185, acc: 68.75%] [G loss: 2.194207]\n",
      "epoch:21 step:20141 [D loss: 0.643792, acc: 63.28%] [G loss: 1.831328]\n",
      "epoch:21 step:20142 [D loss: 0.672526, acc: 60.94%] [G loss: 2.012360]\n",
      "epoch:21 step:20143 [D loss: 0.639318, acc: 64.84%] [G loss: 1.890477]\n",
      "epoch:21 step:20144 [D loss: 0.602754, acc: 67.97%] [G loss: 1.973442]\n",
      "epoch:21 step:20145 [D loss: 0.660267, acc: 64.06%] [G loss: 1.969007]\n",
      "epoch:21 step:20146 [D loss: 0.594135, acc: 67.97%] [G loss: 2.124202]\n",
      "epoch:21 step:20147 [D loss: 0.676265, acc: 62.50%] [G loss: 1.942074]\n",
      "epoch:21 step:20148 [D loss: 0.624987, acc: 67.19%] [G loss: 2.178995]\n",
      "epoch:21 step:20149 [D loss: 0.591421, acc: 68.75%] [G loss: 2.165980]\n",
      "epoch:21 step:20150 [D loss: 0.726974, acc: 57.81%] [G loss: 1.722301]\n",
      "epoch:21 step:20151 [D loss: 0.620569, acc: 65.62%] [G loss: 1.758062]\n",
      "epoch:21 step:20152 [D loss: 0.620835, acc: 66.41%] [G loss: 1.964226]\n",
      "epoch:21 step:20153 [D loss: 0.634550, acc: 64.06%] [G loss: 1.937448]\n",
      "epoch:21 step:20154 [D loss: 0.705817, acc: 57.03%] [G loss: 1.728304]\n",
      "epoch:21 step:20155 [D loss: 0.641931, acc: 65.62%] [G loss: 1.798337]\n",
      "epoch:21 step:20156 [D loss: 0.618703, acc: 67.97%] [G loss: 2.040650]\n",
      "epoch:21 step:20157 [D loss: 0.709499, acc: 52.34%] [G loss: 1.953089]\n",
      "epoch:21 step:20158 [D loss: 0.533882, acc: 76.56%] [G loss: 2.125472]\n",
      "epoch:21 step:20159 [D loss: 0.616325, acc: 67.97%] [G loss: 1.953683]\n",
      "epoch:21 step:20160 [D loss: 0.691356, acc: 53.12%] [G loss: 1.761521]\n",
      "epoch:21 step:20161 [D loss: 0.650495, acc: 57.81%] [G loss: 1.856954]\n",
      "epoch:21 step:20162 [D loss: 0.624028, acc: 69.53%] [G loss: 1.950001]\n",
      "epoch:21 step:20163 [D loss: 0.633742, acc: 66.41%] [G loss: 1.909730]\n",
      "epoch:21 step:20164 [D loss: 0.664725, acc: 58.59%] [G loss: 1.893069]\n",
      "epoch:21 step:20165 [D loss: 0.656278, acc: 61.72%] [G loss: 2.097434]\n",
      "epoch:21 step:20166 [D loss: 0.698435, acc: 53.91%] [G loss: 1.755402]\n",
      "epoch:21 step:20167 [D loss: 0.660537, acc: 56.25%] [G loss: 1.820816]\n",
      "epoch:21 step:20168 [D loss: 0.679212, acc: 58.59%] [G loss: 1.808159]\n",
      "epoch:21 step:20169 [D loss: 0.618805, acc: 67.19%] [G loss: 1.800798]\n",
      "epoch:21 step:20170 [D loss: 0.649452, acc: 60.94%] [G loss: 1.884578]\n",
      "epoch:21 step:20171 [D loss: 0.643615, acc: 60.94%] [G loss: 1.979192]\n",
      "epoch:21 step:20172 [D loss: 0.608531, acc: 68.75%] [G loss: 2.060447]\n",
      "epoch:21 step:20173 [D loss: 0.671803, acc: 57.03%] [G loss: 2.071743]\n",
      "epoch:21 step:20174 [D loss: 0.591494, acc: 71.88%] [G loss: 2.036002]\n",
      "epoch:21 step:20175 [D loss: 0.704543, acc: 57.81%] [G loss: 1.872170]\n",
      "epoch:21 step:20176 [D loss: 0.668595, acc: 57.81%] [G loss: 2.016192]\n",
      "epoch:21 step:20177 [D loss: 0.690297, acc: 59.38%] [G loss: 1.874279]\n",
      "epoch:21 step:20178 [D loss: 0.702977, acc: 53.91%] [G loss: 1.698493]\n",
      "epoch:21 step:20179 [D loss: 0.683236, acc: 59.38%] [G loss: 1.779272]\n",
      "epoch:21 step:20180 [D loss: 0.677320, acc: 59.38%] [G loss: 1.831243]\n",
      "epoch:21 step:20181 [D loss: 0.669126, acc: 54.69%] [G loss: 1.967956]\n",
      "epoch:21 step:20182 [D loss: 0.648008, acc: 64.84%] [G loss: 1.781626]\n",
      "epoch:21 step:20183 [D loss: 0.692245, acc: 57.03%] [G loss: 1.795169]\n",
      "epoch:21 step:20184 [D loss: 0.644506, acc: 60.94%] [G loss: 1.830142]\n",
      "epoch:21 step:20185 [D loss: 0.576752, acc: 74.22%] [G loss: 2.004614]\n",
      "epoch:21 step:20186 [D loss: 0.691923, acc: 53.91%] [G loss: 1.827372]\n",
      "epoch:21 step:20187 [D loss: 0.620099, acc: 64.06%] [G loss: 1.871670]\n",
      "epoch:21 step:20188 [D loss: 0.696537, acc: 56.25%] [G loss: 1.815191]\n",
      "epoch:21 step:20189 [D loss: 0.658075, acc: 58.59%] [G loss: 1.816198]\n",
      "epoch:21 step:20190 [D loss: 0.648752, acc: 64.06%] [G loss: 1.916527]\n",
      "epoch:21 step:20191 [D loss: 0.633269, acc: 60.94%] [G loss: 1.892027]\n",
      "epoch:21 step:20192 [D loss: 0.651976, acc: 60.94%] [G loss: 1.928229]\n",
      "epoch:21 step:20193 [D loss: 0.582118, acc: 71.88%] [G loss: 1.981043]\n",
      "epoch:21 step:20194 [D loss: 0.656161, acc: 61.72%] [G loss: 1.885200]\n",
      "epoch:21 step:20195 [D loss: 0.655960, acc: 63.28%] [G loss: 1.852661]\n",
      "epoch:21 step:20196 [D loss: 0.641049, acc: 58.59%] [G loss: 1.803703]\n",
      "epoch:21 step:20197 [D loss: 0.605348, acc: 68.75%] [G loss: 1.974876]\n",
      "epoch:21 step:20198 [D loss: 0.607261, acc: 71.09%] [G loss: 2.087153]\n",
      "epoch:21 step:20199 [D loss: 0.594397, acc: 67.19%] [G loss: 1.963104]\n",
      "epoch:21 step:20200 [D loss: 0.641647, acc: 65.62%] [G loss: 2.096611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.61029953 1.46148468 6.30749729 4.81765703 3.65829066 5.76440831\n",
      " 4.4363205  4.63589116 4.51055013 3.59712762]\n",
      "##########\n",
      "epoch:21 step:20201 [D loss: 0.641321, acc: 65.62%] [G loss: 1.973797]\n",
      "epoch:21 step:20202 [D loss: 0.671684, acc: 59.38%] [G loss: 1.893513]\n",
      "epoch:21 step:20203 [D loss: 0.642738, acc: 61.72%] [G loss: 1.850415]\n",
      "epoch:21 step:20204 [D loss: 0.686143, acc: 59.38%] [G loss: 1.831122]\n",
      "epoch:21 step:20205 [D loss: 0.653229, acc: 59.38%] [G loss: 1.665342]\n",
      "epoch:21 step:20206 [D loss: 0.683702, acc: 59.38%] [G loss: 1.710161]\n",
      "epoch:21 step:20207 [D loss: 0.709464, acc: 52.34%] [G loss: 1.688442]\n",
      "epoch:21 step:20208 [D loss: 0.659004, acc: 60.16%] [G loss: 1.824333]\n",
      "epoch:21 step:20209 [D loss: 0.579458, acc: 69.53%] [G loss: 2.030989]\n",
      "epoch:21 step:20210 [D loss: 0.659340, acc: 51.56%] [G loss: 1.840714]\n",
      "epoch:21 step:20211 [D loss: 0.638713, acc: 66.41%] [G loss: 1.905319]\n",
      "epoch:21 step:20212 [D loss: 0.567043, acc: 74.22%] [G loss: 1.893711]\n",
      "epoch:21 step:20213 [D loss: 0.621427, acc: 68.75%] [G loss: 2.078883]\n",
      "epoch:21 step:20214 [D loss: 0.647783, acc: 61.72%] [G loss: 1.872185]\n",
      "epoch:21 step:20215 [D loss: 0.666221, acc: 58.59%] [G loss: 1.826533]\n",
      "epoch:21 step:20216 [D loss: 0.611929, acc: 65.62%] [G loss: 1.862423]\n",
      "epoch:21 step:20217 [D loss: 0.680476, acc: 53.91%] [G loss: 1.877433]\n",
      "epoch:21 step:20218 [D loss: 0.655590, acc: 60.16%] [G loss: 1.888578]\n",
      "epoch:21 step:20219 [D loss: 0.659939, acc: 62.50%] [G loss: 1.803512]\n",
      "epoch:21 step:20220 [D loss: 0.658797, acc: 57.03%] [G loss: 1.786560]\n",
      "epoch:21 step:20221 [D loss: 0.640055, acc: 59.38%] [G loss: 1.825903]\n",
      "epoch:21 step:20222 [D loss: 0.642996, acc: 62.50%] [G loss: 1.835103]\n",
      "epoch:21 step:20223 [D loss: 0.662765, acc: 62.50%] [G loss: 1.823293]\n",
      "epoch:21 step:20224 [D loss: 0.666425, acc: 62.50%] [G loss: 1.907101]\n",
      "epoch:21 step:20225 [D loss: 0.640727, acc: 65.62%] [G loss: 1.879404]\n",
      "epoch:21 step:20226 [D loss: 0.570273, acc: 74.22%] [G loss: 2.018559]\n",
      "epoch:21 step:20227 [D loss: 0.609506, acc: 71.88%] [G loss: 2.003910]\n",
      "epoch:21 step:20228 [D loss: 0.571102, acc: 73.44%] [G loss: 2.010298]\n",
      "epoch:21 step:20229 [D loss: 0.655892, acc: 67.97%] [G loss: 2.000782]\n",
      "epoch:21 step:20230 [D loss: 0.624550, acc: 64.84%] [G loss: 1.915212]\n",
      "epoch:21 step:20231 [D loss: 0.591396, acc: 67.97%] [G loss: 2.086140]\n",
      "epoch:21 step:20232 [D loss: 0.583103, acc: 70.31%] [G loss: 1.870290]\n",
      "epoch:21 step:20233 [D loss: 0.589692, acc: 68.75%] [G loss: 2.204576]\n",
      "epoch:21 step:20234 [D loss: 0.613477, acc: 67.97%] [G loss: 1.890547]\n",
      "epoch:21 step:20235 [D loss: 0.604967, acc: 70.31%] [G loss: 2.095778]\n",
      "epoch:21 step:20236 [D loss: 0.663353, acc: 58.59%] [G loss: 1.968794]\n",
      "epoch:21 step:20237 [D loss: 0.668955, acc: 62.50%] [G loss: 1.796886]\n",
      "epoch:21 step:20238 [D loss: 0.627862, acc: 63.28%] [G loss: 1.962379]\n",
      "epoch:21 step:20239 [D loss: 0.685896, acc: 60.94%] [G loss: 1.813263]\n",
      "epoch:21 step:20240 [D loss: 0.644382, acc: 62.50%] [G loss: 1.982681]\n",
      "epoch:21 step:20241 [D loss: 0.600496, acc: 67.19%] [G loss: 2.032080]\n",
      "epoch:21 step:20242 [D loss: 0.660370, acc: 60.94%] [G loss: 1.830442]\n",
      "epoch:21 step:20243 [D loss: 0.726930, acc: 57.03%] [G loss: 1.819729]\n",
      "epoch:21 step:20244 [D loss: 0.659417, acc: 59.38%] [G loss: 1.764619]\n",
      "epoch:21 step:20245 [D loss: 0.687144, acc: 56.25%] [G loss: 1.790892]\n",
      "epoch:21 step:20246 [D loss: 0.665065, acc: 62.50%] [G loss: 1.851848]\n",
      "epoch:21 step:20247 [D loss: 0.631551, acc: 66.41%] [G loss: 2.030447]\n",
      "epoch:21 step:20248 [D loss: 0.633897, acc: 63.28%] [G loss: 1.920361]\n",
      "epoch:21 step:20249 [D loss: 0.634847, acc: 62.50%] [G loss: 1.878564]\n",
      "epoch:21 step:20250 [D loss: 0.684875, acc: 55.47%] [G loss: 1.724198]\n",
      "epoch:21 step:20251 [D loss: 0.670512, acc: 60.94%] [G loss: 1.858073]\n",
      "epoch:21 step:20252 [D loss: 0.668640, acc: 59.38%] [G loss: 1.900880]\n",
      "epoch:21 step:20253 [D loss: 0.720121, acc: 49.22%] [G loss: 1.748541]\n",
      "epoch:21 step:20254 [D loss: 0.652094, acc: 63.28%] [G loss: 1.766946]\n",
      "epoch:21 step:20255 [D loss: 0.637757, acc: 66.41%] [G loss: 1.944506]\n",
      "epoch:21 step:20256 [D loss: 0.640350, acc: 61.72%] [G loss: 1.767089]\n",
      "epoch:21 step:20257 [D loss: 0.673417, acc: 57.81%] [G loss: 1.797212]\n",
      "epoch:21 step:20258 [D loss: 0.684239, acc: 57.03%] [G loss: 1.755660]\n",
      "epoch:21 step:20259 [D loss: 0.641869, acc: 70.31%] [G loss: 1.840998]\n",
      "epoch:21 step:20260 [D loss: 0.672669, acc: 67.19%] [G loss: 2.001910]\n",
      "epoch:21 step:20261 [D loss: 0.656353, acc: 59.38%] [G loss: 1.685184]\n",
      "epoch:21 step:20262 [D loss: 0.591520, acc: 69.53%] [G loss: 1.907503]\n",
      "epoch:21 step:20263 [D loss: 0.622954, acc: 67.97%] [G loss: 1.831152]\n",
      "epoch:21 step:20264 [D loss: 0.627856, acc: 59.38%] [G loss: 1.834829]\n",
      "epoch:21 step:20265 [D loss: 0.625109, acc: 69.53%] [G loss: 2.025281]\n",
      "epoch:21 step:20266 [D loss: 0.615037, acc: 60.94%] [G loss: 1.851954]\n",
      "epoch:21 step:20267 [D loss: 0.700921, acc: 57.81%] [G loss: 1.950805]\n",
      "epoch:21 step:20268 [D loss: 0.633442, acc: 61.72%] [G loss: 1.840550]\n",
      "epoch:21 step:20269 [D loss: 0.691239, acc: 58.59%] [G loss: 1.810003]\n",
      "epoch:21 step:20270 [D loss: 0.705556, acc: 57.81%] [G loss: 1.795824]\n",
      "epoch:21 step:20271 [D loss: 0.622787, acc: 65.62%] [G loss: 1.927121]\n",
      "epoch:21 step:20272 [D loss: 0.675707, acc: 60.16%] [G loss: 1.784883]\n",
      "epoch:21 step:20273 [D loss: 0.644905, acc: 63.28%] [G loss: 1.854344]\n",
      "epoch:21 step:20274 [D loss: 0.682843, acc: 58.59%] [G loss: 1.830636]\n",
      "epoch:21 step:20275 [D loss: 0.634265, acc: 61.72%] [G loss: 1.886486]\n",
      "epoch:21 step:20276 [D loss: 0.639973, acc: 61.72%] [G loss: 1.895985]\n",
      "epoch:21 step:20277 [D loss: 0.656841, acc: 57.81%] [G loss: 1.888039]\n",
      "epoch:21 step:20278 [D loss: 0.662302, acc: 61.72%] [G loss: 1.914490]\n",
      "epoch:21 step:20279 [D loss: 0.595188, acc: 67.19%] [G loss: 2.043296]\n",
      "epoch:21 step:20280 [D loss: 0.590249, acc: 67.19%] [G loss: 1.929974]\n",
      "epoch:21 step:20281 [D loss: 0.634574, acc: 64.06%] [G loss: 1.879125]\n",
      "epoch:21 step:20282 [D loss: 0.649980, acc: 60.94%] [G loss: 1.868606]\n",
      "epoch:21 step:20283 [D loss: 0.637219, acc: 62.50%] [G loss: 1.744635]\n",
      "epoch:21 step:20284 [D loss: 0.670285, acc: 64.06%] [G loss: 1.866406]\n",
      "epoch:21 step:20285 [D loss: 0.624894, acc: 64.06%] [G loss: 1.908705]\n",
      "epoch:21 step:20286 [D loss: 0.602170, acc: 64.06%] [G loss: 2.130076]\n",
      "epoch:21 step:20287 [D loss: 0.656278, acc: 62.50%] [G loss: 1.834359]\n",
      "epoch:21 step:20288 [D loss: 0.630418, acc: 65.62%] [G loss: 1.739200]\n",
      "epoch:21 step:20289 [D loss: 0.644993, acc: 64.06%] [G loss: 1.866037]\n",
      "epoch:21 step:20290 [D loss: 0.607523, acc: 70.31%] [G loss: 1.938092]\n",
      "epoch:21 step:20291 [D loss: 0.685600, acc: 57.81%] [G loss: 1.674952]\n",
      "epoch:21 step:20292 [D loss: 0.677760, acc: 57.81%] [G loss: 1.764993]\n",
      "epoch:21 step:20293 [D loss: 0.673070, acc: 61.72%] [G loss: 1.868663]\n",
      "epoch:21 step:20294 [D loss: 0.647555, acc: 60.16%] [G loss: 1.906818]\n",
      "epoch:21 step:20295 [D loss: 0.653333, acc: 60.16%] [G loss: 1.969255]\n",
      "epoch:21 step:20296 [D loss: 0.660237, acc: 61.72%] [G loss: 1.877217]\n",
      "epoch:21 step:20297 [D loss: 0.612337, acc: 63.28%] [G loss: 1.854128]\n",
      "epoch:21 step:20298 [D loss: 0.634513, acc: 60.94%] [G loss: 1.866409]\n",
      "epoch:21 step:20299 [D loss: 0.724868, acc: 53.12%] [G loss: 1.904633]\n",
      "epoch:21 step:20300 [D loss: 0.671457, acc: 55.47%] [G loss: 1.851579]\n",
      "epoch:21 step:20301 [D loss: 0.600562, acc: 69.53%] [G loss: 1.837756]\n",
      "epoch:21 step:20302 [D loss: 0.684876, acc: 57.03%] [G loss: 1.772631]\n",
      "epoch:21 step:20303 [D loss: 0.621960, acc: 66.41%] [G loss: 1.941877]\n",
      "epoch:21 step:20304 [D loss: 0.668612, acc: 62.50%] [G loss: 1.752672]\n",
      "epoch:21 step:20305 [D loss: 0.661890, acc: 58.59%] [G loss: 1.766702]\n",
      "epoch:21 step:20306 [D loss: 0.648987, acc: 64.84%] [G loss: 1.970137]\n",
      "epoch:21 step:20307 [D loss: 0.638022, acc: 64.84%] [G loss: 1.891307]\n",
      "epoch:21 step:20308 [D loss: 0.609685, acc: 67.19%] [G loss: 1.929634]\n",
      "epoch:21 step:20309 [D loss: 0.562995, acc: 69.53%] [G loss: 1.887536]\n",
      "epoch:21 step:20310 [D loss: 0.613976, acc: 66.41%] [G loss: 1.873522]\n",
      "epoch:21 step:20311 [D loss: 0.631304, acc: 61.72%] [G loss: 1.956564]\n",
      "epoch:21 step:20312 [D loss: 0.587735, acc: 71.88%] [G loss: 2.201377]\n",
      "epoch:21 step:20313 [D loss: 0.649061, acc: 63.28%] [G loss: 1.999721]\n",
      "epoch:21 step:20314 [D loss: 0.588323, acc: 71.88%] [G loss: 2.098764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20315 [D loss: 0.671764, acc: 61.72%] [G loss: 1.996169]\n",
      "epoch:21 step:20316 [D loss: 0.636070, acc: 64.06%] [G loss: 1.914666]\n",
      "epoch:21 step:20317 [D loss: 0.649012, acc: 60.94%] [G loss: 2.068049]\n",
      "epoch:21 step:20318 [D loss: 0.612827, acc: 67.19%] [G loss: 2.022719]\n",
      "epoch:21 step:20319 [D loss: 0.645950, acc: 64.06%] [G loss: 1.984529]\n",
      "epoch:21 step:20320 [D loss: 0.648605, acc: 63.28%] [G loss: 1.765504]\n",
      "epoch:21 step:20321 [D loss: 0.639843, acc: 60.94%] [G loss: 1.913659]\n",
      "epoch:21 step:20322 [D loss: 0.611226, acc: 67.97%] [G loss: 1.908457]\n",
      "epoch:21 step:20323 [D loss: 0.614552, acc: 64.84%] [G loss: 2.181914]\n",
      "epoch:21 step:20324 [D loss: 0.613574, acc: 64.06%] [G loss: 2.138462]\n",
      "epoch:21 step:20325 [D loss: 0.616320, acc: 64.84%] [G loss: 2.257870]\n",
      "epoch:21 step:20326 [D loss: 0.610191, acc: 67.19%] [G loss: 2.061365]\n",
      "epoch:21 step:20327 [D loss: 0.623050, acc: 66.41%] [G loss: 2.130086]\n",
      "epoch:21 step:20328 [D loss: 0.690251, acc: 60.94%] [G loss: 1.926761]\n",
      "epoch:21 step:20329 [D loss: 0.664319, acc: 58.59%] [G loss: 2.020643]\n",
      "epoch:21 step:20330 [D loss: 0.665781, acc: 61.72%] [G loss: 1.837134]\n",
      "epoch:21 step:20331 [D loss: 0.642763, acc: 65.62%] [G loss: 2.029594]\n",
      "epoch:21 step:20332 [D loss: 0.651074, acc: 59.38%] [G loss: 1.921100]\n",
      "epoch:21 step:20333 [D loss: 0.705798, acc: 53.12%] [G loss: 1.795647]\n",
      "epoch:21 step:20334 [D loss: 0.692090, acc: 57.81%] [G loss: 1.748894]\n",
      "epoch:21 step:20335 [D loss: 0.687628, acc: 57.03%] [G loss: 1.752534]\n",
      "epoch:21 step:20336 [D loss: 0.664023, acc: 59.38%] [G loss: 1.696364]\n",
      "epoch:21 step:20337 [D loss: 0.657675, acc: 62.50%] [G loss: 1.750636]\n",
      "epoch:21 step:20338 [D loss: 0.627126, acc: 64.84%] [G loss: 1.884865]\n",
      "epoch:21 step:20339 [D loss: 0.612269, acc: 66.41%] [G loss: 1.789049]\n",
      "epoch:21 step:20340 [D loss: 0.610050, acc: 66.41%] [G loss: 1.813686]\n",
      "epoch:21 step:20341 [D loss: 0.686432, acc: 57.81%] [G loss: 1.792557]\n",
      "epoch:21 step:20342 [D loss: 0.666960, acc: 63.28%] [G loss: 1.970860]\n",
      "epoch:21 step:20343 [D loss: 0.682723, acc: 67.19%] [G loss: 1.666021]\n",
      "epoch:21 step:20344 [D loss: 0.708821, acc: 53.12%] [G loss: 1.744466]\n",
      "epoch:21 step:20345 [D loss: 0.651003, acc: 60.16%] [G loss: 1.951036]\n",
      "epoch:21 step:20346 [D loss: 0.593394, acc: 70.31%] [G loss: 1.772519]\n",
      "epoch:21 step:20347 [D loss: 0.604650, acc: 66.41%] [G loss: 1.868524]\n",
      "epoch:21 step:20348 [D loss: 0.674423, acc: 57.81%] [G loss: 1.805492]\n",
      "epoch:21 step:20349 [D loss: 0.643571, acc: 63.28%] [G loss: 1.859832]\n",
      "epoch:21 step:20350 [D loss: 0.719369, acc: 55.47%] [G loss: 1.764961]\n",
      "epoch:21 step:20351 [D loss: 0.649731, acc: 64.06%] [G loss: 1.914896]\n",
      "epoch:21 step:20352 [D loss: 0.688202, acc: 57.81%] [G loss: 1.830975]\n",
      "epoch:21 step:20353 [D loss: 0.636719, acc: 60.16%] [G loss: 1.779287]\n",
      "epoch:21 step:20354 [D loss: 0.627180, acc: 70.31%] [G loss: 1.883317]\n",
      "epoch:21 step:20355 [D loss: 0.708787, acc: 51.56%] [G loss: 1.808456]\n",
      "epoch:21 step:20356 [D loss: 0.639898, acc: 65.62%] [G loss: 1.934472]\n",
      "epoch:21 step:20357 [D loss: 0.643419, acc: 56.25%] [G loss: 1.941777]\n",
      "epoch:21 step:20358 [D loss: 0.625496, acc: 66.41%] [G loss: 1.972693]\n",
      "epoch:21 step:20359 [D loss: 0.702885, acc: 57.81%] [G loss: 1.708601]\n",
      "epoch:21 step:20360 [D loss: 0.670973, acc: 63.28%] [G loss: 1.780842]\n",
      "epoch:21 step:20361 [D loss: 0.683491, acc: 55.47%] [G loss: 1.813424]\n",
      "epoch:21 step:20362 [D loss: 0.621928, acc: 66.41%] [G loss: 1.929734]\n",
      "epoch:21 step:20363 [D loss: 0.659701, acc: 64.06%] [G loss: 1.911714]\n",
      "epoch:21 step:20364 [D loss: 0.646499, acc: 66.41%] [G loss: 1.923782]\n",
      "epoch:21 step:20365 [D loss: 0.616299, acc: 71.09%] [G loss: 2.026134]\n",
      "epoch:21 step:20366 [D loss: 0.640504, acc: 62.50%] [G loss: 1.934590]\n",
      "epoch:21 step:20367 [D loss: 0.624570, acc: 67.97%] [G loss: 1.900293]\n",
      "epoch:21 step:20368 [D loss: 0.606477, acc: 67.19%] [G loss: 1.992054]\n",
      "epoch:21 step:20369 [D loss: 0.666436, acc: 62.50%] [G loss: 1.952277]\n",
      "epoch:21 step:20370 [D loss: 0.644154, acc: 63.28%] [G loss: 1.939939]\n",
      "epoch:21 step:20371 [D loss: 0.571025, acc: 71.88%] [G loss: 2.086680]\n",
      "epoch:21 step:20372 [D loss: 0.594549, acc: 63.28%] [G loss: 1.946134]\n",
      "epoch:21 step:20373 [D loss: 0.676270, acc: 63.28%] [G loss: 1.818951]\n",
      "epoch:21 step:20374 [D loss: 0.641142, acc: 64.84%] [G loss: 1.947110]\n",
      "epoch:21 step:20375 [D loss: 0.656546, acc: 59.38%] [G loss: 1.811642]\n",
      "epoch:21 step:20376 [D loss: 0.657969, acc: 60.94%] [G loss: 1.858233]\n",
      "epoch:21 step:20377 [D loss: 0.616679, acc: 64.84%] [G loss: 2.025795]\n",
      "epoch:21 step:20378 [D loss: 0.695377, acc: 60.16%] [G loss: 1.922323]\n",
      "epoch:21 step:20379 [D loss: 0.612958, acc: 67.97%] [G loss: 1.817940]\n",
      "epoch:21 step:20380 [D loss: 0.675438, acc: 56.25%] [G loss: 1.785262]\n",
      "epoch:21 step:20381 [D loss: 0.661335, acc: 57.03%] [G loss: 1.831725]\n",
      "epoch:21 step:20382 [D loss: 0.651886, acc: 64.06%] [G loss: 1.875584]\n",
      "epoch:21 step:20383 [D loss: 0.626388, acc: 67.97%] [G loss: 1.859808]\n",
      "epoch:21 step:20384 [D loss: 0.619379, acc: 68.75%] [G loss: 1.969984]\n",
      "epoch:21 step:20385 [D loss: 0.558028, acc: 69.53%] [G loss: 2.036453]\n",
      "epoch:21 step:20386 [D loss: 0.607444, acc: 67.97%] [G loss: 2.073570]\n",
      "epoch:21 step:20387 [D loss: 0.667999, acc: 58.59%] [G loss: 2.050039]\n",
      "epoch:21 step:20388 [D loss: 0.644187, acc: 64.06%] [G loss: 2.131295]\n",
      "epoch:21 step:20389 [D loss: 0.651770, acc: 58.59%] [G loss: 2.011868]\n",
      "epoch:21 step:20390 [D loss: 0.622526, acc: 67.19%] [G loss: 1.903975]\n",
      "epoch:21 step:20391 [D loss: 0.599147, acc: 67.19%] [G loss: 1.868010]\n",
      "epoch:21 step:20392 [D loss: 0.599804, acc: 68.75%] [G loss: 1.891012]\n",
      "epoch:21 step:20393 [D loss: 0.660908, acc: 63.28%] [G loss: 1.773857]\n",
      "epoch:21 step:20394 [D loss: 0.641387, acc: 64.84%] [G loss: 1.988998]\n",
      "epoch:21 step:20395 [D loss: 0.666369, acc: 60.94%] [G loss: 1.976190]\n",
      "epoch:21 step:20396 [D loss: 0.645916, acc: 61.72%] [G loss: 2.013849]\n",
      "epoch:21 step:20397 [D loss: 0.693932, acc: 53.12%] [G loss: 2.013403]\n",
      "epoch:21 step:20398 [D loss: 0.642743, acc: 61.72%] [G loss: 1.954060]\n",
      "epoch:21 step:20399 [D loss: 0.628312, acc: 64.84%] [G loss: 1.836771]\n",
      "epoch:21 step:20400 [D loss: 0.631645, acc: 60.94%] [G loss: 2.053224]\n",
      "##############\n",
      "[2.56145889 1.70567361 6.31777162 4.7693062  3.62590272 5.56848734\n",
      " 4.36807282 4.80028572 4.63660523 3.61796374]\n",
      "##########\n",
      "epoch:21 step:20401 [D loss: 0.594521, acc: 73.44%] [G loss: 1.988894]\n",
      "epoch:21 step:20402 [D loss: 0.639798, acc: 64.06%] [G loss: 1.971756]\n",
      "epoch:21 step:20403 [D loss: 0.618735, acc: 68.75%] [G loss: 1.884416]\n",
      "epoch:21 step:20404 [D loss: 0.691433, acc: 53.12%] [G loss: 1.872669]\n",
      "epoch:21 step:20405 [D loss: 0.664866, acc: 63.28%] [G loss: 1.982358]\n",
      "epoch:21 step:20406 [D loss: 0.673942, acc: 59.38%] [G loss: 1.772947]\n",
      "epoch:21 step:20407 [D loss: 0.645033, acc: 63.28%] [G loss: 1.857279]\n",
      "epoch:21 step:20408 [D loss: 0.629249, acc: 66.41%] [G loss: 1.738217]\n",
      "epoch:21 step:20409 [D loss: 0.669688, acc: 59.38%] [G loss: 1.833144]\n",
      "epoch:21 step:20410 [D loss: 0.626996, acc: 67.19%] [G loss: 1.911699]\n",
      "epoch:21 step:20411 [D loss: 0.653868, acc: 59.38%] [G loss: 1.916785]\n",
      "epoch:21 step:20412 [D loss: 0.627555, acc: 63.28%] [G loss: 1.915372]\n",
      "epoch:21 step:20413 [D loss: 0.653893, acc: 62.50%] [G loss: 1.883144]\n",
      "epoch:21 step:20414 [D loss: 0.624478, acc: 64.84%] [G loss: 1.838080]\n",
      "epoch:21 step:20415 [D loss: 0.672853, acc: 64.06%] [G loss: 1.936536]\n",
      "epoch:21 step:20416 [D loss: 0.696819, acc: 57.03%] [G loss: 1.929498]\n",
      "epoch:21 step:20417 [D loss: 0.661965, acc: 64.06%] [G loss: 1.946913]\n",
      "epoch:21 step:20418 [D loss: 0.660967, acc: 61.72%] [G loss: 1.895898]\n",
      "epoch:21 step:20419 [D loss: 0.702847, acc: 60.16%] [G loss: 1.849851]\n",
      "epoch:21 step:20420 [D loss: 0.654854, acc: 63.28%] [G loss: 1.832296]\n",
      "epoch:21 step:20421 [D loss: 0.643426, acc: 63.28%] [G loss: 1.813779]\n",
      "epoch:21 step:20422 [D loss: 0.639167, acc: 61.72%] [G loss: 1.895053]\n",
      "epoch:21 step:20423 [D loss: 0.587870, acc: 69.53%] [G loss: 1.908538]\n",
      "epoch:21 step:20424 [D loss: 0.626821, acc: 62.50%] [G loss: 1.938046]\n",
      "epoch:21 step:20425 [D loss: 0.688208, acc: 60.94%] [G loss: 1.724080]\n",
      "epoch:21 step:20426 [D loss: 0.651698, acc: 64.06%] [G loss: 1.849649]\n",
      "epoch:21 step:20427 [D loss: 0.652294, acc: 59.38%] [G loss: 1.860210]\n",
      "epoch:21 step:20428 [D loss: 0.631339, acc: 60.94%] [G loss: 1.875679]\n",
      "epoch:21 step:20429 [D loss: 0.643835, acc: 66.41%] [G loss: 1.851933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20430 [D loss: 0.648790, acc: 64.06%] [G loss: 1.697941]\n",
      "epoch:21 step:20431 [D loss: 0.601632, acc: 64.84%] [G loss: 1.914383]\n",
      "epoch:21 step:20432 [D loss: 0.633939, acc: 62.50%] [G loss: 1.876796]\n",
      "epoch:21 step:20433 [D loss: 0.634958, acc: 61.72%] [G loss: 2.011789]\n",
      "epoch:21 step:20434 [D loss: 0.629767, acc: 64.84%] [G loss: 1.955843]\n",
      "epoch:21 step:20435 [D loss: 0.666019, acc: 58.59%] [G loss: 1.792893]\n",
      "epoch:21 step:20436 [D loss: 0.673027, acc: 55.47%] [G loss: 1.780957]\n",
      "epoch:21 step:20437 [D loss: 0.659414, acc: 60.16%] [G loss: 1.813587]\n",
      "epoch:21 step:20438 [D loss: 0.717378, acc: 54.69%] [G loss: 1.850851]\n",
      "epoch:21 step:20439 [D loss: 0.696227, acc: 50.78%] [G loss: 1.881016]\n",
      "epoch:21 step:20440 [D loss: 0.653224, acc: 62.50%] [G loss: 1.827290]\n",
      "epoch:21 step:20441 [D loss: 0.626080, acc: 70.31%] [G loss: 1.881087]\n",
      "epoch:21 step:20442 [D loss: 0.639929, acc: 60.16%] [G loss: 1.719412]\n",
      "epoch:21 step:20443 [D loss: 0.658236, acc: 60.16%] [G loss: 1.790345]\n",
      "epoch:21 step:20444 [D loss: 0.635902, acc: 65.62%] [G loss: 1.889008]\n",
      "epoch:21 step:20445 [D loss: 0.664086, acc: 63.28%] [G loss: 1.831498]\n",
      "epoch:21 step:20446 [D loss: 0.633783, acc: 64.06%] [G loss: 1.954839]\n",
      "epoch:21 step:20447 [D loss: 0.631529, acc: 64.84%] [G loss: 1.888731]\n",
      "epoch:21 step:20448 [D loss: 0.624610, acc: 65.62%] [G loss: 1.773505]\n",
      "epoch:21 step:20449 [D loss: 0.648386, acc: 64.84%] [G loss: 1.903796]\n",
      "epoch:21 step:20450 [D loss: 0.588186, acc: 67.97%] [G loss: 1.987509]\n",
      "epoch:21 step:20451 [D loss: 0.714143, acc: 58.59%] [G loss: 2.078041]\n",
      "epoch:21 step:20452 [D loss: 0.589890, acc: 68.75%] [G loss: 2.033325]\n",
      "epoch:21 step:20453 [D loss: 0.649159, acc: 59.38%] [G loss: 1.799540]\n",
      "epoch:21 step:20454 [D loss: 0.626945, acc: 64.84%] [G loss: 1.971115]\n",
      "epoch:21 step:20455 [D loss: 0.632931, acc: 63.28%] [G loss: 1.967741]\n",
      "epoch:21 step:20456 [D loss: 0.689834, acc: 57.81%] [G loss: 1.833423]\n",
      "epoch:21 step:20457 [D loss: 0.648338, acc: 60.16%] [G loss: 2.009620]\n",
      "epoch:21 step:20458 [D loss: 0.597887, acc: 62.50%] [G loss: 2.155582]\n",
      "epoch:21 step:20459 [D loss: 0.608370, acc: 68.75%] [G loss: 2.102066]\n",
      "epoch:21 step:20460 [D loss: 0.677417, acc: 60.16%] [G loss: 1.966080]\n",
      "epoch:21 step:20461 [D loss: 0.688084, acc: 59.38%] [G loss: 1.665114]\n",
      "epoch:21 step:20462 [D loss: 0.646049, acc: 68.75%] [G loss: 1.851811]\n",
      "epoch:21 step:20463 [D loss: 0.606039, acc: 66.41%] [G loss: 1.955387]\n",
      "epoch:21 step:20464 [D loss: 0.683040, acc: 60.94%] [G loss: 1.966928]\n",
      "epoch:21 step:20465 [D loss: 0.659397, acc: 58.59%] [G loss: 1.864907]\n",
      "epoch:21 step:20466 [D loss: 0.605329, acc: 67.97%] [G loss: 2.014804]\n",
      "epoch:21 step:20467 [D loss: 0.644882, acc: 63.28%] [G loss: 2.103686]\n",
      "epoch:21 step:20468 [D loss: 0.627688, acc: 66.41%] [G loss: 1.901076]\n",
      "epoch:21 step:20469 [D loss: 0.611291, acc: 66.41%] [G loss: 2.051795]\n",
      "epoch:21 step:20470 [D loss: 0.640078, acc: 58.59%] [G loss: 2.020509]\n",
      "epoch:21 step:20471 [D loss: 0.701983, acc: 50.78%] [G loss: 1.823349]\n",
      "epoch:21 step:20472 [D loss: 0.696471, acc: 55.47%] [G loss: 1.867613]\n",
      "epoch:21 step:20473 [D loss: 0.661862, acc: 55.47%] [G loss: 1.919280]\n",
      "epoch:21 step:20474 [D loss: 0.666511, acc: 60.16%] [G loss: 1.945751]\n",
      "epoch:21 step:20475 [D loss: 0.635411, acc: 64.84%] [G loss: 1.931611]\n",
      "epoch:21 step:20476 [D loss: 0.684561, acc: 56.25%] [G loss: 1.780686]\n",
      "epoch:21 step:20477 [D loss: 0.702190, acc: 52.34%] [G loss: 1.798528]\n",
      "epoch:21 step:20478 [D loss: 0.652095, acc: 61.72%] [G loss: 1.797422]\n",
      "epoch:21 step:20479 [D loss: 0.613146, acc: 66.41%] [G loss: 1.881435]\n",
      "epoch:21 step:20480 [D loss: 0.681070, acc: 57.03%] [G loss: 1.931755]\n",
      "epoch:21 step:20481 [D loss: 0.649370, acc: 59.38%] [G loss: 1.924960]\n",
      "epoch:21 step:20482 [D loss: 0.665970, acc: 62.50%] [G loss: 1.931948]\n",
      "epoch:21 step:20483 [D loss: 0.622831, acc: 67.19%] [G loss: 1.887977]\n",
      "epoch:21 step:20484 [D loss: 0.635826, acc: 62.50%] [G loss: 1.867412]\n",
      "epoch:21 step:20485 [D loss: 0.607472, acc: 67.97%] [G loss: 2.018046]\n",
      "epoch:21 step:20486 [D loss: 0.614380, acc: 68.75%] [G loss: 1.909561]\n",
      "epoch:21 step:20487 [D loss: 0.633237, acc: 64.06%] [G loss: 1.940216]\n",
      "epoch:21 step:20488 [D loss: 0.628726, acc: 63.28%] [G loss: 1.733433]\n",
      "epoch:21 step:20489 [D loss: 0.704107, acc: 52.34%] [G loss: 1.769844]\n",
      "epoch:21 step:20490 [D loss: 0.641708, acc: 63.28%] [G loss: 1.841839]\n",
      "epoch:21 step:20491 [D loss: 0.664070, acc: 57.03%] [G loss: 1.819556]\n",
      "epoch:21 step:20492 [D loss: 0.638873, acc: 65.62%] [G loss: 2.029122]\n",
      "epoch:21 step:20493 [D loss: 0.595441, acc: 72.66%] [G loss: 2.210310]\n",
      "epoch:21 step:20494 [D loss: 0.637487, acc: 67.97%] [G loss: 1.895725]\n",
      "epoch:21 step:20495 [D loss: 0.638111, acc: 62.50%] [G loss: 1.877726]\n",
      "epoch:21 step:20496 [D loss: 0.705059, acc: 54.69%] [G loss: 1.899705]\n",
      "epoch:21 step:20497 [D loss: 0.725851, acc: 47.66%] [G loss: 1.583876]\n",
      "epoch:21 step:20498 [D loss: 0.624667, acc: 67.19%] [G loss: 1.789371]\n",
      "epoch:21 step:20499 [D loss: 0.606070, acc: 65.62%] [G loss: 1.775690]\n",
      "epoch:21 step:20500 [D loss: 0.619303, acc: 61.72%] [G loss: 1.862626]\n",
      "epoch:21 step:20501 [D loss: 0.702073, acc: 54.69%] [G loss: 1.792597]\n",
      "epoch:21 step:20502 [D loss: 0.633355, acc: 65.62%] [G loss: 1.931472]\n",
      "epoch:21 step:20503 [D loss: 0.642141, acc: 63.28%] [G loss: 1.877105]\n",
      "epoch:21 step:20504 [D loss: 0.663641, acc: 60.94%] [G loss: 1.724676]\n",
      "epoch:21 step:20505 [D loss: 0.654949, acc: 60.94%] [G loss: 1.756944]\n",
      "epoch:21 step:20506 [D loss: 0.660993, acc: 62.50%] [G loss: 1.792005]\n",
      "epoch:21 step:20507 [D loss: 0.712920, acc: 50.00%] [G loss: 1.800456]\n",
      "epoch:21 step:20508 [D loss: 0.588430, acc: 67.97%] [G loss: 1.969288]\n",
      "epoch:21 step:20509 [D loss: 0.624039, acc: 68.75%] [G loss: 1.941695]\n",
      "epoch:21 step:20510 [D loss: 0.622006, acc: 60.16%] [G loss: 2.000499]\n",
      "epoch:21 step:20511 [D loss: 0.691229, acc: 57.81%] [G loss: 1.865576]\n",
      "epoch:21 step:20512 [D loss: 0.625893, acc: 63.28%] [G loss: 1.811693]\n",
      "epoch:21 step:20513 [D loss: 0.645587, acc: 57.81%] [G loss: 1.806274]\n",
      "epoch:21 step:20514 [D loss: 0.612242, acc: 59.38%] [G loss: 1.908938]\n",
      "epoch:21 step:20515 [D loss: 0.630463, acc: 60.94%] [G loss: 1.922908]\n",
      "epoch:21 step:20516 [D loss: 0.632868, acc: 61.72%] [G loss: 1.909018]\n",
      "epoch:21 step:20517 [D loss: 0.618810, acc: 67.97%] [G loss: 1.965509]\n",
      "epoch:21 step:20518 [D loss: 0.579952, acc: 70.31%] [G loss: 2.043249]\n",
      "epoch:21 step:20519 [D loss: 0.620788, acc: 66.41%] [G loss: 2.051247]\n",
      "epoch:21 step:20520 [D loss: 0.655668, acc: 60.16%] [G loss: 1.877365]\n",
      "epoch:21 step:20521 [D loss: 0.661818, acc: 53.12%] [G loss: 1.928192]\n",
      "epoch:21 step:20522 [D loss: 0.662405, acc: 55.47%] [G loss: 2.079366]\n",
      "epoch:21 step:20523 [D loss: 0.655342, acc: 59.38%] [G loss: 1.883776]\n",
      "epoch:21 step:20524 [D loss: 0.621005, acc: 65.62%] [G loss: 1.916487]\n",
      "epoch:21 step:20525 [D loss: 0.679904, acc: 61.72%] [G loss: 1.885409]\n",
      "epoch:21 step:20526 [D loss: 0.638430, acc: 62.50%] [G loss: 1.918803]\n",
      "epoch:21 step:20527 [D loss: 0.654319, acc: 61.72%] [G loss: 1.812696]\n",
      "epoch:21 step:20528 [D loss: 0.665200, acc: 59.38%] [G loss: 1.691359]\n",
      "epoch:21 step:20529 [D loss: 0.663549, acc: 59.38%] [G loss: 1.835673]\n",
      "epoch:21 step:20530 [D loss: 0.692669, acc: 57.81%] [G loss: 1.900100]\n",
      "epoch:21 step:20531 [D loss: 0.677447, acc: 69.53%] [G loss: 1.899961]\n",
      "epoch:21 step:20532 [D loss: 0.644318, acc: 60.16%] [G loss: 1.683938]\n",
      "epoch:21 step:20533 [D loss: 0.688588, acc: 59.38%] [G loss: 1.815119]\n",
      "epoch:21 step:20534 [D loss: 0.628192, acc: 65.62%] [G loss: 1.835194]\n",
      "epoch:21 step:20535 [D loss: 0.737135, acc: 55.47%] [G loss: 1.813092]\n",
      "epoch:21 step:20536 [D loss: 0.724970, acc: 53.12%] [G loss: 1.680222]\n",
      "epoch:21 step:20537 [D loss: 0.666604, acc: 60.16%] [G loss: 1.837437]\n",
      "epoch:21 step:20538 [D loss: 0.693569, acc: 57.03%] [G loss: 1.793319]\n",
      "epoch:21 step:20539 [D loss: 0.652801, acc: 63.28%] [G loss: 1.754156]\n",
      "epoch:21 step:20540 [D loss: 0.629150, acc: 60.94%] [G loss: 1.888489]\n",
      "epoch:21 step:20541 [D loss: 0.662875, acc: 61.72%] [G loss: 1.792971]\n",
      "epoch:21 step:20542 [D loss: 0.649491, acc: 60.94%] [G loss: 1.753686]\n",
      "epoch:21 step:20543 [D loss: 0.644929, acc: 57.81%] [G loss: 1.823928]\n",
      "epoch:21 step:20544 [D loss: 0.677241, acc: 57.81%] [G loss: 1.738868]\n",
      "epoch:21 step:20545 [D loss: 0.638967, acc: 62.50%] [G loss: 1.758328]\n",
      "epoch:21 step:20546 [D loss: 0.686491, acc: 57.81%] [G loss: 1.774466]\n",
      "epoch:21 step:20547 [D loss: 0.664114, acc: 60.94%] [G loss: 1.778949]\n",
      "epoch:21 step:20548 [D loss: 0.655187, acc: 61.72%] [G loss: 1.817284]\n",
      "epoch:21 step:20549 [D loss: 0.627798, acc: 68.75%] [G loss: 1.781078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20550 [D loss: 0.624591, acc: 67.97%] [G loss: 1.767044]\n",
      "epoch:21 step:20551 [D loss: 0.646191, acc: 59.38%] [G loss: 1.874153]\n",
      "epoch:21 step:20552 [D loss: 0.599933, acc: 67.97%] [G loss: 1.973657]\n",
      "epoch:21 step:20553 [D loss: 0.608142, acc: 68.75%] [G loss: 1.966907]\n",
      "epoch:21 step:20554 [D loss: 0.669406, acc: 60.94%] [G loss: 1.861042]\n",
      "epoch:21 step:20555 [D loss: 0.678594, acc: 56.25%] [G loss: 1.921981]\n",
      "epoch:21 step:20556 [D loss: 0.640146, acc: 64.84%] [G loss: 1.936743]\n",
      "epoch:21 step:20557 [D loss: 0.640535, acc: 55.47%] [G loss: 1.825736]\n",
      "epoch:21 step:20558 [D loss: 0.681764, acc: 64.06%] [G loss: 1.911329]\n",
      "epoch:21 step:20559 [D loss: 0.625524, acc: 64.84%] [G loss: 1.880311]\n",
      "epoch:21 step:20560 [D loss: 0.654972, acc: 59.38%] [G loss: 2.019547]\n",
      "epoch:21 step:20561 [D loss: 0.634884, acc: 62.50%] [G loss: 2.088662]\n",
      "epoch:21 step:20562 [D loss: 0.661217, acc: 61.72%] [G loss: 1.883869]\n",
      "epoch:21 step:20563 [D loss: 0.596054, acc: 67.97%] [G loss: 2.048423]\n",
      "epoch:21 step:20564 [D loss: 0.664530, acc: 56.25%] [G loss: 1.793914]\n",
      "epoch:21 step:20565 [D loss: 0.620821, acc: 65.62%] [G loss: 1.890939]\n",
      "epoch:21 step:20566 [D loss: 0.646654, acc: 64.06%] [G loss: 2.085979]\n",
      "epoch:21 step:20567 [D loss: 0.627896, acc: 64.84%] [G loss: 2.056178]\n",
      "epoch:21 step:20568 [D loss: 0.662212, acc: 60.94%] [G loss: 1.781088]\n",
      "epoch:21 step:20569 [D loss: 0.745856, acc: 52.34%] [G loss: 1.882551]\n",
      "epoch:21 step:20570 [D loss: 0.664628, acc: 58.59%] [G loss: 1.784203]\n",
      "epoch:21 step:20571 [D loss: 0.637218, acc: 68.75%] [G loss: 2.009125]\n",
      "epoch:21 step:20572 [D loss: 0.670967, acc: 61.72%] [G loss: 1.821391]\n",
      "epoch:21 step:20573 [D loss: 0.689500, acc: 60.94%] [G loss: 1.740797]\n",
      "epoch:21 step:20574 [D loss: 0.672308, acc: 65.62%] [G loss: 1.879952]\n",
      "epoch:21 step:20575 [D loss: 0.663913, acc: 57.81%] [G loss: 1.865418]\n",
      "epoch:21 step:20576 [D loss: 0.665101, acc: 57.81%] [G loss: 2.032328]\n",
      "epoch:21 step:20577 [D loss: 0.662879, acc: 56.25%] [G loss: 1.950534]\n",
      "epoch:21 step:20578 [D loss: 0.631639, acc: 63.28%] [G loss: 1.931308]\n",
      "epoch:21 step:20579 [D loss: 0.674703, acc: 57.81%] [G loss: 1.852602]\n",
      "epoch:21 step:20580 [D loss: 0.652719, acc: 64.06%] [G loss: 1.866816]\n",
      "epoch:21 step:20581 [D loss: 0.621964, acc: 65.62%] [G loss: 2.060140]\n",
      "epoch:21 step:20582 [D loss: 0.628776, acc: 69.53%] [G loss: 1.963685]\n",
      "epoch:21 step:20583 [D loss: 0.646673, acc: 61.72%] [G loss: 1.967068]\n",
      "epoch:21 step:20584 [D loss: 0.614537, acc: 62.50%] [G loss: 1.961083]\n",
      "epoch:21 step:20585 [D loss: 0.580480, acc: 72.66%] [G loss: 2.018234]\n",
      "epoch:21 step:20586 [D loss: 0.608670, acc: 67.19%] [G loss: 2.135691]\n",
      "epoch:21 step:20587 [D loss: 0.602930, acc: 64.84%] [G loss: 1.957396]\n",
      "epoch:21 step:20588 [D loss: 0.648883, acc: 65.62%] [G loss: 2.007149]\n",
      "epoch:21 step:20589 [D loss: 0.681225, acc: 63.28%] [G loss: 2.056520]\n",
      "epoch:21 step:20590 [D loss: 0.736446, acc: 51.56%] [G loss: 1.874847]\n",
      "epoch:21 step:20591 [D loss: 0.635585, acc: 57.81%] [G loss: 1.891171]\n",
      "epoch:21 step:20592 [D loss: 0.602636, acc: 65.62%] [G loss: 1.982443]\n",
      "epoch:21 step:20593 [D loss: 0.660555, acc: 61.72%] [G loss: 1.875571]\n",
      "epoch:21 step:20594 [D loss: 0.611403, acc: 71.09%] [G loss: 1.854785]\n",
      "epoch:21 step:20595 [D loss: 0.584542, acc: 70.31%] [G loss: 2.112035]\n",
      "epoch:21 step:20596 [D loss: 0.583507, acc: 67.97%] [G loss: 2.136173]\n",
      "epoch:21 step:20597 [D loss: 0.673974, acc: 57.81%] [G loss: 1.736149]\n",
      "epoch:21 step:20598 [D loss: 0.681685, acc: 60.94%] [G loss: 1.963028]\n",
      "epoch:21 step:20599 [D loss: 0.616474, acc: 67.19%] [G loss: 1.956312]\n",
      "epoch:21 step:20600 [D loss: 0.591749, acc: 68.75%] [G loss: 2.067389]\n",
      "##############\n",
      "[2.40613951 1.59134524 5.93844708 4.77376548 3.55661338 5.62781118\n",
      " 4.42371465 4.45171185 4.46170464 3.64239916]\n",
      "##########\n",
      "epoch:21 step:20601 [D loss: 0.583402, acc: 68.75%] [G loss: 2.219264]\n",
      "epoch:21 step:20602 [D loss: 0.574036, acc: 70.31%] [G loss: 2.175592]\n",
      "epoch:21 step:20603 [D loss: 0.640777, acc: 64.06%] [G loss: 2.075226]\n",
      "epoch:21 step:20604 [D loss: 0.636342, acc: 67.19%] [G loss: 2.210947]\n",
      "epoch:21 step:20605 [D loss: 0.745780, acc: 54.69%] [G loss: 1.837874]\n",
      "epoch:21 step:20606 [D loss: 0.734891, acc: 53.12%] [G loss: 1.931245]\n",
      "epoch:21 step:20607 [D loss: 0.609518, acc: 62.50%] [G loss: 1.967504]\n",
      "epoch:21 step:20608 [D loss: 0.648382, acc: 60.16%] [G loss: 1.867997]\n",
      "epoch:21 step:20609 [D loss: 0.601846, acc: 66.41%] [G loss: 1.903113]\n",
      "epoch:21 step:20610 [D loss: 0.670739, acc: 63.28%] [G loss: 1.886528]\n",
      "epoch:21 step:20611 [D loss: 0.649111, acc: 67.19%] [G loss: 1.896951]\n",
      "epoch:21 step:20612 [D loss: 0.573936, acc: 75.78%] [G loss: 2.060783]\n",
      "epoch:21 step:20613 [D loss: 0.636187, acc: 70.31%] [G loss: 2.197419]\n",
      "epoch:21 step:20614 [D loss: 0.611627, acc: 66.41%] [G loss: 2.458678]\n",
      "epoch:22 step:20615 [D loss: 0.666305, acc: 57.03%] [G loss: 1.753097]\n",
      "epoch:22 step:20616 [D loss: 0.683204, acc: 60.16%] [G loss: 1.949584]\n",
      "epoch:22 step:20617 [D loss: 0.684016, acc: 60.16%] [G loss: 1.898918]\n",
      "epoch:22 step:20618 [D loss: 0.639923, acc: 63.28%] [G loss: 1.893359]\n",
      "epoch:22 step:20619 [D loss: 0.606090, acc: 63.28%] [G loss: 1.887212]\n",
      "epoch:22 step:20620 [D loss: 0.623878, acc: 68.75%] [G loss: 2.159659]\n",
      "epoch:22 step:20621 [D loss: 0.661805, acc: 66.41%] [G loss: 1.886379]\n",
      "epoch:22 step:20622 [D loss: 0.620083, acc: 64.06%] [G loss: 1.971530]\n",
      "epoch:22 step:20623 [D loss: 0.640012, acc: 62.50%] [G loss: 2.024575]\n",
      "epoch:22 step:20624 [D loss: 0.602642, acc: 65.62%] [G loss: 1.937126]\n",
      "epoch:22 step:20625 [D loss: 0.684640, acc: 57.03%] [G loss: 1.908568]\n",
      "epoch:22 step:20626 [D loss: 0.683830, acc: 57.81%] [G loss: 1.828030]\n",
      "epoch:22 step:20627 [D loss: 0.600818, acc: 72.66%] [G loss: 1.926172]\n",
      "epoch:22 step:20628 [D loss: 0.645874, acc: 66.41%] [G loss: 1.989713]\n",
      "epoch:22 step:20629 [D loss: 0.645332, acc: 63.28%] [G loss: 2.017885]\n",
      "epoch:22 step:20630 [D loss: 0.702189, acc: 57.03%] [G loss: 2.126490]\n",
      "epoch:22 step:20631 [D loss: 0.662155, acc: 61.72%] [G loss: 1.898456]\n",
      "epoch:22 step:20632 [D loss: 0.687722, acc: 54.69%] [G loss: 1.903038]\n",
      "epoch:22 step:20633 [D loss: 0.625336, acc: 62.50%] [G loss: 1.927783]\n",
      "epoch:22 step:20634 [D loss: 0.695589, acc: 54.69%] [G loss: 1.688605]\n",
      "epoch:22 step:20635 [D loss: 0.670693, acc: 64.84%] [G loss: 1.649613]\n",
      "epoch:22 step:20636 [D loss: 0.624077, acc: 67.19%] [G loss: 1.807862]\n",
      "epoch:22 step:20637 [D loss: 0.663194, acc: 57.81%] [G loss: 1.824252]\n",
      "epoch:22 step:20638 [D loss: 0.654965, acc: 63.28%] [G loss: 1.869243]\n",
      "epoch:22 step:20639 [D loss: 0.619273, acc: 68.75%] [G loss: 1.951839]\n",
      "epoch:22 step:20640 [D loss: 0.690431, acc: 53.12%] [G loss: 1.849742]\n",
      "epoch:22 step:20641 [D loss: 0.706565, acc: 59.38%] [G loss: 1.711168]\n",
      "epoch:22 step:20642 [D loss: 0.604495, acc: 64.84%] [G loss: 1.824963]\n",
      "epoch:22 step:20643 [D loss: 0.638739, acc: 63.28%] [G loss: 1.879121]\n",
      "epoch:22 step:20644 [D loss: 0.659094, acc: 59.38%] [G loss: 1.756444]\n",
      "epoch:22 step:20645 [D loss: 0.663780, acc: 59.38%] [G loss: 1.753019]\n",
      "epoch:22 step:20646 [D loss: 0.694241, acc: 54.69%] [G loss: 1.752549]\n",
      "epoch:22 step:20647 [D loss: 0.663032, acc: 60.16%] [G loss: 1.842463]\n",
      "epoch:22 step:20648 [D loss: 0.669564, acc: 54.69%] [G loss: 1.768845]\n",
      "epoch:22 step:20649 [D loss: 0.639594, acc: 63.28%] [G loss: 1.826488]\n",
      "epoch:22 step:20650 [D loss: 0.648723, acc: 63.28%] [G loss: 1.892894]\n",
      "epoch:22 step:20651 [D loss: 0.660715, acc: 63.28%] [G loss: 1.938447]\n",
      "epoch:22 step:20652 [D loss: 0.667431, acc: 64.84%] [G loss: 1.921224]\n",
      "epoch:22 step:20653 [D loss: 0.642662, acc: 63.28%] [G loss: 1.789949]\n",
      "epoch:22 step:20654 [D loss: 0.568789, acc: 76.56%] [G loss: 1.956543]\n",
      "epoch:22 step:20655 [D loss: 0.649421, acc: 63.28%] [G loss: 1.762311]\n",
      "epoch:22 step:20656 [D loss: 0.611553, acc: 68.75%] [G loss: 2.059737]\n",
      "epoch:22 step:20657 [D loss: 0.630572, acc: 61.72%] [G loss: 1.933074]\n",
      "epoch:22 step:20658 [D loss: 0.616073, acc: 66.41%] [G loss: 2.109046]\n",
      "epoch:22 step:20659 [D loss: 0.642172, acc: 60.16%] [G loss: 2.249837]\n",
      "epoch:22 step:20660 [D loss: 0.653723, acc: 58.59%] [G loss: 1.872945]\n",
      "epoch:22 step:20661 [D loss: 0.700902, acc: 59.38%] [G loss: 1.918643]\n",
      "epoch:22 step:20662 [D loss: 0.666881, acc: 58.59%] [G loss: 1.878590]\n",
      "epoch:22 step:20663 [D loss: 0.605734, acc: 65.62%] [G loss: 1.987187]\n",
      "epoch:22 step:20664 [D loss: 0.596392, acc: 67.19%] [G loss: 1.952684]\n",
      "epoch:22 step:20665 [D loss: 0.699085, acc: 57.81%] [G loss: 1.958032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20666 [D loss: 0.644370, acc: 60.16%] [G loss: 1.758076]\n",
      "epoch:22 step:20667 [D loss: 0.624681, acc: 60.16%] [G loss: 1.832619]\n",
      "epoch:22 step:20668 [D loss: 0.619318, acc: 64.06%] [G loss: 2.018439]\n",
      "epoch:22 step:20669 [D loss: 0.603675, acc: 66.41%] [G loss: 1.950977]\n",
      "epoch:22 step:20670 [D loss: 0.620319, acc: 67.19%] [G loss: 2.023703]\n",
      "epoch:22 step:20671 [D loss: 0.634801, acc: 67.19%] [G loss: 1.877842]\n",
      "epoch:22 step:20672 [D loss: 0.725762, acc: 56.25%] [G loss: 1.958523]\n",
      "epoch:22 step:20673 [D loss: 0.607836, acc: 66.41%] [G loss: 2.025148]\n",
      "epoch:22 step:20674 [D loss: 0.667116, acc: 60.94%] [G loss: 1.918140]\n",
      "epoch:22 step:20675 [D loss: 0.642951, acc: 64.06%] [G loss: 1.867653]\n",
      "epoch:22 step:20676 [D loss: 0.679558, acc: 55.47%] [G loss: 1.754107]\n",
      "epoch:22 step:20677 [D loss: 0.691264, acc: 58.59%] [G loss: 1.843302]\n",
      "epoch:22 step:20678 [D loss: 0.684869, acc: 54.69%] [G loss: 1.813789]\n",
      "epoch:22 step:20679 [D loss: 0.643900, acc: 62.50%] [G loss: 1.796849]\n",
      "epoch:22 step:20680 [D loss: 0.666413, acc: 63.28%] [G loss: 1.811029]\n",
      "epoch:22 step:20681 [D loss: 0.646986, acc: 63.28%] [G loss: 1.842812]\n",
      "epoch:22 step:20682 [D loss: 0.623212, acc: 65.62%] [G loss: 1.847709]\n",
      "epoch:22 step:20683 [D loss: 0.622887, acc: 63.28%] [G loss: 1.975562]\n",
      "epoch:22 step:20684 [D loss: 0.659832, acc: 60.16%] [G loss: 1.878362]\n",
      "epoch:22 step:20685 [D loss: 0.686765, acc: 57.81%] [G loss: 1.712340]\n",
      "epoch:22 step:20686 [D loss: 0.670484, acc: 61.72%] [G loss: 1.767626]\n",
      "epoch:22 step:20687 [D loss: 0.664376, acc: 57.03%] [G loss: 1.805064]\n",
      "epoch:22 step:20688 [D loss: 0.611380, acc: 64.84%] [G loss: 1.926103]\n",
      "epoch:22 step:20689 [D loss: 0.602174, acc: 71.09%] [G loss: 2.072512]\n",
      "epoch:22 step:20690 [D loss: 0.619351, acc: 60.16%] [G loss: 2.080180]\n",
      "epoch:22 step:20691 [D loss: 0.630834, acc: 64.84%] [G loss: 1.948131]\n",
      "epoch:22 step:20692 [D loss: 0.638741, acc: 57.03%] [G loss: 1.899393]\n",
      "epoch:22 step:20693 [D loss: 0.619614, acc: 65.62%] [G loss: 1.811755]\n",
      "epoch:22 step:20694 [D loss: 0.641362, acc: 60.16%] [G loss: 1.941454]\n",
      "epoch:22 step:20695 [D loss: 0.709087, acc: 51.56%] [G loss: 1.731043]\n",
      "epoch:22 step:20696 [D loss: 0.705287, acc: 56.25%] [G loss: 1.773477]\n",
      "epoch:22 step:20697 [D loss: 0.652758, acc: 59.38%] [G loss: 1.801803]\n",
      "epoch:22 step:20698 [D loss: 0.609541, acc: 67.97%] [G loss: 1.871427]\n",
      "epoch:22 step:20699 [D loss: 0.622996, acc: 65.62%] [G loss: 1.824860]\n",
      "epoch:22 step:20700 [D loss: 0.639287, acc: 60.94%] [G loss: 1.796840]\n",
      "epoch:22 step:20701 [D loss: 0.623032, acc: 64.84%] [G loss: 1.847937]\n",
      "epoch:22 step:20702 [D loss: 0.623988, acc: 68.75%] [G loss: 1.851363]\n",
      "epoch:22 step:20703 [D loss: 0.604796, acc: 71.88%] [G loss: 1.915150]\n",
      "epoch:22 step:20704 [D loss: 0.689934, acc: 53.91%] [G loss: 1.828135]\n",
      "epoch:22 step:20705 [D loss: 0.633538, acc: 61.72%] [G loss: 1.870673]\n",
      "epoch:22 step:20706 [D loss: 0.679513, acc: 58.59%] [G loss: 1.880877]\n",
      "epoch:22 step:20707 [D loss: 0.584039, acc: 71.88%] [G loss: 2.101218]\n",
      "epoch:22 step:20708 [D loss: 0.623376, acc: 64.06%] [G loss: 1.932863]\n",
      "epoch:22 step:20709 [D loss: 0.655278, acc: 58.59%] [G loss: 1.876083]\n",
      "epoch:22 step:20710 [D loss: 0.647370, acc: 64.06%] [G loss: 1.963576]\n",
      "epoch:22 step:20711 [D loss: 0.684032, acc: 64.84%] [G loss: 1.935373]\n",
      "epoch:22 step:20712 [D loss: 0.661844, acc: 62.50%] [G loss: 1.968743]\n",
      "epoch:22 step:20713 [D loss: 0.625604, acc: 68.75%] [G loss: 1.910732]\n",
      "epoch:22 step:20714 [D loss: 0.637234, acc: 65.62%] [G loss: 1.943980]\n",
      "epoch:22 step:20715 [D loss: 0.586204, acc: 68.75%] [G loss: 1.832757]\n",
      "epoch:22 step:20716 [D loss: 0.617188, acc: 62.50%] [G loss: 1.888983]\n",
      "epoch:22 step:20717 [D loss: 0.645930, acc: 62.50%] [G loss: 1.840867]\n",
      "epoch:22 step:20718 [D loss: 0.672059, acc: 62.50%] [G loss: 1.910242]\n",
      "epoch:22 step:20719 [D loss: 0.628095, acc: 62.50%] [G loss: 1.838223]\n",
      "epoch:22 step:20720 [D loss: 0.676185, acc: 59.38%] [G loss: 2.016609]\n",
      "epoch:22 step:20721 [D loss: 0.611358, acc: 71.88%] [G loss: 2.014281]\n",
      "epoch:22 step:20722 [D loss: 0.695034, acc: 60.94%] [G loss: 1.808780]\n",
      "epoch:22 step:20723 [D loss: 0.657188, acc: 59.38%] [G loss: 1.791435]\n",
      "epoch:22 step:20724 [D loss: 0.643732, acc: 61.72%] [G loss: 1.766700]\n",
      "epoch:22 step:20725 [D loss: 0.666506, acc: 60.94%] [G loss: 1.905711]\n",
      "epoch:22 step:20726 [D loss: 0.631155, acc: 68.75%] [G loss: 1.933204]\n",
      "epoch:22 step:20727 [D loss: 0.627316, acc: 62.50%] [G loss: 2.057798]\n",
      "epoch:22 step:20728 [D loss: 0.643557, acc: 61.72%] [G loss: 1.897213]\n",
      "epoch:22 step:20729 [D loss: 0.614260, acc: 67.97%] [G loss: 2.096513]\n",
      "epoch:22 step:20730 [D loss: 0.537915, acc: 78.12%] [G loss: 2.204343]\n",
      "epoch:22 step:20731 [D loss: 0.655468, acc: 61.72%] [G loss: 2.161582]\n",
      "epoch:22 step:20732 [D loss: 0.622244, acc: 65.62%] [G loss: 1.965009]\n",
      "epoch:22 step:20733 [D loss: 0.610387, acc: 67.97%] [G loss: 2.057747]\n",
      "epoch:22 step:20734 [D loss: 0.617902, acc: 67.19%] [G loss: 2.041008]\n",
      "epoch:22 step:20735 [D loss: 0.684498, acc: 57.81%] [G loss: 1.995522]\n",
      "epoch:22 step:20736 [D loss: 0.638675, acc: 66.41%] [G loss: 2.108108]\n",
      "epoch:22 step:20737 [D loss: 0.631865, acc: 64.06%] [G loss: 1.962522]\n",
      "epoch:22 step:20738 [D loss: 0.735767, acc: 53.91%] [G loss: 1.932174]\n",
      "epoch:22 step:20739 [D loss: 0.735677, acc: 52.34%] [G loss: 1.739077]\n",
      "epoch:22 step:20740 [D loss: 0.674745, acc: 59.38%] [G loss: 1.951480]\n",
      "epoch:22 step:20741 [D loss: 0.683963, acc: 54.69%] [G loss: 1.761655]\n",
      "epoch:22 step:20742 [D loss: 0.687449, acc: 55.47%] [G loss: 1.795316]\n",
      "epoch:22 step:20743 [D loss: 0.574182, acc: 74.22%] [G loss: 1.929385]\n",
      "epoch:22 step:20744 [D loss: 0.624183, acc: 68.75%] [G loss: 2.011996]\n",
      "epoch:22 step:20745 [D loss: 0.654477, acc: 55.47%] [G loss: 2.062726]\n",
      "epoch:22 step:20746 [D loss: 0.637734, acc: 58.59%] [G loss: 1.758960]\n",
      "epoch:22 step:20747 [D loss: 0.695630, acc: 54.69%] [G loss: 1.718748]\n",
      "epoch:22 step:20748 [D loss: 0.678276, acc: 53.91%] [G loss: 1.700884]\n",
      "epoch:22 step:20749 [D loss: 0.668702, acc: 58.59%] [G loss: 1.874575]\n",
      "epoch:22 step:20750 [D loss: 0.663347, acc: 57.81%] [G loss: 1.708010]\n",
      "epoch:22 step:20751 [D loss: 0.687936, acc: 57.81%] [G loss: 1.825640]\n",
      "epoch:22 step:20752 [D loss: 0.661843, acc: 58.59%] [G loss: 1.762425]\n",
      "epoch:22 step:20753 [D loss: 0.624606, acc: 64.06%] [G loss: 1.757017]\n",
      "epoch:22 step:20754 [D loss: 0.641340, acc: 60.94%] [G loss: 1.923428]\n",
      "epoch:22 step:20755 [D loss: 0.600212, acc: 69.53%] [G loss: 1.906907]\n",
      "epoch:22 step:20756 [D loss: 0.661653, acc: 63.28%] [G loss: 1.932130]\n",
      "epoch:22 step:20757 [D loss: 0.638361, acc: 60.16%] [G loss: 1.764856]\n",
      "epoch:22 step:20758 [D loss: 0.632441, acc: 67.19%] [G loss: 1.811596]\n",
      "epoch:22 step:20759 [D loss: 0.711282, acc: 56.25%] [G loss: 1.858921]\n",
      "epoch:22 step:20760 [D loss: 0.642612, acc: 61.72%] [G loss: 2.000957]\n",
      "epoch:22 step:20761 [D loss: 0.613288, acc: 65.62%] [G loss: 1.858508]\n",
      "epoch:22 step:20762 [D loss: 0.698424, acc: 54.69%] [G loss: 1.706725]\n",
      "epoch:22 step:20763 [D loss: 0.636595, acc: 63.28%] [G loss: 2.080068]\n",
      "epoch:22 step:20764 [D loss: 0.641145, acc: 57.81%] [G loss: 1.974443]\n",
      "epoch:22 step:20765 [D loss: 0.625817, acc: 64.06%] [G loss: 1.918037]\n",
      "epoch:22 step:20766 [D loss: 0.713073, acc: 52.34%] [G loss: 1.813291]\n",
      "epoch:22 step:20767 [D loss: 0.628248, acc: 62.50%] [G loss: 1.819498]\n",
      "epoch:22 step:20768 [D loss: 0.664779, acc: 61.72%] [G loss: 1.913857]\n",
      "epoch:22 step:20769 [D loss: 0.633926, acc: 64.06%] [G loss: 1.838569]\n",
      "epoch:22 step:20770 [D loss: 0.637282, acc: 62.50%] [G loss: 1.954710]\n",
      "epoch:22 step:20771 [D loss: 0.612150, acc: 66.41%] [G loss: 1.940380]\n",
      "epoch:22 step:20772 [D loss: 0.683915, acc: 57.81%] [G loss: 1.889438]\n",
      "epoch:22 step:20773 [D loss: 0.612383, acc: 68.75%] [G loss: 1.941303]\n",
      "epoch:22 step:20774 [D loss: 0.700866, acc: 56.25%] [G loss: 1.858889]\n",
      "epoch:22 step:20775 [D loss: 0.679249, acc: 53.91%] [G loss: 1.718176]\n",
      "epoch:22 step:20776 [D loss: 0.650303, acc: 58.59%] [G loss: 1.841491]\n",
      "epoch:22 step:20777 [D loss: 0.610712, acc: 74.22%] [G loss: 1.866999]\n",
      "epoch:22 step:20778 [D loss: 0.657942, acc: 58.59%] [G loss: 1.707122]\n",
      "epoch:22 step:20779 [D loss: 0.649507, acc: 60.16%] [G loss: 1.769135]\n",
      "epoch:22 step:20780 [D loss: 0.696244, acc: 50.00%] [G loss: 1.759870]\n",
      "epoch:22 step:20781 [D loss: 0.624699, acc: 69.53%] [G loss: 1.896770]\n",
      "epoch:22 step:20782 [D loss: 0.665010, acc: 64.06%] [G loss: 1.761022]\n",
      "epoch:22 step:20783 [D loss: 0.690320, acc: 53.12%] [G loss: 1.928009]\n",
      "epoch:22 step:20784 [D loss: 0.649423, acc: 56.25%] [G loss: 1.818235]\n",
      "epoch:22 step:20785 [D loss: 0.651242, acc: 64.06%] [G loss: 1.846058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20786 [D loss: 0.627206, acc: 66.41%] [G loss: 1.801100]\n",
      "epoch:22 step:20787 [D loss: 0.631959, acc: 65.62%] [G loss: 1.896967]\n",
      "epoch:22 step:20788 [D loss: 0.631947, acc: 64.06%] [G loss: 1.812611]\n",
      "epoch:22 step:20789 [D loss: 0.674257, acc: 54.69%] [G loss: 1.872561]\n",
      "epoch:22 step:20790 [D loss: 0.682465, acc: 62.50%] [G loss: 1.827212]\n",
      "epoch:22 step:20791 [D loss: 0.643130, acc: 60.16%] [G loss: 1.892681]\n",
      "epoch:22 step:20792 [D loss: 0.629535, acc: 64.84%] [G loss: 1.823662]\n",
      "epoch:22 step:20793 [D loss: 0.663047, acc: 58.59%] [G loss: 1.865099]\n",
      "epoch:22 step:20794 [D loss: 0.673260, acc: 60.16%] [G loss: 1.903077]\n",
      "epoch:22 step:20795 [D loss: 0.622334, acc: 65.62%] [G loss: 1.878161]\n",
      "epoch:22 step:20796 [D loss: 0.678222, acc: 57.03%] [G loss: 1.817341]\n",
      "epoch:22 step:20797 [D loss: 0.737564, acc: 48.44%] [G loss: 1.926233]\n",
      "epoch:22 step:20798 [D loss: 0.650310, acc: 63.28%] [G loss: 1.851017]\n",
      "epoch:22 step:20799 [D loss: 0.643739, acc: 64.84%] [G loss: 1.806498]\n",
      "epoch:22 step:20800 [D loss: 0.683055, acc: 57.81%] [G loss: 1.738952]\n",
      "##############\n",
      "[2.43437465 1.261604   6.19284368 4.72214082 3.62914277 5.42903951\n",
      " 4.29627476 4.63307239 4.44841941 3.51540716]\n",
      "##########\n",
      "epoch:22 step:20801 [D loss: 0.689553, acc: 55.47%] [G loss: 1.859758]\n",
      "epoch:22 step:20802 [D loss: 0.669870, acc: 56.25%] [G loss: 1.783928]\n",
      "epoch:22 step:20803 [D loss: 0.632777, acc: 67.97%] [G loss: 1.764325]\n",
      "epoch:22 step:20804 [D loss: 0.611042, acc: 64.06%] [G loss: 1.818437]\n",
      "epoch:22 step:20805 [D loss: 0.639383, acc: 60.94%] [G loss: 1.824990]\n",
      "epoch:22 step:20806 [D loss: 0.612652, acc: 67.19%] [G loss: 2.100230]\n",
      "epoch:22 step:20807 [D loss: 0.681283, acc: 56.25%] [G loss: 1.937470]\n",
      "epoch:22 step:20808 [D loss: 0.643394, acc: 65.62%] [G loss: 2.053675]\n",
      "epoch:22 step:20809 [D loss: 0.676444, acc: 59.38%] [G loss: 1.924996]\n",
      "epoch:22 step:20810 [D loss: 0.625211, acc: 61.72%] [G loss: 1.870969]\n",
      "epoch:22 step:20811 [D loss: 0.591333, acc: 69.53%] [G loss: 1.934185]\n",
      "epoch:22 step:20812 [D loss: 0.638214, acc: 62.50%] [G loss: 1.821490]\n",
      "epoch:22 step:20813 [D loss: 0.693627, acc: 55.47%] [G loss: 1.999663]\n",
      "epoch:22 step:20814 [D loss: 0.674503, acc: 63.28%] [G loss: 1.813073]\n",
      "epoch:22 step:20815 [D loss: 0.631570, acc: 62.50%] [G loss: 1.785714]\n",
      "epoch:22 step:20816 [D loss: 0.687849, acc: 60.16%] [G loss: 1.851476]\n",
      "epoch:22 step:20817 [D loss: 0.647644, acc: 59.38%] [G loss: 1.875532]\n",
      "epoch:22 step:20818 [D loss: 0.632956, acc: 67.19%] [G loss: 1.844817]\n",
      "epoch:22 step:20819 [D loss: 0.645137, acc: 59.38%] [G loss: 1.889908]\n",
      "epoch:22 step:20820 [D loss: 0.626431, acc: 62.50%] [G loss: 1.907259]\n",
      "epoch:22 step:20821 [D loss: 0.607541, acc: 67.19%] [G loss: 2.266523]\n",
      "epoch:22 step:20822 [D loss: 0.601961, acc: 67.97%] [G loss: 2.331292]\n",
      "epoch:22 step:20823 [D loss: 0.599284, acc: 67.97%] [G loss: 2.105516]\n",
      "epoch:22 step:20824 [D loss: 0.639112, acc: 61.72%] [G loss: 1.783478]\n",
      "epoch:22 step:20825 [D loss: 0.742043, acc: 48.44%] [G loss: 1.872573]\n",
      "epoch:22 step:20826 [D loss: 0.675695, acc: 60.16%] [G loss: 1.857369]\n",
      "epoch:22 step:20827 [D loss: 0.694803, acc: 54.69%] [G loss: 1.772066]\n",
      "epoch:22 step:20828 [D loss: 0.679344, acc: 57.03%] [G loss: 1.779901]\n",
      "epoch:22 step:20829 [D loss: 0.693635, acc: 58.59%] [G loss: 1.812342]\n",
      "epoch:22 step:20830 [D loss: 0.603945, acc: 66.41%] [G loss: 1.928919]\n",
      "epoch:22 step:20831 [D loss: 0.675155, acc: 63.28%] [G loss: 2.083933]\n",
      "epoch:22 step:20832 [D loss: 0.613204, acc: 66.41%] [G loss: 2.133019]\n",
      "epoch:22 step:20833 [D loss: 0.582459, acc: 66.41%] [G loss: 2.331944]\n",
      "epoch:22 step:20834 [D loss: 0.670294, acc: 58.59%] [G loss: 1.717274]\n",
      "epoch:22 step:20835 [D loss: 0.684838, acc: 52.34%] [G loss: 2.090341]\n",
      "epoch:22 step:20836 [D loss: 0.639748, acc: 62.50%] [G loss: 1.882110]\n",
      "epoch:22 step:20837 [D loss: 0.715590, acc: 56.25%] [G loss: 1.925344]\n",
      "epoch:22 step:20838 [D loss: 0.677211, acc: 57.81%] [G loss: 1.770347]\n",
      "epoch:22 step:20839 [D loss: 0.653152, acc: 60.94%] [G loss: 1.752439]\n",
      "epoch:22 step:20840 [D loss: 0.632946, acc: 67.97%] [G loss: 1.875102]\n",
      "epoch:22 step:20841 [D loss: 0.650138, acc: 62.50%] [G loss: 1.818237]\n",
      "epoch:22 step:20842 [D loss: 0.637887, acc: 63.28%] [G loss: 1.795713]\n",
      "epoch:22 step:20843 [D loss: 0.602554, acc: 72.66%] [G loss: 2.148956]\n",
      "epoch:22 step:20844 [D loss: 0.623203, acc: 67.97%] [G loss: 2.058017]\n",
      "epoch:22 step:20845 [D loss: 0.590621, acc: 67.19%] [G loss: 2.136881]\n",
      "epoch:22 step:20846 [D loss: 0.551996, acc: 74.22%] [G loss: 2.193014]\n",
      "epoch:22 step:20847 [D loss: 0.642390, acc: 59.38%] [G loss: 1.912868]\n",
      "epoch:22 step:20848 [D loss: 0.690916, acc: 57.03%] [G loss: 1.735517]\n",
      "epoch:22 step:20849 [D loss: 0.666025, acc: 64.06%] [G loss: 1.768619]\n",
      "epoch:22 step:20850 [D loss: 0.660660, acc: 64.06%] [G loss: 1.773882]\n",
      "epoch:22 step:20851 [D loss: 0.700753, acc: 53.91%] [G loss: 1.765939]\n",
      "epoch:22 step:20852 [D loss: 0.662055, acc: 58.59%] [G loss: 1.835038]\n",
      "epoch:22 step:20853 [D loss: 0.627027, acc: 63.28%] [G loss: 1.855832]\n",
      "epoch:22 step:20854 [D loss: 0.664041, acc: 60.94%] [G loss: 1.827853]\n",
      "epoch:22 step:20855 [D loss: 0.630228, acc: 67.97%] [G loss: 1.840221]\n",
      "epoch:22 step:20856 [D loss: 0.631016, acc: 67.97%] [G loss: 2.031267]\n",
      "epoch:22 step:20857 [D loss: 0.620334, acc: 67.19%] [G loss: 2.007138]\n",
      "epoch:22 step:20858 [D loss: 0.599445, acc: 72.66%] [G loss: 1.902139]\n",
      "epoch:22 step:20859 [D loss: 0.626703, acc: 67.97%] [G loss: 1.953600]\n",
      "epoch:22 step:20860 [D loss: 0.697152, acc: 54.69%] [G loss: 1.909498]\n",
      "epoch:22 step:20861 [D loss: 0.627456, acc: 66.41%] [G loss: 1.774481]\n",
      "epoch:22 step:20862 [D loss: 0.609432, acc: 64.84%] [G loss: 1.811108]\n",
      "epoch:22 step:20863 [D loss: 0.651332, acc: 58.59%] [G loss: 1.785809]\n",
      "epoch:22 step:20864 [D loss: 0.677786, acc: 60.94%] [G loss: 1.821121]\n",
      "epoch:22 step:20865 [D loss: 0.659773, acc: 61.72%] [G loss: 1.787454]\n",
      "epoch:22 step:20866 [D loss: 0.683713, acc: 57.03%] [G loss: 1.827883]\n",
      "epoch:22 step:20867 [D loss: 0.684901, acc: 53.91%] [G loss: 1.830353]\n",
      "epoch:22 step:20868 [D loss: 0.665615, acc: 58.59%] [G loss: 1.816301]\n",
      "epoch:22 step:20869 [D loss: 0.648323, acc: 65.62%] [G loss: 1.858407]\n",
      "epoch:22 step:20870 [D loss: 0.625286, acc: 68.75%] [G loss: 1.896063]\n",
      "epoch:22 step:20871 [D loss: 0.632959, acc: 63.28%] [G loss: 1.912974]\n",
      "epoch:22 step:20872 [D loss: 0.658795, acc: 64.84%] [G loss: 1.899495]\n",
      "epoch:22 step:20873 [D loss: 0.626483, acc: 65.62%] [G loss: 1.892489]\n",
      "epoch:22 step:20874 [D loss: 0.659192, acc: 57.81%] [G loss: 1.834235]\n",
      "epoch:22 step:20875 [D loss: 0.606610, acc: 64.06%] [G loss: 2.076100]\n",
      "epoch:22 step:20876 [D loss: 0.637210, acc: 61.72%] [G loss: 1.985435]\n",
      "epoch:22 step:20877 [D loss: 0.671666, acc: 62.50%] [G loss: 2.011105]\n",
      "epoch:22 step:20878 [D loss: 0.585044, acc: 70.31%] [G loss: 2.193936]\n",
      "epoch:22 step:20879 [D loss: 0.659988, acc: 59.38%] [G loss: 1.948259]\n",
      "epoch:22 step:20880 [D loss: 0.628533, acc: 67.19%] [G loss: 1.644704]\n",
      "epoch:22 step:20881 [D loss: 0.653497, acc: 64.84%] [G loss: 1.841879]\n",
      "epoch:22 step:20882 [D loss: 0.676789, acc: 63.28%] [G loss: 1.922346]\n",
      "epoch:22 step:20883 [D loss: 0.641004, acc: 63.28%] [G loss: 1.867650]\n",
      "epoch:22 step:20884 [D loss: 0.667735, acc: 60.16%] [G loss: 1.843078]\n",
      "epoch:22 step:20885 [D loss: 0.616423, acc: 66.41%] [G loss: 1.929394]\n",
      "epoch:22 step:20886 [D loss: 0.642311, acc: 60.16%] [G loss: 1.906565]\n",
      "epoch:22 step:20887 [D loss: 0.618355, acc: 65.62%] [G loss: 1.920987]\n",
      "epoch:22 step:20888 [D loss: 0.648971, acc: 60.94%] [G loss: 2.078674]\n",
      "epoch:22 step:20889 [D loss: 0.568076, acc: 73.44%] [G loss: 1.963183]\n",
      "epoch:22 step:20890 [D loss: 0.569116, acc: 67.97%] [G loss: 2.207066]\n",
      "epoch:22 step:20891 [D loss: 0.674132, acc: 60.94%] [G loss: 1.938583]\n",
      "epoch:22 step:20892 [D loss: 0.668436, acc: 60.16%] [G loss: 1.733921]\n",
      "epoch:22 step:20893 [D loss: 0.640789, acc: 60.94%] [G loss: 1.800963]\n",
      "epoch:22 step:20894 [D loss: 0.673553, acc: 55.47%] [G loss: 1.924811]\n",
      "epoch:22 step:20895 [D loss: 0.665370, acc: 55.47%] [G loss: 1.933801]\n",
      "epoch:22 step:20896 [D loss: 0.620668, acc: 60.94%] [G loss: 1.879233]\n",
      "epoch:22 step:20897 [D loss: 0.590001, acc: 69.53%] [G loss: 1.898791]\n",
      "epoch:22 step:20898 [D loss: 0.652434, acc: 61.72%] [G loss: 1.922144]\n",
      "epoch:22 step:20899 [D loss: 0.678555, acc: 57.81%] [G loss: 1.737164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20900 [D loss: 0.633800, acc: 61.72%] [G loss: 2.025758]\n",
      "epoch:22 step:20901 [D loss: 0.684623, acc: 57.81%] [G loss: 1.855884]\n",
      "epoch:22 step:20902 [D loss: 0.709695, acc: 55.47%] [G loss: 1.789774]\n",
      "epoch:22 step:20903 [D loss: 0.642353, acc: 67.19%] [G loss: 2.016873]\n",
      "epoch:22 step:20904 [D loss: 0.611943, acc: 64.06%] [G loss: 1.887351]\n",
      "epoch:22 step:20905 [D loss: 0.656603, acc: 58.59%] [G loss: 1.832335]\n",
      "epoch:22 step:20906 [D loss: 0.627269, acc: 62.50%] [G loss: 1.826365]\n",
      "epoch:22 step:20907 [D loss: 0.627930, acc: 65.62%] [G loss: 1.934930]\n",
      "epoch:22 step:20908 [D loss: 0.633462, acc: 64.84%] [G loss: 1.928195]\n",
      "epoch:22 step:20909 [D loss: 0.622029, acc: 64.06%] [G loss: 1.817187]\n",
      "epoch:22 step:20910 [D loss: 0.688651, acc: 59.38%] [G loss: 1.996361]\n",
      "epoch:22 step:20911 [D loss: 0.616869, acc: 66.41%] [G loss: 1.882893]\n",
      "epoch:22 step:20912 [D loss: 0.601421, acc: 66.41%] [G loss: 2.058420]\n",
      "epoch:22 step:20913 [D loss: 0.622483, acc: 64.84%] [G loss: 2.121197]\n",
      "epoch:22 step:20914 [D loss: 0.599535, acc: 70.31%] [G loss: 1.862723]\n",
      "epoch:22 step:20915 [D loss: 0.723114, acc: 53.12%] [G loss: 1.822153]\n",
      "epoch:22 step:20916 [D loss: 0.626440, acc: 66.41%] [G loss: 1.826852]\n",
      "epoch:22 step:20917 [D loss: 0.611232, acc: 65.62%] [G loss: 1.987199]\n",
      "epoch:22 step:20918 [D loss: 0.672438, acc: 57.81%] [G loss: 1.797245]\n",
      "epoch:22 step:20919 [D loss: 0.650397, acc: 63.28%] [G loss: 1.893684]\n",
      "epoch:22 step:20920 [D loss: 0.666607, acc: 60.16%] [G loss: 1.748079]\n",
      "epoch:22 step:20921 [D loss: 0.650951, acc: 63.28%] [G loss: 1.787663]\n",
      "epoch:22 step:20922 [D loss: 0.624602, acc: 69.53%] [G loss: 1.820886]\n",
      "epoch:22 step:20923 [D loss: 0.667945, acc: 57.03%] [G loss: 1.858341]\n",
      "epoch:22 step:20924 [D loss: 0.631637, acc: 66.41%] [G loss: 1.846365]\n",
      "epoch:22 step:20925 [D loss: 0.597900, acc: 68.75%] [G loss: 1.862794]\n",
      "epoch:22 step:20926 [D loss: 0.588511, acc: 72.66%] [G loss: 2.215385]\n",
      "epoch:22 step:20927 [D loss: 0.600781, acc: 66.41%] [G loss: 2.221500]\n",
      "epoch:22 step:20928 [D loss: 0.568532, acc: 71.88%] [G loss: 2.151861]\n",
      "epoch:22 step:20929 [D loss: 0.605585, acc: 67.19%] [G loss: 2.056298]\n",
      "epoch:22 step:20930 [D loss: 0.683490, acc: 58.59%] [G loss: 1.778623]\n",
      "epoch:22 step:20931 [D loss: 0.657161, acc: 62.50%] [G loss: 1.878620]\n",
      "epoch:22 step:20932 [D loss: 0.634702, acc: 64.84%] [G loss: 1.956950]\n",
      "epoch:22 step:20933 [D loss: 0.693556, acc: 57.03%] [G loss: 1.980383]\n",
      "epoch:22 step:20934 [D loss: 0.661517, acc: 60.94%] [G loss: 1.799439]\n",
      "epoch:22 step:20935 [D loss: 0.624891, acc: 60.16%] [G loss: 1.948033]\n",
      "epoch:22 step:20936 [D loss: 0.659979, acc: 61.72%] [G loss: 1.959631]\n",
      "epoch:22 step:20937 [D loss: 0.689831, acc: 56.25%] [G loss: 1.823673]\n",
      "epoch:22 step:20938 [D loss: 0.656269, acc: 63.28%] [G loss: 1.816462]\n",
      "epoch:22 step:20939 [D loss: 0.645673, acc: 62.50%] [G loss: 1.734830]\n",
      "epoch:22 step:20940 [D loss: 0.649049, acc: 61.72%] [G loss: 1.811013]\n",
      "epoch:22 step:20941 [D loss: 0.661587, acc: 58.59%] [G loss: 1.711607]\n",
      "epoch:22 step:20942 [D loss: 0.676724, acc: 55.47%] [G loss: 1.655583]\n",
      "epoch:22 step:20943 [D loss: 0.636064, acc: 60.94%] [G loss: 1.958713]\n",
      "epoch:22 step:20944 [D loss: 0.652271, acc: 60.16%] [G loss: 1.958570]\n",
      "epoch:22 step:20945 [D loss: 0.593517, acc: 72.66%] [G loss: 1.894093]\n",
      "epoch:22 step:20946 [D loss: 0.643667, acc: 63.28%] [G loss: 1.913898]\n",
      "epoch:22 step:20947 [D loss: 0.678514, acc: 59.38%] [G loss: 1.982261]\n",
      "epoch:22 step:20948 [D loss: 0.657838, acc: 56.25%] [G loss: 2.110574]\n",
      "epoch:22 step:20949 [D loss: 0.633950, acc: 64.06%] [G loss: 1.933660]\n",
      "epoch:22 step:20950 [D loss: 0.609003, acc: 70.31%] [G loss: 2.011042]\n",
      "epoch:22 step:20951 [D loss: 0.702837, acc: 53.91%] [G loss: 2.067571]\n",
      "epoch:22 step:20952 [D loss: 0.607907, acc: 64.06%] [G loss: 1.971333]\n",
      "epoch:22 step:20953 [D loss: 0.754365, acc: 52.34%] [G loss: 1.889200]\n",
      "epoch:22 step:20954 [D loss: 0.684543, acc: 60.94%] [G loss: 1.844780]\n",
      "epoch:22 step:20955 [D loss: 0.689757, acc: 57.03%] [G loss: 1.722318]\n",
      "epoch:22 step:20956 [D loss: 0.686603, acc: 54.69%] [G loss: 1.809180]\n",
      "epoch:22 step:20957 [D loss: 0.658112, acc: 62.50%] [G loss: 1.779550]\n",
      "epoch:22 step:20958 [D loss: 0.617685, acc: 60.94%] [G loss: 1.720774]\n",
      "epoch:22 step:20959 [D loss: 0.654845, acc: 64.06%] [G loss: 1.998404]\n",
      "epoch:22 step:20960 [D loss: 0.595343, acc: 64.84%] [G loss: 1.984026]\n",
      "epoch:22 step:20961 [D loss: 0.610856, acc: 65.62%] [G loss: 2.229854]\n",
      "epoch:22 step:20962 [D loss: 0.633526, acc: 66.41%] [G loss: 1.796425]\n",
      "epoch:22 step:20963 [D loss: 0.700374, acc: 57.03%] [G loss: 1.770642]\n",
      "epoch:22 step:20964 [D loss: 0.609315, acc: 64.84%] [G loss: 1.998594]\n",
      "epoch:22 step:20965 [D loss: 0.644990, acc: 63.28%] [G loss: 1.890204]\n",
      "epoch:22 step:20966 [D loss: 0.660403, acc: 60.16%] [G loss: 1.823849]\n",
      "epoch:22 step:20967 [D loss: 0.613872, acc: 63.28%] [G loss: 1.898448]\n",
      "epoch:22 step:20968 [D loss: 0.615921, acc: 62.50%] [G loss: 2.140093]\n",
      "epoch:22 step:20969 [D loss: 0.695447, acc: 53.91%] [G loss: 1.799278]\n",
      "epoch:22 step:20970 [D loss: 0.662384, acc: 61.72%] [G loss: 1.842138]\n",
      "epoch:22 step:20971 [D loss: 0.598220, acc: 68.75%] [G loss: 2.021676]\n",
      "epoch:22 step:20972 [D loss: 0.647942, acc: 62.50%] [G loss: 2.030295]\n",
      "epoch:22 step:20973 [D loss: 0.667131, acc: 61.72%] [G loss: 1.912040]\n",
      "epoch:22 step:20974 [D loss: 0.638651, acc: 64.06%] [G loss: 1.863095]\n",
      "epoch:22 step:20975 [D loss: 0.656500, acc: 60.16%] [G loss: 1.822152]\n",
      "epoch:22 step:20976 [D loss: 0.718842, acc: 53.12%] [G loss: 1.806167]\n",
      "epoch:22 step:20977 [D loss: 0.630780, acc: 66.41%] [G loss: 1.885454]\n",
      "epoch:22 step:20978 [D loss: 0.627287, acc: 64.84%] [G loss: 1.849517]\n",
      "epoch:22 step:20979 [D loss: 0.685324, acc: 55.47%] [G loss: 1.851361]\n",
      "epoch:22 step:20980 [D loss: 0.640561, acc: 61.72%] [G loss: 1.844324]\n",
      "epoch:22 step:20981 [D loss: 0.657342, acc: 62.50%] [G loss: 2.039324]\n",
      "epoch:22 step:20982 [D loss: 0.633937, acc: 68.75%] [G loss: 1.994674]\n",
      "epoch:22 step:20983 [D loss: 0.626253, acc: 60.94%] [G loss: 1.845754]\n",
      "epoch:22 step:20984 [D loss: 0.597595, acc: 69.53%] [G loss: 2.090179]\n",
      "epoch:22 step:20985 [D loss: 0.644453, acc: 59.38%] [G loss: 1.921278]\n",
      "epoch:22 step:20986 [D loss: 0.645112, acc: 62.50%] [G loss: 1.983068]\n",
      "epoch:22 step:20987 [D loss: 0.680362, acc: 57.81%] [G loss: 1.794125]\n",
      "epoch:22 step:20988 [D loss: 0.650577, acc: 67.19%] [G loss: 2.024020]\n",
      "epoch:22 step:20989 [D loss: 0.686445, acc: 56.25%] [G loss: 1.737263]\n",
      "epoch:22 step:20990 [D loss: 0.603683, acc: 65.62%] [G loss: 1.794729]\n",
      "epoch:22 step:20991 [D loss: 0.612375, acc: 65.62%] [G loss: 1.944491]\n",
      "epoch:22 step:20992 [D loss: 0.659376, acc: 59.38%] [G loss: 1.935328]\n",
      "epoch:22 step:20993 [D loss: 0.646491, acc: 63.28%] [G loss: 1.762303]\n",
      "epoch:22 step:20994 [D loss: 0.683759, acc: 60.16%] [G loss: 1.904230]\n",
      "epoch:22 step:20995 [D loss: 0.612078, acc: 66.41%] [G loss: 1.921307]\n",
      "epoch:22 step:20996 [D loss: 0.690072, acc: 60.94%] [G loss: 1.860382]\n",
      "epoch:22 step:20997 [D loss: 0.667815, acc: 64.06%] [G loss: 1.983019]\n",
      "epoch:22 step:20998 [D loss: 0.656850, acc: 59.38%] [G loss: 1.820549]\n",
      "epoch:22 step:20999 [D loss: 0.701477, acc: 58.59%] [G loss: 1.924752]\n",
      "epoch:22 step:21000 [D loss: 0.657042, acc: 61.72%] [G loss: 1.738909]\n",
      "##############\n",
      "[2.49164384 1.36826921 6.23989536 4.89680982 3.63161716 5.49538447\n",
      " 4.29203252 4.75316299 4.43617504 3.53778307]\n",
      "##########\n",
      "epoch:22 step:21001 [D loss: 0.631676, acc: 70.31%] [G loss: 1.735341]\n",
      "epoch:22 step:21002 [D loss: 0.632969, acc: 64.06%] [G loss: 1.831686]\n",
      "epoch:22 step:21003 [D loss: 0.641027, acc: 65.62%] [G loss: 1.879712]\n",
      "epoch:22 step:21004 [D loss: 0.662414, acc: 54.69%] [G loss: 1.843161]\n",
      "epoch:22 step:21005 [D loss: 0.658924, acc: 62.50%] [G loss: 1.826262]\n",
      "epoch:22 step:21006 [D loss: 0.629456, acc: 62.50%] [G loss: 1.959411]\n",
      "epoch:22 step:21007 [D loss: 0.603022, acc: 68.75%] [G loss: 1.872215]\n",
      "epoch:22 step:21008 [D loss: 0.635810, acc: 68.75%] [G loss: 1.870501]\n",
      "epoch:22 step:21009 [D loss: 0.613358, acc: 67.97%] [G loss: 1.822398]\n",
      "epoch:22 step:21010 [D loss: 0.658887, acc: 57.81%] [G loss: 1.857276]\n",
      "epoch:22 step:21011 [D loss: 0.702524, acc: 55.47%] [G loss: 1.816425]\n",
      "epoch:22 step:21012 [D loss: 0.648761, acc: 66.41%] [G loss: 1.828250]\n",
      "epoch:22 step:21013 [D loss: 0.684845, acc: 56.25%] [G loss: 1.849543]\n",
      "epoch:22 step:21014 [D loss: 0.633566, acc: 64.84%] [G loss: 1.902375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21015 [D loss: 0.631870, acc: 66.41%] [G loss: 1.934716]\n",
      "epoch:22 step:21016 [D loss: 0.673853, acc: 57.81%] [G loss: 1.869573]\n",
      "epoch:22 step:21017 [D loss: 0.624874, acc: 65.62%] [G loss: 1.996940]\n",
      "epoch:22 step:21018 [D loss: 0.670358, acc: 59.38%] [G loss: 2.069772]\n",
      "epoch:22 step:21019 [D loss: 0.600060, acc: 70.31%] [G loss: 2.039865]\n",
      "epoch:22 step:21020 [D loss: 0.609701, acc: 67.19%] [G loss: 2.013275]\n",
      "epoch:22 step:21021 [D loss: 0.641964, acc: 64.06%] [G loss: 1.786661]\n",
      "epoch:22 step:21022 [D loss: 0.640193, acc: 64.06%] [G loss: 1.916297]\n",
      "epoch:22 step:21023 [D loss: 0.632658, acc: 63.28%] [G loss: 2.002755]\n",
      "epoch:22 step:21024 [D loss: 0.652472, acc: 60.94%] [G loss: 1.859961]\n",
      "epoch:22 step:21025 [D loss: 0.722258, acc: 60.16%] [G loss: 1.904729]\n",
      "epoch:22 step:21026 [D loss: 0.643471, acc: 63.28%] [G loss: 1.865910]\n",
      "epoch:22 step:21027 [D loss: 0.685466, acc: 60.16%] [G loss: 2.078486]\n",
      "epoch:22 step:21028 [D loss: 0.650230, acc: 60.94%] [G loss: 1.975339]\n",
      "epoch:22 step:21029 [D loss: 0.670448, acc: 67.19%] [G loss: 1.967044]\n",
      "epoch:22 step:21030 [D loss: 0.624368, acc: 70.31%] [G loss: 1.898343]\n",
      "epoch:22 step:21031 [D loss: 0.679864, acc: 61.72%] [G loss: 1.920167]\n",
      "epoch:22 step:21032 [D loss: 0.650797, acc: 63.28%] [G loss: 1.746000]\n",
      "epoch:22 step:21033 [D loss: 0.705590, acc: 54.69%] [G loss: 1.919479]\n",
      "epoch:22 step:21034 [D loss: 0.657982, acc: 64.06%] [G loss: 1.740393]\n",
      "epoch:22 step:21035 [D loss: 0.631576, acc: 64.06%] [G loss: 1.784549]\n",
      "epoch:22 step:21036 [D loss: 0.656020, acc: 59.38%] [G loss: 1.841671]\n",
      "epoch:22 step:21037 [D loss: 0.685012, acc: 50.00%] [G loss: 1.724247]\n",
      "epoch:22 step:21038 [D loss: 0.642384, acc: 62.50%] [G loss: 1.773365]\n",
      "epoch:22 step:21039 [D loss: 0.683859, acc: 55.47%] [G loss: 1.726933]\n",
      "epoch:22 step:21040 [D loss: 0.646898, acc: 64.84%] [G loss: 1.862338]\n",
      "epoch:22 step:21041 [D loss: 0.659695, acc: 61.72%] [G loss: 1.878428]\n",
      "epoch:22 step:21042 [D loss: 0.607529, acc: 64.06%] [G loss: 1.946562]\n",
      "epoch:22 step:21043 [D loss: 0.649961, acc: 61.72%] [G loss: 2.102013]\n",
      "epoch:22 step:21044 [D loss: 0.605038, acc: 61.72%] [G loss: 2.170108]\n",
      "epoch:22 step:21045 [D loss: 0.614437, acc: 69.53%] [G loss: 1.886589]\n",
      "epoch:22 step:21046 [D loss: 0.692100, acc: 59.38%] [G loss: 1.775665]\n",
      "epoch:22 step:21047 [D loss: 0.651831, acc: 64.06%] [G loss: 1.765523]\n",
      "epoch:22 step:21048 [D loss: 0.614002, acc: 64.84%] [G loss: 1.938127]\n",
      "epoch:22 step:21049 [D loss: 0.589816, acc: 71.88%] [G loss: 1.909472]\n",
      "epoch:22 step:21050 [D loss: 0.636409, acc: 60.94%] [G loss: 1.960900]\n",
      "epoch:22 step:21051 [D loss: 0.676395, acc: 61.72%] [G loss: 1.661979]\n",
      "epoch:22 step:21052 [D loss: 0.670062, acc: 55.47%] [G loss: 1.764805]\n",
      "epoch:22 step:21053 [D loss: 0.664694, acc: 54.69%] [G loss: 1.690897]\n",
      "epoch:22 step:21054 [D loss: 0.626056, acc: 65.62%] [G loss: 1.850392]\n",
      "epoch:22 step:21055 [D loss: 0.659024, acc: 60.16%] [G loss: 1.751738]\n",
      "epoch:22 step:21056 [D loss: 0.651590, acc: 58.59%] [G loss: 1.876625]\n",
      "epoch:22 step:21057 [D loss: 0.637269, acc: 61.72%] [G loss: 1.834077]\n",
      "epoch:22 step:21058 [D loss: 0.605387, acc: 67.19%] [G loss: 1.882835]\n",
      "epoch:22 step:21059 [D loss: 0.688517, acc: 59.38%] [G loss: 1.830597]\n",
      "epoch:22 step:21060 [D loss: 0.643065, acc: 60.94%] [G loss: 1.878466]\n",
      "epoch:22 step:21061 [D loss: 0.623193, acc: 65.62%] [G loss: 1.825817]\n",
      "epoch:22 step:21062 [D loss: 0.667066, acc: 59.38%] [G loss: 1.869287]\n",
      "epoch:22 step:21063 [D loss: 0.637294, acc: 65.62%] [G loss: 1.860159]\n",
      "epoch:22 step:21064 [D loss: 0.658703, acc: 61.72%] [G loss: 1.884275]\n",
      "epoch:22 step:21065 [D loss: 0.617227, acc: 67.19%] [G loss: 1.885417]\n",
      "epoch:22 step:21066 [D loss: 0.666853, acc: 63.28%] [G loss: 1.852500]\n",
      "epoch:22 step:21067 [D loss: 0.660970, acc: 63.28%] [G loss: 1.986743]\n",
      "epoch:22 step:21068 [D loss: 0.631315, acc: 65.62%] [G loss: 1.939359]\n",
      "epoch:22 step:21069 [D loss: 0.603589, acc: 67.19%] [G loss: 1.886107]\n",
      "epoch:22 step:21070 [D loss: 0.635816, acc: 62.50%] [G loss: 1.914399]\n",
      "epoch:22 step:21071 [D loss: 0.615909, acc: 71.09%] [G loss: 2.088129]\n",
      "epoch:22 step:21072 [D loss: 0.691176, acc: 56.25%] [G loss: 1.713506]\n",
      "epoch:22 step:21073 [D loss: 0.689553, acc: 55.47%] [G loss: 1.734045]\n",
      "epoch:22 step:21074 [D loss: 0.659091, acc: 60.16%] [G loss: 1.930936]\n",
      "epoch:22 step:21075 [D loss: 0.694843, acc: 54.69%] [G loss: 1.675535]\n",
      "epoch:22 step:21076 [D loss: 0.660266, acc: 60.94%] [G loss: 1.845427]\n",
      "epoch:22 step:21077 [D loss: 0.634570, acc: 65.62%] [G loss: 1.898146]\n",
      "epoch:22 step:21078 [D loss: 0.674377, acc: 60.16%] [G loss: 1.791092]\n",
      "epoch:22 step:21079 [D loss: 0.655330, acc: 63.28%] [G loss: 1.742193]\n",
      "epoch:22 step:21080 [D loss: 0.657729, acc: 63.28%] [G loss: 1.837086]\n",
      "epoch:22 step:21081 [D loss: 0.624297, acc: 68.75%] [G loss: 1.901567]\n",
      "epoch:22 step:21082 [D loss: 0.679987, acc: 56.25%] [G loss: 2.048175]\n",
      "epoch:22 step:21083 [D loss: 0.600927, acc: 69.53%] [G loss: 1.907633]\n",
      "epoch:22 step:21084 [D loss: 0.679541, acc: 56.25%] [G loss: 1.922728]\n",
      "epoch:22 step:21085 [D loss: 0.560712, acc: 73.44%] [G loss: 2.110092]\n",
      "epoch:22 step:21086 [D loss: 0.625962, acc: 62.50%] [G loss: 2.111227]\n",
      "epoch:22 step:21087 [D loss: 0.670145, acc: 57.03%] [G loss: 1.714303]\n",
      "epoch:22 step:21088 [D loss: 0.663929, acc: 54.69%] [G loss: 1.836720]\n",
      "epoch:22 step:21089 [D loss: 0.625002, acc: 61.72%] [G loss: 1.771610]\n",
      "epoch:22 step:21090 [D loss: 0.591708, acc: 66.41%] [G loss: 1.979233]\n",
      "epoch:22 step:21091 [D loss: 0.685475, acc: 53.12%] [G loss: 1.761722]\n",
      "epoch:22 step:21092 [D loss: 0.678963, acc: 55.47%] [G loss: 1.639815]\n",
      "epoch:22 step:21093 [D loss: 0.663119, acc: 60.16%] [G loss: 1.903395]\n",
      "epoch:22 step:21094 [D loss: 0.634884, acc: 64.06%] [G loss: 1.987345]\n",
      "epoch:22 step:21095 [D loss: 0.595575, acc: 71.09%] [G loss: 2.103402]\n",
      "epoch:22 step:21096 [D loss: 0.701059, acc: 55.47%] [G loss: 1.833235]\n",
      "epoch:22 step:21097 [D loss: 0.677619, acc: 60.16%] [G loss: 1.796200]\n",
      "epoch:22 step:21098 [D loss: 0.626749, acc: 66.41%] [G loss: 1.895257]\n",
      "epoch:22 step:21099 [D loss: 0.649792, acc: 63.28%] [G loss: 1.898135]\n",
      "epoch:22 step:21100 [D loss: 0.649951, acc: 59.38%] [G loss: 1.793851]\n",
      "epoch:22 step:21101 [D loss: 0.629886, acc: 64.84%] [G loss: 1.892506]\n",
      "epoch:22 step:21102 [D loss: 0.636131, acc: 60.94%] [G loss: 1.955925]\n",
      "epoch:22 step:21103 [D loss: 0.696317, acc: 53.12%] [G loss: 1.978985]\n",
      "epoch:22 step:21104 [D loss: 0.673881, acc: 57.03%] [G loss: 1.999412]\n",
      "epoch:22 step:21105 [D loss: 0.632091, acc: 64.06%] [G loss: 1.872252]\n",
      "epoch:22 step:21106 [D loss: 0.683811, acc: 54.69%] [G loss: 1.861543]\n",
      "epoch:22 step:21107 [D loss: 0.615730, acc: 67.19%] [G loss: 1.762466]\n",
      "epoch:22 step:21108 [D loss: 0.602565, acc: 66.41%] [G loss: 1.823645]\n",
      "epoch:22 step:21109 [D loss: 0.612940, acc: 70.31%] [G loss: 2.113508]\n",
      "epoch:22 step:21110 [D loss: 0.645098, acc: 63.28%] [G loss: 1.878357]\n",
      "epoch:22 step:21111 [D loss: 0.674866, acc: 59.38%] [G loss: 1.971467]\n",
      "epoch:22 step:21112 [D loss: 0.626000, acc: 69.53%] [G loss: 2.059223]\n",
      "epoch:22 step:21113 [D loss: 0.582809, acc: 68.75%] [G loss: 2.173294]\n",
      "epoch:22 step:21114 [D loss: 0.697787, acc: 54.69%] [G loss: 1.786559]\n",
      "epoch:22 step:21115 [D loss: 0.694848, acc: 54.69%] [G loss: 1.745788]\n",
      "epoch:22 step:21116 [D loss: 0.669373, acc: 60.16%] [G loss: 1.806473]\n",
      "epoch:22 step:21117 [D loss: 0.655617, acc: 60.16%] [G loss: 1.736254]\n",
      "epoch:22 step:21118 [D loss: 0.648246, acc: 63.28%] [G loss: 1.780941]\n",
      "epoch:22 step:21119 [D loss: 0.656812, acc: 56.25%] [G loss: 1.782487]\n",
      "epoch:22 step:21120 [D loss: 0.658169, acc: 64.06%] [G loss: 1.798928]\n",
      "epoch:22 step:21121 [D loss: 0.712191, acc: 57.03%] [G loss: 1.701445]\n",
      "epoch:22 step:21122 [D loss: 0.638670, acc: 60.94%] [G loss: 1.973471]\n",
      "epoch:22 step:21123 [D loss: 0.686027, acc: 52.34%] [G loss: 1.815196]\n",
      "epoch:22 step:21124 [D loss: 0.660874, acc: 62.50%] [G loss: 1.853084]\n",
      "epoch:22 step:21125 [D loss: 0.670923, acc: 60.94%] [G loss: 1.836601]\n",
      "epoch:22 step:21126 [D loss: 0.636579, acc: 60.94%] [G loss: 1.899603]\n",
      "epoch:22 step:21127 [D loss: 0.636862, acc: 57.81%] [G loss: 1.786233]\n",
      "epoch:22 step:21128 [D loss: 0.674344, acc: 59.38%] [G loss: 1.856449]\n",
      "epoch:22 step:21129 [D loss: 0.617280, acc: 63.28%] [G loss: 1.754870]\n",
      "epoch:22 step:21130 [D loss: 0.682332, acc: 66.41%] [G loss: 1.893558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21131 [D loss: 0.590729, acc: 67.97%] [G loss: 1.849160]\n",
      "epoch:22 step:21132 [D loss: 0.657037, acc: 56.25%] [G loss: 1.697970]\n",
      "epoch:22 step:21133 [D loss: 0.625147, acc: 67.19%] [G loss: 1.943877]\n",
      "epoch:22 step:21134 [D loss: 0.599089, acc: 69.53%] [G loss: 1.907810]\n",
      "epoch:22 step:21135 [D loss: 0.653759, acc: 67.19%] [G loss: 1.934433]\n",
      "epoch:22 step:21136 [D loss: 0.603817, acc: 67.97%] [G loss: 2.055571]\n",
      "epoch:22 step:21137 [D loss: 0.599351, acc: 67.19%] [G loss: 2.045045]\n",
      "epoch:22 step:21138 [D loss: 0.669387, acc: 63.28%] [G loss: 1.991504]\n",
      "epoch:22 step:21139 [D loss: 0.663993, acc: 62.50%] [G loss: 1.906684]\n",
      "epoch:22 step:21140 [D loss: 0.632283, acc: 64.06%] [G loss: 1.794952]\n",
      "epoch:22 step:21141 [D loss: 0.655891, acc: 61.72%] [G loss: 1.989574]\n",
      "epoch:22 step:21142 [D loss: 0.686819, acc: 55.47%] [G loss: 1.667452]\n",
      "epoch:22 step:21143 [D loss: 0.670089, acc: 60.94%] [G loss: 1.738276]\n",
      "epoch:22 step:21144 [D loss: 0.649885, acc: 64.84%] [G loss: 1.844713]\n",
      "epoch:22 step:21145 [D loss: 0.687976, acc: 59.38%] [G loss: 1.782017]\n",
      "epoch:22 step:21146 [D loss: 0.651110, acc: 61.72%] [G loss: 1.898491]\n",
      "epoch:22 step:21147 [D loss: 0.656326, acc: 63.28%] [G loss: 1.790690]\n",
      "epoch:22 step:21148 [D loss: 0.643957, acc: 63.28%] [G loss: 1.897392]\n",
      "epoch:22 step:21149 [D loss: 0.662038, acc: 58.59%] [G loss: 1.790062]\n",
      "epoch:22 step:21150 [D loss: 0.629211, acc: 66.41%] [G loss: 1.857737]\n",
      "epoch:22 step:21151 [D loss: 0.659400, acc: 55.47%] [G loss: 1.871510]\n",
      "epoch:22 step:21152 [D loss: 0.636851, acc: 63.28%] [G loss: 1.799679]\n",
      "epoch:22 step:21153 [D loss: 0.639540, acc: 59.38%] [G loss: 1.940650]\n",
      "epoch:22 step:21154 [D loss: 0.658479, acc: 59.38%] [G loss: 1.775790]\n",
      "epoch:22 step:21155 [D loss: 0.624369, acc: 63.28%] [G loss: 1.810942]\n",
      "epoch:22 step:21156 [D loss: 0.750108, acc: 50.00%] [G loss: 1.883854]\n",
      "epoch:22 step:21157 [D loss: 0.639725, acc: 61.72%] [G loss: 1.849852]\n",
      "epoch:22 step:21158 [D loss: 0.642972, acc: 61.72%] [G loss: 1.897148]\n",
      "epoch:22 step:21159 [D loss: 0.651267, acc: 60.94%] [G loss: 1.925046]\n",
      "epoch:22 step:21160 [D loss: 0.651893, acc: 60.16%] [G loss: 1.732578]\n",
      "epoch:22 step:21161 [D loss: 0.602367, acc: 69.53%] [G loss: 1.899465]\n",
      "epoch:22 step:21162 [D loss: 0.689432, acc: 60.94%] [G loss: 1.949102]\n",
      "epoch:22 step:21163 [D loss: 0.612984, acc: 68.75%] [G loss: 2.000007]\n",
      "epoch:22 step:21164 [D loss: 0.638563, acc: 65.62%] [G loss: 1.994735]\n",
      "epoch:22 step:21165 [D loss: 0.595648, acc: 71.09%] [G loss: 1.934085]\n",
      "epoch:22 step:21166 [D loss: 0.616075, acc: 67.19%] [G loss: 1.981995]\n",
      "epoch:22 step:21167 [D loss: 0.668231, acc: 60.94%] [G loss: 1.784070]\n",
      "epoch:22 step:21168 [D loss: 0.605089, acc: 71.09%] [G loss: 2.159098]\n",
      "epoch:22 step:21169 [D loss: 0.697722, acc: 61.72%] [G loss: 1.960449]\n",
      "epoch:22 step:21170 [D loss: 0.634122, acc: 64.84%] [G loss: 2.014903]\n",
      "epoch:22 step:21171 [D loss: 0.684320, acc: 57.81%] [G loss: 1.956227]\n",
      "epoch:22 step:21172 [D loss: 0.579171, acc: 67.97%] [G loss: 2.007495]\n",
      "epoch:22 step:21173 [D loss: 0.665894, acc: 56.25%] [G loss: 1.866595]\n",
      "epoch:22 step:21174 [D loss: 0.708344, acc: 57.03%] [G loss: 1.872711]\n",
      "epoch:22 step:21175 [D loss: 0.628289, acc: 64.06%] [G loss: 1.855031]\n",
      "epoch:22 step:21176 [D loss: 0.656443, acc: 64.06%] [G loss: 1.986052]\n",
      "epoch:22 step:21177 [D loss: 0.673592, acc: 62.50%] [G loss: 1.831109]\n",
      "epoch:22 step:21178 [D loss: 0.612110, acc: 67.19%] [G loss: 1.941136]\n",
      "epoch:22 step:21179 [D loss: 0.669011, acc: 57.03%] [G loss: 1.875044]\n",
      "epoch:22 step:21180 [D loss: 0.721448, acc: 53.91%] [G loss: 1.737971]\n",
      "epoch:22 step:21181 [D loss: 0.662067, acc: 57.03%] [G loss: 1.896396]\n",
      "epoch:22 step:21182 [D loss: 0.617762, acc: 64.84%] [G loss: 1.883580]\n",
      "epoch:22 step:21183 [D loss: 0.688536, acc: 55.47%] [G loss: 1.847078]\n",
      "epoch:22 step:21184 [D loss: 0.592250, acc: 66.41%] [G loss: 1.952794]\n",
      "epoch:22 step:21185 [D loss: 0.578552, acc: 74.22%] [G loss: 1.951313]\n",
      "epoch:22 step:21186 [D loss: 0.678384, acc: 55.47%] [G loss: 1.961109]\n",
      "epoch:22 step:21187 [D loss: 0.698177, acc: 58.59%] [G loss: 1.735451]\n",
      "epoch:22 step:21188 [D loss: 0.623551, acc: 62.50%] [G loss: 1.953228]\n",
      "epoch:22 step:21189 [D loss: 0.651595, acc: 62.50%] [G loss: 1.819052]\n",
      "epoch:22 step:21190 [D loss: 0.645224, acc: 60.16%] [G loss: 1.793594]\n",
      "epoch:22 step:21191 [D loss: 0.703246, acc: 52.34%] [G loss: 1.770207]\n",
      "epoch:22 step:21192 [D loss: 0.599785, acc: 62.50%] [G loss: 1.939852]\n",
      "epoch:22 step:21193 [D loss: 0.619113, acc: 64.06%] [G loss: 1.722593]\n",
      "epoch:22 step:21194 [D loss: 0.661343, acc: 56.25%] [G loss: 1.854428]\n",
      "epoch:22 step:21195 [D loss: 0.666865, acc: 60.16%] [G loss: 1.876995]\n",
      "epoch:22 step:21196 [D loss: 0.639399, acc: 61.72%] [G loss: 1.942081]\n",
      "epoch:22 step:21197 [D loss: 0.617049, acc: 63.28%] [G loss: 1.899320]\n",
      "epoch:22 step:21198 [D loss: 0.683341, acc: 62.50%] [G loss: 1.868501]\n",
      "epoch:22 step:21199 [D loss: 0.629995, acc: 64.84%] [G loss: 1.900483]\n",
      "epoch:22 step:21200 [D loss: 0.635325, acc: 66.41%] [G loss: 1.905575]\n",
      "##############\n",
      "[2.52418735 1.51715059 6.12983874 4.69882377 3.63570864 5.62346883\n",
      " 4.47143575 4.57851158 4.58744753 3.6640397 ]\n",
      "##########\n",
      "epoch:22 step:21201 [D loss: 0.627071, acc: 60.94%] [G loss: 2.016079]\n",
      "epoch:22 step:21202 [D loss: 0.614335, acc: 62.50%] [G loss: 2.028603]\n",
      "epoch:22 step:21203 [D loss: 0.675238, acc: 55.47%] [G loss: 1.871904]\n",
      "epoch:22 step:21204 [D loss: 0.642216, acc: 62.50%] [G loss: 1.767393]\n",
      "epoch:22 step:21205 [D loss: 0.633079, acc: 59.38%] [G loss: 1.885635]\n",
      "epoch:22 step:21206 [D loss: 0.656713, acc: 62.50%] [G loss: 1.861002]\n",
      "epoch:22 step:21207 [D loss: 0.632190, acc: 62.50%] [G loss: 1.814458]\n",
      "epoch:22 step:21208 [D loss: 0.618602, acc: 64.84%] [G loss: 1.835071]\n",
      "epoch:22 step:21209 [D loss: 0.606014, acc: 70.31%] [G loss: 1.951681]\n",
      "epoch:22 step:21210 [D loss: 0.666804, acc: 54.69%] [G loss: 1.745657]\n",
      "epoch:22 step:21211 [D loss: 0.653096, acc: 60.94%] [G loss: 1.901462]\n",
      "epoch:22 step:21212 [D loss: 0.679543, acc: 59.38%] [G loss: 1.937344]\n",
      "epoch:22 step:21213 [D loss: 0.656397, acc: 59.38%] [G loss: 1.861676]\n",
      "epoch:22 step:21214 [D loss: 0.668665, acc: 60.94%] [G loss: 1.854735]\n",
      "epoch:22 step:21215 [D loss: 0.650961, acc: 64.06%] [G loss: 1.833835]\n",
      "epoch:22 step:21216 [D loss: 0.640360, acc: 64.06%] [G loss: 1.974151]\n",
      "epoch:22 step:21217 [D loss: 0.647545, acc: 57.03%] [G loss: 2.019797]\n",
      "epoch:22 step:21218 [D loss: 0.688552, acc: 61.72%] [G loss: 1.965731]\n",
      "epoch:22 step:21219 [D loss: 0.596713, acc: 68.75%] [G loss: 2.212827]\n",
      "epoch:22 step:21220 [D loss: 0.656062, acc: 62.50%] [G loss: 1.799794]\n",
      "epoch:22 step:21221 [D loss: 0.654957, acc: 61.72%] [G loss: 1.950728]\n",
      "epoch:22 step:21222 [D loss: 0.625995, acc: 63.28%] [G loss: 1.907193]\n",
      "epoch:22 step:21223 [D loss: 0.642565, acc: 64.84%] [G loss: 1.989221]\n",
      "epoch:22 step:21224 [D loss: 0.653166, acc: 61.72%] [G loss: 1.847536]\n",
      "epoch:22 step:21225 [D loss: 0.628017, acc: 62.50%] [G loss: 1.821694]\n",
      "epoch:22 step:21226 [D loss: 0.629679, acc: 72.66%] [G loss: 1.910876]\n",
      "epoch:22 step:21227 [D loss: 0.671118, acc: 57.81%] [G loss: 1.897360]\n",
      "epoch:22 step:21228 [D loss: 0.642280, acc: 72.66%] [G loss: 1.825943]\n",
      "epoch:22 step:21229 [D loss: 0.626261, acc: 59.38%] [G loss: 1.801128]\n",
      "epoch:22 step:21230 [D loss: 0.624799, acc: 65.62%] [G loss: 1.887227]\n",
      "epoch:22 step:21231 [D loss: 0.666063, acc: 56.25%] [G loss: 1.869151]\n",
      "epoch:22 step:21232 [D loss: 0.604701, acc: 68.75%] [G loss: 1.871966]\n",
      "epoch:22 step:21233 [D loss: 0.593894, acc: 70.31%] [G loss: 1.947541]\n",
      "epoch:22 step:21234 [D loss: 0.716280, acc: 49.22%] [G loss: 1.769772]\n",
      "epoch:22 step:21235 [D loss: 0.650422, acc: 57.81%] [G loss: 1.928742]\n",
      "epoch:22 step:21236 [D loss: 0.674644, acc: 57.81%] [G loss: 1.951519]\n",
      "epoch:22 step:21237 [D loss: 0.693318, acc: 60.94%] [G loss: 1.959455]\n",
      "epoch:22 step:21238 [D loss: 0.614252, acc: 70.31%] [G loss: 2.008690]\n",
      "epoch:22 step:21239 [D loss: 0.696758, acc: 53.12%] [G loss: 1.769777]\n",
      "epoch:22 step:21240 [D loss: 0.631432, acc: 68.75%] [G loss: 2.016159]\n",
      "epoch:22 step:21241 [D loss: 0.627007, acc: 67.19%] [G loss: 1.892154]\n",
      "epoch:22 step:21242 [D loss: 0.750043, acc: 51.56%] [G loss: 1.790691]\n",
      "epoch:22 step:21243 [D loss: 0.621404, acc: 69.53%] [G loss: 1.864078]\n",
      "epoch:22 step:21244 [D loss: 0.585908, acc: 69.53%] [G loss: 1.870363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21245 [D loss: 0.608391, acc: 66.41%] [G loss: 2.016088]\n",
      "epoch:22 step:21246 [D loss: 0.595111, acc: 67.97%] [G loss: 1.869687]\n",
      "epoch:22 step:21247 [D loss: 0.641132, acc: 62.50%] [G loss: 1.969605]\n",
      "epoch:22 step:21248 [D loss: 0.621141, acc: 63.28%] [G loss: 1.916238]\n",
      "epoch:22 step:21249 [D loss: 0.596693, acc: 71.88%] [G loss: 1.962168]\n",
      "epoch:22 step:21250 [D loss: 0.647710, acc: 58.59%] [G loss: 1.973197]\n",
      "epoch:22 step:21251 [D loss: 0.634215, acc: 61.72%] [G loss: 1.982797]\n",
      "epoch:22 step:21252 [D loss: 0.595032, acc: 67.19%] [G loss: 1.988502]\n",
      "epoch:22 step:21253 [D loss: 0.601963, acc: 66.41%] [G loss: 1.892117]\n",
      "epoch:22 step:21254 [D loss: 0.687475, acc: 57.03%] [G loss: 1.792359]\n",
      "epoch:22 step:21255 [D loss: 0.656677, acc: 62.50%] [G loss: 1.968344]\n",
      "epoch:22 step:21256 [D loss: 0.664336, acc: 62.50%] [G loss: 2.058498]\n",
      "epoch:22 step:21257 [D loss: 0.729634, acc: 57.03%] [G loss: 1.859915]\n",
      "epoch:22 step:21258 [D loss: 0.624634, acc: 66.41%] [G loss: 1.848315]\n",
      "epoch:22 step:21259 [D loss: 0.583159, acc: 68.75%] [G loss: 1.962961]\n",
      "epoch:22 step:21260 [D loss: 0.611780, acc: 65.62%] [G loss: 2.077569]\n",
      "epoch:22 step:21261 [D loss: 0.623906, acc: 66.41%] [G loss: 2.081255]\n",
      "epoch:22 step:21262 [D loss: 0.596619, acc: 64.84%] [G loss: 2.345074]\n",
      "epoch:22 step:21263 [D loss: 0.609044, acc: 67.19%] [G loss: 2.148722]\n",
      "epoch:22 step:21264 [D loss: 0.662006, acc: 58.59%] [G loss: 2.035749]\n",
      "epoch:22 step:21265 [D loss: 0.693035, acc: 62.50%] [G loss: 1.917995]\n",
      "epoch:22 step:21266 [D loss: 0.607159, acc: 68.75%] [G loss: 2.110595]\n",
      "epoch:22 step:21267 [D loss: 0.637596, acc: 62.50%] [G loss: 1.986494]\n",
      "epoch:22 step:21268 [D loss: 0.637031, acc: 64.84%] [G loss: 2.065753]\n",
      "epoch:22 step:21269 [D loss: 0.709977, acc: 53.91%] [G loss: 1.951528]\n",
      "epoch:22 step:21270 [D loss: 0.702401, acc: 57.03%] [G loss: 1.801973]\n",
      "epoch:22 step:21271 [D loss: 0.730385, acc: 50.78%] [G loss: 1.818586]\n",
      "epoch:22 step:21272 [D loss: 0.700547, acc: 53.12%] [G loss: 1.716048]\n",
      "epoch:22 step:21273 [D loss: 0.655844, acc: 62.50%] [G loss: 1.731594]\n",
      "epoch:22 step:21274 [D loss: 0.665850, acc: 56.25%] [G loss: 1.772013]\n",
      "epoch:22 step:21275 [D loss: 0.672330, acc: 57.81%] [G loss: 1.837030]\n",
      "epoch:22 step:21276 [D loss: 0.631827, acc: 64.84%] [G loss: 1.823348]\n",
      "epoch:22 step:21277 [D loss: 0.636761, acc: 66.41%] [G loss: 1.888323]\n",
      "epoch:22 step:21278 [D loss: 0.684885, acc: 56.25%] [G loss: 1.864087]\n",
      "epoch:22 step:21279 [D loss: 0.662415, acc: 56.25%] [G loss: 1.869057]\n",
      "epoch:22 step:21280 [D loss: 0.682857, acc: 57.81%] [G loss: 1.794658]\n",
      "epoch:22 step:21281 [D loss: 0.699432, acc: 56.25%] [G loss: 1.713986]\n",
      "epoch:22 step:21282 [D loss: 0.638382, acc: 64.06%] [G loss: 1.825333]\n",
      "epoch:22 step:21283 [D loss: 0.654545, acc: 61.72%] [G loss: 1.797171]\n",
      "epoch:22 step:21284 [D loss: 0.625961, acc: 65.62%] [G loss: 1.802106]\n",
      "epoch:22 step:21285 [D loss: 0.713983, acc: 52.34%] [G loss: 1.886588]\n",
      "epoch:22 step:21286 [D loss: 0.675880, acc: 57.81%] [G loss: 1.807662]\n",
      "epoch:22 step:21287 [D loss: 0.666783, acc: 62.50%] [G loss: 1.852072]\n",
      "epoch:22 step:21288 [D loss: 0.541573, acc: 76.56%] [G loss: 1.930637]\n",
      "epoch:22 step:21289 [D loss: 0.678951, acc: 54.69%] [G loss: 1.787923]\n",
      "epoch:22 step:21290 [D loss: 0.700271, acc: 53.91%] [G loss: 1.720459]\n",
      "epoch:22 step:21291 [D loss: 0.633316, acc: 64.84%] [G loss: 1.777441]\n",
      "epoch:22 step:21292 [D loss: 0.686369, acc: 55.47%] [G loss: 1.689444]\n",
      "epoch:22 step:21293 [D loss: 0.622660, acc: 64.06%] [G loss: 1.848677]\n",
      "epoch:22 step:21294 [D loss: 0.644318, acc: 59.38%] [G loss: 1.920879]\n",
      "epoch:22 step:21295 [D loss: 0.588701, acc: 71.09%] [G loss: 2.008184]\n",
      "epoch:22 step:21296 [D loss: 0.666646, acc: 57.81%] [G loss: 1.899306]\n",
      "epoch:22 step:21297 [D loss: 0.663490, acc: 58.59%] [G loss: 1.798102]\n",
      "epoch:22 step:21298 [D loss: 0.711789, acc: 52.34%] [G loss: 1.864383]\n",
      "epoch:22 step:21299 [D loss: 0.640561, acc: 66.41%] [G loss: 1.958239]\n",
      "epoch:22 step:21300 [D loss: 0.655059, acc: 66.41%] [G loss: 1.864691]\n",
      "epoch:22 step:21301 [D loss: 0.631067, acc: 62.50%] [G loss: 1.948455]\n",
      "epoch:22 step:21302 [D loss: 0.622357, acc: 66.41%] [G loss: 1.742059]\n",
      "epoch:22 step:21303 [D loss: 0.623818, acc: 62.50%] [G loss: 1.876038]\n",
      "epoch:22 step:21304 [D loss: 0.638155, acc: 62.50%] [G loss: 2.025757]\n",
      "epoch:22 step:21305 [D loss: 0.698955, acc: 58.59%] [G loss: 1.949927]\n",
      "epoch:22 step:21306 [D loss: 0.652888, acc: 64.06%] [G loss: 1.853240]\n",
      "epoch:22 step:21307 [D loss: 0.681164, acc: 57.81%] [G loss: 2.009529]\n",
      "epoch:22 step:21308 [D loss: 0.561059, acc: 75.00%] [G loss: 2.018956]\n",
      "epoch:22 step:21309 [D loss: 0.609193, acc: 63.28%] [G loss: 1.966510]\n",
      "epoch:22 step:21310 [D loss: 0.680698, acc: 57.03%] [G loss: 1.845451]\n",
      "epoch:22 step:21311 [D loss: 0.621746, acc: 62.50%] [G loss: 1.898511]\n",
      "epoch:22 step:21312 [D loss: 0.639651, acc: 59.38%] [G loss: 1.870500]\n",
      "epoch:22 step:21313 [D loss: 0.640817, acc: 60.94%] [G loss: 1.787492]\n",
      "epoch:22 step:21314 [D loss: 0.745221, acc: 48.44%] [G loss: 1.780131]\n",
      "epoch:22 step:21315 [D loss: 0.603338, acc: 67.19%] [G loss: 1.948546]\n",
      "epoch:22 step:21316 [D loss: 0.687796, acc: 50.78%] [G loss: 1.766109]\n",
      "epoch:22 step:21317 [D loss: 0.674156, acc: 59.38%] [G loss: 1.832747]\n",
      "epoch:22 step:21318 [D loss: 0.639022, acc: 62.50%] [G loss: 1.810261]\n",
      "epoch:22 step:21319 [D loss: 0.662279, acc: 52.34%] [G loss: 1.812760]\n",
      "epoch:22 step:21320 [D loss: 0.654570, acc: 64.06%] [G loss: 1.893394]\n",
      "epoch:22 step:21321 [D loss: 0.635918, acc: 57.81%] [G loss: 1.803158]\n",
      "epoch:22 step:21322 [D loss: 0.586233, acc: 71.88%] [G loss: 1.876594]\n",
      "epoch:22 step:21323 [D loss: 0.640386, acc: 60.94%] [G loss: 1.837765]\n",
      "epoch:22 step:21324 [D loss: 0.644394, acc: 57.03%] [G loss: 1.891145]\n",
      "epoch:22 step:21325 [D loss: 0.653894, acc: 63.28%] [G loss: 1.894172]\n",
      "epoch:22 step:21326 [D loss: 0.652169, acc: 64.06%] [G loss: 2.075206]\n",
      "epoch:22 step:21327 [D loss: 0.633698, acc: 60.94%] [G loss: 1.855660]\n",
      "epoch:22 step:21328 [D loss: 0.667144, acc: 58.59%] [G loss: 1.879538]\n",
      "epoch:22 step:21329 [D loss: 0.671358, acc: 56.25%] [G loss: 1.845853]\n",
      "epoch:22 step:21330 [D loss: 0.668830, acc: 57.81%] [G loss: 1.725832]\n",
      "epoch:22 step:21331 [D loss: 0.654891, acc: 57.81%] [G loss: 1.928724]\n",
      "epoch:22 step:21332 [D loss: 0.649998, acc: 60.94%] [G loss: 1.818306]\n",
      "epoch:22 step:21333 [D loss: 0.647260, acc: 65.62%] [G loss: 1.926226]\n",
      "epoch:22 step:21334 [D loss: 0.684395, acc: 59.38%] [G loss: 1.909313]\n",
      "epoch:22 step:21335 [D loss: 0.646670, acc: 61.72%] [G loss: 1.951186]\n",
      "epoch:22 step:21336 [D loss: 0.673880, acc: 60.16%] [G loss: 1.719480]\n",
      "epoch:22 step:21337 [D loss: 0.624691, acc: 63.28%] [G loss: 1.857100]\n",
      "epoch:22 step:21338 [D loss: 0.674330, acc: 62.50%] [G loss: 1.838265]\n",
      "epoch:22 step:21339 [D loss: 0.629991, acc: 64.84%] [G loss: 1.951548]\n",
      "epoch:22 step:21340 [D loss: 0.610871, acc: 61.72%] [G loss: 2.056591]\n",
      "epoch:22 step:21341 [D loss: 0.655163, acc: 57.03%] [G loss: 1.906500]\n",
      "epoch:22 step:21342 [D loss: 0.660732, acc: 57.81%] [G loss: 1.944979]\n",
      "epoch:22 step:21343 [D loss: 0.636205, acc: 64.84%] [G loss: 1.905674]\n",
      "epoch:22 step:21344 [D loss: 0.630259, acc: 63.28%] [G loss: 1.618279]\n",
      "epoch:22 step:21345 [D loss: 0.652462, acc: 62.50%] [G loss: 1.942764]\n",
      "epoch:22 step:21346 [D loss: 0.603997, acc: 65.62%] [G loss: 1.950459]\n",
      "epoch:22 step:21347 [D loss: 0.623843, acc: 66.41%] [G loss: 2.075197]\n",
      "epoch:22 step:21348 [D loss: 0.651274, acc: 57.81%] [G loss: 1.931512]\n",
      "epoch:22 step:21349 [D loss: 0.645440, acc: 62.50%] [G loss: 1.934321]\n",
      "epoch:22 step:21350 [D loss: 0.636490, acc: 63.28%] [G loss: 1.891018]\n",
      "epoch:22 step:21351 [D loss: 0.624842, acc: 67.97%] [G loss: 1.861933]\n",
      "epoch:22 step:21352 [D loss: 0.666950, acc: 62.50%] [G loss: 1.844904]\n",
      "epoch:22 step:21353 [D loss: 0.645285, acc: 67.19%] [G loss: 1.881936]\n",
      "epoch:22 step:21354 [D loss: 0.645613, acc: 64.84%] [G loss: 1.927103]\n",
      "epoch:22 step:21355 [D loss: 0.642343, acc: 62.50%] [G loss: 1.789630]\n",
      "epoch:22 step:21356 [D loss: 0.633589, acc: 60.16%] [G loss: 1.922595]\n",
      "epoch:22 step:21357 [D loss: 0.605744, acc: 71.09%] [G loss: 1.761273]\n",
      "epoch:22 step:21358 [D loss: 0.642234, acc: 64.84%] [G loss: 1.996100]\n",
      "epoch:22 step:21359 [D loss: 0.686893, acc: 57.81%] [G loss: 1.877213]\n",
      "epoch:22 step:21360 [D loss: 0.607235, acc: 71.88%] [G loss: 1.903899]\n",
      "epoch:22 step:21361 [D loss: 0.627282, acc: 62.50%] [G loss: 2.009320]\n",
      "epoch:22 step:21362 [D loss: 0.650237, acc: 63.28%] [G loss: 1.834314]\n",
      "epoch:22 step:21363 [D loss: 0.676422, acc: 58.59%] [G loss: 1.770315]\n",
      "epoch:22 step:21364 [D loss: 0.633630, acc: 64.06%] [G loss: 1.931291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21365 [D loss: 0.628822, acc: 60.94%] [G loss: 1.747222]\n",
      "epoch:22 step:21366 [D loss: 0.661898, acc: 64.06%] [G loss: 1.782214]\n",
      "epoch:22 step:21367 [D loss: 0.711621, acc: 50.00%] [G loss: 1.824708]\n",
      "epoch:22 step:21368 [D loss: 0.634086, acc: 68.75%] [G loss: 1.803120]\n",
      "epoch:22 step:21369 [D loss: 0.677134, acc: 59.38%] [G loss: 1.790236]\n",
      "epoch:22 step:21370 [D loss: 0.602697, acc: 68.75%] [G loss: 1.834717]\n",
      "epoch:22 step:21371 [D loss: 0.620677, acc: 67.97%] [G loss: 1.781267]\n",
      "epoch:22 step:21372 [D loss: 0.680740, acc: 60.16%] [G loss: 1.703843]\n",
      "epoch:22 step:21373 [D loss: 0.690998, acc: 57.03%] [G loss: 1.785390]\n",
      "epoch:22 step:21374 [D loss: 0.707601, acc: 50.78%] [G loss: 1.733018]\n",
      "epoch:22 step:21375 [D loss: 0.606362, acc: 65.62%] [G loss: 1.802676]\n",
      "epoch:22 step:21376 [D loss: 0.635474, acc: 58.59%] [G loss: 1.707500]\n",
      "epoch:22 step:21377 [D loss: 0.635425, acc: 63.28%] [G loss: 1.928650]\n",
      "epoch:22 step:21378 [D loss: 0.679497, acc: 57.03%] [G loss: 1.860317]\n",
      "epoch:22 step:21379 [D loss: 0.721500, acc: 51.56%] [G loss: 1.736651]\n",
      "epoch:22 step:21380 [D loss: 0.634375, acc: 66.41%] [G loss: 1.795274]\n",
      "epoch:22 step:21381 [D loss: 0.635525, acc: 60.16%] [G loss: 1.759963]\n",
      "epoch:22 step:21382 [D loss: 0.627786, acc: 60.16%] [G loss: 1.876606]\n",
      "epoch:22 step:21383 [D loss: 0.665691, acc: 57.81%] [G loss: 1.974565]\n",
      "epoch:22 step:21384 [D loss: 0.658377, acc: 62.50%] [G loss: 1.876335]\n",
      "epoch:22 step:21385 [D loss: 0.670664, acc: 62.50%] [G loss: 1.767674]\n",
      "epoch:22 step:21386 [D loss: 0.653293, acc: 63.28%] [G loss: 1.764401]\n",
      "epoch:22 step:21387 [D loss: 0.628964, acc: 61.72%] [G loss: 1.828171]\n",
      "epoch:22 step:21388 [D loss: 0.663280, acc: 60.94%] [G loss: 2.114760]\n",
      "epoch:22 step:21389 [D loss: 0.640790, acc: 64.84%] [G loss: 2.057945]\n",
      "epoch:22 step:21390 [D loss: 0.638078, acc: 62.50%] [G loss: 1.797175]\n",
      "epoch:22 step:21391 [D loss: 0.601414, acc: 66.41%] [G loss: 1.880451]\n",
      "epoch:22 step:21392 [D loss: 0.659599, acc: 68.75%] [G loss: 1.788888]\n",
      "epoch:22 step:21393 [D loss: 0.653486, acc: 64.06%] [G loss: 1.837947]\n",
      "epoch:22 step:21394 [D loss: 0.645975, acc: 64.84%] [G loss: 2.019617]\n",
      "epoch:22 step:21395 [D loss: 0.575323, acc: 75.78%] [G loss: 2.098504]\n",
      "epoch:22 step:21396 [D loss: 0.590346, acc: 71.09%] [G loss: 2.050195]\n",
      "epoch:22 step:21397 [D loss: 0.665272, acc: 63.28%] [G loss: 2.063454]\n",
      "epoch:22 step:21398 [D loss: 0.718096, acc: 53.12%] [G loss: 1.765153]\n",
      "epoch:22 step:21399 [D loss: 0.624984, acc: 67.19%] [G loss: 1.930979]\n",
      "epoch:22 step:21400 [D loss: 0.598725, acc: 67.97%] [G loss: 2.107188]\n",
      "##############\n",
      "[2.51320841 1.52971399 6.19791556 4.58828221 3.67935562 5.5786131\n",
      " 4.39541186 4.57022846 4.33274868 3.59445872]\n",
      "##########\n",
      "epoch:22 step:21401 [D loss: 0.689503, acc: 61.72%] [G loss: 1.728184]\n",
      "epoch:22 step:21402 [D loss: 0.711299, acc: 51.56%] [G loss: 1.718031]\n",
      "epoch:22 step:21403 [D loss: 0.630518, acc: 63.28%] [G loss: 1.913494]\n",
      "epoch:22 step:21404 [D loss: 0.622188, acc: 70.31%] [G loss: 1.911728]\n",
      "epoch:22 step:21405 [D loss: 0.671082, acc: 59.38%] [G loss: 1.797817]\n",
      "epoch:22 step:21406 [D loss: 0.653791, acc: 66.41%] [G loss: 2.012953]\n",
      "epoch:22 step:21407 [D loss: 0.631947, acc: 64.06%] [G loss: 2.082946]\n",
      "epoch:22 step:21408 [D loss: 0.633227, acc: 63.28%] [G loss: 1.723066]\n",
      "epoch:22 step:21409 [D loss: 0.701718, acc: 59.38%] [G loss: 1.857818]\n",
      "epoch:22 step:21410 [D loss: 0.674630, acc: 57.81%] [G loss: 1.774771]\n",
      "epoch:22 step:21411 [D loss: 0.683898, acc: 53.12%] [G loss: 1.879866]\n",
      "epoch:22 step:21412 [D loss: 0.671672, acc: 61.72%] [G loss: 1.913474]\n",
      "epoch:22 step:21413 [D loss: 0.642841, acc: 59.38%] [G loss: 1.783552]\n",
      "epoch:22 step:21414 [D loss: 0.714635, acc: 48.44%] [G loss: 1.739871]\n",
      "epoch:22 step:21415 [D loss: 0.693631, acc: 50.78%] [G loss: 1.817742]\n",
      "epoch:22 step:21416 [D loss: 0.637559, acc: 62.50%] [G loss: 1.896007]\n",
      "epoch:22 step:21417 [D loss: 0.671123, acc: 59.38%] [G loss: 1.772365]\n",
      "epoch:22 step:21418 [D loss: 0.607748, acc: 70.31%] [G loss: 1.987701]\n",
      "epoch:22 step:21419 [D loss: 0.625879, acc: 66.41%] [G loss: 1.875557]\n",
      "epoch:22 step:21420 [D loss: 0.643100, acc: 65.62%] [G loss: 1.797442]\n",
      "epoch:22 step:21421 [D loss: 0.594795, acc: 70.31%] [G loss: 1.866009]\n",
      "epoch:22 step:21422 [D loss: 0.647135, acc: 67.97%] [G loss: 1.977592]\n",
      "epoch:22 step:21423 [D loss: 0.621340, acc: 67.19%] [G loss: 1.903330]\n",
      "epoch:22 step:21424 [D loss: 0.670850, acc: 58.59%] [G loss: 1.903148]\n",
      "epoch:22 step:21425 [D loss: 0.684813, acc: 58.59%] [G loss: 1.841022]\n",
      "epoch:22 step:21426 [D loss: 0.634998, acc: 61.72%] [G loss: 1.744807]\n",
      "epoch:22 step:21427 [D loss: 0.633763, acc: 61.72%] [G loss: 1.839483]\n",
      "epoch:22 step:21428 [D loss: 0.599027, acc: 69.53%] [G loss: 1.930162]\n",
      "epoch:22 step:21429 [D loss: 0.610153, acc: 67.97%] [G loss: 2.315985]\n",
      "epoch:22 step:21430 [D loss: 0.633319, acc: 64.06%] [G loss: 2.015417]\n",
      "epoch:22 step:21431 [D loss: 0.690698, acc: 58.59%] [G loss: 1.851119]\n",
      "epoch:22 step:21432 [D loss: 0.682846, acc: 59.38%] [G loss: 1.838014]\n",
      "epoch:22 step:21433 [D loss: 0.649811, acc: 60.16%] [G loss: 1.863817]\n",
      "epoch:22 step:21434 [D loss: 0.721308, acc: 50.00%] [G loss: 1.629894]\n",
      "epoch:22 step:21435 [D loss: 0.694067, acc: 54.69%] [G loss: 1.862669]\n",
      "epoch:22 step:21436 [D loss: 0.624615, acc: 64.06%] [G loss: 1.847877]\n",
      "epoch:22 step:21437 [D loss: 0.621367, acc: 67.19%] [G loss: 1.879911]\n",
      "epoch:22 step:21438 [D loss: 0.634072, acc: 60.16%] [G loss: 1.783627]\n",
      "epoch:22 step:21439 [D loss: 0.602980, acc: 68.75%] [G loss: 1.965131]\n",
      "epoch:22 step:21440 [D loss: 0.606687, acc: 70.31%] [G loss: 1.992569]\n",
      "epoch:22 step:21441 [D loss: 0.645020, acc: 60.94%] [G loss: 1.848477]\n",
      "epoch:22 step:21442 [D loss: 0.704027, acc: 53.12%] [G loss: 1.722479]\n",
      "epoch:22 step:21443 [D loss: 0.703273, acc: 52.34%] [G loss: 1.705677]\n",
      "epoch:22 step:21444 [D loss: 0.622118, acc: 64.84%] [G loss: 1.875889]\n",
      "epoch:22 step:21445 [D loss: 0.634265, acc: 69.53%] [G loss: 1.846395]\n",
      "epoch:22 step:21446 [D loss: 0.619234, acc: 62.50%] [G loss: 1.826856]\n",
      "epoch:22 step:21447 [D loss: 0.616780, acc: 64.84%] [G loss: 2.037976]\n",
      "epoch:22 step:21448 [D loss: 0.670073, acc: 61.72%] [G loss: 1.851523]\n",
      "epoch:22 step:21449 [D loss: 0.679047, acc: 58.59%] [G loss: 1.886453]\n",
      "epoch:22 step:21450 [D loss: 0.655313, acc: 58.59%] [G loss: 1.894531]\n",
      "epoch:22 step:21451 [D loss: 0.595548, acc: 67.19%] [G loss: 1.896690]\n",
      "epoch:22 step:21452 [D loss: 0.653086, acc: 64.06%] [G loss: 1.988885]\n",
      "epoch:22 step:21453 [D loss: 0.627376, acc: 64.06%] [G loss: 1.925530]\n",
      "epoch:22 step:21454 [D loss: 0.639722, acc: 59.38%] [G loss: 1.926009]\n",
      "epoch:22 step:21455 [D loss: 0.646789, acc: 61.72%] [G loss: 2.098152]\n",
      "epoch:22 step:21456 [D loss: 0.633755, acc: 61.72%] [G loss: 1.833864]\n",
      "epoch:22 step:21457 [D loss: 0.617324, acc: 67.97%] [G loss: 1.793981]\n",
      "epoch:22 step:21458 [D loss: 0.659304, acc: 60.16%] [G loss: 2.008861]\n",
      "epoch:22 step:21459 [D loss: 0.607902, acc: 63.28%] [G loss: 1.829261]\n",
      "epoch:22 step:21460 [D loss: 0.678885, acc: 60.94%] [G loss: 1.803398]\n",
      "epoch:22 step:21461 [D loss: 0.610381, acc: 68.75%] [G loss: 1.881163]\n",
      "epoch:22 step:21462 [D loss: 0.633297, acc: 62.50%] [G loss: 1.886645]\n",
      "epoch:22 step:21463 [D loss: 0.597265, acc: 66.41%] [G loss: 1.941967]\n",
      "epoch:22 step:21464 [D loss: 0.701212, acc: 55.47%] [G loss: 1.796443]\n",
      "epoch:22 step:21465 [D loss: 0.660305, acc: 60.16%] [G loss: 1.743989]\n",
      "epoch:22 step:21466 [D loss: 0.685579, acc: 59.38%] [G loss: 1.748796]\n",
      "epoch:22 step:21467 [D loss: 0.681011, acc: 60.16%] [G loss: 1.797141]\n",
      "epoch:22 step:21468 [D loss: 0.651713, acc: 60.16%] [G loss: 1.835901]\n",
      "epoch:22 step:21469 [D loss: 0.638007, acc: 62.50%] [G loss: 1.848814]\n",
      "epoch:22 step:21470 [D loss: 0.688239, acc: 57.81%] [G loss: 1.713226]\n",
      "epoch:22 step:21471 [D loss: 0.626116, acc: 61.72%] [G loss: 1.861844]\n",
      "epoch:22 step:21472 [D loss: 0.735305, acc: 52.34%] [G loss: 1.778715]\n",
      "epoch:22 step:21473 [D loss: 0.702097, acc: 60.16%] [G loss: 1.698653]\n",
      "epoch:22 step:21474 [D loss: 0.661176, acc: 62.50%] [G loss: 1.858856]\n",
      "epoch:22 step:21475 [D loss: 0.668325, acc: 61.72%] [G loss: 1.698046]\n",
      "epoch:22 step:21476 [D loss: 0.661784, acc: 59.38%] [G loss: 1.809268]\n",
      "epoch:22 step:21477 [D loss: 0.613895, acc: 68.75%] [G loss: 1.851815]\n",
      "epoch:22 step:21478 [D loss: 0.648527, acc: 59.38%] [G loss: 1.762403]\n",
      "epoch:22 step:21479 [D loss: 0.663602, acc: 56.25%] [G loss: 1.817024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21480 [D loss: 0.675730, acc: 58.59%] [G loss: 1.692919]\n",
      "epoch:22 step:21481 [D loss: 0.667339, acc: 59.38%] [G loss: 1.758037]\n",
      "epoch:22 step:21482 [D loss: 0.683619, acc: 57.03%] [G loss: 1.739349]\n",
      "epoch:22 step:21483 [D loss: 0.621111, acc: 63.28%] [G loss: 1.992976]\n",
      "epoch:22 step:21484 [D loss: 0.662060, acc: 56.25%] [G loss: 1.915836]\n",
      "epoch:22 step:21485 [D loss: 0.659668, acc: 61.72%] [G loss: 1.872992]\n",
      "epoch:22 step:21486 [D loss: 0.613639, acc: 66.41%] [G loss: 1.839293]\n",
      "epoch:22 step:21487 [D loss: 0.689306, acc: 57.81%] [G loss: 1.748273]\n",
      "epoch:22 step:21488 [D loss: 0.625441, acc: 62.50%] [G loss: 1.707993]\n",
      "epoch:22 step:21489 [D loss: 0.653955, acc: 57.03%] [G loss: 1.924234]\n",
      "epoch:22 step:21490 [D loss: 0.636036, acc: 65.62%] [G loss: 1.820984]\n",
      "epoch:22 step:21491 [D loss: 0.634329, acc: 64.84%] [G loss: 1.751547]\n",
      "epoch:22 step:21492 [D loss: 0.617241, acc: 66.41%] [G loss: 1.845682]\n",
      "epoch:22 step:21493 [D loss: 0.680153, acc: 57.03%] [G loss: 1.910838]\n",
      "epoch:22 step:21494 [D loss: 0.642695, acc: 65.62%] [G loss: 1.937629]\n",
      "epoch:22 step:21495 [D loss: 0.700705, acc: 53.91%] [G loss: 1.840911]\n",
      "epoch:22 step:21496 [D loss: 0.619259, acc: 66.41%] [G loss: 1.926073]\n",
      "epoch:22 step:21497 [D loss: 0.644564, acc: 60.16%] [G loss: 1.878108]\n",
      "epoch:22 step:21498 [D loss: 0.630039, acc: 62.50%] [G loss: 1.981973]\n",
      "epoch:22 step:21499 [D loss: 0.624156, acc: 64.84%] [G loss: 1.857809]\n",
      "epoch:22 step:21500 [D loss: 0.619930, acc: 62.50%] [G loss: 1.970129]\n",
      "epoch:22 step:21501 [D loss: 0.678012, acc: 57.81%] [G loss: 1.847588]\n",
      "epoch:22 step:21502 [D loss: 0.654752, acc: 65.62%] [G loss: 1.695859]\n",
      "epoch:22 step:21503 [D loss: 0.602435, acc: 64.06%] [G loss: 2.027474]\n",
      "epoch:22 step:21504 [D loss: 0.616412, acc: 67.97%] [G loss: 2.026017]\n",
      "epoch:22 step:21505 [D loss: 0.643421, acc: 63.28%] [G loss: 1.830028]\n",
      "epoch:22 step:21506 [D loss: 0.695639, acc: 63.28%] [G loss: 1.741072]\n",
      "epoch:22 step:21507 [D loss: 0.646643, acc: 57.81%] [G loss: 1.908817]\n",
      "epoch:22 step:21508 [D loss: 0.568315, acc: 73.44%] [G loss: 2.100174]\n",
      "epoch:22 step:21509 [D loss: 0.669575, acc: 58.59%] [G loss: 1.889433]\n",
      "epoch:22 step:21510 [D loss: 0.636760, acc: 67.19%] [G loss: 1.790024]\n",
      "epoch:22 step:21511 [D loss: 0.667917, acc: 58.59%] [G loss: 2.097287]\n",
      "epoch:22 step:21512 [D loss: 0.637946, acc: 64.06%] [G loss: 1.888672]\n",
      "epoch:22 step:21513 [D loss: 0.631801, acc: 60.94%] [G loss: 2.001730]\n",
      "epoch:22 step:21514 [D loss: 0.649480, acc: 58.59%] [G loss: 1.794957]\n",
      "epoch:22 step:21515 [D loss: 0.721406, acc: 60.16%] [G loss: 1.866862]\n",
      "epoch:22 step:21516 [D loss: 0.646447, acc: 64.06%] [G loss: 1.953491]\n",
      "epoch:22 step:21517 [D loss: 0.679618, acc: 61.72%] [G loss: 1.864879]\n",
      "epoch:22 step:21518 [D loss: 0.615796, acc: 67.19%] [G loss: 1.881391]\n",
      "epoch:22 step:21519 [D loss: 0.610533, acc: 70.31%] [G loss: 2.007662]\n",
      "epoch:22 step:21520 [D loss: 0.656093, acc: 62.50%] [G loss: 2.010707]\n",
      "epoch:22 step:21521 [D loss: 0.630797, acc: 67.19%] [G loss: 1.993239]\n",
      "epoch:22 step:21522 [D loss: 0.604898, acc: 69.53%] [G loss: 1.994582]\n",
      "epoch:22 step:21523 [D loss: 0.610724, acc: 64.06%] [G loss: 2.046260]\n",
      "epoch:22 step:21524 [D loss: 0.685944, acc: 53.12%] [G loss: 2.092238]\n",
      "epoch:22 step:21525 [D loss: 0.640987, acc: 60.94%] [G loss: 1.939465]\n",
      "epoch:22 step:21526 [D loss: 0.680694, acc: 63.28%] [G loss: 2.047166]\n",
      "epoch:22 step:21527 [D loss: 0.637651, acc: 65.62%] [G loss: 1.926739]\n",
      "epoch:22 step:21528 [D loss: 0.660809, acc: 62.50%] [G loss: 1.871714]\n",
      "epoch:22 step:21529 [D loss: 0.723343, acc: 53.12%] [G loss: 1.805608]\n",
      "epoch:22 step:21530 [D loss: 0.622708, acc: 66.41%] [G loss: 2.000205]\n",
      "epoch:22 step:21531 [D loss: 0.643842, acc: 67.97%] [G loss: 2.006776]\n",
      "epoch:22 step:21532 [D loss: 0.578261, acc: 74.22%] [G loss: 2.131220]\n",
      "epoch:22 step:21533 [D loss: 0.563282, acc: 71.09%] [G loss: 2.152762]\n",
      "epoch:22 step:21534 [D loss: 0.742780, acc: 53.12%] [G loss: 1.870362]\n",
      "epoch:22 step:21535 [D loss: 0.614303, acc: 65.62%] [G loss: 2.074978]\n",
      "epoch:22 step:21536 [D loss: 0.652901, acc: 64.84%] [G loss: 1.884563]\n",
      "epoch:22 step:21537 [D loss: 0.585957, acc: 67.19%] [G loss: 2.097651]\n",
      "epoch:22 step:21538 [D loss: 0.573662, acc: 73.44%] [G loss: 2.051996]\n",
      "epoch:22 step:21539 [D loss: 0.632771, acc: 67.97%] [G loss: 2.107818]\n",
      "epoch:22 step:21540 [D loss: 0.614647, acc: 65.62%] [G loss: 2.232293]\n",
      "epoch:22 step:21541 [D loss: 0.694179, acc: 57.03%] [G loss: 2.206235]\n",
      "epoch:22 step:21542 [D loss: 0.779713, acc: 50.00%] [G loss: 1.673497]\n",
      "epoch:22 step:21543 [D loss: 0.796415, acc: 46.88%] [G loss: 1.974586]\n",
      "epoch:22 step:21544 [D loss: 0.650028, acc: 60.16%] [G loss: 1.954019]\n",
      "epoch:22 step:21545 [D loss: 0.597162, acc: 71.09%] [G loss: 2.072423]\n",
      "epoch:22 step:21546 [D loss: 0.601541, acc: 69.53%] [G loss: 2.001058]\n",
      "epoch:22 step:21547 [D loss: 0.651735, acc: 61.72%] [G loss: 1.847050]\n",
      "epoch:22 step:21548 [D loss: 0.632122, acc: 62.50%] [G loss: 1.810710]\n",
      "epoch:22 step:21549 [D loss: 0.639716, acc: 60.94%] [G loss: 1.937164]\n",
      "epoch:22 step:21550 [D loss: 0.591224, acc: 73.44%] [G loss: 2.043145]\n",
      "epoch:22 step:21551 [D loss: 0.657011, acc: 63.28%] [G loss: 2.272199]\n",
      "epoch:23 step:21552 [D loss: 0.640445, acc: 63.28%] [G loss: 1.802753]\n",
      "epoch:23 step:21553 [D loss: 0.655110, acc: 64.84%] [G loss: 1.958093]\n",
      "epoch:23 step:21554 [D loss: 0.657910, acc: 60.16%] [G loss: 1.951149]\n",
      "epoch:23 step:21555 [D loss: 0.662260, acc: 62.50%] [G loss: 1.912349]\n",
      "epoch:23 step:21556 [D loss: 0.631264, acc: 64.84%] [G loss: 1.887899]\n",
      "epoch:23 step:21557 [D loss: 0.589486, acc: 68.75%] [G loss: 1.981024]\n",
      "epoch:23 step:21558 [D loss: 0.623581, acc: 66.41%] [G loss: 1.858365]\n",
      "epoch:23 step:21559 [D loss: 0.619198, acc: 61.72%] [G loss: 2.062177]\n",
      "epoch:23 step:21560 [D loss: 0.601673, acc: 66.41%] [G loss: 1.975001]\n",
      "epoch:23 step:21561 [D loss: 0.661034, acc: 61.72%] [G loss: 1.903788]\n",
      "epoch:23 step:21562 [D loss: 0.634524, acc: 70.31%] [G loss: 1.952065]\n",
      "epoch:23 step:21563 [D loss: 0.606133, acc: 63.28%] [G loss: 1.957649]\n",
      "epoch:23 step:21564 [D loss: 0.641827, acc: 64.84%] [G loss: 1.971527]\n",
      "epoch:23 step:21565 [D loss: 0.612393, acc: 67.97%] [G loss: 1.853415]\n",
      "epoch:23 step:21566 [D loss: 0.607424, acc: 64.84%] [G loss: 1.991808]\n",
      "epoch:23 step:21567 [D loss: 0.639336, acc: 64.06%] [G loss: 2.062479]\n",
      "epoch:23 step:21568 [D loss: 0.671154, acc: 67.97%] [G loss: 1.848485]\n",
      "epoch:23 step:21569 [D loss: 0.647302, acc: 65.62%] [G loss: 1.852704]\n",
      "epoch:23 step:21570 [D loss: 0.638567, acc: 64.06%] [G loss: 1.841601]\n",
      "epoch:23 step:21571 [D loss: 0.721206, acc: 50.78%] [G loss: 1.737078]\n",
      "epoch:23 step:21572 [D loss: 0.673817, acc: 55.47%] [G loss: 1.757374]\n",
      "epoch:23 step:21573 [D loss: 0.693081, acc: 50.78%] [G loss: 1.715964]\n",
      "epoch:23 step:21574 [D loss: 0.599046, acc: 74.22%] [G loss: 2.044317]\n",
      "epoch:23 step:21575 [D loss: 0.629361, acc: 71.88%] [G loss: 1.802565]\n",
      "epoch:23 step:21576 [D loss: 0.569588, acc: 72.66%] [G loss: 2.099334]\n",
      "epoch:23 step:21577 [D loss: 0.641752, acc: 59.38%] [G loss: 1.912726]\n",
      "epoch:23 step:21578 [D loss: 0.681280, acc: 58.59%] [G loss: 1.876334]\n",
      "epoch:23 step:21579 [D loss: 0.627881, acc: 68.75%] [G loss: 1.960706]\n",
      "epoch:23 step:21580 [D loss: 0.621274, acc: 68.75%] [G loss: 1.952684]\n",
      "epoch:23 step:21581 [D loss: 0.631453, acc: 67.19%] [G loss: 1.889713]\n",
      "epoch:23 step:21582 [D loss: 0.658342, acc: 55.47%] [G loss: 1.786508]\n",
      "epoch:23 step:21583 [D loss: 0.639339, acc: 65.62%] [G loss: 1.835847]\n",
      "epoch:23 step:21584 [D loss: 0.620167, acc: 65.62%] [G loss: 1.772641]\n",
      "epoch:23 step:21585 [D loss: 0.662548, acc: 61.72%] [G loss: 1.940996]\n",
      "epoch:23 step:21586 [D loss: 0.628470, acc: 64.06%] [G loss: 1.811741]\n",
      "epoch:23 step:21587 [D loss: 0.626232, acc: 65.62%] [G loss: 2.011907]\n",
      "epoch:23 step:21588 [D loss: 0.614149, acc: 64.06%] [G loss: 2.160227]\n",
      "epoch:23 step:21589 [D loss: 0.649319, acc: 60.94%] [G loss: 1.982531]\n",
      "epoch:23 step:21590 [D loss: 0.725111, acc: 49.22%] [G loss: 1.903781]\n",
      "epoch:23 step:21591 [D loss: 0.616047, acc: 65.62%] [G loss: 2.147767]\n",
      "epoch:23 step:21592 [D loss: 0.608920, acc: 63.28%] [G loss: 1.845865]\n",
      "epoch:23 step:21593 [D loss: 0.602674, acc: 68.75%] [G loss: 2.002011]\n",
      "epoch:23 step:21594 [D loss: 0.648249, acc: 64.06%] [G loss: 1.855444]\n",
      "epoch:23 step:21595 [D loss: 0.610261, acc: 67.97%] [G loss: 1.931567]\n",
      "epoch:23 step:21596 [D loss: 0.674999, acc: 57.81%] [G loss: 1.839936]\n",
      "epoch:23 step:21597 [D loss: 0.673536, acc: 61.72%] [G loss: 1.843249]\n",
      "epoch:23 step:21598 [D loss: 0.581245, acc: 75.00%] [G loss: 2.027830]\n",
      "epoch:23 step:21599 [D loss: 0.644666, acc: 62.50%] [G loss: 2.032992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21600 [D loss: 0.586616, acc: 67.97%] [G loss: 1.975483]\n",
      "##############\n",
      "[2.23757577 1.56246831 6.16982891 4.71619638 3.54616882 5.51186756\n",
      " 4.38367876 4.61086463 4.44867348 3.62784817]\n",
      "##########\n",
      "epoch:23 step:21601 [D loss: 0.632159, acc: 65.62%] [G loss: 1.901535]\n",
      "epoch:23 step:21602 [D loss: 0.628278, acc: 67.97%] [G loss: 1.884028]\n",
      "epoch:23 step:21603 [D loss: 0.636685, acc: 67.19%] [G loss: 1.774379]\n",
      "epoch:23 step:21604 [D loss: 0.662206, acc: 60.94%] [G loss: 1.948235]\n",
      "epoch:23 step:21605 [D loss: 0.591349, acc: 68.75%] [G loss: 2.058670]\n",
      "epoch:23 step:21606 [D loss: 0.558163, acc: 75.00%] [G loss: 2.022924]\n",
      "epoch:23 step:21607 [D loss: 0.622554, acc: 64.06%] [G loss: 1.940981]\n",
      "epoch:23 step:21608 [D loss: 0.684236, acc: 60.16%] [G loss: 1.941046]\n",
      "epoch:23 step:21609 [D loss: 0.682941, acc: 60.94%] [G loss: 1.890330]\n",
      "epoch:23 step:21610 [D loss: 0.677839, acc: 61.72%] [G loss: 1.938554]\n",
      "epoch:23 step:21611 [D loss: 0.641735, acc: 56.25%] [G loss: 1.990440]\n",
      "epoch:23 step:21612 [D loss: 0.626406, acc: 69.53%] [G loss: 1.934191]\n",
      "epoch:23 step:21613 [D loss: 0.606403, acc: 65.62%] [G loss: 2.047971]\n",
      "epoch:23 step:21614 [D loss: 0.634645, acc: 64.84%] [G loss: 2.042019]\n",
      "epoch:23 step:21615 [D loss: 0.700555, acc: 59.38%] [G loss: 1.954896]\n",
      "epoch:23 step:21616 [D loss: 0.653934, acc: 56.25%] [G loss: 1.943696]\n",
      "epoch:23 step:21617 [D loss: 0.632928, acc: 61.72%] [G loss: 1.809565]\n",
      "epoch:23 step:21618 [D loss: 0.648283, acc: 57.81%] [G loss: 1.826786]\n",
      "epoch:23 step:21619 [D loss: 0.613781, acc: 65.62%] [G loss: 1.960457]\n",
      "epoch:23 step:21620 [D loss: 0.632446, acc: 60.94%] [G loss: 1.971313]\n",
      "epoch:23 step:21621 [D loss: 0.654477, acc: 57.81%] [G loss: 1.999835]\n",
      "epoch:23 step:21622 [D loss: 0.663731, acc: 59.38%] [G loss: 1.886819]\n",
      "epoch:23 step:21623 [D loss: 0.646523, acc: 63.28%] [G loss: 2.029445]\n",
      "epoch:23 step:21624 [D loss: 0.655102, acc: 60.94%] [G loss: 1.785936]\n",
      "epoch:23 step:21625 [D loss: 0.668489, acc: 60.94%] [G loss: 1.835174]\n",
      "epoch:23 step:21626 [D loss: 0.634751, acc: 64.84%] [G loss: 2.113556]\n",
      "epoch:23 step:21627 [D loss: 0.663080, acc: 59.38%] [G loss: 1.902217]\n",
      "epoch:23 step:21628 [D loss: 0.628142, acc: 65.62%] [G loss: 2.075250]\n",
      "epoch:23 step:21629 [D loss: 0.694727, acc: 59.38%] [G loss: 1.838317]\n",
      "epoch:23 step:21630 [D loss: 0.714954, acc: 57.03%] [G loss: 1.787028]\n",
      "epoch:23 step:21631 [D loss: 0.718300, acc: 54.69%] [G loss: 1.816244]\n",
      "epoch:23 step:21632 [D loss: 0.669247, acc: 59.38%] [G loss: 1.796451]\n",
      "epoch:23 step:21633 [D loss: 0.698737, acc: 57.03%] [G loss: 1.831556]\n",
      "epoch:23 step:21634 [D loss: 0.679500, acc: 57.81%] [G loss: 1.909519]\n",
      "epoch:23 step:21635 [D loss: 0.646301, acc: 58.59%] [G loss: 1.942907]\n",
      "epoch:23 step:21636 [D loss: 0.643796, acc: 60.16%] [G loss: 1.726339]\n",
      "epoch:23 step:21637 [D loss: 0.686982, acc: 50.00%] [G loss: 1.741785]\n",
      "epoch:23 step:21638 [D loss: 0.639911, acc: 64.06%] [G loss: 1.854035]\n",
      "epoch:23 step:21639 [D loss: 0.645259, acc: 63.28%] [G loss: 1.943155]\n",
      "epoch:23 step:21640 [D loss: 0.675158, acc: 57.81%] [G loss: 1.863853]\n",
      "epoch:23 step:21641 [D loss: 0.624455, acc: 64.84%] [G loss: 1.764474]\n",
      "epoch:23 step:21642 [D loss: 0.642830, acc: 61.72%] [G loss: 1.938584]\n",
      "epoch:23 step:21643 [D loss: 0.571758, acc: 74.22%] [G loss: 1.857685]\n",
      "epoch:23 step:21644 [D loss: 0.633602, acc: 63.28%] [G loss: 1.953410]\n",
      "epoch:23 step:21645 [D loss: 0.674112, acc: 59.38%] [G loss: 1.975505]\n",
      "epoch:23 step:21646 [D loss: 0.649436, acc: 63.28%] [G loss: 1.890416]\n",
      "epoch:23 step:21647 [D loss: 0.639847, acc: 60.94%] [G loss: 2.055859]\n",
      "epoch:23 step:21648 [D loss: 0.589258, acc: 69.53%] [G loss: 1.931124]\n",
      "epoch:23 step:21649 [D loss: 0.638256, acc: 66.41%] [G loss: 1.711577]\n",
      "epoch:23 step:21650 [D loss: 0.672626, acc: 60.16%] [G loss: 1.820978]\n",
      "epoch:23 step:21651 [D loss: 0.622086, acc: 68.75%] [G loss: 1.967883]\n",
      "epoch:23 step:21652 [D loss: 0.630096, acc: 62.50%] [G loss: 1.825687]\n",
      "epoch:23 step:21653 [D loss: 0.702429, acc: 58.59%] [G loss: 1.895931]\n",
      "epoch:23 step:21654 [D loss: 0.669971, acc: 60.94%] [G loss: 1.837932]\n",
      "epoch:23 step:21655 [D loss: 0.667908, acc: 64.06%] [G loss: 1.930940]\n",
      "epoch:23 step:21656 [D loss: 0.638142, acc: 64.06%] [G loss: 1.882100]\n",
      "epoch:23 step:21657 [D loss: 0.618929, acc: 67.19%] [G loss: 1.982306]\n",
      "epoch:23 step:21658 [D loss: 0.579454, acc: 71.88%] [G loss: 2.060012]\n",
      "epoch:23 step:21659 [D loss: 0.695751, acc: 57.03%] [G loss: 1.812575]\n",
      "epoch:23 step:21660 [D loss: 0.609727, acc: 68.75%] [G loss: 1.781021]\n",
      "epoch:23 step:21661 [D loss: 0.631913, acc: 66.41%] [G loss: 1.915038]\n",
      "epoch:23 step:21662 [D loss: 0.640165, acc: 60.16%] [G loss: 2.048901]\n",
      "epoch:23 step:21663 [D loss: 0.650972, acc: 64.84%] [G loss: 1.928696]\n",
      "epoch:23 step:21664 [D loss: 0.610146, acc: 63.28%] [G loss: 2.080909]\n",
      "epoch:23 step:21665 [D loss: 0.614456, acc: 70.31%] [G loss: 1.903802]\n",
      "epoch:23 step:21666 [D loss: 0.683566, acc: 59.38%] [G loss: 2.325125]\n",
      "epoch:23 step:21667 [D loss: 0.602098, acc: 69.53%] [G loss: 2.031829]\n",
      "epoch:23 step:21668 [D loss: 0.628158, acc: 64.84%] [G loss: 2.245216]\n",
      "epoch:23 step:21669 [D loss: 0.655874, acc: 64.06%] [G loss: 2.057730]\n",
      "epoch:23 step:21670 [D loss: 0.595068, acc: 71.09%] [G loss: 2.149559]\n",
      "epoch:23 step:21671 [D loss: 0.658424, acc: 58.59%] [G loss: 1.950130]\n",
      "epoch:23 step:21672 [D loss: 0.649992, acc: 63.28%] [G loss: 1.959547]\n",
      "epoch:23 step:21673 [D loss: 0.612513, acc: 61.72%] [G loss: 2.208232]\n",
      "epoch:23 step:21674 [D loss: 0.726064, acc: 52.34%] [G loss: 2.021776]\n",
      "epoch:23 step:21675 [D loss: 0.665220, acc: 61.72%] [G loss: 1.806218]\n",
      "epoch:23 step:21676 [D loss: 0.729777, acc: 50.00%] [G loss: 1.779834]\n",
      "epoch:23 step:21677 [D loss: 0.587004, acc: 68.75%] [G loss: 1.887805]\n",
      "epoch:23 step:21678 [D loss: 0.695798, acc: 56.25%] [G loss: 1.788988]\n",
      "epoch:23 step:21679 [D loss: 0.629127, acc: 64.06%] [G loss: 1.832879]\n",
      "epoch:23 step:21680 [D loss: 0.651328, acc: 64.06%] [G loss: 1.801156]\n",
      "epoch:23 step:21681 [D loss: 0.644644, acc: 67.97%] [G loss: 1.803402]\n",
      "epoch:23 step:21682 [D loss: 0.641499, acc: 63.28%] [G loss: 1.893214]\n",
      "epoch:23 step:21683 [D loss: 0.672885, acc: 56.25%] [G loss: 1.855518]\n",
      "epoch:23 step:21684 [D loss: 0.672140, acc: 53.12%] [G loss: 1.808877]\n",
      "epoch:23 step:21685 [D loss: 0.659684, acc: 59.38%] [G loss: 1.880080]\n",
      "epoch:23 step:21686 [D loss: 0.649530, acc: 61.72%] [G loss: 1.814804]\n",
      "epoch:23 step:21687 [D loss: 0.672383, acc: 59.38%] [G loss: 1.945044]\n",
      "epoch:23 step:21688 [D loss: 0.674984, acc: 57.03%] [G loss: 1.926158]\n",
      "epoch:23 step:21689 [D loss: 0.650276, acc: 65.62%] [G loss: 1.784956]\n",
      "epoch:23 step:21690 [D loss: 0.631063, acc: 65.62%] [G loss: 1.858017]\n",
      "epoch:23 step:21691 [D loss: 0.622313, acc: 71.09%] [G loss: 1.818960]\n",
      "epoch:23 step:21692 [D loss: 0.639762, acc: 59.38%] [G loss: 1.785004]\n",
      "epoch:23 step:21693 [D loss: 0.613155, acc: 64.06%] [G loss: 1.999738]\n",
      "epoch:23 step:21694 [D loss: 0.670600, acc: 56.25%] [G loss: 1.797801]\n",
      "epoch:23 step:21695 [D loss: 0.639391, acc: 62.50%] [G loss: 1.934326]\n",
      "epoch:23 step:21696 [D loss: 0.616111, acc: 65.62%] [G loss: 1.899067]\n",
      "epoch:23 step:21697 [D loss: 0.709645, acc: 55.47%] [G loss: 1.856316]\n",
      "epoch:23 step:21698 [D loss: 0.618540, acc: 67.97%] [G loss: 1.775611]\n",
      "epoch:23 step:21699 [D loss: 0.672746, acc: 55.47%] [G loss: 1.908533]\n",
      "epoch:23 step:21700 [D loss: 0.616684, acc: 66.41%] [G loss: 1.834998]\n",
      "epoch:23 step:21701 [D loss: 0.658846, acc: 64.84%] [G loss: 1.929495]\n",
      "epoch:23 step:21702 [D loss: 0.589696, acc: 68.75%] [G loss: 1.910557]\n",
      "epoch:23 step:21703 [D loss: 0.659401, acc: 60.16%] [G loss: 1.935423]\n",
      "epoch:23 step:21704 [D loss: 0.687978, acc: 54.69%] [G loss: 1.875043]\n",
      "epoch:23 step:21705 [D loss: 0.596401, acc: 69.53%] [G loss: 1.929807]\n",
      "epoch:23 step:21706 [D loss: 0.671840, acc: 62.50%] [G loss: 1.803720]\n",
      "epoch:23 step:21707 [D loss: 0.590011, acc: 67.97%] [G loss: 2.004216]\n",
      "epoch:23 step:21708 [D loss: 0.628639, acc: 58.59%] [G loss: 1.875118]\n",
      "epoch:23 step:21709 [D loss: 0.597694, acc: 67.97%] [G loss: 1.825703]\n",
      "epoch:23 step:21710 [D loss: 0.713841, acc: 52.34%] [G loss: 1.868215]\n",
      "epoch:23 step:21711 [D loss: 0.714693, acc: 51.56%] [G loss: 1.756230]\n",
      "epoch:23 step:21712 [D loss: 0.666422, acc: 60.94%] [G loss: 1.833474]\n",
      "epoch:23 step:21713 [D loss: 0.586422, acc: 71.88%] [G loss: 1.873246]\n",
      "epoch:23 step:21714 [D loss: 0.640968, acc: 66.41%] [G loss: 1.829014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21715 [D loss: 0.672676, acc: 60.16%] [G loss: 1.809556]\n",
      "epoch:23 step:21716 [D loss: 0.680426, acc: 55.47%] [G loss: 1.848010]\n",
      "epoch:23 step:21717 [D loss: 0.639087, acc: 67.97%] [G loss: 1.853204]\n",
      "epoch:23 step:21718 [D loss: 0.642377, acc: 58.59%] [G loss: 1.952870]\n",
      "epoch:23 step:21719 [D loss: 0.649310, acc: 63.28%] [G loss: 1.973484]\n",
      "epoch:23 step:21720 [D loss: 0.638365, acc: 65.62%] [G loss: 1.892025]\n",
      "epoch:23 step:21721 [D loss: 0.712053, acc: 52.34%] [G loss: 1.795314]\n",
      "epoch:23 step:21722 [D loss: 0.671576, acc: 58.59%] [G loss: 1.788988]\n",
      "epoch:23 step:21723 [D loss: 0.649271, acc: 61.72%] [G loss: 1.802785]\n",
      "epoch:23 step:21724 [D loss: 0.697454, acc: 57.03%] [G loss: 1.843081]\n",
      "epoch:23 step:21725 [D loss: 0.668047, acc: 61.72%] [G loss: 1.775787]\n",
      "epoch:23 step:21726 [D loss: 0.691133, acc: 54.69%] [G loss: 1.842621]\n",
      "epoch:23 step:21727 [D loss: 0.650502, acc: 64.84%] [G loss: 1.856841]\n",
      "epoch:23 step:21728 [D loss: 0.633911, acc: 67.97%] [G loss: 1.848156]\n",
      "epoch:23 step:21729 [D loss: 0.631379, acc: 64.84%] [G loss: 1.853953]\n",
      "epoch:23 step:21730 [D loss: 0.671499, acc: 60.16%] [G loss: 1.910471]\n",
      "epoch:23 step:21731 [D loss: 0.637464, acc: 64.06%] [G loss: 1.777969]\n",
      "epoch:23 step:21732 [D loss: 0.654417, acc: 64.06%] [G loss: 1.765279]\n",
      "epoch:23 step:21733 [D loss: 0.601908, acc: 68.75%] [G loss: 1.801741]\n",
      "epoch:23 step:21734 [D loss: 0.655058, acc: 62.50%] [G loss: 1.852088]\n",
      "epoch:23 step:21735 [D loss: 0.615016, acc: 67.97%] [G loss: 1.847321]\n",
      "epoch:23 step:21736 [D loss: 0.690490, acc: 57.03%] [G loss: 1.960487]\n",
      "epoch:23 step:21737 [D loss: 0.622062, acc: 61.72%] [G loss: 1.881334]\n",
      "epoch:23 step:21738 [D loss: 0.703505, acc: 59.38%] [G loss: 1.984118]\n",
      "epoch:23 step:21739 [D loss: 0.675186, acc: 57.81%] [G loss: 1.977780]\n",
      "epoch:23 step:21740 [D loss: 0.687803, acc: 58.59%] [G loss: 1.827940]\n",
      "epoch:23 step:21741 [D loss: 0.701330, acc: 52.34%] [G loss: 1.941353]\n",
      "epoch:23 step:21742 [D loss: 0.652598, acc: 61.72%] [G loss: 1.904760]\n",
      "epoch:23 step:21743 [D loss: 0.660174, acc: 64.06%] [G loss: 1.897023]\n",
      "epoch:23 step:21744 [D loss: 0.643650, acc: 59.38%] [G loss: 1.975663]\n",
      "epoch:23 step:21745 [D loss: 0.615193, acc: 67.97%] [G loss: 1.979264]\n",
      "epoch:23 step:21746 [D loss: 0.645482, acc: 61.72%] [G loss: 1.883071]\n",
      "epoch:23 step:21747 [D loss: 0.640097, acc: 64.06%] [G loss: 1.766303]\n",
      "epoch:23 step:21748 [D loss: 0.583199, acc: 63.28%] [G loss: 1.862787]\n",
      "epoch:23 step:21749 [D loss: 0.612238, acc: 71.88%] [G loss: 1.897232]\n",
      "epoch:23 step:21750 [D loss: 0.635782, acc: 62.50%] [G loss: 1.868696]\n",
      "epoch:23 step:21751 [D loss: 0.673328, acc: 61.72%] [G loss: 1.965517]\n",
      "epoch:23 step:21752 [D loss: 0.628948, acc: 61.72%] [G loss: 1.910334]\n",
      "epoch:23 step:21753 [D loss: 0.668009, acc: 59.38%] [G loss: 1.857600]\n",
      "epoch:23 step:21754 [D loss: 0.615635, acc: 65.62%] [G loss: 1.845666]\n",
      "epoch:23 step:21755 [D loss: 0.675423, acc: 59.38%] [G loss: 1.779493]\n",
      "epoch:23 step:21756 [D loss: 0.671229, acc: 64.06%] [G loss: 1.834650]\n",
      "epoch:23 step:21757 [D loss: 0.638518, acc: 63.28%] [G loss: 1.889387]\n",
      "epoch:23 step:21758 [D loss: 0.611616, acc: 62.50%] [G loss: 2.057317]\n",
      "epoch:23 step:21759 [D loss: 0.667560, acc: 65.62%] [G loss: 2.168873]\n",
      "epoch:23 step:21760 [D loss: 0.535704, acc: 73.44%] [G loss: 2.221526]\n",
      "epoch:23 step:21761 [D loss: 0.724020, acc: 53.91%] [G loss: 1.800059]\n",
      "epoch:23 step:21762 [D loss: 0.638866, acc: 63.28%] [G loss: 1.806595]\n",
      "epoch:23 step:21763 [D loss: 0.698344, acc: 51.56%] [G loss: 1.826323]\n",
      "epoch:23 step:21764 [D loss: 0.732381, acc: 57.81%] [G loss: 1.735510]\n",
      "epoch:23 step:21765 [D loss: 0.644203, acc: 63.28%] [G loss: 1.788524]\n",
      "epoch:23 step:21766 [D loss: 0.650752, acc: 56.25%] [G loss: 1.882989]\n",
      "epoch:23 step:21767 [D loss: 0.589691, acc: 69.53%] [G loss: 2.022797]\n",
      "epoch:23 step:21768 [D loss: 0.650093, acc: 64.06%] [G loss: 1.929055]\n",
      "epoch:23 step:21769 [D loss: 0.566373, acc: 73.44%] [G loss: 2.063906]\n",
      "epoch:23 step:21770 [D loss: 0.597714, acc: 67.97%] [G loss: 2.089301]\n",
      "epoch:23 step:21771 [D loss: 0.705687, acc: 54.69%] [G loss: 1.872032]\n",
      "epoch:23 step:21772 [D loss: 0.646697, acc: 60.16%] [G loss: 2.078525]\n",
      "epoch:23 step:21773 [D loss: 0.624169, acc: 66.41%] [G loss: 2.009441]\n",
      "epoch:23 step:21774 [D loss: 0.603157, acc: 66.41%] [G loss: 1.960173]\n",
      "epoch:23 step:21775 [D loss: 0.706300, acc: 52.34%] [G loss: 1.933106]\n",
      "epoch:23 step:21776 [D loss: 0.674982, acc: 57.81%] [G loss: 1.751625]\n",
      "epoch:23 step:21777 [D loss: 0.654583, acc: 59.38%] [G loss: 1.888300]\n",
      "epoch:23 step:21778 [D loss: 0.579244, acc: 75.78%] [G loss: 1.830723]\n",
      "epoch:23 step:21779 [D loss: 0.695507, acc: 51.56%] [G loss: 1.773005]\n",
      "epoch:23 step:21780 [D loss: 0.609218, acc: 69.53%] [G loss: 2.032566]\n",
      "epoch:23 step:21781 [D loss: 0.614766, acc: 69.53%] [G loss: 2.104783]\n",
      "epoch:23 step:21782 [D loss: 0.561443, acc: 70.31%] [G loss: 2.498003]\n",
      "epoch:23 step:21783 [D loss: 0.555245, acc: 77.34%] [G loss: 2.448170]\n",
      "epoch:23 step:21784 [D loss: 0.690754, acc: 57.81%] [G loss: 1.974523]\n",
      "epoch:23 step:21785 [D loss: 0.673138, acc: 57.03%] [G loss: 1.854806]\n",
      "epoch:23 step:21786 [D loss: 0.656275, acc: 62.50%] [G loss: 1.751742]\n",
      "epoch:23 step:21787 [D loss: 0.698518, acc: 55.47%] [G loss: 1.954416]\n",
      "epoch:23 step:21788 [D loss: 0.653712, acc: 60.16%] [G loss: 1.900070]\n",
      "epoch:23 step:21789 [D loss: 0.631412, acc: 63.28%] [G loss: 1.905532]\n",
      "epoch:23 step:21790 [D loss: 0.653660, acc: 58.59%] [G loss: 1.989071]\n",
      "epoch:23 step:21791 [D loss: 0.639706, acc: 63.28%] [G loss: 1.991352]\n",
      "epoch:23 step:21792 [D loss: 0.614229, acc: 65.62%] [G loss: 2.093378]\n",
      "epoch:23 step:21793 [D loss: 0.617813, acc: 67.19%] [G loss: 2.052258]\n",
      "epoch:23 step:21794 [D loss: 0.623665, acc: 65.62%] [G loss: 2.027977]\n",
      "epoch:23 step:21795 [D loss: 0.601616, acc: 71.09%] [G loss: 1.845949]\n",
      "epoch:23 step:21796 [D loss: 0.649823, acc: 56.25%] [G loss: 2.012399]\n",
      "epoch:23 step:21797 [D loss: 0.683647, acc: 60.16%] [G loss: 1.904576]\n",
      "epoch:23 step:21798 [D loss: 0.627662, acc: 66.41%] [G loss: 1.973297]\n",
      "epoch:23 step:21799 [D loss: 0.645165, acc: 65.62%] [G loss: 1.993586]\n",
      "epoch:23 step:21800 [D loss: 0.670924, acc: 63.28%] [G loss: 1.823776]\n",
      "##############\n",
      "[2.56894693 1.39419334 6.12857833 4.81628917 3.6927105  5.63878288\n",
      " 4.43717694 4.71163747 4.6187207  3.77371758]\n",
      "##########\n",
      "epoch:23 step:21801 [D loss: 0.697548, acc: 50.78%] [G loss: 1.681227]\n",
      "epoch:23 step:21802 [D loss: 0.648066, acc: 63.28%] [G loss: 1.735310]\n",
      "epoch:23 step:21803 [D loss: 0.670145, acc: 56.25%] [G loss: 1.675596]\n",
      "epoch:23 step:21804 [D loss: 0.658581, acc: 60.94%] [G loss: 1.733642]\n",
      "epoch:23 step:21805 [D loss: 0.677074, acc: 60.94%] [G loss: 1.677594]\n",
      "epoch:23 step:21806 [D loss: 0.632149, acc: 63.28%] [G loss: 1.783065]\n",
      "epoch:23 step:21807 [D loss: 0.633369, acc: 64.84%] [G loss: 1.800651]\n",
      "epoch:23 step:21808 [D loss: 0.659260, acc: 61.72%] [G loss: 1.960010]\n",
      "epoch:23 step:21809 [D loss: 0.692155, acc: 60.94%] [G loss: 1.730582]\n",
      "epoch:23 step:21810 [D loss: 0.642567, acc: 64.84%] [G loss: 1.905838]\n",
      "epoch:23 step:21811 [D loss: 0.642623, acc: 60.16%] [G loss: 1.804446]\n",
      "epoch:23 step:21812 [D loss: 0.638883, acc: 64.06%] [G loss: 1.832564]\n",
      "epoch:23 step:21813 [D loss: 0.602422, acc: 67.19%] [G loss: 1.840067]\n",
      "epoch:23 step:21814 [D loss: 0.634073, acc: 61.72%] [G loss: 2.019163]\n",
      "epoch:23 step:21815 [D loss: 0.637197, acc: 64.84%] [G loss: 2.003408]\n",
      "epoch:23 step:21816 [D loss: 0.648641, acc: 61.72%] [G loss: 1.922539]\n",
      "epoch:23 step:21817 [D loss: 0.625064, acc: 64.84%] [G loss: 1.986456]\n",
      "epoch:23 step:21818 [D loss: 0.665738, acc: 63.28%] [G loss: 1.828609]\n",
      "epoch:23 step:21819 [D loss: 0.650917, acc: 61.72%] [G loss: 1.797262]\n",
      "epoch:23 step:21820 [D loss: 0.651484, acc: 56.25%] [G loss: 2.058425]\n",
      "epoch:23 step:21821 [D loss: 0.607972, acc: 66.41%] [G loss: 2.011737]\n",
      "epoch:23 step:21822 [D loss: 0.672825, acc: 67.97%] [G loss: 1.786935]\n",
      "epoch:23 step:21823 [D loss: 0.609600, acc: 67.19%] [G loss: 1.991401]\n",
      "epoch:23 step:21824 [D loss: 0.671983, acc: 57.81%] [G loss: 1.766917]\n",
      "epoch:23 step:21825 [D loss: 0.617387, acc: 64.06%] [G loss: 2.008539]\n",
      "epoch:23 step:21826 [D loss: 0.627676, acc: 67.19%] [G loss: 2.066546]\n",
      "epoch:23 step:21827 [D loss: 0.558632, acc: 67.19%] [G loss: 2.134124]\n",
      "epoch:23 step:21828 [D loss: 0.658134, acc: 62.50%] [G loss: 1.876562]\n",
      "epoch:23 step:21829 [D loss: 0.640404, acc: 64.84%] [G loss: 1.950470]\n",
      "epoch:23 step:21830 [D loss: 0.635710, acc: 66.41%] [G loss: 1.969231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21831 [D loss: 0.696725, acc: 60.16%] [G loss: 1.960194]\n",
      "epoch:23 step:21832 [D loss: 0.682706, acc: 54.69%] [G loss: 1.752209]\n",
      "epoch:23 step:21833 [D loss: 0.649282, acc: 62.50%] [G loss: 1.800133]\n",
      "epoch:23 step:21834 [D loss: 0.666758, acc: 62.50%] [G loss: 1.782417]\n",
      "epoch:23 step:21835 [D loss: 0.666496, acc: 61.72%] [G loss: 1.948586]\n",
      "epoch:23 step:21836 [D loss: 0.648937, acc: 65.62%] [G loss: 1.866237]\n",
      "epoch:23 step:21837 [D loss: 0.630095, acc: 64.84%] [G loss: 1.851887]\n",
      "epoch:23 step:21838 [D loss: 0.660736, acc: 59.38%] [G loss: 1.915256]\n",
      "epoch:23 step:21839 [D loss: 0.669248, acc: 60.94%] [G loss: 1.872497]\n",
      "epoch:23 step:21840 [D loss: 0.675212, acc: 59.38%] [G loss: 1.823877]\n",
      "epoch:23 step:21841 [D loss: 0.677914, acc: 52.34%] [G loss: 1.902884]\n",
      "epoch:23 step:21842 [D loss: 0.604612, acc: 66.41%] [G loss: 1.862146]\n",
      "epoch:23 step:21843 [D loss: 0.665044, acc: 64.84%] [G loss: 1.956259]\n",
      "epoch:23 step:21844 [D loss: 0.627198, acc: 64.84%] [G loss: 2.014500]\n",
      "epoch:23 step:21845 [D loss: 0.674902, acc: 61.72%] [G loss: 1.877082]\n",
      "epoch:23 step:21846 [D loss: 0.645712, acc: 60.94%] [G loss: 1.905648]\n",
      "epoch:23 step:21847 [D loss: 0.600238, acc: 71.88%] [G loss: 1.949213]\n",
      "epoch:23 step:21848 [D loss: 0.613880, acc: 65.62%] [G loss: 1.823227]\n",
      "epoch:23 step:21849 [D loss: 0.614772, acc: 67.19%] [G loss: 1.936229]\n",
      "epoch:23 step:21850 [D loss: 0.602362, acc: 62.50%] [G loss: 2.030920]\n",
      "epoch:23 step:21851 [D loss: 0.611033, acc: 68.75%] [G loss: 2.064635]\n",
      "epoch:23 step:21852 [D loss: 0.689879, acc: 58.59%] [G loss: 1.787628]\n",
      "epoch:23 step:21853 [D loss: 0.649412, acc: 58.59%] [G loss: 1.902792]\n",
      "epoch:23 step:21854 [D loss: 0.692326, acc: 58.59%] [G loss: 1.846888]\n",
      "epoch:23 step:21855 [D loss: 0.644370, acc: 61.72%] [G loss: 1.847310]\n",
      "epoch:23 step:21856 [D loss: 0.669842, acc: 59.38%] [G loss: 1.727522]\n",
      "epoch:23 step:21857 [D loss: 0.652394, acc: 60.94%] [G loss: 1.693589]\n",
      "epoch:23 step:21858 [D loss: 0.594692, acc: 70.31%] [G loss: 1.871057]\n",
      "epoch:23 step:21859 [D loss: 0.604214, acc: 67.19%] [G loss: 1.970762]\n",
      "epoch:23 step:21860 [D loss: 0.634930, acc: 64.84%] [G loss: 1.878888]\n",
      "epoch:23 step:21861 [D loss: 0.679768, acc: 57.03%] [G loss: 1.967913]\n",
      "epoch:23 step:21862 [D loss: 0.670470, acc: 64.06%] [G loss: 1.763080]\n",
      "epoch:23 step:21863 [D loss: 0.629688, acc: 64.06%] [G loss: 2.131502]\n",
      "epoch:23 step:21864 [D loss: 0.597286, acc: 65.62%] [G loss: 1.974573]\n",
      "epoch:23 step:21865 [D loss: 0.597396, acc: 69.53%] [G loss: 2.064555]\n",
      "epoch:23 step:21866 [D loss: 0.581367, acc: 74.22%] [G loss: 2.125054]\n",
      "epoch:23 step:21867 [D loss: 0.692542, acc: 57.03%] [G loss: 1.775088]\n",
      "epoch:23 step:21868 [D loss: 0.661612, acc: 63.28%] [G loss: 1.876132]\n",
      "epoch:23 step:21869 [D loss: 0.626953, acc: 64.06%] [G loss: 2.058981]\n",
      "epoch:23 step:21870 [D loss: 0.622809, acc: 63.28%] [G loss: 1.883246]\n",
      "epoch:23 step:21871 [D loss: 0.615428, acc: 64.06%] [G loss: 1.834964]\n",
      "epoch:23 step:21872 [D loss: 0.633195, acc: 60.94%] [G loss: 1.981722]\n",
      "epoch:23 step:21873 [D loss: 0.646176, acc: 56.25%] [G loss: 1.750127]\n",
      "epoch:23 step:21874 [D loss: 0.709399, acc: 54.69%] [G loss: 1.785950]\n",
      "epoch:23 step:21875 [D loss: 0.624859, acc: 69.53%] [G loss: 1.809109]\n",
      "epoch:23 step:21876 [D loss: 0.627324, acc: 64.06%] [G loss: 1.911813]\n",
      "epoch:23 step:21877 [D loss: 0.656235, acc: 65.62%] [G loss: 1.874961]\n",
      "epoch:23 step:21878 [D loss: 0.601173, acc: 64.06%] [G loss: 1.773104]\n",
      "epoch:23 step:21879 [D loss: 0.639296, acc: 64.84%] [G loss: 1.884755]\n",
      "epoch:23 step:21880 [D loss: 0.668928, acc: 61.72%] [G loss: 1.929842]\n",
      "epoch:23 step:21881 [D loss: 0.625795, acc: 65.62%] [G loss: 1.985003]\n",
      "epoch:23 step:21882 [D loss: 0.627447, acc: 61.72%] [G loss: 1.921871]\n",
      "epoch:23 step:21883 [D loss: 0.597047, acc: 64.06%] [G loss: 2.040299]\n",
      "epoch:23 step:21884 [D loss: 0.636662, acc: 64.06%] [G loss: 1.888616]\n",
      "epoch:23 step:21885 [D loss: 0.645850, acc: 63.28%] [G loss: 2.007085]\n",
      "epoch:23 step:21886 [D loss: 0.639816, acc: 60.94%] [G loss: 1.975897]\n",
      "epoch:23 step:21887 [D loss: 0.662310, acc: 65.62%] [G loss: 2.059299]\n",
      "epoch:23 step:21888 [D loss: 0.627819, acc: 65.62%] [G loss: 1.933172]\n",
      "epoch:23 step:21889 [D loss: 0.659589, acc: 64.06%] [G loss: 1.985229]\n",
      "epoch:23 step:21890 [D loss: 0.637395, acc: 64.06%] [G loss: 1.853823]\n",
      "epoch:23 step:21891 [D loss: 0.576014, acc: 66.41%] [G loss: 2.003458]\n",
      "epoch:23 step:21892 [D loss: 0.695204, acc: 53.91%] [G loss: 1.793178]\n",
      "epoch:23 step:21893 [D loss: 0.718087, acc: 53.12%] [G loss: 1.816278]\n",
      "epoch:23 step:21894 [D loss: 0.667792, acc: 65.62%] [G loss: 1.861013]\n",
      "epoch:23 step:21895 [D loss: 0.659494, acc: 67.97%] [G loss: 1.907321]\n",
      "epoch:23 step:21896 [D loss: 0.603534, acc: 64.84%] [G loss: 2.179541]\n",
      "epoch:23 step:21897 [D loss: 0.559096, acc: 66.41%] [G loss: 2.202042]\n",
      "epoch:23 step:21898 [D loss: 0.586745, acc: 68.75%] [G loss: 2.493970]\n",
      "epoch:23 step:21899 [D loss: 0.624477, acc: 62.50%] [G loss: 1.994634]\n",
      "epoch:23 step:21900 [D loss: 0.695320, acc: 58.59%] [G loss: 1.877312]\n",
      "epoch:23 step:21901 [D loss: 0.632179, acc: 64.06%] [G loss: 1.944818]\n",
      "epoch:23 step:21902 [D loss: 0.610383, acc: 64.84%] [G loss: 1.890565]\n",
      "epoch:23 step:21903 [D loss: 0.661376, acc: 58.59%] [G loss: 1.787150]\n",
      "epoch:23 step:21904 [D loss: 0.692778, acc: 56.25%] [G loss: 1.876933]\n",
      "epoch:23 step:21905 [D loss: 0.669653, acc: 60.16%] [G loss: 2.039322]\n",
      "epoch:23 step:21906 [D loss: 0.695595, acc: 57.81%] [G loss: 1.720973]\n",
      "epoch:23 step:21907 [D loss: 0.618842, acc: 65.62%] [G loss: 1.722449]\n",
      "epoch:23 step:21908 [D loss: 0.640719, acc: 62.50%] [G loss: 1.759970]\n",
      "epoch:23 step:21909 [D loss: 0.618788, acc: 68.75%] [G loss: 1.856391]\n",
      "epoch:23 step:21910 [D loss: 0.625924, acc: 64.06%] [G loss: 1.994462]\n",
      "epoch:23 step:21911 [D loss: 0.604338, acc: 69.53%] [G loss: 1.858426]\n",
      "epoch:23 step:21912 [D loss: 0.650713, acc: 57.81%] [G loss: 1.796375]\n",
      "epoch:23 step:21913 [D loss: 0.624692, acc: 63.28%] [G loss: 1.755339]\n",
      "epoch:23 step:21914 [D loss: 0.703007, acc: 56.25%] [G loss: 1.952961]\n",
      "epoch:23 step:21915 [D loss: 0.699021, acc: 60.16%] [G loss: 1.927435]\n",
      "epoch:23 step:21916 [D loss: 0.676684, acc: 59.38%] [G loss: 1.876712]\n",
      "epoch:23 step:21917 [D loss: 0.618259, acc: 65.62%] [G loss: 1.833803]\n",
      "epoch:23 step:21918 [D loss: 0.643963, acc: 65.62%] [G loss: 1.930970]\n",
      "epoch:23 step:21919 [D loss: 0.683709, acc: 55.47%] [G loss: 1.842454]\n",
      "epoch:23 step:21920 [D loss: 0.653630, acc: 63.28%] [G loss: 1.818144]\n",
      "epoch:23 step:21921 [D loss: 0.623650, acc: 66.41%] [G loss: 1.993685]\n",
      "epoch:23 step:21922 [D loss: 0.594550, acc: 64.84%] [G loss: 1.834489]\n",
      "epoch:23 step:21923 [D loss: 0.672749, acc: 61.72%] [G loss: 1.846174]\n",
      "epoch:23 step:21924 [D loss: 0.726937, acc: 54.69%] [G loss: 1.745291]\n",
      "epoch:23 step:21925 [D loss: 0.634003, acc: 67.97%] [G loss: 1.701566]\n",
      "epoch:23 step:21926 [D loss: 0.678124, acc: 54.69%] [G loss: 1.728897]\n",
      "epoch:23 step:21927 [D loss: 0.628145, acc: 66.41%] [G loss: 1.720307]\n",
      "epoch:23 step:21928 [D loss: 0.651963, acc: 64.84%] [G loss: 1.787219]\n",
      "epoch:23 step:21929 [D loss: 0.680923, acc: 59.38%] [G loss: 1.733801]\n",
      "epoch:23 step:21930 [D loss: 0.690727, acc: 54.69%] [G loss: 1.817048]\n",
      "epoch:23 step:21931 [D loss: 0.646823, acc: 60.16%] [G loss: 1.860202]\n",
      "epoch:23 step:21932 [D loss: 0.650234, acc: 67.19%] [G loss: 2.071930]\n",
      "epoch:23 step:21933 [D loss: 0.595698, acc: 69.53%] [G loss: 1.864076]\n",
      "epoch:23 step:21934 [D loss: 0.662809, acc: 58.59%] [G loss: 1.823122]\n",
      "epoch:23 step:21935 [D loss: 0.610980, acc: 67.19%] [G loss: 1.958700]\n",
      "epoch:23 step:21936 [D loss: 0.617436, acc: 66.41%] [G loss: 2.052186]\n",
      "epoch:23 step:21937 [D loss: 0.721926, acc: 49.22%] [G loss: 1.815577]\n",
      "epoch:23 step:21938 [D loss: 0.680229, acc: 57.81%] [G loss: 1.792943]\n",
      "epoch:23 step:21939 [D loss: 0.683236, acc: 57.03%] [G loss: 1.847629]\n",
      "epoch:23 step:21940 [D loss: 0.624140, acc: 67.19%] [G loss: 1.858428]\n",
      "epoch:23 step:21941 [D loss: 0.652544, acc: 57.81%] [G loss: 1.852936]\n",
      "epoch:23 step:21942 [D loss: 0.651618, acc: 56.25%] [G loss: 1.784189]\n",
      "epoch:23 step:21943 [D loss: 0.643569, acc: 61.72%] [G loss: 1.750157]\n",
      "epoch:23 step:21944 [D loss: 0.636364, acc: 65.62%] [G loss: 1.808963]\n",
      "epoch:23 step:21945 [D loss: 0.685431, acc: 60.16%] [G loss: 1.845178]\n",
      "epoch:23 step:21946 [D loss: 0.656596, acc: 60.94%] [G loss: 1.875978]\n",
      "epoch:23 step:21947 [D loss: 0.623954, acc: 61.72%] [G loss: 1.756077]\n",
      "epoch:23 step:21948 [D loss: 0.684723, acc: 54.69%] [G loss: 1.840495]\n",
      "epoch:23 step:21949 [D loss: 0.638010, acc: 61.72%] [G loss: 1.930931]\n",
      "epoch:23 step:21950 [D loss: 0.692294, acc: 62.50%] [G loss: 1.852816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21951 [D loss: 0.674868, acc: 54.69%] [G loss: 1.727143]\n",
      "epoch:23 step:21952 [D loss: 0.666800, acc: 57.03%] [G loss: 1.840765]\n",
      "epoch:23 step:21953 [D loss: 0.635670, acc: 69.53%] [G loss: 1.867880]\n",
      "epoch:23 step:21954 [D loss: 0.635482, acc: 62.50%] [G loss: 1.884575]\n",
      "epoch:23 step:21955 [D loss: 0.677152, acc: 58.59%] [G loss: 1.900970]\n",
      "epoch:23 step:21956 [D loss: 0.599501, acc: 68.75%] [G loss: 2.034466]\n",
      "epoch:23 step:21957 [D loss: 0.618265, acc: 66.41%] [G loss: 1.963546]\n",
      "epoch:23 step:21958 [D loss: 0.624295, acc: 62.50%] [G loss: 1.887845]\n",
      "epoch:23 step:21959 [D loss: 0.695226, acc: 59.38%] [G loss: 1.825998]\n",
      "epoch:23 step:21960 [D loss: 0.631544, acc: 66.41%] [G loss: 1.957209]\n",
      "epoch:23 step:21961 [D loss: 0.684504, acc: 57.03%] [G loss: 1.919977]\n",
      "epoch:23 step:21962 [D loss: 0.631996, acc: 67.97%] [G loss: 1.811705]\n",
      "epoch:23 step:21963 [D loss: 0.649443, acc: 66.41%] [G loss: 1.817470]\n",
      "epoch:23 step:21964 [D loss: 0.653848, acc: 62.50%] [G loss: 1.877150]\n",
      "epoch:23 step:21965 [D loss: 0.679016, acc: 57.03%] [G loss: 1.884898]\n",
      "epoch:23 step:21966 [D loss: 0.659730, acc: 59.38%] [G loss: 1.933559]\n",
      "epoch:23 step:21967 [D loss: 0.584301, acc: 70.31%] [G loss: 2.253678]\n",
      "epoch:23 step:21968 [D loss: 0.664811, acc: 68.75%] [G loss: 2.003269]\n",
      "epoch:23 step:21969 [D loss: 0.653428, acc: 64.84%] [G loss: 1.930092]\n",
      "epoch:23 step:21970 [D loss: 0.657698, acc: 63.28%] [G loss: 1.953971]\n",
      "epoch:23 step:21971 [D loss: 0.614637, acc: 60.94%] [G loss: 1.761056]\n",
      "epoch:23 step:21972 [D loss: 0.646278, acc: 59.38%] [G loss: 1.946879]\n",
      "epoch:23 step:21973 [D loss: 0.680880, acc: 60.16%] [G loss: 1.846945]\n",
      "epoch:23 step:21974 [D loss: 0.657508, acc: 61.72%] [G loss: 1.822460]\n",
      "epoch:23 step:21975 [D loss: 0.696283, acc: 58.59%] [G loss: 1.960350]\n",
      "epoch:23 step:21976 [D loss: 0.670643, acc: 58.59%] [G loss: 1.816554]\n",
      "epoch:23 step:21977 [D loss: 0.632073, acc: 70.31%] [G loss: 1.867559]\n",
      "epoch:23 step:21978 [D loss: 0.633813, acc: 63.28%] [G loss: 1.958548]\n",
      "epoch:23 step:21979 [D loss: 0.574505, acc: 71.09%] [G loss: 2.107066]\n",
      "epoch:23 step:21980 [D loss: 0.609084, acc: 67.97%] [G loss: 2.058564]\n",
      "epoch:23 step:21981 [D loss: 0.595139, acc: 67.19%] [G loss: 1.997121]\n",
      "epoch:23 step:21982 [D loss: 0.703259, acc: 61.72%] [G loss: 2.042951]\n",
      "epoch:23 step:21983 [D loss: 0.635483, acc: 62.50%] [G loss: 1.832190]\n",
      "epoch:23 step:21984 [D loss: 0.657169, acc: 54.69%] [G loss: 1.864838]\n",
      "epoch:23 step:21985 [D loss: 0.638404, acc: 59.38%] [G loss: 2.039269]\n",
      "epoch:23 step:21986 [D loss: 0.647871, acc: 63.28%] [G loss: 2.068266]\n",
      "epoch:23 step:21987 [D loss: 0.652554, acc: 64.84%] [G loss: 1.908599]\n",
      "epoch:23 step:21988 [D loss: 0.744887, acc: 55.47%] [G loss: 1.686016]\n",
      "epoch:23 step:21989 [D loss: 0.692717, acc: 58.59%] [G loss: 1.755122]\n",
      "epoch:23 step:21990 [D loss: 0.651230, acc: 62.50%] [G loss: 1.821604]\n",
      "epoch:23 step:21991 [D loss: 0.656134, acc: 58.59%] [G loss: 1.892694]\n",
      "epoch:23 step:21992 [D loss: 0.692716, acc: 54.69%] [G loss: 1.803722]\n",
      "epoch:23 step:21993 [D loss: 0.642263, acc: 64.06%] [G loss: 1.773638]\n",
      "epoch:23 step:21994 [D loss: 0.678477, acc: 57.81%] [G loss: 1.784634]\n",
      "epoch:23 step:21995 [D loss: 0.670914, acc: 60.16%] [G loss: 1.810640]\n",
      "epoch:23 step:21996 [D loss: 0.683857, acc: 59.38%] [G loss: 1.848614]\n",
      "epoch:23 step:21997 [D loss: 0.662968, acc: 61.72%] [G loss: 1.768227]\n",
      "epoch:23 step:21998 [D loss: 0.646738, acc: 64.06%] [G loss: 1.767367]\n",
      "epoch:23 step:21999 [D loss: 0.660489, acc: 60.16%] [G loss: 1.874525]\n",
      "epoch:23 step:22000 [D loss: 0.645137, acc: 60.94%] [G loss: 1.815859]\n",
      "##############\n",
      "[2.4436301  1.56337962 6.14026279 4.62416431 3.6089795  5.59137019\n",
      " 4.36226555 4.56073047 4.49339182 3.43278777]\n",
      "##########\n",
      "epoch:23 step:22001 [D loss: 0.637072, acc: 62.50%] [G loss: 1.826321]\n",
      "epoch:23 step:22002 [D loss: 0.595472, acc: 71.09%] [G loss: 1.897341]\n",
      "epoch:23 step:22003 [D loss: 0.617070, acc: 70.31%] [G loss: 1.760510]\n",
      "epoch:23 step:22004 [D loss: 0.646766, acc: 63.28%] [G loss: 1.912045]\n",
      "epoch:23 step:22005 [D loss: 0.646082, acc: 67.19%] [G loss: 1.828357]\n",
      "epoch:23 step:22006 [D loss: 0.587854, acc: 67.97%] [G loss: 1.922652]\n",
      "epoch:23 step:22007 [D loss: 0.594887, acc: 69.53%] [G loss: 1.906005]\n",
      "epoch:23 step:22008 [D loss: 0.608335, acc: 68.75%] [G loss: 1.865708]\n",
      "epoch:23 step:22009 [D loss: 0.694071, acc: 53.12%] [G loss: 1.812331]\n",
      "epoch:23 step:22010 [D loss: 0.633571, acc: 64.84%] [G loss: 1.822736]\n",
      "epoch:23 step:22011 [D loss: 0.735571, acc: 52.34%] [G loss: 1.774064]\n",
      "epoch:23 step:22012 [D loss: 0.631415, acc: 67.97%] [G loss: 1.770329]\n",
      "epoch:23 step:22013 [D loss: 0.665745, acc: 58.59%] [G loss: 1.850293]\n",
      "epoch:23 step:22014 [D loss: 0.685029, acc: 56.25%] [G loss: 1.914993]\n",
      "epoch:23 step:22015 [D loss: 0.596116, acc: 64.84%] [G loss: 1.932477]\n",
      "epoch:23 step:22016 [D loss: 0.688311, acc: 57.03%] [G loss: 1.808529]\n",
      "epoch:23 step:22017 [D loss: 0.697993, acc: 53.12%] [G loss: 1.910987]\n",
      "epoch:23 step:22018 [D loss: 0.653651, acc: 62.50%] [G loss: 1.838876]\n",
      "epoch:23 step:22019 [D loss: 0.609662, acc: 67.19%] [G loss: 2.126894]\n",
      "epoch:23 step:22020 [D loss: 0.579391, acc: 73.44%] [G loss: 2.191553]\n",
      "epoch:23 step:22021 [D loss: 0.600175, acc: 67.19%] [G loss: 1.993433]\n",
      "epoch:23 step:22022 [D loss: 0.594865, acc: 69.53%] [G loss: 2.187088]\n",
      "epoch:23 step:22023 [D loss: 0.614874, acc: 68.75%] [G loss: 2.056404]\n",
      "epoch:23 step:22024 [D loss: 0.765759, acc: 52.34%] [G loss: 1.793187]\n",
      "epoch:23 step:22025 [D loss: 0.676367, acc: 55.47%] [G loss: 1.808018]\n",
      "epoch:23 step:22026 [D loss: 0.647571, acc: 64.06%] [G loss: 1.877468]\n",
      "epoch:23 step:22027 [D loss: 0.646262, acc: 58.59%] [G loss: 1.994828]\n",
      "epoch:23 step:22028 [D loss: 0.649248, acc: 61.72%] [G loss: 1.807773]\n",
      "epoch:23 step:22029 [D loss: 0.714563, acc: 53.91%] [G loss: 1.731787]\n",
      "epoch:23 step:22030 [D loss: 0.637097, acc: 64.84%] [G loss: 1.989028]\n",
      "epoch:23 step:22031 [D loss: 0.621744, acc: 65.62%] [G loss: 1.984778]\n",
      "epoch:23 step:22032 [D loss: 0.574371, acc: 73.44%] [G loss: 2.158032]\n",
      "epoch:23 step:22033 [D loss: 0.730565, acc: 50.78%] [G loss: 1.737291]\n",
      "epoch:23 step:22034 [D loss: 0.631698, acc: 62.50%] [G loss: 1.911523]\n",
      "epoch:23 step:22035 [D loss: 0.611957, acc: 66.41%] [G loss: 1.929659]\n",
      "epoch:23 step:22036 [D loss: 0.677030, acc: 60.94%] [G loss: 1.932651]\n",
      "epoch:23 step:22037 [D loss: 0.628865, acc: 69.53%] [G loss: 1.856246]\n",
      "epoch:23 step:22038 [D loss: 0.647392, acc: 62.50%] [G loss: 1.951249]\n",
      "epoch:23 step:22039 [D loss: 0.599691, acc: 67.97%] [G loss: 2.072311]\n",
      "epoch:23 step:22040 [D loss: 0.672225, acc: 60.16%] [G loss: 1.811868]\n",
      "epoch:23 step:22041 [D loss: 0.760562, acc: 51.56%] [G loss: 1.792110]\n",
      "epoch:23 step:22042 [D loss: 0.692994, acc: 53.12%] [G loss: 1.909263]\n",
      "epoch:23 step:22043 [D loss: 0.654665, acc: 63.28%] [G loss: 1.824733]\n",
      "epoch:23 step:22044 [D loss: 0.607732, acc: 64.06%] [G loss: 1.812558]\n",
      "epoch:23 step:22045 [D loss: 0.641650, acc: 61.72%] [G loss: 1.830244]\n",
      "epoch:23 step:22046 [D loss: 0.615542, acc: 71.09%] [G loss: 2.094818]\n",
      "epoch:23 step:22047 [D loss: 0.615010, acc: 61.72%] [G loss: 1.951539]\n",
      "epoch:23 step:22048 [D loss: 0.619127, acc: 67.19%] [G loss: 2.047342]\n",
      "epoch:23 step:22049 [D loss: 0.628602, acc: 66.41%] [G loss: 2.036911]\n",
      "epoch:23 step:22050 [D loss: 0.595044, acc: 68.75%] [G loss: 2.057868]\n",
      "epoch:23 step:22051 [D loss: 0.709289, acc: 53.12%] [G loss: 1.840593]\n",
      "epoch:23 step:22052 [D loss: 0.730824, acc: 55.47%] [G loss: 1.721399]\n",
      "epoch:23 step:22053 [D loss: 0.684428, acc: 56.25%] [G loss: 1.762874]\n",
      "epoch:23 step:22054 [D loss: 0.727100, acc: 54.69%] [G loss: 1.730570]\n",
      "epoch:23 step:22055 [D loss: 0.595689, acc: 67.97%] [G loss: 1.916078]\n",
      "epoch:23 step:22056 [D loss: 0.623018, acc: 66.41%] [G loss: 1.771645]\n",
      "epoch:23 step:22057 [D loss: 0.698210, acc: 53.91%] [G loss: 1.744887]\n",
      "epoch:23 step:22058 [D loss: 0.654816, acc: 58.59%] [G loss: 1.787119]\n",
      "epoch:23 step:22059 [D loss: 0.647561, acc: 57.81%] [G loss: 1.888773]\n",
      "epoch:23 step:22060 [D loss: 0.619211, acc: 63.28%] [G loss: 1.813855]\n",
      "epoch:23 step:22061 [D loss: 0.640831, acc: 63.28%] [G loss: 1.814031]\n",
      "epoch:23 step:22062 [D loss: 0.679624, acc: 58.59%] [G loss: 1.797410]\n",
      "epoch:23 step:22063 [D loss: 0.624064, acc: 64.06%] [G loss: 1.878623]\n",
      "epoch:23 step:22064 [D loss: 0.594938, acc: 70.31%] [G loss: 2.041657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22065 [D loss: 0.618296, acc: 63.28%] [G loss: 1.911223]\n",
      "epoch:23 step:22066 [D loss: 0.586729, acc: 71.09%] [G loss: 1.862729]\n",
      "epoch:23 step:22067 [D loss: 0.630134, acc: 66.41%] [G loss: 2.108637]\n",
      "epoch:23 step:22068 [D loss: 0.634943, acc: 64.06%] [G loss: 1.977075]\n",
      "epoch:23 step:22069 [D loss: 0.723188, acc: 51.56%] [G loss: 1.700743]\n",
      "epoch:23 step:22070 [D loss: 0.669140, acc: 58.59%] [G loss: 1.845015]\n",
      "epoch:23 step:22071 [D loss: 0.648868, acc: 63.28%] [G loss: 1.920412]\n",
      "epoch:23 step:22072 [D loss: 0.612839, acc: 67.19%] [G loss: 1.951589]\n",
      "epoch:23 step:22073 [D loss: 0.664094, acc: 58.59%] [G loss: 1.982919]\n",
      "epoch:23 step:22074 [D loss: 0.609033, acc: 71.88%] [G loss: 1.973366]\n",
      "epoch:23 step:22075 [D loss: 0.609844, acc: 68.75%] [G loss: 1.926020]\n",
      "epoch:23 step:22076 [D loss: 0.662629, acc: 53.12%] [G loss: 1.953410]\n",
      "epoch:23 step:22077 [D loss: 0.663815, acc: 65.62%] [G loss: 1.897344]\n",
      "epoch:23 step:22078 [D loss: 0.665319, acc: 58.59%] [G loss: 1.876274]\n",
      "epoch:23 step:22079 [D loss: 0.730244, acc: 51.56%] [G loss: 1.714087]\n",
      "epoch:23 step:22080 [D loss: 0.666227, acc: 60.16%] [G loss: 1.734785]\n",
      "epoch:23 step:22081 [D loss: 0.662352, acc: 58.59%] [G loss: 1.752336]\n",
      "epoch:23 step:22082 [D loss: 0.660978, acc: 58.59%] [G loss: 1.771864]\n",
      "epoch:23 step:22083 [D loss: 0.646905, acc: 66.41%] [G loss: 1.849055]\n",
      "epoch:23 step:22084 [D loss: 0.623072, acc: 57.03%] [G loss: 1.768270]\n",
      "epoch:23 step:22085 [D loss: 0.626987, acc: 64.84%] [G loss: 1.843540]\n",
      "epoch:23 step:22086 [D loss: 0.603858, acc: 67.19%] [G loss: 1.692227]\n",
      "epoch:23 step:22087 [D loss: 0.604210, acc: 64.84%] [G loss: 1.904594]\n",
      "epoch:23 step:22088 [D loss: 0.641341, acc: 57.81%] [G loss: 1.831114]\n",
      "epoch:23 step:22089 [D loss: 0.684785, acc: 53.91%] [G loss: 1.916368]\n",
      "epoch:23 step:22090 [D loss: 0.589939, acc: 70.31%] [G loss: 1.989930]\n",
      "epoch:23 step:22091 [D loss: 0.658885, acc: 57.81%] [G loss: 1.930899]\n",
      "epoch:23 step:22092 [D loss: 0.674063, acc: 57.03%] [G loss: 1.839291]\n",
      "epoch:23 step:22093 [D loss: 0.671286, acc: 57.03%] [G loss: 1.764735]\n",
      "epoch:23 step:22094 [D loss: 0.654679, acc: 60.16%] [G loss: 1.749588]\n",
      "epoch:23 step:22095 [D loss: 0.646478, acc: 62.50%] [G loss: 1.763911]\n",
      "epoch:23 step:22096 [D loss: 0.684900, acc: 54.69%] [G loss: 1.874758]\n",
      "epoch:23 step:22097 [D loss: 0.643102, acc: 62.50%] [G loss: 1.753302]\n",
      "epoch:23 step:22098 [D loss: 0.627363, acc: 64.84%] [G loss: 1.868479]\n",
      "epoch:23 step:22099 [D loss: 0.680210, acc: 64.06%] [G loss: 1.898822]\n",
      "epoch:23 step:22100 [D loss: 0.650477, acc: 65.62%] [G loss: 1.896131]\n",
      "epoch:23 step:22101 [D loss: 0.624084, acc: 64.84%] [G loss: 1.914193]\n",
      "epoch:23 step:22102 [D loss: 0.593188, acc: 65.62%] [G loss: 1.960397]\n",
      "epoch:23 step:22103 [D loss: 0.600962, acc: 64.06%] [G loss: 1.897565]\n",
      "epoch:23 step:22104 [D loss: 0.711625, acc: 51.56%] [G loss: 1.831892]\n",
      "epoch:23 step:22105 [D loss: 0.591405, acc: 74.22%] [G loss: 2.088484]\n",
      "epoch:23 step:22106 [D loss: 0.625020, acc: 65.62%] [G loss: 1.979572]\n",
      "epoch:23 step:22107 [D loss: 0.620279, acc: 67.97%] [G loss: 1.917735]\n",
      "epoch:23 step:22108 [D loss: 0.549951, acc: 70.31%] [G loss: 2.162443]\n",
      "epoch:23 step:22109 [D loss: 0.651206, acc: 60.94%] [G loss: 2.022647]\n",
      "epoch:23 step:22110 [D loss: 0.634241, acc: 64.84%] [G loss: 1.935240]\n",
      "epoch:23 step:22111 [D loss: 0.679156, acc: 54.69%] [G loss: 1.894889]\n",
      "epoch:23 step:22112 [D loss: 0.628755, acc: 69.53%] [G loss: 1.876400]\n",
      "epoch:23 step:22113 [D loss: 0.646630, acc: 61.72%] [G loss: 1.988847]\n",
      "epoch:23 step:22114 [D loss: 0.640640, acc: 64.84%] [G loss: 1.844335]\n",
      "epoch:23 step:22115 [D loss: 0.616866, acc: 64.06%] [G loss: 1.957317]\n",
      "epoch:23 step:22116 [D loss: 0.661216, acc: 54.69%] [G loss: 1.794555]\n",
      "epoch:23 step:22117 [D loss: 0.741222, acc: 51.56%] [G loss: 1.784991]\n",
      "epoch:23 step:22118 [D loss: 0.633580, acc: 64.06%] [G loss: 1.832711]\n",
      "epoch:23 step:22119 [D loss: 0.664372, acc: 59.38%] [G loss: 1.775841]\n",
      "epoch:23 step:22120 [D loss: 0.652597, acc: 60.94%] [G loss: 1.826293]\n",
      "epoch:23 step:22121 [D loss: 0.660629, acc: 64.06%] [G loss: 1.806057]\n",
      "epoch:23 step:22122 [D loss: 0.628164, acc: 66.41%] [G loss: 1.891224]\n",
      "epoch:23 step:22123 [D loss: 0.648965, acc: 62.50%] [G loss: 1.870250]\n",
      "epoch:23 step:22124 [D loss: 0.674707, acc: 60.16%] [G loss: 1.733070]\n",
      "epoch:23 step:22125 [D loss: 0.660577, acc: 60.94%] [G loss: 1.850762]\n",
      "epoch:23 step:22126 [D loss: 0.658383, acc: 60.16%] [G loss: 1.818750]\n",
      "epoch:23 step:22127 [D loss: 0.747201, acc: 46.09%] [G loss: 1.818123]\n",
      "epoch:23 step:22128 [D loss: 0.688034, acc: 60.94%] [G loss: 1.801075]\n",
      "epoch:23 step:22129 [D loss: 0.683564, acc: 57.81%] [G loss: 1.828795]\n",
      "epoch:23 step:22130 [D loss: 0.697887, acc: 57.03%] [G loss: 1.757575]\n",
      "epoch:23 step:22131 [D loss: 0.685079, acc: 60.94%] [G loss: 1.767035]\n",
      "epoch:23 step:22132 [D loss: 0.643733, acc: 60.94%] [G loss: 1.747936]\n",
      "epoch:23 step:22133 [D loss: 0.647350, acc: 55.47%] [G loss: 1.906239]\n",
      "epoch:23 step:22134 [D loss: 0.618829, acc: 68.75%] [G loss: 1.872813]\n",
      "epoch:23 step:22135 [D loss: 0.620039, acc: 64.84%] [G loss: 1.788151]\n",
      "epoch:23 step:22136 [D loss: 0.610176, acc: 62.50%] [G loss: 1.786254]\n",
      "epoch:23 step:22137 [D loss: 0.620491, acc: 67.97%] [G loss: 1.822943]\n",
      "epoch:23 step:22138 [D loss: 0.670163, acc: 55.47%] [G loss: 1.931651]\n",
      "epoch:23 step:22139 [D loss: 0.618985, acc: 62.50%] [G loss: 2.098964]\n",
      "epoch:23 step:22140 [D loss: 0.675841, acc: 55.47%] [G loss: 1.896730]\n",
      "epoch:23 step:22141 [D loss: 0.667503, acc: 60.16%] [G loss: 1.868985]\n",
      "epoch:23 step:22142 [D loss: 0.637830, acc: 67.19%] [G loss: 2.023994]\n",
      "epoch:23 step:22143 [D loss: 0.662467, acc: 59.38%] [G loss: 1.814176]\n",
      "epoch:23 step:22144 [D loss: 0.599770, acc: 69.53%] [G loss: 1.953074]\n",
      "epoch:23 step:22145 [D loss: 0.651829, acc: 64.84%] [G loss: 1.854454]\n",
      "epoch:23 step:22146 [D loss: 0.662506, acc: 63.28%] [G loss: 1.948764]\n",
      "epoch:23 step:22147 [D loss: 0.672866, acc: 58.59%] [G loss: 1.800064]\n",
      "epoch:23 step:22148 [D loss: 0.623563, acc: 71.09%] [G loss: 1.714120]\n",
      "epoch:23 step:22149 [D loss: 0.645796, acc: 61.72%] [G loss: 1.833151]\n",
      "epoch:23 step:22150 [D loss: 0.654120, acc: 58.59%] [G loss: 1.700352]\n",
      "epoch:23 step:22151 [D loss: 0.673370, acc: 59.38%] [G loss: 1.851680]\n",
      "epoch:23 step:22152 [D loss: 0.622844, acc: 66.41%] [G loss: 2.061483]\n",
      "epoch:23 step:22153 [D loss: 0.584221, acc: 67.97%] [G loss: 1.945955]\n",
      "epoch:23 step:22154 [D loss: 0.625459, acc: 67.97%] [G loss: 2.147102]\n",
      "epoch:23 step:22155 [D loss: 0.653487, acc: 65.62%] [G loss: 1.960393]\n",
      "epoch:23 step:22156 [D loss: 0.624109, acc: 66.41%] [G loss: 1.988969]\n",
      "epoch:23 step:22157 [D loss: 0.709252, acc: 55.47%] [G loss: 1.743600]\n",
      "epoch:23 step:22158 [D loss: 0.614404, acc: 61.72%] [G loss: 1.907893]\n",
      "epoch:23 step:22159 [D loss: 0.698248, acc: 58.59%] [G loss: 1.909545]\n",
      "epoch:23 step:22160 [D loss: 0.656279, acc: 59.38%] [G loss: 1.874211]\n",
      "epoch:23 step:22161 [D loss: 0.621170, acc: 61.72%] [G loss: 1.917236]\n",
      "epoch:23 step:22162 [D loss: 0.655451, acc: 65.62%] [G loss: 1.794517]\n",
      "epoch:23 step:22163 [D loss: 0.667860, acc: 56.25%] [G loss: 1.749247]\n",
      "epoch:23 step:22164 [D loss: 0.657833, acc: 60.16%] [G loss: 1.893424]\n",
      "epoch:23 step:22165 [D loss: 0.663195, acc: 58.59%] [G loss: 1.818173]\n",
      "epoch:23 step:22166 [D loss: 0.626102, acc: 65.62%] [G loss: 1.836034]\n",
      "epoch:23 step:22167 [D loss: 0.683504, acc: 63.28%] [G loss: 1.944373]\n",
      "epoch:23 step:22168 [D loss: 0.715130, acc: 53.12%] [G loss: 1.699034]\n",
      "epoch:23 step:22169 [D loss: 0.641381, acc: 63.28%] [G loss: 1.857535]\n",
      "epoch:23 step:22170 [D loss: 0.634563, acc: 65.62%] [G loss: 1.731041]\n",
      "epoch:23 step:22171 [D loss: 0.617024, acc: 67.19%] [G loss: 1.849667]\n",
      "epoch:23 step:22172 [D loss: 0.657780, acc: 59.38%] [G loss: 1.876768]\n",
      "epoch:23 step:22173 [D loss: 0.619958, acc: 62.50%] [G loss: 1.918934]\n",
      "epoch:23 step:22174 [D loss: 0.603613, acc: 67.19%] [G loss: 1.953458]\n",
      "epoch:23 step:22175 [D loss: 0.613841, acc: 62.50%] [G loss: 1.981659]\n",
      "epoch:23 step:22176 [D loss: 0.725483, acc: 52.34%] [G loss: 1.726070]\n",
      "epoch:23 step:22177 [D loss: 0.646705, acc: 59.38%] [G loss: 1.886824]\n",
      "epoch:23 step:22178 [D loss: 0.654735, acc: 57.81%] [G loss: 1.849039]\n",
      "epoch:23 step:22179 [D loss: 0.677232, acc: 52.34%] [G loss: 1.740615]\n",
      "epoch:23 step:22180 [D loss: 0.643098, acc: 64.84%] [G loss: 2.021925]\n",
      "epoch:23 step:22181 [D loss: 0.656814, acc: 57.03%] [G loss: 1.894083]\n",
      "epoch:23 step:22182 [D loss: 0.586299, acc: 66.41%] [G loss: 1.982070]\n",
      "epoch:23 step:22183 [D loss: 0.627243, acc: 63.28%] [G loss: 1.943632]\n",
      "epoch:23 step:22184 [D loss: 0.667719, acc: 56.25%] [G loss: 1.986469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22185 [D loss: 0.650808, acc: 58.59%] [G loss: 2.077455]\n",
      "epoch:23 step:22186 [D loss: 0.629592, acc: 61.72%] [G loss: 1.975523]\n",
      "epoch:23 step:22187 [D loss: 0.664614, acc: 57.03%] [G loss: 1.860331]\n",
      "epoch:23 step:22188 [D loss: 0.644803, acc: 64.84%] [G loss: 1.962280]\n",
      "epoch:23 step:22189 [D loss: 0.663504, acc: 60.94%] [G loss: 1.941095]\n",
      "epoch:23 step:22190 [D loss: 0.641567, acc: 60.16%] [G loss: 1.888038]\n",
      "epoch:23 step:22191 [D loss: 0.642110, acc: 66.41%] [G loss: 1.824981]\n",
      "epoch:23 step:22192 [D loss: 0.662980, acc: 57.81%] [G loss: 1.823853]\n",
      "epoch:23 step:22193 [D loss: 0.637158, acc: 62.50%] [G loss: 1.922924]\n",
      "epoch:23 step:22194 [D loss: 0.666874, acc: 64.06%] [G loss: 1.942112]\n",
      "epoch:23 step:22195 [D loss: 0.620786, acc: 67.97%] [G loss: 1.922108]\n",
      "epoch:23 step:22196 [D loss: 0.626524, acc: 61.72%] [G loss: 2.044118]\n",
      "epoch:23 step:22197 [D loss: 0.640714, acc: 63.28%] [G loss: 1.917875]\n",
      "epoch:23 step:22198 [D loss: 0.640606, acc: 59.38%] [G loss: 1.939240]\n",
      "epoch:23 step:22199 [D loss: 0.641448, acc: 62.50%] [G loss: 2.328948]\n",
      "epoch:23 step:22200 [D loss: 0.586219, acc: 72.66%] [G loss: 2.045931]\n",
      "##############\n",
      "[2.46215515 1.74788347 6.15435722 4.77301926 3.55422297 5.41995254\n",
      " 4.34441973 4.75859853 4.40292973 3.62630706]\n",
      "##########\n",
      "epoch:23 step:22201 [D loss: 0.634919, acc: 63.28%] [G loss: 2.043403]\n",
      "epoch:23 step:22202 [D loss: 0.597526, acc: 71.09%] [G loss: 1.995104]\n",
      "epoch:23 step:22203 [D loss: 0.622593, acc: 67.97%] [G loss: 1.980459]\n",
      "epoch:23 step:22204 [D loss: 0.592236, acc: 70.31%] [G loss: 2.090100]\n",
      "epoch:23 step:22205 [D loss: 0.675535, acc: 60.16%] [G loss: 1.942035]\n",
      "epoch:23 step:22206 [D loss: 0.667099, acc: 58.59%] [G loss: 1.845243]\n",
      "epoch:23 step:22207 [D loss: 0.692363, acc: 60.94%] [G loss: 1.856146]\n",
      "epoch:23 step:22208 [D loss: 0.702072, acc: 50.00%] [G loss: 1.724678]\n",
      "epoch:23 step:22209 [D loss: 0.650368, acc: 61.72%] [G loss: 1.791399]\n",
      "epoch:23 step:22210 [D loss: 0.680629, acc: 56.25%] [G loss: 1.834255]\n",
      "epoch:23 step:22211 [D loss: 0.588760, acc: 74.22%] [G loss: 1.979425]\n",
      "epoch:23 step:22212 [D loss: 0.642320, acc: 63.28%] [G loss: 2.075319]\n",
      "epoch:23 step:22213 [D loss: 0.742032, acc: 53.91%] [G loss: 1.917938]\n",
      "epoch:23 step:22214 [D loss: 0.640015, acc: 62.50%] [G loss: 1.842349]\n",
      "epoch:23 step:22215 [D loss: 0.690576, acc: 59.38%] [G loss: 1.808361]\n",
      "epoch:23 step:22216 [D loss: 0.677524, acc: 57.03%] [G loss: 1.909962]\n",
      "epoch:23 step:22217 [D loss: 0.659037, acc: 60.16%] [G loss: 1.802563]\n",
      "epoch:23 step:22218 [D loss: 0.653898, acc: 59.38%] [G loss: 1.822930]\n",
      "epoch:23 step:22219 [D loss: 0.666016, acc: 58.59%] [G loss: 1.782710]\n",
      "epoch:23 step:22220 [D loss: 0.652577, acc: 64.84%] [G loss: 1.689106]\n",
      "epoch:23 step:22221 [D loss: 0.682844, acc: 54.69%] [G loss: 1.789407]\n",
      "epoch:23 step:22222 [D loss: 0.629541, acc: 59.38%] [G loss: 1.814946]\n",
      "epoch:23 step:22223 [D loss: 0.631229, acc: 65.62%] [G loss: 1.806569]\n",
      "epoch:23 step:22224 [D loss: 0.655403, acc: 61.72%] [G loss: 1.858709]\n",
      "epoch:23 step:22225 [D loss: 0.620035, acc: 64.06%] [G loss: 1.924939]\n",
      "epoch:23 step:22226 [D loss: 0.697900, acc: 59.38%] [G loss: 1.788833]\n",
      "epoch:23 step:22227 [D loss: 0.654718, acc: 60.94%] [G loss: 1.751278]\n",
      "epoch:23 step:22228 [D loss: 0.685243, acc: 57.03%] [G loss: 1.860959]\n",
      "epoch:23 step:22229 [D loss: 0.639119, acc: 69.53%] [G loss: 1.825632]\n",
      "epoch:23 step:22230 [D loss: 0.616949, acc: 71.88%] [G loss: 1.833739]\n",
      "epoch:23 step:22231 [D loss: 0.634132, acc: 64.84%] [G loss: 1.814849]\n",
      "epoch:23 step:22232 [D loss: 0.638563, acc: 64.84%] [G loss: 1.842759]\n",
      "epoch:23 step:22233 [D loss: 0.638648, acc: 60.94%] [G loss: 1.881016]\n",
      "epoch:23 step:22234 [D loss: 0.669608, acc: 60.94%] [G loss: 1.840940]\n",
      "epoch:23 step:22235 [D loss: 0.626245, acc: 64.06%] [G loss: 1.845868]\n",
      "epoch:23 step:22236 [D loss: 0.628890, acc: 67.19%] [G loss: 2.042466]\n",
      "epoch:23 step:22237 [D loss: 0.653103, acc: 64.06%] [G loss: 1.863751]\n",
      "epoch:23 step:22238 [D loss: 0.650107, acc: 61.72%] [G loss: 1.712109]\n",
      "epoch:23 step:22239 [D loss: 0.619588, acc: 62.50%] [G loss: 1.950162]\n",
      "epoch:23 step:22240 [D loss: 0.640449, acc: 64.84%] [G loss: 1.946778]\n",
      "epoch:23 step:22241 [D loss: 0.654839, acc: 60.16%] [G loss: 1.993873]\n",
      "epoch:23 step:22242 [D loss: 0.648551, acc: 60.16%] [G loss: 1.935975]\n",
      "epoch:23 step:22243 [D loss: 0.624977, acc: 65.62%] [G loss: 2.133794]\n",
      "epoch:23 step:22244 [D loss: 0.580984, acc: 74.22%] [G loss: 2.011605]\n",
      "epoch:23 step:22245 [D loss: 0.504129, acc: 79.69%] [G loss: 2.227916]\n",
      "epoch:23 step:22246 [D loss: 0.672116, acc: 63.28%] [G loss: 2.045981]\n",
      "epoch:23 step:22247 [D loss: 0.697233, acc: 57.03%] [G loss: 1.800419]\n",
      "epoch:23 step:22248 [D loss: 0.674079, acc: 57.03%] [G loss: 1.798996]\n",
      "epoch:23 step:22249 [D loss: 0.653402, acc: 64.84%] [G loss: 1.687913]\n",
      "epoch:23 step:22250 [D loss: 0.651817, acc: 63.28%] [G loss: 1.851324]\n",
      "epoch:23 step:22251 [D loss: 0.635070, acc: 66.41%] [G loss: 1.973479]\n",
      "epoch:23 step:22252 [D loss: 0.639643, acc: 64.84%] [G loss: 1.954482]\n",
      "epoch:23 step:22253 [D loss: 0.663379, acc: 60.16%] [G loss: 1.872758]\n",
      "epoch:23 step:22254 [D loss: 0.637233, acc: 60.16%] [G loss: 1.812952]\n",
      "epoch:23 step:22255 [D loss: 0.656871, acc: 62.50%] [G loss: 1.743670]\n",
      "epoch:23 step:22256 [D loss: 0.672893, acc: 56.25%] [G loss: 1.919708]\n",
      "epoch:23 step:22257 [D loss: 0.625962, acc: 64.06%] [G loss: 2.016560]\n",
      "epoch:23 step:22258 [D loss: 0.620367, acc: 68.75%] [G loss: 1.985074]\n",
      "epoch:23 step:22259 [D loss: 0.641438, acc: 62.50%] [G loss: 1.928076]\n",
      "epoch:23 step:22260 [D loss: 0.635688, acc: 62.50%] [G loss: 1.851897]\n",
      "epoch:23 step:22261 [D loss: 0.660798, acc: 60.94%] [G loss: 1.859626]\n",
      "epoch:23 step:22262 [D loss: 0.649441, acc: 59.38%] [G loss: 1.925600]\n",
      "epoch:23 step:22263 [D loss: 0.649688, acc: 66.41%] [G loss: 1.866336]\n",
      "epoch:23 step:22264 [D loss: 0.680982, acc: 59.38%] [G loss: 1.798414]\n",
      "epoch:23 step:22265 [D loss: 0.635090, acc: 60.94%] [G loss: 1.790676]\n",
      "epoch:23 step:22266 [D loss: 0.620004, acc: 62.50%] [G loss: 1.782028]\n",
      "epoch:23 step:22267 [D loss: 0.737153, acc: 55.47%] [G loss: 1.709736]\n",
      "epoch:23 step:22268 [D loss: 0.673691, acc: 63.28%] [G loss: 1.849863]\n",
      "epoch:23 step:22269 [D loss: 0.654306, acc: 60.94%] [G loss: 1.817158]\n",
      "epoch:23 step:22270 [D loss: 0.598633, acc: 67.19%] [G loss: 2.009013]\n",
      "epoch:23 step:22271 [D loss: 0.650247, acc: 66.41%] [G loss: 1.967067]\n",
      "epoch:23 step:22272 [D loss: 0.652720, acc: 60.94%] [G loss: 1.993065]\n",
      "epoch:23 step:22273 [D loss: 0.644582, acc: 63.28%] [G loss: 1.778449]\n",
      "epoch:23 step:22274 [D loss: 0.622511, acc: 64.06%] [G loss: 1.944050]\n",
      "epoch:23 step:22275 [D loss: 0.634501, acc: 62.50%] [G loss: 1.975795]\n",
      "epoch:23 step:22276 [D loss: 0.591234, acc: 66.41%] [G loss: 1.853978]\n",
      "epoch:23 step:22277 [D loss: 0.634006, acc: 61.72%] [G loss: 1.981191]\n",
      "epoch:23 step:22278 [D loss: 0.627455, acc: 65.62%] [G loss: 1.897227]\n",
      "epoch:23 step:22279 [D loss: 0.648385, acc: 62.50%] [G loss: 1.920215]\n",
      "epoch:23 step:22280 [D loss: 0.673105, acc: 60.94%] [G loss: 1.856307]\n",
      "epoch:23 step:22281 [D loss: 0.655084, acc: 66.41%] [G loss: 1.769832]\n",
      "epoch:23 step:22282 [D loss: 0.623057, acc: 64.06%] [G loss: 1.707244]\n",
      "epoch:23 step:22283 [D loss: 0.600814, acc: 72.66%] [G loss: 1.900306]\n",
      "epoch:23 step:22284 [D loss: 0.622766, acc: 65.62%] [G loss: 1.882035]\n",
      "epoch:23 step:22285 [D loss: 0.678690, acc: 57.03%] [G loss: 1.819095]\n",
      "epoch:23 step:22286 [D loss: 0.668505, acc: 56.25%] [G loss: 1.815772]\n",
      "epoch:23 step:22287 [D loss: 0.602034, acc: 71.88%] [G loss: 1.881446]\n",
      "epoch:23 step:22288 [D loss: 0.655541, acc: 64.06%] [G loss: 1.878082]\n",
      "epoch:23 step:22289 [D loss: 0.637608, acc: 63.28%] [G loss: 1.856692]\n",
      "epoch:23 step:22290 [D loss: 0.671549, acc: 56.25%] [G loss: 1.791943]\n",
      "epoch:23 step:22291 [D loss: 0.677178, acc: 59.38%] [G loss: 1.808942]\n",
      "epoch:23 step:22292 [D loss: 0.657340, acc: 64.84%] [G loss: 1.964026]\n",
      "epoch:23 step:22293 [D loss: 0.630695, acc: 62.50%] [G loss: 1.824088]\n",
      "epoch:23 step:22294 [D loss: 0.638773, acc: 65.62%] [G loss: 1.920090]\n",
      "epoch:23 step:22295 [D loss: 0.657682, acc: 63.28%] [G loss: 1.806139]\n",
      "epoch:23 step:22296 [D loss: 0.666818, acc: 51.56%] [G loss: 1.980496]\n",
      "epoch:23 step:22297 [D loss: 0.586596, acc: 74.22%] [G loss: 1.990088]\n",
      "epoch:23 step:22298 [D loss: 0.581805, acc: 72.66%] [G loss: 2.069225]\n",
      "epoch:23 step:22299 [D loss: 0.662960, acc: 61.72%] [G loss: 2.007330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22300 [D loss: 0.658035, acc: 60.94%] [G loss: 1.825489]\n",
      "epoch:23 step:22301 [D loss: 0.627264, acc: 58.59%] [G loss: 1.917914]\n",
      "epoch:23 step:22302 [D loss: 0.649920, acc: 57.81%] [G loss: 1.883132]\n",
      "epoch:23 step:22303 [D loss: 0.676702, acc: 57.81%] [G loss: 1.612656]\n",
      "epoch:23 step:22304 [D loss: 0.660550, acc: 57.03%] [G loss: 1.743420]\n",
      "epoch:23 step:22305 [D loss: 0.638460, acc: 66.41%] [G loss: 2.079129]\n",
      "epoch:23 step:22306 [D loss: 0.608131, acc: 64.06%] [G loss: 1.829351]\n",
      "epoch:23 step:22307 [D loss: 0.674582, acc: 56.25%] [G loss: 2.017006]\n",
      "epoch:23 step:22308 [D loss: 0.694368, acc: 57.81%] [G loss: 1.996879]\n",
      "epoch:23 step:22309 [D loss: 0.636227, acc: 65.62%] [G loss: 1.881089]\n",
      "epoch:23 step:22310 [D loss: 0.712039, acc: 57.03%] [G loss: 1.686455]\n",
      "epoch:23 step:22311 [D loss: 0.693974, acc: 61.72%] [G loss: 1.785164]\n",
      "epoch:23 step:22312 [D loss: 0.661692, acc: 58.59%] [G loss: 1.841452]\n",
      "epoch:23 step:22313 [D loss: 0.667809, acc: 58.59%] [G loss: 1.852213]\n",
      "epoch:23 step:22314 [D loss: 0.649256, acc: 60.16%] [G loss: 1.884758]\n",
      "epoch:23 step:22315 [D loss: 0.613498, acc: 67.19%] [G loss: 1.726670]\n",
      "epoch:23 step:22316 [D loss: 0.717043, acc: 56.25%] [G loss: 1.643878]\n",
      "epoch:23 step:22317 [D loss: 0.673580, acc: 62.50%] [G loss: 1.832489]\n",
      "epoch:23 step:22318 [D loss: 0.661981, acc: 63.28%] [G loss: 1.797664]\n",
      "epoch:23 step:22319 [D loss: 0.680958, acc: 57.03%] [G loss: 1.878149]\n",
      "epoch:23 step:22320 [D loss: 0.659371, acc: 57.03%] [G loss: 1.821811]\n",
      "epoch:23 step:22321 [D loss: 0.669871, acc: 54.69%] [G loss: 1.807678]\n",
      "epoch:23 step:22322 [D loss: 0.659938, acc: 67.19%] [G loss: 1.839048]\n",
      "epoch:23 step:22323 [D loss: 0.635528, acc: 64.06%] [G loss: 1.798328]\n",
      "epoch:23 step:22324 [D loss: 0.628364, acc: 63.28%] [G loss: 1.842700]\n",
      "epoch:23 step:22325 [D loss: 0.649564, acc: 62.50%] [G loss: 1.951836]\n",
      "epoch:23 step:22326 [D loss: 0.633489, acc: 64.84%] [G loss: 2.174683]\n",
      "epoch:23 step:22327 [D loss: 0.635044, acc: 60.94%] [G loss: 1.883237]\n",
      "epoch:23 step:22328 [D loss: 0.645310, acc: 64.06%] [G loss: 1.811543]\n",
      "epoch:23 step:22329 [D loss: 0.628007, acc: 65.62%] [G loss: 1.779108]\n",
      "epoch:23 step:22330 [D loss: 0.700604, acc: 53.12%] [G loss: 1.952920]\n",
      "epoch:23 step:22331 [D loss: 0.689239, acc: 55.47%] [G loss: 1.949845]\n",
      "epoch:23 step:22332 [D loss: 0.592384, acc: 68.75%] [G loss: 2.201939]\n",
      "epoch:23 step:22333 [D loss: 0.630656, acc: 59.38%] [G loss: 2.180365]\n",
      "epoch:23 step:22334 [D loss: 0.641197, acc: 61.72%] [G loss: 1.946688]\n",
      "epoch:23 step:22335 [D loss: 0.665518, acc: 60.94%] [G loss: 1.851062]\n",
      "epoch:23 step:22336 [D loss: 0.638108, acc: 66.41%] [G loss: 1.899083]\n",
      "epoch:23 step:22337 [D loss: 0.628307, acc: 64.06%] [G loss: 2.001090]\n",
      "epoch:23 step:22338 [D loss: 0.673254, acc: 59.38%] [G loss: 1.794576]\n",
      "epoch:23 step:22339 [D loss: 0.697238, acc: 50.00%] [G loss: 1.837894]\n",
      "epoch:23 step:22340 [D loss: 0.690894, acc: 55.47%] [G loss: 1.890594]\n",
      "epoch:23 step:22341 [D loss: 0.683910, acc: 57.03%] [G loss: 1.830884]\n",
      "epoch:23 step:22342 [D loss: 0.642514, acc: 62.50%] [G loss: 1.781382]\n",
      "epoch:23 step:22343 [D loss: 0.649745, acc: 65.62%] [G loss: 1.891749]\n",
      "epoch:23 step:22344 [D loss: 0.659317, acc: 59.38%] [G loss: 2.002930]\n",
      "epoch:23 step:22345 [D loss: 0.695048, acc: 53.91%] [G loss: 1.603743]\n",
      "epoch:23 step:22346 [D loss: 0.636842, acc: 63.28%] [G loss: 1.746142]\n",
      "epoch:23 step:22347 [D loss: 0.606940, acc: 66.41%] [G loss: 1.894991]\n",
      "epoch:23 step:22348 [D loss: 0.627110, acc: 64.06%] [G loss: 1.884734]\n",
      "epoch:23 step:22349 [D loss: 0.657031, acc: 58.59%] [G loss: 1.818022]\n",
      "epoch:23 step:22350 [D loss: 0.674441, acc: 63.28%] [G loss: 1.767581]\n",
      "epoch:23 step:22351 [D loss: 0.717360, acc: 54.69%] [G loss: 1.637278]\n",
      "epoch:23 step:22352 [D loss: 0.648595, acc: 62.50%] [G loss: 1.760016]\n",
      "epoch:23 step:22353 [D loss: 0.624029, acc: 67.97%] [G loss: 1.895660]\n",
      "epoch:23 step:22354 [D loss: 0.604722, acc: 71.09%] [G loss: 2.009875]\n",
      "epoch:23 step:22355 [D loss: 0.624465, acc: 65.62%] [G loss: 1.881215]\n",
      "epoch:23 step:22356 [D loss: 0.623692, acc: 60.94%] [G loss: 1.919493]\n",
      "epoch:23 step:22357 [D loss: 0.655426, acc: 61.72%] [G loss: 2.034426]\n",
      "epoch:23 step:22358 [D loss: 0.597350, acc: 68.75%] [G loss: 1.923563]\n",
      "epoch:23 step:22359 [D loss: 0.652043, acc: 57.03%] [G loss: 1.910566]\n",
      "epoch:23 step:22360 [D loss: 0.655325, acc: 67.19%] [G loss: 1.756140]\n",
      "epoch:23 step:22361 [D loss: 0.618008, acc: 68.75%] [G loss: 2.001644]\n",
      "epoch:23 step:22362 [D loss: 0.638231, acc: 66.41%] [G loss: 1.824052]\n",
      "epoch:23 step:22363 [D loss: 0.683775, acc: 58.59%] [G loss: 1.760865]\n",
      "epoch:23 step:22364 [D loss: 0.649525, acc: 66.41%] [G loss: 1.850600]\n",
      "epoch:23 step:22365 [D loss: 0.694212, acc: 57.03%] [G loss: 1.886976]\n",
      "epoch:23 step:22366 [D loss: 0.732118, acc: 55.47%] [G loss: 2.194046]\n",
      "epoch:23 step:22367 [D loss: 0.585917, acc: 67.97%] [G loss: 2.012852]\n",
      "epoch:23 step:22368 [D loss: 0.675755, acc: 60.94%] [G loss: 1.823218]\n",
      "epoch:23 step:22369 [D loss: 0.714063, acc: 52.34%] [G loss: 1.749732]\n",
      "epoch:23 step:22370 [D loss: 0.621463, acc: 62.50%] [G loss: 1.960120]\n",
      "epoch:23 step:22371 [D loss: 0.690387, acc: 59.38%] [G loss: 1.686348]\n",
      "epoch:23 step:22372 [D loss: 0.655965, acc: 53.12%] [G loss: 1.882222]\n",
      "epoch:23 step:22373 [D loss: 0.665981, acc: 63.28%] [G loss: 1.892548]\n",
      "epoch:23 step:22374 [D loss: 0.633236, acc: 64.84%] [G loss: 2.000895]\n",
      "epoch:23 step:22375 [D loss: 0.623993, acc: 68.75%] [G loss: 1.819407]\n",
      "epoch:23 step:22376 [D loss: 0.622561, acc: 67.97%] [G loss: 1.836932]\n",
      "epoch:23 step:22377 [D loss: 0.661384, acc: 60.16%] [G loss: 1.763687]\n",
      "epoch:23 step:22378 [D loss: 0.669531, acc: 60.94%] [G loss: 1.910851]\n",
      "epoch:23 step:22379 [D loss: 0.708336, acc: 52.34%] [G loss: 1.728202]\n",
      "epoch:23 step:22380 [D loss: 0.695569, acc: 50.78%] [G loss: 1.769810]\n",
      "epoch:23 step:22381 [D loss: 0.636211, acc: 60.16%] [G loss: 1.802301]\n",
      "epoch:23 step:22382 [D loss: 0.696163, acc: 57.03%] [G loss: 1.869396]\n",
      "epoch:23 step:22383 [D loss: 0.638901, acc: 66.41%] [G loss: 1.851727]\n",
      "epoch:23 step:22384 [D loss: 0.662396, acc: 60.94%] [G loss: 1.879711]\n",
      "epoch:23 step:22385 [D loss: 0.621905, acc: 66.41%] [G loss: 1.837593]\n",
      "epoch:23 step:22386 [D loss: 0.634022, acc: 62.50%] [G loss: 1.904458]\n",
      "epoch:23 step:22387 [D loss: 0.653077, acc: 58.59%] [G loss: 1.836504]\n",
      "epoch:23 step:22388 [D loss: 0.648335, acc: 60.94%] [G loss: 1.904658]\n",
      "epoch:23 step:22389 [D loss: 0.629269, acc: 64.06%] [G loss: 1.789293]\n",
      "epoch:23 step:22390 [D loss: 0.617321, acc: 67.19%] [G loss: 1.860586]\n",
      "epoch:23 step:22391 [D loss: 0.640106, acc: 60.16%] [G loss: 1.980285]\n",
      "epoch:23 step:22392 [D loss: 0.647975, acc: 60.94%] [G loss: 1.955976]\n",
      "epoch:23 step:22393 [D loss: 0.650236, acc: 62.50%] [G loss: 1.948528]\n",
      "epoch:23 step:22394 [D loss: 0.647721, acc: 68.75%] [G loss: 1.852904]\n",
      "epoch:23 step:22395 [D loss: 0.647732, acc: 60.94%] [G loss: 1.898094]\n",
      "epoch:23 step:22396 [D loss: 0.618751, acc: 64.84%] [G loss: 1.934219]\n",
      "epoch:23 step:22397 [D loss: 0.664174, acc: 60.94%] [G loss: 1.805814]\n",
      "epoch:23 step:22398 [D loss: 0.638226, acc: 63.28%] [G loss: 1.833412]\n",
      "epoch:23 step:22399 [D loss: 0.655898, acc: 58.59%] [G loss: 1.993139]\n",
      "epoch:23 step:22400 [D loss: 0.619784, acc: 71.88%] [G loss: 1.973292]\n",
      "##############\n",
      "[2.52992744 1.50363533 6.14029837 4.68009306 3.57858734 5.73328613\n",
      " 4.41084363 4.58460139 4.42594083 3.63109258]\n",
      "##########\n",
      "epoch:23 step:22401 [D loss: 0.654906, acc: 60.16%] [G loss: 1.906072]\n",
      "epoch:23 step:22402 [D loss: 0.671117, acc: 58.59%] [G loss: 1.818530]\n",
      "epoch:23 step:22403 [D loss: 0.623957, acc: 60.16%] [G loss: 1.819065]\n",
      "epoch:23 step:22404 [D loss: 0.646964, acc: 62.50%] [G loss: 1.836887]\n",
      "epoch:23 step:22405 [D loss: 0.640482, acc: 63.28%] [G loss: 1.841017]\n",
      "epoch:23 step:22406 [D loss: 0.652813, acc: 58.59%] [G loss: 1.773854]\n",
      "epoch:23 step:22407 [D loss: 0.707427, acc: 57.03%] [G loss: 1.722494]\n",
      "epoch:23 step:22408 [D loss: 0.639954, acc: 64.84%] [G loss: 1.822041]\n",
      "epoch:23 step:22409 [D loss: 0.675420, acc: 55.47%] [G loss: 1.784764]\n",
      "epoch:23 step:22410 [D loss: 0.683948, acc: 57.03%] [G loss: 1.704200]\n",
      "epoch:23 step:22411 [D loss: 0.633283, acc: 64.84%] [G loss: 1.977611]\n",
      "epoch:23 step:22412 [D loss: 0.607444, acc: 64.06%] [G loss: 1.894009]\n",
      "epoch:23 step:22413 [D loss: 0.693875, acc: 55.47%] [G loss: 1.768177]\n",
      "epoch:23 step:22414 [D loss: 0.601564, acc: 66.41%] [G loss: 1.847971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22415 [D loss: 0.663136, acc: 62.50%] [G loss: 1.718870]\n",
      "epoch:23 step:22416 [D loss: 0.654577, acc: 59.38%] [G loss: 1.691580]\n",
      "epoch:23 step:22417 [D loss: 0.601361, acc: 67.97%] [G loss: 1.809228]\n",
      "epoch:23 step:22418 [D loss: 0.683459, acc: 54.69%] [G loss: 1.834918]\n",
      "epoch:23 step:22419 [D loss: 0.653900, acc: 61.72%] [G loss: 1.926044]\n",
      "epoch:23 step:22420 [D loss: 0.647511, acc: 61.72%] [G loss: 1.708718]\n",
      "epoch:23 step:22421 [D loss: 0.682705, acc: 57.81%] [G loss: 1.864029]\n",
      "epoch:23 step:22422 [D loss: 0.706172, acc: 53.12%] [G loss: 1.750481]\n",
      "epoch:23 step:22423 [D loss: 0.640163, acc: 61.72%] [G loss: 1.775565]\n",
      "epoch:23 step:22424 [D loss: 0.678775, acc: 66.41%] [G loss: 1.752156]\n",
      "epoch:23 step:22425 [D loss: 0.683358, acc: 57.03%] [G loss: 1.907371]\n",
      "epoch:23 step:22426 [D loss: 0.607929, acc: 61.72%] [G loss: 1.999546]\n",
      "epoch:23 step:22427 [D loss: 0.646588, acc: 62.50%] [G loss: 1.785883]\n",
      "epoch:23 step:22428 [D loss: 0.649400, acc: 57.81%] [G loss: 1.788665]\n",
      "epoch:23 step:22429 [D loss: 0.687528, acc: 59.38%] [G loss: 1.788523]\n",
      "epoch:23 step:22430 [D loss: 0.615462, acc: 64.06%] [G loss: 1.800831]\n",
      "epoch:23 step:22431 [D loss: 0.643068, acc: 63.28%] [G loss: 1.842569]\n",
      "epoch:23 step:22432 [D loss: 0.624511, acc: 69.53%] [G loss: 1.880382]\n",
      "epoch:23 step:22433 [D loss: 0.623987, acc: 63.28%] [G loss: 1.920065]\n",
      "epoch:23 step:22434 [D loss: 0.678447, acc: 57.81%] [G loss: 1.797976]\n",
      "epoch:23 step:22435 [D loss: 0.615495, acc: 66.41%] [G loss: 1.746043]\n",
      "epoch:23 step:22436 [D loss: 0.672452, acc: 62.50%] [G loss: 1.900253]\n",
      "epoch:23 step:22437 [D loss: 0.614138, acc: 64.06%] [G loss: 1.914791]\n",
      "epoch:23 step:22438 [D loss: 0.646083, acc: 59.38%] [G loss: 1.861257]\n",
      "epoch:23 step:22439 [D loss: 0.643878, acc: 62.50%] [G loss: 1.936233]\n",
      "epoch:23 step:22440 [D loss: 0.630684, acc: 64.06%] [G loss: 1.895434]\n",
      "epoch:23 step:22441 [D loss: 0.624209, acc: 64.84%] [G loss: 1.966187]\n",
      "epoch:23 step:22442 [D loss: 0.627514, acc: 61.72%] [G loss: 1.764492]\n",
      "epoch:23 step:22443 [D loss: 0.669651, acc: 59.38%] [G loss: 1.949122]\n",
      "epoch:23 step:22444 [D loss: 0.654876, acc: 59.38%] [G loss: 2.000850]\n",
      "epoch:23 step:22445 [D loss: 0.650974, acc: 57.03%] [G loss: 2.033926]\n",
      "epoch:23 step:22446 [D loss: 0.665345, acc: 60.16%] [G loss: 1.828190]\n",
      "epoch:23 step:22447 [D loss: 0.698089, acc: 59.38%] [G loss: 1.820007]\n",
      "epoch:23 step:22448 [D loss: 0.655041, acc: 60.16%] [G loss: 1.968618]\n",
      "epoch:23 step:22449 [D loss: 0.637276, acc: 63.28%] [G loss: 1.888436]\n",
      "epoch:23 step:22450 [D loss: 0.654922, acc: 61.72%] [G loss: 1.974491]\n",
      "epoch:23 step:22451 [D loss: 0.608586, acc: 65.62%] [G loss: 1.973199]\n",
      "epoch:23 step:22452 [D loss: 0.648974, acc: 65.62%] [G loss: 1.919220]\n",
      "epoch:23 step:22453 [D loss: 0.658918, acc: 60.16%] [G loss: 1.850103]\n",
      "epoch:23 step:22454 [D loss: 0.612003, acc: 63.28%] [G loss: 1.865672]\n",
      "epoch:23 step:22455 [D loss: 0.645433, acc: 63.28%] [G loss: 2.056151]\n",
      "epoch:23 step:22456 [D loss: 0.630542, acc: 59.38%] [G loss: 1.842441]\n",
      "epoch:23 step:22457 [D loss: 0.595880, acc: 63.28%] [G loss: 1.913686]\n",
      "epoch:23 step:22458 [D loss: 0.617346, acc: 67.19%] [G loss: 1.880890]\n",
      "epoch:23 step:22459 [D loss: 0.584525, acc: 72.66%] [G loss: 1.978171]\n",
      "epoch:23 step:22460 [D loss: 0.609472, acc: 65.62%] [G loss: 2.207947]\n",
      "epoch:23 step:22461 [D loss: 0.679318, acc: 55.47%] [G loss: 2.034279]\n",
      "epoch:23 step:22462 [D loss: 0.678851, acc: 62.50%] [G loss: 1.938604]\n",
      "epoch:23 step:22463 [D loss: 0.559609, acc: 75.78%] [G loss: 2.373214]\n",
      "epoch:23 step:22464 [D loss: 0.678032, acc: 59.38%] [G loss: 1.920646]\n",
      "epoch:23 step:22465 [D loss: 0.694782, acc: 53.12%] [G loss: 1.869337]\n",
      "epoch:23 step:22466 [D loss: 0.660009, acc: 64.06%] [G loss: 1.970171]\n",
      "epoch:23 step:22467 [D loss: 0.614907, acc: 67.19%] [G loss: 1.970761]\n",
      "epoch:23 step:22468 [D loss: 0.687791, acc: 57.81%] [G loss: 1.951035]\n",
      "epoch:23 step:22469 [D loss: 0.602858, acc: 64.06%] [G loss: 2.147592]\n",
      "epoch:23 step:22470 [D loss: 0.543105, acc: 74.22%] [G loss: 2.053596]\n",
      "epoch:23 step:22471 [D loss: 0.730915, acc: 51.56%] [G loss: 1.984530]\n",
      "epoch:23 step:22472 [D loss: 0.676584, acc: 60.16%] [G loss: 1.941496]\n",
      "epoch:23 step:22473 [D loss: 0.582434, acc: 68.75%] [G loss: 1.913624]\n",
      "epoch:23 step:22474 [D loss: 0.615744, acc: 69.53%] [G loss: 2.085752]\n",
      "epoch:23 step:22475 [D loss: 0.576699, acc: 67.19%] [G loss: 2.082376]\n",
      "epoch:23 step:22476 [D loss: 0.615223, acc: 68.75%] [G loss: 2.104009]\n",
      "epoch:23 step:22477 [D loss: 0.645394, acc: 67.97%] [G loss: 2.197819]\n",
      "epoch:23 step:22478 [D loss: 0.611091, acc: 63.28%] [G loss: 1.996088]\n",
      "epoch:23 step:22479 [D loss: 0.738932, acc: 53.91%] [G loss: 1.763773]\n",
      "epoch:23 step:22480 [D loss: 0.714963, acc: 54.69%] [G loss: 1.810107]\n",
      "epoch:23 step:22481 [D loss: 0.565061, acc: 71.88%] [G loss: 2.117817]\n",
      "epoch:23 step:22482 [D loss: 0.657038, acc: 64.06%] [G loss: 1.946525]\n",
      "epoch:23 step:22483 [D loss: 0.642563, acc: 63.28%] [G loss: 1.944594]\n",
      "epoch:23 step:22484 [D loss: 0.698633, acc: 60.16%] [G loss: 2.100967]\n",
      "epoch:23 step:22485 [D loss: 0.582642, acc: 67.19%] [G loss: 2.027912]\n",
      "epoch:23 step:22486 [D loss: 0.602676, acc: 70.31%] [G loss: 2.003246]\n",
      "epoch:23 step:22487 [D loss: 0.601054, acc: 68.75%] [G loss: 2.137581]\n",
      "epoch:23 step:22488 [D loss: 0.602032, acc: 67.97%] [G loss: 2.613215]\n",
      "epoch:24 step:22489 [D loss: 0.659485, acc: 64.06%] [G loss: 1.980637]\n",
      "epoch:24 step:22490 [D loss: 0.596236, acc: 68.75%] [G loss: 1.954581]\n",
      "epoch:24 step:22491 [D loss: 0.670801, acc: 60.94%] [G loss: 1.943402]\n",
      "epoch:24 step:22492 [D loss: 0.713488, acc: 55.47%] [G loss: 1.915355]\n",
      "epoch:24 step:22493 [D loss: 0.622927, acc: 65.62%] [G loss: 1.997067]\n",
      "epoch:24 step:22494 [D loss: 0.629950, acc: 63.28%] [G loss: 1.839212]\n",
      "epoch:24 step:22495 [D loss: 0.647138, acc: 59.38%] [G loss: 1.875162]\n",
      "epoch:24 step:22496 [D loss: 0.651992, acc: 64.84%] [G loss: 1.932979]\n",
      "epoch:24 step:22497 [D loss: 0.629787, acc: 67.97%] [G loss: 2.098106]\n",
      "epoch:24 step:22498 [D loss: 0.571838, acc: 71.09%] [G loss: 2.015061]\n",
      "epoch:24 step:22499 [D loss: 0.597616, acc: 67.19%] [G loss: 1.923651]\n",
      "epoch:24 step:22500 [D loss: 0.646436, acc: 62.50%] [G loss: 1.966589]\n",
      "epoch:24 step:22501 [D loss: 0.601651, acc: 63.28%] [G loss: 2.081417]\n",
      "epoch:24 step:22502 [D loss: 0.646754, acc: 62.50%] [G loss: 1.844454]\n",
      "epoch:24 step:22503 [D loss: 0.644020, acc: 60.94%] [G loss: 2.006453]\n",
      "epoch:24 step:22504 [D loss: 0.621648, acc: 64.06%] [G loss: 1.987399]\n",
      "epoch:24 step:22505 [D loss: 0.669263, acc: 59.38%] [G loss: 1.978953]\n",
      "epoch:24 step:22506 [D loss: 0.625730, acc: 64.06%] [G loss: 1.825571]\n",
      "epoch:24 step:22507 [D loss: 0.643304, acc: 65.62%] [G loss: 1.813936]\n",
      "epoch:24 step:22508 [D loss: 0.749250, acc: 52.34%] [G loss: 1.724013]\n",
      "epoch:24 step:22509 [D loss: 0.715529, acc: 55.47%] [G loss: 1.720929]\n",
      "epoch:24 step:22510 [D loss: 0.683716, acc: 59.38%] [G loss: 1.774441]\n",
      "epoch:24 step:22511 [D loss: 0.652968, acc: 62.50%] [G loss: 1.918766]\n",
      "epoch:24 step:22512 [D loss: 0.626803, acc: 64.84%] [G loss: 1.869726]\n",
      "epoch:24 step:22513 [D loss: 0.619466, acc: 69.53%] [G loss: 1.990237]\n",
      "epoch:24 step:22514 [D loss: 0.646543, acc: 64.06%] [G loss: 1.869711]\n",
      "epoch:24 step:22515 [D loss: 0.667342, acc: 60.94%] [G loss: 1.847724]\n",
      "epoch:24 step:22516 [D loss: 0.638882, acc: 60.94%] [G loss: 1.893660]\n",
      "epoch:24 step:22517 [D loss: 0.664385, acc: 62.50%] [G loss: 1.909128]\n",
      "epoch:24 step:22518 [D loss: 0.666851, acc: 62.50%] [G loss: 1.912087]\n",
      "epoch:24 step:22519 [D loss: 0.682551, acc: 64.84%] [G loss: 1.786879]\n",
      "epoch:24 step:22520 [D loss: 0.724803, acc: 55.47%] [G loss: 1.871075]\n",
      "epoch:24 step:22521 [D loss: 0.655392, acc: 60.94%] [G loss: 1.811235]\n",
      "epoch:24 step:22522 [D loss: 0.647122, acc: 58.59%] [G loss: 1.920377]\n",
      "epoch:24 step:22523 [D loss: 0.624988, acc: 64.06%] [G loss: 1.927672]\n",
      "epoch:24 step:22524 [D loss: 0.601590, acc: 66.41%] [G loss: 2.150304]\n",
      "epoch:24 step:22525 [D loss: 0.617344, acc: 64.06%] [G loss: 1.874038]\n",
      "epoch:24 step:22526 [D loss: 0.653393, acc: 57.03%] [G loss: 1.727521]\n",
      "epoch:24 step:22527 [D loss: 0.633461, acc: 62.50%] [G loss: 1.963005]\n",
      "epoch:24 step:22528 [D loss: 0.573476, acc: 69.53%] [G loss: 2.090178]\n",
      "epoch:24 step:22529 [D loss: 0.659649, acc: 66.41%] [G loss: 1.821713]\n",
      "epoch:24 step:22530 [D loss: 0.603502, acc: 66.41%] [G loss: 2.004697]\n",
      "epoch:24 step:22531 [D loss: 0.665363, acc: 62.50%] [G loss: 1.902764]\n",
      "epoch:24 step:22532 [D loss: 0.719128, acc: 57.03%] [G loss: 1.772634]\n",
      "epoch:24 step:22533 [D loss: 0.654415, acc: 66.41%] [G loss: 1.802625]\n",
      "epoch:24 step:22534 [D loss: 0.596333, acc: 67.19%] [G loss: 1.885332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22535 [D loss: 0.651559, acc: 59.38%] [G loss: 1.941229]\n",
      "epoch:24 step:22536 [D loss: 0.661193, acc: 55.47%] [G loss: 1.836116]\n",
      "epoch:24 step:22537 [D loss: 0.649333, acc: 60.94%] [G loss: 1.819864]\n",
      "epoch:24 step:22538 [D loss: 0.615978, acc: 60.16%] [G loss: 2.035611]\n",
      "epoch:24 step:22539 [D loss: 0.658834, acc: 59.38%] [G loss: 1.830682]\n",
      "epoch:24 step:22540 [D loss: 0.671784, acc: 53.91%] [G loss: 1.828877]\n",
      "epoch:24 step:22541 [D loss: 0.633339, acc: 63.28%] [G loss: 1.825498]\n",
      "epoch:24 step:22542 [D loss: 0.640422, acc: 63.28%] [G loss: 1.798117]\n",
      "epoch:24 step:22543 [D loss: 0.613330, acc: 69.53%] [G loss: 1.996447]\n",
      "epoch:24 step:22544 [D loss: 0.639827, acc: 61.72%] [G loss: 1.967619]\n",
      "epoch:24 step:22545 [D loss: 0.676363, acc: 60.16%] [G loss: 1.932979]\n",
      "epoch:24 step:22546 [D loss: 0.662555, acc: 58.59%] [G loss: 1.824015]\n",
      "epoch:24 step:22547 [D loss: 0.705211, acc: 54.69%] [G loss: 1.842623]\n",
      "epoch:24 step:22548 [D loss: 0.649047, acc: 56.25%] [G loss: 1.864840]\n",
      "epoch:24 step:22549 [D loss: 0.644512, acc: 65.62%] [G loss: 1.689518]\n",
      "epoch:24 step:22550 [D loss: 0.621684, acc: 65.62%] [G loss: 1.950768]\n",
      "epoch:24 step:22551 [D loss: 0.666531, acc: 61.72%] [G loss: 1.859933]\n",
      "epoch:24 step:22552 [D loss: 0.692191, acc: 55.47%] [G loss: 1.961555]\n",
      "epoch:24 step:22553 [D loss: 0.667679, acc: 56.25%] [G loss: 1.768520]\n",
      "epoch:24 step:22554 [D loss: 0.625181, acc: 67.97%] [G loss: 1.877554]\n",
      "epoch:24 step:22555 [D loss: 0.645785, acc: 62.50%] [G loss: 1.831849]\n",
      "epoch:24 step:22556 [D loss: 0.648163, acc: 57.03%] [G loss: 1.879668]\n",
      "epoch:24 step:22557 [D loss: 0.594937, acc: 68.75%] [G loss: 1.893483]\n",
      "epoch:24 step:22558 [D loss: 0.670734, acc: 64.06%] [G loss: 1.901904]\n",
      "epoch:24 step:22559 [D loss: 0.680245, acc: 53.12%] [G loss: 1.768060]\n",
      "epoch:24 step:22560 [D loss: 0.695650, acc: 56.25%] [G loss: 1.750439]\n",
      "epoch:24 step:22561 [D loss: 0.639763, acc: 61.72%] [G loss: 1.976504]\n",
      "epoch:24 step:22562 [D loss: 0.652285, acc: 57.81%] [G loss: 1.914509]\n",
      "epoch:24 step:22563 [D loss: 0.613221, acc: 64.84%] [G loss: 2.195017]\n",
      "epoch:24 step:22564 [D loss: 0.602779, acc: 64.84%] [G loss: 2.003412]\n",
      "epoch:24 step:22565 [D loss: 0.616389, acc: 64.06%] [G loss: 1.926378]\n",
      "epoch:24 step:22566 [D loss: 0.681609, acc: 53.91%] [G loss: 1.808032]\n",
      "epoch:24 step:22567 [D loss: 0.681173, acc: 57.81%] [G loss: 1.764625]\n",
      "epoch:24 step:22568 [D loss: 0.643188, acc: 64.06%] [G loss: 1.837364]\n",
      "epoch:24 step:22569 [D loss: 0.667240, acc: 62.50%] [G loss: 1.728550]\n",
      "epoch:24 step:22570 [D loss: 0.683184, acc: 59.38%] [G loss: 1.873425]\n",
      "epoch:24 step:22571 [D loss: 0.668019, acc: 59.38%] [G loss: 1.843445]\n",
      "epoch:24 step:22572 [D loss: 0.633304, acc: 63.28%] [G loss: 1.752285]\n",
      "epoch:24 step:22573 [D loss: 0.685577, acc: 55.47%] [G loss: 1.587523]\n",
      "epoch:24 step:22574 [D loss: 0.676739, acc: 53.91%] [G loss: 1.792598]\n",
      "epoch:24 step:22575 [D loss: 0.657444, acc: 55.47%] [G loss: 1.774467]\n",
      "epoch:24 step:22576 [D loss: 0.605660, acc: 68.75%] [G loss: 1.765718]\n",
      "epoch:24 step:22577 [D loss: 0.665258, acc: 57.81%] [G loss: 1.895878]\n",
      "epoch:24 step:22578 [D loss: 0.686525, acc: 58.59%] [G loss: 1.805057]\n",
      "epoch:24 step:22579 [D loss: 0.641925, acc: 64.06%] [G loss: 1.819575]\n",
      "epoch:24 step:22580 [D loss: 0.636200, acc: 65.62%] [G loss: 1.956565]\n",
      "epoch:24 step:22581 [D loss: 0.583184, acc: 72.66%] [G loss: 1.924364]\n",
      "epoch:24 step:22582 [D loss: 0.622745, acc: 64.84%] [G loss: 1.848439]\n",
      "epoch:24 step:22583 [D loss: 0.654186, acc: 60.94%] [G loss: 1.810347]\n",
      "epoch:24 step:22584 [D loss: 0.601989, acc: 63.28%] [G loss: 1.982424]\n",
      "epoch:24 step:22585 [D loss: 0.670526, acc: 60.94%] [G loss: 1.884660]\n",
      "epoch:24 step:22586 [D loss: 0.635690, acc: 65.62%] [G loss: 1.797523]\n",
      "epoch:24 step:22587 [D loss: 0.661445, acc: 60.94%] [G loss: 1.860207]\n",
      "epoch:24 step:22588 [D loss: 0.592045, acc: 69.53%] [G loss: 1.801622]\n",
      "epoch:24 step:22589 [D loss: 0.641550, acc: 62.50%] [G loss: 1.794845]\n",
      "epoch:24 step:22590 [D loss: 0.680157, acc: 60.16%] [G loss: 1.822800]\n",
      "epoch:24 step:22591 [D loss: 0.669159, acc: 56.25%] [G loss: 1.906760]\n",
      "epoch:24 step:22592 [D loss: 0.632430, acc: 63.28%] [G loss: 1.832000]\n",
      "epoch:24 step:22593 [D loss: 0.737806, acc: 47.66%] [G loss: 1.781204]\n",
      "epoch:24 step:22594 [D loss: 0.615834, acc: 65.62%] [G loss: 1.855715]\n",
      "epoch:24 step:22595 [D loss: 0.600796, acc: 64.84%] [G loss: 2.154362]\n",
      "epoch:24 step:22596 [D loss: 0.687868, acc: 56.25%] [G loss: 1.749557]\n",
      "epoch:24 step:22597 [D loss: 0.708173, acc: 54.69%] [G loss: 1.705683]\n",
      "epoch:24 step:22598 [D loss: 0.656373, acc: 58.59%] [G loss: 1.713668]\n",
      "epoch:24 step:22599 [D loss: 0.591980, acc: 71.09%] [G loss: 1.913165]\n",
      "epoch:24 step:22600 [D loss: 0.624547, acc: 63.28%] [G loss: 1.997715]\n",
      "##############\n",
      "[2.62434236 1.40367426 6.01783131 4.49566934 3.59694897 5.63351157\n",
      " 4.44615387 4.51268333 4.29865463 3.55349112]\n",
      "##########\n",
      "epoch:24 step:22601 [D loss: 0.630440, acc: 64.84%] [G loss: 2.058241]\n",
      "epoch:24 step:22602 [D loss: 0.590537, acc: 71.88%] [G loss: 1.956471]\n",
      "epoch:24 step:22603 [D loss: 0.642122, acc: 61.72%] [G loss: 2.125921]\n",
      "epoch:24 step:22604 [D loss: 0.576767, acc: 68.75%] [G loss: 2.247489]\n",
      "epoch:24 step:22605 [D loss: 0.609056, acc: 64.84%] [G loss: 2.062756]\n",
      "epoch:24 step:22606 [D loss: 0.620068, acc: 67.19%] [G loss: 1.961423]\n",
      "epoch:24 step:22607 [D loss: 0.578582, acc: 75.00%] [G loss: 2.491182]\n",
      "epoch:24 step:22608 [D loss: 0.654971, acc: 60.16%] [G loss: 1.956033]\n",
      "epoch:24 step:22609 [D loss: 0.675566, acc: 56.25%] [G loss: 1.957133]\n",
      "epoch:24 step:22610 [D loss: 0.600944, acc: 72.66%] [G loss: 2.255696]\n",
      "epoch:24 step:22611 [D loss: 0.713281, acc: 59.38%] [G loss: 1.899345]\n",
      "epoch:24 step:22612 [D loss: 0.706191, acc: 60.94%] [G loss: 1.872470]\n",
      "epoch:24 step:22613 [D loss: 0.719042, acc: 50.00%] [G loss: 1.637304]\n",
      "epoch:24 step:22614 [D loss: 0.655985, acc: 62.50%] [G loss: 1.864657]\n",
      "epoch:24 step:22615 [D loss: 0.660783, acc: 60.94%] [G loss: 1.694297]\n",
      "epoch:24 step:22616 [D loss: 0.672105, acc: 56.25%] [G loss: 1.753628]\n",
      "epoch:24 step:22617 [D loss: 0.646358, acc: 60.16%] [G loss: 1.767290]\n",
      "epoch:24 step:22618 [D loss: 0.598979, acc: 64.84%] [G loss: 2.002099]\n",
      "epoch:24 step:22619 [D loss: 0.615979, acc: 62.50%] [G loss: 1.903042]\n",
      "epoch:24 step:22620 [D loss: 0.615106, acc: 62.50%] [G loss: 1.922056]\n",
      "epoch:24 step:22621 [D loss: 0.676380, acc: 58.59%] [G loss: 1.772547]\n",
      "epoch:24 step:22622 [D loss: 0.629669, acc: 63.28%] [G loss: 1.805094]\n",
      "epoch:24 step:22623 [D loss: 0.663993, acc: 60.94%] [G loss: 1.870720]\n",
      "epoch:24 step:22624 [D loss: 0.667839, acc: 62.50%] [G loss: 1.875424]\n",
      "epoch:24 step:22625 [D loss: 0.639895, acc: 61.72%] [G loss: 1.768538]\n",
      "epoch:24 step:22626 [D loss: 0.664137, acc: 59.38%] [G loss: 1.708913]\n",
      "epoch:24 step:22627 [D loss: 0.696523, acc: 53.12%] [G loss: 1.845077]\n",
      "epoch:24 step:22628 [D loss: 0.641844, acc: 63.28%] [G loss: 1.840928]\n",
      "epoch:24 step:22629 [D loss: 0.670913, acc: 58.59%] [G loss: 1.769439]\n",
      "epoch:24 step:22630 [D loss: 0.687098, acc: 53.91%] [G loss: 1.716184]\n",
      "epoch:24 step:22631 [D loss: 0.691740, acc: 53.12%] [G loss: 1.861969]\n",
      "epoch:24 step:22632 [D loss: 0.616628, acc: 66.41%] [G loss: 1.813836]\n",
      "epoch:24 step:22633 [D loss: 0.624149, acc: 66.41%] [G loss: 1.863990]\n",
      "epoch:24 step:22634 [D loss: 0.643082, acc: 64.06%] [G loss: 2.069367]\n",
      "epoch:24 step:22635 [D loss: 0.672668, acc: 59.38%] [G loss: 1.819018]\n",
      "epoch:24 step:22636 [D loss: 0.674443, acc: 55.47%] [G loss: 1.917682]\n",
      "epoch:24 step:22637 [D loss: 0.590516, acc: 71.88%] [G loss: 1.860389]\n",
      "epoch:24 step:22638 [D loss: 0.620683, acc: 67.97%] [G loss: 1.908056]\n",
      "epoch:24 step:22639 [D loss: 0.607805, acc: 70.31%] [G loss: 2.092946]\n",
      "epoch:24 step:22640 [D loss: 0.662012, acc: 62.50%] [G loss: 2.016401]\n",
      "epoch:24 step:22641 [D loss: 0.677084, acc: 60.16%] [G loss: 1.881525]\n",
      "epoch:24 step:22642 [D loss: 0.603247, acc: 67.19%] [G loss: 1.866935]\n",
      "epoch:24 step:22643 [D loss: 0.587558, acc: 64.06%] [G loss: 2.035011]\n",
      "epoch:24 step:22644 [D loss: 0.673691, acc: 56.25%] [G loss: 2.019859]\n",
      "epoch:24 step:22645 [D loss: 0.638496, acc: 61.72%] [G loss: 1.814268]\n",
      "epoch:24 step:22646 [D loss: 0.656526, acc: 64.84%] [G loss: 2.032241]\n",
      "epoch:24 step:22647 [D loss: 0.620803, acc: 68.75%] [G loss: 1.988088]\n",
      "epoch:24 step:22648 [D loss: 0.719431, acc: 53.12%] [G loss: 1.832507]\n",
      "epoch:24 step:22649 [D loss: 0.701333, acc: 53.91%] [G loss: 1.836816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22650 [D loss: 0.647223, acc: 62.50%] [G loss: 1.952372]\n",
      "epoch:24 step:22651 [D loss: 0.625535, acc: 64.84%] [G loss: 1.904438]\n",
      "epoch:24 step:22652 [D loss: 0.648724, acc: 63.28%] [G loss: 1.901028]\n",
      "epoch:24 step:22653 [D loss: 0.653067, acc: 64.84%] [G loss: 1.866296]\n",
      "epoch:24 step:22654 [D loss: 0.652721, acc: 60.16%] [G loss: 1.886498]\n",
      "epoch:24 step:22655 [D loss: 0.648629, acc: 60.94%] [G loss: 1.960862]\n",
      "epoch:24 step:22656 [D loss: 0.614158, acc: 66.41%] [G loss: 1.940425]\n",
      "epoch:24 step:22657 [D loss: 0.641097, acc: 60.94%] [G loss: 1.875537]\n",
      "epoch:24 step:22658 [D loss: 0.675267, acc: 57.81%] [G loss: 1.710125]\n",
      "epoch:24 step:22659 [D loss: 0.608080, acc: 65.62%] [G loss: 1.913822]\n",
      "epoch:24 step:22660 [D loss: 0.629269, acc: 60.94%] [G loss: 1.815868]\n",
      "epoch:24 step:22661 [D loss: 0.661438, acc: 61.72%] [G loss: 1.699702]\n",
      "epoch:24 step:22662 [D loss: 0.682526, acc: 58.59%] [G loss: 1.730410]\n",
      "epoch:24 step:22663 [D loss: 0.625550, acc: 61.72%] [G loss: 1.702383]\n",
      "epoch:24 step:22664 [D loss: 0.619001, acc: 67.19%] [G loss: 1.840228]\n",
      "epoch:24 step:22665 [D loss: 0.673113, acc: 54.69%] [G loss: 1.874665]\n",
      "epoch:24 step:22666 [D loss: 0.650002, acc: 61.72%] [G loss: 1.976850]\n",
      "epoch:24 step:22667 [D loss: 0.652480, acc: 60.16%] [G loss: 1.816197]\n",
      "epoch:24 step:22668 [D loss: 0.663219, acc: 59.38%] [G loss: 1.836123]\n",
      "epoch:24 step:22669 [D loss: 0.671349, acc: 58.59%] [G loss: 1.834580]\n",
      "epoch:24 step:22670 [D loss: 0.697886, acc: 53.12%] [G loss: 1.869835]\n",
      "epoch:24 step:22671 [D loss: 0.687437, acc: 53.91%] [G loss: 1.818905]\n",
      "epoch:24 step:22672 [D loss: 0.667571, acc: 54.69%] [G loss: 1.893621]\n",
      "epoch:24 step:22673 [D loss: 0.618719, acc: 64.06%] [G loss: 1.822907]\n",
      "epoch:24 step:22674 [D loss: 0.605836, acc: 69.53%] [G loss: 1.895902]\n",
      "epoch:24 step:22675 [D loss: 0.642252, acc: 65.62%] [G loss: 1.889445]\n",
      "epoch:24 step:22676 [D loss: 0.704358, acc: 59.38%] [G loss: 1.932962]\n",
      "epoch:24 step:22677 [D loss: 0.676667, acc: 60.94%] [G loss: 1.825196]\n",
      "epoch:24 step:22678 [D loss: 0.611672, acc: 70.31%] [G loss: 1.913454]\n",
      "epoch:24 step:22679 [D loss: 0.643300, acc: 58.59%] [G loss: 1.800742]\n",
      "epoch:24 step:22680 [D loss: 0.659444, acc: 62.50%] [G loss: 1.865653]\n",
      "epoch:24 step:22681 [D loss: 0.647684, acc: 65.62%] [G loss: 1.863489]\n",
      "epoch:24 step:22682 [D loss: 0.613184, acc: 71.88%] [G loss: 1.925393]\n",
      "epoch:24 step:22683 [D loss: 0.621396, acc: 64.84%] [G loss: 1.893833]\n",
      "epoch:24 step:22684 [D loss: 0.662662, acc: 57.03%] [G loss: 1.800424]\n",
      "epoch:24 step:22685 [D loss: 0.633494, acc: 70.31%] [G loss: 1.933319]\n",
      "epoch:24 step:22686 [D loss: 0.649364, acc: 67.19%] [G loss: 2.042407]\n",
      "epoch:24 step:22687 [D loss: 0.665912, acc: 60.16%] [G loss: 1.853899]\n",
      "epoch:24 step:22688 [D loss: 0.646760, acc: 60.94%] [G loss: 1.757656]\n",
      "epoch:24 step:22689 [D loss: 0.583692, acc: 67.97%] [G loss: 1.998195]\n",
      "epoch:24 step:22690 [D loss: 0.613556, acc: 67.97%] [G loss: 2.057155]\n",
      "epoch:24 step:22691 [D loss: 0.693348, acc: 57.03%] [G loss: 1.982055]\n",
      "epoch:24 step:22692 [D loss: 0.667253, acc: 61.72%] [G loss: 1.865352]\n",
      "epoch:24 step:22693 [D loss: 0.598586, acc: 67.19%] [G loss: 1.904187]\n",
      "epoch:24 step:22694 [D loss: 0.597187, acc: 67.97%] [G loss: 2.059856]\n",
      "epoch:24 step:22695 [D loss: 0.628560, acc: 63.28%] [G loss: 2.120872]\n",
      "epoch:24 step:22696 [D loss: 0.541560, acc: 70.31%] [G loss: 2.163749]\n",
      "epoch:24 step:22697 [D loss: 0.590604, acc: 73.44%] [G loss: 2.146348]\n",
      "epoch:24 step:22698 [D loss: 0.712881, acc: 53.91%] [G loss: 1.885005]\n",
      "epoch:24 step:22699 [D loss: 0.698288, acc: 60.94%] [G loss: 1.953843]\n",
      "epoch:24 step:22700 [D loss: 0.686786, acc: 56.25%] [G loss: 1.900891]\n",
      "epoch:24 step:22701 [D loss: 0.713799, acc: 53.12%] [G loss: 1.788320]\n",
      "epoch:24 step:22702 [D loss: 0.724389, acc: 49.22%] [G loss: 1.733281]\n",
      "epoch:24 step:22703 [D loss: 0.690753, acc: 57.81%] [G loss: 1.773456]\n",
      "epoch:24 step:22704 [D loss: 0.621103, acc: 66.41%] [G loss: 2.003647]\n",
      "epoch:24 step:22705 [D loss: 0.695747, acc: 64.84%] [G loss: 1.856488]\n",
      "epoch:24 step:22706 [D loss: 0.643530, acc: 57.03%] [G loss: 1.898940]\n",
      "epoch:24 step:22707 [D loss: 0.615316, acc: 65.62%] [G loss: 2.099351]\n",
      "epoch:24 step:22708 [D loss: 0.729432, acc: 57.03%] [G loss: 1.854817]\n",
      "epoch:24 step:22709 [D loss: 0.632918, acc: 62.50%] [G loss: 1.980490]\n",
      "epoch:24 step:22710 [D loss: 0.678901, acc: 57.81%] [G loss: 1.898398]\n",
      "epoch:24 step:22711 [D loss: 0.675864, acc: 53.12%] [G loss: 1.885621]\n",
      "epoch:24 step:22712 [D loss: 0.640245, acc: 57.81%] [G loss: 1.852540]\n",
      "epoch:24 step:22713 [D loss: 0.683323, acc: 53.12%] [G loss: 1.842567]\n",
      "epoch:24 step:22714 [D loss: 0.687851, acc: 57.03%] [G loss: 1.853586]\n",
      "epoch:24 step:22715 [D loss: 0.633538, acc: 64.84%] [G loss: 1.818885]\n",
      "epoch:24 step:22716 [D loss: 0.620175, acc: 61.72%] [G loss: 1.748563]\n",
      "epoch:24 step:22717 [D loss: 0.633521, acc: 63.28%] [G loss: 1.980478]\n",
      "epoch:24 step:22718 [D loss: 0.611059, acc: 64.06%] [G loss: 2.227840]\n",
      "epoch:24 step:22719 [D loss: 0.541425, acc: 74.22%] [G loss: 2.358525]\n",
      "epoch:24 step:22720 [D loss: 0.554053, acc: 78.12%] [G loss: 2.401994]\n",
      "epoch:24 step:22721 [D loss: 0.698187, acc: 56.25%] [G loss: 1.918082]\n",
      "epoch:24 step:22722 [D loss: 0.700799, acc: 53.12%] [G loss: 1.912679]\n",
      "epoch:24 step:22723 [D loss: 0.696017, acc: 57.03%] [G loss: 1.932152]\n",
      "epoch:24 step:22724 [D loss: 0.680461, acc: 60.94%] [G loss: 1.851830]\n",
      "epoch:24 step:22725 [D loss: 0.596606, acc: 65.62%] [G loss: 1.929987]\n",
      "epoch:24 step:22726 [D loss: 0.640404, acc: 65.62%] [G loss: 1.979410]\n",
      "epoch:24 step:22727 [D loss: 0.627498, acc: 67.97%] [G loss: 1.866009]\n",
      "epoch:24 step:22728 [D loss: 0.623589, acc: 66.41%] [G loss: 1.799785]\n",
      "epoch:24 step:22729 [D loss: 0.608192, acc: 66.41%] [G loss: 1.967743]\n",
      "epoch:24 step:22730 [D loss: 0.626050, acc: 64.06%] [G loss: 1.990849]\n",
      "epoch:24 step:22731 [D loss: 0.656299, acc: 58.59%] [G loss: 2.085481]\n",
      "epoch:24 step:22732 [D loss: 0.595855, acc: 68.75%] [G loss: 1.798396]\n",
      "epoch:24 step:22733 [D loss: 0.637225, acc: 65.62%] [G loss: 1.972668]\n",
      "epoch:24 step:22734 [D loss: 0.635775, acc: 64.84%] [G loss: 1.915056]\n",
      "epoch:24 step:22735 [D loss: 0.663149, acc: 58.59%] [G loss: 1.947483]\n",
      "epoch:24 step:22736 [D loss: 0.682735, acc: 58.59%] [G loss: 2.074018]\n",
      "epoch:24 step:22737 [D loss: 0.711427, acc: 53.91%] [G loss: 1.873293]\n",
      "epoch:24 step:22738 [D loss: 0.754590, acc: 51.56%] [G loss: 1.705954]\n",
      "epoch:24 step:22739 [D loss: 0.707201, acc: 51.56%] [G loss: 1.731480]\n",
      "epoch:24 step:22740 [D loss: 0.697414, acc: 55.47%] [G loss: 1.749864]\n",
      "epoch:24 step:22741 [D loss: 0.662176, acc: 60.16%] [G loss: 1.828214]\n",
      "epoch:24 step:22742 [D loss: 0.667124, acc: 59.38%] [G loss: 1.761860]\n",
      "epoch:24 step:22743 [D loss: 0.719479, acc: 51.56%] [G loss: 1.930441]\n",
      "epoch:24 step:22744 [D loss: 0.602955, acc: 68.75%] [G loss: 1.939958]\n",
      "epoch:24 step:22745 [D loss: 0.702460, acc: 60.16%] [G loss: 1.840980]\n",
      "epoch:24 step:22746 [D loss: 0.630583, acc: 65.62%] [G loss: 1.788441]\n",
      "epoch:24 step:22747 [D loss: 0.626437, acc: 67.19%] [G loss: 1.773175]\n",
      "epoch:24 step:22748 [D loss: 0.700727, acc: 57.03%] [G loss: 1.805035]\n",
      "epoch:24 step:22749 [D loss: 0.679564, acc: 62.50%] [G loss: 1.807110]\n",
      "epoch:24 step:22750 [D loss: 0.643572, acc: 60.16%] [G loss: 1.993986]\n",
      "epoch:24 step:22751 [D loss: 0.627718, acc: 63.28%] [G loss: 1.851476]\n",
      "epoch:24 step:22752 [D loss: 0.598123, acc: 67.97%] [G loss: 1.984340]\n",
      "epoch:24 step:22753 [D loss: 0.630228, acc: 60.16%] [G loss: 1.814095]\n",
      "epoch:24 step:22754 [D loss: 0.634697, acc: 57.81%] [G loss: 1.822892]\n",
      "epoch:24 step:22755 [D loss: 0.681967, acc: 63.28%] [G loss: 1.916955]\n",
      "epoch:24 step:22756 [D loss: 0.681591, acc: 66.41%] [G loss: 1.795505]\n",
      "epoch:24 step:22757 [D loss: 0.649863, acc: 62.50%] [G loss: 1.881532]\n",
      "epoch:24 step:22758 [D loss: 0.634334, acc: 59.38%] [G loss: 1.838856]\n",
      "epoch:24 step:22759 [D loss: 0.688703, acc: 52.34%] [G loss: 1.919057]\n",
      "epoch:24 step:22760 [D loss: 0.653082, acc: 64.84%] [G loss: 1.889386]\n",
      "epoch:24 step:22761 [D loss: 0.615178, acc: 63.28%] [G loss: 2.031825]\n",
      "epoch:24 step:22762 [D loss: 0.611247, acc: 66.41%] [G loss: 2.016297]\n",
      "epoch:24 step:22763 [D loss: 0.579784, acc: 64.84%] [G loss: 1.955095]\n",
      "epoch:24 step:22764 [D loss: 0.593169, acc: 69.53%] [G loss: 2.236836]\n",
      "epoch:24 step:22765 [D loss: 0.674365, acc: 62.50%] [G loss: 2.005058]\n",
      "epoch:24 step:22766 [D loss: 0.652733, acc: 60.16%] [G loss: 1.781822]\n",
      "epoch:24 step:22767 [D loss: 0.647187, acc: 65.62%] [G loss: 1.856019]\n",
      "epoch:24 step:22768 [D loss: 0.628327, acc: 64.84%] [G loss: 1.910779]\n",
      "epoch:24 step:22769 [D loss: 0.678171, acc: 57.03%] [G loss: 1.869398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22770 [D loss: 0.662307, acc: 64.84%] [G loss: 1.873857]\n",
      "epoch:24 step:22771 [D loss: 0.660441, acc: 60.94%] [G loss: 1.827920]\n",
      "epoch:24 step:22772 [D loss: 0.714638, acc: 52.34%] [G loss: 1.798455]\n",
      "epoch:24 step:22773 [D loss: 0.641096, acc: 63.28%] [G loss: 1.864194]\n",
      "epoch:24 step:22774 [D loss: 0.647228, acc: 61.72%] [G loss: 1.858598]\n",
      "epoch:24 step:22775 [D loss: 0.650440, acc: 58.59%] [G loss: 1.754900]\n",
      "epoch:24 step:22776 [D loss: 0.681164, acc: 50.00%] [G loss: 1.766222]\n",
      "epoch:24 step:22777 [D loss: 0.604631, acc: 66.41%] [G loss: 1.934949]\n",
      "epoch:24 step:22778 [D loss: 0.655693, acc: 64.06%] [G loss: 1.923511]\n",
      "epoch:24 step:22779 [D loss: 0.649184, acc: 57.81%] [G loss: 1.646884]\n",
      "epoch:24 step:22780 [D loss: 0.657011, acc: 61.72%] [G loss: 1.815096]\n",
      "epoch:24 step:22781 [D loss: 0.611334, acc: 65.62%] [G loss: 1.916120]\n",
      "epoch:24 step:22782 [D loss: 0.721038, acc: 57.03%] [G loss: 1.972698]\n",
      "epoch:24 step:22783 [D loss: 0.618049, acc: 63.28%] [G loss: 1.846376]\n",
      "epoch:24 step:22784 [D loss: 0.623477, acc: 60.94%] [G loss: 1.980003]\n",
      "epoch:24 step:22785 [D loss: 0.676918, acc: 62.50%] [G loss: 1.711202]\n",
      "epoch:24 step:22786 [D loss: 0.688558, acc: 60.16%] [G loss: 1.976490]\n",
      "epoch:24 step:22787 [D loss: 0.658558, acc: 60.94%] [G loss: 1.962278]\n",
      "epoch:24 step:22788 [D loss: 0.618853, acc: 64.84%] [G loss: 1.919356]\n",
      "epoch:24 step:22789 [D loss: 0.679684, acc: 60.16%] [G loss: 1.745759]\n",
      "epoch:24 step:22790 [D loss: 0.612121, acc: 64.06%] [G loss: 1.910949]\n",
      "epoch:24 step:22791 [D loss: 0.596010, acc: 70.31%] [G loss: 1.924936]\n",
      "epoch:24 step:22792 [D loss: 0.688588, acc: 57.03%] [G loss: 1.837843]\n",
      "epoch:24 step:22793 [D loss: 0.650083, acc: 63.28%] [G loss: 1.850201]\n",
      "epoch:24 step:22794 [D loss: 0.613315, acc: 66.41%] [G loss: 1.860527]\n",
      "epoch:24 step:22795 [D loss: 0.635397, acc: 62.50%] [G loss: 1.770462]\n",
      "epoch:24 step:22796 [D loss: 0.632450, acc: 67.19%] [G loss: 1.843143]\n",
      "epoch:24 step:22797 [D loss: 0.633692, acc: 61.72%] [G loss: 1.840507]\n",
      "epoch:24 step:22798 [D loss: 0.647081, acc: 60.94%] [G loss: 1.917296]\n",
      "epoch:24 step:22799 [D loss: 0.624081, acc: 66.41%] [G loss: 1.748231]\n",
      "epoch:24 step:22800 [D loss: 0.585146, acc: 64.84%] [G loss: 2.120855]\n",
      "##############\n",
      "[2.49843904 1.49175033 6.0415913  4.68215005 3.47882996 5.61158304\n",
      " 4.35216581 4.45061868 4.52051321 3.44334306]\n",
      "##########\n",
      "epoch:24 step:22801 [D loss: 0.598934, acc: 65.62%] [G loss: 2.117286]\n",
      "epoch:24 step:22802 [D loss: 0.555427, acc: 73.44%] [G loss: 2.259683]\n",
      "epoch:24 step:22803 [D loss: 0.552871, acc: 75.00%] [G loss: 2.167985]\n",
      "epoch:24 step:22804 [D loss: 0.694132, acc: 60.94%] [G loss: 1.759802]\n",
      "epoch:24 step:22805 [D loss: 0.643093, acc: 63.28%] [G loss: 1.792601]\n",
      "epoch:24 step:22806 [D loss: 0.644196, acc: 64.06%] [G loss: 1.929575]\n",
      "epoch:24 step:22807 [D loss: 0.648549, acc: 60.94%] [G loss: 1.836794]\n",
      "epoch:24 step:22808 [D loss: 0.657236, acc: 62.50%] [G loss: 1.724487]\n",
      "epoch:24 step:22809 [D loss: 0.656545, acc: 64.06%] [G loss: 2.006834]\n",
      "epoch:24 step:22810 [D loss: 0.650987, acc: 57.03%] [G loss: 1.924631]\n",
      "epoch:24 step:22811 [D loss: 0.688681, acc: 62.50%] [G loss: 1.824401]\n",
      "epoch:24 step:22812 [D loss: 0.637994, acc: 71.09%] [G loss: 1.817227]\n",
      "epoch:24 step:22813 [D loss: 0.626346, acc: 63.28%] [G loss: 1.877147]\n",
      "epoch:24 step:22814 [D loss: 0.634020, acc: 64.84%] [G loss: 1.835524]\n",
      "epoch:24 step:22815 [D loss: 0.645239, acc: 64.06%] [G loss: 1.743522]\n",
      "epoch:24 step:22816 [D loss: 0.677637, acc: 64.84%] [G loss: 1.934481]\n",
      "epoch:24 step:22817 [D loss: 0.619547, acc: 66.41%] [G loss: 1.910568]\n",
      "epoch:24 step:22818 [D loss: 0.615898, acc: 62.50%] [G loss: 1.802691]\n",
      "epoch:24 step:22819 [D loss: 0.645305, acc: 67.97%] [G loss: 1.976968]\n",
      "epoch:24 step:22820 [D loss: 0.661261, acc: 54.69%] [G loss: 1.856881]\n",
      "epoch:24 step:22821 [D loss: 0.649929, acc: 62.50%] [G loss: 1.835778]\n",
      "epoch:24 step:22822 [D loss: 0.706739, acc: 59.38%] [G loss: 1.865660]\n",
      "epoch:24 step:22823 [D loss: 0.650991, acc: 60.16%] [G loss: 1.894813]\n",
      "epoch:24 step:22824 [D loss: 0.645697, acc: 65.62%] [G loss: 1.932108]\n",
      "epoch:24 step:22825 [D loss: 0.605870, acc: 65.62%] [G loss: 2.071177]\n",
      "epoch:24 step:22826 [D loss: 0.626637, acc: 68.75%] [G loss: 1.912386]\n",
      "epoch:24 step:22827 [D loss: 0.708057, acc: 60.94%] [G loss: 1.949388]\n",
      "epoch:24 step:22828 [D loss: 0.624710, acc: 62.50%] [G loss: 1.933374]\n",
      "epoch:24 step:22829 [D loss: 0.687911, acc: 53.91%] [G loss: 1.792535]\n",
      "epoch:24 step:22830 [D loss: 0.730609, acc: 54.69%] [G loss: 1.734882]\n",
      "epoch:24 step:22831 [D loss: 0.668487, acc: 60.94%] [G loss: 1.736617]\n",
      "epoch:24 step:22832 [D loss: 0.663537, acc: 58.59%] [G loss: 1.772443]\n",
      "epoch:24 step:22833 [D loss: 0.585762, acc: 66.41%] [G loss: 2.188012]\n",
      "epoch:24 step:22834 [D loss: 0.621205, acc: 63.28%] [G loss: 2.219059]\n",
      "epoch:24 step:22835 [D loss: 0.635305, acc: 64.06%] [G loss: 2.155999]\n",
      "epoch:24 step:22836 [D loss: 0.685247, acc: 57.81%] [G loss: 1.752801]\n",
      "epoch:24 step:22837 [D loss: 0.685413, acc: 57.81%] [G loss: 1.818334]\n",
      "epoch:24 step:22838 [D loss: 0.615785, acc: 65.62%] [G loss: 1.748721]\n",
      "epoch:24 step:22839 [D loss: 0.633116, acc: 61.72%] [G loss: 1.783954]\n",
      "epoch:24 step:22840 [D loss: 0.662532, acc: 64.84%] [G loss: 1.801213]\n",
      "epoch:24 step:22841 [D loss: 0.665092, acc: 64.84%] [G loss: 1.900493]\n",
      "epoch:24 step:22842 [D loss: 0.650620, acc: 56.25%] [G loss: 2.084610]\n",
      "epoch:24 step:22843 [D loss: 0.653932, acc: 65.62%] [G loss: 1.873471]\n",
      "epoch:24 step:22844 [D loss: 0.692300, acc: 58.59%] [G loss: 1.919299]\n",
      "epoch:24 step:22845 [D loss: 0.614986, acc: 64.06%] [G loss: 1.857367]\n",
      "epoch:24 step:22846 [D loss: 0.622121, acc: 68.75%] [G loss: 2.019796]\n",
      "epoch:24 step:22847 [D loss: 0.586669, acc: 69.53%] [G loss: 1.931161]\n",
      "epoch:24 step:22848 [D loss: 0.605297, acc: 66.41%] [G loss: 2.104384]\n",
      "epoch:24 step:22849 [D loss: 0.603185, acc: 70.31%] [G loss: 1.861804]\n",
      "epoch:24 step:22850 [D loss: 0.664661, acc: 59.38%] [G loss: 1.841598]\n",
      "epoch:24 step:22851 [D loss: 0.645183, acc: 60.94%] [G loss: 2.019759]\n",
      "epoch:24 step:22852 [D loss: 0.652240, acc: 64.06%] [G loss: 1.825922]\n",
      "epoch:24 step:22853 [D loss: 0.666053, acc: 63.28%] [G loss: 1.916681]\n",
      "epoch:24 step:22854 [D loss: 0.659663, acc: 60.94%] [G loss: 1.896605]\n",
      "epoch:24 step:22855 [D loss: 0.682106, acc: 57.03%] [G loss: 2.067626]\n",
      "epoch:24 step:22856 [D loss: 0.597064, acc: 69.53%] [G loss: 2.024458]\n",
      "epoch:24 step:22857 [D loss: 0.656436, acc: 58.59%] [G loss: 1.938790]\n",
      "epoch:24 step:22858 [D loss: 0.602005, acc: 69.53%] [G loss: 2.029396]\n",
      "epoch:24 step:22859 [D loss: 0.607842, acc: 66.41%] [G loss: 2.230639]\n",
      "epoch:24 step:22860 [D loss: 0.667126, acc: 58.59%] [G loss: 1.885136]\n",
      "epoch:24 step:22861 [D loss: 0.681330, acc: 54.69%] [G loss: 1.713755]\n",
      "epoch:24 step:22862 [D loss: 0.595152, acc: 66.41%] [G loss: 1.904658]\n",
      "epoch:24 step:22863 [D loss: 0.642000, acc: 61.72%] [G loss: 1.829066]\n",
      "epoch:24 step:22864 [D loss: 0.746949, acc: 50.78%] [G loss: 1.853380]\n",
      "epoch:24 step:22865 [D loss: 0.653134, acc: 65.62%] [G loss: 1.821349]\n",
      "epoch:24 step:22866 [D loss: 0.671655, acc: 62.50%] [G loss: 2.008693]\n",
      "epoch:24 step:22867 [D loss: 0.645711, acc: 59.38%] [G loss: 1.830425]\n",
      "epoch:24 step:22868 [D loss: 0.601881, acc: 67.97%] [G loss: 2.067859]\n",
      "epoch:24 step:22869 [D loss: 0.613079, acc: 64.06%] [G loss: 1.886078]\n",
      "epoch:24 step:22870 [D loss: 0.633589, acc: 63.28%] [G loss: 1.950098]\n",
      "epoch:24 step:22871 [D loss: 0.619852, acc: 64.84%] [G loss: 1.859419]\n",
      "epoch:24 step:22872 [D loss: 0.635972, acc: 60.94%] [G loss: 1.982243]\n",
      "epoch:24 step:22873 [D loss: 0.658439, acc: 64.06%] [G loss: 1.989235]\n",
      "epoch:24 step:22874 [D loss: 0.748882, acc: 53.91%] [G loss: 1.822358]\n",
      "epoch:24 step:22875 [D loss: 0.659486, acc: 62.50%] [G loss: 1.869753]\n",
      "epoch:24 step:22876 [D loss: 0.621663, acc: 63.28%] [G loss: 1.855672]\n",
      "epoch:24 step:22877 [D loss: 0.638635, acc: 61.72%] [G loss: 1.909755]\n",
      "epoch:24 step:22878 [D loss: 0.662242, acc: 57.03%] [G loss: 1.908651]\n",
      "epoch:24 step:22879 [D loss: 0.622572, acc: 71.88%] [G loss: 1.776247]\n",
      "epoch:24 step:22880 [D loss: 0.694158, acc: 58.59%] [G loss: 1.902351]\n",
      "epoch:24 step:22881 [D loss: 0.652054, acc: 57.81%] [G loss: 1.948397]\n",
      "epoch:24 step:22882 [D loss: 0.692206, acc: 60.94%] [G loss: 1.797213]\n",
      "epoch:24 step:22883 [D loss: 0.644627, acc: 61.72%] [G loss: 1.936617]\n",
      "epoch:24 step:22884 [D loss: 0.641739, acc: 62.50%] [G loss: 1.829273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22885 [D loss: 0.703156, acc: 55.47%] [G loss: 1.787002]\n",
      "epoch:24 step:22886 [D loss: 0.657356, acc: 60.16%] [G loss: 1.862804]\n",
      "epoch:24 step:22887 [D loss: 0.618434, acc: 64.06%] [G loss: 1.996394]\n",
      "epoch:24 step:22888 [D loss: 0.637024, acc: 59.38%] [G loss: 1.969989]\n",
      "epoch:24 step:22889 [D loss: 0.668154, acc: 59.38%] [G loss: 1.842122]\n",
      "epoch:24 step:22890 [D loss: 0.660093, acc: 61.72%] [G loss: 1.885938]\n",
      "epoch:24 step:22891 [D loss: 0.676852, acc: 60.94%] [G loss: 1.784688]\n",
      "epoch:24 step:22892 [D loss: 0.608101, acc: 65.62%] [G loss: 1.845836]\n",
      "epoch:24 step:22893 [D loss: 0.586509, acc: 75.00%] [G loss: 1.862136]\n",
      "epoch:24 step:22894 [D loss: 0.646188, acc: 64.84%] [G loss: 2.032783]\n",
      "epoch:24 step:22895 [D loss: 0.642924, acc: 62.50%] [G loss: 1.785628]\n",
      "epoch:24 step:22896 [D loss: 0.689979, acc: 55.47%] [G loss: 1.920059]\n",
      "epoch:24 step:22897 [D loss: 0.622753, acc: 62.50%] [G loss: 1.905900]\n",
      "epoch:24 step:22898 [D loss: 0.601713, acc: 73.44%] [G loss: 1.794164]\n",
      "epoch:24 step:22899 [D loss: 0.696749, acc: 55.47%] [G loss: 1.782524]\n",
      "epoch:24 step:22900 [D loss: 0.648248, acc: 64.84%] [G loss: 1.908130]\n",
      "epoch:24 step:22901 [D loss: 0.679405, acc: 54.69%] [G loss: 1.843636]\n",
      "epoch:24 step:22902 [D loss: 0.706460, acc: 53.91%] [G loss: 1.961554]\n",
      "epoch:24 step:22903 [D loss: 0.595887, acc: 69.53%] [G loss: 2.064710]\n",
      "epoch:24 step:22904 [D loss: 0.625955, acc: 64.06%] [G loss: 2.131153]\n",
      "epoch:24 step:22905 [D loss: 0.622186, acc: 65.62%] [G loss: 1.900692]\n",
      "epoch:24 step:22906 [D loss: 0.685616, acc: 57.81%] [G loss: 1.851527]\n",
      "epoch:24 step:22907 [D loss: 0.628006, acc: 63.28%] [G loss: 1.882085]\n",
      "epoch:24 step:22908 [D loss: 0.612523, acc: 64.84%] [G loss: 1.883637]\n",
      "epoch:24 step:22909 [D loss: 0.662024, acc: 60.16%] [G loss: 1.852517]\n",
      "epoch:24 step:22910 [D loss: 0.661204, acc: 59.38%] [G loss: 1.811782]\n",
      "epoch:24 step:22911 [D loss: 0.687880, acc: 53.91%] [G loss: 1.863408]\n",
      "epoch:24 step:22912 [D loss: 0.640276, acc: 65.62%] [G loss: 1.807437]\n",
      "epoch:24 step:22913 [D loss: 0.638248, acc: 66.41%] [G loss: 2.039662]\n",
      "epoch:24 step:22914 [D loss: 0.639783, acc: 64.06%] [G loss: 1.864700]\n",
      "epoch:24 step:22915 [D loss: 0.582774, acc: 71.09%] [G loss: 1.995147]\n",
      "epoch:24 step:22916 [D loss: 0.625425, acc: 66.41%] [G loss: 2.046247]\n",
      "epoch:24 step:22917 [D loss: 0.601831, acc: 71.09%] [G loss: 2.215969]\n",
      "epoch:24 step:22918 [D loss: 0.566746, acc: 71.88%] [G loss: 2.111436]\n",
      "epoch:24 step:22919 [D loss: 0.580867, acc: 71.88%] [G loss: 2.078915]\n",
      "epoch:24 step:22920 [D loss: 0.670307, acc: 60.16%] [G loss: 1.935584]\n",
      "epoch:24 step:22921 [D loss: 0.664403, acc: 63.28%] [G loss: 1.969751]\n",
      "epoch:24 step:22922 [D loss: 0.534660, acc: 78.12%] [G loss: 2.095442]\n",
      "epoch:24 step:22923 [D loss: 0.679379, acc: 60.94%] [G loss: 1.974398]\n",
      "epoch:24 step:22924 [D loss: 0.653593, acc: 61.72%] [G loss: 2.039899]\n",
      "epoch:24 step:22925 [D loss: 0.654542, acc: 60.94%] [G loss: 1.651410]\n",
      "epoch:24 step:22926 [D loss: 0.691889, acc: 54.69%] [G loss: 1.859365]\n",
      "epoch:24 step:22927 [D loss: 0.666155, acc: 56.25%] [G loss: 1.902244]\n",
      "epoch:24 step:22928 [D loss: 0.675267, acc: 60.16%] [G loss: 1.856891]\n",
      "epoch:24 step:22929 [D loss: 0.707070, acc: 57.03%] [G loss: 1.956638]\n",
      "epoch:24 step:22930 [D loss: 0.654616, acc: 59.38%] [G loss: 1.938815]\n",
      "epoch:24 step:22931 [D loss: 0.636185, acc: 67.19%] [G loss: 1.902779]\n",
      "epoch:24 step:22932 [D loss: 0.672672, acc: 61.72%] [G loss: 1.879176]\n",
      "epoch:24 step:22933 [D loss: 0.661882, acc: 62.50%] [G loss: 1.754920]\n",
      "epoch:24 step:22934 [D loss: 0.638095, acc: 61.72%] [G loss: 1.849401]\n",
      "epoch:24 step:22935 [D loss: 0.642777, acc: 64.84%] [G loss: 1.795227]\n",
      "epoch:24 step:22936 [D loss: 0.695356, acc: 52.34%] [G loss: 1.784591]\n",
      "epoch:24 step:22937 [D loss: 0.681877, acc: 60.16%] [G loss: 1.803790]\n",
      "epoch:24 step:22938 [D loss: 0.638396, acc: 64.84%] [G loss: 1.892100]\n",
      "epoch:24 step:22939 [D loss: 0.680077, acc: 61.72%] [G loss: 1.884472]\n",
      "epoch:24 step:22940 [D loss: 0.641128, acc: 63.28%] [G loss: 1.921618]\n",
      "epoch:24 step:22941 [D loss: 0.663212, acc: 64.84%] [G loss: 1.966142]\n",
      "epoch:24 step:22942 [D loss: 0.693835, acc: 60.16%] [G loss: 1.944947]\n",
      "epoch:24 step:22943 [D loss: 0.617843, acc: 67.97%] [G loss: 1.816438]\n",
      "epoch:24 step:22944 [D loss: 0.680217, acc: 61.72%] [G loss: 1.827555]\n",
      "epoch:24 step:22945 [D loss: 0.596998, acc: 69.53%] [G loss: 2.046981]\n",
      "epoch:24 step:22946 [D loss: 0.596952, acc: 69.53%] [G loss: 1.812265]\n",
      "epoch:24 step:22947 [D loss: 0.662822, acc: 59.38%] [G loss: 1.863130]\n",
      "epoch:24 step:22948 [D loss: 0.677038, acc: 57.81%] [G loss: 1.828653]\n",
      "epoch:24 step:22949 [D loss: 0.651211, acc: 64.06%] [G loss: 1.838791]\n",
      "epoch:24 step:22950 [D loss: 0.657798, acc: 61.72%] [G loss: 1.930725]\n",
      "epoch:24 step:22951 [D loss: 0.662006, acc: 60.16%] [G loss: 1.816942]\n",
      "epoch:24 step:22952 [D loss: 0.657128, acc: 60.16%] [G loss: 1.821017]\n",
      "epoch:24 step:22953 [D loss: 0.655489, acc: 62.50%] [G loss: 1.900631]\n",
      "epoch:24 step:22954 [D loss: 0.626135, acc: 66.41%] [G loss: 1.911181]\n",
      "epoch:24 step:22955 [D loss: 0.616665, acc: 68.75%] [G loss: 1.858273]\n",
      "epoch:24 step:22956 [D loss: 0.575448, acc: 71.88%] [G loss: 1.923745]\n",
      "epoch:24 step:22957 [D loss: 0.583801, acc: 70.31%] [G loss: 2.030411]\n",
      "epoch:24 step:22958 [D loss: 0.699273, acc: 60.16%] [G loss: 2.049170]\n",
      "epoch:24 step:22959 [D loss: 0.559615, acc: 72.66%] [G loss: 2.363403]\n",
      "epoch:24 step:22960 [D loss: 0.662760, acc: 64.84%] [G loss: 2.120161]\n",
      "epoch:24 step:22961 [D loss: 0.722453, acc: 52.34%] [G loss: 1.705633]\n",
      "epoch:24 step:22962 [D loss: 0.647787, acc: 56.25%] [G loss: 1.960671]\n",
      "epoch:24 step:22963 [D loss: 0.658755, acc: 61.72%] [G loss: 1.906256]\n",
      "epoch:24 step:22964 [D loss: 0.617691, acc: 64.84%] [G loss: 2.013157]\n",
      "epoch:24 step:22965 [D loss: 0.672499, acc: 57.81%] [G loss: 1.850156]\n",
      "epoch:24 step:22966 [D loss: 0.667950, acc: 60.94%] [G loss: 1.857900]\n",
      "epoch:24 step:22967 [D loss: 0.634701, acc: 66.41%] [G loss: 2.141160]\n",
      "epoch:24 step:22968 [D loss: 0.617630, acc: 72.66%] [G loss: 2.040653]\n",
      "epoch:24 step:22969 [D loss: 0.552524, acc: 70.31%] [G loss: 2.157130]\n",
      "epoch:24 step:22970 [D loss: 0.689448, acc: 57.81%] [G loss: 1.902359]\n",
      "epoch:24 step:22971 [D loss: 0.713449, acc: 57.81%] [G loss: 1.880728]\n",
      "epoch:24 step:22972 [D loss: 0.686074, acc: 60.94%] [G loss: 1.994904]\n",
      "epoch:24 step:22973 [D loss: 0.667947, acc: 57.03%] [G loss: 1.906510]\n",
      "epoch:24 step:22974 [D loss: 0.634925, acc: 60.94%] [G loss: 1.822388]\n",
      "epoch:24 step:22975 [D loss: 0.628009, acc: 62.50%] [G loss: 1.960482]\n",
      "epoch:24 step:22976 [D loss: 0.568491, acc: 71.88%] [G loss: 2.084338]\n",
      "epoch:24 step:22977 [D loss: 0.640025, acc: 60.94%] [G loss: 2.024964]\n",
      "epoch:24 step:22978 [D loss: 0.667971, acc: 57.81%] [G loss: 1.942023]\n",
      "epoch:24 step:22979 [D loss: 0.635340, acc: 60.16%] [G loss: 1.981348]\n",
      "epoch:24 step:22980 [D loss: 0.651765, acc: 64.84%] [G loss: 1.775178]\n",
      "epoch:24 step:22981 [D loss: 0.598480, acc: 72.66%] [G loss: 1.944496]\n",
      "epoch:24 step:22982 [D loss: 0.678303, acc: 62.50%] [G loss: 2.145515]\n",
      "epoch:24 step:22983 [D loss: 0.636566, acc: 67.19%] [G loss: 1.998696]\n",
      "epoch:24 step:22984 [D loss: 0.684455, acc: 62.50%] [G loss: 2.014729]\n",
      "epoch:24 step:22985 [D loss: 0.681763, acc: 61.72%] [G loss: 1.956638]\n",
      "epoch:24 step:22986 [D loss: 0.661747, acc: 60.16%] [G loss: 1.922277]\n",
      "epoch:24 step:22987 [D loss: 0.619921, acc: 64.06%] [G loss: 1.928902]\n",
      "epoch:24 step:22988 [D loss: 0.641158, acc: 64.06%] [G loss: 1.764606]\n",
      "epoch:24 step:22989 [D loss: 0.680766, acc: 65.62%] [G loss: 1.870364]\n",
      "epoch:24 step:22990 [D loss: 0.662201, acc: 59.38%] [G loss: 1.776684]\n",
      "epoch:24 step:22991 [D loss: 0.636221, acc: 66.41%] [G loss: 1.918232]\n",
      "epoch:24 step:22992 [D loss: 0.569989, acc: 71.09%] [G loss: 2.020332]\n",
      "epoch:24 step:22993 [D loss: 0.616687, acc: 67.19%] [G loss: 1.858420]\n",
      "epoch:24 step:22994 [D loss: 0.664954, acc: 60.16%] [G loss: 1.898022]\n",
      "epoch:24 step:22995 [D loss: 0.688980, acc: 59.38%] [G loss: 1.959419]\n",
      "epoch:24 step:22996 [D loss: 0.574019, acc: 68.75%] [G loss: 1.987267]\n",
      "epoch:24 step:22997 [D loss: 0.616411, acc: 64.06%] [G loss: 1.911752]\n",
      "epoch:24 step:22998 [D loss: 0.661761, acc: 60.94%] [G loss: 1.958139]\n",
      "epoch:24 step:22999 [D loss: 0.706014, acc: 57.81%] [G loss: 1.757057]\n",
      "epoch:24 step:23000 [D loss: 0.645089, acc: 58.59%] [G loss: 1.941964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.48451318 1.32448068 6.36812835 4.8424684  3.49966492 5.4629208\n",
      " 4.30099024 4.62703633 4.60893965 3.65124047]\n",
      "##########\n",
      "epoch:24 step:23001 [D loss: 0.661028, acc: 64.06%] [G loss: 2.117189]\n",
      "epoch:24 step:23002 [D loss: 0.675774, acc: 59.38%] [G loss: 1.883287]\n",
      "epoch:24 step:23003 [D loss: 0.610288, acc: 67.97%] [G loss: 2.043714]\n",
      "epoch:24 step:23004 [D loss: 0.574419, acc: 69.53%] [G loss: 2.131673]\n",
      "epoch:24 step:23005 [D loss: 0.686961, acc: 56.25%] [G loss: 1.882458]\n",
      "epoch:24 step:23006 [D loss: 0.686949, acc: 54.69%] [G loss: 1.836540]\n",
      "epoch:24 step:23007 [D loss: 0.644903, acc: 62.50%] [G loss: 1.893039]\n",
      "epoch:24 step:23008 [D loss: 0.646556, acc: 60.16%] [G loss: 1.966705]\n",
      "epoch:24 step:23009 [D loss: 0.634957, acc: 63.28%] [G loss: 1.768366]\n",
      "epoch:24 step:23010 [D loss: 0.618650, acc: 62.50%] [G loss: 1.938259]\n",
      "epoch:24 step:23011 [D loss: 0.610200, acc: 66.41%] [G loss: 1.922217]\n",
      "epoch:24 step:23012 [D loss: 0.617195, acc: 71.88%] [G loss: 1.811010]\n",
      "epoch:24 step:23013 [D loss: 0.648775, acc: 60.16%] [G loss: 1.962433]\n",
      "epoch:24 step:23014 [D loss: 0.652247, acc: 64.06%] [G loss: 1.952179]\n",
      "epoch:24 step:23015 [D loss: 0.665372, acc: 57.81%] [G loss: 1.809317]\n",
      "epoch:24 step:23016 [D loss: 0.689873, acc: 55.47%] [G loss: 1.785747]\n",
      "epoch:24 step:23017 [D loss: 0.678984, acc: 57.03%] [G loss: 1.794053]\n",
      "epoch:24 step:23018 [D loss: 0.652671, acc: 60.16%] [G loss: 1.742613]\n",
      "epoch:24 step:23019 [D loss: 0.678556, acc: 57.03%] [G loss: 1.773374]\n",
      "epoch:24 step:23020 [D loss: 0.593091, acc: 68.75%] [G loss: 1.926055]\n",
      "epoch:24 step:23021 [D loss: 0.651006, acc: 63.28%] [G loss: 2.025453]\n",
      "epoch:24 step:23022 [D loss: 0.646781, acc: 59.38%] [G loss: 2.003361]\n",
      "epoch:24 step:23023 [D loss: 0.647816, acc: 61.72%] [G loss: 1.792609]\n",
      "epoch:24 step:23024 [D loss: 0.626302, acc: 67.97%] [G loss: 2.110020]\n",
      "epoch:24 step:23025 [D loss: 0.640390, acc: 62.50%] [G loss: 1.901170]\n",
      "epoch:24 step:23026 [D loss: 0.699245, acc: 53.12%] [G loss: 1.749222]\n",
      "epoch:24 step:23027 [D loss: 0.621792, acc: 61.72%] [G loss: 1.863825]\n",
      "epoch:24 step:23028 [D loss: 0.658893, acc: 57.81%] [G loss: 1.951115]\n",
      "epoch:24 step:23029 [D loss: 0.623031, acc: 62.50%] [G loss: 1.824170]\n",
      "epoch:24 step:23030 [D loss: 0.649171, acc: 63.28%] [G loss: 1.771456]\n",
      "epoch:24 step:23031 [D loss: 0.642681, acc: 63.28%] [G loss: 1.961661]\n",
      "epoch:24 step:23032 [D loss: 0.620772, acc: 67.97%] [G loss: 1.926790]\n",
      "epoch:24 step:23033 [D loss: 0.628596, acc: 60.94%] [G loss: 2.049809]\n",
      "epoch:24 step:23034 [D loss: 0.663329, acc: 63.28%] [G loss: 1.858385]\n",
      "epoch:24 step:23035 [D loss: 0.631607, acc: 60.16%] [G loss: 1.820857]\n",
      "epoch:24 step:23036 [D loss: 0.635120, acc: 64.84%] [G loss: 1.951003]\n",
      "epoch:24 step:23037 [D loss: 0.605680, acc: 70.31%] [G loss: 1.981172]\n",
      "epoch:24 step:23038 [D loss: 0.646549, acc: 62.50%] [G loss: 1.938870]\n",
      "epoch:24 step:23039 [D loss: 0.693862, acc: 64.84%] [G loss: 1.976332]\n",
      "epoch:24 step:23040 [D loss: 0.602620, acc: 73.44%] [G loss: 2.086818]\n",
      "epoch:24 step:23041 [D loss: 0.656749, acc: 57.03%] [G loss: 1.811271]\n",
      "epoch:24 step:23042 [D loss: 0.621716, acc: 66.41%] [G loss: 2.044933]\n",
      "epoch:24 step:23043 [D loss: 0.649661, acc: 60.16%] [G loss: 1.893702]\n",
      "epoch:24 step:23044 [D loss: 0.569216, acc: 64.06%] [G loss: 2.145139]\n",
      "epoch:24 step:23045 [D loss: 0.588648, acc: 68.75%] [G loss: 1.962855]\n",
      "epoch:24 step:23046 [D loss: 0.637330, acc: 63.28%] [G loss: 2.068350]\n",
      "epoch:24 step:23047 [D loss: 0.655133, acc: 57.81%] [G loss: 1.835743]\n",
      "epoch:24 step:23048 [D loss: 0.652169, acc: 57.81%] [G loss: 1.864577]\n",
      "epoch:24 step:23049 [D loss: 0.678035, acc: 61.72%] [G loss: 1.831445]\n",
      "epoch:24 step:23050 [D loss: 0.655331, acc: 64.06%] [G loss: 2.102674]\n",
      "epoch:24 step:23051 [D loss: 0.619716, acc: 66.41%] [G loss: 2.020780]\n",
      "epoch:24 step:23052 [D loss: 0.582788, acc: 70.31%] [G loss: 2.235190]\n",
      "epoch:24 step:23053 [D loss: 0.708862, acc: 55.47%] [G loss: 1.811602]\n",
      "epoch:24 step:23054 [D loss: 0.718664, acc: 56.25%] [G loss: 1.729668]\n",
      "epoch:24 step:23055 [D loss: 0.697852, acc: 53.91%] [G loss: 1.812400]\n",
      "epoch:24 step:23056 [D loss: 0.703648, acc: 51.56%] [G loss: 1.906295]\n",
      "epoch:24 step:23057 [D loss: 0.653287, acc: 63.28%] [G loss: 1.838597]\n",
      "epoch:24 step:23058 [D loss: 0.636993, acc: 61.72%] [G loss: 1.833287]\n",
      "epoch:24 step:23059 [D loss: 0.644979, acc: 65.62%] [G loss: 1.984815]\n",
      "epoch:24 step:23060 [D loss: 0.637543, acc: 59.38%] [G loss: 1.837104]\n",
      "epoch:24 step:23061 [D loss: 0.684999, acc: 55.47%] [G loss: 1.789140]\n",
      "epoch:24 step:23062 [D loss: 0.611192, acc: 67.19%] [G loss: 1.818087]\n",
      "epoch:24 step:23063 [D loss: 0.642373, acc: 66.41%] [G loss: 1.914083]\n",
      "epoch:24 step:23064 [D loss: 0.671158, acc: 57.81%] [G loss: 1.741434]\n",
      "epoch:24 step:23065 [D loss: 0.633738, acc: 58.59%] [G loss: 1.769663]\n",
      "epoch:24 step:23066 [D loss: 0.595059, acc: 67.19%] [G loss: 1.856676]\n",
      "epoch:24 step:23067 [D loss: 0.617443, acc: 65.62%] [G loss: 1.871259]\n",
      "epoch:24 step:23068 [D loss: 0.723392, acc: 53.12%] [G loss: 1.874024]\n",
      "epoch:24 step:23069 [D loss: 0.659526, acc: 63.28%] [G loss: 1.789486]\n",
      "epoch:24 step:23070 [D loss: 0.636547, acc: 67.19%] [G loss: 1.843024]\n",
      "epoch:24 step:23071 [D loss: 0.597608, acc: 62.50%] [G loss: 1.901185]\n",
      "epoch:24 step:23072 [D loss: 0.691283, acc: 52.34%] [G loss: 1.718095]\n",
      "epoch:24 step:23073 [D loss: 0.660421, acc: 60.94%] [G loss: 2.136311]\n",
      "epoch:24 step:23074 [D loss: 0.705665, acc: 58.59%] [G loss: 1.853988]\n",
      "epoch:24 step:23075 [D loss: 0.623116, acc: 63.28%] [G loss: 1.965336]\n",
      "epoch:24 step:23076 [D loss: 0.619172, acc: 61.72%] [G loss: 1.989114]\n",
      "epoch:24 step:23077 [D loss: 0.638227, acc: 64.06%] [G loss: 1.750209]\n",
      "epoch:24 step:23078 [D loss: 0.645013, acc: 64.06%] [G loss: 1.978073]\n",
      "epoch:24 step:23079 [D loss: 0.626251, acc: 64.06%] [G loss: 1.906425]\n",
      "epoch:24 step:23080 [D loss: 0.662463, acc: 64.06%] [G loss: 1.950993]\n",
      "epoch:24 step:23081 [D loss: 0.596585, acc: 67.19%] [G loss: 1.979734]\n",
      "epoch:24 step:23082 [D loss: 0.681484, acc: 57.81%] [G loss: 1.889820]\n",
      "epoch:24 step:23083 [D loss: 0.647263, acc: 59.38%] [G loss: 1.921646]\n",
      "epoch:24 step:23084 [D loss: 0.742685, acc: 49.22%] [G loss: 1.838849]\n",
      "epoch:24 step:23085 [D loss: 0.664895, acc: 63.28%] [G loss: 1.859096]\n",
      "epoch:24 step:23086 [D loss: 0.648762, acc: 58.59%] [G loss: 1.783912]\n",
      "epoch:24 step:23087 [D loss: 0.634230, acc: 63.28%] [G loss: 1.879255]\n",
      "epoch:24 step:23088 [D loss: 0.686940, acc: 59.38%] [G loss: 1.829290]\n",
      "epoch:24 step:23089 [D loss: 0.656436, acc: 58.59%] [G loss: 1.937449]\n",
      "epoch:24 step:23090 [D loss: 0.637806, acc: 58.59%] [G loss: 1.934147]\n",
      "epoch:24 step:23091 [D loss: 0.661599, acc: 61.72%] [G loss: 1.878239]\n",
      "epoch:24 step:23092 [D loss: 0.668109, acc: 62.50%] [G loss: 1.872134]\n",
      "epoch:24 step:23093 [D loss: 0.664883, acc: 55.47%] [G loss: 1.836998]\n",
      "epoch:24 step:23094 [D loss: 0.692891, acc: 54.69%] [G loss: 1.911874]\n",
      "epoch:24 step:23095 [D loss: 0.643725, acc: 60.16%] [G loss: 1.907510]\n",
      "epoch:24 step:23096 [D loss: 0.634662, acc: 56.25%] [G loss: 1.796712]\n",
      "epoch:24 step:23097 [D loss: 0.650541, acc: 64.06%] [G loss: 1.896792]\n",
      "epoch:24 step:23098 [D loss: 0.679762, acc: 55.47%] [G loss: 1.800411]\n",
      "epoch:24 step:23099 [D loss: 0.637411, acc: 65.62%] [G loss: 1.740881]\n",
      "epoch:24 step:23100 [D loss: 0.696530, acc: 57.03%] [G loss: 1.857054]\n",
      "epoch:24 step:23101 [D loss: 0.646377, acc: 64.84%] [G loss: 1.822679]\n",
      "epoch:24 step:23102 [D loss: 0.668441, acc: 60.16%] [G loss: 1.674910]\n",
      "epoch:24 step:23103 [D loss: 0.660046, acc: 55.47%] [G loss: 1.684231]\n",
      "epoch:24 step:23104 [D loss: 0.691178, acc: 57.81%] [G loss: 1.878124]\n",
      "epoch:24 step:23105 [D loss: 0.680480, acc: 60.16%] [G loss: 1.752023]\n",
      "epoch:24 step:23106 [D loss: 0.635447, acc: 67.97%] [G loss: 1.892673]\n",
      "epoch:24 step:23107 [D loss: 0.643927, acc: 59.38%] [G loss: 1.804386]\n",
      "epoch:24 step:23108 [D loss: 0.645346, acc: 64.84%] [G loss: 1.944881]\n",
      "epoch:24 step:23109 [D loss: 0.668556, acc: 58.59%] [G loss: 1.886930]\n",
      "epoch:24 step:23110 [D loss: 0.635804, acc: 62.50%] [G loss: 1.816647]\n",
      "epoch:24 step:23111 [D loss: 0.637079, acc: 60.16%] [G loss: 1.891343]\n",
      "epoch:24 step:23112 [D loss: 0.614723, acc: 71.09%] [G loss: 2.035492]\n",
      "epoch:24 step:23113 [D loss: 0.637787, acc: 65.62%] [G loss: 1.794911]\n",
      "epoch:24 step:23114 [D loss: 0.640177, acc: 63.28%] [G loss: 1.884683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23115 [D loss: 0.670960, acc: 60.94%] [G loss: 1.721375]\n",
      "epoch:24 step:23116 [D loss: 0.680559, acc: 60.94%] [G loss: 1.809962]\n",
      "epoch:24 step:23117 [D loss: 0.648556, acc: 59.38%] [G loss: 1.972839]\n",
      "epoch:24 step:23118 [D loss: 0.624130, acc: 64.06%] [G loss: 1.873361]\n",
      "epoch:24 step:23119 [D loss: 0.656567, acc: 63.28%] [G loss: 1.895540]\n",
      "epoch:24 step:23120 [D loss: 0.648917, acc: 61.72%] [G loss: 1.872529]\n",
      "epoch:24 step:23121 [D loss: 0.622364, acc: 61.72%] [G loss: 1.933693]\n",
      "epoch:24 step:23122 [D loss: 0.593723, acc: 67.97%] [G loss: 1.826142]\n",
      "epoch:24 step:23123 [D loss: 0.634927, acc: 65.62%] [G loss: 2.109595]\n",
      "epoch:24 step:23124 [D loss: 0.656881, acc: 64.84%] [G loss: 1.848476]\n",
      "epoch:24 step:23125 [D loss: 0.662346, acc: 62.50%] [G loss: 1.941672]\n",
      "epoch:24 step:23126 [D loss: 0.637435, acc: 67.19%] [G loss: 1.755663]\n",
      "epoch:24 step:23127 [D loss: 0.646488, acc: 61.72%] [G loss: 1.877900]\n",
      "epoch:24 step:23128 [D loss: 0.645761, acc: 61.72%] [G loss: 1.834576]\n",
      "epoch:24 step:23129 [D loss: 0.667608, acc: 63.28%] [G loss: 1.903536]\n",
      "epoch:24 step:23130 [D loss: 0.676657, acc: 60.16%] [G loss: 2.076381]\n",
      "epoch:24 step:23131 [D loss: 0.641168, acc: 63.28%] [G loss: 1.914626]\n",
      "epoch:24 step:23132 [D loss: 0.604518, acc: 63.28%] [G loss: 1.982600]\n",
      "epoch:24 step:23133 [D loss: 0.625393, acc: 65.62%] [G loss: 1.903774]\n",
      "epoch:24 step:23134 [D loss: 0.649763, acc: 64.06%] [G loss: 2.000847]\n",
      "epoch:24 step:23135 [D loss: 0.567164, acc: 73.44%] [G loss: 2.319463]\n",
      "epoch:24 step:23136 [D loss: 0.608526, acc: 66.41%] [G loss: 2.283745]\n",
      "epoch:24 step:23137 [D loss: 0.624363, acc: 59.38%] [G loss: 2.131230]\n",
      "epoch:24 step:23138 [D loss: 0.673594, acc: 64.84%] [G loss: 2.048898]\n",
      "epoch:24 step:23139 [D loss: 0.621075, acc: 60.94%] [G loss: 1.990056]\n",
      "epoch:24 step:23140 [D loss: 0.628069, acc: 66.41%] [G loss: 1.961867]\n",
      "epoch:24 step:23141 [D loss: 0.649330, acc: 60.16%] [G loss: 2.044490]\n",
      "epoch:24 step:23142 [D loss: 0.674616, acc: 61.72%] [G loss: 2.009045]\n",
      "epoch:24 step:23143 [D loss: 0.687257, acc: 59.38%] [G loss: 1.792324]\n",
      "epoch:24 step:23144 [D loss: 0.699767, acc: 59.38%] [G loss: 1.846099]\n",
      "epoch:24 step:23145 [D loss: 0.698152, acc: 52.34%] [G loss: 1.702650]\n",
      "epoch:24 step:23146 [D loss: 0.708692, acc: 56.25%] [G loss: 1.750175]\n",
      "epoch:24 step:23147 [D loss: 0.655504, acc: 57.03%] [G loss: 1.833989]\n",
      "epoch:24 step:23148 [D loss: 0.641608, acc: 63.28%] [G loss: 1.854893]\n",
      "epoch:24 step:23149 [D loss: 0.690918, acc: 58.59%] [G loss: 1.880229]\n",
      "epoch:24 step:23150 [D loss: 0.678675, acc: 57.81%] [G loss: 1.840750]\n",
      "epoch:24 step:23151 [D loss: 0.649947, acc: 59.38%] [G loss: 1.862363]\n",
      "epoch:24 step:23152 [D loss: 0.662693, acc: 57.81%] [G loss: 1.733961]\n",
      "epoch:24 step:23153 [D loss: 0.615238, acc: 64.84%] [G loss: 1.738634]\n",
      "epoch:24 step:23154 [D loss: 0.689922, acc: 56.25%] [G loss: 1.728539]\n",
      "epoch:24 step:23155 [D loss: 0.633021, acc: 63.28%] [G loss: 1.839633]\n",
      "epoch:24 step:23156 [D loss: 0.639205, acc: 66.41%] [G loss: 1.832170]\n",
      "epoch:24 step:23157 [D loss: 0.639567, acc: 67.19%] [G loss: 1.793913]\n",
      "epoch:24 step:23158 [D loss: 0.650922, acc: 63.28%] [G loss: 1.769437]\n",
      "epoch:24 step:23159 [D loss: 0.669344, acc: 63.28%] [G loss: 1.886450]\n",
      "epoch:24 step:23160 [D loss: 0.649658, acc: 64.06%] [G loss: 1.728409]\n",
      "epoch:24 step:23161 [D loss: 0.699064, acc: 52.34%] [G loss: 1.855269]\n",
      "epoch:24 step:23162 [D loss: 0.661753, acc: 60.16%] [G loss: 1.821353]\n",
      "epoch:24 step:23163 [D loss: 0.704253, acc: 59.38%] [G loss: 1.698445]\n",
      "epoch:24 step:23164 [D loss: 0.645734, acc: 62.50%] [G loss: 1.767741]\n",
      "epoch:24 step:23165 [D loss: 0.630102, acc: 58.59%] [G loss: 1.869502]\n",
      "epoch:24 step:23166 [D loss: 0.648934, acc: 62.50%] [G loss: 1.795377]\n",
      "epoch:24 step:23167 [D loss: 0.697163, acc: 61.72%] [G loss: 1.805623]\n",
      "epoch:24 step:23168 [D loss: 0.681536, acc: 62.50%] [G loss: 1.806736]\n",
      "epoch:24 step:23169 [D loss: 0.592846, acc: 75.00%] [G loss: 1.806129]\n",
      "epoch:24 step:23170 [D loss: 0.710428, acc: 56.25%] [G loss: 1.767951]\n",
      "epoch:24 step:23171 [D loss: 0.642679, acc: 60.16%] [G loss: 1.812719]\n",
      "epoch:24 step:23172 [D loss: 0.689890, acc: 57.03%] [G loss: 1.785693]\n",
      "epoch:24 step:23173 [D loss: 0.653007, acc: 64.06%] [G loss: 1.713278]\n",
      "epoch:24 step:23174 [D loss: 0.643750, acc: 63.28%] [G loss: 2.008300]\n",
      "epoch:24 step:23175 [D loss: 0.640097, acc: 66.41%] [G loss: 1.861182]\n",
      "epoch:24 step:23176 [D loss: 0.642794, acc: 62.50%] [G loss: 1.928714]\n",
      "epoch:24 step:23177 [D loss: 0.659032, acc: 61.72%] [G loss: 1.971073]\n",
      "epoch:24 step:23178 [D loss: 0.635313, acc: 67.97%] [G loss: 1.977316]\n",
      "epoch:24 step:23179 [D loss: 0.658836, acc: 62.50%] [G loss: 2.116800]\n",
      "epoch:24 step:23180 [D loss: 0.621608, acc: 64.84%] [G loss: 1.982932]\n",
      "epoch:24 step:23181 [D loss: 0.649980, acc: 58.59%] [G loss: 1.831597]\n",
      "epoch:24 step:23182 [D loss: 0.603017, acc: 66.41%] [G loss: 2.104499]\n",
      "epoch:24 step:23183 [D loss: 0.626473, acc: 65.62%] [G loss: 1.890282]\n",
      "epoch:24 step:23184 [D loss: 0.679898, acc: 60.16%] [G loss: 1.764228]\n",
      "epoch:24 step:23185 [D loss: 0.663152, acc: 56.25%] [G loss: 1.833534]\n",
      "epoch:24 step:23186 [D loss: 0.677725, acc: 59.38%] [G loss: 1.782186]\n",
      "epoch:24 step:23187 [D loss: 0.636258, acc: 64.06%] [G loss: 1.811803]\n",
      "epoch:24 step:23188 [D loss: 0.642611, acc: 56.25%] [G loss: 1.940596]\n",
      "epoch:24 step:23189 [D loss: 0.686935, acc: 60.94%] [G loss: 1.898379]\n",
      "epoch:24 step:23190 [D loss: 0.640244, acc: 64.06%] [G loss: 1.785029]\n",
      "epoch:24 step:23191 [D loss: 0.637273, acc: 64.06%] [G loss: 1.907148]\n",
      "epoch:24 step:23192 [D loss: 0.638870, acc: 61.72%] [G loss: 1.791328]\n",
      "epoch:24 step:23193 [D loss: 0.636400, acc: 64.06%] [G loss: 1.888740]\n",
      "epoch:24 step:23194 [D loss: 0.613658, acc: 70.31%] [G loss: 1.998946]\n",
      "epoch:24 step:23195 [D loss: 0.610319, acc: 68.75%] [G loss: 1.983500]\n",
      "epoch:24 step:23196 [D loss: 0.637382, acc: 64.84%] [G loss: 2.015761]\n",
      "epoch:24 step:23197 [D loss: 0.635108, acc: 64.06%] [G loss: 1.986220]\n",
      "epoch:24 step:23198 [D loss: 0.625246, acc: 66.41%] [G loss: 1.783293]\n",
      "epoch:24 step:23199 [D loss: 0.652685, acc: 67.97%] [G loss: 1.985195]\n",
      "epoch:24 step:23200 [D loss: 0.633393, acc: 63.28%] [G loss: 2.071912]\n",
      "##############\n",
      "[2.48595022 1.76571863 5.92674376 4.60615849 3.39501592 5.51394416\n",
      " 4.21643786 4.5929066  4.46925288 3.72048954]\n",
      "##########\n",
      "epoch:24 step:23201 [D loss: 0.659575, acc: 64.06%] [G loss: 1.941355]\n",
      "epoch:24 step:23202 [D loss: 0.674150, acc: 59.38%] [G loss: 1.887981]\n",
      "epoch:24 step:23203 [D loss: 0.694898, acc: 52.34%] [G loss: 1.826146]\n",
      "epoch:24 step:23204 [D loss: 0.727052, acc: 47.66%] [G loss: 1.735891]\n",
      "epoch:24 step:23205 [D loss: 0.643665, acc: 60.94%] [G loss: 1.762092]\n",
      "epoch:24 step:23206 [D loss: 0.648791, acc: 65.62%] [G loss: 1.930835]\n",
      "epoch:24 step:23207 [D loss: 0.643901, acc: 62.50%] [G loss: 1.930936]\n",
      "epoch:24 step:23208 [D loss: 0.655014, acc: 56.25%] [G loss: 1.915906]\n",
      "epoch:24 step:23209 [D loss: 0.575293, acc: 70.31%] [G loss: 1.970557]\n",
      "epoch:24 step:23210 [D loss: 0.652516, acc: 61.72%] [G loss: 1.944851]\n",
      "epoch:24 step:23211 [D loss: 0.693015, acc: 53.91%] [G loss: 1.790148]\n",
      "epoch:24 step:23212 [D loss: 0.620055, acc: 62.50%] [G loss: 1.959927]\n",
      "epoch:24 step:23213 [D loss: 0.722263, acc: 51.56%] [G loss: 1.967076]\n",
      "epoch:24 step:23214 [D loss: 0.628770, acc: 65.62%] [G loss: 1.880438]\n",
      "epoch:24 step:23215 [D loss: 0.633476, acc: 58.59%] [G loss: 1.936625]\n",
      "epoch:24 step:23216 [D loss: 0.605431, acc: 66.41%] [G loss: 1.888623]\n",
      "epoch:24 step:23217 [D loss: 0.680976, acc: 57.03%] [G loss: 1.917760]\n",
      "epoch:24 step:23218 [D loss: 0.672008, acc: 58.59%] [G loss: 1.826983]\n",
      "epoch:24 step:23219 [D loss: 0.690870, acc: 57.81%] [G loss: 1.801118]\n",
      "epoch:24 step:23220 [D loss: 0.673127, acc: 60.16%] [G loss: 1.791123]\n",
      "epoch:24 step:23221 [D loss: 0.605778, acc: 62.50%] [G loss: 1.844718]\n",
      "epoch:24 step:23222 [D loss: 0.623353, acc: 63.28%] [G loss: 1.934558]\n",
      "epoch:24 step:23223 [D loss: 0.614065, acc: 65.62%] [G loss: 1.811527]\n",
      "epoch:24 step:23224 [D loss: 0.652537, acc: 61.72%] [G loss: 1.879868]\n",
      "epoch:24 step:23225 [D loss: 0.628134, acc: 66.41%] [G loss: 1.760945]\n",
      "epoch:24 step:23226 [D loss: 0.637689, acc: 67.97%] [G loss: 1.885567]\n",
      "epoch:24 step:23227 [D loss: 0.654884, acc: 57.03%] [G loss: 1.699837]\n",
      "epoch:24 step:23228 [D loss: 0.635280, acc: 62.50%] [G loss: 1.872993]\n",
      "epoch:24 step:23229 [D loss: 0.667391, acc: 57.81%] [G loss: 1.810333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23230 [D loss: 0.685323, acc: 58.59%] [G loss: 1.817926]\n",
      "epoch:24 step:23231 [D loss: 0.647036, acc: 61.72%] [G loss: 1.814670]\n",
      "epoch:24 step:23232 [D loss: 0.617529, acc: 63.28%] [G loss: 1.833727]\n",
      "epoch:24 step:23233 [D loss: 0.649640, acc: 58.59%] [G loss: 1.788527]\n",
      "epoch:24 step:23234 [D loss: 0.625638, acc: 62.50%] [G loss: 2.083357]\n",
      "epoch:24 step:23235 [D loss: 0.622631, acc: 60.94%] [G loss: 1.960863]\n",
      "epoch:24 step:23236 [D loss: 0.675195, acc: 63.28%] [G loss: 1.924662]\n",
      "epoch:24 step:23237 [D loss: 0.688045, acc: 61.72%] [G loss: 1.802165]\n",
      "epoch:24 step:23238 [D loss: 0.667583, acc: 61.72%] [G loss: 1.796951]\n",
      "epoch:24 step:23239 [D loss: 0.635712, acc: 59.38%] [G loss: 1.789151]\n",
      "epoch:24 step:23240 [D loss: 0.723036, acc: 52.34%] [G loss: 1.686802]\n",
      "epoch:24 step:23241 [D loss: 0.663840, acc: 57.03%] [G loss: 1.789843]\n",
      "epoch:24 step:23242 [D loss: 0.646970, acc: 64.06%] [G loss: 2.014221]\n",
      "epoch:24 step:23243 [D loss: 0.632926, acc: 66.41%] [G loss: 1.860766]\n",
      "epoch:24 step:23244 [D loss: 0.691306, acc: 54.69%] [G loss: 1.835741]\n",
      "epoch:24 step:23245 [D loss: 0.633542, acc: 60.16%] [G loss: 1.922689]\n",
      "epoch:24 step:23246 [D loss: 0.660186, acc: 58.59%] [G loss: 1.859769]\n",
      "epoch:24 step:23247 [D loss: 0.706826, acc: 56.25%] [G loss: 1.780690]\n",
      "epoch:24 step:23248 [D loss: 0.617694, acc: 60.94%] [G loss: 1.789677]\n",
      "epoch:24 step:23249 [D loss: 0.668587, acc: 55.47%] [G loss: 1.805439]\n",
      "epoch:24 step:23250 [D loss: 0.691805, acc: 51.56%] [G loss: 1.846984]\n",
      "epoch:24 step:23251 [D loss: 0.676932, acc: 60.94%] [G loss: 1.853846]\n",
      "epoch:24 step:23252 [D loss: 0.632842, acc: 62.50%] [G loss: 1.772099]\n",
      "epoch:24 step:23253 [D loss: 0.676607, acc: 60.16%] [G loss: 1.667636]\n",
      "epoch:24 step:23254 [D loss: 0.698526, acc: 53.91%] [G loss: 1.906963]\n",
      "epoch:24 step:23255 [D loss: 0.655493, acc: 61.72%] [G loss: 1.742984]\n",
      "epoch:24 step:23256 [D loss: 0.658392, acc: 65.62%] [G loss: 1.747637]\n",
      "epoch:24 step:23257 [D loss: 0.606662, acc: 60.94%] [G loss: 1.916696]\n",
      "epoch:24 step:23258 [D loss: 0.642690, acc: 60.16%] [G loss: 1.977452]\n",
      "epoch:24 step:23259 [D loss: 0.651878, acc: 66.41%] [G loss: 1.879125]\n",
      "epoch:24 step:23260 [D loss: 0.619567, acc: 65.62%] [G loss: 1.861365]\n",
      "epoch:24 step:23261 [D loss: 0.641842, acc: 63.28%] [G loss: 1.838941]\n",
      "epoch:24 step:23262 [D loss: 0.608626, acc: 72.66%] [G loss: 2.205645]\n",
      "epoch:24 step:23263 [D loss: 0.592776, acc: 65.62%] [G loss: 2.147904]\n",
      "epoch:24 step:23264 [D loss: 0.674805, acc: 59.38%] [G loss: 1.878526]\n",
      "epoch:24 step:23265 [D loss: 0.583313, acc: 66.41%] [G loss: 1.870541]\n",
      "epoch:24 step:23266 [D loss: 0.662507, acc: 62.50%] [G loss: 1.957810]\n",
      "epoch:24 step:23267 [D loss: 0.636549, acc: 67.19%] [G loss: 1.893312]\n",
      "epoch:24 step:23268 [D loss: 0.635515, acc: 63.28%] [G loss: 1.963306]\n",
      "epoch:24 step:23269 [D loss: 0.625474, acc: 65.62%] [G loss: 2.043757]\n",
      "epoch:24 step:23270 [D loss: 0.595048, acc: 67.19%] [G loss: 2.124526]\n",
      "epoch:24 step:23271 [D loss: 0.679773, acc: 61.72%] [G loss: 1.896649]\n",
      "epoch:24 step:23272 [D loss: 0.664841, acc: 57.03%] [G loss: 1.826839]\n",
      "epoch:24 step:23273 [D loss: 0.609344, acc: 66.41%] [G loss: 1.937859]\n",
      "epoch:24 step:23274 [D loss: 0.581469, acc: 70.31%] [G loss: 2.016187]\n",
      "epoch:24 step:23275 [D loss: 0.715472, acc: 48.44%] [G loss: 1.800175]\n",
      "epoch:24 step:23276 [D loss: 0.654068, acc: 65.62%] [G loss: 1.882342]\n",
      "epoch:24 step:23277 [D loss: 0.626956, acc: 66.41%] [G loss: 1.888833]\n",
      "epoch:24 step:23278 [D loss: 0.693182, acc: 53.91%] [G loss: 1.874299]\n",
      "epoch:24 step:23279 [D loss: 0.605484, acc: 69.53%] [G loss: 1.970185]\n",
      "epoch:24 step:23280 [D loss: 0.653529, acc: 66.41%] [G loss: 2.065967]\n",
      "epoch:24 step:23281 [D loss: 0.653111, acc: 61.72%] [G loss: 2.051285]\n",
      "epoch:24 step:23282 [D loss: 0.663675, acc: 62.50%] [G loss: 1.743801]\n",
      "epoch:24 step:23283 [D loss: 0.657738, acc: 62.50%] [G loss: 1.838011]\n",
      "epoch:24 step:23284 [D loss: 0.674045, acc: 61.72%] [G loss: 1.940742]\n",
      "epoch:24 step:23285 [D loss: 0.663296, acc: 57.81%] [G loss: 1.798804]\n",
      "epoch:24 step:23286 [D loss: 0.645644, acc: 61.72%] [G loss: 1.788479]\n",
      "epoch:24 step:23287 [D loss: 0.654190, acc: 59.38%] [G loss: 1.889198]\n",
      "epoch:24 step:23288 [D loss: 0.702918, acc: 54.69%] [G loss: 1.704285]\n",
      "epoch:24 step:23289 [D loss: 0.764177, acc: 46.09%] [G loss: 1.794594]\n",
      "epoch:24 step:23290 [D loss: 0.673048, acc: 54.69%] [G loss: 1.768224]\n",
      "epoch:24 step:23291 [D loss: 0.682827, acc: 56.25%] [G loss: 1.751193]\n",
      "epoch:24 step:23292 [D loss: 0.628522, acc: 64.06%] [G loss: 2.097345]\n",
      "epoch:24 step:23293 [D loss: 0.646963, acc: 60.16%] [G loss: 1.963497]\n",
      "epoch:24 step:23294 [D loss: 0.599062, acc: 64.84%] [G loss: 1.788529]\n",
      "epoch:24 step:23295 [D loss: 0.625622, acc: 64.06%] [G loss: 1.942672]\n",
      "epoch:24 step:23296 [D loss: 0.645348, acc: 61.72%] [G loss: 1.804253]\n",
      "epoch:24 step:23297 [D loss: 0.654288, acc: 64.06%] [G loss: 1.881522]\n",
      "epoch:24 step:23298 [D loss: 0.605372, acc: 68.75%] [G loss: 2.072452]\n",
      "epoch:24 step:23299 [D loss: 0.629236, acc: 64.84%] [G loss: 1.767546]\n",
      "epoch:24 step:23300 [D loss: 0.659524, acc: 61.72%] [G loss: 1.880650]\n",
      "epoch:24 step:23301 [D loss: 0.620735, acc: 63.28%] [G loss: 1.935243]\n",
      "epoch:24 step:23302 [D loss: 0.632559, acc: 68.75%] [G loss: 1.888654]\n",
      "epoch:24 step:23303 [D loss: 0.623180, acc: 67.19%] [G loss: 2.204079]\n",
      "epoch:24 step:23304 [D loss: 0.615894, acc: 67.19%] [G loss: 2.029026]\n",
      "epoch:24 step:23305 [D loss: 0.643392, acc: 62.50%] [G loss: 1.772402]\n",
      "epoch:24 step:23306 [D loss: 0.634703, acc: 62.50%] [G loss: 1.875627]\n",
      "epoch:24 step:23307 [D loss: 0.664807, acc: 59.38%] [G loss: 1.981707]\n",
      "epoch:24 step:23308 [D loss: 0.719545, acc: 53.12%] [G loss: 1.713841]\n",
      "epoch:24 step:23309 [D loss: 0.676157, acc: 66.41%] [G loss: 1.752023]\n",
      "epoch:24 step:23310 [D loss: 0.651533, acc: 60.94%] [G loss: 1.910217]\n",
      "epoch:24 step:23311 [D loss: 0.586661, acc: 71.09%] [G loss: 2.010898]\n",
      "epoch:24 step:23312 [D loss: 0.622647, acc: 63.28%] [G loss: 1.849487]\n",
      "epoch:24 step:23313 [D loss: 0.635921, acc: 60.16%] [G loss: 1.980099]\n",
      "epoch:24 step:23314 [D loss: 0.673480, acc: 60.16%] [G loss: 1.802537]\n",
      "epoch:24 step:23315 [D loss: 0.699253, acc: 53.12%] [G loss: 1.737298]\n",
      "epoch:24 step:23316 [D loss: 0.647845, acc: 61.72%] [G loss: 1.689255]\n",
      "epoch:24 step:23317 [D loss: 0.686219, acc: 57.81%] [G loss: 1.727627]\n",
      "epoch:24 step:23318 [D loss: 0.651147, acc: 61.72%] [G loss: 1.798258]\n",
      "epoch:24 step:23319 [D loss: 0.664911, acc: 57.03%] [G loss: 1.733245]\n",
      "epoch:24 step:23320 [D loss: 0.656050, acc: 59.38%] [G loss: 1.921277]\n",
      "epoch:24 step:23321 [D loss: 0.669351, acc: 57.03%] [G loss: 1.928646]\n",
      "epoch:24 step:23322 [D loss: 0.653126, acc: 57.81%] [G loss: 1.790998]\n",
      "epoch:24 step:23323 [D loss: 0.664836, acc: 56.25%] [G loss: 1.758223]\n",
      "epoch:24 step:23324 [D loss: 0.622460, acc: 69.53%] [G loss: 1.816791]\n",
      "epoch:24 step:23325 [D loss: 0.629050, acc: 64.06%] [G loss: 1.902720]\n",
      "epoch:24 step:23326 [D loss: 0.674189, acc: 59.38%] [G loss: 1.888252]\n",
      "epoch:24 step:23327 [D loss: 0.589794, acc: 69.53%] [G loss: 1.844228]\n",
      "epoch:24 step:23328 [D loss: 0.629958, acc: 64.84%] [G loss: 1.868518]\n",
      "epoch:24 step:23329 [D loss: 0.643736, acc: 63.28%] [G loss: 2.010419]\n",
      "epoch:24 step:23330 [D loss: 0.605246, acc: 67.19%] [G loss: 1.970993]\n",
      "epoch:24 step:23331 [D loss: 0.690177, acc: 56.25%] [G loss: 1.749186]\n",
      "epoch:24 step:23332 [D loss: 0.656025, acc: 64.06%] [G loss: 1.704452]\n",
      "epoch:24 step:23333 [D loss: 0.602036, acc: 67.19%] [G loss: 2.025630]\n",
      "epoch:24 step:23334 [D loss: 0.621175, acc: 64.06%] [G loss: 1.759471]\n",
      "epoch:24 step:23335 [D loss: 0.634831, acc: 64.84%] [G loss: 1.945678]\n",
      "epoch:24 step:23336 [D loss: 0.650870, acc: 65.62%] [G loss: 1.987656]\n",
      "epoch:24 step:23337 [D loss: 0.595998, acc: 71.09%] [G loss: 2.072736]\n",
      "epoch:24 step:23338 [D loss: 0.717043, acc: 52.34%] [G loss: 1.818001]\n",
      "epoch:24 step:23339 [D loss: 0.645039, acc: 62.50%] [G loss: 1.824485]\n",
      "epoch:24 step:23340 [D loss: 0.608701, acc: 67.19%] [G loss: 1.979792]\n",
      "epoch:24 step:23341 [D loss: 0.652439, acc: 60.16%] [G loss: 1.886690]\n",
      "epoch:24 step:23342 [D loss: 0.630639, acc: 64.84%] [G loss: 1.811951]\n",
      "epoch:24 step:23343 [D loss: 0.627327, acc: 64.84%] [G loss: 1.855507]\n",
      "epoch:24 step:23344 [D loss: 0.665819, acc: 64.06%] [G loss: 1.829760]\n",
      "epoch:24 step:23345 [D loss: 0.665764, acc: 61.72%] [G loss: 1.844602]\n",
      "epoch:24 step:23346 [D loss: 0.704062, acc: 54.69%] [G loss: 1.881185]\n",
      "epoch:24 step:23347 [D loss: 0.665677, acc: 64.06%] [G loss: 1.949806]\n",
      "epoch:24 step:23348 [D loss: 0.645756, acc: 60.16%] [G loss: 1.851208]\n",
      "epoch:24 step:23349 [D loss: 0.666977, acc: 56.25%] [G loss: 1.851990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23350 [D loss: 0.690763, acc: 54.69%] [G loss: 1.694492]\n",
      "epoch:24 step:23351 [D loss: 0.650961, acc: 61.72%] [G loss: 1.778898]\n",
      "epoch:24 step:23352 [D loss: 0.629187, acc: 63.28%] [G loss: 1.854783]\n",
      "epoch:24 step:23353 [D loss: 0.667310, acc: 64.06%] [G loss: 1.709250]\n",
      "epoch:24 step:23354 [D loss: 0.642991, acc: 66.41%] [G loss: 1.804594]\n",
      "epoch:24 step:23355 [D loss: 0.678896, acc: 58.59%] [G loss: 1.831009]\n",
      "epoch:24 step:23356 [D loss: 0.626846, acc: 66.41%] [G loss: 2.044982]\n",
      "epoch:24 step:23357 [D loss: 0.654516, acc: 61.72%] [G loss: 1.727480]\n",
      "epoch:24 step:23358 [D loss: 0.674944, acc: 60.94%] [G loss: 1.809310]\n",
      "epoch:24 step:23359 [D loss: 0.633994, acc: 64.84%] [G loss: 1.824931]\n",
      "epoch:24 step:23360 [D loss: 0.691559, acc: 57.03%] [G loss: 1.760150]\n",
      "epoch:24 step:23361 [D loss: 0.659992, acc: 57.81%] [G loss: 1.615892]\n",
      "epoch:24 step:23362 [D loss: 0.667348, acc: 61.72%] [G loss: 1.787635]\n",
      "epoch:24 step:23363 [D loss: 0.635592, acc: 64.06%] [G loss: 2.026958]\n",
      "epoch:24 step:23364 [D loss: 0.614984, acc: 67.19%] [G loss: 1.771082]\n",
      "epoch:24 step:23365 [D loss: 0.673153, acc: 60.16%] [G loss: 1.767255]\n",
      "epoch:24 step:23366 [D loss: 0.601149, acc: 69.53%] [G loss: 1.865403]\n",
      "epoch:24 step:23367 [D loss: 0.625681, acc: 64.84%] [G loss: 1.892363]\n",
      "epoch:24 step:23368 [D loss: 0.656068, acc: 65.62%] [G loss: 1.806056]\n",
      "epoch:24 step:23369 [D loss: 0.617450, acc: 64.84%] [G loss: 1.991310]\n",
      "epoch:24 step:23370 [D loss: 0.629870, acc: 64.06%] [G loss: 1.845201]\n",
      "epoch:24 step:23371 [D loss: 0.611541, acc: 66.41%] [G loss: 1.926943]\n",
      "epoch:24 step:23372 [D loss: 0.601461, acc: 67.97%] [G loss: 2.031684]\n",
      "epoch:24 step:23373 [D loss: 0.633637, acc: 63.28%] [G loss: 1.883270]\n",
      "epoch:24 step:23374 [D loss: 0.565458, acc: 70.31%] [G loss: 1.997197]\n",
      "epoch:24 step:23375 [D loss: 0.679596, acc: 57.03%] [G loss: 1.911686]\n",
      "epoch:24 step:23376 [D loss: 0.693392, acc: 54.69%] [G loss: 1.945764]\n",
      "epoch:24 step:23377 [D loss: 0.559809, acc: 72.66%] [G loss: 1.884045]\n",
      "epoch:24 step:23378 [D loss: 0.630006, acc: 64.06%] [G loss: 1.940542]\n",
      "epoch:24 step:23379 [D loss: 0.671050, acc: 57.81%] [G loss: 1.887364]\n",
      "epoch:24 step:23380 [D loss: 0.683310, acc: 60.94%] [G loss: 1.891741]\n",
      "epoch:24 step:23381 [D loss: 0.666252, acc: 62.50%] [G loss: 1.883788]\n",
      "epoch:24 step:23382 [D loss: 0.601506, acc: 67.19%] [G loss: 1.947951]\n",
      "epoch:24 step:23383 [D loss: 0.685266, acc: 57.03%] [G loss: 1.784227]\n",
      "epoch:24 step:23384 [D loss: 0.656881, acc: 59.38%] [G loss: 1.760653]\n",
      "epoch:24 step:23385 [D loss: 0.659507, acc: 60.94%] [G loss: 1.769263]\n",
      "epoch:24 step:23386 [D loss: 0.683014, acc: 57.03%] [G loss: 1.877137]\n",
      "epoch:24 step:23387 [D loss: 0.608998, acc: 63.28%] [G loss: 1.905092]\n",
      "epoch:24 step:23388 [D loss: 0.633813, acc: 63.28%] [G loss: 1.948952]\n",
      "epoch:24 step:23389 [D loss: 0.614134, acc: 67.19%] [G loss: 1.864047]\n",
      "epoch:24 step:23390 [D loss: 0.672437, acc: 57.81%] [G loss: 1.807810]\n",
      "epoch:24 step:23391 [D loss: 0.634055, acc: 60.94%] [G loss: 1.766448]\n",
      "epoch:24 step:23392 [D loss: 0.611319, acc: 63.28%] [G loss: 1.995503]\n",
      "epoch:24 step:23393 [D loss: 0.612111, acc: 63.28%] [G loss: 2.004042]\n",
      "epoch:24 step:23394 [D loss: 0.571182, acc: 71.09%] [G loss: 1.909165]\n",
      "epoch:24 step:23395 [D loss: 0.691153, acc: 59.38%] [G loss: 1.955053]\n",
      "epoch:24 step:23396 [D loss: 0.600233, acc: 65.62%] [G loss: 2.032893]\n",
      "epoch:24 step:23397 [D loss: 0.607729, acc: 64.84%] [G loss: 2.018707]\n",
      "epoch:24 step:23398 [D loss: 0.664202, acc: 63.28%] [G loss: 1.924585]\n",
      "epoch:24 step:23399 [D loss: 0.646125, acc: 65.62%] [G loss: 1.974996]\n",
      "epoch:24 step:23400 [D loss: 0.637064, acc: 63.28%] [G loss: 2.125904]\n",
      "##############\n",
      "[2.34430085 1.60250082 6.24060922 4.68188958 3.71170148 5.57936724\n",
      " 4.41207824 4.70341768 4.43500246 3.55484016]\n",
      "##########\n",
      "epoch:24 step:23401 [D loss: 0.668600, acc: 58.59%] [G loss: 1.919976]\n",
      "epoch:24 step:23402 [D loss: 0.637261, acc: 60.94%] [G loss: 1.964969]\n",
      "epoch:24 step:23403 [D loss: 0.688825, acc: 60.16%] [G loss: 2.024876]\n",
      "epoch:24 step:23404 [D loss: 0.675387, acc: 60.94%] [G loss: 2.029893]\n",
      "epoch:24 step:23405 [D loss: 0.639123, acc: 62.50%] [G loss: 1.964987]\n",
      "epoch:24 step:23406 [D loss: 0.595942, acc: 72.66%] [G loss: 2.069360]\n",
      "epoch:24 step:23407 [D loss: 0.619864, acc: 69.53%] [G loss: 2.176284]\n",
      "epoch:24 step:23408 [D loss: 0.684677, acc: 51.56%] [G loss: 1.823077]\n",
      "epoch:24 step:23409 [D loss: 0.653142, acc: 60.16%] [G loss: 2.051739]\n",
      "epoch:24 step:23410 [D loss: 0.620028, acc: 66.41%] [G loss: 1.949383]\n",
      "epoch:24 step:23411 [D loss: 0.620223, acc: 65.62%] [G loss: 2.106048]\n",
      "epoch:24 step:23412 [D loss: 0.551094, acc: 75.78%] [G loss: 2.215473]\n",
      "epoch:24 step:23413 [D loss: 0.608898, acc: 65.62%] [G loss: 2.099266]\n",
      "epoch:24 step:23414 [D loss: 0.574909, acc: 67.19%] [G loss: 2.116841]\n",
      "epoch:24 step:23415 [D loss: 0.644094, acc: 62.50%] [G loss: 2.084316]\n",
      "epoch:24 step:23416 [D loss: 0.700491, acc: 55.47%] [G loss: 1.889952]\n",
      "epoch:24 step:23417 [D loss: 0.775079, acc: 49.22%] [G loss: 1.974033]\n",
      "epoch:24 step:23418 [D loss: 0.595847, acc: 70.31%] [G loss: 2.145741]\n",
      "epoch:24 step:23419 [D loss: 0.596428, acc: 65.62%] [G loss: 1.973497]\n",
      "epoch:24 step:23420 [D loss: 0.595295, acc: 64.06%] [G loss: 1.922537]\n",
      "epoch:24 step:23421 [D loss: 0.600072, acc: 70.31%] [G loss: 1.924905]\n",
      "epoch:24 step:23422 [D loss: 0.634804, acc: 66.41%] [G loss: 2.052635]\n",
      "epoch:24 step:23423 [D loss: 0.625989, acc: 71.09%] [G loss: 2.044329]\n",
      "epoch:24 step:23424 [D loss: 0.614543, acc: 64.06%] [G loss: 2.089557]\n",
      "epoch:24 step:23425 [D loss: 0.600793, acc: 69.53%] [G loss: 2.440712]\n",
      "epoch:25 step:23426 [D loss: 0.646116, acc: 61.72%] [G loss: 1.754201]\n",
      "epoch:25 step:23427 [D loss: 0.656688, acc: 57.81%] [G loss: 1.840861]\n",
      "epoch:25 step:23428 [D loss: 0.673162, acc: 64.84%] [G loss: 1.949724]\n",
      "epoch:25 step:23429 [D loss: 0.619275, acc: 61.72%] [G loss: 1.854924]\n",
      "epoch:25 step:23430 [D loss: 0.667641, acc: 60.94%] [G loss: 1.908191]\n",
      "epoch:25 step:23431 [D loss: 0.626182, acc: 62.50%] [G loss: 1.838462]\n",
      "epoch:25 step:23432 [D loss: 0.635965, acc: 64.06%] [G loss: 1.879399]\n",
      "epoch:25 step:23433 [D loss: 0.579995, acc: 72.66%] [G loss: 1.935535]\n",
      "epoch:25 step:23434 [D loss: 0.604782, acc: 66.41%] [G loss: 2.052482]\n",
      "epoch:25 step:23435 [D loss: 0.605410, acc: 70.31%] [G loss: 2.095363]\n",
      "epoch:25 step:23436 [D loss: 0.601309, acc: 71.09%] [G loss: 2.042625]\n",
      "epoch:25 step:23437 [D loss: 0.661106, acc: 60.16%] [G loss: 1.915157]\n",
      "epoch:25 step:23438 [D loss: 0.722151, acc: 50.78%] [G loss: 1.909082]\n",
      "epoch:25 step:23439 [D loss: 0.613577, acc: 67.19%] [G loss: 1.717037]\n",
      "epoch:25 step:23440 [D loss: 0.597522, acc: 65.62%] [G loss: 2.025893]\n",
      "epoch:25 step:23441 [D loss: 0.614195, acc: 65.62%] [G loss: 2.124429]\n",
      "epoch:25 step:23442 [D loss: 0.602880, acc: 71.09%] [G loss: 1.971857]\n",
      "epoch:25 step:23443 [D loss: 0.660317, acc: 64.06%] [G loss: 2.132829]\n",
      "epoch:25 step:23444 [D loss: 0.682153, acc: 59.38%] [G loss: 1.984422]\n",
      "epoch:25 step:23445 [D loss: 0.732703, acc: 51.56%] [G loss: 1.774174]\n",
      "epoch:25 step:23446 [D loss: 0.736277, acc: 53.12%] [G loss: 1.794957]\n",
      "epoch:25 step:23447 [D loss: 0.655607, acc: 59.38%] [G loss: 1.743647]\n",
      "epoch:25 step:23448 [D loss: 0.611681, acc: 69.53%] [G loss: 1.851869]\n",
      "epoch:25 step:23449 [D loss: 0.631819, acc: 62.50%] [G loss: 1.924219]\n",
      "epoch:25 step:23450 [D loss: 0.591755, acc: 71.09%] [G loss: 2.091037]\n",
      "epoch:25 step:23451 [D loss: 0.642121, acc: 63.28%] [G loss: 1.833670]\n",
      "epoch:25 step:23452 [D loss: 0.701954, acc: 57.03%] [G loss: 1.890834]\n",
      "epoch:25 step:23453 [D loss: 0.653181, acc: 60.94%] [G loss: 1.815112]\n",
      "epoch:25 step:23454 [D loss: 0.617115, acc: 65.62%] [G loss: 1.966398]\n",
      "epoch:25 step:23455 [D loss: 0.663860, acc: 60.94%] [G loss: 1.876701]\n",
      "epoch:25 step:23456 [D loss: 0.708525, acc: 54.69%] [G loss: 1.684878]\n",
      "epoch:25 step:23457 [D loss: 0.661962, acc: 60.16%] [G loss: 1.785154]\n",
      "epoch:25 step:23458 [D loss: 0.650154, acc: 60.16%] [G loss: 1.893480]\n",
      "epoch:25 step:23459 [D loss: 0.609820, acc: 64.06%] [G loss: 1.884466]\n",
      "epoch:25 step:23460 [D loss: 0.642804, acc: 63.28%] [G loss: 1.933742]\n",
      "epoch:25 step:23461 [D loss: 0.617249, acc: 67.19%] [G loss: 1.855117]\n",
      "epoch:25 step:23462 [D loss: 0.626727, acc: 63.28%] [G loss: 1.888172]\n",
      "epoch:25 step:23463 [D loss: 0.699889, acc: 58.59%] [G loss: 1.894125]\n",
      "epoch:25 step:23464 [D loss: 0.610919, acc: 64.84%] [G loss: 1.819016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23465 [D loss: 0.597453, acc: 67.19%] [G loss: 1.911821]\n",
      "epoch:25 step:23466 [D loss: 0.664794, acc: 60.16%] [G loss: 1.876680]\n",
      "epoch:25 step:23467 [D loss: 0.609658, acc: 66.41%] [G loss: 1.985203]\n",
      "epoch:25 step:23468 [D loss: 0.647406, acc: 60.16%] [G loss: 1.910409]\n",
      "epoch:25 step:23469 [D loss: 0.642346, acc: 62.50%] [G loss: 1.819540]\n",
      "epoch:25 step:23470 [D loss: 0.670209, acc: 57.03%] [G loss: 2.040331]\n",
      "epoch:25 step:23471 [D loss: 0.639404, acc: 63.28%] [G loss: 1.918824]\n",
      "epoch:25 step:23472 [D loss: 0.676562, acc: 58.59%] [G loss: 1.959841]\n",
      "epoch:25 step:23473 [D loss: 0.654499, acc: 59.38%] [G loss: 2.147970]\n",
      "epoch:25 step:23474 [D loss: 0.631915, acc: 60.94%] [G loss: 2.004836]\n",
      "epoch:25 step:23475 [D loss: 0.689043, acc: 58.59%] [G loss: 2.029523]\n",
      "epoch:25 step:23476 [D loss: 0.660598, acc: 60.16%] [G loss: 1.759888]\n",
      "epoch:25 step:23477 [D loss: 0.653163, acc: 65.62%] [G loss: 1.810322]\n",
      "epoch:25 step:23478 [D loss: 0.705567, acc: 57.81%] [G loss: 1.832056]\n",
      "epoch:25 step:23479 [D loss: 0.623080, acc: 64.84%] [G loss: 2.078812]\n",
      "epoch:25 step:23480 [D loss: 0.660521, acc: 56.25%] [G loss: 1.880741]\n",
      "epoch:25 step:23481 [D loss: 0.655274, acc: 61.72%] [G loss: 1.941933]\n",
      "epoch:25 step:23482 [D loss: 0.639191, acc: 66.41%] [G loss: 1.832759]\n",
      "epoch:25 step:23483 [D loss: 0.635456, acc: 56.25%] [G loss: 1.729499]\n",
      "epoch:25 step:23484 [D loss: 0.658823, acc: 54.69%] [G loss: 1.839049]\n",
      "epoch:25 step:23485 [D loss: 0.667091, acc: 60.94%] [G loss: 1.947119]\n",
      "epoch:25 step:23486 [D loss: 0.662113, acc: 62.50%] [G loss: 1.779257]\n",
      "epoch:25 step:23487 [D loss: 0.624600, acc: 66.41%] [G loss: 1.854827]\n",
      "epoch:25 step:23488 [D loss: 0.710892, acc: 57.03%] [G loss: 1.804455]\n",
      "epoch:25 step:23489 [D loss: 0.656439, acc: 64.84%] [G loss: 1.859866]\n",
      "epoch:25 step:23490 [D loss: 0.687760, acc: 54.69%] [G loss: 1.819107]\n",
      "epoch:25 step:23491 [D loss: 0.700989, acc: 57.03%] [G loss: 1.719889]\n",
      "epoch:25 step:23492 [D loss: 0.638835, acc: 59.38%] [G loss: 1.826059]\n",
      "epoch:25 step:23493 [D loss: 0.634603, acc: 65.62%] [G loss: 1.819370]\n",
      "epoch:25 step:23494 [D loss: 0.598545, acc: 65.62%] [G loss: 2.017856]\n",
      "epoch:25 step:23495 [D loss: 0.658840, acc: 57.81%] [G loss: 2.021438]\n",
      "epoch:25 step:23496 [D loss: 0.688435, acc: 55.47%] [G loss: 1.776966]\n",
      "epoch:25 step:23497 [D loss: 0.673975, acc: 57.81%] [G loss: 1.882575]\n",
      "epoch:25 step:23498 [D loss: 0.658470, acc: 60.16%] [G loss: 1.808168]\n",
      "epoch:25 step:23499 [D loss: 0.648927, acc: 61.72%] [G loss: 1.955849]\n",
      "epoch:25 step:23500 [D loss: 0.639211, acc: 69.53%] [G loss: 1.942070]\n",
      "epoch:25 step:23501 [D loss: 0.655403, acc: 57.81%] [G loss: 2.008657]\n",
      "epoch:25 step:23502 [D loss: 0.615578, acc: 67.19%] [G loss: 2.068357]\n",
      "epoch:25 step:23503 [D loss: 0.679754, acc: 59.38%] [G loss: 1.760181]\n",
      "epoch:25 step:23504 [D loss: 0.644127, acc: 63.28%] [G loss: 1.867319]\n",
      "epoch:25 step:23505 [D loss: 0.654017, acc: 60.16%] [G loss: 1.856726]\n",
      "epoch:25 step:23506 [D loss: 0.679455, acc: 54.69%] [G loss: 1.687985]\n",
      "epoch:25 step:23507 [D loss: 0.677341, acc: 61.72%] [G loss: 1.762728]\n",
      "epoch:25 step:23508 [D loss: 0.607844, acc: 64.84%] [G loss: 1.845738]\n",
      "epoch:25 step:23509 [D loss: 0.609841, acc: 63.28%] [G loss: 1.858485]\n",
      "epoch:25 step:23510 [D loss: 0.608686, acc: 67.97%] [G loss: 1.860682]\n",
      "epoch:25 step:23511 [D loss: 0.707125, acc: 58.59%] [G loss: 1.756152]\n",
      "epoch:25 step:23512 [D loss: 0.638175, acc: 64.84%] [G loss: 1.799544]\n",
      "epoch:25 step:23513 [D loss: 0.661950, acc: 62.50%] [G loss: 1.871532]\n",
      "epoch:25 step:23514 [D loss: 0.609059, acc: 74.22%] [G loss: 1.825526]\n",
      "epoch:25 step:23515 [D loss: 0.636630, acc: 60.94%] [G loss: 1.797295]\n",
      "epoch:25 step:23516 [D loss: 0.636346, acc: 64.06%] [G loss: 1.856616]\n",
      "epoch:25 step:23517 [D loss: 0.703567, acc: 57.81%] [G loss: 1.925683]\n",
      "epoch:25 step:23518 [D loss: 0.644643, acc: 64.84%] [G loss: 1.998212]\n",
      "epoch:25 step:23519 [D loss: 0.677633, acc: 57.03%] [G loss: 1.756975]\n",
      "epoch:25 step:23520 [D loss: 0.678124, acc: 56.25%] [G loss: 1.854295]\n",
      "epoch:25 step:23521 [D loss: 0.640085, acc: 64.84%] [G loss: 1.875121]\n",
      "epoch:25 step:23522 [D loss: 0.695014, acc: 53.91%] [G loss: 1.781427]\n",
      "epoch:25 step:23523 [D loss: 0.651664, acc: 60.94%] [G loss: 1.780543]\n",
      "epoch:25 step:23524 [D loss: 0.656043, acc: 60.16%] [G loss: 1.778643]\n",
      "epoch:25 step:23525 [D loss: 0.642848, acc: 64.06%] [G loss: 1.916787]\n",
      "epoch:25 step:23526 [D loss: 0.657292, acc: 62.50%] [G loss: 1.827450]\n",
      "epoch:25 step:23527 [D loss: 0.641911, acc: 60.94%] [G loss: 1.857610]\n",
      "epoch:25 step:23528 [D loss: 0.650775, acc: 59.38%] [G loss: 1.741868]\n",
      "epoch:25 step:23529 [D loss: 0.674611, acc: 61.72%] [G loss: 1.881120]\n",
      "epoch:25 step:23530 [D loss: 0.597726, acc: 67.97%] [G loss: 2.051633]\n",
      "epoch:25 step:23531 [D loss: 0.679342, acc: 65.62%] [G loss: 2.061285]\n",
      "epoch:25 step:23532 [D loss: 0.590412, acc: 64.84%] [G loss: 2.079438]\n",
      "epoch:25 step:23533 [D loss: 0.650075, acc: 60.94%] [G loss: 1.797011]\n",
      "epoch:25 step:23534 [D loss: 0.697709, acc: 57.81%] [G loss: 1.676474]\n",
      "epoch:25 step:23535 [D loss: 0.646016, acc: 64.84%] [G loss: 1.859962]\n",
      "epoch:25 step:23536 [D loss: 0.641464, acc: 63.28%] [G loss: 1.889016]\n",
      "epoch:25 step:23537 [D loss: 0.636647, acc: 62.50%] [G loss: 1.996505]\n",
      "epoch:25 step:23538 [D loss: 0.571286, acc: 72.66%] [G loss: 1.947244]\n",
      "epoch:25 step:23539 [D loss: 0.619312, acc: 64.06%] [G loss: 2.064548]\n",
      "epoch:25 step:23540 [D loss: 0.564566, acc: 76.56%] [G loss: 2.104044]\n",
      "epoch:25 step:23541 [D loss: 0.594184, acc: 65.62%] [G loss: 2.193768]\n",
      "epoch:25 step:23542 [D loss: 0.608346, acc: 71.88%] [G loss: 2.093529]\n",
      "epoch:25 step:23543 [D loss: 0.602836, acc: 71.09%] [G loss: 1.999768]\n",
      "epoch:25 step:23544 [D loss: 0.593010, acc: 68.75%] [G loss: 2.290512]\n",
      "epoch:25 step:23545 [D loss: 0.637955, acc: 64.06%] [G loss: 1.925994]\n",
      "epoch:25 step:23546 [D loss: 0.644764, acc: 64.06%] [G loss: 1.974590]\n",
      "epoch:25 step:23547 [D loss: 0.648768, acc: 58.59%] [G loss: 2.077136]\n",
      "epoch:25 step:23548 [D loss: 0.625467, acc: 64.06%] [G loss: 1.822893]\n",
      "epoch:25 step:23549 [D loss: 0.722431, acc: 53.91%] [G loss: 1.936060]\n",
      "epoch:25 step:23550 [D loss: 0.712415, acc: 53.91%] [G loss: 1.812808]\n",
      "epoch:25 step:23551 [D loss: 0.644000, acc: 61.72%] [G loss: 1.918641]\n",
      "epoch:25 step:23552 [D loss: 0.654179, acc: 63.28%] [G loss: 1.905239]\n",
      "epoch:25 step:23553 [D loss: 0.626830, acc: 63.28%] [G loss: 1.843203]\n",
      "epoch:25 step:23554 [D loss: 0.688132, acc: 52.34%] [G loss: 1.954621]\n",
      "epoch:25 step:23555 [D loss: 0.633310, acc: 64.84%] [G loss: 1.879966]\n",
      "epoch:25 step:23556 [D loss: 0.670090, acc: 60.16%] [G loss: 1.958004]\n",
      "epoch:25 step:23557 [D loss: 0.649265, acc: 60.16%] [G loss: 1.781310]\n",
      "epoch:25 step:23558 [D loss: 0.612181, acc: 67.97%] [G loss: 1.793995]\n",
      "epoch:25 step:23559 [D loss: 0.714714, acc: 54.69%] [G loss: 1.852686]\n",
      "epoch:25 step:23560 [D loss: 0.643155, acc: 64.06%] [G loss: 1.927010]\n",
      "epoch:25 step:23561 [D loss: 0.680238, acc: 60.94%] [G loss: 1.684247]\n",
      "epoch:25 step:23562 [D loss: 0.666999, acc: 54.69%] [G loss: 1.757321]\n",
      "epoch:25 step:23563 [D loss: 0.718374, acc: 53.91%] [G loss: 1.782265]\n",
      "epoch:25 step:23564 [D loss: 0.717112, acc: 51.56%] [G loss: 1.694577]\n",
      "epoch:25 step:23565 [D loss: 0.662441, acc: 61.72%] [G loss: 1.796836]\n",
      "epoch:25 step:23566 [D loss: 0.630191, acc: 59.38%] [G loss: 1.819334]\n",
      "epoch:25 step:23567 [D loss: 0.634285, acc: 65.62%] [G loss: 1.724998]\n",
      "epoch:25 step:23568 [D loss: 0.711563, acc: 50.78%] [G loss: 1.772510]\n",
      "epoch:25 step:23569 [D loss: 0.652002, acc: 60.16%] [G loss: 1.829546]\n",
      "epoch:25 step:23570 [D loss: 0.629098, acc: 61.72%] [G loss: 1.882264]\n",
      "epoch:25 step:23571 [D loss: 0.646057, acc: 63.28%] [G loss: 1.912750]\n",
      "epoch:25 step:23572 [D loss: 0.648275, acc: 64.84%] [G loss: 1.849138]\n",
      "epoch:25 step:23573 [D loss: 0.711519, acc: 63.28%] [G loss: 1.626957]\n",
      "epoch:25 step:23574 [D loss: 0.642948, acc: 61.72%] [G loss: 1.899127]\n",
      "epoch:25 step:23575 [D loss: 0.650761, acc: 58.59%] [G loss: 1.780333]\n",
      "epoch:25 step:23576 [D loss: 0.595719, acc: 71.88%] [G loss: 1.995179]\n",
      "epoch:25 step:23577 [D loss: 0.657965, acc: 56.25%] [G loss: 1.962054]\n",
      "epoch:25 step:23578 [D loss: 0.686591, acc: 60.16%] [G loss: 2.090674]\n",
      "epoch:25 step:23579 [D loss: 0.615943, acc: 64.06%] [G loss: 1.944050]\n",
      "epoch:25 step:23580 [D loss: 0.640373, acc: 64.06%] [G loss: 1.844638]\n",
      "epoch:25 step:23581 [D loss: 0.637560, acc: 60.16%] [G loss: 1.975918]\n",
      "epoch:25 step:23582 [D loss: 0.643744, acc: 60.94%] [G loss: 1.794723]\n",
      "epoch:25 step:23583 [D loss: 0.612038, acc: 67.19%] [G loss: 1.852575]\n",
      "epoch:25 step:23584 [D loss: 0.688701, acc: 55.47%] [G loss: 1.857288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23585 [D loss: 0.681849, acc: 60.94%] [G loss: 1.822797]\n",
      "epoch:25 step:23586 [D loss: 0.677708, acc: 64.06%] [G loss: 1.960777]\n",
      "epoch:25 step:23587 [D loss: 0.679082, acc: 57.03%] [G loss: 1.817380]\n",
      "epoch:25 step:23588 [D loss: 0.685101, acc: 56.25%] [G loss: 1.876509]\n",
      "epoch:25 step:23589 [D loss: 0.660610, acc: 62.50%] [G loss: 1.718551]\n",
      "epoch:25 step:23590 [D loss: 0.643312, acc: 62.50%] [G loss: 2.024545]\n",
      "epoch:25 step:23591 [D loss: 0.696259, acc: 56.25%] [G loss: 1.897433]\n",
      "epoch:25 step:23592 [D loss: 0.620290, acc: 68.75%] [G loss: 1.869256]\n",
      "epoch:25 step:23593 [D loss: 0.634281, acc: 64.06%] [G loss: 1.824351]\n",
      "epoch:25 step:23594 [D loss: 0.620291, acc: 63.28%] [G loss: 1.791777]\n",
      "epoch:25 step:23595 [D loss: 0.658388, acc: 61.72%] [G loss: 1.873703]\n",
      "epoch:25 step:23596 [D loss: 0.618383, acc: 67.19%] [G loss: 1.912272]\n",
      "epoch:25 step:23597 [D loss: 0.680681, acc: 57.81%] [G loss: 1.726960]\n",
      "epoch:25 step:23598 [D loss: 0.632841, acc: 66.41%] [G loss: 1.841533]\n",
      "epoch:25 step:23599 [D loss: 0.654123, acc: 60.16%] [G loss: 1.809619]\n",
      "epoch:25 step:23600 [D loss: 0.663308, acc: 59.38%] [G loss: 1.727051]\n",
      "##############\n",
      "[2.49969882 1.5386505  6.02749727 4.69993546 3.66127235 5.6417274\n",
      " 4.47991189 4.70486109 4.27270288 3.82810436]\n",
      "##########\n",
      "epoch:25 step:23601 [D loss: 0.674624, acc: 58.59%] [G loss: 1.681813]\n",
      "epoch:25 step:23602 [D loss: 0.635409, acc: 70.31%] [G loss: 1.808972]\n",
      "epoch:25 step:23603 [D loss: 0.635322, acc: 61.72%] [G loss: 1.781733]\n",
      "epoch:25 step:23604 [D loss: 0.679160, acc: 57.81%] [G loss: 1.824352]\n",
      "epoch:25 step:23605 [D loss: 0.661865, acc: 55.47%] [G loss: 1.778896]\n",
      "epoch:25 step:23606 [D loss: 0.683448, acc: 57.81%] [G loss: 1.908488]\n",
      "epoch:25 step:23607 [D loss: 0.653783, acc: 59.38%] [G loss: 1.843219]\n",
      "epoch:25 step:23608 [D loss: 0.731321, acc: 56.25%] [G loss: 1.727378]\n",
      "epoch:25 step:23609 [D loss: 0.674790, acc: 60.16%] [G loss: 1.752846]\n",
      "epoch:25 step:23610 [D loss: 0.728353, acc: 53.12%] [G loss: 1.881407]\n",
      "epoch:25 step:23611 [D loss: 0.638707, acc: 64.06%] [G loss: 1.766398]\n",
      "epoch:25 step:23612 [D loss: 0.683471, acc: 57.03%] [G loss: 1.879004]\n",
      "epoch:25 step:23613 [D loss: 0.629335, acc: 64.84%] [G loss: 1.908032]\n",
      "epoch:25 step:23614 [D loss: 0.660597, acc: 58.59%] [G loss: 1.701122]\n",
      "epoch:25 step:23615 [D loss: 0.641647, acc: 64.06%] [G loss: 1.831411]\n",
      "epoch:25 step:23616 [D loss: 0.624671, acc: 64.84%] [G loss: 1.780527]\n",
      "epoch:25 step:23617 [D loss: 0.669699, acc: 57.81%] [G loss: 1.905213]\n",
      "epoch:25 step:23618 [D loss: 0.665921, acc: 60.94%] [G loss: 1.795404]\n",
      "epoch:25 step:23619 [D loss: 0.628426, acc: 65.62%] [G loss: 1.872741]\n",
      "epoch:25 step:23620 [D loss: 0.660587, acc: 59.38%] [G loss: 1.882566]\n",
      "epoch:25 step:23621 [D loss: 0.636357, acc: 64.06%] [G loss: 1.852083]\n",
      "epoch:25 step:23622 [D loss: 0.675594, acc: 61.72%] [G loss: 1.881753]\n",
      "epoch:25 step:23623 [D loss: 0.662355, acc: 59.38%] [G loss: 1.876009]\n",
      "epoch:25 step:23624 [D loss: 0.664324, acc: 57.81%] [G loss: 1.785937]\n",
      "epoch:25 step:23625 [D loss: 0.683482, acc: 54.69%] [G loss: 1.766834]\n",
      "epoch:25 step:23626 [D loss: 0.654397, acc: 60.16%] [G loss: 1.967668]\n",
      "epoch:25 step:23627 [D loss: 0.623854, acc: 67.19%] [G loss: 1.834145]\n",
      "epoch:25 step:23628 [D loss: 0.652812, acc: 60.16%] [G loss: 1.970132]\n",
      "epoch:25 step:23629 [D loss: 0.645271, acc: 63.28%] [G loss: 1.959701]\n",
      "epoch:25 step:23630 [D loss: 0.671642, acc: 58.59%] [G loss: 1.834807]\n",
      "epoch:25 step:23631 [D loss: 0.621740, acc: 67.97%] [G loss: 1.881058]\n",
      "epoch:25 step:23632 [D loss: 0.614299, acc: 67.97%] [G loss: 2.047374]\n",
      "epoch:25 step:23633 [D loss: 0.658573, acc: 60.94%] [G loss: 2.124629]\n",
      "epoch:25 step:23634 [D loss: 0.597853, acc: 70.31%] [G loss: 1.925588]\n",
      "epoch:25 step:23635 [D loss: 0.645365, acc: 67.19%] [G loss: 1.862202]\n",
      "epoch:25 step:23636 [D loss: 0.653046, acc: 60.16%] [G loss: 1.653919]\n",
      "epoch:25 step:23637 [D loss: 0.660983, acc: 57.81%] [G loss: 1.785311]\n",
      "epoch:25 step:23638 [D loss: 0.688266, acc: 57.81%] [G loss: 1.876685]\n",
      "epoch:25 step:23639 [D loss: 0.661827, acc: 52.34%] [G loss: 1.704802]\n",
      "epoch:25 step:23640 [D loss: 0.668214, acc: 63.28%] [G loss: 1.906098]\n",
      "epoch:25 step:23641 [D loss: 0.629694, acc: 65.62%] [G loss: 2.039075]\n",
      "epoch:25 step:23642 [D loss: 0.673276, acc: 59.38%] [G loss: 2.053755]\n",
      "epoch:25 step:23643 [D loss: 0.565507, acc: 73.44%] [G loss: 2.056979]\n",
      "epoch:25 step:23644 [D loss: 0.622370, acc: 67.97%] [G loss: 2.041800]\n",
      "epoch:25 step:23645 [D loss: 0.725366, acc: 58.59%] [G loss: 1.772223]\n",
      "epoch:25 step:23646 [D loss: 0.610493, acc: 67.19%] [G loss: 1.933724]\n",
      "epoch:25 step:23647 [D loss: 0.680367, acc: 56.25%] [G loss: 1.898684]\n",
      "epoch:25 step:23648 [D loss: 0.664923, acc: 58.59%] [G loss: 1.850559]\n",
      "epoch:25 step:23649 [D loss: 0.658775, acc: 56.25%] [G loss: 1.764238]\n",
      "epoch:25 step:23650 [D loss: 0.703215, acc: 53.12%] [G loss: 1.730341]\n",
      "epoch:25 step:23651 [D loss: 0.641314, acc: 67.19%] [G loss: 1.762201]\n",
      "epoch:25 step:23652 [D loss: 0.668060, acc: 60.94%] [G loss: 1.789130]\n",
      "epoch:25 step:23653 [D loss: 0.658044, acc: 59.38%] [G loss: 1.686569]\n",
      "epoch:25 step:23654 [D loss: 0.622842, acc: 65.62%] [G loss: 2.179923]\n",
      "epoch:25 step:23655 [D loss: 0.618797, acc: 62.50%] [G loss: 2.132524]\n",
      "epoch:25 step:23656 [D loss: 0.568722, acc: 71.09%] [G loss: 2.365314]\n",
      "epoch:25 step:23657 [D loss: 0.536171, acc: 72.66%] [G loss: 2.242416]\n",
      "epoch:25 step:23658 [D loss: 0.674206, acc: 60.16%] [G loss: 1.951984]\n",
      "epoch:25 step:23659 [D loss: 0.690366, acc: 60.94%] [G loss: 1.715385]\n",
      "epoch:25 step:23660 [D loss: 0.657133, acc: 60.16%] [G loss: 1.695408]\n",
      "epoch:25 step:23661 [D loss: 0.662728, acc: 57.81%] [G loss: 1.798202]\n",
      "epoch:25 step:23662 [D loss: 0.626127, acc: 63.28%] [G loss: 1.884536]\n",
      "epoch:25 step:23663 [D loss: 0.658347, acc: 61.72%] [G loss: 1.855765]\n",
      "epoch:25 step:23664 [D loss: 0.683464, acc: 57.81%] [G loss: 1.747349]\n",
      "epoch:25 step:23665 [D loss: 0.651068, acc: 61.72%] [G loss: 1.843664]\n",
      "epoch:25 step:23666 [D loss: 0.643817, acc: 62.50%] [G loss: 1.733404]\n",
      "epoch:25 step:23667 [D loss: 0.651635, acc: 64.06%] [G loss: 1.919473]\n",
      "epoch:25 step:23668 [D loss: 0.673847, acc: 62.50%] [G loss: 1.960847]\n",
      "epoch:25 step:23669 [D loss: 0.693070, acc: 56.25%] [G loss: 1.901794]\n",
      "epoch:25 step:23670 [D loss: 0.606130, acc: 70.31%] [G loss: 2.001779]\n",
      "epoch:25 step:23671 [D loss: 0.660612, acc: 60.16%] [G loss: 1.804704]\n",
      "epoch:25 step:23672 [D loss: 0.659241, acc: 60.94%] [G loss: 1.938514]\n",
      "epoch:25 step:23673 [D loss: 0.629258, acc: 62.50%] [G loss: 1.985321]\n",
      "epoch:25 step:23674 [D loss: 0.704306, acc: 53.91%] [G loss: 1.737592]\n",
      "epoch:25 step:23675 [D loss: 0.729918, acc: 50.78%] [G loss: 1.740350]\n",
      "epoch:25 step:23676 [D loss: 0.665764, acc: 62.50%] [G loss: 1.695587]\n",
      "epoch:25 step:23677 [D loss: 0.696747, acc: 53.91%] [G loss: 1.748214]\n",
      "epoch:25 step:23678 [D loss: 0.633700, acc: 64.84%] [G loss: 1.871378]\n",
      "epoch:25 step:23679 [D loss: 0.691411, acc: 55.47%] [G loss: 1.832544]\n",
      "epoch:25 step:23680 [D loss: 0.685743, acc: 57.03%] [G loss: 1.801607]\n",
      "epoch:25 step:23681 [D loss: 0.697735, acc: 52.34%] [G loss: 1.776180]\n",
      "epoch:25 step:23682 [D loss: 0.672994, acc: 55.47%] [G loss: 1.673202]\n",
      "epoch:25 step:23683 [D loss: 0.662666, acc: 59.38%] [G loss: 1.763742]\n",
      "epoch:25 step:23684 [D loss: 0.627845, acc: 68.75%] [G loss: 1.799071]\n",
      "epoch:25 step:23685 [D loss: 0.629692, acc: 62.50%] [G loss: 1.805830]\n",
      "epoch:25 step:23686 [D loss: 0.655441, acc: 59.38%] [G loss: 1.847391]\n",
      "epoch:25 step:23687 [D loss: 0.619541, acc: 66.41%] [G loss: 1.836552]\n",
      "epoch:25 step:23688 [D loss: 0.651296, acc: 64.84%] [G loss: 1.832476]\n",
      "epoch:25 step:23689 [D loss: 0.631391, acc: 60.94%] [G loss: 1.968456]\n",
      "epoch:25 step:23690 [D loss: 0.646083, acc: 60.16%] [G loss: 1.856600]\n",
      "epoch:25 step:23691 [D loss: 0.604303, acc: 69.53%] [G loss: 1.800212]\n",
      "epoch:25 step:23692 [D loss: 0.620848, acc: 64.84%] [G loss: 1.976295]\n",
      "epoch:25 step:23693 [D loss: 0.663503, acc: 60.16%] [G loss: 1.835128]\n",
      "epoch:25 step:23694 [D loss: 0.637956, acc: 59.38%] [G loss: 1.946534]\n",
      "epoch:25 step:23695 [D loss: 0.578428, acc: 72.66%] [G loss: 1.860836]\n",
      "epoch:25 step:23696 [D loss: 0.649434, acc: 62.50%] [G loss: 1.926128]\n",
      "epoch:25 step:23697 [D loss: 0.674603, acc: 56.25%] [G loss: 1.915592]\n",
      "epoch:25 step:23698 [D loss: 0.591770, acc: 65.62%] [G loss: 1.805993]\n",
      "epoch:25 step:23699 [D loss: 0.622438, acc: 60.94%] [G loss: 2.082601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23700 [D loss: 0.637212, acc: 63.28%] [G loss: 1.974035]\n",
      "epoch:25 step:23701 [D loss: 0.591343, acc: 70.31%] [G loss: 2.104465]\n",
      "epoch:25 step:23702 [D loss: 0.607163, acc: 64.06%] [G loss: 1.893393]\n",
      "epoch:25 step:23703 [D loss: 0.670129, acc: 57.81%] [G loss: 1.813192]\n",
      "epoch:25 step:23704 [D loss: 0.632828, acc: 64.84%] [G loss: 1.853655]\n",
      "epoch:25 step:23705 [D loss: 0.604484, acc: 69.53%] [G loss: 1.841087]\n",
      "epoch:25 step:23706 [D loss: 0.657137, acc: 63.28%] [G loss: 1.868563]\n",
      "epoch:25 step:23707 [D loss: 0.659338, acc: 61.72%] [G loss: 1.873413]\n",
      "epoch:25 step:23708 [D loss: 0.594650, acc: 71.09%] [G loss: 1.791902]\n",
      "epoch:25 step:23709 [D loss: 0.656973, acc: 62.50%] [G loss: 1.825127]\n",
      "epoch:25 step:23710 [D loss: 0.628500, acc: 60.94%] [G loss: 1.825392]\n",
      "epoch:25 step:23711 [D loss: 0.631589, acc: 64.84%] [G loss: 1.880861]\n",
      "epoch:25 step:23712 [D loss: 0.633539, acc: 66.41%] [G loss: 1.932673]\n",
      "epoch:25 step:23713 [D loss: 0.630476, acc: 64.84%] [G loss: 1.868283]\n",
      "epoch:25 step:23714 [D loss: 0.608290, acc: 64.06%] [G loss: 2.105853]\n",
      "epoch:25 step:23715 [D loss: 0.609061, acc: 63.28%] [G loss: 1.999295]\n",
      "epoch:25 step:23716 [D loss: 0.637641, acc: 67.19%] [G loss: 1.831636]\n",
      "epoch:25 step:23717 [D loss: 0.613874, acc: 67.19%] [G loss: 1.896775]\n",
      "epoch:25 step:23718 [D loss: 0.642025, acc: 60.16%] [G loss: 1.959580]\n",
      "epoch:25 step:23719 [D loss: 0.713852, acc: 58.59%] [G loss: 1.814305]\n",
      "epoch:25 step:23720 [D loss: 0.604730, acc: 68.75%] [G loss: 1.848675]\n",
      "epoch:25 step:23721 [D loss: 0.605610, acc: 66.41%] [G loss: 1.941093]\n",
      "epoch:25 step:23722 [D loss: 0.653246, acc: 63.28%] [G loss: 1.814462]\n",
      "epoch:25 step:23723 [D loss: 0.628323, acc: 65.62%] [G loss: 1.989258]\n",
      "epoch:25 step:23724 [D loss: 0.590517, acc: 69.53%] [G loss: 1.970390]\n",
      "epoch:25 step:23725 [D loss: 0.651007, acc: 64.06%] [G loss: 1.990039]\n",
      "epoch:25 step:23726 [D loss: 0.671776, acc: 59.38%] [G loss: 1.815854]\n",
      "epoch:25 step:23727 [D loss: 0.616425, acc: 67.97%] [G loss: 1.892618]\n",
      "epoch:25 step:23728 [D loss: 0.645497, acc: 62.50%] [G loss: 1.931552]\n",
      "epoch:25 step:23729 [D loss: 0.690229, acc: 57.03%] [G loss: 1.810611]\n",
      "epoch:25 step:23730 [D loss: 0.650666, acc: 64.84%] [G loss: 1.833211]\n",
      "epoch:25 step:23731 [D loss: 0.691297, acc: 64.06%] [G loss: 1.800164]\n",
      "epoch:25 step:23732 [D loss: 0.622877, acc: 67.97%] [G loss: 1.821229]\n",
      "epoch:25 step:23733 [D loss: 0.653693, acc: 57.81%] [G loss: 1.886753]\n",
      "epoch:25 step:23734 [D loss: 0.634648, acc: 63.28%] [G loss: 1.926464]\n",
      "epoch:25 step:23735 [D loss: 0.606304, acc: 71.09%] [G loss: 1.857701]\n",
      "epoch:25 step:23736 [D loss: 0.633991, acc: 66.41%] [G loss: 1.983950]\n",
      "epoch:25 step:23737 [D loss: 0.582623, acc: 67.97%] [G loss: 2.393986]\n",
      "epoch:25 step:23738 [D loss: 0.633562, acc: 69.53%] [G loss: 2.111403]\n",
      "epoch:25 step:23739 [D loss: 0.567004, acc: 71.88%] [G loss: 2.165494]\n",
      "epoch:25 step:23740 [D loss: 0.670146, acc: 62.50%] [G loss: 2.097859]\n",
      "epoch:25 step:23741 [D loss: 0.721896, acc: 55.47%] [G loss: 1.818504]\n",
      "epoch:25 step:23742 [D loss: 0.686601, acc: 59.38%] [G loss: 1.954540]\n",
      "epoch:25 step:23743 [D loss: 0.671505, acc: 56.25%] [G loss: 1.882826]\n",
      "epoch:25 step:23744 [D loss: 0.667361, acc: 56.25%] [G loss: 1.942529]\n",
      "epoch:25 step:23745 [D loss: 0.609956, acc: 70.31%] [G loss: 1.831817]\n",
      "epoch:25 step:23746 [D loss: 0.613716, acc: 65.62%] [G loss: 1.913054]\n",
      "epoch:25 step:23747 [D loss: 0.625507, acc: 65.62%] [G loss: 1.880332]\n",
      "epoch:25 step:23748 [D loss: 0.695786, acc: 57.03%] [G loss: 1.846260]\n",
      "epoch:25 step:23749 [D loss: 0.639718, acc: 67.19%] [G loss: 1.819627]\n",
      "epoch:25 step:23750 [D loss: 0.632898, acc: 64.84%] [G loss: 1.921908]\n",
      "epoch:25 step:23751 [D loss: 0.713792, acc: 55.47%] [G loss: 1.773129]\n",
      "epoch:25 step:23752 [D loss: 0.624696, acc: 67.97%] [G loss: 1.847126]\n",
      "epoch:25 step:23753 [D loss: 0.664617, acc: 60.16%] [G loss: 1.838955]\n",
      "epoch:25 step:23754 [D loss: 0.656984, acc: 59.38%] [G loss: 1.707726]\n",
      "epoch:25 step:23755 [D loss: 0.674101, acc: 64.06%] [G loss: 2.087084]\n",
      "epoch:25 step:23756 [D loss: 0.651238, acc: 58.59%] [G loss: 1.948753]\n",
      "epoch:25 step:23757 [D loss: 0.621701, acc: 64.84%] [G loss: 1.902076]\n",
      "epoch:25 step:23758 [D loss: 0.626170, acc: 61.72%] [G loss: 1.891909]\n",
      "epoch:25 step:23759 [D loss: 0.707976, acc: 57.81%] [G loss: 2.009936]\n",
      "epoch:25 step:23760 [D loss: 0.596920, acc: 71.09%] [G loss: 2.013533]\n",
      "epoch:25 step:23761 [D loss: 0.700211, acc: 51.56%] [G loss: 1.918922]\n",
      "epoch:25 step:23762 [D loss: 0.655291, acc: 60.16%] [G loss: 1.948709]\n",
      "epoch:25 step:23763 [D loss: 0.648693, acc: 61.72%] [G loss: 1.923033]\n",
      "epoch:25 step:23764 [D loss: 0.634149, acc: 63.28%] [G loss: 1.892090]\n",
      "epoch:25 step:23765 [D loss: 0.663059, acc: 60.16%] [G loss: 1.913321]\n",
      "epoch:25 step:23766 [D loss: 0.713584, acc: 54.69%] [G loss: 1.691249]\n",
      "epoch:25 step:23767 [D loss: 0.686326, acc: 54.69%] [G loss: 1.719389]\n",
      "epoch:25 step:23768 [D loss: 0.669201, acc: 55.47%] [G loss: 1.922434]\n",
      "epoch:25 step:23769 [D loss: 0.718927, acc: 51.56%] [G loss: 1.784206]\n",
      "epoch:25 step:23770 [D loss: 0.585425, acc: 74.22%] [G loss: 1.845685]\n",
      "epoch:25 step:23771 [D loss: 0.630287, acc: 67.19%] [G loss: 2.034798]\n",
      "epoch:25 step:23772 [D loss: 0.564697, acc: 76.56%] [G loss: 2.104722]\n",
      "epoch:25 step:23773 [D loss: 0.689929, acc: 57.81%] [G loss: 1.765330]\n",
      "epoch:25 step:23774 [D loss: 0.662282, acc: 59.38%] [G loss: 1.689669]\n",
      "epoch:25 step:23775 [D loss: 0.635934, acc: 66.41%] [G loss: 1.915895]\n",
      "epoch:25 step:23776 [D loss: 0.700732, acc: 56.25%] [G loss: 1.753601]\n",
      "epoch:25 step:23777 [D loss: 0.664537, acc: 59.38%] [G loss: 1.764569]\n",
      "epoch:25 step:23778 [D loss: 0.660547, acc: 58.59%] [G loss: 1.859070]\n",
      "epoch:25 step:23779 [D loss: 0.621413, acc: 65.62%] [G loss: 2.044006]\n",
      "epoch:25 step:23780 [D loss: 0.694155, acc: 60.16%] [G loss: 1.793085]\n",
      "epoch:25 step:23781 [D loss: 0.641675, acc: 67.19%] [G loss: 1.710281]\n",
      "epoch:25 step:23782 [D loss: 0.653642, acc: 56.25%] [G loss: 1.846628]\n",
      "epoch:25 step:23783 [D loss: 0.665590, acc: 58.59%] [G loss: 1.788164]\n",
      "epoch:25 step:23784 [D loss: 0.603796, acc: 69.53%] [G loss: 1.879666]\n",
      "epoch:25 step:23785 [D loss: 0.636201, acc: 61.72%] [G loss: 1.946549]\n",
      "epoch:25 step:23786 [D loss: 0.608768, acc: 71.88%] [G loss: 1.917430]\n",
      "epoch:25 step:23787 [D loss: 0.679536, acc: 57.81%] [G loss: 1.849137]\n",
      "epoch:25 step:23788 [D loss: 0.626881, acc: 67.97%] [G loss: 1.930791]\n",
      "epoch:25 step:23789 [D loss: 0.712416, acc: 44.53%] [G loss: 1.852927]\n",
      "epoch:25 step:23790 [D loss: 0.600871, acc: 71.09%] [G loss: 1.750983]\n",
      "epoch:25 step:23791 [D loss: 0.639442, acc: 63.28%] [G loss: 1.762097]\n",
      "epoch:25 step:23792 [D loss: 0.675253, acc: 56.25%] [G loss: 1.950296]\n",
      "epoch:25 step:23793 [D loss: 0.660877, acc: 57.81%] [G loss: 1.843251]\n",
      "epoch:25 step:23794 [D loss: 0.610738, acc: 67.19%] [G loss: 1.817196]\n",
      "epoch:25 step:23795 [D loss: 0.634258, acc: 64.06%] [G loss: 1.979443]\n",
      "epoch:25 step:23796 [D loss: 0.599418, acc: 65.62%] [G loss: 2.005600]\n",
      "epoch:25 step:23797 [D loss: 0.608184, acc: 71.88%] [G loss: 1.879900]\n",
      "epoch:25 step:23798 [D loss: 0.688034, acc: 57.81%] [G loss: 1.814562]\n",
      "epoch:25 step:23799 [D loss: 0.646283, acc: 58.59%] [G loss: 1.844227]\n",
      "epoch:25 step:23800 [D loss: 0.679426, acc: 57.81%] [G loss: 1.864144]\n",
      "##############\n",
      "[2.45455145 1.18489438 6.0424113  4.60276851 3.45536198 5.38869196\n",
      " 4.62165711 4.36158205 4.45112467 3.48134687]\n",
      "##########\n",
      "epoch:25 step:23801 [D loss: 0.641031, acc: 60.16%] [G loss: 1.803402]\n",
      "epoch:25 step:23802 [D loss: 0.714970, acc: 52.34%] [G loss: 1.662914]\n",
      "epoch:25 step:23803 [D loss: 0.622737, acc: 68.75%] [G loss: 1.745651]\n",
      "epoch:25 step:23804 [D loss: 0.592691, acc: 70.31%] [G loss: 1.875142]\n",
      "epoch:25 step:23805 [D loss: 0.593620, acc: 68.75%] [G loss: 2.024224]\n",
      "epoch:25 step:23806 [D loss: 0.637275, acc: 60.94%] [G loss: 2.068673]\n",
      "epoch:25 step:23807 [D loss: 0.667819, acc: 58.59%] [G loss: 1.892181]\n",
      "epoch:25 step:23808 [D loss: 0.575532, acc: 71.09%] [G loss: 1.934002]\n",
      "epoch:25 step:23809 [D loss: 0.623005, acc: 65.62%] [G loss: 2.049956]\n",
      "epoch:25 step:23810 [D loss: 0.622428, acc: 64.06%] [G loss: 2.071980]\n",
      "epoch:25 step:23811 [D loss: 0.666063, acc: 60.16%] [G loss: 1.846684]\n",
      "epoch:25 step:23812 [D loss: 0.683580, acc: 57.03%] [G loss: 1.791410]\n",
      "epoch:25 step:23813 [D loss: 0.675139, acc: 60.16%] [G loss: 1.885361]\n",
      "epoch:25 step:23814 [D loss: 0.647861, acc: 58.59%] [G loss: 1.856919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23815 [D loss: 0.605698, acc: 64.06%] [G loss: 2.014018]\n",
      "epoch:25 step:23816 [D loss: 0.658017, acc: 59.38%] [G loss: 1.912427]\n",
      "epoch:25 step:23817 [D loss: 0.594250, acc: 68.75%] [G loss: 1.862885]\n",
      "epoch:25 step:23818 [D loss: 0.684213, acc: 56.25%] [G loss: 1.920191]\n",
      "epoch:25 step:23819 [D loss: 0.670790, acc: 57.03%] [G loss: 1.828079]\n",
      "epoch:25 step:23820 [D loss: 0.611441, acc: 64.06%] [G loss: 2.005551]\n",
      "epoch:25 step:23821 [D loss: 0.685115, acc: 54.69%] [G loss: 1.768360]\n",
      "epoch:25 step:23822 [D loss: 0.711223, acc: 54.69%] [G loss: 1.758485]\n",
      "epoch:25 step:23823 [D loss: 0.628567, acc: 64.06%] [G loss: 1.951696]\n",
      "epoch:25 step:23824 [D loss: 0.668569, acc: 58.59%] [G loss: 1.842394]\n",
      "epoch:25 step:23825 [D loss: 0.679873, acc: 64.84%] [G loss: 1.936578]\n",
      "epoch:25 step:23826 [D loss: 0.615990, acc: 66.41%] [G loss: 1.922774]\n",
      "epoch:25 step:23827 [D loss: 0.631997, acc: 64.84%] [G loss: 1.895327]\n",
      "epoch:25 step:23828 [D loss: 0.685289, acc: 57.81%] [G loss: 1.918993]\n",
      "epoch:25 step:23829 [D loss: 0.636365, acc: 64.06%] [G loss: 1.962207]\n",
      "epoch:25 step:23830 [D loss: 0.631234, acc: 63.28%] [G loss: 2.017560]\n",
      "epoch:25 step:23831 [D loss: 0.611527, acc: 71.09%] [G loss: 2.070047]\n",
      "epoch:25 step:23832 [D loss: 0.629124, acc: 62.50%] [G loss: 1.857748]\n",
      "epoch:25 step:23833 [D loss: 0.664216, acc: 57.03%] [G loss: 1.857028]\n",
      "epoch:25 step:23834 [D loss: 0.659848, acc: 60.16%] [G loss: 1.932368]\n",
      "epoch:25 step:23835 [D loss: 0.605751, acc: 66.41%] [G loss: 1.942112]\n",
      "epoch:25 step:23836 [D loss: 0.642542, acc: 64.06%] [G loss: 1.848685]\n",
      "epoch:25 step:23837 [D loss: 0.611908, acc: 66.41%] [G loss: 1.847290]\n",
      "epoch:25 step:23838 [D loss: 0.639129, acc: 64.06%] [G loss: 1.993186]\n",
      "epoch:25 step:23839 [D loss: 0.697010, acc: 57.81%] [G loss: 1.992003]\n",
      "epoch:25 step:23840 [D loss: 0.612418, acc: 67.19%] [G loss: 1.949769]\n",
      "epoch:25 step:23841 [D loss: 0.622421, acc: 67.19%] [G loss: 2.141778]\n",
      "epoch:25 step:23842 [D loss: 0.598910, acc: 68.75%] [G loss: 2.088923]\n",
      "epoch:25 step:23843 [D loss: 0.619695, acc: 66.41%] [G loss: 2.009652]\n",
      "epoch:25 step:23844 [D loss: 0.691774, acc: 60.16%] [G loss: 1.978296]\n",
      "epoch:25 step:23845 [D loss: 0.683758, acc: 57.03%] [G loss: 1.871109]\n",
      "epoch:25 step:23846 [D loss: 0.646139, acc: 65.62%] [G loss: 1.837150]\n",
      "epoch:25 step:23847 [D loss: 0.650661, acc: 61.72%] [G loss: 1.988527]\n",
      "epoch:25 step:23848 [D loss: 0.656816, acc: 60.94%] [G loss: 1.883353]\n",
      "epoch:25 step:23849 [D loss: 0.690569, acc: 56.25%] [G loss: 1.775543]\n",
      "epoch:25 step:23850 [D loss: 0.660510, acc: 58.59%] [G loss: 1.953487]\n",
      "epoch:25 step:23851 [D loss: 0.595899, acc: 67.97%] [G loss: 1.860341]\n",
      "epoch:25 step:23852 [D loss: 0.637446, acc: 65.62%] [G loss: 1.913312]\n",
      "epoch:25 step:23853 [D loss: 0.591146, acc: 67.97%] [G loss: 2.259321]\n",
      "epoch:25 step:23854 [D loss: 0.626215, acc: 65.62%] [G loss: 2.145817]\n",
      "epoch:25 step:23855 [D loss: 0.547755, acc: 75.78%] [G loss: 2.172091]\n",
      "epoch:25 step:23856 [D loss: 0.623508, acc: 63.28%] [G loss: 2.078310]\n",
      "epoch:25 step:23857 [D loss: 0.726048, acc: 51.56%] [G loss: 1.881692]\n",
      "epoch:25 step:23858 [D loss: 0.717416, acc: 53.91%] [G loss: 1.975055]\n",
      "epoch:25 step:23859 [D loss: 0.607088, acc: 69.53%] [G loss: 1.807261]\n",
      "epoch:25 step:23860 [D loss: 0.652076, acc: 64.06%] [G loss: 1.896506]\n",
      "epoch:25 step:23861 [D loss: 0.562160, acc: 73.44%] [G loss: 1.991508]\n",
      "epoch:25 step:23862 [D loss: 0.776840, acc: 50.00%] [G loss: 1.794697]\n",
      "epoch:25 step:23863 [D loss: 0.640612, acc: 60.16%] [G loss: 1.778535]\n",
      "epoch:25 step:23864 [D loss: 0.672211, acc: 61.72%] [G loss: 1.802799]\n",
      "epoch:25 step:23865 [D loss: 0.608143, acc: 63.28%] [G loss: 1.838600]\n",
      "epoch:25 step:23866 [D loss: 0.747789, acc: 51.56%] [G loss: 1.757849]\n",
      "epoch:25 step:23867 [D loss: 0.681507, acc: 60.16%] [G loss: 1.793793]\n",
      "epoch:25 step:23868 [D loss: 0.671861, acc: 55.47%] [G loss: 1.784598]\n",
      "epoch:25 step:23869 [D loss: 0.725267, acc: 56.25%] [G loss: 1.831737]\n",
      "epoch:25 step:23870 [D loss: 0.649064, acc: 61.72%] [G loss: 1.771981]\n",
      "epoch:25 step:23871 [D loss: 0.683795, acc: 61.72%] [G loss: 1.783677]\n",
      "epoch:25 step:23872 [D loss: 0.654504, acc: 62.50%] [G loss: 1.815652]\n",
      "epoch:25 step:23873 [D loss: 0.656708, acc: 57.81%] [G loss: 1.838976]\n",
      "epoch:25 step:23874 [D loss: 0.650908, acc: 60.94%] [G loss: 1.764963]\n",
      "epoch:25 step:23875 [D loss: 0.622997, acc: 67.19%] [G loss: 1.968259]\n",
      "epoch:25 step:23876 [D loss: 0.615499, acc: 63.28%] [G loss: 1.918790]\n",
      "epoch:25 step:23877 [D loss: 0.621636, acc: 64.84%] [G loss: 1.779465]\n",
      "epoch:25 step:23878 [D loss: 0.671376, acc: 60.94%] [G loss: 1.973362]\n",
      "epoch:25 step:23879 [D loss: 0.679568, acc: 55.47%] [G loss: 1.874417]\n",
      "epoch:25 step:23880 [D loss: 0.643360, acc: 66.41%] [G loss: 1.728779]\n",
      "epoch:25 step:23881 [D loss: 0.664212, acc: 59.38%] [G loss: 1.840413]\n",
      "epoch:25 step:23882 [D loss: 0.619202, acc: 65.62%] [G loss: 1.985791]\n",
      "epoch:25 step:23883 [D loss: 0.659838, acc: 64.06%] [G loss: 1.879147]\n",
      "epoch:25 step:23884 [D loss: 0.701528, acc: 56.25%] [G loss: 1.825155]\n",
      "epoch:25 step:23885 [D loss: 0.632384, acc: 67.19%] [G loss: 1.820763]\n",
      "epoch:25 step:23886 [D loss: 0.610617, acc: 64.84%] [G loss: 1.890051]\n",
      "epoch:25 step:23887 [D loss: 0.625368, acc: 65.62%] [G loss: 1.932968]\n",
      "epoch:25 step:23888 [D loss: 0.659757, acc: 60.16%] [G loss: 1.890756]\n",
      "epoch:25 step:23889 [D loss: 0.633111, acc: 64.84%] [G loss: 1.828290]\n",
      "epoch:25 step:23890 [D loss: 0.664568, acc: 60.16%] [G loss: 1.871529]\n",
      "epoch:25 step:23891 [D loss: 0.621866, acc: 62.50%] [G loss: 1.864068]\n",
      "epoch:25 step:23892 [D loss: 0.655248, acc: 60.16%] [G loss: 1.956470]\n",
      "epoch:25 step:23893 [D loss: 0.616543, acc: 64.84%] [G loss: 2.061532]\n",
      "epoch:25 step:23894 [D loss: 0.615914, acc: 64.06%] [G loss: 2.055791]\n",
      "epoch:25 step:23895 [D loss: 0.684353, acc: 58.59%] [G loss: 1.914270]\n",
      "epoch:25 step:23896 [D loss: 0.584680, acc: 70.31%] [G loss: 2.245879]\n",
      "epoch:25 step:23897 [D loss: 0.616296, acc: 70.31%] [G loss: 1.957946]\n",
      "epoch:25 step:23898 [D loss: 0.647885, acc: 66.41%] [G loss: 1.909718]\n",
      "epoch:25 step:23899 [D loss: 0.643542, acc: 60.16%] [G loss: 2.021992]\n",
      "epoch:25 step:23900 [D loss: 0.592475, acc: 64.06%] [G loss: 2.086824]\n",
      "epoch:25 step:23901 [D loss: 0.654742, acc: 60.16%] [G loss: 2.011742]\n",
      "epoch:25 step:23902 [D loss: 0.650926, acc: 64.06%] [G loss: 1.779720]\n",
      "epoch:25 step:23903 [D loss: 0.685996, acc: 60.94%] [G loss: 1.845952]\n",
      "epoch:25 step:23904 [D loss: 0.643273, acc: 59.38%] [G loss: 2.052462]\n",
      "epoch:25 step:23905 [D loss: 0.671709, acc: 60.94%] [G loss: 1.975431]\n",
      "epoch:25 step:23906 [D loss: 0.592214, acc: 67.19%] [G loss: 2.130889]\n",
      "epoch:25 step:23907 [D loss: 0.711023, acc: 50.00%] [G loss: 1.841271]\n",
      "epoch:25 step:23908 [D loss: 0.677138, acc: 57.03%] [G loss: 1.853629]\n",
      "epoch:25 step:23909 [D loss: 0.647134, acc: 61.72%] [G loss: 1.867063]\n",
      "epoch:25 step:23910 [D loss: 0.684960, acc: 57.81%] [G loss: 1.799062]\n",
      "epoch:25 step:23911 [D loss: 0.698922, acc: 53.12%] [G loss: 1.805376]\n",
      "epoch:25 step:23912 [D loss: 0.674939, acc: 55.47%] [G loss: 1.740005]\n",
      "epoch:25 step:23913 [D loss: 0.632606, acc: 62.50%] [G loss: 1.996189]\n",
      "epoch:25 step:23914 [D loss: 0.673909, acc: 60.94%] [G loss: 1.807226]\n",
      "epoch:25 step:23915 [D loss: 0.653683, acc: 62.50%] [G loss: 1.852036]\n",
      "epoch:25 step:23916 [D loss: 0.597720, acc: 67.97%] [G loss: 1.914794]\n",
      "epoch:25 step:23917 [D loss: 0.681759, acc: 61.72%] [G loss: 1.705214]\n",
      "epoch:25 step:23918 [D loss: 0.619470, acc: 64.06%] [G loss: 1.896631]\n",
      "epoch:25 step:23919 [D loss: 0.612205, acc: 64.84%] [G loss: 1.926932]\n",
      "epoch:25 step:23920 [D loss: 0.602036, acc: 65.62%] [G loss: 2.152297]\n",
      "epoch:25 step:23921 [D loss: 0.627480, acc: 64.84%] [G loss: 1.862592]\n",
      "epoch:25 step:23922 [D loss: 0.613709, acc: 67.97%] [G loss: 2.011437]\n",
      "epoch:25 step:23923 [D loss: 0.597762, acc: 70.31%] [G loss: 1.953077]\n",
      "epoch:25 step:23924 [D loss: 0.626876, acc: 62.50%] [G loss: 2.062599]\n",
      "epoch:25 step:23925 [D loss: 0.762389, acc: 46.09%] [G loss: 1.909359]\n",
      "epoch:25 step:23926 [D loss: 0.676433, acc: 59.38%] [G loss: 1.700250]\n",
      "epoch:25 step:23927 [D loss: 0.708656, acc: 54.69%] [G loss: 1.828589]\n",
      "epoch:25 step:23928 [D loss: 0.684283, acc: 54.69%] [G loss: 1.980534]\n",
      "epoch:25 step:23929 [D loss: 0.637937, acc: 63.28%] [G loss: 1.835880]\n",
      "epoch:25 step:23930 [D loss: 0.670204, acc: 57.81%] [G loss: 1.768184]\n",
      "epoch:25 step:23931 [D loss: 0.692408, acc: 58.59%] [G loss: 1.818802]\n",
      "epoch:25 step:23932 [D loss: 0.670648, acc: 60.16%] [G loss: 1.763926]\n",
      "epoch:25 step:23933 [D loss: 0.620436, acc: 67.97%] [G loss: 1.996051]\n",
      "epoch:25 step:23934 [D loss: 0.637869, acc: 61.72%] [G loss: 1.891000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23935 [D loss: 0.673422, acc: 58.59%] [G loss: 1.773170]\n",
      "epoch:25 step:23936 [D loss: 0.664934, acc: 57.81%] [G loss: 1.713725]\n",
      "epoch:25 step:23937 [D loss: 0.616024, acc: 67.97%] [G loss: 2.016558]\n",
      "epoch:25 step:23938 [D loss: 0.674412, acc: 57.03%] [G loss: 1.871978]\n",
      "epoch:25 step:23939 [D loss: 0.701309, acc: 55.47%] [G loss: 1.876844]\n",
      "epoch:25 step:23940 [D loss: 0.618107, acc: 61.72%] [G loss: 1.993982]\n",
      "epoch:25 step:23941 [D loss: 0.574799, acc: 74.22%] [G loss: 2.090344]\n",
      "epoch:25 step:23942 [D loss: 0.638123, acc: 64.84%] [G loss: 1.889286]\n",
      "epoch:25 step:23943 [D loss: 0.650196, acc: 64.06%] [G loss: 1.825741]\n",
      "epoch:25 step:23944 [D loss: 0.659137, acc: 61.72%] [G loss: 1.953880]\n",
      "epoch:25 step:23945 [D loss: 0.607827, acc: 67.19%] [G loss: 1.874296]\n",
      "epoch:25 step:23946 [D loss: 0.646164, acc: 63.28%] [G loss: 1.907478]\n",
      "epoch:25 step:23947 [D loss: 0.648729, acc: 60.94%] [G loss: 1.948190]\n",
      "epoch:25 step:23948 [D loss: 0.617485, acc: 62.50%] [G loss: 1.943632]\n",
      "epoch:25 step:23949 [D loss: 0.719418, acc: 57.03%] [G loss: 1.761876]\n",
      "epoch:25 step:23950 [D loss: 0.735533, acc: 57.81%] [G loss: 1.758014]\n",
      "epoch:25 step:23951 [D loss: 0.626345, acc: 66.41%] [G loss: 1.955330]\n",
      "epoch:25 step:23952 [D loss: 0.658774, acc: 66.41%] [G loss: 1.875056]\n",
      "epoch:25 step:23953 [D loss: 0.707126, acc: 55.47%] [G loss: 1.745106]\n",
      "epoch:25 step:23954 [D loss: 0.695562, acc: 57.03%] [G loss: 1.708782]\n",
      "epoch:25 step:23955 [D loss: 0.670483, acc: 56.25%] [G loss: 1.753910]\n",
      "epoch:25 step:23956 [D loss: 0.695771, acc: 50.00%] [G loss: 1.732239]\n",
      "epoch:25 step:23957 [D loss: 0.613033, acc: 67.19%] [G loss: 1.822376]\n",
      "epoch:25 step:23958 [D loss: 0.652659, acc: 58.59%] [G loss: 1.909611]\n",
      "epoch:25 step:23959 [D loss: 0.638023, acc: 64.84%] [G loss: 1.844988]\n",
      "epoch:25 step:23960 [D loss: 0.637236, acc: 64.06%] [G loss: 1.742551]\n",
      "epoch:25 step:23961 [D loss: 0.638129, acc: 64.06%] [G loss: 2.004891]\n",
      "epoch:25 step:23962 [D loss: 0.676703, acc: 58.59%] [G loss: 1.838650]\n",
      "epoch:25 step:23963 [D loss: 0.669059, acc: 60.94%] [G loss: 1.758345]\n",
      "epoch:25 step:23964 [D loss: 0.650660, acc: 64.06%] [G loss: 1.700861]\n",
      "epoch:25 step:23965 [D loss: 0.666745, acc: 60.16%] [G loss: 1.793704]\n",
      "epoch:25 step:23966 [D loss: 0.606142, acc: 71.88%] [G loss: 1.863579]\n",
      "epoch:25 step:23967 [D loss: 0.705113, acc: 50.00%] [G loss: 1.767571]\n",
      "epoch:25 step:23968 [D loss: 0.639661, acc: 62.50%] [G loss: 1.726243]\n",
      "epoch:25 step:23969 [D loss: 0.659235, acc: 60.16%] [G loss: 1.816689]\n",
      "epoch:25 step:23970 [D loss: 0.621015, acc: 65.62%] [G loss: 1.904439]\n",
      "epoch:25 step:23971 [D loss: 0.692325, acc: 54.69%] [G loss: 1.842043]\n",
      "epoch:25 step:23972 [D loss: 0.653238, acc: 63.28%] [G loss: 1.914442]\n",
      "epoch:25 step:23973 [D loss: 0.624731, acc: 60.16%] [G loss: 1.882737]\n",
      "epoch:25 step:23974 [D loss: 0.647647, acc: 58.59%] [G loss: 1.904043]\n",
      "epoch:25 step:23975 [D loss: 0.638628, acc: 63.28%] [G loss: 1.978217]\n",
      "epoch:25 step:23976 [D loss: 0.627867, acc: 63.28%] [G loss: 1.891022]\n",
      "epoch:25 step:23977 [D loss: 0.643463, acc: 58.59%] [G loss: 1.892168]\n",
      "epoch:25 step:23978 [D loss: 0.649242, acc: 60.16%] [G loss: 1.895480]\n",
      "epoch:25 step:23979 [D loss: 0.649300, acc: 65.62%] [G loss: 1.897691]\n",
      "epoch:25 step:23980 [D loss: 0.711280, acc: 60.16%] [G loss: 1.823046]\n",
      "epoch:25 step:23981 [D loss: 0.631947, acc: 62.50%] [G loss: 1.906203]\n",
      "epoch:25 step:23982 [D loss: 0.607851, acc: 63.28%] [G loss: 1.945778]\n",
      "epoch:25 step:23983 [D loss: 0.640737, acc: 63.28%] [G loss: 1.971885]\n",
      "epoch:25 step:23984 [D loss: 0.675437, acc: 58.59%] [G loss: 1.799247]\n",
      "epoch:25 step:23985 [D loss: 0.658563, acc: 63.28%] [G loss: 1.906954]\n",
      "epoch:25 step:23986 [D loss: 0.672920, acc: 63.28%] [G loss: 1.858452]\n",
      "epoch:25 step:23987 [D loss: 0.631550, acc: 60.94%] [G loss: 1.860608]\n",
      "epoch:25 step:23988 [D loss: 0.641214, acc: 60.94%] [G loss: 1.942109]\n",
      "epoch:25 step:23989 [D loss: 0.551803, acc: 71.88%] [G loss: 2.031544]\n",
      "epoch:25 step:23990 [D loss: 0.611739, acc: 68.75%] [G loss: 1.842970]\n",
      "epoch:25 step:23991 [D loss: 0.724753, acc: 52.34%] [G loss: 1.761521]\n",
      "epoch:25 step:23992 [D loss: 0.641611, acc: 65.62%] [G loss: 1.907491]\n",
      "epoch:25 step:23993 [D loss: 0.687296, acc: 56.25%] [G loss: 1.930090]\n",
      "epoch:25 step:23994 [D loss: 0.684201, acc: 57.03%] [G loss: 1.820258]\n",
      "epoch:25 step:23995 [D loss: 0.629587, acc: 69.53%] [G loss: 1.913545]\n",
      "epoch:25 step:23996 [D loss: 0.642917, acc: 61.72%] [G loss: 1.909282]\n",
      "epoch:25 step:23997 [D loss: 0.709432, acc: 52.34%] [G loss: 1.701739]\n",
      "epoch:25 step:23998 [D loss: 0.657279, acc: 57.81%] [G loss: 1.813197]\n",
      "epoch:25 step:23999 [D loss: 0.617568, acc: 66.41%] [G loss: 1.877608]\n",
      "epoch:25 step:24000 [D loss: 0.661286, acc: 64.84%] [G loss: 1.884993]\n",
      "##############\n",
      "[2.44091429 1.67251227 6.14660088 4.78566576 3.48703319 5.59063772\n",
      " 4.47082829 4.43526541 4.53876913 3.60350334]\n",
      "##########\n",
      "epoch:25 step:24001 [D loss: 0.663456, acc: 56.25%] [G loss: 1.789215]\n",
      "epoch:25 step:24002 [D loss: 0.670836, acc: 61.72%] [G loss: 1.670427]\n",
      "epoch:25 step:24003 [D loss: 0.654385, acc: 59.38%] [G loss: 1.755634]\n",
      "epoch:25 step:24004 [D loss: 0.670725, acc: 59.38%] [G loss: 1.743859]\n",
      "epoch:25 step:24005 [D loss: 0.689847, acc: 57.81%] [G loss: 1.759214]\n",
      "epoch:25 step:24006 [D loss: 0.639404, acc: 60.94%] [G loss: 1.737588]\n",
      "epoch:25 step:24007 [D loss: 0.657374, acc: 62.50%] [G loss: 1.875103]\n",
      "epoch:25 step:24008 [D loss: 0.655611, acc: 60.94%] [G loss: 1.889279]\n",
      "epoch:25 step:24009 [D loss: 0.683767, acc: 56.25%] [G loss: 1.695259]\n",
      "epoch:25 step:24010 [D loss: 0.667985, acc: 57.03%] [G loss: 1.920047]\n",
      "epoch:25 step:24011 [D loss: 0.709814, acc: 52.34%] [G loss: 1.825836]\n",
      "epoch:25 step:24012 [D loss: 0.668786, acc: 59.38%] [G loss: 1.863671]\n",
      "epoch:25 step:24013 [D loss: 0.703359, acc: 61.72%] [G loss: 1.945291]\n",
      "epoch:25 step:24014 [D loss: 0.625362, acc: 64.84%] [G loss: 1.883629]\n",
      "epoch:25 step:24015 [D loss: 0.661460, acc: 57.03%] [G loss: 1.811853]\n",
      "epoch:25 step:24016 [D loss: 0.648126, acc: 62.50%] [G loss: 1.895778]\n",
      "epoch:25 step:24017 [D loss: 0.643230, acc: 61.72%] [G loss: 1.796345]\n",
      "epoch:25 step:24018 [D loss: 0.667161, acc: 64.06%] [G loss: 1.791057]\n",
      "epoch:25 step:24019 [D loss: 0.639226, acc: 61.72%] [G loss: 1.789778]\n",
      "epoch:25 step:24020 [D loss: 0.645114, acc: 64.06%] [G loss: 1.821318]\n",
      "epoch:25 step:24021 [D loss: 0.634156, acc: 65.62%] [G loss: 1.760266]\n",
      "epoch:25 step:24022 [D loss: 0.672618, acc: 60.94%] [G loss: 1.890996]\n",
      "epoch:25 step:24023 [D loss: 0.617425, acc: 64.84%] [G loss: 1.996943]\n",
      "epoch:25 step:24024 [D loss: 0.657878, acc: 60.16%] [G loss: 1.852290]\n",
      "epoch:25 step:24025 [D loss: 0.619044, acc: 67.97%] [G loss: 1.947242]\n",
      "epoch:25 step:24026 [D loss: 0.691451, acc: 56.25%] [G loss: 1.832524]\n",
      "epoch:25 step:24027 [D loss: 0.638769, acc: 60.94%] [G loss: 1.872832]\n",
      "epoch:25 step:24028 [D loss: 0.623997, acc: 66.41%] [G loss: 1.848352]\n",
      "epoch:25 step:24029 [D loss: 0.580613, acc: 72.66%] [G loss: 1.989840]\n",
      "epoch:25 step:24030 [D loss: 0.586150, acc: 74.22%] [G loss: 1.966131]\n",
      "epoch:25 step:24031 [D loss: 0.625457, acc: 66.41%] [G loss: 1.906775]\n",
      "epoch:25 step:24032 [D loss: 0.661253, acc: 57.81%] [G loss: 1.940809]\n",
      "epoch:25 step:24033 [D loss: 0.655139, acc: 58.59%] [G loss: 1.910106]\n",
      "epoch:25 step:24034 [D loss: 0.644651, acc: 60.16%] [G loss: 1.892341]\n",
      "epoch:25 step:24035 [D loss: 0.631719, acc: 64.84%] [G loss: 1.748972]\n",
      "epoch:25 step:24036 [D loss: 0.652256, acc: 55.47%] [G loss: 1.981756]\n",
      "epoch:25 step:24037 [D loss: 0.700212, acc: 56.25%] [G loss: 1.881658]\n",
      "epoch:25 step:24038 [D loss: 0.652621, acc: 61.72%] [G loss: 1.713455]\n",
      "epoch:25 step:24039 [D loss: 0.674331, acc: 56.25%] [G loss: 1.698081]\n",
      "epoch:25 step:24040 [D loss: 0.676975, acc: 57.03%] [G loss: 1.839053]\n",
      "epoch:25 step:24041 [D loss: 0.645691, acc: 61.72%] [G loss: 2.033961]\n",
      "epoch:25 step:24042 [D loss: 0.646831, acc: 57.03%] [G loss: 1.801089]\n",
      "epoch:25 step:24043 [D loss: 0.669181, acc: 59.38%] [G loss: 1.927557]\n",
      "epoch:25 step:24044 [D loss: 0.647640, acc: 62.50%] [G loss: 1.790300]\n",
      "epoch:25 step:24045 [D loss: 0.678714, acc: 54.69%] [G loss: 1.854173]\n",
      "epoch:25 step:24046 [D loss: 0.654781, acc: 57.81%] [G loss: 1.762382]\n",
      "epoch:25 step:24047 [D loss: 0.664237, acc: 61.72%] [G loss: 2.040256]\n",
      "epoch:25 step:24048 [D loss: 0.648365, acc: 61.72%] [G loss: 1.726285]\n",
      "epoch:25 step:24049 [D loss: 0.648190, acc: 60.94%] [G loss: 1.961670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24050 [D loss: 0.674852, acc: 57.03%] [G loss: 1.771151]\n",
      "epoch:25 step:24051 [D loss: 0.632529, acc: 64.84%] [G loss: 1.811665]\n",
      "epoch:25 step:24052 [D loss: 0.635744, acc: 63.28%] [G loss: 1.736091]\n",
      "epoch:25 step:24053 [D loss: 0.682137, acc: 57.81%] [G loss: 1.718697]\n",
      "epoch:25 step:24054 [D loss: 0.585890, acc: 68.75%] [G loss: 1.992214]\n",
      "epoch:25 step:24055 [D loss: 0.622154, acc: 63.28%] [G loss: 1.960824]\n",
      "epoch:25 step:24056 [D loss: 0.626115, acc: 71.88%] [G loss: 1.835763]\n",
      "epoch:25 step:24057 [D loss: 0.610461, acc: 65.62%] [G loss: 2.040313]\n",
      "epoch:25 step:24058 [D loss: 0.637101, acc: 58.59%] [G loss: 2.023829]\n",
      "epoch:25 step:24059 [D loss: 0.638022, acc: 64.84%] [G loss: 1.912365]\n",
      "epoch:25 step:24060 [D loss: 0.649515, acc: 62.50%] [G loss: 2.106578]\n",
      "epoch:25 step:24061 [D loss: 0.624019, acc: 64.06%] [G loss: 1.972059]\n",
      "epoch:25 step:24062 [D loss: 0.631006, acc: 68.75%] [G loss: 1.953408]\n",
      "epoch:25 step:24063 [D loss: 0.676660, acc: 58.59%] [G loss: 1.892254]\n",
      "epoch:25 step:24064 [D loss: 0.679244, acc: 57.81%] [G loss: 1.834068]\n",
      "epoch:25 step:24065 [D loss: 0.634389, acc: 64.84%] [G loss: 1.857502]\n",
      "epoch:25 step:24066 [D loss: 0.654576, acc: 59.38%] [G loss: 1.873305]\n",
      "epoch:25 step:24067 [D loss: 0.639900, acc: 66.41%] [G loss: 1.944174]\n",
      "epoch:25 step:24068 [D loss: 0.609418, acc: 67.19%] [G loss: 1.963465]\n",
      "epoch:25 step:24069 [D loss: 0.609253, acc: 66.41%] [G loss: 2.038995]\n",
      "epoch:25 step:24070 [D loss: 0.670206, acc: 56.25%] [G loss: 1.864352]\n",
      "epoch:25 step:24071 [D loss: 0.602074, acc: 68.75%] [G loss: 2.167110]\n",
      "epoch:25 step:24072 [D loss: 0.563853, acc: 76.56%] [G loss: 2.245393]\n",
      "epoch:25 step:24073 [D loss: 0.564817, acc: 70.31%] [G loss: 2.226077]\n",
      "epoch:25 step:24074 [D loss: 0.634740, acc: 67.97%] [G loss: 2.144546]\n",
      "epoch:25 step:24075 [D loss: 0.634717, acc: 64.06%] [G loss: 2.191021]\n",
      "epoch:25 step:24076 [D loss: 0.612949, acc: 65.62%] [G loss: 1.895655]\n",
      "epoch:25 step:24077 [D loss: 0.650468, acc: 58.59%] [G loss: 2.039166]\n",
      "epoch:25 step:24078 [D loss: 0.662865, acc: 58.59%] [G loss: 2.087846]\n",
      "epoch:25 step:24079 [D loss: 0.648371, acc: 60.94%] [G loss: 2.102703]\n",
      "epoch:25 step:24080 [D loss: 0.705063, acc: 58.59%] [G loss: 1.967452]\n",
      "epoch:25 step:24081 [D loss: 0.657338, acc: 60.16%] [G loss: 1.768231]\n",
      "epoch:25 step:24082 [D loss: 0.686389, acc: 55.47%] [G loss: 2.011694]\n",
      "epoch:25 step:24083 [D loss: 0.689234, acc: 60.16%] [G loss: 1.811644]\n",
      "epoch:25 step:24084 [D loss: 0.650329, acc: 60.94%] [G loss: 1.883604]\n",
      "epoch:25 step:24085 [D loss: 0.664905, acc: 61.72%] [G loss: 1.901151]\n",
      "epoch:25 step:24086 [D loss: 0.667916, acc: 63.28%] [G loss: 1.881200]\n",
      "epoch:25 step:24087 [D loss: 0.664776, acc: 60.94%] [G loss: 1.884363]\n",
      "epoch:25 step:24088 [D loss: 0.629766, acc: 63.28%] [G loss: 1.973090]\n",
      "epoch:25 step:24089 [D loss: 0.639716, acc: 64.84%] [G loss: 1.717630]\n",
      "epoch:25 step:24090 [D loss: 0.620228, acc: 65.62%] [G loss: 1.958912]\n",
      "epoch:25 step:24091 [D loss: 0.697802, acc: 56.25%] [G loss: 1.819586]\n",
      "epoch:25 step:24092 [D loss: 0.732303, acc: 51.56%] [G loss: 1.729928]\n",
      "epoch:25 step:24093 [D loss: 0.636354, acc: 66.41%] [G loss: 1.871256]\n",
      "epoch:25 step:24094 [D loss: 0.670154, acc: 57.81%] [G loss: 1.813697]\n",
      "epoch:25 step:24095 [D loss: 0.674991, acc: 54.69%] [G loss: 1.749397]\n",
      "epoch:25 step:24096 [D loss: 0.678942, acc: 56.25%] [G loss: 1.846195]\n",
      "epoch:25 step:24097 [D loss: 0.602341, acc: 65.62%] [G loss: 1.814679]\n",
      "epoch:25 step:24098 [D loss: 0.666506, acc: 64.84%] [G loss: 1.822789]\n",
      "epoch:25 step:24099 [D loss: 0.620395, acc: 64.84%] [G loss: 1.861086]\n",
      "epoch:25 step:24100 [D loss: 0.699891, acc: 50.78%] [G loss: 1.791553]\n",
      "epoch:25 step:24101 [D loss: 0.641557, acc: 60.94%] [G loss: 1.744076]\n",
      "epoch:25 step:24102 [D loss: 0.609825, acc: 68.75%] [G loss: 1.986418]\n",
      "epoch:25 step:24103 [D loss: 0.688289, acc: 57.03%] [G loss: 1.794042]\n",
      "epoch:25 step:24104 [D loss: 0.628600, acc: 67.19%] [G loss: 1.903366]\n",
      "epoch:25 step:24105 [D loss: 0.618234, acc: 62.50%] [G loss: 1.869045]\n",
      "epoch:25 step:24106 [D loss: 0.609901, acc: 60.94%] [G loss: 1.982169]\n",
      "epoch:25 step:24107 [D loss: 0.603259, acc: 67.97%] [G loss: 1.755028]\n",
      "epoch:25 step:24108 [D loss: 0.629432, acc: 63.28%] [G loss: 1.917294]\n",
      "epoch:25 step:24109 [D loss: 0.672126, acc: 60.16%] [G loss: 1.799174]\n",
      "epoch:25 step:24110 [D loss: 0.638809, acc: 64.84%] [G loss: 1.831965]\n",
      "epoch:25 step:24111 [D loss: 0.630460, acc: 65.62%] [G loss: 1.891221]\n",
      "epoch:25 step:24112 [D loss: 0.636616, acc: 66.41%] [G loss: 1.838428]\n",
      "epoch:25 step:24113 [D loss: 0.615922, acc: 67.97%] [G loss: 1.848898]\n",
      "epoch:25 step:24114 [D loss: 0.632542, acc: 62.50%] [G loss: 2.017530]\n",
      "epoch:25 step:24115 [D loss: 0.618849, acc: 60.94%] [G loss: 2.134504]\n",
      "epoch:25 step:24116 [D loss: 0.594742, acc: 68.75%] [G loss: 1.989125]\n",
      "epoch:25 step:24117 [D loss: 0.610053, acc: 64.84%] [G loss: 1.965977]\n",
      "epoch:25 step:24118 [D loss: 0.647035, acc: 66.41%] [G loss: 1.965103]\n",
      "epoch:25 step:24119 [D loss: 0.611026, acc: 68.75%] [G loss: 2.064392]\n",
      "epoch:25 step:24120 [D loss: 0.585257, acc: 68.75%] [G loss: 1.942119]\n",
      "epoch:25 step:24121 [D loss: 0.691839, acc: 57.81%] [G loss: 1.890102]\n",
      "epoch:25 step:24122 [D loss: 0.646885, acc: 57.81%] [G loss: 1.803048]\n",
      "epoch:25 step:24123 [D loss: 0.667920, acc: 57.81%] [G loss: 1.818400]\n",
      "epoch:25 step:24124 [D loss: 0.606359, acc: 68.75%] [G loss: 2.147464]\n",
      "epoch:25 step:24125 [D loss: 0.679912, acc: 57.03%] [G loss: 1.839518]\n",
      "epoch:25 step:24126 [D loss: 0.659752, acc: 64.84%] [G loss: 2.056949]\n",
      "epoch:25 step:24127 [D loss: 0.716457, acc: 50.78%] [G loss: 1.883169]\n",
      "epoch:25 step:24128 [D loss: 0.652394, acc: 59.38%] [G loss: 1.906534]\n",
      "epoch:25 step:24129 [D loss: 0.655116, acc: 62.50%] [G loss: 1.798012]\n",
      "epoch:25 step:24130 [D loss: 0.651031, acc: 64.06%] [G loss: 1.855021]\n",
      "epoch:25 step:24131 [D loss: 0.639017, acc: 61.72%] [G loss: 1.878565]\n",
      "epoch:25 step:24132 [D loss: 0.613686, acc: 65.62%] [G loss: 2.183517]\n",
      "epoch:25 step:24133 [D loss: 0.621885, acc: 66.41%] [G loss: 1.867202]\n",
      "epoch:25 step:24134 [D loss: 0.670192, acc: 61.72%] [G loss: 1.865906]\n",
      "epoch:25 step:24135 [D loss: 0.707491, acc: 57.03%] [G loss: 1.804699]\n",
      "epoch:25 step:24136 [D loss: 0.618683, acc: 65.62%] [G loss: 2.012781]\n",
      "epoch:25 step:24137 [D loss: 0.628182, acc: 60.94%] [G loss: 1.913184]\n",
      "epoch:25 step:24138 [D loss: 0.666739, acc: 59.38%] [G loss: 1.921187]\n",
      "epoch:25 step:24139 [D loss: 0.671980, acc: 61.72%] [G loss: 1.858016]\n",
      "epoch:25 step:24140 [D loss: 0.637746, acc: 64.06%] [G loss: 1.936277]\n",
      "epoch:25 step:24141 [D loss: 0.673087, acc: 61.72%] [G loss: 1.793572]\n",
      "epoch:25 step:24142 [D loss: 0.660785, acc: 57.81%] [G loss: 1.853051]\n",
      "epoch:25 step:24143 [D loss: 0.670223, acc: 63.28%] [G loss: 1.860090]\n",
      "epoch:25 step:24144 [D loss: 0.534898, acc: 78.12%] [G loss: 1.995168]\n",
      "epoch:25 step:24145 [D loss: 0.617914, acc: 62.50%] [G loss: 2.044733]\n",
      "epoch:25 step:24146 [D loss: 0.627361, acc: 59.38%] [G loss: 1.925410]\n",
      "epoch:25 step:24147 [D loss: 0.692824, acc: 55.47%] [G loss: 1.886222]\n",
      "epoch:25 step:24148 [D loss: 0.638392, acc: 61.72%] [G loss: 1.871192]\n",
      "epoch:25 step:24149 [D loss: 0.632927, acc: 64.06%] [G loss: 1.909467]\n",
      "epoch:25 step:24150 [D loss: 0.649261, acc: 64.06%] [G loss: 1.975864]\n",
      "epoch:25 step:24151 [D loss: 0.617947, acc: 67.97%] [G loss: 1.960489]\n",
      "epoch:25 step:24152 [D loss: 0.637964, acc: 60.94%] [G loss: 1.845007]\n",
      "epoch:25 step:24153 [D loss: 0.650318, acc: 64.06%] [G loss: 2.049859]\n",
      "epoch:25 step:24154 [D loss: 0.682227, acc: 57.81%] [G loss: 1.856574]\n",
      "epoch:25 step:24155 [D loss: 0.658032, acc: 58.59%] [G loss: 1.731440]\n",
      "epoch:25 step:24156 [D loss: 0.665355, acc: 64.06%] [G loss: 1.927986]\n",
      "epoch:25 step:24157 [D loss: 0.645273, acc: 62.50%] [G loss: 1.858016]\n",
      "epoch:25 step:24158 [D loss: 0.657622, acc: 57.81%] [G loss: 1.808761]\n",
      "epoch:25 step:24159 [D loss: 0.631655, acc: 66.41%] [G loss: 1.827565]\n",
      "epoch:25 step:24160 [D loss: 0.654666, acc: 60.94%] [G loss: 1.982774]\n",
      "epoch:25 step:24161 [D loss: 0.613731, acc: 67.19%] [G loss: 1.918488]\n",
      "epoch:25 step:24162 [D loss: 0.685148, acc: 57.81%] [G loss: 1.896038]\n",
      "epoch:25 step:24163 [D loss: 0.626497, acc: 66.41%] [G loss: 1.798912]\n",
      "epoch:25 step:24164 [D loss: 0.667201, acc: 57.81%] [G loss: 1.754923]\n",
      "epoch:25 step:24165 [D loss: 0.651353, acc: 60.94%] [G loss: 1.920171]\n",
      "epoch:25 step:24166 [D loss: 0.638268, acc: 64.84%] [G loss: 1.872914]\n",
      "epoch:25 step:24167 [D loss: 0.713473, acc: 56.25%] [G loss: 1.957346]\n",
      "epoch:25 step:24168 [D loss: 0.655088, acc: 61.72%] [G loss: 1.936762]\n",
      "epoch:25 step:24169 [D loss: 0.657296, acc: 60.16%] [G loss: 1.892063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24170 [D loss: 0.654231, acc: 62.50%] [G loss: 1.850053]\n",
      "epoch:25 step:24171 [D loss: 0.683675, acc: 64.06%] [G loss: 1.937056]\n",
      "epoch:25 step:24172 [D loss: 0.625989, acc: 61.72%] [G loss: 1.904896]\n",
      "epoch:25 step:24173 [D loss: 0.652919, acc: 63.28%] [G loss: 1.795187]\n",
      "epoch:25 step:24174 [D loss: 0.681727, acc: 56.25%] [G loss: 1.708196]\n",
      "epoch:25 step:24175 [D loss: 0.669485, acc: 54.69%] [G loss: 1.842174]\n",
      "epoch:25 step:24176 [D loss: 0.673927, acc: 57.03%] [G loss: 1.791390]\n",
      "epoch:25 step:24177 [D loss: 0.667047, acc: 58.59%] [G loss: 1.735908]\n",
      "epoch:25 step:24178 [D loss: 0.690518, acc: 57.03%] [G loss: 1.817044]\n",
      "epoch:25 step:24179 [D loss: 0.590757, acc: 69.53%] [G loss: 1.925606]\n",
      "epoch:25 step:24180 [D loss: 0.651879, acc: 60.16%] [G loss: 1.948212]\n",
      "epoch:25 step:24181 [D loss: 0.635514, acc: 63.28%] [G loss: 2.022951]\n",
      "epoch:25 step:24182 [D loss: 0.625208, acc: 64.06%] [G loss: 1.889555]\n",
      "epoch:25 step:24183 [D loss: 0.637932, acc: 60.94%] [G loss: 1.709059]\n",
      "epoch:25 step:24184 [D loss: 0.599321, acc: 67.97%] [G loss: 1.884569]\n",
      "epoch:25 step:24185 [D loss: 0.689169, acc: 55.47%] [G loss: 1.890763]\n",
      "epoch:25 step:24186 [D loss: 0.729388, acc: 53.91%] [G loss: 1.871755]\n",
      "epoch:25 step:24187 [D loss: 0.713780, acc: 54.69%] [G loss: 1.802983]\n",
      "epoch:25 step:24188 [D loss: 0.649190, acc: 58.59%] [G loss: 1.963900]\n",
      "epoch:25 step:24189 [D loss: 0.655083, acc: 60.94%] [G loss: 1.857098]\n",
      "epoch:25 step:24190 [D loss: 0.703101, acc: 60.16%] [G loss: 1.703296]\n",
      "epoch:25 step:24191 [D loss: 0.675208, acc: 57.81%] [G loss: 1.781188]\n",
      "epoch:25 step:24192 [D loss: 0.686219, acc: 62.50%] [G loss: 1.964234]\n",
      "epoch:25 step:24193 [D loss: 0.685347, acc: 58.59%] [G loss: 1.959557]\n",
      "epoch:25 step:24194 [D loss: 0.659667, acc: 60.16%] [G loss: 1.894976]\n",
      "epoch:25 step:24195 [D loss: 0.624320, acc: 61.72%] [G loss: 1.748747]\n",
      "epoch:25 step:24196 [D loss: 0.663846, acc: 62.50%] [G loss: 1.890060]\n",
      "epoch:25 step:24197 [D loss: 0.695192, acc: 54.69%] [G loss: 1.692151]\n",
      "epoch:25 step:24198 [D loss: 0.637390, acc: 64.06%] [G loss: 1.907416]\n",
      "epoch:25 step:24199 [D loss: 0.623490, acc: 62.50%] [G loss: 2.100984]\n",
      "epoch:25 step:24200 [D loss: 0.595709, acc: 65.62%] [G loss: 2.209169]\n",
      "##############\n",
      "[2.48929228 1.49807285 6.10130019 4.55078949 3.44688308 5.52156523\n",
      " 4.4507358  4.6108912  4.50222939 3.70613739]\n",
      "##########\n",
      "epoch:25 step:24201 [D loss: 0.648041, acc: 56.25%] [G loss: 1.909589]\n",
      "epoch:25 step:24202 [D loss: 0.626347, acc: 64.84%] [G loss: 2.033441]\n",
      "epoch:25 step:24203 [D loss: 0.653064, acc: 62.50%] [G loss: 1.886705]\n",
      "epoch:25 step:24204 [D loss: 0.648238, acc: 60.94%] [G loss: 1.845660]\n",
      "epoch:25 step:24205 [D loss: 0.667590, acc: 57.81%] [G loss: 1.958472]\n",
      "epoch:25 step:24206 [D loss: 0.606068, acc: 66.41%] [G loss: 2.117627]\n",
      "epoch:25 step:24207 [D loss: 0.575695, acc: 66.41%] [G loss: 2.169227]\n",
      "epoch:25 step:24208 [D loss: 0.628807, acc: 58.59%] [G loss: 1.828043]\n",
      "epoch:25 step:24209 [D loss: 0.703190, acc: 52.34%] [G loss: 1.875261]\n",
      "epoch:25 step:24210 [D loss: 0.619126, acc: 65.62%] [G loss: 1.873580]\n",
      "epoch:25 step:24211 [D loss: 0.617358, acc: 61.72%] [G loss: 2.013069]\n",
      "epoch:25 step:24212 [D loss: 0.738925, acc: 46.88%] [G loss: 1.773006]\n",
      "epoch:25 step:24213 [D loss: 0.674460, acc: 60.16%] [G loss: 1.829088]\n",
      "epoch:25 step:24214 [D loss: 0.640923, acc: 64.06%] [G loss: 1.893862]\n",
      "epoch:25 step:24215 [D loss: 0.562374, acc: 69.53%] [G loss: 2.072970]\n",
      "epoch:25 step:24216 [D loss: 0.631786, acc: 60.94%] [G loss: 1.899334]\n",
      "epoch:25 step:24217 [D loss: 0.608756, acc: 67.19%] [G loss: 2.211060]\n",
      "epoch:25 step:24218 [D loss: 0.636672, acc: 63.28%] [G loss: 2.049409]\n",
      "epoch:25 step:24219 [D loss: 0.704267, acc: 54.69%] [G loss: 1.776579]\n",
      "epoch:25 step:24220 [D loss: 0.670180, acc: 61.72%] [G loss: 1.732173]\n",
      "epoch:25 step:24221 [D loss: 0.651370, acc: 63.28%] [G loss: 1.930340]\n",
      "epoch:25 step:24222 [D loss: 0.687855, acc: 55.47%] [G loss: 1.767875]\n",
      "epoch:25 step:24223 [D loss: 0.644789, acc: 64.84%] [G loss: 1.926003]\n",
      "epoch:25 step:24224 [D loss: 0.623834, acc: 56.25%] [G loss: 1.794592]\n",
      "epoch:25 step:24225 [D loss: 0.693766, acc: 54.69%] [G loss: 1.751378]\n",
      "epoch:25 step:24226 [D loss: 0.680361, acc: 57.81%] [G loss: 1.866045]\n",
      "epoch:25 step:24227 [D loss: 0.627590, acc: 65.62%] [G loss: 1.768918]\n",
      "epoch:25 step:24228 [D loss: 0.659924, acc: 61.72%] [G loss: 1.901197]\n",
      "epoch:25 step:24229 [D loss: 0.704364, acc: 54.69%] [G loss: 1.913060]\n",
      "epoch:25 step:24230 [D loss: 0.604313, acc: 67.97%] [G loss: 1.845298]\n",
      "epoch:25 step:24231 [D loss: 0.657466, acc: 60.94%] [G loss: 1.896832]\n",
      "epoch:25 step:24232 [D loss: 0.608500, acc: 67.19%] [G loss: 1.791674]\n",
      "epoch:25 step:24233 [D loss: 0.626000, acc: 67.19%] [G loss: 1.990592]\n",
      "epoch:25 step:24234 [D loss: 0.606717, acc: 68.75%] [G loss: 1.912214]\n",
      "epoch:25 step:24235 [D loss: 0.646599, acc: 62.50%] [G loss: 1.872795]\n",
      "epoch:25 step:24236 [D loss: 0.638422, acc: 65.62%] [G loss: 1.995155]\n",
      "epoch:25 step:24237 [D loss: 0.663351, acc: 57.03%] [G loss: 1.841235]\n",
      "epoch:25 step:24238 [D loss: 0.656006, acc: 60.94%] [G loss: 1.804933]\n",
      "epoch:25 step:24239 [D loss: 0.648139, acc: 63.28%] [G loss: 1.869576]\n",
      "epoch:25 step:24240 [D loss: 0.576484, acc: 69.53%] [G loss: 2.265167]\n",
      "epoch:25 step:24241 [D loss: 0.568380, acc: 73.44%] [G loss: 1.964750]\n",
      "epoch:25 step:24242 [D loss: 0.663064, acc: 57.03%] [G loss: 1.764102]\n",
      "epoch:25 step:24243 [D loss: 0.690725, acc: 52.34%] [G loss: 1.875459]\n",
      "epoch:25 step:24244 [D loss: 0.589992, acc: 69.53%] [G loss: 1.909930]\n",
      "epoch:25 step:24245 [D loss: 0.719043, acc: 52.34%] [G loss: 1.754509]\n",
      "epoch:25 step:24246 [D loss: 0.648936, acc: 64.06%] [G loss: 1.850807]\n",
      "epoch:25 step:24247 [D loss: 0.650123, acc: 57.81%] [G loss: 1.872259]\n",
      "epoch:25 step:24248 [D loss: 0.608560, acc: 68.75%] [G loss: 1.965762]\n",
      "epoch:25 step:24249 [D loss: 0.593608, acc: 64.06%] [G loss: 1.849046]\n",
      "epoch:25 step:24250 [D loss: 0.578770, acc: 73.44%] [G loss: 2.009891]\n",
      "epoch:25 step:24251 [D loss: 0.628366, acc: 68.75%] [G loss: 1.883618]\n",
      "epoch:25 step:24252 [D loss: 0.644366, acc: 62.50%] [G loss: 1.774900]\n",
      "epoch:25 step:24253 [D loss: 0.736105, acc: 53.12%] [G loss: 1.757560]\n",
      "epoch:25 step:24254 [D loss: 0.688857, acc: 56.25%] [G loss: 1.707999]\n",
      "epoch:25 step:24255 [D loss: 0.672609, acc: 59.38%] [G loss: 1.717288]\n",
      "epoch:25 step:24256 [D loss: 0.607756, acc: 67.97%] [G loss: 1.916146]\n",
      "epoch:25 step:24257 [D loss: 0.686144, acc: 64.06%] [G loss: 1.933818]\n",
      "epoch:25 step:24258 [D loss: 0.600814, acc: 66.41%] [G loss: 1.990535]\n",
      "epoch:25 step:24259 [D loss: 0.704982, acc: 53.12%] [G loss: 1.953731]\n",
      "epoch:25 step:24260 [D loss: 0.686913, acc: 61.72%] [G loss: 1.843113]\n",
      "epoch:25 step:24261 [D loss: 0.616847, acc: 65.62%] [G loss: 1.882427]\n",
      "epoch:25 step:24262 [D loss: 0.594607, acc: 67.97%] [G loss: 1.768380]\n",
      "epoch:25 step:24263 [D loss: 0.623542, acc: 65.62%] [G loss: 1.846999]\n",
      "epoch:25 step:24264 [D loss: 0.644681, acc: 64.84%] [G loss: 1.987451]\n",
      "epoch:25 step:24265 [D loss: 0.674647, acc: 60.94%] [G loss: 2.003530]\n",
      "epoch:25 step:24266 [D loss: 0.717772, acc: 51.56%] [G loss: 1.917621]\n",
      "epoch:25 step:24267 [D loss: 0.614023, acc: 67.19%] [G loss: 2.038071]\n",
      "epoch:25 step:24268 [D loss: 0.692999, acc: 56.25%] [G loss: 1.778096]\n",
      "epoch:25 step:24269 [D loss: 0.692618, acc: 57.81%] [G loss: 1.864792]\n",
      "epoch:25 step:24270 [D loss: 0.644597, acc: 65.62%] [G loss: 1.944486]\n",
      "epoch:25 step:24271 [D loss: 0.696157, acc: 57.03%] [G loss: 1.863490]\n",
      "epoch:25 step:24272 [D loss: 0.678365, acc: 56.25%] [G loss: 1.892384]\n",
      "epoch:25 step:24273 [D loss: 0.698061, acc: 58.59%] [G loss: 1.858455]\n",
      "epoch:25 step:24274 [D loss: 0.622769, acc: 64.06%] [G loss: 1.980829]\n",
      "epoch:25 step:24275 [D loss: 0.725966, acc: 50.78%] [G loss: 1.841054]\n",
      "epoch:25 step:24276 [D loss: 0.663617, acc: 60.94%] [G loss: 1.738046]\n",
      "epoch:25 step:24277 [D loss: 0.617778, acc: 63.28%] [G loss: 1.911119]\n",
      "epoch:25 step:24278 [D loss: 0.654920, acc: 61.72%] [G loss: 1.805290]\n",
      "epoch:25 step:24279 [D loss: 0.655432, acc: 59.38%] [G loss: 1.818917]\n",
      "epoch:25 step:24280 [D loss: 0.675641, acc: 57.81%] [G loss: 1.797899]\n",
      "epoch:25 step:24281 [D loss: 0.692201, acc: 56.25%] [G loss: 1.636656]\n",
      "epoch:25 step:24282 [D loss: 0.642703, acc: 63.28%] [G loss: 1.837329]\n",
      "epoch:25 step:24283 [D loss: 0.715756, acc: 56.25%] [G loss: 1.764193]\n",
      "epoch:25 step:24284 [D loss: 0.753528, acc: 47.66%] [G loss: 1.719182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24285 [D loss: 0.641199, acc: 67.97%] [G loss: 1.776421]\n",
      "epoch:25 step:24286 [D loss: 0.676970, acc: 60.94%] [G loss: 1.630808]\n",
      "epoch:25 step:24287 [D loss: 0.640844, acc: 63.28%] [G loss: 1.733007]\n",
      "epoch:25 step:24288 [D loss: 0.663858, acc: 57.81%] [G loss: 1.687013]\n",
      "epoch:25 step:24289 [D loss: 0.651808, acc: 57.81%] [G loss: 1.761483]\n",
      "epoch:25 step:24290 [D loss: 0.661949, acc: 60.94%] [G loss: 1.784616]\n",
      "epoch:25 step:24291 [D loss: 0.624419, acc: 62.50%] [G loss: 1.804136]\n",
      "epoch:25 step:24292 [D loss: 0.663241, acc: 67.19%] [G loss: 1.799368]\n",
      "epoch:25 step:24293 [D loss: 0.621276, acc: 68.75%] [G loss: 1.819574]\n",
      "epoch:25 step:24294 [D loss: 0.723815, acc: 57.03%] [G loss: 1.775583]\n",
      "epoch:25 step:24295 [D loss: 0.678511, acc: 62.50%] [G loss: 1.815594]\n",
      "epoch:25 step:24296 [D loss: 0.703615, acc: 56.25%] [G loss: 1.775055]\n",
      "epoch:25 step:24297 [D loss: 0.671455, acc: 64.84%] [G loss: 1.800674]\n",
      "epoch:25 step:24298 [D loss: 0.625986, acc: 66.41%] [G loss: 1.730771]\n",
      "epoch:25 step:24299 [D loss: 0.657546, acc: 59.38%] [G loss: 1.636538]\n",
      "epoch:25 step:24300 [D loss: 0.628318, acc: 67.19%] [G loss: 1.901773]\n",
      "epoch:25 step:24301 [D loss: 0.674057, acc: 53.91%] [G loss: 1.842926]\n",
      "epoch:25 step:24302 [D loss: 0.618516, acc: 64.06%] [G loss: 1.786137]\n",
      "epoch:25 step:24303 [D loss: 0.664928, acc: 63.28%] [G loss: 1.734325]\n",
      "epoch:25 step:24304 [D loss: 0.657728, acc: 58.59%] [G loss: 1.749361]\n",
      "epoch:25 step:24305 [D loss: 0.659879, acc: 62.50%] [G loss: 1.805365]\n",
      "epoch:25 step:24306 [D loss: 0.635670, acc: 63.28%] [G loss: 1.737871]\n",
      "epoch:25 step:24307 [D loss: 0.632175, acc: 61.72%] [G loss: 1.827086]\n",
      "epoch:25 step:24308 [D loss: 0.649610, acc: 65.62%] [G loss: 1.799009]\n",
      "epoch:25 step:24309 [D loss: 0.612500, acc: 66.41%] [G loss: 1.851047]\n",
      "epoch:25 step:24310 [D loss: 0.623514, acc: 63.28%] [G loss: 1.879254]\n",
      "epoch:25 step:24311 [D loss: 0.591004, acc: 69.53%] [G loss: 1.931015]\n",
      "epoch:25 step:24312 [D loss: 0.633548, acc: 63.28%] [G loss: 1.896020]\n",
      "epoch:25 step:24313 [D loss: 0.640189, acc: 62.50%] [G loss: 1.865325]\n",
      "epoch:25 step:24314 [D loss: 0.645903, acc: 64.06%] [G loss: 2.041013]\n",
      "epoch:25 step:24315 [D loss: 0.611515, acc: 65.62%] [G loss: 1.905325]\n",
      "epoch:25 step:24316 [D loss: 0.676017, acc: 55.47%] [G loss: 1.877714]\n",
      "epoch:25 step:24317 [D loss: 0.663601, acc: 58.59%] [G loss: 1.849365]\n",
      "epoch:25 step:24318 [D loss: 0.649412, acc: 64.06%] [G loss: 1.889982]\n",
      "epoch:25 step:24319 [D loss: 0.604615, acc: 62.50%] [G loss: 2.014451]\n",
      "epoch:25 step:24320 [D loss: 0.649030, acc: 61.72%] [G loss: 1.962306]\n",
      "epoch:25 step:24321 [D loss: 0.661945, acc: 60.94%] [G loss: 1.837062]\n",
      "epoch:25 step:24322 [D loss: 0.657405, acc: 60.16%] [G loss: 1.848401]\n",
      "epoch:25 step:24323 [D loss: 0.582869, acc: 67.97%] [G loss: 1.855543]\n",
      "epoch:25 step:24324 [D loss: 0.578524, acc: 69.53%] [G loss: 2.094304]\n",
      "epoch:25 step:24325 [D loss: 0.634252, acc: 60.16%] [G loss: 1.985608]\n",
      "epoch:25 step:24326 [D loss: 0.646310, acc: 64.06%] [G loss: 1.958597]\n",
      "epoch:25 step:24327 [D loss: 0.663855, acc: 61.72%] [G loss: 1.860989]\n",
      "epoch:25 step:24328 [D loss: 0.707122, acc: 56.25%] [G loss: 1.819030]\n",
      "epoch:25 step:24329 [D loss: 0.640214, acc: 65.62%] [G loss: 1.894500]\n",
      "epoch:25 step:24330 [D loss: 0.603749, acc: 64.84%] [G loss: 1.914516]\n",
      "epoch:25 step:24331 [D loss: 0.668518, acc: 60.16%] [G loss: 2.096923]\n",
      "epoch:25 step:24332 [D loss: 0.626367, acc: 65.62%] [G loss: 1.864286]\n",
      "epoch:25 step:24333 [D loss: 0.615377, acc: 69.53%] [G loss: 2.015279]\n",
      "epoch:25 step:24334 [D loss: 0.570108, acc: 67.97%] [G loss: 2.188607]\n",
      "epoch:25 step:24335 [D loss: 0.649397, acc: 62.50%] [G loss: 1.904441]\n",
      "epoch:25 step:24336 [D loss: 0.631562, acc: 66.41%] [G loss: 2.039774]\n",
      "epoch:25 step:24337 [D loss: 0.668034, acc: 62.50%] [G loss: 2.124101]\n",
      "epoch:25 step:24338 [D loss: 0.698446, acc: 51.56%] [G loss: 1.948021]\n",
      "epoch:25 step:24339 [D loss: 0.745398, acc: 51.56%] [G loss: 1.834213]\n",
      "epoch:25 step:24340 [D loss: 0.683799, acc: 60.16%] [G loss: 1.905922]\n",
      "epoch:25 step:24341 [D loss: 0.679339, acc: 62.50%] [G loss: 1.959273]\n",
      "epoch:25 step:24342 [D loss: 0.601831, acc: 64.84%] [G loss: 1.850216]\n",
      "epoch:25 step:24343 [D loss: 0.631968, acc: 64.06%] [G loss: 1.879300]\n",
      "epoch:25 step:24344 [D loss: 0.622818, acc: 65.62%] [G loss: 2.032076]\n",
      "epoch:25 step:24345 [D loss: 0.695305, acc: 61.72%] [G loss: 1.917537]\n",
      "epoch:25 step:24346 [D loss: 0.638392, acc: 64.84%] [G loss: 1.911142]\n",
      "epoch:25 step:24347 [D loss: 0.596243, acc: 67.97%] [G loss: 1.857811]\n",
      "epoch:25 step:24348 [D loss: 0.598065, acc: 71.88%] [G loss: 2.038553]\n",
      "epoch:25 step:24349 [D loss: 0.575710, acc: 74.22%] [G loss: 1.952541]\n",
      "epoch:25 step:24350 [D loss: 0.637905, acc: 62.50%] [G loss: 1.938200]\n",
      "epoch:25 step:24351 [D loss: 0.625700, acc: 64.84%] [G loss: 2.007845]\n",
      "epoch:25 step:24352 [D loss: 0.625121, acc: 59.38%] [G loss: 2.110473]\n",
      "epoch:25 step:24353 [D loss: 0.751676, acc: 52.34%] [G loss: 1.909197]\n",
      "epoch:25 step:24354 [D loss: 0.714950, acc: 56.25%] [G loss: 1.926819]\n",
      "epoch:25 step:24355 [D loss: 0.640355, acc: 62.50%] [G loss: 1.972064]\n",
      "epoch:25 step:24356 [D loss: 0.624996, acc: 66.41%] [G loss: 2.020339]\n",
      "epoch:25 step:24357 [D loss: 0.666244, acc: 53.91%] [G loss: 1.931761]\n",
      "epoch:25 step:24358 [D loss: 0.587618, acc: 69.53%] [G loss: 1.808719]\n",
      "epoch:25 step:24359 [D loss: 0.637889, acc: 63.28%] [G loss: 1.880114]\n",
      "epoch:25 step:24360 [D loss: 0.623067, acc: 65.62%] [G loss: 1.932517]\n",
      "epoch:25 step:24361 [D loss: 0.634094, acc: 62.50%] [G loss: 2.049020]\n",
      "epoch:25 step:24362 [D loss: 0.559496, acc: 71.88%] [G loss: 2.578259]\n",
      "epoch:26 step:24363 [D loss: 0.674392, acc: 63.28%] [G loss: 1.926054]\n",
      "epoch:26 step:24364 [D loss: 0.665243, acc: 59.38%] [G loss: 1.854789]\n",
      "epoch:26 step:24365 [D loss: 0.636896, acc: 61.72%] [G loss: 1.935018]\n",
      "epoch:26 step:24366 [D loss: 0.655740, acc: 64.84%] [G loss: 1.901988]\n",
      "epoch:26 step:24367 [D loss: 0.665871, acc: 59.38%] [G loss: 1.861957]\n",
      "epoch:26 step:24368 [D loss: 0.642949, acc: 64.06%] [G loss: 1.901047]\n",
      "epoch:26 step:24369 [D loss: 0.604486, acc: 67.19%] [G loss: 1.956040]\n",
      "epoch:26 step:24370 [D loss: 0.661990, acc: 60.16%] [G loss: 2.054945]\n",
      "epoch:26 step:24371 [D loss: 0.609018, acc: 66.41%] [G loss: 1.893172]\n",
      "epoch:26 step:24372 [D loss: 0.597887, acc: 65.62%] [G loss: 2.062809]\n",
      "epoch:26 step:24373 [D loss: 0.662875, acc: 59.38%] [G loss: 1.929306]\n",
      "epoch:26 step:24374 [D loss: 0.625154, acc: 66.41%] [G loss: 1.883384]\n",
      "epoch:26 step:24375 [D loss: 0.646452, acc: 60.94%] [G loss: 1.844853]\n",
      "epoch:26 step:24376 [D loss: 0.600072, acc: 71.09%] [G loss: 1.935402]\n",
      "epoch:26 step:24377 [D loss: 0.603030, acc: 67.97%] [G loss: 1.997378]\n",
      "epoch:26 step:24378 [D loss: 0.619023, acc: 64.06%] [G loss: 2.054814]\n",
      "epoch:26 step:24379 [D loss: 0.664421, acc: 55.47%] [G loss: 1.876347]\n",
      "epoch:26 step:24380 [D loss: 0.656986, acc: 57.81%] [G loss: 1.911549]\n",
      "epoch:26 step:24381 [D loss: 0.680231, acc: 58.59%] [G loss: 1.845212]\n",
      "epoch:26 step:24382 [D loss: 0.722500, acc: 50.78%] [G loss: 1.683598]\n",
      "epoch:26 step:24383 [D loss: 0.702569, acc: 53.12%] [G loss: 1.861686]\n",
      "epoch:26 step:24384 [D loss: 0.673325, acc: 65.62%] [G loss: 1.833227]\n",
      "epoch:26 step:24385 [D loss: 0.678506, acc: 60.94%] [G loss: 2.001827]\n",
      "epoch:26 step:24386 [D loss: 0.633498, acc: 67.19%] [G loss: 1.924279]\n",
      "epoch:26 step:24387 [D loss: 0.631135, acc: 67.97%] [G loss: 2.066917]\n",
      "epoch:26 step:24388 [D loss: 0.663458, acc: 57.81%] [G loss: 1.886318]\n",
      "epoch:26 step:24389 [D loss: 0.709330, acc: 57.81%] [G loss: 1.707151]\n",
      "epoch:26 step:24390 [D loss: 0.621453, acc: 64.84%] [G loss: 1.730191]\n",
      "epoch:26 step:24391 [D loss: 0.608804, acc: 67.19%] [G loss: 1.939928]\n",
      "epoch:26 step:24392 [D loss: 0.683985, acc: 53.91%] [G loss: 1.757382]\n",
      "epoch:26 step:24393 [D loss: 0.719285, acc: 55.47%] [G loss: 1.644627]\n",
      "epoch:26 step:24394 [D loss: 0.667287, acc: 60.16%] [G loss: 1.736522]\n",
      "epoch:26 step:24395 [D loss: 0.617786, acc: 65.62%] [G loss: 1.760488]\n",
      "epoch:26 step:24396 [D loss: 0.683527, acc: 53.91%] [G loss: 1.935548]\n",
      "epoch:26 step:24397 [D loss: 0.651592, acc: 63.28%] [G loss: 1.797886]\n",
      "epoch:26 step:24398 [D loss: 0.623059, acc: 65.62%] [G loss: 1.874369]\n",
      "epoch:26 step:24399 [D loss: 0.640878, acc: 62.50%] [G loss: 1.997765]\n",
      "epoch:26 step:24400 [D loss: 0.691463, acc: 57.03%] [G loss: 1.821301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.4785845  1.80985696 5.94642164 4.61105107 3.32927505 5.61702232\n",
      " 4.24245268 4.63465696 4.41142694 3.49690191]\n",
      "##########\n",
      "epoch:26 step:24401 [D loss: 0.648815, acc: 62.50%] [G loss: 1.836088]\n",
      "epoch:26 step:24402 [D loss: 0.588782, acc: 67.97%] [G loss: 2.195093]\n",
      "epoch:26 step:24403 [D loss: 0.632592, acc: 62.50%] [G loss: 1.854278]\n",
      "epoch:26 step:24404 [D loss: 0.620819, acc: 67.19%] [G loss: 1.876312]\n",
      "epoch:26 step:24405 [D loss: 0.649254, acc: 59.38%] [G loss: 1.796862]\n",
      "epoch:26 step:24406 [D loss: 0.629052, acc: 64.84%] [G loss: 1.793250]\n",
      "epoch:26 step:24407 [D loss: 0.662862, acc: 59.38%] [G loss: 1.773391]\n",
      "epoch:26 step:24408 [D loss: 0.689636, acc: 60.16%] [G loss: 1.739595]\n",
      "epoch:26 step:24409 [D loss: 0.627313, acc: 64.06%] [G loss: 1.855211]\n",
      "epoch:26 step:24410 [D loss: 0.637377, acc: 62.50%] [G loss: 1.926138]\n",
      "epoch:26 step:24411 [D loss: 0.597954, acc: 62.50%] [G loss: 1.938699]\n",
      "epoch:26 step:24412 [D loss: 0.660660, acc: 64.06%] [G loss: 1.954521]\n",
      "epoch:26 step:24413 [D loss: 0.695190, acc: 61.72%] [G loss: 1.845226]\n",
      "epoch:26 step:24414 [D loss: 0.655131, acc: 59.38%] [G loss: 1.814093]\n",
      "epoch:26 step:24415 [D loss: 0.659382, acc: 64.06%] [G loss: 1.932771]\n",
      "epoch:26 step:24416 [D loss: 0.661015, acc: 61.72%] [G loss: 1.833861]\n",
      "epoch:26 step:24417 [D loss: 0.579257, acc: 70.31%] [G loss: 2.006304]\n",
      "epoch:26 step:24418 [D loss: 0.618776, acc: 71.09%] [G loss: 1.900718]\n",
      "epoch:26 step:24419 [D loss: 0.697300, acc: 53.91%] [G loss: 1.887501]\n",
      "epoch:26 step:24420 [D loss: 0.652628, acc: 64.06%] [G loss: 1.854014]\n",
      "epoch:26 step:24421 [D loss: 0.644432, acc: 57.03%] [G loss: 1.902673]\n",
      "epoch:26 step:24422 [D loss: 0.650667, acc: 59.38%] [G loss: 1.840950]\n",
      "epoch:26 step:24423 [D loss: 0.664080, acc: 64.84%] [G loss: 1.875955]\n",
      "epoch:26 step:24424 [D loss: 0.660703, acc: 64.06%] [G loss: 1.957013]\n",
      "epoch:26 step:24425 [D loss: 0.713408, acc: 53.91%] [G loss: 1.767874]\n",
      "epoch:26 step:24426 [D loss: 0.666319, acc: 60.16%] [G loss: 1.842953]\n",
      "epoch:26 step:24427 [D loss: 0.597336, acc: 70.31%] [G loss: 1.862582]\n",
      "epoch:26 step:24428 [D loss: 0.592082, acc: 66.41%] [G loss: 1.955391]\n",
      "epoch:26 step:24429 [D loss: 0.637781, acc: 64.06%] [G loss: 1.951740]\n",
      "epoch:26 step:24430 [D loss: 0.656760, acc: 60.16%] [G loss: 1.769653]\n",
      "epoch:26 step:24431 [D loss: 0.627740, acc: 69.53%] [G loss: 2.098142]\n",
      "epoch:26 step:24432 [D loss: 0.609546, acc: 68.75%] [G loss: 1.908833]\n",
      "epoch:26 step:24433 [D loss: 0.674901, acc: 60.16%] [G loss: 1.834833]\n",
      "epoch:26 step:24434 [D loss: 0.612668, acc: 67.19%] [G loss: 1.809704]\n",
      "epoch:26 step:24435 [D loss: 0.669141, acc: 57.03%] [G loss: 1.781561]\n",
      "epoch:26 step:24436 [D loss: 0.663083, acc: 61.72%] [G loss: 1.861297]\n",
      "epoch:26 step:24437 [D loss: 0.614148, acc: 67.97%] [G loss: 2.147681]\n",
      "epoch:26 step:24438 [D loss: 0.625428, acc: 61.72%] [G loss: 1.958005]\n",
      "epoch:26 step:24439 [D loss: 0.574814, acc: 73.44%] [G loss: 2.012476]\n",
      "epoch:26 step:24440 [D loss: 0.620825, acc: 67.97%] [G loss: 1.946686]\n",
      "epoch:26 step:24441 [D loss: 0.640927, acc: 61.72%] [G loss: 1.896265]\n",
      "epoch:26 step:24442 [D loss: 0.665993, acc: 57.03%] [G loss: 1.862381]\n",
      "epoch:26 step:24443 [D loss: 0.688520, acc: 60.16%] [G loss: 1.739519]\n",
      "epoch:26 step:24444 [D loss: 0.688065, acc: 56.25%] [G loss: 1.720639]\n",
      "epoch:26 step:24445 [D loss: 0.692934, acc: 50.78%] [G loss: 1.696426]\n",
      "epoch:26 step:24446 [D loss: 0.672833, acc: 58.59%] [G loss: 1.806401]\n",
      "epoch:26 step:24447 [D loss: 0.631929, acc: 62.50%] [G loss: 1.812310]\n",
      "epoch:26 step:24448 [D loss: 0.659890, acc: 57.81%] [G loss: 1.709934]\n",
      "epoch:26 step:24449 [D loss: 0.646572, acc: 64.84%] [G loss: 1.910195]\n",
      "epoch:26 step:24450 [D loss: 0.636899, acc: 64.06%] [G loss: 1.965136]\n",
      "epoch:26 step:24451 [D loss: 0.666694, acc: 58.59%] [G loss: 1.871878]\n",
      "epoch:26 step:24452 [D loss: 0.668260, acc: 60.16%] [G loss: 1.910569]\n",
      "epoch:26 step:24453 [D loss: 0.652060, acc: 60.16%] [G loss: 1.766121]\n",
      "epoch:26 step:24454 [D loss: 0.657377, acc: 59.38%] [G loss: 2.067615]\n",
      "epoch:26 step:24455 [D loss: 0.579436, acc: 74.22%] [G loss: 1.971454]\n",
      "epoch:26 step:24456 [D loss: 0.643937, acc: 61.72%] [G loss: 1.842937]\n",
      "epoch:26 step:24457 [D loss: 0.676365, acc: 66.41%] [G loss: 1.793711]\n",
      "epoch:26 step:24458 [D loss: 0.693876, acc: 54.69%] [G loss: 1.868520]\n",
      "epoch:26 step:24459 [D loss: 0.636513, acc: 60.16%] [G loss: 1.867929]\n",
      "epoch:26 step:24460 [D loss: 0.639883, acc: 61.72%] [G loss: 1.817729]\n",
      "epoch:26 step:24461 [D loss: 0.653916, acc: 58.59%] [G loss: 1.787673]\n",
      "epoch:26 step:24462 [D loss: 0.609820, acc: 64.06%] [G loss: 1.851891]\n",
      "epoch:26 step:24463 [D loss: 0.636072, acc: 64.84%] [G loss: 1.876361]\n",
      "epoch:26 step:24464 [D loss: 0.637622, acc: 57.81%] [G loss: 1.767067]\n",
      "epoch:26 step:24465 [D loss: 0.644549, acc: 61.72%] [G loss: 1.847240]\n",
      "epoch:26 step:24466 [D loss: 0.649784, acc: 57.81%] [G loss: 1.866731]\n",
      "epoch:26 step:24467 [D loss: 0.637003, acc: 64.06%] [G loss: 1.903343]\n",
      "epoch:26 step:24468 [D loss: 0.690540, acc: 67.19%] [G loss: 2.142341]\n",
      "epoch:26 step:24469 [D loss: 0.597063, acc: 71.09%] [G loss: 2.180021]\n",
      "epoch:26 step:24470 [D loss: 0.689039, acc: 57.03%] [G loss: 1.863542]\n",
      "epoch:26 step:24471 [D loss: 0.697180, acc: 52.34%] [G loss: 1.683513]\n",
      "epoch:26 step:24472 [D loss: 0.732964, acc: 53.12%] [G loss: 1.849520]\n",
      "epoch:26 step:24473 [D loss: 0.669013, acc: 60.94%] [G loss: 1.855782]\n",
      "epoch:26 step:24474 [D loss: 0.617878, acc: 66.41%] [G loss: 1.845422]\n",
      "epoch:26 step:24475 [D loss: 0.632819, acc: 66.41%] [G loss: 1.899720]\n",
      "epoch:26 step:24476 [D loss: 0.635390, acc: 65.62%] [G loss: 2.010525]\n",
      "epoch:26 step:24477 [D loss: 0.594731, acc: 74.22%] [G loss: 2.022778]\n",
      "epoch:26 step:24478 [D loss: 0.665658, acc: 62.50%] [G loss: 2.170579]\n",
      "epoch:26 step:24479 [D loss: 0.583784, acc: 67.97%] [G loss: 2.074934]\n",
      "epoch:26 step:24480 [D loss: 0.637982, acc: 65.62%] [G loss: 1.991773]\n",
      "epoch:26 step:24481 [D loss: 0.536099, acc: 75.78%] [G loss: 2.353373]\n",
      "epoch:26 step:24482 [D loss: 0.723064, acc: 57.81%] [G loss: 1.931307]\n",
      "epoch:26 step:24483 [D loss: 0.625558, acc: 64.06%] [G loss: 2.044815]\n",
      "epoch:26 step:24484 [D loss: 0.689254, acc: 57.03%] [G loss: 1.994962]\n",
      "epoch:26 step:24485 [D loss: 0.626316, acc: 64.84%] [G loss: 1.998205]\n",
      "epoch:26 step:24486 [D loss: 0.695037, acc: 60.94%] [G loss: 1.750579]\n",
      "epoch:26 step:24487 [D loss: 0.712304, acc: 48.44%] [G loss: 1.742823]\n",
      "epoch:26 step:24488 [D loss: 0.649030, acc: 62.50%] [G loss: 1.801225]\n",
      "epoch:26 step:24489 [D loss: 0.695903, acc: 53.91%] [G loss: 1.905705]\n",
      "epoch:26 step:24490 [D loss: 0.648606, acc: 64.06%] [G loss: 1.920200]\n",
      "epoch:26 step:24491 [D loss: 0.653228, acc: 64.06%] [G loss: 1.779419]\n",
      "epoch:26 step:24492 [D loss: 0.579635, acc: 71.88%] [G loss: 2.032411]\n",
      "epoch:26 step:24493 [D loss: 0.681345, acc: 67.19%] [G loss: 1.900927]\n",
      "epoch:26 step:24494 [D loss: 0.624576, acc: 66.41%] [G loss: 1.866292]\n",
      "epoch:26 step:24495 [D loss: 0.697894, acc: 53.91%] [G loss: 1.732375]\n",
      "epoch:26 step:24496 [D loss: 0.652788, acc: 60.16%] [G loss: 1.867254]\n",
      "epoch:26 step:24497 [D loss: 0.603579, acc: 64.84%] [G loss: 1.775903]\n",
      "epoch:26 step:24498 [D loss: 0.649407, acc: 67.19%] [G loss: 2.005241]\n",
      "epoch:26 step:24499 [D loss: 0.630537, acc: 65.62%] [G loss: 1.868551]\n",
      "epoch:26 step:24500 [D loss: 0.666493, acc: 58.59%] [G loss: 1.854823]\n",
      "epoch:26 step:24501 [D loss: 0.627236, acc: 60.94%] [G loss: 1.861143]\n",
      "epoch:26 step:24502 [D loss: 0.697380, acc: 56.25%] [G loss: 1.814145]\n",
      "epoch:26 step:24503 [D loss: 0.679211, acc: 55.47%] [G loss: 1.778065]\n",
      "epoch:26 step:24504 [D loss: 0.646848, acc: 59.38%] [G loss: 1.816635]\n",
      "epoch:26 step:24505 [D loss: 0.678711, acc: 60.16%] [G loss: 1.759739]\n",
      "epoch:26 step:24506 [D loss: 0.649836, acc: 56.25%] [G loss: 1.814339]\n",
      "epoch:26 step:24507 [D loss: 0.693386, acc: 57.03%] [G loss: 1.862098]\n",
      "epoch:26 step:24508 [D loss: 0.674743, acc: 58.59%] [G loss: 1.785649]\n",
      "epoch:26 step:24509 [D loss: 0.672976, acc: 61.72%] [G loss: 1.886886]\n",
      "epoch:26 step:24510 [D loss: 0.691002, acc: 53.91%] [G loss: 1.747036]\n",
      "epoch:26 step:24511 [D loss: 0.650206, acc: 57.03%] [G loss: 1.775519]\n",
      "epoch:26 step:24512 [D loss: 0.625590, acc: 60.16%] [G loss: 1.882782]\n",
      "epoch:26 step:24513 [D loss: 0.660218, acc: 61.72%] [G loss: 1.947329]\n",
      "epoch:26 step:24514 [D loss: 0.711925, acc: 49.22%] [G loss: 1.804486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24515 [D loss: 0.635422, acc: 61.72%] [G loss: 1.860188]\n",
      "epoch:26 step:24516 [D loss: 0.666251, acc: 61.72%] [G loss: 1.835974]\n",
      "epoch:26 step:24517 [D loss: 0.668262, acc: 58.59%] [G loss: 1.821066]\n",
      "epoch:26 step:24518 [D loss: 0.631704, acc: 65.62%] [G loss: 1.972065]\n",
      "epoch:26 step:24519 [D loss: 0.640027, acc: 62.50%] [G loss: 1.780454]\n",
      "epoch:26 step:24520 [D loss: 0.685550, acc: 58.59%] [G loss: 1.874925]\n",
      "epoch:26 step:24521 [D loss: 0.628847, acc: 64.84%] [G loss: 1.930579]\n",
      "epoch:26 step:24522 [D loss: 0.720839, acc: 53.12%] [G loss: 1.722237]\n",
      "epoch:26 step:24523 [D loss: 0.667658, acc: 58.59%] [G loss: 1.693003]\n",
      "epoch:26 step:24524 [D loss: 0.629877, acc: 67.19%] [G loss: 1.870814]\n",
      "epoch:26 step:24525 [D loss: 0.621651, acc: 68.75%] [G loss: 1.847564]\n",
      "epoch:26 step:24526 [D loss: 0.657678, acc: 64.06%] [G loss: 1.658388]\n",
      "epoch:26 step:24527 [D loss: 0.628615, acc: 67.19%] [G loss: 1.832373]\n",
      "epoch:26 step:24528 [D loss: 0.653063, acc: 67.97%] [G loss: 1.778402]\n",
      "epoch:26 step:24529 [D loss: 0.600637, acc: 67.19%] [G loss: 1.886924]\n",
      "epoch:26 step:24530 [D loss: 0.674501, acc: 58.59%] [G loss: 1.838812]\n",
      "epoch:26 step:24531 [D loss: 0.676395, acc: 54.69%] [G loss: 1.793151]\n",
      "epoch:26 step:24532 [D loss: 0.625766, acc: 64.84%] [G loss: 1.755680]\n",
      "epoch:26 step:24533 [D loss: 0.645964, acc: 58.59%] [G loss: 1.861827]\n",
      "epoch:26 step:24534 [D loss: 0.633080, acc: 65.62%] [G loss: 1.835381]\n",
      "epoch:26 step:24535 [D loss: 0.669321, acc: 54.69%] [G loss: 1.740297]\n",
      "epoch:26 step:24536 [D loss: 0.669699, acc: 56.25%] [G loss: 1.766837]\n",
      "epoch:26 step:24537 [D loss: 0.670390, acc: 57.81%] [G loss: 1.727568]\n",
      "epoch:26 step:24538 [D loss: 0.662667, acc: 61.72%] [G loss: 1.842658]\n",
      "epoch:26 step:24539 [D loss: 0.655848, acc: 61.72%] [G loss: 1.790027]\n",
      "epoch:26 step:24540 [D loss: 0.629134, acc: 63.28%] [G loss: 1.930458]\n",
      "epoch:26 step:24541 [D loss: 0.655732, acc: 64.84%] [G loss: 1.837343]\n",
      "epoch:26 step:24542 [D loss: 0.656856, acc: 56.25%] [G loss: 1.800769]\n",
      "epoch:26 step:24543 [D loss: 0.686423, acc: 53.91%] [G loss: 1.800624]\n",
      "epoch:26 step:24544 [D loss: 0.665048, acc: 62.50%] [G loss: 1.804997]\n",
      "epoch:26 step:24545 [D loss: 0.690540, acc: 60.16%] [G loss: 1.838754]\n",
      "epoch:26 step:24546 [D loss: 0.665519, acc: 57.81%] [G loss: 1.827363]\n",
      "epoch:26 step:24547 [D loss: 0.622356, acc: 67.97%] [G loss: 1.886263]\n",
      "epoch:26 step:24548 [D loss: 0.643139, acc: 66.41%] [G loss: 1.726646]\n",
      "epoch:26 step:24549 [D loss: 0.648567, acc: 60.94%] [G loss: 1.893960]\n",
      "epoch:26 step:24550 [D loss: 0.680115, acc: 53.91%] [G loss: 1.845225]\n",
      "epoch:26 step:24551 [D loss: 0.681987, acc: 54.69%] [G loss: 1.794424]\n",
      "epoch:26 step:24552 [D loss: 0.608101, acc: 66.41%] [G loss: 1.872261]\n",
      "epoch:26 step:24553 [D loss: 0.621993, acc: 63.28%] [G loss: 1.888006]\n",
      "epoch:26 step:24554 [D loss: 0.657009, acc: 54.69%] [G loss: 1.952596]\n",
      "epoch:26 step:24555 [D loss: 0.599836, acc: 69.53%] [G loss: 1.936333]\n",
      "epoch:26 step:24556 [D loss: 0.625570, acc: 68.75%] [G loss: 1.884546]\n",
      "epoch:26 step:24557 [D loss: 0.676881, acc: 59.38%] [G loss: 1.900645]\n",
      "epoch:26 step:24558 [D loss: 0.684521, acc: 56.25%] [G loss: 1.681854]\n",
      "epoch:26 step:24559 [D loss: 0.625996, acc: 64.84%] [G loss: 1.905201]\n",
      "epoch:26 step:24560 [D loss: 0.672663, acc: 55.47%] [G loss: 1.763222]\n",
      "epoch:26 step:24561 [D loss: 0.621141, acc: 64.06%] [G loss: 1.887075]\n",
      "epoch:26 step:24562 [D loss: 0.618449, acc: 62.50%] [G loss: 1.873692]\n",
      "epoch:26 step:24563 [D loss: 0.660794, acc: 60.16%] [G loss: 1.824641]\n",
      "epoch:26 step:24564 [D loss: 0.661484, acc: 60.94%] [G loss: 1.866008]\n",
      "epoch:26 step:24565 [D loss: 0.671680, acc: 57.81%] [G loss: 1.878290]\n",
      "epoch:26 step:24566 [D loss: 0.610704, acc: 66.41%] [G loss: 1.873412]\n",
      "epoch:26 step:24567 [D loss: 0.678788, acc: 61.72%] [G loss: 1.805668]\n",
      "epoch:26 step:24568 [D loss: 0.575023, acc: 71.09%] [G loss: 1.946594]\n",
      "epoch:26 step:24569 [D loss: 0.594368, acc: 67.19%] [G loss: 2.085908]\n",
      "epoch:26 step:24570 [D loss: 0.664080, acc: 57.03%] [G loss: 2.121816]\n",
      "epoch:26 step:24571 [D loss: 0.590316, acc: 71.09%] [G loss: 2.228351]\n",
      "epoch:26 step:24572 [D loss: 0.682832, acc: 62.50%] [G loss: 1.867122]\n",
      "epoch:26 step:24573 [D loss: 0.680308, acc: 64.84%] [G loss: 1.848996]\n",
      "epoch:26 step:24574 [D loss: 0.658065, acc: 57.81%] [G loss: 1.968483]\n",
      "epoch:26 step:24575 [D loss: 0.644077, acc: 61.72%] [G loss: 1.846417]\n",
      "epoch:26 step:24576 [D loss: 0.688007, acc: 53.91%] [G loss: 1.868060]\n",
      "epoch:26 step:24577 [D loss: 0.645850, acc: 61.72%] [G loss: 1.781043]\n",
      "epoch:26 step:24578 [D loss: 0.626146, acc: 61.72%] [G loss: 2.014142]\n",
      "epoch:26 step:24579 [D loss: 0.632308, acc: 67.19%] [G loss: 1.914724]\n",
      "epoch:26 step:24580 [D loss: 0.641579, acc: 60.94%] [G loss: 2.045240]\n",
      "epoch:26 step:24581 [D loss: 0.599301, acc: 67.19%] [G loss: 2.062678]\n",
      "epoch:26 step:24582 [D loss: 0.691980, acc: 58.59%] [G loss: 1.868455]\n",
      "epoch:26 step:24583 [D loss: 0.644555, acc: 62.50%] [G loss: 2.008769]\n",
      "epoch:26 step:24584 [D loss: 0.680295, acc: 59.38%] [G loss: 1.820641]\n",
      "epoch:26 step:24585 [D loss: 0.668145, acc: 57.81%] [G loss: 1.925081]\n",
      "epoch:26 step:24586 [D loss: 0.681903, acc: 59.38%] [G loss: 1.729458]\n",
      "epoch:26 step:24587 [D loss: 0.641580, acc: 66.41%] [G loss: 1.732244]\n",
      "epoch:26 step:24588 [D loss: 0.649325, acc: 58.59%] [G loss: 1.871882]\n",
      "epoch:26 step:24589 [D loss: 0.616198, acc: 69.53%] [G loss: 1.856268]\n",
      "epoch:26 step:24590 [D loss: 0.713365, acc: 55.47%] [G loss: 1.674377]\n",
      "epoch:26 step:24591 [D loss: 0.590748, acc: 64.84%] [G loss: 2.012738]\n",
      "epoch:26 step:24592 [D loss: 0.631416, acc: 65.62%] [G loss: 2.214921]\n",
      "epoch:26 step:24593 [D loss: 0.567456, acc: 75.00%] [G loss: 2.377415]\n",
      "epoch:26 step:24594 [D loss: 0.597457, acc: 68.75%] [G loss: 2.378461]\n",
      "epoch:26 step:24595 [D loss: 0.618058, acc: 68.75%] [G loss: 1.894003]\n",
      "epoch:26 step:24596 [D loss: 0.676500, acc: 57.81%] [G loss: 1.857769]\n",
      "epoch:26 step:24597 [D loss: 0.671749, acc: 60.94%] [G loss: 1.872814]\n",
      "epoch:26 step:24598 [D loss: 0.616495, acc: 65.62%] [G loss: 1.834360]\n",
      "epoch:26 step:24599 [D loss: 0.691817, acc: 60.94%] [G loss: 2.021976]\n",
      "epoch:26 step:24600 [D loss: 0.637900, acc: 64.06%] [G loss: 1.975748]\n",
      "##############\n",
      "[2.48738348 1.62655539 6.02700696 4.74879333 3.45293712 5.63030833\n",
      " 4.43588955 4.71277803 4.49398328 3.45013975]\n",
      "##########\n",
      "epoch:26 step:24601 [D loss: 0.660531, acc: 59.38%] [G loss: 1.855599]\n",
      "epoch:26 step:24602 [D loss: 0.621508, acc: 64.84%] [G loss: 2.032118]\n",
      "epoch:26 step:24603 [D loss: 0.633703, acc: 59.38%] [G loss: 1.817112]\n",
      "epoch:26 step:24604 [D loss: 0.630012, acc: 67.97%] [G loss: 1.883896]\n",
      "epoch:26 step:24605 [D loss: 0.660194, acc: 67.19%] [G loss: 1.887599]\n",
      "epoch:26 step:24606 [D loss: 0.644665, acc: 57.03%] [G loss: 1.938918]\n",
      "epoch:26 step:24607 [D loss: 0.646580, acc: 64.84%] [G loss: 1.950457]\n",
      "epoch:26 step:24608 [D loss: 0.642815, acc: 66.41%] [G loss: 1.945087]\n",
      "epoch:26 step:24609 [D loss: 0.654548, acc: 62.50%] [G loss: 1.975314]\n",
      "epoch:26 step:24610 [D loss: 0.664396, acc: 61.72%] [G loss: 2.104075]\n",
      "epoch:26 step:24611 [D loss: 0.725888, acc: 53.12%] [G loss: 1.730024]\n",
      "epoch:26 step:24612 [D loss: 0.714783, acc: 50.78%] [G loss: 1.676700]\n",
      "epoch:26 step:24613 [D loss: 0.689734, acc: 57.03%] [G loss: 1.667954]\n",
      "epoch:26 step:24614 [D loss: 0.695484, acc: 51.56%] [G loss: 1.817679]\n",
      "epoch:26 step:24615 [D loss: 0.651720, acc: 57.03%] [G loss: 1.771514]\n",
      "epoch:26 step:24616 [D loss: 0.699633, acc: 51.56%] [G loss: 1.726354]\n",
      "epoch:26 step:24617 [D loss: 0.603155, acc: 66.41%] [G loss: 1.765252]\n",
      "epoch:26 step:24618 [D loss: 0.637670, acc: 63.28%] [G loss: 1.891201]\n",
      "epoch:26 step:24619 [D loss: 0.688895, acc: 59.38%] [G loss: 1.848502]\n",
      "epoch:26 step:24620 [D loss: 0.650073, acc: 62.50%] [G loss: 1.764102]\n",
      "epoch:26 step:24621 [D loss: 0.665216, acc: 56.25%] [G loss: 1.855454]\n",
      "epoch:26 step:24622 [D loss: 0.642132, acc: 60.94%] [G loss: 1.777524]\n",
      "epoch:26 step:24623 [D loss: 0.618953, acc: 69.53%] [G loss: 1.901509]\n",
      "epoch:26 step:24624 [D loss: 0.613924, acc: 64.06%] [G loss: 1.896074]\n",
      "epoch:26 step:24625 [D loss: 0.681617, acc: 63.28%] [G loss: 1.794306]\n",
      "epoch:26 step:24626 [D loss: 0.617485, acc: 66.41%] [G loss: 1.910392]\n",
      "epoch:26 step:24627 [D loss: 0.643520, acc: 62.50%] [G loss: 1.830180]\n",
      "epoch:26 step:24628 [D loss: 0.665536, acc: 60.16%] [G loss: 1.793824]\n",
      "epoch:26 step:24629 [D loss: 0.659154, acc: 64.06%] [G loss: 1.869156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24630 [D loss: 0.673000, acc: 55.47%] [G loss: 1.770763]\n",
      "epoch:26 step:24631 [D loss: 0.655219, acc: 60.94%] [G loss: 1.876960]\n",
      "epoch:26 step:24632 [D loss: 0.601749, acc: 71.09%] [G loss: 1.907290]\n",
      "epoch:26 step:24633 [D loss: 0.690241, acc: 58.59%] [G loss: 1.785762]\n",
      "epoch:26 step:24634 [D loss: 0.647148, acc: 66.41%] [G loss: 1.834707]\n",
      "epoch:26 step:24635 [D loss: 0.629740, acc: 66.41%] [G loss: 1.879623]\n",
      "epoch:26 step:24636 [D loss: 0.627833, acc: 64.84%] [G loss: 2.102868]\n",
      "epoch:26 step:24637 [D loss: 0.640017, acc: 60.16%] [G loss: 1.978810]\n",
      "epoch:26 step:24638 [D loss: 0.614710, acc: 64.06%] [G loss: 2.105482]\n",
      "epoch:26 step:24639 [D loss: 0.641163, acc: 56.25%] [G loss: 1.902236]\n",
      "epoch:26 step:24640 [D loss: 0.668607, acc: 53.91%] [G loss: 1.824049]\n",
      "epoch:26 step:24641 [D loss: 0.647951, acc: 60.94%] [G loss: 1.774394]\n",
      "epoch:26 step:24642 [D loss: 0.601157, acc: 66.41%] [G loss: 1.843962]\n",
      "epoch:26 step:24643 [D loss: 0.621644, acc: 66.41%] [G loss: 1.888050]\n",
      "epoch:26 step:24644 [D loss: 0.626661, acc: 57.81%] [G loss: 1.994234]\n",
      "epoch:26 step:24645 [D loss: 0.591757, acc: 67.97%] [G loss: 1.905466]\n",
      "epoch:26 step:24646 [D loss: 0.638274, acc: 65.62%] [G loss: 1.927336]\n",
      "epoch:26 step:24647 [D loss: 0.682709, acc: 54.69%] [G loss: 1.949308]\n",
      "epoch:26 step:24648 [D loss: 0.636145, acc: 64.84%] [G loss: 1.913237]\n",
      "epoch:26 step:24649 [D loss: 0.694790, acc: 58.59%] [G loss: 1.921675]\n",
      "epoch:26 step:24650 [D loss: 0.667999, acc: 55.47%] [G loss: 1.843864]\n",
      "epoch:26 step:24651 [D loss: 0.598894, acc: 67.97%] [G loss: 1.883199]\n",
      "epoch:26 step:24652 [D loss: 0.681015, acc: 60.94%] [G loss: 1.942034]\n",
      "epoch:26 step:24653 [D loss: 0.672469, acc: 60.16%] [G loss: 1.836209]\n",
      "epoch:26 step:24654 [D loss: 0.645230, acc: 60.94%] [G loss: 1.957400]\n",
      "epoch:26 step:24655 [D loss: 0.603256, acc: 66.41%] [G loss: 1.973510]\n",
      "epoch:26 step:24656 [D loss: 0.689678, acc: 54.69%] [G loss: 1.891632]\n",
      "epoch:26 step:24657 [D loss: 0.647172, acc: 63.28%] [G loss: 1.828332]\n",
      "epoch:26 step:24658 [D loss: 0.694423, acc: 53.91%] [G loss: 1.796656]\n",
      "epoch:26 step:24659 [D loss: 0.624600, acc: 64.84%] [G loss: 1.867075]\n",
      "epoch:26 step:24660 [D loss: 0.626560, acc: 66.41%] [G loss: 1.892634]\n",
      "epoch:26 step:24661 [D loss: 0.625913, acc: 62.50%] [G loss: 1.999809]\n",
      "epoch:26 step:24662 [D loss: 0.608457, acc: 62.50%] [G loss: 1.959213]\n",
      "epoch:26 step:24663 [D loss: 0.701200, acc: 57.03%] [G loss: 1.866008]\n",
      "epoch:26 step:24664 [D loss: 0.589018, acc: 70.31%] [G loss: 1.989115]\n",
      "epoch:26 step:24665 [D loss: 0.642326, acc: 64.06%] [G loss: 1.905027]\n",
      "epoch:26 step:24666 [D loss: 0.711600, acc: 54.69%] [G loss: 1.815729]\n",
      "epoch:26 step:24667 [D loss: 0.689924, acc: 58.59%] [G loss: 1.985849]\n",
      "epoch:26 step:24668 [D loss: 0.716906, acc: 51.56%] [G loss: 1.748661]\n",
      "epoch:26 step:24669 [D loss: 0.672310, acc: 58.59%] [G loss: 1.777677]\n",
      "epoch:26 step:24670 [D loss: 0.684226, acc: 59.38%] [G loss: 1.666110]\n",
      "epoch:26 step:24671 [D loss: 0.619404, acc: 63.28%] [G loss: 1.821008]\n",
      "epoch:26 step:24672 [D loss: 0.663090, acc: 59.38%] [G loss: 1.865444]\n",
      "epoch:26 step:24673 [D loss: 0.646155, acc: 64.84%] [G loss: 1.855349]\n",
      "epoch:26 step:24674 [D loss: 0.655454, acc: 62.50%] [G loss: 1.978825]\n",
      "epoch:26 step:24675 [D loss: 0.582469, acc: 67.97%] [G loss: 1.917360]\n",
      "epoch:26 step:24676 [D loss: 0.581543, acc: 73.44%] [G loss: 2.046339]\n",
      "epoch:26 step:24677 [D loss: 0.599024, acc: 68.75%] [G loss: 2.214661]\n",
      "epoch:26 step:24678 [D loss: 0.673841, acc: 59.38%] [G loss: 1.791478]\n",
      "epoch:26 step:24679 [D loss: 0.667509, acc: 61.72%] [G loss: 1.935212]\n",
      "epoch:26 step:24680 [D loss: 0.636010, acc: 64.06%] [G loss: 1.907287]\n",
      "epoch:26 step:24681 [D loss: 0.663271, acc: 57.03%] [G loss: 1.630735]\n",
      "epoch:26 step:24682 [D loss: 0.683722, acc: 60.16%] [G loss: 1.884451]\n",
      "epoch:26 step:24683 [D loss: 0.666564, acc: 59.38%] [G loss: 1.976741]\n",
      "epoch:26 step:24684 [D loss: 0.657739, acc: 56.25%] [G loss: 1.809449]\n",
      "epoch:26 step:24685 [D loss: 0.667115, acc: 66.41%] [G loss: 1.832341]\n",
      "epoch:26 step:24686 [D loss: 0.620439, acc: 59.38%] [G loss: 1.961628]\n",
      "epoch:26 step:24687 [D loss: 0.617297, acc: 64.84%] [G loss: 1.850224]\n",
      "epoch:26 step:24688 [D loss: 0.589675, acc: 65.62%] [G loss: 1.821915]\n",
      "epoch:26 step:24689 [D loss: 0.647835, acc: 59.38%] [G loss: 1.924959]\n",
      "epoch:26 step:24690 [D loss: 0.687670, acc: 60.94%] [G loss: 1.993861]\n",
      "epoch:26 step:24691 [D loss: 0.669373, acc: 64.84%] [G loss: 1.842401]\n",
      "epoch:26 step:24692 [D loss: 0.662388, acc: 64.06%] [G loss: 1.965546]\n",
      "epoch:26 step:24693 [D loss: 0.601541, acc: 72.66%] [G loss: 1.967417]\n",
      "epoch:26 step:24694 [D loss: 0.624301, acc: 67.19%] [G loss: 1.970197]\n",
      "epoch:26 step:24695 [D loss: 0.639634, acc: 63.28%] [G loss: 2.004406]\n",
      "epoch:26 step:24696 [D loss: 0.633483, acc: 57.03%] [G loss: 2.052725]\n",
      "epoch:26 step:24697 [D loss: 0.711140, acc: 57.81%] [G loss: 1.875923]\n",
      "epoch:26 step:24698 [D loss: 0.640390, acc: 60.94%] [G loss: 1.901588]\n",
      "epoch:26 step:24699 [D loss: 0.619648, acc: 74.22%] [G loss: 1.866733]\n",
      "epoch:26 step:24700 [D loss: 0.606176, acc: 69.53%] [G loss: 1.875461]\n",
      "epoch:26 step:24701 [D loss: 0.619984, acc: 64.06%] [G loss: 1.892224]\n",
      "epoch:26 step:24702 [D loss: 0.581009, acc: 71.09%] [G loss: 2.034215]\n",
      "epoch:26 step:24703 [D loss: 0.733362, acc: 50.78%] [G loss: 1.843231]\n",
      "epoch:26 step:24704 [D loss: 0.688336, acc: 54.69%] [G loss: 1.852745]\n",
      "epoch:26 step:24705 [D loss: 0.675525, acc: 53.12%] [G loss: 1.748057]\n",
      "epoch:26 step:24706 [D loss: 0.642339, acc: 58.59%] [G loss: 1.811649]\n",
      "epoch:26 step:24707 [D loss: 0.607794, acc: 65.62%] [G loss: 2.043532]\n",
      "epoch:26 step:24708 [D loss: 0.635340, acc: 62.50%] [G loss: 2.229742]\n",
      "epoch:26 step:24709 [D loss: 0.619748, acc: 65.62%] [G loss: 2.153590]\n",
      "epoch:26 step:24710 [D loss: 0.655899, acc: 60.94%] [G loss: 1.830003]\n",
      "epoch:26 step:24711 [D loss: 0.685801, acc: 59.38%] [G loss: 1.677433]\n",
      "epoch:26 step:24712 [D loss: 0.639057, acc: 65.62%] [G loss: 1.953813]\n",
      "epoch:26 step:24713 [D loss: 0.629018, acc: 64.84%] [G loss: 1.713611]\n",
      "epoch:26 step:24714 [D loss: 0.707987, acc: 56.25%] [G loss: 1.876163]\n",
      "epoch:26 step:24715 [D loss: 0.640074, acc: 62.50%] [G loss: 1.980018]\n",
      "epoch:26 step:24716 [D loss: 0.594500, acc: 69.53%] [G loss: 2.068763]\n",
      "epoch:26 step:24717 [D loss: 0.703225, acc: 58.59%] [G loss: 1.892168]\n",
      "epoch:26 step:24718 [D loss: 0.671674, acc: 59.38%] [G loss: 1.838805]\n",
      "epoch:26 step:24719 [D loss: 0.657462, acc: 63.28%] [G loss: 1.723741]\n",
      "epoch:26 step:24720 [D loss: 0.621230, acc: 63.28%] [G loss: 1.920597]\n",
      "epoch:26 step:24721 [D loss: 0.654767, acc: 57.81%] [G loss: 1.933562]\n",
      "epoch:26 step:24722 [D loss: 0.668591, acc: 62.50%] [G loss: 1.891615]\n",
      "epoch:26 step:24723 [D loss: 0.670728, acc: 59.38%] [G loss: 1.726751]\n",
      "epoch:26 step:24724 [D loss: 0.629976, acc: 64.06%] [G loss: 1.882638]\n",
      "epoch:26 step:24725 [D loss: 0.673975, acc: 57.81%] [G loss: 1.905104]\n",
      "epoch:26 step:24726 [D loss: 0.639721, acc: 70.31%] [G loss: 1.821300]\n",
      "epoch:26 step:24727 [D loss: 0.680143, acc: 53.91%] [G loss: 1.761395]\n",
      "epoch:26 step:24728 [D loss: 0.683942, acc: 60.94%] [G loss: 1.937368]\n",
      "epoch:26 step:24729 [D loss: 0.621631, acc: 67.97%] [G loss: 1.919213]\n",
      "epoch:26 step:24730 [D loss: 0.682362, acc: 60.94%] [G loss: 1.803190]\n",
      "epoch:26 step:24731 [D loss: 0.645283, acc: 64.84%] [G loss: 1.832557]\n",
      "epoch:26 step:24732 [D loss: 0.639158, acc: 64.06%] [G loss: 1.926555]\n",
      "epoch:26 step:24733 [D loss: 0.616672, acc: 69.53%] [G loss: 1.994860]\n",
      "epoch:26 step:24734 [D loss: 0.598117, acc: 66.41%] [G loss: 1.763179]\n",
      "epoch:26 step:24735 [D loss: 0.670677, acc: 54.69%] [G loss: 1.711457]\n",
      "epoch:26 step:24736 [D loss: 0.625294, acc: 64.06%] [G loss: 1.881276]\n",
      "epoch:26 step:24737 [D loss: 0.722417, acc: 52.34%] [G loss: 1.719159]\n",
      "epoch:26 step:24738 [D loss: 0.628834, acc: 62.50%] [G loss: 1.951963]\n",
      "epoch:26 step:24739 [D loss: 0.681540, acc: 53.91%] [G loss: 1.695372]\n",
      "epoch:26 step:24740 [D loss: 0.641997, acc: 61.72%] [G loss: 1.806992]\n",
      "epoch:26 step:24741 [D loss: 0.637155, acc: 62.50%] [G loss: 2.010634]\n",
      "epoch:26 step:24742 [D loss: 0.674001, acc: 57.81%] [G loss: 1.923589]\n",
      "epoch:26 step:24743 [D loss: 0.599451, acc: 70.31%] [G loss: 2.003237]\n",
      "epoch:26 step:24744 [D loss: 0.655842, acc: 57.03%] [G loss: 1.840935]\n",
      "epoch:26 step:24745 [D loss: 0.634014, acc: 57.81%] [G loss: 1.825830]\n",
      "epoch:26 step:24746 [D loss: 0.618836, acc: 64.84%] [G loss: 1.865464]\n",
      "epoch:26 step:24747 [D loss: 0.636035, acc: 58.59%] [G loss: 1.907500]\n",
      "epoch:26 step:24748 [D loss: 0.670350, acc: 55.47%] [G loss: 1.745417]\n",
      "epoch:26 step:24749 [D loss: 0.646728, acc: 64.06%] [G loss: 1.667727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24750 [D loss: 0.643113, acc: 63.28%] [G loss: 1.826220]\n",
      "epoch:26 step:24751 [D loss: 0.657178, acc: 60.16%] [G loss: 1.765719]\n",
      "epoch:26 step:24752 [D loss: 0.611138, acc: 63.28%] [G loss: 1.956890]\n",
      "epoch:26 step:24753 [D loss: 0.665387, acc: 60.94%] [G loss: 1.909385]\n",
      "epoch:26 step:24754 [D loss: 0.631287, acc: 65.62%] [G loss: 1.897260]\n",
      "epoch:26 step:24755 [D loss: 0.651948, acc: 60.94%] [G loss: 1.806017]\n",
      "epoch:26 step:24756 [D loss: 0.691929, acc: 60.16%] [G loss: 1.795422]\n",
      "epoch:26 step:24757 [D loss: 0.631536, acc: 62.50%] [G loss: 1.878797]\n",
      "epoch:26 step:24758 [D loss: 0.674748, acc: 56.25%] [G loss: 1.912707]\n",
      "epoch:26 step:24759 [D loss: 0.666089, acc: 56.25%] [G loss: 1.774418]\n",
      "epoch:26 step:24760 [D loss: 0.635981, acc: 64.06%] [G loss: 1.848546]\n",
      "epoch:26 step:24761 [D loss: 0.696880, acc: 57.81%] [G loss: 1.949755]\n",
      "epoch:26 step:24762 [D loss: 0.668903, acc: 59.38%] [G loss: 1.868744]\n",
      "epoch:26 step:24763 [D loss: 0.656811, acc: 60.16%] [G loss: 1.854883]\n",
      "epoch:26 step:24764 [D loss: 0.670720, acc: 58.59%] [G loss: 1.944081]\n",
      "epoch:26 step:24765 [D loss: 0.651217, acc: 65.62%] [G loss: 1.878224]\n",
      "epoch:26 step:24766 [D loss: 0.649512, acc: 59.38%] [G loss: 2.018272]\n",
      "epoch:26 step:24767 [D loss: 0.676973, acc: 60.94%] [G loss: 1.998471]\n",
      "epoch:26 step:24768 [D loss: 0.626282, acc: 60.94%] [G loss: 2.057768]\n",
      "epoch:26 step:24769 [D loss: 0.661056, acc: 61.72%] [G loss: 1.884185]\n",
      "epoch:26 step:24770 [D loss: 0.680102, acc: 53.12%] [G loss: 1.723671]\n",
      "epoch:26 step:24771 [D loss: 0.708914, acc: 53.12%] [G loss: 1.817837]\n",
      "epoch:26 step:24772 [D loss: 0.646456, acc: 62.50%] [G loss: 1.821522]\n",
      "epoch:26 step:24773 [D loss: 0.699567, acc: 57.81%] [G loss: 1.843382]\n",
      "epoch:26 step:24774 [D loss: 0.603518, acc: 65.62%] [G loss: 1.992376]\n",
      "epoch:26 step:24775 [D loss: 0.668076, acc: 60.16%] [G loss: 1.944667]\n",
      "epoch:26 step:24776 [D loss: 0.616189, acc: 68.75%] [G loss: 2.170452]\n",
      "epoch:26 step:24777 [D loss: 0.671202, acc: 61.72%] [G loss: 1.934778]\n",
      "epoch:26 step:24778 [D loss: 0.602877, acc: 63.28%] [G loss: 1.864106]\n",
      "epoch:26 step:24779 [D loss: 0.638232, acc: 68.75%] [G loss: 1.948559]\n",
      "epoch:26 step:24780 [D loss: 0.700687, acc: 51.56%] [G loss: 1.693926]\n",
      "epoch:26 step:24781 [D loss: 0.687839, acc: 53.12%] [G loss: 1.834985]\n",
      "epoch:26 step:24782 [D loss: 0.650877, acc: 62.50%] [G loss: 1.798925]\n",
      "epoch:26 step:24783 [D loss: 0.636110, acc: 62.50%] [G loss: 1.958652]\n",
      "epoch:26 step:24784 [D loss: 0.656885, acc: 64.84%] [G loss: 1.813319]\n",
      "epoch:26 step:24785 [D loss: 0.666735, acc: 64.06%] [G loss: 1.837839]\n",
      "epoch:26 step:24786 [D loss: 0.662092, acc: 56.25%] [G loss: 1.749854]\n",
      "epoch:26 step:24787 [D loss: 0.702578, acc: 56.25%] [G loss: 1.852268]\n",
      "epoch:26 step:24788 [D loss: 0.666861, acc: 60.94%] [G loss: 1.868549]\n",
      "epoch:26 step:24789 [D loss: 0.644470, acc: 63.28%] [G loss: 1.833428]\n",
      "epoch:26 step:24790 [D loss: 0.612355, acc: 69.53%] [G loss: 2.126969]\n",
      "epoch:26 step:24791 [D loss: 0.568014, acc: 71.09%] [G loss: 2.035559]\n",
      "epoch:26 step:24792 [D loss: 0.611958, acc: 65.62%] [G loss: 2.185350]\n",
      "epoch:26 step:24793 [D loss: 0.638891, acc: 61.72%] [G loss: 1.926064]\n",
      "epoch:26 step:24794 [D loss: 0.737279, acc: 54.69%] [G loss: 1.831915]\n",
      "epoch:26 step:24795 [D loss: 0.676732, acc: 54.69%] [G loss: 1.929113]\n",
      "epoch:26 step:24796 [D loss: 0.679096, acc: 58.59%] [G loss: 1.869938]\n",
      "epoch:26 step:24797 [D loss: 0.636860, acc: 57.03%] [G loss: 1.903939]\n",
      "epoch:26 step:24798 [D loss: 0.654048, acc: 61.72%] [G loss: 1.959629]\n",
      "epoch:26 step:24799 [D loss: 0.746470, acc: 42.19%] [G loss: 1.638873]\n",
      "epoch:26 step:24800 [D loss: 0.674154, acc: 57.03%] [G loss: 1.712193]\n",
      "##############\n",
      "[2.52800209 1.49865381 6.20905771 4.59180183 3.43883137 5.48672902\n",
      " 4.34848323 4.63748401 4.52028016 3.57748425]\n",
      "##########\n",
      "epoch:26 step:24801 [D loss: 0.613098, acc: 64.84%] [G loss: 1.760525]\n",
      "epoch:26 step:24802 [D loss: 0.663965, acc: 64.06%] [G loss: 1.787496]\n",
      "epoch:26 step:24803 [D loss: 0.662066, acc: 63.28%] [G loss: 1.752914]\n",
      "epoch:26 step:24804 [D loss: 0.658556, acc: 60.16%] [G loss: 1.735318]\n",
      "epoch:26 step:24805 [D loss: 0.650249, acc: 64.84%] [G loss: 1.830146]\n",
      "epoch:26 step:24806 [D loss: 0.713653, acc: 53.91%] [G loss: 1.839916]\n",
      "epoch:26 step:24807 [D loss: 0.661860, acc: 67.19%] [G loss: 1.791796]\n",
      "epoch:26 step:24808 [D loss: 0.705119, acc: 54.69%] [G loss: 1.742390]\n",
      "epoch:26 step:24809 [D loss: 0.651851, acc: 65.62%] [G loss: 1.773421]\n",
      "epoch:26 step:24810 [D loss: 0.665580, acc: 57.03%] [G loss: 1.795134]\n",
      "epoch:26 step:24811 [D loss: 0.647405, acc: 65.62%] [G loss: 1.766972]\n",
      "epoch:26 step:24812 [D loss: 0.623436, acc: 66.41%] [G loss: 1.771380]\n",
      "epoch:26 step:24813 [D loss: 0.630465, acc: 63.28%] [G loss: 1.810902]\n",
      "epoch:26 step:24814 [D loss: 0.644073, acc: 66.41%] [G loss: 1.860979]\n",
      "epoch:26 step:24815 [D loss: 0.649058, acc: 60.94%] [G loss: 1.830706]\n",
      "epoch:26 step:24816 [D loss: 0.634174, acc: 61.72%] [G loss: 1.977133]\n",
      "epoch:26 step:24817 [D loss: 0.662114, acc: 62.50%] [G loss: 1.780671]\n",
      "epoch:26 step:24818 [D loss: 0.650655, acc: 60.94%] [G loss: 1.847440]\n",
      "epoch:26 step:24819 [D loss: 0.631657, acc: 67.97%] [G loss: 1.909367]\n",
      "epoch:26 step:24820 [D loss: 0.698083, acc: 59.38%] [G loss: 1.752410]\n",
      "epoch:26 step:24821 [D loss: 0.637146, acc: 64.06%] [G loss: 1.761247]\n",
      "epoch:26 step:24822 [D loss: 0.709570, acc: 50.78%] [G loss: 1.711099]\n",
      "epoch:26 step:24823 [D loss: 0.644741, acc: 64.06%] [G loss: 1.638063]\n",
      "epoch:26 step:24824 [D loss: 0.642970, acc: 60.16%] [G loss: 1.765401]\n",
      "epoch:26 step:24825 [D loss: 0.679346, acc: 58.59%] [G loss: 1.843449]\n",
      "epoch:26 step:24826 [D loss: 0.696490, acc: 57.81%] [G loss: 1.733875]\n",
      "epoch:26 step:24827 [D loss: 0.678948, acc: 57.03%] [G loss: 1.895963]\n",
      "epoch:26 step:24828 [D loss: 0.632282, acc: 67.19%] [G loss: 1.864011]\n",
      "epoch:26 step:24829 [D loss: 0.640310, acc: 62.50%] [G loss: 1.846145]\n",
      "epoch:26 step:24830 [D loss: 0.650884, acc: 65.62%] [G loss: 1.944713]\n",
      "epoch:26 step:24831 [D loss: 0.637010, acc: 61.72%] [G loss: 2.074795]\n",
      "epoch:26 step:24832 [D loss: 0.637986, acc: 59.38%] [G loss: 1.961384]\n",
      "epoch:26 step:24833 [D loss: 0.622971, acc: 66.41%] [G loss: 2.009973]\n",
      "epoch:26 step:24834 [D loss: 0.649324, acc: 67.97%] [G loss: 2.060746]\n",
      "epoch:26 step:24835 [D loss: 0.712548, acc: 55.47%] [G loss: 1.767162]\n",
      "epoch:26 step:24836 [D loss: 0.629167, acc: 70.31%] [G loss: 1.813014]\n",
      "epoch:26 step:24837 [D loss: 0.718294, acc: 54.69%] [G loss: 1.742674]\n",
      "epoch:26 step:24838 [D loss: 0.650814, acc: 57.81%] [G loss: 1.842504]\n",
      "epoch:26 step:24839 [D loss: 0.679505, acc: 60.94%] [G loss: 1.815807]\n",
      "epoch:26 step:24840 [D loss: 0.650528, acc: 60.94%] [G loss: 1.896912]\n",
      "epoch:26 step:24841 [D loss: 0.623129, acc: 67.97%] [G loss: 1.874659]\n",
      "epoch:26 step:24842 [D loss: 0.630332, acc: 62.50%] [G loss: 2.054287]\n",
      "epoch:26 step:24843 [D loss: 0.621652, acc: 60.16%] [G loss: 2.009290]\n",
      "epoch:26 step:24844 [D loss: 0.678389, acc: 56.25%] [G loss: 1.696695]\n",
      "epoch:26 step:24845 [D loss: 0.651481, acc: 57.81%] [G loss: 1.697021]\n",
      "epoch:26 step:24846 [D loss: 0.648145, acc: 62.50%] [G loss: 1.862312]\n",
      "epoch:26 step:24847 [D loss: 0.687596, acc: 55.47%] [G loss: 1.797559]\n",
      "epoch:26 step:24848 [D loss: 0.629032, acc: 67.97%] [G loss: 1.849490]\n",
      "epoch:26 step:24849 [D loss: 0.680848, acc: 60.16%] [G loss: 1.891112]\n",
      "epoch:26 step:24850 [D loss: 0.668295, acc: 60.94%] [G loss: 1.969279]\n",
      "epoch:26 step:24851 [D loss: 0.658411, acc: 58.59%] [G loss: 1.909258]\n",
      "epoch:26 step:24852 [D loss: 0.651242, acc: 65.62%] [G loss: 1.724369]\n",
      "epoch:26 step:24853 [D loss: 0.707675, acc: 52.34%] [G loss: 1.794922]\n",
      "epoch:26 step:24854 [D loss: 0.669905, acc: 57.81%] [G loss: 1.800180]\n",
      "epoch:26 step:24855 [D loss: 0.654862, acc: 62.50%] [G loss: 1.716414]\n",
      "epoch:26 step:24856 [D loss: 0.636098, acc: 63.28%] [G loss: 1.877504]\n",
      "epoch:26 step:24857 [D loss: 0.589471, acc: 69.53%] [G loss: 1.942636]\n",
      "epoch:26 step:24858 [D loss: 0.659802, acc: 58.59%] [G loss: 1.809416]\n",
      "epoch:26 step:24859 [D loss: 0.581667, acc: 66.41%] [G loss: 1.941241]\n",
      "epoch:26 step:24860 [D loss: 0.612923, acc: 67.19%] [G loss: 1.996336]\n",
      "epoch:26 step:24861 [D loss: 0.618739, acc: 67.19%] [G loss: 2.125125]\n",
      "epoch:26 step:24862 [D loss: 0.667824, acc: 61.72%] [G loss: 1.816536]\n",
      "epoch:26 step:24863 [D loss: 0.675234, acc: 58.59%] [G loss: 1.793849]\n",
      "epoch:26 step:24864 [D loss: 0.706487, acc: 49.22%] [G loss: 1.769803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24865 [D loss: 0.666688, acc: 58.59%] [G loss: 1.848973]\n",
      "epoch:26 step:24866 [D loss: 0.618682, acc: 68.75%] [G loss: 2.015927]\n",
      "epoch:26 step:24867 [D loss: 0.640127, acc: 61.72%] [G loss: 1.786816]\n",
      "epoch:26 step:24868 [D loss: 0.646962, acc: 64.06%] [G loss: 1.678677]\n",
      "epoch:26 step:24869 [D loss: 0.668596, acc: 63.28%] [G loss: 1.794423]\n",
      "epoch:26 step:24870 [D loss: 0.660048, acc: 56.25%] [G loss: 1.906537]\n",
      "epoch:26 step:24871 [D loss: 0.653806, acc: 62.50%] [G loss: 1.802032]\n",
      "epoch:26 step:24872 [D loss: 0.673857, acc: 56.25%] [G loss: 1.834901]\n",
      "epoch:26 step:24873 [D loss: 0.695875, acc: 53.91%] [G loss: 1.751065]\n",
      "epoch:26 step:24874 [D loss: 0.632111, acc: 66.41%] [G loss: 1.859869]\n",
      "epoch:26 step:24875 [D loss: 0.662637, acc: 60.94%] [G loss: 1.746083]\n",
      "epoch:26 step:24876 [D loss: 0.644780, acc: 71.09%] [G loss: 1.815120]\n",
      "epoch:26 step:24877 [D loss: 0.611697, acc: 67.19%] [G loss: 1.766359]\n",
      "epoch:26 step:24878 [D loss: 0.621839, acc: 61.72%] [G loss: 1.851357]\n",
      "epoch:26 step:24879 [D loss: 0.668932, acc: 57.03%] [G loss: 1.904137]\n",
      "epoch:26 step:24880 [D loss: 0.692297, acc: 54.69%] [G loss: 1.619612]\n",
      "epoch:26 step:24881 [D loss: 0.664593, acc: 60.94%] [G loss: 1.885738]\n",
      "epoch:26 step:24882 [D loss: 0.635482, acc: 60.16%] [G loss: 1.859749]\n",
      "epoch:26 step:24883 [D loss: 0.642944, acc: 59.38%] [G loss: 1.865341]\n",
      "epoch:26 step:24884 [D loss: 0.595871, acc: 68.75%] [G loss: 2.022473]\n",
      "epoch:26 step:24885 [D loss: 0.600625, acc: 71.88%] [G loss: 2.094137]\n",
      "epoch:26 step:24886 [D loss: 0.679783, acc: 60.16%] [G loss: 1.926018]\n",
      "epoch:26 step:24887 [D loss: 0.662858, acc: 56.25%] [G loss: 1.902623]\n",
      "epoch:26 step:24888 [D loss: 0.608138, acc: 67.97%] [G loss: 1.998627]\n",
      "epoch:26 step:24889 [D loss: 0.626759, acc: 64.84%] [G loss: 1.860644]\n",
      "epoch:26 step:24890 [D loss: 0.663977, acc: 56.25%] [G loss: 1.767864]\n",
      "epoch:26 step:24891 [D loss: 0.661433, acc: 61.72%] [G loss: 1.788802]\n",
      "epoch:26 step:24892 [D loss: 0.709130, acc: 53.12%] [G loss: 1.666479]\n",
      "epoch:26 step:24893 [D loss: 0.685726, acc: 60.16%] [G loss: 1.736586]\n",
      "epoch:26 step:24894 [D loss: 0.724308, acc: 55.47%] [G loss: 1.862177]\n",
      "epoch:26 step:24895 [D loss: 0.640164, acc: 60.94%] [G loss: 1.877660]\n",
      "epoch:26 step:24896 [D loss: 0.656152, acc: 62.50%] [G loss: 1.851537]\n",
      "epoch:26 step:24897 [D loss: 0.658548, acc: 60.16%] [G loss: 1.766119]\n",
      "epoch:26 step:24898 [D loss: 0.636461, acc: 64.06%] [G loss: 1.796681]\n",
      "epoch:26 step:24899 [D loss: 0.671914, acc: 58.59%] [G loss: 1.853358]\n",
      "epoch:26 step:24900 [D loss: 0.676347, acc: 58.59%] [G loss: 1.733500]\n",
      "epoch:26 step:24901 [D loss: 0.640002, acc: 60.94%] [G loss: 1.896117]\n",
      "epoch:26 step:24902 [D loss: 0.639621, acc: 62.50%] [G loss: 1.820673]\n",
      "epoch:26 step:24903 [D loss: 0.666086, acc: 63.28%] [G loss: 1.811459]\n",
      "epoch:26 step:24904 [D loss: 0.711267, acc: 53.91%] [G loss: 1.754762]\n",
      "epoch:26 step:24905 [D loss: 0.672835, acc: 60.94%] [G loss: 1.813087]\n",
      "epoch:26 step:24906 [D loss: 0.660845, acc: 55.47%] [G loss: 1.869428]\n",
      "epoch:26 step:24907 [D loss: 0.640251, acc: 63.28%] [G loss: 1.852605]\n",
      "epoch:26 step:24908 [D loss: 0.644355, acc: 66.41%] [G loss: 1.792700]\n",
      "epoch:26 step:24909 [D loss: 0.678225, acc: 60.16%] [G loss: 1.792418]\n",
      "epoch:26 step:24910 [D loss: 0.638445, acc: 67.19%] [G loss: 1.911653]\n",
      "epoch:26 step:24911 [D loss: 0.576706, acc: 74.22%] [G loss: 1.879827]\n",
      "epoch:26 step:24912 [D loss: 0.657154, acc: 58.59%] [G loss: 1.979218]\n",
      "epoch:26 step:24913 [D loss: 0.604488, acc: 67.19%] [G loss: 2.040116]\n",
      "epoch:26 step:24914 [D loss: 0.616521, acc: 67.19%] [G loss: 1.933072]\n",
      "epoch:26 step:24915 [D loss: 0.646639, acc: 59.38%] [G loss: 1.800484]\n",
      "epoch:26 step:24916 [D loss: 0.594354, acc: 68.75%] [G loss: 2.006070]\n",
      "epoch:26 step:24917 [D loss: 0.641250, acc: 61.72%] [G loss: 1.881704]\n",
      "epoch:26 step:24918 [D loss: 0.618297, acc: 66.41%] [G loss: 1.849903]\n",
      "epoch:26 step:24919 [D loss: 0.599357, acc: 67.97%] [G loss: 1.978606]\n",
      "epoch:26 step:24920 [D loss: 0.606621, acc: 64.06%] [G loss: 2.030006]\n",
      "epoch:26 step:24921 [D loss: 0.650763, acc: 62.50%] [G loss: 1.852483]\n",
      "epoch:26 step:24922 [D loss: 0.640265, acc: 70.31%] [G loss: 1.991518]\n",
      "epoch:26 step:24923 [D loss: 0.659193, acc: 60.16%] [G loss: 1.784784]\n",
      "epoch:26 step:24924 [D loss: 0.658936, acc: 68.75%] [G loss: 2.053975]\n",
      "epoch:26 step:24925 [D loss: 0.641697, acc: 60.94%] [G loss: 1.917508]\n",
      "epoch:26 step:24926 [D loss: 0.578254, acc: 72.66%] [G loss: 2.106866]\n",
      "epoch:26 step:24927 [D loss: 0.648454, acc: 64.84%] [G loss: 1.746093]\n",
      "epoch:26 step:24928 [D loss: 0.719751, acc: 53.12%] [G loss: 1.735961]\n",
      "epoch:26 step:24929 [D loss: 0.674372, acc: 53.91%] [G loss: 1.804764]\n",
      "epoch:26 step:24930 [D loss: 0.687507, acc: 58.59%] [G loss: 1.779565]\n",
      "epoch:26 step:24931 [D loss: 0.625874, acc: 64.84%] [G loss: 1.824239]\n",
      "epoch:26 step:24932 [D loss: 0.615438, acc: 67.19%] [G loss: 1.917425]\n",
      "epoch:26 step:24933 [D loss: 0.633849, acc: 65.62%] [G loss: 1.899623]\n",
      "epoch:26 step:24934 [D loss: 0.648760, acc: 61.72%] [G loss: 1.706935]\n",
      "epoch:26 step:24935 [D loss: 0.664863, acc: 57.03%] [G loss: 1.717701]\n",
      "epoch:26 step:24936 [D loss: 0.675584, acc: 55.47%] [G loss: 1.886235]\n",
      "epoch:26 step:24937 [D loss: 0.659031, acc: 55.47%] [G loss: 1.914918]\n",
      "epoch:26 step:24938 [D loss: 0.638361, acc: 62.50%] [G loss: 1.804591]\n",
      "epoch:26 step:24939 [D loss: 0.668479, acc: 56.25%] [G loss: 1.826581]\n",
      "epoch:26 step:24940 [D loss: 0.704541, acc: 56.25%] [G loss: 1.813147]\n",
      "epoch:26 step:24941 [D loss: 0.627550, acc: 67.19%] [G loss: 1.763115]\n",
      "epoch:26 step:24942 [D loss: 0.691916, acc: 60.16%] [G loss: 1.724721]\n",
      "epoch:26 step:24943 [D loss: 0.639116, acc: 61.72%] [G loss: 1.822883]\n",
      "epoch:26 step:24944 [D loss: 0.559680, acc: 76.56%] [G loss: 1.846503]\n",
      "epoch:26 step:24945 [D loss: 0.652389, acc: 56.25%] [G loss: 1.928228]\n",
      "epoch:26 step:24946 [D loss: 0.734796, acc: 56.25%] [G loss: 1.720837]\n",
      "epoch:26 step:24947 [D loss: 0.635933, acc: 64.06%] [G loss: 1.975558]\n",
      "epoch:26 step:24948 [D loss: 0.638618, acc: 62.50%] [G loss: 1.840709]\n",
      "epoch:26 step:24949 [D loss: 0.636978, acc: 62.50%] [G loss: 1.910076]\n",
      "epoch:26 step:24950 [D loss: 0.618161, acc: 61.72%] [G loss: 1.974913]\n",
      "epoch:26 step:24951 [D loss: 0.674292, acc: 54.69%] [G loss: 1.879812]\n",
      "epoch:26 step:24952 [D loss: 0.613123, acc: 67.97%] [G loss: 1.915963]\n",
      "epoch:26 step:24953 [D loss: 0.644191, acc: 59.38%] [G loss: 1.956042]\n",
      "epoch:26 step:24954 [D loss: 0.654411, acc: 60.16%] [G loss: 1.785992]\n",
      "epoch:26 step:24955 [D loss: 0.620687, acc: 65.62%] [G loss: 1.970185]\n",
      "epoch:26 step:24956 [D loss: 0.700065, acc: 54.69%] [G loss: 1.914884]\n",
      "epoch:26 step:24957 [D loss: 0.625917, acc: 64.06%] [G loss: 1.893323]\n",
      "epoch:26 step:24958 [D loss: 0.731321, acc: 53.91%] [G loss: 1.847474]\n",
      "epoch:26 step:24959 [D loss: 0.678961, acc: 59.38%] [G loss: 1.790831]\n",
      "epoch:26 step:24960 [D loss: 0.642023, acc: 62.50%] [G loss: 1.865208]\n",
      "epoch:26 step:24961 [D loss: 0.664188, acc: 59.38%] [G loss: 1.788781]\n",
      "epoch:26 step:24962 [D loss: 0.644475, acc: 59.38%] [G loss: 1.933938]\n",
      "epoch:26 step:24963 [D loss: 0.658850, acc: 61.72%] [G loss: 1.820986]\n",
      "epoch:26 step:24964 [D loss: 0.646261, acc: 62.50%] [G loss: 1.742666]\n",
      "epoch:26 step:24965 [D loss: 0.615693, acc: 64.06%] [G loss: 1.971097]\n",
      "epoch:26 step:24966 [D loss: 0.658741, acc: 58.59%] [G loss: 1.918821]\n",
      "epoch:26 step:24967 [D loss: 0.620654, acc: 66.41%] [G loss: 1.846760]\n",
      "epoch:26 step:24968 [D loss: 0.666843, acc: 57.81%] [G loss: 1.766512]\n",
      "epoch:26 step:24969 [D loss: 0.626950, acc: 64.84%] [G loss: 1.888110]\n",
      "epoch:26 step:24970 [D loss: 0.686659, acc: 54.69%] [G loss: 1.767487]\n",
      "epoch:26 step:24971 [D loss: 0.631541, acc: 66.41%] [G loss: 1.900533]\n",
      "epoch:26 step:24972 [D loss: 0.665488, acc: 61.72%] [G loss: 1.770660]\n",
      "epoch:26 step:24973 [D loss: 0.607076, acc: 69.53%] [G loss: 1.803641]\n",
      "epoch:26 step:24974 [D loss: 0.687580, acc: 57.03%] [G loss: 1.836961]\n",
      "epoch:26 step:24975 [D loss: 0.627012, acc: 67.19%] [G loss: 1.854107]\n",
      "epoch:26 step:24976 [D loss: 0.677250, acc: 54.69%] [G loss: 1.786051]\n",
      "epoch:26 step:24977 [D loss: 0.714359, acc: 56.25%] [G loss: 1.653969]\n",
      "epoch:26 step:24978 [D loss: 0.657393, acc: 58.59%] [G loss: 1.816689]\n",
      "epoch:26 step:24979 [D loss: 0.647905, acc: 62.50%] [G loss: 1.753116]\n",
      "epoch:26 step:24980 [D loss: 0.636821, acc: 61.72%] [G loss: 1.866251]\n",
      "epoch:26 step:24981 [D loss: 0.634954, acc: 63.28%] [G loss: 1.819801]\n",
      "epoch:26 step:24982 [D loss: 0.603168, acc: 71.88%] [G loss: 1.816678]\n",
      "epoch:26 step:24983 [D loss: 0.641711, acc: 65.62%] [G loss: 1.762064]\n",
      "epoch:26 step:24984 [D loss: 0.692934, acc: 63.28%] [G loss: 1.912966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24985 [D loss: 0.624476, acc: 62.50%] [G loss: 1.889646]\n",
      "epoch:26 step:24986 [D loss: 0.585302, acc: 72.66%] [G loss: 1.916553]\n",
      "epoch:26 step:24987 [D loss: 0.625031, acc: 66.41%] [G loss: 1.798857]\n",
      "epoch:26 step:24988 [D loss: 0.679770, acc: 59.38%] [G loss: 1.877420]\n",
      "epoch:26 step:24989 [D loss: 0.643407, acc: 66.41%] [G loss: 1.827947]\n",
      "epoch:26 step:24990 [D loss: 0.638517, acc: 64.84%] [G loss: 1.716689]\n",
      "epoch:26 step:24991 [D loss: 0.698188, acc: 55.47%] [G loss: 1.806712]\n",
      "epoch:26 step:24992 [D loss: 0.639302, acc: 60.94%] [G loss: 2.030667]\n",
      "epoch:26 step:24993 [D loss: 0.606284, acc: 65.62%] [G loss: 2.043184]\n",
      "epoch:26 step:24994 [D loss: 0.647198, acc: 65.62%] [G loss: 1.958783]\n",
      "epoch:26 step:24995 [D loss: 0.652565, acc: 64.06%] [G loss: 2.020292]\n",
      "epoch:26 step:24996 [D loss: 0.619624, acc: 63.28%] [G loss: 1.869288]\n",
      "epoch:26 step:24997 [D loss: 0.639982, acc: 62.50%] [G loss: 1.956145]\n",
      "epoch:26 step:24998 [D loss: 0.584553, acc: 67.19%] [G loss: 1.971794]\n",
      "epoch:26 step:24999 [D loss: 0.588049, acc: 71.88%] [G loss: 2.116908]\n",
      "epoch:26 step:25000 [D loss: 0.631804, acc: 69.53%] [G loss: 1.891142]\n",
      "##############\n",
      "[2.57187569 1.52010512 5.95569441 4.91504896 3.59923115 5.52065544\n",
      " 4.39495536 4.61341671 4.40525088 3.65516433]\n",
      "##########\n",
      "epoch:26 step:25001 [D loss: 0.636927, acc: 62.50%] [G loss: 1.904138]\n",
      "epoch:26 step:25002 [D loss: 0.681965, acc: 62.50%] [G loss: 1.844971]\n",
      "epoch:26 step:25003 [D loss: 0.608024, acc: 65.62%] [G loss: 2.005592]\n",
      "epoch:26 step:25004 [D loss: 0.583005, acc: 72.66%] [G loss: 2.081854]\n",
      "epoch:26 step:25005 [D loss: 0.652154, acc: 57.81%] [G loss: 1.854914]\n",
      "epoch:26 step:25006 [D loss: 0.708410, acc: 54.69%] [G loss: 1.826433]\n",
      "epoch:26 step:25007 [D loss: 0.669862, acc: 60.94%] [G loss: 1.902998]\n",
      "epoch:26 step:25008 [D loss: 0.638454, acc: 61.72%] [G loss: 1.964761]\n",
      "epoch:26 step:25009 [D loss: 0.644485, acc: 60.94%] [G loss: 2.075294]\n",
      "epoch:26 step:25010 [D loss: 0.575945, acc: 67.97%] [G loss: 2.306046]\n",
      "epoch:26 step:25011 [D loss: 0.626495, acc: 63.28%] [G loss: 2.194037]\n",
      "epoch:26 step:25012 [D loss: 0.596548, acc: 68.75%] [G loss: 1.929577]\n",
      "epoch:26 step:25013 [D loss: 0.663805, acc: 55.47%] [G loss: 1.909588]\n",
      "epoch:26 step:25014 [D loss: 0.673769, acc: 60.16%] [G loss: 1.964380]\n",
      "epoch:26 step:25015 [D loss: 0.641286, acc: 62.50%] [G loss: 1.854235]\n",
      "epoch:26 step:25016 [D loss: 0.630190, acc: 60.94%] [G loss: 1.991828]\n",
      "epoch:26 step:25017 [D loss: 0.682926, acc: 61.72%] [G loss: 1.975119]\n",
      "epoch:26 step:25018 [D loss: 0.715543, acc: 53.12%] [G loss: 1.739842]\n",
      "epoch:26 step:25019 [D loss: 0.674419, acc: 58.59%] [G loss: 1.753416]\n",
      "epoch:26 step:25020 [D loss: 0.648016, acc: 57.81%] [G loss: 1.652159]\n",
      "epoch:26 step:25021 [D loss: 0.672052, acc: 58.59%] [G loss: 1.819759]\n",
      "epoch:26 step:25022 [D loss: 0.633604, acc: 59.38%] [G loss: 1.947577]\n",
      "epoch:26 step:25023 [D loss: 0.674981, acc: 60.16%] [G loss: 1.794912]\n",
      "epoch:26 step:25024 [D loss: 0.638263, acc: 63.28%] [G loss: 2.002087]\n",
      "epoch:26 step:25025 [D loss: 0.681459, acc: 61.72%] [G loss: 1.751280]\n",
      "epoch:26 step:25026 [D loss: 0.658039, acc: 65.62%] [G loss: 1.843099]\n",
      "epoch:26 step:25027 [D loss: 0.644680, acc: 60.16%] [G loss: 1.873393]\n",
      "epoch:26 step:25028 [D loss: 0.705870, acc: 53.91%] [G loss: 1.796023]\n",
      "epoch:26 step:25029 [D loss: 0.678059, acc: 53.91%] [G loss: 1.681351]\n",
      "epoch:26 step:25030 [D loss: 0.641993, acc: 71.09%] [G loss: 1.917838]\n",
      "epoch:26 step:25031 [D loss: 0.667745, acc: 58.59%] [G loss: 1.814220]\n",
      "epoch:26 step:25032 [D loss: 0.674839, acc: 60.94%] [G loss: 1.689938]\n",
      "epoch:26 step:25033 [D loss: 0.636845, acc: 63.28%] [G loss: 1.843765]\n",
      "epoch:26 step:25034 [D loss: 0.630536, acc: 64.06%] [G loss: 1.888937]\n",
      "epoch:26 step:25035 [D loss: 0.681037, acc: 57.81%] [G loss: 1.834892]\n",
      "epoch:26 step:25036 [D loss: 0.604273, acc: 69.53%] [G loss: 1.811991]\n",
      "epoch:26 step:25037 [D loss: 0.665169, acc: 54.69%] [G loss: 1.784115]\n",
      "epoch:26 step:25038 [D loss: 0.656099, acc: 60.16%] [G loss: 1.747707]\n",
      "epoch:26 step:25039 [D loss: 0.632953, acc: 65.62%] [G loss: 1.839820]\n",
      "epoch:26 step:25040 [D loss: 0.613479, acc: 66.41%] [G loss: 1.749013]\n",
      "epoch:26 step:25041 [D loss: 0.643070, acc: 60.16%] [G loss: 1.894425]\n",
      "epoch:26 step:25042 [D loss: 0.643742, acc: 61.72%] [G loss: 1.924710]\n",
      "epoch:26 step:25043 [D loss: 0.605250, acc: 67.19%] [G loss: 1.826846]\n",
      "epoch:26 step:25044 [D loss: 0.717484, acc: 53.91%] [G loss: 1.852179]\n",
      "epoch:26 step:25045 [D loss: 0.649061, acc: 61.72%] [G loss: 1.786203]\n",
      "epoch:26 step:25046 [D loss: 0.701873, acc: 54.69%] [G loss: 1.681517]\n",
      "epoch:26 step:25047 [D loss: 0.664387, acc: 57.03%] [G loss: 1.902328]\n",
      "epoch:26 step:25048 [D loss: 0.611584, acc: 69.53%] [G loss: 1.804081]\n",
      "epoch:26 step:25049 [D loss: 0.612869, acc: 64.84%] [G loss: 2.020813]\n",
      "epoch:26 step:25050 [D loss: 0.617619, acc: 67.19%] [G loss: 1.903744]\n",
      "epoch:26 step:25051 [D loss: 0.586777, acc: 71.88%] [G loss: 1.968379]\n",
      "epoch:26 step:25052 [D loss: 0.579689, acc: 69.53%] [G loss: 2.098299]\n",
      "epoch:26 step:25053 [D loss: 0.617825, acc: 67.97%] [G loss: 2.033962]\n",
      "epoch:26 step:25054 [D loss: 0.643376, acc: 60.16%] [G loss: 1.943887]\n",
      "epoch:26 step:25055 [D loss: 0.615156, acc: 65.62%] [G loss: 2.066636]\n",
      "epoch:26 step:25056 [D loss: 0.600094, acc: 67.97%] [G loss: 2.136361]\n",
      "epoch:26 step:25057 [D loss: 0.618102, acc: 64.84%] [G loss: 2.098265]\n",
      "epoch:26 step:25058 [D loss: 0.609563, acc: 66.41%] [G loss: 1.825402]\n",
      "epoch:26 step:25059 [D loss: 0.684048, acc: 55.47%] [G loss: 1.909939]\n",
      "epoch:26 step:25060 [D loss: 0.694510, acc: 54.69%] [G loss: 1.885310]\n",
      "epoch:26 step:25061 [D loss: 0.603015, acc: 65.62%] [G loss: 1.882559]\n",
      "epoch:26 step:25062 [D loss: 0.695969, acc: 57.81%] [G loss: 1.787566]\n",
      "epoch:26 step:25063 [D loss: 0.634862, acc: 62.50%] [G loss: 2.007623]\n",
      "epoch:26 step:25064 [D loss: 0.664707, acc: 58.59%] [G loss: 1.806448]\n",
      "epoch:26 step:25065 [D loss: 0.673041, acc: 65.62%] [G loss: 1.803131]\n",
      "epoch:26 step:25066 [D loss: 0.710601, acc: 57.81%] [G loss: 1.781510]\n",
      "epoch:26 step:25067 [D loss: 0.596429, acc: 69.53%] [G loss: 1.890955]\n",
      "epoch:26 step:25068 [D loss: 0.633636, acc: 65.62%] [G loss: 1.926013]\n",
      "epoch:26 step:25069 [D loss: 0.691702, acc: 56.25%] [G loss: 1.888588]\n",
      "epoch:26 step:25070 [D loss: 0.659934, acc: 62.50%] [G loss: 2.097684]\n",
      "epoch:26 step:25071 [D loss: 0.599257, acc: 67.97%] [G loss: 2.043540]\n",
      "epoch:26 step:25072 [D loss: 0.668941, acc: 61.72%] [G loss: 1.672488]\n",
      "epoch:26 step:25073 [D loss: 0.664915, acc: 66.41%] [G loss: 1.907092]\n",
      "epoch:26 step:25074 [D loss: 0.657838, acc: 60.94%] [G loss: 1.902033]\n",
      "epoch:26 step:25075 [D loss: 0.608204, acc: 65.62%] [G loss: 1.820168]\n",
      "epoch:26 step:25076 [D loss: 0.668712, acc: 57.03%] [G loss: 1.854790]\n",
      "epoch:26 step:25077 [D loss: 0.631977, acc: 63.28%] [G loss: 1.871366]\n",
      "epoch:26 step:25078 [D loss: 0.686792, acc: 56.25%] [G loss: 1.762592]\n",
      "epoch:26 step:25079 [D loss: 0.637705, acc: 61.72%] [G loss: 1.967023]\n",
      "epoch:26 step:25080 [D loss: 0.636574, acc: 62.50%] [G loss: 1.844397]\n",
      "epoch:26 step:25081 [D loss: 0.595084, acc: 70.31%] [G loss: 2.003573]\n",
      "epoch:26 step:25082 [D loss: 0.656177, acc: 57.03%] [G loss: 1.951961]\n",
      "epoch:26 step:25083 [D loss: 0.662158, acc: 64.06%] [G loss: 2.006533]\n",
      "epoch:26 step:25084 [D loss: 0.749436, acc: 46.88%] [G loss: 1.925082]\n",
      "epoch:26 step:25085 [D loss: 0.633660, acc: 68.75%] [G loss: 1.983003]\n",
      "epoch:26 step:25086 [D loss: 0.630003, acc: 64.06%] [G loss: 1.936515]\n",
      "epoch:26 step:25087 [D loss: 0.692218, acc: 57.03%] [G loss: 1.835725]\n",
      "epoch:26 step:25088 [D loss: 0.659376, acc: 61.72%] [G loss: 1.935706]\n",
      "epoch:26 step:25089 [D loss: 0.660976, acc: 60.16%] [G loss: 1.884121]\n",
      "epoch:26 step:25090 [D loss: 0.635252, acc: 59.38%] [G loss: 1.758706]\n",
      "epoch:26 step:25091 [D loss: 0.693644, acc: 53.91%] [G loss: 1.840451]\n",
      "epoch:26 step:25092 [D loss: 0.646931, acc: 60.94%] [G loss: 1.777464]\n",
      "epoch:26 step:25093 [D loss: 0.674262, acc: 60.94%] [G loss: 1.696280]\n",
      "epoch:26 step:25094 [D loss: 0.614489, acc: 68.75%] [G loss: 1.805609]\n",
      "epoch:26 step:25095 [D loss: 0.600976, acc: 72.66%] [G loss: 1.807180]\n",
      "epoch:26 step:25096 [D loss: 0.639383, acc: 58.59%] [G loss: 1.858393]\n",
      "epoch:26 step:25097 [D loss: 0.628337, acc: 63.28%] [G loss: 1.918069]\n",
      "epoch:26 step:25098 [D loss: 0.614105, acc: 68.75%] [G loss: 1.975034]\n",
      "epoch:26 step:25099 [D loss: 0.626343, acc: 65.62%] [G loss: 1.858174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25100 [D loss: 0.679372, acc: 57.03%] [G loss: 1.857249]\n",
      "epoch:26 step:25101 [D loss: 0.678838, acc: 57.03%] [G loss: 1.937299]\n",
      "epoch:26 step:25102 [D loss: 0.625649, acc: 64.06%] [G loss: 1.975345]\n",
      "epoch:26 step:25103 [D loss: 0.651035, acc: 60.16%] [G loss: 1.842297]\n",
      "epoch:26 step:25104 [D loss: 0.619032, acc: 67.19%] [G loss: 1.834543]\n",
      "epoch:26 step:25105 [D loss: 0.650024, acc: 60.94%] [G loss: 1.819668]\n",
      "epoch:26 step:25106 [D loss: 0.628132, acc: 66.41%] [G loss: 1.958358]\n",
      "epoch:26 step:25107 [D loss: 0.606698, acc: 67.97%] [G loss: 1.910003]\n",
      "epoch:26 step:25108 [D loss: 0.632021, acc: 64.84%] [G loss: 2.037314]\n",
      "epoch:26 step:25109 [D loss: 0.622588, acc: 67.97%] [G loss: 2.116367]\n",
      "epoch:26 step:25110 [D loss: 0.627648, acc: 60.94%] [G loss: 1.987160]\n",
      "epoch:26 step:25111 [D loss: 0.735138, acc: 52.34%] [G loss: 1.784411]\n",
      "epoch:26 step:25112 [D loss: 0.632090, acc: 57.81%] [G loss: 1.839373]\n",
      "epoch:26 step:25113 [D loss: 0.640659, acc: 67.19%] [G loss: 1.966691]\n",
      "epoch:26 step:25114 [D loss: 0.682189, acc: 60.94%] [G loss: 1.821187]\n",
      "epoch:26 step:25115 [D loss: 0.672724, acc: 56.25%] [G loss: 1.995953]\n",
      "epoch:26 step:25116 [D loss: 0.642185, acc: 67.97%] [G loss: 1.924103]\n",
      "epoch:26 step:25117 [D loss: 0.680355, acc: 54.69%] [G loss: 1.778533]\n",
      "epoch:26 step:25118 [D loss: 0.647312, acc: 66.41%] [G loss: 1.843478]\n",
      "epoch:26 step:25119 [D loss: 0.645110, acc: 64.84%] [G loss: 1.849954]\n",
      "epoch:26 step:25120 [D loss: 0.631995, acc: 67.97%] [G loss: 1.927135]\n",
      "epoch:26 step:25121 [D loss: 0.657498, acc: 58.59%] [G loss: 1.862373]\n",
      "epoch:26 step:25122 [D loss: 0.635894, acc: 60.94%] [G loss: 1.902423]\n",
      "epoch:26 step:25123 [D loss: 0.658693, acc: 61.72%] [G loss: 1.880054]\n",
      "epoch:26 step:25124 [D loss: 0.673727, acc: 60.16%] [G loss: 1.798989]\n",
      "epoch:26 step:25125 [D loss: 0.651108, acc: 64.84%] [G loss: 2.012765]\n",
      "epoch:26 step:25126 [D loss: 0.673002, acc: 57.03%] [G loss: 1.687587]\n",
      "epoch:26 step:25127 [D loss: 0.671513, acc: 60.16%] [G loss: 1.762990]\n",
      "epoch:26 step:25128 [D loss: 0.717898, acc: 57.03%] [G loss: 1.696664]\n",
      "epoch:26 step:25129 [D loss: 0.712856, acc: 57.03%] [G loss: 1.807602]\n",
      "epoch:26 step:25130 [D loss: 0.715168, acc: 56.25%] [G loss: 1.799911]\n",
      "epoch:26 step:25131 [D loss: 0.673272, acc: 59.38%] [G loss: 1.843953]\n",
      "epoch:26 step:25132 [D loss: 0.624422, acc: 67.97%] [G loss: 1.767702]\n",
      "epoch:26 step:25133 [D loss: 0.665052, acc: 57.03%] [G loss: 1.820300]\n",
      "epoch:26 step:25134 [D loss: 0.646235, acc: 61.72%] [G loss: 1.959724]\n",
      "epoch:26 step:25135 [D loss: 0.639089, acc: 67.19%] [G loss: 1.899771]\n",
      "epoch:26 step:25136 [D loss: 0.610805, acc: 68.75%] [G loss: 1.965984]\n",
      "epoch:26 step:25137 [D loss: 0.639658, acc: 57.81%] [G loss: 2.064515]\n",
      "epoch:26 step:25138 [D loss: 0.657710, acc: 60.16%] [G loss: 1.872704]\n",
      "epoch:26 step:25139 [D loss: 0.640692, acc: 64.06%] [G loss: 1.798120]\n",
      "epoch:26 step:25140 [D loss: 0.643917, acc: 60.16%] [G loss: 1.927717]\n",
      "epoch:26 step:25141 [D loss: 0.617072, acc: 69.53%] [G loss: 1.790868]\n",
      "epoch:26 step:25142 [D loss: 0.658517, acc: 62.50%] [G loss: 1.915375]\n",
      "epoch:26 step:25143 [D loss: 0.639455, acc: 63.28%] [G loss: 1.930022]\n",
      "epoch:26 step:25144 [D loss: 0.655018, acc: 58.59%] [G loss: 2.081949]\n",
      "epoch:26 step:25145 [D loss: 0.687140, acc: 57.81%] [G loss: 1.844129]\n",
      "epoch:26 step:25146 [D loss: 0.709211, acc: 50.00%] [G loss: 1.789264]\n",
      "epoch:26 step:25147 [D loss: 0.674300, acc: 57.03%] [G loss: 1.719580]\n",
      "epoch:26 step:25148 [D loss: 0.652490, acc: 61.72%] [G loss: 1.910456]\n",
      "epoch:26 step:25149 [D loss: 0.640829, acc: 66.41%] [G loss: 1.806643]\n",
      "epoch:26 step:25150 [D loss: 0.670075, acc: 55.47%] [G loss: 1.930779]\n",
      "epoch:26 step:25151 [D loss: 0.679316, acc: 56.25%] [G loss: 1.884116]\n",
      "epoch:26 step:25152 [D loss: 0.644348, acc: 64.06%] [G loss: 1.925039]\n",
      "epoch:26 step:25153 [D loss: 0.689035, acc: 61.72%] [G loss: 1.843569]\n",
      "epoch:26 step:25154 [D loss: 0.667257, acc: 59.38%] [G loss: 2.041457]\n",
      "epoch:26 step:25155 [D loss: 0.634894, acc: 61.72%] [G loss: 1.860280]\n",
      "epoch:26 step:25156 [D loss: 0.697772, acc: 54.69%] [G loss: 1.709607]\n",
      "epoch:26 step:25157 [D loss: 0.652317, acc: 58.59%] [G loss: 1.762637]\n",
      "epoch:26 step:25158 [D loss: 0.706165, acc: 55.47%] [G loss: 1.867835]\n",
      "epoch:26 step:25159 [D loss: 0.629590, acc: 64.84%] [G loss: 1.868146]\n",
      "epoch:26 step:25160 [D loss: 0.634952, acc: 59.38%] [G loss: 1.879616]\n",
      "epoch:26 step:25161 [D loss: 0.674502, acc: 57.81%] [G loss: 1.962361]\n",
      "epoch:26 step:25162 [D loss: 0.733617, acc: 54.69%] [G loss: 1.695137]\n",
      "epoch:26 step:25163 [D loss: 0.665560, acc: 57.03%] [G loss: 1.735037]\n",
      "epoch:26 step:25164 [D loss: 0.662600, acc: 61.72%] [G loss: 1.748970]\n",
      "epoch:26 step:25165 [D loss: 0.680222, acc: 57.03%] [G loss: 1.869249]\n",
      "epoch:26 step:25166 [D loss: 0.638117, acc: 67.19%] [G loss: 1.829320]\n",
      "epoch:26 step:25167 [D loss: 0.636958, acc: 62.50%] [G loss: 1.897093]\n",
      "epoch:26 step:25168 [D loss: 0.674968, acc: 58.59%] [G loss: 1.971570]\n",
      "epoch:26 step:25169 [D loss: 0.578241, acc: 68.75%] [G loss: 1.894282]\n",
      "epoch:26 step:25170 [D loss: 0.652074, acc: 59.38%] [G loss: 1.913486]\n",
      "epoch:26 step:25171 [D loss: 0.645927, acc: 62.50%] [G loss: 1.959570]\n",
      "epoch:26 step:25172 [D loss: 0.630513, acc: 60.94%] [G loss: 1.803080]\n",
      "epoch:26 step:25173 [D loss: 0.666578, acc: 64.84%] [G loss: 1.853664]\n",
      "epoch:26 step:25174 [D loss: 0.660835, acc: 60.16%] [G loss: 1.811867]\n",
      "epoch:26 step:25175 [D loss: 0.637157, acc: 64.06%] [G loss: 1.810413]\n",
      "epoch:26 step:25176 [D loss: 0.649783, acc: 58.59%] [G loss: 1.734860]\n",
      "epoch:26 step:25177 [D loss: 0.589657, acc: 70.31%] [G loss: 2.066227]\n",
      "epoch:26 step:25178 [D loss: 0.599846, acc: 71.88%] [G loss: 2.123380]\n",
      "epoch:26 step:25179 [D loss: 0.718256, acc: 58.59%] [G loss: 1.717738]\n",
      "epoch:26 step:25180 [D loss: 0.651136, acc: 59.38%] [G loss: 1.827265]\n",
      "epoch:26 step:25181 [D loss: 0.702911, acc: 56.25%] [G loss: 1.836709]\n",
      "epoch:26 step:25182 [D loss: 0.660723, acc: 59.38%] [G loss: 1.700040]\n",
      "epoch:26 step:25183 [D loss: 0.664129, acc: 56.25%] [G loss: 1.848943]\n",
      "epoch:26 step:25184 [D loss: 0.627978, acc: 67.19%] [G loss: 1.901142]\n",
      "epoch:26 step:25185 [D loss: 0.669936, acc: 58.59%] [G loss: 1.991689]\n",
      "epoch:26 step:25186 [D loss: 0.657490, acc: 60.94%] [G loss: 1.955485]\n",
      "epoch:26 step:25187 [D loss: 0.629131, acc: 66.41%] [G loss: 1.951994]\n",
      "epoch:26 step:25188 [D loss: 0.656728, acc: 60.94%] [G loss: 1.924775]\n",
      "epoch:26 step:25189 [D loss: 0.613680, acc: 69.53%] [G loss: 1.792611]\n",
      "epoch:26 step:25190 [D loss: 0.665015, acc: 60.16%] [G loss: 1.726693]\n",
      "epoch:26 step:25191 [D loss: 0.654974, acc: 59.38%] [G loss: 1.828627]\n",
      "epoch:26 step:25192 [D loss: 0.669697, acc: 57.03%] [G loss: 1.792152]\n",
      "epoch:26 step:25193 [D loss: 0.634215, acc: 65.62%] [G loss: 1.762245]\n",
      "epoch:26 step:25194 [D loss: 0.648865, acc: 67.19%] [G loss: 1.903190]\n",
      "epoch:26 step:25195 [D loss: 0.572689, acc: 75.00%] [G loss: 1.984986]\n",
      "epoch:26 step:25196 [D loss: 0.675841, acc: 62.50%] [G loss: 1.821567]\n",
      "epoch:26 step:25197 [D loss: 0.648386, acc: 61.72%] [G loss: 1.879437]\n",
      "epoch:26 step:25198 [D loss: 0.673924, acc: 61.72%] [G loss: 1.767201]\n",
      "epoch:26 step:25199 [D loss: 0.607251, acc: 71.09%] [G loss: 1.929873]\n",
      "epoch:26 step:25200 [D loss: 0.671923, acc: 60.94%] [G loss: 1.867494]\n",
      "##############\n",
      "[2.44408679 1.37610413 6.31569239 4.7440943  3.44732884 5.46920526\n",
      " 4.30720368 4.86593296 4.485872   3.60484223]\n",
      "##########\n",
      "epoch:26 step:25201 [D loss: 0.649624, acc: 67.97%] [G loss: 1.929385]\n",
      "epoch:26 step:25202 [D loss: 0.631598, acc: 65.62%] [G loss: 1.938939]\n",
      "epoch:26 step:25203 [D loss: 0.689972, acc: 59.38%] [G loss: 1.917107]\n",
      "epoch:26 step:25204 [D loss: 0.597983, acc: 68.75%] [G loss: 1.947625]\n",
      "epoch:26 step:25205 [D loss: 0.631056, acc: 68.75%] [G loss: 1.758501]\n",
      "epoch:26 step:25206 [D loss: 0.664680, acc: 59.38%] [G loss: 1.906720]\n",
      "epoch:26 step:25207 [D loss: 0.629194, acc: 59.38%] [G loss: 1.889283]\n",
      "epoch:26 step:25208 [D loss: 0.704323, acc: 50.78%] [G loss: 1.881770]\n",
      "epoch:26 step:25209 [D loss: 0.715771, acc: 51.56%] [G loss: 1.896924]\n",
      "epoch:26 step:25210 [D loss: 0.643081, acc: 57.81%] [G loss: 1.886666]\n",
      "epoch:26 step:25211 [D loss: 0.644561, acc: 65.62%] [G loss: 1.896415]\n",
      "epoch:26 step:25212 [D loss: 0.678773, acc: 58.59%] [G loss: 1.752887]\n",
      "epoch:26 step:25213 [D loss: 0.656091, acc: 52.34%] [G loss: 1.721270]\n",
      "epoch:26 step:25214 [D loss: 0.648593, acc: 63.28%] [G loss: 1.979700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25215 [D loss: 0.653120, acc: 58.59%] [G loss: 1.891370]\n",
      "epoch:26 step:25216 [D loss: 0.654798, acc: 59.38%] [G loss: 1.899352]\n",
      "epoch:26 step:25217 [D loss: 0.650567, acc: 60.16%] [G loss: 1.864439]\n",
      "epoch:26 step:25218 [D loss: 0.670704, acc: 60.94%] [G loss: 1.855834]\n",
      "epoch:26 step:25219 [D loss: 0.674774, acc: 54.69%] [G loss: 1.821242]\n",
      "epoch:26 step:25220 [D loss: 0.681164, acc: 56.25%] [G loss: 1.930757]\n",
      "epoch:26 step:25221 [D loss: 0.664817, acc: 60.94%] [G loss: 1.781319]\n",
      "epoch:26 step:25222 [D loss: 0.668228, acc: 57.81%] [G loss: 1.829792]\n",
      "epoch:26 step:25223 [D loss: 0.642249, acc: 60.94%] [G loss: 1.837721]\n",
      "epoch:26 step:25224 [D loss: 0.686787, acc: 55.47%] [G loss: 1.710703]\n",
      "epoch:26 step:25225 [D loss: 0.655492, acc: 61.72%] [G loss: 1.817903]\n",
      "epoch:26 step:25226 [D loss: 0.654658, acc: 63.28%] [G loss: 1.800230]\n",
      "epoch:26 step:25227 [D loss: 0.672804, acc: 59.38%] [G loss: 1.697741]\n",
      "epoch:26 step:25228 [D loss: 0.593337, acc: 74.22%] [G loss: 1.778480]\n",
      "epoch:26 step:25229 [D loss: 0.696774, acc: 53.91%] [G loss: 1.711366]\n",
      "epoch:26 step:25230 [D loss: 0.590689, acc: 70.31%] [G loss: 1.850452]\n",
      "epoch:26 step:25231 [D loss: 0.665655, acc: 56.25%] [G loss: 1.784242]\n",
      "epoch:26 step:25232 [D loss: 0.650973, acc: 66.41%] [G loss: 1.843864]\n",
      "epoch:26 step:25233 [D loss: 0.627415, acc: 66.41%] [G loss: 1.752656]\n",
      "epoch:26 step:25234 [D loss: 0.642594, acc: 64.06%] [G loss: 1.960226]\n",
      "epoch:26 step:25235 [D loss: 0.667165, acc: 59.38%] [G loss: 1.832695]\n",
      "epoch:26 step:25236 [D loss: 0.696031, acc: 57.03%] [G loss: 1.863968]\n",
      "epoch:26 step:25237 [D loss: 0.709936, acc: 60.94%] [G loss: 1.896657]\n",
      "epoch:26 step:25238 [D loss: 0.647893, acc: 61.72%] [G loss: 1.851774]\n",
      "epoch:26 step:25239 [D loss: 0.670112, acc: 53.91%] [G loss: 1.796641]\n",
      "epoch:26 step:25240 [D loss: 0.660806, acc: 57.81%] [G loss: 1.714789]\n",
      "epoch:26 step:25241 [D loss: 0.656825, acc: 59.38%] [G loss: 1.771930]\n",
      "epoch:26 step:25242 [D loss: 0.639475, acc: 61.72%] [G loss: 1.749074]\n",
      "epoch:26 step:25243 [D loss: 0.657272, acc: 57.03%] [G loss: 1.710613]\n",
      "epoch:26 step:25244 [D loss: 0.599662, acc: 67.97%] [G loss: 1.890971]\n",
      "epoch:26 step:25245 [D loss: 0.669870, acc: 60.94%] [G loss: 1.834118]\n",
      "epoch:26 step:25246 [D loss: 0.651860, acc: 61.72%] [G loss: 1.874669]\n",
      "epoch:26 step:25247 [D loss: 0.665106, acc: 62.50%] [G loss: 1.714765]\n",
      "epoch:26 step:25248 [D loss: 0.632682, acc: 63.28%] [G loss: 1.979305]\n",
      "epoch:26 step:25249 [D loss: 0.674170, acc: 61.72%] [G loss: 1.816730]\n",
      "epoch:26 step:25250 [D loss: 0.624239, acc: 64.06%] [G loss: 1.874107]\n",
      "epoch:26 step:25251 [D loss: 0.619263, acc: 70.31%] [G loss: 1.869831]\n",
      "epoch:26 step:25252 [D loss: 0.612661, acc: 67.97%] [G loss: 2.054320]\n",
      "epoch:26 step:25253 [D loss: 0.668080, acc: 64.06%] [G loss: 1.755153]\n",
      "epoch:26 step:25254 [D loss: 0.706235, acc: 55.47%] [G loss: 1.829337]\n",
      "epoch:26 step:25255 [D loss: 0.657805, acc: 58.59%] [G loss: 1.925908]\n",
      "epoch:26 step:25256 [D loss: 0.605172, acc: 66.41%] [G loss: 1.944395]\n",
      "epoch:26 step:25257 [D loss: 0.695075, acc: 52.34%] [G loss: 1.873979]\n",
      "epoch:26 step:25258 [D loss: 0.712742, acc: 55.47%] [G loss: 1.745032]\n",
      "epoch:26 step:25259 [D loss: 0.628781, acc: 61.72%] [G loss: 1.823033]\n",
      "epoch:26 step:25260 [D loss: 0.617579, acc: 65.62%] [G loss: 1.860987]\n",
      "epoch:26 step:25261 [D loss: 0.617570, acc: 64.84%] [G loss: 1.905746]\n",
      "epoch:26 step:25262 [D loss: 0.650342, acc: 63.28%] [G loss: 1.877635]\n",
      "epoch:26 step:25263 [D loss: 0.649675, acc: 64.06%] [G loss: 1.841792]\n",
      "epoch:26 step:25264 [D loss: 0.684717, acc: 56.25%] [G loss: 1.899845]\n",
      "epoch:26 step:25265 [D loss: 0.640934, acc: 60.94%] [G loss: 1.958460]\n",
      "epoch:26 step:25266 [D loss: 0.636354, acc: 64.84%] [G loss: 1.788445]\n",
      "epoch:26 step:25267 [D loss: 0.611779, acc: 67.19%] [G loss: 2.003970]\n",
      "epoch:26 step:25268 [D loss: 0.658381, acc: 58.59%] [G loss: 1.902313]\n",
      "epoch:26 step:25269 [D loss: 0.628355, acc: 64.06%] [G loss: 1.899629]\n",
      "epoch:26 step:25270 [D loss: 0.623642, acc: 67.97%] [G loss: 1.905333]\n",
      "epoch:26 step:25271 [D loss: 0.642269, acc: 59.38%] [G loss: 2.025228]\n",
      "epoch:26 step:25272 [D loss: 0.604949, acc: 64.84%] [G loss: 1.902656]\n",
      "epoch:26 step:25273 [D loss: 0.692605, acc: 55.47%] [G loss: 1.822365]\n",
      "epoch:26 step:25274 [D loss: 0.618545, acc: 65.62%] [G loss: 2.111529]\n",
      "epoch:26 step:25275 [D loss: 0.682928, acc: 55.47%] [G loss: 1.859170]\n",
      "epoch:26 step:25276 [D loss: 0.661829, acc: 60.16%] [G loss: 1.736618]\n",
      "epoch:26 step:25277 [D loss: 0.681523, acc: 63.28%] [G loss: 1.827535]\n",
      "epoch:26 step:25278 [D loss: 0.605354, acc: 72.66%] [G loss: 1.867825]\n",
      "epoch:26 step:25279 [D loss: 0.663649, acc: 55.47%] [G loss: 1.856894]\n",
      "epoch:26 step:25280 [D loss: 0.584484, acc: 73.44%] [G loss: 1.994682]\n",
      "epoch:26 step:25281 [D loss: 0.595501, acc: 66.41%] [G loss: 2.126178]\n",
      "epoch:26 step:25282 [D loss: 0.734113, acc: 52.34%] [G loss: 1.895210]\n",
      "epoch:26 step:25283 [D loss: 0.582164, acc: 70.31%] [G loss: 2.020987]\n",
      "epoch:26 step:25284 [D loss: 0.648909, acc: 57.81%] [G loss: 1.941008]\n",
      "epoch:26 step:25285 [D loss: 0.623987, acc: 66.41%] [G loss: 1.971633]\n",
      "epoch:26 step:25286 [D loss: 0.541622, acc: 80.47%] [G loss: 2.169591]\n",
      "epoch:26 step:25287 [D loss: 0.618958, acc: 63.28%] [G loss: 2.080904]\n",
      "epoch:26 step:25288 [D loss: 0.637997, acc: 63.28%] [G loss: 2.045075]\n",
      "epoch:26 step:25289 [D loss: 0.644634, acc: 64.06%] [G loss: 2.064240]\n",
      "epoch:26 step:25290 [D loss: 0.755587, acc: 50.00%] [G loss: 1.905410]\n",
      "epoch:26 step:25291 [D loss: 0.735623, acc: 45.31%] [G loss: 2.090491]\n",
      "epoch:26 step:25292 [D loss: 0.673525, acc: 60.16%] [G loss: 1.981585]\n",
      "epoch:26 step:25293 [D loss: 0.603917, acc: 67.97%] [G loss: 1.981556]\n",
      "epoch:26 step:25294 [D loss: 0.700939, acc: 53.12%] [G loss: 1.752175]\n",
      "epoch:26 step:25295 [D loss: 0.613513, acc: 67.97%] [G loss: 1.903890]\n",
      "epoch:26 step:25296 [D loss: 0.652789, acc: 59.38%] [G loss: 2.002041]\n",
      "epoch:26 step:25297 [D loss: 0.565873, acc: 74.22%] [G loss: 2.038133]\n",
      "epoch:26 step:25298 [D loss: 0.554102, acc: 71.09%] [G loss: 2.113242]\n",
      "epoch:26 step:25299 [D loss: 0.596847, acc: 71.88%] [G loss: 2.526019]\n",
      "epoch:27 step:25300 [D loss: 0.772229, acc: 53.91%] [G loss: 1.782732]\n",
      "epoch:27 step:25301 [D loss: 0.646857, acc: 64.84%] [G loss: 1.827648]\n",
      "epoch:27 step:25302 [D loss: 0.690079, acc: 58.59%] [G loss: 1.743392]\n",
      "epoch:27 step:25303 [D loss: 0.693834, acc: 54.69%] [G loss: 1.739342]\n",
      "epoch:27 step:25304 [D loss: 0.650212, acc: 63.28%] [G loss: 1.861947]\n",
      "epoch:27 step:25305 [D loss: 0.667203, acc: 61.72%] [G loss: 1.964331]\n",
      "epoch:27 step:25306 [D loss: 0.639282, acc: 66.41%] [G loss: 1.929494]\n",
      "epoch:27 step:25307 [D loss: 0.647461, acc: 64.06%] [G loss: 1.795010]\n",
      "epoch:27 step:25308 [D loss: 0.631548, acc: 65.62%] [G loss: 1.998455]\n",
      "epoch:27 step:25309 [D loss: 0.614982, acc: 67.19%] [G loss: 1.834161]\n",
      "epoch:27 step:25310 [D loss: 0.670618, acc: 60.16%] [G loss: 1.907049]\n",
      "epoch:27 step:25311 [D loss: 0.674335, acc: 62.50%] [G loss: 1.937671]\n",
      "epoch:27 step:25312 [D loss: 0.611052, acc: 65.62%] [G loss: 1.857922]\n",
      "epoch:27 step:25313 [D loss: 0.605083, acc: 62.50%] [G loss: 1.865629]\n",
      "epoch:27 step:25314 [D loss: 0.603030, acc: 65.62%] [G loss: 1.849839]\n",
      "epoch:27 step:25315 [D loss: 0.583076, acc: 71.88%] [G loss: 2.049879]\n",
      "epoch:27 step:25316 [D loss: 0.627945, acc: 64.84%] [G loss: 2.007886]\n",
      "epoch:27 step:25317 [D loss: 0.643373, acc: 60.16%] [G loss: 1.953348]\n",
      "epoch:27 step:25318 [D loss: 0.714846, acc: 53.91%] [G loss: 1.839150]\n",
      "epoch:27 step:25319 [D loss: 0.661733, acc: 60.94%] [G loss: 1.820594]\n",
      "epoch:27 step:25320 [D loss: 0.706187, acc: 53.12%] [G loss: 1.853865]\n",
      "epoch:27 step:25321 [D loss: 0.636576, acc: 65.62%] [G loss: 1.852462]\n",
      "epoch:27 step:25322 [D loss: 0.658121, acc: 60.94%] [G loss: 2.115279]\n",
      "epoch:27 step:25323 [D loss: 0.608180, acc: 64.84%] [G loss: 1.867671]\n",
      "epoch:27 step:25324 [D loss: 0.603400, acc: 64.84%] [G loss: 2.058467]\n",
      "epoch:27 step:25325 [D loss: 0.623983, acc: 67.19%] [G loss: 1.813302]\n",
      "epoch:27 step:25326 [D loss: 0.656240, acc: 61.72%] [G loss: 1.880123]\n",
      "epoch:27 step:25327 [D loss: 0.643799, acc: 63.28%] [G loss: 1.894099]\n",
      "epoch:27 step:25328 [D loss: 0.633486, acc: 64.84%] [G loss: 1.926325]\n",
      "epoch:27 step:25329 [D loss: 0.633820, acc: 60.94%] [G loss: 1.902379]\n",
      "epoch:27 step:25330 [D loss: 0.719774, acc: 51.56%] [G loss: 1.807555]\n",
      "epoch:27 step:25331 [D loss: 0.648393, acc: 64.06%] [G loss: 1.770462]\n",
      "epoch:27 step:25332 [D loss: 0.623362, acc: 63.28%] [G loss: 1.799241]\n",
      "epoch:27 step:25333 [D loss: 0.673296, acc: 60.94%] [G loss: 1.789700]\n",
      "epoch:27 step:25334 [D loss: 0.635076, acc: 66.41%] [G loss: 1.854219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25335 [D loss: 0.653909, acc: 63.28%] [G loss: 2.053971]\n",
      "epoch:27 step:25336 [D loss: 0.585376, acc: 73.44%] [G loss: 2.035569]\n",
      "epoch:27 step:25337 [D loss: 0.710094, acc: 54.69%] [G loss: 1.886284]\n",
      "epoch:27 step:25338 [D loss: 0.635460, acc: 62.50%] [G loss: 1.920465]\n",
      "epoch:27 step:25339 [D loss: 0.604124, acc: 60.16%] [G loss: 1.922596]\n",
      "epoch:27 step:25340 [D loss: 0.632509, acc: 67.19%] [G loss: 1.904725]\n",
      "epoch:27 step:25341 [D loss: 0.680251, acc: 60.16%] [G loss: 1.913607]\n",
      "epoch:27 step:25342 [D loss: 0.630449, acc: 62.50%] [G loss: 1.867580]\n",
      "epoch:27 step:25343 [D loss: 0.659468, acc: 57.03%] [G loss: 1.795086]\n",
      "epoch:27 step:25344 [D loss: 0.668184, acc: 59.38%] [G loss: 1.811397]\n",
      "epoch:27 step:25345 [D loss: 0.631686, acc: 67.19%] [G loss: 1.831224]\n",
      "epoch:27 step:25346 [D loss: 0.606903, acc: 65.62%] [G loss: 1.903397]\n",
      "epoch:27 step:25347 [D loss: 0.639715, acc: 62.50%] [G loss: 1.982308]\n",
      "epoch:27 step:25348 [D loss: 0.639862, acc: 61.72%] [G loss: 2.096417]\n",
      "epoch:27 step:25349 [D loss: 0.684583, acc: 59.38%] [G loss: 1.940989]\n",
      "epoch:27 step:25350 [D loss: 0.666417, acc: 53.91%] [G loss: 1.826775]\n",
      "epoch:27 step:25351 [D loss: 0.667772, acc: 61.72%] [G loss: 1.920939]\n",
      "epoch:27 step:25352 [D loss: 0.653442, acc: 60.16%] [G loss: 1.872510]\n",
      "epoch:27 step:25353 [D loss: 0.630812, acc: 60.94%] [G loss: 1.910026]\n",
      "epoch:27 step:25354 [D loss: 0.645378, acc: 62.50%] [G loss: 1.951480]\n",
      "epoch:27 step:25355 [D loss: 0.696340, acc: 57.81%] [G loss: 1.902829]\n",
      "epoch:27 step:25356 [D loss: 0.612337, acc: 67.97%] [G loss: 1.965738]\n",
      "epoch:27 step:25357 [D loss: 0.664069, acc: 57.81%] [G loss: 1.818595]\n",
      "epoch:27 step:25358 [D loss: 0.649891, acc: 63.28%] [G loss: 1.909125]\n",
      "epoch:27 step:25359 [D loss: 0.668997, acc: 63.28%] [G loss: 1.892231]\n",
      "epoch:27 step:25360 [D loss: 0.635042, acc: 60.16%] [G loss: 1.743822]\n",
      "epoch:27 step:25361 [D loss: 0.642210, acc: 61.72%] [G loss: 1.879878]\n",
      "epoch:27 step:25362 [D loss: 0.686305, acc: 60.94%] [G loss: 1.773012]\n",
      "epoch:27 step:25363 [D loss: 0.633176, acc: 64.84%] [G loss: 1.840580]\n",
      "epoch:27 step:25364 [D loss: 0.657509, acc: 60.94%] [G loss: 1.789451]\n",
      "epoch:27 step:25365 [D loss: 0.621485, acc: 57.03%] [G loss: 1.972846]\n",
      "epoch:27 step:25366 [D loss: 0.617290, acc: 64.84%] [G loss: 1.888826]\n",
      "epoch:27 step:25367 [D loss: 0.644109, acc: 62.50%] [G loss: 1.777002]\n",
      "epoch:27 step:25368 [D loss: 0.601272, acc: 64.84%] [G loss: 1.885431]\n",
      "epoch:27 step:25369 [D loss: 0.648365, acc: 64.84%] [G loss: 1.918454]\n",
      "epoch:27 step:25370 [D loss: 0.676781, acc: 56.25%] [G loss: 1.750847]\n",
      "epoch:27 step:25371 [D loss: 0.658920, acc: 58.59%] [G loss: 1.755291]\n",
      "epoch:27 step:25372 [D loss: 0.681099, acc: 57.03%] [G loss: 1.728010]\n",
      "epoch:27 step:25373 [D loss: 0.599497, acc: 67.97%] [G loss: 1.932321]\n",
      "epoch:27 step:25374 [D loss: 0.649740, acc: 64.84%] [G loss: 2.030716]\n",
      "epoch:27 step:25375 [D loss: 0.646280, acc: 63.28%] [G loss: 2.058671]\n",
      "epoch:27 step:25376 [D loss: 0.626434, acc: 65.62%] [G loss: 2.111049]\n",
      "epoch:27 step:25377 [D loss: 0.640617, acc: 59.38%] [G loss: 1.819596]\n",
      "epoch:27 step:25378 [D loss: 0.696314, acc: 52.34%] [G loss: 1.730072]\n",
      "epoch:27 step:25379 [D loss: 0.654194, acc: 60.16%] [G loss: 1.970534]\n",
      "epoch:27 step:25380 [D loss: 0.692448, acc: 60.94%] [G loss: 1.696153]\n",
      "epoch:27 step:25381 [D loss: 0.694471, acc: 50.78%] [G loss: 1.795588]\n",
      "epoch:27 step:25382 [D loss: 0.619976, acc: 61.72%] [G loss: 1.872394]\n",
      "epoch:27 step:25383 [D loss: 0.642155, acc: 66.41%] [G loss: 1.970593]\n",
      "epoch:27 step:25384 [D loss: 0.671992, acc: 57.03%] [G loss: 1.709592]\n",
      "epoch:27 step:25385 [D loss: 0.623204, acc: 64.06%] [G loss: 1.691762]\n",
      "epoch:27 step:25386 [D loss: 0.639800, acc: 61.72%] [G loss: 1.897266]\n",
      "epoch:27 step:25387 [D loss: 0.685436, acc: 62.50%] [G loss: 1.743866]\n",
      "epoch:27 step:25388 [D loss: 0.644403, acc: 63.28%] [G loss: 1.804144]\n",
      "epoch:27 step:25389 [D loss: 0.641988, acc: 67.19%] [G loss: 1.866111]\n",
      "epoch:27 step:25390 [D loss: 0.651950, acc: 63.28%] [G loss: 1.936580]\n",
      "epoch:27 step:25391 [D loss: 0.687645, acc: 56.25%] [G loss: 1.888966]\n",
      "epoch:27 step:25392 [D loss: 0.606949, acc: 67.97%] [G loss: 1.859404]\n",
      "epoch:27 step:25393 [D loss: 0.648331, acc: 58.59%] [G loss: 1.723783]\n",
      "epoch:27 step:25394 [D loss: 0.677931, acc: 58.59%] [G loss: 1.756126]\n",
      "epoch:27 step:25395 [D loss: 0.651409, acc: 62.50%] [G loss: 1.875846]\n",
      "epoch:27 step:25396 [D loss: 0.654710, acc: 59.38%] [G loss: 1.815969]\n",
      "epoch:27 step:25397 [D loss: 0.633237, acc: 67.19%] [G loss: 1.774913]\n",
      "epoch:27 step:25398 [D loss: 0.649298, acc: 64.06%] [G loss: 1.792717]\n",
      "epoch:27 step:25399 [D loss: 0.583614, acc: 71.09%] [G loss: 1.954989]\n",
      "epoch:27 step:25400 [D loss: 0.638513, acc: 61.72%] [G loss: 1.949710]\n",
      "##############\n",
      "[2.49000524 1.3846725  6.12174502 4.74342976 3.48665479 5.66618777\n",
      " 4.43374725 4.62944763 4.32111984 3.71261553]\n",
      "##########\n",
      "epoch:27 step:25401 [D loss: 0.676473, acc: 59.38%] [G loss: 1.898326]\n",
      "epoch:27 step:25402 [D loss: 0.640426, acc: 64.06%] [G loss: 1.839288]\n",
      "epoch:27 step:25403 [D loss: 0.635789, acc: 58.59%] [G loss: 1.894722]\n",
      "epoch:27 step:25404 [D loss: 0.620156, acc: 65.62%] [G loss: 1.848941]\n",
      "epoch:27 step:25405 [D loss: 0.654867, acc: 63.28%] [G loss: 1.863752]\n",
      "epoch:27 step:25406 [D loss: 0.620885, acc: 63.28%] [G loss: 2.123635]\n",
      "epoch:27 step:25407 [D loss: 0.673728, acc: 61.72%] [G loss: 1.717430]\n",
      "epoch:27 step:25408 [D loss: 0.670577, acc: 60.94%] [G loss: 1.887851]\n",
      "epoch:27 step:25409 [D loss: 0.650309, acc: 59.38%] [G loss: 1.811144]\n",
      "epoch:27 step:25410 [D loss: 0.607287, acc: 69.53%] [G loss: 1.927740]\n",
      "epoch:27 step:25411 [D loss: 0.727555, acc: 53.91%] [G loss: 1.945651]\n",
      "epoch:27 step:25412 [D loss: 0.584716, acc: 73.44%] [G loss: 2.003607]\n",
      "epoch:27 step:25413 [D loss: 0.616998, acc: 69.53%] [G loss: 1.985933]\n",
      "epoch:27 step:25414 [D loss: 0.624620, acc: 64.06%] [G loss: 2.080601]\n",
      "epoch:27 step:25415 [D loss: 0.615284, acc: 72.66%] [G loss: 2.070169]\n",
      "epoch:27 step:25416 [D loss: 0.578568, acc: 71.09%] [G loss: 2.042296]\n",
      "epoch:27 step:25417 [D loss: 0.607457, acc: 62.50%] [G loss: 2.089818]\n",
      "epoch:27 step:25418 [D loss: 0.571685, acc: 69.53%] [G loss: 2.299367]\n",
      "epoch:27 step:25419 [D loss: 0.679714, acc: 58.59%] [G loss: 1.832613]\n",
      "epoch:27 step:25420 [D loss: 0.672966, acc: 61.72%] [G loss: 1.901512]\n",
      "epoch:27 step:25421 [D loss: 0.648170, acc: 67.19%] [G loss: 2.061606]\n",
      "epoch:27 step:25422 [D loss: 0.644671, acc: 57.81%] [G loss: 1.878943]\n",
      "epoch:27 step:25423 [D loss: 0.632241, acc: 60.94%] [G loss: 1.788195]\n",
      "epoch:27 step:25424 [D loss: 0.737046, acc: 60.94%] [G loss: 1.849227]\n",
      "epoch:27 step:25425 [D loss: 0.620018, acc: 61.72%] [G loss: 2.081662]\n",
      "epoch:27 step:25426 [D loss: 0.634040, acc: 58.59%] [G loss: 1.752162]\n",
      "epoch:27 step:25427 [D loss: 0.620034, acc: 64.06%] [G loss: 1.908565]\n",
      "epoch:27 step:25428 [D loss: 0.658343, acc: 57.81%] [G loss: 1.943441]\n",
      "epoch:27 step:25429 [D loss: 0.657670, acc: 60.94%] [G loss: 1.969726]\n",
      "epoch:27 step:25430 [D loss: 0.646934, acc: 60.94%] [G loss: 1.916602]\n",
      "epoch:27 step:25431 [D loss: 0.649554, acc: 60.16%] [G loss: 1.905047]\n",
      "epoch:27 step:25432 [D loss: 0.759043, acc: 47.66%] [G loss: 1.841346]\n",
      "epoch:27 step:25433 [D loss: 0.657753, acc: 62.50%] [G loss: 1.844984]\n",
      "epoch:27 step:25434 [D loss: 0.667915, acc: 58.59%] [G loss: 1.728321]\n",
      "epoch:27 step:25435 [D loss: 0.637019, acc: 65.62%] [G loss: 1.802197]\n",
      "epoch:27 step:25436 [D loss: 0.718819, acc: 53.12%] [G loss: 1.830947]\n",
      "epoch:27 step:25437 [D loss: 0.633642, acc: 64.84%] [G loss: 1.776360]\n",
      "epoch:27 step:25438 [D loss: 0.666322, acc: 56.25%] [G loss: 1.743184]\n",
      "epoch:27 step:25439 [D loss: 0.650763, acc: 60.16%] [G loss: 1.869061]\n",
      "epoch:27 step:25440 [D loss: 0.667224, acc: 65.62%] [G loss: 1.976531]\n",
      "epoch:27 step:25441 [D loss: 0.619832, acc: 65.62%] [G loss: 1.827332]\n",
      "epoch:27 step:25442 [D loss: 0.676050, acc: 56.25%] [G loss: 1.841228]\n",
      "epoch:27 step:25443 [D loss: 0.647753, acc: 58.59%] [G loss: 1.876079]\n",
      "epoch:27 step:25444 [D loss: 0.657017, acc: 60.94%] [G loss: 1.909924]\n",
      "epoch:27 step:25445 [D loss: 0.600024, acc: 71.09%] [G loss: 1.920911]\n",
      "epoch:27 step:25446 [D loss: 0.638571, acc: 66.41%] [G loss: 1.902674]\n",
      "epoch:27 step:25447 [D loss: 0.671599, acc: 53.12%] [G loss: 1.743943]\n",
      "epoch:27 step:25448 [D loss: 0.612083, acc: 61.72%] [G loss: 2.002077]\n",
      "epoch:27 step:25449 [D loss: 0.652716, acc: 63.28%] [G loss: 1.989963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25450 [D loss: 0.597774, acc: 70.31%] [G loss: 2.019777]\n",
      "epoch:27 step:25451 [D loss: 0.634935, acc: 60.16%] [G loss: 1.936724]\n",
      "epoch:27 step:25452 [D loss: 0.636395, acc: 63.28%] [G loss: 1.744469]\n",
      "epoch:27 step:25453 [D loss: 0.649759, acc: 58.59%] [G loss: 1.969573]\n",
      "epoch:27 step:25454 [D loss: 0.661519, acc: 58.59%] [G loss: 1.847414]\n",
      "epoch:27 step:25455 [D loss: 0.659820, acc: 57.81%] [G loss: 1.773393]\n",
      "epoch:27 step:25456 [D loss: 0.670070, acc: 59.38%] [G loss: 1.778608]\n",
      "epoch:27 step:25457 [D loss: 0.605532, acc: 70.31%] [G loss: 1.771435]\n",
      "epoch:27 step:25458 [D loss: 0.646835, acc: 61.72%] [G loss: 1.754766]\n",
      "epoch:27 step:25459 [D loss: 0.712808, acc: 54.69%] [G loss: 1.710867]\n",
      "epoch:27 step:25460 [D loss: 0.656993, acc: 59.38%] [G loss: 1.859325]\n",
      "epoch:27 step:25461 [D loss: 0.612414, acc: 68.75%] [G loss: 1.984834]\n",
      "epoch:27 step:25462 [D loss: 0.690599, acc: 58.59%] [G loss: 1.868187]\n",
      "epoch:27 step:25463 [D loss: 0.632425, acc: 59.38%] [G loss: 1.772651]\n",
      "epoch:27 step:25464 [D loss: 0.616300, acc: 69.53%] [G loss: 1.791162]\n",
      "epoch:27 step:25465 [D loss: 0.687506, acc: 55.47%] [G loss: 1.853969]\n",
      "epoch:27 step:25466 [D loss: 0.633714, acc: 67.19%] [G loss: 1.868894]\n",
      "epoch:27 step:25467 [D loss: 0.675922, acc: 66.41%] [G loss: 1.755698]\n",
      "epoch:27 step:25468 [D loss: 0.627667, acc: 60.94%] [G loss: 1.854051]\n",
      "epoch:27 step:25469 [D loss: 0.706866, acc: 52.34%] [G loss: 1.859712]\n",
      "epoch:27 step:25470 [D loss: 0.667559, acc: 63.28%] [G loss: 1.861521]\n",
      "epoch:27 step:25471 [D loss: 0.659595, acc: 59.38%] [G loss: 1.863486]\n",
      "epoch:27 step:25472 [D loss: 0.679719, acc: 56.25%] [G loss: 1.767877]\n",
      "epoch:27 step:25473 [D loss: 0.646126, acc: 63.28%] [G loss: 1.764816]\n",
      "epoch:27 step:25474 [D loss: 0.651484, acc: 64.84%] [G loss: 1.935512]\n",
      "epoch:27 step:25475 [D loss: 0.671868, acc: 57.03%] [G loss: 1.802995]\n",
      "epoch:27 step:25476 [D loss: 0.667688, acc: 54.69%] [G loss: 1.806660]\n",
      "epoch:27 step:25477 [D loss: 0.662789, acc: 57.81%] [G loss: 1.790465]\n",
      "epoch:27 step:25478 [D loss: 0.627769, acc: 64.84%] [G loss: 1.693958]\n",
      "epoch:27 step:25479 [D loss: 0.664442, acc: 57.03%] [G loss: 1.834814]\n",
      "epoch:27 step:25480 [D loss: 0.678828, acc: 58.59%] [G loss: 1.767440]\n",
      "epoch:27 step:25481 [D loss: 0.647639, acc: 57.81%] [G loss: 1.679333]\n",
      "epoch:27 step:25482 [D loss: 0.641605, acc: 60.94%] [G loss: 1.705252]\n",
      "epoch:27 step:25483 [D loss: 0.661443, acc: 58.59%] [G loss: 1.883820]\n",
      "epoch:27 step:25484 [D loss: 0.640159, acc: 64.84%] [G loss: 1.784268]\n",
      "epoch:27 step:25485 [D loss: 0.658640, acc: 61.72%] [G loss: 1.798502]\n",
      "epoch:27 step:25486 [D loss: 0.715754, acc: 58.59%] [G loss: 1.752829]\n",
      "epoch:27 step:25487 [D loss: 0.655064, acc: 60.94%] [G loss: 1.958797]\n",
      "epoch:27 step:25488 [D loss: 0.609179, acc: 65.62%] [G loss: 1.818471]\n",
      "epoch:27 step:25489 [D loss: 0.646201, acc: 64.06%] [G loss: 1.925344]\n",
      "epoch:27 step:25490 [D loss: 0.630816, acc: 65.62%] [G loss: 1.837629]\n",
      "epoch:27 step:25491 [D loss: 0.619011, acc: 64.84%] [G loss: 1.913772]\n",
      "epoch:27 step:25492 [D loss: 0.663976, acc: 57.03%] [G loss: 1.834619]\n",
      "epoch:27 step:25493 [D loss: 0.629111, acc: 63.28%] [G loss: 2.054702]\n",
      "epoch:27 step:25494 [D loss: 0.634538, acc: 65.62%] [G loss: 1.868088]\n",
      "epoch:27 step:25495 [D loss: 0.686620, acc: 53.91%] [G loss: 1.905894]\n",
      "epoch:27 step:25496 [D loss: 0.654843, acc: 63.28%] [G loss: 1.896104]\n",
      "epoch:27 step:25497 [D loss: 0.641451, acc: 66.41%] [G loss: 1.968248]\n",
      "epoch:27 step:25498 [D loss: 0.683283, acc: 57.81%] [G loss: 1.900335]\n",
      "epoch:27 step:25499 [D loss: 0.684336, acc: 59.38%] [G loss: 1.824300]\n",
      "epoch:27 step:25500 [D loss: 0.661087, acc: 63.28%] [G loss: 1.757549]\n",
      "epoch:27 step:25501 [D loss: 0.644141, acc: 64.84%] [G loss: 1.811732]\n",
      "epoch:27 step:25502 [D loss: 0.694703, acc: 58.59%] [G loss: 1.904914]\n",
      "epoch:27 step:25503 [D loss: 0.679575, acc: 54.69%] [G loss: 1.807662]\n",
      "epoch:27 step:25504 [D loss: 0.657409, acc: 60.16%] [G loss: 1.823627]\n",
      "epoch:27 step:25505 [D loss: 0.664298, acc: 60.16%] [G loss: 1.903928]\n",
      "epoch:27 step:25506 [D loss: 0.599350, acc: 69.53%] [G loss: 1.971218]\n",
      "epoch:27 step:25507 [D loss: 0.580462, acc: 72.66%] [G loss: 2.128478]\n",
      "epoch:27 step:25508 [D loss: 0.615985, acc: 64.84%] [G loss: 2.221357]\n",
      "epoch:27 step:25509 [D loss: 0.665446, acc: 62.50%] [G loss: 1.821255]\n",
      "epoch:27 step:25510 [D loss: 0.653888, acc: 61.72%] [G loss: 1.767620]\n",
      "epoch:27 step:25511 [D loss: 0.682983, acc: 53.91%] [G loss: 1.824246]\n",
      "epoch:27 step:25512 [D loss: 0.678759, acc: 59.38%] [G loss: 1.858241]\n",
      "epoch:27 step:25513 [D loss: 0.659403, acc: 58.59%] [G loss: 1.786658]\n",
      "epoch:27 step:25514 [D loss: 0.671470, acc: 60.94%] [G loss: 1.726225]\n",
      "epoch:27 step:25515 [D loss: 0.679536, acc: 53.12%] [G loss: 2.038245]\n",
      "epoch:27 step:25516 [D loss: 0.602955, acc: 66.41%] [G loss: 1.830326]\n",
      "epoch:27 step:25517 [D loss: 0.593849, acc: 73.44%] [G loss: 2.027947]\n",
      "epoch:27 step:25518 [D loss: 0.593509, acc: 70.31%] [G loss: 2.087178]\n",
      "epoch:27 step:25519 [D loss: 0.701862, acc: 57.03%] [G loss: 1.804429]\n",
      "epoch:27 step:25520 [D loss: 0.706584, acc: 57.81%] [G loss: 1.830173]\n",
      "epoch:27 step:25521 [D loss: 0.640910, acc: 61.72%] [G loss: 1.900270]\n",
      "epoch:27 step:25522 [D loss: 0.651641, acc: 61.72%] [G loss: 1.949787]\n",
      "epoch:27 step:25523 [D loss: 0.637602, acc: 64.06%] [G loss: 1.822412]\n",
      "epoch:27 step:25524 [D loss: 0.668018, acc: 64.06%] [G loss: 1.762904]\n",
      "epoch:27 step:25525 [D loss: 0.666083, acc: 60.16%] [G loss: 1.846077]\n",
      "epoch:27 step:25526 [D loss: 0.592257, acc: 68.75%] [G loss: 1.948032]\n",
      "epoch:27 step:25527 [D loss: 0.666135, acc: 57.81%] [G loss: 1.883375]\n",
      "epoch:27 step:25528 [D loss: 0.599661, acc: 60.94%] [G loss: 2.051023]\n",
      "epoch:27 step:25529 [D loss: 0.597542, acc: 65.62%] [G loss: 2.050738]\n",
      "epoch:27 step:25530 [D loss: 0.557526, acc: 71.09%] [G loss: 2.314370]\n",
      "epoch:27 step:25531 [D loss: 0.579199, acc: 71.09%] [G loss: 2.178187]\n",
      "epoch:27 step:25532 [D loss: 0.708049, acc: 60.16%] [G loss: 1.878710]\n",
      "epoch:27 step:25533 [D loss: 0.663521, acc: 61.72%] [G loss: 1.776644]\n",
      "epoch:27 step:25534 [D loss: 0.670928, acc: 60.94%] [G loss: 1.807556]\n",
      "epoch:27 step:25535 [D loss: 0.637740, acc: 64.84%] [G loss: 1.848028]\n",
      "epoch:27 step:25536 [D loss: 0.724100, acc: 57.81%] [G loss: 1.912795]\n",
      "epoch:27 step:25537 [D loss: 0.696051, acc: 55.47%] [G loss: 1.946898]\n",
      "epoch:27 step:25538 [D loss: 0.659152, acc: 59.38%] [G loss: 1.790064]\n",
      "epoch:27 step:25539 [D loss: 0.615712, acc: 67.97%] [G loss: 1.941221]\n",
      "epoch:27 step:25540 [D loss: 0.659734, acc: 64.84%] [G loss: 1.917455]\n",
      "epoch:27 step:25541 [D loss: 0.630672, acc: 60.94%] [G loss: 1.969966]\n",
      "epoch:27 step:25542 [D loss: 0.654894, acc: 61.72%] [G loss: 1.924360]\n",
      "epoch:27 step:25543 [D loss: 0.608283, acc: 64.06%] [G loss: 1.871768]\n",
      "epoch:27 step:25544 [D loss: 0.655858, acc: 61.72%] [G loss: 1.789300]\n",
      "epoch:27 step:25545 [D loss: 0.660685, acc: 62.50%] [G loss: 1.817229]\n",
      "epoch:27 step:25546 [D loss: 0.634943, acc: 63.28%] [G loss: 1.929399]\n",
      "epoch:27 step:25547 [D loss: 0.638671, acc: 64.84%] [G loss: 1.997834]\n",
      "epoch:27 step:25548 [D loss: 0.683561, acc: 60.94%] [G loss: 1.768891]\n",
      "epoch:27 step:25549 [D loss: 0.736280, acc: 53.12%] [G loss: 1.725288]\n",
      "epoch:27 step:25550 [D loss: 0.668139, acc: 62.50%] [G loss: 1.752326]\n",
      "epoch:27 step:25551 [D loss: 0.628353, acc: 68.75%] [G loss: 1.878599]\n",
      "epoch:27 step:25552 [D loss: 0.634366, acc: 67.97%] [G loss: 1.729186]\n",
      "epoch:27 step:25553 [D loss: 0.604363, acc: 70.31%] [G loss: 1.797296]\n",
      "epoch:27 step:25554 [D loss: 0.677335, acc: 60.16%] [G loss: 1.808622]\n",
      "epoch:27 step:25555 [D loss: 0.688614, acc: 60.94%] [G loss: 1.981209]\n",
      "epoch:27 step:25556 [D loss: 0.672396, acc: 53.91%] [G loss: 1.712883]\n",
      "epoch:27 step:25557 [D loss: 0.620318, acc: 65.62%] [G loss: 1.849409]\n",
      "epoch:27 step:25558 [D loss: 0.667993, acc: 60.94%] [G loss: 1.783484]\n",
      "epoch:27 step:25559 [D loss: 0.637181, acc: 58.59%] [G loss: 1.849127]\n",
      "epoch:27 step:25560 [D loss: 0.617689, acc: 65.62%] [G loss: 1.946590]\n",
      "epoch:27 step:25561 [D loss: 0.650348, acc: 61.72%] [G loss: 1.851668]\n",
      "epoch:27 step:25562 [D loss: 0.644256, acc: 60.94%] [G loss: 1.794317]\n",
      "epoch:27 step:25563 [D loss: 0.654514, acc: 61.72%] [G loss: 2.051389]\n",
      "epoch:27 step:25564 [D loss: 0.763345, acc: 42.97%] [G loss: 1.720405]\n",
      "epoch:27 step:25565 [D loss: 0.617829, acc: 63.28%] [G loss: 1.838065]\n",
      "epoch:27 step:25566 [D loss: 0.639020, acc: 62.50%] [G loss: 1.998003]\n",
      "epoch:27 step:25567 [D loss: 0.651315, acc: 62.50%] [G loss: 1.795091]\n",
      "epoch:27 step:25568 [D loss: 0.642045, acc: 62.50%] [G loss: 1.920829]\n",
      "epoch:27 step:25569 [D loss: 0.666232, acc: 59.38%] [G loss: 2.014072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25570 [D loss: 0.631176, acc: 64.06%] [G loss: 1.880191]\n",
      "epoch:27 step:25571 [D loss: 0.672915, acc: 58.59%] [G loss: 1.824008]\n",
      "epoch:27 step:25572 [D loss: 0.663902, acc: 63.28%] [G loss: 1.867118]\n",
      "epoch:27 step:25573 [D loss: 0.599207, acc: 74.22%] [G loss: 1.985975]\n",
      "epoch:27 step:25574 [D loss: 0.619161, acc: 62.50%] [G loss: 1.914944]\n",
      "epoch:27 step:25575 [D loss: 0.581691, acc: 67.19%] [G loss: 2.074575]\n",
      "epoch:27 step:25576 [D loss: 0.703900, acc: 53.91%] [G loss: 1.905184]\n",
      "epoch:27 step:25577 [D loss: 0.646436, acc: 60.94%] [G loss: 1.738720]\n",
      "epoch:27 step:25578 [D loss: 0.639145, acc: 63.28%] [G loss: 1.904455]\n",
      "epoch:27 step:25579 [D loss: 0.650833, acc: 63.28%] [G loss: 1.916240]\n",
      "epoch:27 step:25580 [D loss: 0.657220, acc: 63.28%] [G loss: 1.777306]\n",
      "epoch:27 step:25581 [D loss: 0.711295, acc: 53.91%] [G loss: 1.773505]\n",
      "epoch:27 step:25582 [D loss: 0.627409, acc: 64.84%] [G loss: 1.948286]\n",
      "epoch:27 step:25583 [D loss: 0.645970, acc: 64.06%] [G loss: 1.861328]\n",
      "epoch:27 step:25584 [D loss: 0.677723, acc: 58.59%] [G loss: 1.829191]\n",
      "epoch:27 step:25585 [D loss: 0.663630, acc: 60.94%] [G loss: 1.826421]\n",
      "epoch:27 step:25586 [D loss: 0.640667, acc: 62.50%] [G loss: 1.881616]\n",
      "epoch:27 step:25587 [D loss: 0.658809, acc: 57.03%] [G loss: 1.799147]\n",
      "epoch:27 step:25588 [D loss: 0.658021, acc: 61.72%] [G loss: 1.902771]\n",
      "epoch:27 step:25589 [D loss: 0.616065, acc: 64.84%] [G loss: 1.859045]\n",
      "epoch:27 step:25590 [D loss: 0.719003, acc: 52.34%] [G loss: 1.832257]\n",
      "epoch:27 step:25591 [D loss: 0.631614, acc: 64.84%] [G loss: 1.909337]\n",
      "epoch:27 step:25592 [D loss: 0.659297, acc: 60.16%] [G loss: 1.901254]\n",
      "epoch:27 step:25593 [D loss: 0.665213, acc: 61.72%] [G loss: 1.816197]\n",
      "epoch:27 step:25594 [D loss: 0.660720, acc: 58.59%] [G loss: 1.927500]\n",
      "epoch:27 step:25595 [D loss: 0.663860, acc: 55.47%] [G loss: 1.822700]\n",
      "epoch:27 step:25596 [D loss: 0.665130, acc: 57.03%] [G loss: 1.761508]\n",
      "epoch:27 step:25597 [D loss: 0.577917, acc: 71.09%] [G loss: 2.015437]\n",
      "epoch:27 step:25598 [D loss: 0.618864, acc: 65.62%] [G loss: 1.898585]\n",
      "epoch:27 step:25599 [D loss: 0.641147, acc: 63.28%] [G loss: 1.859344]\n",
      "epoch:27 step:25600 [D loss: 0.642864, acc: 67.97%] [G loss: 1.811767]\n",
      "##############\n",
      "[2.4672081  1.33571143 6.16104815 4.69174744 3.63465995 5.46804377\n",
      " 4.59192687 4.69938088 4.45834799 3.61594488]\n",
      "##########\n",
      "epoch:27 step:25601 [D loss: 0.621495, acc: 63.28%] [G loss: 1.849978]\n",
      "epoch:27 step:25602 [D loss: 0.667110, acc: 59.38%] [G loss: 1.769139]\n",
      "epoch:27 step:25603 [D loss: 0.597916, acc: 69.53%] [G loss: 1.865563]\n",
      "epoch:27 step:25604 [D loss: 0.698575, acc: 60.16%] [G loss: 1.870339]\n",
      "epoch:27 step:25605 [D loss: 0.644380, acc: 61.72%] [G loss: 1.966038]\n",
      "epoch:27 step:25606 [D loss: 0.652080, acc: 58.59%] [G loss: 1.994440]\n",
      "epoch:27 step:25607 [D loss: 0.718515, acc: 57.81%] [G loss: 1.720309]\n",
      "epoch:27 step:25608 [D loss: 0.649086, acc: 60.16%] [G loss: 1.851438]\n",
      "epoch:27 step:25609 [D loss: 0.652701, acc: 66.41%] [G loss: 1.946517]\n",
      "epoch:27 step:25610 [D loss: 0.600371, acc: 65.62%] [G loss: 1.877108]\n",
      "epoch:27 step:25611 [D loss: 0.717829, acc: 56.25%] [G loss: 2.072118]\n",
      "epoch:27 step:25612 [D loss: 0.638671, acc: 66.41%] [G loss: 1.902296]\n",
      "epoch:27 step:25613 [D loss: 0.610587, acc: 67.97%] [G loss: 2.040248]\n",
      "epoch:27 step:25614 [D loss: 0.619541, acc: 65.62%] [G loss: 2.098464]\n",
      "epoch:27 step:25615 [D loss: 0.769728, acc: 48.44%] [G loss: 1.738528]\n",
      "epoch:27 step:25616 [D loss: 0.663442, acc: 61.72%] [G loss: 1.872011]\n",
      "epoch:27 step:25617 [D loss: 0.604976, acc: 60.94%] [G loss: 1.850406]\n",
      "epoch:27 step:25618 [D loss: 0.621860, acc: 64.06%] [G loss: 1.780433]\n",
      "epoch:27 step:25619 [D loss: 0.625167, acc: 64.06%] [G loss: 1.902528]\n",
      "epoch:27 step:25620 [D loss: 0.694766, acc: 53.91%] [G loss: 1.885186]\n",
      "epoch:27 step:25621 [D loss: 0.649769, acc: 62.50%] [G loss: 1.731002]\n",
      "epoch:27 step:25622 [D loss: 0.700480, acc: 55.47%] [G loss: 1.794631]\n",
      "epoch:27 step:25623 [D loss: 0.644962, acc: 63.28%] [G loss: 1.777628]\n",
      "epoch:27 step:25624 [D loss: 0.617834, acc: 65.62%] [G loss: 1.820217]\n",
      "epoch:27 step:25625 [D loss: 0.635570, acc: 63.28%] [G loss: 1.860469]\n",
      "epoch:27 step:25626 [D loss: 0.648783, acc: 63.28%] [G loss: 1.843710]\n",
      "epoch:27 step:25627 [D loss: 0.697387, acc: 60.16%] [G loss: 1.808197]\n",
      "epoch:27 step:25628 [D loss: 0.590345, acc: 71.09%] [G loss: 1.898908]\n",
      "epoch:27 step:25629 [D loss: 0.645887, acc: 62.50%] [G loss: 2.010219]\n",
      "epoch:27 step:25630 [D loss: 0.597299, acc: 67.19%] [G loss: 2.002105]\n",
      "epoch:27 step:25631 [D loss: 0.626392, acc: 71.09%] [G loss: 1.943390]\n",
      "epoch:27 step:25632 [D loss: 0.613733, acc: 69.53%] [G loss: 1.933985]\n",
      "epoch:27 step:25633 [D loss: 0.647784, acc: 60.16%] [G loss: 1.918121]\n",
      "epoch:27 step:25634 [D loss: 0.677623, acc: 57.03%] [G loss: 2.074204]\n",
      "epoch:27 step:25635 [D loss: 0.599975, acc: 66.41%] [G loss: 2.085311]\n",
      "epoch:27 step:25636 [D loss: 0.655710, acc: 57.81%] [G loss: 1.874603]\n",
      "epoch:27 step:25637 [D loss: 0.610200, acc: 63.28%] [G loss: 1.917516]\n",
      "epoch:27 step:25638 [D loss: 0.651142, acc: 66.41%] [G loss: 1.932342]\n",
      "epoch:27 step:25639 [D loss: 0.621117, acc: 65.62%] [G loss: 1.888618]\n",
      "epoch:27 step:25640 [D loss: 0.639030, acc: 61.72%] [G loss: 1.776701]\n",
      "epoch:27 step:25641 [D loss: 0.717510, acc: 52.34%] [G loss: 1.813346]\n",
      "epoch:27 step:25642 [D loss: 0.678070, acc: 57.03%] [G loss: 1.892084]\n",
      "epoch:27 step:25643 [D loss: 0.638231, acc: 62.50%] [G loss: 1.982640]\n",
      "epoch:27 step:25644 [D loss: 0.589648, acc: 72.66%] [G loss: 1.964010]\n",
      "epoch:27 step:25645 [D loss: 0.589667, acc: 71.09%] [G loss: 2.092197]\n",
      "epoch:27 step:25646 [D loss: 0.578305, acc: 67.97%] [G loss: 2.257831]\n",
      "epoch:27 step:25647 [D loss: 0.680049, acc: 57.03%] [G loss: 1.817637]\n",
      "epoch:27 step:25648 [D loss: 0.678507, acc: 63.28%] [G loss: 1.756895]\n",
      "epoch:27 step:25649 [D loss: 0.688692, acc: 59.38%] [G loss: 1.921175]\n",
      "epoch:27 step:25650 [D loss: 0.688962, acc: 56.25%] [G loss: 1.783219]\n",
      "epoch:27 step:25651 [D loss: 0.655081, acc: 61.72%] [G loss: 1.827150]\n",
      "epoch:27 step:25652 [D loss: 0.700217, acc: 53.12%] [G loss: 1.793774]\n",
      "epoch:27 step:25653 [D loss: 0.640983, acc: 64.84%] [G loss: 2.056756]\n",
      "epoch:27 step:25654 [D loss: 0.727421, acc: 51.56%] [G loss: 1.719024]\n",
      "epoch:27 step:25655 [D loss: 0.660307, acc: 57.81%] [G loss: 1.818114]\n",
      "epoch:27 step:25656 [D loss: 0.668891, acc: 60.94%] [G loss: 1.926997]\n",
      "epoch:27 step:25657 [D loss: 0.640523, acc: 65.62%] [G loss: 1.896412]\n",
      "epoch:27 step:25658 [D loss: 0.641729, acc: 59.38%] [G loss: 1.875548]\n",
      "epoch:27 step:25659 [D loss: 0.628566, acc: 64.84%] [G loss: 1.935345]\n",
      "epoch:27 step:25660 [D loss: 0.652029, acc: 65.62%] [G loss: 1.836458]\n",
      "epoch:27 step:25661 [D loss: 0.710263, acc: 53.91%] [G loss: 1.945672]\n",
      "epoch:27 step:25662 [D loss: 0.616238, acc: 60.94%] [G loss: 1.864980]\n",
      "epoch:27 step:25663 [D loss: 0.606203, acc: 67.19%] [G loss: 1.897621]\n",
      "epoch:27 step:25664 [D loss: 0.612358, acc: 69.53%] [G loss: 1.823354]\n",
      "epoch:27 step:25665 [D loss: 0.653189, acc: 59.38%] [G loss: 1.996027]\n",
      "epoch:27 step:25666 [D loss: 0.653513, acc: 60.16%] [G loss: 1.925438]\n",
      "epoch:27 step:25667 [D loss: 0.669408, acc: 61.72%] [G loss: 1.747918]\n",
      "epoch:27 step:25668 [D loss: 0.653848, acc: 62.50%] [G loss: 1.879878]\n",
      "epoch:27 step:25669 [D loss: 0.670725, acc: 63.28%] [G loss: 1.993419]\n",
      "epoch:27 step:25670 [D loss: 0.607254, acc: 64.84%] [G loss: 2.050346]\n",
      "epoch:27 step:25671 [D loss: 0.680395, acc: 57.81%] [G loss: 1.837915]\n",
      "epoch:27 step:25672 [D loss: 0.667310, acc: 58.59%] [G loss: 1.677302]\n",
      "epoch:27 step:25673 [D loss: 0.634535, acc: 64.84%] [G loss: 1.836519]\n",
      "epoch:27 step:25674 [D loss: 0.639961, acc: 65.62%] [G loss: 1.857087]\n",
      "epoch:27 step:25675 [D loss: 0.673560, acc: 58.59%] [G loss: 1.789441]\n",
      "epoch:27 step:25676 [D loss: 0.635227, acc: 62.50%] [G loss: 1.781896]\n",
      "epoch:27 step:25677 [D loss: 0.677529, acc: 60.16%] [G loss: 1.864143]\n",
      "epoch:27 step:25678 [D loss: 0.631497, acc: 64.06%] [G loss: 1.934357]\n",
      "epoch:27 step:25679 [D loss: 0.634528, acc: 65.62%] [G loss: 1.919036]\n",
      "epoch:27 step:25680 [D loss: 0.605175, acc: 70.31%] [G loss: 2.040751]\n",
      "epoch:27 step:25681 [D loss: 0.659274, acc: 60.94%] [G loss: 1.918579]\n",
      "epoch:27 step:25682 [D loss: 0.610458, acc: 70.31%] [G loss: 2.014700]\n",
      "epoch:27 step:25683 [D loss: 0.621503, acc: 64.84%] [G loss: 1.867580]\n",
      "epoch:27 step:25684 [D loss: 0.680087, acc: 59.38%] [G loss: 1.959400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25685 [D loss: 0.676079, acc: 55.47%] [G loss: 1.668186]\n",
      "epoch:27 step:25686 [D loss: 0.682715, acc: 56.25%] [G loss: 1.881400]\n",
      "epoch:27 step:25687 [D loss: 0.665438, acc: 58.59%] [G loss: 1.706123]\n",
      "epoch:27 step:25688 [D loss: 0.607478, acc: 63.28%] [G loss: 1.857876]\n",
      "epoch:27 step:25689 [D loss: 0.607721, acc: 68.75%] [G loss: 1.947436]\n",
      "epoch:27 step:25690 [D loss: 0.636670, acc: 60.94%] [G loss: 1.808140]\n",
      "epoch:27 step:25691 [D loss: 0.624042, acc: 64.06%] [G loss: 1.928817]\n",
      "epoch:27 step:25692 [D loss: 0.667374, acc: 57.81%] [G loss: 1.851240]\n",
      "epoch:27 step:25693 [D loss: 0.655513, acc: 63.28%] [G loss: 1.730593]\n",
      "epoch:27 step:25694 [D loss: 0.628865, acc: 64.84%] [G loss: 1.935848]\n",
      "epoch:27 step:25695 [D loss: 0.655238, acc: 59.38%] [G loss: 1.859844]\n",
      "epoch:27 step:25696 [D loss: 0.660866, acc: 57.81%] [G loss: 1.788304]\n",
      "epoch:27 step:25697 [D loss: 0.660516, acc: 60.94%] [G loss: 1.888551]\n",
      "epoch:27 step:25698 [D loss: 0.590175, acc: 70.31%] [G loss: 1.894140]\n",
      "epoch:27 step:25699 [D loss: 0.722576, acc: 56.25%] [G loss: 1.911283]\n",
      "epoch:27 step:25700 [D loss: 0.627799, acc: 63.28%] [G loss: 1.806869]\n",
      "epoch:27 step:25701 [D loss: 0.670410, acc: 55.47%] [G loss: 1.727682]\n",
      "epoch:27 step:25702 [D loss: 0.564910, acc: 71.09%] [G loss: 1.929279]\n",
      "epoch:27 step:25703 [D loss: 0.644974, acc: 64.84%] [G loss: 1.982919]\n",
      "epoch:27 step:25704 [D loss: 0.578298, acc: 70.31%] [G loss: 2.022157]\n",
      "epoch:27 step:25705 [D loss: 0.654875, acc: 61.72%] [G loss: 2.007668]\n",
      "epoch:27 step:25706 [D loss: 0.663743, acc: 59.38%] [G loss: 1.900562]\n",
      "epoch:27 step:25707 [D loss: 0.707886, acc: 57.03%] [G loss: 1.761494]\n",
      "epoch:27 step:25708 [D loss: 0.673905, acc: 60.16%] [G loss: 1.845625]\n",
      "epoch:27 step:25709 [D loss: 0.702722, acc: 53.91%] [G loss: 1.855650]\n",
      "epoch:27 step:25710 [D loss: 0.717260, acc: 55.47%] [G loss: 1.740858]\n",
      "epoch:27 step:25711 [D loss: 0.660762, acc: 57.81%] [G loss: 1.935537]\n",
      "epoch:27 step:25712 [D loss: 0.608471, acc: 64.84%] [G loss: 2.051191]\n",
      "epoch:27 step:25713 [D loss: 0.632575, acc: 69.53%] [G loss: 1.941626]\n",
      "epoch:27 step:25714 [D loss: 0.615115, acc: 70.31%] [G loss: 1.892444]\n",
      "epoch:27 step:25715 [D loss: 0.575822, acc: 64.06%] [G loss: 1.987870]\n",
      "epoch:27 step:25716 [D loss: 0.645275, acc: 60.94%] [G loss: 1.987233]\n",
      "epoch:27 step:25717 [D loss: 0.686927, acc: 64.06%] [G loss: 1.852941]\n",
      "epoch:27 step:25718 [D loss: 0.677832, acc: 56.25%] [G loss: 1.897618]\n",
      "epoch:27 step:25719 [D loss: 0.657425, acc: 62.50%] [G loss: 1.891027]\n",
      "epoch:27 step:25720 [D loss: 0.669288, acc: 60.16%] [G loss: 1.899966]\n",
      "epoch:27 step:25721 [D loss: 0.659817, acc: 64.84%] [G loss: 1.806612]\n",
      "epoch:27 step:25722 [D loss: 0.650136, acc: 63.28%] [G loss: 1.676119]\n",
      "epoch:27 step:25723 [D loss: 0.664461, acc: 57.03%] [G loss: 1.826535]\n",
      "epoch:27 step:25724 [D loss: 0.707756, acc: 60.16%] [G loss: 1.901800]\n",
      "epoch:27 step:25725 [D loss: 0.621976, acc: 64.84%] [G loss: 1.875511]\n",
      "epoch:27 step:25726 [D loss: 0.636491, acc: 65.62%] [G loss: 1.852319]\n",
      "epoch:27 step:25727 [D loss: 0.610044, acc: 67.19%] [G loss: 2.006941]\n",
      "epoch:27 step:25728 [D loss: 0.577937, acc: 69.53%] [G loss: 1.920027]\n",
      "epoch:27 step:25729 [D loss: 0.628285, acc: 67.19%] [G loss: 2.017345]\n",
      "epoch:27 step:25730 [D loss: 0.652467, acc: 59.38%] [G loss: 1.916899]\n",
      "epoch:27 step:25731 [D loss: 0.641142, acc: 64.84%] [G loss: 1.972035]\n",
      "epoch:27 step:25732 [D loss: 0.670927, acc: 61.72%] [G loss: 1.978138]\n",
      "epoch:27 step:25733 [D loss: 0.634783, acc: 63.28%] [G loss: 1.866782]\n",
      "epoch:27 step:25734 [D loss: 0.638969, acc: 66.41%] [G loss: 1.829018]\n",
      "epoch:27 step:25735 [D loss: 0.676508, acc: 58.59%] [G loss: 1.939649]\n",
      "epoch:27 step:25736 [D loss: 0.658834, acc: 60.16%] [G loss: 1.760018]\n",
      "epoch:27 step:25737 [D loss: 0.674650, acc: 59.38%] [G loss: 1.804663]\n",
      "epoch:27 step:25738 [D loss: 0.674282, acc: 64.06%] [G loss: 1.768043]\n",
      "epoch:27 step:25739 [D loss: 0.661544, acc: 60.16%] [G loss: 1.847993]\n",
      "epoch:27 step:25740 [D loss: 0.644189, acc: 61.72%] [G loss: 1.764471]\n",
      "epoch:27 step:25741 [D loss: 0.728730, acc: 50.78%] [G loss: 1.862244]\n",
      "epoch:27 step:25742 [D loss: 0.639026, acc: 62.50%] [G loss: 1.929289]\n",
      "epoch:27 step:25743 [D loss: 0.669583, acc: 60.16%] [G loss: 1.746492]\n",
      "epoch:27 step:25744 [D loss: 0.641715, acc: 63.28%] [G loss: 1.701051]\n",
      "epoch:27 step:25745 [D loss: 0.635439, acc: 64.06%] [G loss: 1.825958]\n",
      "epoch:27 step:25746 [D loss: 0.676293, acc: 57.81%] [G loss: 1.811997]\n",
      "epoch:27 step:25747 [D loss: 0.705947, acc: 55.47%] [G loss: 1.884778]\n",
      "epoch:27 step:25748 [D loss: 0.643939, acc: 64.06%] [G loss: 1.873893]\n",
      "epoch:27 step:25749 [D loss: 0.649149, acc: 64.06%] [G loss: 1.714200]\n",
      "epoch:27 step:25750 [D loss: 0.614735, acc: 67.97%] [G loss: 1.876986]\n",
      "epoch:27 step:25751 [D loss: 0.652317, acc: 60.94%] [G loss: 1.942109]\n",
      "epoch:27 step:25752 [D loss: 0.621347, acc: 67.19%] [G loss: 1.892462]\n",
      "epoch:27 step:25753 [D loss: 0.685061, acc: 59.38%] [G loss: 1.945451]\n",
      "epoch:27 step:25754 [D loss: 0.636564, acc: 64.06%] [G loss: 1.859980]\n",
      "epoch:27 step:25755 [D loss: 0.633482, acc: 66.41%] [G loss: 1.890219]\n",
      "epoch:27 step:25756 [D loss: 0.627498, acc: 66.41%] [G loss: 2.057297]\n",
      "epoch:27 step:25757 [D loss: 0.693007, acc: 53.91%] [G loss: 1.907120]\n",
      "epoch:27 step:25758 [D loss: 0.722144, acc: 50.00%] [G loss: 1.762817]\n",
      "epoch:27 step:25759 [D loss: 0.722502, acc: 54.69%] [G loss: 1.821798]\n",
      "epoch:27 step:25760 [D loss: 0.692795, acc: 62.50%] [G loss: 1.844068]\n",
      "epoch:27 step:25761 [D loss: 0.713720, acc: 52.34%] [G loss: 1.793337]\n",
      "epoch:27 step:25762 [D loss: 0.623123, acc: 69.53%] [G loss: 1.779936]\n",
      "epoch:27 step:25763 [D loss: 0.644970, acc: 67.19%] [G loss: 1.699777]\n",
      "epoch:27 step:25764 [D loss: 0.668416, acc: 58.59%] [G loss: 1.946823]\n",
      "epoch:27 step:25765 [D loss: 0.708691, acc: 53.91%] [G loss: 1.760683]\n",
      "epoch:27 step:25766 [D loss: 0.629252, acc: 62.50%] [G loss: 1.755122]\n",
      "epoch:27 step:25767 [D loss: 0.606137, acc: 67.97%] [G loss: 1.988680]\n",
      "epoch:27 step:25768 [D loss: 0.624729, acc: 64.06%] [G loss: 2.104303]\n",
      "epoch:27 step:25769 [D loss: 0.608151, acc: 67.97%] [G loss: 1.888226]\n",
      "epoch:27 step:25770 [D loss: 0.585111, acc: 70.31%] [G loss: 1.999210]\n",
      "epoch:27 step:25771 [D loss: 0.685083, acc: 56.25%] [G loss: 2.093520]\n",
      "epoch:27 step:25772 [D loss: 0.688778, acc: 52.34%] [G loss: 1.680773]\n",
      "epoch:27 step:25773 [D loss: 0.648830, acc: 60.94%] [G loss: 2.040637]\n",
      "epoch:27 step:25774 [D loss: 0.680902, acc: 55.47%] [G loss: 1.801151]\n",
      "epoch:27 step:25775 [D loss: 0.603823, acc: 66.41%] [G loss: 1.941771]\n",
      "epoch:27 step:25776 [D loss: 0.726706, acc: 51.56%] [G loss: 1.802387]\n",
      "epoch:27 step:25777 [D loss: 0.685300, acc: 57.81%] [G loss: 1.836456]\n",
      "epoch:27 step:25778 [D loss: 0.584622, acc: 71.88%] [G loss: 1.876775]\n",
      "epoch:27 step:25779 [D loss: 0.615837, acc: 73.44%] [G loss: 1.939832]\n",
      "epoch:27 step:25780 [D loss: 0.587423, acc: 67.19%] [G loss: 2.023873]\n",
      "epoch:27 step:25781 [D loss: 0.709647, acc: 57.81%] [G loss: 1.772560]\n",
      "epoch:27 step:25782 [D loss: 0.712060, acc: 55.47%] [G loss: 1.866492]\n",
      "epoch:27 step:25783 [D loss: 0.660149, acc: 56.25%] [G loss: 1.945057]\n",
      "epoch:27 step:25784 [D loss: 0.671053, acc: 62.50%] [G loss: 1.878785]\n",
      "epoch:27 step:25785 [D loss: 0.620118, acc: 67.97%] [G loss: 1.743949]\n",
      "epoch:27 step:25786 [D loss: 0.644991, acc: 61.72%] [G loss: 1.853512]\n",
      "epoch:27 step:25787 [D loss: 0.616789, acc: 63.28%] [G loss: 1.921956]\n",
      "epoch:27 step:25788 [D loss: 0.649100, acc: 64.06%] [G loss: 1.782457]\n",
      "epoch:27 step:25789 [D loss: 0.683302, acc: 59.38%] [G loss: 1.686209]\n",
      "epoch:27 step:25790 [D loss: 0.626355, acc: 64.06%] [G loss: 1.978762]\n",
      "epoch:27 step:25791 [D loss: 0.641102, acc: 65.62%] [G loss: 1.815669]\n",
      "epoch:27 step:25792 [D loss: 0.699627, acc: 51.56%] [G loss: 1.718240]\n",
      "epoch:27 step:25793 [D loss: 0.602050, acc: 72.66%] [G loss: 1.942048]\n",
      "epoch:27 step:25794 [D loss: 0.650444, acc: 66.41%] [G loss: 1.929565]\n",
      "epoch:27 step:25795 [D loss: 0.625507, acc: 65.62%] [G loss: 1.894780]\n",
      "epoch:27 step:25796 [D loss: 0.628227, acc: 64.06%] [G loss: 1.900420]\n",
      "epoch:27 step:25797 [D loss: 0.610465, acc: 70.31%] [G loss: 1.851536]\n",
      "epoch:27 step:25798 [D loss: 0.628985, acc: 69.53%] [G loss: 1.884672]\n",
      "epoch:27 step:25799 [D loss: 0.613338, acc: 64.06%] [G loss: 1.816030]\n",
      "epoch:27 step:25800 [D loss: 0.692781, acc: 60.94%] [G loss: 1.886105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.35725405 1.44830837 6.28995134 4.53554133 3.57841303 5.51949541\n",
      " 4.35924896 4.81569096 4.45308585 3.5311304 ]\n",
      "##########\n",
      "epoch:27 step:25801 [D loss: 0.681247, acc: 51.56%] [G loss: 1.630213]\n",
      "epoch:27 step:25802 [D loss: 0.678906, acc: 57.03%] [G loss: 1.877223]\n",
      "epoch:27 step:25803 [D loss: 0.673703, acc: 61.72%] [G loss: 2.017144]\n",
      "epoch:27 step:25804 [D loss: 0.651575, acc: 60.16%] [G loss: 1.848442]\n",
      "epoch:27 step:25805 [D loss: 0.685038, acc: 52.34%] [G loss: 1.775455]\n",
      "epoch:27 step:25806 [D loss: 0.631951, acc: 66.41%] [G loss: 1.832210]\n",
      "epoch:27 step:25807 [D loss: 0.598643, acc: 64.84%] [G loss: 1.938637]\n",
      "epoch:27 step:25808 [D loss: 0.609094, acc: 67.19%] [G loss: 1.967153]\n",
      "epoch:27 step:25809 [D loss: 0.677500, acc: 62.50%] [G loss: 1.918673]\n",
      "epoch:27 step:25810 [D loss: 0.705364, acc: 50.78%] [G loss: 1.642470]\n",
      "epoch:27 step:25811 [D loss: 0.657461, acc: 64.06%] [G loss: 1.868774]\n",
      "epoch:27 step:25812 [D loss: 0.633026, acc: 64.06%] [G loss: 1.900822]\n",
      "epoch:27 step:25813 [D loss: 0.674816, acc: 53.91%] [G loss: 1.897591]\n",
      "epoch:27 step:25814 [D loss: 0.600315, acc: 69.53%] [G loss: 1.975939]\n",
      "epoch:27 step:25815 [D loss: 0.616503, acc: 67.97%] [G loss: 1.974158]\n",
      "epoch:27 step:25816 [D loss: 0.618827, acc: 61.72%] [G loss: 1.956616]\n",
      "epoch:27 step:25817 [D loss: 0.670963, acc: 60.94%] [G loss: 1.707742]\n",
      "epoch:27 step:25818 [D loss: 0.611323, acc: 62.50%] [G loss: 1.897273]\n",
      "epoch:27 step:25819 [D loss: 0.617999, acc: 64.06%] [G loss: 2.061122]\n",
      "epoch:27 step:25820 [D loss: 0.628379, acc: 68.75%] [G loss: 1.781472]\n",
      "epoch:27 step:25821 [D loss: 0.558703, acc: 73.44%] [G loss: 2.048817]\n",
      "epoch:27 step:25822 [D loss: 0.659772, acc: 65.62%] [G loss: 1.925761]\n",
      "epoch:27 step:25823 [D loss: 0.670805, acc: 61.72%] [G loss: 2.035939]\n",
      "epoch:27 step:25824 [D loss: 0.721214, acc: 57.03%] [G loss: 1.866009]\n",
      "epoch:27 step:25825 [D loss: 0.587324, acc: 71.88%] [G loss: 1.877712]\n",
      "epoch:27 step:25826 [D loss: 0.673823, acc: 59.38%] [G loss: 1.874952]\n",
      "epoch:27 step:25827 [D loss: 0.703485, acc: 50.78%] [G loss: 1.810129]\n",
      "epoch:27 step:25828 [D loss: 0.683511, acc: 63.28%] [G loss: 1.711118]\n",
      "epoch:27 step:25829 [D loss: 0.666757, acc: 57.03%] [G loss: 1.702135]\n",
      "epoch:27 step:25830 [D loss: 0.684758, acc: 56.25%] [G loss: 1.701701]\n",
      "epoch:27 step:25831 [D loss: 0.610773, acc: 66.41%] [G loss: 1.874148]\n",
      "epoch:27 step:25832 [D loss: 0.636401, acc: 61.72%] [G loss: 1.911503]\n",
      "epoch:27 step:25833 [D loss: 0.668895, acc: 58.59%] [G loss: 1.797294]\n",
      "epoch:27 step:25834 [D loss: 0.666784, acc: 59.38%] [G loss: 1.806227]\n",
      "epoch:27 step:25835 [D loss: 0.631775, acc: 67.19%] [G loss: 1.932341]\n",
      "epoch:27 step:25836 [D loss: 0.644370, acc: 62.50%] [G loss: 1.712362]\n",
      "epoch:27 step:25837 [D loss: 0.671814, acc: 53.12%] [G loss: 1.779730]\n",
      "epoch:27 step:25838 [D loss: 0.672692, acc: 57.81%] [G loss: 1.899110]\n",
      "epoch:27 step:25839 [D loss: 0.653391, acc: 59.38%] [G loss: 1.937177]\n",
      "epoch:27 step:25840 [D loss: 0.606046, acc: 64.06%] [G loss: 1.844655]\n",
      "epoch:27 step:25841 [D loss: 0.695366, acc: 56.25%] [G loss: 1.756712]\n",
      "epoch:27 step:25842 [D loss: 0.685243, acc: 56.25%] [G loss: 1.801044]\n",
      "epoch:27 step:25843 [D loss: 0.671456, acc: 57.03%] [G loss: 1.816065]\n",
      "epoch:27 step:25844 [D loss: 0.637006, acc: 67.19%] [G loss: 2.024717]\n",
      "epoch:27 step:25845 [D loss: 0.695692, acc: 61.72%] [G loss: 1.712725]\n",
      "epoch:27 step:25846 [D loss: 0.635927, acc: 63.28%] [G loss: 1.804330]\n",
      "epoch:27 step:25847 [D loss: 0.675123, acc: 60.16%] [G loss: 1.836314]\n",
      "epoch:27 step:25848 [D loss: 0.635172, acc: 66.41%] [G loss: 1.873513]\n",
      "epoch:27 step:25849 [D loss: 0.645023, acc: 59.38%] [G loss: 1.827091]\n",
      "epoch:27 step:25850 [D loss: 0.606487, acc: 64.84%] [G loss: 1.837311]\n",
      "epoch:27 step:25851 [D loss: 0.637810, acc: 64.06%] [G loss: 1.870891]\n",
      "epoch:27 step:25852 [D loss: 0.657479, acc: 61.72%] [G loss: 1.840642]\n",
      "epoch:27 step:25853 [D loss: 0.601318, acc: 67.97%] [G loss: 2.043036]\n",
      "epoch:27 step:25854 [D loss: 0.587609, acc: 70.31%] [G loss: 1.925631]\n",
      "epoch:27 step:25855 [D loss: 0.636460, acc: 67.19%] [G loss: 1.928258]\n",
      "epoch:27 step:25856 [D loss: 0.632919, acc: 64.84%] [G loss: 1.978499]\n",
      "epoch:27 step:25857 [D loss: 0.599079, acc: 68.75%] [G loss: 1.964473]\n",
      "epoch:27 step:25858 [D loss: 0.648043, acc: 60.16%] [G loss: 1.897140]\n",
      "epoch:27 step:25859 [D loss: 0.679823, acc: 56.25%] [G loss: 1.783629]\n",
      "epoch:27 step:25860 [D loss: 0.637681, acc: 64.06%] [G loss: 1.860032]\n",
      "epoch:27 step:25861 [D loss: 0.622989, acc: 64.84%] [G loss: 1.934518]\n",
      "epoch:27 step:25862 [D loss: 0.671667, acc: 57.03%] [G loss: 1.935664]\n",
      "epoch:27 step:25863 [D loss: 0.564031, acc: 71.09%] [G loss: 2.108412]\n",
      "epoch:27 step:25864 [D loss: 0.661875, acc: 65.62%] [G loss: 1.743926]\n",
      "epoch:27 step:25865 [D loss: 0.736247, acc: 51.56%] [G loss: 1.779871]\n",
      "epoch:27 step:25866 [D loss: 0.665347, acc: 60.94%] [G loss: 1.810034]\n",
      "epoch:27 step:25867 [D loss: 0.627809, acc: 67.19%] [G loss: 1.763205]\n",
      "epoch:27 step:25868 [D loss: 0.634933, acc: 60.16%] [G loss: 1.914935]\n",
      "epoch:27 step:25869 [D loss: 0.655879, acc: 63.28%] [G loss: 1.936927]\n",
      "epoch:27 step:25870 [D loss: 0.622093, acc: 66.41%] [G loss: 1.897422]\n",
      "epoch:27 step:25871 [D loss: 0.631166, acc: 64.06%] [G loss: 1.848067]\n",
      "epoch:27 step:25872 [D loss: 0.701967, acc: 58.59%] [G loss: 1.853066]\n",
      "epoch:27 step:25873 [D loss: 0.686989, acc: 59.38%] [G loss: 1.979389]\n",
      "epoch:27 step:25874 [D loss: 0.612549, acc: 72.66%] [G loss: 1.850832]\n",
      "epoch:27 step:25875 [D loss: 0.688122, acc: 57.81%] [G loss: 1.798243]\n",
      "epoch:27 step:25876 [D loss: 0.633877, acc: 68.75%] [G loss: 1.725954]\n",
      "epoch:27 step:25877 [D loss: 0.638257, acc: 67.97%] [G loss: 1.824991]\n",
      "epoch:27 step:25878 [D loss: 0.635918, acc: 65.62%] [G loss: 1.903597]\n",
      "epoch:27 step:25879 [D loss: 0.676866, acc: 59.38%] [G loss: 1.809773]\n",
      "epoch:27 step:25880 [D loss: 0.689291, acc: 59.38%] [G loss: 1.699701]\n",
      "epoch:27 step:25881 [D loss: 0.620724, acc: 64.06%] [G loss: 1.815137]\n",
      "epoch:27 step:25882 [D loss: 0.654719, acc: 60.16%] [G loss: 1.946309]\n",
      "epoch:27 step:25883 [D loss: 0.647631, acc: 63.28%] [G loss: 1.844592]\n",
      "epoch:27 step:25884 [D loss: 0.644334, acc: 64.06%] [G loss: 1.904662]\n",
      "epoch:27 step:25885 [D loss: 0.616298, acc: 62.50%] [G loss: 1.756374]\n",
      "epoch:27 step:25886 [D loss: 0.685917, acc: 60.16%] [G loss: 1.948626]\n",
      "epoch:27 step:25887 [D loss: 0.593232, acc: 67.97%] [G loss: 2.082006]\n",
      "epoch:27 step:25888 [D loss: 0.610799, acc: 67.97%] [G loss: 1.889416]\n",
      "epoch:27 step:25889 [D loss: 0.659997, acc: 58.59%] [G loss: 1.909455]\n",
      "epoch:27 step:25890 [D loss: 0.618787, acc: 64.84%] [G loss: 1.917375]\n",
      "epoch:27 step:25891 [D loss: 0.663090, acc: 61.72%] [G loss: 1.779244]\n",
      "epoch:27 step:25892 [D loss: 0.684594, acc: 56.25%] [G loss: 1.788071]\n",
      "epoch:27 step:25893 [D loss: 0.664702, acc: 56.25%] [G loss: 1.726251]\n",
      "epoch:27 step:25894 [D loss: 0.631993, acc: 65.62%] [G loss: 1.870496]\n",
      "epoch:27 step:25895 [D loss: 0.668004, acc: 57.03%] [G loss: 1.745486]\n",
      "epoch:27 step:25896 [D loss: 0.652385, acc: 60.94%] [G loss: 1.802976]\n",
      "epoch:27 step:25897 [D loss: 0.667436, acc: 60.16%] [G loss: 1.801497]\n",
      "epoch:27 step:25898 [D loss: 0.646777, acc: 64.06%] [G loss: 1.834960]\n",
      "epoch:27 step:25899 [D loss: 0.658524, acc: 60.94%] [G loss: 1.737360]\n",
      "epoch:27 step:25900 [D loss: 0.649762, acc: 62.50%] [G loss: 1.840189]\n",
      "epoch:27 step:25901 [D loss: 0.638674, acc: 61.72%] [G loss: 1.754483]\n",
      "epoch:27 step:25902 [D loss: 0.581543, acc: 71.09%] [G loss: 1.908027]\n",
      "epoch:27 step:25903 [D loss: 0.701396, acc: 53.91%] [G loss: 1.784958]\n",
      "epoch:27 step:25904 [D loss: 0.632055, acc: 71.09%] [G loss: 1.951306]\n",
      "epoch:27 step:25905 [D loss: 0.618408, acc: 68.75%] [G loss: 1.920883]\n",
      "epoch:27 step:25906 [D loss: 0.645165, acc: 62.50%] [G loss: 1.909820]\n",
      "epoch:27 step:25907 [D loss: 0.630223, acc: 66.41%] [G loss: 1.808451]\n",
      "epoch:27 step:25908 [D loss: 0.705354, acc: 57.81%] [G loss: 1.869832]\n",
      "epoch:27 step:25909 [D loss: 0.666953, acc: 61.72%] [G loss: 1.776128]\n",
      "epoch:27 step:25910 [D loss: 0.639379, acc: 64.84%] [G loss: 1.834673]\n",
      "epoch:27 step:25911 [D loss: 0.647739, acc: 64.84%] [G loss: 1.832564]\n",
      "epoch:27 step:25912 [D loss: 0.662433, acc: 61.72%] [G loss: 1.778798]\n",
      "epoch:27 step:25913 [D loss: 0.657658, acc: 59.38%] [G loss: 1.765266]\n",
      "epoch:27 step:25914 [D loss: 0.646339, acc: 55.47%] [G loss: 1.700611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25915 [D loss: 0.643816, acc: 60.94%] [G loss: 1.949105]\n",
      "epoch:27 step:25916 [D loss: 0.648055, acc: 65.62%] [G loss: 1.777222]\n",
      "epoch:27 step:25917 [D loss: 0.601845, acc: 64.06%] [G loss: 1.909833]\n",
      "epoch:27 step:25918 [D loss: 0.631787, acc: 64.84%] [G loss: 1.839867]\n",
      "epoch:27 step:25919 [D loss: 0.646626, acc: 61.72%] [G loss: 1.951125]\n",
      "epoch:27 step:25920 [D loss: 0.626788, acc: 61.72%] [G loss: 1.769772]\n",
      "epoch:27 step:25921 [D loss: 0.644540, acc: 64.06%] [G loss: 1.877289]\n",
      "epoch:27 step:25922 [D loss: 0.646131, acc: 61.72%] [G loss: 1.705345]\n",
      "epoch:27 step:25923 [D loss: 0.646087, acc: 64.84%] [G loss: 2.082673]\n",
      "epoch:27 step:25924 [D loss: 0.668210, acc: 57.03%] [G loss: 1.758825]\n",
      "epoch:27 step:25925 [D loss: 0.716693, acc: 50.00%] [G loss: 1.778326]\n",
      "epoch:27 step:25926 [D loss: 0.638290, acc: 64.06%] [G loss: 1.855469]\n",
      "epoch:27 step:25927 [D loss: 0.651735, acc: 64.06%] [G loss: 1.745983]\n",
      "epoch:27 step:25928 [D loss: 0.631770, acc: 65.62%] [G loss: 1.893505]\n",
      "epoch:27 step:25929 [D loss: 0.627229, acc: 64.06%] [G loss: 1.769429]\n",
      "epoch:27 step:25930 [D loss: 0.598288, acc: 67.97%] [G loss: 1.903014]\n",
      "epoch:27 step:25931 [D loss: 0.584862, acc: 73.44%] [G loss: 1.927629]\n",
      "epoch:27 step:25932 [D loss: 0.610262, acc: 69.53%] [G loss: 1.839366]\n",
      "epoch:27 step:25933 [D loss: 0.636559, acc: 64.06%] [G loss: 2.099414]\n",
      "epoch:27 step:25934 [D loss: 0.609173, acc: 63.28%] [G loss: 2.165488]\n",
      "epoch:27 step:25935 [D loss: 0.601456, acc: 63.28%] [G loss: 2.030809]\n",
      "epoch:27 step:25936 [D loss: 0.554951, acc: 76.56%] [G loss: 2.303212]\n",
      "epoch:27 step:25937 [D loss: 0.650710, acc: 61.72%] [G loss: 1.969200]\n",
      "epoch:27 step:25938 [D loss: 0.707766, acc: 55.47%] [G loss: 1.929558]\n",
      "epoch:27 step:25939 [D loss: 0.662776, acc: 64.06%] [G loss: 1.819139]\n",
      "epoch:27 step:25940 [D loss: 0.660460, acc: 62.50%] [G loss: 1.904680]\n",
      "epoch:27 step:25941 [D loss: 0.595182, acc: 63.28%] [G loss: 1.956322]\n",
      "epoch:27 step:25942 [D loss: 0.669671, acc: 64.84%] [G loss: 1.881052]\n",
      "epoch:27 step:25943 [D loss: 0.606281, acc: 68.75%] [G loss: 2.022419]\n",
      "epoch:27 step:25944 [D loss: 0.624343, acc: 61.72%] [G loss: 2.028166]\n",
      "epoch:27 step:25945 [D loss: 0.618949, acc: 67.19%] [G loss: 2.107791]\n",
      "epoch:27 step:25946 [D loss: 0.628109, acc: 62.50%] [G loss: 2.233468]\n",
      "epoch:27 step:25947 [D loss: 0.559695, acc: 69.53%] [G loss: 2.326320]\n",
      "epoch:27 step:25948 [D loss: 0.611461, acc: 69.53%] [G loss: 1.986991]\n",
      "epoch:27 step:25949 [D loss: 0.637544, acc: 60.16%] [G loss: 2.349255]\n",
      "epoch:27 step:25950 [D loss: 0.665562, acc: 57.81%] [G loss: 1.981087]\n",
      "epoch:27 step:25951 [D loss: 0.718845, acc: 57.81%] [G loss: 1.844454]\n",
      "epoch:27 step:25952 [D loss: 0.582320, acc: 67.19%] [G loss: 1.891630]\n",
      "epoch:27 step:25953 [D loss: 0.663897, acc: 57.03%] [G loss: 1.998491]\n",
      "epoch:27 step:25954 [D loss: 0.689412, acc: 54.69%] [G loss: 1.860094]\n",
      "epoch:27 step:25955 [D loss: 0.626474, acc: 62.50%] [G loss: 1.889302]\n",
      "epoch:27 step:25956 [D loss: 0.683072, acc: 53.12%] [G loss: 1.814034]\n",
      "epoch:27 step:25957 [D loss: 0.714936, acc: 53.91%] [G loss: 1.750496]\n",
      "epoch:27 step:25958 [D loss: 0.628585, acc: 66.41%] [G loss: 1.936284]\n",
      "epoch:27 step:25959 [D loss: 0.609700, acc: 66.41%] [G loss: 1.884207]\n",
      "epoch:27 step:25960 [D loss: 0.632312, acc: 64.06%] [G loss: 1.784842]\n",
      "epoch:27 step:25961 [D loss: 0.657016, acc: 59.38%] [G loss: 1.964756]\n",
      "epoch:27 step:25962 [D loss: 0.708017, acc: 55.47%] [G loss: 1.836938]\n",
      "epoch:27 step:25963 [D loss: 0.671595, acc: 60.94%] [G loss: 1.781200]\n",
      "epoch:27 step:25964 [D loss: 0.656084, acc: 59.38%] [G loss: 1.824328]\n",
      "epoch:27 step:25965 [D loss: 0.667151, acc: 59.38%] [G loss: 1.791458]\n",
      "epoch:27 step:25966 [D loss: 0.667339, acc: 61.72%] [G loss: 1.759695]\n",
      "epoch:27 step:25967 [D loss: 0.607275, acc: 64.06%] [G loss: 1.820152]\n",
      "epoch:27 step:25968 [D loss: 0.686413, acc: 51.56%] [G loss: 1.794044]\n",
      "epoch:27 step:25969 [D loss: 0.619052, acc: 64.84%] [G loss: 1.899278]\n",
      "epoch:27 step:25970 [D loss: 0.642489, acc: 61.72%] [G loss: 1.772467]\n",
      "epoch:27 step:25971 [D loss: 0.662099, acc: 61.72%] [G loss: 1.980923]\n",
      "epoch:27 step:25972 [D loss: 0.692591, acc: 58.59%] [G loss: 1.948454]\n",
      "epoch:27 step:25973 [D loss: 0.688070, acc: 59.38%] [G loss: 1.896199]\n",
      "epoch:27 step:25974 [D loss: 0.644372, acc: 60.16%] [G loss: 1.746840]\n",
      "epoch:27 step:25975 [D loss: 0.618739, acc: 67.19%] [G loss: 1.827683]\n",
      "epoch:27 step:25976 [D loss: 0.610040, acc: 64.84%] [G loss: 1.906747]\n",
      "epoch:27 step:25977 [D loss: 0.638162, acc: 65.62%] [G loss: 1.938342]\n",
      "epoch:27 step:25978 [D loss: 0.658003, acc: 61.72%] [G loss: 1.922071]\n",
      "epoch:27 step:25979 [D loss: 0.642890, acc: 61.72%] [G loss: 1.953861]\n",
      "epoch:27 step:25980 [D loss: 0.579154, acc: 70.31%] [G loss: 2.073362]\n",
      "epoch:27 step:25981 [D loss: 0.755574, acc: 51.56%] [G loss: 1.816114]\n",
      "epoch:27 step:25982 [D loss: 0.686149, acc: 57.03%] [G loss: 1.826253]\n",
      "epoch:27 step:25983 [D loss: 0.631771, acc: 65.62%] [G loss: 1.788711]\n",
      "epoch:27 step:25984 [D loss: 0.669937, acc: 62.50%] [G loss: 1.907547]\n",
      "epoch:27 step:25985 [D loss: 0.652374, acc: 64.06%] [G loss: 1.810291]\n",
      "epoch:27 step:25986 [D loss: 0.663637, acc: 57.81%] [G loss: 1.836476]\n",
      "epoch:27 step:25987 [D loss: 0.608092, acc: 66.41%] [G loss: 1.940166]\n",
      "epoch:27 step:25988 [D loss: 0.612935, acc: 69.53%] [G loss: 1.966761]\n",
      "epoch:27 step:25989 [D loss: 0.646286, acc: 61.72%] [G loss: 1.955527]\n",
      "epoch:27 step:25990 [D loss: 0.547488, acc: 74.22%] [G loss: 2.130178]\n",
      "epoch:27 step:25991 [D loss: 0.677434, acc: 59.38%] [G loss: 2.031619]\n",
      "epoch:27 step:25992 [D loss: 0.632696, acc: 63.28%] [G loss: 2.131509]\n",
      "epoch:27 step:25993 [D loss: 0.592611, acc: 65.62%] [G loss: 2.130253]\n",
      "epoch:27 step:25994 [D loss: 0.688254, acc: 57.03%] [G loss: 2.031374]\n",
      "epoch:27 step:25995 [D loss: 0.640538, acc: 64.06%] [G loss: 1.792546]\n",
      "epoch:27 step:25996 [D loss: 0.588305, acc: 71.09%] [G loss: 1.806714]\n",
      "epoch:27 step:25997 [D loss: 0.633104, acc: 61.72%] [G loss: 1.773144]\n",
      "epoch:27 step:25998 [D loss: 0.651269, acc: 62.50%] [G loss: 1.906624]\n",
      "epoch:27 step:25999 [D loss: 0.674086, acc: 57.03%] [G loss: 1.785259]\n",
      "epoch:27 step:26000 [D loss: 0.599508, acc: 67.19%] [G loss: 2.034362]\n",
      "##############\n",
      "[2.64465798 1.81476083 6.32726168 4.84269628 3.67228874 5.74903665\n",
      " 4.47584334 4.78036089 4.69518469 3.79817714]\n",
      "##########\n",
      "epoch:27 step:26001 [D loss: 0.662100, acc: 65.62%] [G loss: 1.858346]\n",
      "epoch:27 step:26002 [D loss: 0.687996, acc: 55.47%] [G loss: 1.883063]\n",
      "epoch:27 step:26003 [D loss: 0.653729, acc: 59.38%] [G loss: 1.827266]\n",
      "epoch:27 step:26004 [D loss: 0.678565, acc: 63.28%] [G loss: 1.884562]\n",
      "epoch:27 step:26005 [D loss: 0.607268, acc: 67.19%] [G loss: 1.901438]\n",
      "epoch:27 step:26006 [D loss: 0.644186, acc: 65.62%] [G loss: 1.891557]\n",
      "epoch:27 step:26007 [D loss: 0.643626, acc: 64.84%] [G loss: 2.040577]\n",
      "epoch:27 step:26008 [D loss: 0.580941, acc: 70.31%] [G loss: 1.968197]\n",
      "epoch:27 step:26009 [D loss: 0.650877, acc: 66.41%] [G loss: 1.888169]\n",
      "epoch:27 step:26010 [D loss: 0.653188, acc: 64.06%] [G loss: 1.784519]\n",
      "epoch:27 step:26011 [D loss: 0.668002, acc: 58.59%] [G loss: 1.872402]\n",
      "epoch:27 step:26012 [D loss: 0.689535, acc: 58.59%] [G loss: 1.800058]\n",
      "epoch:27 step:26013 [D loss: 0.638405, acc: 60.16%] [G loss: 1.848962]\n",
      "epoch:27 step:26014 [D loss: 0.647284, acc: 64.84%] [G loss: 1.831008]\n",
      "epoch:27 step:26015 [D loss: 0.698236, acc: 51.56%] [G loss: 1.855721]\n",
      "epoch:27 step:26016 [D loss: 0.697924, acc: 57.03%] [G loss: 1.822754]\n",
      "epoch:27 step:26017 [D loss: 0.657803, acc: 60.94%] [G loss: 1.877363]\n",
      "epoch:27 step:26018 [D loss: 0.602622, acc: 67.97%] [G loss: 1.941925]\n",
      "epoch:27 step:26019 [D loss: 0.601819, acc: 64.06%] [G loss: 2.061439]\n",
      "epoch:27 step:26020 [D loss: 0.649470, acc: 65.62%] [G loss: 2.003458]\n",
      "epoch:27 step:26021 [D loss: 0.714966, acc: 54.69%] [G loss: 1.754460]\n",
      "epoch:27 step:26022 [D loss: 0.676279, acc: 58.59%] [G loss: 1.804206]\n",
      "epoch:27 step:26023 [D loss: 0.677349, acc: 58.59%] [G loss: 1.909768]\n",
      "epoch:27 step:26024 [D loss: 0.687752, acc: 55.47%] [G loss: 2.051748]\n",
      "epoch:27 step:26025 [D loss: 0.628393, acc: 67.19%] [G loss: 2.008680]\n",
      "epoch:27 step:26026 [D loss: 0.651887, acc: 64.06%] [G loss: 1.868590]\n",
      "epoch:27 step:26027 [D loss: 0.673225, acc: 57.81%] [G loss: 1.939235]\n",
      "epoch:27 step:26028 [D loss: 0.632897, acc: 65.62%] [G loss: 1.760921]\n",
      "epoch:27 step:26029 [D loss: 0.620163, acc: 64.84%] [G loss: 1.820931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26030 [D loss: 0.654479, acc: 57.03%] [G loss: 1.902035]\n",
      "epoch:27 step:26031 [D loss: 0.673216, acc: 60.94%] [G loss: 1.693699]\n",
      "epoch:27 step:26032 [D loss: 0.640502, acc: 62.50%] [G loss: 1.882336]\n",
      "epoch:27 step:26033 [D loss: 0.639146, acc: 62.50%] [G loss: 1.790280]\n",
      "epoch:27 step:26034 [D loss: 0.671984, acc: 57.81%] [G loss: 1.864047]\n",
      "epoch:27 step:26035 [D loss: 0.592055, acc: 72.66%] [G loss: 1.903652]\n",
      "epoch:27 step:26036 [D loss: 0.636189, acc: 57.81%] [G loss: 1.937893]\n",
      "epoch:27 step:26037 [D loss: 0.629860, acc: 63.28%] [G loss: 1.825663]\n",
      "epoch:27 step:26038 [D loss: 0.648041, acc: 59.38%] [G loss: 1.818830]\n",
      "epoch:27 step:26039 [D loss: 0.592626, acc: 68.75%] [G loss: 1.987319]\n",
      "epoch:27 step:26040 [D loss: 0.682063, acc: 57.81%] [G loss: 1.728311]\n",
      "epoch:27 step:26041 [D loss: 0.673420, acc: 64.06%] [G loss: 1.844371]\n",
      "epoch:27 step:26042 [D loss: 0.675094, acc: 60.94%] [G loss: 1.777907]\n",
      "epoch:27 step:26043 [D loss: 0.622025, acc: 67.97%] [G loss: 1.923887]\n",
      "epoch:27 step:26044 [D loss: 0.653165, acc: 59.38%] [G loss: 1.878632]\n",
      "epoch:27 step:26045 [D loss: 0.621012, acc: 67.97%] [G loss: 1.953389]\n",
      "epoch:27 step:26046 [D loss: 0.628313, acc: 64.84%] [G loss: 2.026364]\n",
      "epoch:27 step:26047 [D loss: 0.688831, acc: 57.81%] [G loss: 1.821403]\n",
      "epoch:27 step:26048 [D loss: 0.620767, acc: 66.41%] [G loss: 1.863082]\n",
      "epoch:27 step:26049 [D loss: 0.664534, acc: 62.50%] [G loss: 1.936792]\n",
      "epoch:27 step:26050 [D loss: 0.668661, acc: 56.25%] [G loss: 1.865085]\n",
      "epoch:27 step:26051 [D loss: 0.686731, acc: 56.25%] [G loss: 1.700994]\n",
      "epoch:27 step:26052 [D loss: 0.651863, acc: 57.81%] [G loss: 1.908715]\n",
      "epoch:27 step:26053 [D loss: 0.616648, acc: 67.97%] [G loss: 1.842905]\n",
      "epoch:27 step:26054 [D loss: 0.655326, acc: 60.94%] [G loss: 1.926046]\n",
      "epoch:27 step:26055 [D loss: 0.622182, acc: 64.84%] [G loss: 1.805623]\n",
      "epoch:27 step:26056 [D loss: 0.638482, acc: 67.19%] [G loss: 1.889523]\n",
      "epoch:27 step:26057 [D loss: 0.638277, acc: 62.50%] [G loss: 1.889666]\n",
      "epoch:27 step:26058 [D loss: 0.641028, acc: 66.41%] [G loss: 1.853299]\n",
      "epoch:27 step:26059 [D loss: 0.698340, acc: 56.25%] [G loss: 1.890677]\n",
      "epoch:27 step:26060 [D loss: 0.648828, acc: 64.84%] [G loss: 1.877722]\n",
      "epoch:27 step:26061 [D loss: 0.633874, acc: 60.16%] [G loss: 1.773962]\n",
      "epoch:27 step:26062 [D loss: 0.639275, acc: 65.62%] [G loss: 2.001194]\n",
      "epoch:27 step:26063 [D loss: 0.664303, acc: 58.59%] [G loss: 1.829577]\n",
      "epoch:27 step:26064 [D loss: 0.692311, acc: 54.69%] [G loss: 1.755838]\n",
      "epoch:27 step:26065 [D loss: 0.709089, acc: 57.81%] [G loss: 1.806020]\n",
      "epoch:27 step:26066 [D loss: 0.684123, acc: 59.38%] [G loss: 1.800749]\n",
      "epoch:27 step:26067 [D loss: 0.649502, acc: 60.94%] [G loss: 1.869371]\n",
      "epoch:27 step:26068 [D loss: 0.694210, acc: 54.69%] [G loss: 1.888101]\n",
      "epoch:27 step:26069 [D loss: 0.585846, acc: 75.78%] [G loss: 1.857895]\n",
      "epoch:27 step:26070 [D loss: 0.582664, acc: 71.09%] [G loss: 1.824249]\n",
      "epoch:27 step:26071 [D loss: 0.644637, acc: 64.84%] [G loss: 1.849857]\n",
      "epoch:27 step:26072 [D loss: 0.664892, acc: 60.16%] [G loss: 1.907485]\n",
      "epoch:27 step:26073 [D loss: 0.619331, acc: 64.84%] [G loss: 2.212142]\n",
      "epoch:27 step:26074 [D loss: 0.652888, acc: 66.41%] [G loss: 2.165478]\n",
      "epoch:27 step:26075 [D loss: 0.643317, acc: 66.41%] [G loss: 1.995642]\n",
      "epoch:27 step:26076 [D loss: 0.678990, acc: 58.59%] [G loss: 1.979796]\n",
      "epoch:27 step:26077 [D loss: 0.660052, acc: 55.47%] [G loss: 1.874473]\n",
      "epoch:27 step:26078 [D loss: 0.637965, acc: 61.72%] [G loss: 1.849763]\n",
      "epoch:27 step:26079 [D loss: 0.645045, acc: 62.50%] [G loss: 1.904039]\n",
      "epoch:27 step:26080 [D loss: 0.607443, acc: 64.84%] [G loss: 2.077066]\n",
      "epoch:27 step:26081 [D loss: 0.582212, acc: 70.31%] [G loss: 2.079027]\n",
      "epoch:27 step:26082 [D loss: 0.608721, acc: 66.41%] [G loss: 1.954162]\n",
      "epoch:27 step:26083 [D loss: 0.767645, acc: 50.78%] [G loss: 1.783796]\n",
      "epoch:27 step:26084 [D loss: 0.667191, acc: 61.72%] [G loss: 2.032316]\n",
      "epoch:27 step:26085 [D loss: 0.613771, acc: 64.84%] [G loss: 1.991635]\n",
      "epoch:27 step:26086 [D loss: 0.590812, acc: 67.19%] [G loss: 1.843376]\n",
      "epoch:27 step:26087 [D loss: 0.726780, acc: 50.78%] [G loss: 1.821923]\n",
      "epoch:27 step:26088 [D loss: 0.649927, acc: 65.62%] [G loss: 1.974514]\n",
      "epoch:27 step:26089 [D loss: 0.666236, acc: 62.50%] [G loss: 1.980829]\n",
      "epoch:27 step:26090 [D loss: 0.621610, acc: 66.41%] [G loss: 2.112419]\n",
      "epoch:27 step:26091 [D loss: 0.645261, acc: 63.28%] [G loss: 1.971792]\n",
      "epoch:27 step:26092 [D loss: 0.646137, acc: 59.38%] [G loss: 2.015461]\n",
      "epoch:27 step:26093 [D loss: 0.722410, acc: 50.78%] [G loss: 1.760137]\n",
      "epoch:27 step:26094 [D loss: 0.661425, acc: 63.28%] [G loss: 1.775418]\n",
      "epoch:27 step:26095 [D loss: 0.604710, acc: 69.53%] [G loss: 1.874632]\n",
      "epoch:27 step:26096 [D loss: 0.681064, acc: 59.38%] [G loss: 1.817301]\n",
      "epoch:27 step:26097 [D loss: 0.632876, acc: 64.06%] [G loss: 1.890184]\n",
      "epoch:27 step:26098 [D loss: 0.658790, acc: 63.28%] [G loss: 1.696596]\n",
      "epoch:27 step:26099 [D loss: 0.730616, acc: 51.56%] [G loss: 1.677433]\n",
      "epoch:27 step:26100 [D loss: 0.711668, acc: 50.78%] [G loss: 1.742861]\n",
      "epoch:27 step:26101 [D loss: 0.666130, acc: 62.50%] [G loss: 1.873437]\n",
      "epoch:27 step:26102 [D loss: 0.707891, acc: 57.03%] [G loss: 1.822873]\n",
      "epoch:27 step:26103 [D loss: 0.594539, acc: 71.88%] [G loss: 1.851622]\n",
      "epoch:27 step:26104 [D loss: 0.620448, acc: 62.50%] [G loss: 1.888808]\n",
      "epoch:27 step:26105 [D loss: 0.641651, acc: 62.50%] [G loss: 1.860543]\n",
      "epoch:27 step:26106 [D loss: 0.628259, acc: 61.72%] [G loss: 2.019281]\n",
      "epoch:27 step:26107 [D loss: 0.658118, acc: 63.28%] [G loss: 1.779794]\n",
      "epoch:27 step:26108 [D loss: 0.631540, acc: 66.41%] [G loss: 1.844784]\n",
      "epoch:27 step:26109 [D loss: 0.628381, acc: 66.41%] [G loss: 1.852998]\n",
      "epoch:27 step:26110 [D loss: 0.650549, acc: 65.62%] [G loss: 1.909667]\n",
      "epoch:27 step:26111 [D loss: 0.648493, acc: 60.16%] [G loss: 1.785368]\n",
      "epoch:27 step:26112 [D loss: 0.684961, acc: 51.56%] [G loss: 1.851608]\n",
      "epoch:27 step:26113 [D loss: 0.621529, acc: 66.41%] [G loss: 1.840795]\n",
      "epoch:27 step:26114 [D loss: 0.633887, acc: 60.94%] [G loss: 2.115185]\n",
      "epoch:27 step:26115 [D loss: 0.593299, acc: 68.75%] [G loss: 2.172526]\n",
      "epoch:27 step:26116 [D loss: 0.642362, acc: 71.88%] [G loss: 1.808975]\n",
      "epoch:27 step:26117 [D loss: 0.702443, acc: 55.47%] [G loss: 1.777931]\n",
      "epoch:27 step:26118 [D loss: 0.639311, acc: 59.38%] [G loss: 1.810582]\n",
      "epoch:27 step:26119 [D loss: 0.689311, acc: 55.47%] [G loss: 1.675208]\n",
      "epoch:27 step:26120 [D loss: 0.654923, acc: 61.72%] [G loss: 1.961166]\n",
      "epoch:27 step:26121 [D loss: 0.681859, acc: 55.47%] [G loss: 1.772566]\n",
      "epoch:27 step:26122 [D loss: 0.621880, acc: 65.62%] [G loss: 2.002016]\n",
      "epoch:27 step:26123 [D loss: 0.656706, acc: 60.94%] [G loss: 1.892691]\n",
      "epoch:27 step:26124 [D loss: 0.650396, acc: 63.28%] [G loss: 1.935627]\n",
      "epoch:27 step:26125 [D loss: 0.636751, acc: 59.38%] [G loss: 1.970172]\n",
      "epoch:27 step:26126 [D loss: 0.665321, acc: 62.50%] [G loss: 1.823407]\n",
      "epoch:27 step:26127 [D loss: 0.624275, acc: 62.50%] [G loss: 1.877819]\n",
      "epoch:27 step:26128 [D loss: 0.687948, acc: 59.38%] [G loss: 1.851887]\n",
      "epoch:27 step:26129 [D loss: 0.661575, acc: 58.59%] [G loss: 1.861578]\n",
      "epoch:27 step:26130 [D loss: 0.693980, acc: 60.16%] [G loss: 1.859986]\n",
      "epoch:27 step:26131 [D loss: 0.638003, acc: 64.84%] [G loss: 1.799395]\n",
      "epoch:27 step:26132 [D loss: 0.615268, acc: 66.41%] [G loss: 1.871785]\n",
      "epoch:27 step:26133 [D loss: 0.657865, acc: 65.62%] [G loss: 1.752756]\n",
      "epoch:27 step:26134 [D loss: 0.648245, acc: 63.28%] [G loss: 2.016690]\n",
      "epoch:27 step:26135 [D loss: 0.665325, acc: 53.12%] [G loss: 1.856611]\n",
      "epoch:27 step:26136 [D loss: 0.675240, acc: 57.81%] [G loss: 1.986555]\n",
      "epoch:27 step:26137 [D loss: 0.665525, acc: 60.16%] [G loss: 1.825314]\n",
      "epoch:27 step:26138 [D loss: 0.630608, acc: 60.16%] [G loss: 1.844725]\n",
      "epoch:27 step:26139 [D loss: 0.663566, acc: 63.28%] [G loss: 1.967819]\n",
      "epoch:27 step:26140 [D loss: 0.606647, acc: 71.09%] [G loss: 1.851445]\n",
      "epoch:27 step:26141 [D loss: 0.605357, acc: 69.53%] [G loss: 2.009968]\n",
      "epoch:27 step:26142 [D loss: 0.589602, acc: 64.84%] [G loss: 1.982405]\n",
      "epoch:27 step:26143 [D loss: 0.663561, acc: 62.50%] [G loss: 1.916498]\n",
      "epoch:27 step:26144 [D loss: 0.655728, acc: 61.72%] [G loss: 1.953892]\n",
      "epoch:27 step:26145 [D loss: 0.671789, acc: 62.50%] [G loss: 1.969095]\n",
      "epoch:27 step:26146 [D loss: 0.603372, acc: 66.41%] [G loss: 1.877573]\n",
      "epoch:27 step:26147 [D loss: 0.633761, acc: 66.41%] [G loss: 1.750689]\n",
      "epoch:27 step:26148 [D loss: 0.669707, acc: 57.81%] [G loss: 1.993117]\n",
      "epoch:27 step:26149 [D loss: 0.665059, acc: 58.59%] [G loss: 1.755690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26150 [D loss: 0.683355, acc: 57.03%] [G loss: 1.727314]\n",
      "epoch:27 step:26151 [D loss: 0.631147, acc: 60.16%] [G loss: 1.890211]\n",
      "epoch:27 step:26152 [D loss: 0.719301, acc: 52.34%] [G loss: 1.904306]\n",
      "epoch:27 step:26153 [D loss: 0.667914, acc: 63.28%] [G loss: 1.806752]\n",
      "epoch:27 step:26154 [D loss: 0.666338, acc: 60.94%] [G loss: 1.825468]\n",
      "epoch:27 step:26155 [D loss: 0.650795, acc: 62.50%] [G loss: 1.770820]\n",
      "epoch:27 step:26156 [D loss: 0.663237, acc: 64.06%] [G loss: 1.984016]\n",
      "epoch:27 step:26157 [D loss: 0.673043, acc: 59.38%] [G loss: 1.819179]\n",
      "epoch:27 step:26158 [D loss: 0.648392, acc: 60.16%] [G loss: 1.831945]\n",
      "epoch:27 step:26159 [D loss: 0.579116, acc: 71.88%] [G loss: 1.828316]\n",
      "epoch:27 step:26160 [D loss: 0.617223, acc: 69.53%] [G loss: 1.902883]\n",
      "epoch:27 step:26161 [D loss: 0.677081, acc: 61.72%] [G loss: 1.811297]\n",
      "epoch:27 step:26162 [D loss: 0.600204, acc: 66.41%] [G loss: 1.920809]\n",
      "epoch:27 step:26163 [D loss: 0.644621, acc: 60.94%] [G loss: 1.704764]\n",
      "epoch:27 step:26164 [D loss: 0.601926, acc: 68.75%] [G loss: 1.836137]\n",
      "epoch:27 step:26165 [D loss: 0.572709, acc: 72.66%] [G loss: 2.021993]\n",
      "epoch:27 step:26166 [D loss: 0.661009, acc: 57.81%] [G loss: 1.775728]\n",
      "epoch:27 step:26167 [D loss: 0.626543, acc: 67.19%] [G loss: 2.067444]\n",
      "epoch:27 step:26168 [D loss: 0.638875, acc: 63.28%] [G loss: 1.810023]\n",
      "epoch:27 step:26169 [D loss: 0.629415, acc: 62.50%] [G loss: 1.924114]\n",
      "epoch:27 step:26170 [D loss: 0.677558, acc: 60.16%] [G loss: 1.986853]\n",
      "epoch:27 step:26171 [D loss: 0.682097, acc: 57.03%] [G loss: 1.857365]\n",
      "epoch:27 step:26172 [D loss: 0.678391, acc: 61.72%] [G loss: 1.904290]\n",
      "epoch:27 step:26173 [D loss: 0.595962, acc: 71.09%] [G loss: 1.935341]\n",
      "epoch:27 step:26174 [D loss: 0.636553, acc: 62.50%] [G loss: 1.979754]\n",
      "epoch:27 step:26175 [D loss: 0.632224, acc: 60.94%] [G loss: 1.930487]\n",
      "epoch:27 step:26176 [D loss: 0.606231, acc: 69.53%] [G loss: 1.856082]\n",
      "epoch:27 step:26177 [D loss: 0.687175, acc: 53.91%] [G loss: 1.889442]\n",
      "epoch:27 step:26178 [D loss: 0.648228, acc: 59.38%] [G loss: 1.948827]\n",
      "epoch:27 step:26179 [D loss: 0.636946, acc: 63.28%] [G loss: 2.119848]\n",
      "epoch:27 step:26180 [D loss: 0.649301, acc: 60.94%] [G loss: 1.853648]\n",
      "epoch:27 step:26181 [D loss: 0.658478, acc: 58.59%] [G loss: 1.985424]\n",
      "epoch:27 step:26182 [D loss: 0.653439, acc: 62.50%] [G loss: 1.872976]\n",
      "epoch:27 step:26183 [D loss: 0.624969, acc: 64.06%] [G loss: 2.012269]\n",
      "epoch:27 step:26184 [D loss: 0.623598, acc: 64.06%] [G loss: 1.933227]\n",
      "epoch:27 step:26185 [D loss: 0.615057, acc: 66.41%] [G loss: 1.984187]\n",
      "epoch:27 step:26186 [D loss: 0.715121, acc: 51.56%] [G loss: 1.921498]\n",
      "epoch:27 step:26187 [D loss: 0.676188, acc: 58.59%] [G loss: 1.877232]\n",
      "epoch:27 step:26188 [D loss: 0.608976, acc: 64.84%] [G loss: 1.920974]\n",
      "epoch:27 step:26189 [D loss: 0.656354, acc: 60.16%] [G loss: 1.949274]\n",
      "epoch:27 step:26190 [D loss: 0.632403, acc: 68.75%] [G loss: 1.808053]\n",
      "epoch:27 step:26191 [D loss: 0.693265, acc: 60.94%] [G loss: 1.878645]\n",
      "epoch:27 step:26192 [D loss: 0.667943, acc: 62.50%] [G loss: 2.066631]\n",
      "epoch:27 step:26193 [D loss: 0.662735, acc: 57.03%] [G loss: 1.931919]\n",
      "epoch:27 step:26194 [D loss: 0.671191, acc: 60.94%] [G loss: 1.810126]\n",
      "epoch:27 step:26195 [D loss: 0.638390, acc: 60.94%] [G loss: 1.848757]\n",
      "epoch:27 step:26196 [D loss: 0.671943, acc: 60.16%] [G loss: 1.971485]\n",
      "epoch:27 step:26197 [D loss: 0.695235, acc: 57.81%] [G loss: 1.900276]\n",
      "epoch:27 step:26198 [D loss: 0.646008, acc: 59.38%] [G loss: 1.817776]\n",
      "epoch:27 step:26199 [D loss: 0.656917, acc: 61.72%] [G loss: 1.844372]\n",
      "epoch:27 step:26200 [D loss: 0.658790, acc: 57.81%] [G loss: 1.989435]\n",
      "##############\n",
      "[2.6018015  1.63041699 6.14400953 4.82444233 3.50960437 5.34970702\n",
      " 4.38612995 4.61069466 4.6128527  3.59880041]\n",
      "##########\n",
      "epoch:27 step:26201 [D loss: 0.688056, acc: 54.69%] [G loss: 1.871694]\n",
      "epoch:27 step:26202 [D loss: 0.625915, acc: 64.84%] [G loss: 1.847784]\n",
      "epoch:27 step:26203 [D loss: 0.658982, acc: 61.72%] [G loss: 2.048476]\n",
      "epoch:27 step:26204 [D loss: 0.635173, acc: 63.28%] [G loss: 2.004108]\n",
      "epoch:27 step:26205 [D loss: 0.671805, acc: 59.38%] [G loss: 1.813996]\n",
      "epoch:27 step:26206 [D loss: 0.635486, acc: 65.62%] [G loss: 1.804246]\n",
      "epoch:27 step:26207 [D loss: 0.669707, acc: 61.72%] [G loss: 1.885916]\n",
      "epoch:27 step:26208 [D loss: 0.673032, acc: 58.59%] [G loss: 2.086542]\n",
      "epoch:27 step:26209 [D loss: 0.647060, acc: 61.72%] [G loss: 2.010143]\n",
      "epoch:27 step:26210 [D loss: 0.601311, acc: 67.19%] [G loss: 1.831861]\n",
      "epoch:27 step:26211 [D loss: 0.671807, acc: 57.03%] [G loss: 1.976032]\n",
      "epoch:27 step:26212 [D loss: 0.688225, acc: 53.91%] [G loss: 1.814667]\n",
      "epoch:27 step:26213 [D loss: 0.681365, acc: 54.69%] [G loss: 1.866213]\n",
      "epoch:27 step:26214 [D loss: 0.669353, acc: 60.94%] [G loss: 1.660647]\n",
      "epoch:27 step:26215 [D loss: 0.613030, acc: 65.62%] [G loss: 1.916684]\n",
      "epoch:27 step:26216 [D loss: 0.675503, acc: 58.59%] [G loss: 1.913771]\n",
      "epoch:27 step:26217 [D loss: 0.612345, acc: 65.62%] [G loss: 2.081035]\n",
      "epoch:27 step:26218 [D loss: 0.569089, acc: 70.31%] [G loss: 2.108781]\n",
      "epoch:27 step:26219 [D loss: 0.708249, acc: 53.91%] [G loss: 1.796525]\n",
      "epoch:27 step:26220 [D loss: 0.668826, acc: 62.50%] [G loss: 1.854731]\n",
      "epoch:27 step:26221 [D loss: 0.622994, acc: 64.06%] [G loss: 1.845246]\n",
      "epoch:27 step:26222 [D loss: 0.622375, acc: 68.75%] [G loss: 2.170470]\n",
      "epoch:27 step:26223 [D loss: 0.582264, acc: 71.88%] [G loss: 2.020275]\n",
      "epoch:27 step:26224 [D loss: 0.644764, acc: 62.50%] [G loss: 2.133601]\n",
      "epoch:27 step:26225 [D loss: 0.592834, acc: 67.19%] [G loss: 2.156303]\n",
      "epoch:27 step:26226 [D loss: 0.656635, acc: 62.50%] [G loss: 1.951438]\n",
      "epoch:27 step:26227 [D loss: 0.806029, acc: 49.22%] [G loss: 1.747110]\n",
      "epoch:27 step:26228 [D loss: 0.747308, acc: 45.31%] [G loss: 1.917720]\n",
      "epoch:27 step:26229 [D loss: 0.602991, acc: 69.53%] [G loss: 2.051937]\n",
      "epoch:27 step:26230 [D loss: 0.609884, acc: 64.06%] [G loss: 1.971079]\n",
      "epoch:27 step:26231 [D loss: 0.668924, acc: 57.03%] [G loss: 1.823475]\n",
      "epoch:27 step:26232 [D loss: 0.601179, acc: 66.41%] [G loss: 1.870138]\n",
      "epoch:27 step:26233 [D loss: 0.650880, acc: 62.50%] [G loss: 1.858981]\n",
      "epoch:27 step:26234 [D loss: 0.590268, acc: 72.66%] [G loss: 2.100671]\n",
      "epoch:27 step:26235 [D loss: 0.687357, acc: 60.94%] [G loss: 2.015574]\n",
      "epoch:27 step:26236 [D loss: 0.572769, acc: 74.22%] [G loss: 2.297574]\n",
      "epoch:28 step:26237 [D loss: 0.694559, acc: 57.03%] [G loss: 1.856266]\n",
      "epoch:28 step:26238 [D loss: 0.670689, acc: 60.16%] [G loss: 1.904947]\n",
      "epoch:28 step:26239 [D loss: 0.658754, acc: 61.72%] [G loss: 1.905613]\n",
      "epoch:28 step:26240 [D loss: 0.692285, acc: 55.47%] [G loss: 1.884472]\n",
      "epoch:28 step:26241 [D loss: 0.656453, acc: 59.38%] [G loss: 1.786633]\n",
      "epoch:28 step:26242 [D loss: 0.629289, acc: 60.16%] [G loss: 1.907936]\n",
      "epoch:28 step:26243 [D loss: 0.636880, acc: 64.84%] [G loss: 1.872492]\n",
      "epoch:28 step:26244 [D loss: 0.646029, acc: 60.94%] [G loss: 1.945393]\n",
      "epoch:28 step:26245 [D loss: 0.588497, acc: 68.75%] [G loss: 2.033099]\n",
      "epoch:28 step:26246 [D loss: 0.661484, acc: 62.50%] [G loss: 2.078368]\n",
      "epoch:28 step:26247 [D loss: 0.620848, acc: 67.97%] [G loss: 1.885629]\n",
      "epoch:28 step:26248 [D loss: 0.706560, acc: 59.38%] [G loss: 1.853304]\n",
      "epoch:28 step:26249 [D loss: 0.674681, acc: 60.16%] [G loss: 1.860019]\n",
      "epoch:28 step:26250 [D loss: 0.615471, acc: 71.09%] [G loss: 1.980816]\n",
      "epoch:28 step:26251 [D loss: 0.598884, acc: 71.88%] [G loss: 2.166060]\n",
      "epoch:28 step:26252 [D loss: 0.618622, acc: 68.75%] [G loss: 2.059941]\n",
      "epoch:28 step:26253 [D loss: 0.632706, acc: 65.62%] [G loss: 1.994624]\n",
      "epoch:28 step:26254 [D loss: 0.659761, acc: 61.72%] [G loss: 1.987788]\n",
      "epoch:28 step:26255 [D loss: 0.654817, acc: 60.16%] [G loss: 1.887213]\n",
      "epoch:28 step:26256 [D loss: 0.755247, acc: 47.66%] [G loss: 1.743755]\n",
      "epoch:28 step:26257 [D loss: 0.627184, acc: 60.94%] [G loss: 1.776599]\n",
      "epoch:28 step:26258 [D loss: 0.691205, acc: 54.69%] [G loss: 1.709644]\n",
      "epoch:28 step:26259 [D loss: 0.654282, acc: 62.50%] [G loss: 1.855782]\n",
      "epoch:28 step:26260 [D loss: 0.629918, acc: 64.84%] [G loss: 1.784929]\n",
      "epoch:28 step:26261 [D loss: 0.635446, acc: 64.06%] [G loss: 2.062256]\n",
      "epoch:28 step:26262 [D loss: 0.642421, acc: 62.50%] [G loss: 1.798574]\n",
      "epoch:28 step:26263 [D loss: 0.632912, acc: 64.06%] [G loss: 1.843827]\n",
      "epoch:28 step:26264 [D loss: 0.685572, acc: 60.94%] [G loss: 1.781511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26265 [D loss: 0.656629, acc: 64.06%] [G loss: 1.965013]\n",
      "epoch:28 step:26266 [D loss: 0.625147, acc: 60.16%] [G loss: 1.989981]\n",
      "epoch:28 step:26267 [D loss: 0.683747, acc: 59.38%] [G loss: 1.692636]\n",
      "epoch:28 step:26268 [D loss: 0.713038, acc: 51.56%] [G loss: 1.783380]\n",
      "epoch:28 step:26269 [D loss: 0.620630, acc: 64.06%] [G loss: 1.706891]\n",
      "epoch:28 step:26270 [D loss: 0.641430, acc: 61.72%] [G loss: 1.799703]\n",
      "epoch:28 step:26271 [D loss: 0.647021, acc: 63.28%] [G loss: 1.914329]\n",
      "epoch:28 step:26272 [D loss: 0.613073, acc: 64.06%] [G loss: 1.913748]\n",
      "epoch:28 step:26273 [D loss: 0.609338, acc: 71.09%] [G loss: 2.031726]\n",
      "epoch:28 step:26274 [D loss: 0.694374, acc: 57.81%] [G loss: 1.855445]\n",
      "epoch:28 step:26275 [D loss: 0.661694, acc: 59.38%] [G loss: 1.831191]\n",
      "epoch:28 step:26276 [D loss: 0.613685, acc: 61.72%] [G loss: 2.031171]\n",
      "epoch:28 step:26277 [D loss: 0.656311, acc: 64.06%] [G loss: 1.896488]\n",
      "epoch:28 step:26278 [D loss: 0.639473, acc: 67.97%] [G loss: 1.938134]\n",
      "epoch:28 step:26279 [D loss: 0.703227, acc: 57.81%] [G loss: 2.041486]\n",
      "epoch:28 step:26280 [D loss: 0.647880, acc: 60.16%] [G loss: 1.790159]\n",
      "epoch:28 step:26281 [D loss: 0.672148, acc: 58.59%] [G loss: 1.850375]\n",
      "epoch:28 step:26282 [D loss: 0.661470, acc: 56.25%] [G loss: 1.786947]\n",
      "epoch:28 step:26283 [D loss: 0.621068, acc: 63.28%] [G loss: 1.927364]\n",
      "epoch:28 step:26284 [D loss: 0.649162, acc: 59.38%] [G loss: 1.961130]\n",
      "epoch:28 step:26285 [D loss: 0.575601, acc: 75.78%] [G loss: 2.063610]\n",
      "epoch:28 step:26286 [D loss: 0.630597, acc: 67.19%] [G loss: 2.059174]\n",
      "epoch:28 step:26287 [D loss: 0.634053, acc: 63.28%] [G loss: 1.793117]\n",
      "epoch:28 step:26288 [D loss: 0.686385, acc: 56.25%] [G loss: 1.873294]\n",
      "epoch:28 step:26289 [D loss: 0.621664, acc: 64.06%] [G loss: 1.954357]\n",
      "epoch:28 step:26290 [D loss: 0.624735, acc: 64.06%] [G loss: 1.959578]\n",
      "epoch:28 step:26291 [D loss: 0.629725, acc: 66.41%] [G loss: 1.892215]\n",
      "epoch:28 step:26292 [D loss: 0.610103, acc: 64.06%] [G loss: 2.053501]\n",
      "epoch:28 step:26293 [D loss: 0.661427, acc: 60.94%] [G loss: 1.866924]\n",
      "epoch:28 step:26294 [D loss: 0.635124, acc: 64.84%] [G loss: 1.899220]\n",
      "epoch:28 step:26295 [D loss: 0.691548, acc: 58.59%] [G loss: 1.886854]\n",
      "epoch:28 step:26296 [D loss: 0.700991, acc: 61.72%] [G loss: 1.817696]\n",
      "epoch:28 step:26297 [D loss: 0.658181, acc: 62.50%] [G loss: 1.792396]\n",
      "epoch:28 step:26298 [D loss: 0.641715, acc: 67.19%] [G loss: 1.871003]\n",
      "epoch:28 step:26299 [D loss: 0.645321, acc: 59.38%] [G loss: 1.886881]\n",
      "epoch:28 step:26300 [D loss: 0.681877, acc: 57.81%] [G loss: 2.001984]\n",
      "epoch:28 step:26301 [D loss: 0.620645, acc: 66.41%] [G loss: 1.959464]\n",
      "epoch:28 step:26302 [D loss: 0.634384, acc: 63.28%] [G loss: 1.875094]\n",
      "epoch:28 step:26303 [D loss: 0.688699, acc: 60.16%] [G loss: 1.987774]\n",
      "epoch:28 step:26304 [D loss: 0.665115, acc: 60.16%] [G loss: 1.805544]\n",
      "epoch:28 step:26305 [D loss: 0.633751, acc: 60.94%] [G loss: 1.914997]\n",
      "epoch:28 step:26306 [D loss: 0.637843, acc: 63.28%] [G loss: 2.107731]\n",
      "epoch:28 step:26307 [D loss: 0.660971, acc: 56.25%] [G loss: 1.666097]\n",
      "epoch:28 step:26308 [D loss: 0.661752, acc: 59.38%] [G loss: 1.778802]\n",
      "epoch:28 step:26309 [D loss: 0.672513, acc: 60.16%] [G loss: 1.848825]\n",
      "epoch:28 step:26310 [D loss: 0.631926, acc: 66.41%] [G loss: 1.917541]\n",
      "epoch:28 step:26311 [D loss: 0.644082, acc: 64.84%] [G loss: 2.112001]\n",
      "epoch:28 step:26312 [D loss: 0.625301, acc: 64.84%] [G loss: 1.885677]\n",
      "epoch:28 step:26313 [D loss: 0.608211, acc: 64.84%] [G loss: 2.002648]\n",
      "epoch:28 step:26314 [D loss: 0.676865, acc: 58.59%] [G loss: 1.792194]\n",
      "epoch:28 step:26315 [D loss: 0.653731, acc: 60.16%] [G loss: 1.834220]\n",
      "epoch:28 step:26316 [D loss: 0.691900, acc: 62.50%] [G loss: 1.755973]\n",
      "epoch:28 step:26317 [D loss: 0.691373, acc: 60.16%] [G loss: 1.799382]\n",
      "epoch:28 step:26318 [D loss: 0.656157, acc: 59.38%] [G loss: 1.917374]\n",
      "epoch:28 step:26319 [D loss: 0.678316, acc: 58.59%] [G loss: 1.760123]\n",
      "epoch:28 step:26320 [D loss: 0.653281, acc: 60.16%] [G loss: 1.884248]\n",
      "epoch:28 step:26321 [D loss: 0.663866, acc: 63.28%] [G loss: 1.754450]\n",
      "epoch:28 step:26322 [D loss: 0.691648, acc: 57.03%] [G loss: 1.723697]\n",
      "epoch:28 step:26323 [D loss: 0.698362, acc: 56.25%] [G loss: 1.760978]\n",
      "epoch:28 step:26324 [D loss: 0.646550, acc: 57.81%] [G loss: 1.797743]\n",
      "epoch:28 step:26325 [D loss: 0.647362, acc: 63.28%] [G loss: 1.720344]\n",
      "epoch:28 step:26326 [D loss: 0.661449, acc: 60.94%] [G loss: 1.782517]\n",
      "epoch:28 step:26327 [D loss: 0.655147, acc: 60.16%] [G loss: 1.724023]\n",
      "epoch:28 step:26328 [D loss: 0.666487, acc: 57.03%] [G loss: 1.824282]\n",
      "epoch:28 step:26329 [D loss: 0.626972, acc: 65.62%] [G loss: 1.833376]\n",
      "epoch:28 step:26330 [D loss: 0.615621, acc: 69.53%] [G loss: 1.792378]\n",
      "epoch:28 step:26331 [D loss: 0.657141, acc: 60.94%] [G loss: 1.899963]\n",
      "epoch:28 step:26332 [D loss: 0.668542, acc: 59.38%] [G loss: 1.818260]\n",
      "epoch:28 step:26333 [D loss: 0.655410, acc: 60.16%] [G loss: 1.881961]\n",
      "epoch:28 step:26334 [D loss: 0.630321, acc: 60.94%] [G loss: 1.869918]\n",
      "epoch:28 step:26335 [D loss: 0.704038, acc: 47.66%] [G loss: 1.777449]\n",
      "epoch:28 step:26336 [D loss: 0.619475, acc: 67.97%] [G loss: 1.834153]\n",
      "epoch:28 step:26337 [D loss: 0.639315, acc: 61.72%] [G loss: 1.835457]\n",
      "epoch:28 step:26338 [D loss: 0.690254, acc: 57.03%] [G loss: 1.881315]\n",
      "epoch:28 step:26339 [D loss: 0.651072, acc: 60.94%] [G loss: 1.707989]\n",
      "epoch:28 step:26340 [D loss: 0.624657, acc: 62.50%] [G loss: 1.847572]\n",
      "epoch:28 step:26341 [D loss: 0.715924, acc: 56.25%] [G loss: 1.936616]\n",
      "epoch:28 step:26342 [D loss: 0.624117, acc: 67.97%] [G loss: 2.063569]\n",
      "epoch:28 step:26343 [D loss: 0.614377, acc: 68.75%] [G loss: 1.994667]\n",
      "epoch:28 step:26344 [D loss: 0.702294, acc: 59.38%] [G loss: 1.780455]\n",
      "epoch:28 step:26345 [D loss: 0.692433, acc: 55.47%] [G loss: 1.784796]\n",
      "epoch:28 step:26346 [D loss: 0.632611, acc: 64.84%] [G loss: 1.792665]\n",
      "epoch:28 step:26347 [D loss: 0.595577, acc: 65.62%] [G loss: 1.843027]\n",
      "epoch:28 step:26348 [D loss: 0.631068, acc: 60.16%] [G loss: 2.028900]\n",
      "epoch:28 step:26349 [D loss: 0.646886, acc: 60.94%] [G loss: 1.891787]\n",
      "epoch:28 step:26350 [D loss: 0.666740, acc: 64.84%] [G loss: 1.928742]\n",
      "epoch:28 step:26351 [D loss: 0.631159, acc: 64.06%] [G loss: 1.966208]\n",
      "epoch:28 step:26352 [D loss: 0.594635, acc: 64.84%] [G loss: 2.161853]\n",
      "epoch:28 step:26353 [D loss: 0.659118, acc: 65.62%] [G loss: 2.040177]\n",
      "epoch:28 step:26354 [D loss: 0.691974, acc: 55.47%] [G loss: 1.880921]\n",
      "epoch:28 step:26355 [D loss: 0.639684, acc: 62.50%] [G loss: 1.957306]\n",
      "epoch:28 step:26356 [D loss: 0.678899, acc: 59.38%] [G loss: 1.801858]\n",
      "epoch:28 step:26357 [D loss: 0.638156, acc: 62.50%] [G loss: 1.903494]\n",
      "epoch:28 step:26358 [D loss: 0.632209, acc: 59.38%] [G loss: 2.142436]\n",
      "epoch:28 step:26359 [D loss: 0.670731, acc: 60.94%] [G loss: 1.945100]\n",
      "epoch:28 step:26360 [D loss: 0.708579, acc: 55.47%] [G loss: 1.852073]\n",
      "epoch:28 step:26361 [D loss: 0.713795, acc: 53.12%] [G loss: 1.734713]\n",
      "epoch:28 step:26362 [D loss: 0.610408, acc: 64.06%] [G loss: 1.810126]\n",
      "epoch:28 step:26363 [D loss: 0.658950, acc: 61.72%] [G loss: 1.779913]\n",
      "epoch:28 step:26364 [D loss: 0.639589, acc: 65.62%] [G loss: 1.809617]\n",
      "epoch:28 step:26365 [D loss: 0.628367, acc: 64.84%] [G loss: 1.823588]\n",
      "epoch:28 step:26366 [D loss: 0.611748, acc: 69.53%] [G loss: 1.881959]\n",
      "epoch:28 step:26367 [D loss: 0.642099, acc: 65.62%] [G loss: 1.908411]\n",
      "epoch:28 step:26368 [D loss: 0.693828, acc: 55.47%] [G loss: 1.770481]\n",
      "epoch:28 step:26369 [D loss: 0.643670, acc: 64.06%] [G loss: 1.706051]\n",
      "epoch:28 step:26370 [D loss: 0.690004, acc: 55.47%] [G loss: 1.818861]\n",
      "epoch:28 step:26371 [D loss: 0.617893, acc: 64.84%] [G loss: 1.862478]\n",
      "epoch:28 step:26372 [D loss: 0.683837, acc: 61.72%] [G loss: 1.738602]\n",
      "epoch:28 step:26373 [D loss: 0.698587, acc: 50.78%] [G loss: 1.785078]\n",
      "epoch:28 step:26374 [D loss: 0.656844, acc: 61.72%] [G loss: 1.747748]\n",
      "epoch:28 step:26375 [D loss: 0.635424, acc: 57.81%] [G loss: 1.713394]\n",
      "epoch:28 step:26376 [D loss: 0.708584, acc: 51.56%] [G loss: 1.822638]\n",
      "epoch:28 step:26377 [D loss: 0.670087, acc: 57.03%] [G loss: 1.738960]\n",
      "epoch:28 step:26378 [D loss: 0.683512, acc: 55.47%] [G loss: 1.882044]\n",
      "epoch:28 step:26379 [D loss: 0.631591, acc: 65.62%] [G loss: 1.854834]\n",
      "epoch:28 step:26380 [D loss: 0.629158, acc: 62.50%] [G loss: 1.848378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26381 [D loss: 0.670429, acc: 57.81%] [G loss: 1.823005]\n",
      "epoch:28 step:26382 [D loss: 0.681712, acc: 60.94%] [G loss: 1.767290]\n",
      "epoch:28 step:26383 [D loss: 0.682526, acc: 60.16%] [G loss: 1.826194]\n",
      "epoch:28 step:26384 [D loss: 0.678137, acc: 54.69%] [G loss: 1.750104]\n",
      "epoch:28 step:26385 [D loss: 0.657523, acc: 58.59%] [G loss: 1.829902]\n",
      "epoch:28 step:26386 [D loss: 0.654700, acc: 64.06%] [G loss: 1.970592]\n",
      "epoch:28 step:26387 [D loss: 0.605810, acc: 70.31%] [G loss: 1.867787]\n",
      "epoch:28 step:26388 [D loss: 0.646224, acc: 65.62%] [G loss: 1.820540]\n",
      "epoch:28 step:26389 [D loss: 0.681042, acc: 57.03%] [G loss: 1.779246]\n",
      "epoch:28 step:26390 [D loss: 0.596268, acc: 64.84%] [G loss: 1.894169]\n",
      "epoch:28 step:26391 [D loss: 0.630438, acc: 62.50%] [G loss: 1.817828]\n",
      "epoch:28 step:26392 [D loss: 0.644263, acc: 60.94%] [G loss: 1.762272]\n",
      "epoch:28 step:26393 [D loss: 0.695675, acc: 51.56%] [G loss: 1.810259]\n",
      "epoch:28 step:26394 [D loss: 0.665799, acc: 60.16%] [G loss: 1.796109]\n",
      "epoch:28 step:26395 [D loss: 0.659608, acc: 63.28%] [G loss: 1.772017]\n",
      "epoch:28 step:26396 [D loss: 0.714044, acc: 52.34%] [G loss: 1.748733]\n",
      "epoch:28 step:26397 [D loss: 0.678648, acc: 60.16%] [G loss: 1.782525]\n",
      "epoch:28 step:26398 [D loss: 0.638283, acc: 65.62%] [G loss: 1.756048]\n",
      "epoch:28 step:26399 [D loss: 0.679265, acc: 58.59%] [G loss: 1.757443]\n",
      "epoch:28 step:26400 [D loss: 0.677625, acc: 58.59%] [G loss: 1.784650]\n",
      "##############\n",
      "[2.58918829 1.29039565 6.09162693 4.66530488 3.43179527 5.60794313\n",
      " 4.47216593 4.38296669 4.50365039 3.48320179]\n",
      "##########\n",
      "epoch:28 step:26401 [D loss: 0.643647, acc: 55.47%] [G loss: 1.785487]\n",
      "epoch:28 step:26402 [D loss: 0.672416, acc: 63.28%] [G loss: 1.752353]\n",
      "epoch:28 step:26403 [D loss: 0.629452, acc: 62.50%] [G loss: 1.822249]\n",
      "epoch:28 step:26404 [D loss: 0.595328, acc: 70.31%] [G loss: 1.899564]\n",
      "epoch:28 step:26405 [D loss: 0.683447, acc: 55.47%] [G loss: 1.788271]\n",
      "epoch:28 step:26406 [D loss: 0.690381, acc: 57.03%] [G loss: 1.829100]\n",
      "epoch:28 step:26407 [D loss: 0.676308, acc: 59.38%] [G loss: 1.850756]\n",
      "epoch:28 step:26408 [D loss: 0.724388, acc: 56.25%] [G loss: 1.736156]\n",
      "epoch:28 step:26409 [D loss: 0.680950, acc: 56.25%] [G loss: 1.749005]\n",
      "epoch:28 step:26410 [D loss: 0.694932, acc: 49.22%] [G loss: 1.807169]\n",
      "epoch:28 step:26411 [D loss: 0.663663, acc: 57.03%] [G loss: 1.748295]\n",
      "epoch:28 step:26412 [D loss: 0.633577, acc: 57.81%] [G loss: 1.657175]\n",
      "epoch:28 step:26413 [D loss: 0.652808, acc: 61.72%] [G loss: 1.692371]\n",
      "epoch:28 step:26414 [D loss: 0.707460, acc: 53.91%] [G loss: 1.783273]\n",
      "epoch:28 step:26415 [D loss: 0.660782, acc: 61.72%] [G loss: 1.747660]\n",
      "epoch:28 step:26416 [D loss: 0.667355, acc: 60.94%] [G loss: 1.827577]\n",
      "epoch:28 step:26417 [D loss: 0.713705, acc: 49.22%] [G loss: 1.726838]\n",
      "epoch:28 step:26418 [D loss: 0.679255, acc: 57.03%] [G loss: 1.736471]\n",
      "epoch:28 step:26419 [D loss: 0.698046, acc: 56.25%] [G loss: 1.730381]\n",
      "epoch:28 step:26420 [D loss: 0.614872, acc: 69.53%] [G loss: 1.783970]\n",
      "epoch:28 step:26421 [D loss: 0.650408, acc: 63.28%] [G loss: 1.746629]\n",
      "epoch:28 step:26422 [D loss: 0.688441, acc: 56.25%] [G loss: 1.756943]\n",
      "epoch:28 step:26423 [D loss: 0.646202, acc: 58.59%] [G loss: 1.896885]\n",
      "epoch:28 step:26424 [D loss: 0.659901, acc: 55.47%] [G loss: 1.962510]\n",
      "epoch:28 step:26425 [D loss: 0.642343, acc: 59.38%] [G loss: 1.710996]\n",
      "epoch:28 step:26426 [D loss: 0.691836, acc: 51.56%] [G loss: 1.760313]\n",
      "epoch:28 step:26427 [D loss: 0.615604, acc: 70.31%] [G loss: 1.852735]\n",
      "epoch:28 step:26428 [D loss: 0.635923, acc: 63.28%] [G loss: 1.820724]\n",
      "epoch:28 step:26429 [D loss: 0.675039, acc: 64.06%] [G loss: 1.801177]\n",
      "epoch:28 step:26430 [D loss: 0.642407, acc: 64.06%] [G loss: 1.847356]\n",
      "epoch:28 step:26431 [D loss: 0.687500, acc: 53.91%] [G loss: 1.796410]\n",
      "epoch:28 step:26432 [D loss: 0.733908, acc: 49.22%] [G loss: 1.648551]\n",
      "epoch:28 step:26433 [D loss: 0.670790, acc: 61.72%] [G loss: 1.685338]\n",
      "epoch:28 step:26434 [D loss: 0.631435, acc: 63.28%] [G loss: 1.903515]\n",
      "epoch:28 step:26435 [D loss: 0.638507, acc: 60.94%] [G loss: 1.726907]\n",
      "epoch:28 step:26436 [D loss: 0.696301, acc: 53.12%] [G loss: 1.757707]\n",
      "epoch:28 step:26437 [D loss: 0.658854, acc: 60.16%] [G loss: 1.781765]\n",
      "epoch:28 step:26438 [D loss: 0.654580, acc: 60.94%] [G loss: 1.748951]\n",
      "epoch:28 step:26439 [D loss: 0.648439, acc: 59.38%] [G loss: 1.702811]\n",
      "epoch:28 step:26440 [D loss: 0.679988, acc: 54.69%] [G loss: 1.805794]\n",
      "epoch:28 step:26441 [D loss: 0.658810, acc: 60.16%] [G loss: 1.702031]\n",
      "epoch:28 step:26442 [D loss: 0.633433, acc: 64.84%] [G loss: 1.877979]\n",
      "epoch:28 step:26443 [D loss: 0.676770, acc: 60.16%] [G loss: 1.975046]\n",
      "epoch:28 step:26444 [D loss: 0.577553, acc: 71.09%] [G loss: 1.928635]\n",
      "epoch:28 step:26445 [D loss: 0.607922, acc: 66.41%] [G loss: 1.964695]\n",
      "epoch:28 step:26446 [D loss: 0.644293, acc: 63.28%] [G loss: 1.791072]\n",
      "epoch:28 step:26447 [D loss: 0.680739, acc: 55.47%] [G loss: 1.685639]\n",
      "epoch:28 step:26448 [D loss: 0.634163, acc: 62.50%] [G loss: 1.758582]\n",
      "epoch:28 step:26449 [D loss: 0.635912, acc: 62.50%] [G loss: 1.695952]\n",
      "epoch:28 step:26450 [D loss: 0.661180, acc: 62.50%] [G loss: 1.895329]\n",
      "epoch:28 step:26451 [D loss: 0.710387, acc: 54.69%] [G loss: 1.901309]\n",
      "epoch:28 step:26452 [D loss: 0.640548, acc: 60.94%] [G loss: 1.946398]\n",
      "epoch:28 step:26453 [D loss: 0.632037, acc: 64.06%] [G loss: 1.879346]\n",
      "epoch:28 step:26454 [D loss: 0.621832, acc: 67.19%] [G loss: 1.993793]\n",
      "epoch:28 step:26455 [D loss: 0.595398, acc: 64.84%] [G loss: 2.097164]\n",
      "epoch:28 step:26456 [D loss: 0.708752, acc: 57.03%] [G loss: 1.962288]\n",
      "epoch:28 step:26457 [D loss: 0.637669, acc: 60.94%] [G loss: 2.054385]\n",
      "epoch:28 step:26458 [D loss: 0.691084, acc: 57.03%] [G loss: 1.816319]\n",
      "epoch:28 step:26459 [D loss: 0.667110, acc: 59.38%] [G loss: 1.858848]\n",
      "epoch:28 step:26460 [D loss: 0.647970, acc: 58.59%] [G loss: 1.922230]\n",
      "epoch:28 step:26461 [D loss: 0.692996, acc: 57.03%] [G loss: 1.802768]\n",
      "epoch:28 step:26462 [D loss: 0.656475, acc: 60.16%] [G loss: 1.725218]\n",
      "epoch:28 step:26463 [D loss: 0.649256, acc: 62.50%] [G loss: 1.909847]\n",
      "epoch:28 step:26464 [D loss: 0.685930, acc: 57.81%] [G loss: 1.795844]\n",
      "epoch:28 step:26465 [D loss: 0.656863, acc: 62.50%] [G loss: 2.008404]\n",
      "epoch:28 step:26466 [D loss: 0.559095, acc: 74.22%] [G loss: 2.066908]\n",
      "epoch:28 step:26467 [D loss: 0.570372, acc: 72.66%] [G loss: 2.284973]\n",
      "epoch:28 step:26468 [D loss: 0.614962, acc: 64.84%] [G loss: 2.035082]\n",
      "epoch:28 step:26469 [D loss: 0.630682, acc: 61.72%] [G loss: 1.837920]\n",
      "epoch:28 step:26470 [D loss: 0.632163, acc: 67.19%] [G loss: 1.748323]\n",
      "epoch:28 step:26471 [D loss: 0.625996, acc: 64.84%] [G loss: 1.860251]\n",
      "epoch:28 step:26472 [D loss: 0.635772, acc: 60.94%] [G loss: 1.912507]\n",
      "epoch:28 step:26473 [D loss: 0.651519, acc: 60.16%] [G loss: 1.834378]\n",
      "epoch:28 step:26474 [D loss: 0.637627, acc: 62.50%] [G loss: 1.835115]\n",
      "epoch:28 step:26475 [D loss: 0.696867, acc: 50.78%] [G loss: 1.895213]\n",
      "epoch:28 step:26476 [D loss: 0.605866, acc: 66.41%] [G loss: 2.014915]\n",
      "epoch:28 step:26477 [D loss: 0.638285, acc: 69.53%] [G loss: 1.904102]\n",
      "epoch:28 step:26478 [D loss: 0.664993, acc: 57.81%] [G loss: 1.942651]\n",
      "epoch:28 step:26479 [D loss: 0.666178, acc: 60.94%] [G loss: 1.836288]\n",
      "epoch:28 step:26480 [D loss: 0.683006, acc: 55.47%] [G loss: 1.858213]\n",
      "epoch:28 step:26481 [D loss: 0.636224, acc: 67.97%] [G loss: 1.849110]\n",
      "epoch:28 step:26482 [D loss: 0.640217, acc: 60.94%] [G loss: 1.927256]\n",
      "epoch:28 step:26483 [D loss: 0.647742, acc: 62.50%] [G loss: 1.800287]\n",
      "epoch:28 step:26484 [D loss: 0.615854, acc: 69.53%] [G loss: 1.972263]\n",
      "epoch:28 step:26485 [D loss: 0.688713, acc: 53.12%] [G loss: 1.828567]\n",
      "epoch:28 step:26486 [D loss: 0.719594, acc: 50.78%] [G loss: 1.879191]\n",
      "epoch:28 step:26487 [D loss: 0.702039, acc: 57.81%] [G loss: 1.773652]\n",
      "epoch:28 step:26488 [D loss: 0.676950, acc: 58.59%] [G loss: 1.798653]\n",
      "epoch:28 step:26489 [D loss: 0.624124, acc: 67.97%] [G loss: 1.811774]\n",
      "epoch:28 step:26490 [D loss: 0.701254, acc: 53.91%] [G loss: 1.808047]\n",
      "epoch:28 step:26491 [D loss: 0.663938, acc: 57.03%] [G loss: 1.850287]\n",
      "epoch:28 step:26492 [D loss: 0.628603, acc: 65.62%] [G loss: 1.749944]\n",
      "epoch:28 step:26493 [D loss: 0.652086, acc: 59.38%] [G loss: 1.676607]\n",
      "epoch:28 step:26494 [D loss: 0.662950, acc: 53.91%] [G loss: 1.770918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26495 [D loss: 0.642001, acc: 67.97%] [G loss: 1.925552]\n",
      "epoch:28 step:26496 [D loss: 0.676183, acc: 57.03%] [G loss: 1.789940]\n",
      "epoch:28 step:26497 [D loss: 0.686557, acc: 57.81%] [G loss: 1.851451]\n",
      "epoch:28 step:26498 [D loss: 0.617947, acc: 68.75%] [G loss: 1.968862]\n",
      "epoch:28 step:26499 [D loss: 0.655536, acc: 59.38%] [G loss: 1.896371]\n",
      "epoch:28 step:26500 [D loss: 0.655485, acc: 63.28%] [G loss: 1.999347]\n",
      "epoch:28 step:26501 [D loss: 0.682001, acc: 56.25%] [G loss: 1.756066]\n",
      "epoch:28 step:26502 [D loss: 0.654897, acc: 57.81%] [G loss: 1.917489]\n",
      "epoch:28 step:26503 [D loss: 0.652154, acc: 60.16%] [G loss: 1.845140]\n",
      "epoch:28 step:26504 [D loss: 0.663462, acc: 64.06%] [G loss: 1.768456]\n",
      "epoch:28 step:26505 [D loss: 0.648729, acc: 60.16%] [G loss: 1.903889]\n",
      "epoch:28 step:26506 [D loss: 0.700559, acc: 59.38%] [G loss: 1.792225]\n",
      "epoch:28 step:26507 [D loss: 0.659772, acc: 61.72%] [G loss: 1.788054]\n",
      "epoch:28 step:26508 [D loss: 0.650542, acc: 60.94%] [G loss: 1.878529]\n",
      "epoch:28 step:26509 [D loss: 0.629667, acc: 63.28%] [G loss: 1.842994]\n",
      "epoch:28 step:26510 [D loss: 0.618750, acc: 64.84%] [G loss: 1.995491]\n",
      "epoch:28 step:26511 [D loss: 0.602099, acc: 63.28%] [G loss: 1.900640]\n",
      "epoch:28 step:26512 [D loss: 0.589959, acc: 65.62%] [G loss: 2.169912]\n",
      "epoch:28 step:26513 [D loss: 0.695757, acc: 53.91%] [G loss: 1.756625]\n",
      "epoch:28 step:26514 [D loss: 0.653476, acc: 61.72%] [G loss: 1.840308]\n",
      "epoch:28 step:26515 [D loss: 0.627271, acc: 67.97%] [G loss: 1.867609]\n",
      "epoch:28 step:26516 [D loss: 0.581741, acc: 70.31%] [G loss: 1.864756]\n",
      "epoch:28 step:26517 [D loss: 0.660816, acc: 56.25%] [G loss: 1.804851]\n",
      "epoch:28 step:26518 [D loss: 0.666669, acc: 60.16%] [G loss: 1.778589]\n",
      "epoch:28 step:26519 [D loss: 0.643592, acc: 63.28%] [G loss: 1.793409]\n",
      "epoch:28 step:26520 [D loss: 0.664699, acc: 61.72%] [G loss: 1.670171]\n",
      "epoch:28 step:26521 [D loss: 0.682343, acc: 60.16%] [G loss: 1.679662]\n",
      "epoch:28 step:26522 [D loss: 0.627478, acc: 64.84%] [G loss: 1.932445]\n",
      "epoch:28 step:26523 [D loss: 0.664378, acc: 60.94%] [G loss: 1.758263]\n",
      "epoch:28 step:26524 [D loss: 0.652952, acc: 57.03%] [G loss: 1.790180]\n",
      "epoch:28 step:26525 [D loss: 0.652654, acc: 60.94%] [G loss: 1.785482]\n",
      "epoch:28 step:26526 [D loss: 0.663684, acc: 60.16%] [G loss: 1.796560]\n",
      "epoch:28 step:26527 [D loss: 0.705319, acc: 54.69%] [G loss: 1.874770]\n",
      "epoch:28 step:26528 [D loss: 0.675567, acc: 57.03%] [G loss: 1.734998]\n",
      "epoch:28 step:26529 [D loss: 0.638515, acc: 61.72%] [G loss: 1.799129]\n",
      "epoch:28 step:26530 [D loss: 0.661084, acc: 59.38%] [G loss: 1.714707]\n",
      "epoch:28 step:26531 [D loss: 0.652571, acc: 60.94%] [G loss: 1.804760]\n",
      "epoch:28 step:26532 [D loss: 0.595184, acc: 68.75%] [G loss: 1.833134]\n",
      "epoch:28 step:26533 [D loss: 0.648722, acc: 64.06%] [G loss: 1.785061]\n",
      "epoch:28 step:26534 [D loss: 0.621470, acc: 62.50%] [G loss: 1.975520]\n",
      "epoch:28 step:26535 [D loss: 0.602489, acc: 71.09%] [G loss: 1.954264]\n",
      "epoch:28 step:26536 [D loss: 0.632545, acc: 60.94%] [G loss: 1.784202]\n",
      "epoch:28 step:26537 [D loss: 0.643724, acc: 61.72%] [G loss: 1.743539]\n",
      "epoch:28 step:26538 [D loss: 0.704445, acc: 57.81%] [G loss: 1.995716]\n",
      "epoch:28 step:26539 [D loss: 0.703636, acc: 54.69%] [G loss: 1.881715]\n",
      "epoch:28 step:26540 [D loss: 0.642339, acc: 58.59%] [G loss: 1.851161]\n",
      "epoch:28 step:26541 [D loss: 0.630961, acc: 62.50%] [G loss: 1.836539]\n",
      "epoch:28 step:26542 [D loss: 0.670177, acc: 60.16%] [G loss: 1.845978]\n",
      "epoch:28 step:26543 [D loss: 0.667504, acc: 60.16%] [G loss: 1.802729]\n",
      "epoch:28 step:26544 [D loss: 0.636104, acc: 63.28%] [G loss: 1.817232]\n",
      "epoch:28 step:26545 [D loss: 0.599973, acc: 69.53%] [G loss: 1.861618]\n",
      "epoch:28 step:26546 [D loss: 0.636092, acc: 60.94%] [G loss: 1.929153]\n",
      "epoch:28 step:26547 [D loss: 0.648036, acc: 64.06%] [G loss: 1.781067]\n",
      "epoch:28 step:26548 [D loss: 0.578605, acc: 68.75%] [G loss: 2.068727]\n",
      "epoch:28 step:26549 [D loss: 0.658757, acc: 60.94%] [G loss: 2.106528]\n",
      "epoch:28 step:26550 [D loss: 0.610928, acc: 66.41%] [G loss: 2.154505]\n",
      "epoch:28 step:26551 [D loss: 0.590218, acc: 71.09%] [G loss: 1.989449]\n",
      "epoch:28 step:26552 [D loss: 0.720579, acc: 53.91%] [G loss: 1.749164]\n",
      "epoch:28 step:26553 [D loss: 0.707542, acc: 55.47%] [G loss: 1.847569]\n",
      "epoch:28 step:26554 [D loss: 0.690329, acc: 53.91%] [G loss: 1.865816]\n",
      "epoch:28 step:26555 [D loss: 0.641094, acc: 61.72%] [G loss: 1.804741]\n",
      "epoch:28 step:26556 [D loss: 0.623736, acc: 71.09%] [G loss: 1.830268]\n",
      "epoch:28 step:26557 [D loss: 0.656643, acc: 64.06%] [G loss: 1.930295]\n",
      "epoch:28 step:26558 [D loss: 0.655688, acc: 60.94%] [G loss: 1.879570]\n",
      "epoch:28 step:26559 [D loss: 0.652002, acc: 58.59%] [G loss: 1.697492]\n",
      "epoch:28 step:26560 [D loss: 0.609370, acc: 66.41%] [G loss: 1.854126]\n",
      "epoch:28 step:26561 [D loss: 0.612804, acc: 66.41%] [G loss: 1.875108]\n",
      "epoch:28 step:26562 [D loss: 0.654982, acc: 57.03%] [G loss: 1.902905]\n",
      "epoch:28 step:26563 [D loss: 0.680871, acc: 63.28%] [G loss: 1.799946]\n",
      "epoch:28 step:26564 [D loss: 0.650653, acc: 59.38%] [G loss: 1.872517]\n",
      "epoch:28 step:26565 [D loss: 0.651120, acc: 59.38%] [G loss: 1.969819]\n",
      "epoch:28 step:26566 [D loss: 0.627925, acc: 67.97%] [G loss: 1.947886]\n",
      "epoch:28 step:26567 [D loss: 0.583115, acc: 68.75%] [G loss: 1.990775]\n",
      "epoch:28 step:26568 [D loss: 0.606232, acc: 65.62%] [G loss: 1.869849]\n",
      "epoch:28 step:26569 [D loss: 0.655022, acc: 60.94%] [G loss: 1.939877]\n",
      "epoch:28 step:26570 [D loss: 0.670317, acc: 57.03%] [G loss: 1.936928]\n",
      "epoch:28 step:26571 [D loss: 0.638337, acc: 64.06%] [G loss: 1.920168]\n",
      "epoch:28 step:26572 [D loss: 0.661143, acc: 60.94%] [G loss: 1.833662]\n",
      "epoch:28 step:26573 [D loss: 0.663690, acc: 64.06%] [G loss: 1.870050]\n",
      "epoch:28 step:26574 [D loss: 0.597274, acc: 67.19%] [G loss: 1.956866]\n",
      "epoch:28 step:26575 [D loss: 0.686725, acc: 54.69%] [G loss: 1.737811]\n",
      "epoch:28 step:26576 [D loss: 0.567593, acc: 77.34%] [G loss: 1.960056]\n",
      "epoch:28 step:26577 [D loss: 0.713099, acc: 54.69%] [G loss: 1.796203]\n",
      "epoch:28 step:26578 [D loss: 0.692571, acc: 54.69%] [G loss: 1.784834]\n",
      "epoch:28 step:26579 [D loss: 0.677706, acc: 59.38%] [G loss: 1.705696]\n",
      "epoch:28 step:26580 [D loss: 0.626124, acc: 63.28%] [G loss: 1.737852]\n",
      "epoch:28 step:26581 [D loss: 0.586129, acc: 67.97%] [G loss: 1.969020]\n",
      "epoch:28 step:26582 [D loss: 0.590831, acc: 68.75%] [G loss: 2.138891]\n",
      "epoch:28 step:26583 [D loss: 0.599687, acc: 70.31%] [G loss: 2.418967]\n",
      "epoch:28 step:26584 [D loss: 0.709370, acc: 54.69%] [G loss: 1.729357]\n",
      "epoch:28 step:26585 [D loss: 0.716639, acc: 53.12%] [G loss: 1.788674]\n",
      "epoch:28 step:26586 [D loss: 0.617085, acc: 69.53%] [G loss: 1.847769]\n",
      "epoch:28 step:26587 [D loss: 0.615022, acc: 64.84%] [G loss: 1.862015]\n",
      "epoch:28 step:26588 [D loss: 0.719872, acc: 57.03%] [G loss: 1.951790]\n",
      "epoch:28 step:26589 [D loss: 0.682006, acc: 60.94%] [G loss: 1.733565]\n",
      "epoch:28 step:26590 [D loss: 0.642742, acc: 67.19%] [G loss: 1.913961]\n",
      "epoch:28 step:26591 [D loss: 0.703489, acc: 50.78%] [G loss: 1.654009]\n",
      "epoch:28 step:26592 [D loss: 0.643819, acc: 61.72%] [G loss: 1.812571]\n",
      "epoch:28 step:26593 [D loss: 0.671397, acc: 58.59%] [G loss: 1.945155]\n",
      "epoch:28 step:26594 [D loss: 0.649524, acc: 64.84%] [G loss: 1.830755]\n",
      "epoch:28 step:26595 [D loss: 0.680438, acc: 59.38%] [G loss: 1.952453]\n",
      "epoch:28 step:26596 [D loss: 0.610183, acc: 64.84%] [G loss: 1.900885]\n",
      "epoch:28 step:26597 [D loss: 0.640831, acc: 60.16%] [G loss: 1.755521]\n",
      "epoch:28 step:26598 [D loss: 0.666233, acc: 63.28%] [G loss: 1.806506]\n",
      "epoch:28 step:26599 [D loss: 0.692650, acc: 56.25%] [G loss: 1.830772]\n",
      "epoch:28 step:26600 [D loss: 0.637865, acc: 66.41%] [G loss: 1.816024]\n",
      "##############\n",
      "[2.51497955 1.41207388 6.32017194 4.7075662  3.46707153 5.67138572\n",
      " 4.39718276 4.65444108 4.47428484 3.65918459]\n",
      "##########\n",
      "epoch:28 step:26601 [D loss: 0.694660, acc: 61.72%] [G loss: 1.910101]\n",
      "epoch:28 step:26602 [D loss: 0.648849, acc: 60.94%] [G loss: 1.786979]\n",
      "epoch:28 step:26603 [D loss: 0.655610, acc: 60.16%] [G loss: 1.904366]\n",
      "epoch:28 step:26604 [D loss: 0.633922, acc: 64.06%] [G loss: 1.963142]\n",
      "epoch:28 step:26605 [D loss: 0.603415, acc: 67.97%] [G loss: 1.976929]\n",
      "epoch:28 step:26606 [D loss: 0.636566, acc: 60.16%] [G loss: 2.043306]\n",
      "epoch:28 step:26607 [D loss: 0.635724, acc: 60.94%] [G loss: 2.137547]\n",
      "epoch:28 step:26608 [D loss: 0.618926, acc: 69.53%] [G loss: 1.932662]\n",
      "epoch:28 step:26609 [D loss: 0.647778, acc: 60.94%] [G loss: 1.764706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26610 [D loss: 0.659764, acc: 64.84%] [G loss: 1.961518]\n",
      "epoch:28 step:26611 [D loss: 0.603881, acc: 64.84%] [G loss: 1.825973]\n",
      "epoch:28 step:26612 [D loss: 0.642073, acc: 61.72%] [G loss: 1.767617]\n",
      "epoch:28 step:26613 [D loss: 0.699601, acc: 51.56%] [G loss: 1.823889]\n",
      "epoch:28 step:26614 [D loss: 0.621500, acc: 66.41%] [G loss: 1.732501]\n",
      "epoch:28 step:26615 [D loss: 0.653371, acc: 57.81%] [G loss: 1.906219]\n",
      "epoch:28 step:26616 [D loss: 0.665213, acc: 60.16%] [G loss: 2.008411]\n",
      "epoch:28 step:26617 [D loss: 0.651999, acc: 60.94%] [G loss: 1.925634]\n",
      "epoch:28 step:26618 [D loss: 0.629989, acc: 67.97%] [G loss: 1.900216]\n",
      "epoch:28 step:26619 [D loss: 0.637281, acc: 60.94%] [G loss: 1.840850]\n",
      "epoch:28 step:26620 [D loss: 0.594572, acc: 70.31%] [G loss: 1.996396]\n",
      "epoch:28 step:26621 [D loss: 0.594254, acc: 67.19%] [G loss: 1.950209]\n",
      "epoch:28 step:26622 [D loss: 0.699118, acc: 56.25%] [G loss: 1.847637]\n",
      "epoch:28 step:26623 [D loss: 0.679776, acc: 57.81%] [G loss: 1.813947]\n",
      "epoch:28 step:26624 [D loss: 0.647570, acc: 65.62%] [G loss: 1.788836]\n",
      "epoch:28 step:26625 [D loss: 0.651842, acc: 61.72%] [G loss: 1.909193]\n",
      "epoch:28 step:26626 [D loss: 0.712521, acc: 56.25%] [G loss: 1.873520]\n",
      "epoch:28 step:26627 [D loss: 0.641128, acc: 57.81%] [G loss: 1.784690]\n",
      "epoch:28 step:26628 [D loss: 0.654999, acc: 64.06%] [G loss: 1.783638]\n",
      "epoch:28 step:26629 [D loss: 0.670595, acc: 50.78%] [G loss: 1.969412]\n",
      "epoch:28 step:26630 [D loss: 0.697813, acc: 59.38%] [G loss: 1.767904]\n",
      "epoch:28 step:26631 [D loss: 0.637304, acc: 64.06%] [G loss: 1.865827]\n",
      "epoch:28 step:26632 [D loss: 0.656326, acc: 63.28%] [G loss: 1.760398]\n",
      "epoch:28 step:26633 [D loss: 0.663695, acc: 57.81%] [G loss: 1.669554]\n",
      "epoch:28 step:26634 [D loss: 0.658514, acc: 58.59%] [G loss: 1.940776]\n",
      "epoch:28 step:26635 [D loss: 0.645744, acc: 61.72%] [G loss: 1.884923]\n",
      "epoch:28 step:26636 [D loss: 0.671194, acc: 53.91%] [G loss: 1.734326]\n",
      "epoch:28 step:26637 [D loss: 0.682884, acc: 53.91%] [G loss: 1.843388]\n",
      "epoch:28 step:26638 [D loss: 0.667911, acc: 57.81%] [G loss: 1.757312]\n",
      "epoch:28 step:26639 [D loss: 0.630848, acc: 65.62%] [G loss: 1.892562]\n",
      "epoch:28 step:26640 [D loss: 0.628153, acc: 66.41%] [G loss: 1.919182]\n",
      "epoch:28 step:26641 [D loss: 0.614112, acc: 66.41%] [G loss: 2.125452]\n",
      "epoch:28 step:26642 [D loss: 0.595768, acc: 71.88%] [G loss: 2.021498]\n",
      "epoch:28 step:26643 [D loss: 0.669231, acc: 58.59%] [G loss: 1.889157]\n",
      "epoch:28 step:26644 [D loss: 0.645525, acc: 67.19%] [G loss: 1.796814]\n",
      "epoch:28 step:26645 [D loss: 0.684712, acc: 53.91%] [G loss: 1.803101]\n",
      "epoch:28 step:26646 [D loss: 0.664336, acc: 60.94%] [G loss: 1.805621]\n",
      "epoch:28 step:26647 [D loss: 0.644944, acc: 60.16%] [G loss: 1.863050]\n",
      "epoch:28 step:26648 [D loss: 0.602726, acc: 74.22%] [G loss: 1.916823]\n",
      "epoch:28 step:26649 [D loss: 0.622951, acc: 68.75%] [G loss: 1.937900]\n",
      "epoch:28 step:26650 [D loss: 0.619240, acc: 69.53%] [G loss: 2.051881]\n",
      "epoch:28 step:26651 [D loss: 0.620030, acc: 66.41%] [G loss: 2.019430]\n",
      "epoch:28 step:26652 [D loss: 0.579600, acc: 70.31%] [G loss: 1.967177]\n",
      "epoch:28 step:26653 [D loss: 0.642206, acc: 58.59%] [G loss: 1.801925]\n",
      "epoch:28 step:26654 [D loss: 0.681041, acc: 59.38%] [G loss: 1.733618]\n",
      "epoch:28 step:26655 [D loss: 0.671588, acc: 63.28%] [G loss: 2.002089]\n",
      "epoch:28 step:26656 [D loss: 0.606302, acc: 64.84%] [G loss: 1.900729]\n",
      "epoch:28 step:26657 [D loss: 0.662840, acc: 54.69%] [G loss: 1.880317]\n",
      "epoch:28 step:26658 [D loss: 0.639222, acc: 63.28%] [G loss: 1.848759]\n",
      "epoch:28 step:26659 [D loss: 0.640690, acc: 64.06%] [G loss: 1.814627]\n",
      "epoch:28 step:26660 [D loss: 0.636219, acc: 62.50%] [G loss: 1.795659]\n",
      "epoch:28 step:26661 [D loss: 0.687704, acc: 63.28%] [G loss: 1.880386]\n",
      "epoch:28 step:26662 [D loss: 0.656207, acc: 64.84%] [G loss: 1.938249]\n",
      "epoch:28 step:26663 [D loss: 0.642501, acc: 65.62%] [G loss: 1.904932]\n",
      "epoch:28 step:26664 [D loss: 0.639727, acc: 61.72%] [G loss: 2.009268]\n",
      "epoch:28 step:26665 [D loss: 0.612844, acc: 66.41%] [G loss: 1.987064]\n",
      "epoch:28 step:26666 [D loss: 0.597852, acc: 66.41%] [G loss: 1.944072]\n",
      "epoch:28 step:26667 [D loss: 0.694068, acc: 57.03%] [G loss: 1.908634]\n",
      "epoch:28 step:26668 [D loss: 0.736358, acc: 53.12%] [G loss: 1.785743]\n",
      "epoch:28 step:26669 [D loss: 0.637430, acc: 64.06%] [G loss: 1.776488]\n",
      "epoch:28 step:26670 [D loss: 0.608066, acc: 66.41%] [G loss: 1.863662]\n",
      "epoch:28 step:26671 [D loss: 0.630489, acc: 59.38%] [G loss: 1.830204]\n",
      "epoch:28 step:26672 [D loss: 0.653249, acc: 62.50%] [G loss: 1.729705]\n",
      "epoch:28 step:26673 [D loss: 0.721569, acc: 55.47%] [G loss: 1.698950]\n",
      "epoch:28 step:26674 [D loss: 0.692983, acc: 53.91%] [G loss: 1.815125]\n",
      "epoch:28 step:26675 [D loss: 0.648771, acc: 59.38%] [G loss: 1.789749]\n",
      "epoch:28 step:26676 [D loss: 0.658371, acc: 61.72%] [G loss: 1.757749]\n",
      "epoch:28 step:26677 [D loss: 0.646468, acc: 64.84%] [G loss: 1.806134]\n",
      "epoch:28 step:26678 [D loss: 0.645984, acc: 61.72%] [G loss: 1.821321]\n",
      "epoch:28 step:26679 [D loss: 0.645744, acc: 61.72%] [G loss: 1.842069]\n",
      "epoch:28 step:26680 [D loss: 0.664699, acc: 59.38%] [G loss: 1.794149]\n",
      "epoch:28 step:26681 [D loss: 0.642912, acc: 64.84%] [G loss: 1.913524]\n",
      "epoch:28 step:26682 [D loss: 0.648625, acc: 61.72%] [G loss: 1.813752]\n",
      "epoch:28 step:26683 [D loss: 0.623729, acc: 64.06%] [G loss: 1.790087]\n",
      "epoch:28 step:26684 [D loss: 0.648874, acc: 62.50%] [G loss: 1.795628]\n",
      "epoch:28 step:26685 [D loss: 0.677919, acc: 55.47%] [G loss: 1.803835]\n",
      "epoch:28 step:26686 [D loss: 0.695376, acc: 55.47%] [G loss: 1.850955]\n",
      "epoch:28 step:26687 [D loss: 0.688440, acc: 59.38%] [G loss: 1.785997]\n",
      "epoch:28 step:26688 [D loss: 0.658269, acc: 60.16%] [G loss: 1.690638]\n",
      "epoch:28 step:26689 [D loss: 0.684238, acc: 57.81%] [G loss: 1.931564]\n",
      "epoch:28 step:26690 [D loss: 0.689144, acc: 56.25%] [G loss: 1.827403]\n",
      "epoch:28 step:26691 [D loss: 0.628843, acc: 64.84%] [G loss: 1.827219]\n",
      "epoch:28 step:26692 [D loss: 0.672714, acc: 61.72%] [G loss: 1.966417]\n",
      "epoch:28 step:26693 [D loss: 0.587674, acc: 71.09%] [G loss: 2.030544]\n",
      "epoch:28 step:26694 [D loss: 0.680071, acc: 55.47%] [G loss: 1.719199]\n",
      "epoch:28 step:26695 [D loss: 0.664602, acc: 62.50%] [G loss: 1.785734]\n",
      "epoch:28 step:26696 [D loss: 0.651622, acc: 61.72%] [G loss: 1.721756]\n",
      "epoch:28 step:26697 [D loss: 0.661937, acc: 64.84%] [G loss: 1.813540]\n",
      "epoch:28 step:26698 [D loss: 0.629447, acc: 62.50%] [G loss: 1.821799]\n",
      "epoch:28 step:26699 [D loss: 0.665466, acc: 61.72%] [G loss: 1.880927]\n",
      "epoch:28 step:26700 [D loss: 0.663682, acc: 59.38%] [G loss: 1.843386]\n",
      "epoch:28 step:26701 [D loss: 0.626056, acc: 67.97%] [G loss: 1.907649]\n",
      "epoch:28 step:26702 [D loss: 0.628952, acc: 65.62%] [G loss: 1.890327]\n",
      "epoch:28 step:26703 [D loss: 0.649028, acc: 57.03%] [G loss: 1.865936]\n",
      "epoch:28 step:26704 [D loss: 0.643965, acc: 63.28%] [G loss: 1.986249]\n",
      "epoch:28 step:26705 [D loss: 0.636793, acc: 63.28%] [G loss: 2.078189]\n",
      "epoch:28 step:26706 [D loss: 0.657168, acc: 60.16%] [G loss: 1.890856]\n",
      "epoch:28 step:26707 [D loss: 0.555689, acc: 72.66%] [G loss: 2.101659]\n",
      "epoch:28 step:26708 [D loss: 0.668363, acc: 61.72%] [G loss: 2.124161]\n",
      "epoch:28 step:26709 [D loss: 0.753618, acc: 46.88%] [G loss: 1.845417]\n",
      "epoch:28 step:26710 [D loss: 0.673982, acc: 59.38%] [G loss: 1.884333]\n",
      "epoch:28 step:26711 [D loss: 0.608571, acc: 67.19%] [G loss: 1.886386]\n",
      "epoch:28 step:26712 [D loss: 0.679437, acc: 59.38%] [G loss: 1.931638]\n",
      "epoch:28 step:26713 [D loss: 0.661913, acc: 58.59%] [G loss: 1.679632]\n",
      "epoch:28 step:26714 [D loss: 0.674079, acc: 60.94%] [G loss: 1.794010]\n",
      "epoch:28 step:26715 [D loss: 0.577818, acc: 70.31%] [G loss: 1.990224]\n",
      "epoch:28 step:26716 [D loss: 0.620154, acc: 64.06%] [G loss: 1.936264]\n",
      "epoch:28 step:26717 [D loss: 0.585959, acc: 70.31%] [G loss: 2.081215]\n",
      "epoch:28 step:26718 [D loss: 0.654167, acc: 64.06%] [G loss: 1.913506]\n",
      "epoch:28 step:26719 [D loss: 0.614689, acc: 60.94%] [G loss: 1.992441]\n",
      "epoch:28 step:26720 [D loss: 0.670945, acc: 59.38%] [G loss: 1.969071]\n",
      "epoch:28 step:26721 [D loss: 0.702234, acc: 61.72%] [G loss: 1.842729]\n",
      "epoch:28 step:26722 [D loss: 0.665630, acc: 57.03%] [G loss: 1.780463]\n",
      "epoch:28 step:26723 [D loss: 0.641752, acc: 64.06%] [G loss: 1.893113]\n",
      "epoch:28 step:26724 [D loss: 0.599730, acc: 69.53%] [G loss: 2.035904]\n",
      "epoch:28 step:26725 [D loss: 0.603441, acc: 63.28%] [G loss: 1.961032]\n",
      "epoch:28 step:26726 [D loss: 0.656043, acc: 59.38%] [G loss: 1.918426]\n",
      "epoch:28 step:26727 [D loss: 0.629319, acc: 64.06%] [G loss: 1.927610]\n",
      "epoch:28 step:26728 [D loss: 0.639236, acc: 64.84%] [G loss: 1.731435]\n",
      "epoch:28 step:26729 [D loss: 0.662965, acc: 59.38%] [G loss: 1.839888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26730 [D loss: 0.611870, acc: 64.84%] [G loss: 1.918793]\n",
      "epoch:28 step:26731 [D loss: 0.594348, acc: 74.22%] [G loss: 2.056476]\n",
      "epoch:28 step:26732 [D loss: 0.698804, acc: 58.59%] [G loss: 1.801753]\n",
      "epoch:28 step:26733 [D loss: 0.615215, acc: 65.62%] [G loss: 1.935160]\n",
      "epoch:28 step:26734 [D loss: 0.654686, acc: 59.38%] [G loss: 2.014587]\n",
      "epoch:28 step:26735 [D loss: 0.594829, acc: 66.41%] [G loss: 2.029653]\n",
      "epoch:28 step:26736 [D loss: 0.672907, acc: 57.81%] [G loss: 1.768621]\n",
      "epoch:28 step:26737 [D loss: 0.729719, acc: 50.78%] [G loss: 1.851498]\n",
      "epoch:28 step:26738 [D loss: 0.703538, acc: 52.34%] [G loss: 1.603620]\n",
      "epoch:28 step:26739 [D loss: 0.660938, acc: 57.03%] [G loss: 1.855896]\n",
      "epoch:28 step:26740 [D loss: 0.622611, acc: 67.97%] [G loss: 2.094234]\n",
      "epoch:28 step:26741 [D loss: 0.640931, acc: 64.06%] [G loss: 1.887455]\n",
      "epoch:28 step:26742 [D loss: 0.647572, acc: 63.28%] [G loss: 1.739904]\n",
      "epoch:28 step:26743 [D loss: 0.630054, acc: 63.28%] [G loss: 1.894872]\n",
      "epoch:28 step:26744 [D loss: 0.653567, acc: 59.38%] [G loss: 1.982703]\n",
      "epoch:28 step:26745 [D loss: 0.671809, acc: 62.50%] [G loss: 1.997661]\n",
      "epoch:28 step:26746 [D loss: 0.685342, acc: 60.94%] [G loss: 1.915892]\n",
      "epoch:28 step:26747 [D loss: 0.687825, acc: 55.47%] [G loss: 1.830212]\n",
      "epoch:28 step:26748 [D loss: 0.678349, acc: 58.59%] [G loss: 1.817887]\n",
      "epoch:28 step:26749 [D loss: 0.602457, acc: 70.31%] [G loss: 1.878493]\n",
      "epoch:28 step:26750 [D loss: 0.613395, acc: 69.53%] [G loss: 1.924221]\n",
      "epoch:28 step:26751 [D loss: 0.668341, acc: 64.06%] [G loss: 1.892695]\n",
      "epoch:28 step:26752 [D loss: 0.614722, acc: 66.41%] [G loss: 1.925517]\n",
      "epoch:28 step:26753 [D loss: 0.605238, acc: 70.31%] [G loss: 1.917392]\n",
      "epoch:28 step:26754 [D loss: 0.635335, acc: 62.50%] [G loss: 1.874969]\n",
      "epoch:28 step:26755 [D loss: 0.632053, acc: 67.97%] [G loss: 1.731475]\n",
      "epoch:28 step:26756 [D loss: 0.662319, acc: 56.25%] [G loss: 1.904316]\n",
      "epoch:28 step:26757 [D loss: 0.682665, acc: 53.91%] [G loss: 1.923481]\n",
      "epoch:28 step:26758 [D loss: 0.618277, acc: 66.41%] [G loss: 2.044610]\n",
      "epoch:28 step:26759 [D loss: 0.628287, acc: 64.06%] [G loss: 1.927894]\n",
      "epoch:28 step:26760 [D loss: 0.629515, acc: 67.19%] [G loss: 1.937005]\n",
      "epoch:28 step:26761 [D loss: 0.631051, acc: 67.19%] [G loss: 1.816654]\n",
      "epoch:28 step:26762 [D loss: 0.623379, acc: 64.84%] [G loss: 1.837501]\n",
      "epoch:28 step:26763 [D loss: 0.602521, acc: 64.06%] [G loss: 1.821815]\n",
      "epoch:28 step:26764 [D loss: 0.714319, acc: 53.12%] [G loss: 1.618825]\n",
      "epoch:28 step:26765 [D loss: 0.648107, acc: 58.59%] [G loss: 1.785323]\n",
      "epoch:28 step:26766 [D loss: 0.653075, acc: 62.50%] [G loss: 1.797604]\n",
      "epoch:28 step:26767 [D loss: 0.684583, acc: 59.38%] [G loss: 1.769249]\n",
      "epoch:28 step:26768 [D loss: 0.611783, acc: 64.06%] [G loss: 2.057672]\n",
      "epoch:28 step:26769 [D loss: 0.700081, acc: 53.12%] [G loss: 1.972553]\n",
      "epoch:28 step:26770 [D loss: 0.603753, acc: 67.19%] [G loss: 2.026880]\n",
      "epoch:28 step:26771 [D loss: 0.700941, acc: 53.12%] [G loss: 1.874046]\n",
      "epoch:28 step:26772 [D loss: 0.645609, acc: 64.06%] [G loss: 1.913612]\n",
      "epoch:28 step:26773 [D loss: 0.661315, acc: 59.38%] [G loss: 1.798470]\n",
      "epoch:28 step:26774 [D loss: 0.665440, acc: 61.72%] [G loss: 1.873047]\n",
      "epoch:28 step:26775 [D loss: 0.666185, acc: 55.47%] [G loss: 1.811301]\n",
      "epoch:28 step:26776 [D loss: 0.658158, acc: 63.28%] [G loss: 1.705972]\n",
      "epoch:28 step:26777 [D loss: 0.689017, acc: 59.38%] [G loss: 1.830428]\n",
      "epoch:28 step:26778 [D loss: 0.640645, acc: 57.03%] [G loss: 1.751663]\n",
      "epoch:28 step:26779 [D loss: 0.633629, acc: 64.84%] [G loss: 1.925388]\n",
      "epoch:28 step:26780 [D loss: 0.649543, acc: 60.16%] [G loss: 1.886431]\n",
      "epoch:28 step:26781 [D loss: 0.606600, acc: 68.75%] [G loss: 1.873237]\n",
      "epoch:28 step:26782 [D loss: 0.710698, acc: 53.12%] [G loss: 1.933632]\n",
      "epoch:28 step:26783 [D loss: 0.654395, acc: 62.50%] [G loss: 1.947556]\n",
      "epoch:28 step:26784 [D loss: 0.634054, acc: 64.06%] [G loss: 1.738026]\n",
      "epoch:28 step:26785 [D loss: 0.589987, acc: 74.22%] [G loss: 1.946396]\n",
      "epoch:28 step:26786 [D loss: 0.618906, acc: 66.41%] [G loss: 2.079752]\n",
      "epoch:28 step:26787 [D loss: 0.658545, acc: 60.94%] [G loss: 2.002244]\n",
      "epoch:28 step:26788 [D loss: 0.682108, acc: 57.03%] [G loss: 2.056268]\n",
      "epoch:28 step:26789 [D loss: 0.633358, acc: 64.84%] [G loss: 1.758767]\n",
      "epoch:28 step:26790 [D loss: 0.583086, acc: 70.31%] [G loss: 2.094390]\n",
      "epoch:28 step:26791 [D loss: 0.632024, acc: 68.75%] [G loss: 2.059649]\n",
      "epoch:28 step:26792 [D loss: 0.594651, acc: 67.19%] [G loss: 2.207788]\n",
      "epoch:28 step:26793 [D loss: 0.600899, acc: 67.97%] [G loss: 1.997649]\n",
      "epoch:28 step:26794 [D loss: 0.611761, acc: 64.06%] [G loss: 2.021098]\n",
      "epoch:28 step:26795 [D loss: 0.721588, acc: 53.91%] [G loss: 1.785804]\n",
      "epoch:28 step:26796 [D loss: 0.664831, acc: 59.38%] [G loss: 1.722263]\n",
      "epoch:28 step:26797 [D loss: 0.666963, acc: 60.16%] [G loss: 1.873090]\n",
      "epoch:28 step:26798 [D loss: 0.653110, acc: 60.94%] [G loss: 1.933403]\n",
      "epoch:28 step:26799 [D loss: 0.644543, acc: 64.06%] [G loss: 1.995129]\n",
      "epoch:28 step:26800 [D loss: 0.668726, acc: 66.41%] [G loss: 2.111217]\n",
      "##############\n",
      "[2.42918442 1.82117441 6.25422552 4.33650351 3.46683632 5.51135707\n",
      " 4.3797749  4.69810816 4.49031995 3.53500748]\n",
      "##########\n",
      "epoch:28 step:26801 [D loss: 0.678870, acc: 60.94%] [G loss: 1.808508]\n",
      "epoch:28 step:26802 [D loss: 0.736085, acc: 44.53%] [G loss: 1.679712]\n",
      "epoch:28 step:26803 [D loss: 0.697429, acc: 52.34%] [G loss: 1.871735]\n",
      "epoch:28 step:26804 [D loss: 0.679821, acc: 56.25%] [G loss: 1.825002]\n",
      "epoch:28 step:26805 [D loss: 0.693120, acc: 57.03%] [G loss: 1.778538]\n",
      "epoch:28 step:26806 [D loss: 0.646769, acc: 62.50%] [G loss: 1.816240]\n",
      "epoch:28 step:26807 [D loss: 0.648774, acc: 64.06%] [G loss: 1.799293]\n",
      "epoch:28 step:26808 [D loss: 0.707022, acc: 54.69%] [G loss: 1.702555]\n",
      "epoch:28 step:26809 [D loss: 0.705054, acc: 47.66%] [G loss: 1.608821]\n",
      "epoch:28 step:26810 [D loss: 0.611067, acc: 65.62%] [G loss: 1.944134]\n",
      "epoch:28 step:26811 [D loss: 0.672966, acc: 61.72%] [G loss: 1.683345]\n",
      "epoch:28 step:26812 [D loss: 0.664797, acc: 57.81%] [G loss: 1.719753]\n",
      "epoch:28 step:26813 [D loss: 0.658990, acc: 61.72%] [G loss: 1.733494]\n",
      "epoch:28 step:26814 [D loss: 0.579068, acc: 75.00%] [G loss: 1.881241]\n",
      "epoch:28 step:26815 [D loss: 0.618749, acc: 64.84%] [G loss: 1.707456]\n",
      "epoch:28 step:26816 [D loss: 0.669934, acc: 64.06%] [G loss: 1.846606]\n",
      "epoch:28 step:26817 [D loss: 0.611184, acc: 68.75%] [G loss: 1.807091]\n",
      "epoch:28 step:26818 [D loss: 0.675994, acc: 60.16%] [G loss: 1.888166]\n",
      "epoch:28 step:26819 [D loss: 0.634692, acc: 66.41%] [G loss: 1.831282]\n",
      "epoch:28 step:26820 [D loss: 0.678577, acc: 60.94%] [G loss: 1.750394]\n",
      "epoch:28 step:26821 [D loss: 0.679354, acc: 57.81%] [G loss: 1.911070]\n",
      "epoch:28 step:26822 [D loss: 0.652530, acc: 58.59%] [G loss: 1.813739]\n",
      "epoch:28 step:26823 [D loss: 0.676338, acc: 59.38%] [G loss: 1.783853]\n",
      "epoch:28 step:26824 [D loss: 0.678292, acc: 57.03%] [G loss: 1.945027]\n",
      "epoch:28 step:26825 [D loss: 0.678867, acc: 58.59%] [G loss: 1.865222]\n",
      "epoch:28 step:26826 [D loss: 0.685186, acc: 60.94%] [G loss: 1.847875]\n",
      "epoch:28 step:26827 [D loss: 0.645849, acc: 63.28%] [G loss: 1.853038]\n",
      "epoch:28 step:26828 [D loss: 0.721586, acc: 53.91%] [G loss: 1.801846]\n",
      "epoch:28 step:26829 [D loss: 0.625231, acc: 60.94%] [G loss: 1.887721]\n",
      "epoch:28 step:26830 [D loss: 0.662173, acc: 60.94%] [G loss: 1.738922]\n",
      "epoch:28 step:26831 [D loss: 0.660846, acc: 64.06%] [G loss: 1.808350]\n",
      "epoch:28 step:26832 [D loss: 0.657144, acc: 60.16%] [G loss: 1.741999]\n",
      "epoch:28 step:26833 [D loss: 0.679763, acc: 57.81%] [G loss: 1.643840]\n",
      "epoch:28 step:26834 [D loss: 0.701623, acc: 54.69%] [G loss: 1.742839]\n",
      "epoch:28 step:26835 [D loss: 0.609237, acc: 67.97%] [G loss: 1.697084]\n",
      "epoch:28 step:26836 [D loss: 0.683219, acc: 62.50%] [G loss: 1.785630]\n",
      "epoch:28 step:26837 [D loss: 0.642828, acc: 64.06%] [G loss: 1.747257]\n",
      "epoch:28 step:26838 [D loss: 0.677841, acc: 60.94%] [G loss: 1.898796]\n",
      "epoch:28 step:26839 [D loss: 0.641598, acc: 60.16%] [G loss: 1.919999]\n",
      "epoch:28 step:26840 [D loss: 0.658940, acc: 61.72%] [G loss: 1.888380]\n",
      "epoch:28 step:26841 [D loss: 0.673694, acc: 56.25%] [G loss: 2.026039]\n",
      "epoch:28 step:26842 [D loss: 0.645923, acc: 65.62%] [G loss: 1.721086]\n",
      "epoch:28 step:26843 [D loss: 0.639498, acc: 65.62%] [G loss: 1.871837]\n",
      "epoch:28 step:26844 [D loss: 0.625140, acc: 58.59%] [G loss: 1.829236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26845 [D loss: 0.672449, acc: 64.06%] [G loss: 1.902486]\n",
      "epoch:28 step:26846 [D loss: 0.594473, acc: 67.97%] [G loss: 1.818849]\n",
      "epoch:28 step:26847 [D loss: 0.685288, acc: 53.91%] [G loss: 1.802430]\n",
      "epoch:28 step:26848 [D loss: 0.646100, acc: 62.50%] [G loss: 1.923787]\n",
      "epoch:28 step:26849 [D loss: 0.599111, acc: 67.97%] [G loss: 1.927684]\n",
      "epoch:28 step:26850 [D loss: 0.671696, acc: 58.59%] [G loss: 1.785336]\n",
      "epoch:28 step:26851 [D loss: 0.700449, acc: 56.25%] [G loss: 1.670446]\n",
      "epoch:28 step:26852 [D loss: 0.646390, acc: 65.62%] [G loss: 1.821523]\n",
      "epoch:28 step:26853 [D loss: 0.674612, acc: 55.47%] [G loss: 1.739741]\n",
      "epoch:28 step:26854 [D loss: 0.665755, acc: 64.84%] [G loss: 1.775276]\n",
      "epoch:28 step:26855 [D loss: 0.680182, acc: 57.03%] [G loss: 1.840631]\n",
      "epoch:28 step:26856 [D loss: 0.684306, acc: 56.25%] [G loss: 1.726592]\n",
      "epoch:28 step:26857 [D loss: 0.681747, acc: 54.69%] [G loss: 1.806906]\n",
      "epoch:28 step:26858 [D loss: 0.666977, acc: 55.47%] [G loss: 1.890578]\n",
      "epoch:28 step:26859 [D loss: 0.639199, acc: 64.84%] [G loss: 1.787331]\n",
      "epoch:28 step:26860 [D loss: 0.652797, acc: 54.69%] [G loss: 2.002633]\n",
      "epoch:28 step:26861 [D loss: 0.662022, acc: 57.81%] [G loss: 1.793285]\n",
      "epoch:28 step:26862 [D loss: 0.673008, acc: 61.72%] [G loss: 1.785007]\n",
      "epoch:28 step:26863 [D loss: 0.652816, acc: 60.16%] [G loss: 1.899961]\n",
      "epoch:28 step:26864 [D loss: 0.721172, acc: 56.25%] [G loss: 1.798869]\n",
      "epoch:28 step:26865 [D loss: 0.587226, acc: 71.88%] [G loss: 1.827426]\n",
      "epoch:28 step:26866 [D loss: 0.663559, acc: 61.72%] [G loss: 1.976003]\n",
      "epoch:28 step:26867 [D loss: 0.579274, acc: 72.66%] [G loss: 2.000340]\n",
      "epoch:28 step:26868 [D loss: 0.614469, acc: 66.41%] [G loss: 1.989284]\n",
      "epoch:28 step:26869 [D loss: 0.617562, acc: 60.94%] [G loss: 1.922664]\n",
      "epoch:28 step:26870 [D loss: 0.613924, acc: 67.97%] [G loss: 2.076162]\n",
      "epoch:28 step:26871 [D loss: 0.636075, acc: 64.06%] [G loss: 1.874137]\n",
      "epoch:28 step:26872 [D loss: 0.639796, acc: 66.41%] [G loss: 1.837047]\n",
      "epoch:28 step:26873 [D loss: 0.596138, acc: 67.19%] [G loss: 1.901477]\n",
      "epoch:28 step:26874 [D loss: 0.631487, acc: 65.62%] [G loss: 1.944364]\n",
      "epoch:28 step:26875 [D loss: 0.647930, acc: 58.59%] [G loss: 2.029766]\n",
      "epoch:28 step:26876 [D loss: 0.668602, acc: 55.47%] [G loss: 1.952163]\n",
      "epoch:28 step:26877 [D loss: 0.654402, acc: 64.06%] [G loss: 1.966922]\n",
      "epoch:28 step:26878 [D loss: 0.630063, acc: 63.28%] [G loss: 1.994306]\n",
      "epoch:28 step:26879 [D loss: 0.641714, acc: 63.28%] [G loss: 1.951701]\n",
      "epoch:28 step:26880 [D loss: 0.704671, acc: 62.50%] [G loss: 1.954643]\n",
      "epoch:28 step:26881 [D loss: 0.643346, acc: 59.38%] [G loss: 1.882499]\n",
      "epoch:28 step:26882 [D loss: 0.588265, acc: 68.75%] [G loss: 1.997785]\n",
      "epoch:28 step:26883 [D loss: 0.608091, acc: 66.41%] [G loss: 2.082958]\n",
      "epoch:28 step:26884 [D loss: 0.593557, acc: 65.62%] [G loss: 2.259104]\n",
      "epoch:28 step:26885 [D loss: 0.648993, acc: 58.59%] [G loss: 2.205223]\n",
      "epoch:28 step:26886 [D loss: 0.634905, acc: 63.28%] [G loss: 2.045770]\n",
      "epoch:28 step:26887 [D loss: 0.606664, acc: 65.62%] [G loss: 1.970393]\n",
      "epoch:28 step:26888 [D loss: 0.660533, acc: 66.41%] [G loss: 1.856245]\n",
      "epoch:28 step:26889 [D loss: 0.624864, acc: 66.41%] [G loss: 1.939085]\n",
      "epoch:28 step:26890 [D loss: 0.615355, acc: 66.41%] [G loss: 1.994597]\n",
      "epoch:28 step:26891 [D loss: 0.656005, acc: 58.59%] [G loss: 1.742148]\n",
      "epoch:28 step:26892 [D loss: 0.662567, acc: 59.38%] [G loss: 1.878568]\n",
      "epoch:28 step:26893 [D loss: 0.679895, acc: 56.25%] [G loss: 1.705342]\n",
      "epoch:28 step:26894 [D loss: 0.652975, acc: 58.59%] [G loss: 1.836762]\n",
      "epoch:28 step:26895 [D loss: 0.644080, acc: 64.06%] [G loss: 1.815017]\n",
      "epoch:28 step:26896 [D loss: 0.631951, acc: 64.06%] [G loss: 2.132982]\n",
      "epoch:28 step:26897 [D loss: 0.636086, acc: 62.50%] [G loss: 1.821463]\n",
      "epoch:28 step:26898 [D loss: 0.638152, acc: 61.72%] [G loss: 1.896657]\n",
      "epoch:28 step:26899 [D loss: 0.674026, acc: 58.59%] [G loss: 1.884466]\n",
      "epoch:28 step:26900 [D loss: 0.717879, acc: 51.56%] [G loss: 1.853747]\n",
      "epoch:28 step:26901 [D loss: 0.630937, acc: 64.84%] [G loss: 1.748366]\n",
      "epoch:28 step:26902 [D loss: 0.670110, acc: 63.28%] [G loss: 1.843717]\n",
      "epoch:28 step:26903 [D loss: 0.678730, acc: 57.81%] [G loss: 1.773939]\n",
      "epoch:28 step:26904 [D loss: 0.683743, acc: 62.50%] [G loss: 1.880497]\n",
      "epoch:28 step:26905 [D loss: 0.661474, acc: 59.38%] [G loss: 1.778228]\n",
      "epoch:28 step:26906 [D loss: 0.674661, acc: 56.25%] [G loss: 1.728431]\n",
      "epoch:28 step:26907 [D loss: 0.661730, acc: 62.50%] [G loss: 1.861527]\n",
      "epoch:28 step:26908 [D loss: 0.630661, acc: 66.41%] [G loss: 1.845613]\n",
      "epoch:28 step:26909 [D loss: 0.657270, acc: 64.84%] [G loss: 1.876780]\n",
      "epoch:28 step:26910 [D loss: 0.631778, acc: 67.19%] [G loss: 1.777668]\n",
      "epoch:28 step:26911 [D loss: 0.642945, acc: 64.06%] [G loss: 1.820024]\n",
      "epoch:28 step:26912 [D loss: 0.690605, acc: 57.81%] [G loss: 1.743678]\n",
      "epoch:28 step:26913 [D loss: 0.640768, acc: 64.84%] [G loss: 1.753792]\n",
      "epoch:28 step:26914 [D loss: 0.644900, acc: 64.84%] [G loss: 1.845260]\n",
      "epoch:28 step:26915 [D loss: 0.659592, acc: 65.62%] [G loss: 1.782175]\n",
      "epoch:28 step:26916 [D loss: 0.617161, acc: 64.84%] [G loss: 1.935559]\n",
      "epoch:28 step:26917 [D loss: 0.586296, acc: 73.44%] [G loss: 1.895085]\n",
      "epoch:28 step:26918 [D loss: 0.701422, acc: 53.91%] [G loss: 1.833617]\n",
      "epoch:28 step:26919 [D loss: 0.669262, acc: 63.28%] [G loss: 1.689378]\n",
      "epoch:28 step:26920 [D loss: 0.691231, acc: 59.38%] [G loss: 1.784094]\n",
      "epoch:28 step:26921 [D loss: 0.678657, acc: 58.59%] [G loss: 1.849670]\n",
      "epoch:28 step:26922 [D loss: 0.659771, acc: 60.16%] [G loss: 1.974623]\n",
      "epoch:28 step:26923 [D loss: 0.645827, acc: 64.84%] [G loss: 1.926331]\n",
      "epoch:28 step:26924 [D loss: 0.649526, acc: 60.94%] [G loss: 1.876890]\n",
      "epoch:28 step:26925 [D loss: 0.661274, acc: 63.28%] [G loss: 1.871941]\n",
      "epoch:28 step:26926 [D loss: 0.663029, acc: 63.28%] [G loss: 1.998687]\n",
      "epoch:28 step:26927 [D loss: 0.664866, acc: 59.38%] [G loss: 1.931212]\n",
      "epoch:28 step:26928 [D loss: 0.584283, acc: 69.53%] [G loss: 2.051352]\n",
      "epoch:28 step:26929 [D loss: 0.629542, acc: 63.28%] [G loss: 1.872220]\n",
      "epoch:28 step:26930 [D loss: 0.594200, acc: 71.88%] [G loss: 2.095676]\n",
      "epoch:28 step:26931 [D loss: 0.622485, acc: 64.06%] [G loss: 1.941263]\n",
      "epoch:28 step:26932 [D loss: 0.617978, acc: 64.84%] [G loss: 1.867374]\n",
      "epoch:28 step:26933 [D loss: 0.634028, acc: 62.50%] [G loss: 1.867143]\n",
      "epoch:28 step:26934 [D loss: 0.655392, acc: 55.47%] [G loss: 1.816650]\n",
      "epoch:28 step:26935 [D loss: 0.639076, acc: 66.41%] [G loss: 1.919232]\n",
      "epoch:28 step:26936 [D loss: 0.660706, acc: 64.84%] [G loss: 1.812481]\n",
      "epoch:28 step:26937 [D loss: 0.664609, acc: 59.38%] [G loss: 1.805918]\n",
      "epoch:28 step:26938 [D loss: 0.661904, acc: 59.38%] [G loss: 1.763054]\n",
      "epoch:28 step:26939 [D loss: 0.733670, acc: 51.56%] [G loss: 1.831868]\n",
      "epoch:28 step:26940 [D loss: 0.712041, acc: 51.56%] [G loss: 1.700421]\n",
      "epoch:28 step:26941 [D loss: 0.626795, acc: 64.06%] [G loss: 1.817307]\n",
      "epoch:28 step:26942 [D loss: 0.633272, acc: 67.19%] [G loss: 1.992445]\n",
      "epoch:28 step:26943 [D loss: 0.650764, acc: 64.06%] [G loss: 1.804040]\n",
      "epoch:28 step:26944 [D loss: 0.600711, acc: 69.53%] [G loss: 1.895805]\n",
      "epoch:28 step:26945 [D loss: 0.609085, acc: 69.53%] [G loss: 1.890970]\n",
      "epoch:28 step:26946 [D loss: 0.706654, acc: 53.12%] [G loss: 1.774522]\n",
      "epoch:28 step:26947 [D loss: 0.651772, acc: 60.94%] [G loss: 1.924609]\n",
      "epoch:28 step:26948 [D loss: 0.629013, acc: 63.28%] [G loss: 1.930370]\n",
      "epoch:28 step:26949 [D loss: 0.658400, acc: 60.16%] [G loss: 1.862111]\n",
      "epoch:28 step:26950 [D loss: 0.629115, acc: 70.31%] [G loss: 1.822817]\n",
      "epoch:28 step:26951 [D loss: 0.657681, acc: 60.94%] [G loss: 1.865047]\n",
      "epoch:28 step:26952 [D loss: 0.663546, acc: 60.94%] [G loss: 1.763482]\n",
      "epoch:28 step:26953 [D loss: 0.652307, acc: 62.50%] [G loss: 1.789812]\n",
      "epoch:28 step:26954 [D loss: 0.633681, acc: 64.06%] [G loss: 1.775044]\n",
      "epoch:28 step:26955 [D loss: 0.643516, acc: 66.41%] [G loss: 2.035816]\n",
      "epoch:28 step:26956 [D loss: 0.661405, acc: 57.81%] [G loss: 1.855447]\n",
      "epoch:28 step:26957 [D loss: 0.642339, acc: 67.19%] [G loss: 1.963166]\n",
      "epoch:28 step:26958 [D loss: 0.671901, acc: 60.16%] [G loss: 1.814582]\n",
      "epoch:28 step:26959 [D loss: 0.640749, acc: 64.06%] [G loss: 1.989018]\n",
      "epoch:28 step:26960 [D loss: 0.642634, acc: 63.28%] [G loss: 1.950796]\n",
      "epoch:28 step:26961 [D loss: 0.627248, acc: 66.41%] [G loss: 1.994989]\n",
      "epoch:28 step:26962 [D loss: 0.610653, acc: 64.84%] [G loss: 1.965431]\n",
      "epoch:28 step:26963 [D loss: 0.643068, acc: 60.94%] [G loss: 1.927004]\n",
      "epoch:28 step:26964 [D loss: 0.691575, acc: 59.38%] [G loss: 2.008958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26965 [D loss: 0.629999, acc: 64.84%] [G loss: 1.780184]\n",
      "epoch:28 step:26966 [D loss: 0.628264, acc: 66.41%] [G loss: 1.849265]\n",
      "epoch:28 step:26967 [D loss: 0.670029, acc: 59.38%] [G loss: 1.884318]\n",
      "epoch:28 step:26968 [D loss: 0.619768, acc: 64.84%] [G loss: 1.793558]\n",
      "epoch:28 step:26969 [D loss: 0.658469, acc: 54.69%] [G loss: 1.790555]\n",
      "epoch:28 step:26970 [D loss: 0.706296, acc: 56.25%] [G loss: 1.724014]\n",
      "epoch:28 step:26971 [D loss: 0.633793, acc: 66.41%] [G loss: 1.927616]\n",
      "epoch:28 step:26972 [D loss: 0.652697, acc: 58.59%] [G loss: 1.905521]\n",
      "epoch:28 step:26973 [D loss: 0.654333, acc: 66.41%] [G loss: 1.937922]\n",
      "epoch:28 step:26974 [D loss: 0.619622, acc: 67.97%] [G loss: 1.833120]\n",
      "epoch:28 step:26975 [D loss: 0.679505, acc: 53.91%] [G loss: 1.895784]\n",
      "epoch:28 step:26976 [D loss: 0.587884, acc: 71.09%] [G loss: 2.050182]\n",
      "epoch:28 step:26977 [D loss: 0.646755, acc: 63.28%] [G loss: 1.841870]\n",
      "epoch:28 step:26978 [D loss: 0.667920, acc: 61.72%] [G loss: 1.724491]\n",
      "epoch:28 step:26979 [D loss: 0.691898, acc: 59.38%] [G loss: 1.959436]\n",
      "epoch:28 step:26980 [D loss: 0.617855, acc: 69.53%] [G loss: 1.928240]\n",
      "epoch:28 step:26981 [D loss: 0.676313, acc: 61.72%] [G loss: 1.989325]\n",
      "epoch:28 step:26982 [D loss: 0.554268, acc: 75.78%] [G loss: 1.975523]\n",
      "epoch:28 step:26983 [D loss: 0.636381, acc: 61.72%] [G loss: 1.968696]\n",
      "epoch:28 step:26984 [D loss: 0.663396, acc: 57.03%] [G loss: 1.759452]\n",
      "epoch:28 step:26985 [D loss: 0.688931, acc: 56.25%] [G loss: 1.755160]\n",
      "epoch:28 step:26986 [D loss: 0.643006, acc: 58.59%] [G loss: 1.797289]\n",
      "epoch:28 step:26987 [D loss: 0.609145, acc: 65.62%] [G loss: 1.869200]\n",
      "epoch:28 step:26988 [D loss: 0.730532, acc: 52.34%] [G loss: 1.743805]\n",
      "epoch:28 step:26989 [D loss: 0.712711, acc: 57.81%] [G loss: 1.568817]\n",
      "epoch:28 step:26990 [D loss: 0.636451, acc: 63.28%] [G loss: 1.828221]\n",
      "epoch:28 step:26991 [D loss: 0.664532, acc: 59.38%] [G loss: 1.937867]\n",
      "epoch:28 step:26992 [D loss: 0.650306, acc: 63.28%] [G loss: 1.871975]\n",
      "epoch:28 step:26993 [D loss: 0.648194, acc: 62.50%] [G loss: 1.983197]\n",
      "epoch:28 step:26994 [D loss: 0.662581, acc: 60.94%] [G loss: 1.880927]\n",
      "epoch:28 step:26995 [D loss: 0.705949, acc: 56.25%] [G loss: 1.847648]\n",
      "epoch:28 step:26996 [D loss: 0.620576, acc: 64.06%] [G loss: 1.801165]\n",
      "epoch:28 step:26997 [D loss: 0.678327, acc: 60.16%] [G loss: 1.937234]\n",
      "epoch:28 step:26998 [D loss: 0.701019, acc: 56.25%] [G loss: 1.833213]\n",
      "epoch:28 step:26999 [D loss: 0.613440, acc: 64.84%] [G loss: 1.783392]\n",
      "epoch:28 step:27000 [D loss: 0.643722, acc: 60.94%] [G loss: 1.686537]\n",
      "##############\n",
      "[2.48407592 1.45403469 5.98269964 4.55355793 3.54752734 5.76852508\n",
      " 4.16133991 4.65134556 4.40504466 3.57313632]\n",
      "##########\n",
      "epoch:28 step:27001 [D loss: 0.674560, acc: 58.59%] [G loss: 1.723599]\n",
      "epoch:28 step:27002 [D loss: 0.684743, acc: 53.12%] [G loss: 1.811426]\n",
      "epoch:28 step:27003 [D loss: 0.631631, acc: 63.28%] [G loss: 1.899576]\n",
      "epoch:28 step:27004 [D loss: 0.665864, acc: 57.81%] [G loss: 1.717937]\n",
      "epoch:28 step:27005 [D loss: 0.638196, acc: 65.62%] [G loss: 1.942253]\n",
      "epoch:28 step:27006 [D loss: 0.636392, acc: 64.84%] [G loss: 1.957617]\n",
      "epoch:28 step:27007 [D loss: 0.688037, acc: 60.94%] [G loss: 1.785585]\n",
      "epoch:28 step:27008 [D loss: 0.618279, acc: 67.97%] [G loss: 1.910760]\n",
      "epoch:28 step:27009 [D loss: 0.637406, acc: 61.72%] [G loss: 1.879555]\n",
      "epoch:28 step:27010 [D loss: 0.631299, acc: 69.53%] [G loss: 2.096002]\n",
      "epoch:28 step:27011 [D loss: 0.610706, acc: 62.50%] [G loss: 1.964852]\n",
      "epoch:28 step:27012 [D loss: 0.624699, acc: 64.84%] [G loss: 1.907421]\n",
      "epoch:28 step:27013 [D loss: 0.662162, acc: 60.16%] [G loss: 1.922435]\n",
      "epoch:28 step:27014 [D loss: 0.665037, acc: 65.62%] [G loss: 1.895806]\n",
      "epoch:28 step:27015 [D loss: 0.687182, acc: 55.47%] [G loss: 1.868720]\n",
      "epoch:28 step:27016 [D loss: 0.659452, acc: 61.72%] [G loss: 1.840129]\n",
      "epoch:28 step:27017 [D loss: 0.592448, acc: 67.19%] [G loss: 2.073173]\n",
      "epoch:28 step:27018 [D loss: 0.610047, acc: 63.28%] [G loss: 1.955893]\n",
      "epoch:28 step:27019 [D loss: 0.693247, acc: 57.03%] [G loss: 1.836966]\n",
      "epoch:28 step:27020 [D loss: 0.673777, acc: 55.47%] [G loss: 1.878920]\n",
      "epoch:28 step:27021 [D loss: 0.644029, acc: 60.16%] [G loss: 1.902209]\n",
      "epoch:28 step:27022 [D loss: 0.621650, acc: 65.62%] [G loss: 2.010062]\n",
      "epoch:28 step:27023 [D loss: 0.670522, acc: 57.03%] [G loss: 1.677581]\n",
      "epoch:28 step:27024 [D loss: 0.726877, acc: 53.91%] [G loss: 1.754457]\n",
      "epoch:28 step:27025 [D loss: 0.631883, acc: 62.50%] [G loss: 1.901778]\n",
      "epoch:28 step:27026 [D loss: 0.632830, acc: 67.97%] [G loss: 1.913553]\n",
      "epoch:28 step:27027 [D loss: 0.624849, acc: 64.06%] [G loss: 1.898116]\n",
      "epoch:28 step:27028 [D loss: 0.628802, acc: 62.50%] [G loss: 1.983468]\n",
      "epoch:28 step:27029 [D loss: 0.613442, acc: 64.06%] [G loss: 1.811247]\n",
      "epoch:28 step:27030 [D loss: 0.745180, acc: 49.22%] [G loss: 1.695205]\n",
      "epoch:28 step:27031 [D loss: 0.644694, acc: 62.50%] [G loss: 1.854945]\n",
      "epoch:28 step:27032 [D loss: 0.626086, acc: 67.97%] [G loss: 1.832258]\n",
      "epoch:28 step:27033 [D loss: 0.657312, acc: 60.94%] [G loss: 1.842082]\n",
      "epoch:28 step:27034 [D loss: 0.649701, acc: 64.06%] [G loss: 1.781322]\n",
      "epoch:28 step:27035 [D loss: 0.700654, acc: 54.69%] [G loss: 1.824838]\n",
      "epoch:28 step:27036 [D loss: 0.669355, acc: 60.16%] [G loss: 1.746037]\n",
      "epoch:28 step:27037 [D loss: 0.675547, acc: 57.81%] [G loss: 1.876480]\n",
      "epoch:28 step:27038 [D loss: 0.652016, acc: 62.50%] [G loss: 1.838404]\n",
      "epoch:28 step:27039 [D loss: 0.667196, acc: 60.16%] [G loss: 1.805833]\n",
      "epoch:28 step:27040 [D loss: 0.616048, acc: 68.75%] [G loss: 1.799081]\n",
      "epoch:28 step:27041 [D loss: 0.673905, acc: 60.16%] [G loss: 1.870861]\n",
      "epoch:28 step:27042 [D loss: 0.635104, acc: 64.84%] [G loss: 1.847022]\n",
      "epoch:28 step:27043 [D loss: 0.610885, acc: 64.06%] [G loss: 1.829250]\n",
      "epoch:28 step:27044 [D loss: 0.652899, acc: 58.59%] [G loss: 1.753173]\n",
      "epoch:28 step:27045 [D loss: 0.692884, acc: 59.38%] [G loss: 1.800668]\n",
      "epoch:28 step:27046 [D loss: 0.695579, acc: 56.25%] [G loss: 1.903730]\n",
      "epoch:28 step:27047 [D loss: 0.643444, acc: 60.94%] [G loss: 1.914271]\n",
      "epoch:28 step:27048 [D loss: 0.702863, acc: 57.03%] [G loss: 1.769997]\n",
      "epoch:28 step:27049 [D loss: 0.645664, acc: 60.94%] [G loss: 1.924275]\n",
      "epoch:28 step:27050 [D loss: 0.639743, acc: 69.53%] [G loss: 1.826704]\n",
      "epoch:28 step:27051 [D loss: 0.582310, acc: 67.97%] [G loss: 1.961935]\n",
      "epoch:28 step:27052 [D loss: 0.606057, acc: 67.97%] [G loss: 2.009233]\n",
      "epoch:28 step:27053 [D loss: 0.674267, acc: 62.50%] [G loss: 1.898742]\n",
      "epoch:28 step:27054 [D loss: 0.688314, acc: 60.16%] [G loss: 1.690675]\n",
      "epoch:28 step:27055 [D loss: 0.632328, acc: 63.28%] [G loss: 1.906720]\n",
      "epoch:28 step:27056 [D loss: 0.737376, acc: 53.12%] [G loss: 1.737430]\n",
      "epoch:28 step:27057 [D loss: 0.666113, acc: 58.59%] [G loss: 1.728953]\n",
      "epoch:28 step:27058 [D loss: 0.631070, acc: 65.62%] [G loss: 1.798736]\n",
      "epoch:28 step:27059 [D loss: 0.618171, acc: 64.06%] [G loss: 2.060594]\n",
      "epoch:28 step:27060 [D loss: 0.634413, acc: 66.41%] [G loss: 1.829880]\n",
      "epoch:28 step:27061 [D loss: 0.644907, acc: 61.72%] [G loss: 1.930440]\n",
      "epoch:28 step:27062 [D loss: 0.662148, acc: 64.84%] [G loss: 1.923856]\n",
      "epoch:28 step:27063 [D loss: 0.665211, acc: 58.59%] [G loss: 1.755903]\n",
      "epoch:28 step:27064 [D loss: 0.734965, acc: 49.22%] [G loss: 1.726512]\n",
      "epoch:28 step:27065 [D loss: 0.661161, acc: 57.81%] [G loss: 1.689955]\n",
      "epoch:28 step:27066 [D loss: 0.687088, acc: 55.47%] [G loss: 1.802783]\n",
      "epoch:28 step:27067 [D loss: 0.675752, acc: 66.41%] [G loss: 1.782762]\n",
      "epoch:28 step:27068 [D loss: 0.679041, acc: 60.16%] [G loss: 1.796942]\n",
      "epoch:28 step:27069 [D loss: 0.595141, acc: 67.19%] [G loss: 1.963149]\n",
      "epoch:28 step:27070 [D loss: 0.676234, acc: 59.38%] [G loss: 1.796881]\n",
      "epoch:28 step:27071 [D loss: 0.668733, acc: 56.25%] [G loss: 1.832864]\n",
      "epoch:28 step:27072 [D loss: 0.649001, acc: 64.06%] [G loss: 1.842572]\n",
      "epoch:28 step:27073 [D loss: 0.630828, acc: 63.28%] [G loss: 1.897730]\n",
      "epoch:28 step:27074 [D loss: 0.658198, acc: 64.06%] [G loss: 1.834765]\n",
      "epoch:28 step:27075 [D loss: 0.645688, acc: 63.28%] [G loss: 1.752393]\n",
      "epoch:28 step:27076 [D loss: 0.626777, acc: 61.72%] [G loss: 1.814521]\n",
      "epoch:28 step:27077 [D loss: 0.641581, acc: 67.19%] [G loss: 1.890160]\n",
      "epoch:28 step:27078 [D loss: 0.655578, acc: 64.84%] [G loss: 1.883022]\n",
      "epoch:28 step:27079 [D loss: 0.659957, acc: 58.59%] [G loss: 1.896883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27080 [D loss: 0.628373, acc: 64.06%] [G loss: 1.933820]\n",
      "epoch:28 step:27081 [D loss: 0.634377, acc: 64.84%] [G loss: 1.846514]\n",
      "epoch:28 step:27082 [D loss: 0.635445, acc: 63.28%] [G loss: 1.714620]\n",
      "epoch:28 step:27083 [D loss: 0.602205, acc: 64.06%] [G loss: 1.965851]\n",
      "epoch:28 step:27084 [D loss: 0.659121, acc: 60.16%] [G loss: 1.913742]\n",
      "epoch:28 step:27085 [D loss: 0.628933, acc: 59.38%] [G loss: 2.004633]\n",
      "epoch:28 step:27086 [D loss: 0.694310, acc: 53.91%] [G loss: 1.759549]\n",
      "epoch:28 step:27087 [D loss: 0.681308, acc: 59.38%] [G loss: 1.838242]\n",
      "epoch:28 step:27088 [D loss: 0.670797, acc: 56.25%] [G loss: 1.810879]\n",
      "epoch:28 step:27089 [D loss: 0.611934, acc: 66.41%] [G loss: 1.917263]\n",
      "epoch:28 step:27090 [D loss: 0.643512, acc: 62.50%] [G loss: 1.899324]\n",
      "epoch:28 step:27091 [D loss: 0.610239, acc: 64.06%] [G loss: 1.898267]\n",
      "epoch:28 step:27092 [D loss: 0.647781, acc: 57.81%] [G loss: 1.828825]\n",
      "epoch:28 step:27093 [D loss: 0.631771, acc: 67.19%] [G loss: 1.863260]\n",
      "epoch:28 step:27094 [D loss: 0.758916, acc: 47.66%] [G loss: 1.765135]\n",
      "epoch:28 step:27095 [D loss: 0.687109, acc: 51.56%] [G loss: 1.888604]\n",
      "epoch:28 step:27096 [D loss: 0.636591, acc: 65.62%] [G loss: 1.823309]\n",
      "epoch:28 step:27097 [D loss: 0.661336, acc: 62.50%] [G loss: 1.676972]\n",
      "epoch:28 step:27098 [D loss: 0.636059, acc: 64.84%] [G loss: 1.712046]\n",
      "epoch:28 step:27099 [D loss: 0.671387, acc: 58.59%] [G loss: 1.757953]\n",
      "epoch:28 step:27100 [D loss: 0.651407, acc: 61.72%] [G loss: 1.822545]\n",
      "epoch:28 step:27101 [D loss: 0.646804, acc: 64.06%] [G loss: 1.818062]\n",
      "epoch:28 step:27102 [D loss: 0.636398, acc: 62.50%] [G loss: 1.874626]\n",
      "epoch:28 step:27103 [D loss: 0.689126, acc: 57.81%] [G loss: 1.784452]\n",
      "epoch:28 step:27104 [D loss: 0.675005, acc: 59.38%] [G loss: 1.902690]\n",
      "epoch:28 step:27105 [D loss: 0.665994, acc: 59.38%] [G loss: 1.665333]\n",
      "epoch:28 step:27106 [D loss: 0.648146, acc: 59.38%] [G loss: 1.795900]\n",
      "epoch:28 step:27107 [D loss: 0.673367, acc: 59.38%] [G loss: 1.774613]\n",
      "epoch:28 step:27108 [D loss: 0.650189, acc: 63.28%] [G loss: 1.719719]\n",
      "epoch:28 step:27109 [D loss: 0.716621, acc: 57.03%] [G loss: 1.722804]\n",
      "epoch:28 step:27110 [D loss: 0.671560, acc: 62.50%] [G loss: 1.659371]\n",
      "epoch:28 step:27111 [D loss: 0.679086, acc: 58.59%] [G loss: 1.809574]\n",
      "epoch:28 step:27112 [D loss: 0.664864, acc: 60.16%] [G loss: 1.747632]\n",
      "epoch:28 step:27113 [D loss: 0.593231, acc: 66.41%] [G loss: 1.839764]\n",
      "epoch:28 step:27114 [D loss: 0.653021, acc: 59.38%] [G loss: 1.700212]\n",
      "epoch:28 step:27115 [D loss: 0.616613, acc: 67.19%] [G loss: 1.853238]\n",
      "epoch:28 step:27116 [D loss: 0.661853, acc: 58.59%] [G loss: 1.935948]\n",
      "epoch:28 step:27117 [D loss: 0.687441, acc: 57.03%] [G loss: 1.865451]\n",
      "epoch:28 step:27118 [D loss: 0.629502, acc: 64.06%] [G loss: 1.963282]\n",
      "epoch:28 step:27119 [D loss: 0.629941, acc: 64.84%] [G loss: 1.805947]\n",
      "epoch:28 step:27120 [D loss: 0.637283, acc: 64.84%] [G loss: 2.078776]\n",
      "epoch:28 step:27121 [D loss: 0.690215, acc: 62.50%] [G loss: 1.721489]\n",
      "epoch:28 step:27122 [D loss: 0.588537, acc: 69.53%] [G loss: 1.882680]\n",
      "epoch:28 step:27123 [D loss: 0.646991, acc: 62.50%] [G loss: 1.944667]\n",
      "epoch:28 step:27124 [D loss: 0.599662, acc: 68.75%] [G loss: 1.877523]\n",
      "epoch:28 step:27125 [D loss: 0.675285, acc: 57.03%] [G loss: 1.919435]\n",
      "epoch:28 step:27126 [D loss: 0.608337, acc: 71.88%] [G loss: 2.017388]\n",
      "epoch:28 step:27127 [D loss: 0.666685, acc: 56.25%] [G loss: 1.846673]\n",
      "epoch:28 step:27128 [D loss: 0.664698, acc: 57.81%] [G loss: 1.768785]\n",
      "epoch:28 step:27129 [D loss: 0.646415, acc: 58.59%] [G loss: 1.928082]\n",
      "epoch:28 step:27130 [D loss: 0.645858, acc: 67.19%] [G loss: 1.993071]\n",
      "epoch:28 step:27131 [D loss: 0.642676, acc: 60.94%] [G loss: 1.990048]\n",
      "epoch:28 step:27132 [D loss: 0.667768, acc: 53.91%] [G loss: 1.723246]\n",
      "epoch:28 step:27133 [D loss: 0.644144, acc: 67.97%] [G loss: 1.871974]\n",
      "epoch:28 step:27134 [D loss: 0.654752, acc: 58.59%] [G loss: 1.868872]\n",
      "epoch:28 step:27135 [D loss: 0.619957, acc: 65.62%] [G loss: 2.000890]\n",
      "epoch:28 step:27136 [D loss: 0.612867, acc: 71.09%] [G loss: 2.090189]\n",
      "epoch:28 step:27137 [D loss: 0.672430, acc: 63.28%] [G loss: 1.993856]\n",
      "epoch:28 step:27138 [D loss: 0.646264, acc: 63.28%] [G loss: 1.828344]\n",
      "epoch:28 step:27139 [D loss: 0.691720, acc: 53.91%] [G loss: 1.809671]\n",
      "epoch:28 step:27140 [D loss: 0.676450, acc: 59.38%] [G loss: 1.838119]\n",
      "epoch:28 step:27141 [D loss: 0.652013, acc: 64.84%] [G loss: 1.979292]\n",
      "epoch:28 step:27142 [D loss: 0.654055, acc: 58.59%] [G loss: 2.030331]\n",
      "epoch:28 step:27143 [D loss: 0.632960, acc: 64.06%] [G loss: 1.872905]\n",
      "epoch:28 step:27144 [D loss: 0.685832, acc: 56.25%] [G loss: 1.959469]\n",
      "epoch:28 step:27145 [D loss: 0.623496, acc: 62.50%] [G loss: 1.992649]\n",
      "epoch:28 step:27146 [D loss: 0.591161, acc: 71.09%] [G loss: 1.999360]\n",
      "epoch:28 step:27147 [D loss: 0.639239, acc: 65.62%] [G loss: 1.817190]\n",
      "epoch:28 step:27148 [D loss: 0.634333, acc: 62.50%] [G loss: 2.100497]\n",
      "epoch:28 step:27149 [D loss: 0.717272, acc: 53.91%] [G loss: 1.831784]\n",
      "epoch:28 step:27150 [D loss: 0.671185, acc: 60.94%] [G loss: 1.857161]\n",
      "epoch:28 step:27151 [D loss: 0.642563, acc: 67.97%] [G loss: 1.858795]\n",
      "epoch:28 step:27152 [D loss: 0.697071, acc: 57.81%] [G loss: 1.836716]\n",
      "epoch:28 step:27153 [D loss: 0.649029, acc: 63.28%] [G loss: 1.853663]\n",
      "epoch:28 step:27154 [D loss: 0.623359, acc: 66.41%] [G loss: 2.091732]\n",
      "epoch:28 step:27155 [D loss: 0.577809, acc: 69.53%] [G loss: 1.968419]\n",
      "epoch:28 step:27156 [D loss: 0.643834, acc: 61.72%] [G loss: 1.883978]\n",
      "epoch:28 step:27157 [D loss: 0.661127, acc: 62.50%] [G loss: 2.016568]\n",
      "epoch:28 step:27158 [D loss: 0.662421, acc: 57.03%] [G loss: 1.862314]\n",
      "epoch:28 step:27159 [D loss: 0.600443, acc: 68.75%] [G loss: 1.979671]\n",
      "epoch:28 step:27160 [D loss: 0.626155, acc: 64.06%] [G loss: 2.041732]\n",
      "epoch:28 step:27161 [D loss: 0.615569, acc: 66.41%] [G loss: 2.016776]\n",
      "epoch:28 step:27162 [D loss: 0.567142, acc: 70.31%] [G loss: 2.125752]\n",
      "epoch:28 step:27163 [D loss: 0.692555, acc: 57.03%] [G loss: 1.886420]\n",
      "epoch:28 step:27164 [D loss: 0.797637, acc: 46.88%] [G loss: 1.777849]\n",
      "epoch:28 step:27165 [D loss: 0.741458, acc: 50.00%] [G loss: 1.848068]\n",
      "epoch:28 step:27166 [D loss: 0.637000, acc: 63.28%] [G loss: 1.964100]\n",
      "epoch:28 step:27167 [D loss: 0.580861, acc: 74.22%] [G loss: 1.944601]\n",
      "epoch:28 step:27168 [D loss: 0.669777, acc: 64.06%] [G loss: 1.866585]\n",
      "epoch:28 step:27169 [D loss: 0.649218, acc: 63.28%] [G loss: 1.759332]\n",
      "epoch:28 step:27170 [D loss: 0.619227, acc: 65.62%] [G loss: 1.943842]\n",
      "epoch:28 step:27171 [D loss: 0.667310, acc: 57.81%] [G loss: 1.837599]\n",
      "epoch:28 step:27172 [D loss: 0.639851, acc: 67.97%] [G loss: 1.994525]\n",
      "epoch:28 step:27173 [D loss: 0.684700, acc: 60.16%] [G loss: 2.273356]\n",
      "epoch:29 step:27174 [D loss: 0.644519, acc: 60.94%] [G loss: 1.842540]\n",
      "epoch:29 step:27175 [D loss: 0.715513, acc: 57.03%] [G loss: 1.957564]\n",
      "epoch:29 step:27176 [D loss: 0.632634, acc: 62.50%] [G loss: 1.934775]\n",
      "epoch:29 step:27177 [D loss: 0.687990, acc: 56.25%] [G loss: 1.698076]\n",
      "epoch:29 step:27178 [D loss: 0.641096, acc: 63.28%] [G loss: 1.827966]\n",
      "epoch:29 step:27179 [D loss: 0.652418, acc: 60.94%] [G loss: 1.888062]\n",
      "epoch:29 step:27180 [D loss: 0.618210, acc: 64.06%] [G loss: 2.006824]\n",
      "epoch:29 step:27181 [D loss: 0.644263, acc: 64.84%] [G loss: 2.097562]\n",
      "epoch:29 step:27182 [D loss: 0.656487, acc: 60.94%] [G loss: 1.989085]\n",
      "epoch:29 step:27183 [D loss: 0.685967, acc: 58.59%] [G loss: 1.871391]\n",
      "epoch:29 step:27184 [D loss: 0.631561, acc: 60.94%] [G loss: 2.019202]\n",
      "epoch:29 step:27185 [D loss: 0.622269, acc: 62.50%] [G loss: 1.745022]\n",
      "epoch:29 step:27186 [D loss: 0.679151, acc: 60.94%] [G loss: 1.952499]\n",
      "epoch:29 step:27187 [D loss: 0.609056, acc: 65.62%] [G loss: 1.798000]\n",
      "epoch:29 step:27188 [D loss: 0.611158, acc: 71.88%] [G loss: 1.972940]\n",
      "epoch:29 step:27189 [D loss: 0.672599, acc: 63.28%] [G loss: 2.181637]\n",
      "epoch:29 step:27190 [D loss: 0.636010, acc: 60.94%] [G loss: 1.894727]\n",
      "epoch:29 step:27191 [D loss: 0.664134, acc: 59.38%] [G loss: 1.980422]\n",
      "epoch:29 step:27192 [D loss: 0.660343, acc: 62.50%] [G loss: 1.836976]\n",
      "epoch:29 step:27193 [D loss: 0.739167, acc: 49.22%] [G loss: 1.632051]\n",
      "epoch:29 step:27194 [D loss: 0.693886, acc: 55.47%] [G loss: 1.746724]\n",
      "epoch:29 step:27195 [D loss: 0.647516, acc: 66.41%] [G loss: 1.774701]\n",
      "epoch:29 step:27196 [D loss: 0.604034, acc: 70.31%] [G loss: 1.996308]\n",
      "epoch:29 step:27197 [D loss: 0.651291, acc: 58.59%] [G loss: 1.801046]\n",
      "epoch:29 step:27198 [D loss: 0.637664, acc: 60.16%] [G loss: 1.944677]\n",
      "epoch:29 step:27199 [D loss: 0.642059, acc: 61.72%] [G loss: 1.681352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27200 [D loss: 0.657348, acc: 59.38%] [G loss: 1.767875]\n",
      "##############\n",
      "[2.43633796 1.64186712 6.16576229 4.55700615 3.41005116 5.60825131\n",
      " 4.19619534 4.8882615  4.49168652 3.45499664]\n",
      "##########\n",
      "epoch:29 step:27201 [D loss: 0.635179, acc: 66.41%] [G loss: 1.839177]\n",
      "epoch:29 step:27202 [D loss: 0.581009, acc: 71.09%] [G loss: 1.927215]\n",
      "epoch:29 step:27203 [D loss: 0.620525, acc: 65.62%] [G loss: 1.957909]\n",
      "epoch:29 step:27204 [D loss: 0.689067, acc: 57.03%] [G loss: 1.701942]\n",
      "epoch:29 step:27205 [D loss: 0.704505, acc: 57.81%] [G loss: 1.696722]\n",
      "epoch:29 step:27206 [D loss: 0.627902, acc: 63.28%] [G loss: 1.912203]\n",
      "epoch:29 step:27207 [D loss: 0.624253, acc: 67.19%] [G loss: 1.765161]\n",
      "epoch:29 step:27208 [D loss: 0.629252, acc: 69.53%] [G loss: 1.898687]\n",
      "epoch:29 step:27209 [D loss: 0.645904, acc: 61.72%] [G loss: 1.786613]\n",
      "epoch:29 step:27210 [D loss: 0.627770, acc: 63.28%] [G loss: 1.871575]\n",
      "epoch:29 step:27211 [D loss: 0.597864, acc: 66.41%] [G loss: 1.986475]\n",
      "epoch:29 step:27212 [D loss: 0.636341, acc: 60.94%] [G loss: 1.867582]\n",
      "epoch:29 step:27213 [D loss: 0.661142, acc: 61.72%] [G loss: 1.966764]\n",
      "epoch:29 step:27214 [D loss: 0.664757, acc: 61.72%] [G loss: 1.775088]\n",
      "epoch:29 step:27215 [D loss: 0.659285, acc: 58.59%] [G loss: 1.815948]\n",
      "epoch:29 step:27216 [D loss: 0.664697, acc: 67.19%] [G loss: 1.712840]\n",
      "epoch:29 step:27217 [D loss: 0.682968, acc: 55.47%] [G loss: 1.818825]\n",
      "epoch:29 step:27218 [D loss: 0.635725, acc: 59.38%] [G loss: 1.823804]\n",
      "epoch:29 step:27219 [D loss: 0.672045, acc: 62.50%] [G loss: 1.801033]\n",
      "epoch:29 step:27220 [D loss: 0.611200, acc: 69.53%] [G loss: 1.875785]\n",
      "epoch:29 step:27221 [D loss: 0.602491, acc: 67.97%] [G loss: 1.963724]\n",
      "epoch:29 step:27222 [D loss: 0.601291, acc: 67.19%] [G loss: 1.963511]\n",
      "epoch:29 step:27223 [D loss: 0.645254, acc: 61.72%] [G loss: 1.903728]\n",
      "epoch:29 step:27224 [D loss: 0.724885, acc: 51.56%] [G loss: 1.817508]\n",
      "epoch:29 step:27225 [D loss: 0.639509, acc: 63.28%] [G loss: 1.896024]\n",
      "epoch:29 step:27226 [D loss: 0.663726, acc: 63.28%] [G loss: 1.891938]\n",
      "epoch:29 step:27227 [D loss: 0.618018, acc: 64.06%] [G loss: 1.953403]\n",
      "epoch:29 step:27228 [D loss: 0.629546, acc: 63.28%] [G loss: 1.958479]\n",
      "epoch:29 step:27229 [D loss: 0.603145, acc: 68.75%] [G loss: 2.003189]\n",
      "epoch:29 step:27230 [D loss: 0.668931, acc: 60.94%] [G loss: 1.933020]\n",
      "epoch:29 step:27231 [D loss: 0.658273, acc: 61.72%] [G loss: 1.896654]\n",
      "epoch:29 step:27232 [D loss: 0.674859, acc: 54.69%] [G loss: 1.843585]\n",
      "epoch:29 step:27233 [D loss: 0.679940, acc: 58.59%] [G loss: 1.812026]\n",
      "epoch:29 step:27234 [D loss: 0.662642, acc: 60.16%] [G loss: 1.919982]\n",
      "epoch:29 step:27235 [D loss: 0.668681, acc: 59.38%] [G loss: 1.904675]\n",
      "epoch:29 step:27236 [D loss: 0.604711, acc: 68.75%] [G loss: 1.834263]\n",
      "epoch:29 step:27237 [D loss: 0.640212, acc: 62.50%] [G loss: 1.953277]\n",
      "epoch:29 step:27238 [D loss: 0.699338, acc: 58.59%] [G loss: 1.833159]\n",
      "epoch:29 step:27239 [D loss: 0.642982, acc: 64.06%] [G loss: 1.897941]\n",
      "epoch:29 step:27240 [D loss: 0.613605, acc: 66.41%] [G loss: 1.909681]\n",
      "epoch:29 step:27241 [D loss: 0.666660, acc: 59.38%] [G loss: 1.867605]\n",
      "epoch:29 step:27242 [D loss: 0.622778, acc: 64.84%] [G loss: 1.899680]\n",
      "epoch:29 step:27243 [D loss: 0.679677, acc: 60.94%] [G loss: 1.925815]\n",
      "epoch:29 step:27244 [D loss: 0.625617, acc: 69.53%] [G loss: 1.826570]\n",
      "epoch:29 step:27245 [D loss: 0.674995, acc: 54.69%] [G loss: 1.864254]\n",
      "epoch:29 step:27246 [D loss: 0.647820, acc: 59.38%] [G loss: 1.833369]\n",
      "epoch:29 step:27247 [D loss: 0.617842, acc: 66.41%] [G loss: 1.989031]\n",
      "epoch:29 step:27248 [D loss: 0.619671, acc: 65.62%] [G loss: 2.031106]\n",
      "epoch:29 step:27249 [D loss: 0.633039, acc: 61.72%] [G loss: 2.089486]\n",
      "epoch:29 step:27250 [D loss: 0.611945, acc: 67.97%] [G loss: 2.009437]\n",
      "epoch:29 step:27251 [D loss: 0.714811, acc: 53.91%] [G loss: 1.728834]\n",
      "epoch:29 step:27252 [D loss: 0.654529, acc: 60.16%] [G loss: 1.827625]\n",
      "epoch:29 step:27253 [D loss: 0.683498, acc: 59.38%] [G loss: 1.843444]\n",
      "epoch:29 step:27254 [D loss: 0.668687, acc: 59.38%] [G loss: 1.790709]\n",
      "epoch:29 step:27255 [D loss: 0.668398, acc: 55.47%] [G loss: 1.769725]\n",
      "epoch:29 step:27256 [D loss: 0.683682, acc: 60.94%] [G loss: 1.854348]\n",
      "epoch:29 step:27257 [D loss: 0.630949, acc: 63.28%] [G loss: 1.846888]\n",
      "epoch:29 step:27258 [D loss: 0.659758, acc: 64.06%] [G loss: 1.763760]\n",
      "epoch:29 step:27259 [D loss: 0.641414, acc: 66.41%] [G loss: 1.723660]\n",
      "epoch:29 step:27260 [D loss: 0.624100, acc: 63.28%] [G loss: 1.888931]\n",
      "epoch:29 step:27261 [D loss: 0.624150, acc: 66.41%] [G loss: 1.798853]\n",
      "epoch:29 step:27262 [D loss: 0.644758, acc: 59.38%] [G loss: 1.950344]\n",
      "epoch:29 step:27263 [D loss: 0.645786, acc: 64.84%] [G loss: 1.862077]\n",
      "epoch:29 step:27264 [D loss: 0.616333, acc: 67.97%] [G loss: 1.823042]\n",
      "epoch:29 step:27265 [D loss: 0.668691, acc: 57.81%] [G loss: 1.888991]\n",
      "epoch:29 step:27266 [D loss: 0.620957, acc: 65.62%] [G loss: 1.915173]\n",
      "epoch:29 step:27267 [D loss: 0.602720, acc: 68.75%] [G loss: 1.872022]\n",
      "epoch:29 step:27268 [D loss: 0.687214, acc: 57.81%] [G loss: 1.847960]\n",
      "epoch:29 step:27269 [D loss: 0.637782, acc: 64.06%] [G loss: 1.883733]\n",
      "epoch:29 step:27270 [D loss: 0.637995, acc: 60.94%] [G loss: 1.871933]\n",
      "epoch:29 step:27271 [D loss: 0.688040, acc: 57.03%] [G loss: 1.825074]\n",
      "epoch:29 step:27272 [D loss: 0.721107, acc: 52.34%] [G loss: 1.726325]\n",
      "epoch:29 step:27273 [D loss: 0.591282, acc: 68.75%] [G loss: 1.807124]\n",
      "epoch:29 step:27274 [D loss: 0.625407, acc: 64.84%] [G loss: 1.941894]\n",
      "epoch:29 step:27275 [D loss: 0.658714, acc: 64.06%] [G loss: 1.810967]\n",
      "epoch:29 step:27276 [D loss: 0.686426, acc: 56.25%] [G loss: 1.802394]\n",
      "epoch:29 step:27277 [D loss: 0.633131, acc: 65.62%] [G loss: 1.812269]\n",
      "epoch:29 step:27278 [D loss: 0.647090, acc: 56.25%] [G loss: 1.969222]\n",
      "epoch:29 step:27279 [D loss: 0.584014, acc: 70.31%] [G loss: 2.088483]\n",
      "epoch:29 step:27280 [D loss: 0.594313, acc: 68.75%] [G loss: 2.234233]\n",
      "epoch:29 step:27281 [D loss: 0.702370, acc: 57.03%] [G loss: 1.769932]\n",
      "epoch:29 step:27282 [D loss: 0.679378, acc: 57.81%] [G loss: 1.833636]\n",
      "epoch:29 step:27283 [D loss: 0.690982, acc: 54.69%] [G loss: 1.884827]\n",
      "epoch:29 step:27284 [D loss: 0.624205, acc: 67.97%] [G loss: 1.959001]\n",
      "epoch:29 step:27285 [D loss: 0.607319, acc: 62.50%] [G loss: 1.892465]\n",
      "epoch:29 step:27286 [D loss: 0.671685, acc: 60.16%] [G loss: 1.921112]\n",
      "epoch:29 step:27287 [D loss: 0.639975, acc: 65.62%] [G loss: 1.986716]\n",
      "epoch:29 step:27288 [D loss: 0.599006, acc: 69.53%] [G loss: 2.179781]\n",
      "epoch:29 step:27289 [D loss: 0.591519, acc: 74.22%] [G loss: 2.114701]\n",
      "epoch:29 step:27290 [D loss: 0.602761, acc: 67.19%] [G loss: 2.296393]\n",
      "epoch:29 step:27291 [D loss: 0.635245, acc: 58.59%] [G loss: 2.115231]\n",
      "epoch:29 step:27292 [D loss: 0.608753, acc: 65.62%] [G loss: 2.201560]\n",
      "epoch:29 step:27293 [D loss: 0.735965, acc: 52.34%] [G loss: 1.917882]\n",
      "epoch:29 step:27294 [D loss: 0.673601, acc: 58.59%] [G loss: 2.006360]\n",
      "epoch:29 step:27295 [D loss: 0.699266, acc: 56.25%] [G loss: 2.031646]\n",
      "epoch:29 step:27296 [D loss: 0.672170, acc: 64.06%] [G loss: 1.882005]\n",
      "epoch:29 step:27297 [D loss: 0.702915, acc: 57.03%] [G loss: 1.666155]\n",
      "epoch:29 step:27298 [D loss: 0.684958, acc: 52.34%] [G loss: 1.716810]\n",
      "epoch:29 step:27299 [D loss: 0.605643, acc: 63.28%] [G loss: 1.928063]\n",
      "epoch:29 step:27300 [D loss: 0.613616, acc: 64.84%] [G loss: 1.743894]\n",
      "epoch:29 step:27301 [D loss: 0.643036, acc: 64.84%] [G loss: 1.933716]\n",
      "epoch:29 step:27302 [D loss: 0.652460, acc: 63.28%] [G loss: 1.796524]\n",
      "epoch:29 step:27303 [D loss: 0.604829, acc: 71.88%] [G loss: 1.894124]\n",
      "epoch:29 step:27304 [D loss: 0.676292, acc: 57.03%] [G loss: 1.905627]\n",
      "epoch:29 step:27305 [D loss: 0.661241, acc: 64.84%] [G loss: 1.885598]\n",
      "epoch:29 step:27306 [D loss: 0.717727, acc: 53.12%] [G loss: 1.686577]\n",
      "epoch:29 step:27307 [D loss: 0.615522, acc: 68.75%] [G loss: 1.725837]\n",
      "epoch:29 step:27308 [D loss: 0.672177, acc: 57.03%] [G loss: 1.703090]\n",
      "epoch:29 step:27309 [D loss: 0.628699, acc: 62.50%] [G loss: 1.773127]\n",
      "epoch:29 step:27310 [D loss: 0.684206, acc: 60.16%] [G loss: 1.826799]\n",
      "epoch:29 step:27311 [D loss: 0.674258, acc: 59.38%] [G loss: 1.836232]\n",
      "epoch:29 step:27312 [D loss: 0.650771, acc: 56.25%] [G loss: 1.868302]\n",
      "epoch:29 step:27313 [D loss: 0.702699, acc: 57.81%] [G loss: 1.808432]\n",
      "epoch:29 step:27314 [D loss: 0.675809, acc: 56.25%] [G loss: 1.807007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27315 [D loss: 0.654093, acc: 62.50%] [G loss: 1.868173]\n",
      "epoch:29 step:27316 [D loss: 0.629793, acc: 66.41%] [G loss: 1.872910]\n",
      "epoch:29 step:27317 [D loss: 0.696745, acc: 57.03%] [G loss: 1.737190]\n",
      "epoch:29 step:27318 [D loss: 0.678813, acc: 56.25%] [G loss: 1.816346]\n",
      "epoch:29 step:27319 [D loss: 0.582578, acc: 78.12%] [G loss: 1.920150]\n",
      "epoch:29 step:27320 [D loss: 0.668415, acc: 60.94%] [G loss: 1.856472]\n",
      "epoch:29 step:27321 [D loss: 0.696146, acc: 55.47%] [G loss: 1.816750]\n",
      "epoch:29 step:27322 [D loss: 0.666334, acc: 59.38%] [G loss: 1.812416]\n",
      "epoch:29 step:27323 [D loss: 0.630919, acc: 71.88%] [G loss: 1.938839]\n",
      "epoch:29 step:27324 [D loss: 0.678069, acc: 59.38%] [G loss: 2.041027]\n",
      "epoch:29 step:27325 [D loss: 0.636897, acc: 67.19%] [G loss: 1.909881]\n",
      "epoch:29 step:27326 [D loss: 0.715035, acc: 49.22%] [G loss: 1.828903]\n",
      "epoch:29 step:27327 [D loss: 0.642752, acc: 60.94%] [G loss: 1.817199]\n",
      "epoch:29 step:27328 [D loss: 0.633684, acc: 63.28%] [G loss: 1.925392]\n",
      "epoch:29 step:27329 [D loss: 0.667152, acc: 60.16%] [G loss: 1.917814]\n",
      "epoch:29 step:27330 [D loss: 0.625925, acc: 64.06%] [G loss: 1.825194]\n",
      "epoch:29 step:27331 [D loss: 0.681500, acc: 60.16%] [G loss: 1.831118]\n",
      "epoch:29 step:27332 [D loss: 0.631699, acc: 62.50%] [G loss: 1.811080]\n",
      "epoch:29 step:27333 [D loss: 0.695403, acc: 52.34%] [G loss: 1.662482]\n",
      "epoch:29 step:27334 [D loss: 0.636795, acc: 57.81%] [G loss: 1.775459]\n",
      "epoch:29 step:27335 [D loss: 0.651695, acc: 59.38%] [G loss: 1.844444]\n",
      "epoch:29 step:27336 [D loss: 0.655007, acc: 64.06%] [G loss: 1.844840]\n",
      "epoch:29 step:27337 [D loss: 0.689238, acc: 67.97%] [G loss: 1.736797]\n",
      "epoch:29 step:27338 [D loss: 0.655885, acc: 57.03%] [G loss: 1.874672]\n",
      "epoch:29 step:27339 [D loss: 0.622892, acc: 68.75%] [G loss: 1.877208]\n",
      "epoch:29 step:27340 [D loss: 0.626833, acc: 59.38%] [G loss: 1.902336]\n",
      "epoch:29 step:27341 [D loss: 0.599576, acc: 65.62%] [G loss: 1.881438]\n",
      "epoch:29 step:27342 [D loss: 0.661850, acc: 61.72%] [G loss: 1.877311]\n",
      "epoch:29 step:27343 [D loss: 0.679712, acc: 58.59%] [G loss: 1.762826]\n",
      "epoch:29 step:27344 [D loss: 0.695074, acc: 55.47%] [G loss: 1.895986]\n",
      "epoch:29 step:27345 [D loss: 0.654071, acc: 63.28%] [G loss: 1.784613]\n",
      "epoch:29 step:27346 [D loss: 0.621395, acc: 67.97%] [G loss: 1.836815]\n",
      "epoch:29 step:27347 [D loss: 0.704634, acc: 53.91%] [G loss: 1.786575]\n",
      "epoch:29 step:27348 [D loss: 0.658793, acc: 60.16%] [G loss: 1.666137]\n",
      "epoch:29 step:27349 [D loss: 0.653970, acc: 60.16%] [G loss: 1.802856]\n",
      "epoch:29 step:27350 [D loss: 0.641085, acc: 63.28%] [G loss: 1.748795]\n",
      "epoch:29 step:27351 [D loss: 0.594867, acc: 66.41%] [G loss: 1.832494]\n",
      "epoch:29 step:27352 [D loss: 0.662301, acc: 60.16%] [G loss: 1.727971]\n",
      "epoch:29 step:27353 [D loss: 0.656775, acc: 60.94%] [G loss: 1.833467]\n",
      "epoch:29 step:27354 [D loss: 0.637040, acc: 64.06%] [G loss: 1.778147]\n",
      "epoch:29 step:27355 [D loss: 0.657332, acc: 60.16%] [G loss: 1.828281]\n",
      "epoch:29 step:27356 [D loss: 0.679802, acc: 56.25%] [G loss: 1.914061]\n",
      "epoch:29 step:27357 [D loss: 0.635540, acc: 66.41%] [G loss: 1.771312]\n",
      "epoch:29 step:27358 [D loss: 0.670765, acc: 61.72%] [G loss: 1.737600]\n",
      "epoch:29 step:27359 [D loss: 0.681187, acc: 60.16%] [G loss: 1.715083]\n",
      "epoch:29 step:27360 [D loss: 0.676466, acc: 58.59%] [G loss: 1.812703]\n",
      "epoch:29 step:27361 [D loss: 0.631326, acc: 64.06%] [G loss: 1.953399]\n",
      "epoch:29 step:27362 [D loss: 0.699321, acc: 58.59%] [G loss: 1.866135]\n",
      "epoch:29 step:27363 [D loss: 0.666918, acc: 60.94%] [G loss: 1.907388]\n",
      "epoch:29 step:27364 [D loss: 0.657444, acc: 60.16%] [G loss: 1.856158]\n",
      "epoch:29 step:27365 [D loss: 0.569056, acc: 77.34%] [G loss: 1.938005]\n",
      "epoch:29 step:27366 [D loss: 0.683205, acc: 50.00%] [G loss: 1.865417]\n",
      "epoch:29 step:27367 [D loss: 0.617735, acc: 63.28%] [G loss: 2.008004]\n",
      "epoch:29 step:27368 [D loss: 0.665926, acc: 60.94%] [G loss: 1.862535]\n",
      "epoch:29 step:27369 [D loss: 0.611945, acc: 64.84%] [G loss: 1.777817]\n",
      "epoch:29 step:27370 [D loss: 0.667433, acc: 60.16%] [G loss: 1.790028]\n",
      "epoch:29 step:27371 [D loss: 0.672259, acc: 60.94%] [G loss: 1.980156]\n",
      "epoch:29 step:27372 [D loss: 0.637654, acc: 65.62%] [G loss: 1.907378]\n",
      "epoch:29 step:27373 [D loss: 0.690487, acc: 53.12%] [G loss: 1.651965]\n",
      "epoch:29 step:27374 [D loss: 0.654654, acc: 58.59%] [G loss: 1.686980]\n",
      "epoch:29 step:27375 [D loss: 0.653833, acc: 60.16%] [G loss: 1.920981]\n",
      "epoch:29 step:27376 [D loss: 0.679938, acc: 52.34%] [G loss: 1.791548]\n",
      "epoch:29 step:27377 [D loss: 0.665795, acc: 60.16%] [G loss: 1.864060]\n",
      "epoch:29 step:27378 [D loss: 0.682061, acc: 55.47%] [G loss: 1.866691]\n",
      "epoch:29 step:27379 [D loss: 0.591276, acc: 71.09%] [G loss: 1.913684]\n",
      "epoch:29 step:27380 [D loss: 0.635887, acc: 65.62%] [G loss: 1.986930]\n",
      "epoch:29 step:27381 [D loss: 0.646887, acc: 59.38%] [G loss: 1.986778]\n",
      "epoch:29 step:27382 [D loss: 0.607947, acc: 67.19%] [G loss: 2.079725]\n",
      "epoch:29 step:27383 [D loss: 0.706643, acc: 54.69%] [G loss: 1.744097]\n",
      "epoch:29 step:27384 [D loss: 0.630330, acc: 60.94%] [G loss: 1.844026]\n",
      "epoch:29 step:27385 [D loss: 0.641894, acc: 66.41%] [G loss: 1.974961]\n",
      "epoch:29 step:27386 [D loss: 0.637075, acc: 62.50%] [G loss: 1.743233]\n",
      "epoch:29 step:27387 [D loss: 0.678580, acc: 55.47%] [G loss: 1.952311]\n",
      "epoch:29 step:27388 [D loss: 0.652427, acc: 63.28%] [G loss: 1.943119]\n",
      "epoch:29 step:27389 [D loss: 0.597086, acc: 68.75%] [G loss: 1.917940]\n",
      "epoch:29 step:27390 [D loss: 0.637712, acc: 65.62%] [G loss: 1.953087]\n",
      "epoch:29 step:27391 [D loss: 0.602748, acc: 71.09%] [G loss: 1.939005]\n",
      "epoch:29 step:27392 [D loss: 0.624807, acc: 62.50%] [G loss: 2.317271]\n",
      "epoch:29 step:27393 [D loss: 0.684002, acc: 57.81%] [G loss: 1.864416]\n",
      "epoch:29 step:27394 [D loss: 0.636749, acc: 68.75%] [G loss: 1.861209]\n",
      "epoch:29 step:27395 [D loss: 0.643252, acc: 60.16%] [G loss: 2.022968]\n",
      "epoch:29 step:27396 [D loss: 0.666132, acc: 61.72%] [G loss: 1.851482]\n",
      "epoch:29 step:27397 [D loss: 0.682014, acc: 58.59%] [G loss: 1.751105]\n",
      "epoch:29 step:27398 [D loss: 0.672206, acc: 53.91%] [G loss: 1.855509]\n",
      "epoch:29 step:27399 [D loss: 0.652151, acc: 64.06%] [G loss: 1.824943]\n",
      "epoch:29 step:27400 [D loss: 0.623650, acc: 68.75%] [G loss: 1.838517]\n",
      "##############\n",
      "[2.60300707 1.6721411  5.91733499 4.7444652  3.57226406 5.60079347\n",
      " 4.57661431 4.87384477 4.35039649 3.82115122]\n",
      "##########\n",
      "epoch:29 step:27401 [D loss: 0.673100, acc: 60.16%] [G loss: 1.694804]\n",
      "epoch:29 step:27402 [D loss: 0.636901, acc: 57.03%] [G loss: 1.991190]\n",
      "epoch:29 step:27403 [D loss: 0.521513, acc: 78.91%] [G loss: 2.174350]\n",
      "epoch:29 step:27404 [D loss: 0.626252, acc: 65.62%] [G loss: 2.248272]\n",
      "epoch:29 step:27405 [D loss: 0.601637, acc: 67.19%] [G loss: 2.267265]\n",
      "epoch:29 step:27406 [D loss: 0.655235, acc: 59.38%] [G loss: 1.954373]\n",
      "epoch:29 step:27407 [D loss: 0.636267, acc: 64.06%] [G loss: 1.816365]\n",
      "epoch:29 step:27408 [D loss: 0.694852, acc: 54.69%] [G loss: 1.822422]\n",
      "epoch:29 step:27409 [D loss: 0.671722, acc: 57.81%] [G loss: 1.791473]\n",
      "epoch:29 step:27410 [D loss: 0.649211, acc: 61.72%] [G loss: 1.975564]\n",
      "epoch:29 step:27411 [D loss: 0.616485, acc: 63.28%] [G loss: 1.923644]\n",
      "epoch:29 step:27412 [D loss: 0.580271, acc: 70.31%] [G loss: 1.932397]\n",
      "epoch:29 step:27413 [D loss: 0.601798, acc: 69.53%] [G loss: 1.884274]\n",
      "epoch:29 step:27414 [D loss: 0.643806, acc: 62.50%] [G loss: 1.994524]\n",
      "epoch:29 step:27415 [D loss: 0.620659, acc: 63.28%] [G loss: 1.981675]\n",
      "epoch:29 step:27416 [D loss: 0.646349, acc: 65.62%] [G loss: 1.849366]\n",
      "epoch:29 step:27417 [D loss: 0.635423, acc: 64.84%] [G loss: 1.909056]\n",
      "epoch:29 step:27418 [D loss: 0.648730, acc: 70.31%] [G loss: 1.961792]\n",
      "epoch:29 step:27419 [D loss: 0.671118, acc: 60.16%] [G loss: 1.830462]\n",
      "epoch:29 step:27420 [D loss: 0.705149, acc: 60.94%] [G loss: 1.788829]\n",
      "epoch:29 step:27421 [D loss: 0.601342, acc: 68.75%] [G loss: 2.100842]\n",
      "epoch:29 step:27422 [D loss: 0.676932, acc: 56.25%] [G loss: 1.697601]\n",
      "epoch:29 step:27423 [D loss: 0.685814, acc: 52.34%] [G loss: 1.862652]\n",
      "epoch:29 step:27424 [D loss: 0.661495, acc: 59.38%] [G loss: 1.759838]\n",
      "epoch:29 step:27425 [D loss: 0.675061, acc: 66.41%] [G loss: 1.801214]\n",
      "epoch:29 step:27426 [D loss: 0.678109, acc: 52.34%] [G loss: 1.813827]\n",
      "epoch:29 step:27427 [D loss: 0.628157, acc: 60.16%] [G loss: 1.650531]\n",
      "epoch:29 step:27428 [D loss: 0.686468, acc: 57.03%] [G loss: 1.765658]\n",
      "epoch:29 step:27429 [D loss: 0.662918, acc: 60.94%] [G loss: 1.812598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27430 [D loss: 0.660380, acc: 62.50%] [G loss: 1.727875]\n",
      "epoch:29 step:27431 [D loss: 0.606376, acc: 68.75%] [G loss: 1.914159]\n",
      "epoch:29 step:27432 [D loss: 0.677035, acc: 57.03%] [G loss: 1.820240]\n",
      "epoch:29 step:27433 [D loss: 0.674401, acc: 60.16%] [G loss: 1.832038]\n",
      "epoch:29 step:27434 [D loss: 0.702982, acc: 53.12%] [G loss: 1.919030]\n",
      "epoch:29 step:27435 [D loss: 0.639009, acc: 60.16%] [G loss: 1.904197]\n",
      "epoch:29 step:27436 [D loss: 0.602632, acc: 69.53%] [G loss: 1.996215]\n",
      "epoch:29 step:27437 [D loss: 0.649940, acc: 64.06%] [G loss: 1.974379]\n",
      "epoch:29 step:27438 [D loss: 0.671982, acc: 59.38%] [G loss: 1.791795]\n",
      "epoch:29 step:27439 [D loss: 0.622867, acc: 61.72%] [G loss: 1.818885]\n",
      "epoch:29 step:27440 [D loss: 0.674990, acc: 57.81%] [G loss: 1.816733]\n",
      "epoch:29 step:27441 [D loss: 0.678382, acc: 56.25%] [G loss: 1.760755]\n",
      "epoch:29 step:27442 [D loss: 0.640709, acc: 64.84%] [G loss: 1.932274]\n",
      "epoch:29 step:27443 [D loss: 0.670215, acc: 61.72%] [G loss: 1.847092]\n",
      "epoch:29 step:27444 [D loss: 0.623395, acc: 65.62%] [G loss: 1.820239]\n",
      "epoch:29 step:27445 [D loss: 0.642223, acc: 64.84%] [G loss: 2.078465]\n",
      "epoch:29 step:27446 [D loss: 0.678723, acc: 55.47%] [G loss: 1.851820]\n",
      "epoch:29 step:27447 [D loss: 0.600288, acc: 64.84%] [G loss: 1.928730]\n",
      "epoch:29 step:27448 [D loss: 0.608521, acc: 63.28%] [G loss: 1.994836]\n",
      "epoch:29 step:27449 [D loss: 0.642912, acc: 59.38%] [G loss: 2.033559]\n",
      "epoch:29 step:27450 [D loss: 0.660861, acc: 57.03%] [G loss: 1.847834]\n",
      "epoch:29 step:27451 [D loss: 0.712020, acc: 55.47%] [G loss: 1.675435]\n",
      "epoch:29 step:27452 [D loss: 0.663749, acc: 59.38%] [G loss: 1.770320]\n",
      "epoch:29 step:27453 [D loss: 0.648425, acc: 60.94%] [G loss: 1.884436]\n",
      "epoch:29 step:27454 [D loss: 0.678607, acc: 56.25%] [G loss: 1.711144]\n",
      "epoch:29 step:27455 [D loss: 0.654013, acc: 64.06%] [G loss: 1.648319]\n",
      "epoch:29 step:27456 [D loss: 0.618092, acc: 64.84%] [G loss: 1.854955]\n",
      "epoch:29 step:27457 [D loss: 0.656505, acc: 58.59%] [G loss: 1.794582]\n",
      "epoch:29 step:27458 [D loss: 0.675971, acc: 56.25%] [G loss: 1.779022]\n",
      "epoch:29 step:27459 [D loss: 0.647814, acc: 62.50%] [G loss: 1.870801]\n",
      "epoch:29 step:27460 [D loss: 0.621297, acc: 67.19%] [G loss: 1.939627]\n",
      "epoch:29 step:27461 [D loss: 0.606652, acc: 68.75%] [G loss: 1.784502]\n",
      "epoch:29 step:27462 [D loss: 0.655671, acc: 64.06%] [G loss: 1.912900]\n",
      "epoch:29 step:27463 [D loss: 0.695703, acc: 51.56%] [G loss: 1.829966]\n",
      "epoch:29 step:27464 [D loss: 0.700140, acc: 56.25%] [G loss: 1.773991]\n",
      "epoch:29 step:27465 [D loss: 0.671241, acc: 56.25%] [G loss: 1.849512]\n",
      "epoch:29 step:27466 [D loss: 0.611044, acc: 63.28%] [G loss: 1.895592]\n",
      "epoch:29 step:27467 [D loss: 0.691478, acc: 58.59%] [G loss: 1.734476]\n",
      "epoch:29 step:27468 [D loss: 0.649533, acc: 60.94%] [G loss: 1.845163]\n",
      "epoch:29 step:27469 [D loss: 0.634486, acc: 66.41%] [G loss: 1.843642]\n",
      "epoch:29 step:27470 [D loss: 0.649657, acc: 64.06%] [G loss: 1.830674]\n",
      "epoch:29 step:27471 [D loss: 0.612381, acc: 69.53%] [G loss: 2.034650]\n",
      "epoch:29 step:27472 [D loss: 0.635521, acc: 60.16%] [G loss: 2.142800]\n",
      "epoch:29 step:27473 [D loss: 0.638359, acc: 64.06%] [G loss: 1.712502]\n",
      "epoch:29 step:27474 [D loss: 0.635901, acc: 64.06%] [G loss: 1.833165]\n",
      "epoch:29 step:27475 [D loss: 0.644182, acc: 61.72%] [G loss: 1.840347]\n",
      "epoch:29 step:27476 [D loss: 0.654730, acc: 60.94%] [G loss: 1.784288]\n",
      "epoch:29 step:27477 [D loss: 0.610381, acc: 60.94%] [G loss: 1.828562]\n",
      "epoch:29 step:27478 [D loss: 0.671271, acc: 55.47%] [G loss: 1.796444]\n",
      "epoch:29 step:27479 [D loss: 0.641193, acc: 67.19%] [G loss: 1.883719]\n",
      "epoch:29 step:27480 [D loss: 0.640854, acc: 64.84%] [G loss: 1.833830]\n",
      "epoch:29 step:27481 [D loss: 0.688081, acc: 60.16%] [G loss: 1.888370]\n",
      "epoch:29 step:27482 [D loss: 0.629655, acc: 64.84%] [G loss: 1.814160]\n",
      "epoch:29 step:27483 [D loss: 0.639426, acc: 68.75%] [G loss: 1.797408]\n",
      "epoch:29 step:27484 [D loss: 0.637083, acc: 64.84%] [G loss: 1.851742]\n",
      "epoch:29 step:27485 [D loss: 0.645455, acc: 64.06%] [G loss: 2.137667]\n",
      "epoch:29 step:27486 [D loss: 0.631180, acc: 63.28%] [G loss: 1.959619]\n",
      "epoch:29 step:27487 [D loss: 0.586976, acc: 74.22%] [G loss: 1.984267]\n",
      "epoch:29 step:27488 [D loss: 0.602124, acc: 68.75%] [G loss: 2.018899]\n",
      "epoch:29 step:27489 [D loss: 0.711648, acc: 55.47%] [G loss: 1.819462]\n",
      "epoch:29 step:27490 [D loss: 0.626680, acc: 66.41%] [G loss: 1.749260]\n",
      "epoch:29 step:27491 [D loss: 0.596776, acc: 66.41%] [G loss: 1.938750]\n",
      "epoch:29 step:27492 [D loss: 0.694710, acc: 56.25%] [G loss: 1.833252]\n",
      "epoch:29 step:27493 [D loss: 0.656060, acc: 57.03%] [G loss: 1.909533]\n",
      "epoch:29 step:27494 [D loss: 0.622046, acc: 64.84%] [G loss: 1.879879]\n",
      "epoch:29 step:27495 [D loss: 0.693245, acc: 65.62%] [G loss: 1.817726]\n",
      "epoch:29 step:27496 [D loss: 0.660227, acc: 60.94%] [G loss: 1.839526]\n",
      "epoch:29 step:27497 [D loss: 0.620863, acc: 64.06%] [G loss: 1.847766]\n",
      "epoch:29 step:27498 [D loss: 0.699765, acc: 55.47%] [G loss: 1.781962]\n",
      "epoch:29 step:27499 [D loss: 0.642729, acc: 63.28%] [G loss: 1.901438]\n",
      "epoch:29 step:27500 [D loss: 0.681295, acc: 60.94%] [G loss: 1.725901]\n",
      "epoch:29 step:27501 [D loss: 0.671740, acc: 57.81%] [G loss: 1.855763]\n",
      "epoch:29 step:27502 [D loss: 0.665977, acc: 57.03%] [G loss: 1.961537]\n",
      "epoch:29 step:27503 [D loss: 0.640208, acc: 63.28%] [G loss: 1.946624]\n",
      "epoch:29 step:27504 [D loss: 0.649653, acc: 63.28%] [G loss: 1.775398]\n",
      "epoch:29 step:27505 [D loss: 0.579648, acc: 76.56%] [G loss: 1.948166]\n",
      "epoch:29 step:27506 [D loss: 0.638689, acc: 64.84%] [G loss: 1.883516]\n",
      "epoch:29 step:27507 [D loss: 0.644640, acc: 60.16%] [G loss: 2.053076]\n",
      "epoch:29 step:27508 [D loss: 0.627284, acc: 67.19%] [G loss: 1.906556]\n",
      "epoch:29 step:27509 [D loss: 0.618805, acc: 69.53%] [G loss: 2.159934]\n",
      "epoch:29 step:27510 [D loss: 0.649117, acc: 60.94%] [G loss: 1.994800]\n",
      "epoch:29 step:27511 [D loss: 0.643284, acc: 64.84%] [G loss: 2.047055]\n",
      "epoch:29 step:27512 [D loss: 0.638236, acc: 63.28%] [G loss: 1.974909]\n",
      "epoch:29 step:27513 [D loss: 0.604889, acc: 66.41%] [G loss: 1.923123]\n",
      "epoch:29 step:27514 [D loss: 0.731590, acc: 52.34%] [G loss: 1.925912]\n",
      "epoch:29 step:27515 [D loss: 0.663368, acc: 56.25%] [G loss: 1.890465]\n",
      "epoch:29 step:27516 [D loss: 0.668087, acc: 56.25%] [G loss: 1.768455]\n",
      "epoch:29 step:27517 [D loss: 0.655634, acc: 60.16%] [G loss: 1.856673]\n",
      "epoch:29 step:27518 [D loss: 0.651810, acc: 60.16%] [G loss: 1.974540]\n",
      "epoch:29 step:27519 [D loss: 0.635727, acc: 60.94%] [G loss: 2.104583]\n",
      "epoch:29 step:27520 [D loss: 0.569017, acc: 65.62%] [G loss: 2.027148]\n",
      "epoch:29 step:27521 [D loss: 0.704587, acc: 54.69%] [G loss: 1.820115]\n",
      "epoch:29 step:27522 [D loss: 0.700349, acc: 53.91%] [G loss: 1.672570]\n",
      "epoch:29 step:27523 [D loss: 0.628013, acc: 66.41%] [G loss: 1.921514]\n",
      "epoch:29 step:27524 [D loss: 0.692414, acc: 52.34%] [G loss: 1.878607]\n",
      "epoch:29 step:27525 [D loss: 0.705435, acc: 61.72%] [G loss: 1.738412]\n",
      "epoch:29 step:27526 [D loss: 0.671073, acc: 60.94%] [G loss: 1.803686]\n",
      "epoch:29 step:27527 [D loss: 0.588393, acc: 64.06%] [G loss: 1.977760]\n",
      "epoch:29 step:27528 [D loss: 0.710845, acc: 52.34%] [G loss: 1.809385]\n",
      "epoch:29 step:27529 [D loss: 0.639967, acc: 66.41%] [G loss: 1.813708]\n",
      "epoch:29 step:27530 [D loss: 0.645558, acc: 62.50%] [G loss: 1.836591]\n",
      "epoch:29 step:27531 [D loss: 0.595178, acc: 70.31%] [G loss: 2.031047]\n",
      "epoch:29 step:27532 [D loss: 0.639351, acc: 64.84%] [G loss: 1.902992]\n",
      "epoch:29 step:27533 [D loss: 0.632926, acc: 64.84%] [G loss: 2.116385]\n",
      "epoch:29 step:27534 [D loss: 0.673635, acc: 60.94%] [G loss: 1.843723]\n",
      "epoch:29 step:27535 [D loss: 0.687349, acc: 57.81%] [G loss: 1.691994]\n",
      "epoch:29 step:27536 [D loss: 0.627045, acc: 67.19%] [G loss: 1.806017]\n",
      "epoch:29 step:27537 [D loss: 0.648551, acc: 60.16%] [G loss: 1.888517]\n",
      "epoch:29 step:27538 [D loss: 0.611162, acc: 66.41%] [G loss: 1.843588]\n",
      "epoch:29 step:27539 [D loss: 0.615370, acc: 69.53%] [G loss: 1.873627]\n",
      "epoch:29 step:27540 [D loss: 0.633822, acc: 59.38%] [G loss: 1.864908]\n",
      "epoch:29 step:27541 [D loss: 0.627046, acc: 67.97%] [G loss: 1.953455]\n",
      "epoch:29 step:27542 [D loss: 0.663100, acc: 58.59%] [G loss: 1.857108]\n",
      "epoch:29 step:27543 [D loss: 0.652055, acc: 58.59%] [G loss: 2.119410]\n",
      "epoch:29 step:27544 [D loss: 0.633246, acc: 64.84%] [G loss: 2.157468]\n",
      "epoch:29 step:27545 [D loss: 0.602414, acc: 68.75%] [G loss: 1.981922]\n",
      "epoch:29 step:27546 [D loss: 0.692120, acc: 53.91%] [G loss: 1.763666]\n",
      "epoch:29 step:27547 [D loss: 0.653487, acc: 58.59%] [G loss: 1.862659]\n",
      "epoch:29 step:27548 [D loss: 0.687778, acc: 55.47%] [G loss: 1.745530]\n",
      "epoch:29 step:27549 [D loss: 0.688595, acc: 55.47%] [G loss: 1.759689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27550 [D loss: 0.679866, acc: 58.59%] [G loss: 1.735748]\n",
      "epoch:29 step:27551 [D loss: 0.640677, acc: 63.28%] [G loss: 1.864820]\n",
      "epoch:29 step:27552 [D loss: 0.619984, acc: 63.28%] [G loss: 2.045815]\n",
      "epoch:29 step:27553 [D loss: 0.617433, acc: 66.41%] [G loss: 1.986179]\n",
      "epoch:29 step:27554 [D loss: 0.608069, acc: 70.31%] [G loss: 1.954936]\n",
      "epoch:29 step:27555 [D loss: 0.685328, acc: 59.38%] [G loss: 1.913472]\n",
      "epoch:29 step:27556 [D loss: 0.618194, acc: 67.19%] [G loss: 1.822959]\n",
      "epoch:29 step:27557 [D loss: 0.628523, acc: 67.19%] [G loss: 1.891041]\n",
      "epoch:29 step:27558 [D loss: 0.590077, acc: 68.75%] [G loss: 2.009794]\n",
      "epoch:29 step:27559 [D loss: 0.676094, acc: 59.38%] [G loss: 1.759941]\n",
      "epoch:29 step:27560 [D loss: 0.660855, acc: 60.94%] [G loss: 1.734789]\n",
      "epoch:29 step:27561 [D loss: 0.634823, acc: 71.09%] [G loss: 1.872915]\n",
      "epoch:29 step:27562 [D loss: 0.678841, acc: 61.72%] [G loss: 1.959502]\n",
      "epoch:29 step:27563 [D loss: 0.622918, acc: 66.41%] [G loss: 1.957385]\n",
      "epoch:29 step:27564 [D loss: 0.641774, acc: 67.19%] [G loss: 1.822414]\n",
      "epoch:29 step:27565 [D loss: 0.645154, acc: 62.50%] [G loss: 1.837948]\n",
      "epoch:29 step:27566 [D loss: 0.680729, acc: 62.50%] [G loss: 1.881286]\n",
      "epoch:29 step:27567 [D loss: 0.676410, acc: 57.81%] [G loss: 1.751403]\n",
      "epoch:29 step:27568 [D loss: 0.683329, acc: 57.03%] [G loss: 2.005248]\n",
      "epoch:29 step:27569 [D loss: 0.681478, acc: 54.69%] [G loss: 1.753425]\n",
      "epoch:29 step:27570 [D loss: 0.670648, acc: 60.16%] [G loss: 1.811989]\n",
      "epoch:29 step:27571 [D loss: 0.647322, acc: 57.03%] [G loss: 1.918622]\n",
      "epoch:29 step:27572 [D loss: 0.632052, acc: 59.38%] [G loss: 1.993041]\n",
      "epoch:29 step:27573 [D loss: 0.633793, acc: 67.19%] [G loss: 1.866892]\n",
      "epoch:29 step:27574 [D loss: 0.674923, acc: 60.94%] [G loss: 1.789235]\n",
      "epoch:29 step:27575 [D loss: 0.601589, acc: 67.97%] [G loss: 1.974277]\n",
      "epoch:29 step:27576 [D loss: 0.704454, acc: 55.47%] [G loss: 1.914362]\n",
      "epoch:29 step:27577 [D loss: 0.678482, acc: 57.81%] [G loss: 2.006306]\n",
      "epoch:29 step:27578 [D loss: 0.640941, acc: 57.81%] [G loss: 1.948382]\n",
      "epoch:29 step:27579 [D loss: 0.604149, acc: 67.19%] [G loss: 1.912315]\n",
      "epoch:29 step:27580 [D loss: 0.660226, acc: 60.16%] [G loss: 1.820833]\n",
      "epoch:29 step:27581 [D loss: 0.744663, acc: 55.47%] [G loss: 1.759697]\n",
      "epoch:29 step:27582 [D loss: 0.629603, acc: 64.06%] [G loss: 1.832994]\n",
      "epoch:29 step:27583 [D loss: 0.665663, acc: 58.59%] [G loss: 1.812352]\n",
      "epoch:29 step:27584 [D loss: 0.699901, acc: 53.12%] [G loss: 1.828782]\n",
      "epoch:29 step:27585 [D loss: 0.684959, acc: 62.50%] [G loss: 1.795514]\n",
      "epoch:29 step:27586 [D loss: 0.636018, acc: 60.16%] [G loss: 1.798089]\n",
      "epoch:29 step:27587 [D loss: 0.615230, acc: 65.62%] [G loss: 1.787221]\n",
      "epoch:29 step:27588 [D loss: 0.679432, acc: 60.94%] [G loss: 1.949571]\n",
      "epoch:29 step:27589 [D loss: 0.618196, acc: 66.41%] [G loss: 1.981201]\n",
      "epoch:29 step:27590 [D loss: 0.637638, acc: 65.62%] [G loss: 2.065698]\n",
      "epoch:29 step:27591 [D loss: 0.685432, acc: 57.03%] [G loss: 1.788492]\n",
      "epoch:29 step:27592 [D loss: 0.667676, acc: 58.59%] [G loss: 1.822501]\n",
      "epoch:29 step:27593 [D loss: 0.683171, acc: 62.50%] [G loss: 1.859632]\n",
      "epoch:29 step:27594 [D loss: 0.711614, acc: 54.69%] [G loss: 1.790045]\n",
      "epoch:29 step:27595 [D loss: 0.670949, acc: 58.59%] [G loss: 1.921870]\n",
      "epoch:29 step:27596 [D loss: 0.649060, acc: 60.94%] [G loss: 1.806056]\n",
      "epoch:29 step:27597 [D loss: 0.696091, acc: 53.91%] [G loss: 1.843035]\n",
      "epoch:29 step:27598 [D loss: 0.692236, acc: 56.25%] [G loss: 1.819627]\n",
      "epoch:29 step:27599 [D loss: 0.643668, acc: 63.28%] [G loss: 1.710979]\n",
      "epoch:29 step:27600 [D loss: 0.657733, acc: 67.19%] [G loss: 1.873347]\n",
      "##############\n",
      "[2.59875992 1.34480648 6.14603962 4.71406466 3.45308369 5.49756731\n",
      " 4.41635502 4.6570821  4.45685161 3.6010945 ]\n",
      "##########\n",
      "epoch:29 step:27601 [D loss: 0.601160, acc: 70.31%] [G loss: 2.020557]\n",
      "epoch:29 step:27602 [D loss: 0.618825, acc: 65.62%] [G loss: 2.080954]\n",
      "epoch:29 step:27603 [D loss: 0.631929, acc: 66.41%] [G loss: 1.996446]\n",
      "epoch:29 step:27604 [D loss: 0.609370, acc: 65.62%] [G loss: 1.972116]\n",
      "epoch:29 step:27605 [D loss: 0.661059, acc: 63.28%] [G loss: 1.695397]\n",
      "epoch:29 step:27606 [D loss: 0.645237, acc: 60.16%] [G loss: 1.822406]\n",
      "epoch:29 step:27607 [D loss: 0.630364, acc: 65.62%] [G loss: 1.882773]\n",
      "epoch:29 step:27608 [D loss: 0.644815, acc: 62.50%] [G loss: 1.891791]\n",
      "epoch:29 step:27609 [D loss: 0.617912, acc: 64.84%] [G loss: 1.984885]\n",
      "epoch:29 step:27610 [D loss: 0.700403, acc: 59.38%] [G loss: 1.845177]\n",
      "epoch:29 step:27611 [D loss: 0.672707, acc: 57.81%] [G loss: 1.821166]\n",
      "epoch:29 step:27612 [D loss: 0.678811, acc: 57.81%] [G loss: 1.648491]\n",
      "epoch:29 step:27613 [D loss: 0.690172, acc: 53.12%] [G loss: 1.837287]\n",
      "epoch:29 step:27614 [D loss: 0.655237, acc: 62.50%] [G loss: 1.765504]\n",
      "epoch:29 step:27615 [D loss: 0.660323, acc: 55.47%] [G loss: 1.828863]\n",
      "epoch:29 step:27616 [D loss: 0.615029, acc: 65.62%] [G loss: 1.922184]\n",
      "epoch:29 step:27617 [D loss: 0.631975, acc: 64.06%] [G loss: 1.852927]\n",
      "epoch:29 step:27618 [D loss: 0.652487, acc: 60.16%] [G loss: 1.793168]\n",
      "epoch:29 step:27619 [D loss: 0.643757, acc: 64.84%] [G loss: 1.878712]\n",
      "epoch:29 step:27620 [D loss: 0.663628, acc: 60.16%] [G loss: 1.792471]\n",
      "epoch:29 step:27621 [D loss: 0.666918, acc: 61.72%] [G loss: 1.964589]\n",
      "epoch:29 step:27622 [D loss: 0.627140, acc: 64.06%] [G loss: 1.831904]\n",
      "epoch:29 step:27623 [D loss: 0.682570, acc: 56.25%] [G loss: 1.874470]\n",
      "epoch:29 step:27624 [D loss: 0.679069, acc: 63.28%] [G loss: 1.851032]\n",
      "epoch:29 step:27625 [D loss: 0.663763, acc: 61.72%] [G loss: 1.760873]\n",
      "epoch:29 step:27626 [D loss: 0.630769, acc: 66.41%] [G loss: 1.965457]\n",
      "epoch:29 step:27627 [D loss: 0.665432, acc: 59.38%] [G loss: 1.774062]\n",
      "epoch:29 step:27628 [D loss: 0.614531, acc: 64.06%] [G loss: 1.932346]\n",
      "epoch:29 step:27629 [D loss: 0.614665, acc: 64.84%] [G loss: 1.924042]\n",
      "epoch:29 step:27630 [D loss: 0.607470, acc: 66.41%] [G loss: 1.970458]\n",
      "epoch:29 step:27631 [D loss: 0.709270, acc: 55.47%] [G loss: 1.722555]\n",
      "epoch:29 step:27632 [D loss: 0.653576, acc: 57.81%] [G loss: 1.846419]\n",
      "epoch:29 step:27633 [D loss: 0.719995, acc: 53.12%] [G loss: 1.807528]\n",
      "epoch:29 step:27634 [D loss: 0.688191, acc: 55.47%] [G loss: 1.800675]\n",
      "epoch:29 step:27635 [D loss: 0.666071, acc: 58.59%] [G loss: 1.795296]\n",
      "epoch:29 step:27636 [D loss: 0.647020, acc: 61.72%] [G loss: 1.850810]\n",
      "epoch:29 step:27637 [D loss: 0.661764, acc: 58.59%] [G loss: 1.819917]\n",
      "epoch:29 step:27638 [D loss: 0.677098, acc: 55.47%] [G loss: 1.905143]\n",
      "epoch:29 step:27639 [D loss: 0.632437, acc: 64.84%] [G loss: 1.862229]\n",
      "epoch:29 step:27640 [D loss: 0.650653, acc: 61.72%] [G loss: 1.874693]\n",
      "epoch:29 step:27641 [D loss: 0.665915, acc: 65.62%] [G loss: 2.000697]\n",
      "epoch:29 step:27642 [D loss: 0.612505, acc: 68.75%] [G loss: 2.084743]\n",
      "epoch:29 step:27643 [D loss: 0.614847, acc: 67.19%] [G loss: 1.954486]\n",
      "epoch:29 step:27644 [D loss: 0.641047, acc: 60.94%] [G loss: 2.213393]\n",
      "epoch:29 step:27645 [D loss: 0.657498, acc: 63.28%] [G loss: 2.070003]\n",
      "epoch:29 step:27646 [D loss: 0.662333, acc: 56.25%] [G loss: 1.710094]\n",
      "epoch:29 step:27647 [D loss: 0.731992, acc: 50.00%] [G loss: 1.835316]\n",
      "epoch:29 step:27648 [D loss: 0.684006, acc: 54.69%] [G loss: 1.833105]\n",
      "epoch:29 step:27649 [D loss: 0.633314, acc: 63.28%] [G loss: 1.915717]\n",
      "epoch:29 step:27650 [D loss: 0.625176, acc: 66.41%] [G loss: 1.712101]\n",
      "epoch:29 step:27651 [D loss: 0.679438, acc: 59.38%] [G loss: 1.786924]\n",
      "epoch:29 step:27652 [D loss: 0.564176, acc: 73.44%] [G loss: 2.061828]\n",
      "epoch:29 step:27653 [D loss: 0.603947, acc: 66.41%] [G loss: 1.984993]\n",
      "epoch:29 step:27654 [D loss: 0.589008, acc: 72.66%] [G loss: 2.111507]\n",
      "epoch:29 step:27655 [D loss: 0.684037, acc: 54.69%] [G loss: 1.813884]\n",
      "epoch:29 step:27656 [D loss: 0.605101, acc: 67.97%] [G loss: 1.775261]\n",
      "epoch:29 step:27657 [D loss: 0.635528, acc: 60.94%] [G loss: 2.001317]\n",
      "epoch:29 step:27658 [D loss: 0.648582, acc: 62.50%] [G loss: 1.870628]\n",
      "epoch:29 step:27659 [D loss: 0.676854, acc: 60.94%] [G loss: 1.879138]\n",
      "epoch:29 step:27660 [D loss: 0.642452, acc: 61.72%] [G loss: 1.968102]\n",
      "epoch:29 step:27661 [D loss: 0.590268, acc: 71.09%] [G loss: 2.107501]\n",
      "epoch:29 step:27662 [D loss: 0.618577, acc: 64.84%] [G loss: 1.813390]\n",
      "epoch:29 step:27663 [D loss: 0.608715, acc: 66.41%] [G loss: 1.805409]\n",
      "epoch:29 step:27664 [D loss: 0.637229, acc: 57.03%] [G loss: 1.842077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27665 [D loss: 0.653095, acc: 61.72%] [G loss: 1.890800]\n",
      "epoch:29 step:27666 [D loss: 0.697461, acc: 56.25%] [G loss: 1.879560]\n",
      "epoch:29 step:27667 [D loss: 0.614205, acc: 65.62%] [G loss: 1.937566]\n",
      "epoch:29 step:27668 [D loss: 0.604927, acc: 70.31%] [G loss: 1.970668]\n",
      "epoch:29 step:27669 [D loss: 0.666480, acc: 56.25%] [G loss: 1.975945]\n",
      "epoch:29 step:27670 [D loss: 0.581376, acc: 69.53%] [G loss: 2.081940]\n",
      "epoch:29 step:27671 [D loss: 0.651828, acc: 64.06%] [G loss: 2.056845]\n",
      "epoch:29 step:27672 [D loss: 0.621602, acc: 62.50%] [G loss: 2.162343]\n",
      "epoch:29 step:27673 [D loss: 0.665321, acc: 58.59%] [G loss: 1.842368]\n",
      "epoch:29 step:27674 [D loss: 0.675166, acc: 59.38%] [G loss: 1.770456]\n",
      "epoch:29 step:27675 [D loss: 0.729036, acc: 51.56%] [G loss: 1.841109]\n",
      "epoch:29 step:27676 [D loss: 0.665335, acc: 60.16%] [G loss: 1.886010]\n",
      "epoch:29 step:27677 [D loss: 0.650941, acc: 64.84%] [G loss: 1.866985]\n",
      "epoch:29 step:27678 [D loss: 0.661013, acc: 57.03%] [G loss: 1.896573]\n",
      "epoch:29 step:27679 [D loss: 0.655514, acc: 62.50%] [G loss: 1.731087]\n",
      "epoch:29 step:27680 [D loss: 0.640647, acc: 62.50%] [G loss: 1.788171]\n",
      "epoch:29 step:27681 [D loss: 0.612398, acc: 64.06%] [G loss: 1.973886]\n",
      "epoch:29 step:27682 [D loss: 0.597018, acc: 68.75%] [G loss: 1.936886]\n",
      "epoch:29 step:27683 [D loss: 0.644852, acc: 63.28%] [G loss: 1.854964]\n",
      "epoch:29 step:27684 [D loss: 0.665986, acc: 60.94%] [G loss: 1.771304]\n",
      "epoch:29 step:27685 [D loss: 0.692400, acc: 55.47%] [G loss: 1.815237]\n",
      "epoch:29 step:27686 [D loss: 0.653744, acc: 60.16%] [G loss: 1.920455]\n",
      "epoch:29 step:27687 [D loss: 0.660134, acc: 58.59%] [G loss: 1.807745]\n",
      "epoch:29 step:27688 [D loss: 0.641700, acc: 64.06%] [G loss: 1.861331]\n",
      "epoch:29 step:27689 [D loss: 0.624226, acc: 63.28%] [G loss: 1.952370]\n",
      "epoch:29 step:27690 [D loss: 0.638901, acc: 64.06%] [G loss: 1.874051]\n",
      "epoch:29 step:27691 [D loss: 0.659740, acc: 58.59%] [G loss: 1.809872]\n",
      "epoch:29 step:27692 [D loss: 0.584690, acc: 70.31%] [G loss: 1.963626]\n",
      "epoch:29 step:27693 [D loss: 0.604727, acc: 66.41%] [G loss: 1.990336]\n",
      "epoch:29 step:27694 [D loss: 0.631063, acc: 61.72%] [G loss: 1.907793]\n",
      "epoch:29 step:27695 [D loss: 0.665423, acc: 60.94%] [G loss: 1.834097]\n",
      "epoch:29 step:27696 [D loss: 0.652410, acc: 66.41%] [G loss: 1.981542]\n",
      "epoch:29 step:27697 [D loss: 0.664070, acc: 63.28%] [G loss: 1.844033]\n",
      "epoch:29 step:27698 [D loss: 0.681660, acc: 58.59%] [G loss: 1.780181]\n",
      "epoch:29 step:27699 [D loss: 0.650088, acc: 64.84%] [G loss: 1.858950]\n",
      "epoch:29 step:27700 [D loss: 0.689298, acc: 51.56%] [G loss: 1.853420]\n",
      "epoch:29 step:27701 [D loss: 0.704750, acc: 53.91%] [G loss: 1.616258]\n",
      "epoch:29 step:27702 [D loss: 0.683790, acc: 57.81%] [G loss: 1.721852]\n",
      "epoch:29 step:27703 [D loss: 0.683818, acc: 53.91%] [G loss: 1.757705]\n",
      "epoch:29 step:27704 [D loss: 0.621199, acc: 63.28%] [G loss: 1.778666]\n",
      "epoch:29 step:27705 [D loss: 0.605992, acc: 71.09%] [G loss: 1.963203]\n",
      "epoch:29 step:27706 [D loss: 0.582837, acc: 69.53%] [G loss: 1.942334]\n",
      "epoch:29 step:27707 [D loss: 0.698736, acc: 55.47%] [G loss: 2.040568]\n",
      "epoch:29 step:27708 [D loss: 0.624276, acc: 66.41%] [G loss: 1.793887]\n",
      "epoch:29 step:27709 [D loss: 0.650236, acc: 61.72%] [G loss: 1.822463]\n",
      "epoch:29 step:27710 [D loss: 0.689527, acc: 53.12%] [G loss: 1.677000]\n",
      "epoch:29 step:27711 [D loss: 0.667511, acc: 53.12%] [G loss: 1.776076]\n",
      "epoch:29 step:27712 [D loss: 0.636363, acc: 61.72%] [G loss: 1.937587]\n",
      "epoch:29 step:27713 [D loss: 0.641958, acc: 63.28%] [G loss: 1.714080]\n",
      "epoch:29 step:27714 [D loss: 0.654174, acc: 60.94%] [G loss: 1.788626]\n",
      "epoch:29 step:27715 [D loss: 0.730973, acc: 52.34%] [G loss: 1.848577]\n",
      "epoch:29 step:27716 [D loss: 0.654330, acc: 60.16%] [G loss: 1.996322]\n",
      "epoch:29 step:27717 [D loss: 0.692295, acc: 63.28%] [G loss: 1.884619]\n",
      "epoch:29 step:27718 [D loss: 0.632989, acc: 65.62%] [G loss: 2.051900]\n",
      "epoch:29 step:27719 [D loss: 0.648173, acc: 60.94%] [G loss: 1.886442]\n",
      "epoch:29 step:27720 [D loss: 0.620918, acc: 65.62%] [G loss: 1.810103]\n",
      "epoch:29 step:27721 [D loss: 0.600647, acc: 64.06%] [G loss: 1.989970]\n",
      "epoch:29 step:27722 [D loss: 0.610765, acc: 65.62%] [G loss: 2.052212]\n",
      "epoch:29 step:27723 [D loss: 0.642779, acc: 54.69%] [G loss: 1.956773]\n",
      "epoch:29 step:27724 [D loss: 0.656204, acc: 62.50%] [G loss: 1.990373]\n",
      "epoch:29 step:27725 [D loss: 0.606122, acc: 67.97%] [G loss: 2.003593]\n",
      "epoch:29 step:27726 [D loss: 0.709182, acc: 56.25%] [G loss: 1.905791]\n",
      "epoch:29 step:27727 [D loss: 0.635642, acc: 64.06%] [G loss: 2.081884]\n",
      "epoch:29 step:27728 [D loss: 0.656929, acc: 58.59%] [G loss: 1.884287]\n",
      "epoch:29 step:27729 [D loss: 0.614037, acc: 68.75%] [G loss: 2.035625]\n",
      "epoch:29 step:27730 [D loss: 0.585399, acc: 72.66%] [G loss: 2.057093]\n",
      "epoch:29 step:27731 [D loss: 0.657403, acc: 61.72%] [G loss: 2.073363]\n",
      "epoch:29 step:27732 [D loss: 0.716755, acc: 50.00%] [G loss: 1.873508]\n",
      "epoch:29 step:27733 [D loss: 0.643859, acc: 60.94%] [G loss: 1.867282]\n",
      "epoch:29 step:27734 [D loss: 0.647100, acc: 60.16%] [G loss: 1.805634]\n",
      "epoch:29 step:27735 [D loss: 0.607809, acc: 69.53%] [G loss: 2.086246]\n",
      "epoch:29 step:27736 [D loss: 0.634550, acc: 60.94%] [G loss: 2.088142]\n",
      "epoch:29 step:27737 [D loss: 0.584862, acc: 69.53%] [G loss: 2.112812]\n",
      "epoch:29 step:27738 [D loss: 0.689277, acc: 58.59%] [G loss: 1.772937]\n",
      "epoch:29 step:27739 [D loss: 0.731358, acc: 51.56%] [G loss: 1.666994]\n",
      "epoch:29 step:27740 [D loss: 0.671226, acc: 63.28%] [G loss: 1.858117]\n",
      "epoch:29 step:27741 [D loss: 0.696066, acc: 55.47%] [G loss: 1.825395]\n",
      "epoch:29 step:27742 [D loss: 0.645173, acc: 62.50%] [G loss: 1.789318]\n",
      "epoch:29 step:27743 [D loss: 0.670101, acc: 57.81%] [G loss: 1.724545]\n",
      "epoch:29 step:27744 [D loss: 0.643136, acc: 64.06%] [G loss: 1.766158]\n",
      "epoch:29 step:27745 [D loss: 0.645030, acc: 67.19%] [G loss: 1.807503]\n",
      "epoch:29 step:27746 [D loss: 0.670770, acc: 56.25%] [G loss: 1.791960]\n",
      "epoch:29 step:27747 [D loss: 0.649614, acc: 66.41%] [G loss: 1.972943]\n",
      "epoch:29 step:27748 [D loss: 0.711414, acc: 50.00%] [G loss: 1.774752]\n",
      "epoch:29 step:27749 [D loss: 0.676436, acc: 57.03%] [G loss: 1.715529]\n",
      "epoch:29 step:27750 [D loss: 0.658047, acc: 63.28%] [G loss: 1.816574]\n",
      "epoch:29 step:27751 [D loss: 0.659478, acc: 60.94%] [G loss: 1.849615]\n",
      "epoch:29 step:27752 [D loss: 0.654740, acc: 66.41%] [G loss: 1.714295]\n",
      "epoch:29 step:27753 [D loss: 0.667012, acc: 57.03%] [G loss: 1.823013]\n",
      "epoch:29 step:27754 [D loss: 0.690383, acc: 53.12%] [G loss: 1.787944]\n",
      "epoch:29 step:27755 [D loss: 0.641159, acc: 64.06%] [G loss: 1.853274]\n",
      "epoch:29 step:27756 [D loss: 0.655711, acc: 60.94%] [G loss: 1.858000]\n",
      "epoch:29 step:27757 [D loss: 0.630346, acc: 57.03%] [G loss: 1.896981]\n",
      "epoch:29 step:27758 [D loss: 0.671163, acc: 59.38%] [G loss: 1.907509]\n",
      "epoch:29 step:27759 [D loss: 0.656316, acc: 64.06%] [G loss: 1.767317]\n",
      "epoch:29 step:27760 [D loss: 0.692120, acc: 55.47%] [G loss: 1.905312]\n",
      "epoch:29 step:27761 [D loss: 0.607809, acc: 66.41%] [G loss: 1.861678]\n",
      "epoch:29 step:27762 [D loss: 0.652758, acc: 61.72%] [G loss: 1.913491]\n",
      "epoch:29 step:27763 [D loss: 0.658963, acc: 60.94%] [G loss: 1.806972]\n",
      "epoch:29 step:27764 [D loss: 0.623129, acc: 64.06%] [G loss: 1.863335]\n",
      "epoch:29 step:27765 [D loss: 0.651450, acc: 57.81%] [G loss: 1.774154]\n",
      "epoch:29 step:27766 [D loss: 0.673930, acc: 59.38%] [G loss: 1.861228]\n",
      "epoch:29 step:27767 [D loss: 0.637373, acc: 60.94%] [G loss: 1.847126]\n",
      "epoch:29 step:27768 [D loss: 0.651562, acc: 62.50%] [G loss: 1.897370]\n",
      "epoch:29 step:27769 [D loss: 0.677283, acc: 59.38%] [G loss: 1.830081]\n",
      "epoch:29 step:27770 [D loss: 0.662692, acc: 58.59%] [G loss: 1.909442]\n",
      "epoch:29 step:27771 [D loss: 0.641472, acc: 60.94%] [G loss: 1.815696]\n",
      "epoch:29 step:27772 [D loss: 0.601792, acc: 70.31%] [G loss: 1.920635]\n",
      "epoch:29 step:27773 [D loss: 0.645109, acc: 61.72%] [G loss: 1.944649]\n",
      "epoch:29 step:27774 [D loss: 0.653111, acc: 64.84%] [G loss: 1.945828]\n",
      "epoch:29 step:27775 [D loss: 0.604779, acc: 65.62%] [G loss: 2.033773]\n",
      "epoch:29 step:27776 [D loss: 0.707031, acc: 54.69%] [G loss: 2.052433]\n",
      "epoch:29 step:27777 [D loss: 0.664372, acc: 60.16%] [G loss: 1.900275]\n",
      "epoch:29 step:27778 [D loss: 0.629402, acc: 67.19%] [G loss: 2.024925]\n",
      "epoch:29 step:27779 [D loss: 0.662041, acc: 58.59%] [G loss: 1.819263]\n",
      "epoch:29 step:27780 [D loss: 0.654755, acc: 60.16%] [G loss: 1.883027]\n",
      "epoch:29 step:27781 [D loss: 0.693858, acc: 55.47%] [G loss: 1.805917]\n",
      "epoch:29 step:27782 [D loss: 0.648351, acc: 64.06%] [G loss: 1.851819]\n",
      "epoch:29 step:27783 [D loss: 0.623252, acc: 67.97%] [G loss: 1.789840]\n",
      "epoch:29 step:27784 [D loss: 0.628460, acc: 61.72%] [G loss: 1.891362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27785 [D loss: 0.608133, acc: 67.97%] [G loss: 1.965680]\n",
      "epoch:29 step:27786 [D loss: 0.703209, acc: 57.81%] [G loss: 1.855056]\n",
      "epoch:29 step:27787 [D loss: 0.688432, acc: 56.25%] [G loss: 1.827453]\n",
      "epoch:29 step:27788 [D loss: 0.699960, acc: 52.34%] [G loss: 1.791870]\n",
      "epoch:29 step:27789 [D loss: 0.684099, acc: 55.47%] [G loss: 1.818645]\n",
      "epoch:29 step:27790 [D loss: 0.676000, acc: 60.94%] [G loss: 1.738539]\n",
      "epoch:29 step:27791 [D loss: 0.652366, acc: 60.94%] [G loss: 1.918071]\n",
      "epoch:29 step:27792 [D loss: 0.646927, acc: 67.19%] [G loss: 1.813283]\n",
      "epoch:29 step:27793 [D loss: 0.625768, acc: 60.94%] [G loss: 1.795396]\n",
      "epoch:29 step:27794 [D loss: 0.652920, acc: 63.28%] [G loss: 1.770909]\n",
      "epoch:29 step:27795 [D loss: 0.670973, acc: 67.19%] [G loss: 1.891302]\n",
      "epoch:29 step:27796 [D loss: 0.632710, acc: 64.06%] [G loss: 1.953150]\n",
      "epoch:29 step:27797 [D loss: 0.616726, acc: 60.94%] [G loss: 2.050152]\n",
      "epoch:29 step:27798 [D loss: 0.718976, acc: 51.56%] [G loss: 1.812433]\n",
      "epoch:29 step:27799 [D loss: 0.652128, acc: 63.28%] [G loss: 1.830442]\n",
      "epoch:29 step:27800 [D loss: 0.625346, acc: 67.19%] [G loss: 1.879843]\n",
      "##############\n",
      "[2.58835485 1.6929511  6.05905675 4.72718369 3.59657128 5.71385993\n",
      " 4.39359529 4.65031603 4.50503968 3.61210928]\n",
      "##########\n",
      "epoch:29 step:27801 [D loss: 0.688374, acc: 53.91%] [G loss: 1.809392]\n",
      "epoch:29 step:27802 [D loss: 0.624193, acc: 66.41%] [G loss: 1.877711]\n",
      "epoch:29 step:27803 [D loss: 0.616680, acc: 62.50%] [G loss: 1.883282]\n",
      "epoch:29 step:27804 [D loss: 0.622241, acc: 66.41%] [G loss: 1.985214]\n",
      "epoch:29 step:27805 [D loss: 0.631738, acc: 63.28%] [G loss: 1.961222]\n",
      "epoch:29 step:27806 [D loss: 0.629178, acc: 62.50%] [G loss: 1.983312]\n",
      "epoch:29 step:27807 [D loss: 0.632088, acc: 64.06%] [G loss: 1.957706]\n",
      "epoch:29 step:27808 [D loss: 0.664191, acc: 62.50%] [G loss: 1.853508]\n",
      "epoch:29 step:27809 [D loss: 0.639852, acc: 62.50%] [G loss: 1.963774]\n",
      "epoch:29 step:27810 [D loss: 0.669106, acc: 59.38%] [G loss: 2.024504]\n",
      "epoch:29 step:27811 [D loss: 0.667926, acc: 63.28%] [G loss: 1.857798]\n",
      "epoch:29 step:27812 [D loss: 0.661823, acc: 60.16%] [G loss: 1.830648]\n",
      "epoch:29 step:27813 [D loss: 0.705633, acc: 53.12%] [G loss: 1.832999]\n",
      "epoch:29 step:27814 [D loss: 0.680318, acc: 61.72%] [G loss: 1.817223]\n",
      "epoch:29 step:27815 [D loss: 0.652365, acc: 62.50%] [G loss: 1.902676]\n",
      "epoch:29 step:27816 [D loss: 0.616339, acc: 67.19%] [G loss: 1.884582]\n",
      "epoch:29 step:27817 [D loss: 0.621236, acc: 64.84%] [G loss: 1.870988]\n",
      "epoch:29 step:27818 [D loss: 0.643482, acc: 63.28%] [G loss: 1.830051]\n",
      "epoch:29 step:27819 [D loss: 0.636692, acc: 61.72%] [G loss: 1.964712]\n",
      "epoch:29 step:27820 [D loss: 0.611108, acc: 67.19%] [G loss: 2.007037]\n",
      "epoch:29 step:27821 [D loss: 0.580365, acc: 73.44%] [G loss: 2.120159]\n",
      "epoch:29 step:27822 [D loss: 0.630447, acc: 64.84%] [G loss: 2.158452]\n",
      "epoch:29 step:27823 [D loss: 0.631513, acc: 64.06%] [G loss: 1.996118]\n",
      "epoch:29 step:27824 [D loss: 0.613453, acc: 64.84%] [G loss: 1.965650]\n",
      "epoch:29 step:27825 [D loss: 0.665851, acc: 64.06%] [G loss: 2.061459]\n",
      "epoch:29 step:27826 [D loss: 0.621705, acc: 64.84%] [G loss: 1.957027]\n",
      "epoch:29 step:27827 [D loss: 0.614066, acc: 67.97%] [G loss: 1.960057]\n",
      "epoch:29 step:27828 [D loss: 0.685436, acc: 56.25%] [G loss: 1.813090]\n",
      "epoch:29 step:27829 [D loss: 0.689042, acc: 57.03%] [G loss: 1.771568]\n",
      "epoch:29 step:27830 [D loss: 0.705201, acc: 52.34%] [G loss: 1.840732]\n",
      "epoch:29 step:27831 [D loss: 0.700739, acc: 52.34%] [G loss: 1.690201]\n",
      "epoch:29 step:27832 [D loss: 0.664962, acc: 61.72%] [G loss: 1.629826]\n",
      "epoch:29 step:27833 [D loss: 0.614722, acc: 67.19%] [G loss: 1.927348]\n",
      "epoch:29 step:27834 [D loss: 0.648553, acc: 69.53%] [G loss: 1.824569]\n",
      "epoch:29 step:27835 [D loss: 0.636291, acc: 57.81%] [G loss: 2.022758]\n",
      "epoch:29 step:27836 [D loss: 0.650813, acc: 57.81%] [G loss: 1.779100]\n",
      "epoch:29 step:27837 [D loss: 0.624824, acc: 63.28%] [G loss: 1.776288]\n",
      "epoch:29 step:27838 [D loss: 0.653007, acc: 58.59%] [G loss: 1.839672]\n",
      "epoch:29 step:27839 [D loss: 0.654316, acc: 59.38%] [G loss: 1.865165]\n",
      "epoch:29 step:27840 [D loss: 0.700549, acc: 58.59%] [G loss: 1.785304]\n",
      "epoch:29 step:27841 [D loss: 0.625310, acc: 64.06%] [G loss: 1.910180]\n",
      "epoch:29 step:27842 [D loss: 0.669123, acc: 64.84%] [G loss: 1.757669]\n",
      "epoch:29 step:27843 [D loss: 0.658598, acc: 65.62%] [G loss: 1.880975]\n",
      "epoch:29 step:27844 [D loss: 0.627429, acc: 60.94%] [G loss: 1.766180]\n",
      "epoch:29 step:27845 [D loss: 0.660716, acc: 62.50%] [G loss: 1.838968]\n",
      "epoch:29 step:27846 [D loss: 0.631482, acc: 64.06%] [G loss: 1.749897]\n",
      "epoch:29 step:27847 [D loss: 0.658509, acc: 60.16%] [G loss: 1.863358]\n",
      "epoch:29 step:27848 [D loss: 0.701717, acc: 56.25%] [G loss: 1.750349]\n",
      "epoch:29 step:27849 [D loss: 0.687081, acc: 58.59%] [G loss: 1.690861]\n",
      "epoch:29 step:27850 [D loss: 0.623024, acc: 72.66%] [G loss: 1.822488]\n",
      "epoch:29 step:27851 [D loss: 0.631534, acc: 65.62%] [G loss: 1.867815]\n",
      "epoch:29 step:27852 [D loss: 0.617993, acc: 67.19%] [G loss: 1.852906]\n",
      "epoch:29 step:27853 [D loss: 0.640958, acc: 61.72%] [G loss: 1.942391]\n",
      "epoch:29 step:27854 [D loss: 0.617881, acc: 66.41%] [G loss: 2.057089]\n",
      "epoch:29 step:27855 [D loss: 0.707658, acc: 54.69%] [G loss: 1.882072]\n",
      "epoch:29 step:27856 [D loss: 0.673486, acc: 57.03%] [G loss: 1.891936]\n",
      "epoch:29 step:27857 [D loss: 0.681003, acc: 60.94%] [G loss: 1.867781]\n",
      "epoch:29 step:27858 [D loss: 0.616390, acc: 65.62%] [G loss: 1.720869]\n",
      "epoch:29 step:27859 [D loss: 0.620072, acc: 64.06%] [G loss: 1.857603]\n",
      "epoch:29 step:27860 [D loss: 0.679686, acc: 54.69%] [G loss: 2.000033]\n",
      "epoch:29 step:27861 [D loss: 0.586993, acc: 71.09%] [G loss: 1.983464]\n",
      "epoch:29 step:27862 [D loss: 0.650771, acc: 60.94%] [G loss: 1.915427]\n",
      "epoch:29 step:27863 [D loss: 0.671433, acc: 58.59%] [G loss: 2.031232]\n",
      "epoch:29 step:27864 [D loss: 0.602388, acc: 65.62%] [G loss: 1.985647]\n",
      "epoch:29 step:27865 [D loss: 0.610943, acc: 67.97%] [G loss: 2.076149]\n",
      "epoch:29 step:27866 [D loss: 0.567052, acc: 72.66%] [G loss: 2.045571]\n",
      "epoch:29 step:27867 [D loss: 0.566265, acc: 72.66%] [G loss: 2.052471]\n",
      "epoch:29 step:27868 [D loss: 0.629745, acc: 64.06%] [G loss: 2.031825]\n",
      "epoch:29 step:27869 [D loss: 0.673487, acc: 60.16%] [G loss: 1.886211]\n",
      "epoch:29 step:27870 [D loss: 0.633279, acc: 65.62%] [G loss: 2.025618]\n",
      "epoch:29 step:27871 [D loss: 0.687193, acc: 54.69%] [G loss: 1.791301]\n",
      "epoch:29 step:27872 [D loss: 0.674158, acc: 57.81%] [G loss: 1.928935]\n",
      "epoch:29 step:27873 [D loss: 0.627495, acc: 67.19%] [G loss: 1.929472]\n",
      "epoch:29 step:27874 [D loss: 0.622160, acc: 65.62%] [G loss: 1.925009]\n",
      "epoch:29 step:27875 [D loss: 0.672267, acc: 58.59%] [G loss: 1.803297]\n",
      "epoch:29 step:27876 [D loss: 0.711165, acc: 49.22%] [G loss: 1.852388]\n",
      "epoch:29 step:27877 [D loss: 0.636953, acc: 62.50%] [G loss: 1.767135]\n",
      "epoch:29 step:27878 [D loss: 0.675663, acc: 57.81%] [G loss: 1.786684]\n",
      "epoch:29 step:27879 [D loss: 0.563531, acc: 74.22%] [G loss: 2.042172]\n",
      "epoch:29 step:27880 [D loss: 0.603578, acc: 65.62%] [G loss: 2.091878]\n",
      "epoch:29 step:27881 [D loss: 0.610984, acc: 61.72%] [G loss: 2.019766]\n",
      "epoch:29 step:27882 [D loss: 0.600716, acc: 71.09%] [G loss: 2.027922]\n",
      "epoch:29 step:27883 [D loss: 0.715155, acc: 50.00%] [G loss: 1.755975]\n",
      "epoch:29 step:27884 [D loss: 0.634593, acc: 66.41%] [G loss: 1.940284]\n",
      "epoch:29 step:27885 [D loss: 0.588914, acc: 65.62%] [G loss: 1.929669]\n",
      "epoch:29 step:27886 [D loss: 0.658661, acc: 58.59%] [G loss: 1.981503]\n",
      "epoch:29 step:27887 [D loss: 0.682095, acc: 60.94%] [G loss: 1.926730]\n",
      "epoch:29 step:27888 [D loss: 0.629129, acc: 68.75%] [G loss: 1.835187]\n",
      "epoch:29 step:27889 [D loss: 0.714228, acc: 55.47%] [G loss: 1.748270]\n",
      "epoch:29 step:27890 [D loss: 0.662545, acc: 60.94%] [G loss: 1.823771]\n",
      "epoch:29 step:27891 [D loss: 0.659492, acc: 58.59%] [G loss: 1.796818]\n",
      "epoch:29 step:27892 [D loss: 0.618350, acc: 68.75%] [G loss: 1.960889]\n",
      "epoch:29 step:27893 [D loss: 0.654994, acc: 55.47%] [G loss: 1.889902]\n",
      "epoch:29 step:27894 [D loss: 0.632865, acc: 68.75%] [G loss: 1.967324]\n",
      "epoch:29 step:27895 [D loss: 0.650903, acc: 60.94%] [G loss: 1.818608]\n",
      "epoch:29 step:27896 [D loss: 0.682585, acc: 51.56%] [G loss: 1.837375]\n",
      "epoch:29 step:27897 [D loss: 0.639106, acc: 61.72%] [G loss: 1.723902]\n",
      "epoch:29 step:27898 [D loss: 0.673842, acc: 56.25%] [G loss: 1.942725]\n",
      "epoch:29 step:27899 [D loss: 0.630364, acc: 60.94%] [G loss: 1.829379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27900 [D loss: 0.671824, acc: 62.50%] [G loss: 1.850643]\n",
      "epoch:29 step:27901 [D loss: 0.669560, acc: 60.94%] [G loss: 1.858865]\n",
      "epoch:29 step:27902 [D loss: 0.682136, acc: 57.81%] [G loss: 1.823328]\n",
      "epoch:29 step:27903 [D loss: 0.649981, acc: 62.50%] [G loss: 1.767859]\n",
      "epoch:29 step:27904 [D loss: 0.622555, acc: 64.84%] [G loss: 1.831787]\n",
      "epoch:29 step:27905 [D loss: 0.665841, acc: 56.25%] [G loss: 1.796183]\n",
      "epoch:29 step:27906 [D loss: 0.623570, acc: 61.72%] [G loss: 2.004740]\n",
      "epoch:29 step:27907 [D loss: 0.657718, acc: 60.16%] [G loss: 1.693917]\n",
      "epoch:29 step:27908 [D loss: 0.698374, acc: 53.91%] [G loss: 1.964904]\n",
      "epoch:29 step:27909 [D loss: 0.592635, acc: 69.53%] [G loss: 1.986781]\n",
      "epoch:29 step:27910 [D loss: 0.598863, acc: 69.53%] [G loss: 1.858143]\n",
      "epoch:29 step:27911 [D loss: 0.731234, acc: 54.69%] [G loss: 1.843491]\n",
      "epoch:29 step:27912 [D loss: 0.639030, acc: 60.94%] [G loss: 1.894652]\n",
      "epoch:29 step:27913 [D loss: 0.650037, acc: 55.47%] [G loss: 2.007142]\n",
      "epoch:29 step:27914 [D loss: 0.621209, acc: 69.53%] [G loss: 1.810031]\n",
      "epoch:29 step:27915 [D loss: 0.669563, acc: 60.94%] [G loss: 1.906776]\n",
      "epoch:29 step:27916 [D loss: 0.653674, acc: 60.94%] [G loss: 1.753379]\n",
      "epoch:29 step:27917 [D loss: 0.623995, acc: 63.28%] [G loss: 1.842125]\n",
      "epoch:29 step:27918 [D loss: 0.648976, acc: 58.59%] [G loss: 1.878252]\n",
      "epoch:29 step:27919 [D loss: 0.586426, acc: 70.31%] [G loss: 1.989239]\n",
      "epoch:29 step:27920 [D loss: 0.589850, acc: 68.75%] [G loss: 2.022528]\n",
      "epoch:29 step:27921 [D loss: 0.647711, acc: 65.62%] [G loss: 1.749943]\n",
      "epoch:29 step:27922 [D loss: 0.642494, acc: 60.94%] [G loss: 1.848517]\n",
      "epoch:29 step:27923 [D loss: 0.633810, acc: 60.94%] [G loss: 1.866183]\n",
      "epoch:29 step:27924 [D loss: 0.672403, acc: 60.16%] [G loss: 1.856774]\n",
      "epoch:29 step:27925 [D loss: 0.717407, acc: 56.25%] [G loss: 1.597333]\n",
      "epoch:29 step:27926 [D loss: 0.623349, acc: 57.03%] [G loss: 1.904598]\n",
      "epoch:29 step:27927 [D loss: 0.629928, acc: 66.41%] [G loss: 1.847604]\n",
      "epoch:29 step:27928 [D loss: 0.620780, acc: 65.62%] [G loss: 1.758696]\n",
      "epoch:29 step:27929 [D loss: 0.652733, acc: 65.62%] [G loss: 1.982329]\n",
      "epoch:29 step:27930 [D loss: 0.675193, acc: 57.03%] [G loss: 1.805290]\n",
      "epoch:29 step:27931 [D loss: 0.662017, acc: 62.50%] [G loss: 1.711047]\n",
      "epoch:29 step:27932 [D loss: 0.641821, acc: 60.94%] [G loss: 1.765849]\n",
      "epoch:29 step:27933 [D loss: 0.689185, acc: 54.69%] [G loss: 1.747553]\n",
      "epoch:29 step:27934 [D loss: 0.694811, acc: 52.34%] [G loss: 1.799397]\n",
      "epoch:29 step:27935 [D loss: 0.679494, acc: 57.03%] [G loss: 1.721597]\n",
      "epoch:29 step:27936 [D loss: 0.664125, acc: 60.94%] [G loss: 1.748276]\n",
      "epoch:29 step:27937 [D loss: 0.639015, acc: 68.75%] [G loss: 1.726948]\n",
      "epoch:29 step:27938 [D loss: 0.739098, acc: 53.91%] [G loss: 1.790370]\n",
      "epoch:29 step:27939 [D loss: 0.663755, acc: 60.94%] [G loss: 1.877354]\n",
      "epoch:29 step:27940 [D loss: 0.623854, acc: 62.50%] [G loss: 1.810490]\n",
      "epoch:29 step:27941 [D loss: 0.685314, acc: 55.47%] [G loss: 1.805295]\n",
      "epoch:29 step:27942 [D loss: 0.655062, acc: 67.19%] [G loss: 1.894882]\n",
      "epoch:29 step:27943 [D loss: 0.653576, acc: 61.72%] [G loss: 1.841840]\n",
      "epoch:29 step:27944 [D loss: 0.701016, acc: 51.56%] [G loss: 1.863283]\n",
      "epoch:29 step:27945 [D loss: 0.664786, acc: 60.16%] [G loss: 1.898963]\n",
      "epoch:29 step:27946 [D loss: 0.677109, acc: 52.34%] [G loss: 1.845457]\n",
      "epoch:29 step:27947 [D loss: 0.618325, acc: 66.41%] [G loss: 1.964187]\n",
      "epoch:29 step:27948 [D loss: 0.617484, acc: 64.84%] [G loss: 2.064020]\n",
      "epoch:29 step:27949 [D loss: 0.651299, acc: 56.25%] [G loss: 1.889299]\n",
      "epoch:29 step:27950 [D loss: 0.664872, acc: 63.28%] [G loss: 1.898862]\n",
      "epoch:29 step:27951 [D loss: 0.649797, acc: 57.81%] [G loss: 1.786493]\n",
      "epoch:29 step:27952 [D loss: 0.645427, acc: 67.19%] [G loss: 2.014465]\n",
      "epoch:29 step:27953 [D loss: 0.629668, acc: 61.72%] [G loss: 1.971182]\n",
      "epoch:29 step:27954 [D loss: 0.606170, acc: 73.44%] [G loss: 2.002773]\n",
      "epoch:29 step:27955 [D loss: 0.569016, acc: 74.22%] [G loss: 2.132837]\n",
      "epoch:29 step:27956 [D loss: 0.682865, acc: 57.03%] [G loss: 1.962252]\n",
      "epoch:29 step:27957 [D loss: 0.686290, acc: 58.59%] [G loss: 1.905456]\n",
      "epoch:29 step:27958 [D loss: 0.636422, acc: 64.84%] [G loss: 1.749164]\n",
      "epoch:29 step:27959 [D loss: 0.682898, acc: 63.28%] [G loss: 1.922327]\n",
      "epoch:29 step:27960 [D loss: 0.633875, acc: 58.59%] [G loss: 1.833358]\n",
      "epoch:29 step:27961 [D loss: 0.726521, acc: 51.56%] [G loss: 1.721838]\n",
      "epoch:29 step:27962 [D loss: 0.638908, acc: 65.62%] [G loss: 1.871031]\n",
      "epoch:29 step:27963 [D loss: 0.639294, acc: 60.94%] [G loss: 1.774621]\n",
      "epoch:29 step:27964 [D loss: 0.628321, acc: 65.62%] [G loss: 1.815034]\n",
      "epoch:29 step:27965 [D loss: 0.582028, acc: 64.06%] [G loss: 1.978170]\n",
      "epoch:29 step:27966 [D loss: 0.685539, acc: 60.16%] [G loss: 1.940404]\n",
      "epoch:29 step:27967 [D loss: 0.643239, acc: 61.72%] [G loss: 1.788476]\n",
      "epoch:29 step:27968 [D loss: 0.729506, acc: 53.91%] [G loss: 1.805291]\n",
      "epoch:29 step:27969 [D loss: 0.666459, acc: 56.25%] [G loss: 1.847107]\n",
      "epoch:29 step:27970 [D loss: 0.659505, acc: 65.62%] [G loss: 1.901114]\n",
      "epoch:29 step:27971 [D loss: 0.667381, acc: 57.81%] [G loss: 1.866748]\n",
      "epoch:29 step:27972 [D loss: 0.693384, acc: 53.91%] [G loss: 1.836698]\n",
      "epoch:29 step:27973 [D loss: 0.726444, acc: 56.25%] [G loss: 1.653573]\n",
      "epoch:29 step:27974 [D loss: 0.668230, acc: 57.03%] [G loss: 1.806144]\n",
      "epoch:29 step:27975 [D loss: 0.613778, acc: 66.41%] [G loss: 1.849107]\n",
      "epoch:29 step:27976 [D loss: 0.661866, acc: 58.59%] [G loss: 1.782678]\n",
      "epoch:29 step:27977 [D loss: 0.677789, acc: 55.47%] [G loss: 1.738946]\n",
      "epoch:29 step:27978 [D loss: 0.615185, acc: 67.19%] [G loss: 1.789612]\n",
      "epoch:29 step:27979 [D loss: 0.652372, acc: 63.28%] [G loss: 1.798518]\n",
      "epoch:29 step:27980 [D loss: 0.626318, acc: 60.16%] [G loss: 1.884534]\n",
      "epoch:29 step:27981 [D loss: 0.643676, acc: 62.50%] [G loss: 1.784192]\n",
      "epoch:29 step:27982 [D loss: 0.634393, acc: 64.06%] [G loss: 1.830279]\n",
      "epoch:29 step:27983 [D loss: 0.632993, acc: 68.75%] [G loss: 1.842643]\n",
      "epoch:29 step:27984 [D loss: 0.685819, acc: 54.69%] [G loss: 1.747853]\n",
      "epoch:29 step:27985 [D loss: 0.671645, acc: 55.47%] [G loss: 1.880301]\n",
      "epoch:29 step:27986 [D loss: 0.672159, acc: 59.38%] [G loss: 1.776890]\n",
      "epoch:29 step:27987 [D loss: 0.645304, acc: 63.28%] [G loss: 1.890952]\n",
      "epoch:29 step:27988 [D loss: 0.609862, acc: 65.62%] [G loss: 2.129519]\n",
      "epoch:29 step:27989 [D loss: 0.576724, acc: 70.31%] [G loss: 1.940423]\n",
      "epoch:29 step:27990 [D loss: 0.628138, acc: 62.50%] [G loss: 1.830520]\n",
      "epoch:29 step:27991 [D loss: 0.737202, acc: 47.66%] [G loss: 1.679925]\n",
      "epoch:29 step:27992 [D loss: 0.643464, acc: 64.84%] [G loss: 1.895249]\n",
      "epoch:29 step:27993 [D loss: 0.712691, acc: 54.69%] [G loss: 1.658277]\n",
      "epoch:29 step:27994 [D loss: 0.661350, acc: 63.28%] [G loss: 1.748240]\n",
      "epoch:29 step:27995 [D loss: 0.623129, acc: 68.75%] [G loss: 1.809602]\n",
      "epoch:29 step:27996 [D loss: 0.625167, acc: 63.28%] [G loss: 1.924494]\n",
      "epoch:29 step:27997 [D loss: 0.652163, acc: 58.59%] [G loss: 1.896020]\n",
      "epoch:29 step:27998 [D loss: 0.639378, acc: 67.19%] [G loss: 1.899692]\n",
      "epoch:29 step:27999 [D loss: 0.650887, acc: 52.34%] [G loss: 1.867048]\n",
      "epoch:29 step:28000 [D loss: 0.640710, acc: 64.06%] [G loss: 1.793943]\n",
      "##############\n",
      "[2.44540226 1.31732662 6.0231491  4.73211809 3.30262608 5.74754863\n",
      " 4.49732288 4.59969843 4.36198906 3.64070914]\n",
      "##########\n",
      "epoch:29 step:28001 [D loss: 0.686692, acc: 56.25%] [G loss: 1.689991]\n",
      "epoch:29 step:28002 [D loss: 0.679706, acc: 57.81%] [G loss: 1.811562]\n",
      "epoch:29 step:28003 [D loss: 0.714191, acc: 49.22%] [G loss: 1.786560]\n",
      "epoch:29 step:28004 [D loss: 0.667207, acc: 59.38%] [G loss: 1.800544]\n",
      "epoch:29 step:28005 [D loss: 0.656843, acc: 64.84%] [G loss: 1.814271]\n",
      "epoch:29 step:28006 [D loss: 0.630658, acc: 65.62%] [G loss: 1.976512]\n",
      "epoch:29 step:28007 [D loss: 0.657146, acc: 60.16%] [G loss: 1.756081]\n",
      "epoch:29 step:28008 [D loss: 0.657592, acc: 59.38%] [G loss: 1.826882]\n",
      "epoch:29 step:28009 [D loss: 0.671240, acc: 63.28%] [G loss: 1.933539]\n",
      "epoch:29 step:28010 [D loss: 0.607102, acc: 64.84%] [G loss: 1.885550]\n",
      "epoch:29 step:28011 [D loss: 0.613209, acc: 67.19%] [G loss: 1.939424]\n",
      "epoch:29 step:28012 [D loss: 0.650163, acc: 61.72%] [G loss: 1.783502]\n",
      "epoch:29 step:28013 [D loss: 0.678827, acc: 54.69%] [G loss: 1.990838]\n",
      "epoch:29 step:28014 [D loss: 0.639937, acc: 61.72%] [G loss: 1.960064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28015 [D loss: 0.624531, acc: 63.28%] [G loss: 1.898288]\n",
      "epoch:29 step:28016 [D loss: 0.655608, acc: 61.72%] [G loss: 1.924232]\n",
      "epoch:29 step:28017 [D loss: 0.657845, acc: 57.81%] [G loss: 1.914780]\n",
      "epoch:29 step:28018 [D loss: 0.628602, acc: 60.16%] [G loss: 1.876128]\n",
      "epoch:29 step:28019 [D loss: 0.678655, acc: 61.72%] [G loss: 1.828905]\n",
      "epoch:29 step:28020 [D loss: 0.589605, acc: 66.41%] [G loss: 1.866910]\n",
      "epoch:29 step:28021 [D loss: 0.639463, acc: 64.06%] [G loss: 1.747290]\n",
      "epoch:29 step:28022 [D loss: 0.618296, acc: 64.06%] [G loss: 1.890716]\n",
      "epoch:29 step:28023 [D loss: 0.689719, acc: 52.34%] [G loss: 1.620619]\n",
      "epoch:29 step:28024 [D loss: 0.685382, acc: 57.81%] [G loss: 1.807664]\n",
      "epoch:29 step:28025 [D loss: 0.693895, acc: 56.25%] [G loss: 1.968593]\n",
      "epoch:29 step:28026 [D loss: 0.758088, acc: 46.88%] [G loss: 1.878789]\n",
      "epoch:29 step:28027 [D loss: 0.653259, acc: 58.59%] [G loss: 1.855421]\n",
      "epoch:29 step:28028 [D loss: 0.699560, acc: 54.69%] [G loss: 1.663760]\n",
      "epoch:29 step:28029 [D loss: 0.665142, acc: 59.38%] [G loss: 1.807381]\n",
      "epoch:29 step:28030 [D loss: 0.637451, acc: 61.72%] [G loss: 1.798133]\n",
      "epoch:29 step:28031 [D loss: 0.740474, acc: 52.34%] [G loss: 1.780275]\n",
      "epoch:29 step:28032 [D loss: 0.705529, acc: 53.91%] [G loss: 1.805808]\n",
      "epoch:29 step:28033 [D loss: 0.658740, acc: 55.47%] [G loss: 1.871655]\n",
      "epoch:29 step:28034 [D loss: 0.643911, acc: 65.62%] [G loss: 1.630296]\n",
      "epoch:29 step:28035 [D loss: 0.646545, acc: 62.50%] [G loss: 1.738876]\n",
      "epoch:29 step:28036 [D loss: 0.636111, acc: 63.28%] [G loss: 1.787755]\n",
      "epoch:29 step:28037 [D loss: 0.619564, acc: 61.72%] [G loss: 1.717200]\n",
      "epoch:29 step:28038 [D loss: 0.644229, acc: 61.72%] [G loss: 1.812384]\n",
      "epoch:29 step:28039 [D loss: 0.693885, acc: 50.00%] [G loss: 1.891725]\n",
      "epoch:29 step:28040 [D loss: 0.639172, acc: 61.72%] [G loss: 1.739228]\n",
      "epoch:29 step:28041 [D loss: 0.650869, acc: 61.72%] [G loss: 1.931597]\n",
      "epoch:29 step:28042 [D loss: 0.651815, acc: 60.94%] [G loss: 1.813054]\n",
      "epoch:29 step:28043 [D loss: 0.617646, acc: 64.06%] [G loss: 1.895494]\n",
      "epoch:29 step:28044 [D loss: 0.657965, acc: 60.94%] [G loss: 1.827948]\n",
      "epoch:29 step:28045 [D loss: 0.670853, acc: 60.94%] [G loss: 1.809894]\n",
      "epoch:29 step:28046 [D loss: 0.704805, acc: 52.34%] [G loss: 1.783083]\n",
      "epoch:29 step:28047 [D loss: 0.648812, acc: 60.16%] [G loss: 1.723292]\n",
      "epoch:29 step:28048 [D loss: 0.626302, acc: 60.94%] [G loss: 1.985663]\n",
      "epoch:29 step:28049 [D loss: 0.606381, acc: 67.97%] [G loss: 1.851551]\n",
      "epoch:29 step:28050 [D loss: 0.682380, acc: 53.91%] [G loss: 1.793043]\n",
      "epoch:29 step:28051 [D loss: 0.638953, acc: 59.38%] [G loss: 1.763570]\n",
      "epoch:29 step:28052 [D loss: 0.649802, acc: 62.50%] [G loss: 1.764332]\n",
      "epoch:29 step:28053 [D loss: 0.629077, acc: 64.84%] [G loss: 1.879124]\n",
      "epoch:29 step:28054 [D loss: 0.627662, acc: 65.62%] [G loss: 2.008994]\n",
      "epoch:29 step:28055 [D loss: 0.669378, acc: 60.94%] [G loss: 1.882378]\n",
      "epoch:29 step:28056 [D loss: 0.659771, acc: 63.28%] [G loss: 1.798140]\n",
      "epoch:29 step:28057 [D loss: 0.636308, acc: 63.28%] [G loss: 1.909544]\n",
      "epoch:29 step:28058 [D loss: 0.672094, acc: 56.25%] [G loss: 1.902067]\n",
      "epoch:29 step:28059 [D loss: 0.647481, acc: 58.59%] [G loss: 1.918754]\n",
      "epoch:29 step:28060 [D loss: 0.681125, acc: 61.72%] [G loss: 1.798373]\n",
      "epoch:29 step:28061 [D loss: 0.643738, acc: 62.50%] [G loss: 1.914444]\n",
      "epoch:29 step:28062 [D loss: 0.606405, acc: 67.19%] [G loss: 1.919897]\n",
      "epoch:29 step:28063 [D loss: 0.658640, acc: 58.59%] [G loss: 1.854251]\n",
      "epoch:29 step:28064 [D loss: 0.686588, acc: 63.28%] [G loss: 1.697586]\n",
      "epoch:29 step:28065 [D loss: 0.671735, acc: 57.03%] [G loss: 1.823846]\n",
      "epoch:29 step:28066 [D loss: 0.716397, acc: 57.81%] [G loss: 1.861514]\n",
      "epoch:29 step:28067 [D loss: 0.608840, acc: 67.97%] [G loss: 2.046443]\n",
      "epoch:29 step:28068 [D loss: 0.673778, acc: 57.03%] [G loss: 1.685052]\n",
      "epoch:29 step:28069 [D loss: 0.682908, acc: 55.47%] [G loss: 1.795750]\n",
      "epoch:29 step:28070 [D loss: 0.614824, acc: 66.41%] [G loss: 1.913206]\n",
      "epoch:29 step:28071 [D loss: 0.653903, acc: 60.16%] [G loss: 1.830976]\n",
      "epoch:29 step:28072 [D loss: 0.620444, acc: 64.06%] [G loss: 1.902880]\n",
      "epoch:29 step:28073 [D loss: 0.666384, acc: 60.16%] [G loss: 1.708086]\n",
      "epoch:29 step:28074 [D loss: 0.609661, acc: 65.62%] [G loss: 1.915070]\n",
      "epoch:29 step:28075 [D loss: 0.660948, acc: 62.50%] [G loss: 1.895662]\n",
      "epoch:29 step:28076 [D loss: 0.648598, acc: 58.59%] [G loss: 1.822531]\n",
      "epoch:29 step:28077 [D loss: 0.638134, acc: 64.06%] [G loss: 1.965371]\n",
      "epoch:29 step:28078 [D loss: 0.646107, acc: 62.50%] [G loss: 1.892058]\n",
      "epoch:29 step:28079 [D loss: 0.657689, acc: 60.94%] [G loss: 1.884570]\n",
      "epoch:29 step:28080 [D loss: 0.579049, acc: 72.66%] [G loss: 2.047990]\n",
      "epoch:29 step:28081 [D loss: 0.653718, acc: 58.59%] [G loss: 1.924420]\n",
      "epoch:29 step:28082 [D loss: 0.624737, acc: 67.97%] [G loss: 1.954420]\n",
      "epoch:29 step:28083 [D loss: 0.654618, acc: 58.59%] [G loss: 1.886994]\n",
      "epoch:29 step:28084 [D loss: 0.658139, acc: 60.94%] [G loss: 2.028625]\n",
      "epoch:29 step:28085 [D loss: 0.656277, acc: 59.38%] [G loss: 1.984119]\n",
      "epoch:29 step:28086 [D loss: 0.623277, acc: 63.28%] [G loss: 1.866708]\n",
      "epoch:29 step:28087 [D loss: 0.667010, acc: 56.25%] [G loss: 1.838102]\n",
      "epoch:29 step:28088 [D loss: 0.706814, acc: 54.69%] [G loss: 1.870234]\n",
      "epoch:29 step:28089 [D loss: 0.657457, acc: 60.16%] [G loss: 1.922709]\n",
      "epoch:29 step:28090 [D loss: 0.681010, acc: 57.03%] [G loss: 1.881258]\n",
      "epoch:29 step:28091 [D loss: 0.619179, acc: 66.41%] [G loss: 1.960045]\n",
      "epoch:29 step:28092 [D loss: 0.600392, acc: 68.75%] [G loss: 2.172844]\n",
      "epoch:29 step:28093 [D loss: 0.731177, acc: 54.69%] [G loss: 1.746265]\n",
      "epoch:29 step:28094 [D loss: 0.618146, acc: 61.72%] [G loss: 1.943863]\n",
      "epoch:29 step:28095 [D loss: 0.609969, acc: 61.72%] [G loss: 1.965787]\n",
      "epoch:29 step:28096 [D loss: 0.624017, acc: 67.19%] [G loss: 2.032732]\n",
      "epoch:29 step:28097 [D loss: 0.578501, acc: 67.97%] [G loss: 2.161134]\n",
      "epoch:29 step:28098 [D loss: 0.646060, acc: 61.72%] [G loss: 2.128001]\n",
      "epoch:29 step:28099 [D loss: 0.567374, acc: 73.44%] [G loss: 2.076968]\n",
      "epoch:29 step:28100 [D loss: 0.721021, acc: 52.34%] [G loss: 1.980463]\n",
      "epoch:29 step:28101 [D loss: 0.741399, acc: 57.03%] [G loss: 1.834900]\n",
      "epoch:29 step:28102 [D loss: 0.817342, acc: 38.28%] [G loss: 1.698780]\n",
      "epoch:29 step:28103 [D loss: 0.643652, acc: 59.38%] [G loss: 2.024020]\n",
      "epoch:29 step:28104 [D loss: 0.643035, acc: 65.62%] [G loss: 2.074331]\n",
      "epoch:29 step:28105 [D loss: 0.664988, acc: 61.72%] [G loss: 1.799802]\n",
      "epoch:29 step:28106 [D loss: 0.677087, acc: 60.16%] [G loss: 1.894296]\n",
      "epoch:29 step:28107 [D loss: 0.638801, acc: 63.28%] [G loss: 1.871379]\n",
      "epoch:29 step:28108 [D loss: 0.633482, acc: 71.88%] [G loss: 1.993430]\n",
      "epoch:29 step:28109 [D loss: 0.560496, acc: 77.34%] [G loss: 2.100336]\n",
      "epoch:29 step:28110 [D loss: 0.611617, acc: 65.62%] [G loss: 2.500759]\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('bigan')):\n",
    "    os.mkdir('saved_models_{}'.format('bigan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('bigan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build the encoder\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        # Encode image\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model\n",
    "        # Trains generator to fool the discriminator\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "                imgs_ = self.generator.predict(z)\n",
    "\n",
    "                # Select a random batch of images and encode\n",
    "                # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                # imgs = X_train[idx]\n",
    "                z_ = self.encoder.predict(image_batch)\n",
    "\n",
    "                # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([z_, image_batch], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "                g_loss = self.bigan_generator.train_on_batch([z, image_batch], [valid, fake])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0],\n",
    "                                                                                   100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.sample_images(global_step, X_test,y_test, sampleSize)\n",
    "    def sample_images(self, epoch,x_test,y_test,sample_num):\n",
    "        r, c = sample_num//10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bigan = BIGAN()\n",
    "    bigan.train(epochs=30, batch_size=64, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
